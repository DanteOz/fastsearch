{"text": " Okay, hi everybody. So I want to talk about my kind of personal opinions about the GPGPU developer experience. I feel like we don't talk about developer experience enough. When we talk about GPGPU, we tend to focus more on performance issues and distributed computing and stuff like that. I know a lot of the audience here is from an academic background. And so folks who focus on GPGPU in academia may not have fully realized how incredibly popular GPGPU has become in the last few years. And to give you a sense, this is the downloads for CUDA toolkit on just one from just one source, which is from the Anaconda Python repository. And as we can see, 11.3 has 1.1 million downloads, 11.4, 1.1 million downloads, 11.1, 1 million downloads. We've got to a point now where literally over a million people are downloading CUDA. So what are all these people doing? They are not writing CUDA kernels. If you look at the Kaggle developer survey, actually most developers are now, data scientists are now using things like TensorFlow and PyTorch and Lightning and Fast AI. And so GPGPU is being used extremely extensively around the world now through these higher level libraries and nearly always via Python. But the thing is that these libraries like PyTorch behind the scenes, they're calling compiled C libraries such as for deep learning, cuDNN or the PyTorch C++ library or the C and C++ mixed library. So although the Python developer is working in Python, there's a point at which they can't easily dig any deeper because it's jumping into compiled code. And in the case of things like cuDNN, it's not even open source code. So what's the issue? Well, the issue is that for Python programmers, there's things that they either can't do at all or can't do conveniently. So because it ends up being turned into these really very big C libraries or pre-compiled libraries, edge deployment can be very difficult. For example, when you install PyTorch, you're actually installing over a gigabyte. It's an over a gigabyte download. And trying to turn your Python code into something that you can then put onto a mobile phone or a Raspberry Pi or whatever is incredibly challenging from a developer experience point of view, it's actually very difficult to debug your work because Python programmers are used to using the Python debugger, but most of the real works that's being done in your code is not happening in Python. It's happening in these lower level libraries. So trying to understand what's really going on is extremely challenging. Same problem for profiling. So obviously we all want our code to run fast. And that's challenging to do when you can't easily just use your Python profiler to jump in and see what's going on, where are the holdups, how do I make it faster? A lot of people think that it's not important when I speak to people, they say it's not important that Python programmers can kind of dig into the underlying kernels and understand them and debug them and customize them because Python programmers are happy working at these higher levels. But actually this is a big challenge because realistically, whether you're doing research or production in industry, at some point you want to dive in and change things. And in my experience, most of the time there's something I would like to try and change that's buried down inside one of these pre-compiled libraries. Also as an educator, it's very hard for me to teach people what's going on because I can't show them the actual code that's really running behind the scenes. And so for understanding the implementation details, whether it's for educational reason or because you want to understand how the algorithm works to think about how you can improve it, this is either impossible or extremely difficult. And this kind of hackability is critical for the developer experience in my opinion. So there's various hacks to try and handle these deficiencies. So for example, PyTorch now has a specialized profiler just for profiling PyTorch. NVIDIA has a specialized profiler as well. These are really neat tools and it's really cool that they're being provided for free. But the fact is that it's still not a great developer experience to have to learn a whole new tool which works in a different way. And that's not actually giving you a consistent view of all of your code. For edge deployment and or even sometimes a web hosting, there are hacks like in particular tracing and just-in-time compiler that are provided by both TensorFlow and PyTorch. So the idea is that you use the JOT or the tracing mechanism to basically turn your Python code into basically some code in a different form. In particular, it's likely to be O and an X which is kind of an open standard for sharing these kind of models. The problem is that Python is a really rich and dynamic language. And so in either of these cases, they're not capable of handling all of the things that Python can do. So for example, in the case of the PyTorch just-in-time compiler, there's all kinds of things where it's just going to give you an error and say, I'm sorry, I don't know how to do that. More frustrating for me, I find is that very often it does something slightly different to how Python works. And it's then very difficult to know why did it work in Python and it didn't work when I compiled it to O and an X. Another very interesting technology is XLA which comes out of Google and is now available as a backend for both TensorFlow and PyTorch. So this is an accelerated linear algebra compiler. It's a similar kind of idea to the PyTorch JIT, but it's something which is specifically designed around creating a really accelerated fast version of your code. And so nowadays it's used, for example, when PyTorch wants to talk to a TPU, it will go through the XLA compiler because that's the best way to create TPU code at this stage through XLA. So these are all nice to have, but they have a lot of shortcomings. It's not nearly as convenient and not nearly as good a developer experience as using just Python and using the Python tools that Python programmers are familiar with. Another very interesting new approach is Jax. Jax is another Google project and it's also a Python library, but it's actually specifically designed to bring Python over to XLA. So it's written from the ground up for XLA. And what's particularly interesting about Jax is that you can kind of write your own kernels. So you're not as limited as you are with tracing and JIT approaches. You're still limited to doing just the stuff that your underlying seed or CUDA or whatever library has written for you. Whereas with Jax you can do a lot more stuff. There's a lot more flexibility. And so this is very interesting approach, but we still have the problem that the code that's running on the accelerator is not the code you wrote. It's a transformation of that code through XLA. And so again, profiling it and debugging it and understanding really what's going on is difficult. Also in order to provide these composable transformations, Jax has a very interesting, but in some ways a very limited programming model. It's highly functional and immutable. And so Jax ends up with this kind of complexity from this functional programming model. State management becomes difficult. Things like random number generation becomes particularly challenging. And obviously in my world of machine learning and DIC learning, random numbers are very important as they are in many other GP GPU areas. So I feel like these are all like amazing technologies. So much impressive work going on, but it doesn't feel like the really long-term solutions. I don't see how any of these things quite end up giving us the developer experience we'd like to be able to offer. Another very interesting technology I wanted to mention is TVM. So TVM is an Apache project nowadays, and you can use TVM directly from Python and you basically end up creating these compute expressions, in this case using a Lambda. And if you're familiar with something like Halide, similar kind of idea, you can basically create a schedule, which will figure out how to, where you can show various ways that you think it might be best run on an accelerator. And in this case, you're actually binding axes to blocks and threads on the accelerator. This is a super convenient way to write kernels. And more importantly, perhaps it also has things like auto schedulers. So this is how you can create things that run as fast as Qt and N or specialized linear algebra libraries from Nvidia or whatever, without having to write all that unrolled loops and memory management and whatnot. But as you can see in the end, it's still not anywhere near as convenient as writing normal Python. And the thing you end up with is this kind of compiled code that again has all the kind of developer experience issues I described before. Perhaps the most interesting path for the future for me right now is Julia. Julia is a fairly new language. But what's really interesting from a GPGPU standpoint is it handles nearly all of the developer experience problems I described, but nearly none of them exist in Julia. And the key thing is that in Julia, you can write kernels that look a lot like you would write in CUDA, but with less boilerplate. And you can do in parallelized operations. You can handle memory. That can all be done in Julia. And so I think this is a really underappreciated, important idea, which is that developers should be able to use the same language and the same tools throughout the hierarchy of abstractions in their program. Again, speaking as an educator, this is incredibly important for teaching people what's going on. It's really important for a researcher because you can hack in at any level. It's really important in industry because you can ensure that you can jump in and make sure the performance is working properly for you at every level. And it also opens up the research world in such a way that things aren't off the table. I find that the things that get worked on in deep learning research are the things that are conveniently accessible through libraries. And a lot of stuff that isn't has just not really been touched because it requires people to go in and write their own CUDA kernels. And very, very, very few people have the patience to do that, at least in the deep learning world. So yeah, really, I guess this is a bit of a plea for the GPGPU community to consider building the next generation of languages and tools, which allows developers to really do everything that they might want to do in a convenient way. For Julia, I feel like there's a lot of gaps in the developer experience there more generally, which I think the community is very familiar with around deployment, and around the amount of memory use that it requires, and the amount of latency it requires to start up and so forth. But I do think at least with Julia, it feels like something that there's a path there that could eventually lead to a really beautiful developer experience. And that's not a path that I see available in really any of the Python frameworks that I see right now. And I would love to see things like TVM being taken, you know, more integrated with those ideas into languages and tools. So yeah, that's the end of my thoughts on that. Thanks very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.56, "text": " Okay, hi everybody. So I want to talk about my kind of personal opinions about the GPGPU", "tokens": [1033, 11, 4879, 2201, 13, 407, 286, 528, 281, 751, 466, 452, 733, 295, 2973, 11819, 466, 264, 26039, 38, 8115], "temperature": 0.0, "avg_logprob": -0.15872577179309932, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.006483882199972868}, {"id": 1, "seek": 0, "start": 8.56, "end": 13.48, "text": " developer experience. I feel like we don't talk about developer experience enough. When", "tokens": [10754, 1752, 13, 286, 841, 411, 321, 500, 380, 751, 466, 10754, 1752, 1547, 13, 1133], "temperature": 0.0, "avg_logprob": -0.15872577179309932, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.006483882199972868}, {"id": 2, "seek": 0, "start": 13.48, "end": 20.72, "text": " we talk about GPGPU, we tend to focus more on performance issues and distributed computing", "tokens": [321, 751, 466, 26039, 38, 8115, 11, 321, 3928, 281, 1879, 544, 322, 3389, 2663, 293, 12631, 15866], "temperature": 0.0, "avg_logprob": -0.15872577179309932, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.006483882199972868}, {"id": 3, "seek": 0, "start": 20.72, "end": 27.82, "text": " and stuff like that. I know a lot of the audience here is from an academic background. And so", "tokens": [293, 1507, 411, 300, 13, 286, 458, 257, 688, 295, 264, 4034, 510, 307, 490, 364, 7778, 3678, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.15872577179309932, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.006483882199972868}, {"id": 4, "seek": 2782, "start": 27.82, "end": 35.480000000000004, "text": " folks who focus on GPGPU in academia may not have fully realized how incredibly popular", "tokens": [4024, 567, 1879, 322, 26039, 38, 8115, 294, 28937, 815, 406, 362, 4498, 5334, 577, 6252, 3743], "temperature": 0.0, "avg_logprob": -0.15293297838808886, "compression_ratio": 1.4153005464480874, "no_speech_prob": 6.501378084067255e-05}, {"id": 5, "seek": 2782, "start": 35.480000000000004, "end": 42.76, "text": " GPGPU has become in the last few years. And to give you a sense, this is the downloads", "tokens": [26039, 38, 8115, 575, 1813, 294, 264, 1036, 1326, 924, 13, 400, 281, 976, 291, 257, 2020, 11, 341, 307, 264, 36553], "temperature": 0.0, "avg_logprob": -0.15293297838808886, "compression_ratio": 1.4153005464480874, "no_speech_prob": 6.501378084067255e-05}, {"id": 6, "seek": 2782, "start": 42.76, "end": 49.120000000000005, "text": " for CUDA toolkit on just one from just one source, which is from the Anaconda Python", "tokens": [337, 29777, 7509, 40167, 322, 445, 472, 490, 445, 472, 4009, 11, 597, 307, 490, 264, 1107, 326, 12233, 15329], "temperature": 0.0, "avg_logprob": -0.15293297838808886, "compression_ratio": 1.4153005464480874, "no_speech_prob": 6.501378084067255e-05}, {"id": 7, "seek": 4912, "start": 49.12, "end": 58.68, "text": " repository. And as we can see, 11.3 has 1.1 million downloads, 11.4, 1.1 million downloads,", "tokens": [25841, 13, 400, 382, 321, 393, 536, 11, 2975, 13, 18, 575, 502, 13, 16, 2459, 36553, 11, 2975, 13, 19, 11, 502, 13, 16, 2459, 36553, 11], "temperature": 0.0, "avg_logprob": -0.12495040893554688, "compression_ratio": 1.6606060606060606, "no_speech_prob": 5.063770731794648e-05}, {"id": 8, "seek": 4912, "start": 58.68, "end": 66.03999999999999, "text": " 11.1, 1 million downloads. We've got to a point now where literally over a million people", "tokens": [2975, 13, 16, 11, 502, 2459, 36553, 13, 492, 600, 658, 281, 257, 935, 586, 689, 3736, 670, 257, 2459, 561], "temperature": 0.0, "avg_logprob": -0.12495040893554688, "compression_ratio": 1.6606060606060606, "no_speech_prob": 5.063770731794648e-05}, {"id": 9, "seek": 4912, "start": 66.03999999999999, "end": 74.32, "text": " are downloading CUDA. So what are all these people doing? They are not writing CUDA kernels.", "tokens": [366, 32529, 29777, 7509, 13, 407, 437, 366, 439, 613, 561, 884, 30, 814, 366, 406, 3579, 29777, 7509, 23434, 1625, 13], "temperature": 0.0, "avg_logprob": -0.12495040893554688, "compression_ratio": 1.6606060606060606, "no_speech_prob": 5.063770731794648e-05}, {"id": 10, "seek": 7432, "start": 74.32, "end": 83.52, "text": " If you look at the Kaggle developer survey, actually most developers are now, data scientists", "tokens": [759, 291, 574, 412, 264, 48751, 22631, 10754, 8984, 11, 767, 881, 8849, 366, 586, 11, 1412, 7708], "temperature": 0.0, "avg_logprob": -0.1550226826821604, "compression_ratio": 1.4450261780104712, "no_speech_prob": 1.5935174815240316e-05}, {"id": 11, "seek": 7432, "start": 83.52, "end": 92.44, "text": " are now using things like TensorFlow and PyTorch and Lightning and Fast AI. And so GPGPU is", "tokens": [366, 586, 1228, 721, 411, 37624, 293, 9953, 51, 284, 339, 293, 28848, 293, 15968, 7318, 13, 400, 370, 26039, 38, 8115, 307], "temperature": 0.0, "avg_logprob": -0.1550226826821604, "compression_ratio": 1.4450261780104712, "no_speech_prob": 1.5935174815240316e-05}, {"id": 12, "seek": 7432, "start": 92.44, "end": 97.83999999999999, "text": " being used extremely extensively around the world now through these higher level libraries", "tokens": [885, 1143, 4664, 32636, 926, 264, 1002, 586, 807, 613, 2946, 1496, 15148], "temperature": 0.0, "avg_logprob": -0.1550226826821604, "compression_ratio": 1.4450261780104712, "no_speech_prob": 1.5935174815240316e-05}, {"id": 13, "seek": 9784, "start": 97.84, "end": 107.52000000000001, "text": " and nearly always via Python. But the thing is that these libraries like PyTorch behind", "tokens": [293, 6217, 1009, 5766, 15329, 13, 583, 264, 551, 307, 300, 613, 15148, 411, 9953, 51, 284, 339, 2261], "temperature": 0.0, "avg_logprob": -0.15992940983302156, "compression_ratio": 1.511111111111111, "no_speech_prob": 5.33801630808739e-06}, {"id": 14, "seek": 9784, "start": 107.52000000000001, "end": 115.60000000000001, "text": " the scenes, they're calling compiled C libraries such as for deep learning, cuDNN or the PyTorch", "tokens": [264, 8026, 11, 436, 434, 5141, 36548, 383, 15148, 1270, 382, 337, 2452, 2539, 11, 2702, 35, 45, 45, 420, 264, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.15992940983302156, "compression_ratio": 1.511111111111111, "no_speech_prob": 5.33801630808739e-06}, {"id": 15, "seek": 9784, "start": 115.60000000000001, "end": 124.64, "text": " C++ library or the C and C++ mixed library. So although the Python developer is working", "tokens": [383, 25472, 6405, 420, 264, 383, 293, 383, 25472, 7467, 6405, 13, 407, 4878, 264, 15329, 10754, 307, 1364], "temperature": 0.0, "avg_logprob": -0.15992940983302156, "compression_ratio": 1.511111111111111, "no_speech_prob": 5.33801630808739e-06}, {"id": 16, "seek": 12464, "start": 124.64, "end": 131.52, "text": " in Python, there's a point at which they can't easily dig any deeper because it's jumping", "tokens": [294, 15329, 11, 456, 311, 257, 935, 412, 597, 436, 393, 380, 3612, 2528, 604, 7731, 570, 309, 311, 11233], "temperature": 0.0, "avg_logprob": -0.0830102264881134, "compression_ratio": 1.6576576576576576, "no_speech_prob": 9.972652151191141e-06}, {"id": 17, "seek": 12464, "start": 131.52, "end": 139.32, "text": " into compiled code. And in the case of things like cuDNN, it's not even open source code.", "tokens": [666, 36548, 3089, 13, 400, 294, 264, 1389, 295, 721, 411, 2702, 35, 45, 45, 11, 309, 311, 406, 754, 1269, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.0830102264881134, "compression_ratio": 1.6576576576576576, "no_speech_prob": 9.972652151191141e-06}, {"id": 18, "seek": 12464, "start": 139.32, "end": 145.36, "text": " So what's the issue? Well, the issue is that for Python programmers, there's things that", "tokens": [407, 437, 311, 264, 2734, 30, 1042, 11, 264, 2734, 307, 300, 337, 15329, 41504, 11, 456, 311, 721, 300], "temperature": 0.0, "avg_logprob": -0.0830102264881134, "compression_ratio": 1.6576576576576576, "no_speech_prob": 9.972652151191141e-06}, {"id": 19, "seek": 12464, "start": 145.36, "end": 154.36, "text": " they either can't do at all or can't do conveniently. So because it ends up being turned into these", "tokens": [436, 2139, 393, 380, 360, 412, 439, 420, 393, 380, 360, 44375, 13, 407, 570, 309, 5314, 493, 885, 3574, 666, 613], "temperature": 0.0, "avg_logprob": -0.0830102264881134, "compression_ratio": 1.6576576576576576, "no_speech_prob": 9.972652151191141e-06}, {"id": 20, "seek": 15436, "start": 154.36, "end": 162.8, "text": " really very big C libraries or pre-compiled libraries, edge deployment can be very difficult.", "tokens": [534, 588, 955, 383, 15148, 420, 659, 12, 21541, 7292, 15148, 11, 4691, 19317, 393, 312, 588, 2252, 13], "temperature": 0.0, "avg_logprob": -0.13015242183909698, "compression_ratio": 1.5676855895196506, "no_speech_prob": 1.4738378013134934e-05}, {"id": 21, "seek": 15436, "start": 162.8, "end": 169.68, "text": " For example, when you install PyTorch, you're actually installing over a gigabyte. It's", "tokens": [1171, 1365, 11, 562, 291, 3625, 9953, 51, 284, 339, 11, 291, 434, 767, 20762, 670, 257, 8741, 34529, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.13015242183909698, "compression_ratio": 1.5676855895196506, "no_speech_prob": 1.4738378013134934e-05}, {"id": 22, "seek": 15436, "start": 169.68, "end": 178.8, "text": " an over a gigabyte download. And trying to turn your Python code into something that", "tokens": [364, 670, 257, 8741, 34529, 5484, 13, 400, 1382, 281, 1261, 428, 15329, 3089, 666, 746, 300], "temperature": 0.0, "avg_logprob": -0.13015242183909698, "compression_ratio": 1.5676855895196506, "no_speech_prob": 1.4738378013134934e-05}, {"id": 23, "seek": 15436, "start": 178.8, "end": 184.20000000000002, "text": " you can then put onto a mobile phone or a Raspberry Pi or whatever is incredibly challenging", "tokens": [291, 393, 550, 829, 3911, 257, 6013, 2593, 420, 257, 41154, 17741, 420, 2035, 307, 6252, 7595], "temperature": 0.0, "avg_logprob": -0.13015242183909698, "compression_ratio": 1.5676855895196506, "no_speech_prob": 1.4738378013134934e-05}, {"id": 24, "seek": 18420, "start": 184.2, "end": 192.6, "text": " from a developer experience point of view, it's actually very difficult to debug your", "tokens": [490, 257, 10754, 1752, 935, 295, 1910, 11, 309, 311, 767, 588, 2252, 281, 24083, 428], "temperature": 0.0, "avg_logprob": -0.1300662603133764, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.670044002821669e-05}, {"id": 25, "seek": 18420, "start": 192.6, "end": 198.51999999999998, "text": " work because Python programmers are used to using the Python debugger, but most of the", "tokens": [589, 570, 15329, 41504, 366, 1143, 281, 1228, 264, 15329, 24083, 1321, 11, 457, 881, 295, 264], "temperature": 0.0, "avg_logprob": -0.1300662603133764, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.670044002821669e-05}, {"id": 26, "seek": 18420, "start": 198.51999999999998, "end": 203.35999999999999, "text": " real works that's being done in your code is not happening in Python. It's happening", "tokens": [957, 1985, 300, 311, 885, 1096, 294, 428, 3089, 307, 406, 2737, 294, 15329, 13, 467, 311, 2737], "temperature": 0.0, "avg_logprob": -0.1300662603133764, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.670044002821669e-05}, {"id": 27, "seek": 18420, "start": 203.35999999999999, "end": 208.83999999999997, "text": " in these lower level libraries. So trying to understand what's really going on is extremely", "tokens": [294, 613, 3126, 1496, 15148, 13, 407, 1382, 281, 1223, 437, 311, 534, 516, 322, 307, 4664], "temperature": 0.0, "avg_logprob": -0.1300662603133764, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.670044002821669e-05}, {"id": 28, "seek": 20884, "start": 208.84, "end": 216.44, "text": " challenging. Same problem for profiling. So obviously we all want our code to run fast.", "tokens": [7595, 13, 10635, 1154, 337, 1740, 4883, 13, 407, 2745, 321, 439, 528, 527, 3089, 281, 1190, 2370, 13], "temperature": 0.0, "avg_logprob": -0.09972377399821858, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8631189959705807e-05}, {"id": 29, "seek": 20884, "start": 216.44, "end": 224.2, "text": " And that's challenging to do when you can't easily just use your Python profiler to jump", "tokens": [400, 300, 311, 7595, 281, 360, 562, 291, 393, 380, 3612, 445, 764, 428, 15329, 1740, 5441, 281, 3012], "temperature": 0.0, "avg_logprob": -0.09972377399821858, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8631189959705807e-05}, {"id": 30, "seek": 20884, "start": 224.2, "end": 230.4, "text": " in and see what's going on, where are the holdups, how do I make it faster? A lot of", "tokens": [294, 293, 536, 437, 311, 516, 322, 11, 689, 366, 264, 1797, 7528, 11, 577, 360, 286, 652, 309, 4663, 30, 316, 688, 295], "temperature": 0.0, "avg_logprob": -0.09972377399821858, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8631189959705807e-05}, {"id": 31, "seek": 20884, "start": 230.4, "end": 236.68, "text": " people think that it's not important when I speak to people, they say it's not important", "tokens": [561, 519, 300, 309, 311, 406, 1021, 562, 286, 1710, 281, 561, 11, 436, 584, 309, 311, 406, 1021], "temperature": 0.0, "avg_logprob": -0.09972377399821858, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8631189959705807e-05}, {"id": 32, "seek": 23668, "start": 236.68, "end": 243.44, "text": " that Python programmers can kind of dig into the underlying kernels and understand them", "tokens": [300, 15329, 41504, 393, 733, 295, 2528, 666, 264, 14217, 23434, 1625, 293, 1223, 552], "temperature": 0.0, "avg_logprob": -0.1571027570300632, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.406046405667439e-06}, {"id": 33, "seek": 23668, "start": 243.44, "end": 250.08, "text": " and debug them and customize them because Python programmers are happy working at these", "tokens": [293, 24083, 552, 293, 19734, 552, 570, 15329, 41504, 366, 2055, 1364, 412, 613], "temperature": 0.0, "avg_logprob": -0.1571027570300632, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.406046405667439e-06}, {"id": 34, "seek": 23668, "start": 250.08, "end": 256.88, "text": " higher levels. But actually this is a big challenge because realistically, whether you're", "tokens": [2946, 4358, 13, 583, 767, 341, 307, 257, 955, 3430, 570, 40734, 11, 1968, 291, 434], "temperature": 0.0, "avg_logprob": -0.1571027570300632, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.406046405667439e-06}, {"id": 35, "seek": 23668, "start": 256.88, "end": 265.28000000000003, "text": " doing research or production in industry, at some point you want to dive in and change", "tokens": [884, 2132, 420, 4265, 294, 3518, 11, 412, 512, 935, 291, 528, 281, 9192, 294, 293, 1319], "temperature": 0.0, "avg_logprob": -0.1571027570300632, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.406046405667439e-06}, {"id": 36, "seek": 26528, "start": 265.28, "end": 270.08, "text": " things. And in my experience, most of the time there's something I would like to try", "tokens": [721, 13, 400, 294, 452, 1752, 11, 881, 295, 264, 565, 456, 311, 746, 286, 576, 411, 281, 853], "temperature": 0.0, "avg_logprob": -0.08709129920372596, "compression_ratio": 1.7061068702290076, "no_speech_prob": 1.7230393495992757e-05}, {"id": 37, "seek": 26528, "start": 270.08, "end": 277.59999999999997, "text": " and change that's buried down inside one of these pre-compiled libraries. Also as an educator,", "tokens": [293, 1319, 300, 311, 14101, 760, 1854, 472, 295, 613, 659, 12, 21541, 7292, 15148, 13, 2743, 382, 364, 31237, 11], "temperature": 0.0, "avg_logprob": -0.08709129920372596, "compression_ratio": 1.7061068702290076, "no_speech_prob": 1.7230393495992757e-05}, {"id": 38, "seek": 26528, "start": 277.59999999999997, "end": 282.35999999999996, "text": " it's very hard for me to teach people what's going on because I can't show them the actual", "tokens": [309, 311, 588, 1152, 337, 385, 281, 2924, 561, 437, 311, 516, 322, 570, 286, 393, 380, 855, 552, 264, 3539], "temperature": 0.0, "avg_logprob": -0.08709129920372596, "compression_ratio": 1.7061068702290076, "no_speech_prob": 1.7230393495992757e-05}, {"id": 39, "seek": 26528, "start": 282.35999999999996, "end": 288.52, "text": " code that's really running behind the scenes. And so for understanding the implementation", "tokens": [3089, 300, 311, 534, 2614, 2261, 264, 8026, 13, 400, 370, 337, 3701, 264, 11420], "temperature": 0.0, "avg_logprob": -0.08709129920372596, "compression_ratio": 1.7061068702290076, "no_speech_prob": 1.7230393495992757e-05}, {"id": 40, "seek": 26528, "start": 288.52, "end": 294.21999999999997, "text": " details, whether it's for educational reason or because you want to understand how the", "tokens": [4365, 11, 1968, 309, 311, 337, 10189, 1778, 420, 570, 291, 528, 281, 1223, 577, 264], "temperature": 0.0, "avg_logprob": -0.08709129920372596, "compression_ratio": 1.7061068702290076, "no_speech_prob": 1.7230393495992757e-05}, {"id": 41, "seek": 29422, "start": 294.22, "end": 300.04, "text": " algorithm works to think about how you can improve it, this is either impossible or extremely", "tokens": [9284, 1985, 281, 519, 466, 577, 291, 393, 3470, 309, 11, 341, 307, 2139, 6243, 420, 4664], "temperature": 0.0, "avg_logprob": -0.11338834652955505, "compression_ratio": 1.5450643776824033, "no_speech_prob": 1.4284638382378034e-05}, {"id": 42, "seek": 29422, "start": 300.04, "end": 307.44000000000005, "text": " difficult. And this kind of hackability is critical for the developer experience in my", "tokens": [2252, 13, 400, 341, 733, 295, 10339, 2310, 307, 4924, 337, 264, 10754, 1752, 294, 452], "temperature": 0.0, "avg_logprob": -0.11338834652955505, "compression_ratio": 1.5450643776824033, "no_speech_prob": 1.4284638382378034e-05}, {"id": 43, "seek": 29422, "start": 307.44000000000005, "end": 315.96000000000004, "text": " opinion. So there's various hacks to try and handle these deficiencies. So for example,", "tokens": [4800, 13, 407, 456, 311, 3683, 33617, 281, 853, 293, 4813, 613, 19248, 31294, 13, 407, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.11338834652955505, "compression_ratio": 1.5450643776824033, "no_speech_prob": 1.4284638382378034e-05}, {"id": 44, "seek": 29422, "start": 315.96000000000004, "end": 323.68, "text": " PyTorch now has a specialized profiler just for profiling PyTorch. NVIDIA has a specialized", "tokens": [9953, 51, 284, 339, 586, 575, 257, 19813, 1740, 5441, 445, 337, 1740, 4883, 9953, 51, 284, 339, 13, 426, 3958, 6914, 575, 257, 19813], "temperature": 0.0, "avg_logprob": -0.11338834652955505, "compression_ratio": 1.5450643776824033, "no_speech_prob": 1.4284638382378034e-05}, {"id": 45, "seek": 32368, "start": 323.68, "end": 328.96, "text": " profiler as well. These are really neat tools and it's really cool that they're being provided", "tokens": [1740, 5441, 382, 731, 13, 1981, 366, 534, 10654, 3873, 293, 309, 311, 534, 1627, 300, 436, 434, 885, 5649], "temperature": 0.0, "avg_logprob": -0.11734072728590532, "compression_ratio": 1.5775862068965518, "no_speech_prob": 5.507485639100196e-06}, {"id": 46, "seek": 32368, "start": 328.96, "end": 334.84000000000003, "text": " for free. But the fact is that it's still not a great developer experience to have to", "tokens": [337, 1737, 13, 583, 264, 1186, 307, 300, 309, 311, 920, 406, 257, 869, 10754, 1752, 281, 362, 281], "temperature": 0.0, "avg_logprob": -0.11734072728590532, "compression_ratio": 1.5775862068965518, "no_speech_prob": 5.507485639100196e-06}, {"id": 47, "seek": 32368, "start": 334.84000000000003, "end": 341.56, "text": " learn a whole new tool which works in a different way. And that's not actually giving you a", "tokens": [1466, 257, 1379, 777, 2290, 597, 1985, 294, 257, 819, 636, 13, 400, 300, 311, 406, 767, 2902, 291, 257], "temperature": 0.0, "avg_logprob": -0.11734072728590532, "compression_ratio": 1.5775862068965518, "no_speech_prob": 5.507485639100196e-06}, {"id": 48, "seek": 32368, "start": 341.56, "end": 352.48, "text": " consistent view of all of your code. For edge deployment and or even sometimes a web hosting,", "tokens": [8398, 1910, 295, 439, 295, 428, 3089, 13, 1171, 4691, 19317, 293, 420, 754, 2171, 257, 3670, 16058, 11], "temperature": 0.0, "avg_logprob": -0.11734072728590532, "compression_ratio": 1.5775862068965518, "no_speech_prob": 5.507485639100196e-06}, {"id": 49, "seek": 35248, "start": 352.48, "end": 358.62, "text": " there are hacks like in particular tracing and just-in-time compiler that are provided", "tokens": [456, 366, 33617, 411, 294, 1729, 25262, 293, 445, 12, 259, 12, 3766, 31958, 300, 366, 5649], "temperature": 0.0, "avg_logprob": -0.15067315824104077, "compression_ratio": 1.5193370165745856, "no_speech_prob": 1.0288982593920082e-05}, {"id": 50, "seek": 35248, "start": 358.62, "end": 369.68, "text": " by both TensorFlow and PyTorch. So the idea is that you use the JOT or the tracing mechanism", "tokens": [538, 1293, 37624, 293, 9953, 51, 284, 339, 13, 407, 264, 1558, 307, 300, 291, 764, 264, 508, 5068, 420, 264, 25262, 7513], "temperature": 0.0, "avg_logprob": -0.15067315824104077, "compression_ratio": 1.5193370165745856, "no_speech_prob": 1.0288982593920082e-05}, {"id": 51, "seek": 35248, "start": 369.68, "end": 379.04, "text": " to basically turn your Python code into basically some code in a different form. In particular,", "tokens": [281, 1936, 1261, 428, 15329, 3089, 666, 1936, 512, 3089, 294, 257, 819, 1254, 13, 682, 1729, 11], "temperature": 0.0, "avg_logprob": -0.15067315824104077, "compression_ratio": 1.5193370165745856, "no_speech_prob": 1.0288982593920082e-05}, {"id": 52, "seek": 37904, "start": 379.04, "end": 386.48, "text": " it's likely to be O and an X which is kind of an open standard for sharing these kind", "tokens": [309, 311, 3700, 281, 312, 422, 293, 364, 1783, 597, 307, 733, 295, 364, 1269, 3832, 337, 5414, 613, 733], "temperature": 0.0, "avg_logprob": -0.13415468257406485, "compression_ratio": 1.573394495412844, "no_speech_prob": 4.565851668303367e-06}, {"id": 53, "seek": 37904, "start": 386.48, "end": 394.56, "text": " of models. The problem is that Python is a really rich and dynamic language. And so in", "tokens": [295, 5245, 13, 440, 1154, 307, 300, 15329, 307, 257, 534, 4593, 293, 8546, 2856, 13, 400, 370, 294], "temperature": 0.0, "avg_logprob": -0.13415468257406485, "compression_ratio": 1.573394495412844, "no_speech_prob": 4.565851668303367e-06}, {"id": 54, "seek": 37904, "start": 394.56, "end": 400.8, "text": " either of these cases, they're not capable of handling all of the things that Python", "tokens": [2139, 295, 613, 3331, 11, 436, 434, 406, 8189, 295, 13175, 439, 295, 264, 721, 300, 15329], "temperature": 0.0, "avg_logprob": -0.13415468257406485, "compression_ratio": 1.573394495412844, "no_speech_prob": 4.565851668303367e-06}, {"id": 55, "seek": 37904, "start": 400.8, "end": 406.8, "text": " can do. So for example, in the case of the PyTorch just-in-time compiler, there's all", "tokens": [393, 360, 13, 407, 337, 1365, 11, 294, 264, 1389, 295, 264, 9953, 51, 284, 339, 445, 12, 259, 12, 3766, 31958, 11, 456, 311, 439], "temperature": 0.0, "avg_logprob": -0.13415468257406485, "compression_ratio": 1.573394495412844, "no_speech_prob": 4.565851668303367e-06}, {"id": 56, "seek": 40680, "start": 406.8, "end": 409.76, "text": " kinds of things where it's just going to give you an error and say, I'm sorry, I don't know", "tokens": [3685, 295, 721, 689, 309, 311, 445, 516, 281, 976, 291, 364, 6713, 293, 584, 11, 286, 478, 2597, 11, 286, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.10518464871815272, "compression_ratio": 1.6383763837638377, "no_speech_prob": 1.3419326933217235e-05}, {"id": 57, "seek": 40680, "start": 409.76, "end": 415.40000000000003, "text": " how to do that. More frustrating for me, I find is that very often it does something", "tokens": [577, 281, 360, 300, 13, 5048, 16522, 337, 385, 11, 286, 915, 307, 300, 588, 2049, 309, 775, 746], "temperature": 0.0, "avg_logprob": -0.10518464871815272, "compression_ratio": 1.6383763837638377, "no_speech_prob": 1.3419326933217235e-05}, {"id": 58, "seek": 40680, "start": 415.40000000000003, "end": 420.8, "text": " slightly different to how Python works. And it's then very difficult to know why did it", "tokens": [4748, 819, 281, 577, 15329, 1985, 13, 400, 309, 311, 550, 588, 2252, 281, 458, 983, 630, 309], "temperature": 0.0, "avg_logprob": -0.10518464871815272, "compression_ratio": 1.6383763837638377, "no_speech_prob": 1.3419326933217235e-05}, {"id": 59, "seek": 40680, "start": 420.8, "end": 428.92, "text": " work in Python and it didn't work when I compiled it to O and an X. Another very interesting", "tokens": [589, 294, 15329, 293, 309, 994, 380, 589, 562, 286, 36548, 309, 281, 422, 293, 364, 1783, 13, 3996, 588, 1880], "temperature": 0.0, "avg_logprob": -0.10518464871815272, "compression_ratio": 1.6383763837638377, "no_speech_prob": 1.3419326933217235e-05}, {"id": 60, "seek": 40680, "start": 428.92, "end": 434.96000000000004, "text": " technology is XLA which comes out of Google and is now available as a backend for both", "tokens": [2899, 307, 1783, 11435, 597, 1487, 484, 295, 3329, 293, 307, 586, 2435, 382, 257, 38087, 337, 1293], "temperature": 0.0, "avg_logprob": -0.10518464871815272, "compression_ratio": 1.6383763837638377, "no_speech_prob": 1.3419326933217235e-05}, {"id": 61, "seek": 43496, "start": 434.96, "end": 440.58, "text": " TensorFlow and PyTorch. So this is an accelerated linear algebra compiler. It's a similar kind", "tokens": [37624, 293, 9953, 51, 284, 339, 13, 407, 341, 307, 364, 29763, 8213, 21989, 31958, 13, 467, 311, 257, 2531, 733], "temperature": 0.0, "avg_logprob": -0.09324410087183903, "compression_ratio": 1.5358649789029535, "no_speech_prob": 2.1443112927954644e-05}, {"id": 62, "seek": 43496, "start": 440.58, "end": 450.0, "text": " of idea to the PyTorch JIT, but it's something which is specifically designed around creating", "tokens": [295, 1558, 281, 264, 9953, 51, 284, 339, 508, 3927, 11, 457, 309, 311, 746, 597, 307, 4682, 4761, 926, 4084], "temperature": 0.0, "avg_logprob": -0.09324410087183903, "compression_ratio": 1.5358649789029535, "no_speech_prob": 2.1443112927954644e-05}, {"id": 63, "seek": 43496, "start": 450.0, "end": 457.29999999999995, "text": " a really accelerated fast version of your code. And so nowadays it's used, for example,", "tokens": [257, 534, 29763, 2370, 3037, 295, 428, 3089, 13, 400, 370, 13434, 309, 311, 1143, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.09324410087183903, "compression_ratio": 1.5358649789029535, "no_speech_prob": 2.1443112927954644e-05}, {"id": 64, "seek": 43496, "start": 457.29999999999995, "end": 464.4, "text": " when PyTorch wants to talk to a TPU, it will go through the XLA compiler because that's", "tokens": [562, 9953, 51, 284, 339, 2738, 281, 751, 281, 257, 314, 8115, 11, 309, 486, 352, 807, 264, 1783, 11435, 31958, 570, 300, 311], "temperature": 0.0, "avg_logprob": -0.09324410087183903, "compression_ratio": 1.5358649789029535, "no_speech_prob": 2.1443112927954644e-05}, {"id": 65, "seek": 46440, "start": 464.4, "end": 473.08, "text": " the best way to create TPU code at this stage through XLA. So these are all nice to have,", "tokens": [264, 1151, 636, 281, 1884, 314, 8115, 3089, 412, 341, 3233, 807, 1783, 11435, 13, 407, 613, 366, 439, 1481, 281, 362, 11], "temperature": 0.0, "avg_logprob": -0.10103306403526893, "compression_ratio": 1.4802259887005649, "no_speech_prob": 2.4824216779961716e-06}, {"id": 66, "seek": 46440, "start": 473.08, "end": 480.84, "text": " but they have a lot of shortcomings. It's not nearly as convenient and not nearly as", "tokens": [457, 436, 362, 257, 688, 295, 2099, 49886, 13, 467, 311, 406, 6217, 382, 10851, 293, 406, 6217, 382], "temperature": 0.0, "avg_logprob": -0.10103306403526893, "compression_ratio": 1.4802259887005649, "no_speech_prob": 2.4824216779961716e-06}, {"id": 67, "seek": 46440, "start": 480.84, "end": 486.91999999999996, "text": " good a developer experience as using just Python and using the Python tools that Python", "tokens": [665, 257, 10754, 1752, 382, 1228, 445, 15329, 293, 1228, 264, 15329, 3873, 300, 15329], "temperature": 0.0, "avg_logprob": -0.10103306403526893, "compression_ratio": 1.4802259887005649, "no_speech_prob": 2.4824216779961716e-06}, {"id": 68, "seek": 48692, "start": 486.92, "end": 495.92, "text": " programmers are familiar with. Another very interesting new approach is Jax. Jax is another", "tokens": [41504, 366, 4963, 365, 13, 3996, 588, 1880, 777, 3109, 307, 508, 2797, 13, 508, 2797, 307, 1071], "temperature": 0.0, "avg_logprob": -0.09730446699893835, "compression_ratio": 1.4550264550264551, "no_speech_prob": 1.1300217011012137e-05}, {"id": 69, "seek": 48692, "start": 495.92, "end": 507.40000000000003, "text": " Google project and it's also a Python library, but it's actually specifically designed to", "tokens": [3329, 1716, 293, 309, 311, 611, 257, 15329, 6405, 11, 457, 309, 311, 767, 4682, 4761, 281], "temperature": 0.0, "avg_logprob": -0.09730446699893835, "compression_ratio": 1.4550264550264551, "no_speech_prob": 1.1300217011012137e-05}, {"id": 70, "seek": 48692, "start": 507.40000000000003, "end": 513.32, "text": " bring Python over to XLA. So it's written from the ground up for XLA. And what's particularly", "tokens": [1565, 15329, 670, 281, 1783, 11435, 13, 407, 309, 311, 3720, 490, 264, 2727, 493, 337, 1783, 11435, 13, 400, 437, 311, 4098], "temperature": 0.0, "avg_logprob": -0.09730446699893835, "compression_ratio": 1.4550264550264551, "no_speech_prob": 1.1300217011012137e-05}, {"id": 71, "seek": 51332, "start": 513.32, "end": 521.48, "text": " interesting about Jax is that you can kind of write your own kernels. So you're not as", "tokens": [1880, 466, 508, 2797, 307, 300, 291, 393, 733, 295, 2464, 428, 1065, 23434, 1625, 13, 407, 291, 434, 406, 382], "temperature": 0.0, "avg_logprob": -0.17123608155684036, "compression_ratio": 1.456989247311828, "no_speech_prob": 3.7852548757655313e-06}, {"id": 72, "seek": 51332, "start": 521.48, "end": 530.72, "text": " limited as you are with tracing and JIT approaches. You're still limited to doing just the stuff", "tokens": [5567, 382, 291, 366, 365, 25262, 293, 508, 3927, 11587, 13, 509, 434, 920, 5567, 281, 884, 445, 264, 1507], "temperature": 0.0, "avg_logprob": -0.17123608155684036, "compression_ratio": 1.456989247311828, "no_speech_prob": 3.7852548757655313e-06}, {"id": 73, "seek": 51332, "start": 530.72, "end": 537.5200000000001, "text": " that your underlying seed or CUDA or whatever library has written for you. Whereas with", "tokens": [300, 428, 14217, 8871, 420, 29777, 7509, 420, 2035, 6405, 575, 3720, 337, 291, 13, 13813, 365], "temperature": 0.0, "avg_logprob": -0.17123608155684036, "compression_ratio": 1.456989247311828, "no_speech_prob": 3.7852548757655313e-06}, {"id": 74, "seek": 53752, "start": 537.52, "end": 544.0799999999999, "text": " Jax you can do a lot more stuff. There's a lot more flexibility. And so this is very", "tokens": [508, 2797, 291, 393, 360, 257, 688, 544, 1507, 13, 821, 311, 257, 688, 544, 12635, 13, 400, 370, 341, 307, 588], "temperature": 0.0, "avg_logprob": -0.12129609529362169, "compression_ratio": 1.6367924528301887, "no_speech_prob": 2.295883177794167e-06}, {"id": 75, "seek": 53752, "start": 544.0799999999999, "end": 551.4, "text": " interesting approach, but we still have the problem that the code that's running on the", "tokens": [1880, 3109, 11, 457, 321, 920, 362, 264, 1154, 300, 264, 3089, 300, 311, 2614, 322, 264], "temperature": 0.0, "avg_logprob": -0.12129609529362169, "compression_ratio": 1.6367924528301887, "no_speech_prob": 2.295883177794167e-06}, {"id": 76, "seek": 53752, "start": 551.4, "end": 558.0, "text": " accelerator is not the code you wrote. It's a transformation of that code through XLA.", "tokens": [39889, 307, 406, 264, 3089, 291, 4114, 13, 467, 311, 257, 9887, 295, 300, 3089, 807, 1783, 11435, 13], "temperature": 0.0, "avg_logprob": -0.12129609529362169, "compression_ratio": 1.6367924528301887, "no_speech_prob": 2.295883177794167e-06}, {"id": 77, "seek": 53752, "start": 558.0, "end": 562.28, "text": " And so again, profiling it and debugging it and understanding really what's going on is", "tokens": [400, 370, 797, 11, 1740, 4883, 309, 293, 45592, 309, 293, 3701, 534, 437, 311, 516, 322, 307], "temperature": 0.0, "avg_logprob": -0.12129609529362169, "compression_ratio": 1.6367924528301887, "no_speech_prob": 2.295883177794167e-06}, {"id": 78, "seek": 56228, "start": 562.28, "end": 569.8, "text": " difficult. Also in order to provide these composable transformations, Jax has a very", "tokens": [2252, 13, 2743, 294, 1668, 281, 2893, 613, 10199, 712, 34852, 11, 508, 2797, 575, 257, 588], "temperature": 0.0, "avg_logprob": -0.16964845021565755, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.450883883080678e-05}, {"id": 79, "seek": 56228, "start": 569.8, "end": 575.52, "text": " interesting, but in some ways a very limited programming model. It's highly functional", "tokens": [1880, 11, 457, 294, 512, 2098, 257, 588, 5567, 9410, 2316, 13, 467, 311, 5405, 11745], "temperature": 0.0, "avg_logprob": -0.16964845021565755, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.450883883080678e-05}, {"id": 80, "seek": 56228, "start": 575.52, "end": 582.8399999999999, "text": " and immutable. And so Jax ends up with this kind of complexity from this functional programming", "tokens": [293, 3397, 32148, 13, 400, 370, 508, 2797, 5314, 493, 365, 341, 733, 295, 14024, 490, 341, 11745, 9410], "temperature": 0.0, "avg_logprob": -0.16964845021565755, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.450883883080678e-05}, {"id": 81, "seek": 56228, "start": 582.8399999999999, "end": 588.24, "text": " model. State management becomes difficult. Things like random number generation becomes", "tokens": [2316, 13, 4533, 4592, 3643, 2252, 13, 9514, 411, 4974, 1230, 5125, 3643], "temperature": 0.0, "avg_logprob": -0.16964845021565755, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.450883883080678e-05}, {"id": 82, "seek": 58824, "start": 588.24, "end": 593.72, "text": " particularly challenging. And obviously in my world of machine learning and DIC learning,", "tokens": [4098, 7595, 13, 400, 2745, 294, 452, 1002, 295, 3479, 2539, 293, 413, 2532, 2539, 11], "temperature": 0.0, "avg_logprob": -0.12280386250193526, "compression_ratio": 1.550660792951542, "no_speech_prob": 6.339001629385166e-06}, {"id": 83, "seek": 58824, "start": 593.72, "end": 600.12, "text": " random numbers are very important as they are in many other GP GPU areas. So I feel", "tokens": [4974, 3547, 366, 588, 1021, 382, 436, 366, 294, 867, 661, 26039, 18407, 3179, 13, 407, 286, 841], "temperature": 0.0, "avg_logprob": -0.12280386250193526, "compression_ratio": 1.550660792951542, "no_speech_prob": 6.339001629385166e-06}, {"id": 84, "seek": 58824, "start": 600.12, "end": 608.08, "text": " like these are all like amazing technologies. So much impressive work going on, but it doesn't", "tokens": [411, 613, 366, 439, 411, 2243, 7943, 13, 407, 709, 8992, 589, 516, 322, 11, 457, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.12280386250193526, "compression_ratio": 1.550660792951542, "no_speech_prob": 6.339001629385166e-06}, {"id": 85, "seek": 58824, "start": 608.08, "end": 614.6800000000001, "text": " feel like the really long-term solutions. I don't see how any of these things quite", "tokens": [841, 411, 264, 534, 938, 12, 7039, 6547, 13, 286, 500, 380, 536, 577, 604, 295, 613, 721, 1596], "temperature": 0.0, "avg_logprob": -0.12280386250193526, "compression_ratio": 1.550660792951542, "no_speech_prob": 6.339001629385166e-06}, {"id": 86, "seek": 61468, "start": 614.68, "end": 621.5999999999999, "text": " end up giving us the developer experience we'd like to be able to offer. Another very", "tokens": [917, 493, 2902, 505, 264, 10754, 1752, 321, 1116, 411, 281, 312, 1075, 281, 2626, 13, 3996, 588], "temperature": 0.0, "avg_logprob": -0.13336618447009427, "compression_ratio": 1.4935622317596566, "no_speech_prob": 1.6441406842204742e-05}, {"id": 87, "seek": 61468, "start": 621.5999999999999, "end": 627.76, "text": " interesting technology I wanted to mention is TVM. So TVM is an Apache project nowadays,", "tokens": [1880, 2899, 286, 1415, 281, 2152, 307, 3558, 44, 13, 407, 3558, 44, 307, 364, 46597, 1716, 13434, 11], "temperature": 0.0, "avg_logprob": -0.13336618447009427, "compression_ratio": 1.4935622317596566, "no_speech_prob": 1.6441406842204742e-05}, {"id": 88, "seek": 61468, "start": 627.76, "end": 634.5999999999999, "text": " and you can use TVM directly from Python and you basically end up creating these compute", "tokens": [293, 291, 393, 764, 3558, 44, 3838, 490, 15329, 293, 291, 1936, 917, 493, 4084, 613, 14722], "temperature": 0.0, "avg_logprob": -0.13336618447009427, "compression_ratio": 1.4935622317596566, "no_speech_prob": 1.6441406842204742e-05}, {"id": 89, "seek": 61468, "start": 634.5999999999999, "end": 641.28, "text": " expressions, in this case using a Lambda. And if you're familiar with something like", "tokens": [15277, 11, 294, 341, 1389, 1228, 257, 45691, 13, 400, 498, 291, 434, 4963, 365, 746, 411], "temperature": 0.0, "avg_logprob": -0.13336618447009427, "compression_ratio": 1.4935622317596566, "no_speech_prob": 1.6441406842204742e-05}, {"id": 90, "seek": 64128, "start": 641.28, "end": 648.36, "text": " Halide, similar kind of idea, you can basically create a schedule, which will figure out how", "tokens": [13896, 482, 11, 2531, 733, 295, 1558, 11, 291, 393, 1936, 1884, 257, 7567, 11, 597, 486, 2573, 484, 577], "temperature": 0.0, "avg_logprob": -0.11538007889670887, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.473826341680251e-05}, {"id": 91, "seek": 64128, "start": 648.36, "end": 656.24, "text": " to, where you can show various ways that you think it might be best run on an accelerator.", "tokens": [281, 11, 689, 291, 393, 855, 3683, 2098, 300, 291, 519, 309, 1062, 312, 1151, 1190, 322, 364, 39889, 13], "temperature": 0.0, "avg_logprob": -0.11538007889670887, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.473826341680251e-05}, {"id": 92, "seek": 64128, "start": 656.24, "end": 663.24, "text": " And in this case, you're actually binding axes to blocks and threads on the accelerator.", "tokens": [400, 294, 341, 1389, 11, 291, 434, 767, 17359, 35387, 281, 8474, 293, 19314, 322, 264, 39889, 13], "temperature": 0.0, "avg_logprob": -0.11538007889670887, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.473826341680251e-05}, {"id": 93, "seek": 64128, "start": 663.24, "end": 669.56, "text": " This is a super convenient way to write kernels. And more importantly, perhaps it also has", "tokens": [639, 307, 257, 1687, 10851, 636, 281, 2464, 23434, 1625, 13, 400, 544, 8906, 11, 4317, 309, 611, 575], "temperature": 0.0, "avg_logprob": -0.11538007889670887, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.473826341680251e-05}, {"id": 94, "seek": 66956, "start": 669.56, "end": 676.3199999999999, "text": " things like auto schedulers. So this is how you can create things that run as fast as", "tokens": [721, 411, 8399, 12000, 433, 13, 407, 341, 307, 577, 291, 393, 1884, 721, 300, 1190, 382, 2370, 382], "temperature": 0.0, "avg_logprob": -0.1199837979816255, "compression_ratio": 1.565217391304348, "no_speech_prob": 9.818056241783779e-06}, {"id": 95, "seek": 66956, "start": 676.3199999999999, "end": 683.68, "text": " Qt and N or specialized linear algebra libraries from Nvidia or whatever, without having to", "tokens": [1249, 83, 293, 426, 420, 19813, 8213, 21989, 15148, 490, 46284, 420, 2035, 11, 1553, 1419, 281], "temperature": 0.0, "avg_logprob": -0.1199837979816255, "compression_ratio": 1.565217391304348, "no_speech_prob": 9.818056241783779e-06}, {"id": 96, "seek": 66956, "start": 683.68, "end": 692.3199999999999, "text": " write all that unrolled loops and memory management and whatnot. But as you can see in the end,", "tokens": [2464, 439, 300, 517, 28850, 16121, 293, 4675, 4592, 293, 25882, 13, 583, 382, 291, 393, 536, 294, 264, 917, 11], "temperature": 0.0, "avg_logprob": -0.1199837979816255, "compression_ratio": 1.565217391304348, "no_speech_prob": 9.818056241783779e-06}, {"id": 97, "seek": 66956, "start": 692.3199999999999, "end": 697.28, "text": " it's still not anywhere near as convenient as writing normal Python. And the thing you", "tokens": [309, 311, 920, 406, 4992, 2651, 382, 10851, 382, 3579, 2710, 15329, 13, 400, 264, 551, 291], "temperature": 0.0, "avg_logprob": -0.1199837979816255, "compression_ratio": 1.565217391304348, "no_speech_prob": 9.818056241783779e-06}, {"id": 98, "seek": 69728, "start": 697.28, "end": 702.56, "text": " end up with is this kind of compiled code that again has all the kind of developer experience", "tokens": [917, 493, 365, 307, 341, 733, 295, 36548, 3089, 300, 797, 575, 439, 264, 733, 295, 10754, 1752], "temperature": 0.0, "avg_logprob": -0.16162055051779445, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.3419453352980781e-05}, {"id": 99, "seek": 69728, "start": 702.56, "end": 711.3199999999999, "text": " issues I described before. Perhaps the most interesting path for the future for me right", "tokens": [2663, 286, 7619, 949, 13, 10517, 264, 881, 1880, 3100, 337, 264, 2027, 337, 385, 558], "temperature": 0.0, "avg_logprob": -0.16162055051779445, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.3419453352980781e-05}, {"id": 100, "seek": 69728, "start": 711.3199999999999, "end": 721.16, "text": " now is Julia. Julia is a fairly new language. But what's really interesting from a GPGPU", "tokens": [586, 307, 18551, 13, 18551, 307, 257, 6457, 777, 2856, 13, 583, 437, 311, 534, 1880, 490, 257, 26039, 38, 8115], "temperature": 0.0, "avg_logprob": -0.16162055051779445, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.3419453352980781e-05}, {"id": 101, "seek": 69728, "start": 721.16, "end": 726.8399999999999, "text": " standpoint is it handles nearly all of the developer experience problems I described,", "tokens": [15827, 307, 309, 18722, 6217, 439, 295, 264, 10754, 1752, 2740, 286, 7619, 11], "temperature": 0.0, "avg_logprob": -0.16162055051779445, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.3419453352980781e-05}, {"id": 102, "seek": 72684, "start": 726.84, "end": 732.76, "text": " but nearly none of them exist in Julia. And the key thing is that in Julia, you can write", "tokens": [457, 6217, 6022, 295, 552, 2514, 294, 18551, 13, 400, 264, 2141, 551, 307, 300, 294, 18551, 11, 291, 393, 2464], "temperature": 0.0, "avg_logprob": -0.12600631445226534, "compression_ratio": 1.5257142857142858, "no_speech_prob": 4.71085013487027e-06}, {"id": 103, "seek": 72684, "start": 732.76, "end": 741.4, "text": " kernels that look a lot like you would write in CUDA, but with less boilerplate. And you", "tokens": [23434, 1625, 300, 574, 257, 688, 411, 291, 576, 2464, 294, 29777, 7509, 11, 457, 365, 1570, 39228, 37008, 13, 400, 291], "temperature": 0.0, "avg_logprob": -0.12600631445226534, "compression_ratio": 1.5257142857142858, "no_speech_prob": 4.71085013487027e-06}, {"id": 104, "seek": 72684, "start": 741.4, "end": 751.44, "text": " can do in parallelized operations. You can handle memory. That can all be done in Julia.", "tokens": [393, 360, 294, 8952, 1602, 7705, 13, 509, 393, 4813, 4675, 13, 663, 393, 439, 312, 1096, 294, 18551, 13], "temperature": 0.0, "avg_logprob": -0.12600631445226534, "compression_ratio": 1.5257142857142858, "no_speech_prob": 4.71085013487027e-06}, {"id": 105, "seek": 75144, "start": 751.44, "end": 762.2800000000001, "text": " And so I think this is a really underappreciated, important idea, which is that developers should", "tokens": [400, 370, 286, 519, 341, 307, 257, 534, 833, 1746, 3326, 770, 11, 1021, 1558, 11, 597, 307, 300, 8849, 820], "temperature": 0.0, "avg_logprob": -0.07566378230140322, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.3006274457438849e-05}, {"id": 106, "seek": 75144, "start": 762.2800000000001, "end": 766.9200000000001, "text": " be able to use the same language and the same tools throughout the hierarchy of abstractions", "tokens": [312, 1075, 281, 764, 264, 912, 2856, 293, 264, 912, 3873, 3710, 264, 22333, 295, 12649, 626], "temperature": 0.0, "avg_logprob": -0.07566378230140322, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.3006274457438849e-05}, {"id": 107, "seek": 75144, "start": 766.9200000000001, "end": 774.12, "text": " in their program. Again, speaking as an educator, this is incredibly important for teaching", "tokens": [294, 641, 1461, 13, 3764, 11, 4124, 382, 364, 31237, 11, 341, 307, 6252, 1021, 337, 4571], "temperature": 0.0, "avg_logprob": -0.07566378230140322, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.3006274457438849e-05}, {"id": 108, "seek": 75144, "start": 774.12, "end": 780.0400000000001, "text": " people what's going on. It's really important for a researcher because you can hack in at", "tokens": [561, 437, 311, 516, 322, 13, 467, 311, 534, 1021, 337, 257, 21751, 570, 291, 393, 10339, 294, 412], "temperature": 0.0, "avg_logprob": -0.07566378230140322, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.3006274457438849e-05}, {"id": 109, "seek": 78004, "start": 780.04, "end": 785.88, "text": " any level. It's really important in industry because you can ensure that you can jump in", "tokens": [604, 1496, 13, 467, 311, 534, 1021, 294, 3518, 570, 291, 393, 5586, 300, 291, 393, 3012, 294], "temperature": 0.0, "avg_logprob": -0.0957014386246844, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.6440988474641927e-05}, {"id": 110, "seek": 78004, "start": 785.88, "end": 794.28, "text": " and make sure the performance is working properly for you at every level. And it also opens", "tokens": [293, 652, 988, 264, 3389, 307, 1364, 6108, 337, 291, 412, 633, 1496, 13, 400, 309, 611, 9870], "temperature": 0.0, "avg_logprob": -0.0957014386246844, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.6440988474641927e-05}, {"id": 111, "seek": 78004, "start": 794.28, "end": 802.24, "text": " up the research world in such a way that things aren't off the table. I find that the things", "tokens": [493, 264, 2132, 1002, 294, 1270, 257, 636, 300, 721, 3212, 380, 766, 264, 3199, 13, 286, 915, 300, 264, 721], "temperature": 0.0, "avg_logprob": -0.0957014386246844, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.6440988474641927e-05}, {"id": 112, "seek": 78004, "start": 802.24, "end": 806.8399999999999, "text": " that get worked on in deep learning research are the things that are conveniently accessible", "tokens": [300, 483, 2732, 322, 294, 2452, 2539, 2132, 366, 264, 721, 300, 366, 44375, 9515], "temperature": 0.0, "avg_logprob": -0.0957014386246844, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.6440988474641927e-05}, {"id": 113, "seek": 80684, "start": 806.84, "end": 815.1600000000001, "text": " through libraries. And a lot of stuff that isn't has just not really been touched because", "tokens": [807, 15148, 13, 400, 257, 688, 295, 1507, 300, 1943, 380, 575, 445, 406, 534, 668, 9828, 570], "temperature": 0.0, "avg_logprob": -0.11588375119195468, "compression_ratio": 1.467032967032967, "no_speech_prob": 6.240606126084458e-06}, {"id": 114, "seek": 80684, "start": 815.1600000000001, "end": 819.48, "text": " it requires people to go in and write their own CUDA kernels. And very, very, very few", "tokens": [309, 7029, 561, 281, 352, 294, 293, 2464, 641, 1065, 29777, 7509, 23434, 1625, 13, 400, 588, 11, 588, 11, 588, 1326], "temperature": 0.0, "avg_logprob": -0.11588375119195468, "compression_ratio": 1.467032967032967, "no_speech_prob": 6.240606126084458e-06}, {"id": 115, "seek": 80684, "start": 819.48, "end": 827.84, "text": " people have the patience to do that, at least in the deep learning world. So yeah, really,", "tokens": [561, 362, 264, 14826, 281, 360, 300, 11, 412, 1935, 294, 264, 2452, 2539, 1002, 13, 407, 1338, 11, 534, 11], "temperature": 0.0, "avg_logprob": -0.11588375119195468, "compression_ratio": 1.467032967032967, "no_speech_prob": 6.240606126084458e-06}, {"id": 116, "seek": 82784, "start": 827.84, "end": 841.4, "text": " I guess this is a bit of a plea for the GPGPU community to consider building the next generation", "tokens": [286, 2041, 341, 307, 257, 857, 295, 257, 42152, 337, 264, 26039, 38, 8115, 1768, 281, 1949, 2390, 264, 958, 5125], "temperature": 0.0, "avg_logprob": -0.10058085862980333, "compression_ratio": 1.3309859154929577, "no_speech_prob": 7.410360467474675e-06}, {"id": 117, "seek": 82784, "start": 841.4, "end": 851.6, "text": " of languages and tools, which allows developers to really do everything that they might want", "tokens": [295, 8650, 293, 3873, 11, 597, 4045, 8849, 281, 534, 360, 1203, 300, 436, 1062, 528], "temperature": 0.0, "avg_logprob": -0.10058085862980333, "compression_ratio": 1.3309859154929577, "no_speech_prob": 7.410360467474675e-06}, {"id": 118, "seek": 85160, "start": 851.6, "end": 860.0400000000001, "text": " to do in a convenient way. For Julia, I feel like there's a lot of gaps in the developer", "tokens": [281, 360, 294, 257, 10851, 636, 13, 1171, 18551, 11, 286, 841, 411, 456, 311, 257, 688, 295, 15031, 294, 264, 10754], "temperature": 0.0, "avg_logprob": -0.12080060604006745, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.372870560269803e-05}, {"id": 119, "seek": 85160, "start": 860.0400000000001, "end": 863.6800000000001, "text": " experience there more generally, which I think the community is very familiar with around", "tokens": [1752, 456, 544, 5101, 11, 597, 286, 519, 264, 1768, 307, 588, 4963, 365, 926], "temperature": 0.0, "avg_logprob": -0.12080060604006745, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.372870560269803e-05}, {"id": 120, "seek": 85160, "start": 863.6800000000001, "end": 868.88, "text": " deployment, and around the amount of memory use that it requires, and the amount of latency", "tokens": [19317, 11, 293, 926, 264, 2372, 295, 4675, 764, 300, 309, 7029, 11, 293, 264, 2372, 295, 27043], "temperature": 0.0, "avg_logprob": -0.12080060604006745, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.372870560269803e-05}, {"id": 121, "seek": 85160, "start": 868.88, "end": 874.36, "text": " it requires to start up and so forth. But I do think at least with Julia, it feels like", "tokens": [309, 7029, 281, 722, 493, 293, 370, 5220, 13, 583, 286, 360, 519, 412, 1935, 365, 18551, 11, 309, 3417, 411], "temperature": 0.0, "avg_logprob": -0.12080060604006745, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.372870560269803e-05}, {"id": 122, "seek": 87436, "start": 874.36, "end": 881.6, "text": " something that there's a path there that could eventually lead to a really beautiful developer", "tokens": [746, 300, 456, 311, 257, 3100, 456, 300, 727, 4728, 1477, 281, 257, 534, 2238, 10754], "temperature": 0.0, "avg_logprob": -0.12385368347167969, "compression_ratio": 1.5991379310344827, "no_speech_prob": 1.593446904735174e-05}, {"id": 123, "seek": 87436, "start": 881.6, "end": 887.96, "text": " experience. And that's not a path that I see available in really any of the Python frameworks", "tokens": [1752, 13, 400, 300, 311, 406, 257, 3100, 300, 286, 536, 2435, 294, 534, 604, 295, 264, 15329, 29834], "temperature": 0.0, "avg_logprob": -0.12385368347167969, "compression_ratio": 1.5991379310344827, "no_speech_prob": 1.593446904735174e-05}, {"id": 124, "seek": 87436, "start": 887.96, "end": 896.4, "text": " that I see right now. And I would love to see things like TVM being taken, you know,", "tokens": [300, 286, 536, 558, 586, 13, 400, 286, 576, 959, 281, 536, 721, 411, 3558, 44, 885, 2726, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.12385368347167969, "compression_ratio": 1.5991379310344827, "no_speech_prob": 1.593446904735174e-05}, {"id": 125, "seek": 87436, "start": 896.4, "end": 903.8000000000001, "text": " more integrated with those ideas into languages and tools. So yeah, that's the end of my thoughts", "tokens": [544, 10919, 365, 729, 3487, 666, 8650, 293, 3873, 13, 407, 1338, 11, 300, 311, 264, 917, 295, 452, 4598], "temperature": 0.0, "avg_logprob": -0.12385368347167969, "compression_ratio": 1.5991379310344827, "no_speech_prob": 1.593446904735174e-05}, {"id": 126, "seek": 90380, "start": 903.8, "end": 905.0, "text": " on that. Thanks very much.", "tokens": [50364, 322, 300, 13, 2561, 588, 709, 13, 50424], "temperature": 0.0, "avg_logprob": -0.17458000183105468, "compression_ratio": 0.7647058823529411, "no_speech_prob": 0.00013444559590425342}], "language": "en"}