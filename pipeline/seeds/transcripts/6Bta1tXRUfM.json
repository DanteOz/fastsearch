{"text": " All right, hi gang, and here we are in lesson 21, joined by the legends themselves, Jono and Tanishq. Hello. Hello. And today you'll be shocked to hear that we are going to look at a Jupyter notebook. Amazing right? We're going to look at notebook 22. This is a pretty quick, just, you know, improvement, pretty simple improvement to our ddpm slash ddim implementation for fashion MNIST. And this is all the same so far, but what I've done is I've made some, one quite significant change and some of the changes we'll be making today are all about making life simpler. And they're kind of reflecting the way the papers have been taking things. And it's interesting to see how the papers have not only made things better, they made things simpler. And so one of the things that I've noticed in recent papers is that there's no longer a concept of end steps, which is something we've always had before and always bothered me a bit, this capital T thing. You know, this T over T, it's basically saying this is time step number, say 500 out of 1000, so it's time step 0.5. Why not just call it 0.5? And the answer is, well, we can. So we talked last time about the cosine scheduler. We didn't end up using it because I came up with an idea which was, you know, simpler and nearly the same, which is just to change our betamax. But in this next, what can I say, let's use the cosine scheduler, but let's try to get rid of the end steps thing and the capital T thing. So here is A bar again. And now I've got rid of the capital T. So now I'm going to assume that your time step is between 0 and 1, and it basically represents what percentage of the way through the diffusion process are you. So 0 would be all noise and 1 would be all, no, sorry, 0 would be all clean and 1 would be all noise. So how far through the forward diffusion process. So other than that, this is exactly the same equation we've already seen. And I realized something else, which is kind of fun, which is you can take the inverse of that. So you can calculate T. So we would basically first take the square root, and we would then take the inverse cos, and we would then divide by 2 over pi, or times pi over 2. So we can both, so it's interesting now, we don't, the alpha bar is not something we look up in a list. It's something we calculate with a function from a float. And so yeah, interestingly, that means we can also calculate T from an alpha bar. So Noisify has changed a little. So now when we get the alpha bar for our time step, we don't look it up. We just call it, call the function. And now the time step is a random float between 0 and 1. Actually between 0 and 0.999. Which actually I'm sure there's a function I could have chosen to do a float in this range, but I just grabbed it because I was lazy. Couldn't be bothered looking it up. Other than that, Noisify is exactly the same. So we're still returning the XT, the time step, which is now a float, and the noise. That's the thing we're going to try and predict. Dependent variable, this tuple there is our inputs to the model. All right, so here is what that looks like. So now when we look at our input to our UNet training process, you can see, you know, we've got a T of 0.05, so 5% of the way through the forward diffusion process, it looks like this. And 65% through, it looks like this. So now the time step, and basically the process is more of a kind of a continuous time step and a continuous process. Rather before we were having these discrete time steps. Here we get just any random value that could be between 0 and 1. And I think, yeah, that's also something. Which can be all this more convenient, you know, to have. Yeah it is convenient. To have a function to call. Yeah, I find this life a little bit easier. So the model's the same, the callbacks are the same, the fitting process is the same. And so something which is kind of fun is that we could now, and we do now, create a little denoise function. So we can take, you know, this batch of data that we generated, the noisified data, so here it is again. And we can denoise it. So we know the T for each element, obviously. So remember T is different for each element now. And we can therefore calculate the alpha bar for each element. And then we can just undo the noisification to get the denoised version. And so if we do that, here's what we get. And so this is great, right? It shows you what actually happens when we run a single step of the model on varyingly partially noised images. And this is something you don't see very often, because I guess not many people are working in these kind of interactive notebook environments where it's really easy to do this kind of thing. So this is really helpful to get a sense of like, okay, if you're 25% of the way through the forward diffusion process, this is what it looks like when you undo that. If you're 95% of the way through it, this is what happens when you undo that. So you can see here, it's basically like, oh, I don't really know what the hell's going on. So at least a noisy mess. Yeah, I guess my feeling from looking at this is, I'm impressed, you know, like this 45% noise thing, it looks all noise to me. It's found the long sleeved top. And yeah, it's actually pretty close to the real one. I looked it up, you might see it later, it's a little bit more of a pattern here, but it even gives a sense of the pattern. So it shows you how impressive this is. So this 35%, you can kind of see there's a shoe there, but it's really picked up the shoe nicely. So these are very impressive models in one step, in my opinion. So okay, so sampling is basically the same, except now, rather than using the range function to create a time steps, we use lin space to create our time steps. So our time steps start at, you know, if we did 1000, it would be 0.999, and they end at zero, and then they're just linearly spaced with this number of steps. So other than that, you know, A bar, we now calculate, and the next A bar is going to be whatever the current step is, minus whatever step. So if you're doing 100 steps, then you'd be minus 0.01. So this is just stepping through linearly. And yeah, that's actually it for changes. So if we just do DDIM for 100 steps, you know, that works really well. We get a fit of three, which is actually quite a bit better than we had on 100 steps for our previous DDIM. So this definitely seems like a good sampling approach. And I know Jono is going to talk a bit more shortly about, you know, some of the things that can make better sampling approaches. But yeah, definitely, we can see it making a difference here. Did you guys have anything you wanted to say about this before we move on? No, but it is a nice transition towards some of the other things we'll be looking at to start thinking about how do we frame this. And it's also good, like the idea. So the original DDPM paper has this 1000 time steps, and a lot of people follow that. But the idea that you don't have to be bound to that, and maybe it is worth breaking that convention. I know Tanish made that meme about, you know, there's 15 competing different standards for notation. But yeah, sometimes it's helpful to reframe it. Okay, time goes from 0 to 1, that can simplify some things, maybe complicates others. But yeah, it's nice to think how you can reframe stuff sometimes. Yeah, and in fact, where we will head today, by the time we get to notebook 23, we will see, you know, even simpler notation. And yeah, simpler notation generally comes. I think what happens is over time, people understand better what's the essence of the problem and the approach, and then that gets reflected in the notation. So, okay, so the next part I wanted to share is something which is an idea we've been working on for a while. And it's some new research. So partly, I guess this is an interesting like insight into how we do research. So this is 22 noise pred. And the basic idea of this was, well, actually, I'm going to take you through it to see what the basic idea is. So what I'm going to do is I'm going to create, okay, so fashion MNIST as before. But I'm going to create a different kind of model. I'm not going to create a model that predicts the noise, given the noised image in T. Instead, I'm going to try to create a model which predicts T, given the noised image. So why did I want to do that? Well, partly, well, entirely, because I was curious. I felt like when I looked at something like this, I thought it was pretty obvious, roughly how much noise each image had. And so I thought, why are we passing noise when we call the model? Why are we passing in the noised image and the amount of noise or the T? Given that I would have thought the model could figure out how much noise there is. So I wanted to check my intention, which is that the model could figure out how much noise there is. So I thought, okay, well, let's create a model that would try and figure out how much noise there is. So I created a different noisify now. And this noisify grabs an alpha bar T randomly. And it's just a random number between 0 and 1. You don't want 1 per item in the batch. And so then after just randomly grabbing an alpha bar T, we then noisify in the usual way. But now our independent variable is the noised image, and the dependent variable is alpha bar T. And so we're going to try to create a model that can predict alpha bar T, given a noised image. Okay, so everything else is the same as usual. And so we can see an example. You've got alpha bar T dot squeeze dot log it. Oh, yeah, that's true. So the alpha bar T goes between 0 and 1. So we've got a choice. Like I mean, we don't have to do anything. But you know, normally, if you've got something between 0 and 1, you might consider putting a sigmoid at the end of your model. But I felt like the difference between 0.999 and 0.99 is very significant, you know. So if we do log it, then we don't need the sigmoid at the end anymore. It'll naturally cover the full range of kind of, you know, it'll be centered at zero, it'll cover all the other normal kind of range of numbers. And it also will treat, you know, equal ratios as equally important at both ends of the spectrum. So that was my hypothesis, was that using log it would be better. I did test it, and it was actually very dramatically better. So without this log it here, my model didn't work well at all. And so this is like an example of where thinking about these details is really important. Because if I hadn't have done this, then I would have come away from this bit of research thinking like, oh, I was wrong. You can't predict noise amount. Yeah, so thanks for pointing that out, Chana. Yeah, so that's why in this example of a mini-batch, you can see that the numbers can be negative or positive. So zero would represent noise, alpha bar of 0.5. So here 3.05 is not very noised at all. Where else, negative one is pretty noisy. So the idea is that, yeah, given this image, you would have to try to predict 3.05. So one thing I was kind of curious about is, like, and it's always useful to know is like, what's the baseline? Like, what counts as good? You know, because often people will say to me like, oh, I created a model and the MSE was 2.6. I'll be like, well, is that good? Well, it's the best I can do. But is it good? Like, or is it better than random? Or is it better than predicting the average? So in this case, I was just like, okay, well, what if we just predicted, actually, this is slightly out of date. I should have said zero here rather than 0.5, but never mind, close enough. So this is before I did the logit thing. So I basically was looking at like, what's the, you know, loss if you just always predicted a constant, which as I said, I should have put zero here, haven't updated it. And so it's like, oh, that would give you a loss of 3.5. Or another way to do it is you could just put MSE here and then look at the MSE loss between 0.5 and your various, just a single mini-batch, which we, yeah, mini-batch of alphabatis logits. Yeah, so, you know, we wanted to get, you know, if we're getting something that's about 3, then we basically haven't done any better than random. And so in this case, this model, it doesn't actually have anything to learn. And it always returns the same thing. So we can just call fit with train equals false just to find the loss. So these are just a couple of ways of getting quickly, finding a loss for a baseline naive model. One thing that thankfully PyTorch will warn you about is if you try to use MSE and your inputs and targets have different shapes, it will broadcast and give you probably not the result you would expect. And it will give you a warning. So one way to avoid that is just to use .flatten on each. So this kind of flattened MSE is useful to avoid both, avoid the warning and also avoid getting weird errors, or sorry, weird results. So we use that for our loss. So the model's the model that we always use. So it's kind of nice. We just use our same old model. Nothing changes, even though we're doing something totally different. Oh, well, okay, that's not quite true. The difference is that our output, we just have one output now. Because this is now a regression model, it's just trying to predict a single number. And so our learner now uses MSE as the loss. Everything else is the same as usual. So we can go ahead and train it. And you can see, okay, the loss is already much better than three. So we're definitely learning something. And we end up with a .075 mean squared error. That's pretty good, considering, you know, there's a pretty wide range of numbers we're trying to predict here. So I'm going to save that as noise prediction on Sigma. So save that model. And so we can take a look at how it's doing by grabbing our one batch of noised images, putting it through our T model. Actually, it's really an alpha bar model. But never mind, call it a T model. And then we can take a look to see what it's predicted for each one. And we can compare it to the actual for each one. And so you can see here, it said, oh, I think this is about .91. And actually, it is .91. So now here, it looks like about .36. And yeah, it is actually .36. So you know, you can see overall .72, it's actually .72. Or it's actually right, this one's .02 off. But yeah, my hypothesis was correct. Which is that we, you know, we can predict the thing that we were putting in manually as input. So there's a couple of reasons I was interested in checking this out. The first was just like, well, yeah, wouldn't it be simpler if we weren't passing in the T each time? You know, why not pass in the T each time? But it also felt like it would open up a wider range of kind of how we can do sampling. The idea of doing sampling by like precisely controlling the amount of noise that you try to remove each time. And then assuming you can remove exactly that amount of noise each time feels limited to me. So I want to try to remove this constraint. So having built this model, I thought, okay, well, you know, which is basically like, okay, I think we don't need to pass T in. Let's try it. So what I then did is I replicated the 22 cosine notebook. I just copied it, pasted it in here. But I made a couple of changes. The first is that Noisify doesn't return T anymore. So there's no way to cheat. We don't know what T is. And so that means that the unit now doesn't have T. So it's actually going to pass zero every time. So it has no ability to learn from T because it doesn't get T. So it doesn't really matter what we pass in. We could have changed the unit to like remove the conditioning on T. But for research, this is just as good, you know, for finding out. And it's good to be lazy when doing research. There's no point doing something a fancy way when you can do it a quick and easy way before you even know if it's going to work. So yeah, that's the only change. So we can then train the model. And we can check the loss. So the loss here is 0.034. And previously it was 0.033. So interestingly, you know, maybe it's a tiny bit worse at that, you know. But it's very close. Okay, so we'll save that model. And then for sampling, I've got exactly the same DDIM step as usual. And my sampling is exactly the same as usual. Except now, when I call the model, I have no T to pass in. So we just pass in this. I mean, I still know T because I'm still using the usual sampling approach, but I'm not passing it to the model. And yeah, we can sample. And what happens is actually pretty garbage. 22 is our fit. And as you can see here, you know, some of the images are still really noisy. So I totally failed. And so that's always a little discouraging when you think something's going to work and it doesn't. But my reaction to that is like, if I think something's going to work and it doesn't, is to think, well, I'm just going to have to do a better job of it. You know, it ought to work. So I tried something different, which is I thought like, okay, since we're not passing in the T, then we're basically saying like, how much noise should you be removing? It doesn't know exactly. So it might remove a little bit more noise that we want, or a little bit less noise than we want. And we know from the, you know, testing we did, that sometimes it's out by like, in this case, 0.02. And I guess if you're out consistently, sometimes it's, yeah, got to end up not removing all the noise. So the change I made was to the DDIM step, which is here. And let me just copy this and get rid of the, I mean, to that sections just to make it a bit easier to read. Okay. So the DDIM step, this is the normal DDIM step. Okay. And so step one is the same. So don't worry about that, because it's the same as we've seen before. But what I did was I actually used my T model. So I passed the noised image into my T model, which is actually an alpha bar model, to get the predicted alpha bar. And this is, remember, the predicted alpha bar for each image. Because we know from here that sometimes, so sometimes it did a pretty good job, right? But sometimes it didn't. So I felt like, okay, we need a predicted alpha bar for each image. What I then discovered is sometimes that could be like really too low, right? So what I wanted to make sure was it wasn't too crazy. So I then found the median for a mini batch of all the predicted alpha bars, and I clamped it to not be too far away from the median. And so then what I did when I did my X0 hat is rather than using alpha bar T, I used the estimated alpha bar T for each image clamped to be not too far away from the median. And so this way it was updating it based on the amount of noise that actually seems to be left behind, rather than the assumed amount of noise that should be left behind, you know, if we assume it's removed the correct amount. And then everything else is the same. So when I did that, it's like, whoa, made all the difference. And here it is. They are beautiful pieces of clothing. So 3.88 versus 3.2, that's possibly close enough. Like I'd have to run it a few times, you know, my guess is maybe it's a tiny bit worse, but it's pretty close. But like this definitely gives me some encouragement that, you know, even though this is like something I just did in a couple of days, where else the kind of the with T approaches have been developed since 2015, and we're now in 2023. You know, I would expect it's quite likely that these kind of like no T approaches could eventually surpass the T based approaches. And like one thing that definitely makes me think like there's room to improve is if I plot the FID or the KID for each sample during the reverse diffusion process, it actually gets worse for a while. I'm like, okay, well, that's a bad sign. I have no idea why that's happening, but it's a sign that, you know, if we could improve each step, then one would assume we could get better than 3.8. So yeah, Tanish, Gautam, do you have any thoughts about that? Any more questions or comments? Maybe to just like to highlight the research process a little bit. It wasn't like this linear thing of like, oh, here's this issue not performing as well as we thought. Oh, here's the fix. We just clamped this. You know, this was like multiple days of like discussing and like Jeremy saying like, you know, I'm tearing my hair out. Do you guys have any ideas? And oh, what about this? And oh, I noticed in the team paper, they do this clamping. Maybe that'll help. Yeah, so there's a lot of back and forth and also a lot of like, you saw the code that was commented out there, prints, XT.min, XT.max, alpha bar, pred, you know, just like seeing, oh, okay. You know, my average prediction is about what I expect, but sometimes the middle of the max goes, you know, two, three, eight, 16, 150, 212 million, infinity, you know, maybe like one or two little values that would just skyrocket out. Yeah. So that kind of like debugging and exploring and printing things out. And actually our initial discussions about this idea, I kind of said to you guys before lesson one of part two, I said like, it feels to me like we shouldn't need the T thing. And so it's actually been like mumbling away in the background for months. Yeah. And I guess, I mean, we should also mention, we have tried this, like a friend of ours trained a no T version of stable diffusion for us. And we did the same sort of thing. I trained a pretty bad T predictor and it sort of generates samples. So we're not like focusing on that large scale stuff yet, but it is fun to like, here we are again, got this idea from Fashion Industry. We are trying these out on some bigger models and seeing, okay, this does seem like maybe it'll work. And so down the line, that future plan is to say, let's actually, you know, spend the time, train a proper model and see, yeah, see how well that does. If it's interesting. You say a friend of ours, we can be more specific. It's Robin, one of the two lead authors of the stable diffusion paper who actually has been fine tuning a real stable diffusion model, which is without T and it's looking super encouraging. So yeah, that'll be fun to play with, with this new, you know, we'll have to train a T predictor for that. See how it looks. Yeah. All right. So I guess the other area we've been talking about kind of doing some research on is this weird thing that came up over the last few weeks where our bug in the DDPM implementation, where we accidentally weren't doing it from minus one to one for the input range, it turned out that actually being from minus one to one wasn't a very good idea anyway. And so we ended up centering it as being from minus 0.5 to 0.5. And Jono and Tanishka have managed to actually find a paper, well, I say find a paper, a paper has come out in the last 24 hours, which has coincidentally cast some light on this and has also cited a paper that we weren't aware of, which was not released in the last 24 hours. So Jono, are you going to tell us a bit about that? Yeah, no, sure. I can do that. So it's funny, this was such perfect timing because I actually got up early this morning planning to run with the different input scalings and the cosine schedule that Jeremy was showing and some of the other schedulers we look at. I thought it might be nice for the lesson to have a little plot of like, what is the fit with these different solvers and input scalings, but it was going to be a lot of work. I was like, not looking forward to doing the groundwork. And then Tanishka sent me this paper, which AK had just tweeted out because he reviews everything that comes up on archive every day on the importance of noise scheduling for diffusion models. This is by a researcher at the Google Brain team, who's also done a really cool recent paper on something called a recurrent interface network outside of the script of this lesson, but also worth checking out. Yeah, so this paper, they're hoping to study this noise scheduling and the strategies that you take for that. And they want to show that number one, noise scheduling is crucial for performance and the optimal one depends on the tasks. When increasing the image size, the noise scheduling that you want changes. And scaling the input data by some factor is a good strategy for working with this. And that's the bit we've been talking about, right? Yeah, that's what we've been doing where we said, oh, do we scale from minus 0.5 to 0.5 or minus one to one or do we normalize? And so they demonstrate the effectiveness by training a really good high resolution model on ImageNet. So class condition model. Correct. Yeah, amazing samples. They'll show one later. So I really like this paper. It's very short and concise and it just gets all the information across. And so they introduced us here. We have this noising process on noiseify function where we have square root of something times X plus square root of one minus that something times the noise. And here they use gamma, gamma of T, which is often used for the continuous time case. So instead of the alpha bar and the beta bar scheduled for a thousand time steps, there'll be some function gamma of T that tells you what your alpha bar should be. Okay. So that's our, our function is actually called A bar, but it's the same thing. Yeah. Same, same thing. So it's gamma of zero to one. And then that's used to noise the image. Interestingly, what they're showing here actually is something that we had discovered and I'd been complaining about that my DTIMs with an eater of less than one weren't working, which is to say when I added extra noise to the image, it wasn't working. And what they're showing here is like, oh yeah, duh, if you use a smaller image, then adding extra noise is probably not a good idea. Yeah. And so they, they, they use a lot of reference in this paper to like information being destroyed and signal to noise ratios. And that's really helpful for thinking about because it's not something that's obvious, but at 64 by 64 pixels, adjacent pixels might have much less in common versus the same amount of noise added at a much higher resolution. The noise kind of averages out and you can still see a lot of the image. So yeah, that's one thing they highlight is that the same noise level for different image sizes might have a, it might be a harder or easier task. And so they investigate some strategies for this. They look at the different noise schedule functions. So we've seen the original version from the DDPM paper. We've seen the cosine schedule and we've seen, I think we might look at, or the next thing that Jeremy's going to show us a sigmoid based schedule. And so they show the continuous time versions of that and they plot how you can change various parameters to get these different gamma functions or in our case, the alpha bar where we starting at all image, no noise at T equals zero, moving to all noise, no image at T equals one. But the path that you take, it's going to be different for these different classes of functions and parameters and the signal to noise ratio, that's what this, or the log signal to noise ratio is going to change over that time as well. And so that's one of the knobs we can tweak. We're saying our diffusion model isn't training that well. We think it might be related to the noise schedule and so on. One of the things you could do is try different noise schedules, either changing the parameters in one class of noise schedule or switching from a linear to a cosine to a sigmoid. And then the second strategy is kind of what we were doing in those experiments, which is just to add some scaling factor to X there. While we were accidentally using B of 0.5. Exactly. And so that's a second dial that you can tweak is to say, keeping your noise schedule fixed, maybe you just scale X zero, which is going to change the ratio of signal to noise. And that's what figure four in C there is what we were accidentally doing. Yes. Yeah, exactly. And so let's see if we can get to, oh yeah. So that again, changed the signal to noise for different scalings you get. So that's fine. So they have a compound, they have a strategy that combines some of those things. And this is the important part. They do their experiments. And so they have a nice table of investigating different schedules, cosine schedules and sigmoid schedules. And in bold are the best results. And you can see for 64 by 64 images versus 128 versus 256, the best schedule is not necessarily always the same. And so that's like important finding number one, depending on what your data looks like, using a different noise schedule might be optimal. There's no one true best schedule. There's no one value of, you know, beta min and beta max, that's just magically the best. Likewise for this input scaling at different sizes with whatever schedules they tested and different values were kind of optimal. And so, yeah, it's just a really great illustration, I guess, that this is another design choice that's implicit or explicitly part of your diffusion model training and sampling is how are you dealing with this noise schedule? What schedule are you following? What scaling are you doing of your inputs? And by using this thinking and doing these experiments, and they come up with a kind of rule of thumb for how to scale the image based on image size, they show that they can, as they increase the resolution, they can still maintain really good performance. Where previously it was quite hard to train a really large resolution pixel space model, and they're able to do that. They get some advantage from their fancy recurrent interface network, but still it's kind of cool that they can say, look, we get state of the art, high quality and 512 by 5N col 1024 by 1024 samples on class-conditioned ImageNet. And using this approach to really consider how well do you train? How many steps do we need to take? One of the other things in this table is that they compare it to previous approaches. Oh, we used a third of the training steps for the same other settings and we get better performance just because we've chosen that input scaling better. And yeah, so that's the paper. They do really nice, great work to the team. And that was really useful. I love that you got up in the morning and thought, oh, it's going to be a hassle training all these different models I need to train for different input scalings and different sampling approaches. I just look at Twitter first. And then you looked at Twitter and there was a paper saying like, hey, we just did a bunch of experiments for different noise schedules and input scaling. Yeah. Does your wife always work that way, Jono? It seems quite blessed. Yeah. It's very lucky like that. Yeah. If you wait long enough, someone else will do it. That's why it's always the time when he starts posting on Twitter. It's like my favorite hour of the day. It's just for all the papers to be posted. Oh, well, thank you for that. So let me switch to notebook 23. Because this notebook is actually largely an implementation of some ideas from this paper that everybody tends to just call it Keras. It's a bit unfair because there's other people. But I will do it anyway. Keras paper. And the reason we're going to look at this is because in this paper, the authors actually take a much more explicit look at the question of input scaling. Their approach was not apparently to accidentally put a bug in their code and then take it out and find it worked worse and then just put it back in again. Their approach was actually to think, how should things be? So that's an interesting approach to doing things. And I guess it works for them. So that's fine. I think our approach is more inflating. Yeah, exactly. Our approach is much more fun because you never quite know what's going to happen. And so, yeah, in their approach, they actually tried to say, like, okay, given all the things that are coming into our model, how can we have them all nicely balanced? So we will skip back and forth between the notebook and the paper. So the start of this is all the same, except now we are actually going to do it minus one to one, because we're not going to rely on accidental bugs anymore, but instead we're going to rely on the Keras paper's carefully designed scaling. I say that, except that I put a bug in this notebook as well. One of the things that's in the Keras paper is, what is the standard deviation of the actual data, which I calculated for a batch? However, this used to say minus 0.5. I used to do the minus 0.5 to 0.5 thing. And so this is actually the standard deviation of the data before I, when it was still minus 0.5. So this is actually half the real standard deviation. For reasons I don't yet understand, this is giving me better scaled results. So this actually should be 0.66. So there's still a bug here, and the bug still seems to work better, so we've still got some mysteries involved. So we're going to leave this. So it's actually, yeah, it's actually not 0.33, it's actually 0.66. Okay, so the basic idea of this paper, actually I'll come back. Well, let me have a little think. Yeah, okay, now we'll start here. The basic idea of this paper is to say, you know what, sometimes maybe predicting the noise is a bad idea. And so like you can either try and predict the noise, or you can try and predict the clean image, and each of those can be a better idea in different situations. If you're given something which is nearly pure noise, you know, the model's given something which is nearly pure noise, and is then asked to predict the noise, that's basically a waste of time, because the whole thing's noise. If you do the opposite, which is you try to get it to predict the clean image, well then if you give it a clean image that's nearly clean, and try to predict the clean image, that's nearly a waste of time as well. So you want something which is like, regardless of how noisy the image is, you want it to be kind of like an equally difficult problem to solve. And so what Keras do is they basically use this new thing called C skip, which is a number which is basically saying like, you know what we should do for the training target, is not just predict the noise all the time, not just predict the clean image all the time, but predict kind of a looped version of one or the other, depending on how noisy it is. So here Y is the plain image, and N is the noise. So Y plus N is the noised image. And so if C skip was zero, then we would be predicting the clean image. And if C skip was one, we would be predicting Y minus Y, we would be predicting the noise. And so you can decide by picking a different C skip whether you're predicting the clean image or the noise. And so as you can see from the way they've written it, they make this a function. They make it a function of sigma. Now this is where we've got to a point now where we've kind of got a fairly much simpler notation. There's no more alpha bars, no more alphas, no more betas, no more beta bars. There's just a single thing called sigma. Unfortunately sigma is the same thing as alpha bar used to be. So we've simplified it, but we've also made things more confusing by using an existing symbol for something totally different. So this is alpha bar. So there's going to be a function that says depending on how much noise there is, we'll either predict the noise or we'll predict the clean image or we'll predict something between the two. So in the paper they showed this chart where they basically said like, okay, let's look at the loss to see how good are we with a trained model at predicting when sigma is really low. So when there's very small alpha bar or when sigma is in the middle or when sigma is really high. And they basically said, you know what, when it's nearly all noise or nearly no noise, you know, we're basically not able to do anything at all. You know, we're basically good at doing things when there's a medium amount of noise. So when deciding, okay, what sigmas are we going to send to this thing, the first thing we need to do is to figure out some sigmas. And they said, okay, well let's pick a distribution of sigmas that matches this red curve here, as you can see. And so this is a normally distributed curve where this is on a log scale. So this is actually a log normal curve. So to get the sigmas that they're going to use, they picked a normally distributed random number and then they x'ed it. And this is called a log normal distribution. And so they used a mean of minus 1.2 and a standard deviation of 1.2. So that means that about one third of the time, they're going to be getting a number that's bigger than zero here. And e to the zero is one. So about one third of the time, they're going to be picking sigmas that are bigger than one. And so here's a histogram I drew of the sigmas that we're going to be using. And so it's nearly always, you know, less than five. But sometimes it's way out here. And so it's quite hard to read these histograms. So this really nice library called Seaborn, which is built on top of Matplotlib, has some more sophisticated and often nicer looking plots. And one of them they have is called a KDE plot, which is a kernel density plot. It's a histogram, but it's smooth. And so I clipped it at 10 so that you could see it better. So you can basically see that the vast majority of the time it's going to be somewhere, you know, about 0.4 or 0.5. But sometimes it's going to be really big. So our Noisify is going to pick a sigma using that lognormal distribution. And then we're going to get the noise as usual. But now we're going to calculate C skip. Because we're going to do that thing we just saw. We're going to find something between the plane image and the noise input. So what do we use for C skip? We calculate it here. And so what we do is we say, what's the total amount of variance at some level of sigma? Well it's going to be sigma squared. That's the definition of the variance of the noise. But we also have the sigma of the data itself. So if we add those two together, we'll get the total variance. And so what the Keras paper said to do is to do the variance of the data divided by the total variance, and use that for C skip. So that means that if your total variance is really big, so in other words it's got a lot of noise, then C skip's going to be really small. So if you've got a lot of noise, then this bit here will be really small. So that means if there's a lot of noise, try to predict the original image. That makes sense, because predicting the noise would be too easy. If there's hardly any noise, then this will be, total variance will be really small. So C skip will be really big. And so if there's hardly any noise, then try to predict the noise. And so that's basically what this C skip does. So it's a kind of slightly weird idea, is that our target, the thing we're trying to do actually, is not the input image, sorry, the original image. It's not the noise, but it's somewhere between the two. And I found the easiest way to understand that is to draw a picture of it. So here is some examples of noised input, with various amounts of, with various sigmas. Remember sigma is alpha bar. So here's an example with very little noise, 0.06. And so in this case, the target is predict the noise. So that's the hard thing to do, is predict the noise. Or else, here's an example, 4.53, which is nearly all noise. So for nearly all noise, the target is predict the image. And then for something which is a little bit between the two, like here, 0.64, the target is predict some of the noise and some of the image. So that's the idea of Paris. And so what this does is it's making the, you know, problem to be solved by the unit equally difficult, regardless of what sigma is. It doesn't solve our input scaling problem. It solves our kind of difficulty scaling problem. To solve the input scaling problem, they do it. I just wanted to make one quick note. And so like this sort of idea of, like is also interpolating between the noise and the image, is this similar to what's called a V objective as well? So there's also a similar kind of, it's yeah, it's very quite similar to what Keras of Dell has, but that's also now been used in a lot of different models. Like for example, stable diffusion 2.0 was trained with this sort of V objective. So people are using this sort of methodology and getting good results. And yeah, so it's an actual practical thing that people are doing. So yeah, just want to make a note of that. Yeah. As is the case of basically all papers created by NVIDIA researchers, of which this is one, it flies under the radar and everybody ignores it. The V objective paper came from the senior author was Tim Salamons, which is Google, right? Yeah. And so anything from Google and OpenAI, everybody listens to. So yeah, although Keras I think has done the more complete version of this. And in fact, the V objective was almost like mentioned in passing in the distillation paper. But yeah, that's the one that everybody has ended up looking at. But I think this is the more complete. I think what happened with the V objective is not many people paid attention to it. I think folks like Kat and Robin and these sorts of folks are actually paying attention to that V objective in that Google brain paper. But then also this paper did a much more principled analysis of this sort of thing. So yeah, I think it's very interesting how, yeah. Sometimes even these sort of side notes in papers that maybe people don't pay much attention to, they can actually be quite important. Yeah. So, okay. So the noise input as usual is the input image plus the noise times the sigma. But then, and then as we discussed, we decide how to kind of decide what our target is. But then we actually take that noise input and we scale it up or down by this number. And the target, we also scale up or down by this number. And those are both calculated in this thing as well. So here's C out and here's C in. Now I just wanted to show one example of where these numbers come from, because for a while they all seem pretty mysterious to me. And I felt like I'd never be smart enough to understand them, particularly because they are explained in the mathematical appendix of this paper, which are always the bits I don't understand, until I actually try to, and then it tends to turn out they're not so bad after all, which was certainly the case here. Which? I think it was up, it was B something, I think. So B6, I think. Is that the one? Oh yeah. So in appendix B6, which does look pretty terrifying. But if you ever, if you actually look at, for example, what we're just looking at, C in, it's like, how do they calculate? So C in is this. Now this is the variance of the noise. This is the variance of the data. Add them together to get the total variance, square roots, the standard deviation, total standard deviation. So it's just the inverse of the total standard deviation, which is what we have here. Where does that come from? Well they just said, you know what, the inputs for a model should have unit variance. Now we know that. We've done that to death in this course. So they said, all right, so, well the inputs to the model is the clean data plus the noise times some number we're going to calculate. And we want that to be one. So the variance of the clean images plus the noise is equal to the variance of the clean images plus the variance of the noise. Okay, so if we want that to be, if we want variance to be one, then divide both sides by this and take the square root. And that tells us our multiplier has to be one over this. That's it. So it's like literally, you know, classical math. The only bit you have to know is that the variance of two things added together is the variance of the two things added together, which is not rocket science either. And in this context, like why we want to do this, when we looked at those sigmas that you're plotting, like the distribution, you've got some that are fairly low, but you've also got some where the standard deviation sigma is like 40. Right. So the variance is super high. Yes. And so we don't want to feed something with standard deviation 40 into our model. You would like it to be closer to unit variance. So we're thinking, okay, well if you divide by roughly 40, that would scale it down. But then we've also got some extra variance from our data. So it's like 40 plus variance of the data of a little bit. We want to scale back down by that to get unit variance. I love this paper because it's basically just doing what we spent weeks doing. I feel like everything that we've done that's improved every model has always been one thing, which is, can we get mean zero, variance one inputs to our model and for all of our activations? And then the only other thing is include enough compute by adding enough layers and enough activations. Those two things seem to be all that matters. Basically well, I guess ResNet's added an extra cool little thing to that, which is to make it even smoother by giving this kind of like identity path. So yeah, basically trying to make things as smooth as possible and as equal everywhere as possible. So yeah, this is what they've done. So they did that for the inputs and then they've also done it for the outputs. And for the outputs, you know, it's basically the same idea. You know, they have basically the same kind of analysis to show that. And so with this, so now, yeah, we've basically, we've got our noise to input. We've got the, you know, kind of linear version somewhere between X naught and the noise to input. We've got the scaling of the output and we've got the scaling of the input. So now for the inputs to our model, we're going to have the scaled noise. We're going to have the sigma and we're going to have the target, which is somewhere between the image and the noise. And so, yeah, so I've, you know, never seen anybody draw a picture of this before. So it was really cool when, you know, being in a notebook, being able to see like, oh, that's what they're doing, you know. So yeah, have a good look at this notebook to see exactly what's going on, because I think it gives you a really good intuition around what problem it's trying to solve. So then I actually checked the noise to input has a standard deviation of one. The mean's not zero, and of course, why would it be? We didn't do anything, you know, the only thing Keras cared about was having the variance one. We could easily adjust the input and output to have a mean of zero as well, and that's something I think we or somebody should try, because I think it does seem to help a bit as we saw with that generalized value stuff we did. But it's less important than the variance. And so same with the target, it's got the one. And yeah, this is where if I change this to the correct value, which is 0.66, then actually it's slightly further away from one, both here and here, quite a lot further away. And maybe that's because actually the data's, well, we know the data's not Gaussian distributed. Pixel data definitely isn't Gaussian distributed. So this bug turned out better. Okay, so the unit's the same, the initialization's the same, this is all the same, train it for a while. We can't compare the losses, right, because our target's different. But what we can do is we can create a denoise that just takes the thing that, as per usual, the thing we had in Noisify, right, and solve for X0. So we've got to multiply by C out and then add C skip by noise to input. Here it is, multiply by C out, add noise to input. Okay, so we can denoise. So let's grab our sigmas from the actual batch we had. Let's calculate C skip, C out, and C in for the sigmas in our mini-batch. Let's use the model to predict the target, given the noise to input and the sigmas, and then denoise it. And so here's our noise to input, which we've already seen, and here's our predictions. And these are absolutely remarkable, in my opinion. Yeah, like this one here, I can barely see it, you know, it's really found... Look at the shirt. There's a shirt here, it's actually really finding the little thing on the front, and let me show you, here's what it should look like. And in cases where the sigma's pretty high, like here, you can see it's really, like, saying, like, I don't know, maybe it's shoes, but it could be something else. Is it shoes? Yeah, it wasn't shoes, but at least it's kind of got the, you know, the bulk of the pixels in the right spot. Yeah, something like this one is 4.5, it has no idea what it is, it's like, oh, maybe it's shoes, maybe it's pants, you know, turns out it is shoes. Yeah, so I think that's fascinating how well it can do. And then the other thing I did, which I thought was fun, was I just created, so I just did a sigma of 80, which is actually what they do when they're doing sampling from pure noise, that's what they consider the pure noise level. So I just created some pure noise and denoised it just for one step. And so here's what happens when you denoise it for one step, and you can see it's kind of overlaid all the possibilities, it's like I can see a pair of shoes here, a pair of pants here, a top here. And sometimes it's kind of like more confident that the noise is actually a pair of pants, and sometimes it's more confident that it's actually shoes. But you can really get a sense of how, like, from pure noise, it starts to make a call about, like, what this noise is actually covering up. And this is also the bit which I feel is, like, I'm the least convinced about when it comes to diffusion models. This first step of going from, like, pure noise to something, and, like, trying to have a good mix of all the possible somethings, I'm, I don't know, it feels a bit hand-wavy to me. It clearly works quite well, but I'm not sure if it's, like, we're getting the full range of possibilities, and I feel like some of the papers we're starting to see are starting to say, like, you know what, maybe this is not quite the right approach. And maybe later in the course we'll look at some of the ones that look at what we call VQ models and tokenized stuff. Anyway, I thought this was pretty interesting to see these pictures, which I don't think, yeah, I've never seen any pictures like this before. So I think this is a fun result from doing all this stuff in notebooks, step by step. Okay, so sampling. So one of the nice things with this is the sampling becomes much, much, much simpler. And so, and I should mention a lot of the code that I'm using, particularly in the sampling section, is heavily inspired by, and some of it's actually copied and pasted from, Cat's KDiffusion repo, which is, I think I mentioned before, is some of the nicest generative modeling code, or maybe the nicest generative modeling code I've ever seen. It's really great. So before we talk about the actual sampling, the first thing we need to talk about is what sigma do we use at each reverse time step? And in the past, we've always, well, nearly always done something which I think has always felt as sketchy as all hell, which is we've just linearly gone down the sigmas or the alpha bars or the Ts. So here, when we're sampling in the previous notebook, we used linspace. So I always felt like that was questionable. And I felt like at the start, you probably, like, it was just noise anyway, so who cared? Who cares? So I, in DDPM v3, I experimented with something that I thought intuitively made more sense. I don't know if you remember this one, but I actually said, oh, let's, for the first hundred time steps, let's actually only run the model every 10 times. And then for the next hundred, let's run it nine times. And the next 100, let's run it every eight times. So basically at the start, be much less careful. And so Keras actually ran a whole bunch of experiments. And they said, yeah, you know what? At the start of training, you know, you can start with a high sigma, but then like step to a much lower sigma in the next step, and then a much lower sigma in the next step. And then the longer, the more you train, step by smaller and smaller steps, so that you spend a lot more time fine tuning carefully at the end, and not very much time at the start. Now, this has its own problems. And in fact, a paper just came out today, which we probably won't talk about today, but maybe another time, which talked about the problems is that in these very early steps, this is the bit where you're trying to create a composition that makes sense. Now for fashion MNIST, we don't have much composing to do. It's just a piece of clothing. But if you're trying to do an astronaut riding a horse, you know, you've got to think about how all those pieces fit together. And this is where that happens. And so I do worry that with the Keras approach is it's not giving that maybe enough time. But as I've said, that's really the same as this step. That whole piece feels a bit wrong to me. But aside from that, I think this makes a lot of sense, which is that, yeah, the sampling, you should jump, you know, by big steps early on, and small steps later on, and make sure that the fine details are just so. So that's what this function does, is it creates this plot. Now it's this schedule of reverse diffusion sigma steps. It's a bit of a weird function, in that it's the rowth root of sigma, where row is 7. So the seventh root of sigma is basically what it's scaling on. But the answer to why it's that is because they tried it, and it turned out to work pretty well. Do you guys remember where this was? This is a truncation error analysis, D1. Nice memory. So this image here, so thanks for reminding me where this is, shows FID as a function of row. So it's basically what, the what root are we taking. And they basically said like if you take the fifth root up, it seems to work well, basically. So yeah, so that's a perfectly good way to do things, is just to try things and see what works. And you'll notice they tried things just like we love on small datasets. Not as small as us, because we're the king of small datasets, but smallish, sci-fi 10, image net 64. That's the way to do things. I saw like, it might have even been the CEO of Hugging Face the other day, tweet something saying only people with huge amounts of GPUs can do research now. And I think it totally misunderstands how research is done, which is research is done on very small datasets. That's the actual research. And then when you're all done, you scale it up at the end. I think we're kind of pushing the envelope in terms of like, yeah, how much can you do? And yeah, we've like re-covered this kind of main substantive path of the Fusion models history step-by-step, showing every improvement and seeing clear improvements across all the papers using nothing but Fashioned MNIST, running on a single GPU in like 15 minutes of training or something per model. So yeah, definitely don't need lots of models. Anyway, okay, so this is the Sigma we're going to jump to. So the denoising is going to involve calculating the CSKIP, COUT and CIN, and calling our model with the CIN scaled data and the Sigma, and then scaling it with COUT and then doing the CSKIP. Okay, so that's just undoing the noiseify. So check this out. This is all that's required to do one step of denoising for the simplest kind of scheduler, which is, sorry, the simplest kind of sampler, which is called Euler. So we basically say, okay, what's the Sigma at time step i? What's the Sigma2 at time step i? And now when I'm talking about time step, I'm really talking about like the step from this function, right? So this is the sampling step. Sampling step, yeah. Okay, so then denoise using the function, and then we say, okay, well, just send back whatever you were given, plus move a little bit in the direction of the denoised image. So the direction is x minus denoised. So that's the noise, that's the gradient, as we discussed right back in the first lesson of this part. So we'll take the noise. If we divide it by Sigma, we get a slope. It's how much noise is there per Sigma. And then the amount that we're stepping is Sigma2 minus Sigma1. So take that slope and multiply it by the change, right? So that's the distance to travel towards the noise, this fraction. You could also think of it this way, and I know this is a very obvious algebraic change, but if we move this over here, you could also think of this as being, oh, of the total amount of noise, the change in Sigma we're doing, what percentage is that? Okay, well, that's the amount we should step. So there's two ways of thinking about the same thing. So again, this is just, you know, high school math. Well, I mean, actually, my seven-year-old daughter has done all these things. It's plus, minus, divide, and times. So we're going to need to do this once per sampling step. So here's a thing called sample, which does that. It's going to go through each sampling step, call our sampler, which initially we're going to do sample Euler, right? With that information, add it to our list of results, and do it again. So that's it. That's all the sampling is. And of course, we need to grab our list of Sigmas to start with. So I think that's pretty cool. And at the very start, we need to create our pure noise image. And so the amount of noise we start with is got a Sigma of 80. Okay, so if we call sample using sample Euler, then we get back some very nice looking images. And believe it or not, our FED is 1.98. So this extremely simple sampler, three lines of code, plus a loop, has given us a FED of 1.98, which is clearly, you know, substantially better than our cosine. Now we can improve it from there. So one potential improvement is to, you might have noticed, we added no new noise at all, right? This is a deterministic scheduler, right? There's no RAND anywhere here. So we can do something called an ancestral Euler sampler, which does add RAND, right? So we basically do the denoising in the usual way, but then we also add some RAND. So what we do need to make sure is, given that we're adding a certain amount of randomness, we need to remove that amount of randomness from the step that we take. So I won't go into the details, but basically there's a way of calculating how much new randomness and how much just going back in the existing direction do we do. And so there's the amount in the existing direction, and there's the amount in the new random direction. And you can just pass in eta, which is just going to, when we pass it into here, is going to scale that. So if we scale it by half, so basically half of it is new noise and half of it is going in the direction that we thought we should go, that makes it better still. Again with 100 steps. Just make sure I'm comparing to the same. Yep, 100 steps. Okay, so that's fair, like with like. Okay, so that's adding a bit of extra noise. Now then, something that I think we might have mentioned back in the first lesson of this part is something called Heun's method. And Heun's method does something which we can pictorially see here to decide where to go, which is basically we say, okay, where are we right now? What's the, you know, at our current point, what's the direction? So we take the tangent line, the slope, right? That's basically all it does is it takes the slope. So it's, oh, here's the slope, you know. Okay. And so if we take that slope, and that would take us to a new spot, and then at that new spot, we can then calculate a slope at the new spot as well. And at the new spot, the slope is something else. So that's it here, right? And then you say like, okay, well, let's go halfway between the two. And let's actually follow that line. And so basically, it's saying like, okay, each of these slopes is going to be inaccurate. But what we could do is calculate the slope of where we are, the slope of where we're going, and then go halfway between the two. It's, I actually find it easier to look at in code, personally. I just going to delete a whole bunch of stuff that's totally irrelevant to this conversation. So take a look at this compared to Euler. So here's our Euler, right? So we're going to do the same first line, exactly the same. Right? Then the denoising is exactly the same. Right? And then the this step here is exactly the same. I've actually just done it in multiple steps for no particular reason. And then it says, okay, well, if this is the last step, then we're done. So actually, the last step is Euler. But then what we do is we then say, well, that's okay, for an Euler step, this is where we'd go. Well, what does that look like if we denoise it? So this calls the model the second time. Right? So where would that take us if we took an Euler step there? And so here, if we do an Euler step there, what's the slope? And so what we then do is we say, oh, okay, well, it's just, just like in the picture, let's take the average. Okay, so let's take the average and then use that, the step. So that's all the Hewn sampler does, is it just takes the average of the slope where we're at and the slope where the Euler method would have taken us. And so if we, now, so notice that it caught the model twice for a single step. So to be fair, since we've been taking 100 steps with Euler, we should take 50 steps with Hewn, right? Because it's going to call the model twice. And still, that is now, whoa, we beat one, which is pretty amazing. And so we could keep going. Check this out. We can even go down to 20. We're doing 40 model evaluations, and this is better than our best Euler, which is pretty crazy. Now, something which you might've noticed is kind of weird about this, or kind of silly about this is we're calling the model twice just in order to average them. But we already have two model results, like without calling it twice, because we could have just looked at the previous time step. And so something called the LMS sampler does that instead. And so the LMS sampler, if I call it with 20, it actually literally does 20 evaluations. And actually it beats Euler with 100 evaluations. And so LMS, I won't go into the details too much. It didn't actually fit into my little sampling very well. So basically largely copied and pasted the CATS code. But the key thing it does is look at, it gets the current sig, sigma, it does the denoising, it calculates the slope, and it stores the slope in a list. And then it grabs the first one from the list. So it's kind of keeping a list of up to, in this case, four at a time. And so it then uses up to the last four to basically, yes, look at the curvature of this and take the next step. So that's pretty smart. And yeah, so I think if you wanted to do super fast sampling, it seems like a pretty good way to do it. I think Giotto, you were telling me that, or maybe it was Pedro was saying that currently people have started to move away, that this was very popular, but people started to move towards a new sampler, which is a bit similar called the DPM++ sampler, something like that. Yeah. Yeah. Yeah. Yeah. But I think it's the same idea. I think it kind of keeps a, let's say, keep a list of recent results and use that. I'll have to check it more closely. Yeah. That's a similar idea. It's like, if it's done more than one step, then it's using some history to do the next thing. Yeah. This history doesn't make a huge amount of sense, I guess, from that perspective. I mean, still, it works very well. This makes more sense. So then, you know, we can compare if we use an actual mini-batch of data, we get about 0.5. So yeah, I feel like this is quite a, you know, stunning result to get close to, very close to real data, at least in terms of fit, you know, really with 40 model evaluations. And the entire, you know, nearly the entire thing here is by making sure we've got unit variance inputs, unit variance outputs, and kind of equally difficult problems to solve in our loss function. Yeah. Plus having that different schedule for sampling, that's completely unrelated to the training schedule. I think that was one of the big things with Kara Siddal's paper was they also could apply this to like, oh, existing diffusion models that have been trained by other papers, we can use our sampler and in fewer steps get better results without any of the other changes. And yeah, I mean, they do a little bit of rearranging equations to get the other papers versions into their C-skip, C-in, C-out framework. But then yeah, it's really nice that these ideas can be applied to, so for example, I think stable diffusion, especially version one was trained DDPM style training, epsilon objective, whatever. But you can now get these different samplers and different sampling schedules and things like that and use that to sample it and do it in 10, 15, 20 steps and get pretty nice results. Yeah. You know, and another nice thing about this paper is they, you know, in fact, it's the name of the paper, elucidating the design space of diffusion based models. You know, they looked at various different papers and approaches and kind of said like, oh, you know what, these are all doing the same thing when we kind of parameterize things in this way. You know, you fill in these parameters, you get this paper and these parameters, you get that paper, you know, and then so we found a better set of parameters, which it was very nice to code because, you know, it really actually ended up simplifying things a whole lot. So if you look through the notebook carefully, which I hope everybody will, you'll see, you know, that the code is really there and simple compared to previous, all the previous ones, in my opinion. Like I feel like every notebook we've done from DDPM onwards, the code's got easier to understand and results. And just to again clarify, like how this connects with some of the previous papers that we've looked at. So like, for example, with the BDIM, the deterministic, that's again, this sort of deterministic approach that's similar to the Euler method sampler that we were just looking at, which was completely deterministic. And then some of something like the Euler ancestral that we were looking at is similar to the standard DDPM approach with the, that was kind of a more stochastic approach. So again, there's just all these sorts of connections that then are kind of nice to see that, again, the sorts of connections between the different papers and how they change it, how they can be expressed in this common framework. Yeah. Thanks, Tanish. So we definitely now are at the point where we can show you the UNet next time. So I think we're, unless any of us come up with interesting new insights on the unconditional diffusion sampling training and sampling process, we might be putting that aside for a while. And instead, we're going to be looking at creating a good quality UNet from scratch. And we're going to look at a different dataset to do that. As we're starting to scale things up a bit, as Jono mentioned in the last lesson. So we're going to be using a 64 by 64 pixel ImageNet subset, or TinyImageNet. So we'll start looking at some three channel images. So I'm sure we're all sick of looking at black and white shoes. So now we get to look at shift dwellings and trolley buses and koala bears and yeah, 200 different things. So that'll be nice. Yeah. All right. Well, thank you, Jono. Thank you, Tanish. That was fun, as always. And yeah, next time will be lesson 22. Bye. Lesson 22. This was lesson 22. Oh, no way. Okay. You're right. See ya. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.24, "text": " All right, hi gang, and here we are in lesson 21, joined by the legends themselves, Jono", "tokens": [50364, 1057, 558, 11, 4879, 10145, 11, 293, 510, 321, 366, 294, 6898, 5080, 11, 6869, 538, 264, 27695, 2969, 11, 7745, 78, 50826], "temperature": 0.0, "avg_logprob": -0.3454324722290039, "compression_ratio": 1.494047619047619, "no_speech_prob": 0.00636079628020525}, {"id": 1, "seek": 0, "start": 9.24, "end": 10.24, "text": " and Tanishq.", "tokens": [50826, 293, 314, 7524, 80, 13, 50876], "temperature": 0.0, "avg_logprob": -0.3454324722290039, "compression_ratio": 1.494047619047619, "no_speech_prob": 0.00636079628020525}, {"id": 2, "seek": 0, "start": 10.24, "end": 11.24, "text": " Hello.", "tokens": [50876, 2425, 13, 50926], "temperature": 0.0, "avg_logprob": -0.3454324722290039, "compression_ratio": 1.494047619047619, "no_speech_prob": 0.00636079628020525}, {"id": 3, "seek": 0, "start": 11.24, "end": 12.24, "text": " Hello.", "tokens": [50926, 2425, 13, 50976], "temperature": 0.0, "avg_logprob": -0.3454324722290039, "compression_ratio": 1.494047619047619, "no_speech_prob": 0.00636079628020525}, {"id": 4, "seek": 0, "start": 12.24, "end": 25.2, "text": " And today you'll be shocked to hear that we are going to look at a Jupyter notebook.", "tokens": [50976, 400, 965, 291, 603, 312, 12763, 281, 1568, 300, 321, 366, 516, 281, 574, 412, 257, 22125, 88, 391, 21060, 13, 51624], "temperature": 0.0, "avg_logprob": -0.3454324722290039, "compression_ratio": 1.494047619047619, "no_speech_prob": 0.00636079628020525}, {"id": 5, "seek": 0, "start": 25.2, "end": 26.2, "text": " Amazing right?", "tokens": [51624, 14165, 558, 30, 51674], "temperature": 0.0, "avg_logprob": -0.3454324722290039, "compression_ratio": 1.494047619047619, "no_speech_prob": 0.00636079628020525}, {"id": 6, "seek": 0, "start": 26.2, "end": 28.88, "text": " We're going to look at notebook 22.", "tokens": [51674, 492, 434, 516, 281, 574, 412, 21060, 5853, 13, 51808], "temperature": 0.0, "avg_logprob": -0.3454324722290039, "compression_ratio": 1.494047619047619, "no_speech_prob": 0.00636079628020525}, {"id": 7, "seek": 2888, "start": 28.88, "end": 46.0, "text": " This is a pretty quick, just, you know, improvement, pretty simple improvement to our ddpm slash", "tokens": [50364, 639, 307, 257, 1238, 1702, 11, 445, 11, 291, 458, 11, 10444, 11, 1238, 2199, 10444, 281, 527, 274, 67, 14395, 17330, 51220], "temperature": 0.0, "avg_logprob": -0.4169409149571469, "compression_ratio": 1.25, "no_speech_prob": 0.02328350767493248}, {"id": 8, "seek": 2888, "start": 46.0, "end": 50.599999999999994, "text": " ddim implementation for fashion MNIST.", "tokens": [51220, 274, 67, 332, 11420, 337, 6700, 376, 45, 19756, 13, 51450], "temperature": 0.0, "avg_logprob": -0.4169409149571469, "compression_ratio": 1.25, "no_speech_prob": 0.02328350767493248}, {"id": 9, "seek": 5060, "start": 50.6, "end": 62.04, "text": " And this is all the same so far, but what I've done is I've made some, one quite significant", "tokens": [50364, 400, 341, 307, 439, 264, 912, 370, 1400, 11, 457, 437, 286, 600, 1096, 307, 286, 600, 1027, 512, 11, 472, 1596, 4776, 50936], "temperature": 0.0, "avg_logprob": -0.2979966572352818, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.027144480496644974}, {"id": 10, "seek": 5060, "start": 62.04, "end": 68.2, "text": " change and some of the changes we'll be making today are all about making life simpler.", "tokens": [50936, 1319, 293, 512, 295, 264, 2962, 321, 603, 312, 1455, 965, 366, 439, 466, 1455, 993, 18587, 13, 51244], "temperature": 0.0, "avg_logprob": -0.2979966572352818, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.027144480496644974}, {"id": 11, "seek": 5060, "start": 68.2, "end": 73.28, "text": " And they're kind of reflecting the way the papers have been taking things.", "tokens": [51244, 400, 436, 434, 733, 295, 23543, 264, 636, 264, 10577, 362, 668, 1940, 721, 13, 51498], "temperature": 0.0, "avg_logprob": -0.2979966572352818, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.027144480496644974}, {"id": 12, "seek": 5060, "start": 73.28, "end": 78.32, "text": " And it's interesting to see how the papers have not only made things better, they made", "tokens": [51498, 400, 309, 311, 1880, 281, 536, 577, 264, 10577, 362, 406, 787, 1027, 721, 1101, 11, 436, 1027, 51750], "temperature": 0.0, "avg_logprob": -0.2979966572352818, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.027144480496644974}, {"id": 13, "seek": 7832, "start": 78.32, "end": 81.36, "text": " things simpler.", "tokens": [50364, 721, 18587, 13, 50516], "temperature": 0.0, "avg_logprob": -0.30468588783627465, "compression_ratio": 1.5096153846153846, "no_speech_prob": 0.0015485836192965508}, {"id": 14, "seek": 7832, "start": 81.36, "end": 88.46, "text": " And so one of the things that I've noticed in recent papers is that there's no longer", "tokens": [50516, 400, 370, 472, 295, 264, 721, 300, 286, 600, 5694, 294, 5162, 10577, 307, 300, 456, 311, 572, 2854, 50871], "temperature": 0.0, "avg_logprob": -0.30468588783627465, "compression_ratio": 1.5096153846153846, "no_speech_prob": 0.0015485836192965508}, {"id": 15, "seek": 7832, "start": 88.46, "end": 94.88, "text": " a concept of end steps, which is something we've always had before and always bothered", "tokens": [50871, 257, 3410, 295, 917, 4439, 11, 597, 307, 746, 321, 600, 1009, 632, 949, 293, 1009, 22996, 51192], "temperature": 0.0, "avg_logprob": -0.30468588783627465, "compression_ratio": 1.5096153846153846, "no_speech_prob": 0.0015485836192965508}, {"id": 16, "seek": 7832, "start": 94.88, "end": 100.88, "text": " me a bit, this capital T thing.", "tokens": [51192, 385, 257, 857, 11, 341, 4238, 314, 551, 13, 51492], "temperature": 0.0, "avg_logprob": -0.30468588783627465, "compression_ratio": 1.5096153846153846, "no_speech_prob": 0.0015485836192965508}, {"id": 17, "seek": 7832, "start": 100.88, "end": 106.67999999999999, "text": " You know, this T over T, it's basically saying this is time step number, say 500 out of 1000,", "tokens": [51492, 509, 458, 11, 341, 314, 670, 314, 11, 309, 311, 1936, 1566, 341, 307, 565, 1823, 1230, 11, 584, 5923, 484, 295, 9714, 11, 51782], "temperature": 0.0, "avg_logprob": -0.30468588783627465, "compression_ratio": 1.5096153846153846, "no_speech_prob": 0.0015485836192965508}, {"id": 18, "seek": 10668, "start": 106.68, "end": 109.28, "text": " so it's time step 0.5.", "tokens": [50364, 370, 309, 311, 565, 1823, 1958, 13, 20, 13, 50494], "temperature": 0.0, "avg_logprob": -0.2636897483568513, "compression_ratio": 1.4375, "no_speech_prob": 0.004982159473001957}, {"id": 19, "seek": 10668, "start": 109.28, "end": 115.24000000000001, "text": " Why not just call it 0.5?", "tokens": [50494, 1545, 406, 445, 818, 309, 1958, 13, 20, 30, 50792], "temperature": 0.0, "avg_logprob": -0.2636897483568513, "compression_ratio": 1.4375, "no_speech_prob": 0.004982159473001957}, {"id": 20, "seek": 10668, "start": 115.24000000000001, "end": 117.96000000000001, "text": " And the answer is, well, we can.", "tokens": [50792, 400, 264, 1867, 307, 11, 731, 11, 321, 393, 13, 50928], "temperature": 0.0, "avg_logprob": -0.2636897483568513, "compression_ratio": 1.4375, "no_speech_prob": 0.004982159473001957}, {"id": 21, "seek": 10668, "start": 117.96000000000001, "end": 121.2, "text": " So we talked last time about the cosine scheduler.", "tokens": [50928, 407, 321, 2825, 1036, 565, 466, 264, 23565, 12000, 260, 13, 51090], "temperature": 0.0, "avg_logprob": -0.2636897483568513, "compression_ratio": 1.4375, "no_speech_prob": 0.004982159473001957}, {"id": 22, "seek": 10668, "start": 121.2, "end": 127.84, "text": " We didn't end up using it because I came up with an idea which was, you know, simpler", "tokens": [51090, 492, 994, 380, 917, 493, 1228, 309, 570, 286, 1361, 493, 365, 364, 1558, 597, 390, 11, 291, 458, 11, 18587, 51422], "temperature": 0.0, "avg_logprob": -0.2636897483568513, "compression_ratio": 1.4375, "no_speech_prob": 0.004982159473001957}, {"id": 23, "seek": 10668, "start": 127.84, "end": 131.96, "text": " and nearly the same, which is just to change our betamax.", "tokens": [51422, 293, 6217, 264, 912, 11, 597, 307, 445, 281, 1319, 527, 778, 2404, 87, 13, 51628], "temperature": 0.0, "avg_logprob": -0.2636897483568513, "compression_ratio": 1.4375, "no_speech_prob": 0.004982159473001957}, {"id": 24, "seek": 13196, "start": 131.96, "end": 136.88, "text": " But in this next, what can I say, let's use the cosine scheduler, but let's try to get", "tokens": [50364, 583, 294, 341, 958, 11, 437, 393, 286, 584, 11, 718, 311, 764, 264, 23565, 12000, 260, 11, 457, 718, 311, 853, 281, 483, 50610], "temperature": 0.0, "avg_logprob": -0.24947336404630455, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.15397748351097107}, {"id": 25, "seek": 13196, "start": 136.88, "end": 143.9, "text": " rid of the end steps thing and the capital T thing.", "tokens": [50610, 3973, 295, 264, 917, 4439, 551, 293, 264, 4238, 314, 551, 13, 50961], "temperature": 0.0, "avg_logprob": -0.24947336404630455, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.15397748351097107}, {"id": 26, "seek": 13196, "start": 143.9, "end": 146.92000000000002, "text": " So here is A bar again.", "tokens": [50961, 407, 510, 307, 316, 2159, 797, 13, 51112], "temperature": 0.0, "avg_logprob": -0.24947336404630455, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.15397748351097107}, {"id": 27, "seek": 13196, "start": 146.92000000000002, "end": 152.64000000000001, "text": " And now I've got rid of the capital T. So now I'm going to assume that your time step", "tokens": [51112, 400, 586, 286, 600, 658, 3973, 295, 264, 4238, 314, 13, 407, 586, 286, 478, 516, 281, 6552, 300, 428, 565, 1823, 51398], "temperature": 0.0, "avg_logprob": -0.24947336404630455, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.15397748351097107}, {"id": 28, "seek": 13196, "start": 152.64000000000001, "end": 158.76000000000002, "text": " is between 0 and 1, and it basically represents what percentage of the way through the diffusion", "tokens": [51398, 307, 1296, 1958, 293, 502, 11, 293, 309, 1936, 8855, 437, 9668, 295, 264, 636, 807, 264, 25242, 51704], "temperature": 0.0, "avg_logprob": -0.24947336404630455, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.15397748351097107}, {"id": 29, "seek": 13196, "start": 158.76000000000002, "end": 160.72, "text": " process are you.", "tokens": [51704, 1399, 366, 291, 13, 51802], "temperature": 0.0, "avg_logprob": -0.24947336404630455, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.15397748351097107}, {"id": 30, "seek": 16072, "start": 160.72, "end": 166.8, "text": " So 0 would be all noise and 1 would be all, no, sorry, 0 would be all clean and 1 would", "tokens": [50364, 407, 1958, 576, 312, 439, 5658, 293, 502, 576, 312, 439, 11, 572, 11, 2597, 11, 1958, 576, 312, 439, 2541, 293, 502, 576, 50668], "temperature": 0.0, "avg_logprob": -0.27114728554007933, "compression_ratio": 1.6586538461538463, "no_speech_prob": 0.05499684810638428}, {"id": 31, "seek": 16072, "start": 166.8, "end": 167.8, "text": " be all noise.", "tokens": [50668, 312, 439, 5658, 13, 50718], "temperature": 0.0, "avg_logprob": -0.27114728554007933, "compression_ratio": 1.6586538461538463, "no_speech_prob": 0.05499684810638428}, {"id": 32, "seek": 16072, "start": 167.8, "end": 172.16, "text": " So how far through the forward diffusion process.", "tokens": [50718, 407, 577, 1400, 807, 264, 2128, 25242, 1399, 13, 50936], "temperature": 0.0, "avg_logprob": -0.27114728554007933, "compression_ratio": 1.6586538461538463, "no_speech_prob": 0.05499684810638428}, {"id": 33, "seek": 16072, "start": 172.16, "end": 174.72, "text": " So other than that, this is exactly the same equation we've already seen.", "tokens": [50936, 407, 661, 813, 300, 11, 341, 307, 2293, 264, 912, 5367, 321, 600, 1217, 1612, 13, 51064], "temperature": 0.0, "avg_logprob": -0.27114728554007933, "compression_ratio": 1.6586538461538463, "no_speech_prob": 0.05499684810638428}, {"id": 34, "seek": 16072, "start": 174.72, "end": 178.0, "text": " And I realized something else, which is kind of fun, which is you can take the inverse", "tokens": [51064, 400, 286, 5334, 746, 1646, 11, 597, 307, 733, 295, 1019, 11, 597, 307, 291, 393, 747, 264, 17340, 51228], "temperature": 0.0, "avg_logprob": -0.27114728554007933, "compression_ratio": 1.6586538461538463, "no_speech_prob": 0.05499684810638428}, {"id": 35, "seek": 16072, "start": 178.0, "end": 179.4, "text": " of that.", "tokens": [51228, 295, 300, 13, 51298], "temperature": 0.0, "avg_logprob": -0.27114728554007933, "compression_ratio": 1.6586538461538463, "no_speech_prob": 0.05499684810638428}, {"id": 36, "seek": 16072, "start": 179.4, "end": 185.44, "text": " So you can calculate T.", "tokens": [51298, 407, 291, 393, 8873, 314, 13, 51600], "temperature": 0.0, "avg_logprob": -0.27114728554007933, "compression_ratio": 1.6586538461538463, "no_speech_prob": 0.05499684810638428}, {"id": 37, "seek": 18544, "start": 185.44, "end": 198.96, "text": " So we would basically first take the square root, and we would then take the inverse cos,", "tokens": [50364, 407, 321, 576, 1936, 700, 747, 264, 3732, 5593, 11, 293, 321, 576, 550, 747, 264, 17340, 3792, 11, 51040], "temperature": 0.0, "avg_logprob": -0.2550421050100615, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.015420455485582352}, {"id": 38, "seek": 18544, "start": 198.96, "end": 206.66, "text": " and we would then divide by 2 over pi, or times pi over 2.", "tokens": [51040, 293, 321, 576, 550, 9845, 538, 568, 670, 3895, 11, 420, 1413, 3895, 670, 568, 13, 51425], "temperature": 0.0, "avg_logprob": -0.2550421050100615, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.015420455485582352}, {"id": 39, "seek": 18544, "start": 206.66, "end": 212.5, "text": " So we can both, so it's interesting now, we don't, the alpha bar is not something we look", "tokens": [51425, 407, 321, 393, 1293, 11, 370, 309, 311, 1880, 586, 11, 321, 500, 380, 11, 264, 8961, 2159, 307, 406, 746, 321, 574, 51717], "temperature": 0.0, "avg_logprob": -0.2550421050100615, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.015420455485582352}, {"id": 40, "seek": 21250, "start": 212.5, "end": 215.34, "text": " up in a list.", "tokens": [50364, 493, 294, 257, 1329, 13, 50506], "temperature": 0.0, "avg_logprob": -0.2612059363003435, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.002050659852102399}, {"id": 41, "seek": 21250, "start": 215.34, "end": 218.42, "text": " It's something we calculate with a function from a float.", "tokens": [50506, 467, 311, 746, 321, 8873, 365, 257, 2445, 490, 257, 15706, 13, 50660], "temperature": 0.0, "avg_logprob": -0.2612059363003435, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.002050659852102399}, {"id": 42, "seek": 21250, "start": 218.42, "end": 224.3, "text": " And so yeah, interestingly, that means we can also calculate T from an alpha bar.", "tokens": [50660, 400, 370, 1338, 11, 25873, 11, 300, 1355, 321, 393, 611, 8873, 314, 490, 364, 8961, 2159, 13, 50954], "temperature": 0.0, "avg_logprob": -0.2612059363003435, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.002050659852102399}, {"id": 43, "seek": 21250, "start": 224.3, "end": 228.64, "text": " So Noisify has changed a little.", "tokens": [50954, 407, 883, 271, 2505, 575, 3105, 257, 707, 13, 51171], "temperature": 0.0, "avg_logprob": -0.2612059363003435, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.002050659852102399}, {"id": 44, "seek": 21250, "start": 228.64, "end": 234.82, "text": " So now when we get the alpha bar for our time step, we don't look it up.", "tokens": [51171, 407, 586, 562, 321, 483, 264, 8961, 2159, 337, 527, 565, 1823, 11, 321, 500, 380, 574, 309, 493, 13, 51480], "temperature": 0.0, "avg_logprob": -0.2612059363003435, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.002050659852102399}, {"id": 45, "seek": 21250, "start": 234.82, "end": 237.98, "text": " We just call it, call the function.", "tokens": [51480, 492, 445, 818, 309, 11, 818, 264, 2445, 13, 51638], "temperature": 0.0, "avg_logprob": -0.2612059363003435, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.002050659852102399}, {"id": 46, "seek": 23798, "start": 237.98, "end": 244.85999999999999, "text": " And now the time step is a random float between 0 and 1.", "tokens": [50364, 400, 586, 264, 565, 1823, 307, 257, 4974, 15706, 1296, 1958, 293, 502, 13, 50708], "temperature": 0.0, "avg_logprob": -0.3290497790807965, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.042075615376234055}, {"id": 47, "seek": 23798, "start": 244.85999999999999, "end": 247.94, "text": " Actually between 0 and 0.999.", "tokens": [50708, 5135, 1296, 1958, 293, 1958, 13, 49017, 13, 50862], "temperature": 0.0, "avg_logprob": -0.3290497790807965, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.042075615376234055}, {"id": 48, "seek": 23798, "start": 247.94, "end": 252.62, "text": " Which actually I'm sure there's a function I could have chosen to do a float in this", "tokens": [50862, 3013, 767, 286, 478, 988, 456, 311, 257, 2445, 286, 727, 362, 8614, 281, 360, 257, 15706, 294, 341, 51096], "temperature": 0.0, "avg_logprob": -0.3290497790807965, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.042075615376234055}, {"id": 49, "seek": 23798, "start": 252.62, "end": 255.01999999999998, "text": " range, but I just grabbed it because I was lazy.", "tokens": [51096, 3613, 11, 457, 286, 445, 18607, 309, 570, 286, 390, 14847, 13, 51216], "temperature": 0.0, "avg_logprob": -0.3290497790807965, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.042075615376234055}, {"id": 50, "seek": 23798, "start": 255.01999999999998, "end": 258.46, "text": " Couldn't be bothered looking it up.", "tokens": [51216, 35800, 380, 312, 22996, 1237, 309, 493, 13, 51388], "temperature": 0.0, "avg_logprob": -0.3290497790807965, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.042075615376234055}, {"id": 51, "seek": 23798, "start": 258.46, "end": 262.06, "text": " Other than that, Noisify is exactly the same.", "tokens": [51388, 5358, 813, 300, 11, 883, 271, 2505, 307, 2293, 264, 912, 13, 51568], "temperature": 0.0, "avg_logprob": -0.3290497790807965, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.042075615376234055}, {"id": 52, "seek": 26206, "start": 262.06, "end": 270.78000000000003, "text": " So we're still returning the XT, the time step, which is now a float, and the noise.", "tokens": [50364, 407, 321, 434, 920, 12678, 264, 1783, 51, 11, 264, 565, 1823, 11, 597, 307, 586, 257, 15706, 11, 293, 264, 5658, 13, 50800], "temperature": 0.0, "avg_logprob": -0.28596737920021525, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.07053849846124649}, {"id": 53, "seek": 26206, "start": 270.78000000000003, "end": 273.06, "text": " That's the thing we're going to try and predict.", "tokens": [50800, 663, 311, 264, 551, 321, 434, 516, 281, 853, 293, 6069, 13, 50914], "temperature": 0.0, "avg_logprob": -0.28596737920021525, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.07053849846124649}, {"id": 54, "seek": 26206, "start": 273.06, "end": 277.78000000000003, "text": " Dependent variable, this tuple there is our inputs to the model.", "tokens": [50914, 4056, 521, 317, 7006, 11, 341, 2604, 781, 456, 307, 527, 15743, 281, 264, 2316, 13, 51150], "temperature": 0.0, "avg_logprob": -0.28596737920021525, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.07053849846124649}, {"id": 55, "seek": 26206, "start": 277.78000000000003, "end": 282.3, "text": " All right, so here is what that looks like.", "tokens": [51150, 1057, 558, 11, 370, 510, 307, 437, 300, 1542, 411, 13, 51376], "temperature": 0.0, "avg_logprob": -0.28596737920021525, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.07053849846124649}, {"id": 56, "seek": 26206, "start": 282.3, "end": 288.58, "text": " So now when we look at our input to our UNet training process, you can see, you know, we've", "tokens": [51376, 407, 586, 562, 321, 574, 412, 527, 4846, 281, 527, 8229, 302, 3097, 1399, 11, 291, 393, 536, 11, 291, 458, 11, 321, 600, 51690], "temperature": 0.0, "avg_logprob": -0.28596737920021525, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.07053849846124649}, {"id": 57, "seek": 28858, "start": 288.58, "end": 293.94, "text": " got a T of 0.05, so 5% of the way through the forward diffusion process, it looks like", "tokens": [50364, 658, 257, 314, 295, 1958, 13, 13328, 11, 370, 1025, 4, 295, 264, 636, 807, 264, 2128, 25242, 1399, 11, 309, 1542, 411, 50632], "temperature": 0.0, "avg_logprob": -0.3819564592720258, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.23083311319351196}, {"id": 58, "seek": 28858, "start": 293.94, "end": 294.94, "text": " this.", "tokens": [50632, 341, 13, 50682], "temperature": 0.0, "avg_logprob": -0.3819564592720258, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.23083311319351196}, {"id": 59, "seek": 28858, "start": 294.94, "end": 300.06, "text": " And 65% through, it looks like this.", "tokens": [50682, 400, 11624, 4, 807, 11, 309, 1542, 411, 341, 13, 50938], "temperature": 0.0, "avg_logprob": -0.3819564592720258, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.23083311319351196}, {"id": 60, "seek": 28858, "start": 300.06, "end": 308.29999999999995, "text": " So now the time step, and basically the process is more of a kind of a continuous time step", "tokens": [50938, 407, 586, 264, 565, 1823, 11, 293, 1936, 264, 1399, 307, 544, 295, 257, 733, 295, 257, 10957, 565, 1823, 51350], "temperature": 0.0, "avg_logprob": -0.3819564592720258, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.23083311319351196}, {"id": 61, "seek": 28858, "start": 308.29999999999995, "end": 309.7, "text": " and a continuous process.", "tokens": [51350, 293, 257, 10957, 1399, 13, 51420], "temperature": 0.0, "avg_logprob": -0.3819564592720258, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.23083311319351196}, {"id": 62, "seek": 28858, "start": 309.7, "end": 313.18, "text": " Rather before we were having these discrete time steps.", "tokens": [51420, 16571, 949, 321, 645, 1419, 613, 27706, 565, 4439, 13, 51594], "temperature": 0.0, "avg_logprob": -0.3819564592720258, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.23083311319351196}, {"id": 63, "seek": 28858, "start": 313.18, "end": 317.58, "text": " Here we get just any random value that could be between 0 and 1.", "tokens": [51594, 1692, 321, 483, 445, 604, 4974, 2158, 300, 727, 312, 1296, 1958, 293, 502, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3819564592720258, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.23083311319351196}, {"id": 64, "seek": 31758, "start": 317.58, "end": 319.62, "text": " And I think, yeah, that's also something.", "tokens": [50364, 400, 286, 519, 11, 1338, 11, 300, 311, 611, 746, 13, 50466], "temperature": 0.0, "avg_logprob": -0.34145912237926923, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.030208466574549675}, {"id": 65, "seek": 31758, "start": 319.62, "end": 322.62, "text": " Which can be all this more convenient, you know, to have.", "tokens": [50466, 3013, 393, 312, 439, 341, 544, 10851, 11, 291, 458, 11, 281, 362, 13, 50616], "temperature": 0.0, "avg_logprob": -0.34145912237926923, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.030208466574549675}, {"id": 66, "seek": 31758, "start": 322.62, "end": 323.62, "text": " Yeah it is convenient.", "tokens": [50616, 865, 309, 307, 10851, 13, 50666], "temperature": 0.0, "avg_logprob": -0.34145912237926923, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.030208466574549675}, {"id": 67, "seek": 31758, "start": 323.62, "end": 324.62, "text": " To have a function to call.", "tokens": [50666, 1407, 362, 257, 2445, 281, 818, 13, 50716], "temperature": 0.0, "avg_logprob": -0.34145912237926923, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.030208466574549675}, {"id": 68, "seek": 31758, "start": 324.62, "end": 331.14, "text": " Yeah, I find this life a little bit easier.", "tokens": [50716, 865, 11, 286, 915, 341, 993, 257, 707, 857, 3571, 13, 51042], "temperature": 0.0, "avg_logprob": -0.34145912237926923, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.030208466574549675}, {"id": 69, "seek": 31758, "start": 331.14, "end": 338.76, "text": " So the model's the same, the callbacks are the same, the fitting process is the same.", "tokens": [51042, 407, 264, 2316, 311, 264, 912, 11, 264, 818, 17758, 366, 264, 912, 11, 264, 15669, 1399, 307, 264, 912, 13, 51423], "temperature": 0.0, "avg_logprob": -0.34145912237926923, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.030208466574549675}, {"id": 70, "seek": 31758, "start": 338.76, "end": 344.21999999999997, "text": " And so something which is kind of fun is that we could now, and we do now, create a little", "tokens": [51423, 400, 370, 746, 597, 307, 733, 295, 1019, 307, 300, 321, 727, 586, 11, 293, 321, 360, 586, 11, 1884, 257, 707, 51696], "temperature": 0.0, "avg_logprob": -0.34145912237926923, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.030208466574549675}, {"id": 71, "seek": 31758, "start": 344.21999999999997, "end": 346.74, "text": " denoise function.", "tokens": [51696, 1441, 38800, 2445, 13, 51822], "temperature": 0.0, "avg_logprob": -0.34145912237926923, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.030208466574549675}, {"id": 72, "seek": 34674, "start": 346.90000000000003, "end": 354.1, "text": " So we can take, you know, this batch of data that we generated, the noisified data, so", "tokens": [50372, 407, 321, 393, 747, 11, 291, 458, 11, 341, 15245, 295, 1412, 300, 321, 10833, 11, 264, 572, 271, 2587, 1412, 11, 370, 50732], "temperature": 0.0, "avg_logprob": -0.2533337197652677, "compression_ratio": 1.6863905325443787, "no_speech_prob": 8.092365169432014e-05}, {"id": 73, "seek": 34674, "start": 354.1, "end": 356.90000000000003, "text": " here it is again.", "tokens": [50732, 510, 309, 307, 797, 13, 50872], "temperature": 0.0, "avg_logprob": -0.2533337197652677, "compression_ratio": 1.6863905325443787, "no_speech_prob": 8.092365169432014e-05}, {"id": 74, "seek": 34674, "start": 356.90000000000003, "end": 359.34000000000003, "text": " And we can denoise it.", "tokens": [50872, 400, 321, 393, 1441, 38800, 309, 13, 50994], "temperature": 0.0, "avg_logprob": -0.2533337197652677, "compression_ratio": 1.6863905325443787, "no_speech_prob": 8.092365169432014e-05}, {"id": 75, "seek": 34674, "start": 359.34000000000003, "end": 363.94, "text": " So we know the T for each element, obviously.", "tokens": [50994, 407, 321, 458, 264, 314, 337, 1184, 4478, 11, 2745, 13, 51224], "temperature": 0.0, "avg_logprob": -0.2533337197652677, "compression_ratio": 1.6863905325443787, "no_speech_prob": 8.092365169432014e-05}, {"id": 76, "seek": 34674, "start": 363.94, "end": 370.5, "text": " So remember T is different for each element now.", "tokens": [51224, 407, 1604, 314, 307, 819, 337, 1184, 4478, 586, 13, 51552], "temperature": 0.0, "avg_logprob": -0.2533337197652677, "compression_ratio": 1.6863905325443787, "no_speech_prob": 8.092365169432014e-05}, {"id": 77, "seek": 34674, "start": 370.5, "end": 374.26, "text": " And we can therefore calculate the alpha bar for each element.", "tokens": [51552, 400, 321, 393, 4412, 8873, 264, 8961, 2159, 337, 1184, 4478, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2533337197652677, "compression_ratio": 1.6863905325443787, "no_speech_prob": 8.092365169432014e-05}, {"id": 78, "seek": 37426, "start": 374.26, "end": 380.9, "text": " And then we can just undo the noisification to get the denoised version.", "tokens": [50364, 400, 550, 321, 393, 445, 23779, 264, 572, 271, 3774, 281, 483, 264, 1441, 78, 2640, 3037, 13, 50696], "temperature": 0.0, "avg_logprob": -0.24731820316637976, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0073446957394480705}, {"id": 79, "seek": 37426, "start": 380.9, "end": 384.12, "text": " And so if we do that, here's what we get.", "tokens": [50696, 400, 370, 498, 321, 360, 300, 11, 510, 311, 437, 321, 483, 13, 50857], "temperature": 0.0, "avg_logprob": -0.24731820316637976, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0073446957394480705}, {"id": 80, "seek": 37426, "start": 384.12, "end": 385.18, "text": " And so this is great, right?", "tokens": [50857, 400, 370, 341, 307, 869, 11, 558, 30, 50910], "temperature": 0.0, "avg_logprob": -0.24731820316637976, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0073446957394480705}, {"id": 81, "seek": 37426, "start": 385.18, "end": 393.5, "text": " It shows you what actually happens when we run a single step of the model on varyingly", "tokens": [50910, 467, 3110, 291, 437, 767, 2314, 562, 321, 1190, 257, 2167, 1823, 295, 264, 2316, 322, 22984, 356, 51326], "temperature": 0.0, "avg_logprob": -0.24731820316637976, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0073446957394480705}, {"id": 82, "seek": 37426, "start": 393.5, "end": 395.18, "text": " partially noised images.", "tokens": [51326, 18886, 572, 2640, 5267, 13, 51410], "temperature": 0.0, "avg_logprob": -0.24731820316637976, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0073446957394480705}, {"id": 83, "seek": 37426, "start": 395.18, "end": 399.62, "text": " And this is something you don't see very often, because I guess not many people are working", "tokens": [51410, 400, 341, 307, 746, 291, 500, 380, 536, 588, 2049, 11, 570, 286, 2041, 406, 867, 561, 366, 1364, 51632], "temperature": 0.0, "avg_logprob": -0.24731820316637976, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0073446957394480705}, {"id": 84, "seek": 37426, "start": 399.62, "end": 403.02, "text": " in these kind of interactive notebook environments where it's really easy to do this kind of", "tokens": [51632, 294, 613, 733, 295, 15141, 21060, 12388, 689, 309, 311, 534, 1858, 281, 360, 341, 733, 295, 51802], "temperature": 0.0, "avg_logprob": -0.24731820316637976, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0073446957394480705}, {"id": 85, "seek": 37426, "start": 403.02, "end": 404.02, "text": " thing.", "tokens": [51802, 551, 13, 51852], "temperature": 0.0, "avg_logprob": -0.24731820316637976, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0073446957394480705}, {"id": 86, "seek": 40402, "start": 404.78, "end": 408.09999999999997, "text": " So this is really helpful to get a sense of like, okay, if you're 25% of the way through", "tokens": [50402, 407, 341, 307, 534, 4961, 281, 483, 257, 2020, 295, 411, 11, 1392, 11, 498, 291, 434, 3552, 4, 295, 264, 636, 807, 50568], "temperature": 0.0, "avg_logprob": -0.2852761949811663, "compression_ratio": 1.7075471698113207, "no_speech_prob": 0.09805525094270706}, {"id": 87, "seek": 40402, "start": 408.09999999999997, "end": 414.47999999999996, "text": " the forward diffusion process, this is what it looks like when you undo that.", "tokens": [50568, 264, 2128, 25242, 1399, 11, 341, 307, 437, 309, 1542, 411, 562, 291, 23779, 300, 13, 50887], "temperature": 0.0, "avg_logprob": -0.2852761949811663, "compression_ratio": 1.7075471698113207, "no_speech_prob": 0.09805525094270706}, {"id": 88, "seek": 40402, "start": 414.47999999999996, "end": 419.9, "text": " If you're 95% of the way through it, this is what happens when you undo that.", "tokens": [50887, 759, 291, 434, 13420, 4, 295, 264, 636, 807, 309, 11, 341, 307, 437, 2314, 562, 291, 23779, 300, 13, 51158], "temperature": 0.0, "avg_logprob": -0.2852761949811663, "compression_ratio": 1.7075471698113207, "no_speech_prob": 0.09805525094270706}, {"id": 89, "seek": 40402, "start": 419.9, "end": 423.21999999999997, "text": " So you can see here, it's basically like, oh, I don't really know what the hell's going", "tokens": [51158, 407, 291, 393, 536, 510, 11, 309, 311, 1936, 411, 11, 1954, 11, 286, 500, 380, 534, 458, 437, 264, 4921, 311, 516, 51324], "temperature": 0.0, "avg_logprob": -0.2852761949811663, "compression_ratio": 1.7075471698113207, "no_speech_prob": 0.09805525094270706}, {"id": 90, "seek": 40402, "start": 423.21999999999997, "end": 424.21999999999997, "text": " on.", "tokens": [51324, 322, 13, 51374], "temperature": 0.0, "avg_logprob": -0.2852761949811663, "compression_ratio": 1.7075471698113207, "no_speech_prob": 0.09805525094270706}, {"id": 91, "seek": 40402, "start": 424.21999999999997, "end": 427.14, "text": " So at least a noisy mess.", "tokens": [51374, 407, 412, 1935, 257, 24518, 2082, 13, 51520], "temperature": 0.0, "avg_logprob": -0.2852761949811663, "compression_ratio": 1.7075471698113207, "no_speech_prob": 0.09805525094270706}, {"id": 92, "seek": 42714, "start": 427.3, "end": 438.62, "text": " Yeah, I guess my feeling from looking at this is, I'm impressed, you know, like this 45%", "tokens": [50372, 865, 11, 286, 2041, 452, 2633, 490, 1237, 412, 341, 307, 11, 286, 478, 11679, 11, 291, 458, 11, 411, 341, 6905, 4, 50938], "temperature": 0.0, "avg_logprob": -0.3036218775380956, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.06464207172393799}, {"id": 93, "seek": 42714, "start": 438.62, "end": 442.06, "text": " noise thing, it looks all noise to me.", "tokens": [50938, 5658, 551, 11, 309, 1542, 439, 5658, 281, 385, 13, 51110], "temperature": 0.0, "avg_logprob": -0.3036218775380956, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.06464207172393799}, {"id": 94, "seek": 42714, "start": 442.06, "end": 447.86, "text": " It's found the long sleeved top.", "tokens": [51110, 467, 311, 1352, 264, 938, 12931, 937, 1192, 13, 51400], "temperature": 0.0, "avg_logprob": -0.3036218775380956, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.06464207172393799}, {"id": 95, "seek": 42714, "start": 447.86, "end": 450.46, "text": " And yeah, it's actually pretty close to the real one.", "tokens": [51400, 400, 1338, 11, 309, 311, 767, 1238, 1998, 281, 264, 957, 472, 13, 51530], "temperature": 0.0, "avg_logprob": -0.3036218775380956, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.06464207172393799}, {"id": 96, "seek": 42714, "start": 450.46, "end": 454.58, "text": " I looked it up, you might see it later, it's a little bit more of a pattern here, but it", "tokens": [51530, 286, 2956, 309, 493, 11, 291, 1062, 536, 309, 1780, 11, 309, 311, 257, 707, 857, 544, 295, 257, 5102, 510, 11, 457, 309, 51736], "temperature": 0.0, "avg_logprob": -0.3036218775380956, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.06464207172393799}, {"id": 97, "seek": 42714, "start": 454.58, "end": 456.02, "text": " even gives a sense of the pattern.", "tokens": [51736, 754, 2709, 257, 2020, 295, 264, 5102, 13, 51808], "temperature": 0.0, "avg_logprob": -0.3036218775380956, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.06464207172393799}, {"id": 98, "seek": 45602, "start": 456.02, "end": 459.09999999999997, "text": " So it shows you how impressive this is.", "tokens": [50364, 407, 309, 3110, 291, 577, 8992, 341, 307, 13, 50518], "temperature": 0.0, "avg_logprob": -0.3179888889707368, "compression_ratio": 1.4460431654676258, "no_speech_prob": 0.002323024906218052}, {"id": 99, "seek": 45602, "start": 459.09999999999997, "end": 462.82, "text": " So this 35%, you can kind of see there's a shoe there, but it's really picked up the", "tokens": [50518, 407, 341, 6976, 8923, 291, 393, 733, 295, 536, 456, 311, 257, 12796, 456, 11, 457, 309, 311, 534, 6183, 493, 264, 50704], "temperature": 0.0, "avg_logprob": -0.3179888889707368, "compression_ratio": 1.4460431654676258, "no_speech_prob": 0.002323024906218052}, {"id": 100, "seek": 45602, "start": 462.82, "end": 464.34, "text": " shoe nicely.", "tokens": [50704, 12796, 9594, 13, 50780], "temperature": 0.0, "avg_logprob": -0.3179888889707368, "compression_ratio": 1.4460431654676258, "no_speech_prob": 0.002323024906218052}, {"id": 101, "seek": 45602, "start": 464.34, "end": 475.09999999999997, "text": " So these are very impressive models in one step, in my opinion.", "tokens": [50780, 407, 613, 366, 588, 8992, 5245, 294, 472, 1823, 11, 294, 452, 4800, 13, 51318], "temperature": 0.0, "avg_logprob": -0.3179888889707368, "compression_ratio": 1.4460431654676258, "no_speech_prob": 0.002323024906218052}, {"id": 102, "seek": 47510, "start": 475.1, "end": 488.22, "text": " So okay, so sampling is basically the same, except now, rather than using the range function", "tokens": [50364, 407, 1392, 11, 370, 21179, 307, 1936, 264, 912, 11, 3993, 586, 11, 2831, 813, 1228, 264, 3613, 2445, 51020], "temperature": 0.0, "avg_logprob": -0.33699452599813773, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.045333459973335266}, {"id": 103, "seek": 47510, "start": 488.22, "end": 491.34000000000003, "text": " to create a time steps, we use lin space to create our time steps.", "tokens": [51020, 281, 1884, 257, 565, 4439, 11, 321, 764, 22896, 1901, 281, 1884, 527, 565, 4439, 13, 51176], "temperature": 0.0, "avg_logprob": -0.33699452599813773, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.045333459973335266}, {"id": 104, "seek": 47510, "start": 491.34000000000003, "end": 497.90000000000003, "text": " So our time steps start at, you know, if we did 1000, it would be 0.999, and they end", "tokens": [51176, 407, 527, 565, 4439, 722, 412, 11, 291, 458, 11, 498, 321, 630, 9714, 11, 309, 576, 312, 1958, 13, 49017, 11, 293, 436, 917, 51504], "temperature": 0.0, "avg_logprob": -0.33699452599813773, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.045333459973335266}, {"id": 105, "seek": 47510, "start": 497.90000000000003, "end": 504.62, "text": " at zero, and then they're just linearly spaced with this number of steps.", "tokens": [51504, 412, 4018, 11, 293, 550, 436, 434, 445, 43586, 43766, 365, 341, 1230, 295, 4439, 13, 51840], "temperature": 0.0, "avg_logprob": -0.33699452599813773, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.045333459973335266}, {"id": 106, "seek": 50462, "start": 505.14, "end": 513.22, "text": " So other than that, you know, A bar, we now calculate, and the next A bar is going to", "tokens": [50390, 407, 661, 813, 300, 11, 291, 458, 11, 316, 2159, 11, 321, 586, 8873, 11, 293, 264, 958, 316, 2159, 307, 516, 281, 50794], "temperature": 0.0, "avg_logprob": -0.28125389762546704, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.002631604904308915}, {"id": 107, "seek": 50462, "start": 513.22, "end": 518.82, "text": " be whatever the current step is, minus whatever step.", "tokens": [50794, 312, 2035, 264, 2190, 1823, 307, 11, 3175, 2035, 1823, 13, 51074], "temperature": 0.0, "avg_logprob": -0.28125389762546704, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.002631604904308915}, {"id": 108, "seek": 50462, "start": 518.82, "end": 522.98, "text": " So if you're doing 100 steps, then you'd be minus 0.01.", "tokens": [51074, 407, 498, 291, 434, 884, 2319, 4439, 11, 550, 291, 1116, 312, 3175, 1958, 13, 10607, 13, 51282], "temperature": 0.0, "avg_logprob": -0.28125389762546704, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.002631604904308915}, {"id": 109, "seek": 50462, "start": 522.98, "end": 529.82, "text": " So this is just stepping through linearly.", "tokens": [51282, 407, 341, 307, 445, 16821, 807, 43586, 13, 51624], "temperature": 0.0, "avg_logprob": -0.28125389762546704, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.002631604904308915}, {"id": 110, "seek": 52982, "start": 529.82, "end": 535.38, "text": " And yeah, that's actually it for changes.", "tokens": [50364, 400, 1338, 11, 300, 311, 767, 309, 337, 2962, 13, 50642], "temperature": 0.0, "avg_logprob": -0.2396523774559818, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.11917401105165482}, {"id": 111, "seek": 52982, "start": 535.38, "end": 544.0600000000001, "text": " So if we just do DDIM for 100 steps, you know, that works really well.", "tokens": [50642, 407, 498, 321, 445, 360, 413, 3085, 44, 337, 2319, 4439, 11, 291, 458, 11, 300, 1985, 534, 731, 13, 51076], "temperature": 0.0, "avg_logprob": -0.2396523774559818, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.11917401105165482}, {"id": 112, "seek": 52982, "start": 544.0600000000001, "end": 553.1800000000001, "text": " We get a fit of three, which is actually quite a bit better than we had on 100 steps for", "tokens": [51076, 492, 483, 257, 3318, 295, 1045, 11, 597, 307, 767, 1596, 257, 857, 1101, 813, 321, 632, 322, 2319, 4439, 337, 51532], "temperature": 0.0, "avg_logprob": -0.2396523774559818, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.11917401105165482}, {"id": 113, "seek": 52982, "start": 553.1800000000001, "end": 554.4200000000001, "text": " our previous DDIM.", "tokens": [51532, 527, 3894, 413, 3085, 44, 13, 51594], "temperature": 0.0, "avg_logprob": -0.2396523774559818, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.11917401105165482}, {"id": 114, "seek": 55442, "start": 554.42, "end": 561.78, "text": " So this definitely seems like a good sampling approach.", "tokens": [50364, 407, 341, 2138, 2544, 411, 257, 665, 21179, 3109, 13, 50732], "temperature": 0.0, "avg_logprob": -0.29151333188547673, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.26223036646842957}, {"id": 115, "seek": 55442, "start": 561.78, "end": 566.6999999999999, "text": " And I know Jono is going to talk a bit more shortly about, you know, some of the things", "tokens": [50732, 400, 286, 458, 7745, 78, 307, 516, 281, 751, 257, 857, 544, 13392, 466, 11, 291, 458, 11, 512, 295, 264, 721, 50978], "temperature": 0.0, "avg_logprob": -0.29151333188547673, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.26223036646842957}, {"id": 116, "seek": 55442, "start": 566.6999999999999, "end": 568.5, "text": " that can make better sampling approaches.", "tokens": [50978, 300, 393, 652, 1101, 21179, 11587, 13, 51068], "temperature": 0.0, "avg_logprob": -0.29151333188547673, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.26223036646842957}, {"id": 117, "seek": 55442, "start": 568.5, "end": 573.42, "text": " But yeah, definitely, we can see it making a difference here.", "tokens": [51068, 583, 1338, 11, 2138, 11, 321, 393, 536, 309, 1455, 257, 2649, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.29151333188547673, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.26223036646842957}, {"id": 118, "seek": 55442, "start": 573.42, "end": 577.9, "text": " Did you guys have anything you wanted to say about this before we move on?", "tokens": [51314, 2589, 291, 1074, 362, 1340, 291, 1415, 281, 584, 466, 341, 949, 321, 1286, 322, 30, 51538], "temperature": 0.0, "avg_logprob": -0.29151333188547673, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.26223036646842957}, {"id": 119, "seek": 55442, "start": 577.9, "end": 582.98, "text": " No, but it is a nice transition towards some of the other things we'll be looking at to", "tokens": [51538, 883, 11, 457, 309, 307, 257, 1481, 6034, 3030, 512, 295, 264, 661, 721, 321, 603, 312, 1237, 412, 281, 51792], "temperature": 0.0, "avg_logprob": -0.29151333188547673, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.26223036646842957}, {"id": 120, "seek": 58298, "start": 582.98, "end": 585.1800000000001, "text": " start thinking about how do we frame this.", "tokens": [50364, 722, 1953, 466, 577, 360, 321, 3920, 341, 13, 50474], "temperature": 0.0, "avg_logprob": -0.3285767731843171, "compression_ratio": 1.6181229773462784, "no_speech_prob": 0.80040043592453}, {"id": 121, "seek": 58298, "start": 585.1800000000001, "end": 588.14, "text": " And it's also good, like the idea.", "tokens": [50474, 400, 309, 311, 611, 665, 11, 411, 264, 1558, 13, 50622], "temperature": 0.0, "avg_logprob": -0.3285767731843171, "compression_ratio": 1.6181229773462784, "no_speech_prob": 0.80040043592453}, {"id": 122, "seek": 58298, "start": 588.14, "end": 593.58, "text": " So the original DDPM paper has this 1000 time steps, and a lot of people follow that.", "tokens": [50622, 407, 264, 3380, 30778, 18819, 3035, 575, 341, 9714, 565, 4439, 11, 293, 257, 688, 295, 561, 1524, 300, 13, 50894], "temperature": 0.0, "avg_logprob": -0.3285767731843171, "compression_ratio": 1.6181229773462784, "no_speech_prob": 0.80040043592453}, {"id": 123, "seek": 58298, "start": 593.58, "end": 597.22, "text": " But the idea that you don't have to be bound to that, and maybe it is worth breaking that", "tokens": [50894, 583, 264, 1558, 300, 291, 500, 380, 362, 281, 312, 5472, 281, 300, 11, 293, 1310, 309, 307, 3163, 7697, 300, 51076], "temperature": 0.0, "avg_logprob": -0.3285767731843171, "compression_ratio": 1.6181229773462784, "no_speech_prob": 0.80040043592453}, {"id": 124, "seek": 58298, "start": 597.22, "end": 598.22, "text": " convention.", "tokens": [51076, 10286, 13, 51126], "temperature": 0.0, "avg_logprob": -0.3285767731843171, "compression_ratio": 1.6181229773462784, "no_speech_prob": 0.80040043592453}, {"id": 125, "seek": 58298, "start": 598.22, "end": 601.46, "text": " I know Tanish made that meme about, you know, there's 15 competing different standards for", "tokens": [51126, 286, 458, 314, 7524, 1027, 300, 21701, 466, 11, 291, 458, 11, 456, 311, 2119, 15439, 819, 7787, 337, 51288], "temperature": 0.0, "avg_logprob": -0.3285767731843171, "compression_ratio": 1.6181229773462784, "no_speech_prob": 0.80040043592453}, {"id": 126, "seek": 58298, "start": 601.46, "end": 602.46, "text": " notation.", "tokens": [51288, 24657, 13, 51338], "temperature": 0.0, "avg_logprob": -0.3285767731843171, "compression_ratio": 1.6181229773462784, "no_speech_prob": 0.80040043592453}, {"id": 127, "seek": 58298, "start": 602.46, "end": 605.14, "text": " But yeah, sometimes it's helpful to reframe it.", "tokens": [51338, 583, 1338, 11, 2171, 309, 311, 4961, 281, 13334, 529, 309, 13, 51472], "temperature": 0.0, "avg_logprob": -0.3285767731843171, "compression_ratio": 1.6181229773462784, "no_speech_prob": 0.80040043592453}, {"id": 128, "seek": 58298, "start": 605.14, "end": 610.1, "text": " Okay, time goes from 0 to 1, that can simplify some things, maybe complicates others.", "tokens": [51472, 1033, 11, 565, 1709, 490, 1958, 281, 502, 11, 300, 393, 20460, 512, 721, 11, 1310, 16060, 1024, 2357, 13, 51720], "temperature": 0.0, "avg_logprob": -0.3285767731843171, "compression_ratio": 1.6181229773462784, "no_speech_prob": 0.80040043592453}, {"id": 129, "seek": 61010, "start": 610.22, "end": 613.14, "text": " But yeah, it's nice to think how you can reframe stuff sometimes.", "tokens": [50370, 583, 1338, 11, 309, 311, 1481, 281, 519, 577, 291, 393, 13334, 529, 1507, 2171, 13, 50516], "temperature": 0.0, "avg_logprob": -0.3470833655631188, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.12076675891876221}, {"id": 130, "seek": 61010, "start": 613.14, "end": 619.1800000000001, "text": " Yeah, and in fact, where we will head today, by the time we get to notebook 23, we will", "tokens": [50516, 865, 11, 293, 294, 1186, 11, 689, 321, 486, 1378, 965, 11, 538, 264, 565, 321, 483, 281, 21060, 6673, 11, 321, 486, 50818], "temperature": 0.0, "avg_logprob": -0.3470833655631188, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.12076675891876221}, {"id": 131, "seek": 61010, "start": 619.1800000000001, "end": 624.58, "text": " see, you know, even simpler notation.", "tokens": [50818, 536, 11, 291, 458, 11, 754, 18587, 24657, 13, 51088], "temperature": 0.0, "avg_logprob": -0.3470833655631188, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.12076675891876221}, {"id": 132, "seek": 61010, "start": 624.58, "end": 626.22, "text": " And yeah, simpler notation generally comes.", "tokens": [51088, 400, 1338, 11, 18587, 24657, 5101, 1487, 13, 51170], "temperature": 0.0, "avg_logprob": -0.3470833655631188, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.12076675891876221}, {"id": 133, "seek": 61010, "start": 626.22, "end": 633.66, "text": " I think what happens is over time, people understand better what's the essence of the", "tokens": [51170, 286, 519, 437, 2314, 307, 670, 565, 11, 561, 1223, 1101, 437, 311, 264, 12801, 295, 264, 51542], "temperature": 0.0, "avg_logprob": -0.3470833655631188, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.12076675891876221}, {"id": 134, "seek": 61010, "start": 633.66, "end": 638.1, "text": " problem and the approach, and then that gets reflected in the notation.", "tokens": [51542, 1154, 293, 264, 3109, 11, 293, 550, 300, 2170, 15502, 294, 264, 24657, 13, 51764], "temperature": 0.0, "avg_logprob": -0.3470833655631188, "compression_ratio": 1.6443514644351465, "no_speech_prob": 0.12076675891876221}, {"id": 135, "seek": 63810, "start": 639.1, "end": 647.14, "text": " So, okay, so the next part I wanted to share is something which is an idea we've been working", "tokens": [50414, 407, 11, 1392, 11, 370, 264, 958, 644, 286, 1415, 281, 2073, 307, 746, 597, 307, 364, 1558, 321, 600, 668, 1364, 50816], "temperature": 0.0, "avg_logprob": -0.3044286241718367, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.010983917862176895}, {"id": 136, "seek": 63810, "start": 647.14, "end": 649.58, "text": " on for a while.", "tokens": [50816, 322, 337, 257, 1339, 13, 50938], "temperature": 0.0, "avg_logprob": -0.3044286241718367, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.010983917862176895}, {"id": 137, "seek": 63810, "start": 649.58, "end": 651.58, "text": " And it's some new research.", "tokens": [50938, 400, 309, 311, 512, 777, 2132, 13, 51038], "temperature": 0.0, "avg_logprob": -0.3044286241718367, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.010983917862176895}, {"id": 138, "seek": 63810, "start": 651.58, "end": 658.62, "text": " So partly, I guess this is an interesting like insight into how we do research.", "tokens": [51038, 407, 17031, 11, 286, 2041, 341, 307, 364, 1880, 411, 11269, 666, 577, 321, 360, 2132, 13, 51390], "temperature": 0.0, "avg_logprob": -0.3044286241718367, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.010983917862176895}, {"id": 139, "seek": 63810, "start": 658.62, "end": 660.9, "text": " So this is 22 noise pred.", "tokens": [51390, 407, 341, 307, 5853, 5658, 3852, 13, 51504], "temperature": 0.0, "avg_logprob": -0.3044286241718367, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.010983917862176895}, {"id": 140, "seek": 63810, "start": 660.9, "end": 665.14, "text": " And the basic idea of this was, well, actually, I'm going to take you through it to see what", "tokens": [51504, 400, 264, 3875, 1558, 295, 341, 390, 11, 731, 11, 767, 11, 286, 478, 516, 281, 747, 291, 807, 309, 281, 536, 437, 51716], "temperature": 0.0, "avg_logprob": -0.3044286241718367, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.010983917862176895}, {"id": 141, "seek": 63810, "start": 665.14, "end": 666.5, "text": " the basic idea is.", "tokens": [51716, 264, 3875, 1558, 307, 13, 51784], "temperature": 0.0, "avg_logprob": -0.3044286241718367, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.010983917862176895}, {"id": 142, "seek": 66650, "start": 666.5, "end": 673.06, "text": " So what I'm going to do is I'm going to create, okay, so fashion MNIST as before.", "tokens": [50364, 407, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 1884, 11, 1392, 11, 370, 6700, 376, 45, 19756, 382, 949, 13, 50692], "temperature": 0.0, "avg_logprob": -0.2718086078249175, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.0037061506882309914}, {"id": 143, "seek": 66650, "start": 673.06, "end": 675.22, "text": " But I'm going to create a different kind of model.", "tokens": [50692, 583, 286, 478, 516, 281, 1884, 257, 819, 733, 295, 2316, 13, 50800], "temperature": 0.0, "avg_logprob": -0.2718086078249175, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.0037061506882309914}, {"id": 144, "seek": 66650, "start": 675.22, "end": 683.74, "text": " I'm not going to create a model that predicts the noise, given the noised image in T. Instead,", "tokens": [50800, 286, 478, 406, 516, 281, 1884, 257, 2316, 300, 6069, 82, 264, 5658, 11, 2212, 264, 572, 2640, 3256, 294, 314, 13, 7156, 11, 51226], "temperature": 0.0, "avg_logprob": -0.2718086078249175, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.0037061506882309914}, {"id": 145, "seek": 66650, "start": 683.74, "end": 690.4, "text": " I'm going to try to create a model which predicts T, given the noised image.", "tokens": [51226, 286, 478, 516, 281, 853, 281, 1884, 257, 2316, 597, 6069, 82, 314, 11, 2212, 264, 572, 2640, 3256, 13, 51559], "temperature": 0.0, "avg_logprob": -0.2718086078249175, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.0037061506882309914}, {"id": 146, "seek": 66650, "start": 690.4, "end": 692.1, "text": " So why did I want to do that?", "tokens": [51559, 407, 983, 630, 286, 528, 281, 360, 300, 30, 51644], "temperature": 0.0, "avg_logprob": -0.2718086078249175, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.0037061506882309914}, {"id": 147, "seek": 66650, "start": 692.1, "end": 694.9, "text": " Well, partly, well, entirely, because I was curious.", "tokens": [51644, 1042, 11, 17031, 11, 731, 11, 7696, 11, 570, 286, 390, 6369, 13, 51784], "temperature": 0.0, "avg_logprob": -0.2718086078249175, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.0037061506882309914}, {"id": 148, "seek": 69490, "start": 695.3, "end": 703.54, "text": " I felt like when I looked at something like this, I thought it was pretty obvious, roughly", "tokens": [50384, 286, 2762, 411, 562, 286, 2956, 412, 746, 411, 341, 11, 286, 1194, 309, 390, 1238, 6322, 11, 9810, 50796], "temperature": 0.0, "avg_logprob": -0.24304805712753466, "compression_ratio": 1.7079207920792079, "no_speech_prob": 0.006797317881137133}, {"id": 149, "seek": 69490, "start": 703.54, "end": 706.98, "text": " how much noise each image had.", "tokens": [50796, 577, 709, 5658, 1184, 3256, 632, 13, 50968], "temperature": 0.0, "avg_logprob": -0.24304805712753466, "compression_ratio": 1.7079207920792079, "no_speech_prob": 0.006797317881137133}, {"id": 150, "seek": 69490, "start": 706.98, "end": 713.02, "text": " And so I thought, why are we passing noise when we call the model?", "tokens": [50968, 400, 370, 286, 1194, 11, 983, 366, 321, 8437, 5658, 562, 321, 818, 264, 2316, 30, 51270], "temperature": 0.0, "avg_logprob": -0.24304805712753466, "compression_ratio": 1.7079207920792079, "no_speech_prob": 0.006797317881137133}, {"id": 151, "seek": 69490, "start": 713.02, "end": 717.3199999999999, "text": " Why are we passing in the noised image and the amount of noise or the T?", "tokens": [51270, 1545, 366, 321, 8437, 294, 264, 572, 2640, 3256, 293, 264, 2372, 295, 5658, 420, 264, 314, 30, 51485], "temperature": 0.0, "avg_logprob": -0.24304805712753466, "compression_ratio": 1.7079207920792079, "no_speech_prob": 0.006797317881137133}, {"id": 152, "seek": 69490, "start": 717.3199999999999, "end": 720.1, "text": " Given that I would have thought the model could figure out how much noise there is.", "tokens": [51485, 18600, 300, 286, 576, 362, 1194, 264, 2316, 727, 2573, 484, 577, 709, 5658, 456, 307, 13, 51624], "temperature": 0.0, "avg_logprob": -0.24304805712753466, "compression_ratio": 1.7079207920792079, "no_speech_prob": 0.006797317881137133}, {"id": 153, "seek": 72010, "start": 720.1, "end": 724.7, "text": " So I wanted to check my intention, which is that the model could figure out how much", "tokens": [50364, 407, 286, 1415, 281, 1520, 452, 7789, 11, 597, 307, 300, 264, 2316, 727, 2573, 484, 577, 709, 50594], "temperature": 0.0, "avg_logprob": -0.2550972858107234, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.004829290322959423}, {"id": 154, "seek": 72010, "start": 724.7, "end": 725.7, "text": " noise there is.", "tokens": [50594, 5658, 456, 307, 13, 50644], "temperature": 0.0, "avg_logprob": -0.2550972858107234, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.004829290322959423}, {"id": 155, "seek": 72010, "start": 725.7, "end": 728.78, "text": " So I thought, okay, well, let's create a model that would try and figure out how much noise", "tokens": [50644, 407, 286, 1194, 11, 1392, 11, 731, 11, 718, 311, 1884, 257, 2316, 300, 576, 853, 293, 2573, 484, 577, 709, 5658, 50798], "temperature": 0.0, "avg_logprob": -0.2550972858107234, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.004829290322959423}, {"id": 156, "seek": 72010, "start": 728.78, "end": 731.98, "text": " there is.", "tokens": [50798, 456, 307, 13, 50958], "temperature": 0.0, "avg_logprob": -0.2550972858107234, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.004829290322959423}, {"id": 157, "seek": 72010, "start": 731.98, "end": 734.02, "text": " So I created a different noisify now.", "tokens": [50958, 407, 286, 2942, 257, 819, 572, 271, 2505, 586, 13, 51060], "temperature": 0.0, "avg_logprob": -0.2550972858107234, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.004829290322959423}, {"id": 158, "seek": 72010, "start": 734.02, "end": 745.5, "text": " And this noisify grabs an alpha bar T randomly.", "tokens": [51060, 400, 341, 572, 271, 2505, 30028, 364, 8961, 2159, 314, 16979, 13, 51634], "temperature": 0.0, "avg_logprob": -0.2550972858107234, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.004829290322959423}, {"id": 159, "seek": 74550, "start": 745.5, "end": 750.1, "text": " And it's just a random number between 0 and 1.", "tokens": [50364, 400, 309, 311, 445, 257, 4974, 1230, 1296, 1958, 293, 502, 13, 50594], "temperature": 0.0, "avg_logprob": -0.2774064964223131, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.22808879613876343}, {"id": 160, "seek": 74550, "start": 750.1, "end": 755.1, "text": " You don't want 1 per item in the batch.", "tokens": [50594, 509, 500, 380, 528, 502, 680, 3174, 294, 264, 15245, 13, 50844], "temperature": 0.0, "avg_logprob": -0.2774064964223131, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.22808879613876343}, {"id": 161, "seek": 74550, "start": 755.1, "end": 760.06, "text": " And so then after just randomly grabbing an alpha bar T, we then noisify in the usual", "tokens": [50844, 400, 370, 550, 934, 445, 16979, 23771, 364, 8961, 2159, 314, 11, 321, 550, 572, 271, 2505, 294, 264, 7713, 51092], "temperature": 0.0, "avg_logprob": -0.2774064964223131, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.22808879613876343}, {"id": 162, "seek": 74550, "start": 760.06, "end": 761.1, "text": " way.", "tokens": [51092, 636, 13, 51144], "temperature": 0.0, "avg_logprob": -0.2774064964223131, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.22808879613876343}, {"id": 163, "seek": 74550, "start": 761.1, "end": 765.9, "text": " But now our independent variable is the noised image, and the dependent variable is alpha", "tokens": [51144, 583, 586, 527, 6695, 7006, 307, 264, 572, 2640, 3256, 11, 293, 264, 12334, 7006, 307, 8961, 51384], "temperature": 0.0, "avg_logprob": -0.2774064964223131, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.22808879613876343}, {"id": 164, "seek": 74550, "start": 765.9, "end": 766.9, "text": " bar T.", "tokens": [51384, 2159, 314, 13, 51434], "temperature": 0.0, "avg_logprob": -0.2774064964223131, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.22808879613876343}, {"id": 165, "seek": 74550, "start": 766.9, "end": 770.98, "text": " And so we're going to try to create a model that can predict alpha bar T, given a noised", "tokens": [51434, 400, 370, 321, 434, 516, 281, 853, 281, 1884, 257, 2316, 300, 393, 6069, 8961, 2159, 314, 11, 2212, 257, 572, 2640, 51638], "temperature": 0.0, "avg_logprob": -0.2774064964223131, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.22808879613876343}, {"id": 166, "seek": 74550, "start": 770.98, "end": 771.98, "text": " image.", "tokens": [51638, 3256, 13, 51688], "temperature": 0.0, "avg_logprob": -0.2774064964223131, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.22808879613876343}, {"id": 167, "seek": 77198, "start": 771.98, "end": 777.3000000000001, "text": " Okay, so everything else is the same as usual.", "tokens": [50364, 1033, 11, 370, 1203, 1646, 307, 264, 912, 382, 7713, 13, 50630], "temperature": 0.0, "avg_logprob": -0.2685574849446615, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0726291835308075}, {"id": 168, "seek": 77198, "start": 777.3000000000001, "end": 779.66, "text": " And so we can see an example.", "tokens": [50630, 400, 370, 321, 393, 536, 364, 1365, 13, 50748], "temperature": 0.0, "avg_logprob": -0.2685574849446615, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0726291835308075}, {"id": 169, "seek": 77198, "start": 779.66, "end": 782.9, "text": " You've got alpha bar T dot squeeze dot log it.", "tokens": [50748, 509, 600, 658, 8961, 2159, 314, 5893, 13578, 5893, 3565, 309, 13, 50910], "temperature": 0.0, "avg_logprob": -0.2685574849446615, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0726291835308075}, {"id": 170, "seek": 77198, "start": 782.9, "end": 784.38, "text": " Oh, yeah, that's true.", "tokens": [50910, 876, 11, 1338, 11, 300, 311, 2074, 13, 50984], "temperature": 0.0, "avg_logprob": -0.2685574849446615, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0726291835308075}, {"id": 171, "seek": 77198, "start": 784.38, "end": 789.46, "text": " So the alpha bar T goes between 0 and 1.", "tokens": [50984, 407, 264, 8961, 2159, 314, 1709, 1296, 1958, 293, 502, 13, 51238], "temperature": 0.0, "avg_logprob": -0.2685574849446615, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0726291835308075}, {"id": 172, "seek": 77198, "start": 789.46, "end": 791.34, "text": " So we've got a choice.", "tokens": [51238, 407, 321, 600, 658, 257, 3922, 13, 51332], "temperature": 0.0, "avg_logprob": -0.2685574849446615, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0726291835308075}, {"id": 173, "seek": 77198, "start": 791.34, "end": 793.1, "text": " Like I mean, we don't have to do anything.", "tokens": [51332, 1743, 286, 914, 11, 321, 500, 380, 362, 281, 360, 1340, 13, 51420], "temperature": 0.0, "avg_logprob": -0.2685574849446615, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0726291835308075}, {"id": 174, "seek": 77198, "start": 793.1, "end": 796.7, "text": " But you know, normally, if you've got something between 0 and 1, you might consider putting", "tokens": [51420, 583, 291, 458, 11, 5646, 11, 498, 291, 600, 658, 746, 1296, 1958, 293, 502, 11, 291, 1062, 1949, 3372, 51600], "temperature": 0.0, "avg_logprob": -0.2685574849446615, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0726291835308075}, {"id": 175, "seek": 77198, "start": 796.7, "end": 799.9, "text": " a sigmoid at the end of your model.", "tokens": [51600, 257, 4556, 3280, 327, 412, 264, 917, 295, 428, 2316, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2685574849446615, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0726291835308075}, {"id": 176, "seek": 79990, "start": 799.9, "end": 809.38, "text": " But I felt like the difference between 0.999 and 0.99 is very significant, you know.", "tokens": [50364, 583, 286, 2762, 411, 264, 2649, 1296, 1958, 13, 49017, 293, 1958, 13, 8494, 307, 588, 4776, 11, 291, 458, 13, 50838], "temperature": 0.0, "avg_logprob": -0.31829882777014445, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.04023232311010361}, {"id": 177, "seek": 79990, "start": 809.38, "end": 814.18, "text": " So if we do log it, then we don't need the sigmoid at the end anymore.", "tokens": [50838, 407, 498, 321, 360, 3565, 309, 11, 550, 321, 500, 380, 643, 264, 4556, 3280, 327, 412, 264, 917, 3602, 13, 51078], "temperature": 0.0, "avg_logprob": -0.31829882777014445, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.04023232311010361}, {"id": 178, "seek": 79990, "start": 814.18, "end": 820.1, "text": " It'll naturally cover the full range of kind of, you know, it'll be centered at zero, it'll", "tokens": [51078, 467, 603, 8195, 2060, 264, 1577, 3613, 295, 733, 295, 11, 291, 458, 11, 309, 603, 312, 18988, 412, 4018, 11, 309, 603, 51374], "temperature": 0.0, "avg_logprob": -0.31829882777014445, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.04023232311010361}, {"id": 179, "seek": 79990, "start": 820.1, "end": 823.4599999999999, "text": " cover all the other normal kind of range of numbers.", "tokens": [51374, 2060, 439, 264, 661, 2710, 733, 295, 3613, 295, 3547, 13, 51542], "temperature": 0.0, "avg_logprob": -0.31829882777014445, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.04023232311010361}, {"id": 180, "seek": 82346, "start": 823.46, "end": 830.26, "text": " And it also will treat, you know, equal ratios as equally important at both ends of the spectrum.", "tokens": [50364, 400, 309, 611, 486, 2387, 11, 291, 458, 11, 2681, 32435, 382, 12309, 1021, 412, 1293, 5314, 295, 264, 11143, 13, 50704], "temperature": 0.0, "avg_logprob": -0.23292700699933871, "compression_ratio": 1.7089041095890412, "no_speech_prob": 0.41850775480270386}, {"id": 181, "seek": 82346, "start": 830.26, "end": 834.14, "text": " So that was my hypothesis, was that using log it would be better.", "tokens": [50704, 407, 300, 390, 452, 17291, 11, 390, 300, 1228, 3565, 309, 576, 312, 1101, 13, 50898], "temperature": 0.0, "avg_logprob": -0.23292700699933871, "compression_ratio": 1.7089041095890412, "no_speech_prob": 0.41850775480270386}, {"id": 182, "seek": 82346, "start": 834.14, "end": 837.4000000000001, "text": " I did test it, and it was actually very dramatically better.", "tokens": [50898, 286, 630, 1500, 309, 11, 293, 309, 390, 767, 588, 17548, 1101, 13, 51061], "temperature": 0.0, "avg_logprob": -0.23292700699933871, "compression_ratio": 1.7089041095890412, "no_speech_prob": 0.41850775480270386}, {"id": 183, "seek": 82346, "start": 837.4000000000001, "end": 841.1800000000001, "text": " So without this log it here, my model didn't work well at all.", "tokens": [51061, 407, 1553, 341, 3565, 309, 510, 11, 452, 2316, 994, 380, 589, 731, 412, 439, 13, 51250], "temperature": 0.0, "avg_logprob": -0.23292700699933871, "compression_ratio": 1.7089041095890412, "no_speech_prob": 0.41850775480270386}, {"id": 184, "seek": 82346, "start": 841.1800000000001, "end": 846.26, "text": " And so this is like an example of where thinking about these details is really important.", "tokens": [51250, 400, 370, 341, 307, 411, 364, 1365, 295, 689, 1953, 466, 613, 4365, 307, 534, 1021, 13, 51504], "temperature": 0.0, "avg_logprob": -0.23292700699933871, "compression_ratio": 1.7089041095890412, "no_speech_prob": 0.41850775480270386}, {"id": 185, "seek": 82346, "start": 846.26, "end": 850.0400000000001, "text": " Because if I hadn't have done this, then I would have come away from this bit of research", "tokens": [51504, 1436, 498, 286, 8782, 380, 362, 1096, 341, 11, 550, 286, 576, 362, 808, 1314, 490, 341, 857, 295, 2132, 51693], "temperature": 0.0, "avg_logprob": -0.23292700699933871, "compression_ratio": 1.7089041095890412, "no_speech_prob": 0.41850775480270386}, {"id": 186, "seek": 82346, "start": 850.0400000000001, "end": 851.34, "text": " thinking like, oh, I was wrong.", "tokens": [51693, 1953, 411, 11, 1954, 11, 286, 390, 2085, 13, 51758], "temperature": 0.0, "avg_logprob": -0.23292700699933871, "compression_ratio": 1.7089041095890412, "no_speech_prob": 0.41850775480270386}, {"id": 187, "seek": 85134, "start": 851.38, "end": 853.58, "text": " You can't predict noise amount.", "tokens": [50366, 509, 393, 380, 6069, 5658, 2372, 13, 50476], "temperature": 0.0, "avg_logprob": -0.38636510059086965, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.12417296320199966}, {"id": 188, "seek": 85134, "start": 853.58, "end": 857.58, "text": " Yeah, so thanks for pointing that out, Chana.", "tokens": [50476, 865, 11, 370, 3231, 337, 12166, 300, 484, 11, 761, 2095, 13, 50676], "temperature": 0.0, "avg_logprob": -0.38636510059086965, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.12417296320199966}, {"id": 189, "seek": 85134, "start": 857.58, "end": 863.6600000000001, "text": " Yeah, so that's why in this example of a mini-batch, you can see that the numbers can be negative", "tokens": [50676, 865, 11, 370, 300, 311, 983, 294, 341, 1365, 295, 257, 8382, 12, 65, 852, 11, 291, 393, 536, 300, 264, 3547, 393, 312, 3671, 50980], "temperature": 0.0, "avg_logprob": -0.38636510059086965, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.12417296320199966}, {"id": 190, "seek": 85134, "start": 863.6600000000001, "end": 864.6600000000001, "text": " or positive.", "tokens": [50980, 420, 3353, 13, 51030], "temperature": 0.0, "avg_logprob": -0.38636510059086965, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.12417296320199966}, {"id": 191, "seek": 85134, "start": 864.6600000000001, "end": 869.3000000000001, "text": " So zero would represent noise, alpha bar of 0.5.", "tokens": [51030, 407, 4018, 576, 2906, 5658, 11, 8961, 2159, 295, 1958, 13, 20, 13, 51262], "temperature": 0.0, "avg_logprob": -0.38636510059086965, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.12417296320199966}, {"id": 192, "seek": 85134, "start": 869.3000000000001, "end": 873.3000000000001, "text": " So here 3.05 is not very noised at all.", "tokens": [51262, 407, 510, 805, 13, 13328, 307, 406, 588, 572, 271, 292, 412, 439, 13, 51462], "temperature": 0.0, "avg_logprob": -0.38636510059086965, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.12417296320199966}, {"id": 193, "seek": 85134, "start": 873.3000000000001, "end": 878.86, "text": " Where else, negative one is pretty noisy.", "tokens": [51462, 2305, 1646, 11, 3671, 472, 307, 1238, 24518, 13, 51740], "temperature": 0.0, "avg_logprob": -0.38636510059086965, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.12417296320199966}, {"id": 194, "seek": 87886, "start": 878.86, "end": 883.98, "text": " So the idea is that, yeah, given this image, you would have to try to predict 3.05.", "tokens": [50364, 407, 264, 1558, 307, 300, 11, 1338, 11, 2212, 341, 3256, 11, 291, 576, 362, 281, 853, 281, 6069, 805, 13, 13328, 13, 50620], "temperature": 0.0, "avg_logprob": -0.27720313035804806, "compression_ratio": 1.5691699604743083, "no_speech_prob": 0.06370437145233154}, {"id": 195, "seek": 87886, "start": 883.98, "end": 891.26, "text": " So one thing I was kind of curious about is, like, and it's always useful to know is like,", "tokens": [50620, 407, 472, 551, 286, 390, 733, 295, 6369, 466, 307, 11, 411, 11, 293, 309, 311, 1009, 4420, 281, 458, 307, 411, 11, 50984], "temperature": 0.0, "avg_logprob": -0.27720313035804806, "compression_ratio": 1.5691699604743083, "no_speech_prob": 0.06370437145233154}, {"id": 196, "seek": 87886, "start": 891.26, "end": 892.26, "text": " what's the baseline?", "tokens": [50984, 437, 311, 264, 20518, 30, 51034], "temperature": 0.0, "avg_logprob": -0.27720313035804806, "compression_ratio": 1.5691699604743083, "no_speech_prob": 0.06370437145233154}, {"id": 197, "seek": 87886, "start": 892.26, "end": 893.26, "text": " Like, what counts as good?", "tokens": [51034, 1743, 11, 437, 14893, 382, 665, 30, 51084], "temperature": 0.0, "avg_logprob": -0.27720313035804806, "compression_ratio": 1.5691699604743083, "no_speech_prob": 0.06370437145233154}, {"id": 198, "seek": 87886, "start": 893.26, "end": 898.38, "text": " You know, because often people will say to me like, oh, I created a model and the MSE", "tokens": [51084, 509, 458, 11, 570, 2049, 561, 486, 584, 281, 385, 411, 11, 1954, 11, 286, 2942, 257, 2316, 293, 264, 376, 5879, 51340], "temperature": 0.0, "avg_logprob": -0.27720313035804806, "compression_ratio": 1.5691699604743083, "no_speech_prob": 0.06370437145233154}, {"id": 199, "seek": 87886, "start": 898.38, "end": 899.38, "text": " was 2.6.", "tokens": [51340, 390, 568, 13, 21, 13, 51390], "temperature": 0.0, "avg_logprob": -0.27720313035804806, "compression_ratio": 1.5691699604743083, "no_speech_prob": 0.06370437145233154}, {"id": 200, "seek": 87886, "start": 899.38, "end": 900.94, "text": " I'll be like, well, is that good?", "tokens": [51390, 286, 603, 312, 411, 11, 731, 11, 307, 300, 665, 30, 51468], "temperature": 0.0, "avg_logprob": -0.27720313035804806, "compression_ratio": 1.5691699604743083, "no_speech_prob": 0.06370437145233154}, {"id": 201, "seek": 87886, "start": 900.94, "end": 902.58, "text": " Well, it's the best I can do.", "tokens": [51468, 1042, 11, 309, 311, 264, 1151, 286, 393, 360, 13, 51550], "temperature": 0.0, "avg_logprob": -0.27720313035804806, "compression_ratio": 1.5691699604743083, "no_speech_prob": 0.06370437145233154}, {"id": 202, "seek": 87886, "start": 902.58, "end": 903.58, "text": " But is it good?", "tokens": [51550, 583, 307, 309, 665, 30, 51600], "temperature": 0.0, "avg_logprob": -0.27720313035804806, "compression_ratio": 1.5691699604743083, "no_speech_prob": 0.06370437145233154}, {"id": 203, "seek": 90358, "start": 903.58, "end": 906.46, "text": " Like, or is it better than random?", "tokens": [50364, 1743, 11, 420, 307, 309, 1101, 813, 4974, 30, 50508], "temperature": 0.0, "avg_logprob": -0.28751457130515967, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.3344111442565918}, {"id": 204, "seek": 90358, "start": 906.46, "end": 910.1800000000001, "text": " Or is it better than predicting the average?", "tokens": [50508, 1610, 307, 309, 1101, 813, 32884, 264, 4274, 30, 50694], "temperature": 0.0, "avg_logprob": -0.28751457130515967, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.3344111442565918}, {"id": 205, "seek": 90358, "start": 910.1800000000001, "end": 915.1800000000001, "text": " So in this case, I was just like, okay, well, what if we just predicted, actually, this", "tokens": [50694, 407, 294, 341, 1389, 11, 286, 390, 445, 411, 11, 1392, 11, 731, 11, 437, 498, 321, 445, 19147, 11, 767, 11, 341, 50944], "temperature": 0.0, "avg_logprob": -0.28751457130515967, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.3344111442565918}, {"id": 206, "seek": 90358, "start": 915.1800000000001, "end": 916.1800000000001, "text": " is slightly out of date.", "tokens": [50944, 307, 4748, 484, 295, 4002, 13, 50994], "temperature": 0.0, "avg_logprob": -0.28751457130515967, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.3344111442565918}, {"id": 207, "seek": 90358, "start": 916.1800000000001, "end": 923.0600000000001, "text": " I should have said zero here rather than 0.5, but never mind, close enough.", "tokens": [50994, 286, 820, 362, 848, 4018, 510, 2831, 813, 1958, 13, 20, 11, 457, 1128, 1575, 11, 1998, 1547, 13, 51338], "temperature": 0.0, "avg_logprob": -0.28751457130515967, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.3344111442565918}, {"id": 208, "seek": 90358, "start": 923.0600000000001, "end": 928.0200000000001, "text": " So this is before I did the logit thing.", "tokens": [51338, 407, 341, 307, 949, 286, 630, 264, 3565, 270, 551, 13, 51586], "temperature": 0.0, "avg_logprob": -0.28751457130515967, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.3344111442565918}, {"id": 209, "seek": 92802, "start": 928.02, "end": 934.78, "text": " So I basically was looking at like, what's the, you know, loss if you just always predicted", "tokens": [50364, 407, 286, 1936, 390, 1237, 412, 411, 11, 437, 311, 264, 11, 291, 458, 11, 4470, 498, 291, 445, 1009, 19147, 50702], "temperature": 0.0, "avg_logprob": -0.2316840420598569, "compression_ratio": 1.4951923076923077, "no_speech_prob": 0.02441466599702835}, {"id": 210, "seek": 92802, "start": 934.78, "end": 941.26, "text": " a constant, which as I said, I should have put zero here, haven't updated it.", "tokens": [50702, 257, 5754, 11, 597, 382, 286, 848, 11, 286, 820, 362, 829, 4018, 510, 11, 2378, 380, 10588, 309, 13, 51026], "temperature": 0.0, "avg_logprob": -0.2316840420598569, "compression_ratio": 1.4951923076923077, "no_speech_prob": 0.02441466599702835}, {"id": 211, "seek": 92802, "start": 941.26, "end": 947.38, "text": " And so it's like, oh, that would give you a loss of 3.5.", "tokens": [51026, 400, 370, 309, 311, 411, 11, 1954, 11, 300, 576, 976, 291, 257, 4470, 295, 805, 13, 20, 13, 51332], "temperature": 0.0, "avg_logprob": -0.2316840420598569, "compression_ratio": 1.4951923076923077, "no_speech_prob": 0.02441466599702835}, {"id": 212, "seek": 92802, "start": 947.38, "end": 955.26, "text": " Or another way to do it is you could just put MSE here and then look at the MSE loss", "tokens": [51332, 1610, 1071, 636, 281, 360, 309, 307, 291, 727, 445, 829, 376, 5879, 510, 293, 550, 574, 412, 264, 376, 5879, 4470, 51726], "temperature": 0.0, "avg_logprob": -0.2316840420598569, "compression_ratio": 1.4951923076923077, "no_speech_prob": 0.02441466599702835}, {"id": 213, "seek": 95526, "start": 955.26, "end": 964.06, "text": " between 0.5 and your various, just a single mini-batch, which we, yeah, mini-batch of", "tokens": [50364, 1296, 1958, 13, 20, 293, 428, 3683, 11, 445, 257, 2167, 8382, 12, 65, 852, 11, 597, 321, 11, 1338, 11, 8382, 12, 65, 852, 295, 50804], "temperature": 0.0, "avg_logprob": -0.3285678863525391, "compression_ratio": 1.5645933014354068, "no_speech_prob": 0.43006813526153564}, {"id": 214, "seek": 95526, "start": 964.06, "end": 965.06, "text": " alphabatis logits.", "tokens": [50804, 419, 950, 455, 267, 271, 3565, 1208, 13, 50854], "temperature": 0.0, "avg_logprob": -0.3285678863525391, "compression_ratio": 1.5645933014354068, "no_speech_prob": 0.43006813526153564}, {"id": 215, "seek": 95526, "start": 965.06, "end": 971.74, "text": " Yeah, so, you know, we wanted to get, you know, if we're getting something that's about", "tokens": [50854, 865, 11, 370, 11, 291, 458, 11, 321, 1415, 281, 483, 11, 291, 458, 11, 498, 321, 434, 1242, 746, 300, 311, 466, 51188], "temperature": 0.0, "avg_logprob": -0.3285678863525391, "compression_ratio": 1.5645933014354068, "no_speech_prob": 0.43006813526153564}, {"id": 216, "seek": 95526, "start": 971.74, "end": 976.7, "text": " 3, then we basically haven't done any better than random.", "tokens": [51188, 805, 11, 550, 321, 1936, 2378, 380, 1096, 604, 1101, 813, 4974, 13, 51436], "temperature": 0.0, "avg_logprob": -0.3285678863525391, "compression_ratio": 1.5645933014354068, "no_speech_prob": 0.43006813526153564}, {"id": 217, "seek": 95526, "start": 976.7, "end": 982.66, "text": " And so in this case, this model, it doesn't actually have anything to learn.", "tokens": [51436, 400, 370, 294, 341, 1389, 11, 341, 2316, 11, 309, 1177, 380, 767, 362, 1340, 281, 1466, 13, 51734], "temperature": 0.0, "avg_logprob": -0.3285678863525391, "compression_ratio": 1.5645933014354068, "no_speech_prob": 0.43006813526153564}, {"id": 218, "seek": 98266, "start": 982.66, "end": 984.2199999999999, "text": " And it always returns the same thing.", "tokens": [50364, 400, 309, 1009, 11247, 264, 912, 551, 13, 50442], "temperature": 0.0, "avg_logprob": -0.3038956035267223, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.052601058036088943}, {"id": 219, "seek": 98266, "start": 984.2199999999999, "end": 988.54, "text": " So we can just call fit with train equals false just to find the loss.", "tokens": [50442, 407, 321, 393, 445, 818, 3318, 365, 3847, 6915, 7908, 445, 281, 915, 264, 4470, 13, 50658], "temperature": 0.0, "avg_logprob": -0.3038956035267223, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.052601058036088943}, {"id": 220, "seek": 98266, "start": 988.54, "end": 999.98, "text": " So these are just a couple of ways of getting quickly, finding a loss for a baseline naive model.", "tokens": [50658, 407, 613, 366, 445, 257, 1916, 295, 2098, 295, 1242, 2661, 11, 5006, 257, 4470, 337, 257, 20518, 29052, 2316, 13, 51230], "temperature": 0.0, "avg_logprob": -0.3038956035267223, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.052601058036088943}, {"id": 221, "seek": 98266, "start": 999.98, "end": 1008.9, "text": " One thing that thankfully PyTorch will warn you about is if you try to use MSE and your", "tokens": [51230, 1485, 551, 300, 27352, 9953, 51, 284, 339, 486, 12286, 291, 466, 307, 498, 291, 853, 281, 764, 376, 5879, 293, 428, 51676], "temperature": 0.0, "avg_logprob": -0.3038956035267223, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.052601058036088943}, {"id": 222, "seek": 100890, "start": 1008.9, "end": 1013.78, "text": " inputs and targets have different shapes, it will broadcast and give you probably not", "tokens": [50364, 15743, 293, 12911, 362, 819, 10854, 11, 309, 486, 9975, 293, 976, 291, 1391, 406, 50608], "temperature": 0.0, "avg_logprob": -0.2727274303593911, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011330722831189632}, {"id": 223, "seek": 100890, "start": 1013.78, "end": 1015.18, "text": " the result you would expect.", "tokens": [50608, 264, 1874, 291, 576, 2066, 13, 50678], "temperature": 0.0, "avg_logprob": -0.2727274303593911, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011330722831189632}, {"id": 224, "seek": 100890, "start": 1015.18, "end": 1016.78, "text": " And it will give you a warning.", "tokens": [50678, 400, 309, 486, 976, 291, 257, 9164, 13, 50758], "temperature": 0.0, "avg_logprob": -0.2727274303593911, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011330722831189632}, {"id": 225, "seek": 100890, "start": 1016.78, "end": 1024.5, "text": " So one way to avoid that is just to use .flatten on each.", "tokens": [50758, 407, 472, 636, 281, 5042, 300, 307, 445, 281, 764, 2411, 3423, 32733, 322, 1184, 13, 51144], "temperature": 0.0, "avg_logprob": -0.2727274303593911, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011330722831189632}, {"id": 226, "seek": 100890, "start": 1024.5, "end": 1029.86, "text": " So this kind of flattened MSE is useful to avoid both, avoid the warning and also avoid", "tokens": [51144, 407, 341, 733, 295, 24183, 292, 376, 5879, 307, 4420, 281, 5042, 1293, 11, 5042, 264, 9164, 293, 611, 5042, 51412], "temperature": 0.0, "avg_logprob": -0.2727274303593911, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011330722831189632}, {"id": 227, "seek": 100890, "start": 1029.86, "end": 1033.7, "text": " getting weird errors, or sorry, weird results.", "tokens": [51412, 1242, 3657, 13603, 11, 420, 2597, 11, 3657, 3542, 13, 51604], "temperature": 0.0, "avg_logprob": -0.2727274303593911, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011330722831189632}, {"id": 228, "seek": 100890, "start": 1033.7, "end": 1035.3, "text": " So we use that for our loss.", "tokens": [51604, 407, 321, 764, 300, 337, 527, 4470, 13, 51684], "temperature": 0.0, "avg_logprob": -0.2727274303593911, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011330722831189632}, {"id": 229, "seek": 100890, "start": 1035.3, "end": 1037.74, "text": " So the model's the model that we always use.", "tokens": [51684, 407, 264, 2316, 311, 264, 2316, 300, 321, 1009, 764, 13, 51806], "temperature": 0.0, "avg_logprob": -0.2727274303593911, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011330722831189632}, {"id": 230, "seek": 100890, "start": 1037.74, "end": 1038.74, "text": " So it's kind of nice.", "tokens": [51806, 407, 309, 311, 733, 295, 1481, 13, 51856], "temperature": 0.0, "avg_logprob": -0.2727274303593911, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011330722831189632}, {"id": 231, "seek": 103874, "start": 1039.58, "end": 1042.34, "text": " We just use our same old model.", "tokens": [50406, 492, 445, 764, 527, 912, 1331, 2316, 13, 50544], "temperature": 0.0, "avg_logprob": -0.32600318908691406, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.042085200548172}, {"id": 232, "seek": 103874, "start": 1042.34, "end": 1048.74, "text": " Nothing changes, even though we're doing something totally different.", "tokens": [50544, 6693, 2962, 11, 754, 1673, 321, 434, 884, 746, 3879, 819, 13, 50864], "temperature": 0.0, "avg_logprob": -0.32600318908691406, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.042085200548172}, {"id": 233, "seek": 103874, "start": 1048.74, "end": 1052.3, "text": " Oh, well, okay, that's not quite true.", "tokens": [50864, 876, 11, 731, 11, 1392, 11, 300, 311, 406, 1596, 2074, 13, 51042], "temperature": 0.0, "avg_logprob": -0.32600318908691406, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.042085200548172}, {"id": 234, "seek": 103874, "start": 1052.3, "end": 1055.1, "text": " The difference is that our output, we just have one output now.", "tokens": [51042, 440, 2649, 307, 300, 527, 5598, 11, 321, 445, 362, 472, 5598, 586, 13, 51182], "temperature": 0.0, "avg_logprob": -0.32600318908691406, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.042085200548172}, {"id": 235, "seek": 103874, "start": 1055.1, "end": 1059.34, "text": " Because this is now a regression model, it's just trying to predict a single number.", "tokens": [51182, 1436, 341, 307, 586, 257, 24590, 2316, 11, 309, 311, 445, 1382, 281, 6069, 257, 2167, 1230, 13, 51394], "temperature": 0.0, "avg_logprob": -0.32600318908691406, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.042085200548172}, {"id": 236, "seek": 103874, "start": 1059.34, "end": 1066.06, "text": " And so our learner now uses MSE as the loss.", "tokens": [51394, 400, 370, 527, 33347, 586, 4960, 376, 5879, 382, 264, 4470, 13, 51730], "temperature": 0.0, "avg_logprob": -0.32600318908691406, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.042085200548172}, {"id": 237, "seek": 103874, "start": 1066.06, "end": 1067.94, "text": " Everything else is the same as usual.", "tokens": [51730, 5471, 1646, 307, 264, 912, 382, 7713, 13, 51824], "temperature": 0.0, "avg_logprob": -0.32600318908691406, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.042085200548172}, {"id": 238, "seek": 106794, "start": 1068.14, "end": 1069.3, "text": " So we can go ahead and train it.", "tokens": [50374, 407, 321, 393, 352, 2286, 293, 3847, 309, 13, 50432], "temperature": 0.0, "avg_logprob": -0.26694229413878245, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.00067714205943048}, {"id": 239, "seek": 106794, "start": 1069.3, "end": 1071.66, "text": " And you can see, okay, the loss is already much better than three.", "tokens": [50432, 400, 291, 393, 536, 11, 1392, 11, 264, 4470, 307, 1217, 709, 1101, 813, 1045, 13, 50550], "temperature": 0.0, "avg_logprob": -0.26694229413878245, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.00067714205943048}, {"id": 240, "seek": 106794, "start": 1071.66, "end": 1074.3400000000001, "text": " So we're definitely learning something.", "tokens": [50550, 407, 321, 434, 2138, 2539, 746, 13, 50684], "temperature": 0.0, "avg_logprob": -0.26694229413878245, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.00067714205943048}, {"id": 241, "seek": 106794, "start": 1074.3400000000001, "end": 1081.14, "text": " And we end up with a .075 mean squared error.", "tokens": [50684, 400, 321, 917, 493, 365, 257, 2411, 15, 11901, 914, 8889, 6713, 13, 51024], "temperature": 0.0, "avg_logprob": -0.26694229413878245, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.00067714205943048}, {"id": 242, "seek": 106794, "start": 1081.14, "end": 1085.66, "text": " That's pretty good, considering, you know, there's a pretty wide range of numbers we're", "tokens": [51024, 663, 311, 1238, 665, 11, 8079, 11, 291, 458, 11, 456, 311, 257, 1238, 4874, 3613, 295, 3547, 321, 434, 51250], "temperature": 0.0, "avg_logprob": -0.26694229413878245, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.00067714205943048}, {"id": 243, "seek": 106794, "start": 1085.66, "end": 1089.5800000000002, "text": " trying to predict here.", "tokens": [51250, 1382, 281, 6069, 510, 13, 51446], "temperature": 0.0, "avg_logprob": -0.26694229413878245, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.00067714205943048}, {"id": 244, "seek": 106794, "start": 1089.5800000000002, "end": 1094.02, "text": " So I'm going to save that as noise prediction on Sigma.", "tokens": [51446, 407, 286, 478, 516, 281, 3155, 300, 382, 5658, 17630, 322, 36595, 13, 51668], "temperature": 0.0, "avg_logprob": -0.26694229413878245, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.00067714205943048}, {"id": 245, "seek": 106794, "start": 1094.02, "end": 1095.9, "text": " So save that model.", "tokens": [51668, 407, 3155, 300, 2316, 13, 51762], "temperature": 0.0, "avg_logprob": -0.26694229413878245, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.00067714205943048}, {"id": 246, "seek": 109590, "start": 1095.98, "end": 1105.18, "text": " And so we can take a look at how it's doing by grabbing our one batch of noised images,", "tokens": [50368, 400, 370, 321, 393, 747, 257, 574, 412, 577, 309, 311, 884, 538, 23771, 527, 472, 15245, 295, 572, 2640, 5267, 11, 50828], "temperature": 0.0, "avg_logprob": -0.27104422920628596, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.001956949010491371}, {"id": 247, "seek": 109590, "start": 1105.18, "end": 1107.7800000000002, "text": " putting it through our T model.", "tokens": [50828, 3372, 309, 807, 527, 314, 2316, 13, 50958], "temperature": 0.0, "avg_logprob": -0.27104422920628596, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.001956949010491371}, {"id": 248, "seek": 109590, "start": 1107.7800000000002, "end": 1109.38, "text": " Actually, it's really an alpha bar model.", "tokens": [50958, 5135, 11, 309, 311, 534, 364, 8961, 2159, 2316, 13, 51038], "temperature": 0.0, "avg_logprob": -0.27104422920628596, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.001956949010491371}, {"id": 249, "seek": 109590, "start": 1109.38, "end": 1112.74, "text": " But never mind, call it a T model.", "tokens": [51038, 583, 1128, 1575, 11, 818, 309, 257, 314, 2316, 13, 51206], "temperature": 0.0, "avg_logprob": -0.27104422920628596, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.001956949010491371}, {"id": 250, "seek": 109590, "start": 1112.74, "end": 1118.02, "text": " And then we can take a look to see what it's predicted for each one.", "tokens": [51206, 400, 550, 321, 393, 747, 257, 574, 281, 536, 437, 309, 311, 19147, 337, 1184, 472, 13, 51470], "temperature": 0.0, "avg_logprob": -0.27104422920628596, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.001956949010491371}, {"id": 251, "seek": 109590, "start": 1118.02, "end": 1121.22, "text": " And we can compare it to the actual for each one.", "tokens": [51470, 400, 321, 393, 6794, 309, 281, 264, 3539, 337, 1184, 472, 13, 51630], "temperature": 0.0, "avg_logprob": -0.27104422920628596, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.001956949010491371}, {"id": 252, "seek": 109590, "start": 1121.22, "end": 1125.02, "text": " And so you can see here, it said, oh, I think this is about .91.", "tokens": [51630, 400, 370, 291, 393, 536, 510, 11, 309, 848, 11, 1954, 11, 286, 519, 341, 307, 466, 2411, 29925, 13, 51820], "temperature": 0.0, "avg_logprob": -0.27104422920628596, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.001956949010491371}, {"id": 253, "seek": 112502, "start": 1125.1399999999999, "end": 1126.1399999999999, "text": " And actually, it is .91.", "tokens": [50370, 400, 767, 11, 309, 307, 2411, 29925, 13, 50420], "temperature": 0.0, "avg_logprob": -0.3095918670902407, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0023596121463924646}, {"id": 254, "seek": 112502, "start": 1126.1399999999999, "end": 1129.54, "text": " So now here, it looks like about .36.", "tokens": [50420, 407, 586, 510, 11, 309, 1542, 411, 466, 2411, 11309, 13, 50590], "temperature": 0.0, "avg_logprob": -0.3095918670902407, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0023596121463924646}, {"id": 255, "seek": 112502, "start": 1129.54, "end": 1131.82, "text": " And yeah, it is actually .36.", "tokens": [50590, 400, 1338, 11, 309, 307, 767, 2411, 11309, 13, 50704], "temperature": 0.0, "avg_logprob": -0.3095918670902407, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0023596121463924646}, {"id": 256, "seek": 112502, "start": 1131.82, "end": 1135.02, "text": " So you know, you can see overall .72, it's actually .72.", "tokens": [50704, 407, 291, 458, 11, 291, 393, 536, 4787, 2411, 28890, 11, 309, 311, 767, 2411, 28890, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3095918670902407, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0023596121463924646}, {"id": 257, "seek": 112502, "start": 1135.02, "end": 1138.46, "text": " Or it's actually right, this one's .02 off.", "tokens": [50864, 1610, 309, 311, 767, 558, 11, 341, 472, 311, 2411, 12756, 766, 13, 51036], "temperature": 0.0, "avg_logprob": -0.3095918670902407, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0023596121463924646}, {"id": 258, "seek": 112502, "start": 1138.46, "end": 1140.9, "text": " But yeah, my hypothesis was correct.", "tokens": [51036, 583, 1338, 11, 452, 17291, 390, 3006, 13, 51158], "temperature": 0.0, "avg_logprob": -0.3095918670902407, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0023596121463924646}, {"id": 259, "seek": 112502, "start": 1140.9, "end": 1147.1, "text": " Which is that we, you know, we can predict the thing that we were putting in manually", "tokens": [51158, 3013, 307, 300, 321, 11, 291, 458, 11, 321, 393, 6069, 264, 551, 300, 321, 645, 3372, 294, 16945, 51468], "temperature": 0.0, "avg_logprob": -0.3095918670902407, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0023596121463924646}, {"id": 260, "seek": 112502, "start": 1147.1, "end": 1148.18, "text": " as input.", "tokens": [51468, 382, 4846, 13, 51522], "temperature": 0.0, "avg_logprob": -0.3095918670902407, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0023596121463924646}, {"id": 261, "seek": 112502, "start": 1148.18, "end": 1153.78, "text": " So there's a couple of reasons I was interested in checking this out.", "tokens": [51522, 407, 456, 311, 257, 1916, 295, 4112, 286, 390, 3102, 294, 8568, 341, 484, 13, 51802], "temperature": 0.0, "avg_logprob": -0.3095918670902407, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0023596121463924646}, {"id": 262, "seek": 115378, "start": 1154.3, "end": 1159.7, "text": " The first was just like, well, yeah, wouldn't it be simpler if we weren't passing in the", "tokens": [50390, 440, 700, 390, 445, 411, 11, 731, 11, 1338, 11, 2759, 380, 309, 312, 18587, 498, 321, 4999, 380, 8437, 294, 264, 50660], "temperature": 0.0, "avg_logprob": -0.272212231412847, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.008984963409602642}, {"id": 263, "seek": 115378, "start": 1159.7, "end": 1160.7, "text": " T each time?", "tokens": [50660, 314, 1184, 565, 30, 50710], "temperature": 0.0, "avg_logprob": -0.272212231412847, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.008984963409602642}, {"id": 264, "seek": 115378, "start": 1160.7, "end": 1163.42, "text": " You know, why not pass in the T each time?", "tokens": [50710, 509, 458, 11, 983, 406, 1320, 294, 264, 314, 1184, 565, 30, 50846], "temperature": 0.0, "avg_logprob": -0.272212231412847, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.008984963409602642}, {"id": 265, "seek": 115378, "start": 1163.42, "end": 1170.78, "text": " But it also felt like it would open up a wider range of kind of how we can do sampling.", "tokens": [50846, 583, 309, 611, 2762, 411, 309, 576, 1269, 493, 257, 11842, 3613, 295, 733, 295, 577, 321, 393, 360, 21179, 13, 51214], "temperature": 0.0, "avg_logprob": -0.272212231412847, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.008984963409602642}, {"id": 266, "seek": 115378, "start": 1170.78, "end": 1177.22, "text": " The idea of doing sampling by like precisely controlling the amount of noise that you try", "tokens": [51214, 440, 1558, 295, 884, 21179, 538, 411, 13402, 14905, 264, 2372, 295, 5658, 300, 291, 853, 51536], "temperature": 0.0, "avg_logprob": -0.272212231412847, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.008984963409602642}, {"id": 267, "seek": 115378, "start": 1177.22, "end": 1179.54, "text": " to remove each time.", "tokens": [51536, 281, 4159, 1184, 565, 13, 51652], "temperature": 0.0, "avg_logprob": -0.272212231412847, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.008984963409602642}, {"id": 268, "seek": 117954, "start": 1179.54, "end": 1185.06, "text": " And then assuming you can remove exactly that amount of noise each time feels limited", "tokens": [50364, 400, 550, 11926, 291, 393, 4159, 2293, 300, 2372, 295, 5658, 1184, 565, 3417, 5567, 50640], "temperature": 0.0, "avg_logprob": -0.26976078748703003, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.01205229852348566}, {"id": 269, "seek": 117954, "start": 1185.06, "end": 1188.34, "text": " to me.", "tokens": [50640, 281, 385, 13, 50804], "temperature": 0.0, "avg_logprob": -0.26976078748703003, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.01205229852348566}, {"id": 270, "seek": 117954, "start": 1188.34, "end": 1194.44, "text": " So I want to try to remove this constraint.", "tokens": [50804, 407, 286, 528, 281, 853, 281, 4159, 341, 25534, 13, 51109], "temperature": 0.0, "avg_logprob": -0.26976078748703003, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.01205229852348566}, {"id": 271, "seek": 117954, "start": 1194.44, "end": 1198.94, "text": " So having built this model, I thought, okay, well, you know, which is basically like, okay,", "tokens": [51109, 407, 1419, 3094, 341, 2316, 11, 286, 1194, 11, 1392, 11, 731, 11, 291, 458, 11, 597, 307, 1936, 411, 11, 1392, 11, 51334], "temperature": 0.0, "avg_logprob": -0.26976078748703003, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.01205229852348566}, {"id": 272, "seek": 117954, "start": 1198.94, "end": 1201.26, "text": " I think we don't need to pass T in.", "tokens": [51334, 286, 519, 321, 500, 380, 643, 281, 1320, 314, 294, 13, 51450], "temperature": 0.0, "avg_logprob": -0.26976078748703003, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.01205229852348566}, {"id": 273, "seek": 117954, "start": 1201.26, "end": 1202.84, "text": " Let's try it.", "tokens": [51450, 961, 311, 853, 309, 13, 51529], "temperature": 0.0, "avg_logprob": -0.26976078748703003, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.01205229852348566}, {"id": 274, "seek": 117954, "start": 1202.84, "end": 1205.86, "text": " So what I then did is I replicated the 22 cosine notebook.", "tokens": [51529, 407, 437, 286, 550, 630, 307, 286, 46365, 264, 5853, 23565, 21060, 13, 51680], "temperature": 0.0, "avg_logprob": -0.26976078748703003, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.01205229852348566}, {"id": 275, "seek": 120586, "start": 1205.9799999999998, "end": 1209.86, "text": " I just copied it, pasted it in here.", "tokens": [50370, 286, 445, 25365, 309, 11, 1791, 292, 309, 294, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2303637374531139, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.011685283854603767}, {"id": 276, "seek": 120586, "start": 1209.86, "end": 1213.9399999999998, "text": " But I made a couple of changes.", "tokens": [50564, 583, 286, 1027, 257, 1916, 295, 2962, 13, 50768], "temperature": 0.0, "avg_logprob": -0.2303637374531139, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.011685283854603767}, {"id": 277, "seek": 120586, "start": 1213.9399999999998, "end": 1217.9399999999998, "text": " The first is that Noisify doesn't return T anymore.", "tokens": [50768, 440, 700, 307, 300, 883, 271, 2505, 1177, 380, 2736, 314, 3602, 13, 50968], "temperature": 0.0, "avg_logprob": -0.2303637374531139, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.011685283854603767}, {"id": 278, "seek": 120586, "start": 1217.9399999999998, "end": 1219.6999999999998, "text": " So there's no way to cheat.", "tokens": [50968, 407, 456, 311, 572, 636, 281, 17470, 13, 51056], "temperature": 0.0, "avg_logprob": -0.2303637374531139, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.011685283854603767}, {"id": 279, "seek": 120586, "start": 1219.6999999999998, "end": 1224.4599999999998, "text": " We don't know what T is.", "tokens": [51056, 492, 500, 380, 458, 437, 314, 307, 13, 51294], "temperature": 0.0, "avg_logprob": -0.2303637374531139, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.011685283854603767}, {"id": 280, "seek": 120586, "start": 1224.4599999999998, "end": 1228.74, "text": " And so that means that the unit now doesn't have T.", "tokens": [51294, 400, 370, 300, 1355, 300, 264, 4985, 586, 1177, 380, 362, 314, 13, 51508], "temperature": 0.0, "avg_logprob": -0.2303637374531139, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.011685283854603767}, {"id": 281, "seek": 120586, "start": 1228.74, "end": 1233.32, "text": " So it's actually going to pass zero every time.", "tokens": [51508, 407, 309, 311, 767, 516, 281, 1320, 4018, 633, 565, 13, 51737], "temperature": 0.0, "avg_logprob": -0.2303637374531139, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.011685283854603767}, {"id": 282, "seek": 123332, "start": 1233.32, "end": 1238.8799999999999, "text": " So it has no ability to learn from T because it doesn't get T. So it doesn't really matter", "tokens": [50364, 407, 309, 575, 572, 3485, 281, 1466, 490, 314, 570, 309, 1177, 380, 483, 314, 13, 407, 309, 1177, 380, 534, 1871, 50642], "temperature": 0.0, "avg_logprob": -0.22567191956535218, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.018262874335050583}, {"id": 283, "seek": 123332, "start": 1238.8799999999999, "end": 1239.8799999999999, "text": " what we pass in.", "tokens": [50642, 437, 321, 1320, 294, 13, 50692], "temperature": 0.0, "avg_logprob": -0.22567191956535218, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.018262874335050583}, {"id": 284, "seek": 123332, "start": 1239.8799999999999, "end": 1247.8, "text": " We could have changed the unit to like remove the conditioning on T. But for research, this", "tokens": [50692, 492, 727, 362, 3105, 264, 4985, 281, 411, 4159, 264, 21901, 322, 314, 13, 583, 337, 2132, 11, 341, 51088], "temperature": 0.0, "avg_logprob": -0.22567191956535218, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.018262874335050583}, {"id": 285, "seek": 123332, "start": 1247.8, "end": 1251.2, "text": " is just as good, you know, for finding out.", "tokens": [51088, 307, 445, 382, 665, 11, 291, 458, 11, 337, 5006, 484, 13, 51258], "temperature": 0.0, "avg_logprob": -0.22567191956535218, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.018262874335050583}, {"id": 286, "seek": 123332, "start": 1251.2, "end": 1253.0, "text": " And it's good to be lazy when doing research.", "tokens": [51258, 400, 309, 311, 665, 281, 312, 14847, 562, 884, 2132, 13, 51348], "temperature": 0.0, "avg_logprob": -0.22567191956535218, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.018262874335050583}, {"id": 287, "seek": 123332, "start": 1253.0, "end": 1257.3999999999999, "text": " There's no point doing something a fancy way when you can do it a quick and easy way before", "tokens": [51348, 821, 311, 572, 935, 884, 746, 257, 10247, 636, 562, 291, 393, 360, 309, 257, 1702, 293, 1858, 636, 949, 51568], "temperature": 0.0, "avg_logprob": -0.22567191956535218, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.018262874335050583}, {"id": 288, "seek": 123332, "start": 1257.3999999999999, "end": 1260.32, "text": " you even know if it's going to work.", "tokens": [51568, 291, 754, 458, 498, 309, 311, 516, 281, 589, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22567191956535218, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.018262874335050583}, {"id": 289, "seek": 123332, "start": 1260.32, "end": 1262.78, "text": " So yeah, that's the only change.", "tokens": [51714, 407, 1338, 11, 300, 311, 264, 787, 1319, 13, 51837], "temperature": 0.0, "avg_logprob": -0.22567191956535218, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.018262874335050583}, {"id": 290, "seek": 126278, "start": 1262.78, "end": 1264.74, "text": " So we can then train the model.", "tokens": [50364, 407, 321, 393, 550, 3847, 264, 2316, 13, 50462], "temperature": 0.0, "avg_logprob": -0.2660002935500372, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.01518839318305254}, {"id": 291, "seek": 126278, "start": 1264.74, "end": 1266.58, "text": " And we can check the loss.", "tokens": [50462, 400, 321, 393, 1520, 264, 4470, 13, 50554], "temperature": 0.0, "avg_logprob": -0.2660002935500372, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.01518839318305254}, {"id": 292, "seek": 126278, "start": 1266.58, "end": 1274.94, "text": " So the loss here is 0.034.", "tokens": [50554, 407, 264, 4470, 510, 307, 1958, 13, 11592, 19, 13, 50972], "temperature": 0.0, "avg_logprob": -0.2660002935500372, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.01518839318305254}, {"id": 293, "seek": 126278, "start": 1274.94, "end": 1276.82, "text": " And previously it was 0.033.", "tokens": [50972, 400, 8046, 309, 390, 1958, 13, 15, 10191, 13, 51066], "temperature": 0.0, "avg_logprob": -0.2660002935500372, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.01518839318305254}, {"id": 294, "seek": 126278, "start": 1276.82, "end": 1285.62, "text": " So interestingly, you know, maybe it's a tiny bit worse at that, you know.", "tokens": [51066, 407, 25873, 11, 291, 458, 11, 1310, 309, 311, 257, 5870, 857, 5324, 412, 300, 11, 291, 458, 13, 51506], "temperature": 0.0, "avg_logprob": -0.2660002935500372, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.01518839318305254}, {"id": 295, "seek": 126278, "start": 1285.62, "end": 1287.66, "text": " But it's very close.", "tokens": [51506, 583, 309, 311, 588, 1998, 13, 51608], "temperature": 0.0, "avg_logprob": -0.2660002935500372, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.01518839318305254}, {"id": 296, "seek": 126278, "start": 1287.66, "end": 1291.58, "text": " Okay, so we'll save that model.", "tokens": [51608, 1033, 11, 370, 321, 603, 3155, 300, 2316, 13, 51804], "temperature": 0.0, "avg_logprob": -0.2660002935500372, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.01518839318305254}, {"id": 297, "seek": 129158, "start": 1291.58, "end": 1302.1399999999999, "text": " And then for sampling, I've got exactly the same DDIM step as usual.", "tokens": [50364, 400, 550, 337, 21179, 11, 286, 600, 658, 2293, 264, 912, 413, 3085, 44, 1823, 382, 7713, 13, 50892], "temperature": 0.0, "avg_logprob": -0.24650878290976247, "compression_ratio": 1.4244604316546763, "no_speech_prob": 0.01016906090080738}, {"id": 298, "seek": 129158, "start": 1302.1399999999999, "end": 1304.78, "text": " And my sampling is exactly the same as usual.", "tokens": [50892, 400, 452, 21179, 307, 2293, 264, 912, 382, 7713, 13, 51024], "temperature": 0.0, "avg_logprob": -0.24650878290976247, "compression_ratio": 1.4244604316546763, "no_speech_prob": 0.01016906090080738}, {"id": 299, "seek": 129158, "start": 1304.78, "end": 1313.4199999999998, "text": " Except now, when I call the model, I have no T to pass in.", "tokens": [51024, 16192, 586, 11, 562, 286, 818, 264, 2316, 11, 286, 362, 572, 314, 281, 1320, 294, 13, 51456], "temperature": 0.0, "avg_logprob": -0.24650878290976247, "compression_ratio": 1.4244604316546763, "no_speech_prob": 0.01016906090080738}, {"id": 300, "seek": 129158, "start": 1313.4199999999998, "end": 1319.5, "text": " So we just pass in this.", "tokens": [51456, 407, 321, 445, 1320, 294, 341, 13, 51760], "temperature": 0.0, "avg_logprob": -0.24650878290976247, "compression_ratio": 1.4244604316546763, "no_speech_prob": 0.01016906090080738}, {"id": 301, "seek": 131950, "start": 1319.54, "end": 1323.18, "text": " I mean, I still know T because I'm still using the usual sampling approach, but I'm not passing", "tokens": [50366, 286, 914, 11, 286, 920, 458, 314, 570, 286, 478, 920, 1228, 264, 7713, 21179, 3109, 11, 457, 286, 478, 406, 8437, 50548], "temperature": 0.0, "avg_logprob": -0.259543251991272, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.003429564880207181}, {"id": 302, "seek": 131950, "start": 1323.18, "end": 1326.1, "text": " it to the model.", "tokens": [50548, 309, 281, 264, 2316, 13, 50694], "temperature": 0.0, "avg_logprob": -0.259543251991272, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.003429564880207181}, {"id": 303, "seek": 131950, "start": 1326.1, "end": 1328.06, "text": " And yeah, we can sample.", "tokens": [50694, 400, 1338, 11, 321, 393, 6889, 13, 50792], "temperature": 0.0, "avg_logprob": -0.259543251991272, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.003429564880207181}, {"id": 304, "seek": 131950, "start": 1328.06, "end": 1332.9, "text": " And what happens is actually pretty garbage.", "tokens": [50792, 400, 437, 2314, 307, 767, 1238, 14150, 13, 51034], "temperature": 0.0, "avg_logprob": -0.259543251991272, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.003429564880207181}, {"id": 305, "seek": 131950, "start": 1332.9, "end": 1337.5, "text": " 22 is our fit.", "tokens": [51034, 5853, 307, 527, 3318, 13, 51264], "temperature": 0.0, "avg_logprob": -0.259543251991272, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.003429564880207181}, {"id": 306, "seek": 131950, "start": 1337.5, "end": 1347.7, "text": " And as you can see here, you know, some of the images are still really noisy.", "tokens": [51264, 400, 382, 291, 393, 536, 510, 11, 291, 458, 11, 512, 295, 264, 5267, 366, 920, 534, 24518, 13, 51774], "temperature": 0.0, "avg_logprob": -0.259543251991272, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.003429564880207181}, {"id": 307, "seek": 134770, "start": 1347.7, "end": 1350.74, "text": " So I totally failed.", "tokens": [50364, 407, 286, 3879, 7612, 13, 50516], "temperature": 0.0, "avg_logprob": -0.22734188503689237, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.0029809260740876198}, {"id": 308, "seek": 134770, "start": 1350.74, "end": 1356.18, "text": " And so that's always a little discouraging when you think something's going to work and", "tokens": [50516, 400, 370, 300, 311, 1009, 257, 707, 21497, 3568, 562, 291, 519, 746, 311, 516, 281, 589, 293, 50788], "temperature": 0.0, "avg_logprob": -0.22734188503689237, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.0029809260740876198}, {"id": 309, "seek": 134770, "start": 1356.18, "end": 1357.6200000000001, "text": " it doesn't.", "tokens": [50788, 309, 1177, 380, 13, 50860], "temperature": 0.0, "avg_logprob": -0.22734188503689237, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.0029809260740876198}, {"id": 310, "seek": 134770, "start": 1357.6200000000001, "end": 1361.3, "text": " But my reaction to that is like, if I think something's going to work and it doesn't,", "tokens": [50860, 583, 452, 5480, 281, 300, 307, 411, 11, 498, 286, 519, 746, 311, 516, 281, 589, 293, 309, 1177, 380, 11, 51044], "temperature": 0.0, "avg_logprob": -0.22734188503689237, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.0029809260740876198}, {"id": 311, "seek": 134770, "start": 1361.3, "end": 1364.5800000000002, "text": " is to think, well, I'm just going to have to do a better job of it.", "tokens": [51044, 307, 281, 519, 11, 731, 11, 286, 478, 445, 516, 281, 362, 281, 360, 257, 1101, 1691, 295, 309, 13, 51208], "temperature": 0.0, "avg_logprob": -0.22734188503689237, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.0029809260740876198}, {"id": 312, "seek": 134770, "start": 1364.5800000000002, "end": 1367.74, "text": " You know, it ought to work.", "tokens": [51208, 509, 458, 11, 309, 13416, 281, 589, 13, 51366], "temperature": 0.0, "avg_logprob": -0.22734188503689237, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.0029809260740876198}, {"id": 313, "seek": 136774, "start": 1367.74, "end": 1378.34, "text": " So I tried something different, which is I thought like, okay, since we're not passing", "tokens": [50364, 407, 286, 3031, 746, 819, 11, 597, 307, 286, 1194, 411, 11, 1392, 11, 1670, 321, 434, 406, 8437, 50894], "temperature": 0.0, "avg_logprob": -0.21972830084305775, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.03404952585697174}, {"id": 314, "seek": 136774, "start": 1378.34, "end": 1386.74, "text": " in the T, then we're basically saying like, how much noise should you be removing?", "tokens": [50894, 294, 264, 314, 11, 550, 321, 434, 1936, 1566, 411, 11, 577, 709, 5658, 820, 291, 312, 12720, 30, 51314], "temperature": 0.0, "avg_logprob": -0.21972830084305775, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.03404952585697174}, {"id": 315, "seek": 136774, "start": 1386.74, "end": 1388.1, "text": " It doesn't know exactly.", "tokens": [51314, 467, 1177, 380, 458, 2293, 13, 51382], "temperature": 0.0, "avg_logprob": -0.21972830084305775, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.03404952585697174}, {"id": 316, "seek": 136774, "start": 1388.1, "end": 1391.94, "text": " So it might remove a little bit more noise that we want, or a little bit less noise than", "tokens": [51382, 407, 309, 1062, 4159, 257, 707, 857, 544, 5658, 300, 321, 528, 11, 420, 257, 707, 857, 1570, 5658, 813, 51574], "temperature": 0.0, "avg_logprob": -0.21972830084305775, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.03404952585697174}, {"id": 317, "seek": 136774, "start": 1391.94, "end": 1392.98, "text": " we want.", "tokens": [51574, 321, 528, 13, 51626], "temperature": 0.0, "avg_logprob": -0.21972830084305775, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.03404952585697174}, {"id": 318, "seek": 139298, "start": 1392.98, "end": 1401.1, "text": " And we know from the, you know, testing we did, that sometimes it's out by like, in this", "tokens": [50364, 400, 321, 458, 490, 264, 11, 291, 458, 11, 4997, 321, 630, 11, 300, 2171, 309, 311, 484, 538, 411, 11, 294, 341, 50770], "temperature": 0.0, "avg_logprob": -0.27565508003694467, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.103706955909729}, {"id": 319, "seek": 139298, "start": 1401.1, "end": 1403.02, "text": " case, 0.02.", "tokens": [50770, 1389, 11, 1958, 13, 12756, 13, 50866], "temperature": 0.0, "avg_logprob": -0.27565508003694467, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.103706955909729}, {"id": 320, "seek": 139298, "start": 1403.02, "end": 1407.98, "text": " And I guess if you're out consistently, sometimes it's, yeah, got to end up not removing all", "tokens": [50866, 400, 286, 2041, 498, 291, 434, 484, 14961, 11, 2171, 309, 311, 11, 1338, 11, 658, 281, 917, 493, 406, 12720, 439, 51114], "temperature": 0.0, "avg_logprob": -0.27565508003694467, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.103706955909729}, {"id": 321, "seek": 139298, "start": 1407.98, "end": 1409.9, "text": " the noise.", "tokens": [51114, 264, 5658, 13, 51210], "temperature": 0.0, "avg_logprob": -0.27565508003694467, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.103706955909729}, {"id": 322, "seek": 139298, "start": 1409.9, "end": 1417.7, "text": " So the change I made was to the DDIM step, which is here.", "tokens": [51210, 407, 264, 1319, 286, 1027, 390, 281, 264, 413, 3085, 44, 1823, 11, 597, 307, 510, 13, 51600], "temperature": 0.0, "avg_logprob": -0.27565508003694467, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.103706955909729}, {"id": 323, "seek": 141770, "start": 1417.7, "end": 1423.42, "text": " And let me just copy this and get rid of the, I mean, to that sections just to make", "tokens": [50364, 400, 718, 385, 445, 5055, 341, 293, 483, 3973, 295, 264, 11, 286, 914, 11, 281, 300, 10863, 445, 281, 652, 50650], "temperature": 0.0, "avg_logprob": -0.3455075117257925, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.06370245665311813}, {"id": 324, "seek": 141770, "start": 1423.42, "end": 1426.3, "text": " it a bit easier to read.", "tokens": [50650, 309, 257, 857, 3571, 281, 1401, 13, 50794], "temperature": 0.0, "avg_logprob": -0.3455075117257925, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.06370245665311813}, {"id": 325, "seek": 141770, "start": 1426.3, "end": 1427.6200000000001, "text": " Okay.", "tokens": [50794, 1033, 13, 50860], "temperature": 0.0, "avg_logprob": -0.3455075117257925, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.06370245665311813}, {"id": 326, "seek": 141770, "start": 1427.6200000000001, "end": 1433.66, "text": " So the DDIM step, this is the normal DDIM step.", "tokens": [50860, 407, 264, 413, 3085, 44, 1823, 11, 341, 307, 264, 2710, 413, 3085, 44, 1823, 13, 51162], "temperature": 0.0, "avg_logprob": -0.3455075117257925, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.06370245665311813}, {"id": 327, "seek": 141770, "start": 1433.66, "end": 1434.66, "text": " Okay.", "tokens": [51162, 1033, 13, 51212], "temperature": 0.0, "avg_logprob": -0.3455075117257925, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.06370245665311813}, {"id": 328, "seek": 141770, "start": 1434.66, "end": 1436.02, "text": " And so step one is the same.", "tokens": [51212, 400, 370, 1823, 472, 307, 264, 912, 13, 51280], "temperature": 0.0, "avg_logprob": -0.3455075117257925, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.06370245665311813}, {"id": 329, "seek": 141770, "start": 1436.02, "end": 1438.7, "text": " So don't worry about that, because it's the same as we've seen before.", "tokens": [51280, 407, 500, 380, 3292, 466, 300, 11, 570, 309, 311, 264, 912, 382, 321, 600, 1612, 949, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3455075117257925, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.06370245665311813}, {"id": 330, "seek": 141770, "start": 1438.7, "end": 1445.6200000000001, "text": " But what I did was I actually used my T model.", "tokens": [51414, 583, 437, 286, 630, 390, 286, 767, 1143, 452, 314, 2316, 13, 51760], "temperature": 0.0, "avg_logprob": -0.3455075117257925, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.06370245665311813}, {"id": 331, "seek": 144562, "start": 1445.62, "end": 1450.9399999999998, "text": " So I passed the noised image into my T model, which is actually an alpha bar model, to get", "tokens": [50364, 407, 286, 4678, 264, 572, 2640, 3256, 666, 452, 314, 2316, 11, 597, 307, 767, 364, 8961, 2159, 2316, 11, 281, 483, 50630], "temperature": 0.0, "avg_logprob": -0.24200551534436412, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.007343592122197151}, {"id": 332, "seek": 144562, "start": 1450.9399999999998, "end": 1453.54, "text": " the predicted alpha bar.", "tokens": [50630, 264, 19147, 8961, 2159, 13, 50760], "temperature": 0.0, "avg_logprob": -0.24200551534436412, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.007343592122197151}, {"id": 333, "seek": 144562, "start": 1453.54, "end": 1457.6, "text": " And this is, remember, the predicted alpha bar for each image.", "tokens": [50760, 400, 341, 307, 11, 1604, 11, 264, 19147, 8961, 2159, 337, 1184, 3256, 13, 50963], "temperature": 0.0, "avg_logprob": -0.24200551534436412, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.007343592122197151}, {"id": 334, "seek": 144562, "start": 1457.6, "end": 1462.6599999999999, "text": " Because we know from here that sometimes, so sometimes it did a pretty good job, right?", "tokens": [50963, 1436, 321, 458, 490, 510, 300, 2171, 11, 370, 2171, 309, 630, 257, 1238, 665, 1691, 11, 558, 30, 51216], "temperature": 0.0, "avg_logprob": -0.24200551534436412, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.007343592122197151}, {"id": 335, "seek": 144562, "start": 1462.6599999999999, "end": 1464.82, "text": " But sometimes it didn't.", "tokens": [51216, 583, 2171, 309, 994, 380, 13, 51324], "temperature": 0.0, "avg_logprob": -0.24200551534436412, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.007343592122197151}, {"id": 336, "seek": 144562, "start": 1464.82, "end": 1470.62, "text": " So I felt like, okay, we need a predicted alpha bar for each image.", "tokens": [51324, 407, 286, 2762, 411, 11, 1392, 11, 321, 643, 257, 19147, 8961, 2159, 337, 1184, 3256, 13, 51614], "temperature": 0.0, "avg_logprob": -0.24200551534436412, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.007343592122197151}, {"id": 337, "seek": 147062, "start": 1470.62, "end": 1479.1599999999999, "text": " What I then discovered is sometimes that could be like really too low, right?", "tokens": [50364, 708, 286, 550, 6941, 307, 2171, 300, 727, 312, 411, 534, 886, 2295, 11, 558, 30, 50791], "temperature": 0.0, "avg_logprob": -0.2288869751824273, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.005383726675063372}, {"id": 338, "seek": 147062, "start": 1479.1599999999999, "end": 1481.52, "text": " So what I wanted to make sure was it wasn't too crazy.", "tokens": [50791, 407, 437, 286, 1415, 281, 652, 988, 390, 309, 2067, 380, 886, 3219, 13, 50909], "temperature": 0.0, "avg_logprob": -0.2288869751824273, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.005383726675063372}, {"id": 339, "seek": 147062, "start": 1481.52, "end": 1487.5, "text": " So I then found the median for a mini batch of all the predicted alpha bars, and I clamped", "tokens": [50909, 407, 286, 550, 1352, 264, 26779, 337, 257, 8382, 15245, 295, 439, 264, 19147, 8961, 10228, 11, 293, 286, 17690, 292, 51208], "temperature": 0.0, "avg_logprob": -0.2288869751824273, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.005383726675063372}, {"id": 340, "seek": 147062, "start": 1487.5, "end": 1493.62, "text": " it to not be too far away from the median.", "tokens": [51208, 309, 281, 406, 312, 886, 1400, 1314, 490, 264, 26779, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2288869751824273, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.005383726675063372}, {"id": 341, "seek": 149362, "start": 1493.62, "end": 1501.06, "text": " And so then what I did when I did my X0 hat is rather than using alpha bar T, I used the", "tokens": [50364, 400, 370, 550, 437, 286, 630, 562, 286, 630, 452, 1783, 15, 2385, 307, 2831, 813, 1228, 8961, 2159, 314, 11, 286, 1143, 264, 50736], "temperature": 0.0, "avg_logprob": -0.22410809862744677, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.02634064480662346}, {"id": 342, "seek": 149362, "start": 1501.06, "end": 1508.6, "text": " estimated alpha bar T for each image clamped to be not too far away from the median.", "tokens": [50736, 14109, 8961, 2159, 314, 337, 1184, 3256, 17690, 292, 281, 312, 406, 886, 1400, 1314, 490, 264, 26779, 13, 51113], "temperature": 0.0, "avg_logprob": -0.22410809862744677, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.02634064480662346}, {"id": 343, "seek": 149362, "start": 1508.6, "end": 1512.9799999999998, "text": " And so this way it was updating it based on the amount of noise that actually seems to", "tokens": [51113, 400, 370, 341, 636, 309, 390, 25113, 309, 2361, 322, 264, 2372, 295, 5658, 300, 767, 2544, 281, 51332], "temperature": 0.0, "avg_logprob": -0.22410809862744677, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.02634064480662346}, {"id": 344, "seek": 149362, "start": 1512.9799999999998, "end": 1520.08, "text": " be left behind, rather than the assumed amount of noise that should be left behind, you know,", "tokens": [51332, 312, 1411, 2261, 11, 2831, 813, 264, 15895, 2372, 295, 5658, 300, 820, 312, 1411, 2261, 11, 291, 458, 11, 51687], "temperature": 0.0, "avg_logprob": -0.22410809862744677, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.02634064480662346}, {"id": 345, "seek": 152008, "start": 1520.08, "end": 1523.76, "text": " if we assume it's removed the correct amount.", "tokens": [50364, 498, 321, 6552, 309, 311, 7261, 264, 3006, 2372, 13, 50548], "temperature": 0.0, "avg_logprob": -0.3044564521918863, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.091373510658741}, {"id": 346, "seek": 152008, "start": 1523.76, "end": 1526.84, "text": " And then everything else is the same.", "tokens": [50548, 400, 550, 1203, 1646, 307, 264, 912, 13, 50702], "temperature": 0.0, "avg_logprob": -0.3044564521918863, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.091373510658741}, {"id": 347, "seek": 152008, "start": 1526.84, "end": 1535.4399999999998, "text": " So when I did that, it's like, whoa, made all the difference.", "tokens": [50702, 407, 562, 286, 630, 300, 11, 309, 311, 411, 11, 13310, 11, 1027, 439, 264, 2649, 13, 51132], "temperature": 0.0, "avg_logprob": -0.3044564521918863, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.091373510658741}, {"id": 348, "seek": 152008, "start": 1535.4399999999998, "end": 1537.26, "text": " And here it is.", "tokens": [51132, 400, 510, 309, 307, 13, 51223], "temperature": 0.0, "avg_logprob": -0.3044564521918863, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.091373510658741}, {"id": 349, "seek": 152008, "start": 1537.26, "end": 1540.54, "text": " They are beautiful pieces of clothing.", "tokens": [51223, 814, 366, 2238, 3755, 295, 11502, 13, 51387], "temperature": 0.0, "avg_logprob": -0.3044564521918863, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.091373510658741}, {"id": 350, "seek": 154054, "start": 1540.54, "end": 1553.26, "text": " So 3.88 versus 3.2, that's possibly close enough.", "tokens": [50364, 407, 805, 13, 16919, 5717, 805, 13, 17, 11, 300, 311, 6264, 1998, 1547, 13, 51000], "temperature": 0.0, "avg_logprob": -0.2681769816080729, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.06652796268463135}, {"id": 351, "seek": 154054, "start": 1553.26, "end": 1557.6599999999999, "text": " Like I'd have to run it a few times, you know, my guess is maybe it's a tiny bit worse, but", "tokens": [51000, 1743, 286, 1116, 362, 281, 1190, 309, 257, 1326, 1413, 11, 291, 458, 11, 452, 2041, 307, 1310, 309, 311, 257, 5870, 857, 5324, 11, 457, 51220], "temperature": 0.0, "avg_logprob": -0.2681769816080729, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.06652796268463135}, {"id": 352, "seek": 154054, "start": 1557.6599999999999, "end": 1560.42, "text": " it's pretty close.", "tokens": [51220, 309, 311, 1238, 1998, 13, 51358], "temperature": 0.0, "avg_logprob": -0.2681769816080729, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.06652796268463135}, {"id": 353, "seek": 154054, "start": 1560.42, "end": 1570.48, "text": " But like this definitely gives me some encouragement that, you know, even though this is like something", "tokens": [51358, 583, 411, 341, 2138, 2709, 385, 512, 25683, 300, 11, 291, 458, 11, 754, 1673, 341, 307, 411, 746, 51861], "temperature": 0.0, "avg_logprob": -0.2681769816080729, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.06652796268463135}, {"id": 354, "seek": 157048, "start": 1571.42, "end": 1574.04, "text": " I just did in a couple of days, where else the kind of the with T approaches have been", "tokens": [50411, 286, 445, 630, 294, 257, 1916, 295, 1708, 11, 689, 1646, 264, 733, 295, 264, 365, 314, 11587, 362, 668, 50542], "temperature": 0.0, "avg_logprob": -0.3260246364549659, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.003272948320955038}, {"id": 355, "seek": 157048, "start": 1574.04, "end": 1579.1200000000001, "text": " developed since 2015, and we're now in 2023.", "tokens": [50542, 4743, 1670, 7546, 11, 293, 321, 434, 586, 294, 44377, 13, 50796], "temperature": 0.0, "avg_logprob": -0.3260246364549659, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.003272948320955038}, {"id": 356, "seek": 157048, "start": 1579.1200000000001, "end": 1587.84, "text": " You know, I would expect it's quite likely that these kind of like no T approaches could", "tokens": [50796, 509, 458, 11, 286, 576, 2066, 309, 311, 1596, 3700, 300, 613, 733, 295, 411, 572, 314, 11587, 727, 51232], "temperature": 0.0, "avg_logprob": -0.3260246364549659, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.003272948320955038}, {"id": 357, "seek": 157048, "start": 1587.84, "end": 1595.14, "text": " eventually surpass the T based approaches.", "tokens": [51232, 4728, 27650, 264, 314, 2361, 11587, 13, 51597], "temperature": 0.0, "avg_logprob": -0.3260246364549659, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.003272948320955038}, {"id": 358, "seek": 157048, "start": 1595.14, "end": 1597.72, "text": " And like one thing that definitely makes me think like there's room to improve is if I", "tokens": [51597, 400, 411, 472, 551, 300, 2138, 1669, 385, 519, 411, 456, 311, 1808, 281, 3470, 307, 498, 286, 51726], "temperature": 0.0, "avg_logprob": -0.3260246364549659, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.003272948320955038}, {"id": 359, "seek": 159772, "start": 1597.8, "end": 1605.1200000000001, "text": " plot the FID or the KID for each sample during the reverse diffusion process, it actually", "tokens": [50368, 7542, 264, 479, 2777, 420, 264, 591, 2777, 337, 1184, 6889, 1830, 264, 9943, 25242, 1399, 11, 309, 767, 50734], "temperature": 0.0, "avg_logprob": -0.363603349165483, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.13112273812294006}, {"id": 360, "seek": 159772, "start": 1605.1200000000001, "end": 1606.1200000000001, "text": " gets worse for a while.", "tokens": [50734, 2170, 5324, 337, 257, 1339, 13, 50784], "temperature": 0.0, "avg_logprob": -0.363603349165483, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.13112273812294006}, {"id": 361, "seek": 159772, "start": 1606.1200000000001, "end": 1609.76, "text": " I'm like, okay, well, that's a bad sign.", "tokens": [50784, 286, 478, 411, 11, 1392, 11, 731, 11, 300, 311, 257, 1578, 1465, 13, 50966], "temperature": 0.0, "avg_logprob": -0.363603349165483, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.13112273812294006}, {"id": 362, "seek": 159772, "start": 1609.76, "end": 1615.04, "text": " I have no idea why that's happening, but it's a sign that, you know, if we could improve", "tokens": [50966, 286, 362, 572, 1558, 983, 300, 311, 2737, 11, 457, 309, 311, 257, 1465, 300, 11, 291, 458, 11, 498, 321, 727, 3470, 51230], "temperature": 0.0, "avg_logprob": -0.363603349165483, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.13112273812294006}, {"id": 363, "seek": 159772, "start": 1615.04, "end": 1618.08, "text": " each step, then one would assume we could get better than 3.8.", "tokens": [51230, 1184, 1823, 11, 550, 472, 576, 6552, 321, 727, 483, 1101, 813, 805, 13, 23, 13, 51382], "temperature": 0.0, "avg_logprob": -0.363603349165483, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.13112273812294006}, {"id": 364, "seek": 159772, "start": 1618.08, "end": 1625.16, "text": " So yeah, Tanish, Gautam, do you have any thoughts about that?", "tokens": [51382, 407, 1338, 11, 314, 7524, 11, 460, 1375, 335, 11, 360, 291, 362, 604, 4598, 466, 300, 30, 51736], "temperature": 0.0, "avg_logprob": -0.363603349165483, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.13112273812294006}, {"id": 365, "seek": 162516, "start": 1625.16, "end": 1629.96, "text": " Any more questions or comments?", "tokens": [50364, 2639, 544, 1651, 420, 3053, 30, 50604], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 366, "seek": 162516, "start": 1629.96, "end": 1633.48, "text": " Maybe to just like to highlight the research process a little bit.", "tokens": [50604, 2704, 281, 445, 411, 281, 5078, 264, 2132, 1399, 257, 707, 857, 13, 50780], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 367, "seek": 162516, "start": 1633.48, "end": 1637.8400000000001, "text": " It wasn't like this linear thing of like, oh, here's this issue not performing as well", "tokens": [50780, 467, 2067, 380, 411, 341, 8213, 551, 295, 411, 11, 1954, 11, 510, 311, 341, 2734, 406, 10205, 382, 731, 50998], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 368, "seek": 162516, "start": 1637.8400000000001, "end": 1638.8400000000001, "text": " as we thought.", "tokens": [50998, 382, 321, 1194, 13, 51048], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 369, "seek": 162516, "start": 1638.8400000000001, "end": 1639.8400000000001, "text": " Oh, here's the fix.", "tokens": [51048, 876, 11, 510, 311, 264, 3191, 13, 51098], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 370, "seek": 162516, "start": 1639.8400000000001, "end": 1640.8400000000001, "text": " We just clamped this.", "tokens": [51098, 492, 445, 17690, 292, 341, 13, 51148], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 371, "seek": 162516, "start": 1640.8400000000001, "end": 1646.6000000000001, "text": " You know, this was like multiple days of like discussing and like Jeremy saying like, you", "tokens": [51148, 509, 458, 11, 341, 390, 411, 3866, 1708, 295, 411, 10850, 293, 411, 17809, 1566, 411, 11, 291, 51436], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 372, "seek": 162516, "start": 1646.6000000000001, "end": 1647.6000000000001, "text": " know, I'm tearing my hair out.", "tokens": [51436, 458, 11, 286, 478, 29401, 452, 2578, 484, 13, 51486], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 373, "seek": 162516, "start": 1647.6000000000001, "end": 1648.6000000000001, "text": " Do you guys have any ideas?", "tokens": [51486, 1144, 291, 1074, 362, 604, 3487, 30, 51536], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 374, "seek": 162516, "start": 1648.6000000000001, "end": 1649.6000000000001, "text": " And oh, what about this?", "tokens": [51536, 400, 1954, 11, 437, 466, 341, 30, 51586], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 375, "seek": 162516, "start": 1649.6000000000001, "end": 1652.0800000000002, "text": " And oh, I noticed in the team paper, they do this clamping.", "tokens": [51586, 400, 1954, 11, 286, 5694, 294, 264, 1469, 3035, 11, 436, 360, 341, 17690, 278, 13, 51710], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 376, "seek": 162516, "start": 1652.0800000000002, "end": 1653.0800000000002, "text": " Maybe that'll help.", "tokens": [51710, 2704, 300, 603, 854, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2972824838426378, "compression_ratio": 1.6928327645051195, "no_speech_prob": 0.7335910201072693}, {"id": 377, "seek": 165308, "start": 1653.08, "end": 1656.1999999999998, "text": " Yeah, so there's a lot of back and forth and also a lot of like, you saw the code that", "tokens": [50364, 865, 11, 370, 456, 311, 257, 688, 295, 646, 293, 5220, 293, 611, 257, 688, 295, 411, 11, 291, 1866, 264, 3089, 300, 50520], "temperature": 0.0, "avg_logprob": -0.3437699470811218, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.7715334296226501}, {"id": 378, "seek": 165308, "start": 1656.1999999999998, "end": 1662.8, "text": " was commented out there, prints, XT.min, XT.max, alpha bar, pred, you know, just like seeing,", "tokens": [50520, 390, 26940, 484, 456, 11, 22305, 11, 1783, 51, 13, 2367, 11, 1783, 51, 13, 41167, 11, 8961, 2159, 11, 3852, 11, 291, 458, 11, 445, 411, 2577, 11, 50850], "temperature": 0.0, "avg_logprob": -0.3437699470811218, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.7715334296226501}, {"id": 379, "seek": 165308, "start": 1662.8, "end": 1663.8, "text": " oh, okay.", "tokens": [50850, 1954, 11, 1392, 13, 50900], "temperature": 0.0, "avg_logprob": -0.3437699470811218, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.7715334296226501}, {"id": 380, "seek": 165308, "start": 1663.8, "end": 1666.6, "text": " You know, my average prediction is about what I expect, but sometimes the middle of the", "tokens": [50900, 509, 458, 11, 452, 4274, 17630, 307, 466, 437, 286, 2066, 11, 457, 2171, 264, 2808, 295, 264, 51040], "temperature": 0.0, "avg_logprob": -0.3437699470811218, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.7715334296226501}, {"id": 381, "seek": 165308, "start": 1666.6, "end": 1674.56, "text": " max goes, you know, two, three, eight, 16, 150, 212 million, infinity, you know, maybe", "tokens": [51040, 11469, 1709, 11, 291, 458, 11, 732, 11, 1045, 11, 3180, 11, 3165, 11, 8451, 11, 568, 4762, 2459, 11, 13202, 11, 291, 458, 11, 1310, 51438], "temperature": 0.0, "avg_logprob": -0.3437699470811218, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.7715334296226501}, {"id": 382, "seek": 165308, "start": 1674.56, "end": 1677.56, "text": " like one or two little values that would just skyrocket out.", "tokens": [51438, 411, 472, 420, 732, 707, 4190, 300, 576, 445, 5443, 37463, 484, 13, 51588], "temperature": 0.0, "avg_logprob": -0.3437699470811218, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.7715334296226501}, {"id": 383, "seek": 165308, "start": 1677.56, "end": 1678.56, "text": " Yeah.", "tokens": [51588, 865, 13, 51638], "temperature": 0.0, "avg_logprob": -0.3437699470811218, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.7715334296226501}, {"id": 384, "seek": 167856, "start": 1678.56, "end": 1682.72, "text": " So that kind of like debugging and exploring and printing things out.", "tokens": [50364, 407, 300, 733, 295, 411, 45592, 293, 12736, 293, 14699, 721, 484, 13, 50572], "temperature": 0.0, "avg_logprob": -0.330597494250146, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.48812684416770935}, {"id": 385, "seek": 167856, "start": 1682.72, "end": 1688.6399999999999, "text": " And actually our initial discussions about this idea, I kind of said to you guys before", "tokens": [50572, 400, 767, 527, 5883, 11088, 466, 341, 1558, 11, 286, 733, 295, 848, 281, 291, 1074, 949, 50868], "temperature": 0.0, "avg_logprob": -0.330597494250146, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.48812684416770935}, {"id": 386, "seek": 167856, "start": 1688.6399999999999, "end": 1694.48, "text": " lesson one of part two, I said like, it feels to me like we shouldn't need the T thing.", "tokens": [50868, 6898, 472, 295, 644, 732, 11, 286, 848, 411, 11, 309, 3417, 281, 385, 411, 321, 4659, 380, 643, 264, 314, 551, 13, 51160], "temperature": 0.0, "avg_logprob": -0.330597494250146, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.48812684416770935}, {"id": 387, "seek": 167856, "start": 1694.48, "end": 1700.6399999999999, "text": " And so it's actually been like mumbling away in the background for months.", "tokens": [51160, 400, 370, 309, 311, 767, 668, 411, 275, 14188, 1314, 294, 264, 3678, 337, 2493, 13, 51468], "temperature": 0.0, "avg_logprob": -0.330597494250146, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.48812684416770935}, {"id": 388, "seek": 167856, "start": 1700.6399999999999, "end": 1701.6399999999999, "text": " Yeah.", "tokens": [51468, 865, 13, 51518], "temperature": 0.0, "avg_logprob": -0.330597494250146, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.48812684416770935}, {"id": 389, "seek": 167856, "start": 1701.6399999999999, "end": 1706.76, "text": " And I guess, I mean, we should also mention, we have tried this, like a friend of ours", "tokens": [51518, 400, 286, 2041, 11, 286, 914, 11, 321, 820, 611, 2152, 11, 321, 362, 3031, 341, 11, 411, 257, 1277, 295, 11896, 51774], "temperature": 0.0, "avg_logprob": -0.330597494250146, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.48812684416770935}, {"id": 390, "seek": 170676, "start": 1706.76, "end": 1710.0, "text": " trained a no T version of stable diffusion for us.", "tokens": [50364, 8895, 257, 572, 314, 3037, 295, 8351, 25242, 337, 505, 13, 50526], "temperature": 0.0, "avg_logprob": -0.30909072187610137, "compression_ratio": 1.623728813559322, "no_speech_prob": 0.614963173866272}, {"id": 391, "seek": 170676, "start": 1710.0, "end": 1711.12, "text": " And we did the same sort of thing.", "tokens": [50526, 400, 321, 630, 264, 912, 1333, 295, 551, 13, 50582], "temperature": 0.0, "avg_logprob": -0.30909072187610137, "compression_ratio": 1.623728813559322, "no_speech_prob": 0.614963173866272}, {"id": 392, "seek": 170676, "start": 1711.12, "end": 1715.6, "text": " I trained a pretty bad T predictor and it sort of generates samples.", "tokens": [50582, 286, 8895, 257, 1238, 1578, 314, 6069, 284, 293, 309, 1333, 295, 23815, 10938, 13, 50806], "temperature": 0.0, "avg_logprob": -0.30909072187610137, "compression_ratio": 1.623728813559322, "no_speech_prob": 0.614963173866272}, {"id": 393, "seek": 170676, "start": 1715.6, "end": 1720.96, "text": " So we're not like focusing on that large scale stuff yet, but it is fun to like, here we", "tokens": [50806, 407, 321, 434, 406, 411, 8416, 322, 300, 2416, 4373, 1507, 1939, 11, 457, 309, 307, 1019, 281, 411, 11, 510, 321, 51074], "temperature": 0.0, "avg_logprob": -0.30909072187610137, "compression_ratio": 1.623728813559322, "no_speech_prob": 0.614963173866272}, {"id": 394, "seek": 170676, "start": 1720.96, "end": 1723.76, "text": " are again, got this idea from Fashion Industry.", "tokens": [51074, 366, 797, 11, 658, 341, 1558, 490, 32782, 2333, 84, 372, 627, 13, 51214], "temperature": 0.0, "avg_logprob": -0.30909072187610137, "compression_ratio": 1.623728813559322, "no_speech_prob": 0.614963173866272}, {"id": 395, "seek": 170676, "start": 1723.76, "end": 1727.92, "text": " We are trying these out on some bigger models and seeing, okay, this does seem like maybe", "tokens": [51214, 492, 366, 1382, 613, 484, 322, 512, 3801, 5245, 293, 2577, 11, 1392, 11, 341, 775, 1643, 411, 1310, 51422], "temperature": 0.0, "avg_logprob": -0.30909072187610137, "compression_ratio": 1.623728813559322, "no_speech_prob": 0.614963173866272}, {"id": 396, "seek": 170676, "start": 1727.92, "end": 1728.96, "text": " it'll work.", "tokens": [51422, 309, 603, 589, 13, 51474], "temperature": 0.0, "avg_logprob": -0.30909072187610137, "compression_ratio": 1.623728813559322, "no_speech_prob": 0.614963173866272}, {"id": 397, "seek": 170676, "start": 1728.96, "end": 1732.2, "text": " And so down the line, that future plan is to say, let's actually, you know, spend the", "tokens": [51474, 400, 370, 760, 264, 1622, 11, 300, 2027, 1393, 307, 281, 584, 11, 718, 311, 767, 11, 291, 458, 11, 3496, 264, 51636], "temperature": 0.0, "avg_logprob": -0.30909072187610137, "compression_ratio": 1.623728813559322, "no_speech_prob": 0.614963173866272}, {"id": 398, "seek": 173220, "start": 1732.2, "end": 1735.92, "text": " time, train a proper model and see, yeah, see how well that does.", "tokens": [50364, 565, 11, 3847, 257, 2296, 2316, 293, 536, 11, 1338, 11, 536, 577, 731, 300, 775, 13, 50550], "temperature": 0.0, "avg_logprob": -0.30466528698406387, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.6616348624229431}, {"id": 399, "seek": 173220, "start": 1735.92, "end": 1736.92, "text": " If it's interesting.", "tokens": [50550, 759, 309, 311, 1880, 13, 50600], "temperature": 0.0, "avg_logprob": -0.30466528698406387, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.6616348624229431}, {"id": 400, "seek": 173220, "start": 1736.92, "end": 1738.96, "text": " You say a friend of ours, we can be more specific.", "tokens": [50600, 509, 584, 257, 1277, 295, 11896, 11, 321, 393, 312, 544, 2685, 13, 50702], "temperature": 0.0, "avg_logprob": -0.30466528698406387, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.6616348624229431}, {"id": 401, "seek": 173220, "start": 1738.96, "end": 1744.24, "text": " It's Robin, one of the two lead authors of the stable diffusion paper who actually has", "tokens": [50702, 467, 311, 16533, 11, 472, 295, 264, 732, 1477, 16552, 295, 264, 8351, 25242, 3035, 567, 767, 575, 50966], "temperature": 0.0, "avg_logprob": -0.30466528698406387, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.6616348624229431}, {"id": 402, "seek": 173220, "start": 1744.24, "end": 1752.04, "text": " been fine tuning a real stable diffusion model, which is without T and it's looking super", "tokens": [50966, 668, 2489, 15164, 257, 957, 8351, 25242, 2316, 11, 597, 307, 1553, 314, 293, 309, 311, 1237, 1687, 51356], "temperature": 0.0, "avg_logprob": -0.30466528698406387, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.6616348624229431}, {"id": 403, "seek": 173220, "start": 1752.04, "end": 1753.04, "text": " encouraging.", "tokens": [51356, 14580, 13, 51406], "temperature": 0.0, "avg_logprob": -0.30466528698406387, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.6616348624229431}, {"id": 404, "seek": 173220, "start": 1753.04, "end": 1759.96, "text": " So yeah, that'll be fun to play with, with this new, you know, we'll have to train a", "tokens": [51406, 407, 1338, 11, 300, 603, 312, 1019, 281, 862, 365, 11, 365, 341, 777, 11, 291, 458, 11, 321, 603, 362, 281, 3847, 257, 51752], "temperature": 0.0, "avg_logprob": -0.30466528698406387, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.6616348624229431}, {"id": 405, "seek": 175996, "start": 1759.96, "end": 1762.2, "text": " T predictor for that.", "tokens": [50364, 314, 6069, 284, 337, 300, 13, 50476], "temperature": 0.0, "avg_logprob": -0.3166036393907335, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.09531592577695847}, {"id": 406, "seek": 175996, "start": 1762.2, "end": 1765.2, "text": " See how it looks.", "tokens": [50476, 3008, 577, 309, 1542, 13, 50626], "temperature": 0.0, "avg_logprob": -0.3166036393907335, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.09531592577695847}, {"id": 407, "seek": 175996, "start": 1765.2, "end": 1766.2, "text": " Yeah.", "tokens": [50626, 865, 13, 50676], "temperature": 0.0, "avg_logprob": -0.3166036393907335, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.09531592577695847}, {"id": 408, "seek": 175996, "start": 1766.2, "end": 1768.96, "text": " All right.", "tokens": [50676, 1057, 558, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3166036393907335, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.09531592577695847}, {"id": 409, "seek": 175996, "start": 1768.96, "end": 1775.14, "text": " So I guess the other area we've been talking about kind of doing some research on is this", "tokens": [50814, 407, 286, 2041, 264, 661, 1859, 321, 600, 668, 1417, 466, 733, 295, 884, 512, 2132, 322, 307, 341, 51123], "temperature": 0.0, "avg_logprob": -0.3166036393907335, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.09531592577695847}, {"id": 410, "seek": 175996, "start": 1775.14, "end": 1782.64, "text": " weird thing that came up over the last few weeks where our bug in the DDPM implementation,", "tokens": [51123, 3657, 551, 300, 1361, 493, 670, 264, 1036, 1326, 3259, 689, 527, 7426, 294, 264, 413, 11373, 44, 11420, 11, 51498], "temperature": 0.0, "avg_logprob": -0.3166036393907335, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.09531592577695847}, {"id": 411, "seek": 175996, "start": 1782.64, "end": 1789.24, "text": " where we accidentally weren't doing it from minus one to one for the input range, it turned", "tokens": [51498, 689, 321, 15715, 4999, 380, 884, 309, 490, 3175, 472, 281, 472, 337, 264, 4846, 3613, 11, 309, 3574, 51828], "temperature": 0.0, "avg_logprob": -0.3166036393907335, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.09531592577695847}, {"id": 412, "seek": 178924, "start": 1789.28, "end": 1795.28, "text": " out that actually being from minus one to one wasn't a very good idea anyway.", "tokens": [50366, 484, 300, 767, 885, 490, 3175, 472, 281, 472, 2067, 380, 257, 588, 665, 1558, 4033, 13, 50666], "temperature": 0.0, "avg_logprob": -0.285222159491645, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.004904925357550383}, {"id": 413, "seek": 178924, "start": 1795.28, "end": 1801.32, "text": " And so we ended up centering it as being from minus 0.5 to 0.5.", "tokens": [50666, 400, 370, 321, 4590, 493, 1489, 1794, 309, 382, 885, 490, 3175, 1958, 13, 20, 281, 1958, 13, 20, 13, 50968], "temperature": 0.0, "avg_logprob": -0.285222159491645, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.004904925357550383}, {"id": 414, "seek": 178924, "start": 1801.32, "end": 1807.84, "text": " And Jono and Tanishka have managed to actually find a paper, well, I say find a paper, a", "tokens": [50968, 400, 7745, 78, 293, 314, 7524, 2330, 362, 6453, 281, 767, 915, 257, 3035, 11, 731, 11, 286, 584, 915, 257, 3035, 11, 257, 51294], "temperature": 0.0, "avg_logprob": -0.285222159491645, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.004904925357550383}, {"id": 415, "seek": 178924, "start": 1807.84, "end": 1816.68, "text": " paper has come out in the last 24 hours, which has coincidentally cast some light on this", "tokens": [51294, 3035, 575, 808, 484, 294, 264, 1036, 4022, 2496, 11, 597, 575, 13001, 36578, 4193, 512, 1442, 322, 341, 51736], "temperature": 0.0, "avg_logprob": -0.285222159491645, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.004904925357550383}, {"id": 416, "seek": 181668, "start": 1816.68, "end": 1821.88, "text": " and has also cited a paper that we weren't aware of, which was not released in the last", "tokens": [50364, 293, 575, 611, 30134, 257, 3035, 300, 321, 4999, 380, 3650, 295, 11, 597, 390, 406, 4736, 294, 264, 1036, 50624], "temperature": 0.0, "avg_logprob": -0.27882854320384837, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.3922455310821533}, {"id": 417, "seek": 181668, "start": 1821.88, "end": 1822.88, "text": " 24 hours.", "tokens": [50624, 4022, 2496, 13, 50674], "temperature": 0.0, "avg_logprob": -0.27882854320384837, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.3922455310821533}, {"id": 418, "seek": 181668, "start": 1822.88, "end": 1825.0800000000002, "text": " So Jono, are you going to tell us a bit about that?", "tokens": [50674, 407, 7745, 78, 11, 366, 291, 516, 281, 980, 505, 257, 857, 466, 300, 30, 50784], "temperature": 0.0, "avg_logprob": -0.27882854320384837, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.3922455310821533}, {"id": 419, "seek": 181668, "start": 1825.0800000000002, "end": 1826.4, "text": " Yeah, no, sure.", "tokens": [50784, 865, 11, 572, 11, 988, 13, 50850], "temperature": 0.0, "avg_logprob": -0.27882854320384837, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.3922455310821533}, {"id": 420, "seek": 181668, "start": 1826.4, "end": 1828.48, "text": " I can do that.", "tokens": [50850, 286, 393, 360, 300, 13, 50954], "temperature": 0.0, "avg_logprob": -0.27882854320384837, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.3922455310821533}, {"id": 421, "seek": 181668, "start": 1828.48, "end": 1832.8, "text": " So it's funny, this was such perfect timing because I actually got up early this morning", "tokens": [50954, 407, 309, 311, 4074, 11, 341, 390, 1270, 2176, 10822, 570, 286, 767, 658, 493, 2440, 341, 2446, 51170], "temperature": 0.0, "avg_logprob": -0.27882854320384837, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.3922455310821533}, {"id": 422, "seek": 181668, "start": 1832.8, "end": 1838.3200000000002, "text": " planning to run with the different input scalings and the cosine schedule that Jeremy was showing", "tokens": [51170, 5038, 281, 1190, 365, 264, 819, 4846, 15664, 1109, 293, 264, 23565, 7567, 300, 17809, 390, 4099, 51446], "temperature": 0.0, "avg_logprob": -0.27882854320384837, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.3922455310821533}, {"id": 423, "seek": 181668, "start": 1838.3200000000002, "end": 1839.92, "text": " and some of the other schedulers we look at.", "tokens": [51446, 293, 512, 295, 264, 661, 12000, 433, 321, 574, 412, 13, 51526], "temperature": 0.0, "avg_logprob": -0.27882854320384837, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.3922455310821533}, {"id": 424, "seek": 181668, "start": 1839.92, "end": 1843.3600000000001, "text": " I thought it might be nice for the lesson to have a little plot of like, what is the", "tokens": [51526, 286, 1194, 309, 1062, 312, 1481, 337, 264, 6898, 281, 362, 257, 707, 7542, 295, 411, 11, 437, 307, 264, 51698], "temperature": 0.0, "avg_logprob": -0.27882854320384837, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.3922455310821533}, {"id": 425, "seek": 184336, "start": 1843.36, "end": 1847.1599999999999, "text": " fit with these different solvers and input scalings, but it was going to be a lot of", "tokens": [50364, 3318, 365, 613, 819, 1404, 840, 293, 4846, 15664, 1109, 11, 457, 309, 390, 516, 281, 312, 257, 688, 295, 50554], "temperature": 0.0, "avg_logprob": -0.30692948868025594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.7742113471031189}, {"id": 426, "seek": 184336, "start": 1847.1599999999999, "end": 1848.1599999999999, "text": " work.", "tokens": [50554, 589, 13, 50604], "temperature": 0.0, "avg_logprob": -0.30692948868025594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.7742113471031189}, {"id": 427, "seek": 184336, "start": 1848.1599999999999, "end": 1850.36, "text": " I was like, not looking forward to doing the groundwork.", "tokens": [50604, 286, 390, 411, 11, 406, 1237, 2128, 281, 884, 264, 2727, 1902, 13, 50714], "temperature": 0.0, "avg_logprob": -0.30692948868025594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.7742113471031189}, {"id": 428, "seek": 184336, "start": 1850.36, "end": 1854.36, "text": " And then Tanishka sent me this paper, which AK had just tweeted out because he reviews", "tokens": [50714, 400, 550, 314, 7524, 2330, 2279, 385, 341, 3035, 11, 597, 24789, 632, 445, 25646, 484, 570, 415, 10229, 50914], "temperature": 0.0, "avg_logprob": -0.30692948868025594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.7742113471031189}, {"id": 429, "seek": 184336, "start": 1854.36, "end": 1858.1599999999999, "text": " everything that comes up on archive every day on the importance of noise scheduling", "tokens": [50914, 1203, 300, 1487, 493, 322, 23507, 633, 786, 322, 264, 7379, 295, 5658, 29055, 51104], "temperature": 0.0, "avg_logprob": -0.30692948868025594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.7742113471031189}, {"id": 430, "seek": 184336, "start": 1858.1599999999999, "end": 1860.1599999999999, "text": " for diffusion models.", "tokens": [51104, 337, 25242, 5245, 13, 51204], "temperature": 0.0, "avg_logprob": -0.30692948868025594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.7742113471031189}, {"id": 431, "seek": 184336, "start": 1860.1599999999999, "end": 1864.32, "text": " This is by a researcher at the Google Brain team, who's also done a really cool recent", "tokens": [51204, 639, 307, 538, 257, 21751, 412, 264, 3329, 29783, 1469, 11, 567, 311, 611, 1096, 257, 534, 1627, 5162, 51412], "temperature": 0.0, "avg_logprob": -0.30692948868025594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.7742113471031189}, {"id": 432, "seek": 184336, "start": 1864.32, "end": 1868.9199999999998, "text": " paper on something called a recurrent interface network outside of the script of this lesson,", "tokens": [51412, 3035, 322, 746, 1219, 257, 18680, 1753, 9226, 3209, 2380, 295, 264, 5755, 295, 341, 6898, 11, 51642], "temperature": 0.0, "avg_logprob": -0.30692948868025594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.7742113471031189}, {"id": 433, "seek": 184336, "start": 1868.9199999999998, "end": 1870.52, "text": " but also worth checking out.", "tokens": [51642, 457, 611, 3163, 8568, 484, 13, 51722], "temperature": 0.0, "avg_logprob": -0.30692948868025594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.7742113471031189}, {"id": 434, "seek": 187052, "start": 1870.68, "end": 1876.52, "text": " Yeah, so this paper, they're hoping to study this noise scheduling and the strategies that", "tokens": [50372, 865, 11, 370, 341, 3035, 11, 436, 434, 7159, 281, 2979, 341, 5658, 29055, 293, 264, 9029, 300, 50664], "temperature": 0.0, "avg_logprob": -0.32012447464131866, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.02227875217795372}, {"id": 435, "seek": 187052, "start": 1876.52, "end": 1877.52, "text": " you take for that.", "tokens": [50664, 291, 747, 337, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.32012447464131866, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.02227875217795372}, {"id": 436, "seek": 187052, "start": 1877.52, "end": 1881.4, "text": " And they want to show that number one, noise scheduling is crucial for performance and", "tokens": [50714, 400, 436, 528, 281, 855, 300, 1230, 472, 11, 5658, 29055, 307, 11462, 337, 3389, 293, 50908], "temperature": 0.0, "avg_logprob": -0.32012447464131866, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.02227875217795372}, {"id": 437, "seek": 187052, "start": 1881.4, "end": 1884.12, "text": " the optimal one depends on the tasks.", "tokens": [50908, 264, 16252, 472, 5946, 322, 264, 9608, 13, 51044], "temperature": 0.0, "avg_logprob": -0.32012447464131866, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.02227875217795372}, {"id": 438, "seek": 187052, "start": 1884.12, "end": 1889.04, "text": " When increasing the image size, the noise scheduling that you want changes.", "tokens": [51044, 1133, 5662, 264, 3256, 2744, 11, 264, 5658, 29055, 300, 291, 528, 2962, 13, 51290], "temperature": 0.0, "avg_logprob": -0.32012447464131866, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.02227875217795372}, {"id": 439, "seek": 187052, "start": 1889.04, "end": 1894.8, "text": " And scaling the input data by some factor is a good strategy for working with this.", "tokens": [51290, 400, 21589, 264, 4846, 1412, 538, 512, 5952, 307, 257, 665, 5206, 337, 1364, 365, 341, 13, 51578], "temperature": 0.0, "avg_logprob": -0.32012447464131866, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.02227875217795372}, {"id": 440, "seek": 187052, "start": 1894.8, "end": 1897.08, "text": " And that's the bit we've been talking about, right?", "tokens": [51578, 400, 300, 311, 264, 857, 321, 600, 668, 1417, 466, 11, 558, 30, 51692], "temperature": 0.0, "avg_logprob": -0.32012447464131866, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.02227875217795372}, {"id": 441, "seek": 189708, "start": 1897.3999999999999, "end": 1900.56, "text": " Yeah, that's what we've been doing where we said, oh, do we scale from minus 0.5 to 0.5", "tokens": [50380, 865, 11, 300, 311, 437, 321, 600, 668, 884, 689, 321, 848, 11, 1954, 11, 360, 321, 4373, 490, 3175, 1958, 13, 20, 281, 1958, 13, 20, 50538], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 442, "seek": 189708, "start": 1900.56, "end": 1903.56, "text": " or minus one to one or do we normalize?", "tokens": [50538, 420, 3175, 472, 281, 472, 420, 360, 321, 2710, 1125, 30, 50688], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 443, "seek": 189708, "start": 1903.56, "end": 1908.28, "text": " And so they demonstrate the effectiveness by training a really good high resolution", "tokens": [50688, 400, 370, 436, 11698, 264, 21208, 538, 3097, 257, 534, 665, 1090, 8669, 50924], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 444, "seek": 189708, "start": 1908.28, "end": 1909.56, "text": " model on ImageNet.", "tokens": [50924, 2316, 322, 29903, 31890, 13, 50988], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 445, "seek": 189708, "start": 1909.56, "end": 1911.08, "text": " So class condition model.", "tokens": [50988, 407, 1508, 4188, 2316, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 446, "seek": 189708, "start": 1911.08, "end": 1912.08, "text": " Correct.", "tokens": [51064, 12753, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 447, "seek": 189708, "start": 1912.08, "end": 1913.56, "text": " Yeah, amazing samples.", "tokens": [51114, 865, 11, 2243, 10938, 13, 51188], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 448, "seek": 189708, "start": 1913.56, "end": 1915.56, "text": " They'll show one later.", "tokens": [51188, 814, 603, 855, 472, 1780, 13, 51288], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 449, "seek": 189708, "start": 1915.56, "end": 1916.82, "text": " So I really like this paper.", "tokens": [51288, 407, 286, 534, 411, 341, 3035, 13, 51351], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 450, "seek": 189708, "start": 1916.82, "end": 1921.36, "text": " It's very short and concise and it just gets all the information across.", "tokens": [51351, 467, 311, 588, 2099, 293, 44882, 293, 309, 445, 2170, 439, 264, 1589, 2108, 13, 51578], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 451, "seek": 189708, "start": 1921.36, "end": 1922.48, "text": " And so they introduced us here.", "tokens": [51578, 400, 370, 436, 7268, 505, 510, 13, 51634], "temperature": 0.0, "avg_logprob": -0.3421588807594119, "compression_ratio": 1.60431654676259, "no_speech_prob": 0.05339065566658974}, {"id": 452, "seek": 192248, "start": 1922.48, "end": 1927.92, "text": " We have this noising process on noiseify function where we have square root of something times", "tokens": [50364, 492, 362, 341, 572, 3436, 1399, 322, 5658, 2505, 2445, 689, 321, 362, 3732, 5593, 295, 746, 1413, 50636], "temperature": 0.0, "avg_logprob": -0.3078390050817419, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.7457974553108215}, {"id": 453, "seek": 192248, "start": 1927.92, "end": 1932.3, "text": " X plus square root of one minus that something times the noise.", "tokens": [50636, 1783, 1804, 3732, 5593, 295, 472, 3175, 300, 746, 1413, 264, 5658, 13, 50855], "temperature": 0.0, "avg_logprob": -0.3078390050817419, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.7457974553108215}, {"id": 454, "seek": 192248, "start": 1932.3, "end": 1937.32, "text": " And here they use gamma, gamma of T, which is often used for the continuous time case.", "tokens": [50855, 400, 510, 436, 764, 15546, 11, 15546, 295, 314, 11, 597, 307, 2049, 1143, 337, 264, 10957, 565, 1389, 13, 51106], "temperature": 0.0, "avg_logprob": -0.3078390050817419, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.7457974553108215}, {"id": 455, "seek": 192248, "start": 1937.32, "end": 1940.96, "text": " So instead of the alpha bar and the beta bar scheduled for a thousand time steps, there'll", "tokens": [51106, 407, 2602, 295, 264, 8961, 2159, 293, 264, 9861, 2159, 15678, 337, 257, 4714, 565, 4439, 11, 456, 603, 51288], "temperature": 0.0, "avg_logprob": -0.3078390050817419, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.7457974553108215}, {"id": 456, "seek": 192248, "start": 1940.96, "end": 1945.44, "text": " be some function gamma of T that tells you what your alpha bar should be.", "tokens": [51288, 312, 512, 2445, 15546, 295, 314, 300, 5112, 291, 437, 428, 8961, 2159, 820, 312, 13, 51512], "temperature": 0.0, "avg_logprob": -0.3078390050817419, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.7457974553108215}, {"id": 457, "seek": 192248, "start": 1945.44, "end": 1946.44, "text": " Okay.", "tokens": [51512, 1033, 13, 51562], "temperature": 0.0, "avg_logprob": -0.3078390050817419, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.7457974553108215}, {"id": 458, "seek": 192248, "start": 1946.44, "end": 1949.24, "text": " So that's our, our function is actually called A bar, but it's the same thing.", "tokens": [51562, 407, 300, 311, 527, 11, 527, 2445, 307, 767, 1219, 316, 2159, 11, 457, 309, 311, 264, 912, 551, 13, 51702], "temperature": 0.0, "avg_logprob": -0.3078390050817419, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.7457974553108215}, {"id": 459, "seek": 192248, "start": 1949.24, "end": 1950.24, "text": " Yeah.", "tokens": [51702, 865, 13, 51752], "temperature": 0.0, "avg_logprob": -0.3078390050817419, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.7457974553108215}, {"id": 460, "seek": 192248, "start": 1950.24, "end": 1951.24, "text": " Same, same thing.", "tokens": [51752, 10635, 11, 912, 551, 13, 51802], "temperature": 0.0, "avg_logprob": -0.3078390050817419, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.7457974553108215}, {"id": 461, "seek": 195124, "start": 1951.24, "end": 1952.8, "text": " So it's gamma of zero to one.", "tokens": [50364, 407, 309, 311, 15546, 295, 4018, 281, 472, 13, 50442], "temperature": 0.0, "avg_logprob": -0.3587679628465996, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.7902118563652039}, {"id": 462, "seek": 195124, "start": 1952.8, "end": 1954.92, "text": " And then that's used to noise the image.", "tokens": [50442, 400, 550, 300, 311, 1143, 281, 5658, 264, 3256, 13, 50548], "temperature": 0.0, "avg_logprob": -0.3587679628465996, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.7902118563652039}, {"id": 463, "seek": 195124, "start": 1954.92, "end": 1960.4, "text": " Interestingly, what they're showing here actually is something that we had discovered and I'd", "tokens": [50548, 30564, 11, 437, 436, 434, 4099, 510, 767, 307, 746, 300, 321, 632, 6941, 293, 286, 1116, 50822], "temperature": 0.0, "avg_logprob": -0.3587679628465996, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.7902118563652039}, {"id": 464, "seek": 195124, "start": 1960.4, "end": 1966.76, "text": " been complaining about that my DTIMs with an eater of less than one weren't working,", "tokens": [50822, 668, 20740, 466, 300, 452, 413, 51, 6324, 82, 365, 364, 40362, 295, 1570, 813, 472, 4999, 380, 1364, 11, 51140], "temperature": 0.0, "avg_logprob": -0.3587679628465996, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.7902118563652039}, {"id": 465, "seek": 195124, "start": 1966.76, "end": 1972.16, "text": " which is to say when I added extra noise to the image, it wasn't working.", "tokens": [51140, 597, 307, 281, 584, 562, 286, 3869, 2857, 5658, 281, 264, 3256, 11, 309, 2067, 380, 1364, 13, 51410], "temperature": 0.0, "avg_logprob": -0.3587679628465996, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.7902118563652039}, {"id": 466, "seek": 195124, "start": 1972.16, "end": 1977.04, "text": " And what they're showing here is like, oh yeah, duh, if you use a smaller image, then", "tokens": [51410, 400, 437, 436, 434, 4099, 510, 307, 411, 11, 1954, 1338, 11, 43763, 11, 498, 291, 764, 257, 4356, 3256, 11, 550, 51654], "temperature": 0.0, "avg_logprob": -0.3587679628465996, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.7902118563652039}, {"id": 467, "seek": 195124, "start": 1977.04, "end": 1980.8, "text": " adding extra noise is probably not a good idea.", "tokens": [51654, 5127, 2857, 5658, 307, 1391, 406, 257, 665, 1558, 13, 51842], "temperature": 0.0, "avg_logprob": -0.3587679628465996, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.7902118563652039}, {"id": 468, "seek": 198080, "start": 1981.36, "end": 1982.36, "text": " Yeah.", "tokens": [50392, 865, 13, 50442], "temperature": 0.0, "avg_logprob": -0.24184161897689577, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.009707949124276638}, {"id": 469, "seek": 198080, "start": 1982.36, "end": 1987.56, "text": " And so they, they, they use a lot of reference in this paper to like information being destroyed", "tokens": [50442, 400, 370, 436, 11, 436, 11, 436, 764, 257, 688, 295, 6408, 294, 341, 3035, 281, 411, 1589, 885, 8937, 50702], "temperature": 0.0, "avg_logprob": -0.24184161897689577, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.009707949124276638}, {"id": 470, "seek": 198080, "start": 1987.56, "end": 1989.76, "text": " and signal to noise ratios.", "tokens": [50702, 293, 6358, 281, 5658, 32435, 13, 50812], "temperature": 0.0, "avg_logprob": -0.24184161897689577, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.009707949124276638}, {"id": 471, "seek": 198080, "start": 1989.76, "end": 1993.04, "text": " And that's really helpful for thinking about because it's not something that's obvious,", "tokens": [50812, 400, 300, 311, 534, 4961, 337, 1953, 466, 570, 309, 311, 406, 746, 300, 311, 6322, 11, 50976], "temperature": 0.0, "avg_logprob": -0.24184161897689577, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.009707949124276638}, {"id": 472, "seek": 198080, "start": 1993.04, "end": 1999.6399999999999, "text": " but at 64 by 64 pixels, adjacent pixels might have much less in common versus the same amount", "tokens": [50976, 457, 412, 12145, 538, 12145, 18668, 11, 24441, 18668, 1062, 362, 709, 1570, 294, 2689, 5717, 264, 912, 2372, 51306], "temperature": 0.0, "avg_logprob": -0.24184161897689577, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.009707949124276638}, {"id": 473, "seek": 198080, "start": 1999.6399999999999, "end": 2002.6, "text": " of noise added at a much higher resolution.", "tokens": [51306, 295, 5658, 3869, 412, 257, 709, 2946, 8669, 13, 51454], "temperature": 0.0, "avg_logprob": -0.24184161897689577, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.009707949124276638}, {"id": 474, "seek": 198080, "start": 2002.6, "end": 2005.36, "text": " The noise kind of averages out and you can still see a lot of the image.", "tokens": [51454, 440, 5658, 733, 295, 42257, 484, 293, 291, 393, 920, 536, 257, 688, 295, 264, 3256, 13, 51592], "temperature": 0.0, "avg_logprob": -0.24184161897689577, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.009707949124276638}, {"id": 475, "seek": 198080, "start": 2005.36, "end": 2010.06, "text": " So yeah, that's one thing they highlight is that the same noise level for different image", "tokens": [51592, 407, 1338, 11, 300, 311, 472, 551, 436, 5078, 307, 300, 264, 912, 5658, 1496, 337, 819, 3256, 51827], "temperature": 0.0, "avg_logprob": -0.24184161897689577, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.009707949124276638}, {"id": 476, "seek": 201006, "start": 2010.06, "end": 2015.06, "text": " sizes might have a, it might be a harder or easier task.", "tokens": [50364, 11602, 1062, 362, 257, 11, 309, 1062, 312, 257, 6081, 420, 3571, 5633, 13, 50614], "temperature": 0.0, "avg_logprob": -0.26115558458411176, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.1500188559293747}, {"id": 477, "seek": 201006, "start": 2015.06, "end": 2016.78, "text": " And so they investigate some strategies for this.", "tokens": [50614, 400, 370, 436, 15013, 512, 9029, 337, 341, 13, 50700], "temperature": 0.0, "avg_logprob": -0.26115558458411176, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.1500188559293747}, {"id": 478, "seek": 201006, "start": 2016.78, "end": 2019.34, "text": " They look at the different noise schedule functions.", "tokens": [50700, 814, 574, 412, 264, 819, 5658, 7567, 6828, 13, 50828], "temperature": 0.0, "avg_logprob": -0.26115558458411176, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.1500188559293747}, {"id": 479, "seek": 201006, "start": 2019.34, "end": 2024.1, "text": " So we've seen the original version from the DDPM paper.", "tokens": [50828, 407, 321, 600, 1612, 264, 3380, 3037, 490, 264, 30778, 18819, 3035, 13, 51066], "temperature": 0.0, "avg_logprob": -0.26115558458411176, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.1500188559293747}, {"id": 480, "seek": 201006, "start": 2024.1, "end": 2030.6599999999999, "text": " We've seen the cosine schedule and we've seen, I think we might look at, or the next thing", "tokens": [51066, 492, 600, 1612, 264, 23565, 7567, 293, 321, 600, 1612, 11, 286, 519, 321, 1062, 574, 412, 11, 420, 264, 958, 551, 51394], "temperature": 0.0, "avg_logprob": -0.26115558458411176, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.1500188559293747}, {"id": 481, "seek": 201006, "start": 2030.6599999999999, "end": 2034.6599999999999, "text": " that Jeremy's going to show us a sigmoid based schedule.", "tokens": [51394, 300, 17809, 311, 516, 281, 855, 505, 257, 4556, 3280, 327, 2361, 7567, 13, 51594], "temperature": 0.0, "avg_logprob": -0.26115558458411176, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.1500188559293747}, {"id": 482, "seek": 201006, "start": 2034.6599999999999, "end": 2038.6799999999998, "text": " And so they show the continuous time versions of that and they plot how you can change various", "tokens": [51594, 400, 370, 436, 855, 264, 10957, 565, 9606, 295, 300, 293, 436, 7542, 577, 291, 393, 1319, 3683, 51795], "temperature": 0.0, "avg_logprob": -0.26115558458411176, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.1500188559293747}, {"id": 483, "seek": 203868, "start": 2038.68, "end": 2046.24, "text": " parameters to get these different gamma functions or in our case, the alpha bar where we starting", "tokens": [50364, 9834, 281, 483, 613, 819, 15546, 6828, 420, 294, 527, 1389, 11, 264, 8961, 2159, 689, 321, 2891, 50742], "temperature": 0.0, "avg_logprob": -0.2786586354079756, "compression_ratio": 1.8384279475982532, "no_speech_prob": 0.12763945758342743}, {"id": 484, "seek": 203868, "start": 2046.24, "end": 2053.76, "text": " at all image, no noise at T equals zero, moving to all noise, no image at T equals one.", "tokens": [50742, 412, 439, 3256, 11, 572, 5658, 412, 314, 6915, 4018, 11, 2684, 281, 439, 5658, 11, 572, 3256, 412, 314, 6915, 472, 13, 51118], "temperature": 0.0, "avg_logprob": -0.2786586354079756, "compression_ratio": 1.8384279475982532, "no_speech_prob": 0.12763945758342743}, {"id": 485, "seek": 203868, "start": 2053.76, "end": 2057.96, "text": " But the path that you take, it's going to be different for these different classes of", "tokens": [51118, 583, 264, 3100, 300, 291, 747, 11, 309, 311, 516, 281, 312, 819, 337, 613, 819, 5359, 295, 51328], "temperature": 0.0, "avg_logprob": -0.2786586354079756, "compression_ratio": 1.8384279475982532, "no_speech_prob": 0.12763945758342743}, {"id": 486, "seek": 203868, "start": 2057.96, "end": 2062.6, "text": " functions and parameters and the signal to noise ratio, that's what this, or the log", "tokens": [51328, 6828, 293, 9834, 293, 264, 6358, 281, 5658, 8509, 11, 300, 311, 437, 341, 11, 420, 264, 3565, 51560], "temperature": 0.0, "avg_logprob": -0.2786586354079756, "compression_ratio": 1.8384279475982532, "no_speech_prob": 0.12763945758342743}, {"id": 487, "seek": 203868, "start": 2062.6, "end": 2068.08, "text": " signal to noise ratio is going to change over that time as well.", "tokens": [51560, 6358, 281, 5658, 8509, 307, 516, 281, 1319, 670, 300, 565, 382, 731, 13, 51834], "temperature": 0.0, "avg_logprob": -0.2786586354079756, "compression_ratio": 1.8384279475982532, "no_speech_prob": 0.12763945758342743}, {"id": 488, "seek": 206808, "start": 2068.08, "end": 2069.7999999999997, "text": " And so that's one of the knobs we can tweak.", "tokens": [50364, 400, 370, 300, 311, 472, 295, 264, 46999, 321, 393, 29879, 13, 50450], "temperature": 0.0, "avg_logprob": -0.30512139291474316, "compression_ratio": 1.7171052631578947, "no_speech_prob": 0.09007785469293594}, {"id": 489, "seek": 206808, "start": 2069.7999999999997, "end": 2072.18, "text": " We're saying our diffusion model isn't training that well.", "tokens": [50450, 492, 434, 1566, 527, 25242, 2316, 1943, 380, 3097, 300, 731, 13, 50569], "temperature": 0.0, "avg_logprob": -0.30512139291474316, "compression_ratio": 1.7171052631578947, "no_speech_prob": 0.09007785469293594}, {"id": 490, "seek": 206808, "start": 2072.18, "end": 2074.92, "text": " We think it might be related to the noise schedule and so on.", "tokens": [50569, 492, 519, 309, 1062, 312, 4077, 281, 264, 5658, 7567, 293, 370, 322, 13, 50706], "temperature": 0.0, "avg_logprob": -0.30512139291474316, "compression_ratio": 1.7171052631578947, "no_speech_prob": 0.09007785469293594}, {"id": 491, "seek": 206808, "start": 2074.92, "end": 2078.56, "text": " One of the things you could do is try different noise schedules, either changing the parameters", "tokens": [50706, 1485, 295, 264, 721, 291, 727, 360, 307, 853, 819, 5658, 28078, 11, 2139, 4473, 264, 9834, 50888], "temperature": 0.0, "avg_logprob": -0.30512139291474316, "compression_ratio": 1.7171052631578947, "no_speech_prob": 0.09007785469293594}, {"id": 492, "seek": 206808, "start": 2078.56, "end": 2084.72, "text": " in one class of noise schedule or switching from a linear to a cosine to a sigmoid.", "tokens": [50888, 294, 472, 1508, 295, 5658, 7567, 420, 16493, 490, 257, 8213, 281, 257, 23565, 281, 257, 4556, 3280, 327, 13, 51196], "temperature": 0.0, "avg_logprob": -0.30512139291474316, "compression_ratio": 1.7171052631578947, "no_speech_prob": 0.09007785469293594}, {"id": 493, "seek": 206808, "start": 2084.72, "end": 2088.48, "text": " And then the second strategy is kind of what we were doing in those experiments, which", "tokens": [51196, 400, 550, 264, 1150, 5206, 307, 733, 295, 437, 321, 645, 884, 294, 729, 12050, 11, 597, 51384], "temperature": 0.0, "avg_logprob": -0.30512139291474316, "compression_ratio": 1.7171052631578947, "no_speech_prob": 0.09007785469293594}, {"id": 494, "seek": 206808, "start": 2088.48, "end": 2091.92, "text": " is just to add some scaling factor to X there.", "tokens": [51384, 307, 445, 281, 909, 512, 21589, 5952, 281, 1783, 456, 13, 51556], "temperature": 0.0, "avg_logprob": -0.30512139291474316, "compression_ratio": 1.7171052631578947, "no_speech_prob": 0.09007785469293594}, {"id": 495, "seek": 206808, "start": 2091.92, "end": 2098.04, "text": " While we were accidentally using B of 0.5.", "tokens": [51556, 3987, 321, 645, 15715, 1228, 363, 295, 1958, 13, 20, 13, 51862], "temperature": 0.0, "avg_logprob": -0.30512139291474316, "compression_ratio": 1.7171052631578947, "no_speech_prob": 0.09007785469293594}, {"id": 496, "seek": 209804, "start": 2099.0, "end": 2100.0, "text": " Exactly.", "tokens": [50412, 7587, 13, 50462], "temperature": 0.0, "avg_logprob": -0.32123310225350515, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.001284241210669279}, {"id": 497, "seek": 209804, "start": 2100.0, "end": 2104.96, "text": " And so that's a second dial that you can tweak is to say, keeping your noise schedule fixed,", "tokens": [50462, 400, 370, 300, 311, 257, 1150, 5502, 300, 291, 393, 29879, 307, 281, 584, 11, 5145, 428, 5658, 7567, 6806, 11, 50710], "temperature": 0.0, "avg_logprob": -0.32123310225350515, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.001284241210669279}, {"id": 498, "seek": 209804, "start": 2104.96, "end": 2109.92, "text": " maybe you just scale X zero, which is going to change the ratio of signal to noise.", "tokens": [50710, 1310, 291, 445, 4373, 1783, 4018, 11, 597, 307, 516, 281, 1319, 264, 8509, 295, 6358, 281, 5658, 13, 50958], "temperature": 0.0, "avg_logprob": -0.32123310225350515, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.001284241210669279}, {"id": 499, "seek": 209804, "start": 2109.92, "end": 2114.6, "text": " And that's what figure four in C there is what we were accidentally doing.", "tokens": [50958, 400, 300, 311, 437, 2573, 1451, 294, 383, 456, 307, 437, 321, 645, 15715, 884, 13, 51192], "temperature": 0.0, "avg_logprob": -0.32123310225350515, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.001284241210669279}, {"id": 500, "seek": 209804, "start": 2114.6, "end": 2115.6, "text": " Yes.", "tokens": [51192, 1079, 13, 51242], "temperature": 0.0, "avg_logprob": -0.32123310225350515, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.001284241210669279}, {"id": 501, "seek": 209804, "start": 2115.6, "end": 2118.84, "text": " Yeah, exactly.", "tokens": [51242, 865, 11, 2293, 13, 51404], "temperature": 0.0, "avg_logprob": -0.32123310225350515, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.001284241210669279}, {"id": 502, "seek": 209804, "start": 2118.84, "end": 2122.52, "text": " And so let's see if we can get to, oh yeah.", "tokens": [51404, 400, 370, 718, 311, 536, 498, 321, 393, 483, 281, 11, 1954, 1338, 13, 51588], "temperature": 0.0, "avg_logprob": -0.32123310225350515, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.001284241210669279}, {"id": 503, "seek": 209804, "start": 2122.52, "end": 2126.7599999999998, "text": " So that again, changed the signal to noise for different scalings you get.", "tokens": [51588, 407, 300, 797, 11, 3105, 264, 6358, 281, 5658, 337, 819, 15664, 1109, 291, 483, 13, 51800], "temperature": 0.0, "avg_logprob": -0.32123310225350515, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.001284241210669279}, {"id": 504, "seek": 212676, "start": 2126.8, "end": 2127.8, "text": " So that's fine.", "tokens": [50366, 407, 300, 311, 2489, 13, 50416], "temperature": 0.0, "avg_logprob": -0.2784372725576725, "compression_ratio": 1.7053941908713692, "no_speech_prob": 0.16883330047130585}, {"id": 505, "seek": 212676, "start": 2127.8, "end": 2131.48, "text": " So they have a compound, they have a strategy that combines some of those things.", "tokens": [50416, 407, 436, 362, 257, 14154, 11, 436, 362, 257, 5206, 300, 29520, 512, 295, 729, 721, 13, 50600], "temperature": 0.0, "avg_logprob": -0.2784372725576725, "compression_ratio": 1.7053941908713692, "no_speech_prob": 0.16883330047130585}, {"id": 506, "seek": 212676, "start": 2131.48, "end": 2132.48, "text": " And this is the important part.", "tokens": [50600, 400, 341, 307, 264, 1021, 644, 13, 50650], "temperature": 0.0, "avg_logprob": -0.2784372725576725, "compression_ratio": 1.7053941908713692, "no_speech_prob": 0.16883330047130585}, {"id": 507, "seek": 212676, "start": 2132.48, "end": 2135.0400000000004, "text": " They do their experiments.", "tokens": [50650, 814, 360, 641, 12050, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2784372725576725, "compression_ratio": 1.7053941908713692, "no_speech_prob": 0.16883330047130585}, {"id": 508, "seek": 212676, "start": 2135.0400000000004, "end": 2140.28, "text": " And so they have a nice table of investigating different schedules, cosine schedules and", "tokens": [50778, 400, 370, 436, 362, 257, 1481, 3199, 295, 22858, 819, 28078, 11, 23565, 28078, 293, 51040], "temperature": 0.0, "avg_logprob": -0.2784372725576725, "compression_ratio": 1.7053941908713692, "no_speech_prob": 0.16883330047130585}, {"id": 509, "seek": 212676, "start": 2140.28, "end": 2141.94, "text": " sigmoid schedules.", "tokens": [51040, 4556, 3280, 327, 28078, 13, 51123], "temperature": 0.0, "avg_logprob": -0.2784372725576725, "compression_ratio": 1.7053941908713692, "no_speech_prob": 0.16883330047130585}, {"id": 510, "seek": 212676, "start": 2141.94, "end": 2143.44, "text": " And in bold are the best results.", "tokens": [51123, 400, 294, 11928, 366, 264, 1151, 3542, 13, 51198], "temperature": 0.0, "avg_logprob": -0.2784372725576725, "compression_ratio": 1.7053941908713692, "no_speech_prob": 0.16883330047130585}, {"id": 511, "seek": 212676, "start": 2143.44, "end": 2150.0400000000004, "text": " And you can see for 64 by 64 images versus 128 versus 256, the best schedule is not necessarily", "tokens": [51198, 400, 291, 393, 536, 337, 12145, 538, 12145, 5267, 5717, 29810, 5717, 38882, 11, 264, 1151, 7567, 307, 406, 4725, 51528], "temperature": 0.0, "avg_logprob": -0.2784372725576725, "compression_ratio": 1.7053941908713692, "no_speech_prob": 0.16883330047130585}, {"id": 512, "seek": 212676, "start": 2150.0400000000004, "end": 2152.6200000000003, "text": " always the same.", "tokens": [51528, 1009, 264, 912, 13, 51657], "temperature": 0.0, "avg_logprob": -0.2784372725576725, "compression_ratio": 1.7053941908713692, "no_speech_prob": 0.16883330047130585}, {"id": 513, "seek": 215262, "start": 2152.62, "end": 2157.7799999999997, "text": " And so that's like important finding number one, depending on what your data looks like,", "tokens": [50364, 400, 370, 300, 311, 411, 1021, 5006, 1230, 472, 11, 5413, 322, 437, 428, 1412, 1542, 411, 11, 50622], "temperature": 0.0, "avg_logprob": -0.30613665426931075, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.37012234330177307}, {"id": 514, "seek": 215262, "start": 2157.7799999999997, "end": 2160.18, "text": " using a different noise schedule might be optimal.", "tokens": [50622, 1228, 257, 819, 5658, 7567, 1062, 312, 16252, 13, 50742], "temperature": 0.0, "avg_logprob": -0.30613665426931075, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.37012234330177307}, {"id": 515, "seek": 215262, "start": 2160.18, "end": 2161.9, "text": " There's no one true best schedule.", "tokens": [50742, 821, 311, 572, 472, 2074, 1151, 7567, 13, 50828], "temperature": 0.0, "avg_logprob": -0.30613665426931075, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.37012234330177307}, {"id": 516, "seek": 215262, "start": 2161.9, "end": 2168.2999999999997, "text": " There's no one value of, you know, beta min and beta max, that's just magically the best.", "tokens": [50828, 821, 311, 572, 472, 2158, 295, 11, 291, 458, 11, 9861, 923, 293, 9861, 11469, 11, 300, 311, 445, 39763, 264, 1151, 13, 51148], "temperature": 0.0, "avg_logprob": -0.30613665426931075, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.37012234330177307}, {"id": 517, "seek": 215262, "start": 2168.2999999999997, "end": 2176.18, "text": " Likewise for this input scaling at different sizes with whatever schedules they tested", "tokens": [51148, 30269, 337, 341, 4846, 21589, 412, 819, 11602, 365, 2035, 28078, 436, 8246, 51542], "temperature": 0.0, "avg_logprob": -0.30613665426931075, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.37012234330177307}, {"id": 518, "seek": 215262, "start": 2176.18, "end": 2181.14, "text": " and different values were kind of optimal.", "tokens": [51542, 293, 819, 4190, 645, 733, 295, 16252, 13, 51790], "temperature": 0.0, "avg_logprob": -0.30613665426931075, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.37012234330177307}, {"id": 519, "seek": 218114, "start": 2181.14, "end": 2187.02, "text": " And so, yeah, it's just a really great illustration, I guess, that this is another design choice", "tokens": [50364, 400, 370, 11, 1338, 11, 309, 311, 445, 257, 534, 869, 22645, 11, 286, 2041, 11, 300, 341, 307, 1071, 1715, 3922, 50658], "temperature": 0.0, "avg_logprob": -0.23076776872601426, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.13659201562404633}, {"id": 520, "seek": 218114, "start": 2187.02, "end": 2192.7799999999997, "text": " that's implicit or explicitly part of your diffusion model training and sampling is how", "tokens": [50658, 300, 311, 26947, 420, 20803, 644, 295, 428, 25242, 2316, 3097, 293, 21179, 307, 577, 50946], "temperature": 0.0, "avg_logprob": -0.23076776872601426, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.13659201562404633}, {"id": 521, "seek": 218114, "start": 2192.7799999999997, "end": 2195.02, "text": " are you dealing with this noise schedule?", "tokens": [50946, 366, 291, 6260, 365, 341, 5658, 7567, 30, 51058], "temperature": 0.0, "avg_logprob": -0.23076776872601426, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.13659201562404633}, {"id": 522, "seek": 218114, "start": 2195.02, "end": 2196.22, "text": " What schedule are you following?", "tokens": [51058, 708, 7567, 366, 291, 3480, 30, 51118], "temperature": 0.0, "avg_logprob": -0.23076776872601426, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.13659201562404633}, {"id": 523, "seek": 218114, "start": 2196.22, "end": 2199.02, "text": " What scaling are you doing of your inputs?", "tokens": [51118, 708, 21589, 366, 291, 884, 295, 428, 15743, 30, 51258], "temperature": 0.0, "avg_logprob": -0.23076776872601426, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.13659201562404633}, {"id": 524, "seek": 218114, "start": 2199.02, "end": 2203.2999999999997, "text": " And by using this thinking and doing these experiments, and they come up with a kind", "tokens": [51258, 400, 538, 1228, 341, 1953, 293, 884, 613, 12050, 11, 293, 436, 808, 493, 365, 257, 733, 51472], "temperature": 0.0, "avg_logprob": -0.23076776872601426, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.13659201562404633}, {"id": 525, "seek": 218114, "start": 2203.2999999999997, "end": 2208.7799999999997, "text": " of rule of thumb for how to scale the image based on image size, they show that they can,", "tokens": [51472, 295, 4978, 295, 9298, 337, 577, 281, 4373, 264, 3256, 2361, 322, 3256, 2744, 11, 436, 855, 300, 436, 393, 11, 51746], "temperature": 0.0, "avg_logprob": -0.23076776872601426, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.13659201562404633}, {"id": 526, "seek": 220878, "start": 2208.78, "end": 2213.0600000000004, "text": " as they increase the resolution, they can still maintain really good performance.", "tokens": [50364, 382, 436, 3488, 264, 8669, 11, 436, 393, 920, 6909, 534, 665, 3389, 13, 50578], "temperature": 0.0, "avg_logprob": -0.3467761537303095, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.5349818468093872}, {"id": 527, "seek": 220878, "start": 2213.0600000000004, "end": 2219.5800000000004, "text": " Where previously it was quite hard to train a really large resolution pixel space model,", "tokens": [50578, 2305, 8046, 309, 390, 1596, 1152, 281, 3847, 257, 534, 2416, 8669, 19261, 1901, 2316, 11, 50904], "temperature": 0.0, "avg_logprob": -0.3467761537303095, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.5349818468093872}, {"id": 528, "seek": 220878, "start": 2219.5800000000004, "end": 2221.1000000000004, "text": " and they're able to do that.", "tokens": [50904, 293, 436, 434, 1075, 281, 360, 300, 13, 50980], "temperature": 0.0, "avg_logprob": -0.3467761537303095, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.5349818468093872}, {"id": 529, "seek": 220878, "start": 2221.1000000000004, "end": 2226.5800000000004, "text": " They get some advantage from their fancy recurrent interface network, but still it's kind of", "tokens": [50980, 814, 483, 512, 5002, 490, 641, 10247, 18680, 1753, 9226, 3209, 11, 457, 920, 309, 311, 733, 295, 51254], "temperature": 0.0, "avg_logprob": -0.3467761537303095, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.5349818468093872}, {"id": 530, "seek": 220878, "start": 2226.5800000000004, "end": 2233.26, "text": " cool that they can say, look, we get state of the art, high quality and 512 by 5N col", "tokens": [51254, 1627, 300, 436, 393, 584, 11, 574, 11, 321, 483, 1785, 295, 264, 1523, 11, 1090, 3125, 293, 1025, 4762, 538, 1025, 45, 1173, 51588], "temperature": 0.0, "avg_logprob": -0.3467761537303095, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.5349818468093872}, {"id": 531, "seek": 223326, "start": 2233.26, "end": 2239.1400000000003, "text": " 1024 by 1024 samples on class-conditioned ImageNet.", "tokens": [50364, 1266, 7911, 538, 1266, 7911, 10938, 322, 1508, 12, 18882, 849, 292, 29903, 31890, 13, 50658], "temperature": 0.0, "avg_logprob": -0.3533567376093033, "compression_ratio": 1.6168582375478928, "no_speech_prob": 0.7458093762397766}, {"id": 532, "seek": 223326, "start": 2239.1400000000003, "end": 2243.0200000000004, "text": " And using this approach to really consider how well do you train?", "tokens": [50658, 400, 1228, 341, 3109, 281, 534, 1949, 577, 731, 360, 291, 3847, 30, 50852], "temperature": 0.0, "avg_logprob": -0.3533567376093033, "compression_ratio": 1.6168582375478928, "no_speech_prob": 0.7458093762397766}, {"id": 533, "seek": 223326, "start": 2243.0200000000004, "end": 2244.26, "text": " How many steps do we need to take?", "tokens": [50852, 1012, 867, 4439, 360, 321, 643, 281, 747, 30, 50914], "temperature": 0.0, "avg_logprob": -0.3533567376093033, "compression_ratio": 1.6168582375478928, "no_speech_prob": 0.7458093762397766}, {"id": 534, "seek": 223326, "start": 2244.26, "end": 2247.5400000000004, "text": " One of the other things in this table is that they compare it to previous approaches.", "tokens": [50914, 1485, 295, 264, 661, 721, 294, 341, 3199, 307, 300, 436, 6794, 309, 281, 3894, 11587, 13, 51078], "temperature": 0.0, "avg_logprob": -0.3533567376093033, "compression_ratio": 1.6168582375478928, "no_speech_prob": 0.7458093762397766}, {"id": 535, "seek": 223326, "start": 2247.5400000000004, "end": 2253.82, "text": " Oh, we used a third of the training steps for the same other settings and we get better", "tokens": [51078, 876, 11, 321, 1143, 257, 2636, 295, 264, 3097, 4439, 337, 264, 912, 661, 6257, 293, 321, 483, 1101, 51392], "temperature": 0.0, "avg_logprob": -0.3533567376093033, "compression_ratio": 1.6168582375478928, "no_speech_prob": 0.7458093762397766}, {"id": 536, "seek": 223326, "start": 2253.82, "end": 2259.1400000000003, "text": " performance just because we've chosen that input scaling better.", "tokens": [51392, 3389, 445, 570, 321, 600, 8614, 300, 4846, 21589, 1101, 13, 51658], "temperature": 0.0, "avg_logprob": -0.3533567376093033, "compression_ratio": 1.6168582375478928, "no_speech_prob": 0.7458093762397766}, {"id": 537, "seek": 223326, "start": 2259.1400000000003, "end": 2260.86, "text": " And yeah, so that's the paper.", "tokens": [51658, 400, 1338, 11, 370, 300, 311, 264, 3035, 13, 51744], "temperature": 0.0, "avg_logprob": -0.3533567376093033, "compression_ratio": 1.6168582375478928, "no_speech_prob": 0.7458093762397766}, {"id": 538, "seek": 226086, "start": 2260.86, "end": 2264.54, "text": " They do really nice, great work to the team.", "tokens": [50364, 814, 360, 534, 1481, 11, 869, 589, 281, 264, 1469, 13, 50548], "temperature": 0.0, "avg_logprob": -0.3504089884238668, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.5617810487747192}, {"id": 539, "seek": 226086, "start": 2264.54, "end": 2265.54, "text": " And that was really useful.", "tokens": [50548, 400, 300, 390, 534, 4420, 13, 50598], "temperature": 0.0, "avg_logprob": -0.3504089884238668, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.5617810487747192}, {"id": 540, "seek": 226086, "start": 2265.54, "end": 2273.78, "text": " I love that you got up in the morning and thought, oh, it's going to be a hassle training", "tokens": [50598, 286, 959, 300, 291, 658, 493, 294, 264, 2446, 293, 1194, 11, 1954, 11, 309, 311, 516, 281, 312, 257, 39526, 3097, 51010], "temperature": 0.0, "avg_logprob": -0.3504089884238668, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.5617810487747192}, {"id": 541, "seek": 226086, "start": 2273.78, "end": 2279.58, "text": " all these different models I need to train for different input scalings and different", "tokens": [51010, 439, 613, 819, 5245, 286, 643, 281, 3847, 337, 819, 4846, 15664, 1109, 293, 819, 51300], "temperature": 0.0, "avg_logprob": -0.3504089884238668, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.5617810487747192}, {"id": 542, "seek": 226086, "start": 2279.58, "end": 2281.1400000000003, "text": " sampling approaches.", "tokens": [51300, 21179, 11587, 13, 51378], "temperature": 0.0, "avg_logprob": -0.3504089884238668, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.5617810487747192}, {"id": 543, "seek": 226086, "start": 2281.1400000000003, "end": 2283.1, "text": " I just look at Twitter first.", "tokens": [51378, 286, 445, 574, 412, 5794, 700, 13, 51476], "temperature": 0.0, "avg_logprob": -0.3504089884238668, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.5617810487747192}, {"id": 544, "seek": 226086, "start": 2283.1, "end": 2287.06, "text": " And then you looked at Twitter and there was a paper saying like, hey, we just did a bunch", "tokens": [51476, 400, 550, 291, 2956, 412, 5794, 293, 456, 390, 257, 3035, 1566, 411, 11, 4177, 11, 321, 445, 630, 257, 3840, 51674], "temperature": 0.0, "avg_logprob": -0.3504089884238668, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.5617810487747192}, {"id": 545, "seek": 228706, "start": 2287.06, "end": 2290.94, "text": " of experiments for different noise schedules and input scaling.", "tokens": [50364, 295, 12050, 337, 819, 5658, 28078, 293, 4846, 21589, 13, 50558], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 546, "seek": 228706, "start": 2290.94, "end": 2291.94, "text": " Yeah.", "tokens": [50558, 865, 13, 50608], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 547, "seek": 228706, "start": 2291.94, "end": 2294.54, "text": " Does your wife always work that way, Jono?", "tokens": [50608, 4402, 428, 3836, 1009, 589, 300, 636, 11, 7745, 78, 30, 50738], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 548, "seek": 228706, "start": 2294.54, "end": 2296.1, "text": " It seems quite blessed.", "tokens": [50738, 467, 2544, 1596, 12351, 13, 50816], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 549, "seek": 228706, "start": 2296.1, "end": 2297.1, "text": " Yeah.", "tokens": [50816, 865, 13, 50866], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 550, "seek": 228706, "start": 2297.1, "end": 2298.58, "text": " It's very lucky like that.", "tokens": [50866, 467, 311, 588, 6356, 411, 300, 13, 50940], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 551, "seek": 228706, "start": 2298.58, "end": 2299.58, "text": " Yeah.", "tokens": [50940, 865, 13, 50990], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 552, "seek": 228706, "start": 2299.58, "end": 2302.62, "text": " If you wait long enough, someone else will do it.", "tokens": [50990, 759, 291, 1699, 938, 1547, 11, 1580, 1646, 486, 360, 309, 13, 51142], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 553, "seek": 228706, "start": 2302.62, "end": 2305.98, "text": " That's why it's always the time when he starts posting on Twitter.", "tokens": [51142, 663, 311, 983, 309, 311, 1009, 264, 565, 562, 415, 3719, 15978, 322, 5794, 13, 51310], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 554, "seek": 228706, "start": 2305.98, "end": 2308.54, "text": " It's like my favorite hour of the day.", "tokens": [51310, 467, 311, 411, 452, 2954, 1773, 295, 264, 786, 13, 51438], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 555, "seek": 228706, "start": 2308.54, "end": 2312.5, "text": " It's just for all the papers to be posted.", "tokens": [51438, 467, 311, 445, 337, 439, 264, 10577, 281, 312, 9437, 13, 51636], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 556, "seek": 228706, "start": 2312.5, "end": 2314.58, "text": " Oh, well, thank you for that.", "tokens": [51636, 876, 11, 731, 11, 1309, 291, 337, 300, 13, 51740], "temperature": 0.0, "avg_logprob": -0.4573759172783523, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.7455309629440308}, {"id": 557, "seek": 231458, "start": 2314.58, "end": 2329.62, "text": " So let me switch to notebook 23.", "tokens": [50364, 407, 718, 385, 3679, 281, 21060, 6673, 13, 51116], "temperature": 0.0, "avg_logprob": -0.3212210337320964, "compression_ratio": 1.183673469387755, "no_speech_prob": 0.04022764042019844}, {"id": 558, "seek": 231458, "start": 2329.62, "end": 2336.7799999999997, "text": " Because this notebook is actually largely an implementation of some ideas from this", "tokens": [51116, 1436, 341, 21060, 307, 767, 11611, 364, 11420, 295, 512, 3487, 490, 341, 51474], "temperature": 0.0, "avg_logprob": -0.3212210337320964, "compression_ratio": 1.183673469387755, "no_speech_prob": 0.04022764042019844}, {"id": 559, "seek": 233678, "start": 2336.78, "end": 2341.6600000000003, "text": " paper that everybody tends to just call it Keras.", "tokens": [50364, 3035, 300, 2201, 12258, 281, 445, 818, 309, 591, 6985, 13, 50608], "temperature": 0.0, "avg_logprob": -0.3593969941139221, "compression_ratio": 1.4240506329113924, "no_speech_prob": 0.25058481097221375}, {"id": 560, "seek": 233678, "start": 2341.6600000000003, "end": 2345.86, "text": " It's a bit unfair because there's other people.", "tokens": [50608, 467, 311, 257, 857, 17019, 570, 456, 311, 661, 561, 13, 50818], "temperature": 0.0, "avg_logprob": -0.3593969941139221, "compression_ratio": 1.4240506329113924, "no_speech_prob": 0.25058481097221375}, {"id": 561, "seek": 233678, "start": 2345.86, "end": 2346.86, "text": " But I will do it anyway.", "tokens": [50818, 583, 286, 486, 360, 309, 4033, 13, 50868], "temperature": 0.0, "avg_logprob": -0.3593969941139221, "compression_ratio": 1.4240506329113924, "no_speech_prob": 0.25058481097221375}, {"id": 562, "seek": 233678, "start": 2346.86, "end": 2350.7400000000002, "text": " Keras paper.", "tokens": [50868, 591, 6985, 3035, 13, 51062], "temperature": 0.0, "avg_logprob": -0.3593969941139221, "compression_ratio": 1.4240506329113924, "no_speech_prob": 0.25058481097221375}, {"id": 563, "seek": 233678, "start": 2350.7400000000002, "end": 2362.26, "text": " And the reason we're going to look at this is because in this paper, the authors actually", "tokens": [51062, 400, 264, 1778, 321, 434, 516, 281, 574, 412, 341, 307, 570, 294, 341, 3035, 11, 264, 16552, 767, 51638], "temperature": 0.0, "avg_logprob": -0.3593969941139221, "compression_ratio": 1.4240506329113924, "no_speech_prob": 0.25058481097221375}, {"id": 564, "seek": 236226, "start": 2362.26, "end": 2369.1400000000003, "text": " take a much more explicit look at the question of input scaling.", "tokens": [50364, 747, 257, 709, 544, 13691, 574, 412, 264, 1168, 295, 4846, 21589, 13, 50708], "temperature": 0.0, "avg_logprob": -0.30964572872735757, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.16661907732486725}, {"id": 565, "seek": 236226, "start": 2369.1400000000003, "end": 2375.5800000000004, "text": " Their approach was not apparently to accidentally put a bug in their code and then take it out", "tokens": [50708, 6710, 3109, 390, 406, 7970, 281, 15715, 829, 257, 7426, 294, 641, 3089, 293, 550, 747, 309, 484, 51030], "temperature": 0.0, "avg_logprob": -0.30964572872735757, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.16661907732486725}, {"id": 566, "seek": 236226, "start": 2375.5800000000004, "end": 2378.5, "text": " and find it worked worse and then just put it back in again.", "tokens": [51030, 293, 915, 309, 2732, 5324, 293, 550, 445, 829, 309, 646, 294, 797, 13, 51176], "temperature": 0.0, "avg_logprob": -0.30964572872735757, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.16661907732486725}, {"id": 567, "seek": 236226, "start": 2378.5, "end": 2382.98, "text": " Their approach was actually to think, how should things be?", "tokens": [51176, 6710, 3109, 390, 767, 281, 519, 11, 577, 820, 721, 312, 30, 51400], "temperature": 0.0, "avg_logprob": -0.30964572872735757, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.16661907732486725}, {"id": 568, "seek": 236226, "start": 2382.98, "end": 2385.2200000000003, "text": " So that's an interesting approach to doing things.", "tokens": [51400, 407, 300, 311, 364, 1880, 3109, 281, 884, 721, 13, 51512], "temperature": 0.0, "avg_logprob": -0.30964572872735757, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.16661907732486725}, {"id": 569, "seek": 236226, "start": 2385.2200000000003, "end": 2386.38, "text": " And I guess it works for them.", "tokens": [51512, 400, 286, 2041, 309, 1985, 337, 552, 13, 51570], "temperature": 0.0, "avg_logprob": -0.30964572872735757, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.16661907732486725}, {"id": 570, "seek": 236226, "start": 2386.38, "end": 2387.38, "text": " So that's fine.", "tokens": [51570, 407, 300, 311, 2489, 13, 51620], "temperature": 0.0, "avg_logprob": -0.30964572872735757, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.16661907732486725}, {"id": 571, "seek": 236226, "start": 2387.38, "end": 2390.0200000000004, "text": " I think our approach is more inflating.", "tokens": [51620, 286, 519, 527, 3109, 307, 544, 9922, 990, 13, 51752], "temperature": 0.0, "avg_logprob": -0.30964572872735757, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.16661907732486725}, {"id": 572, "seek": 236226, "start": 2390.0200000000004, "end": 2391.0200000000004, "text": " Yeah, exactly.", "tokens": [51752, 865, 11, 2293, 13, 51802], "temperature": 0.0, "avg_logprob": -0.30964572872735757, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.16661907732486725}, {"id": 573, "seek": 239102, "start": 2391.3, "end": 2397.02, "text": " Our approach is much more fun because you never quite know what's going to happen.", "tokens": [50378, 2621, 3109, 307, 709, 544, 1019, 570, 291, 1128, 1596, 458, 437, 311, 516, 281, 1051, 13, 50664], "temperature": 0.0, "avg_logprob": -0.25240561962127683, "compression_ratio": 1.4952830188679245, "no_speech_prob": 0.016654139384627342}, {"id": 574, "seek": 239102, "start": 2397.02, "end": 2402.58, "text": " And so, yeah, in their approach, they actually tried to say, like, okay, given all the things", "tokens": [50664, 400, 370, 11, 1338, 11, 294, 641, 3109, 11, 436, 767, 3031, 281, 584, 11, 411, 11, 1392, 11, 2212, 439, 264, 721, 50942], "temperature": 0.0, "avg_logprob": -0.25240561962127683, "compression_ratio": 1.4952830188679245, "no_speech_prob": 0.016654139384627342}, {"id": 575, "seek": 239102, "start": 2402.58, "end": 2409.58, "text": " that are coming into our model, how can we have them all nicely balanced?", "tokens": [50942, 300, 366, 1348, 666, 527, 2316, 11, 577, 393, 321, 362, 552, 439, 9594, 13902, 30, 51292], "temperature": 0.0, "avg_logprob": -0.25240561962127683, "compression_ratio": 1.4952830188679245, "no_speech_prob": 0.016654139384627342}, {"id": 576, "seek": 239102, "start": 2409.58, "end": 2415.98, "text": " So we will skip back and forth between the notebook and the paper.", "tokens": [51292, 407, 321, 486, 10023, 646, 293, 5220, 1296, 264, 21060, 293, 264, 3035, 13, 51612], "temperature": 0.0, "avg_logprob": -0.25240561962127683, "compression_ratio": 1.4952830188679245, "no_speech_prob": 0.016654139384627342}, {"id": 577, "seek": 241598, "start": 2415.98, "end": 2421.62, "text": " So the start of this is all the same, except now we are actually going to do it minus one", "tokens": [50364, 407, 264, 722, 295, 341, 307, 439, 264, 912, 11, 3993, 586, 321, 366, 767, 516, 281, 360, 309, 3175, 472, 50646], "temperature": 0.0, "avg_logprob": -0.24309752259073378, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.026746699586510658}, {"id": 578, "seek": 241598, "start": 2421.62, "end": 2425.54, "text": " to one, because we're not going to rely on accidental bugs anymore, but instead we're", "tokens": [50646, 281, 472, 11, 570, 321, 434, 406, 516, 281, 10687, 322, 38094, 15120, 3602, 11, 457, 2602, 321, 434, 50842], "temperature": 0.0, "avg_logprob": -0.24309752259073378, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.026746699586510658}, {"id": 579, "seek": 241598, "start": 2425.54, "end": 2433.08, "text": " going to rely on the Keras paper's carefully designed scaling.", "tokens": [50842, 516, 281, 10687, 322, 264, 591, 6985, 3035, 311, 7500, 4761, 21589, 13, 51219], "temperature": 0.0, "avg_logprob": -0.24309752259073378, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.026746699586510658}, {"id": 580, "seek": 241598, "start": 2433.08, "end": 2442.26, "text": " I say that, except that I put a bug in this notebook as well.", "tokens": [51219, 286, 584, 300, 11, 3993, 300, 286, 829, 257, 7426, 294, 341, 21060, 382, 731, 13, 51678], "temperature": 0.0, "avg_logprob": -0.24309752259073378, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.026746699586510658}, {"id": 581, "seek": 244226, "start": 2442.26, "end": 2448.1000000000004, "text": " One of the things that's in the Keras paper is, what is the standard deviation of the", "tokens": [50364, 1485, 295, 264, 721, 300, 311, 294, 264, 591, 6985, 3035, 307, 11, 437, 307, 264, 3832, 25163, 295, 264, 50656], "temperature": 0.0, "avg_logprob": -0.27919307890392486, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.026349440217018127}, {"id": 582, "seek": 244226, "start": 2448.1000000000004, "end": 2452.78, "text": " actual data, which I calculated for a batch?", "tokens": [50656, 3539, 1412, 11, 597, 286, 15598, 337, 257, 15245, 30, 50890], "temperature": 0.0, "avg_logprob": -0.27919307890392486, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.026349440217018127}, {"id": 583, "seek": 244226, "start": 2452.78, "end": 2456.5400000000004, "text": " However, this used to say minus 0.5.", "tokens": [50890, 2908, 11, 341, 1143, 281, 584, 3175, 1958, 13, 20, 13, 51078], "temperature": 0.0, "avg_logprob": -0.27919307890392486, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.026349440217018127}, {"id": 584, "seek": 244226, "start": 2456.5400000000004, "end": 2458.7400000000002, "text": " I used to do the minus 0.5 to 0.5 thing.", "tokens": [51078, 286, 1143, 281, 360, 264, 3175, 1958, 13, 20, 281, 1958, 13, 20, 551, 13, 51188], "temperature": 0.0, "avg_logprob": -0.27919307890392486, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.026349440217018127}, {"id": 585, "seek": 244226, "start": 2458.7400000000002, "end": 2463.6200000000003, "text": " And so this is actually the standard deviation of the data before I, when it was still minus", "tokens": [51188, 400, 370, 341, 307, 767, 264, 3832, 25163, 295, 264, 1412, 949, 286, 11, 562, 309, 390, 920, 3175, 51432], "temperature": 0.0, "avg_logprob": -0.27919307890392486, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.026349440217018127}, {"id": 586, "seek": 244226, "start": 2463.6200000000003, "end": 2464.6200000000003, "text": " 0.5.", "tokens": [51432, 1958, 13, 20, 13, 51482], "temperature": 0.0, "avg_logprob": -0.27919307890392486, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.026349440217018127}, {"id": 587, "seek": 244226, "start": 2464.6200000000003, "end": 2468.42, "text": " So this is actually half the real standard deviation.", "tokens": [51482, 407, 341, 307, 767, 1922, 264, 957, 3832, 25163, 13, 51672], "temperature": 0.0, "avg_logprob": -0.27919307890392486, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.026349440217018127}, {"id": 588, "seek": 246842, "start": 2468.42, "end": 2474.54, "text": " For reasons I don't yet understand, this is giving me better scaled results.", "tokens": [50364, 1171, 4112, 286, 500, 380, 1939, 1223, 11, 341, 307, 2902, 385, 1101, 36039, 3542, 13, 50670], "temperature": 0.0, "avg_logprob": -0.21753672549599096, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.13834233582019806}, {"id": 589, "seek": 246842, "start": 2474.54, "end": 2477.66, "text": " So this actually should be 0.66.", "tokens": [50670, 407, 341, 767, 820, 312, 1958, 13, 15237, 13, 50826], "temperature": 0.0, "avg_logprob": -0.21753672549599096, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.13834233582019806}, {"id": 590, "seek": 246842, "start": 2477.66, "end": 2481.1, "text": " So there's still a bug here, and the bug still seems to work better, so we've still got some", "tokens": [50826, 407, 456, 311, 920, 257, 7426, 510, 11, 293, 264, 7426, 920, 2544, 281, 589, 1101, 11, 370, 321, 600, 920, 658, 512, 50998], "temperature": 0.0, "avg_logprob": -0.21753672549599096, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.13834233582019806}, {"id": 591, "seek": 246842, "start": 2481.1, "end": 2482.78, "text": " mysteries involved.", "tokens": [50998, 30785, 3288, 13, 51082], "temperature": 0.0, "avg_logprob": -0.21753672549599096, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.13834233582019806}, {"id": 592, "seek": 246842, "start": 2482.78, "end": 2483.78, "text": " So we're going to leave this.", "tokens": [51082, 407, 321, 434, 516, 281, 1856, 341, 13, 51132], "temperature": 0.0, "avg_logprob": -0.21753672549599096, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.13834233582019806}, {"id": 593, "seek": 246842, "start": 2483.78, "end": 2488.06, "text": " So it's actually, yeah, it's actually not 0.33, it's actually 0.66.", "tokens": [51132, 407, 309, 311, 767, 11, 1338, 11, 309, 311, 767, 406, 1958, 13, 10191, 11, 309, 311, 767, 1958, 13, 15237, 13, 51346], "temperature": 0.0, "avg_logprob": -0.21753672549599096, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.13834233582019806}, {"id": 594, "seek": 248806, "start": 2488.06, "end": 2496.82, "text": " Okay, so the basic idea of this paper, actually I'll come back.", "tokens": [50364, 1033, 11, 370, 264, 3875, 1558, 295, 341, 3035, 11, 767, 286, 603, 808, 646, 13, 50802], "temperature": 0.0, "avg_logprob": -0.3861696455213759, "compression_ratio": 1.4965517241379311, "no_speech_prob": 0.09000552445650101}, {"id": 595, "seek": 248806, "start": 2496.82, "end": 2500.94, "text": " Well, let me have a little think.", "tokens": [50802, 1042, 11, 718, 385, 362, 257, 707, 519, 13, 51008], "temperature": 0.0, "avg_logprob": -0.3861696455213759, "compression_ratio": 1.4965517241379311, "no_speech_prob": 0.09000552445650101}, {"id": 596, "seek": 248806, "start": 2500.94, "end": 2503.46, "text": " Yeah, okay, now we'll start here.", "tokens": [51008, 865, 11, 1392, 11, 586, 321, 603, 722, 510, 13, 51134], "temperature": 0.0, "avg_logprob": -0.3861696455213759, "compression_ratio": 1.4965517241379311, "no_speech_prob": 0.09000552445650101}, {"id": 597, "seek": 248806, "start": 2503.46, "end": 2515.46, "text": " The basic idea of this paper is to say, you know what, sometimes maybe predicting the", "tokens": [51134, 440, 3875, 1558, 295, 341, 3035, 307, 281, 584, 11, 291, 458, 437, 11, 2171, 1310, 32884, 264, 51734], "temperature": 0.0, "avg_logprob": -0.3861696455213759, "compression_ratio": 1.4965517241379311, "no_speech_prob": 0.09000552445650101}, {"id": 598, "seek": 251546, "start": 2515.46, "end": 2521.18, "text": " noise is a bad idea.", "tokens": [50364, 5658, 307, 257, 1578, 1558, 13, 50650], "temperature": 0.0, "avg_logprob": -0.26698852621990704, "compression_ratio": 1.9015544041450778, "no_speech_prob": 0.05747964605689049}, {"id": 599, "seek": 251546, "start": 2521.18, "end": 2524.38, "text": " And so like you can either try and predict the noise, or you can try and predict the", "tokens": [50650, 400, 370, 411, 291, 393, 2139, 853, 293, 6069, 264, 5658, 11, 420, 291, 393, 853, 293, 6069, 264, 50810], "temperature": 0.0, "avg_logprob": -0.26698852621990704, "compression_ratio": 1.9015544041450778, "no_speech_prob": 0.05747964605689049}, {"id": 600, "seek": 251546, "start": 2524.38, "end": 2530.82, "text": " clean image, and each of those can be a better idea in different situations.", "tokens": [50810, 2541, 3256, 11, 293, 1184, 295, 729, 393, 312, 257, 1101, 1558, 294, 819, 6851, 13, 51132], "temperature": 0.0, "avg_logprob": -0.26698852621990704, "compression_ratio": 1.9015544041450778, "no_speech_prob": 0.05747964605689049}, {"id": 601, "seek": 251546, "start": 2530.82, "end": 2535.9, "text": " If you're given something which is nearly pure noise, you know, the model's given something", "tokens": [51132, 759, 291, 434, 2212, 746, 597, 307, 6217, 6075, 5658, 11, 291, 458, 11, 264, 2316, 311, 2212, 746, 51386], "temperature": 0.0, "avg_logprob": -0.26698852621990704, "compression_ratio": 1.9015544041450778, "no_speech_prob": 0.05747964605689049}, {"id": 602, "seek": 251546, "start": 2535.9, "end": 2540.94, "text": " which is nearly pure noise, and is then asked to predict the noise, that's basically a waste", "tokens": [51386, 597, 307, 6217, 6075, 5658, 11, 293, 307, 550, 2351, 281, 6069, 264, 5658, 11, 300, 311, 1936, 257, 5964, 51638], "temperature": 0.0, "avg_logprob": -0.26698852621990704, "compression_ratio": 1.9015544041450778, "no_speech_prob": 0.05747964605689049}, {"id": 603, "seek": 254094, "start": 2540.94, "end": 2546.18, "text": " of time, because the whole thing's noise.", "tokens": [50364, 295, 565, 11, 570, 264, 1379, 551, 311, 5658, 13, 50626], "temperature": 0.0, "avg_logprob": -0.20799748402721477, "compression_ratio": 1.82648401826484, "no_speech_prob": 0.30044955015182495}, {"id": 604, "seek": 254094, "start": 2546.18, "end": 2550.66, "text": " If you do the opposite, which is you try to get it to predict the clean image, well then", "tokens": [50626, 759, 291, 360, 264, 6182, 11, 597, 307, 291, 853, 281, 483, 309, 281, 6069, 264, 2541, 3256, 11, 731, 550, 50850], "temperature": 0.0, "avg_logprob": -0.20799748402721477, "compression_ratio": 1.82648401826484, "no_speech_prob": 0.30044955015182495}, {"id": 605, "seek": 254094, "start": 2550.66, "end": 2553.7000000000003, "text": " if you give it a clean image that's nearly clean, and try to predict the clean image,", "tokens": [50850, 498, 291, 976, 309, 257, 2541, 3256, 300, 311, 6217, 2541, 11, 293, 853, 281, 6069, 264, 2541, 3256, 11, 51002], "temperature": 0.0, "avg_logprob": -0.20799748402721477, "compression_ratio": 1.82648401826484, "no_speech_prob": 0.30044955015182495}, {"id": 606, "seek": 254094, "start": 2553.7000000000003, "end": 2555.66, "text": " that's nearly a waste of time as well.", "tokens": [51002, 300, 311, 6217, 257, 5964, 295, 565, 382, 731, 13, 51100], "temperature": 0.0, "avg_logprob": -0.20799748402721477, "compression_ratio": 1.82648401826484, "no_speech_prob": 0.30044955015182495}, {"id": 607, "seek": 254094, "start": 2555.66, "end": 2560.58, "text": " So you want something which is like, regardless of how noisy the image is, you want it to", "tokens": [51100, 407, 291, 528, 746, 597, 307, 411, 11, 10060, 295, 577, 24518, 264, 3256, 307, 11, 291, 528, 309, 281, 51346], "temperature": 0.0, "avg_logprob": -0.20799748402721477, "compression_ratio": 1.82648401826484, "no_speech_prob": 0.30044955015182495}, {"id": 608, "seek": 254094, "start": 2560.58, "end": 2564.62, "text": " be kind of like an equally difficult problem to solve.", "tokens": [51346, 312, 733, 295, 411, 364, 12309, 2252, 1154, 281, 5039, 13, 51548], "temperature": 0.0, "avg_logprob": -0.20799748402721477, "compression_ratio": 1.82648401826484, "no_speech_prob": 0.30044955015182495}, {"id": 609, "seek": 256462, "start": 2564.62, "end": 2580.14, "text": " And so what Keras do is they basically use this new thing called C skip, which is a number", "tokens": [50364, 400, 370, 437, 591, 6985, 360, 307, 436, 1936, 764, 341, 777, 551, 1219, 383, 10023, 11, 597, 307, 257, 1230, 51140], "temperature": 0.0, "avg_logprob": -0.261341664328504, "compression_ratio": 1.68125, "no_speech_prob": 0.0384264811873436}, {"id": 610, "seek": 256462, "start": 2580.14, "end": 2585.2999999999997, "text": " which is basically saying like, you know what we should do for the training target, is not", "tokens": [51140, 597, 307, 1936, 1566, 411, 11, 291, 458, 437, 321, 820, 360, 337, 264, 3097, 3779, 11, 307, 406, 51398], "temperature": 0.0, "avg_logprob": -0.261341664328504, "compression_ratio": 1.68125, "no_speech_prob": 0.0384264811873436}, {"id": 611, "seek": 256462, "start": 2585.2999999999997, "end": 2591.7, "text": " just predict the noise all the time, not just predict the clean image all the time, but", "tokens": [51398, 445, 6069, 264, 5658, 439, 264, 565, 11, 406, 445, 6069, 264, 2541, 3256, 439, 264, 565, 11, 457, 51718], "temperature": 0.0, "avg_logprob": -0.261341664328504, "compression_ratio": 1.68125, "no_speech_prob": 0.0384264811873436}, {"id": 612, "seek": 259170, "start": 2591.7799999999997, "end": 2598.06, "text": " predict kind of a looped version of one or the other, depending on how noisy it is.", "tokens": [50368, 6069, 733, 295, 257, 6367, 292, 3037, 295, 472, 420, 264, 661, 11, 5413, 322, 577, 24518, 309, 307, 13, 50682], "temperature": 0.0, "avg_logprob": -0.34491450969989484, "compression_ratio": 1.4188034188034189, "no_speech_prob": 0.017985336482524872}, {"id": 613, "seek": 259170, "start": 2598.06, "end": 2609.6, "text": " So here Y is the plain image, and N is the noise.", "tokens": [50682, 407, 510, 398, 307, 264, 11121, 3256, 11, 293, 426, 307, 264, 5658, 13, 51259], "temperature": 0.0, "avg_logprob": -0.34491450969989484, "compression_ratio": 1.4188034188034189, "no_speech_prob": 0.017985336482524872}, {"id": 614, "seek": 259170, "start": 2609.6, "end": 2615.06, "text": " So Y plus N is the noised image.", "tokens": [51259, 407, 398, 1804, 426, 307, 264, 572, 2640, 3256, 13, 51532], "temperature": 0.0, "avg_logprob": -0.34491450969989484, "compression_ratio": 1.4188034188034189, "no_speech_prob": 0.017985336482524872}, {"id": 615, "seek": 261506, "start": 2615.06, "end": 2622.94, "text": " And so if C skip was zero, then we would be predicting the clean image.", "tokens": [50364, 400, 370, 498, 383, 10023, 390, 4018, 11, 550, 321, 576, 312, 32884, 264, 2541, 3256, 13, 50758], "temperature": 0.0, "avg_logprob": -0.22443606552568454, "compression_ratio": 1.9593908629441625, "no_speech_prob": 0.015179348178207874}, {"id": 616, "seek": 261506, "start": 2622.94, "end": 2631.38, "text": " And if C skip was one, we would be predicting Y minus Y, we would be predicting the noise.", "tokens": [50758, 400, 498, 383, 10023, 390, 472, 11, 321, 576, 312, 32884, 398, 3175, 398, 11, 321, 576, 312, 32884, 264, 5658, 13, 51180], "temperature": 0.0, "avg_logprob": -0.22443606552568454, "compression_ratio": 1.9593908629441625, "no_speech_prob": 0.015179348178207874}, {"id": 617, "seek": 261506, "start": 2631.38, "end": 2636.08, "text": " And so you can decide by picking a different C skip whether you're predicting the clean", "tokens": [51180, 400, 370, 291, 393, 4536, 538, 8867, 257, 819, 383, 10023, 1968, 291, 434, 32884, 264, 2541, 51415], "temperature": 0.0, "avg_logprob": -0.22443606552568454, "compression_ratio": 1.9593908629441625, "no_speech_prob": 0.015179348178207874}, {"id": 618, "seek": 261506, "start": 2636.08, "end": 2637.74, "text": " image or the noise.", "tokens": [51415, 3256, 420, 264, 5658, 13, 51498], "temperature": 0.0, "avg_logprob": -0.22443606552568454, "compression_ratio": 1.9593908629441625, "no_speech_prob": 0.015179348178207874}, {"id": 619, "seek": 261506, "start": 2637.74, "end": 2641.22, "text": " And so as you can see from the way they've written it, they make this a function.", "tokens": [51498, 400, 370, 382, 291, 393, 536, 490, 264, 636, 436, 600, 3720, 309, 11, 436, 652, 341, 257, 2445, 13, 51672], "temperature": 0.0, "avg_logprob": -0.22443606552568454, "compression_ratio": 1.9593908629441625, "no_speech_prob": 0.015179348178207874}, {"id": 620, "seek": 261506, "start": 2641.22, "end": 2643.62, "text": " They make it a function of sigma.", "tokens": [51672, 814, 652, 309, 257, 2445, 295, 12771, 13, 51792], "temperature": 0.0, "avg_logprob": -0.22443606552568454, "compression_ratio": 1.9593908629441625, "no_speech_prob": 0.015179348178207874}, {"id": 621, "seek": 264362, "start": 2643.62, "end": 2649.22, "text": " Now this is where we've got to a point now where we've kind of got a fairly much simpler", "tokens": [50364, 823, 341, 307, 689, 321, 600, 658, 281, 257, 935, 586, 689, 321, 600, 733, 295, 658, 257, 6457, 709, 18587, 50644], "temperature": 0.0, "avg_logprob": -0.2674222531525985, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.1365475356578827}, {"id": 622, "seek": 264362, "start": 2649.22, "end": 2650.22, "text": " notation.", "tokens": [50644, 24657, 13, 50694], "temperature": 0.0, "avg_logprob": -0.2674222531525985, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.1365475356578827}, {"id": 623, "seek": 264362, "start": 2650.22, "end": 2654.42, "text": " There's no more alpha bars, no more alphas, no more betas, no more beta bars.", "tokens": [50694, 821, 311, 572, 544, 8961, 10228, 11, 572, 544, 419, 7485, 11, 572, 544, 778, 296, 11, 572, 544, 9861, 10228, 13, 50904], "temperature": 0.0, "avg_logprob": -0.2674222531525985, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.1365475356578827}, {"id": 624, "seek": 264362, "start": 2654.42, "end": 2657.2999999999997, "text": " There's just a single thing called sigma.", "tokens": [50904, 821, 311, 445, 257, 2167, 551, 1219, 12771, 13, 51048], "temperature": 0.0, "avg_logprob": -0.2674222531525985, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.1365475356578827}, {"id": 625, "seek": 264362, "start": 2657.2999999999997, "end": 2662.1, "text": " Unfortunately sigma is the same thing as alpha bar used to be.", "tokens": [51048, 8590, 12771, 307, 264, 912, 551, 382, 8961, 2159, 1143, 281, 312, 13, 51288], "temperature": 0.0, "avg_logprob": -0.2674222531525985, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.1365475356578827}, {"id": 626, "seek": 264362, "start": 2662.1, "end": 2665.96, "text": " So we've simplified it, but we've also made things more confusing by using an existing", "tokens": [51288, 407, 321, 600, 26335, 309, 11, 457, 321, 600, 611, 1027, 721, 544, 13181, 538, 1228, 364, 6741, 51481], "temperature": 0.0, "avg_logprob": -0.2674222531525985, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.1365475356578827}, {"id": 627, "seek": 264362, "start": 2665.96, "end": 2668.2999999999997, "text": " symbol for something totally different.", "tokens": [51481, 5986, 337, 746, 3879, 819, 13, 51598], "temperature": 0.0, "avg_logprob": -0.2674222531525985, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.1365475356578827}, {"id": 628, "seek": 264362, "start": 2668.2999999999997, "end": 2670.7799999999997, "text": " So this is alpha bar.", "tokens": [51598, 407, 341, 307, 8961, 2159, 13, 51722], "temperature": 0.0, "avg_logprob": -0.2674222531525985, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.1365475356578827}, {"id": 629, "seek": 267078, "start": 2670.78, "end": 2677.34, "text": " So there's going to be a function that says depending on how much noise there is, we'll", "tokens": [50364, 407, 456, 311, 516, 281, 312, 257, 2445, 300, 1619, 5413, 322, 577, 709, 5658, 456, 307, 11, 321, 603, 50692], "temperature": 0.0, "avg_logprob": -0.2377054518547611, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.0016483914805576205}, {"id": 630, "seek": 267078, "start": 2677.34, "end": 2682.78, "text": " either predict the noise or we'll predict the clean image or we'll predict something", "tokens": [50692, 2139, 6069, 264, 5658, 420, 321, 603, 6069, 264, 2541, 3256, 420, 321, 603, 6069, 746, 50964], "temperature": 0.0, "avg_logprob": -0.2377054518547611, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.0016483914805576205}, {"id": 631, "seek": 267078, "start": 2682.78, "end": 2685.5, "text": " between the two.", "tokens": [50964, 1296, 264, 732, 13, 51100], "temperature": 0.0, "avg_logprob": -0.2377054518547611, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.0016483914805576205}, {"id": 632, "seek": 267078, "start": 2685.5, "end": 2694.6200000000003, "text": " So in the paper they showed this chart where they basically said like, okay, let's look", "tokens": [51100, 407, 294, 264, 3035, 436, 4712, 341, 6927, 689, 436, 1936, 848, 411, 11, 1392, 11, 718, 311, 574, 51556], "temperature": 0.0, "avg_logprob": -0.2377054518547611, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.0016483914805576205}, {"id": 633, "seek": 269462, "start": 2694.62, "end": 2706.74, "text": " at the loss to see how good are we with a trained model at predicting when sigma is", "tokens": [50364, 412, 264, 4470, 281, 536, 577, 665, 366, 321, 365, 257, 8895, 2316, 412, 32884, 562, 12771, 307, 50970], "temperature": 0.0, "avg_logprob": -0.24378257751464844, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07156215608119965}, {"id": 634, "seek": 269462, "start": 2706.74, "end": 2707.8599999999997, "text": " really low.", "tokens": [50970, 534, 2295, 13, 51026], "temperature": 0.0, "avg_logprob": -0.24378257751464844, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07156215608119965}, {"id": 635, "seek": 269462, "start": 2707.8599999999997, "end": 2713.66, "text": " So when there's very small alpha bar or when sigma is in the middle or when sigma is really", "tokens": [51026, 407, 562, 456, 311, 588, 1359, 8961, 2159, 420, 562, 12771, 307, 294, 264, 2808, 420, 562, 12771, 307, 534, 51316], "temperature": 0.0, "avg_logprob": -0.24378257751464844, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07156215608119965}, {"id": 636, "seek": 269462, "start": 2713.66, "end": 2715.54, "text": " high.", "tokens": [51316, 1090, 13, 51410], "temperature": 0.0, "avg_logprob": -0.24378257751464844, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07156215608119965}, {"id": 637, "seek": 269462, "start": 2715.54, "end": 2721.62, "text": " And they basically said, you know what, when it's nearly all noise or nearly no noise,", "tokens": [51410, 400, 436, 1936, 848, 11, 291, 458, 437, 11, 562, 309, 311, 6217, 439, 5658, 420, 6217, 572, 5658, 11, 51714], "temperature": 0.0, "avg_logprob": -0.24378257751464844, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.07156215608119965}, {"id": 638, "seek": 272162, "start": 2721.62, "end": 2725.7799999999997, "text": " you know, we're basically not able to do anything at all.", "tokens": [50364, 291, 458, 11, 321, 434, 1936, 406, 1075, 281, 360, 1340, 412, 439, 13, 50572], "temperature": 0.0, "avg_logprob": -0.2336393928527832, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.00011235239799134433}, {"id": 639, "seek": 272162, "start": 2725.7799999999997, "end": 2732.74, "text": " You know, we're basically good at doing things when there's a medium amount of noise.", "tokens": [50572, 509, 458, 11, 321, 434, 1936, 665, 412, 884, 721, 562, 456, 311, 257, 6399, 2372, 295, 5658, 13, 50920], "temperature": 0.0, "avg_logprob": -0.2336393928527832, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.00011235239799134433}, {"id": 640, "seek": 272162, "start": 2732.74, "end": 2738.24, "text": " So when deciding, okay, what sigmas are we going to send to this thing, the first thing", "tokens": [50920, 407, 562, 17990, 11, 1392, 11, 437, 4556, 3799, 366, 321, 516, 281, 2845, 281, 341, 551, 11, 264, 700, 551, 51195], "temperature": 0.0, "avg_logprob": -0.2336393928527832, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.00011235239799134433}, {"id": 641, "seek": 272162, "start": 2738.24, "end": 2741.94, "text": " we need to do is to figure out some sigmas.", "tokens": [51195, 321, 643, 281, 360, 307, 281, 2573, 484, 512, 4556, 3799, 13, 51380], "temperature": 0.0, "avg_logprob": -0.2336393928527832, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.00011235239799134433}, {"id": 642, "seek": 272162, "start": 2741.94, "end": 2749.94, "text": " And they said, okay, well let's pick a distribution of sigmas that matches this red curve here,", "tokens": [51380, 400, 436, 848, 11, 1392, 11, 731, 718, 311, 1888, 257, 7316, 295, 4556, 3799, 300, 10676, 341, 2182, 7605, 510, 11, 51780], "temperature": 0.0, "avg_logprob": -0.2336393928527832, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.00011235239799134433}, {"id": 643, "seek": 274994, "start": 2749.94, "end": 2751.7200000000003, "text": " as you can see.", "tokens": [50364, 382, 291, 393, 536, 13, 50453], "temperature": 0.0, "avg_logprob": -0.2518697432529779, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0004655239172279835}, {"id": 644, "seek": 274994, "start": 2751.7200000000003, "end": 2759.84, "text": " And so this is a normally distributed curve where this is on a log scale.", "tokens": [50453, 400, 370, 341, 307, 257, 5646, 12631, 7605, 689, 341, 307, 322, 257, 3565, 4373, 13, 50859], "temperature": 0.0, "avg_logprob": -0.2518697432529779, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0004655239172279835}, {"id": 645, "seek": 274994, "start": 2759.84, "end": 2763.76, "text": " So this is actually a log normal curve.", "tokens": [50859, 407, 341, 307, 767, 257, 3565, 2710, 7605, 13, 51055], "temperature": 0.0, "avg_logprob": -0.2518697432529779, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0004655239172279835}, {"id": 646, "seek": 274994, "start": 2763.76, "end": 2767.82, "text": " So to get the sigmas that they're going to use, they picked a normally distributed random", "tokens": [51055, 407, 281, 483, 264, 4556, 3799, 300, 436, 434, 516, 281, 764, 11, 436, 6183, 257, 5646, 12631, 4974, 51258], "temperature": 0.0, "avg_logprob": -0.2518697432529779, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0004655239172279835}, {"id": 647, "seek": 274994, "start": 2767.82, "end": 2772.44, "text": " number and then they x'ed it.", "tokens": [51258, 1230, 293, 550, 436, 2031, 6, 292, 309, 13, 51489], "temperature": 0.0, "avg_logprob": -0.2518697432529779, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0004655239172279835}, {"id": 648, "seek": 274994, "start": 2772.44, "end": 2776.04, "text": " And this is called a log normal distribution.", "tokens": [51489, 400, 341, 307, 1219, 257, 3565, 2710, 7316, 13, 51669], "temperature": 0.0, "avg_logprob": -0.2518697432529779, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0004655239172279835}, {"id": 649, "seek": 277604, "start": 2776.14, "end": 2783.18, "text": " And so they used a mean of minus 1.2 and a standard deviation of 1.2.", "tokens": [50369, 400, 370, 436, 1143, 257, 914, 295, 3175, 502, 13, 17, 293, 257, 3832, 25163, 295, 502, 13, 17, 13, 50721], "temperature": 0.0, "avg_logprob": -0.23049600389268662, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.005726827774196863}, {"id": 650, "seek": 277604, "start": 2783.18, "end": 2789.0, "text": " So that means that about one third of the time, they're going to be getting a number", "tokens": [50721, 407, 300, 1355, 300, 466, 472, 2636, 295, 264, 565, 11, 436, 434, 516, 281, 312, 1242, 257, 1230, 51012], "temperature": 0.0, "avg_logprob": -0.23049600389268662, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.005726827774196863}, {"id": 651, "seek": 277604, "start": 2789.0, "end": 2794.58, "text": " that's bigger than zero here.", "tokens": [51012, 300, 311, 3801, 813, 4018, 510, 13, 51291], "temperature": 0.0, "avg_logprob": -0.23049600389268662, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.005726827774196863}, {"id": 652, "seek": 277604, "start": 2794.58, "end": 2796.66, "text": " And e to the zero is one.", "tokens": [51291, 400, 308, 281, 264, 4018, 307, 472, 13, 51395], "temperature": 0.0, "avg_logprob": -0.23049600389268662, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.005726827774196863}, {"id": 653, "seek": 277604, "start": 2796.66, "end": 2801.82, "text": " So about one third of the time, they're going to be picking sigmas that are bigger than", "tokens": [51395, 407, 466, 472, 2636, 295, 264, 565, 11, 436, 434, 516, 281, 312, 8867, 4556, 3799, 300, 366, 3801, 813, 51653], "temperature": 0.0, "avg_logprob": -0.23049600389268662, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.005726827774196863}, {"id": 654, "seek": 277604, "start": 2801.82, "end": 2802.82, "text": " one.", "tokens": [51653, 472, 13, 51703], "temperature": 0.0, "avg_logprob": -0.23049600389268662, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.005726827774196863}, {"id": 655, "seek": 280282, "start": 2802.84, "end": 2810.46, "text": " And so here's a histogram I drew of the sigmas that we're going to be using.", "tokens": [50365, 400, 370, 510, 311, 257, 49816, 286, 12804, 295, 264, 4556, 3799, 300, 321, 434, 516, 281, 312, 1228, 13, 50746], "temperature": 0.0, "avg_logprob": -0.2301424199884588, "compression_ratio": 1.56, "no_speech_prob": 0.0014550181804224849}, {"id": 656, "seek": 280282, "start": 2810.46, "end": 2815.5, "text": " And so it's nearly always, you know, less than five.", "tokens": [50746, 400, 370, 309, 311, 6217, 1009, 11, 291, 458, 11, 1570, 813, 1732, 13, 50998], "temperature": 0.0, "avg_logprob": -0.2301424199884588, "compression_ratio": 1.56, "no_speech_prob": 0.0014550181804224849}, {"id": 657, "seek": 280282, "start": 2815.5, "end": 2817.7000000000003, "text": " But sometimes it's way out here.", "tokens": [50998, 583, 2171, 309, 311, 636, 484, 510, 13, 51108], "temperature": 0.0, "avg_logprob": -0.2301424199884588, "compression_ratio": 1.56, "no_speech_prob": 0.0014550181804224849}, {"id": 658, "seek": 280282, "start": 2817.7000000000003, "end": 2820.1400000000003, "text": " And so it's quite hard to read these histograms.", "tokens": [51108, 400, 370, 309, 311, 1596, 1152, 281, 1401, 613, 49816, 82, 13, 51230], "temperature": 0.0, "avg_logprob": -0.2301424199884588, "compression_ratio": 1.56, "no_speech_prob": 0.0014550181804224849}, {"id": 659, "seek": 280282, "start": 2820.1400000000003, "end": 2825.7000000000003, "text": " So this really nice library called Seaborn, which is built on top of Matplotlib, has some", "tokens": [51230, 407, 341, 534, 1481, 6405, 1219, 1100, 455, 1865, 11, 597, 307, 3094, 322, 1192, 295, 6789, 564, 310, 38270, 11, 575, 512, 51508], "temperature": 0.0, "avg_logprob": -0.2301424199884588, "compression_ratio": 1.56, "no_speech_prob": 0.0014550181804224849}, {"id": 660, "seek": 280282, "start": 2825.7000000000003, "end": 2829.8, "text": " more sophisticated and often nicer looking plots.", "tokens": [51508, 544, 16950, 293, 2049, 22842, 1237, 28609, 13, 51713], "temperature": 0.0, "avg_logprob": -0.2301424199884588, "compression_ratio": 1.56, "no_speech_prob": 0.0014550181804224849}, {"id": 661, "seek": 282980, "start": 2829.8, "end": 2834.1200000000003, "text": " And one of them they have is called a KDE plot, which is a kernel density plot.", "tokens": [50364, 400, 472, 295, 552, 436, 362, 307, 1219, 257, 591, 22296, 7542, 11, 597, 307, 257, 28256, 10305, 7542, 13, 50580], "temperature": 0.0, "avg_logprob": -0.2432084036345529, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.0453389510512352}, {"id": 662, "seek": 282980, "start": 2834.1200000000003, "end": 2837.88, "text": " It's a histogram, but it's smooth.", "tokens": [50580, 467, 311, 257, 49816, 11, 457, 309, 311, 5508, 13, 50768], "temperature": 0.0, "avg_logprob": -0.2432084036345529, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.0453389510512352}, {"id": 663, "seek": 282980, "start": 2837.88, "end": 2841.1200000000003, "text": " And so I clipped it at 10 so that you could see it better.", "tokens": [50768, 400, 370, 286, 596, 5529, 309, 412, 1266, 370, 300, 291, 727, 536, 309, 1101, 13, 50930], "temperature": 0.0, "avg_logprob": -0.2432084036345529, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.0453389510512352}, {"id": 664, "seek": 282980, "start": 2841.1200000000003, "end": 2845.52, "text": " So you can basically see that the vast majority of the time it's going to be somewhere, you", "tokens": [50930, 407, 291, 393, 1936, 536, 300, 264, 8369, 6286, 295, 264, 565, 309, 311, 516, 281, 312, 4079, 11, 291, 51150], "temperature": 0.0, "avg_logprob": -0.2432084036345529, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.0453389510512352}, {"id": 665, "seek": 282980, "start": 2845.52, "end": 2848.48, "text": " know, about 0.4 or 0.5.", "tokens": [51150, 458, 11, 466, 1958, 13, 19, 420, 1958, 13, 20, 13, 51298], "temperature": 0.0, "avg_logprob": -0.2432084036345529, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.0453389510512352}, {"id": 666, "seek": 282980, "start": 2848.48, "end": 2851.48, "text": " But sometimes it's going to be really big.", "tokens": [51298, 583, 2171, 309, 311, 516, 281, 312, 534, 955, 13, 51448], "temperature": 0.0, "avg_logprob": -0.2432084036345529, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.0453389510512352}, {"id": 667, "seek": 285148, "start": 2851.48, "end": 2862.72, "text": " So our Noisify is going to pick a sigma using that lognormal distribution.", "tokens": [50364, 407, 527, 883, 271, 2505, 307, 516, 281, 1888, 257, 12771, 1228, 300, 3565, 23157, 7316, 13, 50926], "temperature": 0.0, "avg_logprob": -0.3419988350790055, "compression_ratio": 1.5214285714285714, "no_speech_prob": 0.08266325294971466}, {"id": 668, "seek": 285148, "start": 2862.72, "end": 2865.64, "text": " And then we're going to get the noise as usual.", "tokens": [50926, 400, 550, 321, 434, 516, 281, 483, 264, 5658, 382, 7713, 13, 51072], "temperature": 0.0, "avg_logprob": -0.3419988350790055, "compression_ratio": 1.5214285714285714, "no_speech_prob": 0.08266325294971466}, {"id": 669, "seek": 285148, "start": 2865.64, "end": 2871.16, "text": " But now we're going to calculate C skip.", "tokens": [51072, 583, 586, 321, 434, 516, 281, 8873, 383, 10023, 13, 51348], "temperature": 0.0, "avg_logprob": -0.3419988350790055, "compression_ratio": 1.5214285714285714, "no_speech_prob": 0.08266325294971466}, {"id": 670, "seek": 285148, "start": 2871.16, "end": 2873.28, "text": " Because we're going to do that thing we just saw.", "tokens": [51348, 1436, 321, 434, 516, 281, 360, 300, 551, 321, 445, 1866, 13, 51454], "temperature": 0.0, "avg_logprob": -0.3419988350790055, "compression_ratio": 1.5214285714285714, "no_speech_prob": 0.08266325294971466}, {"id": 671, "seek": 287328, "start": 2873.28, "end": 2881.84, "text": " We're going to find something between the plane image and the noise input.", "tokens": [50364, 492, 434, 516, 281, 915, 746, 1296, 264, 5720, 3256, 293, 264, 5658, 4846, 13, 50792], "temperature": 0.0, "avg_logprob": -0.25079985225901885, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.0033761661034077406}, {"id": 672, "seek": 287328, "start": 2881.84, "end": 2885.5600000000004, "text": " So what do we use for C skip?", "tokens": [50792, 407, 437, 360, 321, 764, 337, 383, 10023, 30, 50978], "temperature": 0.0, "avg_logprob": -0.25079985225901885, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.0033761661034077406}, {"id": 673, "seek": 287328, "start": 2885.5600000000004, "end": 2890.6000000000004, "text": " We calculate it here.", "tokens": [50978, 492, 8873, 309, 510, 13, 51230], "temperature": 0.0, "avg_logprob": -0.25079985225901885, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.0033761661034077406}, {"id": 674, "seek": 287328, "start": 2890.6000000000004, "end": 2897.6800000000003, "text": " And so what we do is we say, what's the total amount of variance at some level of sigma?", "tokens": [51230, 400, 370, 437, 321, 360, 307, 321, 584, 11, 437, 311, 264, 3217, 2372, 295, 21977, 412, 512, 1496, 295, 12771, 30, 51584], "temperature": 0.0, "avg_logprob": -0.25079985225901885, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.0033761661034077406}, {"id": 675, "seek": 287328, "start": 2897.6800000000003, "end": 2900.2400000000002, "text": " Well it's going to be sigma squared.", "tokens": [51584, 1042, 309, 311, 516, 281, 312, 12771, 8889, 13, 51712], "temperature": 0.0, "avg_logprob": -0.25079985225901885, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.0033761661034077406}, {"id": 676, "seek": 287328, "start": 2900.2400000000002, "end": 2903.0400000000004, "text": " That's the definition of the variance of the noise.", "tokens": [51712, 663, 311, 264, 7123, 295, 264, 21977, 295, 264, 5658, 13, 51852], "temperature": 0.0, "avg_logprob": -0.25079985225901885, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.0033761661034077406}, {"id": 677, "seek": 290304, "start": 2903.8, "end": 2906.68, "text": " But we also have the sigma of the data itself.", "tokens": [50402, 583, 321, 611, 362, 264, 12771, 295, 264, 1412, 2564, 13, 50546], "temperature": 0.0, "avg_logprob": -0.21426851099187677, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.004264578223228455}, {"id": 678, "seek": 290304, "start": 2906.68, "end": 2911.32, "text": " So if we add those two together, we'll get the total variance.", "tokens": [50546, 407, 498, 321, 909, 729, 732, 1214, 11, 321, 603, 483, 264, 3217, 21977, 13, 50778], "temperature": 0.0, "avg_logprob": -0.21426851099187677, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.004264578223228455}, {"id": 679, "seek": 290304, "start": 2911.32, "end": 2922.4, "text": " And so what the Keras paper said to do is to do the variance of the data divided by", "tokens": [50778, 400, 370, 437, 264, 591, 6985, 3035, 848, 281, 360, 307, 281, 360, 264, 21977, 295, 264, 1412, 6666, 538, 51332], "temperature": 0.0, "avg_logprob": -0.21426851099187677, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.004264578223228455}, {"id": 680, "seek": 290304, "start": 2922.4, "end": 2927.72, "text": " the total variance, and use that for C skip.", "tokens": [51332, 264, 3217, 21977, 11, 293, 764, 300, 337, 383, 10023, 13, 51598], "temperature": 0.0, "avg_logprob": -0.21426851099187677, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.004264578223228455}, {"id": 681, "seek": 290304, "start": 2927.72, "end": 2931.44, "text": " So that means that if your total variance is really big, so in other words it's got", "tokens": [51598, 407, 300, 1355, 300, 498, 428, 3217, 21977, 307, 534, 955, 11, 370, 294, 661, 2283, 309, 311, 658, 51784], "temperature": 0.0, "avg_logprob": -0.21426851099187677, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.004264578223228455}, {"id": 682, "seek": 293144, "start": 2931.44, "end": 2936.48, "text": " a lot of noise, then C skip's going to be really small.", "tokens": [50364, 257, 688, 295, 5658, 11, 550, 383, 10023, 311, 516, 281, 312, 534, 1359, 13, 50616], "temperature": 0.0, "avg_logprob": -0.21868622977778596, "compression_ratio": 1.969387755102041, "no_speech_prob": 0.0018675383180379868}, {"id": 683, "seek": 293144, "start": 2936.48, "end": 2941.52, "text": " So if you've got a lot of noise, then this bit here will be really small.", "tokens": [50616, 407, 498, 291, 600, 658, 257, 688, 295, 5658, 11, 550, 341, 857, 510, 486, 312, 534, 1359, 13, 50868], "temperature": 0.0, "avg_logprob": -0.21868622977778596, "compression_ratio": 1.969387755102041, "no_speech_prob": 0.0018675383180379868}, {"id": 684, "seek": 293144, "start": 2941.52, "end": 2947.16, "text": " So that means if there's a lot of noise, try to predict the original image.", "tokens": [50868, 407, 300, 1355, 498, 456, 311, 257, 688, 295, 5658, 11, 853, 281, 6069, 264, 3380, 3256, 13, 51150], "temperature": 0.0, "avg_logprob": -0.21868622977778596, "compression_ratio": 1.969387755102041, "no_speech_prob": 0.0018675383180379868}, {"id": 685, "seek": 293144, "start": 2947.16, "end": 2950.52, "text": " That makes sense, because predicting the noise would be too easy.", "tokens": [51150, 663, 1669, 2020, 11, 570, 32884, 264, 5658, 576, 312, 886, 1858, 13, 51318], "temperature": 0.0, "avg_logprob": -0.21868622977778596, "compression_ratio": 1.969387755102041, "no_speech_prob": 0.0018675383180379868}, {"id": 686, "seek": 293144, "start": 2950.52, "end": 2956.96, "text": " If there's hardly any noise, then this will be, total variance will be really small.", "tokens": [51318, 759, 456, 311, 13572, 604, 5658, 11, 550, 341, 486, 312, 11, 3217, 21977, 486, 312, 534, 1359, 13, 51640], "temperature": 0.0, "avg_logprob": -0.21868622977778596, "compression_ratio": 1.969387755102041, "no_speech_prob": 0.0018675383180379868}, {"id": 687, "seek": 293144, "start": 2956.96, "end": 2958.88, "text": " So C skip will be really big.", "tokens": [51640, 407, 383, 10023, 486, 312, 534, 955, 13, 51736], "temperature": 0.0, "avg_logprob": -0.21868622977778596, "compression_ratio": 1.969387755102041, "no_speech_prob": 0.0018675383180379868}, {"id": 688, "seek": 295888, "start": 2958.88, "end": 2965.0, "text": " And so if there's hardly any noise, then try to predict the noise.", "tokens": [50364, 400, 370, 498, 456, 311, 13572, 604, 5658, 11, 550, 853, 281, 6069, 264, 5658, 13, 50670], "temperature": 0.0, "avg_logprob": -0.23403512107001412, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.01205105148255825}, {"id": 689, "seek": 295888, "start": 2965.0, "end": 2968.76, "text": " And so that's basically what this C skip does.", "tokens": [50670, 400, 370, 300, 311, 1936, 437, 341, 383, 10023, 775, 13, 50858], "temperature": 0.0, "avg_logprob": -0.23403512107001412, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.01205105148255825}, {"id": 690, "seek": 295888, "start": 2968.76, "end": 2974.04, "text": " So it's a kind of slightly weird idea, is that our target, the thing we're trying to", "tokens": [50858, 407, 309, 311, 257, 733, 295, 4748, 3657, 1558, 11, 307, 300, 527, 3779, 11, 264, 551, 321, 434, 1382, 281, 51122], "temperature": 0.0, "avg_logprob": -0.23403512107001412, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.01205105148255825}, {"id": 691, "seek": 295888, "start": 2974.04, "end": 2979.56, "text": " do actually, is not the input image, sorry, the original image.", "tokens": [51122, 360, 767, 11, 307, 406, 264, 4846, 3256, 11, 2597, 11, 264, 3380, 3256, 13, 51398], "temperature": 0.0, "avg_logprob": -0.23403512107001412, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.01205105148255825}, {"id": 692, "seek": 295888, "start": 2979.56, "end": 2982.32, "text": " It's not the noise, but it's somewhere between the two.", "tokens": [51398, 467, 311, 406, 264, 5658, 11, 457, 309, 311, 4079, 1296, 264, 732, 13, 51536], "temperature": 0.0, "avg_logprob": -0.23403512107001412, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.01205105148255825}, {"id": 693, "seek": 295888, "start": 2982.32, "end": 2986.8, "text": " And I found the easiest way to understand that is to draw a picture of it.", "tokens": [51536, 400, 286, 1352, 264, 12889, 636, 281, 1223, 300, 307, 281, 2642, 257, 3036, 295, 309, 13, 51760], "temperature": 0.0, "avg_logprob": -0.23403512107001412, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.01205105148255825}, {"id": 694, "seek": 298680, "start": 2986.8, "end": 2994.2400000000002, "text": " So here is some examples of noised input, with various amounts of, with various sigmas.", "tokens": [50364, 407, 510, 307, 512, 5110, 295, 572, 2640, 4846, 11, 365, 3683, 11663, 295, 11, 365, 3683, 4556, 3799, 13, 50736], "temperature": 0.0, "avg_logprob": -0.30709401867057706, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.004070099908858538}, {"id": 695, "seek": 298680, "start": 2994.2400000000002, "end": 2996.4, "text": " Remember sigma is alpha bar.", "tokens": [50736, 5459, 12771, 307, 8961, 2159, 13, 50844], "temperature": 0.0, "avg_logprob": -0.30709401867057706, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.004070099908858538}, {"id": 696, "seek": 298680, "start": 2996.4, "end": 3001.0800000000004, "text": " So here's an example with very little noise, 0.06.", "tokens": [50844, 407, 510, 311, 364, 1365, 365, 588, 707, 5658, 11, 1958, 13, 12791, 13, 51078], "temperature": 0.0, "avg_logprob": -0.30709401867057706, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.004070099908858538}, {"id": 697, "seek": 298680, "start": 3001.0800000000004, "end": 3007.6400000000003, "text": " And so in this case, the target is predict the noise.", "tokens": [51078, 400, 370, 294, 341, 1389, 11, 264, 3779, 307, 6069, 264, 5658, 13, 51406], "temperature": 0.0, "avg_logprob": -0.30709401867057706, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.004070099908858538}, {"id": 698, "seek": 298680, "start": 3007.6400000000003, "end": 3011.5600000000004, "text": " So that's the hard thing to do, is predict the noise.", "tokens": [51406, 407, 300, 311, 264, 1152, 551, 281, 360, 11, 307, 6069, 264, 5658, 13, 51602], "temperature": 0.0, "avg_logprob": -0.30709401867057706, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.004070099908858538}, {"id": 699, "seek": 301156, "start": 3011.56, "end": 3016.44, "text": " Or else, here's an example, 4.53, which is nearly all noise.", "tokens": [50364, 1610, 1646, 11, 510, 311, 364, 1365, 11, 1017, 13, 19584, 11, 597, 307, 6217, 439, 5658, 13, 50608], "temperature": 0.0, "avg_logprob": -0.2288753820020099, "compression_ratio": 1.7, "no_speech_prob": 0.12242747098207474}, {"id": 700, "seek": 301156, "start": 3016.44, "end": 3022.6, "text": " So for nearly all noise, the target is predict the image.", "tokens": [50608, 407, 337, 6217, 439, 5658, 11, 264, 3779, 307, 6069, 264, 3256, 13, 50916], "temperature": 0.0, "avg_logprob": -0.2288753820020099, "compression_ratio": 1.7, "no_speech_prob": 0.12242747098207474}, {"id": 701, "seek": 301156, "start": 3022.6, "end": 3028.32, "text": " And then for something which is a little bit between the two, like here, 0.64, the target", "tokens": [50916, 400, 550, 337, 746, 597, 307, 257, 707, 857, 1296, 264, 732, 11, 411, 510, 11, 1958, 13, 19395, 11, 264, 3779, 51202], "temperature": 0.0, "avg_logprob": -0.2288753820020099, "compression_ratio": 1.7, "no_speech_prob": 0.12242747098207474}, {"id": 702, "seek": 301156, "start": 3028.32, "end": 3032.56, "text": " is predict some of the noise and some of the image.", "tokens": [51202, 307, 6069, 512, 295, 264, 5658, 293, 512, 295, 264, 3256, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2288753820020099, "compression_ratio": 1.7, "no_speech_prob": 0.12242747098207474}, {"id": 703, "seek": 301156, "start": 3032.56, "end": 3036.4, "text": " So that's the idea of Paris.", "tokens": [51414, 407, 300, 311, 264, 1558, 295, 430, 27489, 13, 51606], "temperature": 0.0, "avg_logprob": -0.2288753820020099, "compression_ratio": 1.7, "no_speech_prob": 0.12242747098207474}, {"id": 704, "seek": 303640, "start": 3036.4, "end": 3045.96, "text": " And so what this does is it's making the, you know, problem to be solved by the unit", "tokens": [50364, 400, 370, 437, 341, 775, 307, 309, 311, 1455, 264, 11, 291, 458, 11, 1154, 281, 312, 13041, 538, 264, 4985, 50842], "temperature": 0.0, "avg_logprob": -0.2975886707574549, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.12747988104820251}, {"id": 705, "seek": 303640, "start": 3045.96, "end": 3050.32, "text": " equally difficult, regardless of what sigma is.", "tokens": [50842, 12309, 2252, 11, 10060, 295, 437, 12771, 307, 13, 51060], "temperature": 0.0, "avg_logprob": -0.2975886707574549, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.12747988104820251}, {"id": 706, "seek": 303640, "start": 3050.32, "end": 3054.36, "text": " It doesn't solve our input scaling problem.", "tokens": [51060, 467, 1177, 380, 5039, 527, 4846, 21589, 1154, 13, 51262], "temperature": 0.0, "avg_logprob": -0.2975886707574549, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.12747988104820251}, {"id": 707, "seek": 303640, "start": 3054.36, "end": 3059.0, "text": " It solves our kind of difficulty scaling problem.", "tokens": [51262, 467, 39890, 527, 733, 295, 10360, 21589, 1154, 13, 51494], "temperature": 0.0, "avg_logprob": -0.2975886707574549, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.12747988104820251}, {"id": 708, "seek": 303640, "start": 3059.0, "end": 3062.2400000000002, "text": " To solve the input scaling problem, they do it.", "tokens": [51494, 1407, 5039, 264, 4846, 21589, 1154, 11, 436, 360, 309, 13, 51656], "temperature": 0.0, "avg_logprob": -0.2975886707574549, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.12747988104820251}, {"id": 709, "seek": 306224, "start": 3062.2799999999997, "end": 3065.3999999999996, "text": " I just wanted to make one quick note.", "tokens": [50366, 286, 445, 1415, 281, 652, 472, 1702, 3637, 13, 50522], "temperature": 0.0, "avg_logprob": -0.3309905282382307, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.13821744918823242}, {"id": 710, "seek": 306224, "start": 3065.3999999999996, "end": 3070.08, "text": " And so like this sort of idea of, like is also interpolating between the noise and the", "tokens": [50522, 400, 370, 411, 341, 1333, 295, 1558, 295, 11, 411, 307, 611, 44902, 990, 1296, 264, 5658, 293, 264, 50756], "temperature": 0.0, "avg_logprob": -0.3309905282382307, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.13821744918823242}, {"id": 711, "seek": 306224, "start": 3070.08, "end": 3075.52, "text": " image, is this similar to what's called a V objective as well?", "tokens": [50756, 3256, 11, 307, 341, 2531, 281, 437, 311, 1219, 257, 691, 10024, 382, 731, 30, 51028], "temperature": 0.0, "avg_logprob": -0.3309905282382307, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.13821744918823242}, {"id": 712, "seek": 306224, "start": 3075.52, "end": 3079.3199999999997, "text": " So there's also a similar kind of, it's yeah, it's very quite similar to what Keras of Dell", "tokens": [51028, 407, 456, 311, 611, 257, 2531, 733, 295, 11, 309, 311, 1338, 11, 309, 311, 588, 1596, 2531, 281, 437, 591, 6985, 295, 5831, 75, 51218], "temperature": 0.0, "avg_logprob": -0.3309905282382307, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.13821744918823242}, {"id": 713, "seek": 306224, "start": 3079.3199999999997, "end": 3083.4399999999996, "text": " has, but that's also now been used in a lot of different models.", "tokens": [51218, 575, 11, 457, 300, 311, 611, 586, 668, 1143, 294, 257, 688, 295, 819, 5245, 13, 51424], "temperature": 0.0, "avg_logprob": -0.3309905282382307, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.13821744918823242}, {"id": 714, "seek": 306224, "start": 3083.4399999999996, "end": 3088.62, "text": " Like for example, stable diffusion 2.0 was trained with this sort of V objective.", "tokens": [51424, 1743, 337, 1365, 11, 8351, 25242, 568, 13, 15, 390, 8895, 365, 341, 1333, 295, 691, 10024, 13, 51683], "temperature": 0.0, "avg_logprob": -0.3309905282382307, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.13821744918823242}, {"id": 715, "seek": 308862, "start": 3088.62, "end": 3093.46, "text": " So people are using this sort of methodology and getting good results.", "tokens": [50364, 407, 561, 366, 1228, 341, 1333, 295, 24850, 293, 1242, 665, 3542, 13, 50606], "temperature": 0.0, "avg_logprob": -0.28439081798900256, "compression_ratio": 1.5091743119266054, "no_speech_prob": 0.10372234135866165}, {"id": 716, "seek": 308862, "start": 3093.46, "end": 3097.46, "text": " And yeah, so it's an actual practical thing that people are doing.", "tokens": [50606, 400, 1338, 11, 370, 309, 311, 364, 3539, 8496, 551, 300, 561, 366, 884, 13, 50806], "temperature": 0.0, "avg_logprob": -0.28439081798900256, "compression_ratio": 1.5091743119266054, "no_speech_prob": 0.10372234135866165}, {"id": 717, "seek": 308862, "start": 3097.46, "end": 3099.14, "text": " So yeah, just want to make a note of that.", "tokens": [50806, 407, 1338, 11, 445, 528, 281, 652, 257, 3637, 295, 300, 13, 50890], "temperature": 0.0, "avg_logprob": -0.28439081798900256, "compression_ratio": 1.5091743119266054, "no_speech_prob": 0.10372234135866165}, {"id": 718, "seek": 308862, "start": 3099.14, "end": 3100.14, "text": " Yeah.", "tokens": [50890, 865, 13, 50940], "temperature": 0.0, "avg_logprob": -0.28439081798900256, "compression_ratio": 1.5091743119266054, "no_speech_prob": 0.10372234135866165}, {"id": 719, "seek": 308862, "start": 3100.14, "end": 3107.52, "text": " As is the case of basically all papers created by NVIDIA researchers, of which this is one,", "tokens": [50940, 1018, 307, 264, 1389, 295, 1936, 439, 10577, 2942, 538, 426, 3958, 6914, 10309, 11, 295, 597, 341, 307, 472, 11, 51309], "temperature": 0.0, "avg_logprob": -0.28439081798900256, "compression_ratio": 1.5091743119266054, "no_speech_prob": 0.10372234135866165}, {"id": 720, "seek": 308862, "start": 3107.52, "end": 3111.06, "text": " it flies under the radar and everybody ignores it.", "tokens": [51309, 309, 17414, 833, 264, 16544, 293, 2201, 5335, 2706, 309, 13, 51486], "temperature": 0.0, "avg_logprob": -0.28439081798900256, "compression_ratio": 1.5091743119266054, "no_speech_prob": 0.10372234135866165}, {"id": 721, "seek": 311106, "start": 3111.06, "end": 3117.7799999999997, "text": " The V objective paper came from the senior author was Tim Salamons, which is Google,", "tokens": [50364, 440, 691, 10024, 3035, 1361, 490, 264, 7965, 3793, 390, 7172, 5996, 335, 892, 11, 597, 307, 3329, 11, 50700], "temperature": 0.0, "avg_logprob": -0.35366255776924, "compression_ratio": 1.6488549618320612, "no_speech_prob": 0.20421911776065826}, {"id": 722, "seek": 311106, "start": 3117.7799999999997, "end": 3118.7799999999997, "text": " right?", "tokens": [50700, 558, 30, 50750], "temperature": 0.0, "avg_logprob": -0.35366255776924, "compression_ratio": 1.6488549618320612, "no_speech_prob": 0.20421911776065826}, {"id": 723, "seek": 311106, "start": 3118.7799999999997, "end": 3119.7799999999997, "text": " Yeah.", "tokens": [50750, 865, 13, 50800], "temperature": 0.0, "avg_logprob": -0.35366255776924, "compression_ratio": 1.6488549618320612, "no_speech_prob": 0.20421911776065826}, {"id": 724, "seek": 311106, "start": 3119.7799999999997, "end": 3122.9, "text": " And so anything from Google and OpenAI, everybody listens to.", "tokens": [50800, 400, 370, 1340, 490, 3329, 293, 7238, 48698, 11, 2201, 35959, 281, 13, 50956], "temperature": 0.0, "avg_logprob": -0.35366255776924, "compression_ratio": 1.6488549618320612, "no_speech_prob": 0.20421911776065826}, {"id": 725, "seek": 311106, "start": 3122.9, "end": 3128.1, "text": " So yeah, although Keras I think has done the more complete version of this.", "tokens": [50956, 407, 1338, 11, 4878, 591, 6985, 286, 519, 575, 1096, 264, 544, 3566, 3037, 295, 341, 13, 51216], "temperature": 0.0, "avg_logprob": -0.35366255776924, "compression_ratio": 1.6488549618320612, "no_speech_prob": 0.20421911776065826}, {"id": 726, "seek": 311106, "start": 3128.1, "end": 3135.98, "text": " And in fact, the V objective was almost like mentioned in passing in the distillation paper.", "tokens": [51216, 400, 294, 1186, 11, 264, 691, 10024, 390, 1920, 411, 2835, 294, 8437, 294, 264, 42923, 399, 3035, 13, 51610], "temperature": 0.0, "avg_logprob": -0.35366255776924, "compression_ratio": 1.6488549618320612, "no_speech_prob": 0.20421911776065826}, {"id": 727, "seek": 311106, "start": 3135.98, "end": 3138.5, "text": " But yeah, that's the one that everybody has ended up looking at.", "tokens": [51610, 583, 1338, 11, 300, 311, 264, 472, 300, 2201, 575, 4590, 493, 1237, 412, 13, 51736], "temperature": 0.0, "avg_logprob": -0.35366255776924, "compression_ratio": 1.6488549618320612, "no_speech_prob": 0.20421911776065826}, {"id": 728, "seek": 311106, "start": 3138.5, "end": 3139.98, "text": " But I think this is the more complete.", "tokens": [51736, 583, 286, 519, 341, 307, 264, 544, 3566, 13, 51810], "temperature": 0.0, "avg_logprob": -0.35366255776924, "compression_ratio": 1.6488549618320612, "no_speech_prob": 0.20421911776065826}, {"id": 729, "seek": 313998, "start": 3140.22, "end": 3144.9, "text": " I think what happened with the V objective is not many people paid attention to it.", "tokens": [50376, 286, 519, 437, 2011, 365, 264, 691, 10024, 307, 406, 867, 561, 4835, 3202, 281, 309, 13, 50610], "temperature": 0.0, "avg_logprob": -0.3109424591064453, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.021284176036715508}, {"id": 730, "seek": 313998, "start": 3144.9, "end": 3149.22, "text": " I think folks like Kat and Robin and these sorts of folks are actually paying attention", "tokens": [50610, 286, 519, 4024, 411, 8365, 293, 16533, 293, 613, 7527, 295, 4024, 366, 767, 6229, 3202, 50826], "temperature": 0.0, "avg_logprob": -0.3109424591064453, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.021284176036715508}, {"id": 731, "seek": 313998, "start": 3149.22, "end": 3152.9, "text": " to that V objective in that Google brain paper.", "tokens": [50826, 281, 300, 691, 10024, 294, 300, 3329, 3567, 3035, 13, 51010], "temperature": 0.0, "avg_logprob": -0.3109424591064453, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.021284176036715508}, {"id": 732, "seek": 313998, "start": 3152.9, "end": 3156.54, "text": " But then also this paper did a much more principled analysis of this sort of thing.", "tokens": [51010, 583, 550, 611, 341, 3035, 630, 257, 709, 544, 3681, 15551, 5215, 295, 341, 1333, 295, 551, 13, 51192], "temperature": 0.0, "avg_logprob": -0.3109424591064453, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.021284176036715508}, {"id": 733, "seek": 313998, "start": 3156.54, "end": 3160.22, "text": " So yeah, I think it's very interesting how, yeah.", "tokens": [51192, 407, 1338, 11, 286, 519, 309, 311, 588, 1880, 577, 11, 1338, 13, 51376], "temperature": 0.0, "avg_logprob": -0.3109424591064453, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.021284176036715508}, {"id": 734, "seek": 313998, "start": 3160.22, "end": 3164.7400000000002, "text": " Sometimes even these sort of side notes in papers that maybe people don't pay much attention", "tokens": [51376, 4803, 754, 613, 1333, 295, 1252, 5570, 294, 10577, 300, 1310, 561, 500, 380, 1689, 709, 3202, 51602], "temperature": 0.0, "avg_logprob": -0.3109424591064453, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.021284176036715508}, {"id": 735, "seek": 313998, "start": 3164.7400000000002, "end": 3166.86, "text": " to, they can actually be quite important.", "tokens": [51602, 281, 11, 436, 393, 767, 312, 1596, 1021, 13, 51708], "temperature": 0.0, "avg_logprob": -0.3109424591064453, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.021284176036715508}, {"id": 736, "seek": 313998, "start": 3166.86, "end": 3167.86, "text": " Yeah.", "tokens": [51708, 865, 13, 51758], "temperature": 0.0, "avg_logprob": -0.3109424591064453, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.021284176036715508}, {"id": 737, "seek": 316786, "start": 3167.86, "end": 3170.78, "text": " So, okay.", "tokens": [50364, 407, 11, 1392, 13, 50510], "temperature": 0.0, "avg_logprob": -0.27776398055854884, "compression_ratio": 1.8022598870056497, "no_speech_prob": 0.005640679970383644}, {"id": 738, "seek": 316786, "start": 3170.78, "end": 3177.7000000000003, "text": " So the noise input as usual is the input image plus the noise times the sigma.", "tokens": [50510, 407, 264, 5658, 4846, 382, 7713, 307, 264, 4846, 3256, 1804, 264, 5658, 1413, 264, 12771, 13, 50856], "temperature": 0.0, "avg_logprob": -0.27776398055854884, "compression_ratio": 1.8022598870056497, "no_speech_prob": 0.005640679970383644}, {"id": 739, "seek": 316786, "start": 3177.7000000000003, "end": 3184.46, "text": " But then, and then as we discussed, we decide how to kind of decide what our target is.", "tokens": [50856, 583, 550, 11, 293, 550, 382, 321, 7152, 11, 321, 4536, 577, 281, 733, 295, 4536, 437, 527, 3779, 307, 13, 51194], "temperature": 0.0, "avg_logprob": -0.27776398055854884, "compression_ratio": 1.8022598870056497, "no_speech_prob": 0.005640679970383644}, {"id": 740, "seek": 316786, "start": 3184.46, "end": 3192.06, "text": " But then we actually take that noise input and we scale it up or down by this number.", "tokens": [51194, 583, 550, 321, 767, 747, 300, 5658, 4846, 293, 321, 4373, 309, 493, 420, 760, 538, 341, 1230, 13, 51574], "temperature": 0.0, "avg_logprob": -0.27776398055854884, "compression_ratio": 1.8022598870056497, "no_speech_prob": 0.005640679970383644}, {"id": 741, "seek": 316786, "start": 3192.06, "end": 3197.7000000000003, "text": " And the target, we also scale up or down by this number.", "tokens": [51574, 400, 264, 3779, 11, 321, 611, 4373, 493, 420, 760, 538, 341, 1230, 13, 51856], "temperature": 0.0, "avg_logprob": -0.27776398055854884, "compression_ratio": 1.8022598870056497, "no_speech_prob": 0.005640679970383644}, {"id": 742, "seek": 319770, "start": 3198.54, "end": 3203.3799999999997, "text": " And those are both calculated in this thing as well.", "tokens": [50406, 400, 729, 366, 1293, 15598, 294, 341, 551, 382, 731, 13, 50648], "temperature": 0.0, "avg_logprob": -0.25975313907911796, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.0018101031892001629}, {"id": 743, "seek": 319770, "start": 3203.3799999999997, "end": 3208.14, "text": " So here's C out and here's C in.", "tokens": [50648, 407, 510, 311, 383, 484, 293, 510, 311, 383, 294, 13, 50886], "temperature": 0.0, "avg_logprob": -0.25975313907911796, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.0018101031892001629}, {"id": 744, "seek": 319770, "start": 3208.14, "end": 3212.2599999999998, "text": " Now I just wanted to show one example of where these numbers come from, because for a while", "tokens": [50886, 823, 286, 445, 1415, 281, 855, 472, 1365, 295, 689, 613, 3547, 808, 490, 11, 570, 337, 257, 1339, 51092], "temperature": 0.0, "avg_logprob": -0.25975313907911796, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.0018101031892001629}, {"id": 745, "seek": 319770, "start": 3212.2599999999998, "end": 3214.2599999999998, "text": " they all seem pretty mysterious to me.", "tokens": [51092, 436, 439, 1643, 1238, 13831, 281, 385, 13, 51192], "temperature": 0.0, "avg_logprob": -0.25975313907911796, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.0018101031892001629}, {"id": 746, "seek": 319770, "start": 3214.2599999999998, "end": 3218.18, "text": " And I felt like I'd never be smart enough to understand them, particularly because they", "tokens": [51192, 400, 286, 2762, 411, 286, 1116, 1128, 312, 4069, 1547, 281, 1223, 552, 11, 4098, 570, 436, 51388], "temperature": 0.0, "avg_logprob": -0.25975313907911796, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.0018101031892001629}, {"id": 747, "seek": 319770, "start": 3218.18, "end": 3222.8599999999997, "text": " are explained in the mathematical appendix of this paper, which are always the bits I", "tokens": [51388, 366, 8825, 294, 264, 18894, 34116, 970, 295, 341, 3035, 11, 597, 366, 1009, 264, 9239, 286, 51622], "temperature": 0.0, "avg_logprob": -0.25975313907911796, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.0018101031892001629}, {"id": 748, "seek": 319770, "start": 3222.8599999999997, "end": 3226.74, "text": " don't understand, until I actually try to, and then it tends to turn out they're not", "tokens": [51622, 500, 380, 1223, 11, 1826, 286, 767, 853, 281, 11, 293, 550, 309, 12258, 281, 1261, 484, 436, 434, 406, 51816], "temperature": 0.0, "avg_logprob": -0.25975313907911796, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.0018101031892001629}, {"id": 749, "seek": 322674, "start": 3226.7799999999997, "end": 3231.02, "text": " so bad after all, which was certainly the case here.", "tokens": [50366, 370, 1578, 934, 439, 11, 597, 390, 3297, 264, 1389, 510, 13, 50578], "temperature": 0.0, "avg_logprob": -0.48501433266533744, "compression_ratio": 1.4, "no_speech_prob": 0.014726895838975906}, {"id": 750, "seek": 322674, "start": 3231.02, "end": 3232.02, "text": " Which?", "tokens": [50578, 3013, 30, 50628], "temperature": 0.0, "avg_logprob": -0.48501433266533744, "compression_ratio": 1.4, "no_speech_prob": 0.014726895838975906}, {"id": 751, "seek": 322674, "start": 3232.02, "end": 3241.5, "text": " I think it was up, it was B something, I think.", "tokens": [50628, 286, 519, 309, 390, 493, 11, 309, 390, 363, 746, 11, 286, 519, 13, 51102], "temperature": 0.0, "avg_logprob": -0.48501433266533744, "compression_ratio": 1.4, "no_speech_prob": 0.014726895838975906}, {"id": 752, "seek": 322674, "start": 3241.5, "end": 3243.3399999999997, "text": " So B6, I think.", "tokens": [51102, 407, 363, 21, 11, 286, 519, 13, 51194], "temperature": 0.0, "avg_logprob": -0.48501433266533744, "compression_ratio": 1.4, "no_speech_prob": 0.014726895838975906}, {"id": 753, "seek": 322674, "start": 3243.3399999999997, "end": 3244.3399999999997, "text": " Is that the one?", "tokens": [51194, 1119, 300, 264, 472, 30, 51244], "temperature": 0.0, "avg_logprob": -0.48501433266533744, "compression_ratio": 1.4, "no_speech_prob": 0.014726895838975906}, {"id": 754, "seek": 322674, "start": 3244.3399999999997, "end": 3245.7, "text": " Oh yeah.", "tokens": [51244, 876, 1338, 13, 51312], "temperature": 0.0, "avg_logprob": -0.48501433266533744, "compression_ratio": 1.4, "no_speech_prob": 0.014726895838975906}, {"id": 755, "seek": 322674, "start": 3245.7, "end": 3252.02, "text": " So in appendix B6, which does look pretty terrifying.", "tokens": [51312, 407, 294, 34116, 970, 363, 21, 11, 597, 775, 574, 1238, 18106, 13, 51628], "temperature": 0.0, "avg_logprob": -0.48501433266533744, "compression_ratio": 1.4, "no_speech_prob": 0.014726895838975906}, {"id": 756, "seek": 325202, "start": 3252.02, "end": 3255.46, "text": " But if you ever, if you actually look at, for example, what we're just looking at, C", "tokens": [50364, 583, 498, 291, 1562, 11, 498, 291, 767, 574, 412, 11, 337, 1365, 11, 437, 321, 434, 445, 1237, 412, 11, 383, 50536], "temperature": 0.0, "avg_logprob": -0.31578035591062437, "compression_ratio": 1.8312236286919832, "no_speech_prob": 0.25969573855400085}, {"id": 757, "seek": 325202, "start": 3255.46, "end": 3257.98, "text": " in, it's like, how do they calculate?", "tokens": [50536, 294, 11, 309, 311, 411, 11, 577, 360, 436, 8873, 30, 50662], "temperature": 0.0, "avg_logprob": -0.31578035591062437, "compression_ratio": 1.8312236286919832, "no_speech_prob": 0.25969573855400085}, {"id": 758, "seek": 325202, "start": 3257.98, "end": 3260.5, "text": " So C in is this.", "tokens": [50662, 407, 383, 294, 307, 341, 13, 50788], "temperature": 0.0, "avg_logprob": -0.31578035591062437, "compression_ratio": 1.8312236286919832, "no_speech_prob": 0.25969573855400085}, {"id": 759, "seek": 325202, "start": 3260.5, "end": 3264.22, "text": " Now this is the variance of the noise.", "tokens": [50788, 823, 341, 307, 264, 21977, 295, 264, 5658, 13, 50974], "temperature": 0.0, "avg_logprob": -0.31578035591062437, "compression_ratio": 1.8312236286919832, "no_speech_prob": 0.25969573855400085}, {"id": 760, "seek": 325202, "start": 3264.22, "end": 3265.74, "text": " This is the variance of the data.", "tokens": [50974, 639, 307, 264, 21977, 295, 264, 1412, 13, 51050], "temperature": 0.0, "avg_logprob": -0.31578035591062437, "compression_ratio": 1.8312236286919832, "no_speech_prob": 0.25969573855400085}, {"id": 761, "seek": 325202, "start": 3265.74, "end": 3269.42, "text": " Add them together to get the total variance, square roots, the standard deviation, total", "tokens": [51050, 5349, 552, 1214, 281, 483, 264, 3217, 21977, 11, 3732, 10669, 11, 264, 3832, 25163, 11, 3217, 51234], "temperature": 0.0, "avg_logprob": -0.31578035591062437, "compression_ratio": 1.8312236286919832, "no_speech_prob": 0.25969573855400085}, {"id": 762, "seek": 325202, "start": 3269.42, "end": 3270.42, "text": " standard deviation.", "tokens": [51234, 3832, 25163, 13, 51284], "temperature": 0.0, "avg_logprob": -0.31578035591062437, "compression_ratio": 1.8312236286919832, "no_speech_prob": 0.25969573855400085}, {"id": 763, "seek": 325202, "start": 3270.42, "end": 3275.3, "text": " So it's just the inverse of the total standard deviation, which is what we have here.", "tokens": [51284, 407, 309, 311, 445, 264, 17340, 295, 264, 3217, 3832, 25163, 11, 597, 307, 437, 321, 362, 510, 13, 51528], "temperature": 0.0, "avg_logprob": -0.31578035591062437, "compression_ratio": 1.8312236286919832, "no_speech_prob": 0.25969573855400085}, {"id": 764, "seek": 325202, "start": 3275.3, "end": 3277.3, "text": " Where does that come from?", "tokens": [51528, 2305, 775, 300, 808, 490, 30, 51628], "temperature": 0.0, "avg_logprob": -0.31578035591062437, "compression_ratio": 1.8312236286919832, "no_speech_prob": 0.25969573855400085}, {"id": 765, "seek": 327730, "start": 3277.3, "end": 3285.0600000000004, "text": " Well they just said, you know what, the inputs for a model should have unit variance.", "tokens": [50364, 1042, 436, 445, 848, 11, 291, 458, 437, 11, 264, 15743, 337, 257, 2316, 820, 362, 4985, 21977, 13, 50752], "temperature": 0.0, "avg_logprob": -0.34129243609548987, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.06186759099364281}, {"id": 766, "seek": 327730, "start": 3285.0600000000004, "end": 3287.5, "text": " Now we know that.", "tokens": [50752, 823, 321, 458, 300, 13, 50874], "temperature": 0.0, "avg_logprob": -0.34129243609548987, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.06186759099364281}, {"id": 767, "seek": 327730, "start": 3287.5, "end": 3291.1800000000003, "text": " We've done that to death in this course.", "tokens": [50874, 492, 600, 1096, 300, 281, 2966, 294, 341, 1164, 13, 51058], "temperature": 0.0, "avg_logprob": -0.34129243609548987, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.06186759099364281}, {"id": 768, "seek": 327730, "start": 3291.1800000000003, "end": 3302.1800000000003, "text": " So they said, all right, so, well the inputs to the model is the clean data plus the noise", "tokens": [51058, 407, 436, 848, 11, 439, 558, 11, 370, 11, 731, 264, 15743, 281, 264, 2316, 307, 264, 2541, 1412, 1804, 264, 5658, 51608], "temperature": 0.0, "avg_logprob": -0.34129243609548987, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.06186759099364281}, {"id": 769, "seek": 327730, "start": 3302.1800000000003, "end": 3304.98, "text": " times some number we're going to calculate.", "tokens": [51608, 1413, 512, 1230, 321, 434, 516, 281, 8873, 13, 51748], "temperature": 0.0, "avg_logprob": -0.34129243609548987, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.06186759099364281}, {"id": 770, "seek": 327730, "start": 3304.98, "end": 3307.0600000000004, "text": " And we want that to be one.", "tokens": [51748, 400, 321, 528, 300, 281, 312, 472, 13, 51852], "temperature": 0.0, "avg_logprob": -0.34129243609548987, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.06186759099364281}, {"id": 771, "seek": 330706, "start": 3307.82, "end": 3317.74, "text": " So the variance of the clean images plus the noise is equal to the variance of the clean", "tokens": [50402, 407, 264, 21977, 295, 264, 2541, 5267, 1804, 264, 5658, 307, 2681, 281, 264, 21977, 295, 264, 2541, 50898], "temperature": 0.0, "avg_logprob": -0.2927940253055457, "compression_ratio": 1.8432835820895523, "no_speech_prob": 0.004264517687261105}, {"id": 772, "seek": 330706, "start": 3317.74, "end": 3321.86, "text": " images plus the variance of the noise.", "tokens": [50898, 5267, 1804, 264, 21977, 295, 264, 5658, 13, 51104], "temperature": 0.0, "avg_logprob": -0.2927940253055457, "compression_ratio": 1.8432835820895523, "no_speech_prob": 0.004264517687261105}, {"id": 773, "seek": 330706, "start": 3321.86, "end": 3331.42, "text": " Okay, so if we want that to be, if we want variance to be one, then divide both sides", "tokens": [51104, 1033, 11, 370, 498, 321, 528, 300, 281, 312, 11, 498, 321, 528, 21977, 281, 312, 472, 11, 550, 9845, 1293, 4881, 51582], "temperature": 0.0, "avg_logprob": -0.2927940253055457, "compression_ratio": 1.8432835820895523, "no_speech_prob": 0.004264517687261105}, {"id": 774, "seek": 330706, "start": 3331.42, "end": 3335.1, "text": " by this and take the square root.", "tokens": [51582, 538, 341, 293, 747, 264, 3732, 5593, 13, 51766], "temperature": 0.0, "avg_logprob": -0.2927940253055457, "compression_ratio": 1.8432835820895523, "no_speech_prob": 0.004264517687261105}, {"id": 775, "seek": 333510, "start": 3335.14, "end": 3338.86, "text": " And that tells us our multiplier has to be one over this.", "tokens": [50366, 400, 300, 5112, 505, 527, 44106, 575, 281, 312, 472, 670, 341, 13, 50552], "temperature": 0.0, "avg_logprob": -0.2651769107150048, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.09530244022607803}, {"id": 776, "seek": 333510, "start": 3338.86, "end": 3339.86, "text": " That's it.", "tokens": [50552, 663, 311, 309, 13, 50602], "temperature": 0.0, "avg_logprob": -0.2651769107150048, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.09530244022607803}, {"id": 777, "seek": 333510, "start": 3339.86, "end": 3344.98, "text": " So it's like literally, you know, classical math.", "tokens": [50602, 407, 309, 311, 411, 3736, 11, 291, 458, 11, 13735, 5221, 13, 50858], "temperature": 0.0, "avg_logprob": -0.2651769107150048, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.09530244022607803}, {"id": 778, "seek": 333510, "start": 3344.98, "end": 3351.18, "text": " The only bit you have to know is that the variance of two things added together is the", "tokens": [50858, 440, 787, 857, 291, 362, 281, 458, 307, 300, 264, 21977, 295, 732, 721, 3869, 1214, 307, 264, 51168], "temperature": 0.0, "avg_logprob": -0.2651769107150048, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.09530244022607803}, {"id": 779, "seek": 333510, "start": 3351.18, "end": 3357.8199999999997, "text": " variance of the two things added together, which is not rocket science either.", "tokens": [51168, 21977, 295, 264, 732, 721, 3869, 1214, 11, 597, 307, 406, 13012, 3497, 2139, 13, 51500], "temperature": 0.0, "avg_logprob": -0.2651769107150048, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.09530244022607803}, {"id": 780, "seek": 333510, "start": 3357.8199999999997, "end": 3362.14, "text": " And in this context, like why we want to do this, when we looked at those sigmas that", "tokens": [51500, 400, 294, 341, 4319, 11, 411, 983, 321, 528, 281, 360, 341, 11, 562, 321, 2956, 412, 729, 4556, 3799, 300, 51716], "temperature": 0.0, "avg_logprob": -0.2651769107150048, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.09530244022607803}, {"id": 781, "seek": 336214, "start": 3362.14, "end": 3365.46, "text": " you're plotting, like the distribution, you've got some that are fairly low, but you've also", "tokens": [50364, 291, 434, 41178, 11, 411, 264, 7316, 11, 291, 600, 658, 512, 300, 366, 6457, 2295, 11, 457, 291, 600, 611, 50530], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 782, "seek": 336214, "start": 3365.46, "end": 3368.2599999999998, "text": " got some where the standard deviation sigma is like 40.", "tokens": [50530, 658, 512, 689, 264, 3832, 25163, 12771, 307, 411, 3356, 13, 50670], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 783, "seek": 336214, "start": 3368.2599999999998, "end": 3369.2599999999998, "text": " Right.", "tokens": [50670, 1779, 13, 50720], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 784, "seek": 336214, "start": 3369.2599999999998, "end": 3370.2599999999998, "text": " So the variance is super high.", "tokens": [50720, 407, 264, 21977, 307, 1687, 1090, 13, 50770], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 785, "seek": 336214, "start": 3370.2599999999998, "end": 3371.2599999999998, "text": " Yes.", "tokens": [50770, 1079, 13, 50820], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 786, "seek": 336214, "start": 3371.2599999999998, "end": 3374.8599999999997, "text": " And so we don't want to feed something with standard deviation 40 into our model.", "tokens": [50820, 400, 370, 321, 500, 380, 528, 281, 3154, 746, 365, 3832, 25163, 3356, 666, 527, 2316, 13, 51000], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 787, "seek": 336214, "start": 3374.8599999999997, "end": 3377.02, "text": " You would like it to be closer to unit variance.", "tokens": [51000, 509, 576, 411, 309, 281, 312, 4966, 281, 4985, 21977, 13, 51108], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 788, "seek": 336214, "start": 3377.02, "end": 3381.22, "text": " So we're thinking, okay, well if you divide by roughly 40, that would scale it down.", "tokens": [51108, 407, 321, 434, 1953, 11, 1392, 11, 731, 498, 291, 9845, 538, 9810, 3356, 11, 300, 576, 4373, 309, 760, 13, 51318], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 789, "seek": 336214, "start": 3381.22, "end": 3383.5, "text": " But then we've also got some extra variance from our data.", "tokens": [51318, 583, 550, 321, 600, 611, 658, 512, 2857, 21977, 490, 527, 1412, 13, 51432], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 790, "seek": 336214, "start": 3383.5, "end": 3389.02, "text": " So it's like 40 plus variance of the data of a little bit.", "tokens": [51432, 407, 309, 311, 411, 3356, 1804, 21977, 295, 264, 1412, 295, 257, 707, 857, 13, 51708], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 791, "seek": 336214, "start": 3389.02, "end": 3392.02, "text": " We want to scale back down by that to get unit variance.", "tokens": [51708, 492, 528, 281, 4373, 646, 760, 538, 300, 281, 483, 4985, 21977, 13, 51858], "temperature": 0.0, "avg_logprob": -0.2926244067538316, "compression_ratio": 1.83596214511041, "no_speech_prob": 0.675449550151825}, {"id": 792, "seek": 339202, "start": 3392.9, "end": 3398.9, "text": " I love this paper because it's basically just doing what we spent weeks doing.", "tokens": [50408, 286, 959, 341, 3035, 570, 309, 311, 1936, 445, 884, 437, 321, 4418, 3259, 884, 13, 50708], "temperature": 0.0, "avg_logprob": -0.25839496381355054, "compression_ratio": 1.5112359550561798, "no_speech_prob": 0.0007672745850868523}, {"id": 793, "seek": 339202, "start": 3398.9, "end": 3405.34, "text": " I feel like everything that we've done that's improved every model has always been one thing,", "tokens": [50708, 286, 841, 411, 1203, 300, 321, 600, 1096, 300, 311, 9689, 633, 2316, 575, 1009, 668, 472, 551, 11, 51030], "temperature": 0.0, "avg_logprob": -0.25839496381355054, "compression_ratio": 1.5112359550561798, "no_speech_prob": 0.0007672745850868523}, {"id": 794, "seek": 339202, "start": 3405.34, "end": 3418.18, "text": " which is, can we get mean zero, variance one inputs to our model and for all of our activations?", "tokens": [51030, 597, 307, 11, 393, 321, 483, 914, 4018, 11, 21977, 472, 15743, 281, 527, 2316, 293, 337, 439, 295, 527, 2430, 763, 30, 51672], "temperature": 0.0, "avg_logprob": -0.25839496381355054, "compression_ratio": 1.5112359550561798, "no_speech_prob": 0.0007672745850868523}, {"id": 795, "seek": 341818, "start": 3418.18, "end": 3423.1, "text": " And then the only other thing is include enough compute by adding enough layers and", "tokens": [50364, 400, 550, 264, 787, 661, 551, 307, 4090, 1547, 14722, 538, 5127, 1547, 7914, 293, 50610], "temperature": 0.0, "avg_logprob": -0.31501777752025706, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.059192538261413574}, {"id": 796, "seek": 341818, "start": 3423.1, "end": 3424.8199999999997, "text": " enough activations.", "tokens": [50610, 1547, 2430, 763, 13, 50696], "temperature": 0.0, "avg_logprob": -0.31501777752025706, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.059192538261413574}, {"id": 797, "seek": 341818, "start": 3424.8199999999997, "end": 3428.58, "text": " Those two things seem to be all that matters.", "tokens": [50696, 3950, 732, 721, 1643, 281, 312, 439, 300, 7001, 13, 50884], "temperature": 0.0, "avg_logprob": -0.31501777752025706, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.059192538261413574}, {"id": 798, "seek": 341818, "start": 3428.58, "end": 3432.2599999999998, "text": " Basically well, I guess ResNet's added an extra cool little thing to that, which is", "tokens": [50884, 8537, 731, 11, 286, 2041, 5015, 31890, 311, 3869, 364, 2857, 1627, 707, 551, 281, 300, 11, 597, 307, 51068], "temperature": 0.0, "avg_logprob": -0.31501777752025706, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.059192538261413574}, {"id": 799, "seek": 341818, "start": 3432.2599999999998, "end": 3441.3399999999997, "text": " to make it even smoother by giving this kind of like identity path.", "tokens": [51068, 281, 652, 309, 754, 28640, 538, 2902, 341, 733, 295, 411, 6575, 3100, 13, 51522], "temperature": 0.0, "avg_logprob": -0.31501777752025706, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.059192538261413574}, {"id": 800, "seek": 344134, "start": 3441.42, "end": 3449.34, "text": " So yeah, basically trying to make things as smooth as possible and as equal everywhere", "tokens": [50368, 407, 1338, 11, 1936, 1382, 281, 652, 721, 382, 5508, 382, 1944, 293, 382, 2681, 5315, 50764], "temperature": 0.0, "avg_logprob": -0.27573264824165095, "compression_ratio": 1.7875647668393781, "no_speech_prob": 0.16874635219573975}, {"id": 801, "seek": 344134, "start": 3449.34, "end": 3450.34, "text": " as possible.", "tokens": [50764, 382, 1944, 13, 50814], "temperature": 0.0, "avg_logprob": -0.27573264824165095, "compression_ratio": 1.7875647668393781, "no_speech_prob": 0.16874635219573975}, {"id": 802, "seek": 344134, "start": 3450.34, "end": 3452.26, "text": " So yeah, this is what they've done.", "tokens": [50814, 407, 1338, 11, 341, 307, 437, 436, 600, 1096, 13, 50910], "temperature": 0.0, "avg_logprob": -0.27573264824165095, "compression_ratio": 1.7875647668393781, "no_speech_prob": 0.16874635219573975}, {"id": 803, "seek": 344134, "start": 3452.26, "end": 3458.46, "text": " So they did that for the inputs and then they've also done it for the outputs.", "tokens": [50910, 407, 436, 630, 300, 337, 264, 15743, 293, 550, 436, 600, 611, 1096, 309, 337, 264, 23930, 13, 51220], "temperature": 0.0, "avg_logprob": -0.27573264824165095, "compression_ratio": 1.7875647668393781, "no_speech_prob": 0.16874635219573975}, {"id": 804, "seek": 344134, "start": 3458.46, "end": 3462.94, "text": " And for the outputs, you know, it's basically the same idea.", "tokens": [51220, 400, 337, 264, 23930, 11, 291, 458, 11, 309, 311, 1936, 264, 912, 1558, 13, 51444], "temperature": 0.0, "avg_logprob": -0.27573264824165095, "compression_ratio": 1.7875647668393781, "no_speech_prob": 0.16874635219573975}, {"id": 805, "seek": 344134, "start": 3462.94, "end": 3468.02, "text": " You know, they have basically the same kind of analysis to show that.", "tokens": [51444, 509, 458, 11, 436, 362, 1936, 264, 912, 733, 295, 5215, 281, 855, 300, 13, 51698], "temperature": 0.0, "avg_logprob": -0.27573264824165095, "compression_ratio": 1.7875647668393781, "no_speech_prob": 0.16874635219573975}, {"id": 806, "seek": 346802, "start": 3468.02, "end": 3475.38, "text": " And so with this, so now, yeah, we've basically, we've got our noise to input.", "tokens": [50364, 400, 370, 365, 341, 11, 370, 586, 11, 1338, 11, 321, 600, 1936, 11, 321, 600, 658, 527, 5658, 281, 4846, 13, 50732], "temperature": 0.0, "avg_logprob": -0.2887725830078125, "compression_ratio": 2.1, "no_speech_prob": 0.016402145847678185}, {"id": 807, "seek": 346802, "start": 3475.38, "end": 3480.54, "text": " We've got the, you know, kind of linear version somewhere between X naught and the noise to", "tokens": [50732, 492, 600, 658, 264, 11, 291, 458, 11, 733, 295, 8213, 3037, 4079, 1296, 1783, 13138, 293, 264, 5658, 281, 50990], "temperature": 0.0, "avg_logprob": -0.2887725830078125, "compression_ratio": 2.1, "no_speech_prob": 0.016402145847678185}, {"id": 808, "seek": 346802, "start": 3480.54, "end": 3481.58, "text": " input.", "tokens": [50990, 4846, 13, 51042], "temperature": 0.0, "avg_logprob": -0.2887725830078125, "compression_ratio": 2.1, "no_speech_prob": 0.016402145847678185}, {"id": 809, "seek": 346802, "start": 3481.58, "end": 3485.5, "text": " We've got the scaling of the output and we've got the scaling of the input.", "tokens": [51042, 492, 600, 658, 264, 21589, 295, 264, 5598, 293, 321, 600, 658, 264, 21589, 295, 264, 4846, 13, 51238], "temperature": 0.0, "avg_logprob": -0.2887725830078125, "compression_ratio": 2.1, "no_speech_prob": 0.016402145847678185}, {"id": 810, "seek": 346802, "start": 3485.5, "end": 3491.62, "text": " So now for the inputs to our model, we're going to have the scaled noise.", "tokens": [51238, 407, 586, 337, 264, 15743, 281, 527, 2316, 11, 321, 434, 516, 281, 362, 264, 36039, 5658, 13, 51544], "temperature": 0.0, "avg_logprob": -0.2887725830078125, "compression_ratio": 2.1, "no_speech_prob": 0.016402145847678185}, {"id": 811, "seek": 346802, "start": 3491.62, "end": 3497.66, "text": " We're going to have the sigma and we're going to have the target, which is somewhere between", "tokens": [51544, 492, 434, 516, 281, 362, 264, 12771, 293, 321, 434, 516, 281, 362, 264, 3779, 11, 597, 307, 4079, 1296, 51846], "temperature": 0.0, "avg_logprob": -0.2887725830078125, "compression_ratio": 2.1, "no_speech_prob": 0.016402145847678185}, {"id": 812, "seek": 349766, "start": 3497.66, "end": 3500.18, "text": " the image and the noise.", "tokens": [50364, 264, 3256, 293, 264, 5658, 13, 50490], "temperature": 0.0, "avg_logprob": -0.21814021042415074, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0013458261964842677}, {"id": 813, "seek": 349766, "start": 3500.18, "end": 3506.94, "text": " And so, yeah, so I've, you know, never seen anybody draw a picture of this before.", "tokens": [50490, 400, 370, 11, 1338, 11, 370, 286, 600, 11, 291, 458, 11, 1128, 1612, 4472, 2642, 257, 3036, 295, 341, 949, 13, 50828], "temperature": 0.0, "avg_logprob": -0.21814021042415074, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0013458261964842677}, {"id": 814, "seek": 349766, "start": 3506.94, "end": 3510.8599999999997, "text": " So it was really cool when, you know, being in a notebook, being able to see like, oh,", "tokens": [50828, 407, 309, 390, 534, 1627, 562, 11, 291, 458, 11, 885, 294, 257, 21060, 11, 885, 1075, 281, 536, 411, 11, 1954, 11, 51024], "temperature": 0.0, "avg_logprob": -0.21814021042415074, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0013458261964842677}, {"id": 815, "seek": 349766, "start": 3510.8599999999997, "end": 3513.62, "text": " that's what they're doing, you know.", "tokens": [51024, 300, 311, 437, 436, 434, 884, 11, 291, 458, 13, 51162], "temperature": 0.0, "avg_logprob": -0.21814021042415074, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0013458261964842677}, {"id": 816, "seek": 349766, "start": 3513.62, "end": 3517.2999999999997, "text": " So yeah, have a good look at this notebook to see exactly what's going on, because I", "tokens": [51162, 407, 1338, 11, 362, 257, 665, 574, 412, 341, 21060, 281, 536, 2293, 437, 311, 516, 322, 11, 570, 286, 51346], "temperature": 0.0, "avg_logprob": -0.21814021042415074, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0013458261964842677}, {"id": 817, "seek": 349766, "start": 3517.2999999999997, "end": 3523.18, "text": " think it gives you a really good intuition around what problem it's trying to solve.", "tokens": [51346, 519, 309, 2709, 291, 257, 534, 665, 24002, 926, 437, 1154, 309, 311, 1382, 281, 5039, 13, 51640], "temperature": 0.0, "avg_logprob": -0.21814021042415074, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0013458261964842677}, {"id": 818, "seek": 352318, "start": 3523.18, "end": 3528.4199999999996, "text": " So then I actually checked the noise to input has a standard deviation of one.", "tokens": [50364, 407, 550, 286, 767, 10033, 264, 5658, 281, 4846, 575, 257, 3832, 25163, 295, 472, 13, 50626], "temperature": 0.0, "avg_logprob": -0.2753711407001202, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.1732185035943985}, {"id": 819, "seek": 352318, "start": 3528.4199999999996, "end": 3530.94, "text": " The mean's not zero, and of course, why would it be?", "tokens": [50626, 440, 914, 311, 406, 4018, 11, 293, 295, 1164, 11, 983, 576, 309, 312, 30, 50752], "temperature": 0.0, "avg_logprob": -0.2753711407001202, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.1732185035943985}, {"id": 820, "seek": 352318, "start": 3530.94, "end": 3534.66, "text": " We didn't do anything, you know, the only thing Keras cared about was having the variance", "tokens": [50752, 492, 994, 380, 360, 1340, 11, 291, 458, 11, 264, 787, 551, 591, 6985, 19779, 466, 390, 1419, 264, 21977, 50938], "temperature": 0.0, "avg_logprob": -0.2753711407001202, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.1732185035943985}, {"id": 821, "seek": 352318, "start": 3534.66, "end": 3535.66, "text": " one.", "tokens": [50938, 472, 13, 50988], "temperature": 0.0, "avg_logprob": -0.2753711407001202, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.1732185035943985}, {"id": 822, "seek": 352318, "start": 3535.66, "end": 3540.94, "text": " We could easily adjust the input and output to have a mean of zero as well, and that's", "tokens": [50988, 492, 727, 3612, 4369, 264, 4846, 293, 5598, 281, 362, 257, 914, 295, 4018, 382, 731, 11, 293, 300, 311, 51252], "temperature": 0.0, "avg_logprob": -0.2753711407001202, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.1732185035943985}, {"id": 823, "seek": 352318, "start": 3540.94, "end": 3545.66, "text": " something I think we or somebody should try, because I think it does seem to help a bit", "tokens": [51252, 746, 286, 519, 321, 420, 2618, 820, 853, 11, 570, 286, 519, 309, 775, 1643, 281, 854, 257, 857, 51488], "temperature": 0.0, "avg_logprob": -0.2753711407001202, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.1732185035943985}, {"id": 824, "seek": 352318, "start": 3545.66, "end": 3548.3399999999997, "text": " as we saw with that generalized value stuff we did.", "tokens": [51488, 382, 321, 1866, 365, 300, 44498, 2158, 1507, 321, 630, 13, 51622], "temperature": 0.0, "avg_logprob": -0.2753711407001202, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.1732185035943985}, {"id": 825, "seek": 352318, "start": 3548.3399999999997, "end": 3551.14, "text": " But it's less important than the variance.", "tokens": [51622, 583, 309, 311, 1570, 1021, 813, 264, 21977, 13, 51762], "temperature": 0.0, "avg_logprob": -0.2753711407001202, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.1732185035943985}, {"id": 826, "seek": 355114, "start": 3551.14, "end": 3554.06, "text": " And so same with the target, it's got the one.", "tokens": [50364, 400, 370, 912, 365, 264, 3779, 11, 309, 311, 658, 264, 472, 13, 50510], "temperature": 0.0, "avg_logprob": -0.27323325020926337, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.08879713714122772}, {"id": 827, "seek": 355114, "start": 3554.06, "end": 3560.2999999999997, "text": " And yeah, this is where if I change this to the correct value, which is 0.66, then actually", "tokens": [50510, 400, 1338, 11, 341, 307, 689, 498, 286, 1319, 341, 281, 264, 3006, 2158, 11, 597, 307, 1958, 13, 15237, 11, 550, 767, 50822], "temperature": 0.0, "avg_logprob": -0.27323325020926337, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.08879713714122772}, {"id": 828, "seek": 355114, "start": 3560.2999999999997, "end": 3565.7, "text": " it's slightly further away from one, both here and here, quite a lot further away.", "tokens": [50822, 309, 311, 4748, 3052, 1314, 490, 472, 11, 1293, 510, 293, 510, 11, 1596, 257, 688, 3052, 1314, 13, 51092], "temperature": 0.0, "avg_logprob": -0.27323325020926337, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.08879713714122772}, {"id": 829, "seek": 355114, "start": 3565.7, "end": 3571.74, "text": " And maybe that's because actually the data's, well, we know the data's not Gaussian distributed.", "tokens": [51092, 400, 1310, 300, 311, 570, 767, 264, 1412, 311, 11, 731, 11, 321, 458, 264, 1412, 311, 406, 39148, 12631, 13, 51394], "temperature": 0.0, "avg_logprob": -0.27323325020926337, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.08879713714122772}, {"id": 830, "seek": 355114, "start": 3571.74, "end": 3573.62, "text": " Pixel data definitely isn't Gaussian distributed.", "tokens": [51394, 28323, 1412, 2138, 1943, 380, 39148, 12631, 13, 51488], "temperature": 0.0, "avg_logprob": -0.27323325020926337, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.08879713714122772}, {"id": 831, "seek": 355114, "start": 3573.62, "end": 3577.94, "text": " So this bug turned out better.", "tokens": [51488, 407, 341, 7426, 3574, 484, 1101, 13, 51704], "temperature": 0.0, "avg_logprob": -0.27323325020926337, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.08879713714122772}, {"id": 832, "seek": 357794, "start": 3578.26, "end": 3585.5, "text": " Okay, so the unit's the same, the initialization's the same, this is all the same, train it for", "tokens": [50380, 1033, 11, 370, 264, 4985, 311, 264, 912, 11, 264, 5883, 2144, 311, 264, 912, 11, 341, 307, 439, 264, 912, 11, 3847, 309, 337, 50742], "temperature": 0.0, "avg_logprob": -0.3157213712993421, "compression_ratio": 1.59, "no_speech_prob": 0.0006263265386223793}, {"id": 833, "seek": 357794, "start": 3585.5, "end": 3586.66, "text": " a while.", "tokens": [50742, 257, 1339, 13, 50800], "temperature": 0.0, "avg_logprob": -0.3157213712993421, "compression_ratio": 1.59, "no_speech_prob": 0.0006263265386223793}, {"id": 834, "seek": 357794, "start": 3586.66, "end": 3593.18, "text": " We can't compare the losses, right, because our target's different.", "tokens": [50800, 492, 393, 380, 6794, 264, 15352, 11, 558, 11, 570, 527, 3779, 311, 819, 13, 51126], "temperature": 0.0, "avg_logprob": -0.3157213712993421, "compression_ratio": 1.59, "no_speech_prob": 0.0006263265386223793}, {"id": 835, "seek": 357794, "start": 3593.18, "end": 3600.58, "text": " But what we can do is we can create a denoise that just takes the thing that, as per usual,", "tokens": [51126, 583, 437, 321, 393, 360, 307, 321, 393, 1884, 257, 1441, 38800, 300, 445, 2516, 264, 551, 300, 11, 382, 680, 7713, 11, 51496], "temperature": 0.0, "avg_logprob": -0.3157213712993421, "compression_ratio": 1.59, "no_speech_prob": 0.0006263265386223793}, {"id": 836, "seek": 357794, "start": 3600.58, "end": 3605.18, "text": " the thing we had in Noisify, right, and solve for X0.", "tokens": [51496, 264, 551, 321, 632, 294, 883, 271, 2505, 11, 558, 11, 293, 5039, 337, 1783, 15, 13, 51726], "temperature": 0.0, "avg_logprob": -0.3157213712993421, "compression_ratio": 1.59, "no_speech_prob": 0.0006263265386223793}, {"id": 837, "seek": 360518, "start": 3605.58, "end": 3610.14, "text": " So we've got to multiply by C out and then add C skip by noise to input.", "tokens": [50384, 407, 321, 600, 658, 281, 12972, 538, 383, 484, 293, 550, 909, 383, 10023, 538, 5658, 281, 4846, 13, 50612], "temperature": 0.0, "avg_logprob": -0.32616168128119577, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.1732657253742218}, {"id": 838, "seek": 360518, "start": 3610.14, "end": 3612.7, "text": " Here it is, multiply by C out, add noise to input.", "tokens": [50612, 1692, 309, 307, 11, 12972, 538, 383, 484, 11, 909, 5658, 281, 4846, 13, 50740], "temperature": 0.0, "avg_logprob": -0.32616168128119577, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.1732657253742218}, {"id": 839, "seek": 360518, "start": 3612.7, "end": 3615.2999999999997, "text": " Okay, so we can denoise.", "tokens": [50740, 1033, 11, 370, 321, 393, 1441, 38800, 13, 50870], "temperature": 0.0, "avg_logprob": -0.32616168128119577, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.1732657253742218}, {"id": 840, "seek": 360518, "start": 3615.2999999999997, "end": 3624.22, "text": " So let's grab our sigmas from the actual batch we had.", "tokens": [50870, 407, 718, 311, 4444, 527, 4556, 3799, 490, 264, 3539, 15245, 321, 632, 13, 51316], "temperature": 0.0, "avg_logprob": -0.32616168128119577, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.1732657253742218}, {"id": 841, "seek": 360518, "start": 3624.22, "end": 3629.7799999999997, "text": " Let's calculate C skip, C out, and C in for the sigmas in our mini-batch.", "tokens": [51316, 961, 311, 8873, 383, 10023, 11, 383, 484, 11, 293, 383, 294, 337, 264, 4556, 3799, 294, 527, 8382, 12, 65, 852, 13, 51594], "temperature": 0.0, "avg_logprob": -0.32616168128119577, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.1732657253742218}, {"id": 842, "seek": 362978, "start": 3629.78, "end": 3638.7400000000002, "text": " Let's use the model to predict the target, given the noise to input and the sigmas, and", "tokens": [50364, 961, 311, 764, 264, 2316, 281, 6069, 264, 3779, 11, 2212, 264, 5658, 281, 4846, 293, 264, 4556, 3799, 11, 293, 50812], "temperature": 0.0, "avg_logprob": -0.2736894408268715, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.08034653961658478}, {"id": 843, "seek": 362978, "start": 3638.7400000000002, "end": 3641.78, "text": " then denoise it.", "tokens": [50812, 550, 1441, 38800, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2736894408268715, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.08034653961658478}, {"id": 844, "seek": 362978, "start": 3641.78, "end": 3649.26, "text": " And so here's our noise to input, which we've already seen, and here's our predictions.", "tokens": [50964, 400, 370, 510, 311, 527, 5658, 281, 4846, 11, 597, 321, 600, 1217, 1612, 11, 293, 510, 311, 527, 21264, 13, 51338], "temperature": 0.0, "avg_logprob": -0.2736894408268715, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.08034653961658478}, {"id": 845, "seek": 362978, "start": 3649.26, "end": 3654.5, "text": " And these are absolutely remarkable, in my opinion.", "tokens": [51338, 400, 613, 366, 3122, 12802, 11, 294, 452, 4800, 13, 51600], "temperature": 0.0, "avg_logprob": -0.2736894408268715, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.08034653961658478}, {"id": 846, "seek": 365450, "start": 3655.5, "end": 3659.98, "text": " Yeah, like this one here, I can barely see it, you know, it's really found...", "tokens": [50414, 865, 11, 411, 341, 472, 510, 11, 286, 393, 10268, 536, 309, 11, 291, 458, 11, 309, 311, 534, 1352, 485, 50638], "temperature": 0.0, "avg_logprob": -0.3693258112127131, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.027167273685336113}, {"id": 847, "seek": 365450, "start": 3659.98, "end": 3660.98, "text": " Look at the shirt.", "tokens": [50638, 2053, 412, 264, 8336, 13, 50688], "temperature": 0.0, "avg_logprob": -0.3693258112127131, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.027167273685336113}, {"id": 848, "seek": 365450, "start": 3660.98, "end": 3664.98, "text": " There's a shirt here, it's actually really finding the little thing on the front, and", "tokens": [50688, 821, 311, 257, 8336, 510, 11, 309, 311, 767, 534, 5006, 264, 707, 551, 322, 264, 1868, 11, 293, 50888], "temperature": 0.0, "avg_logprob": -0.3693258112127131, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.027167273685336113}, {"id": 849, "seek": 365450, "start": 3664.98, "end": 3670.34, "text": " let me show you, here's what it should look like.", "tokens": [50888, 718, 385, 855, 291, 11, 510, 311, 437, 309, 820, 574, 411, 13, 51156], "temperature": 0.0, "avg_logprob": -0.3693258112127131, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.027167273685336113}, {"id": 850, "seek": 365450, "start": 3670.34, "end": 3676.82, "text": " And in cases where the sigma's pretty high, like here, you can see it's really, like,", "tokens": [51156, 400, 294, 3331, 689, 264, 12771, 311, 1238, 1090, 11, 411, 510, 11, 291, 393, 536, 309, 311, 534, 11, 411, 11, 51480], "temperature": 0.0, "avg_logprob": -0.3693258112127131, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.027167273685336113}, {"id": 851, "seek": 365450, "start": 3676.82, "end": 3680.66, "text": " saying, like, I don't know, maybe it's shoes, but it could be something else.", "tokens": [51480, 1566, 11, 411, 11, 286, 500, 380, 458, 11, 1310, 309, 311, 6654, 11, 457, 309, 727, 312, 746, 1646, 13, 51672], "temperature": 0.0, "avg_logprob": -0.3693258112127131, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.027167273685336113}, {"id": 852, "seek": 365450, "start": 3680.66, "end": 3681.66, "text": " Is it shoes?", "tokens": [51672, 1119, 309, 6654, 30, 51722], "temperature": 0.0, "avg_logprob": -0.3693258112127131, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.027167273685336113}, {"id": 853, "seek": 368166, "start": 3681.8199999999997, "end": 3687.3799999999997, "text": " Yeah, it wasn't shoes, but at least it's kind of got the, you know, the bulk of the pixels", "tokens": [50372, 865, 11, 309, 2067, 380, 6654, 11, 457, 412, 1935, 309, 311, 733, 295, 658, 264, 11, 291, 458, 11, 264, 16139, 295, 264, 18668, 50650], "temperature": 0.0, "avg_logprob": -0.28997196197509767, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.0057297986932098866}, {"id": 854, "seek": 368166, "start": 3687.3799999999997, "end": 3689.02, "text": " in the right spot.", "tokens": [50650, 294, 264, 558, 4008, 13, 50732], "temperature": 0.0, "avg_logprob": -0.28997196197509767, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.0057297986932098866}, {"id": 855, "seek": 368166, "start": 3689.02, "end": 3694.46, "text": " Yeah, something like this one is 4.5, it has no idea what it is, it's like, oh, maybe it's", "tokens": [50732, 865, 11, 746, 411, 341, 472, 307, 1017, 13, 20, 11, 309, 575, 572, 1558, 437, 309, 307, 11, 309, 311, 411, 11, 1954, 11, 1310, 309, 311, 51004], "temperature": 0.0, "avg_logprob": -0.28997196197509767, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.0057297986932098866}, {"id": 856, "seek": 368166, "start": 3694.46, "end": 3699.58, "text": " shoes, maybe it's pants, you know, turns out it is shoes.", "tokens": [51004, 6654, 11, 1310, 309, 311, 10082, 11, 291, 458, 11, 4523, 484, 309, 307, 6654, 13, 51260], "temperature": 0.0, "avg_logprob": -0.28997196197509767, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.0057297986932098866}, {"id": 857, "seek": 368166, "start": 3699.58, "end": 3707.3799999999997, "text": " Yeah, so I think that's fascinating how well it can do.", "tokens": [51260, 865, 11, 370, 286, 519, 300, 311, 10343, 577, 731, 309, 393, 360, 13, 51650], "temperature": 0.0, "avg_logprob": -0.28997196197509767, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.0057297986932098866}, {"id": 858, "seek": 370738, "start": 3707.38, "end": 3713.94, "text": " And then the other thing I did, which I thought was fun, was I just created, so I just did", "tokens": [50364, 400, 550, 264, 661, 551, 286, 630, 11, 597, 286, 1194, 390, 1019, 11, 390, 286, 445, 2942, 11, 370, 286, 445, 630, 50692], "temperature": 0.0, "avg_logprob": -0.2442931578709529, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.04670747369527817}, {"id": 859, "seek": 370738, "start": 3713.94, "end": 3718.7400000000002, "text": " a sigma of 80, which is actually what they do when they're doing sampling from pure noise,", "tokens": [50692, 257, 12771, 295, 4688, 11, 597, 307, 767, 437, 436, 360, 562, 436, 434, 884, 21179, 490, 6075, 5658, 11, 50932], "temperature": 0.0, "avg_logprob": -0.2442931578709529, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.04670747369527817}, {"id": 860, "seek": 370738, "start": 3718.7400000000002, "end": 3722.34, "text": " that's what they consider the pure noise level.", "tokens": [50932, 300, 311, 437, 436, 1949, 264, 6075, 5658, 1496, 13, 51112], "temperature": 0.0, "avg_logprob": -0.2442931578709529, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.04670747369527817}, {"id": 861, "seek": 370738, "start": 3722.34, "end": 3729.96, "text": " So I just created some pure noise and denoised it just for one step.", "tokens": [51112, 407, 286, 445, 2942, 512, 6075, 5658, 293, 1441, 78, 2640, 309, 445, 337, 472, 1823, 13, 51493], "temperature": 0.0, "avg_logprob": -0.2442931578709529, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.04670747369527817}, {"id": 862, "seek": 370738, "start": 3729.96, "end": 3734.94, "text": " And so here's what happens when you denoise it for one step, and you can see it's kind", "tokens": [51493, 400, 370, 510, 311, 437, 2314, 562, 291, 1441, 38800, 309, 337, 472, 1823, 11, 293, 291, 393, 536, 309, 311, 733, 51742], "temperature": 0.0, "avg_logprob": -0.2442931578709529, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.04670747369527817}, {"id": 863, "seek": 373494, "start": 3734.94, "end": 3738.2200000000003, "text": " of overlaid all the possibilities, it's like I can see a pair of shoes here, a pair of", "tokens": [50364, 295, 670, 875, 327, 439, 264, 12178, 11, 309, 311, 411, 286, 393, 536, 257, 6119, 295, 6654, 510, 11, 257, 6119, 295, 50528], "temperature": 0.0, "avg_logprob": -0.27071910823157075, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.03460962325334549}, {"id": 864, "seek": 373494, "start": 3738.2200000000003, "end": 3741.42, "text": " pants here, a top here.", "tokens": [50528, 10082, 510, 11, 257, 1192, 510, 13, 50688], "temperature": 0.0, "avg_logprob": -0.27071910823157075, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.03460962325334549}, {"id": 865, "seek": 373494, "start": 3741.42, "end": 3747.06, "text": " And sometimes it's kind of like more confident that the noise is actually a pair of pants,", "tokens": [50688, 400, 2171, 309, 311, 733, 295, 411, 544, 6679, 300, 264, 5658, 307, 767, 257, 6119, 295, 10082, 11, 50970], "temperature": 0.0, "avg_logprob": -0.27071910823157075, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.03460962325334549}, {"id": 866, "seek": 373494, "start": 3747.06, "end": 3749.5, "text": " and sometimes it's more confident that it's actually shoes.", "tokens": [50970, 293, 2171, 309, 311, 544, 6679, 300, 309, 311, 767, 6654, 13, 51092], "temperature": 0.0, "avg_logprob": -0.27071910823157075, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.03460962325334549}, {"id": 867, "seek": 373494, "start": 3749.5, "end": 3755.42, "text": " But you can really get a sense of how, like, from pure noise, it starts to make a call", "tokens": [51092, 583, 291, 393, 534, 483, 257, 2020, 295, 577, 11, 411, 11, 490, 6075, 5658, 11, 309, 3719, 281, 652, 257, 818, 51388], "temperature": 0.0, "avg_logprob": -0.27071910823157075, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.03460962325334549}, {"id": 868, "seek": 373494, "start": 3755.42, "end": 3761.66, "text": " about, like, what this noise is actually covering up.", "tokens": [51388, 466, 11, 411, 11, 437, 341, 5658, 307, 767, 10322, 493, 13, 51700], "temperature": 0.0, "avg_logprob": -0.27071910823157075, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.03460962325334549}, {"id": 869, "seek": 376166, "start": 3761.74, "end": 3768.2999999999997, "text": " And this is also the bit which I feel is, like, I'm the least convinced about when it", "tokens": [50368, 400, 341, 307, 611, 264, 857, 597, 286, 841, 307, 11, 411, 11, 286, 478, 264, 1935, 12561, 466, 562, 309, 50696], "temperature": 0.0, "avg_logprob": -0.22926166838249273, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.05417739227414131}, {"id": 870, "seek": 376166, "start": 3768.2999999999997, "end": 3770.2599999999998, "text": " comes to diffusion models.", "tokens": [50696, 1487, 281, 25242, 5245, 13, 50794], "temperature": 0.0, "avg_logprob": -0.22926166838249273, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.05417739227414131}, {"id": 871, "seek": 376166, "start": 3770.2599999999998, "end": 3777.66, "text": " This first step of going from, like, pure noise to something, and, like, trying to have", "tokens": [50794, 639, 700, 1823, 295, 516, 490, 11, 411, 11, 6075, 5658, 281, 746, 11, 293, 11, 411, 11, 1382, 281, 362, 51164], "temperature": 0.0, "avg_logprob": -0.22926166838249273, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.05417739227414131}, {"id": 872, "seek": 376166, "start": 3777.66, "end": 3783.1, "text": " a good mix of all the possible somethings, I'm, I don't know, it feels a bit hand-wavy", "tokens": [51164, 257, 665, 2890, 295, 439, 264, 1944, 746, 82, 11, 286, 478, 11, 286, 500, 380, 458, 11, 309, 3417, 257, 857, 1011, 12, 86, 15498, 51436], "temperature": 0.0, "avg_logprob": -0.22926166838249273, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.05417739227414131}, {"id": 873, "seek": 376166, "start": 3783.1, "end": 3784.1, "text": " to me.", "tokens": [51436, 281, 385, 13, 51486], "temperature": 0.0, "avg_logprob": -0.22926166838249273, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.05417739227414131}, {"id": 874, "seek": 376166, "start": 3784.1, "end": 3788.8599999999997, "text": " It clearly works quite well, but I'm not sure if it's, like, we're getting the full range", "tokens": [51486, 467, 4448, 1985, 1596, 731, 11, 457, 286, 478, 406, 988, 498, 309, 311, 11, 411, 11, 321, 434, 1242, 264, 1577, 3613, 51724], "temperature": 0.0, "avg_logprob": -0.22926166838249273, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.05417739227414131}, {"id": 875, "seek": 378886, "start": 3788.94, "end": 3793.46, "text": " of possibilities, and I feel like some of the papers we're starting to see are starting", "tokens": [50368, 295, 12178, 11, 293, 286, 841, 411, 512, 295, 264, 10577, 321, 434, 2891, 281, 536, 366, 2891, 50594], "temperature": 0.0, "avg_logprob": -0.3180468498019997, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.62559574842453}, {"id": 876, "seek": 378886, "start": 3793.46, "end": 3797.3, "text": " to say, like, you know what, maybe this is not quite the right approach.", "tokens": [50594, 281, 584, 11, 411, 11, 291, 458, 437, 11, 1310, 341, 307, 406, 1596, 264, 558, 3109, 13, 50786], "temperature": 0.0, "avg_logprob": -0.3180468498019997, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.62559574842453}, {"id": 877, "seek": 378886, "start": 3797.3, "end": 3801.94, "text": " And maybe later in the course we'll look at some of the ones that look at what we call", "tokens": [50786, 400, 1310, 1780, 294, 264, 1164, 321, 603, 574, 412, 512, 295, 264, 2306, 300, 574, 412, 437, 321, 818, 51018], "temperature": 0.0, "avg_logprob": -0.3180468498019997, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.62559574842453}, {"id": 878, "seek": 378886, "start": 3801.94, "end": 3805.98, "text": " VQ models and tokenized stuff.", "tokens": [51018, 691, 48, 5245, 293, 14862, 1602, 1507, 13, 51220], "temperature": 0.0, "avg_logprob": -0.3180468498019997, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.62559574842453}, {"id": 879, "seek": 378886, "start": 3805.98, "end": 3811.54, "text": " Anyway, I thought this was pretty interesting to see these pictures, which I don't think,", "tokens": [51220, 5684, 11, 286, 1194, 341, 390, 1238, 1880, 281, 536, 613, 5242, 11, 597, 286, 500, 380, 519, 11, 51498], "temperature": 0.0, "avg_logprob": -0.3180468498019997, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.62559574842453}, {"id": 880, "seek": 378886, "start": 3811.54, "end": 3814.46, "text": " yeah, I've never seen any pictures like this before.", "tokens": [51498, 1338, 11, 286, 600, 1128, 1612, 604, 5242, 411, 341, 949, 13, 51644], "temperature": 0.0, "avg_logprob": -0.3180468498019997, "compression_ratio": 1.6640316205533596, "no_speech_prob": 0.62559574842453}, {"id": 881, "seek": 381446, "start": 3814.78, "end": 3821.9, "text": " So I think this is a fun result from doing all this stuff in notebooks, step by step.", "tokens": [50380, 407, 286, 519, 341, 307, 257, 1019, 1874, 490, 884, 439, 341, 1507, 294, 43782, 11, 1823, 538, 1823, 13, 50736], "temperature": 0.0, "avg_logprob": -0.31525789896647133, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.10968100279569626}, {"id": 882, "seek": 381446, "start": 3821.9, "end": 3825.5, "text": " Okay, so sampling.", "tokens": [50736, 1033, 11, 370, 21179, 13, 50916], "temperature": 0.0, "avg_logprob": -0.31525789896647133, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.10968100279569626}, {"id": 883, "seek": 381446, "start": 3825.5, "end": 3834.78, "text": " So one of the nice things with this is the sampling becomes much, much, much simpler.", "tokens": [50916, 407, 472, 295, 264, 1481, 721, 365, 341, 307, 264, 21179, 3643, 709, 11, 709, 11, 709, 18587, 13, 51380], "temperature": 0.0, "avg_logprob": -0.31525789896647133, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.10968100279569626}, {"id": 884, "seek": 381446, "start": 3834.78, "end": 3842.02, "text": " And so, and I should mention a lot of the code that I'm using, particularly in the sampling", "tokens": [51380, 400, 370, 11, 293, 286, 820, 2152, 257, 688, 295, 264, 3089, 300, 286, 478, 1228, 11, 4098, 294, 264, 21179, 51742], "temperature": 0.0, "avg_logprob": -0.31525789896647133, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.10968100279569626}, {"id": 885, "seek": 384202, "start": 3842.1, "end": 3849.62, "text": " section, is heavily inspired by, and some of it's actually copied and pasted from, Cat's", "tokens": [50368, 3541, 11, 307, 10950, 7547, 538, 11, 293, 512, 295, 309, 311, 767, 25365, 293, 1791, 292, 490, 11, 9565, 311, 50744], "temperature": 0.0, "avg_logprob": -0.3283409118652344, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.007345480378717184}, {"id": 886, "seek": 384202, "start": 3849.62, "end": 3857.58, "text": " KDiffusion repo, which is, I think I mentioned before, is some of the nicest generative modeling", "tokens": [50744, 591, 35, 3661, 5704, 49040, 11, 597, 307, 11, 286, 519, 286, 2835, 949, 11, 307, 512, 295, 264, 45516, 1337, 1166, 15983, 51142], "temperature": 0.0, "avg_logprob": -0.3283409118652344, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.007345480378717184}, {"id": 887, "seek": 384202, "start": 3857.58, "end": 3861.3, "text": " code, or maybe the nicest generative modeling code I've ever seen.", "tokens": [51142, 3089, 11, 420, 1310, 264, 45516, 1337, 1166, 15983, 3089, 286, 600, 1562, 1612, 13, 51328], "temperature": 0.0, "avg_logprob": -0.3283409118652344, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.007345480378717184}, {"id": 888, "seek": 384202, "start": 3861.3, "end": 3865.02, "text": " It's really great.", "tokens": [51328, 467, 311, 534, 869, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3283409118652344, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.007345480378717184}, {"id": 889, "seek": 384202, "start": 3865.02, "end": 3871.78, "text": " So before we talk about the actual sampling, the first thing we need to talk about is what", "tokens": [51514, 407, 949, 321, 751, 466, 264, 3539, 21179, 11, 264, 700, 551, 321, 643, 281, 751, 466, 307, 437, 51852], "temperature": 0.0, "avg_logprob": -0.3283409118652344, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.007345480378717184}, {"id": 890, "seek": 387178, "start": 3872.5400000000004, "end": 3874.46, "text": " sigma do we use at each reverse time step?", "tokens": [50402, 12771, 360, 321, 764, 412, 1184, 9943, 565, 1823, 30, 50498], "temperature": 0.0, "avg_logprob": -0.2789211078565948, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0015978252049535513}, {"id": 891, "seek": 387178, "start": 3874.46, "end": 3879.2200000000003, "text": " And in the past, we've always, well, nearly always done something which I think has always", "tokens": [50498, 400, 294, 264, 1791, 11, 321, 600, 1009, 11, 731, 11, 6217, 1009, 1096, 746, 597, 286, 519, 575, 1009, 50736], "temperature": 0.0, "avg_logprob": -0.2789211078565948, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0015978252049535513}, {"id": 892, "seek": 387178, "start": 3879.2200000000003, "end": 3886.26, "text": " felt as sketchy as all hell, which is we've just linearly gone down the sigmas or the", "tokens": [50736, 2762, 382, 12325, 88, 382, 439, 4921, 11, 597, 307, 321, 600, 445, 43586, 2780, 760, 264, 4556, 3799, 420, 264, 51088], "temperature": 0.0, "avg_logprob": -0.2789211078565948, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0015978252049535513}, {"id": 893, "seek": 387178, "start": 3886.26, "end": 3887.86, "text": " alpha bars or the Ts.", "tokens": [51088, 8961, 10228, 420, 264, 16518, 13, 51168], "temperature": 0.0, "avg_logprob": -0.2789211078565948, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0015978252049535513}, {"id": 894, "seek": 387178, "start": 3887.86, "end": 3892.38, "text": " So here, when we're sampling in the previous notebook, we used linspace.", "tokens": [51168, 407, 510, 11, 562, 321, 434, 21179, 294, 264, 3894, 21060, 11, 321, 1143, 287, 1292, 17940, 13, 51394], "temperature": 0.0, "avg_logprob": -0.2789211078565948, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0015978252049535513}, {"id": 895, "seek": 387178, "start": 3892.38, "end": 3896.7000000000003, "text": " So I always felt like that was questionable.", "tokens": [51394, 407, 286, 1009, 2762, 411, 300, 390, 37158, 13, 51610], "temperature": 0.0, "avg_logprob": -0.2789211078565948, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0015978252049535513}, {"id": 896, "seek": 389670, "start": 3896.7, "end": 3902.14, "text": " And I felt like at the start, you probably, like, it was just noise anyway, so who cared?", "tokens": [50364, 400, 286, 2762, 411, 412, 264, 722, 11, 291, 1391, 11, 411, 11, 309, 390, 445, 5658, 4033, 11, 370, 567, 19779, 30, 50636], "temperature": 0.0, "avg_logprob": -0.3307240706223708, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.20936547219753265}, {"id": 897, "seek": 389670, "start": 3902.14, "end": 3903.3399999999997, "text": " Who cares?", "tokens": [50636, 2102, 12310, 30, 50696], "temperature": 0.0, "avg_logprob": -0.3307240706223708, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.20936547219753265}, {"id": 898, "seek": 389670, "start": 3903.3399999999997, "end": 3911.2599999999998, "text": " So I, in DDPM v3, I experimented with something that I thought intuitively made more sense.", "tokens": [50696, 407, 286, 11, 294, 413, 11373, 44, 371, 18, 11, 286, 5120, 292, 365, 746, 300, 286, 1194, 46506, 1027, 544, 2020, 13, 51092], "temperature": 0.0, "avg_logprob": -0.3307240706223708, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.20936547219753265}, {"id": 899, "seek": 389670, "start": 3911.2599999999998, "end": 3917.8199999999997, "text": " I don't know if you remember this one, but I actually said, oh, let's, for the first", "tokens": [51092, 286, 500, 380, 458, 498, 291, 1604, 341, 472, 11, 457, 286, 767, 848, 11, 1954, 11, 718, 311, 11, 337, 264, 700, 51420], "temperature": 0.0, "avg_logprob": -0.3307240706223708, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.20936547219753265}, {"id": 900, "seek": 389670, "start": 3917.8199999999997, "end": 3921.74, "text": " hundred time steps, let's actually only run the model every 10 times.", "tokens": [51420, 3262, 565, 4439, 11, 718, 311, 767, 787, 1190, 264, 2316, 633, 1266, 1413, 13, 51616], "temperature": 0.0, "avg_logprob": -0.3307240706223708, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.20936547219753265}, {"id": 901, "seek": 389670, "start": 3921.74, "end": 3924.2999999999997, "text": " And then for the next hundred, let's run it nine times.", "tokens": [51616, 400, 550, 337, 264, 958, 3262, 11, 718, 311, 1190, 309, 4949, 1413, 13, 51744], "temperature": 0.0, "avg_logprob": -0.3307240706223708, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.20936547219753265}, {"id": 902, "seek": 389670, "start": 3924.2999999999997, "end": 3926.66, "text": " And the next 100, let's run it every eight times.", "tokens": [51744, 400, 264, 958, 2319, 11, 718, 311, 1190, 309, 633, 3180, 1413, 13, 51862], "temperature": 0.0, "avg_logprob": -0.3307240706223708, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.20936547219753265}, {"id": 903, "seek": 392666, "start": 3926.66, "end": 3930.42, "text": " So basically at the start, be much less careful.", "tokens": [50364, 407, 1936, 412, 264, 722, 11, 312, 709, 1570, 5026, 13, 50552], "temperature": 0.0, "avg_logprob": -0.2304835264710174, "compression_ratio": 1.7444444444444445, "no_speech_prob": 8.220036397688091e-05}, {"id": 904, "seek": 392666, "start": 3930.42, "end": 3937.94, "text": " And so Keras actually ran a whole bunch of experiments.", "tokens": [50552, 400, 370, 591, 6985, 767, 5872, 257, 1379, 3840, 295, 12050, 13, 50928], "temperature": 0.0, "avg_logprob": -0.2304835264710174, "compression_ratio": 1.7444444444444445, "no_speech_prob": 8.220036397688091e-05}, {"id": 905, "seek": 392666, "start": 3937.94, "end": 3941.1, "text": " And they said, yeah, you know what?", "tokens": [50928, 400, 436, 848, 11, 1338, 11, 291, 458, 437, 30, 51086], "temperature": 0.0, "avg_logprob": -0.2304835264710174, "compression_ratio": 1.7444444444444445, "no_speech_prob": 8.220036397688091e-05}, {"id": 906, "seek": 392666, "start": 3941.1, "end": 3947.2599999999998, "text": " At the start of training, you know, you can start with a high sigma, but then like step", "tokens": [51086, 1711, 264, 722, 295, 3097, 11, 291, 458, 11, 291, 393, 722, 365, 257, 1090, 12771, 11, 457, 550, 411, 1823, 51394], "temperature": 0.0, "avg_logprob": -0.2304835264710174, "compression_ratio": 1.7444444444444445, "no_speech_prob": 8.220036397688091e-05}, {"id": 907, "seek": 392666, "start": 3947.2599999999998, "end": 3951.74, "text": " to a much lower sigma in the next step, and then a much lower sigma in the next step.", "tokens": [51394, 281, 257, 709, 3126, 12771, 294, 264, 958, 1823, 11, 293, 550, 257, 709, 3126, 12771, 294, 264, 958, 1823, 13, 51618], "temperature": 0.0, "avg_logprob": -0.2304835264710174, "compression_ratio": 1.7444444444444445, "no_speech_prob": 8.220036397688091e-05}, {"id": 908, "seek": 395174, "start": 3951.8199999999997, "end": 3957.18, "text": " And then the longer, the more you train, step by smaller and smaller steps, so that you", "tokens": [50368, 400, 550, 264, 2854, 11, 264, 544, 291, 3847, 11, 1823, 538, 4356, 293, 4356, 4439, 11, 370, 300, 291, 50636], "temperature": 0.0, "avg_logprob": -0.32611185961430617, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.022622253745794296}, {"id": 909, "seek": 395174, "start": 3957.18, "end": 3964.2999999999997, "text": " spend a lot more time fine tuning carefully at the end, and not very much time at the", "tokens": [50636, 3496, 257, 688, 544, 565, 2489, 15164, 7500, 412, 264, 917, 11, 293, 406, 588, 709, 565, 412, 264, 50992], "temperature": 0.0, "avg_logprob": -0.32611185961430617, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.022622253745794296}, {"id": 910, "seek": 395174, "start": 3964.2999999999997, "end": 3965.2999999999997, "text": " start.", "tokens": [50992, 722, 13, 51042], "temperature": 0.0, "avg_logprob": -0.32611185961430617, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.022622253745794296}, {"id": 911, "seek": 395174, "start": 3965.2999999999997, "end": 3969.02, "text": " Now, this has its own problems.", "tokens": [51042, 823, 11, 341, 575, 1080, 1065, 2740, 13, 51228], "temperature": 0.0, "avg_logprob": -0.32611185961430617, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.022622253745794296}, {"id": 912, "seek": 395174, "start": 3969.02, "end": 3973.02, "text": " And in fact, a paper just came out today, which we probably won't talk about today,", "tokens": [51228, 400, 294, 1186, 11, 257, 3035, 445, 1361, 484, 965, 11, 597, 321, 1391, 1582, 380, 751, 466, 965, 11, 51428], "temperature": 0.0, "avg_logprob": -0.32611185961430617, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.022622253745794296}, {"id": 913, "seek": 395174, "start": 3973.02, "end": 3977.5, "text": " but maybe another time, which talked about the problems is that in these very early steps,", "tokens": [51428, 457, 1310, 1071, 565, 11, 597, 2825, 466, 264, 2740, 307, 300, 294, 613, 588, 2440, 4439, 11, 51652], "temperature": 0.0, "avg_logprob": -0.32611185961430617, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.022622253745794296}, {"id": 914, "seek": 397750, "start": 3977.5, "end": 3982.5, "text": " this is the bit where you're trying to create a composition that makes sense.", "tokens": [50364, 341, 307, 264, 857, 689, 291, 434, 1382, 281, 1884, 257, 12686, 300, 1669, 2020, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2605635664845241, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.12753243744373322}, {"id": 915, "seek": 397750, "start": 3982.5, "end": 3985.14, "text": " Now for fashion MNIST, we don't have much composing to do.", "tokens": [50614, 823, 337, 6700, 376, 45, 19756, 11, 321, 500, 380, 362, 709, 715, 6110, 281, 360, 13, 50746], "temperature": 0.0, "avg_logprob": -0.2605635664845241, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.12753243744373322}, {"id": 916, "seek": 397750, "start": 3985.14, "end": 3987.5, "text": " It's just a piece of clothing.", "tokens": [50746, 467, 311, 445, 257, 2522, 295, 11502, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2605635664845241, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.12753243744373322}, {"id": 917, "seek": 397750, "start": 3987.5, "end": 3992.1, "text": " But if you're trying to do an astronaut riding a horse, you know, you've got to think about", "tokens": [50864, 583, 498, 291, 434, 1382, 281, 360, 364, 18516, 9546, 257, 6832, 11, 291, 458, 11, 291, 600, 658, 281, 519, 466, 51094], "temperature": 0.0, "avg_logprob": -0.2605635664845241, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.12753243744373322}, {"id": 918, "seek": 397750, "start": 3992.1, "end": 3994.98, "text": " how all those pieces fit together.", "tokens": [51094, 577, 439, 729, 3755, 3318, 1214, 13, 51238], "temperature": 0.0, "avg_logprob": -0.2605635664845241, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.12753243744373322}, {"id": 919, "seek": 397750, "start": 3994.98, "end": 3995.98, "text": " And this is where that happens.", "tokens": [51238, 400, 341, 307, 689, 300, 2314, 13, 51288], "temperature": 0.0, "avg_logprob": -0.2605635664845241, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.12753243744373322}, {"id": 920, "seek": 397750, "start": 3995.98, "end": 4001.22, "text": " And so I do worry that with the Keras approach is it's not giving that maybe enough time.", "tokens": [51288, 400, 370, 286, 360, 3292, 300, 365, 264, 591, 6985, 3109, 307, 309, 311, 406, 2902, 300, 1310, 1547, 565, 13, 51550], "temperature": 0.0, "avg_logprob": -0.2605635664845241, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.12753243744373322}, {"id": 921, "seek": 397750, "start": 4001.22, "end": 4004.54, "text": " But as I've said, that's really the same as this step.", "tokens": [51550, 583, 382, 286, 600, 848, 11, 300, 311, 534, 264, 912, 382, 341, 1823, 13, 51716], "temperature": 0.0, "avg_logprob": -0.2605635664845241, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.12753243744373322}, {"id": 922, "seek": 400454, "start": 4004.58, "end": 4009.1, "text": " That whole piece feels a bit wrong to me.", "tokens": [50366, 663, 1379, 2522, 3417, 257, 857, 2085, 281, 385, 13, 50592], "temperature": 0.0, "avg_logprob": -0.2318569997723183, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.038460444658994675}, {"id": 923, "seek": 400454, "start": 4009.1, "end": 4014.18, "text": " But aside from that, I think this makes a lot of sense, which is that, yeah, the sampling,", "tokens": [50592, 583, 7359, 490, 300, 11, 286, 519, 341, 1669, 257, 688, 295, 2020, 11, 597, 307, 300, 11, 1338, 11, 264, 21179, 11, 50846], "temperature": 0.0, "avg_logprob": -0.2318569997723183, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.038460444658994675}, {"id": 924, "seek": 400454, "start": 4014.18, "end": 4019.82, "text": " you should jump, you know, by big steps early on, and small steps later on, and make sure", "tokens": [50846, 291, 820, 3012, 11, 291, 458, 11, 538, 955, 4439, 2440, 322, 11, 293, 1359, 4439, 1780, 322, 11, 293, 652, 988, 51128], "temperature": 0.0, "avg_logprob": -0.2318569997723183, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.038460444658994675}, {"id": 925, "seek": 400454, "start": 4019.82, "end": 4024.14, "text": " that the fine details are just so.", "tokens": [51128, 300, 264, 2489, 4365, 366, 445, 370, 13, 51344], "temperature": 0.0, "avg_logprob": -0.2318569997723183, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.038460444658994675}, {"id": 926, "seek": 400454, "start": 4024.14, "end": 4028.86, "text": " So that's what this function does, is it creates this plot.", "tokens": [51344, 407, 300, 311, 437, 341, 2445, 775, 11, 307, 309, 7829, 341, 7542, 13, 51580], "temperature": 0.0, "avg_logprob": -0.2318569997723183, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.038460444658994675}, {"id": 927, "seek": 402886, "start": 4028.86, "end": 4036.1800000000003, "text": " Now it's this schedule of reverse diffusion sigma steps.", "tokens": [50364, 823, 309, 311, 341, 7567, 295, 9943, 25242, 12771, 4439, 13, 50730], "temperature": 0.0, "avg_logprob": -0.2908355807080681, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.012052420526742935}, {"id": 928, "seek": 402886, "start": 4036.1800000000003, "end": 4043.78, "text": " It's a bit of a weird function, in that it's the rowth root of sigma, where row is 7.", "tokens": [50730, 467, 311, 257, 857, 295, 257, 3657, 2445, 11, 294, 300, 309, 311, 264, 5386, 392, 5593, 295, 12771, 11, 689, 5386, 307, 1614, 13, 51110], "temperature": 0.0, "avg_logprob": -0.2908355807080681, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.012052420526742935}, {"id": 929, "seek": 402886, "start": 4043.78, "end": 4049.7200000000003, "text": " So the seventh root of sigma is basically what it's scaling on.", "tokens": [51110, 407, 264, 17875, 5593, 295, 12771, 307, 1936, 437, 309, 311, 21589, 322, 13, 51407], "temperature": 0.0, "avg_logprob": -0.2908355807080681, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.012052420526742935}, {"id": 930, "seek": 402886, "start": 4049.7200000000003, "end": 4055.9, "text": " But the answer to why it's that is because they tried it, and it turned out to work pretty", "tokens": [51407, 583, 264, 1867, 281, 983, 309, 311, 300, 307, 570, 436, 3031, 309, 11, 293, 309, 3574, 484, 281, 589, 1238, 51716], "temperature": 0.0, "avg_logprob": -0.2908355807080681, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.012052420526742935}, {"id": 931, "seek": 405590, "start": 4055.9, "end": 4056.9, "text": " well.", "tokens": [50364, 731, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3928178971813571, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.4262164831161499}, {"id": 932, "seek": 405590, "start": 4056.9, "end": 4062.2200000000003, "text": " Do you guys remember where this was?", "tokens": [50414, 1144, 291, 1074, 1604, 689, 341, 390, 30, 50680], "temperature": 0.0, "avg_logprob": -0.3928178971813571, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.4262164831161499}, {"id": 933, "seek": 405590, "start": 4062.2200000000003, "end": 4067.42, "text": " This is a truncation error analysis, D1.", "tokens": [50680, 639, 307, 257, 504, 409, 46252, 6713, 5215, 11, 413, 16, 13, 50940], "temperature": 0.0, "avg_logprob": -0.3928178971813571, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.4262164831161499}, {"id": 934, "seek": 405590, "start": 4067.42, "end": 4072.1800000000003, "text": " Nice memory.", "tokens": [50940, 5490, 4675, 13, 51178], "temperature": 0.0, "avg_logprob": -0.3928178971813571, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.4262164831161499}, {"id": 935, "seek": 405590, "start": 4072.1800000000003, "end": 4084.1800000000003, "text": " So this image here, so thanks for reminding me where this is, shows FID as a function", "tokens": [51178, 407, 341, 3256, 510, 11, 370, 3231, 337, 27639, 385, 689, 341, 307, 11, 3110, 479, 2777, 382, 257, 2445, 51778], "temperature": 0.0, "avg_logprob": -0.3928178971813571, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.4262164831161499}, {"id": 936, "seek": 405590, "start": 4084.1800000000003, "end": 4085.1800000000003, "text": " of row.", "tokens": [51778, 295, 5386, 13, 51828], "temperature": 0.0, "avg_logprob": -0.3928178971813571, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.4262164831161499}, {"id": 937, "seek": 408518, "start": 4085.46, "end": 4090.14, "text": " So it's basically what, the what root are we taking.", "tokens": [50378, 407, 309, 311, 1936, 437, 11, 264, 437, 5593, 366, 321, 1940, 13, 50612], "temperature": 0.0, "avg_logprob": -0.344853925275373, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.05261458083987236}, {"id": 938, "seek": 408518, "start": 4090.14, "end": 4098.22, "text": " And they basically said like if you take the fifth root up, it seems to work well, basically.", "tokens": [50612, 400, 436, 1936, 848, 411, 498, 291, 747, 264, 9266, 5593, 493, 11, 309, 2544, 281, 589, 731, 11, 1936, 13, 51016], "temperature": 0.0, "avg_logprob": -0.344853925275373, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.05261458083987236}, {"id": 939, "seek": 408518, "start": 4098.22, "end": 4102.98, "text": " So yeah, so that's a perfectly good way to do things, is just to try things and see what", "tokens": [51016, 407, 1338, 11, 370, 300, 311, 257, 6239, 665, 636, 281, 360, 721, 11, 307, 445, 281, 853, 721, 293, 536, 437, 51254], "temperature": 0.0, "avg_logprob": -0.344853925275373, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.05261458083987236}, {"id": 940, "seek": 408518, "start": 4102.98, "end": 4104.42, "text": " works.", "tokens": [51254, 1985, 13, 51326], "temperature": 0.0, "avg_logprob": -0.344853925275373, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.05261458083987236}, {"id": 941, "seek": 408518, "start": 4104.42, "end": 4108.16, "text": " And you'll notice they tried things just like we love on small datasets.", "tokens": [51326, 400, 291, 603, 3449, 436, 3031, 721, 445, 411, 321, 959, 322, 1359, 42856, 13, 51513], "temperature": 0.0, "avg_logprob": -0.344853925275373, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.05261458083987236}, {"id": 942, "seek": 408518, "start": 4108.16, "end": 4112.98, "text": " Not as small as us, because we're the king of small datasets, but smallish, sci-fi 10,", "tokens": [51513, 1726, 382, 1359, 382, 505, 11, 570, 321, 434, 264, 4867, 295, 1359, 42856, 11, 457, 1359, 742, 11, 2180, 12, 13325, 1266, 11, 51754], "temperature": 0.0, "avg_logprob": -0.344853925275373, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.05261458083987236}, {"id": 943, "seek": 411298, "start": 4112.98, "end": 4115.219999999999, "text": " image net 64.", "tokens": [50364, 3256, 2533, 12145, 13, 50476], "temperature": 0.0, "avg_logprob": -0.2992863786329917, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.6511915326118469}, {"id": 944, "seek": 411298, "start": 4115.219999999999, "end": 4116.219999999999, "text": " That's the way to do things.", "tokens": [50476, 663, 311, 264, 636, 281, 360, 721, 13, 50526], "temperature": 0.0, "avg_logprob": -0.2992863786329917, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.6511915326118469}, {"id": 945, "seek": 411298, "start": 4116.219999999999, "end": 4121.98, "text": " I saw like, it might have even been the CEO of Hugging Face the other day, tweet something", "tokens": [50526, 286, 1866, 411, 11, 309, 1062, 362, 754, 668, 264, 9282, 295, 46892, 3249, 4047, 264, 661, 786, 11, 15258, 746, 50814], "temperature": 0.0, "avg_logprob": -0.2992863786329917, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.6511915326118469}, {"id": 946, "seek": 411298, "start": 4121.98, "end": 4125.82, "text": " saying only people with huge amounts of GPUs can do research now.", "tokens": [50814, 1566, 787, 561, 365, 2603, 11663, 295, 18407, 82, 393, 360, 2132, 586, 13, 51006], "temperature": 0.0, "avg_logprob": -0.2992863786329917, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.6511915326118469}, {"id": 947, "seek": 411298, "start": 4125.82, "end": 4129.54, "text": " And I think it totally misunderstands how research is done, which is research is done", "tokens": [51006, 400, 286, 519, 309, 3879, 35736, 82, 577, 2132, 307, 1096, 11, 597, 307, 2132, 307, 1096, 51192], "temperature": 0.0, "avg_logprob": -0.2992863786329917, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.6511915326118469}, {"id": 948, "seek": 411298, "start": 4129.54, "end": 4132.62, "text": " on very small datasets.", "tokens": [51192, 322, 588, 1359, 42856, 13, 51346], "temperature": 0.0, "avg_logprob": -0.2992863786329917, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.6511915326118469}, {"id": 949, "seek": 411298, "start": 4132.62, "end": 4134.58, "text": " That's the actual research.", "tokens": [51346, 663, 311, 264, 3539, 2132, 13, 51444], "temperature": 0.0, "avg_logprob": -0.2992863786329917, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.6511915326118469}, {"id": 950, "seek": 411298, "start": 4134.58, "end": 4140.339999999999, "text": " And then when you're all done, you scale it up at the end.", "tokens": [51444, 400, 550, 562, 291, 434, 439, 1096, 11, 291, 4373, 309, 493, 412, 264, 917, 13, 51732], "temperature": 0.0, "avg_logprob": -0.2992863786329917, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.6511915326118469}, {"id": 951, "seek": 414034, "start": 4140.38, "end": 4145.900000000001, "text": " I think we're kind of pushing the envelope in terms of like, yeah, how much can you do?", "tokens": [50366, 286, 519, 321, 434, 733, 295, 7380, 264, 19989, 294, 2115, 295, 411, 11, 1338, 11, 577, 709, 393, 291, 360, 30, 50642], "temperature": 0.0, "avg_logprob": -0.3103479601673244, "compression_ratio": 1.549800796812749, "no_speech_prob": 0.0015977902803570032}, {"id": 952, "seek": 414034, "start": 4145.900000000001, "end": 4156.14, "text": " And yeah, we've like re-covered this kind of main substantive path of the Fusion models", "tokens": [50642, 400, 1338, 11, 321, 600, 411, 319, 12, 12516, 292, 341, 733, 295, 2135, 47113, 3100, 295, 264, 36721, 5245, 51154], "temperature": 0.0, "avg_logprob": -0.3103479601673244, "compression_ratio": 1.549800796812749, "no_speech_prob": 0.0015977902803570032}, {"id": 953, "seek": 414034, "start": 4156.14, "end": 4161.58, "text": " history step-by-step, showing every improvement and seeing clear improvements across all the", "tokens": [51154, 2503, 1823, 12, 2322, 12, 16792, 11, 4099, 633, 10444, 293, 2577, 1850, 13797, 2108, 439, 264, 51426], "temperature": 0.0, "avg_logprob": -0.3103479601673244, "compression_ratio": 1.549800796812749, "no_speech_prob": 0.0015977902803570032}, {"id": 954, "seek": 414034, "start": 4161.58, "end": 4166.9400000000005, "text": " papers using nothing but Fashioned MNIST, running on a single GPU in like 15 minutes", "tokens": [51426, 10577, 1228, 1825, 457, 32782, 292, 376, 45, 19756, 11, 2614, 322, 257, 2167, 18407, 294, 411, 2119, 2077, 51694], "temperature": 0.0, "avg_logprob": -0.3103479601673244, "compression_ratio": 1.549800796812749, "no_speech_prob": 0.0015977902803570032}, {"id": 955, "seek": 414034, "start": 4166.9400000000005, "end": 4168.62, "text": " of training or something per model.", "tokens": [51694, 295, 3097, 420, 746, 680, 2316, 13, 51778], "temperature": 0.0, "avg_logprob": -0.3103479601673244, "compression_ratio": 1.549800796812749, "no_speech_prob": 0.0015977902803570032}, {"id": 956, "seek": 416862, "start": 4168.62, "end": 4172.94, "text": " So yeah, definitely don't need lots of models.", "tokens": [50364, 407, 1338, 11, 2138, 500, 380, 643, 3195, 295, 5245, 13, 50580], "temperature": 0.0, "avg_logprob": -0.3527079461847694, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.017176123335957527}, {"id": 957, "seek": 416862, "start": 4172.94, "end": 4178.0199999999995, "text": " Anyway, okay, so this is the Sigma we're going to jump to.", "tokens": [50580, 5684, 11, 1392, 11, 370, 341, 307, 264, 36595, 321, 434, 516, 281, 3012, 281, 13, 50834], "temperature": 0.0, "avg_logprob": -0.3527079461847694, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.017176123335957527}, {"id": 958, "seek": 416862, "start": 4178.0199999999995, "end": 4186.34, "text": " So the denoising is going to involve calculating the CSKIP, COUT and CIN, and calling our model", "tokens": [50834, 407, 264, 1441, 78, 3436, 307, 516, 281, 9494, 28258, 264, 9460, 42, 9139, 11, 383, 27276, 293, 383, 1464, 11, 293, 5141, 527, 2316, 51250], "temperature": 0.0, "avg_logprob": -0.3527079461847694, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.017176123335957527}, {"id": 959, "seek": 416862, "start": 4186.34, "end": 4193.42, "text": " with the CIN scaled data and the Sigma, and then scaling it with COUT and then doing the", "tokens": [51250, 365, 264, 383, 1464, 36039, 1412, 293, 264, 36595, 11, 293, 550, 21589, 309, 365, 383, 27276, 293, 550, 884, 264, 51604], "temperature": 0.0, "avg_logprob": -0.3527079461847694, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.017176123335957527}, {"id": 960, "seek": 416862, "start": 4193.42, "end": 4194.42, "text": " CSKIP.", "tokens": [51604, 9460, 42, 9139, 13, 51654], "temperature": 0.0, "avg_logprob": -0.3527079461847694, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.017176123335957527}, {"id": 961, "seek": 416862, "start": 4194.42, "end": 4197.14, "text": " Okay, so that's just undoing the noiseify.", "tokens": [51654, 1033, 11, 370, 300, 311, 445, 23779, 278, 264, 5658, 2505, 13, 51790], "temperature": 0.0, "avg_logprob": -0.3527079461847694, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.017176123335957527}, {"id": 962, "seek": 419714, "start": 4198.14, "end": 4199.9400000000005, "text": " So check this out.", "tokens": [50414, 407, 1520, 341, 484, 13, 50504], "temperature": 0.0, "avg_logprob": -0.3005951890000352, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006666820263490081}, {"id": 963, "seek": 419714, "start": 4199.9400000000005, "end": 4206.780000000001, "text": " This is all that's required to do one step of denoising for the simplest kind of scheduler,", "tokens": [50504, 639, 307, 439, 300, 311, 4739, 281, 360, 472, 1823, 295, 1441, 78, 3436, 337, 264, 22811, 733, 295, 12000, 260, 11, 50846], "temperature": 0.0, "avg_logprob": -0.3005951890000352, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006666820263490081}, {"id": 964, "seek": 419714, "start": 4206.780000000001, "end": 4210.06, "text": " which is, sorry, the simplest kind of sampler, which is called Euler.", "tokens": [50846, 597, 307, 11, 2597, 11, 264, 22811, 733, 295, 3247, 22732, 11, 597, 307, 1219, 462, 26318, 13, 51010], "temperature": 0.0, "avg_logprob": -0.3005951890000352, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006666820263490081}, {"id": 965, "seek": 419714, "start": 4210.06, "end": 4216.26, "text": " So we basically say, okay, what's the Sigma at time step i?", "tokens": [51010, 407, 321, 1936, 584, 11, 1392, 11, 437, 311, 264, 36595, 412, 565, 1823, 741, 30, 51320], "temperature": 0.0, "avg_logprob": -0.3005951890000352, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006666820263490081}, {"id": 966, "seek": 419714, "start": 4216.26, "end": 4218.42, "text": " What's the Sigma2 at time step i?", "tokens": [51320, 708, 311, 264, 36595, 17, 412, 565, 1823, 741, 30, 51428], "temperature": 0.0, "avg_logprob": -0.3005951890000352, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006666820263490081}, {"id": 967, "seek": 419714, "start": 4218.42, "end": 4222.9400000000005, "text": " And now when I'm talking about time step, I'm really talking about like the step from", "tokens": [51428, 400, 586, 562, 286, 478, 1417, 466, 565, 1823, 11, 286, 478, 534, 1417, 466, 411, 264, 1823, 490, 51654], "temperature": 0.0, "avg_logprob": -0.3005951890000352, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006666820263490081}, {"id": 968, "seek": 419714, "start": 4222.9400000000005, "end": 4224.46, "text": " this function, right?", "tokens": [51654, 341, 2445, 11, 558, 30, 51730], "temperature": 0.0, "avg_logprob": -0.3005951890000352, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0006666820263490081}, {"id": 969, "seek": 422446, "start": 4224.46, "end": 4227.38, "text": " So this is the sampling step.", "tokens": [50364, 407, 341, 307, 264, 21179, 1823, 13, 50510], "temperature": 0.0, "avg_logprob": -0.30401273454938615, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.014061582274734974}, {"id": 970, "seek": 422446, "start": 4227.38, "end": 4229.5, "text": " Sampling step, yeah.", "tokens": [50510, 4832, 11970, 1823, 11, 1338, 13, 50616], "temperature": 0.0, "avg_logprob": -0.30401273454938615, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.014061582274734974}, {"id": 971, "seek": 422446, "start": 4229.5, "end": 4238.06, "text": " Okay, so then denoise using the function, and then we say, okay, well, just send back", "tokens": [50616, 1033, 11, 370, 550, 1441, 38800, 1228, 264, 2445, 11, 293, 550, 321, 584, 11, 1392, 11, 731, 11, 445, 2845, 646, 51044], "temperature": 0.0, "avg_logprob": -0.30401273454938615, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.014061582274734974}, {"id": 972, "seek": 422446, "start": 4238.06, "end": 4245.54, "text": " whatever you were given, plus move a little bit in the direction of the denoised image.", "tokens": [51044, 2035, 291, 645, 2212, 11, 1804, 1286, 257, 707, 857, 294, 264, 3513, 295, 264, 1441, 78, 2640, 3256, 13, 51418], "temperature": 0.0, "avg_logprob": -0.30401273454938615, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.014061582274734974}, {"id": 973, "seek": 422446, "start": 4245.54, "end": 4248.9800000000005, "text": " So the direction is x minus denoised.", "tokens": [51418, 407, 264, 3513, 307, 2031, 3175, 1441, 78, 2640, 13, 51590], "temperature": 0.0, "avg_logprob": -0.30401273454938615, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.014061582274734974}, {"id": 974, "seek": 422446, "start": 4248.9800000000005, "end": 4252.94, "text": " So that's the noise, that's the gradient, as we discussed right back in the first lesson", "tokens": [51590, 407, 300, 311, 264, 5658, 11, 300, 311, 264, 16235, 11, 382, 321, 7152, 558, 646, 294, 264, 700, 6898, 51788], "temperature": 0.0, "avg_logprob": -0.30401273454938615, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.014061582274734974}, {"id": 975, "seek": 422446, "start": 4252.94, "end": 4254.22, "text": " of this part.", "tokens": [51788, 295, 341, 644, 13, 51852], "temperature": 0.0, "avg_logprob": -0.30401273454938615, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.014061582274734974}, {"id": 976, "seek": 425422, "start": 4254.9800000000005, "end": 4256.860000000001, "text": " So we'll take the noise.", "tokens": [50402, 407, 321, 603, 747, 264, 5658, 13, 50496], "temperature": 0.0, "avg_logprob": -0.25869324628044577, "compression_ratio": 1.53125, "no_speech_prob": 0.0016743665328249335}, {"id": 977, "seek": 425422, "start": 4256.860000000001, "end": 4259.22, "text": " If we divide it by Sigma, we get a slope.", "tokens": [50496, 759, 321, 9845, 309, 538, 36595, 11, 321, 483, 257, 13525, 13, 50614], "temperature": 0.0, "avg_logprob": -0.25869324628044577, "compression_ratio": 1.53125, "no_speech_prob": 0.0016743665328249335}, {"id": 978, "seek": 425422, "start": 4259.22, "end": 4263.38, "text": " It's how much noise is there per Sigma.", "tokens": [50614, 467, 311, 577, 709, 5658, 307, 456, 680, 36595, 13, 50822], "temperature": 0.0, "avg_logprob": -0.25869324628044577, "compression_ratio": 1.53125, "no_speech_prob": 0.0016743665328249335}, {"id": 979, "seek": 425422, "start": 4263.38, "end": 4267.360000000001, "text": " And then the amount that we're stepping is Sigma2 minus Sigma1.", "tokens": [50822, 400, 550, 264, 2372, 300, 321, 434, 16821, 307, 36595, 17, 3175, 36595, 16, 13, 51021], "temperature": 0.0, "avg_logprob": -0.25869324628044577, "compression_ratio": 1.53125, "no_speech_prob": 0.0016743665328249335}, {"id": 980, "seek": 425422, "start": 4267.360000000001, "end": 4271.58, "text": " So take that slope and multiply it by the change, right?", "tokens": [51021, 407, 747, 300, 13525, 293, 12972, 309, 538, 264, 1319, 11, 558, 30, 51232], "temperature": 0.0, "avg_logprob": -0.25869324628044577, "compression_ratio": 1.53125, "no_speech_prob": 0.0016743665328249335}, {"id": 981, "seek": 425422, "start": 4271.58, "end": 4280.02, "text": " So that's the distance to travel towards the noise, this fraction.", "tokens": [51232, 407, 300, 311, 264, 4560, 281, 3147, 3030, 264, 5658, 11, 341, 14135, 13, 51654], "temperature": 0.0, "avg_logprob": -0.25869324628044577, "compression_ratio": 1.53125, "no_speech_prob": 0.0016743665328249335}, {"id": 982, "seek": 428002, "start": 4280.660000000001, "end": 4285.22, "text": " You could also think of it this way, and I know this is a very obvious algebraic change,", "tokens": [50396, 509, 727, 611, 519, 295, 309, 341, 636, 11, 293, 286, 458, 341, 307, 257, 588, 6322, 21989, 299, 1319, 11, 50624], "temperature": 0.0, "avg_logprob": -0.24482580025990805, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.006692576687783003}, {"id": 983, "seek": 428002, "start": 4285.22, "end": 4293.660000000001, "text": " but if we move this over here, you could also think of this as being, oh, of the total amount", "tokens": [50624, 457, 498, 321, 1286, 341, 670, 510, 11, 291, 727, 611, 519, 295, 341, 382, 885, 11, 1954, 11, 295, 264, 3217, 2372, 51046], "temperature": 0.0, "avg_logprob": -0.24482580025990805, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.006692576687783003}, {"id": 984, "seek": 428002, "start": 4293.660000000001, "end": 4298.740000000001, "text": " of noise, the change in Sigma we're doing, what percentage is that?", "tokens": [51046, 295, 5658, 11, 264, 1319, 294, 36595, 321, 434, 884, 11, 437, 9668, 307, 300, 30, 51300], "temperature": 0.0, "avg_logprob": -0.24482580025990805, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.006692576687783003}, {"id": 985, "seek": 428002, "start": 4298.740000000001, "end": 4302.540000000001, "text": " Okay, well, that's the amount we should step.", "tokens": [51300, 1033, 11, 731, 11, 300, 311, 264, 2372, 321, 820, 1823, 13, 51490], "temperature": 0.0, "avg_logprob": -0.24482580025990805, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.006692576687783003}, {"id": 986, "seek": 428002, "start": 4302.540000000001, "end": 4306.02, "text": " So there's two ways of thinking about the same thing.", "tokens": [51490, 407, 456, 311, 732, 2098, 295, 1953, 466, 264, 912, 551, 13, 51664], "temperature": 0.0, "avg_logprob": -0.24482580025990805, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.006692576687783003}, {"id": 987, "seek": 430602, "start": 4306.02, "end": 4311.02, "text": " So again, this is just, you know, high school math.", "tokens": [50364, 407, 797, 11, 341, 307, 445, 11, 291, 458, 11, 1090, 1395, 5221, 13, 50614], "temperature": 0.0, "avg_logprob": -0.26722845973738707, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0022169204894453287}, {"id": 988, "seek": 430602, "start": 4311.02, "end": 4318.42, "text": " Well, I mean, actually, my seven-year-old daughter has done all these things.", "tokens": [50614, 1042, 11, 286, 914, 11, 767, 11, 452, 3407, 12, 5294, 12, 2641, 4653, 575, 1096, 439, 613, 721, 13, 50984], "temperature": 0.0, "avg_logprob": -0.26722845973738707, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0022169204894453287}, {"id": 989, "seek": 430602, "start": 4318.42, "end": 4323.900000000001, "text": " It's plus, minus, divide, and times.", "tokens": [50984, 467, 311, 1804, 11, 3175, 11, 9845, 11, 293, 1413, 13, 51258], "temperature": 0.0, "avg_logprob": -0.26722845973738707, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0022169204894453287}, {"id": 990, "seek": 430602, "start": 4323.900000000001, "end": 4329.02, "text": " So we're going to need to do this once per sampling step.", "tokens": [51258, 407, 321, 434, 516, 281, 643, 281, 360, 341, 1564, 680, 21179, 1823, 13, 51514], "temperature": 0.0, "avg_logprob": -0.26722845973738707, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0022169204894453287}, {"id": 991, "seek": 430602, "start": 4329.02, "end": 4333.620000000001, "text": " So here's a thing called sample, which does that.", "tokens": [51514, 407, 510, 311, 257, 551, 1219, 6889, 11, 597, 775, 300, 13, 51744], "temperature": 0.0, "avg_logprob": -0.26722845973738707, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0022169204894453287}, {"id": 992, "seek": 433362, "start": 4333.62, "end": 4341.82, "text": " It's going to go through each sampling step, call our sampler, which initially we're going", "tokens": [50364, 467, 311, 516, 281, 352, 807, 1184, 21179, 1823, 11, 818, 527, 3247, 22732, 11, 597, 9105, 321, 434, 516, 50774], "temperature": 0.0, "avg_logprob": -0.2277311845259233, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.006488101091235876}, {"id": 993, "seek": 433362, "start": 4341.82, "end": 4345.099999999999, "text": " to do sample Euler, right?", "tokens": [50774, 281, 360, 6889, 462, 26318, 11, 558, 30, 50938], "temperature": 0.0, "avg_logprob": -0.2277311845259233, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.006488101091235876}, {"id": 994, "seek": 433362, "start": 4345.099999999999, "end": 4353.18, "text": " With that information, add it to our list of results, and do it again.", "tokens": [50938, 2022, 300, 1589, 11, 909, 309, 281, 527, 1329, 295, 3542, 11, 293, 360, 309, 797, 13, 51342], "temperature": 0.0, "avg_logprob": -0.2277311845259233, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.006488101091235876}, {"id": 995, "seek": 433362, "start": 4353.18, "end": 4354.18, "text": " So that's it.", "tokens": [51342, 407, 300, 311, 309, 13, 51392], "temperature": 0.0, "avg_logprob": -0.2277311845259233, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.006488101091235876}, {"id": 996, "seek": 433362, "start": 4354.18, "end": 4356.34, "text": " That's all the sampling is.", "tokens": [51392, 663, 311, 439, 264, 21179, 307, 13, 51500], "temperature": 0.0, "avg_logprob": -0.2277311845259233, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.006488101091235876}, {"id": 997, "seek": 433362, "start": 4356.34, "end": 4361.3, "text": " And of course, we need to grab our list of Sigmas to start with.", "tokens": [51500, 400, 295, 1164, 11, 321, 643, 281, 4444, 527, 1329, 295, 37763, 3799, 281, 722, 365, 13, 51748], "temperature": 0.0, "avg_logprob": -0.2277311845259233, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.006488101091235876}, {"id": 998, "seek": 436130, "start": 4361.3, "end": 4363.02, "text": " So I think that's pretty cool.", "tokens": [50364, 407, 286, 519, 300, 311, 1238, 1627, 13, 50450], "temperature": 0.0, "avg_logprob": -0.2444159628330976, "compression_ratio": 1.455, "no_speech_prob": 0.014281506650149822}, {"id": 999, "seek": 436130, "start": 4363.02, "end": 4366.9400000000005, "text": " And at the very start, we need to create our pure noise image.", "tokens": [50450, 400, 412, 264, 588, 722, 11, 321, 643, 281, 1884, 527, 6075, 5658, 3256, 13, 50646], "temperature": 0.0, "avg_logprob": -0.2444159628330976, "compression_ratio": 1.455, "no_speech_prob": 0.014281506650149822}, {"id": 1000, "seek": 436130, "start": 4366.9400000000005, "end": 4371.7, "text": " And so the amount of noise we start with is got a Sigma of 80.", "tokens": [50646, 400, 370, 264, 2372, 295, 5658, 321, 722, 365, 307, 658, 257, 36595, 295, 4688, 13, 50884], "temperature": 0.0, "avg_logprob": -0.2444159628330976, "compression_ratio": 1.455, "no_speech_prob": 0.014281506650149822}, {"id": 1001, "seek": 436130, "start": 4371.7, "end": 4385.1, "text": " Okay, so if we call sample using sample Euler, then we get back some very nice looking images.", "tokens": [50884, 1033, 11, 370, 498, 321, 818, 6889, 1228, 6889, 462, 26318, 11, 550, 321, 483, 646, 512, 588, 1481, 1237, 5267, 13, 51554], "temperature": 0.0, "avg_logprob": -0.2444159628330976, "compression_ratio": 1.455, "no_speech_prob": 0.014281506650149822}, {"id": 1002, "seek": 436130, "start": 4385.1, "end": 4388.96, "text": " And believe it or not, our FED is 1.98.", "tokens": [51554, 400, 1697, 309, 420, 406, 11, 527, 479, 4731, 307, 502, 13, 22516, 13, 51747], "temperature": 0.0, "avg_logprob": -0.2444159628330976, "compression_ratio": 1.455, "no_speech_prob": 0.014281506650149822}, {"id": 1003, "seek": 438896, "start": 4388.96, "end": 4404.72, "text": " So this extremely simple sampler, three lines of code, plus a loop, has given us a FED of", "tokens": [50364, 407, 341, 4664, 2199, 3247, 22732, 11, 1045, 3876, 295, 3089, 11, 1804, 257, 6367, 11, 575, 2212, 505, 257, 479, 4731, 295, 51152], "temperature": 0.0, "avg_logprob": -0.2542393207550049, "compression_ratio": 1.2913907284768211, "no_speech_prob": 0.018262458965182304}, {"id": 1004, "seek": 438896, "start": 4404.72, "end": 4413.4800000000005, "text": " 1.98, which is clearly, you know, substantially better than our cosine.", "tokens": [51152, 502, 13, 22516, 11, 597, 307, 4448, 11, 291, 458, 11, 30797, 1101, 813, 527, 23565, 13, 51590], "temperature": 0.0, "avg_logprob": -0.2542393207550049, "compression_ratio": 1.2913907284768211, "no_speech_prob": 0.018262458965182304}, {"id": 1005, "seek": 438896, "start": 4413.4800000000005, "end": 4416.4800000000005, "text": " Now we can improve it from there.", "tokens": [51590, 823, 321, 393, 3470, 309, 490, 456, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2542393207550049, "compression_ratio": 1.2913907284768211, "no_speech_prob": 0.018262458965182304}, {"id": 1006, "seek": 441648, "start": 4416.48, "end": 4423.12, "text": " So one potential improvement is to, you might have noticed, we added no new noise at all,", "tokens": [50364, 407, 472, 3995, 10444, 307, 281, 11, 291, 1062, 362, 5694, 11, 321, 3869, 572, 777, 5658, 412, 439, 11, 50696], "temperature": 0.0, "avg_logprob": -0.22950046578633418, "compression_ratio": 1.5794392523364487, "no_speech_prob": 0.0018968948861584067}, {"id": 1007, "seek": 441648, "start": 4423.12, "end": 4424.12, "text": " right?", "tokens": [50696, 558, 30, 50746], "temperature": 0.0, "avg_logprob": -0.22950046578633418, "compression_ratio": 1.5794392523364487, "no_speech_prob": 0.0018968948861584067}, {"id": 1008, "seek": 441648, "start": 4424.12, "end": 4426.5599999999995, "text": " This is a deterministic scheduler, right?", "tokens": [50746, 639, 307, 257, 15957, 3142, 12000, 260, 11, 558, 30, 50868], "temperature": 0.0, "avg_logprob": -0.22950046578633418, "compression_ratio": 1.5794392523364487, "no_speech_prob": 0.0018968948861584067}, {"id": 1009, "seek": 441648, "start": 4426.5599999999995, "end": 4429.74, "text": " There's no RAND anywhere here.", "tokens": [50868, 821, 311, 572, 497, 8070, 4992, 510, 13, 51027], "temperature": 0.0, "avg_logprob": -0.22950046578633418, "compression_ratio": 1.5794392523364487, "no_speech_prob": 0.0018968948861584067}, {"id": 1010, "seek": 441648, "start": 4429.74, "end": 4439.54, "text": " So we can do something called an ancestral Euler sampler, which does add RAND, right?", "tokens": [51027, 407, 321, 393, 360, 746, 1219, 364, 40049, 462, 26318, 3247, 22732, 11, 597, 775, 909, 497, 8070, 11, 558, 30, 51517], "temperature": 0.0, "avg_logprob": -0.22950046578633418, "compression_ratio": 1.5794392523364487, "no_speech_prob": 0.0018968948861584067}, {"id": 1011, "seek": 441648, "start": 4439.54, "end": 4444.599999999999, "text": " So we basically do the denoising in the usual way, but then we also add some RAND.", "tokens": [51517, 407, 321, 1936, 360, 264, 1441, 78, 3436, 294, 264, 7713, 636, 11, 457, 550, 321, 611, 909, 512, 497, 8070, 13, 51770], "temperature": 0.0, "avg_logprob": -0.22950046578633418, "compression_ratio": 1.5794392523364487, "no_speech_prob": 0.0018968948861584067}, {"id": 1012, "seek": 444460, "start": 4444.72, "end": 4448.620000000001, "text": " So what we do need to make sure is, given that we're adding a certain amount of randomness,", "tokens": [50370, 407, 437, 321, 360, 643, 281, 652, 988, 307, 11, 2212, 300, 321, 434, 5127, 257, 1629, 2372, 295, 4974, 1287, 11, 50565], "temperature": 0.0, "avg_logprob": -0.22210250127883185, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0013044739607721567}, {"id": 1013, "seek": 444460, "start": 4448.620000000001, "end": 4454.620000000001, "text": " we need to remove that amount of randomness from the step that we take.", "tokens": [50565, 321, 643, 281, 4159, 300, 2372, 295, 4974, 1287, 490, 264, 1823, 300, 321, 747, 13, 50865], "temperature": 0.0, "avg_logprob": -0.22210250127883185, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0013044739607721567}, {"id": 1014, "seek": 444460, "start": 4454.620000000001, "end": 4462.620000000001, "text": " So I won't go into the details, but basically there's a way of calculating how much new", "tokens": [50865, 407, 286, 1582, 380, 352, 666, 264, 4365, 11, 457, 1936, 456, 311, 257, 636, 295, 28258, 577, 709, 777, 51265], "temperature": 0.0, "avg_logprob": -0.22210250127883185, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0013044739607721567}, {"id": 1015, "seek": 444460, "start": 4462.620000000001, "end": 4469.14, "text": " randomness and how much just going back in the existing direction do we do.", "tokens": [51265, 4974, 1287, 293, 577, 709, 445, 516, 646, 294, 264, 6741, 3513, 360, 321, 360, 13, 51591], "temperature": 0.0, "avg_logprob": -0.22210250127883185, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0013044739607721567}, {"id": 1016, "seek": 444460, "start": 4469.14, "end": 4473.660000000001, "text": " And so there's the amount in the existing direction, and there's the amount in the new", "tokens": [51591, 400, 370, 456, 311, 264, 2372, 294, 264, 6741, 3513, 11, 293, 456, 311, 264, 2372, 294, 264, 777, 51817], "temperature": 0.0, "avg_logprob": -0.22210250127883185, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.0013044739607721567}, {"id": 1017, "seek": 447366, "start": 4473.72, "end": 4475.28, "text": " random direction.", "tokens": [50367, 4974, 3513, 13, 50445], "temperature": 0.0, "avg_logprob": -0.27288136537047636, "compression_ratio": 1.6594594594594594, "no_speech_prob": 0.0026316416915506124}, {"id": 1018, "seek": 447366, "start": 4475.28, "end": 4484.08, "text": " And you can just pass in eta, which is just going to, when we pass it into here, is going", "tokens": [50445, 400, 291, 393, 445, 1320, 294, 32415, 11, 597, 307, 445, 516, 281, 11, 562, 321, 1320, 309, 666, 510, 11, 307, 516, 50885], "temperature": 0.0, "avg_logprob": -0.27288136537047636, "compression_ratio": 1.6594594594594594, "no_speech_prob": 0.0026316416915506124}, {"id": 1019, "seek": 447366, "start": 4484.08, "end": 4488.5599999999995, "text": " to scale that.", "tokens": [50885, 281, 4373, 300, 13, 51109], "temperature": 0.0, "avg_logprob": -0.27288136537047636, "compression_ratio": 1.6594594594594594, "no_speech_prob": 0.0026316416915506124}, {"id": 1020, "seek": 447366, "start": 4488.5599999999995, "end": 4494.38, "text": " So if we scale it by half, so basically half of it is new noise and half of it is going", "tokens": [51109, 407, 498, 321, 4373, 309, 538, 1922, 11, 370, 1936, 1922, 295, 309, 307, 777, 5658, 293, 1922, 295, 309, 307, 516, 51400], "temperature": 0.0, "avg_logprob": -0.27288136537047636, "compression_ratio": 1.6594594594594594, "no_speech_prob": 0.0026316416915506124}, {"id": 1021, "seek": 447366, "start": 4494.38, "end": 4499.84, "text": " in the direction that we thought we should go, that makes it better still.", "tokens": [51400, 294, 264, 3513, 300, 321, 1194, 321, 820, 352, 11, 300, 1669, 309, 1101, 920, 13, 51673], "temperature": 0.0, "avg_logprob": -0.27288136537047636, "compression_ratio": 1.6594594594594594, "no_speech_prob": 0.0026316416915506124}, {"id": 1022, "seek": 447366, "start": 4499.84, "end": 4502.8, "text": " Again with 100 steps.", "tokens": [51673, 3764, 365, 2319, 4439, 13, 51821], "temperature": 0.0, "avg_logprob": -0.27288136537047636, "compression_ratio": 1.6594594594594594, "no_speech_prob": 0.0026316416915506124}, {"id": 1023, "seek": 450280, "start": 4502.9400000000005, "end": 4505.38, "text": " Just make sure I'm comparing to the same.", "tokens": [50371, 1449, 652, 988, 286, 478, 15763, 281, 264, 912, 13, 50493], "temperature": 0.0, "avg_logprob": -0.3862988194332847, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.002182411728426814}, {"id": 1024, "seek": 450280, "start": 4505.38, "end": 4506.38, "text": " Yep, 100 steps.", "tokens": [50493, 7010, 11, 2319, 4439, 13, 50543], "temperature": 0.0, "avg_logprob": -0.3862988194332847, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.002182411728426814}, {"id": 1025, "seek": 450280, "start": 4506.38, "end": 4509.38, "text": " Okay, so that's fair, like with like.", "tokens": [50543, 1033, 11, 370, 300, 311, 3143, 11, 411, 365, 411, 13, 50693], "temperature": 0.0, "avg_logprob": -0.3862988194332847, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.002182411728426814}, {"id": 1026, "seek": 450280, "start": 4509.38, "end": 4513.14, "text": " Okay, so that's adding a bit of extra noise.", "tokens": [50693, 1033, 11, 370, 300, 311, 5127, 257, 857, 295, 2857, 5658, 13, 50881], "temperature": 0.0, "avg_logprob": -0.3862988194332847, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.002182411728426814}, {"id": 1027, "seek": 450280, "start": 4513.14, "end": 4520.06, "text": " Now then, something that I think we might have mentioned back in the first lesson of", "tokens": [50881, 823, 550, 11, 746, 300, 286, 519, 321, 1062, 362, 2835, 646, 294, 264, 700, 6898, 295, 51227], "temperature": 0.0, "avg_logprob": -0.3862988194332847, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.002182411728426814}, {"id": 1028, "seek": 450280, "start": 4520.06, "end": 4530.18, "text": " this part is something called Heun's method.", "tokens": [51227, 341, 644, 307, 746, 1219, 634, 409, 311, 3170, 13, 51733], "temperature": 0.0, "avg_logprob": -0.3862988194332847, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.002182411728426814}, {"id": 1029, "seek": 453018, "start": 4530.200000000001, "end": 4535.400000000001, "text": " And Heun's method does something which we can pictorially see here to decide where to", "tokens": [50365, 400, 634, 409, 311, 3170, 775, 746, 597, 321, 393, 2317, 284, 2270, 536, 510, 281, 4536, 689, 281, 50625], "temperature": 0.0, "avg_logprob": -0.32422542572021484, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.014501725323498249}, {"id": 1030, "seek": 453018, "start": 4535.400000000001, "end": 4540.16, "text": " go, which is basically we say, okay, where are we right now?", "tokens": [50625, 352, 11, 597, 307, 1936, 321, 584, 11, 1392, 11, 689, 366, 321, 558, 586, 30, 50863], "temperature": 0.0, "avg_logprob": -0.32422542572021484, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.014501725323498249}, {"id": 1031, "seek": 453018, "start": 4540.16, "end": 4544.62, "text": " What's the, you know, at our current point, what's the direction?", "tokens": [50863, 708, 311, 264, 11, 291, 458, 11, 412, 527, 2190, 935, 11, 437, 311, 264, 3513, 30, 51086], "temperature": 0.0, "avg_logprob": -0.32422542572021484, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.014501725323498249}, {"id": 1032, "seek": 453018, "start": 4544.62, "end": 4547.200000000001, "text": " So we take the tangent line, the slope, right?", "tokens": [51086, 407, 321, 747, 264, 27747, 1622, 11, 264, 13525, 11, 558, 30, 51215], "temperature": 0.0, "avg_logprob": -0.32422542572021484, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.014501725323498249}, {"id": 1033, "seek": 453018, "start": 4547.200000000001, "end": 4549.6, "text": " That's basically all it does is it takes the slope.", "tokens": [51215, 663, 311, 1936, 439, 309, 775, 307, 309, 2516, 264, 13525, 13, 51335], "temperature": 0.0, "avg_logprob": -0.32422542572021484, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.014501725323498249}, {"id": 1034, "seek": 453018, "start": 4549.6, "end": 4552.96, "text": " So it's, oh, here's the slope, you know.", "tokens": [51335, 407, 309, 311, 11, 1954, 11, 510, 311, 264, 13525, 11, 291, 458, 13, 51503], "temperature": 0.0, "avg_logprob": -0.32422542572021484, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.014501725323498249}, {"id": 1035, "seek": 453018, "start": 4552.96, "end": 4554.12, "text": " Okay.", "tokens": [51503, 1033, 13, 51561], "temperature": 0.0, "avg_logprob": -0.32422542572021484, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.014501725323498249}, {"id": 1036, "seek": 455412, "start": 4554.12, "end": 4570.34, "text": " And so if we take that slope, and that would take us to a new spot, and then at that new", "tokens": [50364, 400, 370, 498, 321, 747, 300, 13525, 11, 293, 300, 576, 747, 505, 281, 257, 777, 4008, 11, 293, 550, 412, 300, 777, 51175], "temperature": 0.0, "avg_logprob": -0.26950408672464304, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.004133529029786587}, {"id": 1037, "seek": 455412, "start": 4570.34, "end": 4576.88, "text": " spot, we can then calculate a slope at the new spot as well.", "tokens": [51175, 4008, 11, 321, 393, 550, 8873, 257, 13525, 412, 264, 777, 4008, 382, 731, 13, 51502], "temperature": 0.0, "avg_logprob": -0.26950408672464304, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.004133529029786587}, {"id": 1038, "seek": 455412, "start": 4576.88, "end": 4582.42, "text": " And at the new spot, the slope is something else.", "tokens": [51502, 400, 412, 264, 777, 4008, 11, 264, 13525, 307, 746, 1646, 13, 51779], "temperature": 0.0, "avg_logprob": -0.26950408672464304, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.004133529029786587}, {"id": 1039, "seek": 458242, "start": 4582.42, "end": 4585.54, "text": " So that's it here, right?", "tokens": [50364, 407, 300, 311, 309, 510, 11, 558, 30, 50520], "temperature": 0.0, "avg_logprob": -0.26793023459931725, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.16659529507160187}, {"id": 1040, "seek": 458242, "start": 4585.54, "end": 4590.18, "text": " And then you say like, okay, well, let's go halfway between the two.", "tokens": [50520, 400, 550, 291, 584, 411, 11, 1392, 11, 731, 11, 718, 311, 352, 15461, 1296, 264, 732, 13, 50752], "temperature": 0.0, "avg_logprob": -0.26793023459931725, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.16659529507160187}, {"id": 1041, "seek": 458242, "start": 4590.18, "end": 4592.56, "text": " And let's actually follow that line.", "tokens": [50752, 400, 718, 311, 767, 1524, 300, 1622, 13, 50871], "temperature": 0.0, "avg_logprob": -0.26793023459931725, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.16659529507160187}, {"id": 1042, "seek": 458242, "start": 4592.56, "end": 4598.14, "text": " And so basically, it's saying like, okay, each of these slopes is going to be inaccurate.", "tokens": [50871, 400, 370, 1936, 11, 309, 311, 1566, 411, 11, 1392, 11, 1184, 295, 613, 37725, 307, 516, 281, 312, 46443, 13, 51150], "temperature": 0.0, "avg_logprob": -0.26793023459931725, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.16659529507160187}, {"id": 1043, "seek": 458242, "start": 4598.14, "end": 4601.4400000000005, "text": " But what we could do is calculate the slope of where we are, the slope of where we're", "tokens": [51150, 583, 437, 321, 727, 360, 307, 8873, 264, 13525, 295, 689, 321, 366, 11, 264, 13525, 295, 689, 321, 434, 51315], "temperature": 0.0, "avg_logprob": -0.26793023459931725, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.16659529507160187}, {"id": 1044, "seek": 458242, "start": 4601.4400000000005, "end": 4604.2, "text": " going, and then go halfway between the two.", "tokens": [51315, 516, 11, 293, 550, 352, 15461, 1296, 264, 732, 13, 51453], "temperature": 0.0, "avg_logprob": -0.26793023459931725, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.16659529507160187}, {"id": 1045, "seek": 458242, "start": 4604.2, "end": 4610.6, "text": " It's, I actually find it easier to look at in code, personally.", "tokens": [51453, 467, 311, 11, 286, 767, 915, 309, 3571, 281, 574, 412, 294, 3089, 11, 5665, 13, 51773], "temperature": 0.0, "avg_logprob": -0.26793023459931725, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.16659529507160187}, {"id": 1046, "seek": 461060, "start": 4610.6, "end": 4617.22, "text": " I just going to delete a whole bunch of stuff that's totally irrelevant to this conversation.", "tokens": [50364, 286, 445, 516, 281, 12097, 257, 1379, 3840, 295, 1507, 300, 311, 3879, 28682, 281, 341, 3761, 13, 50695], "temperature": 0.0, "avg_logprob": -0.40218953291575116, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.021285617724061012}, {"id": 1047, "seek": 461060, "start": 4617.22, "end": 4620.34, "text": " So take a look at this compared to Euler.", "tokens": [50695, 407, 747, 257, 574, 412, 341, 5347, 281, 462, 26318, 13, 50851], "temperature": 0.0, "avg_logprob": -0.40218953291575116, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.021285617724061012}, {"id": 1048, "seek": 461060, "start": 4620.34, "end": 4625.4800000000005, "text": " So here's our Euler, right?", "tokens": [50851, 407, 510, 311, 527, 462, 26318, 11, 558, 30, 51108], "temperature": 0.0, "avg_logprob": -0.40218953291575116, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.021285617724061012}, {"id": 1049, "seek": 461060, "start": 4625.4800000000005, "end": 4628.5, "text": " So we're going to do the same first line, exactly the same.", "tokens": [51108, 407, 321, 434, 516, 281, 360, 264, 912, 700, 1622, 11, 2293, 264, 912, 13, 51259], "temperature": 0.0, "avg_logprob": -0.40218953291575116, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.021285617724061012}, {"id": 1050, "seek": 461060, "start": 4628.5, "end": 4629.5, "text": " Right?", "tokens": [51259, 1779, 30, 51309], "temperature": 0.0, "avg_logprob": -0.40218953291575116, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.021285617724061012}, {"id": 1051, "seek": 461060, "start": 4629.5, "end": 4632.06, "text": " Then the denoising is exactly the same.", "tokens": [51309, 1396, 264, 1441, 78, 3436, 307, 2293, 264, 912, 13, 51437], "temperature": 0.0, "avg_logprob": -0.40218953291575116, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.021285617724061012}, {"id": 1052, "seek": 461060, "start": 4632.06, "end": 4633.06, "text": " Right?", "tokens": [51437, 1779, 30, 51487], "temperature": 0.0, "avg_logprob": -0.40218953291575116, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.021285617724061012}, {"id": 1053, "seek": 461060, "start": 4633.06, "end": 4635.9800000000005, "text": " And then the this step here is exactly the same.", "tokens": [51487, 400, 550, 264, 341, 1823, 510, 307, 2293, 264, 912, 13, 51633], "temperature": 0.0, "avg_logprob": -0.40218953291575116, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.021285617724061012}, {"id": 1054, "seek": 463598, "start": 4635.98, "end": 4642.5199999999995, "text": " I've actually just done it in multiple steps for no particular reason.", "tokens": [50364, 286, 600, 767, 445, 1096, 309, 294, 3866, 4439, 337, 572, 1729, 1778, 13, 50691], "temperature": 0.0, "avg_logprob": -0.3202755368989089, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.08755143731832504}, {"id": 1055, "seek": 463598, "start": 4642.5199999999995, "end": 4646.2, "text": " And then it says, okay, well, if this is the last step, then we're done.", "tokens": [50691, 400, 550, 309, 1619, 11, 1392, 11, 731, 11, 498, 341, 307, 264, 1036, 1823, 11, 550, 321, 434, 1096, 13, 50875], "temperature": 0.0, "avg_logprob": -0.3202755368989089, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.08755143731832504}, {"id": 1056, "seek": 463598, "start": 4646.2, "end": 4648.639999999999, "text": " So actually, the last step is Euler.", "tokens": [50875, 407, 767, 11, 264, 1036, 1823, 307, 462, 26318, 13, 50997], "temperature": 0.0, "avg_logprob": -0.3202755368989089, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.08755143731832504}, {"id": 1057, "seek": 463598, "start": 4648.639999999999, "end": 4653.799999999999, "text": " But then what we do is we then say, well, that's okay, for an Euler step, this is where", "tokens": [50997, 583, 550, 437, 321, 360, 307, 321, 550, 584, 11, 731, 11, 300, 311, 1392, 11, 337, 364, 462, 26318, 1823, 11, 341, 307, 689, 51255], "temperature": 0.0, "avg_logprob": -0.3202755368989089, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.08755143731832504}, {"id": 1058, "seek": 463598, "start": 4653.799999999999, "end": 4656.719999999999, "text": " we'd go.", "tokens": [51255, 321, 1116, 352, 13, 51401], "temperature": 0.0, "avg_logprob": -0.3202755368989089, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.08755143731832504}, {"id": 1059, "seek": 463598, "start": 4656.719999999999, "end": 4660.459999999999, "text": " Well, what does that look like if we denoise it?", "tokens": [51401, 1042, 11, 437, 775, 300, 574, 411, 498, 321, 1441, 38800, 309, 30, 51588], "temperature": 0.0, "avg_logprob": -0.3202755368989089, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.08755143731832504}, {"id": 1060, "seek": 463598, "start": 4660.459999999999, "end": 4662.5199999999995, "text": " So this calls the model the second time.", "tokens": [51588, 407, 341, 5498, 264, 2316, 264, 1150, 565, 13, 51691], "temperature": 0.0, "avg_logprob": -0.3202755368989089, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.08755143731832504}, {"id": 1061, "seek": 463598, "start": 4662.5199999999995, "end": 4663.5199999999995, "text": " Right?", "tokens": [51691, 1779, 30, 51741], "temperature": 0.0, "avg_logprob": -0.3202755368989089, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.08755143731832504}, {"id": 1062, "seek": 466352, "start": 4663.580000000001, "end": 4668.540000000001, "text": " So where would that take us if we took an Euler step there?", "tokens": [50367, 407, 689, 576, 300, 747, 505, 498, 321, 1890, 364, 462, 26318, 1823, 456, 30, 50615], "temperature": 0.0, "avg_logprob": -0.30732302830137054, "compression_ratio": 1.8390243902439025, "no_speech_prob": 0.14032116532325745}, {"id": 1063, "seek": 466352, "start": 4668.540000000001, "end": 4671.92, "text": " And so here, if we do an Euler step there, what's the slope?", "tokens": [50615, 400, 370, 510, 11, 498, 321, 360, 364, 462, 26318, 1823, 456, 11, 437, 311, 264, 13525, 30, 50784], "temperature": 0.0, "avg_logprob": -0.30732302830137054, "compression_ratio": 1.8390243902439025, "no_speech_prob": 0.14032116532325745}, {"id": 1064, "seek": 466352, "start": 4671.92, "end": 4677.700000000001, "text": " And so what we then do is we say, oh, okay, well, it's just, just like in the picture,", "tokens": [50784, 400, 370, 437, 321, 550, 360, 307, 321, 584, 11, 1954, 11, 1392, 11, 731, 11, 309, 311, 445, 11, 445, 411, 294, 264, 3036, 11, 51073], "temperature": 0.0, "avg_logprob": -0.30732302830137054, "compression_ratio": 1.8390243902439025, "no_speech_prob": 0.14032116532325745}, {"id": 1065, "seek": 466352, "start": 4677.700000000001, "end": 4679.26, "text": " let's take the average.", "tokens": [51073, 718, 311, 747, 264, 4274, 13, 51151], "temperature": 0.0, "avg_logprob": -0.30732302830137054, "compression_ratio": 1.8390243902439025, "no_speech_prob": 0.14032116532325745}, {"id": 1066, "seek": 466352, "start": 4679.26, "end": 4687.540000000001, "text": " Okay, so let's take the average and then use that, the step.", "tokens": [51151, 1033, 11, 370, 718, 311, 747, 264, 4274, 293, 550, 764, 300, 11, 264, 1823, 13, 51565], "temperature": 0.0, "avg_logprob": -0.30732302830137054, "compression_ratio": 1.8390243902439025, "no_speech_prob": 0.14032116532325745}, {"id": 1067, "seek": 466352, "start": 4687.540000000001, "end": 4693.320000000001, "text": " So that's all the Hewn sampler does, is it just takes the average of the slope where", "tokens": [51565, 407, 300, 311, 439, 264, 634, 895, 3247, 22732, 775, 11, 307, 309, 445, 2516, 264, 4274, 295, 264, 13525, 689, 51854], "temperature": 0.0, "avg_logprob": -0.30732302830137054, "compression_ratio": 1.8390243902439025, "no_speech_prob": 0.14032116532325745}, {"id": 1068, "seek": 469332, "start": 4694.12, "end": 4698.34, "text": " we're at and the slope where the Euler method would have taken us.", "tokens": [50404, 321, 434, 412, 293, 264, 13525, 689, 264, 462, 26318, 3170, 576, 362, 2726, 505, 13, 50615], "temperature": 0.0, "avg_logprob": -0.24375693981464092, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00831559021025896}, {"id": 1069, "seek": 469332, "start": 4698.34, "end": 4703.12, "text": " And so if we, now, so notice that it caught the model twice for a single step.", "tokens": [50615, 400, 370, 498, 321, 11, 586, 11, 370, 3449, 300, 309, 5415, 264, 2316, 6091, 337, 257, 2167, 1823, 13, 50854], "temperature": 0.0, "avg_logprob": -0.24375693981464092, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00831559021025896}, {"id": 1070, "seek": 469332, "start": 4703.12, "end": 4707.42, "text": " So to be fair, since we've been taking 100 steps with Euler, we should take 50 steps", "tokens": [50854, 407, 281, 312, 3143, 11, 1670, 321, 600, 668, 1940, 2319, 4439, 365, 462, 26318, 11, 321, 820, 747, 2625, 4439, 51069], "temperature": 0.0, "avg_logprob": -0.24375693981464092, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00831559021025896}, {"id": 1071, "seek": 469332, "start": 4707.42, "end": 4708.42, "text": " with Hewn, right?", "tokens": [51069, 365, 634, 895, 11, 558, 30, 51119], "temperature": 0.0, "avg_logprob": -0.24375693981464092, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00831559021025896}, {"id": 1072, "seek": 469332, "start": 4708.42, "end": 4711.0199999999995, "text": " Because it's going to call the model twice.", "tokens": [51119, 1436, 309, 311, 516, 281, 818, 264, 2316, 6091, 13, 51249], "temperature": 0.0, "avg_logprob": -0.24375693981464092, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00831559021025896}, {"id": 1073, "seek": 469332, "start": 4711.0199999999995, "end": 4718.38, "text": " And still, that is now, whoa, we beat one, which is pretty amazing.", "tokens": [51249, 400, 920, 11, 300, 307, 586, 11, 13310, 11, 321, 4224, 472, 11, 597, 307, 1238, 2243, 13, 51617], "temperature": 0.0, "avg_logprob": -0.24375693981464092, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00831559021025896}, {"id": 1074, "seek": 469332, "start": 4718.38, "end": 4719.62, "text": " And so we could keep going.", "tokens": [51617, 400, 370, 321, 727, 1066, 516, 13, 51679], "temperature": 0.0, "avg_logprob": -0.24375693981464092, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00831559021025896}, {"id": 1075, "seek": 469332, "start": 4719.62, "end": 4720.62, "text": " Check this out.", "tokens": [51679, 6881, 341, 484, 13, 51729], "temperature": 0.0, "avg_logprob": -0.24375693981464092, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00831559021025896}, {"id": 1076, "seek": 469332, "start": 4720.62, "end": 4721.62, "text": " We can even go down to 20.", "tokens": [51729, 492, 393, 754, 352, 760, 281, 945, 13, 51779], "temperature": 0.0, "avg_logprob": -0.24375693981464092, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00831559021025896}, {"id": 1077, "seek": 472162, "start": 4721.62, "end": 4726.48, "text": " We're doing 40 model evaluations, and this is better than our best Euler, which is pretty", "tokens": [50364, 492, 434, 884, 3356, 2316, 43085, 11, 293, 341, 307, 1101, 813, 527, 1151, 462, 26318, 11, 597, 307, 1238, 50607], "temperature": 0.0, "avg_logprob": -0.31475505632223544, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.6546515226364136}, {"id": 1078, "seek": 472162, "start": 4726.48, "end": 4727.48, "text": " crazy.", "tokens": [50607, 3219, 13, 50657], "temperature": 0.0, "avg_logprob": -0.31475505632223544, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.6546515226364136}, {"id": 1079, "seek": 472162, "start": 4727.48, "end": 4732.64, "text": " Now, something which you might've noticed is kind of weird about this, or kind of silly", "tokens": [50657, 823, 11, 746, 597, 291, 1062, 600, 5694, 307, 733, 295, 3657, 466, 341, 11, 420, 733, 295, 11774, 50915], "temperature": 0.0, "avg_logprob": -0.31475505632223544, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.6546515226364136}, {"id": 1080, "seek": 472162, "start": 4732.64, "end": 4739.72, "text": " about this is we're calling the model twice just in order to average them.", "tokens": [50915, 466, 341, 307, 321, 434, 5141, 264, 2316, 6091, 445, 294, 1668, 281, 4274, 552, 13, 51269], "temperature": 0.0, "avg_logprob": -0.31475505632223544, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.6546515226364136}, {"id": 1081, "seek": 472162, "start": 4739.72, "end": 4744.44, "text": " But we already have two model results, like without calling it twice, because we could", "tokens": [51269, 583, 321, 1217, 362, 732, 2316, 3542, 11, 411, 1553, 5141, 309, 6091, 11, 570, 321, 727, 51505], "temperature": 0.0, "avg_logprob": -0.31475505632223544, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.6546515226364136}, {"id": 1082, "seek": 472162, "start": 4744.44, "end": 4747.5599999999995, "text": " have just looked at the previous time step.", "tokens": [51505, 362, 445, 2956, 412, 264, 3894, 565, 1823, 13, 51661], "temperature": 0.0, "avg_logprob": -0.31475505632223544, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.6546515226364136}, {"id": 1083, "seek": 474756, "start": 4747.660000000001, "end": 4756.9400000000005, "text": " And so something called the LMS sampler does that instead.", "tokens": [50369, 400, 370, 746, 1219, 264, 441, 10288, 3247, 22732, 775, 300, 2602, 13, 50833], "temperature": 0.0, "avg_logprob": -0.27212818301453884, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.08267860859632492}, {"id": 1084, "seek": 474756, "start": 4756.9400000000005, "end": 4762.84, "text": " And so the LMS sampler, if I call it with 20, it actually literally does 20 evaluations.", "tokens": [50833, 400, 370, 264, 441, 10288, 3247, 22732, 11, 498, 286, 818, 309, 365, 945, 11, 309, 767, 3736, 775, 945, 43085, 13, 51128], "temperature": 0.0, "avg_logprob": -0.27212818301453884, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.08267860859632492}, {"id": 1085, "seek": 474756, "start": 4762.84, "end": 4767.9800000000005, "text": " And actually it beats Euler with 100 evaluations.", "tokens": [51128, 400, 767, 309, 16447, 462, 26318, 365, 2319, 43085, 13, 51385], "temperature": 0.0, "avg_logprob": -0.27212818301453884, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.08267860859632492}, {"id": 1086, "seek": 474756, "start": 4767.9800000000005, "end": 4769.96, "text": " And so LMS, I won't go into the details too much.", "tokens": [51385, 400, 370, 441, 10288, 11, 286, 1582, 380, 352, 666, 264, 4365, 886, 709, 13, 51484], "temperature": 0.0, "avg_logprob": -0.27212818301453884, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.08267860859632492}, {"id": 1087, "seek": 474756, "start": 4769.96, "end": 4772.42, "text": " It didn't actually fit into my little sampling very well.", "tokens": [51484, 467, 994, 380, 767, 3318, 666, 452, 707, 21179, 588, 731, 13, 51607], "temperature": 0.0, "avg_logprob": -0.27212818301453884, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.08267860859632492}, {"id": 1088, "seek": 474756, "start": 4772.42, "end": 4775.8, "text": " So basically largely copied and pasted the CATS code.", "tokens": [51607, 407, 1936, 11611, 25365, 293, 1791, 292, 264, 41192, 50, 3089, 13, 51776], "temperature": 0.0, "avg_logprob": -0.27212818301453884, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.08267860859632492}, {"id": 1089, "seek": 477580, "start": 4775.820000000001, "end": 4781.1, "text": " But the key thing it does is look at, it gets the current sig, sigma, it does the denoising,", "tokens": [50365, 583, 264, 2141, 551, 309, 775, 307, 574, 412, 11, 309, 2170, 264, 2190, 4556, 11, 12771, 11, 309, 775, 264, 1441, 78, 3436, 11, 50629], "temperature": 0.0, "avg_logprob": -0.29389381408691406, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.03732321411371231}, {"id": 1090, "seek": 477580, "start": 4781.1, "end": 4788.7, "text": " it calculates the slope, and it stores the slope in a list.", "tokens": [50629, 309, 4322, 1024, 264, 13525, 11, 293, 309, 9512, 264, 13525, 294, 257, 1329, 13, 51009], "temperature": 0.0, "avg_logprob": -0.29389381408691406, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.03732321411371231}, {"id": 1091, "seek": 477580, "start": 4788.7, "end": 4796.06, "text": " And then it grabs the first one from the list.", "tokens": [51009, 400, 550, 309, 30028, 264, 700, 472, 490, 264, 1329, 13, 51377], "temperature": 0.0, "avg_logprob": -0.29389381408691406, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.03732321411371231}, {"id": 1092, "seek": 477580, "start": 4796.06, "end": 4801.18, "text": " So it's kind of keeping a list of up to, in this case, four at a time.", "tokens": [51377, 407, 309, 311, 733, 295, 5145, 257, 1329, 295, 493, 281, 11, 294, 341, 1389, 11, 1451, 412, 257, 565, 13, 51633], "temperature": 0.0, "avg_logprob": -0.29389381408691406, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.03732321411371231}, {"id": 1093, "seek": 480118, "start": 4801.18, "end": 4809.860000000001, "text": " And so it then uses up to the last four to basically, yes, look at the curvature of this", "tokens": [50364, 400, 370, 309, 550, 4960, 493, 281, 264, 1036, 1451, 281, 1936, 11, 2086, 11, 574, 412, 264, 37638, 295, 341, 50798], "temperature": 0.0, "avg_logprob": -0.27379734415403556, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.018829384818673134}, {"id": 1094, "seek": 480118, "start": 4809.860000000001, "end": 4812.62, "text": " and take the next step.", "tokens": [50798, 293, 747, 264, 958, 1823, 13, 50936], "temperature": 0.0, "avg_logprob": -0.27379734415403556, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.018829384818673134}, {"id": 1095, "seek": 480118, "start": 4812.62, "end": 4814.54, "text": " So that's pretty smart.", "tokens": [50936, 407, 300, 311, 1238, 4069, 13, 51032], "temperature": 0.0, "avg_logprob": -0.27379734415403556, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.018829384818673134}, {"id": 1096, "seek": 480118, "start": 4814.54, "end": 4825.240000000001, "text": " And yeah, so I think if you wanted to do super fast sampling, it seems like a pretty good", "tokens": [51032, 400, 1338, 11, 370, 286, 519, 498, 291, 1415, 281, 360, 1687, 2370, 21179, 11, 309, 2544, 411, 257, 1238, 665, 51567], "temperature": 0.0, "avg_logprob": -0.27379734415403556, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.018829384818673134}, {"id": 1097, "seek": 480118, "start": 4825.240000000001, "end": 4826.240000000001, "text": " way to do it.", "tokens": [51567, 636, 281, 360, 309, 13, 51617], "temperature": 0.0, "avg_logprob": -0.27379734415403556, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.018829384818673134}, {"id": 1098, "seek": 482624, "start": 4826.4, "end": 4832.179999999999, "text": " I think Giotto, you were telling me that, or maybe it was Pedro was saying that currently", "tokens": [50372, 286, 519, 460, 6471, 1353, 11, 291, 645, 3585, 385, 300, 11, 420, 1310, 309, 390, 26662, 390, 1566, 300, 4362, 50661], "temperature": 0.0, "avg_logprob": -0.3364632596674654, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.23072022199630737}, {"id": 1099, "seek": 482624, "start": 4832.179999999999, "end": 4836.04, "text": " people have started to move away, that this was very popular, but people started to move", "tokens": [50661, 561, 362, 1409, 281, 1286, 1314, 11, 300, 341, 390, 588, 3743, 11, 457, 561, 1409, 281, 1286, 50854], "temperature": 0.0, "avg_logprob": -0.3364632596674654, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.23072022199630737}, {"id": 1100, "seek": 482624, "start": 4836.04, "end": 4842.8, "text": " towards a new sampler, which is a bit similar called the DPM++ sampler, something like that.", "tokens": [50854, 3030, 257, 777, 3247, 22732, 11, 597, 307, 257, 857, 2531, 1219, 264, 413, 18819, 25472, 3247, 22732, 11, 746, 411, 300, 13, 51192], "temperature": 0.0, "avg_logprob": -0.3364632596674654, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.23072022199630737}, {"id": 1101, "seek": 482624, "start": 4842.8, "end": 4843.8, "text": " Yeah.", "tokens": [51192, 865, 13, 51242], "temperature": 0.0, "avg_logprob": -0.3364632596674654, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.23072022199630737}, {"id": 1102, "seek": 482624, "start": 4843.8, "end": 4844.8, "text": " Yeah.", "tokens": [51242, 865, 13, 51292], "temperature": 0.0, "avg_logprob": -0.3364632596674654, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.23072022199630737}, {"id": 1103, "seek": 482624, "start": 4844.8, "end": 4845.8, "text": " Yeah.", "tokens": [51292, 865, 13, 51342], "temperature": 0.0, "avg_logprob": -0.3364632596674654, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.23072022199630737}, {"id": 1104, "seek": 482624, "start": 4845.8, "end": 4846.8, "text": " Yeah.", "tokens": [51342, 865, 13, 51392], "temperature": 0.0, "avg_logprob": -0.3364632596674654, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.23072022199630737}, {"id": 1105, "seek": 482624, "start": 4846.8, "end": 4847.8, "text": " But I think it's the same idea.", "tokens": [51392, 583, 286, 519, 309, 311, 264, 912, 1558, 13, 51442], "temperature": 0.0, "avg_logprob": -0.3364632596674654, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.23072022199630737}, {"id": 1106, "seek": 484780, "start": 4847.8, "end": 4856.28, "text": " I think it kind of keeps a, let's say, keep a list of recent results and use that.", "tokens": [50364, 286, 519, 309, 733, 295, 5965, 257, 11, 718, 311, 584, 11, 1066, 257, 1329, 295, 5162, 3542, 293, 764, 300, 13, 50788], "temperature": 0.0, "avg_logprob": -0.39295857127119854, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.6682482361793518}, {"id": 1107, "seek": 484780, "start": 4856.28, "end": 4857.6, "text": " I'll have to check it more closely.", "tokens": [50788, 286, 603, 362, 281, 1520, 309, 544, 8185, 13, 50854], "temperature": 0.0, "avg_logprob": -0.39295857127119854, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.6682482361793518}, {"id": 1108, "seek": 484780, "start": 4857.6, "end": 4858.6, "text": " Yeah.", "tokens": [50854, 865, 13, 50904], "temperature": 0.0, "avg_logprob": -0.39295857127119854, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.6682482361793518}, {"id": 1109, "seek": 484780, "start": 4858.6, "end": 4860.400000000001, "text": " That's a similar idea.", "tokens": [50904, 663, 311, 257, 2531, 1558, 13, 50994], "temperature": 0.0, "avg_logprob": -0.39295857127119854, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.6682482361793518}, {"id": 1110, "seek": 484780, "start": 4860.400000000001, "end": 4865.4400000000005, "text": " It's like, if it's done more than one step, then it's using some history to do the next", "tokens": [50994, 467, 311, 411, 11, 498, 309, 311, 1096, 544, 813, 472, 1823, 11, 550, 309, 311, 1228, 512, 2503, 281, 360, 264, 958, 51246], "temperature": 0.0, "avg_logprob": -0.39295857127119854, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.6682482361793518}, {"id": 1111, "seek": 484780, "start": 4865.4400000000005, "end": 4866.4400000000005, "text": " thing.", "tokens": [51246, 551, 13, 51296], "temperature": 0.0, "avg_logprob": -0.39295857127119854, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.6682482361793518}, {"id": 1112, "seek": 484780, "start": 4866.4400000000005, "end": 4867.4400000000005, "text": " Yeah.", "tokens": [51296, 865, 13, 51346], "temperature": 0.0, "avg_logprob": -0.39295857127119854, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.6682482361793518}, {"id": 1113, "seek": 484780, "start": 4867.4400000000005, "end": 4871.28, "text": " This history doesn't make a huge amount of sense, I guess, from that perspective.", "tokens": [51346, 639, 276, 468, 827, 1177, 380, 652, 257, 2603, 2372, 295, 2020, 11, 286, 2041, 11, 490, 300, 4585, 13, 51538], "temperature": 0.0, "avg_logprob": -0.39295857127119854, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.6682482361793518}, {"id": 1114, "seek": 484780, "start": 4871.28, "end": 4875.400000000001, "text": " I mean, still, it works very well.", "tokens": [51538, 286, 914, 11, 920, 11, 309, 1985, 588, 731, 13, 51744], "temperature": 0.0, "avg_logprob": -0.39295857127119854, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.6682482361793518}, {"id": 1115, "seek": 484780, "start": 4875.400000000001, "end": 4876.900000000001, "text": " This makes more sense.", "tokens": [51744, 639, 1669, 544, 2020, 13, 51819], "temperature": 0.0, "avg_logprob": -0.39295857127119854, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.6682482361793518}, {"id": 1116, "seek": 487690, "start": 4877.0, "end": 4880.86, "text": " So then, you know, we can compare if we use an actual mini-batch of data, we get about", "tokens": [50369, 407, 550, 11, 291, 458, 11, 321, 393, 6794, 498, 321, 764, 364, 3539, 8382, 12, 65, 852, 295, 1412, 11, 321, 483, 466, 50562], "temperature": 0.0, "avg_logprob": -0.31701426670469085, "compression_ratio": 1.330827067669173, "no_speech_prob": 0.0059102787636220455}, {"id": 1117, "seek": 487690, "start": 4880.86, "end": 4881.86, "text": " 0.5.", "tokens": [50562, 1958, 13, 20, 13, 50612], "temperature": 0.0, "avg_logprob": -0.31701426670469085, "compression_ratio": 1.330827067669173, "no_speech_prob": 0.0059102787636220455}, {"id": 1118, "seek": 487690, "start": 4881.86, "end": 4896.42, "text": " So yeah, I feel like this is quite a, you know, stunning result to get close to, very", "tokens": [50612, 407, 1338, 11, 286, 841, 411, 341, 307, 1596, 257, 11, 291, 458, 11, 18550, 1874, 281, 483, 1998, 281, 11, 588, 51340], "temperature": 0.0, "avg_logprob": -0.31701426670469085, "compression_ratio": 1.330827067669173, "no_speech_prob": 0.0059102787636220455}, {"id": 1119, "seek": 489642, "start": 4896.42, "end": 4907.34, "text": " close to real data, at least in terms of fit, you know, really with 40 model evaluations.", "tokens": [50364, 1998, 281, 957, 1412, 11, 412, 1935, 294, 2115, 295, 3318, 11, 291, 458, 11, 534, 365, 3356, 2316, 43085, 13, 50910], "temperature": 0.0, "avg_logprob": -0.3345462799072266, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.5308837294578552}, {"id": 1120, "seek": 489642, "start": 4907.34, "end": 4914.46, "text": " And the entire, you know, nearly the entire thing here is by making sure we've got unit", "tokens": [50910, 400, 264, 2302, 11, 291, 458, 11, 6217, 264, 2302, 551, 510, 307, 538, 1455, 988, 321, 600, 658, 4985, 51266], "temperature": 0.0, "avg_logprob": -0.3345462799072266, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.5308837294578552}, {"id": 1121, "seek": 489642, "start": 4914.46, "end": 4920.14, "text": " variance inputs, unit variance outputs, and kind of equally difficult problems to solve", "tokens": [51266, 21977, 15743, 11, 4985, 21977, 23930, 11, 293, 733, 295, 12309, 2252, 2740, 281, 5039, 51550], "temperature": 0.0, "avg_logprob": -0.3345462799072266, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.5308837294578552}, {"id": 1122, "seek": 489642, "start": 4920.14, "end": 4922.5, "text": " in our loss function.", "tokens": [51550, 294, 527, 4470, 2445, 13, 51668], "temperature": 0.0, "avg_logprob": -0.3345462799072266, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.5308837294578552}, {"id": 1123, "seek": 489642, "start": 4922.5, "end": 4923.5, "text": " Yeah.", "tokens": [51668, 865, 13, 51718], "temperature": 0.0, "avg_logprob": -0.3345462799072266, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.5308837294578552}, {"id": 1124, "seek": 492350, "start": 4923.5, "end": 4927.58, "text": " Plus having that different schedule for sampling, that's completely unrelated to the training", "tokens": [50364, 7721, 1419, 300, 819, 7567, 337, 21179, 11, 300, 311, 2584, 38967, 281, 264, 3097, 50568], "temperature": 0.0, "avg_logprob": -0.3329663407911948, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.7458916902542114}, {"id": 1125, "seek": 492350, "start": 4927.58, "end": 4928.58, "text": " schedule.", "tokens": [50568, 7567, 13, 50618], "temperature": 0.0, "avg_logprob": -0.3329663407911948, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.7458916902542114}, {"id": 1126, "seek": 492350, "start": 4928.58, "end": 4932.82, "text": " I think that was one of the big things with Kara Siddal's paper was they also could apply", "tokens": [50618, 286, 519, 300, 390, 472, 295, 264, 955, 721, 365, 34838, 318, 14273, 304, 311, 3035, 390, 436, 611, 727, 3079, 50830], "temperature": 0.0, "avg_logprob": -0.3329663407911948, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.7458916902542114}, {"id": 1127, "seek": 492350, "start": 4932.82, "end": 4937.14, "text": " this to like, oh, existing diffusion models that have been trained by other papers, we", "tokens": [50830, 341, 281, 411, 11, 1954, 11, 6741, 25242, 5245, 300, 362, 668, 8895, 538, 661, 10577, 11, 321, 51046], "temperature": 0.0, "avg_logprob": -0.3329663407911948, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.7458916902542114}, {"id": 1128, "seek": 492350, "start": 4937.14, "end": 4943.74, "text": " can use our sampler and in fewer steps get better results without any of the other changes.", "tokens": [51046, 393, 764, 527, 3247, 22732, 293, 294, 13366, 4439, 483, 1101, 3542, 1553, 604, 295, 264, 661, 2962, 13, 51376], "temperature": 0.0, "avg_logprob": -0.3329663407911948, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.7458916902542114}, {"id": 1129, "seek": 492350, "start": 4943.74, "end": 4948.26, "text": " And yeah, I mean, they do a little bit of rearranging equations to get the other papers", "tokens": [51376, 400, 1338, 11, 286, 914, 11, 436, 360, 257, 707, 857, 295, 29875, 9741, 11787, 281, 483, 264, 661, 10577, 51602], "temperature": 0.0, "avg_logprob": -0.3329663407911948, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.7458916902542114}, {"id": 1130, "seek": 494826, "start": 4948.26, "end": 4954.26, "text": " versions into their C-skip, C-in, C-out framework.", "tokens": [50364, 9606, 666, 641, 383, 12, 5161, 647, 11, 383, 12, 259, 11, 383, 12, 346, 8388, 13, 50664], "temperature": 0.0, "avg_logprob": -0.31654236533425073, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.8738369345664978}, {"id": 1131, "seek": 494826, "start": 4954.26, "end": 4958.66, "text": " But then yeah, it's really nice that these ideas can be applied to, so for example, I", "tokens": [50664, 583, 550, 1338, 11, 309, 311, 534, 1481, 300, 613, 3487, 393, 312, 6456, 281, 11, 370, 337, 1365, 11, 286, 50884], "temperature": 0.0, "avg_logprob": -0.31654236533425073, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.8738369345664978}, {"id": 1132, "seek": 494826, "start": 4958.66, "end": 4964.74, "text": " think stable diffusion, especially version one was trained DDPM style training, epsilon", "tokens": [50884, 519, 8351, 25242, 11, 2318, 3037, 472, 390, 8895, 413, 11373, 44, 3758, 3097, 11, 17889, 51188], "temperature": 0.0, "avg_logprob": -0.31654236533425073, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.8738369345664978}, {"id": 1133, "seek": 494826, "start": 4964.74, "end": 4967.1, "text": " objective, whatever.", "tokens": [51188, 10024, 11, 2035, 13, 51306], "temperature": 0.0, "avg_logprob": -0.31654236533425073, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.8738369345664978}, {"id": 1134, "seek": 494826, "start": 4967.1, "end": 4971.1, "text": " But you can now get these different samplers and different sampling schedules and things", "tokens": [51306, 583, 291, 393, 586, 483, 613, 819, 3247, 564, 433, 293, 819, 21179, 28078, 293, 721, 51506], "temperature": 0.0, "avg_logprob": -0.31654236533425073, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.8738369345664978}, {"id": 1135, "seek": 494826, "start": 4971.1, "end": 4977.58, "text": " like that and use that to sample it and do it in 10, 15, 20 steps and get pretty nice", "tokens": [51506, 411, 300, 293, 764, 300, 281, 6889, 309, 293, 360, 309, 294, 1266, 11, 2119, 11, 945, 4439, 293, 483, 1238, 1481, 51830], "temperature": 0.0, "avg_logprob": -0.31654236533425073, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.8738369345664978}, {"id": 1136, "seek": 497758, "start": 4977.58, "end": 4978.58, "text": " results.", "tokens": [50364, 3542, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2935040092468262, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.03113562986254692}, {"id": 1137, "seek": 497758, "start": 4978.58, "end": 4979.58, "text": " Yeah.", "tokens": [50414, 865, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2935040092468262, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.03113562986254692}, {"id": 1138, "seek": 497758, "start": 4979.58, "end": 4988.5, "text": " You know, and another nice thing about this paper is they, you know, in fact, it's the", "tokens": [50464, 509, 458, 11, 293, 1071, 1481, 551, 466, 341, 3035, 307, 436, 11, 291, 458, 11, 294, 1186, 11, 309, 311, 264, 50910], "temperature": 0.0, "avg_logprob": -0.2935040092468262, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.03113562986254692}, {"id": 1139, "seek": 497758, "start": 4988.5, "end": 4994.5, "text": " name of the paper, elucidating the design space of diffusion based models.", "tokens": [50910, 1315, 295, 264, 3035, 11, 806, 1311, 327, 990, 264, 1715, 1901, 295, 25242, 2361, 5245, 13, 51210], "temperature": 0.0, "avg_logprob": -0.2935040092468262, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.03113562986254692}, {"id": 1140, "seek": 497758, "start": 4994.5, "end": 4997.94, "text": " You know, they looked at various different papers and approaches and kind of said like,", "tokens": [51210, 509, 458, 11, 436, 2956, 412, 3683, 819, 10577, 293, 11587, 293, 733, 295, 848, 411, 11, 51382], "temperature": 0.0, "avg_logprob": -0.2935040092468262, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.03113562986254692}, {"id": 1141, "seek": 497758, "start": 4997.94, "end": 5004.58, "text": " oh, you know what, these are all doing the same thing when we kind of parameterize things", "tokens": [51382, 1954, 11, 291, 458, 437, 11, 613, 366, 439, 884, 264, 912, 551, 562, 321, 733, 295, 13075, 1125, 721, 51714], "temperature": 0.0, "avg_logprob": -0.2935040092468262, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.03113562986254692}, {"id": 1142, "seek": 497758, "start": 5004.58, "end": 5005.58, "text": " in this way.", "tokens": [51714, 294, 341, 636, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2935040092468262, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.03113562986254692}, {"id": 1143, "seek": 500558, "start": 5005.58, "end": 5008.26, "text": " You know, you fill in these parameters, you get this paper and these parameters, you get", "tokens": [50364, 509, 458, 11, 291, 2836, 294, 613, 9834, 11, 291, 483, 341, 3035, 293, 613, 9834, 11, 291, 483, 50498], "temperature": 0.0, "avg_logprob": -0.3197593416486468, "compression_ratio": 1.735483870967742, "no_speech_prob": 0.3276031017303467}, {"id": 1144, "seek": 500558, "start": 5008.26, "end": 5017.3, "text": " that paper, you know, and then so we found a better set of parameters, which it was very", "tokens": [50498, 300, 3035, 11, 291, 458, 11, 293, 550, 370, 321, 1352, 257, 1101, 992, 295, 9834, 11, 597, 309, 390, 588, 50950], "temperature": 0.0, "avg_logprob": -0.3197593416486468, "compression_ratio": 1.735483870967742, "no_speech_prob": 0.3276031017303467}, {"id": 1145, "seek": 500558, "start": 5017.3, "end": 5026.9, "text": " nice to code because, you know, it really actually ended up simplifying things a whole", "tokens": [50950, 1481, 281, 3089, 570, 11, 291, 458, 11, 309, 534, 767, 4590, 493, 6883, 5489, 721, 257, 1379, 51430], "temperature": 0.0, "avg_logprob": -0.3197593416486468, "compression_ratio": 1.735483870967742, "no_speech_prob": 0.3276031017303467}, {"id": 1146, "seek": 500558, "start": 5026.9, "end": 5027.9, "text": " lot.", "tokens": [51430, 688, 13, 51480], "temperature": 0.0, "avg_logprob": -0.3197593416486468, "compression_ratio": 1.735483870967742, "no_speech_prob": 0.3276031017303467}, {"id": 1147, "seek": 502790, "start": 5027.9, "end": 5035.62, "text": " So if you look through the notebook carefully, which I hope everybody will, you'll see, you", "tokens": [50364, 407, 498, 291, 574, 807, 264, 21060, 7500, 11, 597, 286, 1454, 2201, 486, 11, 291, 603, 536, 11, 291, 50750], "temperature": 0.0, "avg_logprob": -0.3291489124298096, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.4608469307422638}, {"id": 1148, "seek": 502790, "start": 5035.62, "end": 5043.78, "text": " know, that the code is really there and simple compared to previous, all the previous ones,", "tokens": [50750, 458, 11, 300, 264, 3089, 307, 534, 456, 293, 2199, 5347, 281, 3894, 11, 439, 264, 3894, 2306, 11, 51158], "temperature": 0.0, "avg_logprob": -0.3291489124298096, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.4608469307422638}, {"id": 1149, "seek": 502790, "start": 5043.78, "end": 5044.78, "text": " in my opinion.", "tokens": [51158, 294, 452, 4800, 13, 51208], "temperature": 0.0, "avg_logprob": -0.3291489124298096, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.4608469307422638}, {"id": 1150, "seek": 502790, "start": 5044.78, "end": 5050.78, "text": " Like I feel like every notebook we've done from DDPM onwards, the code's got easier to", "tokens": [51208, 1743, 286, 841, 411, 633, 21060, 321, 600, 1096, 490, 413, 11373, 44, 34230, 11, 264, 3089, 311, 658, 3571, 281, 51508], "temperature": 0.0, "avg_logprob": -0.3291489124298096, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.4608469307422638}, {"id": 1151, "seek": 502790, "start": 5050.78, "end": 5056.0199999999995, "text": " understand and results.", "tokens": [51508, 1223, 293, 3542, 13, 51770], "temperature": 0.0, "avg_logprob": -0.3291489124298096, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.4608469307422638}, {"id": 1152, "seek": 505602, "start": 5056.14, "end": 5059.26, "text": " And just to again clarify, like how this connects with some of the previous papers that we've", "tokens": [50370, 400, 445, 281, 797, 17594, 11, 411, 577, 341, 16967, 365, 512, 295, 264, 3894, 10577, 300, 321, 600, 50526], "temperature": 0.0, "avg_logprob": -0.28318807983398436, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.25670963525772095}, {"id": 1153, "seek": 505602, "start": 5059.26, "end": 5060.26, "text": " looked at.", "tokens": [50526, 2956, 412, 13, 50576], "temperature": 0.0, "avg_logprob": -0.28318807983398436, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.25670963525772095}, {"id": 1154, "seek": 505602, "start": 5060.26, "end": 5066.42, "text": " So like, for example, with the BDIM, the deterministic, that's again, this sort of deterministic approach", "tokens": [50576, 407, 411, 11, 337, 1365, 11, 365, 264, 363, 3085, 44, 11, 264, 15957, 3142, 11, 300, 311, 797, 11, 341, 1333, 295, 15957, 3142, 3109, 50884], "temperature": 0.0, "avg_logprob": -0.28318807983398436, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.25670963525772095}, {"id": 1155, "seek": 505602, "start": 5066.42, "end": 5072.1, "text": " that's similar to the Euler method sampler that we were just looking at, which was completely", "tokens": [50884, 300, 311, 2531, 281, 264, 462, 26318, 3170, 3247, 22732, 300, 321, 645, 445, 1237, 412, 11, 597, 390, 2584, 51168], "temperature": 0.0, "avg_logprob": -0.28318807983398436, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.25670963525772095}, {"id": 1156, "seek": 505602, "start": 5072.1, "end": 5073.860000000001, "text": " deterministic.", "tokens": [51168, 15957, 3142, 13, 51256], "temperature": 0.0, "avg_logprob": -0.28318807983398436, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.25670963525772095}, {"id": 1157, "seek": 505602, "start": 5073.860000000001, "end": 5078.14, "text": " And then some of something like the Euler ancestral that we were looking at is similar", "tokens": [51256, 400, 550, 512, 295, 746, 411, 264, 462, 26318, 40049, 300, 321, 645, 1237, 412, 307, 2531, 51470], "temperature": 0.0, "avg_logprob": -0.28318807983398436, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.25670963525772095}, {"id": 1158, "seek": 505602, "start": 5078.14, "end": 5085.620000000001, "text": " to the standard DDPM approach with the, that was kind of a more stochastic approach.", "tokens": [51470, 281, 264, 3832, 413, 11373, 44, 3109, 365, 264, 11, 300, 390, 733, 295, 257, 544, 342, 8997, 2750, 3109, 13, 51844], "temperature": 0.0, "avg_logprob": -0.28318807983398436, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.25670963525772095}, {"id": 1159, "seek": 508562, "start": 5086.22, "end": 5089.9, "text": " So again, there's just all these sorts of connections that then are kind of nice to", "tokens": [50394, 407, 797, 11, 456, 311, 445, 439, 613, 7527, 295, 9271, 300, 550, 366, 733, 295, 1481, 281, 50578], "temperature": 0.0, "avg_logprob": -0.3896921530537222, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0004238417313899845}, {"id": 1160, "seek": 508562, "start": 5089.9, "end": 5093.74, "text": " see that, again, the sorts of connections between the different papers and how they", "tokens": [50578, 536, 300, 11, 797, 11, 264, 7527, 295, 9271, 1296, 264, 819, 10577, 293, 577, 436, 50770], "temperature": 0.0, "avg_logprob": -0.3896921530537222, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0004238417313899845}, {"id": 1161, "seek": 508562, "start": 5093.74, "end": 5097.78, "text": " change it, how they can be expressed in this common framework.", "tokens": [50770, 1319, 309, 11, 577, 436, 393, 312, 12675, 294, 341, 2689, 8388, 13, 50972], "temperature": 0.0, "avg_logprob": -0.3896921530537222, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0004238417313899845}, {"id": 1162, "seek": 508562, "start": 5097.78, "end": 5099.38, "text": " Yeah.", "tokens": [50972, 865, 13, 51052], "temperature": 0.0, "avg_logprob": -0.3896921530537222, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0004238417313899845}, {"id": 1163, "seek": 508562, "start": 5099.38, "end": 5102.0599999999995, "text": " Thanks, Tanish.", "tokens": [51052, 2561, 11, 314, 7524, 13, 51186], "temperature": 0.0, "avg_logprob": -0.3896921530537222, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0004238417313899845}, {"id": 1164, "seek": 508562, "start": 5102.0599999999995, "end": 5111.94, "text": " So we definitely now are at the point where we can show you the UNet next time.", "tokens": [51186, 407, 321, 2138, 586, 366, 412, 264, 935, 689, 321, 393, 855, 291, 264, 8229, 302, 958, 565, 13, 51680], "temperature": 0.0, "avg_logprob": -0.3896921530537222, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0004238417313899845}, {"id": 1165, "seek": 511194, "start": 5112.0599999999995, "end": 5123.62, "text": " So I think we're, unless any of us come up with interesting new insights on the unconditional", "tokens": [50370, 407, 286, 519, 321, 434, 11, 5969, 604, 295, 505, 808, 493, 365, 1880, 777, 14310, 322, 264, 47916, 50948], "temperature": 0.0, "avg_logprob": -0.2620985545809307, "compression_ratio": 1.4623655913978495, "no_speech_prob": 0.014272775501012802}, {"id": 1166, "seek": 511194, "start": 5123.62, "end": 5130.259999999999, "text": " diffusion sampling training and sampling process, we might be putting that aside for a while.", "tokens": [50948, 25242, 21179, 3097, 293, 21179, 1399, 11, 321, 1062, 312, 3372, 300, 7359, 337, 257, 1339, 13, 51280], "temperature": 0.0, "avg_logprob": -0.2620985545809307, "compression_ratio": 1.4623655913978495, "no_speech_prob": 0.014272775501012802}, {"id": 1167, "seek": 511194, "start": 5130.259999999999, "end": 5139.419999999999, "text": " And instead, we're going to be looking at creating a good quality UNet from scratch.", "tokens": [51280, 400, 2602, 11, 321, 434, 516, 281, 312, 1237, 412, 4084, 257, 665, 3125, 8229, 302, 490, 8459, 13, 51738], "temperature": 0.0, "avg_logprob": -0.2620985545809307, "compression_ratio": 1.4623655913978495, "no_speech_prob": 0.014272775501012802}, {"id": 1168, "seek": 513942, "start": 5139.42, "end": 5144.22, "text": " And we're going to look at a different dataset to do that.", "tokens": [50364, 400, 321, 434, 516, 281, 574, 412, 257, 819, 28872, 281, 360, 300, 13, 50604], "temperature": 0.0, "avg_logprob": -0.2870400679738898, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.045344848185777664}, {"id": 1169, "seek": 513942, "start": 5144.22, "end": 5150.02, "text": " As we're starting to scale things up a bit, as Jono mentioned in the last lesson.", "tokens": [50604, 1018, 321, 434, 2891, 281, 4373, 721, 493, 257, 857, 11, 382, 7745, 78, 2835, 294, 264, 1036, 6898, 13, 50894], "temperature": 0.0, "avg_logprob": -0.2870400679738898, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.045344848185777664}, {"id": 1170, "seek": 513942, "start": 5150.02, "end": 5157.78, "text": " So we're going to be using a 64 by 64 pixel ImageNet subset, or TinyImageNet.", "tokens": [50894, 407, 321, 434, 516, 281, 312, 1228, 257, 12145, 538, 12145, 19261, 29903, 31890, 25993, 11, 420, 39992, 31128, 609, 31890, 13, 51282], "temperature": 0.0, "avg_logprob": -0.2870400679738898, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.045344848185777664}, {"id": 1171, "seek": 513942, "start": 5157.78, "end": 5161.78, "text": " So we'll start looking at some three channel images.", "tokens": [51282, 407, 321, 603, 722, 1237, 412, 512, 1045, 2269, 5267, 13, 51482], "temperature": 0.0, "avg_logprob": -0.2870400679738898, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.045344848185777664}, {"id": 1172, "seek": 513942, "start": 5161.78, "end": 5166.5, "text": " So I'm sure we're all sick of looking at black and white shoes.", "tokens": [51482, 407, 286, 478, 988, 321, 434, 439, 4998, 295, 1237, 412, 2211, 293, 2418, 6654, 13, 51718], "temperature": 0.0, "avg_logprob": -0.2870400679738898, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.045344848185777664}, {"id": 1173, "seek": 516650, "start": 5166.58, "end": 5177.94, "text": " So now we get to look at shift dwellings and trolley buses and koala bears and yeah, 200", "tokens": [50368, 407, 586, 321, 483, 281, 574, 412, 5513, 24355, 1109, 293, 20680, 2030, 20519, 293, 8384, 5159, 17276, 293, 1338, 11, 2331, 50936], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1174, "seek": 516650, "start": 5177.94, "end": 5178.94, "text": " different things.", "tokens": [50936, 819, 721, 13, 50986], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1175, "seek": 516650, "start": 5178.94, "end": 5179.94, "text": " So that'll be nice.", "tokens": [50986, 407, 300, 603, 312, 1481, 13, 51036], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1176, "seek": 516650, "start": 5179.94, "end": 5180.94, "text": " Yeah.", "tokens": [51036, 865, 13, 51086], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1177, "seek": 516650, "start": 5180.94, "end": 5181.94, "text": " All right.", "tokens": [51086, 1057, 558, 13, 51136], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1178, "seek": 516650, "start": 5181.94, "end": 5182.94, "text": " Well, thank you, Jono.", "tokens": [51136, 1042, 11, 1309, 291, 11, 7745, 78, 13, 51186], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1179, "seek": 516650, "start": 5182.94, "end": 5183.94, "text": " Thank you, Tanish.", "tokens": [51186, 1044, 291, 11, 314, 7524, 13, 51236], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1180, "seek": 516650, "start": 5183.94, "end": 5184.94, "text": " That was fun, as always.", "tokens": [51236, 663, 390, 1019, 11, 382, 1009, 13, 51286], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1181, "seek": 516650, "start": 5184.94, "end": 5185.94, "text": " And yeah, next time will be lesson 22.", "tokens": [51286, 400, 1338, 11, 958, 565, 486, 312, 6898, 5853, 13, 51336], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1182, "seek": 516650, "start": 5185.94, "end": 5186.94, "text": " Bye.", "tokens": [51336, 4621, 13, 51386], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1183, "seek": 516650, "start": 5186.94, "end": 5187.94, "text": " Lesson 22.", "tokens": [51386, 18649, 266, 5853, 13, 51436], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1184, "seek": 516650, "start": 5187.94, "end": 5188.94, "text": " This was lesson 22.", "tokens": [51436, 639, 390, 6898, 5853, 13, 51486], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1185, "seek": 516650, "start": 5188.94, "end": 5189.94, "text": " Oh, no way.", "tokens": [51486, 876, 11, 572, 636, 13, 51536], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1186, "seek": 516650, "start": 5189.94, "end": 5190.94, "text": " Okay.", "tokens": [51536, 1033, 13, 51586], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1187, "seek": 516650, "start": 5190.94, "end": 5191.94, "text": " You're right.", "tokens": [51586, 509, 434, 558, 13, 51636], "temperature": 0.0, "avg_logprob": -0.31020834426249355, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.02056254632771015}, {"id": 1188, "seek": 519194, "start": 5191.94, "end": 5192.94, "text": " See ya.", "tokens": [50364, 3008, 2478, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1189, "seek": 519194, "start": 5192.94, "end": 5193.94, "text": " Bye.", "tokens": [50414, 4621, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1190, "seek": 519194, "start": 5193.94, "end": 5194.94, "text": " Bye.", "tokens": [50464, 4621, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1191, "seek": 519194, "start": 5194.94, "end": 5195.94, "text": " Bye.", "tokens": [50514, 4621, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1192, "seek": 519194, "start": 5195.94, "end": 5196.94, "text": " Bye.", "tokens": [50564, 4621, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1193, "seek": 519194, "start": 5196.94, "end": 5197.94, "text": " Bye.", "tokens": [50614, 4621, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1194, "seek": 519194, "start": 5197.94, "end": 5198.94, "text": " Bye.", "tokens": [50664, 4621, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1195, "seek": 519194, "start": 5198.94, "end": 5199.94, "text": " Bye.", "tokens": [50714, 4621, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1196, "seek": 519194, "start": 5199.94, "end": 5200.94, "text": " Bye.", "tokens": [50764, 4621, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1197, "seek": 519194, "start": 5200.94, "end": 5201.94, "text": " Bye.", "tokens": [50814, 4621, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1198, "seek": 519194, "start": 5201.94, "end": 5202.94, "text": " Bye.", "tokens": [50864, 4621, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1199, "seek": 519194, "start": 5202.94, "end": 5203.94, "text": " Bye.", "tokens": [50914, 4621, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1200, "seek": 519194, "start": 5203.94, "end": 5204.94, "text": " Bye.", "tokens": [50964, 4621, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1201, "seek": 519194, "start": 5204.94, "end": 5205.94, "text": " Bye.", "tokens": [51014, 4621, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1202, "seek": 519194, "start": 5205.94, "end": 5206.94, "text": " Bye.", "tokens": [51064, 4621, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1203, "seek": 519194, "start": 5206.94, "end": 5207.94, "text": " Bye.", "tokens": [51114, 4621, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1204, "seek": 519194, "start": 5207.94, "end": 5208.94, "text": " Bye.", "tokens": [51164, 4621, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1205, "seek": 519194, "start": 5208.94, "end": 5209.94, "text": " Bye.", "tokens": [51214, 4621, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1206, "seek": 519194, "start": 5209.94, "end": 5210.94, "text": " Bye.", "tokens": [51264, 4621, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1207, "seek": 519194, "start": 5210.94, "end": 5211.94, "text": " Bye.", "tokens": [51314, 4621, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1208, "seek": 519194, "start": 5211.94, "end": 5212.94, "text": " Bye.", "tokens": [51364, 4621, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1209, "seek": 519194, "start": 5212.94, "end": 5213.94, "text": " Bye.", "tokens": [51414, 4621, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1210, "seek": 519194, "start": 5213.94, "end": 5214.94, "text": " Bye.", "tokens": [51464, 4621, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1211, "seek": 519194, "start": 5214.94, "end": 5215.94, "text": " Bye.", "tokens": [51514, 4621, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1212, "seek": 519194, "start": 5215.94, "end": 5216.94, "text": " Bye.", "tokens": [51564, 4621, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1213, "seek": 519194, "start": 5216.94, "end": 5217.94, "text": " Bye.", "tokens": [51614, 4621, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1214, "seek": 519194, "start": 5217.94, "end": 5218.94, "text": " Bye.", "tokens": [51664, 4621, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1215, "seek": 519194, "start": 5218.94, "end": 5219.94, "text": " Bye.", "tokens": [51714, 4621, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}, {"id": 1216, "seek": 519194, "start": 5219.94, "end": 5220.94, "text": " Bye.", "tokens": [51764, 4621, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1787082928569377, "compression_ratio": 7.0, "no_speech_prob": 0.6573787331581116}], "language": "en"}