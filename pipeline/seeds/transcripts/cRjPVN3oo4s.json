{"text": " Some of you have finished Part 1 in the last few days, some of you finished Part 1 in December. I did ask those of you who took it in person to revise the material and make sure it was up to date. But let's do a quick summary of the key things we learned. I came up with these 5 things. I'd be interested to hear if anybody has other key insights that they feel they came away from. So the 5 things are these. Stacks of differentiable nonlinear functions with lots of parameters solve nearly any predictive modeling problem. So when we say neural network, a lot of people are suggesting we should use the phrase differentiable network. If you think about things like the collaborative filtering we did, it was really a couple of embeddings and a dot product. That put us quite a long way. There's nothing very neural-looking about that. But we know that when we stack certain kinds of nonlinear functions on top of each other, the universal approximation theorem tells us that can approximate any computable function to arbitrary precision. We know that if it's differentiable, we can use SGD to find the parameters which match that function. So this to me is kind of like the key insight. But some stacks of functions are better than others for some kinds of data and some kinds of problems. One way to make life very easy, we learned, is transfer learning. I think nearly every network we created in the last course, we used transfer learning, I think particularly in vision and in text. Pretty much everything. So transfer learning generally was throw away the last layer, replace it with a new one that has the right number of outputs, pre-compute the penultimate layer's output, then very quickly create a linear model that goes from that to your preferred answer. You now have something that works pretty well. And then you can fine-tune more and more layers backwards as necessary. And we learned that fine-tuning those additional layers, generally the best way to do that was to pre-compute the last of the layers which you're not fine-tuning. And so then you could just calculate the weights of the remaining ones, and that saved us lots and lots of time. And remember that convolutional layers are slower. Dense layers are bigger. There's an interesting question I've added here. Remember in the last lesson we kind of looked at ResNets and InceptionNets, and in general more modern nets tend not to have any dense layers. So what's the best way to do transfer learning? I'm going to leave that as an open question for now. We're going to look into it a bit during this class, but it's not a question that anybody has answered to my satisfaction. So I'll suggest some ideas. No one's even written a paper that attempts to address it, as far as I'm aware. Given we have transfer learning to get us a long way, the next thing we have to get us a long way is to try and create an architecture which suits our problem, both our data and our loss function. So for example, if we have autocorrelated inputs, so in other words, each input is related to the previous input. So each pixel is similar to the next door pixel, or in a sound wave, each sample is somewhat similar to the previous sample, something like that. That kind of data we tend to like to use CNNs for, as long as it's of a fixed size. If it's a sequence, we like to use an RNN for. If it's a categorical output, we like to use a Softmax for. So there are ways we learned of tuning our architecture, not so that it makes it possible to solve a problem, because any standard dense network can solve any problem, but it just makes it a lot faster and a lot easier to train if you've made sure that your activation functions and your architecture suit the problem. So that was another key thing I think we learned. Something I hope that everybody can narrate is the 5 steps to avoiding overfitting. If you've forgotten them, they're both here and discussed in more detail in Lesson 3. Get more data, fake more data using data augmentation, use more generalizable architecture. Architectures are generalized well, particularly we look at batch normalization. Use regularization techniques as few as we can, because by definition they destroy some data, but we look particularly at using dropout. And then finally if we have to, we can look at reducing the complexity of the architecture. The general approach we learned, this is absolutely key, is first of all, with a new problem, start with a network that's too big, that's not regularized, that can't help but solve the problem, even if it has to overfit terribly. If you can't do that, there's no point in starting to regularize yet. So we start out by trying to overfit terribly. Once we've got to the point that we're getting 100% accuracy and our validation set is terrible because it's overfitting, then we start going through these steps until we get a nice balance. So that's kind of the process that we learned. And then finally, we learned about embeddings as a technique to allow us to use categorical data and specifically the idea of using words or the idea of using latent variables. So in this case, this was the MovieLens dataset for collaborative filtering. That's the five main insights I thought of. Did anybody have any other key takeaways that they think people revising should think about or remember or things they found interesting? If you come up with something, let me know. Question 2. How does having duplicates and training data affect the model created? And if you're using data augmentation, do you end up with duplicate input data? Answer. Duplicates in the input data, it's not a big deal. We shuffle the batch and then you select things randomly. Effectively you're weighting that data point higher than its neighbors. In a big dataset, it's going to make very little difference. If you've got one thing repeated a thousand times and there's only another hundred data points, that's going to be a big problem because you're weighting one data point a thousand times higher. So as you will have seen, we've got a couple of big technology foundation changes. The first one is we're moving from Python 2 to Python 3. Python 2 I think is a good place to start given that a lot of the folks in Part 1 had never coded in Python before and many of them had never written very substantial pieces of software before. A lot of the tutorials out there, like for example one of our preferred starting points, which is learn Python the hard way, is in Python 2, a lot of the existing codes out there are in Python 2. So I thought Python 2 is a good place to start. Yes, Rachel? 2 more questions. One is are you going to post the slides after this? I will post the slides, yes. And the other is could you go through steps for underfitting at some point? Do you have the steps for how to deal with overfitting? Yes, let's do that in a forum thread. Why don't you create a forum thread asking about underfitting. But you don't need to do that in the Part 2 forum, you can do that in the main forum because lots of people would be interested in hearing about that. If you want to revise that, Lesson 3 started by talking about underfitting. So that seemed like a good place to start. I don't think we should keep using Python 2 though for a number of reasons. One is that since then, the IPython folks have come out and said that the next version won't be compatible with Python 2. So that's a problem. Indeed from 2020 onwards, Python 2 will be end-of-life, which means there won't be patches for it. So that's a problem. Also, we're going to be doing more stuff with concurrency and parallel programming this time around. The features in Python 3 are a lot better. And then Python 3.6 was just released, which has some very nice features in particular, some string formatting which for some people is no big deal, but to me it saves a lot of time and makes life a lot easier. So we're going to move across to Python 3. Hopefully you've all gone through the process already. And there's some tips on the forum about how to have both running at the same time. Although I agree with the suggestion I read from somebody, which was go ahead, suck it up and do the translation once now so you don't have to worry about it. Much more interesting and much bigger is the move from Theano to TensorFlow. So Theano we thought was a better starting point because it has a much simpler API. There's very few new concepts to learn to understand Theano. And it doesn't have a whole new ecosystem. You see, TensorFlow lives within Google's whole ecosystem. It has its own build system called Bazel. It's got its own file serialization system called Protobuf. It's got its own profiler method based on Chrome. It's got all this stuff to learn. But if you've come this far, then you're already investing the time. We think it's worth investing the time in TensorFlow because there's a lot of stuff which just in the last few weeks it's started being able to do that's pretty amazing. So Rachel wrote this post about how much TensorFlow sucks. So looking at moving from Theano to TensorFlow, we got invited to the TensorFlow Dev Summit and we were pretty amazed at all the stuff that's literally just been added. So TensorFlow 1 just came out. Here are some of the things, if you Google for TensorFlow Dev Summit videos, you can watch the videos about all this. Perhaps the most exciting thing for us is that they are really investing in a simplified API. So if you look at this code, you can create a deep neural network regressor on a mixture of categorical and real variables using a kind of almost R-like syntax and fit it in two lines of code. You'll see that those lines of code at the bottom, the two lines to fit it, look very much like Keras. Francois Cholet, the Keras author, has been a wonderful influence on Google. In fact, everywhere we saw at the Dev Summit was Keras API influences. So TensorFlow and Keras are becoming more and more one, which is terrific. So one is that they're really investing in the API. The second is that some of the tooling is looking pretty good. So TensorBoard has come a long way. Things like these graphs showing you how your different layers are distributed and how that's changed over time can really help to debug what's going on. So if you get some kind of gradient saturation in a layer, you can dig through these graphs and very quickly find out where. This was one of my favorite talks actually. This guy, if I remember correctly, his name was Daffodil and his signature was an emoji of a daffodil. Very Google. If you watch this video, he has a walk-through showing some of the functionality that's there and how to use it. I thought that was pretty helpful. One of the most important ones to me is that TensorFlow has a great story about productionization. For Part 1, I didn't much care about productionization. It was really about playing around, what can we learn. At this point, I think we might be starting to think about how do I get my stuff online in front of my customers. These points are talking about something in particular which is called TensorFlow Serving. TensorFlow Serving is a system that can take your trained TensorFlow model and create an API for it which does some pretty cool things. For example, think about how hard it would be without the help of some library to productionize your system. You've got one request coming in at a time. You've got n GPUs. How do you make sure that you don't saturate all those GPUs, that you send the request to one that's free, that you don't use up all of your memory. Better still, how do you grab a few requests, put them into a batch, put them all into the GPU at once, get the bits out of the batch, put them back to the people that requested it, all that stuff. Serving does that for you. It's very early days for this software. A lot of things don't work yet, but you can download an early version and start playing with it. I think that's pretty interesting. Question 2 With the high-level API in TensorFlow, what's going to be the difference between the Keras API and the TensorFlow API? That's a great question. In fact, TF.keras will become a namespace. So Keras will become the official top-level API for TensorFlow. In fact, Rachel was the person who announced that. I was just going to add that TensorFlow is introducing a few different libraries at different levels of abstraction. There's this concept of an evaluation API that appears everywhere and basically is the Keras API. I think there's a layers API below the Keras API. So it's being mixed in lots of places. So all the stuff you've learned about Keras is going to be very helpful, not just in using Keras on top of TensorFlow, but in using TensorFlow directly. Another interesting thing about TensorFlow is that they've built a lot of cool integrations with various cluster managers and distributed storage systems and stuff like that. So it will kind of fit into your production systems more neatly, use the data in whatever place it already is more neatly. So if your data is in S3 or something like that, you can generally throw it straight into TensorFlow. Something I found very interesting is that they announced a couple of weeks ago a machine learning toolkit which brings really high-quality implementations of a wide variety of non-deep learning algorithms. So all these are GPU-accelerated, parallelized, and supported by Google. Like a lot of these have a lot of tech behind them. So for example, the random forest, there's a paper, they actually call it the TensorForest, which explains all of the interesting things they did to create a fast GPU-accelerated random forest. Question 2. Will you give an example of how to solve gradient saturation with TensorFlow tools? I'm not sure that I will. We'll see how we go. I think the video from the Dev Summit, which is available online, kind of already shows you that. So I would say look at that first and see if you still have questions. All the videos from the Dev Summit are online. Someone asked about is there an idea for using deep learning on AWS Lambda? Not that I've heard of, no. In general, Google has a service version of TensorFlow Serving called Google Cloud ML, where you can pay them a few cents a transaction and they'll host your model for you. There isn't really something like that through Amazon, as far as I'm aware. And then finally, in terms of TensorFlow, I had an interesting and infuriating few weeks trying to prepare for this class and trying to get something working that would translate French into English. Every single example I found online had major problems. Even the official TensorFlow tutorial missed out a key thing, which is that the lowest level of a language model really should be bidirectional, as this one shows, bidirectional RNN. And their one wasn't. I'm trying to figure out how to make it work. It's horrible. I'm trying to get it to work in Keras, nothing worked properly. Finally, basically the issue is this. Modern RNN systems, like a full neural translation system, involve a lot of tweaking and mucking around with the innards of the RNN, using things that we'll learn about. And there just hasn't been an API that really lets that happen. So I finally got it working by switching to PyTorch, which we'll learn about soon. I was actually going to start the first lesson was going to be about neural translation, and I've put it back because TensorFlow has just released a new system for RNNs which looks like it's going to make all this a lot easier. So this is an exciting idea, there's an API that allows us to create some pretty powerful RNN implementations and we're going to be absolutely needing that when we learn to create translations. Oh yeah, there is one more. Again, early days, but there is something called XLA, which is the Accelerated Linear Algebra. Which is a system which takes TensorFlow code and compiles it. And so for those of you who know something about compiling, you know that a compilation can do a lot of clever stuff in terms of identifying dead code, or unrolling loops, or fusing operations, or whatever. XLA tries to do all that. Now at this stage, it takes your TensorFlow code and turns it into machine code. One of the cool things that lets you do is run it on a mobile phone with almost no supporting libraries using native machine instructions on that phone, much less memory. But one of the really interesting discussions I had at the summit was with Scott Gray, who some of you may have heard of. He was the guy that massively accelerated neural network kernels when he was at Nirvana. He had kernels that were 2 or 3 times faster than NVIDIA's kernels. I don't know of anybody else in the world who knows more about neural network performance than him. He told me that he thinks that XLA is the key to creating performant, concise, expressive neural network code. I really like that idea. The idea is currently, if you look in the TensorFlow code, it's thousands and thousands of lines of C++, all custom-written. The idea is you throw all that away and replace it with a small number of lines of TensorFlow code that get compiled through XLA. So that's something that's actually got me pretty excited. So TensorFlow is pretty interesting. Having said that, it's kind of hideous. The API is full of not-invented-here syndrome. It's clearly written by a bunch of engineers who have not necessarily spent that much time learning about the user interface of APIs. It's full of these Google-isms in terms of having to fit into their ecosystem. But most importantly, like Theano, you have to set up the whole computation graph and then you kind of go run. Which means if you want to do stuff in your computation graph that involves conditionals, if-then statements, if this happens you do this other part of the loop, it's basically impossible. So that's Rachel. It turns out that there's a very different way of programming neural nets, which is dynamic computation, otherwise known as define-through-run. There's a number of libraries that do this, Torch, PyTorch, Chainer, Dynet, they're the ones that come to mind. We're going to be looking at one that was released, but an early version was put out about a month ago called PyTorch, which I've started rewriting a lot of stuff in. A lot of the more complex stuff just becomes suddenly so much easier. Because it becomes easier to do more complex things, I often find I can create faster and more concise code by using this approach. So even though PyTorch is very, very, very new, it is coming out of the same people that built Torch, which really all of Facebook's systems build on top of. I suspect that Facebook are in the process of moving across from Torch to PyTorch. It's already full of incredibly cool stuff, as you'll see. So we will be using increasingly more and more PyTorch during this course. Question 2 Does pre-compiling mean that we'll write TensorFlow code and test it, and then when we train a big model, then we pre-compile the code and train our model? Answer If we're talking about XLA, XLA can be used a number of ways. One is that you come up with some different kind of kernel, some different kind of factorization, something like that. You write it in TensorFlow, you compile it with XLA, and then you make it available to anybody so when they use your layer, they're getting this compiled optimized code. It could mean that when you use TensorFlow Serving, TensorFlow Serving might compile your code using XLA and be serving up an accelerated version of it. One example which came up was for RNNs. RNNs often involve, nowadays as you'll learn, some kind of complex customizations of a bidirectional layer and then some stacked layers and then a tension layer, and then fed into a separate stacked decoder. You can fuse that together into a single layer called a bidirectional attention sequence to sequence, which indeed, people have actually bought that kind of stuff. There's various ways in which neural network compilation can be very helpful. Question What is the relationship between TensorFlow and PyTorch? There's no relationship. TensorFlow is Google's thing. PyTorch is kind of Facebook's thing, but it's also very much a community thing. TensorFlow is a huge, complex beast of a system which uses all kinds of advanced software engineering methods all over the place. In theory that ought to make it terribly fast. In practice, a recent benchmark actually showed it to be about the slowest. I think the reason is because it's so big and complex, it's so hard to get everything to work together. In theory, PyTorch ought to be the slowest because this defined-by-run system means it's way less optimization that the systems can do, but it turned out to be amongst the fastest because it's so easy to write code, it's so much easier to write good code. It's interesting. There are such different approaches, I think it's going to be great to know both because there are going to be some things that are going to be fantastic in TensorFlow and some things that are going to be fantastic in PyTorch. They couldn't be more different, which is why I think they're two good things to learn. So wrapping up this introductory part, I wanted to kind of change your expectations about how you've learned so far to how you're going to learn in the future. Part 1 to me was about showing you best practices. So generally it's like here's a library, here's a problem, you use these libraries, these steps to solve this problem, and you do it this way, and lo and behold we've gotten the top 10 in this Kaggle competition. I tried to select things that had best practices. You now know everything I know about best practices. I don't really have anything else to tell you. So we're now up to stuff I haven't quite figured out yet, nor has anybody else, but you probably need to know. So some of it, for example, like neural translation, that's an example of something that is solved. Google solved it, but they haven't released the way they solved it. So the rest of us are trying to put everything together and figure out how to make something work as well as Google made that work. More often it's going to be, here's a sequence of things you can do that can get some pretty good results here, but there's a thousand things you can do to make it better that no one's tried yet, so that's interesting. Or thirdly, it could be, here's a sequence of things that solves this pretty well, but gosh we wrote a lot of custom code there, didn't we? I'm sure this could be abstracted really nicely, but no one's done that yet. So they're kind of the 3 main categories. So generally at the end of each class, it won't be like, okay that's it, that's how you do this thing. It will be more like, here are the things you can explore. So the homework will be, pick one of these interesting things and dig into it. And generally speaking, that homework will get you to a point that probably no one's done before, or at least probably no one's written down before. I found as I built this, I think nearly every single piece of code I'm presenting, I was unable to find anything online which did that thing correctly. There was often example code that claimed to be something like that, but again and again I found it was missing huge pieces. We'll talk about some of the things it was missing as we go. One very common one was it would only work on a single item at a time, it wouldn't work with a batch, therefore the GPU is basically totally wasted. Or it failed to get anywhere near the performance that was claimed in the paper that it was meant to be based on. So generally speaking, there's going to be lots of opportunities if you're interested to write a little blog post about the things you tried and what worked and what didn't. And you'll generally find that there's no other post like that out there. Particularly if you pick a dataset that's in your domain area, it's very unlikely that somebody has written it. Question 2 Can we use TensorFlow and Torch together? Answer I don't say Torch, say PyTorch. Torch is very similar, but it's written in Lua, which is a very small embedded language, very good for what it is, but not very good for what we want to do. So PyTorch is kind of a port of Torch into Python, which is pretty cool. So can you use them together? Yeah, sure. We'll kind of see a bit of that. In general, you can do a few steps with TensorFlow to get to a certain point, and a few more steps with PyTorch. You can't integrate them into the same network because they're very different approaches, but you can certainly solve a problem with the two of them together. For those of you who have some money left over, I would strongly suggest building a box. The reason I suggest building a box is because you're paying $0.90 an hour for a P2. I know a lot of you are spending a couple of hundred bucks a month on AWS bills. Here is a box that costs $550 and will be about twice as fast as a P2. It's just not good value to use a P2, and it's way slower than it needs to be. Also building a box, it's one of the many things that's just good to learn, is understanding how everything fits together. So I've got some suggestions here about what box to build for various different budgets. You certainly don't have to, but this is my recommendation. Couple of points to make. More RAM helps more than I think people who discuss this stuff online quite appreciate. 12GB of RAM means twice as big of batch sizes, which means half as many steps necessary to get through an epoch. It means more stable gradients, which means you can use higher learning rates. So more RAM I think is often underappreciated. The TitanX is the card that has 12GB RAM. It is a lot more expensive, but you can get the previous generation's version second-hand, it's called the MaxWelt. There's a TitanX Pascal, which is the current one, or the TitanX MaxWelt, which is the previous generation one. The previous generation one is not a big step back at all, it still has 12GB RAM. If you can get one used, that would be a great option. The GTX 1080 and GTX 1070 are absolutely fantastic as well. They're nearly as good as the TitanX, but they just have 8GB rather than 12GB. Going back to a GTX 980, which is the previous generation consumer top-end card, you have the RAM again. So of all the places you're going to spend money on a box, put nearly all of it into the GPU. Every one of these steps, the 1070, the TitanX, Pascal, they're big steps up. As you will have seen from Part 1, if you've got more RAM, it really helps because you can pre-compute more stuff and keep it in RAM. Having said that, there's a new kind of hard drive called an NVMe drive. Non-volatile memory, I think. NVMe drives are quite extraordinary. They're not that far away from RAM-like speeds, but they're hard drives, they're persistent. You have to get a special kind of motherboard, but if you can afford it, sometimes it's only like $400 or $500 to get an NVMe drive. That's going to really allow you to put all of your currently used data on that drive and access it very, very quickly. So that's my other tip. Question 2. Doesn't the batch size also depend heavily on the video RAM? Answer. That's what I was referring to, the 12GB, 8GB. I'm talking about the RAM that's on the video card on the GPU. Question 3. Does upgrading RAM allow bigger batch sizes? Answer. Upgrading the video card's RAM, you can't upgrade the RAM on the card. You buy a card that has X amount of RAM. So you can buy a card that has 12, GTX 1080, 8, GTX 980, 4. So that's on the card. Upgrading the amount of RAM that's in your computer doesn't change your batch size, it just changes the amount you can pre-compute. Unless you use an NVMe drive, in which case RAM is much less important. You don't have to plug everything in. You can go to Central Computers, which is a San Francisco computer shop, for example, and they'll put it all together for you. There's a fantastic thread on the forums. Brendan who's one of the participants in the course has a great Medium post, went there just explaining his whole journey to getting something built and set up. So there's lots of stuff there to help you. It's time to build your box and while you wait for things to install, it's time to start reading papers. So papers are, if you're a philosophy graduate like me, terrifying. They look like theorem 4.1 and glorily 4.2 on the left. But that is an extract from the Atom paper. You all know how to do Atom in Microsoft Excel. It's amazing how most papers manage to make simple things incredibly complex. And a lot of that is because academics need to show other academics how worthy they are of a conference spot, which means showing off all their fancy math skills. So if you really need a proof of the convergence of your optimizer, rather than just running it and seeing if it works, you can study theorem 4.1 and glorily 4.2 and blah blah blah. In general though, the way philosophy graduates read papers is to read the abstract, find out what problem they're solving, read the introduction to learn more about that problem and how previous people have tackled it, jump to the bit at the end called experiments to see how well the thing works. If it works really well, jump back to the bit which has the pseudocode in and try to get that to work. Ideally, hopefully in the meantime, finding that somebody else has written a blog post in simple English like this example with Atom. So don't be disheartened when you start reading deep learning papers, unless you have a math background. Well even if you're a PhD in math and they're still terrifying. I'm sure there's a lot of people who are complaining about a paper just today. You will learn to read the papers. The other thing I'll say is that you'll even see now, there will be a bit that's like, and then we use a softmax layer and there will be the equation for a softmax layer. You'll look at the equation and be like, what the hell? And then it's like, oh I already know what a softmax layer is. And then we use an LSTM. Apparently still in every paper they write the damn LSTM equations as if that's any help to anybody. But okay, it adds more Greek symbols, so be it. Talking of Greek symbols, it's very hard to read and remember things you can't pronounce. So if you don't know how to read the Greek letters, Google the Greek alphabet and learn how to say them. It's just so much easier when you can look at an equation and rather go squiggle something, squiggle something, you can say alpha something and beta something. I know it's a small little thing, but it does make a big difference. So we are all there to help each other read papers. The reason we need to read papers is because as of now, a lot of the things we're doing only exist in very recent paper form. I really think writing is a good idea. In fact, all of your projects I hope will end up in at least one blog. If you don't have a blog, medium.com is a great place to write. We would love to feature your work on fast.ai. So tell us about what you create. We're very keen for more people to get into the deep learning community. When you write this stuff, say hey this is some stuff based on this course I'm doing and here's what I've learnt and here's what I've tried and here's what I've found out. Put the code on GitHub. It's amazing. Even us putting our little AWS setup scripts on GitHub for the MOOC, Rachel had a dozen pull requests within a week with all kinds of little tidbits of like if you're on this version of Mac, this helps this bit, or I've abstracted this out to make it work in Ireland as well as in America. So there's lots of stuff that you can do. I think the most important tip here is don't wait to be perfect before you start writing. What was that tip you told me, Rachel? You should think of your target audience as the person who's one step behind you. So maybe your target audience is someone who's just working through the Part 1 MOOC right now. If your target audience is not Jan Lacorne or Jeffrey Hinton, it's you 6 months ago. Write the thing you would love to have seen because there will be far more people in that target audience than in the Jeffrey Hinton target audience. How are we going for time, Rachel? I've tried to lay out what I think we'll study in Part 2. As I say, what I was planning until quite recently to present today was neural translation, and then two things happened. Google suddenly came up with a much better RNN and sequence-to-sequence API. And then also two, three weeks ago, a new paper came out for generative models which totally changed everything. So that's why we've redone things and we're starting with CNN generative models today. We have a question, where to find the current research papers? Okay, we'll get to that for sure. Let's do that after our break. Assuming that things go as planned, the general topic areas in Part 2 will be CNNs and NLP beyond classification. If you think about it, pretty much everything we did in Part 1 was classification or a little bit of regression. We're going to now be talking more about generative models. It's a little hard to exactly define what I mean by generative models, but we're talking about creating an image or creating a sentence. We're creating bigger outputs. So CNNs beyond classification, generative models for CNNs, means the thing that we could produce could be a picture showing this is where the bicycle is, this is where the person is, this is where the grass is, that's called segmentation. Or it could be taking a black and white image and turning it into a color image, or taking a low-res image and turning it into a high-res image, or taking a photo and turning it into a Van Gogh, or taking a photo and turning it into a sentence describing it. NLP beyond classification can be taking an English sentence and turning it into French, or taking an English story and a question and turning it into an answer of that question about that story. That's chatbots in Q&A. We'll be talking about how to deal with larger datasets, so that both means datasets with more things in it and datasets where the things are bigger. And then finally, something I'm pretty excited about is I've done a lot of work recently finding some interesting stuff about using deep learning for structured data and for time series. So for example, we heard about fraud. So fraud is both of those things. It combines time series, transaction histories, and click histories, and structured data, customer information. Traditionally, that's not been tackled with deep learning, but I've actually found some state-of-the-art, world-class approaches to solving those with deep learning. So I'm really looking forward to sharing that with you. So let's take a 8-minute break, come back at 5-8. Thanks very much. So we're going to learn about this idea of artistic style, or neural style transfer. The idea is that we're going to take a photo and make it look like it was painted in the style of some painter. So our inputs are a photo and style. And so these two things are going to be combined together to create an image which is going to hopefully have the content of the photo and the style of the image. The way we're going to do this is we're going to assume that there is some function where the inputs to this function are the photo, the style image, and some generated image that I've created. And that will return some number where this function will be higher if the generated image really looks like this photo in this style, and lower if it doesn't. So if we can create this loss function that basically says here's my generated image, and it returns back a number saying, oh yes, that generated image does look like that photo in that style, then we could use SGD. We would use SGD not to optimize the weights of a network, we would use SGD to optimize the pixel values of the generated image. So we would be using it to try to optimize the value of this argument. We haven't quite done that before, but conceptually it's identical. Conceptually we can just find the derivative of this function with respect to this input. And then we can try to optimize that input, which is just a set of pixel values, to try and maximize the function. So all we need to do is come up with the function. Come up with a function which will tell us how much does some generated image look like this photo in this style. And the way we're going to do that, step 1, is going to be very simple. We're going to turn it into 2 functions. fcontent, which will take the photo and the generated image, and that will tell us a bigger number if the generated image looks more like the photo, if the content looks the same. And then there'll be a second function, which takes the style image and the generated image, and that will tell us a higher number if this generated image looks like it was painted in the same style as the style image. So we can just turn it into 2 pieces and add them together. So now we need to come up with these 2 parts. Now the first part is very easy. What's a way that we could create a function that returns a higher number if the generated image is more similar to some photo? When you come up with a loss function, the really obvious one is the values of the pixels. The values of the pixels in the generated image, the mean squared error between them and the photo, that mean squared error loss function would be one way of doing this part. The problem with that though is that as I start to turn it into a van Gogh, those pixel values are going to change. They're going to change color because the van Gogh might have been a very blue-looking van Gogh. They'll change the relationships to each other, so it might become a curve or it used to be a straight line. So really, the pixel-wise mean squared error is not going to give us much freedom in trying to create something that still looks like a photo. So here's an idea. Instead, let's take those pixels and stick them through a pre-trained CNN, like VGG, and let's look at the 4th or 5th or 8th convolutional layers activations. Remember back to those Matt Zyler visualizations where we saw that the later layers said, how much does an eyeball look like here, or how much does this look like a star, or how much does this look like the fur of a dog. The later layers were dealing with bigger objects and more semantic concepts. So if we were to use a later layer's activations as our loss function, then we could really change the style and the color and all kinds of stuff and really would be saying, does the eye still look like an eye? Does the beak still look like a beak? Does the rock still look like a rock? And if the answer is yes, then okay, that's good. This is something that matches in terms of the meaning of the content, even though the pixels look very different. And so that's exactly what we're going to do. So for fcontent, we're going to say that's just the VGG activations of some convolutional layer. Which one? We can try some. So that's actually enough for us to get started. Let's try and build something that optimizes pixels using a loss function of the VGG network some convolutional layer. This is the neural style notebook. And much of what we're going to look at is going to look very similar. The first thing you'll see which doesn't look similar to before is I've got this thing called limitmem. Remember you can always see the source code for something by putting two question marks. Limitmem is just these three lines of code, which I noticed somebody kindly has already pasted in the forum. One of the many things I dislike about TensorFlow for our kind of work is that all of the defaults are production defaults. One of the defaults is it will use up all of your memory on all of your graphics cards. I'm currently running this on a server with 4 graphics cards, which I'm meant to be sharing with my colleagues at the university here. Every time I run a notebook, nobody else can use any of the graphics cards, they're going to be really, really pissed. And this nice little gig I have of running these little classes is going to disappear very quickly. So I need to make sure I run limitmem very soon, as soon as I start running a notebook. Honestly I think this is a poor choice by the TensorFlow authors because somebody putting something in production is going to be taking time to optimize things. I don't give a shit about the defaults. Somebody who's hacking something together to quickly see if they can get something working very much wants nice defaults. So this is one of the many, many places where TensorFlow makes some odd little annoying decisions. But anyway, every time now I create a new notebook, I copy this line in and make sure I run it so this does not use up all of your memory. So I've got a link to the paper that we're looking at, and indeed we can open it. Now is a good time to talk about how helpful it is to use some kind of paper reading system. I really like this one, it's free, it's called Mendeley Desktop. Mendeley lets you, as you find papers, you can save them into a folder on your computer. Mendeley will automatically watch that folder. Any PDF that appears there gets added to your library. It's really quite cool because what it then does is it finds the archive ID and then you can click this little button here and it will go to archive and grab all of the information such as the abstract and so forth and fill it out for you. This is really great because now any time I want to find out what I've read, which has got anything to do with style, I can type style and up pop all of the papers. Believe me, after a long time of reading papers without something like this, it basically goes in one ear and out the other. Literally I've read papers a year later and at the end of it I've realized, I read that before. I don't remember anything else about it, but I know I read it before. This way I really find that my knowledge builds. As I find references, I'm immediately there looking at the references. The other thing you can do is as you start reading the paper, as you can see, my notes and highlights are saved. They're also duplicated on my mobile devices and my other computers and they're all synced up. It's really cool. Speaking about Archive is a great time to answer a question we had earlier about how do you find papers. The vast, vast, vast majority of deep learning papers get put up on archive.org a long, long, long time before they're in any journal or conference. If you wait until they're in a conference proceedings, you're many, many months or maybe even a year behind. So pretty much everybody uses Archive. You can go to the AI section of Archive and see what's there, but that's not really what anybody does. What everybody does instead is use Archive Sanity, the Archive Sanity Preserver. This is something that the wonderful Andrej Karpathy built. What it lets you do is to create a library of articles that somebody tells you to read or that you're interested in or you come across. As you create that library by clicking this little save button, it then recommends more papers like it. Or even once you start reading a paper, you go Show Similar and it will then show you other papers that are similar to this paper. It seems to do a pretty damn good job of it. So you can really explore and get lost in that whole area. So that's one great way to do it. As you do that, you'll find that if you go to Archive, one of the buttons that it has is a Bookmark on Mendeley button. So even from the abstract here, bang, straight into your library and the next time you load up Mendeley, it's all there. Then you can put things into folders, so the different parts of the course, I've created folders for them and kind of keep track of what I'm reading that way. A good little trick to know about Archive.org is that you often want to know where it's from. If you go to the first page on the left-hand side, you can see the date here. Another cool tip is that the file name, the first 4 digits are the year and month for that file. So there's a couple of handy little tips. As well as Archive Sanity, another really great place for finding papers is Twitter. Now if you haven't really used Twitter before, or haven't really used Twitter for this purpose before, it's hard to know where to start. So I try to make things easy for people by favoriting lots of the interesting deep learning papers that I come across. So if you go to Jeremy P. Howard's page and click on Likes, you'll find that there are a thousand links to papers here. As you can see, there's generally a few every day. That's useful for a number of reasons. One is to get some ideas for papers to read, but perhaps more importantly is to see who's posting these cool links. And then you can follow them as well. Rachel, can you throw that box to that gentleman? The black one. Yes, that's it. Question from the audience. Just to speak, it's not a question, it's just information about Archive. There is someone who has built a skill on Amazon Alexa. By asking Alexa to give the most recent paper from Archive, and actually she reads abstract for you, and you can filter the most recent papers for you. The other place which I find extremely helpful is Reddit Machine Learning. Again, there's a lot less that goes through Reddit than goes through Twitter. But generally the really interesting things tend to turn up here. You can often see the discussions of it. For example, there was a great discussion of PyTorch versus TensorFlow in the last day or two. There's a couple of good places to get started. Anything I missed, Rachel? I think that's good. I have two questions on the image stuff when you go back to style. I'm ready. One of them was if the app Prisma is using something like this. Yes, Prisma is using exactly this. The other is, is it better to calculate f-content for a higher layer for VGG and use a lower layer for app style since the higher layer of abstracts are captured in the higher layer and the lower layer captures textures? Probably, let's try it, shall we? We haven't learned about f-style yet, so we're just going to look at f-content first. I've got some more links to some things you can look at here in the notebook. So the data I've linked to in the lesson thread on the forum, I've just grabbed a random sample of about 20,000 ImageNet images and I've also put them into bcols arrays. So you can set up your paths appropriately. I haven't given you this pickle, you can figure out how to get the file names easily enough, so I'm not going to do everything for you. I've grabbed one of those pictures. Thank you for the person who's showing all the other stuff to pip install, that's very helpful. So that's going to be our content image. Given that we're using VGG, as per usual, we're going to have to subtract out the mean pixel value from ImageNet and reverse the channel order because of course that's what the original VGG authors did. So we're going to create an array from the image by just running it through that preprocessing function. Later on, we're going to be running things through a network and generating images. Those generated images we're going to have to add back on that mean and undo that reordering. So this is what this deprocessing function is going to be for. Now I've kind of hand-waved over these functions before and how they work. But I'm going to stop hand-waving for a moment because it's actually quite interesting. Have you ever thought about how is it that we're able to take x, which is a 4-dimensional tensor, batch size by height by width by channels. Notice this is not the same as Theano. Theano was batch size by channels by height by width. We're not doing that anymore. Now kind of more natural, batch size by height by width by channels. We're taking a 4-dimensional tensor and we're subtracting from it a vector. How is it making that work? The way it's making that work is because it's doing something called broadcasting. Broadcasting refers to any kind of operation where you have arrays, tensors, of different dimensions and you do element-wise operations on two tensors of different dimensions. How does that work? This idea actually goes back to the early 1960s to an amazing programming language called APL. APL stands for A Programming Language. APL was written by an extraordinary person called Kenneth Iverson. Originally APL was a paper describing a new mathematical notation. This new mathematical notation was designed to be more flexible and far more precise than traditional mathematical notation. He then went on to create a programming language that implemented this mathematical notation. APL refers to the notation, which he described as notation as a tool for thought. He really, unlike the TensorFlow authors, understood the importance of a good API. He recognized that the mathematical notation can change how you think about math. He created a notation which was incredibly expressive. His son now has gone on to carry the torch and he now continues to support a direct descendant of APL, which is called Jay. If you ever want to find what I think is the most elegant programming language in the world, you can go to jsoftware.com and check this out. How many of you here have used regular expressions? How many of you, the first time you looked at a complex regular expression, thought that is totally intuitive? You will feel the same way about Jay. The first time that you look at a piece of Jay, you'll go, what the bloody hell. Because it's an even more expressive and a much older language than regular expressions. Here's an example of a line of Jay. What's going on here is that this is a language which at its heart almost never requires you to write a single loop because it does everything with multidimensional tensors and broadcasting. Everything we're going to learn about today with broadcasting is a very diluted, simplified, crapified version of what APL created in the early 60s. Which is not to say anything rude about Python's implementation, it's one of the best. Jay and APL totally blow it away. If you want to really expand your brain and have fun, check out Jay. In the meantime, what does Keras, Thiano, and TensorFlow broadcasting look like? Let's look at some examples. So here we have, oh I like them too, Input AI and WOWML Newsletters. Here is a vector, a one-dimensional tensor minus a scalar. That makes perfect sense, that you can subtract a scalar from a one-dimensional tensor. But what is it actually doing? What it's actually doing is it's taking this 2 and it's replicating it 3 times. This is actually element-wise, 1, 2, 3, minus 2, 2, 2. It has broadcasted the scalar across the 3-element vector 1, 2, 3. So there's our first example of broadcasting. So in general, broadcasting has a very specific set of rules, which is this. You can take two tensors, and you first of all take the shorter tensor, the tensor with less dimensions, and prepend unit axes to the front. What do I mean when I say prepend unit axes? Here's an example of prepending unit axes. Take the vector 2, 3 and prepend 3 unit axes on the front. It is now a 4-dimensional tensor of shape 1, 1, 1, 2. So if you turn a row into a column, you're adding one unit axis. If you're then turning it into a single slice, you're adding another unit axis. So you can always make something into a higher dimensionality by adding unit axes. So when you broadcast, it takes the thing with less axes, less dimensions, and adds prepends unit axes to the front. And then what it does is it says, let's take this first example. It's taken this thing which has no axes, it's a scalar, and turns it into a vector of length 1. Then what it does is it finds anything which is of length 1 and duplicates it enough times so that it matches the other thing. So here we have something which is a 4-dimensional tensor of size 5, 1, 3, 2. So it's got 2 columns, 3 rows, 1 slice, and 5 cubes. And then we're going to subtract from it a vector of length 2. So remember from our definition, it's then going to automatically reshape this by prepending unit axes until it's the same length. And then it's going to copy this thing 3 times, this thing 1 time, and this thing 5 times. So the shape is 5, 1, 3, 2. So it's going to subtract this vector from every row, every slice, every cube. So you can play around with these little broadcasting examples and try to get a real feel for how to make broadcasting work for you. So in this case, we were able to take a 4-dimensional tensor and subtract from it a 3-dimensional vector, knowing that it is going to copy that 3-dimensional vector of channels to every row, to every column, to every batch. So in the end, it's just done what we mean. It subtracted the mean average of the channels from all of the images the way we wanted it to. But it's been amazing how often I've taken code that I've downloaded off the internet and made it often 10 or 20 times smaller in terms of lines of code just by using lots of broadcasting. And the reason I'm talking about this now is because we're going to be using this a lot. So play with it. And as I say, if you really want to have fun, play with it in J. Okay, so that was a diversion, but it's one that's going to be important throughout this. So we've now basically got the data that we want. So next thing we need is a VGG model. Here's the thing though, when we're doing generative models, we want to be very careful of throwing away information. One of the main ways to throw away information is to use max-pooling. When you use max-pooling, let's say 2,2 max-pooling, you're throwing away 3 quarters of the previous layer and just keeping the highest one. In generative models, when you use something like max-pooling, you make it very hard to undo that and get back the original data. So if we were to use max-pooling with this idea of our f-content, and we say what does the 4th layer of activations look like, if we've used max-pooling, then we don't really know what 3 quarters of the data look like. Slightly better is to use average-pooling instead of max-pooling. At least with average-pooling, we're using all of the data to create that average. We've still kind of thrown away 3 quarters of it, but at least it's all been incorporated into calculating that average. So the only thing I did to turn VGG16 into VGG16-average was to do a search-and-replace in that file from max-pooling to average-pooling. And it's just going to give us some slightly smoother, slightly nicer results. You're going to see this a lot with generative models. We do little tweaks just to try to lose as little information as possible. You can just think of this as VGG16. Question 2 Shouldn't we use something like ResNet instead of VGG since the residual blocks carry more context? Answer We'll look at using ResNet over the coming weeks. It's a lot harder to use ResNet for anything beyond basic classification for a number of reasons. One is that just the structure of ResNet blocks is much more complex. If you're not careful, you're going to end up picking something that's on one of those little arms of the ResNet rather than one of the additive mergers of the ResNet. It's not going to give you any meaningful information. You also have to be careful because the ResNet blocks most of the time are just slightly fine-tuning their previous block, adding the residuals. It's not really adding new types of information. Honestly the truth is I haven't seen any good research at all about where to use ResNet or Inception architectures for things like generative models or for transfer learning or anything like that. So we're going to be trying to look at some of that stuff in this course, but it's far from straightforward. Question 3 Should we put in batch normalization? Answer In Part 1 of the course, I never actually added batch norm to the convolutional part of the model. So that's kind of irrelevant because we're not using any fully connected layers. More generally, is batch norm helpful for generative models? I'm not sure that we have a great answer to that. Try it. Question 4 Will the pre-trained weights change if we're using average pooling instead of max pooling? Answer That's a great question. The pre-trained weights, clearly the optimal weights would change. But having said that, it's still going to do a reasonable job without tweaking the weights because the relationships between the activations isn't going to change. So again, this would be an interesting thing to try if you want to download ImageNet and try fine-tuning it with average pooling, see if you can actually see a difference in the outputs that come out or not. It's not something I've tried. So here is the output tensor of one of the late layers of VGG16. So if you remember, there are different blocks of VGG where there's a number of 3x3 cons in a row, and then there's a pooling layer, and then there's another block of 3x3 cons, and then a pooling layer. This is the last block of the cons layers, and this is the first conv of that block. I think this is maybe like the third-last layer of the convolutional section of VGG. This is kind of like large receptive field, very complex concepts being captured at this late stage. So what we're going to do is we need to create our target. So for our bird, when we put that bird through VGG, what is the value of that layer's activations? One of the things I suggested you revise was the stuff from the Keras FAQ about how to get layer outputs. One simple way to do that is to create a new model which takes our model's input as input and instead of using the final output as output, we can use this layer as output. This is now a model which when we call .predict, it will return this set of activations. So that's all we've done here. Now we're going to be using this inside the GPU, we're going to be using this as a target. So to give us something which is going to live in the GPU, A, and B, we can use symbolically in a computation graph, B, we wrap it with K.variable. To remind you, K, so whatever Keras in the docs, they use the Keras.backend module, they always call it K. I don't know why. So K refers to the API that Keras provides which provides a way of talking to either Theano or TensorFlow with the same API. So both Theano and TensorFlow have a concept of variables and placeholders and dot functions and subtraction functions and softmax activations and so forth. So this K.module is where all of those functions live. This is just a way of creating a variable, which if we're using Theano, it will create a Theano variable. If we're using TensorFlow, it creates a TensorFlow variable. Where possible, I'm trying to use this rather than TensorFlow directly. But I could have absolutely said tf.variable and it would work just as well because we're using the TensorFlow backend. So this has now created a symbolic variable that contains the activations of block5.conv1. So what we now want to do is to generate an image which we're going to use SGD to gradually make the activations of that image look more and more like this variable. So how are we going to do that? Let's just skip over 202 for a moment and think about some pieces. So we're going to need to define a loss function. The loss function is just the mean squared error between two things. One thing is of course that target, that thing we just created, which is the value of our layer using the bird image. We use the bird image array. So that's our target. And then what are we going to get close to that? What we want to get close to that is whatever the value is of that layer at the moment. What is layer equal? So layer is just a symbolic object, it's nothing in it. So we're going to have to feed it with data later. So remember this is kind of the interesting way you define computation graphs with TensorFlow and Theano. It's like you define it with these symbolic things now and you feed it with data later. So you've got this symbolic thing called layer, and we can't actually calculate this yet. So at this stage, this is just a computation graph we're building. Now of course, any time we have a computation graph, we can get its gradients. Yes, Richard. Question for readability. Can you scroll down when you're going over code snippets so that they're centered? Yes. Thank you. Okay, so now that we have a computation graph that calculates the loss function we're interested in, this is fcontent. If we're going to try to optimize our generated image, we're going to need to know the gradients. So here we can get the gradients, and again we use K.gradients rather than TensorFlow gradients or Theano gradients just so we can use it with any backend we like. The function we're trying to get gradients of is the loss function, which we just calculated. And then we want it with respect to, not some weights, but with respect to the input of the model. So this is the thing that we want to change, is the input to the model so as to minimize our loss. So they're the gradients. So now that we've done that, we can go ahead and create our function. And so the input to the function is just model.input, and the outputs to the function will be the loss and the gradients. So that's nearly everything we need. The last step we need to do is to actually run an optimizer. Normally when we run an optimizer, we use some kind of SGD. Now the S in SGD is for stochastic. In this case, there's nothing stochastic. We're not creating lots of random batches and getting different gradients every time. So why use stochastic gradient descent when we don't have a stochastic problem to solve? In fact, there's a much longer history of optimization methods which are deterministic, going back to Newton's method, which many of you will be familiar with. The basic idea of these much faster deterministic optimization methods is that rather than saying, Where's the gradient, which direction does it go? Let's just go a small little step in that direction. Learning rate times gradient, small little step, small little step. Because I have no idea how far to go. And it's stochastic, so it's going to keep changing. So next time I look it will be a totally different direction. Instead with a deterministic optimization, we find out which direction to go and then we find out what is the optimum distance to go in that direction. If you know this is the direction I want to go and it looks like this, then the way we find the optimum is we go a small distance. Then we go twice as far as that, twice as far as that, twice as far as that. We keep going until the slope changes sign. Once the slope changes sign, we know it's called bracketing. We've bracketed the minimum of that function. And then we can use bisection to find the minimum. So now we've bracketed it, we find halfway between the two. Is it on the left or the right of that? Okay, halfway between the two of those. Is it left or the right of that? So we use bracketing and bisection to find the optimum in that direction. That's called a line search. So all of these optimization techniques rely on the basic idea of a line search. Now once you've done the line search, you've found the optimal value in that direction, in our downhill direction. That doesn't necessarily mean we've found the optimal value across our entire space. What we then do is we replete the process. Find out what's the downhill direction now, use line search to find the optimum in that direction. So the problem with that is that in a saddle point, you will still often find yourself going backwards and forwards in a rather unfortunate way. So the faster optimization approach is when they're going to go in a new direction, they don't just say which direction is down, they say which direction is the most downhill but also the most different to the previous directions I've gone. That's called finding a conjugate direction. So the good news is you don't need to really know any of those details. All you need to know is that there is a module called scipy.optimize. And in scipy.optimize are lots of handy deterministic optimizers. The two most common used are conjugate gradient, or CG, and BFGS. They differ in the detail of how do they decide what direction to go next, which direction is both the most downhill and also the most different to the previous directions we've gone. The particular version we're going to use is a limited-memory BFGS. So the important thing is not how it works, the important thing for us is how do we use it. Question about Loss plus Grads. So this is an array containing a single thing, which is Loss. Grads is already an array, or a list I should say, which is a list of all of the loss with respect to all of the inputs. So plus in Python on two lists simply joins the two lists together. This is a list containing the loss and all of the gradients. Ant colony optimization lives in the class known as metaheuristics, like genetic algorithms or simulated annealing. There's a wide range of optimization algorithms that are designed for very difficult to optimize functions, functions which are extremely bumpy. So these techniques all use a lot of randomization in order to avoid the bumps. In our case, we're using mean-squared error, which is a nice smooth objective, so we can use the much faster convex optimization method. Question about non-convex problem or convex optimization. How do we use one of these optimizers? Basically you provide the name of the optimizer, which in this case is minimize something using BFTS, and you have to pass it 3 things. A function which will return the loss value at the current point, a starting point, and a function which will return the gradients at the current point. Now unfortunately, we have a function which returns the loss and the gradients together, which is not what this wants. So a minor little detail is that we create a simple little class. All this class does, and again the details really aren't important, but all this class does is that when loss is called, it calls that function that we created, passing in the current value of the data. It gets back the loss and the gradients, and it returns the loss. Later on, when the optimizer asks for the gradients, it returns those gradients that I stored back here. So all this is doing is it's a little class which allows us to basically turn a keras function that returns the loss and the gradients together into 2 functions, one which returns the loss, one which returns the gradients. So it's a pretty minor detail, but it's a handy thing to have in your toolbox because it means you now have something that can use deterministic optimizers on keras functions. So all we do is we loop through a small number of times, calling that optimizer each time, and passing in some starting point. So the starting point is just a random image. So we just create a random image and here is what a random image looks like. So let's go ahead and run that so we can see the results. I haven't actually ran this yet. Oh, there it comes. Good. Run, run, run. Generate random image. Solve. Okay, so you can see it going along and solving here. Here's one I prepared earlier. And here at the end of the 10th iteration is the result. So remember what we did was we started with this image, we called an optimizer which took that image and attempted to optimize this loss function where the target was the value of this layer for our bird image. And the thing it was comparing it to was the layer for the generated image. So we started with this, we ran that optimizer a bunch of times, calculating the gradient of that loss with respect to the input to the model, the very pixels themselves. And after 10 iterations, it had turned this random image into this thing. So this is the thing which optimizes the block5, conf1 layer. And you can see it still looks like a bird, but by this point it really doesn't care what the background looks like. It cares a lot what the eye looks like and the beak looks like and the feathers look like because these things all matter to ImageNet to make sure it correctly sees it as a bird. If we look at an earlier layer, let's look at block4, conf1, you can see it's getting the details more correct. We do our artistic style, we can choose which layer will be our fcontent. And if we choose an earlier one, it's going to give it less degrees of freedom to look like a different kind of bird, but it's going to look more like our original bird. And so then here's a video showing how that happens. There are the 10 steps. It's often helpful to be able to visualize the iterations of your generators at work. So feel free to borrow this very simple code. You can just use matplotlib. We actually used this in the last class. Remember we optimized our linear optimizer, we animated it. You just have to define a function that gets called at each step of the animation, and then you can just call animation.funcanimation passing in that function. That's a nice way that you can animate your own generators. Question, we're using Keras and TensorFlow to extract the VGG features, these are used by SciPy for BFGS. Does the BFGS also run on the GPU? No, there's really very little for the BFGS to do. Really for an optimizer, all of the work is in calling the loss function and the gradients. The actual work of doing the bisection and doing the bracketing is so trivial that we just don't care about that. It doesn't take any time. There's a question about the checkerboard artifact, the geometric pattern. This is actually not a checkerboard artifact exactly. Checkerboard artifacts we will look at later, they look a little bit different. That was my interpretation mistake, not the questioner's. I'm not exactly sure why this particular kind of noise has appeared, honestly. It's an interesting question. How would batching work? It doesn't. There's no batching to do. We have a single image which is being optimized. There's really no batching to do here. We'll look at a version which uses a very different approach and has batching shortly. Question 2. Has anyone tried something like this by averaging or combining the activations of multiple bird images to create some kind of prototypical or novel bird? Generative adversarial networks do something like that, but probably not quite. I'm not sure, maybe not quite. Where can people get the pickle file? They don't. You have to get a list of file names yourself from the list of files that you've downloaded. Just to make sure I understand this, someone says in this example we started with a random image, but if we started with the actual image as the initial condition, we would get the original image back, right? I would assume so. I can't see why it wouldn't. The gradients would all be zero. Question 3. Would it be useful to use a tool like Quiver to figure out which BGG layer to use for this? It's so easy just to try a few and see what works. We're nearly out of time, we haven't got through as much as I hoped, but we're going to finish off this piece. We're now going to do fstyle. fstyle is nearly identical, all of the code is nearly identical. The only thing different is a, we're going to not feed in a photo, we're going to feed in a painting. Here are a few styles we could choose from. We could do Van Gogh, we could do this drawing, or we could do The Simpsons. We pick one of those and we create the style array in the same way as before. Chuck it through BGG. This time though, we're going to use multiple layers. So I've created a dictionary from the name of the layer to its output. We're going to use that to create an array of a number of the outputs. We're going to grab the first, second and third block outputs. So we're going to create our target as before. But we're going to use a different loss function. The loss function is called styleLoss. Just like before, it's going to use the MSE. But rather than just the MSE on the activations, it's the MSE on something called the Gram matrix of the activations. What is a Gram matrix? A Gram matrix is very simply the dot product of a matrix with its own transpose. So here it is here, the dot product of some matrix with its own transpose. I've just got to divide it by here to create an average. So what is this matrix that we're taking the dot product of and its transpose? What it is, is that we start with our image. Remember the image is height by width by channels. We change the order of dimensions, so it's channels by height by width. And then we do a batch flatten. So what batch flatten does is it takes everything except the first dimension and flattens it out into a vector. This is now going to be a matrix where the rows are channels and the columns are a flattened version of the height by width. So if this is C by H by W, the result of this will be C rows and H times W columns. So when you take the dot product of something with a transpose of itself, what you're basically doing is creating something a lot like a correlation matrix. You're saying how much is each row similar to each other row. So if you think about it a number of ways. You can think about it like a cosine. A cosine is basically just a dot product. You can think of it as a correlation matrix, it's basically a normalized version of this. So maybe if it's not clear to you, write it down on a piece of paper on the way home tonight. Just think about taking the rows of a matrix and then flipping it around and you're basically then turning them into columns and then you're multiplying the rows by the columns. It's basically the same as taking each row and comparing it to each other row. So that's what this Gram matrix is. It's basically saying for every channel, how similar are its values to each other channel. So if channel 1 in most parts of the image is very similar to channel 3 in most parts of the image, then 1,3 of this result will be a higher number. So it's kind of a weird matrix. It basically tells us, it's like a fingerprint of how the channels relate to us, each other, in this particular image, or how the filters relate to each other in a particular layer of this particular image. I think the most important thing to recognize is that there is no geometry left here at all. The x and the y coordinates are totally thrown away. They're actually flattened out. So this loss function can by definition in no way at all contain anything about the content of the image because it's thrown away all of the x and y information. And all that's left is some kind of fingerprint of how the channels relate to each other, or how the filters relate to each other. So this style loss then says for 2 different images, how do these fingerprints differ, how similar are these fingerprints. So it turns out that if you now do the exact same steps as before, using that as our loss function, and you run it through a few iterations, it looks like that. It looks a lot like the original bang-off, but without any of the content. So the question is, why? The answer is, nobody the fuck knows. So a paper just came out 2 weeks ago called Demystifying Neural Style Transfer with a mathematical treatment where they claim to have an answer to this question. But as the point at which this was created, a year and a half ago, until now, no one really knows why that happens. But the important thing the authors of this paper realized is if we can create a function that gives you content loss and a function that gives you style loss, and you add the two together and optimize them, you can do neural style. So all I can assume, they don't say how they did it in the paper, all I can assume is that they tried a few different things. They knew that they had to throw away all of the geometry, so they probably tried a few things and threw away the geometry. At some point they looked at this and said, holy shit, that's it. So now that we have this magical thing, there's the Simpsons, all we have to do is add the two together. Here's our bird, which I'll call Source. We've got our style layers, I'm actually going to take the top 5 now. Here's our content layer, I'm going to take block4conv2. As promised, for our loss function, I'm just going to add the two together. Style loss for all of the style layers plus the content loss. And I'm going to divide the content loss by 10. This is something you can play with. In the paper you'll see they play with it. How much style loss versus how much content loss. Get the gradients, evaluator, solve it, and there it is. So other than the fact that we don't really know why the style loss works, but it does, everything else kind of fits together. So there's the bird as Van Gogh, there's the bird as the Simpsons, and there's the bird in the style of a bird picture. Question 2 Question 2 Question 2 Since the publication of that paper, has anyone used any other loss functions for fstyle that achieve similar results? Answer Yes, so as I mentioned just a couple of weeks ago, there was a paper, I'll put it on the forum, that tries to generalize this loss function. It turns out actually that this particular loss function seems to be about the best that they could come up with. So it's 9 o'clock, so we have run out of time. So we're going to move some of this lesson to the next lesson. But to give you a sense of where we're going to head, what we're going to do is we're going to take this thing where you have to optimize every single image separately, and we're going to train a CNN, and we're going to train a CNN which will learn how to turn a picture into a Van Gogh version of that picture. So that's basically going to be what we're going to learn next time. And we're also going to learn about adversarial networks, which is where we're going to create two networks. One will be designed to generate pictures like this, and the other will be designed to try and classify whether this is a real Simpsons picture or a fake Simpsons picture. And then you'll do one, generate, the other, discriminate, generate, discriminate. And by doing that, we can take any generative model and make it better by basically having something else learn to pick the difference between it, the real, and the fake. Then finally we're going to learn about a particular thing that came out 3 weeks ago called the Wasserstein GAN, which is the reason I actually decided to move all this forward. Generative adversarial networks basically didn't work very well at all until about 3 weeks ago. Now that they do work, suddenly there's a shitload of stuff that nobody's done yet which you can do for the first time. So we're going to look at that next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.040000000000001, "text": " Some of you have finished Part 1 in the last few days, some of you finished Part 1 in December.", "tokens": [50364, 2188, 295, 291, 362, 4335, 4100, 502, 294, 264, 1036, 1326, 1708, 11, 512, 295, 291, 4335, 4100, 502, 294, 7687, 13, 50766], "temperature": 0.0, "avg_logprob": -0.31727655410766603, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.00046524469507858157}, {"id": 1, "seek": 0, "start": 8.040000000000001, "end": 13.8, "text": " I did ask those of you who took it in person to revise the material and make sure it was", "tokens": [50766, 286, 630, 1029, 729, 295, 291, 567, 1890, 309, 294, 954, 281, 44252, 264, 2527, 293, 652, 988, 309, 390, 51054], "temperature": 0.0, "avg_logprob": -0.31727655410766603, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.00046524469507858157}, {"id": 2, "seek": 0, "start": 13.8, "end": 14.8, "text": " up to date.", "tokens": [51054, 493, 281, 4002, 13, 51104], "temperature": 0.0, "avg_logprob": -0.31727655410766603, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.00046524469507858157}, {"id": 3, "seek": 0, "start": 14.8, "end": 20.28, "text": " But let's do a quick summary of the key things we learned.", "tokens": [51104, 583, 718, 311, 360, 257, 1702, 12691, 295, 264, 2141, 721, 321, 3264, 13, 51378], "temperature": 0.0, "avg_logprob": -0.31727655410766603, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.00046524469507858157}, {"id": 4, "seek": 0, "start": 20.28, "end": 22.84, "text": " I came up with these 5 things.", "tokens": [51378, 286, 1361, 493, 365, 613, 1025, 721, 13, 51506], "temperature": 0.0, "avg_logprob": -0.31727655410766603, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.00046524469507858157}, {"id": 5, "seek": 0, "start": 22.84, "end": 29.64, "text": " I'd be interested to hear if anybody has other key insights that they feel they came away", "tokens": [51506, 286, 1116, 312, 3102, 281, 1568, 498, 4472, 575, 661, 2141, 14310, 300, 436, 841, 436, 1361, 1314, 51846], "temperature": 0.0, "avg_logprob": -0.31727655410766603, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.00046524469507858157}, {"id": 6, "seek": 2964, "start": 29.64, "end": 30.64, "text": " from.", "tokens": [50364, 490, 13, 50414], "temperature": 0.0, "avg_logprob": -0.309439092874527, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.0010484635131433606}, {"id": 7, "seek": 2964, "start": 30.64, "end": 36.2, "text": " So the 5 things are these.", "tokens": [50414, 407, 264, 1025, 721, 366, 613, 13, 50692], "temperature": 0.0, "avg_logprob": -0.309439092874527, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.0010484635131433606}, {"id": 8, "seek": 2964, "start": 36.2, "end": 46.32, "text": " Stacks of differentiable nonlinear functions with lots of parameters solve nearly any predictive", "tokens": [50692, 745, 7424, 295, 819, 9364, 2107, 28263, 6828, 365, 3195, 295, 9834, 5039, 6217, 604, 35521, 51198], "temperature": 0.0, "avg_logprob": -0.309439092874527, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.0010484635131433606}, {"id": 9, "seek": 2964, "start": 46.32, "end": 47.72, "text": " modeling problem.", "tokens": [51198, 15983, 1154, 13, 51268], "temperature": 0.0, "avg_logprob": -0.309439092874527, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.0010484635131433606}, {"id": 10, "seek": 2964, "start": 47.72, "end": 53.68, "text": " So when we say neural network, a lot of people are suggesting we should use the phrase differentiable", "tokens": [51268, 407, 562, 321, 584, 18161, 3209, 11, 257, 688, 295, 561, 366, 18094, 321, 820, 764, 264, 9535, 819, 9364, 51566], "temperature": 0.0, "avg_logprob": -0.309439092874527, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.0010484635131433606}, {"id": 11, "seek": 2964, "start": 53.68, "end": 54.68, "text": " network.", "tokens": [51566, 3209, 13, 51616], "temperature": 0.0, "avg_logprob": -0.309439092874527, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.0010484635131433606}, {"id": 12, "seek": 5468, "start": 54.68, "end": 60.64, "text": " If you think about things like the collaborative filtering we did, it was really a couple of", "tokens": [50364, 759, 291, 519, 466, 721, 411, 264, 16555, 30822, 321, 630, 11, 309, 390, 534, 257, 1916, 295, 50662], "temperature": 0.0, "avg_logprob": -0.3059195267526727, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.00035143643617630005}, {"id": 13, "seek": 5468, "start": 60.64, "end": 62.6, "text": " embeddings and a dot product.", "tokens": [50662, 12240, 29432, 293, 257, 5893, 1674, 13, 50760], "temperature": 0.0, "avg_logprob": -0.3059195267526727, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.00035143643617630005}, {"id": 14, "seek": 5468, "start": 62.6, "end": 64.64, "text": " That put us quite a long way.", "tokens": [50760, 663, 829, 505, 1596, 257, 938, 636, 13, 50862], "temperature": 0.0, "avg_logprob": -0.3059195267526727, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.00035143643617630005}, {"id": 15, "seek": 5468, "start": 64.64, "end": 69.6, "text": " There's nothing very neural-looking about that.", "tokens": [50862, 821, 311, 1825, 588, 18161, 12, 16129, 466, 300, 13, 51110], "temperature": 0.0, "avg_logprob": -0.3059195267526727, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.00035143643617630005}, {"id": 16, "seek": 5468, "start": 69.6, "end": 75.44, "text": " But we know that when we stack certain kinds of nonlinear functions on top of each other,", "tokens": [51110, 583, 321, 458, 300, 562, 321, 8630, 1629, 3685, 295, 2107, 28263, 6828, 322, 1192, 295, 1184, 661, 11, 51402], "temperature": 0.0, "avg_logprob": -0.3059195267526727, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.00035143643617630005}, {"id": 17, "seek": 5468, "start": 75.44, "end": 81.56, "text": " the universal approximation theorem tells us that can approximate any computable function", "tokens": [51402, 264, 11455, 28023, 20904, 5112, 505, 300, 393, 30874, 604, 2807, 712, 2445, 51708], "temperature": 0.0, "avg_logprob": -0.3059195267526727, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.00035143643617630005}, {"id": 18, "seek": 5468, "start": 81.56, "end": 83.46000000000001, "text": " to arbitrary precision.", "tokens": [51708, 281, 23211, 18356, 13, 51803], "temperature": 0.0, "avg_logprob": -0.3059195267526727, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.00035143643617630005}, {"id": 19, "seek": 8346, "start": 83.46, "end": 89.05999999999999, "text": " We know that if it's differentiable, we can use SGD to find the parameters which match", "tokens": [50364, 492, 458, 300, 498, 309, 311, 819, 9364, 11, 321, 393, 764, 34520, 35, 281, 915, 264, 9834, 597, 2995, 50644], "temperature": 0.0, "avg_logprob": -0.271841253553118, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.00023050649906508625}, {"id": 20, "seek": 8346, "start": 89.05999999999999, "end": 91.25999999999999, "text": " that function.", "tokens": [50644, 300, 2445, 13, 50754], "temperature": 0.0, "avg_logprob": -0.271841253553118, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.00023050649906508625}, {"id": 21, "seek": 8346, "start": 91.25999999999999, "end": 96.58, "text": " So this to me is kind of like the key insight.", "tokens": [50754, 407, 341, 281, 385, 307, 733, 295, 411, 264, 2141, 11269, 13, 51020], "temperature": 0.0, "avg_logprob": -0.271841253553118, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.00023050649906508625}, {"id": 22, "seek": 8346, "start": 96.58, "end": 104.25999999999999, "text": " But some stacks of functions are better than others for some kinds of data and some kinds", "tokens": [51020, 583, 512, 30792, 295, 6828, 366, 1101, 813, 2357, 337, 512, 3685, 295, 1412, 293, 512, 3685, 51404], "temperature": 0.0, "avg_logprob": -0.271841253553118, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.00023050649906508625}, {"id": 23, "seek": 8346, "start": 104.25999999999999, "end": 106.86, "text": " of problems.", "tokens": [51404, 295, 2740, 13, 51534], "temperature": 0.0, "avg_logprob": -0.271841253553118, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.00023050649906508625}, {"id": 24, "seek": 8346, "start": 106.86, "end": 111.94, "text": " One way to make life very easy, we learned, is transfer learning.", "tokens": [51534, 1485, 636, 281, 652, 993, 588, 1858, 11, 321, 3264, 11, 307, 5003, 2539, 13, 51788], "temperature": 0.0, "avg_logprob": -0.271841253553118, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.00023050649906508625}, {"id": 25, "seek": 11194, "start": 111.94, "end": 119.42, "text": " I think nearly every network we created in the last course, we used transfer learning,", "tokens": [50364, 286, 519, 6217, 633, 3209, 321, 2942, 294, 264, 1036, 1164, 11, 321, 1143, 5003, 2539, 11, 50738], "temperature": 0.0, "avg_logprob": -0.3176229265001085, "compression_ratio": 1.6057692307692308, "no_speech_prob": 9.028030035551637e-05}, {"id": 26, "seek": 11194, "start": 119.42, "end": 123.46, "text": " I think particularly in vision and in text.", "tokens": [50738, 286, 519, 4098, 294, 5201, 293, 294, 2487, 13, 50940], "temperature": 0.0, "avg_logprob": -0.3176229265001085, "compression_ratio": 1.6057692307692308, "no_speech_prob": 9.028030035551637e-05}, {"id": 27, "seek": 11194, "start": 123.46, "end": 124.46, "text": " Pretty much everything.", "tokens": [50940, 10693, 709, 1203, 13, 50990], "temperature": 0.0, "avg_logprob": -0.3176229265001085, "compression_ratio": 1.6057692307692308, "no_speech_prob": 9.028030035551637e-05}, {"id": 28, "seek": 11194, "start": 124.46, "end": 130.9, "text": " So transfer learning generally was throw away the last layer, replace it with a new one", "tokens": [50990, 407, 5003, 2539, 5101, 390, 3507, 1314, 264, 1036, 4583, 11, 7406, 309, 365, 257, 777, 472, 51312], "temperature": 0.0, "avg_logprob": -0.3176229265001085, "compression_ratio": 1.6057692307692308, "no_speech_prob": 9.028030035551637e-05}, {"id": 29, "seek": 11194, "start": 130.9, "end": 137.5, "text": " that has the right number of outputs, pre-compute the penultimate layer's output, then very", "tokens": [51312, 300, 575, 264, 558, 1230, 295, 23930, 11, 659, 12, 21541, 1169, 264, 3435, 723, 2905, 4583, 311, 5598, 11, 550, 588, 51642], "temperature": 0.0, "avg_logprob": -0.3176229265001085, "compression_ratio": 1.6057692307692308, "no_speech_prob": 9.028030035551637e-05}, {"id": 30, "seek": 13750, "start": 137.5, "end": 143.62, "text": " quickly create a linear model that goes from that to your preferred answer.", "tokens": [50364, 2661, 1884, 257, 8213, 2316, 300, 1709, 490, 300, 281, 428, 16494, 1867, 13, 50670], "temperature": 0.0, "avg_logprob": -0.2814969930562887, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.06465102732181549}, {"id": 31, "seek": 13750, "start": 143.62, "end": 146.26, "text": " You now have something that works pretty well.", "tokens": [50670, 509, 586, 362, 746, 300, 1985, 1238, 731, 13, 50802], "temperature": 0.0, "avg_logprob": -0.2814969930562887, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.06465102732181549}, {"id": 32, "seek": 13750, "start": 146.26, "end": 150.66, "text": " And then you can fine-tune more and more layers backwards as necessary.", "tokens": [50802, 400, 550, 291, 393, 2489, 12, 83, 2613, 544, 293, 544, 7914, 12204, 382, 4818, 13, 51022], "temperature": 0.0, "avg_logprob": -0.2814969930562887, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.06465102732181549}, {"id": 33, "seek": 13750, "start": 150.66, "end": 155.22, "text": " And we learned that fine-tuning those additional layers, generally the best way to do that", "tokens": [51022, 400, 321, 3264, 300, 2489, 12, 83, 37726, 729, 4497, 7914, 11, 5101, 264, 1151, 636, 281, 360, 300, 51250], "temperature": 0.0, "avg_logprob": -0.2814969930562887, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.06465102732181549}, {"id": 34, "seek": 13750, "start": 155.22, "end": 159.86, "text": " was to pre-compute the last of the layers which you're not fine-tuning.", "tokens": [51250, 390, 281, 659, 12, 21541, 1169, 264, 1036, 295, 264, 7914, 597, 291, 434, 406, 2489, 12, 83, 37726, 13, 51482], "temperature": 0.0, "avg_logprob": -0.2814969930562887, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.06465102732181549}, {"id": 35, "seek": 13750, "start": 159.86, "end": 165.42000000000002, "text": " And so then you could just calculate the weights of the remaining ones, and that saved us lots", "tokens": [51482, 400, 370, 550, 291, 727, 445, 8873, 264, 17443, 295, 264, 8877, 2306, 11, 293, 300, 6624, 505, 3195, 51760], "temperature": 0.0, "avg_logprob": -0.2814969930562887, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.06465102732181549}, {"id": 36, "seek": 16542, "start": 165.42, "end": 168.1, "text": " and lots of time.", "tokens": [50364, 293, 3195, 295, 565, 13, 50498], "temperature": 0.0, "avg_logprob": -0.5669600963592529, "compression_ratio": 1.0, "no_speech_prob": 0.5888529419898987}, {"id": 37, "seek": 16542, "start": 168.1, "end": 193.26, "text": " And remember that convolutional layers are slower.", "tokens": [50498, 400, 1604, 300, 45216, 304, 7914, 366, 14009, 13, 51756], "temperature": 0.0, "avg_logprob": -0.5669600963592529, "compression_ratio": 1.0, "no_speech_prob": 0.5888529419898987}, {"id": 38, "seek": 19326, "start": 194.26, "end": 197.26, "text": " Dense layers are bigger.", "tokens": [50414, 413, 1288, 7914, 366, 3801, 13, 50564], "temperature": 0.0, "avg_logprob": -0.32029124173251067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.11436330527067184}, {"id": 39, "seek": 19326, "start": 197.26, "end": 201.89999999999998, "text": " There's an interesting question I've added here.", "tokens": [50564, 821, 311, 364, 1880, 1168, 286, 600, 3869, 510, 13, 50796], "temperature": 0.0, "avg_logprob": -0.32029124173251067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.11436330527067184}, {"id": 40, "seek": 19326, "start": 201.89999999999998, "end": 208.22, "text": " Remember in the last lesson we kind of looked at ResNets and InceptionNets, and in general", "tokens": [50796, 5459, 294, 264, 1036, 6898, 321, 733, 295, 2956, 412, 5015, 45, 1385, 293, 682, 7311, 45, 1385, 11, 293, 294, 2674, 51112], "temperature": 0.0, "avg_logprob": -0.32029124173251067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.11436330527067184}, {"id": 41, "seek": 19326, "start": 208.22, "end": 213.06, "text": " more modern nets tend not to have any dense layers.", "tokens": [51112, 544, 4363, 36170, 3928, 406, 281, 362, 604, 18011, 7914, 13, 51354], "temperature": 0.0, "avg_logprob": -0.32029124173251067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.11436330527067184}, {"id": 42, "seek": 19326, "start": 213.06, "end": 215.78, "text": " So what's the best way to do transfer learning?", "tokens": [51354, 407, 437, 311, 264, 1151, 636, 281, 360, 5003, 2539, 30, 51490], "temperature": 0.0, "avg_logprob": -0.32029124173251067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.11436330527067184}, {"id": 43, "seek": 19326, "start": 215.78, "end": 217.78, "text": " I'm going to leave that as an open question for now.", "tokens": [51490, 286, 478, 516, 281, 1856, 300, 382, 364, 1269, 1168, 337, 586, 13, 51590], "temperature": 0.0, "avg_logprob": -0.32029124173251067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.11436330527067184}, {"id": 44, "seek": 19326, "start": 217.78, "end": 222.18, "text": " We're going to look into it a bit during this class, but it's not a question that anybody", "tokens": [51590, 492, 434, 516, 281, 574, 666, 309, 257, 857, 1830, 341, 1508, 11, 457, 309, 311, 406, 257, 1168, 300, 4472, 51810], "temperature": 0.0, "avg_logprob": -0.32029124173251067, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.11436330527067184}, {"id": 45, "seek": 22218, "start": 222.18, "end": 224.66, "text": " has answered to my satisfaction.", "tokens": [50364, 575, 10103, 281, 452, 18715, 13, 50488], "temperature": 0.0, "avg_logprob": -0.2899806764390733, "compression_ratio": 1.5674418604651164, "no_speech_prob": 0.004133922979235649}, {"id": 46, "seek": 22218, "start": 224.66, "end": 228.9, "text": " So I'll suggest some ideas.", "tokens": [50488, 407, 286, 603, 3402, 512, 3487, 13, 50700], "temperature": 0.0, "avg_logprob": -0.2899806764390733, "compression_ratio": 1.5674418604651164, "no_speech_prob": 0.004133922979235649}, {"id": 47, "seek": 22218, "start": 228.9, "end": 237.74, "text": " No one's even written a paper that attempts to address it, as far as I'm aware.", "tokens": [50700, 883, 472, 311, 754, 3720, 257, 3035, 300, 15257, 281, 2985, 309, 11, 382, 1400, 382, 286, 478, 3650, 13, 51142], "temperature": 0.0, "avg_logprob": -0.2899806764390733, "compression_ratio": 1.5674418604651164, "no_speech_prob": 0.004133922979235649}, {"id": 48, "seek": 22218, "start": 237.74, "end": 241.10000000000002, "text": " Given we have transfer learning to get us a long way, the next thing we have to get", "tokens": [51142, 18600, 321, 362, 5003, 2539, 281, 483, 505, 257, 938, 636, 11, 264, 958, 551, 321, 362, 281, 483, 51310], "temperature": 0.0, "avg_logprob": -0.2899806764390733, "compression_ratio": 1.5674418604651164, "no_speech_prob": 0.004133922979235649}, {"id": 49, "seek": 22218, "start": 241.10000000000002, "end": 247.98000000000002, "text": " us a long way is to try and create an architecture which suits our problem, both our data and", "tokens": [51310, 505, 257, 938, 636, 307, 281, 853, 293, 1884, 364, 9482, 597, 15278, 527, 1154, 11, 1293, 527, 1412, 293, 51654], "temperature": 0.0, "avg_logprob": -0.2899806764390733, "compression_ratio": 1.5674418604651164, "no_speech_prob": 0.004133922979235649}, {"id": 50, "seek": 22218, "start": 247.98000000000002, "end": 249.70000000000002, "text": " our loss function.", "tokens": [51654, 527, 4470, 2445, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2899806764390733, "compression_ratio": 1.5674418604651164, "no_speech_prob": 0.004133922979235649}, {"id": 51, "seek": 24970, "start": 249.7, "end": 257.74, "text": " So for example, if we have autocorrelated inputs, so in other words, each input is related", "tokens": [50364, 407, 337, 1365, 11, 498, 321, 362, 45833, 284, 12004, 15743, 11, 370, 294, 661, 2283, 11, 1184, 4846, 307, 4077, 50766], "temperature": 0.0, "avg_logprob": -0.2890534834428267, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0009850045898929238}, {"id": 52, "seek": 24970, "start": 257.74, "end": 258.74, "text": " to the previous input.", "tokens": [50766, 281, 264, 3894, 4846, 13, 50816], "temperature": 0.0, "avg_logprob": -0.2890534834428267, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0009850045898929238}, {"id": 53, "seek": 24970, "start": 258.74, "end": 264.34, "text": " So each pixel is similar to the next door pixel, or in a sound wave, each sample is", "tokens": [50816, 407, 1184, 19261, 307, 2531, 281, 264, 958, 2853, 19261, 11, 420, 294, 257, 1626, 5772, 11, 1184, 6889, 307, 51096], "temperature": 0.0, "avg_logprob": -0.2890534834428267, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0009850045898929238}, {"id": 54, "seek": 24970, "start": 264.34, "end": 267.53999999999996, "text": " somewhat similar to the previous sample, something like that.", "tokens": [51096, 8344, 2531, 281, 264, 3894, 6889, 11, 746, 411, 300, 13, 51256], "temperature": 0.0, "avg_logprob": -0.2890534834428267, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0009850045898929238}, {"id": 55, "seek": 24970, "start": 267.53999999999996, "end": 273.02, "text": " That kind of data we tend to like to use CNNs for, as long as it's of a fixed size.", "tokens": [51256, 663, 733, 295, 1412, 321, 3928, 281, 411, 281, 764, 24859, 82, 337, 11, 382, 938, 382, 309, 311, 295, 257, 6806, 2744, 13, 51530], "temperature": 0.0, "avg_logprob": -0.2890534834428267, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0009850045898929238}, {"id": 56, "seek": 24970, "start": 273.02, "end": 277.14, "text": " If it's a sequence, we like to use an RNN for.", "tokens": [51530, 759, 309, 311, 257, 8310, 11, 321, 411, 281, 764, 364, 45702, 45, 337, 13, 51736], "temperature": 0.0, "avg_logprob": -0.2890534834428267, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0009850045898929238}, {"id": 57, "seek": 27714, "start": 277.14, "end": 280.7, "text": " If it's a categorical output, we like to use a Softmax for.", "tokens": [50364, 759, 309, 311, 257, 19250, 804, 5598, 11, 321, 411, 281, 764, 257, 16985, 41167, 337, 13, 50542], "temperature": 0.0, "avg_logprob": -0.27490593039471173, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.0013670233311131597}, {"id": 58, "seek": 27714, "start": 280.7, "end": 285.82, "text": " So there are ways we learned of tuning our architecture, not so that it makes it possible", "tokens": [50542, 407, 456, 366, 2098, 321, 3264, 295, 15164, 527, 9482, 11, 406, 370, 300, 309, 1669, 309, 1944, 50798], "temperature": 0.0, "avg_logprob": -0.27490593039471173, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.0013670233311131597}, {"id": 59, "seek": 27714, "start": 285.82, "end": 292.78, "text": " to solve a problem, because any standard dense network can solve any problem, but it just", "tokens": [50798, 281, 5039, 257, 1154, 11, 570, 604, 3832, 18011, 3209, 393, 5039, 604, 1154, 11, 457, 309, 445, 51146], "temperature": 0.0, "avg_logprob": -0.27490593039471173, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.0013670233311131597}, {"id": 60, "seek": 27714, "start": 292.78, "end": 300.32, "text": " makes it a lot faster and a lot easier to train if you've made sure that your activation", "tokens": [51146, 1669, 309, 257, 688, 4663, 293, 257, 688, 3571, 281, 3847, 498, 291, 600, 1027, 988, 300, 428, 24433, 51523], "temperature": 0.0, "avg_logprob": -0.27490593039471173, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.0013670233311131597}, {"id": 61, "seek": 27714, "start": 300.32, "end": 304.41999999999996, "text": " functions and your architecture suit the problem.", "tokens": [51523, 6828, 293, 428, 9482, 5722, 264, 1154, 13, 51728], "temperature": 0.0, "avg_logprob": -0.27490593039471173, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.0013670233311131597}, {"id": 62, "seek": 30442, "start": 304.82, "end": 312.14000000000004, "text": " So that was another key thing I think we learned.", "tokens": [50384, 407, 300, 390, 1071, 2141, 551, 286, 519, 321, 3264, 13, 50750], "temperature": 0.0, "avg_logprob": -0.31009624530742697, "compression_ratio": 1.5245098039215685, "no_speech_prob": 0.0010162409162148833}, {"id": 63, "seek": 30442, "start": 312.14000000000004, "end": 318.34000000000003, "text": " Something I hope that everybody can narrate is the 5 steps to avoiding overfitting.", "tokens": [50750, 6595, 286, 1454, 300, 2201, 393, 6397, 473, 307, 264, 1025, 4439, 281, 20220, 670, 69, 2414, 13, 51060], "temperature": 0.0, "avg_logprob": -0.31009624530742697, "compression_ratio": 1.5245098039215685, "no_speech_prob": 0.0010162409162148833}, {"id": 64, "seek": 30442, "start": 318.34000000000003, "end": 323.90000000000003, "text": " If you've forgotten them, they're both here and discussed in more detail in Lesson 3.", "tokens": [51060, 759, 291, 600, 11832, 552, 11, 436, 434, 1293, 510, 293, 7152, 294, 544, 2607, 294, 18649, 266, 805, 13, 51338], "temperature": 0.0, "avg_logprob": -0.31009624530742697, "compression_ratio": 1.5245098039215685, "no_speech_prob": 0.0010162409162148833}, {"id": 65, "seek": 30442, "start": 323.90000000000003, "end": 331.86, "text": " Get more data, fake more data using data augmentation, use more generalizable architecture.", "tokens": [51338, 3240, 544, 1412, 11, 7592, 544, 1412, 1228, 1412, 14501, 19631, 11, 764, 544, 2674, 22395, 9482, 13, 51736], "temperature": 0.0, "avg_logprob": -0.31009624530742697, "compression_ratio": 1.5245098039215685, "no_speech_prob": 0.0010162409162148833}, {"id": 66, "seek": 33186, "start": 332.38, "end": 337.26, "text": " Architectures are generalized well, particularly we look at batch normalization.", "tokens": [50390, 29306, 1303, 366, 44498, 731, 11, 4098, 321, 574, 412, 15245, 2710, 2144, 13, 50634], "temperature": 0.0, "avg_logprob": -0.3602092701901672, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.00035143777495250106}, {"id": 67, "seek": 33186, "start": 337.26, "end": 344.82, "text": " Use regularization techniques as few as we can, because by definition they destroy some", "tokens": [50634, 8278, 3890, 2144, 7512, 382, 1326, 382, 321, 393, 11, 570, 538, 7123, 436, 5293, 512, 51012], "temperature": 0.0, "avg_logprob": -0.3602092701901672, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.00035143777495250106}, {"id": 68, "seek": 33186, "start": 344.82, "end": 349.38, "text": " data, but we look particularly at using dropout.", "tokens": [51012, 1412, 11, 457, 321, 574, 4098, 412, 1228, 3270, 346, 13, 51240], "temperature": 0.0, "avg_logprob": -0.3602092701901672, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.00035143777495250106}, {"id": 69, "seek": 33186, "start": 349.38, "end": 354.42, "text": " And then finally if we have to, we can look at reducing the complexity of the architecture.", "tokens": [51240, 400, 550, 2721, 498, 321, 362, 281, 11, 321, 393, 574, 412, 12245, 264, 14024, 295, 264, 9482, 13, 51492], "temperature": 0.0, "avg_logprob": -0.3602092701901672, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.00035143777495250106}, {"id": 70, "seek": 33186, "start": 354.42, "end": 361.82, "text": " The general approach we learned, this is absolutely key, is first of all, with a new problem,", "tokens": [51492, 440, 2674, 3109, 321, 3264, 11, 341, 307, 3122, 2141, 11, 307, 700, 295, 439, 11, 365, 257, 777, 1154, 11, 51862], "temperature": 0.0, "avg_logprob": -0.3602092701901672, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.00035143777495250106}, {"id": 71, "seek": 36182, "start": 362.78, "end": 367.78, "text": " start with a network that's too big, that's not regularized, that can't help but solve", "tokens": [50412, 722, 365, 257, 3209, 300, 311, 886, 955, 11, 300, 311, 406, 3890, 1602, 11, 300, 393, 380, 854, 457, 5039, 50662], "temperature": 0.0, "avg_logprob": -0.26223251147147936, "compression_ratio": 1.73046875, "no_speech_prob": 0.0035935796331614256}, {"id": 72, "seek": 36182, "start": 367.78, "end": 372.74, "text": " the problem, even if it has to overfit terribly.", "tokens": [50662, 264, 1154, 11, 754, 498, 309, 575, 281, 670, 6845, 22903, 13, 50910], "temperature": 0.0, "avg_logprob": -0.26223251147147936, "compression_ratio": 1.73046875, "no_speech_prob": 0.0035935796331614256}, {"id": 73, "seek": 36182, "start": 372.74, "end": 376.82, "text": " If you can't do that, there's no point in starting to regularize yet.", "tokens": [50910, 759, 291, 393, 380, 360, 300, 11, 456, 311, 572, 935, 294, 2891, 281, 3890, 1125, 1939, 13, 51114], "temperature": 0.0, "avg_logprob": -0.26223251147147936, "compression_ratio": 1.73046875, "no_speech_prob": 0.0035935796331614256}, {"id": 74, "seek": 36182, "start": 376.82, "end": 379.78, "text": " So we start out by trying to overfit terribly.", "tokens": [51114, 407, 321, 722, 484, 538, 1382, 281, 670, 6845, 22903, 13, 51262], "temperature": 0.0, "avg_logprob": -0.26223251147147936, "compression_ratio": 1.73046875, "no_speech_prob": 0.0035935796331614256}, {"id": 75, "seek": 36182, "start": 379.78, "end": 383.78, "text": " Once we've got to the point that we're getting 100% accuracy and our validation set is terrible", "tokens": [51262, 3443, 321, 600, 658, 281, 264, 935, 300, 321, 434, 1242, 2319, 4, 14170, 293, 527, 24071, 992, 307, 6237, 51462], "temperature": 0.0, "avg_logprob": -0.26223251147147936, "compression_ratio": 1.73046875, "no_speech_prob": 0.0035935796331614256}, {"id": 76, "seek": 36182, "start": 383.78, "end": 391.14, "text": " because it's overfitting, then we start going through these steps until we get a nice balance.", "tokens": [51462, 570, 309, 311, 670, 69, 2414, 11, 550, 321, 722, 516, 807, 613, 4439, 1826, 321, 483, 257, 1481, 4772, 13, 51830], "temperature": 0.0, "avg_logprob": -0.26223251147147936, "compression_ratio": 1.73046875, "no_speech_prob": 0.0035935796331614256}, {"id": 77, "seek": 39114, "start": 391.46, "end": 394.34, "text": " So that's kind of the process that we learned.", "tokens": [50380, 407, 300, 311, 733, 295, 264, 1399, 300, 321, 3264, 13, 50524], "temperature": 0.0, "avg_logprob": -0.3811292913224962, "compression_ratio": 1.5625, "no_speech_prob": 0.00015843595610931516}, {"id": 78, "seek": 39114, "start": 394.34, "end": 400.86, "text": " And then finally, we learned about embeddings as a technique to allow us to use categorical", "tokens": [50524, 400, 550, 2721, 11, 321, 3264, 466, 12240, 29432, 382, 257, 6532, 281, 2089, 505, 281, 764, 19250, 804, 50850], "temperature": 0.0, "avg_logprob": -0.3811292913224962, "compression_ratio": 1.5625, "no_speech_prob": 0.00015843595610931516}, {"id": 79, "seek": 39114, "start": 400.86, "end": 408.74, "text": " data and specifically the idea of using words or the idea of using latent variables.", "tokens": [50850, 1412, 293, 4682, 264, 1558, 295, 1228, 2283, 420, 264, 1558, 295, 1228, 48994, 9102, 13, 51244], "temperature": 0.0, "avg_logprob": -0.3811292913224962, "compression_ratio": 1.5625, "no_speech_prob": 0.00015843595610931516}, {"id": 80, "seek": 39114, "start": 408.74, "end": 418.65999999999997, "text": " So in this case, this was the MovieLens dataset for collaborative filtering.", "tokens": [51244, 407, 294, 341, 1389, 11, 341, 390, 264, 28766, 43, 694, 28872, 337, 16555, 30822, 13, 51740], "temperature": 0.0, "avg_logprob": -0.3811292913224962, "compression_ratio": 1.5625, "no_speech_prob": 0.00015843595610931516}, {"id": 81, "seek": 41866, "start": 418.66, "end": 422.02000000000004, "text": " That's the five main insights I thought of.", "tokens": [50364, 663, 311, 264, 1732, 2135, 14310, 286, 1194, 295, 13, 50532], "temperature": 0.0, "avg_logprob": -0.4730013529459635, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0169147290289402}, {"id": 82, "seek": 41866, "start": 422.02000000000004, "end": 430.98, "text": " Did anybody have any other key takeaways that they think people revising should think about", "tokens": [50532, 2589, 4472, 362, 604, 661, 2141, 45584, 300, 436, 519, 561, 3698, 3436, 820, 519, 466, 50980], "temperature": 0.0, "avg_logprob": -0.4730013529459635, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0169147290289402}, {"id": 83, "seek": 41866, "start": 430.98, "end": 433.98, "text": " or remember or things they found interesting?", "tokens": [50980, 420, 1604, 420, 721, 436, 1352, 1880, 30, 51130], "temperature": 0.0, "avg_logprob": -0.4730013529459635, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0169147290289402}, {"id": 84, "seek": 41866, "start": 433.98, "end": 440.22, "text": " If you come up with something, let me know.", "tokens": [51130, 759, 291, 808, 493, 365, 746, 11, 718, 385, 458, 13, 51442], "temperature": 0.0, "avg_logprob": -0.4730013529459635, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0169147290289402}, {"id": 85, "seek": 41866, "start": 440.22, "end": 442.94000000000005, "text": " Question 2.", "tokens": [51442, 14464, 568, 13, 51578], "temperature": 0.0, "avg_logprob": -0.4730013529459635, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0169147290289402}, {"id": 86, "seek": 41866, "start": 442.94000000000005, "end": 446.70000000000005, "text": " How does having duplicates and training data affect the model created?", "tokens": [51578, 1012, 775, 1419, 17154, 1024, 293, 3097, 1412, 3345, 264, 2316, 2942, 30, 51766], "temperature": 0.0, "avg_logprob": -0.4730013529459635, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0169147290289402}, {"id": 87, "seek": 44670, "start": 446.74, "end": 452.74, "text": " And if you're using data augmentation, do you end up with duplicate input data?", "tokens": [50366, 400, 498, 291, 434, 1228, 1412, 14501, 19631, 11, 360, 291, 917, 493, 365, 23976, 4846, 1412, 30, 50666], "temperature": 0.0, "avg_logprob": -0.3821741450916637, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.013848385773599148}, {"id": 88, "seek": 44670, "start": 452.74, "end": 453.74, "text": " Answer.", "tokens": [50666, 24545, 13, 50716], "temperature": 0.0, "avg_logprob": -0.3821741450916637, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.013848385773599148}, {"id": 89, "seek": 44670, "start": 453.74, "end": 461.02, "text": " Duplicates in the input data, it's not a big deal.", "tokens": [50716, 5153, 4770, 1024, 294, 264, 4846, 1412, 11, 309, 311, 406, 257, 955, 2028, 13, 51080], "temperature": 0.0, "avg_logprob": -0.3821741450916637, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.013848385773599148}, {"id": 90, "seek": 44670, "start": 461.02, "end": 465.38, "text": " We shuffle the batch and then you select things randomly.", "tokens": [51080, 492, 39426, 264, 15245, 293, 550, 291, 3048, 721, 16979, 13, 51298], "temperature": 0.0, "avg_logprob": -0.3821741450916637, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.013848385773599148}, {"id": 91, "seek": 44670, "start": 465.38, "end": 471.14, "text": " Effectively you're weighting that data point higher than its neighbors.", "tokens": [51298, 17764, 3413, 291, 434, 3364, 278, 300, 1412, 935, 2946, 813, 1080, 12512, 13, 51586], "temperature": 0.0, "avg_logprob": -0.3821741450916637, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.013848385773599148}, {"id": 92, "seek": 44670, "start": 471.14, "end": 474.02, "text": " In a big dataset, it's going to make very little difference.", "tokens": [51586, 682, 257, 955, 28872, 11, 309, 311, 516, 281, 652, 588, 707, 2649, 13, 51730], "temperature": 0.0, "avg_logprob": -0.3821741450916637, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.013848385773599148}, {"id": 93, "seek": 47402, "start": 474.02, "end": 477.46, "text": " If you've got one thing repeated a thousand times and there's only another hundred data", "tokens": [50364, 759, 291, 600, 658, 472, 551, 10477, 257, 4714, 1413, 293, 456, 311, 787, 1071, 3262, 1412, 50536], "temperature": 0.0, "avg_logprob": -0.29881625309168736, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005469189956784248}, {"id": 94, "seek": 47402, "start": 477.46, "end": 480.82, "text": " points, that's going to be a big problem because you're weighting one data point a thousand", "tokens": [50536, 2793, 11, 300, 311, 516, 281, 312, 257, 955, 1154, 570, 291, 434, 3364, 278, 472, 1412, 935, 257, 4714, 50704], "temperature": 0.0, "avg_logprob": -0.29881625309168736, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005469189956784248}, {"id": 95, "seek": 47402, "start": 480.82, "end": 485.9, "text": " times higher.", "tokens": [50704, 1413, 2946, 13, 50958], "temperature": 0.0, "avg_logprob": -0.29881625309168736, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005469189956784248}, {"id": 96, "seek": 47402, "start": 485.9, "end": 492.09999999999997, "text": " So as you will have seen, we've got a couple of big technology foundation changes.", "tokens": [50958, 407, 382, 291, 486, 362, 1612, 11, 321, 600, 658, 257, 1916, 295, 955, 2899, 7030, 2962, 13, 51268], "temperature": 0.0, "avg_logprob": -0.29881625309168736, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005469189956784248}, {"id": 97, "seek": 47402, "start": 492.09999999999997, "end": 496.65999999999997, "text": " The first one is we're moving from Python 2 to Python 3.", "tokens": [51268, 440, 700, 472, 307, 321, 434, 2684, 490, 15329, 568, 281, 15329, 805, 13, 51496], "temperature": 0.0, "avg_logprob": -0.29881625309168736, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005469189956784248}, {"id": 98, "seek": 47402, "start": 496.65999999999997, "end": 503.21999999999997, "text": " Python 2 I think is a good place to start given that a lot of the folks in Part 1 had", "tokens": [51496, 15329, 568, 286, 519, 307, 257, 665, 1081, 281, 722, 2212, 300, 257, 688, 295, 264, 4024, 294, 4100, 502, 632, 51824], "temperature": 0.0, "avg_logprob": -0.29881625309168736, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005469189956784248}, {"id": 99, "seek": 50322, "start": 503.42, "end": 509.3, "text": " never coded in Python before and many of them had never written very substantial pieces", "tokens": [50374, 1128, 34874, 294, 15329, 949, 293, 867, 295, 552, 632, 1128, 3720, 588, 16726, 3755, 50668], "temperature": 0.0, "avg_logprob": -0.3466125183105469, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.04535172879695892}, {"id": 100, "seek": 50322, "start": 509.3, "end": 511.38000000000005, "text": " of software before.", "tokens": [50668, 295, 4722, 949, 13, 50772], "temperature": 0.0, "avg_logprob": -0.3466125183105469, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.04535172879695892}, {"id": 101, "seek": 50322, "start": 511.38000000000005, "end": 516.3000000000001, "text": " A lot of the tutorials out there, like for example one of our preferred starting points,", "tokens": [50772, 316, 688, 295, 264, 17616, 484, 456, 11, 411, 337, 1365, 472, 295, 527, 16494, 2891, 2793, 11, 51018], "temperature": 0.0, "avg_logprob": -0.3466125183105469, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.04535172879695892}, {"id": 102, "seek": 50322, "start": 516.3000000000001, "end": 521.1, "text": " which is learn Python the hard way, is in Python 2, a lot of the existing codes out", "tokens": [51018, 597, 307, 1466, 15329, 264, 1152, 636, 11, 307, 294, 15329, 568, 11, 257, 688, 295, 264, 6741, 14211, 484, 51258], "temperature": 0.0, "avg_logprob": -0.3466125183105469, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.04535172879695892}, {"id": 103, "seek": 50322, "start": 521.1, "end": 522.1, "text": " there are in Python 2.", "tokens": [51258, 456, 366, 294, 15329, 568, 13, 51308], "temperature": 0.0, "avg_logprob": -0.3466125183105469, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.04535172879695892}, {"id": 104, "seek": 50322, "start": 522.1, "end": 524.1, "text": " So I thought Python 2 is a good place to start.", "tokens": [51308, 407, 286, 1194, 15329, 568, 307, 257, 665, 1081, 281, 722, 13, 51408], "temperature": 0.0, "avg_logprob": -0.3466125183105469, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.04535172879695892}, {"id": 105, "seek": 50322, "start": 524.1, "end": 525.1, "text": " Yes, Rachel?", "tokens": [51408, 1079, 11, 14246, 30, 51458], "temperature": 0.0, "avg_logprob": -0.3466125183105469, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.04535172879695892}, {"id": 106, "seek": 50322, "start": 525.1, "end": 526.1, "text": " 2 more questions.", "tokens": [51458, 568, 544, 1651, 13, 51508], "temperature": 0.0, "avg_logprob": -0.3466125183105469, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.04535172879695892}, {"id": 107, "seek": 50322, "start": 526.1, "end": 528.78, "text": " One is are you going to post the slides after this?", "tokens": [51508, 1485, 307, 366, 291, 516, 281, 2183, 264, 9788, 934, 341, 30, 51642], "temperature": 0.0, "avg_logprob": -0.3466125183105469, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.04535172879695892}, {"id": 108, "seek": 50322, "start": 528.78, "end": 530.4200000000001, "text": " I will post the slides, yes.", "tokens": [51642, 286, 486, 2183, 264, 9788, 11, 2086, 13, 51724], "temperature": 0.0, "avg_logprob": -0.3466125183105469, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.04535172879695892}, {"id": 109, "seek": 53042, "start": 530.42, "end": 534.6999999999999, "text": " And the other is could you go through steps for underfitting at some point?", "tokens": [50364, 400, 264, 661, 307, 727, 291, 352, 807, 4439, 337, 833, 69, 2414, 412, 512, 935, 30, 50578], "temperature": 0.0, "avg_logprob": -0.3771078036381648, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.17552146315574646}, {"id": 110, "seek": 53042, "start": 534.6999999999999, "end": 538.14, "text": " Do you have the steps for how to deal with overfitting?", "tokens": [50578, 1144, 291, 362, 264, 4439, 337, 577, 281, 2028, 365, 670, 69, 2414, 30, 50750], "temperature": 0.0, "avg_logprob": -0.3771078036381648, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.17552146315574646}, {"id": 111, "seek": 53042, "start": 538.14, "end": 539.9799999999999, "text": " Yes, let's do that in a forum thread.", "tokens": [50750, 1079, 11, 718, 311, 360, 300, 294, 257, 17542, 7207, 13, 50842], "temperature": 0.0, "avg_logprob": -0.3771078036381648, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.17552146315574646}, {"id": 112, "seek": 53042, "start": 539.9799999999999, "end": 542.9799999999999, "text": " Why don't you create a forum thread asking about underfitting.", "tokens": [50842, 1545, 500, 380, 291, 1884, 257, 17542, 7207, 3365, 466, 833, 69, 2414, 13, 50992], "temperature": 0.0, "avg_logprob": -0.3771078036381648, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.17552146315574646}, {"id": 113, "seek": 53042, "start": 542.9799999999999, "end": 547.5, "text": " But you don't need to do that in the Part 2 forum, you can do that in the main forum", "tokens": [50992, 583, 291, 500, 380, 643, 281, 360, 300, 294, 264, 4100, 568, 17542, 11, 291, 393, 360, 300, 294, 264, 2135, 17542, 51218], "temperature": 0.0, "avg_logprob": -0.3771078036381648, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.17552146315574646}, {"id": 114, "seek": 53042, "start": 547.5, "end": 551.02, "text": " because lots of people would be interested in hearing about that.", "tokens": [51218, 570, 3195, 295, 561, 576, 312, 3102, 294, 4763, 466, 300, 13, 51394], "temperature": 0.0, "avg_logprob": -0.3771078036381648, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.17552146315574646}, {"id": 115, "seek": 55102, "start": 551.02, "end": 561.74, "text": " If you want to revise that, Lesson 3 started by talking about underfitting.", "tokens": [50364, 759, 291, 528, 281, 44252, 300, 11, 18649, 266, 805, 1409, 538, 1417, 466, 833, 69, 2414, 13, 50900], "temperature": 0.0, "avg_logprob": -0.29384955539498275, "compression_ratio": 1.5248868778280542, "no_speech_prob": 0.05032920092344284}, {"id": 116, "seek": 55102, "start": 561.74, "end": 564.46, "text": " So that seemed like a good place to start.", "tokens": [50900, 407, 300, 6576, 411, 257, 665, 1081, 281, 722, 13, 51036], "temperature": 0.0, "avg_logprob": -0.29384955539498275, "compression_ratio": 1.5248868778280542, "no_speech_prob": 0.05032920092344284}, {"id": 117, "seek": 55102, "start": 564.46, "end": 568.26, "text": " I don't think we should keep using Python 2 though for a number of reasons.", "tokens": [51036, 286, 500, 380, 519, 321, 820, 1066, 1228, 15329, 568, 1673, 337, 257, 1230, 295, 4112, 13, 51226], "temperature": 0.0, "avg_logprob": -0.29384955539498275, "compression_ratio": 1.5248868778280542, "no_speech_prob": 0.05032920092344284}, {"id": 118, "seek": 55102, "start": 568.26, "end": 572.62, "text": " One is that since then, the IPython folks have come out and said that the next version", "tokens": [51226, 1485, 307, 300, 1670, 550, 11, 264, 8671, 88, 11943, 4024, 362, 808, 484, 293, 848, 300, 264, 958, 3037, 51444], "temperature": 0.0, "avg_logprob": -0.29384955539498275, "compression_ratio": 1.5248868778280542, "no_speech_prob": 0.05032920092344284}, {"id": 119, "seek": 55102, "start": 572.62, "end": 574.42, "text": " won't be compatible with Python 2.", "tokens": [51444, 1582, 380, 312, 18218, 365, 15329, 568, 13, 51534], "temperature": 0.0, "avg_logprob": -0.29384955539498275, "compression_ratio": 1.5248868778280542, "no_speech_prob": 0.05032920092344284}, {"id": 120, "seek": 55102, "start": 574.42, "end": 577.02, "text": " So that's a problem.", "tokens": [51534, 407, 300, 311, 257, 1154, 13, 51664], "temperature": 0.0, "avg_logprob": -0.29384955539498275, "compression_ratio": 1.5248868778280542, "no_speech_prob": 0.05032920092344284}, {"id": 121, "seek": 57702, "start": 577.02, "end": 583.14, "text": " Indeed from 2020 onwards, Python 2 will be end-of-life, which means there won't be patches", "tokens": [50364, 15061, 490, 4808, 34230, 11, 15329, 568, 486, 312, 917, 12, 2670, 12, 9073, 11, 597, 1355, 456, 1582, 380, 312, 26531, 50670], "temperature": 0.0, "avg_logprob": -0.2955206036567688, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.13844721019268036}, {"id": 122, "seek": 57702, "start": 583.14, "end": 584.14, "text": " for it.", "tokens": [50670, 337, 309, 13, 50720], "temperature": 0.0, "avg_logprob": -0.2955206036567688, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.13844721019268036}, {"id": 123, "seek": 57702, "start": 584.14, "end": 585.14, "text": " So that's a problem.", "tokens": [50720, 407, 300, 311, 257, 1154, 13, 50770], "temperature": 0.0, "avg_logprob": -0.2955206036567688, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.13844721019268036}, {"id": 124, "seek": 57702, "start": 585.14, "end": 589.02, "text": " Also, we're going to be doing more stuff with concurrency and parallel programming this", "tokens": [50770, 2743, 11, 321, 434, 516, 281, 312, 884, 544, 1507, 365, 23702, 10457, 293, 8952, 9410, 341, 50964], "temperature": 0.0, "avg_logprob": -0.2955206036567688, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.13844721019268036}, {"id": 125, "seek": 57702, "start": 589.02, "end": 591.34, "text": " time around.", "tokens": [50964, 565, 926, 13, 51080], "temperature": 0.0, "avg_logprob": -0.2955206036567688, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.13844721019268036}, {"id": 126, "seek": 57702, "start": 591.34, "end": 594.18, "text": " The features in Python 3 are a lot better.", "tokens": [51080, 440, 4122, 294, 15329, 805, 366, 257, 688, 1101, 13, 51222], "temperature": 0.0, "avg_logprob": -0.2955206036567688, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.13844721019268036}, {"id": 127, "seek": 57702, "start": 594.18, "end": 598.6999999999999, "text": " And then Python 3.6 was just released, which has some very nice features in particular,", "tokens": [51222, 400, 550, 15329, 805, 13, 21, 390, 445, 4736, 11, 597, 575, 512, 588, 1481, 4122, 294, 1729, 11, 51448], "temperature": 0.0, "avg_logprob": -0.2955206036567688, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.13844721019268036}, {"id": 128, "seek": 57702, "start": 598.6999999999999, "end": 602.78, "text": " some string formatting which for some people is no big deal, but to me it saves a lot of", "tokens": [51448, 512, 6798, 39366, 597, 337, 512, 561, 307, 572, 955, 2028, 11, 457, 281, 385, 309, 19155, 257, 688, 295, 51652], "temperature": 0.0, "avg_logprob": -0.2955206036567688, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.13844721019268036}, {"id": 129, "seek": 57702, "start": 602.78, "end": 604.54, "text": " time and makes life a lot easier.", "tokens": [51652, 565, 293, 1669, 993, 257, 688, 3571, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2955206036567688, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.13844721019268036}, {"id": 130, "seek": 60454, "start": 604.54, "end": 607.3, "text": " So we're going to move across to Python 3.", "tokens": [50364, 407, 321, 434, 516, 281, 1286, 2108, 281, 15329, 805, 13, 50502], "temperature": 0.0, "avg_logprob": -0.32796000338148795, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.02297680638730526}, {"id": 131, "seek": 60454, "start": 607.3, "end": 611.18, "text": " Hopefully you've all gone through the process already.", "tokens": [50502, 10429, 291, 600, 439, 2780, 807, 264, 1399, 1217, 13, 50696], "temperature": 0.0, "avg_logprob": -0.32796000338148795, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.02297680638730526}, {"id": 132, "seek": 60454, "start": 611.18, "end": 616.54, "text": " And there's some tips on the forum about how to have both running at the same time.", "tokens": [50696, 400, 456, 311, 512, 6082, 322, 264, 17542, 466, 577, 281, 362, 1293, 2614, 412, 264, 912, 565, 13, 50964], "temperature": 0.0, "avg_logprob": -0.32796000338148795, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.02297680638730526}, {"id": 133, "seek": 60454, "start": 616.54, "end": 621.9399999999999, "text": " Although I agree with the suggestion I read from somebody, which was go ahead, suck it", "tokens": [50964, 5780, 286, 3986, 365, 264, 16541, 286, 1401, 490, 2618, 11, 597, 390, 352, 2286, 11, 9967, 309, 51234], "temperature": 0.0, "avg_logprob": -0.32796000338148795, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.02297680638730526}, {"id": 134, "seek": 60454, "start": 621.9399999999999, "end": 630.6999999999999, "text": " up and do the translation once now so you don't have to worry about it.", "tokens": [51234, 493, 293, 360, 264, 12853, 1564, 586, 370, 291, 500, 380, 362, 281, 3292, 466, 309, 13, 51672], "temperature": 0.0, "avg_logprob": -0.32796000338148795, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.02297680638730526}, {"id": 135, "seek": 63070, "start": 630.7, "end": 635.6600000000001, "text": " Much more interesting and much bigger is the move from Theano to TensorFlow.", "tokens": [50364, 12313, 544, 1880, 293, 709, 3801, 307, 264, 1286, 490, 440, 3730, 281, 37624, 13, 50612], "temperature": 0.0, "avg_logprob": -0.2794795704779224, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.21205760538578033}, {"id": 136, "seek": 63070, "start": 635.6600000000001, "end": 641.82, "text": " So Theano we thought was a better starting point because it has a much simpler API.", "tokens": [50612, 407, 440, 3730, 321, 1194, 390, 257, 1101, 2891, 935, 570, 309, 575, 257, 709, 18587, 9362, 13, 50920], "temperature": 0.0, "avg_logprob": -0.2794795704779224, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.21205760538578033}, {"id": 137, "seek": 63070, "start": 641.82, "end": 646.1400000000001, "text": " There's very few new concepts to learn to understand Theano.", "tokens": [50920, 821, 311, 588, 1326, 777, 10392, 281, 1466, 281, 1223, 440, 3730, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2794795704779224, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.21205760538578033}, {"id": 138, "seek": 63070, "start": 646.1400000000001, "end": 648.0200000000001, "text": " And it doesn't have a whole new ecosystem.", "tokens": [51136, 400, 309, 1177, 380, 362, 257, 1379, 777, 11311, 13, 51230], "temperature": 0.0, "avg_logprob": -0.2794795704779224, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.21205760538578033}, {"id": 139, "seek": 63070, "start": 648.0200000000001, "end": 652.86, "text": " You see, TensorFlow lives within Google's whole ecosystem.", "tokens": [51230, 509, 536, 11, 37624, 2909, 1951, 3329, 311, 1379, 11311, 13, 51472], "temperature": 0.0, "avg_logprob": -0.2794795704779224, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.21205760538578033}, {"id": 140, "seek": 63070, "start": 652.86, "end": 655.1400000000001, "text": " It has its own build system called Bazel.", "tokens": [51472, 467, 575, 1080, 1065, 1322, 1185, 1219, 42220, 338, 13, 51586], "temperature": 0.0, "avg_logprob": -0.2794795704779224, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.21205760538578033}, {"id": 141, "seek": 63070, "start": 655.1400000000001, "end": 658.9000000000001, "text": " It's got its own file serialization system called Protobuf.", "tokens": [51586, 467, 311, 658, 1080, 1065, 3991, 17436, 2144, 1185, 1219, 10019, 996, 2947, 13, 51774], "temperature": 0.0, "avg_logprob": -0.2794795704779224, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.21205760538578033}, {"id": 142, "seek": 65890, "start": 658.9, "end": 661.86, "text": " It's got its own profiler method based on Chrome.", "tokens": [50364, 467, 311, 658, 1080, 1065, 1740, 5441, 3170, 2361, 322, 15327, 13, 50512], "temperature": 0.0, "avg_logprob": -0.28711124342315053, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0005703144706785679}, {"id": 143, "seek": 65890, "start": 661.86, "end": 664.74, "text": " It's got all this stuff to learn.", "tokens": [50512, 467, 311, 658, 439, 341, 1507, 281, 1466, 13, 50656], "temperature": 0.0, "avg_logprob": -0.28711124342315053, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0005703144706785679}, {"id": 144, "seek": 65890, "start": 664.74, "end": 669.42, "text": " But if you've come this far, then you're already investing the time.", "tokens": [50656, 583, 498, 291, 600, 808, 341, 1400, 11, 550, 291, 434, 1217, 10978, 264, 565, 13, 50890], "temperature": 0.0, "avg_logprob": -0.28711124342315053, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0005703144706785679}, {"id": 145, "seek": 65890, "start": 669.42, "end": 673.4599999999999, "text": " We think it's worth investing the time in TensorFlow because there's a lot of stuff", "tokens": [50890, 492, 519, 309, 311, 3163, 10978, 264, 565, 294, 37624, 570, 456, 311, 257, 688, 295, 1507, 51092], "temperature": 0.0, "avg_logprob": -0.28711124342315053, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0005703144706785679}, {"id": 146, "seek": 65890, "start": 673.4599999999999, "end": 678.6999999999999, "text": " which just in the last few weeks it's started being able to do that's pretty amazing.", "tokens": [51092, 597, 445, 294, 264, 1036, 1326, 3259, 309, 311, 1409, 885, 1075, 281, 360, 300, 311, 1238, 2243, 13, 51354], "temperature": 0.0, "avg_logprob": -0.28711124342315053, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0005703144706785679}, {"id": 147, "seek": 65890, "start": 678.6999999999999, "end": 688.74, "text": " So Rachel wrote this post about how much TensorFlow sucks.", "tokens": [51354, 407, 14246, 4114, 341, 2183, 466, 577, 709, 37624, 15846, 13, 51856], "temperature": 0.0, "avg_logprob": -0.28711124342315053, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0005703144706785679}, {"id": 148, "seek": 68874, "start": 689.58, "end": 708.94, "text": " So looking at moving from Theano to TensorFlow, we got invited to the TensorFlow Dev Summit", "tokens": [50406, 407, 1237, 412, 2684, 490, 440, 3730, 281, 37624, 11, 321, 658, 9185, 281, 264, 37624, 9096, 28726, 51374], "temperature": 0.0, "avg_logprob": -0.37148456573486327, "compression_ratio": 1.302325581395349, "no_speech_prob": 0.031142883002758026}, {"id": 149, "seek": 68874, "start": 708.94, "end": 715.1, "text": " and we were pretty amazed at all the stuff that's literally just been added.", "tokens": [51374, 293, 321, 645, 1238, 20507, 412, 439, 264, 1507, 300, 311, 3736, 445, 668, 3869, 13, 51682], "temperature": 0.0, "avg_logprob": -0.37148456573486327, "compression_ratio": 1.302325581395349, "no_speech_prob": 0.031142883002758026}, {"id": 150, "seek": 71510, "start": 715.1800000000001, "end": 720.5400000000001, "text": " So TensorFlow 1 just came out.", "tokens": [50368, 407, 37624, 502, 445, 1361, 484, 13, 50636], "temperature": 0.0, "avg_logprob": -0.2924969841452206, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.00468178978189826}, {"id": 151, "seek": 71510, "start": 720.5400000000001, "end": 724.38, "text": " Here are some of the things, if you Google for TensorFlow Dev Summit videos, you can", "tokens": [50636, 1692, 366, 512, 295, 264, 721, 11, 498, 291, 3329, 337, 37624, 9096, 28726, 2145, 11, 291, 393, 50828], "temperature": 0.0, "avg_logprob": -0.2924969841452206, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.00468178978189826}, {"id": 152, "seek": 71510, "start": 724.38, "end": 726.5400000000001, "text": " watch the videos about all this.", "tokens": [50828, 1159, 264, 2145, 466, 439, 341, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2924969841452206, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.00468178978189826}, {"id": 153, "seek": 71510, "start": 726.5400000000001, "end": 731.62, "text": " Perhaps the most exciting thing for us is that they are really investing in a simplified", "tokens": [50936, 10517, 264, 881, 4670, 551, 337, 505, 307, 300, 436, 366, 534, 10978, 294, 257, 26335, 51190], "temperature": 0.0, "avg_logprob": -0.2924969841452206, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.00468178978189826}, {"id": 154, "seek": 71510, "start": 731.62, "end": 732.62, "text": " API.", "tokens": [51190, 9362, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2924969841452206, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.00468178978189826}, {"id": 155, "seek": 71510, "start": 732.62, "end": 741.1, "text": " So if you look at this code, you can create a deep neural network regressor on a mixture", "tokens": [51240, 407, 498, 291, 574, 412, 341, 3089, 11, 291, 393, 1884, 257, 2452, 18161, 3209, 1121, 735, 284, 322, 257, 9925, 51664], "temperature": 0.0, "avg_logprob": -0.2924969841452206, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.00468178978189826}, {"id": 156, "seek": 74110, "start": 741.22, "end": 748.86, "text": " of categorical and real variables using a kind of almost R-like syntax and fit it in", "tokens": [50370, 295, 19250, 804, 293, 957, 9102, 1228, 257, 733, 295, 1920, 497, 12, 4092, 28431, 293, 3318, 309, 294, 50752], "temperature": 0.0, "avg_logprob": -0.2926909321486348, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.08881276100873947}, {"id": 157, "seek": 74110, "start": 748.86, "end": 750.62, "text": " two lines of code.", "tokens": [50752, 732, 3876, 295, 3089, 13, 50840], "temperature": 0.0, "avg_logprob": -0.2926909321486348, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.08881276100873947}, {"id": 158, "seek": 74110, "start": 750.62, "end": 753.94, "text": " You'll see that those lines of code at the bottom, the two lines to fit it, look very", "tokens": [50840, 509, 603, 536, 300, 729, 3876, 295, 3089, 412, 264, 2767, 11, 264, 732, 3876, 281, 3318, 309, 11, 574, 588, 51006], "temperature": 0.0, "avg_logprob": -0.2926909321486348, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.08881276100873947}, {"id": 159, "seek": 74110, "start": 753.94, "end": 754.94, "text": " much like Keras.", "tokens": [51006, 709, 411, 591, 6985, 13, 51056], "temperature": 0.0, "avg_logprob": -0.2926909321486348, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.08881276100873947}, {"id": 160, "seek": 74110, "start": 754.94, "end": 760.1800000000001, "text": " Francois Cholet, the Keras author, has been a wonderful influence on Google.", "tokens": [51056, 34695, 271, 761, 401, 302, 11, 264, 591, 6985, 3793, 11, 575, 668, 257, 3715, 6503, 322, 3329, 13, 51318], "temperature": 0.0, "avg_logprob": -0.2926909321486348, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.08881276100873947}, {"id": 161, "seek": 74110, "start": 760.1800000000001, "end": 768.48, "text": " In fact, everywhere we saw at the Dev Summit was Keras API influences.", "tokens": [51318, 682, 1186, 11, 5315, 321, 1866, 412, 264, 9096, 28726, 390, 591, 6985, 9362, 21222, 13, 51733], "temperature": 0.0, "avg_logprob": -0.2926909321486348, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.08881276100873947}, {"id": 162, "seek": 76848, "start": 768.48, "end": 774.08, "text": " So TensorFlow and Keras are becoming more and more one, which is terrific.", "tokens": [50364, 407, 37624, 293, 591, 6985, 366, 5617, 544, 293, 544, 472, 11, 597, 307, 20899, 13, 50644], "temperature": 0.0, "avg_logprob": -0.2548113275081553, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.000607074296567589}, {"id": 163, "seek": 76848, "start": 774.08, "end": 778.48, "text": " So one is that they're really investing in the API.", "tokens": [50644, 407, 472, 307, 300, 436, 434, 534, 10978, 294, 264, 9362, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2548113275081553, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.000607074296567589}, {"id": 164, "seek": 76848, "start": 778.48, "end": 781.4, "text": " The second is that some of the tooling is looking pretty good.", "tokens": [50864, 440, 1150, 307, 300, 512, 295, 264, 46593, 307, 1237, 1238, 665, 13, 51010], "temperature": 0.0, "avg_logprob": -0.2548113275081553, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.000607074296567589}, {"id": 165, "seek": 76848, "start": 781.4, "end": 784.64, "text": " So TensorBoard has come a long way.", "tokens": [51010, 407, 34306, 22493, 515, 575, 808, 257, 938, 636, 13, 51172], "temperature": 0.0, "avg_logprob": -0.2548113275081553, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.000607074296567589}, {"id": 166, "seek": 76848, "start": 784.64, "end": 789.8000000000001, "text": " Things like these graphs showing you how your different layers are distributed and how that's", "tokens": [51172, 9514, 411, 613, 24877, 4099, 291, 577, 428, 819, 7914, 366, 12631, 293, 577, 300, 311, 51430], "temperature": 0.0, "avg_logprob": -0.2548113275081553, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.000607074296567589}, {"id": 167, "seek": 76848, "start": 789.8000000000001, "end": 794.28, "text": " changed over time can really help to debug what's going on.", "tokens": [51430, 3105, 670, 565, 393, 534, 854, 281, 24083, 437, 311, 516, 322, 13, 51654], "temperature": 0.0, "avg_logprob": -0.2548113275081553, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.000607074296567589}, {"id": 168, "seek": 79428, "start": 794.28, "end": 799.48, "text": " So if you get some kind of gradient saturation in a layer, you can dig through these graphs", "tokens": [50364, 407, 498, 291, 483, 512, 733, 295, 16235, 27090, 294, 257, 4583, 11, 291, 393, 2528, 807, 613, 24877, 50624], "temperature": 0.0, "avg_logprob": -0.31935815811157225, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.4921519160270691}, {"id": 169, "seek": 79428, "start": 799.48, "end": 804.0, "text": " and very quickly find out where.", "tokens": [50624, 293, 588, 2661, 915, 484, 689, 13, 50850], "temperature": 0.0, "avg_logprob": -0.31935815811157225, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.4921519160270691}, {"id": 170, "seek": 79428, "start": 804.0, "end": 807.28, "text": " This was one of my favorite talks actually.", "tokens": [50850, 639, 390, 472, 295, 452, 2954, 6686, 767, 13, 51014], "temperature": 0.0, "avg_logprob": -0.31935815811157225, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.4921519160270691}, {"id": 171, "seek": 79428, "start": 807.28, "end": 811.92, "text": " This guy, if I remember correctly, his name was Daffodil and his signature was an emoji", "tokens": [51014, 639, 2146, 11, 498, 286, 1604, 8944, 11, 702, 1315, 390, 413, 2518, 378, 388, 293, 702, 13397, 390, 364, 31595, 51246], "temperature": 0.0, "avg_logprob": -0.31935815811157225, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.4921519160270691}, {"id": 172, "seek": 79428, "start": 811.92, "end": 812.92, "text": " of a daffodil.", "tokens": [51246, 295, 257, 1120, 602, 378, 388, 13, 51296], "temperature": 0.0, "avg_logprob": -0.31935815811157225, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.4921519160270691}, {"id": 173, "seek": 79428, "start": 812.92, "end": 813.92, "text": " Very Google.", "tokens": [51296, 4372, 3329, 13, 51346], "temperature": 0.0, "avg_logprob": -0.31935815811157225, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.4921519160270691}, {"id": 174, "seek": 81392, "start": 813.92, "end": 830.92, "text": " If you watch this video, he has a walk-through showing some of the functionality that's there", "tokens": [50364, 759, 291, 1159, 341, 960, 11, 415, 575, 257, 1792, 12, 11529, 4099, 512, 295, 264, 14980, 300, 311, 456, 51214], "temperature": 0.0, "avg_logprob": -0.3300697265132781, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.05749083682894707}, {"id": 175, "seek": 81392, "start": 830.92, "end": 831.92, "text": " and how to use it.", "tokens": [51214, 293, 577, 281, 764, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3300697265132781, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.05749083682894707}, {"id": 176, "seek": 81392, "start": 831.92, "end": 837.0, "text": " I thought that was pretty helpful.", "tokens": [51264, 286, 1194, 300, 390, 1238, 4961, 13, 51518], "temperature": 0.0, "avg_logprob": -0.3300697265132781, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.05749083682894707}, {"id": 177, "seek": 81392, "start": 837.0, "end": 843.5999999999999, "text": " One of the most important ones to me is that TensorFlow has a great story about productionization.", "tokens": [51518, 1485, 295, 264, 881, 1021, 2306, 281, 385, 307, 300, 37624, 575, 257, 869, 1657, 466, 4265, 2144, 13, 51848], "temperature": 0.0, "avg_logprob": -0.3300697265132781, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.05749083682894707}, {"id": 178, "seek": 84360, "start": 844.28, "end": 846.28, "text": " For Part 1, I didn't much care about productionization.", "tokens": [50398, 1171, 4100, 502, 11, 286, 994, 380, 709, 1127, 466, 4265, 2144, 13, 50498], "temperature": 0.0, "avg_logprob": -0.298360683299877, "compression_ratio": 1.6340579710144927, "no_speech_prob": 5.73875404370483e-05}, {"id": 179, "seek": 84360, "start": 846.28, "end": 851.08, "text": " It was really about playing around, what can we learn.", "tokens": [50498, 467, 390, 534, 466, 2433, 926, 11, 437, 393, 321, 1466, 13, 50738], "temperature": 0.0, "avg_logprob": -0.298360683299877, "compression_ratio": 1.6340579710144927, "no_speech_prob": 5.73875404370483e-05}, {"id": 180, "seek": 84360, "start": 851.08, "end": 856.16, "text": " At this point, I think we might be starting to think about how do I get my stuff online", "tokens": [50738, 1711, 341, 935, 11, 286, 519, 321, 1062, 312, 2891, 281, 519, 466, 577, 360, 286, 483, 452, 1507, 2950, 50992], "temperature": 0.0, "avg_logprob": -0.298360683299877, "compression_ratio": 1.6340579710144927, "no_speech_prob": 5.73875404370483e-05}, {"id": 181, "seek": 84360, "start": 856.16, "end": 859.0400000000001, "text": " in front of my customers.", "tokens": [50992, 294, 1868, 295, 452, 4581, 13, 51136], "temperature": 0.0, "avg_logprob": -0.298360683299877, "compression_ratio": 1.6340579710144927, "no_speech_prob": 5.73875404370483e-05}, {"id": 182, "seek": 84360, "start": 859.0400000000001, "end": 863.32, "text": " These points are talking about something in particular which is called TensorFlow Serving.", "tokens": [51136, 1981, 2793, 366, 1417, 466, 746, 294, 1729, 597, 307, 1219, 37624, 4210, 798, 13, 51350], "temperature": 0.0, "avg_logprob": -0.298360683299877, "compression_ratio": 1.6340579710144927, "no_speech_prob": 5.73875404370483e-05}, {"id": 183, "seek": 84360, "start": 863.32, "end": 869.12, "text": " TensorFlow Serving is a system that can take your trained TensorFlow model and create an", "tokens": [51350, 37624, 4210, 798, 307, 257, 1185, 300, 393, 747, 428, 8895, 37624, 2316, 293, 1884, 364, 51640], "temperature": 0.0, "avg_logprob": -0.298360683299877, "compression_ratio": 1.6340579710144927, "no_speech_prob": 5.73875404370483e-05}, {"id": 184, "seek": 84360, "start": 869.12, "end": 872.72, "text": " API for it which does some pretty cool things.", "tokens": [51640, 9362, 337, 309, 597, 775, 512, 1238, 1627, 721, 13, 51820], "temperature": 0.0, "avg_logprob": -0.298360683299877, "compression_ratio": 1.6340579710144927, "no_speech_prob": 5.73875404370483e-05}, {"id": 185, "seek": 87272, "start": 872.72, "end": 879.6, "text": " For example, think about how hard it would be without the help of some library to productionize", "tokens": [50364, 1171, 1365, 11, 519, 466, 577, 1152, 309, 576, 312, 1553, 264, 854, 295, 512, 6405, 281, 4265, 1125, 50708], "temperature": 0.0, "avg_logprob": -0.2562416833022545, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.000100713710708078}, {"id": 186, "seek": 87272, "start": 879.6, "end": 880.6, "text": " your system.", "tokens": [50708, 428, 1185, 13, 50758], "temperature": 0.0, "avg_logprob": -0.2562416833022545, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.000100713710708078}, {"id": 187, "seek": 87272, "start": 880.6, "end": 884.32, "text": " You've got one request coming in at a time.", "tokens": [50758, 509, 600, 658, 472, 5308, 1348, 294, 412, 257, 565, 13, 50944], "temperature": 0.0, "avg_logprob": -0.2562416833022545, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.000100713710708078}, {"id": 188, "seek": 87272, "start": 884.32, "end": 887.4, "text": " You've got n GPUs.", "tokens": [50944, 509, 600, 658, 297, 18407, 82, 13, 51098], "temperature": 0.0, "avg_logprob": -0.2562416833022545, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.000100713710708078}, {"id": 189, "seek": 87272, "start": 887.4, "end": 891.28, "text": " How do you make sure that you don't saturate all those GPUs, that you send the request", "tokens": [51098, 1012, 360, 291, 652, 988, 300, 291, 500, 380, 21160, 473, 439, 729, 18407, 82, 11, 300, 291, 2845, 264, 5308, 51292], "temperature": 0.0, "avg_logprob": -0.2562416833022545, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.000100713710708078}, {"id": 190, "seek": 87272, "start": 891.28, "end": 894.64, "text": " to one that's free, that you don't use up all of your memory.", "tokens": [51292, 281, 472, 300, 311, 1737, 11, 300, 291, 500, 380, 764, 493, 439, 295, 428, 4675, 13, 51460], "temperature": 0.0, "avg_logprob": -0.2562416833022545, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.000100713710708078}, {"id": 191, "seek": 87272, "start": 894.64, "end": 901.1600000000001, "text": " Better still, how do you grab a few requests, put them into a batch, put them all into the", "tokens": [51460, 15753, 920, 11, 577, 360, 291, 4444, 257, 1326, 12475, 11, 829, 552, 666, 257, 15245, 11, 829, 552, 439, 666, 264, 51786], "temperature": 0.0, "avg_logprob": -0.2562416833022545, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.000100713710708078}, {"id": 192, "seek": 90116, "start": 901.16, "end": 904.64, "text": " GPU at once, get the bits out of the batch, put them back to the people that requested", "tokens": [50364, 18407, 412, 1564, 11, 483, 264, 9239, 484, 295, 264, 15245, 11, 829, 552, 646, 281, 264, 561, 300, 16436, 50538], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 193, "seek": 90116, "start": 904.64, "end": 907.16, "text": " it, all that stuff.", "tokens": [50538, 309, 11, 439, 300, 1507, 13, 50664], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 194, "seek": 90116, "start": 907.16, "end": 909.24, "text": " Serving does that for you.", "tokens": [50664, 4210, 798, 775, 300, 337, 291, 13, 50768], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 195, "seek": 90116, "start": 909.24, "end": 911.68, "text": " It's very early days for this software.", "tokens": [50768, 467, 311, 588, 2440, 1708, 337, 341, 4722, 13, 50890], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 196, "seek": 90116, "start": 911.68, "end": 916.6, "text": " A lot of things don't work yet, but you can download an early version and start playing", "tokens": [50890, 316, 688, 295, 721, 500, 380, 589, 1939, 11, 457, 291, 393, 5484, 364, 2440, 3037, 293, 722, 2433, 51136], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 197, "seek": 90116, "start": 916.6, "end": 917.6, "text": " with it.", "tokens": [51136, 365, 309, 13, 51186], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 198, "seek": 90116, "start": 917.6, "end": 918.6, "text": " I think that's pretty interesting.", "tokens": [51186, 286, 519, 300, 311, 1238, 1880, 13, 51236], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 199, "seek": 90116, "start": 918.6, "end": 919.88, "text": " Question 2", "tokens": [51236, 14464, 568, 51300], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 200, "seek": 90116, "start": 919.88, "end": 925.28, "text": " With the high-level API in TensorFlow, what's going to be the difference between the Keras", "tokens": [51300, 2022, 264, 1090, 12, 12418, 9362, 294, 37624, 11, 437, 311, 516, 281, 312, 264, 2649, 1296, 264, 591, 6985, 51570], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 201, "seek": 90116, "start": 925.28, "end": 926.88, "text": " API and the TensorFlow API?", "tokens": [51570, 9362, 293, 264, 37624, 9362, 30, 51650], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 202, "seek": 90116, "start": 926.88, "end": 930.0, "text": " That's a great question.", "tokens": [51650, 663, 311, 257, 869, 1168, 13, 51806], "temperature": 0.0, "avg_logprob": -0.35035492653070494, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.05033015459775925}, {"id": 203, "seek": 93000, "start": 930.0, "end": 939.52, "text": " In fact, TF.keras will become a namespace.", "tokens": [50364, 682, 1186, 11, 40964, 13, 5767, 296, 486, 1813, 257, 5288, 17940, 13, 50840], "temperature": 0.0, "avg_logprob": -0.4280487060546875, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.005641857162117958}, {"id": 204, "seek": 93000, "start": 939.52, "end": 945.2, "text": " So Keras will become the official top-level API for TensorFlow.", "tokens": [50840, 407, 591, 6985, 486, 1813, 264, 4783, 1192, 12, 12418, 9362, 337, 37624, 13, 51124], "temperature": 0.0, "avg_logprob": -0.4280487060546875, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.005641857162117958}, {"id": 205, "seek": 93000, "start": 945.2, "end": 949.4, "text": " In fact, Rachel was the person who announced that.", "tokens": [51124, 682, 1186, 11, 14246, 390, 264, 954, 567, 7548, 300, 13, 51334], "temperature": 0.0, "avg_logprob": -0.4280487060546875, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.005641857162117958}, {"id": 206, "seek": 93000, "start": 949.4, "end": 954.12, "text": " I was just going to add that TensorFlow is introducing a few different libraries at different", "tokens": [51334, 286, 390, 445, 516, 281, 909, 300, 37624, 307, 15424, 257, 1326, 819, 15148, 412, 819, 51570], "temperature": 0.0, "avg_logprob": -0.4280487060546875, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.005641857162117958}, {"id": 207, "seek": 93000, "start": 954.12, "end": 955.12, "text": " levels of abstraction.", "tokens": [51570, 4358, 295, 37765, 13, 51620], "temperature": 0.0, "avg_logprob": -0.4280487060546875, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.005641857162117958}, {"id": 208, "seek": 95512, "start": 956.12, "end": 965.4, "text": " There's this concept of an evaluation API that appears everywhere and basically is the", "tokens": [50414, 821, 311, 341, 3410, 295, 364, 13344, 9362, 300, 7038, 5315, 293, 1936, 307, 264, 50878], "temperature": 0.0, "avg_logprob": -0.3013958994547526, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.13116946816444397}, {"id": 209, "seek": 95512, "start": 965.4, "end": 966.4, "text": " Keras API.", "tokens": [50878, 591, 6985, 9362, 13, 50928], "temperature": 0.0, "avg_logprob": -0.3013958994547526, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.13116946816444397}, {"id": 210, "seek": 95512, "start": 966.4, "end": 971.2, "text": " I think there's a layers API below the Keras API.", "tokens": [50928, 286, 519, 456, 311, 257, 7914, 9362, 2507, 264, 591, 6985, 9362, 13, 51168], "temperature": 0.0, "avg_logprob": -0.3013958994547526, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.13116946816444397}, {"id": 211, "seek": 95512, "start": 971.2, "end": 973.96, "text": " So it's being mixed in lots of places.", "tokens": [51168, 407, 309, 311, 885, 7467, 294, 3195, 295, 3190, 13, 51306], "temperature": 0.0, "avg_logprob": -0.3013958994547526, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.13116946816444397}, {"id": 212, "seek": 95512, "start": 973.96, "end": 978.5600000000001, "text": " So all the stuff you've learned about Keras is going to be very helpful, not just in using", "tokens": [51306, 407, 439, 264, 1507, 291, 600, 3264, 466, 591, 6985, 307, 516, 281, 312, 588, 4961, 11, 406, 445, 294, 1228, 51536], "temperature": 0.0, "avg_logprob": -0.3013958994547526, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.13116946816444397}, {"id": 213, "seek": 97856, "start": 978.56, "end": 985.8399999999999, "text": " Keras on top of TensorFlow, but in using TensorFlow directly.", "tokens": [50364, 591, 6985, 322, 1192, 295, 37624, 11, 457, 294, 1228, 37624, 3838, 13, 50728], "temperature": 0.0, "avg_logprob": -0.2929474639892578, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.050326962023973465}, {"id": 214, "seek": 97856, "start": 985.8399999999999, "end": 990.76, "text": " Another interesting thing about TensorFlow is that they've built a lot of cool integrations", "tokens": [50728, 3996, 1880, 551, 466, 37624, 307, 300, 436, 600, 3094, 257, 688, 295, 1627, 3572, 763, 50974], "temperature": 0.0, "avg_logprob": -0.2929474639892578, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.050326962023973465}, {"id": 215, "seek": 97856, "start": 990.76, "end": 994.7199999999999, "text": " with various cluster managers and distributed storage systems and stuff like that.", "tokens": [50974, 365, 3683, 13630, 14084, 293, 12631, 6725, 3652, 293, 1507, 411, 300, 13, 51172], "temperature": 0.0, "avg_logprob": -0.2929474639892578, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.050326962023973465}, {"id": 216, "seek": 97856, "start": 994.7199999999999, "end": 999.76, "text": " So it will kind of fit into your production systems more neatly, use the data in whatever", "tokens": [51172, 407, 309, 486, 733, 295, 3318, 666, 428, 4265, 3652, 544, 36634, 11, 764, 264, 1412, 294, 2035, 51424], "temperature": 0.0, "avg_logprob": -0.2929474639892578, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.050326962023973465}, {"id": 217, "seek": 97856, "start": 999.76, "end": 1001.76, "text": " place it already is more neatly.", "tokens": [51424, 1081, 309, 1217, 307, 544, 36634, 13, 51524], "temperature": 0.0, "avg_logprob": -0.2929474639892578, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.050326962023973465}, {"id": 218, "seek": 97856, "start": 1001.76, "end": 1006.92, "text": " So if your data is in S3 or something like that, you can generally throw it straight", "tokens": [51524, 407, 498, 428, 1412, 307, 294, 318, 18, 420, 746, 411, 300, 11, 291, 393, 5101, 3507, 309, 2997, 51782], "temperature": 0.0, "avg_logprob": -0.2929474639892578, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.050326962023973465}, {"id": 219, "seek": 100692, "start": 1006.92, "end": 1008.28, "text": " into TensorFlow.", "tokens": [50364, 666, 37624, 13, 50432], "temperature": 0.0, "avg_logprob": -0.32847565664371975, "compression_ratio": 1.4439024390243902, "no_speech_prob": 0.011687202379107475}, {"id": 220, "seek": 100692, "start": 1008.28, "end": 1016.64, "text": " Something I found very interesting is that they announced a couple of weeks ago a machine", "tokens": [50432, 6595, 286, 1352, 588, 1880, 307, 300, 436, 7548, 257, 1916, 295, 3259, 2057, 257, 3479, 50850], "temperature": 0.0, "avg_logprob": -0.32847565664371975, "compression_ratio": 1.4439024390243902, "no_speech_prob": 0.011687202379107475}, {"id": 221, "seek": 100692, "start": 1016.64, "end": 1023.3199999999999, "text": " learning toolkit which brings really high-quality implementations of a wide variety of non-deep", "tokens": [50850, 2539, 40167, 597, 5607, 534, 1090, 12, 11286, 4445, 763, 295, 257, 4874, 5673, 295, 2107, 12, 38422, 51184], "temperature": 0.0, "avg_logprob": -0.32847565664371975, "compression_ratio": 1.4439024390243902, "no_speech_prob": 0.011687202379107475}, {"id": 222, "seek": 100692, "start": 1023.3199999999999, "end": 1025.5, "text": " learning algorithms.", "tokens": [51184, 2539, 14642, 13, 51293], "temperature": 0.0, "avg_logprob": -0.32847565664371975, "compression_ratio": 1.4439024390243902, "no_speech_prob": 0.011687202379107475}, {"id": 223, "seek": 100692, "start": 1025.5, "end": 1035.52, "text": " So all these are GPU-accelerated, parallelized, and supported by Google.", "tokens": [51293, 407, 439, 613, 366, 18407, 12, 326, 4933, 260, 770, 11, 8952, 1602, 11, 293, 8104, 538, 3329, 13, 51794], "temperature": 0.0, "avg_logprob": -0.32847565664371975, "compression_ratio": 1.4439024390243902, "no_speech_prob": 0.011687202379107475}, {"id": 224, "seek": 103552, "start": 1035.52, "end": 1038.48, "text": " Like a lot of these have a lot of tech behind them.", "tokens": [50364, 1743, 257, 688, 295, 613, 362, 257, 688, 295, 7553, 2261, 552, 13, 50512], "temperature": 0.0, "avg_logprob": -0.3903748098045889, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00857740081846714}, {"id": 225, "seek": 103552, "start": 1038.48, "end": 1043.72, "text": " So for example, the random forest, there's a paper, they actually call it the TensorForest,", "tokens": [50512, 407, 337, 1365, 11, 264, 4974, 6719, 11, 456, 311, 257, 3035, 11, 436, 767, 818, 309, 264, 34306, 37, 38620, 11, 50774], "temperature": 0.0, "avg_logprob": -0.3903748098045889, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00857740081846714}, {"id": 226, "seek": 103552, "start": 1043.72, "end": 1049.0, "text": " which explains all of the interesting things they did to create a fast GPU-accelerated", "tokens": [50774, 597, 13948, 439, 295, 264, 1880, 721, 436, 630, 281, 1884, 257, 2370, 18407, 12, 326, 4933, 260, 770, 51038], "temperature": 0.0, "avg_logprob": -0.3903748098045889, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00857740081846714}, {"id": 227, "seek": 103552, "start": 1049.0, "end": 1050.0, "text": " random forest.", "tokens": [51038, 4974, 6719, 13, 51088], "temperature": 0.0, "avg_logprob": -0.3903748098045889, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00857740081846714}, {"id": 228, "seek": 103552, "start": 1050.0, "end": 1051.0, "text": " Question 2.", "tokens": [51088, 14464, 568, 13, 51138], "temperature": 0.0, "avg_logprob": -0.3903748098045889, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00857740081846714}, {"id": 229, "seek": 103552, "start": 1051.0, "end": 1059.0, "text": " Will you give an example of how to solve gradient saturation with TensorFlow tools?", "tokens": [51138, 3099, 291, 976, 364, 1365, 295, 577, 281, 5039, 16235, 27090, 365, 37624, 3873, 30, 51538], "temperature": 0.0, "avg_logprob": -0.3903748098045889, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00857740081846714}, {"id": 230, "seek": 103552, "start": 1059.0, "end": 1061.72, "text": " I'm not sure that I will.", "tokens": [51538, 286, 478, 406, 988, 300, 286, 486, 13, 51674], "temperature": 0.0, "avg_logprob": -0.3903748098045889, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00857740081846714}, {"id": 231, "seek": 106172, "start": 1061.72, "end": 1063.0, "text": " We'll see how we go.", "tokens": [50364, 492, 603, 536, 577, 321, 352, 13, 50428], "temperature": 0.0, "avg_logprob": -0.384551596134267, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.2538454830646515}, {"id": 232, "seek": 106172, "start": 1063.0, "end": 1067.8, "text": " I think the video from the Dev Summit, which is available online, kind of already shows", "tokens": [50428, 286, 519, 264, 960, 490, 264, 9096, 28726, 11, 597, 307, 2435, 2950, 11, 733, 295, 1217, 3110, 50668], "temperature": 0.0, "avg_logprob": -0.384551596134267, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.2538454830646515}, {"id": 233, "seek": 106172, "start": 1067.8, "end": 1068.8, "text": " you that.", "tokens": [50668, 291, 300, 13, 50718], "temperature": 0.0, "avg_logprob": -0.384551596134267, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.2538454830646515}, {"id": 234, "seek": 106172, "start": 1068.8, "end": 1073.24, "text": " So I would say look at that first and see if you still have questions.", "tokens": [50718, 407, 286, 576, 584, 574, 412, 300, 700, 293, 536, 498, 291, 920, 362, 1651, 13, 50940], "temperature": 0.0, "avg_logprob": -0.384551596134267, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.2538454830646515}, {"id": 235, "seek": 106172, "start": 1073.24, "end": 1076.6000000000001, "text": " All the videos from the Dev Summit are online.", "tokens": [50940, 1057, 264, 2145, 490, 264, 9096, 28726, 366, 2950, 13, 51108], "temperature": 0.0, "avg_logprob": -0.384551596134267, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.2538454830646515}, {"id": 236, "seek": 106172, "start": 1076.6000000000001, "end": 1084.28, "text": " Someone asked about is there an idea for using deep learning on AWS Lambda?", "tokens": [51108, 8734, 2351, 466, 307, 456, 364, 1558, 337, 1228, 2452, 2539, 322, 17650, 45691, 30, 51492], "temperature": 0.0, "avg_logprob": -0.384551596134267, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.2538454830646515}, {"id": 237, "seek": 106172, "start": 1084.28, "end": 1090.16, "text": " Not that I've heard of, no.", "tokens": [51492, 1726, 300, 286, 600, 2198, 295, 11, 572, 13, 51786], "temperature": 0.0, "avg_logprob": -0.384551596134267, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.2538454830646515}, {"id": 238, "seek": 109016, "start": 1090.16, "end": 1100.4, "text": " In general, Google has a service version of TensorFlow Serving called Google Cloud ML,", "tokens": [50364, 682, 2674, 11, 3329, 575, 257, 2643, 3037, 295, 37624, 4210, 798, 1219, 3329, 8061, 21601, 11, 50876], "temperature": 0.0, "avg_logprob": -0.2852446759333376, "compression_ratio": 1.3626373626373627, "no_speech_prob": 0.0008040841785259545}, {"id": 239, "seek": 109016, "start": 1100.4, "end": 1106.3600000000001, "text": " where you can pay them a few cents a transaction and they'll host your model for you.", "tokens": [50876, 689, 291, 393, 1689, 552, 257, 1326, 14941, 257, 14425, 293, 436, 603, 3975, 428, 2316, 337, 291, 13, 51174], "temperature": 0.0, "avg_logprob": -0.2852446759333376, "compression_ratio": 1.3626373626373627, "no_speech_prob": 0.0008040841785259545}, {"id": 240, "seek": 109016, "start": 1106.3600000000001, "end": 1112.92, "text": " There isn't really something like that through Amazon, as far as I'm aware.", "tokens": [51174, 821, 1943, 380, 534, 746, 411, 300, 807, 6795, 11, 382, 1400, 382, 286, 478, 3650, 13, 51502], "temperature": 0.0, "avg_logprob": -0.2852446759333376, "compression_ratio": 1.3626373626373627, "no_speech_prob": 0.0008040841785259545}, {"id": 241, "seek": 111292, "start": 1112.92, "end": 1120.4, "text": " And then finally, in terms of TensorFlow, I had an interesting and infuriating few weeks", "tokens": [50364, 400, 550, 2721, 11, 294, 2115, 295, 37624, 11, 286, 632, 364, 1880, 293, 1536, 9744, 990, 1326, 3259, 50738], "temperature": 0.0, "avg_logprob": -0.26644960202668844, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.09008955955505371}, {"id": 242, "seek": 111292, "start": 1120.4, "end": 1124.1200000000001, "text": " trying to prepare for this class and trying to get something working that would translate", "tokens": [50738, 1382, 281, 5940, 337, 341, 1508, 293, 1382, 281, 483, 746, 1364, 300, 576, 13799, 50924], "temperature": 0.0, "avg_logprob": -0.26644960202668844, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.09008955955505371}, {"id": 243, "seek": 111292, "start": 1124.1200000000001, "end": 1127.52, "text": " French into English.", "tokens": [50924, 5522, 666, 3669, 13, 51094], "temperature": 0.0, "avg_logprob": -0.26644960202668844, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.09008955955505371}, {"id": 244, "seek": 111292, "start": 1127.52, "end": 1132.52, "text": " Every single example I found online had major problems.", "tokens": [51094, 2048, 2167, 1365, 286, 1352, 2950, 632, 2563, 2740, 13, 51344], "temperature": 0.0, "avg_logprob": -0.26644960202668844, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.09008955955505371}, {"id": 245, "seek": 111292, "start": 1132.52, "end": 1138.88, "text": " Even the official TensorFlow tutorial missed out a key thing, which is that the lowest", "tokens": [51344, 2754, 264, 4783, 37624, 7073, 6721, 484, 257, 2141, 551, 11, 597, 307, 300, 264, 12437, 51662], "temperature": 0.0, "avg_logprob": -0.26644960202668844, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.09008955955505371}, {"id": 246, "seek": 113888, "start": 1138.88, "end": 1144.64, "text": " level of a language model really should be bidirectional, as this one shows, bidirectional", "tokens": [50364, 1496, 295, 257, 2856, 2316, 534, 820, 312, 12957, 621, 41048, 11, 382, 341, 472, 3110, 11, 12957, 621, 41048, 50652], "temperature": 0.0, "avg_logprob": -0.3311951955159505, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.15002226829528809}, {"id": 247, "seek": 113888, "start": 1144.64, "end": 1145.64, "text": " RNN.", "tokens": [50652, 45702, 45, 13, 50702], "temperature": 0.0, "avg_logprob": -0.3311951955159505, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.15002226829528809}, {"id": 248, "seek": 113888, "start": 1145.64, "end": 1146.64, "text": " And their one wasn't.", "tokens": [50702, 400, 641, 472, 2067, 380, 13, 50752], "temperature": 0.0, "avg_logprob": -0.3311951955159505, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.15002226829528809}, {"id": 249, "seek": 113888, "start": 1146.64, "end": 1147.64, "text": " I'm trying to figure out how to make it work.", "tokens": [50752, 286, 478, 1382, 281, 2573, 484, 577, 281, 652, 309, 589, 13, 50802], "temperature": 0.0, "avg_logprob": -0.3311951955159505, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.15002226829528809}, {"id": 250, "seek": 113888, "start": 1147.64, "end": 1148.64, "text": " It's horrible.", "tokens": [50802, 467, 311, 9263, 13, 50852], "temperature": 0.0, "avg_logprob": -0.3311951955159505, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.15002226829528809}, {"id": 251, "seek": 113888, "start": 1148.64, "end": 1153.68, "text": " I'm trying to get it to work in Keras, nothing worked properly.", "tokens": [50852, 286, 478, 1382, 281, 483, 309, 281, 589, 294, 591, 6985, 11, 1825, 2732, 6108, 13, 51104], "temperature": 0.0, "avg_logprob": -0.3311951955159505, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.15002226829528809}, {"id": 252, "seek": 113888, "start": 1153.68, "end": 1157.92, "text": " Finally, basically the issue is this.", "tokens": [51104, 6288, 11, 1936, 264, 2734, 307, 341, 13, 51316], "temperature": 0.0, "avg_logprob": -0.3311951955159505, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.15002226829528809}, {"id": 253, "seek": 113888, "start": 1157.92, "end": 1166.22, "text": " Modern RNN systems, like a full neural translation system, involve a lot of tweaking and mucking", "tokens": [51316, 19814, 45702, 45, 3652, 11, 411, 257, 1577, 18161, 12853, 1185, 11, 9494, 257, 688, 295, 6986, 2456, 293, 275, 33260, 51731], "temperature": 0.0, "avg_logprob": -0.3311951955159505, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.15002226829528809}, {"id": 254, "seek": 116622, "start": 1166.22, "end": 1171.74, "text": " around with the innards of the RNN, using things that we'll learn about.", "tokens": [50364, 926, 365, 264, 7714, 2287, 295, 264, 45702, 45, 11, 1228, 721, 300, 321, 603, 1466, 466, 13, 50640], "temperature": 0.0, "avg_logprob": -0.29602675862831646, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.012821109034121037}, {"id": 255, "seek": 116622, "start": 1171.74, "end": 1178.14, "text": " And there just hasn't been an API that really lets that happen.", "tokens": [50640, 400, 456, 445, 6132, 380, 668, 364, 9362, 300, 534, 6653, 300, 1051, 13, 50960], "temperature": 0.0, "avg_logprob": -0.29602675862831646, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.012821109034121037}, {"id": 256, "seek": 116622, "start": 1178.14, "end": 1183.9, "text": " So I finally got it working by switching to PyTorch, which we'll learn about soon.", "tokens": [50960, 407, 286, 2721, 658, 309, 1364, 538, 16493, 281, 9953, 51, 284, 339, 11, 597, 321, 603, 1466, 466, 2321, 13, 51248], "temperature": 0.0, "avg_logprob": -0.29602675862831646, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.012821109034121037}, {"id": 257, "seek": 116622, "start": 1183.9, "end": 1188.3600000000001, "text": " I was actually going to start the first lesson was going to be about neural translation,", "tokens": [51248, 286, 390, 767, 516, 281, 722, 264, 700, 6898, 390, 516, 281, 312, 466, 18161, 12853, 11, 51471], "temperature": 0.0, "avg_logprob": -0.29602675862831646, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.012821109034121037}, {"id": 258, "seek": 116622, "start": 1188.3600000000001, "end": 1195.82, "text": " and I've put it back because TensorFlow has just released a new system for RNNs which", "tokens": [51471, 293, 286, 600, 829, 309, 646, 570, 37624, 575, 445, 4736, 257, 777, 1185, 337, 45702, 45, 82, 597, 51844], "temperature": 0.0, "avg_logprob": -0.29602675862831646, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.012821109034121037}, {"id": 259, "seek": 119582, "start": 1195.82, "end": 1198.1399999999999, "text": " looks like it's going to make all this a lot easier.", "tokens": [50364, 1542, 411, 309, 311, 516, 281, 652, 439, 341, 257, 688, 3571, 13, 50480], "temperature": 0.0, "avg_logprob": -0.32669265747070314, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.013848464004695415}, {"id": 260, "seek": 119582, "start": 1198.1399999999999, "end": 1205.26, "text": " So this is an exciting idea, there's an API that allows us to create some pretty powerful", "tokens": [50480, 407, 341, 307, 364, 4670, 1558, 11, 456, 311, 364, 9362, 300, 4045, 505, 281, 1884, 512, 1238, 4005, 50836], "temperature": 0.0, "avg_logprob": -0.32669265747070314, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.013848464004695415}, {"id": 261, "seek": 119582, "start": 1205.26, "end": 1209.3, "text": " RNN implementations and we're going to be absolutely needing that when we learn to create", "tokens": [50836, 45702, 45, 4445, 763, 293, 321, 434, 516, 281, 312, 3122, 18006, 300, 562, 321, 1466, 281, 1884, 51038], "temperature": 0.0, "avg_logprob": -0.32669265747070314, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.013848464004695415}, {"id": 262, "seek": 119582, "start": 1209.3, "end": 1210.3, "text": " translations.", "tokens": [51038, 37578, 13, 51088], "temperature": 0.0, "avg_logprob": -0.32669265747070314, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.013848464004695415}, {"id": 263, "seek": 119582, "start": 1210.3, "end": 1213.82, "text": " Oh yeah, there is one more.", "tokens": [51088, 876, 1338, 11, 456, 307, 472, 544, 13, 51264], "temperature": 0.0, "avg_logprob": -0.32669265747070314, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.013848464004695415}, {"id": 264, "seek": 119582, "start": 1213.82, "end": 1220.22, "text": " Again, early days, but there is something called XLA, which is the Accelerated Linear", "tokens": [51264, 3764, 11, 2440, 1708, 11, 457, 456, 307, 746, 1219, 1783, 11435, 11, 597, 307, 264, 5725, 6185, 770, 14670, 289, 51584], "temperature": 0.0, "avg_logprob": -0.32669265747070314, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.013848464004695415}, {"id": 265, "seek": 119582, "start": 1220.22, "end": 1221.22, "text": " Algebra.", "tokens": [51584, 967, 19983, 13, 51634], "temperature": 0.0, "avg_logprob": -0.32669265747070314, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.013848464004695415}, {"id": 266, "seek": 122122, "start": 1221.22, "end": 1233.34, "text": " Which is a system which takes TensorFlow code and compiles it.", "tokens": [50364, 3013, 307, 257, 1185, 597, 2516, 37624, 3089, 293, 715, 4680, 309, 13, 50970], "temperature": 0.0, "avg_logprob": -0.3073082703810472, "compression_ratio": 1.5364583333333333, "no_speech_prob": 0.051082201302051544}, {"id": 267, "seek": 122122, "start": 1233.34, "end": 1237.5, "text": " And so for those of you who know something about compiling, you know that a compilation", "tokens": [50970, 400, 370, 337, 729, 295, 291, 567, 458, 746, 466, 715, 4883, 11, 291, 458, 300, 257, 40261, 51178], "temperature": 0.0, "avg_logprob": -0.3073082703810472, "compression_ratio": 1.5364583333333333, "no_speech_prob": 0.051082201302051544}, {"id": 268, "seek": 122122, "start": 1237.5, "end": 1245.9, "text": " can do a lot of clever stuff in terms of identifying dead code, or unrolling loops, or fusing operations,", "tokens": [51178, 393, 360, 257, 688, 295, 13494, 1507, 294, 2115, 295, 16696, 3116, 3089, 11, 420, 517, 18688, 16121, 11, 420, 283, 7981, 7705, 11, 51598], "temperature": 0.0, "avg_logprob": -0.3073082703810472, "compression_ratio": 1.5364583333333333, "no_speech_prob": 0.051082201302051544}, {"id": 269, "seek": 122122, "start": 1245.9, "end": 1246.9, "text": " or whatever.", "tokens": [51598, 420, 2035, 13, 51648], "temperature": 0.0, "avg_logprob": -0.3073082703810472, "compression_ratio": 1.5364583333333333, "no_speech_prob": 0.051082201302051544}, {"id": 270, "seek": 122122, "start": 1246.9, "end": 1248.8600000000001, "text": " XLA tries to do all that.", "tokens": [51648, 1783, 11435, 9898, 281, 360, 439, 300, 13, 51746], "temperature": 0.0, "avg_logprob": -0.3073082703810472, "compression_ratio": 1.5364583333333333, "no_speech_prob": 0.051082201302051544}, {"id": 271, "seek": 124886, "start": 1248.86, "end": 1253.4199999999998, "text": " Now at this stage, it takes your TensorFlow code and turns it into machine code.", "tokens": [50364, 823, 412, 341, 3233, 11, 309, 2516, 428, 37624, 3089, 293, 4523, 309, 666, 3479, 3089, 13, 50592], "temperature": 0.0, "avg_logprob": -0.33281536535783246, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.0026730012614279985}, {"id": 272, "seek": 124886, "start": 1253.4199999999998, "end": 1258.58, "text": " One of the cool things that lets you do is run it on a mobile phone with almost no supporting", "tokens": [50592, 1485, 295, 264, 1627, 721, 300, 6653, 291, 360, 307, 1190, 309, 322, 257, 6013, 2593, 365, 1920, 572, 7231, 50850], "temperature": 0.0, "avg_logprob": -0.33281536535783246, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.0026730012614279985}, {"id": 273, "seek": 124886, "start": 1258.58, "end": 1266.2199999999998, "text": " libraries using native machine instructions on that phone, much less memory.", "tokens": [50850, 15148, 1228, 8470, 3479, 9415, 322, 300, 2593, 11, 709, 1570, 4675, 13, 51232], "temperature": 0.0, "avg_logprob": -0.33281536535783246, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.0026730012614279985}, {"id": 274, "seek": 124886, "start": 1266.2199999999998, "end": 1272.1399999999999, "text": " But one of the really interesting discussions I had at the summit was with Scott Gray, who", "tokens": [51232, 583, 472, 295, 264, 534, 1880, 11088, 286, 632, 412, 264, 21564, 390, 365, 6659, 22668, 11, 567, 51528], "temperature": 0.0, "avg_logprob": -0.33281536535783246, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.0026730012614279985}, {"id": 275, "seek": 124886, "start": 1272.1399999999999, "end": 1273.34, "text": " some of you may have heard of.", "tokens": [51528, 512, 295, 291, 815, 362, 2198, 295, 13, 51588], "temperature": 0.0, "avg_logprob": -0.33281536535783246, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.0026730012614279985}, {"id": 276, "seek": 127334, "start": 1273.34, "end": 1280.82, "text": " He was the guy that massively accelerated neural network kernels when he was at Nirvana.", "tokens": [50364, 634, 390, 264, 2146, 300, 29379, 29763, 18161, 3209, 23434, 1625, 562, 415, 390, 412, 44813, 39259, 13, 50738], "temperature": 0.0, "avg_logprob": -0.25802181543928854, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.17779704928398132}, {"id": 277, "seek": 127334, "start": 1280.82, "end": 1285.8999999999999, "text": " He had kernels that were 2 or 3 times faster than NVIDIA's kernels.", "tokens": [50738, 634, 632, 23434, 1625, 300, 645, 568, 420, 805, 1413, 4663, 813, 426, 3958, 6914, 311, 23434, 1625, 13, 50992], "temperature": 0.0, "avg_logprob": -0.25802181543928854, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.17779704928398132}, {"id": 278, "seek": 127334, "start": 1285.8999999999999, "end": 1290.6999999999998, "text": " I don't know of anybody else in the world who knows more about neural network performance", "tokens": [50992, 286, 500, 380, 458, 295, 4472, 1646, 294, 264, 1002, 567, 3255, 544, 466, 18161, 3209, 3389, 51232], "temperature": 0.0, "avg_logprob": -0.25802181543928854, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.17779704928398132}, {"id": 279, "seek": 127334, "start": 1290.6999999999998, "end": 1292.4599999999998, "text": " than him.", "tokens": [51232, 813, 796, 13, 51320], "temperature": 0.0, "avg_logprob": -0.25802181543928854, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.17779704928398132}, {"id": 280, "seek": 127334, "start": 1292.4599999999998, "end": 1302.62, "text": " He told me that he thinks that XLA is the key to creating performant, concise, expressive", "tokens": [51320, 634, 1907, 385, 300, 415, 7309, 300, 1783, 11435, 307, 264, 2141, 281, 4084, 2042, 394, 11, 44882, 11, 40189, 51828], "temperature": 0.0, "avg_logprob": -0.25802181543928854, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.17779704928398132}, {"id": 281, "seek": 130262, "start": 1302.8999999999999, "end": 1306.58, "text": " neural network code.", "tokens": [50378, 18161, 3209, 3089, 13, 50562], "temperature": 0.0, "avg_logprob": -0.29006606974500293, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.006488238926976919}, {"id": 282, "seek": 130262, "start": 1306.58, "end": 1307.58, "text": " I really like that idea.", "tokens": [50562, 286, 534, 411, 300, 1558, 13, 50612], "temperature": 0.0, "avg_logprob": -0.29006606974500293, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.006488238926976919}, {"id": 283, "seek": 130262, "start": 1307.58, "end": 1311.9399999999998, "text": " The idea is currently, if you look in the TensorFlow code, it's thousands and thousands", "tokens": [50612, 440, 1558, 307, 4362, 11, 498, 291, 574, 294, 264, 37624, 3089, 11, 309, 311, 5383, 293, 5383, 50830], "temperature": 0.0, "avg_logprob": -0.29006606974500293, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.006488238926976919}, {"id": 284, "seek": 130262, "start": 1311.9399999999998, "end": 1315.82, "text": " of lines of C++, all custom-written.", "tokens": [50830, 295, 3876, 295, 383, 25472, 11, 439, 2375, 12, 26859, 13, 51024], "temperature": 0.0, "avg_logprob": -0.29006606974500293, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.006488238926976919}, {"id": 285, "seek": 130262, "start": 1315.82, "end": 1320.5, "text": " The idea is you throw all that away and replace it with a small number of lines of TensorFlow", "tokens": [51024, 440, 1558, 307, 291, 3507, 439, 300, 1314, 293, 7406, 309, 365, 257, 1359, 1230, 295, 3876, 295, 37624, 51258], "temperature": 0.0, "avg_logprob": -0.29006606974500293, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.006488238926976919}, {"id": 286, "seek": 130262, "start": 1320.5, "end": 1323.06, "text": " code that get compiled through XLA.", "tokens": [51258, 3089, 300, 483, 36548, 807, 1783, 11435, 13, 51386], "temperature": 0.0, "avg_logprob": -0.29006606974500293, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.006488238926976919}, {"id": 287, "seek": 130262, "start": 1323.06, "end": 1328.86, "text": " So that's something that's actually got me pretty excited.", "tokens": [51386, 407, 300, 311, 746, 300, 311, 767, 658, 385, 1238, 2919, 13, 51676], "temperature": 0.0, "avg_logprob": -0.29006606974500293, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.006488238926976919}, {"id": 288, "seek": 132886, "start": 1328.86, "end": 1334.62, "text": " So TensorFlow is pretty interesting.", "tokens": [50364, 407, 37624, 307, 1238, 1880, 13, 50652], "temperature": 0.0, "avg_logprob": -0.23717490364523494, "compression_ratio": 1.3967391304347827, "no_speech_prob": 0.00041731499368324876}, {"id": 289, "seek": 132886, "start": 1334.62, "end": 1340.4199999999998, "text": " Having said that, it's kind of hideous.", "tokens": [50652, 10222, 848, 300, 11, 309, 311, 733, 295, 6479, 563, 13, 50942], "temperature": 0.0, "avg_logprob": -0.23717490364523494, "compression_ratio": 1.3967391304347827, "no_speech_prob": 0.00041731499368324876}, {"id": 290, "seek": 132886, "start": 1340.4199999999998, "end": 1345.34, "text": " The API is full of not-invented-here syndrome.", "tokens": [50942, 440, 9362, 307, 1577, 295, 406, 12, 259, 2475, 292, 12, 6703, 19371, 13, 51188], "temperature": 0.0, "avg_logprob": -0.23717490364523494, "compression_ratio": 1.3967391304347827, "no_speech_prob": 0.00041731499368324876}, {"id": 291, "seek": 132886, "start": 1345.34, "end": 1350.78, "text": " It's clearly written by a bunch of engineers who have not necessarily spent that much time", "tokens": [51188, 467, 311, 4448, 3720, 538, 257, 3840, 295, 11955, 567, 362, 406, 4725, 4418, 300, 709, 565, 51460], "temperature": 0.0, "avg_logprob": -0.23717490364523494, "compression_ratio": 1.3967391304347827, "no_speech_prob": 0.00041731499368324876}, {"id": 292, "seek": 132886, "start": 1350.78, "end": 1355.62, "text": " learning about the user interface of APIs.", "tokens": [51460, 2539, 466, 264, 4195, 9226, 295, 21445, 13, 51702], "temperature": 0.0, "avg_logprob": -0.23717490364523494, "compression_ratio": 1.3967391304347827, "no_speech_prob": 0.00041731499368324876}, {"id": 293, "seek": 135562, "start": 1355.62, "end": 1363.06, "text": " It's full of these Google-isms in terms of having to fit into their ecosystem.", "tokens": [50364, 467, 311, 1577, 295, 613, 3329, 12, 13539, 294, 2115, 295, 1419, 281, 3318, 666, 641, 11311, 13, 50736], "temperature": 0.0, "avg_logprob": -0.3217540780703227, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.008711314760148525}, {"id": 294, "seek": 135562, "start": 1363.06, "end": 1371.8999999999999, "text": " But most importantly, like Theano, you have to set up the whole computation graph and", "tokens": [50736, 583, 881, 8906, 11, 411, 440, 3730, 11, 291, 362, 281, 992, 493, 264, 1379, 24903, 4295, 293, 51178], "temperature": 0.0, "avg_logprob": -0.3217540780703227, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.008711314760148525}, {"id": 295, "seek": 135562, "start": 1371.8999999999999, "end": 1374.7399999999998, "text": " then you kind of go run.", "tokens": [51178, 550, 291, 733, 295, 352, 1190, 13, 51320], "temperature": 0.0, "avg_logprob": -0.3217540780703227, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.008711314760148525}, {"id": 296, "seek": 135562, "start": 1374.7399999999998, "end": 1378.6599999999999, "text": " Which means if you want to do stuff in your computation graph that involves conditionals,", "tokens": [51320, 3013, 1355, 498, 291, 528, 281, 360, 1507, 294, 428, 24903, 4295, 300, 11626, 4188, 1124, 11, 51516], "temperature": 0.0, "avg_logprob": -0.3217540780703227, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.008711314760148525}, {"id": 297, "seek": 135562, "start": 1378.6599999999999, "end": 1384.26, "text": " if-then statements, if this happens you do this other part of the loop, it's basically", "tokens": [51516, 498, 12, 19096, 12363, 11, 498, 341, 2314, 291, 360, 341, 661, 644, 295, 264, 6367, 11, 309, 311, 1936, 51796], "temperature": 0.0, "avg_logprob": -0.3217540780703227, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.008711314760148525}, {"id": 298, "seek": 135562, "start": 1384.26, "end": 1385.26, "text": " impossible.", "tokens": [51796, 6243, 13, 51846], "temperature": 0.0, "avg_logprob": -0.3217540780703227, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.008711314760148525}, {"id": 299, "seek": 138526, "start": 1385.26, "end": 1392.58, "text": " So that's Rachel.", "tokens": [50364, 407, 300, 311, 14246, 13, 50730], "temperature": 0.0, "avg_logprob": -0.37437637646993, "compression_ratio": 1.3910614525139664, "no_speech_prob": 0.0014550480991601944}, {"id": 300, "seek": 138526, "start": 1392.58, "end": 1399.82, "text": " It turns out that there's a very different way of programming neural nets, which is dynamic", "tokens": [50730, 467, 4523, 484, 300, 456, 311, 257, 588, 819, 636, 295, 9410, 18161, 36170, 11, 597, 307, 8546, 51092], "temperature": 0.0, "avg_logprob": -0.37437637646993, "compression_ratio": 1.3910614525139664, "no_speech_prob": 0.0014550480991601944}, {"id": 301, "seek": 138526, "start": 1399.82, "end": 1403.58, "text": " computation, otherwise known as define-through-run.", "tokens": [51092, 24903, 11, 5911, 2570, 382, 6964, 12, 11529, 12, 12997, 13, 51280], "temperature": 0.0, "avg_logprob": -0.37437637646993, "compression_ratio": 1.3910614525139664, "no_speech_prob": 0.0014550480991601944}, {"id": 302, "seek": 138526, "start": 1403.58, "end": 1411.74, "text": " There's a number of libraries that do this, Torch, PyTorch, Chainer, Dynet, they're the", "tokens": [51280, 821, 311, 257, 1230, 295, 15148, 300, 360, 341, 11, 7160, 339, 11, 9953, 51, 284, 339, 11, 33252, 260, 11, 413, 2534, 302, 11, 436, 434, 264, 51688], "temperature": 0.0, "avg_logprob": -0.37437637646993, "compression_ratio": 1.3910614525139664, "no_speech_prob": 0.0014550480991601944}, {"id": 303, "seek": 141174, "start": 1411.74, "end": 1416.18, "text": " ones that come to mind.", "tokens": [50364, 2306, 300, 808, 281, 1575, 13, 50586], "temperature": 0.0, "avg_logprob": -0.28043961860764194, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.04146165773272514}, {"id": 304, "seek": 141174, "start": 1416.18, "end": 1422.38, "text": " We're going to be looking at one that was released, but an early version was put out", "tokens": [50586, 492, 434, 516, 281, 312, 1237, 412, 472, 300, 390, 4736, 11, 457, 364, 2440, 3037, 390, 829, 484, 50896], "temperature": 0.0, "avg_logprob": -0.28043961860764194, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.04146165773272514}, {"id": 305, "seek": 141174, "start": 1422.38, "end": 1428.86, "text": " about a month ago called PyTorch, which I've started rewriting a lot of stuff in.", "tokens": [50896, 466, 257, 1618, 2057, 1219, 9953, 51, 284, 339, 11, 597, 286, 600, 1409, 319, 19868, 257, 688, 295, 1507, 294, 13, 51220], "temperature": 0.0, "avg_logprob": -0.28043961860764194, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.04146165773272514}, {"id": 306, "seek": 141174, "start": 1428.86, "end": 1434.14, "text": " A lot of the more complex stuff just becomes suddenly so much easier.", "tokens": [51220, 316, 688, 295, 264, 544, 3997, 1507, 445, 3643, 5800, 370, 709, 3571, 13, 51484], "temperature": 0.0, "avg_logprob": -0.28043961860764194, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.04146165773272514}, {"id": 307, "seek": 143414, "start": 1434.14, "end": 1441.8200000000002, "text": " Because it becomes easier to do more complex things, I often find I can create faster and", "tokens": [50364, 1436, 309, 3643, 3571, 281, 360, 544, 3997, 721, 11, 286, 2049, 915, 286, 393, 1884, 4663, 293, 50748], "temperature": 0.0, "avg_logprob": -0.22532799779152385, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09670231491327286}, {"id": 308, "seek": 143414, "start": 1441.8200000000002, "end": 1446.0, "text": " more concise code by using this approach.", "tokens": [50748, 544, 44882, 3089, 538, 1228, 341, 3109, 13, 50957], "temperature": 0.0, "avg_logprob": -0.22532799779152385, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09670231491327286}, {"id": 309, "seek": 143414, "start": 1446.0, "end": 1452.7, "text": " So even though PyTorch is very, very, very new, it is coming out of the same people that", "tokens": [50957, 407, 754, 1673, 9953, 51, 284, 339, 307, 588, 11, 588, 11, 588, 777, 11, 309, 307, 1348, 484, 295, 264, 912, 561, 300, 51292], "temperature": 0.0, "avg_logprob": -0.22532799779152385, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09670231491327286}, {"id": 310, "seek": 143414, "start": 1452.7, "end": 1458.14, "text": " built Torch, which really all of Facebook's systems build on top of.", "tokens": [51292, 3094, 7160, 339, 11, 597, 534, 439, 295, 4384, 311, 3652, 1322, 322, 1192, 295, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22532799779152385, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09670231491327286}, {"id": 311, "seek": 143414, "start": 1458.14, "end": 1463.3200000000002, "text": " I suspect that Facebook are in the process of moving across from Torch to PyTorch.", "tokens": [51564, 286, 9091, 300, 4384, 366, 294, 264, 1399, 295, 2684, 2108, 490, 7160, 339, 281, 9953, 51, 284, 339, 13, 51823], "temperature": 0.0, "avg_logprob": -0.22532799779152385, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09670231491327286}, {"id": 312, "seek": 146332, "start": 1463.5, "end": 1469.04, "text": " It's already full of incredibly cool stuff, as you'll see.", "tokens": [50373, 467, 311, 1217, 1577, 295, 6252, 1627, 1507, 11, 382, 291, 603, 536, 13, 50650], "temperature": 0.0, "avg_logprob": -0.40146572618599397, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.04208692908287048}, {"id": 313, "seek": 146332, "start": 1469.04, "end": 1475.48, "text": " So we will be using increasingly more and more PyTorch during this course.", "tokens": [50650, 407, 321, 486, 312, 1228, 12980, 544, 293, 544, 9953, 51, 284, 339, 1830, 341, 1164, 13, 50972], "temperature": 0.0, "avg_logprob": -0.40146572618599397, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.04208692908287048}, {"id": 314, "seek": 146332, "start": 1475.48, "end": 1478.04, "text": " Question 2", "tokens": [50972, 14464, 568, 51100], "temperature": 0.0, "avg_logprob": -0.40146572618599397, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.04208692908287048}, {"id": 315, "seek": 146332, "start": 1478.04, "end": 1486.3999999999999, "text": " Does pre-compiling mean that we'll write TensorFlow code and test it, and then when we train a", "tokens": [51100, 4402, 659, 12, 21541, 4883, 914, 300, 321, 603, 2464, 37624, 3089, 293, 1500, 309, 11, 293, 550, 562, 321, 3847, 257, 51518], "temperature": 0.0, "avg_logprob": -0.40146572618599397, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.04208692908287048}, {"id": 316, "seek": 146332, "start": 1486.3999999999999, "end": 1490.3999999999999, "text": " big model, then we pre-compile the code and train our model?", "tokens": [51518, 955, 2316, 11, 550, 321, 659, 12, 21541, 794, 264, 3089, 293, 3847, 527, 2316, 30, 51718], "temperature": 0.0, "avg_logprob": -0.40146572618599397, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.04208692908287048}, {"id": 317, "seek": 149040, "start": 1490.52, "end": 1491.92, "text": " Answer", "tokens": [50370, 24545, 50440], "temperature": 0.0, "avg_logprob": -0.3368216073641213, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.0010649588657543063}, {"id": 318, "seek": 149040, "start": 1491.92, "end": 1496.48, "text": " If we're talking about XLA, XLA can be used a number of ways.", "tokens": [50440, 759, 321, 434, 1417, 466, 1783, 11435, 11, 1783, 11435, 393, 312, 1143, 257, 1230, 295, 2098, 13, 50668], "temperature": 0.0, "avg_logprob": -0.3368216073641213, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.0010649588657543063}, {"id": 319, "seek": 149040, "start": 1496.48, "end": 1504.2, "text": " One is that you come up with some different kind of kernel, some different kind of factorization,", "tokens": [50668, 1485, 307, 300, 291, 808, 493, 365, 512, 819, 733, 295, 28256, 11, 512, 819, 733, 295, 5952, 2144, 11, 51054], "temperature": 0.0, "avg_logprob": -0.3368216073641213, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.0010649588657543063}, {"id": 320, "seek": 149040, "start": 1504.2, "end": 1505.2, "text": " something like that.", "tokens": [51054, 746, 411, 300, 13, 51104], "temperature": 0.0, "avg_logprob": -0.3368216073641213, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.0010649588657543063}, {"id": 321, "seek": 149040, "start": 1505.2, "end": 1510.72, "text": " You write it in TensorFlow, you compile it with XLA, and then you make it available to", "tokens": [51104, 509, 2464, 309, 294, 37624, 11, 291, 31413, 309, 365, 1783, 11435, 11, 293, 550, 291, 652, 309, 2435, 281, 51380], "temperature": 0.0, "avg_logprob": -0.3368216073641213, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.0010649588657543063}, {"id": 322, "seek": 149040, "start": 1510.72, "end": 1517.52, "text": " anybody so when they use your layer, they're getting this compiled optimized code.", "tokens": [51380, 4472, 370, 562, 436, 764, 428, 4583, 11, 436, 434, 1242, 341, 36548, 26941, 3089, 13, 51720], "temperature": 0.0, "avg_logprob": -0.3368216073641213, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.0010649588657543063}, {"id": 323, "seek": 151752, "start": 1517.52, "end": 1523.48, "text": " It could mean that when you use TensorFlow Serving, TensorFlow Serving might compile", "tokens": [50364, 467, 727, 914, 300, 562, 291, 764, 37624, 4210, 798, 11, 37624, 4210, 798, 1062, 31413, 50662], "temperature": 0.0, "avg_logprob": -0.33337085081799195, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.0006771925836801529}, {"id": 324, "seek": 151752, "start": 1523.48, "end": 1529.12, "text": " your code using XLA and be serving up an accelerated version of it.", "tokens": [50662, 428, 3089, 1228, 1783, 11435, 293, 312, 8148, 493, 364, 29763, 3037, 295, 309, 13, 50944], "temperature": 0.0, "avg_logprob": -0.33337085081799195, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.0006771925836801529}, {"id": 325, "seek": 151752, "start": 1529.12, "end": 1531.92, "text": " One example which came up was for RNNs.", "tokens": [50944, 1485, 1365, 597, 1361, 493, 390, 337, 45702, 45, 82, 13, 51084], "temperature": 0.0, "avg_logprob": -0.33337085081799195, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.0006771925836801529}, {"id": 326, "seek": 151752, "start": 1531.92, "end": 1538.72, "text": " RNNs often involve, nowadays as you'll learn, some kind of complex customizations of a bidirectional", "tokens": [51084, 45702, 45, 82, 2049, 9494, 11, 13434, 382, 291, 603, 1466, 11, 512, 733, 295, 3997, 2375, 14455, 295, 257, 12957, 621, 41048, 51424], "temperature": 0.0, "avg_logprob": -0.33337085081799195, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.0006771925836801529}, {"id": 327, "seek": 151752, "start": 1538.72, "end": 1543.28, "text": " layer and then some stacked layers and then a tension layer, and then fed into a separate", "tokens": [51424, 4583, 293, 550, 512, 28867, 7914, 293, 550, 257, 8980, 4583, 11, 293, 550, 4636, 666, 257, 4994, 51652], "temperature": 0.0, "avg_logprob": -0.33337085081799195, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.0006771925836801529}, {"id": 328, "seek": 151752, "start": 1543.28, "end": 1545.48, "text": " stacked decoder.", "tokens": [51652, 28867, 979, 19866, 13, 51762], "temperature": 0.0, "avg_logprob": -0.33337085081799195, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.0006771925836801529}, {"id": 329, "seek": 154548, "start": 1545.48, "end": 1553.88, "text": " You can fuse that together into a single layer called a bidirectional attention sequence", "tokens": [50364, 509, 393, 31328, 300, 1214, 666, 257, 2167, 4583, 1219, 257, 12957, 621, 41048, 3202, 8310, 50784], "temperature": 0.0, "avg_logprob": -0.37091833002427044, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004198778886348009}, {"id": 330, "seek": 154548, "start": 1553.88, "end": 1559.32, "text": " to sequence, which indeed, people have actually bought that kind of stuff.", "tokens": [50784, 281, 8310, 11, 597, 6451, 11, 561, 362, 767, 4243, 300, 733, 295, 1507, 13, 51056], "temperature": 0.0, "avg_logprob": -0.37091833002427044, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004198778886348009}, {"id": 331, "seek": 154548, "start": 1559.32, "end": 1567.52, "text": " There's various ways in which neural network compilation can be very helpful.", "tokens": [51056, 821, 311, 3683, 2098, 294, 597, 18161, 3209, 40261, 393, 312, 588, 4961, 13, 51466], "temperature": 0.0, "avg_logprob": -0.37091833002427044, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004198778886348009}, {"id": 332, "seek": 154548, "start": 1567.52, "end": 1574.28, "text": " Question What is the relationship between TensorFlow and PyTorch?", "tokens": [51466, 14464, 708, 307, 264, 2480, 1296, 37624, 293, 9953, 51, 284, 339, 30, 51804], "temperature": 0.0, "avg_logprob": -0.37091833002427044, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004198778886348009}, {"id": 333, "seek": 157428, "start": 1574.28, "end": 1575.28, "text": " There's no relationship.", "tokens": [50364, 821, 311, 572, 2480, 13, 50414], "temperature": 0.0, "avg_logprob": -0.37013218402862547, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.024053240194916725}, {"id": 334, "seek": 157428, "start": 1575.28, "end": 1579.8, "text": " TensorFlow is Google's thing.", "tokens": [50414, 37624, 307, 3329, 311, 551, 13, 50640], "temperature": 0.0, "avg_logprob": -0.37013218402862547, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.024053240194916725}, {"id": 335, "seek": 157428, "start": 1579.8, "end": 1586.84, "text": " PyTorch is kind of Facebook's thing, but it's also very much a community thing.", "tokens": [50640, 9953, 51, 284, 339, 307, 733, 295, 4384, 311, 551, 11, 457, 309, 311, 611, 588, 709, 257, 1768, 551, 13, 50992], "temperature": 0.0, "avg_logprob": -0.37013218402862547, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.024053240194916725}, {"id": 336, "seek": 157428, "start": 1586.84, "end": 1596.6, "text": " TensorFlow is a huge, complex beast of a system which uses all kinds of advanced software", "tokens": [50992, 37624, 307, 257, 2603, 11, 3997, 13464, 295, 257, 1185, 597, 4960, 439, 3685, 295, 7339, 4722, 51480], "temperature": 0.0, "avg_logprob": -0.37013218402862547, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.024053240194916725}, {"id": 337, "seek": 157428, "start": 1596.6, "end": 1599.8, "text": " engineering methods all over the place.", "tokens": [51480, 7043, 7150, 439, 670, 264, 1081, 13, 51640], "temperature": 0.0, "avg_logprob": -0.37013218402862547, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.024053240194916725}, {"id": 338, "seek": 157428, "start": 1599.8, "end": 1601.72, "text": " In theory that ought to make it terribly fast.", "tokens": [51640, 682, 5261, 300, 13416, 281, 652, 309, 22903, 2370, 13, 51736], "temperature": 0.0, "avg_logprob": -0.37013218402862547, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.024053240194916725}, {"id": 339, "seek": 160172, "start": 1601.72, "end": 1605.28, "text": " In practice, a recent benchmark actually showed it to be about the slowest.", "tokens": [50364, 682, 3124, 11, 257, 5162, 18927, 767, 4712, 309, 281, 312, 466, 264, 2964, 377, 13, 50542], "temperature": 0.0, "avg_logprob": -0.2744055818920293, "compression_ratio": 1.7423076923076923, "no_speech_prob": 0.011507934890687466}, {"id": 340, "seek": 160172, "start": 1605.28, "end": 1609.24, "text": " I think the reason is because it's so big and complex, it's so hard to get everything", "tokens": [50542, 286, 519, 264, 1778, 307, 570, 309, 311, 370, 955, 293, 3997, 11, 309, 311, 370, 1152, 281, 483, 1203, 50740], "temperature": 0.0, "avg_logprob": -0.2744055818920293, "compression_ratio": 1.7423076923076923, "no_speech_prob": 0.011507934890687466}, {"id": 341, "seek": 160172, "start": 1609.24, "end": 1610.24, "text": " to work together.", "tokens": [50740, 281, 589, 1214, 13, 50790], "temperature": 0.0, "avg_logprob": -0.2744055818920293, "compression_ratio": 1.7423076923076923, "no_speech_prob": 0.011507934890687466}, {"id": 342, "seek": 160172, "start": 1610.24, "end": 1616.32, "text": " In theory, PyTorch ought to be the slowest because this defined-by-run system means it's", "tokens": [50790, 682, 5261, 11, 9953, 51, 284, 339, 13416, 281, 312, 264, 2964, 377, 570, 341, 7642, 12, 2322, 12, 12997, 1185, 1355, 309, 311, 51094], "temperature": 0.0, "avg_logprob": -0.2744055818920293, "compression_ratio": 1.7423076923076923, "no_speech_prob": 0.011507934890687466}, {"id": 343, "seek": 160172, "start": 1616.32, "end": 1621.8, "text": " way less optimization that the systems can do, but it turned out to be amongst the fastest", "tokens": [51094, 636, 1570, 19618, 300, 264, 3652, 393, 360, 11, 457, 309, 3574, 484, 281, 312, 12918, 264, 14573, 51368], "temperature": 0.0, "avg_logprob": -0.2744055818920293, "compression_ratio": 1.7423076923076923, "no_speech_prob": 0.011507934890687466}, {"id": 344, "seek": 160172, "start": 1621.8, "end": 1629.24, "text": " because it's so easy to write code, it's so much easier to write good code.", "tokens": [51368, 570, 309, 311, 370, 1858, 281, 2464, 3089, 11, 309, 311, 370, 709, 3571, 281, 2464, 665, 3089, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2744055818920293, "compression_ratio": 1.7423076923076923, "no_speech_prob": 0.011507934890687466}, {"id": 345, "seek": 160172, "start": 1629.24, "end": 1630.24, "text": " It's interesting.", "tokens": [51740, 467, 311, 1880, 13, 51790], "temperature": 0.0, "avg_logprob": -0.2744055818920293, "compression_ratio": 1.7423076923076923, "no_speech_prob": 0.011507934890687466}, {"id": 346, "seek": 163024, "start": 1630.24, "end": 1635.52, "text": " There are such different approaches, I think it's going to be great to know both because", "tokens": [50364, 821, 366, 1270, 819, 11587, 11, 286, 519, 309, 311, 516, 281, 312, 869, 281, 458, 1293, 570, 50628], "temperature": 0.0, "avg_logprob": -0.29304174582163495, "compression_ratio": 1.7876106194690264, "no_speech_prob": 0.011331629008054733}, {"id": 347, "seek": 163024, "start": 1635.52, "end": 1638.64, "text": " there are going to be some things that are going to be fantastic in TensorFlow and some", "tokens": [50628, 456, 366, 516, 281, 312, 512, 721, 300, 366, 516, 281, 312, 5456, 294, 37624, 293, 512, 50784], "temperature": 0.0, "avg_logprob": -0.29304174582163495, "compression_ratio": 1.7876106194690264, "no_speech_prob": 0.011331629008054733}, {"id": 348, "seek": 163024, "start": 1638.64, "end": 1641.68, "text": " things that are going to be fantastic in PyTorch.", "tokens": [50784, 721, 300, 366, 516, 281, 312, 5456, 294, 9953, 51, 284, 339, 13, 50936], "temperature": 0.0, "avg_logprob": -0.29304174582163495, "compression_ratio": 1.7876106194690264, "no_speech_prob": 0.011331629008054733}, {"id": 349, "seek": 163024, "start": 1641.68, "end": 1648.28, "text": " They couldn't be more different, which is why I think they're two good things to learn.", "tokens": [50936, 814, 2809, 380, 312, 544, 819, 11, 597, 307, 983, 286, 519, 436, 434, 732, 665, 721, 281, 1466, 13, 51266], "temperature": 0.0, "avg_logprob": -0.29304174582163495, "compression_ratio": 1.7876106194690264, "no_speech_prob": 0.011331629008054733}, {"id": 350, "seek": 163024, "start": 1648.28, "end": 1657.76, "text": " So wrapping up this introductory part, I wanted to kind of change your expectations about", "tokens": [51266, 407, 21993, 493, 341, 39048, 644, 11, 286, 1415, 281, 733, 295, 1319, 428, 9843, 466, 51740], "temperature": 0.0, "avg_logprob": -0.29304174582163495, "compression_ratio": 1.7876106194690264, "no_speech_prob": 0.011331629008054733}, {"id": 351, "seek": 165776, "start": 1657.76, "end": 1661.28, "text": " how you've learned so far to how you're going to learn in the future.", "tokens": [50364, 577, 291, 600, 3264, 370, 1400, 281, 577, 291, 434, 516, 281, 1466, 294, 264, 2027, 13, 50540], "temperature": 0.0, "avg_logprob": -0.37134570701449526, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.07263581454753876}, {"id": 352, "seek": 165776, "start": 1661.28, "end": 1664.08, "text": " Part 1 to me was about showing you best practices.", "tokens": [50540, 4100, 502, 281, 385, 390, 466, 4099, 291, 1151, 7525, 13, 50680], "temperature": 0.0, "avg_logprob": -0.37134570701449526, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.07263581454753876}, {"id": 353, "seek": 165776, "start": 1664.08, "end": 1670.4, "text": " So generally it's like here's a library, here's a problem, you use these libraries, these", "tokens": [50680, 407, 5101, 309, 311, 411, 510, 311, 257, 6405, 11, 510, 311, 257, 1154, 11, 291, 764, 613, 15148, 11, 613, 50996], "temperature": 0.0, "avg_logprob": -0.37134570701449526, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.07263581454753876}, {"id": 354, "seek": 165776, "start": 1670.4, "end": 1674.92, "text": " steps to solve this problem, and you do it this way, and lo and behold we've gotten the", "tokens": [50996, 4439, 281, 5039, 341, 1154, 11, 293, 291, 360, 309, 341, 636, 11, 293, 450, 293, 27234, 321, 600, 5768, 264, 51222], "temperature": 0.0, "avg_logprob": -0.37134570701449526, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.07263581454753876}, {"id": 355, "seek": 165776, "start": 1674.92, "end": 1680.0, "text": " top 10 in this Kaggle competition.", "tokens": [51222, 1192, 1266, 294, 341, 48751, 22631, 6211, 13, 51476], "temperature": 0.0, "avg_logprob": -0.37134570701449526, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.07263581454753876}, {"id": 356, "seek": 165776, "start": 1680.0, "end": 1685.68, "text": " I tried to select things that had best practices.", "tokens": [51476, 286, 3031, 281, 3048, 721, 300, 632, 1151, 7525, 13, 51760], "temperature": 0.0, "avg_logprob": -0.37134570701449526, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.07263581454753876}, {"id": 357, "seek": 168568, "start": 1686.68, "end": 1689.72, "text": " You now know everything I know about best practices.", "tokens": [50414, 509, 586, 458, 1203, 286, 458, 466, 1151, 7525, 13, 50566], "temperature": 0.0, "avg_logprob": -0.2980519504081912, "compression_ratio": 1.5577889447236182, "no_speech_prob": 0.0017546105664223433}, {"id": 358, "seek": 168568, "start": 1689.72, "end": 1692.0800000000002, "text": " I don't really have anything else to tell you.", "tokens": [50566, 286, 500, 380, 534, 362, 1340, 1646, 281, 980, 291, 13, 50684], "temperature": 0.0, "avg_logprob": -0.2980519504081912, "compression_ratio": 1.5577889447236182, "no_speech_prob": 0.0017546105664223433}, {"id": 359, "seek": 168568, "start": 1692.0800000000002, "end": 1700.48, "text": " So we're now up to stuff I haven't quite figured out yet, nor has anybody else, but you probably", "tokens": [50684, 407, 321, 434, 586, 493, 281, 1507, 286, 2378, 380, 1596, 8932, 484, 1939, 11, 6051, 575, 4472, 1646, 11, 457, 291, 1391, 51104], "temperature": 0.0, "avg_logprob": -0.2980519504081912, "compression_ratio": 1.5577889447236182, "no_speech_prob": 0.0017546105664223433}, {"id": 360, "seek": 168568, "start": 1700.48, "end": 1702.3400000000001, "text": " need to know.", "tokens": [51104, 643, 281, 458, 13, 51197], "temperature": 0.0, "avg_logprob": -0.2980519504081912, "compression_ratio": 1.5577889447236182, "no_speech_prob": 0.0017546105664223433}, {"id": 361, "seek": 168568, "start": 1702.3400000000001, "end": 1710.72, "text": " So some of it, for example, like neural translation, that's an example of something that is solved.", "tokens": [51197, 407, 512, 295, 309, 11, 337, 1365, 11, 411, 18161, 12853, 11, 300, 311, 364, 1365, 295, 746, 300, 307, 13041, 13, 51616], "temperature": 0.0, "avg_logprob": -0.2980519504081912, "compression_ratio": 1.5577889447236182, "no_speech_prob": 0.0017546105664223433}, {"id": 362, "seek": 171072, "start": 1710.72, "end": 1715.32, "text": " Google solved it, but they haven't released the way they solved it.", "tokens": [50364, 3329, 13041, 309, 11, 457, 436, 2378, 380, 4736, 264, 636, 436, 13041, 309, 13, 50594], "temperature": 0.0, "avg_logprob": -0.28242054163852587, "compression_ratio": 1.7637130801687764, "no_speech_prob": 0.06954008340835571}, {"id": 363, "seek": 171072, "start": 1715.32, "end": 1720.2, "text": " So the rest of us are trying to put everything together and figure out how to make something", "tokens": [50594, 407, 264, 1472, 295, 505, 366, 1382, 281, 829, 1203, 1214, 293, 2573, 484, 577, 281, 652, 746, 50838], "temperature": 0.0, "avg_logprob": -0.28242054163852587, "compression_ratio": 1.7637130801687764, "no_speech_prob": 0.06954008340835571}, {"id": 364, "seek": 171072, "start": 1720.2, "end": 1723.76, "text": " work as well as Google made that work.", "tokens": [50838, 589, 382, 731, 382, 3329, 1027, 300, 589, 13, 51016], "temperature": 0.0, "avg_logprob": -0.28242054163852587, "compression_ratio": 1.7637130801687764, "no_speech_prob": 0.06954008340835571}, {"id": 365, "seek": 171072, "start": 1723.76, "end": 1730.6000000000001, "text": " More often it's going to be, here's a sequence of things you can do that can get some pretty", "tokens": [51016, 5048, 2049, 309, 311, 516, 281, 312, 11, 510, 311, 257, 8310, 295, 721, 291, 393, 360, 300, 393, 483, 512, 1238, 51358], "temperature": 0.0, "avg_logprob": -0.28242054163852587, "compression_ratio": 1.7637130801687764, "no_speech_prob": 0.06954008340835571}, {"id": 366, "seek": 171072, "start": 1730.6000000000001, "end": 1735.48, "text": " good results here, but there's a thousand things you can do to make it better that no", "tokens": [51358, 665, 3542, 510, 11, 457, 456, 311, 257, 4714, 721, 291, 393, 360, 281, 652, 309, 1101, 300, 572, 51602], "temperature": 0.0, "avg_logprob": -0.28242054163852587, "compression_ratio": 1.7637130801687764, "no_speech_prob": 0.06954008340835571}, {"id": 367, "seek": 171072, "start": 1735.48, "end": 1739.64, "text": " one's tried yet, so that's interesting.", "tokens": [51602, 472, 311, 3031, 1939, 11, 370, 300, 311, 1880, 13, 51810], "temperature": 0.0, "avg_logprob": -0.28242054163852587, "compression_ratio": 1.7637130801687764, "no_speech_prob": 0.06954008340835571}, {"id": 368, "seek": 173964, "start": 1739.64, "end": 1745.5200000000002, "text": " Or thirdly, it could be, here's a sequence of things that solves this pretty well, but", "tokens": [50364, 1610, 2636, 356, 11, 309, 727, 312, 11, 510, 311, 257, 8310, 295, 721, 300, 39890, 341, 1238, 731, 11, 457, 50658], "temperature": 0.0, "avg_logprob": -0.3099694486524238, "compression_ratio": 1.6600790513833992, "no_speech_prob": 0.0002453691267874092}, {"id": 369, "seek": 173964, "start": 1745.5200000000002, "end": 1749.68, "text": " gosh we wrote a lot of custom code there, didn't we?", "tokens": [50658, 6502, 321, 4114, 257, 688, 295, 2375, 3089, 456, 11, 994, 380, 321, 30, 50866], "temperature": 0.0, "avg_logprob": -0.3099694486524238, "compression_ratio": 1.6600790513833992, "no_speech_prob": 0.0002453691267874092}, {"id": 370, "seek": 173964, "start": 1749.68, "end": 1753.0800000000002, "text": " I'm sure this could be abstracted really nicely, but no one's done that yet.", "tokens": [50866, 286, 478, 988, 341, 727, 312, 12649, 292, 534, 9594, 11, 457, 572, 472, 311, 1096, 300, 1939, 13, 51036], "temperature": 0.0, "avg_logprob": -0.3099694486524238, "compression_ratio": 1.6600790513833992, "no_speech_prob": 0.0002453691267874092}, {"id": 371, "seek": 173964, "start": 1753.0800000000002, "end": 1755.3600000000001, "text": " So they're kind of the 3 main categories.", "tokens": [51036, 407, 436, 434, 733, 295, 264, 805, 2135, 10479, 13, 51150], "temperature": 0.0, "avg_logprob": -0.3099694486524238, "compression_ratio": 1.6600790513833992, "no_speech_prob": 0.0002453691267874092}, {"id": 372, "seek": 173964, "start": 1755.3600000000001, "end": 1759.44, "text": " So generally at the end of each class, it won't be like, okay that's it, that's how", "tokens": [51150, 407, 5101, 412, 264, 917, 295, 1184, 1508, 11, 309, 1582, 380, 312, 411, 11, 1392, 300, 311, 309, 11, 300, 311, 577, 51354], "temperature": 0.0, "avg_logprob": -0.3099694486524238, "compression_ratio": 1.6600790513833992, "no_speech_prob": 0.0002453691267874092}, {"id": 373, "seek": 173964, "start": 1759.44, "end": 1760.44, "text": " you do this thing.", "tokens": [51354, 291, 360, 341, 551, 13, 51404], "temperature": 0.0, "avg_logprob": -0.3099694486524238, "compression_ratio": 1.6600790513833992, "no_speech_prob": 0.0002453691267874092}, {"id": 374, "seek": 173964, "start": 1760.44, "end": 1764.0400000000002, "text": " It will be more like, here are the things you can explore.", "tokens": [51404, 467, 486, 312, 544, 411, 11, 510, 366, 264, 721, 291, 393, 6839, 13, 51584], "temperature": 0.0, "avg_logprob": -0.3099694486524238, "compression_ratio": 1.6600790513833992, "no_speech_prob": 0.0002453691267874092}, {"id": 375, "seek": 176404, "start": 1764.04, "end": 1771.52, "text": " So the homework will be, pick one of these interesting things and dig into it.", "tokens": [50364, 407, 264, 14578, 486, 312, 11, 1888, 472, 295, 613, 1880, 721, 293, 2528, 666, 309, 13, 50738], "temperature": 0.0, "avg_logprob": -0.27404258824601957, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.024053068831562996}, {"id": 376, "seek": 176404, "start": 1771.52, "end": 1776.76, "text": " And generally speaking, that homework will get you to a point that probably no one's", "tokens": [50738, 400, 5101, 4124, 11, 300, 14578, 486, 483, 291, 281, 257, 935, 300, 1391, 572, 472, 311, 51000], "temperature": 0.0, "avg_logprob": -0.27404258824601957, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.024053068831562996}, {"id": 377, "seek": 176404, "start": 1776.76, "end": 1780.28, "text": " done before, or at least probably no one's written down before.", "tokens": [51000, 1096, 949, 11, 420, 412, 1935, 1391, 572, 472, 311, 3720, 760, 949, 13, 51176], "temperature": 0.0, "avg_logprob": -0.27404258824601957, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.024053068831562996}, {"id": 378, "seek": 176404, "start": 1780.28, "end": 1789.3999999999999, "text": " I found as I built this, I think nearly every single piece of code I'm presenting, I was", "tokens": [51176, 286, 1352, 382, 286, 3094, 341, 11, 286, 519, 6217, 633, 2167, 2522, 295, 3089, 286, 478, 15578, 11, 286, 390, 51632], "temperature": 0.0, "avg_logprob": -0.27404258824601957, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.024053068831562996}, {"id": 379, "seek": 178940, "start": 1789.4, "end": 1794.2, "text": " unable to find anything online which did that thing correctly.", "tokens": [50364, 11299, 281, 915, 1340, 2950, 597, 630, 300, 551, 8944, 13, 50604], "temperature": 0.0, "avg_logprob": -0.3034574742219886, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.1081872284412384}, {"id": 380, "seek": 178940, "start": 1794.2, "end": 1799.88, "text": " There was often example code that claimed to be something like that, but again and again", "tokens": [50604, 821, 390, 2049, 1365, 3089, 300, 12941, 281, 312, 746, 411, 300, 11, 457, 797, 293, 797, 50888], "temperature": 0.0, "avg_logprob": -0.3034574742219886, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.1081872284412384}, {"id": 381, "seek": 178940, "start": 1799.88, "end": 1802.2, "text": " I found it was missing huge pieces.", "tokens": [50888, 286, 1352, 309, 390, 5361, 2603, 3755, 13, 51004], "temperature": 0.0, "avg_logprob": -0.3034574742219886, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.1081872284412384}, {"id": 382, "seek": 178940, "start": 1802.2, "end": 1806.16, "text": " We'll talk about some of the things it was missing as we go.", "tokens": [51004, 492, 603, 751, 466, 512, 295, 264, 721, 309, 390, 5361, 382, 321, 352, 13, 51202], "temperature": 0.0, "avg_logprob": -0.3034574742219886, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.1081872284412384}, {"id": 383, "seek": 178940, "start": 1806.16, "end": 1811.44, "text": " One very common one was it would only work on a single item at a time, it wouldn't work", "tokens": [51202, 1485, 588, 2689, 472, 390, 309, 576, 787, 589, 322, 257, 2167, 3174, 412, 257, 565, 11, 309, 2759, 380, 589, 51466], "temperature": 0.0, "avg_logprob": -0.3034574742219886, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.1081872284412384}, {"id": 384, "seek": 178940, "start": 1811.44, "end": 1816.24, "text": " with a batch, therefore the GPU is basically totally wasted.", "tokens": [51466, 365, 257, 15245, 11, 4412, 264, 18407, 307, 1936, 3879, 19496, 13, 51706], "temperature": 0.0, "avg_logprob": -0.3034574742219886, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.1081872284412384}, {"id": 385, "seek": 181624, "start": 1816.24, "end": 1821.08, "text": " Or it failed to get anywhere near the performance that was claimed in the paper that it was", "tokens": [50364, 1610, 309, 7612, 281, 483, 4992, 2651, 264, 3389, 300, 390, 12941, 294, 264, 3035, 300, 309, 390, 50606], "temperature": 0.0, "avg_logprob": -0.3130242891400774, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.03461821377277374}, {"id": 386, "seek": 181624, "start": 1821.08, "end": 1824.08, "text": " meant to be based on.", "tokens": [50606, 4140, 281, 312, 2361, 322, 13, 50756], "temperature": 0.0, "avg_logprob": -0.3130242891400774, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.03461821377277374}, {"id": 387, "seek": 181624, "start": 1824.08, "end": 1828.52, "text": " So generally speaking, there's going to be lots of opportunities if you're interested", "tokens": [50756, 407, 5101, 4124, 11, 456, 311, 516, 281, 312, 3195, 295, 4786, 498, 291, 434, 3102, 50978], "temperature": 0.0, "avg_logprob": -0.3130242891400774, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.03461821377277374}, {"id": 388, "seek": 181624, "start": 1828.52, "end": 1833.36, "text": " to write a little blog post about the things you tried and what worked and what didn't.", "tokens": [50978, 281, 2464, 257, 707, 6968, 2183, 466, 264, 721, 291, 3031, 293, 437, 2732, 293, 437, 994, 380, 13, 51220], "temperature": 0.0, "avg_logprob": -0.3130242891400774, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.03461821377277374}, {"id": 389, "seek": 181624, "start": 1833.36, "end": 1839.4, "text": " And you'll generally find that there's no other post like that out there.", "tokens": [51220, 400, 291, 603, 5101, 915, 300, 456, 311, 572, 661, 2183, 411, 300, 484, 456, 13, 51522], "temperature": 0.0, "avg_logprob": -0.3130242891400774, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.03461821377277374}, {"id": 390, "seek": 181624, "start": 1839.4, "end": 1844.72, "text": " Particularly if you pick a dataset that's in your domain area, it's very unlikely that", "tokens": [51522, 32281, 498, 291, 1888, 257, 28872, 300, 311, 294, 428, 9274, 1859, 11, 309, 311, 588, 17518, 300, 51788], "temperature": 0.0, "avg_logprob": -0.3130242891400774, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.03461821377277374}, {"id": 391, "seek": 184472, "start": 1844.72, "end": 1845.72, "text": " somebody has written it.", "tokens": [50364, 2618, 575, 3720, 309, 13, 50414], "temperature": 0.0, "avg_logprob": -0.41744962552698645, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.07055781036615372}, {"id": 392, "seek": 184472, "start": 1845.72, "end": 1851.72, "text": " Question 2 Can we use TensorFlow and Torch together?", "tokens": [50414, 14464, 568, 1664, 321, 764, 37624, 293, 7160, 339, 1214, 30, 50714], "temperature": 0.0, "avg_logprob": -0.41744962552698645, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.07055781036615372}, {"id": 393, "seek": 184472, "start": 1851.72, "end": 1855.84, "text": " Answer I don't say Torch, say PyTorch.", "tokens": [50714, 24545, 286, 500, 380, 584, 7160, 339, 11, 584, 9953, 51, 284, 339, 13, 50920], "temperature": 0.0, "avg_logprob": -0.41744962552698645, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.07055781036615372}, {"id": 394, "seek": 184472, "start": 1855.84, "end": 1864.72, "text": " Torch is very similar, but it's written in Lua, which is a very small embedded language,", "tokens": [50920, 7160, 339, 307, 588, 2531, 11, 457, 309, 311, 3720, 294, 441, 4398, 11, 597, 307, 257, 588, 1359, 16741, 2856, 11, 51364], "temperature": 0.0, "avg_logprob": -0.41744962552698645, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.07055781036615372}, {"id": 395, "seek": 184472, "start": 1864.72, "end": 1870.32, "text": " very good for what it is, but not very good for what we want to do.", "tokens": [51364, 588, 665, 337, 437, 309, 307, 11, 457, 406, 588, 665, 337, 437, 321, 528, 281, 360, 13, 51644], "temperature": 0.0, "avg_logprob": -0.41744962552698645, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.07055781036615372}, {"id": 396, "seek": 187032, "start": 1870.32, "end": 1877.8, "text": " So PyTorch is kind of a port of Torch into Python, which is pretty cool.", "tokens": [50364, 407, 9953, 51, 284, 339, 307, 733, 295, 257, 2436, 295, 7160, 339, 666, 15329, 11, 597, 307, 1238, 1627, 13, 50738], "temperature": 0.0, "avg_logprob": -0.3375388210259595, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.007232693489640951}, {"id": 397, "seek": 187032, "start": 1877.8, "end": 1880.1599999999999, "text": " So can you use them together?", "tokens": [50738, 407, 393, 291, 764, 552, 1214, 30, 50856], "temperature": 0.0, "avg_logprob": -0.3375388210259595, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.007232693489640951}, {"id": 398, "seek": 187032, "start": 1880.1599999999999, "end": 1882.32, "text": " Yeah, sure.", "tokens": [50856, 865, 11, 988, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3375388210259595, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.007232693489640951}, {"id": 399, "seek": 187032, "start": 1882.32, "end": 1883.6799999999998, "text": " We'll kind of see a bit of that.", "tokens": [50964, 492, 603, 733, 295, 536, 257, 857, 295, 300, 13, 51032], "temperature": 0.0, "avg_logprob": -0.3375388210259595, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.007232693489640951}, {"id": 400, "seek": 187032, "start": 1883.6799999999998, "end": 1888.48, "text": " In general, you can do a few steps with TensorFlow to get to a certain point, and a few more", "tokens": [51032, 682, 2674, 11, 291, 393, 360, 257, 1326, 4439, 365, 37624, 281, 483, 281, 257, 1629, 935, 11, 293, 257, 1326, 544, 51272], "temperature": 0.0, "avg_logprob": -0.3375388210259595, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.007232693489640951}, {"id": 401, "seek": 187032, "start": 1888.48, "end": 1890.4399999999998, "text": " steps with PyTorch.", "tokens": [51272, 4439, 365, 9953, 51, 284, 339, 13, 51370], "temperature": 0.0, "avg_logprob": -0.3375388210259595, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.007232693489640951}, {"id": 402, "seek": 187032, "start": 1890.4399999999998, "end": 1894.8, "text": " You can't integrate them into the same network because they're very different approaches,", "tokens": [51370, 509, 393, 380, 13365, 552, 666, 264, 912, 3209, 570, 436, 434, 588, 819, 11587, 11, 51588], "temperature": 0.0, "avg_logprob": -0.3375388210259595, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.007232693489640951}, {"id": 403, "seek": 189480, "start": 1894.8, "end": 1902.84, "text": " but you can certainly solve a problem with the two of them together.", "tokens": [50364, 457, 291, 393, 3297, 5039, 257, 1154, 365, 264, 732, 295, 552, 1214, 13, 50766], "temperature": 0.0, "avg_logprob": -0.3169749906693382, "compression_ratio": 1.481651376146789, "no_speech_prob": 0.1480426788330078}, {"id": 404, "seek": 189480, "start": 1902.84, "end": 1908.48, "text": " For those of you who have some money left over, I would strongly suggest building a", "tokens": [50766, 1171, 729, 295, 291, 567, 362, 512, 1460, 1411, 670, 11, 286, 576, 10613, 3402, 2390, 257, 51048], "temperature": 0.0, "avg_logprob": -0.3169749906693382, "compression_ratio": 1.481651376146789, "no_speech_prob": 0.1480426788330078}, {"id": 405, "seek": 189480, "start": 1908.48, "end": 1909.48, "text": " box.", "tokens": [51048, 2424, 13, 51098], "temperature": 0.0, "avg_logprob": -0.3169749906693382, "compression_ratio": 1.481651376146789, "no_speech_prob": 0.1480426788330078}, {"id": 406, "seek": 189480, "start": 1909.48, "end": 1918.12, "text": " The reason I suggest building a box is because you're paying $0.90 an hour for a P2.", "tokens": [51098, 440, 1778, 286, 3402, 2390, 257, 2424, 307, 570, 291, 434, 6229, 1848, 15, 13, 7771, 364, 1773, 337, 257, 430, 17, 13, 51530], "temperature": 0.0, "avg_logprob": -0.3169749906693382, "compression_ratio": 1.481651376146789, "no_speech_prob": 0.1480426788330078}, {"id": 407, "seek": 189480, "start": 1918.12, "end": 1924.04, "text": " I know a lot of you are spending a couple of hundred bucks a month on AWS bills.", "tokens": [51530, 286, 458, 257, 688, 295, 291, 366, 6434, 257, 1916, 295, 3262, 11829, 257, 1618, 322, 17650, 12433, 13, 51826], "temperature": 0.0, "avg_logprob": -0.3169749906693382, "compression_ratio": 1.481651376146789, "no_speech_prob": 0.1480426788330078}, {"id": 408, "seek": 192404, "start": 1924.28, "end": 1934.2, "text": " Here is a box that costs $550 and will be about twice as fast as a P2.", "tokens": [50376, 1692, 307, 257, 2424, 300, 5497, 1848, 20, 2803, 293, 486, 312, 466, 6091, 382, 2370, 382, 257, 430, 17, 13, 50872], "temperature": 0.0, "avg_logprob": -0.28569369316101073, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0012842996511608362}, {"id": 409, "seek": 192404, "start": 1934.2, "end": 1943.6, "text": " It's just not good value to use a P2, and it's way slower than it needs to be.", "tokens": [50872, 467, 311, 445, 406, 665, 2158, 281, 764, 257, 430, 17, 11, 293, 309, 311, 636, 14009, 813, 309, 2203, 281, 312, 13, 51342], "temperature": 0.0, "avg_logprob": -0.28569369316101073, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0012842996511608362}, {"id": 410, "seek": 192404, "start": 1943.6, "end": 1949.32, "text": " Also building a box, it's one of the many things that's just good to learn, is understanding", "tokens": [51342, 2743, 2390, 257, 2424, 11, 309, 311, 472, 295, 264, 867, 721, 300, 311, 445, 665, 281, 1466, 11, 307, 3701, 51628], "temperature": 0.0, "avg_logprob": -0.28569369316101073, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0012842996511608362}, {"id": 411, "seek": 192404, "start": 1949.32, "end": 1951.6, "text": " how everything fits together.", "tokens": [51628, 577, 1203, 9001, 1214, 13, 51742], "temperature": 0.0, "avg_logprob": -0.28569369316101073, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0012842996511608362}, {"id": 412, "seek": 195160, "start": 1951.6, "end": 1959.36, "text": " So I've got some suggestions here about what box to build for various different budgets.", "tokens": [50364, 407, 286, 600, 658, 512, 13396, 510, 466, 437, 2424, 281, 1322, 337, 3683, 819, 26708, 13, 50752], "temperature": 0.0, "avg_logprob": -0.3326007781490203, "compression_ratio": 1.3645833333333333, "no_speech_prob": 0.00020027352729812264}, {"id": 413, "seek": 195160, "start": 1959.36, "end": 1964.8799999999999, "text": " You certainly don't have to, but this is my recommendation.", "tokens": [50752, 509, 3297, 500, 380, 362, 281, 11, 457, 341, 307, 452, 11879, 13, 51028], "temperature": 0.0, "avg_logprob": -0.3326007781490203, "compression_ratio": 1.3645833333333333, "no_speech_prob": 0.00020027352729812264}, {"id": 414, "seek": 195160, "start": 1964.8799999999999, "end": 1968.4399999999998, "text": " Couple of points to make.", "tokens": [51028, 38266, 295, 2793, 281, 652, 13, 51206], "temperature": 0.0, "avg_logprob": -0.3326007781490203, "compression_ratio": 1.3645833333333333, "no_speech_prob": 0.00020027352729812264}, {"id": 415, "seek": 195160, "start": 1968.4399999999998, "end": 1975.08, "text": " More RAM helps more than I think people who discuss this stuff online quite appreciate.", "tokens": [51206, 5048, 14561, 3665, 544, 813, 286, 519, 561, 567, 2248, 341, 1507, 2950, 1596, 4449, 13, 51538], "temperature": 0.0, "avg_logprob": -0.3326007781490203, "compression_ratio": 1.3645833333333333, "no_speech_prob": 0.00020027352729812264}, {"id": 416, "seek": 197508, "start": 1976.08, "end": 1981.96, "text": " 12GB of RAM means twice as big of batch sizes, which means half as many steps necessary to", "tokens": [50414, 2272, 8769, 295, 14561, 1355, 6091, 382, 955, 295, 15245, 11602, 11, 597, 1355, 1922, 382, 867, 4439, 4818, 281, 50708], "temperature": 0.0, "avg_logprob": -0.34407601789994674, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.008985311724245548}, {"id": 417, "seek": 197508, "start": 1981.96, "end": 1982.96, "text": " get through an epoch.", "tokens": [50708, 483, 807, 364, 30992, 339, 13, 50758], "temperature": 0.0, "avg_logprob": -0.34407601789994674, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.008985311724245548}, {"id": 418, "seek": 197508, "start": 1982.96, "end": 1987.72, "text": " It means more stable gradients, which means you can use higher learning rates.", "tokens": [50758, 467, 1355, 544, 8351, 2771, 2448, 11, 597, 1355, 291, 393, 764, 2946, 2539, 6846, 13, 50996], "temperature": 0.0, "avg_logprob": -0.34407601789994674, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.008985311724245548}, {"id": 419, "seek": 197508, "start": 1987.72, "end": 1990.96, "text": " So more RAM I think is often underappreciated.", "tokens": [50996, 407, 544, 14561, 286, 519, 307, 2049, 833, 1746, 3326, 770, 13, 51158], "temperature": 0.0, "avg_logprob": -0.34407601789994674, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.008985311724245548}, {"id": 420, "seek": 197508, "start": 1990.96, "end": 1996.04, "text": " The TitanX is the card that has 12GB RAM.", "tokens": [51158, 440, 17731, 55, 307, 264, 2920, 300, 575, 2272, 8769, 14561, 13, 51412], "temperature": 0.0, "avg_logprob": -0.34407601789994674, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.008985311724245548}, {"id": 421, "seek": 197508, "start": 1996.04, "end": 2001.9199999999998, "text": " It is a lot more expensive, but you can get the previous generation's version second-hand,", "tokens": [51412, 467, 307, 257, 688, 544, 5124, 11, 457, 291, 393, 483, 264, 3894, 5125, 311, 3037, 1150, 12, 5543, 11, 51706], "temperature": 0.0, "avg_logprob": -0.34407601789994674, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.008985311724245548}, {"id": 422, "seek": 197508, "start": 2001.9199999999998, "end": 2002.9199999999998, "text": " it's called the MaxWelt.", "tokens": [51706, 309, 311, 1219, 264, 7402, 54, 2018, 13, 51756], "temperature": 0.0, "avg_logprob": -0.34407601789994674, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.008985311724245548}, {"id": 423, "seek": 200292, "start": 2003.24, "end": 2008.52, "text": " There's a TitanX Pascal, which is the current one, or the TitanX MaxWelt, which is the previous", "tokens": [50380, 821, 311, 257, 17731, 55, 41723, 11, 597, 307, 264, 2190, 472, 11, 420, 264, 17731, 55, 7402, 54, 2018, 11, 597, 307, 264, 3894, 50644], "temperature": 0.0, "avg_logprob": -0.31376579370391505, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.008985343389213085}, {"id": 424, "seek": 200292, "start": 2008.52, "end": 2010.02, "text": " generation one.", "tokens": [50644, 5125, 472, 13, 50719], "temperature": 0.0, "avg_logprob": -0.31376579370391505, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.008985343389213085}, {"id": 425, "seek": 200292, "start": 2010.02, "end": 2014.8400000000001, "text": " The previous generation one is not a big step back at all, it still has 12GB RAM.", "tokens": [50719, 440, 3894, 5125, 472, 307, 406, 257, 955, 1823, 646, 412, 439, 11, 309, 920, 575, 2272, 8769, 14561, 13, 50960], "temperature": 0.0, "avg_logprob": -0.31376579370391505, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.008985343389213085}, {"id": 426, "seek": 200292, "start": 2014.8400000000001, "end": 2021.92, "text": " If you can get one used, that would be a great option.", "tokens": [50960, 759, 291, 393, 483, 472, 1143, 11, 300, 576, 312, 257, 869, 3614, 13, 51314], "temperature": 0.0, "avg_logprob": -0.31376579370391505, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.008985343389213085}, {"id": 427, "seek": 200292, "start": 2021.92, "end": 2028.52, "text": " The GTX 1080 and GTX 1070 are absolutely fantastic as well.", "tokens": [51314, 440, 17530, 55, 24547, 293, 17530, 55, 1266, 5867, 366, 3122, 5456, 382, 731, 13, 51644], "temperature": 0.0, "avg_logprob": -0.31376579370391505, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.008985343389213085}, {"id": 428, "seek": 202852, "start": 2028.52, "end": 2034.76, "text": " They're nearly as good as the TitanX, but they just have 8GB rather than 12GB.", "tokens": [50364, 814, 434, 6217, 382, 665, 382, 264, 17731, 55, 11, 457, 436, 445, 362, 1649, 8769, 2831, 813, 2272, 8769, 13, 50676], "temperature": 0.0, "avg_logprob": -0.29273621432752495, "compression_ratio": 1.385, "no_speech_prob": 0.04958781972527504}, {"id": 429, "seek": 202852, "start": 2034.76, "end": 2041.76, "text": " Going back to a GTX 980, which is the previous generation consumer top-end card, you have", "tokens": [50676, 10963, 646, 281, 257, 17530, 55, 1722, 4702, 11, 597, 307, 264, 3894, 5125, 9711, 1192, 12, 521, 2920, 11, 291, 362, 51026], "temperature": 0.0, "avg_logprob": -0.29273621432752495, "compression_ratio": 1.385, "no_speech_prob": 0.04958781972527504}, {"id": 430, "seek": 202852, "start": 2041.76, "end": 2043.52, "text": " the RAM again.", "tokens": [51026, 264, 14561, 797, 13, 51114], "temperature": 0.0, "avg_logprob": -0.29273621432752495, "compression_ratio": 1.385, "no_speech_prob": 0.04958781972527504}, {"id": 431, "seek": 202852, "start": 2043.52, "end": 2049.44, "text": " So of all the places you're going to spend money on a box, put nearly all of it into", "tokens": [51114, 407, 295, 439, 264, 3190, 291, 434, 516, 281, 3496, 1460, 322, 257, 2424, 11, 829, 6217, 439, 295, 309, 666, 51410], "temperature": 0.0, "avg_logprob": -0.29273621432752495, "compression_ratio": 1.385, "no_speech_prob": 0.04958781972527504}, {"id": 432, "seek": 202852, "start": 2049.44, "end": 2051.12, "text": " the GPU.", "tokens": [51410, 264, 18407, 13, 51494], "temperature": 0.0, "avg_logprob": -0.29273621432752495, "compression_ratio": 1.385, "no_speech_prob": 0.04958781972527504}, {"id": 433, "seek": 205112, "start": 2051.12, "end": 2062.12, "text": " Every one of these steps, the 1070, the TitanX, Pascal, they're big steps up.", "tokens": [50364, 2048, 472, 295, 613, 4439, 11, 264, 1266, 5867, 11, 264, 17731, 55, 11, 41723, 11, 436, 434, 955, 4439, 493, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3837893926180326, "compression_ratio": 1.4504132231404958, "no_speech_prob": 0.09946703910827637}, {"id": 434, "seek": 205112, "start": 2062.12, "end": 2066.56, "text": " As you will have seen from Part 1, if you've got more RAM, it really helps because you", "tokens": [50914, 1018, 291, 486, 362, 1612, 490, 4100, 502, 11, 498, 291, 600, 658, 544, 14561, 11, 309, 534, 3665, 570, 291, 51136], "temperature": 0.0, "avg_logprob": -0.3837893926180326, "compression_ratio": 1.4504132231404958, "no_speech_prob": 0.09946703910827637}, {"id": 435, "seek": 205112, "start": 2066.56, "end": 2069.2, "text": " can pre-compute more stuff and keep it in RAM.", "tokens": [51136, 393, 659, 12, 21541, 1169, 544, 1507, 293, 1066, 309, 294, 14561, 13, 51268], "temperature": 0.0, "avg_logprob": -0.3837893926180326, "compression_ratio": 1.4504132231404958, "no_speech_prob": 0.09946703910827637}, {"id": 436, "seek": 205112, "start": 2069.2, "end": 2074.6, "text": " Having said that, there's a new kind of hard drive called an NVMe drive.", "tokens": [51268, 10222, 848, 300, 11, 456, 311, 257, 777, 733, 295, 1152, 3332, 1219, 364, 46512, 12671, 3332, 13, 51538], "temperature": 0.0, "avg_logprob": -0.3837893926180326, "compression_ratio": 1.4504132231404958, "no_speech_prob": 0.09946703910827637}, {"id": 437, "seek": 205112, "start": 2074.6, "end": 2077.2799999999997, "text": " Non-volatile memory, I think.", "tokens": [51538, 8774, 12, 9646, 17445, 4675, 11, 286, 519, 13, 51672], "temperature": 0.0, "avg_logprob": -0.3837893926180326, "compression_ratio": 1.4504132231404958, "no_speech_prob": 0.09946703910827637}, {"id": 438, "seek": 205112, "start": 2077.2799999999997, "end": 2079.92, "text": " NVMe drives are quite extraordinary.", "tokens": [51672, 46512, 12671, 11754, 366, 1596, 10581, 13, 51804], "temperature": 0.0, "avg_logprob": -0.3837893926180326, "compression_ratio": 1.4504132231404958, "no_speech_prob": 0.09946703910827637}, {"id": 439, "seek": 207992, "start": 2079.92, "end": 2087.8, "text": " They're not that far away from RAM-like speeds, but they're hard drives, they're persistent.", "tokens": [50364, 814, 434, 406, 300, 1400, 1314, 490, 14561, 12, 4092, 16411, 11, 457, 436, 434, 1152, 11754, 11, 436, 434, 24315, 13, 50758], "temperature": 0.0, "avg_logprob": -0.3541146136344747, "compression_ratio": 1.5, "no_speech_prob": 0.36656418442726135}, {"id": 440, "seek": 207992, "start": 2087.8, "end": 2092.92, "text": " You have to get a special kind of motherboard, but if you can afford it, sometimes it's only", "tokens": [50758, 509, 362, 281, 483, 257, 2121, 733, 295, 32916, 11, 457, 498, 291, 393, 6157, 309, 11, 2171, 309, 311, 787, 51014], "temperature": 0.0, "avg_logprob": -0.3541146136344747, "compression_ratio": 1.5, "no_speech_prob": 0.36656418442726135}, {"id": 441, "seek": 207992, "start": 2092.92, "end": 2097.96, "text": " like $400 or $500 to get an NVMe drive.", "tokens": [51014, 411, 1848, 13741, 420, 1848, 7526, 281, 483, 364, 46512, 12671, 3332, 13, 51266], "temperature": 0.0, "avg_logprob": -0.3541146136344747, "compression_ratio": 1.5, "no_speech_prob": 0.36656418442726135}, {"id": 442, "seek": 207992, "start": 2097.96, "end": 2103.92, "text": " That's going to really allow you to put all of your currently used data on that drive", "tokens": [51266, 663, 311, 516, 281, 534, 2089, 291, 281, 829, 439, 295, 428, 4362, 1143, 1412, 322, 300, 3332, 51564], "temperature": 0.0, "avg_logprob": -0.3541146136344747, "compression_ratio": 1.5, "no_speech_prob": 0.36656418442726135}, {"id": 443, "seek": 207992, "start": 2103.92, "end": 2105.92, "text": " and access it very, very quickly.", "tokens": [51564, 293, 2105, 309, 588, 11, 588, 2661, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3541146136344747, "compression_ratio": 1.5, "no_speech_prob": 0.36656418442726135}, {"id": 444, "seek": 210592, "start": 2105.92, "end": 2106.92, "text": " So that's my other tip.", "tokens": [50364, 407, 300, 311, 452, 661, 4125, 13, 50414], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 445, "seek": 210592, "start": 2106.92, "end": 2107.92, "text": " Question 2.", "tokens": [50414, 14464, 568, 13, 50464], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 446, "seek": 210592, "start": 2107.92, "end": 2110.92, "text": " Doesn't the batch size also depend heavily on the video RAM?", "tokens": [50464, 12955, 380, 264, 15245, 2744, 611, 5672, 10950, 322, 264, 960, 14561, 30, 50614], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 447, "seek": 210592, "start": 2110.92, "end": 2111.92, "text": " Answer.", "tokens": [50614, 24545, 13, 50664], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 448, "seek": 210592, "start": 2111.92, "end": 2112.92, "text": " That's what I was referring to, the 12GB, 8GB.", "tokens": [50664, 663, 311, 437, 286, 390, 13761, 281, 11, 264, 2272, 8769, 11, 1649, 8769, 13, 50714], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 449, "seek": 210592, "start": 2112.92, "end": 2117.92, "text": " I'm talking about the RAM that's on the video card on the GPU.", "tokens": [50714, 286, 478, 1417, 466, 264, 14561, 300, 311, 322, 264, 960, 2920, 322, 264, 18407, 13, 50964], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 450, "seek": 210592, "start": 2117.92, "end": 2118.92, "text": " Question 3.", "tokens": [50964, 14464, 805, 13, 51014], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 451, "seek": 210592, "start": 2118.92, "end": 2120.92, "text": " Does upgrading RAM allow bigger batch sizes?", "tokens": [51014, 4402, 36249, 14561, 2089, 3801, 15245, 11602, 30, 51114], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 452, "seek": 210592, "start": 2120.92, "end": 2121.92, "text": " Answer.", "tokens": [51114, 24545, 13, 51164], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 453, "seek": 210592, "start": 2121.92, "end": 2127.52, "text": " Upgrading the video card's RAM, you can't upgrade the RAM on the card.", "tokens": [51164, 5858, 7165, 278, 264, 960, 2920, 311, 14561, 11, 291, 393, 380, 11484, 264, 14561, 322, 264, 2920, 13, 51444], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 454, "seek": 210592, "start": 2127.52, "end": 2130.92, "text": " You buy a card that has X amount of RAM.", "tokens": [51444, 509, 2256, 257, 2920, 300, 575, 1783, 2372, 295, 14561, 13, 51614], "temperature": 0.0, "avg_logprob": -0.35047974625254064, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.4493754506111145}, {"id": 455, "seek": 213092, "start": 2130.92, "end": 2135.92, "text": " So you can buy a card that has 12, GTX 1080, 8, GTX 980, 4.", "tokens": [50364, 407, 291, 393, 2256, 257, 2920, 300, 575, 2272, 11, 17530, 55, 24547, 11, 1649, 11, 17530, 55, 1722, 4702, 11, 1017, 13, 50614], "temperature": 0.0, "avg_logprob": -0.37730615445882965, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.10818545520305634}, {"id": 456, "seek": 213092, "start": 2135.92, "end": 2138.32, "text": " So that's on the card.", "tokens": [50614, 407, 300, 311, 322, 264, 2920, 13, 50734], "temperature": 0.0, "avg_logprob": -0.37730615445882965, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.10818545520305634}, {"id": 457, "seek": 213092, "start": 2138.32, "end": 2142.8, "text": " Upgrading the amount of RAM that's in your computer doesn't change your batch size, it", "tokens": [50734, 5858, 7165, 278, 264, 2372, 295, 14561, 300, 311, 294, 428, 3820, 1177, 380, 1319, 428, 15245, 2744, 11, 309, 50958], "temperature": 0.0, "avg_logprob": -0.37730615445882965, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.10818545520305634}, {"id": 458, "seek": 213092, "start": 2142.8, "end": 2147.8, "text": " just changes the amount you can pre-compute.", "tokens": [50958, 445, 2962, 264, 2372, 291, 393, 659, 12, 21541, 1169, 13, 51208], "temperature": 0.0, "avg_logprob": -0.37730615445882965, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.10818545520305634}, {"id": 459, "seek": 213092, "start": 2147.8, "end": 2153.96, "text": " Unless you use an NVMe drive, in which case RAM is much less important.", "tokens": [51208, 16581, 291, 764, 364, 46512, 12671, 3332, 11, 294, 597, 1389, 14561, 307, 709, 1570, 1021, 13, 51516], "temperature": 0.0, "avg_logprob": -0.37730615445882965, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.10818545520305634}, {"id": 460, "seek": 213092, "start": 2153.96, "end": 2156.8, "text": " You don't have to plug everything in.", "tokens": [51516, 509, 500, 380, 362, 281, 5452, 1203, 294, 13, 51658], "temperature": 0.0, "avg_logprob": -0.37730615445882965, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.10818545520305634}, {"id": 461, "seek": 215680, "start": 2156.8, "end": 2160.76, "text": " You can go to Central Computers, which is a San Francisco computer shop, for example,", "tokens": [50364, 509, 393, 352, 281, 9701, 37804, 433, 11, 597, 307, 257, 5271, 12279, 3820, 3945, 11, 337, 1365, 11, 50562], "temperature": 0.0, "avg_logprob": -0.3739796090633311, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.10818295925855637}, {"id": 462, "seek": 215680, "start": 2160.76, "end": 2163.96, "text": " and they'll put it all together for you.", "tokens": [50562, 293, 436, 603, 829, 309, 439, 1214, 337, 291, 13, 50722], "temperature": 0.0, "avg_logprob": -0.3739796090633311, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.10818295925855637}, {"id": 463, "seek": 215680, "start": 2163.96, "end": 2168.44, "text": " There's a fantastic thread on the forums.", "tokens": [50722, 821, 311, 257, 5456, 7207, 322, 264, 26998, 13, 50946], "temperature": 0.0, "avg_logprob": -0.3739796090633311, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.10818295925855637}, {"id": 464, "seek": 215680, "start": 2168.44, "end": 2172.48, "text": " Brendan who's one of the participants in the course has a great Medium post, went there", "tokens": [50946, 48484, 567, 311, 472, 295, 264, 10503, 294, 264, 1164, 575, 257, 869, 38915, 2183, 11, 1437, 456, 51148], "temperature": 0.0, "avg_logprob": -0.3739796090633311, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.10818295925855637}, {"id": 465, "seek": 215680, "start": 2172.48, "end": 2177.0800000000004, "text": " just explaining his whole journey to getting something built and set up.", "tokens": [51148, 445, 13468, 702, 1379, 4671, 281, 1242, 746, 3094, 293, 992, 493, 13, 51378], "temperature": 0.0, "avg_logprob": -0.3739796090633311, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.10818295925855637}, {"id": 466, "seek": 215680, "start": 2177.0800000000004, "end": 2182.88, "text": " So there's lots of stuff there to help you.", "tokens": [51378, 407, 456, 311, 3195, 295, 1507, 456, 281, 854, 291, 13, 51668], "temperature": 0.0, "avg_logprob": -0.3739796090633311, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.10818295925855637}, {"id": 467, "seek": 218288, "start": 2183.04, "end": 2187.48, "text": " It's time to build your box and while you wait for things to install, it's time to start", "tokens": [50372, 467, 311, 565, 281, 1322, 428, 2424, 293, 1339, 291, 1699, 337, 721, 281, 3625, 11, 309, 311, 565, 281, 722, 50594], "temperature": 0.0, "avg_logprob": -0.28346299110574924, "compression_ratio": 1.4678899082568808, "no_speech_prob": 0.05108213797211647}, {"id": 468, "seek": 218288, "start": 2187.48, "end": 2189.28, "text": " reading papers.", "tokens": [50594, 3760, 10577, 13, 50684], "temperature": 0.0, "avg_logprob": -0.28346299110574924, "compression_ratio": 1.4678899082568808, "no_speech_prob": 0.05108213797211647}, {"id": 469, "seek": 218288, "start": 2189.28, "end": 2195.6, "text": " So papers are, if you're a philosophy graduate like me, terrifying.", "tokens": [50684, 407, 10577, 366, 11, 498, 291, 434, 257, 10675, 8080, 411, 385, 11, 18106, 13, 51000], "temperature": 0.0, "avg_logprob": -0.28346299110574924, "compression_ratio": 1.4678899082568808, "no_speech_prob": 0.05108213797211647}, {"id": 470, "seek": 218288, "start": 2195.6, "end": 2201.76, "text": " They look like theorem 4.1 and glorily 4.2 on the left.", "tokens": [51000, 814, 574, 411, 20904, 1017, 13, 16, 293, 1563, 284, 953, 1017, 13, 17, 322, 264, 1411, 13, 51308], "temperature": 0.0, "avg_logprob": -0.28346299110574924, "compression_ratio": 1.4678899082568808, "no_speech_prob": 0.05108213797211647}, {"id": 471, "seek": 218288, "start": 2201.76, "end": 2206.92, "text": " But that is an extract from the Atom paper.", "tokens": [51308, 583, 300, 307, 364, 8947, 490, 264, 1711, 298, 3035, 13, 51566], "temperature": 0.0, "avg_logprob": -0.28346299110574924, "compression_ratio": 1.4678899082568808, "no_speech_prob": 0.05108213797211647}, {"id": 472, "seek": 218288, "start": 2206.92, "end": 2212.2400000000002, "text": " You all know how to do Atom in Microsoft Excel.", "tokens": [51566, 509, 439, 458, 577, 281, 360, 1711, 298, 294, 8116, 19060, 13, 51832], "temperature": 0.0, "avg_logprob": -0.28346299110574924, "compression_ratio": 1.4678899082568808, "no_speech_prob": 0.05108213797211647}, {"id": 473, "seek": 221224, "start": 2212.6, "end": 2219.12, "text": " It's amazing how most papers manage to make simple things incredibly complex.", "tokens": [50382, 467, 311, 2243, 577, 881, 10577, 3067, 281, 652, 2199, 721, 6252, 3997, 13, 50708], "temperature": 0.0, "avg_logprob": -0.2752607981363932, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002182711148634553}, {"id": 474, "seek": 221224, "start": 2219.12, "end": 2225.24, "text": " And a lot of that is because academics need to show other academics how worthy they are", "tokens": [50708, 400, 257, 688, 295, 300, 307, 570, 25695, 643, 281, 855, 661, 25695, 577, 14829, 436, 366, 51014], "temperature": 0.0, "avg_logprob": -0.2752607981363932, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002182711148634553}, {"id": 475, "seek": 221224, "start": 2225.24, "end": 2231.12, "text": " of a conference spot, which means showing off all their fancy math skills.", "tokens": [51014, 295, 257, 7586, 4008, 11, 597, 1355, 4099, 766, 439, 641, 10247, 5221, 3942, 13, 51308], "temperature": 0.0, "avg_logprob": -0.2752607981363932, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002182711148634553}, {"id": 476, "seek": 221224, "start": 2231.12, "end": 2238.56, "text": " So if you really need a proof of the convergence of your optimizer, rather than just running", "tokens": [51308, 407, 498, 291, 534, 643, 257, 8177, 295, 264, 32181, 295, 428, 5028, 6545, 11, 2831, 813, 445, 2614, 51680], "temperature": 0.0, "avg_logprob": -0.2752607981363932, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002182711148634553}, {"id": 477, "seek": 223856, "start": 2238.56, "end": 2244.08, "text": " it and seeing if it works, you can study theorem 4.1 and glorily 4.2 and blah blah", "tokens": [50364, 309, 293, 2577, 498, 309, 1985, 11, 291, 393, 2979, 20904, 1017, 13, 16, 293, 1563, 284, 953, 1017, 13, 17, 293, 12288, 12288, 50640], "temperature": 0.0, "avg_logprob": -0.2971755904380721, "compression_ratio": 1.6125, "no_speech_prob": 0.14414523541927338}, {"id": 478, "seek": 223856, "start": 2244.08, "end": 2245.08, "text": " blah.", "tokens": [50640, 12288, 13, 50690], "temperature": 0.0, "avg_logprob": -0.2971755904380721, "compression_ratio": 1.6125, "no_speech_prob": 0.14414523541927338}, {"id": 479, "seek": 223856, "start": 2245.08, "end": 2253.36, "text": " In general though, the way philosophy graduates read papers is to read the abstract, find", "tokens": [50690, 682, 2674, 1673, 11, 264, 636, 10675, 13577, 1401, 10577, 307, 281, 1401, 264, 12649, 11, 915, 51104], "temperature": 0.0, "avg_logprob": -0.2971755904380721, "compression_ratio": 1.6125, "no_speech_prob": 0.14414523541927338}, {"id": 480, "seek": 223856, "start": 2253.36, "end": 2259.52, "text": " out what problem they're solving, read the introduction to learn more about that problem", "tokens": [51104, 484, 437, 1154, 436, 434, 12606, 11, 1401, 264, 9339, 281, 1466, 544, 466, 300, 1154, 51412], "temperature": 0.0, "avg_logprob": -0.2971755904380721, "compression_ratio": 1.6125, "no_speech_prob": 0.14414523541927338}, {"id": 481, "seek": 223856, "start": 2259.52, "end": 2263.7599999999998, "text": " and how previous people have tackled it, jump to the bit at the end called experiments to", "tokens": [51412, 293, 577, 3894, 561, 362, 9426, 1493, 309, 11, 3012, 281, 264, 857, 412, 264, 917, 1219, 12050, 281, 51624], "temperature": 0.0, "avg_logprob": -0.2971755904380721, "compression_ratio": 1.6125, "no_speech_prob": 0.14414523541927338}, {"id": 482, "seek": 223856, "start": 2263.7599999999998, "end": 2266.0, "text": " see how well the thing works.", "tokens": [51624, 536, 577, 731, 264, 551, 1985, 13, 51736], "temperature": 0.0, "avg_logprob": -0.2971755904380721, "compression_ratio": 1.6125, "no_speech_prob": 0.14414523541927338}, {"id": 483, "seek": 226600, "start": 2266.0, "end": 2270.6, "text": " If it works really well, jump back to the bit which has the pseudocode in and try to", "tokens": [50364, 759, 309, 1985, 534, 731, 11, 3012, 646, 281, 264, 857, 597, 575, 264, 25505, 532, 905, 1429, 294, 293, 853, 281, 50594], "temperature": 0.0, "avg_logprob": -0.3091496521571897, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.08509736508131027}, {"id": 484, "seek": 226600, "start": 2270.6, "end": 2271.6, "text": " get that to work.", "tokens": [50594, 483, 300, 281, 589, 13, 50644], "temperature": 0.0, "avg_logprob": -0.3091496521571897, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.08509736508131027}, {"id": 485, "seek": 226600, "start": 2271.6, "end": 2276.04, "text": " Ideally, hopefully in the meantime, finding that somebody else has written a blog post", "tokens": [50644, 40817, 11, 4696, 294, 264, 14991, 11, 5006, 300, 2618, 1646, 575, 3720, 257, 6968, 2183, 50866], "temperature": 0.0, "avg_logprob": -0.3091496521571897, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.08509736508131027}, {"id": 486, "seek": 226600, "start": 2276.04, "end": 2280.56, "text": " in simple English like this example with Atom.", "tokens": [50866, 294, 2199, 3669, 411, 341, 1365, 365, 1711, 298, 13, 51092], "temperature": 0.0, "avg_logprob": -0.3091496521571897, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.08509736508131027}, {"id": 487, "seek": 226600, "start": 2280.56, "end": 2288.12, "text": " So don't be disheartened when you start reading deep learning papers, unless you have a math", "tokens": [51092, 407, 500, 380, 312, 717, 12864, 5320, 562, 291, 722, 3760, 2452, 2539, 10577, 11, 5969, 291, 362, 257, 5221, 51470], "temperature": 0.0, "avg_logprob": -0.3091496521571897, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.08509736508131027}, {"id": 488, "seek": 226600, "start": 2288.12, "end": 2289.12, "text": " background.", "tokens": [51470, 3678, 13, 51520], "temperature": 0.0, "avg_logprob": -0.3091496521571897, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.08509736508131027}, {"id": 489, "seek": 226600, "start": 2289.12, "end": 2295.92, "text": " Well even if you're a PhD in math and they're still terrifying.", "tokens": [51520, 1042, 754, 498, 291, 434, 257, 14476, 294, 5221, 293, 436, 434, 920, 18106, 13, 51860], "temperature": 0.0, "avg_logprob": -0.3091496521571897, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.08509736508131027}, {"id": 490, "seek": 229592, "start": 2296.04, "end": 2301.84, "text": " I'm sure there's a lot of people who are complaining about a paper just today.", "tokens": [50370, 286, 478, 988, 456, 311, 257, 688, 295, 561, 567, 366, 20740, 466, 257, 3035, 445, 965, 13, 50660], "temperature": 0.0, "avg_logprob": -0.4324871826171875, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.07158999890089035}, {"id": 491, "seek": 229592, "start": 2301.84, "end": 2304.16, "text": " You will learn to read the papers.", "tokens": [50660, 509, 486, 1466, 281, 1401, 264, 10577, 13, 50776], "temperature": 0.0, "avg_logprob": -0.4324871826171875, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.07158999890089035}, {"id": 492, "seek": 229592, "start": 2304.16, "end": 2308.84, "text": " The other thing I'll say is that you'll even see now, there will be a bit that's like,", "tokens": [50776, 440, 661, 551, 286, 603, 584, 307, 300, 291, 603, 754, 536, 586, 11, 456, 486, 312, 257, 857, 300, 311, 411, 11, 51010], "temperature": 0.0, "avg_logprob": -0.4324871826171875, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.07158999890089035}, {"id": 493, "seek": 229592, "start": 2308.84, "end": 2312.8, "text": " and then we use a softmax layer and there will be the equation for a softmax layer.", "tokens": [51010, 293, 550, 321, 764, 257, 2787, 41167, 4583, 293, 456, 486, 312, 264, 5367, 337, 257, 2787, 41167, 4583, 13, 51208], "temperature": 0.0, "avg_logprob": -0.4324871826171875, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.07158999890089035}, {"id": 494, "seek": 229592, "start": 2312.8, "end": 2315.6, "text": " You'll look at the equation and be like, what the hell?", "tokens": [51208, 509, 603, 574, 412, 264, 5367, 293, 312, 411, 11, 437, 264, 4921, 30, 51348], "temperature": 0.0, "avg_logprob": -0.4324871826171875, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.07158999890089035}, {"id": 495, "seek": 229592, "start": 2315.6, "end": 2319.6, "text": " And then it's like, oh I already know what a softmax layer is.", "tokens": [51348, 400, 550, 309, 311, 411, 11, 1954, 286, 1217, 458, 437, 257, 2787, 41167, 4583, 307, 13, 51548], "temperature": 0.0, "avg_logprob": -0.4324871826171875, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.07158999890089035}, {"id": 496, "seek": 229592, "start": 2319.6, "end": 2322.2000000000003, "text": " And then we use an LSTM.", "tokens": [51548, 400, 550, 321, 764, 364, 441, 6840, 44, 13, 51678], "temperature": 0.0, "avg_logprob": -0.4324871826171875, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.07158999890089035}, {"id": 497, "seek": 232220, "start": 2322.2, "end": 2326.9199999999996, "text": " Apparently still in every paper they write the damn LSTM equations as if that's any help", "tokens": [50364, 16755, 920, 294, 633, 3035, 436, 2464, 264, 8151, 441, 6840, 44, 11787, 382, 498, 300, 311, 604, 854, 50600], "temperature": 0.0, "avg_logprob": -0.3174196583356044, "compression_ratio": 1.790035587188612, "no_speech_prob": 0.1191999614238739}, {"id": 498, "seek": 232220, "start": 2326.9199999999996, "end": 2327.9199999999996, "text": " to anybody.", "tokens": [50600, 281, 4472, 13, 50650], "temperature": 0.0, "avg_logprob": -0.3174196583356044, "compression_ratio": 1.790035587188612, "no_speech_prob": 0.1191999614238739}, {"id": 499, "seek": 232220, "start": 2327.9199999999996, "end": 2331.52, "text": " But okay, it adds more Greek symbols, so be it.", "tokens": [50650, 583, 1392, 11, 309, 10860, 544, 10281, 16944, 11, 370, 312, 309, 13, 50830], "temperature": 0.0, "avg_logprob": -0.3174196583356044, "compression_ratio": 1.790035587188612, "no_speech_prob": 0.1191999614238739}, {"id": 500, "seek": 232220, "start": 2331.52, "end": 2338.7599999999998, "text": " Talking of Greek symbols, it's very hard to read and remember things you can't pronounce.", "tokens": [50830, 22445, 295, 10281, 16944, 11, 309, 311, 588, 1152, 281, 1401, 293, 1604, 721, 291, 393, 380, 19567, 13, 51192], "temperature": 0.0, "avg_logprob": -0.3174196583356044, "compression_ratio": 1.790035587188612, "no_speech_prob": 0.1191999614238739}, {"id": 501, "seek": 232220, "start": 2338.7599999999998, "end": 2343.3999999999996, "text": " So if you don't know how to read the Greek letters, Google the Greek alphabet and learn", "tokens": [51192, 407, 498, 291, 500, 380, 458, 577, 281, 1401, 264, 10281, 7825, 11, 3329, 264, 10281, 23339, 293, 1466, 51424], "temperature": 0.0, "avg_logprob": -0.3174196583356044, "compression_ratio": 1.790035587188612, "no_speech_prob": 0.1191999614238739}, {"id": 502, "seek": 232220, "start": 2343.3999999999996, "end": 2345.2, "text": " how to say them.", "tokens": [51424, 577, 281, 584, 552, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3174196583356044, "compression_ratio": 1.790035587188612, "no_speech_prob": 0.1191999614238739}, {"id": 503, "seek": 232220, "start": 2345.2, "end": 2349.16, "text": " It's just so much easier when you can look at an equation and rather go squiggle something,", "tokens": [51514, 467, 311, 445, 370, 709, 3571, 562, 291, 393, 574, 412, 364, 5367, 293, 2831, 352, 2339, 19694, 746, 11, 51712], "temperature": 0.0, "avg_logprob": -0.3174196583356044, "compression_ratio": 1.790035587188612, "no_speech_prob": 0.1191999614238739}, {"id": 504, "seek": 232220, "start": 2349.16, "end": 2352.0, "text": " squiggle something, you can say alpha something and beta something.", "tokens": [51712, 2339, 19694, 746, 11, 291, 393, 584, 8961, 746, 293, 9861, 746, 13, 51854], "temperature": 0.0, "avg_logprob": -0.3174196583356044, "compression_ratio": 1.790035587188612, "no_speech_prob": 0.1191999614238739}, {"id": 505, "seek": 235200, "start": 2352.8, "end": 2356.16, "text": " I know it's a small little thing, but it does make a big difference.", "tokens": [50404, 286, 458, 309, 311, 257, 1359, 707, 551, 11, 457, 309, 775, 652, 257, 955, 2649, 13, 50572], "temperature": 0.0, "avg_logprob": -0.2507219193856927, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.0004305548791307956}, {"id": 506, "seek": 235200, "start": 2356.16, "end": 2359.68, "text": " So we are all there to help each other read papers.", "tokens": [50572, 407, 321, 366, 439, 456, 281, 854, 1184, 661, 1401, 10577, 13, 50748], "temperature": 0.0, "avg_logprob": -0.2507219193856927, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.0004305548791307956}, {"id": 507, "seek": 235200, "start": 2359.68, "end": 2364.76, "text": " The reason we need to read papers is because as of now, a lot of the things we're doing", "tokens": [50748, 440, 1778, 321, 643, 281, 1401, 10577, 307, 570, 382, 295, 586, 11, 257, 688, 295, 264, 721, 321, 434, 884, 51002], "temperature": 0.0, "avg_logprob": -0.2507219193856927, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.0004305548791307956}, {"id": 508, "seek": 235200, "start": 2364.76, "end": 2375.68, "text": " only exist in very recent paper form.", "tokens": [51002, 787, 2514, 294, 588, 5162, 3035, 1254, 13, 51548], "temperature": 0.0, "avg_logprob": -0.2507219193856927, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.0004305548791307956}, {"id": 509, "seek": 235200, "start": 2375.68, "end": 2378.08, "text": " I really think writing is a good idea.", "tokens": [51548, 286, 534, 519, 3579, 307, 257, 665, 1558, 13, 51668], "temperature": 0.0, "avg_logprob": -0.2507219193856927, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.0004305548791307956}, {"id": 510, "seek": 237808, "start": 2378.08, "end": 2382.92, "text": " In fact, all of your projects I hope will end up in at least one blog.", "tokens": [50364, 682, 1186, 11, 439, 295, 428, 4455, 286, 1454, 486, 917, 493, 294, 412, 1935, 472, 6968, 13, 50606], "temperature": 0.0, "avg_logprob": -0.2760389768160306, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.4999157190322876}, {"id": 511, "seek": 237808, "start": 2382.92, "end": 2387.98, "text": " If you don't have a blog, medium.com is a great place to write.", "tokens": [50606, 759, 291, 500, 380, 362, 257, 6968, 11, 6399, 13, 1112, 307, 257, 869, 1081, 281, 2464, 13, 50859], "temperature": 0.0, "avg_logprob": -0.2760389768160306, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.4999157190322876}, {"id": 512, "seek": 237808, "start": 2387.98, "end": 2392.4, "text": " We would love to feature your work on fast.ai.", "tokens": [50859, 492, 576, 959, 281, 4111, 428, 589, 322, 2370, 13, 1301, 13, 51080], "temperature": 0.0, "avg_logprob": -0.2760389768160306, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.4999157190322876}, {"id": 513, "seek": 237808, "start": 2392.4, "end": 2396.08, "text": " So tell us about what you create.", "tokens": [51080, 407, 980, 505, 466, 437, 291, 1884, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2760389768160306, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.4999157190322876}, {"id": 514, "seek": 237808, "start": 2396.08, "end": 2402.5, "text": " We're very keen for more people to get into the deep learning community.", "tokens": [51264, 492, 434, 588, 20297, 337, 544, 561, 281, 483, 666, 264, 2452, 2539, 1768, 13, 51585], "temperature": 0.0, "avg_logprob": -0.2760389768160306, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.4999157190322876}, {"id": 515, "seek": 237808, "start": 2402.5, "end": 2406.92, "text": " When you write this stuff, say hey this is some stuff based on this course I'm doing", "tokens": [51585, 1133, 291, 2464, 341, 1507, 11, 584, 4177, 341, 307, 512, 1507, 2361, 322, 341, 1164, 286, 478, 884, 51806], "temperature": 0.0, "avg_logprob": -0.2760389768160306, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.4999157190322876}, {"id": 516, "seek": 240692, "start": 2407.08, "end": 2410.8, "text": " and here's what I've learnt and here's what I've tried and here's what I've found out.", "tokens": [50372, 293, 510, 311, 437, 286, 600, 18991, 293, 510, 311, 437, 286, 600, 3031, 293, 510, 311, 437, 286, 600, 1352, 484, 13, 50558], "temperature": 0.0, "avg_logprob": -0.37048878179532346, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.6688043475151062}, {"id": 517, "seek": 240692, "start": 2410.8, "end": 2412.8, "text": " Put the code on GitHub.", "tokens": [50558, 4935, 264, 3089, 322, 23331, 13, 50658], "temperature": 0.0, "avg_logprob": -0.37048878179532346, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.6688043475151062}, {"id": 518, "seek": 240692, "start": 2412.8, "end": 2414.64, "text": " It's amazing.", "tokens": [50658, 467, 311, 2243, 13, 50750], "temperature": 0.0, "avg_logprob": -0.37048878179532346, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.6688043475151062}, {"id": 519, "seek": 240692, "start": 2414.64, "end": 2423.6800000000003, "text": " Even us putting our little AWS setup scripts on GitHub for the MOOC, Rachel had a dozen", "tokens": [50750, 2754, 505, 3372, 527, 707, 17650, 8657, 23294, 322, 23331, 337, 264, 49197, 34, 11, 14246, 632, 257, 16654, 51202], "temperature": 0.0, "avg_logprob": -0.37048878179532346, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.6688043475151062}, {"id": 520, "seek": 240692, "start": 2423.6800000000003, "end": 2431.32, "text": " pull requests within a week with all kinds of little tidbits of like if you're on this", "tokens": [51202, 2235, 12475, 1951, 257, 1243, 365, 439, 3685, 295, 707, 9422, 34010, 295, 411, 498, 291, 434, 322, 341, 51584], "temperature": 0.0, "avg_logprob": -0.37048878179532346, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.6688043475151062}, {"id": 521, "seek": 240692, "start": 2431.32, "end": 2436.04, "text": " version of Mac, this helps this bit, or I've abstracted this out to make it work in Ireland", "tokens": [51584, 3037, 295, 5707, 11, 341, 3665, 341, 857, 11, 420, 286, 600, 12649, 292, 341, 484, 281, 652, 309, 589, 294, 15880, 51820], "temperature": 0.0, "avg_logprob": -0.37048878179532346, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.6688043475151062}, {"id": 522, "seek": 243604, "start": 2436.16, "end": 2439.16, "text": " as well as in America.", "tokens": [50370, 382, 731, 382, 294, 3374, 13, 50520], "temperature": 0.0, "avg_logprob": -0.3375802088265467, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.07263479381799698}, {"id": 523, "seek": 243604, "start": 2439.16, "end": 2444.24, "text": " So there's lots of stuff that you can do.", "tokens": [50520, 407, 456, 311, 3195, 295, 1507, 300, 291, 393, 360, 13, 50774], "temperature": 0.0, "avg_logprob": -0.3375802088265467, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.07263479381799698}, {"id": 524, "seek": 243604, "start": 2444.24, "end": 2450.12, "text": " I think the most important tip here is don't wait to be perfect before you start writing.", "tokens": [50774, 286, 519, 264, 881, 1021, 4125, 510, 307, 500, 380, 1699, 281, 312, 2176, 949, 291, 722, 3579, 13, 51068], "temperature": 0.0, "avg_logprob": -0.3375802088265467, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.07263479381799698}, {"id": 525, "seek": 243604, "start": 2450.12, "end": 2454.56, "text": " What was that tip you told me, Rachel?", "tokens": [51068, 708, 390, 300, 4125, 291, 1907, 385, 11, 14246, 30, 51290], "temperature": 0.0, "avg_logprob": -0.3375802088265467, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.07263479381799698}, {"id": 526, "seek": 243604, "start": 2454.56, "end": 2458.2799999999997, "text": " You should think of your target audience as the person who's one step behind you.", "tokens": [51290, 509, 820, 519, 295, 428, 3779, 4034, 382, 264, 954, 567, 311, 472, 1823, 2261, 291, 13, 51476], "temperature": 0.0, "avg_logprob": -0.3375802088265467, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.07263479381799698}, {"id": 527, "seek": 243604, "start": 2458.2799999999997, "end": 2461.84, "text": " So maybe your target audience is someone who's just working through the Part 1 MOOC right", "tokens": [51476, 407, 1310, 428, 3779, 4034, 307, 1580, 567, 311, 445, 1364, 807, 264, 4100, 502, 49197, 34, 558, 51654], "temperature": 0.0, "avg_logprob": -0.3375802088265467, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.07263479381799698}, {"id": 528, "seek": 243604, "start": 2461.84, "end": 2462.84, "text": " now.", "tokens": [51654, 586, 13, 51704], "temperature": 0.0, "avg_logprob": -0.3375802088265467, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.07263479381799698}, {"id": 529, "seek": 246284, "start": 2462.84, "end": 2470.44, "text": " If your target audience is not Jan Lacorne or Jeffrey Hinton, it's you 6 months ago.", "tokens": [50364, 759, 428, 3779, 4034, 307, 406, 4956, 441, 326, 284, 716, 420, 28721, 389, 12442, 11, 309, 311, 291, 1386, 2493, 2057, 13, 50744], "temperature": 0.0, "avg_logprob": -0.551865215032873, "compression_ratio": 1.5823529411764705, "no_speech_prob": 0.2538476586341858}, {"id": 530, "seek": 246284, "start": 2470.44, "end": 2475.0, "text": " Write the thing you would love to have seen because there will be far more people in that", "tokens": [50744, 23499, 264, 551, 291, 576, 959, 281, 362, 1612, 570, 456, 486, 312, 1400, 544, 561, 294, 300, 50972], "temperature": 0.0, "avg_logprob": -0.551865215032873, "compression_ratio": 1.5823529411764705, "no_speech_prob": 0.2538476586341858}, {"id": 531, "seek": 246284, "start": 2475.0, "end": 2478.44, "text": " target audience than in the Jeffrey Hinton target audience.", "tokens": [50972, 3779, 4034, 813, 294, 264, 28721, 389, 12442, 3779, 4034, 13, 51144], "temperature": 0.0, "avg_logprob": -0.551865215032873, "compression_ratio": 1.5823529411764705, "no_speech_prob": 0.2538476586341858}, {"id": 532, "seek": 246284, "start": 2478.44, "end": 2482.52, "text": " How are we going for time, Rachel?", "tokens": [51144, 1012, 366, 321, 516, 337, 565, 11, 14246, 30, 51348], "temperature": 0.0, "avg_logprob": -0.551865215032873, "compression_ratio": 1.5823529411764705, "no_speech_prob": 0.2538476586341858}, {"id": 533, "seek": 248252, "start": 2482.52, "end": 2496.7599999999998, "text": " I've tried to lay out what I think we'll study in Part 2.", "tokens": [50364, 286, 600, 3031, 281, 2360, 484, 437, 286, 519, 321, 603, 2979, 294, 4100, 568, 13, 51076], "temperature": 0.0, "avg_logprob": -0.38870541254679364, "compression_ratio": 1.3065693430656935, "no_speech_prob": 0.02887020818889141}, {"id": 534, "seek": 248252, "start": 2496.7599999999998, "end": 2505.12, "text": " As I say, what I was planning until quite recently to present today was neural translation,", "tokens": [51076, 1018, 286, 584, 11, 437, 286, 390, 5038, 1826, 1596, 3938, 281, 1974, 965, 390, 18161, 12853, 11, 51494], "temperature": 0.0, "avg_logprob": -0.38870541254679364, "compression_ratio": 1.3065693430656935, "no_speech_prob": 0.02887020818889141}, {"id": 535, "seek": 248252, "start": 2505.12, "end": 2506.6, "text": " and then two things happened.", "tokens": [51494, 293, 550, 732, 721, 2011, 13, 51568], "temperature": 0.0, "avg_logprob": -0.38870541254679364, "compression_ratio": 1.3065693430656935, "no_speech_prob": 0.02887020818889141}, {"id": 536, "seek": 250660, "start": 2506.6, "end": 2513.08, "text": " Google suddenly came up with a much better RNN and sequence-to-sequence API.", "tokens": [50364, 3329, 5800, 1361, 493, 365, 257, 709, 1101, 45702, 45, 293, 8310, 12, 1353, 12, 11834, 655, 9362, 13, 50688], "temperature": 0.0, "avg_logprob": -0.3548007541232639, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.21998172998428345}, {"id": 537, "seek": 250660, "start": 2513.08, "end": 2519.58, "text": " And then also two, three weeks ago, a new paper came out for generative models which", "tokens": [50688, 400, 550, 611, 732, 11, 1045, 3259, 2057, 11, 257, 777, 3035, 1361, 484, 337, 1337, 1166, 5245, 597, 51013], "temperature": 0.0, "avg_logprob": -0.3548007541232639, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.21998172998428345}, {"id": 538, "seek": 250660, "start": 2519.58, "end": 2521.24, "text": " totally changed everything.", "tokens": [51013, 3879, 3105, 1203, 13, 51096], "temperature": 0.0, "avg_logprob": -0.3548007541232639, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.21998172998428345}, {"id": 539, "seek": 250660, "start": 2521.24, "end": 2526.44, "text": " So that's why we've redone things and we're starting with CNN generative models today.", "tokens": [51096, 407, 300, 311, 983, 321, 600, 2182, 546, 721, 293, 321, 434, 2891, 365, 24859, 1337, 1166, 5245, 965, 13, 51356], "temperature": 0.0, "avg_logprob": -0.3548007541232639, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.21998172998428345}, {"id": 540, "seek": 250660, "start": 2526.44, "end": 2531.64, "text": " We have a question, where to find the current research papers?", "tokens": [51356, 492, 362, 257, 1168, 11, 689, 281, 915, 264, 2190, 2132, 10577, 30, 51616], "temperature": 0.0, "avg_logprob": -0.3548007541232639, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.21998172998428345}, {"id": 541, "seek": 250660, "start": 2531.64, "end": 2534.6, "text": " Okay, we'll get to that for sure.", "tokens": [51616, 1033, 11, 321, 603, 483, 281, 300, 337, 988, 13, 51764], "temperature": 0.0, "avg_logprob": -0.3548007541232639, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.21998172998428345}, {"id": 542, "seek": 253460, "start": 2534.6, "end": 2537.68, "text": " Let's do that after our break.", "tokens": [50364, 961, 311, 360, 300, 934, 527, 1821, 13, 50518], "temperature": 0.0, "avg_logprob": -0.3845650776323066, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.004982247017323971}, {"id": 543, "seek": 253460, "start": 2537.68, "end": 2548.12, "text": " Assuming that things go as planned, the general topic areas in Part 2 will be CNNs and NLP", "tokens": [50518, 6281, 24919, 300, 721, 352, 382, 8589, 11, 264, 2674, 4829, 3179, 294, 4100, 568, 486, 312, 24859, 82, 293, 426, 45196, 51040], "temperature": 0.0, "avg_logprob": -0.3845650776323066, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.004982247017323971}, {"id": 544, "seek": 253460, "start": 2548.12, "end": 2549.12, "text": " beyond classification.", "tokens": [51040, 4399, 21538, 13, 51090], "temperature": 0.0, "avg_logprob": -0.3845650776323066, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.004982247017323971}, {"id": 545, "seek": 253460, "start": 2549.12, "end": 2555.24, "text": " If you think about it, pretty much everything we did in Part 1 was classification or a little", "tokens": [51090, 759, 291, 519, 466, 309, 11, 1238, 709, 1203, 321, 630, 294, 4100, 502, 390, 21538, 420, 257, 707, 51396], "temperature": 0.0, "avg_logprob": -0.3845650776323066, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.004982247017323971}, {"id": 546, "seek": 253460, "start": 2555.24, "end": 2557.56, "text": " bit of regression.", "tokens": [51396, 857, 295, 24590, 13, 51512], "temperature": 0.0, "avg_logprob": -0.3845650776323066, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.004982247017323971}, {"id": 547, "seek": 253460, "start": 2557.56, "end": 2560.2, "text": " We're going to now be talking more about generative models.", "tokens": [51512, 492, 434, 516, 281, 586, 312, 1417, 544, 466, 1337, 1166, 5245, 13, 51644], "temperature": 0.0, "avg_logprob": -0.3845650776323066, "compression_ratio": 1.5095238095238095, "no_speech_prob": 0.004982247017323971}, {"id": 548, "seek": 256020, "start": 2561.2, "end": 2565.7999999999997, "text": " It's a little hard to exactly define what I mean by generative models, but we're talking", "tokens": [50414, 467, 311, 257, 707, 1152, 281, 2293, 6964, 437, 286, 914, 538, 1337, 1166, 5245, 11, 457, 321, 434, 1417, 50644], "temperature": 0.0, "avg_logprob": -0.3049953574001199, "compression_ratio": 1.7276595744680852, "no_speech_prob": 0.020964015275239944}, {"id": 549, "seek": 256020, "start": 2565.7999999999997, "end": 2571.4399999999996, "text": " about creating an image or creating a sentence.", "tokens": [50644, 466, 4084, 364, 3256, 420, 4084, 257, 8174, 13, 50926], "temperature": 0.0, "avg_logprob": -0.3049953574001199, "compression_ratio": 1.7276595744680852, "no_speech_prob": 0.020964015275239944}, {"id": 550, "seek": 256020, "start": 2571.4399999999996, "end": 2575.3599999999997, "text": " We're creating bigger outputs.", "tokens": [50926, 492, 434, 4084, 3801, 23930, 13, 51122], "temperature": 0.0, "avg_logprob": -0.3049953574001199, "compression_ratio": 1.7276595744680852, "no_speech_prob": 0.020964015275239944}, {"id": 551, "seek": 256020, "start": 2575.3599999999997, "end": 2580.64, "text": " So CNNs beyond classification, generative models for CNNs, means the thing that we could", "tokens": [51122, 407, 24859, 82, 4399, 21538, 11, 1337, 1166, 5245, 337, 24859, 82, 11, 1355, 264, 551, 300, 321, 727, 51386], "temperature": 0.0, "avg_logprob": -0.3049953574001199, "compression_ratio": 1.7276595744680852, "no_speech_prob": 0.020964015275239944}, {"id": 552, "seek": 256020, "start": 2580.64, "end": 2585.96, "text": " produce could be a picture showing this is where the bicycle is, this is where the person", "tokens": [51386, 5258, 727, 312, 257, 3036, 4099, 341, 307, 689, 264, 20888, 307, 11, 341, 307, 689, 264, 954, 51652], "temperature": 0.0, "avg_logprob": -0.3049953574001199, "compression_ratio": 1.7276595744680852, "no_speech_prob": 0.020964015275239944}, {"id": 553, "seek": 256020, "start": 2585.96, "end": 2589.08, "text": " is, this is where the grass is, that's called segmentation.", "tokens": [51652, 307, 11, 341, 307, 689, 264, 8054, 307, 11, 300, 311, 1219, 9469, 399, 13, 51808], "temperature": 0.0, "avg_logprob": -0.3049953574001199, "compression_ratio": 1.7276595744680852, "no_speech_prob": 0.020964015275239944}, {"id": 554, "seek": 258908, "start": 2589.16, "end": 2593.16, "text": " Or it could be taking a black and white image and turning it into a color image, or taking", "tokens": [50368, 1610, 309, 727, 312, 1940, 257, 2211, 293, 2418, 3256, 293, 6246, 309, 666, 257, 2017, 3256, 11, 420, 1940, 50568], "temperature": 0.0, "avg_logprob": -0.24993234310510024, "compression_ratio": 2.289473684210526, "no_speech_prob": 0.003538056742399931}, {"id": 555, "seek": 258908, "start": 2593.16, "end": 2597.04, "text": " a low-res image and turning it into a high-res image, or taking a photo and turning it into", "tokens": [50568, 257, 2295, 12, 495, 3256, 293, 6246, 309, 666, 257, 1090, 12, 495, 3256, 11, 420, 1940, 257, 5052, 293, 6246, 309, 666, 50762], "temperature": 0.0, "avg_logprob": -0.24993234310510024, "compression_ratio": 2.289473684210526, "no_speech_prob": 0.003538056742399931}, {"id": 556, "seek": 258908, "start": 2597.04, "end": 2604.3199999999997, "text": " a Van Gogh, or taking a photo and turning it into a sentence describing it.", "tokens": [50762, 257, 8979, 39690, 71, 11, 420, 1940, 257, 5052, 293, 6246, 309, 666, 257, 8174, 16141, 309, 13, 51126], "temperature": 0.0, "avg_logprob": -0.24993234310510024, "compression_ratio": 2.289473684210526, "no_speech_prob": 0.003538056742399931}, {"id": 557, "seek": 258908, "start": 2604.3199999999997, "end": 2611.92, "text": " NLP beyond classification can be taking an English sentence and turning it into French,", "tokens": [51126, 426, 45196, 4399, 21538, 393, 312, 1940, 364, 3669, 8174, 293, 6246, 309, 666, 5522, 11, 51506], "temperature": 0.0, "avg_logprob": -0.24993234310510024, "compression_ratio": 2.289473684210526, "no_speech_prob": 0.003538056742399931}, {"id": 558, "seek": 258908, "start": 2611.92, "end": 2618.0, "text": " or taking an English story and a question and turning it into an answer of that question", "tokens": [51506, 420, 1940, 364, 3669, 1657, 293, 257, 1168, 293, 6246, 309, 666, 364, 1867, 295, 300, 1168, 51810], "temperature": 0.0, "avg_logprob": -0.24993234310510024, "compression_ratio": 2.289473684210526, "no_speech_prob": 0.003538056742399931}, {"id": 559, "seek": 261800, "start": 2618.0, "end": 2619.0, "text": " about that story.", "tokens": [50364, 466, 300, 1657, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3235330581665039, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.028870321810245514}, {"id": 560, "seek": 261800, "start": 2619.0, "end": 2622.92, "text": " That's chatbots in Q&A.", "tokens": [50414, 663, 311, 5081, 65, 1971, 294, 1249, 5, 32, 13, 50610], "temperature": 0.0, "avg_logprob": -0.3235330581665039, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.028870321810245514}, {"id": 561, "seek": 261800, "start": 2622.92, "end": 2627.08, "text": " We'll be talking about how to deal with larger datasets, so that both means datasets with", "tokens": [50610, 492, 603, 312, 1417, 466, 577, 281, 2028, 365, 4833, 42856, 11, 370, 300, 1293, 1355, 42856, 365, 50818], "temperature": 0.0, "avg_logprob": -0.3235330581665039, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.028870321810245514}, {"id": 562, "seek": 261800, "start": 2627.08, "end": 2632.16, "text": " more things in it and datasets where the things are bigger.", "tokens": [50818, 544, 721, 294, 309, 293, 42856, 689, 264, 721, 366, 3801, 13, 51072], "temperature": 0.0, "avg_logprob": -0.3235330581665039, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.028870321810245514}, {"id": 563, "seek": 261800, "start": 2632.16, "end": 2636.6, "text": " And then finally, something I'm pretty excited about is I've done a lot of work recently", "tokens": [51072, 400, 550, 2721, 11, 746, 286, 478, 1238, 2919, 466, 307, 286, 600, 1096, 257, 688, 295, 589, 3938, 51294], "temperature": 0.0, "avg_logprob": -0.3235330581665039, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.028870321810245514}, {"id": 564, "seek": 261800, "start": 2636.6, "end": 2641.36, "text": " finding some interesting stuff about using deep learning for structured data and for", "tokens": [51294, 5006, 512, 1880, 1507, 466, 1228, 2452, 2539, 337, 18519, 1412, 293, 337, 51532], "temperature": 0.0, "avg_logprob": -0.3235330581665039, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.028870321810245514}, {"id": 565, "seek": 261800, "start": 2641.36, "end": 2642.36, "text": " time series.", "tokens": [51532, 565, 2638, 13, 51582], "temperature": 0.0, "avg_logprob": -0.3235330581665039, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.028870321810245514}, {"id": 566, "seek": 261800, "start": 2642.36, "end": 2645.56, "text": " So for example, we heard about fraud.", "tokens": [51582, 407, 337, 1365, 11, 321, 2198, 466, 14560, 13, 51742], "temperature": 0.0, "avg_logprob": -0.3235330581665039, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.028870321810245514}, {"id": 567, "seek": 261800, "start": 2645.56, "end": 2647.12, "text": " So fraud is both of those things.", "tokens": [51742, 407, 14560, 307, 1293, 295, 729, 721, 13, 51820], "temperature": 0.0, "avg_logprob": -0.3235330581665039, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.028870321810245514}, {"id": 568, "seek": 264712, "start": 2647.24, "end": 2652.68, "text": " It combines time series, transaction histories, and click histories, and structured data,", "tokens": [50370, 467, 29520, 565, 2638, 11, 14425, 30631, 11, 293, 2052, 30631, 11, 293, 18519, 1412, 11, 50642], "temperature": 0.0, "avg_logprob": -0.3335028047914858, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0019267165334895253}, {"id": 569, "seek": 264712, "start": 2652.68, "end": 2653.68, "text": " customer information.", "tokens": [50642, 5474, 1589, 13, 50692], "temperature": 0.0, "avg_logprob": -0.3335028047914858, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0019267165334895253}, {"id": 570, "seek": 264712, "start": 2653.68, "end": 2659.16, "text": " Traditionally, that's not been tackled with deep learning, but I've actually found some", "tokens": [50692, 22017, 15899, 11, 300, 311, 406, 668, 9426, 1493, 365, 2452, 2539, 11, 457, 286, 600, 767, 1352, 512, 50966], "temperature": 0.0, "avg_logprob": -0.3335028047914858, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0019267165334895253}, {"id": 571, "seek": 264712, "start": 2659.16, "end": 2664.7999999999997, "text": " state-of-the-art, world-class approaches to solving those with deep learning.", "tokens": [50966, 1785, 12, 2670, 12, 3322, 12, 446, 11, 1002, 12, 11665, 11587, 281, 12606, 729, 365, 2452, 2539, 13, 51248], "temperature": 0.0, "avg_logprob": -0.3335028047914858, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0019267165334895253}, {"id": 572, "seek": 264712, "start": 2664.7999999999997, "end": 2669.24, "text": " So I'm really looking forward to sharing that with you.", "tokens": [51248, 407, 286, 478, 534, 1237, 2128, 281, 5414, 300, 365, 291, 13, 51470], "temperature": 0.0, "avg_logprob": -0.3335028047914858, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0019267165334895253}, {"id": 573, "seek": 266924, "start": 2669.24, "end": 2677.3599999999997, "text": " So let's take a 8-minute break, come back at 5-8.", "tokens": [50364, 407, 718, 311, 747, 257, 1649, 12, 18256, 1821, 11, 808, 646, 412, 1025, 12, 23, 13, 50770], "temperature": 0.0, "avg_logprob": -0.32990257461349687, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.02228560484945774}, {"id": 574, "seek": 266924, "start": 2677.3599999999997, "end": 2682.52, "text": " Thanks very much.", "tokens": [50770, 2561, 588, 709, 13, 51028], "temperature": 0.0, "avg_logprob": -0.32990257461349687, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.02228560484945774}, {"id": 575, "seek": 266924, "start": 2682.52, "end": 2687.4399999999996, "text": " So we're going to learn about this idea of artistic style, or neural style transfer.", "tokens": [51028, 407, 321, 434, 516, 281, 1466, 466, 341, 1558, 295, 17090, 3758, 11, 420, 18161, 3758, 5003, 13, 51274], "temperature": 0.0, "avg_logprob": -0.32990257461349687, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.02228560484945774}, {"id": 576, "seek": 266924, "start": 2687.4399999999996, "end": 2693.64, "text": " The idea is that we're going to take a photo and make it look like it was painted in the", "tokens": [51274, 440, 1558, 307, 300, 321, 434, 516, 281, 747, 257, 5052, 293, 652, 309, 574, 411, 309, 390, 11797, 294, 264, 51584], "temperature": 0.0, "avg_logprob": -0.32990257461349687, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.02228560484945774}, {"id": 577, "seek": 266924, "start": 2693.64, "end": 2695.2, "text": " style of some painter.", "tokens": [51584, 3758, 295, 512, 26619, 13, 51662], "temperature": 0.0, "avg_logprob": -0.32990257461349687, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.02228560484945774}, {"id": 578, "seek": 269520, "start": 2695.2, "end": 2722.6, "text": " So our inputs are a photo and style.", "tokens": [50364, 407, 527, 15743, 366, 257, 5052, 293, 3758, 13, 51734], "temperature": 0.0, "avg_logprob": -0.5975442299476037, "compression_ratio": 0.8181818181818182, "no_speech_prob": 0.25380048155784607}, {"id": 579, "seek": 272260, "start": 2722.6, "end": 2735.2799999999997, "text": " And so these two things are going to be combined together to create an image which is going", "tokens": [50364, 400, 370, 613, 732, 721, 366, 516, 281, 312, 9354, 1214, 281, 1884, 364, 3256, 597, 307, 516, 50998], "temperature": 0.0, "avg_logprob": -0.34620501778342505, "compression_ratio": 1.1973684210526316, "no_speech_prob": 0.46097394824028015}, {"id": 580, "seek": 273528, "start": 2735.28, "end": 2757.0400000000004, "text": " to hopefully have the content of the photo and the style of the image.", "tokens": [50364, 281, 4696, 362, 264, 2701, 295, 264, 5052, 293, 264, 3758, 295, 264, 3256, 13, 51452], "temperature": 0.0, "avg_logprob": -0.3814006855613307, "compression_ratio": 1.09375, "no_speech_prob": 0.013020152226090431}, {"id": 581, "seek": 275704, "start": 2757.04, "end": 2766.04, "text": " The way we're going to do this is we're going to assume that there is some function where", "tokens": [50364, 440, 636, 321, 434, 516, 281, 360, 341, 307, 321, 434, 516, 281, 6552, 300, 456, 307, 512, 2445, 689, 50814], "temperature": 0.0, "avg_logprob": -0.237990889438363, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.015906034037470818}, {"id": 582, "seek": 275704, "start": 2766.04, "end": 2783.64, "text": " the inputs to this function are the photo, the style image, and some generated image", "tokens": [50814, 264, 15743, 281, 341, 2445, 366, 264, 5052, 11, 264, 3758, 3256, 11, 293, 512, 10833, 3256, 51694], "temperature": 0.0, "avg_logprob": -0.237990889438363, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.015906034037470818}, {"id": 583, "seek": 278364, "start": 2783.64, "end": 2787.8799999999997, "text": " that I've created.", "tokens": [50364, 300, 286, 600, 2942, 13, 50576], "temperature": 0.0, "avg_logprob": -0.2897290587425232, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.16237306594848633}, {"id": 584, "seek": 278364, "start": 2787.8799999999997, "end": 2798.92, "text": " And that will return some number where this function will be higher if the generated image", "tokens": [50576, 400, 300, 486, 2736, 512, 1230, 689, 341, 2445, 486, 312, 2946, 498, 264, 10833, 3256, 51128], "temperature": 0.0, "avg_logprob": -0.2897290587425232, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.16237306594848633}, {"id": 585, "seek": 278364, "start": 2798.92, "end": 2805.3199999999997, "text": " really looks like this photo in this style, and lower if it doesn't.", "tokens": [51128, 534, 1542, 411, 341, 5052, 294, 341, 3758, 11, 293, 3126, 498, 309, 1177, 380, 13, 51448], "temperature": 0.0, "avg_logprob": -0.2897290587425232, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.16237306594848633}, {"id": 586, "seek": 278364, "start": 2805.3199999999997, "end": 2811.7599999999998, "text": " So if we can create this loss function that basically says here's my generated image,", "tokens": [51448, 407, 498, 321, 393, 1884, 341, 4470, 2445, 300, 1936, 1619, 510, 311, 452, 10833, 3256, 11, 51770], "temperature": 0.0, "avg_logprob": -0.2897290587425232, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.16237306594848633}, {"id": 587, "seek": 281176, "start": 2811.76, "end": 2816.2400000000002, "text": " and it returns back a number saying, oh yes, that generated image does look like that photo", "tokens": [50364, 293, 309, 11247, 646, 257, 1230, 1566, 11, 1954, 2086, 11, 300, 10833, 3256, 775, 574, 411, 300, 5052, 50588], "temperature": 0.0, "avg_logprob": -0.2783927698244994, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.02517867274582386}, {"id": 588, "seek": 281176, "start": 2816.2400000000002, "end": 2820.28, "text": " in that style, then we could use SGD.", "tokens": [50588, 294, 300, 3758, 11, 550, 321, 727, 764, 34520, 35, 13, 50790], "temperature": 0.0, "avg_logprob": -0.2783927698244994, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.02517867274582386}, {"id": 589, "seek": 281176, "start": 2820.28, "end": 2830.44, "text": " We would use SGD not to optimize the weights of a network, we would use SGD to optimize", "tokens": [50790, 492, 576, 764, 34520, 35, 406, 281, 19719, 264, 17443, 295, 257, 3209, 11, 321, 576, 764, 34520, 35, 281, 19719, 51298], "temperature": 0.0, "avg_logprob": -0.2783927698244994, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.02517867274582386}, {"id": 590, "seek": 281176, "start": 2830.44, "end": 2833.6800000000003, "text": " the pixel values of the generated image.", "tokens": [51298, 264, 19261, 4190, 295, 264, 10833, 3256, 13, 51460], "temperature": 0.0, "avg_logprob": -0.2783927698244994, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.02517867274582386}, {"id": 591, "seek": 281176, "start": 2833.6800000000003, "end": 2841.28, "text": " So we would be using it to try to optimize the value of this argument.", "tokens": [51460, 407, 321, 576, 312, 1228, 309, 281, 853, 281, 19719, 264, 2158, 295, 341, 6770, 13, 51840], "temperature": 0.0, "avg_logprob": -0.2783927698244994, "compression_ratio": 1.7688172043010753, "no_speech_prob": 0.02517867274582386}, {"id": 592, "seek": 284128, "start": 2841.8, "end": 2847.5600000000004, "text": " We haven't quite done that before, but conceptually it's identical.", "tokens": [50390, 492, 2378, 380, 1596, 1096, 300, 949, 11, 457, 3410, 671, 309, 311, 14800, 13, 50678], "temperature": 0.0, "avg_logprob": -0.24291305082390108, "compression_ratio": 1.6313131313131313, "no_speech_prob": 8.092755888355896e-05}, {"id": 593, "seek": 284128, "start": 2847.5600000000004, "end": 2857.1200000000003, "text": " Conceptually we can just find the derivative of this function with respect to this input.", "tokens": [50678, 47482, 671, 321, 393, 445, 915, 264, 13760, 295, 341, 2445, 365, 3104, 281, 341, 4846, 13, 51156], "temperature": 0.0, "avg_logprob": -0.24291305082390108, "compression_ratio": 1.6313131313131313, "no_speech_prob": 8.092755888355896e-05}, {"id": 594, "seek": 284128, "start": 2857.1200000000003, "end": 2861.52, "text": " And then we can try to optimize that input, which is just a set of pixel values, to try", "tokens": [51156, 400, 550, 321, 393, 853, 281, 19719, 300, 4846, 11, 597, 307, 445, 257, 992, 295, 19261, 4190, 11, 281, 853, 51376], "temperature": 0.0, "avg_logprob": -0.24291305082390108, "compression_ratio": 1.6313131313131313, "no_speech_prob": 8.092755888355896e-05}, {"id": 595, "seek": 284128, "start": 2861.52, "end": 2864.78, "text": " and maximize the function.", "tokens": [51376, 293, 19874, 264, 2445, 13, 51539], "temperature": 0.0, "avg_logprob": -0.24291305082390108, "compression_ratio": 1.6313131313131313, "no_speech_prob": 8.092755888355896e-05}, {"id": 596, "seek": 284128, "start": 2864.78, "end": 2869.84, "text": " So all we need to do is come up with the function.", "tokens": [51539, 407, 439, 321, 643, 281, 360, 307, 808, 493, 365, 264, 2445, 13, 51792], "temperature": 0.0, "avg_logprob": -0.24291305082390108, "compression_ratio": 1.6313131313131313, "no_speech_prob": 8.092755888355896e-05}, {"id": 597, "seek": 286984, "start": 2869.84, "end": 2876.52, "text": " Come up with a function which will tell us how much does some generated image look like", "tokens": [50364, 2492, 493, 365, 257, 2445, 597, 486, 980, 505, 577, 709, 775, 512, 10833, 3256, 574, 411, 50698], "temperature": 0.0, "avg_logprob": -0.2673044870066088, "compression_ratio": 1.7005347593582887, "no_speech_prob": 0.006488236598670483}, {"id": 598, "seek": 286984, "start": 2876.52, "end": 2880.48, "text": " this photo in this style.", "tokens": [50698, 341, 5052, 294, 341, 3758, 13, 50896], "temperature": 0.0, "avg_logprob": -0.2673044870066088, "compression_ratio": 1.7005347593582887, "no_speech_prob": 0.006488236598670483}, {"id": 599, "seek": 286984, "start": 2880.48, "end": 2883.48, "text": " And the way we're going to do that, step 1, is going to be very simple.", "tokens": [50896, 400, 264, 636, 321, 434, 516, 281, 360, 300, 11, 1823, 502, 11, 307, 516, 281, 312, 588, 2199, 13, 51046], "temperature": 0.0, "avg_logprob": -0.2673044870066088, "compression_ratio": 1.7005347593582887, "no_speech_prob": 0.006488236598670483}, {"id": 600, "seek": 286984, "start": 2883.48, "end": 2887.7200000000003, "text": " We're going to turn it into 2 functions.", "tokens": [51046, 492, 434, 516, 281, 1261, 309, 666, 568, 6828, 13, 51258], "temperature": 0.0, "avg_logprob": -0.2673044870066088, "compression_ratio": 1.7005347593582887, "no_speech_prob": 0.006488236598670483}, {"id": 601, "seek": 286984, "start": 2887.7200000000003, "end": 2897.2400000000002, "text": " fcontent, which will take the photo and the generated image, and that will tell us a bigger", "tokens": [51258, 283, 9000, 317, 11, 597, 486, 747, 264, 5052, 293, 264, 10833, 3256, 11, 293, 300, 486, 980, 505, 257, 3801, 51734], "temperature": 0.0, "avg_logprob": -0.2673044870066088, "compression_ratio": 1.7005347593582887, "no_speech_prob": 0.006488236598670483}, {"id": 602, "seek": 289724, "start": 2897.2799999999997, "end": 2904.68, "text": " number if the generated image looks more like the photo, if the content looks the same.", "tokens": [50366, 1230, 498, 264, 10833, 3256, 1542, 544, 411, 264, 5052, 11, 498, 264, 2701, 1542, 264, 912, 13, 50736], "temperature": 0.0, "avg_logprob": -0.3086125234539589, "compression_ratio": 1.83, "no_speech_prob": 0.015906190499663353}, {"id": 603, "seek": 289724, "start": 2904.68, "end": 2913.1, "text": " And then there'll be a second function, which takes the style image and the generated image,", "tokens": [50736, 400, 550, 456, 603, 312, 257, 1150, 2445, 11, 597, 2516, 264, 3758, 3256, 293, 264, 10833, 3256, 11, 51157], "temperature": 0.0, "avg_logprob": -0.3086125234539589, "compression_ratio": 1.83, "no_speech_prob": 0.015906190499663353}, {"id": 604, "seek": 289724, "start": 2913.1, "end": 2920.08, "text": " and that will tell us a higher number if this generated image looks like it was painted", "tokens": [51157, 293, 300, 486, 980, 505, 257, 2946, 1230, 498, 341, 10833, 3256, 1542, 411, 309, 390, 11797, 51506], "temperature": 0.0, "avg_logprob": -0.3086125234539589, "compression_ratio": 1.83, "no_speech_prob": 0.015906190499663353}, {"id": 605, "seek": 289724, "start": 2920.08, "end": 2922.3999999999996, "text": " in the same style as the style image.", "tokens": [51506, 294, 264, 912, 3758, 382, 264, 3758, 3256, 13, 51622], "temperature": 0.0, "avg_logprob": -0.3086125234539589, "compression_ratio": 1.83, "no_speech_prob": 0.015906190499663353}, {"id": 606, "seek": 289724, "start": 2922.3999999999996, "end": 2926.64, "text": " So we can just turn it into 2 pieces and add them together.", "tokens": [51622, 407, 321, 393, 445, 1261, 309, 666, 568, 3755, 293, 909, 552, 1214, 13, 51834], "temperature": 0.0, "avg_logprob": -0.3086125234539589, "compression_ratio": 1.83, "no_speech_prob": 0.015906190499663353}, {"id": 607, "seek": 292664, "start": 2927.04, "end": 2933.64, "text": " So now we need to come up with these 2 parts.", "tokens": [50384, 407, 586, 321, 643, 281, 808, 493, 365, 613, 568, 3166, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3609882082257952, "compression_ratio": 1.3918918918918919, "no_speech_prob": 0.002981024794280529}, {"id": 608, "seek": 292664, "start": 2933.64, "end": 2940.56, "text": " Now the first part is very easy.", "tokens": [50714, 823, 264, 700, 644, 307, 588, 1858, 13, 51060], "temperature": 0.0, "avg_logprob": -0.3609882082257952, "compression_ratio": 1.3918918918918919, "no_speech_prob": 0.002981024794280529}, {"id": 609, "seek": 292664, "start": 2940.56, "end": 2949.68, "text": " What's a way that we could create a function that returns a higher number if the generated", "tokens": [51060, 708, 311, 257, 636, 300, 321, 727, 1884, 257, 2445, 300, 11247, 257, 2946, 1230, 498, 264, 10833, 51516], "temperature": 0.0, "avg_logprob": -0.3609882082257952, "compression_ratio": 1.3918918918918919, "no_speech_prob": 0.002981024794280529}, {"id": 610, "seek": 292664, "start": 2949.68, "end": 2952.72, "text": " image is more similar to some photo?", "tokens": [51516, 3256, 307, 544, 2531, 281, 512, 5052, 30, 51668], "temperature": 0.0, "avg_logprob": -0.3609882082257952, "compression_ratio": 1.3918918918918919, "no_speech_prob": 0.002981024794280529}, {"id": 611, "seek": 295272, "start": 2953.72, "end": 2964.3999999999996, "text": " When you come up with a loss function, the really obvious one is the values of the pixels.", "tokens": [50414, 1133, 291, 808, 493, 365, 257, 4470, 2445, 11, 264, 534, 6322, 472, 307, 264, 4190, 295, 264, 18668, 13, 50948], "temperature": 0.0, "avg_logprob": -0.33597858368404326, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0036499821580946445}, {"id": 612, "seek": 295272, "start": 2964.3999999999996, "end": 2970.2799999999997, "text": " The values of the pixels in the generated image, the mean squared error between them", "tokens": [50948, 440, 4190, 295, 264, 18668, 294, 264, 10833, 3256, 11, 264, 914, 8889, 6713, 1296, 552, 51242], "temperature": 0.0, "avg_logprob": -0.33597858368404326, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0036499821580946445}, {"id": 613, "seek": 295272, "start": 2970.2799999999997, "end": 2979.7999999999997, "text": " and the photo, that mean squared error loss function would be one way of doing this part.", "tokens": [51242, 293, 264, 5052, 11, 300, 914, 8889, 6713, 4470, 2445, 576, 312, 472, 636, 295, 884, 341, 644, 13, 51718], "temperature": 0.0, "avg_logprob": -0.33597858368404326, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0036499821580946445}, {"id": 614, "seek": 297980, "start": 2979.88, "end": 2987.6800000000003, "text": " The problem with that though is that as I start to turn it into a van Gogh, those pixel", "tokens": [50368, 440, 1154, 365, 300, 1673, 307, 300, 382, 286, 722, 281, 1261, 309, 666, 257, 3161, 39690, 71, 11, 729, 19261, 50758], "temperature": 0.0, "avg_logprob": -0.30263653668490326, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0001195985241793096}, {"id": 615, "seek": 297980, "start": 2987.6800000000003, "end": 2988.6800000000003, "text": " values are going to change.", "tokens": [50758, 4190, 366, 516, 281, 1319, 13, 50808], "temperature": 0.0, "avg_logprob": -0.30263653668490326, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0001195985241793096}, {"id": 616, "seek": 297980, "start": 2988.6800000000003, "end": 2992.4, "text": " They're going to change color because the van Gogh might have been a very blue-looking", "tokens": [50808, 814, 434, 516, 281, 1319, 2017, 570, 264, 3161, 39690, 71, 1062, 362, 668, 257, 588, 3344, 12, 16129, 50994], "temperature": 0.0, "avg_logprob": -0.30263653668490326, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0001195985241793096}, {"id": 617, "seek": 297980, "start": 2992.4, "end": 2993.4, "text": " van Gogh.", "tokens": [50994, 3161, 39690, 71, 13, 51044], "temperature": 0.0, "avg_logprob": -0.30263653668490326, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0001195985241793096}, {"id": 618, "seek": 297980, "start": 2993.4, "end": 2999.0, "text": " They'll change the relationships to each other, so it might become a curve or it used to be", "tokens": [51044, 814, 603, 1319, 264, 6159, 281, 1184, 661, 11, 370, 309, 1062, 1813, 257, 7605, 420, 309, 1143, 281, 312, 51324], "temperature": 0.0, "avg_logprob": -0.30263653668490326, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0001195985241793096}, {"id": 619, "seek": 297980, "start": 2999.0, "end": 3001.0, "text": " a straight line.", "tokens": [51324, 257, 2997, 1622, 13, 51424], "temperature": 0.0, "avg_logprob": -0.30263653668490326, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0001195985241793096}, {"id": 620, "seek": 300100, "start": 3001.0, "end": 3009.88, "text": " So really, the pixel-wise mean squared error is not going to give us much freedom in trying", "tokens": [50364, 407, 534, 11, 264, 19261, 12, 3711, 914, 8889, 6713, 307, 406, 516, 281, 976, 505, 709, 5645, 294, 1382, 50808], "temperature": 0.0, "avg_logprob": -0.359615774715648, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.011868965812027454}, {"id": 621, "seek": 300100, "start": 3009.88, "end": 3013.56, "text": " to create something that still looks like a photo.", "tokens": [50808, 281, 1884, 746, 300, 920, 1542, 411, 257, 5052, 13, 50992], "temperature": 0.0, "avg_logprob": -0.359615774715648, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.011868965812027454}, {"id": 622, "seek": 300100, "start": 3013.56, "end": 3015.52, "text": " So here's an idea.", "tokens": [50992, 407, 510, 311, 364, 1558, 13, 51090], "temperature": 0.0, "avg_logprob": -0.359615774715648, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.011868965812027454}, {"id": 623, "seek": 300100, "start": 3015.52, "end": 3029.68, "text": " Instead, let's take those pixels and stick them through a pre-trained CNN, like VGG,", "tokens": [51090, 7156, 11, 718, 311, 747, 729, 18668, 293, 2897, 552, 807, 257, 659, 12, 17227, 2001, 24859, 11, 411, 691, 27561, 11, 51798], "temperature": 0.0, "avg_logprob": -0.359615774715648, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.011868965812027454}, {"id": 624, "seek": 302968, "start": 3029.68, "end": 3036.0, "text": " and let's look at the 4th or 5th or 8th convolutional layers activations.", "tokens": [50364, 293, 718, 311, 574, 412, 264, 1017, 392, 420, 1025, 392, 420, 1649, 392, 45216, 304, 7914, 2430, 763, 13, 50680], "temperature": 0.0, "avg_logprob": -0.33101828304337866, "compression_ratio": 1.699421965317919, "no_speech_prob": 0.001098728273063898}, {"id": 625, "seek": 302968, "start": 3036.0, "end": 3045.3599999999997, "text": " Remember back to those Matt Zyler visualizations where we saw that the later layers said, how", "tokens": [50680, 5459, 646, 281, 729, 7397, 1176, 88, 1918, 5056, 14455, 689, 321, 1866, 300, 264, 1780, 7914, 848, 11, 577, 51148], "temperature": 0.0, "avg_logprob": -0.33101828304337866, "compression_ratio": 1.699421965317919, "no_speech_prob": 0.001098728273063898}, {"id": 626, "seek": 302968, "start": 3045.3599999999997, "end": 3051.08, "text": " much does an eyeball look like here, or how much does this look like a star, or how much", "tokens": [51148, 709, 775, 364, 38868, 574, 411, 510, 11, 420, 577, 709, 775, 341, 574, 411, 257, 3543, 11, 420, 577, 709, 51434], "temperature": 0.0, "avg_logprob": -0.33101828304337866, "compression_ratio": 1.699421965317919, "no_speech_prob": 0.001098728273063898}, {"id": 627, "seek": 302968, "start": 3051.08, "end": 3053.9199999999996, "text": " does this look like the fur of a dog.", "tokens": [51434, 775, 341, 574, 411, 264, 2687, 295, 257, 3000, 13, 51576], "temperature": 0.0, "avg_logprob": -0.33101828304337866, "compression_ratio": 1.699421965317919, "no_speech_prob": 0.001098728273063898}, {"id": 628, "seek": 305392, "start": 3053.92, "end": 3060.64, "text": " The later layers were dealing with bigger objects and more semantic concepts.", "tokens": [50364, 440, 1780, 7914, 645, 6260, 365, 3801, 6565, 293, 544, 47982, 10392, 13, 50700], "temperature": 0.0, "avg_logprob": -0.27950829046743886, "compression_ratio": 1.7617021276595746, "no_speech_prob": 0.05920952558517456}, {"id": 629, "seek": 305392, "start": 3060.64, "end": 3067.56, "text": " So if we were to use a later layer's activations as our loss function, then we could really", "tokens": [50700, 407, 498, 321, 645, 281, 764, 257, 1780, 4583, 311, 2430, 763, 382, 527, 4470, 2445, 11, 550, 321, 727, 534, 51046], "temperature": 0.0, "avg_logprob": -0.27950829046743886, "compression_ratio": 1.7617021276595746, "no_speech_prob": 0.05920952558517456}, {"id": 630, "seek": 305392, "start": 3067.56, "end": 3072.4, "text": " change the style and the color and all kinds of stuff and really would be saying, does", "tokens": [51046, 1319, 264, 3758, 293, 264, 2017, 293, 439, 3685, 295, 1507, 293, 534, 576, 312, 1566, 11, 775, 51288], "temperature": 0.0, "avg_logprob": -0.27950829046743886, "compression_ratio": 1.7617021276595746, "no_speech_prob": 0.05920952558517456}, {"id": 631, "seek": 305392, "start": 3072.4, "end": 3074.92, "text": " the eye still look like an eye?", "tokens": [51288, 264, 3313, 920, 574, 411, 364, 3313, 30, 51414], "temperature": 0.0, "avg_logprob": -0.27950829046743886, "compression_ratio": 1.7617021276595746, "no_speech_prob": 0.05920952558517456}, {"id": 632, "seek": 305392, "start": 3074.92, "end": 3077.28, "text": " Does the beak still look like a beak?", "tokens": [51414, 4402, 264, 48663, 920, 574, 411, 257, 48663, 30, 51532], "temperature": 0.0, "avg_logprob": -0.27950829046743886, "compression_ratio": 1.7617021276595746, "no_speech_prob": 0.05920952558517456}, {"id": 633, "seek": 305392, "start": 3077.28, "end": 3079.44, "text": " Does the rock still look like a rock?", "tokens": [51532, 4402, 264, 3727, 920, 574, 411, 257, 3727, 30, 51640], "temperature": 0.0, "avg_logprob": -0.27950829046743886, "compression_ratio": 1.7617021276595746, "no_speech_prob": 0.05920952558517456}, {"id": 634, "seek": 305392, "start": 3079.44, "end": 3083.48, "text": " And if the answer is yes, then okay, that's good.", "tokens": [51640, 400, 498, 264, 1867, 307, 2086, 11, 550, 1392, 11, 300, 311, 665, 13, 51842], "temperature": 0.0, "avg_logprob": -0.27950829046743886, "compression_ratio": 1.7617021276595746, "no_speech_prob": 0.05920952558517456}, {"id": 635, "seek": 308348, "start": 3084.04, "end": 3088.16, "text": " This is something that matches in terms of the meaning of the content, even though the", "tokens": [50392, 639, 307, 746, 300, 10676, 294, 2115, 295, 264, 3620, 295, 264, 2701, 11, 754, 1673, 264, 50598], "temperature": 0.0, "avg_logprob": -0.2980471002050193, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.008187868632376194}, {"id": 636, "seek": 308348, "start": 3088.16, "end": 3090.44, "text": " pixels look very different.", "tokens": [50598, 18668, 574, 588, 819, 13, 50712], "temperature": 0.0, "avg_logprob": -0.2980471002050193, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.008187868632376194}, {"id": 637, "seek": 308348, "start": 3090.44, "end": 3092.32, "text": " And so that's exactly what we're going to do.", "tokens": [50712, 400, 370, 300, 311, 2293, 437, 321, 434, 516, 281, 360, 13, 50806], "temperature": 0.0, "avg_logprob": -0.2980471002050193, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.008187868632376194}, {"id": 638, "seek": 308348, "start": 3092.32, "end": 3105.0, "text": " So for fcontent, we're going to say that's just the VGG activations of some convolutional", "tokens": [50806, 407, 337, 283, 9000, 317, 11, 321, 434, 516, 281, 584, 300, 311, 445, 264, 691, 27561, 2430, 763, 295, 512, 45216, 304, 51440], "temperature": 0.0, "avg_logprob": -0.2980471002050193, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.008187868632376194}, {"id": 639, "seek": 308348, "start": 3105.0, "end": 3106.32, "text": " layer.", "tokens": [51440, 4583, 13, 51506], "temperature": 0.0, "avg_logprob": -0.2980471002050193, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.008187868632376194}, {"id": 640, "seek": 308348, "start": 3106.32, "end": 3107.32, "text": " Which one?", "tokens": [51506, 3013, 472, 30, 51556], "temperature": 0.0, "avg_logprob": -0.2980471002050193, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.008187868632376194}, {"id": 641, "seek": 308348, "start": 3107.32, "end": 3109.96, "text": " We can try some.", "tokens": [51556, 492, 393, 853, 512, 13, 51688], "temperature": 0.0, "avg_logprob": -0.2980471002050193, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.008187868632376194}, {"id": 642, "seek": 310996, "start": 3109.96, "end": 3114.04, "text": " So that's actually enough for us to get started.", "tokens": [50364, 407, 300, 311, 767, 1547, 337, 505, 281, 483, 1409, 13, 50568], "temperature": 0.0, "avg_logprob": -0.288288249525913, "compression_ratio": 1.2945736434108528, "no_speech_prob": 0.0015731104649603367}, {"id": 643, "seek": 310996, "start": 3114.04, "end": 3123.32, "text": " Let's try and build something that optimizes pixels using a loss function of the VGG network", "tokens": [50568, 961, 311, 853, 293, 1322, 746, 300, 5028, 5660, 18668, 1228, 257, 4470, 2445, 295, 264, 691, 27561, 3209, 51032], "temperature": 0.0, "avg_logprob": -0.288288249525913, "compression_ratio": 1.2945736434108528, "no_speech_prob": 0.0015731104649603367}, {"id": 644, "seek": 310996, "start": 3123.32, "end": 3132.0, "text": " some convolutional layer.", "tokens": [51032, 512, 45216, 304, 4583, 13, 51466], "temperature": 0.0, "avg_logprob": -0.288288249525913, "compression_ratio": 1.2945736434108528, "no_speech_prob": 0.0015731104649603367}, {"id": 645, "seek": 313200, "start": 3132.0, "end": 3138.48, "text": " This is the neural style notebook.", "tokens": [50364, 639, 307, 264, 18161, 3758, 21060, 13, 50688], "temperature": 0.0, "avg_logprob": -0.36442847000925166, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.02442280389368534}, {"id": 646, "seek": 313200, "start": 3138.48, "end": 3144.52, "text": " And much of what we're going to look at is going to look very similar.", "tokens": [50688, 400, 709, 295, 437, 321, 434, 516, 281, 574, 412, 307, 516, 281, 574, 588, 2531, 13, 50990], "temperature": 0.0, "avg_logprob": -0.36442847000925166, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.02442280389368534}, {"id": 647, "seek": 313200, "start": 3144.52, "end": 3149.28, "text": " The first thing you'll see which doesn't look similar to before is I've got this thing called", "tokens": [50990, 440, 700, 551, 291, 603, 536, 597, 1177, 380, 574, 2531, 281, 949, 307, 286, 600, 658, 341, 551, 1219, 51228], "temperature": 0.0, "avg_logprob": -0.36442847000925166, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.02442280389368534}, {"id": 648, "seek": 313200, "start": 3149.28, "end": 3151.28, "text": " limitmem.", "tokens": [51228, 4948, 17886, 13, 51328], "temperature": 0.0, "avg_logprob": -0.36442847000925166, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.02442280389368534}, {"id": 649, "seek": 315128, "start": 3151.28, "end": 3163.4, "text": " Remember you can always see the source code for something by putting two question marks.", "tokens": [50364, 5459, 291, 393, 1009, 536, 264, 4009, 3089, 337, 746, 538, 3372, 732, 1168, 10640, 13, 50970], "temperature": 0.0, "avg_logprob": -0.3367517666938977, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.01615268364548683}, {"id": 650, "seek": 315128, "start": 3163.4, "end": 3167.88, "text": " Limitmem is just these three lines of code, which I noticed somebody kindly has already", "tokens": [50970, 16406, 270, 17886, 307, 445, 613, 1045, 3876, 295, 3089, 11, 597, 286, 5694, 2618, 29736, 575, 1217, 51194], "temperature": 0.0, "avg_logprob": -0.3367517666938977, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.01615268364548683}, {"id": 651, "seek": 315128, "start": 3167.88, "end": 3171.5600000000004, "text": " pasted in the forum.", "tokens": [51194, 1791, 292, 294, 264, 17542, 13, 51378], "temperature": 0.0, "avg_logprob": -0.3367517666938977, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.01615268364548683}, {"id": 652, "seek": 315128, "start": 3171.5600000000004, "end": 3177.0800000000004, "text": " One of the many things I dislike about TensorFlow for our kind of work is that all of the defaults", "tokens": [51378, 1485, 295, 264, 867, 721, 286, 26006, 466, 37624, 337, 527, 733, 295, 589, 307, 300, 439, 295, 264, 7576, 82, 51654], "temperature": 0.0, "avg_logprob": -0.3367517666938977, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.01615268364548683}, {"id": 653, "seek": 315128, "start": 3177.0800000000004, "end": 3178.96, "text": " are production defaults.", "tokens": [51654, 366, 4265, 7576, 82, 13, 51748], "temperature": 0.0, "avg_logprob": -0.3367517666938977, "compression_ratio": 1.5358851674641147, "no_speech_prob": 0.01615268364548683}, {"id": 654, "seek": 317896, "start": 3179.08, "end": 3184.52, "text": " One of the defaults is it will use up all of your memory on all of your graphics cards.", "tokens": [50370, 1485, 295, 264, 7576, 82, 307, 309, 486, 764, 493, 439, 295, 428, 4675, 322, 439, 295, 428, 11837, 5632, 13, 50642], "temperature": 0.0, "avg_logprob": -0.26532883820710357, "compression_ratio": 1.7932203389830508, "no_speech_prob": 0.0006263331160880625}, {"id": 655, "seek": 317896, "start": 3184.52, "end": 3188.46, "text": " I'm currently running this on a server with 4 graphics cards, which I'm meant to be sharing", "tokens": [50642, 286, 478, 4362, 2614, 341, 322, 257, 7154, 365, 1017, 11837, 5632, 11, 597, 286, 478, 4140, 281, 312, 5414, 50839], "temperature": 0.0, "avg_logprob": -0.26532883820710357, "compression_ratio": 1.7932203389830508, "no_speech_prob": 0.0006263331160880625}, {"id": 656, "seek": 317896, "start": 3188.46, "end": 3192.32, "text": " with my colleagues at the university here.", "tokens": [50839, 365, 452, 7734, 412, 264, 5454, 510, 13, 51032], "temperature": 0.0, "avg_logprob": -0.26532883820710357, "compression_ratio": 1.7932203389830508, "no_speech_prob": 0.0006263331160880625}, {"id": 657, "seek": 317896, "start": 3192.32, "end": 3196.12, "text": " Every time I run a notebook, nobody else can use any of the graphics cards, they're going", "tokens": [51032, 2048, 565, 286, 1190, 257, 21060, 11, 5079, 1646, 393, 764, 604, 295, 264, 11837, 5632, 11, 436, 434, 516, 51222], "temperature": 0.0, "avg_logprob": -0.26532883820710357, "compression_ratio": 1.7932203389830508, "no_speech_prob": 0.0006263331160880625}, {"id": 658, "seek": 317896, "start": 3196.12, "end": 3197.64, "text": " to be really, really pissed.", "tokens": [51222, 281, 312, 534, 11, 534, 23795, 13, 51298], "temperature": 0.0, "avg_logprob": -0.26532883820710357, "compression_ratio": 1.7932203389830508, "no_speech_prob": 0.0006263331160880625}, {"id": 659, "seek": 317896, "start": 3197.64, "end": 3202.04, "text": " And this nice little gig I have of running these little classes is going to disappear", "tokens": [51298, 400, 341, 1481, 707, 8741, 286, 362, 295, 2614, 613, 707, 5359, 307, 516, 281, 11596, 51518], "temperature": 0.0, "avg_logprob": -0.26532883820710357, "compression_ratio": 1.7932203389830508, "no_speech_prob": 0.0006263331160880625}, {"id": 660, "seek": 317896, "start": 3202.04, "end": 3203.2, "text": " very quickly.", "tokens": [51518, 588, 2661, 13, 51576], "temperature": 0.0, "avg_logprob": -0.26532883820710357, "compression_ratio": 1.7932203389830508, "no_speech_prob": 0.0006263331160880625}, {"id": 661, "seek": 317896, "start": 3203.2, "end": 3208.84, "text": " So I need to make sure I run limitmem very soon, as soon as I start running a notebook.", "tokens": [51576, 407, 286, 643, 281, 652, 988, 286, 1190, 4948, 17886, 588, 2321, 11, 382, 2321, 382, 286, 722, 2614, 257, 21060, 13, 51858], "temperature": 0.0, "avg_logprob": -0.26532883820710357, "compression_ratio": 1.7932203389830508, "no_speech_prob": 0.0006263331160880625}, {"id": 662, "seek": 320884, "start": 3209.7200000000003, "end": 3217.2000000000003, "text": " Honestly I think this is a poor choice by the TensorFlow authors because somebody putting", "tokens": [50408, 12348, 286, 519, 341, 307, 257, 4716, 3922, 538, 264, 37624, 16552, 570, 2618, 3372, 50782], "temperature": 0.0, "avg_logprob": -0.3024151523907979, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00030061608413234353}, {"id": 663, "seek": 320884, "start": 3217.2000000000003, "end": 3220.84, "text": " something in production is going to be taking time to optimize things.", "tokens": [50782, 746, 294, 4265, 307, 516, 281, 312, 1940, 565, 281, 19719, 721, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3024151523907979, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00030061608413234353}, {"id": 664, "seek": 320884, "start": 3220.84, "end": 3223.28, "text": " I don't give a shit about the defaults.", "tokens": [50964, 286, 500, 380, 976, 257, 4611, 466, 264, 7576, 82, 13, 51086], "temperature": 0.0, "avg_logprob": -0.3024151523907979, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00030061608413234353}, {"id": 665, "seek": 320884, "start": 3223.28, "end": 3226.4, "text": " Somebody who's hacking something together to quickly see if they can get something working", "tokens": [51086, 13463, 567, 311, 31422, 746, 1214, 281, 2661, 536, 498, 436, 393, 483, 746, 1364, 51242], "temperature": 0.0, "avg_logprob": -0.3024151523907979, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00030061608413234353}, {"id": 666, "seek": 320884, "start": 3226.4, "end": 3228.44, "text": " very much wants nice defaults.", "tokens": [51242, 588, 709, 2738, 1481, 7576, 82, 13, 51344], "temperature": 0.0, "avg_logprob": -0.3024151523907979, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00030061608413234353}, {"id": 667, "seek": 320884, "start": 3228.44, "end": 3234.2000000000003, "text": " So this is one of the many, many places where TensorFlow makes some odd little annoying", "tokens": [51344, 407, 341, 307, 472, 295, 264, 867, 11, 867, 3190, 689, 37624, 1669, 512, 7401, 707, 11304, 51632], "temperature": 0.0, "avg_logprob": -0.3024151523907979, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00030061608413234353}, {"id": 668, "seek": 320884, "start": 3234.2000000000003, "end": 3235.2000000000003, "text": " decisions.", "tokens": [51632, 5327, 13, 51682], "temperature": 0.0, "avg_logprob": -0.3024151523907979, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00030061608413234353}, {"id": 669, "seek": 323520, "start": 3235.2, "end": 3241.2799999999997, "text": " But anyway, every time now I create a new notebook, I copy this line in and make sure", "tokens": [50364, 583, 4033, 11, 633, 565, 586, 286, 1884, 257, 777, 21060, 11, 286, 5055, 341, 1622, 294, 293, 652, 988, 50668], "temperature": 0.0, "avg_logprob": -0.27648101455863866, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.017176559194922447}, {"id": 670, "seek": 323520, "start": 3241.2799999999997, "end": 3248.0, "text": " I run it so this does not use up all of your memory.", "tokens": [50668, 286, 1190, 309, 370, 341, 775, 406, 764, 493, 439, 295, 428, 4675, 13, 51004], "temperature": 0.0, "avg_logprob": -0.27648101455863866, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.017176559194922447}, {"id": 671, "seek": 323520, "start": 3248.0, "end": 3256.08, "text": " So I've got a link to the paper that we're looking at, and indeed we can open it.", "tokens": [51004, 407, 286, 600, 658, 257, 2113, 281, 264, 3035, 300, 321, 434, 1237, 412, 11, 293, 6451, 321, 393, 1269, 309, 13, 51408], "temperature": 0.0, "avg_logprob": -0.27648101455863866, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.017176559194922447}, {"id": 672, "seek": 323520, "start": 3256.08, "end": 3264.04, "text": " Now is a good time to talk about how helpful it is to use some kind of paper reading system.", "tokens": [51408, 823, 307, 257, 665, 565, 281, 751, 466, 577, 4961, 309, 307, 281, 764, 512, 733, 295, 3035, 3760, 1185, 13, 51806], "temperature": 0.0, "avg_logprob": -0.27648101455863866, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.017176559194922447}, {"id": 673, "seek": 326404, "start": 3264.04, "end": 3270.08, "text": " I really like this one, it's free, it's called Mendeley Desktop.", "tokens": [50364, 286, 534, 411, 341, 472, 11, 309, 311, 1737, 11, 309, 311, 1219, 376, 5445, 3420, 49044, 13, 50666], "temperature": 0.0, "avg_logprob": -0.2759490799117874, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.00031015489366836846}, {"id": 674, "seek": 326404, "start": 3270.08, "end": 3277.48, "text": " Mendeley lets you, as you find papers, you can save them into a folder on your computer.", "tokens": [50666, 376, 5445, 3420, 6653, 291, 11, 382, 291, 915, 10577, 11, 291, 393, 3155, 552, 666, 257, 10820, 322, 428, 3820, 13, 51036], "temperature": 0.0, "avg_logprob": -0.2759490799117874, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.00031015489366836846}, {"id": 675, "seek": 326404, "start": 3277.48, "end": 3279.6, "text": " Mendeley will automatically watch that folder.", "tokens": [51036, 376, 5445, 3420, 486, 6772, 1159, 300, 10820, 13, 51142], "temperature": 0.0, "avg_logprob": -0.2759490799117874, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.00031015489366836846}, {"id": 676, "seek": 326404, "start": 3279.6, "end": 3283.88, "text": " Any PDF that appears there gets added to your library.", "tokens": [51142, 2639, 17752, 300, 7038, 456, 2170, 3869, 281, 428, 6405, 13, 51356], "temperature": 0.0, "avg_logprob": -0.2759490799117874, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.00031015489366836846}, {"id": 677, "seek": 326404, "start": 3283.88, "end": 3294.0, "text": " It's really quite cool because what it then does is it finds the archive ID and then you", "tokens": [51356, 467, 311, 534, 1596, 1627, 570, 437, 309, 550, 775, 307, 309, 10704, 264, 23507, 7348, 293, 550, 291, 51862], "temperature": 0.0, "avg_logprob": -0.2759490799117874, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.00031015489366836846}, {"id": 678, "seek": 329400, "start": 3294.96, "end": 3306.32, "text": " can click this little button here and it will go to archive and grab all of the information", "tokens": [50412, 393, 2052, 341, 707, 2960, 510, 293, 309, 486, 352, 281, 23507, 293, 4444, 439, 295, 264, 1589, 50980], "temperature": 0.0, "avg_logprob": -0.33794906992971163, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0018386533483862877}, {"id": 679, "seek": 329400, "start": 3306.32, "end": 3310.6, "text": " such as the abstract and so forth and fill it out for you.", "tokens": [50980, 1270, 382, 264, 12649, 293, 370, 5220, 293, 2836, 309, 484, 337, 291, 13, 51194], "temperature": 0.0, "avg_logprob": -0.33794906992971163, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0018386533483862877}, {"id": 680, "seek": 329400, "start": 3310.6, "end": 3316.28, "text": " This is really great because now any time I want to find out what I've read, which has", "tokens": [51194, 639, 307, 534, 869, 570, 586, 604, 565, 286, 528, 281, 915, 484, 437, 286, 600, 1401, 11, 597, 575, 51478], "temperature": 0.0, "avg_logprob": -0.33794906992971163, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0018386533483862877}, {"id": 681, "seek": 329400, "start": 3316.28, "end": 3323.64, "text": " got anything to do with style, I can type style and up pop all of the papers.", "tokens": [51478, 658, 1340, 281, 360, 365, 3758, 11, 286, 393, 2010, 3758, 293, 493, 1665, 439, 295, 264, 10577, 13, 51846], "temperature": 0.0, "avg_logprob": -0.33794906992971163, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0018386533483862877}, {"id": 682, "seek": 332364, "start": 3324.2799999999997, "end": 3330.2799999999997, "text": " Believe me, after a long time of reading papers without something like this, it basically", "tokens": [50396, 21486, 385, 11, 934, 257, 938, 565, 295, 3760, 10577, 1553, 746, 411, 341, 11, 309, 1936, 50696], "temperature": 0.0, "avg_logprob": -0.2565049225429319, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.013222696259617805}, {"id": 683, "seek": 332364, "start": 3330.2799999999997, "end": 3333.0, "text": " goes in one ear and out the other.", "tokens": [50696, 1709, 294, 472, 1273, 293, 484, 264, 661, 13, 50832], "temperature": 0.0, "avg_logprob": -0.2565049225429319, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.013222696259617805}, {"id": 684, "seek": 332364, "start": 3333.0, "end": 3337.3599999999997, "text": " Literally I've read papers a year later and at the end of it I've realized, I read that", "tokens": [50832, 23768, 286, 600, 1401, 10577, 257, 1064, 1780, 293, 412, 264, 917, 295, 309, 286, 600, 5334, 11, 286, 1401, 300, 51050], "temperature": 0.0, "avg_logprob": -0.2565049225429319, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.013222696259617805}, {"id": 685, "seek": 332364, "start": 3337.3599999999997, "end": 3338.3599999999997, "text": " before.", "tokens": [51050, 949, 13, 51100], "temperature": 0.0, "avg_logprob": -0.2565049225429319, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.013222696259617805}, {"id": 686, "seek": 332364, "start": 3338.3599999999997, "end": 3342.2799999999997, "text": " I don't remember anything else about it, but I know I read it before.", "tokens": [51100, 286, 500, 380, 1604, 1340, 1646, 466, 309, 11, 457, 286, 458, 286, 1401, 309, 949, 13, 51296], "temperature": 0.0, "avg_logprob": -0.2565049225429319, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.013222696259617805}, {"id": 687, "seek": 332364, "start": 3342.2799999999997, "end": 3346.72, "text": " This way I really find that my knowledge builds.", "tokens": [51296, 639, 636, 286, 534, 915, 300, 452, 3601, 15182, 13, 51518], "temperature": 0.0, "avg_logprob": -0.2565049225429319, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.013222696259617805}, {"id": 688, "seek": 332364, "start": 3346.72, "end": 3351.44, "text": " As I find references, I'm immediately there looking at the references.", "tokens": [51518, 1018, 286, 915, 15400, 11, 286, 478, 4258, 456, 1237, 412, 264, 15400, 13, 51754], "temperature": 0.0, "avg_logprob": -0.2565049225429319, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.013222696259617805}, {"id": 689, "seek": 335144, "start": 3351.44, "end": 3361.32, "text": " The other thing you can do is as you start reading the paper, as you can see, my notes", "tokens": [50364, 440, 661, 551, 291, 393, 360, 307, 382, 291, 722, 3760, 264, 3035, 11, 382, 291, 393, 536, 11, 452, 5570, 50858], "temperature": 0.0, "avg_logprob": -0.285859122643104, "compression_ratio": 1.490066225165563, "no_speech_prob": 0.0005193036631681025}, {"id": 690, "seek": 335144, "start": 3361.32, "end": 3364.52, "text": " and highlights are saved.", "tokens": [50858, 293, 14254, 366, 6624, 13, 51018], "temperature": 0.0, "avg_logprob": -0.285859122643104, "compression_ratio": 1.490066225165563, "no_speech_prob": 0.0005193036631681025}, {"id": 691, "seek": 335144, "start": 3364.52, "end": 3369.7200000000003, "text": " They're also duplicated on my mobile devices and my other computers and they're all synced", "tokens": [51018, 814, 434, 611, 1581, 564, 3587, 322, 452, 6013, 5759, 293, 452, 661, 10807, 293, 436, 434, 439, 5451, 1232, 51278], "temperature": 0.0, "avg_logprob": -0.285859122643104, "compression_ratio": 1.490066225165563, "no_speech_prob": 0.0005193036631681025}, {"id": 692, "seek": 335144, "start": 3369.7200000000003, "end": 3370.7200000000003, "text": " up.", "tokens": [51278, 493, 13, 51328], "temperature": 0.0, "avg_logprob": -0.285859122643104, "compression_ratio": 1.490066225165563, "no_speech_prob": 0.0005193036631681025}, {"id": 693, "seek": 335144, "start": 3370.7200000000003, "end": 3374.84, "text": " It's really cool.", "tokens": [51328, 467, 311, 534, 1627, 13, 51534], "temperature": 0.0, "avg_logprob": -0.285859122643104, "compression_ratio": 1.490066225165563, "no_speech_prob": 0.0005193036631681025}, {"id": 694, "seek": 337484, "start": 3374.84, "end": 3382.2400000000002, "text": " Speaking about Archive is a great time to answer a question we had earlier about how", "tokens": [50364, 13069, 466, 10984, 488, 307, 257, 869, 565, 281, 1867, 257, 1168, 321, 632, 3071, 466, 577, 50734], "temperature": 0.0, "avg_logprob": -0.32698385643236566, "compression_ratio": 1.488235294117647, "no_speech_prob": 0.0356772281229496}, {"id": 695, "seek": 337484, "start": 3382.2400000000002, "end": 3384.28, "text": " do you find papers.", "tokens": [50734, 360, 291, 915, 10577, 13, 50836], "temperature": 0.0, "avg_logprob": -0.32698385643236566, "compression_ratio": 1.488235294117647, "no_speech_prob": 0.0356772281229496}, {"id": 696, "seek": 337484, "start": 3384.28, "end": 3395.08, "text": " The vast, vast, vast majority of deep learning papers get put up on archive.org a long, long,", "tokens": [50836, 440, 8369, 11, 8369, 11, 8369, 6286, 295, 2452, 2539, 10577, 483, 829, 493, 322, 23507, 13, 4646, 257, 938, 11, 938, 11, 51376], "temperature": 0.0, "avg_logprob": -0.32698385643236566, "compression_ratio": 1.488235294117647, "no_speech_prob": 0.0356772281229496}, {"id": 697, "seek": 337484, "start": 3395.08, "end": 3399.2400000000002, "text": " long time before they're in any journal or conference.", "tokens": [51376, 938, 565, 949, 436, 434, 294, 604, 6708, 420, 7586, 13, 51584], "temperature": 0.0, "avg_logprob": -0.32698385643236566, "compression_ratio": 1.488235294117647, "no_speech_prob": 0.0356772281229496}, {"id": 698, "seek": 339924, "start": 3399.56, "end": 3405.72, "text": " If you wait until they're in a conference proceedings, you're many, many months or maybe", "tokens": [50380, 759, 291, 1699, 1826, 436, 434, 294, 257, 7586, 37254, 11, 291, 434, 867, 11, 867, 2493, 420, 1310, 50688], "temperature": 0.0, "avg_logprob": -0.25332956728727923, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.009267929010093212}, {"id": 699, "seek": 339924, "start": 3405.72, "end": 3407.66, "text": " even a year behind.", "tokens": [50688, 754, 257, 1064, 2261, 13, 50785], "temperature": 0.0, "avg_logprob": -0.25332956728727923, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.009267929010093212}, {"id": 700, "seek": 339924, "start": 3407.66, "end": 3411.16, "text": " So pretty much everybody uses Archive.", "tokens": [50785, 407, 1238, 709, 2201, 4960, 10984, 488, 13, 50960], "temperature": 0.0, "avg_logprob": -0.25332956728727923, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.009267929010093212}, {"id": 701, "seek": 339924, "start": 3411.16, "end": 3419.7999999999997, "text": " You can go to the AI section of Archive and see what's there, but that's not really what", "tokens": [50960, 509, 393, 352, 281, 264, 7318, 3541, 295, 10984, 488, 293, 536, 437, 311, 456, 11, 457, 300, 311, 406, 534, 437, 51392], "temperature": 0.0, "avg_logprob": -0.25332956728727923, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.009267929010093212}, {"id": 702, "seek": 339924, "start": 3419.7999999999997, "end": 3421.72, "text": " anybody does.", "tokens": [51392, 4472, 775, 13, 51488], "temperature": 0.0, "avg_logprob": -0.25332956728727923, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.009267929010093212}, {"id": 703, "seek": 342172, "start": 3421.72, "end": 3435.3199999999997, "text": " What everybody does instead is use Archive Sanity, the Archive Sanity Preserver.", "tokens": [50364, 708, 2201, 775, 2602, 307, 764, 10984, 488, 5271, 507, 11, 264, 10984, 488, 5271, 507, 2718, 38241, 13, 51044], "temperature": 0.0, "avg_logprob": -0.3055560420936262, "compression_ratio": 1.5, "no_speech_prob": 0.03621940314769745}, {"id": 704, "seek": 342172, "start": 3435.3199999999997, "end": 3439.7599999999998, "text": " This is something that the wonderful Andrej Karpathy built.", "tokens": [51044, 639, 307, 746, 300, 264, 3715, 20667, 73, 591, 6529, 9527, 3094, 13, 51266], "temperature": 0.0, "avg_logprob": -0.3055560420936262, "compression_ratio": 1.5, "no_speech_prob": 0.03621940314769745}, {"id": 705, "seek": 342172, "start": 3439.7599999999998, "end": 3445.48, "text": " What it lets you do is to create a library of articles that somebody tells you to read", "tokens": [51266, 708, 309, 6653, 291, 360, 307, 281, 1884, 257, 6405, 295, 11290, 300, 2618, 5112, 291, 281, 1401, 51552], "temperature": 0.0, "avg_logprob": -0.3055560420936262, "compression_ratio": 1.5, "no_speech_prob": 0.03621940314769745}, {"id": 706, "seek": 342172, "start": 3445.48, "end": 3448.0, "text": " or that you're interested in or you come across.", "tokens": [51552, 420, 300, 291, 434, 3102, 294, 420, 291, 808, 2108, 13, 51678], "temperature": 0.0, "avg_logprob": -0.3055560420936262, "compression_ratio": 1.5, "no_speech_prob": 0.03621940314769745}, {"id": 707, "seek": 344800, "start": 3448.0, "end": 3453.88, "text": " As you create that library by clicking this little save button, it then recommends more", "tokens": [50364, 1018, 291, 1884, 300, 6405, 538, 9697, 341, 707, 3155, 2960, 11, 309, 550, 34556, 544, 50658], "temperature": 0.0, "avg_logprob": -0.3041210367222025, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.015663521364331245}, {"id": 708, "seek": 344800, "start": 3453.88, "end": 3454.88, "text": " papers like it.", "tokens": [50658, 10577, 411, 309, 13, 50708], "temperature": 0.0, "avg_logprob": -0.3041210367222025, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.015663521364331245}, {"id": 709, "seek": 344800, "start": 3454.88, "end": 3462.16, "text": " Or even once you start reading a paper, you go Show Similar and it will then show you", "tokens": [50708, 1610, 754, 1564, 291, 722, 3760, 257, 3035, 11, 291, 352, 6895, 10905, 293, 309, 486, 550, 855, 291, 51072], "temperature": 0.0, "avg_logprob": -0.3041210367222025, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.015663521364331245}, {"id": 710, "seek": 344800, "start": 3462.16, "end": 3465.6, "text": " other papers that are similar to this paper.", "tokens": [51072, 661, 10577, 300, 366, 2531, 281, 341, 3035, 13, 51244], "temperature": 0.0, "avg_logprob": -0.3041210367222025, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.015663521364331245}, {"id": 711, "seek": 344800, "start": 3465.6, "end": 3467.36, "text": " It seems to do a pretty damn good job of it.", "tokens": [51244, 467, 2544, 281, 360, 257, 1238, 8151, 665, 1691, 295, 309, 13, 51332], "temperature": 0.0, "avg_logprob": -0.3041210367222025, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.015663521364331245}, {"id": 712, "seek": 344800, "start": 3467.36, "end": 3473.56, "text": " So you can really explore and get lost in that whole area.", "tokens": [51332, 407, 291, 393, 534, 6839, 293, 483, 2731, 294, 300, 1379, 1859, 13, 51642], "temperature": 0.0, "avg_logprob": -0.3041210367222025, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.015663521364331245}, {"id": 713, "seek": 344800, "start": 3473.56, "end": 3475.0, "text": " So that's one great way to do it.", "tokens": [51642, 407, 300, 311, 472, 869, 636, 281, 360, 309, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3041210367222025, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.015663521364331245}, {"id": 714, "seek": 347500, "start": 3475.0, "end": 3486.32, "text": " As you do that, you'll find that if you go to Archive, one of the buttons that it has", "tokens": [50364, 1018, 291, 360, 300, 11, 291, 603, 915, 300, 498, 291, 352, 281, 10984, 488, 11, 472, 295, 264, 9905, 300, 309, 575, 50930], "temperature": 0.0, "avg_logprob": -0.28835479162072625, "compression_ratio": 1.5518867924528301, "no_speech_prob": 0.01615263894200325}, {"id": 715, "seek": 347500, "start": 3486.32, "end": 3488.88, "text": " is a Bookmark on Mendeley button.", "tokens": [50930, 307, 257, 9476, 5638, 322, 376, 5445, 3420, 2960, 13, 51058], "temperature": 0.0, "avg_logprob": -0.28835479162072625, "compression_ratio": 1.5518867924528301, "no_speech_prob": 0.01615263894200325}, {"id": 716, "seek": 347500, "start": 3488.88, "end": 3492.84, "text": " So even from the abstract here, bang, straight into your library and the next time you load", "tokens": [51058, 407, 754, 490, 264, 12649, 510, 11, 8550, 11, 2997, 666, 428, 6405, 293, 264, 958, 565, 291, 3677, 51256], "temperature": 0.0, "avg_logprob": -0.28835479162072625, "compression_ratio": 1.5518867924528301, "no_speech_prob": 0.01615263894200325}, {"id": 717, "seek": 347500, "start": 3492.84, "end": 3495.16, "text": " up Mendeley, it's all there.", "tokens": [51256, 493, 376, 5445, 3420, 11, 309, 311, 439, 456, 13, 51372], "temperature": 0.0, "avg_logprob": -0.28835479162072625, "compression_ratio": 1.5518867924528301, "no_speech_prob": 0.01615263894200325}, {"id": 718, "seek": 347500, "start": 3495.16, "end": 3504.32, "text": " Then you can put things into folders, so the different parts of the course, I've created", "tokens": [51372, 1396, 291, 393, 829, 721, 666, 31082, 11, 370, 264, 819, 3166, 295, 264, 1164, 11, 286, 600, 2942, 51830], "temperature": 0.0, "avg_logprob": -0.28835479162072625, "compression_ratio": 1.5518867924528301, "no_speech_prob": 0.01615263894200325}, {"id": 719, "seek": 350432, "start": 3504.32, "end": 3510.44, "text": " folders for them and kind of keep track of what I'm reading that way.", "tokens": [50364, 31082, 337, 552, 293, 733, 295, 1066, 2837, 295, 437, 286, 478, 3760, 300, 636, 13, 50670], "temperature": 0.0, "avg_logprob": -0.30318836371103924, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.04146171733736992}, {"id": 720, "seek": 350432, "start": 3510.44, "end": 3519.28, "text": " A good little trick to know about Archive.org is that you often want to know where it's", "tokens": [50670, 316, 665, 707, 4282, 281, 458, 466, 10984, 488, 13, 4646, 307, 300, 291, 2049, 528, 281, 458, 689, 309, 311, 51112], "temperature": 0.0, "avg_logprob": -0.30318836371103924, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.04146171733736992}, {"id": 721, "seek": 350432, "start": 3519.28, "end": 3520.28, "text": " from.", "tokens": [51112, 490, 13, 51162], "temperature": 0.0, "avg_logprob": -0.30318836371103924, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.04146171733736992}, {"id": 722, "seek": 350432, "start": 3520.28, "end": 3525.1200000000003, "text": " If you go to the first page on the left-hand side, you can see the date here.", "tokens": [51162, 759, 291, 352, 281, 264, 700, 3028, 322, 264, 1411, 12, 5543, 1252, 11, 291, 393, 536, 264, 4002, 510, 13, 51404], "temperature": 0.0, "avg_logprob": -0.30318836371103924, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.04146171733736992}, {"id": 723, "seek": 350432, "start": 3525.1200000000003, "end": 3532.8, "text": " Another cool tip is that the file name, the first 4 digits are the year and month for", "tokens": [51404, 3996, 1627, 4125, 307, 300, 264, 3991, 1315, 11, 264, 700, 1017, 27011, 366, 264, 1064, 293, 1618, 337, 51788], "temperature": 0.0, "avg_logprob": -0.30318836371103924, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.04146171733736992}, {"id": 724, "seek": 350432, "start": 3532.8, "end": 3533.8, "text": " that file.", "tokens": [51788, 300, 3991, 13, 51838], "temperature": 0.0, "avg_logprob": -0.30318836371103924, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.04146171733736992}, {"id": 725, "seek": 353380, "start": 3534.28, "end": 3538.0800000000004, "text": " So there's a couple of handy little tips.", "tokens": [50388, 407, 456, 311, 257, 1916, 295, 13239, 707, 6082, 13, 50578], "temperature": 0.0, "avg_logprob": -0.29153259595235187, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00012731089373119175}, {"id": 726, "seek": 353380, "start": 3538.0800000000004, "end": 3546.5600000000004, "text": " As well as Archive Sanity, another really great place for finding papers is Twitter.", "tokens": [50578, 1018, 731, 382, 10984, 488, 5271, 507, 11, 1071, 534, 869, 1081, 337, 5006, 10577, 307, 5794, 13, 51002], "temperature": 0.0, "avg_logprob": -0.29153259595235187, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00012731089373119175}, {"id": 727, "seek": 353380, "start": 3546.5600000000004, "end": 3550.04, "text": " Now if you haven't really used Twitter before, or haven't really used Twitter for this purpose", "tokens": [51002, 823, 498, 291, 2378, 380, 534, 1143, 5794, 949, 11, 420, 2378, 380, 534, 1143, 5794, 337, 341, 4334, 51176], "temperature": 0.0, "avg_logprob": -0.29153259595235187, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00012731089373119175}, {"id": 728, "seek": 353380, "start": 3550.04, "end": 3554.6800000000003, "text": " before, it's hard to know where to start.", "tokens": [51176, 949, 11, 309, 311, 1152, 281, 458, 689, 281, 722, 13, 51408], "temperature": 0.0, "avg_logprob": -0.29153259595235187, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00012731089373119175}, {"id": 729, "seek": 353380, "start": 3554.6800000000003, "end": 3561.88, "text": " So I try to make things easy for people by favoriting lots of the interesting deep learning", "tokens": [51408, 407, 286, 853, 281, 652, 721, 1858, 337, 561, 538, 2294, 1748, 3195, 295, 264, 1880, 2452, 2539, 51768], "temperature": 0.0, "avg_logprob": -0.29153259595235187, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00012731089373119175}, {"id": 730, "seek": 353380, "start": 3561.88, "end": 3562.88, "text": " papers that I come across.", "tokens": [51768, 10577, 300, 286, 808, 2108, 13, 51818], "temperature": 0.0, "avg_logprob": -0.29153259595235187, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00012731089373119175}, {"id": 731, "seek": 356288, "start": 3562.96, "end": 3580.7200000000003, "text": " So if you go to Jeremy P. Howard's page and click on Likes, you'll find that there are", "tokens": [50368, 407, 498, 291, 352, 281, 17809, 430, 13, 17626, 311, 3028, 293, 2052, 322, 441, 8916, 11, 291, 603, 915, 300, 456, 366, 51256], "temperature": 0.0, "avg_logprob": -0.2979872303624307, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.030213436111807823}, {"id": 732, "seek": 356288, "start": 3580.7200000000003, "end": 3585.76, "text": " a thousand links to papers here.", "tokens": [51256, 257, 4714, 6123, 281, 10577, 510, 13, 51508], "temperature": 0.0, "avg_logprob": -0.2979872303624307, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.030213436111807823}, {"id": 733, "seek": 356288, "start": 3585.76, "end": 3588.84, "text": " As you can see, there's generally a few every day.", "tokens": [51508, 1018, 291, 393, 536, 11, 456, 311, 5101, 257, 1326, 633, 786, 13, 51662], "temperature": 0.0, "avg_logprob": -0.2979872303624307, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.030213436111807823}, {"id": 734, "seek": 356288, "start": 3588.84, "end": 3590.1600000000003, "text": " That's useful for a number of reasons.", "tokens": [51662, 663, 311, 4420, 337, 257, 1230, 295, 4112, 13, 51728], "temperature": 0.0, "avg_logprob": -0.2979872303624307, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.030213436111807823}, {"id": 735, "seek": 359016, "start": 3590.16, "end": 3594.56, "text": " One is to get some ideas for papers to read, but perhaps more importantly is to see who's", "tokens": [50364, 1485, 307, 281, 483, 512, 3487, 337, 10577, 281, 1401, 11, 457, 4317, 544, 8906, 307, 281, 536, 567, 311, 50584], "temperature": 0.0, "avg_logprob": -0.4898294622247869, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.41035303473472595}, {"id": 736, "seek": 359016, "start": 3594.56, "end": 3596.7599999999998, "text": " posting these cool links.", "tokens": [50584, 15978, 613, 1627, 6123, 13, 50694], "temperature": 0.0, "avg_logprob": -0.4898294622247869, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.41035303473472595}, {"id": 737, "seek": 359016, "start": 3596.7599999999998, "end": 3598.24, "text": " And then you can follow them as well.", "tokens": [50694, 400, 550, 291, 393, 1524, 552, 382, 731, 13, 50768], "temperature": 0.0, "avg_logprob": -0.4898294622247869, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.41035303473472595}, {"id": 738, "seek": 359016, "start": 3598.24, "end": 3603.6, "text": " Rachel, can you throw that box to that gentleman?", "tokens": [50768, 14246, 11, 393, 291, 3507, 300, 2424, 281, 300, 15761, 30, 51036], "temperature": 0.0, "avg_logprob": -0.4898294622247869, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.41035303473472595}, {"id": 739, "seek": 359016, "start": 3603.6, "end": 3604.6, "text": " The black one.", "tokens": [51036, 440, 2211, 472, 13, 51086], "temperature": 0.0, "avg_logprob": -0.4898294622247869, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.41035303473472595}, {"id": 740, "seek": 359016, "start": 3604.6, "end": 3605.6, "text": " Yes, that's it.", "tokens": [51086, 1079, 11, 300, 311, 309, 13, 51136], "temperature": 0.0, "avg_logprob": -0.4898294622247869, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.41035303473472595}, {"id": 741, "seek": 359016, "start": 3605.6, "end": 3606.6, "text": " Question from the audience.", "tokens": [51136, 14464, 490, 264, 4034, 13, 51186], "temperature": 0.0, "avg_logprob": -0.4898294622247869, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.41035303473472595}, {"id": 742, "seek": 359016, "start": 3606.6, "end": 3610.3999999999996, "text": " Just to speak, it's not a question, it's just information about Archive.", "tokens": [51186, 1449, 281, 1710, 11, 309, 311, 406, 257, 1168, 11, 309, 311, 445, 1589, 466, 10984, 488, 13, 51376], "temperature": 0.0, "avg_logprob": -0.4898294622247869, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.41035303473472595}, {"id": 743, "seek": 359016, "start": 3610.3999999999996, "end": 3617.2, "text": " There is someone who has built a skill on Amazon Alexa.", "tokens": [51376, 821, 307, 1580, 567, 575, 3094, 257, 5389, 322, 6795, 22595, 13, 51716], "temperature": 0.0, "avg_logprob": -0.4898294622247869, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.41035303473472595}, {"id": 744, "seek": 361720, "start": 3617.2, "end": 3622.68, "text": " By asking Alexa to give the most recent paper from Archive, and actually she reads abstract", "tokens": [50364, 3146, 3365, 22595, 281, 976, 264, 881, 5162, 3035, 490, 10984, 488, 11, 293, 767, 750, 15700, 12649, 50638], "temperature": 0.0, "avg_logprob": -0.41221812985978035, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.07252621650695801}, {"id": 745, "seek": 361720, "start": 3622.68, "end": 3635.16, "text": " for you, and you can filter the most recent papers for you.", "tokens": [50638, 337, 291, 11, 293, 291, 393, 6608, 264, 881, 5162, 10577, 337, 291, 13, 51262], "temperature": 0.0, "avg_logprob": -0.41221812985978035, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.07252621650695801}, {"id": 746, "seek": 361720, "start": 3635.16, "end": 3641.7599999999998, "text": " The other place which I find extremely helpful is Reddit Machine Learning.", "tokens": [51262, 440, 661, 1081, 597, 286, 915, 4664, 4961, 307, 32210, 22155, 15205, 13, 51592], "temperature": 0.0, "avg_logprob": -0.41221812985978035, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.07252621650695801}, {"id": 747, "seek": 364176, "start": 3642.76, "end": 3649.44, "text": " Again, there's a lot less that goes through Reddit than goes through Twitter.", "tokens": [50414, 3764, 11, 456, 311, 257, 688, 1570, 300, 1709, 807, 32210, 813, 1709, 807, 5794, 13, 50748], "temperature": 0.0, "avg_logprob": -0.32080566315423875, "compression_ratio": 1.587378640776699, "no_speech_prob": 0.001264405669644475}, {"id": 748, "seek": 364176, "start": 3649.44, "end": 3655.76, "text": " But generally the really interesting things tend to turn up here.", "tokens": [50748, 583, 5101, 264, 534, 1880, 721, 3928, 281, 1261, 493, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.32080566315423875, "compression_ratio": 1.587378640776699, "no_speech_prob": 0.001264405669644475}, {"id": 749, "seek": 364176, "start": 3655.76, "end": 3658.2000000000003, "text": " You can often see the discussions of it.", "tokens": [51064, 509, 393, 2049, 536, 264, 11088, 295, 309, 13, 51186], "temperature": 0.0, "avg_logprob": -0.32080566315423875, "compression_ratio": 1.587378640776699, "no_speech_prob": 0.001264405669644475}, {"id": 750, "seek": 364176, "start": 3658.2000000000003, "end": 3664.5600000000004, "text": " For example, there was a great discussion of PyTorch versus TensorFlow in the last day", "tokens": [51186, 1171, 1365, 11, 456, 390, 257, 869, 5017, 295, 9953, 51, 284, 339, 5717, 37624, 294, 264, 1036, 786, 51504], "temperature": 0.0, "avg_logprob": -0.32080566315423875, "compression_ratio": 1.587378640776699, "no_speech_prob": 0.001264405669644475}, {"id": 751, "seek": 364176, "start": 3664.5600000000004, "end": 3665.5600000000004, "text": " or two.", "tokens": [51504, 420, 732, 13, 51554], "temperature": 0.0, "avg_logprob": -0.32080566315423875, "compression_ratio": 1.587378640776699, "no_speech_prob": 0.001264405669644475}, {"id": 752, "seek": 364176, "start": 3665.5600000000004, "end": 3671.44, "text": " There's a couple of good places to get started.", "tokens": [51554, 821, 311, 257, 1916, 295, 665, 3190, 281, 483, 1409, 13, 51848], "temperature": 0.0, "avg_logprob": -0.32080566315423875, "compression_ratio": 1.587378640776699, "no_speech_prob": 0.001264405669644475}, {"id": 753, "seek": 367144, "start": 3671.44, "end": 3672.44, "text": " Anything I missed, Rachel?", "tokens": [50364, 11998, 286, 6721, 11, 14246, 30, 50414], "temperature": 0.0, "avg_logprob": -0.34881367599755003, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.3701901137828827}, {"id": 754, "seek": 367144, "start": 3672.44, "end": 3673.44, "text": " I think that's good.", "tokens": [50414, 286, 519, 300, 311, 665, 13, 50464], "temperature": 0.0, "avg_logprob": -0.34881367599755003, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.3701901137828827}, {"id": 755, "seek": 367144, "start": 3673.44, "end": 3679.2400000000002, "text": " I have two questions on the image stuff when you go back to style.", "tokens": [50464, 286, 362, 732, 1651, 322, 264, 3256, 1507, 562, 291, 352, 646, 281, 3758, 13, 50754], "temperature": 0.0, "avg_logprob": -0.34881367599755003, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.3701901137828827}, {"id": 756, "seek": 367144, "start": 3679.2400000000002, "end": 3681.88, "text": " I'm ready.", "tokens": [50754, 286, 478, 1919, 13, 50886], "temperature": 0.0, "avg_logprob": -0.34881367599755003, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.3701901137828827}, {"id": 757, "seek": 367144, "start": 3681.88, "end": 3685.52, "text": " One of them was if the app Prisma is using something like this.", "tokens": [50886, 1485, 295, 552, 390, 498, 264, 724, 2114, 14795, 307, 1228, 746, 411, 341, 13, 51068], "temperature": 0.0, "avg_logprob": -0.34881367599755003, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.3701901137828827}, {"id": 758, "seek": 367144, "start": 3685.52, "end": 3688.92, "text": " Yes, Prisma is using exactly this.", "tokens": [51068, 1079, 11, 2114, 14795, 307, 1228, 2293, 341, 13, 51238], "temperature": 0.0, "avg_logprob": -0.34881367599755003, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.3701901137828827}, {"id": 759, "seek": 367144, "start": 3688.92, "end": 3695.0, "text": " The other is, is it better to calculate f-content for a higher layer for VGG and use a lower", "tokens": [51238, 440, 661, 307, 11, 307, 309, 1101, 281, 8873, 283, 12, 9000, 317, 337, 257, 2946, 4583, 337, 691, 27561, 293, 764, 257, 3126, 51542], "temperature": 0.0, "avg_logprob": -0.34881367599755003, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.3701901137828827}, {"id": 760, "seek": 367144, "start": 3695.0, "end": 3700.04, "text": " layer for app style since the higher layer of abstracts are captured in the higher layer", "tokens": [51542, 4583, 337, 724, 3758, 1670, 264, 2946, 4583, 295, 12649, 82, 366, 11828, 294, 264, 2946, 4583, 51794], "temperature": 0.0, "avg_logprob": -0.34881367599755003, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.3701901137828827}, {"id": 761, "seek": 370004, "start": 3700.04, "end": 3701.88, "text": " and the lower layer captures textures?", "tokens": [50364, 293, 264, 3126, 4583, 27986, 24501, 30, 50456], "temperature": 0.0, "avg_logprob": -0.3239228853615381, "compression_ratio": 1.5514018691588785, "no_speech_prob": 0.1871267855167389}, {"id": 762, "seek": 370004, "start": 3701.88, "end": 3704.8, "text": " Probably, let's try it, shall we?", "tokens": [50456, 9210, 11, 718, 311, 853, 309, 11, 4393, 321, 30, 50602], "temperature": 0.0, "avg_logprob": -0.3239228853615381, "compression_ratio": 1.5514018691588785, "no_speech_prob": 0.1871267855167389}, {"id": 763, "seek": 370004, "start": 3704.8, "end": 3708.88, "text": " We haven't learned about f-style yet, so we're just going to look at f-content first.", "tokens": [50602, 492, 2378, 380, 3264, 466, 283, 12, 15014, 1939, 11, 370, 321, 434, 445, 516, 281, 574, 412, 283, 12, 9000, 317, 700, 13, 50806], "temperature": 0.0, "avg_logprob": -0.3239228853615381, "compression_ratio": 1.5514018691588785, "no_speech_prob": 0.1871267855167389}, {"id": 764, "seek": 370004, "start": 3708.88, "end": 3717.72, "text": " I've got some more links to some things you can look at here in the notebook.", "tokens": [50806, 286, 600, 658, 512, 544, 6123, 281, 512, 721, 291, 393, 574, 412, 510, 294, 264, 21060, 13, 51248], "temperature": 0.0, "avg_logprob": -0.3239228853615381, "compression_ratio": 1.5514018691588785, "no_speech_prob": 0.1871267855167389}, {"id": 765, "seek": 370004, "start": 3717.72, "end": 3726.04, "text": " So the data I've linked to in the lesson thread on the forum, I've just grabbed a random sample", "tokens": [51248, 407, 264, 1412, 286, 600, 9408, 281, 294, 264, 6898, 7207, 322, 264, 17542, 11, 286, 600, 445, 18607, 257, 4974, 6889, 51664], "temperature": 0.0, "avg_logprob": -0.3239228853615381, "compression_ratio": 1.5514018691588785, "no_speech_prob": 0.1871267855167389}, {"id": 766, "seek": 372604, "start": 3726.04, "end": 3732.12, "text": " of about 20,000 ImageNet images and I've also put them into bcols arrays.", "tokens": [50364, 295, 466, 945, 11, 1360, 29903, 31890, 5267, 293, 286, 600, 611, 829, 552, 666, 272, 8768, 82, 41011, 13, 50668], "temperature": 0.0, "avg_logprob": -0.3475948909543595, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.2628251910209656}, {"id": 767, "seek": 372604, "start": 3732.12, "end": 3736.84, "text": " So you can set up your paths appropriately.", "tokens": [50668, 407, 291, 393, 992, 493, 428, 14518, 23505, 13, 50904], "temperature": 0.0, "avg_logprob": -0.3475948909543595, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.2628251910209656}, {"id": 768, "seek": 372604, "start": 3736.84, "end": 3740.6, "text": " I haven't given you this pickle, you can figure out how to get the file names easily enough,", "tokens": [50904, 286, 2378, 380, 2212, 291, 341, 31433, 11, 291, 393, 2573, 484, 577, 281, 483, 264, 3991, 5288, 3612, 1547, 11, 51092], "temperature": 0.0, "avg_logprob": -0.3475948909543595, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.2628251910209656}, {"id": 769, "seek": 372604, "start": 3740.6, "end": 3742.96, "text": " so I'm not going to do everything for you.", "tokens": [51092, 370, 286, 478, 406, 516, 281, 360, 1203, 337, 291, 13, 51210], "temperature": 0.0, "avg_logprob": -0.3475948909543595, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.2628251910209656}, {"id": 770, "seek": 372604, "start": 3742.96, "end": 3748.96, "text": " I've grabbed one of those pictures.", "tokens": [51210, 286, 600, 18607, 472, 295, 729, 5242, 13, 51510], "temperature": 0.0, "avg_logprob": -0.3475948909543595, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.2628251910209656}, {"id": 771, "seek": 372604, "start": 3748.96, "end": 3753.0, "text": " Thank you for the person who's showing all the other stuff to pip install, that's very", "tokens": [51510, 1044, 291, 337, 264, 954, 567, 311, 4099, 439, 264, 661, 1507, 281, 8489, 3625, 11, 300, 311, 588, 51712], "temperature": 0.0, "avg_logprob": -0.3475948909543595, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.2628251910209656}, {"id": 772, "seek": 372604, "start": 3753.0, "end": 3754.0, "text": " helpful.", "tokens": [51712, 4961, 13, 51762], "temperature": 0.0, "avg_logprob": -0.3475948909543595, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.2628251910209656}, {"id": 773, "seek": 375400, "start": 3754.0, "end": 3757.56, "text": " So that's going to be our content image.", "tokens": [50364, 407, 300, 311, 516, 281, 312, 527, 2701, 3256, 13, 50542], "temperature": 0.0, "avg_logprob": -0.2831417373988939, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008711293339729309}, {"id": 774, "seek": 375400, "start": 3757.56, "end": 3765.24, "text": " Given that we're using VGG, as per usual, we're going to have to subtract out the mean", "tokens": [50542, 18600, 300, 321, 434, 1228, 691, 27561, 11, 382, 680, 7713, 11, 321, 434, 516, 281, 362, 281, 16390, 484, 264, 914, 50926], "temperature": 0.0, "avg_logprob": -0.2831417373988939, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008711293339729309}, {"id": 775, "seek": 375400, "start": 3765.24, "end": 3771.48, "text": " pixel value from ImageNet and reverse the channel order because of course that's what", "tokens": [50926, 19261, 2158, 490, 29903, 31890, 293, 9943, 264, 2269, 1668, 570, 295, 1164, 300, 311, 437, 51238], "temperature": 0.0, "avg_logprob": -0.2831417373988939, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008711293339729309}, {"id": 776, "seek": 375400, "start": 3771.48, "end": 3774.88, "text": " the original VGG authors did.", "tokens": [51238, 264, 3380, 691, 27561, 16552, 630, 13, 51408], "temperature": 0.0, "avg_logprob": -0.2831417373988939, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008711293339729309}, {"id": 777, "seek": 375400, "start": 3774.88, "end": 3778.6, "text": " So we're going to create an array from the image by just running it through that preprocessing", "tokens": [51408, 407, 321, 434, 516, 281, 1884, 364, 10225, 490, 264, 3256, 538, 445, 2614, 309, 807, 300, 2666, 340, 780, 278, 51594], "temperature": 0.0, "avg_logprob": -0.2831417373988939, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008711293339729309}, {"id": 778, "seek": 375400, "start": 3778.6, "end": 3782.36, "text": " function.", "tokens": [51594, 2445, 13, 51782], "temperature": 0.0, "avg_logprob": -0.2831417373988939, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008711293339729309}, {"id": 779, "seek": 378236, "start": 3782.36, "end": 3786.28, "text": " Later on, we're going to be running things through a network and generating images.", "tokens": [50364, 11965, 322, 11, 321, 434, 516, 281, 312, 2614, 721, 807, 257, 3209, 293, 17746, 5267, 13, 50560], "temperature": 0.0, "avg_logprob": -0.25341151310847354, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.010328120552003384}, {"id": 780, "seek": 378236, "start": 3786.28, "end": 3792.36, "text": " Those generated images we're going to have to add back on that mean and undo that reordering.", "tokens": [50560, 3950, 10833, 5267, 321, 434, 516, 281, 362, 281, 909, 646, 322, 300, 914, 293, 23779, 300, 319, 765, 1794, 13, 50864], "temperature": 0.0, "avg_logprob": -0.25341151310847354, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.010328120552003384}, {"id": 781, "seek": 378236, "start": 3792.36, "end": 3798.36, "text": " So this is what this deprocessing function is going to be for.", "tokens": [50864, 407, 341, 307, 437, 341, 1367, 340, 780, 278, 2445, 307, 516, 281, 312, 337, 13, 51164], "temperature": 0.0, "avg_logprob": -0.25341151310847354, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.010328120552003384}, {"id": 782, "seek": 378236, "start": 3798.36, "end": 3807.1600000000003, "text": " Now I've kind of hand-waved over these functions before and how they work.", "tokens": [51164, 823, 286, 600, 733, 295, 1011, 12, 86, 12865, 670, 613, 6828, 949, 293, 577, 436, 589, 13, 51604], "temperature": 0.0, "avg_logprob": -0.25341151310847354, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.010328120552003384}, {"id": 783, "seek": 378236, "start": 3807.1600000000003, "end": 3810.6, "text": " But I'm going to stop hand-waving for a moment because it's actually quite interesting.", "tokens": [51604, 583, 286, 478, 516, 281, 1590, 1011, 12, 86, 6152, 337, 257, 1623, 570, 309, 311, 767, 1596, 1880, 13, 51776], "temperature": 0.0, "avg_logprob": -0.25341151310847354, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.010328120552003384}, {"id": 784, "seek": 381060, "start": 3810.6, "end": 3815.8399999999997, "text": " Have you ever thought about how is it that we're able to take x, which is a 4-dimensional", "tokens": [50364, 3560, 291, 1562, 1194, 466, 577, 307, 309, 300, 321, 434, 1075, 281, 747, 2031, 11, 597, 307, 257, 1017, 12, 18759, 50626], "temperature": 0.0, "avg_logprob": -0.3470159307256475, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.1403319239616394}, {"id": 785, "seek": 381060, "start": 3815.8399999999997, "end": 3821.3199999999997, "text": " tensor, batch size by height by width by channels.", "tokens": [50626, 40863, 11, 15245, 2744, 538, 6681, 538, 11402, 538, 9235, 13, 50900], "temperature": 0.0, "avg_logprob": -0.3470159307256475, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.1403319239616394}, {"id": 786, "seek": 381060, "start": 3821.3199999999997, "end": 3823.12, "text": " Notice this is not the same as Theano.", "tokens": [50900, 13428, 341, 307, 406, 264, 912, 382, 440, 3730, 13, 50990], "temperature": 0.0, "avg_logprob": -0.3470159307256475, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.1403319239616394}, {"id": 787, "seek": 381060, "start": 3823.12, "end": 3827.24, "text": " Theano was batch size by channels by height by width.", "tokens": [50990, 440, 3730, 390, 15245, 2744, 538, 9235, 538, 6681, 538, 11402, 13, 51196], "temperature": 0.0, "avg_logprob": -0.3470159307256475, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.1403319239616394}, {"id": 788, "seek": 381060, "start": 3827.24, "end": 3829.7599999999998, "text": " We're not doing that anymore.", "tokens": [51196, 492, 434, 406, 884, 300, 3602, 13, 51322], "temperature": 0.0, "avg_logprob": -0.3470159307256475, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.1403319239616394}, {"id": 789, "seek": 381060, "start": 3829.7599999999998, "end": 3833.7999999999997, "text": " Now kind of more natural, batch size by height by width by channels.", "tokens": [51322, 823, 733, 295, 544, 3303, 11, 15245, 2744, 538, 6681, 538, 11402, 538, 9235, 13, 51524], "temperature": 0.0, "avg_logprob": -0.3470159307256475, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.1403319239616394}, {"id": 790, "seek": 381060, "start": 3833.7999999999997, "end": 3840.48, "text": " We're taking a 4-dimensional tensor and we're subtracting from it a vector.", "tokens": [51524, 492, 434, 1940, 257, 1017, 12, 18759, 40863, 293, 321, 434, 16390, 278, 490, 309, 257, 8062, 13, 51858], "temperature": 0.0, "avg_logprob": -0.3470159307256475, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.1403319239616394}, {"id": 791, "seek": 384048, "start": 3841.36, "end": 3844.72, "text": " How is it making that work?", "tokens": [50408, 1012, 307, 309, 1455, 300, 589, 30, 50576], "temperature": 0.0, "avg_logprob": -0.29973305882634343, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0004655265365727246}, {"id": 792, "seek": 384048, "start": 3844.72, "end": 3851.32, "text": " The way it's making that work is because it's doing something called broadcasting.", "tokens": [50576, 440, 636, 309, 311, 1455, 300, 589, 307, 570, 309, 311, 884, 746, 1219, 30024, 13, 50906], "temperature": 0.0, "avg_logprob": -0.29973305882634343, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0004655265365727246}, {"id": 793, "seek": 384048, "start": 3851.32, "end": 3858.4, "text": " Broadcasting refers to any kind of operation where you have arrays, tensors, of different", "tokens": [50906, 14074, 48860, 14942, 281, 604, 733, 295, 6916, 689, 291, 362, 41011, 11, 10688, 830, 11, 295, 819, 51260], "temperature": 0.0, "avg_logprob": -0.29973305882634343, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0004655265365727246}, {"id": 794, "seek": 384048, "start": 3858.4, "end": 3867.16, "text": " dimensions and you do element-wise operations on two tensors of different dimensions.", "tokens": [51260, 12819, 293, 291, 360, 4478, 12, 3711, 7705, 322, 732, 10688, 830, 295, 819, 12819, 13, 51698], "temperature": 0.0, "avg_logprob": -0.29973305882634343, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0004655265365727246}, {"id": 795, "seek": 384048, "start": 3867.16, "end": 3869.88, "text": " How does that work?", "tokens": [51698, 1012, 775, 300, 589, 30, 51834], "temperature": 0.0, "avg_logprob": -0.29973305882634343, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0004655265365727246}, {"id": 796, "seek": 386988, "start": 3870.28, "end": 3877.12, "text": " This idea actually goes back to the early 1960s to an amazing programming language called", "tokens": [50384, 639, 1558, 767, 1709, 646, 281, 264, 2440, 16157, 82, 281, 364, 2243, 9410, 2856, 1219, 50726], "temperature": 0.0, "avg_logprob": -0.2834043791799834, "compression_ratio": 1.451086956521739, "no_speech_prob": 4.9086596845882013e-05}, {"id": 797, "seek": 386988, "start": 3877.12, "end": 3878.12, "text": " APL.", "tokens": [50726, 5372, 43, 13, 50776], "temperature": 0.0, "avg_logprob": -0.2834043791799834, "compression_ratio": 1.451086956521739, "no_speech_prob": 4.9086596845882013e-05}, {"id": 798, "seek": 386988, "start": 3878.12, "end": 3881.92, "text": " APL stands for A Programming Language.", "tokens": [50776, 5372, 43, 7382, 337, 316, 8338, 2810, 24445, 13, 50966], "temperature": 0.0, "avg_logprob": -0.2834043791799834, "compression_ratio": 1.451086956521739, "no_speech_prob": 4.9086596845882013e-05}, {"id": 799, "seek": 386988, "start": 3881.92, "end": 3887.2400000000002, "text": " APL was written by an extraordinary person called Kenneth Iverson.", "tokens": [50966, 5372, 43, 390, 3720, 538, 364, 10581, 954, 1219, 33735, 286, 840, 266, 13, 51232], "temperature": 0.0, "avg_logprob": -0.2834043791799834, "compression_ratio": 1.451086956521739, "no_speech_prob": 4.9086596845882013e-05}, {"id": 800, "seek": 386988, "start": 3887.2400000000002, "end": 3894.36, "text": " Originally APL was a paper describing a new mathematical notation.", "tokens": [51232, 28696, 5372, 43, 390, 257, 3035, 16141, 257, 777, 18894, 24657, 13, 51588], "temperature": 0.0, "avg_logprob": -0.2834043791799834, "compression_ratio": 1.451086956521739, "no_speech_prob": 4.9086596845882013e-05}, {"id": 801, "seek": 389436, "start": 3894.36, "end": 3900.4, "text": " This new mathematical notation was designed to be more flexible and far more precise than", "tokens": [50364, 639, 777, 18894, 24657, 390, 4761, 281, 312, 544, 11358, 293, 1400, 544, 13600, 813, 50666], "temperature": 0.0, "avg_logprob": -0.27681288719177244, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.007460620254278183}, {"id": 802, "seek": 389436, "start": 3900.4, "end": 3902.48, "text": " traditional mathematical notation.", "tokens": [50666, 5164, 18894, 24657, 13, 50770], "temperature": 0.0, "avg_logprob": -0.27681288719177244, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.007460620254278183}, {"id": 803, "seek": 389436, "start": 3902.48, "end": 3907.36, "text": " He then went on to create a programming language that implemented this mathematical notation.", "tokens": [50770, 634, 550, 1437, 322, 281, 1884, 257, 9410, 2856, 300, 12270, 341, 18894, 24657, 13, 51014], "temperature": 0.0, "avg_logprob": -0.27681288719177244, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.007460620254278183}, {"id": 804, "seek": 389436, "start": 3907.36, "end": 3915.44, "text": " APL refers to the notation, which he described as notation as a tool for thought.", "tokens": [51014, 5372, 43, 14942, 281, 264, 24657, 11, 597, 415, 7619, 382, 24657, 382, 257, 2290, 337, 1194, 13, 51418], "temperature": 0.0, "avg_logprob": -0.27681288719177244, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.007460620254278183}, {"id": 805, "seek": 389436, "start": 3915.44, "end": 3921.1200000000003, "text": " He really, unlike the TensorFlow authors, understood the importance of a good API.", "tokens": [51418, 634, 534, 11, 8343, 264, 37624, 16552, 11, 7320, 264, 7379, 295, 257, 665, 9362, 13, 51702], "temperature": 0.0, "avg_logprob": -0.27681288719177244, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.007460620254278183}, {"id": 806, "seek": 392112, "start": 3921.12, "end": 3926.6, "text": " He recognized that the mathematical notation can change how you think about math.", "tokens": [50364, 634, 9823, 300, 264, 18894, 24657, 393, 1319, 577, 291, 519, 466, 5221, 13, 50638], "temperature": 0.0, "avg_logprob": -0.3517936583488218, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.005469222087413073}, {"id": 807, "seek": 392112, "start": 3926.6, "end": 3934.88, "text": " He created a notation which was incredibly expressive.", "tokens": [50638, 634, 2942, 257, 24657, 597, 390, 6252, 40189, 13, 51052], "temperature": 0.0, "avg_logprob": -0.3517936583488218, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.005469222087413073}, {"id": 808, "seek": 392112, "start": 3934.88, "end": 3944.2799999999997, "text": " His son now has gone on to carry the torch and he now continues to support a direct descendant", "tokens": [51052, 2812, 1872, 586, 575, 2780, 322, 281, 3985, 264, 27822, 293, 415, 586, 6515, 281, 1406, 257, 2047, 16333, 394, 51522], "temperature": 0.0, "avg_logprob": -0.3517936583488218, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.005469222087413073}, {"id": 809, "seek": 392112, "start": 3944.2799999999997, "end": 3947.2, "text": " of APL, which is called Jay.", "tokens": [51522, 295, 5372, 43, 11, 597, 307, 1219, 11146, 13, 51668], "temperature": 0.0, "avg_logprob": -0.3517936583488218, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.005469222087413073}, {"id": 810, "seek": 394720, "start": 3947.4399999999996, "end": 3953.96, "text": " If you ever want to find what I think is the most elegant programming language in the world,", "tokens": [50376, 759, 291, 1562, 528, 281, 915, 437, 286, 519, 307, 264, 881, 21117, 9410, 2856, 294, 264, 1002, 11, 50702], "temperature": 0.0, "avg_logprob": -0.2761013074354692, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.008711198344826698}, {"id": 811, "seek": 394720, "start": 3953.96, "end": 3958.4399999999996, "text": " you can go to jsoftware.com and check this out.", "tokens": [50702, 291, 393, 352, 281, 361, 13908, 3039, 13, 1112, 293, 1520, 341, 484, 13, 50926], "temperature": 0.0, "avg_logprob": -0.2761013074354692, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.008711198344826698}, {"id": 812, "seek": 394720, "start": 3958.4399999999996, "end": 3963.48, "text": " How many of you here have used regular expressions?", "tokens": [50926, 1012, 867, 295, 291, 510, 362, 1143, 3890, 15277, 30, 51178], "temperature": 0.0, "avg_logprob": -0.2761013074354692, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.008711198344826698}, {"id": 813, "seek": 394720, "start": 3963.48, "end": 3967.9199999999996, "text": " How many of you, the first time you looked at a complex regular expression, thought that", "tokens": [51178, 1012, 867, 295, 291, 11, 264, 700, 565, 291, 2956, 412, 257, 3997, 3890, 6114, 11, 1194, 300, 51400], "temperature": 0.0, "avg_logprob": -0.2761013074354692, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.008711198344826698}, {"id": 814, "seek": 394720, "start": 3967.9199999999996, "end": 3971.7599999999998, "text": " is totally intuitive?", "tokens": [51400, 307, 3879, 21769, 30, 51592], "temperature": 0.0, "avg_logprob": -0.2761013074354692, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.008711198344826698}, {"id": 815, "seek": 394720, "start": 3971.7599999999998, "end": 3974.2, "text": " You will feel the same way about Jay.", "tokens": [51592, 509, 486, 841, 264, 912, 636, 466, 11146, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2761013074354692, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.008711198344826698}, {"id": 816, "seek": 397420, "start": 3974.2, "end": 3982.24, "text": " The first time that you look at a piece of Jay, you'll go, what the bloody hell.", "tokens": [50364, 440, 700, 565, 300, 291, 574, 412, 257, 2522, 295, 11146, 11, 291, 603, 352, 11, 437, 264, 18938, 4921, 13, 50766], "temperature": 0.0, "avg_logprob": -0.28114065690474077, "compression_ratio": 1.3945578231292517, "no_speech_prob": 0.002511447761207819}, {"id": 817, "seek": 397420, "start": 3982.24, "end": 3993.24, "text": " Because it's an even more expressive and a much older language than regular expressions.", "tokens": [50766, 1436, 309, 311, 364, 754, 544, 40189, 293, 257, 709, 4906, 2856, 813, 3890, 15277, 13, 51316], "temperature": 0.0, "avg_logprob": -0.28114065690474077, "compression_ratio": 1.3945578231292517, "no_speech_prob": 0.002511447761207819}, {"id": 818, "seek": 397420, "start": 3993.24, "end": 3998.56, "text": " Here's an example of a line of Jay.", "tokens": [51316, 1692, 311, 364, 1365, 295, 257, 1622, 295, 11146, 13, 51582], "temperature": 0.0, "avg_logprob": -0.28114065690474077, "compression_ratio": 1.3945578231292517, "no_speech_prob": 0.002511447761207819}, {"id": 819, "seek": 399856, "start": 3998.88, "end": 4005.68, "text": " What's going on here is that this is a language which at its heart almost never requires you", "tokens": [50380, 708, 311, 516, 322, 510, 307, 300, 341, 307, 257, 2856, 597, 412, 1080, 1917, 1920, 1128, 7029, 291, 50720], "temperature": 0.0, "avg_logprob": -0.3279156826510288, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.021615197882056236}, {"id": 820, "seek": 399856, "start": 4005.68, "end": 4011.68, "text": " to write a single loop because it does everything with multidimensional tensors and broadcasting.", "tokens": [50720, 281, 2464, 257, 2167, 6367, 570, 309, 775, 1203, 365, 2120, 327, 332, 11075, 10688, 830, 293, 30024, 13, 51020], "temperature": 0.0, "avg_logprob": -0.3279156826510288, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.021615197882056236}, {"id": 821, "seek": 399856, "start": 4011.68, "end": 4018.68, "text": " Everything we're going to learn about today with broadcasting is a very diluted, simplified,", "tokens": [51020, 5471, 321, 434, 516, 281, 1466, 466, 965, 365, 30024, 307, 257, 588, 11504, 4866, 11, 26335, 11, 51370], "temperature": 0.0, "avg_logprob": -0.3279156826510288, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.021615197882056236}, {"id": 822, "seek": 399856, "start": 4018.68, "end": 4022.84, "text": " crapified version of what APL created in the early 60s.", "tokens": [51370, 12426, 2587, 3037, 295, 437, 5372, 43, 2942, 294, 264, 2440, 4060, 82, 13, 51578], "temperature": 0.0, "avg_logprob": -0.3279156826510288, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.021615197882056236}, {"id": 823, "seek": 399856, "start": 4022.84, "end": 4028.08, "text": " Which is not to say anything rude about Python's implementation, it's one of the best.", "tokens": [51578, 3013, 307, 406, 281, 584, 1340, 18895, 466, 15329, 311, 11420, 11, 309, 311, 472, 295, 264, 1151, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3279156826510288, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.021615197882056236}, {"id": 824, "seek": 402808, "start": 4028.6, "end": 4033.92, "text": " Jay and APL totally blow it away.", "tokens": [50390, 11146, 293, 5372, 43, 3879, 6327, 309, 1314, 13, 50656], "temperature": 0.0, "avg_logprob": -0.46275899496423195, "compression_ratio": 1.3842364532019704, "no_speech_prob": 0.0015247749397531152}, {"id": 825, "seek": 402808, "start": 4033.92, "end": 4038.4, "text": " If you want to really expand your brain and have fun, check out Jay.", "tokens": [50656, 759, 291, 528, 281, 534, 5268, 428, 3567, 293, 362, 1019, 11, 1520, 484, 11146, 13, 50880], "temperature": 0.0, "avg_logprob": -0.46275899496423195, "compression_ratio": 1.3842364532019704, "no_speech_prob": 0.0015247749397531152}, {"id": 826, "seek": 402808, "start": 4038.4, "end": 4045.6, "text": " In the meantime, what does Keras, Thiano, and TensorFlow broadcasting look like?", "tokens": [50880, 682, 264, 14991, 11, 437, 775, 591, 6985, 11, 334, 6254, 11, 293, 37624, 30024, 574, 411, 30, 51240], "temperature": 0.0, "avg_logprob": -0.46275899496423195, "compression_ratio": 1.3842364532019704, "no_speech_prob": 0.0015247749397531152}, {"id": 827, "seek": 402808, "start": 4045.6, "end": 4048.7999999999997, "text": " Let's look at some examples.", "tokens": [51240, 961, 311, 574, 412, 512, 5110, 13, 51400], "temperature": 0.0, "avg_logprob": -0.46275899496423195, "compression_ratio": 1.3842364532019704, "no_speech_prob": 0.0015247749397531152}, {"id": 828, "seek": 402808, "start": 4048.7999999999997, "end": 4053.84, "text": " So here we have, oh I like them too, Input AI and WOWML Newsletters.", "tokens": [51400, 407, 510, 321, 362, 11, 1954, 286, 411, 552, 886, 11, 682, 2582, 7318, 293, 34728, 12683, 7987, 2631, 1559, 13, 51652], "temperature": 0.0, "avg_logprob": -0.46275899496423195, "compression_ratio": 1.3842364532019704, "no_speech_prob": 0.0015247749397531152}, {"id": 829, "seek": 405384, "start": 4054.84, "end": 4066.56, "text": " Here is a vector, a one-dimensional tensor minus a scalar.", "tokens": [50414, 1692, 307, 257, 8062, 11, 257, 472, 12, 18759, 40863, 3175, 257, 39684, 13, 51000], "temperature": 0.0, "avg_logprob": -0.28974654709083447, "compression_ratio": 1.5766871165644172, "no_speech_prob": 0.027168992906808853}, {"id": 830, "seek": 405384, "start": 4066.56, "end": 4071.4, "text": " That makes perfect sense, that you can subtract a scalar from a one-dimensional tensor.", "tokens": [51000, 663, 1669, 2176, 2020, 11, 300, 291, 393, 16390, 257, 39684, 490, 257, 472, 12, 18759, 40863, 13, 51242], "temperature": 0.0, "avg_logprob": -0.28974654709083447, "compression_ratio": 1.5766871165644172, "no_speech_prob": 0.027168992906808853}, {"id": 831, "seek": 405384, "start": 4071.4, "end": 4073.32, "text": " But what is it actually doing?", "tokens": [51242, 583, 437, 307, 309, 767, 884, 30, 51338], "temperature": 0.0, "avg_logprob": -0.28974654709083447, "compression_ratio": 1.5766871165644172, "no_speech_prob": 0.027168992906808853}, {"id": 832, "seek": 405384, "start": 4073.32, "end": 4078.28, "text": " What it's actually doing is it's taking this 2 and it's replicating it 3 times.", "tokens": [51338, 708, 309, 311, 767, 884, 307, 309, 311, 1940, 341, 568, 293, 309, 311, 3248, 30541, 309, 805, 1413, 13, 51586], "temperature": 0.0, "avg_logprob": -0.28974654709083447, "compression_ratio": 1.5766871165644172, "no_speech_prob": 0.027168992906808853}, {"id": 833, "seek": 407828, "start": 4078.84, "end": 4085.0, "text": " This is actually element-wise, 1, 2, 3, minus 2, 2, 2.", "tokens": [50392, 639, 307, 767, 4478, 12, 3711, 11, 502, 11, 568, 11, 805, 11, 3175, 568, 11, 568, 11, 568, 13, 50700], "temperature": 0.0, "avg_logprob": -0.3032901627676828, "compression_ratio": 1.31496062992126, "no_speech_prob": 0.00857744924724102}, {"id": 834, "seek": 407828, "start": 4085.0, "end": 4095.0, "text": " It has broadcasted the scalar across the 3-element vector 1, 2, 3.", "tokens": [50700, 467, 575, 9975, 292, 264, 39684, 2108, 264, 805, 12, 68, 3054, 8062, 502, 11, 568, 11, 805, 13, 51200], "temperature": 0.0, "avg_logprob": -0.3032901627676828, "compression_ratio": 1.31496062992126, "no_speech_prob": 0.00857744924724102}, {"id": 835, "seek": 407828, "start": 4095.0, "end": 4100.24, "text": " So there's our first example of broadcasting.", "tokens": [51200, 407, 456, 311, 527, 700, 1365, 295, 30024, 13, 51462], "temperature": 0.0, "avg_logprob": -0.3032901627676828, "compression_ratio": 1.31496062992126, "no_speech_prob": 0.00857744924724102}, {"id": 836, "seek": 410024, "start": 4100.24, "end": 4109.88, "text": " So in general, broadcasting has a very specific set of rules, which is this.", "tokens": [50364, 407, 294, 2674, 11, 30024, 575, 257, 588, 2685, 992, 295, 4474, 11, 597, 307, 341, 13, 50846], "temperature": 0.0, "avg_logprob": -0.28160745551787225, "compression_ratio": 1.5885416666666667, "no_speech_prob": 0.0060035730712115765}, {"id": 837, "seek": 410024, "start": 4109.88, "end": 4117.08, "text": " You can take two tensors, and you first of all take the shorter tensor, the tensor with", "tokens": [50846, 509, 393, 747, 732, 10688, 830, 11, 293, 291, 700, 295, 439, 747, 264, 11639, 40863, 11, 264, 40863, 365, 51206], "temperature": 0.0, "avg_logprob": -0.28160745551787225, "compression_ratio": 1.5885416666666667, "no_speech_prob": 0.0060035730712115765}, {"id": 838, "seek": 410024, "start": 4117.08, "end": 4121.28, "text": " less dimensions, and prepend unit axes to the front.", "tokens": [51206, 1570, 12819, 11, 293, 2666, 521, 4985, 35387, 281, 264, 1868, 13, 51416], "temperature": 0.0, "avg_logprob": -0.28160745551787225, "compression_ratio": 1.5885416666666667, "no_speech_prob": 0.0060035730712115765}, {"id": 839, "seek": 410024, "start": 4121.28, "end": 4123.76, "text": " What do I mean when I say prepend unit axes?", "tokens": [51416, 708, 360, 286, 914, 562, 286, 584, 2666, 521, 4985, 35387, 30, 51540], "temperature": 0.0, "avg_logprob": -0.28160745551787225, "compression_ratio": 1.5885416666666667, "no_speech_prob": 0.0060035730712115765}, {"id": 840, "seek": 410024, "start": 4123.76, "end": 4127.28, "text": " Here's an example of prepending unit axes.", "tokens": [51540, 1692, 311, 364, 1365, 295, 2666, 2029, 4985, 35387, 13, 51716], "temperature": 0.0, "avg_logprob": -0.28160745551787225, "compression_ratio": 1.5885416666666667, "no_speech_prob": 0.0060035730712115765}, {"id": 841, "seek": 412728, "start": 4127.32, "end": 4132.679999999999, "text": " Take the vector 2, 3 and prepend 3 unit axes on the front.", "tokens": [50366, 3664, 264, 8062, 568, 11, 805, 293, 2666, 521, 805, 4985, 35387, 322, 264, 1868, 13, 50634], "temperature": 0.0, "avg_logprob": -0.2537367615518691, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.015906255692243576}, {"id": 842, "seek": 412728, "start": 4132.679999999999, "end": 4138.759999999999, "text": " It is now a 4-dimensional tensor of shape 1, 1, 1, 2.", "tokens": [50634, 467, 307, 586, 257, 1017, 12, 18759, 40863, 295, 3909, 502, 11, 502, 11, 502, 11, 568, 13, 50938], "temperature": 0.0, "avg_logprob": -0.2537367615518691, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.015906255692243576}, {"id": 843, "seek": 412728, "start": 4138.759999999999, "end": 4145.04, "text": " So if you turn a row into a column, you're adding one unit axis.", "tokens": [50938, 407, 498, 291, 1261, 257, 5386, 666, 257, 7738, 11, 291, 434, 5127, 472, 4985, 10298, 13, 51252], "temperature": 0.0, "avg_logprob": -0.2537367615518691, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.015906255692243576}, {"id": 844, "seek": 412728, "start": 4145.04, "end": 4151.12, "text": " If you're then turning it into a single slice, you're adding another unit axis.", "tokens": [51252, 759, 291, 434, 550, 6246, 309, 666, 257, 2167, 13153, 11, 291, 434, 5127, 1071, 4985, 10298, 13, 51556], "temperature": 0.0, "avg_logprob": -0.2537367615518691, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.015906255692243576}, {"id": 845, "seek": 415112, "start": 4151.599999999999, "end": 4158.0, "text": " So you can always make something into a higher dimensionality by adding unit axes.", "tokens": [50388, 407, 291, 393, 1009, 652, 746, 666, 257, 2946, 10139, 1860, 538, 5127, 4985, 35387, 13, 50708], "temperature": 0.0, "avg_logprob": -0.303608628892407, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0039454009383916855}, {"id": 846, "seek": 415112, "start": 4158.0, "end": 4164.5199999999995, "text": " So when you broadcast, it takes the thing with less axes, less dimensions, and adds", "tokens": [50708, 407, 562, 291, 9975, 11, 309, 2516, 264, 551, 365, 1570, 35387, 11, 1570, 12819, 11, 293, 10860, 51034], "temperature": 0.0, "avg_logprob": -0.303608628892407, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0039454009383916855}, {"id": 847, "seek": 415112, "start": 4164.5199999999995, "end": 4167.64, "text": " prepends unit axes to the front.", "tokens": [51034, 2666, 2581, 4985, 35387, 281, 264, 1868, 13, 51190], "temperature": 0.0, "avg_logprob": -0.303608628892407, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0039454009383916855}, {"id": 848, "seek": 415112, "start": 4167.64, "end": 4172.46, "text": " And then what it does is it says, let's take this first example.", "tokens": [51190, 400, 550, 437, 309, 775, 307, 309, 1619, 11, 718, 311, 747, 341, 700, 1365, 13, 51431], "temperature": 0.0, "avg_logprob": -0.303608628892407, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0039454009383916855}, {"id": 849, "seek": 415112, "start": 4172.46, "end": 4178.36, "text": " It's taken this thing which has no axes, it's a scalar, and turns it into a vector of length", "tokens": [51431, 467, 311, 2726, 341, 551, 597, 575, 572, 35387, 11, 309, 311, 257, 39684, 11, 293, 4523, 309, 666, 257, 8062, 295, 4641, 51726], "temperature": 0.0, "avg_logprob": -0.303608628892407, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0039454009383916855}, {"id": 850, "seek": 415112, "start": 4178.36, "end": 4180.24, "text": " 1.", "tokens": [51726, 502, 13, 51820], "temperature": 0.0, "avg_logprob": -0.303608628892407, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0039454009383916855}, {"id": 851, "seek": 418024, "start": 4180.36, "end": 4187.8, "text": " Then what it does is it finds anything which is of length 1 and duplicates it enough times", "tokens": [50370, 1396, 437, 309, 775, 307, 309, 10704, 1340, 597, 307, 295, 4641, 502, 293, 17154, 1024, 309, 1547, 1413, 50742], "temperature": 0.0, "avg_logprob": -0.2604415291234067, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.0001911040599225089}, {"id": 852, "seek": 418024, "start": 4187.8, "end": 4190.5199999999995, "text": " so that it matches the other thing.", "tokens": [50742, 370, 300, 309, 10676, 264, 661, 551, 13, 50878], "temperature": 0.0, "avg_logprob": -0.2604415291234067, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.0001911040599225089}, {"id": 853, "seek": 418024, "start": 4190.5199999999995, "end": 4197.88, "text": " So here we have something which is a 4-dimensional tensor of size 5, 1, 3, 2.", "tokens": [50878, 407, 510, 321, 362, 746, 597, 307, 257, 1017, 12, 18759, 40863, 295, 2744, 1025, 11, 502, 11, 805, 11, 568, 13, 51246], "temperature": 0.0, "avg_logprob": -0.2604415291234067, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.0001911040599225089}, {"id": 854, "seek": 418024, "start": 4197.88, "end": 4209.28, "text": " So it's got 2 columns, 3 rows, 1 slice, and 5 cubes.", "tokens": [51246, 407, 309, 311, 658, 568, 13766, 11, 805, 13241, 11, 502, 13153, 11, 293, 1025, 25415, 13, 51816], "temperature": 0.0, "avg_logprob": -0.2604415291234067, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.0001911040599225089}, {"id": 855, "seek": 420928, "start": 4209.32, "end": 4212.96, "text": " And then we're going to subtract from it a vector of length 2.", "tokens": [50366, 400, 550, 321, 434, 516, 281, 16390, 490, 309, 257, 8062, 295, 4641, 568, 13, 50548], "temperature": 0.0, "avg_logprob": -0.24217575073242187, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0007793650147505105}, {"id": 856, "seek": 420928, "start": 4212.96, "end": 4223.0, "text": " So remember from our definition, it's then going to automatically reshape this by prepending", "tokens": [50548, 407, 1604, 490, 527, 7123, 11, 309, 311, 550, 516, 281, 6772, 725, 42406, 341, 538, 2666, 2029, 51050], "temperature": 0.0, "avg_logprob": -0.24217575073242187, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0007793650147505105}, {"id": 857, "seek": 420928, "start": 4223.0, "end": 4228.0, "text": " unit axes until it's the same length.", "tokens": [51050, 4985, 35387, 1826, 309, 311, 264, 912, 4641, 13, 51300], "temperature": 0.0, "avg_logprob": -0.24217575073242187, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0007793650147505105}, {"id": 858, "seek": 420928, "start": 4228.0, "end": 4237.32, "text": " And then it's going to copy this thing 3 times, this thing 1 time, and this thing 5 times.", "tokens": [51300, 400, 550, 309, 311, 516, 281, 5055, 341, 551, 805, 1413, 11, 341, 551, 502, 565, 11, 293, 341, 551, 1025, 1413, 13, 51766], "temperature": 0.0, "avg_logprob": -0.24217575073242187, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0007793650147505105}, {"id": 859, "seek": 423732, "start": 4237.36, "end": 4244.92, "text": " So the shape is 5, 1, 3, 2.", "tokens": [50366, 407, 264, 3909, 307, 1025, 11, 502, 11, 805, 11, 568, 13, 50744], "temperature": 0.0, "avg_logprob": -0.24216833981600674, "compression_ratio": 1.4484848484848485, "no_speech_prob": 6.402004510164261e-05}, {"id": 860, "seek": 423732, "start": 4244.92, "end": 4255.48, "text": " So it's going to subtract this vector from every row, every slice, every cube.", "tokens": [50744, 407, 309, 311, 516, 281, 16390, 341, 8062, 490, 633, 5386, 11, 633, 13153, 11, 633, 13728, 13, 51272], "temperature": 0.0, "avg_logprob": -0.24216833981600674, "compression_ratio": 1.4484848484848485, "no_speech_prob": 6.402004510164261e-05}, {"id": 861, "seek": 423732, "start": 4255.48, "end": 4263.719999999999, "text": " So you can play around with these little broadcasting examples and try to get a real feel for how", "tokens": [51272, 407, 291, 393, 862, 926, 365, 613, 707, 30024, 5110, 293, 853, 281, 483, 257, 957, 841, 337, 577, 51684], "temperature": 0.0, "avg_logprob": -0.24216833981600674, "compression_ratio": 1.4484848484848485, "no_speech_prob": 6.402004510164261e-05}, {"id": 862, "seek": 423732, "start": 4263.719999999999, "end": 4265.719999999999, "text": " to make broadcasting work for you.", "tokens": [51684, 281, 652, 30024, 589, 337, 291, 13, 51784], "temperature": 0.0, "avg_logprob": -0.24216833981600674, "compression_ratio": 1.4484848484848485, "no_speech_prob": 6.402004510164261e-05}, {"id": 863, "seek": 426572, "start": 4265.76, "end": 4273.360000000001, "text": " So in this case, we were able to take a 4-dimensional tensor and subtract from it a 3-dimensional", "tokens": [50366, 407, 294, 341, 1389, 11, 321, 645, 1075, 281, 747, 257, 1017, 12, 18759, 40863, 293, 16390, 490, 309, 257, 805, 12, 18759, 50746], "temperature": 0.0, "avg_logprob": -0.29281537909256783, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.004133954644203186}, {"id": 864, "seek": 426572, "start": 4273.360000000001, "end": 4279.240000000001, "text": " vector, knowing that it is going to copy that 3-dimensional vector of channels to every", "tokens": [50746, 8062, 11, 5276, 300, 309, 307, 516, 281, 5055, 300, 805, 12, 18759, 8062, 295, 9235, 281, 633, 51040], "temperature": 0.0, "avg_logprob": -0.29281537909256783, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.004133954644203186}, {"id": 865, "seek": 426572, "start": 4279.240000000001, "end": 4282.84, "text": " row, to every column, to every batch.", "tokens": [51040, 5386, 11, 281, 633, 7738, 11, 281, 633, 15245, 13, 51220], "temperature": 0.0, "avg_logprob": -0.29281537909256783, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.004133954644203186}, {"id": 866, "seek": 426572, "start": 4282.84, "end": 4287.280000000001, "text": " So in the end, it's just done what we mean.", "tokens": [51220, 407, 294, 264, 917, 11, 309, 311, 445, 1096, 437, 321, 914, 13, 51442], "temperature": 0.0, "avg_logprob": -0.29281537909256783, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.004133954644203186}, {"id": 867, "seek": 426572, "start": 4287.280000000001, "end": 4294.68, "text": " It subtracted the mean average of the channels from all of the images the way we wanted it", "tokens": [51442, 467, 16390, 292, 264, 914, 4274, 295, 264, 9235, 490, 439, 295, 264, 5267, 264, 636, 321, 1415, 309, 51812], "temperature": 0.0, "avg_logprob": -0.29281537909256783, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.004133954644203186}, {"id": 868, "seek": 429468, "start": 4294.68, "end": 4295.68, "text": " to.", "tokens": [50364, 281, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2976156507219587, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14414598047733307}, {"id": 869, "seek": 429468, "start": 4295.68, "end": 4300.96, "text": " But it's been amazing how often I've taken code that I've downloaded off the internet", "tokens": [50414, 583, 309, 311, 668, 2243, 577, 2049, 286, 600, 2726, 3089, 300, 286, 600, 21748, 766, 264, 4705, 50678], "temperature": 0.0, "avg_logprob": -0.2976156507219587, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14414598047733307}, {"id": 870, "seek": 429468, "start": 4300.96, "end": 4307.92, "text": " and made it often 10 or 20 times smaller in terms of lines of code just by using lots", "tokens": [50678, 293, 1027, 309, 2049, 1266, 420, 945, 1413, 4356, 294, 2115, 295, 3876, 295, 3089, 445, 538, 1228, 3195, 51026], "temperature": 0.0, "avg_logprob": -0.2976156507219587, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14414598047733307}, {"id": 871, "seek": 429468, "start": 4307.92, "end": 4309.72, "text": " of broadcasting.", "tokens": [51026, 295, 30024, 13, 51116], "temperature": 0.0, "avg_logprob": -0.2976156507219587, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14414598047733307}, {"id": 872, "seek": 429468, "start": 4309.72, "end": 4312.56, "text": " And the reason I'm talking about this now is because we're going to be using this a", "tokens": [51116, 400, 264, 1778, 286, 478, 1417, 466, 341, 586, 307, 570, 321, 434, 516, 281, 312, 1228, 341, 257, 51258], "temperature": 0.0, "avg_logprob": -0.2976156507219587, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14414598047733307}, {"id": 873, "seek": 429468, "start": 4312.56, "end": 4313.56, "text": " lot.", "tokens": [51258, 688, 13, 51308], "temperature": 0.0, "avg_logprob": -0.2976156507219587, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14414598047733307}, {"id": 874, "seek": 429468, "start": 4313.56, "end": 4316.240000000001, "text": " So play with it.", "tokens": [51308, 407, 862, 365, 309, 13, 51442], "temperature": 0.0, "avg_logprob": -0.2976156507219587, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14414598047733307}, {"id": 875, "seek": 429468, "start": 4316.240000000001, "end": 4321.200000000001, "text": " And as I say, if you really want to have fun, play with it in J.", "tokens": [51442, 400, 382, 286, 584, 11, 498, 291, 534, 528, 281, 362, 1019, 11, 862, 365, 309, 294, 508, 13, 51690], "temperature": 0.0, "avg_logprob": -0.2976156507219587, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14414598047733307}, {"id": 876, "seek": 432120, "start": 4321.2, "end": 4327.4, "text": " Okay, so that was a diversion, but it's one that's going to be important throughout this.", "tokens": [50364, 1033, 11, 370, 300, 390, 257, 49422, 11, 457, 309, 311, 472, 300, 311, 516, 281, 312, 1021, 3710, 341, 13, 50674], "temperature": 0.0, "avg_logprob": -0.30951695383330924, "compression_ratio": 1.5364583333333333, "no_speech_prob": 0.001064961776137352}, {"id": 877, "seek": 432120, "start": 4327.4, "end": 4336.28, "text": " So we've now basically got the data that we want.", "tokens": [50674, 407, 321, 600, 586, 1936, 658, 264, 1412, 300, 321, 528, 13, 51118], "temperature": 0.0, "avg_logprob": -0.30951695383330924, "compression_ratio": 1.5364583333333333, "no_speech_prob": 0.001064961776137352}, {"id": 878, "seek": 432120, "start": 4336.28, "end": 4339.639999999999, "text": " So next thing we need is a VGG model.", "tokens": [51118, 407, 958, 551, 321, 643, 307, 257, 691, 27561, 2316, 13, 51286], "temperature": 0.0, "avg_logprob": -0.30951695383330924, "compression_ratio": 1.5364583333333333, "no_speech_prob": 0.001064961776137352}, {"id": 879, "seek": 432120, "start": 4339.639999999999, "end": 4347.72, "text": " Here's the thing though, when we're doing generative models, we want to be very careful", "tokens": [51286, 1692, 311, 264, 551, 1673, 11, 562, 321, 434, 884, 1337, 1166, 5245, 11, 321, 528, 281, 312, 588, 5026, 51690], "temperature": 0.0, "avg_logprob": -0.30951695383330924, "compression_ratio": 1.5364583333333333, "no_speech_prob": 0.001064961776137352}, {"id": 880, "seek": 432120, "start": 4347.72, "end": 4350.5199999999995, "text": " of throwing away information.", "tokens": [51690, 295, 10238, 1314, 1589, 13, 51830], "temperature": 0.0, "avg_logprob": -0.30951695383330924, "compression_ratio": 1.5364583333333333, "no_speech_prob": 0.001064961776137352}, {"id": 881, "seek": 435052, "start": 4350.84, "end": 4355.320000000001, "text": " One of the main ways to throw away information is to use max-pooling.", "tokens": [50380, 1485, 295, 264, 2135, 2098, 281, 3507, 1314, 1589, 307, 281, 764, 11469, 12, 17374, 278, 13, 50604], "temperature": 0.0, "avg_logprob": -0.22792942649439762, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.985957275494002e-05}, {"id": 882, "seek": 435052, "start": 4355.320000000001, "end": 4361.52, "text": " When you use max-pooling, let's say 2,2 max-pooling, you're throwing away 3 quarters of the previous", "tokens": [50604, 1133, 291, 764, 11469, 12, 17374, 278, 11, 718, 311, 584, 568, 11, 17, 11469, 12, 17374, 278, 11, 291, 434, 10238, 1314, 805, 20612, 295, 264, 3894, 50914], "temperature": 0.0, "avg_logprob": -0.22792942649439762, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.985957275494002e-05}, {"id": 883, "seek": 435052, "start": 4361.52, "end": 4366.52, "text": " layer and just keeping the highest one.", "tokens": [50914, 4583, 293, 445, 5145, 264, 6343, 472, 13, 51164], "temperature": 0.0, "avg_logprob": -0.22792942649439762, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.985957275494002e-05}, {"id": 884, "seek": 435052, "start": 4366.52, "end": 4372.400000000001, "text": " In generative models, when you use something like max-pooling, you make it very hard to", "tokens": [51164, 682, 1337, 1166, 5245, 11, 562, 291, 764, 746, 411, 11469, 12, 17374, 278, 11, 291, 652, 309, 588, 1152, 281, 51458], "temperature": 0.0, "avg_logprob": -0.22792942649439762, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.985957275494002e-05}, {"id": 885, "seek": 435052, "start": 4372.400000000001, "end": 4376.0, "text": " undo that and get back the original data.", "tokens": [51458, 23779, 300, 293, 483, 646, 264, 3380, 1412, 13, 51638], "temperature": 0.0, "avg_logprob": -0.22792942649439762, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.985957275494002e-05}, {"id": 886, "seek": 437600, "start": 4377.0, "end": 4383.24, "text": " So if we were to use max-pooling with this idea of our f-content, and we say what does", "tokens": [50414, 407, 498, 321, 645, 281, 764, 11469, 12, 17374, 278, 365, 341, 1558, 295, 527, 283, 12, 9000, 317, 11, 293, 321, 584, 437, 775, 50726], "temperature": 0.0, "avg_logprob": -0.3060089747111003, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.0031726802699267864}, {"id": 887, "seek": 437600, "start": 4383.24, "end": 4389.6, "text": " the 4th layer of activations look like, if we've used max-pooling, then we don't really", "tokens": [50726, 264, 1017, 392, 4583, 295, 2430, 763, 574, 411, 11, 498, 321, 600, 1143, 11469, 12, 17374, 278, 11, 550, 321, 500, 380, 534, 51044], "temperature": 0.0, "avg_logprob": -0.3060089747111003, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.0031726802699267864}, {"id": 888, "seek": 437600, "start": 4389.6, "end": 4394.72, "text": " know what 3 quarters of the data look like.", "tokens": [51044, 458, 437, 805, 20612, 295, 264, 1412, 574, 411, 13, 51300], "temperature": 0.0, "avg_logprob": -0.3060089747111003, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.0031726802699267864}, {"id": 889, "seek": 437600, "start": 4394.72, "end": 4398.96, "text": " Slightly better is to use average-pooling instead of max-pooling.", "tokens": [51300, 318, 44872, 1101, 307, 281, 764, 4274, 12, 17374, 278, 2602, 295, 11469, 12, 17374, 278, 13, 51512], "temperature": 0.0, "avg_logprob": -0.3060089747111003, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.0031726802699267864}, {"id": 890, "seek": 437600, "start": 4398.96, "end": 4403.88, "text": " At least with average-pooling, we're using all of the data to create that average.", "tokens": [51512, 1711, 1935, 365, 4274, 12, 17374, 278, 11, 321, 434, 1228, 439, 295, 264, 1412, 281, 1884, 300, 4274, 13, 51758], "temperature": 0.0, "avg_logprob": -0.3060089747111003, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.0031726802699267864}, {"id": 891, "seek": 440388, "start": 4403.88, "end": 4409.4800000000005, "text": " We've still kind of thrown away 3 quarters of it, but at least it's all been incorporated", "tokens": [50364, 492, 600, 920, 733, 295, 11732, 1314, 805, 20612, 295, 309, 11, 457, 412, 1935, 309, 311, 439, 668, 21654, 50644], "temperature": 0.0, "avg_logprob": -0.2572660792957653, "compression_ratio": 1.568, "no_speech_prob": 0.03021419793367386}, {"id": 892, "seek": 440388, "start": 4409.4800000000005, "end": 4411.8, "text": " into calculating that average.", "tokens": [50644, 666, 28258, 300, 4274, 13, 50760], "temperature": 0.0, "avg_logprob": -0.2572660792957653, "compression_ratio": 1.568, "no_speech_prob": 0.03021419793367386}, {"id": 893, "seek": 440388, "start": 4411.8, "end": 4418.64, "text": " So the only thing I did to turn VGG16 into VGG16-average was to do a search-and-replace", "tokens": [50760, 407, 264, 787, 551, 286, 630, 281, 1261, 691, 27561, 6866, 666, 691, 27561, 6866, 12, 64, 3623, 390, 281, 360, 257, 3164, 12, 474, 12, 265, 6742, 51102], "temperature": 0.0, "avg_logprob": -0.2572660792957653, "compression_ratio": 1.568, "no_speech_prob": 0.03021419793367386}, {"id": 894, "seek": 440388, "start": 4418.64, "end": 4422.6, "text": " in that file from max-pooling to average-pooling.", "tokens": [51102, 294, 300, 3991, 490, 11469, 12, 17374, 278, 281, 4274, 12, 17374, 278, 13, 51300], "temperature": 0.0, "avg_logprob": -0.2572660792957653, "compression_ratio": 1.568, "no_speech_prob": 0.03021419793367386}, {"id": 895, "seek": 440388, "start": 4422.6, "end": 4426.4400000000005, "text": " And it's just going to give us some slightly smoother, slightly nicer results.", "tokens": [51300, 400, 309, 311, 445, 516, 281, 976, 505, 512, 4748, 28640, 11, 4748, 22842, 3542, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2572660792957653, "compression_ratio": 1.568, "no_speech_prob": 0.03021419793367386}, {"id": 896, "seek": 440388, "start": 4426.4400000000005, "end": 4429.16, "text": " You're going to see this a lot with generative models.", "tokens": [51492, 509, 434, 516, 281, 536, 341, 257, 688, 365, 1337, 1166, 5245, 13, 51628], "temperature": 0.0, "avg_logprob": -0.2572660792957653, "compression_ratio": 1.568, "no_speech_prob": 0.03021419793367386}, {"id": 897, "seek": 442916, "start": 4429.16, "end": 4435.92, "text": " We do little tweaks just to try to lose as little information as possible.", "tokens": [50364, 492, 360, 707, 46664, 445, 281, 853, 281, 3624, 382, 707, 1589, 382, 1944, 13, 50702], "temperature": 0.0, "avg_logprob": -0.3817420810848087, "compression_ratio": 1.416243654822335, "no_speech_prob": 0.03114357776939869}, {"id": 898, "seek": 442916, "start": 4435.92, "end": 4437.92, "text": " You can just think of this as VGG16.", "tokens": [50702, 509, 393, 445, 519, 295, 341, 382, 691, 27561, 6866, 13, 50802], "temperature": 0.0, "avg_logprob": -0.3817420810848087, "compression_ratio": 1.416243654822335, "no_speech_prob": 0.03114357776939869}, {"id": 899, "seek": 442916, "start": 4437.92, "end": 4438.92, "text": " Question 2", "tokens": [50802, 14464, 568, 50852], "temperature": 0.0, "avg_logprob": -0.3817420810848087, "compression_ratio": 1.416243654822335, "no_speech_prob": 0.03114357776939869}, {"id": 900, "seek": 442916, "start": 4438.92, "end": 4443.68, "text": " Shouldn't we use something like ResNet instead of VGG since the residual blocks carry more", "tokens": [50852, 34170, 380, 321, 764, 746, 411, 5015, 31890, 2602, 295, 691, 27561, 1670, 264, 27980, 8474, 3985, 544, 51090], "temperature": 0.0, "avg_logprob": -0.3817420810848087, "compression_ratio": 1.416243654822335, "no_speech_prob": 0.03114357776939869}, {"id": 901, "seek": 442916, "start": 4443.68, "end": 4444.68, "text": " context?", "tokens": [51090, 4319, 30, 51140], "temperature": 0.0, "avg_logprob": -0.3817420810848087, "compression_ratio": 1.416243654822335, "no_speech_prob": 0.03114357776939869}, {"id": 902, "seek": 442916, "start": 4444.68, "end": 4451.12, "text": " Answer We'll look at using ResNet over the coming", "tokens": [51140, 24545, 492, 603, 574, 412, 1228, 5015, 31890, 670, 264, 1348, 51462], "temperature": 0.0, "avg_logprob": -0.3817420810848087, "compression_ratio": 1.416243654822335, "no_speech_prob": 0.03114357776939869}, {"id": 903, "seek": 442916, "start": 4451.12, "end": 4454.4, "text": " weeks.", "tokens": [51462, 3259, 13, 51626], "temperature": 0.0, "avg_logprob": -0.3817420810848087, "compression_ratio": 1.416243654822335, "no_speech_prob": 0.03114357776939869}, {"id": 904, "seek": 445440, "start": 4454.4, "end": 4465.839999999999, "text": " It's a lot harder to use ResNet for anything beyond basic classification for a number of", "tokens": [50364, 467, 311, 257, 688, 6081, 281, 764, 5015, 31890, 337, 1340, 4399, 3875, 21538, 337, 257, 1230, 295, 50936], "temperature": 0.0, "avg_logprob": -0.31405852561773256, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03621958568692207}, {"id": 905, "seek": 445440, "start": 4465.839999999999, "end": 4467.5199999999995, "text": " reasons.", "tokens": [50936, 4112, 13, 51020], "temperature": 0.0, "avg_logprob": -0.31405852561773256, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03621958568692207}, {"id": 906, "seek": 445440, "start": 4467.5199999999995, "end": 4471.96, "text": " One is that just the structure of ResNet blocks is much more complex.", "tokens": [51020, 1485, 307, 300, 445, 264, 3877, 295, 5015, 31890, 8474, 307, 709, 544, 3997, 13, 51242], "temperature": 0.0, "avg_logprob": -0.31405852561773256, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03621958568692207}, {"id": 907, "seek": 445440, "start": 4471.96, "end": 4475.879999999999, "text": " If you're not careful, you're going to end up picking something that's on one of those", "tokens": [51242, 759, 291, 434, 406, 5026, 11, 291, 434, 516, 281, 917, 493, 8867, 746, 300, 311, 322, 472, 295, 729, 51438], "temperature": 0.0, "avg_logprob": -0.31405852561773256, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03621958568692207}, {"id": 908, "seek": 445440, "start": 4475.879999999999, "end": 4481.679999999999, "text": " little arms of the ResNet rather than one of the additive mergers of the ResNet.", "tokens": [51438, 707, 5812, 295, 264, 5015, 31890, 2831, 813, 472, 295, 264, 45558, 3551, 9458, 295, 264, 5015, 31890, 13, 51728], "temperature": 0.0, "avg_logprob": -0.31405852561773256, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.03621958568692207}, {"id": 909, "seek": 448168, "start": 4481.68, "end": 4485.76, "text": " It's not going to give you any meaningful information.", "tokens": [50364, 467, 311, 406, 516, 281, 976, 291, 604, 10995, 1589, 13, 50568], "temperature": 0.0, "avg_logprob": -0.28771630252700253, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.004399396944791079}, {"id": 910, "seek": 448168, "start": 4485.76, "end": 4491.16, "text": " You also have to be careful because the ResNet blocks most of the time are just slightly", "tokens": [50568, 509, 611, 362, 281, 312, 5026, 570, 264, 5015, 31890, 8474, 881, 295, 264, 565, 366, 445, 4748, 50838], "temperature": 0.0, "avg_logprob": -0.28771630252700253, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.004399396944791079}, {"id": 911, "seek": 448168, "start": 4491.16, "end": 4496.26, "text": " fine-tuning their previous block, adding the residuals.", "tokens": [50838, 2489, 12, 83, 37726, 641, 3894, 3461, 11, 5127, 264, 27980, 82, 13, 51093], "temperature": 0.0, "avg_logprob": -0.28771630252700253, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.004399396944791079}, {"id": 912, "seek": 448168, "start": 4496.26, "end": 4503.360000000001, "text": " It's not really adding new types of information.", "tokens": [51093, 467, 311, 406, 534, 5127, 777, 3467, 295, 1589, 13, 51448], "temperature": 0.0, "avg_logprob": -0.28771630252700253, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.004399396944791079}, {"id": 913, "seek": 448168, "start": 4503.360000000001, "end": 4511.0, "text": " Honestly the truth is I haven't seen any good research at all about where to use ResNet", "tokens": [51448, 12348, 264, 3494, 307, 286, 2378, 380, 1612, 604, 665, 2132, 412, 439, 466, 689, 281, 764, 5015, 31890, 51830], "temperature": 0.0, "avg_logprob": -0.28771630252700253, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.004399396944791079}, {"id": 914, "seek": 451100, "start": 4511.0, "end": 4517.16, "text": " or Inception architectures for things like generative models or for transfer learning", "tokens": [50364, 420, 682, 7311, 6331, 1303, 337, 721, 411, 1337, 1166, 5245, 420, 337, 5003, 2539, 50672], "temperature": 0.0, "avg_logprob": -0.31507734457651776, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.034100279211997986}, {"id": 915, "seek": 451100, "start": 4517.16, "end": 4518.16, "text": " or anything like that.", "tokens": [50672, 420, 1340, 411, 300, 13, 50722], "temperature": 0.0, "avg_logprob": -0.31507734457651776, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.034100279211997986}, {"id": 916, "seek": 451100, "start": 4518.16, "end": 4520.76, "text": " So we're going to be trying to look at some of that stuff in this course, but it's far", "tokens": [50722, 407, 321, 434, 516, 281, 312, 1382, 281, 574, 412, 512, 295, 300, 1507, 294, 341, 1164, 11, 457, 309, 311, 1400, 50852], "temperature": 0.0, "avg_logprob": -0.31507734457651776, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.034100279211997986}, {"id": 917, "seek": 451100, "start": 4520.76, "end": 4521.76, "text": " from straightforward.", "tokens": [50852, 490, 15325, 13, 50902], "temperature": 0.0, "avg_logprob": -0.31507734457651776, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.034100279211997986}, {"id": 918, "seek": 451100, "start": 4521.76, "end": 4522.76, "text": " Question 3", "tokens": [50902, 14464, 805, 50952], "temperature": 0.0, "avg_logprob": -0.31507734457651776, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.034100279211997986}, {"id": 919, "seek": 451100, "start": 4522.76, "end": 4525.76, "text": " Should we put in batch normalization?", "tokens": [50952, 6454, 321, 829, 294, 15245, 2710, 2144, 30, 51102], "temperature": 0.0, "avg_logprob": -0.31507734457651776, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.034100279211997986}, {"id": 920, "seek": 451100, "start": 4525.76, "end": 4534.8, "text": " Answer In Part 1 of the course, I never actually added", "tokens": [51102, 24545, 682, 4100, 502, 295, 264, 1164, 11, 286, 1128, 767, 3869, 51554], "temperature": 0.0, "avg_logprob": -0.31507734457651776, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.034100279211997986}, {"id": 921, "seek": 451100, "start": 4534.8, "end": 4538.04, "text": " batch norm to the convolutional part of the model.", "tokens": [51554, 15245, 2026, 281, 264, 45216, 304, 644, 295, 264, 2316, 13, 51716], "temperature": 0.0, "avg_logprob": -0.31507734457651776, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.034100279211997986}, {"id": 922, "seek": 453804, "start": 4538.04, "end": 4543.68, "text": " So that's kind of irrelevant because we're not using any fully connected layers.", "tokens": [50364, 407, 300, 311, 733, 295, 28682, 570, 321, 434, 406, 1228, 604, 4498, 4582, 7914, 13, 50646], "temperature": 0.0, "avg_logprob": -0.35341663360595704, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.10970265418291092}, {"id": 923, "seek": 453804, "start": 4543.68, "end": 4547.4, "text": " More generally, is batch norm helpful for generative models?", "tokens": [50646, 5048, 5101, 11, 307, 15245, 2026, 4961, 337, 1337, 1166, 5245, 30, 50832], "temperature": 0.0, "avg_logprob": -0.35341663360595704, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.10970265418291092}, {"id": 924, "seek": 453804, "start": 4547.4, "end": 4550.36, "text": " I'm not sure that we have a great answer to that.", "tokens": [50832, 286, 478, 406, 988, 300, 321, 362, 257, 869, 1867, 281, 300, 13, 50980], "temperature": 0.0, "avg_logprob": -0.35341663360595704, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.10970265418291092}, {"id": 925, "seek": 453804, "start": 4550.36, "end": 4551.36, "text": " Try it.", "tokens": [50980, 6526, 309, 13, 51030], "temperature": 0.0, "avg_logprob": -0.35341663360595704, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.10970265418291092}, {"id": 926, "seek": 453804, "start": 4551.36, "end": 4552.36, "text": " Question 4", "tokens": [51030, 14464, 1017, 51080], "temperature": 0.0, "avg_logprob": -0.35341663360595704, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.10970265418291092}, {"id": 927, "seek": 453804, "start": 4552.36, "end": 4557.12, "text": " Will the pre-trained weights change if we're using average pooling instead of max pooling?", "tokens": [51080, 3099, 264, 659, 12, 17227, 2001, 17443, 1319, 498, 321, 434, 1228, 4274, 7005, 278, 2602, 295, 11469, 7005, 278, 30, 51318], "temperature": 0.0, "avg_logprob": -0.35341663360595704, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.10970265418291092}, {"id": 928, "seek": 453804, "start": 4557.12, "end": 4561.4, "text": " Answer That's a great question.", "tokens": [51318, 24545, 663, 311, 257, 869, 1168, 13, 51532], "temperature": 0.0, "avg_logprob": -0.35341663360595704, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.10970265418291092}, {"id": 929, "seek": 456140, "start": 4561.4, "end": 4567.28, "text": " The pre-trained weights, clearly the optimal weights would change.", "tokens": [50364, 440, 659, 12, 17227, 2001, 17443, 11, 4448, 264, 16252, 17443, 576, 1319, 13, 50658], "temperature": 0.0, "avg_logprob": -0.2645447205524055, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.00482965586706996}, {"id": 930, "seek": 456140, "start": 4567.28, "end": 4573.139999999999, "text": " But having said that, it's still going to do a reasonable job without tweaking the weights", "tokens": [50658, 583, 1419, 848, 300, 11, 309, 311, 920, 516, 281, 360, 257, 10585, 1691, 1553, 6986, 2456, 264, 17443, 50951], "temperature": 0.0, "avg_logprob": -0.2645447205524055, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.00482965586706996}, {"id": 931, "seek": 456140, "start": 4573.139999999999, "end": 4579.28, "text": " because the relationships between the activations isn't going to change.", "tokens": [50951, 570, 264, 6159, 1296, 264, 2430, 763, 1943, 380, 516, 281, 1319, 13, 51258], "temperature": 0.0, "avg_logprob": -0.2645447205524055, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.00482965586706996}, {"id": 932, "seek": 456140, "start": 4579.28, "end": 4585.08, "text": " So again, this would be an interesting thing to try if you want to download ImageNet and", "tokens": [51258, 407, 797, 11, 341, 576, 312, 364, 1880, 551, 281, 853, 498, 291, 528, 281, 5484, 29903, 31890, 293, 51548], "temperature": 0.0, "avg_logprob": -0.2645447205524055, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.00482965586706996}, {"id": 933, "seek": 456140, "start": 4585.08, "end": 4589.92, "text": " try fine-tuning it with average pooling, see if you can actually see a difference in the", "tokens": [51548, 853, 2489, 12, 83, 37726, 309, 365, 4274, 7005, 278, 11, 536, 498, 291, 393, 767, 536, 257, 2649, 294, 264, 51790], "temperature": 0.0, "avg_logprob": -0.2645447205524055, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.00482965586706996}, {"id": 934, "seek": 458992, "start": 4589.92, "end": 4591.92, "text": " outputs that come out or not.", "tokens": [50364, 23930, 300, 808, 484, 420, 406, 13, 50464], "temperature": 0.0, "avg_logprob": -0.3518135452270508, "compression_ratio": 1.7540106951871657, "no_speech_prob": 0.09137868881225586}, {"id": 935, "seek": 458992, "start": 4591.92, "end": 4596.24, "text": " It's not something I've tried.", "tokens": [50464, 467, 311, 406, 746, 286, 600, 3031, 13, 50680], "temperature": 0.0, "avg_logprob": -0.3518135452270508, "compression_ratio": 1.7540106951871657, "no_speech_prob": 0.09137868881225586}, {"id": 936, "seek": 458992, "start": 4596.24, "end": 4604.68, "text": " So here is the output tensor of one of the late layers of VGG16.", "tokens": [50680, 407, 510, 307, 264, 5598, 40863, 295, 472, 295, 264, 3469, 7914, 295, 691, 27561, 6866, 13, 51102], "temperature": 0.0, "avg_logprob": -0.3518135452270508, "compression_ratio": 1.7540106951871657, "no_speech_prob": 0.09137868881225586}, {"id": 937, "seek": 458992, "start": 4604.68, "end": 4612.6, "text": " So if you remember, there are different blocks of VGG where there's a number of 3x3 cons", "tokens": [51102, 407, 498, 291, 1604, 11, 456, 366, 819, 8474, 295, 691, 27561, 689, 456, 311, 257, 1230, 295, 805, 87, 18, 1014, 51498], "temperature": 0.0, "avg_logprob": -0.3518135452270508, "compression_ratio": 1.7540106951871657, "no_speech_prob": 0.09137868881225586}, {"id": 938, "seek": 458992, "start": 4612.6, "end": 4616.8, "text": " in a row, and then there's a pooling layer, and then there's another block of 3x3 cons,", "tokens": [51498, 294, 257, 5386, 11, 293, 550, 456, 311, 257, 7005, 278, 4583, 11, 293, 550, 456, 311, 1071, 3461, 295, 805, 87, 18, 1014, 11, 51708], "temperature": 0.0, "avg_logprob": -0.3518135452270508, "compression_ratio": 1.7540106951871657, "no_speech_prob": 0.09137868881225586}, {"id": 939, "seek": 458992, "start": 4616.8, "end": 4618.6, "text": " and then a pooling layer.", "tokens": [51708, 293, 550, 257, 7005, 278, 4583, 13, 51798], "temperature": 0.0, "avg_logprob": -0.3518135452270508, "compression_ratio": 1.7540106951871657, "no_speech_prob": 0.09137868881225586}, {"id": 940, "seek": 461860, "start": 4618.6, "end": 4624.160000000001, "text": " This is the last block of the cons layers, and this is the first conv of that block.", "tokens": [50364, 639, 307, 264, 1036, 3461, 295, 264, 1014, 7914, 11, 293, 341, 307, 264, 700, 3754, 295, 300, 3461, 13, 50642], "temperature": 0.0, "avg_logprob": -0.3543282664099405, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0020190079230815172}, {"id": 941, "seek": 461860, "start": 4624.160000000001, "end": 4629.68, "text": " I think this is maybe like the third-last layer of the convolutional section of VGG.", "tokens": [50642, 286, 519, 341, 307, 1310, 411, 264, 2636, 12, 15459, 4583, 295, 264, 45216, 304, 3541, 295, 691, 27561, 13, 50918], "temperature": 0.0, "avg_logprob": -0.3543282664099405, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0020190079230815172}, {"id": 942, "seek": 461860, "start": 4629.68, "end": 4638.72, "text": " This is kind of like large receptive field, very complex concepts being captured at this", "tokens": [50918, 639, 307, 733, 295, 411, 2416, 45838, 2519, 11, 588, 3997, 10392, 885, 11828, 412, 341, 51370], "temperature": 0.0, "avg_logprob": -0.3543282664099405, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0020190079230815172}, {"id": 943, "seek": 461860, "start": 4638.72, "end": 4640.92, "text": " late stage.", "tokens": [51370, 3469, 3233, 13, 51480], "temperature": 0.0, "avg_logprob": -0.3543282664099405, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0020190079230815172}, {"id": 944, "seek": 461860, "start": 4640.92, "end": 4647.6, "text": " So what we're going to do is we need to create our target.", "tokens": [51480, 407, 437, 321, 434, 516, 281, 360, 307, 321, 643, 281, 1884, 527, 3779, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3543282664099405, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0020190079230815172}, {"id": 945, "seek": 464760, "start": 4647.6, "end": 4661.120000000001, "text": " So for our bird, when we put that bird through VGG, what is the value of that layer's activations?", "tokens": [50364, 407, 337, 527, 5255, 11, 562, 321, 829, 300, 5255, 807, 691, 27561, 11, 437, 307, 264, 2158, 295, 300, 4583, 311, 2430, 763, 30, 51040], "temperature": 0.0, "avg_logprob": -0.23079432716852502, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.922176544321701e-05}, {"id": 946, "seek": 464760, "start": 4661.120000000001, "end": 4666.360000000001, "text": " One of the things I suggested you revise was the stuff from the Keras FAQ about how to", "tokens": [51040, 1485, 295, 264, 721, 286, 10945, 291, 44252, 390, 264, 1507, 490, 264, 591, 6985, 19894, 48, 466, 577, 281, 51302], "temperature": 0.0, "avg_logprob": -0.23079432716852502, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.922176544321701e-05}, {"id": 947, "seek": 464760, "start": 4666.360000000001, "end": 4669.64, "text": " get layer outputs.", "tokens": [51302, 483, 4583, 23930, 13, 51466], "temperature": 0.0, "avg_logprob": -0.23079432716852502, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.922176544321701e-05}, {"id": 948, "seek": 464760, "start": 4669.64, "end": 4675.320000000001, "text": " One simple way to do that is to create a new model which takes our model's input as input", "tokens": [51466, 1485, 2199, 636, 281, 360, 300, 307, 281, 1884, 257, 777, 2316, 597, 2516, 527, 2316, 311, 4846, 382, 4846, 51750], "temperature": 0.0, "avg_logprob": -0.23079432716852502, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.922176544321701e-05}, {"id": 949, "seek": 467532, "start": 4675.719999999999, "end": 4680.88, "text": " and instead of using the final output as output, we can use this layer as output.", "tokens": [50384, 293, 2602, 295, 1228, 264, 2572, 5598, 382, 5598, 11, 321, 393, 764, 341, 4583, 382, 5598, 13, 50642], "temperature": 0.0, "avg_logprob": -0.2527424882097942, "compression_ratio": 1.6312849162011174, "no_speech_prob": 8.220181916840374e-05}, {"id": 950, "seek": 467532, "start": 4680.88, "end": 4690.04, "text": " This is now a model which when we call .predict, it will return this set of activations.", "tokens": [50642, 639, 307, 586, 257, 2316, 597, 562, 321, 818, 2411, 79, 24945, 11, 309, 486, 2736, 341, 992, 295, 2430, 763, 13, 51100], "temperature": 0.0, "avg_logprob": -0.2527424882097942, "compression_ratio": 1.6312849162011174, "no_speech_prob": 8.220181916840374e-05}, {"id": 951, "seek": 467532, "start": 4690.04, "end": 4693.2, "text": " So that's all we've done here.", "tokens": [51100, 407, 300, 311, 439, 321, 600, 1096, 510, 13, 51258], "temperature": 0.0, "avg_logprob": -0.2527424882097942, "compression_ratio": 1.6312849162011174, "no_speech_prob": 8.220181916840374e-05}, {"id": 952, "seek": 467532, "start": 4693.2, "end": 4701.4, "text": " Now we're going to be using this inside the GPU, we're going to be using this as a target.", "tokens": [51258, 823, 321, 434, 516, 281, 312, 1228, 341, 1854, 264, 18407, 11, 321, 434, 516, 281, 312, 1228, 341, 382, 257, 3779, 13, 51668], "temperature": 0.0, "avg_logprob": -0.2527424882097942, "compression_ratio": 1.6312849162011174, "no_speech_prob": 8.220181916840374e-05}, {"id": 953, "seek": 470140, "start": 4701.4, "end": 4707.7, "text": " So to give us something which is going to live in the GPU, A, and B, we can use symbolically", "tokens": [50364, 407, 281, 976, 505, 746, 597, 307, 516, 281, 1621, 294, 264, 18407, 11, 316, 11, 293, 363, 11, 321, 393, 764, 5986, 984, 50679], "temperature": 0.0, "avg_logprob": -0.33495714447715064, "compression_ratio": 1.4240837696335078, "no_speech_prob": 0.0012065855553373694}, {"id": 954, "seek": 470140, "start": 4707.7, "end": 4715.12, "text": " in a computation graph, B, we wrap it with K.variable.", "tokens": [50679, 294, 257, 24903, 4295, 11, 363, 11, 321, 7019, 309, 365, 591, 13, 34033, 712, 13, 51050], "temperature": 0.0, "avg_logprob": -0.33495714447715064, "compression_ratio": 1.4240837696335078, "no_speech_prob": 0.0012065855553373694}, {"id": 955, "seek": 470140, "start": 4715.12, "end": 4725.44, "text": " To remind you, K, so whatever Keras in the docs, they use the Keras.backend module, they", "tokens": [51050, 1407, 4160, 291, 11, 591, 11, 370, 2035, 591, 6985, 294, 264, 45623, 11, 436, 764, 264, 591, 6985, 13, 3207, 521, 10088, 11, 436, 51566], "temperature": 0.0, "avg_logprob": -0.33495714447715064, "compression_ratio": 1.4240837696335078, "no_speech_prob": 0.0012065855553373694}, {"id": 956, "seek": 470140, "start": 4725.44, "end": 4727.16, "text": " always call it K.", "tokens": [51566, 1009, 818, 309, 591, 13, 51652], "temperature": 0.0, "avg_logprob": -0.33495714447715064, "compression_ratio": 1.4240837696335078, "no_speech_prob": 0.0012065855553373694}, {"id": 957, "seek": 470140, "start": 4727.16, "end": 4728.599999999999, "text": " I don't know why.", "tokens": [51652, 286, 500, 380, 458, 983, 13, 51724], "temperature": 0.0, "avg_logprob": -0.33495714447715064, "compression_ratio": 1.4240837696335078, "no_speech_prob": 0.0012065855553373694}, {"id": 958, "seek": 472860, "start": 4728.6, "end": 4737.200000000001, "text": " So K refers to the API that Keras provides which provides a way of talking to either", "tokens": [50364, 407, 591, 14942, 281, 264, 9362, 300, 591, 6985, 6417, 597, 6417, 257, 636, 295, 1417, 281, 2139, 50794], "temperature": 0.0, "avg_logprob": -0.27565140322030307, "compression_ratio": 1.68, "no_speech_prob": 0.002115695271641016}, {"id": 959, "seek": 472860, "start": 4737.200000000001, "end": 4740.88, "text": " Theano or TensorFlow with the same API.", "tokens": [50794, 440, 3730, 420, 37624, 365, 264, 912, 9362, 13, 50978], "temperature": 0.0, "avg_logprob": -0.27565140322030307, "compression_ratio": 1.68, "no_speech_prob": 0.002115695271641016}, {"id": 960, "seek": 472860, "start": 4740.88, "end": 4748.04, "text": " So both Theano and TensorFlow have a concept of variables and placeholders and dot functions", "tokens": [50978, 407, 1293, 440, 3730, 293, 37624, 362, 257, 3410, 295, 9102, 293, 1081, 12916, 293, 5893, 6828, 51336], "temperature": 0.0, "avg_logprob": -0.27565140322030307, "compression_ratio": 1.68, "no_speech_prob": 0.002115695271641016}, {"id": 961, "seek": 472860, "start": 4748.04, "end": 4751.72, "text": " and subtraction functions and softmax activations and so forth.", "tokens": [51336, 293, 16390, 313, 6828, 293, 2787, 41167, 2430, 763, 293, 370, 5220, 13, 51520], "temperature": 0.0, "avg_logprob": -0.27565140322030307, "compression_ratio": 1.68, "no_speech_prob": 0.002115695271641016}, {"id": 962, "seek": 472860, "start": 4751.72, "end": 4757.4800000000005, "text": " So this K.module is where all of those functions live.", "tokens": [51520, 407, 341, 591, 13, 8014, 2271, 307, 689, 439, 295, 729, 6828, 1621, 13, 51808], "temperature": 0.0, "avg_logprob": -0.27565140322030307, "compression_ratio": 1.68, "no_speech_prob": 0.002115695271641016}, {"id": 963, "seek": 475748, "start": 4757.48, "end": 4761.24, "text": " This is just a way of creating a variable, which if we're using Theano, it will create", "tokens": [50364, 639, 307, 445, 257, 636, 295, 4084, 257, 7006, 11, 597, 498, 321, 434, 1228, 440, 3730, 11, 309, 486, 1884, 50552], "temperature": 0.0, "avg_logprob": -0.31647754752117657, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.004070129711180925}, {"id": 964, "seek": 475748, "start": 4761.24, "end": 4762.24, "text": " a Theano variable.", "tokens": [50552, 257, 440, 3730, 7006, 13, 50602], "temperature": 0.0, "avg_logprob": -0.31647754752117657, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.004070129711180925}, {"id": 965, "seek": 475748, "start": 4762.24, "end": 4766.759999999999, "text": " If we're using TensorFlow, it creates a TensorFlow variable.", "tokens": [50602, 759, 321, 434, 1228, 37624, 11, 309, 7829, 257, 37624, 7006, 13, 50828], "temperature": 0.0, "avg_logprob": -0.31647754752117657, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.004070129711180925}, {"id": 966, "seek": 475748, "start": 4766.759999999999, "end": 4772.04, "text": " Where possible, I'm trying to use this rather than TensorFlow directly.", "tokens": [50828, 2305, 1944, 11, 286, 478, 1382, 281, 764, 341, 2831, 813, 37624, 3838, 13, 51092], "temperature": 0.0, "avg_logprob": -0.31647754752117657, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.004070129711180925}, {"id": 967, "seek": 475748, "start": 4772.04, "end": 4776.879999999999, "text": " But I could have absolutely said tf.variable and it would work just as well because we're", "tokens": [51092, 583, 286, 727, 362, 3122, 848, 256, 69, 13, 34033, 712, 293, 309, 576, 589, 445, 382, 731, 570, 321, 434, 51334], "temperature": 0.0, "avg_logprob": -0.31647754752117657, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.004070129711180925}, {"id": 968, "seek": 475748, "start": 4776.879999999999, "end": 4781.12, "text": " using the TensorFlow backend.", "tokens": [51334, 1228, 264, 37624, 38087, 13, 51546], "temperature": 0.0, "avg_logprob": -0.31647754752117657, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.004070129711180925}, {"id": 969, "seek": 478112, "start": 4781.12, "end": 4789.38, "text": " So this has now created a symbolic variable that contains the activations of block5.conv1.", "tokens": [50364, 407, 341, 575, 586, 2942, 257, 25755, 7006, 300, 8306, 264, 2430, 763, 295, 3461, 20, 13, 1671, 85, 16, 13, 50777], "temperature": 0.0, "avg_logprob": -0.2939293108488384, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.008187880739569664}, {"id": 970, "seek": 478112, "start": 4789.38, "end": 4797.98, "text": " So what we now want to do is to generate an image which we're going to use SGD to gradually", "tokens": [50777, 407, 437, 321, 586, 528, 281, 360, 307, 281, 8460, 364, 3256, 597, 321, 434, 516, 281, 764, 34520, 35, 281, 13145, 51207], "temperature": 0.0, "avg_logprob": -0.2939293108488384, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.008187880739569664}, {"id": 971, "seek": 478112, "start": 4797.98, "end": 4804.34, "text": " make the activations of that image look more and more like this variable.", "tokens": [51207, 652, 264, 2430, 763, 295, 300, 3256, 574, 544, 293, 544, 411, 341, 7006, 13, 51525], "temperature": 0.0, "avg_logprob": -0.2939293108488384, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.008187880739569664}, {"id": 972, "seek": 478112, "start": 4804.34, "end": 4805.7, "text": " So how are we going to do that?", "tokens": [51525, 407, 577, 366, 321, 516, 281, 360, 300, 30, 51593], "temperature": 0.0, "avg_logprob": -0.2939293108488384, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.008187880739569664}, {"id": 973, "seek": 478112, "start": 4805.7, "end": 4810.7, "text": " Let's just skip over 202 for a moment and think about some pieces.", "tokens": [51593, 961, 311, 445, 10023, 670, 945, 17, 337, 257, 1623, 293, 519, 466, 512, 3755, 13, 51843], "temperature": 0.0, "avg_logprob": -0.2939293108488384, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.008187880739569664}, {"id": 974, "seek": 481070, "start": 4811.28, "end": 4813.4, "text": " So we're going to need to define a loss function.", "tokens": [50393, 407, 321, 434, 516, 281, 643, 281, 6964, 257, 4470, 2445, 13, 50499], "temperature": 0.0, "avg_logprob": -0.34684620732846466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 5.64979127375409e-05}, {"id": 975, "seek": 481070, "start": 4813.4, "end": 4818.54, "text": " The loss function is just the mean squared error between two things.", "tokens": [50499, 440, 4470, 2445, 307, 445, 264, 914, 8889, 6713, 1296, 732, 721, 13, 50756], "temperature": 0.0, "avg_logprob": -0.34684620732846466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 5.64979127375409e-05}, {"id": 976, "seek": 481070, "start": 4818.54, "end": 4827.96, "text": " One thing is of course that target, that thing we just created, which is the value of our", "tokens": [50756, 1485, 551, 307, 295, 1164, 300, 3779, 11, 300, 551, 321, 445, 2942, 11, 597, 307, 264, 2158, 295, 527, 51227], "temperature": 0.0, "avg_logprob": -0.34684620732846466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 5.64979127375409e-05}, {"id": 977, "seek": 481070, "start": 4827.96, "end": 4832.5199999999995, "text": " layer using the bird image.", "tokens": [51227, 4583, 1228, 264, 5255, 3256, 13, 51455], "temperature": 0.0, "avg_logprob": -0.34684620732846466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 5.64979127375409e-05}, {"id": 978, "seek": 481070, "start": 4832.5199999999995, "end": 4836.76, "text": " We use the bird image array.", "tokens": [51455, 492, 764, 264, 5255, 3256, 10225, 13, 51667], "temperature": 0.0, "avg_logprob": -0.34684620732846466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 5.64979127375409e-05}, {"id": 979, "seek": 481070, "start": 4836.76, "end": 4839.0, "text": " So that's our target.", "tokens": [51667, 407, 300, 311, 527, 3779, 13, 51779], "temperature": 0.0, "avg_logprob": -0.34684620732846466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 5.64979127375409e-05}, {"id": 980, "seek": 481070, "start": 4839.0, "end": 4840.5599999999995, "text": " And then what are we going to get close to that?", "tokens": [51779, 400, 550, 437, 366, 321, 516, 281, 483, 1998, 281, 300, 30, 51857], "temperature": 0.0, "avg_logprob": -0.34684620732846466, "compression_ratio": 1.6470588235294117, "no_speech_prob": 5.64979127375409e-05}, {"id": 981, "seek": 484056, "start": 4840.620000000001, "end": 4849.620000000001, "text": " What we want to get close to that is whatever the value is of that layer at the moment.", "tokens": [50367, 708, 321, 528, 281, 483, 1998, 281, 300, 307, 2035, 264, 2158, 307, 295, 300, 4583, 412, 264, 1623, 13, 50817], "temperature": 0.0, "avg_logprob": -0.35603182828879054, "compression_ratio": 1.545, "no_speech_prob": 1.1478766282380093e-05}, {"id": 982, "seek": 484056, "start": 4849.620000000001, "end": 4850.620000000001, "text": " What is layer equal?", "tokens": [50817, 708, 307, 4583, 2681, 30, 50867], "temperature": 0.0, "avg_logprob": -0.35603182828879054, "compression_ratio": 1.545, "no_speech_prob": 1.1478766282380093e-05}, {"id": 983, "seek": 484056, "start": 4850.620000000001, "end": 4858.38, "text": " So layer is just a symbolic object, it's nothing in it.", "tokens": [50867, 407, 4583, 307, 445, 257, 25755, 2657, 11, 309, 311, 1825, 294, 309, 13, 51255], "temperature": 0.0, "avg_logprob": -0.35603182828879054, "compression_ratio": 1.545, "no_speech_prob": 1.1478766282380093e-05}, {"id": 984, "seek": 484056, "start": 4858.38, "end": 4863.860000000001, "text": " So we're going to have to feed it with data later.", "tokens": [51255, 407, 321, 434, 516, 281, 362, 281, 3154, 309, 365, 1412, 1780, 13, 51529], "temperature": 0.0, "avg_logprob": -0.35603182828879054, "compression_ratio": 1.545, "no_speech_prob": 1.1478766282380093e-05}, {"id": 985, "seek": 484056, "start": 4863.860000000001, "end": 4868.9400000000005, "text": " So remember this is kind of the interesting way you define computation graphs with TensorFlow", "tokens": [51529, 407, 1604, 341, 307, 733, 295, 264, 1880, 636, 291, 6964, 24903, 24877, 365, 37624, 51783], "temperature": 0.0, "avg_logprob": -0.35603182828879054, "compression_ratio": 1.545, "no_speech_prob": 1.1478766282380093e-05}, {"id": 986, "seek": 486894, "start": 4868.96, "end": 4869.96, "text": " and Theano.", "tokens": [50365, 293, 440, 3730, 13, 50415], "temperature": 0.0, "avg_logprob": -0.39440627925652116, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09009163081645966}, {"id": 987, "seek": 486894, "start": 4869.96, "end": 4874.96, "text": " It's like you define it with these symbolic things now and you feed it with data later.", "tokens": [50415, 467, 311, 411, 291, 6964, 309, 365, 613, 25755, 721, 586, 293, 291, 3154, 309, 365, 1412, 1780, 13, 50665], "temperature": 0.0, "avg_logprob": -0.39440627925652116, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09009163081645966}, {"id": 988, "seek": 486894, "start": 4874.96, "end": 4880.759999999999, "text": " So you've got this symbolic thing called layer, and we can't actually calculate this yet.", "tokens": [50665, 407, 291, 600, 658, 341, 25755, 551, 1219, 4583, 11, 293, 321, 393, 380, 767, 8873, 341, 1939, 13, 50955], "temperature": 0.0, "avg_logprob": -0.39440627925652116, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09009163081645966}, {"id": 989, "seek": 486894, "start": 4880.759999999999, "end": 4884.96, "text": " So at this stage, this is just a computation graph we're building.", "tokens": [50955, 407, 412, 341, 3233, 11, 341, 307, 445, 257, 24903, 4295, 321, 434, 2390, 13, 51165], "temperature": 0.0, "avg_logprob": -0.39440627925652116, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09009163081645966}, {"id": 990, "seek": 486894, "start": 4884.96, "end": 4888.96, "text": " Now of course, any time we have a computation graph, we can get its gradients.", "tokens": [51165, 823, 295, 1164, 11, 604, 565, 321, 362, 257, 24903, 4295, 11, 321, 393, 483, 1080, 2771, 2448, 13, 51365], "temperature": 0.0, "avg_logprob": -0.39440627925652116, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09009163081645966}, {"id": 991, "seek": 486894, "start": 4888.96, "end": 4889.96, "text": " Yes, Richard.", "tokens": [51365, 1079, 11, 9809, 13, 51415], "temperature": 0.0, "avg_logprob": -0.39440627925652116, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09009163081645966}, {"id": 992, "seek": 486894, "start": 4889.96, "end": 4890.96, "text": " Question for readability.", "tokens": [51415, 14464, 337, 1401, 2310, 13, 51465], "temperature": 0.0, "avg_logprob": -0.39440627925652116, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09009163081645966}, {"id": 993, "seek": 486894, "start": 4890.96, "end": 4897.96, "text": " Can you scroll down when you're going over code snippets so that they're centered?", "tokens": [51465, 1664, 291, 11369, 760, 562, 291, 434, 516, 670, 3089, 35623, 1385, 370, 300, 436, 434, 18988, 30, 51815], "temperature": 0.0, "avg_logprob": -0.39440627925652116, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09009163081645966}, {"id": 994, "seek": 489796, "start": 4897.9800000000005, "end": 4898.9800000000005, "text": " Yes.", "tokens": [50365, 1079, 13, 50415], "temperature": 0.0, "avg_logprob": -0.2880297457234243, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.00036258931504562497}, {"id": 995, "seek": 489796, "start": 4898.9800000000005, "end": 4900.9800000000005, "text": " Thank you.", "tokens": [50415, 1044, 291, 13, 50515], "temperature": 0.0, "avg_logprob": -0.2880297457234243, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.00036258931504562497}, {"id": 996, "seek": 489796, "start": 4900.9800000000005, "end": 4909.62, "text": " Okay, so now that we have a computation graph that calculates the loss function we're interested", "tokens": [50515, 1033, 11, 370, 586, 300, 321, 362, 257, 24903, 4295, 300, 4322, 1024, 264, 4470, 2445, 321, 434, 3102, 50947], "temperature": 0.0, "avg_logprob": -0.2880297457234243, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.00036258931504562497}, {"id": 997, "seek": 489796, "start": 4909.62, "end": 4912.36, "text": " in, this is fcontent.", "tokens": [50947, 294, 11, 341, 307, 283, 9000, 317, 13, 51084], "temperature": 0.0, "avg_logprob": -0.2880297457234243, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.00036258931504562497}, {"id": 998, "seek": 489796, "start": 4912.36, "end": 4918.74, "text": " If we're going to try to optimize our generated image, we're going to need to know the gradients.", "tokens": [51084, 759, 321, 434, 516, 281, 853, 281, 19719, 527, 10833, 3256, 11, 321, 434, 516, 281, 643, 281, 458, 264, 2771, 2448, 13, 51403], "temperature": 0.0, "avg_logprob": -0.2880297457234243, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.00036258931504562497}, {"id": 999, "seek": 489796, "start": 4918.74, "end": 4923.3, "text": " So here we can get the gradients, and again we use K.gradients rather than TensorFlow", "tokens": [51403, 407, 510, 321, 393, 483, 264, 2771, 2448, 11, 293, 797, 321, 764, 591, 13, 7165, 2448, 2831, 813, 37624, 51631], "temperature": 0.0, "avg_logprob": -0.2880297457234243, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.00036258931504562497}, {"id": 1000, "seek": 492330, "start": 4923.3, "end": 4932.2, "text": " gradients or Theano gradients just so we can use it with any backend we like.", "tokens": [50364, 2771, 2448, 420, 440, 3730, 2771, 2448, 445, 370, 321, 393, 764, 309, 365, 604, 38087, 321, 411, 13, 50809], "temperature": 0.0, "avg_logprob": -0.30175289154052737, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.19930167496204376}, {"id": 1001, "seek": 492330, "start": 4932.2, "end": 4937.0, "text": " The function we're trying to get gradients of is the loss function, which we just calculated.", "tokens": [50809, 440, 2445, 321, 434, 1382, 281, 483, 2771, 2448, 295, 307, 264, 4470, 2445, 11, 597, 321, 445, 15598, 13, 51049], "temperature": 0.0, "avg_logprob": -0.30175289154052737, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.19930167496204376}, {"id": 1002, "seek": 492330, "start": 4937.0, "end": 4942.64, "text": " And then we want it with respect to, not some weights, but with respect to the input of", "tokens": [51049, 400, 550, 321, 528, 309, 365, 3104, 281, 11, 406, 512, 17443, 11, 457, 365, 3104, 281, 264, 4846, 295, 51331], "temperature": 0.0, "avg_logprob": -0.30175289154052737, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.19930167496204376}, {"id": 1003, "seek": 492330, "start": 4942.64, "end": 4943.64, "text": " the model.", "tokens": [51331, 264, 2316, 13, 51381], "temperature": 0.0, "avg_logprob": -0.30175289154052737, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.19930167496204376}, {"id": 1004, "seek": 492330, "start": 4943.64, "end": 4950.22, "text": " So this is the thing that we want to change, is the input to the model so as to minimize", "tokens": [51381, 407, 341, 307, 264, 551, 300, 321, 528, 281, 1319, 11, 307, 264, 4846, 281, 264, 2316, 370, 382, 281, 17522, 51710], "temperature": 0.0, "avg_logprob": -0.30175289154052737, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.19930167496204376}, {"id": 1005, "seek": 492330, "start": 4950.22, "end": 4951.6, "text": " our loss.", "tokens": [51710, 527, 4470, 13, 51779], "temperature": 0.0, "avg_logprob": -0.30175289154052737, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.19930167496204376}, {"id": 1006, "seek": 495160, "start": 4951.620000000001, "end": 4955.54, "text": " So they're the gradients.", "tokens": [50365, 407, 436, 434, 264, 2771, 2448, 13, 50561], "temperature": 0.0, "avg_logprob": -0.2808723025851779, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.00043055674177594483}, {"id": 1007, "seek": 495160, "start": 4955.54, "end": 4959.820000000001, "text": " So now that we've done that, we can go ahead and create our function.", "tokens": [50561, 407, 586, 300, 321, 600, 1096, 300, 11, 321, 393, 352, 2286, 293, 1884, 527, 2445, 13, 50775], "temperature": 0.0, "avg_logprob": -0.2808723025851779, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.00043055674177594483}, {"id": 1008, "seek": 495160, "start": 4959.820000000001, "end": 4965.54, "text": " And so the input to the function is just model.input, and the outputs to the function will be the", "tokens": [50775, 400, 370, 264, 4846, 281, 264, 2445, 307, 445, 2316, 13, 259, 2582, 11, 293, 264, 23930, 281, 264, 2445, 486, 312, 264, 51061], "temperature": 0.0, "avg_logprob": -0.2808723025851779, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.00043055674177594483}, {"id": 1009, "seek": 495160, "start": 4965.54, "end": 4969.740000000001, "text": " loss and the gradients.", "tokens": [51061, 4470, 293, 264, 2771, 2448, 13, 51271], "temperature": 0.0, "avg_logprob": -0.2808723025851779, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.00043055674177594483}, {"id": 1010, "seek": 495160, "start": 4969.740000000001, "end": 4972.58, "text": " So that's nearly everything we need.", "tokens": [51271, 407, 300, 311, 6217, 1203, 321, 643, 13, 51413], "temperature": 0.0, "avg_logprob": -0.2808723025851779, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.00043055674177594483}, {"id": 1011, "seek": 495160, "start": 4972.58, "end": 4977.38, "text": " The last step we need to do is to actually run an optimizer.", "tokens": [51413, 440, 1036, 1823, 321, 643, 281, 360, 307, 281, 767, 1190, 364, 5028, 6545, 13, 51653], "temperature": 0.0, "avg_logprob": -0.2808723025851779, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.00043055674177594483}, {"id": 1012, "seek": 497738, "start": 4978.08, "end": 4983.0, "text": " Normally when we run an optimizer, we use some kind of SGD.", "tokens": [50399, 17424, 562, 321, 1190, 364, 5028, 6545, 11, 321, 764, 512, 733, 295, 34520, 35, 13, 50645], "temperature": 0.0, "avg_logprob": -0.23571650901537264, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.0005527784232981503}, {"id": 1013, "seek": 497738, "start": 4983.0, "end": 4987.84, "text": " Now the S in SGD is for stochastic.", "tokens": [50645, 823, 264, 318, 294, 34520, 35, 307, 337, 342, 8997, 2750, 13, 50887], "temperature": 0.0, "avg_logprob": -0.23571650901537264, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.0005527784232981503}, {"id": 1014, "seek": 497738, "start": 4987.84, "end": 4990.6, "text": " In this case, there's nothing stochastic.", "tokens": [50887, 682, 341, 1389, 11, 456, 311, 1825, 342, 8997, 2750, 13, 51025], "temperature": 0.0, "avg_logprob": -0.23571650901537264, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.0005527784232981503}, {"id": 1015, "seek": 497738, "start": 4990.6, "end": 4996.12, "text": " We're not creating lots of random batches and getting different gradients every time.", "tokens": [51025, 492, 434, 406, 4084, 3195, 295, 4974, 15245, 279, 293, 1242, 819, 2771, 2448, 633, 565, 13, 51301], "temperature": 0.0, "avg_logprob": -0.23571650901537264, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.0005527784232981503}, {"id": 1016, "seek": 497738, "start": 4996.12, "end": 5003.4800000000005, "text": " So why use stochastic gradient descent when we don't have a stochastic problem to solve?", "tokens": [51301, 407, 983, 764, 342, 8997, 2750, 16235, 23475, 562, 321, 500, 380, 362, 257, 342, 8997, 2750, 1154, 281, 5039, 30, 51669], "temperature": 0.0, "avg_logprob": -0.23571650901537264, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.0005527784232981503}, {"id": 1017, "seek": 500348, "start": 5003.54, "end": 5009.66, "text": " In fact, there's a much longer history of optimization methods which are deterministic,", "tokens": [50367, 682, 1186, 11, 456, 311, 257, 709, 2854, 2503, 295, 19618, 7150, 597, 366, 15957, 3142, 11, 50673], "temperature": 0.0, "avg_logprob": -0.3204400950464709, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.0010484680533409119}, {"id": 1018, "seek": 500348, "start": 5009.66, "end": 5020.9, "text": " going back to Newton's method, which many of you will be familiar with.", "tokens": [50673, 516, 646, 281, 19541, 311, 3170, 11, 597, 867, 295, 291, 486, 312, 4963, 365, 13, 51235], "temperature": 0.0, "avg_logprob": -0.3204400950464709, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.0010484680533409119}, {"id": 1019, "seek": 500348, "start": 5020.9, "end": 5030.62, "text": " The basic idea of these much faster deterministic optimization methods is that rather than saying,", "tokens": [51235, 440, 3875, 1558, 295, 613, 709, 4663, 15957, 3142, 19618, 7150, 307, 300, 2831, 813, 1566, 11, 51721], "temperature": 0.0, "avg_logprob": -0.3204400950464709, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.0010484680533409119}, {"id": 1020, "seek": 503062, "start": 5030.84, "end": 5034.12, "text": " Where's the gradient, which direction does it go?", "tokens": [50375, 2305, 311, 264, 16235, 11, 597, 3513, 775, 309, 352, 30, 50539], "temperature": 0.0, "avg_logprob": -0.3109649908347208, "compression_ratio": 1.85546875, "no_speech_prob": 0.005139600019901991}, {"id": 1021, "seek": 503062, "start": 5034.12, "end": 5037.4, "text": " Let's just go a small little step in that direction.", "tokens": [50539, 961, 311, 445, 352, 257, 1359, 707, 1823, 294, 300, 3513, 13, 50703], "temperature": 0.0, "avg_logprob": -0.3109649908347208, "compression_ratio": 1.85546875, "no_speech_prob": 0.005139600019901991}, {"id": 1022, "seek": 503062, "start": 5037.4, "end": 5040.4, "text": " Learning rate times gradient, small little step, small little step.", "tokens": [50703, 15205, 3314, 1413, 16235, 11, 1359, 707, 1823, 11, 1359, 707, 1823, 13, 50853], "temperature": 0.0, "avg_logprob": -0.3109649908347208, "compression_ratio": 1.85546875, "no_speech_prob": 0.005139600019901991}, {"id": 1023, "seek": 503062, "start": 5040.4, "end": 5043.24, "text": " Because I have no idea how far to go.", "tokens": [50853, 1436, 286, 362, 572, 1558, 577, 1400, 281, 352, 13, 50995], "temperature": 0.0, "avg_logprob": -0.3109649908347208, "compression_ratio": 1.85546875, "no_speech_prob": 0.005139600019901991}, {"id": 1024, "seek": 503062, "start": 5043.24, "end": 5045.24, "text": " And it's stochastic, so it's going to keep changing.", "tokens": [50995, 400, 309, 311, 342, 8997, 2750, 11, 370, 309, 311, 516, 281, 1066, 4473, 13, 51095], "temperature": 0.0, "avg_logprob": -0.3109649908347208, "compression_ratio": 1.85546875, "no_speech_prob": 0.005139600019901991}, {"id": 1025, "seek": 503062, "start": 5045.24, "end": 5049.599999999999, "text": " So next time I look it will be a totally different direction.", "tokens": [51095, 407, 958, 565, 286, 574, 309, 486, 312, 257, 3879, 819, 3513, 13, 51313], "temperature": 0.0, "avg_logprob": -0.3109649908347208, "compression_ratio": 1.85546875, "no_speech_prob": 0.005139600019901991}, {"id": 1026, "seek": 503062, "start": 5049.599999999999, "end": 5055.599999999999, "text": " Instead with a deterministic optimization, we find out which direction to go and then", "tokens": [51313, 7156, 365, 257, 15957, 3142, 19618, 11, 321, 915, 484, 597, 3513, 281, 352, 293, 550, 51613], "temperature": 0.0, "avg_logprob": -0.3109649908347208, "compression_ratio": 1.85546875, "no_speech_prob": 0.005139600019901991}, {"id": 1027, "seek": 503062, "start": 5055.599999999999, "end": 5059.92, "text": " we find out what is the optimum distance to go in that direction.", "tokens": [51613, 321, 915, 484, 437, 307, 264, 39326, 4560, 281, 352, 294, 300, 3513, 13, 51829], "temperature": 0.0, "avg_logprob": -0.3109649908347208, "compression_ratio": 1.85546875, "no_speech_prob": 0.005139600019901991}, {"id": 1028, "seek": 505992, "start": 5060.22, "end": 5065.1, "text": " If you know this is the direction I want to go and it looks like this, then the way we", "tokens": [50379, 759, 291, 458, 341, 307, 264, 3513, 286, 528, 281, 352, 293, 309, 1542, 411, 341, 11, 550, 264, 636, 321, 50623], "temperature": 0.0, "avg_logprob": -0.298282711322491, "compression_ratio": 1.8911290322580645, "no_speech_prob": 0.0016229657921940088}, {"id": 1029, "seek": 505992, "start": 5065.1, "end": 5067.46, "text": " find the optimum is we go a small distance.", "tokens": [50623, 915, 264, 39326, 307, 321, 352, 257, 1359, 4560, 13, 50741], "temperature": 0.0, "avg_logprob": -0.298282711322491, "compression_ratio": 1.8911290322580645, "no_speech_prob": 0.0016229657921940088}, {"id": 1030, "seek": 505992, "start": 5067.46, "end": 5072.3, "text": " Then we go twice as far as that, twice as far as that, twice as far as that.", "tokens": [50741, 1396, 321, 352, 6091, 382, 1400, 382, 300, 11, 6091, 382, 1400, 382, 300, 11, 6091, 382, 1400, 382, 300, 13, 50983], "temperature": 0.0, "avg_logprob": -0.298282711322491, "compression_ratio": 1.8911290322580645, "no_speech_prob": 0.0016229657921940088}, {"id": 1031, "seek": 505992, "start": 5072.3, "end": 5076.9800000000005, "text": " We keep going until the slope changes sign.", "tokens": [50983, 492, 1066, 516, 1826, 264, 13525, 2962, 1465, 13, 51217], "temperature": 0.0, "avg_logprob": -0.298282711322491, "compression_ratio": 1.8911290322580645, "no_speech_prob": 0.0016229657921940088}, {"id": 1032, "seek": 505992, "start": 5076.9800000000005, "end": 5080.18, "text": " Once the slope changes sign, we know it's called bracketing.", "tokens": [51217, 3443, 264, 13525, 2962, 1465, 11, 321, 458, 309, 311, 1219, 12305, 9880, 13, 51377], "temperature": 0.0, "avg_logprob": -0.298282711322491, "compression_ratio": 1.8911290322580645, "no_speech_prob": 0.0016229657921940088}, {"id": 1033, "seek": 505992, "start": 5080.18, "end": 5083.42, "text": " We've bracketed the minimum of that function.", "tokens": [51377, 492, 600, 12305, 10993, 264, 7285, 295, 300, 2445, 13, 51539], "temperature": 0.0, "avg_logprob": -0.298282711322491, "compression_ratio": 1.8911290322580645, "no_speech_prob": 0.0016229657921940088}, {"id": 1034, "seek": 505992, "start": 5083.42, "end": 5085.88, "text": " And then we can use bisection to find the minimum.", "tokens": [51539, 400, 550, 321, 393, 764, 7393, 10183, 281, 915, 264, 7285, 13, 51662], "temperature": 0.0, "avg_logprob": -0.298282711322491, "compression_ratio": 1.8911290322580645, "no_speech_prob": 0.0016229657921940088}, {"id": 1035, "seek": 505992, "start": 5085.88, "end": 5089.1, "text": " So now we've bracketed it, we find halfway between the two.", "tokens": [51662, 407, 586, 321, 600, 12305, 10993, 309, 11, 321, 915, 15461, 1296, 264, 732, 13, 51823], "temperature": 0.0, "avg_logprob": -0.298282711322491, "compression_ratio": 1.8911290322580645, "no_speech_prob": 0.0016229657921940088}, {"id": 1036, "seek": 508910, "start": 5089.280000000001, "end": 5091.280000000001, "text": " Is it on the left or the right of that?", "tokens": [50373, 1119, 309, 322, 264, 1411, 420, 264, 558, 295, 300, 30, 50473], "temperature": 0.0, "avg_logprob": -0.33558533408425073, "compression_ratio": 1.8513513513513513, "no_speech_prob": 6.0141479480080307e-05}, {"id": 1037, "seek": 508910, "start": 5091.280000000001, "end": 5092.84, "text": " Okay, halfway between the two of those.", "tokens": [50473, 1033, 11, 15461, 1296, 264, 732, 295, 729, 13, 50551], "temperature": 0.0, "avg_logprob": -0.33558533408425073, "compression_ratio": 1.8513513513513513, "no_speech_prob": 6.0141479480080307e-05}, {"id": 1038, "seek": 508910, "start": 5092.84, "end": 5094.160000000001, "text": " Is it left or the right of that?", "tokens": [50551, 1119, 309, 1411, 420, 264, 558, 295, 300, 30, 50617], "temperature": 0.0, "avg_logprob": -0.33558533408425073, "compression_ratio": 1.8513513513513513, "no_speech_prob": 6.0141479480080307e-05}, {"id": 1039, "seek": 508910, "start": 5094.160000000001, "end": 5102.240000000001, "text": " So we use bracketing and bisection to find the optimum in that direction.", "tokens": [50617, 407, 321, 764, 12305, 9880, 293, 7393, 10183, 281, 915, 264, 39326, 294, 300, 3513, 13, 51021], "temperature": 0.0, "avg_logprob": -0.33558533408425073, "compression_ratio": 1.8513513513513513, "no_speech_prob": 6.0141479480080307e-05}, {"id": 1040, "seek": 508910, "start": 5102.240000000001, "end": 5104.400000000001, "text": " That's called a line search.", "tokens": [51021, 663, 311, 1219, 257, 1622, 3164, 13, 51129], "temperature": 0.0, "avg_logprob": -0.33558533408425073, "compression_ratio": 1.8513513513513513, "no_speech_prob": 6.0141479480080307e-05}, {"id": 1041, "seek": 508910, "start": 5104.400000000001, "end": 5111.04, "text": " So all of these optimization techniques rely on the basic idea of a line search.", "tokens": [51129, 407, 439, 295, 613, 19618, 7512, 10687, 322, 264, 3875, 1558, 295, 257, 1622, 3164, 13, 51461], "temperature": 0.0, "avg_logprob": -0.33558533408425073, "compression_ratio": 1.8513513513513513, "no_speech_prob": 6.0141479480080307e-05}, {"id": 1042, "seek": 508910, "start": 5111.04, "end": 5117.18, "text": " Now once you've done the line search, you've found the optimal value in that direction,", "tokens": [51461, 823, 1564, 291, 600, 1096, 264, 1622, 3164, 11, 291, 600, 1352, 264, 16252, 2158, 294, 300, 3513, 11, 51768], "temperature": 0.0, "avg_logprob": -0.33558533408425073, "compression_ratio": 1.8513513513513513, "no_speech_prob": 6.0141479480080307e-05}, {"id": 1043, "seek": 508910, "start": 5117.18, "end": 5119.08, "text": " in our downhill direction.", "tokens": [51768, 294, 527, 29929, 3513, 13, 51863], "temperature": 0.0, "avg_logprob": -0.33558533408425073, "compression_ratio": 1.8513513513513513, "no_speech_prob": 6.0141479480080307e-05}, {"id": 1044, "seek": 511908, "start": 5119.26, "end": 5124.5, "text": " That doesn't necessarily mean we've found the optimal value across our entire space.", "tokens": [50373, 663, 1177, 380, 4725, 914, 321, 600, 1352, 264, 16252, 2158, 2108, 527, 2302, 1901, 13, 50635], "temperature": 0.0, "avg_logprob": -0.3233473883734809, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.00012148157838964835}, {"id": 1045, "seek": 511908, "start": 5124.5, "end": 5127.42, "text": " What we then do is we replete the process.", "tokens": [50635, 708, 321, 550, 360, 307, 321, 319, 781, 975, 264, 1399, 13, 50781], "temperature": 0.0, "avg_logprob": -0.3233473883734809, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.00012148157838964835}, {"id": 1046, "seek": 511908, "start": 5127.42, "end": 5131.9, "text": " Find out what's the downhill direction now, use line search to find the optimum in that", "tokens": [50781, 11809, 484, 437, 311, 264, 29929, 3513, 586, 11, 764, 1622, 3164, 281, 915, 264, 39326, 294, 300, 51005], "temperature": 0.0, "avg_logprob": -0.3233473883734809, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.00012148157838964835}, {"id": 1047, "seek": 511908, "start": 5131.9, "end": 5134.64, "text": " direction.", "tokens": [51005, 3513, 13, 51142], "temperature": 0.0, "avg_logprob": -0.3233473883734809, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.00012148157838964835}, {"id": 1048, "seek": 511908, "start": 5134.64, "end": 5142.22, "text": " So the problem with that is that in a saddle point, you will still often find yourself", "tokens": [51142, 407, 264, 1154, 365, 300, 307, 300, 294, 257, 30459, 935, 11, 291, 486, 920, 2049, 915, 1803, 51521], "temperature": 0.0, "avg_logprob": -0.3233473883734809, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.00012148157838964835}, {"id": 1049, "seek": 511908, "start": 5142.22, "end": 5147.14, "text": " going backwards and forwards in a rather unfortunate way.", "tokens": [51521, 516, 12204, 293, 30126, 294, 257, 2831, 17843, 636, 13, 51767], "temperature": 0.0, "avg_logprob": -0.3233473883734809, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.00012148157838964835}, {"id": 1050, "seek": 514714, "start": 5147.200000000001, "end": 5155.88, "text": " So the faster optimization approach is when they're going to go in a new direction, they", "tokens": [50367, 407, 264, 4663, 19618, 3109, 307, 562, 436, 434, 516, 281, 352, 294, 257, 777, 3513, 11, 436, 50801], "temperature": 0.0, "avg_logprob": -0.2974772958194508, "compression_ratio": 1.7163461538461537, "no_speech_prob": 1.7778544133761898e-05}, {"id": 1051, "seek": 514714, "start": 5155.88, "end": 5161.200000000001, "text": " don't just say which direction is down, they say which direction is the most downhill but", "tokens": [50801, 500, 380, 445, 584, 597, 3513, 307, 760, 11, 436, 584, 597, 3513, 307, 264, 881, 29929, 457, 51067], "temperature": 0.0, "avg_logprob": -0.2974772958194508, "compression_ratio": 1.7163461538461537, "no_speech_prob": 1.7778544133761898e-05}, {"id": 1052, "seek": 514714, "start": 5161.200000000001, "end": 5165.52, "text": " also the most different to the previous directions I've gone.", "tokens": [51067, 611, 264, 881, 819, 281, 264, 3894, 11095, 286, 600, 2780, 13, 51283], "temperature": 0.0, "avg_logprob": -0.2974772958194508, "compression_ratio": 1.7163461538461537, "no_speech_prob": 1.7778544133761898e-05}, {"id": 1053, "seek": 514714, "start": 5165.52, "end": 5168.4400000000005, "text": " That's called finding a conjugate direction.", "tokens": [51283, 663, 311, 1219, 5006, 257, 45064, 3513, 13, 51429], "temperature": 0.0, "avg_logprob": -0.2974772958194508, "compression_ratio": 1.7163461538461537, "no_speech_prob": 1.7778544133761898e-05}, {"id": 1054, "seek": 514714, "start": 5168.4400000000005, "end": 5172.04, "text": " So the good news is you don't need to really know any of those details.", "tokens": [51429, 407, 264, 665, 2583, 307, 291, 500, 380, 643, 281, 534, 458, 604, 295, 729, 4365, 13, 51609], "temperature": 0.0, "avg_logprob": -0.2974772958194508, "compression_ratio": 1.7163461538461537, "no_speech_prob": 1.7778544133761898e-05}, {"id": 1055, "seek": 517204, "start": 5172.1, "end": 5183.58, "text": " All you need to know is that there is a module called scipy.optimize.", "tokens": [50367, 1057, 291, 643, 281, 458, 307, 300, 456, 307, 257, 10088, 1219, 2180, 8200, 13, 5747, 43890, 13, 50941], "temperature": 0.0, "avg_logprob": -0.3096585354562533, "compression_ratio": 1.3581081081081081, "no_speech_prob": 0.06465261429548264}, {"id": 1056, "seek": 517204, "start": 5183.58, "end": 5189.3, "text": " And in scipy.optimize are lots of handy deterministic optimizers.", "tokens": [50941, 400, 294, 2180, 8200, 13, 5747, 43890, 366, 3195, 295, 13239, 15957, 3142, 5028, 22525, 13, 51227], "temperature": 0.0, "avg_logprob": -0.3096585354562533, "compression_ratio": 1.3581081081081081, "no_speech_prob": 0.06465261429548264}, {"id": 1057, "seek": 517204, "start": 5189.3, "end": 5200.12, "text": " The two most common used are conjugate gradient, or CG, and BFGS.", "tokens": [51227, 440, 732, 881, 2689, 1143, 366, 45064, 16235, 11, 420, 38007, 11, 293, 363, 37, 24446, 13, 51768], "temperature": 0.0, "avg_logprob": -0.3096585354562533, "compression_ratio": 1.3581081081081081, "no_speech_prob": 0.06465261429548264}, {"id": 1058, "seek": 520012, "start": 5200.12, "end": 5205.46, "text": " They differ in the detail of how do they decide what direction to go next, which direction", "tokens": [50364, 814, 743, 294, 264, 2607, 295, 577, 360, 436, 4536, 437, 3513, 281, 352, 958, 11, 597, 3513, 50631], "temperature": 0.0, "avg_logprob": -0.27620377010769315, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.04336527734994888}, {"id": 1059, "seek": 520012, "start": 5205.46, "end": 5210.36, "text": " is both the most downhill and also the most different to the previous directions we've", "tokens": [50631, 307, 1293, 264, 881, 29929, 293, 611, 264, 881, 819, 281, 264, 3894, 11095, 321, 600, 50876], "temperature": 0.0, "avg_logprob": -0.27620377010769315, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.04336527734994888}, {"id": 1060, "seek": 520012, "start": 5210.36, "end": 5211.36, "text": " gone.", "tokens": [50876, 2780, 13, 50926], "temperature": 0.0, "avg_logprob": -0.27620377010769315, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.04336527734994888}, {"id": 1061, "seek": 520012, "start": 5211.36, "end": 5217.62, "text": " The particular version we're going to use is a limited-memory BFGS.", "tokens": [50926, 440, 1729, 3037, 321, 434, 516, 281, 764, 307, 257, 5567, 12, 17886, 827, 363, 37, 24446, 13, 51239], "temperature": 0.0, "avg_logprob": -0.27620377010769315, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.04336527734994888}, {"id": 1062, "seek": 520012, "start": 5217.62, "end": 5226.38, "text": " So the important thing is not how it works, the important thing for us is how do we use", "tokens": [51239, 407, 264, 1021, 551, 307, 406, 577, 309, 1985, 11, 264, 1021, 551, 337, 505, 307, 577, 360, 321, 764, 51677], "temperature": 0.0, "avg_logprob": -0.27620377010769315, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.04336527734994888}, {"id": 1063, "seek": 520012, "start": 5226.38, "end": 5227.38, "text": " it.", "tokens": [51677, 309, 13, 51727], "temperature": 0.0, "avg_logprob": -0.27620377010769315, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.04336527734994888}, {"id": 1064, "seek": 522738, "start": 5227.38, "end": 5237.54, "text": " Question about Loss plus Grads.", "tokens": [50364, 14464, 466, 441, 772, 1804, 16710, 82, 13, 50872], "temperature": 0.0, "avg_logprob": -0.42889803647994995, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.06853686273097992}, {"id": 1065, "seek": 522738, "start": 5237.54, "end": 5244.86, "text": " So this is an array containing a single thing, which is Loss.", "tokens": [50872, 407, 341, 307, 364, 10225, 19273, 257, 2167, 551, 11, 597, 307, 441, 772, 13, 51238], "temperature": 0.0, "avg_logprob": -0.42889803647994995, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.06853686273097992}, {"id": 1066, "seek": 522738, "start": 5244.86, "end": 5251.02, "text": " Grads is already an array, or a list I should say, which is a list of all of the loss with", "tokens": [51238, 2606, 5834, 307, 1217, 364, 10225, 11, 420, 257, 1329, 286, 820, 584, 11, 597, 307, 257, 1329, 295, 439, 295, 264, 4470, 365, 51546], "temperature": 0.0, "avg_logprob": -0.42889803647994995, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.06853686273097992}, {"id": 1067, "seek": 522738, "start": 5251.02, "end": 5252.9800000000005, "text": " respect to all of the inputs.", "tokens": [51546, 3104, 281, 439, 295, 264, 15743, 13, 51644], "temperature": 0.0, "avg_logprob": -0.42889803647994995, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.06853686273097992}, {"id": 1068, "seek": 525298, "start": 5252.98, "end": 5258.94, "text": " So plus in Python on two lists simply joins the two lists together.", "tokens": [50364, 407, 1804, 294, 15329, 322, 732, 14511, 2935, 24397, 264, 732, 14511, 1214, 13, 50662], "temperature": 0.0, "avg_logprob": -0.38836921056111656, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.03210053592920303}, {"id": 1069, "seek": 525298, "start": 5258.94, "end": 5262.419999999999, "text": " This is a list containing the loss and all of the gradients.", "tokens": [50662, 639, 307, 257, 1329, 19273, 264, 4470, 293, 439, 295, 264, 2771, 2448, 13, 50836], "temperature": 0.0, "avg_logprob": -0.38836921056111656, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.03210053592920303}, {"id": 1070, "seek": 525298, "start": 5262.419999999999, "end": 5276.299999999999, "text": " Ant colony optimization lives in the class known as metaheuristics, like genetic algorithms", "tokens": [50836, 5130, 23028, 19618, 2909, 294, 264, 1508, 2570, 382, 1131, 545, 13629, 6006, 11, 411, 12462, 14642, 51530], "temperature": 0.0, "avg_logprob": -0.38836921056111656, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.03210053592920303}, {"id": 1071, "seek": 525298, "start": 5276.299999999999, "end": 5279.339999999999, "text": " or simulated annealing.", "tokens": [51530, 420, 41713, 22256, 4270, 13, 51682], "temperature": 0.0, "avg_logprob": -0.38836921056111656, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.03210053592920303}, {"id": 1072, "seek": 527934, "start": 5279.34, "end": 5285.34, "text": " There's a wide range of optimization algorithms that are designed for very difficult to optimize", "tokens": [50364, 821, 311, 257, 4874, 3613, 295, 19618, 14642, 300, 366, 4761, 337, 588, 2252, 281, 19719, 50664], "temperature": 0.0, "avg_logprob": -0.333936713990711, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.2974414825439453}, {"id": 1073, "seek": 527934, "start": 5285.34, "end": 5289.34, "text": " functions, functions which are extremely bumpy.", "tokens": [50664, 6828, 11, 6828, 597, 366, 4664, 49400, 13, 50864], "temperature": 0.0, "avg_logprob": -0.333936713990711, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.2974414825439453}, {"id": 1074, "seek": 527934, "start": 5289.34, "end": 5297.7, "text": " So these techniques all use a lot of randomization in order to avoid the bumps.", "tokens": [50864, 407, 613, 7512, 439, 764, 257, 688, 295, 4974, 2144, 294, 1668, 281, 5042, 264, 27719, 13, 51282], "temperature": 0.0, "avg_logprob": -0.333936713990711, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.2974414825439453}, {"id": 1075, "seek": 527934, "start": 5297.7, "end": 5303.7, "text": " In our case, we're using mean-squared error, which is a nice smooth objective, so we can", "tokens": [51282, 682, 527, 1389, 11, 321, 434, 1228, 914, 12, 33292, 1642, 6713, 11, 597, 307, 257, 1481, 5508, 10024, 11, 370, 321, 393, 51582], "temperature": 0.0, "avg_logprob": -0.333936713990711, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.2974414825439453}, {"id": 1076, "seek": 527934, "start": 5303.7, "end": 5308.06, "text": " use the much faster convex optimization method.", "tokens": [51582, 764, 264, 709, 4663, 42432, 19618, 3170, 13, 51800], "temperature": 0.0, "avg_logprob": -0.333936713990711, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.2974414825439453}, {"id": 1077, "seek": 530806, "start": 5308.06, "end": 5323.18, "text": " Question about non-convex problem or convex optimization.", "tokens": [50364, 14464, 466, 2107, 12, 1671, 303, 87, 1154, 420, 42432, 19618, 13, 51120], "temperature": 0.0, "avg_logprob": -0.5348804394404093, "compression_ratio": 1.4014598540145986, "no_speech_prob": 0.024422943592071533}, {"id": 1078, "seek": 530806, "start": 5323.18, "end": 5326.780000000001, "text": " How do we use one of these optimizers?", "tokens": [51120, 1012, 360, 321, 764, 472, 295, 613, 5028, 22525, 30, 51300], "temperature": 0.0, "avg_logprob": -0.5348804394404093, "compression_ratio": 1.4014598540145986, "no_speech_prob": 0.024422943592071533}, {"id": 1079, "seek": 530806, "start": 5326.780000000001, "end": 5331.820000000001, "text": " Basically you provide the name of the optimizer, which in this case is minimize something using", "tokens": [51300, 8537, 291, 2893, 264, 1315, 295, 264, 5028, 6545, 11, 597, 294, 341, 1389, 307, 17522, 746, 1228, 51552], "temperature": 0.0, "avg_logprob": -0.5348804394404093, "compression_ratio": 1.4014598540145986, "no_speech_prob": 0.024422943592071533}, {"id": 1080, "seek": 533182, "start": 5331.82, "end": 5338.099999999999, "text": " BFTS, and you have to pass it 3 things.", "tokens": [50364, 363, 37, 7327, 11, 293, 291, 362, 281, 1320, 309, 805, 721, 13, 50678], "temperature": 0.0, "avg_logprob": -0.2777999914609469, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.08151882886886597}, {"id": 1081, "seek": 533182, "start": 5338.099999999999, "end": 5349.46, "text": " A function which will return the loss value at the current point, a starting point, and", "tokens": [50678, 316, 2445, 597, 486, 2736, 264, 4470, 2158, 412, 264, 2190, 935, 11, 257, 2891, 935, 11, 293, 51246], "temperature": 0.0, "avg_logprob": -0.2777999914609469, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.08151882886886597}, {"id": 1082, "seek": 533182, "start": 5349.46, "end": 5354.7, "text": " a function which will return the gradients at the current point.", "tokens": [51246, 257, 2445, 597, 486, 2736, 264, 2771, 2448, 412, 264, 2190, 935, 13, 51508], "temperature": 0.0, "avg_logprob": -0.2777999914609469, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.08151882886886597}, {"id": 1083, "seek": 535470, "start": 5354.7, "end": 5362.58, "text": " Now unfortunately, we have a function which returns the loss and the gradients together,", "tokens": [50364, 823, 7015, 11, 321, 362, 257, 2445, 597, 11247, 264, 4470, 293, 264, 2771, 2448, 1214, 11, 50758], "temperature": 0.0, "avg_logprob": -0.31041414087468927, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.033085644245147705}, {"id": 1084, "seek": 535470, "start": 5362.58, "end": 5365.98, "text": " which is not what this wants.", "tokens": [50758, 597, 307, 406, 437, 341, 2738, 13, 50928], "temperature": 0.0, "avg_logprob": -0.31041414087468927, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.033085644245147705}, {"id": 1085, "seek": 535470, "start": 5365.98, "end": 5374.099999999999, "text": " So a minor little detail is that we create a simple little class.", "tokens": [50928, 407, 257, 6696, 707, 2607, 307, 300, 321, 1884, 257, 2199, 707, 1508, 13, 51334], "temperature": 0.0, "avg_logprob": -0.31041414087468927, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.033085644245147705}, {"id": 1086, "seek": 535470, "start": 5374.099999999999, "end": 5377.98, "text": " All this class does, and again the details really aren't important, but all this class", "tokens": [51334, 1057, 341, 1508, 775, 11, 293, 797, 264, 4365, 534, 3212, 380, 1021, 11, 457, 439, 341, 1508, 51528], "temperature": 0.0, "avg_logprob": -0.31041414087468927, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.033085644245147705}, {"id": 1087, "seek": 537798, "start": 5377.98, "end": 5385.78, "text": " does is that when loss is called, it calls that function that we created, passing in", "tokens": [50364, 775, 307, 300, 562, 4470, 307, 1219, 11, 309, 5498, 300, 2445, 300, 321, 2942, 11, 8437, 294, 50754], "temperature": 0.0, "avg_logprob": -0.2754022920286501, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.17553327977657318}, {"id": 1088, "seek": 537798, "start": 5385.78, "end": 5388.86, "text": " the current value of the data.", "tokens": [50754, 264, 2190, 2158, 295, 264, 1412, 13, 50908], "temperature": 0.0, "avg_logprob": -0.2754022920286501, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.17553327977657318}, {"id": 1089, "seek": 537798, "start": 5388.86, "end": 5396.74, "text": " It gets back the loss and the gradients, and it returns the loss.", "tokens": [50908, 467, 2170, 646, 264, 4470, 293, 264, 2771, 2448, 11, 293, 309, 11247, 264, 4470, 13, 51302], "temperature": 0.0, "avg_logprob": -0.2754022920286501, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.17553327977657318}, {"id": 1090, "seek": 537798, "start": 5396.74, "end": 5403.339999999999, "text": " Later on, when the optimizer asks for the gradients, it returns those gradients that", "tokens": [51302, 11965, 322, 11, 562, 264, 5028, 6545, 8962, 337, 264, 2771, 2448, 11, 309, 11247, 729, 2771, 2448, 300, 51632], "temperature": 0.0, "avg_logprob": -0.2754022920286501, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.17553327977657318}, {"id": 1091, "seek": 537798, "start": 5403.339999999999, "end": 5404.98, "text": " I stored back here.", "tokens": [51632, 286, 12187, 646, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2754022920286501, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.17553327977657318}, {"id": 1092, "seek": 540498, "start": 5404.98, "end": 5410.54, "text": " So all this is doing is it's a little class which allows us to basically turn a keras", "tokens": [50364, 407, 439, 341, 307, 884, 307, 309, 311, 257, 707, 1508, 597, 4045, 505, 281, 1936, 1261, 257, 350, 6985, 50642], "temperature": 0.0, "avg_logprob": -0.25804901123046875, "compression_ratio": 1.8009049773755657, "no_speech_prob": 0.002981023397296667}, {"id": 1093, "seek": 540498, "start": 5410.54, "end": 5416.179999999999, "text": " function that returns the loss and the gradients together into 2 functions, one which returns", "tokens": [50642, 2445, 300, 11247, 264, 4470, 293, 264, 2771, 2448, 1214, 666, 568, 6828, 11, 472, 597, 11247, 50924], "temperature": 0.0, "avg_logprob": -0.25804901123046875, "compression_ratio": 1.8009049773755657, "no_speech_prob": 0.002981023397296667}, {"id": 1094, "seek": 540498, "start": 5416.179999999999, "end": 5418.82, "text": " the loss, one which returns the gradients.", "tokens": [50924, 264, 4470, 11, 472, 597, 11247, 264, 2771, 2448, 13, 51056], "temperature": 0.0, "avg_logprob": -0.25804901123046875, "compression_ratio": 1.8009049773755657, "no_speech_prob": 0.002981023397296667}, {"id": 1095, "seek": 540498, "start": 5418.82, "end": 5422.7, "text": " So it's a pretty minor detail, but it's a handy thing to have in your toolbox because", "tokens": [51056, 407, 309, 311, 257, 1238, 6696, 2607, 11, 457, 309, 311, 257, 13239, 551, 281, 362, 294, 428, 44593, 570, 51250], "temperature": 0.0, "avg_logprob": -0.25804901123046875, "compression_ratio": 1.8009049773755657, "no_speech_prob": 0.002981023397296667}, {"id": 1096, "seek": 540498, "start": 5422.7, "end": 5431.179999999999, "text": " it means you now have something that can use deterministic optimizers on keras functions.", "tokens": [51250, 309, 1355, 291, 586, 362, 746, 300, 393, 764, 15957, 3142, 5028, 22525, 322, 350, 6985, 6828, 13, 51674], "temperature": 0.0, "avg_logprob": -0.25804901123046875, "compression_ratio": 1.8009049773755657, "no_speech_prob": 0.002981023397296667}, {"id": 1097, "seek": 543118, "start": 5431.18, "end": 5438.5, "text": " So all we do is we loop through a small number of times, calling that optimizer each time,", "tokens": [50364, 407, 439, 321, 360, 307, 321, 6367, 807, 257, 1359, 1230, 295, 1413, 11, 5141, 300, 5028, 6545, 1184, 565, 11, 50730], "temperature": 0.0, "avg_logprob": -0.3090670050644293, "compression_ratio": 1.7443181818181819, "no_speech_prob": 0.00522011099383235}, {"id": 1098, "seek": 543118, "start": 5438.5, "end": 5443.02, "text": " and passing in some starting point.", "tokens": [50730, 293, 8437, 294, 512, 2891, 935, 13, 50956], "temperature": 0.0, "avg_logprob": -0.3090670050644293, "compression_ratio": 1.7443181818181819, "no_speech_prob": 0.00522011099383235}, {"id": 1099, "seek": 543118, "start": 5443.02, "end": 5449.18, "text": " So the starting point is just a random image.", "tokens": [50956, 407, 264, 2891, 935, 307, 445, 257, 4974, 3256, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3090670050644293, "compression_ratio": 1.7443181818181819, "no_speech_prob": 0.00522011099383235}, {"id": 1100, "seek": 543118, "start": 5449.18, "end": 5455.780000000001, "text": " So we just create a random image and here is what a random image looks like.", "tokens": [51264, 407, 321, 445, 1884, 257, 4974, 3256, 293, 510, 307, 437, 257, 4974, 3256, 1542, 411, 13, 51594], "temperature": 0.0, "avg_logprob": -0.3090670050644293, "compression_ratio": 1.7443181818181819, "no_speech_prob": 0.00522011099383235}, {"id": 1101, "seek": 543118, "start": 5455.780000000001, "end": 5458.860000000001, "text": " So let's go ahead and run that so we can see the results.", "tokens": [51594, 407, 718, 311, 352, 2286, 293, 1190, 300, 370, 321, 393, 536, 264, 3542, 13, 51748], "temperature": 0.0, "avg_logprob": -0.3090670050644293, "compression_ratio": 1.7443181818181819, "no_speech_prob": 0.00522011099383235}, {"id": 1102, "seek": 545886, "start": 5459.54, "end": 5461.54, "text": " I haven't actually ran this yet.", "tokens": [50398, 286, 2378, 380, 767, 5872, 341, 1939, 13, 50498], "temperature": 0.0, "avg_logprob": -0.8068397876828216, "compression_ratio": 1.10752688172043, "no_speech_prob": 0.8103479743003845}, {"id": 1103, "seek": 545886, "start": 5467.259999999999, "end": 5469.259999999999, "text": " Oh, there it comes.", "tokens": [50784, 876, 11, 456, 309, 1487, 13, 50884], "temperature": 0.0, "avg_logprob": -0.8068397876828216, "compression_ratio": 1.10752688172043, "no_speech_prob": 0.8103479743003845}, {"id": 1104, "seek": 545886, "start": 5469.259999999999, "end": 5471.259999999999, "text": " Good.", "tokens": [50884, 2205, 13, 50984], "temperature": 0.0, "avg_logprob": -0.8068397876828216, "compression_ratio": 1.10752688172043, "no_speech_prob": 0.8103479743003845}, {"id": 1105, "seek": 545886, "start": 5476.0599999999995, "end": 5478.0599999999995, "text": " Run, run, run.", "tokens": [51224, 8950, 11, 1190, 11, 1190, 13, 51324], "temperature": 0.0, "avg_logprob": -0.8068397876828216, "compression_ratio": 1.10752688172043, "no_speech_prob": 0.8103479743003845}, {"id": 1106, "seek": 545886, "start": 5478.0599999999995, "end": 5480.0599999999995, "text": " Generate random image.", "tokens": [51324, 15409, 473, 4974, 3256, 13, 51424], "temperature": 0.0, "avg_logprob": -0.8068397876828216, "compression_ratio": 1.10752688172043, "no_speech_prob": 0.8103479743003845}, {"id": 1107, "seek": 545886, "start": 5483.66, "end": 5485.66, "text": " Solve.", "tokens": [51604, 7026, 303, 13, 51704], "temperature": 0.0, "avg_logprob": -0.8068397876828216, "compression_ratio": 1.10752688172043, "no_speech_prob": 0.8103479743003845}, {"id": 1108, "seek": 548886, "start": 5489.86, "end": 5493.86, "text": " Okay, so you can see it going along and solving here.", "tokens": [50414, 1033, 11, 370, 291, 393, 536, 309, 516, 2051, 293, 12606, 510, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3154217553517175, "compression_ratio": 1.391566265060241, "no_speech_prob": 0.0002571415971033275}, {"id": 1109, "seek": 548886, "start": 5493.86, "end": 5497.0599999999995, "text": " Here's one I prepared earlier.", "tokens": [50614, 1692, 311, 472, 286, 4927, 3071, 13, 50774], "temperature": 0.0, "avg_logprob": -0.3154217553517175, "compression_ratio": 1.391566265060241, "no_speech_prob": 0.0002571415971033275}, {"id": 1110, "seek": 548886, "start": 5497.0599999999995, "end": 5502.38, "text": " And here at the end of the 10th iteration is the result.", "tokens": [50774, 400, 510, 412, 264, 917, 295, 264, 1266, 392, 24784, 307, 264, 1874, 13, 51040], "temperature": 0.0, "avg_logprob": -0.3154217553517175, "compression_ratio": 1.391566265060241, "no_speech_prob": 0.0002571415971033275}, {"id": 1111, "seek": 548886, "start": 5502.38, "end": 5511.179999999999, "text": " So remember what we did was we started with this image, we called an optimizer which took", "tokens": [51040, 407, 1604, 437, 321, 630, 390, 321, 1409, 365, 341, 3256, 11, 321, 1219, 364, 5028, 6545, 597, 1890, 51480], "temperature": 0.0, "avg_logprob": -0.3154217553517175, "compression_ratio": 1.391566265060241, "no_speech_prob": 0.0002571415971033275}, {"id": 1112, "seek": 551118, "start": 5511.18, "end": 5521.54, "text": " that image and attempted to optimize this loss function where the target was the value", "tokens": [50364, 300, 3256, 293, 18997, 281, 19719, 341, 4470, 2445, 689, 264, 3779, 390, 264, 2158, 50882], "temperature": 0.0, "avg_logprob": -0.2990748628656915, "compression_ratio": 1.53125, "no_speech_prob": 0.040237754583358765}, {"id": 1113, "seek": 551118, "start": 5521.54, "end": 5529.780000000001, "text": " of this layer for our bird image.", "tokens": [50882, 295, 341, 4583, 337, 527, 5255, 3256, 13, 51294], "temperature": 0.0, "avg_logprob": -0.2990748628656915, "compression_ratio": 1.53125, "no_speech_prob": 0.040237754583358765}, {"id": 1114, "seek": 551118, "start": 5529.780000000001, "end": 5536.62, "text": " And the thing it was comparing it to was the layer for the generated image.", "tokens": [51294, 400, 264, 551, 309, 390, 15763, 309, 281, 390, 264, 4583, 337, 264, 10833, 3256, 13, 51636], "temperature": 0.0, "avg_logprob": -0.2990748628656915, "compression_ratio": 1.53125, "no_speech_prob": 0.040237754583358765}, {"id": 1115, "seek": 553662, "start": 5536.62, "end": 5542.18, "text": " So we started with this, we ran that optimizer a bunch of times, calculating the gradient", "tokens": [50364, 407, 321, 1409, 365, 341, 11, 321, 5872, 300, 5028, 6545, 257, 3840, 295, 1413, 11, 28258, 264, 16235, 50642], "temperature": 0.0, "avg_logprob": -0.30388651265726463, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.00049553532153368}, {"id": 1116, "seek": 553662, "start": 5542.18, "end": 5548.5, "text": " of that loss with respect to the input to the model, the very pixels themselves.", "tokens": [50642, 295, 300, 4470, 365, 3104, 281, 264, 4846, 281, 264, 2316, 11, 264, 588, 18668, 2969, 13, 50958], "temperature": 0.0, "avg_logprob": -0.30388651265726463, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.00049553532153368}, {"id": 1117, "seek": 553662, "start": 5548.5, "end": 5554.42, "text": " And after 10 iterations, it had turned this random image into this thing.", "tokens": [50958, 400, 934, 1266, 36540, 11, 309, 632, 3574, 341, 4974, 3256, 666, 341, 551, 13, 51254], "temperature": 0.0, "avg_logprob": -0.30388651265726463, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.00049553532153368}, {"id": 1118, "seek": 553662, "start": 5554.42, "end": 5565.18, "text": " So this is the thing which optimizes the block5, conf1 layer.", "tokens": [51254, 407, 341, 307, 264, 551, 597, 5028, 5660, 264, 3461, 20, 11, 1497, 16, 4583, 13, 51792], "temperature": 0.0, "avg_logprob": -0.30388651265726463, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.00049553532153368}, {"id": 1119, "seek": 556518, "start": 5565.18, "end": 5569.820000000001, "text": " And you can see it still looks like a bird, but by this point it really doesn't care what", "tokens": [50364, 400, 291, 393, 536, 309, 920, 1542, 411, 257, 5255, 11, 457, 538, 341, 935, 309, 534, 1177, 380, 1127, 437, 50596], "temperature": 0.0, "avg_logprob": -0.26285353330808264, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0005112573271617293}, {"id": 1120, "seek": 556518, "start": 5569.820000000001, "end": 5571.740000000001, "text": " the background looks like.", "tokens": [50596, 264, 3678, 1542, 411, 13, 50692], "temperature": 0.0, "avg_logprob": -0.26285353330808264, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0005112573271617293}, {"id": 1121, "seek": 556518, "start": 5571.740000000001, "end": 5574.740000000001, "text": " It cares a lot what the eye looks like and the beak looks like and the feathers look", "tokens": [50692, 467, 12310, 257, 688, 437, 264, 3313, 1542, 411, 293, 264, 48663, 1542, 411, 293, 264, 27044, 574, 50842], "temperature": 0.0, "avg_logprob": -0.26285353330808264, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0005112573271617293}, {"id": 1122, "seek": 556518, "start": 5574.740000000001, "end": 5580.46, "text": " like because these things all matter to ImageNet to make sure it correctly sees it as a bird.", "tokens": [50842, 411, 570, 613, 721, 439, 1871, 281, 29903, 31890, 281, 652, 988, 309, 8944, 8194, 309, 382, 257, 5255, 13, 51128], "temperature": 0.0, "avg_logprob": -0.26285353330808264, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0005112573271617293}, {"id": 1123, "seek": 556518, "start": 5580.46, "end": 5589.9800000000005, "text": " If we look at an earlier layer, let's look at block4, conf1, you can see it's getting", "tokens": [51128, 759, 321, 574, 412, 364, 3071, 4583, 11, 718, 311, 574, 412, 3461, 19, 11, 1497, 16, 11, 291, 393, 536, 309, 311, 1242, 51604], "temperature": 0.0, "avg_logprob": -0.26285353330808264, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0005112573271617293}, {"id": 1124, "seek": 556518, "start": 5589.9800000000005, "end": 5591.820000000001, "text": " the details more correct.", "tokens": [51604, 264, 4365, 544, 3006, 13, 51696], "temperature": 0.0, "avg_logprob": -0.26285353330808264, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0005112573271617293}, {"id": 1125, "seek": 559182, "start": 5592.46, "end": 5597.38, "text": " We do our artistic style, we can choose which layer will be our fcontent.", "tokens": [50396, 492, 360, 527, 17090, 3758, 11, 321, 393, 2826, 597, 4583, 486, 312, 527, 283, 9000, 317, 13, 50642], "temperature": 0.0, "avg_logprob": -0.27952015216533954, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.02262900397181511}, {"id": 1126, "seek": 559182, "start": 5597.38, "end": 5603.46, "text": " And if we choose an earlier one, it's going to give it less degrees of freedom to look", "tokens": [50642, 400, 498, 321, 2826, 364, 3071, 472, 11, 309, 311, 516, 281, 976, 309, 1570, 5310, 295, 5645, 281, 574, 50946], "temperature": 0.0, "avg_logprob": -0.27952015216533954, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.02262900397181511}, {"id": 1127, "seek": 559182, "start": 5603.46, "end": 5608.78, "text": " like a different kind of bird, but it's going to look more like our original bird.", "tokens": [50946, 411, 257, 819, 733, 295, 5255, 11, 457, 309, 311, 516, 281, 574, 544, 411, 527, 3380, 5255, 13, 51212], "temperature": 0.0, "avg_logprob": -0.27952015216533954, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.02262900397181511}, {"id": 1128, "seek": 560878, "start": 5608.78, "end": 5619.139999999999, "text": " And so then here's a video showing how that happens.", "tokens": [50364, 400, 370, 550, 510, 311, 257, 960, 4099, 577, 300, 2314, 13, 50882], "temperature": 0.0, "avg_logprob": -0.3579992525505297, "compression_ratio": 1.4023668639053255, "no_speech_prob": 0.0293117668479681}, {"id": 1129, "seek": 560878, "start": 5619.139999999999, "end": 5622.3, "text": " There are the 10 steps.", "tokens": [50882, 821, 366, 264, 1266, 4439, 13, 51040], "temperature": 0.0, "avg_logprob": -0.3579992525505297, "compression_ratio": 1.4023668639053255, "no_speech_prob": 0.0293117668479681}, {"id": 1130, "seek": 560878, "start": 5622.3, "end": 5632.94, "text": " It's often helpful to be able to visualize the iterations of your generators at work.", "tokens": [51040, 467, 311, 2049, 4961, 281, 312, 1075, 281, 23273, 264, 36540, 295, 428, 38662, 412, 589, 13, 51572], "temperature": 0.0, "avg_logprob": -0.3579992525505297, "compression_ratio": 1.4023668639053255, "no_speech_prob": 0.0293117668479681}, {"id": 1131, "seek": 560878, "start": 5632.94, "end": 5636.74, "text": " So feel free to borrow this very simple code.", "tokens": [51572, 407, 841, 1737, 281, 11172, 341, 588, 2199, 3089, 13, 51762], "temperature": 0.0, "avg_logprob": -0.3579992525505297, "compression_ratio": 1.4023668639053255, "no_speech_prob": 0.0293117668479681}, {"id": 1132, "seek": 560878, "start": 5636.74, "end": 5637.74, "text": " You can just use matplotlib.", "tokens": [51762, 509, 393, 445, 764, 3803, 564, 310, 38270, 13, 51812], "temperature": 0.0, "avg_logprob": -0.3579992525505297, "compression_ratio": 1.4023668639053255, "no_speech_prob": 0.0293117668479681}, {"id": 1133, "seek": 563774, "start": 5638.54, "end": 5642.62, "text": " We actually used this in the last class.", "tokens": [50404, 492, 767, 1143, 341, 294, 264, 1036, 1508, 13, 50608], "temperature": 0.0, "avg_logprob": -0.344012188911438, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.2598091661930084}, {"id": 1134, "seek": 563774, "start": 5642.62, "end": 5648.0599999999995, "text": " Remember we optimized our linear optimizer, we animated it.", "tokens": [50608, 5459, 321, 26941, 527, 8213, 5028, 6545, 11, 321, 18947, 309, 13, 50880], "temperature": 0.0, "avg_logprob": -0.344012188911438, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.2598091661930084}, {"id": 1135, "seek": 563774, "start": 5648.0599999999995, "end": 5653.219999999999, "text": " You just have to define a function that gets called at each step of the animation, and", "tokens": [50880, 509, 445, 362, 281, 6964, 257, 2445, 300, 2170, 1219, 412, 1184, 1823, 295, 264, 9603, 11, 293, 51138], "temperature": 0.0, "avg_logprob": -0.344012188911438, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.2598091661930084}, {"id": 1136, "seek": 563774, "start": 5653.219999999999, "end": 5657.62, "text": " then you can just call animation.funcanimation passing in that function.", "tokens": [51138, 550, 291, 393, 445, 818, 9603, 13, 15930, 66, 17869, 399, 8437, 294, 300, 2445, 13, 51358], "temperature": 0.0, "avg_logprob": -0.344012188911438, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.2598091661930084}, {"id": 1137, "seek": 563774, "start": 5657.62, "end": 5661.219999999999, "text": " That's a nice way that you can animate your own generators.", "tokens": [51358, 663, 311, 257, 1481, 636, 300, 291, 393, 36439, 428, 1065, 38662, 13, 51538], "temperature": 0.0, "avg_logprob": -0.344012188911438, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.2598091661930084}, {"id": 1138, "seek": 566122, "start": 5661.22, "end": 5669.9800000000005, "text": " Question, we're using Keras and TensorFlow to extract the VGG features, these are used", "tokens": [50364, 14464, 11, 321, 434, 1228, 591, 6985, 293, 37624, 281, 8947, 264, 691, 27561, 4122, 11, 613, 366, 1143, 50802], "temperature": 0.0, "avg_logprob": -0.304986537187949, "compression_ratio": 1.4438775510204083, "no_speech_prob": 0.03732649236917496}, {"id": 1139, "seek": 566122, "start": 5669.9800000000005, "end": 5672.820000000001, "text": " by SciPy for BFGS.", "tokens": [50802, 538, 16942, 47, 88, 337, 363, 37, 24446, 13, 50944], "temperature": 0.0, "avg_logprob": -0.304986537187949, "compression_ratio": 1.4438775510204083, "no_speech_prob": 0.03732649236917496}, {"id": 1140, "seek": 566122, "start": 5672.820000000001, "end": 5676.06, "text": " Does the BFGS also run on the GPU?", "tokens": [50944, 4402, 264, 363, 37, 24446, 611, 1190, 322, 264, 18407, 30, 51106], "temperature": 0.0, "avg_logprob": -0.304986537187949, "compression_ratio": 1.4438775510204083, "no_speech_prob": 0.03732649236917496}, {"id": 1141, "seek": 566122, "start": 5676.06, "end": 5682.780000000001, "text": " No, there's really very little for the BFGS to do.", "tokens": [51106, 883, 11, 456, 311, 534, 588, 707, 337, 264, 363, 37, 24446, 281, 360, 13, 51442], "temperature": 0.0, "avg_logprob": -0.304986537187949, "compression_ratio": 1.4438775510204083, "no_speech_prob": 0.03732649236917496}, {"id": 1142, "seek": 566122, "start": 5682.780000000001, "end": 5688.62, "text": " Really for an optimizer, all of the work is in calling the loss function and the gradients.", "tokens": [51442, 4083, 337, 364, 5028, 6545, 11, 439, 295, 264, 589, 307, 294, 5141, 264, 4470, 2445, 293, 264, 2771, 2448, 13, 51734], "temperature": 0.0, "avg_logprob": -0.304986537187949, "compression_ratio": 1.4438775510204083, "no_speech_prob": 0.03732649236917496}, {"id": 1143, "seek": 568862, "start": 5688.62, "end": 5696.74, "text": " The actual work of doing the bisection and doing the bracketing is so trivial that we", "tokens": [50364, 440, 3539, 589, 295, 884, 264, 7393, 10183, 293, 884, 264, 12305, 9880, 307, 370, 26703, 300, 321, 50770], "temperature": 0.0, "avg_logprob": -0.3781828719578432, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.04885701462626457}, {"id": 1144, "seek": 568862, "start": 5696.74, "end": 5698.3, "text": " just don't care about that.", "tokens": [50770, 445, 500, 380, 1127, 466, 300, 13, 50848], "temperature": 0.0, "avg_logprob": -0.3781828719578432, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.04885701462626457}, {"id": 1145, "seek": 568862, "start": 5698.3, "end": 5700.26, "text": " It doesn't take any time.", "tokens": [50848, 467, 1177, 380, 747, 604, 565, 13, 50946], "temperature": 0.0, "avg_logprob": -0.3781828719578432, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.04885701462626457}, {"id": 1146, "seek": 568862, "start": 5700.26, "end": 5707.099999999999, "text": " There's a question about the checkerboard artifact, the geometric pattern.", "tokens": [50946, 821, 311, 257, 1168, 466, 264, 1520, 260, 3787, 34806, 11, 264, 33246, 5102, 13, 51288], "temperature": 0.0, "avg_logprob": -0.3781828719578432, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.04885701462626457}, {"id": 1147, "seek": 568862, "start": 5707.099999999999, "end": 5710.14, "text": " This is actually not a checkerboard artifact exactly.", "tokens": [51288, 639, 307, 767, 406, 257, 1520, 260, 3787, 34806, 2293, 13, 51440], "temperature": 0.0, "avg_logprob": -0.3781828719578432, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.04885701462626457}, {"id": 1148, "seek": 568862, "start": 5710.14, "end": 5715.22, "text": " Checkerboard artifacts we will look at later, they look a little bit different.", "tokens": [51440, 6881, 260, 3787, 24617, 321, 486, 574, 412, 1780, 11, 436, 574, 257, 707, 857, 819, 13, 51694], "temperature": 0.0, "avg_logprob": -0.3781828719578432, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.04885701462626457}, {"id": 1149, "seek": 571522, "start": 5715.22, "end": 5725.38, "text": " That was my interpretation mistake, not the questioner's.", "tokens": [50364, 663, 390, 452, 14174, 6146, 11, 406, 264, 1168, 260, 311, 13, 50872], "temperature": 0.0, "avg_logprob": -0.3974944182804653, "compression_ratio": 1.3624161073825503, "no_speech_prob": 0.009267969988286495}, {"id": 1150, "seek": 571522, "start": 5725.38, "end": 5735.22, "text": " I'm not exactly sure why this particular kind of noise has appeared, honestly.", "tokens": [50872, 286, 478, 406, 2293, 988, 983, 341, 1729, 733, 295, 5658, 575, 8516, 11, 6095, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3974944182804653, "compression_ratio": 1.3624161073825503, "no_speech_prob": 0.009267969988286495}, {"id": 1151, "seek": 571522, "start": 5735.22, "end": 5738.34, "text": " It's an interesting question.", "tokens": [51364, 467, 311, 364, 1880, 1168, 13, 51520], "temperature": 0.0, "avg_logprob": -0.3974944182804653, "compression_ratio": 1.3624161073825503, "no_speech_prob": 0.009267969988286495}, {"id": 1152, "seek": 571522, "start": 5738.34, "end": 5742.9800000000005, "text": " How would batching work?", "tokens": [51520, 1012, 576, 15245, 278, 589, 30, 51752], "temperature": 0.0, "avg_logprob": -0.3974944182804653, "compression_ratio": 1.3624161073825503, "no_speech_prob": 0.009267969988286495}, {"id": 1153, "seek": 571522, "start": 5742.9800000000005, "end": 5743.9800000000005, "text": " It doesn't.", "tokens": [51752, 467, 1177, 380, 13, 51802], "temperature": 0.0, "avg_logprob": -0.3974944182804653, "compression_ratio": 1.3624161073825503, "no_speech_prob": 0.009267969988286495}, {"id": 1154, "seek": 574398, "start": 5744.62, "end": 5749.0599999999995, "text": " There's no batching to do.", "tokens": [50396, 821, 311, 572, 15245, 278, 281, 360, 13, 50618], "temperature": 0.0, "avg_logprob": -0.3762984456895273, "compression_ratio": 1.5606060606060606, "no_speech_prob": 5.2252198656788096e-05}, {"id": 1155, "seek": 574398, "start": 5749.0599999999995, "end": 5758.179999999999, "text": " We have a single image which is being optimized.", "tokens": [50618, 492, 362, 257, 2167, 3256, 597, 307, 885, 26941, 13, 51074], "temperature": 0.0, "avg_logprob": -0.3762984456895273, "compression_ratio": 1.5606060606060606, "no_speech_prob": 5.2252198656788096e-05}, {"id": 1156, "seek": 574398, "start": 5758.179999999999, "end": 5759.7, "text": " There's really no batching to do here.", "tokens": [51074, 821, 311, 534, 572, 15245, 278, 281, 360, 510, 13, 51150], "temperature": 0.0, "avg_logprob": -0.3762984456895273, "compression_ratio": 1.5606060606060606, "no_speech_prob": 5.2252198656788096e-05}, {"id": 1157, "seek": 574398, "start": 5759.7, "end": 5763.98, "text": " We'll look at a version which uses a very different approach and has batching shortly.", "tokens": [51150, 492, 603, 574, 412, 257, 3037, 597, 4960, 257, 588, 819, 3109, 293, 575, 15245, 278, 13392, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3762984456895273, "compression_ratio": 1.5606060606060606, "no_speech_prob": 5.2252198656788096e-05}, {"id": 1158, "seek": 574398, "start": 5763.98, "end": 5764.98, "text": " Question 2.", "tokens": [51364, 14464, 568, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3762984456895273, "compression_ratio": 1.5606060606060606, "no_speech_prob": 5.2252198656788096e-05}, {"id": 1159, "seek": 574398, "start": 5764.98, "end": 5770.419999999999, "text": " Has anyone tried something like this by averaging or combining the activations of multiple bird", "tokens": [51414, 8646, 2878, 3031, 746, 411, 341, 538, 47308, 420, 21928, 264, 2430, 763, 295, 3866, 5255, 51686], "temperature": 0.0, "avg_logprob": -0.3762984456895273, "compression_ratio": 1.5606060606060606, "no_speech_prob": 5.2252198656788096e-05}, {"id": 1160, "seek": 577042, "start": 5770.42, "end": 5779.9800000000005, "text": " images to create some kind of prototypical or novel bird?", "tokens": [50364, 5267, 281, 1884, 512, 733, 295, 46219, 34061, 420, 7613, 5255, 30, 50842], "temperature": 0.0, "avg_logprob": -0.42590134938557944, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.14607806503772736}, {"id": 1161, "seek": 577042, "start": 5779.9800000000005, "end": 5784.02, "text": " Generative adversarial networks do something like that, but probably not quite.", "tokens": [50842, 15409, 1166, 17641, 44745, 9590, 360, 746, 411, 300, 11, 457, 1391, 406, 1596, 13, 51044], "temperature": 0.0, "avg_logprob": -0.42590134938557944, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.14607806503772736}, {"id": 1162, "seek": 577042, "start": 5784.02, "end": 5788.74, "text": " I'm not sure, maybe not quite.", "tokens": [51044, 286, 478, 406, 988, 11, 1310, 406, 1596, 13, 51280], "temperature": 0.0, "avg_logprob": -0.42590134938557944, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.14607806503772736}, {"id": 1163, "seek": 577042, "start": 5788.74, "end": 5791.7, "text": " Where can people get the pickle file?", "tokens": [51280, 2305, 393, 561, 483, 264, 31433, 3991, 30, 51428], "temperature": 0.0, "avg_logprob": -0.42590134938557944, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.14607806503772736}, {"id": 1164, "seek": 577042, "start": 5791.7, "end": 5793.38, "text": " They don't.", "tokens": [51428, 814, 500, 380, 13, 51512], "temperature": 0.0, "avg_logprob": -0.42590134938557944, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.14607806503772736}, {"id": 1165, "seek": 579338, "start": 5793.38, "end": 5804.5, "text": " You have to get a list of file names yourself from the list of files that you've downloaded.", "tokens": [50364, 509, 362, 281, 483, 257, 1329, 295, 3991, 5288, 1803, 490, 264, 1329, 295, 7098, 300, 291, 600, 21748, 13, 50920], "temperature": 0.0, "avg_logprob": -0.3474279880523682, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.2750934362411499}, {"id": 1166, "seek": 579338, "start": 5804.5, "end": 5810.06, "text": " Just to make sure I understand this, someone says in this example we started with a random", "tokens": [50920, 1449, 281, 652, 988, 286, 1223, 341, 11, 1580, 1619, 294, 341, 1365, 321, 1409, 365, 257, 4974, 51198], "temperature": 0.0, "avg_logprob": -0.3474279880523682, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.2750934362411499}, {"id": 1167, "seek": 579338, "start": 5810.06, "end": 5815.26, "text": " image, but if we started with the actual image as the initial condition, we would get the", "tokens": [51198, 3256, 11, 457, 498, 321, 1409, 365, 264, 3539, 3256, 382, 264, 5883, 4188, 11, 321, 576, 483, 264, 51458], "temperature": 0.0, "avg_logprob": -0.3474279880523682, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.2750934362411499}, {"id": 1168, "seek": 579338, "start": 5815.26, "end": 5816.86, "text": " original image back, right?", "tokens": [51458, 3380, 3256, 646, 11, 558, 30, 51538], "temperature": 0.0, "avg_logprob": -0.3474279880523682, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.2750934362411499}, {"id": 1169, "seek": 579338, "start": 5816.86, "end": 5817.86, "text": " I would assume so.", "tokens": [51538, 286, 576, 6552, 370, 13, 51588], "temperature": 0.0, "avg_logprob": -0.3474279880523682, "compression_ratio": 1.6080402010050252, "no_speech_prob": 0.2750934362411499}, {"id": 1170, "seek": 581786, "start": 5817.86, "end": 5823.78, "text": " I can't see why it wouldn't.", "tokens": [50364, 286, 393, 380, 536, 983, 309, 2759, 380, 13, 50660], "temperature": 0.0, "avg_logprob": -0.4787369134291163, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.4648495316505432}, {"id": 1171, "seek": 581786, "start": 5823.78, "end": 5824.78, "text": " The gradients would all be zero.", "tokens": [50660, 440, 2771, 2448, 576, 439, 312, 4018, 13, 50710], "temperature": 0.0, "avg_logprob": -0.4787369134291163, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.4648495316505432}, {"id": 1172, "seek": 581786, "start": 5824.78, "end": 5825.78, "text": " Question 3.", "tokens": [50710, 14464, 805, 13, 50760], "temperature": 0.0, "avg_logprob": -0.4787369134291163, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.4648495316505432}, {"id": 1173, "seek": 581786, "start": 5825.78, "end": 5847.219999999999, "text": " Would it be useful to use a tool like Quiver to figure out which BGG layer to use for this?", "tokens": [50760, 6068, 309, 312, 4420, 281, 764, 257, 2290, 411, 2326, 1837, 281, 2573, 484, 597, 363, 27561, 4583, 281, 764, 337, 341, 30, 51832], "temperature": 0.0, "avg_logprob": -0.4787369134291163, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.4648495316505432}, {"id": 1174, "seek": 584722, "start": 5847.58, "end": 5853.42, "text": " It's so easy just to try a few and see what works.", "tokens": [50382, 467, 311, 370, 1858, 445, 281, 853, 257, 1326, 293, 536, 437, 1985, 13, 50674], "temperature": 0.0, "avg_logprob": -0.3459009901385441, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.02194807305932045}, {"id": 1175, "seek": 584722, "start": 5853.42, "end": 5858.18, "text": " We're nearly out of time, we haven't got through as much as I hoped, but we're going to finish", "tokens": [50674, 492, 434, 6217, 484, 295, 565, 11, 321, 2378, 380, 658, 807, 382, 709, 382, 286, 19737, 11, 457, 321, 434, 516, 281, 2413, 50912], "temperature": 0.0, "avg_logprob": -0.3459009901385441, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.02194807305932045}, {"id": 1176, "seek": 584722, "start": 5858.18, "end": 5859.18, "text": " off this piece.", "tokens": [50912, 766, 341, 2522, 13, 50962], "temperature": 0.0, "avg_logprob": -0.3459009901385441, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.02194807305932045}, {"id": 1177, "seek": 584722, "start": 5859.18, "end": 5864.18, "text": " We're now going to do fstyle.", "tokens": [50962, 492, 434, 586, 516, 281, 360, 283, 15014, 13, 51212], "temperature": 0.0, "avg_logprob": -0.3459009901385441, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.02194807305932045}, {"id": 1178, "seek": 584722, "start": 5864.18, "end": 5869.860000000001, "text": " fstyle is nearly identical, all of the code is nearly identical.", "tokens": [51212, 283, 15014, 307, 6217, 14800, 11, 439, 295, 264, 3089, 307, 6217, 14800, 13, 51496], "temperature": 0.0, "avg_logprob": -0.3459009901385441, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.02194807305932045}, {"id": 1179, "seek": 584722, "start": 5869.860000000001, "end": 5875.1, "text": " The only thing different is a, we're going to not feed in a photo, we're going to feed", "tokens": [51496, 440, 787, 551, 819, 307, 257, 11, 321, 434, 516, 281, 406, 3154, 294, 257, 5052, 11, 321, 434, 516, 281, 3154, 51758], "temperature": 0.0, "avg_logprob": -0.3459009901385441, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.02194807305932045}, {"id": 1180, "seek": 584722, "start": 5875.1, "end": 5876.1, "text": " in a painting.", "tokens": [51758, 294, 257, 5370, 13, 51808], "temperature": 0.0, "avg_logprob": -0.3459009901385441, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.02194807305932045}, {"id": 1181, "seek": 587610, "start": 5876.18, "end": 5877.700000000001, "text": " Here are a few styles we could choose from.", "tokens": [50368, 1692, 366, 257, 1326, 13273, 321, 727, 2826, 490, 13, 50444], "temperature": 0.0, "avg_logprob": -0.40455191476004465, "compression_ratio": 1.5825688073394495, "no_speech_prob": 0.0015731072053313255}, {"id": 1182, "seek": 587610, "start": 5877.700000000001, "end": 5885.42, "text": " We could do Van Gogh, we could do this drawing, or we could do The Simpsons.", "tokens": [50444, 492, 727, 360, 8979, 39690, 71, 11, 321, 727, 360, 341, 6316, 11, 420, 321, 727, 360, 440, 3998, 1878, 892, 13, 50830], "temperature": 0.0, "avg_logprob": -0.40455191476004465, "compression_ratio": 1.5825688073394495, "no_speech_prob": 0.0015731072053313255}, {"id": 1183, "seek": 587610, "start": 5885.42, "end": 5892.3, "text": " We pick one of those and we create the style array in the same way as before.", "tokens": [50830, 492, 1888, 472, 295, 729, 293, 321, 1884, 264, 3758, 10225, 294, 264, 912, 636, 382, 949, 13, 51174], "temperature": 0.0, "avg_logprob": -0.40455191476004465, "compression_ratio": 1.5825688073394495, "no_speech_prob": 0.0015731072053313255}, {"id": 1184, "seek": 587610, "start": 5892.3, "end": 5893.3, "text": " Chuck it through BGG.", "tokens": [51174, 21607, 309, 807, 363, 27561, 13, 51224], "temperature": 0.0, "avg_logprob": -0.40455191476004465, "compression_ratio": 1.5825688073394495, "no_speech_prob": 0.0015731072053313255}, {"id": 1185, "seek": 587610, "start": 5893.3, "end": 5897.38, "text": " This time though, we're going to use multiple layers.", "tokens": [51224, 639, 565, 1673, 11, 321, 434, 516, 281, 764, 3866, 7914, 13, 51428], "temperature": 0.0, "avg_logprob": -0.40455191476004465, "compression_ratio": 1.5825688073394495, "no_speech_prob": 0.0015731072053313255}, {"id": 1186, "seek": 587610, "start": 5897.38, "end": 5901.620000000001, "text": " So I've created a dictionary from the name of the layer to its output.", "tokens": [51428, 407, 286, 600, 2942, 257, 25890, 490, 264, 1315, 295, 264, 4583, 281, 1080, 5598, 13, 51640], "temperature": 0.0, "avg_logprob": -0.40455191476004465, "compression_ratio": 1.5825688073394495, "no_speech_prob": 0.0015731072053313255}, {"id": 1187, "seek": 590162, "start": 5901.62, "end": 5910.34, "text": " We're going to use that to create an array of a number of the outputs.", "tokens": [50364, 492, 434, 516, 281, 764, 300, 281, 1884, 364, 10225, 295, 257, 1230, 295, 264, 23930, 13, 50800], "temperature": 0.0, "avg_logprob": -0.3496309613424634, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.005301876924932003}, {"id": 1188, "seek": 590162, "start": 5910.34, "end": 5918.82, "text": " We're going to grab the first, second and third block outputs.", "tokens": [50800, 492, 434, 516, 281, 4444, 264, 700, 11, 1150, 293, 2636, 3461, 23930, 13, 51224], "temperature": 0.0, "avg_logprob": -0.3496309613424634, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.005301876924932003}, {"id": 1189, "seek": 590162, "start": 5918.82, "end": 5923.74, "text": " So we're going to create our target as before.", "tokens": [51224, 407, 321, 434, 516, 281, 1884, 527, 3779, 382, 949, 13, 51470], "temperature": 0.0, "avg_logprob": -0.3496309613424634, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.005301876924932003}, {"id": 1190, "seek": 590162, "start": 5923.74, "end": 5928.94, "text": " But we're going to use a different loss function.", "tokens": [51470, 583, 321, 434, 516, 281, 764, 257, 819, 4470, 2445, 13, 51730], "temperature": 0.0, "avg_logprob": -0.3496309613424634, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.005301876924932003}, {"id": 1191, "seek": 592894, "start": 5928.94, "end": 5931.62, "text": " The loss function is called styleLoss.", "tokens": [50364, 440, 4470, 2445, 307, 1219, 3758, 43, 772, 13, 50498], "temperature": 0.0, "avg_logprob": -0.3323706484389031, "compression_ratio": 1.6137566137566137, "no_speech_prob": 0.0045384131371974945}, {"id": 1192, "seek": 592894, "start": 5931.62, "end": 5934.94, "text": " Just like before, it's going to use the MSE.", "tokens": [50498, 1449, 411, 949, 11, 309, 311, 516, 281, 764, 264, 376, 5879, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3323706484389031, "compression_ratio": 1.6137566137566137, "no_speech_prob": 0.0045384131371974945}, {"id": 1193, "seek": 592894, "start": 5934.94, "end": 5940.0599999999995, "text": " But rather than just the MSE on the activations, it's the MSE on something called the Gram", "tokens": [50664, 583, 2831, 813, 445, 264, 376, 5879, 322, 264, 2430, 763, 11, 309, 311, 264, 376, 5879, 322, 746, 1219, 264, 22130, 50920], "temperature": 0.0, "avg_logprob": -0.3323706484389031, "compression_ratio": 1.6137566137566137, "no_speech_prob": 0.0045384131371974945}, {"id": 1194, "seek": 592894, "start": 5940.0599999999995, "end": 5942.74, "text": " matrix of the activations.", "tokens": [50920, 8141, 295, 264, 2430, 763, 13, 51054], "temperature": 0.0, "avg_logprob": -0.3323706484389031, "compression_ratio": 1.6137566137566137, "no_speech_prob": 0.0045384131371974945}, {"id": 1195, "seek": 592894, "start": 5942.74, "end": 5944.379999999999, "text": " What is a Gram matrix?", "tokens": [51054, 708, 307, 257, 22130, 8141, 30, 51136], "temperature": 0.0, "avg_logprob": -0.3323706484389031, "compression_ratio": 1.6137566137566137, "no_speech_prob": 0.0045384131371974945}, {"id": 1196, "seek": 592894, "start": 5944.379999999999, "end": 5953.339999999999, "text": " A Gram matrix is very simply the dot product of a matrix with its own transpose.", "tokens": [51136, 316, 22130, 8141, 307, 588, 2935, 264, 5893, 1674, 295, 257, 8141, 365, 1080, 1065, 25167, 13, 51584], "temperature": 0.0, "avg_logprob": -0.3323706484389031, "compression_ratio": 1.6137566137566137, "no_speech_prob": 0.0045384131371974945}, {"id": 1197, "seek": 595334, "start": 5954.34, "end": 5959.900000000001, "text": " So here it is here, the dot product of some matrix with its own transpose.", "tokens": [50414, 407, 510, 309, 307, 510, 11, 264, 5893, 1674, 295, 512, 8141, 365, 1080, 1065, 25167, 13, 50692], "temperature": 0.0, "avg_logprob": -0.37182019396526056, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.06954031437635422}, {"id": 1198, "seek": 595334, "start": 5959.900000000001, "end": 5965.9400000000005, "text": " I've just got to divide it by here to create an average.", "tokens": [50692, 286, 600, 445, 658, 281, 9845, 309, 538, 510, 281, 1884, 364, 4274, 13, 50994], "temperature": 0.0, "avg_logprob": -0.37182019396526056, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.06954031437635422}, {"id": 1199, "seek": 595334, "start": 5965.9400000000005, "end": 5971.38, "text": " So what is this matrix that we're taking the dot product of and its transpose?", "tokens": [50994, 407, 437, 307, 341, 8141, 300, 321, 434, 1940, 264, 5893, 1674, 295, 293, 1080, 25167, 30, 51266], "temperature": 0.0, "avg_logprob": -0.37182019396526056, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.06954031437635422}, {"id": 1200, "seek": 595334, "start": 5971.38, "end": 5974.22, "text": " What it is, is that we start with our image.", "tokens": [51266, 708, 309, 307, 11, 307, 300, 321, 722, 365, 527, 3256, 13, 51408], "temperature": 0.0, "avg_logprob": -0.37182019396526056, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.06954031437635422}, {"id": 1201, "seek": 595334, "start": 5974.22, "end": 5977.9800000000005, "text": " Remember the image is height by width by channels.", "tokens": [51408, 5459, 264, 3256, 307, 6681, 538, 11402, 538, 9235, 13, 51596], "temperature": 0.0, "avg_logprob": -0.37182019396526056, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.06954031437635422}, {"id": 1202, "seek": 597798, "start": 5977.98, "end": 5984.259999999999, "text": " We change the order of dimensions, so it's channels by height by width.", "tokens": [50364, 492, 1319, 264, 1668, 295, 12819, 11, 370, 309, 311, 9235, 538, 6681, 538, 11402, 13, 50678], "temperature": 0.0, "avg_logprob": -0.2702846743843772, "compression_ratio": 1.7, "no_speech_prob": 0.026355087757110596}, {"id": 1203, "seek": 597798, "start": 5984.259999999999, "end": 5986.62, "text": " And then we do a batch flatten.", "tokens": [50678, 400, 550, 321, 360, 257, 15245, 24183, 13, 50796], "temperature": 0.0, "avg_logprob": -0.2702846743843772, "compression_ratio": 1.7, "no_speech_prob": 0.026355087757110596}, {"id": 1204, "seek": 597798, "start": 5986.62, "end": 5991.5, "text": " So what batch flatten does is it takes everything except the first dimension and flattens it", "tokens": [50796, 407, 437, 15245, 24183, 775, 307, 309, 2516, 1203, 3993, 264, 700, 10139, 293, 932, 1591, 694, 309, 51040], "temperature": 0.0, "avg_logprob": -0.2702846743843772, "compression_ratio": 1.7, "no_speech_prob": 0.026355087757110596}, {"id": 1205, "seek": 597798, "start": 5991.5, "end": 5993.179999999999, "text": " out into a vector.", "tokens": [51040, 484, 666, 257, 8062, 13, 51124], "temperature": 0.0, "avg_logprob": -0.2702846743843772, "compression_ratio": 1.7, "no_speech_prob": 0.026355087757110596}, {"id": 1206, "seek": 597798, "start": 5993.179999999999, "end": 6001.0199999999995, "text": " This is now going to be a matrix where the rows are channels and the columns are a flattened", "tokens": [51124, 639, 307, 586, 516, 281, 312, 257, 8141, 689, 264, 13241, 366, 9235, 293, 264, 13766, 366, 257, 24183, 292, 51516], "temperature": 0.0, "avg_logprob": -0.2702846743843772, "compression_ratio": 1.7, "no_speech_prob": 0.026355087757110596}, {"id": 1207, "seek": 597798, "start": 6001.0199999999995, "end": 6003.62, "text": " version of the height by width.", "tokens": [51516, 3037, 295, 264, 6681, 538, 11402, 13, 51646], "temperature": 0.0, "avg_logprob": -0.2702846743843772, "compression_ratio": 1.7, "no_speech_prob": 0.026355087757110596}, {"id": 1208, "seek": 600362, "start": 6003.62, "end": 6012.86, "text": " So if this is C by H by W, the result of this will be C rows and H times W columns.", "tokens": [50364, 407, 498, 341, 307, 383, 538, 389, 538, 343, 11, 264, 1874, 295, 341, 486, 312, 383, 13241, 293, 389, 1413, 343, 13766, 13, 50826], "temperature": 0.0, "avg_logprob": -0.2541838670388246, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.004007274750620127}, {"id": 1209, "seek": 600362, "start": 6012.86, "end": 6019.88, "text": " So when you take the dot product of something with a transpose of itself, what you're basically", "tokens": [50826, 407, 562, 291, 747, 264, 5893, 1674, 295, 746, 365, 257, 25167, 295, 2564, 11, 437, 291, 434, 1936, 51177], "temperature": 0.0, "avg_logprob": -0.2541838670388246, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.004007274750620127}, {"id": 1210, "seek": 600362, "start": 6019.88, "end": 6023.54, "text": " doing is creating something a lot like a correlation matrix.", "tokens": [51177, 884, 307, 4084, 746, 257, 688, 411, 257, 20009, 8141, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2541838670388246, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.004007274750620127}, {"id": 1211, "seek": 600362, "start": 6023.54, "end": 6031.0599999999995, "text": " You're saying how much is each row similar to each other row.", "tokens": [51360, 509, 434, 1566, 577, 709, 307, 1184, 5386, 2531, 281, 1184, 661, 5386, 13, 51736], "temperature": 0.0, "avg_logprob": -0.2541838670388246, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.004007274750620127}, {"id": 1212, "seek": 603106, "start": 6031.06, "end": 6036.5, "text": " So if you think about it a number of ways.", "tokens": [50364, 407, 498, 291, 519, 466, 309, 257, 1230, 295, 2098, 13, 50636], "temperature": 0.0, "avg_logprob": -0.3151977402823312, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.0017274488927796483}, {"id": 1213, "seek": 603106, "start": 6036.5, "end": 6037.5, "text": " You can think about it like a cosine.", "tokens": [50636, 509, 393, 519, 466, 309, 411, 257, 23565, 13, 50686], "temperature": 0.0, "avg_logprob": -0.3151977402823312, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.0017274488927796483}, {"id": 1214, "seek": 603106, "start": 6037.5, "end": 6042.4400000000005, "text": " A cosine is basically just a dot product.", "tokens": [50686, 316, 23565, 307, 1936, 445, 257, 5893, 1674, 13, 50933], "temperature": 0.0, "avg_logprob": -0.3151977402823312, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.0017274488927796483}, {"id": 1215, "seek": 603106, "start": 6042.4400000000005, "end": 6051.22, "text": " You can think of it as a correlation matrix, it's basically a normalized version of this.", "tokens": [50933, 509, 393, 519, 295, 309, 382, 257, 20009, 8141, 11, 309, 311, 1936, 257, 48704, 3037, 295, 341, 13, 51372], "temperature": 0.0, "avg_logprob": -0.3151977402823312, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.0017274488927796483}, {"id": 1216, "seek": 603106, "start": 6051.22, "end": 6055.26, "text": " So maybe if it's not clear to you, write it down on a piece of paper on the way home tonight.", "tokens": [51372, 407, 1310, 498, 309, 311, 406, 1850, 281, 291, 11, 2464, 309, 760, 322, 257, 2522, 295, 3035, 322, 264, 636, 1280, 4440, 13, 51574], "temperature": 0.0, "avg_logprob": -0.3151977402823312, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.0017274488927796483}, {"id": 1217, "seek": 605526, "start": 6055.26, "end": 6062.66, "text": " Just think about taking the rows of a matrix and then flipping it around and you're basically", "tokens": [50364, 1449, 519, 466, 1940, 264, 13241, 295, 257, 8141, 293, 550, 26886, 309, 926, 293, 291, 434, 1936, 50734], "temperature": 0.0, "avg_logprob": -0.34061018625895184, "compression_ratio": 1.8502415458937198, "no_speech_prob": 0.05500447005033493}, {"id": 1218, "seek": 605526, "start": 6062.66, "end": 6067.46, "text": " then turning them into columns and then you're multiplying the rows by the columns.", "tokens": [50734, 550, 6246, 552, 666, 13766, 293, 550, 291, 434, 30955, 264, 13241, 538, 264, 13766, 13, 50974], "temperature": 0.0, "avg_logprob": -0.34061018625895184, "compression_ratio": 1.8502415458937198, "no_speech_prob": 0.05500447005033493}, {"id": 1219, "seek": 605526, "start": 6067.46, "end": 6073.22, "text": " It's basically the same as taking each row and comparing it to each other row.", "tokens": [50974, 467, 311, 1936, 264, 912, 382, 1940, 1184, 5386, 293, 15763, 309, 281, 1184, 661, 5386, 13, 51262], "temperature": 0.0, "avg_logprob": -0.34061018625895184, "compression_ratio": 1.8502415458937198, "no_speech_prob": 0.05500447005033493}, {"id": 1220, "seek": 605526, "start": 6073.22, "end": 6075.26, "text": " So that's what this Gram matrix is.", "tokens": [51262, 407, 300, 311, 437, 341, 22130, 8141, 307, 13, 51364], "temperature": 0.0, "avg_logprob": -0.34061018625895184, "compression_ratio": 1.8502415458937198, "no_speech_prob": 0.05500447005033493}, {"id": 1221, "seek": 605526, "start": 6075.26, "end": 6084.54, "text": " It's basically saying for every channel, how similar are its values to each other channel.", "tokens": [51364, 467, 311, 1936, 1566, 337, 633, 2269, 11, 577, 2531, 366, 1080, 4190, 281, 1184, 661, 2269, 13, 51828], "temperature": 0.0, "avg_logprob": -0.34061018625895184, "compression_ratio": 1.8502415458937198, "no_speech_prob": 0.05500447005033493}, {"id": 1222, "seek": 608454, "start": 6084.82, "end": 6093.38, "text": " So if channel 1 in most parts of the image is very similar to channel 3 in most parts", "tokens": [50378, 407, 498, 2269, 502, 294, 881, 3166, 295, 264, 3256, 307, 588, 2531, 281, 2269, 805, 294, 881, 3166, 50806], "temperature": 0.0, "avg_logprob": -0.2760327790912829, "compression_ratio": 1.7871287128712872, "no_speech_prob": 8.75026744324714e-05}, {"id": 1223, "seek": 608454, "start": 6093.38, "end": 6100.3, "text": " of the image, then 1,3 of this result will be a higher number.", "tokens": [50806, 295, 264, 3256, 11, 550, 502, 11, 18, 295, 341, 1874, 486, 312, 257, 2946, 1230, 13, 51152], "temperature": 0.0, "avg_logprob": -0.2760327790912829, "compression_ratio": 1.7871287128712872, "no_speech_prob": 8.75026744324714e-05}, {"id": 1224, "seek": 608454, "start": 6100.3, "end": 6102.22, "text": " So it's kind of a weird matrix.", "tokens": [51152, 407, 309, 311, 733, 295, 257, 3657, 8141, 13, 51248], "temperature": 0.0, "avg_logprob": -0.2760327790912829, "compression_ratio": 1.7871287128712872, "no_speech_prob": 8.75026744324714e-05}, {"id": 1225, "seek": 608454, "start": 6102.22, "end": 6109.62, "text": " It basically tells us, it's like a fingerprint of how the channels relate to us, each other,", "tokens": [51248, 467, 1936, 5112, 505, 11, 309, 311, 411, 257, 30715, 295, 577, 264, 9235, 10961, 281, 505, 11, 1184, 661, 11, 51618], "temperature": 0.0, "avg_logprob": -0.2760327790912829, "compression_ratio": 1.7871287128712872, "no_speech_prob": 8.75026744324714e-05}, {"id": 1226, "seek": 608454, "start": 6109.62, "end": 6113.7, "text": " in this particular image, or how the filters relate to each other in a particular layer", "tokens": [51618, 294, 341, 1729, 3256, 11, 420, 577, 264, 15995, 10961, 281, 1184, 661, 294, 257, 1729, 4583, 51822], "temperature": 0.0, "avg_logprob": -0.2760327790912829, "compression_ratio": 1.7871287128712872, "no_speech_prob": 8.75026744324714e-05}, {"id": 1227, "seek": 611370, "start": 6113.86, "end": 6114.86, "text": " of this particular image.", "tokens": [50372, 295, 341, 1729, 3256, 13, 50422], "temperature": 0.0, "avg_logprob": -0.28941127232142855, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.006797503679990768}, {"id": 1228, "seek": 611370, "start": 6114.86, "end": 6120.94, "text": " I think the most important thing to recognize is that there is no geometry left here at", "tokens": [50422, 286, 519, 264, 881, 1021, 551, 281, 5521, 307, 300, 456, 307, 572, 18426, 1411, 510, 412, 50726], "temperature": 0.0, "avg_logprob": -0.28941127232142855, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.006797503679990768}, {"id": 1229, "seek": 611370, "start": 6120.94, "end": 6121.94, "text": " all.", "tokens": [50726, 439, 13, 50776], "temperature": 0.0, "avg_logprob": -0.28941127232142855, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.006797503679990768}, {"id": 1230, "seek": 611370, "start": 6121.94, "end": 6125.58, "text": " The x and the y coordinates are totally thrown away.", "tokens": [50776, 440, 2031, 293, 264, 288, 21056, 366, 3879, 11732, 1314, 13, 50958], "temperature": 0.0, "avg_logprob": -0.28941127232142855, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.006797503679990768}, {"id": 1231, "seek": 611370, "start": 6125.58, "end": 6128.58, "text": " They're actually flattened out.", "tokens": [50958, 814, 434, 767, 24183, 292, 484, 13, 51108], "temperature": 0.0, "avg_logprob": -0.28941127232142855, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.006797503679990768}, {"id": 1232, "seek": 611370, "start": 6128.58, "end": 6136.22, "text": " So this loss function can by definition in no way at all contain anything about the content", "tokens": [51108, 407, 341, 4470, 2445, 393, 538, 7123, 294, 572, 636, 412, 439, 5304, 1340, 466, 264, 2701, 51490], "temperature": 0.0, "avg_logprob": -0.28941127232142855, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.006797503679990768}, {"id": 1233, "seek": 611370, "start": 6136.22, "end": 6140.82, "text": " of the image because it's thrown away all of the x and y information.", "tokens": [51490, 295, 264, 3256, 570, 309, 311, 11732, 1314, 439, 295, 264, 2031, 293, 288, 1589, 13, 51720], "temperature": 0.0, "avg_logprob": -0.28941127232142855, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.006797503679990768}, {"id": 1234, "seek": 614082, "start": 6140.94, "end": 6146.299999999999, "text": " And all that's left is some kind of fingerprint of how the channels relate to each other,", "tokens": [50370, 400, 439, 300, 311, 1411, 307, 512, 733, 295, 30715, 295, 577, 264, 9235, 10961, 281, 1184, 661, 11, 50638], "temperature": 0.0, "avg_logprob": -0.3473362922668457, "compression_ratio": 1.768041237113402, "no_speech_prob": 0.017712226137518883}, {"id": 1235, "seek": 614082, "start": 6146.299999999999, "end": 6149.74, "text": " or how the filters relate to each other.", "tokens": [50638, 420, 577, 264, 15995, 10961, 281, 1184, 661, 13, 50810], "temperature": 0.0, "avg_logprob": -0.3473362922668457, "compression_ratio": 1.768041237113402, "no_speech_prob": 0.017712226137518883}, {"id": 1236, "seek": 614082, "start": 6149.74, "end": 6156.66, "text": " So this style loss then says for 2 different images, how do these fingerprints differ,", "tokens": [50810, 407, 341, 3758, 4470, 550, 1619, 337, 568, 819, 5267, 11, 577, 360, 613, 42170, 743, 11, 51156], "temperature": 0.0, "avg_logprob": -0.3473362922668457, "compression_ratio": 1.768041237113402, "no_speech_prob": 0.017712226137518883}, {"id": 1237, "seek": 614082, "start": 6156.66, "end": 6159.78, "text": " how similar are these fingerprints.", "tokens": [51156, 577, 2531, 366, 613, 42170, 13, 51312], "temperature": 0.0, "avg_logprob": -0.3473362922668457, "compression_ratio": 1.768041237113402, "no_speech_prob": 0.017712226137518883}, {"id": 1238, "seek": 614082, "start": 6159.78, "end": 6164.62, "text": " So it turns out that if you now do the exact same steps as before, using that as our loss", "tokens": [51312, 407, 309, 4523, 484, 300, 498, 291, 586, 360, 264, 1900, 912, 4439, 382, 949, 11, 1228, 300, 382, 527, 4470, 51554], "temperature": 0.0, "avg_logprob": -0.3473362922668457, "compression_ratio": 1.768041237113402, "no_speech_prob": 0.017712226137518883}, {"id": 1239, "seek": 616462, "start": 6164.62, "end": 6173.7, "text": " function, and you run it through a few iterations, it looks like that.", "tokens": [50364, 2445, 11, 293, 291, 1190, 309, 807, 257, 1326, 36540, 11, 309, 1542, 411, 300, 13, 50818], "temperature": 0.0, "avg_logprob": -0.30730597826899314, "compression_ratio": 1.36, "no_speech_prob": 0.49606454372406006}, {"id": 1240, "seek": 616462, "start": 6173.7, "end": 6184.22, "text": " It looks a lot like the original bang-off, but without any of the content.", "tokens": [50818, 467, 1542, 257, 688, 411, 264, 3380, 8550, 12, 4506, 11, 457, 1553, 604, 295, 264, 2701, 13, 51344], "temperature": 0.0, "avg_logprob": -0.30730597826899314, "compression_ratio": 1.36, "no_speech_prob": 0.49606454372406006}, {"id": 1241, "seek": 616462, "start": 6184.22, "end": 6188.22, "text": " So the question is, why?", "tokens": [51344, 407, 264, 1168, 307, 11, 983, 30, 51544], "temperature": 0.0, "avg_logprob": -0.30730597826899314, "compression_ratio": 1.36, "no_speech_prob": 0.49606454372406006}, {"id": 1242, "seek": 618822, "start": 6188.22, "end": 6194.66, "text": " The answer is, nobody the fuck knows.", "tokens": [50364, 440, 1867, 307, 11, 5079, 264, 3275, 3255, 13, 50686], "temperature": 0.0, "avg_logprob": -0.2998634431420303, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.0007793650147505105}, {"id": 1243, "seek": 618822, "start": 6194.66, "end": 6200.38, "text": " So a paper just came out 2 weeks ago called Demystifying Neural Style Transfer with a", "tokens": [50686, 407, 257, 3035, 445, 1361, 484, 568, 3259, 2057, 1219, 413, 3633, 372, 5489, 1734, 1807, 27004, 35025, 365, 257, 50972], "temperature": 0.0, "avg_logprob": -0.2998634431420303, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.0007793650147505105}, {"id": 1244, "seek": 618822, "start": 6200.38, "end": 6204.780000000001, "text": " mathematical treatment where they claim to have an answer to this question.", "tokens": [50972, 18894, 5032, 689, 436, 3932, 281, 362, 364, 1867, 281, 341, 1168, 13, 51192], "temperature": 0.0, "avg_logprob": -0.2998634431420303, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.0007793650147505105}, {"id": 1245, "seek": 618822, "start": 6204.780000000001, "end": 6213.26, "text": " But as the point at which this was created, a year and a half ago, until now, no one really", "tokens": [51192, 583, 382, 264, 935, 412, 597, 341, 390, 2942, 11, 257, 1064, 293, 257, 1922, 2057, 11, 1826, 586, 11, 572, 472, 534, 51616], "temperature": 0.0, "avg_logprob": -0.2998634431420303, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.0007793650147505105}, {"id": 1246, "seek": 618822, "start": 6213.26, "end": 6216.1, "text": " knows why that happens.", "tokens": [51616, 3255, 983, 300, 2314, 13, 51758], "temperature": 0.0, "avg_logprob": -0.2998634431420303, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.0007793650147505105}, {"id": 1247, "seek": 621610, "start": 6216.1, "end": 6221.14, "text": " But the important thing the authors of this paper realized is if we can create a function", "tokens": [50364, 583, 264, 1021, 551, 264, 16552, 295, 341, 3035, 5334, 307, 498, 321, 393, 1884, 257, 2445, 50616], "temperature": 0.0, "avg_logprob": -0.34185250600179035, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.038465410470962524}, {"id": 1248, "seek": 621610, "start": 6221.14, "end": 6225.700000000001, "text": " that gives you content loss and a function that gives you style loss, and you add the", "tokens": [50616, 300, 2709, 291, 2701, 4470, 293, 257, 2445, 300, 2709, 291, 3758, 4470, 11, 293, 291, 909, 264, 50844], "temperature": 0.0, "avg_logprob": -0.34185250600179035, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.038465410470962524}, {"id": 1249, "seek": 621610, "start": 6225.700000000001, "end": 6229.740000000001, "text": " two together and optimize them, you can do neural style.", "tokens": [50844, 732, 1214, 293, 19719, 552, 11, 291, 393, 360, 18161, 3758, 13, 51046], "temperature": 0.0, "avg_logprob": -0.34185250600179035, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.038465410470962524}, {"id": 1250, "seek": 621610, "start": 6229.740000000001, "end": 6233.38, "text": " So all I can assume, they don't say how they did it in the paper, all I can assume is that", "tokens": [51046, 407, 439, 286, 393, 6552, 11, 436, 500, 380, 584, 577, 436, 630, 309, 294, 264, 3035, 11, 439, 286, 393, 6552, 307, 300, 51228], "temperature": 0.0, "avg_logprob": -0.34185250600179035, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.038465410470962524}, {"id": 1251, "seek": 621610, "start": 6233.38, "end": 6235.42, "text": " they tried a few different things.", "tokens": [51228, 436, 3031, 257, 1326, 819, 721, 13, 51330], "temperature": 0.0, "avg_logprob": -0.34185250600179035, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.038465410470962524}, {"id": 1252, "seek": 621610, "start": 6235.42, "end": 6239.14, "text": " They knew that they had to throw away all of the geometry, so they probably tried a", "tokens": [51330, 814, 2586, 300, 436, 632, 281, 3507, 1314, 439, 295, 264, 18426, 11, 370, 436, 1391, 3031, 257, 51516], "temperature": 0.0, "avg_logprob": -0.34185250600179035, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.038465410470962524}, {"id": 1253, "seek": 621610, "start": 6239.14, "end": 6240.820000000001, "text": " few things and threw away the geometry.", "tokens": [51516, 1326, 721, 293, 11918, 1314, 264, 18426, 13, 51600], "temperature": 0.0, "avg_logprob": -0.34185250600179035, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.038465410470962524}, {"id": 1254, "seek": 624082, "start": 6240.82, "end": 6246.219999999999, "text": " At some point they looked at this and said, holy shit, that's it.", "tokens": [50364, 1711, 512, 935, 436, 2956, 412, 341, 293, 848, 11, 10622, 4611, 11, 300, 311, 309, 13, 50634], "temperature": 0.0, "avg_logprob": -0.37666104390070987, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.392303466796875}, {"id": 1255, "seek": 624082, "start": 6246.219999999999, "end": 6254.5, "text": " So now that we have this magical thing, there's the Simpsons, all we have to do is add the", "tokens": [50634, 407, 586, 300, 321, 362, 341, 12066, 551, 11, 456, 311, 264, 3998, 1878, 892, 11, 439, 321, 362, 281, 360, 307, 909, 264, 51048], "temperature": 0.0, "avg_logprob": -0.37666104390070987, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.392303466796875}, {"id": 1256, "seek": 624082, "start": 6254.5, "end": 6255.5, "text": " two together.", "tokens": [51048, 732, 1214, 13, 51098], "temperature": 0.0, "avg_logprob": -0.37666104390070987, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.392303466796875}, {"id": 1257, "seek": 624082, "start": 6255.5, "end": 6259.42, "text": " Here's our bird, which I'll call Source.", "tokens": [51098, 1692, 311, 527, 5255, 11, 597, 286, 603, 818, 29629, 13, 51294], "temperature": 0.0, "avg_logprob": -0.37666104390070987, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.392303466796875}, {"id": 1258, "seek": 624082, "start": 6259.42, "end": 6265.58, "text": " We've got our style layers, I'm actually going to take the top 5 now.", "tokens": [51294, 492, 600, 658, 527, 3758, 7914, 11, 286, 478, 767, 516, 281, 747, 264, 1192, 1025, 586, 13, 51602], "temperature": 0.0, "avg_logprob": -0.37666104390070987, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.392303466796875}, {"id": 1259, "seek": 624082, "start": 6265.58, "end": 6270.0599999999995, "text": " Here's our content layer, I'm going to take block4conv2.", "tokens": [51602, 1692, 311, 527, 2701, 4583, 11, 286, 478, 516, 281, 747, 3461, 19, 1671, 85, 17, 13, 51826], "temperature": 0.0, "avg_logprob": -0.37666104390070987, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.392303466796875}, {"id": 1260, "seek": 627006, "start": 6270.1, "end": 6275.3, "text": " As promised, for our loss function, I'm just going to add the two together.", "tokens": [50366, 1018, 10768, 11, 337, 527, 4470, 2445, 11, 286, 478, 445, 516, 281, 909, 264, 732, 1214, 13, 50626], "temperature": 0.0, "avg_logprob": -0.34192917897151065, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.005139591172337532}, {"id": 1261, "seek": 627006, "start": 6275.3, "end": 6281.9400000000005, "text": " Style loss for all of the style layers plus the content loss.", "tokens": [50626, 27004, 4470, 337, 439, 295, 264, 3758, 7914, 1804, 264, 2701, 4470, 13, 50958], "temperature": 0.0, "avg_logprob": -0.34192917897151065, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.005139591172337532}, {"id": 1262, "seek": 627006, "start": 6281.9400000000005, "end": 6283.900000000001, "text": " And I'm going to divide the content loss by 10.", "tokens": [50958, 400, 286, 478, 516, 281, 9845, 264, 2701, 4470, 538, 1266, 13, 51056], "temperature": 0.0, "avg_logprob": -0.34192917897151065, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.005139591172337532}, {"id": 1263, "seek": 627006, "start": 6283.900000000001, "end": 6285.42, "text": " This is something you can play with.", "tokens": [51056, 639, 307, 746, 291, 393, 862, 365, 13, 51132], "temperature": 0.0, "avg_logprob": -0.34192917897151065, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.005139591172337532}, {"id": 1264, "seek": 627006, "start": 6285.42, "end": 6288.5, "text": " In the paper you'll see they play with it.", "tokens": [51132, 682, 264, 3035, 291, 603, 536, 436, 862, 365, 309, 13, 51286], "temperature": 0.0, "avg_logprob": -0.34192917897151065, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.005139591172337532}, {"id": 1265, "seek": 627006, "start": 6288.5, "end": 6290.9400000000005, "text": " How much style loss versus how much content loss.", "tokens": [51286, 1012, 709, 3758, 4470, 5717, 577, 709, 2701, 4470, 13, 51408], "temperature": 0.0, "avg_logprob": -0.34192917897151065, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.005139591172337532}, {"id": 1266, "seek": 627006, "start": 6290.9400000000005, "end": 6299.1, "text": " Get the gradients, evaluator, solve it, and there it is.", "tokens": [51408, 3240, 264, 2771, 2448, 11, 6133, 1639, 11, 5039, 309, 11, 293, 456, 309, 307, 13, 51816], "temperature": 0.0, "avg_logprob": -0.34192917897151065, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.005139591172337532}, {"id": 1267, "seek": 629910, "start": 6299.14, "end": 6305.54, "text": " So other than the fact that we don't really know why the style loss works, but it does,", "tokens": [50366, 407, 661, 813, 264, 1186, 300, 321, 500, 380, 534, 458, 983, 264, 3758, 4470, 1985, 11, 457, 309, 775, 11, 50686], "temperature": 0.0, "avg_logprob": -0.3913855596419868, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.004982255399227142}, {"id": 1268, "seek": 629910, "start": 6305.54, "end": 6307.38, "text": " everything else kind of fits together.", "tokens": [50686, 1203, 1646, 733, 295, 9001, 1214, 13, 50778], "temperature": 0.0, "avg_logprob": -0.3913855596419868, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.004982255399227142}, {"id": 1269, "seek": 629910, "start": 6307.38, "end": 6312.18, "text": " So there's the bird as Van Gogh, there's the bird as the Simpsons, and there's the bird", "tokens": [50778, 407, 456, 311, 264, 5255, 382, 8979, 39690, 71, 11, 456, 311, 264, 5255, 382, 264, 3998, 1878, 892, 11, 293, 456, 311, 264, 5255, 51018], "temperature": 0.0, "avg_logprob": -0.3913855596419868, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.004982255399227142}, {"id": 1270, "seek": 629910, "start": 6312.18, "end": 6315.18, "text": " in the style of a bird picture.", "tokens": [51018, 294, 264, 3758, 295, 257, 5255, 3036, 13, 51168], "temperature": 0.0, "avg_logprob": -0.3913855596419868, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.004982255399227142}, {"id": 1271, "seek": 629910, "start": 6315.18, "end": 6316.22, "text": " Question 2", "tokens": [51168, 14464, 568, 51220], "temperature": 0.0, "avg_logprob": -0.3913855596419868, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.004982255399227142}, {"id": 1272, "seek": 629910, "start": 6316.22, "end": 6317.22, "text": " Question 2", "tokens": [51220, 14464, 568, 51270], "temperature": 0.0, "avg_logprob": -0.3913855596419868, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.004982255399227142}, {"id": 1273, "seek": 629910, "start": 6317.22, "end": 6318.22, "text": " Question 2", "tokens": [51270, 14464, 568, 51320], "temperature": 0.0, "avg_logprob": -0.3913855596419868, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.004982255399227142}, {"id": 1274, "seek": 629910, "start": 6318.22, "end": 6322.9400000000005, "text": " Since the publication of that paper, has anyone used any other loss functions for fstyle that", "tokens": [51320, 4162, 264, 19953, 295, 300, 3035, 11, 575, 2878, 1143, 604, 661, 4470, 6828, 337, 283, 15014, 300, 51556], "temperature": 0.0, "avg_logprob": -0.3913855596419868, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.004982255399227142}, {"id": 1275, "seek": 629910, "start": 6322.9400000000005, "end": 6323.9400000000005, "text": " achieve similar results?", "tokens": [51556, 4584, 2531, 3542, 30, 51606], "temperature": 0.0, "avg_logprob": -0.3913855596419868, "compression_ratio": 1.7456140350877194, "no_speech_prob": 0.004982255399227142}, {"id": 1276, "seek": 632394, "start": 6323.94, "end": 6324.94, "text": " Answer", "tokens": [50364, 24545, 50414], "temperature": 0.0, "avg_logprob": -0.3221397691100608, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.05261782929301262}, {"id": 1277, "seek": 632394, "start": 6324.94, "end": 6328.419999999999, "text": " Yes, so as I mentioned just a couple of weeks ago, there was a paper, I'll put it on the", "tokens": [50414, 1079, 11, 370, 382, 286, 2835, 445, 257, 1916, 295, 3259, 2057, 11, 456, 390, 257, 3035, 11, 286, 603, 829, 309, 322, 264, 50588], "temperature": 0.0, "avg_logprob": -0.3221397691100608, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.05261782929301262}, {"id": 1278, "seek": 632394, "start": 6328.419999999999, "end": 6332.099999999999, "text": " forum, that tries to generalize this loss function.", "tokens": [50588, 17542, 11, 300, 9898, 281, 2674, 1125, 341, 4470, 2445, 13, 50772], "temperature": 0.0, "avg_logprob": -0.3221397691100608, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.05261782929301262}, {"id": 1279, "seek": 632394, "start": 6332.099999999999, "end": 6336.339999999999, "text": " It turns out actually that this particular loss function seems to be about the best that", "tokens": [50772, 467, 4523, 484, 767, 300, 341, 1729, 4470, 2445, 2544, 281, 312, 466, 264, 1151, 300, 50984], "temperature": 0.0, "avg_logprob": -0.3221397691100608, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.05261782929301262}, {"id": 1280, "seek": 632394, "start": 6336.339999999999, "end": 6339.219999999999, "text": " they could come up with.", "tokens": [50984, 436, 727, 808, 493, 365, 13, 51128], "temperature": 0.0, "avg_logprob": -0.3221397691100608, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.05261782929301262}, {"id": 1281, "seek": 632394, "start": 6339.219999999999, "end": 6342.7, "text": " So it's 9 o'clock, so we have run out of time.", "tokens": [51128, 407, 309, 311, 1722, 277, 6, 9023, 11, 370, 321, 362, 1190, 484, 295, 565, 13, 51302], "temperature": 0.0, "avg_logprob": -0.3221397691100608, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.05261782929301262}, {"id": 1282, "seek": 632394, "start": 6342.7, "end": 6345.94, "text": " So we're going to move some of this lesson to the next lesson.", "tokens": [51302, 407, 321, 434, 516, 281, 1286, 512, 295, 341, 6898, 281, 264, 958, 6898, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3221397691100608, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.05261782929301262}, {"id": 1283, "seek": 632394, "start": 6345.94, "end": 6351.0199999999995, "text": " But to give you a sense of where we're going to head, what we're going to do is we're going", "tokens": [51464, 583, 281, 976, 291, 257, 2020, 295, 689, 321, 434, 516, 281, 1378, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 51718], "temperature": 0.0, "avg_logprob": -0.3221397691100608, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.05261782929301262}, {"id": 1284, "seek": 635102, "start": 6351.02, "end": 6356.02, "text": " to take this thing where you have to optimize every single image separately, and we're going", "tokens": [50364, 281, 747, 341, 551, 689, 291, 362, 281, 19719, 633, 2167, 3256, 14759, 11, 293, 321, 434, 516, 50614], "temperature": 0.0, "avg_logprob": -0.2696186701456706, "compression_ratio": 1.924, "no_speech_prob": 0.2393333613872528}, {"id": 1285, "seek": 635102, "start": 6356.02, "end": 6363.620000000001, "text": " to train a CNN, and we're going to train a CNN which will learn how to turn a picture", "tokens": [50614, 281, 3847, 257, 24859, 11, 293, 321, 434, 516, 281, 3847, 257, 24859, 597, 486, 1466, 577, 281, 1261, 257, 3036, 50994], "temperature": 0.0, "avg_logprob": -0.2696186701456706, "compression_ratio": 1.924, "no_speech_prob": 0.2393333613872528}, {"id": 1286, "seek": 635102, "start": 6363.620000000001, "end": 6365.860000000001, "text": " into a Van Gogh version of that picture.", "tokens": [50994, 666, 257, 8979, 39690, 71, 3037, 295, 300, 3036, 13, 51106], "temperature": 0.0, "avg_logprob": -0.2696186701456706, "compression_ratio": 1.924, "no_speech_prob": 0.2393333613872528}, {"id": 1287, "seek": 635102, "start": 6365.860000000001, "end": 6369.3, "text": " So that's basically going to be what we're going to learn next time.", "tokens": [51106, 407, 300, 311, 1936, 516, 281, 312, 437, 321, 434, 516, 281, 1466, 958, 565, 13, 51278], "temperature": 0.0, "avg_logprob": -0.2696186701456706, "compression_ratio": 1.924, "no_speech_prob": 0.2393333613872528}, {"id": 1288, "seek": 635102, "start": 6369.3, "end": 6373.22, "text": " And we're also going to learn about adversarial networks, which is where we're going to create", "tokens": [51278, 400, 321, 434, 611, 516, 281, 1466, 466, 17641, 44745, 9590, 11, 597, 307, 689, 321, 434, 516, 281, 1884, 51474], "temperature": 0.0, "avg_logprob": -0.2696186701456706, "compression_ratio": 1.924, "no_speech_prob": 0.2393333613872528}, {"id": 1289, "seek": 635102, "start": 6373.22, "end": 6374.700000000001, "text": " two networks.", "tokens": [51474, 732, 9590, 13, 51548], "temperature": 0.0, "avg_logprob": -0.2696186701456706, "compression_ratio": 1.924, "no_speech_prob": 0.2393333613872528}, {"id": 1290, "seek": 635102, "start": 6374.700000000001, "end": 6379.820000000001, "text": " One will be designed to generate pictures like this, and the other will be designed", "tokens": [51548, 1485, 486, 312, 4761, 281, 8460, 5242, 411, 341, 11, 293, 264, 661, 486, 312, 4761, 51804], "temperature": 0.0, "avg_logprob": -0.2696186701456706, "compression_ratio": 1.924, "no_speech_prob": 0.2393333613872528}, {"id": 1291, "seek": 637982, "start": 6379.82, "end": 6386.62, "text": " to try and classify whether this is a real Simpsons picture or a fake Simpsons picture.", "tokens": [50364, 281, 853, 293, 33872, 1968, 341, 307, 257, 957, 3998, 1878, 892, 3036, 420, 257, 7592, 3998, 1878, 892, 3036, 13, 50704], "temperature": 0.0, "avg_logprob": -0.27185606956481934, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.009559359401464462}, {"id": 1292, "seek": 637982, "start": 6386.62, "end": 6391.7, "text": " And then you'll do one, generate, the other, discriminate, generate, discriminate.", "tokens": [50704, 400, 550, 291, 603, 360, 472, 11, 8460, 11, 264, 661, 11, 47833, 11, 8460, 11, 47833, 13, 50958], "temperature": 0.0, "avg_logprob": -0.27185606956481934, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.009559359401464462}, {"id": 1293, "seek": 637982, "start": 6391.7, "end": 6399.299999999999, "text": " And by doing that, we can take any generative model and make it better by basically having", "tokens": [50958, 400, 538, 884, 300, 11, 321, 393, 747, 604, 1337, 1166, 2316, 293, 652, 309, 1101, 538, 1936, 1419, 51338], "temperature": 0.0, "avg_logprob": -0.27185606956481934, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.009559359401464462}, {"id": 1294, "seek": 637982, "start": 6399.299999999999, "end": 6405.179999999999, "text": " something else learn to pick the difference between it, the real, and the fake.", "tokens": [51338, 746, 1646, 1466, 281, 1888, 264, 2649, 1296, 309, 11, 264, 957, 11, 293, 264, 7592, 13, 51632], "temperature": 0.0, "avg_logprob": -0.27185606956481934, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.009559359401464462}, {"id": 1295, "seek": 637982, "start": 6405.179999999999, "end": 6408.179999999999, "text": " Then finally we're going to learn about a particular thing that came out 3 weeks ago", "tokens": [51632, 1396, 2721, 321, 434, 516, 281, 1466, 466, 257, 1729, 551, 300, 1361, 484, 805, 3259, 2057, 51782], "temperature": 0.0, "avg_logprob": -0.27185606956481934, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.009559359401464462}, {"id": 1296, "seek": 640818, "start": 6408.34, "end": 6414.9400000000005, "text": " called the Wasserstein GAN, which is the reason I actually decided to move all this forward.", "tokens": [50372, 1219, 264, 17351, 9089, 460, 1770, 11, 597, 307, 264, 1778, 286, 767, 3047, 281, 1286, 439, 341, 2128, 13, 50702], "temperature": 0.0, "avg_logprob": -0.2851344098101605, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.004264353308826685}, {"id": 1297, "seek": 640818, "start": 6414.9400000000005, "end": 6418.9400000000005, "text": " Generative adversarial networks basically didn't work very well at all until about 3", "tokens": [50702, 15409, 1166, 17641, 44745, 9590, 1936, 994, 380, 589, 588, 731, 412, 439, 1826, 466, 805, 50902], "temperature": 0.0, "avg_logprob": -0.2851344098101605, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.004264353308826685}, {"id": 1298, "seek": 640818, "start": 6418.9400000000005, "end": 6419.9400000000005, "text": " weeks ago.", "tokens": [50902, 3259, 2057, 13, 50952], "temperature": 0.0, "avg_logprob": -0.2851344098101605, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.004264353308826685}, {"id": 1299, "seek": 640818, "start": 6419.9400000000005, "end": 6424.34, "text": " Now that they do work, suddenly there's a shitload of stuff that nobody's done yet which", "tokens": [50952, 823, 300, 436, 360, 589, 11, 5800, 456, 311, 257, 4611, 2907, 295, 1507, 300, 5079, 311, 1096, 1939, 597, 51172], "temperature": 0.0, "avg_logprob": -0.2851344098101605, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.004264353308826685}, {"id": 1300, "seek": 640818, "start": 6424.34, "end": 6426.280000000001, "text": " you can do for the first time.", "tokens": [51172, 291, 393, 360, 337, 264, 700, 565, 13, 51269], "temperature": 0.0, "avg_logprob": -0.2851344098101605, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.004264353308826685}, {"id": 1301, "seek": 640818, "start": 6426.280000000001, "end": 6428.66, "text": " So we're going to look at that next week.", "tokens": [51269, 407, 321, 434, 516, 281, 574, 412, 300, 958, 1243, 13, 51388], "temperature": 0.0, "avg_logprob": -0.2851344098101605, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.004264353308826685}], "language": "en"}