{"text": " Alright, I'm going to go ahead and get started. So yeah, just to announce again, I won't be here next week, so no class on Tuesday. And then Thursday there will be the test at the final exam during the normal class time, and David Uminski will be here to proctor that. On Tuesday, the final draft of the blog post is due, as well as homework three. And homework three is just a single question. Yes. And I won't have my regular office hours tomorrow afternoon, but I could meet with you either this afternoon or tomorrow morning if you need to. So just ask me after class. And then I just wanted to thank everyone for being such an engaged group of students. I've really enjoyed teaching this class. This has been fun. So thank you. Yeah, so we'll get into it. We've run out of time a little bit, so we're not going to fully cover lesson seven, but I did want to go a little bit further in it. Just to remember kind of what we did last time, we were looking at...oh. Thanks for pointing that out. Thank you. This will make class a lot easier to follow. Yeah, so last time we talked about eigen-decompositions, and there's kind of this close relationship between the eigen-decomposition and the SVD. And so even though we're looking specifically at how to do eigen-decompositions, similar techniques are used for calculating the SVD. And so we were looking at this dbPedia dataset, which is a dataset of all the Wikipedia links, and we are finding the kind of the principal eigenvector of that. And what's significant about that eigenvector? What information is it giving us? I think the most significant is that we can easily find any power of the matrix just by multiplying the eigenvalues and finding this diagonal of the matrix. So that's something that's really powerful about having the full eigen decomposition, is that you can quickly take powers of matrices. Yes. And then what's kind of the meaning? So we're actually just finding...so you need kind of all the eigenvectors and eigenvalues to do that. For the dbPedia dataset, we're just finding a single eigenvector. What's the meaning of that eigenvector? Matthew and Valentin, can you throw the microphone to Matthew McCalland? Good ducking. You're talking about the solution vector? Yes. Isn't that the probability, I guess the overall probability that it will end there if you do a random walk? Yes, so it is kind of telling you the probability that you're going to be on each page if you're randomly walking. So what does that mean for ones that have higher probabilities? Maybe it's more likely to end there. Yeah, so this is kind of the idea behind PageRank, which is Google's original algorithm, but it's giving you the most important pages because people are more likely to end up there. Thank you. So yeah, that's kind of the idea that even though it's a single eigenvector, it's actually pretty important because it's giving you... some people call that the stationary distribution when you have a Markov chain of kind of where things will end up. And for webpages, that's kind of the relative importance. Is there some meaning to the actual values in the matrix? Yeah, so we talked about... we had kind of our original matrix, which was the graph adjacency matrix, and those were zeros and ones. And there when you take values... so once you start taking powers, they can kind of be any integer values. And that's giving you the number of ways you could get from page A to page B in K steps. So if you took the third power, that's kind of saying how many ways are there to get from A to B taking three hops. So you could go... yeah. So you could like normalize, you could look at one and be like... Yeah, exactly. Yeah, so then it becomes a probability when you normalize. Yeah, good question. Yeah, and so the method we looked at last time, the power method... Go down to it. I would just highlight again, since we've talked about sparse matrices a lot, here we kind of very naturally had our data in the coordinate sparse form, since we just know kind of this page links to this page, and we don't explicitly have... This is all the pages that it doesn't link to, and there's not really a point in storing all that. So we kind of use coordinate form to create a sparse matrix, and then we switch to compressed sparse row format. Why did we switch? Sam? Because it takes half as many reads from memory. Exactly. So for matrix multiplication, it's the same number of floating point operations, but half as many reads from memory to be in sparse row format as opposed to coordinate, sparse coordinate format. Yeah, and the kind of general idea behind the power method is that we're multiplying A times... I call it the scores because it's kind of the score of how important each page is, but you could also think of that as the probability of a person being on a particular page, and we're just doing that a bunch of times, but normalizing to make sure that things don't go to zero or to infinity. Oh, and then something I wanted to point out is, if you remember back in lesson three when we did surveillance video background removal, we used Facebook's fast randomized PCA SVD library, FBPCA, and in the source code I found a comment where they're using the power method at one point, which I thought was exciting, so showing up in the real world. Any questions on that and what we did last time? Okay, so we'll go on to the QR algorithm. So last time we were just finding a single eigenvector, and the QR algorithm lets us find potentially all the eigenvectors. Oh, and this is a paper called the second eigenvalue of the Google matrix, which I thought was neat, and it kind of lists some benefits of what you can get from even just the second eigenvalue, and that's that it relates to the convergence rate as well as the stability given kind of as the structure of the web's changing, detection of spammers, and for the design of algorithms to speed up paint-drink. You can check that out for more information. This is, yeah, again, kind of more practical uses of the second eigenvalue. So I just want to avoid confusion. So right now we're going to talk about the QR algorithm. Later in class we'll talk about the QR decomposition, and the QR decomposition is something we've been using from time to time, and the decomposition is what takes A and gives you back Q and R, where Q has orthonormal columns in the reduced form, or for the full form it's an orthonormal matrix, and R is upper triangular. And then the QR algorithm, which is different, uses the QR decomposition. So just to keep those separate in your mind. So a little bit of linear algebra review. Two matrices, A and B, are similar if there's a non-singular matrix X such that B equals X inverse AX. Does anyone remember what non-singular means? Yes, it has full rank, and that's equivalent to saying it has an inverse, exactly. So this relationship, so we watched a 3Blue1Brown video last time. What's the kind of analogy they used about similar matrices? Kelsey? Sam, can you pass the microphone? Yes, yeah, what's happening between A and B? Exactly, so A and B are kind of different coordinate systems, and we can think of kind of applying this X as a way to translate between them. So we're wanting to kind of translate from A to B. And they also, they had a nice, I think at one point they described this whenever you multiply by X inverse in front and X on the right hand side, that that's kind of a mathematical form of empathy, but transforming A to kind of understand B better. So there's a theorem that A and X inverse AX have the same eigenvalues. So this is going to turn out to be useful, but A and, so here, A and B have the same eigenvalues if they're related this way. And then, yeah, I apologize that there's a new vocabulary, but a sure factorization is a factorization basically exactly like this only if the matrix in the middle is upper triangular. So here, you would still say A and T are similar, and T is upper triangular. Actually, first I'll ask, does anyone know what the eigenvalues of T or of an upper triangular matrix are? Do you want to grab the microphone? Is it the diagonal? Exactly. Yeah, so the eigenvalues of a triangular matrix or the diagonal. So what would the eigenvalues of A be in this case? You could say it. Yeah. Yes, so this is why the sure factorization is going to end up being useful is that we're, you know, finding a matrix that's similar to, you know, there's nothing special about A, like it's not necessarily triangular, but it's similar to a triangular matrix, and then it's really easy to get the eigenvalues of a triangular matrix because you just grab the diagonal. Any questions? Okay. And so we're kind of going to go over the basic version of the QR algorithm, which is really simple, and then you have a loop and you get the QR decomposition of A, and then you kind of set your new A to be equal to R times Q. And under certain conditions, this algorithm is going to converge and give you the sure form of A, which means that it's going to return something triangular. So now I've kind of written this. So here I've got some pseudocode. I've written it again with subscripts. And so I'm saying QK, RK is equal to A from your previous iteration. So we're going to use the QR factorization, and then we're setting our next K equal to R times Q, which is different than the matrix we just factored because we're taking the factors and switching their order. So the way to think about this is, so AK equals QK times RK, because that's the definition of a QR factorization. Then we could multiply by Q inverse on each side and get QK inverse times AK equals RK. And why is that when I multiply by Q inverse? Just how did I get from this line to this line? Wait, grab the microphone. It cancels Q on the right side. Yeah, exactly, because Q is orthonormal. Thank you, Sam. So Q is orthonormal, so it cancels out when we multiply by Q inverse. So then we can kind of take this equation, and what happens when we multiply R times Q? We can say, oh, really, that's like saying Q inverse times A times Q. So I've just plugged this in for RK down here, and I get Q inverse AK QK. And so this, and we're not going to do kind of all the proofs for there's some other relationships. But the idea here is that by getting the QR factorization and then switching, we're kind of just repeatedly applying these similarity transformations to A. And so after a bunch of iterations, we're ending up with QK inverse. Oh, there should be some dots here. Let me fix that. OK, QK inverse dot dot dot on down to Q2 inverse, Q1 inverse times A times Q1 times Q2 dot dot dot up to QK. So what we're getting back is something that's definitely similar to our original matrix we started with. And then Trevathan has a proof showing that if we were, so I was using these subscript Ks to talk about the values we were getting each time we iterated. So just to denote that we had a different QR and A for each iteration. Here I'm taking a power again. So A raised to the Kth power is going to be equal to Q1 times Q2 dot dot dot QK times RK RK minus one dot dot dot R1. And so you can look in Trevathan on page 216 to 217 for the proof of that if you're interested. But the kind of the main idea here is that the QR algorithm is coming up with orthonormal bases for successive powers of AK. And so let's see what this looks like in code. So this is the pure QR algorithm. I'm making a copy of A, but you often would do this in place. And when you do something in place, you're, you know, writing over your original matrix, which saves memory. Although if you wanted to use your original matrix again later, that would be a problem. And so I'm just going through a bunch of iterate. Well, so I initialize Q to be the identity and then I. I'm getting the QR factorization just using numPy's implementation. Get back Q and R set AK equal to R times Q. So I flip the order. And then I'm keeping this kind of a running total of what happens each time you multiply Q by each other. And then at the end, I'm returning AK and QQ. And so if we do that on this is just a randomly generated matrix A. So let me. We do this with fewer iterations. This is a lot. And so I'm just printing it out every 100 iterations so we can see how AK is updating. So what do you see happening? So this is our original. Actually, this is. This is after 10 iterations, but it's still kind of a full matrix. We put the original A here so you can compare. So even after 10 iterations, we can see the bottom left hand corner. You need this. This should be larger. Perhaps you're able to read this all right. The. The bottom left hand corner is starting to get closer to zero, right? We've gone from these values of like point four point five down to point oh six. Oops. And then as it goes, now we're getting some zeros along this kind of left hand triangular. Getting more. More. And so what is this matrix heading towards? What type of matrix? Exactly triangular. So we can see we're getting kind of more and more zeros in this bottom left part. And so here I've just done this for 1000 iterations. We don't quite have a triangular matrix, but we've gotten quite close, right? So we still have this point four seven here and here, but the other values are zeros. And if it was perfectly triangular, what would we expect? About the eigenvalues. Exactly Sam, can you pass the microphone to Linda? Thank you. The diagonal T of A will equal to what the original is. The eigenvalues. Yes. Yeah. So the eigenvalues of this. A K or T that's triangular would be equal to the eigenvalues of the original. And so here we can compare that. What are the eigenvalues of A? Notice the first one two point seven eight. That's the first value here. So we did find that eigenvalue. We also have. Negative point one one eight three two. That's this this value. So we've let me see if we have any more. OK, so here it looks like we've just gotten two of the eigenvalues. But the idea is that when and if this converged, you could have all of them. And we can also check that the. The Q we get back is is worth the normal. So downsides to this algorithm are that it's really slow and it's also not guaranteed to converge. But I think it is neat that we get. Yeah, we can get some of the eigenvalues with this because it's a fairly simple approach. This idea of just taking the QR factorization and then switching them and doing R times Q and taking the Q factor QR factorization of that, switching those R times Q and getting the QR factorization of that. Any questions about this, about the pure QR algorithm? OK, so practical QR algorithm is adding shifts. So the idea is basically just instead of factoring AK is QKRK. We'll get the QR factorization of AK. Minus some scalar times the identity into Q and R. And then we'll do R times Q and we have to kind of add back on the scalar times the identity to kind of cancel that out. And that'll be our new AK. So it's quite similar. It's just we've basically subtracted something from the diagonal, then the QR factorization, multiply R times Q and add back on what we had subtracted from the diagonal and keep going. And this is called adding shifts. The SK is kind of the shift that you're like moving it over a little bit. And this speeds up convergence and also can help it converge in cases where it wouldn't otherwise. And I really I wanted you to see this because it shows up in a lot of different numerical linear algebra methods as a way to speed things up. And this will be this is the homework three problem is to take the PR the QR algorithm from above and add shifts to it. So that'll just be kind of modifying this algorithm up here to no longer be no longer doing the QR decomposition on AK, but using AK minus the scalar times the identity. And the recommendation for. So ideally you want SK to be an eigenvalue that you've already calculated. And you can kind of just use the diagonal of where you're current or use a value from the diagonal of where you're currently at for that. Any questions about this? Right. So in that case, we're going to. Move on to. Oh, actually, I should just comment a few things. So this is a lot better than the unshifted version, since that was not guaranteed to converge and was really slow. But this is this is still order of N to the fourth power, which is bad. For for runtime. For symmetric matrices, it would be O of N cubed. So we've got a method for finding all the eigenvalues, but it's really slow. And so what's done in practice is that if you start with something called a Hessenberg matrix, which has zeros below the first sub diagonal, it's a lot faster. And so in practice, what you'll do. So this picture is from Trevathan is you'll have. So your QR algorithm is going to end up being phase two, but you'll have a phase one that introduces a bunch of zeros and gets you to something that's almost triangular. So you'll see this matrix H is like a triangular matrix, except it has an extra. This is called a sub diagonal, but right below the diagonal, there's some non zero values. And so you'll kind of use one algorithm. So get things to this Hessenberg form. And then you could use the QR algorithm to get from Hessenberg to your triangular matrix. Matthew and Linda, can you pass the microphone? I would just keep going. So you could. It's just really slow. So this is a way to feed it up, speed it up, is that this phase one is going to be much quicker. Yeah, but it would work if you had the time. So it's faster up to that point and then slower after the point. So it's like, well, we're using a new algorithm here. So then, yeah, kind of once you're at this Hessenberg, you're then it's faster to just get to the. Yeah. Yeah, good question. OK, and so, yeah, we're not going to we're not going to talk about phase one. If you're interested, I've written about it in the notebook, though, and so you can go through this later on your own. But, yeah, I just wanted to kind of at least give you a little bit of a map of how finding the eigenvalues works. And this is like I would say a fairly complicated area of numerical linear algebra just because you're kind of having to like switch between algorithms. And there are like different versions of what you can use for phase one or phase two. I don't want to go bigger. OK, so now we're going to start notebook eight. This is this is exciting. Final notebook. And then this is also we're going to learn how to implement the QR factorization, which is something we've been using kind of for most of the course. So you'll remember we used it just now for computing eigenvalues. It was also a way to compute the least squares regression. It showed up in the primary component pursuit, how we did robust PCA showed up in our randomized rangefinder. So it's a really important factorization. In fact, Trevathan says one algorithm in numerical linear algebra is more important than all the others. QR factorization. So real kind of real fundamental building block. So here I'm just illustrating. So we're using numPy's QR factorization. We take in matrix A, we get back Q and R, and we can confirm. Q is Q's or orthonormal and R is triangular. And so today we're going to actually talk about three different. Methods for finding the QR factorization and there'll be kind of some fun examples at the end of how they have different stability properties from each other. But first I wanted to review the idea of projections in linear algebra. So the idea of a projection is that actually here I should go ahead and go to this link. This is there's something called the immersive linear algebra. Textbook and it's still kind of to be like fairly mathy and it's explanations of things, but all the graphics are interactive, which is kind of nice. So I was just going to show their. Their example for projection. If it loads. OK. So the basic idea is here we have a vector U that we're projecting onto V. This is their diagram for 3.4. And then W is the projection. Move you around and see how how W is changing. What it is is you can think of it as you're kind of decomposing you into this component W that lies along along V, as well as something that is. Perpendicular or orthonormal to V, so you're kind of taking out all part. Like any part of you that can be represented by these being taken out, that's W, and then you're just left with this like orthonormal part in this dotted line would be you minus W. Questions just about the kind of like idea of you're getting. Getting the part of you that lies along the and then what's left over has to be perpendicular to the. OK. So you can also do this with a plane and I took this from a blog post that I thought was pretty good that you might like called the linear algebra view of least squares regression. So I just show you that that's out there. Which we've kind of covered somewhat when we covered least squares regression. So here we've got a plane A and really this plane is like the column space of the matrix A. And a vector B and now we're trying to find you know what's the vector that lies on the plane. That kind of captures you know every part of B that's parallel to the plane so we get that and then we've got this B minus P and that's perpendicular to the plane. So we can get the formula for projection coming from the idea that OK this dotted line must be perpendicular to the projection. And so here and I apologize the different variable names B minus. XA dot with A is going to be zero. So in this case XA is the projection of B onto a line A and X hat is kind of the scalar that you're multiplying by. And those are dotted with each other because that's that's what's true of perpendicular is just kind of like another name for orthonormal. Or sorry orthogonal. They're not necessarily going to have length one but they're orthogonal to each other. And so then to find what the scalar is you can do A dot B divided by A dot A. Questions on projection or orthogonality. And Trevathan I think has a whole section on projectors in the kind of in the first part of the book since it is a concept that shows up a lot in numerical linear algebra. OK so then we'll get to so it's going to turn out projection is really useful for Gram-Schmidt. So classical Gram-Schmidt the idea is to remember our goal is we're trying to come up with a QR factorization and in Gram-Schmidt we're going to come up with one column of Q at a time. And the idea is kind of iterate through and each time we want to calculate a single projection. So projection PJ AJ where you're kind of projecting onto the space orthogonal to the span of your previous cues. So what this looks like code wise is so we're going to just pick off the J column of the and then. Well the first time through. I guess we still so we need to project we need to project. A onto our already existing cues in the initial case we don't have any so you can kind of just like take the first column of Q and have it normalized. Sorry first column of A normalize it and that's going to be your first column of Q. And then to get the second column of Q you want to kind of take the second column of a projected onto the first column of Q subtract that off and then that'll be your second column of Q once you normalize. And you go through and so each time you're kind of taking a column of a and you want to represent it but you need to kind of subtract off everything that's already been accounted for in your queue. And so as you do this you're creating. You know these are orthonormal columns of Q that are going to represent your columns of a. And so we can we can test this out. So we want to confirm that this algorithm so to make sure that you have like a valid algorithm that's actually finding your QR factorization. You want to check that Q times are gives you your original matrix back. That's an important part of being a factorization. We want to check that Q is actually orthonormal which is what we expected. And I probably should have checked that R is. Is actually triangular and it is. In this case maybe a is not missing a might not be square but yeah that you're getting something. Triangular back so that's that's Graham Schmidt. Questions about classic Graham Schmidt. Kelsey and can you pass the microphone. She must be like right now I'm having. Okay. Yeah. See. This will work. Okay. I'm going to see if I can put maybe put them side by side so you can kind of. Whoops. See the algorithm. And. And since pseudo code just see if Travis writes this out. Okay so one thing that Trevinson has that might be helpful is what you're coming up with. So here a one is going to be the first column of a. And basically that's just going to be a multiple of Q one. So we're trying to build an orthonormal basis starting off with a. And actually let me write and look myself space. This over to the side for a moment. So you can think of we're kind of starting with we've got these columns a one a two up through a and. We want to end up with this being we're kind of trying to find you know what what can we choose for these columns Q one up to Q Q and. Such that. Got this triangular matrix are. So all zeros and so we had to think about that is that you're getting a set of equations actually maybe I put this full screen just for a little bit and then I'll go back. And getting the set of equations a one one or sorry a one is going to be just a multiple of Q one. And we're going to store that in the. Triangular matrix are so how would so we have a how would we choose what what are one one and Q one should be. The ideas Matthew you pass the microphone down. He wanted. He wanted an hour one one. So we don't we don't have the Q's or the ours yet we're trying to find them. But basically. Yeah you want to. So that's that equation would hold but what other property do we want Q to have in addition. Yes. Well so. Oh you're very close but yeah Tim want to chime in. So yeah Matthew suggestion was kind of choose Q one to be equal to a one which is close Tim. You just said our one one to be the norm of a one X Q one to be the. Exactly yeah so you want to. Sorry you would set our one one to be the norm of a one Q one is just a one divided by. Yeah so the idea is you need to normalize it because Q one has to have length one is kind of the additional property of an orthonormal set of vectors. And so take a one normalize it that's Q one and then our one one should be the norm so that you get the right magnitude back. And kind of thinking about that in terms of vectors it's Q one is pointing in the exact same direction as a one but it's just been scaled appropriately to have length one. And let me even maybe write that. Like if this. This is a one remember a one's a column so it's a vector. Then. Q one is going in the exact same direction except it just has length. One. Very clear one. And then the length of all of a one will set to be our one one. Or rather we set our one one to be the length of a one. OK so what's the what's the next equation I could write. Yes Matthew. A two equals our one two times a one Q one plus our two two times Q two. Yes. Yeah that's great. So our A two is going to equal our one two times Q one plus our two two times Q two. And so that's just coming from. We're kind of taking the second column of our scroll over it second column of our multiplying that by the matrix Q which just picks off Q one and Q two. Remember everything below our two two is zero. So zero coefficient for Q three Q four up to Q n. And so taking this linear combination of the columns of Q we get our one two times Q one plus our two two times Q two. Equals a two. And so at this point we know what a two is or no we do not know it. Oh yes we know a two because that's our original matrix a. So we've got a two. We know what Q one is because we just solved for that in the previous step. And so now we're trying to find Q two R two two and R one two. And we have this constraint that we're going to want Q one and Q two to be orthogonal to each other. And Q two is going to have to have norm one. And so that gives us enough information that we can. What we can do is project a two on to the Q one. And we'll get a kind of R one two is the scalar we need given that kind of to get back to the full kind of component of a two. And then we can find Q two is just kind of what's what's left over the perpendicular part. Questions about that. Matthew. How do you because I know you want to Q two are orthogonal to each other. But there are a lot of orthogonal directions. Yes. Yeah. How do you pick. Yeah. So at that point what you're doing is you're going to have over here. A two minus R one two. Q one. And so in a picture this A two minus R one two Q one would be the dotted line. And actually let me draw that picture. So if this is. A two. This is Q one. Remember Q one's only got length one. Different colors. This is these are perpendicular to each other. So the idea is we projected a two onto Q one and we get that. R one two is the scalar you know since we might have to scale Q one to be the appropriate length. And then now we're interested in this kind of perpendicular piece that's left over. And then in red it's OK if I erase this first diagram over here. Everyone. Not get too crowded. Space. So then in red what we have is. This thing is a two. Minus R one two. Q one. And we're going to want that to be equal to. R two two Q two. So how do we get from so we've got a two. Minus R one two Q one. How do we get this from this red vector. How do we decide what R two two is. And what Q two is. You want to normalize the vectors of Q two. Exactly. Yes. Yeah. So we normalize them because again let me draw it. So maybe this is what length one is. So that'll be Q two. And then we have to use R two two to get the right length. Q two is giving us the direction but just with length one. And R two two gives us the correct. Magnitude. There are more questions about this. Yeah. This is a great question to kind of draw this out in more detail. Linda can you pass the microphone. Can you go through Q three so we can see. Sure. Yeah. Yeah. Let's go through Q three. And this will get a little bit trickier with draw drawing just because it's going to be in three dimensions. OK. So now we've got a three equals R one three times Q one plus R two three times Q two plus the next line are three three times Q three. And so that comes from taking the third column of R and that's giving us a linear combination of the QI and now we're just picking off the first three QI and everything else zeros out. Also just put a line to kind of separate this part. Can I erase the drawing from. I do. OK. Let me just go down to have a little bit more space. So now we're going to have to imagine that we're in three dimensions. Kind of draw a background. So we're in three dimensions and we've already found. I'm actually going to make length one a little bit longer just so this doesn't get too tight. But this is Q one. And this is Q two. So we have two two vectors and these are orthogonal to each other. And that can be. You know remember we're in three dimensions so that can look different depending on kind of what angle we're viewing this from. But we've got this Q one and Q two and we're interested in taking our vector. A three and we want to find a good Q three for it. So what should we do first. Exactly. So we're going to project it onto Q one. Sorry my one looks like a two. That. Yeah. So we projected onto Q one. I probably should have drawn this. I'm going to redraw a three just so that it is clear what it's a projection would look like. OK. So maybe a three is actually off in this direction. We projected onto Q one. And then we're interested in that in this case is also going to kind of have to be extended out to be the appropriate distance. So we subtract that off. And then what do we what do we want to do next. Matthew and can you pass the microphone Linda. Do we have to take care of the Q2 projection. Exactly. Yeah. So we'll project onto Q2 to kind of get that part. And so in this case. Yeah and it's really hard to visualize these in three dimensions. But the idea is that the way I've drawn it there might be like very little of a three that's represented on Q2. But we've got some component here of a three was represented from Q2 and part of it was represented with Q1. And then we're interested in what's left. So we're kind of looking at. I'll do these in black. So we kind of have like the Q1 part of a three and Q2 part of a three. But then that probably doesn't capture all of a three. And so whatever's left is going to be the Q3 part of a three. So we would do kind of a three minus you know this Q1 times a scalar that gets this direction minus Q2 times a scalar that gets this direction. And then what we're left with is Q3 once we normalize it. I guess I don't know maybe in this case that would be like. Maybe there's something like going a little bit this way that hasn't been captured so far and that could be. And that could even be like that would be R33 times Q3 and then we normalize it to get OK this is Q3. Questions about this? All right and we are kind of well due for a break so it's 12.04. So let's meet back here at 12.10 to continue. Thanks everyone. All right let's go ahead and start back up. So I wanted to kind of go back to the code now that we've seen these pictures of what's going on. And so what's happening with this inner for loop. So the outer for loop is where we're trying to find like a particular Q, QI or Q2. And then in this inner for loop is where we're doing the projections onto the previous Qs. So for the case of Q3 we're trying to find this for kind of outer loop J equals 3. And then we go in here and we project the third A3 onto Q1 and subtract that off. And we project it onto Q2 and subtract that off and then that's going to leave us with what we can use for our Q3. Any more questions about this? OK. Yeah so this is what we saw. This is the perhaps like the most straightforward way of coming up with a QR factorization. Unfortunately turns out it's unstable which we'll see an example of later on. So we're going to use the next look at the modified Gram-Schmidt. And modified Gram-Schmidt basically so in classic Gram-Schmidt for each J you're coming up with this single projection onto your QJ to get the kind of the final like this is Q3. You know we projected onto onto it. And in that case Q3 is the projection onto the space that's orthogonal to Q1 and Q2. For modified Gram-Schmidt you're doing. Sorry hold on a moment. Actually I think what I showed you is really modified Gram-Schmidt of doing the projection separately. Oh we're going to. No. Yeah really in some ways this is good because modified Gram-Schmidt was actually used that were like for each J we're kind of calculating onto. Yeah getting something that's the part that's perpendicular to Q1 and Q2. Let me I'll email you about this I'll think about this some more. Okay no no because like okay so with this one I was starting with the column and then kind of subtracting off how it projected onto the other ones with modified Gram-Schmidt. I really am kind of starting by normalizing my vector and saying that that's QI. Yeah let me let me continue and I will email you an update about modified Gram-Schmidt. But keep keep that picture of kind of like the general idea is that Gram-Schmidt is all about projections and that it's about constructing both classical and modified Gram-Schmidt are constructing your matrix Q one column at a time and that kind of each outer iteration at the end of it you want to have an additional column of Q. So this is a kind of a point about Gram-Schmidt if you were to stop part of the way through you would have like a partial set of Q's that are orthonormal to each other. And so Trevathan refers to this as triangular orthogonalization that you're kind of just coming up with this one Q. You could actually think of that as being a lot of smaller or not smaller but less informative triangular matrices because basically kind of doing these projections could be represented by this triangular operation. You're just getting one one Q and kind of the focus is on building Q. So Householder is a different approach where basically you're focused on zeroing out the bottom triangle and A. And so kind of the whole focus is putting and basically each iteration for Householder you're going to take another column and make everything below the diagonal zeros. And you keep doing that and it turns out that that's equivalent to multiplying by a orthonormal matrix each time. So kind of like yeah I guess the opposite thing is center with that Gram-Schmidt the central idea is getting these Q's and you're focused on what do we need to get the next Q. So with Householder it's how can I zero out some more more entries below the diagonal. So with Householder I think is a lot harder to think about conceptually. I even think I might. Yeah we're not going to get into the details of the algorithm. Let's check that what we get. So there's this there's also this concept called Householder Reflectors. And so it's basically where I put my stylus. Each time with Householder you're multiplying by a block matrix where part of the matrix is the identity. So block matrix is a way of kind of putting together smaller matrices into a bigger matrix. And part of that is the identity matrix and then part of it is this special matrix called a Householder Reflector. And this is kind of designed and what's going to happen is that I is getting bigger and bigger as you iterate. Let me check back. So you're you're going in one direction of I either getting bigger or smaller F getting smaller or bigger. And this this whole thing is orthonormal. And the idea is that multiplying this times it's on this side this times a is going to zero out an additional column. So you'll get kind of more zeros zero kind of constructing constructing these matrices. So we calculate Q and R using these block matrices F. And here I'm kind of just showing OK so it must be that I yeah I starts out small so this is a one by one identity matrix right here. And F is three by three. Actually then we can see this would be nice to look at. Now we've got a two by two identity matrix in this corner and F is two by two. Here I is three by three and then F ends up having having to be one since it's got to be orthonormal. It's either going to be one or negative one. You're kind of constructing these and the key is you get back Q and R. You can check. Well so actually householder is described in Trevathan doesn't explicitly calculate Q. It just calculates R. It turns out for most problems you don't need and you don't actually need Q. And there are kind of these implicit methods that can use the householder reflectors to get to get what would Q times Q transpose times B be or Q transpose solve. Q X equals B. So either like multiply or solve that equation. I mean this is something that kind of shows up in numerical linear algebra periodically is that you don't always like need the explicit matrix. You just need to know what is that matrix times a vector or what is the solution of that. Can I solve a system of equations A X equals B when I know A and B. Yeah so I think I'm going to go on to. Sorry that I'm not getting into the details on these and householders covered in Trevathan. But it's a less intuitive algorithm than Gram-Schmidt where you're taking these projections. And I want to see like I think part of the fun part is kind of seeing how these are different from each other numerically. And I wanted to definitely make sure we get to this with plenty of time. So this is coming from Lecture 9 of Trevathan. It has a few examples of how they vary. And so one of these is comparing the classic versus modified Gram-Schmidt. And so here we're going to kind of intentionally construct a matrix that has singular values spaced by factors of two between two to the negative one and two to the negative n plus one. So that means the magnitude of the singular values is really varying logarithmically. And so kind of to make this matrix. We're taking NP dot power of two then to this range from negative one to negative n plus one. And so we're using the idea of a kind of SVD to construct a matrix that has the singular values we want. So we'll just get kind of two orthonormal matrices for U and V. And then S we're putting what we want along the diagonal and multiplying U times S times V. And that will presumably have this singular value decomposition with the values of S that we've created. I should stop. Are there questions about this way of constructing a matrix to kind of make sure it has the singular values that we want? And this is the reverse of the SVD is we're coming up with two orthonormal matrices and a diagonal matrices and multiplying them together. OK, so we create a we do a classical Gram-Schmidt and modified Gram-Schmidt on it. And then actually I should have checked that. And these might end up I'll see if this works. They might be off by a sign. You can't have variations with kind of where your negative signs are. Oh, actually, the point is these are going to not be the same. So ideally, we want these to be similar to each other. But it turns out that they're they're not. So we can graph S, which is what we know the true singular values to be. And those are the red dots. And then we're graphing the diagonal of R and the diagonal of Rm, which is the modified Gram-Schmidt R and the classic Gram-Schmidt. It was the blue. And so first I want to ask you why why should the diagonals why would we expect those to be similar to the singular values or to be the singular values? The diagonal of R. Oh, wait, grab the microphone. Because of the relationship between the eigenvalues and the singular values. So the yes. Well, there's another piece that I think you're probably thinking. And why are you talking about eigenvalues? I can talk to the microphone. Should they be related to the singular values? Right. So the eigenvalues are related to the singular values. And then how is that related to the diagonal of R? Linda, do you want to say it? It's a triangle. Yeah, since it's a triangular matrix, the diagonal. Yeah, it gives you the eigenvalues, which are the singular values in this case. Is it singular value also eigenvalues? So how do they relate it? So singular values. Yeah. So I think the values are like a generalization of eigenvalues. And the idea, let me go back to the. Notepad. Is that singular values we can think of as. A times. V equals. Sigma times U. And then eigenvalues can be thought of as AX equals. Lambda X. And so notice that this first equation, typically like we're talking about the SVD, we're writing out the full matrices. This is just thinking about particular vectors and they're. You know, like one kind of one vector at a time or one pair of vectors being U and one singular value from the diagonal. So this is, you know, coming from A equals. U sigma V. And we would have. A number of these for different singular values. Kelsey, can you pass the microphone, Linda? Is this new reality kind of like this? It is, yeah. The stretching that you're doing. Like a version and like elongating or. Yes, yes, exactly. Yeah, that's a great. Great thing to note. So it's kind of with singular values. Often you'll see these pictures. Maybe you're taking. A circle and perhaps A transforms it to an ellipse. Oops. So if matrix A changes the unit circle into this ellipse. The. You're getting the. The vectors, U and V are basically like a change of basis. And so you can think of kind of this is really relates back to the three blue one brown video that we're kind of interested in. Like what's the I don't know, like more natural basis for this ellipse. And that would be like, oh, that's a different color. You know, like really like these are the axes of that. And so those vectors kind of the directions are coming from your V and the magnitude is coming from Sigma. As Kelsey said. And here really, that's like, you know, what was just a unit axis. And so that's basically that looks familiar in thinking about change of basis. And so singular value decomposition is really a change of basis. And then kind of going back to. Actually, first I should ask, are there questions about that? This idea of SVD being a bit like a change of basis. Sam, pass the microphone back. So would you be would have would you have a magnitude of one in the basis provided by a like in this case, you're stretching it. So the elongation would be with respect to the original axes. So I think so the elongation is captured by the Sigma so that you. So I guess my question is, would you have a magnitude of one in this case? So you you does have a magnitude of one. Yes. And so looking at kind of the eigen eigenvalues, eigenvectors. So here this is a very similar equation. And the difference is just that you you don't have separate you and V. You just have X. And so this. This could be written. Why was I able to change the order of Sigma and X? When I went to the the matrix form. Yeah, because Sigma is just a scalar. So you're and you're just putting those on the diagonal of your your big Sigma or so I big Lambda. And so then this is. This should look familiar of kind of this is what it means for a Lambda to be similar. We have this change of basis, and that's what the eigen decomposition is doing. Do all do all matrices have a full eigen decomposition? Thumbs up for yes, thumbs down for no. OK, good. I see mostly thumbs down. Yeah. So not all not all matrices have a full eigen decomposition. A full eigen decomposition is where you can get a matrix X that's orthonormal or. A matrix X of full rank and then but all all matrices have a singular value decomposition, so it's kind of more general. I mean, it doesn't have this additional constraint on it. So kind of going back to this. This comparison of you know, we're trying to get the true singular values. We know what they're supposed to be. We look at the diagonal and we're definitely we're right up here. Like it seems, you know, both classic and modified did well. Does anyone want to guess what? One of these places where it's leveling off is. This is a concept or term that we talked about in lesson one. Any guesses? Kelsey, grab the microphone from Sam. Yes, machine epsilon. So this is the and in particular that's for modified Gram-Schmidt. It can't capture something that small. Tim. Why does it look like it levels around to the negative 57? Yeah, I am just the way the axes ended up. I'm not fully sure about that. That was like close enough that I was like this seems right on. But yeah, I am. I don't know why it's not. Not perfect. Good question. Yes, I've written down here. So there's a way from NumPy dot F info to find out what NumPy saying machine epsilon is for float 64. And so. And remember, we've you know, we're graphing this by powers of two. We get that epsilon is around negative 52. This is close to that. And then the square root of epsilon is around to the negative 26. Remember, when for values less than one. Square root makes things larger, not smaller. It's kind of a reverse for greater than one. And so this is, I thought, like a neat illustration of kind of when when machine epsilon is posing a problem and also kind of the difference between depending on machine epsilon for your accuracy and depending on the square root of machine epsilon, which is what classic Gram-Schmidt does. And this is although it's a contrived example, it's nice because we know exactly kind of what the answer should be. And we see when each one stops following following the line. Questions about this example. Or machine epsilon. And so classic classic Gram-Schmidt is not not used in practice. For this reason. Okay, so now we're going to look at another example. And if we were if we were to do householder on this graph, it's basically the same accuracy as modified Gram-Schmidt. So it levels off at the same place. So I had it on here at one point, but it's harder to read. But in terms of accuracy. Well, for this example for accuracy, householder and modified Gram-Schmidt are giving you the same thing. Okay, so the next example we're going to look at. Is a matrix and this is this is also coming from Lecture 9 of Tremblathan. So we have a matrix A that's point seven comma point seven oh seven one one and then point seven zero zero zero one comma point seven oh seven one one. And you can probably just from looking at this matrix already guess maybe. Oops, I need to run this again. Guess where this this may be going. So this matrix is close to not having full rank because the. Second row was almost a repeat of the first row. You know, there's just this difference of one to the negative fifth between them. So we'll use our modified Gram-Schmidt to get Q and R. We can use householder to get Q and R. And I did this, I guess, both with. Our version and NumPy's. And then check that all the QR factorizations work. And so here notice that in all cases, we're not actually getting back the original A because this bottom left entry has been. Has ended up point seven, although that is close to what the original A was. But now we can check how close is Q to being perfectly orthonormal. And so here there's no difference between what we're getting from Gram-Schmidt and householder in terms of reconstructing our A. So looking to get a measure on kind of how close Q is to being orthonormal will take Q. Q times Q transpose minus the identity. If it was perfectly orthonormal, what would Q times Q transpose minus the identity be? Someone whisper it, but yeah, zero, because it was perfectly orthonormal Q times Q transpose would be the identity. Here we're looking at the norm of that. And you'll notice with Gram-Schmidt, it's. Something to the negative 11th, 10 to the negative 11th, and with householder, it's 10 to the negative 16th. And so that's kind of an example that householder got us a Q that was closer to being perfectly orthonormal than Gram-Schmidt did. And I believe NumPy's implementation of QR is using householder. Linda and Tim has the microphone. So if we could go back to the graph, I was wondering what makes the difference between the classic and modified that's causing that. So the difference lies in the algorithms. And we didn't really talk about what's going on with modified Gram-Schmidt. But modified Gram-Schmidt basically looks like you've just kind of made this algebraic change. Like you're doing the same thing, but you're kind of changing the order of how you do things. But it's something that the operations are compounding, or like the air is compounding differently between them. Even though mathematically, you know, if you're like proving it on paper and pencil, it's like, oh, these should be equivalent. Good question. Other questions? I think that might be all I want to say about, well, how briefly put a classified Gram-Schmidt and modified Gram-Schmidt next to each other just to compare. Oh, you know what it is. Sorry. With modified Gram-Schmidt. Yeah. So what I showed you with the picture when I drew the vectors, that was classic Gram-Schmidt, where you're kind of doing these projections once. Modified Gram-Schmidt, it's like you're modifying each value of. Like you're getting this whole matrix of projections that you're modifying each time. So we have V, which is what we're kind of copying A into. And so like here with classic Gram-Schmidt, you just need to have a single vector V each time. You're just kind of like, you know, projecting it and then you subtract off something else, project something, subtract off something else. Whereas here you actually have to have a whole matrix V and like in this. Well. Yeah, and it affects what your norms are. So basically it's like you're calculating more projections kind of in this more complicated way. Like each projection is really made up of several small projections. Matthew, and can you pass the microphone? Oh, this is Matthew. I may have raised my hand too soon. Oh, I said I wasn't planning on it, but yeah, we can talk about it. Yeah, and Kelsey also has her hand up, it seems about this. I want to change topics a little bit. Oh yeah. Is there like some intuition for why. RQ ends up like why it converges to something? Let me go back to the QR factorization. All right, it's more triangular every time you do that. RQ. And not I don't have anything off my head of yeah, like intuition for that in particular. Okay, Sam has something and then I will say a little bit about the final exam after that. One more question. The diagonal that's produced or close to diagonal, if we have all real eigenvalues, then it will merge to a total diagonal. Looking at the matrix, how are we supposed to know which of the diagonal are the eigenvalues? Is it ones that have no other values like in the column or row? Yeah, so I'm actually not sure if this is a theorem, like in just all the cases I did, if it like didn't have other stuff beneath it, it was turning out to be an eigenvalue, but I don't know if that's always the case or not. That's a good question. Okay, so yeah, about the final. So that's going to be all on paper, so no computers for that. Yeah, I've tried to take questions, many of them are things, yeah, all of them are definitely related to what we've done in class and in the homework and in the notebooks. So a mix, so there are some examples where I give you code and ask you to modify it, and this will be written, but then there are also conceptual questions. Anything about the algorithms? Well, what do you mean, anything about the algorithms? Yeah, so there are questions, yeah, on the algorithms that we've learned and also on the specific examples that we've covered in the class. No. Yeah, and I tried to stay away from stuff that I thought was like, just like annoying memorization, like I wanted it to more be like, do you understand the concepts? And like key points that I thought you should know of maybe kind of like what an algorithm gives you or what an algorithm is used for, but yeah, you definitely do not have to regurgitate the algorithms. Yeah, I've tried to make it not something that's like tricky or overly, like I tried to make it fair, I guess, and like kind of about the like concepts and important parts. Any other questions about it? Okay, and we're on, this can end a few minutes early today, and then I will definitely be available through email, so yeah, feel free to email me if you have questions in the next week. Alright, that's all. Thank you. Thanks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.0, "text": " Alright, I'm going to go ahead and get started. So yeah, just to announce again, I won't be here next week, so no class on Tuesday.", "tokens": [2798, 11, 286, 478, 516, 281, 352, 2286, 293, 483, 1409, 13, 407, 1338, 11, 445, 281, 7478, 797, 11, 286, 1582, 380, 312, 510, 958, 1243, 11, 370, 572, 1508, 322, 10017, 13], "temperature": 0.0, "avg_logprob": -0.21678167343139648, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.007933673448860645}, {"id": 1, "seek": 0, "start": 8.0, "end": 17.0, "text": " And then Thursday there will be the test at the final exam during the normal class time, and David Uminski will be here to proctor that.", "tokens": [400, 550, 10383, 456, 486, 312, 264, 1500, 412, 264, 2572, 1139, 1830, 264, 2710, 1508, 565, 11, 293, 4389, 3301, 38984, 486, 312, 510, 281, 447, 1672, 300, 13], "temperature": 0.0, "avg_logprob": -0.21678167343139648, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.007933673448860645}, {"id": 2, "seek": 0, "start": 17.0, "end": 26.0, "text": " On Tuesday, the final draft of the blog post is due, as well as homework three. And homework three is just a single question.", "tokens": [1282, 10017, 11, 264, 2572, 11206, 295, 264, 6968, 2183, 307, 3462, 11, 382, 731, 382, 14578, 1045, 13, 400, 14578, 1045, 307, 445, 257, 2167, 1168, 13], "temperature": 0.0, "avg_logprob": -0.21678167343139648, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.007933673448860645}, {"id": 3, "seek": 2600, "start": 26.0, "end": 37.0, "text": " Yes. And I won't have my regular office hours tomorrow afternoon, but I could meet with you either this afternoon or tomorrow morning if you need to.", "tokens": [1079, 13, 400, 286, 1582, 380, 362, 452, 3890, 3398, 2496, 4153, 6499, 11, 457, 286, 727, 1677, 365, 291, 2139, 341, 6499, 420, 4153, 2446, 498, 291, 643, 281, 13], "temperature": 0.0, "avg_logprob": -0.0889697904172151, "compression_ratio": 1.5431472081218274, "no_speech_prob": 8.885179704520851e-05}, {"id": 4, "seek": 2600, "start": 37.0, "end": 47.0, "text": " So just ask me after class. And then I just wanted to thank everyone for being such an engaged group of students. I've really enjoyed teaching this class.", "tokens": [407, 445, 1029, 385, 934, 1508, 13, 400, 550, 286, 445, 1415, 281, 1309, 1518, 337, 885, 1270, 364, 8237, 1594, 295, 1731, 13, 286, 600, 534, 4626, 4571, 341, 1508, 13], "temperature": 0.0, "avg_logprob": -0.0889697904172151, "compression_ratio": 1.5431472081218274, "no_speech_prob": 8.885179704520851e-05}, {"id": 5, "seek": 4700, "start": 47.0, "end": 58.0, "text": " This has been fun. So thank you. Yeah, so we'll get into it. We've run out of time a little bit, so we're not going to fully cover lesson seven,", "tokens": [639, 575, 668, 1019, 13, 407, 1309, 291, 13, 865, 11, 370, 321, 603, 483, 666, 309, 13, 492, 600, 1190, 484, 295, 565, 257, 707, 857, 11, 370, 321, 434, 406, 516, 281, 4498, 2060, 6898, 3407, 11], "temperature": 0.0, "avg_logprob": -0.13428990464461477, "compression_ratio": 1.4860335195530727, "no_speech_prob": 1.3004868378629908e-05}, {"id": 6, "seek": 4700, "start": 58.0, "end": 72.0, "text": " but I did want to go a little bit further in it. Just to remember kind of what we did last time, we were looking at...oh.", "tokens": [457, 286, 630, 528, 281, 352, 257, 707, 857, 3052, 294, 309, 13, 1449, 281, 1604, 733, 295, 437, 321, 630, 1036, 565, 11, 321, 645, 1237, 412, 485, 1445, 13], "temperature": 0.0, "avg_logprob": -0.13428990464461477, "compression_ratio": 1.4860335195530727, "no_speech_prob": 1.3004868378629908e-05}, {"id": 7, "seek": 7200, "start": 72.0, "end": 87.0, "text": " Thanks for pointing that out. Thank you. This will make class a lot easier to follow.", "tokens": [2561, 337, 12166, 300, 484, 13, 1044, 291, 13, 639, 486, 652, 1508, 257, 688, 3571, 281, 1524, 13], "temperature": 0.0, "avg_logprob": -0.09252134958902995, "compression_ratio": 1.4807692307692308, "no_speech_prob": 2.2119227651273832e-05}, {"id": 8, "seek": 7200, "start": 87.0, "end": 96.0, "text": " Yeah, so last time we talked about eigen-decompositions, and there's kind of this close relationship between the eigen-decomposition and the SVD.", "tokens": [865, 11, 370, 1036, 565, 321, 2825, 466, 10446, 12, 1479, 21541, 329, 2451, 11, 293, 456, 311, 733, 295, 341, 1998, 2480, 1296, 264, 10446, 12, 1479, 21541, 5830, 293, 264, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.09252134958902995, "compression_ratio": 1.4807692307692308, "no_speech_prob": 2.2119227651273832e-05}, {"id": 9, "seek": 9600, "start": 96.0, "end": 109.0, "text": " And so even though we're looking specifically at how to do eigen-decompositions, similar techniques are used for calculating the SVD.", "tokens": [400, 370, 754, 1673, 321, 434, 1237, 4682, 412, 577, 281, 360, 10446, 12, 1479, 21541, 329, 2451, 11, 2531, 7512, 366, 1143, 337, 28258, 264, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.11173568359793049, "compression_ratio": 1.4848484848484849, "no_speech_prob": 8.663490916660521e-06}, {"id": 10, "seek": 9600, "start": 109.0, "end": 122.0, "text": " And so we were looking at this dbPedia dataset, which is a dataset of all the Wikipedia links, and we are finding the kind of the principal eigenvector of that.", "tokens": [400, 370, 321, 645, 1237, 412, 341, 274, 65, 47, 14212, 28872, 11, 597, 307, 257, 28872, 295, 439, 264, 28999, 6123, 11, 293, 321, 366, 5006, 264, 733, 295, 264, 9716, 10446, 303, 1672, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.11173568359793049, "compression_ratio": 1.4848484848484849, "no_speech_prob": 8.663490916660521e-06}, {"id": 11, "seek": 12200, "start": 122.0, "end": 139.0, "text": " And what's significant about that eigenvector? What information is it giving us?", "tokens": [400, 437, 311, 4776, 466, 300, 10446, 303, 1672, 30, 708, 1589, 307, 309, 2902, 505, 30], "temperature": 0.0, "avg_logprob": -0.07108424958728608, "compression_ratio": 1.0666666666666667, "no_speech_prob": 1.3406765901891049e-05}, {"id": 12, "seek": 13900, "start": 139.0, "end": 154.0, "text": " I think the most significant is that we can easily find any power of the matrix just by multiplying the eigenvalues and finding this diagonal of the matrix.", "tokens": [286, 519, 264, 881, 4776, 307, 300, 321, 393, 3612, 915, 604, 1347, 295, 264, 8141, 445, 538, 30955, 264, 10446, 46033, 293, 5006, 341, 21539, 295, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13117125677683997, "compression_ratio": 1.6, "no_speech_prob": 1.5203105249383952e-05}, {"id": 13, "seek": 13900, "start": 154.0, "end": 166.0, "text": " So that's something that's really powerful about having the full eigen decomposition, is that you can quickly take powers of matrices. Yes.", "tokens": [407, 300, 311, 746, 300, 311, 534, 4005, 466, 1419, 264, 1577, 10446, 48356, 11, 307, 300, 291, 393, 2661, 747, 8674, 295, 32284, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.13117125677683997, "compression_ratio": 1.6, "no_speech_prob": 1.5203105249383952e-05}, {"id": 14, "seek": 16600, "start": 166.0, "end": 173.0, "text": " And then what's kind of the meaning? So we're actually just finding...so you need kind of all the eigenvectors and eigenvalues to do that.", "tokens": [400, 550, 437, 311, 733, 295, 264, 3620, 30, 407, 321, 434, 767, 445, 5006, 485, 539, 291, 643, 733, 295, 439, 264, 10446, 303, 5547, 293, 10446, 46033, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.09263370488141034, "compression_ratio": 1.2777777777777777, "no_speech_prob": 1.6961324945441447e-05}, {"id": 15, "seek": 17300, "start": 173.0, "end": 196.0, "text": " For the dbPedia dataset, we're just finding a single eigenvector. What's the meaning of that eigenvector?", "tokens": [1171, 264, 274, 65, 47, 14212, 28872, 11, 321, 434, 445, 5006, 257, 2167, 10446, 303, 1672, 13, 708, 311, 264, 3620, 295, 300, 10446, 303, 1672, 30], "temperature": 0.0, "avg_logprob": -0.056109171360731125, "compression_ratio": 1.1538461538461537, "no_speech_prob": 5.6819508245098405e-06}, {"id": 16, "seek": 19600, "start": 196.0, "end": 206.0, "text": " Matthew and Valentin, can you throw the microphone to Matthew McCalland?", "tokens": [12434, 293, 17961, 259, 11, 393, 291, 3507, 264, 10952, 281, 12434, 12061, 336, 474, 30], "temperature": 0.0, "avg_logprob": -0.19306185888865637, "compression_ratio": 1.409356725146199, "no_speech_prob": 1.5933983377180994e-05}, {"id": 17, "seek": 19600, "start": 206.0, "end": 210.0, "text": " Good ducking.", "tokens": [2205, 12482, 278, 13], "temperature": 0.0, "avg_logprob": -0.19306185888865637, "compression_ratio": 1.409356725146199, "no_speech_prob": 1.5933983377180994e-05}, {"id": 18, "seek": 19600, "start": 210.0, "end": 214.0, "text": " You're talking about the solution vector? Yes.", "tokens": [509, 434, 1417, 466, 264, 3827, 8062, 30, 1079, 13], "temperature": 0.0, "avg_logprob": -0.19306185888865637, "compression_ratio": 1.409356725146199, "no_speech_prob": 1.5933983377180994e-05}, {"id": 19, "seek": 19600, "start": 214.0, "end": 222.0, "text": " Isn't that the probability, I guess the overall probability that it will end there if you do a random walk?", "tokens": [6998, 380, 300, 264, 8482, 11, 286, 2041, 264, 4787, 8482, 300, 309, 486, 917, 456, 498, 291, 360, 257, 4974, 1792, 30], "temperature": 0.0, "avg_logprob": -0.19306185888865637, "compression_ratio": 1.409356725146199, "no_speech_prob": 1.5933983377180994e-05}, {"id": 20, "seek": 22200, "start": 222.0, "end": 227.0, "text": " Yes, so it is kind of telling you the probability that you're going to be on each page if you're randomly walking.", "tokens": [1079, 11, 370, 309, 307, 733, 295, 3585, 291, 264, 8482, 300, 291, 434, 516, 281, 312, 322, 1184, 3028, 498, 291, 434, 16979, 4494, 13], "temperature": 0.0, "avg_logprob": -0.15946050034355871, "compression_ratio": 1.6680672268907564, "no_speech_prob": 3.822429425781593e-05}, {"id": 21, "seek": 22200, "start": 227.0, "end": 231.0, "text": " So what does that mean for ones that have higher probabilities?", "tokens": [407, 437, 775, 300, 914, 337, 2306, 300, 362, 2946, 33783, 30], "temperature": 0.0, "avg_logprob": -0.15946050034355871, "compression_ratio": 1.6680672268907564, "no_speech_prob": 3.822429425781593e-05}, {"id": 22, "seek": 22200, "start": 231.0, "end": 233.0, "text": " Maybe it's more likely to end there.", "tokens": [2704, 309, 311, 544, 3700, 281, 917, 456, 13], "temperature": 0.0, "avg_logprob": -0.15946050034355871, "compression_ratio": 1.6680672268907564, "no_speech_prob": 3.822429425781593e-05}, {"id": 23, "seek": 22200, "start": 233.0, "end": 242.0, "text": " Yeah, so this is kind of the idea behind PageRank, which is Google's original algorithm, but it's giving you the most important pages because people are more likely to end up there.", "tokens": [865, 11, 370, 341, 307, 733, 295, 264, 1558, 2261, 21217, 49, 657, 11, 597, 307, 3329, 311, 3380, 9284, 11, 457, 309, 311, 2902, 291, 264, 881, 1021, 7183, 570, 561, 366, 544, 3700, 281, 917, 493, 456, 13], "temperature": 0.0, "avg_logprob": -0.15946050034355871, "compression_ratio": 1.6680672268907564, "no_speech_prob": 3.822429425781593e-05}, {"id": 24, "seek": 24200, "start": 242.0, "end": 252.0, "text": " Thank you. So yeah, that's kind of the idea that even though it's a single eigenvector, it's actually pretty important because it's giving you...", "tokens": [1044, 291, 13, 407, 1338, 11, 300, 311, 733, 295, 264, 1558, 300, 754, 1673, 309, 311, 257, 2167, 10446, 303, 1672, 11, 309, 311, 767, 1238, 1021, 570, 309, 311, 2902, 291, 485], "temperature": 0.0, "avg_logprob": -0.13766532608225376, "compression_ratio": 1.5792079207920793, "no_speech_prob": 8.939234248828143e-06}, {"id": 25, "seek": 24200, "start": 252.0, "end": 259.0, "text": " some people call that the stationary distribution when you have a Markov chain of kind of where things will end up.", "tokens": [512, 561, 818, 300, 264, 30452, 7316, 562, 291, 362, 257, 3934, 5179, 5021, 295, 733, 295, 689, 721, 486, 917, 493, 13], "temperature": 0.0, "avg_logprob": -0.13766532608225376, "compression_ratio": 1.5792079207920793, "no_speech_prob": 8.939234248828143e-06}, {"id": 26, "seek": 24200, "start": 259.0, "end": 262.0, "text": " And for webpages, that's kind of the relative importance.", "tokens": [400, 337, 3670, 79, 1660, 11, 300, 311, 733, 295, 264, 4972, 7379, 13], "temperature": 0.0, "avg_logprob": -0.13766532608225376, "compression_ratio": 1.5792079207920793, "no_speech_prob": 8.939234248828143e-06}, {"id": 27, "seek": 26200, "start": 262.0, "end": 272.0, "text": " Is there some meaning to the actual values in the matrix?", "tokens": [1119, 456, 512, 3620, 281, 264, 3539, 4190, 294, 264, 8141, 30], "temperature": 0.0, "avg_logprob": -0.16659945657808486, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.44632667292899e-06}, {"id": 28, "seek": 26200, "start": 272.0, "end": 283.0, "text": " Yeah, so we talked about... we had kind of our original matrix, which was the graph adjacency matrix, and those were zeros and ones.", "tokens": [865, 11, 370, 321, 2825, 466, 485, 321, 632, 733, 295, 527, 3380, 8141, 11, 597, 390, 264, 4295, 22940, 3020, 8141, 11, 293, 729, 645, 35193, 293, 2306, 13], "temperature": 0.0, "avg_logprob": -0.16659945657808486, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.44632667292899e-06}, {"id": 29, "seek": 26200, "start": 283.0, "end": 290.0, "text": " And there when you take values... so once you start taking powers, they can kind of be any integer values.", "tokens": [400, 456, 562, 291, 747, 4190, 485, 370, 1564, 291, 722, 1940, 8674, 11, 436, 393, 733, 295, 312, 604, 24922, 4190, 13], "temperature": 0.0, "avg_logprob": -0.16659945657808486, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.44632667292899e-06}, {"id": 30, "seek": 29000, "start": 290.0, "end": 298.0, "text": " And that's giving you the number of ways you could get from page A to page B in K steps.", "tokens": [400, 300, 311, 2902, 291, 264, 1230, 295, 2098, 291, 727, 483, 490, 3028, 316, 281, 3028, 363, 294, 591, 4439, 13], "temperature": 0.0, "avg_logprob": -0.16606983484006396, "compression_ratio": 1.716279069767442, "no_speech_prob": 7.410746547975577e-06}, {"id": 31, "seek": 29000, "start": 298.0, "end": 308.0, "text": " So if you took the third power, that's kind of saying how many ways are there to get from A to B taking three hops.", "tokens": [407, 498, 291, 1890, 264, 2636, 1347, 11, 300, 311, 733, 295, 1566, 577, 867, 2098, 366, 456, 281, 483, 490, 316, 281, 363, 1940, 1045, 47579, 13], "temperature": 0.0, "avg_logprob": -0.16606983484006396, "compression_ratio": 1.716279069767442, "no_speech_prob": 7.410746547975577e-06}, {"id": 32, "seek": 29000, "start": 308.0, "end": 310.0, "text": " So you could go... yeah.", "tokens": [407, 291, 727, 352, 485, 1338, 13], "temperature": 0.0, "avg_logprob": -0.16606983484006396, "compression_ratio": 1.716279069767442, "no_speech_prob": 7.410746547975577e-06}, {"id": 33, "seek": 29000, "start": 310.0, "end": 314.0, "text": " So you could like normalize, you could look at one and be like...", "tokens": [407, 291, 727, 411, 2710, 1125, 11, 291, 727, 574, 412, 472, 293, 312, 411, 485], "temperature": 0.0, "avg_logprob": -0.16606983484006396, "compression_ratio": 1.716279069767442, "no_speech_prob": 7.410746547975577e-06}, {"id": 34, "seek": 29000, "start": 314.0, "end": 319.0, "text": " Yeah, exactly. Yeah, so then it becomes a probability when you normalize.", "tokens": [865, 11, 2293, 13, 865, 11, 370, 550, 309, 3643, 257, 8482, 562, 291, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.16606983484006396, "compression_ratio": 1.716279069767442, "no_speech_prob": 7.410746547975577e-06}, {"id": 35, "seek": 31900, "start": 319.0, "end": 324.0, "text": " Yeah, good question.", "tokens": [865, 11, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.14322854470515597, "compression_ratio": 1.4545454545454546, "no_speech_prob": 4.0293643905897625e-06}, {"id": 36, "seek": 31900, "start": 324.0, "end": 329.0, "text": " Yeah, and so the method we looked at last time, the power method...", "tokens": [865, 11, 293, 370, 264, 3170, 321, 2956, 412, 1036, 565, 11, 264, 1347, 3170, 485], "temperature": 0.0, "avg_logprob": -0.14322854470515597, "compression_ratio": 1.4545454545454546, "no_speech_prob": 4.0293643905897625e-06}, {"id": 37, "seek": 31900, "start": 329.0, "end": 337.0, "text": " Go down to it.", "tokens": [1037, 760, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.14322854470515597, "compression_ratio": 1.4545454545454546, "no_speech_prob": 4.0293643905897625e-06}, {"id": 38, "seek": 31900, "start": 337.0, "end": 347.0, "text": " I would just highlight again, since we've talked about sparse matrices a lot, here we kind of very naturally had our data in the coordinate sparse form,", "tokens": [286, 576, 445, 5078, 797, 11, 1670, 321, 600, 2825, 466, 637, 11668, 32284, 257, 688, 11, 510, 321, 733, 295, 588, 8195, 632, 527, 1412, 294, 264, 15670, 637, 11668, 1254, 11], "temperature": 0.0, "avg_logprob": -0.14322854470515597, "compression_ratio": 1.4545454545454546, "no_speech_prob": 4.0293643905897625e-06}, {"id": 39, "seek": 34700, "start": 347.0, "end": 354.0, "text": " since we just know kind of this page links to this page, and we don't explicitly have...", "tokens": [1670, 321, 445, 458, 733, 295, 341, 3028, 6123, 281, 341, 3028, 11, 293, 321, 500, 380, 20803, 362, 485], "temperature": 0.0, "avg_logprob": -0.10585352662321809, "compression_ratio": 1.6129032258064515, "no_speech_prob": 5.422027243184857e-06}, {"id": 40, "seek": 34700, "start": 354.0, "end": 361.0, "text": " This is all the pages that it doesn't link to, and there's not really a point in storing all that.", "tokens": [639, 307, 439, 264, 7183, 300, 309, 1177, 380, 2113, 281, 11, 293, 456, 311, 406, 534, 257, 935, 294, 26085, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.10585352662321809, "compression_ratio": 1.6129032258064515, "no_speech_prob": 5.422027243184857e-06}, {"id": 41, "seek": 34700, "start": 361.0, "end": 371.0, "text": " So we kind of use coordinate form to create a sparse matrix, and then we switch to compressed sparse row format.", "tokens": [407, 321, 733, 295, 764, 15670, 1254, 281, 1884, 257, 637, 11668, 8141, 11, 293, 550, 321, 3679, 281, 30353, 637, 11668, 5386, 7877, 13], "temperature": 0.0, "avg_logprob": -0.10585352662321809, "compression_ratio": 1.6129032258064515, "no_speech_prob": 5.422027243184857e-06}, {"id": 42, "seek": 37100, "start": 371.0, "end": 383.0, "text": " Why did we switch?", "tokens": [1545, 630, 321, 3679, 30], "temperature": 0.0, "avg_logprob": -0.09210266850211403, "compression_ratio": 1.218045112781955, "no_speech_prob": 3.041510353796184e-06}, {"id": 43, "seek": 37100, "start": 383.0, "end": 386.0, "text": " Sam?", "tokens": [4832, 30], "temperature": 0.0, "avg_logprob": -0.09210266850211403, "compression_ratio": 1.218045112781955, "no_speech_prob": 3.041510353796184e-06}, {"id": 44, "seek": 37100, "start": 386.0, "end": 388.0, "text": " Because it takes half as many reads from memory.", "tokens": [1436, 309, 2516, 1922, 382, 867, 15700, 490, 4675, 13], "temperature": 0.0, "avg_logprob": -0.09210266850211403, "compression_ratio": 1.218045112781955, "no_speech_prob": 3.041510353796184e-06}, {"id": 45, "seek": 37100, "start": 388.0, "end": 394.0, "text": " Exactly. So for matrix multiplication, it's the same number of floating point operations,", "tokens": [7587, 13, 407, 337, 8141, 27290, 11, 309, 311, 264, 912, 1230, 295, 12607, 935, 7705, 11], "temperature": 0.0, "avg_logprob": -0.09210266850211403, "compression_ratio": 1.218045112781955, "no_speech_prob": 3.041510353796184e-06}, {"id": 46, "seek": 39400, "start": 394.0, "end": 409.0, "text": " but half as many reads from memory to be in sparse row format as opposed to coordinate, sparse coordinate format.", "tokens": [457, 1922, 382, 867, 15700, 490, 4675, 281, 312, 294, 637, 11668, 5386, 7877, 382, 8851, 281, 15670, 11, 637, 11668, 15670, 7877, 13], "temperature": 0.0, "avg_logprob": -0.09954319953918457, "compression_ratio": 1.4413793103448276, "no_speech_prob": 2.9022537546552485e-06}, {"id": 47, "seek": 39400, "start": 409.0, "end": 418.0, "text": " Yeah, and the kind of general idea behind the power method is that we're multiplying A times...", "tokens": [865, 11, 293, 264, 733, 295, 2674, 1558, 2261, 264, 1347, 3170, 307, 300, 321, 434, 30955, 316, 1413, 485], "temperature": 0.0, "avg_logprob": -0.09954319953918457, "compression_ratio": 1.4413793103448276, "no_speech_prob": 2.9022537546552485e-06}, {"id": 48, "seek": 41800, "start": 418.0, "end": 428.0, "text": " I call it the scores because it's kind of the score of how important each page is, but you could also think of that as the probability of a person being on a particular page,", "tokens": [286, 818, 309, 264, 13444, 570, 309, 311, 733, 295, 264, 6175, 295, 577, 1021, 1184, 3028, 307, 11, 457, 291, 727, 611, 519, 295, 300, 382, 264, 8482, 295, 257, 954, 885, 322, 257, 1729, 3028, 11], "temperature": 0.0, "avg_logprob": -0.07512682676315308, "compression_ratio": 1.553191489361702, "no_speech_prob": 4.2892452256637625e-06}, {"id": 49, "seek": 41800, "start": 428.0, "end": 442.0, "text": " and we're just doing that a bunch of times, but normalizing to make sure that things don't go to zero or to infinity.", "tokens": [293, 321, 434, 445, 884, 300, 257, 3840, 295, 1413, 11, 457, 2710, 3319, 281, 652, 988, 300, 721, 500, 380, 352, 281, 4018, 420, 281, 13202, 13], "temperature": 0.0, "avg_logprob": -0.07512682676315308, "compression_ratio": 1.553191489361702, "no_speech_prob": 4.2892452256637625e-06}, {"id": 50, "seek": 44200, "start": 442.0, "end": 451.0, "text": " Oh, and then something I wanted to point out is, if you remember back in lesson three when we did surveillance video background removal,", "tokens": [876, 11, 293, 550, 746, 286, 1415, 281, 935, 484, 307, 11, 498, 291, 1604, 646, 294, 6898, 1045, 562, 321, 630, 18475, 960, 3678, 17933, 11], "temperature": 0.0, "avg_logprob": -0.12165565490722656, "compression_ratio": 1.427860696517413, "no_speech_prob": 7.410964826704003e-06}, {"id": 51, "seek": 44200, "start": 451.0, "end": 462.0, "text": " we used Facebook's fast randomized PCA SVD library, FBPCA, and in the source code I found a comment where they're using the power method at one point,", "tokens": [321, 1143, 4384, 311, 2370, 38513, 6465, 32, 31910, 35, 6405, 11, 479, 33, 12986, 32, 11, 293, 294, 264, 4009, 3089, 286, 1352, 257, 2871, 689, 436, 434, 1228, 264, 1347, 3170, 412, 472, 935, 11], "temperature": 0.0, "avg_logprob": -0.12165565490722656, "compression_ratio": 1.427860696517413, "no_speech_prob": 7.410964826704003e-06}, {"id": 52, "seek": 46200, "start": 462.0, "end": 482.0, "text": " which I thought was exciting, so showing up in the real world. Any questions on that and what we did last time?", "tokens": [597, 286, 1194, 390, 4670, 11, 370, 4099, 493, 294, 264, 957, 1002, 13, 2639, 1651, 322, 300, 293, 437, 321, 630, 1036, 565, 30], "temperature": 0.0, "avg_logprob": -0.1224701240144927, "compression_ratio": 1.168421052631579, "no_speech_prob": 1.4823325500401552e-06}, {"id": 53, "seek": 48200, "start": 482.0, "end": 499.0, "text": " Okay, so we'll go on to the QR algorithm. So last time we were just finding a single eigenvector, and the QR algorithm lets us find potentially all the eigenvectors.", "tokens": [1033, 11, 370, 321, 603, 352, 322, 281, 264, 32784, 9284, 13, 407, 1036, 565, 321, 645, 445, 5006, 257, 2167, 10446, 303, 1672, 11, 293, 264, 32784, 9284, 6653, 505, 915, 7263, 439, 264, 10446, 303, 5547, 13], "temperature": 0.0, "avg_logprob": -0.0886676034262014, "compression_ratio": 1.4102564102564104, "no_speech_prob": 1.4823283436271595e-06}, {"id": 54, "seek": 49900, "start": 499.0, "end": 516.0, "text": " Oh, and this is a paper called the second eigenvalue of the Google matrix, which I thought was neat, and it kind of lists some benefits of what you can get from even just the second eigenvalue,", "tokens": [876, 11, 293, 341, 307, 257, 3035, 1219, 264, 1150, 10446, 29155, 295, 264, 3329, 8141, 11, 597, 286, 1194, 390, 10654, 11, 293, 309, 733, 295, 14511, 512, 5311, 295, 437, 291, 393, 483, 490, 754, 445, 264, 1150, 10446, 29155, 11], "temperature": 0.0, "avg_logprob": -0.09857041785057555, "compression_ratio": 1.4402985074626866, "no_speech_prob": 7.527559773734538e-06}, {"id": 55, "seek": 51600, "start": 516.0, "end": 531.0, "text": " and that's that it relates to the convergence rate as well as the stability given kind of as the structure of the web's changing, detection of spammers, and for the design of algorithms to speed up paint-drink.", "tokens": [293, 300, 311, 300, 309, 16155, 281, 264, 32181, 3314, 382, 731, 382, 264, 11826, 2212, 733, 295, 382, 264, 3877, 295, 264, 3670, 311, 4473, 11, 17784, 295, 637, 48414, 11, 293, 337, 264, 1715, 295, 14642, 281, 3073, 493, 4225, 12, 16753, 475, 13], "temperature": 0.0, "avg_logprob": -0.10226836385606211, "compression_ratio": 1.5837320574162679, "no_speech_prob": 6.143859536678065e-06}, {"id": 56, "seek": 51600, "start": 531.0, "end": 539.0, "text": " You can check that out for more information. This is, yeah, again, kind of more practical uses of the second eigenvalue.", "tokens": [509, 393, 1520, 300, 484, 337, 544, 1589, 13, 639, 307, 11, 1338, 11, 797, 11, 733, 295, 544, 8496, 4960, 295, 264, 1150, 10446, 29155, 13], "temperature": 0.0, "avg_logprob": -0.10226836385606211, "compression_ratio": 1.5837320574162679, "no_speech_prob": 6.143859536678065e-06}, {"id": 57, "seek": 53900, "start": 539.0, "end": 552.0, "text": " So I just want to avoid confusion. So right now we're going to talk about the QR algorithm. Later in class we'll talk about the QR decomposition, and the QR decomposition is something we've been using from time to time,", "tokens": [407, 286, 445, 528, 281, 5042, 15075, 13, 407, 558, 586, 321, 434, 516, 281, 751, 466, 264, 32784, 9284, 13, 11965, 294, 1508, 321, 603, 751, 466, 264, 32784, 48356, 11, 293, 264, 32784, 48356, 307, 746, 321, 600, 668, 1228, 490, 565, 281, 565, 11], "temperature": 0.0, "avg_logprob": -0.07417968675202015, "compression_ratio": 1.5208333333333333, "no_speech_prob": 5.337895800039405e-06}, {"id": 58, "seek": 55200, "start": 552.0, "end": 571.0, "text": " and the decomposition is what takes A and gives you back Q and R, where Q has orthonormal columns in the reduced form, or for the full form it's an orthonormal matrix, and R is upper triangular.", "tokens": [293, 264, 48356, 307, 437, 2516, 316, 293, 2709, 291, 646, 1249, 293, 497, 11, 689, 1249, 575, 420, 11943, 24440, 13766, 294, 264, 9212, 1254, 11, 420, 337, 264, 1577, 1254, 309, 311, 364, 420, 11943, 24440, 8141, 11, 293, 497, 307, 6597, 38190, 13], "temperature": 0.0, "avg_logprob": -0.06585235118865967, "compression_ratio": 1.4057971014492754, "no_speech_prob": 1.3925049415774993e-06}, {"id": 59, "seek": 57100, "start": 571.0, "end": 583.0, "text": " And then the QR algorithm, which is different, uses the QR decomposition. So just to keep those separate in your mind.", "tokens": [400, 550, 264, 32784, 9284, 11, 597, 307, 819, 11, 4960, 264, 32784, 48356, 13, 407, 445, 281, 1066, 729, 4994, 294, 428, 1575, 13], "temperature": 0.0, "avg_logprob": -0.07126268025102286, "compression_ratio": 1.1683168316831682, "no_speech_prob": 5.626317829410254e-07}, {"id": 60, "seek": 58300, "start": 583.0, "end": 602.0, "text": " So a little bit of linear algebra review. Two matrices, A and B, are similar if there's a non-singular matrix X such that B equals X inverse AX. Does anyone remember what non-singular means?", "tokens": [407, 257, 707, 857, 295, 8213, 21989, 3131, 13, 4453, 32284, 11, 316, 293, 363, 11, 366, 2531, 498, 456, 311, 257, 2107, 12, 82, 278, 1040, 8141, 1783, 1270, 300, 363, 6915, 1783, 17340, 316, 55, 13, 4402, 2878, 1604, 437, 2107, 12, 82, 278, 1040, 1355, 30], "temperature": 0.0, "avg_logprob": -0.09032271043309625, "compression_ratio": 1.3380281690140845, "no_speech_prob": 1.4284501958172768e-05}, {"id": 61, "seek": 60200, "start": 602.0, "end": 614.0, "text": " Yes, it has full rank, and that's equivalent to saying it has an inverse, exactly. So this relationship, so we watched a 3Blue1Brown video last time.", "tokens": [1079, 11, 309, 575, 1577, 6181, 11, 293, 300, 311, 10344, 281, 1566, 309, 575, 364, 17340, 11, 2293, 13, 407, 341, 2480, 11, 370, 321, 6337, 257, 805, 45231, 16, 22170, 648, 960, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.1444562586342416, "compression_ratio": 1.221311475409836, "no_speech_prob": 4.222512416163227e-06}, {"id": 62, "seek": 61400, "start": 614.0, "end": 639.0, "text": " What's the kind of analogy they used about similar matrices? Kelsey? Sam, can you pass the microphone?", "tokens": [708, 311, 264, 733, 295, 21663, 436, 1143, 466, 2531, 32284, 30, 44714, 30, 4832, 11, 393, 291, 1320, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.09163359495309684, "compression_ratio": 1.0736842105263158, "no_speech_prob": 1.2482208830988384e-06}, {"id": 63, "seek": 63900, "start": 639.0, "end": 658.0, "text": " Yes, yeah, what's happening between A and B? Exactly, so A and B are kind of different coordinate systems, and we can think of kind of applying this X as a way to translate between them.", "tokens": [1079, 11, 1338, 11, 437, 311, 2737, 1296, 316, 293, 363, 30, 7587, 11, 370, 316, 293, 363, 366, 733, 295, 819, 15670, 3652, 11, 293, 321, 393, 519, 295, 733, 295, 9275, 341, 1783, 382, 257, 636, 281, 13799, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.14994071392302818, "compression_ratio": 1.3676470588235294, "no_speech_prob": 7.888940672273748e-06}, {"id": 64, "seek": 65800, "start": 658.0, "end": 673.0, "text": " So we're wanting to kind of translate from A to B. And they also, they had a nice, I think at one point they described this whenever you multiply by X inverse in front and X on the right hand side,", "tokens": [407, 321, 434, 7935, 281, 733, 295, 13799, 490, 316, 281, 363, 13, 400, 436, 611, 11, 436, 632, 257, 1481, 11, 286, 519, 412, 472, 935, 436, 7619, 341, 5699, 291, 12972, 538, 1783, 17340, 294, 1868, 293, 1783, 322, 264, 558, 1011, 1252, 11], "temperature": 0.0, "avg_logprob": -0.095755642407561, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.6797105217847275e-06}, {"id": 65, "seek": 65800, "start": 673.0, "end": 684.0, "text": " that that's kind of a mathematical form of empathy, but transforming A to kind of understand B better.", "tokens": [300, 300, 311, 733, 295, 257, 18894, 1254, 295, 18701, 11, 457, 27210, 316, 281, 733, 295, 1223, 363, 1101, 13], "temperature": 0.0, "avg_logprob": -0.095755642407561, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.6797105217847275e-06}, {"id": 66, "seek": 68400, "start": 684.0, "end": 709.0, "text": " So there's a theorem that A and X inverse AX have the same eigenvalues. So this is going to turn out to be useful, but A and, so here, A and B have the same eigenvalues if they're related this way.", "tokens": [407, 456, 311, 257, 20904, 300, 316, 293, 1783, 17340, 316, 55, 362, 264, 912, 10446, 46033, 13, 407, 341, 307, 516, 281, 1261, 484, 281, 312, 4420, 11, 457, 316, 293, 11, 370, 510, 11, 316, 293, 363, 362, 264, 912, 10446, 46033, 498, 436, 434, 4077, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.07591146989302201, "compression_ratio": 1.5271317829457365, "no_speech_prob": 8.66449045133777e-06}, {"id": 67, "seek": 70900, "start": 709.0, "end": 725.0, "text": " And then, yeah, I apologize that there's a new vocabulary, but a sure factorization is a factorization basically exactly like this only if the matrix in the middle is upper triangular.", "tokens": [400, 550, 11, 1338, 11, 286, 12328, 300, 456, 311, 257, 777, 19864, 11, 457, 257, 988, 5952, 2144, 307, 257, 5952, 2144, 1936, 2293, 411, 341, 787, 498, 264, 8141, 294, 264, 2808, 307, 6597, 38190, 13], "temperature": 0.0, "avg_logprob": -0.09644232477460589, "compression_ratio": 1.3834586466165413, "no_speech_prob": 1.9946812699345173e-06}, {"id": 68, "seek": 72500, "start": 725.0, "end": 746.0, "text": " So here, you would still say A and T are similar, and T is upper triangular. Actually, first I'll ask, does anyone know what the eigenvalues of T or of an upper triangular matrix are?", "tokens": [407, 510, 11, 291, 576, 920, 584, 316, 293, 314, 366, 2531, 11, 293, 314, 307, 6597, 38190, 13, 5135, 11, 700, 286, 603, 1029, 11, 775, 2878, 458, 437, 264, 10446, 46033, 295, 314, 420, 295, 364, 6597, 38190, 8141, 366, 30], "temperature": 0.0, "avg_logprob": -0.1492462564021983, "compression_ratio": 1.3357664233576643, "no_speech_prob": 2.2602880562772043e-06}, {"id": 69, "seek": 74600, "start": 746.0, "end": 758.0, "text": " Do you want to grab the microphone? Is it the diagonal? Exactly. Yeah, so the eigenvalues of a triangular matrix or the diagonal.", "tokens": [1144, 291, 528, 281, 4444, 264, 10952, 30, 1119, 309, 264, 21539, 30, 7587, 13, 865, 11, 370, 264, 10446, 46033, 295, 257, 38190, 8141, 420, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.16857426383278587, "compression_ratio": 1.443661971830986, "no_speech_prob": 4.565874860418262e-06}, {"id": 70, "seek": 74600, "start": 758.0, "end": 767.0, "text": " So what would the eigenvalues of A be in this case? You could say it. Yeah.", "tokens": [407, 437, 576, 264, 10446, 46033, 295, 316, 312, 294, 341, 1389, 30, 509, 727, 584, 309, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.16857426383278587, "compression_ratio": 1.443661971830986, "no_speech_prob": 4.565874860418262e-06}, {"id": 71, "seek": 76700, "start": 767.0, "end": 780.0, "text": " Yes, so this is why the sure factorization is going to end up being useful is that we're, you know, finding a matrix that's similar to, you know, there's nothing special about A, like it's not necessarily triangular,", "tokens": [1079, 11, 370, 341, 307, 983, 264, 988, 5952, 2144, 307, 516, 281, 917, 493, 885, 4420, 307, 300, 321, 434, 11, 291, 458, 11, 5006, 257, 8141, 300, 311, 2531, 281, 11, 291, 458, 11, 456, 311, 1825, 2121, 466, 316, 11, 411, 309, 311, 406, 4725, 38190, 11], "temperature": 0.0, "avg_logprob": -0.09374871747247104, "compression_ratio": 1.6820276497695852, "no_speech_prob": 1.7061657899830607e-06}, {"id": 72, "seek": 76700, "start": 780.0, "end": 794.0, "text": " but it's similar to a triangular matrix, and then it's really easy to get the eigenvalues of a triangular matrix because you just grab the diagonal.", "tokens": [457, 309, 311, 2531, 281, 257, 38190, 8141, 11, 293, 550, 309, 311, 534, 1858, 281, 483, 264, 10446, 46033, 295, 257, 38190, 8141, 570, 291, 445, 4444, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.09374871747247104, "compression_ratio": 1.6820276497695852, "no_speech_prob": 1.7061657899830607e-06}, {"id": 73, "seek": 79400, "start": 794.0, "end": 803.0, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.23764276504516602, "compression_ratio": 0.7142857142857143, "no_speech_prob": 1.750248520693276e-05}, {"id": 74, "seek": 79400, "start": 803.0, "end": 806.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.23764276504516602, "compression_ratio": 0.7142857142857143, "no_speech_prob": 1.750248520693276e-05}, {"id": 75, "seek": 80600, "start": 806.0, "end": 824.0, "text": " And so we're kind of going to go over the basic version of the QR algorithm, which is really simple, and then you have a loop and you get the QR decomposition of A, and then you kind of set your new A to be equal to R times Q.", "tokens": [400, 370, 321, 434, 733, 295, 516, 281, 352, 670, 264, 3875, 3037, 295, 264, 32784, 9284, 11, 597, 307, 534, 2199, 11, 293, 550, 291, 362, 257, 6367, 293, 291, 483, 264, 32784, 48356, 295, 316, 11, 293, 550, 291, 733, 295, 992, 428, 777, 316, 281, 312, 2681, 281, 497, 1413, 1249, 13], "temperature": 0.0, "avg_logprob": -0.09289820719573458, "compression_ratio": 1.516778523489933, "no_speech_prob": 1.1125328455818817e-05}, {"id": 76, "seek": 82400, "start": 824.0, "end": 838.0, "text": " And under certain conditions, this algorithm is going to converge and give you the sure form of A, which means that it's going to return something triangular.", "tokens": [400, 833, 1629, 4487, 11, 341, 9284, 307, 516, 281, 41881, 293, 976, 291, 264, 988, 1254, 295, 316, 11, 597, 1355, 300, 309, 311, 516, 281, 2736, 746, 38190, 13], "temperature": 0.0, "avg_logprob": -0.03535799639565604, "compression_ratio": 1.3389830508474576, "no_speech_prob": 4.222666120767826e-06}, {"id": 77, "seek": 83800, "start": 838.0, "end": 854.0, "text": " So now I've kind of written this. So here I've got some pseudocode. I've written it again with subscripts. And so I'm saying QK, RK is equal to A from your previous iteration.", "tokens": [407, 586, 286, 600, 733, 295, 3720, 341, 13, 407, 510, 286, 600, 658, 512, 25505, 532, 905, 1429, 13, 286, 600, 3720, 309, 797, 365, 2325, 39280, 13, 400, 370, 286, 478, 1566, 1249, 42, 11, 497, 42, 307, 2681, 281, 316, 490, 428, 3894, 24784, 13], "temperature": 0.0, "avg_logprob": -0.13806781402001014, "compression_ratio": 1.3059701492537314, "no_speech_prob": 4.15723616242758e-06}, {"id": 78, "seek": 85400, "start": 854.0, "end": 873.0, "text": " So we're going to use the QR factorization, and then we're setting our next K equal to R times Q, which is different than the matrix we just factored because we're taking the factors and switching their order.", "tokens": [407, 321, 434, 516, 281, 764, 264, 32784, 5952, 2144, 11, 293, 550, 321, 434, 3287, 527, 958, 591, 2681, 281, 497, 1413, 1249, 11, 597, 307, 819, 813, 264, 8141, 321, 445, 1186, 2769, 570, 321, 434, 1940, 264, 6771, 293, 16493, 641, 1668, 13], "temperature": 0.0, "avg_logprob": -0.11527228355407715, "compression_ratio": 1.4615384615384615, "no_speech_prob": 6.240686616365565e-06}, {"id": 79, "seek": 87300, "start": 873.0, "end": 885.0, "text": " So the way to think about this is, so AK equals QK times RK, because that's the definition of a QR factorization.", "tokens": [407, 264, 636, 281, 519, 466, 341, 307, 11, 370, 24789, 6915, 1249, 42, 1413, 497, 42, 11, 570, 300, 311, 264, 7123, 295, 257, 32784, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.14386552839136835, "compression_ratio": 1.4879518072289157, "no_speech_prob": 2.482432591932593e-06}, {"id": 80, "seek": 87300, "start": 885.0, "end": 902.0, "text": " Then we could multiply by Q inverse on each side and get QK inverse times AK equals RK. And why is that when I multiply by Q inverse?", "tokens": [1396, 321, 727, 12972, 538, 1249, 17340, 322, 1184, 1252, 293, 483, 1249, 42, 17340, 1413, 24789, 6915, 497, 42, 13, 400, 983, 307, 300, 562, 286, 12972, 538, 1249, 17340, 30], "temperature": 0.0, "avg_logprob": -0.14386552839136835, "compression_ratio": 1.4879518072289157, "no_speech_prob": 2.482432591932593e-06}, {"id": 81, "seek": 90200, "start": 902.0, "end": 920.0, "text": " Just how did I get from this line to this line? Wait, grab the microphone.", "tokens": [1449, 577, 630, 286, 483, 490, 341, 1622, 281, 341, 1622, 30, 3802, 11, 4444, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.14112893017855557, "compression_ratio": 1.0277777777777777, "no_speech_prob": 1.2029358003928792e-05}, {"id": 82, "seek": 92000, "start": 920.0, "end": 938.0, "text": " It cancels Q on the right side. Yeah, exactly, because Q is orthonormal. Thank you, Sam. So Q is orthonormal, so it cancels out when we multiply by Q inverse.", "tokens": [467, 393, 66, 1625, 1249, 322, 264, 558, 1252, 13, 865, 11, 2293, 11, 570, 1249, 307, 420, 11943, 24440, 13, 1044, 291, 11, 4832, 13, 407, 1249, 307, 420, 11943, 24440, 11, 370, 309, 393, 66, 1625, 484, 562, 321, 12972, 538, 1249, 17340, 13], "temperature": 0.0, "avg_logprob": -0.1536261749267578, "compression_ratio": 1.3389830508474576, "no_speech_prob": 3.2376806302636396e-06}, {"id": 83, "seek": 93800, "start": 938.0, "end": 951.0, "text": " So then we can kind of take this equation, and what happens when we multiply R times Q? We can say, oh, really, that's like saying Q inverse times A times Q.", "tokens": [407, 550, 321, 393, 733, 295, 747, 341, 5367, 11, 293, 437, 2314, 562, 321, 12972, 497, 1413, 1249, 30, 492, 393, 584, 11, 1954, 11, 534, 11, 300, 311, 411, 1566, 1249, 17340, 1413, 316, 1413, 1249, 13], "temperature": 0.0, "avg_logprob": -0.10656425979111221, "compression_ratio": 1.5539906103286385, "no_speech_prob": 3.966881649830611e-06}, {"id": 84, "seek": 93800, "start": 951.0, "end": 966.0, "text": " So I've just plugged this in for RK down here, and I get Q inverse AK QK. And so this, and we're not going to do kind of all the proofs for there's some other relationships.", "tokens": [407, 286, 600, 445, 25679, 341, 294, 337, 497, 42, 760, 510, 11, 293, 286, 483, 1249, 17340, 24789, 1249, 42, 13, 400, 370, 341, 11, 293, 321, 434, 406, 516, 281, 360, 733, 295, 439, 264, 8177, 82, 337, 456, 311, 512, 661, 6159, 13], "temperature": 0.0, "avg_logprob": -0.10656425979111221, "compression_ratio": 1.5539906103286385, "no_speech_prob": 3.966881649830611e-06}, {"id": 85, "seek": 96600, "start": 966.0, "end": 977.0, "text": " But the idea here is that by getting the QR factorization and then switching, we're kind of just repeatedly applying these similarity transformations to A.", "tokens": [583, 264, 1558, 510, 307, 300, 538, 1242, 264, 32784, 5952, 2144, 293, 550, 16493, 11, 321, 434, 733, 295, 445, 18227, 9275, 613, 32194, 34852, 281, 316, 13], "temperature": 0.0, "avg_logprob": -0.04245402957453872, "compression_ratio": 1.4427083333333333, "no_speech_prob": 3.0415233140956843e-06}, {"id": 86, "seek": 96600, "start": 977.0, "end": 992.0, "text": " And so after a bunch of iterations, we're ending up with QK inverse. Oh, there should be some dots here. Let me fix that.", "tokens": [400, 370, 934, 257, 3840, 295, 36540, 11, 321, 434, 8121, 493, 365, 1249, 42, 17340, 13, 876, 11, 456, 820, 312, 512, 15026, 510, 13, 961, 385, 3191, 300, 13], "temperature": 0.0, "avg_logprob": -0.04245402957453872, "compression_ratio": 1.4427083333333333, "no_speech_prob": 3.0415233140956843e-06}, {"id": 87, "seek": 99200, "start": 992.0, "end": 1003.0, "text": " OK, QK inverse dot dot dot on down to Q2 inverse, Q1 inverse times A times Q1 times Q2 dot dot dot up to QK.", "tokens": [2264, 11, 1249, 42, 17340, 5893, 5893, 5893, 322, 760, 281, 1249, 17, 17340, 11, 1249, 16, 17340, 1413, 316, 1413, 1249, 16, 1413, 1249, 17, 5893, 5893, 5893, 493, 281, 1249, 42, 13], "temperature": 0.0, "avg_logprob": -0.09184041649404198, "compression_ratio": 1.6140350877192982, "no_speech_prob": 3.72653994418215e-06}, {"id": 88, "seek": 99200, "start": 1003.0, "end": 1010.0, "text": " So what we're getting back is something that's definitely similar to our original matrix we started with.", "tokens": [407, 437, 321, 434, 1242, 646, 307, 746, 300, 311, 2138, 2531, 281, 527, 3380, 8141, 321, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.09184041649404198, "compression_ratio": 1.6140350877192982, "no_speech_prob": 3.72653994418215e-06}, {"id": 89, "seek": 99200, "start": 1010.0, "end": 1020.0, "text": " And then Trevathan has a proof showing that if we were, so I was using these subscript Ks to talk about the values we were getting each time we iterated.", "tokens": [400, 550, 8648, 85, 9390, 575, 257, 8177, 4099, 300, 498, 321, 645, 11, 370, 286, 390, 1228, 613, 2325, 662, 591, 82, 281, 751, 466, 264, 4190, 321, 645, 1242, 1184, 565, 321, 17138, 770, 13], "temperature": 0.0, "avg_logprob": -0.09184041649404198, "compression_ratio": 1.6140350877192982, "no_speech_prob": 3.72653994418215e-06}, {"id": 90, "seek": 102000, "start": 1020.0, "end": 1027.0, "text": " So just to denote that we had a different QR and A for each iteration. Here I'm taking a power again.", "tokens": [407, 445, 281, 45708, 300, 321, 632, 257, 819, 32784, 293, 316, 337, 1184, 24784, 13, 1692, 286, 478, 1940, 257, 1347, 797, 13], "temperature": 0.0, "avg_logprob": -0.0938819082159745, "compression_ratio": 1.4858490566037736, "no_speech_prob": 6.338918410619954e-06}, {"id": 91, "seek": 102000, "start": 1027.0, "end": 1039.0, "text": " So A raised to the Kth power is going to be equal to Q1 times Q2 dot dot dot QK times RK RK minus one dot dot dot R1.", "tokens": [407, 316, 6005, 281, 264, 591, 392, 1347, 307, 516, 281, 312, 2681, 281, 1249, 16, 1413, 1249, 17, 5893, 5893, 5893, 1249, 42, 1413, 497, 42, 497, 42, 3175, 472, 5893, 5893, 5893, 497, 16, 13], "temperature": 0.0, "avg_logprob": -0.0938819082159745, "compression_ratio": 1.4858490566037736, "no_speech_prob": 6.338918410619954e-06}, {"id": 92, "seek": 102000, "start": 1039.0, "end": 1045.0, "text": " And so you can look in Trevathan on page 216 to 217 for the proof of that if you're interested.", "tokens": [400, 370, 291, 393, 574, 294, 8648, 85, 9390, 322, 3028, 5080, 21, 281, 5080, 22, 337, 264, 8177, 295, 300, 498, 291, 434, 3102, 13], "temperature": 0.0, "avg_logprob": -0.0938819082159745, "compression_ratio": 1.4858490566037736, "no_speech_prob": 6.338918410619954e-06}, {"id": 93, "seek": 104500, "start": 1045.0, "end": 1057.0, "text": " But the kind of the main idea here is that the QR algorithm is coming up with orthonormal bases for successive powers of AK.", "tokens": [583, 264, 733, 295, 264, 2135, 1558, 510, 307, 300, 264, 32784, 9284, 307, 1348, 493, 365, 420, 11943, 24440, 17949, 337, 48043, 8674, 295, 24789, 13], "temperature": 0.0, "avg_logprob": -0.10796187958627376, "compression_ratio": 1.4335664335664335, "no_speech_prob": 9.665565812611021e-06}, {"id": 94, "seek": 104500, "start": 1057.0, "end": 1066.0, "text": " And so let's see what this looks like in code. So this is the pure QR algorithm.", "tokens": [400, 370, 718, 311, 536, 437, 341, 1542, 411, 294, 3089, 13, 407, 341, 307, 264, 6075, 32784, 9284, 13], "temperature": 0.0, "avg_logprob": -0.10796187958627376, "compression_ratio": 1.4335664335664335, "no_speech_prob": 9.665565812611021e-06}, {"id": 95, "seek": 106600, "start": 1066.0, "end": 1080.0, "text": " I'm making a copy of A, but you often would do this in place. And when you do something in place, you're, you know, writing over your original matrix, which saves memory.", "tokens": [286, 478, 1455, 257, 5055, 295, 316, 11, 457, 291, 2049, 576, 360, 341, 294, 1081, 13, 400, 562, 291, 360, 746, 294, 1081, 11, 291, 434, 11, 291, 458, 11, 3579, 670, 428, 3380, 8141, 11, 597, 19155, 4675, 13], "temperature": 0.0, "avg_logprob": -0.09911972926213192, "compression_ratio": 1.5146198830409356, "no_speech_prob": 6.240775746846339e-06}, {"id": 96, "seek": 106600, "start": 1080.0, "end": 1085.0, "text": " Although if you wanted to use your original matrix again later, that would be a problem.", "tokens": [5780, 498, 291, 1415, 281, 764, 428, 3380, 8141, 797, 1780, 11, 300, 576, 312, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.09911972926213192, "compression_ratio": 1.5146198830409356, "no_speech_prob": 6.240775746846339e-06}, {"id": 97, "seek": 108500, "start": 1085.0, "end": 1096.0, "text": " And so I'm just going through a bunch of iterate. Well, so I initialize Q to be the identity and then I.", "tokens": [400, 370, 286, 478, 445, 516, 807, 257, 3840, 295, 44497, 13, 1042, 11, 370, 286, 5883, 1125, 1249, 281, 312, 264, 6575, 293, 550, 286, 13], "temperature": 0.0, "avg_logprob": -0.18575386379076086, "compression_ratio": 1.3941176470588235, "no_speech_prob": 1.602778979759023e-06}, {"id": 98, "seek": 108500, "start": 1096.0, "end": 1102.0, "text": " I'm getting the QR factorization just using numPy's implementation.", "tokens": [286, 478, 1242, 264, 32784, 5952, 2144, 445, 1228, 1031, 47, 88, 311, 11420, 13], "temperature": 0.0, "avg_logprob": -0.18575386379076086, "compression_ratio": 1.3941176470588235, "no_speech_prob": 1.602778979759023e-06}, {"id": 99, "seek": 108500, "start": 1102.0, "end": 1108.0, "text": " Get back Q and R set AK equal to R times Q. So I flip the order.", "tokens": [3240, 646, 1249, 293, 497, 992, 24789, 2681, 281, 497, 1413, 1249, 13, 407, 286, 7929, 264, 1668, 13], "temperature": 0.0, "avg_logprob": -0.18575386379076086, "compression_ratio": 1.3941176470588235, "no_speech_prob": 1.602778979759023e-06}, {"id": 100, "seek": 110800, "start": 1108.0, "end": 1116.0, "text": " And then I'm keeping this kind of a running total of what happens each time you multiply Q by each other.", "tokens": [400, 550, 286, 478, 5145, 341, 733, 295, 257, 2614, 3217, 295, 437, 2314, 1184, 565, 291, 12972, 1249, 538, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.09493962272268827, "compression_ratio": 1.4313725490196079, "no_speech_prob": 2.4823759758874075e-06}, {"id": 101, "seek": 110800, "start": 1116.0, "end": 1123.0, "text": " And then at the end, I'm returning AK and QQ.", "tokens": [400, 550, 412, 264, 917, 11, 286, 478, 12678, 24789, 293, 1249, 48, 13], "temperature": 0.0, "avg_logprob": -0.09493962272268827, "compression_ratio": 1.4313725490196079, "no_speech_prob": 2.4823759758874075e-06}, {"id": 102, "seek": 110800, "start": 1123.0, "end": 1129.0, "text": " And so if we do that on this is just a randomly generated matrix A.", "tokens": [400, 370, 498, 321, 360, 300, 322, 341, 307, 445, 257, 16979, 10833, 8141, 316, 13], "temperature": 0.0, "avg_logprob": -0.09493962272268827, "compression_ratio": 1.4313725490196079, "no_speech_prob": 2.4823759758874075e-06}, {"id": 103, "seek": 112900, "start": 1129.0, "end": 1140.0, "text": " So let me. We do this with fewer iterations. This is a lot.", "tokens": [407, 718, 385, 13, 492, 360, 341, 365, 13366, 36540, 13, 639, 307, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.11915043092543079, "compression_ratio": 1.4102564102564104, "no_speech_prob": 2.190751501984778e-06}, {"id": 104, "seek": 112900, "start": 1140.0, "end": 1146.0, "text": " And so I'm just printing it out every 100 iterations so we can see how AK is updating.", "tokens": [400, 370, 286, 478, 445, 14699, 309, 484, 633, 2319, 36540, 370, 321, 393, 536, 577, 24789, 307, 25113, 13], "temperature": 0.0, "avg_logprob": -0.11915043092543079, "compression_ratio": 1.4102564102564104, "no_speech_prob": 2.190751501984778e-06}, {"id": 105, "seek": 112900, "start": 1146.0, "end": 1150.0, "text": " So what do you see happening? So this is our original. Actually, this is.", "tokens": [407, 437, 360, 291, 536, 2737, 30, 407, 341, 307, 527, 3380, 13, 5135, 11, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.11915043092543079, "compression_ratio": 1.4102564102564104, "no_speech_prob": 2.190751501984778e-06}, {"id": 106, "seek": 115000, "start": 1150.0, "end": 1160.0, "text": " This is after 10 iterations, but it's still kind of a full matrix. We put the original A here so you can compare.", "tokens": [639, 307, 934, 1266, 36540, 11, 457, 309, 311, 920, 733, 295, 257, 1577, 8141, 13, 492, 829, 264, 3380, 316, 510, 370, 291, 393, 6794, 13], "temperature": 0.0, "avg_logprob": -0.15424712137742477, "compression_ratio": 1.6066350710900474, "no_speech_prob": 2.9944012567284517e-06}, {"id": 107, "seek": 115000, "start": 1160.0, "end": 1164.0, "text": " So even after 10 iterations, we can see the bottom left hand corner.", "tokens": [407, 754, 934, 1266, 36540, 11, 321, 393, 536, 264, 2767, 1411, 1011, 4538, 13], "temperature": 0.0, "avg_logprob": -0.15424712137742477, "compression_ratio": 1.6066350710900474, "no_speech_prob": 2.9944012567284517e-06}, {"id": 108, "seek": 115000, "start": 1164.0, "end": 1172.0, "text": " You need this. This should be larger. Perhaps you're able to read this all right.", "tokens": [509, 643, 341, 13, 639, 820, 312, 4833, 13, 10517, 291, 434, 1075, 281, 1401, 341, 439, 558, 13], "temperature": 0.0, "avg_logprob": -0.15424712137742477, "compression_ratio": 1.6066350710900474, "no_speech_prob": 2.9944012567284517e-06}, {"id": 109, "seek": 115000, "start": 1172.0, "end": 1178.0, "text": " The. The bottom left hand corner is starting to get closer to zero, right?", "tokens": [440, 13, 440, 2767, 1411, 1011, 4538, 307, 2891, 281, 483, 4966, 281, 4018, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15424712137742477, "compression_ratio": 1.6066350710900474, "no_speech_prob": 2.9944012567284517e-06}, {"id": 110, "seek": 117800, "start": 1178.0, "end": 1184.0, "text": " We've gone from these values of like point four point five down to point oh six.", "tokens": [492, 600, 2780, 490, 613, 4190, 295, 411, 935, 1451, 935, 1732, 760, 281, 935, 1954, 2309, 13], "temperature": 0.0, "avg_logprob": -0.16262329618136087, "compression_ratio": 1.330827067669173, "no_speech_prob": 1.9947065084124915e-06}, {"id": 111, "seek": 117800, "start": 1184.0, "end": 1197.0, "text": " Oops.", "tokens": [21726, 13], "temperature": 0.0, "avg_logprob": -0.16262329618136087, "compression_ratio": 1.330827067669173, "no_speech_prob": 1.9947065084124915e-06}, {"id": 112, "seek": 117800, "start": 1197.0, "end": 1204.0, "text": " And then as it goes, now we're getting some zeros along this kind of left hand triangular.", "tokens": [400, 550, 382, 309, 1709, 11, 586, 321, 434, 1242, 512, 35193, 2051, 341, 733, 295, 1411, 1011, 38190, 13], "temperature": 0.0, "avg_logprob": -0.16262329618136087, "compression_ratio": 1.330827067669173, "no_speech_prob": 1.9947065084124915e-06}, {"id": 113, "seek": 120400, "start": 1204.0, "end": 1208.0, "text": " Getting more.", "tokens": [13674, 544, 13], "temperature": 0.0, "avg_logprob": -0.1278432112473708, "compression_ratio": 1.4142011834319526, "no_speech_prob": 5.422070444183191e-06}, {"id": 114, "seek": 120400, "start": 1208.0, "end": 1214.0, "text": " More. And so what is this matrix heading towards?", "tokens": [5048, 13, 400, 370, 437, 307, 341, 8141, 9864, 3030, 30], "temperature": 0.0, "avg_logprob": -0.1278432112473708, "compression_ratio": 1.4142011834319526, "no_speech_prob": 5.422070444183191e-06}, {"id": 115, "seek": 120400, "start": 1214.0, "end": 1217.0, "text": " What type of matrix?", "tokens": [708, 2010, 295, 8141, 30], "temperature": 0.0, "avg_logprob": -0.1278432112473708, "compression_ratio": 1.4142011834319526, "no_speech_prob": 5.422070444183191e-06}, {"id": 116, "seek": 120400, "start": 1217.0, "end": 1227.0, "text": " Exactly triangular. So we can see we're getting kind of more and more zeros in this bottom left part.", "tokens": [7587, 38190, 13, 407, 321, 393, 536, 321, 434, 1242, 733, 295, 544, 293, 544, 35193, 294, 341, 2767, 1411, 644, 13], "temperature": 0.0, "avg_logprob": -0.1278432112473708, "compression_ratio": 1.4142011834319526, "no_speech_prob": 5.422070444183191e-06}, {"id": 117, "seek": 120400, "start": 1227.0, "end": 1231.0, "text": " And so here I've just done this for 1000 iterations.", "tokens": [400, 370, 510, 286, 600, 445, 1096, 341, 337, 9714, 36540, 13], "temperature": 0.0, "avg_logprob": -0.1278432112473708, "compression_ratio": 1.4142011834319526, "no_speech_prob": 5.422070444183191e-06}, {"id": 118, "seek": 123100, "start": 1231.0, "end": 1236.0, "text": " We don't quite have a triangular matrix, but we've gotten quite close, right?", "tokens": [492, 500, 380, 1596, 362, 257, 38190, 8141, 11, 457, 321, 600, 5768, 1596, 1998, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.0802948086760765, "compression_ratio": 1.3252032520325203, "no_speech_prob": 3.844860202661948e-06}, {"id": 119, "seek": 123100, "start": 1236.0, "end": 1246.0, "text": " So we still have this point four seven here and here, but the other values are zeros.", "tokens": [407, 321, 920, 362, 341, 935, 1451, 3407, 510, 293, 510, 11, 457, 264, 661, 4190, 366, 35193, 13], "temperature": 0.0, "avg_logprob": -0.0802948086760765, "compression_ratio": 1.3252032520325203, "no_speech_prob": 3.844860202661948e-06}, {"id": 120, "seek": 124600, "start": 1246.0, "end": 1252.0, "text": " And if it was perfectly triangular, what would we expect?", "tokens": [400, 498, 309, 390, 6239, 38190, 11, 437, 576, 321, 2066, 30], "temperature": 0.0, "avg_logprob": -0.17770725641495141, "compression_ratio": 1.1735537190082646, "no_speech_prob": 7.646036465303041e-06}, {"id": 121, "seek": 124600, "start": 1252.0, "end": 1263.0, "text": " About the eigenvalues.", "tokens": [7769, 264, 10446, 46033, 13], "temperature": 0.0, "avg_logprob": -0.17770725641495141, "compression_ratio": 1.1735537190082646, "no_speech_prob": 7.646036465303041e-06}, {"id": 122, "seek": 124600, "start": 1263.0, "end": 1269.0, "text": " Exactly Sam, can you pass the microphone to Linda? Thank you.", "tokens": [7587, 4832, 11, 393, 291, 1320, 264, 10952, 281, 20324, 30, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.17770725641495141, "compression_ratio": 1.1735537190082646, "no_speech_prob": 7.646036465303041e-06}, {"id": 123, "seek": 126900, "start": 1269.0, "end": 1276.0, "text": " The diagonal T of A will equal to what the original is.", "tokens": [440, 21539, 314, 295, 316, 486, 2681, 281, 437, 264, 3380, 307, 13], "temperature": 0.0, "avg_logprob": -0.17289928436279298, "compression_ratio": 1.7989949748743719, "no_speech_prob": 6.338862476695795e-06}, {"id": 124, "seek": 126900, "start": 1276.0, "end": 1280.0, "text": " The eigenvalues. Yes. Yeah. So the eigenvalues of this.", "tokens": [440, 10446, 46033, 13, 1079, 13, 865, 13, 407, 264, 10446, 46033, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.17289928436279298, "compression_ratio": 1.7989949748743719, "no_speech_prob": 6.338862476695795e-06}, {"id": 125, "seek": 126900, "start": 1280.0, "end": 1285.0, "text": " A K or T that's triangular would be equal to the eigenvalues of the original.", "tokens": [316, 591, 420, 314, 300, 311, 38190, 576, 312, 2681, 281, 264, 10446, 46033, 295, 264, 3380, 13], "temperature": 0.0, "avg_logprob": -0.17289928436279298, "compression_ratio": 1.7989949748743719, "no_speech_prob": 6.338862476695795e-06}, {"id": 126, "seek": 126900, "start": 1285.0, "end": 1289.0, "text": " And so here we can compare that. What are the eigenvalues of A?", "tokens": [400, 370, 510, 321, 393, 6794, 300, 13, 708, 366, 264, 10446, 46033, 295, 316, 30], "temperature": 0.0, "avg_logprob": -0.17289928436279298, "compression_ratio": 1.7989949748743719, "no_speech_prob": 6.338862476695795e-06}, {"id": 127, "seek": 126900, "start": 1289.0, "end": 1293.0, "text": " Notice the first one two point seven eight. That's the first value here.", "tokens": [13428, 264, 700, 472, 732, 935, 3407, 3180, 13, 663, 311, 264, 700, 2158, 510, 13], "temperature": 0.0, "avg_logprob": -0.17289928436279298, "compression_ratio": 1.7989949748743719, "no_speech_prob": 6.338862476695795e-06}, {"id": 128, "seek": 126900, "start": 1293.0, "end": 1296.0, "text": " So we did find that eigenvalue.", "tokens": [407, 321, 630, 915, 300, 10446, 29155, 13], "temperature": 0.0, "avg_logprob": -0.17289928436279298, "compression_ratio": 1.7989949748743719, "no_speech_prob": 6.338862476695795e-06}, {"id": 129, "seek": 129600, "start": 1296.0, "end": 1302.0, "text": " We also have.", "tokens": [492, 611, 362, 13], "temperature": 0.0, "avg_logprob": -0.09723106183503803, "compression_ratio": 1.3777777777777778, "no_speech_prob": 1.4823359606452868e-06}, {"id": 130, "seek": 129600, "start": 1302.0, "end": 1307.0, "text": " Negative point one one eight three two. That's this this value.", "tokens": [43230, 935, 472, 472, 3180, 1045, 732, 13, 663, 311, 341, 341, 2158, 13], "temperature": 0.0, "avg_logprob": -0.09723106183503803, "compression_ratio": 1.3777777777777778, "no_speech_prob": 1.4823359606452868e-06}, {"id": 131, "seek": 129600, "start": 1307.0, "end": 1313.0, "text": " So we've let me see if we have any more.", "tokens": [407, 321, 600, 718, 385, 536, 498, 321, 362, 604, 544, 13], "temperature": 0.0, "avg_logprob": -0.09723106183503803, "compression_ratio": 1.3777777777777778, "no_speech_prob": 1.4823359606452868e-06}, {"id": 132, "seek": 129600, "start": 1313.0, "end": 1320.0, "text": " OK, so here it looks like we've just gotten two of the eigenvalues.", "tokens": [2264, 11, 370, 510, 309, 1542, 411, 321, 600, 445, 5768, 732, 295, 264, 10446, 46033, 13], "temperature": 0.0, "avg_logprob": -0.09723106183503803, "compression_ratio": 1.3777777777777778, "no_speech_prob": 1.4823359606452868e-06}, {"id": 133, "seek": 132000, "start": 1320.0, "end": 1328.0, "text": " But the idea is that when and if this converged, you could have all of them.", "tokens": [583, 264, 1558, 307, 300, 562, 293, 498, 341, 9652, 3004, 11, 291, 727, 362, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.09593080437701682, "compression_ratio": 1.25, "no_speech_prob": 2.0904087705275742e-06}, {"id": 134, "seek": 132000, "start": 1328.0, "end": 1331.0, "text": " And we can also check that the.", "tokens": [400, 321, 393, 611, 1520, 300, 264, 13], "temperature": 0.0, "avg_logprob": -0.09593080437701682, "compression_ratio": 1.25, "no_speech_prob": 2.0904087705275742e-06}, {"id": 135, "seek": 132000, "start": 1331.0, "end": 1338.0, "text": " The Q we get back is is worth the normal.", "tokens": [440, 1249, 321, 483, 646, 307, 307, 3163, 264, 2710, 13], "temperature": 0.0, "avg_logprob": -0.09593080437701682, "compression_ratio": 1.25, "no_speech_prob": 2.0904087705275742e-06}, {"id": 136, "seek": 133800, "start": 1338.0, "end": 1351.0, "text": " So downsides to this algorithm are that it's really slow and it's also not guaranteed to converge.", "tokens": [407, 21554, 1875, 281, 341, 9284, 366, 300, 309, 311, 534, 2964, 293, 309, 311, 611, 406, 18031, 281, 41881, 13], "temperature": 0.0, "avg_logprob": -0.06629293006763123, "compression_ratio": 1.4267515923566878, "no_speech_prob": 7.571046580778784e-07}, {"id": 137, "seek": 133800, "start": 1351.0, "end": 1359.0, "text": " But I think it is neat that we get. Yeah, we can get some of the eigenvalues with this because it's a fairly simple approach.", "tokens": [583, 286, 519, 309, 307, 10654, 300, 321, 483, 13, 865, 11, 321, 393, 483, 512, 295, 264, 10446, 46033, 365, 341, 570, 309, 311, 257, 6457, 2199, 3109, 13], "temperature": 0.0, "avg_logprob": -0.06629293006763123, "compression_ratio": 1.4267515923566878, "no_speech_prob": 7.571046580778784e-07}, {"id": 138, "seek": 135900, "start": 1359.0, "end": 1374.0, "text": " This idea of just taking the QR factorization and then switching them and doing R times Q and taking the Q factor QR factorization of that, switching those R times Q and getting the QR factorization of that.", "tokens": [639, 1558, 295, 445, 1940, 264, 32784, 5952, 2144, 293, 550, 16493, 552, 293, 884, 497, 1413, 1249, 293, 1940, 264, 1249, 5952, 32784, 5952, 2144, 295, 300, 11, 16493, 729, 497, 1413, 1249, 293, 1242, 264, 32784, 5952, 2144, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.14164846738179523, "compression_ratio": 1.8450704225352113, "no_speech_prob": 3.1874819796939846e-06}, {"id": 139, "seek": 135900, "start": 1374.0, "end": 1385.0, "text": " Any questions about this, about the pure QR algorithm?", "tokens": [2639, 1651, 466, 341, 11, 466, 264, 6075, 32784, 9284, 30], "temperature": 0.0, "avg_logprob": -0.14164846738179523, "compression_ratio": 1.8450704225352113, "no_speech_prob": 3.1874819796939846e-06}, {"id": 140, "seek": 138500, "start": 1385.0, "end": 1390.0, "text": " OK, so practical QR algorithm is adding shifts.", "tokens": [2264, 11, 370, 8496, 32784, 9284, 307, 5127, 19201, 13], "temperature": 0.0, "avg_logprob": -0.15239856202723617, "compression_ratio": 1.3445945945945945, "no_speech_prob": 4.356746103439946e-06}, {"id": 141, "seek": 138500, "start": 1390.0, "end": 1396.0, "text": " So the idea is basically just instead of factoring AK is QKRK.", "tokens": [407, 264, 1558, 307, 1936, 445, 2602, 295, 1186, 3662, 24789, 307, 1249, 42, 49, 42, 13], "temperature": 0.0, "avg_logprob": -0.15239856202723617, "compression_ratio": 1.3445945945945945, "no_speech_prob": 4.356746103439946e-06}, {"id": 142, "seek": 138500, "start": 1396.0, "end": 1401.0, "text": " We'll get the QR factorization of AK.", "tokens": [492, 603, 483, 264, 32784, 5952, 2144, 295, 24789, 13], "temperature": 0.0, "avg_logprob": -0.15239856202723617, "compression_ratio": 1.3445945945945945, "no_speech_prob": 4.356746103439946e-06}, {"id": 143, "seek": 138500, "start": 1401.0, "end": 1406.0, "text": " Minus some scalar times the identity into Q and R.", "tokens": [2829, 301, 512, 39684, 1413, 264, 6575, 666, 1249, 293, 497, 13], "temperature": 0.0, "avg_logprob": -0.15239856202723617, "compression_ratio": 1.3445945945945945, "no_speech_prob": 4.356746103439946e-06}, {"id": 144, "seek": 140600, "start": 1406.0, "end": 1415.0, "text": " And then we'll do R times Q and we have to kind of add back on the scalar times the identity to kind of cancel that out.", "tokens": [400, 550, 321, 603, 360, 497, 1413, 1249, 293, 321, 362, 281, 733, 295, 909, 646, 322, 264, 39684, 1413, 264, 6575, 281, 733, 295, 10373, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.08571795887417263, "compression_ratio": 1.6904761904761905, "no_speech_prob": 3.187456513842335e-06}, {"id": 145, "seek": 140600, "start": 1415.0, "end": 1419.0, "text": " And that'll be our new AK. So it's quite similar.", "tokens": [400, 300, 603, 312, 527, 777, 24789, 13, 407, 309, 311, 1596, 2531, 13], "temperature": 0.0, "avg_logprob": -0.08571795887417263, "compression_ratio": 1.6904761904761905, "no_speech_prob": 3.187456513842335e-06}, {"id": 146, "seek": 140600, "start": 1419.0, "end": 1432.0, "text": " It's just we've basically subtracted something from the diagonal, then the QR factorization, multiply R times Q and add back on what we had subtracted from the diagonal and keep going.", "tokens": [467, 311, 445, 321, 600, 1936, 16390, 292, 746, 490, 264, 21539, 11, 550, 264, 32784, 5952, 2144, 11, 12972, 497, 1413, 1249, 293, 909, 646, 322, 437, 321, 632, 16390, 292, 490, 264, 21539, 293, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.08571795887417263, "compression_ratio": 1.6904761904761905, "no_speech_prob": 3.187456513842335e-06}, {"id": 147, "seek": 143200, "start": 1432.0, "end": 1440.0, "text": " And this is called adding shifts. The SK is kind of the shift that you're like moving it over a little bit.", "tokens": [400, 341, 307, 1219, 5127, 19201, 13, 440, 21483, 307, 733, 295, 264, 5513, 300, 291, 434, 411, 2684, 309, 670, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.06414214576162942, "compression_ratio": 1.5746606334841629, "no_speech_prob": 1.0615572136885021e-05}, {"id": 148, "seek": 143200, "start": 1440.0, "end": 1446.0, "text": " And this speeds up convergence and also can help it converge in cases where it wouldn't otherwise.", "tokens": [400, 341, 16411, 493, 32181, 293, 611, 393, 854, 309, 41881, 294, 3331, 689, 309, 2759, 380, 5911, 13], "temperature": 0.0, "avg_logprob": -0.06414214576162942, "compression_ratio": 1.5746606334841629, "no_speech_prob": 1.0615572136885021e-05}, {"id": 149, "seek": 143200, "start": 1446.0, "end": 1456.0, "text": " And I really I wanted you to see this because it shows up in a lot of different numerical linear algebra methods as a way to speed things up.", "tokens": [400, 286, 534, 286, 1415, 291, 281, 536, 341, 570, 309, 3110, 493, 294, 257, 688, 295, 819, 29054, 8213, 21989, 7150, 382, 257, 636, 281, 3073, 721, 493, 13], "temperature": 0.0, "avg_logprob": -0.06414214576162942, "compression_ratio": 1.5746606334841629, "no_speech_prob": 1.0615572136885021e-05}, {"id": 150, "seek": 145600, "start": 1456.0, "end": 1465.0, "text": " And this will be this is the homework three problem is to take the PR the QR algorithm from above and add shifts to it.", "tokens": [400, 341, 486, 312, 341, 307, 264, 14578, 1045, 1154, 307, 281, 747, 264, 11568, 264, 32784, 9284, 490, 3673, 293, 909, 19201, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.0928803842459152, "compression_ratio": 1.5621621621621622, "no_speech_prob": 3.288675543444697e-06}, {"id": 151, "seek": 145600, "start": 1465.0, "end": 1483.0, "text": " So that'll just be kind of modifying this algorithm up here to no longer be no longer doing the QR decomposition on AK, but using AK minus the scalar times the identity.", "tokens": [407, 300, 603, 445, 312, 733, 295, 42626, 341, 9284, 493, 510, 281, 572, 2854, 312, 572, 2854, 884, 264, 32784, 48356, 322, 24789, 11, 457, 1228, 24789, 3175, 264, 39684, 1413, 264, 6575, 13], "temperature": 0.0, "avg_logprob": -0.0928803842459152, "compression_ratio": 1.5621621621621622, "no_speech_prob": 3.288675543444697e-06}, {"id": 152, "seek": 148300, "start": 1483.0, "end": 1495.0, "text": " And the recommendation for. So ideally you want SK to be an eigenvalue that you've already calculated.", "tokens": [400, 264, 11879, 337, 13, 407, 22915, 291, 528, 21483, 281, 312, 364, 10446, 29155, 300, 291, 600, 1217, 15598, 13], "temperature": 0.0, "avg_logprob": -0.11199832380863659, "compression_ratio": 1.5894039735099337, "no_speech_prob": 8.267516932392027e-06}, {"id": 153, "seek": 148300, "start": 1495.0, "end": 1503.0, "text": " And you can kind of just use the diagonal of where you're current or use a value from the diagonal of where you're currently at for that.", "tokens": [400, 291, 393, 733, 295, 445, 764, 264, 21539, 295, 689, 291, 434, 2190, 420, 764, 257, 2158, 490, 264, 21539, 295, 689, 291, 434, 4362, 412, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.11199832380863659, "compression_ratio": 1.5894039735099337, "no_speech_prob": 8.267516932392027e-06}, {"id": 154, "seek": 150300, "start": 1503.0, "end": 1513.0, "text": " Any questions about this?", "tokens": [2639, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.1268494509268498, "compression_ratio": 1.3988439306358382, "no_speech_prob": 5.954769676463911e-06}, {"id": 155, "seek": 150300, "start": 1513.0, "end": 1517.0, "text": " Right. So in that case, we're going to.", "tokens": [1779, 13, 407, 294, 300, 1389, 11, 321, 434, 516, 281, 13], "temperature": 0.0, "avg_logprob": -0.1268494509268498, "compression_ratio": 1.3988439306358382, "no_speech_prob": 5.954769676463911e-06}, {"id": 156, "seek": 150300, "start": 1517.0, "end": 1522.0, "text": " Move on to. Oh, actually, I should just comment a few things.", "tokens": [10475, 322, 281, 13, 876, 11, 767, 11, 286, 820, 445, 2871, 257, 1326, 721, 13], "temperature": 0.0, "avg_logprob": -0.1268494509268498, "compression_ratio": 1.3988439306358382, "no_speech_prob": 5.954769676463911e-06}, {"id": 157, "seek": 150300, "start": 1522.0, "end": 1529.0, "text": " So this is a lot better than the unshifted version, since that was not guaranteed to converge and was really slow.", "tokens": [407, 341, 307, 257, 688, 1101, 813, 264, 2693, 71, 2008, 292, 3037, 11, 1670, 300, 390, 406, 18031, 281, 41881, 293, 390, 534, 2964, 13], "temperature": 0.0, "avg_logprob": -0.1268494509268498, "compression_ratio": 1.3988439306358382, "no_speech_prob": 5.954769676463911e-06}, {"id": 158, "seek": 152900, "start": 1529.0, "end": 1538.0, "text": " But this is this is still order of N to the fourth power, which is bad.", "tokens": [583, 341, 307, 341, 307, 920, 1668, 295, 426, 281, 264, 6409, 1347, 11, 597, 307, 1578, 13], "temperature": 0.0, "avg_logprob": -0.12205497554091156, "compression_ratio": 1.3782051282051282, "no_speech_prob": 1.5534776593995048e-06}, {"id": 159, "seek": 152900, "start": 1538.0, "end": 1544.0, "text": " For for runtime. For symmetric matrices, it would be O of N cubed.", "tokens": [1171, 337, 34474, 13, 1171, 32330, 32284, 11, 309, 576, 312, 422, 295, 426, 36510, 13], "temperature": 0.0, "avg_logprob": -0.12205497554091156, "compression_ratio": 1.3782051282051282, "no_speech_prob": 1.5534776593995048e-06}, {"id": 160, "seek": 152900, "start": 1544.0, "end": 1549.0, "text": " So we've got a method for finding all the eigenvalues, but it's really slow.", "tokens": [407, 321, 600, 658, 257, 3170, 337, 5006, 439, 264, 10446, 46033, 11, 457, 309, 311, 534, 2964, 13], "temperature": 0.0, "avg_logprob": -0.12205497554091156, "compression_ratio": 1.3782051282051282, "no_speech_prob": 1.5534776593995048e-06}, {"id": 161, "seek": 154900, "start": 1549.0, "end": 1561.0, "text": " And so what's done in practice is that if you start with something called a Hessenberg matrix, which has zeros below the first sub diagonal, it's a lot faster.", "tokens": [400, 370, 437, 311, 1096, 294, 3124, 307, 300, 498, 291, 722, 365, 746, 1219, 257, 24951, 6873, 8141, 11, 597, 575, 35193, 2507, 264, 700, 1422, 21539, 11, 309, 311, 257, 688, 4663, 13], "temperature": 0.0, "avg_logprob": -0.09326532775280523, "compression_ratio": 1.6468253968253967, "no_speech_prob": 1.5056625670695212e-06}, {"id": 162, "seek": 154900, "start": 1561.0, "end": 1568.0, "text": " And so in practice, what you'll do. So this picture is from Trevathan is you'll have.", "tokens": [400, 370, 294, 3124, 11, 437, 291, 603, 360, 13, 407, 341, 3036, 307, 490, 8648, 85, 9390, 307, 291, 603, 362, 13], "temperature": 0.0, "avg_logprob": -0.09326532775280523, "compression_ratio": 1.6468253968253967, "no_speech_prob": 1.5056625670695212e-06}, {"id": 163, "seek": 154900, "start": 1568.0, "end": 1578.0, "text": " So your QR algorithm is going to end up being phase two, but you'll have a phase one that introduces a bunch of zeros and gets you to something that's almost triangular.", "tokens": [407, 428, 32784, 9284, 307, 516, 281, 917, 493, 885, 5574, 732, 11, 457, 291, 603, 362, 257, 5574, 472, 300, 31472, 257, 3840, 295, 35193, 293, 2170, 291, 281, 746, 300, 311, 1920, 38190, 13], "temperature": 0.0, "avg_logprob": -0.09326532775280523, "compression_ratio": 1.6468253968253967, "no_speech_prob": 1.5056625670695212e-06}, {"id": 164, "seek": 157800, "start": 1578.0, "end": 1584.0, "text": " So you'll see this matrix H is like a triangular matrix, except it has an extra.", "tokens": [407, 291, 603, 536, 341, 8141, 389, 307, 411, 257, 38190, 8141, 11, 3993, 309, 575, 364, 2857, 13], "temperature": 0.0, "avg_logprob": -0.09718123147653979, "compression_ratio": 1.6394230769230769, "no_speech_prob": 3.1380209293274675e-06}, {"id": 165, "seek": 157800, "start": 1584.0, "end": 1589.0, "text": " This is called a sub diagonal, but right below the diagonal, there's some non zero values.", "tokens": [639, 307, 1219, 257, 1422, 21539, 11, 457, 558, 2507, 264, 21539, 11, 456, 311, 512, 2107, 4018, 4190, 13], "temperature": 0.0, "avg_logprob": -0.09718123147653979, "compression_ratio": 1.6394230769230769, "no_speech_prob": 3.1380209293274675e-06}, {"id": 166, "seek": 157800, "start": 1589.0, "end": 1594.0, "text": " And so you'll kind of use one algorithm. So get things to this Hessenberg form.", "tokens": [400, 370, 291, 603, 733, 295, 764, 472, 9284, 13, 407, 483, 721, 281, 341, 24951, 6873, 1254, 13], "temperature": 0.0, "avg_logprob": -0.09718123147653979, "compression_ratio": 1.6394230769230769, "no_speech_prob": 3.1380209293274675e-06}, {"id": 167, "seek": 157800, "start": 1594.0, "end": 1602.0, "text": " And then you could use the QR algorithm to get from Hessenberg to your triangular matrix.", "tokens": [400, 550, 291, 727, 764, 264, 32784, 9284, 281, 483, 490, 24951, 6873, 281, 428, 38190, 8141, 13], "temperature": 0.0, "avg_logprob": -0.09718123147653979, "compression_ratio": 1.6394230769230769, "no_speech_prob": 3.1380209293274675e-06}, {"id": 168, "seek": 160200, "start": 1602.0, "end": 1610.0, "text": " Matthew and Linda, can you pass the microphone?", "tokens": [12434, 293, 20324, 11, 393, 291, 1320, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.1942791991181426, "compression_ratio": 1.5643564356435644, "no_speech_prob": 8.139330930134747e-06}, {"id": 169, "seek": 160200, "start": 1610.0, "end": 1613.0, "text": " I would just keep going.", "tokens": [286, 576, 445, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.1942791991181426, "compression_ratio": 1.5643564356435644, "no_speech_prob": 8.139330930134747e-06}, {"id": 170, "seek": 160200, "start": 1613.0, "end": 1622.0, "text": " So you could. It's just really slow. So this is a way to feed it up, speed it up, is that this phase one is going to be much quicker.", "tokens": [407, 291, 727, 13, 467, 311, 445, 534, 2964, 13, 407, 341, 307, 257, 636, 281, 3154, 309, 493, 11, 3073, 309, 493, 11, 307, 300, 341, 5574, 472, 307, 516, 281, 312, 709, 16255, 13], "temperature": 0.0, "avg_logprob": -0.1942791991181426, "compression_ratio": 1.5643564356435644, "no_speech_prob": 8.139330930134747e-06}, {"id": 171, "seek": 160200, "start": 1622.0, "end": 1626.0, "text": " Yeah, but it would work if you had the time.", "tokens": [865, 11, 457, 309, 576, 589, 498, 291, 632, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.1942791991181426, "compression_ratio": 1.5643564356435644, "no_speech_prob": 8.139330930134747e-06}, {"id": 172, "seek": 160200, "start": 1626.0, "end": 1630.0, "text": " So it's faster up to that point and then slower after the point.", "tokens": [407, 309, 311, 4663, 493, 281, 300, 935, 293, 550, 14009, 934, 264, 935, 13], "temperature": 0.0, "avg_logprob": -0.1942791991181426, "compression_ratio": 1.5643564356435644, "no_speech_prob": 8.139330930134747e-06}, {"id": 173, "seek": 163000, "start": 1630.0, "end": 1640.0, "text": " So it's like, well, we're using a new algorithm here. So then, yeah, kind of once you're at this Hessenberg, you're then it's faster to just get to the. Yeah.", "tokens": [407, 309, 311, 411, 11, 731, 11, 321, 434, 1228, 257, 777, 9284, 510, 13, 407, 550, 11, 1338, 11, 733, 295, 1564, 291, 434, 412, 341, 24951, 6873, 11, 291, 434, 550, 309, 311, 4663, 281, 445, 483, 281, 264, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.09994785063857332, "compression_ratio": 1.663716814159292, "no_speech_prob": 4.860231001657667e-06}, {"id": 174, "seek": 163000, "start": 1640.0, "end": 1644.0, "text": " Yeah, good question.", "tokens": [865, 11, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.09994785063857332, "compression_ratio": 1.663716814159292, "no_speech_prob": 4.860231001657667e-06}, {"id": 175, "seek": 163000, "start": 1644.0, "end": 1648.0, "text": " OK, and so, yeah, we're not going to we're not going to talk about phase one.", "tokens": [2264, 11, 293, 370, 11, 1338, 11, 321, 434, 406, 516, 281, 321, 434, 406, 516, 281, 751, 466, 5574, 472, 13], "temperature": 0.0, "avg_logprob": -0.09994785063857332, "compression_ratio": 1.663716814159292, "no_speech_prob": 4.860231001657667e-06}, {"id": 176, "seek": 163000, "start": 1648.0, "end": 1657.0, "text": " If you're interested, I've written about it in the notebook, though, and so you can go through this later on your own.", "tokens": [759, 291, 434, 3102, 11, 286, 600, 3720, 466, 309, 294, 264, 21060, 11, 1673, 11, 293, 370, 291, 393, 352, 807, 341, 1780, 322, 428, 1065, 13], "temperature": 0.0, "avg_logprob": -0.09994785063857332, "compression_ratio": 1.663716814159292, "no_speech_prob": 4.860231001657667e-06}, {"id": 177, "seek": 165700, "start": 1657.0, "end": 1664.0, "text": " But, yeah, I just wanted to kind of at least give you a little bit of a map of how finding the eigenvalues works.", "tokens": [583, 11, 1338, 11, 286, 445, 1415, 281, 733, 295, 412, 1935, 976, 291, 257, 707, 857, 295, 257, 4471, 295, 577, 5006, 264, 10446, 46033, 1985, 13], "temperature": 0.0, "avg_logprob": -0.08521499866392554, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.288620746388915e-06}, {"id": 178, "seek": 165700, "start": 1664.0, "end": 1672.0, "text": " And this is like I would say a fairly complicated area of numerical linear algebra just because you're kind of having to like switch between algorithms.", "tokens": [400, 341, 307, 411, 286, 576, 584, 257, 6457, 6179, 1859, 295, 29054, 8213, 21989, 445, 570, 291, 434, 733, 295, 1419, 281, 411, 3679, 1296, 14642, 13], "temperature": 0.0, "avg_logprob": -0.08521499866392554, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.288620746388915e-06}, {"id": 179, "seek": 165700, "start": 1672.0, "end": 1686.0, "text": " And there are like different versions of what you can use for phase one or phase two.", "tokens": [400, 456, 366, 411, 819, 9606, 295, 437, 291, 393, 764, 337, 5574, 472, 420, 5574, 732, 13], "temperature": 0.0, "avg_logprob": -0.08521499866392554, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.288620746388915e-06}, {"id": 180, "seek": 168600, "start": 1686.0, "end": 1690.0, "text": " I don't want to go bigger.", "tokens": [286, 500, 380, 528, 281, 352, 3801, 13], "temperature": 0.0, "avg_logprob": -0.09921696450975206, "compression_ratio": 1.4860335195530727, "no_speech_prob": 4.494906079344219e-06}, {"id": 181, "seek": 168600, "start": 1690.0, "end": 1694.0, "text": " OK, so now we're going to start notebook eight.", "tokens": [2264, 11, 370, 586, 321, 434, 516, 281, 722, 21060, 3180, 13], "temperature": 0.0, "avg_logprob": -0.09921696450975206, "compression_ratio": 1.4860335195530727, "no_speech_prob": 4.494906079344219e-06}, {"id": 182, "seek": 168600, "start": 1694.0, "end": 1697.0, "text": " This is this is exciting. Final notebook.", "tokens": [639, 307, 341, 307, 4670, 13, 13443, 21060, 13], "temperature": 0.0, "avg_logprob": -0.09921696450975206, "compression_ratio": 1.4860335195530727, "no_speech_prob": 4.494906079344219e-06}, {"id": 183, "seek": 168600, "start": 1697.0, "end": 1710.0, "text": " And then this is also we're going to learn how to implement the QR factorization, which is something we've been using kind of for most of the course.", "tokens": [400, 550, 341, 307, 611, 321, 434, 516, 281, 1466, 577, 281, 4445, 264, 32784, 5952, 2144, 11, 597, 307, 746, 321, 600, 668, 1228, 733, 295, 337, 881, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.09921696450975206, "compression_ratio": 1.4860335195530727, "no_speech_prob": 4.494906079344219e-06}, {"id": 184, "seek": 171000, "start": 1710.0, "end": 1716.0, "text": " So you'll remember we used it just now for computing eigenvalues.", "tokens": [407, 291, 603, 1604, 321, 1143, 309, 445, 586, 337, 15866, 10446, 46033, 13], "temperature": 0.0, "avg_logprob": -0.08965387449159727, "compression_ratio": 1.5743801652892562, "no_speech_prob": 2.769204456853913e-06}, {"id": 185, "seek": 171000, "start": 1716.0, "end": 1720.0, "text": " It was also a way to compute the least squares regression.", "tokens": [467, 390, 611, 257, 636, 281, 14722, 264, 1935, 19368, 24590, 13], "temperature": 0.0, "avg_logprob": -0.08965387449159727, "compression_ratio": 1.5743801652892562, "no_speech_prob": 2.769204456853913e-06}, {"id": 186, "seek": 171000, "start": 1720.0, "end": 1727.0, "text": " It showed up in the primary component pursuit, how we did robust PCA showed up in our randomized rangefinder.", "tokens": [467, 4712, 493, 294, 264, 6194, 6542, 23365, 11, 577, 321, 630, 13956, 6465, 32, 4712, 493, 294, 527, 38513, 3613, 38977, 13], "temperature": 0.0, "avg_logprob": -0.08965387449159727, "compression_ratio": 1.5743801652892562, "no_speech_prob": 2.769204456853913e-06}, {"id": 187, "seek": 171000, "start": 1727.0, "end": 1731.0, "text": " So it's a really important factorization.", "tokens": [407, 309, 311, 257, 534, 1021, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.08965387449159727, "compression_ratio": 1.5743801652892562, "no_speech_prob": 2.769204456853913e-06}, {"id": 188, "seek": 171000, "start": 1731.0, "end": 1736.0, "text": " In fact, Trevathan says one algorithm in numerical linear algebra is more important than all the others.", "tokens": [682, 1186, 11, 8648, 85, 9390, 1619, 472, 9284, 294, 29054, 8213, 21989, 307, 544, 1021, 813, 439, 264, 2357, 13], "temperature": 0.0, "avg_logprob": -0.08965387449159727, "compression_ratio": 1.5743801652892562, "no_speech_prob": 2.769204456853913e-06}, {"id": 189, "seek": 173600, "start": 1736.0, "end": 1746.0, "text": " QR factorization. So real kind of real fundamental building block.", "tokens": [32784, 5952, 2144, 13, 407, 957, 733, 295, 957, 8088, 2390, 3461, 13], "temperature": 0.0, "avg_logprob": -0.1880794744022557, "compression_ratio": 1.36986301369863, "no_speech_prob": 2.123306103385403e-06}, {"id": 190, "seek": 173600, "start": 1746.0, "end": 1749.0, "text": " So here I'm just illustrating.", "tokens": [407, 510, 286, 478, 445, 8490, 8754, 13], "temperature": 0.0, "avg_logprob": -0.1880794744022557, "compression_ratio": 1.36986301369863, "no_speech_prob": 2.123306103385403e-06}, {"id": 191, "seek": 173600, "start": 1749.0, "end": 1752.0, "text": " So we're using numPy's QR factorization.", "tokens": [407, 321, 434, 1228, 1031, 47, 88, 311, 32784, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.1880794744022557, "compression_ratio": 1.36986301369863, "no_speech_prob": 2.123306103385403e-06}, {"id": 192, "seek": 173600, "start": 1752.0, "end": 1758.0, "text": " We take in matrix A, we get back Q and R, and we can confirm.", "tokens": [492, 747, 294, 8141, 316, 11, 321, 483, 646, 1249, 293, 497, 11, 293, 321, 393, 9064, 13], "temperature": 0.0, "avg_logprob": -0.1880794744022557, "compression_ratio": 1.36986301369863, "no_speech_prob": 2.123306103385403e-06}, {"id": 193, "seek": 175800, "start": 1758.0, "end": 1770.0, "text": " Q is Q's or orthonormal and R is triangular.", "tokens": [1249, 307, 1249, 311, 420, 420, 11943, 24440, 293, 497, 307, 38190, 13], "temperature": 0.0, "avg_logprob": -0.12979293236365685, "compression_ratio": 1.4725274725274726, "no_speech_prob": 3.611867668951163e-06}, {"id": 194, "seek": 175800, "start": 1770.0, "end": 1775.0, "text": " And so today we're going to actually talk about three different.", "tokens": [400, 370, 965, 321, 434, 516, 281, 767, 751, 466, 1045, 819, 13], "temperature": 0.0, "avg_logprob": -0.12979293236365685, "compression_ratio": 1.4725274725274726, "no_speech_prob": 3.611867668951163e-06}, {"id": 195, "seek": 175800, "start": 1775.0, "end": 1785.0, "text": " Methods for finding the QR factorization and there'll be kind of some fun examples at the end of how they have different stability properties from each other.", "tokens": [25285, 82, 337, 5006, 264, 32784, 5952, 2144, 293, 456, 603, 312, 733, 295, 512, 1019, 5110, 412, 264, 917, 295, 577, 436, 362, 819, 11826, 7221, 490, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.12979293236365685, "compression_ratio": 1.4725274725274726, "no_speech_prob": 3.611867668951163e-06}, {"id": 196, "seek": 178500, "start": 1785.0, "end": 1791.0, "text": " But first I wanted to review the idea of projections in linear algebra.", "tokens": [583, 700, 286, 1415, 281, 3131, 264, 1558, 295, 32371, 294, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.10941885102469966, "compression_ratio": 1.5379310344827586, "no_speech_prob": 1.3845387911715079e-05}, {"id": 197, "seek": 178500, "start": 1791.0, "end": 1800.0, "text": " So the idea of a projection is that actually here I should go ahead and go to this link.", "tokens": [407, 264, 1558, 295, 257, 22743, 307, 300, 767, 510, 286, 820, 352, 2286, 293, 352, 281, 341, 2113, 13], "temperature": 0.0, "avg_logprob": -0.10941885102469966, "compression_ratio": 1.5379310344827586, "no_speech_prob": 1.3845387911715079e-05}, {"id": 198, "seek": 178500, "start": 1800.0, "end": 1808.0, "text": " This is there's something called the immersive linear algebra.", "tokens": [639, 307, 456, 311, 746, 1219, 264, 35409, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.10941885102469966, "compression_ratio": 1.5379310344827586, "no_speech_prob": 1.3845387911715079e-05}, {"id": 199, "seek": 180800, "start": 1808.0, "end": 1820.0, "text": " Textbook and it's still kind of to be like fairly mathy and it's explanations of things, but all the graphics are interactive, which is kind of nice.", "tokens": [18643, 2939, 293, 309, 311, 920, 733, 295, 281, 312, 411, 6457, 5221, 88, 293, 309, 311, 28708, 295, 721, 11, 457, 439, 264, 11837, 366, 15141, 11, 597, 307, 733, 295, 1481, 13], "temperature": 0.0, "avg_logprob": -0.15321867393724847, "compression_ratio": 1.391566265060241, "no_speech_prob": 1.4509901120618451e-05}, {"id": 200, "seek": 180800, "start": 1820.0, "end": 1823.0, "text": " So I was just going to show their.", "tokens": [407, 286, 390, 445, 516, 281, 855, 641, 13], "temperature": 0.0, "avg_logprob": -0.15321867393724847, "compression_ratio": 1.391566265060241, "no_speech_prob": 1.4509901120618451e-05}, {"id": 201, "seek": 180800, "start": 1823.0, "end": 1832.0, "text": " Their example for projection.", "tokens": [6710, 1365, 337, 22743, 13], "temperature": 0.0, "avg_logprob": -0.15321867393724847, "compression_ratio": 1.391566265060241, "no_speech_prob": 1.4509901120618451e-05}, {"id": 202, "seek": 180800, "start": 1832.0, "end": 1834.0, "text": " If it loads.", "tokens": [759, 309, 12668, 13], "temperature": 0.0, "avg_logprob": -0.15321867393724847, "compression_ratio": 1.391566265060241, "no_speech_prob": 1.4509901120618451e-05}, {"id": 203, "seek": 180800, "start": 1834.0, "end": 1836.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.15321867393724847, "compression_ratio": 1.391566265060241, "no_speech_prob": 1.4509901120618451e-05}, {"id": 204, "seek": 183600, "start": 1836.0, "end": 1841.0, "text": " So the basic idea is here we have a vector U that we're projecting onto V.", "tokens": [407, 264, 3875, 1558, 307, 510, 321, 362, 257, 8062, 624, 300, 321, 434, 43001, 3911, 691, 13], "temperature": 0.0, "avg_logprob": -0.1298719762445806, "compression_ratio": 1.587378640776699, "no_speech_prob": 1.4062810805626214e-05}, {"id": 205, "seek": 183600, "start": 1841.0, "end": 1845.0, "text": " This is their diagram for 3.4.", "tokens": [639, 307, 641, 10686, 337, 805, 13, 19, 13], "temperature": 0.0, "avg_logprob": -0.1298719762445806, "compression_ratio": 1.587378640776699, "no_speech_prob": 1.4062810805626214e-05}, {"id": 206, "seek": 183600, "start": 1845.0, "end": 1849.0, "text": " And then W is the projection.", "tokens": [400, 550, 343, 307, 264, 22743, 13], "temperature": 0.0, "avg_logprob": -0.1298719762445806, "compression_ratio": 1.587378640776699, "no_speech_prob": 1.4062810805626214e-05}, {"id": 207, "seek": 183600, "start": 1849.0, "end": 1853.0, "text": " Move you around and see how how W is changing.", "tokens": [10475, 291, 926, 293, 536, 577, 577, 343, 307, 4473, 13], "temperature": 0.0, "avg_logprob": -0.1298719762445806, "compression_ratio": 1.587378640776699, "no_speech_prob": 1.4062810805626214e-05}, {"id": 208, "seek": 183600, "start": 1853.0, "end": 1864.0, "text": " What it is is you can think of it as you're kind of decomposing you into this component W that lies along along V, as well as something that is.", "tokens": [708, 309, 307, 307, 291, 393, 519, 295, 309, 382, 291, 434, 733, 295, 22867, 6110, 291, 666, 341, 6542, 343, 300, 9134, 2051, 2051, 691, 11, 382, 731, 382, 746, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.1298719762445806, "compression_ratio": 1.587378640776699, "no_speech_prob": 1.4062810805626214e-05}, {"id": 209, "seek": 186400, "start": 1864.0, "end": 1868.0, "text": " Perpendicular or orthonormal to V, so you're kind of taking out all part.", "tokens": [3026, 18608, 14646, 420, 420, 11943, 24440, 281, 691, 11, 370, 291, 434, 733, 295, 1940, 484, 439, 644, 13], "temperature": 0.0, "avg_logprob": -0.13586984197777438, "compression_ratio": 1.6040609137055837, "no_speech_prob": 1.994685135287e-06}, {"id": 210, "seek": 186400, "start": 1868.0, "end": 1882.0, "text": " Like any part of you that can be represented by these being taken out, that's W, and then you're just left with this like orthonormal part in this dotted line would be you minus W.", "tokens": [1743, 604, 644, 295, 291, 300, 393, 312, 10379, 538, 613, 885, 2726, 484, 11, 300, 311, 343, 11, 293, 550, 291, 434, 445, 1411, 365, 341, 411, 420, 11943, 24440, 644, 294, 341, 37459, 1622, 576, 312, 291, 3175, 343, 13], "temperature": 0.0, "avg_logprob": -0.13586984197777438, "compression_ratio": 1.6040609137055837, "no_speech_prob": 1.994685135287e-06}, {"id": 211, "seek": 186400, "start": 1882.0, "end": 1889.0, "text": " Questions just about the kind of like idea of you're getting.", "tokens": [27738, 445, 466, 264, 733, 295, 411, 1558, 295, 291, 434, 1242, 13], "temperature": 0.0, "avg_logprob": -0.13586984197777438, "compression_ratio": 1.6040609137055837, "no_speech_prob": 1.994685135287e-06}, {"id": 212, "seek": 188900, "start": 1889.0, "end": 1910.0, "text": " Getting the part of you that lies along the and then what's left over has to be perpendicular to the.", "tokens": [13674, 264, 644, 295, 291, 300, 9134, 2051, 264, 293, 550, 437, 311, 1411, 670, 575, 281, 312, 26734, 281, 264, 13], "temperature": 0.0, "avg_logprob": -0.1319603125254313, "compression_ratio": 1.141304347826087, "no_speech_prob": 5.954868356639054e-06}, {"id": 213, "seek": 188900, "start": 1910.0, "end": 1912.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1319603125254313, "compression_ratio": 1.141304347826087, "no_speech_prob": 5.954868356639054e-06}, {"id": 214, "seek": 191200, "start": 1912.0, "end": 1923.0, "text": " So you can also do this with a plane and I took this from a blog post that I thought was pretty good that you might like called the linear algebra view of least squares regression.", "tokens": [407, 291, 393, 611, 360, 341, 365, 257, 5720, 293, 286, 1890, 341, 490, 257, 6968, 2183, 300, 286, 1194, 390, 1238, 665, 300, 291, 1062, 411, 1219, 264, 8213, 21989, 1910, 295, 1935, 19368, 24590, 13], "temperature": 0.0, "avg_logprob": -0.09728265489850725, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.7602592379262205e-06}, {"id": 215, "seek": 191200, "start": 1923.0, "end": 1926.0, "text": " So I just show you that that's out there.", "tokens": [407, 286, 445, 855, 291, 300, 300, 311, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.09728265489850725, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.7602592379262205e-06}, {"id": 216, "seek": 191200, "start": 1926.0, "end": 1933.0, "text": " Which we've kind of covered somewhat when we covered least squares regression.", "tokens": [3013, 321, 600, 733, 295, 5343, 8344, 562, 321, 5343, 1935, 19368, 24590, 13], "temperature": 0.0, "avg_logprob": -0.09728265489850725, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.7602592379262205e-06}, {"id": 217, "seek": 193300, "start": 1933.0, "end": 1942.0, "text": " So here we've got a plane A and really this plane is like the column space of the matrix A.", "tokens": [407, 510, 321, 600, 658, 257, 5720, 316, 293, 534, 341, 5720, 307, 411, 264, 7738, 1901, 295, 264, 8141, 316, 13], "temperature": 0.0, "avg_logprob": -0.1433584082360361, "compression_ratio": 1.4198473282442747, "no_speech_prob": 2.057992332993308e-06}, {"id": 218, "seek": 193300, "start": 1942.0, "end": 1949.0, "text": " And a vector B and now we're trying to find you know what's the vector that lies on the plane.", "tokens": [400, 257, 8062, 363, 293, 586, 321, 434, 1382, 281, 915, 291, 458, 437, 311, 264, 8062, 300, 9134, 322, 264, 5720, 13], "temperature": 0.0, "avg_logprob": -0.1433584082360361, "compression_ratio": 1.4198473282442747, "no_speech_prob": 2.057992332993308e-06}, {"id": 219, "seek": 194900, "start": 1949.0, "end": 1968.0, "text": " That kind of captures you know every part of B that's parallel to the plane so we get that and then we've got this B minus P and that's perpendicular to the plane.", "tokens": [663, 733, 295, 27986, 291, 458, 633, 644, 295, 363, 300, 311, 8952, 281, 264, 5720, 370, 321, 483, 300, 293, 550, 321, 600, 658, 341, 363, 3175, 430, 293, 300, 311, 26734, 281, 264, 5720, 13], "temperature": 0.0, "avg_logprob": -0.09099286940039658, "compression_ratio": 1.393162393162393, "no_speech_prob": 9.818031685426831e-06}, {"id": 220, "seek": 196800, "start": 1968.0, "end": 1982.0, "text": " So we can get the formula for projection coming from the idea that OK this dotted line must be perpendicular to the projection.", "tokens": [407, 321, 393, 483, 264, 8513, 337, 22743, 1348, 490, 264, 1558, 300, 2264, 341, 37459, 1622, 1633, 312, 26734, 281, 264, 22743, 13], "temperature": 0.0, "avg_logprob": -0.11984953710011073, "compression_ratio": 1.3987730061349692, "no_speech_prob": 2.482390982549987e-06}, {"id": 221, "seek": 196800, "start": 1982.0, "end": 1989.0, "text": " And so here and I apologize the different variable names B minus.", "tokens": [400, 370, 510, 293, 286, 12328, 264, 819, 7006, 5288, 363, 3175, 13], "temperature": 0.0, "avg_logprob": -0.11984953710011073, "compression_ratio": 1.3987730061349692, "no_speech_prob": 2.482390982549987e-06}, {"id": 222, "seek": 196800, "start": 1989.0, "end": 1994.0, "text": " XA dot with A is going to be zero.", "tokens": [1783, 32, 5893, 365, 316, 307, 516, 281, 312, 4018, 13], "temperature": 0.0, "avg_logprob": -0.11984953710011073, "compression_ratio": 1.3987730061349692, "no_speech_prob": 2.482390982549987e-06}, {"id": 223, "seek": 199400, "start": 1994.0, "end": 2006.0, "text": " So in this case XA is the projection of B onto a line A and X hat is kind of the scalar that you're multiplying by.", "tokens": [407, 294, 341, 1389, 1783, 32, 307, 264, 22743, 295, 363, 3911, 257, 1622, 316, 293, 1783, 2385, 307, 733, 295, 264, 39684, 300, 291, 434, 30955, 538, 13], "temperature": 0.0, "avg_logprob": -0.11039814267839704, "compression_ratio": 1.5027322404371584, "no_speech_prob": 2.9022635317232925e-06}, {"id": 224, "seek": 199400, "start": 2006.0, "end": 2014.0, "text": " And those are dotted with each other because that's that's what's true of perpendicular is just kind of like another name for orthonormal.", "tokens": [400, 729, 366, 37459, 365, 1184, 661, 570, 300, 311, 300, 311, 437, 311, 2074, 295, 26734, 307, 445, 733, 295, 411, 1071, 1315, 337, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.11039814267839704, "compression_ratio": 1.5027322404371584, "no_speech_prob": 2.9022635317232925e-06}, {"id": 225, "seek": 199400, "start": 2014.0, "end": 2017.0, "text": " Or sorry orthogonal.", "tokens": [1610, 2597, 41488, 13], "temperature": 0.0, "avg_logprob": -0.11039814267839704, "compression_ratio": 1.5027322404371584, "no_speech_prob": 2.9022635317232925e-06}, {"id": 226, "seek": 201700, "start": 2017.0, "end": 2025.0, "text": " They're not necessarily going to have length one but they're orthogonal to each other.", "tokens": [814, 434, 406, 4725, 516, 281, 362, 4641, 472, 457, 436, 434, 41488, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.07751449832209835, "compression_ratio": 1.4206896551724137, "no_speech_prob": 1.3081578345008893e-06}, {"id": 227, "seek": 201700, "start": 2025.0, "end": 2036.0, "text": " And so then to find what the scalar is you can do A dot B divided by A dot A.", "tokens": [400, 370, 550, 281, 915, 437, 264, 39684, 307, 291, 393, 360, 316, 5893, 363, 6666, 538, 316, 5893, 316, 13], "temperature": 0.0, "avg_logprob": -0.07751449832209835, "compression_ratio": 1.4206896551724137, "no_speech_prob": 1.3081578345008893e-06}, {"id": 228, "seek": 201700, "start": 2036.0, "end": 2042.0, "text": " Questions on projection or orthogonality.", "tokens": [27738, 322, 22743, 420, 38130, 266, 1860, 13], "temperature": 0.0, "avg_logprob": -0.07751449832209835, "compression_ratio": 1.4206896551724137, "no_speech_prob": 1.3081578345008893e-06}, {"id": 229, "seek": 204200, "start": 2042.0, "end": 2058.0, "text": " And Trevathan I think has a whole section on projectors in the kind of in the first part of the book since it is a concept that shows up a lot in numerical linear algebra.", "tokens": [400, 8648, 85, 9390, 286, 519, 575, 257, 1379, 3541, 322, 1716, 830, 294, 264, 733, 295, 294, 264, 700, 644, 295, 264, 1446, 1670, 309, 307, 257, 3410, 300, 3110, 493, 257, 688, 294, 29054, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.09875387385271597, "compression_ratio": 1.4432432432432432, "no_speech_prob": 2.2958511181059293e-06}, {"id": 230, "seek": 204200, "start": 2058.0, "end": 2067.0, "text": " OK so then we'll get to so it's going to turn out projection is really useful for Gram-Schmidt.", "tokens": [2264, 370, 550, 321, 603, 483, 281, 370, 309, 311, 516, 281, 1261, 484, 22743, 307, 534, 4420, 337, 22130, 12, 31560, 39000, 13], "temperature": 0.0, "avg_logprob": -0.09875387385271597, "compression_ratio": 1.4432432432432432, "no_speech_prob": 2.2958511181059293e-06}, {"id": 231, "seek": 206700, "start": 2067.0, "end": 2080.0, "text": " So classical Gram-Schmidt the idea is to remember our goal is we're trying to come up with a QR factorization and in Gram-Schmidt we're going to come up with one column of Q at a time.", "tokens": [407, 13735, 22130, 12, 31560, 39000, 264, 1558, 307, 281, 1604, 527, 3387, 307, 321, 434, 1382, 281, 808, 493, 365, 257, 32784, 5952, 2144, 293, 294, 22130, 12, 31560, 39000, 321, 434, 516, 281, 808, 493, 365, 472, 7738, 295, 1249, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.10025165450405067, "compression_ratio": 1.6091954022988506, "no_speech_prob": 6.438679974962724e-06}, {"id": 232, "seek": 206700, "start": 2080.0, "end": 2088.0, "text": " And the idea is kind of iterate through and each time we want to calculate a single projection.", "tokens": [400, 264, 1558, 307, 733, 295, 44497, 807, 293, 1184, 565, 321, 528, 281, 8873, 257, 2167, 22743, 13], "temperature": 0.0, "avg_logprob": -0.10025165450405067, "compression_ratio": 1.6091954022988506, "no_speech_prob": 6.438679974962724e-06}, {"id": 233, "seek": 208800, "start": 2088.0, "end": 2102.0, "text": " So projection PJ AJ where you're kind of projecting onto the space orthogonal to the span of your previous cues.", "tokens": [407, 22743, 30549, 32759, 689, 291, 434, 733, 295, 43001, 3911, 264, 1901, 41488, 281, 264, 16174, 295, 428, 3894, 32192, 13], "temperature": 0.0, "avg_logprob": -0.17104671551631048, "compression_ratio": 1.1914893617021276, "no_speech_prob": 3.237695636926219e-06}, {"id": 234, "seek": 210200, "start": 2102.0, "end": 2119.0, "text": " So what this looks like code wise is so we're going to just pick off the J column of the and then.", "tokens": [407, 437, 341, 1542, 411, 3089, 10829, 307, 370, 321, 434, 516, 281, 445, 1888, 766, 264, 508, 7738, 295, 264, 293, 550, 13], "temperature": 0.0, "avg_logprob": -0.16726909743414986, "compression_ratio": 1.2450980392156863, "no_speech_prob": 4.289113803679356e-06}, {"id": 235, "seek": 210200, "start": 2119.0, "end": 2126.0, "text": " Well the first time through.", "tokens": [1042, 264, 700, 565, 807, 13], "temperature": 0.0, "avg_logprob": -0.16726909743414986, "compression_ratio": 1.2450980392156863, "no_speech_prob": 4.289113803679356e-06}, {"id": 236, "seek": 212600, "start": 2126.0, "end": 2133.0, "text": " I guess we still so we need to project we need to project.", "tokens": [286, 2041, 321, 920, 370, 321, 643, 281, 1716, 321, 643, 281, 1716, 13], "temperature": 0.0, "avg_logprob": -0.1326385243733724, "compression_ratio": 1.6647727272727273, "no_speech_prob": 6.438700893340865e-06}, {"id": 237, "seek": 212600, "start": 2133.0, "end": 2143.0, "text": " A onto our already existing cues in the initial case we don't have any so you can kind of just like take the first column of Q and have it normalized.", "tokens": [316, 3911, 527, 1217, 6741, 32192, 294, 264, 5883, 1389, 321, 500, 380, 362, 604, 370, 291, 393, 733, 295, 445, 411, 747, 264, 700, 7738, 295, 1249, 293, 362, 309, 48704, 13], "temperature": 0.0, "avg_logprob": -0.1326385243733724, "compression_ratio": 1.6647727272727273, "no_speech_prob": 6.438700893340865e-06}, {"id": 238, "seek": 212600, "start": 2143.0, "end": 2147.0, "text": " Sorry first column of A normalize it and that's going to be your first column of Q.", "tokens": [4919, 700, 7738, 295, 316, 2710, 1125, 309, 293, 300, 311, 516, 281, 312, 428, 700, 7738, 295, 1249, 13], "temperature": 0.0, "avg_logprob": -0.1326385243733724, "compression_ratio": 1.6647727272727273, "no_speech_prob": 6.438700893340865e-06}, {"id": 239, "seek": 214700, "start": 2147.0, "end": 2165.0, "text": " And then to get the second column of Q you want to kind of take the second column of a projected onto the first column of Q subtract that off and then that'll be your second column of Q once you normalize.", "tokens": [400, 550, 281, 483, 264, 1150, 7738, 295, 1249, 291, 528, 281, 733, 295, 747, 264, 1150, 7738, 295, 257, 26231, 3911, 264, 700, 7738, 295, 1249, 16390, 300, 766, 293, 550, 300, 603, 312, 428, 1150, 7738, 295, 1249, 1564, 291, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.09346717916509156, "compression_ratio": 1.9190476190476191, "no_speech_prob": 3.187468337273458e-06}, {"id": 240, "seek": 214700, "start": 2165.0, "end": 2176.0, "text": " And you go through and so each time you're kind of taking a column of a and you want to represent it but you need to kind of subtract off everything that's already been accounted for in your queue.", "tokens": [400, 291, 352, 807, 293, 370, 1184, 565, 291, 434, 733, 295, 1940, 257, 7738, 295, 257, 293, 291, 528, 281, 2906, 309, 457, 291, 643, 281, 733, 295, 16390, 766, 1203, 300, 311, 1217, 668, 43138, 337, 294, 428, 18639, 13], "temperature": 0.0, "avg_logprob": -0.09346717916509156, "compression_ratio": 1.9190476190476191, "no_speech_prob": 3.187468337273458e-06}, {"id": 241, "seek": 217600, "start": 2176.0, "end": 2180.0, "text": " And so as you do this you're creating.", "tokens": [400, 370, 382, 291, 360, 341, 291, 434, 4084, 13], "temperature": 0.0, "avg_logprob": -0.13078091541926065, "compression_ratio": 1.3983050847457628, "no_speech_prob": 5.014605449105147e-06}, {"id": 242, "seek": 217600, "start": 2180.0, "end": 2192.0, "text": " You know these are orthonormal columns of Q that are going to represent your columns of a.", "tokens": [509, 458, 613, 366, 420, 11943, 24440, 13766, 295, 1249, 300, 366, 516, 281, 2906, 428, 13766, 295, 257, 13], "temperature": 0.0, "avg_logprob": -0.13078091541926065, "compression_ratio": 1.3983050847457628, "no_speech_prob": 5.014605449105147e-06}, {"id": 243, "seek": 217600, "start": 2192.0, "end": 2202.0, "text": " And so we can we can test this out.", "tokens": [400, 370, 321, 393, 321, 393, 1500, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.13078091541926065, "compression_ratio": 1.3983050847457628, "no_speech_prob": 5.014605449105147e-06}, {"id": 244, "seek": 220200, "start": 2202.0, "end": 2210.0, "text": " So we want to confirm that this algorithm so to make sure that you have like a valid algorithm that's actually finding your QR factorization.", "tokens": [407, 321, 528, 281, 9064, 300, 341, 9284, 370, 281, 652, 988, 300, 291, 362, 411, 257, 7363, 9284, 300, 311, 767, 5006, 428, 32784, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.09581499335206585, "compression_ratio": 1.695, "no_speech_prob": 7.766699127387255e-06}, {"id": 245, "seek": 220200, "start": 2210.0, "end": 2214.0, "text": " You want to check that Q times are gives you your original matrix back.", "tokens": [509, 528, 281, 1520, 300, 1249, 1413, 366, 2709, 291, 428, 3380, 8141, 646, 13], "temperature": 0.0, "avg_logprob": -0.09581499335206585, "compression_ratio": 1.695, "no_speech_prob": 7.766699127387255e-06}, {"id": 246, "seek": 220200, "start": 2214.0, "end": 2217.0, "text": " That's an important part of being a factorization.", "tokens": [663, 311, 364, 1021, 644, 295, 885, 257, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.09581499335206585, "compression_ratio": 1.695, "no_speech_prob": 7.766699127387255e-06}, {"id": 247, "seek": 220200, "start": 2217.0, "end": 2224.0, "text": " We want to check that Q is actually orthonormal which is what we expected.", "tokens": [492, 528, 281, 1520, 300, 1249, 307, 767, 420, 11943, 24440, 597, 307, 437, 321, 5176, 13], "temperature": 0.0, "avg_logprob": -0.09581499335206585, "compression_ratio": 1.695, "no_speech_prob": 7.766699127387255e-06}, {"id": 248, "seek": 222400, "start": 2224.0, "end": 2232.0, "text": " And I probably should have checked that R is.", "tokens": [400, 286, 1391, 820, 362, 10033, 300, 497, 307, 13], "temperature": 0.0, "avg_logprob": -0.20695141156514485, "compression_ratio": 1.4303797468354431, "no_speech_prob": 2.521544956834987e-06}, {"id": 249, "seek": 222400, "start": 2232.0, "end": 2236.0, "text": " Is actually triangular and it is.", "tokens": [1119, 767, 38190, 293, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.20695141156514485, "compression_ratio": 1.4303797468354431, "no_speech_prob": 2.521544956834987e-06}, {"id": 250, "seek": 222400, "start": 2236.0, "end": 2245.0, "text": " In this case maybe a is not missing a might not be square but yeah that you're getting something.", "tokens": [682, 341, 1389, 1310, 257, 307, 406, 5361, 257, 1062, 406, 312, 3732, 457, 1338, 300, 291, 434, 1242, 746, 13], "temperature": 0.0, "avg_logprob": -0.20695141156514485, "compression_ratio": 1.4303797468354431, "no_speech_prob": 2.521544956834987e-06}, {"id": 251, "seek": 222400, "start": 2245.0, "end": 2250.0, "text": " Triangular back so that's that's Graham Schmidt.", "tokens": [10931, 656, 1040, 646, 370, 300, 311, 300, 311, 22691, 42621, 13], "temperature": 0.0, "avg_logprob": -0.20695141156514485, "compression_ratio": 1.4303797468354431, "no_speech_prob": 2.521544956834987e-06}, {"id": 252, "seek": 225000, "start": 2250.0, "end": 2258.0, "text": " Questions about classic Graham Schmidt.", "tokens": [27738, 466, 7230, 22691, 42621, 13], "temperature": 0.0, "avg_logprob": -0.33389993147416547, "compression_ratio": 1.1538461538461537, "no_speech_prob": 8.139351848512888e-06}, {"id": 253, "seek": 225000, "start": 2258.0, "end": 2264.0, "text": " Kelsey and can you pass the microphone.", "tokens": [44714, 293, 393, 291, 1320, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.33389993147416547, "compression_ratio": 1.1538461538461537, "no_speech_prob": 8.139351848512888e-06}, {"id": 254, "seek": 225000, "start": 2264.0, "end": 2267.0, "text": " She must be like right now I'm having.", "tokens": [1240, 1633, 312, 411, 558, 586, 286, 478, 1419, 13], "temperature": 0.0, "avg_logprob": -0.33389993147416547, "compression_ratio": 1.1538461538461537, "no_speech_prob": 8.139351848512888e-06}, {"id": 255, "seek": 225000, "start": 2267.0, "end": 2270.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.33389993147416547, "compression_ratio": 1.1538461538461537, "no_speech_prob": 8.139351848512888e-06}, {"id": 256, "seek": 225000, "start": 2270.0, "end": 2273.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.33389993147416547, "compression_ratio": 1.1538461538461537, "no_speech_prob": 8.139351848512888e-06}, {"id": 257, "seek": 225000, "start": 2273.0, "end": 2278.0, "text": " See.", "tokens": [3008, 13], "temperature": 0.0, "avg_logprob": -0.33389993147416547, "compression_ratio": 1.1538461538461537, "no_speech_prob": 8.139351848512888e-06}, {"id": 258, "seek": 227800, "start": 2278.0, "end": 2285.0, "text": " This will work.", "tokens": [639, 486, 589, 13], "temperature": 0.0, "avg_logprob": -0.30075448751449585, "compression_ratio": 0.6521739130434783, "no_speech_prob": 0.0003513034316711128}, {"id": 259, "seek": 228500, "start": 2285.0, "end": 2310.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2099818785985311, "compression_ratio": 0.38461538461538464, "no_speech_prob": 3.8824855437269434e-05}, {"id": 260, "seek": 231000, "start": 2310.0, "end": 2315.0, "text": " I'm going to see if I can put maybe put them side by side so you can kind of.", "tokens": [286, 478, 516, 281, 536, 498, 286, 393, 829, 1310, 829, 552, 1252, 538, 1252, 370, 291, 393, 733, 295, 13], "temperature": 0.0, "avg_logprob": -0.2417462911361303, "compression_ratio": 1.1595744680851063, "no_speech_prob": 3.3403862289560493e-06}, {"id": 261, "seek": 231000, "start": 2315.0, "end": 2324.0, "text": " Whoops.", "tokens": [45263, 13], "temperature": 0.0, "avg_logprob": -0.2417462911361303, "compression_ratio": 1.1595744680851063, "no_speech_prob": 3.3403862289560493e-06}, {"id": 262, "seek": 231000, "start": 2324.0, "end": 2327.0, "text": " See the algorithm.", "tokens": [3008, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.2417462911361303, "compression_ratio": 1.1595744680851063, "no_speech_prob": 3.3403862289560493e-06}, {"id": 263, "seek": 231000, "start": 2327.0, "end": 2330.0, "text": " And.", "tokens": [400, 13], "temperature": 0.0, "avg_logprob": -0.2417462911361303, "compression_ratio": 1.1595744680851063, "no_speech_prob": 3.3403862289560493e-06}, {"id": 264, "seek": 233000, "start": 2330.0, "end": 2353.0, "text": " And since pseudo code just see if Travis writes this out.", "tokens": [400, 1670, 35899, 3089, 445, 536, 498, 24430, 13657, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.512255072593689, "compression_ratio": 0.9047619047619048, "no_speech_prob": 3.2884736356209032e-06}, {"id": 265, "seek": 235300, "start": 2353.0, "end": 2368.0, "text": " Okay so one thing that Trevinson has that might be helpful is what you're coming up with.", "tokens": [1033, 370, 472, 551, 300, 8648, 85, 14321, 575, 300, 1062, 312, 4961, 307, 437, 291, 434, 1348, 493, 365, 13], "temperature": 0.0, "avg_logprob": -0.31357872009277343, "compression_ratio": 1.0595238095238095, "no_speech_prob": 3.0714425520272925e-05}, {"id": 266, "seek": 236800, "start": 2368.0, "end": 2385.0, "text": " So here a one is going to be the first column of a.", "tokens": [407, 510, 257, 472, 307, 516, 281, 312, 264, 700, 7738, 295, 257, 13], "temperature": 0.0, "avg_logprob": -0.11585176908052884, "compression_ratio": 1.3937007874015748, "no_speech_prob": 1.6186380889848806e-05}, {"id": 267, "seek": 236800, "start": 2385.0, "end": 2388.0, "text": " And basically that's just going to be a multiple of Q one.", "tokens": [400, 1936, 300, 311, 445, 516, 281, 312, 257, 3866, 295, 1249, 472, 13], "temperature": 0.0, "avg_logprob": -0.11585176908052884, "compression_ratio": 1.3937007874015748, "no_speech_prob": 1.6186380889848806e-05}, {"id": 268, "seek": 236800, "start": 2388.0, "end": 2393.0, "text": " So we're trying to build an orthonormal basis starting off with a.", "tokens": [407, 321, 434, 1382, 281, 1322, 364, 420, 11943, 24440, 5143, 2891, 766, 365, 257, 13], "temperature": 0.0, "avg_logprob": -0.11585176908052884, "compression_ratio": 1.3937007874015748, "no_speech_prob": 1.6186380889848806e-05}, {"id": 269, "seek": 239300, "start": 2393.0, "end": 2399.0, "text": " And actually let me write and look myself space.", "tokens": [400, 767, 718, 385, 2464, 293, 574, 2059, 1901, 13], "temperature": 0.0, "avg_logprob": -0.1503837933907142, "compression_ratio": 1.3381294964028776, "no_speech_prob": 1.1300399819447193e-05}, {"id": 270, "seek": 239300, "start": 2399.0, "end": 2404.0, "text": " This over to the side for a moment.", "tokens": [639, 670, 281, 264, 1252, 337, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1503837933907142, "compression_ratio": 1.3381294964028776, "no_speech_prob": 1.1300399819447193e-05}, {"id": 271, "seek": 239300, "start": 2404.0, "end": 2415.0, "text": " So you can think of we're kind of starting with we've got these columns a one a two up through a and.", "tokens": [407, 291, 393, 519, 295, 321, 434, 733, 295, 2891, 365, 321, 600, 658, 613, 13766, 257, 472, 257, 732, 493, 807, 257, 293, 13], "temperature": 0.0, "avg_logprob": -0.1503837933907142, "compression_ratio": 1.3381294964028776, "no_speech_prob": 1.1300399819447193e-05}, {"id": 272, "seek": 241500, "start": 2415.0, "end": 2434.0, "text": " We want to end up with this being we're kind of trying to find you know what what can we choose for these columns Q one up to Q Q and.", "tokens": [492, 528, 281, 917, 493, 365, 341, 885, 321, 434, 733, 295, 1382, 281, 915, 291, 458, 437, 437, 393, 321, 2826, 337, 613, 13766, 1249, 472, 493, 281, 1249, 1249, 293, 13], "temperature": 0.0, "avg_logprob": -0.14485832623073033, "compression_ratio": 1.2608695652173914, "no_speech_prob": 1.5936124327708967e-05}, {"id": 273, "seek": 241500, "start": 2434.0, "end": 2440.0, "text": " Such that.", "tokens": [9653, 300, 13], "temperature": 0.0, "avg_logprob": -0.14485832623073033, "compression_ratio": 1.2608695652173914, "no_speech_prob": 1.5936124327708967e-05}, {"id": 274, "seek": 244000, "start": 2440.0, "end": 2446.0, "text": " Got this triangular matrix are.", "tokens": [5803, 341, 38190, 8141, 366, 13], "temperature": 0.0, "avg_logprob": -0.17047178590452516, "compression_ratio": 1.581151832460733, "no_speech_prob": 3.61185334440961e-06}, {"id": 275, "seek": 244000, "start": 2446.0, "end": 2456.0, "text": " So all zeros and so we had to think about that is that you're getting a set of equations actually maybe I put this full screen just for a little bit and then I'll go back.", "tokens": [407, 439, 35193, 293, 370, 321, 632, 281, 519, 466, 300, 307, 300, 291, 434, 1242, 257, 992, 295, 11787, 767, 1310, 286, 829, 341, 1577, 2568, 445, 337, 257, 707, 857, 293, 550, 286, 603, 352, 646, 13], "temperature": 0.0, "avg_logprob": -0.17047178590452516, "compression_ratio": 1.581151832460733, "no_speech_prob": 3.61185334440961e-06}, {"id": 276, "seek": 244000, "start": 2456.0, "end": 2464.0, "text": " And getting the set of equations a one one or sorry a one is going to be just a multiple of Q one.", "tokens": [400, 1242, 264, 992, 295, 11787, 257, 472, 472, 420, 2597, 257, 472, 307, 516, 281, 312, 445, 257, 3866, 295, 1249, 472, 13], "temperature": 0.0, "avg_logprob": -0.17047178590452516, "compression_ratio": 1.581151832460733, "no_speech_prob": 3.61185334440961e-06}, {"id": 277, "seek": 246400, "start": 2464.0, "end": 2470.0, "text": " And we're going to store that in the.", "tokens": [400, 321, 434, 516, 281, 3531, 300, 294, 264, 13], "temperature": 0.0, "avg_logprob": -0.14267827189245888, "compression_ratio": 1.3454545454545455, "no_speech_prob": 1.1124711818411015e-05}, {"id": 278, "seek": 246400, "start": 2470.0, "end": 2492.0, "text": " Triangular matrix are so how would so we have a how would we choose what what are one one and Q one should be.", "tokens": [10931, 656, 1040, 8141, 366, 370, 577, 576, 370, 321, 362, 257, 577, 576, 321, 2826, 437, 437, 366, 472, 472, 293, 1249, 472, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.14267827189245888, "compression_ratio": 1.3454545454545455, "no_speech_prob": 1.1124711818411015e-05}, {"id": 279, "seek": 249200, "start": 2492.0, "end": 2498.0, "text": " The ideas Matthew you pass the microphone down.", "tokens": [440, 3487, 12434, 291, 1320, 264, 10952, 760, 13], "temperature": 0.0, "avg_logprob": -0.37348438444591703, "compression_ratio": 1.4028776978417266, "no_speech_prob": 1.321088620898081e-05}, {"id": 280, "seek": 249200, "start": 2498.0, "end": 2500.0, "text": " He wanted.", "tokens": [634, 1415, 13], "temperature": 0.0, "avg_logprob": -0.37348438444591703, "compression_ratio": 1.4028776978417266, "no_speech_prob": 1.321088620898081e-05}, {"id": 281, "seek": 249200, "start": 2500.0, "end": 2505.0, "text": " He wanted an hour one one.", "tokens": [634, 1415, 364, 1773, 472, 472, 13], "temperature": 0.0, "avg_logprob": -0.37348438444591703, "compression_ratio": 1.4028776978417266, "no_speech_prob": 1.321088620898081e-05}, {"id": 282, "seek": 249200, "start": 2505.0, "end": 2511.0, "text": " So we don't we don't have the Q's or the ours yet we're trying to find them.", "tokens": [407, 321, 500, 380, 321, 500, 380, 362, 264, 1249, 311, 420, 264, 11896, 1939, 321, 434, 1382, 281, 915, 552, 13], "temperature": 0.0, "avg_logprob": -0.37348438444591703, "compression_ratio": 1.4028776978417266, "no_speech_prob": 1.321088620898081e-05}, {"id": 283, "seek": 249200, "start": 2511.0, "end": 2513.0, "text": " But basically.", "tokens": [583, 1936, 13], "temperature": 0.0, "avg_logprob": -0.37348438444591703, "compression_ratio": 1.4028776978417266, "no_speech_prob": 1.321088620898081e-05}, {"id": 284, "seek": 249200, "start": 2513.0, "end": 2518.0, "text": " Yeah you want to.", "tokens": [865, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.37348438444591703, "compression_ratio": 1.4028776978417266, "no_speech_prob": 1.321088620898081e-05}, {"id": 285, "seek": 251800, "start": 2518.0, "end": 2525.0, "text": " So that's that equation would hold but what other property do we want Q to have in addition.", "tokens": [407, 300, 311, 300, 5367, 576, 1797, 457, 437, 661, 4707, 360, 321, 528, 1249, 281, 362, 294, 4500, 13], "temperature": 0.0, "avg_logprob": -0.21909489430172344, "compression_ratio": 1.4764705882352942, "no_speech_prob": 2.014318852161523e-05}, {"id": 286, "seek": 251800, "start": 2525.0, "end": 2530.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.21909489430172344, "compression_ratio": 1.4764705882352942, "no_speech_prob": 2.014318852161523e-05}, {"id": 287, "seek": 251800, "start": 2530.0, "end": 2532.0, "text": " Well so.", "tokens": [1042, 370, 13], "temperature": 0.0, "avg_logprob": -0.21909489430172344, "compression_ratio": 1.4764705882352942, "no_speech_prob": 2.014318852161523e-05}, {"id": 288, "seek": 251800, "start": 2532.0, "end": 2537.0, "text": " Oh you're very close but yeah Tim want to chime in.", "tokens": [876, 291, 434, 588, 1998, 457, 1338, 7172, 528, 281, 40921, 294, 13], "temperature": 0.0, "avg_logprob": -0.21909489430172344, "compression_ratio": 1.4764705882352942, "no_speech_prob": 2.014318852161523e-05}, {"id": 289, "seek": 251800, "start": 2537.0, "end": 2543.0, "text": " So yeah Matthew suggestion was kind of choose Q one to be equal to a one which is close Tim.", "tokens": [407, 1338, 12434, 16541, 390, 733, 295, 2826, 1249, 472, 281, 312, 2681, 281, 257, 472, 597, 307, 1998, 7172, 13], "temperature": 0.0, "avg_logprob": -0.21909489430172344, "compression_ratio": 1.4764705882352942, "no_speech_prob": 2.014318852161523e-05}, {"id": 290, "seek": 254300, "start": 2543.0, "end": 2548.0, "text": " You just said our one one to be the norm of a one X Q one to be the.", "tokens": [509, 445, 848, 527, 472, 472, 281, 312, 264, 2026, 295, 257, 472, 1783, 1249, 472, 281, 312, 264, 13], "temperature": 0.0, "avg_logprob": -0.25169234580182015, "compression_ratio": 1.8162162162162163, "no_speech_prob": 6.539961304952158e-06}, {"id": 291, "seek": 254300, "start": 2548.0, "end": 2551.0, "text": " Exactly yeah so you want to.", "tokens": [7587, 1338, 370, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.25169234580182015, "compression_ratio": 1.8162162162162163, "no_speech_prob": 6.539961304952158e-06}, {"id": 292, "seek": 254300, "start": 2551.0, "end": 2559.0, "text": " Sorry you would set our one one to be the norm of a one Q one is just a one divided by.", "tokens": [4919, 291, 576, 992, 527, 472, 472, 281, 312, 264, 2026, 295, 257, 472, 1249, 472, 307, 445, 257, 472, 6666, 538, 13], "temperature": 0.0, "avg_logprob": -0.25169234580182015, "compression_ratio": 1.8162162162162163, "no_speech_prob": 6.539961304952158e-06}, {"id": 293, "seek": 254300, "start": 2559.0, "end": 2568.0, "text": " Yeah so the idea is you need to normalize it because Q one has to have length one is kind of the additional property of an orthonormal set of vectors.", "tokens": [865, 370, 264, 1558, 307, 291, 643, 281, 2710, 1125, 309, 570, 1249, 472, 575, 281, 362, 4641, 472, 307, 733, 295, 264, 4497, 4707, 295, 364, 420, 11943, 24440, 992, 295, 18875, 13], "temperature": 0.0, "avg_logprob": -0.25169234580182015, "compression_ratio": 1.8162162162162163, "no_speech_prob": 6.539961304952158e-06}, {"id": 294, "seek": 256800, "start": 2568.0, "end": 2579.0, "text": " And so take a one normalize it that's Q one and then our one one should be the norm so that you get the right magnitude back.", "tokens": [400, 370, 747, 257, 472, 2710, 1125, 309, 300, 311, 1249, 472, 293, 550, 527, 472, 472, 820, 312, 264, 2026, 370, 300, 291, 483, 264, 558, 15668, 646, 13], "temperature": 0.0, "avg_logprob": -0.08274783028496636, "compression_ratio": 1.5549738219895288, "no_speech_prob": 5.368694928620243e-07}, {"id": 295, "seek": 256800, "start": 2579.0, "end": 2592.0, "text": " And kind of thinking about that in terms of vectors it's Q one is pointing in the exact same direction as a one but it's just been scaled appropriately to have length one.", "tokens": [400, 733, 295, 1953, 466, 300, 294, 2115, 295, 18875, 309, 311, 1249, 472, 307, 12166, 294, 264, 1900, 912, 3513, 382, 257, 472, 457, 309, 311, 445, 668, 36039, 23505, 281, 362, 4641, 472, 13], "temperature": 0.0, "avg_logprob": -0.08274783028496636, "compression_ratio": 1.5549738219895288, "no_speech_prob": 5.368694928620243e-07}, {"id": 296, "seek": 259200, "start": 2592.0, "end": 2598.0, "text": " And let me even maybe write that.", "tokens": [400, 718, 385, 754, 1310, 2464, 300, 13], "temperature": 0.0, "avg_logprob": -0.14709264840652694, "compression_ratio": 1.3557046979865772, "no_speech_prob": 4.029409865324851e-06}, {"id": 297, "seek": 259200, "start": 2598.0, "end": 2601.0, "text": " Like if this.", "tokens": [1743, 498, 341, 13], "temperature": 0.0, "avg_logprob": -0.14709264840652694, "compression_ratio": 1.3557046979865772, "no_speech_prob": 4.029409865324851e-06}, {"id": 298, "seek": 259200, "start": 2601.0, "end": 2606.0, "text": " This is a one remember a one's a column so it's a vector.", "tokens": [639, 307, 257, 472, 1604, 257, 472, 311, 257, 7738, 370, 309, 311, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.14709264840652694, "compression_ratio": 1.3557046979865772, "no_speech_prob": 4.029409865324851e-06}, {"id": 299, "seek": 259200, "start": 2606.0, "end": 2608.0, "text": " Then.", "tokens": [1396, 13], "temperature": 0.0, "avg_logprob": -0.14709264840652694, "compression_ratio": 1.3557046979865772, "no_speech_prob": 4.029409865324851e-06}, {"id": 300, "seek": 259200, "start": 2608.0, "end": 2615.0, "text": " Q one is going in the exact same direction except it just has length.", "tokens": [1249, 472, 307, 516, 294, 264, 1900, 912, 3513, 3993, 309, 445, 575, 4641, 13], "temperature": 0.0, "avg_logprob": -0.14709264840652694, "compression_ratio": 1.3557046979865772, "no_speech_prob": 4.029409865324851e-06}, {"id": 301, "seek": 259200, "start": 2615.0, "end": 2617.0, "text": " One.", "tokens": [1485, 13], "temperature": 0.0, "avg_logprob": -0.14709264840652694, "compression_ratio": 1.3557046979865772, "no_speech_prob": 4.029409865324851e-06}, {"id": 302, "seek": 259200, "start": 2617.0, "end": 2621.0, "text": " Very clear one.", "tokens": [4372, 1850, 472, 13], "temperature": 0.0, "avg_logprob": -0.14709264840652694, "compression_ratio": 1.3557046979865772, "no_speech_prob": 4.029409865324851e-06}, {"id": 303, "seek": 262100, "start": 2621.0, "end": 2629.0, "text": " And then the length of all of a one will set to be our one one.", "tokens": [400, 550, 264, 4641, 295, 439, 295, 257, 472, 486, 992, 281, 312, 527, 472, 472, 13], "temperature": 0.0, "avg_logprob": -0.1296969212983784, "compression_ratio": 1.4691358024691359, "no_speech_prob": 3.3404612622689456e-06}, {"id": 304, "seek": 262100, "start": 2629.0, "end": 2640.0, "text": " Or rather we set our one one to be the length of a one.", "tokens": [1610, 2831, 321, 992, 527, 472, 472, 281, 312, 264, 4641, 295, 257, 472, 13], "temperature": 0.0, "avg_logprob": -0.1296969212983784, "compression_ratio": 1.4691358024691359, "no_speech_prob": 3.3404612622689456e-06}, {"id": 305, "seek": 264000, "start": 2640.0, "end": 2651.0, "text": " OK so what's the what's the next equation I could write.", "tokens": [2264, 370, 437, 311, 264, 437, 311, 264, 958, 5367, 286, 727, 2464, 13], "temperature": 0.0, "avg_logprob": -0.24247336387634277, "compression_ratio": 1.4, "no_speech_prob": 1.1124959200969897e-05}, {"id": 306, "seek": 264000, "start": 2651.0, "end": 2653.0, "text": " Yes Matthew.", "tokens": [1079, 12434, 13], "temperature": 0.0, "avg_logprob": -0.24247336387634277, "compression_ratio": 1.4, "no_speech_prob": 1.1124959200969897e-05}, {"id": 307, "seek": 264000, "start": 2653.0, "end": 2664.0, "text": " A two equals our one two times a one Q one plus our two two times Q two.", "tokens": [316, 732, 6915, 527, 472, 732, 1413, 257, 472, 1249, 472, 1804, 527, 732, 732, 1413, 1249, 732, 13], "temperature": 0.0, "avg_logprob": -0.24247336387634277, "compression_ratio": 1.4, "no_speech_prob": 1.1124959200969897e-05}, {"id": 308, "seek": 264000, "start": 2664.0, "end": 2668.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.24247336387634277, "compression_ratio": 1.4, "no_speech_prob": 1.1124959200969897e-05}, {"id": 309, "seek": 266800, "start": 2668.0, "end": 2678.0, "text": " Yeah that's great. So our A two is going to equal our one two times Q one plus our two two times Q two.", "tokens": [865, 300, 311, 869, 13, 407, 527, 316, 732, 307, 516, 281, 2681, 527, 472, 732, 1413, 1249, 472, 1804, 527, 732, 732, 1413, 1249, 732, 13], "temperature": 0.0, "avg_logprob": -0.12028271273562782, "compression_ratio": 1.6457142857142857, "no_speech_prob": 1.1842704225273337e-05}, {"id": 310, "seek": 266800, "start": 2678.0, "end": 2684.0, "text": " And so that's just coming from.", "tokens": [400, 370, 300, 311, 445, 1348, 490, 13], "temperature": 0.0, "avg_logprob": -0.12028271273562782, "compression_ratio": 1.6457142857142857, "no_speech_prob": 1.1842704225273337e-05}, {"id": 311, "seek": 266800, "start": 2684.0, "end": 2695.0, "text": " We're kind of taking the second column of our scroll over it second column of our multiplying that by the matrix Q which just picks off Q one and Q two.", "tokens": [492, 434, 733, 295, 1940, 264, 1150, 7738, 295, 527, 11369, 670, 309, 1150, 7738, 295, 527, 30955, 300, 538, 264, 8141, 1249, 597, 445, 16137, 766, 1249, 472, 293, 1249, 732, 13], "temperature": 0.0, "avg_logprob": -0.12028271273562782, "compression_ratio": 1.6457142857142857, "no_speech_prob": 1.1842704225273337e-05}, {"id": 312, "seek": 269500, "start": 2695.0, "end": 2698.0, "text": " Remember everything below our two two is zero.", "tokens": [5459, 1203, 2507, 527, 732, 732, 307, 4018, 13], "temperature": 0.0, "avg_logprob": -0.16802290081977844, "compression_ratio": 1.4870129870129871, "no_speech_prob": 3.6119049582339358e-06}, {"id": 313, "seek": 269500, "start": 2698.0, "end": 2703.0, "text": " So zero coefficient for Q three Q four up to Q n.", "tokens": [407, 4018, 17619, 337, 1249, 1045, 1249, 1451, 493, 281, 1249, 297, 13], "temperature": 0.0, "avg_logprob": -0.16802290081977844, "compression_ratio": 1.4870129870129871, "no_speech_prob": 3.6119049582339358e-06}, {"id": 314, "seek": 269500, "start": 2703.0, "end": 2713.0, "text": " And so taking this linear combination of the columns of Q we get our one two times Q one plus our two two times Q two.", "tokens": [400, 370, 1940, 341, 8213, 6562, 295, 264, 13766, 295, 1249, 321, 483, 527, 472, 732, 1413, 1249, 472, 1804, 527, 732, 732, 1413, 1249, 732, 13], "temperature": 0.0, "avg_logprob": -0.16802290081977844, "compression_ratio": 1.4870129870129871, "no_speech_prob": 3.6119049582339358e-06}, {"id": 315, "seek": 269500, "start": 2713.0, "end": 2716.0, "text": " Equals a two.", "tokens": [15624, 1124, 257, 732, 13], "temperature": 0.0, "avg_logprob": -0.16802290081977844, "compression_ratio": 1.4870129870129871, "no_speech_prob": 3.6119049582339358e-06}, {"id": 316, "seek": 271600, "start": 2716.0, "end": 2725.0, "text": " And so at this point we know what a two is or no we do not know it. Oh yes we know a two because that's our original matrix a.", "tokens": [400, 370, 412, 341, 935, 321, 458, 437, 257, 732, 307, 420, 572, 321, 360, 406, 458, 309, 13, 876, 2086, 321, 458, 257, 732, 570, 300, 311, 527, 3380, 8141, 257, 13], "temperature": 0.0, "avg_logprob": -0.12049568130309324, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.422231528333214e-07}, {"id": 317, "seek": 271600, "start": 2725.0, "end": 2731.0, "text": " So we've got a two. We know what Q one is because we just solved for that in the previous step.", "tokens": [407, 321, 600, 658, 257, 732, 13, 492, 458, 437, 1249, 472, 307, 570, 321, 445, 13041, 337, 300, 294, 264, 3894, 1823, 13], "temperature": 0.0, "avg_logprob": -0.12049568130309324, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.422231528333214e-07}, {"id": 318, "seek": 271600, "start": 2731.0, "end": 2738.0, "text": " And so now we're trying to find Q two R two two and R one two.", "tokens": [400, 370, 586, 321, 434, 1382, 281, 915, 1249, 732, 497, 732, 732, 293, 497, 472, 732, 13], "temperature": 0.0, "avg_logprob": -0.12049568130309324, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.422231528333214e-07}, {"id": 319, "seek": 273800, "start": 2738.0, "end": 2746.0, "text": " And we have this constraint that we're going to want Q one and Q two to be orthogonal to each other.", "tokens": [400, 321, 362, 341, 25534, 300, 321, 434, 516, 281, 528, 1249, 472, 293, 1249, 732, 281, 312, 41488, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.11818374906267438, "compression_ratio": 1.6466666666666667, "no_speech_prob": 3.6687631563836476e-06}, {"id": 320, "seek": 273800, "start": 2746.0, "end": 2750.0, "text": " And Q two is going to have to have norm one.", "tokens": [400, 1249, 732, 307, 516, 281, 362, 281, 362, 2026, 472, 13], "temperature": 0.0, "avg_logprob": -0.11818374906267438, "compression_ratio": 1.6466666666666667, "no_speech_prob": 3.6687631563836476e-06}, {"id": 321, "seek": 273800, "start": 2750.0, "end": 2757.0, "text": " And so that gives us enough information that we can.", "tokens": [400, 370, 300, 2709, 505, 1547, 1589, 300, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.11818374906267438, "compression_ratio": 1.6466666666666667, "no_speech_prob": 3.6687631563836476e-06}, {"id": 322, "seek": 273800, "start": 2757.0, "end": 2764.0, "text": " What we can do is project a two on to the Q one.", "tokens": [708, 321, 393, 360, 307, 1716, 257, 732, 322, 281, 264, 1249, 472, 13], "temperature": 0.0, "avg_logprob": -0.11818374906267438, "compression_ratio": 1.6466666666666667, "no_speech_prob": 3.6687631563836476e-06}, {"id": 323, "seek": 276400, "start": 2764.0, "end": 2779.0, "text": " And we'll get a kind of R one two is the scalar we need given that kind of to get back to the full kind of component of a two.", "tokens": [400, 321, 603, 483, 257, 733, 295, 497, 472, 732, 307, 264, 39684, 321, 643, 2212, 300, 733, 295, 281, 483, 646, 281, 264, 1577, 733, 295, 6542, 295, 257, 732, 13], "temperature": 0.0, "avg_logprob": -0.09307438640271203, "compression_ratio": 1.55, "no_speech_prob": 1.1015771406164276e-06}, {"id": 324, "seek": 276400, "start": 2779.0, "end": 2788.0, "text": " And then we can find Q two is just kind of what's what's left over the perpendicular part.", "tokens": [400, 550, 321, 393, 915, 1249, 732, 307, 445, 733, 295, 437, 311, 437, 311, 1411, 670, 264, 26734, 644, 13], "temperature": 0.0, "avg_logprob": -0.09307438640271203, "compression_ratio": 1.55, "no_speech_prob": 1.1015771406164276e-06}, {"id": 325, "seek": 278800, "start": 2788.0, "end": 2794.0, "text": " Questions about that.", "tokens": [27738, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.3741697831587358, "compression_ratio": 1.4126984126984128, "no_speech_prob": 3.94355010939762e-05}, {"id": 326, "seek": 278800, "start": 2794.0, "end": 2796.0, "text": " Matthew.", "tokens": [12434, 13], "temperature": 0.0, "avg_logprob": -0.3741697831587358, "compression_ratio": 1.4126984126984128, "no_speech_prob": 3.94355010939762e-05}, {"id": 327, "seek": 278800, "start": 2796.0, "end": 2803.0, "text": " How do you because I know you want to Q two are orthogonal to each other.", "tokens": [1012, 360, 291, 570, 286, 458, 291, 528, 281, 1249, 732, 366, 41488, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.3741697831587358, "compression_ratio": 1.4126984126984128, "no_speech_prob": 3.94355010939762e-05}, {"id": 328, "seek": 278800, "start": 2803.0, "end": 2808.0, "text": " But there are a lot of orthogonal directions.", "tokens": [583, 456, 366, 257, 688, 295, 41488, 11095, 13], "temperature": 0.0, "avg_logprob": -0.3741697831587358, "compression_ratio": 1.4126984126984128, "no_speech_prob": 3.94355010939762e-05}, {"id": 329, "seek": 278800, "start": 2808.0, "end": 2810.0, "text": " Yes. Yeah.", "tokens": [1079, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.3741697831587358, "compression_ratio": 1.4126984126984128, "no_speech_prob": 3.94355010939762e-05}, {"id": 330, "seek": 278800, "start": 2810.0, "end": 2812.0, "text": " How do you pick.", "tokens": [1012, 360, 291, 1888, 13], "temperature": 0.0, "avg_logprob": -0.3741697831587358, "compression_ratio": 1.4126984126984128, "no_speech_prob": 3.94355010939762e-05}, {"id": 331, "seek": 281200, "start": 2812.0, "end": 2821.0, "text": " Yeah. So at that point what you're doing is you're going to have over here.", "tokens": [865, 13, 407, 412, 300, 935, 437, 291, 434, 884, 307, 291, 434, 516, 281, 362, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.15827023780952065, "compression_ratio": 1.472, "no_speech_prob": 5.954619155090768e-06}, {"id": 332, "seek": 281200, "start": 2821.0, "end": 2826.0, "text": " A two minus R one two.", "tokens": [316, 732, 3175, 497, 472, 732, 13], "temperature": 0.0, "avg_logprob": -0.15827023780952065, "compression_ratio": 1.472, "no_speech_prob": 5.954619155090768e-06}, {"id": 333, "seek": 281200, "start": 2826.0, "end": 2828.0, "text": " Q one.", "tokens": [1249, 472, 13], "temperature": 0.0, "avg_logprob": -0.15827023780952065, "compression_ratio": 1.472, "no_speech_prob": 5.954619155090768e-06}, {"id": 334, "seek": 281200, "start": 2828.0, "end": 2835.0, "text": " And so in a picture this A two minus R one two Q one would be the dotted line.", "tokens": [400, 370, 294, 257, 3036, 341, 316, 732, 3175, 497, 472, 732, 1249, 472, 576, 312, 264, 37459, 1622, 13], "temperature": 0.0, "avg_logprob": -0.15827023780952065, "compression_ratio": 1.472, "no_speech_prob": 5.954619155090768e-06}, {"id": 335, "seek": 283500, "start": 2835.0, "end": 2844.0, "text": " And actually let me draw that picture. So if this is.", "tokens": [400, 767, 718, 385, 2642, 300, 3036, 13, 407, 498, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.17612154917283493, "compression_ratio": 0.9230769230769231, "no_speech_prob": 5.014533599023707e-06}, {"id": 336, "seek": 283500, "start": 2844.0, "end": 2856.0, "text": " A two.", "tokens": [316, 732, 13], "temperature": 0.0, "avg_logprob": -0.17612154917283493, "compression_ratio": 0.9230769230769231, "no_speech_prob": 5.014533599023707e-06}, {"id": 337, "seek": 285600, "start": 2856.0, "end": 2867.0, "text": " This is Q one. Remember Q one's only got length one.", "tokens": [639, 307, 1249, 472, 13, 5459, 1249, 472, 311, 787, 658, 4641, 472, 13], "temperature": 0.0, "avg_logprob": -0.16311022814582377, "compression_ratio": 1.2061855670103092, "no_speech_prob": 2.6014154173026327e-06}, {"id": 338, "seek": 285600, "start": 2867.0, "end": 2874.0, "text": " Different colors.", "tokens": [20825, 4577, 13], "temperature": 0.0, "avg_logprob": -0.16311022814582377, "compression_ratio": 1.2061855670103092, "no_speech_prob": 2.6014154173026327e-06}, {"id": 339, "seek": 285600, "start": 2874.0, "end": 2877.0, "text": " This is these are perpendicular to each other.", "tokens": [639, 307, 613, 366, 26734, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.16311022814582377, "compression_ratio": 1.2061855670103092, "no_speech_prob": 2.6014154173026327e-06}, {"id": 340, "seek": 287700, "start": 2877.0, "end": 2886.0, "text": " So the idea is we projected a two onto Q one and we get that.", "tokens": [407, 264, 1558, 307, 321, 26231, 257, 732, 3911, 1249, 472, 293, 321, 483, 300, 13], "temperature": 0.0, "avg_logprob": -0.1359431403023856, "compression_ratio": 1.49079754601227, "no_speech_prob": 7.224314231280005e-07}, {"id": 341, "seek": 287700, "start": 2886.0, "end": 2894.0, "text": " R one two is the scalar you know since we might have to scale Q one to be the appropriate length.", "tokens": [497, 472, 732, 307, 264, 39684, 291, 458, 1670, 321, 1062, 362, 281, 4373, 1249, 472, 281, 312, 264, 6854, 4641, 13], "temperature": 0.0, "avg_logprob": -0.1359431403023856, "compression_ratio": 1.49079754601227, "no_speech_prob": 7.224314231280005e-07}, {"id": 342, "seek": 287700, "start": 2894.0, "end": 2900.0, "text": " And then now we're interested in this kind of perpendicular piece that's left over.", "tokens": [400, 550, 586, 321, 434, 3102, 294, 341, 733, 295, 26734, 2522, 300, 311, 1411, 670, 13], "temperature": 0.0, "avg_logprob": -0.1359431403023856, "compression_ratio": 1.49079754601227, "no_speech_prob": 7.224314231280005e-07}, {"id": 343, "seek": 290000, "start": 2900.0, "end": 2907.0, "text": " And then in red it's OK if I erase this first diagram over here.", "tokens": [400, 550, 294, 2182, 309, 311, 2264, 498, 286, 23525, 341, 700, 10686, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.31606311798095704, "compression_ratio": 1.096774193548387, "no_speech_prob": 5.771771611762233e-06}, {"id": 344, "seek": 290000, "start": 2907.0, "end": 2909.0, "text": " Everyone.", "tokens": [5198, 13], "temperature": 0.0, "avg_logprob": -0.31606311798095704, "compression_ratio": 1.096774193548387, "no_speech_prob": 5.771771611762233e-06}, {"id": 345, "seek": 290000, "start": 2909.0, "end": 2911.0, "text": " Not get too crowded.", "tokens": [1726, 483, 886, 21634, 13], "temperature": 0.0, "avg_logprob": -0.31606311798095704, "compression_ratio": 1.096774193548387, "no_speech_prob": 5.771771611762233e-06}, {"id": 346, "seek": 290000, "start": 2911.0, "end": 2916.0, "text": " Space.", "tokens": [8705, 13], "temperature": 0.0, "avg_logprob": -0.31606311798095704, "compression_ratio": 1.096774193548387, "no_speech_prob": 5.771771611762233e-06}, {"id": 347, "seek": 291600, "start": 2916.0, "end": 2931.0, "text": " So then in red what we have is.", "tokens": [407, 550, 294, 2182, 437, 321, 362, 307, 13], "temperature": 0.0, "avg_logprob": -0.19591094465816722, "compression_ratio": 1.0555555555555556, "no_speech_prob": 1.308163632529613e-06}, {"id": 348, "seek": 291600, "start": 2931.0, "end": 2934.0, "text": " This thing is a two.", "tokens": [639, 551, 307, 257, 732, 13], "temperature": 0.0, "avg_logprob": -0.19591094465816722, "compression_ratio": 1.0555555555555556, "no_speech_prob": 1.308163632529613e-06}, {"id": 349, "seek": 291600, "start": 2934.0, "end": 2937.0, "text": " Minus R one two.", "tokens": [2829, 301, 497, 472, 732, 13], "temperature": 0.0, "avg_logprob": -0.19591094465816722, "compression_ratio": 1.0555555555555556, "no_speech_prob": 1.308163632529613e-06}, {"id": 350, "seek": 291600, "start": 2937.0, "end": 2939.0, "text": " Q one.", "tokens": [1249, 472, 13], "temperature": 0.0, "avg_logprob": -0.19591094465816722, "compression_ratio": 1.0555555555555556, "no_speech_prob": 1.308163632529613e-06}, {"id": 351, "seek": 293900, "start": 2939.0, "end": 2948.0, "text": " And we're going to want that to be equal to. R two two Q two.", "tokens": [400, 321, 434, 516, 281, 528, 300, 281, 312, 2681, 281, 13, 497, 732, 732, 1249, 732, 13], "temperature": 0.0, "avg_logprob": -0.16332180235120985, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.1125218406959902e-05}, {"id": 352, "seek": 293900, "start": 2948.0, "end": 2951.0, "text": " So how do we get from so we've got a two.", "tokens": [407, 577, 360, 321, 483, 490, 370, 321, 600, 658, 257, 732, 13], "temperature": 0.0, "avg_logprob": -0.16332180235120985, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.1125218406959902e-05}, {"id": 353, "seek": 293900, "start": 2951.0, "end": 2953.0, "text": " Minus R one two Q one.", "tokens": [2829, 301, 497, 472, 732, 1249, 472, 13], "temperature": 0.0, "avg_logprob": -0.16332180235120985, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.1125218406959902e-05}, {"id": 354, "seek": 293900, "start": 2953.0, "end": 2959.0, "text": " How do we get this from this red vector. How do we decide what R two two is.", "tokens": [1012, 360, 321, 483, 341, 490, 341, 2182, 8062, 13, 1012, 360, 321, 4536, 437, 497, 732, 732, 307, 13], "temperature": 0.0, "avg_logprob": -0.16332180235120985, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.1125218406959902e-05}, {"id": 355, "seek": 293900, "start": 2959.0, "end": 2963.0, "text": " And what Q two is.", "tokens": [400, 437, 1249, 732, 307, 13], "temperature": 0.0, "avg_logprob": -0.16332180235120985, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.1125218406959902e-05}, {"id": 356, "seek": 293900, "start": 2963.0, "end": 2967.0, "text": " You want to normalize the vectors of Q two.", "tokens": [509, 528, 281, 2710, 1125, 264, 18875, 295, 1249, 732, 13], "temperature": 0.0, "avg_logprob": -0.16332180235120985, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.1125218406959902e-05}, {"id": 357, "seek": 296700, "start": 2967.0, "end": 2970.0, "text": " Exactly. Yes.", "tokens": [7587, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.153852890277731, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.255225460132351e-06}, {"id": 358, "seek": 296700, "start": 2970.0, "end": 2974.0, "text": " Yeah. So we normalize them because again let me draw it.", "tokens": [865, 13, 407, 321, 2710, 1125, 552, 570, 797, 718, 385, 2642, 309, 13], "temperature": 0.0, "avg_logprob": -0.153852890277731, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.255225460132351e-06}, {"id": 359, "seek": 296700, "start": 2974.0, "end": 2977.0, "text": " So maybe this is what length one is.", "tokens": [407, 1310, 341, 307, 437, 4641, 472, 307, 13], "temperature": 0.0, "avg_logprob": -0.153852890277731, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.255225460132351e-06}, {"id": 360, "seek": 296700, "start": 2977.0, "end": 2979.0, "text": " So that'll be Q two.", "tokens": [407, 300, 603, 312, 1249, 732, 13], "temperature": 0.0, "avg_logprob": -0.153852890277731, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.255225460132351e-06}, {"id": 361, "seek": 296700, "start": 2979.0, "end": 2982.0, "text": " And then we have to use R two two to get the right length.", "tokens": [400, 550, 321, 362, 281, 764, 497, 732, 732, 281, 483, 264, 558, 4641, 13], "temperature": 0.0, "avg_logprob": -0.153852890277731, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.255225460132351e-06}, {"id": 362, "seek": 296700, "start": 2982.0, "end": 2986.0, "text": " Q two is giving us the direction but just with length one.", "tokens": [1249, 732, 307, 2902, 505, 264, 3513, 457, 445, 365, 4641, 472, 13], "temperature": 0.0, "avg_logprob": -0.153852890277731, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.255225460132351e-06}, {"id": 363, "seek": 296700, "start": 2986.0, "end": 2990.0, "text": " And R two two gives us the correct.", "tokens": [400, 497, 732, 732, 2709, 505, 264, 3006, 13], "temperature": 0.0, "avg_logprob": -0.153852890277731, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.255225460132351e-06}, {"id": 364, "seek": 299000, "start": 2990.0, "end": 2997.0, "text": " Magnitude.", "tokens": [19664, 4377, 13], "temperature": 0.0, "avg_logprob": -0.21047253835768925, "compression_ratio": 1.3275862068965518, "no_speech_prob": 5.0145322347816546e-06}, {"id": 365, "seek": 299000, "start": 2997.0, "end": 3009.0, "text": " There are more questions about this. Yeah. This is a great question to kind of draw this out in more detail.", "tokens": [821, 366, 544, 1651, 466, 341, 13, 865, 13, 639, 307, 257, 869, 1168, 281, 733, 295, 2642, 341, 484, 294, 544, 2607, 13], "temperature": 0.0, "avg_logprob": -0.21047253835768925, "compression_ratio": 1.3275862068965518, "no_speech_prob": 5.0145322347816546e-06}, {"id": 366, "seek": 299000, "start": 3009.0, "end": 3019.0, "text": " Linda can you pass the microphone.", "tokens": [20324, 393, 291, 1320, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.21047253835768925, "compression_ratio": 1.3275862068965518, "no_speech_prob": 5.0145322347816546e-06}, {"id": 367, "seek": 301900, "start": 3019.0, "end": 3027.0, "text": " Can you go through Q three so we can see.", "tokens": [1664, 291, 352, 807, 1249, 1045, 370, 321, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.18612655840421977, "compression_ratio": 1.4586466165413534, "no_speech_prob": 3.500747880025301e-06}, {"id": 368, "seek": 301900, "start": 3027.0, "end": 3031.0, "text": " Sure. Yeah.", "tokens": [4894, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.18612655840421977, "compression_ratio": 1.4586466165413534, "no_speech_prob": 3.500747880025301e-06}, {"id": 369, "seek": 301900, "start": 3031.0, "end": 3034.0, "text": " Yeah. Let's go through Q three.", "tokens": [865, 13, 961, 311, 352, 807, 1249, 1045, 13], "temperature": 0.0, "avg_logprob": -0.18612655840421977, "compression_ratio": 1.4586466165413534, "no_speech_prob": 3.500747880025301e-06}, {"id": 370, "seek": 301900, "start": 3034.0, "end": 3047.0, "text": " And this will get a little bit trickier with draw drawing just because it's going to be in three dimensions.", "tokens": [400, 341, 486, 483, 257, 707, 857, 4282, 811, 365, 2642, 6316, 445, 570, 309, 311, 516, 281, 312, 294, 1045, 12819, 13], "temperature": 0.0, "avg_logprob": -0.18612655840421977, "compression_ratio": 1.4586466165413534, "no_speech_prob": 3.500747880025301e-06}, {"id": 371, "seek": 304700, "start": 3047.0, "end": 3068.0, "text": " OK. So now we've got a three equals R one three times Q one plus R two three times Q two plus the next line are three three times Q three.", "tokens": [2264, 13, 407, 586, 321, 600, 658, 257, 1045, 6915, 497, 472, 1045, 1413, 1249, 472, 1804, 497, 732, 1045, 1413, 1249, 732, 1804, 264, 958, 1622, 366, 1045, 1045, 1413, 1249, 1045, 13], "temperature": 0.0, "avg_logprob": -0.1920337802485416, "compression_ratio": 1.5, "no_speech_prob": 3.089458914473653e-06}, {"id": 372, "seek": 306800, "start": 3068.0, "end": 3086.0, "text": " And so that comes from taking the third column of R and that's giving us a linear combination of the QI and now we're just picking off the first three QI and everything else zeros out.", "tokens": [400, 370, 300, 1487, 490, 1940, 264, 2636, 7738, 295, 497, 293, 300, 311, 2902, 505, 257, 8213, 6562, 295, 264, 1249, 40, 293, 586, 321, 434, 445, 8867, 766, 264, 700, 1045, 1249, 40, 293, 1203, 1646, 35193, 484, 13], "temperature": 0.0, "avg_logprob": -0.09616034717883094, "compression_ratio": 1.475, "no_speech_prob": 7.811406703694956e-07}, {"id": 373, "seek": 306800, "start": 3086.0, "end": 3091.0, "text": " Also just put a line to kind of separate this part.", "tokens": [2743, 445, 829, 257, 1622, 281, 733, 295, 4994, 341, 644, 13], "temperature": 0.0, "avg_logprob": -0.09616034717883094, "compression_ratio": 1.475, "no_speech_prob": 7.811406703694956e-07}, {"id": 374, "seek": 309100, "start": 3091.0, "end": 3098.0, "text": " Can I erase the drawing from.", "tokens": [1664, 286, 23525, 264, 6316, 490, 13], "temperature": 0.0, "avg_logprob": -0.21945103732022372, "compression_ratio": 0.7837837837837838, "no_speech_prob": 4.003770663985051e-05}, {"id": 375, "seek": 309800, "start": 3098.0, "end": 3125.0, "text": " I do.", "tokens": [286, 360, 13], "temperature": 0.0, "avg_logprob": -0.40992116928100586, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.00013742124428972602}, {"id": 376, "seek": 312500, "start": 3125.0, "end": 3135.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1355500931435443, "compression_ratio": 1.275, "no_speech_prob": 1.5292133639377425e-06}, {"id": 377, "seek": 312500, "start": 3135.0, "end": 3139.0, "text": " Let me just go down to have a little bit more space.", "tokens": [961, 385, 445, 352, 760, 281, 362, 257, 707, 857, 544, 1901, 13], "temperature": 0.0, "avg_logprob": -0.1355500931435443, "compression_ratio": 1.275, "no_speech_prob": 1.5292133639377425e-06}, {"id": 378, "seek": 312500, "start": 3139.0, "end": 3146.0, "text": " So now we're going to have to imagine that we're in three dimensions.", "tokens": [407, 586, 321, 434, 516, 281, 362, 281, 3811, 300, 321, 434, 294, 1045, 12819, 13], "temperature": 0.0, "avg_logprob": -0.1355500931435443, "compression_ratio": 1.275, "no_speech_prob": 1.5292133639377425e-06}, {"id": 379, "seek": 312500, "start": 3146.0, "end": 3153.0, "text": " Kind of draw a background.", "tokens": [9242, 295, 2642, 257, 3678, 13], "temperature": 0.0, "avg_logprob": -0.1355500931435443, "compression_ratio": 1.275, "no_speech_prob": 1.5292133639377425e-06}, {"id": 380, "seek": 315300, "start": 3153.0, "end": 3161.0, "text": " So we're in three dimensions and we've already found.", "tokens": [407, 321, 434, 294, 1045, 12819, 293, 321, 600, 1217, 1352, 13], "temperature": 0.0, "avg_logprob": -0.09795673543756658, "compression_ratio": 1.3405797101449275, "no_speech_prob": 4.157247531111352e-06}, {"id": 381, "seek": 315300, "start": 3161.0, "end": 3167.0, "text": " I'm actually going to make length one a little bit longer just so this doesn't get too tight.", "tokens": [286, 478, 767, 516, 281, 652, 4641, 472, 257, 707, 857, 2854, 445, 370, 341, 1177, 380, 483, 886, 4524, 13], "temperature": 0.0, "avg_logprob": -0.09795673543756658, "compression_ratio": 1.3405797101449275, "no_speech_prob": 4.157247531111352e-06}, {"id": 382, "seek": 315300, "start": 3167.0, "end": 3171.0, "text": " But this is Q one.", "tokens": [583, 341, 307, 1249, 472, 13], "temperature": 0.0, "avg_logprob": -0.09795673543756658, "compression_ratio": 1.3405797101449275, "no_speech_prob": 4.157247531111352e-06}, {"id": 383, "seek": 315300, "start": 3171.0, "end": 3176.0, "text": " And this is Q two.", "tokens": [400, 341, 307, 1249, 732, 13], "temperature": 0.0, "avg_logprob": -0.09795673543756658, "compression_ratio": 1.3405797101449275, "no_speech_prob": 4.157247531111352e-06}, {"id": 384, "seek": 317600, "start": 3176.0, "end": 3183.0, "text": " So we have two two vectors and these are orthogonal to each other.", "tokens": [407, 321, 362, 732, 732, 18875, 293, 613, 366, 41488, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.08627678797795223, "compression_ratio": 1.4452054794520548, "no_speech_prob": 1.0844838698176318e-06}, {"id": 385, "seek": 317600, "start": 3183.0, "end": 3186.0, "text": " And that can be.", "tokens": [400, 300, 393, 312, 13], "temperature": 0.0, "avg_logprob": -0.08627678797795223, "compression_ratio": 1.4452054794520548, "no_speech_prob": 1.0844838698176318e-06}, {"id": 386, "seek": 317600, "start": 3186.0, "end": 3192.0, "text": " You know remember we're in three dimensions so that can look different depending on kind of what angle we're viewing this from.", "tokens": [509, 458, 1604, 321, 434, 294, 1045, 12819, 370, 300, 393, 574, 819, 5413, 322, 733, 295, 437, 5802, 321, 434, 17480, 341, 490, 13], "temperature": 0.0, "avg_logprob": -0.08627678797795223, "compression_ratio": 1.4452054794520548, "no_speech_prob": 1.0844838698176318e-06}, {"id": 387, "seek": 319200, "start": 3192.0, "end": 3206.0, "text": " But we've got this Q one and Q two and we're interested in taking our vector.", "tokens": [583, 321, 600, 658, 341, 1249, 472, 293, 1249, 732, 293, 321, 434, 3102, 294, 1940, 527, 8062, 13], "temperature": 0.0, "avg_logprob": -0.13530374566713968, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.93329310604895e-06}, {"id": 388, "seek": 319200, "start": 3206.0, "end": 3210.0, "text": " A three and we want to find a good Q three for it.", "tokens": [316, 1045, 293, 321, 528, 281, 915, 257, 665, 1249, 1045, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.13530374566713968, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.93329310604895e-06}, {"id": 389, "seek": 319200, "start": 3210.0, "end": 3221.0, "text": " So what should we do first.", "tokens": [407, 437, 820, 321, 360, 700, 13], "temperature": 0.0, "avg_logprob": -0.13530374566713968, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.93329310604895e-06}, {"id": 390, "seek": 322100, "start": 3221.0, "end": 3224.0, "text": " Exactly. So we're going to project it onto Q one.", "tokens": [7587, 13, 407, 321, 434, 516, 281, 1716, 309, 3911, 1249, 472, 13], "temperature": 0.0, "avg_logprob": -0.2344350627824372, "compression_ratio": 1.3025210084033614, "no_speech_prob": 1.8738357994152466e-06}, {"id": 391, "seek": 322100, "start": 3224.0, "end": 3227.0, "text": " Sorry my one looks like a two.", "tokens": [4919, 452, 472, 1542, 411, 257, 732, 13], "temperature": 0.0, "avg_logprob": -0.2344350627824372, "compression_ratio": 1.3025210084033614, "no_speech_prob": 1.8738357994152466e-06}, {"id": 392, "seek": 322100, "start": 3227.0, "end": 3232.0, "text": " That.", "tokens": [663, 13], "temperature": 0.0, "avg_logprob": -0.2344350627824372, "compression_ratio": 1.3025210084033614, "no_speech_prob": 1.8738357994152466e-06}, {"id": 393, "seek": 322100, "start": 3232.0, "end": 3236.0, "text": " Yeah. So we projected onto Q one.", "tokens": [865, 13, 407, 321, 26231, 3911, 1249, 472, 13], "temperature": 0.0, "avg_logprob": -0.2344350627824372, "compression_ratio": 1.3025210084033614, "no_speech_prob": 1.8738357994152466e-06}, {"id": 394, "seek": 322100, "start": 3236.0, "end": 3238.0, "text": " I probably should have drawn this.", "tokens": [286, 1391, 820, 362, 10117, 341, 13], "temperature": 0.0, "avg_logprob": -0.2344350627824372, "compression_ratio": 1.3025210084033614, "no_speech_prob": 1.8738357994152466e-06}, {"id": 395, "seek": 323800, "start": 3238.0, "end": 3251.0, "text": " I'm going to redraw a three just so that it is clear what it's a projection would look like.", "tokens": [286, 478, 516, 281, 2182, 5131, 257, 1045, 445, 370, 300, 309, 307, 1850, 437, 309, 311, 257, 22743, 576, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.14229345321655273, "compression_ratio": 1.3206106870229009, "no_speech_prob": 1.228900941896427e-06}, {"id": 396, "seek": 323800, "start": 3251.0, "end": 3257.0, "text": " OK. So maybe a three is actually off in this direction.", "tokens": [2264, 13, 407, 1310, 257, 1045, 307, 767, 766, 294, 341, 3513, 13], "temperature": 0.0, "avg_logprob": -0.14229345321655273, "compression_ratio": 1.3206106870229009, "no_speech_prob": 1.228900941896427e-06}, {"id": 397, "seek": 323800, "start": 3257.0, "end": 3263.0, "text": " We projected onto Q one.", "tokens": [492, 26231, 3911, 1249, 472, 13], "temperature": 0.0, "avg_logprob": -0.14229345321655273, "compression_ratio": 1.3206106870229009, "no_speech_prob": 1.228900941896427e-06}, {"id": 398, "seek": 326300, "start": 3263.0, "end": 3274.0, "text": " And then we're interested in that in this case is also going to kind of have to be extended out to be the appropriate distance.", "tokens": [400, 550, 321, 434, 3102, 294, 300, 294, 341, 1389, 307, 611, 516, 281, 733, 295, 362, 281, 312, 10913, 484, 281, 312, 264, 6854, 4560, 13], "temperature": 0.0, "avg_logprob": -0.07814645767211914, "compression_ratio": 1.492537313432836, "no_speech_prob": 2.3687111934123095e-06}, {"id": 399, "seek": 326300, "start": 3274.0, "end": 3277.0, "text": " So we subtract that off.", "tokens": [407, 321, 16390, 300, 766, 13], "temperature": 0.0, "avg_logprob": -0.07814645767211914, "compression_ratio": 1.492537313432836, "no_speech_prob": 2.3687111934123095e-06}, {"id": 400, "seek": 326300, "start": 3277.0, "end": 3291.0, "text": " And then what do we what do we want to do next.", "tokens": [400, 550, 437, 360, 321, 437, 360, 321, 528, 281, 360, 958, 13], "temperature": 0.0, "avg_logprob": -0.07814645767211914, "compression_ratio": 1.492537313432836, "no_speech_prob": 2.3687111934123095e-06}, {"id": 401, "seek": 329100, "start": 3291.0, "end": 3297.0, "text": " Matthew and can you pass the microphone Linda.", "tokens": [12434, 293, 393, 291, 1320, 264, 10952, 20324, 13], "temperature": 0.0, "avg_logprob": -0.1907453945704869, "compression_ratio": 1.416184971098266, "no_speech_prob": 4.56579709862126e-06}, {"id": 402, "seek": 329100, "start": 3297.0, "end": 3301.0, "text": " Do we have to take care of the Q2 projection. Exactly. Yeah.", "tokens": [1144, 321, 362, 281, 747, 1127, 295, 264, 1249, 17, 22743, 13, 7587, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.1907453945704869, "compression_ratio": 1.416184971098266, "no_speech_prob": 4.56579709862126e-06}, {"id": 403, "seek": 329100, "start": 3301.0, "end": 3305.0, "text": " So we'll project onto Q2 to kind of get that part.", "tokens": [407, 321, 603, 1716, 3911, 1249, 17, 281, 733, 295, 483, 300, 644, 13], "temperature": 0.0, "avg_logprob": -0.1907453945704869, "compression_ratio": 1.416184971098266, "no_speech_prob": 4.56579709862126e-06}, {"id": 404, "seek": 329100, "start": 3305.0, "end": 3308.0, "text": " And so in this case.", "tokens": [400, 370, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.1907453945704869, "compression_ratio": 1.416184971098266, "no_speech_prob": 4.56579709862126e-06}, {"id": 405, "seek": 329100, "start": 3308.0, "end": 3312.0, "text": " Yeah and it's really hard to visualize these in three dimensions.", "tokens": [865, 293, 309, 311, 534, 1152, 281, 23273, 613, 294, 1045, 12819, 13], "temperature": 0.0, "avg_logprob": -0.1907453945704869, "compression_ratio": 1.416184971098266, "no_speech_prob": 4.56579709862126e-06}, {"id": 406, "seek": 331200, "start": 3312.0, "end": 3322.0, "text": " But the idea is that the way I've drawn it there might be like very little of a three that's represented on Q2.", "tokens": [583, 264, 1558, 307, 300, 264, 636, 286, 600, 10117, 309, 456, 1062, 312, 411, 588, 707, 295, 257, 1045, 300, 311, 10379, 322, 1249, 17, 13], "temperature": 0.0, "avg_logprob": -0.08034778322492327, "compression_ratio": 1.607361963190184, "no_speech_prob": 4.3138444993928715e-07}, {"id": 407, "seek": 331200, "start": 3322.0, "end": 3332.0, "text": " But we've got some component here of a three was represented from Q2 and part of it was represented with Q1.", "tokens": [583, 321, 600, 658, 512, 6542, 510, 295, 257, 1045, 390, 10379, 490, 1249, 17, 293, 644, 295, 309, 390, 10379, 365, 1249, 16, 13], "temperature": 0.0, "avg_logprob": -0.08034778322492327, "compression_ratio": 1.607361963190184, "no_speech_prob": 4.3138444993928715e-07}, {"id": 408, "seek": 331200, "start": 3332.0, "end": 3335.0, "text": " And then we're interested in what's left.", "tokens": [400, 550, 321, 434, 3102, 294, 437, 311, 1411, 13], "temperature": 0.0, "avg_logprob": -0.08034778322492327, "compression_ratio": 1.607361963190184, "no_speech_prob": 4.3138444993928715e-07}, {"id": 409, "seek": 333500, "start": 3335.0, "end": 3342.0, "text": " So we're kind of looking at.", "tokens": [407, 321, 434, 733, 295, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.08549220221383232, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.1910900639122701e-06}, {"id": 410, "seek": 333500, "start": 3342.0, "end": 3345.0, "text": " I'll do these in black.", "tokens": [286, 603, 360, 613, 294, 2211, 13], "temperature": 0.0, "avg_logprob": -0.08549220221383232, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.1910900639122701e-06}, {"id": 411, "seek": 333500, "start": 3345.0, "end": 3351.0, "text": " So we kind of have like the Q1 part of a three and Q2 part of a three.", "tokens": [407, 321, 733, 295, 362, 411, 264, 1249, 16, 644, 295, 257, 1045, 293, 1249, 17, 644, 295, 257, 1045, 13], "temperature": 0.0, "avg_logprob": -0.08549220221383232, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.1910900639122701e-06}, {"id": 412, "seek": 333500, "start": 3351.0, "end": 3355.0, "text": " But then that probably doesn't capture all of a three.", "tokens": [583, 550, 300, 1391, 1177, 380, 7983, 439, 295, 257, 1045, 13], "temperature": 0.0, "avg_logprob": -0.08549220221383232, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.1910900639122701e-06}, {"id": 413, "seek": 333500, "start": 3355.0, "end": 3360.0, "text": " And so whatever's left is going to be the Q3 part of a three.", "tokens": [400, 370, 2035, 311, 1411, 307, 516, 281, 312, 264, 1249, 18, 644, 295, 257, 1045, 13], "temperature": 0.0, "avg_logprob": -0.08549220221383232, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.1910900639122701e-06}, {"id": 414, "seek": 336000, "start": 3360.0, "end": 3372.0, "text": " So we would do kind of a three minus you know this Q1 times a scalar that gets this direction minus Q2 times a scalar that gets this direction.", "tokens": [407, 321, 576, 360, 733, 295, 257, 1045, 3175, 291, 458, 341, 1249, 16, 1413, 257, 39684, 300, 2170, 341, 3513, 3175, 1249, 17, 1413, 257, 39684, 300, 2170, 341, 3513, 13], "temperature": 0.0, "avg_logprob": -0.07609410639162417, "compression_ratio": 1.534351145038168, "no_speech_prob": 1.7880341829368263e-06}, {"id": 415, "seek": 336000, "start": 3372.0, "end": 3386.0, "text": " And then what we're left with is Q3 once we normalize it.", "tokens": [400, 550, 437, 321, 434, 1411, 365, 307, 1249, 18, 1564, 321, 2710, 1125, 309, 13], "temperature": 0.0, "avg_logprob": -0.07609410639162417, "compression_ratio": 1.534351145038168, "no_speech_prob": 1.7880341829368263e-06}, {"id": 416, "seek": 338600, "start": 3386.0, "end": 3391.0, "text": " I guess I don't know maybe in this case that would be like.", "tokens": [286, 2041, 286, 500, 380, 458, 1310, 294, 341, 1389, 300, 576, 312, 411, 13], "temperature": 0.0, "avg_logprob": -0.09999721114699905, "compression_ratio": 1.5789473684210527, "no_speech_prob": 7.527870366175193e-06}, {"id": 417, "seek": 338600, "start": 3391.0, "end": 3402.0, "text": " Maybe there's something like going a little bit this way that hasn't been captured so far and that could be.", "tokens": [2704, 456, 311, 746, 411, 516, 257, 707, 857, 341, 636, 300, 6132, 380, 668, 11828, 370, 1400, 293, 300, 727, 312, 13], "temperature": 0.0, "avg_logprob": -0.09999721114699905, "compression_ratio": 1.5789473684210527, "no_speech_prob": 7.527870366175193e-06}, {"id": 418, "seek": 338600, "start": 3402.0, "end": 3413.0, "text": " And that could even be like that would be R33 times Q3 and then we normalize it to get OK this is Q3.", "tokens": [400, 300, 727, 754, 312, 411, 300, 576, 312, 497, 10191, 1413, 1249, 18, 293, 550, 321, 2710, 1125, 309, 281, 483, 2264, 341, 307, 1249, 18, 13], "temperature": 0.0, "avg_logprob": -0.09999721114699905, "compression_ratio": 1.5789473684210527, "no_speech_prob": 7.527870366175193e-06}, {"id": 419, "seek": 341300, "start": 3413.0, "end": 3421.0, "text": " Questions about this?", "tokens": [27738, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.16608954449089205, "compression_ratio": 1.2113821138211383, "no_speech_prob": 6.643067536060698e-06}, {"id": 420, "seek": 341300, "start": 3421.0, "end": 3427.0, "text": " All right and we are kind of well due for a break so it's 12.04.", "tokens": [1057, 558, 293, 321, 366, 733, 295, 731, 3462, 337, 257, 1821, 370, 309, 311, 2272, 13, 14565, 13], "temperature": 0.0, "avg_logprob": -0.16608954449089205, "compression_ratio": 1.2113821138211383, "no_speech_prob": 6.643067536060698e-06}, {"id": 421, "seek": 341300, "start": 3427.0, "end": 3432.0, "text": " So let's meet back here at 12.10 to continue.", "tokens": [407, 718, 311, 1677, 646, 510, 412, 2272, 13, 3279, 281, 2354, 13], "temperature": 0.0, "avg_logprob": -0.16608954449089205, "compression_ratio": 1.2113821138211383, "no_speech_prob": 6.643067536060698e-06}, {"id": 422, "seek": 341300, "start": 3432.0, "end": 3441.0, "text": " Thanks everyone.", "tokens": [2561, 1518, 13], "temperature": 0.0, "avg_logprob": -0.16608954449089205, "compression_ratio": 1.2113821138211383, "no_speech_prob": 6.643067536060698e-06}, {"id": 423, "seek": 344100, "start": 3441.0, "end": 3446.0, "text": " All right let's go ahead and start back up.", "tokens": [1057, 558, 718, 311, 352, 2286, 293, 722, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.09086749523500853, "compression_ratio": 1.4972677595628416, "no_speech_prob": 1.4737942365172785e-05}, {"id": 424, "seek": 344100, "start": 3446.0, "end": 3453.0, "text": " So I wanted to kind of go back to the code now that we've seen these pictures of what's going on.", "tokens": [407, 286, 1415, 281, 733, 295, 352, 646, 281, 264, 3089, 586, 300, 321, 600, 1612, 613, 5242, 295, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.09086749523500853, "compression_ratio": 1.4972677595628416, "no_speech_prob": 1.4737942365172785e-05}, {"id": 425, "seek": 344100, "start": 3453.0, "end": 3457.0, "text": " And so what's happening with this inner for loop.", "tokens": [400, 370, 437, 311, 2737, 365, 341, 7284, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.09086749523500853, "compression_ratio": 1.4972677595628416, "no_speech_prob": 1.4737942365172785e-05}, {"id": 426, "seek": 344100, "start": 3457.0, "end": 3466.0, "text": " So the outer for loop is where we're trying to find like a particular Q, QI or Q2.", "tokens": [407, 264, 10847, 337, 6367, 307, 689, 321, 434, 1382, 281, 915, 411, 257, 1729, 1249, 11, 1249, 40, 420, 1249, 17, 13], "temperature": 0.0, "avg_logprob": -0.09086749523500853, "compression_ratio": 1.4972677595628416, "no_speech_prob": 1.4737942365172785e-05}, {"id": 427, "seek": 346600, "start": 3466.0, "end": 3473.0, "text": " And then in this inner for loop is where we're doing the projections onto the previous Qs.", "tokens": [400, 550, 294, 341, 7284, 337, 6367, 307, 689, 321, 434, 884, 264, 32371, 3911, 264, 3894, 1249, 82, 13], "temperature": 0.0, "avg_logprob": -0.13338577915245378, "compression_ratio": 1.5644171779141105, "no_speech_prob": 1.9524644812918268e-05}, {"id": 428, "seek": 346600, "start": 3473.0, "end": 3480.0, "text": " So for the case of Q3 we're trying to find this for kind of outer loop J equals 3.", "tokens": [407, 337, 264, 1389, 295, 1249, 18, 321, 434, 1382, 281, 915, 341, 337, 733, 295, 10847, 6367, 508, 6915, 805, 13], "temperature": 0.0, "avg_logprob": -0.13338577915245378, "compression_ratio": 1.5644171779141105, "no_speech_prob": 1.9524644812918268e-05}, {"id": 429, "seek": 346600, "start": 3480.0, "end": 3489.0, "text": " And then we go in here and we project the third A3 onto Q1 and subtract that off.", "tokens": [400, 550, 321, 352, 294, 510, 293, 321, 1716, 264, 2636, 316, 18, 3911, 1249, 16, 293, 16390, 300, 766, 13], "temperature": 0.0, "avg_logprob": -0.13338577915245378, "compression_ratio": 1.5644171779141105, "no_speech_prob": 1.9524644812918268e-05}, {"id": 430, "seek": 348900, "start": 3489.0, "end": 3498.0, "text": " And we project it onto Q2 and subtract that off and then that's going to leave us with what we can use for our Q3.", "tokens": [400, 321, 1716, 309, 3911, 1249, 17, 293, 16390, 300, 766, 293, 550, 300, 311, 516, 281, 1856, 505, 365, 437, 321, 393, 764, 337, 527, 1249, 18, 13], "temperature": 0.0, "avg_logprob": -0.1397146224975586, "compression_ratio": 1.3088235294117647, "no_speech_prob": 3.2377104162151227e-06}, {"id": 431, "seek": 348900, "start": 3498.0, "end": 3505.0, "text": " Any more questions about this?", "tokens": [2639, 544, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.1397146224975586, "compression_ratio": 1.3088235294117647, "no_speech_prob": 3.2377104162151227e-06}, {"id": 432, "seek": 348900, "start": 3505.0, "end": 3508.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1397146224975586, "compression_ratio": 1.3088235294117647, "no_speech_prob": 3.2377104162151227e-06}, {"id": 433, "seek": 348900, "start": 3508.0, "end": 3511.0, "text": " Yeah so this is what we saw.", "tokens": [865, 370, 341, 307, 437, 321, 1866, 13], "temperature": 0.0, "avg_logprob": -0.1397146224975586, "compression_ratio": 1.3088235294117647, "no_speech_prob": 3.2377104162151227e-06}, {"id": 434, "seek": 351100, "start": 3511.0, "end": 3519.0, "text": " This is the perhaps like the most straightforward way of coming up with a QR factorization.", "tokens": [639, 307, 264, 4317, 411, 264, 881, 15325, 636, 295, 1348, 493, 365, 257, 32784, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.1008941682718568, "compression_ratio": 1.3905325443786982, "no_speech_prob": 2.8129463771620067e-06}, {"id": 435, "seek": 351100, "start": 3519.0, "end": 3529.0, "text": " Unfortunately turns out it's unstable which we'll see an example of later on.", "tokens": [8590, 4523, 484, 309, 311, 23742, 597, 321, 603, 536, 364, 1365, 295, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.1008941682718568, "compression_ratio": 1.3905325443786982, "no_speech_prob": 2.8129463771620067e-06}, {"id": 436, "seek": 351100, "start": 3529.0, "end": 3534.0, "text": " So we're going to use the next look at the modified Gram-Schmidt.", "tokens": [407, 321, 434, 516, 281, 764, 264, 958, 574, 412, 264, 15873, 22130, 12, 31560, 39000, 13], "temperature": 0.0, "avg_logprob": -0.1008941682718568, "compression_ratio": 1.3905325443786982, "no_speech_prob": 2.8129463771620067e-06}, {"id": 437, "seek": 353400, "start": 3534.0, "end": 3557.0, "text": " And modified Gram-Schmidt basically so in classic Gram-Schmidt for each J you're coming up with this single projection onto your QJ to get the kind of the final like this is Q3.", "tokens": [400, 15873, 22130, 12, 31560, 39000, 1936, 370, 294, 7230, 22130, 12, 31560, 39000, 337, 1184, 508, 291, 434, 1348, 493, 365, 341, 2167, 22743, 3911, 428, 1249, 41, 281, 483, 264, 733, 295, 264, 2572, 411, 341, 307, 1249, 18, 13], "temperature": 0.0, "avg_logprob": -0.18030388458915378, "compression_ratio": 1.3014705882352942, "no_speech_prob": 4.565861672745086e-06}, {"id": 438, "seek": 355700, "start": 3557.0, "end": 3564.0, "text": " You know we projected onto onto it.", "tokens": [509, 458, 321, 26231, 3911, 3911, 309, 13], "temperature": 0.0, "avg_logprob": -0.15449180205663046, "compression_ratio": 1.2698412698412698, "no_speech_prob": 5.093563231639564e-06}, {"id": 439, "seek": 355700, "start": 3564.0, "end": 3571.0, "text": " And in that case Q3 is the projection onto the space that's orthogonal to Q1 and Q2.", "tokens": [400, 294, 300, 1389, 1249, 18, 307, 264, 22743, 3911, 264, 1901, 300, 311, 41488, 281, 1249, 16, 293, 1249, 17, 13], "temperature": 0.0, "avg_logprob": -0.15449180205663046, "compression_ratio": 1.2698412698412698, "no_speech_prob": 5.093563231639564e-06}, {"id": 440, "seek": 355700, "start": 3571.0, "end": 3584.0, "text": " For modified Gram-Schmidt you're doing.", "tokens": [1171, 15873, 22130, 12, 31560, 39000, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.15449180205663046, "compression_ratio": 1.2698412698412698, "no_speech_prob": 5.093563231639564e-06}, {"id": 441, "seek": 358400, "start": 3584.0, "end": 3607.0, "text": " Sorry hold on a moment.", "tokens": [4919, 1797, 322, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.12212667465209961, "compression_ratio": 0.7419354838709677, "no_speech_prob": 2.2471920601674356e-05}, {"id": 442, "seek": 360700, "start": 3607.0, "end": 3616.0, "text": " Actually I think what I showed you is really modified Gram-Schmidt of doing the projection separately.", "tokens": [5135, 286, 519, 437, 286, 4712, 291, 307, 534, 15873, 22130, 12, 31560, 39000, 295, 884, 264, 22743, 14759, 13], "temperature": 0.0, "avg_logprob": -0.19426205374977806, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.2098224715373362e-06}, {"id": 443, "seek": 360700, "start": 3616.0, "end": 3622.0, "text": " Oh we're going to. No.", "tokens": [876, 321, 434, 516, 281, 13, 883, 13], "temperature": 0.0, "avg_logprob": -0.19426205374977806, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.2098224715373362e-06}, {"id": 444, "seek": 360700, "start": 3622.0, "end": 3635.0, "text": " Yeah really in some ways this is good because modified Gram-Schmidt was actually used that were", "tokens": [865, 534, 294, 512, 2098, 341, 307, 665, 570, 15873, 22130, 12, 31560, 39000, 390, 767, 1143, 300, 645], "temperature": 0.0, "avg_logprob": -0.19426205374977806, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.2098224715373362e-06}, {"id": 445, "seek": 363500, "start": 3635.0, "end": 3642.0, "text": " like for each J we're kind of calculating onto.", "tokens": [411, 337, 1184, 508, 321, 434, 733, 295, 28258, 3911, 13], "temperature": 0.0, "avg_logprob": -0.150681173100191, "compression_ratio": 1.198019801980198, "no_speech_prob": 1.544564111100044e-05}, {"id": 446, "seek": 363500, "start": 3642.0, "end": 3649.0, "text": " Yeah getting something that's the part that's perpendicular to Q1 and Q2.", "tokens": [865, 1242, 746, 300, 311, 264, 644, 300, 311, 26734, 281, 1249, 16, 293, 1249, 17, 13], "temperature": 0.0, "avg_logprob": -0.150681173100191, "compression_ratio": 1.198019801980198, "no_speech_prob": 1.544564111100044e-05}, {"id": 447, "seek": 364900, "start": 3649.0, "end": 3667.0, "text": " Let me I'll email you about this I'll think about this some more.", "tokens": [961, 385, 286, 603, 3796, 291, 466, 341, 286, 603, 519, 466, 341, 512, 544, 13], "temperature": 0.0, "avg_logprob": -0.11278553009033203, "compression_ratio": 1.1607142857142858, "no_speech_prob": 3.785180297199986e-06}, {"id": 448, "seek": 366700, "start": 3667.0, "end": 3679.0, "text": " Okay no no because like okay so with this one I was starting with the column and then kind of subtracting off how it projected onto the other ones with modified Gram-Schmidt.", "tokens": [1033, 572, 572, 570, 411, 1392, 370, 365, 341, 472, 286, 390, 2891, 365, 264, 7738, 293, 550, 733, 295, 16390, 278, 766, 577, 309, 26231, 3911, 264, 661, 2306, 365, 15873, 22130, 12, 31560, 39000, 13], "temperature": 0.0, "avg_logprob": -0.10375648591576553, "compression_ratio": 1.3384615384615384, "no_speech_prob": 2.406017074463307e-06}, {"id": 449, "seek": 367900, "start": 3679.0, "end": 3704.0, "text": " I really am kind of starting by normalizing my vector and saying that that's QI.", "tokens": [286, 534, 669, 733, 295, 2891, 538, 2710, 3319, 452, 8062, 293, 1566, 300, 300, 311, 1249, 40, 13], "temperature": 0.0, "avg_logprob": -0.08738042997277301, "compression_ratio": 1.0256410256410255, "no_speech_prob": 3.5558764466259163e-06}, {"id": 450, "seek": 370400, "start": 3704.0, "end": 3710.0, "text": " Yeah let me let me continue and I will email you an update about modified Gram-Schmidt.", "tokens": [865, 718, 385, 718, 385, 2354, 293, 286, 486, 3796, 291, 364, 5623, 466, 15873, 22130, 12, 31560, 39000, 13], "temperature": 0.0, "avg_logprob": -0.073780397268442, "compression_ratio": 1.670520231213873, "no_speech_prob": 2.2602598619414493e-06}, {"id": 451, "seek": 370400, "start": 3710.0, "end": 3724.0, "text": " But keep keep that picture of kind of like the general idea is that Gram-Schmidt is all about projections and that it's about constructing both classical and modified Gram-Schmidt are constructing your", "tokens": [583, 1066, 1066, 300, 3036, 295, 733, 295, 411, 264, 2674, 1558, 307, 300, 22130, 12, 31560, 39000, 307, 439, 466, 32371, 293, 300, 309, 311, 466, 39969, 1293, 13735, 293, 15873, 22130, 12, 31560, 39000, 366, 39969, 428], "temperature": 0.0, "avg_logprob": -0.073780397268442, "compression_ratio": 1.670520231213873, "no_speech_prob": 2.2602598619414493e-06}, {"id": 452, "seek": 372400, "start": 3724.0, "end": 3741.0, "text": " matrix Q one column at a time and that kind of each outer iteration at the end of it you want to have an additional column of Q. So this is a kind of a point about Gram-Schmidt if you were to stop part of the way through you would have", "tokens": [8141, 1249, 472, 7738, 412, 257, 565, 293, 300, 733, 295, 1184, 10847, 24784, 412, 264, 917, 295, 309, 291, 528, 281, 362, 364, 4497, 7738, 295, 1249, 13, 407, 341, 307, 257, 733, 295, 257, 935, 466, 22130, 12, 31560, 39000, 498, 291, 645, 281, 1590, 644, 295, 264, 636, 807, 291, 576, 362], "temperature": 0.0, "avg_logprob": -0.10140174930378543, "compression_ratio": 1.525974025974026, "no_speech_prob": 5.771768883278128e-06}, {"id": 453, "seek": 374100, "start": 3741.0, "end": 3757.0, "text": " like a partial set of Q's that are orthonormal to each other. And so Trevathan refers to this as triangular orthogonalization that you're kind of just coming up with this one Q.", "tokens": [411, 257, 14641, 992, 295, 1249, 311, 300, 366, 420, 11943, 24440, 281, 1184, 661, 13, 400, 370, 8648, 85, 9390, 14942, 281, 341, 382, 38190, 41488, 2144, 300, 291, 434, 733, 295, 445, 1348, 493, 365, 341, 472, 1249, 13], "temperature": 0.0, "avg_logprob": -0.13361423280504015, "compression_ratio": 1.3409090909090908, "no_speech_prob": 8.664447705086786e-06}, {"id": 454, "seek": 375700, "start": 3757.0, "end": 3776.0, "text": " You could actually think of that as being a lot of smaller or not smaller but less informative triangular matrices because basically kind of doing these projections could be represented by this triangular operation.", "tokens": [509, 727, 767, 519, 295, 300, 382, 885, 257, 688, 295, 4356, 420, 406, 4356, 457, 1570, 27759, 38190, 32284, 570, 1936, 733, 295, 884, 613, 32371, 727, 312, 10379, 538, 341, 38190, 6916, 13], "temperature": 0.0, "avg_logprob": -0.10964704171205178, "compression_ratio": 1.5357142857142858, "no_speech_prob": 7.296261173905805e-06}, {"id": 455, "seek": 377600, "start": 3776.0, "end": 3793.0, "text": " You're just getting one one Q and kind of the focus is on building Q. So Householder is a different approach where basically you're focused on zeroing out the bottom triangle and A.", "tokens": [509, 434, 445, 1242, 472, 472, 1249, 293, 733, 295, 264, 1879, 307, 322, 2390, 1249, 13, 407, 4928, 20480, 307, 257, 819, 3109, 689, 1936, 291, 434, 5178, 322, 4018, 278, 484, 264, 2767, 13369, 293, 316, 13], "temperature": 0.0, "avg_logprob": -0.10779251550373278, "compression_ratio": 1.674641148325359, "no_speech_prob": 2.4060507257672725e-06}, {"id": 456, "seek": 377600, "start": 3793.0, "end": 3804.0, "text": " And so kind of the whole focus is putting and basically each iteration for Householder you're going to take another column and make everything below the diagonal zeros.", "tokens": [400, 370, 733, 295, 264, 1379, 1879, 307, 3372, 293, 1936, 1184, 24784, 337, 4928, 20480, 291, 434, 516, 281, 747, 1071, 7738, 293, 652, 1203, 2507, 264, 21539, 35193, 13], "temperature": 0.0, "avg_logprob": -0.10779251550373278, "compression_ratio": 1.674641148325359, "no_speech_prob": 2.4060507257672725e-06}, {"id": 457, "seek": 380400, "start": 3804.0, "end": 3813.0, "text": " And you keep doing that and it turns out that that's equivalent to multiplying by a orthonormal matrix each time.", "tokens": [400, 291, 1066, 884, 300, 293, 309, 4523, 484, 300, 300, 311, 10344, 281, 30955, 538, 257, 420, 11943, 24440, 8141, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.13520352269562197, "compression_ratio": 1.4820512820512821, "no_speech_prob": 1.0451372872921638e-05}, {"id": 458, "seek": 380400, "start": 3813.0, "end": 3824.0, "text": " So kind of like yeah I guess the opposite thing is center with that Gram-Schmidt the central idea is getting these Q's and you're focused on what do we need to get the next Q.", "tokens": [407, 733, 295, 411, 1338, 286, 2041, 264, 6182, 551, 307, 3056, 365, 300, 22130, 12, 31560, 39000, 264, 5777, 1558, 307, 1242, 613, 1249, 311, 293, 291, 434, 5178, 322, 437, 360, 321, 643, 281, 483, 264, 958, 1249, 13], "temperature": 0.0, "avg_logprob": -0.13520352269562197, "compression_ratio": 1.4820512820512821, "no_speech_prob": 1.0451372872921638e-05}, {"id": 459, "seek": 382400, "start": 3824.0, "end": 3837.0, "text": " So with Householder it's how can I zero out some more more entries below the diagonal.", "tokens": [407, 365, 4928, 20480, 309, 311, 577, 393, 286, 4018, 484, 512, 544, 544, 23041, 2507, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.0833883700163468, "compression_ratio": 1.0617283950617284, "no_speech_prob": 1.0844904636542196e-06}, {"id": 460, "seek": 383700, "start": 3837.0, "end": 3857.0, "text": " So with Householder I think is a lot harder to think about conceptually.", "tokens": [407, 365, 4928, 20480, 286, 519, 307, 257, 688, 6081, 281, 519, 466, 3410, 671, 13], "temperature": 0.0, "avg_logprob": -0.13329626321792604, "compression_ratio": 1.0140845070422535, "no_speech_prob": 2.6425125270179706e-06}, {"id": 461, "seek": 385700, "start": 3857.0, "end": 3867.0, "text": " I even think I might. Yeah we're not going to get into the details of the algorithm.", "tokens": [286, 754, 519, 286, 1062, 13, 865, 321, 434, 406, 516, 281, 483, 666, 264, 4365, 295, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.11595088958740235, "compression_ratio": 1.3955223880597014, "no_speech_prob": 3.785143462664564e-06}, {"id": 462, "seek": 385700, "start": 3867.0, "end": 3873.0, "text": " Let's check that what we get. So there's this there's also this concept called Householder Reflectors.", "tokens": [961, 311, 1520, 300, 437, 321, 483, 13, 407, 456, 311, 341, 456, 311, 611, 341, 3410, 1219, 4928, 20480, 16957, 1809, 830, 13], "temperature": 0.0, "avg_logprob": -0.11595088958740235, "compression_ratio": 1.3955223880597014, "no_speech_prob": 3.785143462664564e-06}, {"id": 463, "seek": 387300, "start": 3873.0, "end": 3886.0, "text": " And so it's basically where I put my stylus.", "tokens": [400, 370, 309, 311, 1936, 689, 286, 829, 452, 7952, 3063, 13], "temperature": 0.0, "avg_logprob": -0.24447083473205566, "compression_ratio": 0.8461538461538461, "no_speech_prob": 2.521454689485836e-06}, {"id": 464, "seek": 388600, "start": 3886.0, "end": 3907.0, "text": " Each time with Householder you're multiplying by a block matrix where part of the matrix is the identity.", "tokens": [6947, 565, 365, 4928, 20480, 291, 434, 30955, 538, 257, 3461, 8141, 689, 644, 295, 264, 8141, 307, 264, 6575, 13], "temperature": 0.0, "avg_logprob": -0.14498601489596896, "compression_ratio": 1.437956204379562, "no_speech_prob": 3.555889179551741e-06}, {"id": 465, "seek": 388600, "start": 3907.0, "end": 3912.0, "text": " So block matrix is a way of kind of putting together smaller matrices into a bigger matrix.", "tokens": [407, 3461, 8141, 307, 257, 636, 295, 733, 295, 3372, 1214, 4356, 32284, 666, 257, 3801, 8141, 13], "temperature": 0.0, "avg_logprob": -0.14498601489596896, "compression_ratio": 1.437956204379562, "no_speech_prob": 3.555889179551741e-06}, {"id": 466, "seek": 391200, "start": 3912.0, "end": 3920.0, "text": " And part of that is the identity matrix and then part of it is this special matrix called a Householder Reflector.", "tokens": [400, 644, 295, 300, 307, 264, 6575, 8141, 293, 550, 644, 295, 309, 307, 341, 2121, 8141, 1219, 257, 4928, 20480, 16957, 306, 1672, 13], "temperature": 0.0, "avg_logprob": -0.1048923461667953, "compression_ratio": 1.4969325153374233, "no_speech_prob": 9.368181054014713e-06}, {"id": 467, "seek": 391200, "start": 3920.0, "end": 3929.0, "text": " And this is kind of designed and what's going to happen is that I is getting bigger and bigger as you iterate.", "tokens": [400, 341, 307, 733, 295, 4761, 293, 437, 311, 516, 281, 1051, 307, 300, 286, 307, 1242, 3801, 293, 3801, 382, 291, 44497, 13], "temperature": 0.0, "avg_logprob": -0.1048923461667953, "compression_ratio": 1.4969325153374233, "no_speech_prob": 9.368181054014713e-06}, {"id": 468, "seek": 391200, "start": 3929.0, "end": 3931.0, "text": " Let me check back.", "tokens": [961, 385, 1520, 646, 13], "temperature": 0.0, "avg_logprob": -0.1048923461667953, "compression_ratio": 1.4969325153374233, "no_speech_prob": 9.368181054014713e-06}, {"id": 469, "seek": 393100, "start": 3931.0, "end": 3943.0, "text": " So you're you're going in one direction of I either getting bigger or smaller F getting smaller or bigger.", "tokens": [407, 291, 434, 291, 434, 516, 294, 472, 3513, 295, 286, 2139, 1242, 3801, 420, 4356, 479, 1242, 4356, 420, 3801, 13], "temperature": 0.0, "avg_logprob": -0.16347323197584887, "compression_ratio": 1.6625, "no_speech_prob": 1.1015711152140284e-06}, {"id": 470, "seek": 393100, "start": 3943.0, "end": 3948.0, "text": " And this this whole thing is orthonormal.", "tokens": [400, 341, 341, 1379, 551, 307, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.16347323197584887, "compression_ratio": 1.6625, "no_speech_prob": 1.1015711152140284e-06}, {"id": 471, "seek": 393100, "start": 3948.0, "end": 3959.0, "text": " And the idea is that multiplying this times it's on this side this times a is going to zero out an additional column.", "tokens": [400, 264, 1558, 307, 300, 30955, 341, 1413, 309, 311, 322, 341, 1252, 341, 1413, 257, 307, 516, 281, 4018, 484, 364, 4497, 7738, 13], "temperature": 0.0, "avg_logprob": -0.16347323197584887, "compression_ratio": 1.6625, "no_speech_prob": 1.1015711152140284e-06}, {"id": 472, "seek": 395900, "start": 3959.0, "end": 3974.0, "text": " So you'll get kind of more zeros zero kind of constructing constructing these matrices.", "tokens": [407, 291, 603, 483, 733, 295, 544, 35193, 4018, 733, 295, 39969, 39969, 613, 32284, 13], "temperature": 0.0, "avg_logprob": -0.12790297269821166, "compression_ratio": 1.2083333333333333, "no_speech_prob": 4.3567711145442445e-06}, {"id": 473, "seek": 397400, "start": 3974.0, "end": 3996.0, "text": " So we calculate Q and R using these block matrices F.", "tokens": [407, 321, 8873, 1249, 293, 497, 1228, 613, 3461, 32284, 479, 13], "temperature": 0.0, "avg_logprob": -0.21972745656967163, "compression_ratio": 0.8688524590163934, "no_speech_prob": 7.1830959313956555e-06}, {"id": 474, "seek": 399600, "start": 3996.0, "end": 4005.0, "text": " And here I'm kind of just showing OK so it must be that I yeah I starts out small so this is a one by one identity matrix right here.", "tokens": [400, 510, 286, 478, 733, 295, 445, 4099, 2264, 370, 309, 1633, 312, 300, 286, 1338, 286, 3719, 484, 1359, 370, 341, 307, 257, 472, 538, 472, 6575, 8141, 558, 510, 13], "temperature": 0.0, "avg_logprob": -0.1384659767150879, "compression_ratio": 1.4172185430463575, "no_speech_prob": 8.26749783300329e-06}, {"id": 475, "seek": 399600, "start": 4005.0, "end": 4011.0, "text": " And F is three by three.", "tokens": [400, 479, 307, 1045, 538, 1045, 13], "temperature": 0.0, "avg_logprob": -0.1384659767150879, "compression_ratio": 1.4172185430463575, "no_speech_prob": 8.26749783300329e-06}, {"id": 476, "seek": 399600, "start": 4011.0, "end": 4023.0, "text": " Actually then we can see this would be nice to look at.", "tokens": [5135, 550, 321, 393, 536, 341, 576, 312, 1481, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.1384659767150879, "compression_ratio": 1.4172185430463575, "no_speech_prob": 8.26749783300329e-06}, {"id": 477, "seek": 402300, "start": 4023.0, "end": 4040.0, "text": " Now we've got a two by two identity matrix in this corner and F is two by two.", "tokens": [823, 321, 600, 658, 257, 732, 538, 732, 6575, 8141, 294, 341, 4538, 293, 479, 307, 732, 538, 732, 13], "temperature": 0.0, "avg_logprob": -0.08635973563561072, "compression_ratio": 1.6071428571428572, "no_speech_prob": 9.080204108613543e-06}, {"id": 478, "seek": 402300, "start": 4040.0, "end": 4047.0, "text": " Here I is three by three and then F ends up having having to be one since it's got to be orthonormal.", "tokens": [1692, 286, 307, 1045, 538, 1045, 293, 550, 479, 5314, 493, 1419, 1419, 281, 312, 472, 1670, 309, 311, 658, 281, 312, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.08635973563561072, "compression_ratio": 1.6071428571428572, "no_speech_prob": 9.080204108613543e-06}, {"id": 479, "seek": 402300, "start": 4047.0, "end": 4050.0, "text": " It's either going to be one or negative one.", "tokens": [467, 311, 2139, 516, 281, 312, 472, 420, 3671, 472, 13], "temperature": 0.0, "avg_logprob": -0.08635973563561072, "compression_ratio": 1.6071428571428572, "no_speech_prob": 9.080204108613543e-06}, {"id": 480, "seek": 405000, "start": 4050.0, "end": 4056.0, "text": " You're kind of constructing these and the key is you get back Q and R.", "tokens": [509, 434, 733, 295, 39969, 613, 293, 264, 2141, 307, 291, 483, 646, 1249, 293, 497, 13], "temperature": 0.0, "avg_logprob": -0.18002443313598632, "compression_ratio": 1.5055555555555555, "no_speech_prob": 8.530039849574678e-06}, {"id": 481, "seek": 405000, "start": 4056.0, "end": 4062.0, "text": " You can check. Well so actually householder is described in Trevathan doesn't explicitly calculate Q.", "tokens": [509, 393, 1520, 13, 1042, 370, 767, 9888, 260, 307, 7619, 294, 8648, 85, 9390, 1177, 380, 20803, 8873, 1249, 13], "temperature": 0.0, "avg_logprob": -0.18002443313598632, "compression_ratio": 1.5055555555555555, "no_speech_prob": 8.530039849574678e-06}, {"id": 482, "seek": 405000, "start": 4062.0, "end": 4070.0, "text": " It just calculates R. It turns out for most problems you don't need and you don't actually need Q.", "tokens": [467, 445, 4322, 1024, 497, 13, 467, 4523, 484, 337, 881, 2740, 291, 500, 380, 643, 293, 291, 500, 380, 767, 643, 1249, 13], "temperature": 0.0, "avg_logprob": -0.18002443313598632, "compression_ratio": 1.5055555555555555, "no_speech_prob": 8.530039849574678e-06}, {"id": 483, "seek": 407000, "start": 4070.0, "end": 4088.0, "text": " And there are kind of these implicit methods that can use the householder reflectors to get to get what would Q times Q transpose times B be or Q transpose solve.", "tokens": [400, 456, 366, 733, 295, 613, 26947, 7150, 300, 393, 764, 264, 9888, 260, 5031, 830, 281, 483, 281, 483, 437, 576, 1249, 1413, 1249, 25167, 1413, 363, 312, 420, 1249, 25167, 5039, 13], "temperature": 0.0, "avg_logprob": -0.11902780417936394, "compression_ratio": 1.6283185840707965, "no_speech_prob": 5.1737756621150766e-06}, {"id": 484, "seek": 407000, "start": 4088.0, "end": 4092.0, "text": " Q X equals B. So either like multiply or solve that equation.", "tokens": [1249, 1783, 6915, 363, 13, 407, 2139, 411, 12972, 420, 5039, 300, 5367, 13], "temperature": 0.0, "avg_logprob": -0.11902780417936394, "compression_ratio": 1.6283185840707965, "no_speech_prob": 5.1737756621150766e-06}, {"id": 485, "seek": 407000, "start": 4092.0, "end": 4098.0, "text": " I mean this is something that kind of shows up in numerical linear algebra periodically is that you don't always like need the explicit matrix.", "tokens": [286, 914, 341, 307, 746, 300, 733, 295, 3110, 493, 294, 29054, 8213, 21989, 38916, 307, 300, 291, 500, 380, 1009, 411, 643, 264, 13691, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11902780417936394, "compression_ratio": 1.6283185840707965, "no_speech_prob": 5.1737756621150766e-06}, {"id": 486, "seek": 409800, "start": 4098.0, "end": 4106.0, "text": " You just need to know what is that matrix times a vector or what is the solution of that.", "tokens": [509, 445, 643, 281, 458, 437, 307, 300, 8141, 1413, 257, 8062, 420, 437, 307, 264, 3827, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.07798053820927937, "compression_ratio": 1.1710526315789473, "no_speech_prob": 1.9832230464089662e-05}, {"id": 487, "seek": 410600, "start": 4106.0, "end": 4128.0, "text": " Can I solve a system of equations A X equals B when I know A and B.", "tokens": [1664, 286, 5039, 257, 1185, 295, 11787, 316, 1783, 6915, 363, 562, 286, 458, 316, 293, 363, 13], "temperature": 0.0, "avg_logprob": -0.15712029283696954, "compression_ratio": 0.9852941176470589, "no_speech_prob": 2.902235110013862e-06}, {"id": 488, "seek": 412800, "start": 4128.0, "end": 4136.0, "text": " Yeah so I think I'm going to go on to. Sorry that I'm not getting into the details on these and householders covered in Trevathan.", "tokens": [865, 370, 286, 519, 286, 478, 516, 281, 352, 322, 281, 13, 4919, 300, 286, 478, 406, 1242, 666, 264, 4365, 322, 613, 293, 9888, 433, 5343, 294, 8648, 85, 9390, 13], "temperature": 0.0, "avg_logprob": -0.08798751464256874, "compression_ratio": 1.599236641221374, "no_speech_prob": 5.7716497394721955e-06}, {"id": 489, "seek": 412800, "start": 4136.0, "end": 4142.0, "text": " But it's a less intuitive algorithm than Gram-Schmidt where you're taking these projections.", "tokens": [583, 309, 311, 257, 1570, 21769, 9284, 813, 22130, 12, 31560, 39000, 689, 291, 434, 1940, 613, 32371, 13], "temperature": 0.0, "avg_logprob": -0.08798751464256874, "compression_ratio": 1.599236641221374, "no_speech_prob": 5.7716497394721955e-06}, {"id": 490, "seek": 412800, "start": 4142.0, "end": 4149.0, "text": " And I want to see like I think part of the fun part is kind of seeing how these are different from each other numerically.", "tokens": [400, 286, 528, 281, 536, 411, 286, 519, 644, 295, 264, 1019, 644, 307, 733, 295, 2577, 577, 613, 366, 819, 490, 1184, 661, 7866, 984, 13], "temperature": 0.0, "avg_logprob": -0.08798751464256874, "compression_ratio": 1.599236641221374, "no_speech_prob": 5.7716497394721955e-06}, {"id": 491, "seek": 412800, "start": 4149.0, "end": 4154.0, "text": " And I wanted to definitely make sure we get to this with plenty of time.", "tokens": [400, 286, 1415, 281, 2138, 652, 988, 321, 483, 281, 341, 365, 7140, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.08798751464256874, "compression_ratio": 1.599236641221374, "no_speech_prob": 5.7716497394721955e-06}, {"id": 492, "seek": 415400, "start": 4154.0, "end": 4162.0, "text": " So this is coming from Lecture 9 of Trevathan. It has a few examples of how they vary.", "tokens": [407, 341, 307, 1348, 490, 37196, 540, 1722, 295, 8648, 85, 9390, 13, 467, 575, 257, 1326, 5110, 295, 577, 436, 10559, 13], "temperature": 0.0, "avg_logprob": -0.08838557061694917, "compression_ratio": 1.5491071428571428, "no_speech_prob": 3.844833827315597e-06}, {"id": 493, "seek": 415400, "start": 4162.0, "end": 4168.0, "text": " And so one of these is comparing the classic versus modified Gram-Schmidt.", "tokens": [400, 370, 472, 295, 613, 307, 15763, 264, 7230, 5717, 15873, 22130, 12, 31560, 39000, 13], "temperature": 0.0, "avg_logprob": -0.08838557061694917, "compression_ratio": 1.5491071428571428, "no_speech_prob": 3.844833827315597e-06}, {"id": 494, "seek": 415400, "start": 4168.0, "end": 4182.0, "text": " And so here we're going to kind of intentionally construct a matrix that has singular values spaced by factors of two between two to the negative one and two to the negative n plus one.", "tokens": [400, 370, 510, 321, 434, 516, 281, 733, 295, 22062, 7690, 257, 8141, 300, 575, 20010, 4190, 43766, 538, 6771, 295, 732, 1296, 732, 281, 264, 3671, 472, 293, 732, 281, 264, 3671, 297, 1804, 472, 13], "temperature": 0.0, "avg_logprob": -0.08838557061694917, "compression_ratio": 1.5491071428571428, "no_speech_prob": 3.844833827315597e-06}, {"id": 495, "seek": 418200, "start": 4182.0, "end": 4191.0, "text": " So that means the magnitude of the singular values is really varying logarithmically.", "tokens": [407, 300, 1355, 264, 15668, 295, 264, 20010, 4190, 307, 534, 22984, 41473, 32674, 984, 13], "temperature": 0.0, "avg_logprob": -0.08807633541248462, "compression_ratio": 1.423841059602649, "no_speech_prob": 8.939387953432743e-06}, {"id": 496, "seek": 418200, "start": 4191.0, "end": 4196.0, "text": " And so kind of to make this matrix.", "tokens": [400, 370, 733, 295, 281, 652, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08807633541248462, "compression_ratio": 1.423841059602649, "no_speech_prob": 8.939387953432743e-06}, {"id": 497, "seek": 418200, "start": 4196.0, "end": 4205.0, "text": " We're taking NP dot power of two then to this range from negative one to negative n plus one.", "tokens": [492, 434, 1940, 38611, 5893, 1347, 295, 732, 550, 281, 341, 3613, 490, 3671, 472, 281, 3671, 297, 1804, 472, 13], "temperature": 0.0, "avg_logprob": -0.08807633541248462, "compression_ratio": 1.423841059602649, "no_speech_prob": 8.939387953432743e-06}, {"id": 498, "seek": 420500, "start": 4205.0, "end": 4213.0, "text": " And so we're using the idea of a kind of SVD to construct a matrix that has the singular values we want.", "tokens": [400, 370, 321, 434, 1228, 264, 1558, 295, 257, 733, 295, 31910, 35, 281, 7690, 257, 8141, 300, 575, 264, 20010, 4190, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.09822805268423898, "compression_ratio": 1.5204678362573099, "no_speech_prob": 2.058011887129396e-06}, {"id": 499, "seek": 420500, "start": 4213.0, "end": 4220.0, "text": " So we'll just get kind of two orthonormal matrices for U and V.", "tokens": [407, 321, 603, 445, 483, 733, 295, 732, 420, 11943, 24440, 32284, 337, 624, 293, 691, 13], "temperature": 0.0, "avg_logprob": -0.09822805268423898, "compression_ratio": 1.5204678362573099, "no_speech_prob": 2.058011887129396e-06}, {"id": 500, "seek": 420500, "start": 4220.0, "end": 4226.0, "text": " And then S we're putting what we want along the diagonal and multiplying U times S times V.", "tokens": [400, 550, 318, 321, 434, 3372, 437, 321, 528, 2051, 264, 21539, 293, 30955, 624, 1413, 318, 1413, 691, 13], "temperature": 0.0, "avg_logprob": -0.09822805268423898, "compression_ratio": 1.5204678362573099, "no_speech_prob": 2.058011887129396e-06}, {"id": 501, "seek": 422600, "start": 4226.0, "end": 4236.0, "text": " And that will presumably have this singular value decomposition with the values of S that we've created.", "tokens": [400, 300, 486, 26742, 362, 341, 20010, 2158, 48356, 365, 264, 4190, 295, 318, 300, 321, 600, 2942, 13], "temperature": 0.0, "avg_logprob": -0.0778272106962384, "compression_ratio": 1.4876543209876543, "no_speech_prob": 9.422362836630782e-07}, {"id": 502, "seek": 422600, "start": 4236.0, "end": 4251.0, "text": " I should stop. Are there questions about this way of constructing a matrix to kind of make sure it has the singular values that we want?", "tokens": [286, 820, 1590, 13, 2014, 456, 1651, 466, 341, 636, 295, 39969, 257, 8141, 281, 733, 295, 652, 988, 309, 575, 264, 20010, 4190, 300, 321, 528, 30], "temperature": 0.0, "avg_logprob": -0.0778272106962384, "compression_ratio": 1.4876543209876543, "no_speech_prob": 9.422362836630782e-07}, {"id": 503, "seek": 425100, "start": 4251.0, "end": 4265.0, "text": " And this is the reverse of the SVD is we're coming up with two orthonormal matrices and a diagonal matrices and multiplying them together.", "tokens": [400, 341, 307, 264, 9943, 295, 264, 31910, 35, 307, 321, 434, 1348, 493, 365, 732, 420, 11943, 24440, 32284, 293, 257, 21539, 32284, 293, 30955, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10063552034312281, "compression_ratio": 1.4350649350649352, "no_speech_prob": 6.962065072002588e-06}, {"id": 504, "seek": 425100, "start": 4265.0, "end": 4272.0, "text": " OK, so we create a we do a classical Gram-Schmidt and modified Gram-Schmidt on it.", "tokens": [2264, 11, 370, 321, 1884, 257, 321, 360, 257, 13735, 22130, 12, 31560, 39000, 293, 15873, 22130, 12, 31560, 39000, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.10063552034312281, "compression_ratio": 1.4350649350649352, "no_speech_prob": 6.962065072002588e-06}, {"id": 505, "seek": 427200, "start": 4272.0, "end": 4281.0, "text": " And then actually I should have checked that.", "tokens": [400, 550, 767, 286, 820, 362, 10033, 300, 13], "temperature": 0.0, "avg_logprob": -0.16659673484596046, "compression_ratio": 1.174757281553398, "no_speech_prob": 1.0129445399797987e-05}, {"id": 506, "seek": 427200, "start": 4281.0, "end": 4283.0, "text": " And these might end up I'll see if this works.", "tokens": [400, 613, 1062, 917, 493, 286, 603, 536, 498, 341, 1985, 13], "temperature": 0.0, "avg_logprob": -0.16659673484596046, "compression_ratio": 1.174757281553398, "no_speech_prob": 1.0129445399797987e-05}, {"id": 507, "seek": 427200, "start": 4283.0, "end": 4288.0, "text": " They might be off by a sign.", "tokens": [814, 1062, 312, 766, 538, 257, 1465, 13], "temperature": 0.0, "avg_logprob": -0.16659673484596046, "compression_ratio": 1.174757281553398, "no_speech_prob": 1.0129445399797987e-05}, {"id": 508, "seek": 428800, "start": 4288.0, "end": 4304.0, "text": " You can't have variations with kind of where your negative signs are.", "tokens": [509, 393, 380, 362, 17840, 365, 733, 295, 689, 428, 3671, 7880, 366, 13], "temperature": 0.0, "avg_logprob": -0.109436175402473, "compression_ratio": 1.385185185185185, "no_speech_prob": 2.601570486149285e-06}, {"id": 509, "seek": 428800, "start": 4304.0, "end": 4307.0, "text": " Oh, actually, the point is these are going to not be the same.", "tokens": [876, 11, 767, 11, 264, 935, 307, 613, 366, 516, 281, 406, 312, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.109436175402473, "compression_ratio": 1.385185185185185, "no_speech_prob": 2.601570486149285e-06}, {"id": 510, "seek": 428800, "start": 4307.0, "end": 4311.0, "text": " So ideally, we want these to be similar to each other.", "tokens": [407, 22915, 11, 321, 528, 613, 281, 312, 2531, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.109436175402473, "compression_ratio": 1.385185185185185, "no_speech_prob": 2.601570486149285e-06}, {"id": 511, "seek": 431100, "start": 4311.0, "end": 4322.0, "text": " But it turns out that they're they're not. So we can graph S, which is what we know the true singular values to be.", "tokens": [583, 309, 4523, 484, 300, 436, 434, 436, 434, 406, 13, 407, 321, 393, 4295, 318, 11, 597, 307, 437, 321, 458, 264, 2074, 20010, 4190, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.12152482317639636, "compression_ratio": 1.6829268292682926, "no_speech_prob": 2.260291921629687e-06}, {"id": 512, "seek": 431100, "start": 4322.0, "end": 4338.0, "text": " And those are the red dots. And then we're graphing the diagonal of R and the diagonal of Rm, which is the modified Gram-Schmidt R and the classic Gram-Schmidt.", "tokens": [400, 729, 366, 264, 2182, 15026, 13, 400, 550, 321, 434, 1295, 79, 571, 264, 21539, 295, 497, 293, 264, 21539, 295, 497, 76, 11, 597, 307, 264, 15873, 22130, 12, 31560, 39000, 497, 293, 264, 7230, 22130, 12, 31560, 39000, 13], "temperature": 0.0, "avg_logprob": -0.12152482317639636, "compression_ratio": 1.6829268292682926, "no_speech_prob": 2.260291921629687e-06}, {"id": 513, "seek": 433800, "start": 4338.0, "end": 4350.0, "text": " It was the blue. And so first I want to ask you why why should the diagonals why would we expect those to be similar to the singular values or to be the singular values?", "tokens": [467, 390, 264, 3344, 13, 400, 370, 700, 286, 528, 281, 1029, 291, 983, 983, 820, 264, 17405, 1124, 983, 576, 321, 2066, 729, 281, 312, 2531, 281, 264, 20010, 4190, 420, 281, 312, 264, 20010, 4190, 30], "temperature": 0.0, "avg_logprob": -0.19769985392942266, "compression_ratio": 1.4797297297297298, "no_speech_prob": 1.130024429585319e-05}, {"id": 514, "seek": 433800, "start": 4350.0, "end": 4357.0, "text": " The diagonal of R.", "tokens": [440, 21539, 295, 497, 13], "temperature": 0.0, "avg_logprob": -0.19769985392942266, "compression_ratio": 1.4797297297297298, "no_speech_prob": 1.130024429585319e-05}, {"id": 515, "seek": 433800, "start": 4357.0, "end": 4361.0, "text": " Oh, wait, grab the microphone.", "tokens": [876, 11, 1699, 11, 4444, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.19769985392942266, "compression_ratio": 1.4797297297297298, "no_speech_prob": 1.130024429585319e-05}, {"id": 516, "seek": 436100, "start": 4361.0, "end": 4368.0, "text": " Because of the relationship between the eigenvalues and the singular values.", "tokens": [1436, 295, 264, 2480, 1296, 264, 10446, 46033, 293, 264, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.21026666702762728, "compression_ratio": 1.883495145631068, "no_speech_prob": 5.337842594599351e-06}, {"id": 517, "seek": 436100, "start": 4368.0, "end": 4375.0, "text": " So the yes. Well, there's another piece that I think you're probably thinking. And why are you talking about eigenvalues?", "tokens": [407, 264, 2086, 13, 1042, 11, 456, 311, 1071, 2522, 300, 286, 519, 291, 434, 1391, 1953, 13, 400, 983, 366, 291, 1417, 466, 10446, 46033, 30], "temperature": 0.0, "avg_logprob": -0.21026666702762728, "compression_ratio": 1.883495145631068, "no_speech_prob": 5.337842594599351e-06}, {"id": 518, "seek": 436100, "start": 4375.0, "end": 4377.0, "text": " I can talk to the microphone.", "tokens": [286, 393, 751, 281, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.21026666702762728, "compression_ratio": 1.883495145631068, "no_speech_prob": 5.337842594599351e-06}, {"id": 519, "seek": 436100, "start": 4377.0, "end": 4379.0, "text": " Should they be related to the singular values?", "tokens": [6454, 436, 312, 4077, 281, 264, 20010, 4190, 30], "temperature": 0.0, "avg_logprob": -0.21026666702762728, "compression_ratio": 1.883495145631068, "no_speech_prob": 5.337842594599351e-06}, {"id": 520, "seek": 436100, "start": 4379.0, "end": 4387.0, "text": " Right. So the eigenvalues are related to the singular values. And then how is that related to the diagonal of R?", "tokens": [1779, 13, 407, 264, 10446, 46033, 366, 4077, 281, 264, 20010, 4190, 13, 400, 550, 577, 307, 300, 4077, 281, 264, 21539, 295, 497, 30], "temperature": 0.0, "avg_logprob": -0.21026666702762728, "compression_ratio": 1.883495145631068, "no_speech_prob": 5.337842594599351e-06}, {"id": 521, "seek": 438700, "start": 4387.0, "end": 4391.0, "text": " Linda, do you want to say it?", "tokens": [20324, 11, 360, 291, 528, 281, 584, 309, 30], "temperature": 0.0, "avg_logprob": -0.23197389685589334, "compression_ratio": 1.5677419354838709, "no_speech_prob": 1.2606565178430174e-05}, {"id": 522, "seek": 438700, "start": 4391.0, "end": 4392.0, "text": " It's a triangle.", "tokens": [467, 311, 257, 13369, 13], "temperature": 0.0, "avg_logprob": -0.23197389685589334, "compression_ratio": 1.5677419354838709, "no_speech_prob": 1.2606565178430174e-05}, {"id": 523, "seek": 438700, "start": 4392.0, "end": 4402.0, "text": " Yeah, since it's a triangular matrix, the diagonal. Yeah, it gives you the eigenvalues, which are the singular values in this case.", "tokens": [865, 11, 1670, 309, 311, 257, 38190, 8141, 11, 264, 21539, 13, 865, 11, 309, 2709, 291, 264, 10446, 46033, 11, 597, 366, 264, 20010, 4190, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.23197389685589334, "compression_ratio": 1.5677419354838709, "no_speech_prob": 1.2606565178430174e-05}, {"id": 524, "seek": 438700, "start": 4402.0, "end": 4410.0, "text": " Is it singular value also eigenvalues? So how do they relate it?", "tokens": [1119, 309, 20010, 2158, 611, 10446, 46033, 30, 407, 577, 360, 436, 10961, 309, 30], "temperature": 0.0, "avg_logprob": -0.23197389685589334, "compression_ratio": 1.5677419354838709, "no_speech_prob": 1.2606565178430174e-05}, {"id": 525, "seek": 441000, "start": 4410.0, "end": 4418.0, "text": " So singular values. Yeah. So I think the values are like a generalization of eigenvalues.", "tokens": [407, 20010, 4190, 13, 865, 13, 407, 286, 519, 264, 4190, 366, 411, 257, 2674, 2144, 295, 10446, 46033, 13], "temperature": 0.0, "avg_logprob": -0.24591286237849747, "compression_ratio": 1.238532110091743, "no_speech_prob": 5.093515483167721e-06}, {"id": 526, "seek": 441000, "start": 4418.0, "end": 4422.0, "text": " And the idea, let me go back to the.", "tokens": [400, 264, 1558, 11, 718, 385, 352, 646, 281, 264, 13], "temperature": 0.0, "avg_logprob": -0.24591286237849747, "compression_ratio": 1.238532110091743, "no_speech_prob": 5.093515483167721e-06}, {"id": 527, "seek": 441000, "start": 4422.0, "end": 4432.0, "text": " Notepad.", "tokens": [1726, 595, 345, 13], "temperature": 0.0, "avg_logprob": -0.24591286237849747, "compression_ratio": 1.238532110091743, "no_speech_prob": 5.093515483167721e-06}, {"id": 528, "seek": 443200, "start": 4432.0, "end": 4440.0, "text": " Is that singular values we can think of as.", "tokens": [1119, 300, 20010, 4190, 321, 393, 519, 295, 382, 13], "temperature": 0.0, "avg_logprob": -0.1682342529296875, "compression_ratio": 1.2727272727272727, "no_speech_prob": 1.8631382772582583e-05}, {"id": 529, "seek": 443200, "start": 4440.0, "end": 4443.0, "text": " A times.", "tokens": [316, 1413, 13], "temperature": 0.0, "avg_logprob": -0.1682342529296875, "compression_ratio": 1.2727272727272727, "no_speech_prob": 1.8631382772582583e-05}, {"id": 530, "seek": 443200, "start": 4443.0, "end": 4446.0, "text": " V equals.", "tokens": [691, 6915, 13], "temperature": 0.0, "avg_logprob": -0.1682342529296875, "compression_ratio": 1.2727272727272727, "no_speech_prob": 1.8631382772582583e-05}, {"id": 531, "seek": 443200, "start": 4446.0, "end": 4448.0, "text": " Sigma times U.", "tokens": [36595, 1413, 624, 13], "temperature": 0.0, "avg_logprob": -0.1682342529296875, "compression_ratio": 1.2727272727272727, "no_speech_prob": 1.8631382772582583e-05}, {"id": 532, "seek": 443200, "start": 4448.0, "end": 4454.0, "text": " And then eigenvalues can be thought of as AX equals.", "tokens": [400, 550, 10446, 46033, 393, 312, 1194, 295, 382, 316, 55, 6915, 13], "temperature": 0.0, "avg_logprob": -0.1682342529296875, "compression_ratio": 1.2727272727272727, "no_speech_prob": 1.8631382772582583e-05}, {"id": 533, "seek": 443200, "start": 4454.0, "end": 4457.0, "text": " Lambda X.", "tokens": [45691, 1783, 13], "temperature": 0.0, "avg_logprob": -0.1682342529296875, "compression_ratio": 1.2727272727272727, "no_speech_prob": 1.8631382772582583e-05}, {"id": 534, "seek": 445700, "start": 4457.0, "end": 4463.0, "text": " And so notice that this first equation, typically like we're talking about the SVD, we're writing out the full matrices.", "tokens": [400, 370, 3449, 300, 341, 700, 5367, 11, 5850, 411, 321, 434, 1417, 466, 264, 31910, 35, 11, 321, 434, 3579, 484, 264, 1577, 32284, 13], "temperature": 0.0, "avg_logprob": -0.13230282730526394, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.202905968966661e-05}, {"id": 535, "seek": 445700, "start": 4463.0, "end": 4468.0, "text": " This is just thinking about particular vectors and they're.", "tokens": [639, 307, 445, 1953, 466, 1729, 18875, 293, 436, 434, 13], "temperature": 0.0, "avg_logprob": -0.13230282730526394, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.202905968966661e-05}, {"id": 536, "seek": 445700, "start": 4468.0, "end": 4477.0, "text": " You know, like one kind of one vector at a time or one pair of vectors being U and one singular value from the diagonal.", "tokens": [509, 458, 11, 411, 472, 733, 295, 472, 8062, 412, 257, 565, 420, 472, 6119, 295, 18875, 885, 624, 293, 472, 20010, 2158, 490, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.13230282730526394, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.202905968966661e-05}, {"id": 537, "seek": 447700, "start": 4477.0, "end": 4488.0, "text": " So this is, you know, coming from A equals. U sigma V.", "tokens": [407, 341, 307, 11, 291, 458, 11, 1348, 490, 316, 6915, 13, 624, 12771, 691, 13], "temperature": 0.0, "avg_logprob": -0.16132588187853494, "compression_ratio": 1.2388059701492538, "no_speech_prob": 3.1380916425405303e-06}, {"id": 538, "seek": 447700, "start": 4488.0, "end": 4493.0, "text": " And we would have.", "tokens": [400, 321, 576, 362, 13], "temperature": 0.0, "avg_logprob": -0.16132588187853494, "compression_ratio": 1.2388059701492538, "no_speech_prob": 3.1380916425405303e-06}, {"id": 539, "seek": 447700, "start": 4493.0, "end": 4504.0, "text": " A number of these for different singular values. Kelsey, can you pass the microphone, Linda?", "tokens": [316, 1230, 295, 613, 337, 819, 20010, 4190, 13, 44714, 11, 393, 291, 1320, 264, 10952, 11, 20324, 30], "temperature": 0.0, "avg_logprob": -0.16132588187853494, "compression_ratio": 1.2388059701492538, "no_speech_prob": 3.1380916425405303e-06}, {"id": 540, "seek": 450400, "start": 4504.0, "end": 4508.0, "text": " Is this new reality kind of like this?", "tokens": [1119, 341, 777, 4103, 733, 295, 411, 341, 30], "temperature": 0.0, "avg_logprob": -0.27498276233673097, "compression_ratio": 1.4277777777777778, "no_speech_prob": 7.527622074121609e-06}, {"id": 541, "seek": 450400, "start": 4508.0, "end": 4511.0, "text": " It is, yeah.", "tokens": [467, 307, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.27498276233673097, "compression_ratio": 1.4277777777777778, "no_speech_prob": 7.527622074121609e-06}, {"id": 542, "seek": 450400, "start": 4511.0, "end": 4514.0, "text": " The stretching that you're doing.", "tokens": [440, 19632, 300, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.27498276233673097, "compression_ratio": 1.4277777777777778, "no_speech_prob": 7.527622074121609e-06}, {"id": 543, "seek": 450400, "start": 4514.0, "end": 4518.0, "text": " Like a version and like elongating or.", "tokens": [1743, 257, 3037, 293, 411, 40786, 990, 420, 13], "temperature": 0.0, "avg_logprob": -0.27498276233673097, "compression_ratio": 1.4277777777777778, "no_speech_prob": 7.527622074121609e-06}, {"id": 544, "seek": 450400, "start": 4518.0, "end": 4523.0, "text": " Yes, yes, exactly. Yeah, that's a great.", "tokens": [1079, 11, 2086, 11, 2293, 13, 865, 11, 300, 311, 257, 869, 13], "temperature": 0.0, "avg_logprob": -0.27498276233673097, "compression_ratio": 1.4277777777777778, "no_speech_prob": 7.527622074121609e-06}, {"id": 545, "seek": 450400, "start": 4523.0, "end": 4528.0, "text": " Great thing to note. So it's kind of with singular values.", "tokens": [3769, 551, 281, 3637, 13, 407, 309, 311, 733, 295, 365, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.27498276233673097, "compression_ratio": 1.4277777777777778, "no_speech_prob": 7.527622074121609e-06}, {"id": 546, "seek": 450400, "start": 4528.0, "end": 4533.0, "text": " Often you'll see these pictures.", "tokens": [20043, 291, 603, 536, 613, 5242, 13], "temperature": 0.0, "avg_logprob": -0.27498276233673097, "compression_ratio": 1.4277777777777778, "no_speech_prob": 7.527622074121609e-06}, {"id": 547, "seek": 453300, "start": 4533.0, "end": 4536.0, "text": " Maybe you're taking.", "tokens": [2704, 291, 434, 1940, 13], "temperature": 0.0, "avg_logprob": -0.1893229257492792, "compression_ratio": 1.2142857142857142, "no_speech_prob": 1.473836528020911e-05}, {"id": 548, "seek": 453300, "start": 4536.0, "end": 4545.0, "text": " A circle and perhaps A transforms it to an ellipse.", "tokens": [316, 6329, 293, 4317, 316, 35592, 309, 281, 364, 8284, 48041, 13], "temperature": 0.0, "avg_logprob": -0.1893229257492792, "compression_ratio": 1.2142857142857142, "no_speech_prob": 1.473836528020911e-05}, {"id": 549, "seek": 453300, "start": 4545.0, "end": 4552.0, "text": " Oops.", "tokens": [21726, 13], "temperature": 0.0, "avg_logprob": -0.1893229257492792, "compression_ratio": 1.2142857142857142, "no_speech_prob": 1.473836528020911e-05}, {"id": 550, "seek": 453300, "start": 4552.0, "end": 4559.0, "text": " So if matrix A changes the unit circle into this ellipse.", "tokens": [407, 498, 8141, 316, 2962, 264, 4985, 6329, 666, 341, 8284, 48041, 13], "temperature": 0.0, "avg_logprob": -0.1893229257492792, "compression_ratio": 1.2142857142857142, "no_speech_prob": 1.473836528020911e-05}, {"id": 551, "seek": 455900, "start": 4559.0, "end": 4563.0, "text": " The.", "tokens": [440, 13], "temperature": 0.0, "avg_logprob": -0.1633883325677169, "compression_ratio": 1.5601851851851851, "no_speech_prob": 6.962009138078429e-06}, {"id": 552, "seek": 455900, "start": 4563.0, "end": 4565.0, "text": " You're getting the.", "tokens": [509, 434, 1242, 264, 13], "temperature": 0.0, "avg_logprob": -0.1633883325677169, "compression_ratio": 1.5601851851851851, "no_speech_prob": 6.962009138078429e-06}, {"id": 553, "seek": 455900, "start": 4565.0, "end": 4569.0, "text": " The vectors, U and V are basically like a change of basis.", "tokens": [440, 18875, 11, 624, 293, 691, 366, 1936, 411, 257, 1319, 295, 5143, 13], "temperature": 0.0, "avg_logprob": -0.1633883325677169, "compression_ratio": 1.5601851851851851, "no_speech_prob": 6.962009138078429e-06}, {"id": 554, "seek": 455900, "start": 4569.0, "end": 4576.0, "text": " And so you can think of kind of this is really relates back to the three blue one brown video that we're kind of interested in.", "tokens": [400, 370, 291, 393, 519, 295, 733, 295, 341, 307, 534, 16155, 646, 281, 264, 1045, 3344, 472, 6292, 960, 300, 321, 434, 733, 295, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.1633883325677169, "compression_ratio": 1.5601851851851851, "no_speech_prob": 6.962009138078429e-06}, {"id": 555, "seek": 455900, "start": 4576.0, "end": 4580.0, "text": " Like what's the I don't know, like more natural basis for this ellipse.", "tokens": [1743, 437, 311, 264, 286, 500, 380, 458, 11, 411, 544, 3303, 5143, 337, 341, 8284, 48041, 13], "temperature": 0.0, "avg_logprob": -0.1633883325677169, "compression_ratio": 1.5601851851851851, "no_speech_prob": 6.962009138078429e-06}, {"id": 556, "seek": 455900, "start": 4580.0, "end": 4583.0, "text": " And that would be like, oh, that's a different color.", "tokens": [400, 300, 576, 312, 411, 11, 1954, 11, 300, 311, 257, 819, 2017, 13], "temperature": 0.0, "avg_logprob": -0.1633883325677169, "compression_ratio": 1.5601851851851851, "no_speech_prob": 6.962009138078429e-06}, {"id": 557, "seek": 458300, "start": 4583.0, "end": 4589.0, "text": " You know, like really like these are the axes of that.", "tokens": [509, 458, 11, 411, 534, 411, 613, 366, 264, 35387, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.1312807256525213, "compression_ratio": 1.515527950310559, "no_speech_prob": 2.0580048385454575e-06}, {"id": 558, "seek": 458300, "start": 4589.0, "end": 4598.0, "text": " And so those vectors kind of the directions are coming from your V and the magnitude is coming from Sigma.", "tokens": [400, 370, 729, 18875, 733, 295, 264, 11095, 366, 1348, 490, 428, 691, 293, 264, 15668, 307, 1348, 490, 36595, 13], "temperature": 0.0, "avg_logprob": -0.1312807256525213, "compression_ratio": 1.515527950310559, "no_speech_prob": 2.0580048385454575e-06}, {"id": 559, "seek": 458300, "start": 4598.0, "end": 4603.0, "text": " As Kelsey said.", "tokens": [1018, 44714, 848, 13], "temperature": 0.0, "avg_logprob": -0.1312807256525213, "compression_ratio": 1.515527950310559, "no_speech_prob": 2.0580048385454575e-06}, {"id": 560, "seek": 458300, "start": 4603.0, "end": 4610.0, "text": " And here really, that's like, you know, what was just a unit axis.", "tokens": [400, 510, 534, 11, 300, 311, 411, 11, 291, 458, 11, 437, 390, 445, 257, 4985, 10298, 13], "temperature": 0.0, "avg_logprob": -0.1312807256525213, "compression_ratio": 1.515527950310559, "no_speech_prob": 2.0580048385454575e-06}, {"id": 561, "seek": 461000, "start": 4610.0, "end": 4613.0, "text": " And so that's basically that looks familiar in thinking about change of basis.", "tokens": [400, 370, 300, 311, 1936, 300, 1542, 4963, 294, 1953, 466, 1319, 295, 5143, 13], "temperature": 0.0, "avg_logprob": -0.23248914770177892, "compression_ratio": 1.6201117318435754, "no_speech_prob": 1.933325847858214e-06}, {"id": 562, "seek": 461000, "start": 4613.0, "end": 4622.0, "text": " And so singular value decomposition is really a change of basis.", "tokens": [400, 370, 20010, 2158, 48356, 307, 534, 257, 1319, 295, 5143, 13], "temperature": 0.0, "avg_logprob": -0.23248914770177892, "compression_ratio": 1.6201117318435754, "no_speech_prob": 1.933325847858214e-06}, {"id": 563, "seek": 461000, "start": 4622.0, "end": 4625.0, "text": " And then kind of going back to.", "tokens": [400, 550, 733, 295, 516, 646, 281, 13], "temperature": 0.0, "avg_logprob": -0.23248914770177892, "compression_ratio": 1.6201117318435754, "no_speech_prob": 1.933325847858214e-06}, {"id": 564, "seek": 461000, "start": 4625.0, "end": 4629.0, "text": " Actually, first I should ask, are there questions about that?", "tokens": [5135, 11, 700, 286, 820, 1029, 11, 366, 456, 1651, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.23248914770177892, "compression_ratio": 1.6201117318435754, "no_speech_prob": 1.933325847858214e-06}, {"id": 565, "seek": 461000, "start": 4629.0, "end": 4639.0, "text": " This idea of SVD being a bit like a change of basis.", "tokens": [639, 1558, 295, 31910, 35, 885, 257, 857, 411, 257, 1319, 295, 5143, 13], "temperature": 0.0, "avg_logprob": -0.23248914770177892, "compression_ratio": 1.6201117318435754, "no_speech_prob": 1.933325847858214e-06}, {"id": 566, "seek": 463900, "start": 4639.0, "end": 4643.0, "text": " Sam, pass the microphone back.", "tokens": [4832, 11, 1320, 264, 10952, 646, 13], "temperature": 0.0, "avg_logprob": -0.22911899968197472, "compression_ratio": 1.5136986301369864, "no_speech_prob": 2.3185213649412617e-05}, {"id": 567, "seek": 463900, "start": 4643.0, "end": 4658.0, "text": " So would you be would have would you have a magnitude of one in the basis provided by a like in this case, you're stretching it.", "tokens": [407, 576, 291, 312, 576, 362, 576, 291, 362, 257, 15668, 295, 472, 294, 264, 5143, 5649, 538, 257, 411, 294, 341, 1389, 11, 291, 434, 19632, 309, 13], "temperature": 0.0, "avg_logprob": -0.22911899968197472, "compression_ratio": 1.5136986301369864, "no_speech_prob": 2.3185213649412617e-05}, {"id": 568, "seek": 463900, "start": 4658.0, "end": 4667.0, "text": " So the elongation would be with respect to the original axes.", "tokens": [407, 264, 40786, 399, 576, 312, 365, 3104, 281, 264, 3380, 35387, 13], "temperature": 0.0, "avg_logprob": -0.22911899968197472, "compression_ratio": 1.5136986301369864, "no_speech_prob": 2.3185213649412617e-05}, {"id": 569, "seek": 466700, "start": 4667.0, "end": 4675.0, "text": " So I think so the elongation is captured by the Sigma so that you.", "tokens": [407, 286, 519, 370, 264, 40786, 399, 307, 11828, 538, 264, 36595, 370, 300, 291, 13], "temperature": 0.0, "avg_logprob": -0.19466375112533568, "compression_ratio": 1.2818181818181817, "no_speech_prob": 2.2825381165603176e-05}, {"id": 570, "seek": 466700, "start": 4675.0, "end": 4681.0, "text": " So I guess my question is, would you have a magnitude of one in this case?", "tokens": [407, 286, 2041, 452, 1168, 307, 11, 576, 291, 362, 257, 15668, 295, 472, 294, 341, 1389, 30], "temperature": 0.0, "avg_logprob": -0.19466375112533568, "compression_ratio": 1.2818181818181817, "no_speech_prob": 2.2825381165603176e-05}, {"id": 571, "seek": 468100, "start": 4681.0, "end": 4697.0, "text": " So you you does have a magnitude of one. Yes.", "tokens": [407, 291, 291, 775, 362, 257, 15668, 295, 472, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.15181955214469664, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.29607927496545e-06}, {"id": 572, "seek": 468100, "start": 4697.0, "end": 4701.0, "text": " And so looking at kind of the eigen eigenvalues, eigenvectors.", "tokens": [400, 370, 1237, 412, 733, 295, 264, 10446, 10446, 46033, 11, 10446, 303, 5547, 13], "temperature": 0.0, "avg_logprob": -0.15181955214469664, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.29607927496545e-06}, {"id": 573, "seek": 468100, "start": 4701.0, "end": 4703.0, "text": " So here this is a very similar equation.", "tokens": [407, 510, 341, 307, 257, 588, 2531, 5367, 13], "temperature": 0.0, "avg_logprob": -0.15181955214469664, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.29607927496545e-06}, {"id": 574, "seek": 468100, "start": 4703.0, "end": 4708.0, "text": " And the difference is just that you you don't have separate you and V.", "tokens": [400, 264, 2649, 307, 445, 300, 291, 291, 500, 380, 362, 4994, 291, 293, 691, 13], "temperature": 0.0, "avg_logprob": -0.15181955214469664, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.29607927496545e-06}, {"id": 575, "seek": 470800, "start": 4708.0, "end": 4713.0, "text": " You just have X.", "tokens": [509, 445, 362, 1783, 13], "temperature": 0.0, "avg_logprob": -0.18352365493774414, "compression_ratio": 0.896551724137931, "no_speech_prob": 2.66867209575139e-05}, {"id": 576, "seek": 470800, "start": 4713.0, "end": 4722.0, "text": " And so this.", "tokens": [400, 370, 341, 13], "temperature": 0.0, "avg_logprob": -0.18352365493774414, "compression_ratio": 0.896551724137931, "no_speech_prob": 2.66867209575139e-05}, {"id": 577, "seek": 470800, "start": 4722.0, "end": 4730.0, "text": " This could be written.", "tokens": [639, 727, 312, 3720, 13], "temperature": 0.0, "avg_logprob": -0.18352365493774414, "compression_ratio": 0.896551724137931, "no_speech_prob": 2.66867209575139e-05}, {"id": 578, "seek": 473000, "start": 4730.0, "end": 4739.0, "text": " Why was I able to change the order of Sigma and X?", "tokens": [1545, 390, 286, 1075, 281, 1319, 264, 1668, 295, 36595, 293, 1783, 30], "temperature": 0.0, "avg_logprob": -0.1845920979976654, "compression_ratio": 1.4203821656050954, "no_speech_prob": 1.5935591363813728e-05}, {"id": 579, "seek": 473000, "start": 4739.0, "end": 4745.0, "text": " When I went to the the matrix form.", "tokens": [1133, 286, 1437, 281, 264, 264, 8141, 1254, 13], "temperature": 0.0, "avg_logprob": -0.1845920979976654, "compression_ratio": 1.4203821656050954, "no_speech_prob": 1.5935591363813728e-05}, {"id": 580, "seek": 473000, "start": 4745.0, "end": 4748.0, "text": " Yeah, because Sigma is just a scalar.", "tokens": [865, 11, 570, 36595, 307, 445, 257, 39684, 13], "temperature": 0.0, "avg_logprob": -0.1845920979976654, "compression_ratio": 1.4203821656050954, "no_speech_prob": 1.5935591363813728e-05}, {"id": 581, "seek": 473000, "start": 4748.0, "end": 4754.0, "text": " So you're and you're just putting those on the diagonal of your your big Sigma or so I big Lambda.", "tokens": [407, 291, 434, 293, 291, 434, 445, 3372, 729, 322, 264, 21539, 295, 428, 428, 955, 36595, 420, 370, 286, 955, 45691, 13], "temperature": 0.0, "avg_logprob": -0.1845920979976654, "compression_ratio": 1.4203821656050954, "no_speech_prob": 1.5935591363813728e-05}, {"id": 582, "seek": 475400, "start": 4754.0, "end": 4762.0, "text": " And so then this is.", "tokens": [400, 370, 550, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.11602686882019043, "compression_ratio": 1.375, "no_speech_prob": 5.422135927801719e-06}, {"id": 583, "seek": 475400, "start": 4762.0, "end": 4769.0, "text": " This should look familiar of kind of this is what it means for a Lambda to be similar.", "tokens": [639, 820, 574, 4963, 295, 733, 295, 341, 307, 437, 309, 1355, 337, 257, 45691, 281, 312, 2531, 13], "temperature": 0.0, "avg_logprob": -0.11602686882019043, "compression_ratio": 1.375, "no_speech_prob": 5.422135927801719e-06}, {"id": 584, "seek": 475400, "start": 4769.0, "end": 4783.0, "text": " We have this change of basis, and that's what the eigen decomposition is doing.", "tokens": [492, 362, 341, 1319, 295, 5143, 11, 293, 300, 311, 437, 264, 10446, 48356, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.11602686882019043, "compression_ratio": 1.375, "no_speech_prob": 5.422135927801719e-06}, {"id": 585, "seek": 478300, "start": 4783.0, "end": 4794.0, "text": " Do all do all matrices have a full eigen decomposition?", "tokens": [1144, 439, 360, 439, 32284, 362, 257, 1577, 10446, 48356, 30], "temperature": 0.0, "avg_logprob": -0.14524208415638318, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.5206265743472613e-05}, {"id": 586, "seek": 478300, "start": 4794.0, "end": 4798.0, "text": " Thumbs up for yes, thumbs down for no.", "tokens": [334, 7808, 493, 337, 2086, 11, 8838, 760, 337, 572, 13], "temperature": 0.0, "avg_logprob": -0.14524208415638318, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.5206265743472613e-05}, {"id": 587, "seek": 478300, "start": 4798.0, "end": 4800.0, "text": " OK, good. I see mostly thumbs down. Yeah.", "tokens": [2264, 11, 665, 13, 286, 536, 5240, 8838, 760, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.14524208415638318, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.5206265743472613e-05}, {"id": 588, "seek": 478300, "start": 4800.0, "end": 4804.0, "text": " So not all not all matrices have a full eigen decomposition.", "tokens": [407, 406, 439, 406, 439, 32284, 362, 257, 1577, 10446, 48356, 13], "temperature": 0.0, "avg_logprob": -0.14524208415638318, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.5206265743472613e-05}, {"id": 589, "seek": 478300, "start": 4804.0, "end": 4811.0, "text": " A full eigen decomposition is where you can get a matrix X that's orthonormal or.", "tokens": [316, 1577, 10446, 48356, 307, 689, 291, 393, 483, 257, 8141, 1783, 300, 311, 420, 11943, 24440, 420, 13], "temperature": 0.0, "avg_logprob": -0.14524208415638318, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.5206265743472613e-05}, {"id": 590, "seek": 481100, "start": 4811.0, "end": 4820.0, "text": " A matrix X of full rank and then but all all matrices have a singular value decomposition, so it's kind of more general.", "tokens": [316, 8141, 1783, 295, 1577, 6181, 293, 550, 457, 439, 439, 32284, 362, 257, 20010, 2158, 48356, 11, 370, 309, 311, 733, 295, 544, 2674, 13], "temperature": 0.0, "avg_logprob": -0.136331610483666, "compression_ratio": 1.5026455026455026, "no_speech_prob": 3.7852530567761278e-06}, {"id": 591, "seek": 481100, "start": 4820.0, "end": 4830.0, "text": " I mean, it doesn't have this additional constraint on it.", "tokens": [286, 914, 11, 309, 1177, 380, 362, 341, 4497, 25534, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.136331610483666, "compression_ratio": 1.5026455026455026, "no_speech_prob": 3.7852530567761278e-06}, {"id": 592, "seek": 481100, "start": 4830.0, "end": 4833.0, "text": " So kind of going back to this.", "tokens": [407, 733, 295, 516, 646, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.136331610483666, "compression_ratio": 1.5026455026455026, "no_speech_prob": 3.7852530567761278e-06}, {"id": 593, "seek": 481100, "start": 4833.0, "end": 4838.0, "text": " This comparison of you know, we're trying to get the true singular values.", "tokens": [639, 9660, 295, 291, 458, 11, 321, 434, 1382, 281, 483, 264, 2074, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.136331610483666, "compression_ratio": 1.5026455026455026, "no_speech_prob": 3.7852530567761278e-06}, {"id": 594, "seek": 483800, "start": 4838.0, "end": 4844.0, "text": " We know what they're supposed to be. We look at the diagonal and we're definitely we're right up here.", "tokens": [492, 458, 437, 436, 434, 3442, 281, 312, 13, 492, 574, 412, 264, 21539, 293, 321, 434, 2138, 321, 434, 558, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.11771902171048251, "compression_ratio": 1.4378698224852071, "no_speech_prob": 5.7716938499652315e-06}, {"id": 595, "seek": 483800, "start": 4844.0, "end": 4851.0, "text": " Like it seems, you know, both classic and modified did well.", "tokens": [1743, 309, 2544, 11, 291, 458, 11, 1293, 7230, 293, 15873, 630, 731, 13], "temperature": 0.0, "avg_logprob": -0.11771902171048251, "compression_ratio": 1.4378698224852071, "no_speech_prob": 5.7716938499652315e-06}, {"id": 596, "seek": 483800, "start": 4851.0, "end": 4854.0, "text": " Does anyone want to guess what?", "tokens": [4402, 2878, 528, 281, 2041, 437, 30], "temperature": 0.0, "avg_logprob": -0.11771902171048251, "compression_ratio": 1.4378698224852071, "no_speech_prob": 5.7716938499652315e-06}, {"id": 597, "seek": 483800, "start": 4854.0, "end": 4867.0, "text": " One of these places where it's leveling off is.", "tokens": [1485, 295, 613, 3190, 689, 309, 311, 40617, 766, 307, 13], "temperature": 0.0, "avg_logprob": -0.11771902171048251, "compression_ratio": 1.4378698224852071, "no_speech_prob": 5.7716938499652315e-06}, {"id": 598, "seek": 486700, "start": 4867.0, "end": 4882.0, "text": " This is a concept or term that we talked about in lesson one.", "tokens": [639, 307, 257, 3410, 420, 1433, 300, 321, 2825, 466, 294, 6898, 472, 13], "temperature": 0.0, "avg_logprob": -0.1854433724374482, "compression_ratio": 1.108910891089109, "no_speech_prob": 3.905366156686796e-06}, {"id": 599, "seek": 486700, "start": 4882.0, "end": 4888.0, "text": " Any guesses?", "tokens": [2639, 42703, 30], "temperature": 0.0, "avg_logprob": -0.1854433724374482, "compression_ratio": 1.108910891089109, "no_speech_prob": 3.905366156686796e-06}, {"id": 600, "seek": 486700, "start": 4888.0, "end": 4893.0, "text": " Kelsey, grab the microphone from Sam.", "tokens": [44714, 11, 4444, 264, 10952, 490, 4832, 13], "temperature": 0.0, "avg_logprob": -0.1854433724374482, "compression_ratio": 1.108910891089109, "no_speech_prob": 3.905366156686796e-06}, {"id": 601, "seek": 489300, "start": 4893.0, "end": 4901.0, "text": " Yes, machine epsilon. So this is the and in particular that's for modified Gram-Schmidt.", "tokens": [1079, 11, 3479, 17889, 13, 407, 341, 307, 264, 293, 294, 1729, 300, 311, 337, 15873, 22130, 12, 31560, 39000, 13], "temperature": 0.0, "avg_logprob": -0.3202703916109525, "compression_ratio": 1.2818791946308725, "no_speech_prob": 3.42656894645188e-05}, {"id": 602, "seek": 489300, "start": 4901.0, "end": 4914.0, "text": " It can't capture something that small. Tim.", "tokens": [467, 393, 380, 7983, 746, 300, 1359, 13, 7172, 13], "temperature": 0.0, "avg_logprob": -0.3202703916109525, "compression_ratio": 1.2818791946308725, "no_speech_prob": 3.42656894645188e-05}, {"id": 603, "seek": 489300, "start": 4914.0, "end": 4920.0, "text": " Why does it look like it levels around to the negative 57?", "tokens": [1545, 775, 309, 574, 411, 309, 4358, 926, 281, 264, 3671, 21423, 30], "temperature": 0.0, "avg_logprob": -0.3202703916109525, "compression_ratio": 1.2818791946308725, "no_speech_prob": 3.42656894645188e-05}, {"id": 604, "seek": 492000, "start": 4920.0, "end": 4924.0, "text": " Yeah, I am just the way the axes ended up.", "tokens": [865, 11, 286, 669, 445, 264, 636, 264, 35387, 4590, 493, 13], "temperature": 0.0, "avg_logprob": -0.17284696242388556, "compression_ratio": 1.4612068965517242, "no_speech_prob": 1.863019679149147e-05}, {"id": 605, "seek": 492000, "start": 4924.0, "end": 4929.0, "text": " I'm not fully sure about that. That was like close enough that I was like this seems right on.", "tokens": [286, 478, 406, 4498, 988, 466, 300, 13, 663, 390, 411, 1998, 1547, 300, 286, 390, 411, 341, 2544, 558, 322, 13], "temperature": 0.0, "avg_logprob": -0.17284696242388556, "compression_ratio": 1.4612068965517242, "no_speech_prob": 1.863019679149147e-05}, {"id": 606, "seek": 492000, "start": 4929.0, "end": 4932.0, "text": " But yeah, I am. I don't know why it's not.", "tokens": [583, 1338, 11, 286, 669, 13, 286, 500, 380, 458, 983, 309, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.17284696242388556, "compression_ratio": 1.4612068965517242, "no_speech_prob": 1.863019679149147e-05}, {"id": 607, "seek": 492000, "start": 4932.0, "end": 4936.0, "text": " Not perfect. Good question.", "tokens": [1726, 2176, 13, 2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.17284696242388556, "compression_ratio": 1.4612068965517242, "no_speech_prob": 1.863019679149147e-05}, {"id": 608, "seek": 492000, "start": 4936.0, "end": 4947.0, "text": " Yes, I've written down here. So there's a way from NumPy dot F info to find out what NumPy saying machine epsilon is for float 64.", "tokens": [1079, 11, 286, 600, 3720, 760, 510, 13, 407, 456, 311, 257, 636, 490, 22592, 47, 88, 5893, 479, 13614, 281, 915, 484, 437, 22592, 47, 88, 1566, 3479, 17889, 307, 337, 15706, 12145, 13], "temperature": 0.0, "avg_logprob": -0.17284696242388556, "compression_ratio": 1.4612068965517242, "no_speech_prob": 1.863019679149147e-05}, {"id": 609, "seek": 494700, "start": 4947.0, "end": 4950.0, "text": " And so.", "tokens": [400, 370, 13], "temperature": 0.0, "avg_logprob": -0.11161260043873507, "compression_ratio": 1.5638297872340425, "no_speech_prob": 1.061539751390228e-05}, {"id": 610, "seek": 494700, "start": 4950.0, "end": 4954.0, "text": " And remember, we've you know, we're graphing this by powers of two.", "tokens": [400, 1604, 11, 321, 600, 291, 458, 11, 321, 434, 1295, 79, 571, 341, 538, 8674, 295, 732, 13], "temperature": 0.0, "avg_logprob": -0.11161260043873507, "compression_ratio": 1.5638297872340425, "no_speech_prob": 1.061539751390228e-05}, {"id": 611, "seek": 494700, "start": 4954.0, "end": 4957.0, "text": " We get that epsilon is around negative 52.", "tokens": [492, 483, 300, 17889, 307, 926, 3671, 18079, 13], "temperature": 0.0, "avg_logprob": -0.11161260043873507, "compression_ratio": 1.5638297872340425, "no_speech_prob": 1.061539751390228e-05}, {"id": 612, "seek": 494700, "start": 4957.0, "end": 4959.0, "text": " This is close to that.", "tokens": [639, 307, 1998, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.11161260043873507, "compression_ratio": 1.5638297872340425, "no_speech_prob": 1.061539751390228e-05}, {"id": 613, "seek": 494700, "start": 4959.0, "end": 4966.0, "text": " And then the square root of epsilon is around to the negative 26.", "tokens": [400, 550, 264, 3732, 5593, 295, 17889, 307, 926, 281, 264, 3671, 7551, 13], "temperature": 0.0, "avg_logprob": -0.11161260043873507, "compression_ratio": 1.5638297872340425, "no_speech_prob": 1.061539751390228e-05}, {"id": 614, "seek": 494700, "start": 4966.0, "end": 4971.0, "text": " Remember, when for values less than one.", "tokens": [5459, 11, 562, 337, 4190, 1570, 813, 472, 13], "temperature": 0.0, "avg_logprob": -0.11161260043873507, "compression_ratio": 1.5638297872340425, "no_speech_prob": 1.061539751390228e-05}, {"id": 615, "seek": 494700, "start": 4971.0, "end": 4975.0, "text": " Square root makes things larger, not smaller.", "tokens": [16463, 5593, 1669, 721, 4833, 11, 406, 4356, 13], "temperature": 0.0, "avg_logprob": -0.11161260043873507, "compression_ratio": 1.5638297872340425, "no_speech_prob": 1.061539751390228e-05}, {"id": 616, "seek": 497500, "start": 4975.0, "end": 4980.0, "text": " It's kind of a reverse for greater than one.", "tokens": [467, 311, 733, 295, 257, 9943, 337, 5044, 813, 472, 13], "temperature": 0.0, "avg_logprob": -0.07469152759861301, "compression_ratio": 1.6292682926829267, "no_speech_prob": 3.187522679581889e-06}, {"id": 617, "seek": 497500, "start": 4980.0, "end": 5000.0, "text": " And so this is, I thought, like a neat illustration of kind of when when machine epsilon is posing a problem and also kind of the difference between depending on machine epsilon for your accuracy and depending on the square root of machine epsilon, which is what classic Gram-Schmidt does.", "tokens": [400, 370, 341, 307, 11, 286, 1194, 11, 411, 257, 10654, 22645, 295, 733, 295, 562, 562, 3479, 17889, 307, 40378, 257, 1154, 293, 611, 733, 295, 264, 2649, 1296, 5413, 322, 3479, 17889, 337, 428, 14170, 293, 5413, 322, 264, 3732, 5593, 295, 3479, 17889, 11, 597, 307, 437, 7230, 22130, 12, 31560, 39000, 775, 13], "temperature": 0.0, "avg_logprob": -0.07469152759861301, "compression_ratio": 1.6292682926829267, "no_speech_prob": 3.187522679581889e-06}, {"id": 618, "seek": 500000, "start": 5000.0, "end": 5014.0, "text": " And this is although it's a contrived example, it's nice because we know exactly kind of what the answer should be. And we see when each one stops following following the line.", "tokens": [400, 341, 307, 4878, 309, 311, 257, 660, 470, 937, 1365, 11, 309, 311, 1481, 570, 321, 458, 2293, 733, 295, 437, 264, 1867, 820, 312, 13, 400, 321, 536, 562, 1184, 472, 10094, 3480, 3480, 264, 1622, 13], "temperature": 0.0, "avg_logprob": -0.1074013454573495, "compression_ratio": 1.4303797468354431, "no_speech_prob": 7.766720045765396e-06}, {"id": 619, "seek": 500000, "start": 5014.0, "end": 5018.0, "text": " Questions about this example.", "tokens": [27738, 466, 341, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1074013454573495, "compression_ratio": 1.4303797468354431, "no_speech_prob": 7.766720045765396e-06}, {"id": 620, "seek": 500000, "start": 5018.0, "end": 5024.0, "text": " Or machine epsilon.", "tokens": [1610, 3479, 17889, 13], "temperature": 0.0, "avg_logprob": -0.1074013454573495, "compression_ratio": 1.4303797468354431, "no_speech_prob": 7.766720045765396e-06}, {"id": 621, "seek": 502400, "start": 5024.0, "end": 5030.0, "text": " And so classic classic Gram-Schmidt is not not used in practice.", "tokens": [400, 370, 7230, 7230, 22130, 12, 31560, 39000, 307, 406, 406, 1143, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.11366001176245419, "compression_ratio": 1.5078534031413613, "no_speech_prob": 2.1567768726526992e-06}, {"id": 622, "seek": 502400, "start": 5030.0, "end": 5037.0, "text": " For this reason.", "tokens": [1171, 341, 1778, 13], "temperature": 0.0, "avg_logprob": -0.11366001176245419, "compression_ratio": 1.5078534031413613, "no_speech_prob": 2.1567768726526992e-06}, {"id": 623, "seek": 502400, "start": 5037.0, "end": 5040.0, "text": " Okay, so now we're going to look at another example.", "tokens": [1033, 11, 370, 586, 321, 434, 516, 281, 574, 412, 1071, 1365, 13], "temperature": 0.0, "avg_logprob": -0.11366001176245419, "compression_ratio": 1.5078534031413613, "no_speech_prob": 2.1567768726526992e-06}, {"id": 624, "seek": 502400, "start": 5040.0, "end": 5046.0, "text": " And if we were if we were to do householder on this graph, it's basically the same accuracy as modified Gram-Schmidt.", "tokens": [400, 498, 321, 645, 498, 321, 645, 281, 360, 9888, 260, 322, 341, 4295, 11, 309, 311, 1936, 264, 912, 14170, 382, 15873, 22130, 12, 31560, 39000, 13], "temperature": 0.0, "avg_logprob": -0.11366001176245419, "compression_ratio": 1.5078534031413613, "no_speech_prob": 2.1567768726526992e-06}, {"id": 625, "seek": 502400, "start": 5046.0, "end": 5050.0, "text": " So it levels off at the same place.", "tokens": [407, 309, 4358, 766, 412, 264, 912, 1081, 13], "temperature": 0.0, "avg_logprob": -0.11366001176245419, "compression_ratio": 1.5078534031413613, "no_speech_prob": 2.1567768726526992e-06}, {"id": 626, "seek": 505000, "start": 5050.0, "end": 5056.0, "text": " So I had it on here at one point, but it's harder to read. But in terms of accuracy.", "tokens": [407, 286, 632, 309, 322, 510, 412, 472, 935, 11, 457, 309, 311, 6081, 281, 1401, 13, 583, 294, 2115, 295, 14170, 13], "temperature": 0.0, "avg_logprob": -0.07626043860592059, "compression_ratio": 1.4035087719298245, "no_speech_prob": 3.0415440051001497e-06}, {"id": 627, "seek": 505000, "start": 5056.0, "end": 5066.0, "text": " Well, for this example for accuracy, householder and modified Gram-Schmidt are giving you the same thing.", "tokens": [1042, 11, 337, 341, 1365, 337, 14170, 11, 9888, 260, 293, 15873, 22130, 12, 31560, 39000, 366, 2902, 291, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.07626043860592059, "compression_ratio": 1.4035087719298245, "no_speech_prob": 3.0415440051001497e-06}, {"id": 628, "seek": 505000, "start": 5066.0, "end": 5071.0, "text": " Okay, so the next example we're going to look at.", "tokens": [1033, 11, 370, 264, 958, 1365, 321, 434, 516, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.07626043860592059, "compression_ratio": 1.4035087719298245, "no_speech_prob": 3.0415440051001497e-06}, {"id": 629, "seek": 507100, "start": 5071.0, "end": 5089.0, "text": " Is a matrix and this is this is also coming from Lecture 9 of Tremblathan. So we have a matrix A that's point seven comma point seven oh seven one one and then point seven zero zero zero one comma point seven oh seven one one.", "tokens": [1119, 257, 8141, 293, 341, 307, 341, 307, 611, 1348, 490, 37196, 540, 1722, 295, 8648, 2504, 75, 9390, 13, 407, 321, 362, 257, 8141, 316, 300, 311, 935, 3407, 22117, 935, 3407, 1954, 3407, 472, 472, 293, 550, 935, 3407, 4018, 4018, 4018, 472, 22117, 935, 3407, 1954, 3407, 472, 472, 13], "temperature": 0.0, "avg_logprob": -0.180415255682809, "compression_ratio": 1.7712765957446808, "no_speech_prob": 1.3419278729998041e-05}, {"id": 630, "seek": 507100, "start": 5089.0, "end": 5094.0, "text": " And you can probably just from looking at this matrix already guess maybe.", "tokens": [400, 291, 393, 1391, 445, 490, 1237, 412, 341, 8141, 1217, 2041, 1310, 13], "temperature": 0.0, "avg_logprob": -0.180415255682809, "compression_ratio": 1.7712765957446808, "no_speech_prob": 1.3419278729998041e-05}, {"id": 631, "seek": 507100, "start": 5094.0, "end": 5099.0, "text": " Oops, I need to run this again.", "tokens": [21726, 11, 286, 643, 281, 1190, 341, 797, 13], "temperature": 0.0, "avg_logprob": -0.180415255682809, "compression_ratio": 1.7712765957446808, "no_speech_prob": 1.3419278729998041e-05}, {"id": 632, "seek": 509900, "start": 5099.0, "end": 5102.0, "text": " Guess where this this may be going.", "tokens": [17795, 689, 341, 341, 815, 312, 516, 13], "temperature": 0.0, "avg_logprob": -0.1074616241455078, "compression_ratio": 1.4842105263157894, "no_speech_prob": 1.095271909434814e-05}, {"id": 633, "seek": 509900, "start": 5102.0, "end": 5112.0, "text": " So this matrix is close to not having full rank because the.", "tokens": [407, 341, 8141, 307, 1998, 281, 406, 1419, 1577, 6181, 570, 264, 13], "temperature": 0.0, "avg_logprob": -0.1074616241455078, "compression_ratio": 1.4842105263157894, "no_speech_prob": 1.095271909434814e-05}, {"id": 634, "seek": 509900, "start": 5112.0, "end": 5121.0, "text": " Second row was almost a repeat of the first row. You know, there's just this difference of one to the negative fifth between them.", "tokens": [5736, 5386, 390, 1920, 257, 7149, 295, 264, 700, 5386, 13, 509, 458, 11, 456, 311, 445, 341, 2649, 295, 472, 281, 264, 3671, 9266, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.1074616241455078, "compression_ratio": 1.4842105263157894, "no_speech_prob": 1.095271909434814e-05}, {"id": 635, "seek": 509900, "start": 5121.0, "end": 5126.0, "text": " So we'll use our modified Gram-Schmidt to get Q and R.", "tokens": [407, 321, 603, 764, 527, 15873, 22130, 12, 31560, 39000, 281, 483, 1249, 293, 497, 13], "temperature": 0.0, "avg_logprob": -0.1074616241455078, "compression_ratio": 1.4842105263157894, "no_speech_prob": 1.095271909434814e-05}, {"id": 636, "seek": 512600, "start": 5126.0, "end": 5130.0, "text": " We can use householder to get Q and R.", "tokens": [492, 393, 764, 9888, 260, 281, 483, 1249, 293, 497, 13], "temperature": 0.0, "avg_logprob": -0.1489599083043352, "compression_ratio": 1.4656084656084656, "no_speech_prob": 1.6441363186459057e-05}, {"id": 637, "seek": 512600, "start": 5130.0, "end": 5134.0, "text": " And I did this, I guess, both with.", "tokens": [400, 286, 630, 341, 11, 286, 2041, 11, 1293, 365, 13], "temperature": 0.0, "avg_logprob": -0.1489599083043352, "compression_ratio": 1.4656084656084656, "no_speech_prob": 1.6441363186459057e-05}, {"id": 638, "seek": 512600, "start": 5134.0, "end": 5137.0, "text": " Our version and NumPy's.", "tokens": [2621, 3037, 293, 22592, 47, 88, 311, 13], "temperature": 0.0, "avg_logprob": -0.1489599083043352, "compression_ratio": 1.4656084656084656, "no_speech_prob": 1.6441363186459057e-05}, {"id": 639, "seek": 512600, "start": 5137.0, "end": 5143.0, "text": " And then check that all the QR factorizations work.", "tokens": [400, 550, 1520, 300, 439, 264, 32784, 5952, 14455, 589, 13], "temperature": 0.0, "avg_logprob": -0.1489599083043352, "compression_ratio": 1.4656084656084656, "no_speech_prob": 1.6441363186459057e-05}, {"id": 640, "seek": 512600, "start": 5143.0, "end": 5153.0, "text": " And so here notice that in all cases, we're not actually getting back the original A because this bottom left entry has been.", "tokens": [400, 370, 510, 3449, 300, 294, 439, 3331, 11, 321, 434, 406, 767, 1242, 646, 264, 3380, 316, 570, 341, 2767, 1411, 8729, 575, 668, 13], "temperature": 0.0, "avg_logprob": -0.1489599083043352, "compression_ratio": 1.4656084656084656, "no_speech_prob": 1.6441363186459057e-05}, {"id": 641, "seek": 515300, "start": 5153.0, "end": 5158.0, "text": " Has ended up point seven, although that is close to what the original A was.", "tokens": [8646, 4590, 493, 935, 3407, 11, 4878, 300, 307, 1998, 281, 437, 264, 3380, 316, 390, 13], "temperature": 0.0, "avg_logprob": -0.08424753271123414, "compression_ratio": 1.5627705627705628, "no_speech_prob": 3.966892563767033e-06}, {"id": 642, "seek": 515300, "start": 5158.0, "end": 5163.0, "text": " But now we can check how close is Q to being perfectly orthonormal.", "tokens": [583, 586, 321, 393, 1520, 577, 1998, 307, 1249, 281, 885, 6239, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.08424753271123414, "compression_ratio": 1.5627705627705628, "no_speech_prob": 3.966892563767033e-06}, {"id": 643, "seek": 515300, "start": 5163.0, "end": 5171.0, "text": " And so here there's no difference between what we're getting from Gram-Schmidt and householder in terms of reconstructing our A.", "tokens": [400, 370, 510, 456, 311, 572, 2649, 1296, 437, 321, 434, 1242, 490, 22130, 12, 31560, 39000, 293, 9888, 260, 294, 2115, 295, 31499, 278, 527, 316, 13], "temperature": 0.0, "avg_logprob": -0.08424753271123414, "compression_ratio": 1.5627705627705628, "no_speech_prob": 3.966892563767033e-06}, {"id": 644, "seek": 515300, "start": 5171.0, "end": 5179.0, "text": " So looking to get a measure on kind of how close Q is to being orthonormal will take Q.", "tokens": [407, 1237, 281, 483, 257, 3481, 322, 733, 295, 577, 1998, 1249, 307, 281, 885, 420, 11943, 24440, 486, 747, 1249, 13], "temperature": 0.0, "avg_logprob": -0.08424753271123414, "compression_ratio": 1.5627705627705628, "no_speech_prob": 3.966892563767033e-06}, {"id": 645, "seek": 517900, "start": 5179.0, "end": 5191.0, "text": " Q times Q transpose minus the identity. If it was perfectly orthonormal, what would Q times Q transpose minus the identity be?", "tokens": [1249, 1413, 1249, 25167, 3175, 264, 6575, 13, 759, 309, 390, 6239, 420, 11943, 24440, 11, 437, 576, 1249, 1413, 1249, 25167, 3175, 264, 6575, 312, 30], "temperature": 0.0, "avg_logprob": -0.13548925698521624, "compression_ratio": 1.7759562841530054, "no_speech_prob": 9.08022775547579e-06}, {"id": 646, "seek": 517900, "start": 5191.0, "end": 5199.0, "text": " Someone whisper it, but yeah, zero, because it was perfectly orthonormal Q times Q transpose would be the identity.", "tokens": [8734, 26018, 309, 11, 457, 1338, 11, 4018, 11, 570, 309, 390, 6239, 420, 11943, 24440, 1249, 1413, 1249, 25167, 576, 312, 264, 6575, 13], "temperature": 0.0, "avg_logprob": -0.13548925698521624, "compression_ratio": 1.7759562841530054, "no_speech_prob": 9.08022775547579e-06}, {"id": 647, "seek": 517900, "start": 5199.0, "end": 5205.0, "text": " Here we're looking at the norm of that. And you'll notice with Gram-Schmidt, it's.", "tokens": [1692, 321, 434, 1237, 412, 264, 2026, 295, 300, 13, 400, 291, 603, 3449, 365, 22130, 12, 31560, 39000, 11, 309, 311, 13], "temperature": 0.0, "avg_logprob": -0.13548925698521624, "compression_ratio": 1.7759562841530054, "no_speech_prob": 9.08022775547579e-06}, {"id": 648, "seek": 520500, "start": 5205.0, "end": 5214.0, "text": " Something to the negative 11th, 10 to the negative 11th, and with householder, it's 10 to the negative 16th.", "tokens": [6595, 281, 264, 3671, 2975, 392, 11, 1266, 281, 264, 3671, 2975, 392, 11, 293, 365, 9888, 260, 11, 309, 311, 1266, 281, 264, 3671, 3165, 392, 13], "temperature": 0.0, "avg_logprob": -0.07241602677565355, "compression_ratio": 1.5827814569536425, "no_speech_prob": 4.936871846439317e-06}, {"id": 649, "seek": 520500, "start": 5214.0, "end": 5228.0, "text": " And so that's kind of an example that householder got us a Q that was closer to being perfectly orthonormal than Gram-Schmidt did.", "tokens": [400, 370, 300, 311, 733, 295, 364, 1365, 300, 9888, 260, 658, 505, 257, 1249, 300, 390, 4966, 281, 885, 6239, 420, 11943, 24440, 813, 22130, 12, 31560, 39000, 630, 13], "temperature": 0.0, "avg_logprob": -0.07241602677565355, "compression_ratio": 1.5827814569536425, "no_speech_prob": 4.936871846439317e-06}, {"id": 650, "seek": 522800, "start": 5228.0, "end": 5248.0, "text": " And I believe NumPy's implementation of QR is using householder.", "tokens": [400, 286, 1697, 22592, 47, 88, 311, 11420, 295, 32784, 307, 1228, 9888, 260, 13], "temperature": 0.0, "avg_logprob": -0.12018053020749773, "compression_ratio": 1.0208333333333333, "no_speech_prob": 1.8447378806740744e-06}, {"id": 651, "seek": 522800, "start": 5248.0, "end": 5255.0, "text": " Linda and Tim has the microphone.", "tokens": [20324, 293, 7172, 575, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.12018053020749773, "compression_ratio": 1.0208333333333333, "no_speech_prob": 1.8447378806740744e-06}, {"id": 652, "seek": 525500, "start": 5255.0, "end": 5267.0, "text": " So if we could go back to the graph, I was wondering what makes the difference between the classic and modified that's causing that.", "tokens": [407, 498, 321, 727, 352, 646, 281, 264, 4295, 11, 286, 390, 6359, 437, 1669, 264, 2649, 1296, 264, 7230, 293, 15873, 300, 311, 9853, 300, 13], "temperature": 0.0, "avg_logprob": -0.13191432717405718, "compression_ratio": 1.6046511627906976, "no_speech_prob": 1.4737599485670216e-05}, {"id": 653, "seek": 525500, "start": 5267.0, "end": 5274.0, "text": " So the difference lies in the algorithms. And we didn't really talk about what's going on with modified Gram-Schmidt.", "tokens": [407, 264, 2649, 9134, 294, 264, 14642, 13, 400, 321, 994, 380, 534, 751, 466, 437, 311, 516, 322, 365, 15873, 22130, 12, 31560, 39000, 13], "temperature": 0.0, "avg_logprob": -0.13191432717405718, "compression_ratio": 1.6046511627906976, "no_speech_prob": 1.4737599485670216e-05}, {"id": 654, "seek": 525500, "start": 5274.0, "end": 5279.0, "text": " But modified Gram-Schmidt basically looks like you've just kind of made this algebraic change.", "tokens": [583, 15873, 22130, 12, 31560, 39000, 1936, 1542, 411, 291, 600, 445, 733, 295, 1027, 341, 21989, 299, 1319, 13], "temperature": 0.0, "avg_logprob": -0.13191432717405718, "compression_ratio": 1.6046511627906976, "no_speech_prob": 1.4737599485670216e-05}, {"id": 655, "seek": 527900, "start": 5279.0, "end": 5285.0, "text": " Like you're doing the same thing, but you're kind of changing the order of how you do things.", "tokens": [1743, 291, 434, 884, 264, 912, 551, 11, 457, 291, 434, 733, 295, 4473, 264, 1668, 295, 577, 291, 360, 721, 13], "temperature": 0.0, "avg_logprob": -0.0793728534086251, "compression_ratio": 1.6292682926829267, "no_speech_prob": 1.4823290257481858e-06}, {"id": 656, "seek": 527900, "start": 5285.0, "end": 5292.0, "text": " But it's something that the operations are compounding, or like the air is compounding differently between them.", "tokens": [583, 309, 311, 746, 300, 264, 7705, 366, 14154, 278, 11, 420, 411, 264, 1988, 307, 14154, 278, 7614, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.0793728534086251, "compression_ratio": 1.6292682926829267, "no_speech_prob": 1.4823290257481858e-06}, {"id": 657, "seek": 527900, "start": 5292.0, "end": 5302.0, "text": " Even though mathematically, you know, if you're like proving it on paper and pencil, it's like, oh, these should be equivalent.", "tokens": [2754, 1673, 44003, 11, 291, 458, 11, 498, 291, 434, 411, 27221, 309, 322, 3035, 293, 10985, 11, 309, 311, 411, 11, 1954, 11, 613, 820, 312, 10344, 13], "temperature": 0.0, "avg_logprob": -0.0793728534086251, "compression_ratio": 1.6292682926829267, "no_speech_prob": 1.4823290257481858e-06}, {"id": 658, "seek": 530200, "start": 5302.0, "end": 5309.0, "text": " Good question.", "tokens": [2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2815289298693339, "compression_ratio": 1.0, "no_speech_prob": 1.4284163626143709e-05}, {"id": 659, "seek": 530200, "start": 5309.0, "end": 5322.0, "text": " Other questions?", "tokens": [5358, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2815289298693339, "compression_ratio": 1.0, "no_speech_prob": 1.4284163626143709e-05}, {"id": 660, "seek": 532200, "start": 5322.0, "end": 5347.0, "text": " I think that might be all I want to say about, well, how briefly put a classified Gram-Schmidt and modified Gram-Schmidt next to each other just to compare.", "tokens": [286, 519, 300, 1062, 312, 439, 286, 528, 281, 584, 466, 11, 731, 11, 577, 10515, 829, 257, 20627, 22130, 12, 31560, 39000, 293, 15873, 22130, 12, 31560, 39000, 958, 281, 1184, 661, 445, 281, 6794, 13], "temperature": 0.0, "avg_logprob": -0.13347373357633266, "compression_ratio": 1.3220338983050848, "no_speech_prob": 8.013025762920734e-06}, {"id": 661, "seek": 534700, "start": 5347.0, "end": 5352.0, "text": " Oh, you know what it is. Sorry. With modified Gram-Schmidt. Yeah.", "tokens": [876, 11, 291, 458, 437, 309, 307, 13, 4919, 13, 2022, 15873, 22130, 12, 31560, 39000, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.132799274043033, "compression_ratio": 1.5277777777777777, "no_speech_prob": 5.862595571670681e-06}, {"id": 662, "seek": 534700, "start": 5352.0, "end": 5359.0, "text": " So what I showed you with the picture when I drew the vectors, that was classic Gram-Schmidt, where you're kind of doing these projections once.", "tokens": [407, 437, 286, 4712, 291, 365, 264, 3036, 562, 286, 12804, 264, 18875, 11, 300, 390, 7230, 22130, 12, 31560, 39000, 11, 689, 291, 434, 733, 295, 884, 613, 32371, 1564, 13], "temperature": 0.0, "avg_logprob": -0.132799274043033, "compression_ratio": 1.5277777777777777, "no_speech_prob": 5.862595571670681e-06}, {"id": 663, "seek": 534700, "start": 5359.0, "end": 5371.0, "text": " Modified Gram-Schmidt, it's like you're modifying each value of.", "tokens": [6583, 2587, 22130, 12, 31560, 39000, 11, 309, 311, 411, 291, 434, 42626, 1184, 2158, 295, 13], "temperature": 0.0, "avg_logprob": -0.132799274043033, "compression_ratio": 1.5277777777777777, "no_speech_prob": 5.862595571670681e-06}, {"id": 664, "seek": 537100, "start": 5371.0, "end": 5378.0, "text": " Like you're getting this whole matrix of projections that you're modifying each time.", "tokens": [1743, 291, 434, 1242, 341, 1379, 8141, 295, 32371, 300, 291, 434, 42626, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.1118164895072816, "compression_ratio": 1.4337349397590362, "no_speech_prob": 2.72642455456662e-06}, {"id": 665, "seek": 537100, "start": 5378.0, "end": 5387.0, "text": " So we have V, which is what we're kind of copying A into.", "tokens": [407, 321, 362, 691, 11, 597, 307, 437, 321, 434, 733, 295, 27976, 316, 666, 13], "temperature": 0.0, "avg_logprob": -0.1118164895072816, "compression_ratio": 1.4337349397590362, "no_speech_prob": 2.72642455456662e-06}, {"id": 666, "seek": 537100, "start": 5387.0, "end": 5395.0, "text": " And so like here with classic Gram-Schmidt, you just need to have a single vector V each time.", "tokens": [400, 370, 411, 510, 365, 7230, 22130, 12, 31560, 39000, 11, 291, 445, 643, 281, 362, 257, 2167, 8062, 691, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.1118164895072816, "compression_ratio": 1.4337349397590362, "no_speech_prob": 2.72642455456662e-06}, {"id": 667, "seek": 539500, "start": 5395.0, "end": 5409.0, "text": " You're just kind of like, you know, projecting it and then you subtract off something else, project something, subtract off something else.", "tokens": [509, 434, 445, 733, 295, 411, 11, 291, 458, 11, 43001, 309, 293, 550, 291, 16390, 766, 746, 1646, 11, 1716, 746, 11, 16390, 766, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.17757755279541015, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.0289173587807454e-05}, {"id": 668, "seek": 539500, "start": 5409.0, "end": 5423.0, "text": " Whereas here you actually have to have a whole matrix V and like in this.", "tokens": [13813, 510, 291, 767, 362, 281, 362, 257, 1379, 8141, 691, 293, 411, 294, 341, 13], "temperature": 0.0, "avg_logprob": -0.17757755279541015, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.0289173587807454e-05}, {"id": 669, "seek": 542300, "start": 5423.0, "end": 5428.0, "text": " Well.", "tokens": [1042, 13], "temperature": 0.0, "avg_logprob": -0.10133609771728516, "compression_ratio": 1.4391891891891893, "no_speech_prob": 3.785218268603785e-06}, {"id": 670, "seek": 542300, "start": 5428.0, "end": 5435.0, "text": " Yeah, and it affects what your norms are. So basically it's like you're calculating more projections kind of in this more complicated way.", "tokens": [865, 11, 293, 309, 11807, 437, 428, 24357, 366, 13, 407, 1936, 309, 311, 411, 291, 434, 28258, 544, 32371, 733, 295, 294, 341, 544, 6179, 636, 13], "temperature": 0.0, "avg_logprob": -0.10133609771728516, "compression_ratio": 1.4391891891891893, "no_speech_prob": 3.785218268603785e-06}, {"id": 671, "seek": 542300, "start": 5435.0, "end": 5443.0, "text": " Like each projection is really made up of several small projections.", "tokens": [1743, 1184, 22743, 307, 534, 1027, 493, 295, 2940, 1359, 32371, 13], "temperature": 0.0, "avg_logprob": -0.10133609771728516, "compression_ratio": 1.4391891891891893, "no_speech_prob": 3.785218268603785e-06}, {"id": 672, "seek": 544300, "start": 5443.0, "end": 5453.0, "text": " Matthew, and can you pass the microphone? Oh, this is Matthew.", "tokens": [12434, 11, 293, 393, 291, 1320, 264, 10952, 30, 876, 11, 341, 307, 12434, 13], "temperature": 0.0, "avg_logprob": -0.29390450624319225, "compression_ratio": 1.296875, "no_speech_prob": 1.1841610103147104e-05}, {"id": 673, "seek": 544300, "start": 5453.0, "end": 5459.0, "text": " I may have raised my hand too soon.", "tokens": [286, 815, 362, 6005, 452, 1011, 886, 2321, 13], "temperature": 0.0, "avg_logprob": -0.29390450624319225, "compression_ratio": 1.296875, "no_speech_prob": 1.1841610103147104e-05}, {"id": 674, "seek": 544300, "start": 5459.0, "end": 5464.0, "text": " Oh, I said I wasn't planning on it, but yeah, we can talk about it.", "tokens": [876, 11, 286, 848, 286, 2067, 380, 5038, 322, 309, 11, 457, 1338, 11, 321, 393, 751, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.29390450624319225, "compression_ratio": 1.296875, "no_speech_prob": 1.1841610103147104e-05}, {"id": 675, "seek": 546400, "start": 5464.0, "end": 5473.0, "text": " Yeah, and Kelsey also has her hand up, it seems about this.", "tokens": [865, 11, 293, 44714, 611, 575, 720, 1011, 493, 11, 309, 2544, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.31315495675070243, "compression_ratio": 1.3263888888888888, "no_speech_prob": 1.1123736840090714e-05}, {"id": 676, "seek": 546400, "start": 5473.0, "end": 5480.0, "text": " I want to change topics a little bit. Oh yeah.", "tokens": [286, 528, 281, 1319, 8378, 257, 707, 857, 13, 876, 1338, 13], "temperature": 0.0, "avg_logprob": -0.31315495675070243, "compression_ratio": 1.3263888888888888, "no_speech_prob": 1.1123736840090714e-05}, {"id": 677, "seek": 546400, "start": 5480.0, "end": 5483.0, "text": " Is there like some intuition for why.", "tokens": [1119, 456, 411, 512, 24002, 337, 983, 13], "temperature": 0.0, "avg_logprob": -0.31315495675070243, "compression_ratio": 1.3263888888888888, "no_speech_prob": 1.1123736840090714e-05}, {"id": 678, "seek": 546400, "start": 5483.0, "end": 5490.0, "text": " RQ ends up like why it converges to something?", "tokens": [497, 48, 5314, 493, 411, 983, 309, 9652, 2880, 281, 746, 30], "temperature": 0.0, "avg_logprob": -0.31315495675070243, "compression_ratio": 1.3263888888888888, "no_speech_prob": 1.1123736840090714e-05}, {"id": 679, "seek": 549000, "start": 5490.0, "end": 5495.0, "text": " Let me go back to the QR factorization.", "tokens": [961, 385, 352, 646, 281, 264, 32784, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.24505213328770228, "compression_ratio": 0.8297872340425532, "no_speech_prob": 0.0002821993257384747}, {"id": 680, "seek": 549500, "start": 5495.0, "end": 5524.0, "text": " All right, it's more triangular every time you do that. RQ.", "tokens": [1057, 558, 11, 309, 311, 544, 38190, 633, 565, 291, 360, 300, 13, 497, 48, 13], "temperature": 0.0, "avg_logprob": -0.30072219371795655, "compression_ratio": 0.8805970149253731, "no_speech_prob": 0.0007766297203488648}, {"id": 681, "seek": 552400, "start": 5524.0, "end": 5536.0, "text": " And not I don't have anything off my head of yeah, like intuition for that in particular.", "tokens": [50364, 400, 406, 286, 500, 380, 362, 1340, 766, 452, 1378, 295, 1338, 11, 411, 24002, 337, 300, 294, 1729, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3716302954632303, "compression_ratio": 1.072289156626506, "no_speech_prob": 2.505759766791016e-05}, {"id": 682, "seek": 555400, "start": 5554.0, "end": 5559.0, "text": " Okay, Sam has something and then I will say a little bit about the final exam after that.", "tokens": [1033, 11, 4832, 575, 746, 293, 550, 286, 486, 584, 257, 707, 857, 466, 264, 2572, 1139, 934, 300, 13], "temperature": 0.0, "avg_logprob": -0.24994557698567707, "compression_ratio": 1.1134020618556701, "no_speech_prob": 0.25322800874710083}, {"id": 683, "seek": 555400, "start": 5559.0, "end": 5564.0, "text": " One more question.", "tokens": [1485, 544, 1168, 13], "temperature": 0.0, "avg_logprob": -0.24994557698567707, "compression_ratio": 1.1134020618556701, "no_speech_prob": 0.25322800874710083}, {"id": 684, "seek": 556400, "start": 5564.0, "end": 5586.0, "text": " The diagonal that's produced or close to diagonal, if we have all real eigenvalues, then it will merge to a total diagonal.", "tokens": [440, 21539, 300, 311, 7126, 420, 1998, 281, 21539, 11, 498, 321, 362, 439, 957, 10446, 46033, 11, 550, 309, 486, 22183, 281, 257, 3217, 21539, 13], "temperature": 0.0, "avg_logprob": -0.2159558573076802, "compression_ratio": 1.2551020408163265, "no_speech_prob": 0.00015836193051654845}, {"id": 685, "seek": 558600, "start": 5586.0, "end": 5594.0, "text": " Looking at the matrix, how are we supposed to know which of the diagonal are the eigenvalues?", "tokens": [11053, 412, 264, 8141, 11, 577, 366, 321, 3442, 281, 458, 597, 295, 264, 21539, 366, 264, 10446, 46033, 30], "temperature": 0.0, "avg_logprob": -0.14528782336742846, "compression_ratio": 1.5315789473684212, "no_speech_prob": 1.921999137266539e-05}, {"id": 686, "seek": 558600, "start": 5594.0, "end": 5603.0, "text": " Is it ones that have no other values like in the column or row?", "tokens": [1119, 309, 2306, 300, 362, 572, 661, 4190, 411, 294, 264, 7738, 420, 5386, 30], "temperature": 0.0, "avg_logprob": -0.14528782336742846, "compression_ratio": 1.5315789473684212, "no_speech_prob": 1.921999137266539e-05}, {"id": 687, "seek": 558600, "start": 5603.0, "end": 5613.0, "text": " Yeah, so I'm actually not sure if this is a theorem, like in just all the cases I did, if it like didn't have other stuff beneath it,", "tokens": [865, 11, 370, 286, 478, 767, 406, 988, 498, 341, 307, 257, 20904, 11, 411, 294, 445, 439, 264, 3331, 286, 630, 11, 498, 309, 411, 994, 380, 362, 661, 1507, 17149, 309, 11], "temperature": 0.0, "avg_logprob": -0.14528782336742846, "compression_ratio": 1.5315789473684212, "no_speech_prob": 1.921999137266539e-05}, {"id": 688, "seek": 561300, "start": 5613.0, "end": 5623.0, "text": " it was turning out to be an eigenvalue, but I don't know if that's always the case or not.", "tokens": [309, 390, 6246, 484, 281, 312, 364, 10446, 29155, 11, 457, 286, 500, 380, 458, 498, 300, 311, 1009, 264, 1389, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.10054796461075072, "compression_ratio": 1.4149659863945578, "no_speech_prob": 1.2606509699253365e-05}, {"id": 689, "seek": 561300, "start": 5623.0, "end": 5629.0, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.10054796461075072, "compression_ratio": 1.4149659863945578, "no_speech_prob": 1.2606509699253365e-05}, {"id": 690, "seek": 561300, "start": 5629.0, "end": 5641.0, "text": " Okay, so yeah, about the final. So that's going to be all on paper, so no computers for that.", "tokens": [1033, 11, 370, 1338, 11, 466, 264, 2572, 13, 407, 300, 311, 516, 281, 312, 439, 322, 3035, 11, 370, 572, 10807, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.10054796461075072, "compression_ratio": 1.4149659863945578, "no_speech_prob": 1.2606509699253365e-05}, {"id": 691, "seek": 564100, "start": 5641.0, "end": 5654.0, "text": " Yeah, I've tried to take questions, many of them are things, yeah, all of them are definitely related to what we've done in class and in the homework and in the notebooks.", "tokens": [865, 11, 286, 600, 3031, 281, 747, 1651, 11, 867, 295, 552, 366, 721, 11, 1338, 11, 439, 295, 552, 366, 2138, 4077, 281, 437, 321, 600, 1096, 294, 1508, 293, 294, 264, 14578, 293, 294, 264, 43782, 13], "temperature": 0.0, "avg_logprob": -0.20975427849348202, "compression_ratio": 1.4132231404958677, "no_speech_prob": 2.178102295147255e-05}, {"id": 692, "seek": 565400, "start": 5654.0, "end": 5675.0, "text": " So a mix, so there are some examples where I give you code and ask you to modify it, and this will be written, but then there are also conceptual questions.", "tokens": [407, 257, 2890, 11, 370, 456, 366, 512, 5110, 689, 286, 976, 291, 3089, 293, 1029, 291, 281, 16927, 309, 11, 293, 341, 486, 312, 3720, 11, 457, 550, 456, 366, 611, 24106, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2192121588665506, "compression_ratio": 1.375, "no_speech_prob": 8.938685823522974e-06}, {"id": 693, "seek": 565400, "start": 5675.0, "end": 5680.0, "text": " Anything about the algorithms?", "tokens": [11998, 466, 264, 14642, 30], "temperature": 0.0, "avg_logprob": -0.2192121588665506, "compression_ratio": 1.375, "no_speech_prob": 8.938685823522974e-06}, {"id": 694, "seek": 568000, "start": 5680.0, "end": 5690.0, "text": " Well, what do you mean, anything about the algorithms?", "tokens": [1042, 11, 437, 360, 291, 914, 11, 1340, 466, 264, 14642, 30], "temperature": 0.0, "avg_logprob": -0.11194005608558655, "compression_ratio": 1.4477611940298507, "no_speech_prob": 4.09279937230167e-06}, {"id": 695, "seek": 568000, "start": 5690.0, "end": 5700.0, "text": " Yeah, so there are questions, yeah, on the algorithms that we've learned and also on the specific examples that we've covered in the class.", "tokens": [865, 11, 370, 456, 366, 1651, 11, 1338, 11, 322, 264, 14642, 300, 321, 600, 3264, 293, 611, 322, 264, 2685, 5110, 300, 321, 600, 5343, 294, 264, 1508, 13], "temperature": 0.0, "avg_logprob": -0.11194005608558655, "compression_ratio": 1.4477611940298507, "no_speech_prob": 4.09279937230167e-06}, {"id": 696, "seek": 570000, "start": 5700.0, "end": 5713.0, "text": " No. Yeah, and I tried to stay away from stuff that I thought was like, just like annoying memorization, like I wanted it to more be like, do you understand the concepts?", "tokens": [883, 13, 865, 11, 293, 286, 3031, 281, 1754, 1314, 490, 1507, 300, 286, 1194, 390, 411, 11, 445, 411, 11304, 10560, 2144, 11, 411, 286, 1415, 309, 281, 544, 312, 411, 11, 360, 291, 1223, 264, 10392, 30], "temperature": 0.0, "avg_logprob": -0.12964923991713412, "compression_ratio": 1.2900763358778626, "no_speech_prob": 5.682167284248862e-06}, {"id": 697, "seek": 571300, "start": 5713.0, "end": 5734.0, "text": " And like key points that I thought you should know of maybe kind of like what an algorithm gives you or what an algorithm is used for, but yeah, you definitely do not have to regurgitate the algorithms.", "tokens": [400, 411, 2141, 2793, 300, 286, 1194, 291, 820, 458, 295, 1310, 733, 295, 411, 437, 364, 9284, 2709, 291, 420, 437, 364, 9284, 307, 1143, 337, 11, 457, 1338, 11, 291, 2138, 360, 406, 362, 281, 1121, 5476, 8086, 264, 14642, 13], "temperature": 0.0, "avg_logprob": -0.07686410051711062, "compression_ratio": 1.4962962962962962, "no_speech_prob": 5.771636097051669e-06}, {"id": 698, "seek": 573400, "start": 5734.0, "end": 5754.0, "text": " Yeah, I've tried to make it not something that's like tricky or overly, like I tried to make it fair, I guess, and like kind of about the like concepts and important parts.", "tokens": [865, 11, 286, 600, 3031, 281, 652, 309, 406, 746, 300, 311, 411, 12414, 420, 24324, 11, 411, 286, 3031, 281, 652, 309, 3143, 11, 286, 2041, 11, 293, 411, 733, 295, 466, 264, 411, 10392, 293, 1021, 3166, 13], "temperature": 0.0, "avg_logprob": -0.08140329214242789, "compression_ratio": 1.463768115942029, "no_speech_prob": 2.7263370157015743e-06}, {"id": 699, "seek": 573400, "start": 5754.0, "end": 5758.0, "text": " Any other questions about it?", "tokens": [2639, 661, 1651, 466, 309, 30], "temperature": 0.0, "avg_logprob": -0.08140329214242789, "compression_ratio": 1.463768115942029, "no_speech_prob": 2.7263370157015743e-06}, {"id": 700, "seek": 575800, "start": 5758.0, "end": 5773.0, "text": " Okay, and we're on, this can end a few minutes early today, and then I will definitely be available through email, so yeah, feel free to email me if you have questions in the next week.", "tokens": [1033, 11, 293, 321, 434, 322, 11, 341, 393, 917, 257, 1326, 2077, 2440, 965, 11, 293, 550, 286, 486, 2138, 312, 2435, 807, 3796, 11, 370, 1338, 11, 841, 1737, 281, 3796, 385, 498, 291, 362, 1651, 294, 264, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.18751100633965165, "compression_ratio": 1.40625, "no_speech_prob": 1.1122056093881838e-05}, {"id": 701, "seek": 575800, "start": 5773.0, "end": 5779.0, "text": " Alright, that's all. Thank you.", "tokens": [2798, 11, 300, 311, 439, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.18751100633965165, "compression_ratio": 1.40625, "no_speech_prob": 1.1122056093881838e-05}, {"id": 702, "seek": 577900, "start": 5779.0, "end": 5791.0, "text": " Thanks.", "tokens": [50364, 2561, 13, 50964], "temperature": 0.0, "avg_logprob": -0.38934051990509033, "compression_ratio": 0.4666666666666667, "no_speech_prob": 0.0002586543851066381}], "language": "en"}