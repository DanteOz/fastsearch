{"text": " Alright welcome back Something to mention somebody asked on the forums really good question was like How do I deal with version control and notebooks? the question was something like every time I change the notebook Jeremy goes and changes it on git and then I do a git pull and I end up with a conflict and And that's that happens a lot with notebooks because notebooks behind the scenes are JSON files Which like every time you run even a cell without changing it it updates that little number saying like what numbered cell this is and so Now suddenly there's a change and so trying to merge Notebook changes is a nightmare so My suggestion like a simple way to do it is is when you're looking at Some notebook Like lesson to RF interpretation you want to start playing around with this First thing I would do would be to go file make a copy and Then in the copy say file rename and give it a name that starts with TMP So that will hide it from get right and so now you've got your own version of that notebook that you can That you can play with okay And so if you now do a git pull and see that the original changed it won't conflict with yours And you can now see there are two different versions There Are different ways of kind of dealing with this Jupiter notebook get problem like everybody has it one one is there are some hooks You can use it like remove all of the cell outputs before you commit to get but in this case I actually want the outputs to be in the repo so you can read it on github and see it So it's a minor issue, but it's it's something which catches everybody Yes Before we move on to interpretation of the random forest model, I wonder if we could summarize the relationship between the hyperparameters on the random forest and its Effect on you know overfitting and dealing with co-linearity and yeah, yeah, yeah, yeah, yeah That sounds like a question born from experience absolutely so I Got to go back to lesson 1 rf If you're ever unsure about where I am you can always see my top here courses ml1 lesson 1 In terms of the hyperparameters that Are interesting and I'm ignoring I'm ignoring like pre-processing, but just the actual hyperparameters The First one of interest I would say is the set RF samples command which determines how many Rows are in each sample so in each tree you're created from how many rows? And each tree So before we start a new tree we either bootstrap a sample so sampling with replacement from the whole thing or we pull Out a subsample of a smaller number of rows, and then we build a tree from there, so So step one is we've got our whole big data set and We grab a few rows at random from it, and we turn them into a smaller data set and then from that We build a tree right so that's the size of that is set RF samples so when we change that size Let's say this originally had like a million rows, and we said set RF samples 20,000 right and then we're going to grow a tree from there Assuming that The tree remains kind of balanced as we grow it can somebody tell me how many layers deep? Would this tree be and assuming we're growing it until every leaf is of size one yes log base two of 20,000 Right okay, so the the depth of the tree Doesn't actually vary that much depending on the number of samples right because it's it's related to the log of the size Can somebody tell me at the very bottom so once we go all the way down to the bottom how many? Leaf nodes would there be? Speak up what? 20,000 right because every single leaf node has a single thing in it, so we've got Obviously a linear relationship between the number of leaf nodes and the size of the sample So when you decrease the sample size It means that there are less kind of Final decisions that can be made right so therefore the tree is is going to be less rich in terms of what it can predict because it's just making less different individual decisions and It also is making less binary choices to get to those decisions so therefore Setting RF samples lower is going to mean that you overfit less But it also means that you're going to have a less accurate Individual tree model right and so remember the way Breiman the inventor of random forest described this is that you're trying to do two things when you build a model When you build a model with bagging one is that Each individual tree or as a scale under say each individual estimator Is as Accurate as possible right on the training set so it's like each model is a strong predictive model but then the across the estimators The correlation between them is as low as possible So that when you average them out together you end up with something that generalizes So by decreasing the set RF samples number We are actually decreasing the power of the estimator and increasing the correlation And so is that going to result in a better or a worse validation set result for you? It depends right. This is the kind of compromise which you have to figure out when you do machine learning models Can you pass that back there? If I wait if I put the OOB value equal to 2 So it is basically dividing every 30 it ensures that every 30% of the data won't be there in each tree right? The OOB say again OOB if I put OOB equal to true in random forest So isn't that make sure that out of my entire data 37% of data won't be there in every tree So all OOB equals true So all OOB equals true So all OOB equals true So all OOB equals true So all OOB equals true So all OOB equals true So all OOB equals true So all OOB equals true does is it says? Whatever your sub sample is it might be a bootstrap sample or it might be a Sub-sample Take all of the other rows right and put them into a freach tree and put them into a different data set and Calculate the the error on those so it doesn't actually impact training at all It just gives you an additional metric which is the OOB error so if you Don't have a validation set then this allows you to get kind of a quasi validation set for free If you want to Set out a sample RF sample so the the default is actually if you say reset RF samples and That causes it to bootstrap, so it'll sample a new data set as big as the original one, but with replacement Okay, so obviously the second benefit of set RF samples is that you can run More quickly and particularly if you're running on a really large data set like a hundred million rows You know it won't be possible to run it on the full data set So you would either have to pick a sub sample of you yourself before you start or you set RF samples The second key parameter that we learned about was min samples leaf Okay, so if I changed min samples leaf before we assumed that min samples leaf was equal to one All right, if I said it equal to two Then what would be my new? Depth how deep would it be? Yes log base to 20,000 minus one okay, so each time we double the min samples leaf We're removing one layer from the tree and Fine I'll come back to you again since you're doing so well how many leaf nodes would there be in that case? What how many leaf nodes would there be in that case? 10,000 okay, so we're going to be again dividing the number of leaf nodes by that number so The result of increasing min samples leaf is that now each of our leaf nodes has more than one thing in so we're going to get A more stable average that we're calculating in each tree, okay? We've got a little bit less depth okay, we've got less decisions to make and we've got a smaller number of leaf nodes So again, we would expect the result of that would be that each estimator would be less predictive But the estimators would be also less correlated So again this might help us to avoid overfitting could you pass the microphone over here, please? Hi, Jeremy, I'm not sure if In that case every node will have exactly two no it won't necessarily have exactly two and I thank you for mentioning that So it might try to do a split and so one reason well What would be an example ten she that you wouldn't split even if you had a hundred nodes? What might be a reason for that sorry a hundred items in a leaf node? They're all the same. They're all the same in terms of Well once the independent saw the dependent And it has the dependent right I mean I guess either but much more likely would be the dependent so if you get to a leaf node where Every single one of them has the same auction price or in classification like every single one of them is a dog Then there is no split that you can do that's going to improve your information right and remember Information is the term we use in a kind of a general sense in random for us to describe the amount of Difference about about additional information we create from a split is like how much are we improving the model? So you'll often see this in this word information gain which means like how much better did the model get by adding an additional split point? And it could be based on RMSE or it could be based on cross entropy or it could be based on how different to the standard Deviations or or whatever so that's just a general term okay? So that's the second thing that we can do which again It's going to speed up our training because it's like one less set of decisions to make remember even though there's one less set of decisions those decisions like have as Much data again as the previous set so like each layer of the tree can take like twice as long as the previous layer So it could definitely speed up training, and it could definitely make it generalize better So then the third one that we had was max features Who wants to tell me what max features? does I'm going to pass that back over there Okay, Vinay Which is determines how many features you're going to use in each tree in this case? It's a fraction up, so you're going to use half of the features for each tree Nearly right or kind of right can you be more specific or can somebody else be more specific? It's not exactly for each tree Chen Shi That is it for each tree randomly sample half of the Features so not quite it's not for each tree so the the set don't pass it to Karen so the set are of samples picks a Picks a subset of samples a subset of rows for each tree But min samples leaf sorry that max features doesn't quite do that is that something different At each split we will be at each split set Split it will Yeah, right So it kind of sounds like a small difference, but it's actually quite a different way of thinking about it Which is we do our set RF samples so we pull out our sub sample or a bootstrap sample And that's kept for the whole tree And we have all of the columns in there right and then with max Features equals point five at each point within at each split we pick a different half of The features and then here will take a pick a different half of the features and here will pick a different half of the Features and so the reason we do that is because we want the trees to be as as rich as possible, right? So particularly like if you if you were only doing a small number of trees like you had only ten trees and And you picked the same column set all the way through the tree You're not really getting much variety and what kind of things are confined okay, so this this way at least in theory Seems to be something which is going to give us a better set of trees is picking a different random subset of features at every decision point So the overall effect of max features again It's the same it's going to mean that the each individual tree is probably going to be less accurate But the trees are going to be more varied and in particular here This can be critical because like imagine that you've got one feature. That's just super predictive It's so predictive that like every random sub sample you look at always starts out by splitting on that same Feature then the trees are going to be very similar in the sense like they all have the same initial split, right? but There may be some other interesting initial splits because they create different interactions of variables so by like half the time That feature won't even be available at the top of the tree so half at least half the trees are going to have a different Initial split so it definitely can give us more Variation and therefore again it can help us to create more generalized trees that have less correlation with each other Even though the individual trees probably won't be as predictive in Practice we actually looked at have a little picture of this that as as you add more trees Right if you have max features equals none that's going to use all the features every time right then with like very very few trees That can still give you a pretty good error But as you create more trees It's not going to help as much because they're all pretty similar because they're all trying every single variable Where else if you say max features equals square root or max pictures equals log 2 then as we add more estimators We see improvements, okay, so there's an interesting interaction between those two and this is from the sklearn docs this cool little chart Okay So then things which don't impact our training at all and jobs Simply says how many CPU how many cores do we run on okay? So it'll make it faster up to a point generally speaking making this more than like eight or so they may have diminishing returns Minus one says use all of your cores So there's I don't know why the default is to only use one core that seems weird to me You'll definitely get more performance by using more cores because all of you have computers with more than one core nowadays And then our B score equals true simply Allows us to see the OOB score if you don't say that it doesn't calculate it and Particularly if you had set RF samples pretty small compared to a big data set OOB is going to take forever to calculate Hopefully at some point. We'll be able to fix the library so that doesn't happen There's no reason it need be that way, but right now. That's that's how the library works Okay so there our Base you know key basic parameters that we can change there are More that you can see in the docs or shift tab to have a look at them But the ones you've seen are the ones that I've found useful to play with so feel free to play with others as well And generally speaking you know max features of as I said max features of like either None Means all of them About point five or Square root Or log you know kind of those Trees seem to work pretty well and then for min samples leaf You know I would generally try kind of one three five ten twenty five You know hundred and like as you start doing that if you notice by the time you get to ten It's already getting worse So there's no point going further if you get to a hundred it's still going better, and you can keep trying right but they're the kind of General amounts that most things in to sit in All right, so random forest interpretation is something which You could use to create some really cool kaggle kernels now obviously one issue is the fastai Library is not available in kaggle kernels, but if you look inside fastai.structured Right remember you can just use Double question mark to look at the source code for something or you can go into the editor to have a look at it You'll see that most of the methods We're using are a small number of lines of code in this library and have no dependencies on anything So you could just copy that that all if you need to use one of those functions just copy it into your kernel And and if you do just say this is from the fastai library you can link to it on github because it's available on github It's open source, but you don't need to Import the whole thing right so this is a cool trick is that because you're the first people to learn how to use these tools You can start to show things that other people haven't seen right so for example This confidence based on tree variance is something which doesn't exist anywhere else Feature importance definitely does and that's already in quite a lot of kaggle kernels If you're looking at a competition or a data set that when nobody's done feature importance Being the first person to do that is always going to win lots of votes because it's like the most important thing is Like which features are important? So Last time we let's just make sure we've got our tree data So we need to change this to add one extra thing alright, so that's going to load in that data It is our data, okay So As I mentioned when we do a model interpretation I tend to set RF samples to some subset something small enough that I can run a model in under 10 seconds or so Because there's just no point run running a super accurate model 50,000 is more than enough To to see you'll basically see each time you run an interpretation You'll get the same results back and so as long as that's true, then you you're already using enough data, okay? So feature importance we learned it works by randomly shuffling a column Each column one at a time and then seeing how accurate the model the pre trained model the model We've already built is when you pass it in all the data as before but with one column shuffle so Some of the questions I got after class kind of reminded me that it's very easy to under appreciate how Powerful and kind of magic this approach is and so to explain I'll mention a couple of the questions that I heard so one question was like Why don't we or what if we just create took one column at a time and created a tree on? Just each one column at a time, so we've got our data set. It's got a bunch of columns So why don't we just like grab that column and just build a tree from that right and then like we'll see which which columns Tree is the most predictive Can anybody tell me? Why what why that may give misleading results about feature importance? Okay Features yeah, if we just shuffle them It will be at randomness and we were able to both capture the interactions and the importance of the future it's great Yeah, and and so This issue of interactions is not a minor detail. It's like It's massively important. So I like think about this Bulldozers data set where for example where there's one field called year made and there's one field called sale date and like If we think about it It's pretty obvious that what matters is the combination of these two which in other words is like How old is the piece of equipment when it got sold? So if we only included one of these? We're going to massively underestimate how important that feature is now Here's a really important point though if you It's pretty much always possible to create a simple like logistic regression Which is as good as pretty much any random forest if you know ahead of time Exactly what variables you need exactly how they interact exactly how they need to be transformed and so forth right so in this case for example We could have created a new field which was equal to year made So sale date or sale year minus year made and we could have fed that to a model and got you know Got that interaction for us, but the point is we never know that like you You never like you might have a guess of it I think some of these things are interacted in this way And I think this thing we need to take the log and so forth But you know the truth is that the way the world works the causal structures You know they've got many many things interacting in many many subtle ways right and so that's why using trees Whether it be gradient boosting machines or random forests works, so well So can you pass that to Terrence place? One thing that bit me years ago was also I tried that Doing one variable at a time thinking oh well. I'll figure out which one's most correlated with the dependent variable, but what it doesn't Pull apart is that what if all variables are basically copied the same variable then they're all going to seem equally important But in fact, it's really just one factor Yeah, and that's also true here, so if we had like a column appeared twice Right then shuffling that column isn't going to make the model much worse right there'll be if you think about like how it's built Some of the times particularly if we had like max features is point five and some of the times We're going to get version a of the column some of the times you get going to get version B of the column so like half the time Shuffling cut version a of the column is going to make a tree a bit worse half the time It's going to make you know column B. I'll make it a bit worse, and so it'll show that both of those features are somewhat important And it'll kind of like share the importance between the two features, and so this is why I'll write Collinearity but collinearity literally means that they're linearly related so this isn't quite right But this is why having two variables that are related closely related to each other or more variables that are closely related to each other Means that you will often Underestimate their importance using this this random forest technique Yes Terrence and so once we've shuffled and we get a new model What exactly are the units of these importances is this a change in the R squared? Yeah? I mean it depends on the library. We're using so the units are kind of like I Never think about them. I just kind of know that like in this particular library You know point oh oh five is often kind of a cutoff I would tend to use but all I actually care about is is this picture right which is the feature importance Ordered for each variable and then kind of zooming in turning into a bar plot, and I'm kind of like okay, you know Here they're all pretty flat, and I can see okay That's about point oh oh five and so I remove them at that point and just see like the model Hopefully the validation score didn't get worse, and if it did get worse So I just increase this a little bit sorry decrease this a little bit until it it doesn't get worse So yeah the the the the units of measure of this don't matter too much And we'll learn later about a second way of doing variable importance by the way can you pass that over there? Is one of the goals here to remove variables that I guess Your study your score will not Get worse if you remove them, so you might as well get rid of them. Yeah, so that's what we're going to do next so So what having looked at our feature importance plot we said okay? It looks like the ones like less than point oh oh five You know a kind of this long tail of boringness So I said let's try removing them right so let's just try grabbing the columns where it's greater than point oh oh five And I said let's create a new data frame called DF keep which is DF train with just those kept columns created a new training and validation set with just those columns created a new random forest, and I looked to see how the Validation set score and the validation set RMSE changed, and I found they got a tiny bit better So if they're about the same or a tiny bit better than the thinking my thinking is well This is just as good a model, but it's now simpler and so now when I redo the feature importance There's less collinearity Right and so in this case I saw that year made went from being like quite a bit better than the next best thing which was coupler system to Way better than the next best thing right and coupler system went from being like quite a bit more important than the next two to Equally important to the next two so it did seem to definitely change these feature importances And hopefully give me some more insight there So How does that help our model in general like what does it mean that you're made is now way ahead of the others? Yeah, so we're going to dig into that kind of now, but basically It tells us That for example if we're looking for like how are we dealing with missing values is there noise in the data? You know if it's a high cardinality categorical variable they're all different steps We were take so for example if it was a high cardinality categorical variable that was originally a string right like for example I think like maybe fi product class description. I remember one of the Ones we looked at the other day had like first of all was the type of vehicle and then a hyphen and then like the size Of the vehicle we might look at that and be like okay. Well that was an important column Let's try like splitting it into two on hyphen and then take that bit Which is like the size of it and trying you know pause it and convert convert it into an integer You know we can try and do some feature engineering and basically until you know which ones are important You don't know where to focus that feature engineering time you can talk to your client You know and say you know or you know if you're doing this inside your workplace You go and talk to the folks that like were responsible for creating this data so in this if you were actually working at a bulldozer auction company you might now go to the actual auctioneers and say I'm really surprised the coupler system seems to be driving people's pricing decisions so much Why do you think that might be and they can say to you? Oh, it's actually because only these classes of vehicles have coupler systems or only this manufacturer has coupler systems and so frankly This is actually not telling you about coupler systems, but about something else and oh hey that reminds me That's that that's something else. We actually have measured that it's in this different CSV file I'll go get it for you, but kind of helps you focus your attention So I had a fun little problem this weekend as you know I introduced a couple of crazy computations in Into my random forest and all of a sudden they're like oh my god These are the most important variables ever squashing all of the others then I got a terrible score and then is that because Now that I think I have my scores Computed correctly what I noticed is that the importance went through the roof, but the validation set was still bad or got worse is that because somehow that computation allowed the training To almost like an identifier map exactly what the answer was going to be for training, but of course that doesn't Generalize to the validation set is that what I is that what I observed Okay, so this There's two reasons why your validation score Might not be very good. Um, let's go up here Okay, so we got these five numbers right the RMSE of the training validation R squared of the training validation and the R squared of the ORP Okay, so the first thing that we're going to do is we're going to take a look at the Okay, so there's two reasons and really in the end what we care about like for this Kaggle competition is the RMSE of the validation set Assuming we've created a good validation set so in Terrence's case He's saying this number is this thing I care about Got worse when I did some feature engineering. Why is that? Okay There's two possible reasons Reason one is that you're overfitting if you're overfitting Then your OOB Will also get worse If you're doing a huge data set with a small set RF sample, so you can't use an OOB then instead Create a second validation set which is a random sample Okay, and and do that right? So in other words if your OOB or your random sample validation set is Has got much worse, then you must be overfitting I Think in your case Terrence, it's unlikely that's the problem because random forests Don't overfit that badly like it's very hard to get them to overfit that badly Unless you use some really weird parameters like only one estimator for example like once you've got ten trees in there There should be enough variation that you know you can definitely overfit But not so much that you're going to destroy your validation score by adding a variable So I think you'll find that's probably not the case, but it's easy to check and if it's not the case Then you'll see that your OOB score or your random sample validation score hasn't got worse, okay? So the second reason your validation score can get worse if your OOB score hasn't got worse You're not overfitting, but your validation score has got worse that means you're you're doing something that is true in the training set But not true in the validation set So this can only happen when your validation set is not a random sample So for example in this bulldozer competition or in the grocery shopping competition. We've intentionally made a validation set That's for a different date range. It's for the most recent two weeks, right? And so if something different happened in the last two weeks to the previous weeks then You could totally Break your validation set so for example If there was some kind of unique identifier which is like Different in the two date periods then you could learn to identify things using that identifier in the training set But then like the last two weeks may have a totally different set of IDs for the different set of behavior Could get a lot worse Yeah, what you're describing is not common though and so I'm a bit skeptical it might be a bug but Hopefully there's enough things you can now use to figure out if it is a bug we'll be interested to hear what you learn Okay so that's that's feature importance and so I'd like to compare that to how feature importance is normally done in industry and in academic communities outside of machine learning like in psychology and economics and so forth and Generally speaking people in those kind of environments tend to use Some kind of linear regression logistic regression general linear models So they start with their data set and they basically say that was weird Oh, okay, so they start with their data set And they say I'm going to assume that I know the kind of parametric relationship between my independent variables and my dependent variable so I'm going to assume that it's a linear relationship say or it's a linear relationship with a link function like a sigmoid you get to create logistic regression say and so Assuming that I already know that I can now write this as an equation So if we've got like x1 x2 so forth right I can say alright my y values are equal to a x1 plus b x2 Equals y and therefore I can find out the feature importance easily enough by just looking at these Coefficients and saying like which one's the highest particularly if you've normalized the data first right so There's this kind of trope out there. It's it's very common. Which is that like this is somehow More accurate or more pure or in some way better way of doing feature importance But that couldn't be further from the truth right if you think about it If you were like if you were missing an interaction Right or if you were missing a transformation you needed Or if you've anyway Been anything less than a hundred percent perfect in all of your pre-processing so that your Model is the absolute correct truth of this situation right unless you've got all of that correct then your coefficients are wrong All right your coefficients are telling you in your totally wrong model. This is how important those things are right which is basically meaningless so We're also the random forest feature importance. It's telling you in this extremely High parameter highly flexible functional form with few if any statistical assumptions. This is your feature importance right So I would be very cautious you know and and again I can't stress this enough one You when you leave and sand when you leave this program you are much more often going to see people talk about logistic regression coefficients than you're going to see them talk about random forest variable importance and Every time you see that happen you should be very very very skeptical If what you're saying anytime you read a paper in economics or in psychology or the marketing department tells you they did this regression or whatever every single time Those coefficients are going to be massively biased by any issues in the model Furthermore If they've done so much pre-processing that actually the model is pretty accurate then now you're looking at coefficients that are going to be of like a coefficient of some principal component from a PCA or a coefficient of some Distance from some cluster or something at which point they're very very hard to interpret anyway They're not actual variables right so they're kind of the two options I've seen when people try to use classic statistical techniques to do or cover a variable importance equivalent I Think things are starting to change Slowly you know there are there are some fields that are starting to realize that this is totally the wrong way to do things But it's been You know nearly 20 years since random forests appeared, so it takes a long time. You know people say that the only way that Knowledge really advances is when the previous generation dies, and that's kind of true right like particularly academics, you know they make a career of being good at a particular sub thing and You know often don't it you know it's not until the next generation comes along that that people notice that oh That's actually a longer a good way to do things, and I think that's what's happened here Okay so We've got now a model which isn't really any better as a predictive accuracy wise But it's kind of we're getting a good sense that there seems to be like four main important things when it was made the capital system its size and Its product classification okay, so that's cool There Is something else that we can do however, which is we can do something called one hot encoding? So this is kind of where we're talking about categorical variables, so remember a categorical variable. Let's say we had like a string high And remember the order we got was kind of back weird it was high low medium, so it was in alphabetical order by default right? Was our original category for like usage band or something and so we mapped it to zero one Two right and so by the time it gets into our data frame. It's now a number So the random forest doesn't know that it was originally a category. It's just a number right so when the random forest is built It basically says oh is it Greater than one or not or is it greater than naught or not you know basically the two possible decisions it could have made For For something with like five or six bands you know it Could be that just one of the levels of a category is actually interesting right so like if it was like very high very low Or or unknown Right then we've never like six levels and maybe The only thing that mattered was whether it was like unknown Maybe like not knowing its size somehow impacts the price and so if we wanted to be able to recognize that and Particularly if like it just so happened that the way that the numbers were coded was it unknown ended up in the middle Right then what it's going to do is it's going to say okay? There is a difference between these two groups. You know less than or equal to two versus greater than two And then when it gets into this this leaf here It's going to say oh there's a difference between these two between less than four and greater than or equal to four and it's going to Take two splits to get to the point where we can see that it's actually unknown that matters So this is a little inefficient And we're kind of like wasting tree computation and like wasting tree computation matters because every time we do a split We're having the amount of data at least that we have to do more analysis So it's going to make our tree less rich less effective if we're not giving the data in a way That's kind of convenient for it to do the work it needs to do so what we could do instead is Create six columns We could create a column called is very high is very low is high is Is unknown is low is medium and each one would be ones and zeros right so either one or zero So we had six columns just one moment So having added six additional columns to our data set the random forest Now has the ability to pick one of these and say like oh, let's have a look at is unknown There's one possible split I can do which is one versus zero Let's see if that's any good right so it actually now has the ability in a single step to pull out a single category level and so This this kind of coding is called one hot encoding and for many many types of machine learning model this is like Necessary something like this is necessary like if you're doing logistic regression You can't possibly put in a categorical variable that goes not through five because there's obviously no really linear relationship between that and anything right so one hot encoding a Lot of people incorrectly assume that all machine learning requires one hot encoding But in this case, I'm going to show you how we could use it optionally and see whether it might improve things sometimes yeah Hi Jeremy, so if we have six categories like in this case would there be any problems with adding a column for each of the? Categories so because in linear regression we said we had to do it like if there's six categories We should only do it for five of them. Yeah, so Um it you certainly can say oh wait Let's not worry about adding is medium because we can infer it from the other five I would say include it anyway because like rather than the otherwise the random forest would have to say is Very high no is very low no is high no is unknown low is low no okay, and finally on on there, right? So it's like five decisions to get to that point so the reason in Linear models that you need to not include one is because linear models hate collinearity But we don't care about about that here So we can do one hot encoding easily enough and the way we do it is we pass One extra parameter to proc df which is what's the max? Number of Categories right so if we say it's seven then anything with less than seven levels Is going to be turned into one hot encoded bunch of columns right so in this case? This has got six levels, so this would be one hot encoded Where else like zip code has more than six levels and so that would be left as a number and so generally speaking you obviously Probably wouldn't want a one hot encode zip code right because that's just going to create masses of data memory problems Computation problems and so forth right so so this is like another parameter that you can play around with so If I do that Try it out run the random forest as per usual you can see what happens to the R-squared of the validation set and to the RMSE of the validation set and in this case I found it got a little bit worse This isn't always the case, and it's going to depend on your data set You know it do you have a data set where you know single categories tend to be quite important? Or not in this particular case it didn't make it more predictive however What it did do is that we now have different features right so the proxy F puts the name of the variable and then an underscore and then the level name and So interestingly it turns out that where else before it said that Enclosure was somewhat important When we do it as one hot encoded it actually says enclosure E ROTS with a C is the most important thing so For at least the purpose of like interpreting your model you should always try one hot encoding you know Quite a few of your variables, and so I often find somewhere around six or seven is pretty good You can try like making that number as high as you can So that it doesn't take forever to compute and the feature importance doesn't include like Really tiny levels that aren't interesting so that's kind of up to you to play it play around with But in this case like this is actually I found this very interesting it clearly tells me I need to find out what enclosure E ROTS with a C is Why is it important because like it means nothing to me right? And but it's in the most important thing so I should go figure that out Savannah had a question So can you explain how changing the next number of categories works because for me it just seems like there's five categories There's five categories. Oh, yeah, sorry, so it's it's just like All it's doing is saying like okay. Here's a column called zip code here's a column called usage band and Here's a column Sex right I don't know whatever right and so like zip code has whatever 5,000 levels the number of levels in a category we call its cardinality Okay So it has a cardinality of 5,000 usage band maybe has a cardinality of six sex has maybe a cardinality of two So when Procter F goes through and says okay This is a categorical variable should I one hot encoded it checks the cardinality Against max and cats and says all 5,000 is bigger than 7 So I don't want hot encoded and then it goes to usage band 6 is less than 7 I do one hot encoded goes to sex 2 is less than 7. I do one encoded so it just says for each variable How do I decide whether the one hot encoded or not? No, once we decide to one hot encode it does not keep the original variable Well, you don't need a labeling code if the if so if the best is an interval it can approximate that With multiple one hot encoding levels Yeah, so like you know, it's a The the truth is that each column is going to have some You know Different you know should it be labeling coded or not you know which you could make on a case-by-case basis. I find in practice It's just not that sensitive to this and so I find like just Using a single number for the whole data set gives me what I need But you know if you were Building a model that really had to be as awesome as possible And you had lots and lots of time to do it you can go through man You know don't use property if you can go through manually and decide which things to use dummies or not You'll see in the code if you look at the code for prop DF Proc DF Right like I never want you to feel like The code that happens to be in the fast AI library is the code that you're limited to right? So where is that done? You can see that The max NCAT gets passed to numerical eyes and numerical eyes Simply Checks okay is that a numeric type and is the number of categories either not been passed to us at all or We've got more unique Values than there are categories and if so we're going to use the categorical codes So for any column where that's where it's skipped over that right so it's remained as a category Then at the very end we just go pandas dot get dummies we pass in the whole data frame and so pandas dot get dummies You pass in the whole data frame it checks for anything That's still a categorical variable and it turns it into a dummy variable Which is another way of saying a one-hot encoding so you know with that kind of approach you can easily Override it and do your own dummy verification variable ization Did you have a question for so some data has Quite obvious order like if you have like a rating system like good bad Poor or whatever things like that There's an order to that and showing that order by doing the dummy variable thing Probably will work to your benefit So is there a way to just force it to leave alone one variable just like Convert it beforehand yourself Not not in the library And to remind you like unless we explicitly do something about it. We're not going to get that order so when we When we import the data This is in lesson 1 RF We showed how By default the categories are ordered alphabetically And we have the ability to order them Properly so yeah, if you've actually made an effort to turn your ordinal variables into proper ordinals Using prop DF Can destroy that if you have max in cats so the simple thing the simple way to avoid that is if we know that we always want to use the codes for usage band rather than the You know like never one hot encoded you could just go ahead and replace it right you could just say okay Let's just go DF dot usage band equals DF dot usage band dot cat dot codes, and it's now an integer And so it'll never get changed All right, so So We kind of have already seen how Variables which are basically measuring the same thing can kind of confuse our variable importance And there can also make our random forests slightly less good because it requires like more computation to do the same thing There's more columns to check So I'm going to do some more work to try and remove redundant features And the way I do that is to do something called a dendrogram And it's a kind of hierarchical clustering so cluster analysis Is something where you're trying to look at objects? They can be either rows in a data set or columns and find which ones are similar to each other so often You'll see people particularly talking about cluster analysis. They normally refer to rows of data, and they'll say like oh, let's plot it Right and like oh, there's a cluster and there's a cluster right a Common type of cluster analysis time to permitting we may get around to talking about this in some detail is called k-means Which is basically where you assume that you don't have any labels at all and you take basically a a couple of data points at random and you gradually Find the ones that are near to it and move them closer and closer to centroids and you kind of repeat it again and again And it's an iterative approach that you basically tell it how many clusters you want and it'll tell you where it thinks the clusters are I really and I don't know why but I really underused technique 20 30 years ago. It was much more popular than it is today is Hierarchical clustering Hierarchical Also known as a glomerative clustering and in hierarchical or agglomerative clustering We basically look at every pair of option of every pair of objects and say okay, which two objects are the closest? Right. So in this case we might go, okay Those two objects are the closest and so we've kind of like delete them and replace it with the midpoint of the two and then Okay, here are the next two closest we delete them and replace them with the midpoint of the two and you keep doing that Again and again, right since we kind of removing points and replacing them with their averages You're gradually reducing a number of points By pairwise combining and the cool thing is you can plot that like so, right? So if rather than looking at points you look at variables we can say okay Which two variables are the most similar that says okay sale year and sale elapsed are very similar so the kind of horizontal axis here is How similar are the two points that are being compared right? So if they're closer to the right, it means they're very similar So say oh year and sale elapsed have been combined and they were very similar What do you measure? Again it's like who cares, you know, it'll be like the correlation coefficient or something like that, you know in this particular case what I actually did So you get to tell it so in this case, I actually used Spearman's are so You guys familiar with correlation coefficients already so correlation is cut is almost exactly the same as the r squared, right? But it's between two variables rather than a variable and its prediction the problem with normal correlation is that If the I've got a new workbook here, um If you have data that looks like this then you can Do a correlation and you'll get a good result, right? But if you've got data which looks like This right and you try and do a correlation it assumes linearity that's not very good, right? So there's a thing called a rank correlation a really simple idea. It's replace every point By its rank, right? So instead of like so we basically say, okay, this is the smallest so we'll call that one Two there's the next one three. Here's the next one four Five, right? So you just replace every number by its rank, right? And then you do the same for the y-axis So we'll call that one two Three and so forth, right? And so then you do it like a new plot where you don't plot the data But you plot the rank of the data and if you think about it the rank of this data set is going to look An exact line because every time something was greater on the x-axis. It was also greater on the y-axis Y-axis so if we do a correlation on the rank that's called a rank correlation Okay, and so because I want to find the Columns that are similar in a way that the random forest would find them similar Random forests don't care about linearity. They just care about ordering So a rank correlation is the the right way to think about that. So Spearman's are is is the name of the most common rank correlation But you can literally replace the data with its rank and chuck it at the regular correlation and you'll get basically The same answer the only difference is in how ties are handled. It's a pretty minor issue Like if you had like a full parabola in that rank correlation, you know We will not right right. It has to be has to be monotonic. Okay. Yeah. Yeah Okay, so Once I've got a correlation matrix there's basically a couple of standard steps you do to turn that into a Dendrogram which I have to look up on stack overflow each time I do it You basically turn it into a distance matrix and then you create something that tells you You know which things are connected to which other things hierarchically so this kind of us These two and this step here like just three standard steps that you always have to do to create a dendrogram and So then you can plot it And so alright So say all year and say what's going to be measuring basically the same thing at least in terms of rank which is not surprising Because say elapsed is the number of days since the first day in my data set So obviously these two are nearly entirely correlated with some ties Grouse attracts and hydraulics flow and coupler system all seem to be measuring the same thing and this is interesting because remember coupler system It said was super important right and so this rather supports our hypothesis that it's nothing to do with whether it's a coupler system But whether it's whatever kind of vehicle it is that has these kind of features Product group and product groups desks seem to be measuring the same thing if I base model and if I model desks seem to be measuring the same thing and so once we get past that Everything else like suddenly the things are further away, so I'm probably going to not worry about those So we're going to look into these one two Three four groups that are very similar could you pass that over there? Is it important that graph that the similarity between stick length and Enclosure is higher than with stick length and anything that's higher. Yeah pretty much I mean it it's a little hard to interpret but given that stick length and enclosure don't join up until way over here It would strongly suggest that then that they're a long way away from each other Otherwise you would expect them to have joined up earlier I mean it's it's possible to construct like a synthetic data set where you kind of end up joining things that were close to each other Through different paths So you've got to be a bit careful, but I think it's fair to probably assume that stick length or enclosure are probably very different So they are very different But would they be more similar than for example stick length and sail day of the year? Which is very top No, there's nothing to suggest that here because like the point is to notice where they sit in this tree Right and they both they sit in totally different halves of the tree But really to actually know that the best way would be to actually look at this BM and our correlation matrix Right if you just want to know how similar is this thing to this thing this BM and our correlation matrix tells you that can you? Pass that over there So today's we are passing the data frame right Say again we are passing the data frame or I'll be passing the model to it. This is just a data frame So we're passing in DF keep so that's the data frame Containing the whatever it was 30 or so features that our random forest thought was interesting So there's no random forest being used here the measure the distance measure is being done entirely on rank correlation So what I then do is I take these these groups Right and I create a little function that I call get out of band score right which is it does a random forest for some data frame I Make sure that I've taken that data frame and split it into a training and validation set And then I call fit and return the OOB score right so basically what I'm going to do is I'm going to try removing Each one of these one two three four five six seven eight nine or so Variables one at a time and see which ones I can remove and it doesn't make the OOB score get worse And each time I run this I get slightly different results So actually it looks like last time I had seven things not not eight things So you can see I just do a loop through each of the things that I'm thinking like maybe I could get rid of this Because it's redundant and I print out the column name and the OOB score of a model that is trained after dropping that one column Okay, so the OOB score on my whole data frame is point eight nine and then after dropping each one of these things They're basically none of them get much worse say all elapsed is Getting quite a bit worse than say all year, but like it looks like pretty much everything else I can drop with like only like a third decimal place problem So obviously though you've got to remember the Dendrogram like let's take fi model desk and fi based model Right they're very similar to each other right so what this says isn't that I can get rid of both of them Right I can get rid of one of them because they're basically measuring the same thing Okay, so so then I try it I say okay. Let's try getting rid of one from each group say all year fi based model and grouser tracks Okay, and like let's now have a look. It's like okay. I've gone from point eight nine oh two point eight eight eight It's like again so close as to be meaningless so that sounds good simpler is better So I'm now going to drop those columns from my data frame And then I can try running the full model Again, and I can see you know so reset RS samples Means I'm using my whole data frame my whole bootstrap sample Use 40 estimators, and I've got point nine oh seven okay, so I've now got a Model which is smaller and simpler, and I'm getting a good score for So at this point I've now Got rid of as many columns as I feel I comfortably can ones that either didn't have a good feature importance or were Highly related to other variables and the model didn't get worse significantly with that when I removed them So now I'm at the point where I want to try and really understand my data better by taking advantage of the model And they're going to use something called partial dependence and again This is something that you could like using the Kaggle kernel and lots of people are going to appreciate this because almost nobody knows about Partial dependence, and it's a very very powerful technique What we're going to do is we're going to find out for the features that are important How do they relate to the dependent variable? Right so let's have a look right so let's again since we're doing interpretation We'll set set our samples to 50,000 to run things quickly We'll take our data frame We'll get our feature importance and notice that we're using Max and cat because I'm actually pretty interested in terms of for interpretation and seeing the individual levels And so here's the top ten and so let's try and learn more about those top ten So year made is the second most important so one obvious thing we could do would be to plot year Made Against sale elapsed because as we've talked about already like it just seems to make sense. They're both important But it seems very likely that they kind of combine together to find like how old was the The product when it was sold so we could try plotting year made against sale elapsed to see how they relate to each other and when we do We get this very ugly graph, and it shows us that year made Actually has a whole bunch that are a thousand Right so clearly you know this is where I would tend to go back to the client or whatever and say okay I'm guessing that these bulldozers weren't actually made in the year 1000, and they would presumably say to me Oh, yes, they're ones where we don't know when it was made you know maybe before 1986 We didn't track that or maybe the things that are sold in Illinois You don't have that data provided or or whatever they'll tell us some reason so In order to Understand this plot better. I'm just going to remove them from this interpretation section of the analysis I'm just going to say okay. Let's just grab things where you made is greater than 1930 okay? So let's now look at the relationship between year made and sale price, and there's a really great Package called gg plot Gigi plot originally was an R package gg stands for the grammar of graphics and the grammar of graphics is like this very powerful way of thinking about how to produce Charts in a very flexible way. I'm not going to be talking about it much in this class There's lots of information available online But I definitely recommend it as a great package to use gg plot Which you can pip install it's part of the fast AI environment already Gigi plot in Python has basically the same Parameters and API is the R version the R version is much better documented So you should read its documentation to learn how to use it, but basically you say okay. I want to create a plot of This data frame now when you create plots Most of the data sets you're using are going to be Too big to plot as in like if you do a scatter plot. It'll create so many dots that it's just a big mess And it'll take forever and remember when you're plotting things You just you're you're looking at it right so there's no point plotting something with a hundred million samples When if you only used a hundred thousand samples it's going to be pixel identical Right so that's why I call get sample first so get sample just grabs a random sample. Okay, so I'm just going to grab 500 points For now okay, so I've got to grab 500 points from my data frame I got a plot Year made against sale price AES stands for aesthetic. This is the basic way that you set up your Columns in Gigi plot okay, so this says to plot these columns from this data frame And then you there's this weird thing in Gigi plot where plus means basically add chart elements, okay, so I'm going to add a smoother so Most of the very very often you'll find that a scatter plot is very hard to see what's going on because there's too much randomness Where else a smoother basically creates a little linear regression for every little subset of the graph And so it kind of joins it up and allows you to see a nice smooth curve, okay? so this is like the main way that I tend to look at univariate relationships and And by adding standard error equals true it also shows me the confidence interval of this smoother right So Loess stands for locally weighted regression, which is this idea of like doing kind of like doing lots of little linear regressions So we can see here the relationship between year made and sale price is kind of all over the place right which is like Not really what I would expect. I would I would have expected that more recent Stuff that sold more recently Would probably be like more expensive because of inflation and because they're like more current models and so forth And the problem is that when you look at a univariate relationship like this. There's a whole lot of Collinearity going on a whole lot of interactions that are being lost so for example Why did the price drop? Here is it actually because like things made between 1991 and 1997 Are less valuable or is actually because most of them were also sold during that time and actually there was like maybe a recession then Or maybe it was like the product sold during that time a lot more people were buying Types of vehicle that were less expensive like there's all kinds of reasons for that and so again as Data scientists one of the things you're going to keep seeing is that at the companies that you join people will come to you with With these kind of univariate charts where they'll say like oh my god our sales in Chicago have Disappeared they've got really bad or people aren't clicking on this ad anymore and they'll show you a chart that looks like this and they'll be like what happened and Most of the time you'll find the answer to the question. What happened is that there's something else going on right so actually all in Chicago Last week actually we were doing a new promotion, and that's why our you know revenue went down It's not because people aren't buying stuff in Chicago anymore. It's because the prices were lower for instance So what we really want to be able to do is say well What's the relationship between sale price and year made all other things being equal? So All other things being equal basically means If we sold something in 1990 versus 1980 and it was exactly the same thing to exactly the same person and exactly the same Auction so on and so forth what would have been the difference in price? and so to do that we do something called a partial dependence plot and This is a partial dependence plot. There's a really nice library which nobody's heard of called PDP Which does these partial dependence plots and what happens is this we've got our sample of 500 data points Right and we're going to do something really interesting we're going to take each one of those hundred randomly chosen options and We're going to make a little data set out of it right so like here's our Here's our Come on one up Here's our data set of like 500 auctions and Here's our columns One of which is the thing that we're interested in which is year made so here's year made Okay, and what we're going to do is we're never going to try and create a chart Where we're going to try and say all other things being equal in 1960 How much did Bulldozers cost how much did things cost in options and so the way we're going to do that is we're going to replace the year Made column with 1960 we're going to copy in the value 1960 again and again and again all the way down right so now every row The year made is 1960 and all of the other data is going to be exactly the same And we're going to take our random forest and we're going to pass all this through our random forest to predict the sale price So that will tell us for everything that was auctioned how much do we think it would have been sold for if that thing was made in 1960 and That's what we're going to plot here All right, that's the price we're going to plot here, and then we're going to do the same thing for 1961 All right, we're going to replace all these and do 1961 Yeah so to be clear We've already fit the random forest yes, and then we're just passing a new year and seeing what it determines the price should be yeah So this is a lot like the way we did feature importance, but rather than randomly shuffling the column We're going to replace the column with a constant value All right, so randomly shuffling the column tells us how accurate it is when you don't use that column anymore Replacing the whole column with a constant tells us or estimates for us how much we would have sold that product for In that auction on that day in that place if that product had been made in 1961 All right, so we basically then take the average of all of the sale prices that we calculate from that random forest And so we do it in 1961 and we get this value, right? So what the partial dependence plot here shows us is each of these light blue lines Actually is showing us all 500 lines, so it says for row number one in our data set If we sold it in 1960 we're going to index that to zero right so we'll call that zero right if we sold it in 1970 that particular Auction would have been here if we sold it in 1980 it would have been here if we sold in 1990 It would have been here, so we actually plot all 500 Predictions of how much every one of those 500 Auctions would have gone for if we replace it if we replace it a year made with each of these different values And then then this dark line here is the average Right so this tells us How much would we have sold? on average all of those options for if all of those products were actually made in 1985 1990 1993 1994 and so forth and so you can see what's happened here is at least in the period where we have a reasonable Out of data which is since 1990 this is basically a totally straight line Which is what you would expect right because if it was sold on the same date And it was the same kind of tractor that was sold to the same person in the same auction house Then you would expect more recent vehicles to be more expensive Because of inflation and because they're they're newer Like they're not they're not as secondhand and you would expect that relationship to be roughly linear, and that's exactly what we're finding Okay, so by removing all of these externalities it often allows us to see the truth Much more clearly as a question at the back. Can you pass that back there? You're done, okay so this this partial dependence plot concept is something which is using a random forest to get us a more clear Interpretation of what's going on in our data and so the steps were to first of all Look at the feature importance to tell us like which things do we think we care about and Then to use the partial dependence plot to tell us What's going on on average? right There's another cool thing we can do with PDP is we can use clusters and what clusters does is it uses cluster analysis? to look at all of these each one of the 500 rows and say To some of those 500 rows kind of move in the same way and like we can kind of see it seems like there's a whole Lot of rows that kind of go down and then up and there seems to be a bunch of rows that kind of go up And then go flat like it does seem like there are some kind of different types of behaviors being hidden And so here is the result of doing that cluster analysis, right is we still get the same average But it says here are kind of the five most common shapes that we see And this is where you could then go in and say all right. It looks like some kinds of vehicle actually After 1990 their prices are pretty flat and before that they were pretty linear Some kinds of vehicle are kind of exactly the opposite and so like different kinds of vehicle have these different shapes Right and so this is something you could dig into I think there's one at the back. Oh you're good. Okay? So what we're going to do with this information well the purpose of Interpretation is to learn about a data set and so why do you want to learn about a data set? It's because you it's because you want to do something with it right so in this case It's not so much something if you're trying to win a Kaggle competition I mean it can be a little bit like some of these insights might make you realize. Oh, I could Transform this variable or create this interaction or whatever Obviously feature importance is super important for Kaggle competitions, but this one's much more for like real life You know so this is when you're talking to somebody and you say to them like Okay, those plots you've been showing me which actually say that like there was this kind of dip in prices You know based on like things made between 1990 and 1997 there wasn't really you know actually it was they were in Precreasing there was actually something else going on at that time No, it's basically the thing that allows you to say like So whatever this outcome. I'm trying to drive in my business is this is how something's driving it right so if it's like I'm looking at you know kind of advertising technology. What's driving clicks that I'm actually digging in to say okay This is actually how clicks are being driven. This is actually the variable that's driving it. This is how it's related So therefore we should change our behavior in this way. That's really the goal of any model I guess there's two possible goals one goal of a model is just to get the predictions like if you're doing hedge fund trading You probably just want to know what the price of that equity is going to be if you're doing insurance You probably just want to know how much claims that guy's going to have but probably most of the time You're actually trying to change Something about how you do business how you do marketing how you do logistics So the thing you actually care about is how the things are related to each other All right, I'm sorry Can you explain again when you scroll up and you were looking at the sale price you're may looking at the entire model? And you saw that dip And you said something about that dip didn't signify what we thought it did can you explain why yeah So this is like a classic Boring univariate plot right so this is basically just taking all of the dots all of the auctions plotting year-made against sale price, and we're going to just fitting a rough average through them and so It's true that products made between 1992 and 1997 on average in our data set being sold for less So like very often in business you'll hear somebody look at something like this, and they'll be like oh we should We should stop auctioning equipment that is made in that year in those years because like we're getting less money for for example But if the truth actually is that during those years It's just that people were making more Small industrial equipment where you would expect it to be sold for less and actually our profit on it is just as high for instance or During those years. It's not that it's not things made during those years now would have Would be cheaper. It's that during those years When we were selling things in those years they were cheaper because like there was a recession going on So if you're trying to like actually take some action based on this You probably don't just care about the fact that things made in those years are cheaper on average, but how does that impact? today, you know so So this this approach where we actually say let's try and remove all of these externalities So if something is sold on the same day to the same person of the same kind of vehicle Then actually how does year made impact price and so this basically says for example if I am deciding What to buy at an option then this is kind of saying to me, okay like Getting a more recent vehicle on average really does on average give you more money Which is not what the kind of the naive univariate plot said How's it Tyler So For like this bulldozer bulldozers made in 2010 probably are not Close to the type of bulldozers that were made in 1960 right, and if you're taking something that would be so very different like a 2010 bulldozer and then trying to just drop it to say oh if it was made in 1960 that May cause poor Prediction at a point because it's so far outside absolutely training absolutely so you know I think that's a good point. It's you know it's a limitation Of a random forest is if you're got a kind of data point That's like of a client you know which is kind of like in a part of the space that it's not seen before like Maybe people didn't put air conditioning really in bulldozers in 1960 and you're saying how much would this Bulldozer with air conditioning have gone for in 1960 you don't really have any information to know that so You know you it's a It's it's this is still the best technique I know of but it's it's not perfect And you know you kind of hope that the trees are still going to find some Useful truth even though it hasn't seen that combination of features before but yeah, it's something to be aware of So you can also do the same thing in a PDP Interaction plot and a PDP interaction plot which is really what I'm trying to get to here is like how to sale elapsed and Year made together impact price and so if I do a PDP interaction plot it shows me sale elapsed versus price It shows me year made versus price and it shows me the combination Versus price remember this is always log of price. That's why these prices look weird right and so you can see that the combination of sale Elapsed and year made is as you would expect later dates so more or less time Is Giving me. Oh, sorry. It's the other way around isn't it so the highest prices Those where there's the least elapsed and the most recent year made So you can see here. There's the univariate relationship between sale elapsed and price And here is the univariate relationship between year made and price And then here is the combination of the two It's enough to see like clearly that these two things are driving price together You can also see these are not like simple diagonal lines, so it's kind of some interesting interaction going on And so based on looking at these plots It's enough to make me think oh we should maybe put in some kind of interaction term and see what happens So let's come back to that in a moment, but let's just look at a couple more Remember in this case I did one hot encoding Way back at the top here. I said max n cat equals 7 so I've got like Enclosure erots with AC so if you've got one hot encoded variables You can pass an array of them To pit plot PDP and it'll treat them as a category Right and so in this case. I'm going to create a PDP plot of these three categories. I'm going to call it enclosure And I can see here that enclosure erots with AC on average are more expensive than enclosure erots and And enclosure erots it actually looks like enclosure erots and closure erots are pretty similar Where else erots with AC is higher? So this is you know at this point. You know I'd probably be inclined to hop into Google and like type erots and erots And find out what the hell these things are And here we go So it turns out that erots is enclosed rollover predictive structure and so it turns out that if your Your bulldozer is fully enclosed then optionally you can also get air conditioning So it turns out that actually this thing is telling us whether it's got air conditioning if it's an open structure Then obviously you don't have air conditioning at all so that's what these three levels are and so we've now learnt All other things being equal the same bulldozer sold at the same time Built at the same time sold to the same person is going to be quite a bit more expensive As if it has air conditioning than if it doesn't okay, so again. We're kind of getting this nice interpretation ability and You know now that I spent some time with this data set I've certainly noticed that this you know knowing this is the most important thing you do notice that there's a lot more Air-conditioned bulldozers nowadays, and they used to be and so there's definitely an interaction between kind of date and that So based on that earlier interaction analysis. I've tried First of all setting everything before 1950 to 1950 because it seems to be some kind of missing value I've been set age to be equal to sale year minus year made and so then I try running a random forest on that and Indeed age is now the single biggest thing Sale elapsed is way back down here Year made is back down here, so we've kind of used this to find an interaction But remember of course a random forest can creator it can create an interaction through having multiple split points So we shouldn't assume that this is actually going to be a better result and in practice I actually found when I looked at my score and my RMSC adding age was actually a little worse and we'll see about that later probably in the next lesson Okay So one last thing is tree interpreter so This is also in the category of things that most people don't know exist, but it's super important almost pointless for like Kaggle competitions, but super important for real life and here's the idea let's say you're an insurance company and somebody rings up and you give them a quote and They say oh, that's $500 more than last year why? Okay, so in general you've made a prediction from some model and somebody asks why and And so this is where we use this method called tree interpreter and what tree interpreter does is It allows us to take a particular row So in this case, we're going to pick Row number zero right so here here is row zero right presumably. This is like a year made I don't know what all the codes stand for but like his is all of the columns in row zero What I can do with a tree interpreter is I can go ti dot predict pass in my random forest Pass in my row so this would be like this particular Customers insurance information or this in this case this particular auction right and it'll give me back Three things the first is the prediction from the random forest The second is the bias the bias is basically the average sale price Across the whole original data set right so like remember in our random forest we started with single trees Well we haven't got a draw in there anymore, but remember we started with a single tree in our random forest First and we split it once and then we split that once and then we split that once right and we said like oh What's the average value for the whole data set then? What's the average value for those where the first split was true? And then what's the average value where the next split was also true until eventually you get down to the leaf nodes where you've got? The average value you predict right? So you can kind of think of it this way if this for a single tree if this is our final leaf node right? Maybe we're predicting like nine point one right and then maybe the average log sale price for the whole The whole lot is like ten point two right that's the average for all the options And so you could kind of like work your way down here, so let's go and create this That's actually go and run this so we can see it okay So let's go back and redraw this single tree you'll find like in Jupyter notebooks often a lot of the things we create like Videos progress bars and stuff they don't know how to like save themselves to the file So you'll see just like a little string here, and so you actually have to rerun it to create the string So this was the single tree that we created So the whole data set had an average log sale price of 10.2 The data set for those with coupler system equals true Had an average of 10.3 The data set for coupler system equals true enclosure less than point less than two was nine point nine and Then eventually we get all the way up here and also a model ID less than 4573 It's ten point two so you could kind of like say okay. Why did this particular? Row let's say we had a row that ended up over in this leaf node Why did we predict ten point two well it's because we started with ten point one nine and then because the coupler system was Was less than point five so it was actually false We added About point two to that so we went from ten point one to ten point three right so ten point two to ten point three So we added a little bit because this one is true and then to go from ten point three to nine point nine So because enclosure is less than two we subtracted About point four and then because model ID was less than forty five hundred we added about point seven Right so you could see like with a single tree you could like break down like why is it that we predicted? 10.2 right and it's like at each one of these decision points. We're adding or subtracting a little bit from the value So what we could then do is we could do that for all the trees and Then we could take the average so every time we see enclosure Did we increase or decrease the value and how much by every time we see model ID? did we increase or decrease the value and how much by and so we could take the average of all of those and That's what ends up in this thing called contributions So here is all of our predictors and Here is the value of each and so this is telling us and I've sorted them here that The fact that this thing was made in 1999 was the thing that most negatively impacted our prediction and the fact that the age of the vehicle was 11 years was what most positively impacted I think you actually need to sort after you zip them together. They seem to be sorted negative point five Well, no values are sorted, but then they're just reassigned to the columns in the original order Which is why? Is what's most thank you? Thank you That makes perfect sense Yes, we need to do an index sort Okay Thank you. We'll make sure we fix that by next week So we need to sort columns by the index from contributions So then there's this thing called bias and so the bias is just the the average With before we start doing any splits, right? So if you basically start with the average log of value and then we went down each tree and each time we saw year made we had some impact coupler system some Impact product size some impact and so forth, right? Okay, so I think what we might do is we might come back to because we kind of out of time we might come back to tree interpreter Next time but the basic idea. This is the last this was the last of our key interpretation points and the basic idea is that we want some ability to Not only tell us about the model as a whole and how it works on average But to look at how the model makes predictions for an individual Row and that's what we're doing here. Okay, great. Thanks everybody. See you on Thursday", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.0, "text": " Alright welcome back", "tokens": [2798, 2928, 646], "temperature": 0.0, "avg_logprob": -0.18189356062147352, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0011876916978508234}, {"id": 1, "seek": 0, "start": 3.58, "end": 8.38, "text": " Something to mention somebody asked on the forums really good question was like", "tokens": [6595, 281, 2152, 2618, 2351, 322, 264, 26998, 534, 665, 1168, 390, 411], "temperature": 0.0, "avg_logprob": -0.18189356062147352, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0011876916978508234}, {"id": 2, "seek": 0, "start": 9.120000000000001, "end": 12.38, "text": " How do I deal with version control and notebooks?", "tokens": [1012, 360, 286, 2028, 365, 3037, 1969, 293, 43782, 30], "temperature": 0.0, "avg_logprob": -0.18189356062147352, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0011876916978508234}, {"id": 3, "seek": 0, "start": 13.14, "end": 18.84, "text": " the question was something like every time I change the notebook Jeremy goes and changes it on git and then I do a git pull and", "tokens": [264, 1168, 390, 746, 411, 633, 565, 286, 1319, 264, 21060, 17809, 1709, 293, 2962, 309, 322, 18331, 293, 550, 286, 360, 257, 18331, 2235, 293], "temperature": 0.0, "avg_logprob": -0.18189356062147352, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0011876916978508234}, {"id": 4, "seek": 0, "start": 18.84, "end": 20.84, "text": " I end up with a conflict and", "tokens": [286, 917, 493, 365, 257, 6596, 293], "temperature": 0.0, "avg_logprob": -0.18189356062147352, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0011876916978508234}, {"id": 5, "seek": 0, "start": 21.44, "end": 27.82, "text": " And that's that happens a lot with notebooks because notebooks behind the scenes are JSON files", "tokens": [400, 300, 311, 300, 2314, 257, 688, 365, 43782, 570, 43782, 2261, 264, 8026, 366, 31828, 7098], "temperature": 0.0, "avg_logprob": -0.18189356062147352, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0011876916978508234}, {"id": 6, "seek": 2782, "start": 27.82, "end": 35.38, "text": " Which like every time you run even a cell without changing it it updates that little number saying like what numbered cell this is and so", "tokens": [3013, 411, 633, 565, 291, 1190, 754, 257, 2815, 1553, 4473, 309, 309, 9205, 300, 707, 1230, 1566, 411, 437, 40936, 2815, 341, 307, 293, 370], "temperature": 0.0, "avg_logprob": -0.21149197253552113, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1300568985461723e-05}, {"id": 7, "seek": 2782, "start": 35.38, "end": 38.1, "text": " Now suddenly there's a change and so trying to merge", "tokens": [823, 5800, 456, 311, 257, 1319, 293, 370, 1382, 281, 22183], "temperature": 0.0, "avg_logprob": -0.21149197253552113, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1300568985461723e-05}, {"id": 8, "seek": 2782, "start": 38.66, "end": 40.66, "text": " Notebook changes is a nightmare", "tokens": [11633, 2939, 2962, 307, 257, 18724], "temperature": 0.0, "avg_logprob": -0.21149197253552113, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1300568985461723e-05}, {"id": 9, "seek": 2782, "start": 41.02, "end": 42.1, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.21149197253552113, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1300568985461723e-05}, {"id": 10, "seek": 2782, "start": 42.1, "end": 47.1, "text": " My suggestion like a simple way to do it is is when you're looking at", "tokens": [1222, 16541, 411, 257, 2199, 636, 281, 360, 309, 307, 307, 562, 291, 434, 1237, 412], "temperature": 0.0, "avg_logprob": -0.21149197253552113, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1300568985461723e-05}, {"id": 11, "seek": 2782, "start": 48.66, "end": 50.66, "text": " Some notebook", "tokens": [2188, 21060], "temperature": 0.0, "avg_logprob": -0.21149197253552113, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1300568985461723e-05}, {"id": 12, "seek": 2782, "start": 51.379999999999995, "end": 54.900000000000006, "text": " Like lesson to RF interpretation you want to start playing around with this", "tokens": [1743, 6898, 281, 26204, 14174, 291, 528, 281, 722, 2433, 926, 365, 341], "temperature": 0.0, "avg_logprob": -0.21149197253552113, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1300568985461723e-05}, {"id": 13, "seek": 5490, "start": 54.9, "end": 61.46, "text": " First thing I would do would be to go file make a copy and", "tokens": [2386, 551, 286, 576, 360, 576, 312, 281, 352, 3991, 652, 257, 5055, 293], "temperature": 0.0, "avg_logprob": -0.1854400634765625, "compression_ratio": 1.7095435684647302, "no_speech_prob": 7.071786512824474e-06}, {"id": 14, "seek": 5490, "start": 62.18, "end": 66.88, "text": " Then in the copy say file rename and give it a name that starts with TMP", "tokens": [1396, 294, 264, 5055, 584, 3991, 36741, 293, 976, 309, 257, 1315, 300, 3719, 365, 314, 12224], "temperature": 0.0, "avg_logprob": -0.1854400634765625, "compression_ratio": 1.7095435684647302, "no_speech_prob": 7.071786512824474e-06}, {"id": 15, "seek": 5490, "start": 67.58, "end": 72.5, "text": " So that will hide it from get right and so now you've got your own version of that notebook that you can", "tokens": [407, 300, 486, 6479, 309, 490, 483, 558, 293, 370, 586, 291, 600, 658, 428, 1065, 3037, 295, 300, 21060, 300, 291, 393], "temperature": 0.0, "avg_logprob": -0.1854400634765625, "compression_ratio": 1.7095435684647302, "no_speech_prob": 7.071786512824474e-06}, {"id": 16, "seek": 5490, "start": 72.78, "end": 74.42, "text": " That you can play with okay", "tokens": [663, 291, 393, 862, 365, 1392], "temperature": 0.0, "avg_logprob": -0.1854400634765625, "compression_ratio": 1.7095435684647302, "no_speech_prob": 7.071786512824474e-06}, {"id": 17, "seek": 5490, "start": 74.42, "end": 78.78, "text": " And so if you now do a git pull and see that the original changed it won't conflict with yours", "tokens": [400, 370, 498, 291, 586, 360, 257, 18331, 2235, 293, 536, 300, 264, 3380, 3105, 309, 1582, 380, 6596, 365, 6342], "temperature": 0.0, "avg_logprob": -0.1854400634765625, "compression_ratio": 1.7095435684647302, "no_speech_prob": 7.071786512824474e-06}, {"id": 18, "seek": 5490, "start": 78.78, "end": 81.32, "text": " And you can now see there are two different versions", "tokens": [400, 291, 393, 586, 536, 456, 366, 732, 819, 9606], "temperature": 0.0, "avg_logprob": -0.1854400634765625, "compression_ratio": 1.7095435684647302, "no_speech_prob": 7.071786512824474e-06}, {"id": 19, "seek": 8132, "start": 81.32, "end": 83.32, "text": " There", "tokens": [821], "temperature": 0.0, "avg_logprob": -0.15604288553454213, "compression_ratio": 1.638655462184874, "no_speech_prob": 3.500825869195978e-06}, {"id": 20, "seek": 8132, "start": 84.27999999999999, "end": 90.72, "text": " Are different ways of kind of dealing with this Jupiter notebook get problem like everybody has it one one is there are some hooks", "tokens": [2014, 819, 2098, 295, 733, 295, 6260, 365, 341, 24567, 21060, 483, 1154, 411, 2201, 575, 309, 472, 472, 307, 456, 366, 512, 26485], "temperature": 0.0, "avg_logprob": -0.15604288553454213, "compression_ratio": 1.638655462184874, "no_speech_prob": 3.500825869195978e-06}, {"id": 21, "seek": 8132, "start": 90.72, "end": 96.11999999999999, "text": " You can use it like remove all of the cell outputs before you commit to get but in this case", "tokens": [509, 393, 764, 309, 411, 4159, 439, 295, 264, 2815, 23930, 949, 291, 5599, 281, 483, 457, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.15604288553454213, "compression_ratio": 1.638655462184874, "no_speech_prob": 3.500825869195978e-06}, {"id": 22, "seek": 8132, "start": 96.11999999999999, "end": 100.58, "text": " I actually want the outputs to be in the repo so you can read it on github and see it", "tokens": [286, 767, 528, 264, 23930, 281, 312, 294, 264, 49040, 370, 291, 393, 1401, 309, 322, 290, 355, 836, 293, 536, 309], "temperature": 0.0, "avg_logprob": -0.15604288553454213, "compression_ratio": 1.638655462184874, "no_speech_prob": 3.500825869195978e-06}, {"id": 23, "seek": 8132, "start": 101.44, "end": 106.24, "text": " So it's a minor issue, but it's it's something which catches everybody", "tokens": [407, 309, 311, 257, 6696, 2734, 11, 457, 309, 311, 309, 311, 746, 597, 25496, 2201], "temperature": 0.0, "avg_logprob": -0.15604288553454213, "compression_ratio": 1.638655462184874, "no_speech_prob": 3.500825869195978e-06}, {"id": 24, "seek": 10624, "start": 106.24, "end": 108.24, "text": " Yes", "tokens": [1079], "temperature": 0.0, "avg_logprob": -0.3348310708999634, "compression_ratio": 1.613861386138614, "no_speech_prob": 3.4266435250174254e-05}, {"id": 25, "seek": 10624, "start": 113.83999999999999, "end": 120.03999999999999, "text": " Before we move on to interpretation of the random forest model, I wonder if we could summarize the relationship between the", "tokens": [4546, 321, 1286, 322, 281, 14174, 295, 264, 4974, 6719, 2316, 11, 286, 2441, 498, 321, 727, 20858, 264, 2480, 1296, 264], "temperature": 0.0, "avg_logprob": -0.3348310708999634, "compression_ratio": 1.613861386138614, "no_speech_prob": 3.4266435250174254e-05}, {"id": 26, "seek": 10624, "start": 121.32, "end": 123.32, "text": " hyperparameters on the random forest and its", "tokens": [9848, 2181, 335, 6202, 322, 264, 4974, 6719, 293, 1080], "temperature": 0.0, "avg_logprob": -0.3348310708999634, "compression_ratio": 1.613861386138614, "no_speech_prob": 3.4266435250174254e-05}, {"id": 27, "seek": 10624, "start": 124.28, "end": 129.34, "text": " Effect on you know overfitting and dealing with co-linearity and yeah, yeah, yeah, yeah, yeah", "tokens": [17764, 322, 291, 458, 670, 69, 2414, 293, 6260, 365, 598, 12, 1889, 17409, 293, 1338, 11, 1338, 11, 1338, 11, 1338, 11, 1338], "temperature": 0.0, "avg_logprob": -0.3348310708999634, "compression_ratio": 1.613861386138614, "no_speech_prob": 3.4266435250174254e-05}, {"id": 28, "seek": 10624, "start": 129.34, "end": 131.84, "text": " That sounds like a question born from experience", "tokens": [663, 3263, 411, 257, 1168, 4232, 490, 1752], "temperature": 0.0, "avg_logprob": -0.3348310708999634, "compression_ratio": 1.613861386138614, "no_speech_prob": 3.4266435250174254e-05}, {"id": 29, "seek": 10624, "start": 132.76, "end": 134.76, "text": " absolutely", "tokens": [3122], "temperature": 0.0, "avg_logprob": -0.3348310708999634, "compression_ratio": 1.613861386138614, "no_speech_prob": 3.4266435250174254e-05}, {"id": 30, "seek": 13476, "start": 134.76, "end": 136.76, "text": " so I", "tokens": [370, 286], "temperature": 0.0, "avg_logprob": -0.2558560120432, "compression_ratio": 1.5406976744186047, "no_speech_prob": 6.144102371763438e-06}, {"id": 31, "seek": 13476, "start": 137.79999999999998, "end": 140.04, "text": " Got to go back to lesson 1 rf", "tokens": [5803, 281, 352, 646, 281, 6898, 502, 367, 69], "temperature": 0.0, "avg_logprob": -0.2558560120432, "compression_ratio": 1.5406976744186047, "no_speech_prob": 6.144102371763438e-06}, {"id": 32, "seek": 13476, "start": 140.72, "end": 145.48, "text": " If you're ever unsure about where I am you can always see my top here courses ml1 lesson 1", "tokens": [759, 291, 434, 1562, 32486, 466, 689, 286, 669, 291, 393, 1009, 536, 452, 1192, 510, 7712, 23271, 16, 6898, 502], "temperature": 0.0, "avg_logprob": -0.2558560120432, "compression_ratio": 1.5406976744186047, "no_speech_prob": 6.144102371763438e-06}, {"id": 33, "seek": 13476, "start": 148.28, "end": 152.48, "text": " In terms of the hyperparameters that", "tokens": [682, 2115, 295, 264, 9848, 2181, 335, 6202, 300], "temperature": 0.0, "avg_logprob": -0.2558560120432, "compression_ratio": 1.5406976744186047, "no_speech_prob": 6.144102371763438e-06}, {"id": 34, "seek": 13476, "start": 153.92, "end": 160.2, "text": " Are interesting and I'm ignoring I'm ignoring like pre-processing, but just the actual hyperparameters", "tokens": [2014, 1880, 293, 286, 478, 26258, 286, 478, 26258, 411, 659, 12, 41075, 278, 11, 457, 445, 264, 3539, 9848, 2181, 335, 6202], "temperature": 0.0, "avg_logprob": -0.2558560120432, "compression_ratio": 1.5406976744186047, "no_speech_prob": 6.144102371763438e-06}, {"id": 35, "seek": 16020, "start": 160.2, "end": 162.64, "text": " The", "tokens": [440], "temperature": 0.0, "avg_logprob": -0.23663486578525642, "compression_ratio": 1.6063829787234043, "no_speech_prob": 2.5612630452087615e-06}, {"id": 36, "seek": 16020, "start": 162.64, "end": 166.56, "text": " First one of interest I would say is the set RF samples", "tokens": [2386, 472, 295, 1179, 286, 576, 584, 307, 264, 992, 26204, 10938], "temperature": 0.0, "avg_logprob": -0.23663486578525642, "compression_ratio": 1.6063829787234043, "no_speech_prob": 2.5612630452087615e-06}, {"id": 37, "seek": 16020, "start": 167.44, "end": 170.0, "text": " command which determines how many", "tokens": [5622, 597, 24799, 577, 867], "temperature": 0.0, "avg_logprob": -0.23663486578525642, "compression_ratio": 1.6063829787234043, "no_speech_prob": 2.5612630452087615e-06}, {"id": 38, "seek": 16020, "start": 170.76, "end": 175.83999999999997, "text": " Rows are in each sample so in each tree you're created from how many rows?", "tokens": [497, 1509, 366, 294, 1184, 6889, 370, 294, 1184, 4230, 291, 434, 2942, 490, 577, 867, 13241, 30], "temperature": 0.0, "avg_logprob": -0.23663486578525642, "compression_ratio": 1.6063829787234043, "no_speech_prob": 2.5612630452087615e-06}, {"id": 39, "seek": 16020, "start": 177.6, "end": 179.6, "text": " And each tree", "tokens": [400, 1184, 4230], "temperature": 0.0, "avg_logprob": -0.23663486578525642, "compression_ratio": 1.6063829787234043, "no_speech_prob": 2.5612630452087615e-06}, {"id": 40, "seek": 16020, "start": 180.07999999999998, "end": 188.56, "text": " So before we start a new tree we either bootstrap a sample so sampling with replacement from the whole thing or we pull", "tokens": [407, 949, 321, 722, 257, 777, 4230, 321, 2139, 11450, 372, 4007, 257, 6889, 370, 21179, 365, 14419, 490, 264, 1379, 551, 420, 321, 2235], "temperature": 0.0, "avg_logprob": -0.23663486578525642, "compression_ratio": 1.6063829787234043, "no_speech_prob": 2.5612630452087615e-06}, {"id": 41, "seek": 18856, "start": 188.56, "end": 194.48, "text": " Out a subsample of a smaller number of rows, and then we build a tree from there, so", "tokens": [5925, 257, 2090, 335, 781, 295, 257, 4356, 1230, 295, 13241, 11, 293, 550, 321, 1322, 257, 4230, 490, 456, 11, 370], "temperature": 0.0, "avg_logprob": -0.14815864562988282, "compression_ratio": 1.6958762886597938, "no_speech_prob": 1.9333488125994336e-06}, {"id": 42, "seek": 18856, "start": 197.68, "end": 200.96, "text": " So step one is we've got our whole big data set and", "tokens": [407, 1823, 472, 307, 321, 600, 658, 527, 1379, 955, 1412, 992, 293], "temperature": 0.0, "avg_logprob": -0.14815864562988282, "compression_ratio": 1.6958762886597938, "no_speech_prob": 1.9333488125994336e-06}, {"id": 43, "seek": 18856, "start": 201.28, "end": 207.52, "text": " We grab a few rows at random from it, and we turn them into a smaller data set and then from that", "tokens": [492, 4444, 257, 1326, 13241, 412, 4974, 490, 309, 11, 293, 321, 1261, 552, 666, 257, 4356, 1412, 992, 293, 550, 490, 300], "temperature": 0.0, "avg_logprob": -0.14815864562988282, "compression_ratio": 1.6958762886597938, "no_speech_prob": 1.9333488125994336e-06}, {"id": 44, "seek": 18856, "start": 208.0, "end": 215.08, "text": " We build a tree right so that's the size of that is set RF samples so when we change that size", "tokens": [492, 1322, 257, 4230, 558, 370, 300, 311, 264, 2744, 295, 300, 307, 992, 26204, 10938, 370, 562, 321, 1319, 300, 2744], "temperature": 0.0, "avg_logprob": -0.14815864562988282, "compression_ratio": 1.6958762886597938, "no_speech_prob": 1.9333488125994336e-06}, {"id": 45, "seek": 21508, "start": 215.08, "end": 220.92000000000002, "text": " Let's say this originally had like a million rows, and we said set RF samples", "tokens": [961, 311, 584, 341, 7993, 632, 411, 257, 2459, 13241, 11, 293, 321, 848, 992, 26204, 10938], "temperature": 0.0, "avg_logprob": -0.14322723661150252, "compression_ratio": 1.5, "no_speech_prob": 1.7061779544746969e-06}, {"id": 46, "seek": 21508, "start": 221.36, "end": 225.44, "text": " 20,000 right and then we're going to grow a tree from there", "tokens": [945, 11, 1360, 558, 293, 550, 321, 434, 516, 281, 1852, 257, 4230, 490, 456], "temperature": 0.0, "avg_logprob": -0.14322723661150252, "compression_ratio": 1.5, "no_speech_prob": 1.7061779544746969e-06}, {"id": 47, "seek": 21508, "start": 227.64000000000001, "end": 229.24, "text": " Assuming that", "tokens": [6281, 24919, 300], "temperature": 0.0, "avg_logprob": -0.14322723661150252, "compression_ratio": 1.5, "no_speech_prob": 1.7061779544746969e-06}, {"id": 48, "seek": 21508, "start": 229.24, "end": 236.04000000000002, "text": " The tree remains kind of balanced as we grow it can somebody tell me how many layers deep?", "tokens": [440, 4230, 7023, 733, 295, 13902, 382, 321, 1852, 309, 393, 2618, 980, 385, 577, 867, 7914, 2452, 30], "temperature": 0.0, "avg_logprob": -0.14322723661150252, "compression_ratio": 1.5, "no_speech_prob": 1.7061779544746969e-06}, {"id": 49, "seek": 21508, "start": 236.72000000000003, "end": 242.16000000000003, "text": " Would this tree be and assuming we're growing it until every leaf is of size one yes", "tokens": [6068, 341, 4230, 312, 293, 11926, 321, 434, 4194, 309, 1826, 633, 10871, 307, 295, 2744, 472, 2086], "temperature": 0.0, "avg_logprob": -0.14322723661150252, "compression_ratio": 1.5, "no_speech_prob": 1.7061779544746969e-06}, {"id": 50, "seek": 24216, "start": 242.16, "end": 245.56, "text": " log base two of 20,000", "tokens": [3565, 3096, 732, 295, 945, 11, 1360], "temperature": 0.0, "avg_logprob": -0.21305387898495323, "compression_ratio": 1.4973262032085561, "no_speech_prob": 5.989267606310023e-07}, {"id": 51, "seek": 24216, "start": 249.32, "end": 253.56, "text": " Right okay, so the the depth of the tree", "tokens": [1779, 1392, 11, 370, 264, 264, 7161, 295, 264, 4230], "temperature": 0.0, "avg_logprob": -0.21305387898495323, "compression_ratio": 1.4973262032085561, "no_speech_prob": 5.989267606310023e-07}, {"id": 52, "seek": 24216, "start": 254.76, "end": 260.0, "text": " Doesn't actually vary that much depending on the number of samples right because it's it's", "tokens": [12955, 380, 767, 10559, 300, 709, 5413, 322, 264, 1230, 295, 10938, 558, 570, 309, 311, 309, 311], "temperature": 0.0, "avg_logprob": -0.21305387898495323, "compression_ratio": 1.4973262032085561, "no_speech_prob": 5.989267606310023e-07}, {"id": 53, "seek": 24216, "start": 261.56, "end": 263.56, "text": " related to the log of the size", "tokens": [4077, 281, 264, 3565, 295, 264, 2744], "temperature": 0.0, "avg_logprob": -0.21305387898495323, "compression_ratio": 1.4973262032085561, "no_speech_prob": 5.989267606310023e-07}, {"id": 54, "seek": 24216, "start": 264.71999999999997, "end": 269.8, "text": " Can somebody tell me at the very bottom so once we go all the way down to the bottom how many?", "tokens": [1664, 2618, 980, 385, 412, 264, 588, 2767, 370, 1564, 321, 352, 439, 264, 636, 760, 281, 264, 2767, 577, 867, 30], "temperature": 0.0, "avg_logprob": -0.21305387898495323, "compression_ratio": 1.4973262032085561, "no_speech_prob": 5.989267606310023e-07}, {"id": 55, "seek": 26980, "start": 269.8, "end": 272.08, "text": " Leaf nodes would there be?", "tokens": [32290, 13891, 576, 456, 312, 30], "temperature": 0.0, "avg_logprob": -0.2071369425455729, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.804990905744489e-07}, {"id": 56, "seek": 26980, "start": 275.6, "end": 277.6, "text": " Speak up what?", "tokens": [27868, 493, 437, 30], "temperature": 0.0, "avg_logprob": -0.2071369425455729, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.804990905744489e-07}, {"id": 57, "seek": 26980, "start": 277.64, "end": 283.32, "text": " 20,000 right because every single leaf node has a single thing in it, so we've got", "tokens": [945, 11, 1360, 558, 570, 633, 2167, 10871, 9984, 575, 257, 2167, 551, 294, 309, 11, 370, 321, 600, 658], "temperature": 0.0, "avg_logprob": -0.2071369425455729, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.804990905744489e-07}, {"id": 58, "seek": 26980, "start": 284.12, "end": 288.76, "text": " Obviously a linear relationship between the number of leaf nodes and the size of the sample", "tokens": [7580, 257, 8213, 2480, 1296, 264, 1230, 295, 10871, 13891, 293, 264, 2744, 295, 264, 6889], "temperature": 0.0, "avg_logprob": -0.2071369425455729, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.804990905744489e-07}, {"id": 59, "seek": 26980, "start": 290.2, "end": 293.64, "text": " So when you decrease the sample size", "tokens": [407, 562, 291, 11514, 264, 6889, 2744], "temperature": 0.0, "avg_logprob": -0.2071369425455729, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.804990905744489e-07}, {"id": 60, "seek": 26980, "start": 294.64, "end": 297.48, "text": " It means that there are less kind of", "tokens": [467, 1355, 300, 456, 366, 1570, 733, 295], "temperature": 0.0, "avg_logprob": -0.2071369425455729, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.804990905744489e-07}, {"id": 61, "seek": 29748, "start": 297.48, "end": 302.96000000000004, "text": " Final decisions that can be made right so therefore the tree is is", "tokens": [13443, 5327, 300, 393, 312, 1027, 558, 370, 4412, 264, 4230, 307, 307], "temperature": 0.0, "avg_logprob": -0.17877997051585803, "compression_ratio": 1.7616822429906542, "no_speech_prob": 1.4144717397357454e-06}, {"id": 62, "seek": 29748, "start": 303.92, "end": 309.44, "text": " going to be less rich in terms of what it can predict because it's just making less different individual decisions and", "tokens": [516, 281, 312, 1570, 4593, 294, 2115, 295, 437, 309, 393, 6069, 570, 309, 311, 445, 1455, 1570, 819, 2609, 5327, 293], "temperature": 0.0, "avg_logprob": -0.17877997051585803, "compression_ratio": 1.7616822429906542, "no_speech_prob": 1.4144717397357454e-06}, {"id": 63, "seek": 29748, "start": 309.76, "end": 311.76, "text": " It also is making", "tokens": [467, 611, 307, 1455], "temperature": 0.0, "avg_logprob": -0.17877997051585803, "compression_ratio": 1.7616822429906542, "no_speech_prob": 1.4144717397357454e-06}, {"id": 64, "seek": 29748, "start": 311.76, "end": 315.68, "text": " less binary choices to get to those decisions so therefore", "tokens": [1570, 17434, 7994, 281, 483, 281, 729, 5327, 370, 4412], "temperature": 0.0, "avg_logprob": -0.17877997051585803, "compression_ratio": 1.7616822429906542, "no_speech_prob": 1.4144717397357454e-06}, {"id": 65, "seek": 29748, "start": 316.52000000000004, "end": 322.0, "text": " Setting RF samples lower is going to mean that you overfit less", "tokens": [21063, 26204, 10938, 3126, 307, 516, 281, 914, 300, 291, 670, 6845, 1570], "temperature": 0.0, "avg_logprob": -0.17877997051585803, "compression_ratio": 1.7616822429906542, "no_speech_prob": 1.4144717397357454e-06}, {"id": 66, "seek": 29748, "start": 323.04, "end": 325.84000000000003, "text": " But it also means that you're going to have a less", "tokens": [583, 309, 611, 1355, 300, 291, 434, 516, 281, 362, 257, 1570], "temperature": 0.0, "avg_logprob": -0.17877997051585803, "compression_ratio": 1.7616822429906542, "no_speech_prob": 1.4144717397357454e-06}, {"id": 67, "seek": 32584, "start": 325.84, "end": 327.4, "text": " accurate", "tokens": [8559], "temperature": 0.0, "avg_logprob": -0.2823332997111531, "compression_ratio": 1.6574585635359116, "no_speech_prob": 2.2603144316235557e-06}, {"id": 68, "seek": 32584, "start": 327.4, "end": 330.88, "text": " Individual tree model right and so remember the way", "tokens": [37292, 4230, 2316, 558, 293, 370, 1604, 264, 636], "temperature": 0.0, "avg_logprob": -0.2823332997111531, "compression_ratio": 1.6574585635359116, "no_speech_prob": 2.2603144316235557e-06}, {"id": 69, "seek": 32584, "start": 331.28, "end": 336.47999999999996, "text": " Breiman the inventor of random forest described this is that you're trying to do two things when you build a model", "tokens": [7090, 25504, 264, 41593, 295, 4974, 6719, 7619, 341, 307, 300, 291, 434, 1382, 281, 360, 732, 721, 562, 291, 1322, 257, 2316], "temperature": 0.0, "avg_logprob": -0.2823332997111531, "compression_ratio": 1.6574585635359116, "no_speech_prob": 2.2603144316235557e-06}, {"id": 70, "seek": 32584, "start": 336.88, "end": 338.88, "text": " When you build a model with bagging", "tokens": [1133, 291, 1322, 257, 2316, 365, 3411, 3249], "temperature": 0.0, "avg_logprob": -0.2823332997111531, "compression_ratio": 1.6574585635359116, "no_speech_prob": 2.2603144316235557e-06}, {"id": 71, "seek": 32584, "start": 339.35999999999996, "end": 341.28, "text": " one is that", "tokens": [472, 307, 300], "temperature": 0.0, "avg_logprob": -0.2823332997111531, "compression_ratio": 1.6574585635359116, "no_speech_prob": 2.2603144316235557e-06}, {"id": 72, "seek": 32584, "start": 341.28, "end": 346.12, "text": " Each individual tree or as a scale under say each individual estimator", "tokens": [6947, 2609, 4230, 420, 382, 257, 4373, 833, 584, 1184, 2609, 8017, 1639], "temperature": 0.0, "avg_logprob": -0.2823332997111531, "compression_ratio": 1.6574585635359116, "no_speech_prob": 2.2603144316235557e-06}, {"id": 73, "seek": 32584, "start": 348.79999999999995, "end": 350.28, "text": " Is", "tokens": [1119], "temperature": 0.0, "avg_logprob": -0.2823332997111531, "compression_ratio": 1.6574585635359116, "no_speech_prob": 2.2603144316235557e-06}, {"id": 74, "seek": 32584, "start": 350.28, "end": 351.79999999999995, "text": " as", "tokens": [382], "temperature": 0.0, "avg_logprob": -0.2823332997111531, "compression_ratio": 1.6574585635359116, "no_speech_prob": 2.2603144316235557e-06}, {"id": 75, "seek": 35180, "start": 351.8, "end": 358.68, "text": " Accurate as possible right on the training set so it's like each model is a strong predictive model", "tokens": [5725, 33144, 382, 1944, 558, 322, 264, 3097, 992, 370, 309, 311, 411, 1184, 2316, 307, 257, 2068, 35521, 2316], "temperature": 0.0, "avg_logprob": -0.17844149470329285, "compression_ratio": 1.5847953216374269, "no_speech_prob": 1.602805923539563e-06}, {"id": 76, "seek": 35180, "start": 359.0, "end": 360.92, "text": " but then the", "tokens": [457, 550, 264], "temperature": 0.0, "avg_logprob": -0.17844149470329285, "compression_ratio": 1.5847953216374269, "no_speech_prob": 1.602805923539563e-06}, {"id": 77, "seek": 35180, "start": 360.92, "end": 362.92, "text": " across the estimators", "tokens": [2108, 264, 8017, 3391], "temperature": 0.0, "avg_logprob": -0.17844149470329285, "compression_ratio": 1.5847953216374269, "no_speech_prob": 1.602805923539563e-06}, {"id": 78, "seek": 35180, "start": 368.04, "end": 372.2, "text": " The correlation between them is as low as possible", "tokens": [440, 20009, 1296, 552, 307, 382, 2295, 382, 1944], "temperature": 0.0, "avg_logprob": -0.17844149470329285, "compression_ratio": 1.5847953216374269, "no_speech_prob": 1.602805923539563e-06}, {"id": 79, "seek": 35180, "start": 373.32, "end": 377.84000000000003, "text": " So that when you average them out together you end up with something that generalizes", "tokens": [407, 300, 562, 291, 4274, 552, 484, 1214, 291, 917, 493, 365, 746, 300, 2674, 5660], "temperature": 0.0, "avg_logprob": -0.17844149470329285, "compression_ratio": 1.5847953216374269, "no_speech_prob": 1.602805923539563e-06}, {"id": 80, "seek": 37784, "start": 377.84, "end": 381.64, "text": " So by decreasing the set RF samples number", "tokens": [407, 538, 23223, 264, 992, 26204, 10938, 1230], "temperature": 0.0, "avg_logprob": -0.1344751516977946, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.14387272435124e-06}, {"id": 81, "seek": 37784, "start": 381.71999999999997, "end": 386.47999999999996, "text": " We are actually decreasing the power of the estimator and increasing the correlation", "tokens": [492, 366, 767, 23223, 264, 1347, 295, 264, 8017, 1639, 293, 5662, 264, 20009], "temperature": 0.0, "avg_logprob": -0.1344751516977946, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.14387272435124e-06}, {"id": 82, "seek": 37784, "start": 386.47999999999996, "end": 391.71999999999997, "text": " And so is that going to result in a better or a worse validation set result for you?", "tokens": [400, 370, 307, 300, 516, 281, 1874, 294, 257, 1101, 420, 257, 5324, 24071, 992, 1874, 337, 291, 30], "temperature": 0.0, "avg_logprob": -0.1344751516977946, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.14387272435124e-06}, {"id": 83, "seek": 37784, "start": 392.71999999999997, "end": 398.15999999999997, "text": " It depends right. This is the kind of compromise which you have to figure out when you do", "tokens": [467, 5946, 558, 13, 639, 307, 264, 733, 295, 18577, 597, 291, 362, 281, 2573, 484, 562, 291, 360], "temperature": 0.0, "avg_logprob": -0.1344751516977946, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.14387272435124e-06}, {"id": 84, "seek": 37784, "start": 398.88, "end": 400.88, "text": " machine learning models", "tokens": [3479, 2539, 5245], "temperature": 0.0, "avg_logprob": -0.1344751516977946, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.14387272435124e-06}, {"id": 85, "seek": 37784, "start": 402.32, "end": 404.32, "text": " Can you pass that back there?", "tokens": [1664, 291, 1320, 300, 646, 456, 30], "temperature": 0.0, "avg_logprob": -0.1344751516977946, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.14387272435124e-06}, {"id": 86, "seek": 40432, "start": 404.32, "end": 406.32, "text": " If I wait if I put the OOB value equal to 2", "tokens": [759, 286, 1699, 498, 286, 829, 264, 422, 46, 33, 2158, 2681, 281, 568], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 87, "seek": 40432, "start": 406.32, "end": 410.32, "text": " So it is basically dividing every 30 it ensures that every 30% of the data won't be there in each tree right?", "tokens": [407, 309, 307, 1936, 26764, 633, 2217, 309, 28111, 300, 633, 2217, 4, 295, 264, 1412, 1582, 380, 312, 456, 294, 1184, 4230, 558, 30], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 88, "seek": 40432, "start": 410.32, "end": 412.32, "text": " The OOB say again", "tokens": [440, 422, 46, 33, 584, 797], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 89, "seek": 40432, "start": 412.32, "end": 414.32, "text": " OOB if I put OOB equal to true in random forest", "tokens": [422, 46, 33, 498, 286, 829, 422, 46, 33, 2681, 281, 2074, 294, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 90, "seek": 40432, "start": 414.32, "end": 418.32, "text": " So isn't that make sure that out of my entire data 37% of data won't be there in every tree", "tokens": [407, 1943, 380, 300, 652, 988, 300, 484, 295, 452, 2302, 1412, 13435, 4, 295, 1412, 1582, 380, 312, 456, 294, 633, 4230], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 91, "seek": 40432, "start": 418.32, "end": 420.32, "text": " So all OOB equals true", "tokens": [407, 439, 422, 46, 33, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 92, "seek": 40432, "start": 420.32, "end": 422.32, "text": " So all OOB equals true", "tokens": [407, 439, 422, 46, 33, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 93, "seek": 40432, "start": 422.32, "end": 424.32, "text": " So all OOB equals true", "tokens": [407, 439, 422, 46, 33, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 94, "seek": 40432, "start": 424.32, "end": 426.32, "text": " So all OOB equals true", "tokens": [407, 439, 422, 46, 33, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 95, "seek": 40432, "start": 426.32, "end": 428.32, "text": " So all OOB equals true", "tokens": [407, 439, 422, 46, 33, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 96, "seek": 40432, "start": 428.32, "end": 430.32, "text": " So all OOB equals true", "tokens": [407, 439, 422, 46, 33, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 97, "seek": 40432, "start": 430.32, "end": 432.32, "text": " So all OOB equals true", "tokens": [407, 439, 422, 46, 33, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.3311982456641861, "compression_ratio": 2.371859296482412, "no_speech_prob": 8.479356620227918e-05}, {"id": 98, "seek": 43232, "start": 432.32, "end": 435.8, "text": " So all OOB equals true does is it says?", "tokens": [407, 439, 422, 46, 33, 6915, 2074, 775, 307, 309, 1619, 30], "temperature": 0.0, "avg_logprob": -0.20837342337276157, "compression_ratio": 1.6051282051282052, "no_speech_prob": 5.594305093836738e-06}, {"id": 99, "seek": 43232, "start": 437.09999999999997, "end": 442.2, "text": " Whatever your sub sample is it might be a bootstrap sample or it might be a", "tokens": [8541, 428, 1422, 6889, 307, 309, 1062, 312, 257, 11450, 372, 4007, 6889, 420, 309, 1062, 312, 257], "temperature": 0.0, "avg_logprob": -0.20837342337276157, "compression_ratio": 1.6051282051282052, "no_speech_prob": 5.594305093836738e-06}, {"id": 100, "seek": 43232, "start": 443.71999999999997, "end": 445.4, "text": " Sub-sample", "tokens": [8511, 12, 19988, 781], "temperature": 0.0, "avg_logprob": -0.20837342337276157, "compression_ratio": 1.6051282051282052, "no_speech_prob": 5.594305093836738e-06}, {"id": 101, "seek": 43232, "start": 445.4, "end": 447.4, "text": " Take all of the other rows", "tokens": [3664, 439, 295, 264, 661, 13241], "temperature": 0.0, "avg_logprob": -0.20837342337276157, "compression_ratio": 1.6051282051282052, "no_speech_prob": 5.594305093836738e-06}, {"id": 102, "seek": 43232, "start": 448.56, "end": 453.03999999999996, "text": " right and put them into a freach tree and put them into a different data set and", "tokens": [558, 293, 829, 552, 666, 257, 2130, 608, 4230, 293, 829, 552, 666, 257, 819, 1412, 992, 293], "temperature": 0.0, "avg_logprob": -0.20837342337276157, "compression_ratio": 1.6051282051282052, "no_speech_prob": 5.594305093836738e-06}, {"id": 103, "seek": 43232, "start": 454.0, "end": 459.15999999999997, "text": " Calculate the the error on those so it doesn't actually impact training at all", "tokens": [3511, 2444, 473, 264, 264, 6713, 322, 729, 370, 309, 1177, 380, 767, 2712, 3097, 412, 439], "temperature": 0.0, "avg_logprob": -0.20837342337276157, "compression_ratio": 1.6051282051282052, "no_speech_prob": 5.594305093836738e-06}, {"id": 104, "seek": 45916, "start": 459.16, "end": 464.52000000000004, "text": " It just gives you an additional metric which is the OOB error so if you", "tokens": [467, 445, 2709, 291, 364, 4497, 20678, 597, 307, 264, 422, 46, 33, 6713, 370, 498, 291], "temperature": 0.0, "avg_logprob": -0.2923288072858538, "compression_ratio": 1.4807692307692308, "no_speech_prob": 2.3320560558204306e-06}, {"id": 105, "seek": 45916, "start": 465.56, "end": 467.56, "text": " Don't have a validation set", "tokens": [1468, 380, 362, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.2923288072858538, "compression_ratio": 1.4807692307692308, "no_speech_prob": 2.3320560558204306e-06}, {"id": 106, "seek": 45916, "start": 468.48, "end": 471.64000000000004, "text": " then this allows you to get kind of a", "tokens": [550, 341, 4045, 291, 281, 483, 733, 295, 257], "temperature": 0.0, "avg_logprob": -0.2923288072858538, "compression_ratio": 1.4807692307692308, "no_speech_prob": 2.3320560558204306e-06}, {"id": 107, "seek": 45916, "start": 472.44000000000005, "end": 475.04, "text": " quasi validation set for free", "tokens": [20954, 24071, 992, 337, 1737], "temperature": 0.0, "avg_logprob": -0.2923288072858538, "compression_ratio": 1.4807692307692308, "no_speech_prob": 2.3320560558204306e-06}, {"id": 108, "seek": 45916, "start": 478.88, "end": 480.88, "text": " If you want to", "tokens": [759, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.2923288072858538, "compression_ratio": 1.4807692307692308, "no_speech_prob": 2.3320560558204306e-06}, {"id": 109, "seek": 45916, "start": 481.20000000000005, "end": 483.20000000000005, "text": " Set out a sample", "tokens": [8928, 484, 257, 6889], "temperature": 0.0, "avg_logprob": -0.2923288072858538, "compression_ratio": 1.4807692307692308, "no_speech_prob": 2.3320560558204306e-06}, {"id": 110, "seek": 45916, "start": 484.12, "end": 485.88, "text": " RF sample", "tokens": [26204, 6889], "temperature": 0.0, "avg_logprob": -0.2923288072858538, "compression_ratio": 1.4807692307692308, "no_speech_prob": 2.3320560558204306e-06}, {"id": 111, "seek": 45916, "start": 485.88, "end": 487.88, "text": " so the the default is", "tokens": [370, 264, 264, 7576, 307], "temperature": 0.0, "avg_logprob": -0.2923288072858538, "compression_ratio": 1.4807692307692308, "no_speech_prob": 2.3320560558204306e-06}, {"id": 112, "seek": 48788, "start": 487.88, "end": 490.04, "text": " actually if you say", "tokens": [767, 498, 291, 584], "temperature": 0.0, "avg_logprob": -0.2294724146525065, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.7330430637230165e-06}, {"id": 113, "seek": 48788, "start": 490.8, "end": 493.8, "text": " reset RF samples and", "tokens": [14322, 26204, 10938, 293], "temperature": 0.0, "avg_logprob": -0.2294724146525065, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.7330430637230165e-06}, {"id": 114, "seek": 48788, "start": 495.08, "end": 502.0, "text": " That causes it to bootstrap, so it'll sample a new data set as big as the original one, but with replacement", "tokens": [663, 7700, 309, 281, 11450, 372, 4007, 11, 370, 309, 603, 6889, 257, 777, 1412, 992, 382, 955, 382, 264, 3380, 472, 11, 457, 365, 14419], "temperature": 0.0, "avg_logprob": -0.2294724146525065, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.7330430637230165e-06}, {"id": 115, "seek": 48788, "start": 507.71999999999997, "end": 513.08, "text": " Okay, so obviously the second benefit of set RF samples is that you can run", "tokens": [1033, 11, 370, 2745, 264, 1150, 5121, 295, 992, 26204, 10938, 307, 300, 291, 393, 1190], "temperature": 0.0, "avg_logprob": -0.2294724146525065, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.7330430637230165e-06}, {"id": 116, "seek": 51308, "start": 513.08, "end": 518.1800000000001, "text": " More quickly and particularly if you're running on a really large data set like a hundred million rows", "tokens": [5048, 2661, 293, 4098, 498, 291, 434, 2614, 322, 257, 534, 2416, 1412, 992, 411, 257, 3262, 2459, 13241], "temperature": 0.0, "avg_logprob": -0.14345248027514385, "compression_ratio": 1.6311475409836065, "no_speech_prob": 7.81146241024544e-07}, {"id": 117, "seek": 51308, "start": 518.24, "end": 521.6800000000001, "text": " You know it won't be possible to run it on the full data set", "tokens": [509, 458, 309, 1582, 380, 312, 1944, 281, 1190, 309, 322, 264, 1577, 1412, 992], "temperature": 0.0, "avg_logprob": -0.14345248027514385, "compression_ratio": 1.6311475409836065, "no_speech_prob": 7.81146241024544e-07}, {"id": 118, "seek": 51308, "start": 521.6800000000001, "end": 526.96, "text": " So you would either have to pick a sub sample of you yourself before you start or you set RF samples", "tokens": [407, 291, 576, 2139, 362, 281, 1888, 257, 1422, 6889, 295, 291, 1803, 949, 291, 722, 420, 291, 992, 26204, 10938], "temperature": 0.0, "avg_logprob": -0.14345248027514385, "compression_ratio": 1.6311475409836065, "no_speech_prob": 7.81146241024544e-07}, {"id": 119, "seek": 51308, "start": 528.4000000000001, "end": 533.96, "text": " The second key parameter that we learned about was min samples leaf", "tokens": [440, 1150, 2141, 13075, 300, 321, 3264, 466, 390, 923, 10938, 10871], "temperature": 0.0, "avg_logprob": -0.14345248027514385, "compression_ratio": 1.6311475409836065, "no_speech_prob": 7.81146241024544e-07}, {"id": 120, "seek": 51308, "start": 535.1600000000001, "end": 540.88, "text": " Okay, so if I changed min samples leaf before we assumed that min", "tokens": [1033, 11, 370, 498, 286, 3105, 923, 10938, 10871, 949, 321, 15895, 300, 923], "temperature": 0.0, "avg_logprob": -0.14345248027514385, "compression_ratio": 1.6311475409836065, "no_speech_prob": 7.81146241024544e-07}, {"id": 121, "seek": 54088, "start": 540.88, "end": 542.88, "text": " samples leaf was equal to", "tokens": [10938, 10871, 390, 2681, 281], "temperature": 0.0, "avg_logprob": -0.2502264854235527, "compression_ratio": 1.436046511627907, "no_speech_prob": 9.422421385352209e-07}, {"id": 122, "seek": 54088, "start": 543.6, "end": 545.6, "text": " one", "tokens": [472], "temperature": 0.0, "avg_logprob": -0.2502264854235527, "compression_ratio": 1.436046511627907, "no_speech_prob": 9.422421385352209e-07}, {"id": 123, "seek": 54088, "start": 545.6, "end": 548.12, "text": " All right, if I said it equal to two", "tokens": [1057, 558, 11, 498, 286, 848, 309, 2681, 281, 732], "temperature": 0.0, "avg_logprob": -0.2502264854235527, "compression_ratio": 1.436046511627907, "no_speech_prob": 9.422421385352209e-07}, {"id": 124, "seek": 54088, "start": 549.24, "end": 551.24, "text": " Then what would be my new?", "tokens": [1396, 437, 576, 312, 452, 777, 30], "temperature": 0.0, "avg_logprob": -0.2502264854235527, "compression_ratio": 1.436046511627907, "no_speech_prob": 9.422421385352209e-07}, {"id": 125, "seek": 54088, "start": 552.0, "end": 554.0, "text": " Depth how deep would it be?", "tokens": [4056, 392, 577, 2452, 576, 309, 312, 30], "temperature": 0.0, "avg_logprob": -0.2502264854235527, "compression_ratio": 1.436046511627907, "no_speech_prob": 9.422421385352209e-07}, {"id": 126, "seek": 54088, "start": 557.76, "end": 564.76, "text": " Yes log base to 20,000 minus one okay, so each time we double the min samples leaf", "tokens": [1079, 3565, 3096, 281, 945, 11, 1360, 3175, 472, 1392, 11, 370, 1184, 565, 321, 3834, 264, 923, 10938, 10871], "temperature": 0.0, "avg_logprob": -0.2502264854235527, "compression_ratio": 1.436046511627907, "no_speech_prob": 9.422421385352209e-07}, {"id": 127, "seek": 54088, "start": 564.76, "end": 567.12, "text": " We're removing one layer from the tree", "tokens": [492, 434, 12720, 472, 4583, 490, 264, 4230], "temperature": 0.0, "avg_logprob": -0.2502264854235527, "compression_ratio": 1.436046511627907, "no_speech_prob": 9.422421385352209e-07}, {"id": 128, "seek": 54088, "start": 568.24, "end": 569.56, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2502264854235527, "compression_ratio": 1.436046511627907, "no_speech_prob": 9.422421385352209e-07}, {"id": 129, "seek": 56956, "start": 569.56, "end": 574.7199999999999, "text": " Fine I'll come back to you again since you're doing so well how many leaf nodes would there be in that case?", "tokens": [12024, 286, 603, 808, 646, 281, 291, 797, 1670, 291, 434, 884, 370, 731, 577, 867, 10871, 13891, 576, 456, 312, 294, 300, 1389, 30], "temperature": 0.0, "avg_logprob": -0.200669345571034, "compression_ratio": 1.7046979865771812, "no_speech_prob": 4.356842509878334e-06}, {"id": 130, "seek": 56956, "start": 578.92, "end": 582.28, "text": " What how many leaf nodes would there be in that case?", "tokens": [708, 577, 867, 10871, 13891, 576, 456, 312, 294, 300, 1389, 30], "temperature": 0.0, "avg_logprob": -0.200669345571034, "compression_ratio": 1.7046979865771812, "no_speech_prob": 4.356842509878334e-06}, {"id": 131, "seek": 56956, "start": 586.88, "end": 592.8, "text": " 10,000 okay, so we're going to be again dividing the number of leaf nodes by that number so", "tokens": [1266, 11, 1360, 1392, 11, 370, 321, 434, 516, 281, 312, 797, 26764, 264, 1230, 295, 10871, 13891, 538, 300, 1230, 370], "temperature": 0.0, "avg_logprob": -0.200669345571034, "compression_ratio": 1.7046979865771812, "no_speech_prob": 4.356842509878334e-06}, {"id": 132, "seek": 59280, "start": 592.8, "end": 600.4, "text": " The result of increasing min samples leaf is that now each of our leaf nodes has more than one thing in so we're going to get", "tokens": [440, 1874, 295, 5662, 923, 10938, 10871, 307, 300, 586, 1184, 295, 527, 10871, 13891, 575, 544, 813, 472, 551, 294, 370, 321, 434, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.12810479378213688, "compression_ratio": 1.777292576419214, "no_speech_prob": 7.22441825473652e-07}, {"id": 133, "seek": 59280, "start": 600.4, "end": 605.56, "text": " A more stable average that we're calculating in each tree, okay?", "tokens": [316, 544, 8351, 4274, 300, 321, 434, 28258, 294, 1184, 4230, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.12810479378213688, "compression_ratio": 1.777292576419214, "no_speech_prob": 7.22441825473652e-07}, {"id": 134, "seek": 59280, "start": 609.16, "end": 614.7199999999999, "text": " We've got a little bit less depth okay, we've got less decisions to make and we've got a smaller number of leaf nodes", "tokens": [492, 600, 658, 257, 707, 857, 1570, 7161, 1392, 11, 321, 600, 658, 1570, 5327, 281, 652, 293, 321, 600, 658, 257, 4356, 1230, 295, 10871, 13891], "temperature": 0.0, "avg_logprob": -0.12810479378213688, "compression_ratio": 1.777292576419214, "no_speech_prob": 7.22441825473652e-07}, {"id": 135, "seek": 59280, "start": 614.7199999999999, "end": 620.52, "text": " So again, we would expect the result of that would be that each estimator would be less predictive", "tokens": [407, 797, 11, 321, 576, 2066, 264, 1874, 295, 300, 576, 312, 300, 1184, 8017, 1639, 576, 312, 1570, 35521], "temperature": 0.0, "avg_logprob": -0.12810479378213688, "compression_ratio": 1.777292576419214, "no_speech_prob": 7.22441825473652e-07}, {"id": 136, "seek": 62052, "start": 620.52, "end": 625.76, "text": " But the estimators would be also less correlated", "tokens": [583, 264, 8017, 3391, 576, 312, 611, 1570, 38574], "temperature": 0.0, "avg_logprob": -0.1970830180428245, "compression_ratio": 1.5765765765765767, "no_speech_prob": 1.1659361916827038e-05}, {"id": 137, "seek": 62052, "start": 625.76, "end": 631.4399999999999, "text": " So again this might help us to avoid overfitting could you pass the microphone over here, please?", "tokens": [407, 797, 341, 1062, 854, 505, 281, 5042, 670, 69, 2414, 727, 291, 1320, 264, 10952, 670, 510, 11, 1767, 30], "temperature": 0.0, "avg_logprob": -0.1970830180428245, "compression_ratio": 1.5765765765765767, "no_speech_prob": 1.1659361916827038e-05}, {"id": 138, "seek": 62052, "start": 633.84, "end": 635.84, "text": " Hi, Jeremy, I'm not sure if", "tokens": [2421, 11, 17809, 11, 286, 478, 406, 988, 498], "temperature": 0.0, "avg_logprob": -0.1970830180428245, "compression_ratio": 1.5765765765765767, "no_speech_prob": 1.1659361916827038e-05}, {"id": 139, "seek": 62052, "start": 636.1999999999999, "end": 642.96, "text": " In that case every node will have exactly two no it won't necessarily have exactly two and I thank you for mentioning that", "tokens": [682, 300, 1389, 633, 9984, 486, 362, 2293, 732, 572, 309, 1582, 380, 4725, 362, 2293, 732, 293, 286, 1309, 291, 337, 18315, 300], "temperature": 0.0, "avg_logprob": -0.1970830180428245, "compression_ratio": 1.5765765765765767, "no_speech_prob": 1.1659361916827038e-05}, {"id": 140, "seek": 62052, "start": 643.6, "end": 648.0799999999999, "text": " So it might try to do a split and so one reason well", "tokens": [407, 309, 1062, 853, 281, 360, 257, 7472, 293, 370, 472, 1778, 731], "temperature": 0.0, "avg_logprob": -0.1970830180428245, "compression_ratio": 1.5765765765765767, "no_speech_prob": 1.1659361916827038e-05}, {"id": 141, "seek": 64808, "start": 648.08, "end": 653.9200000000001, "text": " What would be an example ten she that you wouldn't split even if you had a hundred nodes?", "tokens": [708, 576, 312, 364, 1365, 2064, 750, 300, 291, 2759, 380, 7472, 754, 498, 291, 632, 257, 3262, 13891, 30], "temperature": 0.0, "avg_logprob": -0.21705309549967447, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.7330437458440429e-06}, {"id": 142, "seek": 64808, "start": 654.0, "end": 658.0400000000001, "text": " What might be a reason for that sorry a hundred items in a leaf node?", "tokens": [708, 1062, 312, 257, 1778, 337, 300, 2597, 257, 3262, 4754, 294, 257, 10871, 9984, 30], "temperature": 0.0, "avg_logprob": -0.21705309549967447, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.7330437458440429e-06}, {"id": 143, "seek": 64808, "start": 658.64, "end": 661.5200000000001, "text": " They're all the same. They're all the same in terms of", "tokens": [814, 434, 439, 264, 912, 13, 814, 434, 439, 264, 912, 294, 2115, 295], "temperature": 0.0, "avg_logprob": -0.21705309549967447, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.7330437458440429e-06}, {"id": 144, "seek": 64808, "start": 662.76, "end": 665.6800000000001, "text": " Well once the independent saw the dependent", "tokens": [1042, 1564, 264, 6695, 1866, 264, 12334], "temperature": 0.0, "avg_logprob": -0.21705309549967447, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.7330437458440429e-06}, {"id": 145, "seek": 64808, "start": 668.2800000000001, "end": 673.4000000000001, "text": " And it has the dependent right I mean I guess either but much more likely would be the dependent so if you get to a leaf", "tokens": [400, 309, 575, 264, 12334, 558, 286, 914, 286, 2041, 2139, 457, 709, 544, 3700, 576, 312, 264, 12334, 370, 498, 291, 483, 281, 257, 10871], "temperature": 0.0, "avg_logprob": -0.21705309549967447, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.7330437458440429e-06}, {"id": 146, "seek": 64808, "start": 673.4000000000001, "end": 675.4000000000001, "text": " node where", "tokens": [9984, 689], "temperature": 0.0, "avg_logprob": -0.21705309549967447, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.7330437458440429e-06}, {"id": 147, "seek": 67540, "start": 675.4, "end": 680.92, "text": " Every single one of them has the same auction price or in classification like every single one of them is a dog", "tokens": [2048, 2167, 472, 295, 552, 575, 264, 912, 24139, 3218, 420, 294, 21538, 411, 633, 2167, 472, 295, 552, 307, 257, 3000], "temperature": 0.0, "avg_logprob": -0.16966005166371664, "compression_ratio": 1.7644628099173554, "no_speech_prob": 9.818130820349324e-06}, {"id": 148, "seek": 67540, "start": 681.04, "end": 686.34, "text": " Then there is no split that you can do that's going to improve your information right and remember", "tokens": [1396, 456, 307, 572, 7472, 300, 291, 393, 360, 300, 311, 516, 281, 3470, 428, 1589, 558, 293, 1604], "temperature": 0.0, "avg_logprob": -0.16966005166371664, "compression_ratio": 1.7644628099173554, "no_speech_prob": 9.818130820349324e-06}, {"id": 149, "seek": 67540, "start": 687.28, "end": 691.9599999999999, "text": " Information is the term we use in a kind of a general sense in random for us to describe", "tokens": [15357, 307, 264, 1433, 321, 764, 294, 257, 733, 295, 257, 2674, 2020, 294, 4974, 337, 505, 281, 6786], "temperature": 0.0, "avg_logprob": -0.16966005166371664, "compression_ratio": 1.7644628099173554, "no_speech_prob": 9.818130820349324e-06}, {"id": 150, "seek": 67540, "start": 693.24, "end": 695.24, "text": " the amount of", "tokens": [264, 2372, 295], "temperature": 0.0, "avg_logprob": -0.16966005166371664, "compression_ratio": 1.7644628099173554, "no_speech_prob": 9.818130820349324e-06}, {"id": 151, "seek": 67540, "start": 695.3199999999999, "end": 700.62, "text": " Difference about about additional information we create from a split is like how much are we improving the model?", "tokens": [35940, 5158, 466, 466, 4497, 1589, 321, 1884, 490, 257, 7472, 307, 411, 577, 709, 366, 321, 11470, 264, 2316, 30], "temperature": 0.0, "avg_logprob": -0.16966005166371664, "compression_ratio": 1.7644628099173554, "no_speech_prob": 9.818130820349324e-06}, {"id": 152, "seek": 70062, "start": 700.62, "end": 707.92, "text": " So you'll often see this in this word information gain which means like how much better did the model get by adding an additional split point?", "tokens": [407, 291, 603, 2049, 536, 341, 294, 341, 1349, 1589, 6052, 597, 1355, 411, 577, 709, 1101, 630, 264, 2316, 483, 538, 5127, 364, 4497, 7472, 935, 30], "temperature": 0.0, "avg_logprob": -0.16727729915648468, "compression_ratio": 1.8278145695364238, "no_speech_prob": 1.5294064041881938e-06}, {"id": 153, "seek": 70062, "start": 708.44, "end": 714.0600000000001, "text": " And it could be based on RMSE or it could be based on cross entropy or it could be based on how different to the standard", "tokens": [400, 309, 727, 312, 2361, 322, 23790, 5879, 420, 309, 727, 312, 2361, 322, 3278, 30867, 420, 309, 727, 312, 2361, 322, 577, 819, 281, 264, 3832], "temperature": 0.0, "avg_logprob": -0.16727729915648468, "compression_ratio": 1.8278145695364238, "no_speech_prob": 1.5294064041881938e-06}, {"id": 154, "seek": 70062, "start": 714.08, "end": 717.48, "text": " Deviations or or whatever so that's just a general term okay?", "tokens": [48565, 763, 420, 420, 2035, 370, 300, 311, 445, 257, 2674, 1433, 1392, 30], "temperature": 0.0, "avg_logprob": -0.16727729915648468, "compression_ratio": 1.8278145695364238, "no_speech_prob": 1.5294064041881938e-06}, {"id": 155, "seek": 70062, "start": 717.48, "end": 720.08, "text": " So that's the second thing that we can do which again", "tokens": [407, 300, 311, 264, 1150, 551, 300, 321, 393, 360, 597, 797], "temperature": 0.0, "avg_logprob": -0.16727729915648468, "compression_ratio": 1.8278145695364238, "no_speech_prob": 1.5294064041881938e-06}, {"id": 156, "seek": 70062, "start": 720.08, "end": 724.72, "text": " It's going to speed up our training because it's like one less set of decisions to make", "tokens": [467, 311, 516, 281, 3073, 493, 527, 3097, 570, 309, 311, 411, 472, 1570, 992, 295, 5327, 281, 652], "temperature": 0.0, "avg_logprob": -0.16727729915648468, "compression_ratio": 1.8278145695364238, "no_speech_prob": 1.5294064041881938e-06}, {"id": 157, "seek": 70062, "start": 725.04, "end": 729.92, "text": " remember even though there's one less set of decisions those decisions like have as", "tokens": [1604, 754, 1673, 456, 311, 472, 1570, 992, 295, 5327, 729, 5327, 411, 362, 382], "temperature": 0.0, "avg_logprob": -0.16727729915648468, "compression_ratio": 1.8278145695364238, "no_speech_prob": 1.5294064041881938e-06}, {"id": 158, "seek": 72992, "start": 729.92, "end": 736.3399999999999, "text": " Much data again as the previous set so like each layer of the tree can take like twice as long as the previous layer", "tokens": [12313, 1412, 797, 382, 264, 3894, 992, 370, 411, 1184, 4583, 295, 264, 4230, 393, 747, 411, 6091, 382, 938, 382, 264, 3894, 4583], "temperature": 0.0, "avg_logprob": -0.18333811047433438, "compression_ratio": 1.6813725490196079, "no_speech_prob": 3.611957936300314e-06}, {"id": 159, "seek": 72992, "start": 736.36, "end": 740.9599999999999, "text": " So it could definitely speed up training, and it could definitely make it generalize better", "tokens": [407, 309, 727, 2138, 3073, 493, 3097, 11, 293, 309, 727, 2138, 652, 309, 2674, 1125, 1101], "temperature": 0.0, "avg_logprob": -0.18333811047433438, "compression_ratio": 1.6813725490196079, "no_speech_prob": 3.611957936300314e-06}, {"id": 160, "seek": 72992, "start": 743.28, "end": 745.92, "text": " So then the third one that we had was", "tokens": [407, 550, 264, 2636, 472, 300, 321, 632, 390], "temperature": 0.0, "avg_logprob": -0.18333811047433438, "compression_ratio": 1.6813725490196079, "no_speech_prob": 3.611957936300314e-06}, {"id": 161, "seek": 72992, "start": 746.64, "end": 748.4799999999999, "text": " max features", "tokens": [11469, 4122], "temperature": 0.0, "avg_logprob": -0.18333811047433438, "compression_ratio": 1.6813725490196079, "no_speech_prob": 3.611957936300314e-06}, {"id": 162, "seek": 72992, "start": 748.4799999999999, "end": 751.1999999999999, "text": " Who wants to tell me what max features?", "tokens": [2102, 2738, 281, 980, 385, 437, 11469, 4122, 30], "temperature": 0.0, "avg_logprob": -0.18333811047433438, "compression_ratio": 1.6813725490196079, "no_speech_prob": 3.611957936300314e-06}, {"id": 163, "seek": 72992, "start": 751.8399999999999, "end": 753.8399999999999, "text": " does", "tokens": [775], "temperature": 0.0, "avg_logprob": -0.18333811047433438, "compression_ratio": 1.6813725490196079, "no_speech_prob": 3.611957936300314e-06}, {"id": 164, "seek": 72992, "start": 754.28, "end": 756.28, "text": " I'm going to pass that back over there", "tokens": [286, 478, 516, 281, 1320, 300, 646, 670, 456], "temperature": 0.0, "avg_logprob": -0.18333811047433438, "compression_ratio": 1.6813725490196079, "no_speech_prob": 3.611957936300314e-06}, {"id": 165, "seek": 75628, "start": 756.28, "end": 758.28, "text": " Okay, Vinay", "tokens": [1033, 11, 15011, 320], "temperature": 0.0, "avg_logprob": -0.3158943976885007, "compression_ratio": 1.664864864864865, "no_speech_prob": 3.5352903068996966e-05}, {"id": 166, "seek": 75628, "start": 758.28, "end": 766.28, "text": " Which is determines how many features you're going to use in each tree in this case?", "tokens": [3013, 307, 24799, 577, 867, 4122, 291, 434, 516, 281, 764, 294, 1184, 4230, 294, 341, 1389, 30], "temperature": 0.0, "avg_logprob": -0.3158943976885007, "compression_ratio": 1.664864864864865, "no_speech_prob": 3.5352903068996966e-05}, {"id": 167, "seek": 75628, "start": 766.28, "end": 771.22, "text": " It's a fraction up, so you're going to use half of the features for each tree", "tokens": [467, 311, 257, 14135, 493, 11, 370, 291, 434, 516, 281, 764, 1922, 295, 264, 4122, 337, 1184, 4230], "temperature": 0.0, "avg_logprob": -0.3158943976885007, "compression_ratio": 1.664864864864865, "no_speech_prob": 3.5352903068996966e-05}, {"id": 168, "seek": 75628, "start": 773.04, "end": 779.24, "text": " Nearly right or kind of right can you be more specific or can somebody else be more specific? It's not exactly for each tree", "tokens": [38000, 558, 420, 733, 295, 558, 393, 291, 312, 544, 2685, 420, 393, 2618, 1646, 312, 544, 2685, 30, 467, 311, 406, 2293, 337, 1184, 4230], "temperature": 0.0, "avg_logprob": -0.3158943976885007, "compression_ratio": 1.664864864864865, "no_speech_prob": 3.5352903068996966e-05}, {"id": 169, "seek": 75628, "start": 780.04, "end": 782.04, "text": " Chen Shi", "tokens": [13682, 25580], "temperature": 0.0, "avg_logprob": -0.3158943976885007, "compression_ratio": 1.664864864864865, "no_speech_prob": 3.5352903068996966e-05}, {"id": 170, "seek": 78204, "start": 782.04, "end": 787.4399999999999, "text": " That is it for each tree randomly sample half of the", "tokens": [663, 307, 309, 337, 1184, 4230, 16979, 6889, 1922, 295, 264], "temperature": 0.0, "avg_logprob": -0.3303275922449624, "compression_ratio": 1.7734806629834254, "no_speech_prob": 3.7852678360650316e-06}, {"id": 171, "seek": 78204, "start": 788.3199999999999, "end": 794.68, "text": " Features so not quite it's not for each tree so the the set don't pass it to Karen so the set are of samples", "tokens": [3697, 3377, 370, 406, 1596, 309, 311, 406, 337, 1184, 4230, 370, 264, 264, 992, 500, 380, 1320, 309, 281, 14834, 370, 264, 992, 366, 295, 10938], "temperature": 0.0, "avg_logprob": -0.3303275922449624, "compression_ratio": 1.7734806629834254, "no_speech_prob": 3.7852678360650316e-06}, {"id": 172, "seek": 78204, "start": 795.12, "end": 797.12, "text": " picks a", "tokens": [16137, 257], "temperature": 0.0, "avg_logprob": -0.3303275922449624, "compression_ratio": 1.7734806629834254, "no_speech_prob": 3.7852678360650316e-06}, {"id": 173, "seek": 78204, "start": 798.4399999999999, "end": 802.68, "text": " Picks a subset of samples a subset of rows for each tree", "tokens": [430, 7663, 257, 25993, 295, 10938, 257, 25993, 295, 13241, 337, 1184, 4230], "temperature": 0.0, "avg_logprob": -0.3303275922449624, "compression_ratio": 1.7734806629834254, "no_speech_prob": 3.7852678360650316e-06}, {"id": 174, "seek": 78204, "start": 804.36, "end": 809.12, "text": " But min samples leaf sorry that max features doesn't quite do that is that something different", "tokens": [583, 923, 10938, 10871, 2597, 300, 11469, 4122, 1177, 380, 1596, 360, 300, 307, 300, 746, 819], "temperature": 0.0, "avg_logprob": -0.3303275922449624, "compression_ratio": 1.7734806629834254, "no_speech_prob": 3.7852678360650316e-06}, {"id": 175, "seek": 80912, "start": 809.12, "end": 813.32, "text": " At each split we will be at each split set", "tokens": [1711, 1184, 7472, 321, 486, 312, 412, 1184, 7472, 992], "temperature": 0.0, "avg_logprob": -0.2377883791923523, "compression_ratio": 1.640552995391705, "no_speech_prob": 6.854234015918337e-06}, {"id": 176, "seek": 80912, "start": 814.0, "end": 816.16, "text": " Split it will", "tokens": [45111, 309, 486], "temperature": 0.0, "avg_logprob": -0.2377883791923523, "compression_ratio": 1.640552995391705, "no_speech_prob": 6.854234015918337e-06}, {"id": 177, "seek": 80912, "start": 818.0, "end": 819.84, "text": " Yeah, right", "tokens": [865, 11, 558], "temperature": 0.0, "avg_logprob": -0.2377883791923523, "compression_ratio": 1.640552995391705, "no_speech_prob": 6.854234015918337e-06}, {"id": 178, "seek": 80912, "start": 819.84, "end": 824.64, "text": " So it kind of sounds like a small difference, but it's actually quite a different way of thinking about it", "tokens": [407, 309, 733, 295, 3263, 411, 257, 1359, 2649, 11, 457, 309, 311, 767, 1596, 257, 819, 636, 295, 1953, 466, 309], "temperature": 0.0, "avg_logprob": -0.2377883791923523, "compression_ratio": 1.640552995391705, "no_speech_prob": 6.854234015918337e-06}, {"id": 179, "seek": 80912, "start": 824.64, "end": 830.0, "text": " Which is we do our set RF samples so we pull out our sub sample or a bootstrap sample", "tokens": [3013, 307, 321, 360, 527, 992, 26204, 10938, 370, 321, 2235, 484, 527, 1422, 6889, 420, 257, 11450, 372, 4007, 6889], "temperature": 0.0, "avg_logprob": -0.2377883791923523, "compression_ratio": 1.640552995391705, "no_speech_prob": 6.854234015918337e-06}, {"id": 180, "seek": 80912, "start": 830.0, "end": 832.0, "text": " And that's kept for the whole tree", "tokens": [400, 300, 311, 4305, 337, 264, 1379, 4230], "temperature": 0.0, "avg_logprob": -0.2377883791923523, "compression_ratio": 1.640552995391705, "no_speech_prob": 6.854234015918337e-06}, {"id": 181, "seek": 80912, "start": 832.64, "end": 836.32, "text": " And we have all of the columns in there right and then with", "tokens": [400, 321, 362, 439, 295, 264, 13766, 294, 456, 558, 293, 550, 365], "temperature": 0.0, "avg_logprob": -0.2377883791923523, "compression_ratio": 1.640552995391705, "no_speech_prob": 6.854234015918337e-06}, {"id": 182, "seek": 83632, "start": 836.32, "end": 838.2, "text": " max", "tokens": [11469], "temperature": 0.0, "avg_logprob": -0.19768363354252835, "compression_ratio": 1.918552036199095, "no_speech_prob": 2.0904417397105135e-06}, {"id": 183, "seek": 83632, "start": 838.2, "end": 844.44, "text": " Features equals point five at each point within at each split we pick a different half of", "tokens": [3697, 3377, 6915, 935, 1732, 412, 1184, 935, 1951, 412, 1184, 7472, 321, 1888, 257, 819, 1922, 295], "temperature": 0.0, "avg_logprob": -0.19768363354252835, "compression_ratio": 1.918552036199095, "no_speech_prob": 2.0904417397105135e-06}, {"id": 184, "seek": 83632, "start": 844.9200000000001, "end": 849.6800000000001, "text": " The features and then here will take a pick a different half of the features and here will pick a different half of the", "tokens": [440, 4122, 293, 550, 510, 486, 747, 257, 1888, 257, 819, 1922, 295, 264, 4122, 293, 510, 486, 1888, 257, 819, 1922, 295, 264], "temperature": 0.0, "avg_logprob": -0.19768363354252835, "compression_ratio": 1.918552036199095, "no_speech_prob": 2.0904417397105135e-06}, {"id": 185, "seek": 83632, "start": 849.6800000000001, "end": 857.0400000000001, "text": " Features and so the reason we do that is because we want the trees to be as as rich as possible, right?", "tokens": [3697, 3377, 293, 370, 264, 1778, 321, 360, 300, 307, 570, 321, 528, 264, 5852, 281, 312, 382, 382, 4593, 382, 1944, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19768363354252835, "compression_ratio": 1.918552036199095, "no_speech_prob": 2.0904417397105135e-06}, {"id": 186, "seek": 83632, "start": 857.2, "end": 862.4000000000001, "text": " So particularly like if you if you were only doing a small number of trees like you had only ten trees and", "tokens": [407, 4098, 411, 498, 291, 498, 291, 645, 787, 884, 257, 1359, 1230, 295, 5852, 411, 291, 632, 787, 2064, 5852, 293], "temperature": 0.0, "avg_logprob": -0.19768363354252835, "compression_ratio": 1.918552036199095, "no_speech_prob": 2.0904417397105135e-06}, {"id": 187, "seek": 86240, "start": 862.4, "end": 867.04, "text": " And you picked the same column set all the way through the tree", "tokens": [400, 291, 6183, 264, 912, 7738, 992, 439, 264, 636, 807, 264, 4230], "temperature": 0.0, "avg_logprob": -0.1524247441973005, "compression_ratio": 1.6211453744493391, "no_speech_prob": 2.6425705073052086e-06}, {"id": 188, "seek": 86240, "start": 867.4399999999999, "end": 874.16, "text": " You're not really getting much variety and what kind of things are confined okay, so this this way at least in theory", "tokens": [509, 434, 406, 534, 1242, 709, 5673, 293, 437, 733, 295, 721, 366, 31745, 1392, 11, 370, 341, 341, 636, 412, 1935, 294, 5261], "temperature": 0.0, "avg_logprob": -0.1524247441973005, "compression_ratio": 1.6211453744493391, "no_speech_prob": 2.6425705073052086e-06}, {"id": 189, "seek": 86240, "start": 875.84, "end": 879.88, "text": " Seems to be something which is going to give us a better set of trees is picking a different", "tokens": [22524, 281, 312, 746, 597, 307, 516, 281, 976, 505, 257, 1101, 992, 295, 5852, 307, 8867, 257, 819], "temperature": 0.0, "avg_logprob": -0.1524247441973005, "compression_ratio": 1.6211453744493391, "no_speech_prob": 2.6425705073052086e-06}, {"id": 190, "seek": 86240, "start": 880.48, "end": 884.24, "text": " random subset of features at every decision point", "tokens": [4974, 25993, 295, 4122, 412, 633, 3537, 935], "temperature": 0.0, "avg_logprob": -0.1524247441973005, "compression_ratio": 1.6211453744493391, "no_speech_prob": 2.6425705073052086e-06}, {"id": 191, "seek": 86240, "start": 887.6, "end": 890.0799999999999, "text": " So the overall effect of max features again", "tokens": [407, 264, 4787, 1802, 295, 11469, 4122, 797], "temperature": 0.0, "avg_logprob": -0.1524247441973005, "compression_ratio": 1.6211453744493391, "no_speech_prob": 2.6425705073052086e-06}, {"id": 192, "seek": 89008, "start": 890.08, "end": 896.7, "text": " It's the same it's going to mean that the each individual tree is probably going to be less accurate", "tokens": [467, 311, 264, 912, 309, 311, 516, 281, 914, 300, 264, 1184, 2609, 4230, 307, 1391, 516, 281, 312, 1570, 8559], "temperature": 0.0, "avg_logprob": -0.12336199133245794, "compression_ratio": 1.7977941176470589, "no_speech_prob": 3.089482106588548e-06}, {"id": 193, "seek": 89008, "start": 898.2, "end": 901.76, "text": " But the trees are going to be more varied and in particular here", "tokens": [583, 264, 5852, 366, 516, 281, 312, 544, 22877, 293, 294, 1729, 510], "temperature": 0.0, "avg_logprob": -0.12336199133245794, "compression_ratio": 1.7977941176470589, "no_speech_prob": 3.089482106588548e-06}, {"id": 194, "seek": 89008, "start": 902.5600000000001, "end": 908.1800000000001, "text": " This can be critical because like imagine that you've got one feature. That's just super predictive", "tokens": [639, 393, 312, 4924, 570, 411, 3811, 300, 291, 600, 658, 472, 4111, 13, 663, 311, 445, 1687, 35521], "temperature": 0.0, "avg_logprob": -0.12336199133245794, "compression_ratio": 1.7977941176470589, "no_speech_prob": 3.089482106588548e-06}, {"id": 195, "seek": 89008, "start": 908.1800000000001, "end": 913.96, "text": " It's so predictive that like every random sub sample you look at always starts out by splitting on that same", "tokens": [467, 311, 370, 35521, 300, 411, 633, 4974, 1422, 6889, 291, 574, 412, 1009, 3719, 484, 538, 30348, 322, 300, 912], "temperature": 0.0, "avg_logprob": -0.12336199133245794, "compression_ratio": 1.7977941176470589, "no_speech_prob": 3.089482106588548e-06}, {"id": 196, "seek": 89008, "start": 914.12, "end": 919.96, "text": " Feature then the trees are going to be very similar in the sense like they all have the same initial split, right?", "tokens": [3697, 1503, 550, 264, 5852, 366, 516, 281, 312, 588, 2531, 294, 264, 2020, 411, 436, 439, 362, 264, 912, 5883, 7472, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12336199133245794, "compression_ratio": 1.7977941176470589, "no_speech_prob": 3.089482106588548e-06}, {"id": 197, "seek": 91996, "start": 919.96, "end": 921.52, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.18223065596360427, "compression_ratio": 1.7919708029197081, "no_speech_prob": 8.851546340338245e-07}, {"id": 198, "seek": 91996, "start": 921.52, "end": 928.8000000000001, "text": " There may be some other interesting initial splits because they create different interactions of variables so by like half the time", "tokens": [821, 815, 312, 512, 661, 1880, 5883, 37741, 570, 436, 1884, 819, 13280, 295, 9102, 370, 538, 411, 1922, 264, 565], "temperature": 0.0, "avg_logprob": -0.18223065596360427, "compression_ratio": 1.7919708029197081, "no_speech_prob": 8.851546340338245e-07}, {"id": 199, "seek": 91996, "start": 929.44, "end": 934.88, "text": " That feature won't even be available at the top of the tree so half at least half the trees are going to have a different", "tokens": [663, 4111, 1582, 380, 754, 312, 2435, 412, 264, 1192, 295, 264, 4230, 370, 1922, 412, 1935, 1922, 264, 5852, 366, 516, 281, 362, 257, 819], "temperature": 0.0, "avg_logprob": -0.18223065596360427, "compression_ratio": 1.7919708029197081, "no_speech_prob": 8.851546340338245e-07}, {"id": 200, "seek": 91996, "start": 935.2, "end": 937.6, "text": " Initial split so it definitely can give us more", "tokens": [22937, 831, 7472, 370, 309, 2138, 393, 976, 505, 544], "temperature": 0.0, "avg_logprob": -0.18223065596360427, "compression_ratio": 1.7919708029197081, "no_speech_prob": 8.851546340338245e-07}, {"id": 201, "seek": 91996, "start": 938.64, "end": 944.44, "text": " Variation and therefore again it can help us to create more generalized trees that have less correlation with each other", "tokens": [32511, 399, 293, 4412, 797, 309, 393, 854, 505, 281, 1884, 544, 44498, 5852, 300, 362, 1570, 20009, 365, 1184, 661], "temperature": 0.0, "avg_logprob": -0.18223065596360427, "compression_ratio": 1.7919708029197081, "no_speech_prob": 8.851546340338245e-07}, {"id": 202, "seek": 91996, "start": 944.6800000000001, "end": 947.6800000000001, "text": " Even though the individual trees probably won't be as predictive", "tokens": [2754, 1673, 264, 2609, 5852, 1391, 1582, 380, 312, 382, 35521], "temperature": 0.0, "avg_logprob": -0.18223065596360427, "compression_ratio": 1.7919708029197081, "no_speech_prob": 8.851546340338245e-07}, {"id": 203, "seek": 94768, "start": 947.68, "end": 949.4399999999999, "text": " in", "tokens": [294], "temperature": 0.0, "avg_logprob": -0.13262791434923807, "compression_ratio": 1.7574468085106383, "no_speech_prob": 7.224424507512595e-07}, {"id": 204, "seek": 94768, "start": 949.4399999999999, "end": 954.7199999999999, "text": " Practice we actually looked at have a little picture of this that as as you add more trees", "tokens": [27904, 321, 767, 2956, 412, 362, 257, 707, 3036, 295, 341, 300, 382, 382, 291, 909, 544, 5852], "temperature": 0.0, "avg_logprob": -0.13262791434923807, "compression_ratio": 1.7574468085106383, "no_speech_prob": 7.224424507512595e-07}, {"id": 205, "seek": 94768, "start": 955.3599999999999, "end": 962.52, "text": " Right if you have max features equals none that's going to use all the features every time right then with like very very few trees", "tokens": [1779, 498, 291, 362, 11469, 4122, 6915, 6022, 300, 311, 516, 281, 764, 439, 264, 4122, 633, 565, 558, 550, 365, 411, 588, 588, 1326, 5852], "temperature": 0.0, "avg_logprob": -0.13262791434923807, "compression_ratio": 1.7574468085106383, "no_speech_prob": 7.224424507512595e-07}, {"id": 206, "seek": 94768, "start": 962.52, "end": 964.76, "text": " That can still give you a pretty good error", "tokens": [663, 393, 920, 976, 291, 257, 1238, 665, 6713], "temperature": 0.0, "avg_logprob": -0.13262791434923807, "compression_ratio": 1.7574468085106383, "no_speech_prob": 7.224424507512595e-07}, {"id": 207, "seek": 94768, "start": 965.52, "end": 967.52, "text": " But as you create more trees", "tokens": [583, 382, 291, 1884, 544, 5852], "temperature": 0.0, "avg_logprob": -0.13262791434923807, "compression_ratio": 1.7574468085106383, "no_speech_prob": 7.224424507512595e-07}, {"id": 208, "seek": 94768, "start": 968.2399999999999, "end": 973.9599999999999, "text": " It's not going to help as much because they're all pretty similar because they're all trying every single variable", "tokens": [467, 311, 406, 516, 281, 854, 382, 709, 570, 436, 434, 439, 1238, 2531, 570, 436, 434, 439, 1382, 633, 2167, 7006], "temperature": 0.0, "avg_logprob": -0.13262791434923807, "compression_ratio": 1.7574468085106383, "no_speech_prob": 7.224424507512595e-07}, {"id": 209, "seek": 97396, "start": 973.96, "end": 981.9200000000001, "text": " Where else if you say max features equals square root or max pictures equals log 2 then as we add more estimators", "tokens": [2305, 1646, 498, 291, 584, 11469, 4122, 6915, 3732, 5593, 420, 11469, 5242, 6915, 3565, 568, 550, 382, 321, 909, 544, 8017, 3391], "temperature": 0.0, "avg_logprob": -0.2331639384175395, "compression_ratio": 1.5637860082304527, "no_speech_prob": 5.368742108657898e-07}, {"id": 210, "seek": 97396, "start": 982.12, "end": 989.9200000000001, "text": " We see improvements, okay, so there's an interesting interaction between those two and this is from the sklearn docs this cool little chart", "tokens": [492, 536, 13797, 11, 1392, 11, 370, 456, 311, 364, 1880, 9285, 1296, 729, 732, 293, 341, 307, 490, 264, 1110, 306, 1083, 45623, 341, 1627, 707, 6927], "temperature": 0.0, "avg_logprob": -0.2331639384175395, "compression_ratio": 1.5637860082304527, "no_speech_prob": 5.368742108657898e-07}, {"id": 211, "seek": 97396, "start": 991.1600000000001, "end": 992.64, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2331639384175395, "compression_ratio": 1.5637860082304527, "no_speech_prob": 5.368742108657898e-07}, {"id": 212, "seek": 97396, "start": 992.64, "end": 997.76, "text": " So then things which don't impact our training at all and jobs", "tokens": [407, 550, 721, 597, 500, 380, 2712, 527, 3097, 412, 439, 293, 4782], "temperature": 0.0, "avg_logprob": -0.2331639384175395, "compression_ratio": 1.5637860082304527, "no_speech_prob": 5.368742108657898e-07}, {"id": 213, "seek": 97396, "start": 998.36, "end": 1002.4000000000001, "text": " Simply says how many CPU how many cores do we run on okay?", "tokens": [19596, 1619, 577, 867, 13199, 577, 867, 24826, 360, 321, 1190, 322, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2331639384175395, "compression_ratio": 1.5637860082304527, "no_speech_prob": 5.368742108657898e-07}, {"id": 214, "seek": 100240, "start": 1002.4, "end": 1010.5, "text": " So it'll make it faster up to a point generally speaking making this more than like eight or so they may have diminishing returns", "tokens": [407, 309, 603, 652, 309, 4663, 493, 281, 257, 935, 5101, 4124, 1455, 341, 544, 813, 411, 3180, 420, 370, 436, 815, 362, 15739, 3807, 11247], "temperature": 0.0, "avg_logprob": -0.18644142150878906, "compression_ratio": 1.6612903225806452, "no_speech_prob": 7.766893759253435e-06}, {"id": 215, "seek": 100240, "start": 1011.16, "end": 1013.16, "text": " Minus one says use all of your cores", "tokens": [2829, 301, 472, 1619, 764, 439, 295, 428, 24826], "temperature": 0.0, "avg_logprob": -0.18644142150878906, "compression_ratio": 1.6612903225806452, "no_speech_prob": 7.766893759253435e-06}, {"id": 216, "seek": 100240, "start": 1014.4, "end": 1021.4, "text": " So there's I don't know why the default is to only use one core that seems weird to me", "tokens": [407, 456, 311, 286, 500, 380, 458, 983, 264, 7576, 307, 281, 787, 764, 472, 4965, 300, 2544, 3657, 281, 385], "temperature": 0.0, "avg_logprob": -0.18644142150878906, "compression_ratio": 1.6612903225806452, "no_speech_prob": 7.766893759253435e-06}, {"id": 217, "seek": 100240, "start": 1021.56, "end": 1027.1, "text": " You'll definitely get more performance by using more cores because all of you have computers with more than one core nowadays", "tokens": [509, 603, 2138, 483, 544, 3389, 538, 1228, 544, 24826, 570, 439, 295, 291, 362, 10807, 365, 544, 813, 472, 4965, 13434], "temperature": 0.0, "avg_logprob": -0.18644142150878906, "compression_ratio": 1.6612903225806452, "no_speech_prob": 7.766893759253435e-06}, {"id": 218, "seek": 100240, "start": 1027.1, "end": 1029.96, "text": " And then our B score equals true", "tokens": [400, 550, 527, 363, 6175, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.18644142150878906, "compression_ratio": 1.6612903225806452, "no_speech_prob": 7.766893759253435e-06}, {"id": 219, "seek": 102996, "start": 1029.96, "end": 1031.96, "text": " simply", "tokens": [2935], "temperature": 0.0, "avg_logprob": -0.2023352095224325, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.4060923351498786e-06}, {"id": 220, "seek": 102996, "start": 1032.1200000000001, "end": 1038.04, "text": " Allows us to see the OOB score if you don't say that it doesn't calculate it and", "tokens": [1057, 1509, 505, 281, 536, 264, 422, 46, 33, 6175, 498, 291, 500, 380, 584, 300, 309, 1177, 380, 8873, 309, 293], "temperature": 0.0, "avg_logprob": -0.2023352095224325, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.4060923351498786e-06}, {"id": 221, "seek": 102996, "start": 1038.44, "end": 1045.4, "text": " Particularly if you had set RF samples pretty small compared to a big data set OOB is going to take forever to calculate", "tokens": [32281, 498, 291, 632, 992, 26204, 10938, 1238, 1359, 5347, 281, 257, 955, 1412, 992, 422, 46, 33, 307, 516, 281, 747, 5680, 281, 8873], "temperature": 0.0, "avg_logprob": -0.2023352095224325, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.4060923351498786e-06}, {"id": 222, "seek": 102996, "start": 1046.04, "end": 1049.1200000000001, "text": " Hopefully at some point. We'll be able to fix the library so that doesn't happen", "tokens": [10429, 412, 512, 935, 13, 492, 603, 312, 1075, 281, 3191, 264, 6405, 370, 300, 1177, 380, 1051], "temperature": 0.0, "avg_logprob": -0.2023352095224325, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.4060923351498786e-06}, {"id": 223, "seek": 102996, "start": 1049.1200000000001, "end": 1054.2, "text": " There's no reason it need be that way, but right now. That's that's how the library works", "tokens": [821, 311, 572, 1778, 309, 643, 312, 300, 636, 11, 457, 558, 586, 13, 663, 311, 300, 311, 577, 264, 6405, 1985], "temperature": 0.0, "avg_logprob": -0.2023352095224325, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.4060923351498786e-06}, {"id": 224, "seek": 102996, "start": 1055.3600000000001, "end": 1057.3600000000001, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2023352095224325, "compression_ratio": 1.5867768595041323, "no_speech_prob": 2.4060923351498786e-06}, {"id": 225, "seek": 105736, "start": 1057.36, "end": 1059.36, "text": " so there our", "tokens": [370, 456, 527], "temperature": 0.0, "avg_logprob": -0.20346137914764748, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.356837507657474e-06}, {"id": 226, "seek": 105736, "start": 1060.08, "end": 1064.52, "text": " Base you know key basic parameters that we can change there are", "tokens": [21054, 291, 458, 2141, 3875, 9834, 300, 321, 393, 1319, 456, 366], "temperature": 0.0, "avg_logprob": -0.20346137914764748, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.356837507657474e-06}, {"id": 227, "seek": 105736, "start": 1065.12, "end": 1068.8999999999999, "text": " More that you can see in the docs or shift tab to have a look at them", "tokens": [5048, 300, 291, 393, 536, 294, 264, 45623, 420, 5513, 4421, 281, 362, 257, 574, 412, 552], "temperature": 0.0, "avg_logprob": -0.20346137914764748, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.356837507657474e-06}, {"id": 228, "seek": 105736, "start": 1069.4399999999998, "end": 1074.6999999999998, "text": " But the ones you've seen are the ones that I've found useful to play with so feel free to play with others as well", "tokens": [583, 264, 2306, 291, 600, 1612, 366, 264, 2306, 300, 286, 600, 1352, 4420, 281, 862, 365, 370, 841, 1737, 281, 862, 365, 2357, 382, 731], "temperature": 0.0, "avg_logprob": -0.20346137914764748, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.356837507657474e-06}, {"id": 229, "seek": 105736, "start": 1075.4399999999998, "end": 1081.26, "text": " And generally speaking you know max features of as I said max features of like either", "tokens": [400, 5101, 4124, 291, 458, 11469, 4122, 295, 382, 286, 848, 11469, 4122, 295, 411, 2139], "temperature": 0.0, "avg_logprob": -0.20346137914764748, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.356837507657474e-06}, {"id": 230, "seek": 105736, "start": 1083.6799999999998, "end": 1085.6799999999998, "text": " None", "tokens": [14492], "temperature": 0.0, "avg_logprob": -0.20346137914764748, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.356837507657474e-06}, {"id": 231, "seek": 108568, "start": 1085.68, "end": 1087.68, "text": " Means all of them", "tokens": [40290, 439, 295, 552], "temperature": 0.0, "avg_logprob": -0.3123080483798323, "compression_ratio": 1.4383561643835616, "no_speech_prob": 4.029425781482132e-06}, {"id": 232, "seek": 108568, "start": 1089.6000000000001, "end": 1092.88, "text": " About point five or", "tokens": [7769, 935, 1732, 420], "temperature": 0.0, "avg_logprob": -0.3123080483798323, "compression_ratio": 1.4383561643835616, "no_speech_prob": 4.029425781482132e-06}, {"id": 233, "seek": 108568, "start": 1095.92, "end": 1097.92, "text": " Square root", "tokens": [16463, 5593], "temperature": 0.0, "avg_logprob": -0.3123080483798323, "compression_ratio": 1.4383561643835616, "no_speech_prob": 4.029425781482132e-06}, {"id": 234, "seek": 108568, "start": 1098.04, "end": 1100.2, "text": " Or log you know kind of those", "tokens": [1610, 3565, 291, 458, 733, 295, 729], "temperature": 0.0, "avg_logprob": -0.3123080483798323, "compression_ratio": 1.4383561643835616, "no_speech_prob": 4.029425781482132e-06}, {"id": 235, "seek": 108568, "start": 1101.04, "end": 1104.64, "text": " Trees seem to work pretty well and then for min samples leaf", "tokens": [314, 4856, 1643, 281, 589, 1238, 731, 293, 550, 337, 923, 10938, 10871], "temperature": 0.0, "avg_logprob": -0.3123080483798323, "compression_ratio": 1.4383561643835616, "no_speech_prob": 4.029425781482132e-06}, {"id": 236, "seek": 108568, "start": 1106.3200000000002, "end": 1112.44, "text": " You know I would generally try kind of one three five ten twenty five", "tokens": [509, 458, 286, 576, 5101, 853, 733, 295, 472, 1045, 1732, 2064, 7699, 1732], "temperature": 0.0, "avg_logprob": -0.3123080483798323, "compression_ratio": 1.4383561643835616, "no_speech_prob": 4.029425781482132e-06}, {"id": 237, "seek": 111244, "start": 1112.44, "end": 1117.3, "text": " You know hundred and like as you start doing that if you notice by the time you get to ten", "tokens": [509, 458, 3262, 293, 411, 382, 291, 722, 884, 300, 498, 291, 3449, 538, 264, 565, 291, 483, 281, 2064], "temperature": 0.0, "avg_logprob": -0.24124125776619748, "compression_ratio": 1.6529680365296804, "no_speech_prob": 1.505694513070921e-06}, {"id": 238, "seek": 111244, "start": 1117.3, "end": 1118.0800000000002, "text": " It's already getting worse", "tokens": [467, 311, 1217, 1242, 5324], "temperature": 0.0, "avg_logprob": -0.24124125776619748, "compression_ratio": 1.6529680365296804, "no_speech_prob": 1.505694513070921e-06}, {"id": 239, "seek": 111244, "start": 1118.0800000000002, "end": 1124.64, "text": " So there's no point going further if you get to a hundred it's still going better, and you can keep trying right but they're the kind of", "tokens": [407, 456, 311, 572, 935, 516, 3052, 498, 291, 483, 281, 257, 3262, 309, 311, 920, 516, 1101, 11, 293, 291, 393, 1066, 1382, 558, 457, 436, 434, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.24124125776619748, "compression_ratio": 1.6529680365296804, "no_speech_prob": 1.505694513070921e-06}, {"id": 240, "seek": 111244, "start": 1125.4, "end": 1127.8, "text": " General amounts that most things in to sit in", "tokens": [6996, 11663, 300, 881, 721, 294, 281, 1394, 294], "temperature": 0.0, "avg_logprob": -0.24124125776619748, "compression_ratio": 1.6529680365296804, "no_speech_prob": 1.505694513070921e-06}, {"id": 241, "seek": 111244, "start": 1130.24, "end": 1136.56, "text": " All right, so random forest interpretation is something which", "tokens": [1057, 558, 11, 370, 4974, 6719, 14174, 307, 746, 597], "temperature": 0.0, "avg_logprob": -0.24124125776619748, "compression_ratio": 1.6529680365296804, "no_speech_prob": 1.505694513070921e-06}, {"id": 242, "seek": 113656, "start": 1136.56, "end": 1144.2, "text": " You could use to create some really cool kaggle kernels now obviously one issue is the fastai", "tokens": [509, 727, 764, 281, 1884, 512, 534, 1627, 350, 559, 22631, 23434, 1625, 586, 2745, 472, 2734, 307, 264, 2370, 1301], "temperature": 0.0, "avg_logprob": -0.22052615621815558, "compression_ratio": 1.617117117117117, "no_speech_prob": 9.721520655148197e-07}, {"id": 243, "seek": 113656, "start": 1144.52, "end": 1149.96, "text": " Library is not available in kaggle kernels, but if you look inside fastai.structured", "tokens": [12806, 307, 406, 2435, 294, 350, 559, 22631, 23434, 1625, 11, 457, 498, 291, 574, 1854, 2370, 1301, 13, 372, 46847], "temperature": 0.0, "avg_logprob": -0.22052615621815558, "compression_ratio": 1.617117117117117, "no_speech_prob": 9.721520655148197e-07}, {"id": 244, "seek": 113656, "start": 1151.0, "end": 1153.0, "text": " Right remember you can just use", "tokens": [1779, 1604, 291, 393, 445, 764], "temperature": 0.0, "avg_logprob": -0.22052615621815558, "compression_ratio": 1.617117117117117, "no_speech_prob": 9.721520655148197e-07}, {"id": 245, "seek": 113656, "start": 1153.76, "end": 1159.0, "text": " Double question mark to look at the source code for something or you can go into the editor to have a look at it", "tokens": [16633, 1168, 1491, 281, 574, 412, 264, 4009, 3089, 337, 746, 420, 291, 393, 352, 666, 264, 9839, 281, 362, 257, 574, 412, 309], "temperature": 0.0, "avg_logprob": -0.22052615621815558, "compression_ratio": 1.617117117117117, "no_speech_prob": 9.721520655148197e-07}, {"id": 246, "seek": 113656, "start": 1159.48, "end": 1161.8, "text": " You'll see that most of the methods", "tokens": [509, 603, 536, 300, 881, 295, 264, 7150], "temperature": 0.0, "avg_logprob": -0.22052615621815558, "compression_ratio": 1.617117117117117, "no_speech_prob": 9.721520655148197e-07}, {"id": 247, "seek": 116180, "start": 1161.8, "end": 1167.6399999999999, "text": " We're using are a small number of lines of code in this library and have no dependencies on anything", "tokens": [492, 434, 1228, 366, 257, 1359, 1230, 295, 3876, 295, 3089, 294, 341, 6405, 293, 362, 572, 36606, 322, 1340], "temperature": 0.0, "avg_logprob": -0.16021984379465987, "compression_ratio": 1.7805755395683454, "no_speech_prob": 3.446564278419828e-06}, {"id": 248, "seek": 116180, "start": 1167.6399999999999, "end": 1173.36, "text": " So you could just copy that that all if you need to use one of those functions just copy it into your kernel", "tokens": [407, 291, 727, 445, 5055, 300, 300, 439, 498, 291, 643, 281, 764, 472, 295, 729, 6828, 445, 5055, 309, 666, 428, 28256], "temperature": 0.0, "avg_logprob": -0.16021984379465987, "compression_ratio": 1.7805755395683454, "no_speech_prob": 3.446564278419828e-06}, {"id": 249, "seek": 116180, "start": 1173.9199999999998, "end": 1180.1399999999999, "text": " And and if you do just say this is from the fastai library you can link to it on github because it's available on github", "tokens": [400, 293, 498, 291, 360, 445, 584, 341, 307, 490, 264, 2370, 1301, 6405, 291, 393, 2113, 281, 309, 322, 290, 355, 836, 570, 309, 311, 2435, 322, 290, 355, 836], "temperature": 0.0, "avg_logprob": -0.16021984379465987, "compression_ratio": 1.7805755395683454, "no_speech_prob": 3.446564278419828e-06}, {"id": 250, "seek": 116180, "start": 1180.1399999999999, "end": 1182.22, "text": " It's open source, but you don't need to", "tokens": [467, 311, 1269, 4009, 11, 457, 291, 500, 380, 643, 281], "temperature": 0.0, "avg_logprob": -0.16021984379465987, "compression_ratio": 1.7805755395683454, "no_speech_prob": 3.446564278419828e-06}, {"id": 251, "seek": 116180, "start": 1182.96, "end": 1188.76, "text": " Import the whole thing right so this is a cool trick is that because you're the first people to learn how to use these tools", "tokens": [26391, 264, 1379, 551, 558, 370, 341, 307, 257, 1627, 4282, 307, 300, 570, 291, 434, 264, 700, 561, 281, 1466, 577, 281, 764, 613, 3873], "temperature": 0.0, "avg_logprob": -0.16021984379465987, "compression_ratio": 1.7805755395683454, "no_speech_prob": 3.446564278419828e-06}, {"id": 252, "seek": 118876, "start": 1188.76, "end": 1192.94, "text": " You can start to show things that other people haven't seen right so for example", "tokens": [509, 393, 722, 281, 855, 721, 300, 661, 561, 2378, 380, 1612, 558, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.14006434260187922, "compression_ratio": 1.6847457627118645, "no_speech_prob": 4.860409262619214e-06}, {"id": 253, "seek": 118876, "start": 1193.68, "end": 1197.48, "text": " This confidence based on tree variance is something which doesn't exist anywhere else", "tokens": [639, 6687, 2361, 322, 4230, 21977, 307, 746, 597, 1177, 380, 2514, 4992, 1646], "temperature": 0.0, "avg_logprob": -0.14006434260187922, "compression_ratio": 1.6847457627118645, "no_speech_prob": 4.860409262619214e-06}, {"id": 254, "seek": 118876, "start": 1198.64, "end": 1202.96, "text": " Feature importance definitely does and that's already in quite a lot of kaggle kernels", "tokens": [3697, 1503, 7379, 2138, 775, 293, 300, 311, 1217, 294, 1596, 257, 688, 295, 350, 559, 22631, 23434, 1625], "temperature": 0.0, "avg_logprob": -0.14006434260187922, "compression_ratio": 1.6847457627118645, "no_speech_prob": 4.860409262619214e-06}, {"id": 255, "seek": 118876, "start": 1202.96, "end": 1207.18, "text": " If you're looking at a competition or a data set that when nobody's done feature importance", "tokens": [759, 291, 434, 1237, 412, 257, 6211, 420, 257, 1412, 992, 300, 562, 5079, 311, 1096, 4111, 7379], "temperature": 0.0, "avg_logprob": -0.14006434260187922, "compression_ratio": 1.6847457627118645, "no_speech_prob": 4.860409262619214e-06}, {"id": 256, "seek": 118876, "start": 1207.48, "end": 1212.84, "text": " Being the first person to do that is always going to win lots of votes because it's like the most important thing is", "tokens": [8891, 264, 700, 954, 281, 360, 300, 307, 1009, 516, 281, 1942, 3195, 295, 12068, 570, 309, 311, 411, 264, 881, 1021, 551, 307], "temperature": 0.0, "avg_logprob": -0.14006434260187922, "compression_ratio": 1.6847457627118645, "no_speech_prob": 4.860409262619214e-06}, {"id": 257, "seek": 118876, "start": 1213.0, "end": 1215.0, "text": " Like which features are important?", "tokens": [1743, 597, 4122, 366, 1021, 30], "temperature": 0.0, "avg_logprob": -0.14006434260187922, "compression_ratio": 1.6847457627118645, "no_speech_prob": 4.860409262619214e-06}, {"id": 258, "seek": 121500, "start": 1215.0, "end": 1217.0, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.2776568430774617, "compression_ratio": 1.3968253968253967, "no_speech_prob": 9.818106263992377e-06}, {"id": 259, "seek": 121500, "start": 1218.92, "end": 1224.08, "text": " Last time we let's just make sure we've got our tree data", "tokens": [5264, 565, 321, 718, 311, 445, 652, 988, 321, 600, 658, 527, 4230, 1412], "temperature": 0.0, "avg_logprob": -0.2776568430774617, "compression_ratio": 1.3968253968253967, "no_speech_prob": 9.818106263992377e-06}, {"id": 260, "seek": 121500, "start": 1227.16, "end": 1232.9, "text": " So we need to change this to add one extra thing alright, so that's going to load in that data", "tokens": [407, 321, 643, 281, 1319, 341, 281, 909, 472, 2857, 551, 5845, 11, 370, 300, 311, 516, 281, 3677, 294, 300, 1412], "temperature": 0.0, "avg_logprob": -0.2776568430774617, "compression_ratio": 1.3968253968253967, "no_speech_prob": 9.818106263992377e-06}, {"id": 261, "seek": 121500, "start": 1235.8, "end": 1238.74, "text": " It is our data, okay", "tokens": [467, 307, 527, 1412, 11, 1392], "temperature": 0.0, "avg_logprob": -0.2776568430774617, "compression_ratio": 1.3968253968253967, "no_speech_prob": 9.818106263992377e-06}, {"id": 262, "seek": 123874, "start": 1238.74, "end": 1240.74, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.2108292946448693, "compression_ratio": 1.5631067961165048, "no_speech_prob": 3.237717692172737e-06}, {"id": 263, "seek": 123874, "start": 1243.82, "end": 1246.34, "text": " As I mentioned when we do a model interpretation", "tokens": [1018, 286, 2835, 562, 321, 360, 257, 2316, 14174], "temperature": 0.0, "avg_logprob": -0.2108292946448693, "compression_ratio": 1.5631067961165048, "no_speech_prob": 3.237717692172737e-06}, {"id": 264, "seek": 123874, "start": 1246.34, "end": 1253.58, "text": " I tend to set RF samples to some subset something small enough that I can run a model in under 10 seconds or so", "tokens": [286, 3928, 281, 992, 26204, 10938, 281, 512, 25993, 746, 1359, 1547, 300, 286, 393, 1190, 257, 2316, 294, 833, 1266, 3949, 420, 370], "temperature": 0.0, "avg_logprob": -0.2108292946448693, "compression_ratio": 1.5631067961165048, "no_speech_prob": 3.237717692172737e-06}, {"id": 265, "seek": 123874, "start": 1254.02, "end": 1259.56, "text": " Because there's just no point run running a super accurate model 50,000 is more than enough", "tokens": [1436, 456, 311, 445, 572, 935, 1190, 2614, 257, 1687, 8559, 2316, 2625, 11, 1360, 307, 544, 813, 1547], "temperature": 0.0, "avg_logprob": -0.2108292946448693, "compression_ratio": 1.5631067961165048, "no_speech_prob": 3.237717692172737e-06}, {"id": 266, "seek": 123874, "start": 1260.26, "end": 1264.18, "text": " To to see you'll basically see each time you run an interpretation", "tokens": [1407, 281, 536, 291, 603, 1936, 536, 1184, 565, 291, 1190, 364, 14174], "temperature": 0.0, "avg_logprob": -0.2108292946448693, "compression_ratio": 1.5631067961165048, "no_speech_prob": 3.237717692172737e-06}, {"id": 267, "seek": 126418, "start": 1264.18, "end": 1270.42, "text": " You'll get the same results back and so as long as that's true, then you you're already using enough data, okay?", "tokens": [509, 603, 483, 264, 912, 3542, 646, 293, 370, 382, 938, 382, 300, 311, 2074, 11, 550, 291, 291, 434, 1217, 1228, 1547, 1412, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.17192474533529842, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.338191494454804e-07}, {"id": 268, "seek": 126418, "start": 1275.26, "end": 1282.9, "text": " So feature importance we learned it works by randomly shuffling a column", "tokens": [407, 4111, 7379, 321, 3264, 309, 1985, 538, 16979, 402, 1245, 1688, 257, 7738], "temperature": 0.0, "avg_logprob": -0.17192474533529842, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.338191494454804e-07}, {"id": 269, "seek": 126418, "start": 1284.02, "end": 1289.44, "text": " Each column one at a time and then seeing how accurate the model the pre trained model the model", "tokens": [6947, 7738, 472, 412, 257, 565, 293, 550, 2577, 577, 8559, 264, 2316, 264, 659, 8895, 2316, 264, 2316], "temperature": 0.0, "avg_logprob": -0.17192474533529842, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.338191494454804e-07}, {"id": 270, "seek": 128944, "start": 1289.44, "end": 1295.8600000000001, "text": " We've already built is when you pass it in all the data as before but with one column shuffle", "tokens": [492, 600, 1217, 3094, 307, 562, 291, 1320, 309, 294, 439, 264, 1412, 382, 949, 457, 365, 472, 7738, 39426], "temperature": 0.0, "avg_logprob": -0.2072501113449318, "compression_ratio": 1.4475138121546962, "no_speech_prob": 1.0030119028670015e-06}, {"id": 271, "seek": 128944, "start": 1296.46, "end": 1298.46, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2072501113449318, "compression_ratio": 1.4475138121546962, "no_speech_prob": 1.0030119028670015e-06}, {"id": 272, "seek": 128944, "start": 1300.1000000000001, "end": 1303.26, "text": " Some of the questions I got after class kind of", "tokens": [2188, 295, 264, 1651, 286, 658, 934, 1508, 733, 295], "temperature": 0.0, "avg_logprob": -0.2072501113449318, "compression_ratio": 1.4475138121546962, "no_speech_prob": 1.0030119028670015e-06}, {"id": 273, "seek": 128944, "start": 1304.5800000000002, "end": 1307.3400000000001, "text": " reminded me that it's very easy to", "tokens": [15920, 385, 300, 309, 311, 588, 1858, 281], "temperature": 0.0, "avg_logprob": -0.2072501113449318, "compression_ratio": 1.4475138121546962, "no_speech_prob": 1.0030119028670015e-06}, {"id": 274, "seek": 128944, "start": 1308.46, "end": 1310.46, "text": " under appreciate how", "tokens": [833, 4449, 577], "temperature": 0.0, "avg_logprob": -0.2072501113449318, "compression_ratio": 1.4475138121546962, "no_speech_prob": 1.0030119028670015e-06}, {"id": 275, "seek": 128944, "start": 1311.5800000000002, "end": 1316.7, "text": " Powerful and kind of magic this approach is and so to explain", "tokens": [7086, 906, 293, 733, 295, 5585, 341, 3109, 307, 293, 370, 281, 2903], "temperature": 0.0, "avg_logprob": -0.2072501113449318, "compression_ratio": 1.4475138121546962, "no_speech_prob": 1.0030119028670015e-06}, {"id": 276, "seek": 131670, "start": 1316.7, "end": 1321.9, "text": " I'll mention a couple of the questions that I heard so one question was like", "tokens": [286, 603, 2152, 257, 1916, 295, 264, 1651, 300, 286, 2198, 370, 472, 1168, 390, 411], "temperature": 0.0, "avg_logprob": -0.16066856384277345, "compression_ratio": 1.748917748917749, "no_speech_prob": 1.034851152326155e-06}, {"id": 277, "seek": 131670, "start": 1322.82, "end": 1329.5800000000002, "text": " Why don't we or what if we just create took one column at a time and created a tree on?", "tokens": [1545, 500, 380, 321, 420, 437, 498, 321, 445, 1884, 1890, 472, 7738, 412, 257, 565, 293, 2942, 257, 4230, 322, 30], "temperature": 0.0, "avg_logprob": -0.16066856384277345, "compression_ratio": 1.748917748917749, "no_speech_prob": 1.034851152326155e-06}, {"id": 278, "seek": 131670, "start": 1330.42, "end": 1334.92, "text": " Just each one column at a time, so we've got our data set. It's got a bunch of columns", "tokens": [1449, 1184, 472, 7738, 412, 257, 565, 11, 370, 321, 600, 658, 527, 1412, 992, 13, 467, 311, 658, 257, 3840, 295, 13766], "temperature": 0.0, "avg_logprob": -0.16066856384277345, "compression_ratio": 1.748917748917749, "no_speech_prob": 1.034851152326155e-06}, {"id": 279, "seek": 131670, "start": 1335.02, "end": 1342.14, "text": " So why don't we just like grab that column and just build a tree from that right and then like we'll see which which columns", "tokens": [407, 983, 500, 380, 321, 445, 411, 4444, 300, 7738, 293, 445, 1322, 257, 4230, 490, 300, 558, 293, 550, 411, 321, 603, 536, 597, 597, 13766], "temperature": 0.0, "avg_logprob": -0.16066856384277345, "compression_ratio": 1.748917748917749, "no_speech_prob": 1.034851152326155e-06}, {"id": 280, "seek": 131670, "start": 1342.42, "end": 1344.8600000000001, "text": " Tree is the most predictive", "tokens": [22291, 307, 264, 881, 35521], "temperature": 0.0, "avg_logprob": -0.16066856384277345, "compression_ratio": 1.748917748917749, "no_speech_prob": 1.034851152326155e-06}, {"id": 281, "seek": 134486, "start": 1344.86, "end": 1346.86, "text": " Can anybody tell me?", "tokens": [1664, 4472, 980, 385, 30], "temperature": 0.0, "avg_logprob": -0.3146282983204675, "compression_ratio": 1.457142857142857, "no_speech_prob": 1.6441859770566225e-05}, {"id": 282, "seek": 134486, "start": 1347.9799999999998, "end": 1353.04, "text": " Why what why that may give misleading results about feature importance?", "tokens": [1545, 437, 983, 300, 815, 976, 36429, 3542, 466, 4111, 7379, 30], "temperature": 0.0, "avg_logprob": -0.3146282983204675, "compression_ratio": 1.457142857142857, "no_speech_prob": 1.6441859770566225e-05}, {"id": 283, "seek": 134486, "start": 1357.9799999999998, "end": 1359.9799999999998, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.3146282983204675, "compression_ratio": 1.457142857142857, "no_speech_prob": 1.6441859770566225e-05}, {"id": 284, "seek": 134486, "start": 1363.82, "end": 1365.9799999999998, "text": " Features yeah, if we just shuffle them", "tokens": [3697, 3377, 1338, 11, 498, 321, 445, 39426, 552], "temperature": 0.0, "avg_logprob": -0.3146282983204675, "compression_ratio": 1.457142857142857, "no_speech_prob": 1.6441859770566225e-05}, {"id": 285, "seek": 134486, "start": 1366.3, "end": 1373.2199999999998, "text": " It will be at randomness and we were able to both capture the interactions and the importance of the future it's great", "tokens": [467, 486, 312, 412, 4974, 1287, 293, 321, 645, 1075, 281, 1293, 7983, 264, 13280, 293, 264, 7379, 295, 264, 2027, 309, 311, 869], "temperature": 0.0, "avg_logprob": -0.3146282983204675, "compression_ratio": 1.457142857142857, "no_speech_prob": 1.6441859770566225e-05}, {"id": 286, "seek": 137322, "start": 1373.22, "end": 1375.22, "text": " Yeah, and and so", "tokens": [865, 11, 293, 293, 370], "temperature": 0.0, "avg_logprob": -0.24737137242367394, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.228914811690629e-06}, {"id": 287, "seek": 137322, "start": 1375.82, "end": 1379.74, "text": " This issue of interactions is not a minor detail. It's like", "tokens": [639, 2734, 295, 13280, 307, 406, 257, 6696, 2607, 13, 467, 311, 411], "temperature": 0.0, "avg_logprob": -0.24737137242367394, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.228914811690629e-06}, {"id": 288, "seek": 137322, "start": 1380.34, "end": 1383.78, "text": " It's massively important. So I like think about this", "tokens": [467, 311, 29379, 1021, 13, 407, 286, 411, 519, 466, 341], "temperature": 0.0, "avg_logprob": -0.24737137242367394, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.228914811690629e-06}, {"id": 289, "seek": 137322, "start": 1384.82, "end": 1389.9, "text": " Bulldozers data set where for example where there's one field called year made and", "tokens": [14131, 2595, 41698, 1412, 992, 689, 337, 1365, 689, 456, 311, 472, 2519, 1219, 1064, 1027, 293], "temperature": 0.0, "avg_logprob": -0.24737137242367394, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.228914811690629e-06}, {"id": 290, "seek": 137322, "start": 1390.54, "end": 1393.7, "text": " there's one field called sale date and", "tokens": [456, 311, 472, 2519, 1219, 8680, 4002, 293], "temperature": 0.0, "avg_logprob": -0.24737137242367394, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.228914811690629e-06}, {"id": 291, "seek": 137322, "start": 1395.06, "end": 1396.66, "text": " like", "tokens": [411], "temperature": 0.0, "avg_logprob": -0.24737137242367394, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.228914811690629e-06}, {"id": 292, "seek": 137322, "start": 1396.66, "end": 1398.1000000000001, "text": " If we think about it", "tokens": [759, 321, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.24737137242367394, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.228914811690629e-06}, {"id": 293, "seek": 139810, "start": 1398.1, "end": 1403.5, "text": " It's pretty obvious that what matters is the combination of these two which in other words is like", "tokens": [467, 311, 1238, 6322, 300, 437, 7001, 307, 264, 6562, 295, 613, 732, 597, 294, 661, 2283, 307, 411], "temperature": 0.0, "avg_logprob": -0.1498971149839204, "compression_ratio": 1.6367521367521367, "no_speech_prob": 1.3081728411634685e-06}, {"id": 294, "seek": 139810, "start": 1403.9399999999998, "end": 1409.3999999999999, "text": " How old is the piece of equipment when it got sold? So if we only included one of these?", "tokens": [1012, 1331, 307, 264, 2522, 295, 5927, 562, 309, 658, 3718, 30, 407, 498, 321, 787, 5556, 472, 295, 613, 30], "temperature": 0.0, "avg_logprob": -0.1498971149839204, "compression_ratio": 1.6367521367521367, "no_speech_prob": 1.3081728411634685e-06}, {"id": 295, "seek": 139810, "start": 1410.3, "end": 1414.6, "text": " We're going to massively underestimate how important that feature is now", "tokens": [492, 434, 516, 281, 29379, 35826, 577, 1021, 300, 4111, 307, 586], "temperature": 0.0, "avg_logprob": -0.1498971149839204, "compression_ratio": 1.6367521367521367, "no_speech_prob": 1.3081728411634685e-06}, {"id": 296, "seek": 139810, "start": 1416.3799999999999, "end": 1418.74, "text": " Here's a really important point though if you", "tokens": [1692, 311, 257, 534, 1021, 935, 1673, 498, 291], "temperature": 0.0, "avg_logprob": -0.1498971149839204, "compression_ratio": 1.6367521367521367, "no_speech_prob": 1.3081728411634685e-06}, {"id": 297, "seek": 139810, "start": 1420.5, "end": 1426.34, "text": " It's pretty much always possible to create a simple like logistic regression", "tokens": [467, 311, 1238, 709, 1009, 1944, 281, 1884, 257, 2199, 411, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.1498971149839204, "compression_ratio": 1.6367521367521367, "no_speech_prob": 1.3081728411634685e-06}, {"id": 298, "seek": 142634, "start": 1426.34, "end": 1431.78, "text": " Which is as good as pretty much any random forest if you know ahead of time", "tokens": [3013, 307, 382, 665, 382, 1238, 709, 604, 4974, 6719, 498, 291, 458, 2286, 295, 565], "temperature": 0.0, "avg_logprob": -0.16757968980438856, "compression_ratio": 1.7676348547717842, "no_speech_prob": 5.368729603105749e-07}, {"id": 299, "seek": 142634, "start": 1432.26, "end": 1439.5, "text": " Exactly what variables you need exactly how they interact exactly how they need to be transformed and so forth right so in this case for example", "tokens": [7587, 437, 9102, 291, 643, 2293, 577, 436, 4648, 2293, 577, 436, 643, 281, 312, 16894, 293, 370, 5220, 558, 370, 294, 341, 1389, 337, 1365], "temperature": 0.0, "avg_logprob": -0.16757968980438856, "compression_ratio": 1.7676348547717842, "no_speech_prob": 5.368729603105749e-07}, {"id": 300, "seek": 142634, "start": 1439.8999999999999, "end": 1443.86, "text": " We could have created a new field which was equal to year made", "tokens": [492, 727, 362, 2942, 257, 777, 2519, 597, 390, 2681, 281, 1064, 1027], "temperature": 0.0, "avg_logprob": -0.16757968980438856, "compression_ratio": 1.7676348547717842, "no_speech_prob": 5.368729603105749e-07}, {"id": 301, "seek": 142634, "start": 1444.3799999999999, "end": 1451.4599999999998, "text": " So sale date or sale year minus year made and we could have fed that to a model and got you know", "tokens": [407, 8680, 4002, 420, 8680, 1064, 3175, 1064, 1027, 293, 321, 727, 362, 4636, 300, 281, 257, 2316, 293, 658, 291, 458], "temperature": 0.0, "avg_logprob": -0.16757968980438856, "compression_ratio": 1.7676348547717842, "no_speech_prob": 5.368729603105749e-07}, {"id": 302, "seek": 145146, "start": 1451.46, "end": 1457.82, "text": " Got that interaction for us, but the point is we never know that like you", "tokens": [5803, 300, 9285, 337, 505, 11, 457, 264, 935, 307, 321, 1128, 458, 300, 411, 291], "temperature": 0.0, "avg_logprob": -0.13434571845858706, "compression_ratio": 1.8225108225108224, "no_speech_prob": 3.7266233903210377e-06}, {"id": 303, "seek": 145146, "start": 1457.82, "end": 1460.02, "text": " You never like you might have a guess of it", "tokens": [509, 1128, 411, 291, 1062, 362, 257, 2041, 295, 309], "temperature": 0.0, "avg_logprob": -0.13434571845858706, "compression_ratio": 1.8225108225108224, "no_speech_prob": 3.7266233903210377e-06}, {"id": 304, "seek": 145146, "start": 1460.02, "end": 1462.02, "text": " I think some of these things are interacted in this way", "tokens": [286, 519, 512, 295, 613, 721, 366, 49621, 294, 341, 636], "temperature": 0.0, "avg_logprob": -0.13434571845858706, "compression_ratio": 1.8225108225108224, "no_speech_prob": 3.7266233903210377e-06}, {"id": 305, "seek": 145146, "start": 1462.02, "end": 1464.58, "text": " And I think this thing we need to take the log and so forth", "tokens": [400, 286, 519, 341, 551, 321, 643, 281, 747, 264, 3565, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.13434571845858706, "compression_ratio": 1.8225108225108224, "no_speech_prob": 3.7266233903210377e-06}, {"id": 306, "seek": 145146, "start": 1464.82, "end": 1470.3400000000001, "text": " But you know the truth is that the way the world works the causal structures", "tokens": [583, 291, 458, 264, 3494, 307, 300, 264, 636, 264, 1002, 1985, 264, 38755, 9227], "temperature": 0.0, "avg_logprob": -0.13434571845858706, "compression_ratio": 1.8225108225108224, "no_speech_prob": 3.7266233903210377e-06}, {"id": 307, "seek": 145146, "start": 1470.3400000000001, "end": 1477.1200000000001, "text": " You know they've got many many things interacting in many many subtle ways right and so that's why using trees", "tokens": [509, 458, 436, 600, 658, 867, 867, 721, 18017, 294, 867, 867, 13743, 2098, 558, 293, 370, 300, 311, 983, 1228, 5852], "temperature": 0.0, "avg_logprob": -0.13434571845858706, "compression_ratio": 1.8225108225108224, "no_speech_prob": 3.7266233903210377e-06}, {"id": 308, "seek": 147712, "start": 1477.12, "end": 1481.7199999999998, "text": " Whether it be gradient boosting machines or random forests works, so well", "tokens": [8503, 309, 312, 16235, 43117, 8379, 420, 4974, 21700, 1985, 11, 370, 731], "temperature": 0.0, "avg_logprob": -0.20845049780768318, "compression_ratio": 1.4830917874396135, "no_speech_prob": 1.172636984847486e-06}, {"id": 309, "seek": 147712, "start": 1482.56, "end": 1484.78, "text": " So can you pass that to Terrence place?", "tokens": [407, 393, 291, 1320, 300, 281, 6564, 10760, 1081, 30], "temperature": 0.0, "avg_logprob": -0.20845049780768318, "compression_ratio": 1.4830917874396135, "no_speech_prob": 1.172636984847486e-06}, {"id": 310, "seek": 147712, "start": 1488.6399999999999, "end": 1493.8799999999999, "text": " One thing that bit me years ago was also I tried that", "tokens": [1485, 551, 300, 857, 385, 924, 2057, 390, 611, 286, 3031, 300], "temperature": 0.0, "avg_logprob": -0.20845049780768318, "compression_ratio": 1.4830917874396135, "no_speech_prob": 1.172636984847486e-06}, {"id": 311, "seek": 147712, "start": 1495.08, "end": 1502.1999999999998, "text": " Doing one variable at a time thinking oh well. I'll figure out which one's most correlated with the dependent variable, but what it doesn't", "tokens": [18496, 472, 7006, 412, 257, 565, 1953, 1954, 731, 13, 286, 603, 2573, 484, 597, 472, 311, 881, 38574, 365, 264, 12334, 7006, 11, 457, 437, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.20845049780768318, "compression_ratio": 1.4830917874396135, "no_speech_prob": 1.172636984847486e-06}, {"id": 312, "seek": 150220, "start": 1502.2, "end": 1510.3600000000001, "text": " Pull apart is that what if all variables are basically copied the same variable then they're all going to seem equally important", "tokens": [15074, 4936, 307, 300, 437, 498, 439, 9102, 366, 1936, 25365, 264, 912, 7006, 550, 436, 434, 439, 516, 281, 1643, 12309, 1021], "temperature": 0.0, "avg_logprob": -0.18071153889531674, "compression_ratio": 1.6233766233766234, "no_speech_prob": 2.482454647179111e-06}, {"id": 313, "seek": 150220, "start": 1510.3600000000001, "end": 1512.66, "text": " But in fact, it's really just one factor", "tokens": [583, 294, 1186, 11, 309, 311, 534, 445, 472, 5952], "temperature": 0.0, "avg_logprob": -0.18071153889531674, "compression_ratio": 1.6233766233766234, "no_speech_prob": 2.482454647179111e-06}, {"id": 314, "seek": 150220, "start": 1514.0, "end": 1518.68, "text": " Yeah, and that's also true here, so if we had like a column", "tokens": [865, 11, 293, 300, 311, 611, 2074, 510, 11, 370, 498, 321, 632, 411, 257, 7738], "temperature": 0.0, "avg_logprob": -0.18071153889531674, "compression_ratio": 1.6233766233766234, "no_speech_prob": 2.482454647179111e-06}, {"id": 315, "seek": 150220, "start": 1519.72, "end": 1521.24, "text": " appeared twice", "tokens": [8516, 6091], "temperature": 0.0, "avg_logprob": -0.18071153889531674, "compression_ratio": 1.6233766233766234, "no_speech_prob": 2.482454647179111e-06}, {"id": 316, "seek": 150220, "start": 1521.24, "end": 1529.1200000000001, "text": " Right then shuffling that column isn't going to make the model much worse right there'll be if you think about like how it's built", "tokens": [1779, 550, 402, 1245, 1688, 300, 7738, 1943, 380, 516, 281, 652, 264, 2316, 709, 5324, 558, 456, 603, 312, 498, 291, 519, 466, 411, 577, 309, 311, 3094], "temperature": 0.0, "avg_logprob": -0.18071153889531674, "compression_ratio": 1.6233766233766234, "no_speech_prob": 2.482454647179111e-06}, {"id": 317, "seek": 152912, "start": 1529.12, "end": 1534.9599999999998, "text": " Some of the times particularly if we had like max features is point five and some of the times", "tokens": [2188, 295, 264, 1413, 4098, 498, 321, 632, 411, 11469, 4122, 307, 935, 1732, 293, 512, 295, 264, 1413], "temperature": 0.0, "avg_logprob": -0.18237630252180428, "compression_ratio": 2.1095238095238096, "no_speech_prob": 1.1544572089405847e-06}, {"id": 318, "seek": 152912, "start": 1534.9599999999998, "end": 1539.12, "text": " We're going to get version a of the column some of the times you get going to get version B of the column so like", "tokens": [492, 434, 516, 281, 483, 3037, 257, 295, 264, 7738, 512, 295, 264, 1413, 291, 483, 516, 281, 483, 3037, 363, 295, 264, 7738, 370, 411], "temperature": 0.0, "avg_logprob": -0.18237630252180428, "compression_ratio": 2.1095238095238096, "no_speech_prob": 1.1544572089405847e-06}, {"id": 319, "seek": 152912, "start": 1539.7199999999998, "end": 1541.7199999999998, "text": " half the time", "tokens": [1922, 264, 565], "temperature": 0.0, "avg_logprob": -0.18237630252180428, "compression_ratio": 2.1095238095238096, "no_speech_prob": 1.1544572089405847e-06}, {"id": 320, "seek": 152912, "start": 1542.1599999999999, "end": 1545.9799999999998, "text": " Shuffling cut version a of the column is going to make a tree a bit worse half the time", "tokens": [1160, 1245, 1688, 1723, 3037, 257, 295, 264, 7738, 307, 516, 281, 652, 257, 4230, 257, 857, 5324, 1922, 264, 565], "temperature": 0.0, "avg_logprob": -0.18237630252180428, "compression_ratio": 2.1095238095238096, "no_speech_prob": 1.1544572089405847e-06}, {"id": 321, "seek": 152912, "start": 1545.9799999999998, "end": 1549.08, "text": " It's going to make you know column B. I'll make it a bit worse, and so it'll show", "tokens": [467, 311, 516, 281, 652, 291, 458, 7738, 363, 13, 286, 603, 652, 309, 257, 857, 5324, 11, 293, 370, 309, 603, 855], "temperature": 0.0, "avg_logprob": -0.18237630252180428, "compression_ratio": 2.1095238095238096, "no_speech_prob": 1.1544572089405847e-06}, {"id": 322, "seek": 152912, "start": 1549.76, "end": 1551.9599999999998, "text": " that both of those features are", "tokens": [300, 1293, 295, 729, 4122, 366], "temperature": 0.0, "avg_logprob": -0.18237630252180428, "compression_ratio": 2.1095238095238096, "no_speech_prob": 1.1544572089405847e-06}, {"id": 323, "seek": 152912, "start": 1553.28, "end": 1555.0, "text": " somewhat important", "tokens": [8344, 1021], "temperature": 0.0, "avg_logprob": -0.18237630252180428, "compression_ratio": 2.1095238095238096, "no_speech_prob": 1.1544572089405847e-06}, {"id": 324, "seek": 155500, "start": 1555.0, "end": 1559.76, "text": " And it'll kind of like share the importance between the two features, and so this is why", "tokens": [400, 309, 603, 733, 295, 411, 2073, 264, 7379, 1296, 264, 732, 4122, 11, 293, 370, 341, 307, 983], "temperature": 0.0, "avg_logprob": -0.22986888885498047, "compression_ratio": 1.881578947368421, "no_speech_prob": 1.0845101314771455e-06}, {"id": 325, "seek": 155500, "start": 1562.08, "end": 1563.32, "text": " I'll write", "tokens": [286, 603, 2464], "temperature": 0.0, "avg_logprob": -0.22986888885498047, "compression_ratio": 1.881578947368421, "no_speech_prob": 1.0845101314771455e-06}, {"id": 326, "seek": 155500, "start": 1563.32, "end": 1568.62, "text": " Collinearity but collinearity literally means that they're linearly related so this isn't quite right", "tokens": [4586, 533, 17409, 457, 1263, 533, 17409, 3736, 1355, 300, 436, 434, 43586, 4077, 370, 341, 1943, 380, 1596, 558], "temperature": 0.0, "avg_logprob": -0.22986888885498047, "compression_ratio": 1.881578947368421, "no_speech_prob": 1.0845101314771455e-06}, {"id": 327, "seek": 155500, "start": 1569.16, "end": 1576.64, "text": " But this is why having two variables that are related closely related to each other or more variables that are closely related to each other", "tokens": [583, 341, 307, 983, 1419, 732, 9102, 300, 366, 4077, 8185, 4077, 281, 1184, 661, 420, 544, 9102, 300, 366, 8185, 4077, 281, 1184, 661], "temperature": 0.0, "avg_logprob": -0.22986888885498047, "compression_ratio": 1.881578947368421, "no_speech_prob": 1.0845101314771455e-06}, {"id": 328, "seek": 155500, "start": 1576.8, "end": 1578.8, "text": " Means that you will often", "tokens": [40290, 300, 291, 486, 2049], "temperature": 0.0, "avg_logprob": -0.22986888885498047, "compression_ratio": 1.881578947368421, "no_speech_prob": 1.0845101314771455e-06}, {"id": 329, "seek": 155500, "start": 1579.96, "end": 1583.24, "text": " Underestimate their importance using this this random forest", "tokens": [6974, 377, 2905, 641, 7379, 1228, 341, 341, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.22986888885498047, "compression_ratio": 1.881578947368421, "no_speech_prob": 1.0845101314771455e-06}, {"id": 330, "seek": 158324, "start": 1583.24, "end": 1585.24, "text": " technique", "tokens": [6532], "temperature": 0.0, "avg_logprob": -0.18115877550701762, "compression_ratio": 1.509433962264151, "no_speech_prob": 1.482353013670945e-06}, {"id": 331, "seek": 158324, "start": 1585.56, "end": 1589.86, "text": " Yes Terrence and so once we've shuffled and we get a", "tokens": [1079, 6564, 10760, 293, 370, 1564, 321, 600, 402, 33974, 293, 321, 483, 257], "temperature": 0.0, "avg_logprob": -0.18115877550701762, "compression_ratio": 1.509433962264151, "no_speech_prob": 1.482353013670945e-06}, {"id": 332, "seek": 158324, "start": 1590.68, "end": 1592.16, "text": " new model", "tokens": [777, 2316], "temperature": 0.0, "avg_logprob": -0.18115877550701762, "compression_ratio": 1.509433962264151, "no_speech_prob": 1.482353013670945e-06}, {"id": 333, "seek": 158324, "start": 1592.16, "end": 1597.6, "text": " What exactly are the units of these importances is this a change in the R squared? Yeah?", "tokens": [708, 2293, 366, 264, 6815, 295, 613, 974, 2676, 307, 341, 257, 1319, 294, 264, 497, 8889, 30, 865, 30], "temperature": 0.0, "avg_logprob": -0.18115877550701762, "compression_ratio": 1.509433962264151, "no_speech_prob": 1.482353013670945e-06}, {"id": 334, "seek": 158324, "start": 1597.6, "end": 1601.56, "text": " I mean it depends on the library. We're using so the units are kind of like I", "tokens": [286, 914, 309, 5946, 322, 264, 6405, 13, 492, 434, 1228, 370, 264, 6815, 366, 733, 295, 411, 286], "temperature": 0.0, "avg_logprob": -0.18115877550701762, "compression_ratio": 1.509433962264151, "no_speech_prob": 1.482353013670945e-06}, {"id": 335, "seek": 158324, "start": 1603.16, "end": 1608.32, "text": " Never think about them. I just kind of know that like in this particular library", "tokens": [7344, 519, 466, 552, 13, 286, 445, 733, 295, 458, 300, 411, 294, 341, 1729, 6405], "temperature": 0.0, "avg_logprob": -0.18115877550701762, "compression_ratio": 1.509433962264151, "no_speech_prob": 1.482353013670945e-06}, {"id": 336, "seek": 160832, "start": 1608.32, "end": 1612.56, "text": " You know point oh oh five is often kind of a cutoff", "tokens": [509, 458, 935, 1954, 1954, 1732, 307, 2049, 733, 295, 257, 1723, 4506], "temperature": 0.0, "avg_logprob": -0.19476445515950522, "compression_ratio": 1.7088607594936709, "no_speech_prob": 3.466321061296185e-07}, {"id": 337, "seek": 160832, "start": 1612.56, "end": 1618.24, "text": " I would tend to use but all I actually care about is is this picture right which is the", "tokens": [286, 576, 3928, 281, 764, 457, 439, 286, 767, 1127, 466, 307, 307, 341, 3036, 558, 597, 307, 264], "temperature": 0.0, "avg_logprob": -0.19476445515950522, "compression_ratio": 1.7088607594936709, "no_speech_prob": 3.466321061296185e-07}, {"id": 338, "seek": 160832, "start": 1619.2, "end": 1621.2, "text": " feature importance", "tokens": [4111, 7379], "temperature": 0.0, "avg_logprob": -0.19476445515950522, "compression_ratio": 1.7088607594936709, "no_speech_prob": 3.466321061296185e-07}, {"id": 339, "seek": 160832, "start": 1621.9199999999998, "end": 1627.8, "text": " Ordered for each variable and then kind of zooming in turning into a bar plot, and I'm kind of like okay, you know", "tokens": [16321, 292, 337, 1184, 7006, 293, 550, 733, 295, 48226, 294, 6246, 666, 257, 2159, 7542, 11, 293, 286, 478, 733, 295, 411, 1392, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.19476445515950522, "compression_ratio": 1.7088607594936709, "no_speech_prob": 3.466321061296185e-07}, {"id": 340, "seek": 160832, "start": 1628.6399999999999, "end": 1632.48, "text": " Here they're all pretty flat, and I can see okay", "tokens": [1692, 436, 434, 439, 1238, 4962, 11, 293, 286, 393, 536, 1392], "temperature": 0.0, "avg_logprob": -0.19476445515950522, "compression_ratio": 1.7088607594936709, "no_speech_prob": 3.466321061296185e-07}, {"id": 341, "seek": 163248, "start": 1632.48, "end": 1639.04, "text": " That's about point oh oh five and so I remove them at that point and just see like the model", "tokens": [663, 311, 466, 935, 1954, 1954, 1732, 293, 370, 286, 4159, 552, 412, 300, 935, 293, 445, 536, 411, 264, 2316], "temperature": 0.0, "avg_logprob": -0.18592125010267596, "compression_ratio": 1.766798418972332, "no_speech_prob": 2.1233665847830707e-06}, {"id": 342, "seek": 163248, "start": 1639.04, "end": 1642.48, "text": " Hopefully the validation score didn't get worse, and if it did get worse", "tokens": [10429, 264, 24071, 6175, 994, 380, 483, 5324, 11, 293, 498, 309, 630, 483, 5324], "temperature": 0.0, "avg_logprob": -0.18592125010267596, "compression_ratio": 1.766798418972332, "no_speech_prob": 2.1233665847830707e-06}, {"id": 343, "seek": 163248, "start": 1642.48, "end": 1647.4, "text": " So I just increase this a little bit sorry decrease this a little bit until it it doesn't get worse", "tokens": [407, 286, 445, 3488, 341, 257, 707, 857, 2597, 11514, 341, 257, 707, 857, 1826, 309, 309, 1177, 380, 483, 5324], "temperature": 0.0, "avg_logprob": -0.18592125010267596, "compression_ratio": 1.766798418972332, "no_speech_prob": 2.1233665847830707e-06}, {"id": 344, "seek": 163248, "start": 1648.72, "end": 1653.38, "text": " So yeah the the the the units of measure of this don't matter too much", "tokens": [407, 1338, 264, 264, 264, 264, 6815, 295, 3481, 295, 341, 500, 380, 1871, 886, 709], "temperature": 0.0, "avg_logprob": -0.18592125010267596, "compression_ratio": 1.766798418972332, "no_speech_prob": 2.1233665847830707e-06}, {"id": 345, "seek": 165338, "start": 1653.38, "end": 1662.5, "text": " And we'll learn later about a second way of doing variable importance by the way can you pass that over there?", "tokens": [400, 321, 603, 1466, 1780, 466, 257, 1150, 636, 295, 884, 7006, 7379, 538, 264, 636, 393, 291, 1320, 300, 670, 456, 30], "temperature": 0.0, "avg_logprob": -0.2303605137101139, "compression_ratio": 1.5544554455445545, "no_speech_prob": 4.78506444778759e-06}, {"id": 346, "seek": 165338, "start": 1662.7, "end": 1667.6200000000001, "text": " Is one of the goals here to remove variables that I", "tokens": [1119, 472, 295, 264, 5493, 510, 281, 4159, 9102, 300, 286], "temperature": 0.0, "avg_logprob": -0.2303605137101139, "compression_ratio": 1.5544554455445545, "no_speech_prob": 4.78506444778759e-06}, {"id": 347, "seek": 165338, "start": 1668.9, "end": 1669.94, "text": " guess", "tokens": [2041], "temperature": 0.0, "avg_logprob": -0.2303605137101139, "compression_ratio": 1.5544554455445545, "no_speech_prob": 4.78506444778759e-06}, {"id": 348, "seek": 165338, "start": 1669.94, "end": 1672.14, "text": " Your study your score will not", "tokens": [2260, 2979, 428, 6175, 486, 406], "temperature": 0.0, "avg_logprob": -0.2303605137101139, "compression_ratio": 1.5544554455445545, "no_speech_prob": 4.78506444778759e-06}, {"id": 349, "seek": 165338, "start": 1673.2600000000002, "end": 1678.94, "text": " Get worse if you remove them, so you might as well get rid of them. Yeah, so that's what we're going to do next so", "tokens": [3240, 5324, 498, 291, 4159, 552, 11, 370, 291, 1062, 382, 731, 483, 3973, 295, 552, 13, 865, 11, 370, 300, 311, 437, 321, 434, 516, 281, 360, 958, 370], "temperature": 0.0, "avg_logprob": -0.2303605137101139, "compression_ratio": 1.5544554455445545, "no_speech_prob": 4.78506444778759e-06}, {"id": 350, "seek": 167894, "start": 1678.94, "end": 1684.8200000000002, "text": " So what having looked at our feature importance plot we said okay?", "tokens": [407, 437, 1419, 2956, 412, 527, 4111, 7379, 7542, 321, 848, 1392, 30], "temperature": 0.0, "avg_logprob": -0.22718457380930582, "compression_ratio": 1.6824034334763949, "no_speech_prob": 4.785061719303485e-06}, {"id": 351, "seek": 167894, "start": 1684.8200000000002, "end": 1688.02, "text": " It looks like the ones like less than point oh oh five", "tokens": [467, 1542, 411, 264, 2306, 411, 1570, 813, 935, 1954, 1954, 1732], "temperature": 0.0, "avg_logprob": -0.22718457380930582, "compression_ratio": 1.6824034334763949, "no_speech_prob": 4.785061719303485e-06}, {"id": 352, "seek": 167894, "start": 1688.74, "end": 1692.5800000000002, "text": " You know a kind of this long tail of boringness", "tokens": [509, 458, 257, 733, 295, 341, 938, 6838, 295, 9989, 1287], "temperature": 0.0, "avg_logprob": -0.22718457380930582, "compression_ratio": 1.6824034334763949, "no_speech_prob": 4.785061719303485e-06}, {"id": 353, "seek": 167894, "start": 1693.02, "end": 1699.8400000000001, "text": " So I said let's try removing them right so let's just try grabbing the columns where it's greater than point oh oh five", "tokens": [407, 286, 848, 718, 311, 853, 12720, 552, 558, 370, 718, 311, 445, 853, 23771, 264, 13766, 689, 309, 311, 5044, 813, 935, 1954, 1954, 1732], "temperature": 0.0, "avg_logprob": -0.22718457380930582, "compression_ratio": 1.6824034334763949, "no_speech_prob": 4.785061719303485e-06}, {"id": 354, "seek": 167894, "start": 1699.8400000000001, "end": 1706.78, "text": " And I said let's create a new data frame called DF keep which is DF train with just those kept columns", "tokens": [400, 286, 848, 718, 311, 1884, 257, 777, 1412, 3920, 1219, 48336, 1066, 597, 307, 48336, 3847, 365, 445, 729, 4305, 13766], "temperature": 0.0, "avg_logprob": -0.22718457380930582, "compression_ratio": 1.6824034334763949, "no_speech_prob": 4.785061719303485e-06}, {"id": 355, "seek": 170678, "start": 1706.78, "end": 1714.54, "text": " created a new training and validation set with just those columns created a new random forest, and I looked to see how the", "tokens": [2942, 257, 777, 3097, 293, 24071, 992, 365, 445, 729, 13766, 2942, 257, 777, 4974, 6719, 11, 293, 286, 2956, 281, 536, 577, 264], "temperature": 0.0, "avg_logprob": -0.1585318280249527, "compression_ratio": 1.7413793103448276, "no_speech_prob": 2.68418489213218e-06}, {"id": 356, "seek": 170678, "start": 1715.74, "end": 1721.94, "text": " Validation set score and the validation set RMSE changed, and I found they got a tiny bit better", "tokens": [7188, 327, 399, 992, 6175, 293, 264, 24071, 992, 23790, 5879, 3105, 11, 293, 286, 1352, 436, 658, 257, 5870, 857, 1101], "temperature": 0.0, "avg_logprob": -0.1585318280249527, "compression_ratio": 1.7413793103448276, "no_speech_prob": 2.68418489213218e-06}, {"id": 357, "seek": 170678, "start": 1722.8999999999999, "end": 1728.42, "text": " So if they're about the same or a tiny bit better than the thinking my thinking is well", "tokens": [407, 498, 436, 434, 466, 264, 912, 420, 257, 5870, 857, 1101, 813, 264, 1953, 452, 1953, 307, 731], "temperature": 0.0, "avg_logprob": -0.1585318280249527, "compression_ratio": 1.7413793103448276, "no_speech_prob": 2.68418489213218e-06}, {"id": 358, "seek": 170678, "start": 1728.42, "end": 1734.58, "text": " This is just as good a model, but it's now simpler and so now when I redo the feature importance", "tokens": [639, 307, 445, 382, 665, 257, 2316, 11, 457, 309, 311, 586, 18587, 293, 370, 586, 562, 286, 29956, 264, 4111, 7379], "temperature": 0.0, "avg_logprob": -0.1585318280249527, "compression_ratio": 1.7413793103448276, "no_speech_prob": 2.68418489213218e-06}, {"id": 359, "seek": 173458, "start": 1734.58, "end": 1736.58, "text": " There's less collinearity", "tokens": [821, 311, 1570, 1263, 533, 17409], "temperature": 0.0, "avg_logprob": -0.20138403709898603, "compression_ratio": 2.005128205128205, "no_speech_prob": 1.5056974689287017e-06}, {"id": 360, "seek": 173458, "start": 1737.4199999999998, "end": 1742.08, "text": " Right and so in this case I saw that year made went from being like", "tokens": [1779, 293, 370, 294, 341, 1389, 286, 1866, 300, 1064, 1027, 1437, 490, 885, 411], "temperature": 0.0, "avg_logprob": -0.20138403709898603, "compression_ratio": 2.005128205128205, "no_speech_prob": 1.5056974689287017e-06}, {"id": 361, "seek": 173458, "start": 1742.6999999999998, "end": 1746.58, "text": " quite a bit better than the next best thing which was coupler system to", "tokens": [1596, 257, 857, 1101, 813, 264, 958, 1151, 551, 597, 390, 1384, 22732, 1185, 281], "temperature": 0.0, "avg_logprob": -0.20138403709898603, "compression_ratio": 2.005128205128205, "no_speech_prob": 1.5056974689287017e-06}, {"id": 362, "seek": 173458, "start": 1747.6999999999998, "end": 1749.6999999999998, "text": " Way better than the next best thing", "tokens": [9558, 1101, 813, 264, 958, 1151, 551], "temperature": 0.0, "avg_logprob": -0.20138403709898603, "compression_ratio": 2.005128205128205, "no_speech_prob": 1.5056974689287017e-06}, {"id": 363, "seek": 173458, "start": 1749.8999999999999, "end": 1756.74, "text": " right and coupler system went from being like quite a bit more important than the next two to", "tokens": [558, 293, 1384, 22732, 1185, 1437, 490, 885, 411, 1596, 257, 857, 544, 1021, 813, 264, 958, 732, 281], "temperature": 0.0, "avg_logprob": -0.20138403709898603, "compression_ratio": 2.005128205128205, "no_speech_prob": 1.5056974689287017e-06}, {"id": 364, "seek": 173458, "start": 1757.54, "end": 1763.08, "text": " Equally important to the next two so it did seem to definitely change these feature importances", "tokens": [15624, 379, 1021, 281, 264, 958, 732, 370, 309, 630, 1643, 281, 2138, 1319, 613, 4111, 974, 2676], "temperature": 0.0, "avg_logprob": -0.20138403709898603, "compression_ratio": 2.005128205128205, "no_speech_prob": 1.5056974689287017e-06}, {"id": 365, "seek": 176308, "start": 1763.08, "end": 1765.08, "text": " And hopefully give me some more insight there", "tokens": [400, 4696, 976, 385, 512, 544, 11269, 456], "temperature": 0.0, "avg_logprob": -0.17665262972370963, "compression_ratio": 1.5560538116591929, "no_speech_prob": 7.183153684309218e-06}, {"id": 366, "seek": 176308, "start": 1770.06, "end": 1771.86, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.17665262972370963, "compression_ratio": 1.5560538116591929, "no_speech_prob": 7.183153684309218e-06}, {"id": 367, "seek": 176308, "start": 1771.86, "end": 1777.6999999999998, "text": " How does that help our model in general like what does it mean that you're made is now way ahead of the others?", "tokens": [1012, 775, 300, 854, 527, 2316, 294, 2674, 411, 437, 775, 309, 914, 300, 291, 434, 1027, 307, 586, 636, 2286, 295, 264, 2357, 30], "temperature": 0.0, "avg_logprob": -0.17665262972370963, "compression_ratio": 1.5560538116591929, "no_speech_prob": 7.183153684309218e-06}, {"id": 368, "seek": 176308, "start": 1777.6999999999998, "end": 1781.6999999999998, "text": " Yeah, so we're going to dig into that kind of now, but basically", "tokens": [865, 11, 370, 321, 434, 516, 281, 2528, 666, 300, 733, 295, 586, 11, 457, 1936], "temperature": 0.0, "avg_logprob": -0.17665262972370963, "compression_ratio": 1.5560538116591929, "no_speech_prob": 7.183153684309218e-06}, {"id": 369, "seek": 176308, "start": 1782.6599999999999, "end": 1784.6599999999999, "text": " It tells us", "tokens": [467, 5112, 505], "temperature": 0.0, "avg_logprob": -0.17665262972370963, "compression_ratio": 1.5560538116591929, "no_speech_prob": 7.183153684309218e-06}, {"id": 370, "seek": 178466, "start": 1784.66, "end": 1792.66, "text": " That for example if we're looking for like how are we dealing with missing values is there noise in the data?", "tokens": [663, 337, 1365, 498, 321, 434, 1237, 337, 411, 577, 366, 321, 6260, 365, 5361, 4190, 307, 456, 5658, 294, 264, 1412, 30], "temperature": 0.0, "avg_logprob": -0.1784596914773459, "compression_ratio": 1.7713004484304933, "no_speech_prob": 3.4465444969100645e-06}, {"id": 371, "seek": 178466, "start": 1794.18, "end": 1799.0600000000002, "text": " You know if it's a high cardinality categorical variable they're all different steps", "tokens": [509, 458, 498, 309, 311, 257, 1090, 2920, 259, 1860, 19250, 804, 7006, 436, 434, 439, 819, 4439], "temperature": 0.0, "avg_logprob": -0.1784596914773459, "compression_ratio": 1.7713004484304933, "no_speech_prob": 3.4465444969100645e-06}, {"id": 372, "seek": 178466, "start": 1799.0600000000002, "end": 1805.98, "text": " We were take so for example if it was a high cardinality categorical variable that was originally a string right like for example", "tokens": [492, 645, 747, 370, 337, 1365, 498, 309, 390, 257, 1090, 2920, 259, 1860, 19250, 804, 7006, 300, 390, 7993, 257, 6798, 558, 411, 337, 1365], "temperature": 0.0, "avg_logprob": -0.1784596914773459, "compression_ratio": 1.7713004484304933, "no_speech_prob": 3.4465444969100645e-06}, {"id": 373, "seek": 178466, "start": 1805.98, "end": 1810.6200000000001, "text": " I think like maybe fi product class description. I remember one of the", "tokens": [286, 519, 411, 1310, 15848, 1674, 1508, 3855, 13, 286, 1604, 472, 295, 264], "temperature": 0.0, "avg_logprob": -0.1784596914773459, "compression_ratio": 1.7713004484304933, "no_speech_prob": 3.4465444969100645e-06}, {"id": 374, "seek": 181062, "start": 1810.62, "end": 1816.6599999999999, "text": " Ones we looked at the other day had like first of all was the type of vehicle and then a hyphen and then like the size", "tokens": [1282, 279, 321, 2956, 412, 264, 661, 786, 632, 411, 700, 295, 439, 390, 264, 2010, 295, 5864, 293, 550, 257, 2477, 47059, 293, 550, 411, 264, 2744], "temperature": 0.0, "avg_logprob": -0.1420571750268004, "compression_ratio": 1.9518900343642611, "no_speech_prob": 3.668835688586114e-06}, {"id": 375, "seek": 181062, "start": 1816.6599999999999, "end": 1820.62, "text": " Of the vehicle we might look at that and be like okay. Well that was an important column", "tokens": [2720, 264, 5864, 321, 1062, 574, 412, 300, 293, 312, 411, 1392, 13, 1042, 300, 390, 364, 1021, 7738], "temperature": 0.0, "avg_logprob": -0.1420571750268004, "compression_ratio": 1.9518900343642611, "no_speech_prob": 3.668835688586114e-06}, {"id": 376, "seek": 181062, "start": 1820.6599999999999, "end": 1824.8999999999999, "text": " Let's try like splitting it into two on hyphen and then take that bit", "tokens": [961, 311, 853, 411, 30348, 309, 666, 732, 322, 2477, 47059, 293, 550, 747, 300, 857], "temperature": 0.0, "avg_logprob": -0.1420571750268004, "compression_ratio": 1.9518900343642611, "no_speech_prob": 3.668835688586114e-06}, {"id": 377, "seek": 181062, "start": 1824.8999999999999, "end": 1828.6599999999999, "text": " Which is like the size of it and trying you know pause it and convert convert it into an integer", "tokens": [3013, 307, 411, 264, 2744, 295, 309, 293, 1382, 291, 458, 10465, 309, 293, 7620, 7620, 309, 666, 364, 24922], "temperature": 0.0, "avg_logprob": -0.1420571750268004, "compression_ratio": 1.9518900343642611, "no_speech_prob": 3.668835688586114e-06}, {"id": 378, "seek": 181062, "start": 1829.1799999999998, "end": 1833.82, "text": " You know we can try and do some feature engineering and basically until you know which ones are important", "tokens": [509, 458, 321, 393, 853, 293, 360, 512, 4111, 7043, 293, 1936, 1826, 291, 458, 597, 2306, 366, 1021], "temperature": 0.0, "avg_logprob": -0.1420571750268004, "compression_ratio": 1.9518900343642611, "no_speech_prob": 3.668835688586114e-06}, {"id": 379, "seek": 181062, "start": 1834.9399999999998, "end": 1839.4599999999998, "text": " You don't know where to focus that feature engineering time you can talk to your client", "tokens": [509, 500, 380, 458, 689, 281, 1879, 300, 4111, 7043, 565, 291, 393, 751, 281, 428, 6423], "temperature": 0.0, "avg_logprob": -0.1420571750268004, "compression_ratio": 1.9518900343642611, "no_speech_prob": 3.668835688586114e-06}, {"id": 380, "seek": 183946, "start": 1839.46, "end": 1844.82, "text": " You know and say you know or you know if you're doing this inside your workplace", "tokens": [509, 458, 293, 584, 291, 458, 420, 291, 458, 498, 291, 434, 884, 341, 1854, 428, 15328], "temperature": 0.0, "avg_logprob": -0.1921437237713788, "compression_ratio": 1.7615384615384615, "no_speech_prob": 6.24088943368406e-06}, {"id": 381, "seek": 183946, "start": 1845.02, "end": 1849.7, "text": " You go and talk to the folks that like were responsible for creating this data", "tokens": [509, 352, 293, 751, 281, 264, 4024, 300, 411, 645, 6250, 337, 4084, 341, 1412], "temperature": 0.0, "avg_logprob": -0.1921437237713788, "compression_ratio": 1.7615384615384615, "no_speech_prob": 6.24088943368406e-06}, {"id": 382, "seek": 183946, "start": 1849.7, "end": 1852.3400000000001, "text": " so in this if you were actually working at a", "tokens": [370, 294, 341, 498, 291, 645, 767, 1364, 412, 257], "temperature": 0.0, "avg_logprob": -0.1921437237713788, "compression_ratio": 1.7615384615384615, "no_speech_prob": 6.24088943368406e-06}, {"id": 383, "seek": 183946, "start": 1852.94, "end": 1857.74, "text": " bulldozer auction company you might now go to the actual auctioneers and say", "tokens": [4693, 2595, 4527, 24139, 2237, 291, 1062, 586, 352, 281, 264, 3539, 24139, 68, 433, 293, 584], "temperature": 0.0, "avg_logprob": -0.1921437237713788, "compression_ratio": 1.7615384615384615, "no_speech_prob": 6.24088943368406e-06}, {"id": 384, "seek": 183946, "start": 1857.94, "end": 1863.2, "text": " I'm really surprised the coupler system seems to be driving people's pricing decisions so much", "tokens": [286, 478, 534, 6100, 264, 1384, 22732, 1185, 2544, 281, 312, 4840, 561, 311, 17621, 5327, 370, 709], "temperature": 0.0, "avg_logprob": -0.1921437237713788, "compression_ratio": 1.7615384615384615, "no_speech_prob": 6.24088943368406e-06}, {"id": 385, "seek": 183946, "start": 1863.42, "end": 1867.42, "text": " Why do you think that might be and they can say to you? Oh, it's actually because", "tokens": [1545, 360, 291, 519, 300, 1062, 312, 293, 436, 393, 584, 281, 291, 30, 876, 11, 309, 311, 767, 570], "temperature": 0.0, "avg_logprob": -0.1921437237713788, "compression_ratio": 1.7615384615384615, "no_speech_prob": 6.24088943368406e-06}, {"id": 386, "seek": 186742, "start": 1867.42, "end": 1875.26, "text": " only these classes of vehicles have coupler systems or only this manufacturer has coupler systems and so frankly", "tokens": [787, 613, 5359, 295, 8948, 362, 1384, 22732, 3652, 420, 787, 341, 18022, 575, 1384, 22732, 3652, 293, 370, 11939], "temperature": 0.0, "avg_logprob": -0.20620284683402929, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.4465613225620473e-06}, {"id": 387, "seek": 186742, "start": 1875.54, "end": 1881.1000000000001, "text": " This is actually not telling you about coupler systems, but about something else and oh hey that reminds me", "tokens": [639, 307, 767, 406, 3585, 291, 466, 1384, 22732, 3652, 11, 457, 466, 746, 1646, 293, 1954, 4177, 300, 12025, 385], "temperature": 0.0, "avg_logprob": -0.20620284683402929, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.4465613225620473e-06}, {"id": 388, "seek": 186742, "start": 1881.1000000000001, "end": 1886.3000000000002, "text": " That's that that's something else. We actually have measured that it's in this different CSV file", "tokens": [663, 311, 300, 300, 311, 746, 1646, 13, 492, 767, 362, 12690, 300, 309, 311, 294, 341, 819, 48814, 3991], "temperature": 0.0, "avg_logprob": -0.20620284683402929, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.4465613225620473e-06}, {"id": 389, "seek": 186742, "start": 1886.3000000000002, "end": 1890.46, "text": " I'll go get it for you, but kind of helps you focus your attention", "tokens": [286, 603, 352, 483, 309, 337, 291, 11, 457, 733, 295, 3665, 291, 1879, 428, 3202], "temperature": 0.0, "avg_logprob": -0.20620284683402929, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.4465613225620473e-06}, {"id": 390, "seek": 189046, "start": 1890.46, "end": 1895.1000000000001, "text": " So I had a fun little problem this weekend as you know I introduced a couple of", "tokens": [407, 286, 632, 257, 1019, 707, 1154, 341, 6711, 382, 291, 458, 286, 7268, 257, 1916, 295], "temperature": 0.0, "avg_logprob": -0.31468825765175396, "compression_ratio": 1.6398467432950192, "no_speech_prob": 2.4679395210114308e-05}, {"id": 391, "seek": 189046, "start": 1895.74, "end": 1897.74, "text": " crazy computations in", "tokens": [3219, 2807, 763, 294], "temperature": 0.0, "avg_logprob": -0.31468825765175396, "compression_ratio": 1.6398467432950192, "no_speech_prob": 2.4679395210114308e-05}, {"id": 392, "seek": 189046, "start": 1898.22, "end": 1901.22, "text": " Into my random forest and all of a sudden they're like oh my god", "tokens": [23373, 452, 4974, 6719, 293, 439, 295, 257, 3990, 436, 434, 411, 1954, 452, 3044], "temperature": 0.0, "avg_logprob": -0.31468825765175396, "compression_ratio": 1.6398467432950192, "no_speech_prob": 2.4679395210114308e-05}, {"id": 393, "seek": 189046, "start": 1901.22, "end": 1908.18, "text": " These are the most important variables ever squashing all of the others then I got a terrible score and then is that because", "tokens": [1981, 366, 264, 881, 1021, 9102, 1562, 2339, 11077, 439, 295, 264, 2357, 550, 286, 658, 257, 6237, 6175, 293, 550, 307, 300, 570], "temperature": 0.0, "avg_logprob": -0.31468825765175396, "compression_ratio": 1.6398467432950192, "no_speech_prob": 2.4679395210114308e-05}, {"id": 394, "seek": 189046, "start": 1908.66, "end": 1910.94, "text": " Now that I think I have my scores", "tokens": [823, 300, 286, 519, 286, 362, 452, 13444], "temperature": 0.0, "avg_logprob": -0.31468825765175396, "compression_ratio": 1.6398467432950192, "no_speech_prob": 2.4679395210114308e-05}, {"id": 395, "seek": 189046, "start": 1911.26, "end": 1917.3400000000001, "text": " Computed correctly what I noticed is that the importance went through the roof, but the validation set", "tokens": [37804, 292, 8944, 437, 286, 5694, 307, 300, 264, 7379, 1437, 807, 264, 8418, 11, 457, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.31468825765175396, "compression_ratio": 1.6398467432950192, "no_speech_prob": 2.4679395210114308e-05}, {"id": 396, "seek": 191734, "start": 1917.34, "end": 1922.34, "text": " was still bad or got worse is that because somehow that computation", "tokens": [390, 920, 1578, 420, 658, 5324, 307, 300, 570, 6063, 300, 24903], "temperature": 0.0, "avg_logprob": -0.41420649274995053, "compression_ratio": 1.6292682926829267, "no_speech_prob": 3.089468236794346e-06}, {"id": 397, "seek": 191734, "start": 1922.6599999999999, "end": 1924.6599999999999, "text": " allowed the training", "tokens": [4350, 264, 3097], "temperature": 0.0, "avg_logprob": -0.41420649274995053, "compression_ratio": 1.6292682926829267, "no_speech_prob": 3.089468236794346e-06}, {"id": 398, "seek": 191734, "start": 1924.98, "end": 1931.78, "text": " To almost like an identifier map exactly what the answer was going to be for training, but of course that doesn't", "tokens": [1407, 1920, 411, 364, 45690, 4471, 2293, 437, 264, 1867, 390, 516, 281, 312, 337, 3097, 11, 457, 295, 1164, 300, 1177, 380], "temperature": 0.0, "avg_logprob": -0.41420649274995053, "compression_ratio": 1.6292682926829267, "no_speech_prob": 3.089468236794346e-06}, {"id": 399, "seek": 191734, "start": 1932.8999999999999, "end": 1935.8999999999999, "text": " Generalize to the validation set is that what I is that what I observed", "tokens": [6996, 1125, 281, 264, 24071, 992, 307, 300, 437, 286, 307, 300, 437, 286, 13095], "temperature": 0.0, "avg_logprob": -0.41420649274995053, "compression_ratio": 1.6292682926829267, "no_speech_prob": 3.089468236794346e-06}, {"id": 400, "seek": 191734, "start": 1936.58, "end": 1938.58, "text": " Okay, so this", "tokens": [1033, 11, 370, 341], "temperature": 0.0, "avg_logprob": -0.41420649274995053, "compression_ratio": 1.6292682926829267, "no_speech_prob": 3.089468236794346e-06}, {"id": 401, "seek": 191734, "start": 1938.58, "end": 1941.78, "text": " There's two reasons why your validation score", "tokens": [821, 311, 732, 4112, 983, 428, 24071, 6175], "temperature": 0.0, "avg_logprob": -0.41420649274995053, "compression_ratio": 1.6292682926829267, "no_speech_prob": 3.089468236794346e-06}, {"id": 402, "seek": 194178, "start": 1941.78, "end": 1949.54, "text": " Might not be very good. Um, let's go up here", "tokens": [23964, 406, 312, 588, 665, 13, 3301, 11, 718, 311, 352, 493, 510], "temperature": 0.0, "avg_logprob": -0.7858437163920342, "compression_ratio": 1.68944099378882, "no_speech_prob": 6.048850536899408e-06}, {"id": 403, "seek": 194178, "start": 1952.5, "end": 1956.1399999999999, "text": " Okay, so we got these five numbers right the", "tokens": [1033, 11, 370, 321, 658, 613, 1732, 3547, 558, 264], "temperature": 0.0, "avg_logprob": -0.7858437163920342, "compression_ratio": 1.68944099378882, "no_speech_prob": 6.048850536899408e-06}, {"id": 404, "seek": 194178, "start": 1957.42, "end": 1959.66, "text": " RMSE of the training", "tokens": [23790, 5879, 295, 264, 3097], "temperature": 0.0, "avg_logprob": -0.7858437163920342, "compression_ratio": 1.68944099378882, "no_speech_prob": 6.048850536899408e-06}, {"id": 405, "seek": 194178, "start": 1960.46, "end": 1961.86, "text": " validation", "tokens": [24071], "temperature": 0.0, "avg_logprob": -0.7858437163920342, "compression_ratio": 1.68944099378882, "no_speech_prob": 6.048850536899408e-06}, {"id": 406, "seek": 194178, "start": 1961.86, "end": 1965.5, "text": " R squared of the training validation and the R squared of the ORP", "tokens": [497, 8889, 295, 264, 3097, 24071, 293, 264, 497, 8889, 295, 264, 19654, 47], "temperature": 0.0, "avg_logprob": -0.7858437163920342, "compression_ratio": 1.68944099378882, "no_speech_prob": 6.048850536899408e-06}, {"id": 407, "seek": 194178, "start": 1965.82, "end": 1970.1, "text": " Okay, so the first thing that we're going to do is we're going to take a look at the", "tokens": [1033, 11, 370, 264, 700, 551, 300, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 747, 257, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.7858437163920342, "compression_ratio": 1.68944099378882, "no_speech_prob": 6.048850536899408e-06}, {"id": 408, "seek": 197010, "start": 1970.1, "end": 1978.58, "text": " Okay, so there's two reasons and really in the end what we care about like for this Kaggle competition is the RMSE of the validation set", "tokens": [1033, 11, 370, 456, 311, 732, 4112, 293, 534, 294, 264, 917, 437, 321, 1127, 466, 411, 337, 341, 48751, 22631, 6211, 307, 264, 23790, 5879, 295, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.21464109420776367, "compression_ratio": 1.6721311475409837, "no_speech_prob": 3.2887296583794523e-06}, {"id": 409, "seek": 197010, "start": 1978.6999999999998, "end": 1983.1799999999998, "text": " Assuming we've created a good validation set so in Terrence's case", "tokens": [6281, 24919, 321, 600, 2942, 257, 665, 24071, 992, 370, 294, 6564, 10760, 311, 1389], "temperature": 0.0, "avg_logprob": -0.21464109420776367, "compression_ratio": 1.6721311475409837, "no_speech_prob": 3.2887296583794523e-06}, {"id": 410, "seek": 197010, "start": 1983.1799999999998, "end": 1986.1399999999999, "text": " He's saying this number is this thing I care about", "tokens": [634, 311, 1566, 341, 1230, 307, 341, 551, 286, 1127, 466], "temperature": 0.0, "avg_logprob": -0.21464109420776367, "compression_ratio": 1.6721311475409837, "no_speech_prob": 3.2887296583794523e-06}, {"id": 411, "seek": 197010, "start": 1986.9399999999998, "end": 1991.62, "text": " Got worse when I did some feature engineering. Why is that? Okay", "tokens": [5803, 5324, 562, 286, 630, 512, 4111, 7043, 13, 1545, 307, 300, 30, 1033], "temperature": 0.0, "avg_logprob": -0.21464109420776367, "compression_ratio": 1.6721311475409837, "no_speech_prob": 3.2887296583794523e-06}, {"id": 412, "seek": 197010, "start": 1992.3, "end": 1994.3, "text": " There's two possible reasons", "tokens": [821, 311, 732, 1944, 4112], "temperature": 0.0, "avg_logprob": -0.21464109420776367, "compression_ratio": 1.6721311475409837, "no_speech_prob": 3.2887296583794523e-06}, {"id": 413, "seek": 197010, "start": 1994.6999999999998, "end": 1999.2199999999998, "text": " Reason one is that you're overfitting if you're overfitting", "tokens": [39693, 472, 307, 300, 291, 434, 670, 69, 2414, 498, 291, 434, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.21464109420776367, "compression_ratio": 1.6721311475409837, "no_speech_prob": 3.2887296583794523e-06}, {"id": 414, "seek": 199922, "start": 1999.22, "end": 2001.22, "text": " Then your OOB", "tokens": [1396, 428, 422, 46, 33], "temperature": 0.0, "avg_logprob": -0.21605748002247144, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.026130459853448e-06}, {"id": 415, "seek": 199922, "start": 2002.1000000000001, "end": 2004.1000000000001, "text": " Will also get worse", "tokens": [3099, 611, 483, 5324], "temperature": 0.0, "avg_logprob": -0.21605748002247144, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.026130459853448e-06}, {"id": 416, "seek": 199922, "start": 2004.66, "end": 2011.7, "text": " If you're doing a huge data set with a small set RF sample, so you can't use an OOB then instead", "tokens": [759, 291, 434, 884, 257, 2603, 1412, 992, 365, 257, 1359, 992, 26204, 6889, 11, 370, 291, 393, 380, 764, 364, 422, 46, 33, 550, 2602], "temperature": 0.0, "avg_logprob": -0.21605748002247144, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.026130459853448e-06}, {"id": 417, "seek": 199922, "start": 2012.6200000000001, "end": 2015.8600000000001, "text": " Create a second validation set which is a random sample", "tokens": [20248, 257, 1150, 24071, 992, 597, 307, 257, 4974, 6889], "temperature": 0.0, "avg_logprob": -0.21605748002247144, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.026130459853448e-06}, {"id": 418, "seek": 199922, "start": 2016.38, "end": 2023.7, "text": " Okay, and and do that right? So in other words if your OOB or your random sample validation set is", "tokens": [1033, 11, 293, 293, 360, 300, 558, 30, 407, 294, 661, 2283, 498, 428, 422, 46, 33, 420, 428, 4974, 6889, 24071, 992, 307], "temperature": 0.0, "avg_logprob": -0.21605748002247144, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.026130459853448e-06}, {"id": 419, "seek": 202370, "start": 2023.7, "end": 2028.14, "text": " Has got much worse, then you must be overfitting", "tokens": [8646, 658, 709, 5324, 11, 550, 291, 1633, 312, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.191416445232573, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.994429678437882e-06}, {"id": 420, "seek": 202370, "start": 2030.02, "end": 2031.74, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.191416445232573, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.994429678437882e-06}, {"id": 421, "seek": 202370, "start": 2031.74, "end": 2036.22, "text": " Think in your case Terrence, it's unlikely that's the problem because random forests", "tokens": [6557, 294, 428, 1389, 6564, 10760, 11, 309, 311, 17518, 300, 311, 264, 1154, 570, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.191416445232573, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.994429678437882e-06}, {"id": 422, "seek": 202370, "start": 2037.74, "end": 2042.8600000000001, "text": " Don't overfit that badly like it's very hard to get them to overfit that badly", "tokens": [1468, 380, 670, 6845, 300, 13425, 411, 309, 311, 588, 1152, 281, 483, 552, 281, 670, 6845, 300, 13425], "temperature": 0.0, "avg_logprob": -0.191416445232573, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.994429678437882e-06}, {"id": 423, "seek": 202370, "start": 2043.22, "end": 2050.64, "text": " Unless you use some really weird parameters like only one estimator for example like once you've got ten trees in there", "tokens": [16581, 291, 764, 512, 534, 3657, 9834, 411, 787, 472, 8017, 1639, 337, 1365, 411, 1564, 291, 600, 658, 2064, 5852, 294, 456], "temperature": 0.0, "avg_logprob": -0.191416445232573, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.994429678437882e-06}, {"id": 424, "seek": 205064, "start": 2050.64, "end": 2054.72, "text": " There should be enough variation that you know you can definitely overfit", "tokens": [821, 820, 312, 1547, 12990, 300, 291, 458, 291, 393, 2138, 670, 6845], "temperature": 0.0, "avg_logprob": -0.1376940581175658, "compression_ratio": 1.8641975308641976, "no_speech_prob": 2.2959111447562464e-06}, {"id": 425, "seek": 205064, "start": 2054.8799999999997, "end": 2059.06, "text": " But not so much that you're going to destroy your validation score by adding a variable", "tokens": [583, 406, 370, 709, 300, 291, 434, 516, 281, 5293, 428, 24071, 6175, 538, 5127, 257, 7006], "temperature": 0.0, "avg_logprob": -0.1376940581175658, "compression_ratio": 1.8641975308641976, "no_speech_prob": 2.2959111447562464e-06}, {"id": 426, "seek": 205064, "start": 2059.06, "end": 2064.72, "text": " So I think you'll find that's probably not the case, but it's easy to check and if it's not the case", "tokens": [407, 286, 519, 291, 603, 915, 300, 311, 1391, 406, 264, 1389, 11, 457, 309, 311, 1858, 281, 1520, 293, 498, 309, 311, 406, 264, 1389], "temperature": 0.0, "avg_logprob": -0.1376940581175658, "compression_ratio": 1.8641975308641976, "no_speech_prob": 2.2959111447562464e-06}, {"id": 427, "seek": 205064, "start": 2064.92, "end": 2070.7799999999997, "text": " Then you'll see that your OOB score or your random sample validation score hasn't got worse, okay?", "tokens": [1396, 291, 603, 536, 300, 428, 422, 46, 33, 6175, 420, 428, 4974, 6889, 24071, 6175, 6132, 380, 658, 5324, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1376940581175658, "compression_ratio": 1.8641975308641976, "no_speech_prob": 2.2959111447562464e-06}, {"id": 428, "seek": 205064, "start": 2071.2799999999997, "end": 2077.04, "text": " So the second reason your validation score can get worse if your OOB score hasn't got worse", "tokens": [407, 264, 1150, 1778, 428, 24071, 6175, 393, 483, 5324, 498, 428, 422, 46, 33, 6175, 6132, 380, 658, 5324], "temperature": 0.0, "avg_logprob": -0.1376940581175658, "compression_ratio": 1.8641975308641976, "no_speech_prob": 2.2959111447562464e-06}, {"id": 429, "seek": 207704, "start": 2077.04, "end": 2085.2799999999997, "text": " You're not overfitting, but your validation score has got worse that means you're you're doing something that is true in the training set", "tokens": [509, 434, 406, 670, 69, 2414, 11, 457, 428, 24071, 6175, 575, 658, 5324, 300, 1355, 291, 434, 291, 434, 884, 746, 300, 307, 2074, 294, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.15836081138023964, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.22527683035878e-06}, {"id": 430, "seek": 207704, "start": 2085.72, "end": 2087.96, "text": " But not true in the validation set", "tokens": [583, 406, 2074, 294, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.15836081138023964, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.22527683035878e-06}, {"id": 431, "seek": 207704, "start": 2088.7599999999998, "end": 2094.7599999999998, "text": " So this can only happen when your validation set is not a random sample", "tokens": [407, 341, 393, 787, 1051, 562, 428, 24071, 992, 307, 406, 257, 4974, 6889], "temperature": 0.0, "avg_logprob": -0.15836081138023964, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.22527683035878e-06}, {"id": 432, "seek": 207704, "start": 2094.92, "end": 2102.12, "text": " So for example in this bulldozer competition or in the grocery shopping competition. We've intentionally made a validation set", "tokens": [407, 337, 1365, 294, 341, 4693, 2595, 4527, 6211, 420, 294, 264, 14410, 8688, 6211, 13, 492, 600, 22062, 1027, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.15836081138023964, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.22527683035878e-06}, {"id": 433, "seek": 207704, "start": 2102.12, "end": 2106.64, "text": " That's for a different date range. It's for the most recent two weeks, right?", "tokens": [663, 311, 337, 257, 819, 4002, 3613, 13, 467, 311, 337, 264, 881, 5162, 732, 3259, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15836081138023964, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.22527683035878e-06}, {"id": 434, "seek": 210664, "start": 2106.64, "end": 2112.8799999999997, "text": " And so if something different happened in the last two weeks to the previous weeks", "tokens": [400, 370, 498, 746, 819, 2011, 294, 264, 1036, 732, 3259, 281, 264, 3894, 3259], "temperature": 0.0, "avg_logprob": -0.17337058341666445, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.902301275753416e-06}, {"id": 435, "seek": 210664, "start": 2113.96, "end": 2115.48, "text": " then", "tokens": [550], "temperature": 0.0, "avg_logprob": -0.17337058341666445, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.902301275753416e-06}, {"id": 436, "seek": 210664, "start": 2115.48, "end": 2117.3599999999997, "text": " You could totally", "tokens": [509, 727, 3879], "temperature": 0.0, "avg_logprob": -0.17337058341666445, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.902301275753416e-06}, {"id": 437, "seek": 210664, "start": 2117.3599999999997, "end": 2119.92, "text": " Break your validation set so for example", "tokens": [16925, 428, 24071, 992, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.17337058341666445, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.902301275753416e-06}, {"id": 438, "seek": 210664, "start": 2121.2799999999997, "end": 2123.8399999999997, "text": " If there was some kind of unique identifier", "tokens": [759, 456, 390, 512, 733, 295, 3845, 45690], "temperature": 0.0, "avg_logprob": -0.17337058341666445, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.902301275753416e-06}, {"id": 439, "seek": 210664, "start": 2124.7999999999997, "end": 2126.7999999999997, "text": " which is like", "tokens": [597, 307, 411], "temperature": 0.0, "avg_logprob": -0.17337058341666445, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.902301275753416e-06}, {"id": 440, "seek": 210664, "start": 2128.3599999999997, "end": 2134.24, "text": " Different in the two date periods then you could learn to identify things using that identifier in the training set", "tokens": [20825, 294, 264, 732, 4002, 13804, 550, 291, 727, 1466, 281, 5876, 721, 1228, 300, 45690, 294, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.17337058341666445, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.902301275753416e-06}, {"id": 441, "seek": 213424, "start": 2134.24, "end": 2138.9199999999996, "text": " But then like the last two weeks may have a totally different set of IDs for the different set of behavior", "tokens": [583, 550, 411, 264, 1036, 732, 3259, 815, 362, 257, 3879, 819, 992, 295, 48212, 337, 264, 819, 992, 295, 5223], "temperature": 0.0, "avg_logprob": -0.15661660107699307, "compression_ratio": 1.5644444444444445, "no_speech_prob": 1.9947217424487462e-06}, {"id": 442, "seek": 213424, "start": 2139.56, "end": 2141.56, "text": " Could get a lot worse", "tokens": [7497, 483, 257, 688, 5324], "temperature": 0.0, "avg_logprob": -0.15661660107699307, "compression_ratio": 1.5644444444444445, "no_speech_prob": 1.9947217424487462e-06}, {"id": 443, "seek": 213424, "start": 2143.3199999999997, "end": 2151.12, "text": " Yeah, what you're describing is not common though and so I'm a bit skeptical it might be a bug but", "tokens": [865, 11, 437, 291, 434, 16141, 307, 406, 2689, 1673, 293, 370, 286, 478, 257, 857, 28601, 309, 1062, 312, 257, 7426, 457], "temperature": 0.0, "avg_logprob": -0.15661660107699307, "compression_ratio": 1.5644444444444445, "no_speech_prob": 1.9947217424487462e-06}, {"id": 444, "seek": 213424, "start": 2152.8399999999997, "end": 2158.58, "text": " Hopefully there's enough things you can now use to figure out if it is a bug we'll be interested to hear what you learn", "tokens": [10429, 456, 311, 1547, 721, 291, 393, 586, 764, 281, 2573, 484, 498, 309, 307, 257, 7426, 321, 603, 312, 3102, 281, 1568, 437, 291, 1466], "temperature": 0.0, "avg_logprob": -0.15661660107699307, "compression_ratio": 1.5644444444444445, "no_speech_prob": 1.9947217424487462e-06}, {"id": 445, "seek": 213424, "start": 2160.9599999999996, "end": 2162.6, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.15661660107699307, "compression_ratio": 1.5644444444444445, "no_speech_prob": 1.9947217424487462e-06}, {"id": 446, "seek": 216260, "start": 2162.6, "end": 2165.8399999999997, "text": " so that's that's feature importance and so", "tokens": [370, 300, 311, 300, 311, 4111, 7379, 293, 370], "temperature": 0.0, "avg_logprob": -0.1671462235627351, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.29627936379984e-06}, {"id": 447, "seek": 216260, "start": 2167.0, "end": 2169.0, "text": " I'd like to compare that to", "tokens": [286, 1116, 411, 281, 6794, 300, 281], "temperature": 0.0, "avg_logprob": -0.1671462235627351, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.29627936379984e-06}, {"id": 448, "seek": 216260, "start": 2170.3199999999997, "end": 2172.3199999999997, "text": " how feature importance is", "tokens": [577, 4111, 7379, 307], "temperature": 0.0, "avg_logprob": -0.1671462235627351, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.29627936379984e-06}, {"id": 449, "seek": 216260, "start": 2173.24, "end": 2175.24, "text": " normally done in", "tokens": [5646, 1096, 294], "temperature": 0.0, "avg_logprob": -0.1671462235627351, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.29627936379984e-06}, {"id": 450, "seek": 216260, "start": 2175.3199999999997, "end": 2177.3199999999997, "text": " industry and in", "tokens": [3518, 293, 294], "temperature": 0.0, "avg_logprob": -0.1671462235627351, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.29627936379984e-06}, {"id": 451, "seek": 216260, "start": 2177.36, "end": 2182.48, "text": " academic communities outside of machine learning like in psychology and economics and so forth and", "tokens": [7778, 4456, 2380, 295, 3479, 2539, 411, 294, 15105, 293, 14564, 293, 370, 5220, 293], "temperature": 0.0, "avg_logprob": -0.1671462235627351, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.29627936379984e-06}, {"id": 452, "seek": 216260, "start": 2182.68, "end": 2186.08, "text": " Generally speaking people in those kind of environments tend to use", "tokens": [21082, 4124, 561, 294, 729, 733, 295, 12388, 3928, 281, 764], "temperature": 0.0, "avg_logprob": -0.1671462235627351, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.29627936379984e-06}, {"id": 453, "seek": 216260, "start": 2186.96, "end": 2190.64, "text": " Some kind of linear regression logistic regression general linear models", "tokens": [2188, 733, 295, 8213, 24590, 3565, 3142, 24590, 2674, 8213, 5245], "temperature": 0.0, "avg_logprob": -0.1671462235627351, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.29627936379984e-06}, {"id": 454, "seek": 219064, "start": 2190.64, "end": 2195.96, "text": " So they start with their data set and they basically say that was weird", "tokens": [407, 436, 722, 365, 641, 1412, 992, 293, 436, 1936, 584, 300, 390, 3657], "temperature": 0.0, "avg_logprob": -0.19512682075960092, "compression_ratio": 1.9943181818181819, "no_speech_prob": 5.3381063480628654e-06}, {"id": 455, "seek": 219064, "start": 2198.6, "end": 2201.7999999999997, "text": " Oh, okay, so they start with their data set", "tokens": [876, 11, 1392, 11, 370, 436, 722, 365, 641, 1412, 992], "temperature": 0.0, "avg_logprob": -0.19512682075960092, "compression_ratio": 1.9943181818181819, "no_speech_prob": 5.3381063480628654e-06}, {"id": 456, "seek": 219064, "start": 2202.6, "end": 2205.2, "text": " And they say I'm going to assume that I know", "tokens": [400, 436, 584, 286, 478, 516, 281, 6552, 300, 286, 458], "temperature": 0.0, "avg_logprob": -0.19512682075960092, "compression_ratio": 1.9943181818181819, "no_speech_prob": 5.3381063480628654e-06}, {"id": 457, "seek": 219064, "start": 2206.12, "end": 2212.08, "text": " the kind of parametric relationship between my independent variables and my dependent variable", "tokens": [264, 733, 295, 6220, 17475, 2480, 1296, 452, 6695, 9102, 293, 452, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.19512682075960092, "compression_ratio": 1.9943181818181819, "no_speech_prob": 5.3381063480628654e-06}, {"id": 458, "seek": 219064, "start": 2212.08, "end": 2217.16, "text": " so I'm going to assume that it's a linear relationship say or it's a linear relationship with a", "tokens": [370, 286, 478, 516, 281, 6552, 300, 309, 311, 257, 8213, 2480, 584, 420, 309, 311, 257, 8213, 2480, 365, 257], "temperature": 0.0, "avg_logprob": -0.19512682075960092, "compression_ratio": 1.9943181818181819, "no_speech_prob": 5.3381063480628654e-06}, {"id": 459, "seek": 221716, "start": 2217.16, "end": 2222.8399999999997, "text": " link function like a sigmoid you get to create logistic regression say and so", "tokens": [2113, 2445, 411, 257, 4556, 3280, 327, 291, 483, 281, 1884, 3565, 3142, 24590, 584, 293, 370], "temperature": 0.0, "avg_logprob": -0.2511838072089739, "compression_ratio": 1.5844748858447488, "no_speech_prob": 4.289303433324676e-06}, {"id": 460, "seek": 221716, "start": 2223.3199999999997, "end": 2226.7999999999997, "text": " Assuming that I already know that I can now write this as an equation", "tokens": [6281, 24919, 300, 286, 1217, 458, 300, 286, 393, 586, 2464, 341, 382, 364, 5367], "temperature": 0.0, "avg_logprob": -0.2511838072089739, "compression_ratio": 1.5844748858447488, "no_speech_prob": 4.289303433324676e-06}, {"id": 461, "seek": 221716, "start": 2226.7999999999997, "end": 2233.44, "text": " So if we've got like x1 x2 so forth right I can say alright my y values are equal to", "tokens": [407, 498, 321, 600, 658, 411, 2031, 16, 2031, 17, 370, 5220, 558, 286, 393, 584, 5845, 452, 288, 4190, 366, 2681, 281], "temperature": 0.0, "avg_logprob": -0.2511838072089739, "compression_ratio": 1.5844748858447488, "no_speech_prob": 4.289303433324676e-06}, {"id": 462, "seek": 221716, "start": 2233.96, "end": 2237.3399999999997, "text": " a x1 plus b x2", "tokens": [257, 2031, 16, 1804, 272, 2031, 17], "temperature": 0.0, "avg_logprob": -0.2511838072089739, "compression_ratio": 1.5844748858447488, "no_speech_prob": 4.289303433324676e-06}, {"id": 463, "seek": 221716, "start": 2239.0, "end": 2244.8799999999997, "text": " Equals y and therefore I can find out the feature importance easily enough by just looking at these", "tokens": [15624, 1124, 288, 293, 4412, 286, 393, 915, 484, 264, 4111, 7379, 3612, 1547, 538, 445, 1237, 412, 613], "temperature": 0.0, "avg_logprob": -0.2511838072089739, "compression_ratio": 1.5844748858447488, "no_speech_prob": 4.289303433324676e-06}, {"id": 464, "seek": 224488, "start": 2244.88, "end": 2251.56, "text": " Coefficients and saying like which one's the highest particularly if you've normalized the data first right so", "tokens": [3066, 68, 31739, 293, 1566, 411, 597, 472, 311, 264, 6343, 4098, 498, 291, 600, 48704, 264, 1412, 700, 558, 370], "temperature": 0.0, "avg_logprob": -0.16691299887264477, "compression_ratio": 1.5676855895196506, "no_speech_prob": 4.565947165247053e-06}, {"id": 465, "seek": 224488, "start": 2253.08, "end": 2259.6, "text": " There's this kind of trope out there. It's it's very common. Which is that like this is somehow", "tokens": [821, 311, 341, 733, 295, 4495, 494, 484, 456, 13, 467, 311, 309, 311, 588, 2689, 13, 3013, 307, 300, 411, 341, 307, 6063], "temperature": 0.0, "avg_logprob": -0.16691299887264477, "compression_ratio": 1.5676855895196506, "no_speech_prob": 4.565947165247053e-06}, {"id": 466, "seek": 224488, "start": 2261.48, "end": 2267.04, "text": " More accurate or more pure or in some way better way of doing feature importance", "tokens": [5048, 8559, 420, 544, 6075, 420, 294, 512, 636, 1101, 636, 295, 884, 4111, 7379], "temperature": 0.0, "avg_logprob": -0.16691299887264477, "compression_ratio": 1.5676855895196506, "no_speech_prob": 4.565947165247053e-06}, {"id": 467, "seek": 224488, "start": 2267.84, "end": 2272.08, "text": " But that couldn't be further from the truth right if you think about it", "tokens": [583, 300, 2809, 380, 312, 3052, 490, 264, 3494, 558, 498, 291, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.16691299887264477, "compression_ratio": 1.5676855895196506, "no_speech_prob": 4.565947165247053e-06}, {"id": 468, "seek": 227208, "start": 2272.08, "end": 2275.44, "text": " If you were like if you were missing an interaction", "tokens": [759, 291, 645, 411, 498, 291, 645, 5361, 364, 9285], "temperature": 0.0, "avg_logprob": -0.18677668028239963, "compression_ratio": 1.7254901960784315, "no_speech_prob": 3.3405249268980697e-06}, {"id": 469, "seek": 227208, "start": 2276.08, "end": 2279.48, "text": " Right or if you were missing a transformation you needed", "tokens": [1779, 420, 498, 291, 645, 5361, 257, 9887, 291, 2978], "temperature": 0.0, "avg_logprob": -0.18677668028239963, "compression_ratio": 1.7254901960784315, "no_speech_prob": 3.3405249268980697e-06}, {"id": 470, "seek": 227208, "start": 2280.24, "end": 2282.24, "text": " Or if you've anyway", "tokens": [1610, 498, 291, 600, 4033], "temperature": 0.0, "avg_logprob": -0.18677668028239963, "compression_ratio": 1.7254901960784315, "no_speech_prob": 3.3405249268980697e-06}, {"id": 471, "seek": 227208, "start": 2283.48, "end": 2288.96, "text": " Been anything less than a hundred percent perfect in all of your pre-processing so that your", "tokens": [32839, 1340, 1570, 813, 257, 3262, 3043, 2176, 294, 439, 295, 428, 659, 12, 41075, 278, 370, 300, 428], "temperature": 0.0, "avg_logprob": -0.18677668028239963, "compression_ratio": 1.7254901960784315, "no_speech_prob": 3.3405249268980697e-06}, {"id": 472, "seek": 227208, "start": 2289.4, "end": 2296.4, "text": " Model is the absolute correct truth of this situation right unless you've got all of that correct then your coefficients are wrong", "tokens": [17105, 307, 264, 8236, 3006, 3494, 295, 341, 2590, 558, 5969, 291, 600, 658, 439, 295, 300, 3006, 550, 428, 31994, 366, 2085], "temperature": 0.0, "avg_logprob": -0.18677668028239963, "compression_ratio": 1.7254901960784315, "no_speech_prob": 3.3405249268980697e-06}, {"id": 473, "seek": 229640, "start": 2296.4, "end": 2304.88, "text": " All right your coefficients are telling you in your totally wrong model. This is how important those things are right which is basically meaningless", "tokens": [1057, 558, 428, 31994, 366, 3585, 291, 294, 428, 3879, 2085, 2316, 13, 639, 307, 577, 1021, 729, 721, 366, 558, 597, 307, 1936, 33232], "temperature": 0.0, "avg_logprob": -0.21320608217422277, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.626393999591528e-07}, {"id": 474, "seek": 229640, "start": 2306.12, "end": 2307.28, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.21320608217422277, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.626393999591528e-07}, {"id": 475, "seek": 229640, "start": 2307.28, "end": 2312.64, "text": " We're also the random forest feature importance. It's telling you in this extremely", "tokens": [492, 434, 611, 264, 4974, 6719, 4111, 7379, 13, 467, 311, 3585, 291, 294, 341, 4664], "temperature": 0.0, "avg_logprob": -0.21320608217422277, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.626393999591528e-07}, {"id": 476, "seek": 229640, "start": 2313.52, "end": 2320.08, "text": " High parameter highly flexible functional form with few if any statistical assumptions. This is your feature importance", "tokens": [5229, 13075, 5405, 11358, 11745, 1254, 365, 1326, 498, 604, 22820, 17695, 13, 639, 307, 428, 4111, 7379], "temperature": 0.0, "avg_logprob": -0.21320608217422277, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.626393999591528e-07}, {"id": 477, "seek": 229640, "start": 2320.96, "end": 2322.96, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.21320608217422277, "compression_ratio": 1.663594470046083, "no_speech_prob": 5.626393999591528e-07}, {"id": 478, "seek": 232296, "start": 2322.96, "end": 2327.92, "text": " So I would be very cautious you know and and again I can't stress this enough one", "tokens": [407, 286, 576, 312, 588, 25278, 291, 458, 293, 293, 797, 286, 393, 380, 4244, 341, 1547, 472], "temperature": 0.0, "avg_logprob": -0.22720023445461107, "compression_ratio": 1.8440677966101695, "no_speech_prob": 1.5056941720104078e-06}, {"id": 479, "seek": 232296, "start": 2327.92, "end": 2334.4, "text": " You when you leave and sand when you leave this program you are much more often going to see people talk about logistic regression", "tokens": [509, 562, 291, 1856, 293, 4932, 562, 291, 1856, 341, 1461, 291, 366, 709, 544, 2049, 516, 281, 536, 561, 751, 466, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.22720023445461107, "compression_ratio": 1.8440677966101695, "no_speech_prob": 1.5056941720104078e-06}, {"id": 480, "seek": 232296, "start": 2334.76, "end": 2338.48, "text": " coefficients than you're going to see them talk about random forest variable importance and", "tokens": [31994, 813, 291, 434, 516, 281, 536, 552, 751, 466, 4974, 6719, 7006, 7379, 293], "temperature": 0.0, "avg_logprob": -0.22720023445461107, "compression_ratio": 1.8440677966101695, "no_speech_prob": 1.5056941720104078e-06}, {"id": 481, "seek": 232296, "start": 2338.64, "end": 2342.0, "text": " Every time you see that happen you should be very very very skeptical", "tokens": [2048, 565, 291, 536, 300, 1051, 291, 820, 312, 588, 588, 588, 28601], "temperature": 0.0, "avg_logprob": -0.22720023445461107, "compression_ratio": 1.8440677966101695, "no_speech_prob": 1.5056941720104078e-06}, {"id": 482, "seek": 232296, "start": 2342.32, "end": 2349.36, "text": " If what you're saying anytime you read a paper in economics or in psychology or the marketing department tells you they did this regression or whatever", "tokens": [759, 437, 291, 434, 1566, 13038, 291, 1401, 257, 3035, 294, 14564, 420, 294, 15105, 420, 264, 6370, 5882, 5112, 291, 436, 630, 341, 24590, 420, 2035], "temperature": 0.0, "avg_logprob": -0.22720023445461107, "compression_ratio": 1.8440677966101695, "no_speech_prob": 1.5056941720104078e-06}, {"id": 483, "seek": 232296, "start": 2349.92, "end": 2351.44, "text": " every single time", "tokens": [633, 2167, 565], "temperature": 0.0, "avg_logprob": -0.22720023445461107, "compression_ratio": 1.8440677966101695, "no_speech_prob": 1.5056941720104078e-06}, {"id": 484, "seek": 235144, "start": 2351.44, "end": 2355.48, "text": " Those coefficients are going to be massively biased by any", "tokens": [3950, 31994, 366, 516, 281, 312, 29379, 28035, 538, 604], "temperature": 0.0, "avg_logprob": -0.21908792194567228, "compression_ratio": 1.7521008403361344, "no_speech_prob": 5.3381140787678305e-06}, {"id": 485, "seek": 235144, "start": 2356.48, "end": 2358.48, "text": " issues in the model", "tokens": [2663, 294, 264, 2316], "temperature": 0.0, "avg_logprob": -0.21908792194567228, "compression_ratio": 1.7521008403361344, "no_speech_prob": 5.3381140787678305e-06}, {"id": 486, "seek": 235144, "start": 2359.48, "end": 2361.48, "text": " Furthermore", "tokens": [23999], "temperature": 0.0, "avg_logprob": -0.21908792194567228, "compression_ratio": 1.7521008403361344, "no_speech_prob": 5.3381140787678305e-06}, {"id": 487, "seek": 235144, "start": 2361.52, "end": 2366.2000000000003, "text": " If they've done so much pre-processing that actually the model is pretty accurate", "tokens": [759, 436, 600, 1096, 370, 709, 659, 12, 41075, 278, 300, 767, 264, 2316, 307, 1238, 8559], "temperature": 0.0, "avg_logprob": -0.21908792194567228, "compression_ratio": 1.7521008403361344, "no_speech_prob": 5.3381140787678305e-06}, {"id": 488, "seek": 235144, "start": 2366.64, "end": 2369.84, "text": " then now you're looking at coefficients that are going to be of like a", "tokens": [550, 586, 291, 434, 1237, 412, 31994, 300, 366, 516, 281, 312, 295, 411, 257], "temperature": 0.0, "avg_logprob": -0.21908792194567228, "compression_ratio": 1.7521008403361344, "no_speech_prob": 5.3381140787678305e-06}, {"id": 489, "seek": 235144, "start": 2370.84, "end": 2375.32, "text": " coefficient of some principal component from a PCA or a coefficient of some", "tokens": [17619, 295, 512, 9716, 6542, 490, 257, 6465, 32, 420, 257, 17619, 295, 512], "temperature": 0.0, "avg_logprob": -0.21908792194567228, "compression_ratio": 1.7521008403361344, "no_speech_prob": 5.3381140787678305e-06}, {"id": 490, "seek": 235144, "start": 2375.68, "end": 2380.7200000000003, "text": " Distance from some cluster or something at which point they're very very hard to interpret anyway", "tokens": [9840, 719, 490, 512, 13630, 420, 746, 412, 597, 935, 436, 434, 588, 588, 1152, 281, 7302, 4033], "temperature": 0.0, "avg_logprob": -0.21908792194567228, "compression_ratio": 1.7521008403361344, "no_speech_prob": 5.3381140787678305e-06}, {"id": 491, "seek": 238072, "start": 2380.72, "end": 2384.2799999999997, "text": " They're not actual variables right so they're kind of the two options", "tokens": [814, 434, 406, 3539, 9102, 558, 370, 436, 434, 733, 295, 264, 732, 3956], "temperature": 0.0, "avg_logprob": -0.18380841754731678, "compression_ratio": 1.6199095022624435, "no_speech_prob": 1.6797237094579032e-06}, {"id": 492, "seek": 238072, "start": 2384.2799999999997, "end": 2391.0, "text": " I've seen when people try to use classic statistical techniques to do or cover a variable importance", "tokens": [286, 600, 1612, 562, 561, 853, 281, 764, 7230, 22820, 7512, 281, 360, 420, 2060, 257, 7006, 7379], "temperature": 0.0, "avg_logprob": -0.18380841754731678, "compression_ratio": 1.6199095022624435, "no_speech_prob": 1.6797237094579032e-06}, {"id": 493, "seek": 238072, "start": 2391.3599999999997, "end": 2393.3599999999997, "text": " equivalent", "tokens": [10344], "temperature": 0.0, "avg_logprob": -0.18380841754731678, "compression_ratio": 1.6199095022624435, "no_speech_prob": 1.6797237094579032e-06}, {"id": 494, "seek": 238072, "start": 2394.08, "end": 2395.48, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.18380841754731678, "compression_ratio": 1.6199095022624435, "no_speech_prob": 1.6797237094579032e-06}, {"id": 495, "seek": 238072, "start": 2395.48, "end": 2397.48, "text": " Think things are starting to change", "tokens": [6557, 721, 366, 2891, 281, 1319], "temperature": 0.0, "avg_logprob": -0.18380841754731678, "compression_ratio": 1.6199095022624435, "no_speech_prob": 1.6797237094579032e-06}, {"id": 496, "seek": 238072, "start": 2398.6, "end": 2404.0, "text": " Slowly you know there are there are some fields that are starting to realize that this is totally the wrong way to do things", "tokens": [29674, 291, 458, 456, 366, 456, 366, 512, 7909, 300, 366, 2891, 281, 4325, 300, 341, 307, 3879, 264, 2085, 636, 281, 360, 721], "temperature": 0.0, "avg_logprob": -0.18380841754731678, "compression_ratio": 1.6199095022624435, "no_speech_prob": 1.6797237094579032e-06}, {"id": 497, "seek": 238072, "start": 2404.0, "end": 2405.7999999999997, "text": " But it's been", "tokens": [583, 309, 311, 668], "temperature": 0.0, "avg_logprob": -0.18380841754731678, "compression_ratio": 1.6199095022624435, "no_speech_prob": 1.6797237094579032e-06}, {"id": 498, "seek": 240580, "start": 2405.8, "end": 2413.04, "text": " You know nearly 20 years since random forests appeared, so it takes a long time. You know people say that the only way that", "tokens": [509, 458, 6217, 945, 924, 1670, 4974, 21700, 8516, 11, 370, 309, 2516, 257, 938, 565, 13, 509, 458, 561, 584, 300, 264, 787, 636, 300], "temperature": 0.0, "avg_logprob": -0.14710894132915295, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.565917151921894e-06}, {"id": 499, "seek": 240580, "start": 2413.88, "end": 2418.7200000000003, "text": " Knowledge really advances is when the previous generation dies, and that's kind of true right like", "tokens": [32906, 534, 25297, 307, 562, 264, 3894, 5125, 2714, 11, 293, 300, 311, 733, 295, 2074, 558, 411], "temperature": 0.0, "avg_logprob": -0.14710894132915295, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.565917151921894e-06}, {"id": 500, "seek": 240580, "start": 2419.32, "end": 2423.88, "text": " particularly academics, you know they make a career of being good at a particular sub thing and", "tokens": [4098, 25695, 11, 291, 458, 436, 652, 257, 3988, 295, 885, 665, 412, 257, 1729, 1422, 551, 293], "temperature": 0.0, "avg_logprob": -0.14710894132915295, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.565917151921894e-06}, {"id": 501, "seek": 240580, "start": 2425.3, "end": 2432.36, "text": " You know often don't it you know it's not until the next generation comes along that that people notice that oh", "tokens": [509, 458, 2049, 500, 380, 309, 291, 458, 309, 311, 406, 1826, 264, 958, 5125, 1487, 2051, 300, 300, 561, 3449, 300, 1954], "temperature": 0.0, "avg_logprob": -0.14710894132915295, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.565917151921894e-06}, {"id": 502, "seek": 243236, "start": 2432.36, "end": 2436.1200000000003, "text": " That's actually a longer a good way to do things, and I think that's what's happened here", "tokens": [663, 311, 767, 257, 2854, 257, 665, 636, 281, 360, 721, 11, 293, 286, 519, 300, 311, 437, 311, 2011, 510], "temperature": 0.0, "avg_logprob": -0.16368906696637472, "compression_ratio": 1.5690376569037656, "no_speech_prob": 1.3631126421387307e-05}, {"id": 503, "seek": 243236, "start": 2438.52, "end": 2439.6, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.16368906696637472, "compression_ratio": 1.5690376569037656, "no_speech_prob": 1.3631126421387307e-05}, {"id": 504, "seek": 243236, "start": 2439.6, "end": 2440.6800000000003, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.16368906696637472, "compression_ratio": 1.5690376569037656, "no_speech_prob": 1.3631126421387307e-05}, {"id": 505, "seek": 243236, "start": 2440.6800000000003, "end": 2445.36, "text": " We've got now a model which isn't really any better as a predictive accuracy wise", "tokens": [492, 600, 658, 586, 257, 2316, 597, 1943, 380, 534, 604, 1101, 382, 257, 35521, 14170, 10829], "temperature": 0.0, "avg_logprob": -0.16368906696637472, "compression_ratio": 1.5690376569037656, "no_speech_prob": 1.3631126421387307e-05}, {"id": 506, "seek": 243236, "start": 2445.7200000000003, "end": 2451.4, "text": " But it's kind of we're getting a good sense that there seems to be like four main important things", "tokens": [583, 309, 311, 733, 295, 321, 434, 1242, 257, 665, 2020, 300, 456, 2544, 281, 312, 411, 1451, 2135, 1021, 721], "temperature": 0.0, "avg_logprob": -0.16368906696637472, "compression_ratio": 1.5690376569037656, "no_speech_prob": 1.3631126421387307e-05}, {"id": 507, "seek": 243236, "start": 2452.08, "end": 2455.32, "text": " when it was made the capital system its size and", "tokens": [562, 309, 390, 1027, 264, 4238, 1185, 1080, 2744, 293], "temperature": 0.0, "avg_logprob": -0.16368906696637472, "compression_ratio": 1.5690376569037656, "no_speech_prob": 1.3631126421387307e-05}, {"id": 508, "seek": 243236, "start": 2456.0, "end": 2459.36, "text": " Its product classification okay, so that's cool", "tokens": [6953, 1674, 21538, 1392, 11, 370, 300, 311, 1627], "temperature": 0.0, "avg_logprob": -0.16368906696637472, "compression_ratio": 1.5690376569037656, "no_speech_prob": 1.3631126421387307e-05}, {"id": 509, "seek": 245936, "start": 2459.36, "end": 2461.2400000000002, "text": " There", "tokens": [821], "temperature": 0.0, "avg_logprob": -0.20363292694091797, "compression_ratio": 1.6484018264840183, "no_speech_prob": 2.332064013899071e-06}, {"id": 510, "seek": 245936, "start": 2461.2400000000002, "end": 2467.1200000000003, "text": " Is something else that we can do however, which is we can do something called one hot encoding?", "tokens": [1119, 746, 1646, 300, 321, 393, 360, 4461, 11, 597, 307, 321, 393, 360, 746, 1219, 472, 2368, 43430, 30], "temperature": 0.0, "avg_logprob": -0.20363292694091797, "compression_ratio": 1.6484018264840183, "no_speech_prob": 2.332064013899071e-06}, {"id": 511, "seek": 245936, "start": 2467.92, "end": 2474.88, "text": " So this is kind of where we're talking about categorical variables, so remember a categorical variable. Let's say we had like", "tokens": [407, 341, 307, 733, 295, 689, 321, 434, 1417, 466, 19250, 804, 9102, 11, 370, 1604, 257, 19250, 804, 7006, 13, 961, 311, 584, 321, 632, 411], "temperature": 0.0, "avg_logprob": -0.20363292694091797, "compression_ratio": 1.6484018264840183, "no_speech_prob": 2.332064013899071e-06}, {"id": 512, "seek": 245936, "start": 2476.6400000000003, "end": 2478.36, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.20363292694091797, "compression_ratio": 1.6484018264840183, "no_speech_prob": 2.332064013899071e-06}, {"id": 513, "seek": 245936, "start": 2478.36, "end": 2480.36, "text": " string high", "tokens": [6798, 1090], "temperature": 0.0, "avg_logprob": -0.20363292694091797, "compression_ratio": 1.6484018264840183, "no_speech_prob": 2.332064013899071e-06}, {"id": 514, "seek": 248036, "start": 2480.36, "end": 2489.36, "text": " And remember the order we got was kind of back weird it was high low medium, so it was in alphabetical order by default right?", "tokens": [400, 1604, 264, 1668, 321, 658, 390, 733, 295, 646, 3657, 309, 390, 1090, 2295, 6399, 11, 370, 309, 390, 294, 23339, 804, 1668, 538, 7576, 558, 30], "temperature": 0.0, "avg_logprob": -0.1893437024459098, "compression_ratio": 1.743801652892562, "no_speech_prob": 1.505699401604943e-06}, {"id": 515, "seek": 248036, "start": 2489.44, "end": 2495.02, "text": " Was our original category for like usage band or something and so we mapped it to zero", "tokens": [3027, 527, 3380, 7719, 337, 411, 14924, 4116, 420, 746, 293, 370, 321, 33318, 309, 281, 4018], "temperature": 0.0, "avg_logprob": -0.1893437024459098, "compression_ratio": 1.743801652892562, "no_speech_prob": 1.505699401604943e-06}, {"id": 516, "seek": 248036, "start": 2495.88, "end": 2497.1600000000003, "text": " one", "tokens": [472], "temperature": 0.0, "avg_logprob": -0.1893437024459098, "compression_ratio": 1.743801652892562, "no_speech_prob": 1.505699401604943e-06}, {"id": 517, "seek": 248036, "start": 2497.1600000000003, "end": 2501.76, "text": " Two right and so by the time it gets into our data frame. It's now a number", "tokens": [4453, 558, 293, 370, 538, 264, 565, 309, 2170, 666, 527, 1412, 3920, 13, 467, 311, 586, 257, 1230], "temperature": 0.0, "avg_logprob": -0.1893437024459098, "compression_ratio": 1.743801652892562, "no_speech_prob": 1.505699401604943e-06}, {"id": 518, "seek": 250176, "start": 2501.76, "end": 2510.2400000000002, "text": " So the random forest doesn't know that it was originally a category. It's just a number right so when the random forest is built", "tokens": [407, 264, 4974, 6719, 1177, 380, 458, 300, 309, 390, 7993, 257, 7719, 13, 467, 311, 445, 257, 1230, 558, 370, 562, 264, 4974, 6719, 307, 3094], "temperature": 0.0, "avg_logprob": -0.15199960636187204, "compression_ratio": 1.675, "no_speech_prob": 7.112425919331145e-07}, {"id": 519, "seek": 250176, "start": 2510.2400000000002, "end": 2512.2400000000002, "text": " It basically says oh is it", "tokens": [467, 1936, 1619, 1954, 307, 309], "temperature": 0.0, "avg_logprob": -0.15199960636187204, "compression_ratio": 1.675, "no_speech_prob": 7.112425919331145e-07}, {"id": 520, "seek": 250176, "start": 2512.88, "end": 2519.6000000000004, "text": " Greater than one or not or is it greater than naught or not you know basically the two possible decisions it could have made", "tokens": [38410, 813, 472, 420, 406, 420, 307, 309, 5044, 813, 13138, 420, 406, 291, 458, 1936, 264, 732, 1944, 5327, 309, 727, 362, 1027], "temperature": 0.0, "avg_logprob": -0.15199960636187204, "compression_ratio": 1.675, "no_speech_prob": 7.112425919331145e-07}, {"id": 521, "seek": 250176, "start": 2523.84, "end": 2525.84, "text": " For", "tokens": [1171], "temperature": 0.0, "avg_logprob": -0.15199960636187204, "compression_ratio": 1.675, "no_speech_prob": 7.112425919331145e-07}, {"id": 522, "seek": 252584, "start": 2525.84, "end": 2531.1600000000003, "text": " For something with like five or six bands you know it", "tokens": [1171, 746, 365, 411, 1732, 420, 2309, 13543, 291, 458, 309], "temperature": 0.0, "avg_logprob": -0.25430395796492294, "compression_ratio": 1.6129032258064515, "no_speech_prob": 2.406095063633984e-06}, {"id": 523, "seek": 252584, "start": 2531.8, "end": 2538.36, "text": " Could be that just one of the levels of a category is actually interesting right so like if it was like very high", "tokens": [7497, 312, 300, 445, 472, 295, 264, 4358, 295, 257, 7719, 307, 767, 1880, 558, 370, 411, 498, 309, 390, 411, 588, 1090], "temperature": 0.0, "avg_logprob": -0.25430395796492294, "compression_ratio": 1.6129032258064515, "no_speech_prob": 2.406095063633984e-06}, {"id": 524, "seek": 252584, "start": 2539.88, "end": 2541.88, "text": " very low", "tokens": [588, 2295], "temperature": 0.0, "avg_logprob": -0.25430395796492294, "compression_ratio": 1.6129032258064515, "no_speech_prob": 2.406095063633984e-06}, {"id": 525, "seek": 252584, "start": 2542.56, "end": 2544.56, "text": " Or or unknown", "tokens": [1610, 420, 9841], "temperature": 0.0, "avg_logprob": -0.25430395796492294, "compression_ratio": 1.6129032258064515, "no_speech_prob": 2.406095063633984e-06}, {"id": 526, "seek": 252584, "start": 2545.6000000000004, "end": 2548.6800000000003, "text": " Right then we've never like six levels and maybe", "tokens": [1779, 550, 321, 600, 1128, 411, 2309, 4358, 293, 1310], "temperature": 0.0, "avg_logprob": -0.25430395796492294, "compression_ratio": 1.6129032258064515, "no_speech_prob": 2.406095063633984e-06}, {"id": 527, "seek": 252584, "start": 2549.2400000000002, "end": 2551.7200000000003, "text": " The only thing that mattered was whether it was like unknown", "tokens": [440, 787, 551, 300, 44282, 390, 1968, 309, 390, 411, 9841], "temperature": 0.0, "avg_logprob": -0.25430395796492294, "compression_ratio": 1.6129032258064515, "no_speech_prob": 2.406095063633984e-06}, {"id": 528, "seek": 255172, "start": 2551.72, "end": 2558.3199999999997, "text": " Maybe like not knowing its size somehow impacts the price and so if we wanted to be able to recognize that and", "tokens": [2704, 411, 406, 5276, 1080, 2744, 6063, 11606, 264, 3218, 293, 370, 498, 321, 1415, 281, 312, 1075, 281, 5521, 300, 293], "temperature": 0.0, "avg_logprob": -0.15230035781860352, "compression_ratio": 1.669172932330827, "no_speech_prob": 1.4367469702847302e-06}, {"id": 529, "seek": 255172, "start": 2558.56, "end": 2565.16, "text": " Particularly if like it just so happened that the way that the numbers were coded was it unknown ended up in the middle", "tokens": [32281, 498, 411, 309, 445, 370, 2011, 300, 264, 636, 300, 264, 3547, 645, 34874, 390, 309, 9841, 4590, 493, 294, 264, 2808], "temperature": 0.0, "avg_logprob": -0.15230035781860352, "compression_ratio": 1.669172932330827, "no_speech_prob": 1.4367469702847302e-06}, {"id": 530, "seek": 255172, "start": 2568.0, "end": 2570.8399999999997, "text": " Right then what it's going to do is it's going to say okay?", "tokens": [1779, 550, 437, 309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 584, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15230035781860352, "compression_ratio": 1.669172932330827, "no_speech_prob": 1.4367469702847302e-06}, {"id": 531, "seek": 255172, "start": 2570.8399999999997, "end": 2576.08, "text": " There is a difference between these two groups. You know less than or equal to two versus greater than two", "tokens": [821, 307, 257, 2649, 1296, 613, 732, 3935, 13, 509, 458, 1570, 813, 420, 2681, 281, 732, 5717, 5044, 813, 732], "temperature": 0.0, "avg_logprob": -0.15230035781860352, "compression_ratio": 1.669172932330827, "no_speech_prob": 1.4367469702847302e-06}, {"id": 532, "seek": 255172, "start": 2576.72, "end": 2579.64, "text": " And then when it gets into this this leaf here", "tokens": [400, 550, 562, 309, 2170, 666, 341, 341, 10871, 510], "temperature": 0.0, "avg_logprob": -0.15230035781860352, "compression_ratio": 1.669172932330827, "no_speech_prob": 1.4367469702847302e-06}, {"id": 533, "seek": 257964, "start": 2579.64, "end": 2586.2799999999997, "text": " It's going to say oh there's a difference between these two between less than four and greater than or equal to four and it's going to", "tokens": [467, 311, 516, 281, 584, 1954, 456, 311, 257, 2649, 1296, 613, 732, 1296, 1570, 813, 1451, 293, 5044, 813, 420, 2681, 281, 1451, 293, 309, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.16557783346909744, "compression_ratio": 1.8167330677290836, "no_speech_prob": 8.186352715711109e-07}, {"id": 534, "seek": 257964, "start": 2586.2799999999997, "end": 2592.4, "text": " Take two splits to get to the point where we can see that it's actually unknown that matters", "tokens": [3664, 732, 37741, 281, 483, 281, 264, 935, 689, 321, 393, 536, 300, 309, 311, 767, 9841, 300, 7001], "temperature": 0.0, "avg_logprob": -0.16557783346909744, "compression_ratio": 1.8167330677290836, "no_speech_prob": 8.186352715711109e-07}, {"id": 535, "seek": 257964, "start": 2593.2799999999997, "end": 2595.2799999999997, "text": " So this is a little inefficient", "tokens": [407, 341, 307, 257, 707, 43495], "temperature": 0.0, "avg_logprob": -0.16557783346909744, "compression_ratio": 1.8167330677290836, "no_speech_prob": 8.186352715711109e-07}, {"id": 536, "seek": 257964, "start": 2595.2799999999997, "end": 2601.56, "text": " And we're kind of like wasting tree computation and like wasting tree computation matters because every time we do a split", "tokens": [400, 321, 434, 733, 295, 411, 20457, 4230, 24903, 293, 411, 20457, 4230, 24903, 7001, 570, 633, 565, 321, 360, 257, 7472], "temperature": 0.0, "avg_logprob": -0.16557783346909744, "compression_ratio": 1.8167330677290836, "no_speech_prob": 8.186352715711109e-07}, {"id": 537, "seek": 257964, "start": 2601.64, "end": 2605.96, "text": " We're having the amount of data at least that we have to do more analysis", "tokens": [492, 434, 1419, 264, 2372, 295, 1412, 412, 1935, 300, 321, 362, 281, 360, 544, 5215], "temperature": 0.0, "avg_logprob": -0.16557783346909744, "compression_ratio": 1.8167330677290836, "no_speech_prob": 8.186352715711109e-07}, {"id": 538, "seek": 260596, "start": 2605.96, "end": 2613.1, "text": " So it's going to make our tree less rich less effective if we're not giving the data in a way", "tokens": [407, 309, 311, 516, 281, 652, 527, 4230, 1570, 4593, 1570, 4942, 498, 321, 434, 406, 2902, 264, 1412, 294, 257, 636], "temperature": 0.0, "avg_logprob": -0.21218385451879257, "compression_ratio": 1.611764705882353, "no_speech_prob": 1.0845155884453561e-06}, {"id": 539, "seek": 260596, "start": 2613.1, "end": 2616.32, "text": " That's kind of convenient for it to do the work it needs to do", "tokens": [663, 311, 733, 295, 10851, 337, 309, 281, 360, 264, 589, 309, 2203, 281, 360], "temperature": 0.0, "avg_logprob": -0.21218385451879257, "compression_ratio": 1.611764705882353, "no_speech_prob": 1.0845155884453561e-06}, {"id": 540, "seek": 260596, "start": 2617.7200000000003, "end": 2619.8, "text": " so what we could do instead is", "tokens": [370, 437, 321, 727, 360, 2602, 307], "temperature": 0.0, "avg_logprob": -0.21218385451879257, "compression_ratio": 1.611764705882353, "no_speech_prob": 1.0845155884453561e-06}, {"id": 541, "seek": 260596, "start": 2622.28, "end": 2623.64, "text": " Create", "tokens": [20248], "temperature": 0.0, "avg_logprob": -0.21218385451879257, "compression_ratio": 1.611764705882353, "no_speech_prob": 1.0845155884453561e-06}, {"id": 542, "seek": 260596, "start": 2623.64, "end": 2625.64, "text": " six columns", "tokens": [2309, 13766], "temperature": 0.0, "avg_logprob": -0.21218385451879257, "compression_ratio": 1.611764705882353, "no_speech_prob": 1.0845155884453561e-06}, {"id": 543, "seek": 260596, "start": 2625.76, "end": 2628.48, "text": " We could create a column called is very high is", "tokens": [492, 727, 1884, 257, 7738, 1219, 307, 588, 1090, 307], "temperature": 0.0, "avg_logprob": -0.21218385451879257, "compression_ratio": 1.611764705882353, "no_speech_prob": 1.0845155884453561e-06}, {"id": 544, "seek": 260596, "start": 2629.2400000000002, "end": 2631.76, "text": " very low is high is", "tokens": [588, 2295, 307, 1090, 307], "temperature": 0.0, "avg_logprob": -0.21218385451879257, "compression_ratio": 1.611764705882353, "no_speech_prob": 1.0845155884453561e-06}, {"id": 545, "seek": 263176, "start": 2631.76, "end": 2639.82, "text": " Is unknown is low is medium and each one would be ones and zeros right so either one or zero", "tokens": [1119, 9841, 307, 2295, 307, 6399, 293, 1184, 472, 576, 312, 2306, 293, 35193, 558, 370, 2139, 472, 420, 4018], "temperature": 0.0, "avg_logprob": -0.21782859490842235, "compression_ratio": 1.4926470588235294, "no_speech_prob": 3.989709966845112e-07}, {"id": 546, "seek": 263176, "start": 2643.96, "end": 2646.76, "text": " So we had six columns just one moment", "tokens": [407, 321, 632, 2309, 13766, 445, 472, 1623], "temperature": 0.0, "avg_logprob": -0.21782859490842235, "compression_ratio": 1.4926470588235294, "no_speech_prob": 3.989709966845112e-07}, {"id": 547, "seek": 263176, "start": 2648.96, "end": 2655.0800000000004, "text": " So having added six additional columns to our data set the random forest", "tokens": [407, 1419, 3869, 2309, 4497, 13766, 281, 527, 1412, 992, 264, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.21782859490842235, "compression_ratio": 1.4926470588235294, "no_speech_prob": 3.989709966845112e-07}, {"id": 548, "seek": 265508, "start": 2655.08, "end": 2661.7599999999998, "text": " Now has the ability to pick one of these and say like oh, let's have a look at is unknown", "tokens": [823, 575, 264, 3485, 281, 1888, 472, 295, 613, 293, 584, 411, 1954, 11, 718, 311, 362, 257, 574, 412, 307, 9841], "temperature": 0.0, "avg_logprob": -0.18208214332317485, "compression_ratio": 1.6231884057971016, "no_speech_prob": 1.5623942317688488e-07}, {"id": 549, "seek": 265508, "start": 2662.04, "end": 2665.0, "text": " There's one possible split I can do which is one versus zero", "tokens": [821, 311, 472, 1944, 7472, 286, 393, 360, 597, 307, 472, 5717, 4018], "temperature": 0.0, "avg_logprob": -0.18208214332317485, "compression_ratio": 1.6231884057971016, "no_speech_prob": 1.5623942317688488e-07}, {"id": 550, "seek": 265508, "start": 2665.16, "end": 2671.3199999999997, "text": " Let's see if that's any good right so it actually now has the ability in a single step to pull out a single", "tokens": [961, 311, 536, 498, 300, 311, 604, 665, 558, 370, 309, 767, 586, 575, 264, 3485, 294, 257, 2167, 1823, 281, 2235, 484, 257, 2167], "temperature": 0.0, "avg_logprob": -0.18208214332317485, "compression_ratio": 1.6231884057971016, "no_speech_prob": 1.5623942317688488e-07}, {"id": 551, "seek": 265508, "start": 2671.6, "end": 2673.6, "text": " category level and so", "tokens": [7719, 1496, 293, 370], "temperature": 0.0, "avg_logprob": -0.18208214332317485, "compression_ratio": 1.6231884057971016, "no_speech_prob": 1.5623942317688488e-07}, {"id": 552, "seek": 265508, "start": 2674.7999999999997, "end": 2680.96, "text": " This this kind of coding is called one hot encoding and", "tokens": [639, 341, 733, 295, 17720, 307, 1219, 472, 2368, 43430, 293], "temperature": 0.0, "avg_logprob": -0.18208214332317485, "compression_ratio": 1.6231884057971016, "no_speech_prob": 1.5623942317688488e-07}, {"id": 553, "seek": 268096, "start": 2680.96, "end": 2686.84, "text": " for many many types of machine learning model this is like", "tokens": [337, 867, 867, 3467, 295, 3479, 2539, 2316, 341, 307, 411], "temperature": 0.0, "avg_logprob": -0.1877495485193589, "compression_ratio": 1.7352941176470589, "no_speech_prob": 8.579222026128264e-07}, {"id": 554, "seek": 268096, "start": 2688.48, "end": 2692.44, "text": " Necessary something like this is necessary like if you're doing logistic regression", "tokens": [1734, 780, 822, 746, 411, 341, 307, 4818, 411, 498, 291, 434, 884, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.1877495485193589, "compression_ratio": 1.7352941176470589, "no_speech_prob": 8.579222026128264e-07}, {"id": 555, "seek": 268096, "start": 2692.6, "end": 2700.84, "text": " You can't possibly put in a categorical variable that goes not through five because there's obviously no really linear relationship between that and anything right", "tokens": [509, 393, 380, 6264, 829, 294, 257, 19250, 804, 7006, 300, 1709, 406, 807, 1732, 570, 456, 311, 2745, 572, 534, 8213, 2480, 1296, 300, 293, 1340, 558], "temperature": 0.0, "avg_logprob": -0.1877495485193589, "compression_ratio": 1.7352941176470589, "no_speech_prob": 8.579222026128264e-07}, {"id": 556, "seek": 268096, "start": 2701.4, "end": 2703.4, "text": " so one hot encoding a", "tokens": [370, 472, 2368, 43430, 257], "temperature": 0.0, "avg_logprob": -0.1877495485193589, "compression_ratio": 1.7352941176470589, "no_speech_prob": 8.579222026128264e-07}, {"id": 557, "seek": 268096, "start": 2703.8, "end": 2708.56, "text": " Lot of people incorrectly assume that all machine learning requires one hot encoding", "tokens": [20131, 295, 561, 42892, 6552, 300, 439, 3479, 2539, 7029, 472, 2368, 43430], "temperature": 0.0, "avg_logprob": -0.1877495485193589, "compression_ratio": 1.7352941176470589, "no_speech_prob": 8.579222026128264e-07}, {"id": 558, "seek": 270856, "start": 2708.56, "end": 2716.04, "text": " But in this case, I'm going to show you how we could use it optionally and see whether it might improve things sometimes yeah", "tokens": [583, 294, 341, 1389, 11, 286, 478, 516, 281, 855, 291, 577, 321, 727, 764, 309, 3614, 379, 293, 536, 1968, 309, 1062, 3470, 721, 2171, 1338], "temperature": 0.0, "avg_logprob": -0.2213440719915896, "compression_ratio": 1.6527196652719665, "no_speech_prob": 8.397969395446125e-06}, {"id": 559, "seek": 270856, "start": 2716.56, "end": 2724.56, "text": " Hi Jeremy, so if we have six categories like in this case would there be any problems with adding a column for each of the?", "tokens": [2421, 17809, 11, 370, 498, 321, 362, 2309, 10479, 411, 294, 341, 1389, 576, 456, 312, 604, 2740, 365, 5127, 257, 7738, 337, 1184, 295, 264, 30], "temperature": 0.0, "avg_logprob": -0.2213440719915896, "compression_ratio": 1.6527196652719665, "no_speech_prob": 8.397969395446125e-06}, {"id": 560, "seek": 270856, "start": 2725.32, "end": 2730.24, "text": " Categories so because in linear regression we said we had to do it like if there's six categories", "tokens": [383, 2968, 2083, 370, 570, 294, 8213, 24590, 321, 848, 321, 632, 281, 360, 309, 411, 498, 456, 311, 2309, 10479], "temperature": 0.0, "avg_logprob": -0.2213440719915896, "compression_ratio": 1.6527196652719665, "no_speech_prob": 8.397969395446125e-06}, {"id": 561, "seek": 270856, "start": 2730.24, "end": 2732.64, "text": " We should only do it for five of them. Yeah, so", "tokens": [492, 820, 787, 360, 309, 337, 1732, 295, 552, 13, 865, 11, 370], "temperature": 0.0, "avg_logprob": -0.2213440719915896, "compression_ratio": 1.6527196652719665, "no_speech_prob": 8.397969395446125e-06}, {"id": 562, "seek": 273264, "start": 2732.64, "end": 2737.16, "text": " Um it you certainly can say oh wait", "tokens": [3301, 309, 291, 3297, 393, 584, 1954, 1699], "temperature": 0.0, "avg_logprob": -0.2244667277616613, "compression_ratio": 1.6157635467980296, "no_speech_prob": 1.067700509338465e-06}, {"id": 563, "seek": 273264, "start": 2737.16, "end": 2742.64, "text": " Let's not worry about adding is medium because we can infer it from the other five", "tokens": [961, 311, 406, 3292, 466, 5127, 307, 6399, 570, 321, 393, 13596, 309, 490, 264, 661, 1732], "temperature": 0.0, "avg_logprob": -0.2244667277616613, "compression_ratio": 1.6157635467980296, "no_speech_prob": 1.067700509338465e-06}, {"id": 564, "seek": 273264, "start": 2745.2, "end": 2747.2, "text": " I would say include it anyway", "tokens": [286, 576, 584, 4090, 309, 4033], "temperature": 0.0, "avg_logprob": -0.2244667277616613, "compression_ratio": 1.6157635467980296, "no_speech_prob": 1.067700509338465e-06}, {"id": 565, "seek": 273264, "start": 2748.4, "end": 2749.96, "text": " because like", "tokens": [570, 411], "temperature": 0.0, "avg_logprob": -0.2244667277616613, "compression_ratio": 1.6157635467980296, "no_speech_prob": 1.067700509338465e-06}, {"id": 566, "seek": 273264, "start": 2749.96, "end": 2753.24, "text": " rather than the otherwise the random forest would have to say is", "tokens": [2831, 813, 264, 5911, 264, 4974, 6719, 576, 362, 281, 584, 307], "temperature": 0.0, "avg_logprob": -0.2244667277616613, "compression_ratio": 1.6157635467980296, "no_speech_prob": 1.067700509338465e-06}, {"id": 567, "seek": 273264, "start": 2754.12, "end": 2760.56, "text": " Very high no is very low no is high no is unknown low is low no okay, and finally on on there, right?", "tokens": [4372, 1090, 572, 307, 588, 2295, 572, 307, 1090, 572, 307, 9841, 2295, 307, 2295, 572, 1392, 11, 293, 2721, 322, 322, 456, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2244667277616613, "compression_ratio": 1.6157635467980296, "no_speech_prob": 1.067700509338465e-06}, {"id": 568, "seek": 276056, "start": 2760.56, "end": 2763.72, "text": " So it's like five decisions to get to that point so", "tokens": [407, 309, 311, 411, 1732, 5327, 281, 483, 281, 300, 935, 370], "temperature": 0.0, "avg_logprob": -0.17461559507581922, "compression_ratio": 1.5847953216374269, "no_speech_prob": 1.5294085642381106e-06}, {"id": 569, "seek": 276056, "start": 2764.48, "end": 2766.48, "text": " the reason in", "tokens": [264, 1778, 294], "temperature": 0.0, "avg_logprob": -0.17461559507581922, "compression_ratio": 1.5847953216374269, "no_speech_prob": 1.5294085642381106e-06}, {"id": 570, "seek": 276056, "start": 2767.7999999999997, "end": 2774.2, "text": " Linear models that you need to not include one is because linear models hate collinearity", "tokens": [14670, 289, 5245, 300, 291, 643, 281, 406, 4090, 472, 307, 570, 8213, 5245, 4700, 1263, 533, 17409], "temperature": 0.0, "avg_logprob": -0.17461559507581922, "compression_ratio": 1.5847953216374269, "no_speech_prob": 1.5294085642381106e-06}, {"id": 571, "seek": 276056, "start": 2774.7599999999998, "end": 2777.06, "text": " But we don't care about about that here", "tokens": [583, 321, 500, 380, 1127, 466, 466, 300, 510], "temperature": 0.0, "avg_logprob": -0.17461559507581922, "compression_ratio": 1.5847953216374269, "no_speech_prob": 1.5294085642381106e-06}, {"id": 572, "seek": 276056, "start": 2779.64, "end": 2785.72, "text": " So we can do one hot encoding easily enough and the way we do it is we pass", "tokens": [407, 321, 393, 360, 472, 2368, 43430, 3612, 1547, 293, 264, 636, 321, 360, 309, 307, 321, 1320], "temperature": 0.0, "avg_logprob": -0.17461559507581922, "compression_ratio": 1.5847953216374269, "no_speech_prob": 1.5294085642381106e-06}, {"id": 573, "seek": 278572, "start": 2785.72, "end": 2791.9599999999996, "text": " One extra parameter to proc df which is what's the max?", "tokens": [1485, 2857, 13075, 281, 9510, 274, 69, 597, 307, 437, 311, 264, 11469, 30], "temperature": 0.0, "avg_logprob": -0.251972319204596, "compression_ratio": 1.5454545454545454, "no_speech_prob": 3.576352867185051e-07}, {"id": 574, "seek": 278572, "start": 2793.7599999999998, "end": 2795.48, "text": " Number of", "tokens": [5118, 295], "temperature": 0.0, "avg_logprob": -0.251972319204596, "compression_ratio": 1.5454545454545454, "no_speech_prob": 3.576352867185051e-07}, {"id": 575, "seek": 278572, "start": 2795.48, "end": 2802.7999999999997, "text": " Categories right so if we say it's seven then anything with less than seven levels", "tokens": [383, 2968, 2083, 558, 370, 498, 321, 584, 309, 311, 3407, 550, 1340, 365, 1570, 813, 3407, 4358], "temperature": 0.0, "avg_logprob": -0.251972319204596, "compression_ratio": 1.5454545454545454, "no_speech_prob": 3.576352867185051e-07}, {"id": 576, "seek": 278572, "start": 2803.3199999999997, "end": 2809.16, "text": " Is going to be turned into one hot encoded bunch of columns right so in this case?", "tokens": [1119, 516, 281, 312, 3574, 666, 472, 2368, 2058, 12340, 3840, 295, 13766, 558, 370, 294, 341, 1389, 30], "temperature": 0.0, "avg_logprob": -0.251972319204596, "compression_ratio": 1.5454545454545454, "no_speech_prob": 3.576352867185051e-07}, {"id": 577, "seek": 278572, "start": 2809.16, "end": 2812.3599999999997, "text": " This has got six levels, so this would be one hot encoded", "tokens": [639, 575, 658, 2309, 4358, 11, 370, 341, 576, 312, 472, 2368, 2058, 12340], "temperature": 0.0, "avg_logprob": -0.251972319204596, "compression_ratio": 1.5454545454545454, "no_speech_prob": 3.576352867185051e-07}, {"id": 578, "seek": 281236, "start": 2812.36, "end": 2819.6400000000003, "text": " Where else like zip code has more than six levels and so that would be left as a number and so generally speaking you obviously", "tokens": [2305, 1646, 411, 20730, 3089, 575, 544, 813, 2309, 4358, 293, 370, 300, 576, 312, 1411, 382, 257, 1230, 293, 370, 5101, 4124, 291, 2745], "temperature": 0.0, "avg_logprob": -0.16513393038795107, "compression_ratio": 1.617391304347826, "no_speech_prob": 4.812511633645045e-07}, {"id": 579, "seek": 281236, "start": 2820.4, "end": 2827.08, "text": " Probably wouldn't want a one hot encode zip code right because that's just going to create masses of data memory problems", "tokens": [9210, 2759, 380, 528, 257, 472, 2368, 2058, 1429, 20730, 3089, 558, 570, 300, 311, 445, 516, 281, 1884, 23935, 295, 1412, 4675, 2740], "temperature": 0.0, "avg_logprob": -0.16513393038795107, "compression_ratio": 1.617391304347826, "no_speech_prob": 4.812511633645045e-07}, {"id": 580, "seek": 281236, "start": 2827.28, "end": 2833.98, "text": " Computation problems and so forth right so so this is like another parameter that you can play around with so", "tokens": [37804, 399, 2740, 293, 370, 5220, 558, 370, 370, 341, 307, 411, 1071, 13075, 300, 291, 393, 862, 926, 365, 370], "temperature": 0.0, "avg_logprob": -0.16513393038795107, "compression_ratio": 1.617391304347826, "no_speech_prob": 4.812511633645045e-07}, {"id": 581, "seek": 281236, "start": 2834.6800000000003, "end": 2836.6800000000003, "text": " If I do that", "tokens": [759, 286, 360, 300], "temperature": 0.0, "avg_logprob": -0.16513393038795107, "compression_ratio": 1.617391304347826, "no_speech_prob": 4.812511633645045e-07}, {"id": 582, "seek": 283668, "start": 2836.68, "end": 2842.64, "text": " Try it out run the random forest as per usual you can see what happens to the", "tokens": [6526, 309, 484, 1190, 264, 4974, 6719, 382, 680, 7713, 291, 393, 536, 437, 2314, 281, 264], "temperature": 0.0, "avg_logprob": -0.15522451902690687, "compression_ratio": 1.6774193548387097, "no_speech_prob": 6.048871455277549e-06}, {"id": 583, "seek": 283668, "start": 2843.9199999999996, "end": 2849.3199999999997, "text": " R-squared of the validation set and to the RMSE of the validation set and in this case", "tokens": [497, 12, 33292, 1642, 295, 264, 24071, 992, 293, 281, 264, 23790, 5879, 295, 264, 24071, 992, 293, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.15522451902690687, "compression_ratio": 1.6774193548387097, "no_speech_prob": 6.048871455277549e-06}, {"id": 584, "seek": 283668, "start": 2849.3199999999997, "end": 2851.3199999999997, "text": " I found it got a little bit worse", "tokens": [286, 1352, 309, 658, 257, 707, 857, 5324], "temperature": 0.0, "avg_logprob": -0.15522451902690687, "compression_ratio": 1.6774193548387097, "no_speech_prob": 6.048871455277549e-06}, {"id": 585, "seek": 283668, "start": 2852.8799999999997, "end": 2856.04, "text": " This isn't always the case, and it's going to depend on your data set", "tokens": [639, 1943, 380, 1009, 264, 1389, 11, 293, 309, 311, 516, 281, 5672, 322, 428, 1412, 992], "temperature": 0.0, "avg_logprob": -0.15522451902690687, "compression_ratio": 1.6774193548387097, "no_speech_prob": 6.048871455277549e-06}, {"id": 586, "seek": 283668, "start": 2856.04, "end": 2861.3999999999996, "text": " You know it do you have a data set where you know single categories tend to be quite important?", "tokens": [509, 458, 309, 360, 291, 362, 257, 1412, 992, 689, 291, 458, 2167, 10479, 3928, 281, 312, 1596, 1021, 30], "temperature": 0.0, "avg_logprob": -0.15522451902690687, "compression_ratio": 1.6774193548387097, "no_speech_prob": 6.048871455277549e-06}, {"id": 587, "seek": 286140, "start": 2861.4, "end": 2867.78, "text": " Or not in this particular case it didn't make it more predictive however", "tokens": [1610, 406, 294, 341, 1729, 1389, 309, 994, 380, 652, 309, 544, 35521, 4461], "temperature": 0.0, "avg_logprob": -0.22372602161608243, "compression_ratio": 1.5951219512195123, "no_speech_prob": 2.1568057491094805e-06}, {"id": 588, "seek": 286140, "start": 2868.6800000000003, "end": 2874.2000000000003, "text": " What it did do is that we now have different features right so the proxy F", "tokens": [708, 309, 630, 360, 307, 300, 321, 586, 362, 819, 4122, 558, 370, 264, 29690, 479], "temperature": 0.0, "avg_logprob": -0.22372602161608243, "compression_ratio": 1.5951219512195123, "no_speech_prob": 2.1568057491094805e-06}, {"id": 589, "seek": 286140, "start": 2874.2000000000003, "end": 2878.2400000000002, "text": " puts the name of the variable and then an underscore and then the level name and", "tokens": [8137, 264, 1315, 295, 264, 7006, 293, 550, 364, 37556, 293, 550, 264, 1496, 1315, 293], "temperature": 0.0, "avg_logprob": -0.22372602161608243, "compression_ratio": 1.5951219512195123, "no_speech_prob": 2.1568057491094805e-06}, {"id": 590, "seek": 286140, "start": 2878.88, "end": 2884.28, "text": " So interestingly it turns out that where else before it said that", "tokens": [407, 25873, 309, 4523, 484, 300, 689, 1646, 949, 309, 848, 300], "temperature": 0.0, "avg_logprob": -0.22372602161608243, "compression_ratio": 1.5951219512195123, "no_speech_prob": 2.1568057491094805e-06}, {"id": 591, "seek": 286140, "start": 2884.88, "end": 2887.28, "text": " Enclosure was somewhat important", "tokens": [2193, 3474, 7641, 390, 8344, 1021], "temperature": 0.0, "avg_logprob": -0.22372602161608243, "compression_ratio": 1.5951219512195123, "no_speech_prob": 2.1568057491094805e-06}, {"id": 592, "seek": 288728, "start": 2887.28, "end": 2892.4, "text": " When we do it as one hot encoded it actually says enclosure", "tokens": [1133, 321, 360, 309, 382, 472, 2368, 2058, 12340, 309, 767, 1619, 34093], "temperature": 0.0, "avg_logprob": -0.2550975840578797, "compression_ratio": 1.5603448275862069, "no_speech_prob": 1.7061796597772627e-06}, {"id": 593, "seek": 288728, "start": 2892.8, "end": 2895.88, "text": " E ROTS with a C is the most important thing", "tokens": [462, 497, 5068, 50, 365, 257, 383, 307, 264, 881, 1021, 551], "temperature": 0.0, "avg_logprob": -0.2550975840578797, "compression_ratio": 1.5603448275862069, "no_speech_prob": 1.7061796597772627e-06}, {"id": 594, "seek": 288728, "start": 2896.8, "end": 2897.96, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2550975840578797, "compression_ratio": 1.5603448275862069, "no_speech_prob": 1.7061796597772627e-06}, {"id": 595, "seek": 288728, "start": 2897.96, "end": 2902.96, "text": " For at least the purpose of like interpreting your model you should always try", "tokens": [1171, 412, 1935, 264, 4334, 295, 411, 37395, 428, 2316, 291, 820, 1009, 853], "temperature": 0.0, "avg_logprob": -0.2550975840578797, "compression_ratio": 1.5603448275862069, "no_speech_prob": 1.7061796597772627e-06}, {"id": 596, "seek": 288728, "start": 2903.5600000000004, "end": 2905.84, "text": " one hot encoding you know", "tokens": [472, 2368, 43430, 291, 458], "temperature": 0.0, "avg_logprob": -0.2550975840578797, "compression_ratio": 1.5603448275862069, "no_speech_prob": 1.7061796597772627e-06}, {"id": 597, "seek": 288728, "start": 2906.6400000000003, "end": 2911.2400000000002, "text": " Quite a few of your variables, and so I often find somewhere around six or seven is pretty good", "tokens": [20464, 257, 1326, 295, 428, 9102, 11, 293, 370, 286, 2049, 915, 4079, 926, 2309, 420, 3407, 307, 1238, 665], "temperature": 0.0, "avg_logprob": -0.2550975840578797, "compression_ratio": 1.5603448275862069, "no_speech_prob": 1.7061796597772627e-06}, {"id": 598, "seek": 288728, "start": 2911.88, "end": 2915.84, "text": " You can try like making that number as high as you can", "tokens": [509, 393, 853, 411, 1455, 300, 1230, 382, 1090, 382, 291, 393], "temperature": 0.0, "avg_logprob": -0.2550975840578797, "compression_ratio": 1.5603448275862069, "no_speech_prob": 1.7061796597772627e-06}, {"id": 599, "seek": 291584, "start": 2915.84, "end": 2922.08, "text": " So that it doesn't take forever to compute and the feature importance doesn't include like", "tokens": [407, 300, 309, 1177, 380, 747, 5680, 281, 14722, 293, 264, 4111, 7379, 1177, 380, 4090, 411], "temperature": 0.0, "avg_logprob": -0.20600696362947163, "compression_ratio": 1.6554621848739495, "no_speech_prob": 5.173886165721342e-06}, {"id": 600, "seek": 291584, "start": 2923.52, "end": 2928.08, "text": " Really tiny levels that aren't interesting so that's kind of up to you to play it play around with", "tokens": [4083, 5870, 4358, 300, 3212, 380, 1880, 370, 300, 311, 733, 295, 493, 281, 291, 281, 862, 309, 862, 926, 365], "temperature": 0.0, "avg_logprob": -0.20600696362947163, "compression_ratio": 1.6554621848739495, "no_speech_prob": 5.173886165721342e-06}, {"id": 601, "seek": 291584, "start": 2930.08, "end": 2937.0, "text": " But in this case like this is actually I found this very interesting it clearly tells me I need to find out what", "tokens": [583, 294, 341, 1389, 411, 341, 307, 767, 286, 1352, 341, 588, 1880, 309, 4448, 5112, 385, 286, 643, 281, 915, 484, 437], "temperature": 0.0, "avg_logprob": -0.20600696362947163, "compression_ratio": 1.6554621848739495, "no_speech_prob": 5.173886165721342e-06}, {"id": 602, "seek": 291584, "start": 2937.36, "end": 2939.6800000000003, "text": " enclosure E ROTS with a C is", "tokens": [34093, 462, 497, 5068, 50, 365, 257, 383, 307], "temperature": 0.0, "avg_logprob": -0.20600696362947163, "compression_ratio": 1.6554621848739495, "no_speech_prob": 5.173886165721342e-06}, {"id": 603, "seek": 291584, "start": 2940.32, "end": 2945.0, "text": " Why is it important because like it means nothing to me right?", "tokens": [1545, 307, 309, 1021, 570, 411, 309, 1355, 1825, 281, 385, 558, 30], "temperature": 0.0, "avg_logprob": -0.20600696362947163, "compression_ratio": 1.6554621848739495, "no_speech_prob": 5.173886165721342e-06}, {"id": 604, "seek": 294500, "start": 2945.0, "end": 2948.52, "text": " And but it's in the most important thing so I should go figure that out", "tokens": [400, 457, 309, 311, 294, 264, 881, 1021, 551, 370, 286, 820, 352, 2573, 300, 484], "temperature": 0.0, "avg_logprob": -0.2245467753892534, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.5206514035526197e-05}, {"id": 605, "seek": 294500, "start": 2949.2, "end": 2951.2, "text": " Savannah had a question", "tokens": [47902, 632, 257, 1168], "temperature": 0.0, "avg_logprob": -0.2245467753892534, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.5206514035526197e-05}, {"id": 606, "seek": 294500, "start": 2956.0, "end": 2962.3, "text": " So can you explain how changing the next number of categories works because for me it just seems like there's five categories", "tokens": [407, 393, 291, 2903, 577, 4473, 264, 958, 1230, 295, 10479, 1985, 570, 337, 385, 309, 445, 2544, 411, 456, 311, 1732, 10479], "temperature": 0.0, "avg_logprob": -0.2245467753892534, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.5206514035526197e-05}, {"id": 607, "seek": 294500, "start": 2962.3, "end": 2965.28, "text": " There's five categories. Oh, yeah, sorry, so it's it's just like", "tokens": [821, 311, 1732, 10479, 13, 876, 11, 1338, 11, 2597, 11, 370, 309, 311, 309, 311, 445, 411], "temperature": 0.0, "avg_logprob": -0.2245467753892534, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.5206514035526197e-05}, {"id": 608, "seek": 294500, "start": 2967.48, "end": 2971.92, "text": " All it's doing is saying like okay. Here's a column called zip code", "tokens": [1057, 309, 311, 884, 307, 1566, 411, 1392, 13, 1692, 311, 257, 7738, 1219, 20730, 3089], "temperature": 0.0, "avg_logprob": -0.2245467753892534, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.5206514035526197e-05}, {"id": 609, "seek": 297192, "start": 2971.92, "end": 2975.48, "text": " here's a column called usage band and", "tokens": [510, 311, 257, 7738, 1219, 14924, 4116, 293], "temperature": 0.0, "avg_logprob": -0.17126096997942244, "compression_ratio": 1.7727272727272727, "no_speech_prob": 5.0936814659507945e-06}, {"id": 610, "seek": 297192, "start": 2977.76, "end": 2979.6800000000003, "text": " Here's a column", "tokens": [1692, 311, 257, 7738], "temperature": 0.0, "avg_logprob": -0.17126096997942244, "compression_ratio": 1.7727272727272727, "no_speech_prob": 5.0936814659507945e-06}, {"id": 611, "seek": 297192, "start": 2979.6800000000003, "end": 2984.52, "text": " Sex right I don't know whatever right and so like zip code has whatever", "tokens": [29037, 558, 286, 500, 380, 458, 2035, 558, 293, 370, 411, 20730, 3089, 575, 2035], "temperature": 0.0, "avg_logprob": -0.17126096997942244, "compression_ratio": 1.7727272727272727, "no_speech_prob": 5.0936814659507945e-06}, {"id": 612, "seek": 297192, "start": 2985.36, "end": 2989.4, "text": " 5,000 levels the number of levels in a category we call its", "tokens": [1025, 11, 1360, 4358, 264, 1230, 295, 4358, 294, 257, 7719, 321, 818, 1080], "temperature": 0.0, "avg_logprob": -0.17126096997942244, "compression_ratio": 1.7727272727272727, "no_speech_prob": 5.0936814659507945e-06}, {"id": 613, "seek": 297192, "start": 2990.58, "end": 2992.16, "text": " cardinality", "tokens": [2920, 259, 1860], "temperature": 0.0, "avg_logprob": -0.17126096997942244, "compression_ratio": 1.7727272727272727, "no_speech_prob": 5.0936814659507945e-06}, {"id": 614, "seek": 297192, "start": 2992.16, "end": 2993.76, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.17126096997942244, "compression_ratio": 1.7727272727272727, "no_speech_prob": 5.0936814659507945e-06}, {"id": 615, "seek": 297192, "start": 2993.76, "end": 3000.64, "text": " So it has a cardinality of 5,000 usage band maybe has a cardinality of six sex has maybe a cardinality of two", "tokens": [407, 309, 575, 257, 2920, 259, 1860, 295, 1025, 11, 1360, 14924, 4116, 1310, 575, 257, 2920, 259, 1860, 295, 2309, 3260, 575, 1310, 257, 2920, 259, 1860, 295, 732], "temperature": 0.0, "avg_logprob": -0.17126096997942244, "compression_ratio": 1.7727272727272727, "no_speech_prob": 5.0936814659507945e-06}, {"id": 616, "seek": 300064, "start": 3000.64, "end": 3004.62, "text": " So when Procter F goes through and says okay", "tokens": [407, 562, 1705, 349, 260, 479, 1709, 807, 293, 1619, 1392], "temperature": 0.0, "avg_logprob": -0.2525788288490445, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.1544598237378523e-06}, {"id": 617, "seek": 300064, "start": 3004.92, "end": 3010.68, "text": " This is a categorical variable should I one hot encoded it checks the cardinality", "tokens": [639, 307, 257, 19250, 804, 7006, 820, 286, 472, 2368, 2058, 12340, 309, 13834, 264, 2920, 259, 1860], "temperature": 0.0, "avg_logprob": -0.2525788288490445, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.1544598237378523e-06}, {"id": 618, "seek": 300064, "start": 3011.3199999999997, "end": 3016.14, "text": " Against max and cats and says all 5,000 is bigger than 7", "tokens": [29995, 11469, 293, 11111, 293, 1619, 439, 1025, 11, 1360, 307, 3801, 813, 1614], "temperature": 0.0, "avg_logprob": -0.2525788288490445, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.1544598237378523e-06}, {"id": 619, "seek": 300064, "start": 3016.24, "end": 3022.2799999999997, "text": " So I don't want hot encoded and then it goes to usage band 6 is less than 7", "tokens": [407, 286, 500, 380, 528, 2368, 2058, 12340, 293, 550, 309, 1709, 281, 14924, 4116, 1386, 307, 1570, 813, 1614], "temperature": 0.0, "avg_logprob": -0.2525788288490445, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.1544598237378523e-06}, {"id": 620, "seek": 300064, "start": 3022.2799999999997, "end": 3028.8599999999997, "text": " I do one hot encoded goes to sex 2 is less than 7. I do one encoded so it just says for each variable", "tokens": [286, 360, 472, 2368, 2058, 12340, 1709, 281, 3260, 568, 307, 1570, 813, 1614, 13, 286, 360, 472, 2058, 12340, 370, 309, 445, 1619, 337, 1184, 7006], "temperature": 0.0, "avg_logprob": -0.2525788288490445, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.1544598237378523e-06}, {"id": 621, "seek": 302886, "start": 3028.86, "end": 3032.42, "text": " How do I decide whether the one hot encoded or not?", "tokens": [1012, 360, 286, 4536, 1968, 264, 472, 2368, 2058, 12340, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.25624611338631054, "compression_ratio": 1.509933774834437, "no_speech_prob": 6.747992301825434e-06}, {"id": 622, "seek": 302886, "start": 3037.5, "end": 3042.52, "text": " No, once we decide to one hot encode it does not keep the original variable", "tokens": [883, 11, 1564, 321, 4536, 281, 472, 2368, 2058, 1429, 309, 775, 406, 1066, 264, 3380, 7006], "temperature": 0.0, "avg_logprob": -0.25624611338631054, "compression_ratio": 1.509933774834437, "no_speech_prob": 6.747992301825434e-06}, {"id": 623, "seek": 302886, "start": 3053.06, "end": 3058.44, "text": " Well, you don't need a labeling code if the if so if the best is an interval it can approximate that", "tokens": [1042, 11, 291, 500, 380, 643, 257, 40244, 3089, 498, 264, 498, 370, 498, 264, 1151, 307, 364, 15035, 309, 393, 30874, 300], "temperature": 0.0, "avg_logprob": -0.25624611338631054, "compression_ratio": 1.509933774834437, "no_speech_prob": 6.747992301825434e-06}, {"id": 624, "seek": 305844, "start": 3058.44, "end": 3060.64, "text": " With multiple one hot encoding levels", "tokens": [2022, 3866, 472, 2368, 43430, 4358], "temperature": 0.0, "avg_logprob": -0.2548467044172616, "compression_ratio": 1.5406698564593302, "no_speech_prob": 2.2959122816246236e-06}, {"id": 625, "seek": 305844, "start": 3062.7400000000002, "end": 3064.98, "text": " Yeah, so like you know, it's a", "tokens": [865, 11, 370, 411, 291, 458, 11, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.2548467044172616, "compression_ratio": 1.5406698564593302, "no_speech_prob": 2.2959122816246236e-06}, {"id": 626, "seek": 305844, "start": 3066.54, "end": 3070.12, "text": " The the truth is that each column is going to have some", "tokens": [440, 264, 3494, 307, 300, 1184, 7738, 307, 516, 281, 362, 512], "temperature": 0.0, "avg_logprob": -0.2548467044172616, "compression_ratio": 1.5406698564593302, "no_speech_prob": 2.2959122816246236e-06}, {"id": 627, "seek": 305844, "start": 3071.02, "end": 3072.3, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.2548467044172616, "compression_ratio": 1.5406698564593302, "no_speech_prob": 2.2959122816246236e-06}, {"id": 628, "seek": 305844, "start": 3072.3, "end": 3079.2200000000003, "text": " Different you know should it be labeling coded or not you know which you could make on a case-by-case basis. I find in practice", "tokens": [20825, 291, 458, 820, 309, 312, 40244, 34874, 420, 406, 291, 458, 597, 291, 727, 652, 322, 257, 1389, 12, 2322, 12, 9765, 5143, 13, 286, 915, 294, 3124], "temperature": 0.0, "avg_logprob": -0.2548467044172616, "compression_ratio": 1.5406698564593302, "no_speech_prob": 2.2959122816246236e-06}, {"id": 629, "seek": 305844, "start": 3080.86, "end": 3085.1, "text": " It's just not that sensitive to this and so I find like just", "tokens": [467, 311, 445, 406, 300, 9477, 281, 341, 293, 370, 286, 915, 411, 445], "temperature": 0.0, "avg_logprob": -0.2548467044172616, "compression_ratio": 1.5406698564593302, "no_speech_prob": 2.2959122816246236e-06}, {"id": 630, "seek": 308510, "start": 3085.1, "end": 3089.12, "text": " Using a single number for the whole data set gives me what I need", "tokens": [11142, 257, 2167, 1230, 337, 264, 1379, 1412, 992, 2709, 385, 437, 286, 643], "temperature": 0.0, "avg_logprob": -0.24307639782245344, "compression_ratio": 1.6883116883116882, "no_speech_prob": 2.2252750113693764e-06}, {"id": 631, "seek": 308510, "start": 3089.8199999999997, "end": 3091.8199999999997, "text": " But you know if you were", "tokens": [583, 291, 458, 498, 291, 645], "temperature": 0.0, "avg_logprob": -0.24307639782245344, "compression_ratio": 1.6883116883116882, "no_speech_prob": 2.2252750113693764e-06}, {"id": 632, "seek": 308510, "start": 3092.42, "end": 3095.2999999999997, "text": " Building a model that really had to be as awesome as possible", "tokens": [18974, 257, 2316, 300, 534, 632, 281, 312, 382, 3476, 382, 1944], "temperature": 0.0, "avg_logprob": -0.24307639782245344, "compression_ratio": 1.6883116883116882, "no_speech_prob": 2.2252750113693764e-06}, {"id": 633, "seek": 308510, "start": 3095.3399999999997, "end": 3098.58, "text": " And you had lots and lots of time to do it you can go through man", "tokens": [400, 291, 632, 3195, 293, 3195, 295, 565, 281, 360, 309, 291, 393, 352, 807, 587], "temperature": 0.0, "avg_logprob": -0.24307639782245344, "compression_ratio": 1.6883116883116882, "no_speech_prob": 2.2252750113693764e-06}, {"id": 634, "seek": 308510, "start": 3098.58, "end": 3103.18, "text": " You know don't use property if you can go through manually and decide which things to use dummies or not", "tokens": [509, 458, 500, 380, 764, 4707, 498, 291, 393, 352, 807, 16945, 293, 4536, 597, 721, 281, 764, 16784, 38374, 420, 406], "temperature": 0.0, "avg_logprob": -0.24307639782245344, "compression_ratio": 1.6883116883116882, "no_speech_prob": 2.2252750113693764e-06}, {"id": 635, "seek": 308510, "start": 3104.54, "end": 3109.1, "text": " You'll see in the code if you look at the code for prop DF", "tokens": [509, 603, 536, 294, 264, 3089, 498, 291, 574, 412, 264, 3089, 337, 2365, 48336], "temperature": 0.0, "avg_logprob": -0.24307639782245344, "compression_ratio": 1.6883116883116882, "no_speech_prob": 2.2252750113693764e-06}, {"id": 636, "seek": 308510, "start": 3110.5, "end": 3112.5, "text": " Proc DF", "tokens": [1705, 66, 48336], "temperature": 0.0, "avg_logprob": -0.24307639782245344, "compression_ratio": 1.6883116883116882, "no_speech_prob": 2.2252750113693764e-06}, {"id": 637, "seek": 311250, "start": 3112.5, "end": 3115.46, "text": " Right like I never want you to feel like", "tokens": [1779, 411, 286, 1128, 528, 291, 281, 841, 411], "temperature": 0.0, "avg_logprob": -0.20163293038645097, "compression_ratio": 1.4968152866242037, "no_speech_prob": 6.276688964135246e-07}, {"id": 638, "seek": 311250, "start": 3116.7, "end": 3123.1, "text": " The code that happens to be in the fast AI library is the code that you're limited to right? So where is that done?", "tokens": [440, 3089, 300, 2314, 281, 312, 294, 264, 2370, 7318, 6405, 307, 264, 3089, 300, 291, 434, 5567, 281, 558, 30, 407, 689, 307, 300, 1096, 30], "temperature": 0.0, "avg_logprob": -0.20163293038645097, "compression_ratio": 1.4968152866242037, "no_speech_prob": 6.276688964135246e-07}, {"id": 639, "seek": 311250, "start": 3124.3, "end": 3126.3, "text": " You can see that", "tokens": [509, 393, 536, 300], "temperature": 0.0, "avg_logprob": -0.20163293038645097, "compression_ratio": 1.4968152866242037, "no_speech_prob": 6.276688964135246e-07}, {"id": 640, "seek": 311250, "start": 3129.14, "end": 3135.82, "text": " The max NCAT gets passed to numerical eyes and numerical eyes", "tokens": [440, 11469, 20786, 2218, 2170, 4678, 281, 29054, 2575, 293, 29054, 2575], "temperature": 0.0, "avg_logprob": -0.20163293038645097, "compression_ratio": 1.4968152866242037, "no_speech_prob": 6.276688964135246e-07}, {"id": 641, "seek": 313582, "start": 3135.82, "end": 3137.82, "text": " Simply", "tokens": [19596], "temperature": 0.0, "avg_logprob": -0.23108703707471306, "compression_ratio": 1.6071428571428572, "no_speech_prob": 5.0936696425196715e-06}, {"id": 642, "seek": 313582, "start": 3140.98, "end": 3147.78, "text": " Checks okay is that a numeric type and is the number of categories either not been passed to us at all or", "tokens": [3351, 2761, 1392, 307, 300, 257, 7866, 299, 2010, 293, 307, 264, 1230, 295, 10479, 2139, 406, 668, 4678, 281, 505, 412, 439, 420], "temperature": 0.0, "avg_logprob": -0.23108703707471306, "compression_ratio": 1.6071428571428572, "no_speech_prob": 5.0936696425196715e-06}, {"id": 643, "seek": 313582, "start": 3148.46, "end": 3150.46, "text": " We've got more unique", "tokens": [492, 600, 658, 544, 3845], "temperature": 0.0, "avg_logprob": -0.23108703707471306, "compression_ratio": 1.6071428571428572, "no_speech_prob": 5.0936696425196715e-06}, {"id": 644, "seek": 313582, "start": 3150.46, "end": 3154.02, "text": " Values than there are categories and if so we're going to use the categorical codes", "tokens": [7188, 1247, 813, 456, 366, 10479, 293, 498, 370, 321, 434, 516, 281, 764, 264, 19250, 804, 14211], "temperature": 0.0, "avg_logprob": -0.23108703707471306, "compression_ratio": 1.6071428571428572, "no_speech_prob": 5.0936696425196715e-06}, {"id": 645, "seek": 313582, "start": 3155.1800000000003, "end": 3161.38, "text": " So for any column where that's where it's skipped over that right so it's remained as a category", "tokens": [407, 337, 604, 7738, 689, 300, 311, 689, 309, 311, 30193, 670, 300, 558, 370, 309, 311, 12780, 382, 257, 7719], "temperature": 0.0, "avg_logprob": -0.23108703707471306, "compression_ratio": 1.6071428571428572, "no_speech_prob": 5.0936696425196715e-06}, {"id": 646, "seek": 316138, "start": 3161.38, "end": 3167.7000000000003, "text": " Then at the very end we just go pandas dot get dummies we pass in the whole data frame and so pandas dot get dummies", "tokens": [1396, 412, 264, 588, 917, 321, 445, 352, 4565, 296, 5893, 483, 16784, 38374, 321, 1320, 294, 264, 1379, 1412, 3920, 293, 370, 4565, 296, 5893, 483, 16784, 38374], "temperature": 0.0, "avg_logprob": -0.19145795648748223, "compression_ratio": 1.7883817427385893, "no_speech_prob": 8.31524175737286e-07}, {"id": 647, "seek": 316138, "start": 3167.7000000000003, "end": 3169.86, "text": " You pass in the whole data frame it checks for anything", "tokens": [509, 1320, 294, 264, 1379, 1412, 3920, 309, 13834, 337, 1340], "temperature": 0.0, "avg_logprob": -0.19145795648748223, "compression_ratio": 1.7883817427385893, "no_speech_prob": 8.31524175737286e-07}, {"id": 648, "seek": 316138, "start": 3169.86, "end": 3173.6, "text": " That's still a categorical variable and it turns it into a dummy variable", "tokens": [663, 311, 920, 257, 19250, 804, 7006, 293, 309, 4523, 309, 666, 257, 35064, 7006], "temperature": 0.0, "avg_logprob": -0.19145795648748223, "compression_ratio": 1.7883817427385893, "no_speech_prob": 8.31524175737286e-07}, {"id": 649, "seek": 316138, "start": 3173.7000000000003, "end": 3178.82, "text": " Which is another way of saying a one-hot encoding so you know with that kind of approach you can easily", "tokens": [3013, 307, 1071, 636, 295, 1566, 257, 472, 12, 12194, 43430, 370, 291, 458, 365, 300, 733, 295, 3109, 291, 393, 3612], "temperature": 0.0, "avg_logprob": -0.19145795648748223, "compression_ratio": 1.7883817427385893, "no_speech_prob": 8.31524175737286e-07}, {"id": 650, "seek": 316138, "start": 3179.6600000000003, "end": 3181.46, "text": " Override it and do your own", "tokens": [4886, 25502, 309, 293, 360, 428, 1065], "temperature": 0.0, "avg_logprob": -0.19145795648748223, "compression_ratio": 1.7883817427385893, "no_speech_prob": 8.31524175737286e-07}, {"id": 651, "seek": 316138, "start": 3181.46, "end": 3183.46, "text": " dummy verification", "tokens": [35064, 30206], "temperature": 0.0, "avg_logprob": -0.19145795648748223, "compression_ratio": 1.7883817427385893, "no_speech_prob": 8.31524175737286e-07}, {"id": 652, "seek": 316138, "start": 3183.46, "end": 3185.46, "text": " variable ization", "tokens": [7006, 220, 2144], "temperature": 0.0, "avg_logprob": -0.19145795648748223, "compression_ratio": 1.7883817427385893, "no_speech_prob": 8.31524175737286e-07}, {"id": 653, "seek": 318546, "start": 3185.46, "end": 3190.46, "text": " Did you have a question for so some data has", "tokens": [2589, 291, 362, 257, 1168, 337, 370, 512, 1412, 575], "temperature": 0.0, "avg_logprob": -0.20499934219732519, "compression_ratio": 1.6504854368932038, "no_speech_prob": 4.005712617072277e-05}, {"id": 654, "seek": 318546, "start": 3191.18, "end": 3196.06, "text": " Quite obvious order like if you have like a rating system like good bad", "tokens": [20464, 6322, 1668, 411, 498, 291, 362, 411, 257, 10990, 1185, 411, 665, 1578], "temperature": 0.0, "avg_logprob": -0.20499934219732519, "compression_ratio": 1.6504854368932038, "no_speech_prob": 4.005712617072277e-05}, {"id": 655, "seek": 318546, "start": 3197.46, "end": 3199.46, "text": " Poor or whatever things like that", "tokens": [23591, 420, 2035, 721, 411, 300], "temperature": 0.0, "avg_logprob": -0.20499934219732519, "compression_ratio": 1.6504854368932038, "no_speech_prob": 4.005712617072277e-05}, {"id": 656, "seek": 318546, "start": 3199.9, "end": 3204.68, "text": " There's an order to that and showing that order by doing the dummy variable thing", "tokens": [821, 311, 364, 1668, 281, 300, 293, 4099, 300, 1668, 538, 884, 264, 35064, 7006, 551], "temperature": 0.0, "avg_logprob": -0.20499934219732519, "compression_ratio": 1.6504854368932038, "no_speech_prob": 4.005712617072277e-05}, {"id": 657, "seek": 318546, "start": 3205.3, "end": 3207.38, "text": " Probably will work to your benefit", "tokens": [9210, 486, 589, 281, 428, 5121], "temperature": 0.0, "avg_logprob": -0.20499934219732519, "compression_ratio": 1.6504854368932038, "no_speech_prob": 4.005712617072277e-05}, {"id": 658, "seek": 318546, "start": 3207.86, "end": 3213.46, "text": " So is there a way to just force it to leave alone one variable just like", "tokens": [407, 307, 456, 257, 636, 281, 445, 3464, 309, 281, 1856, 3312, 472, 7006, 445, 411], "temperature": 0.0, "avg_logprob": -0.20499934219732519, "compression_ratio": 1.6504854368932038, "no_speech_prob": 4.005712617072277e-05}, {"id": 659, "seek": 321346, "start": 3213.46, "end": 3215.7, "text": " Convert it beforehand yourself", "tokens": [2656, 3281, 309, 22893, 1803], "temperature": 0.0, "avg_logprob": -0.1812583246538716, "compression_ratio": 1.3696969696969696, "no_speech_prob": 1.1911052979485248e-06}, {"id": 660, "seek": 321346, "start": 3217.82, "end": 3219.82, "text": " Not not in the library", "tokens": [1726, 406, 294, 264, 6405], "temperature": 0.0, "avg_logprob": -0.1812583246538716, "compression_ratio": 1.3696969696969696, "no_speech_prob": 1.1911052979485248e-06}, {"id": 661, "seek": 321346, "start": 3219.9, "end": 3228.02, "text": " And to remind you like unless we explicitly do something about it. We're not going to get that order so when we", "tokens": [400, 281, 4160, 291, 411, 5969, 321, 20803, 360, 746, 466, 309, 13, 492, 434, 406, 516, 281, 483, 300, 1668, 370, 562, 321], "temperature": 0.0, "avg_logprob": -0.1812583246538716, "compression_ratio": 1.3696969696969696, "no_speech_prob": 1.1911052979485248e-06}, {"id": 662, "seek": 321346, "start": 3230.58, "end": 3232.58, "text": " When we import the data", "tokens": [1133, 321, 974, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1812583246538716, "compression_ratio": 1.3696969696969696, "no_speech_prob": 1.1911052979485248e-06}, {"id": 663, "seek": 321346, "start": 3235.62, "end": 3237.62, "text": " This is in lesson 1 RF", "tokens": [639, 307, 294, 6898, 502, 26204], "temperature": 0.0, "avg_logprob": -0.1812583246538716, "compression_ratio": 1.3696969696969696, "no_speech_prob": 1.1911052979485248e-06}, {"id": 664, "seek": 321346, "start": 3239.34, "end": 3241.14, "text": " We showed how", "tokens": [492, 4712, 577], "temperature": 0.0, "avg_logprob": -0.1812583246538716, "compression_ratio": 1.3696969696969696, "no_speech_prob": 1.1911052979485248e-06}, {"id": 665, "seek": 324114, "start": 3241.14, "end": 3243.7799999999997, "text": " By default the categories are ordered alphabetically", "tokens": [3146, 7576, 264, 10479, 366, 8866, 23339, 984], "temperature": 0.0, "avg_logprob": -0.22558530976500693, "compression_ratio": 1.585, "no_speech_prob": 2.156799837393919e-06}, {"id": 666, "seek": 324114, "start": 3244.74, "end": 3247.06, "text": " And we have the ability to order them", "tokens": [400, 321, 362, 264, 3485, 281, 1668, 552], "temperature": 0.0, "avg_logprob": -0.22558530976500693, "compression_ratio": 1.585, "no_speech_prob": 2.156799837393919e-06}, {"id": 667, "seek": 324114, "start": 3247.98, "end": 3254.58, "text": " Properly so yeah, if you've actually made an effort to turn your ordinal variables into proper", "tokens": [27627, 356, 370, 1338, 11, 498, 291, 600, 767, 1027, 364, 4630, 281, 1261, 428, 4792, 2071, 9102, 666, 2296], "temperature": 0.0, "avg_logprob": -0.22558530976500693, "compression_ratio": 1.585, "no_speech_prob": 2.156799837393919e-06}, {"id": 668, "seek": 324114, "start": 3255.1, "end": 3257.1, "text": " ordinals", "tokens": [25376, 1124], "temperature": 0.0, "avg_logprob": -0.22558530976500693, "compression_ratio": 1.585, "no_speech_prob": 2.156799837393919e-06}, {"id": 669, "seek": 324114, "start": 3257.8599999999997, "end": 3259.8599999999997, "text": " Using prop DF", "tokens": [11142, 2365, 48336], "temperature": 0.0, "avg_logprob": -0.22558530976500693, "compression_ratio": 1.585, "no_speech_prob": 2.156799837393919e-06}, {"id": 670, "seek": 324114, "start": 3260.94, "end": 3268.58, "text": " Can destroy that if you have max in cats so the simple thing the simple way to avoid that is if we know that", "tokens": [1664, 5293, 300, 498, 291, 362, 11469, 294, 11111, 370, 264, 2199, 551, 264, 2199, 636, 281, 5042, 300, 307, 498, 321, 458, 300], "temperature": 0.0, "avg_logprob": -0.22558530976500693, "compression_ratio": 1.585, "no_speech_prob": 2.156799837393919e-06}, {"id": 671, "seek": 326858, "start": 3268.58, "end": 3272.62, "text": " we always want to use the codes for usage band rather than the", "tokens": [321, 1009, 528, 281, 764, 264, 14211, 337, 14924, 4116, 2831, 813, 264], "temperature": 0.0, "avg_logprob": -0.23250733352288966, "compression_ratio": 1.6864864864864866, "no_speech_prob": 6.240876246010885e-06}, {"id": 672, "seek": 326858, "start": 3273.7799999999997, "end": 3279.2799999999997, "text": " You know like never one hot encoded you could just go ahead and replace it right you could just say okay", "tokens": [509, 458, 411, 1128, 472, 2368, 2058, 12340, 291, 727, 445, 352, 2286, 293, 7406, 309, 558, 291, 727, 445, 584, 1392], "temperature": 0.0, "avg_logprob": -0.23250733352288966, "compression_ratio": 1.6864864864864866, "no_speech_prob": 6.240876246010885e-06}, {"id": 673, "seek": 326858, "start": 3279.34, "end": 3284.74, "text": " Let's just go DF dot usage band equals DF dot usage band dot cat dot codes, and it's now an integer", "tokens": [961, 311, 445, 352, 48336, 5893, 14924, 4116, 6915, 48336, 5893, 14924, 4116, 5893, 3857, 5893, 14211, 11, 293, 309, 311, 586, 364, 24922], "temperature": 0.0, "avg_logprob": -0.23250733352288966, "compression_ratio": 1.6864864864864866, "no_speech_prob": 6.240876246010885e-06}, {"id": 674, "seek": 326858, "start": 3285.2599999999998, "end": 3287.2599999999998, "text": " And so it'll never get changed", "tokens": [400, 370, 309, 603, 1128, 483, 3105], "temperature": 0.0, "avg_logprob": -0.23250733352288966, "compression_ratio": 1.6864864864864866, "no_speech_prob": 6.240876246010885e-06}, {"id": 675, "seek": 326858, "start": 3290.42, "end": 3292.42, "text": " All right, so", "tokens": [1057, 558, 11, 370], "temperature": 0.0, "avg_logprob": -0.23250733352288966, "compression_ratio": 1.6864864864864866, "no_speech_prob": 6.240876246010885e-06}, {"id": 676, "seek": 329242, "start": 3292.42, "end": 3294.42, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.13447521672104346, "compression_ratio": 1.5706521739130435, "no_speech_prob": 9.57080601438065e-07}, {"id": 677, "seek": 329242, "start": 3298.02, "end": 3300.02, "text": " We kind of have already seen how", "tokens": [492, 733, 295, 362, 1217, 1612, 577], "temperature": 0.0, "avg_logprob": -0.13447521672104346, "compression_ratio": 1.5706521739130435, "no_speech_prob": 9.57080601438065e-07}, {"id": 678, "seek": 329242, "start": 3301.94, "end": 3307.14, "text": " Variables which are basically measuring the same thing can kind of confuse our", "tokens": [32511, 2965, 597, 366, 1936, 13389, 264, 912, 551, 393, 733, 295, 28584, 527], "temperature": 0.0, "avg_logprob": -0.13447521672104346, "compression_ratio": 1.5706521739130435, "no_speech_prob": 9.57080601438065e-07}, {"id": 679, "seek": 329242, "start": 3307.5, "end": 3308.98, "text": " variable importance", "tokens": [7006, 7379], "temperature": 0.0, "avg_logprob": -0.13447521672104346, "compression_ratio": 1.5706521739130435, "no_speech_prob": 9.57080601438065e-07}, {"id": 680, "seek": 329242, "start": 3308.98, "end": 3316.2200000000003, "text": " And there can also make our random forests slightly less good because it requires like more computation to do the same thing", "tokens": [400, 456, 393, 611, 652, 527, 4974, 21700, 4748, 1570, 665, 570, 309, 7029, 411, 544, 24903, 281, 360, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.13447521672104346, "compression_ratio": 1.5706521739130435, "no_speech_prob": 9.57080601438065e-07}, {"id": 681, "seek": 329242, "start": 3316.2200000000003, "end": 3318.2200000000003, "text": " There's more columns to check", "tokens": [821, 311, 544, 13766, 281, 1520], "temperature": 0.0, "avg_logprob": -0.13447521672104346, "compression_ratio": 1.5706521739130435, "no_speech_prob": 9.57080601438065e-07}, {"id": 682, "seek": 331822, "start": 3318.22, "end": 3323.4599999999996, "text": " So I'm going to do some more work to try and remove redundant features", "tokens": [407, 286, 478, 516, 281, 360, 512, 544, 589, 281, 853, 293, 4159, 40997, 4122], "temperature": 0.0, "avg_logprob": -0.195480368110571, "compression_ratio": 1.6027397260273972, "no_speech_prob": 7.934472137094417e-07}, {"id": 683, "seek": 331822, "start": 3323.98, "end": 3327.7799999999997, "text": " And the way I do that is to do something called a dendrogram", "tokens": [400, 264, 636, 286, 360, 300, 307, 281, 360, 746, 1219, 257, 274, 521, 340, 1342], "temperature": 0.0, "avg_logprob": -0.195480368110571, "compression_ratio": 1.6027397260273972, "no_speech_prob": 7.934472137094417e-07}, {"id": 684, "seek": 331822, "start": 3328.74, "end": 3334.02, "text": " And it's a kind of hierarchical clustering so cluster analysis", "tokens": [400, 309, 311, 257, 733, 295, 35250, 804, 596, 48673, 370, 13630, 5215], "temperature": 0.0, "avg_logprob": -0.195480368110571, "compression_ratio": 1.6027397260273972, "no_speech_prob": 7.934472137094417e-07}, {"id": 685, "seek": 331822, "start": 3334.7, "end": 3337.8599999999997, "text": " Is something where you're trying to look at objects?", "tokens": [1119, 746, 689, 291, 434, 1382, 281, 574, 412, 6565, 30], "temperature": 0.0, "avg_logprob": -0.195480368110571, "compression_ratio": 1.6027397260273972, "no_speech_prob": 7.934472137094417e-07}, {"id": 686, "seek": 331822, "start": 3337.8999999999996, "end": 3344.4599999999996, "text": " They can be either rows in a data set or columns and find which ones are similar to each other so often", "tokens": [814, 393, 312, 2139, 13241, 294, 257, 1412, 992, 420, 13766, 293, 915, 597, 2306, 366, 2531, 281, 1184, 661, 370, 2049], "temperature": 0.0, "avg_logprob": -0.195480368110571, "compression_ratio": 1.6027397260273972, "no_speech_prob": 7.934472137094417e-07}, {"id": 687, "seek": 334446, "start": 3344.46, "end": 3351.2200000000003, "text": " You'll see people particularly talking about cluster analysis. They normally refer to rows of data, and they'll say like oh, let's plot it", "tokens": [509, 603, 536, 561, 4098, 1417, 466, 13630, 5215, 13, 814, 5646, 2864, 281, 13241, 295, 1412, 11, 293, 436, 603, 584, 411, 1954, 11, 718, 311, 7542, 309], "temperature": 0.0, "avg_logprob": -0.16875865671894338, "compression_ratio": 1.7469387755102042, "no_speech_prob": 2.5215622372343205e-06}, {"id": 688, "seek": 334446, "start": 3351.7, "end": 3356.78, "text": " Right and like oh, there's a cluster and there's a cluster right a", "tokens": [1779, 293, 411, 1954, 11, 456, 311, 257, 13630, 293, 456, 311, 257, 13630, 558, 257], "temperature": 0.0, "avg_logprob": -0.16875865671894338, "compression_ratio": 1.7469387755102042, "no_speech_prob": 2.5215622372343205e-06}, {"id": 689, "seek": 334446, "start": 3357.82, "end": 3364.7, "text": " Common type of cluster analysis time to permitting we may get around to talking about this in some detail is called k-means", "tokens": [18235, 2010, 295, 13630, 5215, 565, 281, 4784, 2414, 321, 815, 483, 926, 281, 1417, 466, 341, 294, 512, 2607, 307, 1219, 350, 12, 1398, 599], "temperature": 0.0, "avg_logprob": -0.16875865671894338, "compression_ratio": 1.7469387755102042, "no_speech_prob": 2.5215622372343205e-06}, {"id": 690, "seek": 334446, "start": 3365.82, "end": 3371.5, "text": " Which is basically where you assume that you don't have any labels at all and you take basically a", "tokens": [3013, 307, 1936, 689, 291, 6552, 300, 291, 500, 380, 362, 604, 16949, 412, 439, 293, 291, 747, 1936, 257], "temperature": 0.0, "avg_logprob": -0.16875865671894338, "compression_ratio": 1.7469387755102042, "no_speech_prob": 2.5215622372343205e-06}, {"id": 691, "seek": 337150, "start": 3371.5, "end": 3373.46, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.24967165082414575, "compression_ratio": 1.6953125, "no_speech_prob": 8.579218047088943e-07}, {"id": 692, "seek": 337150, "start": 3373.46, "end": 3376.8, "text": " couple of data points at random and you gradually", "tokens": [1916, 295, 1412, 2793, 412, 4974, 293, 291, 13145], "temperature": 0.0, "avg_logprob": -0.24967165082414575, "compression_ratio": 1.6953125, "no_speech_prob": 8.579218047088943e-07}, {"id": 693, "seek": 337150, "start": 3377.42, "end": 3383.34, "text": " Find the ones that are near to it and move them closer and closer to centroids and you kind of repeat it again and again", "tokens": [11809, 264, 2306, 300, 366, 2651, 281, 309, 293, 1286, 552, 4966, 293, 4966, 281, 24607, 3742, 293, 291, 733, 295, 7149, 309, 797, 293, 797], "temperature": 0.0, "avg_logprob": -0.24967165082414575, "compression_ratio": 1.6953125, "no_speech_prob": 8.579218047088943e-07}, {"id": 694, "seek": 337150, "start": 3383.86, "end": 3390.14, "text": " And it's an iterative approach that you basically tell it how many clusters you want and it'll tell you where it thinks the clusters are", "tokens": [400, 309, 311, 364, 17138, 1166, 3109, 300, 291, 1936, 980, 309, 577, 867, 23313, 291, 528, 293, 309, 603, 980, 291, 689, 309, 7309, 264, 23313, 366], "temperature": 0.0, "avg_logprob": -0.24967165082414575, "compression_ratio": 1.6953125, "no_speech_prob": 8.579218047088943e-07}, {"id": 695, "seek": 337150, "start": 3391.06, "end": 3395.54, "text": " I really and I don't know why but I really underused technique", "tokens": [286, 534, 293, 286, 500, 380, 458, 983, 457, 286, 534, 833, 4717, 6532], "temperature": 0.0, "avg_logprob": -0.24967165082414575, "compression_ratio": 1.6953125, "no_speech_prob": 8.579218047088943e-07}, {"id": 696, "seek": 337150, "start": 3396.3, "end": 3399.42, "text": " 20 30 years ago. It was much more popular than it is today is", "tokens": [945, 2217, 924, 2057, 13, 467, 390, 709, 544, 3743, 813, 309, 307, 965, 307], "temperature": 0.0, "avg_logprob": -0.24967165082414575, "compression_ratio": 1.6953125, "no_speech_prob": 8.579218047088943e-07}, {"id": 697, "seek": 339942, "start": 3399.42, "end": 3401.42, "text": " Hierarchical clustering", "tokens": [10886, 1178, 804, 596, 48673], "temperature": 0.0, "avg_logprob": -0.2813489532470703, "compression_ratio": 1.7391304347826086, "no_speech_prob": 3.0415824312512996e-06}, {"id": 698, "seek": 339942, "start": 3403.26, "end": 3405.26, "text": " Hierarchical", "tokens": [10886, 1178, 804], "temperature": 0.0, "avg_logprob": -0.2813489532470703, "compression_ratio": 1.7391304347826086, "no_speech_prob": 3.0415824312512996e-06}, {"id": 699, "seek": 339942, "start": 3405.66, "end": 3411.14, "text": " Also known as a glomerative clustering and in hierarchical or agglomerative clustering", "tokens": [2743, 2570, 382, 257, 1563, 14301, 1166, 596, 48673, 293, 294, 35250, 804, 420, 623, 7191, 14301, 1166, 596, 48673], "temperature": 0.0, "avg_logprob": -0.2813489532470703, "compression_ratio": 1.7391304347826086, "no_speech_prob": 3.0415824312512996e-06}, {"id": 700, "seek": 339942, "start": 3411.14, "end": 3418.86, "text": " We basically look at every pair of option of every pair of objects and say okay, which two objects are the closest?", "tokens": [492, 1936, 574, 412, 633, 6119, 295, 3614, 295, 633, 6119, 295, 6565, 293, 584, 1392, 11, 597, 732, 6565, 366, 264, 13699, 30], "temperature": 0.0, "avg_logprob": -0.2813489532470703, "compression_ratio": 1.7391304347826086, "no_speech_prob": 3.0415824312512996e-06}, {"id": 701, "seek": 339942, "start": 3419.38, "end": 3421.84, "text": " Right. So in this case we might go, okay", "tokens": [1779, 13, 407, 294, 341, 1389, 321, 1062, 352, 11, 1392], "temperature": 0.0, "avg_logprob": -0.2813489532470703, "compression_ratio": 1.7391304347826086, "no_speech_prob": 3.0415824312512996e-06}, {"id": 702, "seek": 342184, "start": 3421.84, "end": 3430.4, "text": " Those two objects are the closest and so we've kind of like delete them and replace it with the midpoint of the two and then", "tokens": [3950, 732, 6565, 366, 264, 13699, 293, 370, 321, 600, 733, 295, 411, 12097, 552, 293, 7406, 309, 365, 264, 2062, 6053, 295, 264, 732, 293, 550], "temperature": 0.0, "avg_logprob": -0.1604166207490144, "compression_ratio": 1.9372384937238494, "no_speech_prob": 5.539164362744486e-07}, {"id": 703, "seek": 342184, "start": 3430.4, "end": 3435.28, "text": " Okay, here are the next two closest we delete them and replace them with the midpoint of the two and you keep doing that", "tokens": [1033, 11, 510, 366, 264, 958, 732, 13699, 321, 12097, 552, 293, 7406, 552, 365, 264, 2062, 6053, 295, 264, 732, 293, 291, 1066, 884, 300], "temperature": 0.0, "avg_logprob": -0.1604166207490144, "compression_ratio": 1.9372384937238494, "no_speech_prob": 5.539164362744486e-07}, {"id": 704, "seek": 342184, "start": 3435.28, "end": 3440.04, "text": " Again and again, right since we kind of removing points and replacing them with their averages", "tokens": [3764, 293, 797, 11, 558, 1670, 321, 733, 295, 12720, 2793, 293, 19139, 552, 365, 641, 42257], "temperature": 0.0, "avg_logprob": -0.1604166207490144, "compression_ratio": 1.9372384937238494, "no_speech_prob": 5.539164362744486e-07}, {"id": 705, "seek": 342184, "start": 3440.36, "end": 3442.56, "text": " You're gradually reducing a number of points", "tokens": [509, 434, 13145, 12245, 257, 1230, 295, 2793], "temperature": 0.0, "avg_logprob": -0.1604166207490144, "compression_ratio": 1.9372384937238494, "no_speech_prob": 5.539164362744486e-07}, {"id": 706, "seek": 342184, "start": 3443.08, "end": 3448.7200000000003, "text": " By pairwise combining and the cool thing is you can plot that like so, right?", "tokens": [3146, 6119, 3711, 21928, 293, 264, 1627, 551, 307, 291, 393, 7542, 300, 411, 370, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1604166207490144, "compression_ratio": 1.9372384937238494, "no_speech_prob": 5.539164362744486e-07}, {"id": 707, "seek": 344872, "start": 3448.72, "end": 3453.2999999999997, "text": " So if rather than looking at points you look at variables we can say okay", "tokens": [407, 498, 2831, 813, 1237, 412, 2793, 291, 574, 412, 9102, 321, 393, 584, 1392], "temperature": 0.0, "avg_logprob": -0.19449097219139638, "compression_ratio": 1.8868778280542986, "no_speech_prob": 2.4824728370731464e-06}, {"id": 708, "seek": 344872, "start": 3453.2999999999997, "end": 3458.7599999999998, "text": " Which two variables are the most similar that says okay sale year and sale elapsed are very similar", "tokens": [3013, 732, 9102, 366, 264, 881, 2531, 300, 1619, 1392, 8680, 1064, 293, 8680, 806, 2382, 292, 366, 588, 2531], "temperature": 0.0, "avg_logprob": -0.19449097219139638, "compression_ratio": 1.8868778280542986, "no_speech_prob": 2.4824728370731464e-06}, {"id": 709, "seek": 344872, "start": 3458.7599999999998, "end": 3462.6, "text": " so the kind of horizontal axis here is", "tokens": [370, 264, 733, 295, 12750, 10298, 510, 307], "temperature": 0.0, "avg_logprob": -0.19449097219139638, "compression_ratio": 1.8868778280542986, "no_speech_prob": 2.4824728370731464e-06}, {"id": 710, "seek": 344872, "start": 3464.0, "end": 3469.9199999999996, "text": " How similar are the two points that are being compared right? So if they're closer to the right, it means they're very similar", "tokens": [1012, 2531, 366, 264, 732, 2793, 300, 366, 885, 5347, 558, 30, 407, 498, 436, 434, 4966, 281, 264, 558, 11, 309, 1355, 436, 434, 588, 2531], "temperature": 0.0, "avg_logprob": -0.19449097219139638, "compression_ratio": 1.8868778280542986, "no_speech_prob": 2.4824728370731464e-06}, {"id": 711, "seek": 344872, "start": 3470.16, "end": 3474.24, "text": " So say oh year and sale elapsed have been combined and they were very similar", "tokens": [407, 584, 1954, 1064, 293, 8680, 806, 2382, 292, 362, 668, 9354, 293, 436, 645, 588, 2531], "temperature": 0.0, "avg_logprob": -0.19449097219139638, "compression_ratio": 1.8868778280542986, "no_speech_prob": 2.4824728370731464e-06}, {"id": 712, "seek": 347424, "start": 3474.24, "end": 3476.24, "text": " What do you measure?", "tokens": [708, 360, 291, 3481, 30], "temperature": 0.0, "avg_logprob": -0.2625608858854874, "compression_ratio": 1.7168141592920354, "no_speech_prob": 1.5779553450556705e-06}, {"id": 713, "seek": 347424, "start": 3477.7599999999998, "end": 3486.0, "text": " Again it's like who cares, you know, it'll be like the correlation coefficient or something like that, you know in this particular case what I actually did", "tokens": [3764, 309, 311, 411, 567, 12310, 11, 291, 458, 11, 309, 603, 312, 411, 264, 20009, 17619, 420, 746, 411, 300, 11, 291, 458, 294, 341, 1729, 1389, 437, 286, 767, 630], "temperature": 0.0, "avg_logprob": -0.2625608858854874, "compression_ratio": 1.7168141592920354, "no_speech_prob": 1.5779553450556705e-06}, {"id": 714, "seek": 347424, "start": 3486.7999999999997, "end": 3492.56, "text": " So you get to tell it so in this case, I actually used Spearman's are so", "tokens": [407, 291, 483, 281, 980, 309, 370, 294, 341, 1389, 11, 286, 767, 1143, 3550, 289, 1601, 311, 366, 370], "temperature": 0.0, "avg_logprob": -0.2625608858854874, "compression_ratio": 1.7168141592920354, "no_speech_prob": 1.5779553450556705e-06}, {"id": 715, "seek": 349256, "start": 3492.56, "end": 3502.64, "text": " You guys familiar with correlation coefficients already so correlation is cut is almost exactly the same as the r squared, right?", "tokens": [509, 1074, 4963, 365, 20009, 31994, 1217, 370, 20009, 307, 1723, 307, 1920, 2293, 264, 912, 382, 264, 367, 8889, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3120011942727225, "compression_ratio": 1.5521472392638036, "no_speech_prob": 5.2553523346432485e-06}, {"id": 716, "seek": 349256, "start": 3505.32, "end": 3508.82, "text": " But it's between two variables rather than a variable and its prediction", "tokens": [583, 309, 311, 1296, 732, 9102, 2831, 813, 257, 7006, 293, 1080, 17630], "temperature": 0.0, "avg_logprob": -0.3120011942727225, "compression_ratio": 1.5521472392638036, "no_speech_prob": 5.2553523346432485e-06}, {"id": 717, "seek": 349256, "start": 3509.52, "end": 3511.52, "text": " the problem with", "tokens": [264, 1154, 365], "temperature": 0.0, "avg_logprob": -0.3120011942727225, "compression_ratio": 1.5521472392638036, "no_speech_prob": 5.2553523346432485e-06}, {"id": 718, "seek": 349256, "start": 3512.24, "end": 3514.88, "text": " normal correlation is that", "tokens": [2710, 20009, 307, 300], "temperature": 0.0, "avg_logprob": -0.3120011942727225, "compression_ratio": 1.5521472392638036, "no_speech_prob": 5.2553523346432485e-06}, {"id": 719, "seek": 351488, "start": 3514.88, "end": 3521.48, "text": " If the I've got a new workbook here, um", "tokens": [759, 264, 286, 600, 658, 257, 777, 589, 2939, 510, 11, 1105], "temperature": 0.0, "avg_logprob": -0.2402776571420523, "compression_ratio": 1.6162790697674418, "no_speech_prob": 2.5215622372343205e-06}, {"id": 720, "seek": 351488, "start": 3523.04, "end": 3527.52, "text": " If you have data that looks like this then you can", "tokens": [759, 291, 362, 1412, 300, 1542, 411, 341, 550, 291, 393], "temperature": 0.0, "avg_logprob": -0.2402776571420523, "compression_ratio": 1.6162790697674418, "no_speech_prob": 2.5215622372343205e-06}, {"id": 721, "seek": 351488, "start": 3528.0, "end": 3532.48, "text": " Do a correlation and you'll get a good result, right? But if you've got data", "tokens": [1144, 257, 20009, 293, 291, 603, 483, 257, 665, 1874, 11, 558, 30, 583, 498, 291, 600, 658, 1412], "temperature": 0.0, "avg_logprob": -0.2402776571420523, "compression_ratio": 1.6162790697674418, "no_speech_prob": 2.5215622372343205e-06}, {"id": 722, "seek": 351488, "start": 3533.2000000000003, "end": 3535.2000000000003, "text": " which looks like", "tokens": [597, 1542, 411], "temperature": 0.0, "avg_logprob": -0.2402776571420523, "compression_ratio": 1.6162790697674418, "no_speech_prob": 2.5215622372343205e-06}, {"id": 723, "seek": 351488, "start": 3536.6800000000003, "end": 3542.12, "text": " This right and you try and do a correlation it assumes linearity that's not very good, right?", "tokens": [639, 558, 293, 291, 853, 293, 360, 257, 20009, 309, 37808, 8213, 507, 300, 311, 406, 588, 665, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2402776571420523, "compression_ratio": 1.6162790697674418, "no_speech_prob": 2.5215622372343205e-06}, {"id": 724, "seek": 354212, "start": 3542.12, "end": 3547.8399999999997, "text": " So there's a thing called a rank correlation a really simple idea. It's replace every", "tokens": [407, 456, 311, 257, 551, 1219, 257, 6181, 20009, 257, 534, 2199, 1558, 13, 467, 311, 7406, 633], "temperature": 0.0, "avg_logprob": -0.21368468810464733, "compression_ratio": 1.775229357798165, "no_speech_prob": 5.98925623762625e-07}, {"id": 725, "seek": 354212, "start": 3548.96, "end": 3550.16, "text": " point", "tokens": [935], "temperature": 0.0, "avg_logprob": -0.21368468810464733, "compression_ratio": 1.775229357798165, "no_speech_prob": 5.98925623762625e-07}, {"id": 726, "seek": 354212, "start": 3550.16, "end": 3556.74, "text": " By its rank, right? So instead of like so we basically say, okay, this is the smallest so we'll call that one", "tokens": [3146, 1080, 6181, 11, 558, 30, 407, 2602, 295, 411, 370, 321, 1936, 584, 11, 1392, 11, 341, 307, 264, 16998, 370, 321, 603, 818, 300, 472], "temperature": 0.0, "avg_logprob": -0.21368468810464733, "compression_ratio": 1.775229357798165, "no_speech_prob": 5.98925623762625e-07}, {"id": 727, "seek": 354212, "start": 3557.3199999999997, "end": 3560.72, "text": " Two there's the next one three. Here's the next one four", "tokens": [4453, 456, 311, 264, 958, 472, 1045, 13, 1692, 311, 264, 958, 472, 1451], "temperature": 0.0, "avg_logprob": -0.21368468810464733, "compression_ratio": 1.775229357798165, "no_speech_prob": 5.98925623762625e-07}, {"id": 728, "seek": 354212, "start": 3561.48, "end": 3568.6, "text": " Five, right? So you just replace every number by its rank, right? And then you do the same for the y-axis", "tokens": [9436, 11, 558, 30, 407, 291, 445, 7406, 633, 1230, 538, 1080, 6181, 11, 558, 30, 400, 550, 291, 360, 264, 912, 337, 264, 288, 12, 24633], "temperature": 0.0, "avg_logprob": -0.21368468810464733, "compression_ratio": 1.775229357798165, "no_speech_prob": 5.98925623762625e-07}, {"id": 729, "seek": 354212, "start": 3568.7999999999997, "end": 3570.6, "text": " So we'll call that one", "tokens": [407, 321, 603, 818, 300, 472], "temperature": 0.0, "avg_logprob": -0.21368468810464733, "compression_ratio": 1.775229357798165, "no_speech_prob": 5.98925623762625e-07}, {"id": 730, "seek": 357060, "start": 3570.6, "end": 3572.08, "text": " two", "tokens": [732], "temperature": 0.0, "avg_logprob": -0.13954359942143507, "compression_ratio": 1.7652582159624413, "no_speech_prob": 1.3497000281859073e-06}, {"id": 731, "seek": 357060, "start": 3572.08, "end": 3574.2, "text": " Three and so forth, right?", "tokens": [6244, 293, 370, 5220, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13954359942143507, "compression_ratio": 1.7652582159624413, "no_speech_prob": 1.3497000281859073e-06}, {"id": 732, "seek": 357060, "start": 3574.2, "end": 3578.08, "text": " And so then you do it like a new plot where you don't plot the data", "tokens": [400, 370, 550, 291, 360, 309, 411, 257, 777, 7542, 689, 291, 500, 380, 7542, 264, 1412], "temperature": 0.0, "avg_logprob": -0.13954359942143507, "compression_ratio": 1.7652582159624413, "no_speech_prob": 1.3497000281859073e-06}, {"id": 733, "seek": 357060, "start": 3578.24, "end": 3584.64, "text": " But you plot the rank of the data and if you think about it the rank of this data set is going to look", "tokens": [583, 291, 7542, 264, 6181, 295, 264, 1412, 293, 498, 291, 519, 466, 309, 264, 6181, 295, 341, 1412, 992, 307, 516, 281, 574], "temperature": 0.0, "avg_logprob": -0.13954359942143507, "compression_ratio": 1.7652582159624413, "no_speech_prob": 1.3497000281859073e-06}, {"id": 734, "seek": 357060, "start": 3584.96, "end": 3591.4, "text": " An exact line because every time something was greater on the x-axis. It was also greater on the y-axis", "tokens": [1107, 1900, 1622, 570, 633, 565, 746, 390, 5044, 322, 264, 2031, 12, 24633, 13, 467, 390, 611, 5044, 322, 264, 288, 12, 24633], "temperature": 0.0, "avg_logprob": -0.13954359942143507, "compression_ratio": 1.7652582159624413, "no_speech_prob": 1.3497000281859073e-06}, {"id": 735, "seek": 359140, "start": 3591.4, "end": 3599.4, "text": " Y-axis so if we do a correlation on the rank that's called a rank correlation", "tokens": [398, 12, 24633, 370, 498, 321, 360, 257, 20009, 322, 264, 6181, 300, 311, 1219, 257, 6181, 20009], "temperature": 0.0, "avg_logprob": -0.21610234488903637, "compression_ratio": 1.5664739884393064, "no_speech_prob": 2.9023067327216268e-06}, {"id": 736, "seek": 359140, "start": 3601.6800000000003, "end": 3603.96, "text": " Okay, and so", "tokens": [1033, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.21610234488903637, "compression_ratio": 1.5664739884393064, "no_speech_prob": 2.9023067327216268e-06}, {"id": 737, "seek": 359140, "start": 3605.52, "end": 3607.52, "text": " because I want to find the", "tokens": [570, 286, 528, 281, 915, 264], "temperature": 0.0, "avg_logprob": -0.21610234488903637, "compression_ratio": 1.5664739884393064, "no_speech_prob": 2.9023067327216268e-06}, {"id": 738, "seek": 359140, "start": 3607.96, "end": 3612.6800000000003, "text": " Columns that are similar in a way that the random forest would find them similar", "tokens": [4004, 449, 3695, 300, 366, 2531, 294, 257, 636, 300, 264, 4974, 6719, 576, 915, 552, 2531], "temperature": 0.0, "avg_logprob": -0.21610234488903637, "compression_ratio": 1.5664739884393064, "no_speech_prob": 2.9023067327216268e-06}, {"id": 739, "seek": 359140, "start": 3613.2000000000003, "end": 3616.6, "text": " Random forests don't care about linearity. They just care about ordering", "tokens": [37603, 21700, 500, 380, 1127, 466, 8213, 507, 13, 814, 445, 1127, 466, 21739], "temperature": 0.0, "avg_logprob": -0.21610234488903637, "compression_ratio": 1.5664739884393064, "no_speech_prob": 2.9023067327216268e-06}, {"id": 740, "seek": 361660, "start": 3616.6, "end": 3621.56, "text": " So a rank correlation is the the right way to think about that. So", "tokens": [407, 257, 6181, 20009, 307, 264, 264, 558, 636, 281, 519, 466, 300, 13, 407], "temperature": 0.0, "avg_logprob": -0.20264935734296086, "compression_ratio": 1.7095435684647302, "no_speech_prob": 2.0580439468176337e-06}, {"id": 741, "seek": 361660, "start": 3622.4, "end": 3626.48, "text": " Spearman's are is is the name of the most common rank correlation", "tokens": [3550, 289, 1601, 311, 366, 307, 307, 264, 1315, 295, 264, 881, 2689, 6181, 20009], "temperature": 0.0, "avg_logprob": -0.20264935734296086, "compression_ratio": 1.7095435684647302, "no_speech_prob": 2.0580439468176337e-06}, {"id": 742, "seek": 361660, "start": 3626.48, "end": 3631.7999999999997, "text": " But you can literally replace the data with its rank and chuck it at the regular correlation and you'll get basically", "tokens": [583, 291, 393, 3736, 7406, 264, 1412, 365, 1080, 6181, 293, 20870, 309, 412, 264, 3890, 20009, 293, 291, 603, 483, 1936], "temperature": 0.0, "avg_logprob": -0.20264935734296086, "compression_ratio": 1.7095435684647302, "no_speech_prob": 2.0580439468176337e-06}, {"id": 743, "seek": 361660, "start": 3632.04, "end": 3637.44, "text": " The same answer the only difference is in how ties are handled. It's a pretty minor issue", "tokens": [440, 912, 1867, 264, 787, 2649, 307, 294, 577, 14039, 366, 18033, 13, 467, 311, 257, 1238, 6696, 2734], "temperature": 0.0, "avg_logprob": -0.20264935734296086, "compression_ratio": 1.7095435684647302, "no_speech_prob": 2.0580439468176337e-06}, {"id": 744, "seek": 361660, "start": 3639.7999999999997, "end": 3644.66, "text": " Like if you had like a full parabola in that rank correlation, you know", "tokens": [1743, 498, 291, 632, 411, 257, 1577, 45729, 4711, 294, 300, 6181, 20009, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.20264935734296086, "compression_ratio": 1.7095435684647302, "no_speech_prob": 2.0580439468176337e-06}, {"id": 745, "seek": 364466, "start": 3644.66, "end": 3650.3799999999997, "text": " We will not right right. It has to be has to be monotonic. Okay. Yeah. Yeah", "tokens": [492, 486, 406, 558, 558, 13, 467, 575, 281, 312, 575, 281, 312, 1108, 310, 11630, 13, 1033, 13, 865, 13, 865], "temperature": 0.0, "avg_logprob": -0.2276465180632356, "compression_ratio": 1.4555555555555555, "no_speech_prob": 5.0936841944349e-06}, {"id": 746, "seek": 364466, "start": 3656.2999999999997, "end": 3658.2999999999997, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.2276465180632356, "compression_ratio": 1.4555555555555555, "no_speech_prob": 5.0936841944349e-06}, {"id": 747, "seek": 364466, "start": 3658.3399999999997, "end": 3660.3399999999997, "text": " Once I've got a correlation matrix", "tokens": [3443, 286, 600, 658, 257, 20009, 8141], "temperature": 0.0, "avg_logprob": -0.2276465180632356, "compression_ratio": 1.4555555555555555, "no_speech_prob": 5.0936841944349e-06}, {"id": 748, "seek": 364466, "start": 3660.8599999999997, "end": 3664.8999999999996, "text": " there's basically a couple of standard steps you do to turn that into a", "tokens": [456, 311, 1936, 257, 1916, 295, 3832, 4439, 291, 360, 281, 1261, 300, 666, 257], "temperature": 0.0, "avg_logprob": -0.2276465180632356, "compression_ratio": 1.4555555555555555, "no_speech_prob": 5.0936841944349e-06}, {"id": 749, "seek": 364466, "start": 3665.46, "end": 3670.14, "text": " Dendrogram which I have to look up on stack overflow each time I do it", "tokens": [413, 521, 340, 1342, 597, 286, 362, 281, 574, 493, 322, 8630, 37772, 1184, 565, 286, 360, 309], "temperature": 0.0, "avg_logprob": -0.2276465180632356, "compression_ratio": 1.4555555555555555, "no_speech_prob": 5.0936841944349e-06}, {"id": 750, "seek": 367014, "start": 3670.14, "end": 3675.72, "text": " You basically turn it into a distance matrix and then you create something that tells you", "tokens": [509, 1936, 1261, 309, 666, 257, 4560, 8141, 293, 550, 291, 1884, 746, 300, 5112, 291], "temperature": 0.0, "avg_logprob": -0.2269702722996841, "compression_ratio": 1.645320197044335, "no_speech_prob": 1.9637993773358176e-06}, {"id": 751, "seek": 367014, "start": 3675.72, "end": 3680.7599999999998, "text": " You know which things are connected to which other things hierarchically so this kind of us", "tokens": [509, 458, 597, 721, 366, 4582, 281, 597, 661, 721, 35250, 984, 370, 341, 733, 295, 505], "temperature": 0.0, "avg_logprob": -0.2269702722996841, "compression_ratio": 1.645320197044335, "no_speech_prob": 1.9637993773358176e-06}, {"id": 752, "seek": 367014, "start": 3681.4, "end": 3688.8599999999997, "text": " These two and this step here like just three standard steps that you always have to do to create a dendrogram", "tokens": [1981, 732, 293, 341, 1823, 510, 411, 445, 1045, 3832, 4439, 300, 291, 1009, 362, 281, 360, 281, 1884, 257, 274, 521, 340, 1342], "temperature": 0.0, "avg_logprob": -0.2269702722996841, "compression_ratio": 1.645320197044335, "no_speech_prob": 1.9637993773358176e-06}, {"id": 753, "seek": 367014, "start": 3690.0, "end": 3691.48, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2269702722996841, "compression_ratio": 1.645320197044335, "no_speech_prob": 1.9637993773358176e-06}, {"id": 754, "seek": 367014, "start": 3691.48, "end": 3693.48, "text": " So then you can plot it", "tokens": [407, 550, 291, 393, 7542, 309], "temperature": 0.0, "avg_logprob": -0.2269702722996841, "compression_ratio": 1.645320197044335, "no_speech_prob": 1.9637993773358176e-06}, {"id": 755, "seek": 367014, "start": 3693.7999999999997, "end": 3695.8399999999997, "text": " And so alright", "tokens": [400, 370, 5845], "temperature": 0.0, "avg_logprob": -0.2269702722996841, "compression_ratio": 1.645320197044335, "no_speech_prob": 1.9637993773358176e-06}, {"id": 756, "seek": 369584, "start": 3695.84, "end": 3701.1800000000003, "text": " So say all year and say what's going to be measuring basically the same thing at least in terms of rank which is not surprising", "tokens": [407, 584, 439, 1064, 293, 584, 437, 311, 516, 281, 312, 13389, 1936, 264, 912, 551, 412, 1935, 294, 2115, 295, 6181, 597, 307, 406, 8830], "temperature": 0.0, "avg_logprob": -0.17786470006723873, "compression_ratio": 1.8791946308724832, "no_speech_prob": 2.1907744667259976e-06}, {"id": 757, "seek": 369584, "start": 3701.1800000000003, "end": 3707.2000000000003, "text": " Because say elapsed is the number of days since the first day in my data set", "tokens": [1436, 584, 806, 2382, 292, 307, 264, 1230, 295, 1708, 1670, 264, 700, 786, 294, 452, 1412, 992], "temperature": 0.0, "avg_logprob": -0.17786470006723873, "compression_ratio": 1.8791946308724832, "no_speech_prob": 2.1907744667259976e-06}, {"id": 758, "seek": 369584, "start": 3707.92, "end": 3711.76, "text": " So obviously these two are nearly entirely correlated with some ties", "tokens": [407, 2745, 613, 732, 366, 6217, 7696, 38574, 365, 512, 14039], "temperature": 0.0, "avg_logprob": -0.17786470006723873, "compression_ratio": 1.8791946308724832, "no_speech_prob": 2.1907744667259976e-06}, {"id": 759, "seek": 369584, "start": 3712.6800000000003, "end": 3719.28, "text": " Grouse attracts and hydraulics flow and coupler system all seem to be measuring the same thing and this is interesting because remember coupler system", "tokens": [2606, 1316, 37026, 293, 27510, 1167, 3095, 293, 1384, 22732, 1185, 439, 1643, 281, 312, 13389, 264, 912, 551, 293, 341, 307, 1880, 570, 1604, 1384, 22732, 1185], "temperature": 0.0, "avg_logprob": -0.17786470006723873, "compression_ratio": 1.8791946308724832, "no_speech_prob": 2.1907744667259976e-06}, {"id": 760, "seek": 369584, "start": 3719.28, "end": 3725.44, "text": " It said was super important right and so this rather supports our hypothesis that it's nothing to do with whether it's a coupler system", "tokens": [467, 848, 390, 1687, 1021, 558, 293, 370, 341, 2831, 9346, 527, 17291, 300, 309, 311, 1825, 281, 360, 365, 1968, 309, 311, 257, 1384, 22732, 1185], "temperature": 0.0, "avg_logprob": -0.17786470006723873, "compression_ratio": 1.8791946308724832, "no_speech_prob": 2.1907744667259976e-06}, {"id": 761, "seek": 372544, "start": 3725.44, "end": 3728.7200000000003, "text": " But whether it's whatever kind of vehicle it is that has these kind of features", "tokens": [583, 1968, 309, 311, 2035, 733, 295, 5864, 309, 307, 300, 575, 613, 733, 295, 4122], "temperature": 0.0, "avg_logprob": -0.17927956581115723, "compression_ratio": 1.8206106870229009, "no_speech_prob": 2.5612716854084283e-06}, {"id": 762, "seek": 372544, "start": 3731.32, "end": 3737.08, "text": " Product group and product groups desks seem to be measuring the same thing if I base model and if I model desks seem to be", "tokens": [22005, 1594, 293, 1674, 3935, 730, 1694, 1643, 281, 312, 13389, 264, 912, 551, 498, 286, 3096, 2316, 293, 498, 286, 2316, 730, 1694, 1643, 281, 312], "temperature": 0.0, "avg_logprob": -0.17927956581115723, "compression_ratio": 1.8206106870229009, "no_speech_prob": 2.5612716854084283e-06}, {"id": 763, "seek": 372544, "start": 3737.08, "end": 3740.6, "text": " measuring the same thing and so once we get past that", "tokens": [13389, 264, 912, 551, 293, 370, 1564, 321, 483, 1791, 300], "temperature": 0.0, "avg_logprob": -0.17927956581115723, "compression_ratio": 1.8206106870229009, "no_speech_prob": 2.5612716854084283e-06}, {"id": 764, "seek": 372544, "start": 3741.48, "end": 3746.38, "text": " Everything else like suddenly the things are further away, so I'm probably going to not worry about those", "tokens": [5471, 1646, 411, 5800, 264, 721, 366, 3052, 1314, 11, 370, 286, 478, 1391, 516, 281, 406, 3292, 466, 729], "temperature": 0.0, "avg_logprob": -0.17927956581115723, "compression_ratio": 1.8206106870229009, "no_speech_prob": 2.5612716854084283e-06}, {"id": 765, "seek": 372544, "start": 3746.38, "end": 3748.68, "text": " So we're going to look into these one two", "tokens": [407, 321, 434, 516, 281, 574, 666, 613, 472, 732], "temperature": 0.0, "avg_logprob": -0.17927956581115723, "compression_ratio": 1.8206106870229009, "no_speech_prob": 2.5612716854084283e-06}, {"id": 766, "seek": 374868, "start": 3748.68, "end": 3755.04, "text": " Three four groups that are very similar could you pass that over there?", "tokens": [6244, 1451, 3935, 300, 366, 588, 2531, 727, 291, 1320, 300, 670, 456, 30], "temperature": 0.0, "avg_logprob": -0.2756233215332031, "compression_ratio": 1.6941747572815533, "no_speech_prob": 4.7850476221356075e-06}, {"id": 767, "seek": 374868, "start": 3759.8799999999997, "end": 3764.56, "text": " Is it important that graph that the similarity between stick length and", "tokens": [1119, 309, 1021, 300, 4295, 300, 264, 32194, 1296, 2897, 4641, 293], "temperature": 0.0, "avg_logprob": -0.2756233215332031, "compression_ratio": 1.6941747572815533, "no_speech_prob": 4.7850476221356075e-06}, {"id": 768, "seek": 374868, "start": 3764.7999999999997, "end": 3769.6, "text": " Enclosure is higher than with stick length and anything that's higher. Yeah pretty much", "tokens": [2193, 3474, 7641, 307, 2946, 813, 365, 2897, 4641, 293, 1340, 300, 311, 2946, 13, 865, 1238, 709], "temperature": 0.0, "avg_logprob": -0.2756233215332031, "compression_ratio": 1.6941747572815533, "no_speech_prob": 4.7850476221356075e-06}, {"id": 769, "seek": 374868, "start": 3769.6, "end": 3776.7599999999998, "text": " I mean it it's a little hard to interpret but given that stick length and enclosure don't join up until way over here", "tokens": [286, 914, 309, 309, 311, 257, 707, 1152, 281, 7302, 457, 2212, 300, 2897, 4641, 293, 34093, 500, 380, 3917, 493, 1826, 636, 670, 510], "temperature": 0.0, "avg_logprob": -0.2756233215332031, "compression_ratio": 1.6941747572815533, "no_speech_prob": 4.7850476221356075e-06}, {"id": 770, "seek": 377676, "start": 3776.76, "end": 3782.2000000000003, "text": " It would strongly suggest that then that they're a long way away from each other", "tokens": [467, 576, 10613, 3402, 300, 550, 300, 436, 434, 257, 938, 636, 1314, 490, 1184, 661], "temperature": 0.0, "avg_logprob": -0.17176912381098822, "compression_ratio": 1.7386363636363635, "no_speech_prob": 7.527871275669895e-06}, {"id": 771, "seek": 377676, "start": 3782.5600000000004, "end": 3784.6400000000003, "text": " Otherwise you would expect them to have joined up earlier", "tokens": [10328, 291, 576, 2066, 552, 281, 362, 6869, 493, 3071], "temperature": 0.0, "avg_logprob": -0.17176912381098822, "compression_ratio": 1.7386363636363635, "no_speech_prob": 7.527871275669895e-06}, {"id": 772, "seek": 377676, "start": 3784.6400000000003, "end": 3792.26, "text": " I mean it's it's possible to construct like a synthetic data set where you kind of end up joining things that were close to each other", "tokens": [286, 914, 309, 311, 309, 311, 1944, 281, 7690, 411, 257, 23420, 1412, 992, 689, 291, 733, 295, 917, 493, 5549, 721, 300, 645, 1998, 281, 1184, 661], "temperature": 0.0, "avg_logprob": -0.17176912381098822, "compression_ratio": 1.7386363636363635, "no_speech_prob": 7.527871275669895e-06}, {"id": 773, "seek": 377676, "start": 3792.7200000000003, "end": 3794.6400000000003, "text": " Through different paths", "tokens": [8927, 819, 14518], "temperature": 0.0, "avg_logprob": -0.17176912381098822, "compression_ratio": 1.7386363636363635, "no_speech_prob": 7.527871275669895e-06}, {"id": 774, "seek": 377676, "start": 3794.6400000000003, "end": 3800.92, "text": " So you've got to be a bit careful, but I think it's fair to probably assume that stick length or enclosure are probably very different", "tokens": [407, 291, 600, 658, 281, 312, 257, 857, 5026, 11, 457, 286, 519, 309, 311, 3143, 281, 1391, 6552, 300, 2897, 4641, 420, 34093, 366, 1391, 588, 819], "temperature": 0.0, "avg_logprob": -0.17176912381098822, "compression_ratio": 1.7386363636363635, "no_speech_prob": 7.527871275669895e-06}, {"id": 775, "seek": 377676, "start": 3801.44, "end": 3802.84, "text": " So they are very different", "tokens": [407, 436, 366, 588, 819], "temperature": 0.0, "avg_logprob": -0.17176912381098822, "compression_ratio": 1.7386363636363635, "no_speech_prob": 7.527871275669895e-06}, {"id": 776, "seek": 380284, "start": 3802.84, "end": 3809.44, "text": " But would they be more similar than for example stick length and sail day of the year?", "tokens": [583, 576, 436, 312, 544, 2531, 813, 337, 1365, 2897, 4641, 293, 15758, 786, 295, 264, 1064, 30], "temperature": 0.0, "avg_logprob": -0.2145528581407335, "compression_ratio": 1.651063829787234, "no_speech_prob": 1.1300609003228601e-05}, {"id": 777, "seek": 380284, "start": 3811.04, "end": 3813.04, "text": " Which is very top", "tokens": [3013, 307, 588, 1192], "temperature": 0.0, "avg_logprob": -0.2145528581407335, "compression_ratio": 1.651063829787234, "no_speech_prob": 1.1300609003228601e-05}, {"id": 778, "seek": 380284, "start": 3813.04, "end": 3818.04, "text": " No, there's nothing to suggest that here because like the point is to notice where they sit in this tree", "tokens": [883, 11, 456, 311, 1825, 281, 3402, 300, 510, 570, 411, 264, 935, 307, 281, 3449, 689, 436, 1394, 294, 341, 4230], "temperature": 0.0, "avg_logprob": -0.2145528581407335, "compression_ratio": 1.651063829787234, "no_speech_prob": 1.1300609003228601e-05}, {"id": 779, "seek": 380284, "start": 3818.6000000000004, "end": 3822.36, "text": " Right and they both they sit in totally different halves of the tree", "tokens": [1779, 293, 436, 1293, 436, 1394, 294, 3879, 819, 38490, 295, 264, 4230], "temperature": 0.0, "avg_logprob": -0.2145528581407335, "compression_ratio": 1.651063829787234, "no_speech_prob": 1.1300609003228601e-05}, {"id": 780, "seek": 380284, "start": 3823.92, "end": 3829.6200000000003, "text": " But really to actually know that the best way would be to actually look at this BM and our correlation matrix", "tokens": [583, 534, 281, 767, 458, 300, 264, 1151, 636, 576, 312, 281, 767, 574, 412, 341, 15901, 293, 527, 20009, 8141], "temperature": 0.0, "avg_logprob": -0.2145528581407335, "compression_ratio": 1.651063829787234, "no_speech_prob": 1.1300609003228601e-05}, {"id": 781, "seek": 382962, "start": 3829.62, "end": 3836.2999999999997, "text": " Right if you just want to know how similar is this thing to this thing this BM and our correlation matrix tells you that can you?", "tokens": [1779, 498, 291, 445, 528, 281, 458, 577, 2531, 307, 341, 551, 281, 341, 551, 341, 15901, 293, 527, 20009, 8141, 5112, 291, 300, 393, 291, 30], "temperature": 0.0, "avg_logprob": -0.23355598802919741, "compression_ratio": 1.8137651821862348, "no_speech_prob": 2.64256345872127e-06}, {"id": 782, "seek": 382962, "start": 3836.2999999999997, "end": 3838.2999999999997, "text": " Pass that over there", "tokens": [10319, 300, 670, 456], "temperature": 0.0, "avg_logprob": -0.23355598802919741, "compression_ratio": 1.8137651821862348, "no_speech_prob": 2.64256345872127e-06}, {"id": 783, "seek": 382962, "start": 3840.98, "end": 3843.62, "text": " So today's we are passing the data frame right", "tokens": [407, 965, 311, 321, 366, 8437, 264, 1412, 3920, 558], "temperature": 0.0, "avg_logprob": -0.23355598802919741, "compression_ratio": 1.8137651821862348, "no_speech_prob": 2.64256345872127e-06}, {"id": 784, "seek": 382962, "start": 3845.02, "end": 3850.94, "text": " Say again we are passing the data frame or I'll be passing the model to it. This is just a data frame", "tokens": [6463, 797, 321, 366, 8437, 264, 1412, 3920, 420, 286, 603, 312, 8437, 264, 2316, 281, 309, 13, 639, 307, 445, 257, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.23355598802919741, "compression_ratio": 1.8137651821862348, "no_speech_prob": 2.64256345872127e-06}, {"id": 785, "seek": 382962, "start": 3850.94, "end": 3853.66, "text": " So we're passing in DF keep so that's the data frame", "tokens": [407, 321, 434, 8437, 294, 48336, 1066, 370, 300, 311, 264, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.23355598802919741, "compression_ratio": 1.8137651821862348, "no_speech_prob": 2.64256345872127e-06}, {"id": 786, "seek": 385366, "start": 3853.66, "end": 3858.8199999999997, "text": " Containing the whatever it was 30 or so features that our random forest thought was interesting", "tokens": [4839, 3686, 264, 2035, 309, 390, 2217, 420, 370, 4122, 300, 527, 4974, 6719, 1194, 390, 1880], "temperature": 0.0, "avg_logprob": -0.16411725405989022, "compression_ratio": 1.7345132743362832, "no_speech_prob": 8.398003956244793e-06}, {"id": 787, "seek": 385366, "start": 3860.3599999999997, "end": 3866.72, "text": " So there's no random forest being used here the measure the distance measure is being done entirely on rank correlation", "tokens": [407, 456, 311, 572, 4974, 6719, 885, 1143, 510, 264, 3481, 264, 4560, 3481, 307, 885, 1096, 7696, 322, 6181, 20009], "temperature": 0.0, "avg_logprob": -0.16411725405989022, "compression_ratio": 1.7345132743362832, "no_speech_prob": 8.398003956244793e-06}, {"id": 788, "seek": 385366, "start": 3869.44, "end": 3872.3199999999997, "text": " So what I then do is I take these these groups", "tokens": [407, 437, 286, 550, 360, 307, 286, 747, 613, 613, 3935], "temperature": 0.0, "avg_logprob": -0.16411725405989022, "compression_ratio": 1.7345132743362832, "no_speech_prob": 8.398003956244793e-06}, {"id": 789, "seek": 385366, "start": 3872.7999999999997, "end": 3879.16, "text": " Right and I create a little function that I call get out of band score right which is it does a random forest", "tokens": [1779, 293, 286, 1884, 257, 707, 2445, 300, 286, 818, 483, 484, 295, 4116, 6175, 558, 597, 307, 309, 775, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.16411725405989022, "compression_ratio": 1.7345132743362832, "no_speech_prob": 8.398003956244793e-06}, {"id": 790, "seek": 387916, "start": 3879.16, "end": 3882.24, "text": " for some data frame I", "tokens": [337, 512, 1412, 3920, 286], "temperature": 0.0, "avg_logprob": -0.21951452054475484, "compression_ratio": 1.572192513368984, "no_speech_prob": 6.893599788782012e-07}, {"id": 791, "seek": 387916, "start": 3884.64, "end": 3889.14, "text": " Make sure that I've taken that data frame and split it into a training and validation set", "tokens": [4387, 988, 300, 286, 600, 2726, 300, 1412, 3920, 293, 7472, 309, 666, 257, 3097, 293, 24071, 992], "temperature": 0.0, "avg_logprob": -0.21951452054475484, "compression_ratio": 1.572192513368984, "no_speech_prob": 6.893599788782012e-07}, {"id": 792, "seek": 387916, "start": 3889.72, "end": 3897.48, "text": " And then I call fit and return the OOB score right so basically what I'm going to do is I'm going to try removing", "tokens": [400, 550, 286, 818, 3318, 293, 2736, 264, 422, 46, 33, 6175, 558, 370, 1936, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 853, 12720], "temperature": 0.0, "avg_logprob": -0.21951452054475484, "compression_ratio": 1.572192513368984, "no_speech_prob": 6.893599788782012e-07}, {"id": 793, "seek": 387916, "start": 3898.0, "end": 3902.8799999999997, "text": " Each one of these one two three four five six seven eight nine or so", "tokens": [6947, 472, 295, 613, 472, 732, 1045, 1451, 1732, 2309, 3407, 3180, 4949, 420, 370], "temperature": 0.0, "avg_logprob": -0.21951452054475484, "compression_ratio": 1.572192513368984, "no_speech_prob": 6.893599788782012e-07}, {"id": 794, "seek": 390288, "start": 3902.88, "end": 3910.82, "text": " Variables one at a time and see which ones I can remove and it doesn't make the OOB score get worse", "tokens": [32511, 2965, 472, 412, 257, 565, 293, 536, 597, 2306, 286, 393, 4159, 293, 309, 1177, 380, 652, 264, 422, 46, 33, 6175, 483, 5324], "temperature": 0.0, "avg_logprob": -0.1293500406401498, "compression_ratio": 1.6984732824427482, "no_speech_prob": 3.50083973899018e-06}, {"id": 795, "seek": 390288, "start": 3911.7200000000003, "end": 3914.88, "text": " And each time I run this I get slightly different results", "tokens": [400, 1184, 565, 286, 1190, 341, 286, 483, 4748, 819, 3542], "temperature": 0.0, "avg_logprob": -0.1293500406401498, "compression_ratio": 1.6984732824427482, "no_speech_prob": 3.50083973899018e-06}, {"id": 796, "seek": 390288, "start": 3914.88, "end": 3918.12, "text": " So actually it looks like last time I had seven things not not eight things", "tokens": [407, 767, 309, 1542, 411, 1036, 565, 286, 632, 3407, 721, 406, 406, 3180, 721], "temperature": 0.0, "avg_logprob": -0.1293500406401498, "compression_ratio": 1.6984732824427482, "no_speech_prob": 3.50083973899018e-06}, {"id": 797, "seek": 390288, "start": 3918.3, "end": 3923.28, "text": " So you can see I just do a loop through each of the things that I'm thinking like maybe I could get rid of this", "tokens": [407, 291, 393, 536, 286, 445, 360, 257, 6367, 807, 1184, 295, 264, 721, 300, 286, 478, 1953, 411, 1310, 286, 727, 483, 3973, 295, 341], "temperature": 0.0, "avg_logprob": -0.1293500406401498, "compression_ratio": 1.6984732824427482, "no_speech_prob": 3.50083973899018e-06}, {"id": 798, "seek": 390288, "start": 3923.28, "end": 3930.76, "text": " Because it's redundant and I print out the column name and the OOB score of a model that is trained", "tokens": [1436, 309, 311, 40997, 293, 286, 4482, 484, 264, 7738, 1315, 293, 264, 422, 46, 33, 6175, 295, 257, 2316, 300, 307, 8895], "temperature": 0.0, "avg_logprob": -0.1293500406401498, "compression_ratio": 1.6984732824427482, "no_speech_prob": 3.50083973899018e-06}, {"id": 799, "seek": 393076, "start": 3930.76, "end": 3932.76, "text": " after dropping", "tokens": [934, 13601], "temperature": 0.0, "avg_logprob": -0.2257432125984354, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.058045083686011e-06}, {"id": 800, "seek": 393076, "start": 3932.84, "end": 3934.84, "text": " that one column", "tokens": [300, 472, 7738], "temperature": 0.0, "avg_logprob": -0.2257432125984354, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.058045083686011e-06}, {"id": 801, "seek": 393076, "start": 3935.2000000000003, "end": 3940.32, "text": " Okay, so the OOB score on my whole data frame is point eight nine and", "tokens": [1033, 11, 370, 264, 422, 46, 33, 6175, 322, 452, 1379, 1412, 3920, 307, 935, 3180, 4949, 293], "temperature": 0.0, "avg_logprob": -0.2257432125984354, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.058045083686011e-06}, {"id": 802, "seek": 393076, "start": 3941.32, "end": 3943.32, "text": " then after dropping", "tokens": [550, 934, 13601], "temperature": 0.0, "avg_logprob": -0.2257432125984354, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.058045083686011e-06}, {"id": 803, "seek": 393076, "start": 3943.5600000000004, "end": 3945.5600000000004, "text": " each one of these things", "tokens": [1184, 472, 295, 613, 721], "temperature": 0.0, "avg_logprob": -0.2257432125984354, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.058045083686011e-06}, {"id": 804, "seek": 393076, "start": 3947.5600000000004, "end": 3951.7200000000003, "text": " They're basically none of them get much worse say all elapsed is", "tokens": [814, 434, 1936, 6022, 295, 552, 483, 709, 5324, 584, 439, 806, 2382, 292, 307], "temperature": 0.0, "avg_logprob": -0.2257432125984354, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.058045083686011e-06}, {"id": 805, "seek": 393076, "start": 3952.2000000000003, "end": 3956.44, "text": " Getting quite a bit worse than say all year, but like it looks like pretty much everything else", "tokens": [13674, 1596, 257, 857, 5324, 813, 584, 439, 1064, 11, 457, 411, 309, 1542, 411, 1238, 709, 1203, 1646], "temperature": 0.0, "avg_logprob": -0.2257432125984354, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.058045083686011e-06}, {"id": 806, "seek": 393076, "start": 3956.44, "end": 3959.6000000000004, "text": " I can drop with like only like a third decimal place", "tokens": [286, 393, 3270, 365, 411, 787, 411, 257, 2636, 26601, 1081], "temperature": 0.0, "avg_logprob": -0.2257432125984354, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.058045083686011e-06}, {"id": 807, "seek": 395960, "start": 3959.6, "end": 3961.6, "text": " problem", "tokens": [1154], "temperature": 0.0, "avg_logprob": -0.19101513853860558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 3.1875574677542318e-06}, {"id": 808, "seek": 395960, "start": 3961.6, "end": 3967.96, "text": " So obviously though you've got to remember the Dendrogram like let's take fi model desk and fi based model", "tokens": [407, 2745, 1673, 291, 600, 658, 281, 1604, 264, 413, 521, 340, 1342, 411, 718, 311, 747, 15848, 2316, 10026, 293, 15848, 2361, 2316], "temperature": 0.0, "avg_logprob": -0.19101513853860558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 3.1875574677542318e-06}, {"id": 809, "seek": 395960, "start": 3968.7599999999998, "end": 3974.08, "text": " Right they're very similar to each other right so what this says isn't that I can get rid of both of them", "tokens": [1779, 436, 434, 588, 2531, 281, 1184, 661, 558, 370, 437, 341, 1619, 1943, 380, 300, 286, 393, 483, 3973, 295, 1293, 295, 552], "temperature": 0.0, "avg_logprob": -0.19101513853860558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 3.1875574677542318e-06}, {"id": 810, "seek": 395960, "start": 3974.3199999999997, "end": 3978.7, "text": " Right I can get rid of one of them because they're basically measuring the same thing", "tokens": [1779, 286, 393, 483, 3973, 295, 472, 295, 552, 570, 436, 434, 1936, 13389, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.19101513853860558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 3.1875574677542318e-06}, {"id": 811, "seek": 395960, "start": 3979.16, "end": 3985.52, "text": " Okay, so so then I try it I say okay. Let's try getting rid of one from each group say all year", "tokens": [1033, 11, 370, 370, 550, 286, 853, 309, 286, 584, 1392, 13, 961, 311, 853, 1242, 3973, 295, 472, 490, 1184, 1594, 584, 439, 1064], "temperature": 0.0, "avg_logprob": -0.19101513853860558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 3.1875574677542318e-06}, {"id": 812, "seek": 395960, "start": 3986.04, "end": 3988.04, "text": " fi based model and", "tokens": [15848, 2361, 2316, 293], "temperature": 0.0, "avg_logprob": -0.19101513853860558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 3.1875574677542318e-06}, {"id": 813, "seek": 398804, "start": 3988.04, "end": 3989.96, "text": " grouser tracks", "tokens": [677, 563, 260, 10218], "temperature": 0.0, "avg_logprob": -0.2242228485817133, "compression_ratio": 1.575, "no_speech_prob": 6.681509603367886e-07}, {"id": 814, "seek": 398804, "start": 3989.96, "end": 3996.0, "text": " Okay, and like let's now have a look. It's like okay. I've gone from point eight nine oh two point eight eight eight", "tokens": [1033, 11, 293, 411, 718, 311, 586, 362, 257, 574, 13, 467, 311, 411, 1392, 13, 286, 600, 2780, 490, 935, 3180, 4949, 1954, 732, 935, 3180, 3180, 3180], "temperature": 0.0, "avg_logprob": -0.2242228485817133, "compression_ratio": 1.575, "no_speech_prob": 6.681509603367886e-07}, {"id": 815, "seek": 398804, "start": 3996.0, "end": 4001.92, "text": " It's like again so close as to be meaningless so that sounds good simpler is better", "tokens": [467, 311, 411, 797, 370, 1998, 382, 281, 312, 33232, 370, 300, 3263, 665, 18587, 307, 1101], "temperature": 0.0, "avg_logprob": -0.2242228485817133, "compression_ratio": 1.575, "no_speech_prob": 6.681509603367886e-07}, {"id": 816, "seek": 398804, "start": 4003.2, "end": 4006.04, "text": " So I'm now going to drop those columns", "tokens": [407, 286, 478, 586, 516, 281, 3270, 729, 13766], "temperature": 0.0, "avg_logprob": -0.2242228485817133, "compression_ratio": 1.575, "no_speech_prob": 6.681509603367886e-07}, {"id": 817, "seek": 398804, "start": 4006.88, "end": 4008.88, "text": " from my data frame", "tokens": [490, 452, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.2242228485817133, "compression_ratio": 1.575, "no_speech_prob": 6.681509603367886e-07}, {"id": 818, "seek": 398804, "start": 4010.4, "end": 4013.4, "text": " And then I can try running the full model", "tokens": [400, 550, 286, 393, 853, 2614, 264, 1577, 2316], "temperature": 0.0, "avg_logprob": -0.2242228485817133, "compression_ratio": 1.575, "no_speech_prob": 6.681509603367886e-07}, {"id": 819, "seek": 401340, "start": 4013.4, "end": 4017.6800000000003, "text": " Again, and I can see you know so reset RS samples", "tokens": [3764, 11, 293, 286, 393, 536, 291, 458, 370, 14322, 25855, 10938], "temperature": 0.0, "avg_logprob": -0.2111530539430218, "compression_ratio": 1.5161290322580645, "no_speech_prob": 1.9333554064360214e-06}, {"id": 820, "seek": 401340, "start": 4018.4, "end": 4022.2000000000003, "text": " Means I'm using my whole data frame my whole bootstrap sample", "tokens": [40290, 286, 478, 1228, 452, 1379, 1412, 3920, 452, 1379, 11450, 372, 4007, 6889], "temperature": 0.0, "avg_logprob": -0.2111530539430218, "compression_ratio": 1.5161290322580645, "no_speech_prob": 1.9333554064360214e-06}, {"id": 821, "seek": 401340, "start": 4023.1600000000003, "end": 4029.92, "text": " Use 40 estimators, and I've got point nine oh seven okay, so I've now got a", "tokens": [8278, 3356, 8017, 3391, 11, 293, 286, 600, 658, 935, 4949, 1954, 3407, 1392, 11, 370, 286, 600, 586, 658, 257], "temperature": 0.0, "avg_logprob": -0.2111530539430218, "compression_ratio": 1.5161290322580645, "no_speech_prob": 1.9333554064360214e-06}, {"id": 822, "seek": 401340, "start": 4030.6800000000003, "end": 4036.1600000000003, "text": " Model which is smaller and simpler, and I'm getting a good score for", "tokens": [17105, 597, 307, 4356, 293, 18587, 11, 293, 286, 478, 1242, 257, 665, 6175, 337], "temperature": 0.0, "avg_logprob": -0.2111530539430218, "compression_ratio": 1.5161290322580645, "no_speech_prob": 1.9333554064360214e-06}, {"id": 823, "seek": 401340, "start": 4037.88, "end": 4039.88, "text": " So at this point I've now", "tokens": [407, 412, 341, 935, 286, 600, 586], "temperature": 0.0, "avg_logprob": -0.2111530539430218, "compression_ratio": 1.5161290322580645, "no_speech_prob": 1.9333554064360214e-06}, {"id": 824, "seek": 403988, "start": 4039.88, "end": 4047.76, "text": " Got rid of as many columns as I feel I comfortably can ones that either didn't have a good feature importance or were", "tokens": [5803, 3973, 295, 382, 867, 13766, 382, 286, 841, 286, 25101, 393, 2306, 300, 2139, 994, 380, 362, 257, 665, 4111, 7379, 420, 645], "temperature": 0.0, "avg_logprob": -0.15683883731648074, "compression_ratio": 1.6972477064220184, "no_speech_prob": 8.579224299865018e-07}, {"id": 825, "seek": 403988, "start": 4048.12, "end": 4053.56, "text": " Highly related to other variables and the model didn't get worse significantly with that when I removed them", "tokens": [5229, 356, 4077, 281, 661, 9102, 293, 264, 2316, 994, 380, 483, 5324, 10591, 365, 300, 562, 286, 7261, 552], "temperature": 0.0, "avg_logprob": -0.15683883731648074, "compression_ratio": 1.6972477064220184, "no_speech_prob": 8.579224299865018e-07}, {"id": 826, "seek": 403988, "start": 4054.36, "end": 4060.84, "text": " So now I'm at the point where I want to try and really understand my data better by taking advantage of the model", "tokens": [407, 586, 286, 478, 412, 264, 935, 689, 286, 528, 281, 853, 293, 534, 1223, 452, 1412, 1101, 538, 1940, 5002, 295, 264, 2316], "temperature": 0.0, "avg_logprob": -0.15683883731648074, "compression_ratio": 1.6972477064220184, "no_speech_prob": 8.579224299865018e-07}, {"id": 827, "seek": 403988, "start": 4061.56, "end": 4064.12, "text": " And they're going to use something called partial dependence and again", "tokens": [400, 436, 434, 516, 281, 764, 746, 1219, 14641, 31704, 293, 797], "temperature": 0.0, "avg_logprob": -0.15683883731648074, "compression_ratio": 1.6972477064220184, "no_speech_prob": 8.579224299865018e-07}, {"id": 828, "seek": 403988, "start": 4064.12, "end": 4069.6, "text": " This is something that you could like using the Kaggle kernel and lots of people are going to appreciate this because almost nobody knows about", "tokens": [639, 307, 746, 300, 291, 727, 411, 1228, 264, 48751, 22631, 28256, 293, 3195, 295, 561, 366, 516, 281, 4449, 341, 570, 1920, 5079, 3255, 466], "temperature": 0.0, "avg_logprob": -0.15683883731648074, "compression_ratio": 1.6972477064220184, "no_speech_prob": 8.579224299865018e-07}, {"id": 829, "seek": 406960, "start": 4069.6, "end": 4072.88, "text": " Partial dependence, and it's a very very powerful technique", "tokens": [4100, 831, 31704, 11, 293, 309, 311, 257, 588, 588, 4005, 6532], "temperature": 0.0, "avg_logprob": -0.1684937005514627, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.406095063633984e-06}, {"id": 830, "seek": 406960, "start": 4073.88, "end": 4078.24, "text": " What we're going to do is we're going to find out for the features that are important", "tokens": [708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 915, 484, 337, 264, 4122, 300, 366, 1021], "temperature": 0.0, "avg_logprob": -0.1684937005514627, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.406095063633984e-06}, {"id": 831, "seek": 406960, "start": 4079.04, "end": 4081.8399999999997, "text": " How do they relate to the dependent variable?", "tokens": [1012, 360, 436, 10961, 281, 264, 12334, 7006, 30], "temperature": 0.0, "avg_logprob": -0.1684937005514627, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.406095063633984e-06}, {"id": 832, "seek": 406960, "start": 4082.44, "end": 4087.7, "text": " Right so let's have a look right so let's again since we're doing interpretation", "tokens": [1779, 370, 718, 311, 362, 257, 574, 558, 370, 718, 311, 797, 1670, 321, 434, 884, 14174], "temperature": 0.0, "avg_logprob": -0.1684937005514627, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.406095063633984e-06}, {"id": 833, "seek": 406960, "start": 4087.7, "end": 4090.72, "text": " We'll set set our samples to 50,000 to run things quickly", "tokens": [492, 603, 992, 992, 527, 10938, 281, 2625, 11, 1360, 281, 1190, 721, 2661], "temperature": 0.0, "avg_logprob": -0.1684937005514627, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.406095063633984e-06}, {"id": 834, "seek": 406960, "start": 4093.48, "end": 4095.48, "text": " We'll take our data frame", "tokens": [492, 603, 747, 527, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.1684937005514627, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.406095063633984e-06}, {"id": 835, "seek": 409548, "start": 4095.48, "end": 4099.88, "text": " We'll get our feature importance and notice that we're using", "tokens": [492, 603, 483, 527, 4111, 7379, 293, 3449, 300, 321, 434, 1228], "temperature": 0.0, "avg_logprob": -0.15786798183734602, "compression_ratio": 1.6273584905660377, "no_speech_prob": 8.851545203469868e-07}, {"id": 836, "seek": 409548, "start": 4101.28, "end": 4107.88, "text": " Max and cat because I'm actually pretty interested in terms of for interpretation and seeing the individual levels", "tokens": [7402, 293, 3857, 570, 286, 478, 767, 1238, 3102, 294, 2115, 295, 337, 14174, 293, 2577, 264, 2609, 4358], "temperature": 0.0, "avg_logprob": -0.15786798183734602, "compression_ratio": 1.6273584905660377, "no_speech_prob": 8.851545203469868e-07}, {"id": 837, "seek": 409548, "start": 4109.0, "end": 4113.94, "text": " And so here's the top ten and so let's try and learn more about those top ten", "tokens": [400, 370, 510, 311, 264, 1192, 2064, 293, 370, 718, 311, 853, 293, 1466, 544, 466, 729, 1192, 2064], "temperature": 0.0, "avg_logprob": -0.15786798183734602, "compression_ratio": 1.6273584905660377, "no_speech_prob": 8.851545203469868e-07}, {"id": 838, "seek": 411394, "start": 4113.94, "end": 4123.94, "text": " So year made is the second most important so one obvious thing we could do would be to plot year", "tokens": [407, 1064, 1027, 307, 264, 1150, 881, 1021, 370, 472, 6322, 551, 321, 727, 360, 576, 312, 281, 7542, 1064], "temperature": 0.0, "avg_logprob": -0.20479411295015518, "compression_ratio": 1.53, "no_speech_prob": 1.2679245173785603e-06}, {"id": 839, "seek": 411394, "start": 4126.419999999999, "end": 4128.339999999999, "text": " Made", "tokens": [18330], "temperature": 0.0, "avg_logprob": -0.20479411295015518, "compression_ratio": 1.53, "no_speech_prob": 1.2679245173785603e-06}, {"id": 840, "seek": 411394, "start": 4128.339999999999, "end": 4133.9, "text": " Against sale elapsed because as we've talked about already like it just seems to make sense. They're both important", "tokens": [29995, 8680, 806, 2382, 292, 570, 382, 321, 600, 2825, 466, 1217, 411, 309, 445, 2544, 281, 652, 2020, 13, 814, 434, 1293, 1021], "temperature": 0.0, "avg_logprob": -0.20479411295015518, "compression_ratio": 1.53, "no_speech_prob": 1.2679245173785603e-06}, {"id": 841, "seek": 411394, "start": 4134.46, "end": 4141.54, "text": " But it seems very likely that they kind of combine together to find like how old was the", "tokens": [583, 309, 2544, 588, 3700, 300, 436, 733, 295, 10432, 1214, 281, 915, 411, 577, 1331, 390, 264], "temperature": 0.0, "avg_logprob": -0.20479411295015518, "compression_ratio": 1.53, "no_speech_prob": 1.2679245173785603e-06}, {"id": 842, "seek": 414154, "start": 4141.54, "end": 4148.86, "text": " The product when it was sold so we could try plotting year made against sale elapsed to see how they relate to each other and", "tokens": [440, 1674, 562, 309, 390, 3718, 370, 321, 727, 853, 41178, 1064, 1027, 1970, 8680, 806, 2382, 292, 281, 536, 577, 436, 10961, 281, 1184, 661, 293], "temperature": 0.0, "avg_logprob": -0.13801226995687568, "compression_ratio": 1.6618181818181819, "no_speech_prob": 1.4144727629172849e-06}, {"id": 843, "seek": 414154, "start": 4148.98, "end": 4150.38, "text": " when we do", "tokens": [562, 321, 360], "temperature": 0.0, "avg_logprob": -0.13801226995687568, "compression_ratio": 1.6618181818181819, "no_speech_prob": 1.4144727629172849e-06}, {"id": 844, "seek": 414154, "start": 4150.38, "end": 4154.5, "text": " We get this very ugly graph, and it shows us that year made", "tokens": [492, 483, 341, 588, 12246, 4295, 11, 293, 309, 3110, 505, 300, 1064, 1027], "temperature": 0.0, "avg_logprob": -0.13801226995687568, "compression_ratio": 1.6618181818181819, "no_speech_prob": 1.4144727629172849e-06}, {"id": 845, "seek": 414154, "start": 4155.1, "end": 4157.74, "text": " Actually has a whole bunch that are a thousand", "tokens": [5135, 575, 257, 1379, 3840, 300, 366, 257, 4714], "temperature": 0.0, "avg_logprob": -0.13801226995687568, "compression_ratio": 1.6618181818181819, "no_speech_prob": 1.4144727629172849e-06}, {"id": 846, "seek": 414154, "start": 4158.26, "end": 4163.72, "text": " Right so clearly you know this is where I would tend to go back to the client or whatever and say okay", "tokens": [1779, 370, 4448, 291, 458, 341, 307, 689, 286, 576, 3928, 281, 352, 646, 281, 264, 6423, 420, 2035, 293, 584, 1392], "temperature": 0.0, "avg_logprob": -0.13801226995687568, "compression_ratio": 1.6618181818181819, "no_speech_prob": 1.4144727629172849e-06}, {"id": 847, "seek": 414154, "start": 4163.72, "end": 4168.68, "text": " I'm guessing that these bulldozers weren't actually made in the year 1000, and they would presumably say to me", "tokens": [286, 478, 17939, 300, 613, 4693, 2595, 41698, 4999, 380, 767, 1027, 294, 264, 1064, 9714, 11, 293, 436, 576, 26742, 584, 281, 385], "temperature": 0.0, "avg_logprob": -0.13801226995687568, "compression_ratio": 1.6618181818181819, "no_speech_prob": 1.4144727629172849e-06}, {"id": 848, "seek": 416868, "start": 4168.68, "end": 4174.42, "text": " Oh, yes, they're ones where we don't know when it was made you know maybe before 1986", "tokens": [876, 11, 2086, 11, 436, 434, 2306, 689, 321, 500, 380, 458, 562, 309, 390, 1027, 291, 458, 1310, 949, 27895], "temperature": 0.0, "avg_logprob": -0.17721944336497455, "compression_ratio": 1.6420664206642066, "no_speech_prob": 3.3931264624698088e-06}, {"id": 849, "seek": 416868, "start": 4174.42, "end": 4178.6, "text": " We didn't track that or maybe the things that are sold in Illinois", "tokens": [492, 994, 380, 2837, 300, 420, 1310, 264, 721, 300, 366, 3718, 294, 17508], "temperature": 0.0, "avg_logprob": -0.17721944336497455, "compression_ratio": 1.6420664206642066, "no_speech_prob": 3.3931264624698088e-06}, {"id": 850, "seek": 416868, "start": 4178.6, "end": 4182.8, "text": " You don't have that data provided or or whatever they'll tell us some reason so", "tokens": [509, 500, 380, 362, 300, 1412, 5649, 420, 420, 2035, 436, 603, 980, 505, 512, 1778, 370], "temperature": 0.0, "avg_logprob": -0.17721944336497455, "compression_ratio": 1.6420664206642066, "no_speech_prob": 3.3931264624698088e-06}, {"id": 851, "seek": 416868, "start": 4185.9800000000005, "end": 4187.9800000000005, "text": " In order to", "tokens": [682, 1668, 281], "temperature": 0.0, "avg_logprob": -0.17721944336497455, "compression_ratio": 1.6420664206642066, "no_speech_prob": 3.3931264624698088e-06}, {"id": 852, "seek": 416868, "start": 4187.9800000000005, "end": 4193.280000000001, "text": " Understand this plot better. I'm just going to remove them from this interpretation section of the analysis", "tokens": [26093, 341, 7542, 1101, 13, 286, 478, 445, 516, 281, 4159, 552, 490, 341, 14174, 3541, 295, 264, 5215], "temperature": 0.0, "avg_logprob": -0.17721944336497455, "compression_ratio": 1.6420664206642066, "no_speech_prob": 3.3931264624698088e-06}, {"id": 853, "seek": 419328, "start": 4193.28, "end": 4199.3, "text": " I'm just going to say okay. Let's just grab things where you made is greater than 1930 okay?", "tokens": [286, 478, 445, 516, 281, 584, 1392, 13, 961, 311, 445, 4444, 721, 689, 291, 1027, 307, 5044, 813, 22350, 1392, 30], "temperature": 0.0, "avg_logprob": -0.23213172744918656, "compression_ratio": 1.6026200873362446, "no_speech_prob": 5.539163794310298e-07}, {"id": 854, "seek": 419328, "start": 4199.3, "end": 4206.9, "text": " So let's now look at the relationship between year made and sale price, and there's a really great", "tokens": [407, 718, 311, 586, 574, 412, 264, 2480, 1296, 1064, 1027, 293, 8680, 3218, 11, 293, 456, 311, 257, 534, 869], "temperature": 0.0, "avg_logprob": -0.23213172744918656, "compression_ratio": 1.6026200873362446, "no_speech_prob": 5.539163794310298e-07}, {"id": 855, "seek": 419328, "start": 4208.34, "end": 4210.34, "text": " Package called gg plot", "tokens": [18466, 609, 1219, 290, 70, 7542], "temperature": 0.0, "avg_logprob": -0.23213172744918656, "compression_ratio": 1.6026200873362446, "no_speech_prob": 5.539163794310298e-07}, {"id": 856, "seek": 419328, "start": 4210.42, "end": 4217.599999999999, "text": " Gigi plot originally was an R package gg stands for the grammar of graphics and the grammar of graphics is like this", "tokens": [460, 19789, 7542, 7993, 390, 364, 497, 7372, 290, 70, 7382, 337, 264, 22317, 295, 11837, 293, 264, 22317, 295, 11837, 307, 411, 341], "temperature": 0.0, "avg_logprob": -0.23213172744918656, "compression_ratio": 1.6026200873362446, "no_speech_prob": 5.539163794310298e-07}, {"id": 857, "seek": 419328, "start": 4218.0599999999995, "end": 4220.74, "text": " very powerful way of thinking about", "tokens": [588, 4005, 636, 295, 1953, 466], "temperature": 0.0, "avg_logprob": -0.23213172744918656, "compression_ratio": 1.6026200873362446, "no_speech_prob": 5.539163794310298e-07}, {"id": 858, "seek": 422074, "start": 4220.74, "end": 4222.74, "text": " how to produce", "tokens": [577, 281, 5258], "temperature": 0.0, "avg_logprob": -0.198073503447742, "compression_ratio": 1.4660633484162895, "no_speech_prob": 4.356849331088597e-06}, {"id": 859, "seek": 422074, "start": 4223.98, "end": 4228.46, "text": " Charts in a very flexible way. I'm not going to be talking about it much in this class", "tokens": [761, 11814, 294, 257, 588, 11358, 636, 13, 286, 478, 406, 516, 281, 312, 1417, 466, 309, 709, 294, 341, 1508], "temperature": 0.0, "avg_logprob": -0.198073503447742, "compression_ratio": 1.4660633484162895, "no_speech_prob": 4.356849331088597e-06}, {"id": 860, "seek": 422074, "start": 4228.46, "end": 4230.46, "text": " There's lots of information available online", "tokens": [821, 311, 3195, 295, 1589, 2435, 2950], "temperature": 0.0, "avg_logprob": -0.198073503447742, "compression_ratio": 1.4660633484162895, "no_speech_prob": 4.356849331088597e-06}, {"id": 861, "seek": 422074, "start": 4230.98, "end": 4235.74, "text": " But I definitely recommend it as a great package to use gg plot", "tokens": [583, 286, 2138, 2748, 309, 382, 257, 869, 7372, 281, 764, 290, 70, 7542], "temperature": 0.0, "avg_logprob": -0.198073503447742, "compression_ratio": 1.4660633484162895, "no_speech_prob": 4.356849331088597e-06}, {"id": 862, "seek": 422074, "start": 4236.42, "end": 4240.54, "text": " Which you can pip install it's part of the fast AI environment already", "tokens": [3013, 291, 393, 8489, 3625, 309, 311, 644, 295, 264, 2370, 7318, 2823, 1217], "temperature": 0.0, "avg_logprob": -0.198073503447742, "compression_ratio": 1.4660633484162895, "no_speech_prob": 4.356849331088597e-06}, {"id": 863, "seek": 422074, "start": 4242.62, "end": 4246.139999999999, "text": " Gigi plot in Python has basically the same", "tokens": [460, 19789, 7542, 294, 15329, 575, 1936, 264, 912], "temperature": 0.0, "avg_logprob": -0.198073503447742, "compression_ratio": 1.4660633484162895, "no_speech_prob": 4.356849331088597e-06}, {"id": 864, "seek": 424614, "start": 4246.14, "end": 4250.9400000000005, "text": " Parameters and API is the R version the R version is much better documented", "tokens": [34882, 6202, 293, 9362, 307, 264, 497, 3037, 264, 497, 3037, 307, 709, 1101, 23007], "temperature": 0.0, "avg_logprob": -0.17496423721313475, "compression_ratio": 1.641350210970464, "no_speech_prob": 2.33206606026215e-06}, {"id": 865, "seek": 424614, "start": 4250.9400000000005, "end": 4258.06, "text": " So you should read its documentation to learn how to use it, but basically you say okay. I want to create a plot of", "tokens": [407, 291, 820, 1401, 1080, 14333, 281, 1466, 577, 281, 764, 309, 11, 457, 1936, 291, 584, 1392, 13, 286, 528, 281, 1884, 257, 7542, 295], "temperature": 0.0, "avg_logprob": -0.17496423721313475, "compression_ratio": 1.641350210970464, "no_speech_prob": 2.33206606026215e-06}, {"id": 866, "seek": 424614, "start": 4260.06, "end": 4263.22, "text": " This data frame now when you create plots", "tokens": [639, 1412, 3920, 586, 562, 291, 1884, 28609], "temperature": 0.0, "avg_logprob": -0.17496423721313475, "compression_ratio": 1.641350210970464, "no_speech_prob": 2.33206606026215e-06}, {"id": 867, "seek": 424614, "start": 4264.740000000001, "end": 4267.38, "text": " Most of the data sets you're using are going to be", "tokens": [4534, 295, 264, 1412, 6352, 291, 434, 1228, 366, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.17496423721313475, "compression_ratio": 1.641350210970464, "no_speech_prob": 2.33206606026215e-06}, {"id": 868, "seek": 424614, "start": 4267.9400000000005, "end": 4274.5, "text": " Too big to plot as in like if you do a scatter plot. It'll create so many dots that it's just a big mess", "tokens": [11395, 955, 281, 7542, 382, 294, 411, 498, 291, 360, 257, 34951, 7542, 13, 467, 603, 1884, 370, 867, 15026, 300, 309, 311, 445, 257, 955, 2082], "temperature": 0.0, "avg_logprob": -0.17496423721313475, "compression_ratio": 1.641350210970464, "no_speech_prob": 2.33206606026215e-06}, {"id": 869, "seek": 427450, "start": 4274.5, "end": 4278.38, "text": " And it'll take forever and remember when you're plotting things", "tokens": [400, 309, 603, 747, 5680, 293, 1604, 562, 291, 434, 41178, 721], "temperature": 0.0, "avg_logprob": -0.17620834003795277, "compression_ratio": 1.76171875, "no_speech_prob": 4.222818461130373e-06}, {"id": 870, "seek": 427450, "start": 4279.3, "end": 4284.66, "text": " You just you're you're looking at it right so there's no point plotting something with a hundred million samples", "tokens": [509, 445, 291, 434, 291, 434, 1237, 412, 309, 558, 370, 456, 311, 572, 935, 41178, 746, 365, 257, 3262, 2459, 10938], "temperature": 0.0, "avg_logprob": -0.17620834003795277, "compression_ratio": 1.76171875, "no_speech_prob": 4.222818461130373e-06}, {"id": 871, "seek": 427450, "start": 4285.02, "end": 4288.54, "text": " When if you only used a hundred thousand samples it's going to be pixel identical", "tokens": [1133, 498, 291, 787, 1143, 257, 3262, 4714, 10938, 309, 311, 516, 281, 312, 19261, 14800], "temperature": 0.0, "avg_logprob": -0.17620834003795277, "compression_ratio": 1.76171875, "no_speech_prob": 4.222818461130373e-06}, {"id": 872, "seek": 427450, "start": 4289.46, "end": 4295.98, "text": " Right so that's why I call get sample first so get sample just grabs a random sample. Okay, so I'm just going to grab", "tokens": [1779, 370, 300, 311, 983, 286, 818, 483, 6889, 700, 370, 483, 6889, 445, 30028, 257, 4974, 6889, 13, 1033, 11, 370, 286, 478, 445, 516, 281, 4444], "temperature": 0.0, "avg_logprob": -0.17620834003795277, "compression_ratio": 1.76171875, "no_speech_prob": 4.222818461130373e-06}, {"id": 873, "seek": 427450, "start": 4296.94, "end": 4298.38, "text": " 500 points", "tokens": [5923, 2793], "temperature": 0.0, "avg_logprob": -0.17620834003795277, "compression_ratio": 1.76171875, "no_speech_prob": 4.222818461130373e-06}, {"id": 874, "seek": 427450, "start": 4298.38, "end": 4302.76, "text": " For now okay, so I've got to grab 500 points from my data frame", "tokens": [1171, 586, 1392, 11, 370, 286, 600, 658, 281, 4444, 5923, 2793, 490, 452, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.17620834003795277, "compression_ratio": 1.76171875, "no_speech_prob": 4.222818461130373e-06}, {"id": 875, "seek": 430276, "start": 4302.76, "end": 4304.76, "text": " I got a plot", "tokens": [286, 658, 257, 7542], "temperature": 0.0, "avg_logprob": -0.19037534395853678, "compression_ratio": 1.695970695970696, "no_speech_prob": 7.690368875046261e-07}, {"id": 876, "seek": 430276, "start": 4304.9400000000005, "end": 4309.74, "text": " Year made against sale price AES stands for aesthetic. This is the basic way that you set up your", "tokens": [10289, 1027, 1970, 8680, 3218, 316, 2358, 7382, 337, 20092, 13, 639, 307, 264, 3875, 636, 300, 291, 992, 493, 428], "temperature": 0.0, "avg_logprob": -0.19037534395853678, "compression_ratio": 1.695970695970696, "no_speech_prob": 7.690368875046261e-07}, {"id": 877, "seek": 430276, "start": 4310.42, "end": 4315.68, "text": " Columns in Gigi plot okay, so this says to plot these columns from this data frame", "tokens": [4004, 449, 3695, 294, 460, 19789, 7542, 1392, 11, 370, 341, 1619, 281, 7542, 613, 13766, 490, 341, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.19037534395853678, "compression_ratio": 1.695970695970696, "no_speech_prob": 7.690368875046261e-07}, {"id": 878, "seek": 430276, "start": 4315.68, "end": 4322.900000000001, "text": " And then you there's this weird thing in Gigi plot where plus means basically add chart elements, okay, so I'm going to add a", "tokens": [400, 550, 291, 456, 311, 341, 3657, 551, 294, 460, 19789, 7542, 689, 1804, 1355, 1936, 909, 6927, 4959, 11, 1392, 11, 370, 286, 478, 516, 281, 909, 257], "temperature": 0.0, "avg_logprob": -0.19037534395853678, "compression_ratio": 1.695970695970696, "no_speech_prob": 7.690368875046261e-07}, {"id": 879, "seek": 430276, "start": 4323.62, "end": 4325.18, "text": " smoother", "tokens": [28640], "temperature": 0.0, "avg_logprob": -0.19037534395853678, "compression_ratio": 1.695970695970696, "no_speech_prob": 7.690368875046261e-07}, {"id": 880, "seek": 430276, "start": 4325.18, "end": 4326.3, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.19037534395853678, "compression_ratio": 1.695970695970696, "no_speech_prob": 7.690368875046261e-07}, {"id": 881, "seek": 432630, "start": 4326.3, "end": 4332.42, "text": " Most of the very very often you'll find that a scatter plot is very hard to see what's going on because there's too much randomness", "tokens": [4534, 295, 264, 588, 588, 2049, 291, 603, 915, 300, 257, 34951, 7542, 307, 588, 1152, 281, 536, 437, 311, 516, 322, 570, 456, 311, 886, 709, 4974, 1287], "temperature": 0.0, "avg_logprob": -0.14790711653859992, "compression_ratio": 1.625514403292181, "no_speech_prob": 1.5294101558538387e-06}, {"id": 882, "seek": 432630, "start": 4332.66, "end": 4340.02, "text": " Where else a smoother basically creates a little linear regression for every little subset of the graph", "tokens": [2305, 1646, 257, 28640, 1936, 7829, 257, 707, 8213, 24590, 337, 633, 707, 25993, 295, 264, 4295], "temperature": 0.0, "avg_logprob": -0.14790711653859992, "compression_ratio": 1.625514403292181, "no_speech_prob": 1.5294101558538387e-06}, {"id": 883, "seek": 432630, "start": 4340.02, "end": 4345.1, "text": " And so it kind of joins it up and allows you to see a nice smooth curve, okay?", "tokens": [400, 370, 309, 733, 295, 24397, 309, 493, 293, 4045, 291, 281, 536, 257, 1481, 5508, 7605, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.14790711653859992, "compression_ratio": 1.625514403292181, "no_speech_prob": 1.5294101558538387e-06}, {"id": 884, "seek": 432630, "start": 4345.62, "end": 4350.26, "text": " so this is like the main way that I tend to look at univariate relationships and", "tokens": [370, 341, 307, 411, 264, 2135, 636, 300, 286, 3928, 281, 574, 412, 517, 592, 3504, 473, 6159, 293], "temperature": 0.0, "avg_logprob": -0.14790711653859992, "compression_ratio": 1.625514403292181, "no_speech_prob": 1.5294101558538387e-06}, {"id": 885, "seek": 435026, "start": 4350.26, "end": 4358.12, "text": " And by adding standard error equals true it also shows me the confidence interval of this smoother right", "tokens": [400, 538, 5127, 3832, 6713, 6915, 2074, 309, 611, 3110, 385, 264, 6687, 15035, 295, 341, 28640, 558], "temperature": 0.0, "avg_logprob": -0.20903703230845777, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.034851152326155e-06}, {"id": 886, "seek": 435026, "start": 4359.38, "end": 4364.38, "text": " So Loess stands for locally weighted regression, which is this idea of like doing kind of like doing lots of little", "tokens": [407, 6130, 442, 7382, 337, 16143, 32807, 24590, 11, 597, 307, 341, 1558, 295, 411, 884, 733, 295, 411, 884, 3195, 295, 707], "temperature": 0.0, "avg_logprob": -0.20903703230845777, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.034851152326155e-06}, {"id": 887, "seek": 435026, "start": 4365.26, "end": 4367.26, "text": " linear regressions", "tokens": [8213, 1121, 735, 626], "temperature": 0.0, "avg_logprob": -0.20903703230845777, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.034851152326155e-06}, {"id": 888, "seek": 435026, "start": 4368.5, "end": 4375.6, "text": " So we can see here the relationship between year made and sale price is kind of all over the place right which is like", "tokens": [407, 321, 393, 536, 510, 264, 2480, 1296, 1064, 1027, 293, 8680, 3218, 307, 733, 295, 439, 670, 264, 1081, 558, 597, 307, 411], "temperature": 0.0, "avg_logprob": -0.20903703230845777, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.034851152326155e-06}, {"id": 889, "seek": 437560, "start": 4375.6, "end": 4380.4800000000005, "text": " Not really what I would expect. I would I would have expected that more recent", "tokens": [1726, 534, 437, 286, 576, 2066, 13, 286, 576, 286, 576, 362, 5176, 300, 544, 5162], "temperature": 0.0, "avg_logprob": -0.17272025463627835, "compression_ratio": 1.7469879518072289, "no_speech_prob": 8.31527017908229e-07}, {"id": 890, "seek": 437560, "start": 4382.120000000001, "end": 4384.120000000001, "text": " Stuff that sold more recently", "tokens": [31347, 300, 3718, 544, 3938], "temperature": 0.0, "avg_logprob": -0.17272025463627835, "compression_ratio": 1.7469879518072289, "no_speech_prob": 8.31527017908229e-07}, {"id": 891, "seek": 437560, "start": 4384.4400000000005, "end": 4390.200000000001, "text": " Would probably be like more expensive because of inflation and because they're like more current models and so forth", "tokens": [6068, 1391, 312, 411, 544, 5124, 570, 295, 15860, 293, 570, 436, 434, 411, 544, 2190, 5245, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.17272025463627835, "compression_ratio": 1.7469879518072289, "no_speech_prob": 8.31527017908229e-07}, {"id": 892, "seek": 437560, "start": 4390.200000000001, "end": 4396.04, "text": " And the problem is that when you look at a univariate relationship like this. There's a whole lot of", "tokens": [400, 264, 1154, 307, 300, 562, 291, 574, 412, 257, 517, 592, 3504, 473, 2480, 411, 341, 13, 821, 311, 257, 1379, 688, 295], "temperature": 0.0, "avg_logprob": -0.17272025463627835, "compression_ratio": 1.7469879518072289, "no_speech_prob": 8.31527017908229e-07}, {"id": 893, "seek": 437560, "start": 4397.4800000000005, "end": 4401.5, "text": " Collinearity going on a whole lot of interactions that are being lost so for example", "tokens": [4586, 533, 17409, 516, 322, 257, 1379, 688, 295, 13280, 300, 366, 885, 2731, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.17272025463627835, "compression_ratio": 1.7469879518072289, "no_speech_prob": 8.31527017908229e-07}, {"id": 894, "seek": 437560, "start": 4402.52, "end": 4404.52, "text": " Why did the price drop?", "tokens": [1545, 630, 264, 3218, 3270, 30], "temperature": 0.0, "avg_logprob": -0.17272025463627835, "compression_ratio": 1.7469879518072289, "no_speech_prob": 8.31527017908229e-07}, {"id": 895, "seek": 440452, "start": 4404.52, "end": 4410.120000000001, "text": " Here is it actually because like things made between 1991 and 1997", "tokens": [1692, 307, 309, 767, 570, 411, 721, 1027, 1296, 24097, 293, 22383], "temperature": 0.0, "avg_logprob": -0.22404812083524817, "compression_ratio": 1.760180995475113, "no_speech_prob": 1.2098624893042143e-06}, {"id": 896, "seek": 440452, "start": 4411.120000000001, "end": 4419.040000000001, "text": " Are less valuable or is actually because most of them were also sold during that time and actually there was like maybe a recession then", "tokens": [2014, 1570, 8263, 420, 307, 767, 570, 881, 295, 552, 645, 611, 3718, 1830, 300, 565, 293, 767, 456, 390, 411, 1310, 257, 24828, 550], "temperature": 0.0, "avg_logprob": -0.22404812083524817, "compression_ratio": 1.760180995475113, "no_speech_prob": 1.2098624893042143e-06}, {"id": 897, "seek": 440452, "start": 4419.040000000001, "end": 4424.040000000001, "text": " Or maybe it was like the product sold during that time a lot more people were buying", "tokens": [1610, 1310, 309, 390, 411, 264, 1674, 3718, 1830, 300, 565, 257, 688, 544, 561, 645, 6382], "temperature": 0.0, "avg_logprob": -0.22404812083524817, "compression_ratio": 1.760180995475113, "no_speech_prob": 1.2098624893042143e-06}, {"id": 898, "seek": 440452, "start": 4425.120000000001, "end": 4431.4400000000005, "text": " Types of vehicle that were less expensive like there's all kinds of reasons for that and so again as", "tokens": [5569, 5190, 295, 5864, 300, 645, 1570, 5124, 411, 456, 311, 439, 3685, 295, 4112, 337, 300, 293, 370, 797, 382], "temperature": 0.0, "avg_logprob": -0.22404812083524817, "compression_ratio": 1.760180995475113, "no_speech_prob": 1.2098624893042143e-06}, {"id": 899, "seek": 443144, "start": 4431.44, "end": 4438.36, "text": " Data scientists one of the things you're going to keep seeing is that at the companies that you join people will come to you with", "tokens": [11888, 7708, 472, 295, 264, 721, 291, 434, 516, 281, 1066, 2577, 307, 300, 412, 264, 3431, 300, 291, 3917, 561, 486, 808, 281, 291, 365], "temperature": 0.0, "avg_logprob": -0.16000080871582031, "compression_ratio": 1.8265306122448979, "no_speech_prob": 1.7061745438695652e-06}, {"id": 900, "seek": 443144, "start": 4438.36, "end": 4443.919999999999, "text": " With these kind of univariate charts where they'll say like oh my god our sales in Chicago have", "tokens": [2022, 613, 733, 295, 517, 592, 3504, 473, 17767, 689, 436, 603, 584, 411, 1954, 452, 3044, 527, 5763, 294, 9525, 362], "temperature": 0.0, "avg_logprob": -0.16000080871582031, "compression_ratio": 1.8265306122448979, "no_speech_prob": 1.7061745438695652e-06}, {"id": 901, "seek": 443144, "start": 4444.44, "end": 4448.5599999999995, "text": " Disappeared they've got really bad or people aren't clicking on this ad anymore", "tokens": [4208, 46408, 1642, 436, 600, 658, 534, 1578, 420, 561, 3212, 380, 9697, 322, 341, 614, 3602], "temperature": 0.0, "avg_logprob": -0.16000080871582031, "compression_ratio": 1.8265306122448979, "no_speech_prob": 1.7061745438695652e-06}, {"id": 902, "seek": 443144, "start": 4448.5599999999995, "end": 4452.32, "text": " and they'll show you a chart that looks like this and they'll be like what happened and", "tokens": [293, 436, 603, 855, 291, 257, 6927, 300, 1542, 411, 341, 293, 436, 603, 312, 411, 437, 2011, 293], "temperature": 0.0, "avg_logprob": -0.16000080871582031, "compression_ratio": 1.8265306122448979, "no_speech_prob": 1.7061745438695652e-06}, {"id": 903, "seek": 443144, "start": 4453.08, "end": 4460.5599999999995, "text": " Most of the time you'll find the answer to the question. What happened is that there's something else going on right so actually all in Chicago", "tokens": [4534, 295, 264, 565, 291, 603, 915, 264, 1867, 281, 264, 1168, 13, 708, 2011, 307, 300, 456, 311, 746, 1646, 516, 322, 558, 370, 767, 439, 294, 9525], "temperature": 0.0, "avg_logprob": -0.16000080871582031, "compression_ratio": 1.8265306122448979, "no_speech_prob": 1.7061745438695652e-06}, {"id": 904, "seek": 446056, "start": 4460.56, "end": 4467.72, "text": " Last week actually we were doing a new promotion, and that's why our you know revenue went down", "tokens": [5264, 1243, 767, 321, 645, 884, 257, 777, 15783, 11, 293, 300, 311, 983, 527, 291, 458, 9324, 1437, 760], "temperature": 0.0, "avg_logprob": -0.17120007966694079, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.4593707646781695e-06}, {"id": 905, "seek": 446056, "start": 4467.72, "end": 4472.200000000001, "text": " It's not because people aren't buying stuff in Chicago anymore. It's because the prices were lower for instance", "tokens": [467, 311, 406, 570, 561, 3212, 380, 6382, 1507, 294, 9525, 3602, 13, 467, 311, 570, 264, 7901, 645, 3126, 337, 5197], "temperature": 0.0, "avg_logprob": -0.17120007966694079, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.4593707646781695e-06}, {"id": 906, "seek": 446056, "start": 4472.68, "end": 4476.280000000001, "text": " So what we really want to be able to do is say well", "tokens": [407, 437, 321, 534, 528, 281, 312, 1075, 281, 360, 307, 584, 731], "temperature": 0.0, "avg_logprob": -0.17120007966694079, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.4593707646781695e-06}, {"id": 907, "seek": 446056, "start": 4476.280000000001, "end": 4481.4400000000005, "text": " What's the relationship between sale price and year made all other things being equal?", "tokens": [708, 311, 264, 2480, 1296, 8680, 3218, 293, 1064, 1027, 439, 661, 721, 885, 2681, 30], "temperature": 0.0, "avg_logprob": -0.17120007966694079, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.4593707646781695e-06}, {"id": 908, "seek": 446056, "start": 4483.280000000001, "end": 4485.280000000001, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.17120007966694079, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.4593707646781695e-06}, {"id": 909, "seek": 446056, "start": 4485.92, "end": 4487.92, "text": " All other things being equal", "tokens": [1057, 661, 721, 885, 2681], "temperature": 0.0, "avg_logprob": -0.17120007966694079, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.4593707646781695e-06}, {"id": 910, "seek": 446056, "start": 4488.56, "end": 4490.4800000000005, "text": " basically means", "tokens": [1936, 1355], "temperature": 0.0, "avg_logprob": -0.17120007966694079, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.4593707646781695e-06}, {"id": 911, "seek": 449048, "start": 4490.48, "end": 4497.959999999999, "text": " If we sold something in 1990 versus 1980 and it was exactly the same thing to exactly the same person and exactly the same", "tokens": [759, 321, 3718, 746, 294, 13384, 5717, 13626, 293, 309, 390, 2293, 264, 912, 551, 281, 2293, 264, 912, 954, 293, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.18457513568045078, "compression_ratio": 1.7511961722488039, "no_speech_prob": 3.3931248708540807e-06}, {"id": 912, "seek": 449048, "start": 4498.08, "end": 4501.24, "text": " Auction so on and so forth what would have been the difference in price?", "tokens": [12160, 882, 370, 322, 293, 370, 5220, 437, 576, 362, 668, 264, 2649, 294, 3218, 30], "temperature": 0.0, "avg_logprob": -0.18457513568045078, "compression_ratio": 1.7511961722488039, "no_speech_prob": 3.3931248708540807e-06}, {"id": 913, "seek": 449048, "start": 4502.48, "end": 4507.679999999999, "text": " and so to do that we do something called a partial dependence plot and", "tokens": [293, 370, 281, 360, 300, 321, 360, 746, 1219, 257, 14641, 31704, 7542, 293], "temperature": 0.0, "avg_logprob": -0.18457513568045078, "compression_ratio": 1.7511961722488039, "no_speech_prob": 3.3931248708540807e-06}, {"id": 914, "seek": 449048, "start": 4508.0, "end": 4512.2, "text": " This is a partial dependence plot. There's a really nice library which nobody's heard of", "tokens": [639, 307, 257, 14641, 31704, 7542, 13, 821, 311, 257, 534, 1481, 6405, 597, 5079, 311, 2198, 295], "temperature": 0.0, "avg_logprob": -0.18457513568045078, "compression_ratio": 1.7511961722488039, "no_speech_prob": 3.3931248708540807e-06}, {"id": 915, "seek": 449048, "start": 4512.919999999999, "end": 4514.919999999999, "text": " called PDP", "tokens": [1219, 10464, 47], "temperature": 0.0, "avg_logprob": -0.18457513568045078, "compression_ratio": 1.7511961722488039, "no_speech_prob": 3.3931248708540807e-06}, {"id": 916, "seek": 451492, "start": 4514.92, "end": 4522.28, "text": " Which does these partial dependence plots and what happens is this we've got our sample of 500 data points", "tokens": [3013, 775, 613, 14641, 31704, 28609, 293, 437, 2314, 307, 341, 321, 600, 658, 527, 6889, 295, 5923, 1412, 2793], "temperature": 0.0, "avg_logprob": -0.2320355264764083, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.7266177059791517e-06}, {"id": 917, "seek": 451492, "start": 4522.68, "end": 4524.68, "text": " Right and we're going to do something really interesting", "tokens": [1779, 293, 321, 434, 516, 281, 360, 746, 534, 1880], "temperature": 0.0, "avg_logprob": -0.2320355264764083, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.7266177059791517e-06}, {"id": 918, "seek": 451492, "start": 4524.76, "end": 4529.4400000000005, "text": " we're going to take each one of those hundred randomly chosen options and", "tokens": [321, 434, 516, 281, 747, 1184, 472, 295, 729, 3262, 16979, 8614, 3956, 293], "temperature": 0.0, "avg_logprob": -0.2320355264764083, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.7266177059791517e-06}, {"id": 919, "seek": 451492, "start": 4530.36, "end": 4534.08, "text": " We're going to make a little data set out of it right so like here's our", "tokens": [492, 434, 516, 281, 652, 257, 707, 1412, 992, 484, 295, 309, 558, 370, 411, 510, 311, 527], "temperature": 0.0, "avg_logprob": -0.2320355264764083, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.7266177059791517e-06}, {"id": 920, "seek": 451492, "start": 4537.0, "end": 4539.0, "text": " Here's our", "tokens": [1692, 311, 527], "temperature": 0.0, "avg_logprob": -0.2320355264764083, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.7266177059791517e-06}, {"id": 921, "seek": 451492, "start": 4539.0, "end": 4541.0, "text": " Come on one up", "tokens": [2492, 322, 472, 493], "temperature": 0.0, "avg_logprob": -0.2320355264764083, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.7266177059791517e-06}, {"id": 922, "seek": 451492, "start": 4541.28, "end": 4543.86, "text": " Here's our data set of like 500 auctions", "tokens": [1692, 311, 527, 1412, 992, 295, 411, 5923, 1609, 3916], "temperature": 0.0, "avg_logprob": -0.2320355264764083, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.7266177059791517e-06}, {"id": 923, "seek": 454386, "start": 4543.86, "end": 4545.5, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.1649338082422184, "compression_ratio": 1.6369047619047619, "no_speech_prob": 5.33813454239862e-06}, {"id": 924, "seek": 454386, "start": 4545.5, "end": 4547.5, "text": " Here's our columns", "tokens": [1692, 311, 527, 13766], "temperature": 0.0, "avg_logprob": -0.1649338082422184, "compression_ratio": 1.6369047619047619, "no_speech_prob": 5.33813454239862e-06}, {"id": 925, "seek": 454386, "start": 4548.259999999999, "end": 4553.9, "text": " One of which is the thing that we're interested in which is year made so here's year made", "tokens": [1485, 295, 597, 307, 264, 551, 300, 321, 434, 3102, 294, 597, 307, 1064, 1027, 370, 510, 311, 1064, 1027], "temperature": 0.0, "avg_logprob": -0.1649338082422184, "compression_ratio": 1.6369047619047619, "no_speech_prob": 5.33813454239862e-06}, {"id": 926, "seek": 454386, "start": 4555.219999999999, "end": 4558.9, "text": " Okay, and what we're going to do is we're never going to try and create a chart", "tokens": [1033, 11, 293, 437, 321, 434, 516, 281, 360, 307, 321, 434, 1128, 516, 281, 853, 293, 1884, 257, 6927], "temperature": 0.0, "avg_logprob": -0.1649338082422184, "compression_ratio": 1.6369047619047619, "no_speech_prob": 5.33813454239862e-06}, {"id": 927, "seek": 454386, "start": 4559.339999999999, "end": 4562.9, "text": " Where we're going to try and say all other things being equal in", "tokens": [2305, 321, 434, 516, 281, 853, 293, 584, 439, 661, 721, 885, 2681, 294], "temperature": 0.0, "avg_logprob": -0.1649338082422184, "compression_ratio": 1.6369047619047619, "no_speech_prob": 5.33813454239862e-06}, {"id": 928, "seek": 454386, "start": 4564.0599999999995, "end": 4566.0599999999995, "text": " 1960", "tokens": [16157], "temperature": 0.0, "avg_logprob": -0.1649338082422184, "compression_ratio": 1.6369047619047619, "no_speech_prob": 5.33813454239862e-06}, {"id": 929, "seek": 454386, "start": 4566.98, "end": 4568.98, "text": " How much did", "tokens": [1012, 709, 630], "temperature": 0.0, "avg_logprob": -0.1649338082422184, "compression_ratio": 1.6369047619047619, "no_speech_prob": 5.33813454239862e-06}, {"id": 930, "seek": 456898, "start": 4568.98, "end": 4575.599999999999, "text": " Bulldozers cost how much did things cost in options and so the way we're going to do that is we're going to replace the year", "tokens": [14131, 2595, 41698, 2063, 577, 709, 630, 721, 2063, 294, 3956, 293, 370, 264, 636, 321, 434, 516, 281, 360, 300, 307, 321, 434, 516, 281, 7406, 264, 1064], "temperature": 0.0, "avg_logprob": -0.12857269185834228, "compression_ratio": 1.9282700421940928, "no_speech_prob": 3.3405217436666135e-06}, {"id": 931, "seek": 456898, "start": 4575.62, "end": 4577.62, "text": " Made column with", "tokens": [18330, 7738, 365], "temperature": 0.0, "avg_logprob": -0.12857269185834228, "compression_ratio": 1.9282700421940928, "no_speech_prob": 3.3405217436666135e-06}, {"id": 932, "seek": 456898, "start": 4577.74, "end": 4584.419999999999, "text": " 1960 we're going to copy in the value 1960 again and again and again all the way down right so now every row", "tokens": [16157, 321, 434, 516, 281, 5055, 294, 264, 2158, 16157, 797, 293, 797, 293, 797, 439, 264, 636, 760, 558, 370, 586, 633, 5386], "temperature": 0.0, "avg_logprob": -0.12857269185834228, "compression_ratio": 1.9282700421940928, "no_speech_prob": 3.3405217436666135e-06}, {"id": 933, "seek": 456898, "start": 4584.9, "end": 4588.98, "text": " The year made is 1960 and all of the other data is going to be exactly the same", "tokens": [440, 1064, 1027, 307, 16157, 293, 439, 295, 264, 661, 1412, 307, 516, 281, 312, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.12857269185834228, "compression_ratio": 1.9282700421940928, "no_speech_prob": 3.3405217436666135e-06}, {"id": 934, "seek": 456898, "start": 4589.219999999999, "end": 4593.82, "text": " And we're going to take our random forest and we're going to pass all this through our random forest", "tokens": [400, 321, 434, 516, 281, 747, 527, 4974, 6719, 293, 321, 434, 516, 281, 1320, 439, 341, 807, 527, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.12857269185834228, "compression_ratio": 1.9282700421940928, "no_speech_prob": 3.3405217436666135e-06}, {"id": 935, "seek": 459382, "start": 4593.82, "end": 4598.139999999999, "text": " to predict the sale price", "tokens": [281, 6069, 264, 8680, 3218], "temperature": 0.0, "avg_logprob": -0.20647944473638769, "compression_ratio": 1.6630434782608696, "no_speech_prob": 1.602807628842129e-06}, {"id": 936, "seek": 459382, "start": 4599.94, "end": 4606.299999999999, "text": " So that will tell us for everything that was auctioned how much do we think it would have been sold for", "tokens": [407, 300, 486, 980, 505, 337, 1203, 300, 390, 24139, 292, 577, 709, 360, 321, 519, 309, 576, 362, 668, 3718, 337], "temperature": 0.0, "avg_logprob": -0.20647944473638769, "compression_ratio": 1.6630434782608696, "no_speech_prob": 1.602807628842129e-06}, {"id": 937, "seek": 459382, "start": 4606.78, "end": 4610.0199999999995, "text": " if that thing was made in 1960 and", "tokens": [498, 300, 551, 390, 1027, 294, 16157, 293], "temperature": 0.0, "avg_logprob": -0.20647944473638769, "compression_ratio": 1.6630434782608696, "no_speech_prob": 1.602807628842129e-06}, {"id": 938, "seek": 459382, "start": 4611.5, "end": 4613.5, "text": " That's what we're going to plot here", "tokens": [663, 311, 437, 321, 434, 516, 281, 7542, 510], "temperature": 0.0, "avg_logprob": -0.20647944473638769, "compression_ratio": 1.6630434782608696, "no_speech_prob": 1.602807628842129e-06}, {"id": 939, "seek": 459382, "start": 4614.099999999999, "end": 4618.5, "text": " All right, that's the price we're going to plot here, and then we're going to do the same thing for 1961", "tokens": [1057, 558, 11, 300, 311, 264, 3218, 321, 434, 516, 281, 7542, 510, 11, 293, 550, 321, 434, 516, 281, 360, 264, 912, 551, 337, 41720], "temperature": 0.0, "avg_logprob": -0.20647944473638769, "compression_ratio": 1.6630434782608696, "no_speech_prob": 1.602807628842129e-06}, {"id": 940, "seek": 461850, "start": 4618.5, "end": 4624.38, "text": " All right, we're going to replace all these and do 1961", "tokens": [1057, 558, 11, 321, 434, 516, 281, 7406, 439, 613, 293, 360, 41720], "temperature": 0.0, "avg_logprob": -0.13447492878611494, "compression_ratio": 1.4834123222748816, "no_speech_prob": 3.7266186154738534e-06}, {"id": 941, "seek": 461850, "start": 4629.1, "end": 4630.7, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.13447492878611494, "compression_ratio": 1.4834123222748816, "no_speech_prob": 3.7266186154738534e-06}, {"id": 942, "seek": 461850, "start": 4630.7, "end": 4632.06, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.13447492878611494, "compression_ratio": 1.4834123222748816, "no_speech_prob": 3.7266186154738534e-06}, {"id": 943, "seek": 461850, "start": 4632.06, "end": 4634.06, "text": " to be clear", "tokens": [281, 312, 1850], "temperature": 0.0, "avg_logprob": -0.13447492878611494, "compression_ratio": 1.4834123222748816, "no_speech_prob": 3.7266186154738534e-06}, {"id": 944, "seek": 461850, "start": 4634.06, "end": 4640.78, "text": " We've already fit the random forest yes, and then we're just passing a new year and seeing what it determines the price should be yeah", "tokens": [492, 600, 1217, 3318, 264, 4974, 6719, 2086, 11, 293, 550, 321, 434, 445, 8437, 257, 777, 1064, 293, 2577, 437, 309, 24799, 264, 3218, 820, 312, 1338], "temperature": 0.0, "avg_logprob": -0.13447492878611494, "compression_ratio": 1.4834123222748816, "no_speech_prob": 3.7266186154738534e-06}, {"id": 945, "seek": 461850, "start": 4640.78, "end": 4645.66, "text": " So this is a lot like the way we did feature importance, but rather than randomly shuffling the column", "tokens": [407, 341, 307, 257, 688, 411, 264, 636, 321, 630, 4111, 7379, 11, 457, 2831, 813, 16979, 402, 1245, 1688, 264, 7738], "temperature": 0.0, "avg_logprob": -0.13447492878611494, "compression_ratio": 1.4834123222748816, "no_speech_prob": 3.7266186154738534e-06}, {"id": 946, "seek": 464566, "start": 4645.66, "end": 4648.7, "text": " We're going to replace the column with a constant value", "tokens": [492, 434, 516, 281, 7406, 264, 7738, 365, 257, 5754, 2158], "temperature": 0.0, "avg_logprob": -0.15913869072409237, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.7330463606413105e-06}, {"id": 947, "seek": 464566, "start": 4649.46, "end": 4656.38, "text": " All right, so randomly shuffling the column tells us how accurate it is when you don't use that column anymore", "tokens": [1057, 558, 11, 370, 16979, 402, 1245, 1688, 264, 7738, 5112, 505, 577, 8559, 309, 307, 562, 291, 500, 380, 764, 300, 7738, 3602], "temperature": 0.0, "avg_logprob": -0.15913869072409237, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.7330463606413105e-06}, {"id": 948, "seek": 464566, "start": 4657.42, "end": 4663.94, "text": " Replacing the whole column with a constant tells us or estimates for us how much we would have sold that product for", "tokens": [47762, 5615, 264, 1379, 7738, 365, 257, 5754, 5112, 505, 420, 20561, 337, 505, 577, 709, 321, 576, 362, 3718, 300, 1674, 337], "temperature": 0.0, "avg_logprob": -0.15913869072409237, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.7330463606413105e-06}, {"id": 949, "seek": 464566, "start": 4664.42, "end": 4669.86, "text": " In that auction on that day in that place if that product had been made in 1961", "tokens": [682, 300, 24139, 322, 300, 786, 294, 300, 1081, 498, 300, 1674, 632, 668, 1027, 294, 41720], "temperature": 0.0, "avg_logprob": -0.15913869072409237, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.7330463606413105e-06}, {"id": 950, "seek": 466986, "start": 4669.86, "end": 4676.74, "text": " All right, so we basically then take the average of all of the sale prices that we calculate from that random forest", "tokens": [1057, 558, 11, 370, 321, 1936, 550, 747, 264, 4274, 295, 439, 295, 264, 8680, 7901, 300, 321, 8873, 490, 300, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.1730926846994937, "compression_ratio": 1.6144578313253013, "no_speech_prob": 8.186351010408544e-07}, {"id": 951, "seek": 466986, "start": 4676.74, "end": 4680.9, "text": " And so we do it in 1961 and we get this value, right?", "tokens": [400, 370, 321, 360, 309, 294, 41720, 293, 321, 483, 341, 2158, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1730926846994937, "compression_ratio": 1.6144578313253013, "no_speech_prob": 8.186351010408544e-07}, {"id": 952, "seek": 466986, "start": 4681.7, "end": 4685.78, "text": " So what the partial dependence plot here shows us is each of these light blue lines", "tokens": [407, 437, 264, 14641, 31704, 7542, 510, 3110, 505, 307, 1184, 295, 613, 1442, 3344, 3876], "temperature": 0.0, "avg_logprob": -0.1730926846994937, "compression_ratio": 1.6144578313253013, "no_speech_prob": 8.186351010408544e-07}, {"id": 953, "seek": 466986, "start": 4686.82, "end": 4691.179999999999, "text": " Actually is showing us all 500 lines, so it says", "tokens": [5135, 307, 4099, 505, 439, 5923, 3876, 11, 370, 309, 1619], "temperature": 0.0, "avg_logprob": -0.1730926846994937, "compression_ratio": 1.6144578313253013, "no_speech_prob": 8.186351010408544e-07}, {"id": 954, "seek": 466986, "start": 4692.339999999999, "end": 4695.299999999999, "text": " for row number one in our data set", "tokens": [337, 5386, 1230, 472, 294, 527, 1412, 992], "temperature": 0.0, "avg_logprob": -0.1730926846994937, "compression_ratio": 1.6144578313253013, "no_speech_prob": 8.186351010408544e-07}, {"id": 955, "seek": 469530, "start": 4695.3, "end": 4703.02, "text": " If we sold it in 1960 we're going to index that to zero right so we'll call that zero right if we sold it in", "tokens": [759, 321, 3718, 309, 294, 16157, 321, 434, 516, 281, 8186, 300, 281, 4018, 558, 370, 321, 603, 818, 300, 4018, 558, 498, 321, 3718, 309, 294], "temperature": 0.0, "avg_logprob": -0.1816509420221502, "compression_ratio": 1.8457142857142856, "no_speech_prob": 7.57113809868315e-07}, {"id": 956, "seek": 469530, "start": 4703.5, "end": 4705.5, "text": " 1970 that particular", "tokens": [14577, 300, 1729], "temperature": 0.0, "avg_logprob": -0.1816509420221502, "compression_ratio": 1.8457142857142856, "no_speech_prob": 7.57113809868315e-07}, {"id": 957, "seek": 469530, "start": 4706.38, "end": 4711.06, "text": " Auction would have been here if we sold it in 1980 it would have been here if we sold in 1990", "tokens": [12160, 882, 576, 362, 668, 510, 498, 321, 3718, 309, 294, 13626, 309, 576, 362, 668, 510, 498, 321, 3718, 294, 13384], "temperature": 0.0, "avg_logprob": -0.1816509420221502, "compression_ratio": 1.8457142857142856, "no_speech_prob": 7.57113809868315e-07}, {"id": 958, "seek": 469530, "start": 4711.06, "end": 4713.54, "text": " It would have been here, so we actually plot all", "tokens": [467, 576, 362, 668, 510, 11, 370, 321, 767, 7542, 439], "temperature": 0.0, "avg_logprob": -0.1816509420221502, "compression_ratio": 1.8457142857142856, "no_speech_prob": 7.57113809868315e-07}, {"id": 959, "seek": 469530, "start": 4714.7, "end": 4716.7, "text": " 500", "tokens": [5923], "temperature": 0.0, "avg_logprob": -0.1816509420221502, "compression_ratio": 1.8457142857142856, "no_speech_prob": 7.57113809868315e-07}, {"id": 960, "seek": 469530, "start": 4716.9400000000005, "end": 4719.62, "text": " Predictions of how much every one of those 500", "tokens": [32969, 15607, 295, 577, 709, 633, 472, 295, 729, 5923], "temperature": 0.0, "avg_logprob": -0.1816509420221502, "compression_ratio": 1.8457142857142856, "no_speech_prob": 7.57113809868315e-07}, {"id": 961, "seek": 471962, "start": 4719.62, "end": 4726.7, "text": " Auctions would have gone for if we replace it if we replace it a year made with each of these different values", "tokens": [12160, 3916, 576, 362, 2780, 337, 498, 321, 7406, 309, 498, 321, 7406, 309, 257, 1064, 1027, 365, 1184, 295, 613, 819, 4190], "temperature": 0.0, "avg_logprob": -0.22783377296046206, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.436748675587296e-06}, {"id": 962, "seek": 471962, "start": 4726.7, "end": 4731.16, "text": " And then then this dark line here is the average", "tokens": [400, 550, 550, 341, 2877, 1622, 510, 307, 264, 4274], "temperature": 0.0, "avg_logprob": -0.22783377296046206, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.436748675587296e-06}, {"id": 963, "seek": 471962, "start": 4731.78, "end": 4733.78, "text": " Right so this tells us", "tokens": [1779, 370, 341, 5112, 505], "temperature": 0.0, "avg_logprob": -0.22783377296046206, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.436748675587296e-06}, {"id": 964, "seek": 471962, "start": 4733.78, "end": 4735.78, "text": " How much would we have sold?", "tokens": [1012, 709, 576, 321, 362, 3718, 30], "temperature": 0.0, "avg_logprob": -0.22783377296046206, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.436748675587296e-06}, {"id": 965, "seek": 471962, "start": 4736.18, "end": 4741.5, "text": " on average all of those options for if all of those products were actually", "tokens": [322, 4274, 439, 295, 729, 3956, 337, 498, 439, 295, 729, 3383, 645, 767], "temperature": 0.0, "avg_logprob": -0.22783377296046206, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.436748675587296e-06}, {"id": 966, "seek": 471962, "start": 4742.099999999999, "end": 4744.099999999999, "text": " made in 1985", "tokens": [1027, 294, 28962], "temperature": 0.0, "avg_logprob": -0.22783377296046206, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.436748675587296e-06}, {"id": 967, "seek": 474410, "start": 4744.1, "end": 4751.780000000001, "text": " 1990 1993 1994 and so forth and so you can see what's happened here is at least in the period where we have a reasonable", "tokens": [13384, 25137, 22736, 293, 370, 5220, 293, 370, 291, 393, 536, 437, 311, 2011, 510, 307, 412, 1935, 294, 264, 2896, 689, 321, 362, 257, 10585], "temperature": 0.0, "avg_logprob": -0.14149938448510987, "compression_ratio": 1.7269076305220883, "no_speech_prob": 7.002159918556572e-07}, {"id": 968, "seek": 474410, "start": 4751.780000000001, "end": 4755.76, "text": " Out of data which is since 1990 this is basically a totally straight line", "tokens": [5925, 295, 1412, 597, 307, 1670, 13384, 341, 307, 1936, 257, 3879, 2997, 1622], "temperature": 0.0, "avg_logprob": -0.14149938448510987, "compression_ratio": 1.7269076305220883, "no_speech_prob": 7.002159918556572e-07}, {"id": 969, "seek": 474410, "start": 4756.780000000001, "end": 4761.26, "text": " Which is what you would expect right because if it was sold on the same date", "tokens": [3013, 307, 437, 291, 576, 2066, 558, 570, 498, 309, 390, 3718, 322, 264, 912, 4002], "temperature": 0.0, "avg_logprob": -0.14149938448510987, "compression_ratio": 1.7269076305220883, "no_speech_prob": 7.002159918556572e-07}, {"id": 970, "seek": 474410, "start": 4761.26, "end": 4766.620000000001, "text": " And it was the same kind of tractor that was sold to the same person in the same auction house", "tokens": [400, 309, 390, 264, 912, 733, 295, 31857, 300, 390, 3718, 281, 264, 912, 954, 294, 264, 912, 24139, 1782], "temperature": 0.0, "avg_logprob": -0.14149938448510987, "compression_ratio": 1.7269076305220883, "no_speech_prob": 7.002159918556572e-07}, {"id": 971, "seek": 474410, "start": 4766.900000000001, "end": 4771.900000000001, "text": " Then you would expect more recent vehicles to be more expensive", "tokens": [1396, 291, 576, 2066, 544, 5162, 8948, 281, 312, 544, 5124], "temperature": 0.0, "avg_logprob": -0.14149938448510987, "compression_ratio": 1.7269076305220883, "no_speech_prob": 7.002159918556572e-07}, {"id": 972, "seek": 477190, "start": 4771.9, "end": 4776.5, "text": " Because of inflation and because they're they're newer", "tokens": [1436, 295, 15860, 293, 570, 436, 434, 436, 434, 17628], "temperature": 0.0, "avg_logprob": -0.2862260540326436, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.726451839407673e-06}, {"id": 973, "seek": 477190, "start": 4777.139999999999, "end": 4783.66, "text": " Like they're not they're not as secondhand and you would expect that relationship to be roughly linear, and that's exactly what we're finding", "tokens": [1743, 436, 434, 406, 436, 434, 406, 382, 1150, 5543, 293, 291, 576, 2066, 300, 2480, 281, 312, 9810, 8213, 11, 293, 300, 311, 2293, 437, 321, 434, 5006], "temperature": 0.0, "avg_logprob": -0.2862260540326436, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.726451839407673e-06}, {"id": 974, "seek": 477190, "start": 4784.139999999999, "end": 4786.98, "text": " Okay, so by removing all of these", "tokens": [1033, 11, 370, 538, 12720, 439, 295, 613], "temperature": 0.0, "avg_logprob": -0.2862260540326436, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.726451839407673e-06}, {"id": 975, "seek": 477190, "start": 4788.299999999999, "end": 4792.48, "text": " externalities it often allows us to see the truth", "tokens": [8320, 1088, 309, 2049, 4045, 505, 281, 536, 264, 3494], "temperature": 0.0, "avg_logprob": -0.2862260540326436, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.726451839407673e-06}, {"id": 976, "seek": 477190, "start": 4793.0199999999995, "end": 4796.299999999999, "text": " Much more clearly as a question at the back. Can you pass that back there?", "tokens": [12313, 544, 4448, 382, 257, 1168, 412, 264, 646, 13, 1664, 291, 1320, 300, 646, 456, 30], "temperature": 0.0, "avg_logprob": -0.2862260540326436, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.726451839407673e-06}, {"id": 977, "seek": 477190, "start": 4796.9, "end": 4798.74, "text": " You're done, okay", "tokens": [509, 434, 1096, 11, 1392], "temperature": 0.0, "avg_logprob": -0.2862260540326436, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.726451839407673e-06}, {"id": 978, "seek": 477190, "start": 4798.74, "end": 4800.74, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2862260540326436, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.726451839407673e-06}, {"id": 979, "seek": 480074, "start": 4800.74, "end": 4802.0599999999995, "text": " this", "tokens": [341], "temperature": 0.0, "avg_logprob": -0.21855398503745474, "compression_ratio": 1.7076923076923076, "no_speech_prob": 5.507537480298197e-06}, {"id": 980, "seek": 480074, "start": 4802.0599999999995, "end": 4806.54, "text": " this partial dependence plot concept is something which is", "tokens": [341, 14641, 31704, 7542, 3410, 307, 746, 597, 307], "temperature": 0.0, "avg_logprob": -0.21855398503745474, "compression_ratio": 1.7076923076923076, "no_speech_prob": 5.507537480298197e-06}, {"id": 981, "seek": 480074, "start": 4807.26, "end": 4811.7, "text": " using a random forest to get us a more clear", "tokens": [1228, 257, 4974, 6719, 281, 483, 505, 257, 544, 1850], "temperature": 0.0, "avg_logprob": -0.21855398503745474, "compression_ratio": 1.7076923076923076, "no_speech_prob": 5.507537480298197e-06}, {"id": 982, "seek": 480074, "start": 4812.3, "end": 4817.82, "text": " Interpretation of what's going on in our data and so the steps were to first of all", "tokens": [5751, 6629, 399, 295, 437, 311, 516, 322, 294, 527, 1412, 293, 370, 264, 4439, 645, 281, 700, 295, 439], "temperature": 0.0, "avg_logprob": -0.21855398503745474, "compression_ratio": 1.7076923076923076, "no_speech_prob": 5.507537480298197e-06}, {"id": 983, "seek": 480074, "start": 4819.26, "end": 4824.46, "text": " Look at the feature importance to tell us like which things do we think we care about and", "tokens": [2053, 412, 264, 4111, 7379, 281, 980, 505, 411, 597, 721, 360, 321, 519, 321, 1127, 466, 293], "temperature": 0.0, "avg_logprob": -0.21855398503745474, "compression_ratio": 1.7076923076923076, "no_speech_prob": 5.507537480298197e-06}, {"id": 984, "seek": 480074, "start": 4825.099999999999, "end": 4828.66, "text": " Then to use the partial dependence plot to tell us", "tokens": [1396, 281, 764, 264, 14641, 31704, 7542, 281, 980, 505], "temperature": 0.0, "avg_logprob": -0.21855398503745474, "compression_ratio": 1.7076923076923076, "no_speech_prob": 5.507537480298197e-06}, {"id": 985, "seek": 482866, "start": 4828.66, "end": 4831.5, "text": " What's going on on average?", "tokens": [708, 311, 516, 322, 322, 4274, 30], "temperature": 0.0, "avg_logprob": -0.18529334764802055, "compression_ratio": 1.7052631578947368, "no_speech_prob": 4.6644427698083746e-07}, {"id": 986, "seek": 482866, "start": 4832.5, "end": 4834.34, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.18529334764802055, "compression_ratio": 1.7052631578947368, "no_speech_prob": 4.6644427698083746e-07}, {"id": 987, "seek": 482866, "start": 4834.34, "end": 4840.82, "text": " There's another cool thing we can do with PDP is we can use clusters and what clusters does is it uses cluster analysis?", "tokens": [821, 311, 1071, 1627, 551, 321, 393, 360, 365, 10464, 47, 307, 321, 393, 764, 23313, 293, 437, 23313, 775, 307, 309, 4960, 13630, 5215, 30], "temperature": 0.0, "avg_logprob": -0.18529334764802055, "compression_ratio": 1.7052631578947368, "no_speech_prob": 4.6644427698083746e-07}, {"id": 988, "seek": 482866, "start": 4841.22, "end": 4843.22, "text": " to look at all of these", "tokens": [281, 574, 412, 439, 295, 613], "temperature": 0.0, "avg_logprob": -0.18529334764802055, "compression_ratio": 1.7052631578947368, "no_speech_prob": 4.6644427698083746e-07}, {"id": 989, "seek": 482866, "start": 4843.42, "end": 4845.86, "text": " each one of the 500 rows and say", "tokens": [1184, 472, 295, 264, 5923, 13241, 293, 584], "temperature": 0.0, "avg_logprob": -0.18529334764802055, "compression_ratio": 1.7052631578947368, "no_speech_prob": 4.6644427698083746e-07}, {"id": 990, "seek": 482866, "start": 4846.86, "end": 4852.86, "text": " To some of those 500 rows kind of move in the same way and like we can kind of see it seems like there's a whole", "tokens": [1407, 512, 295, 729, 5923, 13241, 733, 295, 1286, 294, 264, 912, 636, 293, 411, 321, 393, 733, 295, 536, 309, 2544, 411, 456, 311, 257, 1379], "temperature": 0.0, "avg_logprob": -0.18529334764802055, "compression_ratio": 1.7052631578947368, "no_speech_prob": 4.6644427698083746e-07}, {"id": 991, "seek": 485286, "start": 4852.86, "end": 4859.0199999999995, "text": " Lot of rows that kind of go down and then up and there seems to be a bunch of rows that kind of go up", "tokens": [20131, 295, 13241, 300, 733, 295, 352, 760, 293, 550, 493, 293, 456, 2544, 281, 312, 257, 3840, 295, 13241, 300, 733, 295, 352, 493], "temperature": 0.0, "avg_logprob": -0.12290485485180004, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.1544598237378523e-06}, {"id": 992, "seek": 485286, "start": 4859.0199999999995, "end": 4864.16, "text": " And then go flat like it does seem like there are some kind of different types of behaviors being hidden", "tokens": [400, 550, 352, 4962, 411, 309, 775, 1643, 411, 456, 366, 512, 733, 295, 819, 3467, 295, 15501, 885, 7633], "temperature": 0.0, "avg_logprob": -0.12290485485180004, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.1544598237378523e-06}, {"id": 993, "seek": 485286, "start": 4864.16, "end": 4870.679999999999, "text": " And so here is the result of doing that cluster analysis, right is we still get the same average", "tokens": [400, 370, 510, 307, 264, 1874, 295, 884, 300, 13630, 5215, 11, 558, 307, 321, 920, 483, 264, 912, 4274], "temperature": 0.0, "avg_logprob": -0.12290485485180004, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.1544598237378523e-06}, {"id": 994, "seek": 485286, "start": 4871.219999999999, "end": 4875.9, "text": " But it says here are kind of the five most common shapes that we see", "tokens": [583, 309, 1619, 510, 366, 733, 295, 264, 1732, 881, 2689, 10854, 300, 321, 536], "temperature": 0.0, "avg_logprob": -0.12290485485180004, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.1544598237378523e-06}, {"id": 995, "seek": 487590, "start": 4875.9, "end": 4882.7, "text": " And this is where you could then go in and say all right. It looks like some kinds of vehicle actually", "tokens": [400, 341, 307, 689, 291, 727, 550, 352, 294, 293, 584, 439, 558, 13, 467, 1542, 411, 512, 3685, 295, 5864, 767], "temperature": 0.0, "avg_logprob": -0.2133423607304411, "compression_ratio": 1.7566539923954372, "no_speech_prob": 1.6280451973216259e-06}, {"id": 996, "seek": 487590, "start": 4884.46, "end": 4888.9, "text": " After 1990 their prices are pretty flat and before that they were pretty linear", "tokens": [2381, 13384, 641, 7901, 366, 1238, 4962, 293, 949, 300, 436, 645, 1238, 8213], "temperature": 0.0, "avg_logprob": -0.2133423607304411, "compression_ratio": 1.7566539923954372, "no_speech_prob": 1.6280451973216259e-06}, {"id": 997, "seek": 487590, "start": 4889.219999999999, "end": 4895.74, "text": " Some kinds of vehicle are kind of exactly the opposite and so like different kinds of vehicle have these different shapes", "tokens": [2188, 3685, 295, 5864, 366, 733, 295, 2293, 264, 6182, 293, 370, 411, 819, 3685, 295, 5864, 362, 613, 819, 10854], "temperature": 0.0, "avg_logprob": -0.2133423607304411, "compression_ratio": 1.7566539923954372, "no_speech_prob": 1.6280451973216259e-06}, {"id": 998, "seek": 487590, "start": 4896.0199999999995, "end": 4900.0199999999995, "text": " Right and so this is something you could dig into I think there's one at the back. Oh you're good. Okay?", "tokens": [1779, 293, 370, 341, 307, 746, 291, 727, 2528, 666, 286, 519, 456, 311, 472, 412, 264, 646, 13, 876, 291, 434, 665, 13, 1033, 30], "temperature": 0.0, "avg_logprob": -0.2133423607304411, "compression_ratio": 1.7566539923954372, "no_speech_prob": 1.6280451973216259e-06}, {"id": 999, "seek": 487590, "start": 4900.74, "end": 4903.82, "text": " So what we're going to do with this information well", "tokens": [407, 437, 321, 434, 516, 281, 360, 365, 341, 1589, 731], "temperature": 0.0, "avg_logprob": -0.2133423607304411, "compression_ratio": 1.7566539923954372, "no_speech_prob": 1.6280451973216259e-06}, {"id": 1000, "seek": 490382, "start": 4903.82, "end": 4905.82, "text": " the purpose of", "tokens": [264, 4334, 295], "temperature": 0.0, "avg_logprob": -0.1647885520503206, "compression_ratio": 1.7224489795918367, "no_speech_prob": 8.315260515701084e-07}, {"id": 1001, "seek": 490382, "start": 4908.0199999999995, "end": 4912.219999999999, "text": " Interpretation is to learn about a data set and so why do you want to learn about a data set?", "tokens": [5751, 6629, 399, 307, 281, 1466, 466, 257, 1412, 992, 293, 370, 983, 360, 291, 528, 281, 1466, 466, 257, 1412, 992, 30], "temperature": 0.0, "avg_logprob": -0.1647885520503206, "compression_ratio": 1.7224489795918367, "no_speech_prob": 8.315260515701084e-07}, {"id": 1002, "seek": 490382, "start": 4912.78, "end": 4916.66, "text": " It's because you it's because you want to do something with it right so in this case", "tokens": [467, 311, 570, 291, 309, 311, 570, 291, 528, 281, 360, 746, 365, 309, 558, 370, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.1647885520503206, "compression_ratio": 1.7224489795918367, "no_speech_prob": 8.315260515701084e-07}, {"id": 1003, "seek": 490382, "start": 4917.5, "end": 4921.16, "text": " It's not so much something if you're trying to win a Kaggle competition", "tokens": [467, 311, 406, 370, 709, 746, 498, 291, 434, 1382, 281, 1942, 257, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.1647885520503206, "compression_ratio": 1.7224489795918367, "no_speech_prob": 8.315260515701084e-07}, {"id": 1004, "seek": 490382, "start": 4921.16, "end": 4926.0199999999995, "text": " I mean it can be a little bit like some of these insights might make you realize. Oh, I could", "tokens": [286, 914, 309, 393, 312, 257, 707, 857, 411, 512, 295, 613, 14310, 1062, 652, 291, 4325, 13, 876, 11, 286, 727], "temperature": 0.0, "avg_logprob": -0.1647885520503206, "compression_ratio": 1.7224489795918367, "no_speech_prob": 8.315260515701084e-07}, {"id": 1005, "seek": 490382, "start": 4926.82, "end": 4930.0199999999995, "text": " Transform this variable or create this interaction or whatever", "tokens": [27938, 341, 7006, 420, 1884, 341, 9285, 420, 2035], "temperature": 0.0, "avg_logprob": -0.1647885520503206, "compression_ratio": 1.7224489795918367, "no_speech_prob": 8.315260515701084e-07}, {"id": 1006, "seek": 493002, "start": 4930.02, "end": 4936.700000000001, "text": " Obviously feature importance is super important for Kaggle competitions, but this one's much more for like real life", "tokens": [7580, 4111, 7379, 307, 1687, 1021, 337, 48751, 22631, 26185, 11, 457, 341, 472, 311, 709, 544, 337, 411, 957, 993], "temperature": 0.0, "avg_logprob": -0.1500971812122273, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.9333501768414862e-06}, {"id": 1007, "seek": 493002, "start": 4936.700000000001, "end": 4941.02, "text": " You know so this is when you're talking to somebody and you say to them like", "tokens": [509, 458, 370, 341, 307, 562, 291, 434, 1417, 281, 2618, 293, 291, 584, 281, 552, 411], "temperature": 0.0, "avg_logprob": -0.1500971812122273, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.9333501768414862e-06}, {"id": 1008, "seek": 493002, "start": 4942.46, "end": 4948.38, "text": " Okay, those plots you've been showing me which actually say that like there was this kind of dip in prices", "tokens": [1033, 11, 729, 28609, 291, 600, 668, 4099, 385, 597, 767, 584, 300, 411, 456, 390, 341, 733, 295, 10460, 294, 7901], "temperature": 0.0, "avg_logprob": -0.1500971812122273, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.9333501768414862e-06}, {"id": 1009, "seek": 493002, "start": 4948.46, "end": 4956.42, "text": " You know based on like things made between 1990 and 1997 there wasn't really you know actually it was they were in", "tokens": [509, 458, 2361, 322, 411, 721, 1027, 1296, 13384, 293, 22383, 456, 2067, 380, 534, 291, 458, 767, 309, 390, 436, 645, 294], "temperature": 0.0, "avg_logprob": -0.1500971812122273, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.9333501768414862e-06}, {"id": 1010, "seek": 495642, "start": 4956.42, "end": 4959.58, "text": " Precreasing there was actually something else going on at that time", "tokens": [6001, 66, 265, 3349, 456, 390, 767, 746, 1646, 516, 322, 412, 300, 565], "temperature": 0.0, "avg_logprob": -0.16650168377420177, "compression_ratio": 1.8615384615384616, "no_speech_prob": 6.962201950955205e-06}, {"id": 1011, "seek": 495642, "start": 4960.58, "end": 4963.34, "text": " No, it's basically the thing that allows you to say like", "tokens": [883, 11, 309, 311, 1936, 264, 551, 300, 4045, 291, 281, 584, 411], "temperature": 0.0, "avg_logprob": -0.16650168377420177, "compression_ratio": 1.8615384615384616, "no_speech_prob": 6.962201950955205e-06}, {"id": 1012, "seek": 495642, "start": 4963.9800000000005, "end": 4970.78, "text": " So whatever this outcome. I'm trying to drive in my business is this is how something's driving it right so if it's like", "tokens": [407, 2035, 341, 9700, 13, 286, 478, 1382, 281, 3332, 294, 452, 1606, 307, 341, 307, 577, 746, 311, 4840, 309, 558, 370, 498, 309, 311, 411], "temperature": 0.0, "avg_logprob": -0.16650168377420177, "compression_ratio": 1.8615384615384616, "no_speech_prob": 6.962201950955205e-06}, {"id": 1013, "seek": 495642, "start": 4972.42, "end": 4978.54, "text": " I'm looking at you know kind of advertising technology. What's driving clicks that I'm actually digging in to say okay", "tokens": [286, 478, 1237, 412, 291, 458, 733, 295, 13097, 2899, 13, 708, 311, 4840, 18521, 300, 286, 478, 767, 17343, 294, 281, 584, 1392], "temperature": 0.0, "avg_logprob": -0.16650168377420177, "compression_ratio": 1.8615384615384616, "no_speech_prob": 6.962201950955205e-06}, {"id": 1014, "seek": 495642, "start": 4978.54, "end": 4983.34, "text": " This is actually how clicks are being driven. This is actually the variable that's driving it. This is how it's related", "tokens": [639, 307, 767, 577, 18521, 366, 885, 9555, 13, 639, 307, 767, 264, 7006, 300, 311, 4840, 309, 13, 639, 307, 577, 309, 311, 4077], "temperature": 0.0, "avg_logprob": -0.16650168377420177, "compression_ratio": 1.8615384615384616, "no_speech_prob": 6.962201950955205e-06}, {"id": 1015, "seek": 498334, "start": 4983.34, "end": 4988.66, "text": " So therefore we should change our behavior in this way. That's really the goal of any model", "tokens": [407, 4412, 321, 820, 1319, 527, 5223, 294, 341, 636, 13, 663, 311, 534, 264, 3387, 295, 604, 2316], "temperature": 0.0, "avg_logprob": -0.12057986929396952, "compression_ratio": 1.893238434163701, "no_speech_prob": 1.084514792637492e-06}, {"id": 1016, "seek": 498334, "start": 4988.66, "end": 4994.72, "text": " I guess there's two possible goals one goal of a model is just to get the predictions like if you're doing hedge fund trading", "tokens": [286, 2041, 456, 311, 732, 1944, 5493, 472, 3387, 295, 257, 2316, 307, 445, 281, 483, 264, 21264, 411, 498, 291, 434, 884, 25304, 2374, 9529], "temperature": 0.0, "avg_logprob": -0.12057986929396952, "compression_ratio": 1.893238434163701, "no_speech_prob": 1.084514792637492e-06}, {"id": 1017, "seek": 498334, "start": 4994.72, "end": 4999.54, "text": " You probably just want to know what the price of that equity is going to be if you're doing insurance", "tokens": [509, 1391, 445, 528, 281, 458, 437, 264, 3218, 295, 300, 10769, 307, 516, 281, 312, 498, 291, 434, 884, 7214], "temperature": 0.0, "avg_logprob": -0.12057986929396952, "compression_ratio": 1.893238434163701, "no_speech_prob": 1.084514792637492e-06}, {"id": 1018, "seek": 498334, "start": 4999.54, "end": 5004.18, "text": " You probably just want to know how much claims that guy's going to have but probably most of the time", "tokens": [509, 1391, 445, 528, 281, 458, 577, 709, 9441, 300, 2146, 311, 516, 281, 362, 457, 1391, 881, 295, 264, 565], "temperature": 0.0, "avg_logprob": -0.12057986929396952, "compression_ratio": 1.893238434163701, "no_speech_prob": 1.084514792637492e-06}, {"id": 1019, "seek": 498334, "start": 5004.7, "end": 5006.7, "text": " You're actually trying to change", "tokens": [509, 434, 767, 1382, 281, 1319], "temperature": 0.0, "avg_logprob": -0.12057986929396952, "compression_ratio": 1.893238434163701, "no_speech_prob": 1.084514792637492e-06}, {"id": 1020, "seek": 498334, "start": 5006.9800000000005, "end": 5011.06, "text": " Something about how you do business how you do marketing how you do logistics", "tokens": [6595, 466, 577, 291, 360, 1606, 577, 291, 360, 6370, 577, 291, 360, 27420], "temperature": 0.0, "avg_logprob": -0.12057986929396952, "compression_ratio": 1.893238434163701, "no_speech_prob": 1.084514792637492e-06}, {"id": 1021, "seek": 501106, "start": 5011.06, "end": 5014.9800000000005, "text": " So the thing you actually care about is how the things are related to each other", "tokens": [407, 264, 551, 291, 767, 1127, 466, 307, 577, 264, 721, 366, 4077, 281, 1184, 661], "temperature": 0.0, "avg_logprob": -0.19579376712922128, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.540399226651061e-06}, {"id": 1022, "seek": 501106, "start": 5018.34, "end": 5019.22, "text": " All right, I'm sorry", "tokens": [1057, 558, 11, 286, 478, 2597], "temperature": 0.0, "avg_logprob": -0.19579376712922128, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.540399226651061e-06}, {"id": 1023, "seek": 501106, "start": 5019.22, "end": 5023.780000000001, "text": " Can you explain again when you scroll up and you were looking at the sale price you're may looking at the entire model?", "tokens": [1664, 291, 2903, 797, 562, 291, 11369, 493, 293, 291, 645, 1237, 412, 264, 8680, 3218, 291, 434, 815, 1237, 412, 264, 2302, 2316, 30], "temperature": 0.0, "avg_logprob": -0.19579376712922128, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.540399226651061e-06}, {"id": 1024, "seek": 501106, "start": 5023.780000000001, "end": 5025.780000000001, "text": " And you saw that dip", "tokens": [400, 291, 1866, 300, 10460], "temperature": 0.0, "avg_logprob": -0.19579376712922128, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.540399226651061e-06}, {"id": 1025, "seek": 501106, "start": 5028.22, "end": 5033.820000000001, "text": " And you said something about that dip didn't signify what we thought it did can you explain why yeah", "tokens": [400, 291, 848, 746, 466, 300, 10460, 994, 380, 1465, 2505, 437, 321, 1194, 309, 630, 393, 291, 2903, 983, 1338], "temperature": 0.0, "avg_logprob": -0.19579376712922128, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.540399226651061e-06}, {"id": 1026, "seek": 501106, "start": 5034.54, "end": 5036.54, "text": " So this is like a classic", "tokens": [407, 341, 307, 411, 257, 7230], "temperature": 0.0, "avg_logprob": -0.19579376712922128, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.540399226651061e-06}, {"id": 1027, "seek": 503654, "start": 5036.54, "end": 5043.58, "text": " Boring univariate plot right so this is basically just taking all of the dots all of the auctions plotting year-made", "tokens": [363, 3662, 517, 592, 3504, 473, 7542, 558, 370, 341, 307, 1936, 445, 1940, 439, 295, 264, 15026, 439, 295, 264, 1609, 3916, 41178, 1064, 12, 10341], "temperature": 0.0, "avg_logprob": -0.2739504560639587, "compression_ratio": 1.556701030927835, "no_speech_prob": 8.579215773352189e-07}, {"id": 1028, "seek": 503654, "start": 5043.7, "end": 5047.18, "text": " against sale price, and we're going to just fitting a", "tokens": [1970, 8680, 3218, 11, 293, 321, 434, 516, 281, 445, 15669, 257], "temperature": 0.0, "avg_logprob": -0.2739504560639587, "compression_ratio": 1.556701030927835, "no_speech_prob": 8.579215773352189e-07}, {"id": 1029, "seek": 503654, "start": 5047.98, "end": 5050.94, "text": " rough average through them and so", "tokens": [5903, 4274, 807, 552, 293, 370], "temperature": 0.0, "avg_logprob": -0.2739504560639587, "compression_ratio": 1.556701030927835, "no_speech_prob": 8.579215773352189e-07}, {"id": 1030, "seek": 503654, "start": 5053.94, "end": 5056.9, "text": " It's true that products made between", "tokens": [467, 311, 2074, 300, 3383, 1027, 1296], "temperature": 0.0, "avg_logprob": -0.2739504560639587, "compression_ratio": 1.556701030927835, "no_speech_prob": 8.579215773352189e-07}, {"id": 1031, "seek": 503654, "start": 5057.42, "end": 5059.42, "text": " 1992 and", "tokens": [23952, 293], "temperature": 0.0, "avg_logprob": -0.2739504560639587, "compression_ratio": 1.556701030927835, "no_speech_prob": 8.579215773352189e-07}, {"id": 1032, "seek": 503654, "start": 5059.66, "end": 5063.54, "text": " 1997 on average in our data set being sold for less", "tokens": [22383, 322, 4274, 294, 527, 1412, 992, 885, 3718, 337, 1570], "temperature": 0.0, "avg_logprob": -0.2739504560639587, "compression_ratio": 1.556701030927835, "no_speech_prob": 8.579215773352189e-07}, {"id": 1033, "seek": 506354, "start": 5063.54, "end": 5070.06, "text": " So like very often in business you'll hear somebody look at something like this, and they'll be like oh we should", "tokens": [407, 411, 588, 2049, 294, 1606, 291, 603, 1568, 2618, 574, 412, 746, 411, 341, 11, 293, 436, 603, 312, 411, 1954, 321, 820], "temperature": 0.0, "avg_logprob": -0.15055089468484398, "compression_ratio": 1.6970954356846473, "no_speech_prob": 3.2377420211560093e-06}, {"id": 1034, "seek": 506354, "start": 5070.86, "end": 5077.86, "text": " We should stop auctioning equipment that is made in that year in those years because like we're getting less money for for example", "tokens": [492, 820, 1590, 24139, 278, 5927, 300, 307, 1027, 294, 300, 1064, 294, 729, 924, 570, 411, 321, 434, 1242, 1570, 1460, 337, 337, 1365], "temperature": 0.0, "avg_logprob": -0.15055089468484398, "compression_ratio": 1.6970954356846473, "no_speech_prob": 3.2377420211560093e-06}, {"id": 1035, "seek": 506354, "start": 5079.18, "end": 5082.5, "text": " But if the truth actually is that during those years", "tokens": [583, 498, 264, 3494, 767, 307, 300, 1830, 729, 924], "temperature": 0.0, "avg_logprob": -0.15055089468484398, "compression_ratio": 1.6970954356846473, "no_speech_prob": 3.2377420211560093e-06}, {"id": 1036, "seek": 506354, "start": 5084.74, "end": 5087.38, "text": " It's just that people were making more", "tokens": [467, 311, 445, 300, 561, 645, 1455, 544], "temperature": 0.0, "avg_logprob": -0.15055089468484398, "compression_ratio": 1.6970954356846473, "no_speech_prob": 3.2377420211560093e-06}, {"id": 1037, "seek": 508738, "start": 5087.38, "end": 5095.22, "text": " Small industrial equipment where you would expect it to be sold for less and actually our profit on it is just as high", "tokens": [15287, 9987, 5927, 689, 291, 576, 2066, 309, 281, 312, 3718, 337, 1570, 293, 767, 527, 7475, 322, 309, 307, 445, 382, 1090], "temperature": 0.0, "avg_logprob": -0.16015239790374158, "compression_ratio": 1.7935222672064777, "no_speech_prob": 2.190771056120866e-06}, {"id": 1038, "seek": 508738, "start": 5095.38, "end": 5097.38, "text": " for instance or", "tokens": [337, 5197, 420], "temperature": 0.0, "avg_logprob": -0.16015239790374158, "compression_ratio": 1.7935222672064777, "no_speech_prob": 2.190771056120866e-06}, {"id": 1039, "seek": 508738, "start": 5097.58, "end": 5103.3, "text": " During those years. It's not that it's not things made during those years now would have", "tokens": [6842, 729, 924, 13, 467, 311, 406, 300, 309, 311, 406, 721, 1027, 1830, 729, 924, 586, 576, 362], "temperature": 0.0, "avg_logprob": -0.16015239790374158, "compression_ratio": 1.7935222672064777, "no_speech_prob": 2.190771056120866e-06}, {"id": 1040, "seek": 508738, "start": 5104.22, "end": 5106.42, "text": " Would be cheaper. It's that during those years", "tokens": [6068, 312, 12284, 13, 467, 311, 300, 1830, 729, 924], "temperature": 0.0, "avg_logprob": -0.16015239790374158, "compression_ratio": 1.7935222672064777, "no_speech_prob": 2.190771056120866e-06}, {"id": 1041, "seek": 508738, "start": 5107.900000000001, "end": 5112.7, "text": " When we were selling things in those years they were cheaper because like there was a recession going on", "tokens": [1133, 321, 645, 6511, 721, 294, 729, 924, 436, 645, 12284, 570, 411, 456, 390, 257, 24828, 516, 322], "temperature": 0.0, "avg_logprob": -0.16015239790374158, "compression_ratio": 1.7935222672064777, "no_speech_prob": 2.190771056120866e-06}, {"id": 1042, "seek": 508738, "start": 5112.78, "end": 5116.5, "text": " So if you're trying to like actually take some action based on this", "tokens": [407, 498, 291, 434, 1382, 281, 411, 767, 747, 512, 3069, 2361, 322, 341], "temperature": 0.0, "avg_logprob": -0.16015239790374158, "compression_ratio": 1.7935222672064777, "no_speech_prob": 2.190771056120866e-06}, {"id": 1043, "seek": 511650, "start": 5116.5, "end": 5123.26, "text": " You probably don't just care about the fact that things made in those years are cheaper on average, but how does that impact?", "tokens": [509, 1391, 500, 380, 445, 1127, 466, 264, 1186, 300, 721, 1027, 294, 729, 924, 366, 12284, 322, 4274, 11, 457, 577, 775, 300, 2712, 30], "temperature": 0.0, "avg_logprob": -0.15948914006813286, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.521559508750215e-06}, {"id": 1044, "seek": 511650, "start": 5123.98, "end": 5125.98, "text": " today, you know so", "tokens": [965, 11, 291, 458, 370], "temperature": 0.0, "avg_logprob": -0.15948914006813286, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.521559508750215e-06}, {"id": 1045, "seek": 511650, "start": 5127.14, "end": 5133.38, "text": " So this this approach where we actually say let's try and remove all of these externalities", "tokens": [407, 341, 341, 3109, 689, 321, 767, 584, 718, 311, 853, 293, 4159, 439, 295, 613, 8320, 1088], "temperature": 0.0, "avg_logprob": -0.15948914006813286, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.521559508750215e-06}, {"id": 1046, "seek": 511650, "start": 5133.54, "end": 5139.46, "text": " So if something is sold on the same day to the same person of the same kind of vehicle", "tokens": [407, 498, 746, 307, 3718, 322, 264, 912, 786, 281, 264, 912, 954, 295, 264, 912, 733, 295, 5864], "temperature": 0.0, "avg_logprob": -0.15948914006813286, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.521559508750215e-06}, {"id": 1047, "seek": 513946, "start": 5139.46, "end": 5146.62, "text": " Then actually how does year made impact price and so this basically says for example if I am deciding", "tokens": [1396, 767, 577, 775, 1064, 1027, 2712, 3218, 293, 370, 341, 1936, 1619, 337, 1365, 498, 286, 669, 17990], "temperature": 0.0, "avg_logprob": -0.27002195032631476, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.6015993626060663e-06}, {"id": 1048, "seek": 513946, "start": 5147.42, "end": 5151.54, "text": " What to buy at an option then this is kind of saying to me, okay like", "tokens": [708, 281, 2256, 412, 364, 3614, 550, 341, 307, 733, 295, 1566, 281, 385, 11, 1392, 411], "temperature": 0.0, "avg_logprob": -0.27002195032631476, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.6015993626060663e-06}, {"id": 1049, "seek": 513946, "start": 5152.06, "end": 5157.82, "text": " Getting a more recent vehicle on average really does on average give you more money", "tokens": [13674, 257, 544, 5162, 5864, 322, 4274, 534, 775, 322, 4274, 976, 291, 544, 1460], "temperature": 0.0, "avg_logprob": -0.27002195032631476, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.6015993626060663e-06}, {"id": 1050, "seek": 513946, "start": 5158.5, "end": 5162.74, "text": " Which is not what the kind of the naive univariate plot said", "tokens": [3013, 307, 406, 437, 264, 733, 295, 264, 29052, 517, 592, 3504, 473, 7542, 848], "temperature": 0.0, "avg_logprob": -0.27002195032631476, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.6015993626060663e-06}, {"id": 1051, "seek": 513946, "start": 5164.66, "end": 5166.66, "text": " How's it Tyler", "tokens": [1012, 311, 309, 16869], "temperature": 0.0, "avg_logprob": -0.27002195032631476, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.6015993626060663e-06}, {"id": 1052, "seek": 516666, "start": 5166.66, "end": 5168.66, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.24152198590730367, "compression_ratio": 1.576271186440678, "no_speech_prob": 9.167736425297335e-05}, {"id": 1053, "seek": 516666, "start": 5173.34, "end": 5178.5, "text": " For like this bulldozer bulldozers made in 2010 probably are not", "tokens": [1171, 411, 341, 4693, 2595, 4527, 4693, 2595, 41698, 1027, 294, 9657, 1391, 366, 406], "temperature": 0.0, "avg_logprob": -0.24152198590730367, "compression_ratio": 1.576271186440678, "no_speech_prob": 9.167736425297335e-05}, {"id": 1054, "seek": 516666, "start": 5178.98, "end": 5183.139999999999, "text": " Close to the type of bulldozers that were made in 1960", "tokens": [16346, 281, 264, 2010, 295, 4693, 2595, 41698, 300, 645, 1027, 294, 16157], "temperature": 0.0, "avg_logprob": -0.24152198590730367, "compression_ratio": 1.576271186440678, "no_speech_prob": 9.167736425297335e-05}, {"id": 1055, "seek": 516666, "start": 5183.139999999999, "end": 5187.38, "text": " right, and if you're taking something that would be so", "tokens": [558, 11, 293, 498, 291, 434, 1940, 746, 300, 576, 312, 370], "temperature": 0.0, "avg_logprob": -0.24152198590730367, "compression_ratio": 1.576271186440678, "no_speech_prob": 9.167736425297335e-05}, {"id": 1056, "seek": 518738, "start": 5187.38, "end": 5196.74, "text": " very different like a 2010 bulldozer and then trying to just drop it to say oh if it was made in 1960 that", "tokens": [588, 819, 411, 257, 9657, 4693, 2595, 4527, 293, 550, 1382, 281, 445, 3270, 309, 281, 584, 1954, 498, 309, 390, 1027, 294, 16157, 300], "temperature": 0.0, "avg_logprob": -0.24428183309147866, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.4144700344331795e-06}, {"id": 1057, "seek": 518738, "start": 5196.9400000000005, "end": 5198.62, "text": " May cause", "tokens": [1891, 3082], "temperature": 0.0, "avg_logprob": -0.24428183309147866, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.4144700344331795e-06}, {"id": 1058, "seek": 518738, "start": 5198.62, "end": 5200.22, "text": " poor", "tokens": [4716], "temperature": 0.0, "avg_logprob": -0.24428183309147866, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.4144700344331795e-06}, {"id": 1059, "seek": 518738, "start": 5200.22, "end": 5206.66, "text": " Prediction at a point because it's so far outside absolutely training absolutely so you know I think that's a good point. It's you know it's a", "tokens": [32969, 4105, 412, 257, 935, 570, 309, 311, 370, 1400, 2380, 3122, 3097, 3122, 370, 291, 458, 286, 519, 300, 311, 257, 665, 935, 13, 467, 311, 291, 458, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.24428183309147866, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.4144700344331795e-06}, {"id": 1060, "seek": 518738, "start": 5207.82, "end": 5209.14, "text": " limitation", "tokens": [27432], "temperature": 0.0, "avg_logprob": -0.24428183309147866, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.4144700344331795e-06}, {"id": 1061, "seek": 518738, "start": 5209.14, "end": 5213.18, "text": " Of a random forest is if you're got a kind of data point", "tokens": [2720, 257, 4974, 6719, 307, 498, 291, 434, 658, 257, 733, 295, 1412, 935], "temperature": 0.0, "avg_logprob": -0.24428183309147866, "compression_ratio": 1.537037037037037, "no_speech_prob": 1.4144700344331795e-06}, {"id": 1062, "seek": 521318, "start": 5213.18, "end": 5218.46, "text": " That's like of a client you know which is kind of like in a part of the space that it's not seen before like", "tokens": [663, 311, 411, 295, 257, 6423, 291, 458, 597, 307, 733, 295, 411, 294, 257, 644, 295, 264, 1901, 300, 309, 311, 406, 1612, 949, 411], "temperature": 0.0, "avg_logprob": -0.1755517323811849, "compression_ratio": 1.692, "no_speech_prob": 3.905454832420219e-06}, {"id": 1063, "seek": 521318, "start": 5218.780000000001, "end": 5224.02, "text": " Maybe people didn't put air conditioning really in bulldozers in 1960 and you're saying how much would this", "tokens": [2704, 561, 994, 380, 829, 1988, 21901, 534, 294, 4693, 2595, 41698, 294, 16157, 293, 291, 434, 1566, 577, 709, 576, 341], "temperature": 0.0, "avg_logprob": -0.1755517323811849, "compression_ratio": 1.692, "no_speech_prob": 3.905454832420219e-06}, {"id": 1064, "seek": 521318, "start": 5224.42, "end": 5229.38, "text": " Bulldozer with air conditioning have gone for in 1960 you don't really have any information to know that so", "tokens": [14131, 2595, 4527, 365, 1988, 21901, 362, 2780, 337, 294, 16157, 291, 500, 380, 534, 362, 604, 1589, 281, 458, 300, 370], "temperature": 0.0, "avg_logprob": -0.1755517323811849, "compression_ratio": 1.692, "no_speech_prob": 3.905454832420219e-06}, {"id": 1065, "seek": 521318, "start": 5230.66, "end": 5232.66, "text": " You know you it's a", "tokens": [509, 458, 291, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.1755517323811849, "compression_ratio": 1.692, "no_speech_prob": 3.905454832420219e-06}, {"id": 1066, "seek": 521318, "start": 5235.14, "end": 5240.18, "text": " It's it's this is still the best technique I know of but it's it's not perfect", "tokens": [467, 311, 309, 311, 341, 307, 920, 264, 1151, 6532, 286, 458, 295, 457, 309, 311, 309, 311, 406, 2176], "temperature": 0.0, "avg_logprob": -0.1755517323811849, "compression_ratio": 1.692, "no_speech_prob": 3.905454832420219e-06}, {"id": 1067, "seek": 524018, "start": 5240.18, "end": 5246.62, "text": " And you know you kind of hope that the trees are still going to find some", "tokens": [400, 291, 458, 291, 733, 295, 1454, 300, 264, 5852, 366, 920, 516, 281, 915, 512], "temperature": 0.0, "avg_logprob": -0.2056688698389197, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.034850697578804e-06}, {"id": 1068, "seek": 524018, "start": 5247.860000000001, "end": 5250.42, "text": " Useful truth even though it hasn't seen that", "tokens": [8278, 906, 3494, 754, 1673, 309, 6132, 380, 1612, 300], "temperature": 0.0, "avg_logprob": -0.2056688698389197, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.034850697578804e-06}, {"id": 1069, "seek": 524018, "start": 5251.38, "end": 5254.5, "text": " combination of features before but yeah, it's something to be aware of", "tokens": [6562, 295, 4122, 949, 457, 1338, 11, 309, 311, 746, 281, 312, 3650, 295], "temperature": 0.0, "avg_logprob": -0.2056688698389197, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.034850697578804e-06}, {"id": 1070, "seek": 524018, "start": 5256.740000000001, "end": 5261.66, "text": " So you can also do the same thing in a PDP", "tokens": [407, 291, 393, 611, 360, 264, 912, 551, 294, 257, 10464, 47], "temperature": 0.0, "avg_logprob": -0.2056688698389197, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.034850697578804e-06}, {"id": 1071, "seek": 524018, "start": 5262.14, "end": 5267.9800000000005, "text": " Interaction plot and a PDP interaction plot which is really what I'm trying to get to here is like how to sale elapsed and", "tokens": [5751, 2894, 7542, 293, 257, 10464, 47, 9285, 7542, 597, 307, 534, 437, 286, 478, 1382, 281, 483, 281, 510, 307, 411, 577, 281, 8680, 806, 2382, 292, 293], "temperature": 0.0, "avg_logprob": -0.2056688698389197, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.034850697578804e-06}, {"id": 1072, "seek": 526798, "start": 5267.98, "end": 5274.66, "text": " Year made together impact price and so if I do a PDP interaction plot it shows me", "tokens": [10289, 1027, 1214, 2712, 3218, 293, 370, 498, 286, 360, 257, 10464, 47, 9285, 7542, 309, 3110, 385], "temperature": 0.0, "avg_logprob": -0.21893402699674114, "compression_ratio": 1.7772277227722773, "no_speech_prob": 2.8130036753282184e-06}, {"id": 1073, "seek": 526798, "start": 5275.179999999999, "end": 5277.179999999999, "text": " sale elapsed versus price", "tokens": [8680, 806, 2382, 292, 5717, 3218], "temperature": 0.0, "avg_logprob": -0.21893402699674114, "compression_ratio": 1.7772277227722773, "no_speech_prob": 2.8130036753282184e-06}, {"id": 1074, "seek": 526798, "start": 5277.7, "end": 5282.74, "text": " It shows me year made versus price and it shows me the combination", "tokens": [467, 3110, 385, 1064, 1027, 5717, 3218, 293, 309, 3110, 385, 264, 6562], "temperature": 0.0, "avg_logprob": -0.21893402699674114, "compression_ratio": 1.7772277227722773, "no_speech_prob": 2.8130036753282184e-06}, {"id": 1075, "seek": 526798, "start": 5283.339999999999, "end": 5290.139999999999, "text": " Versus price remember this is always log of price. That's why these prices look weird right and so you can see that the combination of sale", "tokens": [12226, 301, 3218, 1604, 341, 307, 1009, 3565, 295, 3218, 13, 663, 311, 983, 613, 7901, 574, 3657, 558, 293, 370, 291, 393, 536, 300, 264, 6562, 295, 8680], "temperature": 0.0, "avg_logprob": -0.21893402699674114, "compression_ratio": 1.7772277227722773, "no_speech_prob": 2.8130036753282184e-06}, {"id": 1076, "seek": 526798, "start": 5290.139999999999, "end": 5293.259999999999, "text": " Elapsed and year made is as you would expect", "tokens": [2699, 2382, 292, 293, 1064, 1027, 307, 382, 291, 576, 2066], "temperature": 0.0, "avg_logprob": -0.21893402699674114, "compression_ratio": 1.7772277227722773, "no_speech_prob": 2.8130036753282184e-06}, {"id": 1077, "seek": 529326, "start": 5293.26, "end": 5297.34, "text": " later dates so more or less time", "tokens": [1780, 11691, 370, 544, 420, 1570, 565], "temperature": 0.0, "avg_logprob": -0.2443124118604158, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.7264550226391293e-06}, {"id": 1078, "seek": 529326, "start": 5299.62, "end": 5301.02, "text": " Is", "tokens": [1119], "temperature": 0.0, "avg_logprob": -0.2443124118604158, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.7264550226391293e-06}, {"id": 1079, "seek": 529326, "start": 5301.02, "end": 5307.5, "text": " Giving me. Oh, sorry. It's the other way around isn't it so the highest prices", "tokens": [28983, 385, 13, 876, 11, 2597, 13, 467, 311, 264, 661, 636, 926, 1943, 380, 309, 370, 264, 6343, 7901], "temperature": 0.0, "avg_logprob": -0.2443124118604158, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.7264550226391293e-06}, {"id": 1080, "seek": 529326, "start": 5308.14, "end": 5313.14, "text": " Those where there's the least elapsed and the most recent year made", "tokens": [3950, 689, 456, 311, 264, 1935, 806, 2382, 292, 293, 264, 881, 5162, 1064, 1027], "temperature": 0.0, "avg_logprob": -0.2443124118604158, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.7264550226391293e-06}, {"id": 1081, "seek": 529326, "start": 5314.22, "end": 5320.02, "text": " So you can see here. There's the univariate relationship between sale elapsed and price", "tokens": [407, 291, 393, 536, 510, 13, 821, 311, 264, 517, 592, 3504, 473, 2480, 1296, 8680, 806, 2382, 292, 293, 3218], "temperature": 0.0, "avg_logprob": -0.2443124118604158, "compression_ratio": 1.5083798882681565, "no_speech_prob": 2.7264550226391293e-06}, {"id": 1082, "seek": 532002, "start": 5320.02, "end": 5324.860000000001, "text": " And here is the univariate relationship between year made and price", "tokens": [400, 510, 307, 264, 517, 592, 3504, 473, 2480, 1296, 1064, 1027, 293, 3218], "temperature": 0.0, "avg_logprob": -0.15558463335037231, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.288727384642698e-06}, {"id": 1083, "seek": 532002, "start": 5325.780000000001, "end": 5329.900000000001, "text": " And then here is the combination of the two", "tokens": [400, 550, 510, 307, 264, 6562, 295, 264, 732], "temperature": 0.0, "avg_logprob": -0.15558463335037231, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.288727384642698e-06}, {"id": 1084, "seek": 532002, "start": 5331.5, "end": 5336.18, "text": " It's enough to see like clearly that these two things are driving price together", "tokens": [467, 311, 1547, 281, 536, 411, 4448, 300, 613, 732, 721, 366, 4840, 3218, 1214], "temperature": 0.0, "avg_logprob": -0.15558463335037231, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.288727384642698e-06}, {"id": 1085, "seek": 532002, "start": 5337.18, "end": 5342.860000000001, "text": " You can also see these are not like simple diagonal lines, so it's kind of some interesting interaction going on", "tokens": [509, 393, 611, 536, 613, 366, 406, 411, 2199, 21539, 3876, 11, 370, 309, 311, 733, 295, 512, 1880, 9285, 516, 322], "temperature": 0.0, "avg_logprob": -0.15558463335037231, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.288727384642698e-06}, {"id": 1086, "seek": 532002, "start": 5343.740000000001, "end": 5346.660000000001, "text": " And so based on looking at these plots", "tokens": [400, 370, 2361, 322, 1237, 412, 613, 28609], "temperature": 0.0, "avg_logprob": -0.15558463335037231, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.288727384642698e-06}, {"id": 1087, "seek": 534666, "start": 5346.66, "end": 5353.5599999999995, "text": " It's enough to make me think oh we should maybe put in some kind of interaction term and see what happens", "tokens": [467, 311, 1547, 281, 652, 385, 519, 1954, 321, 820, 1310, 829, 294, 512, 733, 295, 9285, 1433, 293, 536, 437, 2314], "temperature": 0.0, "avg_logprob": -0.2277126689948658, "compression_ratio": 1.5446808510638297, "no_speech_prob": 1.0188068699790165e-06}, {"id": 1088, "seek": 534666, "start": 5353.86, "end": 5357.0199999999995, "text": " So let's come back to that in a moment, but let's just look at a couple more", "tokens": [407, 718, 311, 808, 646, 281, 300, 294, 257, 1623, 11, 457, 718, 311, 445, 574, 412, 257, 1916, 544], "temperature": 0.0, "avg_logprob": -0.2277126689948658, "compression_ratio": 1.5446808510638297, "no_speech_prob": 1.0188068699790165e-06}, {"id": 1089, "seek": 534666, "start": 5359.26, "end": 5361.78, "text": " Remember in this case I did one hot encoding", "tokens": [5459, 294, 341, 1389, 286, 630, 472, 2368, 43430], "temperature": 0.0, "avg_logprob": -0.2277126689948658, "compression_ratio": 1.5446808510638297, "no_speech_prob": 1.0188068699790165e-06}, {"id": 1090, "seek": 534666, "start": 5362.3, "end": 5366.48, "text": " Way back at the top here. I said max n cat equals 7", "tokens": [9558, 646, 412, 264, 1192, 510, 13, 286, 848, 11469, 297, 3857, 6915, 1614], "temperature": 0.0, "avg_logprob": -0.2277126689948658, "compression_ratio": 1.5446808510638297, "no_speech_prob": 1.0188068699790165e-06}, {"id": 1091, "seek": 534666, "start": 5366.98, "end": 5368.98, "text": " so I've got like", "tokens": [370, 286, 600, 658, 411], "temperature": 0.0, "avg_logprob": -0.2277126689948658, "compression_ratio": 1.5446808510638297, "no_speech_prob": 1.0188068699790165e-06}, {"id": 1092, "seek": 534666, "start": 5369.18, "end": 5373.62, "text": " Enclosure erots with AC so if you've got one hot encoded variables", "tokens": [2193, 3474, 7641, 1189, 1971, 365, 8157, 370, 498, 291, 600, 658, 472, 2368, 2058, 12340, 9102], "temperature": 0.0, "avg_logprob": -0.2277126689948658, "compression_ratio": 1.5446808510638297, "no_speech_prob": 1.0188068699790165e-06}, {"id": 1093, "seek": 537362, "start": 5373.62, "end": 5377.38, "text": " You can pass an array of them", "tokens": [509, 393, 1320, 364, 10225, 295, 552], "temperature": 0.0, "avg_logprob": -0.2783395468470562, "compression_ratio": 1.6519337016574585, "no_speech_prob": 7.224424507512595e-07}, {"id": 1094, "seek": 537362, "start": 5378.099999999999, "end": 5382.62, "text": " To pit plot PDP and it'll treat them as a category", "tokens": [1407, 10147, 7542, 10464, 47, 293, 309, 603, 2387, 552, 382, 257, 7719], "temperature": 0.0, "avg_logprob": -0.2783395468470562, "compression_ratio": 1.6519337016574585, "no_speech_prob": 7.224424507512595e-07}, {"id": 1095, "seek": 537362, "start": 5383.0599999999995, "end": 5389.94, "text": " Right and so in this case. I'm going to create a PDP plot of these three categories. I'm going to call it enclosure", "tokens": [1779, 293, 370, 294, 341, 1389, 13, 286, 478, 516, 281, 1884, 257, 10464, 47, 7542, 295, 613, 1045, 10479, 13, 286, 478, 516, 281, 818, 309, 34093], "temperature": 0.0, "avg_logprob": -0.2783395468470562, "compression_ratio": 1.6519337016574585, "no_speech_prob": 7.224424507512595e-07}, {"id": 1096, "seek": 537362, "start": 5391.26, "end": 5393.62, "text": " And I can see here that", "tokens": [400, 286, 393, 536, 510, 300], "temperature": 0.0, "avg_logprob": -0.2783395468470562, "compression_ratio": 1.6519337016574585, "no_speech_prob": 7.224424507512595e-07}, {"id": 1097, "seek": 537362, "start": 5394.98, "end": 5396.98, "text": " enclosure erots with AC", "tokens": [34093, 1189, 1971, 365, 8157], "temperature": 0.0, "avg_logprob": -0.2783395468470562, "compression_ratio": 1.6519337016574585, "no_speech_prob": 7.224424507512595e-07}, {"id": 1098, "seek": 537362, "start": 5398.34, "end": 5401.98, "text": " on average are more expensive than enclosure erots and", "tokens": [322, 4274, 366, 544, 5124, 813, 34093, 1189, 1971, 293], "temperature": 0.0, "avg_logprob": -0.2783395468470562, "compression_ratio": 1.6519337016574585, "no_speech_prob": 7.224424507512595e-07}, {"id": 1099, "seek": 540198, "start": 5401.98, "end": 5406.74, "text": " And enclosure erots it actually looks like enclosure erots and closure erots are pretty similar", "tokens": [400, 34093, 1189, 1971, 309, 767, 1542, 411, 34093, 1189, 1971, 293, 24653, 1189, 1971, 366, 1238, 2531], "temperature": 0.0, "avg_logprob": -0.2426578712463379, "compression_ratio": 1.7219730941704037, "no_speech_prob": 5.0147168622061145e-06}, {"id": 1100, "seek": 540198, "start": 5407.139999999999, "end": 5409.179999999999, "text": " Where else erots with AC is higher?", "tokens": [2305, 1646, 1189, 1971, 365, 8157, 307, 2946, 30], "temperature": 0.0, "avg_logprob": -0.2426578712463379, "compression_ratio": 1.7219730941704037, "no_speech_prob": 5.0147168622061145e-06}, {"id": 1101, "seek": 540198, "start": 5410.299999999999, "end": 5416.58, "text": " So this is you know at this point. You know I'd probably be inclined to hop into Google and like type erots and erots", "tokens": [407, 341, 307, 291, 458, 412, 341, 935, 13, 509, 458, 286, 1116, 1391, 312, 28173, 281, 3818, 666, 3329, 293, 411, 2010, 1189, 1971, 293, 1189, 1971], "temperature": 0.0, "avg_logprob": -0.2426578712463379, "compression_ratio": 1.7219730941704037, "no_speech_prob": 5.0147168622061145e-06}, {"id": 1102, "seek": 540198, "start": 5417.379999999999, "end": 5419.379999999999, "text": " And find out what the hell these things are", "tokens": [400, 915, 484, 437, 264, 4921, 613, 721, 366], "temperature": 0.0, "avg_logprob": -0.2426578712463379, "compression_ratio": 1.7219730941704037, "no_speech_prob": 5.0147168622061145e-06}, {"id": 1103, "seek": 540198, "start": 5420.0599999999995, "end": 5422.0599999999995, "text": " And here we go", "tokens": [400, 510, 321, 352], "temperature": 0.0, "avg_logprob": -0.2426578712463379, "compression_ratio": 1.7219730941704037, "no_speech_prob": 5.0147168622061145e-06}, {"id": 1104, "seek": 540198, "start": 5422.98, "end": 5425.179999999999, "text": " So it turns out that erots is", "tokens": [407, 309, 4523, 484, 300, 1189, 1971, 307], "temperature": 0.0, "avg_logprob": -0.2426578712463379, "compression_ratio": 1.7219730941704037, "no_speech_prob": 5.0147168622061145e-06}, {"id": 1105, "seek": 540198, "start": 5426.419999999999, "end": 5430.0199999999995, "text": " enclosed rollover predictive structure and so", "tokens": [42089, 744, 1913, 331, 35521, 3877, 293, 370], "temperature": 0.0, "avg_logprob": -0.2426578712463379, "compression_ratio": 1.7219730941704037, "no_speech_prob": 5.0147168622061145e-06}, {"id": 1106, "seek": 543002, "start": 5430.02, "end": 5432.580000000001, "text": " it turns out that if your", "tokens": [309, 4523, 484, 300, 498, 428], "temperature": 0.0, "avg_logprob": -0.1561651329199473, "compression_ratio": 1.8177777777777777, "no_speech_prob": 4.7850676310190465e-06}, {"id": 1107, "seek": 543002, "start": 5436.1, "end": 5441.06, "text": " Your bulldozer is fully enclosed then optionally you can also get air conditioning", "tokens": [2260, 4693, 2595, 4527, 307, 4498, 42089, 550, 3614, 379, 291, 393, 611, 483, 1988, 21901], "temperature": 0.0, "avg_logprob": -0.1561651329199473, "compression_ratio": 1.8177777777777777, "no_speech_prob": 4.7850676310190465e-06}, {"id": 1108, "seek": 543002, "start": 5441.22, "end": 5445.860000000001, "text": " So it turns out that actually this thing is telling us whether it's got air conditioning if it's an open structure", "tokens": [407, 309, 4523, 484, 300, 767, 341, 551, 307, 3585, 505, 1968, 309, 311, 658, 1988, 21901, 498, 309, 311, 364, 1269, 3877], "temperature": 0.0, "avg_logprob": -0.1561651329199473, "compression_ratio": 1.8177777777777777, "no_speech_prob": 4.7850676310190465e-06}, {"id": 1109, "seek": 543002, "start": 5445.860000000001, "end": 5451.14, "text": " Then obviously you don't have air conditioning at all so that's what these three levels are and so we've now learnt", "tokens": [1396, 2745, 291, 500, 380, 362, 1988, 21901, 412, 439, 370, 300, 311, 437, 613, 1045, 4358, 366, 293, 370, 321, 600, 586, 18991], "temperature": 0.0, "avg_logprob": -0.1561651329199473, "compression_ratio": 1.8177777777777777, "no_speech_prob": 4.7850676310190465e-06}, {"id": 1110, "seek": 543002, "start": 5452.02, "end": 5456.780000000001, "text": " All other things being equal the same bulldozer sold at the same time", "tokens": [1057, 661, 721, 885, 2681, 264, 912, 4693, 2595, 4527, 3718, 412, 264, 912, 565], "temperature": 0.0, "avg_logprob": -0.1561651329199473, "compression_ratio": 1.8177777777777777, "no_speech_prob": 4.7850676310190465e-06}, {"id": 1111, "seek": 545678, "start": 5456.78, "end": 5461.66, "text": " Built at the same time sold to the same person is going to be quite a bit more expensive", "tokens": [49822, 412, 264, 912, 565, 3718, 281, 264, 912, 954, 307, 516, 281, 312, 1596, 257, 857, 544, 5124], "temperature": 0.0, "avg_logprob": -0.16134400801225143, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.668842282422702e-06}, {"id": 1112, "seek": 545678, "start": 5461.82, "end": 5468.86, "text": " As if it has air conditioning than if it doesn't okay, so again. We're kind of getting this nice interpretation ability and", "tokens": [1018, 498, 309, 575, 1988, 21901, 813, 498, 309, 1177, 380, 1392, 11, 370, 797, 13, 492, 434, 733, 295, 1242, 341, 1481, 14174, 3485, 293], "temperature": 0.0, "avg_logprob": -0.16134400801225143, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.668842282422702e-06}, {"id": 1113, "seek": 545678, "start": 5470.0199999999995, "end": 5472.179999999999, "text": " You know now that I spent some time with this data set", "tokens": [509, 458, 586, 300, 286, 4418, 512, 565, 365, 341, 1412, 992], "temperature": 0.0, "avg_logprob": -0.16134400801225143, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.668842282422702e-06}, {"id": 1114, "seek": 545678, "start": 5472.179999999999, "end": 5477.78, "text": " I've certainly noticed that this you know knowing this is the most important thing you do notice that there's a lot more", "tokens": [286, 600, 3297, 5694, 300, 341, 291, 458, 5276, 341, 307, 264, 881, 1021, 551, 291, 360, 3449, 300, 456, 311, 257, 688, 544], "temperature": 0.0, "avg_logprob": -0.16134400801225143, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.668842282422702e-06}, {"id": 1115, "seek": 545678, "start": 5479.0199999999995, "end": 5484.74, "text": " Air-conditioned bulldozers nowadays, and they used to be and so there's definitely an interaction between kind of date and that", "tokens": [5774, 12, 18882, 849, 292, 4693, 2595, 41698, 13434, 11, 293, 436, 1143, 281, 312, 293, 370, 456, 311, 2138, 364, 9285, 1296, 733, 295, 4002, 293, 300], "temperature": 0.0, "avg_logprob": -0.16134400801225143, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.668842282422702e-06}, {"id": 1116, "seek": 548474, "start": 5484.74, "end": 5489.219999999999, "text": " So based on that earlier interaction analysis. I've tried", "tokens": [407, 2361, 322, 300, 3071, 9285, 5215, 13, 286, 600, 3031], "temperature": 0.0, "avg_logprob": -0.2542708470271184, "compression_ratio": 1.5671641791044777, "no_speech_prob": 5.338110895536374e-06}, {"id": 1117, "seek": 548474, "start": 5490.179999999999, "end": 5495.58, "text": " First of all setting everything before 1950 to 1950 because it seems to be some kind of missing value", "tokens": [2386, 295, 439, 3287, 1203, 949, 18141, 281, 18141, 570, 309, 2544, 281, 312, 512, 733, 295, 5361, 2158], "temperature": 0.0, "avg_logprob": -0.2542708470271184, "compression_ratio": 1.5671641791044777, "no_speech_prob": 5.338110895536374e-06}, {"id": 1118, "seek": 548474, "start": 5496.26, "end": 5498.74, "text": " I've been set age to be equal to", "tokens": [286, 600, 668, 992, 3205, 281, 312, 2681, 281], "temperature": 0.0, "avg_logprob": -0.2542708470271184, "compression_ratio": 1.5671641791044777, "no_speech_prob": 5.338110895536374e-06}, {"id": 1119, "seek": 548474, "start": 5499.7, "end": 5502.0199999999995, "text": " sale year minus year made", "tokens": [8680, 1064, 3175, 1064, 1027], "temperature": 0.0, "avg_logprob": -0.2542708470271184, "compression_ratio": 1.5671641791044777, "no_speech_prob": 5.338110895536374e-06}, {"id": 1120, "seek": 548474, "start": 5503.34, "end": 5506.7, "text": " and so then I try running a random forest on that and", "tokens": [293, 370, 550, 286, 853, 2614, 257, 4974, 6719, 322, 300, 293], "temperature": 0.0, "avg_logprob": -0.2542708470271184, "compression_ratio": 1.5671641791044777, "no_speech_prob": 5.338110895536374e-06}, {"id": 1121, "seek": 548474, "start": 5507.86, "end": 5512.4, "text": " Indeed age is now the single biggest thing", "tokens": [15061, 3205, 307, 586, 264, 2167, 3880, 551], "temperature": 0.0, "avg_logprob": -0.2542708470271184, "compression_ratio": 1.5671641791044777, "no_speech_prob": 5.338110895536374e-06}, {"id": 1122, "seek": 551240, "start": 5512.4, "end": 5515.0, "text": " Sale elapsed is way back down here", "tokens": [48922, 806, 2382, 292, 307, 636, 646, 760, 510], "temperature": 0.0, "avg_logprob": -0.24427321938907398, "compression_ratio": 1.6255924170616114, "no_speech_prob": 4.0294326026923954e-06}, {"id": 1123, "seek": 551240, "start": 5516.2, "end": 5520.74, "text": " Year made is back down here, so we've kind of used this to find", "tokens": [10289, 1027, 307, 646, 760, 510, 11, 370, 321, 600, 733, 295, 1143, 341, 281, 915], "temperature": 0.0, "avg_logprob": -0.24427321938907398, "compression_ratio": 1.6255924170616114, "no_speech_prob": 4.0294326026923954e-06}, {"id": 1124, "seek": 551240, "start": 5521.4, "end": 5523.4, "text": " an interaction", "tokens": [364, 9285], "temperature": 0.0, "avg_logprob": -0.24427321938907398, "compression_ratio": 1.6255924170616114, "no_speech_prob": 4.0294326026923954e-06}, {"id": 1125, "seek": 551240, "start": 5524.5199999999995, "end": 5530.36, "text": " But remember of course a random forest can creator it can create an interaction through having multiple split points", "tokens": [583, 1604, 295, 1164, 257, 4974, 6719, 393, 14181, 309, 393, 1884, 364, 9285, 807, 1419, 3866, 7472, 2793], "temperature": 0.0, "avg_logprob": -0.24427321938907398, "compression_ratio": 1.6255924170616114, "no_speech_prob": 4.0294326026923954e-06}, {"id": 1126, "seek": 551240, "start": 5530.36, "end": 5534.44, "text": " So we shouldn't assume that this is actually going to be a better result", "tokens": [407, 321, 4659, 380, 6552, 300, 341, 307, 767, 516, 281, 312, 257, 1101, 1874], "temperature": 0.0, "avg_logprob": -0.24427321938907398, "compression_ratio": 1.6255924170616114, "no_speech_prob": 4.0294326026923954e-06}, {"id": 1127, "seek": 551240, "start": 5535.44, "end": 5536.639999999999, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.24427321938907398, "compression_ratio": 1.6255924170616114, "no_speech_prob": 4.0294326026923954e-06}, {"id": 1128, "seek": 551240, "start": 5536.639999999999, "end": 5538.679999999999, "text": " in practice I actually found when I", "tokens": [294, 3124, 286, 767, 1352, 562, 286], "temperature": 0.0, "avg_logprob": -0.24427321938907398, "compression_ratio": 1.6255924170616114, "no_speech_prob": 4.0294326026923954e-06}, {"id": 1129, "seek": 553868, "start": 5538.68, "end": 5542.16, "text": " looked at my score and my", "tokens": [2956, 412, 452, 6175, 293, 452], "temperature": 0.0, "avg_logprob": -0.22395139270358616, "compression_ratio": 1.4756756756756757, "no_speech_prob": 1.1189393944732728e-06}, {"id": 1130, "seek": 553868, "start": 5542.64, "end": 5549.26, "text": " RMSC adding age was actually a little worse and we'll see about that later probably in the next lesson", "tokens": [497, 10288, 34, 5127, 3205, 390, 767, 257, 707, 5324, 293, 321, 603, 536, 466, 300, 1780, 1391, 294, 264, 958, 6898], "temperature": 0.0, "avg_logprob": -0.22395139270358616, "compression_ratio": 1.4756756756756757, "no_speech_prob": 1.1189393944732728e-06}, {"id": 1131, "seek": 553868, "start": 5550.400000000001, "end": 5552.400000000001, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.22395139270358616, "compression_ratio": 1.4756756756756757, "no_speech_prob": 1.1189393944732728e-06}, {"id": 1132, "seek": 553868, "start": 5554.76, "end": 5558.12, "text": " So one last thing is tree interpreter", "tokens": [407, 472, 1036, 551, 307, 4230, 34132], "temperature": 0.0, "avg_logprob": -0.22395139270358616, "compression_ratio": 1.4756756756756757, "no_speech_prob": 1.1189393944732728e-06}, {"id": 1133, "seek": 553868, "start": 5559.16, "end": 5560.96, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.22395139270358616, "compression_ratio": 1.4756756756756757, "no_speech_prob": 1.1189393944732728e-06}, {"id": 1134, "seek": 553868, "start": 5560.96, "end": 5566.400000000001, "text": " This is also in the category of things that most people don't know exist, but it's super important", "tokens": [639, 307, 611, 294, 264, 7719, 295, 721, 300, 881, 561, 500, 380, 458, 2514, 11, 457, 309, 311, 1687, 1021], "temperature": 0.0, "avg_logprob": -0.22395139270358616, "compression_ratio": 1.4756756756756757, "no_speech_prob": 1.1189393944732728e-06}, {"id": 1135, "seek": 556640, "start": 5566.4, "end": 5573.679999999999, "text": " almost pointless for like Kaggle competitions, but super important for real life and here's the idea", "tokens": [1920, 32824, 337, 411, 48751, 22631, 26185, 11, 457, 1687, 1021, 337, 957, 993, 293, 510, 311, 264, 1558], "temperature": 0.0, "avg_logprob": -0.22104815074375697, "compression_ratio": 1.52803738317757, "no_speech_prob": 2.6015959520009346e-06}, {"id": 1136, "seek": 556640, "start": 5574.48, "end": 5577.679999999999, "text": " let's say you're an insurance company and", "tokens": [718, 311, 584, 291, 434, 364, 7214, 2237, 293], "temperature": 0.0, "avg_logprob": -0.22104815074375697, "compression_ratio": 1.52803738317757, "no_speech_prob": 2.6015959520009346e-06}, {"id": 1137, "seek": 556640, "start": 5578.759999999999, "end": 5580.759999999999, "text": " somebody rings up and you give them a quote and", "tokens": [2618, 11136, 493, 293, 291, 976, 552, 257, 6513, 293], "temperature": 0.0, "avg_logprob": -0.22104815074375697, "compression_ratio": 1.52803738317757, "no_speech_prob": 2.6015959520009346e-06}, {"id": 1138, "seek": 556640, "start": 5581.5599999999995, "end": 5583.5599999999995, "text": " They say oh, that's", "tokens": [814, 584, 1954, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.22104815074375697, "compression_ratio": 1.52803738317757, "no_speech_prob": 2.6015959520009346e-06}, {"id": 1139, "seek": 556640, "start": 5583.879999999999, "end": 5586.24, "text": " $500 more than last year why?", "tokens": [1848, 7526, 544, 813, 1036, 1064, 983, 30], "temperature": 0.0, "avg_logprob": -0.22104815074375697, "compression_ratio": 1.52803738317757, "no_speech_prob": 2.6015959520009346e-06}, {"id": 1140, "seek": 556640, "start": 5586.96, "end": 5592.96, "text": " Okay, so in general you've made a prediction from some model and somebody asks why and", "tokens": [1033, 11, 370, 294, 2674, 291, 600, 1027, 257, 17630, 490, 512, 2316, 293, 2618, 8962, 983, 293], "temperature": 0.0, "avg_logprob": -0.22104815074375697, "compression_ratio": 1.52803738317757, "no_speech_prob": 2.6015959520009346e-06}, {"id": 1141, "seek": 559296, "start": 5592.96, "end": 5600.2, "text": " And so this is where we use this method called tree interpreter and what tree interpreter does is", "tokens": [400, 370, 341, 307, 689, 321, 764, 341, 3170, 1219, 4230, 34132, 293, 437, 4230, 34132, 775, 307], "temperature": 0.0, "avg_logprob": -0.21518898010253906, "compression_ratio": 1.6635071090047393, "no_speech_prob": 5.626392294288962e-07}, {"id": 1142, "seek": 559296, "start": 5600.44, "end": 5604.36, "text": " It allows us to take a particular row", "tokens": [467, 4045, 505, 281, 747, 257, 1729, 5386], "temperature": 0.0, "avg_logprob": -0.21518898010253906, "compression_ratio": 1.6635071090047393, "no_speech_prob": 5.626392294288962e-07}, {"id": 1143, "seek": 559296, "start": 5604.84, "end": 5607.28, "text": " So in this case, we're going to pick", "tokens": [407, 294, 341, 1389, 11, 321, 434, 516, 281, 1888], "temperature": 0.0, "avg_logprob": -0.21518898010253906, "compression_ratio": 1.6635071090047393, "no_speech_prob": 5.626392294288962e-07}, {"id": 1144, "seek": 559296, "start": 5607.88, "end": 5613.4, "text": " Row number zero right so here here is row zero right presumably. This is like a year made", "tokens": [20309, 1230, 4018, 558, 370, 510, 510, 307, 5386, 4018, 558, 26742, 13, 639, 307, 411, 257, 1064, 1027], "temperature": 0.0, "avg_logprob": -0.21518898010253906, "compression_ratio": 1.6635071090047393, "no_speech_prob": 5.626392294288962e-07}, {"id": 1145, "seek": 559296, "start": 5613.4, "end": 5619.52, "text": " I don't know what all the codes stand for but like his is all of the columns in row zero", "tokens": [286, 500, 380, 458, 437, 439, 264, 14211, 1463, 337, 457, 411, 702, 307, 439, 295, 264, 13766, 294, 5386, 4018], "temperature": 0.0, "avg_logprob": -0.21518898010253906, "compression_ratio": 1.6635071090047393, "no_speech_prob": 5.626392294288962e-07}, {"id": 1146, "seek": 561952, "start": 5619.52, "end": 5627.160000000001, "text": " What I can do with a tree interpreter is I can go ti dot predict pass in my random forest", "tokens": [708, 286, 393, 360, 365, 257, 4230, 34132, 307, 286, 393, 352, 8757, 5893, 6069, 1320, 294, 452, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.23438184555262737, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1147, "seek": 561952, "start": 5627.76, "end": 5630.46, "text": " Pass in my row so this would be like this particular", "tokens": [10319, 294, 452, 5386, 370, 341, 576, 312, 411, 341, 1729], "temperature": 0.0, "avg_logprob": -0.23438184555262737, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1148, "seek": 561952, "start": 5630.88, "end": 5637.92, "text": " Customers insurance information or this in this case this particular auction right and it'll give me back", "tokens": [16649, 433, 7214, 1589, 420, 341, 294, 341, 1389, 341, 1729, 24139, 558, 293, 309, 603, 976, 385, 646], "temperature": 0.0, "avg_logprob": -0.23438184555262737, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1149, "seek": 561952, "start": 5638.360000000001, "end": 5640.76, "text": " Three things the first is the prediction", "tokens": [6244, 721, 264, 700, 307, 264, 17630], "temperature": 0.0, "avg_logprob": -0.23438184555262737, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1150, "seek": 561952, "start": 5641.4400000000005, "end": 5643.4400000000005, "text": " from the random forest", "tokens": [490, 264, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.23438184555262737, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1151, "seek": 564344, "start": 5643.44, "end": 5649.599999999999, "text": " The second is the bias the bias is basically the average sale price", "tokens": [440, 1150, 307, 264, 12577, 264, 12577, 307, 1936, 264, 4274, 8680, 3218], "temperature": 0.0, "avg_logprob": -0.23752646976047093, "compression_ratio": 1.7108433734939759, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1152, "seek": 564344, "start": 5650.32, "end": 5657.679999999999, "text": " Across the whole original data set right so like remember in our random forest we started with single trees", "tokens": [34527, 264, 1379, 3380, 1412, 992, 558, 370, 411, 1604, 294, 527, 4974, 6719, 321, 1409, 365, 2167, 5852], "temperature": 0.0, "avg_logprob": -0.23752646976047093, "compression_ratio": 1.7108433734939759, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1153, "seek": 564344, "start": 5663.759999999999, "end": 5668.299999999999, "text": " Well we haven't got a draw in there anymore, but remember we started with a single tree in our random forest", "tokens": [1042, 321, 2378, 380, 658, 257, 2642, 294, 456, 3602, 11, 457, 1604, 321, 1409, 365, 257, 2167, 4230, 294, 527, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.23752646976047093, "compression_ratio": 1.7108433734939759, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1154, "seek": 566830, "start": 5668.3, "end": 5675.66, "text": " First and we split it once and then we split that once and then we split that once right and we said like oh", "tokens": [2386, 293, 321, 7472, 309, 1564, 293, 550, 321, 7472, 300, 1564, 293, 550, 321, 7472, 300, 1564, 558, 293, 321, 848, 411, 1954], "temperature": 0.0, "avg_logprob": -0.17610475205883538, "compression_ratio": 2.132978723404255, "no_speech_prob": 6.083578796278744e-07}, {"id": 1155, "seek": 566830, "start": 5675.66, "end": 5679.54, "text": " What's the average value for the whole data set then?", "tokens": [708, 311, 264, 4274, 2158, 337, 264, 1379, 1412, 992, 550, 30], "temperature": 0.0, "avg_logprob": -0.17610475205883538, "compression_ratio": 2.132978723404255, "no_speech_prob": 6.083578796278744e-07}, {"id": 1156, "seek": 566830, "start": 5679.54, "end": 5682.900000000001, "text": " What's the average value for those where the first split was true?", "tokens": [708, 311, 264, 4274, 2158, 337, 729, 689, 264, 700, 7472, 390, 2074, 30], "temperature": 0.0, "avg_logprob": -0.17610475205883538, "compression_ratio": 2.132978723404255, "no_speech_prob": 6.083578796278744e-07}, {"id": 1157, "seek": 566830, "start": 5682.900000000001, "end": 5690.38, "text": " And then what's the average value where the next split was also true until eventually you get down to the leaf nodes where you've got?", "tokens": [400, 550, 437, 311, 264, 4274, 2158, 689, 264, 958, 7472, 390, 611, 2074, 1826, 4728, 291, 483, 760, 281, 264, 10871, 13891, 689, 291, 600, 658, 30], "temperature": 0.0, "avg_logprob": -0.17610475205883538, "compression_ratio": 2.132978723404255, "no_speech_prob": 6.083578796278744e-07}, {"id": 1158, "seek": 566830, "start": 5690.38, "end": 5692.38, "text": " The average value you predict right?", "tokens": [440, 4274, 2158, 291, 6069, 558, 30], "temperature": 0.0, "avg_logprob": -0.17610475205883538, "compression_ratio": 2.132978723404255, "no_speech_prob": 6.083578796278744e-07}, {"id": 1159, "seek": 569238, "start": 5692.38, "end": 5698.78, "text": " So you can kind of think of it this way if this for a single tree if this is our final leaf node right?", "tokens": [407, 291, 393, 733, 295, 519, 295, 309, 341, 636, 498, 341, 337, 257, 2167, 4230, 498, 341, 307, 527, 2572, 10871, 9984, 558, 30], "temperature": 0.0, "avg_logprob": -0.12509331476120722, "compression_ratio": 1.7796610169491525, "no_speech_prob": 6.681504487460188e-07}, {"id": 1160, "seek": 569238, "start": 5698.78, "end": 5705.22, "text": " Maybe we're predicting like nine point one right and then maybe the average log sale price for the whole", "tokens": [2704, 321, 434, 32884, 411, 4949, 935, 472, 558, 293, 550, 1310, 264, 4274, 3565, 8680, 3218, 337, 264, 1379], "temperature": 0.0, "avg_logprob": -0.12509331476120722, "compression_ratio": 1.7796610169491525, "no_speech_prob": 6.681504487460188e-07}, {"id": 1161, "seek": 569238, "start": 5707.1, "end": 5712.3, "text": " The whole lot is like ten point two right that's the average for all the options", "tokens": [440, 1379, 688, 307, 411, 2064, 935, 732, 558, 300, 311, 264, 4274, 337, 439, 264, 3956], "temperature": 0.0, "avg_logprob": -0.12509331476120722, "compression_ratio": 1.7796610169491525, "no_speech_prob": 6.681504487460188e-07}, {"id": 1162, "seek": 569238, "start": 5712.3, "end": 5718.06, "text": " And so you could kind of like work your way down here, so let's go and create this", "tokens": [400, 370, 291, 727, 733, 295, 411, 589, 428, 636, 760, 510, 11, 370, 718, 311, 352, 293, 1884, 341], "temperature": 0.0, "avg_logprob": -0.12509331476120722, "compression_ratio": 1.7796610169491525, "no_speech_prob": 6.681504487460188e-07}, {"id": 1163, "seek": 571806, "start": 5718.06, "end": 5721.780000000001, "text": " That's actually go and run this so we can see it okay", "tokens": [663, 311, 767, 352, 293, 1190, 341, 370, 321, 393, 536, 309, 1392], "temperature": 0.0, "avg_logprob": -0.21427744262072504, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.760337454470573e-06}, {"id": 1164, "seek": 571806, "start": 5723.5, "end": 5728.06, "text": " So let's go back and redraw this single tree you'll find like in", "tokens": [407, 718, 311, 352, 646, 293, 2182, 5131, 341, 2167, 4230, 291, 603, 915, 411, 294], "temperature": 0.0, "avg_logprob": -0.21427744262072504, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.760337454470573e-06}, {"id": 1165, "seek": 571806, "start": 5728.46, "end": 5733.14, "text": " Jupyter notebooks often a lot of the things we create like", "tokens": [22125, 88, 391, 43782, 2049, 257, 688, 295, 264, 721, 321, 1884, 411], "temperature": 0.0, "avg_logprob": -0.21427744262072504, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.760337454470573e-06}, {"id": 1166, "seek": 571806, "start": 5734.1, "end": 5739.46, "text": " Videos progress bars and stuff they don't know how to like save themselves to the file", "tokens": [25903, 4205, 10228, 293, 1507, 436, 500, 380, 458, 577, 281, 411, 3155, 2969, 281, 264, 3991], "temperature": 0.0, "avg_logprob": -0.21427744262072504, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.760337454470573e-06}, {"id": 1167, "seek": 571806, "start": 5739.46, "end": 5743.46, "text": " So you'll see just like a little string here, and so you actually have to rerun it", "tokens": [407, 291, 603, 536, 445, 411, 257, 707, 6798, 510, 11, 293, 370, 291, 767, 362, 281, 43819, 409, 309], "temperature": 0.0, "avg_logprob": -0.21427744262072504, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.760337454470573e-06}, {"id": 1168, "seek": 571806, "start": 5744.14, "end": 5745.780000000001, "text": " to create the", "tokens": [281, 1884, 264], "temperature": 0.0, "avg_logprob": -0.21427744262072504, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.760337454470573e-06}, {"id": 1169, "seek": 574578, "start": 5745.78, "end": 5747.78, "text": " string", "tokens": [6798], "temperature": 0.0, "avg_logprob": -0.1738293038474189, "compression_ratio": 1.7548387096774194, "no_speech_prob": 2.3320678792515537e-06}, {"id": 1170, "seek": 574578, "start": 5749.66, "end": 5752.099999999999, "text": " So this was the single tree that we created", "tokens": [407, 341, 390, 264, 2167, 4230, 300, 321, 2942], "temperature": 0.0, "avg_logprob": -0.1738293038474189, "compression_ratio": 1.7548387096774194, "no_speech_prob": 2.3320678792515537e-06}, {"id": 1171, "seek": 574578, "start": 5757.139999999999, "end": 5761.62, "text": " So the whole data set had an average log sale price of 10.2", "tokens": [407, 264, 1379, 1412, 992, 632, 364, 4274, 3565, 8680, 3218, 295, 1266, 13, 17], "temperature": 0.0, "avg_logprob": -0.1738293038474189, "compression_ratio": 1.7548387096774194, "no_speech_prob": 2.3320678792515537e-06}, {"id": 1172, "seek": 574578, "start": 5762.9, "end": 5765.7, "text": " The data set for those with coupler system equals true", "tokens": [440, 1412, 992, 337, 729, 365, 1384, 22732, 1185, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.1738293038474189, "compression_ratio": 1.7548387096774194, "no_speech_prob": 2.3320678792515537e-06}, {"id": 1173, "seek": 574578, "start": 5766.66, "end": 5768.66, "text": " Had an average of 10.3", "tokens": [12298, 364, 4274, 295, 1266, 13, 18], "temperature": 0.0, "avg_logprob": -0.1738293038474189, "compression_ratio": 1.7548387096774194, "no_speech_prob": 2.3320678792515537e-06}, {"id": 1174, "seek": 576866, "start": 5768.66, "end": 5777.099999999999, "text": " The data set for coupler system equals true enclosure less than point less than two was nine point nine and", "tokens": [440, 1412, 992, 337, 1384, 22732, 1185, 6915, 2074, 34093, 1570, 813, 935, 1570, 813, 732, 390, 4949, 935, 4949, 293], "temperature": 0.0, "avg_logprob": -0.19866119112287248, "compression_ratio": 1.5373831775700935, "no_speech_prob": 7.11242762463371e-07}, {"id": 1175, "seek": 576866, "start": 5777.74, "end": 5783.0599999999995, "text": " Then eventually we get all the way up here and also a model ID less than 4573", "tokens": [1396, 4728, 321, 483, 439, 264, 636, 493, 510, 293, 611, 257, 2316, 7348, 1570, 813, 6905, 33396], "temperature": 0.0, "avg_logprob": -0.19866119112287248, "compression_ratio": 1.5373831775700935, "no_speech_prob": 7.11242762463371e-07}, {"id": 1176, "seek": 576866, "start": 5783.099999999999, "end": 5788.66, "text": " It's ten point two so you could kind of like say okay. Why did this particular?", "tokens": [467, 311, 2064, 935, 732, 370, 291, 727, 733, 295, 411, 584, 1392, 13, 1545, 630, 341, 1729, 30], "temperature": 0.0, "avg_logprob": -0.19866119112287248, "compression_ratio": 1.5373831775700935, "no_speech_prob": 7.11242762463371e-07}, {"id": 1177, "seek": 576866, "start": 5790.0599999999995, "end": 5793.0599999999995, "text": " Row let's say we had a row that ended up over in this leaf node", "tokens": [20309, 718, 311, 584, 321, 632, 257, 5386, 300, 4590, 493, 670, 294, 341, 10871, 9984], "temperature": 0.0, "avg_logprob": -0.19866119112287248, "compression_ratio": 1.5373831775700935, "no_speech_prob": 7.11242762463371e-07}, {"id": 1178, "seek": 579306, "start": 5793.06, "end": 5800.9800000000005, "text": " Why did we predict ten point two well it's because we started with ten point one nine and then because the coupler system was", "tokens": [1545, 630, 321, 6069, 2064, 935, 732, 731, 309, 311, 570, 321, 1409, 365, 2064, 935, 472, 4949, 293, 550, 570, 264, 1384, 22732, 1185, 390], "temperature": 0.0, "avg_logprob": -0.15556297496873506, "compression_ratio": 2.0, "no_speech_prob": 1.5057003110996448e-06}, {"id": 1179, "seek": 579306, "start": 5801.54, "end": 5804.02, "text": " Was less than point five so it was actually false", "tokens": [3027, 1570, 813, 935, 1732, 370, 309, 390, 767, 7908], "temperature": 0.0, "avg_logprob": -0.15556297496873506, "compression_ratio": 2.0, "no_speech_prob": 1.5057003110996448e-06}, {"id": 1180, "seek": 579306, "start": 5805.38, "end": 5807.1, "text": " We added", "tokens": [492, 3869], "temperature": 0.0, "avg_logprob": -0.15556297496873506, "compression_ratio": 2.0, "no_speech_prob": 1.5057003110996448e-06}, {"id": 1181, "seek": 579306, "start": 5807.1, "end": 5812.620000000001, "text": " About point two to that so we went from ten point one to ten point three right so ten point two to ten point three", "tokens": [7769, 935, 732, 281, 300, 370, 321, 1437, 490, 2064, 935, 472, 281, 2064, 935, 1045, 558, 370, 2064, 935, 732, 281, 2064, 935, 1045], "temperature": 0.0, "avg_logprob": -0.15556297496873506, "compression_ratio": 2.0, "no_speech_prob": 1.5057003110996448e-06}, {"id": 1182, "seek": 579306, "start": 5812.620000000001, "end": 5819.580000000001, "text": " So we added a little bit because this one is true and then to go from ten point three to nine point nine", "tokens": [407, 321, 3869, 257, 707, 857, 570, 341, 472, 307, 2074, 293, 550, 281, 352, 490, 2064, 935, 1045, 281, 4949, 935, 4949], "temperature": 0.0, "avg_logprob": -0.15556297496873506, "compression_ratio": 2.0, "no_speech_prob": 1.5057003110996448e-06}, {"id": 1183, "seek": 581958, "start": 5819.58, "end": 5823.34, "text": " So because enclosure is less than two we subtracted", "tokens": [407, 570, 34093, 307, 1570, 813, 732, 321, 16390, 292], "temperature": 0.0, "avg_logprob": -0.19841163054756497, "compression_ratio": 1.623931623931624, "no_speech_prob": 6.577923272743647e-07}, {"id": 1184, "seek": 581958, "start": 5824.42, "end": 5829.94, "text": " About point four and then because model ID was less than forty five hundred we", "tokens": [7769, 935, 1451, 293, 550, 570, 2316, 7348, 390, 1570, 813, 15815, 1732, 3262, 321], "temperature": 0.0, "avg_logprob": -0.19841163054756497, "compression_ratio": 1.623931623931624, "no_speech_prob": 6.577923272743647e-07}, {"id": 1185, "seek": 581958, "start": 5830.7, "end": 5832.78, "text": " added about point seven", "tokens": [3869, 466, 935, 3407], "temperature": 0.0, "avg_logprob": -0.19841163054756497, "compression_ratio": 1.623931623931624, "no_speech_prob": 6.577923272743647e-07}, {"id": 1186, "seek": 581958, "start": 5833.22, "end": 5839.74, "text": " Right so you could see like with a single tree you could like break down like why is it that we predicted?", "tokens": [1779, 370, 291, 727, 536, 411, 365, 257, 2167, 4230, 291, 727, 411, 1821, 760, 411, 983, 307, 309, 300, 321, 19147, 30], "temperature": 0.0, "avg_logprob": -0.19841163054756497, "compression_ratio": 1.623931623931624, "no_speech_prob": 6.577923272743647e-07}, {"id": 1187, "seek": 581958, "start": 5840.42, "end": 5846.34, "text": " 10.2 right and it's like at each one of these decision points. We're adding or subtracting a little bit from the value", "tokens": [1266, 13, 17, 558, 293, 309, 311, 411, 412, 1184, 472, 295, 613, 3537, 2793, 13, 492, 434, 5127, 420, 16390, 278, 257, 707, 857, 490, 264, 2158], "temperature": 0.0, "avg_logprob": -0.19841163054756497, "compression_ratio": 1.623931623931624, "no_speech_prob": 6.577923272743647e-07}, {"id": 1188, "seek": 584634, "start": 5846.34, "end": 5852.1, "text": " So what we could then do is we could do that for all the trees and", "tokens": [407, 437, 321, 727, 550, 360, 307, 321, 727, 360, 300, 337, 439, 264, 5852, 293], "temperature": 0.0, "avg_logprob": -0.18684642917507296, "compression_ratio": 2.0108108108108107, "no_speech_prob": 5.9892687431784e-07}, {"id": 1189, "seek": 584634, "start": 5852.9400000000005, "end": 5857.06, "text": " Then we could take the average so every time we see enclosure", "tokens": [1396, 321, 727, 747, 264, 4274, 370, 633, 565, 321, 536, 34093], "temperature": 0.0, "avg_logprob": -0.18684642917507296, "compression_ratio": 2.0108108108108107, "no_speech_prob": 5.9892687431784e-07}, {"id": 1190, "seek": 584634, "start": 5858.66, "end": 5863.78, "text": " Did we increase or decrease the value and how much by every time we see model ID?", "tokens": [2589, 321, 3488, 420, 11514, 264, 2158, 293, 577, 709, 538, 633, 565, 321, 536, 2316, 7348, 30], "temperature": 0.0, "avg_logprob": -0.18684642917507296, "compression_ratio": 2.0108108108108107, "no_speech_prob": 5.9892687431784e-07}, {"id": 1191, "seek": 584634, "start": 5863.78, "end": 5868.9400000000005, "text": " did we increase or decrease the value and how much by and so we could take the average of all of those and", "tokens": [630, 321, 3488, 420, 11514, 264, 2158, 293, 577, 709, 538, 293, 370, 321, 727, 747, 264, 4274, 295, 439, 295, 729, 293], "temperature": 0.0, "avg_logprob": -0.18684642917507296, "compression_ratio": 2.0108108108108107, "no_speech_prob": 5.9892687431784e-07}, {"id": 1192, "seek": 584634, "start": 5870.3, "end": 5873.88, "text": " That's what ends up in this thing called contributions", "tokens": [663, 311, 437, 5314, 493, 294, 341, 551, 1219, 15725], "temperature": 0.0, "avg_logprob": -0.18684642917507296, "compression_ratio": 2.0108108108108107, "no_speech_prob": 5.9892687431784e-07}, {"id": 1193, "seek": 587388, "start": 5873.88, "end": 5876.04, "text": " So here is all of our", "tokens": [407, 510, 307, 439, 295, 527], "temperature": 0.0, "avg_logprob": -0.22489350182669504, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1194, "seek": 587388, "start": 5877.52, "end": 5879.04, "text": " predictors and", "tokens": [6069, 830, 293], "temperature": 0.0, "avg_logprob": -0.22489350182669504, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1195, "seek": 587388, "start": 5879.04, "end": 5885.16, "text": " Here is the value of each and so this is telling us and I've sorted them here that", "tokens": [1692, 307, 264, 2158, 295, 1184, 293, 370, 341, 307, 3585, 505, 293, 286, 600, 25462, 552, 510, 300], "temperature": 0.0, "avg_logprob": -0.22489350182669504, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1196, "seek": 587388, "start": 5885.88, "end": 5888.04, "text": " The fact that this thing was made in", "tokens": [440, 1186, 300, 341, 551, 390, 1027, 294], "temperature": 0.0, "avg_logprob": -0.22489350182669504, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1197, "seek": 587388, "start": 5888.84, "end": 5894.04, "text": " 1999 was the thing that most negatively impacted our prediction and", "tokens": [19952, 390, 264, 551, 300, 881, 29519, 15653, 527, 17630, 293], "temperature": 0.0, "avg_logprob": -0.22489350182669504, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1198, "seek": 587388, "start": 5894.92, "end": 5899.68, "text": " the fact that the age of the vehicle was", "tokens": [264, 1186, 300, 264, 3205, 295, 264, 5864, 390], "temperature": 0.0, "avg_logprob": -0.22489350182669504, "compression_ratio": 1.6358024691358024, "no_speech_prob": 2.0580419004545547e-06}, {"id": 1199, "seek": 589968, "start": 5899.68, "end": 5904.400000000001, "text": " 11 years was what most positively impacted", "tokens": [2975, 924, 390, 437, 881, 25795, 15653], "temperature": 0.0, "avg_logprob": -0.3136978614621046, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.0953002856695093e-05}, {"id": 1200, "seek": 589968, "start": 5909.16, "end": 5914.56, "text": " I think you actually need to sort after you zip them together. They seem to be sorted negative point five", "tokens": [286, 519, 291, 767, 643, 281, 1333, 934, 291, 20730, 552, 1214, 13, 814, 1643, 281, 312, 25462, 3671, 935, 1732], "temperature": 0.0, "avg_logprob": -0.3136978614621046, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.0953002856695093e-05}, {"id": 1201, "seek": 589968, "start": 5914.56, "end": 5918.68, "text": " Well, no values are sorted, but then they're just reassigned to the columns in the original order", "tokens": [1042, 11, 572, 4190, 366, 25462, 11, 457, 550, 436, 434, 445, 19486, 16690, 281, 264, 13766, 294, 264, 3380, 1668], "temperature": 0.0, "avg_logprob": -0.3136978614621046, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.0953002856695093e-05}, {"id": 1202, "seek": 589968, "start": 5919.52, "end": 5921.52, "text": " Which is why?", "tokens": [3013, 307, 983, 30], "temperature": 0.0, "avg_logprob": -0.3136978614621046, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.0953002856695093e-05}, {"id": 1203, "seek": 589968, "start": 5921.96, "end": 5923.96, "text": " Is what's most thank you?", "tokens": [1119, 437, 311, 881, 1309, 291, 30], "temperature": 0.0, "avg_logprob": -0.3136978614621046, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.0953002856695093e-05}, {"id": 1204, "seek": 589968, "start": 5924.4800000000005, "end": 5925.84, "text": " Thank you", "tokens": [1044, 291], "temperature": 0.0, "avg_logprob": -0.3136978614621046, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.0953002856695093e-05}, {"id": 1205, "seek": 589968, "start": 5925.84, "end": 5927.84, "text": " That makes perfect sense", "tokens": [663, 1669, 2176, 2020], "temperature": 0.0, "avg_logprob": -0.3136978614621046, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.0953002856695093e-05}, {"id": 1206, "seek": 592784, "start": 5927.84, "end": 5930.2, "text": " Yes, we need to do an index sort", "tokens": [1079, 11, 321, 643, 281, 360, 364, 8186, 1333], "temperature": 0.0, "avg_logprob": -0.19465014578282147, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.187532456649933e-06}, {"id": 1207, "seek": 592784, "start": 5930.96, "end": 5932.56, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.19465014578282147, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.187532456649933e-06}, {"id": 1208, "seek": 592784, "start": 5932.56, "end": 5935.16, "text": " Thank you. We'll make sure we fix that by next week", "tokens": [1044, 291, 13, 492, 603, 652, 988, 321, 3191, 300, 538, 958, 1243], "temperature": 0.0, "avg_logprob": -0.19465014578282147, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.187532456649933e-06}, {"id": 1209, "seek": 592784, "start": 5936.04, "end": 5938.04, "text": " So we need to sort", "tokens": [407, 321, 643, 281, 1333], "temperature": 0.0, "avg_logprob": -0.19465014578282147, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.187532456649933e-06}, {"id": 1210, "seek": 592784, "start": 5938.88, "end": 5942.5, "text": " columns by the index from contributions", "tokens": [13766, 538, 264, 8186, 490, 15725], "temperature": 0.0, "avg_logprob": -0.19465014578282147, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.187532456649933e-06}, {"id": 1211, "seek": 592784, "start": 5944.52, "end": 5948.4400000000005, "text": " So then there's this thing called bias and so the bias is just the", "tokens": [407, 550, 456, 311, 341, 551, 1219, 12577, 293, 370, 264, 12577, 307, 445, 264], "temperature": 0.0, "avg_logprob": -0.19465014578282147, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.187532456649933e-06}, {"id": 1212, "seek": 592784, "start": 5949.24, "end": 5951.0, "text": " the average", "tokens": [264, 4274], "temperature": 0.0, "avg_logprob": -0.19465014578282147, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.187532456649933e-06}, {"id": 1213, "seek": 592784, "start": 5951.0, "end": 5955.6, "text": " With before we start doing any splits, right? So if you basically start with the average", "tokens": [2022, 949, 321, 722, 884, 604, 37741, 11, 558, 30, 407, 498, 291, 1936, 722, 365, 264, 4274], "temperature": 0.0, "avg_logprob": -0.19465014578282147, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.187532456649933e-06}, {"id": 1214, "seek": 595560, "start": 5955.6, "end": 5963.660000000001, "text": " log of value and then we went down each tree and each time we saw year made we had some impact coupler system some", "tokens": [3565, 295, 2158, 293, 550, 321, 1437, 760, 1184, 4230, 293, 1184, 565, 321, 1866, 1064, 1027, 321, 632, 512, 2712, 1384, 22732, 1185, 512], "temperature": 0.0, "avg_logprob": -0.18275584266299294, "compression_ratio": 1.8225108225108224, "no_speech_prob": 2.726451839407673e-06}, {"id": 1215, "seek": 595560, "start": 5963.660000000001, "end": 5966.820000000001, "text": " Impact product size some impact and so forth, right?", "tokens": [31005, 1674, 2744, 512, 2712, 293, 370, 5220, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18275584266299294, "compression_ratio": 1.8225108225108224, "no_speech_prob": 2.726451839407673e-06}, {"id": 1216, "seek": 595560, "start": 5970.22, "end": 5975.46, "text": " Okay, so I think what we might do is we might come back to because we kind of out of time we might come back to", "tokens": [1033, 11, 370, 286, 519, 437, 321, 1062, 360, 307, 321, 1062, 808, 646, 281, 570, 321, 733, 295, 484, 295, 565, 321, 1062, 808, 646, 281], "temperature": 0.0, "avg_logprob": -0.18275584266299294, "compression_ratio": 1.8225108225108224, "no_speech_prob": 2.726451839407673e-06}, {"id": 1217, "seek": 595560, "start": 5975.46, "end": 5976.820000000001, "text": " tree interpreter", "tokens": [4230, 34132], "temperature": 0.0, "avg_logprob": -0.18275584266299294, "compression_ratio": 1.8225108225108224, "no_speech_prob": 2.726451839407673e-06}, {"id": 1218, "seek": 595560, "start": 5976.820000000001, "end": 5982.780000000001, "text": " Next time but the basic idea. This is the last this was the last of our key interpretation points", "tokens": [3087, 565, 457, 264, 3875, 1558, 13, 639, 307, 264, 1036, 341, 390, 264, 1036, 295, 527, 2141, 14174, 2793], "temperature": 0.0, "avg_logprob": -0.18275584266299294, "compression_ratio": 1.8225108225108224, "no_speech_prob": 2.726451839407673e-06}, {"id": 1219, "seek": 595560, "start": 5982.780000000001, "end": 5984.780000000001, "text": " and the basic idea is that", "tokens": [293, 264, 3875, 1558, 307, 300], "temperature": 0.0, "avg_logprob": -0.18275584266299294, "compression_ratio": 1.8225108225108224, "no_speech_prob": 2.726451839407673e-06}, {"id": 1220, "seek": 598478, "start": 5984.78, "end": 5986.78, "text": " we want some ability to", "tokens": [321, 528, 512, 3485, 281], "temperature": 0.0, "avg_logprob": -0.19203699202764601, "compression_ratio": 1.4067796610169492, "no_speech_prob": 4.425415681907907e-06}, {"id": 1221, "seek": 598478, "start": 5988.98, "end": 5992.82, "text": " Not only tell us about the model as a whole and how it works on average", "tokens": [1726, 787, 980, 505, 466, 264, 2316, 382, 257, 1379, 293, 577, 309, 1985, 322, 4274], "temperature": 0.0, "avg_logprob": -0.19203699202764601, "compression_ratio": 1.4067796610169492, "no_speech_prob": 4.425415681907907e-06}, {"id": 1222, "seek": 598478, "start": 5992.82, "end": 5996.16, "text": " But to look at how the model makes predictions for an individual", "tokens": [583, 281, 574, 412, 577, 264, 2316, 1669, 21264, 337, 364, 2609], "temperature": 0.0, "avg_logprob": -0.19203699202764601, "compression_ratio": 1.4067796610169492, "no_speech_prob": 4.425415681907907e-06}, {"id": 1223, "seek": 599616, "start": 5996.16, "end": 6014.8, "text": " Row and that's what we're doing here. Okay, great. Thanks everybody. See you on Thursday", "tokens": [50364, 20309, 293, 300, 311, 437, 321, 434, 884, 510, 13, 1033, 11, 869, 13, 2561, 2201, 13, 3008, 291, 322, 10383, 51296], "temperature": 0.0, "avg_logprob": -0.23173932234446207, "compression_ratio": 1.0476190476190477, "no_speech_prob": 9.665703146310989e-06}], "language": "en"}