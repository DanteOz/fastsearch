{"text": " So let's just yeah, let's start by reviewing Kind of what we've learned about optimizing Multi-layer functions with SGD and so the idea is that we've got some data and Then we do something to that data for example we multiply it by a weight matrix and Then we do something to that For example we put it through a softmax or a sigmoid And then we do something to that such as do a cross entropy loss or a root mean squared error loss Okay, and that's going to like give us some scalar So this is going to have no hidden layers this has got a linear layer a Nonlinear activation being a softmax and a loss function being a root mean squared error or a cross entropy All right, and then we've got our input data Put linear nonlinear loss so for example if this was sigmoid or Or softmax and this was cross entropy then that would be logistic regression So it's still a yeah Cross entropy yeah, let's do that next sure For now think of it like think of root means great error same thing some loss function Okay now We'll look at cross entropy again in the moment So how do we calculate the derivative of that with with respect to our weights right so really it would probably be better if we said X comma W here because it's really a function of the weights as well, and so we want the derivative of this With respect to our weights Sorry I put it in the wrong spot G F of X comma W I just screwed up. That's all that's why that didn't make sense all right So to do that we just basically we do the chain rule so we just say that this is equal to H of U and U equals G F or G do equals G of V and V equals F of X So we can just rewrite it like that right and then we can do the chain rule So we can say that's equal to H dash the derivative is H dash U by G dash V by F dash X Happy with all that so far okay, so in Order to take the derivative with respect to the weights therefore We just have to calculate that derivative with respect to W using that exact formula So if we had in there Yeah Yeah, so so D of all that DW would be that yeah So then if if we you know went further here and had like Another linear layer right let's go a bit more room another linear layer I Comma W2 Right so we have another linear layer There's no difference to now calculate the derivative with respect to all of the parameters We can still use the exact same Chain rule right So so don't think of the multi-layer network as being like things that occur at different times it's just a composition of functions and So we just use the chain rule to calculate all the derivatives at once you know There's a they're just a set of parameters that happen to appear in different parts of the function, but the cut that the calculus is no No different so to calculate this with respect to W1 and W2 you know it's it's just you just increase You know W. You can just now just call it W and say W1 is all of those weights So the result that's a great question, so what you're going to have then is Is A list of parameters right so Here's W1 and like it's it's it's probably some kind of higher rank tensor You know like if it's a convolution or layer It'll you know be like a rank three tensor or whatever, but we can flatten it out right? We'll just make it a list of parameters. There's W1. Here's W2 Right just another list of parameters, right And here's our loss Which is a single you know a single number? so therefore our derivative Is just a vector of that same length right? It's how much does changing that value of W affect the loss how much does changing that value of W affect the loss? right, so you can basically think of it as a function like you know y equals a x1 plus b x2 Plus C right and say like oh, what's the derivative of that with respect to a? B and C and you would have three numbers the derivative respect to a and B and C And that's all this is right with the derivative with respect to that weight that weight and that weight and that weight and that weight To get there inside the chain rule We had to calculate and I'm not going to detail here, but we had to calculate like Jacobians so like the derivative when you take a matrix product is you've now got something where you've got like a a weight matrix and You've got an input vector these are the activations from the previous layer right and you've got Some new Output activations right and so now you've got to say like okay for this particular sorry for this particular weight How does Changing this particular weight change This particular output and how does changing this particular weight change this particular output and so forth so you kind of end up with These higher dimensional tensors showing like for every weight. How does it affect? every output right But then by the time you get to the loss function The loss function is going to have like a mean or a sum or something so they're all going to get added up in the end you know and so this kind of thing like I Don't know it drives me a bit crazy to try and calculate it out by hand or even think of it step by step because you tend to have like You just have to remember for every input in a layer for every output in the next layer You know you're going to have to take out for every weight for every output You're going to have to have a separate gradient One good way to look at this is to learn to use pie torches like dot grad Attribute and dot backward method manually and like look up the tutorial the pie torch tutorials And so you can actually start setting up some calculations with a vector input and the vector output and then type dot backward and then say Type dot grad and like look at it right and then do some really small ones with just two or three items in the input and output vectors and like make the make the operation like plus two or something and like See what the shapes are make sure it makes sense Yeah, because it's kind of like this Vector matrix calculus is not like introduces zero new concepts to anything you learned in high school Like strictly speaking, but getting a feel for how these shapes Move around I find took a lot of practice. You know the good news is you almost never have to worry about it Okay, so We were Talking about Then using this kind of logistic regression For NLP and before we got to that point we were talking about using naive base for NLP And the basic idea was that we could take a document right a review like this movie is good and turn it into a bag of words representation consisting of the number of times each word appears All right, and we call this the vocabulary. This is the unique list of words. Okay, and we used the SK learn count vectorizer to automatically generate both the vocabulary which in SK learn they call they call the features And to call create the bag of words representations and the whole group of them then is called a term document matrix, okay And we kind of realized that we could calculate the probability that a Positive review contains the word this by just Averaging the number of times this appears in the positive reviews, and we could do the same for the And we could do the same for the negatives Right and then we could take the ratio of them to get something which if it's greater than one Was a word that appeared more often in the positive reviews or less than one was a word that appeared more often in the negative reviews Okay and Then we realized you know using using Bayes rule that and taking the logs That we could basically end up with something where we could add up the logs of these Plus the log of the ratio of the probabilities that things are in class one versus class zero And end up with something we can compare to zero It's a bit greater than zero then we can predict a document is positive or if it's less than zero We can predict the document is negative, and that was our base rule, right? So we kind of did that From math first principles, and I think we agreed that the naive in naive base was a good description Because it assumes independence, but it's definitely not true But it's an interesting starting point And I think it was interesting to observe when we actually got to the point where like okay now we've you know calculated the the ratio of the probabilities and To it took the log and now rather than multiply them together of course we have to add them up and When when we actually wrote that down we realized like oh That is You know just a standard Weight matrix product plus a bias Right and so then we kind of realized like oh, okay, so like if this is not very good accuracy 80% accuracy Why not improve it by saying hey we know other ways to calculate a Cut a bunch of coefficients and a bunch of biases which is to Learn them in a logistic regression right so in other words this this is the formula we use for a logistic regression And so why don't we just create a logistic regression and fit it? Okay, and it's going to be Give us the same thing, but rather than coefficients and biases which are theoretically correct based on You know this assumption of independence and based on Bayes rule. There'll be the coefficients and biases that are Actually the best in this data All right, so that was kind of where we got to and so The kind of key insight here is like Just about everything I find a machine learning ends up being either like a tree or You know a bunch of matrix products and monumeralities Right like it's everything seems to end up kind of coming down to the same thing And including as it turns out Bayes rule All right, and then it turns out that nearly all of the time then whatever the parameters are in that function Nearly all the time it turns out that they're better learned than Calculated based on theory right and indeed. That's what happened when we actually tried learning those coefficients We got you know 85 percent right, so then We noticed that we could also rather than take the whole term document matrix We could instead just take them the you know ones and zeros for presence or absence of a word And you know sometimes it was you know this equally as good But then we actually tried something else which is we tried adding regularization and with regularization The binarized approach turned out to be a little better right so then regularization Was where we took the loss function And again let's start with RMSE, and then we'll talk about cross entropy loss function was Now predictions Minus our actuals sum that up take the average plus a Penalty Okay, and so this specifically is the L2 penalty if this instead was the absolute value of W Then that would be the L1 penalty, okay? We also noted that We don't really care about the loss function per se we only care about its derivative. That's actually the thing that updates the weights So we can because this is a sum we can take the derivative of each part separately and so the derivative of this part was just that Right and so we kind of learnt that even though these are mathematically equivalent. They have different names This version is called weight decay and it's kind of what's used that term is used in the neural net literature Okay So cross entropy on the other hand you know it's just another loss function like root mean squared error But it's specifically designed for classification right And so here's an example of binary cross entropy, so let's say this is our you know is it a cat or a dog? So let's just say is cat one or a zero so cat cat dog dog cat and these are our Predictions this is the output of our final layer of our neural net or our logistic regression or whatever right then All we do is we say okay, let's take The the actual times the log of the prediction and Then we add to that 1 minus actual times the log of 1 minus the prediction and then take the negative of that whole thing Alright so I Suggested to you to you all that you tried to kind of write the if statement version of this so hopefully you've done that by now Otherwise, I'm about to spoil it for you. So this was y times log y plus 1 minus y times log 1 minus y Right and negative of that okay, so who wants to tell me how to write this as an if statement? I Chenshi hit me okay, so if y equal to Sorry if y equal to 1 then return log y. Mm-hmm otherwise Well, um house return log 1 minus 1 good. Oh, that's the thing is a brackets and you take see Why that's good. So the key insight Chen she's using is that y has two possibilities one or zero Okay, and so very often the math can hide The key insight which I think happens here until you actually think about what the values it can take right, so That's that's all it's doing. It's saying either. Give me that or give me that All right, could you pass that to the back place change? You? Find missing something but do you know the two variables in that statement? You got why? Shouldn't be like why happened a while? Oh, yeah. Thank you As usual it's may missing something Okay Okay, and so then the you know the multi Category version is just the same thing, but you're saying you know it for different more than just y equals 1 or 0 But y equals 0 1 2 3 4 5 6 7 8 9 for instance Okay and So that you know that loss function has a you can figure it out yourself and particularly simple derivative And it also you know another thing you could play with at home if you like is like thinking about how? The derivative looks when you add a sigmoid or a softmax before it you know it turns out at all Turns out very nicely because you've got an XP thing going into a loggy thing so you end up with you know very well-behaved derivatives The reason I guess there's lots of reasons that people use RMSC for regression and cross entropy for classification But most of it comes back to this statistical idea of a best linear unbiased estimator You know and based on the likelihood function it kind of turns out that these have some nice statistical properties It turns out however in practice root means grid error in particular the properties are perhaps more theoretical than actual and actually nowadays using the The absolute deviation rather than the sum of squares deviation Can often work better? So in practice like everything in machine learning I normally try both for a particular data set I'll try both loss functions and see which one works better Unless of course it's a Kaggle competition in which case you're told how Kaggle is going to judge it and you should use the same loss function as Kaggle's evaluation metric all right So yeah, so this is really the key insight is like hey Let's let's not use theory but instead learn things from the data, and you know we hope that we're going to get better results Particularly with regularization we do and then I think the key regularization insight here is hey Let's not like try to reduce the number of parameters in our model But instead like use lots of parameters and then use regularization to figure out Which ones are actually useful right and so then we took that a step further by saying hey given we can do that with Regularization let's create lots more features By adding bigrams and trigrams You know bigrams like by vast and by vengeance and trigrams like by Vengeance full stop and by Vera miles And you know just to keep things a little faster We limited it to 800,000 features, but you know even with the full 70 million features it works Just as well, and it's not a hell of a lot slower So we created a term document matrix again using the full set of n grams for the training set the validation set And so now we can go ahead and say okay our labels is the training set labels as before our independent variables is the the Binarized term document matrix as before And then let's fit a logistic regression to that And do some predictions and we get 90% accuracy, so this is looking pretty good Okay So the logistic regression Let's go back to our naive base right in a naive base We have this term document matrix and then for every feature. We're calculating the probability of that feature occurring if it's class one that probability of that feature occurring if it's class two and then the ratio of those two right and In the paper that we're actually basing this off They call this P this Q and this R right maybe I should just fill that in P Here maybe then we'll say probability to make it more obvious Okay And so then we kind of said hey, let's let's not use these ratios as the coefficients in that in that Matrix multiply, but let's instead like try and learn some coefficients. You know so maybe start out with some random numbers You know and then try and use stochastic gradient descent to find slightly better ones So you'll notice you know some important features here the R Vector is a vector of rank one and its length is equal to the number of features and Of course our logistic regression coefficient matrix is also of length one Sorry rank one and length equal to the number of features right and we're you know we're saying like they're kind of two ways of calculating the same kind of thing right one based on theory one based on data So here is like some of the numbers in R right remember. It's using the log so these numbers which are less than zero represent things which are More likely to be negative and these ones that here are more likely so this one here is more likely to be positive and so Here's either the power of that and so these are the ones we can compare to one rather than to zero So I'm going to do something that hopefully is going to seem weird And so first of all I'm going to talk about I'm going to say what we're going to do and Then I'm going to try and describe why it's weird, and then we'll talk about Why it may not be as weird as we first thought so here's what we're going to do We're going to take our term document matrix And we're going to multiply it by R So what that means is we're going to we can do it here in Excel right so we're going to say let's grab everything in our term document matrix and Multiply it by the equivalent value in the vector of R right so this is like a Broadcasted element wise multiplication not a matrix multiplication Okay And that's what that does Okay, so here is the value of the term document matrix Times R in other words everywhere that a zero appears there a zero appears here and Every time a one appears here the equivalent value of R appears here So we haven't really We haven't really changed much right we've just we've just kind of Changed the ones into something else into the into the Rs from that feature Right and so what we're now going to do is we're going to use this as our independent variables Instead in our logistic regression okay? So here we are multiplied X X NB X naive Bayes version is X times R and now let's do a logistic regression fitting using those independent variables and Let's then do that for the validation set okay and get the predictions and And lo and behold we have a better number Okay, so Let me explain why this hopefully seems surprising Given that we're just multiplying Multiplying oh I picked out the wrong ones. I should have said ah not coif Okay, that's actually ah I got the wrong number okay So that's our independent variables right and then the the logistic regression has come up with some set of coefficients Let's pretend for a moment that these are the coefficients that it happened to come up with right? We could now say well, let's not use this set. Let's not use this Set of independent variables, but let's use the original binarized feature matrix right and then divide all of our coefficients by the values in R and We're going to get exactly the same result mathematically so You know we've got our X naive Bayes version of the independent variables, and we've got some Some set of weights some support some sort of coefficients. I call it w right W1 let's say Where it's found like this is a good set of coefficients making our predictions from right that X and B is simply equal to X times as in element wise times ah right so in other words this is equal to X times ah times the weights and so like we could just change the weights to be that Right and get the same number so This ought to mean that The change that we made to the dependent variable shouldn't have made any difference Because we can calculate exactly the same thing without making that change So there's the question Why did it make a difference? So in order to answer this question I'm going to try and get you all to try and think about this in order to answer this question You need to think about like okay. What are the things that aren't? Mathematically the same why is why is it not identical? What are the reasons like come up with some hypotheses? What are some reasons that may be? we've actually ended up with a Better answer and to figure that out we need to first of all start with like well Why is it even a different answer? Why is that different to that? It's a subtle All right, what do you think I was wondering if it was two different kinds of multiplications You said that one is the element wise multiplication. No they do end up mathematically being the same okay? Pretty much. There's a minor in core, but not that it's not that it's not some order operations thing Let's try can she You are on a roll today, so let's say how you go. I feel like the features are less correlated to each other I Mean I've made a claim that these are mathematically equivalent, so So what are you saying really you know why are we getting different answers? It's good keep on coming up with hypotheses We need lots of wrong answers before we start finding it's the right ones It's like that you know a warmer hotter colder. You know Ernest you're gonna get us hotter Does it have anything to do with the regularization? Yes, and is it the fact that when you so let's start there right so earnest point here is like okay Jeremy You've said they're equivalent, but they're equivalent outcomes right, but you've got through you went through a process to get there and that process Included regularization, and they're not necessarily equivalent Regularization like our loss function has a penalty so yeah help us think through and as to how much that might impact things Well, this is maybe kind of dumb But I'm just noticing that the numbers are bigger in the ones that have been weighted by the the naive base our weights and so For these are bigger and some are smaller some are bigger right, but there are some bigger ones like the variance between the columns Is as much higher the variance is bigger. Yeah, I think that's a very interesting insight. Okay. That's all I got okay So build on that Prince has been on a roll or month, so I'm not sure Is it also consider like considering the dependency of different words is said why it is forming better rather than all Word independent of each other not really I mean it's it's you know again fear it you know theoretically these are creating mathematically equivalent outputs so They're not they're not doing something different except as Ernest mentioned They're getting impacted differently by regularization, so what's So what's regularization right? regularization is we start out with our That was the weirdest thing I forgot to go into screenwriting mode, and it just turns out that you can actually write in Excel And I had no idea that was true I still use screenwriting roads, so I don't kill up my spreadsheet. I just never tried So our loss was equal to like our cross entropy loss you know based on the Predictions or the predictions and the actuals right plus our Penalty so If you're If your weights are large Right then that piece gets bigger Right and it drowns out that piece right, but that's actually the piece we care about right we actually want it to be a good fit So we want to have as little regularization going on as we can get away with we want so we want to have less weights So here's the thing right our value Yes, can you pass it over here? When you see the less weights did you mean lesser weights I do yeah Yeah, and I kind of use the two words a little equivalently which is not quite fair I agree, but the idea is that weights that are pretty close to zero are kind of not there So here's the thing our values of R You know and I'm not a Bayesian weenie, but I'm still going to use the word prior right They're kind of like a prior for like we think that the the different levels of importance and positive or negative of these different features Might be something like that right we think that like bad You know might be More correlated with negative than than good right so our kind of implicit assumption The before was that we have no priors so in other words when we'd said Squared weights we're saying a non zero weight is something we don't want to have right, but actually I think what I really want to say is that differing from the naive Bayes expectation is Something I don't want to do right like only vary from the naive Bayes Prior unless you have good reason to believe otherwise Right and so that's actually what this ends up doing right we end up saying you know what? We think this value is probably free Right and so if you're going to like make it a lot bigger or a lot smaller Right that's going to create the kind of variation in weights. That's going to cause that squared term to go up right so So if you can you know just leave all these values about similar to where they are now Right and so that's what the penalty term is now doing right the penalty term when our inputs is already multiplied by R Is saying penalize things where we're varying it from our naive Bayes? prior Can you pass that there? Why multiply only with the R not Constant like R squared or something like that when the variance would be much higher this time because our Prior comes from an actual theoretical model right so I said like I don't like to rely on theory But I have if I have some theory Then you know maybe we should use that as our starting point rather than starting off by assuming everything's equal So our prior said hey, we've got this model called naive Bayes and the naive Bayes model said If the naive Bayes assumptions were correct Then R is the correct coefficient right in this specific formulation That that's why we pick that because our our prior is based on that that theory Okay, so this is a really interesting insight which I Never really see covered which is this idea is that we can use these like you know traditional Machine learning techniques we can imbue them with this kind of Bayesian sense by by starting out You know incorporating our theoretical expectations Into the data that we give our model right and when we do so That then means we don't have to regularize as much and that's good right because if we regularize a lot But let's try it Let's go back to You know here's our Remember the way they do it in the SKA logistic regression is this is the reciprocal of the amount of regularization penalty, so we'll kind of Add lots of regularization by making it small So that like really hurts That really hurts our accuracy because now It's trying really hard to get those weights down the loss function is overwhelmed By the need to reduce the weights and the need to make it predictive is kind of now seems totally unimportant right so So by kind of starting out and saying you know what don't push the weights down so that you end up ignoring The the terms but instead push them down so that you try to get rid of you know ignore differences from our expectation based on the naive Bayes formulation So that Ends up giving us a very nice Result which actually was originally this this technique was originally presented. I think about 2012 Chris Manning who's a terrific NLP researcher up at Stanford and Cedar Wang who I don't know, but I assume is awesome because this paper is awesome. They basically came up with this with this idea and What they did was they compared it to a number of other approaches on a number of other Datasets so one of the things they tried is this one is the IMDB data set right and so here's naive Bayes SPM on Bygrams and as you can see this approach outperformed the other linear based approaches that they looked at and also some Restricted Boltzmann machine kind of neural net based approaches they looked at now nowadays There are better ways there You know there are better ways to do this and in fact in the deep learning course We showed a new state-of-the-art result that we just developed at fast AI that gets well over 94 percent But still you know like particularly for a linear technique. That's easy fast and intuitive This is pretty good, and you'll notice when they when they did this they only used by grams And I assume that's because they I looked at their code, and it was kind of pretty slow and ugly You know I figured out a way to optimize it a lot more as you saw and so we were able to use Here trigrams, and so we get quite a lot better, so we've got 91.8 This is a 91.2, but other than that it's identical I'll also mean they used a support vector machine, which is almost identical to a logistic regression in this case So there's some minor differences right so I think that's a pretty cool result and you know I will mention You know what you get to see here in class is the result of like many Weeks and often many months of research that I do and so I don't want you to think like this stuff is obvious It's not at all like reading this paper There's no description in the paper of like Why they use this model how it's different why they thought it works? You know it took me a week or two to even realize That it's kind of like mathematically equivalent to a normal logistic regression And then a few more weeks to realize that the difference is actually in the regularization You know like this is kind of like Machine learning as I'm sure you've noticed from the Kaggle competitions you enter you know like you come up with a thousand good ideas 999 of them no matter how confident you are they're going to be great They always turn out to be shit you know and then finally after four weeks one of them finally works and Kind of gives you the enthusiasm to spend another four weeks of misery and frustration This is the norm right and and like For sure the the best Practition as I know in machine learning all share one particular trait in common which is they're very very tenacious You know also known as stubborn and bloody-minded right which is definitely a reputation. I seem to have probably fair Along with another thing which is that they're all very good coders. You know they're very good at turning their ideas into intercode So yeah So you know this was like a really interesting experience for me working through this a few months ago to try and like figure out how to how to at least You know how to explain why this at the at the time kind of state-of-the-art result exists And so once I figured that out I was actually able to build on top of it and make it quite a bit better And I'll show you what I did and this is where it was very very handy to have PyTorch at my disposal Because I was able to kind of create something that was Customized just the way that I wanted to be and also very fast by using the GPU So here's the kind of fast AI version of the NBS VM actually my friend Stephen Meridy who's a terrific Researcher in NLP has christened this the NBS VM plus plus which I thought was lovely So here is that even though there is no SVM. It's the logistic regression, but as I said nearly exactly the same thing So let me first of all show you like the code So this is like we try to like once I figure out like okay This is like the best way I can come up with to do a linear bag of words model. I kind of embed it into Fast AI so you can just write a couple lines of code so the code is basically hey I've got to create a data class for text classification. I want to create it from a bag of words Right here is my bag of words Here are my labels Here is the same thing for the validation set and use up to 2,000 unique words per review which is plenty So then from that model data Construct a learner which is kind of the fast AI generalization of a model Which is based on a dot product of naive Bayes and then fit that model and then do a few epochs and After five epochs. I was already up to 92.2 right so this is now like you know getting quite well above This this linear baseline So let me show you the code for for that So the code is like Horrifyingly short That's it Right and it'll also look on the whole Extremely familiar right there's if there's a few tweaks here Pretend this thing that says embedding pretend. It actually says linear, okay? I'm going to show you embedding in a moment pretend. It says linear So we've got basically a linear layer where the number of features coming with the number of features as the rows and remember SK learn features means number of words basically and then for each row we're going to create one Weight which makes sense right for like a logistic regression every every So not for each row for each word each word has one weight and Then we're going to be multiplying it by the the R values so for each word We have one R value per class so I actually made this so this can handle like not just Positive versus negative, but maybe figuring out like which author created this work There could be five or six authors whatever right and basically We kind of use those linear layers to to get the The value of the weight and the value of the R and then we take the weight Times the R and Then sum it up and so that's just a dot product Okay, so just just a simple dot product just as we would do for any logistic regression And then do the softmax So the very minor tweak That we add to get the better result is this the main one really is this here this plus something All right, and the thing I'm adding is It's a parameter, but I pretty much always use this this version this value 0.4 So what does this do? So what this is doing is it's again kind of changing the prior right so if you think about it Even once we use this R times the term document matrix as their independent variables You really want to start with a question okay the penalty terms are still pushing w down to zero right so what did it mean? for w to be zero right so what would it mean if we had you know Coefficient zero zero zero zero zero All right, so what that would do when we go okay this matrix times these coefficients We still get zero right so a weight of zero still ends up saying I have no opinion on whether this thing is positive or negative On the other hand if they were all one Right then it's basically says my opinion is that the naive Bayes coefficients are exactly right Okay, and so the idea is that I said Zero is almost certainly not The right prior right we shouldn't really be saying if there's no coefficient it means ignore the naive Bayes coefficient One is probably too high right because we actually think that naive Bayes is only kind of part of the answer Right and so I played around with a few different data sets where I basically said Take the weights and add to them Some constant Right and so zero would become in this case zero point four right so in other words the the regularization Penalty is pushing the weights not towards zero but towards this value Right and I found that across a number of data sets zero point four Works pretty well right and it's pretty resilient right so again This is the basic idea is to kind of like get the best of both worlds You know we're we're we're learning from the data using a simple Model, but we're incorporating You know our prior knowledge as best as we can and so it turns out when you say okay Let's let's tell it. You know as weight matrix of zeros Actually means that you should use about you know about half of the R values That ends up that ends up working better than the prior that the weights should all be zero Yes Is the the weights the W is it that the point for the amount of? required the amount of so we have this Where the we have the term where we reduce the amount of error the prediction error RMSE plus we have the regularization and Is a W the point for denote the amount of regularization required so W are the weights? Right so this is calculating our activations, okay, so we calculate our activations as being equal to the weights Times the are Some right, so that's just our normal At our normal linear function right so so the the thing which is being penalized is my weight matrix That's what gets penalized So by saying hey, you know what don't just use W use W plus point four So that's not being penalized It's not part of the weight matrix Okay, so effectively the weight matrix gets 0.4 for free So by doing this even after regularization then every Observe it every feature is getting some form of fate right some form of minimum weight or something like that I'm not necessarily because it could end up choosing a coefficient of negative point four for a feature and so that would say You know what even though naive Bayes says it's the R should be whatever for this feature. I think you should totally ignore it Yeah, great questions Okay We started at 20 past To okay, let's take a break for about eight minutes or so and start back about 25 to Okay so a couple of questions at the break the first was just for a Kind of Reminder or a bit of a summary as to what's going on? Yeah, right and so here we have W plus I'm writing it out. Yeah plus Adjusted weight a weight adjustment times Ah right so so normally what we were doing so Normally what we were doing is saying hey logistic regression is basically Wx right I'm going to ignore the bias okay, and then we were changing it to be W dot times X Right and then we were kind of saying let's do that bit first right Although in this particular case actually now. I look at it. I'm doing it in this code. It doesn't matter obviously in this code I'm actually doing I'm doing this bit first and so so This thing here actually I caught it w which is probably pretty bad. It's actually w times x right so so instead of W times x times R. I've got w times x plus a constant times R right so the key idea here is that regularization Can't draw in yellow that's fair enough Regularization Wants the weights to be zero right because we're trying to it's trying to reduce that Okay, and So what we're saying is like okay. We want to push the weights towards zero because we're saying like that's our like default starting point expectation is the weights are zero and So we want to be in a situation where if the weights are zero Then we have a model that like Makes theoretical or intuitive sense to us right? This model if the weights are zero doesn't make intuitive sense to us right because it's saying hey multiply everything by zero Gets rid of all of that and gets rid of that as well And we were actually saying no we actually think our R is useful. We actually want to keep that right so So instead we say you know what let's take that piece here and add 0.4 to it Right so now if the regularizer is pushing the weights towards zero Then it's pushing the value of this sum towards 0.4 Right and so therefore it's pushing our whole model to 0.4 times R right so in other words Now kind of default starting point if you've regularized all the weights out altogether is to say yeah You know let's use a bit of R. That's probably a good idea Okay So that's the idea right that's the idea is basically you know what happens when When that's zero right and you and you want that to like be something sensible because otherwise Regularizing the weights to move in that direction wouldn't be such a good idea Second question was about N-grams So the n in n-gram can be uni by tri whatever one two three whatever grams so for the This movie is good Right it has four Unigrams this movie is good It has three bigrams this movie movie is is good It has two trigrams This movie is Movie is good Okay Can you pass it So do you mind go back to the W-80 chase down the 0.4 stuff yeah, so I was wondering if this adjustment will harm the predictability of the model because think of extreme extreme case if it's not 0.4 if it's 4,000 and Or right coefficient will be like right essentially so so exactly so so our prior Needs to make sense and so our prior here And you know this is why it's called dot product NB is our prior is that this is something where we think naive Bayes Is a good prior right and so naive Bayes says that R equals P over That's not how you write P P over Q. I have not had much sleep P over Q is a good prior and not only do we think it's a good prior, but we think Times X Plus B is a good model That's that's the naive Bayes model so in other words we expect that You know a coefficient of one is a good coefficient not not four thousand Yeah, so we think specifically we don't think we think zero is probably not a good coefficient right, but we also think that maybe The naive Bayes version is a little overconfident, so maybe one's a little high So we're pretty sure that the right number assuming that our model and a base model is appropriate is between zero and one And no, but what I was thinking is as long as it's not zero you are pushing those coefficients that are supposed to be zero to something not zero and makes the Like high coefficients less distinctive from the coefficients well, but you see they're not supposed to be zero They're supposed to be R Like that's that's what they're supposed to be They're supposed to be R right and so and remember This is inside our forward function, so this is part of what we're taking the gradient of right so it's basically Saying okay, we're still going to you know you can still set Self dot W to anything you like But just the regularizer Wants it to be zero and So we're always saying is okay if you want it to be zero, then I'll try to make zero be You know give a sensible answer That's the basic idea and like yeah, nothing says point fours perfect for every data set I've tried a few different data sets and found various numbers between point three and point six that are optimal But I've never found one where point four is Is less good than zero which is not surprising and I've also never found one where one is better Right so the idea is like this is a reasonable default, but it's another parameter you can play with which I kind of like Right it's another thing you could use Grid search or whatever to figure out for your data set. What's best and you know really the key here being Every model before this one as far as I know has implicitly assumed It should be zero because they just they don't have this parameter right and you know by the way I've actually got a second parameter here as well Which is the same thing I do to R is actually divide R By a parameter which I'm not going to worry too much about it now But again it's another parameter you can use to kind of adjust what the nature of the regularization is You know and I mean in the end I'm a Empiricist not a theoretician. You know I thought this seemed like a good idea Nearly all of my things that seem like a good idea turn out to be stupid this particular one Dave good results you know on this data set and a few other ones as well Okay, could you pass that you were started? Yeah? Yeah, I'm still a little bit confused about the W plus W adjusted uh-huh So you mentioned that we do W plus W adjusted so that the coefficients don't get set to zero That we place some importance on the priors but you also said that the Effect of learning can be that W gets set to a negative value which effectively turns W plus W Zero so if if we are we are allowing the learning process to indeed set the priors to zero So why is that in any way different from just having W because yeah great question because of regularization because we're penalizing it by that Right so in other words We're saying you know what if you're if the best thing to do is to ignore the value of R That'll cost you you're going to have to set W to a negative number Right so only do that if that's clearly a good idea unless it's clearly a good idea, then you should leave Leave it where it is That's that's the only reason like all of this stuff. We've done today is basically entirely about You know maximizing the advantage we get from regularization and saying regularization pushes us towards some default assumption and nearly all of the machine learning literature assumes that default assumption is Everything zero, and I'm saying like it turns out You know it makes sense theoretically and turns out empirically that actually you should decide what your default assumption is And that'll give you better results, so we'd be right to say that In a way you're putting an additional hurdle in the along the way towards getting all coefficients to zero So it will be able to do that if it is really worth it. Yeah exactly So I'd say like the default hurdle without this is is Making a coefficient non zero is the hook hurdle and now I'm saying no the co-op the the hurdle is making a coefficient Not be equal to point for our So this is sum of W squared into C Sum of it is some lambda or C penalty constant Yeah, yeah time something yeah, so the weight decay should also depend on the value of C if it is very less Like if C is right by C. Do you mean this? So if a is point one then the weights might not go towards zero yes, then we might not need great decay so well the Whatever this value. I mean if the if the value of this is zero then there is no Recognization right, but if this value is higher than zero then there is some penalty Right and and presumably we've set it to non zero because we're overfitting So we want some penalty and so if there is some penalty then Then my assertion is that we should penalize things that are different to our prior Not that we should penalize things that are different to zero And our prior is that things should be you know around about equal to R Okay, let's move on thanks for the great questions. I want to talk about Embedding I Said pretend its linear and indeed we can pretend its linear Let me show you how much we can pretend its linear as in n n dot linear create a linear layer Here is our Data matrix All right here are our coefficients if we're doing the R version here our coefficients are right, so if we were to Put those into a column vector Like so right then we could do a matrix multiply of that By that right and so we're going to end up with So here's our matrix is our vector All right, so we're going to end up with One times one plus one times one one times one one times three All right zero times one zero times point three All right, and then the next one zero times one one times one so far All right, and then the next one zero times one one times one so forth okay, so like that the matrix multiply You know of this independent Variable matrix by this coefficient matrix is going to give us an answer okay, so that's that is just a matrix multiply So the question is like okay, well why didn't Jeremy write and end up linear? Why did Jeremy write and end up embedding? And the reason is because if you recall we don't actually store it like this Because this is actually of width 800,000 and of height 25,000 right so rather than storing it like this We actually store it as zero One two three right one two three four zero one two five One two four five okay All the ones one oops that's actually how we store it that is this bag of words contains Which word indexes? That make sense okay, so that's like This is like a sparse way of Storing it right is just list out the indexes in each sentence So given that I Want to now do that matrix multiply that I just showed you to create that same? outcome Right, but I want to do it from this representation So if you think about it All this is actually doing is It's saying a one-hot. You know this is basically one-hot encoded right It's kind of like a dummy dummy matrix version does it have the word this does it have the word movie does it have the word? Is and so forth? So if we took the simple version of like does it have the word this one oh? Right and we multiplied that By that Right then that's just going to return the first item that Makes sense So in general a one hot encoded vector times a matrix is Identical to to looking up that matrix to find the nth row in it Right so this is identical to saying find the zero first second and fifth coefficients Right so they're the same they're exactly the same thing and like it doesn't like in this case. I only have one Coefficient per feature right but actually the way I did this was to have One coefficient per feature for each class Right so in this case is both positive and negative So I actually had kind of like an R positive and an R negative So our negative would be just the opposite right equals that divided by that Now the binary case obviously it's redundant to have both, but what if it was like What's the author of this text is it? Jeremy or Savannah or Terrence right now? We've got three categories. We want three values of R Right so the nice thing is that in this sparse version you know you can just look up You know the zero and the first and the second and the fifth Right and again. It's identical mathematically identical to multiplying by a one hot encoded matrix but When you have sparse inputs It's obviously much much more efficient so this computational trick Which is mathematically identical to not conceptually analogous to mathematically identical to Multiplying by a one hot encoded matrix is called an embedding right, so I'm sure you've all heard or most of you probably heard about embeddings like word embeddings word to vec or glove or whatever and People love to make them sound like there's some Amazing new complex Neural net thing right they're not embedding means Make a multiplication by one hot encoded matrix faster by replacing it with a simple array lookup Okay, so that's why I said you can think of this as if it said self dot w equals nn dot linear and F plus one by one right because it actually does The same thing right it actually is a matrix with those dimensions this actually is a matrix with those dimensions All right, it's a linear layer But it's expecting that the input we're going to give it is not actually a one hot encoded matrix But it's actually a list of integers Right the indexes for each Word or for each item so you can see that the forward function in fast AI Automatically gets for this learner The feature indexes right so they come from The sparse matrix automatically numpy makes it very easy to just grab those those indexes Okay, so in other words there. We've got here. We've got a list of each word index of a of the eight hundred thousand that are in this document and So then this here says look up each of those in our embedding matrix Which is got eight hundred thousand rows and return each thing that you find Okay so mathematically identical to multiplying by the one hot encoded matrix So Makes sense, so that's all an embedding is and so what that means is We can now handle Building any kind of model like a you know whatever kind of neural network Where we have potentially very high cardinality categorical variables as our inputs We Can then just turn them into a numeric code between zero and the number of levels and then we can learn a you know a Linear layer from that as if we had one hot encoded it without ever actually constructing the one hot encoded version and Without ever actually doing that matrix multiply okay instead We will just store the index version and simply do the array lookup Okay, and so the gradients that are flowing back You know basically in the one hot encoded version everything that was a zero has no gradient So the gradients flowing back is best go to update the particular row of the embedding matrix that we used Okay, and so That's fundamentally important for NLP Just like here like you know I wanted to create a PyTorch model that would implement this this ridiculously simple little equation right and To do it without this trick would have meant. I was beating in a 25,000 by a hatred so 800,000 element array Which would have been kind of crazy right and so this this trick allowed me to write you know You know I just replaced the word linear with embedding Replace the thing that feeds the one hot encodings in with something that just feeds the indexes in And that was it that then it kept working and so this now trains You know in about a minute per epoch Okay so What we can now do is we can now take this idea and apply it not just to language But to anything right for example predicting the sales of items at a grocery Yes, where's the Just a quick question, so we are not actually looking up anything right We are just saying that now that array with the indices that is the representation So the represent so we are doing a lookup right the representation That's being stored it for the but for the bag of words is now not 1 1 1 0 0 1 but 0 1 2 5 right and so then We actually have to do our Matrix product right but rather than doing the matrix product. We look up The zero thing and the first thing and the second thing and the fifth thing So that means we are still retaining the one hot encoded matrix No, we didn't there's no one hot encoded matrix used here. This is the one hot encoded matrix, which is not currently highlighted We've currently highlighted the list of indexes and the list of coefficients from the weight matrix that Okay So what we're going to do now is we're kind of going to just go to go a step further and saying like Let's not use a linear model at all Let's use a multi-layer neural network right and let's have the input to that potentially be include some categorical variables right and those categorical variables we will just have as Numeric indexes And so the first layer for those won't be a normal linear layer There will be an embedding layer which we know behaves exactly like a linear layer mathematically And so then I hope will be that we can now use this to create a neural network for any kind of data right and so There was a competition on Kaggle a few years ago called Rossman, which is a German grocery chain Where they asked to predict the sales of items in? in their stores Right and that included the mixture of categorical and continuous variables and in this paper by Gore and Burkhan They described their third place winning entry Which was much simpler than the first place winning entry but nearly as good But much much simpler because they took advantage of this idea of what they call entity embeddings In the paper they they thought I think that they had invented this actually had been written before earlier by Yoshio Benjio and his co-authors in another Kaggle competition which was predicting taxi destinations Although I will say I feel like Gore went a lot further in describing how this can be used in many other ways And so we'll we'll talk about that as well So the So this one is actually in the is in the deep learning one repo okay deal one Lesson three okay Because we talk about some of the deep learning specific aspects in the deep learning course where else in this course we're going to be Talking mainly about the feature engineering And we're also going to be talking about you know kind of this this embedding idea So let's start with the data right so the data Was you know store number one on? the 31st of July 2015 was open They had a promotion going on It was a school holiday. It was not a state holiday, and they sold five thousand two hundred and sixty three items So That's the key Data they provided and so the goal is obviously to predict sales in a test set that has the same information without sales They also tell you that for each store It's of some particular type It sells some particular assortment of goods Its nearest competitor to competitor is some distance away the competitor opened in September 2008 and There's some more information about promos. I don't know the details of what that means like In many Kaggle competitions they let you download External data sets if you wish as long as you share them with other competitors So people oh they also told you what state each store is in so people downloaded a list of the names of the different states of Germany They downloaded a file for each state in Germany for each week Some kind of Google Trend data. I don't know what specific Google Trend they got but there was that For each date they downloaded a whole bunch of temperature information That's it and then here's the test set Okay, so I mean one interesting insight here Is that there was probably a mistake in some ways for? Rossman to design this competition as being one where you could use external data Because in reality you don't actually get to find out next week's weather or next week's Google Trends You know But you know when you're competing in Kaggle you don't care about that you just want to win so you use whatever you can get So let's talk first of all about data cleaning You know that there wasn't really much feature engineering done in this third place winning entry Like by particularly by Kaggle standards where normally every last thing counts This is a great example of how far you can get with with a neural net and it certainly reminds me of the Plains prediction competition we talked about yesterday where the winner did no feature engineering entirely relied on deep learning The laughter in the room I guess is from people who did a little bit more than no feature engineering in that competition so I should mention by the way like I Find that bit where like you work hard at a competition and then it closes and you didn't win and the winner comes out and says This is how I won like that's the bit where you learn the most right like Sometimes that's happened to me, and it's been like oh I thought of that I thought I tried that and then I go back and I realize I Like had a bug there. I didn't test properly and I learned like okay like I really need to learn to like test this thing In this different way Sometimes it's like oh, I thought of that, but I assumed it wouldn't work I've really got to remember to check everything before I make any assumptions And you know sometimes it's just like oh, I I did not think of that technique Wow now I know it's better than everything I just tried because like otherwise somebody says like hey, you know here's a really good technique You're like okay great But when you spent months trying to do something and like somebody else did it better by using that technique That's pretty convincing Right and so like it's kind of hard like I'm not sure if you're using it right now Right and so like it's kind of hard like I'm standing up in front of you saying Here's a bunch of techniques that I've I've used and I've won some Kaggle competitions And I've got some state-of-the-art results, but it's like that's kind of secondhand information by the time it hits you right so It's really great to Yeah, try things out and and also like it's been kind of nice to see Particularly I've noticed in the deep learning course quite a few of my students have you know I've said like this technique works really well And they've tried it and they've got into the top 10 of a Kaggle competition the next day, and they're like Okay, that that counts as working really well, so so yeah Kaggle competitions are Helpful for lots and lots of reasons But you know one of the best ways is what happens after it finishes and so definitely like For the ones that you that are now finishing up make sure you you know watch the forums See what people are sharing in terms of their solutions And you know if you want to learn more about them like don't feel free to ask The winners like hey could you tell me more about this or that people are normally pretty pretty good about explaining? And then ideally try and replicate it yourself right and that can turn into a great blog post You know or a great kernel is to be able to say okay such-and-such said that they use this technique Here's a really short explanation of what that technique is and here's a little bit of code showing how it's implemented And you know here's the result showing you you can get the same result that can be a really interesting write-up as well Okay, so You know it's always nice to kind of have your data reflect Like I don't know be as kind of easy to understand as possible So in this case the data that came from Kaggle used various you know integers for the holidays We can just use a boolean of like was it a holiday or not? So I like to clean that up We've got quite a few different tables. We need to join them all together And I have a standard way of joining things together with pandas. I just use the pandas merge Function and specifically I always do a left join So who wants to tell me what a left join is? Since it's there why don't you go ahead? So you retain all the rows in the left table and you take so you have a key column You match that with the key column in the right side table And you just merge the rows that are also present in the right side table. Yeah, that's a great explanation Good job. I don't have much to add to that the key reason that I always do a left join is That after I do the join I always then check if there were things in the right hand side That are now know right because if so it means that I missed some things Yeah, I haven't shown it here, but I also check that the number of rows hasn't varied before and after If it has that means that the right hand side table wasn't unique Okay, so Even when I'm sure Something's true. I always also assume that I've screwed it up, so I always check So I could go ahead and merge the state names into the weather I can also if you look at the Google Trends table It's got this weak range Which I need to turn into a date in order to join it right and so the nice thing about doing this in pandas Is that pandas gives us access to? You know all of Python Right and so for example inside the the series object is a dot str Attribute that gives you access to all the string processing functions Now just like cat gives you access to the categorical functions dt gives you access to the date time functions So I can now split everything in that column and it's really important to try and use these pandas functions Because they you know they're going to be vectorized accelerated through you know often through 7d at least through you know C code So that runs nice and quickly and Then you know as per usual let's add date metadata to our dates In the end we're basically Denormalizing all these tables, so we're going to put them all into one table so in the Google Trend table There was also though they were mainly trends by state, but there was also trends for the whole of Germany So we kind of put the Germany own you know the whole of Germany ones into a separate data frame So that we can join that so we're going to have like Google Trend for this date and Google Trend for the whole of Germany And so now we can go ahead and start joining Both for the training set and for the test set and then for both check that we don't have zeros and zeros My merge function I Set the suffix if there are two columns that are the same I set the suffix on the left to be nothing at all So it doesn't screw around with the name and the right hand side to be underscore y and in this case I didn't want any of the duplicate ones, so I just went through and Deleted them okay and Then we're going to in a moment. We're going to try to Create a Competition you know the the main competitor for this store has been open since some date Right and so you can just use pandas to date time passing in the year the month and the day Right and so that's going to give us an error unless they all have years and months so so we're going to fill in the missing ones with the 1901 Okay And then what we really know we didn't want to know is like how long is this door been open for? At the time of this particular record all right so we can just do a date subtract Okay Now if you think about it sometimes the competition You know opened later than this particular row so sometimes it's going to be negative, and it doesn't probably make sense To have negatives meaning like it's going to open in X days time now having said that I would never Put in something like this without first of all running a model with it in and without it in right because like our assumptions about About the data very often turn out not to be true now in this case. I didn't invent any of these pre-processing steps I wrote all the code, but it's all based on the third place winners github repo right so Knowing what it takes to get third place in the Kaggle competition I'm pretty sure they would have checked every one of these pre-processing steps and made sure it actually improved their their validation set score Okay So what we're going to be doing is is Creating a neural network where some of the inputs to it are continuous and some of them are Categorical and so what that means in the in the neural net that you know we have We're basically going to have you know this kind of initial weight matrix Right and we're going to have this This input feature vector right and so some of the inputs are just going to be plain continuous numbers Like you know what's the maximum temperature here, or what's the number of kilometers to the nearest store? And some of them are going to be And some of them are going to be One hot encoded Effectively right, but we're not actually going to store it as one hot encoded. We're actually going to store it as the index Right and so the neural net model is going to need to know which of these columns Should you should you basically create an embedding for which ones should you treat? You know as if they were kind of one hot encoded and which ones should you just you feed directly into the linear layer? right and so We're going to tell the model when we get there Which is which but we actually need to think ahead of time about like which ones do we want to treat as? Categorical and which ones are continuous in particular? Things that we're going to treat it as categorical We don't want to create More categories than we need right and so let me show you what I mean the The third place getters in this competition decided that the number of months that the competition was open was something that they were going To use as a categorical variable right and so in order to avoid having more categories than they needed They truncated it at 24 months. They said anything more than 24 months. I'll truncate to 24 So here are the unique values of competition months open and it's all the numbers from naught to 24 right So what that means is that there's going to be you know an embedding matrix? That's going to have basically an embedding vector for things that aren't open yet The things that are open a month the things that are open two months and so forth now They absolutely could have done that as a continuous variable right they could have just had a number here Which is just a single number of how many months has it been open and they could have treated it as continuous and fed it straight into the initial weight matrix What I found though and obviously what these competitors found is Where possible it's best to treat things as categorical variables right And the reason for that is that like when you feed something through an embedding matrix You basically mean it means every level can be treated like totally differently right and so for example in this case Whether something's been open for zero months or one month is right really different Right and so if you fit that in as a continuous variable It would be kind of difficult for the neural net to try and find a functional form That kind of has that that big difference It's possible because neural nets can do anything right, but you're not making it easy for it Where else if you used an embedding treated it as categorical then it'll have a totally different vector for zero versus one Right so it seems like particularly as long as you've got enough data that The treating columns as categorical variables where possible is a better idea and so I say when I say where possible that kind of basically means like Where the cardinality is not too high You know so if this was like You know The sales ID number that was like uniquely different on every row You can't treat that as a categorical variable right because you know It would be a huge embedding matrix and everything only appears once or ditto for like kilometers away from the nearest store To two decimal places you wouldn't make a categorical variable right? So that's kind of the that's kind of the rule of thumb That they both used in this competition in fact if we scroll down to their choices Here is how they did it right there continuous variables with things that were genuinely Continuous like number of kilometers away to the competitor the temperature stuff Right the number you know the specific number in the Google Trend right? Where else everything else basically they treated as categorical Okay, so that's it for today, so Yeah, next time we'll we'll finish this off. We'll see we'll see how to turn this into a neural network and Yeah, kind of wrap things up, so see you then", "segments": [{"id": 0, "seek": 0, "start": 0.56, "end": 3.24, "text": " So let's just yeah, let's start by reviewing", "tokens": [407, 718, 311, 445, 1338, 11, 718, 311, 722, 538, 19576], "temperature": 0.0, "avg_logprob": -0.24995754926632613, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.003537711687386036}, {"id": 1, "seek": 0, "start": 4.2, "end": 6.2, "text": " Kind of what we've learned about", "tokens": [9242, 295, 437, 321, 600, 3264, 466], "temperature": 0.0, "avg_logprob": -0.24995754926632613, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.003537711687386036}, {"id": 2, "seek": 0, "start": 6.88, "end": 8.8, "text": " optimizing", "tokens": [40425], "temperature": 0.0, "avg_logprob": -0.24995754926632613, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.003537711687386036}, {"id": 3, "seek": 0, "start": 8.8, "end": 14.72, "text": " Multi-layer functions with SGD and so the idea is that we've got some data and", "tokens": [29238, 12, 8376, 260, 6828, 365, 34520, 35, 293, 370, 264, 1558, 307, 300, 321, 600, 658, 512, 1412, 293], "temperature": 0.0, "avg_logprob": -0.24995754926632613, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.003537711687386036}, {"id": 4, "seek": 0, "start": 15.6, "end": 18.64, "text": " Then we do something to that data for example", "tokens": [1396, 321, 360, 746, 281, 300, 1412, 337, 1365], "temperature": 0.0, "avg_logprob": -0.24995754926632613, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.003537711687386036}, {"id": 5, "seek": 0, "start": 19.52, "end": 22.36, "text": " we multiply it by a weight matrix and", "tokens": [321, 12972, 309, 538, 257, 3364, 8141, 293], "temperature": 0.0, "avg_logprob": -0.24995754926632613, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.003537711687386036}, {"id": 6, "seek": 0, "start": 23.6, "end": 25.6, "text": " Then we do something to that", "tokens": [1396, 321, 360, 746, 281, 300], "temperature": 0.0, "avg_logprob": -0.24995754926632613, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.003537711687386036}, {"id": 7, "seek": 2560, "start": 25.6, "end": 29.94, "text": " For example we put it through a softmax or a sigmoid", "tokens": [1171, 1365, 321, 829, 309, 807, 257, 2787, 41167, 420, 257, 4556, 3280, 327], "temperature": 0.0, "avg_logprob": -0.24741016734730115, "compression_ratio": 1.4225352112676057, "no_speech_prob": 9.66596053331159e-06}, {"id": 8, "seek": 2560, "start": 31.28, "end": 39.24, "text": " And then we do something to that such as do a cross entropy loss or a root mean squared error loss", "tokens": [400, 550, 321, 360, 746, 281, 300, 1270, 382, 360, 257, 3278, 30867, 4470, 420, 257, 5593, 914, 8889, 6713, 4470], "temperature": 0.0, "avg_logprob": -0.24741016734730115, "compression_ratio": 1.4225352112676057, "no_speech_prob": 9.66596053331159e-06}, {"id": 9, "seek": 2560, "start": 42.68000000000001, "end": 46.84, "text": " Okay, and that's going to like give us some scalar", "tokens": [1033, 11, 293, 300, 311, 516, 281, 411, 976, 505, 512, 39684], "temperature": 0.0, "avg_logprob": -0.24741016734730115, "compression_ratio": 1.4225352112676057, "no_speech_prob": 9.66596053331159e-06}, {"id": 10, "seek": 4684, "start": 46.84, "end": 55.84, "text": " So this is going to have no hidden layers this has got a linear layer a", "tokens": [407, 341, 307, 516, 281, 362, 572, 7633, 7914, 341, 575, 658, 257, 8213, 4583, 257], "temperature": 0.0, "avg_logprob": -0.37201998783991885, "compression_ratio": 1.5432098765432098, "no_speech_prob": 5.507542482519057e-06}, {"id": 11, "seek": 4684, "start": 56.88, "end": 63.760000000000005, "text": " Nonlinear activation being a softmax and a loss function being a root mean squared error or a cross entropy", "tokens": [8774, 28263, 24433, 885, 257, 2787, 41167, 293, 257, 4470, 2445, 885, 257, 5593, 914, 8889, 6713, 420, 257, 3278, 30867], "temperature": 0.0, "avg_logprob": -0.37201998783991885, "compression_ratio": 1.5432098765432098, "no_speech_prob": 5.507542482519057e-06}, {"id": 12, "seek": 4684, "start": 63.760000000000005, "end": 66.92, "text": " All right, and then we've got our input data", "tokens": [1057, 558, 11, 293, 550, 321, 600, 658, 527, 4846, 1412], "temperature": 0.0, "avg_logprob": -0.37201998783991885, "compression_ratio": 1.5432098765432098, "no_speech_prob": 5.507542482519057e-06}, {"id": 13, "seek": 4684, "start": 69.08000000000001, "end": 72.96000000000001, "text": " Put linear nonlinear", "tokens": [4935, 8213, 2107, 28263], "temperature": 0.0, "avg_logprob": -0.37201998783991885, "compression_ratio": 1.5432098765432098, "no_speech_prob": 5.507542482519057e-06}, {"id": 14, "seek": 4684, "start": 74.56, "end": 76.32000000000001, "text": " loss", "tokens": [4470], "temperature": 0.0, "avg_logprob": -0.37201998783991885, "compression_ratio": 1.5432098765432098, "no_speech_prob": 5.507542482519057e-06}, {"id": 15, "seek": 7632, "start": 76.32, "end": 79.44, "text": " so for example if this was", "tokens": [370, 337, 1365, 498, 341, 390], "temperature": 0.0, "avg_logprob": -0.28140136430848317, "compression_ratio": 1.3984375, "no_speech_prob": 5.862760190211702e-06}, {"id": 16, "seek": 7632, "start": 81.24, "end": 83.24, "text": " sigmoid or", "tokens": [4556, 3280, 327, 420], "temperature": 0.0, "avg_logprob": -0.28140136430848317, "compression_ratio": 1.3984375, "no_speech_prob": 5.862760190211702e-06}, {"id": 17, "seek": 7632, "start": 85.44, "end": 91.44, "text": " Or softmax and this was cross entropy then that would be logistic regression", "tokens": [1610, 2787, 41167, 293, 341, 390, 3278, 30867, 550, 300, 576, 312, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.28140136430848317, "compression_ratio": 1.3984375, "no_speech_prob": 5.862760190211702e-06}, {"id": 18, "seek": 7632, "start": 94.67999999999999, "end": 96.67999999999999, "text": " So it's still a yeah", "tokens": [407, 309, 311, 920, 257, 1338], "temperature": 0.0, "avg_logprob": -0.28140136430848317, "compression_ratio": 1.3984375, "no_speech_prob": 5.862760190211702e-06}, {"id": 19, "seek": 7632, "start": 98.72, "end": 102.83999999999999, "text": " Cross entropy yeah, let's do that next sure", "tokens": [11623, 30867, 1338, 11, 718, 311, 360, 300, 958, 988], "temperature": 0.0, "avg_logprob": -0.28140136430848317, "compression_ratio": 1.3984375, "no_speech_prob": 5.862760190211702e-06}, {"id": 20, "seek": 10284, "start": 102.84, "end": 108.60000000000001, "text": " For now think of it like think of root means great error same thing some loss function", "tokens": [1171, 586, 519, 295, 309, 411, 519, 295, 5593, 1355, 869, 6713, 912, 551, 512, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.27957235483022835, "compression_ratio": 1.521978021978022, "no_speech_prob": 6.475939926531282e-07}, {"id": 21, "seek": 10284, "start": 108.96000000000001, "end": 110.68, "text": " Okay now", "tokens": [1033, 586], "temperature": 0.0, "avg_logprob": -0.27957235483022835, "compression_ratio": 1.521978021978022, "no_speech_prob": 6.475939926531282e-07}, {"id": 22, "seek": 10284, "start": 110.68, "end": 112.88000000000001, "text": " We'll look at cross entropy again in the moment", "tokens": [492, 603, 574, 412, 3278, 30867, 797, 294, 264, 1623], "temperature": 0.0, "avg_logprob": -0.27957235483022835, "compression_ratio": 1.521978021978022, "no_speech_prob": 6.475939926531282e-07}, {"id": 23, "seek": 10284, "start": 117.76, "end": 127.84, "text": " So how do we calculate the derivative of that with with respect to our weights right so really it would probably be better if we said", "tokens": [407, 577, 360, 321, 8873, 264, 13760, 295, 300, 365, 365, 3104, 281, 527, 17443, 558, 370, 534, 309, 576, 1391, 312, 1101, 498, 321, 848], "temperature": 0.0, "avg_logprob": -0.27957235483022835, "compression_ratio": 1.521978021978022, "no_speech_prob": 6.475939926531282e-07}, {"id": 24, "seek": 12784, "start": 127.84, "end": 136.04, "text": " X comma W here because it's really a function of the weights as well, and so we want the", "tokens": [1783, 22117, 343, 510, 570, 309, 311, 534, 257, 2445, 295, 264, 17443, 382, 731, 11, 293, 370, 321, 528, 264], "temperature": 0.0, "avg_logprob": -0.26393310228983563, "compression_ratio": 1.464705882352941, "no_speech_prob": 1.1478680789878126e-05}, {"id": 25, "seek": 12784, "start": 136.56, "end": 138.56, "text": " derivative of this", "tokens": [13760, 295, 341], "temperature": 0.0, "avg_logprob": -0.26393310228983563, "compression_ratio": 1.464705882352941, "no_speech_prob": 1.1478680789878126e-05}, {"id": 26, "seek": 12784, "start": 139.44, "end": 142.4, "text": " With respect to our weights", "tokens": [2022, 3104, 281, 527, 17443], "temperature": 0.0, "avg_logprob": -0.26393310228983563, "compression_ratio": 1.464705882352941, "no_speech_prob": 1.1478680789878126e-05}, {"id": 27, "seek": 12784, "start": 146.36, "end": 148.36, "text": " Sorry I put it in the wrong spot", "tokens": [4919, 286, 829, 309, 294, 264, 2085, 4008], "temperature": 0.0, "avg_logprob": -0.26393310228983563, "compression_ratio": 1.464705882352941, "no_speech_prob": 1.1478680789878126e-05}, {"id": 28, "seek": 12784, "start": 149.96, "end": 152.76, "text": " G F of X comma W", "tokens": [460, 479, 295, 1783, 22117, 343], "temperature": 0.0, "avg_logprob": -0.26393310228983563, "compression_ratio": 1.464705882352941, "no_speech_prob": 1.1478680789878126e-05}, {"id": 29, "seek": 15276, "start": 152.76, "end": 158.51999999999998, "text": " I just screwed up. That's all that's why that didn't make sense all right", "tokens": [286, 445, 20331, 493, 13, 663, 311, 439, 300, 311, 983, 300, 994, 380, 652, 2020, 439, 558], "temperature": 0.0, "avg_logprob": -0.2735645354740203, "compression_ratio": 1.4744525547445255, "no_speech_prob": 3.089476422246662e-06}, {"id": 30, "seek": 15276, "start": 161.56, "end": 168.76, "text": " So to do that we just basically we do the chain rule so we just say that this is equal to H of", "tokens": [407, 281, 360, 300, 321, 445, 1936, 321, 360, 264, 5021, 4978, 370, 321, 445, 584, 300, 341, 307, 2681, 281, 389, 295], "temperature": 0.0, "avg_logprob": -0.2735645354740203, "compression_ratio": 1.4744525547445255, "no_speech_prob": 3.089476422246662e-06}, {"id": 31, "seek": 15276, "start": 169.84, "end": 171.84, "text": " U and", "tokens": [624, 293], "temperature": 0.0, "avg_logprob": -0.2735645354740203, "compression_ratio": 1.4744525547445255, "no_speech_prob": 3.089476422246662e-06}, {"id": 32, "seek": 15276, "start": 173.76, "end": 175.76, "text": " U equals", "tokens": [624, 6915], "temperature": 0.0, "avg_logprob": -0.2735645354740203, "compression_ratio": 1.4744525547445255, "no_speech_prob": 3.089476422246662e-06}, {"id": 33, "seek": 15276, "start": 177.28, "end": 180.51999999999998, "text": " G F or G do equals", "tokens": [460, 479, 420, 460, 360, 6915], "temperature": 0.0, "avg_logprob": -0.2735645354740203, "compression_ratio": 1.4744525547445255, "no_speech_prob": 3.089476422246662e-06}, {"id": 34, "seek": 18052, "start": 180.52, "end": 186.76000000000002, "text": " G of V and V equals F of", "tokens": [460, 295, 691, 293, 691, 6915, 479, 295], "temperature": 0.0, "avg_logprob": -0.23763740226014018, "compression_ratio": 1.516778523489933, "no_speech_prob": 1.6280454246953013e-06}, {"id": 35, "seek": 18052, "start": 187.52, "end": 188.68, "text": " X", "tokens": [1783], "temperature": 0.0, "avg_logprob": -0.23763740226014018, "compression_ratio": 1.516778523489933, "no_speech_prob": 1.6280454246953013e-06}, {"id": 36, "seek": 18052, "start": 188.68, "end": 193.38, "text": " So we can just rewrite it like that right and then we can do the chain rule", "tokens": [407, 321, 393, 445, 28132, 309, 411, 300, 558, 293, 550, 321, 393, 360, 264, 5021, 4978], "temperature": 0.0, "avg_logprob": -0.23763740226014018, "compression_ratio": 1.516778523489933, "no_speech_prob": 1.6280454246953013e-06}, {"id": 37, "seek": 18052, "start": 193.64000000000001, "end": 198.28, "text": " So we can say that's equal to H dash the derivative is H dash U by", "tokens": [407, 321, 393, 584, 300, 311, 2681, 281, 389, 8240, 264, 13760, 307, 389, 8240, 624, 538], "temperature": 0.0, "avg_logprob": -0.23763740226014018, "compression_ratio": 1.516778523489933, "no_speech_prob": 1.6280454246953013e-06}, {"id": 38, "seek": 18052, "start": 199.08, "end": 203.04000000000002, "text": " G dash V by F dash X", "tokens": [460, 8240, 691, 538, 479, 8240, 1783], "temperature": 0.0, "avg_logprob": -0.23763740226014018, "compression_ratio": 1.516778523489933, "no_speech_prob": 1.6280454246953013e-06}, {"id": 39, "seek": 18052, "start": 204.76000000000002, "end": 207.92000000000002, "text": " Happy with all that so far okay, so", "tokens": [8277, 365, 439, 300, 370, 1400, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.23763740226014018, "compression_ratio": 1.516778523489933, "no_speech_prob": 1.6280454246953013e-06}, {"id": 40, "seek": 20792, "start": 207.92, "end": 209.72, "text": " in", "tokens": [294], "temperature": 0.0, "avg_logprob": -0.2593673229217529, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.340513330840622e-06}, {"id": 41, "seek": 20792, "start": 209.72, "end": 214.0, "text": " Order to take the derivative with respect to the weights therefore", "tokens": [16321, 281, 747, 264, 13760, 365, 3104, 281, 264, 17443, 4412], "temperature": 0.0, "avg_logprob": -0.2593673229217529, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.340513330840622e-06}, {"id": 42, "seek": 20792, "start": 214.56, "end": 220.2, "text": " We just have to calculate that derivative with respect to W using that exact", "tokens": [492, 445, 362, 281, 8873, 300, 13760, 365, 3104, 281, 343, 1228, 300, 1900], "temperature": 0.0, "avg_logprob": -0.2593673229217529, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.340513330840622e-06}, {"id": 43, "seek": 20792, "start": 220.92, "end": 222.11999999999998, "text": " formula", "tokens": [8513], "temperature": 0.0, "avg_logprob": -0.2593673229217529, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.340513330840622e-06}, {"id": 44, "seek": 20792, "start": 222.11999999999998, "end": 224.27999999999997, "text": " So if we had in there", "tokens": [407, 498, 321, 632, 294, 456], "temperature": 0.0, "avg_logprob": -0.2593673229217529, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.340513330840622e-06}, {"id": 45, "seek": 20792, "start": 225.79999999999998, "end": 227.79999999999998, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.2593673229217529, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.340513330840622e-06}, {"id": 46, "seek": 22780, "start": 227.8, "end": 238.32000000000002, "text": " Yeah, so so D of all that DW would be that yeah", "tokens": [865, 11, 370, 370, 413, 295, 439, 300, 45318, 576, 312, 300, 1338], "temperature": 0.0, "avg_logprob": -0.3088742362128364, "compression_ratio": 1.288, "no_speech_prob": 4.157311650487827e-06}, {"id": 47, "seek": 22780, "start": 241.12, "end": 248.64000000000001, "text": " So then if if we you know went further here and had like", "tokens": [407, 550, 498, 498, 321, 291, 458, 1437, 3052, 510, 293, 632, 411], "temperature": 0.0, "avg_logprob": -0.3088742362128364, "compression_ratio": 1.288, "no_speech_prob": 4.157311650487827e-06}, {"id": 48, "seek": 24864, "start": 248.64, "end": 259.15999999999997, "text": " Another linear layer right let's go a bit more room another linear layer I", "tokens": [3996, 8213, 4583, 558, 718, 311, 352, 257, 857, 544, 1808, 1071, 8213, 4583, 286], "temperature": 0.0, "avg_logprob": -0.3663820337366175, "compression_ratio": 1.5547445255474452, "no_speech_prob": 1.553492666062084e-06}, {"id": 49, "seek": 24864, "start": 262.15999999999997, "end": 264.15999999999997, "text": " Comma", "tokens": [3046, 64], "temperature": 0.0, "avg_logprob": -0.3663820337366175, "compression_ratio": 1.5547445255474452, "no_speech_prob": 1.553492666062084e-06}, {"id": 50, "seek": 24864, "start": 265.8, "end": 267.8, "text": " W2", "tokens": [343, 17], "temperature": 0.0, "avg_logprob": -0.3663820337366175, "compression_ratio": 1.5547445255474452, "no_speech_prob": 1.553492666062084e-06}, {"id": 51, "seek": 24864, "start": 268.03999999999996, "end": 270.44, "text": " Right so we have another linear layer", "tokens": [1779, 370, 321, 362, 1071, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.3663820337366175, "compression_ratio": 1.5547445255474452, "no_speech_prob": 1.553492666062084e-06}, {"id": 52, "seek": 24864, "start": 271.64, "end": 277.71999999999997, "text": " There's no difference to now calculate the derivative with respect to all of the parameters", "tokens": [821, 311, 572, 2649, 281, 586, 8873, 264, 13760, 365, 3104, 281, 439, 295, 264, 9834], "temperature": 0.0, "avg_logprob": -0.3663820337366175, "compression_ratio": 1.5547445255474452, "no_speech_prob": 1.553492666062084e-06}, {"id": 53, "seek": 27772, "start": 277.72, "end": 279.8, "text": " We can still use the exact same", "tokens": [492, 393, 920, 764, 264, 1900, 912], "temperature": 0.0, "avg_logprob": -0.14557800487596162, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.406091425655177e-06}, {"id": 54, "seek": 27772, "start": 280.72, "end": 282.72, "text": " Chain rule right", "tokens": [33252, 4978, 558], "temperature": 0.0, "avg_logprob": -0.14557800487596162, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.406091425655177e-06}, {"id": 55, "seek": 27772, "start": 282.76000000000005, "end": 289.16, "text": " So so don't think of the multi-layer network as being like things that occur at different times", "tokens": [407, 370, 500, 380, 519, 295, 264, 4825, 12, 8376, 260, 3209, 382, 885, 411, 721, 300, 5160, 412, 819, 1413], "temperature": 0.0, "avg_logprob": -0.14557800487596162, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.406091425655177e-06}, {"id": 56, "seek": 27772, "start": 289.16, "end": 291.52000000000004, "text": " it's just a composition of functions and", "tokens": [309, 311, 445, 257, 12686, 295, 6828, 293], "temperature": 0.0, "avg_logprob": -0.14557800487596162, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.406091425655177e-06}, {"id": 57, "seek": 27772, "start": 292.16, "end": 298.12, "text": " So we just use the chain rule to calculate all the derivatives at once you know", "tokens": [407, 321, 445, 764, 264, 5021, 4978, 281, 8873, 439, 264, 33733, 412, 1564, 291, 458], "temperature": 0.0, "avg_logprob": -0.14557800487596162, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.406091425655177e-06}, {"id": 58, "seek": 27772, "start": 298.16, "end": 304.68, "text": " There's a they're just a set of parameters that happen to appear in different parts of the function, but the cut that the calculus is no", "tokens": [821, 311, 257, 436, 434, 445, 257, 992, 295, 9834, 300, 1051, 281, 4204, 294, 819, 3166, 295, 264, 2445, 11, 457, 264, 1723, 300, 264, 33400, 307, 572], "temperature": 0.0, "avg_logprob": -0.14557800487596162, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.406091425655177e-06}, {"id": 59, "seek": 30468, "start": 304.68, "end": 308.68, "text": " No different so to calculate this with respect to", "tokens": [883, 819, 370, 281, 8873, 341, 365, 3104, 281], "temperature": 0.0, "avg_logprob": -0.26307581548821435, "compression_ratio": 1.4970760233918128, "no_speech_prob": 1.733044086904556e-06}, {"id": 60, "seek": 30468, "start": 309.92, "end": 314.52, "text": " W1 and W2 you know it's it's just you just increase", "tokens": [343, 16, 293, 343, 17, 291, 458, 309, 311, 309, 311, 445, 291, 445, 3488], "temperature": 0.0, "avg_logprob": -0.26307581548821435, "compression_ratio": 1.4970760233918128, "no_speech_prob": 1.733044086904556e-06}, {"id": 61, "seek": 30468, "start": 315.40000000000003, "end": 320.08, "text": " You know W. You can just now just call it W and say W1 is all of those weights", "tokens": [509, 458, 343, 13, 509, 393, 445, 586, 445, 818, 309, 343, 293, 584, 343, 16, 307, 439, 295, 729, 17443], "temperature": 0.0, "avg_logprob": -0.26307581548821435, "compression_ratio": 1.4970760233918128, "no_speech_prob": 1.733044086904556e-06}, {"id": 62, "seek": 30468, "start": 323.52, "end": 328.6, "text": " So the result that's a great question, so what you're going to have then is", "tokens": [407, 264, 1874, 300, 311, 257, 869, 1168, 11, 370, 437, 291, 434, 516, 281, 362, 550, 307], "temperature": 0.0, "avg_logprob": -0.26307581548821435, "compression_ratio": 1.4970760233918128, "no_speech_prob": 1.733044086904556e-06}, {"id": 63, "seek": 32860, "start": 328.6, "end": 330.6, "text": " Is", "tokens": [1119], "temperature": 0.0, "avg_logprob": -0.25618149577707483, "compression_ratio": 1.4876543209876543, "no_speech_prob": 9.276340051656007e-07}, {"id": 64, "seek": 32860, "start": 332.08000000000004, "end": 336.36, "text": " A list of parameters right so", "tokens": [316, 1329, 295, 9834, 558, 370], "temperature": 0.0, "avg_logprob": -0.25618149577707483, "compression_ratio": 1.4876543209876543, "no_speech_prob": 9.276340051656007e-07}, {"id": 65, "seek": 32860, "start": 338.52000000000004, "end": 346.04, "text": " Here's W1 and like it's it's it's probably some kind of higher rank tensor", "tokens": [1692, 311, 343, 16, 293, 411, 309, 311, 309, 311, 309, 311, 1391, 512, 733, 295, 2946, 6181, 40863], "temperature": 0.0, "avg_logprob": -0.25618149577707483, "compression_ratio": 1.4876543209876543, "no_speech_prob": 9.276340051656007e-07}, {"id": 66, "seek": 32860, "start": 346.04, "end": 348.88, "text": " You know like if it's a convolution or layer", "tokens": [509, 458, 411, 498, 309, 311, 257, 45216, 420, 4583], "temperature": 0.0, "avg_logprob": -0.25618149577707483, "compression_ratio": 1.4876543209876543, "no_speech_prob": 9.276340051656007e-07}, {"id": 67, "seek": 32860, "start": 349.8, "end": 355.36, "text": " It'll you know be like a rank three tensor or whatever, but we can flatten it out right?", "tokens": [467, 603, 291, 458, 312, 411, 257, 6181, 1045, 40863, 420, 2035, 11, 457, 321, 393, 24183, 309, 484, 558, 30], "temperature": 0.0, "avg_logprob": -0.25618149577707483, "compression_ratio": 1.4876543209876543, "no_speech_prob": 9.276340051656007e-07}, {"id": 68, "seek": 35536, "start": 355.36, "end": 359.64, "text": " We'll just make it a list of parameters. There's W1. Here's W2", "tokens": [492, 603, 445, 652, 309, 257, 1329, 295, 9834, 13, 821, 311, 343, 16, 13, 1692, 311, 343, 17], "temperature": 0.0, "avg_logprob": -0.22034086227416994, "compression_ratio": 1.8040201005025125, "no_speech_prob": 3.668848421511939e-06}, {"id": 69, "seek": 35536, "start": 360.32, "end": 362.12, "text": " Right just another", "tokens": [1779, 445, 1071], "temperature": 0.0, "avg_logprob": -0.22034086227416994, "compression_ratio": 1.8040201005025125, "no_speech_prob": 3.668848421511939e-06}, {"id": 70, "seek": 35536, "start": 362.12, "end": 364.12, "text": " list of parameters, right", "tokens": [1329, 295, 9834, 11, 558], "temperature": 0.0, "avg_logprob": -0.22034086227416994, "compression_ratio": 1.8040201005025125, "no_speech_prob": 3.668848421511939e-06}, {"id": 71, "seek": 35536, "start": 364.84000000000003, "end": 366.64, "text": " And here's our loss", "tokens": [400, 510, 311, 527, 4470], "temperature": 0.0, "avg_logprob": -0.22034086227416994, "compression_ratio": 1.8040201005025125, "no_speech_prob": 3.668848421511939e-06}, {"id": 72, "seek": 35536, "start": 366.64, "end": 368.88, "text": " Which is a single you know a single number?", "tokens": [3013, 307, 257, 2167, 291, 458, 257, 2167, 1230, 30], "temperature": 0.0, "avg_logprob": -0.22034086227416994, "compression_ratio": 1.8040201005025125, "no_speech_prob": 3.668848421511939e-06}, {"id": 73, "seek": 35536, "start": 369.64, "end": 371.64, "text": " so therefore", "tokens": [370, 4412], "temperature": 0.0, "avg_logprob": -0.22034086227416994, "compression_ratio": 1.8040201005025125, "no_speech_prob": 3.668848421511939e-06}, {"id": 74, "seek": 35536, "start": 371.84000000000003, "end": 373.84000000000003, "text": " our derivative", "tokens": [527, 13760], "temperature": 0.0, "avg_logprob": -0.22034086227416994, "compression_ratio": 1.8040201005025125, "no_speech_prob": 3.668848421511939e-06}, {"id": 75, "seek": 35536, "start": 374.88, "end": 377.96000000000004, "text": " Is just a vector of that same length right?", "tokens": [1119, 445, 257, 8062, 295, 300, 912, 4641, 558, 30], "temperature": 0.0, "avg_logprob": -0.22034086227416994, "compression_ratio": 1.8040201005025125, "no_speech_prob": 3.668848421511939e-06}, {"id": 76, "seek": 37796, "start": 377.96, "end": 384.76, "text": " It's how much does changing that value of W affect the loss how much does changing that value of W affect the loss?", "tokens": [467, 311, 577, 709, 775, 4473, 300, 2158, 295, 343, 3345, 264, 4470, 577, 709, 775, 4473, 300, 2158, 295, 343, 3345, 264, 4470, 30], "temperature": 0.0, "avg_logprob": -0.23491524676887357, "compression_ratio": 1.7990196078431373, "no_speech_prob": 2.9480033845175058e-06}, {"id": 77, "seek": 37796, "start": 385.32, "end": 391.71999999999997, "text": " right, so you can basically think of it as a function like you know y equals a", "tokens": [558, 11, 370, 291, 393, 1936, 519, 295, 309, 382, 257, 2445, 411, 291, 458, 288, 6915, 257], "temperature": 0.0, "avg_logprob": -0.23491524676887357, "compression_ratio": 1.7990196078431373, "no_speech_prob": 2.9480033845175058e-06}, {"id": 78, "seek": 37796, "start": 392.52, "end": 395.15999999999997, "text": " x1 plus b x2", "tokens": [2031, 16, 1804, 272, 2031, 17], "temperature": 0.0, "avg_logprob": -0.23491524676887357, "compression_ratio": 1.7990196078431373, "no_speech_prob": 2.9480033845175058e-06}, {"id": 79, "seek": 37796, "start": 396.4, "end": 400.84, "text": " Plus C right and say like oh, what's the derivative of that with respect to a?", "tokens": [7721, 383, 558, 293, 584, 411, 1954, 11, 437, 311, 264, 13760, 295, 300, 365, 3104, 281, 257, 30], "temperature": 0.0, "avg_logprob": -0.23491524676887357, "compression_ratio": 1.7990196078431373, "no_speech_prob": 2.9480033845175058e-06}, {"id": 80, "seek": 37796, "start": 401.12, "end": 407.21999999999997, "text": " B and C and you would have three numbers the derivative respect to a and B and C", "tokens": [363, 293, 383, 293, 291, 576, 362, 1045, 3547, 264, 13760, 3104, 281, 257, 293, 363, 293, 383], "temperature": 0.0, "avg_logprob": -0.23491524676887357, "compression_ratio": 1.7990196078431373, "no_speech_prob": 2.9480033845175058e-06}, {"id": 81, "seek": 40722, "start": 407.22, "end": 412.90000000000003, "text": " And that's all this is right with the derivative with respect to that weight that weight and that weight and that weight and that weight", "tokens": [400, 300, 311, 439, 341, 307, 558, 365, 264, 13760, 365, 3104, 281, 300, 3364, 300, 3364, 293, 300, 3364, 293, 300, 3364, 293, 300, 3364], "temperature": 0.0, "avg_logprob": -0.20085629549893466, "compression_ratio": 1.8781725888324874, "no_speech_prob": 8.99093720363453e-07}, {"id": 82, "seek": 40722, "start": 415.26000000000005, "end": 417.90000000000003, "text": " To get there inside the chain rule", "tokens": [1407, 483, 456, 1854, 264, 5021, 4978], "temperature": 0.0, "avg_logprob": -0.20085629549893466, "compression_ratio": 1.8781725888324874, "no_speech_prob": 8.99093720363453e-07}, {"id": 83, "seek": 40722, "start": 420.78000000000003, "end": 425.82000000000005, "text": " We had to calculate and I'm not going to detail here, but we had to calculate like", "tokens": [492, 632, 281, 8873, 293, 286, 478, 406, 516, 281, 2607, 510, 11, 457, 321, 632, 281, 8873, 411], "temperature": 0.0, "avg_logprob": -0.20085629549893466, "compression_ratio": 1.8781725888324874, "no_speech_prob": 8.99093720363453e-07}, {"id": 84, "seek": 40722, "start": 426.82000000000005, "end": 431.98, "text": " Jacobians so like the derivative when you take a matrix product is", "tokens": [14117, 2567, 370, 411, 264, 13760, 562, 291, 747, 257, 8141, 1674, 307], "temperature": 0.0, "avg_logprob": -0.20085629549893466, "compression_ratio": 1.8781725888324874, "no_speech_prob": 8.99093720363453e-07}, {"id": 85, "seek": 40722, "start": 432.70000000000005, "end": 435.02000000000004, "text": " you've now got something where you've got like a", "tokens": [291, 600, 586, 658, 746, 689, 291, 600, 658, 411, 257], "temperature": 0.0, "avg_logprob": -0.20085629549893466, "compression_ratio": 1.8781725888324874, "no_speech_prob": 8.99093720363453e-07}, {"id": 86, "seek": 43502, "start": 435.02, "end": 437.41999999999996, "text": " a weight matrix and", "tokens": [257, 3364, 8141, 293], "temperature": 0.0, "avg_logprob": -0.23699024075367411, "compression_ratio": 1.7112676056338028, "no_speech_prob": 3.3931241887330543e-06}, {"id": 87, "seek": 43502, "start": 439.06, "end": 445.5, "text": " You've got an input vector these are the activations from the previous layer right and you've got", "tokens": [509, 600, 658, 364, 4846, 8062, 613, 366, 264, 2430, 763, 490, 264, 3894, 4583, 558, 293, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.23699024075367411, "compression_ratio": 1.7112676056338028, "no_speech_prob": 3.3931241887330543e-06}, {"id": 88, "seek": 43502, "start": 449.26, "end": 451.09999999999997, "text": " Some new", "tokens": [2188, 777], "temperature": 0.0, "avg_logprob": -0.23699024075367411, "compression_ratio": 1.7112676056338028, "no_speech_prob": 3.3931241887330543e-06}, {"id": 89, "seek": 43502, "start": 451.09999999999997, "end": 457.9, "text": " Output activations right and so now you've got to say like okay for this particular sorry for this particular", "tokens": [5925, 2582, 2430, 763, 558, 293, 370, 586, 291, 600, 658, 281, 584, 411, 1392, 337, 341, 1729, 2597, 337, 341, 1729], "temperature": 0.0, "avg_logprob": -0.23699024075367411, "compression_ratio": 1.7112676056338028, "no_speech_prob": 3.3931241887330543e-06}, {"id": 90, "seek": 43502, "start": 459.26, "end": 461.26, "text": " weight", "tokens": [3364], "temperature": 0.0, "avg_logprob": -0.23699024075367411, "compression_ratio": 1.7112676056338028, "no_speech_prob": 3.3931241887330543e-06}, {"id": 91, "seek": 46126, "start": 461.26, "end": 463.26, "text": " How does", "tokens": [1012, 775], "temperature": 0.0, "avg_logprob": -0.21590253194173178, "compression_ratio": 1.888268156424581, "no_speech_prob": 4.2228052734571975e-06}, {"id": 92, "seek": 46126, "start": 464.42, "end": 466.9, "text": " Changing this particular weight change", "tokens": [45773, 341, 1729, 3364, 1319], "temperature": 0.0, "avg_logprob": -0.21590253194173178, "compression_ratio": 1.888268156424581, "no_speech_prob": 4.2228052734571975e-06}, {"id": 93, "seek": 46126, "start": 467.74, "end": 476.02, "text": " This particular output and how does changing this particular weight change this particular output and so forth so you kind of end up with", "tokens": [639, 1729, 5598, 293, 577, 775, 4473, 341, 1729, 3364, 1319, 341, 1729, 5598, 293, 370, 5220, 370, 291, 733, 295, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.21590253194173178, "compression_ratio": 1.888268156424581, "no_speech_prob": 4.2228052734571975e-06}, {"id": 94, "seek": 46126, "start": 476.02, "end": 481.62, "text": " These higher dimensional tensors showing like for every weight. How does it affect?", "tokens": [1981, 2946, 18795, 10688, 830, 4099, 411, 337, 633, 3364, 13, 1012, 775, 309, 3345, 30], "temperature": 0.0, "avg_logprob": -0.21590253194173178, "compression_ratio": 1.888268156424581, "no_speech_prob": 4.2228052734571975e-06}, {"id": 95, "seek": 46126, "start": 482.34, "end": 484.34, "text": " every output right", "tokens": [633, 5598, 558], "temperature": 0.0, "avg_logprob": -0.21590253194173178, "compression_ratio": 1.888268156424581, "no_speech_prob": 4.2228052734571975e-06}, {"id": 96, "seek": 46126, "start": 484.62, "end": 486.62, "text": " But then by the time you get to the loss function", "tokens": [583, 550, 538, 264, 565, 291, 483, 281, 264, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.21590253194173178, "compression_ratio": 1.888268156424581, "no_speech_prob": 4.2228052734571975e-06}, {"id": 97, "seek": 48662, "start": 486.62, "end": 492.1, "text": " The loss function is going to have like a mean or a sum or something so they're all going to get added up", "tokens": [440, 4470, 2445, 307, 516, 281, 362, 411, 257, 914, 420, 257, 2408, 420, 746, 370, 436, 434, 439, 516, 281, 483, 3869, 493], "temperature": 0.0, "avg_logprob": -0.17144563280302902, "compression_ratio": 1.8729508196721312, "no_speech_prob": 2.1568102965829894e-06}, {"id": 98, "seek": 48662, "start": 492.62, "end": 496.54, "text": " in the end you know and so this kind of thing like I", "tokens": [294, 264, 917, 291, 458, 293, 370, 341, 733, 295, 551, 411, 286], "temperature": 0.0, "avg_logprob": -0.17144563280302902, "compression_ratio": 1.8729508196721312, "no_speech_prob": 2.1568102965829894e-06}, {"id": 99, "seek": 48662, "start": 497.5, "end": 499.66, "text": " Don't know it drives me a bit crazy to try and", "tokens": [1468, 380, 458, 309, 11754, 385, 257, 857, 3219, 281, 853, 293], "temperature": 0.0, "avg_logprob": -0.17144563280302902, "compression_ratio": 1.8729508196721312, "no_speech_prob": 2.1568102965829894e-06}, {"id": 100, "seek": 48662, "start": 501.18, "end": 505.94, "text": " calculate it out by hand or even think of it step by step because you tend to have like", "tokens": [8873, 309, 484, 538, 1011, 420, 754, 519, 295, 309, 1823, 538, 1823, 570, 291, 3928, 281, 362, 411], "temperature": 0.0, "avg_logprob": -0.17144563280302902, "compression_ratio": 1.8729508196721312, "no_speech_prob": 2.1568102965829894e-06}, {"id": 101, "seek": 48662, "start": 506.7, "end": 511.02, "text": " You just have to remember for every input in a layer for every output in the next layer", "tokens": [509, 445, 362, 281, 1604, 337, 633, 4846, 294, 257, 4583, 337, 633, 5598, 294, 264, 958, 4583], "temperature": 0.0, "avg_logprob": -0.17144563280302902, "compression_ratio": 1.8729508196721312, "no_speech_prob": 2.1568102965829894e-06}, {"id": 102, "seek": 48662, "start": 511.02, "end": 515.3, "text": " You know you're going to have to take out for every weight for every output", "tokens": [509, 458, 291, 434, 516, 281, 362, 281, 747, 484, 337, 633, 3364, 337, 633, 5598], "temperature": 0.0, "avg_logprob": -0.17144563280302902, "compression_ratio": 1.8729508196721312, "no_speech_prob": 2.1568102965829894e-06}, {"id": 103, "seek": 51530, "start": 515.3, "end": 517.3, "text": " You're going to have to have a separate", "tokens": [509, 434, 516, 281, 362, 281, 362, 257, 4994], "temperature": 0.0, "avg_logprob": -0.20788054239182246, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.0894811970938463e-06}, {"id": 104, "seek": 51530, "start": 518.3399999999999, "end": 520.3399999999999, "text": " gradient", "tokens": [16235], "temperature": 0.0, "avg_logprob": -0.20788054239182246, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.0894811970938463e-06}, {"id": 105, "seek": 51530, "start": 520.74, "end": 527.62, "text": " One good way to look at this is to learn to use pie torches like dot grad", "tokens": [1485, 665, 636, 281, 574, 412, 341, 307, 281, 1466, 281, 764, 1730, 3930, 3781, 411, 5893, 2771], "temperature": 0.0, "avg_logprob": -0.20788054239182246, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.0894811970938463e-06}, {"id": 106, "seek": 51530, "start": 528.3, "end": 533.4399999999999, "text": " Attribute and dot backward method manually and like look up the tutorial the pie torch tutorials", "tokens": [7298, 2024, 1169, 293, 5893, 23897, 3170, 16945, 293, 411, 574, 493, 264, 7073, 264, 1730, 27822, 17616], "temperature": 0.0, "avg_logprob": -0.20788054239182246, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.0894811970938463e-06}, {"id": 107, "seek": 51530, "start": 533.4399999999999, "end": 540.88, "text": " And so you can actually start setting up some calculations with a vector input and the vector output and then type dot backward and then say", "tokens": [400, 370, 291, 393, 767, 722, 3287, 493, 512, 20448, 365, 257, 8062, 4846, 293, 264, 8062, 5598, 293, 550, 2010, 5893, 23897, 293, 550, 584], "temperature": 0.0, "avg_logprob": -0.20788054239182246, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.0894811970938463e-06}, {"id": 108, "seek": 54088, "start": 540.88, "end": 546.52, "text": " Type dot grad and like look at it right and then do some really small ones with just two or three", "tokens": [15576, 5893, 2771, 293, 411, 574, 412, 309, 558, 293, 550, 360, 512, 534, 1359, 2306, 365, 445, 732, 420, 1045], "temperature": 0.0, "avg_logprob": -0.16944403289466775, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.1189404176548123e-06}, {"id": 109, "seek": 54088, "start": 547.04, "end": 553.16, "text": " items in the input and output vectors and like make the make the operation like plus two or something and like", "tokens": [4754, 294, 264, 4846, 293, 5598, 18875, 293, 411, 652, 264, 652, 264, 6916, 411, 1804, 732, 420, 746, 293, 411], "temperature": 0.0, "avg_logprob": -0.16944403289466775, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.1189404176548123e-06}, {"id": 110, "seek": 54088, "start": 553.76, "end": 556.64, "text": " See what the shapes are make sure it makes sense", "tokens": [3008, 437, 264, 10854, 366, 652, 988, 309, 1669, 2020], "temperature": 0.0, "avg_logprob": -0.16944403289466775, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.1189404176548123e-06}, {"id": 111, "seek": 54088, "start": 557.56, "end": 559.56, "text": " Yeah, because it's kind of like", "tokens": [865, 11, 570, 309, 311, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.16944403289466775, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.1189404176548123e-06}, {"id": 112, "seek": 54088, "start": 560.28, "end": 562.28, "text": " this", "tokens": [341], "temperature": 0.0, "avg_logprob": -0.16944403289466775, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.1189404176548123e-06}, {"id": 113, "seek": 54088, "start": 562.64, "end": 569.28, "text": " Vector matrix calculus is not like introduces zero new concepts to anything you learned in high school", "tokens": [691, 20814, 8141, 33400, 307, 406, 411, 31472, 4018, 777, 10392, 281, 1340, 291, 3264, 294, 1090, 1395], "temperature": 0.0, "avg_logprob": -0.16944403289466775, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.1189404176548123e-06}, {"id": 114, "seek": 56928, "start": 569.28, "end": 574.56, "text": " Like strictly speaking, but getting a feel for how these shapes", "tokens": [1743, 20792, 4124, 11, 457, 1242, 257, 841, 337, 577, 613, 10854], "temperature": 0.0, "avg_logprob": -0.1717786442149769, "compression_ratio": 1.326797385620915, "no_speech_prob": 4.3568629735091235e-06}, {"id": 115, "seek": 56928, "start": 575.68, "end": 582.88, "text": " Move around I find took a lot of practice. You know the good news is you almost never have to worry about it", "tokens": [10475, 926, 286, 915, 1890, 257, 688, 295, 3124, 13, 509, 458, 264, 665, 2583, 307, 291, 1920, 1128, 362, 281, 3292, 466, 309], "temperature": 0.0, "avg_logprob": -0.1717786442149769, "compression_ratio": 1.326797385620915, "no_speech_prob": 4.3568629735091235e-06}, {"id": 116, "seek": 56928, "start": 587.64, "end": 589.64, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.1717786442149769, "compression_ratio": 1.326797385620915, "no_speech_prob": 4.3568629735091235e-06}, {"id": 117, "seek": 56928, "start": 593.04, "end": 595.04, "text": " We were", "tokens": [492, 645], "temperature": 0.0, "avg_logprob": -0.1717786442149769, "compression_ratio": 1.326797385620915, "no_speech_prob": 4.3568629735091235e-06}, {"id": 118, "seek": 56928, "start": 596.28, "end": 598.12, "text": " Talking about", "tokens": [22445, 466], "temperature": 0.0, "avg_logprob": -0.1717786442149769, "compression_ratio": 1.326797385620915, "no_speech_prob": 4.3568629735091235e-06}, {"id": 119, "seek": 59812, "start": 598.12, "end": 601.16, "text": " Then using this kind of logistic regression", "tokens": [1396, 1228, 341, 733, 295, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.22887784784490411, "compression_ratio": 1.5970149253731343, "no_speech_prob": 1.760335408107494e-06}, {"id": 120, "seek": 59812, "start": 602.5600000000001, "end": 610.5, "text": " For NLP and before we got to that point we were talking about using naive base for NLP", "tokens": [1171, 426, 45196, 293, 949, 321, 658, 281, 300, 935, 321, 645, 1417, 466, 1228, 29052, 3096, 337, 426, 45196], "temperature": 0.0, "avg_logprob": -0.22887784784490411, "compression_ratio": 1.5970149253731343, "no_speech_prob": 1.760335408107494e-06}, {"id": 121, "seek": 59812, "start": 612.48, "end": 616.4, "text": " And the basic idea was that we could take a document", "tokens": [400, 264, 3875, 1558, 390, 300, 321, 727, 747, 257, 4166], "temperature": 0.0, "avg_logprob": -0.22887784784490411, "compression_ratio": 1.5970149253731343, "no_speech_prob": 1.760335408107494e-06}, {"id": 122, "seek": 59812, "start": 617.08, "end": 624.8, "text": " right a review like this movie is good and turn it into a bag of words representation consisting of the number of times", "tokens": [558, 257, 3131, 411, 341, 3169, 307, 665, 293, 1261, 309, 666, 257, 3411, 295, 2283, 10290, 33921, 295, 264, 1230, 295, 1413], "temperature": 0.0, "avg_logprob": -0.22887784784490411, "compression_ratio": 1.5970149253731343, "no_speech_prob": 1.760335408107494e-06}, {"id": 123, "seek": 59812, "start": 625.12, "end": 627.12, "text": " each word appears", "tokens": [1184, 1349, 7038], "temperature": 0.0, "avg_logprob": -0.22887784784490411, "compression_ratio": 1.5970149253731343, "no_speech_prob": 1.760335408107494e-06}, {"id": 124, "seek": 62712, "start": 627.12, "end": 633.88, "text": " All right, and we call this the vocabulary. This is the unique list of words. Okay, and we used the", "tokens": [1057, 558, 11, 293, 321, 818, 341, 264, 19864, 13, 639, 307, 264, 3845, 1329, 295, 2283, 13, 1033, 11, 293, 321, 1143, 264], "temperature": 0.0, "avg_logprob": -0.256016476949056, "compression_ratio": 1.6837606837606838, "no_speech_prob": 7.183199613791658e-06}, {"id": 125, "seek": 62712, "start": 635.2, "end": 642.76, "text": " SK learn count vectorizer to automatically generate both the vocabulary which in SK learn they call they call the features", "tokens": [21483, 1466, 1207, 8062, 6545, 281, 6772, 8460, 1293, 264, 19864, 597, 294, 21483, 1466, 436, 818, 436, 818, 264, 4122], "temperature": 0.0, "avg_logprob": -0.256016476949056, "compression_ratio": 1.6837606837606838, "no_speech_prob": 7.183199613791658e-06}, {"id": 126, "seek": 62712, "start": 643.36, "end": 650.28, "text": " And to call create the bag of words representations and the whole group of them then is called a term document", "tokens": [400, 281, 818, 1884, 264, 3411, 295, 2283, 33358, 293, 264, 1379, 1594, 295, 552, 550, 307, 1219, 257, 1433, 4166], "temperature": 0.0, "avg_logprob": -0.256016476949056, "compression_ratio": 1.6837606837606838, "no_speech_prob": 7.183199613791658e-06}, {"id": 127, "seek": 62712, "start": 650.72, "end": 652.72, "text": " matrix, okay", "tokens": [8141, 11, 1392], "temperature": 0.0, "avg_logprob": -0.256016476949056, "compression_ratio": 1.6837606837606838, "no_speech_prob": 7.183199613791658e-06}, {"id": 128, "seek": 62712, "start": 652.88, "end": 655.82, "text": " And we kind of realized that we could calculate", "tokens": [400, 321, 733, 295, 5334, 300, 321, 727, 8873], "temperature": 0.0, "avg_logprob": -0.256016476949056, "compression_ratio": 1.6837606837606838, "no_speech_prob": 7.183199613791658e-06}, {"id": 129, "seek": 65582, "start": 655.82, "end": 657.74, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.22687808169594295, "compression_ratio": 1.701657458563536, "no_speech_prob": 7.934480095173058e-07}, {"id": 130, "seek": 65582, "start": 657.74, "end": 659.74, "text": " probability that", "tokens": [8482, 300], "temperature": 0.0, "avg_logprob": -0.22687808169594295, "compression_ratio": 1.701657458563536, "no_speech_prob": 7.934480095173058e-07}, {"id": 131, "seek": 65582, "start": 660.3000000000001, "end": 661.9000000000001, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.22687808169594295, "compression_ratio": 1.701657458563536, "no_speech_prob": 7.934480095173058e-07}, {"id": 132, "seek": 65582, "start": 661.9000000000001, "end": 665.2600000000001, "text": " Positive review contains the word this by just", "tokens": [46326, 3131, 8306, 264, 1349, 341, 538, 445], "temperature": 0.0, "avg_logprob": -0.22687808169594295, "compression_ratio": 1.701657458563536, "no_speech_prob": 7.934480095173058e-07}, {"id": 133, "seek": 65582, "start": 666.1400000000001, "end": 670.58, "text": " Averaging the number of times this appears in the positive reviews, and we could do the same for the", "tokens": [316, 331, 3568, 264, 1230, 295, 1413, 341, 7038, 294, 264, 3353, 10229, 11, 293, 321, 727, 360, 264, 912, 337, 264], "temperature": 0.0, "avg_logprob": -0.22687808169594295, "compression_ratio": 1.701657458563536, "no_speech_prob": 7.934480095173058e-07}, {"id": 134, "seek": 65582, "start": 672.7800000000001, "end": 674.7800000000001, "text": " And we could do the same for the negatives", "tokens": [400, 321, 727, 360, 264, 912, 337, 264, 40019], "temperature": 0.0, "avg_logprob": -0.22687808169594295, "compression_ratio": 1.701657458563536, "no_speech_prob": 7.934480095173058e-07}, {"id": 135, "seek": 65582, "start": 675.22, "end": 679.9000000000001, "text": " Right and then we could take the ratio of them to get something which if it's greater than one", "tokens": [1779, 293, 550, 321, 727, 747, 264, 8509, 295, 552, 281, 483, 746, 597, 498, 309, 311, 5044, 813, 472], "temperature": 0.0, "avg_logprob": -0.22687808169594295, "compression_ratio": 1.701657458563536, "no_speech_prob": 7.934480095173058e-07}, {"id": 136, "seek": 67990, "start": 679.9, "end": 686.6999999999999, "text": " Was a word that appeared more often in the positive reviews or less than one was a word that appeared", "tokens": [3027, 257, 1349, 300, 8516, 544, 2049, 294, 264, 3353, 10229, 420, 1570, 813, 472, 390, 257, 1349, 300, 8516], "temperature": 0.0, "avg_logprob": -0.19059798538043934, "compression_ratio": 1.8558139534883722, "no_speech_prob": 6.375533985192305e-07}, {"id": 137, "seek": 67990, "start": 687.18, "end": 689.18, "text": " more often in the negative reviews", "tokens": [544, 2049, 294, 264, 3671, 10229], "temperature": 0.0, "avg_logprob": -0.19059798538043934, "compression_ratio": 1.8558139534883722, "no_speech_prob": 6.375533985192305e-07}, {"id": 138, "seek": 67990, "start": 689.42, "end": 691.42, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.19059798538043934, "compression_ratio": 1.8558139534883722, "no_speech_prob": 6.375533985192305e-07}, {"id": 139, "seek": 67990, "start": 691.42, "end": 692.38, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.19059798538043934, "compression_ratio": 1.8558139534883722, "no_speech_prob": 6.375533985192305e-07}, {"id": 140, "seek": 67990, "start": 692.38, "end": 697.9399999999999, "text": " Then we realized you know using using Bayes rule that and taking the logs", "tokens": [1396, 321, 5334, 291, 458, 1228, 1228, 7840, 279, 4978, 300, 293, 1940, 264, 20820], "temperature": 0.0, "avg_logprob": -0.19059798538043934, "compression_ratio": 1.8558139534883722, "no_speech_prob": 6.375533985192305e-07}, {"id": 141, "seek": 67990, "start": 698.5799999999999, "end": 703.02, "text": " That we could basically end up with something where we could add up the logs of these", "tokens": [663, 321, 727, 1936, 917, 493, 365, 746, 689, 321, 727, 909, 493, 264, 20820, 295, 613], "temperature": 0.0, "avg_logprob": -0.19059798538043934, "compression_ratio": 1.8558139534883722, "no_speech_prob": 6.375533985192305e-07}, {"id": 142, "seek": 70302, "start": 703.02, "end": 709.6999999999999, "text": " Plus the log of the ratio of the probabilities that things are in class one versus class zero", "tokens": [7721, 264, 3565, 295, 264, 8509, 295, 264, 33783, 300, 721, 366, 294, 1508, 472, 5717, 1508, 4018], "temperature": 0.0, "avg_logprob": -0.23693336038028492, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.8130043574492447e-06}, {"id": 143, "seek": 70302, "start": 710.46, "end": 713.5799999999999, "text": " And end up with something we can compare to zero", "tokens": [400, 917, 493, 365, 746, 321, 393, 6794, 281, 4018], "temperature": 0.0, "avg_logprob": -0.23693336038028492, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.8130043574492447e-06}, {"id": 144, "seek": 70302, "start": 714.22, "end": 719.86, "text": " It's a bit greater than zero then we can predict a document is positive or if it's less than zero", "tokens": [467, 311, 257, 857, 5044, 813, 4018, 550, 321, 393, 6069, 257, 4166, 307, 3353, 420, 498, 309, 311, 1570, 813, 4018], "temperature": 0.0, "avg_logprob": -0.23693336038028492, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.8130043574492447e-06}, {"id": 145, "seek": 70302, "start": 719.86, "end": 724.18, "text": " We can predict the document is negative, and that was our base rule, right?", "tokens": [492, 393, 6069, 264, 4166, 307, 3671, 11, 293, 300, 390, 527, 3096, 4978, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23693336038028492, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.8130043574492447e-06}, {"id": 146, "seek": 70302, "start": 725.42, "end": 727.38, "text": " So we kind of did that", "tokens": [407, 321, 733, 295, 630, 300], "temperature": 0.0, "avg_logprob": -0.23693336038028492, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.8130043574492447e-06}, {"id": 147, "seek": 72738, "start": 727.38, "end": 734.1, "text": " From math first principles, and I think we agreed that the naive in naive base was a good description", "tokens": [3358, 5221, 700, 9156, 11, 293, 286, 519, 321, 9166, 300, 264, 29052, 294, 29052, 3096, 390, 257, 665, 3855], "temperature": 0.0, "avg_logprob": -0.20322141647338868, "compression_ratio": 1.692883895131086, "no_speech_prob": 1.7603379092179239e-06}, {"id": 148, "seek": 72738, "start": 735.18, "end": 738.02, "text": " Because it assumes independence, but it's definitely not true", "tokens": [1436, 309, 37808, 14640, 11, 457, 309, 311, 2138, 406, 2074], "temperature": 0.0, "avg_logprob": -0.20322141647338868, "compression_ratio": 1.692883895131086, "no_speech_prob": 1.7603379092179239e-06}, {"id": 149, "seek": 72738, "start": 738.86, "end": 740.66, "text": " But it's an interesting starting point", "tokens": [583, 309, 311, 364, 1880, 2891, 935], "temperature": 0.0, "avg_logprob": -0.20322141647338868, "compression_ratio": 1.692883895131086, "no_speech_prob": 1.7603379092179239e-06}, {"id": 150, "seek": 72738, "start": 740.66, "end": 747.26, "text": " And I think it was interesting to observe when we actually got to the point where like okay now we've you know calculated", "tokens": [400, 286, 519, 309, 390, 1880, 281, 11441, 562, 321, 767, 658, 281, 264, 935, 689, 411, 1392, 586, 321, 600, 291, 458, 15598], "temperature": 0.0, "avg_logprob": -0.20322141647338868, "compression_ratio": 1.692883895131086, "no_speech_prob": 1.7603379092179239e-06}, {"id": 151, "seek": 72738, "start": 747.62, "end": 751.58, "text": " the the ratio of the probabilities and", "tokens": [264, 264, 8509, 295, 264, 33783, 293], "temperature": 0.0, "avg_logprob": -0.20322141647338868, "compression_ratio": 1.692883895131086, "no_speech_prob": 1.7603379092179239e-06}, {"id": 152, "seek": 75158, "start": 751.58, "end": 757.34, "text": " To it took the log and now rather than multiply them together of course we have to add them up and", "tokens": [1407, 309, 1890, 264, 3565, 293, 586, 2831, 813, 12972, 552, 1214, 295, 1164, 321, 362, 281, 909, 552, 493, 293], "temperature": 0.0, "avg_logprob": -0.16967889739245903, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.9947237888118252e-06}, {"id": 153, "seek": 75158, "start": 757.62, "end": 761.14, "text": " When when we actually wrote that down we realized like oh", "tokens": [1133, 562, 321, 767, 4114, 300, 760, 321, 5334, 411, 1954], "temperature": 0.0, "avg_logprob": -0.16967889739245903, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.9947237888118252e-06}, {"id": 154, "seek": 75158, "start": 761.74, "end": 763.74, "text": " That is", "tokens": [663, 307], "temperature": 0.0, "avg_logprob": -0.16967889739245903, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.9947237888118252e-06}, {"id": 155, "seek": 75158, "start": 764.3000000000001, "end": 766.3000000000001, "text": " You know just a standard", "tokens": [509, 458, 445, 257, 3832], "temperature": 0.0, "avg_logprob": -0.16967889739245903, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.9947237888118252e-06}, {"id": 156, "seek": 75158, "start": 768.7, "end": 770.7, "text": " Weight matrix", "tokens": [44464, 8141], "temperature": 0.0, "avg_logprob": -0.16967889739245903, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.9947237888118252e-06}, {"id": 157, "seek": 75158, "start": 770.82, "end": 772.82, "text": " product plus a bias", "tokens": [1674, 1804, 257, 12577], "temperature": 0.0, "avg_logprob": -0.16967889739245903, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.9947237888118252e-06}, {"id": 158, "seek": 75158, "start": 773.46, "end": 778.9000000000001, "text": " Right and so then we kind of realized like oh, okay, so like if this is not very good", "tokens": [1779, 293, 370, 550, 321, 733, 295, 5334, 411, 1954, 11, 1392, 11, 370, 411, 498, 341, 307, 406, 588, 665], "temperature": 0.0, "avg_logprob": -0.16967889739245903, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.9947237888118252e-06}, {"id": 159, "seek": 77890, "start": 778.9, "end": 780.9, "text": " accuracy 80% accuracy", "tokens": [14170, 4688, 4, 14170], "temperature": 0.0, "avg_logprob": -0.2904743334142173, "compression_ratio": 1.6868686868686869, "no_speech_prob": 6.179378715387429e-07}, {"id": 160, "seek": 77890, "start": 784.42, "end": 789.22, "text": " Why not improve it by saying hey we know other ways to calculate a", "tokens": [1545, 406, 3470, 309, 538, 1566, 4177, 321, 458, 661, 2098, 281, 8873, 257], "temperature": 0.0, "avg_logprob": -0.2904743334142173, "compression_ratio": 1.6868686868686869, "no_speech_prob": 6.179378715387429e-07}, {"id": 161, "seek": 77890, "start": 789.8199999999999, "end": 794.4599999999999, "text": " Cut a bunch of coefficients and a bunch of biases which is to", "tokens": [9431, 257, 3840, 295, 31994, 293, 257, 3840, 295, 32152, 597, 307, 281], "temperature": 0.0, "avg_logprob": -0.2904743334142173, "compression_ratio": 1.6868686868686869, "no_speech_prob": 6.179378715387429e-07}, {"id": 162, "seek": 77890, "start": 795.3, "end": 802.22, "text": " Learn them in a logistic regression right so in other words this this is the formula we use for a logistic regression", "tokens": [17216, 552, 294, 257, 3565, 3142, 24590, 558, 370, 294, 661, 2283, 341, 341, 307, 264, 8513, 321, 764, 337, 257, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.2904743334142173, "compression_ratio": 1.6868686868686869, "no_speech_prob": 6.179378715387429e-07}, {"id": 163, "seek": 80222, "start": 802.22, "end": 808.5400000000001, "text": " And so why don't we just create a logistic regression and fit it?", "tokens": [400, 370, 983, 500, 380, 321, 445, 1884, 257, 3565, 3142, 24590, 293, 3318, 309, 30], "temperature": 0.0, "avg_logprob": -0.1858734386723216, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.4593698551834677e-06}, {"id": 164, "seek": 80222, "start": 809.5400000000001, "end": 811.5400000000001, "text": " Okay, and it's going to be", "tokens": [1033, 11, 293, 309, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.1858734386723216, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.4593698551834677e-06}, {"id": 165, "seek": 80222, "start": 812.0600000000001, "end": 819.22, "text": " Give us the same thing, but rather than coefficients and biases which are theoretically correct based on", "tokens": [5303, 505, 264, 912, 551, 11, 457, 2831, 813, 31994, 293, 32152, 597, 366, 29400, 3006, 2361, 322], "temperature": 0.0, "avg_logprob": -0.1858734386723216, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.4593698551834677e-06}, {"id": 166, "seek": 80222, "start": 819.58, "end": 825.9, "text": " You know this assumption of independence and based on Bayes rule. There'll be the coefficients and biases that are", "tokens": [509, 458, 341, 15302, 295, 14640, 293, 2361, 322, 7840, 279, 4978, 13, 821, 603, 312, 264, 31994, 293, 32152, 300, 366], "temperature": 0.0, "avg_logprob": -0.1858734386723216, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.4593698551834677e-06}, {"id": 167, "seek": 80222, "start": 826.62, "end": 828.62, "text": " Actually the best in this data", "tokens": [5135, 264, 1151, 294, 341, 1412], "temperature": 0.0, "avg_logprob": -0.1858734386723216, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.4593698551834677e-06}, {"id": 168, "seek": 82862, "start": 828.62, "end": 832.74, "text": " All right, so that was kind of where we got to and so", "tokens": [1057, 558, 11, 370, 300, 390, 733, 295, 689, 321, 658, 281, 293, 370], "temperature": 0.0, "avg_logprob": -0.20383917255166137, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.5612707759137265e-06}, {"id": 169, "seek": 82862, "start": 834.46, "end": 837.34, "text": " The kind of key insight here is", "tokens": [440, 733, 295, 2141, 11269, 510, 307], "temperature": 0.0, "avg_logprob": -0.20383917255166137, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.5612707759137265e-06}, {"id": 170, "seek": 82862, "start": 838.82, "end": 840.7, "text": " like", "tokens": [411], "temperature": 0.0, "avg_logprob": -0.20383917255166137, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.5612707759137265e-06}, {"id": 171, "seek": 82862, "start": 840.7, "end": 847.26, "text": " Just about everything I find a machine learning ends up being either like a tree or", "tokens": [1449, 466, 1203, 286, 915, 257, 3479, 2539, 5314, 493, 885, 2139, 411, 257, 4230, 420], "temperature": 0.0, "avg_logprob": -0.20383917255166137, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.5612707759137265e-06}, {"id": 172, "seek": 82862, "start": 848.82, "end": 851.54, "text": " You know a bunch of matrix products and monumeralities", "tokens": [509, 458, 257, 3840, 295, 8141, 3383, 293, 1108, 449, 2790, 1088], "temperature": 0.0, "avg_logprob": -0.20383917255166137, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.5612707759137265e-06}, {"id": 173, "seek": 82862, "start": 852.02, "end": 856.54, "text": " Right like it's everything seems to end up kind of coming down to the same thing", "tokens": [1779, 411, 309, 311, 1203, 2544, 281, 917, 493, 733, 295, 1348, 760, 281, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.20383917255166137, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.5612707759137265e-06}, {"id": 174, "seek": 85654, "start": 856.54, "end": 859.9, "text": " And including as it turns out Bayes rule", "tokens": [400, 3009, 382, 309, 4523, 484, 7840, 279, 4978], "temperature": 0.0, "avg_logprob": -0.2186401757326993, "compression_ratio": 1.662162162162162, "no_speech_prob": 1.577957164045074e-06}, {"id": 175, "seek": 85654, "start": 859.9, "end": 867.8399999999999, "text": " All right, and then it turns out that nearly all of the time then whatever the parameters are in that function", "tokens": [1057, 558, 11, 293, 550, 309, 4523, 484, 300, 6217, 439, 295, 264, 565, 550, 2035, 264, 9834, 366, 294, 300, 2445], "temperature": 0.0, "avg_logprob": -0.2186401757326993, "compression_ratio": 1.662162162162162, "no_speech_prob": 1.577957164045074e-06}, {"id": 176, "seek": 85654, "start": 868.8199999999999, "end": 870.8199999999999, "text": " Nearly all the time it turns out that they're better", "tokens": [38000, 439, 264, 565, 309, 4523, 484, 300, 436, 434, 1101], "temperature": 0.0, "avg_logprob": -0.2186401757326993, "compression_ratio": 1.662162162162162, "no_speech_prob": 1.577957164045074e-06}, {"id": 177, "seek": 85654, "start": 871.4599999999999, "end": 873.4599999999999, "text": " learned than", "tokens": [3264, 813], "temperature": 0.0, "avg_logprob": -0.2186401757326993, "compression_ratio": 1.662162162162162, "no_speech_prob": 1.577957164045074e-06}, {"id": 178, "seek": 85654, "start": 874.06, "end": 879.8199999999999, "text": " Calculated based on theory right and indeed. That's what happened when we actually tried learning those coefficients", "tokens": [3511, 2444, 770, 2361, 322, 5261, 558, 293, 6451, 13, 663, 311, 437, 2011, 562, 321, 767, 3031, 2539, 729, 31994], "temperature": 0.0, "avg_logprob": -0.2186401757326993, "compression_ratio": 1.662162162162162, "no_speech_prob": 1.577957164045074e-06}, {"id": 179, "seek": 85654, "start": 880.06, "end": 882.3, "text": " We got you know 85 percent", "tokens": [492, 658, 291, 458, 14695, 3043], "temperature": 0.0, "avg_logprob": -0.2186401757326993, "compression_ratio": 1.662162162162162, "no_speech_prob": 1.577957164045074e-06}, {"id": 180, "seek": 88230, "start": 882.3, "end": 885.26, "text": " right, so then", "tokens": [558, 11, 370, 550], "temperature": 0.0, "avg_logprob": -0.20373148857792722, "compression_ratio": 1.7087378640776698, "no_speech_prob": 1.3287708497955464e-06}, {"id": 181, "seek": 88230, "start": 886.5799999999999, "end": 891.1999999999999, "text": " We noticed that we could also rather than take the whole term document matrix", "tokens": [492, 5694, 300, 321, 727, 611, 2831, 813, 747, 264, 1379, 1433, 4166, 8141], "temperature": 0.0, "avg_logprob": -0.20373148857792722, "compression_ratio": 1.7087378640776698, "no_speech_prob": 1.3287708497955464e-06}, {"id": 182, "seek": 88230, "start": 891.1999999999999, "end": 896.54, "text": " We could instead just take them the you know ones and zeros for presence or absence of a word", "tokens": [492, 727, 2602, 445, 747, 552, 264, 291, 458, 2306, 293, 35193, 337, 6814, 420, 17145, 295, 257, 1349], "temperature": 0.0, "avg_logprob": -0.20373148857792722, "compression_ratio": 1.7087378640776698, "no_speech_prob": 1.3287708497955464e-06}, {"id": 183, "seek": 88230, "start": 897.4599999999999, "end": 901.5, "text": " And you know sometimes it was you know this equally as good", "tokens": [400, 291, 458, 2171, 309, 390, 291, 458, 341, 12309, 382, 665], "temperature": 0.0, "avg_logprob": -0.20373148857792722, "compression_ratio": 1.7087378640776698, "no_speech_prob": 1.3287708497955464e-06}, {"id": 184, "seek": 88230, "start": 903.3, "end": 907.9399999999999, "text": " But then we actually tried something else which is we tried adding regularization and with regularization", "tokens": [583, 550, 321, 767, 3031, 746, 1646, 597, 307, 321, 3031, 5127, 3890, 2144, 293, 365, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.20373148857792722, "compression_ratio": 1.7087378640776698, "no_speech_prob": 1.3287708497955464e-06}, {"id": 185, "seek": 90794, "start": 907.94, "end": 913.48, "text": " The binarized approach turned out to be a little better right so then regularization", "tokens": [440, 5171, 289, 1602, 3109, 3574, 484, 281, 312, 257, 707, 1101, 558, 370, 550, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.26160621643066406, "compression_ratio": 1.46524064171123, "no_speech_prob": 1.1015939662684104e-06}, {"id": 186, "seek": 90794, "start": 915.2600000000001, "end": 917.2600000000001, "text": " Was where we took the loss function", "tokens": [3027, 689, 321, 1890, 264, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.26160621643066406, "compression_ratio": 1.46524064171123, "no_speech_prob": 1.1015939662684104e-06}, {"id": 187, "seek": 90794, "start": 919.7, "end": 925.0, "text": " And again let's start with RMSE, and then we'll talk about cross entropy loss function was", "tokens": [400, 797, 718, 311, 722, 365, 23790, 5879, 11, 293, 550, 321, 603, 751, 466, 3278, 30867, 4470, 2445, 390], "temperature": 0.0, "avg_logprob": -0.26160621643066406, "compression_ratio": 1.46524064171123, "no_speech_prob": 1.1015939662684104e-06}, {"id": 188, "seek": 90794, "start": 927.0600000000001, "end": 929.0600000000001, "text": " Now predictions", "tokens": [823, 21264], "temperature": 0.0, "avg_logprob": -0.26160621643066406, "compression_ratio": 1.46524064171123, "no_speech_prob": 1.1015939662684104e-06}, {"id": 189, "seek": 90794, "start": 929.82, "end": 934.82, "text": " Minus our actuals sum that up take the average", "tokens": [2829, 301, 527, 3539, 82, 2408, 300, 493, 747, 264, 4274], "temperature": 0.0, "avg_logprob": -0.26160621643066406, "compression_ratio": 1.46524064171123, "no_speech_prob": 1.1015939662684104e-06}, {"id": 190, "seek": 93482, "start": 934.82, "end": 936.82, "text": " plus", "tokens": [1804], "temperature": 0.0, "avg_logprob": -0.30127009639033564, "compression_ratio": 1.31496062992126, "no_speech_prob": 7.571136393380584e-07}, {"id": 191, "seek": 93482, "start": 937.1800000000001, "end": 939.1800000000001, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.30127009639033564, "compression_ratio": 1.31496062992126, "no_speech_prob": 7.571136393380584e-07}, {"id": 192, "seek": 93482, "start": 940.5400000000001, "end": 941.7, "text": " Penalty", "tokens": [10571, 304, 874], "temperature": 0.0, "avg_logprob": -0.30127009639033564, "compression_ratio": 1.31496062992126, "no_speech_prob": 7.571136393380584e-07}, {"id": 193, "seek": 93482, "start": 941.7, "end": 950.3000000000001, "text": " Okay, and so this specifically is the L2 penalty if this instead was the absolute value of W", "tokens": [1033, 11, 293, 370, 341, 4682, 307, 264, 441, 17, 16263, 498, 341, 2602, 390, 264, 8236, 2158, 295, 343], "temperature": 0.0, "avg_logprob": -0.30127009639033564, "compression_ratio": 1.31496062992126, "no_speech_prob": 7.571136393380584e-07}, {"id": 194, "seek": 93482, "start": 950.98, "end": 954.4200000000001, "text": " Then that would be the L1 penalty, okay?", "tokens": [1396, 300, 576, 312, 264, 441, 16, 16263, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.30127009639033564, "compression_ratio": 1.31496062992126, "no_speech_prob": 7.571136393380584e-07}, {"id": 195, "seek": 93482, "start": 958.1400000000001, "end": 960.1400000000001, "text": " We also noted that", "tokens": [492, 611, 12964, 300], "temperature": 0.0, "avg_logprob": -0.30127009639033564, "compression_ratio": 1.31496062992126, "no_speech_prob": 7.571136393380584e-07}, {"id": 196, "seek": 96014, "start": 960.14, "end": 967.3199999999999, "text": " We don't really care about the loss function per se we only care about its derivative. That's actually the thing that updates the weights", "tokens": [492, 500, 380, 534, 1127, 466, 264, 4470, 2445, 680, 369, 321, 787, 1127, 466, 1080, 13760, 13, 663, 311, 767, 264, 551, 300, 9205, 264, 17443], "temperature": 0.0, "avg_logprob": -0.17042642616363893, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.459370196243981e-06}, {"id": 197, "seek": 96014, "start": 968.14, "end": 975.74, "text": " So we can because this is a sum we can take the derivative of each part separately and so the derivative of this part was just", "tokens": [407, 321, 393, 570, 341, 307, 257, 2408, 321, 393, 747, 264, 13760, 295, 1184, 644, 14759, 293, 370, 264, 13760, 295, 341, 644, 390, 445], "temperature": 0.0, "avg_logprob": -0.17042642616363893, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.459370196243981e-06}, {"id": 198, "seek": 96014, "start": 977.06, "end": 978.3, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.17042642616363893, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.459370196243981e-06}, {"id": 199, "seek": 96014, "start": 978.3, "end": 984.04, "text": " Right and so we kind of learnt that even though these are mathematically equivalent. They have different names", "tokens": [1779, 293, 370, 321, 733, 295, 18991, 300, 754, 1673, 613, 366, 44003, 10344, 13, 814, 362, 819, 5288], "temperature": 0.0, "avg_logprob": -0.17042642616363893, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.459370196243981e-06}, {"id": 200, "seek": 98404, "start": 984.04, "end": 991.12, "text": " This version is called weight decay and it's kind of what's used that term is used in the neural net literature", "tokens": [639, 3037, 307, 1219, 3364, 21039, 293, 309, 311, 733, 295, 437, 311, 1143, 300, 1433, 307, 1143, 294, 264, 18161, 2533, 10394], "temperature": 0.0, "avg_logprob": -0.20663406054178873, "compression_ratio": 1.514792899408284, "no_speech_prob": 3.156121408665058e-07}, {"id": 201, "seek": 98404, "start": 991.5999999999999, "end": 993.5999999999999, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20663406054178873, "compression_ratio": 1.514792899408284, "no_speech_prob": 3.156121408665058e-07}, {"id": 202, "seek": 98404, "start": 994.64, "end": 1000.9599999999999, "text": " So cross entropy on the other hand you know it's just another loss function like root mean squared error", "tokens": [407, 3278, 30867, 322, 264, 661, 1011, 291, 458, 309, 311, 445, 1071, 4470, 2445, 411, 5593, 914, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.20663406054178873, "compression_ratio": 1.514792899408284, "no_speech_prob": 3.156121408665058e-07}, {"id": 203, "seek": 98404, "start": 1009.04, "end": 1011.04, "text": " But it's specifically designed for", "tokens": [583, 309, 311, 4682, 4761, 337], "temperature": 0.0, "avg_logprob": -0.20663406054178873, "compression_ratio": 1.514792899408284, "no_speech_prob": 3.156121408665058e-07}, {"id": 204, "seek": 101104, "start": 1011.04, "end": 1013.04, "text": " classification", "tokens": [21538], "temperature": 0.0, "avg_logprob": -0.26678218841552737, "compression_ratio": 1.603960396039604, "no_speech_prob": 1.933349949467811e-06}, {"id": 205, "seek": 101104, "start": 1013.04, "end": 1014.3199999999999, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.26678218841552737, "compression_ratio": 1.603960396039604, "no_speech_prob": 1.933349949467811e-06}, {"id": 206, "seek": 101104, "start": 1014.3199999999999, "end": 1021.38, "text": " And so here's an example of binary cross entropy, so let's say this is our you know is it a cat or a dog?", "tokens": [400, 370, 510, 311, 364, 1365, 295, 17434, 3278, 30867, 11, 370, 718, 311, 584, 341, 307, 527, 291, 458, 307, 309, 257, 3857, 420, 257, 3000, 30], "temperature": 0.0, "avg_logprob": -0.26678218841552737, "compression_ratio": 1.603960396039604, "no_speech_prob": 1.933349949467811e-06}, {"id": 207, "seek": 101104, "start": 1021.38, "end": 1028.0, "text": " So let's just say is cat one or a zero so cat cat dog dog cat and these are our", "tokens": [407, 718, 311, 445, 584, 307, 3857, 472, 420, 257, 4018, 370, 3857, 3857, 3000, 3000, 3857, 293, 613, 366, 527], "temperature": 0.0, "avg_logprob": -0.26678218841552737, "compression_ratio": 1.603960396039604, "no_speech_prob": 1.933349949467811e-06}, {"id": 208, "seek": 101104, "start": 1029.04, "end": 1031.2, "text": " Predictions this is the output of our", "tokens": [32969, 15607, 341, 307, 264, 5598, 295, 527], "temperature": 0.0, "avg_logprob": -0.26678218841552737, "compression_ratio": 1.603960396039604, "no_speech_prob": 1.933349949467811e-06}, {"id": 209, "seek": 101104, "start": 1032.0, "end": 1035.08, "text": " final layer of our neural net or our logistic regression or whatever", "tokens": [2572, 4583, 295, 527, 18161, 2533, 420, 527, 3565, 3142, 24590, 420, 2035], "temperature": 0.0, "avg_logprob": -0.26678218841552737, "compression_ratio": 1.603960396039604, "no_speech_prob": 1.933349949467811e-06}, {"id": 210, "seek": 101104, "start": 1036.36, "end": 1038.36, "text": " right then", "tokens": [558, 550], "temperature": 0.0, "avg_logprob": -0.26678218841552737, "compression_ratio": 1.603960396039604, "no_speech_prob": 1.933349949467811e-06}, {"id": 211, "seek": 103836, "start": 1038.36, "end": 1041.36, "text": " All we do is we say okay, let's take", "tokens": [1057, 321, 360, 307, 321, 584, 1392, 11, 718, 311, 747], "temperature": 0.0, "avg_logprob": -0.1679072177156489, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.1544595963641768e-06}, {"id": 212, "seek": 103836, "start": 1043.24, "end": 1045.24, "text": " The the actual", "tokens": [440, 264, 3539], "temperature": 0.0, "avg_logprob": -0.1679072177156489, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.1544595963641768e-06}, {"id": 213, "seek": 103836, "start": 1045.6399999999999, "end": 1047.6399999999999, "text": " times the log of the prediction and", "tokens": [1413, 264, 3565, 295, 264, 17630, 293], "temperature": 0.0, "avg_logprob": -0.1679072177156489, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.1544595963641768e-06}, {"id": 214, "seek": 103836, "start": 1048.3999999999999, "end": 1055.7199999999998, "text": " Then we add to that 1 minus actual times the log of 1 minus the prediction and then take the negative of that whole thing", "tokens": [1396, 321, 909, 281, 300, 502, 3175, 3539, 1413, 264, 3565, 295, 502, 3175, 264, 17630, 293, 550, 747, 264, 3671, 295, 300, 1379, 551], "temperature": 0.0, "avg_logprob": -0.1679072177156489, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.1544595963641768e-06}, {"id": 215, "seek": 103836, "start": 1057.3999999999999, "end": 1059.1999999999998, "text": " Alright", "tokens": [2798], "temperature": 0.0, "avg_logprob": -0.1679072177156489, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.1544595963641768e-06}, {"id": 216, "seek": 103836, "start": 1059.1999999999998, "end": 1061.08, "text": " so I", "tokens": [370, 286], "temperature": 0.0, "avg_logprob": -0.1679072177156489, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.1544595963641768e-06}, {"id": 217, "seek": 103836, "start": 1061.08, "end": 1067.0, "text": " Suggested to you to you all that you tried to kind of write the if statement version of this so hopefully you've done that by now", "tokens": [39131, 2629, 292, 281, 291, 281, 291, 439, 300, 291, 3031, 281, 733, 295, 2464, 264, 498, 5629, 3037, 295, 341, 370, 4696, 291, 600, 1096, 300, 538, 586], "temperature": 0.0, "avg_logprob": -0.1679072177156489, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.1544595963641768e-06}, {"id": 218, "seek": 106700, "start": 1067.0, "end": 1070.4, "text": " Otherwise, I'm about to spoil it for you. So this was", "tokens": [10328, 11, 286, 478, 466, 281, 18630, 309, 337, 291, 13, 407, 341, 390], "temperature": 0.0, "avg_logprob": -0.2651111545847423, "compression_ratio": 1.3971631205673758, "no_speech_prob": 5.338120899978094e-06}, {"id": 219, "seek": 106700, "start": 1071.36, "end": 1072.72, "text": " y", "tokens": [288], "temperature": 0.0, "avg_logprob": -0.2651111545847423, "compression_ratio": 1.3971631205673758, "no_speech_prob": 5.338120899978094e-06}, {"id": 220, "seek": 106700, "start": 1072.72, "end": 1074.72, "text": " times log", "tokens": [1413, 3565], "temperature": 0.0, "avg_logprob": -0.2651111545847423, "compression_ratio": 1.3971631205673758, "no_speech_prob": 5.338120899978094e-06}, {"id": 221, "seek": 106700, "start": 1075.24, "end": 1076.56, "text": " y", "tokens": [288], "temperature": 0.0, "avg_logprob": -0.2651111545847423, "compression_ratio": 1.3971631205673758, "no_speech_prob": 5.338120899978094e-06}, {"id": 222, "seek": 106700, "start": 1076.56, "end": 1078.04, "text": " plus", "tokens": [1804], "temperature": 0.0, "avg_logprob": -0.2651111545847423, "compression_ratio": 1.3971631205673758, "no_speech_prob": 5.338120899978094e-06}, {"id": 223, "seek": 106700, "start": 1078.04, "end": 1081.36, "text": " 1 minus y times log", "tokens": [502, 3175, 288, 1413, 3565], "temperature": 0.0, "avg_logprob": -0.2651111545847423, "compression_ratio": 1.3971631205673758, "no_speech_prob": 5.338120899978094e-06}, {"id": 224, "seek": 106700, "start": 1082.56, "end": 1084.56, "text": " 1 minus y", "tokens": [502, 3175, 288], "temperature": 0.0, "avg_logprob": -0.2651111545847423, "compression_ratio": 1.3971631205673758, "no_speech_prob": 5.338120899978094e-06}, {"id": 225, "seek": 106700, "start": 1084.8, "end": 1090.4, "text": " Right and negative of that okay, so who wants to tell me how to write this as an if statement?", "tokens": [1779, 293, 3671, 295, 300, 1392, 11, 370, 567, 2738, 281, 980, 385, 577, 281, 2464, 341, 382, 364, 498, 5629, 30], "temperature": 0.0, "avg_logprob": -0.2651111545847423, "compression_ratio": 1.3971631205673758, "no_speech_prob": 5.338120899978094e-06}, {"id": 226, "seek": 109040, "start": 1090.4, "end": 1092.4, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.5839688777923584, "compression_ratio": 1.3741007194244603, "no_speech_prob": 2.212350591435097e-05}, {"id": 227, "seek": 109040, "start": 1096.64, "end": 1101.48, "text": " Chenshi hit me okay, so if y equal to", "tokens": [761, 694, 4954, 2045, 385, 1392, 11, 370, 498, 288, 2681, 281], "temperature": 0.0, "avg_logprob": -0.5839688777923584, "compression_ratio": 1.3741007194244603, "no_speech_prob": 2.212350591435097e-05}, {"id": 228, "seek": 109040, "start": 1102.2800000000002, "end": 1107.68, "text": " Sorry if y equal to 1 then return log y. Mm-hmm", "tokens": [4919, 498, 288, 2681, 281, 502, 550, 2736, 3565, 288, 13, 8266, 12, 10250], "temperature": 0.0, "avg_logprob": -0.5839688777923584, "compression_ratio": 1.3741007194244603, "no_speech_prob": 2.212350591435097e-05}, {"id": 229, "seek": 109040, "start": 1108.5600000000002, "end": 1109.72, "text": " otherwise", "tokens": [5911], "temperature": 0.0, "avg_logprob": -0.5839688777923584, "compression_ratio": 1.3741007194244603, "no_speech_prob": 2.212350591435097e-05}, {"id": 230, "seek": 109040, "start": 1109.72, "end": 1116.8000000000002, "text": " Well, um house return log 1 minus 1 good. Oh, that's the thing is a brackets and you take see", "tokens": [1042, 11, 1105, 1782, 2736, 3565, 502, 3175, 502, 665, 13, 876, 11, 300, 311, 264, 551, 307, 257, 26179, 293, 291, 747, 536], "temperature": 0.0, "avg_logprob": -0.5839688777923584, "compression_ratio": 1.3741007194244603, "no_speech_prob": 2.212350591435097e-05}, {"id": 231, "seek": 111680, "start": 1116.8, "end": 1122.6399999999999, "text": " Why that's good. So the key insight Chen she's using is that y has two possibilities one or zero", "tokens": [1545, 300, 311, 665, 13, 407, 264, 2141, 11269, 13682, 750, 311, 1228, 307, 300, 288, 575, 732, 12178, 472, 420, 4018], "temperature": 0.0, "avg_logprob": -0.2584757852082205, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.7061613561963895e-06}, {"id": 232, "seek": 111680, "start": 1122.84, "end": 1125.56, "text": " Okay, and so very often the math", "tokens": [1033, 11, 293, 370, 588, 2049, 264, 5221], "temperature": 0.0, "avg_logprob": -0.2584757852082205, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.7061613561963895e-06}, {"id": 233, "seek": 111680, "start": 1126.08, "end": 1127.76, "text": " can hide", "tokens": [393, 6479], "temperature": 0.0, "avg_logprob": -0.2584757852082205, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.7061613561963895e-06}, {"id": 234, "seek": 111680, "start": 1127.76, "end": 1133.02, "text": " The key insight which I think happens here until you actually think about what the values it can take", "tokens": [440, 2141, 11269, 597, 286, 519, 2314, 510, 1826, 291, 767, 519, 466, 437, 264, 4190, 309, 393, 747], "temperature": 0.0, "avg_logprob": -0.2584757852082205, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.7061613561963895e-06}, {"id": 235, "seek": 111680, "start": 1133.68, "end": 1135.68, "text": " right, so", "tokens": [558, 11, 370], "temperature": 0.0, "avg_logprob": -0.2584757852082205, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.7061613561963895e-06}, {"id": 236, "seek": 111680, "start": 1136.04, "end": 1141.68, "text": " That's that's all it's doing. It's saying either. Give me that or give me that", "tokens": [663, 311, 300, 311, 439, 309, 311, 884, 13, 467, 311, 1566, 2139, 13, 5303, 385, 300, 420, 976, 385, 300], "temperature": 0.0, "avg_logprob": -0.2584757852082205, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.7061613561963895e-06}, {"id": 237, "seek": 114168, "start": 1141.68, "end": 1146.8400000000001, "text": " All right, could you pass that to the back place change? You?", "tokens": [1057, 558, 11, 727, 291, 1320, 300, 281, 264, 646, 1081, 1319, 30, 509, 30], "temperature": 0.0, "avg_logprob": -0.5200786590576172, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.972612133424263e-06}, {"id": 238, "seek": 114168, "start": 1147.72, "end": 1151.96, "text": " Find missing something but do you know the two variables in that statement? You got why?", "tokens": [11809, 5361, 746, 457, 360, 291, 458, 264, 732, 9102, 294, 300, 5629, 30, 509, 658, 983, 30], "temperature": 0.0, "avg_logprob": -0.5200786590576172, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.972612133424263e-06}, {"id": 239, "seek": 114168, "start": 1153.44, "end": 1157.04, "text": " Shouldn't be like why happened a while? Oh, yeah. Thank you", "tokens": [34170, 380, 312, 411, 983, 2011, 257, 1339, 30, 876, 11, 1338, 13, 1044, 291], "temperature": 0.0, "avg_logprob": -0.5200786590576172, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.972612133424263e-06}, {"id": 240, "seek": 114168, "start": 1159.3200000000002, "end": 1162.04, "text": " As usual it's may missing something", "tokens": [1018, 7713, 309, 311, 815, 5361, 746], "temperature": 0.0, "avg_logprob": -0.5200786590576172, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.972612133424263e-06}, {"id": 241, "seek": 114168, "start": 1164.1200000000001, "end": 1166.1200000000001, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.5200786590576172, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.972612133424263e-06}, {"id": 242, "seek": 114168, "start": 1166.96, "end": 1170.28, "text": " Okay, and so then the you know the multi", "tokens": [1033, 11, 293, 370, 550, 264, 291, 458, 264, 4825], "temperature": 0.0, "avg_logprob": -0.5200786590576172, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.972612133424263e-06}, {"id": 243, "seek": 117028, "start": 1170.28, "end": 1177.56, "text": " Category version is just the same thing, but you're saying you know it for different more than just y equals 1 or 0", "tokens": [383, 48701, 3037, 307, 445, 264, 912, 551, 11, 457, 291, 434, 1566, 291, 458, 309, 337, 819, 544, 813, 445, 288, 6915, 502, 420, 1958], "temperature": 0.0, "avg_logprob": -0.2075072113348513, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.9550811783992685e-06}, {"id": 244, "seek": 117028, "start": 1177.56, "end": 1180.82, "text": " But y equals 0 1 2 3 4 5 6 7 8 9 for instance", "tokens": [583, 288, 6915, 1958, 502, 568, 805, 1017, 1025, 1386, 1614, 1649, 1722, 337, 5197], "temperature": 0.0, "avg_logprob": -0.2075072113348513, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.9550811783992685e-06}, {"id": 245, "seek": 117028, "start": 1181.52, "end": 1183.04, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2075072113348513, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.9550811783992685e-06}, {"id": 246, "seek": 117028, "start": 1183.04, "end": 1184.12, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2075072113348513, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.9550811783992685e-06}, {"id": 247, "seek": 117028, "start": 1184.12, "end": 1190.96, "text": " So that you know that loss function has a you can figure it out yourself and particularly simple derivative", "tokens": [407, 300, 291, 458, 300, 4470, 2445, 575, 257, 291, 393, 2573, 309, 484, 1803, 293, 4098, 2199, 13760], "temperature": 0.0, "avg_logprob": -0.2075072113348513, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.9550811783992685e-06}, {"id": 248, "seek": 117028, "start": 1191.48, "end": 1196.44, "text": " And it also you know another thing you could play with at home if you like is like thinking about how?", "tokens": [400, 309, 611, 291, 458, 1071, 551, 291, 727, 862, 365, 412, 1280, 498, 291, 411, 307, 411, 1953, 466, 577, 30], "temperature": 0.0, "avg_logprob": -0.2075072113348513, "compression_ratio": 1.5809128630705394, "no_speech_prob": 5.9550811783992685e-06}, {"id": 249, "seek": 119644, "start": 1196.44, "end": 1201.8400000000001, "text": " The derivative looks when you add a sigmoid or a softmax before it you know it turns out at all", "tokens": [440, 13760, 1542, 562, 291, 909, 257, 4556, 3280, 327, 420, 257, 2787, 41167, 949, 309, 291, 458, 309, 4523, 484, 412, 439], "temperature": 0.0, "avg_logprob": -0.18622429069431348, "compression_ratio": 1.563063063063063, "no_speech_prob": 1.6027995570766507e-06}, {"id": 250, "seek": 119644, "start": 1202.56, "end": 1208.7, "text": " Turns out very nicely because you've got an XP thing going into a loggy thing so you end up with you know very well-behaved", "tokens": [29524, 484, 588, 9594, 570, 291, 600, 658, 364, 33984, 551, 516, 666, 257, 3565, 1480, 551, 370, 291, 917, 493, 365, 291, 458, 588, 731, 12, 29437, 12865], "temperature": 0.0, "avg_logprob": -0.18622429069431348, "compression_ratio": 1.563063063063063, "no_speech_prob": 1.6027995570766507e-06}, {"id": 251, "seek": 119644, "start": 1209.68, "end": 1211.68, "text": " derivatives", "tokens": [33733], "temperature": 0.0, "avg_logprob": -0.18622429069431348, "compression_ratio": 1.563063063063063, "no_speech_prob": 1.6027995570766507e-06}, {"id": 252, "seek": 119644, "start": 1211.92, "end": 1215.8400000000001, "text": " The reason I guess there's lots of reasons that people use", "tokens": [440, 1778, 286, 2041, 456, 311, 3195, 295, 4112, 300, 561, 764], "temperature": 0.0, "avg_logprob": -0.18622429069431348, "compression_ratio": 1.563063063063063, "no_speech_prob": 1.6027995570766507e-06}, {"id": 253, "seek": 119644, "start": 1216.48, "end": 1220.24, "text": " RMSC for regression and cross entropy for classification", "tokens": [497, 10288, 34, 337, 24590, 293, 3278, 30867, 337, 21538], "temperature": 0.0, "avg_logprob": -0.18622429069431348, "compression_ratio": 1.563063063063063, "no_speech_prob": 1.6027995570766507e-06}, {"id": 254, "seek": 122024, "start": 1220.24, "end": 1226.58, "text": " But most of it comes back to this statistical idea of a best linear unbiased estimator", "tokens": [583, 881, 295, 309, 1487, 646, 281, 341, 22820, 1558, 295, 257, 1151, 8213, 517, 5614, 1937, 8017, 1639], "temperature": 0.0, "avg_logprob": -0.14867211643018222, "compression_ratio": 1.6933962264150944, "no_speech_prob": 6.144092822069069e-06}, {"id": 255, "seek": 122024, "start": 1226.58, "end": 1232.44, "text": " You know and based on the likelihood function it kind of turns out that these have some nice statistical properties", "tokens": [509, 458, 293, 2361, 322, 264, 22119, 2445, 309, 733, 295, 4523, 484, 300, 613, 362, 512, 1481, 22820, 7221], "temperature": 0.0, "avg_logprob": -0.14867211643018222, "compression_ratio": 1.6933962264150944, "no_speech_prob": 6.144092822069069e-06}, {"id": 256, "seek": 122024, "start": 1233.32, "end": 1235.32, "text": " It turns out however in practice", "tokens": [467, 4523, 484, 4461, 294, 3124], "temperature": 0.0, "avg_logprob": -0.14867211643018222, "compression_ratio": 1.6933962264150944, "no_speech_prob": 6.144092822069069e-06}, {"id": 257, "seek": 122024, "start": 1236.32, "end": 1238.32, "text": " root means grid error in particular", "tokens": [5593, 1355, 10748, 6713, 294, 1729], "temperature": 0.0, "avg_logprob": -0.14867211643018222, "compression_ratio": 1.6933962264150944, "no_speech_prob": 6.144092822069069e-06}, {"id": 258, "seek": 122024, "start": 1238.76, "end": 1243.92, "text": " the properties are perhaps more theoretical than actual and actually nowadays using the", "tokens": [264, 7221, 366, 4317, 544, 20864, 813, 3539, 293, 767, 13434, 1228, 264], "temperature": 0.0, "avg_logprob": -0.14867211643018222, "compression_ratio": 1.6933962264150944, "no_speech_prob": 6.144092822069069e-06}, {"id": 259, "seek": 124392, "start": 1243.92, "end": 1250.04, "text": " The absolute deviation rather than the sum of squares deviation", "tokens": [440, 8236, 25163, 2831, 813, 264, 2408, 295, 19368, 25163], "temperature": 0.0, "avg_logprob": -0.25742850052682975, "compression_ratio": 1.6507936507936507, "no_speech_prob": 3.3931196412595455e-06}, {"id": 260, "seek": 124392, "start": 1250.8400000000001, "end": 1252.8400000000001, "text": " Can often work better?", "tokens": [1664, 2049, 589, 1101, 30], "temperature": 0.0, "avg_logprob": -0.25742850052682975, "compression_ratio": 1.6507936507936507, "no_speech_prob": 3.3931196412595455e-06}, {"id": 261, "seek": 124392, "start": 1254.48, "end": 1258.8000000000002, "text": " So in practice like everything in machine learning I normally try both for a particular data set", "tokens": [407, 294, 3124, 411, 1203, 294, 3479, 2539, 286, 5646, 853, 1293, 337, 257, 1729, 1412, 992], "temperature": 0.0, "avg_logprob": -0.25742850052682975, "compression_ratio": 1.6507936507936507, "no_speech_prob": 3.3931196412595455e-06}, {"id": 262, "seek": 124392, "start": 1258.8000000000002, "end": 1262.42, "text": " I'll try both loss functions and see which one works better", "tokens": [286, 603, 853, 1293, 4470, 6828, 293, 536, 597, 472, 1985, 1101], "temperature": 0.0, "avg_logprob": -0.25742850052682975, "compression_ratio": 1.6507936507936507, "no_speech_prob": 3.3931196412595455e-06}, {"id": 263, "seek": 124392, "start": 1262.76, "end": 1268.5600000000002, "text": " Unless of course it's a Kaggle competition in which case you're told how Kaggle is going to judge it and you should use the same", "tokens": [16581, 295, 1164, 309, 311, 257, 48751, 22631, 6211, 294, 597, 1389, 291, 434, 1907, 577, 48751, 22631, 307, 516, 281, 6995, 309, 293, 291, 820, 764, 264, 912], "temperature": 0.0, "avg_logprob": -0.25742850052682975, "compression_ratio": 1.6507936507936507, "no_speech_prob": 3.3931196412595455e-06}, {"id": 264, "seek": 124392, "start": 1269.2, "end": 1271.76, "text": " loss function as Kaggle's evaluation metric", "tokens": [4470, 2445, 382, 48751, 22631, 311, 13344, 20678], "temperature": 0.0, "avg_logprob": -0.25742850052682975, "compression_ratio": 1.6507936507936507, "no_speech_prob": 3.3931196412595455e-06}, {"id": 265, "seek": 127176, "start": 1271.76, "end": 1273.76, "text": " all right", "tokens": [439, 558], "temperature": 0.0, "avg_logprob": -0.14789352050194374, "compression_ratio": 1.7845528455284554, "no_speech_prob": 1.3287692581798183e-06}, {"id": 266, "seek": 127176, "start": 1276.08, "end": 1279.22, "text": " So yeah, so this is really the key insight is like hey", "tokens": [407, 1338, 11, 370, 341, 307, 534, 264, 2141, 11269, 307, 411, 4177], "temperature": 0.0, "avg_logprob": -0.14789352050194374, "compression_ratio": 1.7845528455284554, "no_speech_prob": 1.3287692581798183e-06}, {"id": 267, "seek": 127176, "start": 1279.22, "end": 1284.7, "text": " Let's let's not use theory but instead learn things from the data, and you know we hope that we're going to get better results", "tokens": [961, 311, 718, 311, 406, 764, 5261, 457, 2602, 1466, 721, 490, 264, 1412, 11, 293, 291, 458, 321, 1454, 300, 321, 434, 516, 281, 483, 1101, 3542], "temperature": 0.0, "avg_logprob": -0.14789352050194374, "compression_ratio": 1.7845528455284554, "no_speech_prob": 1.3287692581798183e-06}, {"id": 268, "seek": 127176, "start": 1285.48, "end": 1291.14, "text": " Particularly with regularization we do and then I think the key regularization insight here is hey", "tokens": [32281, 365, 3890, 2144, 321, 360, 293, 550, 286, 519, 264, 2141, 3890, 2144, 11269, 510, 307, 4177], "temperature": 0.0, "avg_logprob": -0.14789352050194374, "compression_ratio": 1.7845528455284554, "no_speech_prob": 1.3287692581798183e-06}, {"id": 269, "seek": 127176, "start": 1291.14, "end": 1294.64, "text": " Let's not like try to reduce the number of parameters in our model", "tokens": [961, 311, 406, 411, 853, 281, 5407, 264, 1230, 295, 9834, 294, 527, 2316], "temperature": 0.0, "avg_logprob": -0.14789352050194374, "compression_ratio": 1.7845528455284554, "no_speech_prob": 1.3287692581798183e-06}, {"id": 270, "seek": 127176, "start": 1294.64, "end": 1298.64, "text": " But instead like use lots of parameters and then use regularization to figure out", "tokens": [583, 2602, 411, 764, 3195, 295, 9834, 293, 550, 764, 3890, 2144, 281, 2573, 484], "temperature": 0.0, "avg_logprob": -0.14789352050194374, "compression_ratio": 1.7845528455284554, "no_speech_prob": 1.3287692581798183e-06}, {"id": 271, "seek": 129864, "start": 1298.64, "end": 1305.8000000000002, "text": " Which ones are actually useful right and so then we took that a step further by saying hey given we can do that with", "tokens": [3013, 2306, 366, 767, 4420, 558, 293, 370, 550, 321, 1890, 300, 257, 1823, 3052, 538, 1566, 4177, 2212, 321, 393, 360, 300, 365], "temperature": 0.0, "avg_logprob": -0.2331898949363015, "compression_ratio": 1.7015503875968991, "no_speech_prob": 2.7693924948835047e-06}, {"id": 272, "seek": 129864, "start": 1306.0800000000002, "end": 1308.4, "text": " Regularization let's create lots more features", "tokens": [45659, 2144, 718, 311, 1884, 3195, 544, 4122], "temperature": 0.0, "avg_logprob": -0.2331898949363015, "compression_ratio": 1.7015503875968991, "no_speech_prob": 2.7693924948835047e-06}, {"id": 273, "seek": 129864, "start": 1308.88, "end": 1311.14, "text": " By adding bigrams and trigrams", "tokens": [3146, 5127, 955, 2356, 82, 293, 504, 33737, 82], "temperature": 0.0, "avg_logprob": -0.2331898949363015, "compression_ratio": 1.7015503875968991, "no_speech_prob": 2.7693924948835047e-06}, {"id": 274, "seek": 129864, "start": 1311.6000000000001, "end": 1318.48, "text": " You know bigrams like by vast and by vengeance and trigrams like by Vengeance full stop and by Vera miles", "tokens": [509, 458, 955, 2356, 82, 411, 538, 8369, 293, 538, 43818, 293, 504, 33737, 82, 411, 538, 691, 3032, 719, 1577, 1590, 293, 538, 46982, 6193], "temperature": 0.0, "avg_logprob": -0.2331898949363015, "compression_ratio": 1.7015503875968991, "no_speech_prob": 2.7693924948835047e-06}, {"id": 275, "seek": 129864, "start": 1320.2, "end": 1323.2, "text": " And you know just to keep things a little faster", "tokens": [400, 291, 458, 445, 281, 1066, 721, 257, 707, 4663], "temperature": 0.0, "avg_logprob": -0.2331898949363015, "compression_ratio": 1.7015503875968991, "no_speech_prob": 2.7693924948835047e-06}, {"id": 276, "seek": 132320, "start": 1323.2, "end": 1329.04, "text": " We limited it to 800,000 features, but you know even with the full 70 million features it works", "tokens": [492, 5567, 309, 281, 13083, 11, 1360, 4122, 11, 457, 291, 458, 754, 365, 264, 1577, 5285, 2459, 4122, 309, 1985], "temperature": 0.0, "avg_logprob": -0.1783015260991362, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0147045840276405e-06}, {"id": 277, "seek": 132320, "start": 1329.04, "end": 1331.04, "text": " Just as well, and it's not a hell of a lot slower", "tokens": [1449, 382, 731, 11, 293, 309, 311, 406, 257, 4921, 295, 257, 688, 14009], "temperature": 0.0, "avg_logprob": -0.1783015260991362, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0147045840276405e-06}, {"id": 278, "seek": 132320, "start": 1331.64, "end": 1333.88, "text": " So we created a term document matrix", "tokens": [407, 321, 2942, 257, 1433, 4166, 8141], "temperature": 0.0, "avg_logprob": -0.1783015260991362, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0147045840276405e-06}, {"id": 279, "seek": 132320, "start": 1334.8, "end": 1337.82, "text": " again using the full set of n grams", "tokens": [797, 1228, 264, 1577, 992, 295, 297, 11899], "temperature": 0.0, "avg_logprob": -0.1783015260991362, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0147045840276405e-06}, {"id": 280, "seek": 132320, "start": 1338.8400000000001, "end": 1340.8400000000001, "text": " for the training set the validation set", "tokens": [337, 264, 3097, 992, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.1783015260991362, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0147045840276405e-06}, {"id": 281, "seek": 132320, "start": 1341.68, "end": 1347.48, "text": " And so now we can go ahead and say okay our labels is the training set labels as before", "tokens": [400, 370, 586, 321, 393, 352, 2286, 293, 584, 1392, 527, 16949, 307, 264, 3097, 992, 16949, 382, 949], "temperature": 0.0, "avg_logprob": -0.1783015260991362, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0147045840276405e-06}, {"id": 282, "seek": 132320, "start": 1348.04, "end": 1350.52, "text": " our independent variables is the", "tokens": [527, 6695, 9102, 307, 264], "temperature": 0.0, "avg_logprob": -0.1783015260991362, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.0147045840276405e-06}, {"id": 283, "seek": 135052, "start": 1350.52, "end": 1352.0, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.3595575366103858, "compression_ratio": 1.404109589041096, "no_speech_prob": 1.7061722701328108e-06}, {"id": 284, "seek": 135052, "start": 1352.0, "end": 1354.92, "text": " Binarized term document matrix as before", "tokens": [363, 6470, 1602, 1433, 4166, 8141, 382, 949], "temperature": 0.0, "avg_logprob": -0.3595575366103858, "compression_ratio": 1.404109589041096, "no_speech_prob": 1.7061722701328108e-06}, {"id": 285, "seek": 135052, "start": 1356.0, "end": 1359.68, "text": " And then let's fit a logistic regression to that", "tokens": [400, 550, 718, 311, 3318, 257, 3565, 3142, 24590, 281, 300], "temperature": 0.0, "avg_logprob": -0.3595575366103858, "compression_ratio": 1.404109589041096, "no_speech_prob": 1.7061722701328108e-06}, {"id": 286, "seek": 135052, "start": 1360.84, "end": 1367.96, "text": " And do some predictions and we get 90% accuracy, so this is looking pretty good", "tokens": [400, 360, 512, 21264, 293, 321, 483, 4289, 4, 14170, 11, 370, 341, 307, 1237, 1238, 665], "temperature": 0.0, "avg_logprob": -0.3595575366103858, "compression_ratio": 1.404109589041096, "no_speech_prob": 1.7061722701328108e-06}, {"id": 287, "seek": 135052, "start": 1369.04, "end": 1371.04, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.3595575366103858, "compression_ratio": 1.404109589041096, "no_speech_prob": 1.7061722701328108e-06}, {"id": 288, "seek": 135052, "start": 1373.04, "end": 1376.24, "text": " So the logistic regression", "tokens": [407, 264, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.3595575366103858, "compression_ratio": 1.404109589041096, "no_speech_prob": 1.7061722701328108e-06}, {"id": 289, "seek": 137624, "start": 1376.24, "end": 1380.76, "text": " Let's go back to our naive base right in a naive base", "tokens": [961, 311, 352, 646, 281, 527, 29052, 3096, 558, 294, 257, 29052, 3096], "temperature": 0.0, "avg_logprob": -0.2600692423378549, "compression_ratio": 1.8852459016393444, "no_speech_prob": 5.014688213123009e-06}, {"id": 290, "seek": 137624, "start": 1381.56, "end": 1387.0, "text": " We have this term document matrix and then for every feature. We're calculating", "tokens": [492, 362, 341, 1433, 4166, 8141, 293, 550, 337, 633, 4111, 13, 492, 434, 28258], "temperature": 0.0, "avg_logprob": -0.2600692423378549, "compression_ratio": 1.8852459016393444, "no_speech_prob": 5.014688213123009e-06}, {"id": 291, "seek": 137624, "start": 1387.64, "end": 1395.74, "text": " the probability of that feature occurring if it's class one that probability of that feature occurring if it's class two and then the", "tokens": [264, 8482, 295, 300, 4111, 18386, 498, 309, 311, 1508, 472, 300, 8482, 295, 300, 4111, 18386, 498, 309, 311, 1508, 732, 293, 550, 264], "temperature": 0.0, "avg_logprob": -0.2600692423378549, "compression_ratio": 1.8852459016393444, "no_speech_prob": 5.014688213123009e-06}, {"id": 292, "seek": 137624, "start": 1396.36, "end": 1399.24, "text": " ratio of those two right and", "tokens": [8509, 295, 729, 732, 558, 293], "temperature": 0.0, "avg_logprob": -0.2600692423378549, "compression_ratio": 1.8852459016393444, "no_speech_prob": 5.014688213123009e-06}, {"id": 293, "seek": 137624, "start": 1399.88, "end": 1402.2, "text": " In the paper that we're actually basing this off", "tokens": [682, 264, 3035, 300, 321, 434, 767, 987, 278, 341, 766], "temperature": 0.0, "avg_logprob": -0.2600692423378549, "compression_ratio": 1.8852459016393444, "no_speech_prob": 5.014688213123009e-06}, {"id": 294, "seek": 140220, "start": 1402.2, "end": 1409.0800000000002, "text": " They call this P this Q and this R right maybe I should just fill that in P", "tokens": [814, 818, 341, 430, 341, 1249, 293, 341, 497, 558, 1310, 286, 820, 445, 2836, 300, 294, 430], "temperature": 0.0, "avg_logprob": -0.2351488297985446, "compression_ratio": 1.475, "no_speech_prob": 4.2228170968883205e-06}, {"id": 295, "seek": 140220, "start": 1412.24, "end": 1415.6000000000001, "text": " Here maybe then we'll say probability to make it more obvious", "tokens": [1692, 1310, 550, 321, 603, 584, 8482, 281, 652, 309, 544, 6322], "temperature": 0.0, "avg_logprob": -0.2351488297985446, "compression_ratio": 1.475, "no_speech_prob": 4.2228170968883205e-06}, {"id": 296, "seek": 140220, "start": 1421.72, "end": 1423.72, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2351488297985446, "compression_ratio": 1.475, "no_speech_prob": 4.2228170968883205e-06}, {"id": 297, "seek": 142372, "start": 1423.72, "end": 1432.44, "text": " And so then we kind of said hey, let's let's not use these ratios as the coefficients in that in that", "tokens": [400, 370, 550, 321, 733, 295, 848, 4177, 11, 718, 311, 718, 311, 406, 764, 613, 32435, 382, 264, 31994, 294, 300, 294, 300], "temperature": 0.0, "avg_logprob": -0.12386029387173587, "compression_ratio": 1.6185567010309279, "no_speech_prob": 4.785078999702819e-06}, {"id": 298, "seek": 142372, "start": 1432.88, "end": 1440.56, "text": " Matrix multiply, but let's instead like try and learn some coefficients. You know so maybe start out with some random numbers", "tokens": [36274, 12972, 11, 457, 718, 311, 2602, 411, 853, 293, 1466, 512, 31994, 13, 509, 458, 370, 1310, 722, 484, 365, 512, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.12386029387173587, "compression_ratio": 1.6185567010309279, "no_speech_prob": 4.785078999702819e-06}, {"id": 299, "seek": 142372, "start": 1444.96, "end": 1449.92, "text": " You know and then try and use stochastic gradient descent to find slightly better ones", "tokens": [509, 458, 293, 550, 853, 293, 764, 342, 8997, 2750, 16235, 23475, 281, 915, 4748, 1101, 2306], "temperature": 0.0, "avg_logprob": -0.12386029387173587, "compression_ratio": 1.6185567010309279, "no_speech_prob": 4.785078999702819e-06}, {"id": 300, "seek": 144992, "start": 1449.92, "end": 1455.48, "text": " So you'll notice you know some important features here the R", "tokens": [407, 291, 603, 3449, 291, 458, 512, 1021, 4122, 510, 264, 497], "temperature": 0.0, "avg_logprob": -0.24323682675416441, "compression_ratio": 1.7871287128712872, "no_speech_prob": 1.4144700344331795e-06}, {"id": 301, "seek": 144992, "start": 1458.0, "end": 1461.4, "text": " Vector is a vector of rank one and", "tokens": [691, 20814, 307, 257, 8062, 295, 6181, 472, 293], "temperature": 0.0, "avg_logprob": -0.24323682675416441, "compression_ratio": 1.7871287128712872, "no_speech_prob": 1.4144700344331795e-06}, {"id": 302, "seek": 144992, "start": 1462.16, "end": 1464.64, "text": " its length is equal to the number of features and", "tokens": [1080, 4641, 307, 2681, 281, 264, 1230, 295, 4122, 293], "temperature": 0.0, "avg_logprob": -0.24323682675416441, "compression_ratio": 1.7871287128712872, "no_speech_prob": 1.4144700344331795e-06}, {"id": 303, "seek": 144992, "start": 1465.8000000000002, "end": 1470.3200000000002, "text": " Of course our logistic regression coefficient matrix is also", "tokens": [2720, 1164, 527, 3565, 3142, 24590, 17619, 8141, 307, 611], "temperature": 0.0, "avg_logprob": -0.24323682675416441, "compression_ratio": 1.7871287128712872, "no_speech_prob": 1.4144700344331795e-06}, {"id": 304, "seek": 144992, "start": 1470.96, "end": 1472.52, "text": " of length one", "tokens": [295, 4641, 472], "temperature": 0.0, "avg_logprob": -0.24323682675416441, "compression_ratio": 1.7871287128712872, "no_speech_prob": 1.4144700344331795e-06}, {"id": 305, "seek": 144992, "start": 1472.52, "end": 1478.8400000000001, "text": " Sorry rank one and length equal to the number of features right and we're you know we're saying like they're kind of two ways of calculating", "tokens": [4919, 6181, 472, 293, 4641, 2681, 281, 264, 1230, 295, 4122, 558, 293, 321, 434, 291, 458, 321, 434, 1566, 411, 436, 434, 733, 295, 732, 2098, 295, 28258], "temperature": 0.0, "avg_logprob": -0.24323682675416441, "compression_ratio": 1.7871287128712872, "no_speech_prob": 1.4144700344331795e-06}, {"id": 306, "seek": 147884, "start": 1478.84, "end": 1483.6399999999999, "text": " the same kind of thing right one based on theory one based on", "tokens": [264, 912, 733, 295, 551, 558, 472, 2361, 322, 5261, 472, 2361, 322], "temperature": 0.0, "avg_logprob": -0.1709188060995973, "compression_ratio": 1.819672131147541, "no_speech_prob": 1.6797273474367103e-06}, {"id": 307, "seek": 147884, "start": 1484.24, "end": 1486.24, "text": " data", "tokens": [1412], "temperature": 0.0, "avg_logprob": -0.1709188060995973, "compression_ratio": 1.819672131147541, "no_speech_prob": 1.6797273474367103e-06}, {"id": 308, "seek": 147884, "start": 1486.3999999999999, "end": 1494.04, "text": " So here is like some of the numbers in R right remember. It's using the log so these numbers which are less than zero", "tokens": [407, 510, 307, 411, 512, 295, 264, 3547, 294, 497, 558, 1604, 13, 467, 311, 1228, 264, 3565, 370, 613, 3547, 597, 366, 1570, 813, 4018], "temperature": 0.0, "avg_logprob": -0.1709188060995973, "compression_ratio": 1.819672131147541, "no_speech_prob": 1.6797273474367103e-06}, {"id": 309, "seek": 147884, "start": 1495.12, "end": 1497.12, "text": " represent things which are", "tokens": [2906, 721, 597, 366], "temperature": 0.0, "avg_logprob": -0.1709188060995973, "compression_ratio": 1.819672131147541, "no_speech_prob": 1.6797273474367103e-06}, {"id": 310, "seek": 147884, "start": 1497.8, "end": 1504.72, "text": " More likely to be negative and these ones that here are more likely so this one here is more likely to be positive and so", "tokens": [5048, 3700, 281, 312, 3671, 293, 613, 2306, 300, 510, 366, 544, 3700, 370, 341, 472, 510, 307, 544, 3700, 281, 312, 3353, 293, 370], "temperature": 0.0, "avg_logprob": -0.1709188060995973, "compression_ratio": 1.819672131147541, "no_speech_prob": 1.6797273474367103e-06}, {"id": 311, "seek": 150472, "start": 1504.72, "end": 1510.72, "text": " Here's either the power of that and so these are the ones we can compare to one rather than to zero", "tokens": [1692, 311, 2139, 264, 1347, 295, 300, 293, 370, 613, 366, 264, 2306, 321, 393, 6794, 281, 472, 2831, 813, 281, 4018], "temperature": 0.0, "avg_logprob": -0.14724665555087002, "compression_ratio": 1.9116279069767441, "no_speech_prob": 1.154458345808962e-06}, {"id": 312, "seek": 150472, "start": 1513.32, "end": 1518.6000000000001, "text": " So I'm going to do something that hopefully is going to seem weird", "tokens": [407, 286, 478, 516, 281, 360, 746, 300, 4696, 307, 516, 281, 1643, 3657], "temperature": 0.0, "avg_logprob": -0.14724665555087002, "compression_ratio": 1.9116279069767441, "no_speech_prob": 1.154458345808962e-06}, {"id": 313, "seek": 150472, "start": 1520.56, "end": 1525.1200000000001, "text": " And so first of all I'm going to talk about I'm going to say what we're going to do and", "tokens": [400, 370, 700, 295, 439, 286, 478, 516, 281, 751, 466, 286, 478, 516, 281, 584, 437, 321, 434, 516, 281, 360, 293], "temperature": 0.0, "avg_logprob": -0.14724665555087002, "compression_ratio": 1.9116279069767441, "no_speech_prob": 1.154458345808962e-06}, {"id": 314, "seek": 150472, "start": 1525.72, "end": 1529.84, "text": " Then I'm going to try and describe why it's weird, and then we'll talk about", "tokens": [1396, 286, 478, 516, 281, 853, 293, 6786, 983, 309, 311, 3657, 11, 293, 550, 321, 603, 751, 466], "temperature": 0.0, "avg_logprob": -0.14724665555087002, "compression_ratio": 1.9116279069767441, "no_speech_prob": 1.154458345808962e-06}, {"id": 315, "seek": 150472, "start": 1530.76, "end": 1534.32, "text": " Why it may not be as weird as we first thought so here's what we're going to do", "tokens": [1545, 309, 815, 406, 312, 382, 3657, 382, 321, 700, 1194, 370, 510, 311, 437, 321, 434, 516, 281, 360], "temperature": 0.0, "avg_logprob": -0.14724665555087002, "compression_ratio": 1.9116279069767441, "no_speech_prob": 1.154458345808962e-06}, {"id": 316, "seek": 153432, "start": 1534.32, "end": 1537.4399999999998, "text": " We're going to take our term document matrix", "tokens": [492, 434, 516, 281, 747, 527, 1433, 4166, 8141], "temperature": 0.0, "avg_logprob": -0.20694921102868505, "compression_ratio": 1.7630057803468209, "no_speech_prob": 4.710885605163639e-06}, {"id": 317, "seek": 153432, "start": 1538.1599999999999, "end": 1540.1599999999999, "text": " And we're going to multiply it", "tokens": [400, 321, 434, 516, 281, 12972, 309], "temperature": 0.0, "avg_logprob": -0.20694921102868505, "compression_ratio": 1.7630057803468209, "no_speech_prob": 4.710885605163639e-06}, {"id": 318, "seek": 153432, "start": 1541.12, "end": 1543.12, "text": " by R", "tokens": [538, 497], "temperature": 0.0, "avg_logprob": -0.20694921102868505, "compression_ratio": 1.7630057803468209, "no_speech_prob": 4.710885605163639e-06}, {"id": 319, "seek": 153432, "start": 1543.56, "end": 1548.48, "text": " So what that means is we're going to we can do it here in Excel right so we're going to say", "tokens": [407, 437, 300, 1355, 307, 321, 434, 516, 281, 321, 393, 360, 309, 510, 294, 19060, 558, 370, 321, 434, 516, 281, 584], "temperature": 0.0, "avg_logprob": -0.20694921102868505, "compression_ratio": 1.7630057803468209, "no_speech_prob": 4.710885605163639e-06}, {"id": 320, "seek": 153432, "start": 1550.04, "end": 1552.9199999999998, "text": " let's grab everything in our term document matrix and", "tokens": [718, 311, 4444, 1203, 294, 527, 1433, 4166, 8141, 293], "temperature": 0.0, "avg_logprob": -0.20694921102868505, "compression_ratio": 1.7630057803468209, "no_speech_prob": 4.710885605163639e-06}, {"id": 321, "seek": 153432, "start": 1554.2, "end": 1558.8799999999999, "text": " Multiply it by the equivalent value in the vector of R right so this is like a", "tokens": [31150, 356, 309, 538, 264, 10344, 2158, 294, 264, 8062, 295, 497, 558, 370, 341, 307, 411, 257], "temperature": 0.0, "avg_logprob": -0.20694921102868505, "compression_ratio": 1.7630057803468209, "no_speech_prob": 4.710885605163639e-06}, {"id": 322, "seek": 155888, "start": 1558.88, "end": 1564.2, "text": " Broadcasted element wise multiplication not a matrix multiplication", "tokens": [14074, 3734, 292, 4478, 10829, 27290, 406, 257, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.2859154082181161, "compression_ratio": 1.5906040268456376, "no_speech_prob": 1.1726392585842405e-06}, {"id": 323, "seek": 155888, "start": 1566.2800000000002, "end": 1568.2800000000002, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2859154082181161, "compression_ratio": 1.5906040268456376, "no_speech_prob": 1.1726392585842405e-06}, {"id": 324, "seek": 155888, "start": 1571.2, "end": 1573.2, "text": " And that's what that does", "tokens": [400, 300, 311, 437, 300, 775], "temperature": 0.0, "avg_logprob": -0.2859154082181161, "compression_ratio": 1.5906040268456376, "no_speech_prob": 1.1726392585842405e-06}, {"id": 325, "seek": 155888, "start": 1574.5200000000002, "end": 1579.22, "text": " Okay, so here is the value of the term document matrix", "tokens": [1033, 11, 370, 510, 307, 264, 2158, 295, 264, 1433, 4166, 8141], "temperature": 0.0, "avg_logprob": -0.2859154082181161, "compression_ratio": 1.5906040268456376, "no_speech_prob": 1.1726392585842405e-06}, {"id": 326, "seek": 155888, "start": 1580.0400000000002, "end": 1585.7600000000002, "text": " Times R in other words everywhere that a zero appears there a zero appears here and", "tokens": [11366, 497, 294, 661, 2283, 5315, 300, 257, 4018, 7038, 456, 257, 4018, 7038, 510, 293], "temperature": 0.0, "avg_logprob": -0.2859154082181161, "compression_ratio": 1.5906040268456376, "no_speech_prob": 1.1726392585842405e-06}, {"id": 327, "seek": 158576, "start": 1585.76, "end": 1590.84, "text": " Every time a one appears here the equivalent value of R appears here", "tokens": [2048, 565, 257, 472, 7038, 510, 264, 10344, 2158, 295, 497, 7038, 510], "temperature": 0.0, "avg_logprob": -0.18230211469862195, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.122884658703697e-07}, {"id": 328, "seek": 158576, "start": 1591.56, "end": 1593.56, "text": " So we haven't really", "tokens": [407, 321, 2378, 380, 534], "temperature": 0.0, "avg_logprob": -0.18230211469862195, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.122884658703697e-07}, {"id": 329, "seek": 158576, "start": 1595.12, "end": 1599.44, "text": " We haven't really changed much right we've just we've just kind of", "tokens": [492, 2378, 380, 534, 3105, 709, 558, 321, 600, 445, 321, 600, 445, 733, 295], "temperature": 0.0, "avg_logprob": -0.18230211469862195, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.122884658703697e-07}, {"id": 330, "seek": 158576, "start": 1600.44, "end": 1604.8, "text": " Changed the ones into something else into the into the Rs from that feature", "tokens": [761, 10296, 264, 2306, 666, 746, 1646, 666, 264, 666, 264, 21643, 490, 300, 4111], "temperature": 0.0, "avg_logprob": -0.18230211469862195, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.122884658703697e-07}, {"id": 331, "seek": 158576, "start": 1605.04, "end": 1609.04, "text": " Right and so what we're now going to do is we're going to use this as", "tokens": [1779, 293, 370, 437, 321, 434, 586, 516, 281, 360, 307, 321, 434, 516, 281, 764, 341, 382], "temperature": 0.0, "avg_logprob": -0.18230211469862195, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.122884658703697e-07}, {"id": 332, "seek": 158576, "start": 1610.0, "end": 1612.0, "text": " our independent variables", "tokens": [527, 6695, 9102], "temperature": 0.0, "avg_logprob": -0.18230211469862195, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.122884658703697e-07}, {"id": 333, "seek": 161200, "start": 1612.0, "end": 1615.68, "text": " Instead in our logistic regression okay?", "tokens": [7156, 294, 527, 3565, 3142, 24590, 1392, 30], "temperature": 0.0, "avg_logprob": -0.34787447793143134, "compression_ratio": 1.5491329479768785, "no_speech_prob": 2.123371814377606e-06}, {"id": 334, "seek": 161200, "start": 1616.12, "end": 1621.0, "text": " So here we are multiplied X X NB X naive Bayes version is", "tokens": [407, 510, 321, 366, 17207, 1783, 1783, 426, 33, 1783, 29052, 7840, 279, 3037, 307], "temperature": 0.0, "avg_logprob": -0.34787447793143134, "compression_ratio": 1.5491329479768785, "no_speech_prob": 2.123371814377606e-06}, {"id": 335, "seek": 161200, "start": 1621.36, "end": 1624.94, "text": " X times R and now let's do a logistic regression", "tokens": [1783, 1413, 497, 293, 586, 718, 311, 360, 257, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.34787447793143134, "compression_ratio": 1.5491329479768785, "no_speech_prob": 2.123371814377606e-06}, {"id": 336, "seek": 161200, "start": 1626.24, "end": 1629.44, "text": " fitting using those independent variables and", "tokens": [15669, 1228, 729, 6695, 9102, 293], "temperature": 0.0, "avg_logprob": -0.34787447793143134, "compression_ratio": 1.5491329479768785, "no_speech_prob": 2.123371814377606e-06}, {"id": 337, "seek": 161200, "start": 1631.32, "end": 1633.32, "text": " Let's then", "tokens": [961, 311, 550], "temperature": 0.0, "avg_logprob": -0.34787447793143134, "compression_ratio": 1.5491329479768785, "no_speech_prob": 2.123371814377606e-06}, {"id": 338, "seek": 161200, "start": 1633.52, "end": 1637.6, "text": " do that for the validation set okay and get the predictions and", "tokens": [360, 300, 337, 264, 24071, 992, 1392, 293, 483, 264, 21264, 293], "temperature": 0.0, "avg_logprob": -0.34787447793143134, "compression_ratio": 1.5491329479768785, "no_speech_prob": 2.123371814377606e-06}, {"id": 339, "seek": 163760, "start": 1637.6, "end": 1641.1599999999999, "text": " And lo and behold we have a better number", "tokens": [400, 450, 293, 27234, 321, 362, 257, 1101, 1230], "temperature": 0.0, "avg_logprob": -0.34661964133933737, "compression_ratio": 1.3071895424836601, "no_speech_prob": 4.495152552408399e-06}, {"id": 340, "seek": 163760, "start": 1642.9199999999998, "end": 1644.9199999999998, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.34661964133933737, "compression_ratio": 1.3071895424836601, "no_speech_prob": 4.495152552408399e-06}, {"id": 341, "seek": 163760, "start": 1647.8, "end": 1652.1599999999999, "text": " Let me explain why this hopefully seems surprising", "tokens": [961, 385, 2903, 983, 341, 4696, 2544, 8830], "temperature": 0.0, "avg_logprob": -0.34661964133933737, "compression_ratio": 1.3071895424836601, "no_speech_prob": 4.495152552408399e-06}, {"id": 342, "seek": 163760, "start": 1654.8, "end": 1657.76, "text": " Given that we're just multiplying", "tokens": [18600, 300, 321, 434, 445, 30955], "temperature": 0.0, "avg_logprob": -0.34661964133933737, "compression_ratio": 1.3071895424836601, "no_speech_prob": 4.495152552408399e-06}, {"id": 343, "seek": 165776, "start": 1657.76, "end": 1666.2, "text": " Multiplying oh I picked out the wrong ones. I should have said ah not coif", "tokens": [31150, 7310, 1954, 286, 6183, 484, 264, 2085, 2306, 13, 286, 820, 362, 848, 3716, 406, 598, 351], "temperature": 0.0, "avg_logprob": -0.18413654565811158, "compression_ratio": 1.6320754716981132, "no_speech_prob": 2.29590796152479e-06}, {"id": 344, "seek": 165776, "start": 1669.76, "end": 1673.6, "text": " Okay, that's actually ah I got the wrong number okay", "tokens": [1033, 11, 300, 311, 767, 3716, 286, 658, 264, 2085, 1230, 1392], "temperature": 0.0, "avg_logprob": -0.18413654565811158, "compression_ratio": 1.6320754716981132, "no_speech_prob": 2.29590796152479e-06}, {"id": 345, "seek": 165776, "start": 1674.8, "end": 1682.48, "text": " So that's our independent variables right and then the the logistic regression has come up with some set of coefficients", "tokens": [407, 300, 311, 527, 6695, 9102, 558, 293, 550, 264, 264, 3565, 3142, 24590, 575, 808, 493, 365, 512, 992, 295, 31994], "temperature": 0.0, "avg_logprob": -0.18413654565811158, "compression_ratio": 1.6320754716981132, "no_speech_prob": 2.29590796152479e-06}, {"id": 346, "seek": 168248, "start": 1682.48, "end": 1688.04, "text": " Let's pretend for a moment that these are the coefficients that it happened to come up with right?", "tokens": [961, 311, 11865, 337, 257, 1623, 300, 613, 366, 264, 31994, 300, 309, 2011, 281, 808, 493, 365, 558, 30], "temperature": 0.0, "avg_logprob": -0.1423507069432458, "compression_ratio": 1.5906976744186045, "no_speech_prob": 1.4367473113452434e-06}, {"id": 347, "seek": 168248, "start": 1690.56, "end": 1696.44, "text": " We could now say well, let's not use this set. Let's not use this", "tokens": [492, 727, 586, 584, 731, 11, 718, 311, 406, 764, 341, 992, 13, 961, 311, 406, 764, 341], "temperature": 0.0, "avg_logprob": -0.1423507069432458, "compression_ratio": 1.5906976744186045, "no_speech_prob": 1.4367473113452434e-06}, {"id": 348, "seek": 168248, "start": 1697.76, "end": 1705.88, "text": " Set of independent variables, but let's use the original binarized feature matrix right and then divide all of our coefficients", "tokens": [8928, 295, 6695, 9102, 11, 457, 718, 311, 764, 264, 3380, 5171, 289, 1602, 4111, 8141, 558, 293, 550, 9845, 439, 295, 527, 31994], "temperature": 0.0, "avg_logprob": -0.1423507069432458, "compression_ratio": 1.5906976744186045, "no_speech_prob": 1.4367473113452434e-06}, {"id": 349, "seek": 168248, "start": 1706.92, "end": 1708.92, "text": " by the values in R and", "tokens": [538, 264, 4190, 294, 497, 293], "temperature": 0.0, "avg_logprob": -0.1423507069432458, "compression_ratio": 1.5906976744186045, "no_speech_prob": 1.4367473113452434e-06}, {"id": 350, "seek": 168248, "start": 1709.48, "end": 1711.48, "text": " We're going to get exactly", "tokens": [492, 434, 516, 281, 483, 2293], "temperature": 0.0, "avg_logprob": -0.1423507069432458, "compression_ratio": 1.5906976744186045, "no_speech_prob": 1.4367473113452434e-06}, {"id": 351, "seek": 171148, "start": 1711.48, "end": 1713.04, "text": " the same", "tokens": [264, 912], "temperature": 0.0, "avg_logprob": -0.3111868661547464, "compression_ratio": 1.3974358974358974, "no_speech_prob": 3.5008351915166713e-06}, {"id": 352, "seek": 171148, "start": 1713.04, "end": 1715.04, "text": " result mathematically so", "tokens": [1874, 44003, 370], "temperature": 0.0, "avg_logprob": -0.3111868661547464, "compression_ratio": 1.3974358974358974, "no_speech_prob": 3.5008351915166713e-06}, {"id": 353, "seek": 171148, "start": 1716.64, "end": 1719.24, "text": " You know we've got our", "tokens": [509, 458, 321, 600, 658, 527], "temperature": 0.0, "avg_logprob": -0.3111868661547464, "compression_ratio": 1.3974358974358974, "no_speech_prob": 3.5008351915166713e-06}, {"id": 354, "seek": 171148, "start": 1721.88, "end": 1726.2, "text": " X naive Bayes version of the independent variables, and we've got some", "tokens": [1783, 29052, 7840, 279, 3037, 295, 264, 6695, 9102, 11, 293, 321, 600, 658, 512], "temperature": 0.0, "avg_logprob": -0.3111868661547464, "compression_ratio": 1.3974358974358974, "no_speech_prob": 3.5008351915166713e-06}, {"id": 355, "seek": 171148, "start": 1728.2, "end": 1732.04, "text": " Some set of weights some support some sort of coefficients. I call it w", "tokens": [2188, 992, 295, 17443, 512, 1406, 512, 1333, 295, 31994, 13, 286, 818, 309, 261], "temperature": 0.0, "avg_logprob": -0.3111868661547464, "compression_ratio": 1.3974358974358974, "no_speech_prob": 3.5008351915166713e-06}, {"id": 356, "seek": 171148, "start": 1733.32, "end": 1735.24, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.3111868661547464, "compression_ratio": 1.3974358974358974, "no_speech_prob": 3.5008351915166713e-06}, {"id": 357, "seek": 171148, "start": 1735.24, "end": 1737.24, "text": " W1 let's say", "tokens": [343, 16, 718, 311, 584], "temperature": 0.0, "avg_logprob": -0.3111868661547464, "compression_ratio": 1.3974358974358974, "no_speech_prob": 3.5008351915166713e-06}, {"id": 358, "seek": 173724, "start": 1737.24, "end": 1744.8, "text": " Where it's found like this is a good set of coefficients making our predictions from right that X and B", "tokens": [2305, 309, 311, 1352, 411, 341, 307, 257, 665, 992, 295, 31994, 1455, 527, 21264, 490, 558, 300, 1783, 293, 363], "temperature": 0.0, "avg_logprob": -0.34689007108173675, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.857302206393797e-06}, {"id": 359, "seek": 173724, "start": 1745.76, "end": 1747.76, "text": " is simply equal to", "tokens": [307, 2935, 2681, 281], "temperature": 0.0, "avg_logprob": -0.34689007108173675, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.857302206393797e-06}, {"id": 360, "seek": 173724, "start": 1748.2, "end": 1749.76, "text": " X", "tokens": [1783], "temperature": 0.0, "avg_logprob": -0.34689007108173675, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.857302206393797e-06}, {"id": 361, "seek": 173724, "start": 1749.76, "end": 1752.5, "text": " times as in element wise times", "tokens": [1413, 382, 294, 4478, 10829, 1413], "temperature": 0.0, "avg_logprob": -0.34689007108173675, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.857302206393797e-06}, {"id": 362, "seek": 173724, "start": 1753.64, "end": 1755.1200000000001, "text": " ah", "tokens": [3716], "temperature": 0.0, "avg_logprob": -0.34689007108173675, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.857302206393797e-06}, {"id": 363, "seek": 173724, "start": 1755.1200000000001, "end": 1757.72, "text": " right so in other words this is equal to", "tokens": [558, 370, 294, 661, 2283, 341, 307, 2681, 281], "temperature": 0.0, "avg_logprob": -0.34689007108173675, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.857302206393797e-06}, {"id": 364, "seek": 173724, "start": 1759.92, "end": 1761.6, "text": " X", "tokens": [1783], "temperature": 0.0, "avg_logprob": -0.34689007108173675, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.857302206393797e-06}, {"id": 365, "seek": 173724, "start": 1761.6, "end": 1763.6, "text": " times ah", "tokens": [1413, 3716], "temperature": 0.0, "avg_logprob": -0.34689007108173675, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.857302206393797e-06}, {"id": 366, "seek": 176360, "start": 1763.6, "end": 1770.24, "text": " times the weights and so like we could just change the weights to be that", "tokens": [1413, 264, 17443, 293, 370, 411, 321, 727, 445, 1319, 264, 17443, 281, 312, 300], "temperature": 0.0, "avg_logprob": -0.18868024190266927, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.927854095309158e-07}, {"id": 367, "seek": 176360, "start": 1771.9199999999998, "end": 1773.9199999999998, "text": " Right and get the same number", "tokens": [1779, 293, 483, 264, 912, 1230], "temperature": 0.0, "avg_logprob": -0.18868024190266927, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.927854095309158e-07}, {"id": 368, "seek": 176360, "start": 1774.6399999999999, "end": 1775.7199999999998, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.18868024190266927, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.927854095309158e-07}, {"id": 369, "seek": 176360, "start": 1775.7199999999998, "end": 1777.84, "text": " This ought to mean that", "tokens": [639, 13416, 281, 914, 300], "temperature": 0.0, "avg_logprob": -0.18868024190266927, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.927854095309158e-07}, {"id": 370, "seek": 176360, "start": 1779.1999999999998, "end": 1783.1599999999999, "text": " The change that we made to the dependent variable shouldn't have made any difference", "tokens": [440, 1319, 300, 321, 1027, 281, 264, 12334, 7006, 4659, 380, 362, 1027, 604, 2649], "temperature": 0.0, "avg_logprob": -0.18868024190266927, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.927854095309158e-07}, {"id": 371, "seek": 176360, "start": 1785.08, "end": 1788.36, "text": " Because we can calculate exactly the same thing without making that change", "tokens": [1436, 321, 393, 8873, 2293, 264, 912, 551, 1553, 1455, 300, 1319], "temperature": 0.0, "avg_logprob": -0.18868024190266927, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.927854095309158e-07}, {"id": 372, "seek": 176360, "start": 1789.7199999999998, "end": 1791.7199999999998, "text": " So there's the question", "tokens": [407, 456, 311, 264, 1168], "temperature": 0.0, "avg_logprob": -0.18868024190266927, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.927854095309158e-07}, {"id": 373, "seek": 179172, "start": 1791.72, "end": 1793.72, "text": " Why did it make a difference?", "tokens": [1545, 630, 309, 652, 257, 2649, 30], "temperature": 0.0, "avg_logprob": -0.1463452923682428, "compression_ratio": 1.8875968992248062, "no_speech_prob": 4.4951389099878725e-06}, {"id": 374, "seek": 179172, "start": 1795.08, "end": 1796.6000000000001, "text": " So in order to answer this question", "tokens": [407, 294, 1668, 281, 1867, 341, 1168], "temperature": 0.0, "avg_logprob": -0.1463452923682428, "compression_ratio": 1.8875968992248062, "no_speech_prob": 4.4951389099878725e-06}, {"id": 375, "seek": 179172, "start": 1796.6000000000001, "end": 1800.28, "text": " I'm going to try and get you all to try and think about this in order to answer this question", "tokens": [286, 478, 516, 281, 853, 293, 483, 291, 439, 281, 853, 293, 519, 466, 341, 294, 1668, 281, 1867, 341, 1168], "temperature": 0.0, "avg_logprob": -0.1463452923682428, "compression_ratio": 1.8875968992248062, "no_speech_prob": 4.4951389099878725e-06}, {"id": 376, "seek": 179172, "start": 1800.28, "end": 1803.84, "text": " You need to think about like okay. What are the things that aren't?", "tokens": [509, 643, 281, 519, 466, 411, 1392, 13, 708, 366, 264, 721, 300, 3212, 380, 30], "temperature": 0.0, "avg_logprob": -0.1463452923682428, "compression_ratio": 1.8875968992248062, "no_speech_prob": 4.4951389099878725e-06}, {"id": 377, "seek": 179172, "start": 1805.64, "end": 1811.08, "text": " Mathematically the same why is why is it not identical? What are the reasons like come up with some hypotheses?", "tokens": [15776, 40197, 264, 912, 983, 307, 983, 307, 309, 406, 14800, 30, 708, 366, 264, 4112, 411, 808, 493, 365, 512, 49969, 30], "temperature": 0.0, "avg_logprob": -0.1463452923682428, "compression_ratio": 1.8875968992248062, "no_speech_prob": 4.4951389099878725e-06}, {"id": 378, "seek": 179172, "start": 1811.08, "end": 1813.08, "text": " What are some reasons that may be?", "tokens": [708, 366, 512, 4112, 300, 815, 312, 30], "temperature": 0.0, "avg_logprob": -0.1463452923682428, "compression_ratio": 1.8875968992248062, "no_speech_prob": 4.4951389099878725e-06}, {"id": 379, "seek": 179172, "start": 1813.24, "end": 1815.24, "text": " we've actually ended up with a", "tokens": [321, 600, 767, 4590, 493, 365, 257], "temperature": 0.0, "avg_logprob": -0.1463452923682428, "compression_ratio": 1.8875968992248062, "no_speech_prob": 4.4951389099878725e-06}, {"id": 380, "seek": 179172, "start": 1815.44, "end": 1819.08, "text": " Better answer and to figure that out we need to first of all start with like well", "tokens": [15753, 1867, 293, 281, 2573, 300, 484, 321, 643, 281, 700, 295, 439, 722, 365, 411, 731], "temperature": 0.0, "avg_logprob": -0.1463452923682428, "compression_ratio": 1.8875968992248062, "no_speech_prob": 4.4951389099878725e-06}, {"id": 381, "seek": 181908, "start": 1819.08, "end": 1821.8, "text": " Why is it even a different answer? Why is that different to that?", "tokens": [1545, 307, 309, 754, 257, 819, 1867, 30, 1545, 307, 300, 819, 281, 300, 30], "temperature": 0.0, "avg_logprob": -0.21902869610076256, "compression_ratio": 1.646551724137931, "no_speech_prob": 4.029430783702992e-06}, {"id": 382, "seek": 181908, "start": 1826.12, "end": 1828.12, "text": " It's a subtle", "tokens": [467, 311, 257, 13743], "temperature": 0.0, "avg_logprob": -0.21902869610076256, "compression_ratio": 1.646551724137931, "no_speech_prob": 4.029430783702992e-06}, {"id": 383, "seek": 181908, "start": 1832.48, "end": 1836.6799999999998, "text": " All right, what do you think I was wondering if it was two different kinds of multiplications", "tokens": [1057, 558, 11, 437, 360, 291, 519, 286, 390, 6359, 498, 309, 390, 732, 819, 3685, 295, 17596, 763], "temperature": 0.0, "avg_logprob": -0.21902869610076256, "compression_ratio": 1.646551724137931, "no_speech_prob": 4.029430783702992e-06}, {"id": 384, "seek": 181908, "start": 1836.6799999999998, "end": 1842.36, "text": " You said that one is the element wise multiplication. No they do end up mathematically being the same okay?", "tokens": [509, 848, 300, 472, 307, 264, 4478, 10829, 27290, 13, 883, 436, 360, 917, 493, 44003, 885, 264, 912, 1392, 30], "temperature": 0.0, "avg_logprob": -0.21902869610076256, "compression_ratio": 1.646551724137931, "no_speech_prob": 4.029430783702992e-06}, {"id": 385, "seek": 184236, "start": 1842.36, "end": 1848.36, "text": " Pretty much. There's a minor in core, but not that it's not that it's not some order operations thing", "tokens": [10693, 709, 13, 821, 311, 257, 6696, 294, 4965, 11, 457, 406, 300, 309, 311, 406, 300, 309, 311, 406, 512, 1668, 7705, 551], "temperature": 0.0, "avg_logprob": -0.23941184596011514, "compression_ratio": 1.5756302521008403, "no_speech_prob": 5.955071173957549e-06}, {"id": 386, "seek": 184236, "start": 1848.9599999999998, "end": 1850.9599999999998, "text": " Let's try can she", "tokens": [961, 311, 853, 393, 750], "temperature": 0.0, "avg_logprob": -0.23941184596011514, "compression_ratio": 1.5756302521008403, "no_speech_prob": 5.955071173957549e-06}, {"id": 387, "seek": 184236, "start": 1851.1999999999998, "end": 1857.6799999999998, "text": " You are on a roll today, so let's say how you go. I feel like the features are less correlated to each other I", "tokens": [509, 366, 322, 257, 3373, 965, 11, 370, 718, 311, 584, 577, 291, 352, 13, 286, 841, 411, 264, 4122, 366, 1570, 38574, 281, 1184, 661, 286], "temperature": 0.0, "avg_logprob": -0.23941184596011514, "compression_ratio": 1.5756302521008403, "no_speech_prob": 5.955071173957549e-06}, {"id": 388, "seek": 184236, "start": 1859.8, "end": 1864.9599999999998, "text": " Mean I've made a claim that these are mathematically equivalent, so", "tokens": [12302, 286, 600, 1027, 257, 3932, 300, 613, 366, 44003, 10344, 11, 370], "temperature": 0.0, "avg_logprob": -0.23941184596011514, "compression_ratio": 1.5756302521008403, "no_speech_prob": 5.955071173957549e-06}, {"id": 389, "seek": 186496, "start": 1864.96, "end": 1871.4, "text": " So what are you saying really you know why are we getting different answers?", "tokens": [407, 437, 366, 291, 1566, 534, 291, 458, 983, 366, 321, 1242, 819, 6338, 30], "temperature": 0.0, "avg_logprob": -0.21895791849958787, "compression_ratio": 1.679245283018868, "no_speech_prob": 5.5942864491953515e-06}, {"id": 390, "seek": 186496, "start": 1876.92, "end": 1878.76, "text": " It's good keep on coming up with hypotheses", "tokens": [467, 311, 665, 1066, 322, 1348, 493, 365, 49969], "temperature": 0.0, "avg_logprob": -0.21895791849958787, "compression_ratio": 1.679245283018868, "no_speech_prob": 5.5942864491953515e-06}, {"id": 391, "seek": 186496, "start": 1878.76, "end": 1881.48, "text": " We need lots of wrong answers before we start finding it's the right ones", "tokens": [492, 643, 3195, 295, 2085, 6338, 949, 321, 722, 5006, 309, 311, 264, 558, 2306], "temperature": 0.0, "avg_logprob": -0.21895791849958787, "compression_ratio": 1.679245283018868, "no_speech_prob": 5.5942864491953515e-06}, {"id": 392, "seek": 186496, "start": 1881.48, "end": 1885.52, "text": " It's like that you know a warmer hotter colder. You know Ernest you're gonna get us hotter", "tokens": [467, 311, 411, 300, 291, 458, 257, 21599, 32149, 31020, 13, 509, 458, 24147, 377, 291, 434, 799, 483, 505, 32149], "temperature": 0.0, "avg_logprob": -0.21895791849958787, "compression_ratio": 1.679245283018868, "no_speech_prob": 5.5942864491953515e-06}, {"id": 393, "seek": 186496, "start": 1885.52, "end": 1887.52, "text": " Does it have anything to do with the regularization?", "tokens": [4402, 309, 362, 1340, 281, 360, 365, 264, 3890, 2144, 30], "temperature": 0.0, "avg_logprob": -0.21895791849958787, "compression_ratio": 1.679245283018868, "no_speech_prob": 5.5942864491953515e-06}, {"id": 394, "seek": 186496, "start": 1888.08, "end": 1894.6000000000001, "text": " Yes, and is it the fact that when you so let's start there right so earnest point here is like okay Jeremy", "tokens": [1079, 11, 293, 307, 309, 264, 1186, 300, 562, 291, 370, 718, 311, 722, 456, 558, 370, 48171, 935, 510, 307, 411, 1392, 17809], "temperature": 0.0, "avg_logprob": -0.21895791849958787, "compression_ratio": 1.679245283018868, "no_speech_prob": 5.5942864491953515e-06}, {"id": 395, "seek": 189460, "start": 1894.6, "end": 1901.32, "text": " You've said they're equivalent, but they're equivalent outcomes right, but you've got through you went through a process to get there and that process", "tokens": [509, 600, 848, 436, 434, 10344, 11, 457, 436, 434, 10344, 10070, 558, 11, 457, 291, 600, 658, 807, 291, 1437, 807, 257, 1399, 281, 483, 456, 293, 300, 1399], "temperature": 0.0, "avg_logprob": -0.22117903380267387, "compression_ratio": 1.8115942028985508, "no_speech_prob": 2.1233711322565796e-06}, {"id": 396, "seek": 189460, "start": 1901.62, "end": 1905.0, "text": " Included regularization, and they're not necessarily equivalent", "tokens": [7779, 44412, 3890, 2144, 11, 293, 436, 434, 406, 4725, 10344], "temperature": 0.0, "avg_logprob": -0.22117903380267387, "compression_ratio": 1.8115942028985508, "no_speech_prob": 2.1233711322565796e-06}, {"id": 397, "seek": 189460, "start": 1905.9199999999998, "end": 1912.8, "text": " Regularization like our loss function has a penalty so yeah help us think through and as to how much that might impact things", "tokens": [45659, 2144, 411, 527, 4470, 2445, 575, 257, 16263, 370, 1338, 854, 505, 519, 807, 293, 382, 281, 577, 709, 300, 1062, 2712, 721], "temperature": 0.0, "avg_logprob": -0.22117903380267387, "compression_ratio": 1.8115942028985508, "no_speech_prob": 2.1233711322565796e-06}, {"id": 398, "seek": 189460, "start": 1912.84, "end": 1914.9599999999998, "text": " Well, this is maybe kind of dumb", "tokens": [1042, 11, 341, 307, 1310, 733, 295, 10316], "temperature": 0.0, "avg_logprob": -0.22117903380267387, "compression_ratio": 1.8115942028985508, "no_speech_prob": 2.1233711322565796e-06}, {"id": 399, "seek": 189460, "start": 1914.9599999999998, "end": 1919.8799999999999, "text": " But I'm just noticing that the numbers are bigger in the ones that have been weighted by the the naive base", "tokens": [583, 286, 478, 445, 21814, 300, 264, 3547, 366, 3801, 294, 264, 2306, 300, 362, 668, 32807, 538, 264, 264, 29052, 3096], "temperature": 0.0, "avg_logprob": -0.22117903380267387, "compression_ratio": 1.8115942028985508, "no_speech_prob": 2.1233711322565796e-06}, {"id": 400, "seek": 189460, "start": 1921.0, "end": 1923.0, "text": " our weights and so", "tokens": [527, 17443, 293, 370], "temperature": 0.0, "avg_logprob": -0.22117903380267387, "compression_ratio": 1.8115942028985508, "no_speech_prob": 2.1233711322565796e-06}, {"id": 401, "seek": 192300, "start": 1923.0, "end": 1929.72, "text": " For these are bigger and some are smaller some are bigger right, but there are some bigger ones like the variance between the columns", "tokens": [1171, 613, 366, 3801, 293, 512, 366, 4356, 512, 366, 3801, 558, 11, 457, 456, 366, 512, 3801, 2306, 411, 264, 21977, 1296, 264, 13766], "temperature": 0.0, "avg_logprob": -0.3089468163180064, "compression_ratio": 1.605, "no_speech_prob": 3.905450739694061e-06}, {"id": 402, "seek": 192300, "start": 1929.72, "end": 1935.6, "text": " Is as much higher the variance is bigger. Yeah, I think that's a very interesting insight. Okay. That's all I got okay", "tokens": [1119, 382, 709, 2946, 264, 21977, 307, 3801, 13, 865, 11, 286, 519, 300, 311, 257, 588, 1880, 11269, 13, 1033, 13, 663, 311, 439, 286, 658, 1392], "temperature": 0.0, "avg_logprob": -0.3089468163180064, "compression_ratio": 1.605, "no_speech_prob": 3.905450739694061e-06}, {"id": 403, "seek": 192300, "start": 1935.96, "end": 1937.96, "text": " So build on that", "tokens": [407, 1322, 322, 300], "temperature": 0.0, "avg_logprob": -0.3089468163180064, "compression_ratio": 1.605, "no_speech_prob": 3.905450739694061e-06}, {"id": 404, "seek": 192300, "start": 1942.96, "end": 1944.96, "text": " Prince has been on a roll or month, so", "tokens": [9821, 575, 668, 322, 257, 3373, 420, 1618, 11, 370], "temperature": 0.0, "avg_logprob": -0.3089468163180064, "compression_ratio": 1.605, "no_speech_prob": 3.905450739694061e-06}, {"id": 405, "seek": 192300, "start": 1945.68, "end": 1947.24, "text": " I'm not sure", "tokens": [286, 478, 406, 988], "temperature": 0.0, "avg_logprob": -0.3089468163180064, "compression_ratio": 1.605, "no_speech_prob": 3.905450739694061e-06}, {"id": 406, "seek": 194724, "start": 1947.24, "end": 1955.24, "text": " Is it also consider like considering the dependency of different words is said why it is forming better rather than all", "tokens": [1119, 309, 611, 1949, 411, 8079, 264, 33621, 295, 819, 2283, 307, 848, 983, 309, 307, 15745, 1101, 2831, 813, 439], "temperature": 0.0, "avg_logprob": -0.2613945007324219, "compression_ratio": 1.7238493723849373, "no_speech_prob": 1.363105639029527e-05}, {"id": 407, "seek": 194724, "start": 1955.88, "end": 1964.36, "text": " Word independent of each other not really I mean it's it's you know again fear it you know theoretically these are creating mathematically equivalent outputs", "tokens": [8725, 6695, 295, 1184, 661, 406, 534, 286, 914, 309, 311, 309, 311, 291, 458, 797, 4240, 309, 291, 458, 29400, 613, 366, 4084, 44003, 10344, 23930], "temperature": 0.0, "avg_logprob": -0.2613945007324219, "compression_ratio": 1.7238493723849373, "no_speech_prob": 1.363105639029527e-05}, {"id": 408, "seek": 194724, "start": 1965.52, "end": 1966.84, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2613945007324219, "compression_ratio": 1.7238493723849373, "no_speech_prob": 1.363105639029527e-05}, {"id": 409, "seek": 194724, "start": 1966.84, "end": 1972.36, "text": " They're not they're not doing something different except as Ernest mentioned", "tokens": [814, 434, 406, 436, 434, 406, 884, 746, 819, 3993, 382, 24147, 377, 2835], "temperature": 0.0, "avg_logprob": -0.2613945007324219, "compression_ratio": 1.7238493723849373, "no_speech_prob": 1.363105639029527e-05}, {"id": 410, "seek": 194724, "start": 1972.88, "end": 1974.68, "text": " They're getting", "tokens": [814, 434, 1242], "temperature": 0.0, "avg_logprob": -0.2613945007324219, "compression_ratio": 1.7238493723849373, "no_speech_prob": 1.363105639029527e-05}, {"id": 411, "seek": 197468, "start": 1974.68, "end": 1977.96, "text": " impacted differently by regularization, so what's", "tokens": [15653, 7614, 538, 3890, 2144, 11, 370, 437, 311], "temperature": 0.0, "avg_logprob": -0.21746191771134085, "compression_ratio": 1.6531531531531531, "no_speech_prob": 4.9369537009624764e-06}, {"id": 412, "seek": 197468, "start": 1978.64, "end": 1980.64, "text": " So what's regularization right?", "tokens": [407, 437, 311, 3890, 2144, 558, 30], "temperature": 0.0, "avg_logprob": -0.21746191771134085, "compression_ratio": 1.6531531531531531, "no_speech_prob": 4.9369537009624764e-06}, {"id": 413, "seek": 197468, "start": 1981.8400000000001, "end": 1984.2, "text": " regularization is we start out with our", "tokens": [3890, 2144, 307, 321, 722, 484, 365, 527], "temperature": 0.0, "avg_logprob": -0.21746191771134085, "compression_ratio": 1.6531531531531531, "no_speech_prob": 4.9369537009624764e-06}, {"id": 414, "seek": 197468, "start": 1987.1200000000001, "end": 1993.0, "text": " That was the weirdest thing I forgot to go into screenwriting mode, and it just turns out that you can actually write in Excel", "tokens": [663, 390, 264, 44807, 551, 286, 5298, 281, 352, 666, 2568, 19868, 4391, 11, 293, 309, 445, 4523, 484, 300, 291, 393, 767, 2464, 294, 19060], "temperature": 0.0, "avg_logprob": -0.21746191771134085, "compression_ratio": 1.6531531531531531, "no_speech_prob": 4.9369537009624764e-06}, {"id": 415, "seek": 197468, "start": 1993.0, "end": 1995.0, "text": " And I had no idea that was true", "tokens": [400, 286, 632, 572, 1558, 300, 390, 2074], "temperature": 0.0, "avg_logprob": -0.21746191771134085, "compression_ratio": 1.6531531531531531, "no_speech_prob": 4.9369537009624764e-06}, {"id": 416, "seek": 197468, "start": 1996.4, "end": 2001.7, "text": " I still use screenwriting roads, so I don't kill up my spreadsheet. I just never tried", "tokens": [286, 920, 764, 2568, 19868, 11344, 11, 370, 286, 500, 380, 1961, 493, 452, 27733, 13, 286, 445, 1128, 3031], "temperature": 0.0, "avg_logprob": -0.21746191771134085, "compression_ratio": 1.6531531531531531, "no_speech_prob": 4.9369537009624764e-06}, {"id": 417, "seek": 200170, "start": 2001.7, "end": 2007.9, "text": " So our loss was equal to like our cross entropy loss", "tokens": [407, 527, 4470, 390, 2681, 281, 411, 527, 3278, 30867, 4470], "temperature": 0.0, "avg_logprob": -0.3134548226181342, "compression_ratio": 1.3771929824561404, "no_speech_prob": 1.706172156445973e-06}, {"id": 418, "seek": 200170, "start": 2008.54, "end": 2010.1000000000001, "text": " you know", "tokens": [291, 458], "temperature": 0.0, "avg_logprob": -0.3134548226181342, "compression_ratio": 1.3771929824561404, "no_speech_prob": 1.706172156445973e-06}, {"id": 419, "seek": 200170, "start": 2010.1000000000001, "end": 2012.1000000000001, "text": " based on the", "tokens": [2361, 322, 264], "temperature": 0.0, "avg_logprob": -0.3134548226181342, "compression_ratio": 1.3771929824561404, "no_speech_prob": 1.706172156445973e-06}, {"id": 420, "seek": 200170, "start": 2014.06, "end": 2021.1000000000001, "text": " Predictions or the predictions and the actuals right plus our", "tokens": [32969, 15607, 420, 264, 21264, 293, 264, 3539, 82, 558, 1804, 527], "temperature": 0.0, "avg_logprob": -0.3134548226181342, "compression_ratio": 1.3771929824561404, "no_speech_prob": 1.706172156445973e-06}, {"id": 421, "seek": 200170, "start": 2023.26, "end": 2025.26, "text": " Penalty so", "tokens": [10571, 304, 874, 370], "temperature": 0.0, "avg_logprob": -0.3134548226181342, "compression_ratio": 1.3771929824561404, "no_speech_prob": 1.706172156445973e-06}, {"id": 422, "seek": 200170, "start": 2027.98, "end": 2029.98, "text": " If you're", "tokens": [759, 291, 434], "temperature": 0.0, "avg_logprob": -0.3134548226181342, "compression_ratio": 1.3771929824561404, "no_speech_prob": 1.706172156445973e-06}, {"id": 423, "seek": 202998, "start": 2029.98, "end": 2032.26, "text": " If your weights are large", "tokens": [759, 428, 17443, 366, 2416], "temperature": 0.0, "avg_logprob": -0.14066775278611618, "compression_ratio": 1.7760416666666667, "no_speech_prob": 5.862752459506737e-06}, {"id": 424, "seek": 202998, "start": 2033.26, "end": 2036.34, "text": " Right then that piece gets bigger", "tokens": [1779, 550, 300, 2522, 2170, 3801], "temperature": 0.0, "avg_logprob": -0.14066775278611618, "compression_ratio": 1.7760416666666667, "no_speech_prob": 5.862752459506737e-06}, {"id": 425, "seek": 202998, "start": 2037.06, "end": 2044.78, "text": " Right and it drowns out that piece right, but that's actually the piece we care about right we actually want it to be a good fit", "tokens": [1779, 293, 309, 20337, 82, 484, 300, 2522, 558, 11, 457, 300, 311, 767, 264, 2522, 321, 1127, 466, 558, 321, 767, 528, 309, 281, 312, 257, 665, 3318], "temperature": 0.0, "avg_logprob": -0.14066775278611618, "compression_ratio": 1.7760416666666667, "no_speech_prob": 5.862752459506737e-06}, {"id": 426, "seek": 202998, "start": 2045.54, "end": 2051.78, "text": " So we want to have as little regularization going on as we can get away with we want so we want to have less", "tokens": [407, 321, 528, 281, 362, 382, 707, 3890, 2144, 516, 322, 382, 321, 393, 483, 1314, 365, 321, 528, 370, 321, 528, 281, 362, 1570], "temperature": 0.0, "avg_logprob": -0.14066775278611618, "compression_ratio": 1.7760416666666667, "no_speech_prob": 5.862752459506737e-06}, {"id": 427, "seek": 202998, "start": 2052.54, "end": 2054.54, "text": " weights", "tokens": [17443], "temperature": 0.0, "avg_logprob": -0.14066775278611618, "compression_ratio": 1.7760416666666667, "no_speech_prob": 5.862752459506737e-06}, {"id": 428, "seek": 202998, "start": 2054.9, "end": 2058.2, "text": " So here's the thing right our value", "tokens": [407, 510, 311, 264, 551, 558, 527, 2158], "temperature": 0.0, "avg_logprob": -0.14066775278611618, "compression_ratio": 1.7760416666666667, "no_speech_prob": 5.862752459506737e-06}, {"id": 429, "seek": 205820, "start": 2058.2, "end": 2060.2, "text": " Yes, can you pass it over here?", "tokens": [1079, 11, 393, 291, 1320, 309, 670, 510, 30], "temperature": 0.0, "avg_logprob": -0.21248101750645068, "compression_ratio": 1.6610878661087867, "no_speech_prob": 2.994422857227619e-06}, {"id": 430, "seek": 205820, "start": 2063.16, "end": 2067.2, "text": " When you see the less weights did you mean lesser weights I do yeah", "tokens": [1133, 291, 536, 264, 1570, 17443, 630, 291, 914, 22043, 17443, 286, 360, 1338], "temperature": 0.0, "avg_logprob": -0.21248101750645068, "compression_ratio": 1.6610878661087867, "no_speech_prob": 2.994422857227619e-06}, {"id": 431, "seek": 205820, "start": 2067.4399999999996, "end": 2072.22, "text": " Yeah, and I kind of use the two words a little equivalently which is not quite fair", "tokens": [865, 11, 293, 286, 733, 295, 764, 264, 732, 2283, 257, 707, 9052, 2276, 597, 307, 406, 1596, 3143], "temperature": 0.0, "avg_logprob": -0.21248101750645068, "compression_ratio": 1.6610878661087867, "no_speech_prob": 2.994422857227619e-06}, {"id": 432, "seek": 205820, "start": 2072.22, "end": 2075.7799999999997, "text": " I agree, but the idea is that weights that are pretty close to zero are kind of not there", "tokens": [286, 3986, 11, 457, 264, 1558, 307, 300, 17443, 300, 366, 1238, 1998, 281, 4018, 366, 733, 295, 406, 456], "temperature": 0.0, "avg_logprob": -0.21248101750645068, "compression_ratio": 1.6610878661087867, "no_speech_prob": 2.994422857227619e-06}, {"id": 433, "seek": 205820, "start": 2079.04, "end": 2081.48, "text": " So here's the thing our values of R", "tokens": [407, 510, 311, 264, 551, 527, 4190, 295, 497], "temperature": 0.0, "avg_logprob": -0.21248101750645068, "compression_ratio": 1.6610878661087867, "no_speech_prob": 2.994422857227619e-06}, {"id": 434, "seek": 205820, "start": 2082.8199999999997, "end": 2086.7999999999997, "text": " You know and I'm not a Bayesian weenie, but I'm still going to use the word prior right", "tokens": [509, 458, 293, 286, 478, 406, 257, 7840, 42434, 321, 268, 414, 11, 457, 286, 478, 920, 516, 281, 764, 264, 1349, 4059, 558], "temperature": 0.0, "avg_logprob": -0.21248101750645068, "compression_ratio": 1.6610878661087867, "no_speech_prob": 2.994422857227619e-06}, {"id": 435, "seek": 208680, "start": 2086.8, "end": 2089.8, "text": " They're kind of like a prior for like we think", "tokens": [814, 434, 733, 295, 411, 257, 4059, 337, 411, 321, 519], "temperature": 0.0, "avg_logprob": -0.23384353693793802, "compression_ratio": 1.7039106145251397, "no_speech_prob": 2.6425700525578577e-06}, {"id": 436, "seek": 208680, "start": 2090.7200000000003, "end": 2092.4, "text": " that the", "tokens": [300, 264], "temperature": 0.0, "avg_logprob": -0.23384353693793802, "compression_ratio": 1.7039106145251397, "no_speech_prob": 2.6425700525578577e-06}, {"id": 437, "seek": 208680, "start": 2092.4, "end": 2098.2400000000002, "text": " the different levels of importance and positive or negative of these different features", "tokens": [264, 819, 4358, 295, 7379, 293, 3353, 420, 3671, 295, 613, 819, 4122], "temperature": 0.0, "avg_logprob": -0.23384353693793802, "compression_ratio": 1.7039106145251397, "no_speech_prob": 2.6425700525578577e-06}, {"id": 438, "seek": 208680, "start": 2099.0, "end": 2103.4, "text": " Might be something like that right we think that like bad", "tokens": [23964, 312, 746, 411, 300, 558, 321, 519, 300, 411, 1578], "temperature": 0.0, "avg_logprob": -0.23384353693793802, "compression_ratio": 1.7039106145251397, "no_speech_prob": 2.6425700525578577e-06}, {"id": 439, "seek": 208680, "start": 2104.6800000000003, "end": 2106.6800000000003, "text": " You know might be", "tokens": [509, 458, 1062, 312], "temperature": 0.0, "avg_logprob": -0.23384353693793802, "compression_ratio": 1.7039106145251397, "no_speech_prob": 2.6425700525578577e-06}, {"id": 440, "seek": 210668, "start": 2106.68, "end": 2116.52, "text": " More correlated with negative than than good right so our kind of implicit assumption", "tokens": [5048, 38574, 365, 3671, 813, 813, 665, 558, 370, 527, 733, 295, 26947, 15302], "temperature": 0.0, "avg_logprob": -0.25617447944536603, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.1726372122211615e-06}, {"id": 441, "seek": 210668, "start": 2119.04, "end": 2123.54, "text": " The before was that we have no priors so in other words when we'd said", "tokens": [440, 949, 390, 300, 321, 362, 572, 1790, 830, 370, 294, 661, 2283, 562, 321, 1116, 848], "temperature": 0.0, "avg_logprob": -0.25617447944536603, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.1726372122211615e-06}, {"id": 442, "seek": 210668, "start": 2124.08, "end": 2128.7799999999997, "text": " Squared weights we're saying a non zero weight is something we don't want to have", "tokens": [8683, 1642, 17443, 321, 434, 1566, 257, 2107, 4018, 3364, 307, 746, 321, 500, 380, 528, 281, 362], "temperature": 0.0, "avg_logprob": -0.25617447944536603, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.1726372122211615e-06}, {"id": 443, "seek": 210668, "start": 2129.16, "end": 2133.64, "text": " right, but actually I think what I really want to say is that", "tokens": [558, 11, 457, 767, 286, 519, 437, 286, 534, 528, 281, 584, 307, 300], "temperature": 0.0, "avg_logprob": -0.25617447944536603, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.1726372122211615e-06}, {"id": 444, "seek": 213364, "start": 2133.64, "end": 2137.6, "text": " differing from the naive Bayes expectation is", "tokens": [743, 278, 490, 264, 29052, 7840, 279, 14334, 307], "temperature": 0.0, "avg_logprob": -0.17995689131996848, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.2732695015292848e-07}, {"id": 445, "seek": 213364, "start": 2138.44, "end": 2142.56, "text": " Something I don't want to do right like only vary from the naive Bayes", "tokens": [6595, 286, 500, 380, 528, 281, 360, 558, 411, 787, 10559, 490, 264, 29052, 7840, 279], "temperature": 0.0, "avg_logprob": -0.17995689131996848, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.2732695015292848e-07}, {"id": 446, "seek": 213364, "start": 2143.24, "end": 2146.72, "text": " Prior unless you have good reason to believe otherwise", "tokens": [24032, 5969, 291, 362, 665, 1778, 281, 1697, 5911], "temperature": 0.0, "avg_logprob": -0.17995689131996848, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.2732695015292848e-07}, {"id": 447, "seek": 213364, "start": 2148.16, "end": 2153.8799999999997, "text": " Right and so that's actually what this ends up doing right we end up saying you know what?", "tokens": [1779, 293, 370, 300, 311, 767, 437, 341, 5314, 493, 884, 558, 321, 917, 493, 1566, 291, 458, 437, 30], "temperature": 0.0, "avg_logprob": -0.17995689131996848, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.2732695015292848e-07}, {"id": 448, "seek": 213364, "start": 2155.64, "end": 2158.08, "text": " We think this value is probably free", "tokens": [492, 519, 341, 2158, 307, 1391, 1737], "temperature": 0.0, "avg_logprob": -0.17995689131996848, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.2732695015292848e-07}, {"id": 449, "seek": 215808, "start": 2158.08, "end": 2164.56, "text": " Right and so if you're going to like make it a lot bigger or a lot smaller", "tokens": [1779, 293, 370, 498, 291, 434, 516, 281, 411, 652, 309, 257, 688, 3801, 420, 257, 688, 4356], "temperature": 0.0, "avg_logprob": -0.15415537357330322, "compression_ratio": 1.7675438596491229, "no_speech_prob": 5.989263058836514e-07}, {"id": 450, "seek": 215808, "start": 2165.12, "end": 2172.7999999999997, "text": " Right that's going to create the kind of variation in weights. That's going to cause that squared term to go up right so", "tokens": [1779, 300, 311, 516, 281, 1884, 264, 733, 295, 12990, 294, 17443, 13, 663, 311, 516, 281, 3082, 300, 8889, 1433, 281, 352, 493, 558, 370], "temperature": 0.0, "avg_logprob": -0.15415537357330322, "compression_ratio": 1.7675438596491229, "no_speech_prob": 5.989263058836514e-07}, {"id": 451, "seek": 215808, "start": 2173.4, "end": 2179.2799999999997, "text": " So if you can you know just leave all these values about similar to where they are now", "tokens": [407, 498, 291, 393, 291, 458, 445, 1856, 439, 613, 4190, 466, 2531, 281, 689, 436, 366, 586], "temperature": 0.0, "avg_logprob": -0.15415537357330322, "compression_ratio": 1.7675438596491229, "no_speech_prob": 5.989263058836514e-07}, {"id": 452, "seek": 215808, "start": 2179.92, "end": 2186.96, "text": " Right and so that's what the penalty term is now doing right the penalty term when our inputs is already multiplied by R", "tokens": [1779, 293, 370, 300, 311, 437, 264, 16263, 1433, 307, 586, 884, 558, 264, 16263, 1433, 562, 527, 15743, 307, 1217, 17207, 538, 497], "temperature": 0.0, "avg_logprob": -0.15415537357330322, "compression_ratio": 1.7675438596491229, "no_speech_prob": 5.989263058836514e-07}, {"id": 453, "seek": 218696, "start": 2186.96, "end": 2193.36, "text": " Is saying penalize things where we're varying it from our naive Bayes?", "tokens": [1119, 1566, 13661, 1125, 721, 689, 321, 434, 22984, 309, 490, 527, 29052, 7840, 279, 30], "temperature": 0.0, "avg_logprob": -0.31435194469633554, "compression_ratio": 1.4319526627218935, "no_speech_prob": 4.029426690976834e-06}, {"id": 454, "seek": 218696, "start": 2194.08, "end": 2196.08, "text": " prior", "tokens": [4059], "temperature": 0.0, "avg_logprob": -0.31435194469633554, "compression_ratio": 1.4319526627218935, "no_speech_prob": 4.029426690976834e-06}, {"id": 455, "seek": 218696, "start": 2196.56, "end": 2198.56, "text": " Can you pass that there?", "tokens": [1664, 291, 1320, 300, 456, 30], "temperature": 0.0, "avg_logprob": -0.31435194469633554, "compression_ratio": 1.4319526627218935, "no_speech_prob": 4.029426690976834e-06}, {"id": 456, "seek": 218696, "start": 2200.64, "end": 2202.8, "text": " Why multiply only with the R not", "tokens": [1545, 12972, 787, 365, 264, 497, 406], "temperature": 0.0, "avg_logprob": -0.31435194469633554, "compression_ratio": 1.4319526627218935, "no_speech_prob": 4.029426690976834e-06}, {"id": 457, "seek": 218696, "start": 2203.68, "end": 2207.52, "text": " Constant like R squared or something like that when the variance would be much higher this time", "tokens": [37413, 411, 497, 8889, 420, 746, 411, 300, 562, 264, 21977, 576, 312, 709, 2946, 341, 565], "temperature": 0.0, "avg_logprob": -0.31435194469633554, "compression_ratio": 1.4319526627218935, "no_speech_prob": 4.029426690976834e-06}, {"id": 458, "seek": 218696, "start": 2208.92, "end": 2210.92, "text": " because our", "tokens": [570, 527], "temperature": 0.0, "avg_logprob": -0.31435194469633554, "compression_ratio": 1.4319526627218935, "no_speech_prob": 4.029426690976834e-06}, {"id": 459, "seek": 221092, "start": 2210.92, "end": 2218.2000000000003, "text": " Prior comes from an actual theoretical model right so I said like I don't like to rely on theory", "tokens": [24032, 1487, 490, 364, 3539, 20864, 2316, 558, 370, 286, 848, 411, 286, 500, 380, 411, 281, 10687, 322, 5261], "temperature": 0.0, "avg_logprob": -0.18371930414316606, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.328773464592814e-06}, {"id": 460, "seek": 221092, "start": 2218.64, "end": 2220.64, "text": " But I have if I have some theory", "tokens": [583, 286, 362, 498, 286, 362, 512, 5261], "temperature": 0.0, "avg_logprob": -0.18371930414316606, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.328773464592814e-06}, {"id": 461, "seek": 221092, "start": 2221.56, "end": 2228.08, "text": " Then you know maybe we should use that as our starting point rather than starting off by assuming everything's equal", "tokens": [1396, 291, 458, 1310, 321, 820, 764, 300, 382, 527, 2891, 935, 2831, 813, 2891, 766, 538, 11926, 1203, 311, 2681], "temperature": 0.0, "avg_logprob": -0.18371930414316606, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.328773464592814e-06}, {"id": 462, "seek": 221092, "start": 2228.2000000000003, "end": 2233.7200000000003, "text": " So our prior said hey, we've got this model called naive Bayes and the naive Bayes model said", "tokens": [407, 527, 4059, 848, 4177, 11, 321, 600, 658, 341, 2316, 1219, 29052, 7840, 279, 293, 264, 29052, 7840, 279, 2316, 848], "temperature": 0.0, "avg_logprob": -0.18371930414316606, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.328773464592814e-06}, {"id": 463, "seek": 221092, "start": 2234.76, "end": 2237.16, "text": " If the naive Bayes assumptions were correct", "tokens": [759, 264, 29052, 7840, 279, 17695, 645, 3006], "temperature": 0.0, "avg_logprob": -0.18371930414316606, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.328773464592814e-06}, {"id": 464, "seek": 223716, "start": 2237.16, "end": 2242.52, "text": " Then R is the correct coefficient right in this specific", "tokens": [1396, 497, 307, 264, 3006, 17619, 558, 294, 341, 2685], "temperature": 0.0, "avg_logprob": -0.25550244836246266, "compression_ratio": 1.5405405405405406, "no_speech_prob": 1.0845141105164657e-06}, {"id": 465, "seek": 223716, "start": 2243.52, "end": 2244.6, "text": " formulation", "tokens": [37642], "temperature": 0.0, "avg_logprob": -0.25550244836246266, "compression_ratio": 1.5405405405405406, "no_speech_prob": 1.0845141105164657e-06}, {"id": 466, "seek": 223716, "start": 2244.6, "end": 2249.92, "text": " That that's why we pick that because our our prior is based on that that theory", "tokens": [663, 300, 311, 983, 321, 1888, 300, 570, 527, 527, 4059, 307, 2361, 322, 300, 300, 5261], "temperature": 0.0, "avg_logprob": -0.25550244836246266, "compression_ratio": 1.5405405405405406, "no_speech_prob": 1.0845141105164657e-06}, {"id": 467, "seek": 223716, "start": 2254.68, "end": 2259.18, "text": " Okay, so this is a really interesting insight which I", "tokens": [1033, 11, 370, 341, 307, 257, 534, 1880, 11269, 597, 286], "temperature": 0.0, "avg_logprob": -0.25550244836246266, "compression_ratio": 1.5405405405405406, "no_speech_prob": 1.0845141105164657e-06}, {"id": 468, "seek": 223716, "start": 2260.2799999999997, "end": 2266.64, "text": " Never really see covered which is this idea is that we can use these like you know", "tokens": [7344, 534, 536, 5343, 597, 307, 341, 1558, 307, 300, 321, 393, 764, 613, 411, 291, 458], "temperature": 0.0, "avg_logprob": -0.25550244836246266, "compression_ratio": 1.5405405405405406, "no_speech_prob": 1.0845141105164657e-06}, {"id": 469, "seek": 226664, "start": 2266.64, "end": 2268.4, "text": " traditional", "tokens": [5164], "temperature": 0.0, "avg_logprob": -0.17785249131449152, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.963799149962142e-06}, {"id": 470, "seek": 226664, "start": 2268.4, "end": 2273.16, "text": " Machine learning techniques we can imbue them with this kind of Bayesian sense", "tokens": [22155, 2539, 7512, 321, 393, 566, 65, 622, 552, 365, 341, 733, 295, 7840, 42434, 2020], "temperature": 0.0, "avg_logprob": -0.17785249131449152, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.963799149962142e-06}, {"id": 471, "seek": 226664, "start": 2274.12, "end": 2275.24, "text": " by", "tokens": [538], "temperature": 0.0, "avg_logprob": -0.17785249131449152, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.963799149962142e-06}, {"id": 472, "seek": 226664, "start": 2275.24, "end": 2277.24, "text": " by starting out", "tokens": [538, 2891, 484], "temperature": 0.0, "avg_logprob": -0.17785249131449152, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.963799149962142e-06}, {"id": 473, "seek": 226664, "start": 2277.52, "end": 2278.8399999999997, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.17785249131449152, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.963799149962142e-06}, {"id": 474, "seek": 226664, "start": 2278.8399999999997, "end": 2281.2, "text": " incorporating our theoretical expectations", "tokens": [33613, 527, 20864, 9843], "temperature": 0.0, "avg_logprob": -0.17785249131449152, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.963799149962142e-06}, {"id": 475, "seek": 226664, "start": 2281.96, "end": 2286.24, "text": " Into the data that we give our model right and when we do so", "tokens": [23373, 264, 1412, 300, 321, 976, 527, 2316, 558, 293, 562, 321, 360, 370], "temperature": 0.0, "avg_logprob": -0.17785249131449152, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.963799149962142e-06}, {"id": 476, "seek": 226664, "start": 2286.8799999999997, "end": 2293.4, "text": " That then means we don't have to regularize as much and that's good right because if we regularize a lot", "tokens": [663, 550, 1355, 321, 500, 380, 362, 281, 3890, 1125, 382, 709, 293, 300, 311, 665, 558, 570, 498, 321, 3890, 1125, 257, 688], "temperature": 0.0, "avg_logprob": -0.17785249131449152, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.963799149962142e-06}, {"id": 477, "seek": 229340, "start": 2293.4, "end": 2295.4, "text": " But let's try it", "tokens": [583, 718, 311, 853, 309], "temperature": 0.0, "avg_logprob": -0.30192796115217535, "compression_ratio": 1.3706293706293706, "no_speech_prob": 5.255357336864108e-06}, {"id": 478, "seek": 229340, "start": 2297.64, "end": 2299.64, "text": " Let's go back to", "tokens": [961, 311, 352, 646, 281], "temperature": 0.0, "avg_logprob": -0.30192796115217535, "compression_ratio": 1.3706293706293706, "no_speech_prob": 5.255357336864108e-06}, {"id": 479, "seek": 229340, "start": 2300.56, "end": 2302.56, "text": " You know here's our", "tokens": [509, 458, 510, 311, 527], "temperature": 0.0, "avg_logprob": -0.30192796115217535, "compression_ratio": 1.3706293706293706, "no_speech_prob": 5.255357336864108e-06}, {"id": 480, "seek": 229340, "start": 2309.46, "end": 2315.92, "text": " Remember the way they do it in the SKA logistic regression is this is the reciprocal of the amount of", "tokens": [5459, 264, 636, 436, 360, 309, 294, 264, 318, 16135, 3565, 3142, 24590, 307, 341, 307, 264, 46948, 295, 264, 2372, 295], "temperature": 0.0, "avg_logprob": -0.30192796115217535, "compression_ratio": 1.3706293706293706, "no_speech_prob": 5.255357336864108e-06}, {"id": 481, "seek": 229340, "start": 2317.44, "end": 2319.6800000000003, "text": " regularization penalty, so we'll kind of", "tokens": [3890, 2144, 16263, 11, 370, 321, 603, 733, 295], "temperature": 0.0, "avg_logprob": -0.30192796115217535, "compression_ratio": 1.3706293706293706, "no_speech_prob": 5.255357336864108e-06}, {"id": 482, "seek": 231968, "start": 2319.68, "end": 2324.68, "text": " Add lots of regularization by making it small", "tokens": [5349, 3195, 295, 3890, 2144, 538, 1455, 309, 1359], "temperature": 0.0, "avg_logprob": -0.18347821916852677, "compression_ratio": 1.6157894736842104, "no_speech_prob": 3.576353719836334e-07}, {"id": 483, "seek": 231968, "start": 2328.8799999999997, "end": 2330.8799999999997, "text": " So that like really hurts", "tokens": [407, 300, 411, 534, 11051], "temperature": 0.0, "avg_logprob": -0.18347821916852677, "compression_ratio": 1.6157894736842104, "no_speech_prob": 3.576353719836334e-07}, {"id": 484, "seek": 231968, "start": 2331.6, "end": 2334.12, "text": " That really hurts our accuracy because now", "tokens": [663, 534, 11051, 527, 14170, 570, 586], "temperature": 0.0, "avg_logprob": -0.18347821916852677, "compression_ratio": 1.6157894736842104, "no_speech_prob": 3.576353719836334e-07}, {"id": 485, "seek": 231968, "start": 2335.2, "end": 2339.7999999999997, "text": " It's trying really hard to get those weights down the loss function is overwhelmed", "tokens": [467, 311, 1382, 534, 1152, 281, 483, 729, 17443, 760, 264, 4470, 2445, 307, 19042], "temperature": 0.0, "avg_logprob": -0.18347821916852677, "compression_ratio": 1.6157894736842104, "no_speech_prob": 3.576353719836334e-07}, {"id": 486, "seek": 231968, "start": 2340.3199999999997, "end": 2348.08, "text": " By the need to reduce the weights and the need to make it predictive is kind of now seems totally unimportant", "tokens": [3146, 264, 643, 281, 5407, 264, 17443, 293, 264, 643, 281, 652, 309, 35521, 307, 733, 295, 586, 2544, 3879, 517, 41654], "temperature": 0.0, "avg_logprob": -0.18347821916852677, "compression_ratio": 1.6157894736842104, "no_speech_prob": 3.576353719836334e-07}, {"id": 487, "seek": 234808, "start": 2348.08, "end": 2350.08, "text": " right so", "tokens": [558, 370], "temperature": 0.0, "avg_logprob": -0.16501306843113256, "compression_ratio": 1.6432748538011697, "no_speech_prob": 2.0580362161126686e-06}, {"id": 488, "seek": 234808, "start": 2352.12, "end": 2357.56, "text": " So by kind of starting out and saying you know what don't push the weights down so that you end up", "tokens": [407, 538, 733, 295, 2891, 484, 293, 1566, 291, 458, 437, 500, 380, 2944, 264, 17443, 760, 370, 300, 291, 917, 493], "temperature": 0.0, "avg_logprob": -0.16501306843113256, "compression_ratio": 1.6432748538011697, "no_speech_prob": 2.0580362161126686e-06}, {"id": 489, "seek": 234808, "start": 2358.7599999999998, "end": 2360.04, "text": " ignoring", "tokens": [26258], "temperature": 0.0, "avg_logprob": -0.16501306843113256, "compression_ratio": 1.6432748538011697, "no_speech_prob": 2.0580362161126686e-06}, {"id": 490, "seek": 234808, "start": 2360.04, "end": 2366.0, "text": " The the terms but instead push them down so that you try to get rid of you know ignore", "tokens": [440, 264, 2115, 457, 2602, 2944, 552, 760, 370, 300, 291, 853, 281, 483, 3973, 295, 291, 458, 11200], "temperature": 0.0, "avg_logprob": -0.16501306843113256, "compression_ratio": 1.6432748538011697, "no_speech_prob": 2.0580362161126686e-06}, {"id": 491, "seek": 234808, "start": 2366.68, "end": 2368.68, "text": " differences from our", "tokens": [7300, 490, 527], "temperature": 0.0, "avg_logprob": -0.16501306843113256, "compression_ratio": 1.6432748538011697, "no_speech_prob": 2.0580362161126686e-06}, {"id": 492, "seek": 234808, "start": 2368.68, "end": 2370.68, "text": " expectation based on the naive Bayes", "tokens": [14334, 2361, 322, 264, 29052, 7840, 279], "temperature": 0.0, "avg_logprob": -0.16501306843113256, "compression_ratio": 1.6432748538011697, "no_speech_prob": 2.0580362161126686e-06}, {"id": 493, "seek": 234808, "start": 2371.16, "end": 2373.16, "text": " formulation", "tokens": [37642], "temperature": 0.0, "avg_logprob": -0.16501306843113256, "compression_ratio": 1.6432748538011697, "no_speech_prob": 2.0580362161126686e-06}, {"id": 494, "seek": 237316, "start": 2373.16, "end": 2376.16, "text": " So that", "tokens": [407, 300], "temperature": 0.0, "avg_logprob": -0.22035228374392488, "compression_ratio": 1.5203619909502262, "no_speech_prob": 6.083562880121463e-07}, {"id": 495, "seek": 237316, "start": 2378.56, "end": 2380.56, "text": " Ends up giving us a", "tokens": [6967, 82, 493, 2902, 505, 257], "temperature": 0.0, "avg_logprob": -0.22035228374392488, "compression_ratio": 1.5203619909502262, "no_speech_prob": 6.083562880121463e-07}, {"id": 496, "seek": 237316, "start": 2381.2799999999997, "end": 2382.8799999999997, "text": " very nice", "tokens": [588, 1481], "temperature": 0.0, "avg_logprob": -0.22035228374392488, "compression_ratio": 1.5203619909502262, "no_speech_prob": 6.083562880121463e-07}, {"id": 497, "seek": 237316, "start": 2382.8799999999997, "end": 2389.3999999999996, "text": " Result which actually was originally this this technique was originally presented. I think about 2012", "tokens": [5015, 723, 597, 767, 390, 7993, 341, 341, 6532, 390, 7993, 8212, 13, 286, 519, 466, 9125], "temperature": 0.0, "avg_logprob": -0.22035228374392488, "compression_ratio": 1.5203619909502262, "no_speech_prob": 6.083562880121463e-07}, {"id": 498, "seek": 237316, "start": 2390.04, "end": 2393.48, "text": " Chris Manning who's a terrific NLP researcher up at Stanford and", "tokens": [6688, 2458, 773, 567, 311, 257, 20899, 426, 45196, 21751, 493, 412, 20374, 293], "temperature": 0.0, "avg_logprob": -0.22035228374392488, "compression_ratio": 1.5203619909502262, "no_speech_prob": 6.083562880121463e-07}, {"id": 499, "seek": 237316, "start": 2394.04, "end": 2399.3199999999997, "text": " Cedar Wang who I don't know, but I assume is awesome because this paper is awesome. They basically came up", "tokens": [383, 34385, 14499, 567, 286, 500, 380, 458, 11, 457, 286, 6552, 307, 3476, 570, 341, 3035, 307, 3476, 13, 814, 1936, 1361, 493], "temperature": 0.0, "avg_logprob": -0.22035228374392488, "compression_ratio": 1.5203619909502262, "no_speech_prob": 6.083562880121463e-07}, {"id": 500, "seek": 237316, "start": 2400.12, "end": 2402.12, "text": " with this with this idea", "tokens": [365, 341, 365, 341, 1558], "temperature": 0.0, "avg_logprob": -0.22035228374392488, "compression_ratio": 1.5203619909502262, "no_speech_prob": 6.083562880121463e-07}, {"id": 501, "seek": 240212, "start": 2402.12, "end": 2403.96, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.22822783210060812, "compression_ratio": 1.663265306122449, "no_speech_prob": 1.154456640506396e-06}, {"id": 502, "seek": 240212, "start": 2403.96, "end": 2410.7599999999998, "text": " What they did was they compared it to a number of other approaches on a number of other", "tokens": [708, 436, 630, 390, 436, 5347, 309, 281, 257, 1230, 295, 661, 11587, 322, 257, 1230, 295, 661], "temperature": 0.0, "avg_logprob": -0.22822783210060812, "compression_ratio": 1.663265306122449, "no_speech_prob": 1.154456640506396e-06}, {"id": 503, "seek": 240212, "start": 2411.2799999999997, "end": 2417.72, "text": " Datasets so one of the things they tried is this one is the IMDB data set right and so here's naive Bayes SPM on", "tokens": [9315, 296, 1385, 370, 472, 295, 264, 721, 436, 3031, 307, 341, 472, 307, 264, 21463, 27735, 1412, 992, 558, 293, 370, 510, 311, 29052, 7840, 279, 8420, 44, 322], "temperature": 0.0, "avg_logprob": -0.22822783210060812, "compression_ratio": 1.663265306122449, "no_speech_prob": 1.154456640506396e-06}, {"id": 504, "seek": 240212, "start": 2417.72, "end": 2421.2, "text": " Bygrams and as you can see this approach", "tokens": [3146, 1342, 82, 293, 382, 291, 393, 536, 341, 3109], "temperature": 0.0, "avg_logprob": -0.22822783210060812, "compression_ratio": 1.663265306122449, "no_speech_prob": 1.154456640506396e-06}, {"id": 505, "seek": 240212, "start": 2422.16, "end": 2423.88, "text": " outperformed the other", "tokens": [484, 610, 22892, 264, 661], "temperature": 0.0, "avg_logprob": -0.22822783210060812, "compression_ratio": 1.663265306122449, "no_speech_prob": 1.154456640506396e-06}, {"id": 506, "seek": 240212, "start": 2423.88, "end": 2427.16, "text": " linear based approaches that they looked at and also some", "tokens": [8213, 2361, 11587, 300, 436, 2956, 412, 293, 611, 512], "temperature": 0.0, "avg_logprob": -0.22822783210060812, "compression_ratio": 1.663265306122449, "no_speech_prob": 1.154456640506396e-06}, {"id": 507, "seek": 242716, "start": 2427.16, "end": 2434.44, "text": " Restricted Boltzmann machine kind of neural net based approaches they looked at now nowadays", "tokens": [13094, 3740, 292, 37884, 89, 14912, 3479, 733, 295, 18161, 2533, 2361, 11587, 436, 2956, 412, 586, 13434], "temperature": 0.0, "avg_logprob": -0.15425418814023337, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.785286480706418e-06}, {"id": 508, "seek": 242716, "start": 2435.3199999999997, "end": 2436.7599999999998, "text": " There are better ways there", "tokens": [821, 366, 1101, 2098, 456], "temperature": 0.0, "avg_logprob": -0.15425418814023337, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.785286480706418e-06}, {"id": 509, "seek": 242716, "start": 2436.7599999999998, "end": 2439.66, "text": " You know there are better ways to do this and in fact in the deep learning course", "tokens": [509, 458, 456, 366, 1101, 2098, 281, 360, 341, 293, 294, 1186, 294, 264, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.15425418814023337, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.785286480706418e-06}, {"id": 510, "seek": 242716, "start": 2439.66, "end": 2443.44, "text": " We showed a new state-of-the-art result that we just developed at fast AI that gets", "tokens": [492, 4712, 257, 777, 1785, 12, 2670, 12, 3322, 12, 446, 1874, 300, 321, 445, 4743, 412, 2370, 7318, 300, 2170], "temperature": 0.0, "avg_logprob": -0.15425418814023337, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.785286480706418e-06}, {"id": 511, "seek": 242716, "start": 2444.2, "end": 2446.2, "text": " well over 94 percent", "tokens": [731, 670, 30849, 3043], "temperature": 0.0, "avg_logprob": -0.15425418814023337, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.785286480706418e-06}, {"id": 512, "seek": 242716, "start": 2446.92, "end": 2452.48, "text": " But still you know like particularly for a linear technique. That's easy fast and intuitive", "tokens": [583, 920, 291, 458, 411, 4098, 337, 257, 8213, 6532, 13, 663, 311, 1858, 2370, 293, 21769], "temperature": 0.0, "avg_logprob": -0.15425418814023337, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.785286480706418e-06}, {"id": 513, "seek": 245248, "start": 2452.48, "end": 2457.56, "text": " This is pretty good, and you'll notice when they when they did this they only used by grams", "tokens": [639, 307, 1238, 665, 11, 293, 291, 603, 3449, 562, 436, 562, 436, 630, 341, 436, 787, 1143, 538, 11899], "temperature": 0.0, "avg_logprob": -0.17912285668509348, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.785057626577327e-06}, {"id": 514, "seek": 245248, "start": 2457.56, "end": 2462.16, "text": " And I assume that's because they I looked at their code, and it was kind of pretty slow and ugly", "tokens": [400, 286, 6552, 300, 311, 570, 436, 286, 2956, 412, 641, 3089, 11, 293, 309, 390, 733, 295, 1238, 2964, 293, 12246], "temperature": 0.0, "avg_logprob": -0.17912285668509348, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.785057626577327e-06}, {"id": 515, "seek": 245248, "start": 2463.2, "end": 2468.3, "text": " You know I figured out a way to optimize it a lot more as you saw and so we were able to use", "tokens": [509, 458, 286, 8932, 484, 257, 636, 281, 19719, 309, 257, 688, 544, 382, 291, 1866, 293, 370, 321, 645, 1075, 281, 764], "temperature": 0.0, "avg_logprob": -0.17912285668509348, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.785057626577327e-06}, {"id": 516, "seek": 245248, "start": 2469.84, "end": 2473.84, "text": " Here trigrams, and so we get quite a lot better, so we've got 91.8", "tokens": [1692, 504, 33737, 82, 11, 293, 370, 321, 483, 1596, 257, 688, 1101, 11, 370, 321, 600, 658, 31064, 13, 23], "temperature": 0.0, "avg_logprob": -0.17912285668509348, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.785057626577327e-06}, {"id": 517, "seek": 245248, "start": 2474.04, "end": 2476.84, "text": " This is a 91.2, but other than that it's identical", "tokens": [639, 307, 257, 31064, 13, 17, 11, 457, 661, 813, 300, 309, 311, 14800], "temperature": 0.0, "avg_logprob": -0.17912285668509348, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.785057626577327e-06}, {"id": 518, "seek": 247684, "start": 2476.84, "end": 2482.6000000000004, "text": " I'll also mean they used a support vector machine, which is almost identical to a", "tokens": [286, 603, 611, 914, 436, 1143, 257, 1406, 8062, 3479, 11, 597, 307, 1920, 14800, 281, 257], "temperature": 0.0, "avg_logprob": -0.17280130516992856, "compression_ratio": 1.5235602094240839, "no_speech_prob": 2.1907724203629186e-06}, {"id": 519, "seek": 247684, "start": 2483.2000000000003, "end": 2485.2000000000003, "text": " logistic regression in this case", "tokens": [3565, 3142, 24590, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.17280130516992856, "compression_ratio": 1.5235602094240839, "no_speech_prob": 2.1907724203629186e-06}, {"id": 520, "seek": 247684, "start": 2486.84, "end": 2494.52, "text": " So there's some minor differences right so I think that's a pretty cool result and you know I will mention", "tokens": [407, 456, 311, 512, 6696, 7300, 558, 370, 286, 519, 300, 311, 257, 1238, 1627, 1874, 293, 291, 458, 286, 486, 2152], "temperature": 0.0, "avg_logprob": -0.17280130516992856, "compression_ratio": 1.5235602094240839, "no_speech_prob": 2.1907724203629186e-06}, {"id": 521, "seek": 247684, "start": 2496.76, "end": 2501.46, "text": " You know what you get to see here in class is the result of like", "tokens": [509, 458, 437, 291, 483, 281, 536, 510, 294, 1508, 307, 264, 1874, 295, 411], "temperature": 0.0, "avg_logprob": -0.17280130516992856, "compression_ratio": 1.5235602094240839, "no_speech_prob": 2.1907724203629186e-06}, {"id": 522, "seek": 247684, "start": 2502.2400000000002, "end": 2503.32, "text": " many", "tokens": [867], "temperature": 0.0, "avg_logprob": -0.17280130516992856, "compression_ratio": 1.5235602094240839, "no_speech_prob": 2.1907724203629186e-06}, {"id": 523, "seek": 250332, "start": 2503.32, "end": 2509.94, "text": " Weeks and often many months of research that I do and so I don't want you to think like this stuff is obvious", "tokens": [12615, 82, 293, 2049, 867, 2493, 295, 2132, 300, 286, 360, 293, 370, 286, 500, 380, 528, 291, 281, 519, 411, 341, 1507, 307, 6322], "temperature": 0.0, "avg_logprob": -0.14388441217356715, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.9479970180545934e-06}, {"id": 524, "seek": 250332, "start": 2509.94, "end": 2511.94, "text": " It's not at all like reading this paper", "tokens": [467, 311, 406, 412, 439, 411, 3760, 341, 3035], "temperature": 0.0, "avg_logprob": -0.14388441217356715, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.9479970180545934e-06}, {"id": 525, "seek": 250332, "start": 2513.2000000000003, "end": 2515.8, "text": " There's no description in the paper of like", "tokens": [821, 311, 572, 3855, 294, 264, 3035, 295, 411], "temperature": 0.0, "avg_logprob": -0.14388441217356715, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.9479970180545934e-06}, {"id": 526, "seek": 250332, "start": 2516.4, "end": 2520.88, "text": " Why they use this model how it's different why they thought it works?", "tokens": [1545, 436, 764, 341, 2316, 577, 309, 311, 819, 983, 436, 1194, 309, 1985, 30], "temperature": 0.0, "avg_logprob": -0.14388441217356715, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.9479970180545934e-06}, {"id": 527, "seek": 250332, "start": 2520.96, "end": 2524.2400000000002, "text": " You know it took me a week or two to even realize", "tokens": [509, 458, 309, 1890, 385, 257, 1243, 420, 732, 281, 754, 4325], "temperature": 0.0, "avg_logprob": -0.14388441217356715, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.9479970180545934e-06}, {"id": 528, "seek": 250332, "start": 2524.76, "end": 2528.4, "text": " That it's kind of like mathematically equivalent to a normal logistic regression", "tokens": [663, 309, 311, 733, 295, 411, 44003, 10344, 281, 257, 2710, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.14388441217356715, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.9479970180545934e-06}, {"id": 529, "seek": 252840, "start": 2528.4, "end": 2533.4, "text": " And then a few more weeks to realize that the difference is actually in the regularization", "tokens": [400, 550, 257, 1326, 544, 3259, 281, 4325, 300, 264, 2649, 307, 767, 294, 264, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.17143875977088666, "compression_ratio": 1.7137931034482758, "no_speech_prob": 6.438961008825572e-06}, {"id": 530, "seek": 252840, "start": 2534.6, "end": 2536.6, "text": " You know like this is kind of like", "tokens": [509, 458, 411, 341, 307, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.17143875977088666, "compression_ratio": 1.7137931034482758, "no_speech_prob": 6.438961008825572e-06}, {"id": 531, "seek": 252840, "start": 2537.52, "end": 2544.12, "text": " Machine learning as I'm sure you've noticed from the Kaggle competitions you enter you know like you come up with a thousand good ideas", "tokens": [22155, 2539, 382, 286, 478, 988, 291, 600, 5694, 490, 264, 48751, 22631, 26185, 291, 3242, 291, 458, 411, 291, 808, 493, 365, 257, 4714, 665, 3487], "temperature": 0.0, "avg_logprob": -0.17143875977088666, "compression_ratio": 1.7137931034482758, "no_speech_prob": 6.438961008825572e-06}, {"id": 532, "seek": 252840, "start": 2545.48, "end": 2548.36, "text": " 999 of them no matter how confident you are they're going to be great", "tokens": [1722, 8494, 295, 552, 572, 1871, 577, 6679, 291, 366, 436, 434, 516, 281, 312, 869], "temperature": 0.0, "avg_logprob": -0.17143875977088666, "compression_ratio": 1.7137931034482758, "no_speech_prob": 6.438961008825572e-06}, {"id": 533, "seek": 252840, "start": 2548.36, "end": 2549.48, "text": " They always turn out to be shit", "tokens": [814, 1009, 1261, 484, 281, 312, 4611], "temperature": 0.0, "avg_logprob": -0.17143875977088666, "compression_ratio": 1.7137931034482758, "no_speech_prob": 6.438961008825572e-06}, {"id": 534, "seek": 252840, "start": 2549.48, "end": 2553.96, "text": " you know and then finally after four weeks one of them finally works and", "tokens": [291, 458, 293, 550, 2721, 934, 1451, 3259, 472, 295, 552, 2721, 1985, 293], "temperature": 0.0, "avg_logprob": -0.17143875977088666, "compression_ratio": 1.7137931034482758, "no_speech_prob": 6.438961008825572e-06}, {"id": 535, "seek": 252840, "start": 2554.56, "end": 2557.84, "text": " Kind of gives you the enthusiasm to spend another four weeks", "tokens": [9242, 295, 2709, 291, 264, 23417, 281, 3496, 1071, 1451, 3259], "temperature": 0.0, "avg_logprob": -0.17143875977088666, "compression_ratio": 1.7137931034482758, "no_speech_prob": 6.438961008825572e-06}, {"id": 536, "seek": 255784, "start": 2557.84, "end": 2559.08, "text": " of", "tokens": [295], "temperature": 0.0, "avg_logprob": -0.2134109690219541, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.4593686046282528e-06}, {"id": 537, "seek": 255784, "start": 2559.08, "end": 2561.08, "text": " misery and frustration", "tokens": [32309, 293, 20491], "temperature": 0.0, "avg_logprob": -0.2134109690219541, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.4593686046282528e-06}, {"id": 538, "seek": 255784, "start": 2561.48, "end": 2563.84, "text": " This is the norm right and and like", "tokens": [639, 307, 264, 2026, 558, 293, 293, 411], "temperature": 0.0, "avg_logprob": -0.2134109690219541, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.4593686046282528e-06}, {"id": 539, "seek": 255784, "start": 2565.28, "end": 2567.58, "text": " For sure the the best", "tokens": [1171, 988, 264, 264, 1151], "temperature": 0.0, "avg_logprob": -0.2134109690219541, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.4593686046282528e-06}, {"id": 540, "seek": 255784, "start": 2569.52, "end": 2575.52, "text": " Practition as I know in machine learning all share one particular trait in common which is they're very very tenacious", "tokens": [19170, 849, 382, 286, 458, 294, 3479, 2539, 439, 2073, 472, 1729, 22538, 294, 2689, 597, 307, 436, 434, 588, 588, 2064, 22641], "temperature": 0.0, "avg_logprob": -0.2134109690219541, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.4593686046282528e-06}, {"id": 541, "seek": 255784, "start": 2576.1600000000003, "end": 2582.36, "text": " You know also known as stubborn and bloody-minded right which is definitely a reputation. I seem to have", "tokens": [509, 458, 611, 2570, 382, 24137, 293, 18938, 12, 23310, 558, 597, 307, 2138, 257, 13061, 13, 286, 1643, 281, 362], "temperature": 0.0, "avg_logprob": -0.2134109690219541, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.4593686046282528e-06}, {"id": 542, "seek": 255784, "start": 2583.4, "end": 2585.4, "text": " probably fair", "tokens": [1391, 3143], "temperature": 0.0, "avg_logprob": -0.2134109690219541, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.4593686046282528e-06}, {"id": 543, "seek": 258540, "start": 2585.4, "end": 2592.62, "text": " Along with another thing which is that they're all very good coders. You know they're very good at turning their ideas into intercode", "tokens": [17457, 365, 1071, 551, 597, 307, 300, 436, 434, 439, 588, 665, 17656, 433, 13, 509, 458, 436, 434, 588, 665, 412, 6246, 641, 3487, 666, 728, 22332], "temperature": 0.0, "avg_logprob": -0.14303746904645648, "compression_ratio": 1.7552742616033756, "no_speech_prob": 2.601586402306566e-06}, {"id": 544, "seek": 258540, "start": 2594.08, "end": 2596.0, "text": " So yeah", "tokens": [407, 1338], "temperature": 0.0, "avg_logprob": -0.14303746904645648, "compression_ratio": 1.7552742616033756, "no_speech_prob": 2.601586402306566e-06}, {"id": 545, "seek": 258540, "start": 2596.0, "end": 2602.52, "text": " So you know this was like a really interesting experience for me working through this a few months ago to try and like figure out", "tokens": [407, 291, 458, 341, 390, 411, 257, 534, 1880, 1752, 337, 385, 1364, 807, 341, 257, 1326, 2493, 2057, 281, 853, 293, 411, 2573, 484], "temperature": 0.0, "avg_logprob": -0.14303746904645648, "compression_ratio": 1.7552742616033756, "no_speech_prob": 2.601586402306566e-06}, {"id": 546, "seek": 258540, "start": 2602.52, "end": 2604.52, "text": " how to how to at least", "tokens": [577, 281, 577, 281, 412, 1935], "temperature": 0.0, "avg_logprob": -0.14303746904645648, "compression_ratio": 1.7552742616033756, "no_speech_prob": 2.601586402306566e-06}, {"id": 547, "seek": 258540, "start": 2605.32, "end": 2610.76, "text": " You know how to explain why this at the at the time kind of state-of-the-art result exists", "tokens": [509, 458, 577, 281, 2903, 983, 341, 412, 264, 412, 264, 565, 733, 295, 1785, 12, 2670, 12, 3322, 12, 446, 1874, 8198], "temperature": 0.0, "avg_logprob": -0.14303746904645648, "compression_ratio": 1.7552742616033756, "no_speech_prob": 2.601586402306566e-06}, {"id": 548, "seek": 258540, "start": 2610.76, "end": 2612.96, "text": " And so once I figured that out", "tokens": [400, 370, 1564, 286, 8932, 300, 484], "temperature": 0.0, "avg_logprob": -0.14303746904645648, "compression_ratio": 1.7552742616033756, "no_speech_prob": 2.601586402306566e-06}, {"id": 549, "seek": 261296, "start": 2612.96, "end": 2617.12, "text": " I was actually able to build on top of it and make it quite a bit better", "tokens": [286, 390, 767, 1075, 281, 1322, 322, 1192, 295, 309, 293, 652, 309, 1596, 257, 857, 1101], "temperature": 0.0, "avg_logprob": -0.19645539509881402, "compression_ratio": 1.5874439461883407, "no_speech_prob": 6.04884098720504e-06}, {"id": 550, "seek": 261296, "start": 2617.92, "end": 2622.52, "text": " And I'll show you what I did and this is where it was very very handy to have", "tokens": [400, 286, 603, 855, 291, 437, 286, 630, 293, 341, 307, 689, 309, 390, 588, 588, 13239, 281, 362], "temperature": 0.0, "avg_logprob": -0.19645539509881402, "compression_ratio": 1.5874439461883407, "no_speech_prob": 6.04884098720504e-06}, {"id": 551, "seek": 261296, "start": 2623.28, "end": 2625.28, "text": " PyTorch at my disposal", "tokens": [9953, 51, 284, 339, 412, 452, 26400], "temperature": 0.0, "avg_logprob": -0.19645539509881402, "compression_ratio": 1.5874439461883407, "no_speech_prob": 6.04884098720504e-06}, {"id": 552, "seek": 261296, "start": 2626.36, "end": 2628.76, "text": " Because I was able to kind of create something that was", "tokens": [1436, 286, 390, 1075, 281, 733, 295, 1884, 746, 300, 390], "temperature": 0.0, "avg_logprob": -0.19645539509881402, "compression_ratio": 1.5874439461883407, "no_speech_prob": 6.04884098720504e-06}, {"id": 553, "seek": 261296, "start": 2629.4, "end": 2634.7200000000003, "text": " Customized just the way that I wanted to be and also very fast by using the GPU", "tokens": [16649, 1602, 445, 264, 636, 300, 286, 1415, 281, 312, 293, 611, 588, 2370, 538, 1228, 264, 18407], "temperature": 0.0, "avg_logprob": -0.19645539509881402, "compression_ratio": 1.5874439461883407, "no_speech_prob": 6.04884098720504e-06}, {"id": 554, "seek": 261296, "start": 2636.12, "end": 2639.6, "text": " So here's the kind of fast AI version of the", "tokens": [407, 510, 311, 264, 733, 295, 2370, 7318, 3037, 295, 264], "temperature": 0.0, "avg_logprob": -0.19645539509881402, "compression_ratio": 1.5874439461883407, "no_speech_prob": 6.04884098720504e-06}, {"id": 555, "seek": 263960, "start": 2639.6, "end": 2644.48, "text": " NBS VM actually my friend Stephen Meridy who's a terrific", "tokens": [426, 8176, 18038, 767, 452, 1277, 13391, 6124, 38836, 567, 311, 257, 20899], "temperature": 0.0, "avg_logprob": -0.20707542419433594, "compression_ratio": 1.5485232067510548, "no_speech_prob": 8.013343176571652e-06}, {"id": 556, "seek": 263960, "start": 2646.08, "end": 2652.04, "text": " Researcher in NLP has christened this the NBS VM plus plus which I thought was lovely", "tokens": [10303, 260, 294, 426, 45196, 575, 417, 81, 4821, 292, 341, 264, 426, 8176, 18038, 1804, 1804, 597, 286, 1194, 390, 7496], "temperature": 0.0, "avg_logprob": -0.20707542419433594, "compression_ratio": 1.5485232067510548, "no_speech_prob": 8.013343176571652e-06}, {"id": 557, "seek": 263960, "start": 2652.04, "end": 2657.6, "text": " So here is that even though there is no SVM. It's the logistic regression, but as I said nearly exactly the same thing", "tokens": [407, 510, 307, 300, 754, 1673, 456, 307, 572, 31910, 44, 13, 467, 311, 264, 3565, 3142, 24590, 11, 457, 382, 286, 848, 6217, 2293, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.20707542419433594, "compression_ratio": 1.5485232067510548, "no_speech_prob": 8.013343176571652e-06}, {"id": 558, "seek": 263960, "start": 2659.3199999999997, "end": 2661.72, "text": " So let me first of all show you like the code", "tokens": [407, 718, 385, 700, 295, 439, 855, 291, 411, 264, 3089], "temperature": 0.0, "avg_logprob": -0.20707542419433594, "compression_ratio": 1.5485232067510548, "no_speech_prob": 8.013343176571652e-06}, {"id": 559, "seek": 263960, "start": 2662.24, "end": 2665.2, "text": " So this is like we try to like once I figure out like okay", "tokens": [407, 341, 307, 411, 321, 853, 281, 411, 1564, 286, 2573, 484, 411, 1392], "temperature": 0.0, "avg_logprob": -0.20707542419433594, "compression_ratio": 1.5485232067510548, "no_speech_prob": 8.013343176571652e-06}, {"id": 560, "seek": 266520, "start": 2665.2, "end": 2670.16, "text": " This is like the best way I can come up with to do a linear bag of words model. I kind of embed it into", "tokens": [639, 307, 411, 264, 1151, 636, 286, 393, 808, 493, 365, 281, 360, 257, 8213, 3411, 295, 2283, 2316, 13, 286, 733, 295, 12240, 309, 666], "temperature": 0.0, "avg_logprob": -0.16831948622217718, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.963795057235984e-06}, {"id": 561, "seek": 266520, "start": 2670.16, "end": 2673.7599999999998, "text": " Fast AI so you can just write a couple lines of code so the code is basically hey", "tokens": [15968, 7318, 370, 291, 393, 445, 2464, 257, 1916, 3876, 295, 3089, 370, 264, 3089, 307, 1936, 4177], "temperature": 0.0, "avg_logprob": -0.16831948622217718, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.963795057235984e-06}, {"id": 562, "seek": 266520, "start": 2673.7599999999998, "end": 2680.12, "text": " I've got to create a data class for text classification. I want to create it from a bag of words", "tokens": [286, 600, 658, 281, 1884, 257, 1412, 1508, 337, 2487, 21538, 13, 286, 528, 281, 1884, 309, 490, 257, 3411, 295, 2283], "temperature": 0.0, "avg_logprob": -0.16831948622217718, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.963795057235984e-06}, {"id": 563, "seek": 266520, "start": 2680.7999999999997, "end": 2682.7999999999997, "text": " Right here is my bag of words", "tokens": [1779, 510, 307, 452, 3411, 295, 2283], "temperature": 0.0, "avg_logprob": -0.16831948622217718, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.963795057235984e-06}, {"id": 564, "seek": 266520, "start": 2683.3199999999997, "end": 2685.04, "text": " Here are my labels", "tokens": [1692, 366, 452, 16949], "temperature": 0.0, "avg_logprob": -0.16831948622217718, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.963795057235984e-06}, {"id": 565, "seek": 266520, "start": 2685.04, "end": 2688.12, "text": " Here is the same thing for the validation set", "tokens": [1692, 307, 264, 912, 551, 337, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.16831948622217718, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.963795057235984e-06}, {"id": 566, "seek": 266520, "start": 2688.7999999999997, "end": 2690.7999999999997, "text": " and use up to", "tokens": [293, 764, 493, 281], "temperature": 0.0, "avg_logprob": -0.16831948622217718, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.963795057235984e-06}, {"id": 567, "seek": 269080, "start": 2690.8, "end": 2696.44, "text": " 2,000 unique words per review which is plenty", "tokens": [568, 11, 1360, 3845, 2283, 680, 3131, 597, 307, 7140], "temperature": 0.0, "avg_logprob": -0.2511898489559398, "compression_ratio": 1.509090909090909, "no_speech_prob": 4.029427600471536e-06}, {"id": 568, "seek": 269080, "start": 2699.36, "end": 2701.76, "text": " So then from that model data", "tokens": [407, 550, 490, 300, 2316, 1412], "temperature": 0.0, "avg_logprob": -0.2511898489559398, "compression_ratio": 1.509090909090909, "no_speech_prob": 4.029427600471536e-06}, {"id": 569, "seek": 269080, "start": 2702.5600000000004, "end": 2707.36, "text": " Construct a learner which is kind of the fast AI generalization of a model", "tokens": [8574, 1757, 257, 33347, 597, 307, 733, 295, 264, 2370, 7318, 2674, 2144, 295, 257, 2316], "temperature": 0.0, "avg_logprob": -0.2511898489559398, "compression_ratio": 1.509090909090909, "no_speech_prob": 4.029427600471536e-06}, {"id": 570, "seek": 269080, "start": 2707.92, "end": 2713.4, "text": " Which is based on a dot product of naive Bayes and then fit that model", "tokens": [3013, 307, 2361, 322, 257, 5893, 1674, 295, 29052, 7840, 279, 293, 550, 3318, 300, 2316], "temperature": 0.0, "avg_logprob": -0.2511898489559398, "compression_ratio": 1.509090909090909, "no_speech_prob": 4.029427600471536e-06}, {"id": 571, "seek": 269080, "start": 2714.7200000000003, "end": 2717.44, "text": " and then do a few epochs and", "tokens": [293, 550, 360, 257, 1326, 30992, 28346, 293], "temperature": 0.0, "avg_logprob": -0.2511898489559398, "compression_ratio": 1.509090909090909, "no_speech_prob": 4.029427600471536e-06}, {"id": 572, "seek": 271744, "start": 2717.44, "end": 2720.28, "text": " After five epochs. I was already up to", "tokens": [2381, 1732, 30992, 28346, 13, 286, 390, 1217, 493, 281], "temperature": 0.0, "avg_logprob": -0.2305767816655776, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.4144706028673681e-06}, {"id": 573, "seek": 271744, "start": 2721.36, "end": 2726.68, "text": " 92.2 right so this is now like you know getting quite well above", "tokens": [28225, 13, 17, 558, 370, 341, 307, 586, 411, 291, 458, 1242, 1596, 731, 3673], "temperature": 0.0, "avg_logprob": -0.2305767816655776, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.4144706028673681e-06}, {"id": 574, "seek": 271744, "start": 2727.28, "end": 2729.28, "text": " This this linear baseline", "tokens": [639, 341, 8213, 20518], "temperature": 0.0, "avg_logprob": -0.2305767816655776, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.4144706028673681e-06}, {"id": 575, "seek": 271744, "start": 2730.64, "end": 2734.2000000000003, "text": " So let me show you the code for for that", "tokens": [407, 718, 385, 855, 291, 264, 3089, 337, 337, 300], "temperature": 0.0, "avg_logprob": -0.2305767816655776, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.4144706028673681e-06}, {"id": 576, "seek": 271744, "start": 2740.6, "end": 2742.6, "text": " So the code is like", "tokens": [407, 264, 3089, 307, 411], "temperature": 0.0, "avg_logprob": -0.2305767816655776, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.4144706028673681e-06}, {"id": 577, "seek": 271744, "start": 2743.04, "end": 2744.6, "text": " Horrifyingly short", "tokens": [10691, 81, 5489, 356, 2099], "temperature": 0.0, "avg_logprob": -0.2305767816655776, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.4144706028673681e-06}, {"id": 578, "seek": 271744, "start": 2744.6, "end": 2745.92, "text": " That's it", "tokens": [663, 311, 309], "temperature": 0.0, "avg_logprob": -0.2305767816655776, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.4144706028673681e-06}, {"id": 579, "seek": 274592, "start": 2745.92, "end": 2748.7400000000002, "text": " Right and it'll also look on the whole", "tokens": [1779, 293, 309, 603, 611, 574, 322, 264, 1379], "temperature": 0.0, "avg_logprob": -0.18677274804366262, "compression_ratio": 1.8294573643410852, "no_speech_prob": 3.611955435189884e-06}, {"id": 580, "seek": 274592, "start": 2749.76, "end": 2752.82, "text": " Extremely familiar right there's if there's a few tweaks here", "tokens": [24921, 736, 4963, 558, 456, 311, 498, 456, 311, 257, 1326, 46664, 510], "temperature": 0.0, "avg_logprob": -0.18677274804366262, "compression_ratio": 1.8294573643410852, "no_speech_prob": 3.611955435189884e-06}, {"id": 581, "seek": 274592, "start": 2753.52, "end": 2757.44, "text": " Pretend this thing that says embedding pretend. It actually says linear, okay?", "tokens": [9739, 521, 341, 551, 300, 1619, 12240, 3584, 11865, 13, 467, 767, 1619, 8213, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18677274804366262, "compression_ratio": 1.8294573643410852, "no_speech_prob": 3.611955435189884e-06}, {"id": 582, "seek": 274592, "start": 2757.44, "end": 2759.52, "text": " I'm going to show you embedding in a moment pretend. It says linear", "tokens": [286, 478, 516, 281, 855, 291, 12240, 3584, 294, 257, 1623, 11865, 13, 467, 1619, 8213], "temperature": 0.0, "avg_logprob": -0.18677274804366262, "compression_ratio": 1.8294573643410852, "no_speech_prob": 3.611955435189884e-06}, {"id": 583, "seek": 274592, "start": 2759.52, "end": 2766.56, "text": " So we've got basically a linear layer where the number of features coming with the number of features as the rows and remember", "tokens": [407, 321, 600, 658, 1936, 257, 8213, 4583, 689, 264, 1230, 295, 4122, 1348, 365, 264, 1230, 295, 4122, 382, 264, 13241, 293, 1604], "temperature": 0.0, "avg_logprob": -0.18677274804366262, "compression_ratio": 1.8294573643410852, "no_speech_prob": 3.611955435189884e-06}, {"id": 584, "seek": 274592, "start": 2766.92, "end": 2772.8, "text": " SK learn features means number of words basically and then for each row we're going to create", "tokens": [21483, 1466, 4122, 1355, 1230, 295, 2283, 1936, 293, 550, 337, 1184, 5386, 321, 434, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.18677274804366262, "compression_ratio": 1.8294573643410852, "no_speech_prob": 3.611955435189884e-06}, {"id": 585, "seek": 274592, "start": 2773.48, "end": 2774.6, "text": " one", "tokens": [472], "temperature": 0.0, "avg_logprob": -0.18677274804366262, "compression_ratio": 1.8294573643410852, "no_speech_prob": 3.611955435189884e-06}, {"id": 586, "seek": 277460, "start": 2774.6, "end": 2778.7599999999998, "text": " Weight which makes sense right for like a logistic regression every every", "tokens": [44464, 597, 1669, 2020, 558, 337, 411, 257, 3565, 3142, 24590, 633, 633], "temperature": 0.0, "avg_logprob": -0.21766572470193382, "compression_ratio": 1.6946902654867257, "no_speech_prob": 4.425457063916838e-06}, {"id": 587, "seek": 277460, "start": 2779.4, "end": 2782.64, "text": " So not for each row for each word each word has one weight", "tokens": [407, 406, 337, 1184, 5386, 337, 1184, 1349, 1184, 1349, 575, 472, 3364], "temperature": 0.0, "avg_logprob": -0.21766572470193382, "compression_ratio": 1.6946902654867257, "no_speech_prob": 4.425457063916838e-06}, {"id": 588, "seek": 277460, "start": 2783.64, "end": 2785.12, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.21766572470193382, "compression_ratio": 1.6946902654867257, "no_speech_prob": 4.425457063916838e-06}, {"id": 589, "seek": 277460, "start": 2785.12, "end": 2791.48, "text": " Then we're going to be multiplying it by the the R values so for each word", "tokens": [1396, 321, 434, 516, 281, 312, 30955, 309, 538, 264, 264, 497, 4190, 370, 337, 1184, 1349], "temperature": 0.0, "avg_logprob": -0.21766572470193382, "compression_ratio": 1.6946902654867257, "no_speech_prob": 4.425457063916838e-06}, {"id": 590, "seek": 277460, "start": 2792.44, "end": 2798.0, "text": " We have one R value per class so I actually made this so this can handle like not just", "tokens": [492, 362, 472, 497, 2158, 680, 1508, 370, 286, 767, 1027, 341, 370, 341, 393, 4813, 411, 406, 445], "temperature": 0.0, "avg_logprob": -0.21766572470193382, "compression_ratio": 1.6946902654867257, "no_speech_prob": 4.425457063916838e-06}, {"id": 591, "seek": 277460, "start": 2798.52, "end": 2802.94, "text": " Positive versus negative, but maybe figuring out like which author created this work", "tokens": [46326, 5717, 3671, 11, 457, 1310, 15213, 484, 411, 597, 3793, 2942, 341, 589], "temperature": 0.0, "avg_logprob": -0.21766572470193382, "compression_ratio": 1.6946902654867257, "no_speech_prob": 4.425457063916838e-06}, {"id": 592, "seek": 280294, "start": 2802.94, "end": 2807.04, "text": " There could be five or six authors whatever right and basically", "tokens": [821, 727, 312, 1732, 420, 2309, 16552, 2035, 558, 293, 1936], "temperature": 0.0, "avg_logprob": -0.16720734702216256, "compression_ratio": 1.7121212121212122, "no_speech_prob": 1.0676994861569256e-06}, {"id": 593, "seek": 280294, "start": 2807.7200000000003, "end": 2809.7200000000003, "text": " We kind of use those linear layers", "tokens": [492, 733, 295, 764, 729, 8213, 7914], "temperature": 0.0, "avg_logprob": -0.16720734702216256, "compression_ratio": 1.7121212121212122, "no_speech_prob": 1.0676994861569256e-06}, {"id": 594, "seek": 280294, "start": 2811.12, "end": 2813.12, "text": " to to get the", "tokens": [281, 281, 483, 264], "temperature": 0.0, "avg_logprob": -0.16720734702216256, "compression_ratio": 1.7121212121212122, "no_speech_prob": 1.0676994861569256e-06}, {"id": 595, "seek": 280294, "start": 2813.44, "end": 2818.52, "text": " The value of the weight and the value of the R and then we take the weight", "tokens": [440, 2158, 295, 264, 3364, 293, 264, 2158, 295, 264, 497, 293, 550, 321, 747, 264, 3364], "temperature": 0.0, "avg_logprob": -0.16720734702216256, "compression_ratio": 1.7121212121212122, "no_speech_prob": 1.0676994861569256e-06}, {"id": 596, "seek": 280294, "start": 2819.8, "end": 2821.8, "text": " Times the R and", "tokens": [11366, 264, 497, 293], "temperature": 0.0, "avg_logprob": -0.16720734702216256, "compression_ratio": 1.7121212121212122, "no_speech_prob": 1.0676994861569256e-06}, {"id": 597, "seek": 280294, "start": 2822.16, "end": 2824.92, "text": " Then sum it up and so that's just a dot product", "tokens": [1396, 2408, 309, 493, 293, 370, 300, 311, 445, 257, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.16720734702216256, "compression_ratio": 1.7121212121212122, "no_speech_prob": 1.0676994861569256e-06}, {"id": 598, "seek": 280294, "start": 2825.52, "end": 2831.04, "text": " Okay, so just just a simple dot product just as we would do for any logistic regression", "tokens": [1033, 11, 370, 445, 445, 257, 2199, 5893, 1674, 445, 382, 321, 576, 360, 337, 604, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.16720734702216256, "compression_ratio": 1.7121212121212122, "no_speech_prob": 1.0676994861569256e-06}, {"id": 599, "seek": 283104, "start": 2831.04, "end": 2833.04, "text": " And then do the softmax", "tokens": [400, 550, 360, 264, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.2267897605895996, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.5215613277396187e-06}, {"id": 600, "seek": 283104, "start": 2834.92, "end": 2838.2, "text": " So the very minor tweak", "tokens": [407, 264, 588, 6696, 29879], "temperature": 0.0, "avg_logprob": -0.2267897605895996, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.5215613277396187e-06}, {"id": 601, "seek": 283104, "start": 2840.2799999999997, "end": 2846.8, "text": " That we add to get the better result is this the main one really is this here this plus something", "tokens": [663, 321, 909, 281, 483, 264, 1101, 1874, 307, 341, 264, 2135, 472, 534, 307, 341, 510, 341, 1804, 746], "temperature": 0.0, "avg_logprob": -0.2267897605895996, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.5215613277396187e-06}, {"id": 602, "seek": 283104, "start": 2847.2, "end": 2849.2, "text": " All right, and the thing I'm adding is", "tokens": [1057, 558, 11, 293, 264, 551, 286, 478, 5127, 307], "temperature": 0.0, "avg_logprob": -0.2267897605895996, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.5215613277396187e-06}, {"id": 603, "seek": 283104, "start": 2850.08, "end": 2854.24, "text": " It's a parameter, but I pretty much always use this this version this value 0.4", "tokens": [467, 311, 257, 13075, 11, 457, 286, 1238, 709, 1009, 764, 341, 341, 3037, 341, 2158, 1958, 13, 19], "temperature": 0.0, "avg_logprob": -0.2267897605895996, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.5215613277396187e-06}, {"id": 604, "seek": 283104, "start": 2854.96, "end": 2856.96, "text": " So what does this do?", "tokens": [407, 437, 775, 341, 360, 30], "temperature": 0.0, "avg_logprob": -0.2267897605895996, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.5215613277396187e-06}, {"id": 605, "seek": 285696, "start": 2856.96, "end": 2864.62, "text": " So what this is doing is it's again kind of changing the prior right so if you think about it", "tokens": [407, 437, 341, 307, 884, 307, 309, 311, 797, 733, 295, 4473, 264, 4059, 558, 370, 498, 291, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.19917445863996233, "compression_ratio": 1.507462686567164, "no_speech_prob": 7.811473210495024e-07}, {"id": 606, "seek": 285696, "start": 2868.68, "end": 2875.08, "text": " Even once we use this R times the term document matrix as their independent variables", "tokens": [2754, 1564, 321, 764, 341, 497, 1413, 264, 1433, 4166, 8141, 382, 641, 6695, 9102], "temperature": 0.0, "avg_logprob": -0.19917445863996233, "compression_ratio": 1.507462686567164, "no_speech_prob": 7.811473210495024e-07}, {"id": 607, "seek": 285696, "start": 2876.48, "end": 2884.2, "text": " You really want to start with a question okay the penalty terms are still pushing w down to zero right so what did it mean?", "tokens": [509, 534, 528, 281, 722, 365, 257, 1168, 1392, 264, 16263, 2115, 366, 920, 7380, 261, 760, 281, 4018, 558, 370, 437, 630, 309, 914, 30], "temperature": 0.0, "avg_logprob": -0.19917445863996233, "compression_ratio": 1.507462686567164, "no_speech_prob": 7.811473210495024e-07}, {"id": 608, "seek": 288420, "start": 2884.2, "end": 2890.48, "text": " for w to be zero right so what would it mean if we had you know", "tokens": [337, 261, 281, 312, 4018, 558, 370, 437, 576, 309, 914, 498, 321, 632, 291, 458], "temperature": 0.0, "avg_logprob": -0.21556385861167424, "compression_ratio": 1.7119565217391304, "no_speech_prob": 1.7603391597731388e-06}, {"id": 609, "seek": 288420, "start": 2892.48, "end": 2894.48, "text": " Coefficient zero zero zero zero zero", "tokens": [3066, 68, 7816, 4018, 4018, 4018, 4018, 4018], "temperature": 0.0, "avg_logprob": -0.21556385861167424, "compression_ratio": 1.7119565217391304, "no_speech_prob": 1.7603391597731388e-06}, {"id": 610, "seek": 288420, "start": 2895.8399999999997, "end": 2902.4399999999996, "text": " All right, so what that would do when we go okay this matrix times these coefficients", "tokens": [1057, 558, 11, 370, 437, 300, 576, 360, 562, 321, 352, 1392, 341, 8141, 1413, 613, 31994], "temperature": 0.0, "avg_logprob": -0.21556385861167424, "compression_ratio": 1.7119565217391304, "no_speech_prob": 1.7603391597731388e-06}, {"id": 611, "seek": 288420, "start": 2902.9199999999996, "end": 2907.8199999999997, "text": " We still get zero right so a weight of zero still ends up saying", "tokens": [492, 920, 483, 4018, 558, 370, 257, 3364, 295, 4018, 920, 5314, 493, 1566], "temperature": 0.0, "avg_logprob": -0.21556385861167424, "compression_ratio": 1.7119565217391304, "no_speech_prob": 1.7603391597731388e-06}, {"id": 612, "seek": 288420, "start": 2907.8199999999997, "end": 2910.6, "text": " I have no opinion on whether this thing is positive or negative", "tokens": [286, 362, 572, 4800, 322, 1968, 341, 551, 307, 3353, 420, 3671], "temperature": 0.0, "avg_logprob": -0.21556385861167424, "compression_ratio": 1.7119565217391304, "no_speech_prob": 1.7603391597731388e-06}, {"id": 613, "seek": 291060, "start": 2910.6, "end": 2915.2, "text": " On the other hand if they were all one", "tokens": [1282, 264, 661, 1011, 498, 436, 645, 439, 472], "temperature": 0.0, "avg_logprob": -0.1860646834740272, "compression_ratio": 1.4055944055944056, "no_speech_prob": 1.2289158348721685e-06}, {"id": 614, "seek": 291060, "start": 2918.08, "end": 2925.52, "text": " Right then it's basically says my opinion is that the naive Bayes coefficients are exactly right", "tokens": [1779, 550, 309, 311, 1936, 1619, 452, 4800, 307, 300, 264, 29052, 7840, 279, 31994, 366, 2293, 558], "temperature": 0.0, "avg_logprob": -0.1860646834740272, "compression_ratio": 1.4055944055944056, "no_speech_prob": 1.2289158348721685e-06}, {"id": 615, "seek": 291060, "start": 2926.6, "end": 2930.5, "text": " Okay, and so the idea is that I said", "tokens": [1033, 11, 293, 370, 264, 1558, 307, 300, 286, 848], "temperature": 0.0, "avg_logprob": -0.1860646834740272, "compression_ratio": 1.4055944055944056, "no_speech_prob": 1.2289158348721685e-06}, {"id": 616, "seek": 291060, "start": 2932.7999999999997, "end": 2935.0, "text": " Zero is almost certainly not", "tokens": [17182, 307, 1920, 3297, 406], "temperature": 0.0, "avg_logprob": -0.1860646834740272, "compression_ratio": 1.4055944055944056, "no_speech_prob": 1.2289158348721685e-06}, {"id": 617, "seek": 293500, "start": 2935.0, "end": 2942.36, "text": " The right prior right we shouldn't really be saying if there's no coefficient it means ignore the naive Bayes coefficient", "tokens": [440, 558, 4059, 558, 321, 4659, 380, 534, 312, 1566, 498, 456, 311, 572, 17619, 309, 1355, 11200, 264, 29052, 7840, 279, 17619], "temperature": 0.0, "avg_logprob": -0.1533976764213748, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.8448191667630454e-06}, {"id": 618, "seek": 293500, "start": 2943.44, "end": 2949.92, "text": " One is probably too high right because we actually think that naive Bayes is only kind of part of the answer", "tokens": [1485, 307, 1391, 886, 1090, 558, 570, 321, 767, 519, 300, 29052, 7840, 279, 307, 787, 733, 295, 644, 295, 264, 1867], "temperature": 0.0, "avg_logprob": -0.1533976764213748, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.8448191667630454e-06}, {"id": 619, "seek": 293500, "start": 2950.2, "end": 2955.04, "text": " Right and so I played around with a few different data sets where I basically said", "tokens": [1779, 293, 370, 286, 3737, 926, 365, 257, 1326, 819, 1412, 6352, 689, 286, 1936, 848], "temperature": 0.0, "avg_logprob": -0.1533976764213748, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.8448191667630454e-06}, {"id": 620, "seek": 293500, "start": 2956.64, "end": 2960.04, "text": " Take the weights and add to them", "tokens": [3664, 264, 17443, 293, 909, 281, 552], "temperature": 0.0, "avg_logprob": -0.1533976764213748, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.8448191667630454e-06}, {"id": 621, "seek": 293500, "start": 2961.96, "end": 2963.96, "text": " Some constant", "tokens": [2188, 5754], "temperature": 0.0, "avg_logprob": -0.1533976764213748, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.8448191667630454e-06}, {"id": 622, "seek": 296396, "start": 2963.96, "end": 2969.32, "text": " Right and so zero would become in this case zero point four", "tokens": [1779, 293, 370, 4018, 576, 1813, 294, 341, 1389, 4018, 935, 1451], "temperature": 0.0, "avg_logprob": -0.21422630310058594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0511470236451714e-06}, {"id": 623, "seek": 296396, "start": 2969.96, "end": 2972.28, "text": " right so in other words the", "tokens": [558, 370, 294, 661, 2283, 264], "temperature": 0.0, "avg_logprob": -0.21422630310058594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0511470236451714e-06}, {"id": 624, "seek": 296396, "start": 2973.0, "end": 2975.0, "text": " the regularization", "tokens": [264, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.21422630310058594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0511470236451714e-06}, {"id": 625, "seek": 296396, "start": 2975.6, "end": 2981.2, "text": " Penalty is pushing the weights not towards zero but towards this value", "tokens": [10571, 304, 874, 307, 7380, 264, 17443, 406, 3030, 4018, 457, 3030, 341, 2158], "temperature": 0.0, "avg_logprob": -0.21422630310058594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0511470236451714e-06}, {"id": 626, "seek": 296396, "start": 2981.68, "end": 2985.28, "text": " Right and I found that across a number of data sets zero point four", "tokens": [1779, 293, 286, 1352, 300, 2108, 257, 1230, 295, 1412, 6352, 4018, 935, 1451], "temperature": 0.0, "avg_logprob": -0.21422630310058594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0511470236451714e-06}, {"id": 627, "seek": 296396, "start": 2986.12, "end": 2990.64, "text": " Works pretty well right and it's pretty resilient right so again", "tokens": [27914, 1238, 731, 558, 293, 309, 311, 1238, 23699, 558, 370, 797], "temperature": 0.0, "avg_logprob": -0.21422630310058594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0511470236451714e-06}, {"id": 628, "seek": 299064, "start": 2990.64, "end": 2995.6, "text": " This is the basic idea is to kind of like get the best of both worlds", "tokens": [639, 307, 264, 3875, 1558, 307, 281, 733, 295, 411, 483, 264, 1151, 295, 1293, 13401], "temperature": 0.0, "avg_logprob": -0.16287354060581752, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.726454795265454e-06}, {"id": 629, "seek": 299064, "start": 2995.6, "end": 3000.24, "text": " You know we're we're we're learning from the data using a simple", "tokens": [509, 458, 321, 434, 321, 434, 321, 434, 2539, 490, 264, 1412, 1228, 257, 2199], "temperature": 0.0, "avg_logprob": -0.16287354060581752, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.726454795265454e-06}, {"id": 630, "seek": 299064, "start": 3000.8399999999997, "end": 3002.8399999999997, "text": " Model, but we're incorporating", "tokens": [17105, 11, 457, 321, 434, 33613], "temperature": 0.0, "avg_logprob": -0.16287354060581752, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.726454795265454e-06}, {"id": 631, "seek": 299064, "start": 3003.72, "end": 3010.16, "text": " You know our prior knowledge as best as we can and so it turns out when you say okay", "tokens": [509, 458, 527, 4059, 3601, 382, 1151, 382, 321, 393, 293, 370, 309, 4523, 484, 562, 291, 584, 1392], "temperature": 0.0, "avg_logprob": -0.16287354060581752, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.726454795265454e-06}, {"id": 632, "seek": 299064, "start": 3010.72, "end": 3014.7999999999997, "text": " Let's let's tell it. You know as weight matrix of zeros", "tokens": [961, 311, 718, 311, 980, 309, 13, 509, 458, 382, 3364, 8141, 295, 35193], "temperature": 0.0, "avg_logprob": -0.16287354060581752, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.726454795265454e-06}, {"id": 633, "seek": 301480, "start": 3014.8, "end": 3020.1600000000003, "text": " Actually means that you should use about you know about half of the R values", "tokens": [5135, 1355, 300, 291, 820, 764, 466, 291, 458, 466, 1922, 295, 264, 497, 4190], "temperature": 0.0, "avg_logprob": -0.2740266448573062, "compression_ratio": 1.652694610778443, "no_speech_prob": 3.1875363220024155e-06}, {"id": 634, "seek": 301480, "start": 3022.2000000000003, "end": 3028.6800000000003, "text": " That ends up that ends up working better than the prior that the weights should all be zero", "tokens": [663, 5314, 493, 300, 5314, 493, 1364, 1101, 813, 264, 4059, 300, 264, 17443, 820, 439, 312, 4018], "temperature": 0.0, "avg_logprob": -0.2740266448573062, "compression_ratio": 1.652694610778443, "no_speech_prob": 3.1875363220024155e-06}, {"id": 635, "seek": 301480, "start": 3030.2000000000003, "end": 3031.7200000000003, "text": " Yes", "tokens": [1079], "temperature": 0.0, "avg_logprob": -0.2740266448573062, "compression_ratio": 1.652694610778443, "no_speech_prob": 3.1875363220024155e-06}, {"id": 636, "seek": 301480, "start": 3031.7200000000003, "end": 3038.1200000000003, "text": " Is the the weights the W is it that the point for the amount of?", "tokens": [1119, 264, 264, 17443, 264, 343, 307, 309, 300, 264, 935, 337, 264, 2372, 295, 30], "temperature": 0.0, "avg_logprob": -0.2740266448573062, "compression_ratio": 1.652694610778443, "no_speech_prob": 3.1875363220024155e-06}, {"id": 637, "seek": 301480, "start": 3038.6800000000003, "end": 3039.92, "text": " required", "tokens": [4739], "temperature": 0.0, "avg_logprob": -0.2740266448573062, "compression_ratio": 1.652694610778443, "no_speech_prob": 3.1875363220024155e-06}, {"id": 638, "seek": 301480, "start": 3039.92, "end": 3041.04, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.2740266448573062, "compression_ratio": 1.652694610778443, "no_speech_prob": 3.1875363220024155e-06}, {"id": 639, "seek": 301480, "start": 3041.04, "end": 3042.1600000000003, "text": " amount of", "tokens": [2372, 295], "temperature": 0.0, "avg_logprob": -0.2740266448573062, "compression_ratio": 1.652694610778443, "no_speech_prob": 3.1875363220024155e-06}, {"id": 640, "seek": 301480, "start": 3042.1600000000003, "end": 3044.1600000000003, "text": " so we have this", "tokens": [370, 321, 362, 341], "temperature": 0.0, "avg_logprob": -0.2740266448573062, "compression_ratio": 1.652694610778443, "no_speech_prob": 3.1875363220024155e-06}, {"id": 641, "seek": 304416, "start": 3044.16, "end": 3052.08, "text": " Where the we have the term where we reduce the amount of error the prediction error RMSE plus we have the regularization and", "tokens": [2305, 264, 321, 362, 264, 1433, 689, 321, 5407, 264, 2372, 295, 6713, 264, 17630, 6713, 23790, 5879, 1804, 321, 362, 264, 3890, 2144, 293], "temperature": 0.0, "avg_logprob": -0.2511347055435181, "compression_ratio": 1.8085106382978724, "no_speech_prob": 1.1843012543977238e-05}, {"id": 642, "seek": 304416, "start": 3052.3999999999996, "end": 3057.52, "text": " Is a W the point for denote the amount of regularization required so W are the weights?", "tokens": [1119, 257, 343, 264, 935, 337, 45708, 264, 2372, 295, 3890, 2144, 4739, 370, 343, 366, 264, 17443, 30], "temperature": 0.0, "avg_logprob": -0.2511347055435181, "compression_ratio": 1.8085106382978724, "no_speech_prob": 1.1843012543977238e-05}, {"id": 643, "seek": 304416, "start": 3059.04, "end": 3066.08, "text": " Right so this is calculating our activations, okay, so we calculate our activations as being equal to the weights", "tokens": [1779, 370, 341, 307, 28258, 527, 2430, 763, 11, 1392, 11, 370, 321, 8873, 527, 2430, 763, 382, 885, 2681, 281, 264, 17443], "temperature": 0.0, "avg_logprob": -0.2511347055435181, "compression_ratio": 1.8085106382978724, "no_speech_prob": 1.1843012543977238e-05}, {"id": 644, "seek": 304416, "start": 3069.08, "end": 3071.08, "text": " Times the are", "tokens": [11366, 264, 366], "temperature": 0.0, "avg_logprob": -0.2511347055435181, "compression_ratio": 1.8085106382978724, "no_speech_prob": 1.1843012543977238e-05}, {"id": 645, "seek": 307108, "start": 3071.08, "end": 3075.7, "text": " Some right, so that's just our normal", "tokens": [2188, 558, 11, 370, 300, 311, 445, 527, 2710], "temperature": 0.0, "avg_logprob": -0.24203285011085304, "compression_ratio": 1.6625, "no_speech_prob": 5.422186859505018e-06}, {"id": 646, "seek": 307108, "start": 3078.56, "end": 3086.7599999999998, "text": " At our normal linear function right so so the the thing which is being penalized is", "tokens": [1711, 527, 2710, 8213, 2445, 558, 370, 370, 264, 264, 551, 597, 307, 885, 13661, 1602, 307], "temperature": 0.0, "avg_logprob": -0.24203285011085304, "compression_ratio": 1.6625, "no_speech_prob": 5.422186859505018e-06}, {"id": 647, "seek": 307108, "start": 3087.6, "end": 3089.6, "text": " my weight matrix", "tokens": [452, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.24203285011085304, "compression_ratio": 1.6625, "no_speech_prob": 5.422186859505018e-06}, {"id": 648, "seek": 307108, "start": 3089.64, "end": 3091.64, "text": " That's what gets penalized", "tokens": [663, 311, 437, 2170, 13661, 1602], "temperature": 0.0, "avg_logprob": -0.24203285011085304, "compression_ratio": 1.6625, "no_speech_prob": 5.422186859505018e-06}, {"id": 649, "seek": 307108, "start": 3091.7599999999998, "end": 3096.52, "text": " So by saying hey, you know what don't just use W use W plus point four", "tokens": [407, 538, 1566, 4177, 11, 291, 458, 437, 500, 380, 445, 764, 343, 764, 343, 1804, 935, 1451], "temperature": 0.0, "avg_logprob": -0.24203285011085304, "compression_ratio": 1.6625, "no_speech_prob": 5.422186859505018e-06}, {"id": 650, "seek": 307108, "start": 3097.2, "end": 3099.2, "text": " So that's not being penalized", "tokens": [407, 300, 311, 406, 885, 13661, 1602], "temperature": 0.0, "avg_logprob": -0.24203285011085304, "compression_ratio": 1.6625, "no_speech_prob": 5.422186859505018e-06}, {"id": 651, "seek": 309920, "start": 3099.2, "end": 3101.2, "text": " It's not part of the weight matrix", "tokens": [467, 311, 406, 644, 295, 264, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.29460346337520715, "compression_ratio": 1.5515151515151515, "no_speech_prob": 2.368786908846232e-06}, {"id": 652, "seek": 309920, "start": 3101.8799999999997, "end": 3104.9199999999996, "text": " Okay, so effectively the weight matrix gets", "tokens": [1033, 11, 370, 8659, 264, 3364, 8141, 2170], "temperature": 0.0, "avg_logprob": -0.29460346337520715, "compression_ratio": 1.5515151515151515, "no_speech_prob": 2.368786908846232e-06}, {"id": 653, "seek": 309920, "start": 3106.3599999999997, "end": 3108.3599999999997, "text": " 0.4 for free", "tokens": [1958, 13, 19, 337, 1737], "temperature": 0.0, "avg_logprob": -0.29460346337520715, "compression_ratio": 1.5515151515151515, "no_speech_prob": 2.368786908846232e-06}, {"id": 654, "seek": 309920, "start": 3110.8399999999997, "end": 3112.8399999999997, "text": " So by doing this", "tokens": [407, 538, 884, 341], "temperature": 0.0, "avg_logprob": -0.29460346337520715, "compression_ratio": 1.5515151515151515, "no_speech_prob": 2.368786908846232e-06}, {"id": 655, "seek": 309920, "start": 3113.52, "end": 3116.3599999999997, "text": " even after regularization then every", "tokens": [754, 934, 3890, 2144, 550, 633], "temperature": 0.0, "avg_logprob": -0.29460346337520715, "compression_ratio": 1.5515151515151515, "no_speech_prob": 2.368786908846232e-06}, {"id": 656, "seek": 309920, "start": 3117.3999999999996, "end": 3123.2799999999997, "text": " Observe it every feature is getting some form of fate right some form of minimum weight or something like that", "tokens": [20707, 3768, 309, 633, 4111, 307, 1242, 512, 1254, 295, 12738, 558, 512, 1254, 295, 7285, 3364, 420, 746, 411, 300], "temperature": 0.0, "avg_logprob": -0.29460346337520715, "compression_ratio": 1.5515151515151515, "no_speech_prob": 2.368786908846232e-06}, {"id": 657, "seek": 312328, "start": 3123.28, "end": 3130.84, "text": " I'm not necessarily because it could end up choosing a coefficient of negative point four for a feature and so that would say", "tokens": [286, 478, 406, 4725, 570, 309, 727, 917, 493, 10875, 257, 17619, 295, 3671, 935, 1451, 337, 257, 4111, 293, 370, 300, 576, 584], "temperature": 0.0, "avg_logprob": -0.17947017983214497, "compression_ratio": 1.473170731707317, "no_speech_prob": 1.4367418543770327e-06}, {"id": 658, "seek": 312328, "start": 3132.2400000000002, "end": 3139.0, "text": " You know what even though naive Bayes says it's the R should be whatever for this feature. I think you should totally ignore it", "tokens": [509, 458, 437, 754, 1673, 29052, 7840, 279, 1619, 309, 311, 264, 497, 820, 312, 2035, 337, 341, 4111, 13, 286, 519, 291, 820, 3879, 11200, 309], "temperature": 0.0, "avg_logprob": -0.17947017983214497, "compression_ratio": 1.473170731707317, "no_speech_prob": 1.4367418543770327e-06}, {"id": 659, "seek": 312328, "start": 3139.6000000000004, "end": 3141.6000000000004, "text": " Yeah, great questions", "tokens": [865, 11, 869, 1651], "temperature": 0.0, "avg_logprob": -0.17947017983214497, "compression_ratio": 1.473170731707317, "no_speech_prob": 1.4367418543770327e-06}, {"id": 660, "seek": 312328, "start": 3142.1600000000003, "end": 3144.1600000000003, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.17947017983214497, "compression_ratio": 1.473170731707317, "no_speech_prob": 1.4367418543770327e-06}, {"id": 661, "seek": 312328, "start": 3146.6800000000003, "end": 3149.44, "text": " We started at 20 past", "tokens": [492, 1409, 412, 945, 1791], "temperature": 0.0, "avg_logprob": -0.17947017983214497, "compression_ratio": 1.473170731707317, "no_speech_prob": 1.4367418543770327e-06}, {"id": 662, "seek": 314944, "start": 3149.44, "end": 3157.8, "text": " To okay, let's take a break for about eight minutes or so and start back about 25 to", "tokens": [1407, 1392, 11, 718, 311, 747, 257, 1821, 337, 466, 3180, 2077, 420, 370, 293, 722, 646, 466, 3552, 281], "temperature": 0.0, "avg_logprob": -0.2052253390115405, "compression_ratio": 1.3896103896103895, "no_speech_prob": 1.4738706340722274e-05}, {"id": 663, "seek": 314944, "start": 3164.6, "end": 3166.2000000000003, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2052253390115405, "compression_ratio": 1.3896103896103895, "no_speech_prob": 1.4738706340722274e-05}, {"id": 664, "seek": 314944, "start": 3166.2000000000003, "end": 3171.36, "text": " so a couple of questions at the break the first was just for a", "tokens": [370, 257, 1916, 295, 1651, 412, 264, 1821, 264, 700, 390, 445, 337, 257], "temperature": 0.0, "avg_logprob": -0.2052253390115405, "compression_ratio": 1.3896103896103895, "no_speech_prob": 1.4738706340722274e-05}, {"id": 665, "seek": 314944, "start": 3173.28, "end": 3175.32, "text": " Kind of", "tokens": [9242, 295], "temperature": 0.0, "avg_logprob": -0.2052253390115405, "compression_ratio": 1.3896103896103895, "no_speech_prob": 1.4738706340722274e-05}, {"id": 666, "seek": 317532, "start": 3175.32, "end": 3182.96, "text": " Reminder or a bit of a summary as to what's going on? Yeah, right and so here we have", "tokens": [4080, 5669, 420, 257, 857, 295, 257, 12691, 382, 281, 437, 311, 516, 322, 30, 865, 11, 558, 293, 370, 510, 321, 362], "temperature": 0.0, "avg_logprob": -0.2673674724141105, "compression_ratio": 1.3972602739726028, "no_speech_prob": 3.237740429540281e-06}, {"id": 667, "seek": 317532, "start": 3184.2400000000002, "end": 3186.2400000000002, "text": " W plus", "tokens": [343, 1804], "temperature": 0.0, "avg_logprob": -0.2673674724141105, "compression_ratio": 1.3972602739726028, "no_speech_prob": 3.237740429540281e-06}, {"id": 668, "seek": 317532, "start": 3189.0, "end": 3191.52, "text": " I'm writing it out. Yeah plus", "tokens": [286, 478, 3579, 309, 484, 13, 865, 1804], "temperature": 0.0, "avg_logprob": -0.2673674724141105, "compression_ratio": 1.3972602739726028, "no_speech_prob": 3.237740429540281e-06}, {"id": 669, "seek": 317532, "start": 3193.7200000000003, "end": 3195.7200000000003, "text": " Adjusted weight a weight adjustment", "tokens": [34049, 292, 3364, 257, 3364, 17132], "temperature": 0.0, "avg_logprob": -0.2673674724141105, "compression_ratio": 1.3972602739726028, "no_speech_prob": 3.237740429540281e-06}, {"id": 670, "seek": 317532, "start": 3196.56, "end": 3198.56, "text": " times", "tokens": [1413], "temperature": 0.0, "avg_logprob": -0.2673674724141105, "compression_ratio": 1.3972602739726028, "no_speech_prob": 3.237740429540281e-06}, {"id": 671, "seek": 319856, "start": 3198.56, "end": 3204.96, "text": " Ah right so so normally what we were doing so", "tokens": [2438, 558, 370, 370, 5646, 437, 321, 645, 884, 370], "temperature": 0.0, "avg_logprob": -0.25474967454609115, "compression_ratio": 1.6768292682926829, "no_speech_prob": 3.2377477054978954e-06}, {"id": 672, "seek": 319856, "start": 3206.48, "end": 3210.2, "text": " Normally what we were doing is saying hey logistic regression is basically", "tokens": [17424, 437, 321, 645, 884, 307, 1566, 4177, 3565, 3142, 24590, 307, 1936], "temperature": 0.0, "avg_logprob": -0.25474967454609115, "compression_ratio": 1.6768292682926829, "no_speech_prob": 3.2377477054978954e-06}, {"id": 673, "seek": 319856, "start": 3211.72, "end": 3217.48, "text": " Wx right I'm going to ignore the bias okay, and then we were changing it to be", "tokens": [343, 87, 558, 286, 478, 516, 281, 11200, 264, 12577, 1392, 11, 293, 550, 321, 645, 4473, 309, 281, 312], "temperature": 0.0, "avg_logprob": -0.25474967454609115, "compression_ratio": 1.6768292682926829, "no_speech_prob": 3.2377477054978954e-06}, {"id": 674, "seek": 319856, "start": 3218.48, "end": 3220.48, "text": " W dot", "tokens": [343, 5893], "temperature": 0.0, "avg_logprob": -0.25474967454609115, "compression_ratio": 1.6768292682926829, "no_speech_prob": 3.2377477054978954e-06}, {"id": 675, "seek": 319856, "start": 3220.92, "end": 3222.32, "text": " times", "tokens": [1413], "temperature": 0.0, "avg_logprob": -0.25474967454609115, "compression_ratio": 1.6768292682926829, "no_speech_prob": 3.2377477054978954e-06}, {"id": 676, "seek": 319856, "start": 3222.32, "end": 3223.64, "text": " X", "tokens": [1783], "temperature": 0.0, "avg_logprob": -0.25474967454609115, "compression_ratio": 1.6768292682926829, "no_speech_prob": 3.2377477054978954e-06}, {"id": 677, "seek": 322364, "start": 3223.64, "end": 3229.72, "text": " Right and then we were kind of saying let's do that bit first right", "tokens": [1779, 293, 550, 321, 645, 733, 295, 1566, 718, 311, 360, 300, 857, 700, 558], "temperature": 0.0, "avg_logprob": -0.1620874689586127, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.5612691842979984e-06}, {"id": 678, "seek": 322364, "start": 3232.72, "end": 3238.64, "text": " Although in this particular case actually now. I look at it. I'm doing it in this code. It doesn't matter obviously in this code", "tokens": [5780, 294, 341, 1729, 1389, 767, 586, 13, 286, 574, 412, 309, 13, 286, 478, 884, 309, 294, 341, 3089, 13, 467, 1177, 380, 1871, 2745, 294, 341, 3089], "temperature": 0.0, "avg_logprob": -0.1620874689586127, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.5612691842979984e-06}, {"id": 679, "seek": 322364, "start": 3238.64, "end": 3240.64, "text": " I'm actually doing", "tokens": [286, 478, 767, 884], "temperature": 0.0, "avg_logprob": -0.1620874689586127, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.5612691842979984e-06}, {"id": 680, "seek": 322364, "start": 3245.2, "end": 3247.2, "text": " I'm doing this bit first", "tokens": [286, 478, 884, 341, 857, 700], "temperature": 0.0, "avg_logprob": -0.1620874689586127, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.5612691842979984e-06}, {"id": 681, "seek": 324720, "start": 3247.2, "end": 3252.2, "text": " and so so", "tokens": [293, 370, 370], "temperature": 0.0, "avg_logprob": -0.3770595833107277, "compression_ratio": 1.3934426229508197, "no_speech_prob": 1.6536765770069906e-06}, {"id": 682, "seek": 324720, "start": 3257.3199999999997, "end": 3262.8799999999997, "text": " This thing here actually I caught it w which is probably pretty bad. It's actually w times x", "tokens": [639, 551, 510, 767, 286, 5415, 309, 261, 597, 307, 1391, 1238, 1578, 13, 467, 311, 767, 261, 1413, 2031], "temperature": 0.0, "avg_logprob": -0.3770595833107277, "compression_ratio": 1.3934426229508197, "no_speech_prob": 1.6536765770069906e-06}, {"id": 683, "seek": 324720, "start": 3264.72, "end": 3267.7599999999998, "text": " right so so instead of", "tokens": [558, 370, 370, 2602, 295], "temperature": 0.0, "avg_logprob": -0.3770595833107277, "compression_ratio": 1.3934426229508197, "no_speech_prob": 1.6536765770069906e-06}, {"id": 684, "seek": 324720, "start": 3268.7599999999998, "end": 3273.24, "text": " W times x times R. I've got w times x", "tokens": [343, 1413, 2031, 1413, 497, 13, 286, 600, 658, 261, 1413, 2031], "temperature": 0.0, "avg_logprob": -0.3770595833107277, "compression_ratio": 1.3934426229508197, "no_speech_prob": 1.6536765770069906e-06}, {"id": 685, "seek": 324720, "start": 3274.16, "end": 3276.0, "text": " plus a", "tokens": [1804, 257], "temperature": 0.0, "avg_logprob": -0.3770595833107277, "compression_ratio": 1.3934426229508197, "no_speech_prob": 1.6536765770069906e-06}, {"id": 686, "seek": 327600, "start": 3276.0, "end": 3277.16, "text": " constant", "tokens": [5754], "temperature": 0.0, "avg_logprob": -0.3331472396850586, "compression_ratio": 1.4744525547445255, "no_speech_prob": 1.3081732959108194e-06}, {"id": 687, "seek": 327600, "start": 3277.16, "end": 3279.16, "text": " times R right", "tokens": [1413, 497, 558], "temperature": 0.0, "avg_logprob": -0.3331472396850586, "compression_ratio": 1.4744525547445255, "no_speech_prob": 1.3081732959108194e-06}, {"id": 688, "seek": 327600, "start": 3279.68, "end": 3282.56, "text": " so the key idea here is", "tokens": [370, 264, 2141, 1558, 510, 307], "temperature": 0.0, "avg_logprob": -0.3331472396850586, "compression_ratio": 1.4744525547445255, "no_speech_prob": 1.3081732959108194e-06}, {"id": 689, "seek": 327600, "start": 3283.48, "end": 3285.32, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.3331472396850586, "compression_ratio": 1.4744525547445255, "no_speech_prob": 1.3081732959108194e-06}, {"id": 690, "seek": 327600, "start": 3285.32, "end": 3287.32, "text": " regularization", "tokens": [3890, 2144], "temperature": 0.0, "avg_logprob": -0.3331472396850586, "compression_ratio": 1.4744525547445255, "no_speech_prob": 1.3081732959108194e-06}, {"id": 691, "seek": 327600, "start": 3289.36, "end": 3291.56, "text": " Can't draw in yellow that's fair enough", "tokens": [1664, 380, 2642, 294, 5566, 300, 311, 3143, 1547], "temperature": 0.0, "avg_logprob": -0.3331472396850586, "compression_ratio": 1.4744525547445255, "no_speech_prob": 1.3081732959108194e-06}, {"id": 692, "seek": 327600, "start": 3294.0, "end": 3295.68, "text": " Regularization", "tokens": [45659, 2144], "temperature": 0.0, "avg_logprob": -0.3331472396850586, "compression_ratio": 1.4744525547445255, "no_speech_prob": 1.3081732959108194e-06}, {"id": 693, "seek": 327600, "start": 3295.68, "end": 3302.2, "text": " Wants the weights to be zero right because we're trying to it's trying to reduce", "tokens": [343, 1719, 264, 17443, 281, 312, 4018, 558, 570, 321, 434, 1382, 281, 309, 311, 1382, 281, 5407], "temperature": 0.0, "avg_logprob": -0.3331472396850586, "compression_ratio": 1.4744525547445255, "no_speech_prob": 1.3081732959108194e-06}, {"id": 694, "seek": 330220, "start": 3302.2, "end": 3304.2, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.2372021564217501, "compression_ratio": 1.7106598984771573, "no_speech_prob": 6.786719950468978e-07}, {"id": 695, "seek": 330220, "start": 3305.04, "end": 3307.04, "text": " Okay, and", "tokens": [1033, 11, 293], "temperature": 0.0, "avg_logprob": -0.2372021564217501, "compression_ratio": 1.7106598984771573, "no_speech_prob": 6.786719950468978e-07}, {"id": 696, "seek": 330220, "start": 3307.04, "end": 3313.7999999999997, "text": " So what we're saying is like okay. We want to push the weights towards zero because we're saying like that's our like", "tokens": [407, 437, 321, 434, 1566, 307, 411, 1392, 13, 492, 528, 281, 2944, 264, 17443, 3030, 4018, 570, 321, 434, 1566, 411, 300, 311, 527, 411], "temperature": 0.0, "avg_logprob": -0.2372021564217501, "compression_ratio": 1.7106598984771573, "no_speech_prob": 6.786719950468978e-07}, {"id": 697, "seek": 330220, "start": 3314.72, "end": 3316.4399999999996, "text": " default starting point", "tokens": [7576, 2891, 935], "temperature": 0.0, "avg_logprob": -0.2372021564217501, "compression_ratio": 1.7106598984771573, "no_speech_prob": 6.786719950468978e-07}, {"id": 698, "seek": 330220, "start": 3316.4399999999996, "end": 3318.8399999999997, "text": " expectation is the weights are zero and", "tokens": [14334, 307, 264, 17443, 366, 4018, 293], "temperature": 0.0, "avg_logprob": -0.2372021564217501, "compression_ratio": 1.7106598984771573, "no_speech_prob": 6.786719950468978e-07}, {"id": 699, "seek": 330220, "start": 3319.3199999999997, "end": 3322.24, "text": " So we want to be in a situation where if the weights are zero", "tokens": [407, 321, 528, 281, 312, 294, 257, 2590, 689, 498, 264, 17443, 366, 4018], "temperature": 0.0, "avg_logprob": -0.2372021564217501, "compression_ratio": 1.7106598984771573, "no_speech_prob": 6.786719950468978e-07}, {"id": 700, "seek": 330220, "start": 3322.8799999999997, "end": 3325.3599999999997, "text": " Then we have a model that like", "tokens": [1396, 321, 362, 257, 2316, 300, 411], "temperature": 0.0, "avg_logprob": -0.2372021564217501, "compression_ratio": 1.7106598984771573, "no_speech_prob": 6.786719950468978e-07}, {"id": 701, "seek": 332536, "start": 3325.36, "end": 3331.48, "text": " Makes theoretical or intuitive sense to us right?", "tokens": [25245, 20864, 420, 21769, 2020, 281, 505, 558, 30], "temperature": 0.0, "avg_logprob": -0.23044477535199515, "compression_ratio": 1.700507614213198, "no_speech_prob": 9.132527907240728e-07}, {"id": 702, "seek": 332536, "start": 3332.0, "end": 3340.08, "text": " This model if the weights are zero doesn't make intuitive sense to us right because it's saying hey multiply everything by zero", "tokens": [639, 2316, 498, 264, 17443, 366, 4018, 1177, 380, 652, 21769, 2020, 281, 505, 558, 570, 309, 311, 1566, 4177, 12972, 1203, 538, 4018], "temperature": 0.0, "avg_logprob": -0.23044477535199515, "compression_ratio": 1.700507614213198, "no_speech_prob": 9.132527907240728e-07}, {"id": 703, "seek": 332536, "start": 3340.36, "end": 3342.84, "text": " Gets rid of all of that and gets rid of that as well", "tokens": [460, 1385, 3973, 295, 439, 295, 300, 293, 2170, 3973, 295, 300, 382, 731], "temperature": 0.0, "avg_logprob": -0.23044477535199515, "compression_ratio": 1.700507614213198, "no_speech_prob": 9.132527907240728e-07}, {"id": 704, "seek": 332536, "start": 3343.52, "end": 3350.4, "text": " And we were actually saying no we actually think our R is useful. We actually want to keep that right so", "tokens": [400, 321, 645, 767, 1566, 572, 321, 767, 519, 527, 497, 307, 4420, 13, 492, 767, 528, 281, 1066, 300, 558, 370], "temperature": 0.0, "avg_logprob": -0.23044477535199515, "compression_ratio": 1.700507614213198, "no_speech_prob": 9.132527907240728e-07}, {"id": 705, "seek": 335040, "start": 3350.4, "end": 3358.28, "text": " So instead we say you know what let's take that piece here and add", "tokens": [407, 2602, 321, 584, 291, 458, 437, 718, 311, 747, 300, 2522, 510, 293, 909], "temperature": 0.0, "avg_logprob": -0.209127802597849, "compression_ratio": 1.5654761904761905, "no_speech_prob": 9.721521792016574e-07}, {"id": 706, "seek": 335040, "start": 3360.6800000000003, "end": 3362.28, "text": " 0.4 to it", "tokens": [1958, 13, 19, 281, 309], "temperature": 0.0, "avg_logprob": -0.209127802597849, "compression_ratio": 1.5654761904761905, "no_speech_prob": 9.721521792016574e-07}, {"id": 707, "seek": 335040, "start": 3362.28, "end": 3366.76, "text": " Right so now if the regularizer is pushing the weights towards zero", "tokens": [1779, 370, 586, 498, 264, 3890, 6545, 307, 7380, 264, 17443, 3030, 4018], "temperature": 0.0, "avg_logprob": -0.209127802597849, "compression_ratio": 1.5654761904761905, "no_speech_prob": 9.721521792016574e-07}, {"id": 708, "seek": 335040, "start": 3367.44, "end": 3370.1, "text": " Then it's pushing the value of this sum", "tokens": [1396, 309, 311, 7380, 264, 2158, 295, 341, 2408], "temperature": 0.0, "avg_logprob": -0.209127802597849, "compression_ratio": 1.5654761904761905, "no_speech_prob": 9.721521792016574e-07}, {"id": 709, "seek": 335040, "start": 3370.7200000000003, "end": 3372.7200000000003, "text": " towards 0.4", "tokens": [3030, 1958, 13, 19], "temperature": 0.0, "avg_logprob": -0.209127802597849, "compression_ratio": 1.5654761904761905, "no_speech_prob": 9.721521792016574e-07}, {"id": 710, "seek": 335040, "start": 3373.08, "end": 3378.2400000000002, "text": " Right and so therefore it's pushing our whole model to 0.4 times R", "tokens": [1779, 293, 370, 4412, 309, 311, 7380, 527, 1379, 2316, 281, 1958, 13, 19, 1413, 497], "temperature": 0.0, "avg_logprob": -0.209127802597849, "compression_ratio": 1.5654761904761905, "no_speech_prob": 9.721521792016574e-07}, {"id": 711, "seek": 337824, "start": 3378.24, "end": 3380.56, "text": " right so in other words", "tokens": [558, 370, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.1837313758002387, "compression_ratio": 1.6283185840707965, "no_speech_prob": 5.203560817790276e-07}, {"id": 712, "seek": 337824, "start": 3381.8799999999997, "end": 3387.3999999999996, "text": " Now kind of default starting point if you've regularized all the weights out altogether is to say yeah", "tokens": [823, 733, 295, 7576, 2891, 935, 498, 291, 600, 3890, 1602, 439, 264, 17443, 484, 19051, 307, 281, 584, 1338], "temperature": 0.0, "avg_logprob": -0.1837313758002387, "compression_ratio": 1.6283185840707965, "no_speech_prob": 5.203560817790276e-07}, {"id": 713, "seek": 337824, "start": 3387.3999999999996, "end": 3390.3999999999996, "text": " You know let's use a bit of R. That's probably a good idea", "tokens": [509, 458, 718, 311, 764, 257, 857, 295, 497, 13, 663, 311, 1391, 257, 665, 1558], "temperature": 0.0, "avg_logprob": -0.1837313758002387, "compression_ratio": 1.6283185840707965, "no_speech_prob": 5.203560817790276e-07}, {"id": 714, "seek": 337824, "start": 3392.12, "end": 3394.12, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.1837313758002387, "compression_ratio": 1.6283185840707965, "no_speech_prob": 5.203560817790276e-07}, {"id": 715, "seek": 337824, "start": 3395.6, "end": 3400.04, "text": " So that's the idea right that's the idea is basically you know what happens when", "tokens": [407, 300, 311, 264, 1558, 558, 300, 311, 264, 1558, 307, 1936, 291, 458, 437, 2314, 562], "temperature": 0.0, "avg_logprob": -0.1837313758002387, "compression_ratio": 1.6283185840707965, "no_speech_prob": 5.203560817790276e-07}, {"id": 716, "seek": 340004, "start": 3400.04, "end": 3407.32, "text": " When that's zero right and you and you want that to like be something sensible because otherwise", "tokens": [1133, 300, 311, 4018, 558, 293, 291, 293, 291, 528, 300, 281, 411, 312, 746, 25380, 570, 5911], "temperature": 0.0, "avg_logprob": -0.27072123325232306, "compression_ratio": 1.4970760233918128, "no_speech_prob": 4.313905606068147e-07}, {"id": 717, "seek": 340004, "start": 3409.6, "end": 3413.08, "text": " Regularizing the weights to move in that direction wouldn't be such a good idea", "tokens": [45659, 3319, 264, 17443, 281, 1286, 294, 300, 3513, 2759, 380, 312, 1270, 257, 665, 1558], "temperature": 0.0, "avg_logprob": -0.27072123325232306, "compression_ratio": 1.4970760233918128, "no_speech_prob": 4.313905606068147e-07}, {"id": 718, "seek": 340004, "start": 3415.2, "end": 3419.56, "text": " Second question was about", "tokens": [5736, 1168, 390, 466], "temperature": 0.0, "avg_logprob": -0.27072123325232306, "compression_ratio": 1.4970760233918128, "no_speech_prob": 4.313905606068147e-07}, {"id": 719, "seek": 340004, "start": 3424.24, "end": 3425.52, "text": " N-grams", "tokens": [426, 12, 1342, 82], "temperature": 0.0, "avg_logprob": -0.27072123325232306, "compression_ratio": 1.4970760233918128, "no_speech_prob": 4.313905606068147e-07}, {"id": 720, "seek": 342552, "start": 3425.52, "end": 3433.64, "text": " So the n in n-gram can be uni by tri whatever one two three whatever grams so for the", "tokens": [407, 264, 297, 294, 297, 12, 1342, 393, 312, 36435, 538, 1376, 2035, 472, 732, 1045, 2035, 11899, 370, 337, 264], "temperature": 0.0, "avg_logprob": -0.30884431390201345, "compression_ratio": 1.7950819672131149, "no_speech_prob": 2.4060948362603085e-06}, {"id": 721, "seek": 342552, "start": 3433.92, "end": 3435.92, "text": " This movie is good", "tokens": [639, 3169, 307, 665], "temperature": 0.0, "avg_logprob": -0.30884431390201345, "compression_ratio": 1.7950819672131149, "no_speech_prob": 2.4060948362603085e-06}, {"id": 722, "seek": 342552, "start": 3437.36, "end": 3439.68, "text": " Right it has four", "tokens": [1779, 309, 575, 1451], "temperature": 0.0, "avg_logprob": -0.30884431390201345, "compression_ratio": 1.7950819672131149, "no_speech_prob": 2.4060948362603085e-06}, {"id": 723, "seek": 342552, "start": 3440.52, "end": 3443.68, "text": " Unigrams this movie is good", "tokens": [1156, 33737, 82, 341, 3169, 307, 665], "temperature": 0.0, "avg_logprob": -0.30884431390201345, "compression_ratio": 1.7950819672131149, "no_speech_prob": 2.4060948362603085e-06}, {"id": 724, "seek": 342552, "start": 3444.44, "end": 3450.74, "text": " It has three bigrams this movie movie is is good", "tokens": [467, 575, 1045, 955, 2356, 82, 341, 3169, 3169, 307, 307, 665], "temperature": 0.0, "avg_logprob": -0.30884431390201345, "compression_ratio": 1.7950819672131149, "no_speech_prob": 2.4060948362603085e-06}, {"id": 725, "seek": 342552, "start": 3451.64, "end": 3453.64, "text": " It has two trigrams", "tokens": [467, 575, 732, 504, 33737, 82], "temperature": 0.0, "avg_logprob": -0.30884431390201345, "compression_ratio": 1.7950819672131149, "no_speech_prob": 2.4060948362603085e-06}, {"id": 726, "seek": 345364, "start": 3453.64, "end": 3455.64, "text": " This movie is", "tokens": [639, 3169, 307], "temperature": 0.0, "avg_logprob": -0.35991684595743817, "compression_ratio": 1.4148936170212767, "no_speech_prob": 1.0783132893266156e-05}, {"id": 727, "seek": 345364, "start": 3456.2799999999997, "end": 3458.2799999999997, "text": " Movie is good", "tokens": [28766, 307, 665], "temperature": 0.0, "avg_logprob": -0.35991684595743817, "compression_ratio": 1.4148936170212767, "no_speech_prob": 1.0783132893266156e-05}, {"id": 728, "seek": 345364, "start": 3458.52, "end": 3460.52, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.35991684595743817, "compression_ratio": 1.4148936170212767, "no_speech_prob": 1.0783132893266156e-05}, {"id": 729, "seek": 345364, "start": 3462.0, "end": 3464.0, "text": " Can you pass it", "tokens": [1664, 291, 1320, 309], "temperature": 0.0, "avg_logprob": -0.35991684595743817, "compression_ratio": 1.4148936170212767, "no_speech_prob": 1.0783132893266156e-05}, {"id": 730, "seek": 345364, "start": 3464.92, "end": 3467.2799999999997, "text": " So do you mind go back to the", "tokens": [407, 360, 291, 1575, 352, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.35991684595743817, "compression_ratio": 1.4148936170212767, "no_speech_prob": 1.0783132893266156e-05}, {"id": 731, "seek": 345364, "start": 3468.3599999999997, "end": 3471.92, "text": " W-80 chase down the 0.4 stuff yeah, so I was wondering if this", "tokens": [343, 12, 4702, 15359, 760, 264, 1958, 13, 19, 1507, 1338, 11, 370, 286, 390, 6359, 498, 341], "temperature": 0.0, "avg_logprob": -0.35991684595743817, "compression_ratio": 1.4148936170212767, "no_speech_prob": 1.0783132893266156e-05}, {"id": 732, "seek": 345364, "start": 3472.7599999999998, "end": 3476.44, "text": " adjustment will harm the predictability of the model because", "tokens": [17132, 486, 6491, 264, 6069, 2310, 295, 264, 2316, 570], "temperature": 0.0, "avg_logprob": -0.35991684595743817, "compression_ratio": 1.4148936170212767, "no_speech_prob": 1.0783132893266156e-05}, {"id": 733, "seek": 345364, "start": 3477.04, "end": 3482.64, "text": " think of extreme extreme case if it's not 0.4 if it's 4,000 and", "tokens": [519, 295, 8084, 8084, 1389, 498, 309, 311, 406, 1958, 13, 19, 498, 309, 311, 1017, 11, 1360, 293], "temperature": 0.0, "avg_logprob": -0.35991684595743817, "compression_ratio": 1.4148936170212767, "no_speech_prob": 1.0783132893266156e-05}, {"id": 734, "seek": 348264, "start": 3482.64, "end": 3488.96, "text": " Or right coefficient will be like right essentially so so exactly so so our prior", "tokens": [1610, 558, 17619, 486, 312, 411, 558, 4476, 370, 370, 2293, 370, 370, 527, 4059], "temperature": 0.0, "avg_logprob": -0.2759215108464273, "compression_ratio": 1.675, "no_speech_prob": 1.1659351002890617e-05}, {"id": 735, "seek": 348264, "start": 3489.56, "end": 3492.4, "text": " Needs to make sense and so our prior here", "tokens": [1734, 5147, 281, 652, 2020, 293, 370, 527, 4059, 510], "temperature": 0.0, "avg_logprob": -0.2759215108464273, "compression_ratio": 1.675, "no_speech_prob": 1.1659351002890617e-05}, {"id": 736, "seek": 348264, "start": 3492.4, "end": 3499.2799999999997, "text": " And you know this is why it's called dot product NB is our prior is that this is something where we think naive Bayes", "tokens": [400, 291, 458, 341, 307, 983, 309, 311, 1219, 5893, 1674, 426, 33, 307, 527, 4059, 307, 300, 341, 307, 746, 689, 321, 519, 29052, 7840, 279], "temperature": 0.0, "avg_logprob": -0.2759215108464273, "compression_ratio": 1.675, "no_speech_prob": 1.1659351002890617e-05}, {"id": 737, "seek": 348264, "start": 3499.56, "end": 3504.92, "text": " Is a good prior right and so naive Bayes says that R equals", "tokens": [1119, 257, 665, 4059, 558, 293, 370, 29052, 7840, 279, 1619, 300, 497, 6915], "temperature": 0.0, "avg_logprob": -0.2759215108464273, "compression_ratio": 1.675, "no_speech_prob": 1.1659351002890617e-05}, {"id": 738, "seek": 348264, "start": 3506.2, "end": 3508.2, "text": " P over", "tokens": [430, 670], "temperature": 0.0, "avg_logprob": -0.2759215108464273, "compression_ratio": 1.675, "no_speech_prob": 1.1659351002890617e-05}, {"id": 739, "seek": 348264, "start": 3508.7599999999998, "end": 3510.7599999999998, "text": " That's not how you write P", "tokens": [663, 311, 406, 577, 291, 2464, 430], "temperature": 0.0, "avg_logprob": -0.2759215108464273, "compression_ratio": 1.675, "no_speech_prob": 1.1659351002890617e-05}, {"id": 740, "seek": 351076, "start": 3510.76, "end": 3513.6000000000004, "text": " P over Q. I have not had much sleep", "tokens": [430, 670, 1249, 13, 286, 362, 406, 632, 709, 2817], "temperature": 0.0, "avg_logprob": -0.23493576049804688, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.1365598311385838e-06}, {"id": 741, "seek": 351076, "start": 3514.1600000000003, "end": 3519.44, "text": " P over Q is a good prior and not only do we think it's a good prior, but we think", "tokens": [430, 670, 1249, 307, 257, 665, 4059, 293, 406, 787, 360, 321, 519, 309, 311, 257, 665, 4059, 11, 457, 321, 519], "temperature": 0.0, "avg_logprob": -0.23493576049804688, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.1365598311385838e-06}, {"id": 742, "seek": 351076, "start": 3522.88, "end": 3524.48, "text": " Times X", "tokens": [11366, 1783], "temperature": 0.0, "avg_logprob": -0.23493576049804688, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.1365598311385838e-06}, {"id": 743, "seek": 351076, "start": 3524.48, "end": 3526.5600000000004, "text": " Plus B is a good model", "tokens": [7721, 363, 307, 257, 665, 2316], "temperature": 0.0, "avg_logprob": -0.23493576049804688, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.1365598311385838e-06}, {"id": 744, "seek": 351076, "start": 3527.6000000000004, "end": 3531.6400000000003, "text": " That's that's the naive Bayes model so in other words we expect that", "tokens": [663, 311, 300, 311, 264, 29052, 7840, 279, 2316, 370, 294, 661, 2283, 321, 2066, 300], "temperature": 0.0, "avg_logprob": -0.23493576049804688, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.1365598311385838e-06}, {"id": 745, "seek": 351076, "start": 3532.36, "end": 3538.6400000000003, "text": " You know a coefficient of one is a good coefficient not not four thousand", "tokens": [509, 458, 257, 17619, 295, 472, 307, 257, 665, 17619, 406, 406, 1451, 4714], "temperature": 0.0, "avg_logprob": -0.23493576049804688, "compression_ratio": 1.6256983240223464, "no_speech_prob": 1.1365598311385838e-06}, {"id": 746, "seek": 353864, "start": 3538.64, "end": 3544.8399999999997, "text": " Yeah, so we think specifically we don't think we think zero is probably not a good coefficient", "tokens": [865, 11, 370, 321, 519, 4682, 321, 500, 380, 519, 321, 519, 4018, 307, 1391, 406, 257, 665, 17619], "temperature": 0.0, "avg_logprob": -0.2530346149351539, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.446186257060617e-07}, {"id": 747, "seek": 353864, "start": 3545.6, "end": 3548.08, "text": " right, but we also think that maybe", "tokens": [558, 11, 457, 321, 611, 519, 300, 1310], "temperature": 0.0, "avg_logprob": -0.2530346149351539, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.446186257060617e-07}, {"id": 748, "seek": 353864, "start": 3549.0, "end": 3553.2799999999997, "text": " The naive Bayes version is a little overconfident, so maybe one's a little high", "tokens": [440, 29052, 7840, 279, 3037, 307, 257, 707, 670, 24697, 1078, 11, 370, 1310, 472, 311, 257, 707, 1090], "temperature": 0.0, "avg_logprob": -0.2530346149351539, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.446186257060617e-07}, {"id": 749, "seek": 353864, "start": 3554.0, "end": 3560.52, "text": " So we're pretty sure that the right number assuming that our model and a base model is appropriate is between", "tokens": [407, 321, 434, 1238, 988, 300, 264, 558, 1230, 11926, 300, 527, 2316, 293, 257, 3096, 2316, 307, 6854, 307, 1296], "temperature": 0.0, "avg_logprob": -0.2530346149351539, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.446186257060617e-07}, {"id": 750, "seek": 353864, "start": 3561.16, "end": 3563.16, "text": " zero and one", "tokens": [4018, 293, 472], "temperature": 0.0, "avg_logprob": -0.2530346149351539, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.446186257060617e-07}, {"id": 751, "seek": 356316, "start": 3563.16, "end": 3570.2799999999997, "text": " And no, but what I was thinking is as long as it's not zero you are pushing those", "tokens": [400, 572, 11, 457, 437, 286, 390, 1953, 307, 382, 938, 382, 309, 311, 406, 4018, 291, 366, 7380, 729], "temperature": 0.0, "avg_logprob": -0.2054751201342511, "compression_ratio": 2.0052631578947366, "no_speech_prob": 3.905428911821218e-06}, {"id": 752, "seek": 356316, "start": 3571.3599999999997, "end": 3575.74, "text": " coefficients that are supposed to be zero to something not zero and makes the", "tokens": [31994, 300, 366, 3442, 281, 312, 4018, 281, 746, 406, 4018, 293, 1669, 264], "temperature": 0.0, "avg_logprob": -0.2054751201342511, "compression_ratio": 2.0052631578947366, "no_speech_prob": 3.905428911821218e-06}, {"id": 753, "seek": 356316, "start": 3576.3599999999997, "end": 3583.92, "text": " Like high coefficients less distinctive from the coefficients well, but you see they're not supposed to be zero", "tokens": [1743, 1090, 31994, 1570, 27766, 490, 264, 31994, 731, 11, 457, 291, 536, 436, 434, 406, 3442, 281, 312, 4018], "temperature": 0.0, "avg_logprob": -0.2054751201342511, "compression_ratio": 2.0052631578947366, "no_speech_prob": 3.905428911821218e-06}, {"id": 754, "seek": 356316, "start": 3583.92, "end": 3585.92, "text": " They're supposed to be R", "tokens": [814, 434, 3442, 281, 312, 497], "temperature": 0.0, "avg_logprob": -0.2054751201342511, "compression_ratio": 2.0052631578947366, "no_speech_prob": 3.905428911821218e-06}, {"id": 755, "seek": 356316, "start": 3586.7999999999997, "end": 3588.7999999999997, "text": " Like that's that's what they're supposed to be", "tokens": [1743, 300, 311, 300, 311, 437, 436, 434, 3442, 281, 312], "temperature": 0.0, "avg_logprob": -0.2054751201342511, "compression_ratio": 2.0052631578947366, "no_speech_prob": 3.905428911821218e-06}, {"id": 756, "seek": 358880, "start": 3588.8, "end": 3594.1600000000003, "text": " They're supposed to be R right and so and remember", "tokens": [814, 434, 3442, 281, 312, 497, 558, 293, 370, 293, 1604], "temperature": 0.0, "avg_logprob": -0.20675421924125859, "compression_ratio": 1.5376884422110553, "no_speech_prob": 8.446202173217898e-07}, {"id": 757, "seek": 358880, "start": 3594.5600000000004, "end": 3601.96, "text": " This is inside our forward function, so this is part of what we're taking the gradient of right so it's basically", "tokens": [639, 307, 1854, 527, 2128, 2445, 11, 370, 341, 307, 644, 295, 437, 321, 434, 1940, 264, 16235, 295, 558, 370, 309, 311, 1936], "temperature": 0.0, "avg_logprob": -0.20675421924125859, "compression_ratio": 1.5376884422110553, "no_speech_prob": 8.446202173217898e-07}, {"id": 758, "seek": 358880, "start": 3602.48, "end": 3605.52, "text": " Saying okay, we're still going to you know you can still set", "tokens": [34087, 1392, 11, 321, 434, 920, 516, 281, 291, 458, 291, 393, 920, 992], "temperature": 0.0, "avg_logprob": -0.20675421924125859, "compression_ratio": 1.5376884422110553, "no_speech_prob": 8.446202173217898e-07}, {"id": 759, "seek": 358880, "start": 3606.5600000000004, "end": 3608.5600000000004, "text": " Self dot W to anything you like", "tokens": [16348, 5893, 343, 281, 1340, 291, 411], "temperature": 0.0, "avg_logprob": -0.20675421924125859, "compression_ratio": 1.5376884422110553, "no_speech_prob": 8.446202173217898e-07}, {"id": 760, "seek": 358880, "start": 3610.28, "end": 3612.28, "text": " But just the regularizer", "tokens": [583, 445, 264, 3890, 6545], "temperature": 0.0, "avg_logprob": -0.20675421924125859, "compression_ratio": 1.5376884422110553, "no_speech_prob": 8.446202173217898e-07}, {"id": 761, "seek": 358880, "start": 3612.84, "end": 3614.84, "text": " Wants it to be zero and", "tokens": [343, 1719, 309, 281, 312, 4018, 293], "temperature": 0.0, "avg_logprob": -0.20675421924125859, "compression_ratio": 1.5376884422110553, "no_speech_prob": 8.446202173217898e-07}, {"id": 762, "seek": 361484, "start": 3614.84, "end": 3620.36, "text": " So we're always saying is okay if you want it to be zero, then I'll try to make zero be", "tokens": [407, 321, 434, 1009, 1566, 307, 1392, 498, 291, 528, 309, 281, 312, 4018, 11, 550, 286, 603, 853, 281, 652, 4018, 312], "temperature": 0.0, "avg_logprob": -0.14711422963185353, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.7264484288025415e-06}, {"id": 763, "seek": 361484, "start": 3620.96, "end": 3622.96, "text": " You know give a sensible answer", "tokens": [509, 458, 976, 257, 25380, 1867], "temperature": 0.0, "avg_logprob": -0.14711422963185353, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.7264484288025415e-06}, {"id": 764, "seek": 361484, "start": 3624.28, "end": 3629.48, "text": " That's the basic idea and like yeah, nothing says point fours perfect for every data set", "tokens": [663, 311, 264, 3875, 1558, 293, 411, 1338, 11, 1825, 1619, 935, 1451, 82, 2176, 337, 633, 1412, 992], "temperature": 0.0, "avg_logprob": -0.14711422963185353, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.7264484288025415e-06}, {"id": 765, "seek": 361484, "start": 3629.48, "end": 3635.0, "text": " I've tried a few different data sets and found various numbers between point three and point six that are optimal", "tokens": [286, 600, 3031, 257, 1326, 819, 1412, 6352, 293, 1352, 3683, 3547, 1296, 935, 1045, 293, 935, 2309, 300, 366, 16252], "temperature": 0.0, "avg_logprob": -0.14711422963185353, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.7264484288025415e-06}, {"id": 766, "seek": 361484, "start": 3635.2400000000002, "end": 3638.2000000000003, "text": " But I've never found one where point four is", "tokens": [583, 286, 600, 1128, 1352, 472, 689, 935, 1451, 307], "temperature": 0.0, "avg_logprob": -0.14711422963185353, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.7264484288025415e-06}, {"id": 767, "seek": 363820, "start": 3638.2, "end": 3644.5, "text": " Is less good than zero which is not surprising and I've also never found one where one is better", "tokens": [1119, 1570, 665, 813, 4018, 597, 307, 406, 8830, 293, 286, 600, 611, 1128, 1352, 472, 689, 472, 307, 1101], "temperature": 0.0, "avg_logprob": -0.13591912587483723, "compression_ratio": 1.6401515151515151, "no_speech_prob": 9.42242479595734e-07}, {"id": 768, "seek": 363820, "start": 3644.68, "end": 3649.72, "text": " Right so the idea is like this is a reasonable default, but it's another parameter you can play with which I kind of like", "tokens": [1779, 370, 264, 1558, 307, 411, 341, 307, 257, 10585, 7576, 11, 457, 309, 311, 1071, 13075, 291, 393, 862, 365, 597, 286, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.13591912587483723, "compression_ratio": 1.6401515151515151, "no_speech_prob": 9.42242479595734e-07}, {"id": 769, "seek": 363820, "start": 3649.72, "end": 3651.72, "text": " Right it's another thing you could", "tokens": [1779, 309, 311, 1071, 551, 291, 727], "temperature": 0.0, "avg_logprob": -0.13591912587483723, "compression_ratio": 1.6401515151515151, "no_speech_prob": 9.42242479595734e-07}, {"id": 770, "seek": 363820, "start": 3651.72, "end": 3653.2799999999997, "text": " use", "tokens": [764], "temperature": 0.0, "avg_logprob": -0.13591912587483723, "compression_ratio": 1.6401515151515151, "no_speech_prob": 9.42242479595734e-07}, {"id": 771, "seek": 363820, "start": 3653.2799999999997, "end": 3659.08, "text": " Grid search or whatever to figure out for your data set. What's best and you know really the key here being", "tokens": [42905, 3164, 420, 2035, 281, 2573, 484, 337, 428, 1412, 992, 13, 708, 311, 1151, 293, 291, 458, 534, 264, 2141, 510, 885], "temperature": 0.0, "avg_logprob": -0.13591912587483723, "compression_ratio": 1.6401515151515151, "no_speech_prob": 9.42242479595734e-07}, {"id": 772, "seek": 363820, "start": 3661.4399999999996, "end": 3665.4399999999996, "text": " Every model before this one as far as I know has implicitly assumed", "tokens": [2048, 2316, 949, 341, 472, 382, 1400, 382, 286, 458, 575, 26947, 356, 15895], "temperature": 0.0, "avg_logprob": -0.13591912587483723, "compression_ratio": 1.6401515151515151, "no_speech_prob": 9.42242479595734e-07}, {"id": 773, "seek": 366544, "start": 3665.44, "end": 3670.12, "text": " It should be zero because they just they don't have this parameter right and you know by the way", "tokens": [467, 820, 312, 4018, 570, 436, 445, 436, 500, 380, 362, 341, 13075, 558, 293, 291, 458, 538, 264, 636], "temperature": 0.0, "avg_logprob": -0.15903244018554688, "compression_ratio": 1.7027972027972027, "no_speech_prob": 5.594301910605282e-06}, {"id": 774, "seek": 366544, "start": 3670.12, "end": 3672.12, "text": " I've actually got a second parameter here as well", "tokens": [286, 600, 767, 658, 257, 1150, 13075, 510, 382, 731], "temperature": 0.0, "avg_logprob": -0.15903244018554688, "compression_ratio": 1.7027972027972027, "no_speech_prob": 5.594301910605282e-06}, {"id": 775, "seek": 366544, "start": 3672.88, "end": 3675.96, "text": " Which is the same thing I do to R is actually divide R", "tokens": [3013, 307, 264, 912, 551, 286, 360, 281, 497, 307, 767, 9845, 497], "temperature": 0.0, "avg_logprob": -0.15903244018554688, "compression_ratio": 1.7027972027972027, "no_speech_prob": 5.594301910605282e-06}, {"id": 776, "seek": 366544, "start": 3676.6, "end": 3680.4, "text": " By a parameter which I'm not going to worry too much about it now", "tokens": [3146, 257, 13075, 597, 286, 478, 406, 516, 281, 3292, 886, 709, 466, 309, 586], "temperature": 0.0, "avg_logprob": -0.15903244018554688, "compression_ratio": 1.7027972027972027, "no_speech_prob": 5.594301910605282e-06}, {"id": 777, "seek": 366544, "start": 3680.4, "end": 3685.68, "text": " But again it's another parameter you can use to kind of adjust what the nature of the regularization is", "tokens": [583, 797, 309, 311, 1071, 13075, 291, 393, 764, 281, 733, 295, 4369, 437, 264, 3687, 295, 264, 3890, 2144, 307], "temperature": 0.0, "avg_logprob": -0.15903244018554688, "compression_ratio": 1.7027972027972027, "no_speech_prob": 5.594301910605282e-06}, {"id": 778, "seek": 366544, "start": 3687.52, "end": 3689.52, "text": " You know and I mean in the end I'm a", "tokens": [509, 458, 293, 286, 914, 294, 264, 917, 286, 478, 257], "temperature": 0.0, "avg_logprob": -0.15903244018554688, "compression_ratio": 1.7027972027972027, "no_speech_prob": 5.594301910605282e-06}, {"id": 779, "seek": 366544, "start": 3690.2400000000002, "end": 3693.44, "text": " Empiricist not a theoretician. You know I thought this seemed like a good idea", "tokens": [8599, 347, 299, 468, 406, 257, 14308, 9027, 13, 509, 458, 286, 1194, 341, 6576, 411, 257, 665, 1558], "temperature": 0.0, "avg_logprob": -0.15903244018554688, "compression_ratio": 1.7027972027972027, "no_speech_prob": 5.594301910605282e-06}, {"id": 780, "seek": 369344, "start": 3693.44, "end": 3698.8, "text": " Nearly all of my things that seem like a good idea turn out to be stupid this particular one", "tokens": [38000, 439, 295, 452, 721, 300, 1643, 411, 257, 665, 1558, 1261, 484, 281, 312, 6631, 341, 1729, 472], "temperature": 0.0, "avg_logprob": -0.2795265826982321, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.670098390604835e-05}, {"id": 781, "seek": 369344, "start": 3699.16, "end": 3703.78, "text": " Dave good results you know on this data set and a few other ones as well", "tokens": [11017, 665, 3542, 291, 458, 322, 341, 1412, 992, 293, 257, 1326, 661, 2306, 382, 731], "temperature": 0.0, "avg_logprob": -0.2795265826982321, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.670098390604835e-05}, {"id": 782, "seek": 369344, "start": 3704.48, "end": 3706.7200000000003, "text": " Okay, could you pass that you were started? Yeah?", "tokens": [1033, 11, 727, 291, 1320, 300, 291, 645, 1409, 30, 865, 30], "temperature": 0.0, "avg_logprob": -0.2795265826982321, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.670098390604835e-05}, {"id": 783, "seek": 369344, "start": 3707.56, "end": 3712.28, "text": " Yeah, I'm still a little bit confused about the W plus W adjusted uh-huh", "tokens": [865, 11, 286, 478, 920, 257, 707, 857, 9019, 466, 264, 343, 1804, 343, 19871, 2232, 12, 18710], "temperature": 0.0, "avg_logprob": -0.2795265826982321, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.670098390604835e-05}, {"id": 784, "seek": 369344, "start": 3713.52, "end": 3720.08, "text": " So you mentioned that we do W plus W adjusted so that the coefficients don't get set to zero", "tokens": [407, 291, 2835, 300, 321, 360, 343, 1804, 343, 19871, 370, 300, 264, 31994, 500, 380, 483, 992, 281, 4018], "temperature": 0.0, "avg_logprob": -0.2795265826982321, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.670098390604835e-05}, {"id": 785, "seek": 372008, "start": 3720.08, "end": 3723.08, "text": " That we place some importance on the priors", "tokens": [663, 321, 1081, 512, 7379, 322, 264, 1790, 830], "temperature": 0.0, "avg_logprob": -0.3102719728336778, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.0615919563861098e-05}, {"id": 786, "seek": 372008, "start": 3723.52, "end": 3725.52, "text": " but you also said that the", "tokens": [457, 291, 611, 848, 300, 264], "temperature": 0.0, "avg_logprob": -0.3102719728336778, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.0615919563861098e-05}, {"id": 787, "seek": 372008, "start": 3726.16, "end": 3731.88, "text": " Effect of learning can be that W gets set to a negative value which effectively turns W plus W", "tokens": [17764, 295, 2539, 393, 312, 300, 343, 2170, 992, 281, 257, 3671, 2158, 597, 8659, 4523, 343, 1804, 343], "temperature": 0.0, "avg_logprob": -0.3102719728336778, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.0615919563861098e-05}, {"id": 788, "seek": 372008, "start": 3732.96, "end": 3740.04, "text": " Zero so if if we are we are allowing the learning process to indeed set the priors to zero", "tokens": [17182, 370, 498, 498, 321, 366, 321, 366, 8293, 264, 2539, 1399, 281, 6451, 992, 264, 1790, 830, 281, 4018], "temperature": 0.0, "avg_logprob": -0.3102719728336778, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.0615919563861098e-05}, {"id": 789, "seek": 372008, "start": 3743.08, "end": 3748.0, "text": " So why is that in any way different from just having W because yeah great question because of regularization", "tokens": [407, 983, 307, 300, 294, 604, 636, 819, 490, 445, 1419, 343, 570, 1338, 869, 1168, 570, 295, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.3102719728336778, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.0615919563861098e-05}, {"id": 790, "seek": 374800, "start": 3748.0, "end": 3750.36, "text": " because we're penalizing it by that", "tokens": [570, 321, 434, 13661, 3319, 309, 538, 300], "temperature": 0.0, "avg_logprob": -0.17790262749854555, "compression_ratio": 1.6862745098039216, "no_speech_prob": 1.963798013093765e-06}, {"id": 791, "seek": 374800, "start": 3752.0, "end": 3754.0, "text": " Right so in other words", "tokens": [1779, 370, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.17790262749854555, "compression_ratio": 1.6862745098039216, "no_speech_prob": 1.963798013093765e-06}, {"id": 792, "seek": 374800, "start": 3755.12, "end": 3759.76, "text": " We're saying you know what if you're if the best thing to do is to ignore the value of R", "tokens": [492, 434, 1566, 291, 458, 437, 498, 291, 434, 498, 264, 1151, 551, 281, 360, 307, 281, 11200, 264, 2158, 295, 497], "temperature": 0.0, "avg_logprob": -0.17790262749854555, "compression_ratio": 1.6862745098039216, "no_speech_prob": 1.963798013093765e-06}, {"id": 793, "seek": 374800, "start": 3760.72, "end": 3764.16, "text": " That'll cost you you're going to have to set W to a negative number", "tokens": [663, 603, 2063, 291, 291, 434, 516, 281, 362, 281, 992, 343, 281, 257, 3671, 1230], "temperature": 0.0, "avg_logprob": -0.17790262749854555, "compression_ratio": 1.6862745098039216, "no_speech_prob": 1.963798013093765e-06}, {"id": 794, "seek": 374800, "start": 3765.92, "end": 3773.4, "text": " Right so only do that if that's clearly a good idea unless it's clearly a good idea, then you should leave", "tokens": [1779, 370, 787, 360, 300, 498, 300, 311, 4448, 257, 665, 1558, 5969, 309, 311, 4448, 257, 665, 1558, 11, 550, 291, 820, 1856], "temperature": 0.0, "avg_logprob": -0.17790262749854555, "compression_ratio": 1.6862745098039216, "no_speech_prob": 1.963798013093765e-06}, {"id": 795, "seek": 374800, "start": 3774.36, "end": 3776.36, "text": " Leave it where it is", "tokens": [9825, 309, 689, 309, 307], "temperature": 0.0, "avg_logprob": -0.17790262749854555, "compression_ratio": 1.6862745098039216, "no_speech_prob": 1.963798013093765e-06}, {"id": 796, "seek": 377636, "start": 3776.36, "end": 3782.84, "text": " That's that's the only reason like all of this stuff. We've done today is basically entirely about", "tokens": [663, 311, 300, 311, 264, 787, 1778, 411, 439, 295, 341, 1507, 13, 492, 600, 1096, 965, 307, 1936, 7696, 466], "temperature": 0.0, "avg_logprob": -0.17821431159973145, "compression_ratio": 1.6380090497737556, "no_speech_prob": 2.4824682895996375e-06}, {"id": 797, "seek": 377636, "start": 3783.92, "end": 3787.84, "text": " You know maximizing the advantage we get from regularization and saying regularization", "tokens": [509, 458, 5138, 3319, 264, 5002, 321, 483, 490, 3890, 2144, 293, 1566, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.17821431159973145, "compression_ratio": 1.6380090497737556, "no_speech_prob": 2.4824682895996375e-06}, {"id": 798, "seek": 377636, "start": 3789.04, "end": 3797.08, "text": " pushes us towards some default assumption and nearly all of the machine learning literature assumes that default assumption is", "tokens": [21020, 505, 3030, 512, 7576, 15302, 293, 6217, 439, 295, 264, 3479, 2539, 10394, 37808, 300, 7576, 15302, 307], "temperature": 0.0, "avg_logprob": -0.17821431159973145, "compression_ratio": 1.6380090497737556, "no_speech_prob": 2.4824682895996375e-06}, {"id": 799, "seek": 377636, "start": 3798.04, "end": 3801.1, "text": " Everything zero, and I'm saying like it turns out", "tokens": [5471, 4018, 11, 293, 286, 478, 1566, 411, 309, 4523, 484], "temperature": 0.0, "avg_logprob": -0.17821431159973145, "compression_ratio": 1.6380090497737556, "no_speech_prob": 2.4824682895996375e-06}, {"id": 800, "seek": 380110, "start": 3801.1, "end": 3807.9, "text": " You know it makes sense theoretically and turns out empirically that actually you should decide what your default assumption is", "tokens": [509, 458, 309, 1669, 2020, 29400, 293, 4523, 484, 25790, 984, 300, 767, 291, 820, 4536, 437, 428, 7576, 15302, 307], "temperature": 0.0, "avg_logprob": -0.20051784904635683, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.2959045509196585e-06}, {"id": 801, "seek": 380110, "start": 3808.46, "end": 3812.38, "text": " And that'll give you better results, so we'd be right to say that", "tokens": [400, 300, 603, 976, 291, 1101, 3542, 11, 370, 321, 1116, 312, 558, 281, 584, 300], "temperature": 0.0, "avg_logprob": -0.20051784904635683, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.2959045509196585e-06}, {"id": 802, "seek": 380110, "start": 3812.94, "end": 3819.5, "text": " In a way you're putting an additional hurdle in the along the way towards getting all coefficients to zero", "tokens": [682, 257, 636, 291, 434, 3372, 364, 4497, 47423, 294, 264, 2051, 264, 636, 3030, 1242, 439, 31994, 281, 4018], "temperature": 0.0, "avg_logprob": -0.20051784904635683, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.2959045509196585e-06}, {"id": 803, "seek": 380110, "start": 3819.62, "end": 3823.62, "text": " So it will be able to do that if it is really worth it. Yeah exactly", "tokens": [407, 309, 486, 312, 1075, 281, 360, 300, 498, 309, 307, 534, 3163, 309, 13, 865, 2293], "temperature": 0.0, "avg_logprob": -0.20051784904635683, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.2959045509196585e-06}, {"id": 804, "seek": 380110, "start": 3823.62, "end": 3826.98, "text": " So I'd say like the default hurdle without this is is", "tokens": [407, 286, 1116, 584, 411, 264, 7576, 47423, 1553, 341, 307, 307], "temperature": 0.0, "avg_logprob": -0.20051784904635683, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.2959045509196585e-06}, {"id": 805, "seek": 382698, "start": 3826.98, "end": 3834.26, "text": " Making a coefficient non zero is the hook hurdle and now I'm saying no the co-op the the hurdle is making a coefficient", "tokens": [14595, 257, 17619, 2107, 4018, 307, 264, 6328, 47423, 293, 586, 286, 478, 1566, 572, 264, 598, 12, 404, 264, 264, 47423, 307, 1455, 257, 17619], "temperature": 0.0, "avg_logprob": -0.37261790613974294, "compression_ratio": 1.4903225806451612, "no_speech_prob": 2.225267053290736e-06}, {"id": 806, "seek": 382698, "start": 3834.7400000000002, "end": 3836.7400000000002, "text": " Not be equal to point for our", "tokens": [1726, 312, 2681, 281, 935, 337, 527], "temperature": 0.0, "avg_logprob": -0.37261790613974294, "compression_ratio": 1.4903225806451612, "no_speech_prob": 2.225267053290736e-06}, {"id": 807, "seek": 382698, "start": 3842.46, "end": 3846.86, "text": " So this is sum of W squared into C", "tokens": [407, 341, 307, 2408, 295, 343, 8889, 666, 383], "temperature": 0.0, "avg_logprob": -0.37261790613974294, "compression_ratio": 1.4903225806451612, "no_speech_prob": 2.225267053290736e-06}, {"id": 808, "seek": 382698, "start": 3848.58, "end": 3852.42, "text": " Sum of it is some lambda or C penalty constant", "tokens": [8626, 295, 309, 307, 512, 13607, 420, 383, 16263, 5754], "temperature": 0.0, "avg_logprob": -0.37261790613974294, "compression_ratio": 1.4903225806451612, "no_speech_prob": 2.225267053290736e-06}, {"id": 809, "seek": 385242, "start": 3852.42, "end": 3858.94, "text": " Yeah, yeah time something yeah, so the weight decay should also depend on the value of C if it is very less", "tokens": [865, 11, 1338, 565, 746, 1338, 11, 370, 264, 3364, 21039, 820, 611, 5672, 322, 264, 2158, 295, 383, 498, 309, 307, 588, 1570], "temperature": 0.0, "avg_logprob": -0.31734841935178065, "compression_ratio": 1.6844660194174756, "no_speech_prob": 3.3404421628802083e-06}, {"id": 810, "seek": 385242, "start": 3859.5, "end": 3862.0, "text": " Like if C is right by C. Do you mean this?", "tokens": [1743, 498, 383, 307, 558, 538, 383, 13, 1144, 291, 914, 341, 30], "temperature": 0.0, "avg_logprob": -0.31734841935178065, "compression_ratio": 1.6844660194174756, "no_speech_prob": 3.3404421628802083e-06}, {"id": 811, "seek": 385242, "start": 3864.1800000000003, "end": 3871.78, "text": " So if a is point one then the weights might not go towards zero yes, then we might not need great decay", "tokens": [407, 498, 257, 307, 935, 472, 550, 264, 17443, 1062, 406, 352, 3030, 4018, 2086, 11, 550, 321, 1062, 406, 643, 869, 21039], "temperature": 0.0, "avg_logprob": -0.31734841935178065, "compression_ratio": 1.6844660194174756, "no_speech_prob": 3.3404421628802083e-06}, {"id": 812, "seek": 385242, "start": 3872.1800000000003, "end": 3873.94, "text": " so well the", "tokens": [370, 731, 264], "temperature": 0.0, "avg_logprob": -0.31734841935178065, "compression_ratio": 1.6844660194174756, "no_speech_prob": 3.3404421628802083e-06}, {"id": 813, "seek": 385242, "start": 3873.94, "end": 3877.7200000000003, "text": " Whatever this value. I mean if the if the value of this is zero then there is no", "tokens": [8541, 341, 2158, 13, 286, 914, 498, 264, 498, 264, 2158, 295, 341, 307, 4018, 550, 456, 307, 572], "temperature": 0.0, "avg_logprob": -0.31734841935178065, "compression_ratio": 1.6844660194174756, "no_speech_prob": 3.3404421628802083e-06}, {"id": 814, "seek": 387772, "start": 3877.72, "end": 3883.68, "text": " Recognization right, but if this value is higher than zero then there is some penalty", "tokens": [44682, 2144, 558, 11, 457, 498, 341, 2158, 307, 2946, 813, 4018, 550, 456, 307, 512, 16263], "temperature": 0.0, "avg_logprob": -0.1675307642329823, "compression_ratio": 1.936842105263158, "no_speech_prob": 1.3287728961586254e-06}, {"id": 815, "seek": 387772, "start": 3884.2, "end": 3888.48, "text": " Right and and presumably we've set it to non zero because we're overfitting", "tokens": [1779, 293, 293, 26742, 321, 600, 992, 309, 281, 2107, 4018, 570, 321, 434, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.1675307642329823, "compression_ratio": 1.936842105263158, "no_speech_prob": 1.3287728961586254e-06}, {"id": 816, "seek": 387772, "start": 3888.64, "end": 3893.16, "text": " So we want some penalty and so if there is some penalty then", "tokens": [407, 321, 528, 512, 16263, 293, 370, 498, 456, 307, 512, 16263, 550], "temperature": 0.0, "avg_logprob": -0.1675307642329823, "compression_ratio": 1.936842105263158, "no_speech_prob": 1.3287728961586254e-06}, {"id": 817, "seek": 387772, "start": 3894.3599999999997, "end": 3899.9599999999996, "text": " Then my assertion is that we should penalize things that are different to our prior", "tokens": [1396, 452, 19810, 313, 307, 300, 321, 820, 13661, 1125, 721, 300, 366, 819, 281, 527, 4059], "temperature": 0.0, "avg_logprob": -0.1675307642329823, "compression_ratio": 1.936842105263158, "no_speech_prob": 1.3287728961586254e-06}, {"id": 818, "seek": 387772, "start": 3900.48, "end": 3903.04, "text": " Not that we should penalize things that are different to zero", "tokens": [1726, 300, 321, 820, 13661, 1125, 721, 300, 366, 819, 281, 4018], "temperature": 0.0, "avg_logprob": -0.1675307642329823, "compression_ratio": 1.936842105263158, "no_speech_prob": 1.3287728961586254e-06}, {"id": 819, "seek": 390304, "start": 3903.04, "end": 3911.7, "text": " And our prior is that things should be you know around about equal to R", "tokens": [400, 527, 4059, 307, 300, 721, 820, 312, 291, 458, 926, 466, 2681, 281, 497], "temperature": 0.0, "avg_logprob": -0.22174830185739616, "compression_ratio": 1.4090909090909092, "no_speech_prob": 2.7264552500128048e-06}, {"id": 820, "seek": 390304, "start": 3913.72, "end": 3918.96, "text": " Okay, let's move on thanks for the great questions. I want to talk about", "tokens": [1033, 11, 718, 311, 1286, 322, 3231, 337, 264, 869, 1651, 13, 286, 528, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.22174830185739616, "compression_ratio": 1.4090909090909092, "no_speech_prob": 2.7264552500128048e-06}, {"id": 821, "seek": 390304, "start": 3923.92, "end": 3925.72, "text": " Embedding I", "tokens": [24234, 292, 3584, 286], "temperature": 0.0, "avg_logprob": -0.22174830185739616, "compression_ratio": 1.4090909090909092, "no_speech_prob": 2.7264552500128048e-06}, {"id": 822, "seek": 390304, "start": 3925.72, "end": 3929.2, "text": " Said pretend its linear and indeed we can pretend its linear", "tokens": [26490, 11865, 1080, 8213, 293, 6451, 321, 393, 11865, 1080, 8213], "temperature": 0.0, "avg_logprob": -0.22174830185739616, "compression_ratio": 1.4090909090909092, "no_speech_prob": 2.7264552500128048e-06}, {"id": 823, "seek": 392920, "start": 3929.2, "end": 3934.02, "text": " Let me show you how much we can pretend its linear as in n n dot linear create a linear layer", "tokens": [961, 385, 855, 291, 577, 709, 321, 393, 11865, 1080, 8213, 382, 294, 297, 297, 5893, 8213, 1884, 257, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.245330810546875, "compression_ratio": 1.5470588235294118, "no_speech_prob": 5.95510755374562e-06}, {"id": 824, "seek": 392920, "start": 3935.3199999999997, "end": 3937.3199999999997, "text": " Here is our", "tokens": [1692, 307, 527], "temperature": 0.0, "avg_logprob": -0.245330810546875, "compression_ratio": 1.5470588235294118, "no_speech_prob": 5.95510755374562e-06}, {"id": 825, "seek": 392920, "start": 3937.7599999999998, "end": 3939.56, "text": " Data matrix", "tokens": [11888, 8141], "temperature": 0.0, "avg_logprob": -0.245330810546875, "compression_ratio": 1.5470588235294118, "no_speech_prob": 5.95510755374562e-06}, {"id": 826, "seek": 392920, "start": 3939.56, "end": 3944.48, "text": " All right here are our coefficients if we're doing the R version here our coefficients are", "tokens": [1057, 558, 510, 366, 527, 31994, 498, 321, 434, 884, 264, 497, 3037, 510, 527, 31994, 366], "temperature": 0.0, "avg_logprob": -0.245330810546875, "compression_ratio": 1.5470588235294118, "no_speech_prob": 5.95510755374562e-06}, {"id": 827, "seek": 392920, "start": 3945.64, "end": 3947.96, "text": " right, so if we were to", "tokens": [558, 11, 370, 498, 321, 645, 281], "temperature": 0.0, "avg_logprob": -0.245330810546875, "compression_ratio": 1.5470588235294118, "no_speech_prob": 5.95510755374562e-06}, {"id": 828, "seek": 392920, "start": 3951.2, "end": 3953.2, "text": " Put those into a column vector", "tokens": [4935, 729, 666, 257, 7738, 8062], "temperature": 0.0, "avg_logprob": -0.245330810546875, "compression_ratio": 1.5470588235294118, "no_speech_prob": 5.95510755374562e-06}, {"id": 829, "seek": 395320, "start": 3953.2, "end": 3961.64, "text": " Like so right then we could do a matrix multiply of that", "tokens": [1743, 370, 558, 550, 321, 727, 360, 257, 8141, 12972, 295, 300], "temperature": 0.0, "avg_logprob": -0.2508848190307617, "compression_ratio": 1.287037037037037, "no_speech_prob": 1.7603391597731388e-06}, {"id": 830, "seek": 395320, "start": 3963.3999999999996, "end": 3967.96, "text": " By that right and so we're going to end up with", "tokens": [3146, 300, 558, 293, 370, 321, 434, 516, 281, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.2508848190307617, "compression_ratio": 1.287037037037037, "no_speech_prob": 1.7603391597731388e-06}, {"id": 831, "seek": 395320, "start": 3972.3999999999996, "end": 3976.56, "text": " So here's our matrix is our vector", "tokens": [407, 510, 311, 527, 8141, 307, 527, 8062], "temperature": 0.0, "avg_logprob": -0.2508848190307617, "compression_ratio": 1.287037037037037, "no_speech_prob": 1.7603391597731388e-06}, {"id": 832, "seek": 397656, "start": 3976.56, "end": 3979.56, "text": " All right, so we're going to end up with", "tokens": [1057, 558, 11, 370, 321, 434, 516, 281, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.5157643334340241, "compression_ratio": 1.9298245614035088, "no_speech_prob": 5.7718739299161825e-06}, {"id": 833, "seek": 397656, "start": 3980.48, "end": 3987.12, "text": " One times one plus one times one one times one one times three", "tokens": [1485, 1413, 472, 1804, 472, 1413, 472, 472, 1413, 472, 472, 1413, 1045], "temperature": 0.0, "avg_logprob": -0.5157643334340241, "compression_ratio": 1.9298245614035088, "no_speech_prob": 5.7718739299161825e-06}, {"id": 834, "seek": 397656, "start": 3993.64, "end": 3997.36, "text": " All right zero times one zero times point three", "tokens": [1057, 558, 4018, 1413, 472, 4018, 1413, 935, 1045], "temperature": 0.0, "avg_logprob": -0.5157643334340241, "compression_ratio": 1.9298245614035088, "no_speech_prob": 5.7718739299161825e-06}, {"id": 835, "seek": 397656, "start": 3999.0, "end": 4002.84, "text": " All right, and then the next one zero times one one times one so far", "tokens": [1057, 558, 11, 293, 550, 264, 958, 472, 4018, 1413, 472, 472, 1413, 472, 370, 1400], "temperature": 0.0, "avg_logprob": -0.5157643334340241, "compression_ratio": 1.9298245614035088, "no_speech_prob": 5.7718739299161825e-06}, {"id": 836, "seek": 400284, "start": 4002.84, "end": 4008.48, "text": " All right, and then the next one zero times one one times one so forth okay, so like that the matrix multiply", "tokens": [1057, 558, 11, 293, 550, 264, 958, 472, 4018, 1413, 472, 472, 1413, 472, 370, 5220, 1392, 11, 370, 411, 300, 264, 8141, 12972], "temperature": 0.0, "avg_logprob": -0.19607241610263257, "compression_ratio": 1.7194570135746607, "no_speech_prob": 2.726458887991612e-06}, {"id": 837, "seek": 400284, "start": 4010.32, "end": 4012.32, "text": " You know of", "tokens": [509, 458, 295], "temperature": 0.0, "avg_logprob": -0.19607241610263257, "compression_ratio": 1.7194570135746607, "no_speech_prob": 2.726458887991612e-06}, {"id": 838, "seek": 400284, "start": 4012.36, "end": 4014.36, "text": " this independent", "tokens": [341, 6695], "temperature": 0.0, "avg_logprob": -0.19607241610263257, "compression_ratio": 1.7194570135746607, "no_speech_prob": 2.726458887991612e-06}, {"id": 839, "seek": 400284, "start": 4015.56, "end": 4023.86, "text": " Variable matrix by this coefficient matrix is going to give us an answer okay, so that's that is just a matrix multiply", "tokens": [32511, 712, 8141, 538, 341, 17619, 8141, 307, 516, 281, 976, 505, 364, 1867, 1392, 11, 370, 300, 311, 300, 307, 445, 257, 8141, 12972], "temperature": 0.0, "avg_logprob": -0.19607241610263257, "compression_ratio": 1.7194570135746607, "no_speech_prob": 2.726458887991612e-06}, {"id": 840, "seek": 402386, "start": 4023.86, "end": 4032.1, "text": " So the question is like okay, well why didn't Jeremy write and end up linear? Why did Jeremy write and end up embedding?", "tokens": [407, 264, 1168, 307, 411, 1392, 11, 731, 983, 994, 380, 17809, 2464, 293, 917, 493, 8213, 30, 1545, 630, 17809, 2464, 293, 917, 493, 12240, 3584, 30], "temperature": 0.0, "avg_logprob": -0.22777380101821001, "compression_ratio": 1.6979166666666667, "no_speech_prob": 6.083580501581309e-07}, {"id": 841, "seek": 402386, "start": 4032.98, "end": 4038.7400000000002, "text": " And the reason is because if you recall we don't actually store it like this", "tokens": [400, 264, 1778, 307, 570, 498, 291, 9901, 321, 500, 380, 767, 3531, 309, 411, 341], "temperature": 0.0, "avg_logprob": -0.22777380101821001, "compression_ratio": 1.6979166666666667, "no_speech_prob": 6.083580501581309e-07}, {"id": 842, "seek": 402386, "start": 4039.3, "end": 4041.3, "text": " Because this is actually of width", "tokens": [1436, 341, 307, 767, 295, 11402], "temperature": 0.0, "avg_logprob": -0.22777380101821001, "compression_ratio": 1.6979166666666667, "no_speech_prob": 6.083580501581309e-07}, {"id": 843, "seek": 402386, "start": 4042.1800000000003, "end": 4044.48, "text": " 800,000 and of height", "tokens": [13083, 11, 1360, 293, 295, 6681], "temperature": 0.0, "avg_logprob": -0.22777380101821001, "compression_ratio": 1.6979166666666667, "no_speech_prob": 6.083580501581309e-07}, {"id": 844, "seek": 402386, "start": 4045.7000000000003, "end": 4049.2200000000003, "text": " 25,000 right so rather than storing it like this", "tokens": [3552, 11, 1360, 558, 370, 2831, 813, 26085, 309, 411, 341], "temperature": 0.0, "avg_logprob": -0.22777380101821001, "compression_ratio": 1.6979166666666667, "no_speech_prob": 6.083580501581309e-07}, {"id": 845, "seek": 404922, "start": 4049.22, "end": 4052.8599999999997, "text": " We actually store it as zero", "tokens": [492, 767, 3531, 309, 382, 4018], "temperature": 0.0, "avg_logprob": -0.43996875516829953, "compression_ratio": 1.3846153846153846, "no_speech_prob": 5.338135906640673e-06}, {"id": 846, "seek": 404922, "start": 4056.14, "end": 4065.14, "text": " One two three right one two three four zero one two five", "tokens": [1485, 732, 1045, 558, 472, 732, 1045, 1451, 4018, 472, 732, 1732], "temperature": 0.0, "avg_logprob": -0.43996875516829953, "compression_ratio": 1.3846153846153846, "no_speech_prob": 5.338135906640673e-06}, {"id": 847, "seek": 404922, "start": 4066.8599999999997, "end": 4071.18, "text": " One two four five okay", "tokens": [1485, 732, 1451, 1732, 1392], "temperature": 0.0, "avg_logprob": -0.43996875516829953, "compression_ratio": 1.3846153846153846, "no_speech_prob": 5.338135906640673e-06}, {"id": 848, "seek": 407118, "start": 4071.18, "end": 4079.7799999999997, "text": " All the ones one oops that's actually how we store it that is this bag of words contains", "tokens": [1057, 264, 2306, 472, 34166, 300, 311, 767, 577, 321, 3531, 309, 300, 307, 341, 3411, 295, 2283, 8306], "temperature": 0.0, "avg_logprob": -0.268478051940007, "compression_ratio": 1.48125, "no_speech_prob": 1.2289142432564404e-06}, {"id": 849, "seek": 407118, "start": 4080.58, "end": 4082.58, "text": " Which word indexes?", "tokens": [3013, 1349, 8186, 279, 30], "temperature": 0.0, "avg_logprob": -0.268478051940007, "compression_ratio": 1.48125, "no_speech_prob": 1.2289142432564404e-06}, {"id": 850, "seek": 407118, "start": 4083.3399999999997, "end": 4086.74, "text": " That make sense okay, so that's like", "tokens": [663, 652, 2020, 1392, 11, 370, 300, 311, 411], "temperature": 0.0, "avg_logprob": -0.268478051940007, "compression_ratio": 1.48125, "no_speech_prob": 1.2289142432564404e-06}, {"id": 851, "seek": 407118, "start": 4088.22, "end": 4090.22, "text": " This is like a sparse way of", "tokens": [639, 307, 411, 257, 637, 11668, 636, 295], "temperature": 0.0, "avg_logprob": -0.268478051940007, "compression_ratio": 1.48125, "no_speech_prob": 1.2289142432564404e-06}, {"id": 852, "seek": 407118, "start": 4091.7799999999997, "end": 4096.98, "text": " Storing it right is just list out the indexes in each sentence", "tokens": [745, 3662, 309, 558, 307, 445, 1329, 484, 264, 8186, 279, 294, 1184, 8174], "temperature": 0.0, "avg_logprob": -0.268478051940007, "compression_ratio": 1.48125, "no_speech_prob": 1.2289142432564404e-06}, {"id": 853, "seek": 409698, "start": 4096.98, "end": 4099.98, "text": " So given that I", "tokens": [407, 2212, 300, 286], "temperature": 0.0, "avg_logprob": -0.23689582347869872, "compression_ratio": 1.5, "no_speech_prob": 7.224431897157046e-07}, {"id": 854, "seek": 409698, "start": 4100.74, "end": 4106.98, "text": " Want to now do that matrix multiply that I just showed you to create that same?", "tokens": [11773, 281, 586, 360, 300, 8141, 12972, 300, 286, 445, 4712, 291, 281, 1884, 300, 912, 30], "temperature": 0.0, "avg_logprob": -0.23689582347869872, "compression_ratio": 1.5, "no_speech_prob": 7.224431897157046e-07}, {"id": 855, "seek": 409698, "start": 4107.94, "end": 4109.0599999999995, "text": " outcome", "tokens": [9700], "temperature": 0.0, "avg_logprob": -0.23689582347869872, "compression_ratio": 1.5, "no_speech_prob": 7.224431897157046e-07}, {"id": 856, "seek": 409698, "start": 4109.0599999999995, "end": 4112.339999999999, "text": " Right, but I want to do it from this representation", "tokens": [1779, 11, 457, 286, 528, 281, 360, 309, 490, 341, 10290], "temperature": 0.0, "avg_logprob": -0.23689582347869872, "compression_ratio": 1.5, "no_speech_prob": 7.224431897157046e-07}, {"id": 857, "seek": 409698, "start": 4114.0599999999995, "end": 4116.0599999999995, "text": " So if you think about it", "tokens": [407, 498, 291, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.23689582347869872, "compression_ratio": 1.5, "no_speech_prob": 7.224431897157046e-07}, {"id": 858, "seek": 409698, "start": 4116.98, "end": 4119.219999999999, "text": " All this is actually doing is", "tokens": [1057, 341, 307, 767, 884, 307], "temperature": 0.0, "avg_logprob": -0.23689582347869872, "compression_ratio": 1.5, "no_speech_prob": 7.224431897157046e-07}, {"id": 859, "seek": 409698, "start": 4120.459999999999, "end": 4126.0199999999995, "text": " It's saying a one-hot. You know this is basically one-hot encoded right", "tokens": [467, 311, 1566, 257, 472, 12, 12194, 13, 509, 458, 341, 307, 1936, 472, 12, 12194, 2058, 12340, 558], "temperature": 0.0, "avg_logprob": -0.23689582347869872, "compression_ratio": 1.5, "no_speech_prob": 7.224431897157046e-07}, {"id": 860, "seek": 412602, "start": 4126.02, "end": 4131.660000000001, "text": " It's kind of like a dummy dummy matrix version does it have the word this does it have the word movie does it have the word?", "tokens": [467, 311, 733, 295, 411, 257, 35064, 35064, 8141, 3037, 775, 309, 362, 264, 1349, 341, 775, 309, 362, 264, 1349, 3169, 775, 309, 362, 264, 1349, 30], "temperature": 0.0, "avg_logprob": -0.23872079044939523, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.5779575051055872e-06}, {"id": 861, "seek": 412602, "start": 4131.660000000001, "end": 4133.660000000001, "text": " Is and so forth?", "tokens": [1119, 293, 370, 5220, 30], "temperature": 0.0, "avg_logprob": -0.23872079044939523, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.5779575051055872e-06}, {"id": 862, "seek": 412602, "start": 4134.1, "end": 4138.700000000001, "text": " So if we took the simple version of like does it have the word this one oh?", "tokens": [407, 498, 321, 1890, 264, 2199, 3037, 295, 411, 775, 309, 362, 264, 1349, 341, 472, 1954, 30], "temperature": 0.0, "avg_logprob": -0.23872079044939523, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.5779575051055872e-06}, {"id": 863, "seek": 412602, "start": 4141.14, "end": 4143.620000000001, "text": " Right and we multiplied that", "tokens": [1779, 293, 321, 17207, 300], "temperature": 0.0, "avg_logprob": -0.23872079044939523, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.5779575051055872e-06}, {"id": 864, "seek": 412602, "start": 4146.06, "end": 4148.06, "text": " By that", "tokens": [3146, 300], "temperature": 0.0, "avg_logprob": -0.23872079044939523, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.5779575051055872e-06}, {"id": 865, "seek": 412602, "start": 4148.42, "end": 4152.9400000000005, "text": " Right then that's just going to return the first item", "tokens": [1779, 550, 300, 311, 445, 516, 281, 2736, 264, 700, 3174], "temperature": 0.0, "avg_logprob": -0.23872079044939523, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.5779575051055872e-06}, {"id": 866, "seek": 415294, "start": 4152.94, "end": 4154.66, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.2811286521680427, "compression_ratio": 1.5410958904109588, "no_speech_prob": 5.896400239180366e-07}, {"id": 867, "seek": 415294, "start": 4154.66, "end": 4156.66, "text": " Makes sense", "tokens": [25245, 2020], "temperature": 0.0, "avg_logprob": -0.2811286521680427, "compression_ratio": 1.5410958904109588, "no_speech_prob": 5.896400239180366e-07}, {"id": 868, "seek": 415294, "start": 4157.78, "end": 4159.78, "text": " So in general a", "tokens": [407, 294, 2674, 257], "temperature": 0.0, "avg_logprob": -0.2811286521680427, "compression_ratio": 1.5410958904109588, "no_speech_prob": 5.896400239180366e-07}, {"id": 869, "seek": 415294, "start": 4160.98, "end": 4162.98, "text": " one hot encoded", "tokens": [472, 2368, 2058, 12340], "temperature": 0.0, "avg_logprob": -0.2811286521680427, "compression_ratio": 1.5410958904109588, "no_speech_prob": 5.896400239180366e-07}, {"id": 870, "seek": 415294, "start": 4163.58, "end": 4164.78, "text": " vector", "tokens": [8062], "temperature": 0.0, "avg_logprob": -0.2811286521680427, "compression_ratio": 1.5410958904109588, "no_speech_prob": 5.896400239180366e-07}, {"id": 871, "seek": 415294, "start": 4164.78, "end": 4166.78, "text": " times a matrix is", "tokens": [1413, 257, 8141, 307], "temperature": 0.0, "avg_logprob": -0.2811286521680427, "compression_ratio": 1.5410958904109588, "no_speech_prob": 5.896400239180366e-07}, {"id": 872, "seek": 415294, "start": 4167.5, "end": 4173.66, "text": " Identical to to looking up that matrix to find the nth row in it", "tokens": [25905, 804, 281, 281, 1237, 493, 300, 8141, 281, 915, 264, 297, 392, 5386, 294, 309], "temperature": 0.0, "avg_logprob": -0.2811286521680427, "compression_ratio": 1.5410958904109588, "no_speech_prob": 5.896400239180366e-07}, {"id": 873, "seek": 415294, "start": 4174.139999999999, "end": 4179.299999999999, "text": " Right so this is identical to saying find the zero first second and fifth", "tokens": [1779, 370, 341, 307, 14800, 281, 1566, 915, 264, 4018, 700, 1150, 293, 9266], "temperature": 0.0, "avg_logprob": -0.2811286521680427, "compression_ratio": 1.5410958904109588, "no_speech_prob": 5.896400239180366e-07}, {"id": 874, "seek": 417930, "start": 4179.3, "end": 4181.3, "text": " coefficients", "tokens": [31994], "temperature": 0.0, "avg_logprob": -0.2190092219862827, "compression_ratio": 1.835978835978836, "no_speech_prob": 8.579203836234228e-07}, {"id": 875, "seek": 417930, "start": 4182.46, "end": 4189.66, "text": " Right so they're the same they're exactly the same thing and like it doesn't like in this case. I only have one", "tokens": [1779, 370, 436, 434, 264, 912, 436, 434, 2293, 264, 912, 551, 293, 411, 309, 1177, 380, 411, 294, 341, 1389, 13, 286, 787, 362, 472], "temperature": 0.0, "avg_logprob": -0.2190092219862827, "compression_ratio": 1.835978835978836, "no_speech_prob": 8.579203836234228e-07}, {"id": 876, "seek": 417930, "start": 4190.9800000000005, "end": 4195.62, "text": " Coefficient per feature right but actually the way I did this was to have", "tokens": [3066, 68, 7816, 680, 4111, 558, 457, 767, 264, 636, 286, 630, 341, 390, 281, 362], "temperature": 0.0, "avg_logprob": -0.2190092219862827, "compression_ratio": 1.835978835978836, "no_speech_prob": 8.579203836234228e-07}, {"id": 877, "seek": 417930, "start": 4197.66, "end": 4200.9800000000005, "text": " One coefficient per feature for each class", "tokens": [1485, 17619, 680, 4111, 337, 1184, 1508], "temperature": 0.0, "avg_logprob": -0.2190092219862827, "compression_ratio": 1.835978835978836, "no_speech_prob": 8.579203836234228e-07}, {"id": 878, "seek": 417930, "start": 4201.78, "end": 4203.9800000000005, "text": " Right so in this case is both positive and negative", "tokens": [1779, 370, 294, 341, 1389, 307, 1293, 3353, 293, 3671], "temperature": 0.0, "avg_logprob": -0.2190092219862827, "compression_ratio": 1.835978835978836, "no_speech_prob": 8.579203836234228e-07}, {"id": 879, "seek": 420398, "start": 4203.98, "end": 4209.74, "text": " So I actually had kind of like an R positive and an R negative", "tokens": [407, 286, 767, 632, 733, 295, 411, 364, 497, 3353, 293, 364, 497, 3671], "temperature": 0.0, "avg_logprob": -0.24826366251165216, "compression_ratio": 1.5495495495495495, "no_speech_prob": 2.4824691990943393e-06}, {"id": 880, "seek": 420398, "start": 4210.0599999999995, "end": 4213.339999999999, "text": " So our negative would be just the opposite right equals that", "tokens": [407, 527, 3671, 576, 312, 445, 264, 6182, 558, 6915, 300], "temperature": 0.0, "avg_logprob": -0.24826366251165216, "compression_ratio": 1.5495495495495495, "no_speech_prob": 2.4824691990943393e-06}, {"id": 881, "seek": 420398, "start": 4214.379999999999, "end": 4216.379999999999, "text": " divided by that", "tokens": [6666, 538, 300], "temperature": 0.0, "avg_logprob": -0.24826366251165216, "compression_ratio": 1.5495495495495495, "no_speech_prob": 2.4824691990943393e-06}, {"id": 882, "seek": 420398, "start": 4216.5, "end": 4221.419999999999, "text": " Now the binary case obviously it's redundant to have both, but what if it was like", "tokens": [823, 264, 17434, 1389, 2745, 309, 311, 40997, 281, 362, 1293, 11, 457, 437, 498, 309, 390, 411], "temperature": 0.0, "avg_logprob": -0.24826366251165216, "compression_ratio": 1.5495495495495495, "no_speech_prob": 2.4824691990943393e-06}, {"id": 883, "seek": 420398, "start": 4222.259999999999, "end": 4225.879999999999, "text": " What's the author of this text is it?", "tokens": [708, 311, 264, 3793, 295, 341, 2487, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.24826366251165216, "compression_ratio": 1.5495495495495495, "no_speech_prob": 2.4824691990943393e-06}, {"id": 884, "seek": 420398, "start": 4226.98, "end": 4232.58, "text": " Jeremy or Savannah or Terrence right now? We've got three categories. We want three", "tokens": [17809, 420, 47902, 420, 6564, 10760, 558, 586, 30, 492, 600, 658, 1045, 10479, 13, 492, 528, 1045], "temperature": 0.0, "avg_logprob": -0.24826366251165216, "compression_ratio": 1.5495495495495495, "no_speech_prob": 2.4824691990943393e-06}, {"id": 885, "seek": 423258, "start": 4232.58, "end": 4234.38, "text": " values of R", "tokens": [4190, 295, 497], "temperature": 0.0, "avg_logprob": -0.25683451807776164, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.425461611390347e-06}, {"id": 886, "seek": 423258, "start": 4234.38, "end": 4240.26, "text": " Right so the nice thing is that in this sparse version you know you can just look up", "tokens": [1779, 370, 264, 1481, 551, 307, 300, 294, 341, 637, 11668, 3037, 291, 458, 291, 393, 445, 574, 493], "temperature": 0.0, "avg_logprob": -0.25683451807776164, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.425461611390347e-06}, {"id": 887, "seek": 423258, "start": 4240.5, "end": 4246.54, "text": " You know the zero and the first and the second and the fifth", "tokens": [509, 458, 264, 4018, 293, 264, 700, 293, 264, 1150, 293, 264, 9266], "temperature": 0.0, "avg_logprob": -0.25683451807776164, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.425461611390347e-06}, {"id": 888, "seek": 423258, "start": 4247.1, "end": 4249.1, "text": " Right and again. It's identical", "tokens": [1779, 293, 797, 13, 467, 311, 14800], "temperature": 0.0, "avg_logprob": -0.25683451807776164, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.425461611390347e-06}, {"id": 889, "seek": 423258, "start": 4249.9, "end": 4251.9, "text": " mathematically identical", "tokens": [44003, 14800], "temperature": 0.0, "avg_logprob": -0.25683451807776164, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.425461611390347e-06}, {"id": 890, "seek": 423258, "start": 4251.9, "end": 4254.18, "text": " to multiplying by a one hot encoded matrix", "tokens": [281, 30955, 538, 257, 472, 2368, 2058, 12340, 8141], "temperature": 0.0, "avg_logprob": -0.25683451807776164, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.425461611390347e-06}, {"id": 891, "seek": 423258, "start": 4254.9, "end": 4256.26, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.25683451807776164, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.425461611390347e-06}, {"id": 892, "seek": 423258, "start": 4256.26, "end": 4258.26, "text": " When you have sparse inputs", "tokens": [1133, 291, 362, 637, 11668, 15743], "temperature": 0.0, "avg_logprob": -0.25683451807776164, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.425461611390347e-06}, {"id": 893, "seek": 425826, "start": 4258.26, "end": 4262.14, "text": " It's obviously much much more efficient", "tokens": [467, 311, 2745, 709, 709, 544, 7148], "temperature": 0.0, "avg_logprob": -0.22231442587716238, "compression_ratio": 1.6866359447004609, "no_speech_prob": 9.42243332247017e-07}, {"id": 894, "seek": 425826, "start": 4263.18, "end": 4265.18, "text": " so this", "tokens": [370, 341], "temperature": 0.0, "avg_logprob": -0.22231442587716238, "compression_ratio": 1.6866359447004609, "no_speech_prob": 9.42243332247017e-07}, {"id": 895, "seek": 425826, "start": 4265.9400000000005, "end": 4267.9400000000005, "text": " computational trick", "tokens": [28270, 4282], "temperature": 0.0, "avg_logprob": -0.22231442587716238, "compression_ratio": 1.6866359447004609, "no_speech_prob": 9.42243332247017e-07}, {"id": 896, "seek": 425826, "start": 4268.38, "end": 4274.02, "text": " Which is mathematically identical to not conceptually analogous to mathematically identical to", "tokens": [3013, 307, 44003, 14800, 281, 406, 3410, 671, 16660, 563, 281, 44003, 14800, 281], "temperature": 0.0, "avg_logprob": -0.22231442587716238, "compression_ratio": 1.6866359447004609, "no_speech_prob": 9.42243332247017e-07}, {"id": 897, "seek": 425826, "start": 4274.54, "end": 4278.26, "text": " Multiplying by a one hot encoded matrix is called an embedding", "tokens": [31150, 7310, 538, 257, 472, 2368, 2058, 12340, 8141, 307, 1219, 364, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.22231442587716238, "compression_ratio": 1.6866359447004609, "no_speech_prob": 9.42243332247017e-07}, {"id": 898, "seek": 425826, "start": 4278.860000000001, "end": 4286.9800000000005, "text": " right, so I'm sure you've all heard or most of you probably heard about embeddings like word embeddings word to vec or glove or whatever and", "tokens": [558, 11, 370, 286, 478, 988, 291, 600, 439, 2198, 420, 881, 295, 291, 1391, 2198, 466, 12240, 29432, 411, 1349, 12240, 29432, 1349, 281, 42021, 420, 26928, 420, 2035, 293], "temperature": 0.0, "avg_logprob": -0.22231442587716238, "compression_ratio": 1.6866359447004609, "no_speech_prob": 9.42243332247017e-07}, {"id": 899, "seek": 428698, "start": 4286.98, "end": 4290.66, "text": " People love to make them sound like there's some", "tokens": [3432, 959, 281, 652, 552, 1626, 411, 456, 311, 512], "temperature": 0.0, "avg_logprob": -0.24709830226668392, "compression_ratio": 1.5047619047619047, "no_speech_prob": 2.601607320684707e-06}, {"id": 900, "seek": 428698, "start": 4291.94, "end": 4293.94, "text": " Amazing new complex", "tokens": [14165, 777, 3997], "temperature": 0.0, "avg_logprob": -0.24709830226668392, "compression_ratio": 1.5047619047619047, "no_speech_prob": 2.601607320684707e-06}, {"id": 901, "seek": 428698, "start": 4294.139999999999, "end": 4298.32, "text": " Neural net thing right they're not embedding means", "tokens": [1734, 1807, 2533, 551, 558, 436, 434, 406, 12240, 3584, 1355], "temperature": 0.0, "avg_logprob": -0.24709830226668392, "compression_ratio": 1.5047619047619047, "no_speech_prob": 2.601607320684707e-06}, {"id": 902, "seek": 428698, "start": 4299.419999999999, "end": 4306.299999999999, "text": " Make a multiplication by one hot encoded matrix faster by replacing it with a simple array lookup", "tokens": [4387, 257, 27290, 538, 472, 2368, 2058, 12340, 8141, 4663, 538, 19139, 309, 365, 257, 2199, 10225, 574, 1010], "temperature": 0.0, "avg_logprob": -0.24709830226668392, "compression_ratio": 1.5047619047619047, "no_speech_prob": 2.601607320684707e-06}, {"id": 903, "seek": 428698, "start": 4307.259999999999, "end": 4309.259999999999, "text": " Okay, so that's why I said", "tokens": [1033, 11, 370, 300, 311, 983, 286, 848], "temperature": 0.0, "avg_logprob": -0.24709830226668392, "compression_ratio": 1.5047619047619047, "no_speech_prob": 2.601607320684707e-06}, {"id": 904, "seek": 428698, "start": 4309.9, "end": 4315.78, "text": " you can think of this as if it said self dot w equals nn dot linear and", "tokens": [291, 393, 519, 295, 341, 382, 498, 309, 848, 2698, 5893, 261, 6915, 297, 77, 5893, 8213, 293], "temperature": 0.0, "avg_logprob": -0.24709830226668392, "compression_ratio": 1.5047619047619047, "no_speech_prob": 2.601607320684707e-06}, {"id": 905, "seek": 431578, "start": 4315.78, "end": 4320.259999999999, "text": " F plus one by one right because it actually does", "tokens": [479, 1804, 472, 538, 472, 558, 570, 309, 767, 775], "temperature": 0.0, "avg_logprob": -0.19153152812610974, "compression_ratio": 1.8350515463917525, "no_speech_prob": 2.726461616475717e-06}, {"id": 906, "seek": 431578, "start": 4321.0199999999995, "end": 4328.34, "text": " The same thing right it actually is a matrix with those dimensions this actually is a matrix with those dimensions", "tokens": [440, 912, 551, 558, 309, 767, 307, 257, 8141, 365, 729, 12819, 341, 767, 307, 257, 8141, 365, 729, 12819], "temperature": 0.0, "avg_logprob": -0.19153152812610974, "compression_ratio": 1.8350515463917525, "no_speech_prob": 2.726461616475717e-06}, {"id": 907, "seek": 431578, "start": 4328.34, "end": 4330.34, "text": " All right, it's a linear layer", "tokens": [1057, 558, 11, 309, 311, 257, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.19153152812610974, "compression_ratio": 1.8350515463917525, "no_speech_prob": 2.726461616475717e-06}, {"id": 908, "seek": 431578, "start": 4331.98, "end": 4338.96, "text": " But it's expecting that the input we're going to give it is not actually a one hot encoded matrix", "tokens": [583, 309, 311, 9650, 300, 264, 4846, 321, 434, 516, 281, 976, 309, 307, 406, 767, 257, 472, 2368, 2058, 12340, 8141], "temperature": 0.0, "avg_logprob": -0.19153152812610974, "compression_ratio": 1.8350515463917525, "no_speech_prob": 2.726461616475717e-06}, {"id": 909, "seek": 431578, "start": 4339.259999999999, "end": 4341.42, "text": " But it's actually a list of integers", "tokens": [583, 309, 311, 767, 257, 1329, 295, 41674], "temperature": 0.0, "avg_logprob": -0.19153152812610974, "compression_ratio": 1.8350515463917525, "no_speech_prob": 2.726461616475717e-06}, {"id": 910, "seek": 431578, "start": 4342.179999999999, "end": 4344.179999999999, "text": " Right the indexes for each", "tokens": [1779, 264, 8186, 279, 337, 1184], "temperature": 0.0, "avg_logprob": -0.19153152812610974, "compression_ratio": 1.8350515463917525, "no_speech_prob": 2.726461616475717e-06}, {"id": 911, "seek": 434418, "start": 4344.18, "end": 4350.200000000001, "text": " Word or for each item so you can see that the forward function in fast AI", "tokens": [8725, 420, 337, 1184, 3174, 370, 291, 393, 536, 300, 264, 2128, 2445, 294, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.21410760879516602, "compression_ratio": 1.55, "no_speech_prob": 2.295911372129922e-06}, {"id": 912, "seek": 434418, "start": 4350.740000000001, "end": 4353.12, "text": " Automatically gets for this learner", "tokens": [24619, 5030, 2170, 337, 341, 33347], "temperature": 0.0, "avg_logprob": -0.21410760879516602, "compression_ratio": 1.55, "no_speech_prob": 2.295911372129922e-06}, {"id": 913, "seek": 434418, "start": 4354.1, "end": 4357.34, "text": " The feature indexes right so they come from", "tokens": [440, 4111, 8186, 279, 558, 370, 436, 808, 490], "temperature": 0.0, "avg_logprob": -0.21410760879516602, "compression_ratio": 1.55, "no_speech_prob": 2.295911372129922e-06}, {"id": 914, "seek": 434418, "start": 4357.900000000001, "end": 4364.38, "text": " The sparse matrix automatically numpy makes it very easy to just grab those those indexes", "tokens": [440, 637, 11668, 8141, 6772, 1031, 8200, 1669, 309, 588, 1858, 281, 445, 4444, 729, 729, 8186, 279], "temperature": 0.0, "avg_logprob": -0.21410760879516602, "compression_ratio": 1.55, "no_speech_prob": 2.295911372129922e-06}, {"id": 915, "seek": 434418, "start": 4366.1, "end": 4370.46, "text": " Okay, so in other words there. We've got here. We've got a list of", "tokens": [1033, 11, 370, 294, 661, 2283, 456, 13, 492, 600, 658, 510, 13, 492, 600, 658, 257, 1329, 295], "temperature": 0.0, "avg_logprob": -0.21410760879516602, "compression_ratio": 1.55, "no_speech_prob": 2.295911372129922e-06}, {"id": 916, "seek": 437046, "start": 4370.46, "end": 4375.9800000000005, "text": " each word index of a of the eight hundred thousand that are in this document and", "tokens": [1184, 1349, 8186, 295, 257, 295, 264, 3180, 3262, 4714, 300, 366, 294, 341, 4166, 293], "temperature": 0.0, "avg_logprob": -0.21602567036946616, "compression_ratio": 1.6888888888888889, "no_speech_prob": 7.811465252416383e-07}, {"id": 917, "seek": 437046, "start": 4376.46, "end": 4382.3, "text": " So then this here says look up each of those in our embedding matrix", "tokens": [407, 550, 341, 510, 1619, 574, 493, 1184, 295, 729, 294, 527, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.21602567036946616, "compression_ratio": 1.6888888888888889, "no_speech_prob": 7.811465252416383e-07}, {"id": 918, "seek": 437046, "start": 4382.3, "end": 4387.58, "text": " Which is got eight hundred thousand rows and return each thing that you find", "tokens": [3013, 307, 658, 3180, 3262, 4714, 13241, 293, 2736, 1184, 551, 300, 291, 915], "temperature": 0.0, "avg_logprob": -0.21602567036946616, "compression_ratio": 1.6888888888888889, "no_speech_prob": 7.811465252416383e-07}, {"id": 919, "seek": 437046, "start": 4388.94, "end": 4390.3, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.21602567036946616, "compression_ratio": 1.6888888888888889, "no_speech_prob": 7.811465252416383e-07}, {"id": 920, "seek": 437046, "start": 4390.3, "end": 4393.5, "text": " so mathematically identical to", "tokens": [370, 44003, 14800, 281], "temperature": 0.0, "avg_logprob": -0.21602567036946616, "compression_ratio": 1.6888888888888889, "no_speech_prob": 7.811465252416383e-07}, {"id": 921, "seek": 437046, "start": 4394.66, "end": 4396.66, "text": " multiplying by the one hot encoded matrix", "tokens": [30955, 538, 264, 472, 2368, 2058, 12340, 8141], "temperature": 0.0, "avg_logprob": -0.21602567036946616, "compression_ratio": 1.6888888888888889, "no_speech_prob": 7.811465252416383e-07}, {"id": 922, "seek": 439666, "start": 4396.66, "end": 4398.66, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.21334614829411583, "compression_ratio": 1.441860465116279, "no_speech_prob": 1.3925413213655702e-06}, {"id": 923, "seek": 439666, "start": 4398.98, "end": 4405.26, "text": " Makes sense, so that's all an embedding is and so what that means is", "tokens": [25245, 2020, 11, 370, 300, 311, 439, 364, 12240, 3584, 307, 293, 370, 437, 300, 1355, 307], "temperature": 0.0, "avg_logprob": -0.21334614829411583, "compression_ratio": 1.441860465116279, "no_speech_prob": 1.3925413213655702e-06}, {"id": 924, "seek": 439666, "start": 4409.9, "end": 4411.9, "text": " We can now handle", "tokens": [492, 393, 586, 4813], "temperature": 0.0, "avg_logprob": -0.21334614829411583, "compression_ratio": 1.441860465116279, "no_speech_prob": 1.3925413213655702e-06}, {"id": 925, "seek": 439666, "start": 4412.82, "end": 4417.139999999999, "text": " Building any kind of model like a you know whatever kind of neural network", "tokens": [18974, 604, 733, 295, 2316, 411, 257, 291, 458, 2035, 733, 295, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.21334614829411583, "compression_ratio": 1.441860465116279, "no_speech_prob": 1.3925413213655702e-06}, {"id": 926, "seek": 439666, "start": 4417.7, "end": 4423.0599999999995, "text": " Where we have potentially very high cardinality categorical variables as our inputs", "tokens": [2305, 321, 362, 7263, 588, 1090, 2920, 259, 1860, 19250, 804, 9102, 382, 527, 15743], "temperature": 0.0, "avg_logprob": -0.21334614829411583, "compression_ratio": 1.441860465116279, "no_speech_prob": 1.3925413213655702e-06}, {"id": 927, "seek": 442306, "start": 4423.06, "end": 4425.06, "text": " We", "tokens": [492], "temperature": 0.0, "avg_logprob": -0.23848875466879313, "compression_ratio": 1.664835164835165, "no_speech_prob": 6.786710287087772e-07}, {"id": 928, "seek": 442306, "start": 4425.580000000001, "end": 4431.34, "text": " Can then just turn them into a numeric code between zero and the number of levels and", "tokens": [1664, 550, 445, 1261, 552, 666, 257, 7866, 299, 3089, 1296, 4018, 293, 264, 1230, 295, 4358, 293], "temperature": 0.0, "avg_logprob": -0.23848875466879313, "compression_ratio": 1.664835164835165, "no_speech_prob": 6.786710287087772e-07}, {"id": 929, "seek": 442306, "start": 4432.660000000001, "end": 4434.660000000001, "text": " then we can learn a", "tokens": [550, 321, 393, 1466, 257], "temperature": 0.0, "avg_logprob": -0.23848875466879313, "compression_ratio": 1.664835164835165, "no_speech_prob": 6.786710287087772e-07}, {"id": 930, "seek": 442306, "start": 4435.9400000000005, "end": 4437.9400000000005, "text": " you know a", "tokens": [291, 458, 257], "temperature": 0.0, "avg_logprob": -0.23848875466879313, "compression_ratio": 1.664835164835165, "no_speech_prob": 6.786710287087772e-07}, {"id": 931, "seek": 442306, "start": 4438.3, "end": 4442.02, "text": " Linear layer from that as if we had one hot encoded it", "tokens": [14670, 289, 4583, 490, 300, 382, 498, 321, 632, 472, 2368, 2058, 12340, 309], "temperature": 0.0, "avg_logprob": -0.23848875466879313, "compression_ratio": 1.664835164835165, "no_speech_prob": 6.786710287087772e-07}, {"id": 932, "seek": 442306, "start": 4443.3, "end": 4446.38, "text": " without ever actually constructing the one hot encoded version and", "tokens": [1553, 1562, 767, 39969, 264, 472, 2368, 2058, 12340, 3037, 293], "temperature": 0.0, "avg_logprob": -0.23848875466879313, "compression_ratio": 1.664835164835165, "no_speech_prob": 6.786710287087772e-07}, {"id": 933, "seek": 442306, "start": 4447.42, "end": 4451.3, "text": " Without ever actually doing that matrix multiply okay instead", "tokens": [9129, 1562, 767, 884, 300, 8141, 12972, 1392, 2602], "temperature": 0.0, "avg_logprob": -0.23848875466879313, "compression_ratio": 1.664835164835165, "no_speech_prob": 6.786710287087772e-07}, {"id": 934, "seek": 445130, "start": 4451.3, "end": 4456.12, "text": " We will just store the index version and simply do the array lookup", "tokens": [492, 486, 445, 3531, 264, 8186, 3037, 293, 2935, 360, 264, 10225, 574, 1010], "temperature": 0.0, "avg_logprob": -0.14824398888481988, "compression_ratio": 1.688073394495413, "no_speech_prob": 6.083574817239423e-07}, {"id": 935, "seek": 445130, "start": 4456.66, "end": 4459.58, "text": " Okay, and so the gradients that are flowing back", "tokens": [1033, 11, 293, 370, 264, 2771, 2448, 300, 366, 13974, 646], "temperature": 0.0, "avg_logprob": -0.14824398888481988, "compression_ratio": 1.688073394495413, "no_speech_prob": 6.083574817239423e-07}, {"id": 936, "seek": 445130, "start": 4459.9800000000005, "end": 4464.5, "text": " You know basically in the one hot encoded version everything that was a zero has no gradient", "tokens": [509, 458, 1936, 294, 264, 472, 2368, 2058, 12340, 3037, 1203, 300, 390, 257, 4018, 575, 572, 16235], "temperature": 0.0, "avg_logprob": -0.14824398888481988, "compression_ratio": 1.688073394495413, "no_speech_prob": 6.083574817239423e-07}, {"id": 937, "seek": 445130, "start": 4464.5, "end": 4471.5, "text": " So the gradients flowing back is best go to update the particular row of the embedding matrix that we used", "tokens": [407, 264, 2771, 2448, 13974, 646, 307, 1151, 352, 281, 5623, 264, 1729, 5386, 295, 264, 12240, 3584, 8141, 300, 321, 1143], "temperature": 0.0, "avg_logprob": -0.14824398888481988, "compression_ratio": 1.688073394495413, "no_speech_prob": 6.083574817239423e-07}, {"id": 938, "seek": 445130, "start": 4471.860000000001, "end": 4473.860000000001, "text": " Okay, and so", "tokens": [1033, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.14824398888481988, "compression_ratio": 1.688073394495413, "no_speech_prob": 6.083574817239423e-07}, {"id": 939, "seek": 445130, "start": 4474.34, "end": 4477.7, "text": " That's fundamentally important for NLP", "tokens": [663, 311, 17879, 1021, 337, 426, 45196], "temperature": 0.0, "avg_logprob": -0.14824398888481988, "compression_ratio": 1.688073394495413, "no_speech_prob": 6.083574817239423e-07}, {"id": 940, "seek": 447770, "start": 4477.7, "end": 4481.98, "text": " Just like here like you know I wanted to create a", "tokens": [1449, 411, 510, 411, 291, 458, 286, 1415, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.28038967739452014, "compression_ratio": 1.587962962962963, "no_speech_prob": 1.4593728110412485e-06}, {"id": 941, "seek": 447770, "start": 4482.7, "end": 4485.7, "text": " PyTorch model that would implement this", "tokens": [9953, 51, 284, 339, 2316, 300, 576, 4445, 341], "temperature": 0.0, "avg_logprob": -0.28038967739452014, "compression_ratio": 1.587962962962963, "no_speech_prob": 1.4593728110412485e-06}, {"id": 942, "seek": 447770, "start": 4486.42, "end": 4487.54, "text": " this", "tokens": [341], "temperature": 0.0, "avg_logprob": -0.28038967739452014, "compression_ratio": 1.587962962962963, "no_speech_prob": 1.4593728110412485e-06}, {"id": 943, "seek": 447770, "start": 4487.54, "end": 4490.34, "text": " ridiculously simple little equation right and", "tokens": [41358, 2199, 707, 5367, 558, 293], "temperature": 0.0, "avg_logprob": -0.28038967739452014, "compression_ratio": 1.587962962962963, "no_speech_prob": 1.4593728110412485e-06}, {"id": 944, "seek": 447770, "start": 4491.62, "end": 4498.62, "text": " To do it without this trick would have meant. I was beating in a 25,000 by a hatred so 800,000", "tokens": [1407, 360, 309, 1553, 341, 4282, 576, 362, 4140, 13, 286, 390, 13497, 294, 257, 3552, 11, 1360, 538, 257, 21890, 370, 13083, 11, 1360], "temperature": 0.0, "avg_logprob": -0.28038967739452014, "compression_ratio": 1.587962962962963, "no_speech_prob": 1.4593728110412485e-06}, {"id": 945, "seek": 447770, "start": 4499.66, "end": 4501.66, "text": " element array", "tokens": [4478, 10225], "temperature": 0.0, "avg_logprob": -0.28038967739452014, "compression_ratio": 1.587962962962963, "no_speech_prob": 1.4593728110412485e-06}, {"id": 946, "seek": 450166, "start": 4501.66, "end": 4507.38, "text": " Which would have been kind of crazy right and so this this trick allowed me to write you know", "tokens": [3013, 576, 362, 668, 733, 295, 3219, 558, 293, 370, 341, 341, 4282, 4350, 385, 281, 2464, 291, 458], "temperature": 0.0, "avg_logprob": -0.19381467155788257, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.0348500154577778e-06}, {"id": 947, "seek": 450166, "start": 4507.38, "end": 4510.28, "text": " You know I just replaced the word linear with embedding", "tokens": [509, 458, 286, 445, 10772, 264, 1349, 8213, 365, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.19381467155788257, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.0348500154577778e-06}, {"id": 948, "seek": 450166, "start": 4510.78, "end": 4515.7, "text": " Replace the thing that feeds the one hot encodings in with something that just feeds the indexes in", "tokens": [1300, 6742, 264, 551, 300, 23712, 264, 472, 2368, 2058, 378, 1109, 294, 365, 746, 300, 445, 23712, 264, 8186, 279, 294], "temperature": 0.0, "avg_logprob": -0.19381467155788257, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.0348500154577778e-06}, {"id": 949, "seek": 450166, "start": 4516.0599999999995, "end": 4520.0599999999995, "text": " And that was it that then it kept working and so this now trains", "tokens": [400, 300, 390, 309, 300, 550, 309, 4305, 1364, 293, 370, 341, 586, 16329], "temperature": 0.0, "avg_logprob": -0.19381467155788257, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.0348500154577778e-06}, {"id": 950, "seek": 450166, "start": 4521.139999999999, "end": 4523.38, "text": " You know in about a minute", "tokens": [509, 458, 294, 466, 257, 3456], "temperature": 0.0, "avg_logprob": -0.19381467155788257, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.0348500154577778e-06}, {"id": 951, "seek": 450166, "start": 4523.98, "end": 4525.98, "text": " per epoch", "tokens": [680, 30992, 339], "temperature": 0.0, "avg_logprob": -0.19381467155788257, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.0348500154577778e-06}, {"id": 952, "seek": 450166, "start": 4528.9, "end": 4530.66, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.19381467155788257, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.0348500154577778e-06}, {"id": 953, "seek": 453066, "start": 4530.66, "end": 4532.42, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2318371587724828, "compression_ratio": 1.5060240963855422, "no_speech_prob": 6.540351932926569e-06}, {"id": 954, "seek": 453066, "start": 4532.42, "end": 4538.0199999999995, "text": " What we can now do is we can now take this idea and apply it not just to language", "tokens": [708, 321, 393, 586, 360, 307, 321, 393, 586, 747, 341, 1558, 293, 3079, 309, 406, 445, 281, 2856], "temperature": 0.0, "avg_logprob": -0.2318371587724828, "compression_ratio": 1.5060240963855422, "no_speech_prob": 6.540351932926569e-06}, {"id": 955, "seek": 453066, "start": 4539.0199999999995, "end": 4541.9, "text": " But to anything right for example", "tokens": [583, 281, 1340, 558, 337, 1365], "temperature": 0.0, "avg_logprob": -0.2318371587724828, "compression_ratio": 1.5060240963855422, "no_speech_prob": 6.540351932926569e-06}, {"id": 956, "seek": 453066, "start": 4542.78, "end": 4546.5, "text": " predicting the sales of items at a grocery", "tokens": [32884, 264, 5763, 295, 4754, 412, 257, 14410], "temperature": 0.0, "avg_logprob": -0.2318371587724828, "compression_ratio": 1.5060240963855422, "no_speech_prob": 6.540351932926569e-06}, {"id": 957, "seek": 453066, "start": 4548.38, "end": 4550.38, "text": " Yes, where's the", "tokens": [1079, 11, 689, 311, 264], "temperature": 0.0, "avg_logprob": -0.2318371587724828, "compression_ratio": 1.5060240963855422, "no_speech_prob": 6.540351932926569e-06}, {"id": 958, "seek": 453066, "start": 4552.7, "end": 4556.54, "text": " Just a quick question, so we are not actually looking up anything right", "tokens": [1449, 257, 1702, 1168, 11, 370, 321, 366, 406, 767, 1237, 493, 1340, 558], "temperature": 0.0, "avg_logprob": -0.2318371587724828, "compression_ratio": 1.5060240963855422, "no_speech_prob": 6.540351932926569e-06}, {"id": 959, "seek": 455654, "start": 4556.54, "end": 4561.06, "text": " We are just saying that now that array with the indices that is the representation", "tokens": [492, 366, 445, 1566, 300, 586, 300, 10225, 365, 264, 43840, 300, 307, 264, 10290], "temperature": 0.0, "avg_logprob": -0.2173796828075122, "compression_ratio": 1.751219512195122, "no_speech_prob": 2.684160335775232e-06}, {"id": 960, "seek": 455654, "start": 4561.58, "end": 4566.04, "text": " So the represent so we are doing a lookup right the representation", "tokens": [407, 264, 2906, 370, 321, 366, 884, 257, 574, 1010, 558, 264, 10290], "temperature": 0.0, "avg_logprob": -0.2173796828075122, "compression_ratio": 1.751219512195122, "no_speech_prob": 2.684160335775232e-06}, {"id": 961, "seek": 455654, "start": 4566.04, "end": 4569.82, "text": " That's being stored it for the but for the bag of words is now not", "tokens": [663, 311, 885, 12187, 309, 337, 264, 457, 337, 264, 3411, 295, 2283, 307, 586, 406], "temperature": 0.0, "avg_logprob": -0.2173796828075122, "compression_ratio": 1.751219512195122, "no_speech_prob": 2.684160335775232e-06}, {"id": 962, "seek": 455654, "start": 4570.46, "end": 4576.0199999999995, "text": " 1 1 1 0 0 1 but 0 1 2 5 right and so then", "tokens": [502, 502, 502, 1958, 1958, 502, 457, 1958, 502, 568, 1025, 558, 293, 370, 550], "temperature": 0.0, "avg_logprob": -0.2173796828075122, "compression_ratio": 1.751219512195122, "no_speech_prob": 2.684160335775232e-06}, {"id": 963, "seek": 455654, "start": 4576.7, "end": 4578.7, "text": " We actually have to do our", "tokens": [492, 767, 362, 281, 360, 527], "temperature": 0.0, "avg_logprob": -0.2173796828075122, "compression_ratio": 1.751219512195122, "no_speech_prob": 2.684160335775232e-06}, {"id": 964, "seek": 455654, "start": 4579.42, "end": 4583.2, "text": " Matrix product right but rather than doing the matrix product. We look up", "tokens": [36274, 1674, 558, 457, 2831, 813, 884, 264, 8141, 1674, 13, 492, 574, 493], "temperature": 0.0, "avg_logprob": -0.2173796828075122, "compression_ratio": 1.751219512195122, "no_speech_prob": 2.684160335775232e-06}, {"id": 965, "seek": 458320, "start": 4583.2, "end": 4589.28, "text": " The zero thing and the first thing and the second thing and the fifth thing", "tokens": [440, 4018, 551, 293, 264, 700, 551, 293, 264, 1150, 551, 293, 264, 9266, 551], "temperature": 0.0, "avg_logprob": -0.1579299305760583, "compression_ratio": 1.9732620320855614, "no_speech_prob": 5.255330052023055e-06}, {"id": 966, "seek": 458320, "start": 4591.4, "end": 4594.639999999999, "text": " So that means we are still retaining the one hot encoded matrix", "tokens": [407, 300, 1355, 321, 366, 920, 34936, 264, 472, 2368, 2058, 12340, 8141], "temperature": 0.0, "avg_logprob": -0.1579299305760583, "compression_ratio": 1.9732620320855614, "no_speech_prob": 5.255330052023055e-06}, {"id": 967, "seek": 458320, "start": 4594.96, "end": 4601.86, "text": " No, we didn't there's no one hot encoded matrix used here. This is the one hot encoded matrix, which is not currently highlighted", "tokens": [883, 11, 321, 994, 380, 456, 311, 572, 472, 2368, 2058, 12340, 8141, 1143, 510, 13, 639, 307, 264, 472, 2368, 2058, 12340, 8141, 11, 597, 307, 406, 4362, 17173], "temperature": 0.0, "avg_logprob": -0.1579299305760583, "compression_ratio": 1.9732620320855614, "no_speech_prob": 5.255330052023055e-06}, {"id": 968, "seek": 458320, "start": 4603.96, "end": 4610.74, "text": " We've currently highlighted the list of indexes and the list of coefficients from the weight matrix", "tokens": [492, 600, 4362, 17173, 264, 1329, 295, 8186, 279, 293, 264, 1329, 295, 31994, 490, 264, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.1579299305760583, "compression_ratio": 1.9732620320855614, "no_speech_prob": 5.255330052023055e-06}, {"id": 969, "seek": 461074, "start": 4610.74, "end": 4612.74, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.25223744853159014, "compression_ratio": 1.6733668341708543, "no_speech_prob": 1.7330460195807973e-06}, {"id": 970, "seek": 461074, "start": 4615.46, "end": 4617.46, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.25223744853159014, "compression_ratio": 1.6733668341708543, "no_speech_prob": 1.7330460195807973e-06}, {"id": 971, "seek": 461074, "start": 4618.179999999999, "end": 4622.76, "text": " So what we're going to do now is we're kind of going to just go to go a step further and saying like", "tokens": [407, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 733, 295, 516, 281, 445, 352, 281, 352, 257, 1823, 3052, 293, 1566, 411], "temperature": 0.0, "avg_logprob": -0.25223744853159014, "compression_ratio": 1.6733668341708543, "no_speech_prob": 1.7330460195807973e-06}, {"id": 972, "seek": 461074, "start": 4623.179999999999, "end": 4625.179999999999, "text": " Let's not use a linear model at all", "tokens": [961, 311, 406, 764, 257, 8213, 2316, 412, 439], "temperature": 0.0, "avg_logprob": -0.25223744853159014, "compression_ratio": 1.6733668341708543, "no_speech_prob": 1.7330460195807973e-06}, {"id": 973, "seek": 461074, "start": 4625.74, "end": 4631.9, "text": " Let's use a multi-layer neural network right and let's have the input to that potentially be", "tokens": [961, 311, 764, 257, 4825, 12, 8376, 260, 18161, 3209, 558, 293, 718, 311, 362, 264, 4846, 281, 300, 7263, 312], "temperature": 0.0, "avg_logprob": -0.25223744853159014, "compression_ratio": 1.6733668341708543, "no_speech_prob": 1.7330460195807973e-06}, {"id": 974, "seek": 461074, "start": 4632.94, "end": 4634.94, "text": " include some categorical variables", "tokens": [4090, 512, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.25223744853159014, "compression_ratio": 1.6733668341708543, "no_speech_prob": 1.7330460195807973e-06}, {"id": 975, "seek": 461074, "start": 4635.099999999999, "end": 4638.3, "text": " right and those categorical variables we will just have as", "tokens": [558, 293, 729, 19250, 804, 9102, 321, 486, 445, 362, 382], "temperature": 0.0, "avg_logprob": -0.25223744853159014, "compression_ratio": 1.6733668341708543, "no_speech_prob": 1.7330460195807973e-06}, {"id": 976, "seek": 463830, "start": 4638.3, "end": 4640.3, "text": " Numeric indexes", "tokens": [426, 15583, 299, 8186, 279], "temperature": 0.0, "avg_logprob": -0.2261819494775979, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.237745431761141e-06}, {"id": 977, "seek": 463830, "start": 4642.42, "end": 4646.0, "text": " And so the first layer for those won't be a normal linear layer", "tokens": [400, 370, 264, 700, 4583, 337, 729, 1582, 380, 312, 257, 2710, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.2261819494775979, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.237745431761141e-06}, {"id": 978, "seek": 463830, "start": 4646.06, "end": 4652.62, "text": " There will be an embedding layer which we know behaves exactly like a linear layer mathematically", "tokens": [821, 486, 312, 364, 12240, 3584, 4583, 597, 321, 458, 36896, 2293, 411, 257, 8213, 4583, 44003], "temperature": 0.0, "avg_logprob": -0.2261819494775979, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.237745431761141e-06}, {"id": 979, "seek": 463830, "start": 4653.26, "end": 4659.42, "text": " And so then I hope will be that we can now use this to create a neural network for any kind of data", "tokens": [400, 370, 550, 286, 1454, 486, 312, 300, 321, 393, 586, 764, 341, 281, 1884, 257, 18161, 3209, 337, 604, 733, 295, 1412], "temperature": 0.0, "avg_logprob": -0.2261819494775979, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.237745431761141e-06}, {"id": 980, "seek": 463830, "start": 4660.06, "end": 4662.06, "text": " right and so", "tokens": [558, 293, 370], "temperature": 0.0, "avg_logprob": -0.2261819494775979, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.237745431761141e-06}, {"id": 981, "seek": 463830, "start": 4663.62, "end": 4665.26, "text": " There was a", "tokens": [821, 390, 257], "temperature": 0.0, "avg_logprob": -0.2261819494775979, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.237745431761141e-06}, {"id": 982, "seek": 463830, "start": 4665.26, "end": 4666.7, "text": " competition on", "tokens": [6211, 322], "temperature": 0.0, "avg_logprob": -0.2261819494775979, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.237745431761141e-06}, {"id": 983, "seek": 466670, "start": 4666.7, "end": 4672.179999999999, "text": " Kaggle a few years ago called Rossman, which is a German grocery chain", "tokens": [48751, 22631, 257, 1326, 924, 2057, 1219, 16140, 1601, 11, 597, 307, 257, 6521, 14410, 5021], "temperature": 0.0, "avg_logprob": -0.22703230524637613, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.7330430637230165e-06}, {"id": 984, "seek": 466670, "start": 4673.099999999999, "end": 4677.26, "text": " Where they asked to predict the sales of items in?", "tokens": [2305, 436, 2351, 281, 6069, 264, 5763, 295, 4754, 294, 30], "temperature": 0.0, "avg_logprob": -0.22703230524637613, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.7330430637230165e-06}, {"id": 985, "seek": 466670, "start": 4678.22, "end": 4679.66, "text": " in their stores", "tokens": [294, 641, 9512], "temperature": 0.0, "avg_logprob": -0.22703230524637613, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.7330430637230165e-06}, {"id": 986, "seek": 466670, "start": 4679.66, "end": 4686.0599999999995, "text": " Right and that included the mixture of categorical and continuous variables and in this paper by Gore and Burkhan", "tokens": [1779, 293, 300, 5556, 264, 9925, 295, 19250, 804, 293, 10957, 9102, 293, 294, 341, 3035, 538, 45450, 293, 7031, 74, 3451], "temperature": 0.0, "avg_logprob": -0.22703230524637613, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.7330430637230165e-06}, {"id": 987, "seek": 466670, "start": 4686.099999999999, "end": 4688.5, "text": " They described their third place winning entry", "tokens": [814, 7619, 641, 2636, 1081, 8224, 8729], "temperature": 0.0, "avg_logprob": -0.22703230524637613, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.7330430637230165e-06}, {"id": 988, "seek": 466670, "start": 4689.62, "end": 4694.0199999999995, "text": " Which was much simpler than the first place winning entry", "tokens": [3013, 390, 709, 18587, 813, 264, 700, 1081, 8224, 8729], "temperature": 0.0, "avg_logprob": -0.22703230524637613, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.7330430637230165e-06}, {"id": 989, "seek": 469402, "start": 4694.02, "end": 4696.660000000001, "text": " but nearly as good", "tokens": [457, 6217, 382, 665], "temperature": 0.0, "avg_logprob": -0.2275372714531131, "compression_ratio": 1.5947136563876652, "no_speech_prob": 1.602792508492712e-06}, {"id": 990, "seek": 469402, "start": 4697.34, "end": 4703.02, "text": " But much much simpler because they took advantage of this idea of what they call entity embeddings", "tokens": [583, 709, 709, 18587, 570, 436, 1890, 5002, 295, 341, 1558, 295, 437, 436, 818, 13977, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.2275372714531131, "compression_ratio": 1.5947136563876652, "no_speech_prob": 1.602792508492712e-06}, {"id": 991, "seek": 469402, "start": 4704.540000000001, "end": 4711.1, "text": " In the paper they they thought I think that they had invented this actually had been written before earlier by", "tokens": [682, 264, 3035, 436, 436, 1194, 286, 519, 300, 436, 632, 14479, 341, 767, 632, 668, 3720, 949, 3071, 538], "temperature": 0.0, "avg_logprob": -0.2275372714531131, "compression_ratio": 1.5947136563876652, "no_speech_prob": 1.602792508492712e-06}, {"id": 992, "seek": 469402, "start": 4711.740000000001, "end": 4716.9800000000005, "text": " Yoshio Benjio and his co-authors in another Kaggle competition which was predicting taxi destinations", "tokens": [38949, 1004, 3964, 73, 1004, 293, 702, 598, 12, 40198, 830, 294, 1071, 48751, 22631, 6211, 597, 390, 32884, 18984, 37787], "temperature": 0.0, "avg_logprob": -0.2275372714531131, "compression_ratio": 1.5947136563876652, "no_speech_prob": 1.602792508492712e-06}, {"id": 993, "seek": 469402, "start": 4717.900000000001, "end": 4719.900000000001, "text": " Although I will say I feel like", "tokens": [5780, 286, 486, 584, 286, 841, 411], "temperature": 0.0, "avg_logprob": -0.2275372714531131, "compression_ratio": 1.5947136563876652, "no_speech_prob": 1.602792508492712e-06}, {"id": 994, "seek": 471990, "start": 4719.9, "end": 4725.0199999999995, "text": " Gore went a lot further in describing how this can be", "tokens": [45450, 1437, 257, 688, 3052, 294, 16141, 577, 341, 393, 312], "temperature": 0.0, "avg_logprob": -0.23380493395256274, "compression_ratio": 1.5067567567567568, "no_speech_prob": 2.601588903416996e-06}, {"id": 995, "seek": 471990, "start": 4725.66, "end": 4727.66, "text": " used in many other ways", "tokens": [1143, 294, 867, 661, 2098], "temperature": 0.0, "avg_logprob": -0.23380493395256274, "compression_ratio": 1.5067567567567568, "no_speech_prob": 2.601588903416996e-06}, {"id": 996, "seek": 471990, "start": 4727.74, "end": 4731.139999999999, "text": " And so we'll we'll talk about that as well", "tokens": [400, 370, 321, 603, 321, 603, 751, 466, 300, 382, 731], "temperature": 0.0, "avg_logprob": -0.23380493395256274, "compression_ratio": 1.5067567567567568, "no_speech_prob": 2.601588903416996e-06}, {"id": 997, "seek": 471990, "start": 4733.339999999999, "end": 4734.98, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.23380493395256274, "compression_ratio": 1.5067567567567568, "no_speech_prob": 2.601588903416996e-06}, {"id": 998, "seek": 471990, "start": 4734.98, "end": 4736.98, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.23380493395256274, "compression_ratio": 1.5067567567567568, "no_speech_prob": 2.601588903416996e-06}, {"id": 999, "seek": 471990, "start": 4737.0199999999995, "end": 4742.799999999999, "text": " So this one is actually in the is in the deep learning one repo okay deal one", "tokens": [407, 341, 472, 307, 767, 294, 264, 307, 294, 264, 2452, 2539, 472, 49040, 1392, 2028, 472], "temperature": 0.0, "avg_logprob": -0.23380493395256274, "compression_ratio": 1.5067567567567568, "no_speech_prob": 2.601588903416996e-06}, {"id": 1000, "seek": 471990, "start": 4743.54, "end": 4745.54, "text": " Lesson three okay", "tokens": [18649, 266, 1045, 1392], "temperature": 0.0, "avg_logprob": -0.23380493395256274, "compression_ratio": 1.5067567567567568, "no_speech_prob": 2.601588903416996e-06}, {"id": 1001, "seek": 474554, "start": 4745.54, "end": 4750.82, "text": " Because we talk about some of the deep learning specific aspects in the deep learning course where else in this course we're going to be", "tokens": [1436, 321, 751, 466, 512, 295, 264, 2452, 2539, 2685, 7270, 294, 264, 2452, 2539, 1164, 689, 1646, 294, 341, 1164, 321, 434, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.16152624065956372, "compression_ratio": 1.7069767441860466, "no_speech_prob": 7.29624844097998e-06}, {"id": 1002, "seek": 474554, "start": 4751.38, "end": 4753.38, "text": " Talking mainly about the feature engineering", "tokens": [22445, 8704, 466, 264, 4111, 7043], "temperature": 0.0, "avg_logprob": -0.16152624065956372, "compression_ratio": 1.7069767441860466, "no_speech_prob": 7.29624844097998e-06}, {"id": 1003, "seek": 474554, "start": 4754.06, "end": 4759.86, "text": " And we're also going to be talking about you know kind of this this embedding idea", "tokens": [400, 321, 434, 611, 516, 281, 312, 1417, 466, 291, 458, 733, 295, 341, 341, 12240, 3584, 1558], "temperature": 0.0, "avg_logprob": -0.16152624065956372, "compression_ratio": 1.7069767441860466, "no_speech_prob": 7.29624844097998e-06}, {"id": 1004, "seek": 474554, "start": 4763.66, "end": 4766.7, "text": " So let's start with the data right so the data", "tokens": [407, 718, 311, 722, 365, 264, 1412, 558, 370, 264, 1412], "temperature": 0.0, "avg_logprob": -0.16152624065956372, "compression_ratio": 1.7069767441860466, "no_speech_prob": 7.29624844097998e-06}, {"id": 1005, "seek": 474554, "start": 4767.38, "end": 4769.9, "text": " Was you know store number one on?", "tokens": [3027, 291, 458, 3531, 1230, 472, 322, 30], "temperature": 0.0, "avg_logprob": -0.16152624065956372, "compression_ratio": 1.7069767441860466, "no_speech_prob": 7.29624844097998e-06}, {"id": 1006, "seek": 474554, "start": 4770.9, "end": 4772.9, "text": " the 31st of July 2015", "tokens": [264, 10353, 372, 295, 7370, 7546], "temperature": 0.0, "avg_logprob": -0.16152624065956372, "compression_ratio": 1.7069767441860466, "no_speech_prob": 7.29624844097998e-06}, {"id": 1007, "seek": 477290, "start": 4772.9, "end": 4774.9, "text": " was open", "tokens": [390, 1269], "temperature": 0.0, "avg_logprob": -0.16103874344423594, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.436742536498059e-06}, {"id": 1008, "seek": 477290, "start": 4776.0199999999995, "end": 4778.0199999999995, "text": " They had a promotion going on", "tokens": [814, 632, 257, 15783, 516, 322], "temperature": 0.0, "avg_logprob": -0.16103874344423594, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.436742536498059e-06}, {"id": 1009, "seek": 477290, "start": 4778.219999999999, "end": 4784.66, "text": " It was a school holiday. It was not a state holiday, and they sold five thousand two hundred and sixty three items", "tokens": [467, 390, 257, 1395, 9960, 13, 467, 390, 406, 257, 1785, 9960, 11, 293, 436, 3718, 1732, 4714, 732, 3262, 293, 21390, 1045, 4754], "temperature": 0.0, "avg_logprob": -0.16103874344423594, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.436742536498059e-06}, {"id": 1010, "seek": 477290, "start": 4786.74, "end": 4788.219999999999, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.16103874344423594, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.436742536498059e-06}, {"id": 1011, "seek": 477290, "start": 4788.219999999999, "end": 4789.86, "text": " That's the key", "tokens": [663, 311, 264, 2141], "temperature": 0.0, "avg_logprob": -0.16103874344423594, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.436742536498059e-06}, {"id": 1012, "seek": 477290, "start": 4789.86, "end": 4797.5199999999995, "text": " Data they provided and so the goal is obviously to predict sales in a test set that has the same information without sales", "tokens": [11888, 436, 5649, 293, 370, 264, 3387, 307, 2745, 281, 6069, 5763, 294, 257, 1500, 992, 300, 575, 264, 912, 1589, 1553, 5763], "temperature": 0.0, "avg_logprob": -0.16103874344423594, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.436742536498059e-06}, {"id": 1013, "seek": 479752, "start": 4797.52, "end": 4802.52, "text": " They also tell you that for each store", "tokens": [814, 611, 980, 291, 300, 337, 1184, 3531], "temperature": 0.0, "avg_logprob": -0.1708957619137234, "compression_ratio": 1.6031746031746033, "no_speech_prob": 1.1189373481101939e-06}, {"id": 1014, "seek": 479752, "start": 4803.52, "end": 4805.52, "text": " It's of some particular type", "tokens": [467, 311, 295, 512, 1729, 2010], "temperature": 0.0, "avg_logprob": -0.1708957619137234, "compression_ratio": 1.6031746031746033, "no_speech_prob": 1.1189373481101939e-06}, {"id": 1015, "seek": 479752, "start": 4806.280000000001, "end": 4808.76, "text": " It sells some particular assortment of goods", "tokens": [467, 20897, 512, 1729, 1256, 477, 518, 295, 10179], "temperature": 0.0, "avg_logprob": -0.1708957619137234, "compression_ratio": 1.6031746031746033, "no_speech_prob": 1.1189373481101939e-06}, {"id": 1016, "seek": 479752, "start": 4809.4800000000005, "end": 4812.92, "text": " Its nearest competitor to competitor is some distance away", "tokens": [6953, 23831, 27266, 281, 27266, 307, 512, 4560, 1314], "temperature": 0.0, "avg_logprob": -0.1708957619137234, "compression_ratio": 1.6031746031746033, "no_speech_prob": 1.1189373481101939e-06}, {"id": 1017, "seek": 479752, "start": 4813.6, "end": 4817.240000000001, "text": " the competitor opened in September 2008 and", "tokens": [264, 27266, 5625, 294, 7216, 10389, 293], "temperature": 0.0, "avg_logprob": -0.1708957619137234, "compression_ratio": 1.6031746031746033, "no_speech_prob": 1.1189373481101939e-06}, {"id": 1018, "seek": 479752, "start": 4818.280000000001, "end": 4822.68, "text": " There's some more information about promos. I don't know the details of what that means", "tokens": [821, 311, 512, 544, 1589, 466, 2234, 329, 13, 286, 500, 380, 458, 264, 4365, 295, 437, 300, 1355], "temperature": 0.0, "avg_logprob": -0.1708957619137234, "compression_ratio": 1.6031746031746033, "no_speech_prob": 1.1189373481101939e-06}, {"id": 1019, "seek": 482268, "start": 4822.68, "end": 4824.68, "text": " like", "tokens": [411], "temperature": 0.0, "avg_logprob": -0.19336951971054078, "compression_ratio": 1.7526315789473683, "no_speech_prob": 1.3287668707562261e-06}, {"id": 1020, "seek": 482268, "start": 4825.76, "end": 4828.76, "text": " In many Kaggle competitions they let you", "tokens": [682, 867, 48751, 22631, 26185, 436, 718, 291], "temperature": 0.0, "avg_logprob": -0.19336951971054078, "compression_ratio": 1.7526315789473683, "no_speech_prob": 1.3287668707562261e-06}, {"id": 1021, "seek": 482268, "start": 4829.56, "end": 4830.84, "text": " download", "tokens": [5484], "temperature": 0.0, "avg_logprob": -0.19336951971054078, "compression_ratio": 1.7526315789473683, "no_speech_prob": 1.3287668707562261e-06}, {"id": 1022, "seek": 482268, "start": 4830.84, "end": 4834.88, "text": " External data sets if you wish as long as you share them with other competitors", "tokens": [48277, 1412, 6352, 498, 291, 3172, 382, 938, 382, 291, 2073, 552, 365, 661, 18333], "temperature": 0.0, "avg_logprob": -0.19336951971054078, "compression_ratio": 1.7526315789473683, "no_speech_prob": 1.3287668707562261e-06}, {"id": 1023, "seek": 482268, "start": 4835.52, "end": 4842.56, "text": " So people oh they also told you what state each store is in so people downloaded a list of the names of the different states", "tokens": [407, 561, 1954, 436, 611, 1907, 291, 437, 1785, 1184, 3531, 307, 294, 370, 561, 21748, 257, 1329, 295, 264, 5288, 295, 264, 819, 4368], "temperature": 0.0, "avg_logprob": -0.19336951971054078, "compression_ratio": 1.7526315789473683, "no_speech_prob": 1.3287668707562261e-06}, {"id": 1024, "seek": 482268, "start": 4842.56, "end": 4843.8, "text": " of Germany", "tokens": [295, 7244], "temperature": 0.0, "avg_logprob": -0.19336951971054078, "compression_ratio": 1.7526315789473683, "no_speech_prob": 1.3287668707562261e-06}, {"id": 1025, "seek": 482268, "start": 4843.8, "end": 4848.6, "text": " They downloaded a file for each state in Germany for each week", "tokens": [814, 21748, 257, 3991, 337, 1184, 1785, 294, 7244, 337, 1184, 1243], "temperature": 0.0, "avg_logprob": -0.19336951971054078, "compression_ratio": 1.7526315789473683, "no_speech_prob": 1.3287668707562261e-06}, {"id": 1026, "seek": 484860, "start": 4848.6, "end": 4855.04, "text": " Some kind of Google Trend data. I don't know what specific Google Trend they got but there was that", "tokens": [2188, 733, 295, 3329, 37417, 1412, 13, 286, 500, 380, 458, 437, 2685, 3329, 37417, 436, 658, 457, 456, 390, 300], "temperature": 0.0, "avg_logprob": -0.21531445047129755, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.2887046472751535e-06}, {"id": 1027, "seek": 484860, "start": 4855.72, "end": 4858.88, "text": " For each date they downloaded a whole bunch of temperature information", "tokens": [1171, 1184, 4002, 436, 21748, 257, 1379, 3840, 295, 4292, 1589], "temperature": 0.0, "avg_logprob": -0.21531445047129755, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.2887046472751535e-06}, {"id": 1028, "seek": 484860, "start": 4862.4400000000005, "end": 4864.4400000000005, "text": " That's it and then here's the test set", "tokens": [663, 311, 309, 293, 550, 510, 311, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.21531445047129755, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.2887046472751535e-06}, {"id": 1029, "seek": 484860, "start": 4865.0, "end": 4868.8, "text": " Okay, so I mean one interesting insight here", "tokens": [1033, 11, 370, 286, 914, 472, 1880, 11269, 510], "temperature": 0.0, "avg_logprob": -0.21531445047129755, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.2887046472751535e-06}, {"id": 1030, "seek": 484860, "start": 4868.8, "end": 4872.160000000001, "text": " Is that there was probably a mistake in some ways for?", "tokens": [1119, 300, 456, 390, 1391, 257, 6146, 294, 512, 2098, 337, 30], "temperature": 0.0, "avg_logprob": -0.21531445047129755, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.2887046472751535e-06}, {"id": 1031, "seek": 484860, "start": 4872.6, "end": 4876.0, "text": " Rossman to design this competition as being one where you could use external data", "tokens": [16140, 1601, 281, 1715, 341, 6211, 382, 885, 472, 689, 291, 727, 764, 8320, 1412], "temperature": 0.0, "avg_logprob": -0.21531445047129755, "compression_ratio": 1.6090534979423867, "no_speech_prob": 3.2887046472751535e-06}, {"id": 1032, "seek": 487600, "start": 4876.0, "end": 4882.32, "text": " Because in reality you don't actually get to find out next week's weather or next week's Google Trends", "tokens": [1436, 294, 4103, 291, 500, 380, 767, 483, 281, 915, 484, 958, 1243, 311, 5503, 420, 958, 1243, 311, 3329, 37417, 82], "temperature": 0.0, "avg_logprob": -0.12254995828146463, "compression_ratio": 1.6593886462882097, "no_speech_prob": 3.0894493647792842e-06}, {"id": 1033, "seek": 487600, "start": 4882.76, "end": 4883.96, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.12254995828146463, "compression_ratio": 1.6593886462882097, "no_speech_prob": 3.0894493647792842e-06}, {"id": 1034, "seek": 487600, "start": 4883.96, "end": 4889.96, "text": " But you know when you're competing in Kaggle you don't care about that you just want to win so you use whatever you can get", "tokens": [583, 291, 458, 562, 291, 434, 15439, 294, 48751, 22631, 291, 500, 380, 1127, 466, 300, 291, 445, 528, 281, 1942, 370, 291, 764, 2035, 291, 393, 483], "temperature": 0.0, "avg_logprob": -0.12254995828146463, "compression_ratio": 1.6593886462882097, "no_speech_prob": 3.0894493647792842e-06}, {"id": 1035, "seek": 487600, "start": 4895.88, "end": 4898.16, "text": " So let's talk first of all about data cleaning", "tokens": [407, 718, 311, 751, 700, 295, 439, 466, 1412, 8924], "temperature": 0.0, "avg_logprob": -0.12254995828146463, "compression_ratio": 1.6593886462882097, "no_speech_prob": 3.0894493647792842e-06}, {"id": 1036, "seek": 487600, "start": 4898.16, "end": 4904.12, "text": " You know that there wasn't really much feature engineering done in this third place winning entry", "tokens": [509, 458, 300, 456, 2067, 380, 534, 709, 4111, 7043, 1096, 294, 341, 2636, 1081, 8224, 8729], "temperature": 0.0, "avg_logprob": -0.12254995828146463, "compression_ratio": 1.6593886462882097, "no_speech_prob": 3.0894493647792842e-06}, {"id": 1037, "seek": 490412, "start": 4904.12, "end": 4910.599999999999, "text": " Like by particularly by Kaggle standards where normally every last thing counts", "tokens": [1743, 538, 4098, 538, 48751, 22631, 7787, 689, 5646, 633, 1036, 551, 14893], "temperature": 0.0, "avg_logprob": -0.19334306388065733, "compression_ratio": 1.6626984126984128, "no_speech_prob": 5.954986590950284e-06}, {"id": 1038, "seek": 490412, "start": 4911.4, "end": 4915.24, "text": " This is a great example of how far you can get with with a neural net", "tokens": [639, 307, 257, 869, 1365, 295, 577, 1400, 291, 393, 483, 365, 365, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.19334306388065733, "compression_ratio": 1.6626984126984128, "no_speech_prob": 5.954986590950284e-06}, {"id": 1039, "seek": 490412, "start": 4915.64, "end": 4917.64, "text": " and it certainly reminds me of the", "tokens": [293, 309, 3297, 12025, 385, 295, 264], "temperature": 0.0, "avg_logprob": -0.19334306388065733, "compression_ratio": 1.6626984126984128, "no_speech_prob": 5.954986590950284e-06}, {"id": 1040, "seek": 490412, "start": 4918.0, "end": 4925.36, "text": " Plains prediction competition we talked about yesterday where the winner did no feature engineering entirely relied on deep learning", "tokens": [2149, 2315, 17630, 6211, 321, 2825, 466, 5186, 689, 264, 8507, 630, 572, 4111, 7043, 7696, 35463, 322, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.19334306388065733, "compression_ratio": 1.6626984126984128, "no_speech_prob": 5.954986590950284e-06}, {"id": 1041, "seek": 492536, "start": 4925.36, "end": 4929.92, "text": " The laughter in the room I guess is from people who did a little bit more than no feature engineering", "tokens": [440, 13092, 294, 264, 1808, 286, 2041, 307, 490, 561, 567, 630, 257, 707, 857, 544, 813, 572, 4111, 7043], "temperature": 0.0, "avg_logprob": -0.43403727213541665, "compression_ratio": 1.7028301886792452, "no_speech_prob": 4.356807494332315e-06}, {"id": 1042, "seek": 492536, "start": 4930.639999999999, "end": 4932.639999999999, "text": " in that competition", "tokens": [294, 300, 6211], "temperature": 0.0, "avg_logprob": -0.43403727213541665, "compression_ratio": 1.7028301886792452, "no_speech_prob": 4.356807494332315e-06}, {"id": 1043, "seek": 492536, "start": 4933.44, "end": 4935.12, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.43403727213541665, "compression_ratio": 1.7028301886792452, "no_speech_prob": 4.356807494332315e-06}, {"id": 1044, "seek": 492536, "start": 4935.12, "end": 4937.48, "text": " I should mention by the way like I", "tokens": [286, 820, 2152, 538, 264, 636, 411, 286], "temperature": 0.0, "avg_logprob": -0.43403727213541665, "compression_ratio": 1.7028301886792452, "no_speech_prob": 4.356807494332315e-06}, {"id": 1045, "seek": 492536, "start": 4938.24, "end": 4946.32, "text": " Find that bit where like you work hard at a competition and then it closes and you didn't win and the winner comes out and says", "tokens": [11809, 300, 857, 689, 411, 291, 589, 1152, 412, 257, 6211, 293, 550, 309, 24157, 293, 291, 994, 380, 1942, 293, 264, 8507, 1487, 484, 293, 1619], "temperature": 0.0, "avg_logprob": -0.43403727213541665, "compression_ratio": 1.7028301886792452, "no_speech_prob": 4.356807494332315e-06}, {"id": 1046, "seek": 492536, "start": 4946.32, "end": 4950.719999999999, "text": " This is how I won like that's the bit where you learn the most right like", "tokens": [639, 307, 577, 286, 1582, 411, 300, 311, 264, 857, 689, 291, 1466, 264, 881, 558, 411], "temperature": 0.0, "avg_logprob": -0.43403727213541665, "compression_ratio": 1.7028301886792452, "no_speech_prob": 4.356807494332315e-06}, {"id": 1047, "seek": 495072, "start": 4950.72, "end": 4955.0, "text": " Sometimes that's happened to me, and it's been like oh", "tokens": [4803, 300, 311, 2011, 281, 385, 11, 293, 309, 311, 668, 411, 1954], "temperature": 0.0, "avg_logprob": -0.2768050299750434, "compression_ratio": 1.7782426778242677, "no_speech_prob": 1.2098612387489993e-06}, {"id": 1048, "seek": 495072, "start": 4955.0, "end": 4959.6, "text": " I thought of that I thought I tried that and then I go back and I realize I", "tokens": [286, 1194, 295, 300, 286, 1194, 286, 3031, 300, 293, 550, 286, 352, 646, 293, 286, 4325, 286], "temperature": 0.0, "avg_logprob": -0.2768050299750434, "compression_ratio": 1.7782426778242677, "no_speech_prob": 1.2098612387489993e-06}, {"id": 1049, "seek": 495072, "start": 4960.2, "end": 4967.52, "text": " Like had a bug there. I didn't test properly and I learned like okay like I really need to learn to like test this thing", "tokens": [1743, 632, 257, 7426, 456, 13, 286, 994, 380, 1500, 6108, 293, 286, 3264, 411, 1392, 411, 286, 534, 643, 281, 1466, 281, 411, 1500, 341, 551], "temperature": 0.0, "avg_logprob": -0.2768050299750434, "compression_ratio": 1.7782426778242677, "no_speech_prob": 1.2098612387489993e-06}, {"id": 1050, "seek": 495072, "start": 4967.52, "end": 4969.52, "text": " In this different way", "tokens": [682, 341, 819, 636], "temperature": 0.0, "avg_logprob": -0.2768050299750434, "compression_ratio": 1.7782426778242677, "no_speech_prob": 1.2098612387489993e-06}, {"id": 1051, "seek": 495072, "start": 4969.52, "end": 4973.2, "text": " Sometimes it's like oh, I thought of that, but I assumed it wouldn't work", "tokens": [4803, 309, 311, 411, 1954, 11, 286, 1194, 295, 300, 11, 457, 286, 15895, 309, 2759, 380, 589], "temperature": 0.0, "avg_logprob": -0.2768050299750434, "compression_ratio": 1.7782426778242677, "no_speech_prob": 1.2098612387489993e-06}, {"id": 1052, "seek": 495072, "start": 4973.2, "end": 4976.84, "text": " I've really got to remember to check everything before I make any assumptions", "tokens": [286, 600, 534, 658, 281, 1604, 281, 1520, 1203, 949, 286, 652, 604, 17695], "temperature": 0.0, "avg_logprob": -0.2768050299750434, "compression_ratio": 1.7782426778242677, "no_speech_prob": 1.2098612387489993e-06}, {"id": 1053, "seek": 497684, "start": 4976.84, "end": 4981.72, "text": " And you know sometimes it's just like oh, I I did not think of that technique", "tokens": [400, 291, 458, 2171, 309, 311, 445, 411, 1954, 11, 286, 286, 630, 406, 519, 295, 300, 6532], "temperature": 0.0, "avg_logprob": -0.3959791660308838, "compression_ratio": 1.7595419847328244, "no_speech_prob": 2.1233679490251234e-06}, {"id": 1054, "seek": 497684, "start": 4982.4400000000005, "end": 4985.08, "text": " Wow now I know it's better than everything", "tokens": [3153, 586, 286, 458, 309, 311, 1101, 813, 1203], "temperature": 0.0, "avg_logprob": -0.3959791660308838, "compression_ratio": 1.7595419847328244, "no_speech_prob": 2.1233679490251234e-06}, {"id": 1055, "seek": 497684, "start": 4985.08, "end": 4989.76, "text": " I just tried because like otherwise somebody says like hey, you know here's a really good technique", "tokens": [286, 445, 3031, 570, 411, 5911, 2618, 1619, 411, 4177, 11, 291, 458, 510, 311, 257, 534, 665, 6532], "temperature": 0.0, "avg_logprob": -0.3959791660308838, "compression_ratio": 1.7595419847328244, "no_speech_prob": 2.1233679490251234e-06}, {"id": 1056, "seek": 497684, "start": 4989.76, "end": 4991.76, "text": " You're like okay great", "tokens": [509, 434, 411, 1392, 869], "temperature": 0.0, "avg_logprob": -0.3959791660308838, "compression_ratio": 1.7595419847328244, "no_speech_prob": 2.1233679490251234e-06}, {"id": 1057, "seek": 497684, "start": 4991.76, "end": 4998.12, "text": " But when you spent months trying to do something and like somebody else did it better by using that technique", "tokens": [583, 562, 291, 4418, 2493, 1382, 281, 360, 746, 293, 411, 2618, 1646, 630, 309, 1101, 538, 1228, 300, 6532], "temperature": 0.0, "avg_logprob": -0.3959791660308838, "compression_ratio": 1.7595419847328244, "no_speech_prob": 2.1233679490251234e-06}, {"id": 1058, "seek": 497684, "start": 4998.12, "end": 5000.12, "text": " That's pretty convincing", "tokens": [663, 311, 1238, 24823], "temperature": 0.0, "avg_logprob": -0.3959791660308838, "compression_ratio": 1.7595419847328244, "no_speech_prob": 2.1233679490251234e-06}, {"id": 1059, "seek": 497684, "start": 5000.12, "end": 5003.92, "text": " Right and so like it's kind of hard like I'm not sure if you're using it right now", "tokens": [1779, 293, 370, 411, 309, 311, 733, 295, 1152, 411, 286, 478, 406, 988, 498, 291, 434, 1228, 309, 558, 586], "temperature": 0.0, "avg_logprob": -0.3959791660308838, "compression_ratio": 1.7595419847328244, "no_speech_prob": 2.1233679490251234e-06}, {"id": 1060, "seek": 500392, "start": 5003.92, "end": 5008.08, "text": " Right and so like it's kind of hard like I'm standing up in front of you saying", "tokens": [1779, 293, 370, 411, 309, 311, 733, 295, 1152, 411, 286, 478, 4877, 493, 294, 1868, 295, 291, 1566], "temperature": 0.0, "avg_logprob": -0.12480588842321325, "compression_ratio": 1.7483443708609272, "no_speech_prob": 4.157331204623915e-06}, {"id": 1061, "seek": 500392, "start": 5008.52, "end": 5013.4800000000005, "text": " Here's a bunch of techniques that I've I've used and I've won some Kaggle competitions", "tokens": [1692, 311, 257, 3840, 295, 7512, 300, 286, 600, 286, 600, 1143, 293, 286, 600, 1582, 512, 48751, 22631, 26185], "temperature": 0.0, "avg_logprob": -0.12480588842321325, "compression_ratio": 1.7483443708609272, "no_speech_prob": 4.157331204623915e-06}, {"id": 1062, "seek": 500392, "start": 5013.4800000000005, "end": 5019.24, "text": " And I've got some state-of-the-art results, but it's like that's kind of secondhand information by the time it hits you right so", "tokens": [400, 286, 600, 658, 512, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 11, 457, 309, 311, 411, 300, 311, 733, 295, 1150, 5543, 1589, 538, 264, 565, 309, 8664, 291, 558, 370], "temperature": 0.0, "avg_logprob": -0.12480588842321325, "compression_ratio": 1.7483443708609272, "no_speech_prob": 4.157331204623915e-06}, {"id": 1063, "seek": 500392, "start": 5019.64, "end": 5021.64, "text": " It's really great to", "tokens": [467, 311, 534, 869, 281], "temperature": 0.0, "avg_logprob": -0.12480588842321325, "compression_ratio": 1.7483443708609272, "no_speech_prob": 4.157331204623915e-06}, {"id": 1064, "seek": 500392, "start": 5021.72, "end": 5026.08, "text": " Yeah, try things out and and also like it's been kind of nice to see", "tokens": [865, 11, 853, 721, 484, 293, 293, 611, 411, 309, 311, 668, 733, 295, 1481, 281, 536], "temperature": 0.0, "avg_logprob": -0.12480588842321325, "compression_ratio": 1.7483443708609272, "no_speech_prob": 4.157331204623915e-06}, {"id": 1065, "seek": 500392, "start": 5027.16, "end": 5032.8, "text": " Particularly I've noticed in the deep learning course quite a few of my students have you know I've said like this technique works really well", "tokens": [32281, 286, 600, 5694, 294, 264, 2452, 2539, 1164, 1596, 257, 1326, 295, 452, 1731, 362, 291, 458, 286, 600, 848, 411, 341, 6532, 1985, 534, 731], "temperature": 0.0, "avg_logprob": -0.12480588842321325, "compression_ratio": 1.7483443708609272, "no_speech_prob": 4.157331204623915e-06}, {"id": 1066, "seek": 503280, "start": 5032.8, "end": 5037.52, "text": " And they've tried it and they've got into the top 10 of a Kaggle competition the next day, and they're like", "tokens": [400, 436, 600, 3031, 309, 293, 436, 600, 658, 666, 264, 1192, 1266, 295, 257, 48751, 22631, 6211, 264, 958, 786, 11, 293, 436, 434, 411], "temperature": 0.0, "avg_logprob": -0.1464110022192603, "compression_ratio": 1.7330827067669172, "no_speech_prob": 4.56590987596428e-06}, {"id": 1067, "seek": 503280, "start": 5038.04, "end": 5043.08, "text": " Okay, that that counts as working really well, so so yeah Kaggle competitions are", "tokens": [1033, 11, 300, 300, 14893, 382, 1364, 534, 731, 11, 370, 370, 1338, 48751, 22631, 26185, 366], "temperature": 0.0, "avg_logprob": -0.1464110022192603, "compression_ratio": 1.7330827067669172, "no_speech_prob": 4.56590987596428e-06}, {"id": 1068, "seek": 503280, "start": 5044.76, "end": 5046.400000000001, "text": " Helpful for lots and lots of reasons", "tokens": [10773, 906, 337, 3195, 293, 3195, 295, 4112], "temperature": 0.0, "avg_logprob": -0.1464110022192603, "compression_ratio": 1.7330827067669172, "no_speech_prob": 4.56590987596428e-06}, {"id": 1069, "seek": 503280, "start": 5046.400000000001, "end": 5050.5, "text": " But you know one of the best ways is what happens after it finishes and so definitely like", "tokens": [583, 291, 458, 472, 295, 264, 1151, 2098, 307, 437, 2314, 934, 309, 23615, 293, 370, 2138, 411], "temperature": 0.0, "avg_logprob": -0.1464110022192603, "compression_ratio": 1.7330827067669172, "no_speech_prob": 4.56590987596428e-06}, {"id": 1070, "seek": 503280, "start": 5050.76, "end": 5055.12, "text": " For the ones that you that are now finishing up make sure you you know watch the forums", "tokens": [1171, 264, 2306, 300, 291, 300, 366, 586, 12693, 493, 652, 988, 291, 291, 458, 1159, 264, 26998], "temperature": 0.0, "avg_logprob": -0.1464110022192603, "compression_ratio": 1.7330827067669172, "no_speech_prob": 4.56590987596428e-06}, {"id": 1071, "seek": 503280, "start": 5055.8, "end": 5058.56, "text": " See what people are sharing in terms of their solutions", "tokens": [3008, 437, 561, 366, 5414, 294, 2115, 295, 641, 6547], "temperature": 0.0, "avg_logprob": -0.1464110022192603, "compression_ratio": 1.7330827067669172, "no_speech_prob": 4.56590987596428e-06}, {"id": 1072, "seek": 505856, "start": 5058.56, "end": 5063.68, "text": " And you know if you want to learn more about them like don't feel free to ask", "tokens": [400, 291, 458, 498, 291, 528, 281, 1466, 544, 466, 552, 411, 500, 380, 841, 1737, 281, 1029], "temperature": 0.0, "avg_logprob": -0.15236776966159626, "compression_ratio": 1.7508532423208192, "no_speech_prob": 5.0936537263623904e-06}, {"id": 1073, "seek": 505856, "start": 5064.280000000001, "end": 5069.64, "text": " The winners like hey could you tell me more about this or that people are normally pretty pretty good about explaining?", "tokens": [440, 17193, 411, 4177, 727, 291, 980, 385, 544, 466, 341, 420, 300, 561, 366, 5646, 1238, 1238, 665, 466, 13468, 30], "temperature": 0.0, "avg_logprob": -0.15236776966159626, "compression_ratio": 1.7508532423208192, "no_speech_prob": 5.0936537263623904e-06}, {"id": 1074, "seek": 505856, "start": 5070.320000000001, "end": 5076.6, "text": " And then ideally try and replicate it yourself right and that can turn into a great blog post", "tokens": [400, 550, 22915, 853, 293, 25356, 309, 1803, 558, 293, 300, 393, 1261, 666, 257, 869, 6968, 2183], "temperature": 0.0, "avg_logprob": -0.15236776966159626, "compression_ratio": 1.7508532423208192, "no_speech_prob": 5.0936537263623904e-06}, {"id": 1075, "seek": 505856, "start": 5076.76, "end": 5082.320000000001, "text": " You know or a great kernel is to be able to say okay such-and-such said that they use this technique", "tokens": [509, 458, 420, 257, 869, 28256, 307, 281, 312, 1075, 281, 584, 1392, 1270, 12, 474, 12, 39974, 848, 300, 436, 764, 341, 6532], "temperature": 0.0, "avg_logprob": -0.15236776966159626, "compression_ratio": 1.7508532423208192, "no_speech_prob": 5.0936537263623904e-06}, {"id": 1076, "seek": 505856, "start": 5082.320000000001, "end": 5087.84, "text": " Here's a really short explanation of what that technique is and here's a little bit of code showing how it's implemented", "tokens": [1692, 311, 257, 534, 2099, 10835, 295, 437, 300, 6532, 307, 293, 510, 311, 257, 707, 857, 295, 3089, 4099, 577, 309, 311, 12270], "temperature": 0.0, "avg_logprob": -0.15236776966159626, "compression_ratio": 1.7508532423208192, "no_speech_prob": 5.0936537263623904e-06}, {"id": 1077, "seek": 508784, "start": 5087.84, "end": 5093.92, "text": " And you know here's the result showing you you can get the same result that can be a really interesting write-up as well", "tokens": [400, 291, 458, 510, 311, 264, 1874, 4099, 291, 291, 393, 483, 264, 912, 1874, 300, 393, 312, 257, 534, 1880, 2464, 12, 1010, 382, 731], "temperature": 0.0, "avg_logprob": -0.14175187871697242, "compression_ratio": 1.591743119266055, "no_speech_prob": 7.183198249549605e-06}, {"id": 1078, "seek": 508784, "start": 5097.28, "end": 5099.28, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.14175187871697242, "compression_ratio": 1.591743119266055, "no_speech_prob": 7.183198249549605e-06}, {"id": 1079, "seek": 508784, "start": 5099.96, "end": 5103.360000000001, "text": " You know it's always nice to kind of have your data", "tokens": [509, 458, 309, 311, 1009, 1481, 281, 733, 295, 362, 428, 1412], "temperature": 0.0, "avg_logprob": -0.14175187871697242, "compression_ratio": 1.591743119266055, "no_speech_prob": 7.183198249549605e-06}, {"id": 1080, "seek": 508784, "start": 5104.76, "end": 5106.400000000001, "text": " reflect", "tokens": [5031], "temperature": 0.0, "avg_logprob": -0.14175187871697242, "compression_ratio": 1.591743119266055, "no_speech_prob": 7.183198249549605e-06}, {"id": 1081, "seek": 508784, "start": 5106.400000000001, "end": 5110.42, "text": " Like I don't know be as kind of easy to understand as possible", "tokens": [1743, 286, 500, 380, 458, 312, 382, 733, 295, 1858, 281, 1223, 382, 1944], "temperature": 0.0, "avg_logprob": -0.14175187871697242, "compression_ratio": 1.591743119266055, "no_speech_prob": 7.183198249549605e-06}, {"id": 1082, "seek": 508784, "start": 5110.42, "end": 5115.8, "text": " So in this case the data that came from Kaggle used various you know integers for the holidays", "tokens": [407, 294, 341, 1389, 264, 1412, 300, 1361, 490, 48751, 22631, 1143, 3683, 291, 458, 41674, 337, 264, 15734], "temperature": 0.0, "avg_logprob": -0.14175187871697242, "compression_ratio": 1.591743119266055, "no_speech_prob": 7.183198249549605e-06}, {"id": 1083, "seek": 511580, "start": 5115.8, "end": 5119.2, "text": " We can just use a boolean of like was it a holiday or not?", "tokens": [492, 393, 445, 764, 257, 748, 4812, 282, 295, 411, 390, 309, 257, 9960, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.20768732591108843, "compression_ratio": 1.6141078838174274, "no_speech_prob": 2.2603080651606433e-06}, {"id": 1084, "seek": 511580, "start": 5120.6, "end": 5122.6, "text": " So I like to clean that up", "tokens": [407, 286, 411, 281, 2541, 300, 493], "temperature": 0.0, "avg_logprob": -0.20768732591108843, "compression_ratio": 1.6141078838174274, "no_speech_prob": 2.2603080651606433e-06}, {"id": 1085, "seek": 511580, "start": 5123.24, "end": 5126.52, "text": " We've got quite a few different tables. We need to join them all together", "tokens": [492, 600, 658, 1596, 257, 1326, 819, 8020, 13, 492, 643, 281, 3917, 552, 439, 1214], "temperature": 0.0, "avg_logprob": -0.20768732591108843, "compression_ratio": 1.6141078838174274, "no_speech_prob": 2.2603080651606433e-06}, {"id": 1086, "seek": 511580, "start": 5127.0, "end": 5133.76, "text": " And I have a standard way of joining things together with pandas. I just use the pandas merge", "tokens": [400, 286, 362, 257, 3832, 636, 295, 5549, 721, 1214, 365, 4565, 296, 13, 286, 445, 764, 264, 4565, 296, 22183], "temperature": 0.0, "avg_logprob": -0.20768732591108843, "compression_ratio": 1.6141078838174274, "no_speech_prob": 2.2603080651606433e-06}, {"id": 1087, "seek": 511580, "start": 5134.320000000001, "end": 5137.56, "text": " Function and specifically I always do a left join", "tokens": [11166, 882, 293, 4682, 286, 1009, 360, 257, 1411, 3917], "temperature": 0.0, "avg_logprob": -0.20768732591108843, "compression_ratio": 1.6141078838174274, "no_speech_prob": 2.2603080651606433e-06}, {"id": 1088, "seek": 511580, "start": 5138.6, "end": 5141.360000000001, "text": " So who wants to tell me what a left join is?", "tokens": [407, 567, 2738, 281, 980, 385, 437, 257, 1411, 3917, 307, 30], "temperature": 0.0, "avg_logprob": -0.20768732591108843, "compression_ratio": 1.6141078838174274, "no_speech_prob": 2.2603080651606433e-06}, {"id": 1089, "seek": 511580, "start": 5142.76, "end": 5144.76, "text": " Since it's there why don't you go ahead?", "tokens": [4162, 309, 311, 456, 983, 500, 380, 291, 352, 2286, 30], "temperature": 0.0, "avg_logprob": -0.20768732591108843, "compression_ratio": 1.6141078838174274, "no_speech_prob": 2.2603080651606433e-06}, {"id": 1090, "seek": 514476, "start": 5144.76, "end": 5150.64, "text": " So you retain all the rows in the left table and you take so you have a key column", "tokens": [407, 291, 18340, 439, 264, 13241, 294, 264, 1411, 3199, 293, 291, 747, 370, 291, 362, 257, 2141, 7738], "temperature": 0.0, "avg_logprob": -0.20633386682581017, "compression_ratio": 1.8212765957446808, "no_speech_prob": 2.668763045221567e-05}, {"id": 1091, "seek": 514476, "start": 5150.8, "end": 5153.320000000001, "text": " You match that with the key column in the right side table", "tokens": [509, 2995, 300, 365, 264, 2141, 7738, 294, 264, 558, 1252, 3199], "temperature": 0.0, "avg_logprob": -0.20633386682581017, "compression_ratio": 1.8212765957446808, "no_speech_prob": 2.668763045221567e-05}, {"id": 1092, "seek": 514476, "start": 5153.320000000001, "end": 5157.6, "text": " And you just merge the rows that are also present in the right side table. Yeah, that's a great explanation", "tokens": [400, 291, 445, 22183, 264, 13241, 300, 366, 611, 1974, 294, 264, 558, 1252, 3199, 13, 865, 11, 300, 311, 257, 869, 10835], "temperature": 0.0, "avg_logprob": -0.20633386682581017, "compression_ratio": 1.8212765957446808, "no_speech_prob": 2.668763045221567e-05}, {"id": 1093, "seek": 514476, "start": 5157.68, "end": 5162.56, "text": " Good job. I don't have much to add to that the key reason that I always do a left join is", "tokens": [2205, 1691, 13, 286, 500, 380, 362, 709, 281, 909, 281, 300, 264, 2141, 1778, 300, 286, 1009, 360, 257, 1411, 3917, 307], "temperature": 0.0, "avg_logprob": -0.20633386682581017, "compression_ratio": 1.8212765957446808, "no_speech_prob": 2.668763045221567e-05}, {"id": 1094, "seek": 514476, "start": 5163.8, "end": 5170.400000000001, "text": " That after I do the join I always then check if there were things in the right hand side", "tokens": [663, 934, 286, 360, 264, 3917, 286, 1009, 550, 1520, 498, 456, 645, 721, 294, 264, 558, 1011, 1252], "temperature": 0.0, "avg_logprob": -0.20633386682581017, "compression_ratio": 1.8212765957446808, "no_speech_prob": 2.668763045221567e-05}, {"id": 1095, "seek": 517040, "start": 5170.4, "end": 5175.5199999999995, "text": " That are now know right because if so it means that I missed some things", "tokens": [663, 366, 586, 458, 558, 570, 498, 370, 309, 1355, 300, 286, 6721, 512, 721], "temperature": 0.0, "avg_logprob": -0.20025740278528092, "compression_ratio": 1.5954545454545455, "no_speech_prob": 5.77187256567413e-06}, {"id": 1096, "seek": 517040, "start": 5176.28, "end": 5181.839999999999, "text": " Yeah, I haven't shown it here, but I also check that the number of rows hasn't varied before and after", "tokens": [865, 11, 286, 2378, 380, 4898, 309, 510, 11, 457, 286, 611, 1520, 300, 264, 1230, 295, 13241, 6132, 380, 22877, 949, 293, 934], "temperature": 0.0, "avg_logprob": -0.20025740278528092, "compression_ratio": 1.5954545454545455, "no_speech_prob": 5.77187256567413e-06}, {"id": 1097, "seek": 517040, "start": 5182.32, "end": 5185.92, "text": " If it has that means that the right hand side table wasn't unique", "tokens": [759, 309, 575, 300, 1355, 300, 264, 558, 1011, 1252, 3199, 2067, 380, 3845], "temperature": 0.0, "avg_logprob": -0.20025740278528092, "compression_ratio": 1.5954545454545455, "no_speech_prob": 5.77187256567413e-06}, {"id": 1098, "seek": 517040, "start": 5187.16, "end": 5189.16, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.20025740278528092, "compression_ratio": 1.5954545454545455, "no_speech_prob": 5.77187256567413e-06}, {"id": 1099, "seek": 517040, "start": 5189.48, "end": 5191.2, "text": " Even when I'm sure", "tokens": [2754, 562, 286, 478, 988], "temperature": 0.0, "avg_logprob": -0.20025740278528092, "compression_ratio": 1.5954545454545455, "no_speech_prob": 5.77187256567413e-06}, {"id": 1100, "seek": 517040, "start": 5191.2, "end": 5196.5199999999995, "text": " Something's true. I always also assume that I've screwed it up, so I always check", "tokens": [6595, 311, 2074, 13, 286, 1009, 611, 6552, 300, 286, 600, 20331, 309, 493, 11, 370, 286, 1009, 1520], "temperature": 0.0, "avg_logprob": -0.20025740278528092, "compression_ratio": 1.5954545454545455, "no_speech_prob": 5.77187256567413e-06}, {"id": 1101, "seek": 519652, "start": 5196.52, "end": 5203.160000000001, "text": " So I could go ahead and merge the state names into the weather I can also", "tokens": [407, 286, 727, 352, 2286, 293, 22183, 264, 1785, 5288, 666, 264, 5503, 286, 393, 611], "temperature": 0.0, "avg_logprob": -0.187375734018725, "compression_ratio": 1.515, "no_speech_prob": 1.101592943086871e-06}, {"id": 1102, "seek": 519652, "start": 5205.120000000001, "end": 5207.120000000001, "text": " if you look at the", "tokens": [498, 291, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.187375734018725, "compression_ratio": 1.515, "no_speech_prob": 1.101592943086871e-06}, {"id": 1103, "seek": 519652, "start": 5210.56, "end": 5212.56, "text": " Google Trends table", "tokens": [3329, 37417, 82, 3199], "temperature": 0.0, "avg_logprob": -0.187375734018725, "compression_ratio": 1.515, "no_speech_prob": 1.101592943086871e-06}, {"id": 1104, "seek": 519652, "start": 5212.56, "end": 5214.56, "text": " It's got this weak range", "tokens": [467, 311, 658, 341, 5336, 3613], "temperature": 0.0, "avg_logprob": -0.187375734018725, "compression_ratio": 1.515, "no_speech_prob": 1.101592943086871e-06}, {"id": 1105, "seek": 519652, "start": 5214.92, "end": 5220.96, "text": " Which I need to turn into a date in order to join it right and so the nice thing about doing this in pandas", "tokens": [3013, 286, 643, 281, 1261, 666, 257, 4002, 294, 1668, 281, 3917, 309, 558, 293, 370, 264, 1481, 551, 466, 884, 341, 294, 4565, 296], "temperature": 0.0, "avg_logprob": -0.187375734018725, "compression_ratio": 1.515, "no_speech_prob": 1.101592943086871e-06}, {"id": 1106, "seek": 519652, "start": 5220.96, "end": 5223.080000000001, "text": " Is that pandas gives us access to?", "tokens": [1119, 300, 4565, 296, 2709, 505, 2105, 281, 30], "temperature": 0.0, "avg_logprob": -0.187375734018725, "compression_ratio": 1.515, "no_speech_prob": 1.101592943086871e-06}, {"id": 1107, "seek": 519652, "start": 5223.92, "end": 5225.92, "text": " You know all of Python", "tokens": [509, 458, 439, 295, 15329], "temperature": 0.0, "avg_logprob": -0.187375734018725, "compression_ratio": 1.515, "no_speech_prob": 1.101592943086871e-06}, {"id": 1108, "seek": 522592, "start": 5225.92, "end": 5231.9, "text": " Right and so for example inside the the series object is a dot str", "tokens": [1779, 293, 370, 337, 1365, 1854, 264, 264, 2638, 2657, 307, 257, 5893, 1056], "temperature": 0.0, "avg_logprob": -0.16834422777283867, "compression_ratio": 1.8901960784313725, "no_speech_prob": 2.8130036753282184e-06}, {"id": 1109, "seek": 522592, "start": 5232.5, "end": 5235.56, "text": " Attribute that gives you access to all the string processing functions", "tokens": [7298, 2024, 1169, 300, 2709, 291, 2105, 281, 439, 264, 6798, 9007, 6828], "temperature": 0.0, "avg_logprob": -0.16834422777283867, "compression_ratio": 1.8901960784313725, "no_speech_prob": 2.8130036753282184e-06}, {"id": 1110, "seek": 522592, "start": 5235.96, "end": 5241.78, "text": " Now just like cat gives you access to the categorical functions dt gives you access to the date time functions", "tokens": [823, 445, 411, 3857, 2709, 291, 2105, 281, 264, 19250, 804, 6828, 36423, 2709, 291, 2105, 281, 264, 4002, 565, 6828], "temperature": 0.0, "avg_logprob": -0.16834422777283867, "compression_ratio": 1.8901960784313725, "no_speech_prob": 2.8130036753282184e-06}, {"id": 1111, "seek": 522592, "start": 5241.88, "end": 5247.4800000000005, "text": " So I can now split everything in that column and it's really important to try and use these pandas functions", "tokens": [407, 286, 393, 586, 7472, 1203, 294, 300, 7738, 293, 309, 311, 534, 1021, 281, 853, 293, 764, 613, 4565, 296, 6828], "temperature": 0.0, "avg_logprob": -0.16834422777283867, "compression_ratio": 1.8901960784313725, "no_speech_prob": 2.8130036753282184e-06}, {"id": 1112, "seek": 522592, "start": 5248.04, "end": 5254.88, "text": " Because they you know they're going to be vectorized accelerated through you know often through 7d at least through you know", "tokens": [1436, 436, 291, 458, 436, 434, 516, 281, 312, 8062, 1602, 29763, 807, 291, 458, 2049, 807, 1614, 67, 412, 1935, 807, 291, 458], "temperature": 0.0, "avg_logprob": -0.16834422777283867, "compression_ratio": 1.8901960784313725, "no_speech_prob": 2.8130036753282184e-06}, {"id": 1113, "seek": 525488, "start": 5254.88, "end": 5256.88, "text": " C code", "tokens": [383, 3089], "temperature": 0.0, "avg_logprob": -0.22279838893724524, "compression_ratio": 1.4875, "no_speech_prob": 2.123367721651448e-06}, {"id": 1114, "seek": 525488, "start": 5257.64, "end": 5259.64, "text": " So that runs nice and quickly", "tokens": [407, 300, 6676, 1481, 293, 2661], "temperature": 0.0, "avg_logprob": -0.22279838893724524, "compression_ratio": 1.4875, "no_speech_prob": 2.123367721651448e-06}, {"id": 1115, "seek": 525488, "start": 5260.76, "end": 5262.24, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.22279838893724524, "compression_ratio": 1.4875, "no_speech_prob": 2.123367721651448e-06}, {"id": 1116, "seek": 525488, "start": 5262.24, "end": 5267.4800000000005, "text": " Then you know as per usual let's add date metadata to our dates", "tokens": [1396, 291, 458, 382, 680, 7713, 718, 311, 909, 4002, 26603, 281, 527, 11691], "temperature": 0.0, "avg_logprob": -0.22279838893724524, "compression_ratio": 1.4875, "no_speech_prob": 2.123367721651448e-06}, {"id": 1117, "seek": 525488, "start": 5271.32, "end": 5273.32, "text": " In the end we're basically", "tokens": [682, 264, 917, 321, 434, 1936], "temperature": 0.0, "avg_logprob": -0.22279838893724524, "compression_ratio": 1.4875, "no_speech_prob": 2.123367721651448e-06}, {"id": 1118, "seek": 525488, "start": 5273.400000000001, "end": 5277.76, "text": " Denormalizing all these tables, so we're going to put them all into one table so in the Google Trend", "tokens": [6458, 24440, 3319, 439, 613, 8020, 11, 370, 321, 434, 516, 281, 829, 552, 439, 666, 472, 3199, 370, 294, 264, 3329, 37417], "temperature": 0.0, "avg_logprob": -0.22279838893724524, "compression_ratio": 1.4875, "no_speech_prob": 2.123367721651448e-06}, {"id": 1119, "seek": 525488, "start": 5278.84, "end": 5279.92, "text": " table", "tokens": [3199], "temperature": 0.0, "avg_logprob": -0.22279838893724524, "compression_ratio": 1.4875, "no_speech_prob": 2.123367721651448e-06}, {"id": 1120, "seek": 527992, "start": 5279.92, "end": 5285.84, "text": " There was also though they were mainly trends by state, but there was also trends for the whole of Germany", "tokens": [821, 390, 611, 1673, 436, 645, 8704, 13892, 538, 1785, 11, 457, 456, 390, 611, 13892, 337, 264, 1379, 295, 7244], "temperature": 0.0, "avg_logprob": -0.17900749553333628, "compression_ratio": 1.9703389830508475, "no_speech_prob": 2.058035306617967e-06}, {"id": 1121, "seek": 527992, "start": 5286.24, "end": 5291.26, "text": " So we kind of put the Germany own you know the whole of Germany ones into a separate data frame", "tokens": [407, 321, 733, 295, 829, 264, 7244, 1065, 291, 458, 264, 1379, 295, 7244, 2306, 666, 257, 4994, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.17900749553333628, "compression_ratio": 1.9703389830508475, "no_speech_prob": 2.058035306617967e-06}, {"id": 1122, "seek": 527992, "start": 5291.4800000000005, "end": 5297.64, "text": " So that we can join that so we're going to have like Google Trend for this date and Google Trend for the whole of Germany", "tokens": [407, 300, 321, 393, 3917, 300, 370, 321, 434, 516, 281, 362, 411, 3329, 37417, 337, 341, 4002, 293, 3329, 37417, 337, 264, 1379, 295, 7244], "temperature": 0.0, "avg_logprob": -0.17900749553333628, "compression_ratio": 1.9703389830508475, "no_speech_prob": 2.058035306617967e-06}, {"id": 1123, "seek": 527992, "start": 5299.16, "end": 5301.72, "text": " And so now we can go ahead and start joining", "tokens": [400, 370, 586, 321, 393, 352, 2286, 293, 722, 5549], "temperature": 0.0, "avg_logprob": -0.17900749553333628, "compression_ratio": 1.9703389830508475, "no_speech_prob": 2.058035306617967e-06}, {"id": 1124, "seek": 527992, "start": 5302.28, "end": 5307.74, "text": " Both for the training set and for the test set and then for both check that we don't have zeros", "tokens": [6767, 337, 264, 3097, 992, 293, 337, 264, 1500, 992, 293, 550, 337, 1293, 1520, 300, 321, 500, 380, 362, 35193], "temperature": 0.0, "avg_logprob": -0.17900749553333628, "compression_ratio": 1.9703389830508475, "no_speech_prob": 2.058035306617967e-06}, {"id": 1125, "seek": 530774, "start": 5307.74, "end": 5309.74, "text": " and zeros", "tokens": [293, 35193], "temperature": 0.0, "avg_logprob": -0.20107566917335595, "compression_ratio": 1.6157635467980296, "no_speech_prob": 5.255351879895898e-06}, {"id": 1126, "seek": 530774, "start": 5310.82, "end": 5312.82, "text": " My merge function I", "tokens": [1222, 22183, 2445, 286], "temperature": 0.0, "avg_logprob": -0.20107566917335595, "compression_ratio": 1.6157635467980296, "no_speech_prob": 5.255351879895898e-06}, {"id": 1127, "seek": 530774, "start": 5314.219999999999, "end": 5319.86, "text": " Set the suffix if there are two columns that are the same I set the suffix on the left to be nothing at all", "tokens": [8928, 264, 3889, 970, 498, 456, 366, 732, 13766, 300, 366, 264, 912, 286, 992, 264, 3889, 970, 322, 264, 1411, 281, 312, 1825, 412, 439], "temperature": 0.0, "avg_logprob": -0.20107566917335595, "compression_ratio": 1.6157635467980296, "no_speech_prob": 5.255351879895898e-06}, {"id": 1128, "seek": 530774, "start": 5319.86, "end": 5325.16, "text": " So it doesn't screw around with the name and the right hand side to be underscore y and in this case", "tokens": [407, 309, 1177, 380, 5630, 926, 365, 264, 1315, 293, 264, 558, 1011, 1252, 281, 312, 37556, 288, 293, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.20107566917335595, "compression_ratio": 1.6157635467980296, "no_speech_prob": 5.255351879895898e-06}, {"id": 1129, "seek": 530774, "start": 5325.16, "end": 5328.98, "text": " I didn't want any of the duplicate ones, so I just went through and", "tokens": [286, 994, 380, 528, 604, 295, 264, 23976, 2306, 11, 370, 286, 445, 1437, 807, 293], "temperature": 0.0, "avg_logprob": -0.20107566917335595, "compression_ratio": 1.6157635467980296, "no_speech_prob": 5.255351879895898e-06}, {"id": 1130, "seek": 530774, "start": 5330.84, "end": 5332.84, "text": " Deleted them okay", "tokens": [5831, 10993, 552, 1392], "temperature": 0.0, "avg_logprob": -0.20107566917335595, "compression_ratio": 1.6157635467980296, "no_speech_prob": 5.255351879895898e-06}, {"id": 1131, "seek": 530774, "start": 5333.42, "end": 5334.5, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.20107566917335595, "compression_ratio": 1.6157635467980296, "no_speech_prob": 5.255351879895898e-06}, {"id": 1132, "seek": 533450, "start": 5334.5, "end": 5337.82, "text": " Then we're going to in a moment. We're going to try to", "tokens": [1396, 321, 434, 516, 281, 294, 257, 1623, 13, 492, 434, 516, 281, 853, 281], "temperature": 0.0, "avg_logprob": -0.2113516724225387, "compression_ratio": 1.7366071428571428, "no_speech_prob": 4.63782589577022e-06}, {"id": 1133, "seek": 533450, "start": 5339.26, "end": 5341.26, "text": " Create a", "tokens": [20248, 257], "temperature": 0.0, "avg_logprob": -0.2113516724225387, "compression_ratio": 1.7366071428571428, "no_speech_prob": 4.63782589577022e-06}, {"id": 1134, "seek": 533450, "start": 5341.26, "end": 5346.02, "text": " Competition you know the the main competitor for this store has been open since some date", "tokens": [43634, 291, 458, 264, 264, 2135, 27266, 337, 341, 3531, 575, 668, 1269, 1670, 512, 4002], "temperature": 0.0, "avg_logprob": -0.2113516724225387, "compression_ratio": 1.7366071428571428, "no_speech_prob": 4.63782589577022e-06}, {"id": 1135, "seek": 533450, "start": 5346.58, "end": 5353.82, "text": " Right and so you can just use pandas to date time passing in the year the month and the day", "tokens": [1779, 293, 370, 291, 393, 445, 764, 4565, 296, 281, 4002, 565, 8437, 294, 264, 1064, 264, 1618, 293, 264, 786], "temperature": 0.0, "avg_logprob": -0.2113516724225387, "compression_ratio": 1.7366071428571428, "no_speech_prob": 4.63782589577022e-06}, {"id": 1136, "seek": 533450, "start": 5355.06, "end": 5361.2, "text": " Right and so that's going to give us an error unless they all have years and months so so we're going to fill in the missing", "tokens": [1779, 293, 370, 300, 311, 516, 281, 976, 505, 364, 6713, 5969, 436, 439, 362, 924, 293, 2493, 370, 370, 321, 434, 516, 281, 2836, 294, 264, 5361], "temperature": 0.0, "avg_logprob": -0.2113516724225387, "compression_ratio": 1.7366071428571428, "no_speech_prob": 4.63782589577022e-06}, {"id": 1137, "seek": 533450, "start": 5361.2, "end": 5362.42, "text": " ones with the", "tokens": [2306, 365, 264], "temperature": 0.0, "avg_logprob": -0.2113516724225387, "compression_ratio": 1.7366071428571428, "no_speech_prob": 4.63782589577022e-06}, {"id": 1138, "seek": 533450, "start": 5362.42, "end": 5363.76, "text": " 1901", "tokens": [1294, 10607], "temperature": 0.0, "avg_logprob": -0.2113516724225387, "compression_ratio": 1.7366071428571428, "no_speech_prob": 4.63782589577022e-06}, {"id": 1139, "seek": 536376, "start": 5363.76, "end": 5364.9800000000005, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2347354031680675, "compression_ratio": 1.5947136563876652, "no_speech_prob": 4.092895778740058e-06}, {"id": 1140, "seek": 536376, "start": 5364.9800000000005, "end": 5369.64, "text": " And then what we really know we didn't want to know is like how long is this door been open for?", "tokens": [400, 550, 437, 321, 534, 458, 321, 994, 380, 528, 281, 458, 307, 411, 577, 938, 307, 341, 2853, 668, 1269, 337, 30], "temperature": 0.0, "avg_logprob": -0.2347354031680675, "compression_ratio": 1.5947136563876652, "no_speech_prob": 4.092895778740058e-06}, {"id": 1141, "seek": 536376, "start": 5369.9800000000005, "end": 5374.64, "text": " At the time of this particular record all right so we can just do a date subtract", "tokens": [1711, 264, 565, 295, 341, 1729, 2136, 439, 558, 370, 321, 393, 445, 360, 257, 4002, 16390], "temperature": 0.0, "avg_logprob": -0.2347354031680675, "compression_ratio": 1.5947136563876652, "no_speech_prob": 4.092895778740058e-06}, {"id": 1142, "seek": 536376, "start": 5375.22, "end": 5377.22, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2347354031680675, "compression_ratio": 1.5947136563876652, "no_speech_prob": 4.092895778740058e-06}, {"id": 1143, "seek": 536376, "start": 5378.52, "end": 5381.900000000001, "text": " Now if you think about it sometimes the competition", "tokens": [823, 498, 291, 519, 466, 309, 2171, 264, 6211], "temperature": 0.0, "avg_logprob": -0.2347354031680675, "compression_ratio": 1.5947136563876652, "no_speech_prob": 4.092895778740058e-06}, {"id": 1144, "seek": 536376, "start": 5382.22, "end": 5390.26, "text": " You know opened later than this particular row so sometimes it's going to be negative, and it doesn't probably make sense", "tokens": [509, 458, 5625, 1780, 813, 341, 1729, 5386, 370, 2171, 309, 311, 516, 281, 312, 3671, 11, 293, 309, 1177, 380, 1391, 652, 2020], "temperature": 0.0, "avg_logprob": -0.2347354031680675, "compression_ratio": 1.5947136563876652, "no_speech_prob": 4.092895778740058e-06}, {"id": 1145, "seek": 539026, "start": 5390.26, "end": 5398.56, "text": " To have negatives meaning like it's going to open in X days time now having said that I would never", "tokens": [1407, 362, 40019, 3620, 411, 309, 311, 516, 281, 1269, 294, 1783, 1708, 565, 586, 1419, 848, 300, 286, 576, 1128], "temperature": 0.0, "avg_logprob": -0.21523442379263943, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.7852989862585673e-06}, {"id": 1146, "seek": 539026, "start": 5399.8, "end": 5406.4800000000005, "text": " Put in something like this without first of all running a model with it in and without it in", "tokens": [4935, 294, 746, 411, 341, 1553, 700, 295, 439, 2614, 257, 2316, 365, 309, 294, 293, 1553, 309, 294], "temperature": 0.0, "avg_logprob": -0.21523442379263943, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.7852989862585673e-06}, {"id": 1147, "seek": 539026, "start": 5406.88, "end": 5409.84, "text": " right because like our assumptions about", "tokens": [558, 570, 411, 527, 17695, 466], "temperature": 0.0, "avg_logprob": -0.21523442379263943, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.7852989862585673e-06}, {"id": 1148, "seek": 539026, "start": 5410.6, "end": 5417.2, "text": " About the data very often turn out not to be true now in this case. I didn't invent any of these", "tokens": [7769, 264, 1412, 588, 2049, 1261, 484, 406, 281, 312, 2074, 586, 294, 341, 1389, 13, 286, 994, 380, 7962, 604, 295, 613], "temperature": 0.0, "avg_logprob": -0.21523442379263943, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.7852989862585673e-06}, {"id": 1149, "seek": 539026, "start": 5418.24, "end": 5420.0, "text": " pre-processing steps", "tokens": [659, 12, 41075, 278, 4439], "temperature": 0.0, "avg_logprob": -0.21523442379263943, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.7852989862585673e-06}, {"id": 1150, "seek": 542000, "start": 5420.0, "end": 5426.6, "text": " I wrote all the code, but it's all based on the third place winners github repo right so", "tokens": [286, 4114, 439, 264, 3089, 11, 457, 309, 311, 439, 2361, 322, 264, 2636, 1081, 17193, 290, 355, 836, 49040, 558, 370], "temperature": 0.0, "avg_logprob": -0.1677053714620656, "compression_ratio": 1.559090909090909, "no_speech_prob": 2.726453431023401e-06}, {"id": 1151, "seek": 542000, "start": 5428.16, "end": 5431.06, "text": " Knowing what it takes to get third place in the Kaggle competition", "tokens": [25499, 437, 309, 2516, 281, 483, 2636, 1081, 294, 264, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.1677053714620656, "compression_ratio": 1.559090909090909, "no_speech_prob": 2.726453431023401e-06}, {"id": 1152, "seek": 542000, "start": 5431.2, "end": 5436.88, "text": " I'm pretty sure they would have checked every one of these pre-processing steps and made sure it actually improved their", "tokens": [286, 478, 1238, 988, 436, 576, 362, 10033, 633, 472, 295, 613, 659, 12, 41075, 278, 4439, 293, 1027, 988, 309, 767, 9689, 641], "temperature": 0.0, "avg_logprob": -0.1677053714620656, "compression_ratio": 1.559090909090909, "no_speech_prob": 2.726453431023401e-06}, {"id": 1153, "seek": 542000, "start": 5437.4, "end": 5439.4, "text": " their validation set score", "tokens": [641, 24071, 992, 6175], "temperature": 0.0, "avg_logprob": -0.1677053714620656, "compression_ratio": 1.559090909090909, "no_speech_prob": 2.726453431023401e-06}, {"id": 1154, "seek": 542000, "start": 5440.0, "end": 5442.0, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.1677053714620656, "compression_ratio": 1.559090909090909, "no_speech_prob": 2.726453431023401e-06}, {"id": 1155, "seek": 542000, "start": 5444.12, "end": 5447.08, "text": " So what we're going to be doing is", "tokens": [407, 437, 321, 434, 516, 281, 312, 884, 307], "temperature": 0.0, "avg_logprob": -0.1677053714620656, "compression_ratio": 1.559090909090909, "no_speech_prob": 2.726453431023401e-06}, {"id": 1156, "seek": 544708, "start": 5447.08, "end": 5449.08, "text": " is", "tokens": [307], "temperature": 0.0, "avg_logprob": -0.16876404905972414, "compression_ratio": 1.7083333333333333, "no_speech_prob": 7.453735406670603e-07}, {"id": 1157, "seek": 544708, "start": 5449.2, "end": 5453.04, "text": " Creating a neural network where some of the inputs to it are", "tokens": [40002, 257, 18161, 3209, 689, 512, 295, 264, 15743, 281, 309, 366], "temperature": 0.0, "avg_logprob": -0.16876404905972414, "compression_ratio": 1.7083333333333333, "no_speech_prob": 7.453735406670603e-07}, {"id": 1158, "seek": 544708, "start": 5454.12, "end": 5456.4, "text": " continuous and some of them are", "tokens": [10957, 293, 512, 295, 552, 366], "temperature": 0.0, "avg_logprob": -0.16876404905972414, "compression_ratio": 1.7083333333333333, "no_speech_prob": 7.453735406670603e-07}, {"id": 1159, "seek": 544708, "start": 5457.04, "end": 5463.24, "text": " Categorical and so what that means in the in the neural net that you know we have", "tokens": [383, 2968, 284, 804, 293, 370, 437, 300, 1355, 294, 264, 294, 264, 18161, 2533, 300, 291, 458, 321, 362], "temperature": 0.0, "avg_logprob": -0.16876404905972414, "compression_ratio": 1.7083333333333333, "no_speech_prob": 7.453735406670603e-07}, {"id": 1160, "seek": 546324, "start": 5463.24, "end": 5471.599999999999, "text": " We're basically going to have you know this kind of initial weight matrix", "tokens": [492, 434, 1936, 516, 281, 362, 291, 458, 341, 733, 295, 5883, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.394331690776779, "compression_ratio": 1.7303921568627452, "no_speech_prob": 7.571121045657492e-07}, {"id": 1161, "seek": 546324, "start": 5471.599999999999, "end": 5474.0, "text": " Right and we're going to have this", "tokens": [1779, 293, 321, 434, 516, 281, 362, 341], "temperature": 0.0, "avg_logprob": -0.394331690776779, "compression_ratio": 1.7303921568627452, "no_speech_prob": 7.571121045657492e-07}, {"id": 1162, "seek": 546324, "start": 5474.5199999999995, "end": 5481.4, "text": " This input feature vector right and so some of the inputs are just going to be plain continuous numbers", "tokens": [639, 4846, 4111, 8062, 558, 293, 370, 512, 295, 264, 15743, 366, 445, 516, 281, 312, 11121, 10957, 3547], "temperature": 0.0, "avg_logprob": -0.394331690776779, "compression_ratio": 1.7303921568627452, "no_speech_prob": 7.571121045657492e-07}, {"id": 1163, "seek": 546324, "start": 5481.4, "end": 5487.24, "text": " Like you know what's the maximum temperature here, or what's the number of kilometers to the nearest store?", "tokens": [1743, 291, 458, 437, 311, 264, 6674, 4292, 510, 11, 420, 437, 311, 264, 1230, 295, 13904, 281, 264, 23831, 3531, 30], "temperature": 0.0, "avg_logprob": -0.394331690776779, "compression_ratio": 1.7303921568627452, "no_speech_prob": 7.571121045657492e-07}, {"id": 1164, "seek": 546324, "start": 5488.2, "end": 5490.44, "text": " And some of them are going to be", "tokens": [400, 512, 295, 552, 366, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.394331690776779, "compression_ratio": 1.7303921568627452, "no_speech_prob": 7.571121045657492e-07}, {"id": 1165, "seek": 549044, "start": 5490.44, "end": 5492.679999999999, "text": " And some of them are going to be", "tokens": [400, 512, 295, 552, 366, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.1588087732141668, "compression_ratio": 1.766839378238342, "no_speech_prob": 2.203329643180041e-07}, {"id": 1166, "seek": 549044, "start": 5496.24, "end": 5498.24, "text": " One hot encoded", "tokens": [1485, 2368, 2058, 12340], "temperature": 0.0, "avg_logprob": -0.1588087732141668, "compression_ratio": 1.766839378238342, "no_speech_prob": 2.203329643180041e-07}, {"id": 1167, "seek": 549044, "start": 5500.2, "end": 5505.799999999999, "text": " Effectively right, but we're not actually going to store it as one hot encoded. We're actually going to store it as", "tokens": [17764, 3413, 558, 11, 457, 321, 434, 406, 767, 516, 281, 3531, 309, 382, 472, 2368, 2058, 12340, 13, 492, 434, 767, 516, 281, 3531, 309, 382], "temperature": 0.0, "avg_logprob": -0.1588087732141668, "compression_ratio": 1.766839378238342, "no_speech_prob": 2.203329643180041e-07}, {"id": 1168, "seek": 549044, "start": 5506.719999999999, "end": 5508.2, "text": " the index", "tokens": [264, 8186], "temperature": 0.0, "avg_logprob": -0.1588087732141668, "compression_ratio": 1.766839378238342, "no_speech_prob": 2.203329643180041e-07}, {"id": 1169, "seek": 549044, "start": 5508.2, "end": 5513.879999999999, "text": " Right and so the neural net model is going to need to know which of these columns", "tokens": [1779, 293, 370, 264, 18161, 2533, 2316, 307, 516, 281, 643, 281, 458, 597, 295, 613, 13766], "temperature": 0.0, "avg_logprob": -0.1588087732141668, "compression_ratio": 1.766839378238342, "no_speech_prob": 2.203329643180041e-07}, {"id": 1170, "seek": 549044, "start": 5514.4, "end": 5518.48, "text": " Should you should you basically create an embedding for which ones should you treat?", "tokens": [6454, 291, 820, 291, 1936, 1884, 364, 12240, 3584, 337, 597, 2306, 820, 291, 2387, 30], "temperature": 0.0, "avg_logprob": -0.1588087732141668, "compression_ratio": 1.766839378238342, "no_speech_prob": 2.203329643180041e-07}, {"id": 1171, "seek": 551848, "start": 5518.48, "end": 5525.719999999999, "text": " You know as if they were kind of one hot encoded and which ones should you just you feed directly into the linear layer?", "tokens": [509, 458, 382, 498, 436, 645, 733, 295, 472, 2368, 2058, 12340, 293, 597, 2306, 820, 291, 445, 291, 3154, 3838, 666, 264, 8213, 4583, 30], "temperature": 0.0, "avg_logprob": -0.1489026634781449, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.561272140155779e-06}, {"id": 1172, "seek": 551848, "start": 5526.32, "end": 5528.32, "text": " right and so", "tokens": [558, 293, 370], "temperature": 0.0, "avg_logprob": -0.1489026634781449, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.561272140155779e-06}, {"id": 1173, "seek": 551848, "start": 5528.5199999999995, "end": 5530.5199999999995, "text": " We're going to tell the model when we get there", "tokens": [492, 434, 516, 281, 980, 264, 2316, 562, 321, 483, 456], "temperature": 0.0, "avg_logprob": -0.1489026634781449, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.561272140155779e-06}, {"id": 1174, "seek": 551848, "start": 5530.5199999999995, "end": 5536.48, "text": " Which is which but we actually need to think ahead of time about like which ones do we want to treat as?", "tokens": [3013, 307, 597, 457, 321, 767, 643, 281, 519, 2286, 295, 565, 466, 411, 597, 2306, 360, 321, 528, 281, 2387, 382, 30], "temperature": 0.0, "avg_logprob": -0.1489026634781449, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.561272140155779e-06}, {"id": 1175, "seek": 551848, "start": 5536.719999999999, "end": 5539.799999999999, "text": " Categorical and which ones are continuous in particular?", "tokens": [383, 2968, 284, 804, 293, 597, 2306, 366, 10957, 294, 1729, 30], "temperature": 0.0, "avg_logprob": -0.1489026634781449, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.561272140155779e-06}, {"id": 1176, "seek": 551848, "start": 5540.759999999999, "end": 5542.759999999999, "text": " Things that we're going to treat it as categorical", "tokens": [9514, 300, 321, 434, 516, 281, 2387, 309, 382, 19250, 804], "temperature": 0.0, "avg_logprob": -0.1489026634781449, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.561272140155779e-06}, {"id": 1177, "seek": 551848, "start": 5543.16, "end": 5545.16, "text": " We don't want to create", "tokens": [492, 500, 380, 528, 281, 1884], "temperature": 0.0, "avg_logprob": -0.1489026634781449, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.561272140155779e-06}, {"id": 1178, "seek": 554516, "start": 5545.16, "end": 5548.8, "text": " More categories than we need right and so let me show you what I mean", "tokens": [5048, 10479, 813, 321, 643, 558, 293, 370, 718, 385, 855, 291, 437, 286, 914], "temperature": 0.0, "avg_logprob": -0.15201183665882456, "compression_ratio": 1.8611111111111112, "no_speech_prob": 1.1726384627763764e-06}, {"id": 1179, "seek": 554516, "start": 5549.72, "end": 5551.72, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.15201183665882456, "compression_ratio": 1.8611111111111112, "no_speech_prob": 1.1726384627763764e-06}, {"id": 1180, "seek": 554516, "start": 5551.92, "end": 5558.44, "text": " The third place getters in this competition decided that the number of months that the competition was open was something that they were going", "tokens": [440, 2636, 1081, 483, 1559, 294, 341, 6211, 3047, 300, 264, 1230, 295, 2493, 300, 264, 6211, 390, 1269, 390, 746, 300, 436, 645, 516], "temperature": 0.0, "avg_logprob": -0.15201183665882456, "compression_ratio": 1.8611111111111112, "no_speech_prob": 1.1726384627763764e-06}, {"id": 1181, "seek": 554516, "start": 5558.44, "end": 5564.24, "text": " To use as a categorical variable right and so in order to avoid having more categories than they needed", "tokens": [1407, 764, 382, 257, 19250, 804, 7006, 558, 293, 370, 294, 1668, 281, 5042, 1419, 544, 10479, 813, 436, 2978], "temperature": 0.0, "avg_logprob": -0.15201183665882456, "compression_ratio": 1.8611111111111112, "no_speech_prob": 1.1726384627763764e-06}, {"id": 1182, "seek": 554516, "start": 5564.88, "end": 5571.2, "text": " They truncated it at 24 months. They said anything more than 24 months. I'll truncate to 24", "tokens": [814, 504, 409, 66, 770, 309, 412, 4022, 2493, 13, 814, 848, 1340, 544, 813, 4022, 2493, 13, 286, 603, 504, 409, 66, 473, 281, 4022], "temperature": 0.0, "avg_logprob": -0.15201183665882456, "compression_ratio": 1.8611111111111112, "no_speech_prob": 1.1726384627763764e-06}, {"id": 1183, "seek": 557120, "start": 5571.2, "end": 5577.099999999999, "text": " So here are the unique values of competition months open and it's all the numbers from naught to 24", "tokens": [407, 510, 366, 264, 3845, 4190, 295, 6211, 2493, 1269, 293, 309, 311, 439, 264, 3547, 490, 13138, 281, 4022], "temperature": 0.0, "avg_logprob": -0.1669171891122494, "compression_ratio": 1.854251012145749, "no_speech_prob": 6.375536827363248e-07}, {"id": 1184, "seek": 557120, "start": 5578.12, "end": 5579.599999999999, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.1669171891122494, "compression_ratio": 1.854251012145749, "no_speech_prob": 6.375536827363248e-07}, {"id": 1185, "seek": 557120, "start": 5579.599999999999, "end": 5584.24, "text": " So what that means is that there's going to be you know an embedding matrix?", "tokens": [407, 437, 300, 1355, 307, 300, 456, 311, 516, 281, 312, 291, 458, 364, 12240, 3584, 8141, 30], "temperature": 0.0, "avg_logprob": -0.1669171891122494, "compression_ratio": 1.854251012145749, "no_speech_prob": 6.375536827363248e-07}, {"id": 1186, "seek": 557120, "start": 5584.48, "end": 5589.08, "text": " That's going to have basically an embedding vector for things that aren't open yet", "tokens": [663, 311, 516, 281, 362, 1936, 364, 12240, 3584, 8062, 337, 721, 300, 3212, 380, 1269, 1939], "temperature": 0.0, "avg_logprob": -0.1669171891122494, "compression_ratio": 1.854251012145749, "no_speech_prob": 6.375536827363248e-07}, {"id": 1187, "seek": 557120, "start": 5589.32, "end": 5593.76, "text": " The things that are open a month the things that are open two months and so forth now", "tokens": [440, 721, 300, 366, 1269, 257, 1618, 264, 721, 300, 366, 1269, 732, 2493, 293, 370, 5220, 586], "temperature": 0.0, "avg_logprob": -0.1669171891122494, "compression_ratio": 1.854251012145749, "no_speech_prob": 6.375536827363248e-07}, {"id": 1188, "seek": 559376, "start": 5593.76, "end": 5601.360000000001, "text": " They absolutely could have done that as a continuous variable right they could have just had a number here", "tokens": [814, 3122, 727, 362, 1096, 300, 382, 257, 10957, 7006, 558, 436, 727, 362, 445, 632, 257, 1230, 510], "temperature": 0.0, "avg_logprob": -0.16361226474537569, "compression_ratio": 1.7543859649122806, "no_speech_prob": 3.359677123171423e-07}, {"id": 1189, "seek": 559376, "start": 5601.4400000000005, "end": 5606.8, "text": " Which is just a single number of how many months has it been open and they could have treated it as continuous and fed it", "tokens": [3013, 307, 445, 257, 2167, 1230, 295, 577, 867, 2493, 575, 309, 668, 1269, 293, 436, 727, 362, 8668, 309, 382, 10957, 293, 4636, 309], "temperature": 0.0, "avg_logprob": -0.16361226474537569, "compression_ratio": 1.7543859649122806, "no_speech_prob": 3.359677123171423e-07}, {"id": 1190, "seek": 559376, "start": 5606.8, "end": 5608.92, "text": " straight into the initial weight matrix", "tokens": [2997, 666, 264, 5883, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.16361226474537569, "compression_ratio": 1.7543859649122806, "no_speech_prob": 3.359677123171423e-07}, {"id": 1191, "seek": 559376, "start": 5610.860000000001, "end": 5615.88, "text": " What I found though and obviously what these competitors found is", "tokens": [708, 286, 1352, 1673, 293, 2745, 437, 613, 18333, 1352, 307], "temperature": 0.0, "avg_logprob": -0.16361226474537569, "compression_ratio": 1.7543859649122806, "no_speech_prob": 3.359677123171423e-07}, {"id": 1192, "seek": 559376, "start": 5617.400000000001, "end": 5621.62, "text": " Where possible it's best to treat things as categorical variables", "tokens": [2305, 1944, 309, 311, 1151, 281, 2387, 721, 382, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.16361226474537569, "compression_ratio": 1.7543859649122806, "no_speech_prob": 3.359677123171423e-07}, {"id": 1193, "seek": 562162, "start": 5621.62, "end": 5623.62, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.1823548659300193, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.0845141105164657e-06}, {"id": 1194, "seek": 562162, "start": 5624.38, "end": 5630.099999999999, "text": " And the reason for that is that like when you feed something through an embedding matrix", "tokens": [400, 264, 1778, 337, 300, 307, 300, 411, 562, 291, 3154, 746, 807, 364, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.1823548659300193, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.0845141105164657e-06}, {"id": 1195, "seek": 562162, "start": 5631.38, "end": 5638.34, "text": " You basically mean it means every level can be treated like totally differently right and so for example in this case", "tokens": [509, 1936, 914, 309, 1355, 633, 1496, 393, 312, 8668, 411, 3879, 7614, 558, 293, 370, 337, 1365, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.1823548659300193, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.0845141105164657e-06}, {"id": 1196, "seek": 562162, "start": 5638.86, "end": 5644.46, "text": " Whether something's been open for zero months or one month is right really different", "tokens": [8503, 746, 311, 668, 1269, 337, 4018, 2493, 420, 472, 1618, 307, 558, 534, 819], "temperature": 0.0, "avg_logprob": -0.1823548659300193, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.0845141105164657e-06}, {"id": 1197, "seek": 562162, "start": 5644.9, "end": 5647.94, "text": " Right and so if you fit that in as a continuous variable", "tokens": [1779, 293, 370, 498, 291, 3318, 300, 294, 382, 257, 10957, 7006], "temperature": 0.0, "avg_logprob": -0.1823548659300193, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.0845141105164657e-06}, {"id": 1198, "seek": 564794, "start": 5647.94, "end": 5652.219999999999, "text": " It would be kind of difficult for the neural net to try and find a functional form", "tokens": [467, 576, 312, 733, 295, 2252, 337, 264, 18161, 2533, 281, 853, 293, 915, 257, 11745, 1254], "temperature": 0.0, "avg_logprob": -0.17420539521334463, "compression_ratio": 1.7162629757785468, "no_speech_prob": 5.714996973438247e-07}, {"id": 1199, "seek": 564794, "start": 5652.58, "end": 5654.82, "text": " That kind of has that that big difference", "tokens": [663, 733, 295, 575, 300, 300, 955, 2649], "temperature": 0.0, "avg_logprob": -0.17420539521334463, "compression_ratio": 1.7162629757785468, "no_speech_prob": 5.714996973438247e-07}, {"id": 1200, "seek": 564794, "start": 5654.82, "end": 5658.78, "text": " It's possible because neural nets can do anything right, but you're not making it easy for it", "tokens": [467, 311, 1944, 570, 18161, 36170, 393, 360, 1340, 558, 11, 457, 291, 434, 406, 1455, 309, 1858, 337, 309], "temperature": 0.0, "avg_logprob": -0.17420539521334463, "compression_ratio": 1.7162629757785468, "no_speech_prob": 5.714996973438247e-07}, {"id": 1201, "seek": 564794, "start": 5659.139999999999, "end": 5665.219999999999, "text": " Where else if you used an embedding treated it as categorical then it'll have a totally different vector for zero versus one", "tokens": [2305, 1646, 498, 291, 1143, 364, 12240, 3584, 8668, 309, 382, 19250, 804, 550, 309, 603, 362, 257, 3879, 819, 8062, 337, 4018, 5717, 472], "temperature": 0.0, "avg_logprob": -0.17420539521334463, "compression_ratio": 1.7162629757785468, "no_speech_prob": 5.714996973438247e-07}, {"id": 1202, "seek": 564794, "start": 5665.379999999999, "end": 5669.36, "text": " Right so it seems like particularly as long as you've got enough data", "tokens": [1779, 370, 309, 2544, 411, 4098, 382, 938, 382, 291, 600, 658, 1547, 1412], "temperature": 0.0, "avg_logprob": -0.17420539521334463, "compression_ratio": 1.7162629757785468, "no_speech_prob": 5.714996973438247e-07}, {"id": 1203, "seek": 564794, "start": 5670.339999999999, "end": 5671.86, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.17420539521334463, "compression_ratio": 1.7162629757785468, "no_speech_prob": 5.714996973438247e-07}, {"id": 1204, "seek": 564794, "start": 5671.86, "end": 5676.62, "text": " The treating columns as categorical variables where possible is a better idea", "tokens": [440, 15083, 13766, 382, 19250, 804, 9102, 689, 1944, 307, 257, 1101, 1558], "temperature": 0.0, "avg_logprob": -0.17420539521334463, "compression_ratio": 1.7162629757785468, "no_speech_prob": 5.714996973438247e-07}, {"id": 1205, "seek": 567662, "start": 5676.62, "end": 5681.38, "text": " and so I say when I say where possible that kind of basically means like", "tokens": [293, 370, 286, 584, 562, 286, 584, 689, 1944, 300, 733, 295, 1936, 1355, 411], "temperature": 0.0, "avg_logprob": -0.17453041076660156, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.1189401902811369e-06}, {"id": 1206, "seek": 567662, "start": 5682.9, "end": 5684.9, "text": " Where the cardinality is not too high", "tokens": [2305, 264, 2920, 259, 1860, 307, 406, 886, 1090], "temperature": 0.0, "avg_logprob": -0.17453041076660156, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.1189401902811369e-06}, {"id": 1207, "seek": 567662, "start": 5685.98, "end": 5687.98, "text": " You know so if this was like", "tokens": [509, 458, 370, 498, 341, 390, 411], "temperature": 0.0, "avg_logprob": -0.17453041076660156, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.1189401902811369e-06}, {"id": 1208, "seek": 567662, "start": 5690.18, "end": 5691.62, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.17453041076660156, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.1189401902811369e-06}, {"id": 1209, "seek": 567662, "start": 5691.62, "end": 5695.44, "text": " The sales ID number that was like uniquely different on every row", "tokens": [440, 5763, 7348, 1230, 300, 390, 411, 31474, 819, 322, 633, 5386], "temperature": 0.0, "avg_logprob": -0.17453041076660156, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.1189401902811369e-06}, {"id": 1210, "seek": 567662, "start": 5696.26, "end": 5700.38, "text": " You can't treat that as a categorical variable right because you know", "tokens": [509, 393, 380, 2387, 300, 382, 257, 19250, 804, 7006, 558, 570, 291, 458], "temperature": 0.0, "avg_logprob": -0.17453041076660156, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.1189401902811369e-06}, {"id": 1211, "seek": 570038, "start": 5700.38, "end": 5707.3, "text": " It would be a huge embedding matrix and everything only appears once or ditto for like kilometers away from the nearest store", "tokens": [467, 576, 312, 257, 2603, 12240, 3584, 8141, 293, 1203, 787, 7038, 1564, 420, 274, 34924, 337, 411, 13904, 1314, 490, 264, 23831, 3531], "temperature": 0.0, "avg_logprob": -0.17080438265236475, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.118938939725922e-06}, {"id": 1212, "seek": 570038, "start": 5708.18, "end": 5712.3, "text": " To two decimal places you wouldn't make a categorical variable right?", "tokens": [1407, 732, 26601, 3190, 291, 2759, 380, 652, 257, 19250, 804, 7006, 558, 30], "temperature": 0.0, "avg_logprob": -0.17080438265236475, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.118938939725922e-06}, {"id": 1213, "seek": 570038, "start": 5714.26, "end": 5716.900000000001, "text": " So that's kind of the that's kind of the rule of thumb", "tokens": [407, 300, 311, 733, 295, 264, 300, 311, 733, 295, 264, 4978, 295, 9298], "temperature": 0.0, "avg_logprob": -0.17080438265236475, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.118938939725922e-06}, {"id": 1214, "seek": 570038, "start": 5717.46, "end": 5722.9400000000005, "text": " That they both used in this competition in fact if we scroll down to their choices", "tokens": [663, 436, 1293, 1143, 294, 341, 6211, 294, 1186, 498, 321, 11369, 760, 281, 641, 7994], "temperature": 0.0, "avg_logprob": -0.17080438265236475, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.118938939725922e-06}, {"id": 1215, "seek": 572294, "start": 5722.94, "end": 5730.54, "text": " Here is how they did it right there continuous variables with things that were genuinely", "tokens": [1692, 307, 577, 436, 630, 309, 558, 456, 10957, 9102, 365, 721, 300, 645, 17839], "temperature": 0.0, "avg_logprob": -0.263567571770655, "compression_ratio": 1.6009389671361502, "no_speech_prob": 8.446202173217898e-07}, {"id": 1216, "seek": 572294, "start": 5730.86, "end": 5735.339999999999, "text": " Continuous like number of kilometers away to the competitor the temperature stuff", "tokens": [14674, 12549, 411, 1230, 295, 13904, 1314, 281, 264, 27266, 264, 4292, 1507], "temperature": 0.0, "avg_logprob": -0.263567571770655, "compression_ratio": 1.6009389671361502, "no_speech_prob": 8.446202173217898e-07}, {"id": 1217, "seek": 572294, "start": 5735.98, "end": 5741.0199999999995, "text": " Right the number you know the specific number in the Google Trend right?", "tokens": [1779, 264, 1230, 291, 458, 264, 2685, 1230, 294, 264, 3329, 37417, 558, 30], "temperature": 0.0, "avg_logprob": -0.263567571770655, "compression_ratio": 1.6009389671361502, "no_speech_prob": 8.446202173217898e-07}, {"id": 1218, "seek": 572294, "start": 5742.5, "end": 5745.7, "text": " Where else everything else basically they treated as categorical", "tokens": [2305, 1646, 1203, 1646, 1936, 436, 8668, 382, 19250, 804], "temperature": 0.0, "avg_logprob": -0.263567571770655, "compression_ratio": 1.6009389671361502, "no_speech_prob": 8.446202173217898e-07}, {"id": 1219, "seek": 574570, "start": 5745.7, "end": 5750.46, "text": " Okay, so that's it for today, so", "tokens": [1033, 11, 370, 300, 311, 309, 337, 965, 11, 370], "temperature": 0.0, "avg_logprob": -0.4283817225489123, "compression_ratio": 1.441860465116279, "no_speech_prob": 2.406034582236316e-06}, {"id": 1220, "seek": 574570, "start": 5751.26, "end": 5758.22, "text": " Yeah, next time we'll we'll finish this off. We'll see we'll see how to turn this into a neural network", "tokens": [865, 11, 958, 565, 321, 603, 321, 603, 2413, 341, 766, 13, 492, 603, 536, 321, 603, 536, 577, 281, 1261, 341, 666, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.4283817225489123, "compression_ratio": 1.441860465116279, "no_speech_prob": 2.406034582236316e-06}, {"id": 1221, "seek": 574570, "start": 5759.7, "end": 5761.7, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.4283817225489123, "compression_ratio": 1.441860465116279, "no_speech_prob": 2.406034582236316e-06}, {"id": 1222, "seek": 576170, "start": 5761.7, "end": 5776.179999999999, "text": " Yeah, kind of wrap things up, so see you then", "tokens": [50364, 865, 11, 733, 295, 7019, 721, 493, 11, 370, 536, 291, 550, 51088], "temperature": 0.0, "avg_logprob": -0.35443121592203775, "compression_ratio": 0.8653846153846154, "no_speech_prob": 1.5687055565649644e-05}], "language": "en"}