{"text": " Yeah, thanks to Jeremy for sharing that. And so next I kind of wanted to return to this workbook 6b. I have, or I guess it was workbook six and 6b is going to look at GRUs and this is kind of by popular request. There are several questions about wanting to know what was going on underneath the hood with GRUs. So I'm going to illustrate that, although I want to I want to highlight that in practice. Please just use PyTorch's implementation of a GRU or you know whatever your framework of choice is. This isn't something you would typically implement by hand, but I wanted to show you kind of what it was doing with the low-level details. And I'm going to start with a few slides. So a problem with RNNs is that they suffer from short-term memory. And so what this means is that as your input sequence gets longer, RNNs have more trouble kind of remembering or keeping in their state what happened early on. And so this can be illustrated. An example I took from a Chris Ola blog post is say you had a sentence, I grew up in France and I moved to the USA when I was 25. You could even have some more sentences and then you say I still speak fluent. Do you guys want to guess what the word should be? French, right. And so that's something where you kind of need this key piece of information from very early on. And you could have a lot of words, you know even more words than I've shown in between. And so this can be challenging for RNNs. And so both LSTMs and GRUs are solutions to this. And so you would typically use an LSTM or a GRU. LSTMs are much older than GRUs. They have more gates. Yeah and so go into, so just kind of how these diagrams, and so this is from a Michael Nugin blog post, an illustrated guide to LSTMs and GRUs has really great illustrations. The idea with a simple RNN, so this is not using an LSTM or a GRU, is you would have these cells and you'd have a whole string of these kind of connected together. And basically here, I don't know if you can see, this is the new hidden state coming in and this is the input here. So you get your hidden state kind of from the previous step and then the new input is the next word in your sequence. You're you know combining those using a hyperbolic tan and then that becomes the next hidden state which will be passed to the next cell like this to be combined with the new input. And so what a GRU does, and I chose GRUs since they're more modern and have fewer gates, but it's the same idea if you wanted to look at LSTMs, is that you have a reset gate and an update gate. And I'm going to show, I think this is actually kind of easier to understand looking at code, but the idea, so you're introducing some sigmoids here to combine things and sigmoid gives you values between 0 and 1. And so 0 indicates forgetting. And so this is kind of gives you a few more levers basically to be learning about how to balance kind of taking from your hidden state and your new inputs. So at this point I think I will switch to the code. So just to remember since I guess it's been maybe a week since we did this notebook, here we are working on this synthetic data set of counting the English number, counting in English for numbers 8001, 8002, 8003. And we previously, actually so we're going to we're going to kind of walk through and do some refactoring. So we previously had this model, and this was in the previous lesson, of an embedding and kind of taking your input here. Nv I think is the number of vocab and the number of hidden or size of your hidden, and h is sorry, dimension of the the hidden state. We were using PyTorch's GRU, and we'll tell kind of how many, what we want our dimensions for the the hidden state to be. In the last lesson I had a two here, we were stacking them. I decided to switch that to one just to kind of have the simplest example that we're going to we're going to replace this implementation in a moment. And so stop using PyTorch's and put our own in its place. Then a linear layer, then batch norm, and then self.h. So i to h is hidden input to hidden, h to zero or h to o is hidden to output, and then this is hidden. And we had our forward step is this RNN, which in our case is the GRU. Detach means that you're going to stop kind of cut off that piece of history, that's something you're not going to keep recording for the next step. And then kind of what we do is take the the results of the RNN, apply batch norm, and then apply this linear layer of set of weights that we're learning. And let me add to review, let me ask who remembers what batch norm is? Do you want the catch box? Anyone? Batch norm? Good team catch. Is this just the normalization of just within the batch you're processing at the time? So it's the normalization of the activations in your network. And so and that's useful because otherwise your activations could get very close to zero or they could kind of explode, you know, in any situation where we're doing these repeated multiplications. And so batch norm is kind of learning kind of a basically a mean and standard deviation for your activations to keep them normalized. Yeah, and so again kind of to know like the components of neural networks really are these relatively simple building blocks of linear layers, batch norms, and nonlinearities. So I read this again with just using one GRU as opposed to two, and it actually still did pretty great for I mean for such a simple model of 74% accuracy. And I said let's let's make our own GRU. And so first we're going to do just a little bit of refactoring to make this clearer. And so we created an RNN loop. So remember kind of a key part of the RNN. Actually let me bring that slide up again. It might be helpful to remember. We kind of had done this whole refactoring of how do we even build an RNN. And a key part of that is putting things into a for loop. So we can start with this version where we're not using a for loop. Again remember here each color arrow denotes kind of one set of weights that you're learning. So like one matrix. We don't really want to have you know this complicated architecture of a bunch of if statements where we add each word sequentially. So we refactor that into a for loop. And then we also choose to kind of keep the output within that for loop. So here our call it our RNN loop is it's got a result. And then it's going through and calling some sort of cell which is set oh which is passed in when we call RNN loop. So that's going to be a GRU cell which we'll see in a moment. Then we want to append our hidden state onto the result to keep track of that and keep going. And so yeah this is actually a kind of you know using an RNN can be very very few lines of code as what is what's going on here. And so this is using PyTorch's implementation of a GRU cell. And so when you use a PyTorch GRU the kind of underlying building block to that is the PyTorch GRU cell. We're going to replace that with our own implementation in a moment. We're just refactoring to make it clearer kind of how we do that. Yeah so calling this model 6. Subclasses model 5. And it has an RNN cell. It's got its hidden state. Remember again the hidden state is just a set of activations which kind of contains your information so far of how you're kind of representing what you've what you've seen. We run this it's 80% and this was there's a fair amount of variation just in kind of from time to time. So that was 76 before. What are we going to get now? And this time it's 75%. So this is just kind of replicated the the same type of network. Yes Jeremy? Oh yeah so the that's a great point. The res colon minus 1 is picking out the most recent hidden state which is what you want. And then again detaching the the previous history. Is there more you wanted to say about that Jeremy? Oh the model 5 version. So I see that lot of res comma H is doing this internally. Yes okay so what Jeremy's highlighting this is a great point is that basically the result is kind of like the whole list of everything you've done so far and H is the most recent one. And so this method could have instead just returned res and you could pick off res minus 1 and that gives you H. And so this is kind of just how the PyTorch one was implemented that it returns both but H is really just the last entry in res because the hidden state we're interested in is the most recent one but we're you know keep appending to this array to have our whole history of the hidden states. And so in going from model 5 to model 6 all we've done is refactored by writing our own RNN loop which previously PyTorch was handling but at this point we're still using PyTorch's implementation of GRU cell which is basically you put it inside a loop to get a GRU is yeah so a GRU is a loop with a GRU cell inside of it. And so next let's take a look at what a GRU cell is and so here I used some code from github although heavily refactored on implementing LSTMs and well I just took the GRU part. So we're gonna need we'll have kind of gate X and well we'll have a reset gate an update gate and a new gate and each of those will take the input actually let me go to the math equation I think is only the best way to start with this. Sorry. So a GRU in equations so here H represents the hidden state X represents the input so the update gate is going to be sigmoid of some linear combination of the input plus the hidden state and A and B are sets of weights that we're going to learn. Then the reset gate is sigmoid of a different linear combination of the input and the previous hidden state and again we're going to learn the weights for those for C and D that we're multiplying them by and again remember sigmoid is always something between 0 and 1 so basically this is kind of taking linear combinations of your input and your hidden state and then getting something between 0 and 1 with them. I also want to highlight that these equations can have a bias term as well I left it out just kind of for simplicity but you could have a bias that you're you're learning as well which is just you know constant at the end. Then there's a new gate and new gate is taking hyperbolic tan of a combination of the input and then here you're taking the Hadamard product of the reset gate and the hidden state and does anyone remember what a Hadamard product is? Okay, so is that a yes? Exactly, element-wise multiplication so instead of the row by column of traditional matrix multiplication this is multiplying you know the first element by the first second by the second and so on and that's that represented and I looked this up because actually I was not remembering what symbol to use and people do use a variety of symbols to represent this and so I was looking at one resource that kind of had like the little circles where it's white on the inside kind of like a plain circle so I went with this but it can be represented different ways. So this is your new gate hyperbolic tan is between negative 1 and 1 and then to get the the new hidden state so everything we've been doing so far has been combining the hidden state that was spit out at the previous step with your new input which is the the next word. Now we're going to take basically a weighted, effectively this is a weighted average of the new gate and the previous hidden state so whenever you see something of the form you know z times something plus 1 minus z times something that's in a way a kind of weighted average where you know we're giving z weight to the first thing and 1 minus z weight to the the second item where here z was this update gate that we learned. So this these equations the pieces are all simple it's kind of like a little bit hard to think about what they mean together but this is really kind of just this combination of sigmoids and hyperbolic tan with a bunch of linear averages but it gives us kind of a number of things we get to learn about kind of how much weight to give the in you know the input of the next word versus your previous hidden state at each step and so this will make it easier that if you do kind of want to give less weight to something new and put more more weight on the hidden state from the past that you now kind of have a way to give more weight to the past I guess that's how you could describe it there are questions about these equations I think this will help it's nice to kind of go back and forth between these and the code the code for these. Something else to note is that so we're multiplying xt by a few different things here I've called them a C and G in the next when we do this in code we're basically going to combine that and learn all those variables together but you could then you know split it out into two three separate variables after you learn them which is what we'll do in the code and then H the previous hidden state is being multiplied by BD and H here and so again we can kind of learn those all at once in this matrix and then split it into three pieces and so that's that's what's going on in the code here so here the language this is why we have three times the dimension of the hidden state and we're calling that I to H and H to H and so basically each component down here is so I stands for input H stands for hidden you see the resource reset gate is this combination of input and hidden and basically R is just the first of our our three and that's the one that corresponds to the reset gate so here the kind of naming convention is what are the linear factors you want to multiply the new input that will be going through the reset gate by the input for the update gate and then the input for the new gate so this is kind of the process of you learn these as you know these three kind of bigger or not three as these linear matrices and then break them into three pieces is what's going on here and put them into your your reset gate your update gate and your new gate and then return the weighted average of update gate times hidden state and one minus update gate times new gate there questions about this I can even let me go back to the equations one more time just to to compare so yeah we've kind of combined the three things but we're just taking really and this is I mean this is so much of deep learning is taking linear linear combinations kind of doing a linear operation of multiplying and adding two things and then putting it through a non-linearity such as sigmoid or tange and it's neat that that gives us kind of a more power an RNN that's capable of kind of having this long-term memory and so then what that looks like when we create model 7 is now instead of doing nn.grucell which was calling from pythons nn module and taking the Python implementation of grucell we're using the one we made ourselves and so running this let's see hopefully it trains this time yeah and so we get 73% very similar and the reason we got this you know the similar accuracy on these three versions is these were three versions of the same thing we were just refactoring to write our own grucell any questions yes sorry now that we have the what oh that is a good question so we do not have dropout in this oh you're asking where was the dropout oh and the AWD LSTM so this keep in mind this is like a kind of very simple model that we use to even in the original notebook 6 this was just to kind of illustrate a simple RNN to see the dropout I would go back to so kind of even this version before we didn't have any dropout since it's kind of this tiny tiny data set for a simple model okay yeah I'll look at adding it next week I can pull up a seek to seek translation which is kind of more similar to what AWD LSTM well I guess AWD LSTM we used for a language model just to show and seek to seek where dropout is find it so here you can kind of see in the structure we've got dropout for the encoder embeddings and then we have dropout down here for the output from the decoder so those are two places actually now that I remember AWD LSTM kind of a key thing was that it applies dropout I think basically everywhere like it's got dropout in a few different places but this is oh to show where okay sorry I misunderstood the question yeah we can we can do that next time sorry about that other questions on on GRU's let me just make sure I didn't have any more slides I think this is it yes that is it okay well I will yeah I will see you on Thursday", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.24, "text": " Yeah, thanks to Jeremy for sharing that. And so next I kind of wanted to return", "tokens": [865, 11, 3231, 281, 17809, 337, 5414, 300, 13, 400, 370, 958, 286, 733, 295, 1415, 281, 2736], "temperature": 0.0, "avg_logprob": -0.22934897129352277, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.0061906552873551846}, {"id": 1, "seek": 0, "start": 7.24, "end": 15.56, "text": " to this workbook 6b. I have, or I guess it was workbook six and 6b is going to", "tokens": [281, 341, 589, 2939, 1386, 65, 13, 286, 362, 11, 420, 286, 2041, 309, 390, 589, 2939, 2309, 293, 1386, 65, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.22934897129352277, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.0061906552873551846}, {"id": 2, "seek": 0, "start": 15.56, "end": 19.400000000000002, "text": " look at GRUs and this is kind of by popular request. There are several", "tokens": [574, 412, 10903, 29211, 293, 341, 307, 733, 295, 538, 3743, 5308, 13, 821, 366, 2940], "temperature": 0.0, "avg_logprob": -0.22934897129352277, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.0061906552873551846}, {"id": 3, "seek": 0, "start": 19.400000000000002, "end": 24.240000000000002, "text": " questions about wanting to know what was going on underneath the hood with GRUs.", "tokens": [1651, 466, 7935, 281, 458, 437, 390, 516, 322, 7223, 264, 13376, 365, 10903, 29211, 13], "temperature": 0.0, "avg_logprob": -0.22934897129352277, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.0061906552873551846}, {"id": 4, "seek": 0, "start": 24.240000000000002, "end": 28.76, "text": " So I'm going to illustrate that, although I want to I want to highlight that in", "tokens": [407, 286, 478, 516, 281, 23221, 300, 11, 4878, 286, 528, 281, 286, 528, 281, 5078, 300, 294], "temperature": 0.0, "avg_logprob": -0.22934897129352277, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.0061906552873551846}, {"id": 5, "seek": 2876, "start": 28.76, "end": 34.4, "text": " practice. Please just use PyTorch's implementation of a GRU or you know", "tokens": [3124, 13, 2555, 445, 764, 9953, 51, 284, 339, 311, 11420, 295, 257, 10903, 52, 420, 291, 458], "temperature": 0.0, "avg_logprob": -0.20977039086191276, "compression_ratio": 1.4427860696517414, "no_speech_prob": 7.25273639545776e-05}, {"id": 6, "seek": 2876, "start": 34.4, "end": 39.6, "text": " whatever your framework of choice is. This isn't something you would typically", "tokens": [2035, 428, 8388, 295, 3922, 307, 13, 639, 1943, 380, 746, 291, 576, 5850], "temperature": 0.0, "avg_logprob": -0.20977039086191276, "compression_ratio": 1.4427860696517414, "no_speech_prob": 7.25273639545776e-05}, {"id": 7, "seek": 2876, "start": 39.6, "end": 42.980000000000004, "text": " implement by hand, but I wanted to show you kind of what it was doing with the", "tokens": [4445, 538, 1011, 11, 457, 286, 1415, 281, 855, 291, 733, 295, 437, 309, 390, 884, 365, 264], "temperature": 0.0, "avg_logprob": -0.20977039086191276, "compression_ratio": 1.4427860696517414, "no_speech_prob": 7.25273639545776e-05}, {"id": 8, "seek": 2876, "start": 42.980000000000004, "end": 48.040000000000006, "text": " low-level details. And I'm going to start with a few slides.", "tokens": [2295, 12, 12418, 4365, 13, 400, 286, 478, 516, 281, 722, 365, 257, 1326, 9788, 13], "temperature": 0.0, "avg_logprob": -0.20977039086191276, "compression_ratio": 1.4427860696517414, "no_speech_prob": 7.25273639545776e-05}, {"id": 9, "seek": 4804, "start": 48.04, "end": 63.4, "text": " So a problem with RNNs is that they suffer from short-term memory. And so", "tokens": [407, 257, 1154, 365, 45702, 45, 82, 307, 300, 436, 9753, 490, 2099, 12, 7039, 4675, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.09759712219238281, "compression_ratio": 1.4303797468354431, "no_speech_prob": 4.157172497798456e-06}, {"id": 10, "seek": 4804, "start": 63.4, "end": 69.0, "text": " what this means is that as your input sequence gets longer, RNNs have more", "tokens": [437, 341, 1355, 307, 300, 382, 428, 4846, 8310, 2170, 2854, 11, 45702, 45, 82, 362, 544], "temperature": 0.0, "avg_logprob": -0.09759712219238281, "compression_ratio": 1.4303797468354431, "no_speech_prob": 4.157172497798456e-06}, {"id": 11, "seek": 4804, "start": 69.0, "end": 73.64, "text": " trouble kind of remembering or keeping in their state what happened early on.", "tokens": [5253, 733, 295, 20719, 420, 5145, 294, 641, 1785, 437, 2011, 2440, 322, 13], "temperature": 0.0, "avg_logprob": -0.09759712219238281, "compression_ratio": 1.4303797468354431, "no_speech_prob": 4.157172497798456e-06}, {"id": 12, "seek": 7364, "start": 73.64, "end": 78.56, "text": " And so this can be illustrated. An example I took from a Chris Ola blog post", "tokens": [400, 370, 341, 393, 312, 33875, 13, 1107, 1365, 286, 1890, 490, 257, 6688, 422, 875, 6968, 2183], "temperature": 0.0, "avg_logprob": -0.11152101735599705, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.0055991121334955e-05}, {"id": 13, "seek": 7364, "start": 78.56, "end": 83.72, "text": " is say you had a sentence, I grew up in France and I moved to the USA when I was", "tokens": [307, 584, 291, 632, 257, 8174, 11, 286, 6109, 493, 294, 6190, 293, 286, 4259, 281, 264, 10827, 562, 286, 390], "temperature": 0.0, "avg_logprob": -0.11152101735599705, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.0055991121334955e-05}, {"id": 14, "seek": 7364, "start": 83.72, "end": 87.64, "text": " 25. You could even have some more sentences and then you say I still speak", "tokens": [3552, 13, 509, 727, 754, 362, 512, 544, 16579, 293, 550, 291, 584, 286, 920, 1710], "temperature": 0.0, "avg_logprob": -0.11152101735599705, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.0055991121334955e-05}, {"id": 15, "seek": 7364, "start": 87.64, "end": 94.04, "text": " fluent. Do you guys want to guess what the word should be? French, right. And so", "tokens": [40799, 13, 1144, 291, 1074, 528, 281, 2041, 437, 264, 1349, 820, 312, 30, 5522, 11, 558, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.11152101735599705, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.0055991121334955e-05}, {"id": 16, "seek": 7364, "start": 94.04, "end": 97.76, "text": " that's something where you kind of need this key piece of information from very", "tokens": [300, 311, 746, 689, 291, 733, 295, 643, 341, 2141, 2522, 295, 1589, 490, 588], "temperature": 0.0, "avg_logprob": -0.11152101735599705, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.0055991121334955e-05}, {"id": 17, "seek": 7364, "start": 97.76, "end": 101.4, "text": " early on. And you could have a lot of words, you know even more words than", "tokens": [2440, 322, 13, 400, 291, 727, 362, 257, 688, 295, 2283, 11, 291, 458, 754, 544, 2283, 813], "temperature": 0.0, "avg_logprob": -0.11152101735599705, "compression_ratio": 1.6082474226804124, "no_speech_prob": 4.0055991121334955e-05}, {"id": 18, "seek": 10140, "start": 101.4, "end": 108.48, "text": " I've shown in between. And so this can be challenging for RNNs. And so", "tokens": [286, 600, 4898, 294, 1296, 13, 400, 370, 341, 393, 312, 7595, 337, 45702, 45, 82, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.14752770474082547, "compression_ratio": 1.4784688995215312, "no_speech_prob": 0.00012730172602459788}, {"id": 19, "seek": 10140, "start": 108.48, "end": 115.76, "text": " both LSTMs and GRUs are solutions to this. And so you would typically use an", "tokens": [1293, 441, 6840, 26386, 293, 10903, 29211, 366, 6547, 281, 341, 13, 400, 370, 291, 576, 5850, 764, 364], "temperature": 0.0, "avg_logprob": -0.14752770474082547, "compression_ratio": 1.4784688995215312, "no_speech_prob": 0.00012730172602459788}, {"id": 20, "seek": 10140, "start": 115.76, "end": 125.76, "text": " LSTM or a GRU. LSTMs are much older than GRUs. They have more gates. Yeah and so go", "tokens": [441, 6840, 44, 420, 257, 10903, 52, 13, 441, 6840, 26386, 366, 709, 4906, 813, 10903, 29211, 13, 814, 362, 544, 19792, 13, 865, 293, 370, 352], "temperature": 0.0, "avg_logprob": -0.14752770474082547, "compression_ratio": 1.4784688995215312, "no_speech_prob": 0.00012730172602459788}, {"id": 21, "seek": 10140, "start": 125.76, "end": 130.08, "text": " into, so just kind of how these diagrams, and so this is from a Michael Nugin", "tokens": [666, 11, 370, 445, 733, 295, 577, 613, 36709, 11, 293, 370, 341, 307, 490, 257, 5116, 426, 697, 259], "temperature": 0.0, "avg_logprob": -0.14752770474082547, "compression_ratio": 1.4784688995215312, "no_speech_prob": 0.00012730172602459788}, {"id": 22, "seek": 13008, "start": 130.08, "end": 135.60000000000002, "text": " blog post, an illustrated guide to LSTMs and GRUs has really great", "tokens": [6968, 2183, 11, 364, 33875, 5934, 281, 441, 6840, 26386, 293, 10903, 29211, 575, 534, 869], "temperature": 0.0, "avg_logprob": -0.14187080081146541, "compression_ratio": 1.5757575757575757, "no_speech_prob": 6.813744403189048e-05}, {"id": 23, "seek": 13008, "start": 135.60000000000002, "end": 142.0, "text": " illustrations. The idea with a simple RNN, so this is not using an", "tokens": [34540, 13, 440, 1558, 365, 257, 2199, 45702, 45, 11, 370, 341, 307, 406, 1228, 364], "temperature": 0.0, "avg_logprob": -0.14187080081146541, "compression_ratio": 1.5757575757575757, "no_speech_prob": 6.813744403189048e-05}, {"id": 24, "seek": 13008, "start": 142.0, "end": 148.24, "text": " LSTM or a GRU, is you would have these cells and you'd have a whole string of", "tokens": [441, 6840, 44, 420, 257, 10903, 52, 11, 307, 291, 576, 362, 613, 5438, 293, 291, 1116, 362, 257, 1379, 6798, 295], "temperature": 0.0, "avg_logprob": -0.14187080081146541, "compression_ratio": 1.5757575757575757, "no_speech_prob": 6.813744403189048e-05}, {"id": 25, "seek": 13008, "start": 148.24, "end": 155.0, "text": " these kind of connected together. And basically here, I don't know if you can", "tokens": [613, 733, 295, 4582, 1214, 13, 400, 1936, 510, 11, 286, 500, 380, 458, 498, 291, 393], "temperature": 0.0, "avg_logprob": -0.14187080081146541, "compression_ratio": 1.5757575757575757, "no_speech_prob": 6.813744403189048e-05}, {"id": 26, "seek": 13008, "start": 155.0, "end": 159.68, "text": " see, this is the new hidden state coming in and this is the input here. So", "tokens": [536, 11, 341, 307, 264, 777, 7633, 1785, 1348, 294, 293, 341, 307, 264, 4846, 510, 13, 407], "temperature": 0.0, "avg_logprob": -0.14187080081146541, "compression_ratio": 1.5757575757575757, "no_speech_prob": 6.813744403189048e-05}, {"id": 27, "seek": 15968, "start": 159.68, "end": 164.64000000000001, "text": " you get your hidden state kind of from the previous step and then the new input", "tokens": [291, 483, 428, 7633, 1785, 733, 295, 490, 264, 3894, 1823, 293, 550, 264, 777, 4846], "temperature": 0.0, "avg_logprob": -0.11940236342580694, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.32124691340141e-05}, {"id": 28, "seek": 15968, "start": 164.64000000000001, "end": 169.88, "text": " is the next word in your sequence. You're you know combining those using a", "tokens": [307, 264, 958, 1349, 294, 428, 8310, 13, 509, 434, 291, 458, 21928, 729, 1228, 257], "temperature": 0.0, "avg_logprob": -0.11940236342580694, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.32124691340141e-05}, {"id": 29, "seek": 15968, "start": 169.88, "end": 176.20000000000002, "text": " hyperbolic tan and then that becomes the next hidden state which will be passed", "tokens": [9848, 65, 7940, 7603, 293, 550, 300, 3643, 264, 958, 7633, 1785, 597, 486, 312, 4678], "temperature": 0.0, "avg_logprob": -0.11940236342580694, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.32124691340141e-05}, {"id": 30, "seek": 15968, "start": 176.20000000000002, "end": 181.96, "text": " to the next cell like this to be combined with the new input. And so", "tokens": [281, 264, 958, 2815, 411, 341, 281, 312, 9354, 365, 264, 777, 4846, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.11940236342580694, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.32124691340141e-05}, {"id": 31, "seek": 15968, "start": 181.96, "end": 189.04000000000002, "text": " what a GRU does, and I chose GRUs since they're more modern and have fewer", "tokens": [437, 257, 10903, 52, 775, 11, 293, 286, 5111, 10903, 29211, 1670, 436, 434, 544, 4363, 293, 362, 13366], "temperature": 0.0, "avg_logprob": -0.11940236342580694, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.32124691340141e-05}, {"id": 32, "seek": 18904, "start": 189.04, "end": 194.56, "text": " gates, but it's the same idea if you wanted to look at LSTMs, is that you have", "tokens": [19792, 11, 457, 309, 311, 264, 912, 1558, 498, 291, 1415, 281, 574, 412, 441, 6840, 26386, 11, 307, 300, 291, 362], "temperature": 0.0, "avg_logprob": -0.14615693746828565, "compression_ratio": 1.6025641025641026, "no_speech_prob": 5.307267201715149e-05}, {"id": 33, "seek": 18904, "start": 194.56, "end": 199.79999999999998, "text": " a reset gate and an update gate. And I'm going to show, I think this is", "tokens": [257, 14322, 8539, 293, 364, 5623, 8539, 13, 400, 286, 478, 516, 281, 855, 11, 286, 519, 341, 307], "temperature": 0.0, "avg_logprob": -0.14615693746828565, "compression_ratio": 1.6025641025641026, "no_speech_prob": 5.307267201715149e-05}, {"id": 34, "seek": 18904, "start": 199.79999999999998, "end": 204.16, "text": " actually kind of easier to understand looking at code, but the idea, so you're", "tokens": [767, 733, 295, 3571, 281, 1223, 1237, 412, 3089, 11, 457, 264, 1558, 11, 370, 291, 434], "temperature": 0.0, "avg_logprob": -0.14615693746828565, "compression_ratio": 1.6025641025641026, "no_speech_prob": 5.307267201715149e-05}, {"id": 35, "seek": 18904, "start": 204.16, "end": 210.76, "text": " introducing some sigmoids here to combine things and sigmoid gives", "tokens": [15424, 512, 4556, 3280, 3742, 510, 281, 10432, 721, 293, 4556, 3280, 327, 2709], "temperature": 0.0, "avg_logprob": -0.14615693746828565, "compression_ratio": 1.6025641025641026, "no_speech_prob": 5.307267201715149e-05}, {"id": 36, "seek": 18904, "start": 210.76, "end": 217.39999999999998, "text": " you values between 0 and 1. And so 0 indicates forgetting. And so this is kind", "tokens": [291, 4190, 1296, 1958, 293, 502, 13, 400, 370, 1958, 16203, 25428, 13, 400, 370, 341, 307, 733], "temperature": 0.0, "avg_logprob": -0.14615693746828565, "compression_ratio": 1.6025641025641026, "no_speech_prob": 5.307267201715149e-05}, {"id": 37, "seek": 21740, "start": 217.4, "end": 221.44, "text": " of gives you a few more levers basically to be learning about how", "tokens": [295, 2709, 291, 257, 1326, 544, 45571, 1936, 281, 312, 2539, 466, 577], "temperature": 0.0, "avg_logprob": -0.1347448984781901, "compression_ratio": 1.5544041450777202, "no_speech_prob": 3.8829733966849744e-05}, {"id": 38, "seek": 21740, "start": 221.44, "end": 226.88, "text": " to balance kind of taking from your hidden state and your new inputs. So at", "tokens": [281, 4772, 733, 295, 1940, 490, 428, 7633, 1785, 293, 428, 777, 15743, 13, 407, 412], "temperature": 0.0, "avg_logprob": -0.1347448984781901, "compression_ratio": 1.5544041450777202, "no_speech_prob": 3.8829733966849744e-05}, {"id": 39, "seek": 21740, "start": 226.88, "end": 236.44, "text": " this point I think I will switch to the code. So just to remember since I guess", "tokens": [341, 935, 286, 519, 286, 486, 3679, 281, 264, 3089, 13, 407, 445, 281, 1604, 1670, 286, 2041], "temperature": 0.0, "avg_logprob": -0.1347448984781901, "compression_ratio": 1.5544041450777202, "no_speech_prob": 3.8829733966849744e-05}, {"id": 40, "seek": 21740, "start": 236.44, "end": 244.32, "text": " it's been maybe a week since we did this notebook, here we are working on this", "tokens": [309, 311, 668, 1310, 257, 1243, 1670, 321, 630, 341, 21060, 11, 510, 321, 366, 1364, 322, 341], "temperature": 0.0, "avg_logprob": -0.1347448984781901, "compression_ratio": 1.5544041450777202, "no_speech_prob": 3.8829733966849744e-05}, {"id": 41, "seek": 24432, "start": 244.32, "end": 250.0, "text": " synthetic data set of counting the English number, counting in English for", "tokens": [23420, 1412, 992, 295, 13251, 264, 3669, 1230, 11, 13251, 294, 3669, 337], "temperature": 0.0, "avg_logprob": -0.13162394762039184, "compression_ratio": 1.6137566137566137, "no_speech_prob": 2.586649134173058e-05}, {"id": 42, "seek": 24432, "start": 250.0, "end": 259.36, "text": " numbers 8001, 8002, 8003. And we previously, actually so we're going to", "tokens": [3547, 13083, 16, 11, 13083, 17, 11, 13083, 18, 13, 400, 321, 8046, 11, 767, 370, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.13162394762039184, "compression_ratio": 1.6137566137566137, "no_speech_prob": 2.586649134173058e-05}, {"id": 43, "seek": 24432, "start": 259.36, "end": 264.6, "text": " we're going to kind of walk through and do some refactoring. So we previously had", "tokens": [321, 434, 516, 281, 733, 295, 1792, 807, 293, 360, 512, 1895, 578, 3662, 13, 407, 321, 8046, 632], "temperature": 0.0, "avg_logprob": -0.13162394762039184, "compression_ratio": 1.6137566137566137, "no_speech_prob": 2.586649134173058e-05}, {"id": 44, "seek": 24432, "start": 264.6, "end": 271.84, "text": " this model, and this was in the previous lesson, of an embedding and kind of", "tokens": [341, 2316, 11, 293, 341, 390, 294, 264, 3894, 6898, 11, 295, 364, 12240, 3584, 293, 733, 295], "temperature": 0.0, "avg_logprob": -0.13162394762039184, "compression_ratio": 1.6137566137566137, "no_speech_prob": 2.586649134173058e-05}, {"id": 45, "seek": 27184, "start": 271.84, "end": 280.32, "text": " taking your input here. Nv I think is the number of vocab and the number of hidden", "tokens": [1940, 428, 4846, 510, 13, 426, 85, 286, 519, 307, 264, 1230, 295, 2329, 455, 293, 264, 1230, 295, 7633], "temperature": 0.0, "avg_logprob": -0.21814694247402988, "compression_ratio": 1.6040609137055837, "no_speech_prob": 5.475669604493305e-05}, {"id": 46, "seek": 27184, "start": 280.32, "end": 288.67999999999995, "text": " or size of your hidden, and h is sorry, dimension of the the hidden state. We", "tokens": [420, 2744, 295, 428, 7633, 11, 293, 276, 307, 2597, 11, 10139, 295, 264, 264, 7633, 1785, 13, 492], "temperature": 0.0, "avg_logprob": -0.21814694247402988, "compression_ratio": 1.6040609137055837, "no_speech_prob": 5.475669604493305e-05}, {"id": 47, "seek": 27184, "start": 288.67999999999995, "end": 295.35999999999996, "text": " were using PyTorch's GRU, and we'll tell kind of how many, what we want our", "tokens": [645, 1228, 9953, 51, 284, 339, 311, 10903, 52, 11, 293, 321, 603, 980, 733, 295, 577, 867, 11, 437, 321, 528, 527], "temperature": 0.0, "avg_logprob": -0.21814694247402988, "compression_ratio": 1.6040609137055837, "no_speech_prob": 5.475669604493305e-05}, {"id": 48, "seek": 27184, "start": 295.35999999999996, "end": 300.67999999999995, "text": " dimensions for the the hidden state to be. In the last lesson I had a two here,", "tokens": [12819, 337, 264, 264, 7633, 1785, 281, 312, 13, 682, 264, 1036, 6898, 286, 632, 257, 732, 510, 11], "temperature": 0.0, "avg_logprob": -0.21814694247402988, "compression_ratio": 1.6040609137055837, "no_speech_prob": 5.475669604493305e-05}, {"id": 49, "seek": 30068, "start": 300.68, "end": 304.76, "text": " we were stacking them. I decided to switch that to one just to kind of have", "tokens": [321, 645, 41376, 552, 13, 286, 3047, 281, 3679, 300, 281, 472, 445, 281, 733, 295, 362], "temperature": 0.0, "avg_logprob": -0.177847057940012, "compression_ratio": 1.5602094240837696, "no_speech_prob": 3.426701368880458e-05}, {"id": 50, "seek": 30068, "start": 304.76, "end": 308.24, "text": " the simplest example that we're going to we're going to replace this", "tokens": [264, 22811, 1365, 300, 321, 434, 516, 281, 321, 434, 516, 281, 7406, 341], "temperature": 0.0, "avg_logprob": -0.177847057940012, "compression_ratio": 1.5602094240837696, "no_speech_prob": 3.426701368880458e-05}, {"id": 51, "seek": 30068, "start": 308.24, "end": 312.72, "text": " implementation in a moment. And so stop using PyTorch's and put our own in its", "tokens": [11420, 294, 257, 1623, 13, 400, 370, 1590, 1228, 9953, 51, 284, 339, 311, 293, 829, 527, 1065, 294, 1080], "temperature": 0.0, "avg_logprob": -0.177847057940012, "compression_ratio": 1.5602094240837696, "no_speech_prob": 3.426701368880458e-05}, {"id": 52, "seek": 30068, "start": 312.72, "end": 323.8, "text": " place. Then a linear layer, then batch norm, and then self.h. So i to h is", "tokens": [1081, 13, 1396, 257, 8213, 4583, 11, 550, 15245, 2026, 11, 293, 550, 2698, 13, 71, 13, 407, 741, 281, 276, 307], "temperature": 0.0, "avg_logprob": -0.177847057940012, "compression_ratio": 1.5602094240837696, "no_speech_prob": 3.426701368880458e-05}, {"id": 53, "seek": 32380, "start": 323.8, "end": 331.2, "text": " hidden input to hidden, h to zero or h to o is hidden to output, and then this is", "tokens": [7633, 4846, 281, 7633, 11, 276, 281, 4018, 420, 276, 281, 277, 307, 7633, 281, 5598, 11, 293, 550, 341, 307], "temperature": 0.0, "avg_logprob": -0.17812097483667835, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.384559891448589e-05}, {"id": 54, "seek": 32380, "start": 331.2, "end": 342.2, "text": " hidden. And we had our forward step is this RNN, which in our case is the GRU.", "tokens": [7633, 13, 400, 321, 632, 527, 2128, 1823, 307, 341, 45702, 45, 11, 597, 294, 527, 1389, 307, 264, 10903, 52, 13], "temperature": 0.0, "avg_logprob": -0.17812097483667835, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.384559891448589e-05}, {"id": 55, "seek": 32380, "start": 342.2, "end": 347.24, "text": " Detach means that you're going to stop kind of cut off that piece of history,", "tokens": [4237, 608, 1355, 300, 291, 434, 516, 281, 1590, 733, 295, 1723, 766, 300, 2522, 295, 2503, 11], "temperature": 0.0, "avg_logprob": -0.17812097483667835, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.384559891448589e-05}, {"id": 56, "seek": 32380, "start": 347.24, "end": 350.84000000000003, "text": " that's something you're not going to keep recording for the next step. And", "tokens": [300, 311, 746, 291, 434, 406, 516, 281, 1066, 6613, 337, 264, 958, 1823, 13, 400], "temperature": 0.0, "avg_logprob": -0.17812097483667835, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.384559891448589e-05}, {"id": 57, "seek": 35084, "start": 350.84, "end": 357.71999999999997, "text": " then kind of what we do is take the the results of the RNN, apply batch", "tokens": [550, 733, 295, 437, 321, 360, 307, 747, 264, 264, 3542, 295, 264, 45702, 45, 11, 3079, 15245], "temperature": 0.0, "avg_logprob": -0.15573640853639634, "compression_ratio": 1.4591194968553458, "no_speech_prob": 1.4970351912779734e-05}, {"id": 58, "seek": 35084, "start": 357.71999999999997, "end": 363.0, "text": " norm, and then apply this linear layer of set of weights that we're learning. And", "tokens": [2026, 11, 293, 550, 3079, 341, 8213, 4583, 295, 992, 295, 17443, 300, 321, 434, 2539, 13, 400], "temperature": 0.0, "avg_logprob": -0.15573640853639634, "compression_ratio": 1.4591194968553458, "no_speech_prob": 1.4970351912779734e-05}, {"id": 59, "seek": 35084, "start": 363.0, "end": 370.0, "text": " let me add to review, let me ask who remembers what batch norm is? Do you want", "tokens": [718, 385, 909, 281, 3131, 11, 718, 385, 1029, 567, 26228, 437, 15245, 2026, 307, 30, 1144, 291, 528], "temperature": 0.0, "avg_logprob": -0.15573640853639634, "compression_ratio": 1.4591194968553458, "no_speech_prob": 1.4970351912779734e-05}, {"id": 60, "seek": 37000, "start": 370.0, "end": 384.6, "text": " the catch box? Anyone? Batch norm? Good team catch. Is this just the", "tokens": [264, 3745, 2424, 30, 14643, 30, 363, 852, 2026, 30, 2205, 1469, 3745, 13, 1119, 341, 445, 264], "temperature": 0.0, "avg_logprob": -0.20233368873596191, "compression_ratio": 1.5100671140939597, "no_speech_prob": 1.4970711163186934e-05}, {"id": 61, "seek": 37000, "start": 384.6, "end": 391.12, "text": " normalization of just within the batch you're processing at the time? So it's", "tokens": [2710, 2144, 295, 445, 1951, 264, 15245, 291, 434, 9007, 412, 264, 565, 30, 407, 309, 311], "temperature": 0.0, "avg_logprob": -0.20233368873596191, "compression_ratio": 1.5100671140939597, "no_speech_prob": 1.4970711163186934e-05}, {"id": 62, "seek": 37000, "start": 391.12, "end": 397.56, "text": " the normalization of the activations in your network. And so and that's useful", "tokens": [264, 2710, 2144, 295, 264, 2430, 763, 294, 428, 3209, 13, 400, 370, 293, 300, 311, 4420], "temperature": 0.0, "avg_logprob": -0.20233368873596191, "compression_ratio": 1.5100671140939597, "no_speech_prob": 1.4970711163186934e-05}, {"id": 63, "seek": 39756, "start": 397.56, "end": 401.92, "text": " because otherwise your activations could get very close to zero or they could", "tokens": [570, 5911, 428, 2430, 763, 727, 483, 588, 1998, 281, 4018, 420, 436, 727], "temperature": 0.0, "avg_logprob": -0.12725539487950943, "compression_ratio": 1.6444444444444444, "no_speech_prob": 2.796833541651722e-05}, {"id": 64, "seek": 39756, "start": 401.92, "end": 404.84, "text": " kind of explode, you know, in any situation where we're doing these", "tokens": [733, 295, 21411, 11, 291, 458, 11, 294, 604, 2590, 689, 321, 434, 884, 613], "temperature": 0.0, "avg_logprob": -0.12725539487950943, "compression_ratio": 1.6444444444444444, "no_speech_prob": 2.796833541651722e-05}, {"id": 65, "seek": 39756, "start": 404.84, "end": 411.48, "text": " repeated multiplications. And so batch norm is kind of learning kind of a", "tokens": [10477, 17596, 763, 13, 400, 370, 15245, 2026, 307, 733, 295, 2539, 733, 295, 257], "temperature": 0.0, "avg_logprob": -0.12725539487950943, "compression_ratio": 1.6444444444444444, "no_speech_prob": 2.796833541651722e-05}, {"id": 66, "seek": 39756, "start": 411.48, "end": 415.92, "text": " basically a mean and standard deviation for your activations to keep them", "tokens": [1936, 257, 914, 293, 3832, 25163, 337, 428, 2430, 763, 281, 1066, 552], "temperature": 0.0, "avg_logprob": -0.12725539487950943, "compression_ratio": 1.6444444444444444, "no_speech_prob": 2.796833541651722e-05}, {"id": 67, "seek": 39756, "start": 415.92, "end": 424.88, "text": " normalized. Yeah, and so again kind of to know like the components of neural", "tokens": [48704, 13, 865, 11, 293, 370, 797, 733, 295, 281, 458, 411, 264, 6677, 295, 18161], "temperature": 0.0, "avg_logprob": -0.12725539487950943, "compression_ratio": 1.6444444444444444, "no_speech_prob": 2.796833541651722e-05}, {"id": 68, "seek": 42488, "start": 424.88, "end": 430.56, "text": " networks really are these relatively simple building blocks of linear layers,", "tokens": [9590, 534, 366, 613, 7226, 2199, 2390, 8474, 295, 8213, 7914, 11], "temperature": 0.0, "avg_logprob": -0.12331003024254317, "compression_ratio": 1.4694835680751173, "no_speech_prob": 1.7502421542303637e-05}, {"id": 69, "seek": 42488, "start": 430.56, "end": 437.68, "text": " batch norms, and nonlinearities. So I read this again with just using one GRU as", "tokens": [15245, 24357, 11, 293, 2107, 28263, 1088, 13, 407, 286, 1401, 341, 797, 365, 445, 1228, 472, 10903, 52, 382], "temperature": 0.0, "avg_logprob": -0.12331003024254317, "compression_ratio": 1.4694835680751173, "no_speech_prob": 1.7502421542303637e-05}, {"id": 70, "seek": 42488, "start": 437.68, "end": 443.12, "text": " opposed to two, and it actually still did pretty great for I mean for such a", "tokens": [8851, 281, 732, 11, 293, 309, 767, 920, 630, 1238, 869, 337, 286, 914, 337, 1270, 257], "temperature": 0.0, "avg_logprob": -0.12331003024254317, "compression_ratio": 1.4694835680751173, "no_speech_prob": 1.7502421542303637e-05}, {"id": 71, "seek": 42488, "start": 443.12, "end": 451.82, "text": " simple model of 74% accuracy. And I said let's let's make our own GRU. And so", "tokens": [2199, 2316, 295, 28868, 4, 14170, 13, 400, 286, 848, 718, 311, 718, 311, 652, 527, 1065, 10903, 52, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.12331003024254317, "compression_ratio": 1.4694835680751173, "no_speech_prob": 1.7502421542303637e-05}, {"id": 72, "seek": 45182, "start": 451.82, "end": 456.28, "text": " first we're going to do just a little bit of refactoring to make this clearer.", "tokens": [700, 321, 434, 516, 281, 360, 445, 257, 707, 857, 295, 1895, 578, 3662, 281, 652, 341, 26131, 13], "temperature": 0.0, "avg_logprob": -0.11468436099864819, "compression_ratio": 1.6899563318777293, "no_speech_prob": 1.3709449149246211e-06}, {"id": 73, "seek": 45182, "start": 456.28, "end": 462.98, "text": " And so we created an RNN loop. So remember kind of a key part of the", "tokens": [400, 370, 321, 2942, 364, 45702, 45, 6367, 13, 407, 1604, 733, 295, 257, 2141, 644, 295, 264], "temperature": 0.0, "avg_logprob": -0.11468436099864819, "compression_ratio": 1.6899563318777293, "no_speech_prob": 1.3709449149246211e-06}, {"id": 74, "seek": 45182, "start": 462.98, "end": 470.96, "text": " RNN. Actually let me bring that slide up again. It might be helpful to remember. We", "tokens": [45702, 45, 13, 5135, 718, 385, 1565, 300, 4137, 493, 797, 13, 467, 1062, 312, 4961, 281, 1604, 13, 492], "temperature": 0.0, "avg_logprob": -0.11468436099864819, "compression_ratio": 1.6899563318777293, "no_speech_prob": 1.3709449149246211e-06}, {"id": 75, "seek": 45182, "start": 470.96, "end": 476.68, "text": " kind of had done this whole refactoring of how do we even build an RNN. And a", "tokens": [733, 295, 632, 1096, 341, 1379, 1895, 578, 3662, 295, 577, 360, 321, 754, 1322, 364, 45702, 45, 13, 400, 257], "temperature": 0.0, "avg_logprob": -0.11468436099864819, "compression_ratio": 1.6899563318777293, "no_speech_prob": 1.3709449149246211e-06}, {"id": 76, "seek": 45182, "start": 476.68, "end": 480.44, "text": " key part of that is putting things into a for loop. So we can start with this", "tokens": [2141, 644, 295, 300, 307, 3372, 721, 666, 257, 337, 6367, 13, 407, 321, 393, 722, 365, 341], "temperature": 0.0, "avg_logprob": -0.11468436099864819, "compression_ratio": 1.6899563318777293, "no_speech_prob": 1.3709449149246211e-06}, {"id": 77, "seek": 48044, "start": 480.44, "end": 485.44, "text": " version where we're not using a for loop. Again remember here each color arrow", "tokens": [3037, 689, 321, 434, 406, 1228, 257, 337, 6367, 13, 3764, 1604, 510, 1184, 2017, 11610], "temperature": 0.0, "avg_logprob": -0.12170047176127531, "compression_ratio": 1.6378600823045268, "no_speech_prob": 8.93951528269099e-06}, {"id": 78, "seek": 48044, "start": 485.44, "end": 490.71999999999997, "text": " denotes kind of one set of weights that you're learning. So like one matrix. We", "tokens": [1441, 17251, 733, 295, 472, 992, 295, 17443, 300, 291, 434, 2539, 13, 407, 411, 472, 8141, 13, 492], "temperature": 0.0, "avg_logprob": -0.12170047176127531, "compression_ratio": 1.6378600823045268, "no_speech_prob": 8.93951528269099e-06}, {"id": 79, "seek": 48044, "start": 490.71999999999997, "end": 494.92, "text": " don't really want to have you know this complicated architecture of a bunch of", "tokens": [500, 380, 534, 528, 281, 362, 291, 458, 341, 6179, 9482, 295, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.12170047176127531, "compression_ratio": 1.6378600823045268, "no_speech_prob": 8.93951528269099e-06}, {"id": 80, "seek": 48044, "start": 494.92, "end": 499.36, "text": " if statements where we add each word sequentially. So we refactor that into a", "tokens": [498, 12363, 689, 321, 909, 1184, 1349, 5123, 3137, 13, 407, 321, 1895, 15104, 300, 666, 257], "temperature": 0.0, "avg_logprob": -0.12170047176127531, "compression_ratio": 1.6378600823045268, "no_speech_prob": 8.93951528269099e-06}, {"id": 81, "seek": 48044, "start": 499.36, "end": 507.28, "text": " for loop. And then we also choose to kind of keep the output within that for loop.", "tokens": [337, 6367, 13, 400, 550, 321, 611, 2826, 281, 733, 295, 1066, 264, 5598, 1951, 300, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.12170047176127531, "compression_ratio": 1.6378600823045268, "no_speech_prob": 8.93951528269099e-06}, {"id": 82, "seek": 50728, "start": 507.28, "end": 517.92, "text": " So here our call it our RNN loop is it's got a result. And then it's going through", "tokens": [407, 510, 527, 818, 309, 527, 45702, 45, 6367, 307, 309, 311, 658, 257, 1874, 13, 400, 550, 309, 311, 516, 807], "temperature": 0.0, "avg_logprob": -0.1312161297865317, "compression_ratio": 1.5894039735099337, "no_speech_prob": 7.889095286373049e-06}, {"id": 83, "seek": 50728, "start": 517.92, "end": 527.68, "text": " and calling some sort of cell which is set oh which is passed in when we call", "tokens": [293, 5141, 512, 1333, 295, 2815, 597, 307, 992, 1954, 597, 307, 4678, 294, 562, 321, 818], "temperature": 0.0, "avg_logprob": -0.1312161297865317, "compression_ratio": 1.5894039735099337, "no_speech_prob": 7.889095286373049e-06}, {"id": 84, "seek": 50728, "start": 527.68, "end": 532.64, "text": " RNN loop. So that's going to be a GRU cell which we'll see in a moment. Then we", "tokens": [45702, 45, 6367, 13, 407, 300, 311, 516, 281, 312, 257, 10903, 52, 2815, 597, 321, 603, 536, 294, 257, 1623, 13, 1396, 321], "temperature": 0.0, "avg_logprob": -0.1312161297865317, "compression_ratio": 1.5894039735099337, "no_speech_prob": 7.889095286373049e-06}, {"id": 85, "seek": 53264, "start": 532.64, "end": 539.84, "text": " want to append our hidden state onto the result to keep track of that and keep", "tokens": [528, 281, 34116, 527, 7633, 1785, 3911, 264, 1874, 281, 1066, 2837, 295, 300, 293, 1066], "temperature": 0.0, "avg_logprob": -0.1113185989722777, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.3419605238595977e-05}, {"id": 86, "seek": 53264, "start": 539.84, "end": 545.12, "text": " going. And so yeah this is actually a kind of you know using an RNN can be", "tokens": [516, 13, 400, 370, 1338, 341, 307, 767, 257, 733, 295, 291, 458, 1228, 364, 45702, 45, 393, 312], "temperature": 0.0, "avg_logprob": -0.1113185989722777, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.3419605238595977e-05}, {"id": 87, "seek": 53264, "start": 545.12, "end": 552.68, "text": " very very few lines of code as what is what's going on here. And so this is", "tokens": [588, 588, 1326, 3876, 295, 3089, 382, 437, 307, 437, 311, 516, 322, 510, 13, 400, 370, 341, 307], "temperature": 0.0, "avg_logprob": -0.1113185989722777, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.3419605238595977e-05}, {"id": 88, "seek": 53264, "start": 552.68, "end": 559.76, "text": " using PyTorch's implementation of a GRU cell. And so when you use a PyTorch GRU", "tokens": [1228, 9953, 51, 284, 339, 311, 11420, 295, 257, 10903, 52, 2815, 13, 400, 370, 562, 291, 764, 257, 9953, 51, 284, 339, 10903, 52], "temperature": 0.0, "avg_logprob": -0.1113185989722777, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.3419605238595977e-05}, {"id": 89, "seek": 55976, "start": 559.76, "end": 563.88, "text": " the kind of underlying building block to that is the PyTorch GRU cell. We're", "tokens": [264, 733, 295, 14217, 2390, 3461, 281, 300, 307, 264, 9953, 51, 284, 339, 10903, 52, 2815, 13, 492, 434], "temperature": 0.0, "avg_logprob": -0.1359714911534236, "compression_ratio": 1.5743801652892562, "no_speech_prob": 1.7231013771379367e-05}, {"id": 90, "seek": 55976, "start": 563.88, "end": 567.4399999999999, "text": " going to replace that with our own implementation in a moment. We're just", "tokens": [516, 281, 7406, 300, 365, 527, 1065, 11420, 294, 257, 1623, 13, 492, 434, 445], "temperature": 0.0, "avg_logprob": -0.1359714911534236, "compression_ratio": 1.5743801652892562, "no_speech_prob": 1.7231013771379367e-05}, {"id": 91, "seek": 55976, "start": 567.4399999999999, "end": 573.84, "text": " refactoring to make it clearer kind of how we do that. Yeah so calling this", "tokens": [1895, 578, 3662, 281, 652, 309, 26131, 733, 295, 577, 321, 360, 300, 13, 865, 370, 5141, 341], "temperature": 0.0, "avg_logprob": -0.1359714911534236, "compression_ratio": 1.5743801652892562, "no_speech_prob": 1.7231013771379367e-05}, {"id": 92, "seek": 55976, "start": 573.84, "end": 583.4, "text": " model 6. Subclasses model 5. And it has an RNN cell. It's got its hidden", "tokens": [2316, 1386, 13, 8511, 11665, 279, 2316, 1025, 13, 400, 309, 575, 364, 45702, 45, 2815, 13, 467, 311, 658, 1080, 7633], "temperature": 0.0, "avg_logprob": -0.1359714911534236, "compression_ratio": 1.5743801652892562, "no_speech_prob": 1.7231013771379367e-05}, {"id": 93, "seek": 55976, "start": 583.4, "end": 588.96, "text": " state. Remember again the hidden state is just a set of activations which kind of", "tokens": [1785, 13, 5459, 797, 264, 7633, 1785, 307, 445, 257, 992, 295, 2430, 763, 597, 733, 295], "temperature": 0.0, "avg_logprob": -0.1359714911534236, "compression_ratio": 1.5743801652892562, "no_speech_prob": 1.7231013771379367e-05}, {"id": 94, "seek": 58896, "start": 588.96, "end": 592.96, "text": " contains your information so far of how you're kind of representing what you've", "tokens": [8306, 428, 1589, 370, 1400, 295, 577, 291, 434, 733, 295, 13460, 437, 291, 600], "temperature": 0.0, "avg_logprob": -0.12938489352955537, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.321297845104709e-05}, {"id": 95, "seek": 58896, "start": 592.96, "end": 599.84, "text": " what you've seen. We run this it's 80% and this was there's a fair amount of", "tokens": [437, 291, 600, 1612, 13, 492, 1190, 341, 309, 311, 4688, 4, 293, 341, 390, 456, 311, 257, 3143, 2372, 295], "temperature": 0.0, "avg_logprob": -0.12938489352955537, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.321297845104709e-05}, {"id": 96, "seek": 58896, "start": 599.84, "end": 604.84, "text": " variation just in kind of from time to time. So that was 76 before. What are we", "tokens": [12990, 445, 294, 733, 295, 490, 565, 281, 565, 13, 407, 300, 390, 24733, 949, 13, 708, 366, 321], "temperature": 0.0, "avg_logprob": -0.12938489352955537, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.321297845104709e-05}, {"id": 97, "seek": 58896, "start": 604.84, "end": 617.76, "text": " going to get now? And this time it's 75%. So this is just kind of replicated the", "tokens": [516, 281, 483, 586, 30, 400, 341, 565, 309, 311, 9562, 6856, 407, 341, 307, 445, 733, 295, 46365, 264], "temperature": 0.0, "avg_logprob": -0.12938489352955537, "compression_ratio": 1.6173469387755102, "no_speech_prob": 3.321297845104709e-05}, {"id": 98, "seek": 61776, "start": 617.76, "end": 626.96, "text": " the same type of network. Yes Jeremy? Oh yeah so the that's a great point. The", "tokens": [264, 912, 2010, 295, 3209, 13, 1079, 17809, 30, 876, 1338, 370, 264, 300, 311, 257, 869, 935, 13, 440], "temperature": 0.0, "avg_logprob": -0.16716291109720866, "compression_ratio": 1.378698224852071, "no_speech_prob": 2.2124420866020955e-05}, {"id": 99, "seek": 61776, "start": 626.96, "end": 634.64, "text": " res colon minus 1 is picking out the most recent hidden state which is what", "tokens": [725, 8255, 3175, 502, 307, 8867, 484, 264, 881, 5162, 7633, 1785, 597, 307, 437], "temperature": 0.0, "avg_logprob": -0.16716291109720866, "compression_ratio": 1.378698224852071, "no_speech_prob": 2.2124420866020955e-05}, {"id": 100, "seek": 61776, "start": 634.64, "end": 642.96, "text": " you want. And then again detaching the the previous history. Is there more you", "tokens": [291, 528, 13, 400, 550, 797, 1141, 2834, 264, 264, 3894, 2503, 13, 1119, 456, 544, 291], "temperature": 0.0, "avg_logprob": -0.16716291109720866, "compression_ratio": 1.378698224852071, "no_speech_prob": 2.2124420866020955e-05}, {"id": 101, "seek": 64296, "start": 642.96, "end": 650.84, "text": " wanted to say about that Jeremy? Oh the model 5 version.", "tokens": [1415, 281, 584, 466, 300, 17809, 30, 876, 264, 2316, 1025, 3037, 13], "temperature": 0.0, "avg_logprob": -0.3476513785284919, "compression_ratio": 1.1785714285714286, "no_speech_prob": 1.3419040442386176e-05}, {"id": 102, "seek": 64296, "start": 658.72, "end": 668.36, "text": " So I see that lot of res comma H is doing this internally. Yes okay so what", "tokens": [407, 286, 536, 300, 688, 295, 725, 22117, 389, 307, 884, 341, 19501, 13, 1079, 1392, 370, 437], "temperature": 0.0, "avg_logprob": -0.3476513785284919, "compression_ratio": 1.1785714285714286, "no_speech_prob": 1.3419040442386176e-05}, {"id": 103, "seek": 66836, "start": 668.36, "end": 674.5600000000001, "text": " Jeremy's highlighting this is a great point is that basically the result is", "tokens": [17809, 311, 26551, 341, 307, 257, 869, 935, 307, 300, 1936, 264, 1874, 307], "temperature": 0.0, "avg_logprob": -0.11713154782953951, "compression_ratio": 1.6695278969957081, "no_speech_prob": 4.936927780363476e-06}, {"id": 104, "seek": 66836, "start": 674.5600000000001, "end": 678.12, "text": " kind of like the whole list of everything you've done so far and H is", "tokens": [733, 295, 411, 264, 1379, 1329, 295, 1203, 291, 600, 1096, 370, 1400, 293, 389, 307], "temperature": 0.0, "avg_logprob": -0.11713154782953951, "compression_ratio": 1.6695278969957081, "no_speech_prob": 4.936927780363476e-06}, {"id": 105, "seek": 66836, "start": 678.12, "end": 684.4, "text": " the most recent one. And so this method could have instead just returned res and", "tokens": [264, 881, 5162, 472, 13, 400, 370, 341, 3170, 727, 362, 2602, 445, 8752, 725, 293], "temperature": 0.0, "avg_logprob": -0.11713154782953951, "compression_ratio": 1.6695278969957081, "no_speech_prob": 4.936927780363476e-06}, {"id": 106, "seek": 66836, "start": 684.4, "end": 690.5600000000001, "text": " you could pick off res minus 1 and that gives you H. And so this is kind of just", "tokens": [291, 727, 1888, 766, 725, 3175, 502, 293, 300, 2709, 291, 389, 13, 400, 370, 341, 307, 733, 295, 445], "temperature": 0.0, "avg_logprob": -0.11713154782953951, "compression_ratio": 1.6695278969957081, "no_speech_prob": 4.936927780363476e-06}, {"id": 107, "seek": 66836, "start": 690.5600000000001, "end": 694.48, "text": " how the PyTorch one was implemented that it returns both but H is really just the", "tokens": [577, 264, 9953, 51, 284, 339, 472, 390, 12270, 300, 309, 11247, 1293, 457, 389, 307, 534, 445, 264], "temperature": 0.0, "avg_logprob": -0.11713154782953951, "compression_ratio": 1.6695278969957081, "no_speech_prob": 4.936927780363476e-06}, {"id": 108, "seek": 69448, "start": 694.48, "end": 701.04, "text": " last entry in res because the hidden state we're interested in is the most", "tokens": [1036, 8729, 294, 725, 570, 264, 7633, 1785, 321, 434, 3102, 294, 307, 264, 881], "temperature": 0.0, "avg_logprob": -0.0807709888536103, "compression_ratio": 1.6170212765957446, "no_speech_prob": 6.540319191117305e-06}, {"id": 109, "seek": 69448, "start": 701.04, "end": 705.6, "text": " recent one but we're you know keep appending to this array to have our", "tokens": [5162, 472, 457, 321, 434, 291, 458, 1066, 724, 2029, 281, 341, 10225, 281, 362, 527], "temperature": 0.0, "avg_logprob": -0.0807709888536103, "compression_ratio": 1.6170212765957446, "no_speech_prob": 6.540319191117305e-06}, {"id": 110, "seek": 69448, "start": 705.6, "end": 714.64, "text": " whole history of the hidden states. And so in going from model 5 to model 6 all", "tokens": [1379, 2503, 295, 264, 7633, 4368, 13, 400, 370, 294, 516, 490, 2316, 1025, 281, 2316, 1386, 439], "temperature": 0.0, "avg_logprob": -0.0807709888536103, "compression_ratio": 1.6170212765957446, "no_speech_prob": 6.540319191117305e-06}, {"id": 111, "seek": 69448, "start": 714.64, "end": 719.9200000000001, "text": " we've done is refactored by writing our own RNN loop which previously PyTorch", "tokens": [321, 600, 1096, 307, 1895, 578, 2769, 538, 3579, 527, 1065, 45702, 45, 6367, 597, 8046, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.0807709888536103, "compression_ratio": 1.6170212765957446, "no_speech_prob": 6.540319191117305e-06}, {"id": 112, "seek": 69448, "start": 719.9200000000001, "end": 724.44, "text": " was handling but at this point we're still using PyTorch's implementation of", "tokens": [390, 13175, 457, 412, 341, 935, 321, 434, 920, 1228, 9953, 51, 284, 339, 311, 11420, 295], "temperature": 0.0, "avg_logprob": -0.0807709888536103, "compression_ratio": 1.6170212765957446, "no_speech_prob": 6.540319191117305e-06}, {"id": 113, "seek": 72444, "start": 724.44, "end": 732.0, "text": " GRU cell which is basically you put it inside a loop to get a GRU is yeah so a", "tokens": [10903, 52, 2815, 597, 307, 1936, 291, 829, 309, 1854, 257, 6367, 281, 483, 257, 10903, 52, 307, 1338, 370, 257], "temperature": 0.0, "avg_logprob": -0.12022230658732669, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.4738587196916342e-05}, {"id": 114, "seek": 72444, "start": 732.0, "end": 739.5200000000001, "text": " GRU is a loop with a GRU cell inside of it. And so next let's take a look at what", "tokens": [10903, 52, 307, 257, 6367, 365, 257, 10903, 52, 2815, 1854, 295, 309, 13, 400, 370, 958, 718, 311, 747, 257, 574, 412, 437], "temperature": 0.0, "avg_logprob": -0.12022230658732669, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.4738587196916342e-05}, {"id": 115, "seek": 72444, "start": 739.5200000000001, "end": 746.12, "text": " a GRU cell is and so here I used some code from github although heavily", "tokens": [257, 10903, 52, 2815, 307, 293, 370, 510, 286, 1143, 512, 3089, 490, 290, 355, 836, 4878, 10950], "temperature": 0.0, "avg_logprob": -0.12022230658732669, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.4738587196916342e-05}, {"id": 116, "seek": 74612, "start": 746.12, "end": 756.16, "text": " refactored on implementing LSTMs and well I just took the GRU part. So we're", "tokens": [1895, 578, 2769, 322, 18114, 441, 6840, 26386, 293, 731, 286, 445, 1890, 264, 10903, 52, 644, 13, 407, 321, 434], "temperature": 0.0, "avg_logprob": -0.13448553818922776, "compression_ratio": 1.484076433121019, "no_speech_prob": 4.495117991609732e-06}, {"id": 117, "seek": 74612, "start": 756.16, "end": 762.88, "text": " gonna need we'll have kind of gate X and well we'll have a reset gate an update", "tokens": [799, 643, 321, 603, 362, 733, 295, 8539, 1783, 293, 731, 321, 603, 362, 257, 14322, 8539, 364, 5623], "temperature": 0.0, "avg_logprob": -0.13448553818922776, "compression_ratio": 1.484076433121019, "no_speech_prob": 4.495117991609732e-06}, {"id": 118, "seek": 74612, "start": 762.88, "end": 771.6800000000001, "text": " gate and a new gate and each of those will take the input actually let me go", "tokens": [8539, 293, 257, 777, 8539, 293, 1184, 295, 729, 486, 747, 264, 4846, 767, 718, 385, 352], "temperature": 0.0, "avg_logprob": -0.13448553818922776, "compression_ratio": 1.484076433121019, "no_speech_prob": 4.495117991609732e-06}, {"id": 119, "seek": 77168, "start": 771.68, "end": 782.4, "text": " to the math equation I think is only the best way to start with this. Sorry. So a", "tokens": [281, 264, 5221, 5367, 286, 519, 307, 787, 264, 1151, 636, 281, 722, 365, 341, 13, 4919, 13, 407, 257], "temperature": 0.0, "avg_logprob": -0.18311282335701634, "compression_ratio": 1.5098039215686274, "no_speech_prob": 5.594253252638737e-06}, {"id": 120, "seek": 77168, "start": 782.4, "end": 788.76, "text": " GRU in equations so here H represents the hidden state X represents the input", "tokens": [10903, 52, 294, 11787, 370, 510, 389, 8855, 264, 7633, 1785, 1783, 8855, 264, 4846], "temperature": 0.0, "avg_logprob": -0.18311282335701634, "compression_ratio": 1.5098039215686274, "no_speech_prob": 5.594253252638737e-06}, {"id": 121, "seek": 77168, "start": 788.76, "end": 796.16, "text": " so the update gate is going to be sigmoid of some linear combination of", "tokens": [370, 264, 5623, 8539, 307, 516, 281, 312, 4556, 3280, 327, 295, 512, 8213, 6562, 295], "temperature": 0.0, "avg_logprob": -0.18311282335701634, "compression_ratio": 1.5098039215686274, "no_speech_prob": 5.594253252638737e-06}, {"id": 122, "seek": 79616, "start": 796.16, "end": 802.9599999999999, "text": " the input plus the hidden state and A and B are sets of weights that", "tokens": [264, 4846, 1804, 264, 7633, 1785, 293, 316, 293, 363, 366, 6352, 295, 17443, 300], "temperature": 0.0, "avg_logprob": -0.11365561289330052, "compression_ratio": 1.719298245614035, "no_speech_prob": 8.939487088355236e-06}, {"id": 123, "seek": 79616, "start": 802.9599999999999, "end": 809.8399999999999, "text": " we're going to learn. Then the reset gate is sigmoid of a different linear", "tokens": [321, 434, 516, 281, 1466, 13, 1396, 264, 14322, 8539, 307, 4556, 3280, 327, 295, 257, 819, 8213], "temperature": 0.0, "avg_logprob": -0.11365561289330052, "compression_ratio": 1.719298245614035, "no_speech_prob": 8.939487088355236e-06}, {"id": 124, "seek": 79616, "start": 809.8399999999999, "end": 814.8199999999999, "text": " combination of the input and the previous hidden state and again we're", "tokens": [6562, 295, 264, 4846, 293, 264, 3894, 7633, 1785, 293, 797, 321, 434], "temperature": 0.0, "avg_logprob": -0.11365561289330052, "compression_ratio": 1.719298245614035, "no_speech_prob": 8.939487088355236e-06}, {"id": 125, "seek": 79616, "start": 814.8199999999999, "end": 820.1999999999999, "text": " going to learn the weights for those for C and D that we're multiplying them by", "tokens": [516, 281, 1466, 264, 17443, 337, 729, 337, 383, 293, 413, 300, 321, 434, 30955, 552, 538], "temperature": 0.0, "avg_logprob": -0.11365561289330052, "compression_ratio": 1.719298245614035, "no_speech_prob": 8.939487088355236e-06}, {"id": 126, "seek": 82020, "start": 820.2, "end": 826.88, "text": " and again remember sigmoid is always something between 0 and 1 so basically", "tokens": [293, 797, 1604, 4556, 3280, 327, 307, 1009, 746, 1296, 1958, 293, 502, 370, 1936], "temperature": 0.0, "avg_logprob": -0.10789958997206255, "compression_ratio": 1.7149321266968325, "no_speech_prob": 2.2959000034461496e-06}, {"id": 127, "seek": 82020, "start": 826.88, "end": 830.6400000000001, "text": " this is kind of taking linear combinations of your input and your", "tokens": [341, 307, 733, 295, 1940, 8213, 21267, 295, 428, 4846, 293, 428], "temperature": 0.0, "avg_logprob": -0.10789958997206255, "compression_ratio": 1.7149321266968325, "no_speech_prob": 2.2959000034461496e-06}, {"id": 128, "seek": 82020, "start": 830.6400000000001, "end": 835.8000000000001, "text": " hidden state and then getting something between 0 and 1 with them. I also want to", "tokens": [7633, 1785, 293, 550, 1242, 746, 1296, 1958, 293, 502, 365, 552, 13, 286, 611, 528, 281], "temperature": 0.0, "avg_logprob": -0.10789958997206255, "compression_ratio": 1.7149321266968325, "no_speech_prob": 2.2959000034461496e-06}, {"id": 129, "seek": 82020, "start": 835.8000000000001, "end": 841.2800000000001, "text": " highlight that these equations can have a bias term as well I left it out just", "tokens": [5078, 300, 613, 11787, 393, 362, 257, 12577, 1433, 382, 731, 286, 1411, 309, 484, 445], "temperature": 0.0, "avg_logprob": -0.10789958997206255, "compression_ratio": 1.7149321266968325, "no_speech_prob": 2.2959000034461496e-06}, {"id": 130, "seek": 82020, "start": 841.2800000000001, "end": 844.76, "text": " kind of for simplicity but you could have a bias that you're you're learning", "tokens": [733, 295, 337, 25632, 457, 291, 727, 362, 257, 12577, 300, 291, 434, 291, 434, 2539], "temperature": 0.0, "avg_logprob": -0.10789958997206255, "compression_ratio": 1.7149321266968325, "no_speech_prob": 2.2959000034461496e-06}, {"id": 131, "seek": 84476, "start": 844.76, "end": 851.36, "text": " as well which is just you know constant at the end. Then there's a new gate and", "tokens": [382, 731, 597, 307, 445, 291, 458, 5754, 412, 264, 917, 13, 1396, 456, 311, 257, 777, 8539, 293], "temperature": 0.0, "avg_logprob": -0.1195937141043241, "compression_ratio": 1.5562913907284768, "no_speech_prob": 1.8162066908189445e-06}, {"id": 132, "seek": 84476, "start": 851.36, "end": 862.12, "text": " new gate is taking hyperbolic tan of a combination of the input and then here", "tokens": [777, 8539, 307, 1940, 9848, 65, 7940, 7603, 295, 257, 6562, 295, 264, 4846, 293, 550, 510], "temperature": 0.0, "avg_logprob": -0.1195937141043241, "compression_ratio": 1.5562913907284768, "no_speech_prob": 1.8162066908189445e-06}, {"id": 133, "seek": 84476, "start": 862.12, "end": 868.52, "text": " you're taking the Hadamard product of the reset gate and the hidden state and", "tokens": [291, 434, 1940, 264, 12298, 335, 515, 1674, 295, 264, 14322, 8539, 293, 264, 7633, 1785, 293], "temperature": 0.0, "avg_logprob": -0.1195937141043241, "compression_ratio": 1.5562913907284768, "no_speech_prob": 1.8162066908189445e-06}, {"id": 134, "seek": 86852, "start": 868.52, "end": 878.28, "text": " does anyone remember what a Hadamard product is? Okay, so is that a yes?", "tokens": [775, 2878, 1604, 437, 257, 12298, 335, 515, 1674, 307, 30, 1033, 11, 370, 307, 300, 257, 2086, 30], "temperature": 0.0, "avg_logprob": -0.22502313177269626, "compression_ratio": 1.6759259259259258, "no_speech_prob": 6.438916443585185e-06}, {"id": 135, "seek": 86852, "start": 878.28, "end": 883.68, "text": " Exactly, element-wise multiplication so instead of the row by column of", "tokens": [7587, 11, 4478, 12, 3711, 27290, 370, 2602, 295, 264, 5386, 538, 7738, 295], "temperature": 0.0, "avg_logprob": -0.22502313177269626, "compression_ratio": 1.6759259259259258, "no_speech_prob": 6.438916443585185e-06}, {"id": 136, "seek": 86852, "start": 883.68, "end": 887.28, "text": " traditional matrix multiplication this is multiplying you know the first", "tokens": [5164, 8141, 27290, 341, 307, 30955, 291, 458, 264, 700], "temperature": 0.0, "avg_logprob": -0.22502313177269626, "compression_ratio": 1.6759259259259258, "no_speech_prob": 6.438916443585185e-06}, {"id": 137, "seek": 86852, "start": 887.28, "end": 892.8, "text": " element by the first second by the second and so on and that's that", "tokens": [4478, 538, 264, 700, 1150, 538, 264, 1150, 293, 370, 322, 293, 300, 311, 300], "temperature": 0.0, "avg_logprob": -0.22502313177269626, "compression_ratio": 1.6759259259259258, "no_speech_prob": 6.438916443585185e-06}, {"id": 138, "seek": 86852, "start": 892.8, "end": 896.6, "text": " represented and I looked this up because actually I was not remembering what", "tokens": [10379, 293, 286, 2956, 341, 493, 570, 767, 286, 390, 406, 20719, 437], "temperature": 0.0, "avg_logprob": -0.22502313177269626, "compression_ratio": 1.6759259259259258, "no_speech_prob": 6.438916443585185e-06}, {"id": 139, "seek": 89660, "start": 896.6, "end": 901.28, "text": " symbol to use and people do use a variety of symbols to represent this and", "tokens": [5986, 281, 764, 293, 561, 360, 764, 257, 5673, 295, 16944, 281, 2906, 341, 293], "temperature": 0.0, "avg_logprob": -0.11301826923451525, "compression_ratio": 1.7008928571428572, "no_speech_prob": 8.139541932905558e-06}, {"id": 140, "seek": 89660, "start": 901.28, "end": 905.44, "text": " so I was looking at one resource that kind of had like the little circles", "tokens": [370, 286, 390, 1237, 412, 472, 7684, 300, 733, 295, 632, 411, 264, 707, 13040], "temperature": 0.0, "avg_logprob": -0.11301826923451525, "compression_ratio": 1.7008928571428572, "no_speech_prob": 8.139541932905558e-06}, {"id": 141, "seek": 89660, "start": 905.44, "end": 911.08, "text": " where it's white on the inside kind of like a plain circle so I went with this", "tokens": [689, 309, 311, 2418, 322, 264, 1854, 733, 295, 411, 257, 11121, 6329, 370, 286, 1437, 365, 341], "temperature": 0.0, "avg_logprob": -0.11301826923451525, "compression_ratio": 1.7008928571428572, "no_speech_prob": 8.139541932905558e-06}, {"id": 142, "seek": 89660, "start": 911.08, "end": 916.2, "text": " but it can be represented different ways. So this is your new gate hyperbolic tan", "tokens": [457, 309, 393, 312, 10379, 819, 2098, 13, 407, 341, 307, 428, 777, 8539, 9848, 65, 7940, 7603], "temperature": 0.0, "avg_logprob": -0.11301826923451525, "compression_ratio": 1.7008928571428572, "no_speech_prob": 8.139541932905558e-06}, {"id": 143, "seek": 89660, "start": 916.2, "end": 922.6800000000001, "text": " is between negative 1 and 1 and then to get the the new hidden state so", "tokens": [307, 1296, 3671, 502, 293, 502, 293, 550, 281, 483, 264, 264, 777, 7633, 1785, 370], "temperature": 0.0, "avg_logprob": -0.11301826923451525, "compression_ratio": 1.7008928571428572, "no_speech_prob": 8.139541932905558e-06}, {"id": 144, "seek": 92268, "start": 922.68, "end": 926.8, "text": " everything we've been doing so far has been combining the hidden state that was", "tokens": [1203, 321, 600, 668, 884, 370, 1400, 575, 668, 21928, 264, 7633, 1785, 300, 390], "temperature": 0.0, "avg_logprob": -0.10793269198873769, "compression_ratio": 1.7621145374449338, "no_speech_prob": 2.1444240701384842e-05}, {"id": 145, "seek": 92268, "start": 926.8, "end": 933.0, "text": " spit out at the previous step with your new input which is the the next word. Now", "tokens": [22127, 484, 412, 264, 3894, 1823, 365, 428, 777, 4846, 597, 307, 264, 264, 958, 1349, 13, 823], "temperature": 0.0, "avg_logprob": -0.10793269198873769, "compression_ratio": 1.7621145374449338, "no_speech_prob": 2.1444240701384842e-05}, {"id": 146, "seek": 92268, "start": 933.0, "end": 937.7199999999999, "text": " we're going to take basically a weighted, effectively this is a weighted average", "tokens": [321, 434, 516, 281, 747, 1936, 257, 32807, 11, 8659, 341, 307, 257, 32807, 4274], "temperature": 0.0, "avg_logprob": -0.10793269198873769, "compression_ratio": 1.7621145374449338, "no_speech_prob": 2.1444240701384842e-05}, {"id": 147, "seek": 92268, "start": 937.7199999999999, "end": 943.56, "text": " of the new gate and the previous hidden state so whenever you see something of", "tokens": [295, 264, 777, 8539, 293, 264, 3894, 7633, 1785, 370, 5699, 291, 536, 746, 295], "temperature": 0.0, "avg_logprob": -0.10793269198873769, "compression_ratio": 1.7621145374449338, "no_speech_prob": 2.1444240701384842e-05}, {"id": 148, "seek": 92268, "start": 943.56, "end": 949.8399999999999, "text": " the form you know z times something plus 1 minus z times something that's in a", "tokens": [264, 1254, 291, 458, 710, 1413, 746, 1804, 502, 3175, 710, 1413, 746, 300, 311, 294, 257], "temperature": 0.0, "avg_logprob": -0.10793269198873769, "compression_ratio": 1.7621145374449338, "no_speech_prob": 2.1444240701384842e-05}, {"id": 149, "seek": 94984, "start": 949.84, "end": 954.96, "text": " way a kind of weighted average where you know we're giving z weight to the first", "tokens": [636, 257, 733, 295, 32807, 4274, 689, 291, 458, 321, 434, 2902, 710, 3364, 281, 264, 700], "temperature": 0.0, "avg_logprob": -0.09391886989275615, "compression_ratio": 1.708695652173913, "no_speech_prob": 1.0289354577253107e-05}, {"id": 150, "seek": 94984, "start": 954.96, "end": 961.84, "text": " thing and 1 minus z weight to the the second item where here z was this update", "tokens": [551, 293, 502, 3175, 710, 3364, 281, 264, 264, 1150, 3174, 689, 510, 710, 390, 341, 5623], "temperature": 0.0, "avg_logprob": -0.09391886989275615, "compression_ratio": 1.708695652173913, "no_speech_prob": 1.0289354577253107e-05}, {"id": 151, "seek": 94984, "start": 961.84, "end": 969.44, "text": " gate that we learned. So this these equations the pieces are all simple it's", "tokens": [8539, 300, 321, 3264, 13, 407, 341, 613, 11787, 264, 3755, 366, 439, 2199, 309, 311], "temperature": 0.0, "avg_logprob": -0.09391886989275615, "compression_ratio": 1.708695652173913, "no_speech_prob": 1.0289354577253107e-05}, {"id": 152, "seek": 94984, "start": 969.44, "end": 972.08, "text": " kind of like a little bit hard to think about what they mean together but this", "tokens": [733, 295, 411, 257, 707, 857, 1152, 281, 519, 466, 437, 436, 914, 1214, 457, 341], "temperature": 0.0, "avg_logprob": -0.09391886989275615, "compression_ratio": 1.708695652173913, "no_speech_prob": 1.0289354577253107e-05}, {"id": 153, "seek": 94984, "start": 972.08, "end": 977.72, "text": " is really kind of just this combination of sigmoids and hyperbolic tan with a", "tokens": [307, 534, 733, 295, 445, 341, 6562, 295, 4556, 3280, 3742, 293, 9848, 65, 7940, 7603, 365, 257], "temperature": 0.0, "avg_logprob": -0.09391886989275615, "compression_ratio": 1.708695652173913, "no_speech_prob": 1.0289354577253107e-05}, {"id": 154, "seek": 97772, "start": 977.72, "end": 982.32, "text": " bunch of linear averages but it gives us kind of a number of things we get to", "tokens": [3840, 295, 8213, 42257, 457, 309, 2709, 505, 733, 295, 257, 1230, 295, 721, 321, 483, 281], "temperature": 0.0, "avg_logprob": -0.0811396649009303, "compression_ratio": 1.8364485981308412, "no_speech_prob": 4.425391125550959e-06}, {"id": 155, "seek": 97772, "start": 982.32, "end": 987.9200000000001, "text": " learn about kind of how much weight to give the in you know the input of the", "tokens": [1466, 466, 733, 295, 577, 709, 3364, 281, 976, 264, 294, 291, 458, 264, 4846, 295, 264], "temperature": 0.0, "avg_logprob": -0.0811396649009303, "compression_ratio": 1.8364485981308412, "no_speech_prob": 4.425391125550959e-06}, {"id": 156, "seek": 97772, "start": 987.9200000000001, "end": 992.6, "text": " next word versus your previous hidden state at each step and so this will make", "tokens": [958, 1349, 5717, 428, 3894, 7633, 1785, 412, 1184, 1823, 293, 370, 341, 486, 652], "temperature": 0.0, "avg_logprob": -0.0811396649009303, "compression_ratio": 1.8364485981308412, "no_speech_prob": 4.425391125550959e-06}, {"id": 157, "seek": 97772, "start": 992.6, "end": 996.9200000000001, "text": " it easier that if you do kind of want to give less weight to something new and", "tokens": [309, 3571, 300, 498, 291, 360, 733, 295, 528, 281, 976, 1570, 3364, 281, 746, 777, 293], "temperature": 0.0, "avg_logprob": -0.0811396649009303, "compression_ratio": 1.8364485981308412, "no_speech_prob": 4.425391125550959e-06}, {"id": 158, "seek": 97772, "start": 996.9200000000001, "end": 1002.1600000000001, "text": " put more more weight on the hidden state from the past that you now kind of have", "tokens": [829, 544, 544, 3364, 322, 264, 7633, 1785, 490, 264, 1791, 300, 291, 586, 733, 295, 362], "temperature": 0.0, "avg_logprob": -0.0811396649009303, "compression_ratio": 1.8364485981308412, "no_speech_prob": 4.425391125550959e-06}, {"id": 159, "seek": 100216, "start": 1002.16, "end": 1008.24, "text": " a way to give more weight to the past I guess that's how you could describe it", "tokens": [257, 636, 281, 976, 544, 3364, 281, 264, 1791, 286, 2041, 300, 311, 577, 291, 727, 6786, 309], "temperature": 0.0, "avg_logprob": -0.11334649883970922, "compression_ratio": 1.6340425531914893, "no_speech_prob": 6.8542071858246345e-06}, {"id": 160, "seek": 100216, "start": 1008.24, "end": 1012.0, "text": " there are questions about these equations I think this will help it's", "tokens": [456, 366, 1651, 466, 613, 11787, 286, 519, 341, 486, 854, 309, 311], "temperature": 0.0, "avg_logprob": -0.11334649883970922, "compression_ratio": 1.6340425531914893, "no_speech_prob": 6.8542071858246345e-06}, {"id": 161, "seek": 100216, "start": 1012.0, "end": 1016.28, "text": " nice to kind of go back and forth between these and the code the code for", "tokens": [1481, 281, 733, 295, 352, 646, 293, 5220, 1296, 613, 293, 264, 3089, 264, 3089, 337], "temperature": 0.0, "avg_logprob": -0.11334649883970922, "compression_ratio": 1.6340425531914893, "no_speech_prob": 6.8542071858246345e-06}, {"id": 162, "seek": 100216, "start": 1016.28, "end": 1024.96, "text": " these. Something else to note is that so we're multiplying xt by a few different", "tokens": [613, 13, 6595, 1646, 281, 3637, 307, 300, 370, 321, 434, 30955, 220, 734, 538, 257, 1326, 819], "temperature": 0.0, "avg_logprob": -0.11334649883970922, "compression_ratio": 1.6340425531914893, "no_speech_prob": 6.8542071858246345e-06}, {"id": 163, "seek": 100216, "start": 1024.96, "end": 1030.68, "text": " things here I've called them a C and G in the next when we do this in code we're", "tokens": [721, 510, 286, 600, 1219, 552, 257, 383, 293, 460, 294, 264, 958, 562, 321, 360, 341, 294, 3089, 321, 434], "temperature": 0.0, "avg_logprob": -0.11334649883970922, "compression_ratio": 1.6340425531914893, "no_speech_prob": 6.8542071858246345e-06}, {"id": 164, "seek": 103068, "start": 1030.68, "end": 1034.92, "text": " basically going to combine that and learn all those variables together but", "tokens": [1936, 516, 281, 10432, 300, 293, 1466, 439, 729, 9102, 1214, 457], "temperature": 0.0, "avg_logprob": -0.09036515553792318, "compression_ratio": 1.7035398230088497, "no_speech_prob": 5.771789346908918e-06}, {"id": 165, "seek": 103068, "start": 1034.92, "end": 1039.64, "text": " you could then you know split it out into two three separate variables after", "tokens": [291, 727, 550, 291, 458, 7472, 309, 484, 666, 732, 1045, 4994, 9102, 934], "temperature": 0.0, "avg_logprob": -0.09036515553792318, "compression_ratio": 1.7035398230088497, "no_speech_prob": 5.771789346908918e-06}, {"id": 166, "seek": 103068, "start": 1039.64, "end": 1044.92, "text": " you learn them which is what we'll do in the code and then H the previous hidden", "tokens": [291, 1466, 552, 597, 307, 437, 321, 603, 360, 294, 264, 3089, 293, 550, 389, 264, 3894, 7633], "temperature": 0.0, "avg_logprob": -0.09036515553792318, "compression_ratio": 1.7035398230088497, "no_speech_prob": 5.771789346908918e-06}, {"id": 167, "seek": 103068, "start": 1044.92, "end": 1050.2, "text": " state is being multiplied by BD and H here and so again we can kind of learn", "tokens": [1785, 307, 885, 17207, 538, 363, 35, 293, 389, 510, 293, 370, 797, 321, 393, 733, 295, 1466], "temperature": 0.0, "avg_logprob": -0.09036515553792318, "compression_ratio": 1.7035398230088497, "no_speech_prob": 5.771789346908918e-06}, {"id": 168, "seek": 103068, "start": 1050.2, "end": 1057.1200000000001, "text": " those all at once in this matrix and then split it into three pieces and so", "tokens": [729, 439, 412, 1564, 294, 341, 8141, 293, 550, 7472, 309, 666, 1045, 3755, 293, 370], "temperature": 0.0, "avg_logprob": -0.09036515553792318, "compression_ratio": 1.7035398230088497, "no_speech_prob": 5.771789346908918e-06}, {"id": 169, "seek": 105712, "start": 1057.12, "end": 1062.84, "text": " that's that's what's going on in the code here so here the language this is", "tokens": [300, 311, 300, 311, 437, 311, 516, 322, 294, 264, 3089, 510, 370, 510, 264, 2856, 341, 307], "temperature": 0.0, "avg_logprob": -0.14724965955390304, "compression_ratio": 1.598639455782313, "no_speech_prob": 4.565896233543754e-06}, {"id": 170, "seek": 105712, "start": 1062.84, "end": 1068.2399999999998, "text": " why we have three times the dimension of the hidden state and we're calling that", "tokens": [983, 321, 362, 1045, 1413, 264, 10139, 295, 264, 7633, 1785, 293, 321, 434, 5141, 300], "temperature": 0.0, "avg_logprob": -0.14724965955390304, "compression_ratio": 1.598639455782313, "no_speech_prob": 4.565896233543754e-06}, {"id": 171, "seek": 105712, "start": 1068.2399999999998, "end": 1082.1999999999998, "text": " I to H and H to H and so basically each component down here is so I stands for", "tokens": [286, 281, 389, 293, 389, 281, 389, 293, 370, 1936, 1184, 6542, 760, 510, 307, 370, 286, 7382, 337], "temperature": 0.0, "avg_logprob": -0.14724965955390304, "compression_ratio": 1.598639455782313, "no_speech_prob": 4.565896233543754e-06}, {"id": 172, "seek": 108220, "start": 1082.2, "end": 1087.16, "text": " input H stands for hidden you see the resource reset gate is this combination", "tokens": [4846, 389, 7382, 337, 7633, 291, 536, 264, 7684, 14322, 8539, 307, 341, 6562], "temperature": 0.0, "avg_logprob": -0.1159067153930664, "compression_ratio": 1.6994535519125684, "no_speech_prob": 3.3930987228814047e-06}, {"id": 173, "seek": 108220, "start": 1087.16, "end": 1092.64, "text": " of input and hidden and basically R is just the first of our our three and", "tokens": [295, 4846, 293, 7633, 293, 1936, 497, 307, 445, 264, 700, 295, 527, 527, 1045, 293], "temperature": 0.0, "avg_logprob": -0.1159067153930664, "compression_ratio": 1.6994535519125684, "no_speech_prob": 3.3930987228814047e-06}, {"id": 174, "seek": 108220, "start": 1092.64, "end": 1096.8400000000001, "text": " that's the one that corresponds to the reset gate so here the kind of naming", "tokens": [300, 311, 264, 472, 300, 23249, 281, 264, 14322, 8539, 370, 510, 264, 733, 295, 25290], "temperature": 0.0, "avg_logprob": -0.1159067153930664, "compression_ratio": 1.6994535519125684, "no_speech_prob": 3.3930987228814047e-06}, {"id": 175, "seek": 108220, "start": 1096.8400000000001, "end": 1107.3600000000001, "text": " convention is what are the linear factors you want to multiply the new input that", "tokens": [10286, 307, 437, 366, 264, 8213, 6771, 291, 528, 281, 12972, 264, 777, 4846, 300], "temperature": 0.0, "avg_logprob": -0.1159067153930664, "compression_ratio": 1.6994535519125684, "no_speech_prob": 3.3930987228814047e-06}, {"id": 176, "seek": 110736, "start": 1107.36, "end": 1114.08, "text": " will be going through the reset gate by the input for the update gate and then", "tokens": [486, 312, 516, 807, 264, 14322, 8539, 538, 264, 4846, 337, 264, 5623, 8539, 293, 550], "temperature": 0.0, "avg_logprob": -0.07682922093764595, "compression_ratio": 2.0638297872340425, "no_speech_prob": 9.97273855318781e-06}, {"id": 177, "seek": 110736, "start": 1114.08, "end": 1120.1999999999998, "text": " the input for the new gate so this is kind of the process of you learn these", "tokens": [264, 4846, 337, 264, 777, 8539, 370, 341, 307, 733, 295, 264, 1399, 295, 291, 1466, 613], "temperature": 0.0, "avg_logprob": -0.07682922093764595, "compression_ratio": 2.0638297872340425, "no_speech_prob": 9.97273855318781e-06}, {"id": 178, "seek": 110736, "start": 1120.1999999999998, "end": 1125.08, "text": " as you know these three kind of bigger or not three as these linear matrices", "tokens": [382, 291, 458, 613, 1045, 733, 295, 3801, 420, 406, 1045, 382, 613, 8213, 32284], "temperature": 0.0, "avg_logprob": -0.07682922093764595, "compression_ratio": 2.0638297872340425, "no_speech_prob": 9.97273855318781e-06}, {"id": 179, "seek": 110736, "start": 1125.08, "end": 1129.9599999999998, "text": " and then break them into three pieces is what's going on here and put them into", "tokens": [293, 550, 1821, 552, 666, 1045, 3755, 307, 437, 311, 516, 322, 510, 293, 829, 552, 666], "temperature": 0.0, "avg_logprob": -0.07682922093764595, "compression_ratio": 2.0638297872340425, "no_speech_prob": 9.97273855318781e-06}, {"id": 180, "seek": 110736, "start": 1129.9599999999998, "end": 1135.76, "text": " your your reset gate your update gate and your new gate and then return the", "tokens": [428, 428, 14322, 8539, 428, 5623, 8539, 293, 428, 777, 8539, 293, 550, 2736, 264], "temperature": 0.0, "avg_logprob": -0.07682922093764595, "compression_ratio": 2.0638297872340425, "no_speech_prob": 9.97273855318781e-06}, {"id": 181, "seek": 113576, "start": 1135.76, "end": 1140.64, "text": " weighted average of update gate times hidden state and one minus update gate", "tokens": [32807, 4274, 295, 5623, 8539, 1413, 7633, 1785, 293, 472, 3175, 5623, 8539], "temperature": 0.0, "avg_logprob": -0.17296232715729745, "compression_ratio": 1.6196319018404908, "no_speech_prob": 4.222772531647934e-06}, {"id": 182, "seek": 113576, "start": 1140.64, "end": 1147.6, "text": " times new gate there questions about this", "tokens": [1413, 777, 8539, 456, 1651, 466, 341], "temperature": 0.0, "avg_logprob": -0.17296232715729745, "compression_ratio": 1.6196319018404908, "no_speech_prob": 4.222772531647934e-06}, {"id": 183, "seek": 113576, "start": 1149.6, "end": 1153.92, "text": " I can even let me go back to the equations one more time just to to", "tokens": [286, 393, 754, 718, 385, 352, 646, 281, 264, 11787, 472, 544, 565, 445, 281, 281], "temperature": 0.0, "avg_logprob": -0.17296232715729745, "compression_ratio": 1.6196319018404908, "no_speech_prob": 4.222772531647934e-06}, {"id": 184, "seek": 113576, "start": 1153.92, "end": 1160.4, "text": " compare so yeah we've kind of combined the three things but we're just taking", "tokens": [6794, 370, 1338, 321, 600, 733, 295, 9354, 264, 1045, 721, 457, 321, 434, 445, 1940], "temperature": 0.0, "avg_logprob": -0.17296232715729745, "compression_ratio": 1.6196319018404908, "no_speech_prob": 4.222772531647934e-06}, {"id": 185, "seek": 116040, "start": 1160.4, "end": 1166.0, "text": " really and this is I mean this is so much of deep learning is taking linear", "tokens": [534, 293, 341, 307, 286, 914, 341, 307, 370, 709, 295, 2452, 2539, 307, 1940, 8213], "temperature": 0.0, "avg_logprob": -0.11081919419138055, "compression_ratio": 1.7568807339449541, "no_speech_prob": 1.7231213860213757e-05}, {"id": 186, "seek": 116040, "start": 1166.0, "end": 1171.76, "text": " linear combinations kind of doing a linear operation of multiplying and", "tokens": [8213, 21267, 733, 295, 884, 257, 8213, 6916, 295, 30955, 293], "temperature": 0.0, "avg_logprob": -0.11081919419138055, "compression_ratio": 1.7568807339449541, "no_speech_prob": 1.7231213860213757e-05}, {"id": 187, "seek": 116040, "start": 1171.76, "end": 1175.24, "text": " adding two things and then putting it through a non-linearity such as sigmoid", "tokens": [5127, 732, 721, 293, 550, 3372, 309, 807, 257, 2107, 12, 1889, 17409, 1270, 382, 4556, 3280, 327], "temperature": 0.0, "avg_logprob": -0.11081919419138055, "compression_ratio": 1.7568807339449541, "no_speech_prob": 1.7231213860213757e-05}, {"id": 188, "seek": 116040, "start": 1175.24, "end": 1183.2, "text": " or tange and it's neat that that gives us kind of a more power an RNN that's", "tokens": [420, 256, 933, 293, 309, 311, 10654, 300, 300, 2709, 505, 733, 295, 257, 544, 1347, 364, 45702, 45, 300, 311], "temperature": 0.0, "avg_logprob": -0.11081919419138055, "compression_ratio": 1.7568807339449541, "no_speech_prob": 1.7231213860213757e-05}, {"id": 189, "seek": 116040, "start": 1183.2, "end": 1188.8000000000002, "text": " capable of kind of having this long-term memory and so then what that looks like", "tokens": [8189, 295, 733, 295, 1419, 341, 938, 12, 7039, 4675, 293, 370, 550, 437, 300, 1542, 411], "temperature": 0.0, "avg_logprob": -0.11081919419138055, "compression_ratio": 1.7568807339449541, "no_speech_prob": 1.7231213860213757e-05}, {"id": 190, "seek": 118880, "start": 1188.8, "end": 1195.04, "text": " when we create model 7 is now instead of doing nn.grucell which was calling", "tokens": [562, 321, 1884, 2316, 1614, 307, 586, 2602, 295, 884, 297, 77, 13, 861, 84, 4164, 597, 390, 5141], "temperature": 0.0, "avg_logprob": -0.26168739795684814, "compression_ratio": 1.5061728395061729, "no_speech_prob": 1.863088436948601e-05}, {"id": 191, "seek": 118880, "start": 1195.04, "end": 1199.6, "text": " from pythons nn module and taking the Python implementation of grucell we're", "tokens": [490, 10664, 392, 892, 297, 77, 10088, 293, 1940, 264, 15329, 11420, 295, 677, 84, 4164, 321, 434], "temperature": 0.0, "avg_logprob": -0.26168739795684814, "compression_ratio": 1.5061728395061729, "no_speech_prob": 1.863088436948601e-05}, {"id": 192, "seek": 118880, "start": 1199.6, "end": 1207.52, "text": " using the one we made ourselves and so running this", "tokens": [1228, 264, 472, 321, 1027, 4175, 293, 370, 2614, 341], "temperature": 0.0, "avg_logprob": -0.26168739795684814, "compression_ratio": 1.5061728395061729, "no_speech_prob": 1.863088436948601e-05}, {"id": 193, "seek": 120752, "start": 1207.52, "end": 1215.76, "text": " let's see hopefully it trains this time", "tokens": [718, 311, 536, 4696, 309, 16329, 341, 565], "temperature": 0.0, "avg_logprob": -0.16743107636769614, "compression_ratio": 1.5271317829457365, "no_speech_prob": 1.6700410924386233e-05}, {"id": 194, "seek": 120752, "start": 1222.2, "end": 1228.56, "text": " yeah and so we get 73% very similar and the reason we got this you know the", "tokens": [1338, 293, 370, 321, 483, 28387, 4, 588, 2531, 293, 264, 1778, 321, 658, 341, 291, 458, 264], "temperature": 0.0, "avg_logprob": -0.16743107636769614, "compression_ratio": 1.5271317829457365, "no_speech_prob": 1.6700410924386233e-05}, {"id": 195, "seek": 120752, "start": 1228.56, "end": 1232.04, "text": " similar accuracy on these three versions is these were three versions of the same", "tokens": [2531, 14170, 322, 613, 1045, 9606, 307, 613, 645, 1045, 9606, 295, 264, 912], "temperature": 0.0, "avg_logprob": -0.16743107636769614, "compression_ratio": 1.5271317829457365, "no_speech_prob": 1.6700410924386233e-05}, {"id": 196, "seek": 123204, "start": 1232.04, "end": 1249.72, "text": " thing we were just refactoring to write our own grucell any questions yes sorry", "tokens": [551, 321, 645, 445, 1895, 578, 3662, 281, 2464, 527, 1065, 677, 84, 4164, 604, 1651, 2086, 2597], "temperature": 0.0, "avg_logprob": -0.14642702147018077, "compression_ratio": 1.462962962962963, "no_speech_prob": 4.832070044358261e-05}, {"id": 197, "seek": 123204, "start": 1249.72, "end": 1261.56, "text": " now that we have the what oh that is a good question so we do not have dropout", "tokens": [586, 300, 321, 362, 264, 437, 1954, 300, 307, 257, 665, 1168, 370, 321, 360, 406, 362, 3270, 346], "temperature": 0.0, "avg_logprob": -0.14642702147018077, "compression_ratio": 1.462962962962963, "no_speech_prob": 4.832070044358261e-05}, {"id": 198, "seek": 126156, "start": 1261.56, "end": 1275.04, "text": " in this oh you're asking where was the dropout oh and the AWD LSTM so this keep", "tokens": [294, 341, 1954, 291, 434, 3365, 689, 390, 264, 3270, 346, 1954, 293, 264, 25815, 35, 441, 6840, 44, 370, 341, 1066], "temperature": 0.0, "avg_logprob": -0.12381564825773239, "compression_ratio": 1.44375, "no_speech_prob": 0.00018811484915204346}, {"id": 199, "seek": 126156, "start": 1275.04, "end": 1279.6, "text": " in mind this is like a kind of very simple model that we use to even in the", "tokens": [294, 1575, 341, 307, 411, 257, 733, 295, 588, 2199, 2316, 300, 321, 764, 281, 754, 294, 264], "temperature": 0.0, "avg_logprob": -0.12381564825773239, "compression_ratio": 1.44375, "no_speech_prob": 0.00018811484915204346}, {"id": 200, "seek": 126156, "start": 1279.6, "end": 1286.72, "text": " original notebook 6 this was just to kind of illustrate a simple RNN to see", "tokens": [3380, 21060, 1386, 341, 390, 445, 281, 733, 295, 23221, 257, 2199, 45702, 45, 281, 536], "temperature": 0.0, "avg_logprob": -0.12381564825773239, "compression_ratio": 1.44375, "no_speech_prob": 0.00018811484915204346}, {"id": 201, "seek": 128672, "start": 1286.72, "end": 1292.88, "text": " the dropout I would go back to so kind of even this version before we didn't", "tokens": [264, 3270, 346, 286, 576, 352, 646, 281, 370, 733, 295, 754, 341, 3037, 949, 321, 994, 380], "temperature": 0.0, "avg_logprob": -0.15315483234546803, "compression_ratio": 1.5101010101010102, "no_speech_prob": 2.178161776100751e-05}, {"id": 202, "seek": 128672, "start": 1292.88, "end": 1299.84, "text": " have any dropout since it's kind of this tiny tiny data set for a simple model", "tokens": [362, 604, 3270, 346, 1670, 309, 311, 733, 295, 341, 5870, 5870, 1412, 992, 337, 257, 2199, 2316], "temperature": 0.0, "avg_logprob": -0.15315483234546803, "compression_ratio": 1.5101010101010102, "no_speech_prob": 2.178161776100751e-05}, {"id": 203, "seek": 128672, "start": 1300.2, "end": 1306.3600000000001, "text": " okay yeah I'll look at adding it next week I can pull up a seek to seek", "tokens": [1392, 1338, 286, 603, 574, 412, 5127, 309, 958, 1243, 286, 393, 2235, 493, 257, 8075, 281, 8075], "temperature": 0.0, "avg_logprob": -0.15315483234546803, "compression_ratio": 1.5101010101010102, "no_speech_prob": 2.178161776100751e-05}, {"id": 204, "seek": 128672, "start": 1306.3600000000001, "end": 1313.4, "text": " translation which is kind of more similar to what AWD LSTM well I guess", "tokens": [12853, 597, 307, 733, 295, 544, 2531, 281, 437, 25815, 35, 441, 6840, 44, 731, 286, 2041], "temperature": 0.0, "avg_logprob": -0.15315483234546803, "compression_ratio": 1.5101010101010102, "no_speech_prob": 2.178161776100751e-05}, {"id": 205, "seek": 131340, "start": 1313.4, "end": 1322.6000000000001, "text": " AWD LSTM we used for a language model just to show and seek to seek where", "tokens": [25815, 35, 441, 6840, 44, 321, 1143, 337, 257, 2856, 2316, 445, 281, 855, 293, 8075, 281, 8075, 689], "temperature": 0.0, "avg_logprob": -0.15814046470486387, "compression_ratio": 1.304, "no_speech_prob": 4.356803856353508e-06}, {"id": 206, "seek": 131340, "start": 1322.6000000000001, "end": 1327.0800000000002, "text": " dropout is find it", "tokens": [3270, 346, 307, 915, 309], "temperature": 0.0, "avg_logprob": -0.15814046470486387, "compression_ratio": 1.304, "no_speech_prob": 4.356803856353508e-06}, {"id": 207, "seek": 131340, "start": 1335.52, "end": 1342.3400000000001, "text": " so here you can kind of see in the structure we've got dropout for the", "tokens": [370, 510, 291, 393, 733, 295, 536, 294, 264, 3877, 321, 600, 658, 3270, 346, 337, 264], "temperature": 0.0, "avg_logprob": -0.15814046470486387, "compression_ratio": 1.304, "no_speech_prob": 4.356803856353508e-06}, {"id": 208, "seek": 134234, "start": 1342.34, "end": 1348.32, "text": " encoder embeddings and then we have dropout down here for the output from", "tokens": [2058, 19866, 12240, 29432, 293, 550, 321, 362, 3270, 346, 760, 510, 337, 264, 5598, 490], "temperature": 0.0, "avg_logprob": -0.11169551521219233, "compression_ratio": 1.5940170940170941, "no_speech_prob": 6.108138040872291e-05}, {"id": 209, "seek": 134234, "start": 1348.32, "end": 1354.6399999999999, "text": " the decoder so those are two places actually now that I remember AWD LSTM", "tokens": [264, 979, 19866, 370, 729, 366, 732, 3190, 767, 586, 300, 286, 1604, 25815, 35, 441, 6840, 44], "temperature": 0.0, "avg_logprob": -0.11169551521219233, "compression_ratio": 1.5940170940170941, "no_speech_prob": 6.108138040872291e-05}, {"id": 210, "seek": 134234, "start": 1354.6399999999999, "end": 1358.84, "text": " kind of a key thing was that it applies dropout I think basically everywhere", "tokens": [733, 295, 257, 2141, 551, 390, 300, 309, 13165, 3270, 346, 286, 519, 1936, 5315], "temperature": 0.0, "avg_logprob": -0.11169551521219233, "compression_ratio": 1.5940170940170941, "no_speech_prob": 6.108138040872291e-05}, {"id": 211, "seek": 134234, "start": 1358.84, "end": 1367.6799999999998, "text": " like it's got dropout in a few different places but this is oh to show", "tokens": [411, 309, 311, 658, 3270, 346, 294, 257, 1326, 819, 3190, 457, 341, 307, 1954, 281, 855], "temperature": 0.0, "avg_logprob": -0.11169551521219233, "compression_ratio": 1.5940170940170941, "no_speech_prob": 6.108138040872291e-05}, {"id": 212, "seek": 134234, "start": 1367.6799999999998, "end": 1371.3999999999999, "text": " where okay sorry I misunderstood the question yeah we can we can do that next", "tokens": [689, 1392, 2597, 286, 33870, 264, 1168, 1338, 321, 393, 321, 393, 360, 300, 958], "temperature": 0.0, "avg_logprob": -0.11169551521219233, "compression_ratio": 1.5940170940170941, "no_speech_prob": 6.108138040872291e-05}, {"id": 213, "seek": 137140, "start": 1371.4, "end": 1383.2800000000002, "text": " time sorry about that other questions on on GRU's let me just make sure I didn't", "tokens": [565, 2597, 466, 300, 661, 1651, 322, 322, 10903, 52, 311, 718, 385, 445, 652, 988, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.1809455108642578, "compression_ratio": 1.4045801526717556, "no_speech_prob": 7.140762318158522e-05}, {"id": 214, "seek": 137140, "start": 1383.2800000000002, "end": 1390.3600000000001, "text": " have any more slides I think this is it yes that is it okay well I will yeah I", "tokens": [362, 604, 544, 9788, 286, 519, 341, 307, 309, 2086, 300, 307, 309, 1392, 731, 286, 486, 1338, 286], "temperature": 0.0, "avg_logprob": -0.1809455108642578, "compression_ratio": 1.4045801526717556, "no_speech_prob": 7.140762318158522e-05}, {"id": 215, "seek": 139036, "start": 1390.36, "end": 1402.12, "text": " will see you on Thursday", "tokens": [50364, 486, 536, 291, 322, 10383, 50952], "temperature": 0.0, "avg_logprob": -0.50552898645401, "compression_ratio": 0.75, "no_speech_prob": 7.241693674586713e-05}], "language": "en"}