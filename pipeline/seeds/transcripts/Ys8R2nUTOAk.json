{"text": " Okay it's 11 of 5 I'm gonna get started. So we're finishing up topic modeling. First just can someone remind us what the SVD is? Brad? Oh wait hold on for the microphone. Exactly yes. So singular value decomposition you get a matrix of orthonormal columns and then in the center there's a diagonal matrix and as Brad said it's got the singular values in descending order and then the third matrix U is orthonormal rows. And so we were talking at the end of last time about how we could use a randomized SVD to get the truncated form a lot faster. And so kind of the truncated so full SVD you're getting you know you're able to fully reconstruct your original data matrix which might be useful but if you're trying to do data compression or you just want to do something faster you probably don't need all of that and so it'd be quicker to not calculate the whole thing. So I wanted to remind you that full SVD is slow so this is the calculation back from the very beginning of this notebook. And again to remind you vectors this is actually I'll ask you what is a vectors this is kind of the matrix we've been looking at for the past three days in notebook two. Does anyone remember what data set we're looking at? Do it I see see your lips moving is that a thanks Tim exactly yes so this is the newsgroup state data set so it's kind of posts that people have written on these message boards and on the internet in different categories and our rows are the different words and the columns are the particular post. So when we did the SVD on it using SciPy's implementation this is for a full one it took 28 seconds which is pretty slow. You can see that here and I think I mentioned this before but percent time is a magic or they're called magics in Jupyter notebooks but it's really useful to get the the time of something you're running. So let me check the shape and here we were getting U was 2000 by 2000 we had 2000 singular values and then V was 2000 by 26,000. However if we do the randomized SVD so suppose we were only interested in the top five topics we run that and and this is from scikit-learn that only took 154 milliseconds so this is an order of magnitude faster. So that's kind of the first talk a little bit more about this but that's the idea we're closing closing out today of doing this randomized version and it's a lot quicker. So I looked it up this was a question last time the runtime complexity for SVD is big O of the minimum of M squared N and MN squared. We'll be talking in a later week about how you would actually calculate the SVD. For now we're just going to use SciPy's implementation. We had this question of how can we speed things up without coming up with a better implementation for finding the SVD and our idea was to just use a smaller matrix with smaller N and so and so this right now we're kind of explaining how the randomized SVD works. So instead of calculating the SVD on the full matrix A which was M by N we could just use B equals AQ and have that be M by R where R is a lot smaller than N. So we're actually still using the same SciPy implementation of SVD we're just multiplying our matrix to make it smaller by a random matrix and that's a lot quicker and gives us just the information we need. Here's the calculation again and we can see that the topics are really good. So again as a refresher. Oh Brad? Oh that's a that's a great question. So we want B to have the same column space as A is the hope. So yes that would be a low-rank approximation. I'm not sure that sounds like it like I actually yeah that is that is the case that you do get it yeah best low-rank approximation. That's yeah that's not what we're doing here that's a bit circular almost because in order to do that you have to get the full SVD to know that you're taking the the best low-rank approximation. Here we want to get a low-rank approximation that's kind of good enough and then just do the SVD on that. And so well okay so a difference so it's an approximation of the column space. Yeah so I guess this is another key distinction is A and B have different dimensions because we've made so if A is like our giant matrix B is tall and thin so you know kind of cut off all these columns. So yeah this is an excellent point so B is not actually approximating A but the the hope is that the columns of A span a similar space to the columns of B. So we're kind of using fewer columns to hopefully span the same space. Yeah thank you. And this so and I should also say I've kind of added so this is I tried a different perspective for talking about the randomized SVD from the end of class last time so there's some new material in this notebook that I am updated on github yesterday but if you have the version from Tuesday some of this is new but I just kind of really wanted to emphasize the speed and the fact that we haven't found a new way of finding the SVD yet we're just kind of taking a smaller matrix to speed things up. And then this is from the Facebook research blog post that I keep referencing that had the nice colorful picture and they ran some a few cases that I wanted to highlight for benchmarks but they looked at large dense matrices that were up to a hundred thousand by two hundred thousand in size. Large sparse matrices up to ten to the seventh by ten to the seventh in size and those had ten to the ninth non zero elements and then they also looked at Apache's spark distributed SVD implementation and they just wanted rank ten approximations. So for rank ten calculating out a matrix that is ten million by ten million is really going to be excessive and so you can see and so RPAC is kind of state-of-the-art like it's a matrix factorization package that is widely used so this is not saying that kind of RPAC is bad it's just kind of like how slow it is to fully do these things. They found that it was one second to do the randomized version versus a hundred seconds for ten to the sixth by ten to the fifth matrices and then it was five seconds versus 63 minutes for ten to this oh for the this is like a denser version. So yeah I just wanted to show those numbers are a really big improvement because you yeah five seconds versus 63 minutes is huge and that's also I think it's a really compelling detail when people include things like that in blog post. I should say in preparing that in preparing the next lesson I had found a blog post that had this like threaded implementation to like run something in parallel and it was just like oh this should be faster but they didn't have any like actual times and so I used their code and ran it and it was actually slower with the parallel processing and so that's something it's very valuable to definitely run it and tell people the times. Yeah so that's that's impressive and then we're not going to get into too much detail in this but kind of the idea that should help justify why this why this is okay is the Johnson-Lindenstrasse lemma and that is that states that a small set of points in a high dimensional space can be embedded in a space of a much lower dimension in such a way that distances between the points are nearly preserved and that idea of distances between points being preserved is really about preserving the structure kind of if you have a data set that's in a ton of dimensions you should be able to put it into fewer dimensions and kind of keep the structure and that's what we're doing here with A and B in a sense of Tim. So when you say embedding do you mean like a projection into it? Like what do you mean by embedded into a space? Um yeah yeah projection is a good way to think about it yeah protecting it. Oh another question. So when you're talking about approximating the column space is this in the way that we're talking about approximating the column space where it preserves these relationships between these higher dimensional points? Yeah so with the with the column space going from from A to B so A is the large one actually let me just draw that so kind of A is this size B is the same height but thinner. So here you can think of the data points as the rows and so the idea is kind of you've gone from if this has N columns and this has R columns. And then they're both M tall you have M data points and that those M data points are still have the same structure between them or the same distance between them whether you're looking at A where it's N dimensional or B where it's just R dimensional. Right so the approximation is in the relationship between the rows. Yes yes. Okay great thank you. Okay any other questions and we're not going to we're definitely not going to prove the Johnson Lindenstrasse lemma in here or even if some of the details I think last time. It's okay if you don't have all the steps from last time of kind of this process of how we did the randomized SVD. I mostly want you to kind of get this general intuition of why it might be okay that we're taking this random projection. Okay. Yeah so going back. So we looked at this code last time that I want to go through it again briefly. So to kind of implement the randomized SVD. First we created a randomized range finder. And here this is this is kind of getting our mapping of kind of well getting the random the random matrix that we are going to multiply our original matrix by and to do that we randomly generate a matrix and then you could ignore this for now. We take a QR factorization on a and what that does it actually should check does anyone remember what the QR factorization gives you so you get two matrices back. Q is orthogonal. Yes exactly. Yeah so you get back a orthogonal matrix in an upper triangular and that orthogonal matrix is the one we'll use for our random or sorry we're taking so we're taking randomly generated matrix Q multiply that by a and then take the QR on that. And then we call that from randomized SVD. Does anyone remember so number of components is the kind of number of singular values we want or the ultimate number of topics for our topic modeling. What is the number of over samples. Anyone. Okay this is the this is the buffer that we kind of added on. So we're saying you know if I want five topics it's actually safer. We don't want to just compute like if we make be just have five columns that's actually cutting it a bit close that we're really going to get the five best singular values of a from that. And so what we'll do is we'll add some more on and be like okay let's actually call calculate 15 columns and 15 singular values and then we'll take the top five out of that. So we'll still return five if that's our desired amount, but we're just kind of adding this safety buffer because we do have some randomness we don't know that we're going to get the best five singular values. Yeah, and so that's that's mostly it so you'll notice we're calling the same sci pi Lin out as VD method in here. It's just now we're calling it on B, which is a lot narrower than a. Oh, and then I should roll this up. So then an exercise that I wanted you to try was to write a loop to calculate the air of your decomposition, as you vary the number of topics. And so, and it was take a few minutes to do this but what you're doing here is just using the randomized SVD you don't have to change its implementation at all, but kind of decompose your matrix recompose it and then see what its air is from the original matrix. And this would be useful if you're doing data compression or something kind of to see, you know how much of the original are you capturing and something with. So I just wanted you to kind of loop through through a few numbers to see how it's changing. Remember we originally had 25,000 columns, so we would expect that five singular values is not going to capture everything so the air will be relatively high. So I can talk about the answer to this one. And actually can you remind me did I leave the plot in the notebooks that you had on GitHub. Okay. So what do you what do you notice about the plot, come back to the code in a moment. Brad. Okay, say again. Yeah, yeah, the air drops off. I thought surprisingly quickly for this that. Yeah, you have a lot of air initially but well really I mean around 50 is like kind of the steepest but definitely past like 100 it's really a lot lower and kind of not changing as rapidly. And so you can see like kind of the first the first many singular or first few singular values, giving you a lot of a lot of power kind of capturing a lot but as you go on the additional singular values aren't capturing as much as kind of what this is showing. So, so I did what I did to get this was have a loop, where I was taking the randomized SVD of I, as I is increasing. And I did this in steps of 20. So this is a US and V, I created the reconstructed matrix using you times and p.diag turns s which is just an array into a diagonal matrix by filling in zeros and and p.diag also if you give it a matrix and not an array, then it will return an array of what was on the diagonal, using this from array to a matrix functionality of it. So I'm just doing you times the matrix form times V. And then I'm saying that the air is the norm of the vectors minus reconstructed. Jeremy. I mentioned a trick that I like. Are there any questions about this. All right. Are there any further questions on topic modeling before we start on background removal and robust PCA. Kelsey. I'm sorry. Q by O down here. Yes. Well, so that's actually let me go to the equations, I think show it better. So, you know, we've got our skinnier B and we take the SVD on that. And then what we're interested in is the SVD of a. And so we're kind of plugging back in. Remember from last time we said a is approximately Q times Q transpose a. Where Q ends up being, you know, this random matrix we've used and the Q transpose a is how we got our narrow matrix B. So we're just kind of plugging in. Okay, this is B. Let's kind of plug that into here for Q transpose a. And that's how we get Q times basically the SVD of B. Any other questions. Okay. Oh, Tim. To the randomized range finder algorithm. Yes. You said you're going to explain later the for loop. So I was putting that off. But as a kind of as an intuitive idea, what that's doing is by taking additional powers of a, you're getting something that's more kind of even more in the range of a. So kind of by taking extra powers of a. Hopefully that's helping you kind of really get the important components of the range of a because remember here we're trying to find something that has the same. You know, we want B to have the same range as a and so additional powers of a help us do that. But if we were just to take powers of a without, you know, here we're using this LU decomposition. And we're doing that is just taking powers of a. We would have the problem of either the matrix will kind of be like exploding as its values got larger and larger going towards infinity or they would be shrinking to zero. And either way, that's a problem. And so this LU factorization is having the effect of normalizing it. Yes. Yeah. Jeremy. You can use it. I was just going to bring up the Wikipedia page because I like that they list a ton of different fields where they're saying this Lemma has uses in compressed sensing manifold learning dimensionality reduction and graph embedding. But I just kind of like like that like list of places where this is useful. Yeah, thank you. Any other final SPD and NMF questions and we'll be returning to SPD. We're both going to use it and other applications and then we'll learn more about how it's calculated. Okay, let's start on background removal. This is notebook three. I'm really excited about this one background removal with robust PCA. And so, So here this is our goal that we'll be working on in the notebook is we have a surveillance video. So you see here there's some figures walking across the screen and we want to be able to pick out what's the background and what are the people. So we've taken the picture on the left and broken it into this middle picture and the picture on the right. That's the goal that we're going for. Using something called the real video of three data set here is a link to it if you want to download it so that you can do this as well. I also found some other sources of video data set so something I kind of had to look for for a while. So I would be curious if you do try this on a different video data set to see how how your results are. So I had I had never worked with video data in Python before this. I found the movie pie library which seemed really neat and actually kind of made me want to do more work with videos because here we kind of just use it briefly to convert this into a matrix. Yeah, I've got my imports and then I wanted to show you this. I actually watched the video or at least part of it was too long to watch the whole thing. So there you can see people walking. I didn't color, but I converted it to black and white to kind of keep things simple or simpler. And that's it. That's our data. And this video is 113 seconds long total. So I have some helper methods, and we're not going to super go into those. We're really it's mostly I'm just using so the video library has this get frame that I'll use to kind of pick out specific frames, and then we're stacking those up to form a matrix. And I think that's actually kind of easier to see from I have a few pictures. So this is a single kind of single frame, you know, one point in time. And I actually I guess I should first note. So I scale this and I tried doing it with kind of the full resolution and it was really slow, but I do have some pictures from the high resolution, although I don't recommend that. But here you can enter the scale as a percentage. And so I'm just using 25 percent scale to make it faster. And so an image from one moment in time will be 60 pixels by 80 pixels. And we're going to unroll that into a single column. So kind of each moment in time is going to be this 4800 long column. And then we're stacking those all together from these different points in time. And so we end up with a matrix that's 11300 by 4800 representing the video. And I wanted to show a picture of this is what the whole video looks like. So does anyone have ideas? What do you think those wavy black lines are? You want to toss this? That's probably the people moving. Yes, those are the people moving in the frame. And so then what are the kind of solid horizontal lines? The background. The background. Yeah. So remember each each column here is a single moment in time. And so some things like that sign are never moving and probably just show up as kind of these horizontal white lines. But the people do move. And so those those pixels are different. So the columns are a moment in time. The columns are a moment in time. And this is I think backwards from probably what I wrote in the older version of depending on when you last pulled from GitHub. But yeah, here the columns are a moment in time. Is this all related to optical flow? What do you mean by optical flow? I don't really know what it is. So it's some sort of methodology of tracking movements through like you have two adjacent frames like having morphed one to match the next frame. Oh, OK. Yeah, I'm not familiar with that area. So I'm not sure if. If there are, because that is kind of brings up a good point that here it's assumed that the camera is fixed. And so things are kind of being referenced by I don't know, like, you know, everything that's at X coordinate 20, Y coordinate 30 is showing up at the exact same. You know, when you enroll it, that's kind of point 600. And so it's always going to be showing up in the sixth hundredth row. So this is kind of fixed. Jeremy, we mentioned the other day that we look at a low rank matrix for Tennessee lines on it. So I guess this is showing this is this is a low rank matrix already. This is it's. Yeah, I mean, yeah. So the horizontal lines indicate that it's low rank. And actually, first I should ask who can tell us what rank is when we're talking about matrices? Anyone I haven't heard from Kelsey. So rank is the number of independent columns. Yes. Yeah. So rank is the number of independent columns. And that turns out to be equal to the number of independent rows. Another way to think about it is it's the dimension of the column space or the dimension of the row space, which is equal to the number of linear independent columns and rows. Yes, let me just kind of go back up here. So I definitely encourage it's really important to be able to look at your data and kind of find ways to visualize it. But here we've created our data matrix, put that all into M. This is just a NumPy 2D array. And then here we're kind of just saying let's take all values and in particular or sorry, take all rows and then a particular columns. And that's the point in time that we're looking at. And we do need to reshape it to get it kind of back into a picture. So we've kind of unrolled it into at least if I kept I might have gotten rid of it at one point I had a picture of just like a single a single row, which is pretty boring because it's just doesn't look like anything by itself. You know, it's black, white, gray, black, white, gray. Then we kind of have to reshape it back into our square matrix. So NumPy dot reshape is pretty handy. Questions so far just kind of on what's in M or this the setup. OK. And then we've just talked about SPD. So kind of remember what kind of matrices we're getting back. And so we're going to try first just with a with using randomized SPD to see what we get. So we put in M and then I see I tried this with a few different. So the second parameter is just telling you the number of components you went back or the number of singular values. And I actually felt like I got the best results with two here, but I got back U, S and V. And then I said my low rank matrix is U times and P dot diagonal S times V. And this is what the low rank version looked like. Here I'm just first let me ask, why is this low rank when I do U times S times V? Remember, U, S, V is the truncated SPD that we've asked for. Now we're multiplying those three matrices together. It's lowering because we only have two columns. Exactly. Yes. And so even though and let me go to the picture. Even though use got a ton of non zero values and what S looks like is just. I don't know some value S 1 0 0 S 2. I would actually I guess that means that this. Sorry, this should only have two columns then to line up. But the. Or no, you have two columns. OK, so you was like this. And so use very tall and has a lot of non zero values for that. But as is just kind of got these two components and then we go to V is going to be. Really long. And again, it's got a lot of non zero values known its long narrowness, but it's only only two rows. And so when we multiply. And if you ask and be together, they're only going to be two. And two linearly independent columns in the result, we kind of can't get more complexity than that since we've only got two dimensions in these inputs of what we're multiplying together. And so that's because. When you're multiplying those speaking to the microphone, when you multiply those. Basis vectors in S. Each one of those other vectors is going to be essentially scaling that vector together value zero. Right, so all like when you multiply S. Yes, we're all going to be. Adding the two basis vectors. Five zero and zero. I'm not sure the numbers, but it's just going to be those two. Yeah, so that's a good way of thinking about it. So we talked last time about how you can think of matrix multiplication as taking linear combination of the columns. So here when we do US what we're getting is. And actually let me. I'm going to redraw this. Hold on a moment. OK. So you is two columns you one. And you too, and so then when we do you times S. We're getting. I got something that's tall really that's just going to be S1 times you one is the first column and the second column is S2 times you too. Which is why you can replace the matrix product with an element wise application. So yes. And then when we multiply that by. Our columns, V or sorry Rose V1 V2. I guess at that point we're kind of having to think about dot products, but you can see that like really we're kind of just taking these two long vectors and that's all we have to work with here. We can also write out the dimension of these might be nice to see. Yes, you can see back you is 76,800 by two. This is just two and V is two by 11,300. And so that's kind of showing showing up when we plot this picture and actually I could plot the whole picture again. I think that would be good to see. Okay, this is a little bit slow, so we'll look at the picture of the one. Basically, it's got to have practically the same picture for everything. I guess I mean I guess you could have two different pictures that it could show that it's really not able to capture a whole video's worth of data. Any other questions about this, Kelsey. Oh, yeah, no I encourage you to try this with different ones I just tried it with a few and thought that to look the best. Try it with others. I think though some of the idea is that like, unless you're doing a huge rank like I don't know, so if you were doing like rank 1000 it's like maybe you are trying to capture all the different places people can be or like capture the information about people. But beyond that, I don't know like with a rank of 10 there's not really 10 pieces of information you can capture, you know, I mean you kind of have the background. And then like the next piece of information is like all this information about people. This is sorry, I don't know why this hasn't shown up yet this is quite slow. Oh, let me do that. Although I must if the other ones are showing up. Oh, and Sam Scott a question in the back. In the SK learn randomized SPD. It is yes. Default actually is 10 or 15 it's. Yeah, I think this is coming from a from a paper from play from the HALCO paper. So this is. Yeah, so that's it. That is what SK learn is doing. I don't I don't think that's something you would want to tune yourself unless you had like specific reason to think that that would be helpful. But I think like the kind of the papers develop this like theory of why this should work, or why this does work. So in trying to do it why why we get back just the background doing this would you say that it's because given that the vectors that make up the background in the matrix over time, sort of our most important across entire matrix. The skills are largest, as opposed to the vectors that correspond to people moving since they're very sparse in there. When you limit it to a few singular values. It's going to murder. Right. Yeah, like the most because the most important component. There's I guess there's two things going on. One is that really in any frame, like most of the frame is background, but it's also like looking across time the background is what shows up in like every single point in time like you always have the background there and I mean sometimes you know pieces of are obscured by a person. But for the most part, the, the background. I guess a way to think about it even as if you're looking at this whole picture like most of this picture is the horizontal lines that form the background. So those are going to be the largest, the largest singular value because they're kind of like the most important component. And by the way, you mentioned to send about 1000 kind of a source code, a cool tip I don't think we've mentioned is if you type in your question mark question mark and then the name of the thing. It'll show you the source code. And actually, yeah my Colonel died I need to rerun a few things. Yeah, that's a that's a great point. Thank you. I'm just going to rerun this because I do want to have the matrix to be able to use. I was going to say going back to this, I'm kind of talking about rank and creating the data matrix is a little bit slow. So I have to wait a moment on that. I actually expect rank one to be pretty good. And I think I did it and found that rank two was for some reason better, but really like the background should be able to perfectly be captured by a rank one matrix. So you've unrolled you know 2d space, so that a single picture can show up just as one column and so really the column just of the background. Is running now. Okay. So this is our low rank reconstruction. So we did the SBD and then we've reconstructed our matrix. So if you think back to the math standpoint, what's going on is we're trying to reconstruct this matrix. So you could think of this as a data compression problem, like you didn't want to have to store all of this. So with just rank two, so just these two columns in U, two singular values, two rows in V, we get this much of our original matrix, which is a lot of it. Kind of like everything except the wavy black lines. So then, and a more exciting part I think is the people that we want to see. Although it's easier to pick out the background because that's what we have information about. So what we'll do is just subtract our original matrix minus the low rank approximation, which is just the background. So that hopefully will give us the people. So you can see here, this is what's left. Here I'm just looking at a single point in time, 140. Actually, let me copy that. So you can do this at different points in time. The other one was, yeah, but now I've written over my variables. So yeah, so the low-res version doesn't show quite as much. Actually, I don't. Here. Yeah. So this is the low-res version. So you can't see as clearly, but I was trying to high-res. I recommend staying with low-res just for the speed though. You can see what the people are doing at different points. It's not perfect. We do have these light marks around the people. Here in the high-res version, you can see that the sign that was in the picture, or at least I guess it's white space around the sign was getting captured. But it's pretty good. That's what we get from randomized SVD, which is a pretty simple approach for this. So then we're going through a more complicated algorithm, and this is the result that we'll get with the more complicated algorithm. Which doesn't have the issue with the sign, so I think it is a bit better. I'll be honest, I was disappointed that it wasn't more better because it is a lot more complicated. But it's nice because it's also an example of robust PCA, and we'll be able to talk about a lot of issues that arise. It still uses randomized SVD as part of it. So basically, we have a more complicated algorithm that's iteratively using randomized SVD for one of its steps. Oh, okay. I saw it's about 12 o'clock. So this is probably a good time to stop for our break. We can meet back at 12.07. When we resume, we'll be talking about PCA and robust PCA. Yeah, Jean's picture. Yeah. I'm going to start back up since it's 12.08. One more thing, I announced this last time, but I just wanted to say again that I'm going to be out of town on Friday, which is normally when I have office hours. But let me know if you want to meet today or next Monday or Tuesday. Then while we were on break, I just tried running this again with a rank one approximation because I was curious. So here, this is what a rank one matrix looks like. It's just a single column repeated again and again. So it would give you the exact same background for any point. Here I'm subtracting the original matrix, minus the background and I get the people. So that's pretty amazing, I think, for a rank one approximation. It's not the average. No. So if you did the full, and I should repeat the question was, is rank one matrix like the average? It's not the average. It's still doing an SVD, but it's finding the, basically the matrix that's going to give you the closest. Actually, I'm unsure if that would be the average. You want the matrix that's closest to your original matrix, but only has one rank? I think the average would have a lot more people who are into the background. That's true. Yes. I think, and it might depend on your error metric, but you would want your original data matrix that has the wavy lines. You would want to be minimizing that minus this matrix. I think to have error at just a few points will probably end up being better than having a small error everywhere. If you took the average, then every point that a person ever walked by is going to be like off a little bit so you would have a tiny error everywhere if you did that. That's a good question. Then also, stepping back, I just wanted to say, so with data like this, I don't put it on GitHub, but you can download the data yourself from, let me go back to the link, up here. This is the link of where I got this from. This is video 3 in the real videos. Although I definitely, if you do run this on different videos and get interesting results, please send them to me because I would be curious to see them. Then the other thing I probably should have done more of is, here where I calculate the matrix and it's really slow to calculate, you don't want to have to do that every time you run this. You can use np.save to save the NumPy array. This was for the high-res version, but you would probably want to do it as a low-res version. I'm just giving a file name, passing in my matrix M. I can save it and then next time I run this, instead of creating data matrix video, I could just do np.load. So that's a nice trick. Any other questions about the rank 1 approximation? Let's go. Yes. I'm past the microphone. I didn't hear, but did anything happen with the signpost? Anything happened with what? The signpost that would be in the line. The signpost? I think more what's happening is you're getting some blurs of the people right behind the signpost. It's like having those blurs of the people or what let you giving you that artifact of the signpost. Although- It's hard to see in the low-res version. It's hard to see in the low-res version. Yeah, you have to use the high-res version. I have to see that. But yeah, so here's the, which I just did for the rank 2 approximation up here. I think probably not so much that the signpost is showing up, is that you have these light blurs of people walking by behind it. So principal component analysis. Go to full screen. So typically when you're dealing with a high-dimensional dataset, you want to use the fact that the data usually has a low intrinsic dimensionality. So even though it's in a lot of dimensions, not all those dimensions have information. So you can think of that as lying on a lower dimensional manifold. So this is even with the randomized SVD, the idea of A had all these columns, but really the information of the column space could be captured by something with a lot fewer columns. So principal component analysis lets us eliminate dimensions. So classical PCA is seeking the best rank K estimate L. So I'm just calling it L because it's low rank of a matrix M. So K is a parameter that you're specifying going into it. You're saying, I want to see what the best. We just previously looked at rank 2 and rank 1 approximations, but you could choose any value for K. Then you're going to use an algorithm that's going to minimize the difference between M minus L for any possible L that has the rank you're interested in. So traditional PCA can handle noise in small magnitudes, but it's very brittle to having, even if you just have one observation that's super wrong, that can really throw it off. Which unfortunately in a lot of real-world datasets, you can have some observations that are just very wrong. So robust PCA is a way around this. Robust PCA factors a matrix into the sum of two matrices, low rank matrix L and a sparse matrix S. So next I'll ask you, what does it mean for a matrix to be sparse? Sam. It means that there are a lot of zero values. Exactly. Yes. So there are a lot of zero values. So going back to our example of the people, there are a lot of zero values, and here I'm actually zoomed in. So if you think of all this gray background, those are all zero values, and they're actually even more when you zoom out to the full size. So we want to find a factorization. This factorization is a little bit distinctive in that it's a sum. Most of the factorizations, in fact, I think all the other factorizations we'll learn in this course are multiplication. But here we're factoring something into a sum, and it's the low rank matrix plus a sparse matrix. It's not factoring, it's a decomposition. We are decomposing it into two. Most decompositions are factorizations because they're multiplying. We'll see this in a moment, but in the problem of if your data was corrupted, hopefully most of your data is not corrupted, and so you can think of the corrupted data as being the sparse matrix. So now I have some pictures that I took. This is from a really nice blog post. Let me open it. It's called Rebus Tensor PCA with Tensorly. I haven't tried Tensorly, which is a specific library. But these are really great illustrations. So this is face recognition, and it was done on a dataset where you had multiple pictures of one person's face, but from different angles where the light is coming from different angles. So here there would have been a lot of pictures of this person's face but lit up from different angles. So here's the original. They've added some noise in, or you can say grossly corrupted entries. I think this is pretty amazing. They get from this original, this is the low-rank component and this is the sparse component. So the sparse component is picking out the noise, and the low-rank component is picking out the face. I think particularly this third one is amazing because the one on the left right here, I can't even tell that this is a picture of a face, but they have picked out the face. So this is another application of this. So remember what we're looking at, the low-rank one will be the background and the sparse component will be the people. Here the low-rank is the space and the sparse component is this really awful noise. Then here he takes it up even another level and graze out entire sections in this picture of the face and is still able to recover the faces. Yeah. So it's very impressive. So other applications of this, latent semantic indexing, you could use robust PCA. There the low-rank matrix would be the common words that show up in all documents. So this is and probably in all your documents. Then your sparse matrix could capture a few keywords from each document that make it different than others. Then ranking and collaborative filtering. So this is an issue that Netflix has in terms of some of the data being really bad. I don't think they give examples of this, but I feel like this could even be things of, I don't know if a toddler accidentally enters some ratings for movies that are just totally have nothing to do with that person's actual preferences, but they use robust PCA. You want to say more about this Jeremy? There's a great example. For a lot of you know about the Netflix prize, it's a $1 million machine learning prize. There's a lot of great write-ups of that. The winners had to come up with some, well, interestingly, like before the Netflix prize, people have forgotten about some of these robust decomposition techniques. The winners really walk them back into vogue and they've had a lot of attention since then. Actually, there was- I can read the papers from that because they're very accessible. There was a fun New York Times article on the Netflix prize about Napoleon Dynamite. Are you all familiar with this movie? So apparently, Napoleon Dynamite was a hugely polarizing movie, and it also didn't relate to people's other preferences. So including Napoleon Dynamite in the Netflix problem makes it way more difficult because it's almost random how people will feel about the movie. So I thought that was a fun example of something that could really throw things off. I know I'm very biased as an ex-cattle person, but I was going to say a lot of people complain that the Netflix prize didn't lead directly to results, but the truth is actually the results of the Netflix prize had a huge impact on how people think about all kinds of areas of linear algebra, and collaborative filtering, and super valuables. Actually, yes, studying competition winning results is a great way to further your machine learning and data science skills. Thank you. I'm glad that you- Thanks. Yes, so just- and actually, to ask, raise your hand if you've seen the L1 norm inducing sparsity before. We'll review the concept of it. This is a good thing to be familiar with. I've had this show up in data scientist interviews that I've done. But the idea is that, so with the L1 norm, which is just taking the absolute values of your entries, so what the unit ball looks like is a diamond. Whereas with the L2 norm, the unit ball is a circle. The idea here is where you're looking for where this is going to intersect, with this other curve that you're optimizing. With the L1 norm, that is way most likely to happen at corners. Let me briefly, I guess, say the L1 norm, I'll write it on here. L1, you're taking a summation of absolute values for each. If you have something in multiple dimensions, sorry, this should be an xi. So each of your x values for the L2 norm, you're summing the squares and then taking the square root of the whole thing. These are generalization. So actually, this you're taking the whole thing to the power of one, which is nothing or has no effect. But this is generalization of this idea of the LP norm. But L1 and L2 are the most common. Sorry, for regularization, and this is the penalty term that you're adding when you're doing some optimization problem. So the idea with any of these is that you don't want your weights to get too large, and so you're putting a penalty on the size of your weights. But the type of penalty you put will affect what your answers are. So with the L1, it ends up saying, I want to keep this sparse, and so you're getting more of a penalty for making new things non-zero. Whereas the L2 norm, since you're trying to keep this whole square root of sum of squares to a small value, you will have probably a lot of things with small values. Whereas L1, it's better to maybe have one thing slightly bigger and the other weight zero. So these are how those penalties work. So these are some common pictures that people will show that if you have the problem you're trying to optimize and then also this penalty, thinking about where they intersect for L1 norm, it's going to be a corner which here the x-axis is zero and y is one. Whereas here it's going to be somewhere on the circle. Brad, can you throw the microphone, Jeremy. So one question I've always had with these pictures is that we're saying that with the L1 penalty we can induce sparsity, but we can't with L2 even though when you look at the graph, those points at zero are there. Is that because- Sorry, what points at zero? At the north pole of the circle and at each pole, right? Like here? Yeah, or in the graphs above. Here? You can technically see that you could hit those corners in the square there. But is the reason why we don't because the circles are continuum and the probability of hitting any one of those points is zero? I think it's more when you think about the structure of this. I'll look at this one. So here, these are the contours of what, in a lot of cases, is the air of your model. When you look at these, it's very unlikely that this would be perfectly aligned on the, I mean, it's possible. You could have something that was perfectly here and lying on the x-axis, but that would just be, to me, that implies you had this very artificial data set or something bizarre going on. So you're right, you could happen to hit those, but yeah, the chance is so minimally small. It also to me implies that there was something very artificially constructed about the problem you are minimizing. Tim? Yeah, I feel like the only way that the ellipse or circle would hit, like you'd actually have it hit zero exactly is if the actual minimal, the best solution was lied on one of the axes. That would mean that one of the coordinates was zero in the first place. Right. Not only that it was like one of those had to be zero, but that implies there's no noise in your problem as well. Because it's like even if you had a little bit of noise distorting it, that's going to take it off the axis. Right. Yeah, thank you. Other questions? We're actually not going to get too much into the details of this, but I wanted you to see how the robust PCA can be written as an optimization problem. There, you're minimizing the nuclear norm of L plus some parameter lambda times the L1 norm of S. Subject to you wanting L plus S to equal M. So M is your matrix that you're decomposing. This is basically just saying how do we describe a sparse and low-rank matrix using math? The way to describe a sparse matrix is to say we're minimizing the L1 norm. The way to describe a low-rank matrix is minimizing something called the nuclear norm, which is the L1 norm of the singular values. So you can think of this as results in sparse singular values. If a lot of singular values are zero, that means you have a low-rank matrix. So this is just how this is formally written out. I should say we're definitely not going to get deep into optimization in this course, but if this is a topic that interests you, optimization is a huge field. I linked to in particular, Steven Boyd, who's a professor at Stanford has an online class, videos through Open edX. Also, I just found this last week, he has a tutorial or a short course he gave using Jupiter Notebook, since I have links for that. Yes. Yeah. The link is in here. Part of what I want to show with this, so we'll be looking at an algorithm called primary component pursuit, which is one way of doing robust PCA. Again, robust PCA is this problem statement of decomposing a matrix into a sum of a low-rank matrix and a sparse matrix. This is an example of implementing an algorithm from a paper. It's okay, we're not going to get into all the math details of this. But I wanted to more show you some of the process of doing this. The original paper is this one called robust PCA. It's by collection of researchers from Stanford, UIUC, and Microsoft Research. Then there's a second paper that gives more details that's also used. I have a link to that lower down. I also, in encoding this, I looked at those, I looked at two existing implementations that I found online and their links here. A key thing to know is you don't need to know the math or understand the proofs to implement an algorithm from a paper. It can also be very distracting to try to read all the paper. Some of this depends on your purposes and what you're trying to get out of it. But I think if your goal is to implement it or write the code, I think it's good to try to get there quickly and you don't need to go through all the theorems. For example, let me go back to the Candice Lee Mawn write paper. Okay, here it is. This is 39 pages. I think the introduction is really nicely written on this, and explaining what robust PCA is. It also has some nice, and you can see I've based my notes on some of this. It gives several examples of applications. Number one is video surveillance. This is great to see. But then if you scroll through, you'll see a lot of this is theorems about the correctness of your results, which are going to be less relevant. So you actually have to go all the way down to. It's telling you the architecture of the proof before you even get to the proof. So section 2 is the architecture of the proof, section 3 is the proofs. Well, part of the proofs, and I think section 4 is also. No, section 4 is numerical experiments, which is good. Actually, this is nice that they do show some, using it on the surveillance videos here. Zoom back out. They also used it on faces. Although I think the results, this here they're removing shadows from the faces. So it's a lot subtler to look for. I think you can particularly see it. Like on this one, there's a pretty pronounced shadow here coming from the nose, and you can see that that's been removed. Coming over here. Although you've also lost the gleams in the eyes. But in terms of actually getting to the algorithm of this paper, it doesn't come till page 29. You'll find the algorithm. I'll go back to the notebook. I just wanted to show you that I think it can be intimidating the amount of proofs that come before the algorithm. But if the algorithm is what you're interested in, please skip to the algorithm. Here we are. Here on page 29, we have the principal component pursuit by alternating directions. The basic idea is that they will be taking, and they define this operator cursive D, which is the singular value thresholding. Actually, I should just show their definitions. Here it is. You need to know the definitions of S, which they call curly S, the shrinkage operator, and cursive D, the singular value thresholding operator. The shrinkage operator is basically, here it is. They take the singular values and subtract a value tau off of them. So you have this parameter tau, you take your singular values, subtract that off. If the values were less than tau, you just round them to zero. So you don't want to be flipping the sign on your values, but you just want to make everything that's more than tau away from zero, a little bit smaller. Then if it was within tau of zero, just round it to zero. You have this process of making the singular value smaller. Then for curly D, let me find it here. That's the singular value thresholding operator. Basically, that's taking the SVD and then making your singular value smaller, and then recomposing your matrix. So you're both decreasing the magnitude a little bit, and you're taking some of your singular values to zero is a huge part of this. Is that anything that was within tau of zero, you're just rounding to zero. So you're making your singular values more sparse, which I'll remember is a goal. So it's really very similar to just doing a truncated SVD and then modulating back up together again. That's a great point. Yeah. So Jeremy just said it's similar to doing a truncated SVD, in that you can think of if the columns that you were chopping off all had tau or less for their singular values, that's what you've done here. You've chopped off everything that was less than tau by setting it to zero. So this is really a lot like a truncated SVD. Thank you. Which makes sense because we just find that that creates a low-rank matrix. Yes. Yeah. Going back to this algorithm, the idea is the low-rank matrix you're creating is the singular value thresholding, where you've just chopped off a bunch of singular values because they were tau or less. The sparse matrix, here, they're just looking at the air that still left from your original matrix minus your low-rank, because remember, you want the sparse matrix to be equal to, or you want the sparse plus the low-rank matrices to equal your original matrix. So you'll notice there's this alternating that's happening between, for my low-rank matrix, that's approximating the original matrix minus the sparse one, and the sparse matrix is approximating the original matrix minus the low-rank one. Then this mu times yk is keeping track of what you still have left to approximate. So here, I really just want you to get the broad strokes out of this. I think this idea of the alternating back and forth between, okay, let's try to improve our estimate for the low-rank matrix, okay, let's improve our estimate for the sparse matrix, and going back and forth iteratively is a really nice pattern. Then this idea of the singular value thresholding being really similar to just truncating a bunch of our singular values. Then there's another paper that builds on that one. I don't know what happened. Let me go back down. There's another paper that builds on that one and gives additional detail of how to, well, has this is great, really takes advantage of actually calculating a truncated SVD. Even though we're doing this truncation by throwing away a bunch of our singular values, we don't want to have to do the full SVD first. The second paper gives us estimates of like, okay, you can just calculate this many singular values each time, although we'll still do the thresholding where we truncate. That was weird, I just tried to click on a link and I don't know why it took me back to the top. I'll just show the clips from it. Here in this second paper, our PCA stands for robust PCA. They're using this optimization technique, alternating Lagrange multipliers. Here section four in this second paper has some really helpful implementation details. In particular, they give you a formula of how many singular values to calculate each time. Basically, if it's small enough, you're just incrementing, otherwise, you're taking basically like 20 percent of your dimensionality, so doing the minimum of this SVP. You don't want to be calculating more than 20 percent of whatever the size of your matrix is. In section four, and this is great when papers have this, it gives values for the parameters to start with. Let's get down to the code. Here are the links I mentioned to Stephen Boyd's Open edX videos and his Jupyter Notebooks. I want to show, I guess in particular, it's definitely good to try to have little methods and try to keep things somewhat modular. I think one of the implementations I found online, it was all just in one giant method and really difficult to read. But having here calling it like a shrinkage method for the shrinkage operator that they've described. Remember, this is the one that just subtracts tau. If it was less than tau away from zero, you're setting it to zero and so that's what this is doing. Actually, I guess it contains that of subtracting off from the singular values and then taking an SVD, subtracting off from the singular values and then multiplying it back together to reconstruct. Yes. You'll notice here we're just calling underscore SVD. Here underscore SVD, we've set to use FBPCA, which is a Facebook library for randomized SVD. Let me pull it up. I should have saved timings for this, but basically we just tried this with several different SVD implementations to try to find what was quickest and this was fast, so we went with it. But you could still use Scikit-learns randomized SVD here if you wanted. This is also, I think, an interesting point in that the speed of your implementation with these things matters and that there are different options, but some of them, I at one point started working with this library called PyPropac. I made a pull request to update it to Python 3 and the author of the library was like, I no longer am supporting this. It was all based off of this library ProPac, which came out of someone's PhD thesis 10 or 15 years ago as this Fortran library. So you do sometimes do get into these implementation specifics even in figuring out which libraries to use. So Rachel is now the extra maintainer of PyPropac. Thanks for the introduction. Yes. He did merge my- Actually, he's a poor programmer. Yeah. He was like, if you want this library, you can take over maintaining it. But although he actually didn't switch it off to me. So FVPCA was fast. I'll just say personally, I do find it encouraging when you see that there's a big company supporting a library because that does make me feel like it'll hopefully be maintained. But that's what we're using here. It's a little bit confusing because they've called the method PCA, but it's an SVD. I find that a fair amount that there are people that almost use PCA and SVD somewhat interchangeably. They're slightly different. Yeah, I'm not going to get into right now unless you wanted to talk about it. I mean, they're basically the same almost. I think it's worth mentioning that PCA basically uses SVD. Yes. PCA uses SVD. PCA is typically you're multiplying the matrix by its transpose first. No, they are. Sorry. I did mention they both do that. The only difference as far as I'm aware is that PCA subtracts the main. That's right. PCA subtracts the main. When I want these, I want they with the same. They're almost the same. Okay. Thank you. That is something to be careful of with subtracting the mean. If you start off with a sparse matrix, subtracting the mean makes it no longer sparse, so you don't want to do that. Okay. Actually, let me see if I want to say anything else about this. This is the primary component pursuit method itself. Transposes. Yes. Here, trans just transposes the matrix because this is quicker. You want this matrix to be tall and skinny. If you're given the reverse, you need to transpose the matrix. Do this, it'll be quicker and then go back. Yes. I'll just say here, we've got this loop that we're going through. We're getting a new estimate of our sparse matrix using the shrinkage operator. Then we're getting our low-rank matrix by reconstructing the SVD. We're each time updating how many singular values we want to compute, seeing what our error is and adding that onto Y where we keep this total. Yeah, that's it. Next, I want to show the results that we get from this. Here, it's taking as inputs the maximum number of iterations you want to do as well as a hyperparameter. That hyperparameter, I actually added because I was trying this on different videos and found that it was needing different parameter values to converge. But you can see the error each time of how it's doing. These are the results. We've gotten back our low-rank matrix and our sparse matrix, and we can plot them. So the original, the sparse, and the low-rank. You will see there's still some blurs of people showing up that haven't been fully removed. I wanted to note that extracting a little bit of the foreground is easier than fully identifying the background. You'll notice the middle pictures, I think, look better than the pictures on the far right. That's because, I don't know, as long as you're close, it'll look like people even if you don't have every piece of the person to the right intensity. Whereas with the background, if any of the persons remaining, you do have this little smudge or ghost image. Any questions? On that note, I may stop here before we start since LU factorization is a big meaty topic that we'll get to next. You'll remember LU factorization we saw was used in the randomized SVD that we wrote in that middle for loop, and it's also used in Facebook. The Facebook PCA randomized SVD that we were using has it. So we're going to dig in next time to how LU factorization works. We'll just end, I think, five minutes early.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.64, "text": " Okay it's 11 of 5 I'm gonna get started. So we're finishing up topic modeling.", "tokens": [1033, 309, 311, 2975, 295, 1025, 286, 478, 799, 483, 1409, 13, 407, 321, 434, 12693, 493, 4829, 15983, 13], "temperature": 0.0, "avg_logprob": -0.4124905686629446, "compression_ratio": 1.1130434782608696, "no_speech_prob": 0.02159998007118702}, {"id": 1, "seek": 0, "start": 10.64, "end": 17.240000000000002, "text": " First just can someone remind us what the SVD is?", "tokens": [2386, 445, 393, 1580, 4160, 505, 437, 264, 31910, 35, 307, 30], "temperature": 0.0, "avg_logprob": -0.4124905686629446, "compression_ratio": 1.1130434782608696, "no_speech_prob": 0.02159998007118702}, {"id": 2, "seek": 1724, "start": 17.24, "end": 26.919999999999998, "text": " Brad? Oh wait hold on for the microphone.", "tokens": [11895, 30, 876, 1699, 1797, 322, 337, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.5294981684003558, "compression_ratio": 0.8367346938775511, "no_speech_prob": 0.0011142059229314327}, {"id": 3, "seek": 2692, "start": 26.92, "end": 53.36, "text": " Exactly yes. So singular value decomposition you get a matrix of", "tokens": [7587, 2086, 13, 407, 20010, 2158, 48356, 291, 483, 257, 8141, 295], "temperature": 0.0, "avg_logprob": -0.34807702898979187, "compression_ratio": 0.9411764705882353, "no_speech_prob": 5.142449663253501e-05}, {"id": 4, "seek": 5336, "start": 53.36, "end": 58.44, "text": " orthonormal columns and then in the center there's a diagonal matrix and as", "tokens": [420, 11943, 24440, 13766, 293, 550, 294, 264, 3056, 456, 311, 257, 21539, 8141, 293, 382], "temperature": 0.0, "avg_logprob": -0.11287677401588077, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0001487821718910709}, {"id": 5, "seek": 5336, "start": 58.44, "end": 63.12, "text": " Brad said it's got the singular values in descending order and then the third", "tokens": [11895, 848, 309, 311, 658, 264, 20010, 4190, 294, 40182, 1668, 293, 550, 264, 2636], "temperature": 0.0, "avg_logprob": -0.11287677401588077, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0001487821718910709}, {"id": 6, "seek": 5336, "start": 63.12, "end": 68.6, "text": " matrix U is orthonormal rows. And so we were talking at the end of last time", "tokens": [8141, 624, 307, 420, 11943, 24440, 13241, 13, 400, 370, 321, 645, 1417, 412, 264, 917, 295, 1036, 565], "temperature": 0.0, "avg_logprob": -0.11287677401588077, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0001487821718910709}, {"id": 7, "seek": 5336, "start": 68.6, "end": 75.16, "text": " about how we could use a randomized SVD to get the truncated form a lot faster.", "tokens": [466, 577, 321, 727, 764, 257, 38513, 31910, 35, 281, 483, 264, 504, 409, 66, 770, 1254, 257, 688, 4663, 13], "temperature": 0.0, "avg_logprob": -0.11287677401588077, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0001487821718910709}, {"id": 8, "seek": 5336, "start": 75.16, "end": 82.36, "text": " And so kind of the truncated so full SVD you're getting you know you're able to", "tokens": [400, 370, 733, 295, 264, 504, 409, 66, 770, 370, 1577, 31910, 35, 291, 434, 1242, 291, 458, 291, 434, 1075, 281], "temperature": 0.0, "avg_logprob": -0.11287677401588077, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0001487821718910709}, {"id": 9, "seek": 8236, "start": 82.36, "end": 87.52, "text": " fully reconstruct your original data matrix which might be useful but if", "tokens": [4498, 31499, 428, 3380, 1412, 8141, 597, 1062, 312, 4420, 457, 498], "temperature": 0.0, "avg_logprob": -0.11004723635586826, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.8923845345852897e-05}, {"id": 10, "seek": 8236, "start": 87.52, "end": 90.88, "text": " you're trying to do data compression or you just want to do something faster you", "tokens": [291, 434, 1382, 281, 360, 1412, 19355, 420, 291, 445, 528, 281, 360, 746, 4663, 291], "temperature": 0.0, "avg_logprob": -0.11004723635586826, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.8923845345852897e-05}, {"id": 11, "seek": 8236, "start": 90.88, "end": 93.92, "text": " probably don't need all of that and so it'd be quicker to not calculate the", "tokens": [1391, 500, 380, 643, 439, 295, 300, 293, 370, 309, 1116, 312, 16255, 281, 406, 8873, 264], "temperature": 0.0, "avg_logprob": -0.11004723635586826, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.8923845345852897e-05}, {"id": 12, "seek": 8236, "start": 93.92, "end": 100.44, "text": " whole thing. So I wanted to remind you that full SVD is slow so this is the", "tokens": [1379, 551, 13, 407, 286, 1415, 281, 4160, 291, 300, 1577, 31910, 35, 307, 2964, 370, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.11004723635586826, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.8923845345852897e-05}, {"id": 13, "seek": 8236, "start": 100.44, "end": 105.68, "text": " calculation back from the very beginning of this notebook. And again to remind you", "tokens": [17108, 646, 490, 264, 588, 2863, 295, 341, 21060, 13, 400, 797, 281, 4160, 291], "temperature": 0.0, "avg_logprob": -0.11004723635586826, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.8923845345852897e-05}, {"id": 14, "seek": 8236, "start": 105.68, "end": 110.08, "text": " vectors this is actually I'll ask you what is a vectors this is kind of the", "tokens": [18875, 341, 307, 767, 286, 603, 1029, 291, 437, 307, 257, 18875, 341, 307, 733, 295, 264], "temperature": 0.0, "avg_logprob": -0.11004723635586826, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.8923845345852897e-05}, {"id": 15, "seek": 11008, "start": 110.08, "end": 115.03999999999999, "text": " matrix we've been looking at for the past three days in notebook two. Does", "tokens": [8141, 321, 600, 668, 1237, 412, 337, 264, 1791, 1045, 1708, 294, 21060, 732, 13, 4402], "temperature": 0.0, "avg_logprob": -0.38817693922254776, "compression_ratio": 1.3306451612903225, "no_speech_prob": 7.645915502507705e-06}, {"id": 16, "seek": 11008, "start": 115.03999999999999, "end": 117.6, "text": " anyone remember", "tokens": [2878, 1604], "temperature": 0.0, "avg_logprob": -0.38817693922254776, "compression_ratio": 1.3306451612903225, "no_speech_prob": 7.645915502507705e-06}, {"id": 17, "seek": 11008, "start": 122.72, "end": 134.32, "text": " what data set we're looking at? Do it I see see your lips moving is that a", "tokens": [437, 1412, 992, 321, 434, 1237, 412, 30, 1144, 309, 286, 536, 536, 428, 10118, 2684, 307, 300, 257], "temperature": 0.0, "avg_logprob": -0.38817693922254776, "compression_ratio": 1.3306451612903225, "no_speech_prob": 7.645915502507705e-06}, {"id": 18, "seek": 13432, "start": 134.32, "end": 145.92, "text": " thanks Tim exactly yes so this is the newsgroup state data set so it's kind of", "tokens": [3231, 7172, 2293, 2086, 370, 341, 307, 264, 2583, 17377, 1785, 1412, 992, 370, 309, 311, 733, 295], "temperature": 0.0, "avg_logprob": -0.17470468057168498, "compression_ratio": 1.5888324873096447, "no_speech_prob": 8.885982242645696e-05}, {"id": 19, "seek": 13432, "start": 145.92, "end": 150.48, "text": " posts that people have written on these message boards and on the internet in", "tokens": [12300, 300, 561, 362, 3720, 322, 613, 3636, 13293, 293, 322, 264, 4705, 294], "temperature": 0.0, "avg_logprob": -0.17470468057168498, "compression_ratio": 1.5888324873096447, "no_speech_prob": 8.885982242645696e-05}, {"id": 20, "seek": 13432, "start": 150.48, "end": 154.72, "text": " different categories and our rows are the different words and the columns are", "tokens": [819, 10479, 293, 527, 13241, 366, 264, 819, 2283, 293, 264, 13766, 366], "temperature": 0.0, "avg_logprob": -0.17470468057168498, "compression_ratio": 1.5888324873096447, "no_speech_prob": 8.885982242645696e-05}, {"id": 21, "seek": 13432, "start": 154.72, "end": 162.0, "text": " the particular post. So when we did the SVD on it using SciPy's implementation", "tokens": [264, 1729, 2183, 13, 407, 562, 321, 630, 264, 31910, 35, 322, 309, 1228, 16942, 47, 88, 311, 11420], "temperature": 0.0, "avg_logprob": -0.17470468057168498, "compression_ratio": 1.5888324873096447, "no_speech_prob": 8.885982242645696e-05}, {"id": 22, "seek": 16200, "start": 162.0, "end": 171.0, "text": " this is for a full one it took 28 seconds which is pretty slow. You can see", "tokens": [341, 307, 337, 257, 1577, 472, 309, 1890, 7562, 3949, 597, 307, 1238, 2964, 13, 509, 393, 536], "temperature": 0.0, "avg_logprob": -0.14952360853856925, "compression_ratio": 1.5975103734439835, "no_speech_prob": 5.862403213541256e-06}, {"id": 23, "seek": 16200, "start": 171.0, "end": 176.24, "text": " that here and I think I mentioned this before but percent time is a magic or", "tokens": [300, 510, 293, 286, 519, 286, 2835, 341, 949, 457, 3043, 565, 307, 257, 5585, 420], "temperature": 0.0, "avg_logprob": -0.14952360853856925, "compression_ratio": 1.5975103734439835, "no_speech_prob": 5.862403213541256e-06}, {"id": 24, "seek": 16200, "start": 176.24, "end": 179.92000000000002, "text": " they're called magics in Jupyter notebooks but it's really useful to get", "tokens": [436, 434, 1219, 2258, 1167, 294, 22125, 88, 391, 43782, 457, 309, 311, 534, 4420, 281, 483], "temperature": 0.0, "avg_logprob": -0.14952360853856925, "compression_ratio": 1.5975103734439835, "no_speech_prob": 5.862403213541256e-06}, {"id": 25, "seek": 16200, "start": 179.92000000000002, "end": 185.36, "text": " the the time of something you're running. So let me check the shape and here we", "tokens": [264, 264, 565, 295, 746, 291, 434, 2614, 13, 407, 718, 385, 1520, 264, 3909, 293, 510, 321], "temperature": 0.0, "avg_logprob": -0.14952360853856925, "compression_ratio": 1.5975103734439835, "no_speech_prob": 5.862403213541256e-06}, {"id": 26, "seek": 16200, "start": 185.36, "end": 191.6, "text": " were getting U was 2000 by 2000 we had 2000 singular values and then V was 2000", "tokens": [645, 1242, 624, 390, 8132, 538, 8132, 321, 632, 8132, 20010, 4190, 293, 550, 691, 390, 8132], "temperature": 0.0, "avg_logprob": -0.14952360853856925, "compression_ratio": 1.5975103734439835, "no_speech_prob": 5.862403213541256e-06}, {"id": 27, "seek": 19160, "start": 191.6, "end": 196.76, "text": " by 26,000. However if we do the randomized SVD so suppose we were only", "tokens": [538, 7551, 11, 1360, 13, 2908, 498, 321, 360, 264, 38513, 31910, 35, 370, 7297, 321, 645, 787], "temperature": 0.0, "avg_logprob": -0.1433293024698893, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.0782760909933131e-05}, {"id": 28, "seek": 19160, "start": 196.76, "end": 203.79999999999998, "text": " interested in the top five topics we run that and and this is from scikit-learn", "tokens": [3102, 294, 264, 1192, 1732, 8378, 321, 1190, 300, 293, 293, 341, 307, 490, 2180, 22681, 12, 306, 1083], "temperature": 0.0, "avg_logprob": -0.1433293024698893, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.0782760909933131e-05}, {"id": 29, "seek": 19160, "start": 203.79999999999998, "end": 212.44, "text": " that only took 154 milliseconds so this is an order of magnitude faster. So", "tokens": [300, 787, 1890, 2119, 19, 34184, 370, 341, 307, 364, 1668, 295, 15668, 4663, 13, 407], "temperature": 0.0, "avg_logprob": -0.1433293024698893, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.0782760909933131e-05}, {"id": 30, "seek": 19160, "start": 212.44, "end": 215.84, "text": " that's kind of the first talk a little bit more about this but that's the idea", "tokens": [300, 311, 733, 295, 264, 700, 751, 257, 707, 857, 544, 466, 341, 457, 300, 311, 264, 1558], "temperature": 0.0, "avg_logprob": -0.1433293024698893, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.0782760909933131e-05}, {"id": 31, "seek": 19160, "start": 215.84, "end": 220.88, "text": " we're closing closing out today of doing this randomized version and it's a lot", "tokens": [321, 434, 10377, 10377, 484, 965, 295, 884, 341, 38513, 3037, 293, 309, 311, 257, 688], "temperature": 0.0, "avg_logprob": -0.1433293024698893, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.0782760909933131e-05}, {"id": 32, "seek": 22088, "start": 220.88, "end": 227.24, "text": " quicker. So I looked it up this was a question last time the runtime", "tokens": [16255, 13, 407, 286, 2956, 309, 493, 341, 390, 257, 1168, 1036, 565, 264, 34474], "temperature": 0.0, "avg_logprob": -0.10443420211474101, "compression_ratio": 1.5578512396694215, "no_speech_prob": 2.9308284865692258e-05}, {"id": 33, "seek": 22088, "start": 227.24, "end": 234.51999999999998, "text": " complexity for SVD is big O of the minimum of M squared N and MN squared.", "tokens": [14024, 337, 31910, 35, 307, 955, 422, 295, 264, 7285, 295, 376, 8889, 426, 293, 376, 45, 8889, 13], "temperature": 0.0, "avg_logprob": -0.10443420211474101, "compression_ratio": 1.5578512396694215, "no_speech_prob": 2.9308284865692258e-05}, {"id": 34, "seek": 22088, "start": 234.51999999999998, "end": 239.24, "text": " We'll be talking in a later week about how you would actually calculate the SVD.", "tokens": [492, 603, 312, 1417, 294, 257, 1780, 1243, 466, 577, 291, 576, 767, 8873, 264, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.10443420211474101, "compression_ratio": 1.5578512396694215, "no_speech_prob": 2.9308284865692258e-05}, {"id": 35, "seek": 22088, "start": 239.24, "end": 245.24, "text": " For now we're just going to use SciPy's implementation. We had this question of", "tokens": [1171, 586, 321, 434, 445, 516, 281, 764, 16942, 47, 88, 311, 11420, 13, 492, 632, 341, 1168, 295], "temperature": 0.0, "avg_logprob": -0.10443420211474101, "compression_ratio": 1.5578512396694215, "no_speech_prob": 2.9308284865692258e-05}, {"id": 36, "seek": 22088, "start": 245.24, "end": 249.16, "text": " how can we speed things up without coming up with a better implementation", "tokens": [577, 393, 321, 3073, 721, 493, 1553, 1348, 493, 365, 257, 1101, 11420], "temperature": 0.0, "avg_logprob": -0.10443420211474101, "compression_ratio": 1.5578512396694215, "no_speech_prob": 2.9308284865692258e-05}, {"id": 37, "seek": 24916, "start": 249.16, "end": 254.88, "text": " for finding the SVD and our idea was to just use a smaller matrix with smaller N", "tokens": [337, 5006, 264, 31910, 35, 293, 527, 1558, 390, 281, 445, 764, 257, 4356, 8141, 365, 4356, 426], "temperature": 0.0, "avg_logprob": -0.11038504090420036, "compression_ratio": 1.5841584158415842, "no_speech_prob": 1.8923743482446298e-05}, {"id": 38, "seek": 24916, "start": 254.88, "end": 260.8, "text": " and so and so this right now we're kind of explaining how the randomized SVD", "tokens": [293, 370, 293, 370, 341, 558, 586, 321, 434, 733, 295, 13468, 577, 264, 38513, 31910, 35], "temperature": 0.0, "avg_logprob": -0.11038504090420036, "compression_ratio": 1.5841584158415842, "no_speech_prob": 1.8923743482446298e-05}, {"id": 39, "seek": 24916, "start": 260.8, "end": 267.28, "text": " works. So instead of calculating the SVD on the full matrix A which was M by N we", "tokens": [1985, 13, 407, 2602, 295, 28258, 264, 31910, 35, 322, 264, 1577, 8141, 316, 597, 390, 376, 538, 426, 321], "temperature": 0.0, "avg_logprob": -0.11038504090420036, "compression_ratio": 1.5841584158415842, "no_speech_prob": 1.8923743482446298e-05}, {"id": 40, "seek": 24916, "start": 267.28, "end": 274.28, "text": " could just use B equals AQ and have that be M by R where R is a lot smaller than", "tokens": [727, 445, 764, 363, 6915, 316, 48, 293, 362, 300, 312, 376, 538, 497, 689, 497, 307, 257, 688, 4356, 813], "temperature": 0.0, "avg_logprob": -0.11038504090420036, "compression_ratio": 1.5841584158415842, "no_speech_prob": 1.8923743482446298e-05}, {"id": 41, "seek": 27428, "start": 274.28, "end": 280.32, "text": " N. So we're actually still using the same SciPy implementation of SVD we're", "tokens": [426, 13, 407, 321, 434, 767, 920, 1228, 264, 912, 16942, 47, 88, 11420, 295, 31910, 35, 321, 434], "temperature": 0.0, "avg_logprob": -0.10086038865541157, "compression_ratio": 1.530612244897959, "no_speech_prob": 6.6432353378331754e-06}, {"id": 42, "seek": 27428, "start": 280.32, "end": 290.08, "text": " just multiplying our matrix to make it smaller by a random matrix and that's a", "tokens": [445, 30955, 527, 8141, 281, 652, 309, 4356, 538, 257, 4974, 8141, 293, 300, 311, 257], "temperature": 0.0, "avg_logprob": -0.10086038865541157, "compression_ratio": 1.530612244897959, "no_speech_prob": 6.6432353378331754e-06}, {"id": 43, "seek": 27428, "start": 290.08, "end": 296.35999999999996, "text": " lot quicker and gives us just the information we need. Here's the", "tokens": [688, 16255, 293, 2709, 505, 445, 264, 1589, 321, 643, 13, 1692, 311, 264], "temperature": 0.0, "avg_logprob": -0.10086038865541157, "compression_ratio": 1.530612244897959, "no_speech_prob": 6.6432353378331754e-06}, {"id": 44, "seek": 27428, "start": 296.35999999999996, "end": 301.71999999999997, "text": " calculation again and we can see that the topics are really good. So again as a", "tokens": [17108, 797, 293, 321, 393, 536, 300, 264, 8378, 366, 534, 665, 13, 407, 797, 382, 257], "temperature": 0.0, "avg_logprob": -0.10086038865541157, "compression_ratio": 1.530612244897959, "no_speech_prob": 6.6432353378331754e-06}, {"id": 45, "seek": 30172, "start": 301.72, "end": 320.64000000000004, "text": " refresher. Oh Brad? Oh that's a that's a great question. So we want B to have the", "tokens": [17368, 511, 13, 876, 11895, 30, 876, 300, 311, 257, 300, 311, 257, 869, 1168, 13, 407, 321, 528, 363, 281, 362, 264], "temperature": 0.0, "avg_logprob": -0.2747390124262596, "compression_ratio": 1.3015873015873016, "no_speech_prob": 0.00014422716049011797}, {"id": 46, "seek": 30172, "start": 320.64000000000004, "end": 327.04, "text": " same column space as A is the hope. So yes that would be a low-rank approximation.", "tokens": [912, 7738, 1901, 382, 316, 307, 264, 1454, 13, 407, 2086, 300, 576, 312, 257, 2295, 12, 20479, 28023, 13], "temperature": 0.0, "avg_logprob": -0.2747390124262596, "compression_ratio": 1.3015873015873016, "no_speech_prob": 0.00014422716049011797}, {"id": 47, "seek": 32704, "start": 327.04, "end": 349.24, "text": " I'm not sure that sounds like it like I actually yeah that is that is the case", "tokens": [286, 478, 406, 988, 300, 3263, 411, 309, 411, 286, 767, 1338, 300, 307, 300, 307, 264, 1389], "temperature": 0.0, "avg_logprob": -0.17358225186665852, "compression_ratio": 1.52, "no_speech_prob": 0.00018519481818657368}, {"id": 48, "seek": 32704, "start": 349.24, "end": 353.56, "text": " that you do get it yeah best low-rank approximation. That's yeah that's not", "tokens": [300, 291, 360, 483, 309, 1338, 1151, 2295, 12, 20479, 28023, 13, 663, 311, 1338, 300, 311, 406], "temperature": 0.0, "avg_logprob": -0.17358225186665852, "compression_ratio": 1.52, "no_speech_prob": 0.00018519481818657368}, {"id": 49, "seek": 32704, "start": 353.56, "end": 356.48, "text": " what we're doing here that's a bit circular almost because in order to do", "tokens": [437, 321, 434, 884, 510, 300, 311, 257, 857, 16476, 1920, 570, 294, 1668, 281, 360], "temperature": 0.0, "avg_logprob": -0.17358225186665852, "compression_ratio": 1.52, "no_speech_prob": 0.00018519481818657368}, {"id": 50, "seek": 35648, "start": 356.48, "end": 360.96000000000004, "text": " that you have to get the full SVD to know that you're taking the the best", "tokens": [300, 291, 362, 281, 483, 264, 1577, 31910, 35, 281, 458, 300, 291, 434, 1940, 264, 264, 1151], "temperature": 0.0, "avg_logprob": -0.1528245770201391, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00010390136594651267}, {"id": 51, "seek": 35648, "start": 360.96000000000004, "end": 365.24, "text": " low-rank approximation. Here we want to get a low-rank approximation that's kind", "tokens": [2295, 12, 20479, 28023, 13, 1692, 321, 528, 281, 483, 257, 2295, 12, 20479, 28023, 300, 311, 733], "temperature": 0.0, "avg_logprob": -0.1528245770201391, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00010390136594651267}, {"id": 52, "seek": 35648, "start": 365.24, "end": 373.40000000000003, "text": " of good enough and then just do the SVD on that. And so well okay so a difference", "tokens": [295, 665, 1547, 293, 550, 445, 360, 264, 31910, 35, 322, 300, 13, 400, 370, 731, 1392, 370, 257, 2649], "temperature": 0.0, "avg_logprob": -0.1528245770201391, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00010390136594651267}, {"id": 53, "seek": 35648, "start": 375.40000000000003, "end": 380.0, "text": " so it's an approximation of the column space. Yeah so I guess this is another", "tokens": [370, 309, 311, 364, 28023, 295, 264, 7738, 1901, 13, 865, 370, 286, 2041, 341, 307, 1071], "temperature": 0.0, "avg_logprob": -0.1528245770201391, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00010390136594651267}, {"id": 54, "seek": 35648, "start": 380.0, "end": 384.72, "text": " key distinction is A and B have different dimensions because we've made", "tokens": [2141, 16844, 307, 316, 293, 363, 362, 819, 12819, 570, 321, 600, 1027], "temperature": 0.0, "avg_logprob": -0.1528245770201391, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00010390136594651267}, {"id": 55, "seek": 38472, "start": 384.72, "end": 391.08000000000004, "text": " so if A is like our giant matrix B is tall and thin so you know kind of cut", "tokens": [370, 498, 316, 307, 411, 527, 7410, 8141, 363, 307, 6764, 293, 5862, 370, 291, 458, 733, 295, 1723], "temperature": 0.0, "avg_logprob": -0.12889541067728183, "compression_ratio": 1.6476683937823835, "no_speech_prob": 8.800624527793843e-06}, {"id": 56, "seek": 38472, "start": 391.08000000000004, "end": 394.68, "text": " off all these columns. So yeah this is an excellent point so B is not actually", "tokens": [766, 439, 613, 13766, 13, 407, 1338, 341, 307, 364, 7103, 935, 370, 363, 307, 406, 767], "temperature": 0.0, "avg_logprob": -0.12889541067728183, "compression_ratio": 1.6476683937823835, "no_speech_prob": 8.800624527793843e-06}, {"id": 57, "seek": 38472, "start": 394.68, "end": 402.3, "text": " approximating A but the the hope is that the columns of A span a similar space to", "tokens": [8542, 990, 316, 457, 264, 264, 1454, 307, 300, 264, 13766, 295, 316, 16174, 257, 2531, 1901, 281], "temperature": 0.0, "avg_logprob": -0.12889541067728183, "compression_ratio": 1.6476683937823835, "no_speech_prob": 8.800624527793843e-06}, {"id": 58, "seek": 38472, "start": 402.3, "end": 408.32000000000005, "text": " the columns of B. So we're kind of using fewer columns to hopefully span the same", "tokens": [264, 13766, 295, 363, 13, 407, 321, 434, 733, 295, 1228, 13366, 13766, 281, 4696, 16174, 264, 912], "temperature": 0.0, "avg_logprob": -0.12889541067728183, "compression_ratio": 1.6476683937823835, "no_speech_prob": 8.800624527793843e-06}, {"id": 59, "seek": 40832, "start": 408.32, "end": 421.88, "text": " space. Yeah thank you. And this so and I should also say I've kind of added so", "tokens": [1901, 13, 865, 1309, 291, 13, 400, 341, 370, 293, 286, 820, 611, 584, 286, 600, 733, 295, 3869, 370], "temperature": 0.0, "avg_logprob": -0.12983411318295962, "compression_ratio": 1.4786729857819905, "no_speech_prob": 1.1477890438982286e-05}, {"id": 60, "seek": 40832, "start": 421.88, "end": 425.4, "text": " this is I tried a different perspective for talking about the randomized SVD", "tokens": [341, 307, 286, 3031, 257, 819, 4585, 337, 1417, 466, 264, 38513, 31910, 35], "temperature": 0.0, "avg_logprob": -0.12983411318295962, "compression_ratio": 1.4786729857819905, "no_speech_prob": 1.1477890438982286e-05}, {"id": 61, "seek": 40832, "start": 425.4, "end": 429.24, "text": " from the end of class last time so there's some new material in this notebook", "tokens": [490, 264, 917, 295, 1508, 1036, 565, 370, 456, 311, 512, 777, 2527, 294, 341, 21060], "temperature": 0.0, "avg_logprob": -0.12983411318295962, "compression_ratio": 1.4786729857819905, "no_speech_prob": 1.1477890438982286e-05}, {"id": 62, "seek": 40832, "start": 429.24, "end": 433.48, "text": " that I am updated on github yesterday but if you have the version from Tuesday", "tokens": [300, 286, 669, 10588, 322, 290, 355, 836, 5186, 457, 498, 291, 362, 264, 3037, 490, 10017], "temperature": 0.0, "avg_logprob": -0.12983411318295962, "compression_ratio": 1.4786729857819905, "no_speech_prob": 1.1477890438982286e-05}, {"id": 63, "seek": 43348, "start": 433.48, "end": 438.92, "text": " some of this is new but I just kind of really wanted to emphasize the speed and", "tokens": [512, 295, 341, 307, 777, 457, 286, 445, 733, 295, 534, 1415, 281, 16078, 264, 3073, 293], "temperature": 0.0, "avg_logprob": -0.09441261706144913, "compression_ratio": 1.6309012875536482, "no_speech_prob": 4.53928041679319e-05}, {"id": 64, "seek": 43348, "start": 438.92, "end": 443.16, "text": " the fact that we haven't found a new way of finding the SVD yet we're just kind", "tokens": [264, 1186, 300, 321, 2378, 380, 1352, 257, 777, 636, 295, 5006, 264, 31910, 35, 1939, 321, 434, 445, 733], "temperature": 0.0, "avg_logprob": -0.09441261706144913, "compression_ratio": 1.6309012875536482, "no_speech_prob": 4.53928041679319e-05}, {"id": 65, "seek": 43348, "start": 443.16, "end": 448.52000000000004, "text": " of taking a smaller matrix to speed things up. And then this is from the", "tokens": [295, 1940, 257, 4356, 8141, 281, 3073, 721, 493, 13, 400, 550, 341, 307, 490, 264], "temperature": 0.0, "avg_logprob": -0.09441261706144913, "compression_ratio": 1.6309012875536482, "no_speech_prob": 4.53928041679319e-05}, {"id": 66, "seek": 43348, "start": 448.52000000000004, "end": 453.16, "text": " Facebook research blog post that I keep referencing that had the nice colorful", "tokens": [4384, 2132, 6968, 2183, 300, 286, 1066, 40582, 300, 632, 264, 1481, 18506], "temperature": 0.0, "avg_logprob": -0.09441261706144913, "compression_ratio": 1.6309012875536482, "no_speech_prob": 4.53928041679319e-05}, {"id": 67, "seek": 43348, "start": 453.16, "end": 458.04, "text": " picture and they ran some a few cases that I wanted to highlight for", "tokens": [3036, 293, 436, 5872, 512, 257, 1326, 3331, 300, 286, 1415, 281, 5078, 337], "temperature": 0.0, "avg_logprob": -0.09441261706144913, "compression_ratio": 1.6309012875536482, "no_speech_prob": 4.53928041679319e-05}, {"id": 68, "seek": 45804, "start": 458.04, "end": 465.08000000000004, "text": " benchmarks but they looked at large dense matrices that were up to a", "tokens": [43751, 457, 436, 2956, 412, 2416, 18011, 32284, 300, 645, 493, 281, 257], "temperature": 0.0, "avg_logprob": -0.15028417521509632, "compression_ratio": 1.7971698113207548, "no_speech_prob": 3.2696803828002885e-05}, {"id": 69, "seek": 45804, "start": 465.08000000000004, "end": 471.76000000000005, "text": " hundred thousand by two hundred thousand in size. Large sparse matrices up to ten", "tokens": [3262, 4714, 538, 732, 3262, 4714, 294, 2744, 13, 33092, 637, 11668, 32284, 493, 281, 2064], "temperature": 0.0, "avg_logprob": -0.15028417521509632, "compression_ratio": 1.7971698113207548, "no_speech_prob": 3.2696803828002885e-05}, {"id": 70, "seek": 45804, "start": 471.76000000000005, "end": 475.88, "text": " to the seventh by ten to the seventh in size and those had ten to the ninth non", "tokens": [281, 264, 17875, 538, 2064, 281, 264, 17875, 294, 2744, 293, 729, 632, 2064, 281, 264, 28207, 2107], "temperature": 0.0, "avg_logprob": -0.15028417521509632, "compression_ratio": 1.7971698113207548, "no_speech_prob": 3.2696803828002885e-05}, {"id": 71, "seek": 45804, "start": 475.88, "end": 481.68, "text": " zero elements and then they also looked at Apache's spark distributed SVD", "tokens": [4018, 4959, 293, 550, 436, 611, 2956, 412, 46597, 311, 9908, 12631, 31910, 35], "temperature": 0.0, "avg_logprob": -0.15028417521509632, "compression_ratio": 1.7971698113207548, "no_speech_prob": 3.2696803828002885e-05}, {"id": 72, "seek": 45804, "start": 481.68, "end": 487.28000000000003, "text": " implementation and they just wanted rank ten approximations. So for rank ten", "tokens": [11420, 293, 436, 445, 1415, 6181, 2064, 8542, 763, 13, 407, 337, 6181, 2064], "temperature": 0.0, "avg_logprob": -0.15028417521509632, "compression_ratio": 1.7971698113207548, "no_speech_prob": 3.2696803828002885e-05}, {"id": 73, "seek": 48728, "start": 487.28, "end": 493.15999999999997, "text": " calculating out a matrix that is ten million by ten million is really going", "tokens": [28258, 484, 257, 8141, 300, 307, 2064, 2459, 538, 2064, 2459, 307, 534, 516], "temperature": 0.0, "avg_logprob": -0.13099210078899676, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.977071017085109e-05}, {"id": 74, "seek": 48728, "start": 493.15999999999997, "end": 499.88, "text": " to be excessive and so you can see and so RPAC is kind of state-of-the-art like", "tokens": [281, 312, 22704, 293, 370, 291, 393, 536, 293, 370, 497, 47, 4378, 307, 733, 295, 1785, 12, 2670, 12, 3322, 12, 446, 411], "temperature": 0.0, "avg_logprob": -0.13099210078899676, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.977071017085109e-05}, {"id": 75, "seek": 48728, "start": 499.88, "end": 507.2, "text": " it's a matrix factorization package that is widely used so this is not saying", "tokens": [309, 311, 257, 8141, 5952, 2144, 7372, 300, 307, 13371, 1143, 370, 341, 307, 406, 1566], "temperature": 0.0, "avg_logprob": -0.13099210078899676, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.977071017085109e-05}, {"id": 76, "seek": 48728, "start": 507.2, "end": 510.79999999999995, "text": " that kind of RPAC is bad it's just kind of like how slow it is to fully do these", "tokens": [300, 733, 295, 497, 47, 4378, 307, 1578, 309, 311, 445, 733, 295, 411, 577, 2964, 309, 307, 281, 4498, 360, 613], "temperature": 0.0, "avg_logprob": -0.13099210078899676, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.977071017085109e-05}, {"id": 77, "seek": 48728, "start": 510.79999999999995, "end": 516.48, "text": " things. They found that it was one second to do the randomized version versus a", "tokens": [721, 13, 814, 1352, 300, 309, 390, 472, 1150, 281, 360, 264, 38513, 3037, 5717, 257], "temperature": 0.0, "avg_logprob": -0.13099210078899676, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.977071017085109e-05}, {"id": 78, "seek": 51648, "start": 516.48, "end": 521.48, "text": " hundred seconds for ten to the sixth by ten to the fifth matrices and then it was", "tokens": [3262, 3949, 337, 2064, 281, 264, 15102, 538, 2064, 281, 264, 9266, 32284, 293, 550, 309, 390], "temperature": 0.0, "avg_logprob": -0.1200860088521784, "compression_ratio": 1.7201834862385321, "no_speech_prob": 1.4284771168604493e-05}, {"id": 79, "seek": 51648, "start": 521.48, "end": 530.24, "text": " five seconds versus 63 minutes for ten to this oh for the this is like a denser", "tokens": [1732, 3949, 5717, 25082, 2077, 337, 2064, 281, 341, 1954, 337, 264, 341, 307, 411, 257, 24505, 260], "temperature": 0.0, "avg_logprob": -0.1200860088521784, "compression_ratio": 1.7201834862385321, "no_speech_prob": 1.4284771168604493e-05}, {"id": 80, "seek": 51648, "start": 530.24, "end": 534.72, "text": " version. So yeah I just wanted to show those numbers are a really big", "tokens": [3037, 13, 407, 1338, 286, 445, 1415, 281, 855, 729, 3547, 366, 257, 534, 955], "temperature": 0.0, "avg_logprob": -0.1200860088521784, "compression_ratio": 1.7201834862385321, "no_speech_prob": 1.4284771168604493e-05}, {"id": 81, "seek": 51648, "start": 534.72, "end": 539.32, "text": " improvement because you yeah five seconds versus 63 minutes is huge and", "tokens": [10444, 570, 291, 1338, 1732, 3949, 5717, 25082, 2077, 307, 2603, 293], "temperature": 0.0, "avg_logprob": -0.1200860088521784, "compression_ratio": 1.7201834862385321, "no_speech_prob": 1.4284771168604493e-05}, {"id": 82, "seek": 51648, "start": 539.32, "end": 543.48, "text": " that's also I think it's a really compelling detail when people include", "tokens": [300, 311, 611, 286, 519, 309, 311, 257, 534, 20050, 2607, 562, 561, 4090], "temperature": 0.0, "avg_logprob": -0.1200860088521784, "compression_ratio": 1.7201834862385321, "no_speech_prob": 1.4284771168604493e-05}, {"id": 83, "seek": 54348, "start": 543.48, "end": 551.52, "text": " things like that in blog post. I should say in preparing that in preparing the", "tokens": [721, 411, 300, 294, 6968, 2183, 13, 286, 820, 584, 294, 10075, 300, 294, 10075, 264], "temperature": 0.0, "avg_logprob": -0.11347586385319743, "compression_ratio": 1.812206572769953, "no_speech_prob": 9.972578482120298e-06}, {"id": 84, "seek": 54348, "start": 551.52, "end": 556.5600000000001, "text": " next lesson I had found a blog post that had this like threaded implementation", "tokens": [958, 6898, 286, 632, 1352, 257, 6968, 2183, 300, 632, 341, 411, 47493, 11420], "temperature": 0.0, "avg_logprob": -0.11347586385319743, "compression_ratio": 1.812206572769953, "no_speech_prob": 9.972578482120298e-06}, {"id": 85, "seek": 54348, "start": 556.5600000000001, "end": 560.16, "text": " to like run something in parallel and it was just like oh this should be faster", "tokens": [281, 411, 1190, 746, 294, 8952, 293, 309, 390, 445, 411, 1954, 341, 820, 312, 4663], "temperature": 0.0, "avg_logprob": -0.11347586385319743, "compression_ratio": 1.812206572769953, "no_speech_prob": 9.972578482120298e-06}, {"id": 86, "seek": 54348, "start": 560.16, "end": 564.84, "text": " but they didn't have any like actual times and so I used their code and ran", "tokens": [457, 436, 994, 380, 362, 604, 411, 3539, 1413, 293, 370, 286, 1143, 641, 3089, 293, 5872], "temperature": 0.0, "avg_logprob": -0.11347586385319743, "compression_ratio": 1.812206572769953, "no_speech_prob": 9.972578482120298e-06}, {"id": 87, "seek": 54348, "start": 564.84, "end": 568.88, "text": " it and it was actually slower with the parallel processing and so that's", "tokens": [309, 293, 309, 390, 767, 14009, 365, 264, 8952, 9007, 293, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.11347586385319743, "compression_ratio": 1.812206572769953, "no_speech_prob": 9.972578482120298e-06}, {"id": 88, "seek": 56888, "start": 568.88, "end": 573.4, "text": " something it's very valuable to definitely run it and tell people the times.", "tokens": [746, 309, 311, 588, 8263, 281, 2138, 1190, 309, 293, 980, 561, 264, 1413, 13], "temperature": 0.0, "avg_logprob": -0.14816869434557464, "compression_ratio": 1.6391304347826088, "no_speech_prob": 9.079911251319572e-06}, {"id": 89, "seek": 56888, "start": 573.4, "end": 577.08, "text": " Yeah so that's that's impressive and then we're not going to get into too much", "tokens": [865, 370, 300, 311, 300, 311, 8992, 293, 550, 321, 434, 406, 516, 281, 483, 666, 886, 709], "temperature": 0.0, "avg_logprob": -0.14816869434557464, "compression_ratio": 1.6391304347826088, "no_speech_prob": 9.079911251319572e-06}, {"id": 90, "seek": 56888, "start": 577.08, "end": 584.08, "text": " detail in this but kind of the idea that should help justify why this why this is", "tokens": [2607, 294, 341, 457, 733, 295, 264, 1558, 300, 820, 854, 20833, 983, 341, 983, 341, 307], "temperature": 0.0, "avg_logprob": -0.14816869434557464, "compression_ratio": 1.6391304347826088, "no_speech_prob": 9.079911251319572e-06}, {"id": 91, "seek": 56888, "start": 584.08, "end": 590.28, "text": " okay is the Johnson-Lindenstrasse lemma and that is that states that a", "tokens": [1392, 307, 264, 9779, 12, 43, 10291, 372, 3906, 405, 7495, 1696, 293, 300, 307, 300, 4368, 300, 257], "temperature": 0.0, "avg_logprob": -0.14816869434557464, "compression_ratio": 1.6391304347826088, "no_speech_prob": 9.079911251319572e-06}, {"id": 92, "seek": 56888, "start": 590.28, "end": 594.64, "text": " small set of points in a high dimensional space can be embedded in a", "tokens": [1359, 992, 295, 2793, 294, 257, 1090, 18795, 1901, 393, 312, 16741, 294, 257], "temperature": 0.0, "avg_logprob": -0.14816869434557464, "compression_ratio": 1.6391304347826088, "no_speech_prob": 9.079911251319572e-06}, {"id": 93, "seek": 59464, "start": 594.64, "end": 599.96, "text": " space of a much lower dimension in such a way that distances between the points", "tokens": [1901, 295, 257, 709, 3126, 10139, 294, 1270, 257, 636, 300, 22182, 1296, 264, 2793], "temperature": 0.0, "avg_logprob": -0.09925430122463183, "compression_ratio": 1.838095238095238, "no_speech_prob": 2.2821810489404015e-05}, {"id": 94, "seek": 59464, "start": 599.96, "end": 605.08, "text": " are nearly preserved and that idea of distances between points being", "tokens": [366, 6217, 22242, 293, 300, 1558, 295, 22182, 1296, 2793, 885], "temperature": 0.0, "avg_logprob": -0.09925430122463183, "compression_ratio": 1.838095238095238, "no_speech_prob": 2.2821810489404015e-05}, {"id": 95, "seek": 59464, "start": 605.08, "end": 609.72, "text": " preserved is really about preserving the structure kind of if you have a data set", "tokens": [22242, 307, 534, 466, 33173, 264, 3877, 733, 295, 498, 291, 362, 257, 1412, 992], "temperature": 0.0, "avg_logprob": -0.09925430122463183, "compression_ratio": 1.838095238095238, "no_speech_prob": 2.2821810489404015e-05}, {"id": 96, "seek": 59464, "start": 609.72, "end": 614.16, "text": " that's in a ton of dimensions you should be able to put it into fewer dimensions", "tokens": [300, 311, 294, 257, 2952, 295, 12819, 291, 820, 312, 1075, 281, 829, 309, 666, 13366, 12819], "temperature": 0.0, "avg_logprob": -0.09925430122463183, "compression_ratio": 1.838095238095238, "no_speech_prob": 2.2821810489404015e-05}, {"id": 97, "seek": 59464, "start": 614.16, "end": 618.16, "text": " and kind of keep the structure and that's what we're doing here with A and", "tokens": [293, 733, 295, 1066, 264, 3877, 293, 300, 311, 437, 321, 434, 884, 510, 365, 316, 293], "temperature": 0.0, "avg_logprob": -0.09925430122463183, "compression_ratio": 1.838095238095238, "no_speech_prob": 2.2821810489404015e-05}, {"id": 98, "seek": 61816, "start": 618.16, "end": 625.16, "text": " B in a sense of Tim.", "tokens": [363, 294, 257, 2020, 295, 7172, 13], "temperature": 0.0, "avg_logprob": -0.38817427588290854, "compression_ratio": 1.513157894736842, "no_speech_prob": 6.202546501299366e-05}, {"id": 99, "seek": 61816, "start": 625.16, "end": 630.16, "text": " So when you say embedding do you mean like a projection into it? Like what do you mean by embedded into a space?", "tokens": [407, 562, 291, 584, 12240, 3584, 360, 291, 914, 411, 257, 22743, 666, 309, 30, 1743, 437, 360, 291, 914, 538, 16741, 666, 257, 1901, 30], "temperature": 0.0, "avg_logprob": -0.38817427588290854, "compression_ratio": 1.513157894736842, "no_speech_prob": 6.202546501299366e-05}, {"id": 100, "seek": 61816, "start": 630.16, "end": 637.16, "text": " Um yeah yeah projection is a good way to think about it yeah protecting it. Oh another question.", "tokens": [3301, 1338, 1338, 22743, 307, 257, 665, 636, 281, 519, 466, 309, 1338, 12316, 309, 13, 876, 1071, 1168, 13], "temperature": 0.0, "avg_logprob": -0.38817427588290854, "compression_ratio": 1.513157894736842, "no_speech_prob": 6.202546501299366e-05}, {"id": 101, "seek": 63716, "start": 637.16, "end": 656.16, "text": " So when you're talking about approximating the column space is this in the way that we're talking about approximating the column space where it preserves these relationships between these higher dimensional points?", "tokens": [407, 562, 291, 434, 1417, 466, 8542, 990, 264, 7738, 1901, 307, 341, 294, 264, 636, 300, 321, 434, 1417, 466, 8542, 990, 264, 7738, 1901, 689, 309, 1183, 9054, 613, 6159, 1296, 613, 2946, 18795, 2793, 30], "temperature": 0.0, "avg_logprob": -0.13692015693301246, "compression_ratio": 1.671875, "no_speech_prob": 0.0001464656670577824}, {"id": 102, "seek": 65616, "start": 656.16, "end": 679.16, "text": " Yeah so with the with the column space going from from A to B so A is the large one actually let me just draw that so kind of A is this size B is the same height but thinner.", "tokens": [865, 370, 365, 264, 365, 264, 7738, 1901, 516, 490, 490, 316, 281, 363, 370, 316, 307, 264, 2416, 472, 767, 718, 385, 445, 2642, 300, 370, 733, 295, 316, 307, 341, 2744, 363, 307, 264, 912, 6681, 457, 21905, 13], "temperature": 0.0, "avg_logprob": -0.20147613949245877, "compression_ratio": 1.4146341463414633, "no_speech_prob": 1.8341506802244112e-05}, {"id": 103, "seek": 67916, "start": 679.16, "end": 692.16, "text": " So here you can think of the data points as the rows and so the idea is kind of you've gone from if this has N columns and this has R columns.", "tokens": [407, 510, 291, 393, 519, 295, 264, 1412, 2793, 382, 264, 13241, 293, 370, 264, 1558, 307, 733, 295, 291, 600, 2780, 490, 498, 341, 575, 426, 13766, 293, 341, 575, 497, 13766, 13], "temperature": 0.0, "avg_logprob": -0.11403432099715523, "compression_ratio": 1.8537735849056605, "no_speech_prob": 1.862897079263348e-05}, {"id": 104, "seek": 67916, "start": 692.16, "end": 707.16, "text": " And then they're both M tall you have M data points and that those M data points are still have the same structure between them or the same distance between them whether you're looking at A where it's N dimensional or B where it's just R dimensional.", "tokens": [400, 550, 436, 434, 1293, 376, 6764, 291, 362, 376, 1412, 2793, 293, 300, 729, 376, 1412, 2793, 366, 920, 362, 264, 912, 3877, 1296, 552, 420, 264, 912, 4560, 1296, 552, 1968, 291, 434, 1237, 412, 316, 689, 309, 311, 426, 18795, 420, 363, 689, 309, 311, 445, 497, 18795, 13], "temperature": 0.0, "avg_logprob": -0.11403432099715523, "compression_ratio": 1.8537735849056605, "no_speech_prob": 1.862897079263348e-05}, {"id": 105, "seek": 70716, "start": 707.16, "end": 724.16, "text": " Right so the approximation is in the relationship between the rows. Yes yes. Okay great thank you.", "tokens": [1779, 370, 264, 28023, 307, 294, 264, 2480, 1296, 264, 13241, 13, 1079, 2086, 13, 1033, 869, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.3659331798553467, "compression_ratio": 1.1529411764705881, "no_speech_prob": 1.1544273093022639e-06}, {"id": 106, "seek": 72416, "start": 724.16, "end": 737.16, "text": " Okay any other questions and we're not going to we're definitely not going to prove the Johnson Lindenstrasse lemma in here or even if some of the details I think last time.", "tokens": [1033, 604, 661, 1651, 293, 321, 434, 406, 516, 281, 321, 434, 2138, 406, 516, 281, 7081, 264, 9779, 441, 10291, 372, 3906, 405, 7495, 1696, 294, 510, 420, 754, 498, 512, 295, 264, 4365, 286, 519, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.11828620987709122, "compression_ratio": 1.6734693877551021, "no_speech_prob": 4.6376680984394625e-06}, {"id": 107, "seek": 72416, "start": 737.16, "end": 753.16, "text": " It's okay if you don't have all the steps from last time of kind of this process of how we did the randomized SVD. I mostly want you to kind of get this general intuition of why it might be okay that we're taking this random projection.", "tokens": [467, 311, 1392, 498, 291, 500, 380, 362, 439, 264, 4439, 490, 1036, 565, 295, 733, 295, 341, 1399, 295, 577, 321, 630, 264, 38513, 31910, 35, 13, 286, 5240, 528, 291, 281, 733, 295, 483, 341, 2674, 24002, 295, 983, 309, 1062, 312, 1392, 300, 321, 434, 1940, 341, 4974, 22743, 13], "temperature": 0.0, "avg_logprob": -0.11828620987709122, "compression_ratio": 1.6734693877551021, "no_speech_prob": 4.6376680984394625e-06}, {"id": 108, "seek": 75316, "start": 753.16, "end": 759.16, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1783181769507272, "compression_ratio": 1.3636363636363635, "no_speech_prob": 8.397458259423729e-06}, {"id": 109, "seek": 75316, "start": 759.16, "end": 769.16, "text": " Yeah so going back. So we looked at this code last time that I want to go through it again briefly. So to kind of implement the randomized SVD.", "tokens": [865, 370, 516, 646, 13, 407, 321, 2956, 412, 341, 3089, 1036, 565, 300, 286, 528, 281, 352, 807, 309, 797, 10515, 13, 407, 281, 733, 295, 4445, 264, 38513, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.1783181769507272, "compression_ratio": 1.3636363636363635, "no_speech_prob": 8.397458259423729e-06}, {"id": 110, "seek": 75316, "start": 769.16, "end": 780.16, "text": " First we created a randomized range finder. And here this is", "tokens": [2386, 321, 2942, 257, 38513, 3613, 915, 260, 13, 400, 510, 341, 307], "temperature": 0.0, "avg_logprob": -0.1783181769507272, "compression_ratio": 1.3636363636363635, "no_speech_prob": 8.397458259423729e-06}, {"id": 111, "seek": 78016, "start": 780.16, "end": 788.16, "text": " this is kind of getting our mapping of", "tokens": [341, 307, 733, 295, 1242, 527, 18350, 295], "temperature": 0.0, "avg_logprob": -0.08913430980607576, "compression_ratio": 1.6865671641791045, "no_speech_prob": 4.832281047129072e-05}, {"id": 112, "seek": 78016, "start": 788.16, "end": 800.16, "text": " kind of well getting the random the random matrix that we are going to multiply our original matrix by and to do that we randomly generate a matrix and then you could ignore this for now.", "tokens": [733, 295, 731, 1242, 264, 4974, 264, 4974, 8141, 300, 321, 366, 516, 281, 12972, 527, 3380, 8141, 538, 293, 281, 360, 300, 321, 16979, 8460, 257, 8141, 293, 550, 291, 727, 11200, 341, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.08913430980607576, "compression_ratio": 1.6865671641791045, "no_speech_prob": 4.832281047129072e-05}, {"id": 113, "seek": 80016, "start": 800.16, "end": 818.16, "text": " We take a QR factorization on a and what that does it actually should check does anyone remember what the QR factorization gives you so you get two matrices back.", "tokens": [492, 747, 257, 32784, 5952, 2144, 322, 257, 293, 437, 300, 775, 309, 767, 820, 1520, 775, 2878, 1604, 437, 264, 32784, 5952, 2144, 2709, 291, 370, 291, 483, 732, 32284, 646, 13], "temperature": 0.0, "avg_logprob": -0.18170078380687818, "compression_ratio": 1.4086956521739131, "no_speech_prob": 3.535300129442476e-05}, {"id": 114, "seek": 81816, "start": 818.16, "end": 845.16, "text": " Q is orthogonal. Yes exactly. Yeah so you get back a orthogonal matrix in an upper triangular and that orthogonal matrix is the one we'll use for our random or sorry we're taking so we're taking randomly generated matrix Q multiply that by a and then take the QR on that.", "tokens": [1249, 307, 41488, 13, 1079, 2293, 13, 865, 370, 291, 483, 646, 257, 41488, 8141, 294, 364, 6597, 38190, 293, 300, 41488, 8141, 307, 264, 472, 321, 603, 764, 337, 527, 4974, 420, 2597, 321, 434, 1940, 370, 321, 434, 1940, 16979, 10833, 8141, 1249, 12972, 300, 538, 257, 293, 550, 747, 264, 32784, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.21413493547283236, "compression_ratio": 1.6832298136645962, "no_speech_prob": 6.814450171077624e-05}, {"id": 115, "seek": 84516, "start": 845.16, "end": 857.16, "text": " And then we call that from randomized SVD. Does anyone remember so number of components is the kind of number of singular values we want or the ultimate number of topics for our topic modeling.", "tokens": [400, 550, 321, 818, 300, 490, 38513, 31910, 35, 13, 4402, 2878, 1604, 370, 1230, 295, 6677, 307, 264, 733, 295, 1230, 295, 20010, 4190, 321, 528, 420, 264, 9705, 1230, 295, 8378, 337, 527, 4829, 15983, 13], "temperature": 0.0, "avg_logprob": -0.09364364697383, "compression_ratio": 1.506578947368421, "no_speech_prob": 8.800442628853489e-06}, {"id": 116, "seek": 84516, "start": 857.16, "end": 871.16, "text": " What is the number of over samples.", "tokens": [708, 307, 264, 1230, 295, 670, 10938, 13], "temperature": 0.0, "avg_logprob": -0.09364364697383, "compression_ratio": 1.506578947368421, "no_speech_prob": 8.800442628853489e-06}, {"id": 117, "seek": 87116, "start": 871.16, "end": 876.16, "text": " Anyone.", "tokens": [14643, 13], "temperature": 0.0, "avg_logprob": -0.12924524545669555, "compression_ratio": 1.5853658536585367, "no_speech_prob": 1.0451046364323702e-05}, {"id": 118, "seek": 87116, "start": 876.16, "end": 895.16, "text": " Okay this is the this is the buffer that we kind of added on. So we're saying you know if I want five topics it's actually safer. We don't want to just compute like if we make be just have five columns that's actually cutting it a bit close that we're really going to get the five best singular values of a from that.", "tokens": [1033, 341, 307, 264, 341, 307, 264, 21762, 300, 321, 733, 295, 3869, 322, 13, 407, 321, 434, 1566, 291, 458, 498, 286, 528, 1732, 8378, 309, 311, 767, 15856, 13, 492, 500, 380, 528, 281, 445, 14722, 411, 498, 321, 652, 312, 445, 362, 1732, 13766, 300, 311, 767, 6492, 309, 257, 857, 1998, 300, 321, 434, 534, 516, 281, 483, 264, 1732, 1151, 20010, 4190, 295, 257, 490, 300, 13], "temperature": 0.0, "avg_logprob": -0.12924524545669555, "compression_ratio": 1.5853658536585367, "no_speech_prob": 1.0451046364323702e-05}, {"id": 119, "seek": 89516, "start": 895.16, "end": 904.16, "text": " And so what we'll do is we'll add some more on and be like okay let's actually call calculate 15 columns and 15 singular values and then we'll take the top five out of that.", "tokens": [400, 370, 437, 321, 603, 360, 307, 321, 603, 909, 512, 544, 322, 293, 312, 411, 1392, 718, 311, 767, 818, 8873, 2119, 13766, 293, 2119, 20010, 4190, 293, 550, 321, 603, 747, 264, 1192, 1732, 484, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.06627802226854407, "compression_ratio": 1.6724890829694323, "no_speech_prob": 1.0451209163875319e-05}, {"id": 120, "seek": 89516, "start": 904.16, "end": 920.16, "text": " So we'll still return five if that's our desired amount, but we're just kind of adding this safety buffer because we do have some randomness we don't know that we're going to get the best five singular values.", "tokens": [407, 321, 603, 920, 2736, 1732, 498, 300, 311, 527, 14721, 2372, 11, 457, 321, 434, 445, 733, 295, 5127, 341, 4514, 21762, 570, 321, 360, 362, 512, 4974, 1287, 321, 500, 380, 458, 300, 321, 434, 516, 281, 483, 264, 1151, 1732, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.06627802226854407, "compression_ratio": 1.6724890829694323, "no_speech_prob": 1.0451209163875319e-05}, {"id": 121, "seek": 92016, "start": 920.16, "end": 935.16, "text": " Yeah, and so that's that's mostly it so you'll notice we're calling the same sci pi Lin out as VD method in here. It's just now we're calling it on B, which is a lot narrower than a.", "tokens": [865, 11, 293, 370, 300, 311, 300, 311, 5240, 309, 370, 291, 603, 3449, 321, 434, 5141, 264, 912, 2180, 3895, 9355, 484, 382, 691, 35, 3170, 294, 510, 13, 467, 311, 445, 586, 321, 434, 5141, 309, 322, 363, 11, 597, 307, 257, 688, 46751, 813, 257, 13], "temperature": 0.0, "avg_logprob": -0.22979105435884917, "compression_ratio": 1.4064516129032258, "no_speech_prob": 1.618690112081822e-05}, {"id": 122, "seek": 92016, "start": 935.16, "end": 939.16, "text": " Oh, and then I should roll this up.", "tokens": [876, 11, 293, 550, 286, 820, 3373, 341, 493, 13], "temperature": 0.0, "avg_logprob": -0.22979105435884917, "compression_ratio": 1.4064516129032258, "no_speech_prob": 1.618690112081822e-05}, {"id": 123, "seek": 93916, "start": 939.16, "end": 950.16, "text": " So then an exercise that I wanted you to try was to write a loop to calculate the air of your decomposition, as you vary the number of topics.", "tokens": [407, 550, 364, 5380, 300, 286, 1415, 291, 281, 853, 390, 281, 2464, 257, 6367, 281, 8873, 264, 1988, 295, 428, 48356, 11, 382, 291, 10559, 264, 1230, 295, 8378, 13], "temperature": 0.0, "avg_logprob": -0.07665847369602748, "compression_ratio": 1.3027522935779816, "no_speech_prob": 7.766508133499883e-06}, {"id": 124, "seek": 95016, "start": 950.16, "end": 970.16, "text": " And so, and it was take a few minutes to do this but what you're doing here is just using the randomized SVD you don't have to change its implementation at all, but kind of decompose your matrix recompose it and then see what its air is from the original matrix.", "tokens": [400, 370, 11, 293, 309, 390, 747, 257, 1326, 2077, 281, 360, 341, 457, 437, 291, 434, 884, 510, 307, 445, 1228, 264, 38513, 31910, 35, 291, 500, 380, 362, 281, 1319, 1080, 11420, 412, 439, 11, 457, 733, 295, 22867, 541, 428, 8141, 48000, 541, 309, 293, 550, 536, 437, 1080, 1988, 307, 490, 264, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.12613104260157024, "compression_ratio": 1.4719101123595506, "no_speech_prob": 7.888573236414231e-06}, {"id": 125, "seek": 97016, "start": 970.16, "end": 981.16, "text": " And this would be useful if you're doing data compression or something kind of to see, you know how much of the original are you capturing and something with.", "tokens": [400, 341, 576, 312, 4420, 498, 291, 434, 884, 1412, 19355, 420, 746, 733, 295, 281, 536, 11, 291, 458, 577, 709, 295, 264, 3380, 366, 291, 23384, 293, 746, 365, 13], "temperature": 0.0, "avg_logprob": -0.1312965022193061, "compression_ratio": 1.3277310924369747, "no_speech_prob": 3.7264369439071743e-06}, {"id": 126, "seek": 98116, "start": 981.16, "end": 1006.16, "text": " So I just wanted you to kind of loop through through a few numbers to see how it's changing. Remember we originally had 25,000 columns, so we would expect that five singular values is not going to capture everything so the air will be relatively high.", "tokens": [407, 286, 445, 1415, 291, 281, 733, 295, 6367, 807, 807, 257, 1326, 3547, 281, 536, 577, 309, 311, 4473, 13, 5459, 321, 7993, 632, 3552, 11, 1360, 13766, 11, 370, 321, 576, 2066, 300, 1732, 20010, 4190, 307, 406, 516, 281, 7983, 1203, 370, 264, 1988, 486, 312, 7226, 1090, 13], "temperature": 0.0, "avg_logprob": -0.09804520436695643, "compression_ratio": 1.3944444444444444, "no_speech_prob": 1.8738038534138468e-06}, {"id": 127, "seek": 100616, "start": 1006.16, "end": 1015.16, "text": " So I can talk about the answer to this one.", "tokens": [407, 286, 393, 751, 466, 264, 1867, 281, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.15013028729346492, "compression_ratio": 1.493421052631579, "no_speech_prob": 4.425211500347359e-06}, {"id": 128, "seek": 100616, "start": 1015.16, "end": 1022.16, "text": " And actually can you remind me did I leave the plot in the notebooks that you had on GitHub. Okay.", "tokens": [400, 767, 393, 291, 4160, 385, 630, 286, 1856, 264, 7542, 294, 264, 43782, 300, 291, 632, 322, 23331, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.15013028729346492, "compression_ratio": 1.493421052631579, "no_speech_prob": 4.425211500347359e-06}, {"id": 129, "seek": 100616, "start": 1022.16, "end": 1034.1599999999999, "text": " So what do you what do you notice about the plot, come back to the code in a moment.", "tokens": [407, 437, 360, 291, 437, 360, 291, 3449, 466, 264, 7542, 11, 808, 646, 281, 264, 3089, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.15013028729346492, "compression_ratio": 1.493421052631579, "no_speech_prob": 4.425211500347359e-06}, {"id": 130, "seek": 103416, "start": 1034.16, "end": 1039.16, "text": " Brad.", "tokens": [11895, 13], "temperature": 0.0, "avg_logprob": -0.15482186016283536, "compression_ratio": 1.4974093264248705, "no_speech_prob": 5.954810603725491e-06}, {"id": 131, "seek": 103416, "start": 1039.16, "end": 1041.16, "text": " Okay, say again.", "tokens": [1033, 11, 584, 797, 13], "temperature": 0.0, "avg_logprob": -0.15482186016283536, "compression_ratio": 1.4974093264248705, "no_speech_prob": 5.954810603725491e-06}, {"id": 132, "seek": 103416, "start": 1041.16, "end": 1047.16, "text": " Yeah, yeah, the air drops off. I thought surprisingly quickly for this that.", "tokens": [865, 11, 1338, 11, 264, 1988, 11438, 766, 13, 286, 1194, 17600, 2661, 337, 341, 300, 13], "temperature": 0.0, "avg_logprob": -0.15482186016283536, "compression_ratio": 1.4974093264248705, "no_speech_prob": 5.954810603725491e-06}, {"id": 133, "seek": 103416, "start": 1047.16, "end": 1061.16, "text": " Yeah, you have a lot of air initially but well really I mean around 50 is like kind of the steepest but definitely past like 100 it's really a lot lower and kind of not changing as rapidly.", "tokens": [865, 11, 291, 362, 257, 688, 295, 1988, 9105, 457, 731, 534, 286, 914, 926, 2625, 307, 411, 733, 295, 264, 16841, 377, 457, 2138, 1791, 411, 2319, 309, 311, 534, 257, 688, 3126, 293, 733, 295, 406, 4473, 382, 12910, 13], "temperature": 0.0, "avg_logprob": -0.15482186016283536, "compression_ratio": 1.4974093264248705, "no_speech_prob": 5.954810603725491e-06}, {"id": 134, "seek": 106116, "start": 1061.16, "end": 1077.16, "text": " And so you can see like kind of the first the first many singular or first few singular values, giving you a lot of a lot of power kind of capturing a lot but as you go on the additional singular values aren't capturing as much as kind of what this is showing.", "tokens": [400, 370, 291, 393, 536, 411, 733, 295, 264, 700, 264, 700, 867, 20010, 420, 700, 1326, 20010, 4190, 11, 2902, 291, 257, 688, 295, 257, 688, 295, 1347, 733, 295, 23384, 257, 688, 457, 382, 291, 352, 322, 264, 4497, 20010, 4190, 3212, 380, 23384, 382, 709, 382, 733, 295, 437, 341, 307, 4099, 13], "temperature": 0.0, "avg_logprob": -0.16080236434936523, "compression_ratio": 1.7333333333333334, "no_speech_prob": 6.747842689946992e-06}, {"id": 135, "seek": 107716, "start": 1077.16, "end": 1091.16, "text": " So, so I did what I did to get this was have a loop, where I was taking the randomized SVD of I, as I is increasing. And I did this in steps of 20.", "tokens": [407, 11, 370, 286, 630, 437, 286, 630, 281, 483, 341, 390, 362, 257, 6367, 11, 689, 286, 390, 1940, 264, 38513, 31910, 35, 295, 286, 11, 382, 286, 307, 5662, 13, 400, 286, 630, 341, 294, 4439, 295, 945, 13], "temperature": 0.0, "avg_logprob": -0.3833599090576172, "compression_ratio": 1.2457627118644068, "no_speech_prob": 2.9771339541184716e-05}, {"id": 136, "seek": 109116, "start": 1091.16, "end": 1113.16, "text": " So this is a US and V, I created the reconstructed matrix using you times and p.diag turns s which is just an array into a diagonal matrix by filling in zeros and and p.diag also if you give it a matrix and not an array, then it will return an array of what was on the diagonal,", "tokens": [407, 341, 307, 257, 2546, 293, 691, 11, 286, 2942, 264, 31499, 292, 8141, 1228, 291, 1413, 293, 280, 13, 4504, 559, 4523, 262, 597, 307, 445, 364, 10225, 666, 257, 21539, 8141, 538, 10623, 294, 35193, 293, 293, 280, 13, 4504, 559, 611, 498, 291, 976, 309, 257, 8141, 293, 406, 364, 10225, 11, 550, 309, 486, 2736, 364, 10225, 295, 437, 390, 322, 264, 21539, 11], "temperature": 0.0, "avg_logprob": -0.22929546568128797, "compression_ratio": 1.6352941176470588, "no_speech_prob": 2.507029421394691e-05}, {"id": 137, "seek": 111316, "start": 1113.16, "end": 1129.16, "text": " using this from array to a matrix functionality of it. So I'm just doing you times the matrix form times V. And then I'm saying that the air is the norm of the vectors minus reconstructed.", "tokens": [1228, 341, 490, 10225, 281, 257, 8141, 14980, 295, 309, 13, 407, 286, 478, 445, 884, 291, 1413, 264, 8141, 1254, 1413, 691, 13, 400, 550, 286, 478, 1566, 300, 264, 1988, 307, 264, 2026, 295, 264, 18875, 3175, 31499, 292, 13], "temperature": 0.0, "avg_logprob": -0.1622759246826172, "compression_ratio": 1.4411764705882353, "no_speech_prob": 2.8407273930497468e-05}, {"id": 138, "seek": 111316, "start": 1129.16, "end": 1131.16, "text": " Jeremy.", "tokens": [17809, 13], "temperature": 0.0, "avg_logprob": -0.1622759246826172, "compression_ratio": 1.4411764705882353, "no_speech_prob": 2.8407273930497468e-05}, {"id": 139, "seek": 113116, "start": 1131.16, "end": 1154.16, "text": " I mentioned a trick that I like.", "tokens": [286, 2835, 257, 4282, 300, 286, 411, 13], "temperature": 0.0, "avg_logprob": -0.48867249488830566, "compression_ratio": 0.8, "no_speech_prob": 9.970683095161803e-06}, {"id": 140, "seek": 115416, "start": 1154.16, "end": 1171.16, "text": " Are there any questions about this.", "tokens": [2014, 456, 604, 1651, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.18154480240561746, "compression_ratio": 0.875, "no_speech_prob": 6.961160579521675e-06}, {"id": 141, "seek": 117116, "start": 1171.16, "end": 1188.16, "text": " All right. Are there any further questions on topic modeling before we start on background removal and robust PCA.", "tokens": [1057, 558, 13, 2014, 456, 604, 3052, 1651, 322, 4829, 15983, 949, 321, 722, 322, 3678, 17933, 293, 13956, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.2283247947692871, "compression_ratio": 1.1401869158878504, "no_speech_prob": 6.853700597275747e-06}, {"id": 142, "seek": 117116, "start": 1188.16, "end": 1193.16, "text": " Kelsey.", "tokens": [44714, 13], "temperature": 0.0, "avg_logprob": -0.2283247947692871, "compression_ratio": 1.1401869158878504, "no_speech_prob": 6.853700597275747e-06}, {"id": 143, "seek": 119316, "start": 1193.16, "end": 1215.16, "text": " I'm sorry.", "tokens": [286, 478, 2597, 13], "temperature": 0.0, "avg_logprob": -0.7028676867485046, "compression_ratio": 0.5555555555555556, "no_speech_prob": 2.2824857296654955e-05}, {"id": 144, "seek": 121516, "start": 1215.16, "end": 1228.16, "text": " Q by O down here. Yes. Well, so that's actually let me go to the equations, I think show it better.", "tokens": [1249, 538, 422, 760, 510, 13, 1079, 13, 1042, 11, 370, 300, 311, 767, 718, 385, 352, 281, 264, 11787, 11, 286, 519, 855, 309, 1101, 13], "temperature": 0.0, "avg_logprob": -0.2160941400835591, "compression_ratio": 1.064516129032258, "no_speech_prob": 1.0450821719132364e-05}, {"id": 145, "seek": 122816, "start": 1228.16, "end": 1253.16, "text": " So, you know, we've got our skinnier B and we take the SVD on that.", "tokens": [407, 11, 291, 458, 11, 321, 600, 658, 527, 3178, 19165, 363, 293, 321, 747, 264, 31910, 35, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.4010551071166992, "compression_ratio": 0.9436619718309859, "no_speech_prob": 5.33782122147386e-06}, {"id": 146, "seek": 125316, "start": 1253.16, "end": 1267.16, "text": " And then what we're interested in is the SVD of a. And so we're kind of plugging back in. Remember from last time we said a is approximately Q times Q transpose a.", "tokens": [400, 550, 437, 321, 434, 3102, 294, 307, 264, 31910, 35, 295, 257, 13, 400, 370, 321, 434, 733, 295, 42975, 646, 294, 13, 5459, 490, 1036, 565, 321, 848, 257, 307, 10447, 1249, 1413, 1249, 25167, 257, 13], "temperature": 0.0, "avg_logprob": -0.17822058256282364, "compression_ratio": 1.2734375, "no_speech_prob": 1.80572023964487e-05}, {"id": 147, "seek": 126716, "start": 1267.16, "end": 1283.16, "text": " Where Q ends up being, you know, this random matrix we've used and the Q transpose a is how we got our narrow matrix B. So we're just kind of plugging in. Okay, this is B. Let's kind of plug that into here for Q transpose a.", "tokens": [2305, 1249, 5314, 493, 885, 11, 291, 458, 11, 341, 4974, 8141, 321, 600, 1143, 293, 264, 1249, 25167, 257, 307, 577, 321, 658, 527, 9432, 8141, 363, 13, 407, 321, 434, 445, 733, 295, 42975, 294, 13, 1033, 11, 341, 307, 363, 13, 961, 311, 733, 295, 5452, 300, 666, 510, 337, 1249, 25167, 257, 13], "temperature": 0.0, "avg_logprob": -0.11455607023395475, "compression_ratio": 1.4640522875816993, "no_speech_prob": 5.5939817684702575e-06}, {"id": 148, "seek": 128316, "start": 1283.16, "end": 1299.16, "text": " And that's how we get Q times basically the SVD of B.", "tokens": [400, 300, 311, 577, 321, 483, 1249, 1413, 1936, 264, 31910, 35, 295, 363, 13], "temperature": 0.0, "avg_logprob": -0.08734201431274415, "compression_ratio": 0.9367088607594937, "no_speech_prob": 3.904785899067065e-06}, {"id": 149, "seek": 128316, "start": 1299.16, "end": 1308.16, "text": " Any other questions.", "tokens": [2639, 661, 1651, 13], "temperature": 0.0, "avg_logprob": -0.08734201431274415, "compression_ratio": 0.9367088607594937, "no_speech_prob": 3.904785899067065e-06}, {"id": 150, "seek": 130816, "start": 1308.16, "end": 1313.16, "text": " Okay. Oh, Tim.", "tokens": [1033, 13, 876, 11, 7172, 13], "temperature": 0.0, "avg_logprob": -0.30319716533025104, "compression_ratio": 0.9384615384615385, "no_speech_prob": 3.70446578017436e-05}, {"id": 151, "seek": 130816, "start": 1313.16, "end": 1317.16, "text": " To the randomized range finder algorithm.", "tokens": [1407, 264, 38513, 3613, 915, 260, 9284, 13], "temperature": 0.0, "avg_logprob": -0.30319716533025104, "compression_ratio": 0.9384615384615385, "no_speech_prob": 3.70446578017436e-05}, {"id": 152, "seek": 130816, "start": 1317.16, "end": 1324.16, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.30319716533025104, "compression_ratio": 0.9384615384615385, "no_speech_prob": 3.70446578017436e-05}, {"id": 153, "seek": 132416, "start": 1324.16, "end": 1349.16, "text": " You said you're going to explain later the for loop. So I was putting that off. But as a kind of as an intuitive idea, what that's doing is by taking additional powers of a, you're getting something that's more kind of even more in the range of a.", "tokens": [509, 848, 291, 434, 516, 281, 2903, 1780, 264, 337, 6367, 13, 407, 286, 390, 3372, 300, 766, 13, 583, 382, 257, 733, 295, 382, 364, 21769, 1558, 11, 437, 300, 311, 884, 307, 538, 1940, 4497, 8674, 295, 257, 11, 291, 434, 1242, 746, 300, 311, 544, 733, 295, 754, 544, 294, 264, 3613, 295, 257, 13], "temperature": 0.0, "avg_logprob": -0.17808458881993447, "compression_ratio": 1.5341614906832297, "no_speech_prob": 1.5935389455989935e-05}, {"id": 154, "seek": 134916, "start": 1349.16, "end": 1355.16, "text": " So kind of by taking extra powers of a.", "tokens": [407, 733, 295, 538, 1940, 2857, 8674, 295, 257, 13], "temperature": 0.0, "avg_logprob": -0.11988831559816997, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.4063145499676466e-05}, {"id": 155, "seek": 134916, "start": 1355.16, "end": 1363.16, "text": " Hopefully that's helping you kind of really get the important components of the range of a because remember here we're trying to find something that has the same.", "tokens": [10429, 300, 311, 4315, 291, 733, 295, 534, 483, 264, 1021, 6677, 295, 264, 3613, 295, 257, 570, 1604, 510, 321, 434, 1382, 281, 915, 746, 300, 575, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.11988831559816997, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.4063145499676466e-05}, {"id": 156, "seek": 134916, "start": 1363.16, "end": 1375.16, "text": " You know, we want B to have the same range as a and so additional powers of a help us do that. But if we were just to take powers of a without, you know, here we're using this LU decomposition.", "tokens": [509, 458, 11, 321, 528, 363, 281, 362, 264, 912, 3613, 382, 257, 293, 370, 4497, 8674, 295, 257, 854, 505, 360, 300, 13, 583, 498, 321, 645, 445, 281, 747, 8674, 295, 257, 1553, 11, 291, 458, 11, 510, 321, 434, 1228, 341, 31851, 48356, 13], "temperature": 0.0, "avg_logprob": -0.11988831559816997, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.4063145499676466e-05}, {"id": 157, "seek": 137516, "start": 1375.16, "end": 1388.16, "text": " And we're doing that is just taking powers of a. We would have the problem of either the matrix will kind of be like exploding as its values got larger and larger going towards infinity or they would be shrinking to zero.", "tokens": [400, 321, 434, 884, 300, 307, 445, 1940, 8674, 295, 257, 13, 492, 576, 362, 264, 1154, 295, 2139, 264, 8141, 486, 733, 295, 312, 411, 35175, 382, 1080, 4190, 658, 4833, 293, 4833, 516, 3030, 13202, 420, 436, 576, 312, 41684, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.11226433883478612, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.1478316991997417e-05}, {"id": 158, "seek": 137516, "start": 1388.16, "end": 1398.16, "text": " And either way, that's a problem. And so this LU factorization is having the effect of normalizing it.", "tokens": [400, 2139, 636, 11, 300, 311, 257, 1154, 13, 400, 370, 341, 31851, 5952, 2144, 307, 1419, 264, 1802, 295, 2710, 3319, 309, 13], "temperature": 0.0, "avg_logprob": -0.11226433883478612, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.1478316991997417e-05}, {"id": 159, "seek": 137516, "start": 1398.16, "end": 1400.16, "text": " Yes. Yeah.", "tokens": [1079, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.11226433883478612, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.1478316991997417e-05}, {"id": 160, "seek": 140016, "start": 1400.16, "end": 1410.16, "text": " Jeremy.", "tokens": [17809, 13], "temperature": 0.0, "avg_logprob": -0.6743357181549072, "compression_ratio": 0.4666666666666667, "no_speech_prob": 7.364938937826082e-05}, {"id": 161, "seek": 141016, "start": 1410.16, "end": 1434.16, "text": " You can use it.", "tokens": [509, 393, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.544586976369222, "compression_ratio": 0.6521739130434783, "no_speech_prob": 0.00011942394485231489}, {"id": 162, "seek": 143416, "start": 1434.16, "end": 1449.16, "text": " I was just going to bring up the Wikipedia page because I like that they list a ton of different fields where they're saying this Lemma has uses in compressed sensing manifold learning dimensionality reduction and graph embedding.", "tokens": [286, 390, 445, 516, 281, 1565, 493, 264, 28999, 3028, 570, 286, 411, 300, 436, 1329, 257, 2952, 295, 819, 7909, 689, 436, 434, 1566, 341, 16905, 1696, 575, 4960, 294, 30353, 30654, 47138, 2539, 10139, 1860, 11004, 293, 4295, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.10922468069827918, "compression_ratio": 1.5612244897959184, "no_speech_prob": 4.4250859900785144e-06}, {"id": 163, "seek": 143416, "start": 1449.16, "end": 1458.16, "text": " But I just kind of like like that like list of places where this is useful.", "tokens": [583, 286, 445, 733, 295, 411, 411, 300, 411, 1329, 295, 3190, 689, 341, 307, 4420, 13], "temperature": 0.0, "avg_logprob": -0.10922468069827918, "compression_ratio": 1.5612244897959184, "no_speech_prob": 4.4250859900785144e-06}, {"id": 164, "seek": 145816, "start": 1458.16, "end": 1465.16, "text": " Yeah, thank you.", "tokens": [865, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.1288621975825383, "compression_ratio": 1.3428571428571427, "no_speech_prob": 2.8571387247211533e-06}, {"id": 165, "seek": 145816, "start": 1465.16, "end": 1471.16, "text": " Any other final SPD and NMF questions and we'll be returning to SPD.", "tokens": [2639, 661, 2572, 19572, 293, 426, 44, 37, 1651, 293, 321, 603, 312, 12678, 281, 19572, 13], "temperature": 0.0, "avg_logprob": -0.1288621975825383, "compression_ratio": 1.3428571428571427, "no_speech_prob": 2.8571387247211533e-06}, {"id": 166, "seek": 145816, "start": 1471.16, "end": 1483.16, "text": " We're both going to use it and other applications and then we'll learn more about how it's calculated.", "tokens": [492, 434, 1293, 516, 281, 764, 309, 293, 661, 5821, 293, 550, 321, 603, 1466, 544, 466, 577, 309, 311, 15598, 13], "temperature": 0.0, "avg_logprob": -0.1288621975825383, "compression_ratio": 1.3428571428571427, "no_speech_prob": 2.8571387247211533e-06}, {"id": 167, "seek": 148316, "start": 1483.16, "end": 1494.16, "text": " Okay, let's start on background removal. This is notebook three. I'm really excited about this one background removal with robust PCA.", "tokens": [1033, 11, 718, 311, 722, 322, 3678, 17933, 13, 639, 307, 21060, 1045, 13, 286, 478, 534, 2919, 466, 341, 472, 3678, 17933, 365, 13956, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.20871246183240735, "compression_ratio": 1.2347826086956522, "no_speech_prob": 1.0129178917850368e-05}, {"id": 168, "seek": 148316, "start": 1494.16, "end": 1497.16, "text": " And so,", "tokens": [400, 370, 11], "temperature": 0.0, "avg_logprob": -0.20871246183240735, "compression_ratio": 1.2347826086956522, "no_speech_prob": 1.0129178917850368e-05}, {"id": 169, "seek": 149716, "start": 1497.16, "end": 1513.16, "text": " So here this is our goal that we'll be working on in the notebook is we have a surveillance video. So you see here there's some figures walking across the screen and we want to be able to pick out what's the background and what are the people.", "tokens": [407, 510, 341, 307, 527, 3387, 300, 321, 603, 312, 1364, 322, 294, 264, 21060, 307, 321, 362, 257, 18475, 960, 13, 407, 291, 536, 510, 456, 311, 512, 9624, 4494, 2108, 264, 2568, 293, 321, 528, 281, 312, 1075, 281, 1888, 484, 437, 311, 264, 3678, 293, 437, 366, 264, 561, 13], "temperature": 0.0, "avg_logprob": -0.09223065168961235, "compression_ratio": 1.7522522522522523, "no_speech_prob": 7.18259025234147e-06}, {"id": 170, "seek": 149716, "start": 1513.16, "end": 1524.16, "text": " So we've taken the picture on the left and broken it into this middle picture and the picture on the right. That's the goal that we're going for.", "tokens": [407, 321, 600, 2726, 264, 3036, 322, 264, 1411, 293, 5463, 309, 666, 341, 2808, 3036, 293, 264, 3036, 322, 264, 558, 13, 663, 311, 264, 3387, 300, 321, 434, 516, 337, 13], "temperature": 0.0, "avg_logprob": -0.09223065168961235, "compression_ratio": 1.7522522522522523, "no_speech_prob": 7.18259025234147e-06}, {"id": 171, "seek": 152416, "start": 1524.16, "end": 1533.16, "text": " Using something called the real video of three data set here is a link to it if you want to download it so that you can do this as well.", "tokens": [11142, 746, 1219, 264, 957, 960, 295, 1045, 1412, 992, 510, 307, 257, 2113, 281, 309, 498, 291, 528, 281, 5484, 309, 370, 300, 291, 393, 360, 341, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1085650643637014, "compression_ratio": 1.6682926829268292, "no_speech_prob": 2.1442650904646143e-05}, {"id": 172, "seek": 152416, "start": 1533.16, "end": 1540.16, "text": " I also found some other sources of video data set so something I kind of had to look for for a while.", "tokens": [286, 611, 1352, 512, 661, 7139, 295, 960, 1412, 992, 370, 746, 286, 733, 295, 632, 281, 574, 337, 337, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.1085650643637014, "compression_ratio": 1.6682926829268292, "no_speech_prob": 2.1442650904646143e-05}, {"id": 173, "seek": 152416, "start": 1540.16, "end": 1549.16, "text": " So I would be curious if you do try this on a different video data set to see how how your results are.", "tokens": [407, 286, 576, 312, 6369, 498, 291, 360, 853, 341, 322, 257, 819, 960, 1412, 992, 281, 536, 577, 577, 428, 3542, 366, 13], "temperature": 0.0, "avg_logprob": -0.1085650643637014, "compression_ratio": 1.6682926829268292, "no_speech_prob": 2.1442650904646143e-05}, {"id": 174, "seek": 154916, "start": 1549.16, "end": 1555.16, "text": " So I had I had never worked with video data in Python before this.", "tokens": [407, 286, 632, 286, 632, 1128, 2732, 365, 960, 1412, 294, 15329, 949, 341, 13], "temperature": 0.0, "avg_logprob": -0.07662597069373497, "compression_ratio": 1.5311004784688995, "no_speech_prob": 1.3924908444096218e-06}, {"id": 175, "seek": 154916, "start": 1555.16, "end": 1568.16, "text": " I found the movie pie library which seemed really neat and actually kind of made me want to do more work with videos because here we kind of just use it briefly to convert this into a matrix.", "tokens": [286, 1352, 264, 3169, 1730, 6405, 597, 6576, 534, 10654, 293, 767, 733, 295, 1027, 385, 528, 281, 360, 544, 589, 365, 2145, 570, 510, 321, 733, 295, 445, 764, 309, 10515, 281, 7620, 341, 666, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.07662597069373497, "compression_ratio": 1.5311004784688995, "no_speech_prob": 1.3924908444096218e-06}, {"id": 176, "seek": 154916, "start": 1568.16, "end": 1574.16, "text": " Yeah, I've got my imports and then I wanted to show you this.", "tokens": [865, 11, 286, 600, 658, 452, 41596, 293, 550, 286, 1415, 281, 855, 291, 341, 13], "temperature": 0.0, "avg_logprob": -0.07662597069373497, "compression_ratio": 1.5311004784688995, "no_speech_prob": 1.3924908444096218e-06}, {"id": 177, "seek": 157416, "start": 1574.16, "end": 1580.16, "text": " I actually watched the video or at least part of it was too long to watch the whole thing.", "tokens": [286, 767, 6337, 264, 960, 420, 412, 1935, 644, 295, 309, 390, 886, 938, 281, 1159, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.1318057565128102, "compression_ratio": 1.27, "no_speech_prob": 6.301038229139522e-05}, {"id": 178, "seek": 157416, "start": 1580.16, "end": 1584.16, "text": " So there you can see people walking.", "tokens": [407, 456, 291, 393, 536, 561, 4494, 13], "temperature": 0.0, "avg_logprob": -0.1318057565128102, "compression_ratio": 1.27, "no_speech_prob": 6.301038229139522e-05}, {"id": 179, "seek": 158416, "start": 1584.16, "end": 1609.16, "text": " I didn't color, but I converted it to black and white to kind of keep things simple or simpler.", "tokens": [286, 994, 380, 2017, 11, 457, 286, 16424, 309, 281, 2211, 293, 2418, 281, 733, 295, 1066, 721, 2199, 420, 18587, 13], "temperature": 0.0, "avg_logprob": -0.19953603010911208, "compression_ratio": 1.130952380952381, "no_speech_prob": 3.6118331081524957e-06}, {"id": 180, "seek": 160916, "start": 1609.16, "end": 1614.16, "text": " And that's it. That's our data.", "tokens": [400, 300, 311, 309, 13, 663, 311, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09221052086871603, "compression_ratio": 1.2436974789915967, "no_speech_prob": 4.49482968178927e-06}, {"id": 181, "seek": 160916, "start": 1614.16, "end": 1622.16, "text": " And this video is 113 seconds long total.", "tokens": [400, 341, 960, 307, 2975, 18, 3949, 938, 3217, 13], "temperature": 0.0, "avg_logprob": -0.09221052086871603, "compression_ratio": 1.2436974789915967, "no_speech_prob": 4.49482968178927e-06}, {"id": 182, "seek": 160916, "start": 1622.16, "end": 1628.16, "text": " So I have some helper methods, and we're not going to super go into those.", "tokens": [407, 286, 362, 512, 36133, 7150, 11, 293, 321, 434, 406, 516, 281, 1687, 352, 666, 729, 13], "temperature": 0.0, "avg_logprob": -0.09221052086871603, "compression_ratio": 1.2436974789915967, "no_speech_prob": 4.49482968178927e-06}, {"id": 183, "seek": 162816, "start": 1628.16, "end": 1643.16, "text": " We're really it's mostly I'm just using so the video library has this get frame that I'll use to kind of pick out specific frames, and then we're stacking those up to form a matrix.", "tokens": [492, 434, 534, 309, 311, 5240, 286, 478, 445, 1228, 370, 264, 960, 6405, 575, 341, 483, 3920, 300, 286, 603, 764, 281, 733, 295, 1888, 484, 2685, 12083, 11, 293, 550, 321, 434, 41376, 729, 493, 281, 1254, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.0957734440312241, "compression_ratio": 1.4550561797752808, "no_speech_prob": 7.410897524096072e-06}, {"id": 184, "seek": 162816, "start": 1643.16, "end": 1651.16, "text": " And I think that's actually kind of easier to see from I have a few pictures.", "tokens": [400, 286, 519, 300, 311, 767, 733, 295, 3571, 281, 536, 490, 286, 362, 257, 1326, 5242, 13], "temperature": 0.0, "avg_logprob": -0.0957734440312241, "compression_ratio": 1.4550561797752808, "no_speech_prob": 7.410897524096072e-06}, {"id": 185, "seek": 165116, "start": 1651.16, "end": 1658.16, "text": " So this is a single kind of single frame, you know, one point in time.", "tokens": [407, 341, 307, 257, 2167, 733, 295, 2167, 3920, 11, 291, 458, 11, 472, 935, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.08746654510498048, "compression_ratio": 1.5364583333333333, "no_speech_prob": 7.766444468870759e-06}, {"id": 186, "seek": 165116, "start": 1658.16, "end": 1660.16, "text": " And I actually I guess I should first note.", "tokens": [400, 286, 767, 286, 2041, 286, 820, 700, 3637, 13], "temperature": 0.0, "avg_logprob": -0.08746654510498048, "compression_ratio": 1.5364583333333333, "no_speech_prob": 7.766444468870759e-06}, {"id": 187, "seek": 165116, "start": 1660.16, "end": 1670.16, "text": " So I scale this and I tried doing it with kind of the full resolution and it was really slow, but I do have some pictures from the high resolution, although I don't recommend that.", "tokens": [407, 286, 4373, 341, 293, 286, 3031, 884, 309, 365, 733, 295, 264, 1577, 8669, 293, 309, 390, 534, 2964, 11, 457, 286, 360, 362, 512, 5242, 490, 264, 1090, 8669, 11, 4878, 286, 500, 380, 2748, 300, 13], "temperature": 0.0, "avg_logprob": -0.08746654510498048, "compression_ratio": 1.5364583333333333, "no_speech_prob": 7.766444468870759e-06}, {"id": 188, "seek": 167016, "start": 1670.16, "end": 1684.16, "text": " But here you can enter the scale as a percentage. And so I'm just using 25 percent scale to make it faster. And so an image from one moment in time will be 60 pixels by 80 pixels.", "tokens": [583, 510, 291, 393, 3242, 264, 4373, 382, 257, 9668, 13, 400, 370, 286, 478, 445, 1228, 3552, 3043, 4373, 281, 652, 309, 4663, 13, 400, 370, 364, 3256, 490, 472, 1623, 294, 565, 486, 312, 4060, 18668, 538, 4688, 18668, 13], "temperature": 0.0, "avg_logprob": -0.07824770264003587, "compression_ratio": 1.366412213740458, "no_speech_prob": 1.2218273695907556e-05}, {"id": 189, "seek": 168416, "start": 1684.16, "end": 1700.16, "text": " And we're going to unroll that into a single column. So kind of each moment in time is going to be this 4800 long column. And then we're stacking those all together from these different points in time.", "tokens": [400, 321, 434, 516, 281, 517, 3970, 300, 666, 257, 2167, 7738, 13, 407, 733, 295, 1184, 1623, 294, 565, 307, 516, 281, 312, 341, 11174, 628, 938, 7738, 13, 400, 550, 321, 434, 41376, 729, 439, 1214, 490, 613, 819, 2793, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.07570701326642718, "compression_ratio": 1.5303867403314917, "no_speech_prob": 8.664269444125239e-06}, {"id": 190, "seek": 168416, "start": 1700.16, "end": 1707.16, "text": " And so we end up with a matrix that's 11300 by 4800 representing the video.", "tokens": [400, 370, 321, 917, 493, 365, 257, 8141, 300, 311, 2975, 12566, 538, 11174, 628, 13460, 264, 960, 13], "temperature": 0.0, "avg_logprob": -0.07570701326642718, "compression_ratio": 1.5303867403314917, "no_speech_prob": 8.664269444125239e-06}, {"id": 191, "seek": 170716, "start": 1707.16, "end": 1717.16, "text": " And I wanted to show a picture of this is what the whole video looks like.", "tokens": [400, 286, 1415, 281, 855, 257, 3036, 295, 341, 307, 437, 264, 1379, 960, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1541644831498464, "compression_ratio": 1.3076923076923077, "no_speech_prob": 4.2226129153277725e-06}, {"id": 192, "seek": 170716, "start": 1717.16, "end": 1727.16, "text": " So does anyone have ideas? What do you think those wavy black lines are?", "tokens": [407, 775, 2878, 362, 3487, 30, 708, 360, 291, 519, 729, 261, 15498, 2211, 3876, 366, 30], "temperature": 0.0, "avg_logprob": -0.1541644831498464, "compression_ratio": 1.3076923076923077, "no_speech_prob": 4.2226129153277725e-06}, {"id": 193, "seek": 170716, "start": 1727.16, "end": 1730.16, "text": " You want to toss this?", "tokens": [509, 528, 281, 14432, 341, 30], "temperature": 0.0, "avg_logprob": -0.1541644831498464, "compression_ratio": 1.3076923076923077, "no_speech_prob": 4.2226129153277725e-06}, {"id": 194, "seek": 173016, "start": 1730.16, "end": 1737.16, "text": " That's probably the people moving. Yes, those are the people moving in the frame. And so then what are the kind of solid horizontal lines?", "tokens": [663, 311, 1391, 264, 561, 2684, 13, 1079, 11, 729, 366, 264, 561, 2684, 294, 264, 3920, 13, 400, 370, 550, 437, 366, 264, 733, 295, 5100, 12750, 3876, 30], "temperature": 0.0, "avg_logprob": -0.10334461562487544, "compression_ratio": 1.8068669527896997, "no_speech_prob": 1.4284634744399227e-05}, {"id": 195, "seek": 173016, "start": 1737.16, "end": 1744.16, "text": " The background. The background. Yeah. So remember each each column here is a single moment in time.", "tokens": [440, 3678, 13, 440, 3678, 13, 865, 13, 407, 1604, 1184, 1184, 7738, 510, 307, 257, 2167, 1623, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.10334461562487544, "compression_ratio": 1.8068669527896997, "no_speech_prob": 1.4284634744399227e-05}, {"id": 196, "seek": 173016, "start": 1744.16, "end": 1751.16, "text": " And so some things like that sign are never moving and probably just show up as kind of these horizontal white lines.", "tokens": [400, 370, 512, 721, 411, 300, 1465, 366, 1128, 2684, 293, 1391, 445, 855, 493, 382, 733, 295, 613, 12750, 2418, 3876, 13], "temperature": 0.0, "avg_logprob": -0.10334461562487544, "compression_ratio": 1.8068669527896997, "no_speech_prob": 1.4284634744399227e-05}, {"id": 197, "seek": 173016, "start": 1751.16, "end": 1755.16, "text": " But the people do move. And so those those pixels are different.", "tokens": [583, 264, 561, 360, 1286, 13, 400, 370, 729, 729, 18668, 366, 819, 13], "temperature": 0.0, "avg_logprob": -0.10334461562487544, "compression_ratio": 1.8068669527896997, "no_speech_prob": 1.4284634744399227e-05}, {"id": 198, "seek": 175516, "start": 1755.16, "end": 1760.16, "text": " So the columns are a moment in time.", "tokens": [407, 264, 13766, 366, 257, 1623, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.14411549954800992, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.5688570783822797e-05}, {"id": 199, "seek": 175516, "start": 1760.16, "end": 1763.16, "text": " The columns are a moment in time.", "tokens": [440, 13766, 366, 257, 1623, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.14411549954800992, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.5688570783822797e-05}, {"id": 200, "seek": 175516, "start": 1763.16, "end": 1771.16, "text": " And this is I think backwards from probably what I wrote in the older version of depending on when you last pulled from GitHub.", "tokens": [400, 341, 307, 286, 519, 12204, 490, 1391, 437, 286, 4114, 294, 264, 4906, 3037, 295, 5413, 322, 562, 291, 1036, 7373, 490, 23331, 13], "temperature": 0.0, "avg_logprob": -0.14411549954800992, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.5688570783822797e-05}, {"id": 201, "seek": 175516, "start": 1771.16, "end": 1775.16, "text": " But yeah, here the columns are a moment in time.", "tokens": [583, 1338, 11, 510, 264, 13766, 366, 257, 1623, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.14411549954800992, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.5688570783822797e-05}, {"id": 202, "seek": 175516, "start": 1775.16, "end": 1781.16, "text": " Is this all related to optical flow?", "tokens": [1119, 341, 439, 4077, 281, 20674, 3095, 30], "temperature": 0.0, "avg_logprob": -0.14411549954800992, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.5688570783822797e-05}, {"id": 203, "seek": 178116, "start": 1781.16, "end": 1786.16, "text": " What do you mean by optical flow?", "tokens": [708, 360, 291, 914, 538, 20674, 3095, 30], "temperature": 0.0, "avg_logprob": -0.18070487718324404, "compression_ratio": 1.4404145077720207, "no_speech_prob": 4.399841054691933e-05}, {"id": 204, "seek": 178116, "start": 1786.16, "end": 1802.16, "text": " I don't really know what it is. So it's some sort of methodology of tracking movements through like you have two adjacent frames like having morphed one to match the next frame.", "tokens": [286, 500, 380, 534, 458, 437, 309, 307, 13, 407, 309, 311, 512, 1333, 295, 24850, 295, 11603, 9981, 807, 411, 291, 362, 732, 24441, 12083, 411, 1419, 25778, 292, 472, 281, 2995, 264, 958, 3920, 13], "temperature": 0.0, "avg_logprob": -0.18070487718324404, "compression_ratio": 1.4404145077720207, "no_speech_prob": 4.399841054691933e-05}, {"id": 205, "seek": 178116, "start": 1802.16, "end": 1808.16, "text": " Oh, OK. Yeah, I'm not familiar with that area. So I'm not sure if.", "tokens": [876, 11, 2264, 13, 865, 11, 286, 478, 406, 4963, 365, 300, 1859, 13, 407, 286, 478, 406, 988, 498, 13], "temperature": 0.0, "avg_logprob": -0.18070487718324404, "compression_ratio": 1.4404145077720207, "no_speech_prob": 4.399841054691933e-05}, {"id": 206, "seek": 180816, "start": 1808.16, "end": 1814.16, "text": " If there are, because that is kind of brings up a good point that here it's assumed that the camera is fixed.", "tokens": [759, 456, 366, 11, 570, 300, 307, 733, 295, 5607, 493, 257, 665, 935, 300, 510, 309, 311, 15895, 300, 264, 2799, 307, 6806, 13], "temperature": 0.0, "avg_logprob": -0.14495963494754532, "compression_ratio": 1.7008547008547008, "no_speech_prob": 7.483513763872907e-05}, {"id": 207, "seek": 180816, "start": 1814.16, "end": 1825.16, "text": " And so things are kind of being referenced by I don't know, like, you know, everything that's at X coordinate 20, Y coordinate 30 is showing up at the exact same.", "tokens": [400, 370, 721, 366, 733, 295, 885, 32734, 538, 286, 500, 380, 458, 11, 411, 11, 291, 458, 11, 1203, 300, 311, 412, 1783, 15670, 945, 11, 398, 15670, 2217, 307, 4099, 493, 412, 264, 1900, 912, 13], "temperature": 0.0, "avg_logprob": -0.14495963494754532, "compression_ratio": 1.7008547008547008, "no_speech_prob": 7.483513763872907e-05}, {"id": 208, "seek": 180816, "start": 1825.16, "end": 1831.16, "text": " You know, when you enroll it, that's kind of point 600. And so it's always going to be showing up in the sixth hundredth row.", "tokens": [509, 458, 11, 562, 291, 12266, 309, 11, 300, 311, 733, 295, 935, 11849, 13, 400, 370, 309, 311, 1009, 516, 281, 312, 4099, 493, 294, 264, 15102, 3262, 392, 5386, 13], "temperature": 0.0, "avg_logprob": -0.14495963494754532, "compression_ratio": 1.7008547008547008, "no_speech_prob": 7.483513763872907e-05}, {"id": 209, "seek": 183116, "start": 1831.16, "end": 1839.16, "text": " So this is kind of fixed. Jeremy, we mentioned the other day that we look at a low rank matrix for Tennessee lines on it.", "tokens": [407, 341, 307, 733, 295, 6806, 13, 17809, 11, 321, 2835, 264, 661, 786, 300, 321, 574, 412, 257, 2295, 6181, 8141, 337, 21127, 3876, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.23350983335260758, "compression_ratio": 1.5185185185185186, "no_speech_prob": 1.0951632248179521e-05}, {"id": 210, "seek": 183116, "start": 1839.16, "end": 1847.16, "text": " So I guess this is showing this is this is a low rank matrix already.", "tokens": [407, 286, 2041, 341, 307, 4099, 341, 307, 341, 307, 257, 2295, 6181, 8141, 1217, 13], "temperature": 0.0, "avg_logprob": -0.23350983335260758, "compression_ratio": 1.5185185185185186, "no_speech_prob": 1.0951632248179521e-05}, {"id": 211, "seek": 183116, "start": 1847.16, "end": 1853.16, "text": " This is it's.", "tokens": [639, 307, 309, 311, 13], "temperature": 0.0, "avg_logprob": -0.23350983335260758, "compression_ratio": 1.5185185185185186, "no_speech_prob": 1.0951632248179521e-05}, {"id": 212, "seek": 185316, "start": 1853.16, "end": 1868.16, "text": " Yeah, I mean, yeah. So the horizontal lines indicate that it's low rank. And actually, first I should ask who can tell us what rank is when we're talking about matrices?", "tokens": [865, 11, 286, 914, 11, 1338, 13, 407, 264, 12750, 3876, 13330, 300, 309, 311, 2295, 6181, 13, 400, 767, 11, 700, 286, 820, 1029, 567, 393, 980, 505, 437, 6181, 307, 562, 321, 434, 1417, 466, 32284, 30], "temperature": 0.0, "avg_logprob": -0.1928470899473946, "compression_ratio": 1.3311688311688312, "no_speech_prob": 1.5143014309160208e-07}, {"id": 213, "seek": 185316, "start": 1868.16, "end": 1876.16, "text": " Anyone I haven't heard from Kelsey.", "tokens": [14643, 286, 2378, 380, 2198, 490, 44714, 13], "temperature": 0.0, "avg_logprob": -0.1928470899473946, "compression_ratio": 1.3311688311688312, "no_speech_prob": 1.5143014309160208e-07}, {"id": 214, "seek": 187616, "start": 1876.16, "end": 1883.16, "text": " So rank is the number of independent columns. Yes. Yeah.", "tokens": [407, 6181, 307, 264, 1230, 295, 6695, 13766, 13, 1079, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.13336132821582614, "compression_ratio": 1.8064516129032258, "no_speech_prob": 3.089371830355958e-06}, {"id": 215, "seek": 187616, "start": 1883.16, "end": 1890.16, "text": " So rank is the number of independent columns. And that turns out to be equal to the number of independent rows.", "tokens": [407, 6181, 307, 264, 1230, 295, 6695, 13766, 13, 400, 300, 4523, 484, 281, 312, 2681, 281, 264, 1230, 295, 6695, 13241, 13], "temperature": 0.0, "avg_logprob": -0.13336132821582614, "compression_ratio": 1.8064516129032258, "no_speech_prob": 3.089371830355958e-06}, {"id": 216, "seek": 189016, "start": 1890.16, "end": 1907.16, "text": " Another way to think about it is it's the dimension of the column space or the dimension of the row space, which is equal to the number of linear independent columns and rows.", "tokens": [3996, 636, 281, 519, 466, 309, 307, 309, 311, 264, 10139, 295, 264, 7738, 1901, 420, 264, 10139, 295, 264, 5386, 1901, 11, 597, 307, 2681, 281, 264, 1230, 295, 8213, 6695, 13766, 293, 13241, 13], "temperature": 0.0, "avg_logprob": -0.07025063792361488, "compression_ratio": 1.6, "no_speech_prob": 2.9479390377673553e-06}, {"id": 217, "seek": 189016, "start": 1907.16, "end": 1916.16, "text": " Yes, let me just kind of go back up here. So I definitely encourage it's really important to be able to look at your data and kind of find ways to visualize it.", "tokens": [1079, 11, 718, 385, 445, 733, 295, 352, 646, 493, 510, 13, 407, 286, 2138, 5373, 309, 311, 534, 1021, 281, 312, 1075, 281, 574, 412, 428, 1412, 293, 733, 295, 915, 2098, 281, 23273, 309, 13], "temperature": 0.0, "avg_logprob": -0.07025063792361488, "compression_ratio": 1.6, "no_speech_prob": 2.9479390377673553e-06}, {"id": 218, "seek": 191616, "start": 1916.16, "end": 1924.16, "text": " But here we've created our data matrix, put that all into M. This is just a NumPy 2D array.", "tokens": [583, 510, 321, 600, 2942, 527, 1412, 8141, 11, 829, 300, 439, 666, 376, 13, 639, 307, 445, 257, 22592, 47, 88, 568, 35, 10225, 13], "temperature": 0.0, "avg_logprob": -0.10316364815894594, "compression_ratio": 1.6018518518518519, "no_speech_prob": 1.892466207209509e-05}, {"id": 219, "seek": 191616, "start": 1924.16, "end": 1934.16, "text": " And then here we're kind of just saying let's take all values and in particular or sorry, take all rows and then a particular columns.", "tokens": [400, 550, 510, 321, 434, 733, 295, 445, 1566, 718, 311, 747, 439, 4190, 293, 294, 1729, 420, 2597, 11, 747, 439, 13241, 293, 550, 257, 1729, 13766, 13], "temperature": 0.0, "avg_logprob": -0.10316364815894594, "compression_ratio": 1.6018518518518519, "no_speech_prob": 1.892466207209509e-05}, {"id": 220, "seek": 191616, "start": 1934.16, "end": 1940.16, "text": " And that's the point in time that we're looking at. And we do need to reshape it to get it kind of back into a picture.", "tokens": [400, 300, 311, 264, 935, 294, 565, 300, 321, 434, 1237, 412, 13, 400, 321, 360, 643, 281, 725, 42406, 309, 281, 483, 309, 733, 295, 646, 666, 257, 3036, 13], "temperature": 0.0, "avg_logprob": -0.10316364815894594, "compression_ratio": 1.6018518518518519, "no_speech_prob": 1.892466207209509e-05}, {"id": 221, "seek": 194016, "start": 1940.16, "end": 1959.16, "text": " So we've kind of unrolled it into at least if I kept I might have gotten rid of it at one point I had a picture of just like a single a single row, which is pretty boring because it's just doesn't look like anything by itself.", "tokens": [407, 321, 600, 733, 295, 517, 28850, 309, 666, 412, 1935, 498, 286, 4305, 286, 1062, 362, 5768, 3973, 295, 309, 412, 472, 935, 286, 632, 257, 3036, 295, 445, 411, 257, 2167, 257, 2167, 5386, 11, 597, 307, 1238, 9989, 570, 309, 311, 445, 1177, 380, 574, 411, 1340, 538, 2564, 13], "temperature": 0.0, "avg_logprob": -0.10166498330923227, "compression_ratio": 1.6046511627906976, "no_speech_prob": 5.0146486501034815e-06}, {"id": 222, "seek": 194016, "start": 1959.16, "end": 1965.16, "text": " You know, it's black, white, gray, black, white, gray. Then we kind of have to reshape it back into our square matrix.", "tokens": [509, 458, 11, 309, 311, 2211, 11, 2418, 11, 10855, 11, 2211, 11, 2418, 11, 10855, 13, 1396, 321, 733, 295, 362, 281, 725, 42406, 309, 646, 666, 527, 3732, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10166498330923227, "compression_ratio": 1.6046511627906976, "no_speech_prob": 5.0146486501034815e-06}, {"id": 223, "seek": 196516, "start": 1965.16, "end": 1981.16, "text": " So NumPy dot reshape is pretty handy. Questions so far just kind of on what's in M or this the setup.", "tokens": [407, 22592, 47, 88, 5893, 725, 42406, 307, 1238, 13239, 13, 27738, 370, 1400, 445, 733, 295, 322, 437, 311, 294, 376, 420, 341, 264, 8657, 13], "temperature": 0.0, "avg_logprob": -0.19500123300860006, "compression_ratio": 1.0978260869565217, "no_speech_prob": 6.339068022498395e-06}, {"id": 224, "seek": 198116, "start": 1981.16, "end": 1995.16, "text": " OK. And then we've just talked about SPD. So kind of remember what kind of matrices we're getting back.", "tokens": [2264, 13, 400, 550, 321, 600, 445, 2825, 466, 19572, 13, 407, 733, 295, 1604, 437, 733, 295, 32284, 321, 434, 1242, 646, 13], "temperature": 0.0, "avg_logprob": -0.20002295924168007, "compression_ratio": 1.3785714285714286, "no_speech_prob": 1.1478267879283521e-05}, {"id": 225, "seek": 198116, "start": 1995.16, "end": 2004.16, "text": " And so we're going to try first just with a with using randomized SPD to see what we get.", "tokens": [400, 370, 321, 434, 516, 281, 853, 700, 445, 365, 257, 365, 1228, 38513, 19572, 281, 536, 437, 321, 483, 13], "temperature": 0.0, "avg_logprob": -0.20002295924168007, "compression_ratio": 1.3785714285714286, "no_speech_prob": 1.1478267879283521e-05}, {"id": 226, "seek": 200416, "start": 2004.16, "end": 2016.16, "text": " So we put in M and then I see I tried this with a few different. So the second parameter is just telling you the number of components you went back or the number of singular values.", "tokens": [407, 321, 829, 294, 376, 293, 550, 286, 536, 286, 3031, 341, 365, 257, 1326, 819, 13, 407, 264, 1150, 13075, 307, 445, 3585, 291, 264, 1230, 295, 6677, 291, 1437, 646, 420, 264, 1230, 295, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.13789808485243055, "compression_ratio": 1.5772727272727274, "no_speech_prob": 8.529985279892571e-06}, {"id": 227, "seek": 200416, "start": 2016.16, "end": 2022.16, "text": " And I actually felt like I got the best results with two here, but I got back U, S and V.", "tokens": [400, 286, 767, 2762, 411, 286, 658, 264, 1151, 3542, 365, 732, 510, 11, 457, 286, 658, 646, 624, 11, 318, 293, 691, 13], "temperature": 0.0, "avg_logprob": -0.13789808485243055, "compression_ratio": 1.5772727272727274, "no_speech_prob": 8.529985279892571e-06}, {"id": 228, "seek": 200416, "start": 2022.16, "end": 2029.16, "text": " And then I said my low rank matrix is U times and P dot diagonal S times V.", "tokens": [400, 550, 286, 848, 452, 2295, 6181, 8141, 307, 624, 1413, 293, 430, 5893, 21539, 318, 1413, 691, 13], "temperature": 0.0, "avg_logprob": -0.13789808485243055, "compression_ratio": 1.5772727272727274, "no_speech_prob": 8.529985279892571e-06}, {"id": 229, "seek": 202916, "start": 2029.16, "end": 2050.16, "text": " And this is what the low rank version looked like. Here I'm just first let me ask, why is this low rank when I do U times S times V?", "tokens": [400, 341, 307, 437, 264, 2295, 6181, 3037, 2956, 411, 13, 1692, 286, 478, 445, 700, 718, 385, 1029, 11, 983, 307, 341, 2295, 6181, 562, 286, 360, 624, 1413, 318, 1413, 691, 30], "temperature": 0.0, "avg_logprob": -0.21580766376696134, "compression_ratio": 1.2110091743119267, "no_speech_prob": 4.495103439694503e-06}, {"id": 230, "seek": 205016, "start": 2050.16, "end": 2074.16, "text": " Remember, U, S, V is the truncated SPD that we've asked for. Now we're multiplying those three matrices together.", "tokens": [5459, 11, 624, 11, 318, 11, 691, 307, 264, 504, 409, 66, 770, 19572, 300, 321, 600, 2351, 337, 13, 823, 321, 434, 30955, 729, 1045, 32284, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1638687885168827, "compression_ratio": 1.0970873786407767, "no_speech_prob": 5.337868969945703e-06}, {"id": 231, "seek": 207416, "start": 2074.16, "end": 2089.16, "text": " It's lowering because we only have two columns. Exactly. Yes. And so even though and let me go to the picture.", "tokens": [467, 311, 28124, 570, 321, 787, 362, 732, 13766, 13, 7587, 13, 1079, 13, 400, 370, 754, 1673, 293, 718, 385, 352, 281, 264, 3036, 13], "temperature": 0.0, "avg_logprob": -0.27727584838867186, "compression_ratio": 1.1111111111111112, "no_speech_prob": 2.994356691488065e-06}, {"id": 232, "seek": 208916, "start": 2089.16, "end": 2104.16, "text": " Even though use got a ton of non zero values and what S looks like is just. I don't know some value S 1 0 0 S 2.", "tokens": [2754, 1673, 764, 658, 257, 2952, 295, 2107, 4018, 4190, 293, 437, 318, 1542, 411, 307, 445, 13, 286, 500, 380, 458, 512, 2158, 318, 502, 1958, 1958, 318, 568, 13], "temperature": 0.0, "avg_logprob": -0.21691683133443196, "compression_ratio": 1.3821656050955413, "no_speech_prob": 6.962088264117483e-06}, {"id": 233, "seek": 208916, "start": 2104.16, "end": 2111.16, "text": " I would actually I guess that means that this. Sorry, this should only have two columns then to line up.", "tokens": [286, 576, 767, 286, 2041, 300, 1355, 300, 341, 13, 4919, 11, 341, 820, 787, 362, 732, 13766, 550, 281, 1622, 493, 13], "temperature": 0.0, "avg_logprob": -0.21691683133443196, "compression_ratio": 1.3821656050955413, "no_speech_prob": 6.962088264117483e-06}, {"id": 234, "seek": 211116, "start": 2111.16, "end": 2123.16, "text": " But the. Or no, you have two columns. OK, so you was like this. And so use very tall and has a lot of non zero values for that.", "tokens": [583, 264, 13, 1610, 572, 11, 291, 362, 732, 13766, 13, 2264, 11, 370, 291, 390, 411, 341, 13, 400, 370, 764, 588, 6764, 293, 575, 257, 688, 295, 2107, 4018, 4190, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.1827265759731861, "compression_ratio": 1.6169154228855722, "no_speech_prob": 3.966919393860735e-06}, {"id": 235, "seek": 211116, "start": 2123.16, "end": 2129.16, "text": " But as is just kind of got these two components and then we go to V is going to be.", "tokens": [583, 382, 307, 445, 733, 295, 658, 613, 732, 6677, 293, 550, 321, 352, 281, 691, 307, 516, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.1827265759731861, "compression_ratio": 1.6169154228855722, "no_speech_prob": 3.966919393860735e-06}, {"id": 236, "seek": 211116, "start": 2129.16, "end": 2137.16, "text": " Really long. And again, it's got a lot of non zero values known its long narrowness, but it's only only two rows.", "tokens": [4083, 938, 13, 400, 797, 11, 309, 311, 658, 257, 688, 295, 2107, 4018, 4190, 2570, 1080, 938, 6397, 648, 442, 11, 457, 309, 311, 787, 787, 732, 13241, 13], "temperature": 0.0, "avg_logprob": -0.1827265759731861, "compression_ratio": 1.6169154228855722, "no_speech_prob": 3.966919393860735e-06}, {"id": 237, "seek": 213716, "start": 2137.16, "end": 2145.16, "text": " And so when we multiply. And if you ask and be together, they're only going to be two.", "tokens": [400, 370, 562, 321, 12972, 13, 400, 498, 291, 1029, 293, 312, 1214, 11, 436, 434, 787, 516, 281, 312, 732, 13], "temperature": 0.0, "avg_logprob": -0.2466079145669937, "compression_ratio": 1.5872093023255813, "no_speech_prob": 1.209840092997183e-06}, {"id": 238, "seek": 213716, "start": 2145.16, "end": 2159.16, "text": " And two linearly independent columns in the result, we kind of can't get more complexity than that since we've only got two dimensions in these inputs of what we're multiplying together.", "tokens": [400, 732, 43586, 6695, 13766, 294, 264, 1874, 11, 321, 733, 295, 393, 380, 483, 544, 14024, 813, 300, 1670, 321, 600, 787, 658, 732, 12819, 294, 613, 15743, 295, 437, 321, 434, 30955, 1214, 13], "temperature": 0.0, "avg_logprob": -0.2466079145669937, "compression_ratio": 1.5872093023255813, "no_speech_prob": 1.209840092997183e-06}, {"id": 239, "seek": 215916, "start": 2159.16, "end": 2170.16, "text": " And so that's because. When you're multiplying those speaking to the microphone, when you multiply those.", "tokens": [400, 370, 300, 311, 570, 13, 1133, 291, 434, 30955, 729, 4124, 281, 264, 10952, 11, 562, 291, 12972, 729, 13], "temperature": 0.0, "avg_logprob": -0.3316205342610677, "compression_ratio": 1.530612244897959, "no_speech_prob": 5.3043921070639044e-05}, {"id": 240, "seek": 215916, "start": 2170.16, "end": 2181.16, "text": " Basis vectors in S. Each one of those other vectors is going to be essentially scaling that vector together value zero.", "tokens": [5859, 271, 18875, 294, 318, 13, 6947, 472, 295, 729, 661, 18875, 307, 516, 281, 312, 4476, 21589, 300, 8062, 1214, 2158, 4018, 13], "temperature": 0.0, "avg_logprob": -0.3316205342610677, "compression_ratio": 1.530612244897959, "no_speech_prob": 5.3043921070639044e-05}, {"id": 241, "seek": 218116, "start": 2181.16, "end": 2190.16, "text": " Right, so all like when you multiply S. Yes, we're all going to be. Adding the two basis vectors.", "tokens": [1779, 11, 370, 439, 411, 562, 291, 12972, 318, 13, 1079, 11, 321, 434, 439, 516, 281, 312, 13, 31204, 264, 732, 5143, 18875, 13], "temperature": 0.0, "avg_logprob": -0.2322198460611065, "compression_ratio": 1.5656108597285068, "no_speech_prob": 9.222540938935708e-06}, {"id": 242, "seek": 218116, "start": 2190.16, "end": 2198.16, "text": " Five zero and zero. I'm not sure the numbers, but it's just going to be those two. Yeah, so that's a good way of thinking about it.", "tokens": [9436, 4018, 293, 4018, 13, 286, 478, 406, 988, 264, 3547, 11, 457, 309, 311, 445, 516, 281, 312, 729, 732, 13, 865, 11, 370, 300, 311, 257, 665, 636, 295, 1953, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.2322198460611065, "compression_ratio": 1.5656108597285068, "no_speech_prob": 9.222540938935708e-06}, {"id": 243, "seek": 218116, "start": 2198.16, "end": 2205.16, "text": " So we talked last time about how you can think of matrix multiplication as taking linear combination of the columns.", "tokens": [407, 321, 2825, 1036, 565, 466, 577, 291, 393, 519, 295, 8141, 27290, 382, 1940, 8213, 6562, 295, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.2322198460611065, "compression_ratio": 1.5656108597285068, "no_speech_prob": 9.222540938935708e-06}, {"id": 244, "seek": 220516, "start": 2205.16, "end": 2213.16, "text": " So here when we do US what we're getting is. And actually let me.", "tokens": [407, 510, 562, 321, 360, 2546, 437, 321, 434, 1242, 307, 13, 400, 767, 718, 385, 13], "temperature": 0.0, "avg_logprob": -0.20241623454623753, "compression_ratio": 1.1473684210526316, "no_speech_prob": 1.1659043593681417e-05}, {"id": 245, "seek": 220516, "start": 2213.16, "end": 2231.16, "text": " I'm going to redraw this. Hold on a moment.", "tokens": [286, 478, 516, 281, 2182, 5131, 341, 13, 6962, 322, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.20241623454623753, "compression_ratio": 1.1473684210526316, "no_speech_prob": 1.1659043593681417e-05}, {"id": 246, "seek": 223116, "start": 2231.16, "end": 2239.16, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.25156712532043457, "compression_ratio": 1.0921052631578947, "no_speech_prob": 7.071254913171288e-06}, {"id": 247, "seek": 223116, "start": 2239.16, "end": 2244.16, "text": " So you is two columns you one.", "tokens": [407, 291, 307, 732, 13766, 291, 472, 13], "temperature": 0.0, "avg_logprob": -0.25156712532043457, "compression_ratio": 1.0921052631578947, "no_speech_prob": 7.071254913171288e-06}, {"id": 248, "seek": 223116, "start": 2244.16, "end": 2250.16, "text": " And you too, and so then when we do you times S.", "tokens": [400, 291, 886, 11, 293, 370, 550, 562, 321, 360, 291, 1413, 318, 13], "temperature": 0.0, "avg_logprob": -0.25156712532043457, "compression_ratio": 1.0921052631578947, "no_speech_prob": 7.071254913171288e-06}, {"id": 249, "seek": 225016, "start": 2250.16, "end": 2267.16, "text": " We're getting. I got something that's tall really that's just going to be S1 times you one is the first column and the second column is S2 times you too.", "tokens": [492, 434, 1242, 13, 286, 658, 746, 300, 311, 6764, 534, 300, 311, 445, 516, 281, 312, 318, 16, 1413, 291, 472, 307, 264, 700, 7738, 293, 264, 1150, 7738, 307, 318, 17, 1413, 291, 886, 13], "temperature": 0.0, "avg_logprob": -0.23419196030189252, "compression_ratio": 1.46875, "no_speech_prob": 1.0783067409647629e-05}, {"id": 250, "seek": 225016, "start": 2267.16, "end": 2272.16, "text": " Which is why you can replace the matrix product with an element wise application.", "tokens": [3013, 307, 983, 291, 393, 7406, 264, 8141, 1674, 365, 364, 4478, 10829, 3861, 13], "temperature": 0.0, "avg_logprob": -0.23419196030189252, "compression_ratio": 1.46875, "no_speech_prob": 1.0783067409647629e-05}, {"id": 251, "seek": 227216, "start": 2272.16, "end": 2280.16, "text": " So yes. And then when we multiply that by.", "tokens": [407, 2086, 13, 400, 550, 562, 321, 12972, 300, 538, 13], "temperature": 0.0, "avg_logprob": -0.42519756843303813, "compression_ratio": 0.9629629629629629, "no_speech_prob": 7.071721938700648e-06}, {"id": 252, "seek": 227216, "start": 2280.16, "end": 2292.16, "text": " Our columns, V or sorry Rose V1 V2.", "tokens": [2621, 13766, 11, 691, 420, 2597, 12765, 691, 16, 691, 17, 13], "temperature": 0.0, "avg_logprob": -0.42519756843303813, "compression_ratio": 0.9629629629629629, "no_speech_prob": 7.071721938700648e-06}, {"id": 253, "seek": 229216, "start": 2292.16, "end": 2305.16, "text": " I guess at that point we're kind of having to think about dot products, but you can see that like really we're kind of just taking these two long vectors and that's all we have to work with here.", "tokens": [286, 2041, 412, 300, 935, 321, 434, 733, 295, 1419, 281, 519, 466, 5893, 3383, 11, 457, 291, 393, 536, 300, 411, 534, 321, 434, 733, 295, 445, 1940, 613, 732, 938, 18875, 293, 300, 311, 439, 321, 362, 281, 589, 365, 510, 13], "temperature": 0.0, "avg_logprob": -0.08831407840435322, "compression_ratio": 1.5232558139534884, "no_speech_prob": 8.529982551408466e-06}, {"id": 254, "seek": 229216, "start": 2305.16, "end": 2319.16, "text": " We can also write out the dimension of these might be nice to see.", "tokens": [492, 393, 611, 2464, 484, 264, 10139, 295, 613, 1062, 312, 1481, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.08831407840435322, "compression_ratio": 1.5232558139534884, "no_speech_prob": 8.529982551408466e-06}, {"id": 255, "seek": 231916, "start": 2319.16, "end": 2324.16, "text": " Yes, you can see back you is 76,800 by two.", "tokens": [1079, 11, 291, 393, 536, 646, 291, 307, 24733, 11, 14423, 538, 732, 13], "temperature": 0.0, "avg_logprob": -0.2022232766878807, "compression_ratio": 1.3741496598639455, "no_speech_prob": 1.3210458746470977e-05}, {"id": 256, "seek": 231916, "start": 2324.16, "end": 2331.16, "text": " This is just two and V is two by 11,300.", "tokens": [639, 307, 445, 732, 293, 691, 307, 732, 538, 2975, 11, 12566, 13], "temperature": 0.0, "avg_logprob": -0.2022232766878807, "compression_ratio": 1.3741496598639455, "no_speech_prob": 1.3210458746470977e-05}, {"id": 257, "seek": 231916, "start": 2331.16, "end": 2339.16, "text": " And so that's kind of showing showing up when we plot this picture and actually I could plot the whole picture again.", "tokens": [400, 370, 300, 311, 733, 295, 4099, 4099, 493, 562, 321, 7542, 341, 3036, 293, 767, 286, 727, 7542, 264, 1379, 3036, 797, 13], "temperature": 0.0, "avg_logprob": -0.2022232766878807, "compression_ratio": 1.3741496598639455, "no_speech_prob": 1.3210458746470977e-05}, {"id": 258, "seek": 233916, "start": 2339.16, "end": 2359.16, "text": " I think that would be good to see.", "tokens": [286, 519, 300, 576, 312, 665, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.09840972606952374, "compression_ratio": 0.85, "no_speech_prob": 8.2675032899715e-06}, {"id": 259, "seek": 235916, "start": 2359.16, "end": 2370.16, "text": " Okay, this is a little bit slow, so we'll look at the picture of the one. Basically, it's got to have practically the same picture for everything.", "tokens": [1033, 11, 341, 307, 257, 707, 857, 2964, 11, 370, 321, 603, 574, 412, 264, 3036, 295, 264, 472, 13, 8537, 11, 309, 311, 658, 281, 362, 15667, 264, 912, 3036, 337, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1353338427013821, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.19065009332553e-06}, {"id": 260, "seek": 235916, "start": 2370.16, "end": 2382.16, "text": " I guess I mean I guess you could have two different pictures that it could show that it's really not able to capture a whole video's worth of data.", "tokens": [286, 2041, 286, 914, 286, 2041, 291, 727, 362, 732, 819, 5242, 300, 309, 727, 855, 300, 309, 311, 534, 406, 1075, 281, 7983, 257, 1379, 960, 311, 3163, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1353338427013821, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.19065009332553e-06}, {"id": 261, "seek": 238216, "start": 2382.16, "end": 2396.16, "text": " Any other questions about this, Kelsey.", "tokens": [2639, 661, 1651, 466, 341, 11, 44714, 13], "temperature": 0.0, "avg_logprob": -0.2287472126095794, "compression_ratio": 1.2755905511811023, "no_speech_prob": 6.240525181056e-06}, {"id": 262, "seek": 238216, "start": 2396.16, "end": 2404.16, "text": " Oh, yeah, no I encourage you to try this with different ones I just tried it with a few and thought that to look the best.", "tokens": [876, 11, 1338, 11, 572, 286, 5373, 291, 281, 853, 341, 365, 819, 2306, 286, 445, 3031, 309, 365, 257, 1326, 293, 1194, 300, 281, 574, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.2287472126095794, "compression_ratio": 1.2755905511811023, "no_speech_prob": 6.240525181056e-06}, {"id": 263, "seek": 240416, "start": 2404.16, "end": 2420.16, "text": " Try it with others. I think though some of the idea is that like, unless you're doing a huge rank like I don't know, so if you were doing like rank 1000 it's like maybe you are trying to capture all the different places people can be or like capture the information about people.", "tokens": [6526, 309, 365, 2357, 13, 286, 519, 1673, 512, 295, 264, 1558, 307, 300, 411, 11, 5969, 291, 434, 884, 257, 2603, 6181, 411, 286, 500, 380, 458, 11, 370, 498, 291, 645, 884, 411, 6181, 9714, 309, 311, 411, 1310, 291, 366, 1382, 281, 7983, 439, 264, 819, 3190, 561, 393, 312, 420, 411, 7983, 264, 1589, 466, 561, 13], "temperature": 0.4, "avg_logprob": -0.17960768200102306, "compression_ratio": 1.73046875, "no_speech_prob": 1.1124347111035604e-05}, {"id": 264, "seek": 240416, "start": 2420.16, "end": 2430.16, "text": " But beyond that, I don't know like with a rank of 10 there's not really 10 pieces of information you can capture, you know, I mean you kind of have the background.", "tokens": [583, 4399, 300, 11, 286, 500, 380, 458, 411, 365, 257, 6181, 295, 1266, 456, 311, 406, 534, 1266, 3755, 295, 1589, 291, 393, 7983, 11, 291, 458, 11, 286, 914, 291, 733, 295, 362, 264, 3678, 13], "temperature": 0.4, "avg_logprob": -0.17960768200102306, "compression_ratio": 1.73046875, "no_speech_prob": 1.1124347111035604e-05}, {"id": 265, "seek": 243016, "start": 2430.16, "end": 2439.16, "text": " And then like the next piece of information is like all this information about people.", "tokens": [400, 550, 411, 264, 958, 2522, 295, 1589, 307, 411, 439, 341, 1589, 466, 561, 13], "temperature": 0.0, "avg_logprob": -0.1353748452429678, "compression_ratio": 1.3759398496240602, "no_speech_prob": 7.295956947928062e-06}, {"id": 266, "seek": 243016, "start": 2439.16, "end": 2445.16, "text": " This is sorry, I don't know why this hasn't shown up yet this is quite slow.", "tokens": [639, 307, 2597, 11, 286, 500, 380, 458, 983, 341, 6132, 380, 4898, 493, 1939, 341, 307, 1596, 2964, 13], "temperature": 0.0, "avg_logprob": -0.1353748452429678, "compression_ratio": 1.3759398496240602, "no_speech_prob": 7.295956947928062e-06}, {"id": 267, "seek": 243016, "start": 2445.16, "end": 2452.16, "text": " Oh, let me do that.", "tokens": [876, 11, 718, 385, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1353748452429678, "compression_ratio": 1.3759398496240602, "no_speech_prob": 7.295956947928062e-06}, {"id": 268, "seek": 245216, "start": 2452.16, "end": 2460.16, "text": " Although I must if the other ones are showing up.", "tokens": [5780, 286, 1633, 498, 264, 661, 2306, 366, 4099, 493, 13], "temperature": 0.0, "avg_logprob": -0.3581803526197161, "compression_ratio": 1.058139534883721, "no_speech_prob": 6.603476504096761e-05}, {"id": 269, "seek": 245216, "start": 2460.16, "end": 2465.16, "text": " Oh, and Sam Scott a question in the back.", "tokens": [876, 11, 293, 4832, 6659, 257, 1168, 294, 264, 646, 13], "temperature": 0.0, "avg_logprob": -0.3581803526197161, "compression_ratio": 1.058139534883721, "no_speech_prob": 6.603476504096761e-05}, {"id": 270, "seek": 246516, "start": 2465.16, "end": 2483.16, "text": " In the SK learn randomized SPD.", "tokens": [682, 264, 21483, 1466, 38513, 19572, 13], "temperature": 0.0, "avg_logprob": -0.6791247888044878, "compression_ratio": 0.7948717948717948, "no_speech_prob": 1.6183641491807066e-05}, {"id": 271, "seek": 248316, "start": 2483.16, "end": 2495.16, "text": " It is yes.", "tokens": [467, 307, 2086, 13], "temperature": 0.0, "avg_logprob": -0.3239381479662518, "compression_ratio": 1.2, "no_speech_prob": 2.7102170861326158e-05}, {"id": 272, "seek": 248316, "start": 2495.16, "end": 2498.16, "text": " Default actually is 10 or 15 it's.", "tokens": [9548, 5107, 767, 307, 1266, 420, 2119, 309, 311, 13], "temperature": 0.0, "avg_logprob": -0.3239381479662518, "compression_ratio": 1.2, "no_speech_prob": 2.7102170861326158e-05}, {"id": 273, "seek": 248316, "start": 2498.16, "end": 2504.16, "text": " Yeah, I think this is coming from a from a paper from play from the HALCO paper.", "tokens": [865, 11, 286, 519, 341, 307, 1348, 490, 257, 490, 257, 3035, 490, 862, 490, 264, 389, 3427, 12322, 3035, 13], "temperature": 0.0, "avg_logprob": -0.3239381479662518, "compression_ratio": 1.2, "no_speech_prob": 2.7102170861326158e-05}, {"id": 274, "seek": 250416, "start": 2504.16, "end": 2517.16, "text": " So this is. Yeah, so that's it. That is what SK learn is doing. I don't I don't think that's something you would want to tune yourself unless you had like specific reason to think that that would be helpful.", "tokens": [407, 341, 307, 13, 865, 11, 370, 300, 311, 309, 13, 663, 307, 437, 21483, 1466, 307, 884, 13, 286, 500, 380, 286, 500, 380, 519, 300, 311, 746, 291, 576, 528, 281, 10864, 1803, 5969, 291, 632, 411, 2685, 1778, 281, 519, 300, 300, 576, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1177562117576599, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4737485798832495e-05}, {"id": 275, "seek": 250416, "start": 2517.16, "end": 2524.16, "text": " But I think like the kind of the papers develop this like theory of why this should work, or why this does work.", "tokens": [583, 286, 519, 411, 264, 733, 295, 264, 10577, 1499, 341, 411, 5261, 295, 983, 341, 820, 589, 11, 420, 983, 341, 775, 589, 13], "temperature": 0.0, "avg_logprob": -0.1177562117576599, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4737485798832495e-05}, {"id": 276, "seek": 252416, "start": 2524.16, "end": 2540.16, "text": " So in trying to do it why why we get back just the background doing this would you say that it's because given that the", "tokens": [407, 294, 1382, 281, 360, 309, 983, 983, 321, 483, 646, 445, 264, 3678, 884, 341, 576, 291, 584, 300, 309, 311, 570, 2212, 300, 264], "temperature": 0.0, "avg_logprob": -0.28809585571289065, "compression_ratio": 1.2659574468085106, "no_speech_prob": 0.00012327535660006106}, {"id": 277, "seek": 254016, "start": 2540.16, "end": 2555.16, "text": " vectors that make up the background in the matrix over time, sort of our most important across entire matrix.", "tokens": [18875, 300, 652, 493, 264, 3678, 294, 264, 8141, 670, 565, 11, 1333, 295, 527, 881, 1021, 2108, 2302, 8141, 13], "temperature": 0.0, "avg_logprob": -0.20994241714477538, "compression_ratio": 1.2247191011235956, "no_speech_prob": 0.00011228524817852303}, {"id": 278, "seek": 255516, "start": 2555.16, "end": 2570.16, "text": " The skills are largest, as opposed to the vectors that correspond to people moving since they're very sparse in there. When you limit it to a few singular values. It's going to murder. Right. Yeah, like the most because the most important component.", "tokens": [440, 3942, 366, 6443, 11, 382, 8851, 281, 264, 18875, 300, 6805, 281, 561, 2684, 1670, 436, 434, 588, 637, 11668, 294, 456, 13, 1133, 291, 4948, 309, 281, 257, 1326, 20010, 4190, 13, 467, 311, 516, 281, 6568, 13, 1779, 13, 865, 11, 411, 264, 881, 570, 264, 881, 1021, 6542, 13], "temperature": 0.0, "avg_logprob": -0.26637082350881475, "compression_ratio": 1.439306358381503, "no_speech_prob": 7.60005641495809e-05}, {"id": 279, "seek": 257016, "start": 2570.16, "end": 2585.16, "text": " There's I guess there's two things going on. One is that really in any frame, like most of the frame is background, but it's also like looking across time the background is what shows up in like every single point in time like you always have the background there and I mean sometimes you know pieces of", "tokens": [821, 311, 286, 2041, 456, 311, 732, 721, 516, 322, 13, 1485, 307, 300, 534, 294, 604, 3920, 11, 411, 881, 295, 264, 3920, 307, 3678, 11, 457, 309, 311, 611, 411, 1237, 2108, 565, 264, 3678, 307, 437, 3110, 493, 294, 411, 633, 2167, 935, 294, 565, 411, 291, 1009, 362, 264, 3678, 456, 293, 286, 914, 2171, 291, 458, 3755, 295], "temperature": 0.0, "avg_logprob": -0.12515479415210326, "compression_ratio": 1.6290322580645162, "no_speech_prob": 7.14110501576215e-05}, {"id": 280, "seek": 258516, "start": 2585.16, "end": 2601.16, "text": " are obscured by a person. But for the most part, the, the background. I guess a way to think about it even as if you're looking at this whole picture like most of this picture is the horizontal lines that form the background.", "tokens": [366, 22082, 3831, 538, 257, 954, 13, 583, 337, 264, 881, 644, 11, 264, 11, 264, 3678, 13, 286, 2041, 257, 636, 281, 519, 466, 309, 754, 382, 498, 291, 434, 1237, 412, 341, 1379, 3036, 411, 881, 295, 341, 3036, 307, 264, 12750, 3876, 300, 1254, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.10467886924743652, "compression_ratio": 1.6540284360189574, "no_speech_prob": 8.939107829064596e-06}, {"id": 281, "seek": 258516, "start": 2601.16, "end": 2612.16, "text": " So those are going to be the largest, the largest singular value because they're kind of like the most important component.", "tokens": [407, 729, 366, 516, 281, 312, 264, 6443, 11, 264, 6443, 20010, 2158, 570, 436, 434, 733, 295, 411, 264, 881, 1021, 6542, 13], "temperature": 0.0, "avg_logprob": -0.10467886924743652, "compression_ratio": 1.6540284360189574, "no_speech_prob": 8.939107829064596e-06}, {"id": 282, "seek": 261216, "start": 2612.16, "end": 2625.16, "text": " And by the way, you mentioned to send about 1000 kind of a source code, a cool tip I don't think we've mentioned is if you type in your question mark question mark and then the name of the thing.", "tokens": [400, 538, 264, 636, 11, 291, 2835, 281, 2845, 466, 9714, 733, 295, 257, 4009, 3089, 11, 257, 1627, 4125, 286, 500, 380, 519, 321, 600, 2835, 307, 498, 291, 2010, 294, 428, 1168, 1491, 1168, 1491, 293, 550, 264, 1315, 295, 264, 551, 13], "temperature": 0.0, "avg_logprob": -0.33652961860268804, "compression_ratio": 1.523489932885906, "no_speech_prob": 0.00017942478007171303}, {"id": 283, "seek": 261216, "start": 2625.16, "end": 2628.16, "text": " It'll show you the source code.", "tokens": [467, 603, 855, 291, 264, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.33652961860268804, "compression_ratio": 1.523489932885906, "no_speech_prob": 0.00017942478007171303}, {"id": 284, "seek": 262816, "start": 2628.16, "end": 2644.16, "text": " And actually, yeah my Colonel died I need to rerun a few things.", "tokens": [400, 767, 11, 1338, 452, 28478, 4539, 286, 643, 281, 43819, 409, 257, 1326, 721, 13], "temperature": 0.0, "avg_logprob": -0.23676387468973795, "compression_ratio": 1.1428571428571428, "no_speech_prob": 3.1874753858573968e-06}, {"id": 285, "seek": 262816, "start": 2644.16, "end": 2648.16, "text": " Yeah, that's a that's a great point. Thank you.", "tokens": [865, 11, 300, 311, 257, 300, 311, 257, 869, 935, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.23676387468973795, "compression_ratio": 1.1428571428571428, "no_speech_prob": 3.1874753858573968e-06}, {"id": 286, "seek": 264816, "start": 2648.16, "end": 2665.16, "text": " I'm just going to rerun this because I do want to have the matrix to be able to use.", "tokens": [286, 478, 445, 516, 281, 43819, 409, 341, 570, 286, 360, 528, 281, 362, 264, 8141, 281, 312, 1075, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.1261442408842199, "compression_ratio": 1.5157232704402517, "no_speech_prob": 1.7880271343528875e-06}, {"id": 287, "seek": 264816, "start": 2665.16, "end": 2671.16, "text": " I was going to say going back to this, I'm kind of talking about rank and creating the data matrix is a little bit slow.", "tokens": [286, 390, 516, 281, 584, 516, 646, 281, 341, 11, 286, 478, 733, 295, 1417, 466, 6181, 293, 4084, 264, 1412, 8141, 307, 257, 707, 857, 2964, 13], "temperature": 0.0, "avg_logprob": -0.1261442408842199, "compression_ratio": 1.5157232704402517, "no_speech_prob": 1.7880271343528875e-06}, {"id": 288, "seek": 264816, "start": 2671.16, "end": 2675.16, "text": " So I have to wait a moment on that.", "tokens": [407, 286, 362, 281, 1699, 257, 1623, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.1261442408842199, "compression_ratio": 1.5157232704402517, "no_speech_prob": 1.7880271343528875e-06}, {"id": 289, "seek": 267516, "start": 2675.16, "end": 2689.16, "text": " I actually expect rank one to be pretty good. And I think I did it and found that rank two was for some reason better, but really like the background should be able to perfectly be captured by a rank one matrix.", "tokens": [286, 767, 2066, 6181, 472, 281, 312, 1238, 665, 13, 400, 286, 519, 286, 630, 309, 293, 1352, 300, 6181, 732, 390, 337, 512, 1778, 1101, 11, 457, 534, 411, 264, 3678, 820, 312, 1075, 281, 6239, 312, 11828, 538, 257, 6181, 472, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10514951238826829, "compression_ratio": 1.4256756756756757, "no_speech_prob": 1.3844697605236433e-05}, {"id": 290, "seek": 268916, "start": 2689.16, "end": 2706.16, "text": " So you've unrolled you know 2d space, so that a single picture can show up just as one column and so really the column just of the background.", "tokens": [407, 291, 600, 517, 28850, 291, 458, 568, 67, 1901, 11, 370, 300, 257, 2167, 3036, 393, 855, 493, 445, 382, 472, 7738, 293, 370, 534, 264, 7738, 445, 295, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.16034253867896828, "compression_ratio": 1.2678571428571428, "no_speech_prob": 1.3629583008878399e-05}, {"id": 291, "seek": 270616, "start": 2706.16, "end": 2722.16, "text": " Is running now.", "tokens": [1119, 2614, 586, 13], "temperature": 0.6, "avg_logprob": -0.5082488457361857, "compression_ratio": 0.7241379310344828, "no_speech_prob": 7.1820304583525285e-06}, {"id": 292, "seek": 270616, "start": 2722.16, "end": 2724.16, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.6, "avg_logprob": -0.5082488457361857, "compression_ratio": 0.7241379310344828, "no_speech_prob": 7.1820304583525285e-06}, {"id": 293, "seek": 272416, "start": 2724.16, "end": 2728.52, "text": " So this is our low rank reconstruction.", "tokens": [407, 341, 307, 527, 2295, 6181, 31565, 13], "temperature": 0.0, "avg_logprob": -0.2265899412093624, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.04601031541824341}, {"id": 294, "seek": 272416, "start": 2728.52, "end": 2734.48, "text": " So we did the SBD and then we've reconstructed our matrix.", "tokens": [407, 321, 630, 264, 26944, 35, 293, 550, 321, 600, 31499, 292, 527, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2265899412093624, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.04601031541824341}, {"id": 295, "seek": 272416, "start": 2734.48, "end": 2739.04, "text": " So if you think back to the math standpoint,", "tokens": [407, 498, 291, 519, 646, 281, 264, 5221, 15827, 11], "temperature": 0.0, "avg_logprob": -0.2265899412093624, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.04601031541824341}, {"id": 296, "seek": 272416, "start": 2739.04, "end": 2744.7999999999997, "text": " what's going on is we're trying to reconstruct this matrix.", "tokens": [437, 311, 516, 322, 307, 321, 434, 1382, 281, 31499, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2265899412093624, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.04601031541824341}, {"id": 297, "seek": 272416, "start": 2744.7999999999997, "end": 2746.8399999999997, "text": " So you could think of this as a data compression problem,", "tokens": [407, 291, 727, 519, 295, 341, 382, 257, 1412, 19355, 1154, 11], "temperature": 0.0, "avg_logprob": -0.2265899412093624, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.04601031541824341}, {"id": 298, "seek": 272416, "start": 2746.8399999999997, "end": 2749.12, "text": " like you didn't want to have to store all of this.", "tokens": [411, 291, 994, 380, 528, 281, 362, 281, 3531, 439, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.2265899412093624, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.04601031541824341}, {"id": 299, "seek": 272416, "start": 2749.12, "end": 2752.2, "text": " So with just rank two,", "tokens": [407, 365, 445, 6181, 732, 11], "temperature": 0.0, "avg_logprob": -0.2265899412093624, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.04601031541824341}, {"id": 300, "seek": 275220, "start": 2752.2, "end": 2755.3599999999997, "text": " so just these two columns in U,", "tokens": [370, 445, 613, 732, 13766, 294, 624, 11], "temperature": 0.0, "avg_logprob": -0.1878299145471482, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.2410700946929865e-05}, {"id": 301, "seek": 275220, "start": 2755.3599999999997, "end": 2757.8399999999997, "text": " two singular values, two rows in V,", "tokens": [732, 20010, 4190, 11, 732, 13241, 294, 691, 11], "temperature": 0.0, "avg_logprob": -0.1878299145471482, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.2410700946929865e-05}, {"id": 302, "seek": 275220, "start": 2757.8399999999997, "end": 2762.04, "text": " we get this much of our original matrix, which is a lot of it.", "tokens": [321, 483, 341, 709, 295, 527, 3380, 8141, 11, 597, 307, 257, 688, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1878299145471482, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.2410700946929865e-05}, {"id": 303, "seek": 275220, "start": 2762.04, "end": 2766.24, "text": " Kind of like everything except the wavy black lines.", "tokens": [9242, 295, 411, 1203, 3993, 264, 261, 15498, 2211, 3876, 13], "temperature": 0.0, "avg_logprob": -0.1878299145471482, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.2410700946929865e-05}, {"id": 304, "seek": 275220, "start": 2772.0, "end": 2778.96, "text": " So then, and a more exciting part I think is the people that we want to see.", "tokens": [407, 550, 11, 293, 257, 544, 4670, 644, 286, 519, 307, 264, 561, 300, 321, 528, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.1878299145471482, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.2410700946929865e-05}, {"id": 305, "seek": 275220, "start": 2778.96, "end": 2780.7999999999997, "text": " Although it's easier to pick out", "tokens": [5780, 309, 311, 3571, 281, 1888, 484], "temperature": 0.0, "avg_logprob": -0.1878299145471482, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.2410700946929865e-05}, {"id": 306, "seek": 278080, "start": 2780.8, "end": 2783.5600000000004, "text": " the background because that's what we have information about.", "tokens": [264, 3678, 570, 300, 311, 437, 321, 362, 1589, 466, 13], "temperature": 0.0, "avg_logprob": -0.1774531654689623, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.3550162950414233e-05}, {"id": 307, "seek": 278080, "start": 2783.5600000000004, "end": 2791.36, "text": " So what we'll do is just subtract our original matrix minus the low rank approximation,", "tokens": [407, 437, 321, 603, 360, 307, 445, 16390, 527, 3380, 8141, 3175, 264, 2295, 6181, 28023, 11], "temperature": 0.0, "avg_logprob": -0.1774531654689623, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.3550162950414233e-05}, {"id": 308, "seek": 278080, "start": 2791.36, "end": 2792.92, "text": " which is just the background.", "tokens": [597, 307, 445, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.1774531654689623, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.3550162950414233e-05}, {"id": 309, "seek": 278080, "start": 2792.92, "end": 2795.76, "text": " So that hopefully will give us the people.", "tokens": [407, 300, 4696, 486, 976, 505, 264, 561, 13], "temperature": 0.0, "avg_logprob": -0.1774531654689623, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.3550162950414233e-05}, {"id": 310, "seek": 278080, "start": 2795.76, "end": 2799.7200000000003, "text": " So you can see here, this is what's left.", "tokens": [407, 291, 393, 536, 510, 11, 341, 307, 437, 311, 1411, 13], "temperature": 0.0, "avg_logprob": -0.1774531654689623, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.3550162950414233e-05}, {"id": 311, "seek": 278080, "start": 2799.7200000000003, "end": 2804.0, "text": " Here I'm just looking at a single point in time, 140.", "tokens": [1692, 286, 478, 445, 1237, 412, 257, 2167, 935, 294, 565, 11, 21548, 13], "temperature": 0.0, "avg_logprob": -0.1774531654689623, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.3550162950414233e-05}, {"id": 312, "seek": 278080, "start": 2804.0, "end": 2808.2000000000003, "text": " Actually, let me copy that.", "tokens": [5135, 11, 718, 385, 5055, 300, 13], "temperature": 0.0, "avg_logprob": -0.1774531654689623, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.3550162950414233e-05}, {"id": 313, "seek": 280820, "start": 2808.2, "end": 2814.48, "text": " So you can do this at different points in time.", "tokens": [407, 291, 393, 360, 341, 412, 819, 2793, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.24549298537404915, "compression_ratio": 1.4417177914110428, "no_speech_prob": 1.3845319699612446e-05}, {"id": 314, "seek": 280820, "start": 2816.72, "end": 2819.0, "text": " The other one was, yeah,", "tokens": [440, 661, 472, 390, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.24549298537404915, "compression_ratio": 1.4417177914110428, "no_speech_prob": 1.3845319699612446e-05}, {"id": 315, "seek": 280820, "start": 2819.0, "end": 2821.56, "text": " but now I've written over my variables.", "tokens": [457, 586, 286, 600, 3720, 670, 452, 9102, 13], "temperature": 0.0, "avg_logprob": -0.24549298537404915, "compression_ratio": 1.4417177914110428, "no_speech_prob": 1.3845319699612446e-05}, {"id": 316, "seek": 280820, "start": 2821.56, "end": 2826.08, "text": " So yeah, so the low-res version doesn't show quite as much.", "tokens": [407, 1338, 11, 370, 264, 2295, 12, 495, 3037, 1177, 380, 855, 1596, 382, 709, 13], "temperature": 0.0, "avg_logprob": -0.24549298537404915, "compression_ratio": 1.4417177914110428, "no_speech_prob": 1.3845319699612446e-05}, {"id": 317, "seek": 280820, "start": 2826.08, "end": 2831.8799999999997, "text": " Actually, I don't. Here.", "tokens": [5135, 11, 286, 500, 380, 13, 1692, 13], "temperature": 0.0, "avg_logprob": -0.24549298537404915, "compression_ratio": 1.4417177914110428, "no_speech_prob": 1.3845319699612446e-05}, {"id": 318, "seek": 280820, "start": 2833.16, "end": 2835.96, "text": " Yeah. So this is the low-res version.", "tokens": [865, 13, 407, 341, 307, 264, 2295, 12, 495, 3037, 13], "temperature": 0.0, "avg_logprob": -0.24549298537404915, "compression_ratio": 1.4417177914110428, "no_speech_prob": 1.3845319699612446e-05}, {"id": 319, "seek": 283596, "start": 2835.96, "end": 2839.04, "text": " So you can't see as clearly, but I was trying to high-res.", "tokens": [407, 291, 393, 380, 536, 382, 4448, 11, 457, 286, 390, 1382, 281, 1090, 12, 495, 13], "temperature": 0.0, "avg_logprob": -0.1820031260396098, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.0260856672393857e-06}, {"id": 320, "seek": 283596, "start": 2839.04, "end": 2842.52, "text": " I recommend staying with low-res just for the speed though.", "tokens": [286, 2748, 7939, 365, 2295, 12, 495, 445, 337, 264, 3073, 1673, 13], "temperature": 0.0, "avg_logprob": -0.1820031260396098, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.0260856672393857e-06}, {"id": 321, "seek": 283596, "start": 2842.52, "end": 2847.08, "text": " You can see what the people are doing at different points.", "tokens": [509, 393, 536, 437, 264, 561, 366, 884, 412, 819, 2793, 13], "temperature": 0.0, "avg_logprob": -0.1820031260396098, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.0260856672393857e-06}, {"id": 322, "seek": 283596, "start": 2849.08, "end": 2855.84, "text": " It's not perfect. We do have these light marks around the people.", "tokens": [467, 311, 406, 2176, 13, 492, 360, 362, 613, 1442, 10640, 926, 264, 561, 13], "temperature": 0.0, "avg_logprob": -0.1820031260396098, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.0260856672393857e-06}, {"id": 323, "seek": 283596, "start": 2856.56, "end": 2858.8, "text": " Here in the high-res version,", "tokens": [1692, 294, 264, 1090, 12, 495, 3037, 11], "temperature": 0.0, "avg_logprob": -0.1820031260396098, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.0260856672393857e-06}, {"id": 324, "seek": 283596, "start": 2858.8, "end": 2863.08, "text": " you can see that the sign that was in the picture,", "tokens": [291, 393, 536, 300, 264, 1465, 300, 390, 294, 264, 3036, 11], "temperature": 0.0, "avg_logprob": -0.1820031260396098, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.0260856672393857e-06}, {"id": 325, "seek": 286308, "start": 2863.08, "end": 2866.68, "text": " or at least I guess it's white space", "tokens": [420, 412, 1935, 286, 2041, 309, 311, 2418, 1901], "temperature": 0.0, "avg_logprob": -0.21846371818991267, "compression_ratio": 1.5808080808080809, "no_speech_prob": 6.74787725074566e-06}, {"id": 326, "seek": 286308, "start": 2866.68, "end": 2869.04, "text": " around the sign was getting captured.", "tokens": [926, 264, 1465, 390, 1242, 11828, 13], "temperature": 0.0, "avg_logprob": -0.21846371818991267, "compression_ratio": 1.5808080808080809, "no_speech_prob": 6.74787725074566e-06}, {"id": 327, "seek": 286308, "start": 2869.04, "end": 2871.88, "text": " But it's pretty good.", "tokens": [583, 309, 311, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.21846371818991267, "compression_ratio": 1.5808080808080809, "no_speech_prob": 6.74787725074566e-06}, {"id": 328, "seek": 286308, "start": 2875.4, "end": 2877.96, "text": " That's what we get from randomized SVD,", "tokens": [663, 311, 437, 321, 483, 490, 38513, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.21846371818991267, "compression_ratio": 1.5808080808080809, "no_speech_prob": 6.74787725074566e-06}, {"id": 329, "seek": 286308, "start": 2877.96, "end": 2881.88, "text": " which is a pretty simple approach for this.", "tokens": [597, 307, 257, 1238, 2199, 3109, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.21846371818991267, "compression_ratio": 1.5808080808080809, "no_speech_prob": 6.74787725074566e-06}, {"id": 330, "seek": 286308, "start": 2882.24, "end": 2888.84, "text": " So then we're going through a more complicated algorithm,", "tokens": [407, 550, 321, 434, 516, 807, 257, 544, 6179, 9284, 11], "temperature": 0.0, "avg_logprob": -0.21846371818991267, "compression_ratio": 1.5808080808080809, "no_speech_prob": 6.74787725074566e-06}, {"id": 331, "seek": 286308, "start": 2888.84, "end": 2890.48, "text": " and this is the result that we'll get with", "tokens": [293, 341, 307, 264, 1874, 300, 321, 603, 483, 365], "temperature": 0.0, "avg_logprob": -0.21846371818991267, "compression_ratio": 1.5808080808080809, "no_speech_prob": 6.74787725074566e-06}, {"id": 332, "seek": 286308, "start": 2890.48, "end": 2893.04, "text": " the more complicated algorithm.", "tokens": [264, 544, 6179, 9284, 13], "temperature": 0.0, "avg_logprob": -0.21846371818991267, "compression_ratio": 1.5808080808080809, "no_speech_prob": 6.74787725074566e-06}, {"id": 333, "seek": 289304, "start": 2893.04, "end": 2895.88, "text": " Which doesn't have the issue with the sign,", "tokens": [3013, 1177, 380, 362, 264, 2734, 365, 264, 1465, 11], "temperature": 0.0, "avg_logprob": -0.14402807860815225, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.8341965187573805e-05}, {"id": 334, "seek": 289304, "start": 2895.88, "end": 2897.72, "text": " so I think it is a bit better.", "tokens": [370, 286, 519, 309, 307, 257, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.14402807860815225, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.8341965187573805e-05}, {"id": 335, "seek": 289304, "start": 2897.72, "end": 2901.0, "text": " I'll be honest, I was disappointed that it wasn't more", "tokens": [286, 603, 312, 3245, 11, 286, 390, 13856, 300, 309, 2067, 380, 544], "temperature": 0.0, "avg_logprob": -0.14402807860815225, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.8341965187573805e-05}, {"id": 336, "seek": 289304, "start": 2901.0, "end": 2905.36, "text": " better because it is a lot more complicated.", "tokens": [1101, 570, 309, 307, 257, 688, 544, 6179, 13], "temperature": 0.0, "avg_logprob": -0.14402807860815225, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.8341965187573805e-05}, {"id": 337, "seek": 289304, "start": 2905.7599999999998, "end": 2912.12, "text": " But it's nice because it's also an example of robust PCA,", "tokens": [583, 309, 311, 1481, 570, 309, 311, 611, 364, 1365, 295, 13956, 6465, 32, 11], "temperature": 0.0, "avg_logprob": -0.14402807860815225, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.8341965187573805e-05}, {"id": 338, "seek": 289304, "start": 2912.12, "end": 2914.68, "text": " and we'll be able to talk about a lot of issues that arise.", "tokens": [293, 321, 603, 312, 1075, 281, 751, 466, 257, 688, 295, 2663, 300, 20288, 13], "temperature": 0.0, "avg_logprob": -0.14402807860815225, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.8341965187573805e-05}, {"id": 339, "seek": 289304, "start": 2914.68, "end": 2917.72, "text": " It still uses randomized SVD as part of it.", "tokens": [467, 920, 4960, 38513, 31910, 35, 382, 644, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.14402807860815225, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.8341965187573805e-05}, {"id": 340, "seek": 289304, "start": 2917.72, "end": 2920.04, "text": " So basically, we have a more complicated algorithm", "tokens": [407, 1936, 11, 321, 362, 257, 544, 6179, 9284], "temperature": 0.0, "avg_logprob": -0.14402807860815225, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.8341965187573805e-05}, {"id": 341, "seek": 289304, "start": 2920.04, "end": 2922.84, "text": " that's iteratively using randomized SVD", "tokens": [300, 311, 17138, 19020, 1228, 38513, 31910, 35], "temperature": 0.0, "avg_logprob": -0.14402807860815225, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.8341965187573805e-05}, {"id": 342, "seek": 292284, "start": 2922.84, "end": 2925.1200000000003, "text": " for one of its steps.", "tokens": [337, 472, 295, 1080, 4439, 13], "temperature": 0.0, "avg_logprob": -0.24594405038016184, "compression_ratio": 1.2721518987341771, "no_speech_prob": 7.409880254272139e-06}, {"id": 343, "seek": 292284, "start": 2925.36, "end": 2929.28, "text": " Oh, okay. I saw it's about 12 o'clock.", "tokens": [876, 11, 1392, 13, 286, 1866, 309, 311, 466, 2272, 277, 6, 9023, 13], "temperature": 0.0, "avg_logprob": -0.24594405038016184, "compression_ratio": 1.2721518987341771, "no_speech_prob": 7.409880254272139e-06}, {"id": 344, "seek": 292284, "start": 2929.28, "end": 2933.4, "text": " So this is probably a good time to stop for our break.", "tokens": [407, 341, 307, 1391, 257, 665, 565, 281, 1590, 337, 527, 1821, 13], "temperature": 0.0, "avg_logprob": -0.24594405038016184, "compression_ratio": 1.2721518987341771, "no_speech_prob": 7.409880254272139e-06}, {"id": 345, "seek": 292284, "start": 2933.4, "end": 2935.96, "text": " We can meet back at 12.07.", "tokens": [492, 393, 1677, 646, 412, 2272, 13, 16231, 13], "temperature": 0.0, "avg_logprob": -0.24594405038016184, "compression_ratio": 1.2721518987341771, "no_speech_prob": 7.409880254272139e-06}, {"id": 346, "seek": 292284, "start": 2935.96, "end": 2941.88, "text": " When we resume, we'll be talking about PCA and robust PCA.", "tokens": [1133, 321, 15358, 11, 321, 603, 312, 1417, 466, 6465, 32, 293, 13956, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.24594405038016184, "compression_ratio": 1.2721518987341771, "no_speech_prob": 7.409880254272139e-06}, {"id": 347, "seek": 294188, "start": 2941.88, "end": 2946.1600000000003, "text": " Yeah, Jean's picture.", "tokens": [865, 11, 13854, 311, 3036, 13], "temperature": 0.0, "avg_logprob": -0.2424521976047092, "compression_ratio": 1.4476190476190476, "no_speech_prob": 7.76247452449752e-06}, {"id": 348, "seek": 294188, "start": 2949.88, "end": 2956.28, "text": " Yeah. I'm going to start back up since it's 12.08.", "tokens": [865, 13, 286, 478, 516, 281, 722, 646, 493, 1670, 309, 311, 2272, 13, 16133, 13], "temperature": 0.0, "avg_logprob": -0.2424521976047092, "compression_ratio": 1.4476190476190476, "no_speech_prob": 7.76247452449752e-06}, {"id": 349, "seek": 294188, "start": 2956.48, "end": 2959.88, "text": " One more thing, I announced this last time,", "tokens": [1485, 544, 551, 11, 286, 7548, 341, 1036, 565, 11], "temperature": 0.0, "avg_logprob": -0.2424521976047092, "compression_ratio": 1.4476190476190476, "no_speech_prob": 7.76247452449752e-06}, {"id": 350, "seek": 294188, "start": 2959.88, "end": 2964.36, "text": " but I just wanted to say again that I'm going to be out of town on Friday,", "tokens": [457, 286, 445, 1415, 281, 584, 797, 300, 286, 478, 516, 281, 312, 484, 295, 3954, 322, 6984, 11], "temperature": 0.0, "avg_logprob": -0.2424521976047092, "compression_ratio": 1.4476190476190476, "no_speech_prob": 7.76247452449752e-06}, {"id": 351, "seek": 294188, "start": 2964.36, "end": 2966.1600000000003, "text": " which is normally when I have office hours.", "tokens": [597, 307, 5646, 562, 286, 362, 3398, 2496, 13], "temperature": 0.0, "avg_logprob": -0.2424521976047092, "compression_ratio": 1.4476190476190476, "no_speech_prob": 7.76247452449752e-06}, {"id": 352, "seek": 294188, "start": 2966.1600000000003, "end": 2970.96, "text": " But let me know if you want to meet today or next Monday or Tuesday.", "tokens": [583, 718, 385, 458, 498, 291, 528, 281, 1677, 965, 420, 958, 8138, 420, 10017, 13], "temperature": 0.0, "avg_logprob": -0.2424521976047092, "compression_ratio": 1.4476190476190476, "no_speech_prob": 7.76247452449752e-06}, {"id": 353, "seek": 297096, "start": 2970.96, "end": 2973.88, "text": " Then while we were on break,", "tokens": [1396, 1339, 321, 645, 322, 1821, 11], "temperature": 0.0, "avg_logprob": -0.1734050727752318, "compression_ratio": 1.5432692307692308, "no_speech_prob": 6.399041012628004e-05}, {"id": 354, "seek": 297096, "start": 2973.88, "end": 2980.04, "text": " I just tried running this again with a rank one approximation because I was curious.", "tokens": [286, 445, 3031, 2614, 341, 797, 365, 257, 6181, 472, 28023, 570, 286, 390, 6369, 13], "temperature": 0.0, "avg_logprob": -0.1734050727752318, "compression_ratio": 1.5432692307692308, "no_speech_prob": 6.399041012628004e-05}, {"id": 355, "seek": 297096, "start": 2980.04, "end": 2986.4, "text": " So here, this is what a rank one matrix looks like.", "tokens": [407, 510, 11, 341, 307, 437, 257, 6181, 472, 8141, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1734050727752318, "compression_ratio": 1.5432692307692308, "no_speech_prob": 6.399041012628004e-05}, {"id": 356, "seek": 297096, "start": 2986.4, "end": 2990.0, "text": " It's just a single column repeated again and again.", "tokens": [467, 311, 445, 257, 2167, 7738, 10477, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.1734050727752318, "compression_ratio": 1.5432692307692308, "no_speech_prob": 6.399041012628004e-05}, {"id": 357, "seek": 297096, "start": 2990.0, "end": 2995.44, "text": " So it would give you the exact same background for any point.", "tokens": [407, 309, 576, 976, 291, 264, 1900, 912, 3678, 337, 604, 935, 13], "temperature": 0.0, "avg_logprob": -0.1734050727752318, "compression_ratio": 1.5432692307692308, "no_speech_prob": 6.399041012628004e-05}, {"id": 358, "seek": 297096, "start": 2995.44, "end": 3000.68, "text": " Here I'm subtracting the original matrix,", "tokens": [1692, 286, 478, 16390, 278, 264, 3380, 8141, 11], "temperature": 0.0, "avg_logprob": -0.1734050727752318, "compression_ratio": 1.5432692307692308, "no_speech_prob": 6.399041012628004e-05}, {"id": 359, "seek": 300068, "start": 3000.68, "end": 3003.52, "text": " minus the background and I get the people.", "tokens": [3175, 264, 3678, 293, 286, 483, 264, 561, 13], "temperature": 0.0, "avg_logprob": -0.26232615603676324, "compression_ratio": 1.5582822085889572, "no_speech_prob": 3.943415140383877e-05}, {"id": 360, "seek": 300068, "start": 3003.52, "end": 3005.44, "text": " So that's pretty amazing, I think,", "tokens": [407, 300, 311, 1238, 2243, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.26232615603676324, "compression_ratio": 1.5582822085889572, "no_speech_prob": 3.943415140383877e-05}, {"id": 361, "seek": 300068, "start": 3005.44, "end": 3008.56, "text": " for a rank one approximation.", "tokens": [337, 257, 6181, 472, 28023, 13], "temperature": 0.0, "avg_logprob": -0.26232615603676324, "compression_ratio": 1.5582822085889572, "no_speech_prob": 3.943415140383877e-05}, {"id": 362, "seek": 300068, "start": 3013.24, "end": 3015.48, "text": " It's not the average.", "tokens": [467, 311, 406, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.26232615603676324, "compression_ratio": 1.5582822085889572, "no_speech_prob": 3.943415140383877e-05}, {"id": 363, "seek": 300068, "start": 3015.48, "end": 3021.3199999999997, "text": " No. So if you did the full,", "tokens": [883, 13, 407, 498, 291, 630, 264, 1577, 11], "temperature": 0.0, "avg_logprob": -0.26232615603676324, "compression_ratio": 1.5582822085889572, "no_speech_prob": 3.943415140383877e-05}, {"id": 364, "seek": 300068, "start": 3021.3199999999997, "end": 3023.24, "text": " and I should repeat the question was,", "tokens": [293, 286, 820, 7149, 264, 1168, 390, 11], "temperature": 0.0, "avg_logprob": -0.26232615603676324, "compression_ratio": 1.5582822085889572, "no_speech_prob": 3.943415140383877e-05}, {"id": 365, "seek": 300068, "start": 3023.24, "end": 3025.9199999999996, "text": " is rank one matrix like the average?", "tokens": [307, 6181, 472, 8141, 411, 264, 4274, 30], "temperature": 0.0, "avg_logprob": -0.26232615603676324, "compression_ratio": 1.5582822085889572, "no_speech_prob": 3.943415140383877e-05}, {"id": 366, "seek": 300068, "start": 3025.9199999999996, "end": 3027.04, "text": " It's not the average.", "tokens": [467, 311, 406, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.26232615603676324, "compression_ratio": 1.5582822085889572, "no_speech_prob": 3.943415140383877e-05}, {"id": 367, "seek": 302704, "start": 3027.04, "end": 3032.8, "text": " It's still doing an SVD, but it's finding the,", "tokens": [467, 311, 920, 884, 364, 31910, 35, 11, 457, 309, 311, 5006, 264, 11], "temperature": 0.0, "avg_logprob": -0.2610325764134987, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00013339110591914505}, {"id": 368, "seek": 302704, "start": 3032.8, "end": 3037.2799999999997, "text": " basically the matrix that's going to give you the closest.", "tokens": [1936, 264, 8141, 300, 311, 516, 281, 976, 291, 264, 13699, 13], "temperature": 0.0, "avg_logprob": -0.2610325764134987, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00013339110591914505}, {"id": 369, "seek": 302704, "start": 3040.2799999999997, "end": 3044.68, "text": " Actually, I'm unsure if that would be the average.", "tokens": [5135, 11, 286, 478, 32486, 498, 300, 576, 312, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.2610325764134987, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00013339110591914505}, {"id": 370, "seek": 302704, "start": 3044.68, "end": 3047.52, "text": " You want the matrix that's closest to your original matrix,", "tokens": [509, 528, 264, 8141, 300, 311, 13699, 281, 428, 3380, 8141, 11], "temperature": 0.0, "avg_logprob": -0.2610325764134987, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00013339110591914505}, {"id": 371, "seek": 302704, "start": 3047.52, "end": 3048.84, "text": " but only has one rank?", "tokens": [457, 787, 575, 472, 6181, 30], "temperature": 0.0, "avg_logprob": -0.2610325764134987, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00013339110591914505}, {"id": 372, "seek": 302704, "start": 3048.84, "end": 3054.2799999999997, "text": " I think the average would have a lot more people who are into the background.", "tokens": [286, 519, 264, 4274, 576, 362, 257, 688, 544, 561, 567, 366, 666, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.2610325764134987, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00013339110591914505}, {"id": 373, "seek": 302704, "start": 3054.2799999999997, "end": 3056.52, "text": " That's true. Yes. I think,", "tokens": [663, 311, 2074, 13, 1079, 13, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.2610325764134987, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00013339110591914505}, {"id": 374, "seek": 305652, "start": 3056.52, "end": 3058.6, "text": " and it might depend on your error metric,", "tokens": [293, 309, 1062, 5672, 322, 428, 6713, 20678, 11], "temperature": 0.0, "avg_logprob": -0.16238189250864882, "compression_ratio": 1.6877828054298643, "no_speech_prob": 9.368113751406781e-06}, {"id": 375, "seek": 305652, "start": 3058.6, "end": 3064.72, "text": " but you would want your original data matrix that has the wavy lines.", "tokens": [457, 291, 576, 528, 428, 3380, 1412, 8141, 300, 575, 264, 261, 15498, 3876, 13], "temperature": 0.0, "avg_logprob": -0.16238189250864882, "compression_ratio": 1.6877828054298643, "no_speech_prob": 9.368113751406781e-06}, {"id": 376, "seek": 305652, "start": 3064.72, "end": 3069.8, "text": " You would want to be minimizing that minus this matrix.", "tokens": [509, 576, 528, 281, 312, 46608, 300, 3175, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.16238189250864882, "compression_ratio": 1.6877828054298643, "no_speech_prob": 9.368113751406781e-06}, {"id": 377, "seek": 305652, "start": 3069.8, "end": 3073.08, "text": " I think to have error at just a few points will probably end up", "tokens": [286, 519, 281, 362, 6713, 412, 445, 257, 1326, 2793, 486, 1391, 917, 493], "temperature": 0.0, "avg_logprob": -0.16238189250864882, "compression_ratio": 1.6877828054298643, "no_speech_prob": 9.368113751406781e-06}, {"id": 378, "seek": 305652, "start": 3073.08, "end": 3076.0, "text": " being better than having a small error everywhere.", "tokens": [885, 1101, 813, 1419, 257, 1359, 6713, 5315, 13], "temperature": 0.0, "avg_logprob": -0.16238189250864882, "compression_ratio": 1.6877828054298643, "no_speech_prob": 9.368113751406781e-06}, {"id": 379, "seek": 305652, "start": 3076.0, "end": 3077.92, "text": " If you took the average,", "tokens": [759, 291, 1890, 264, 4274, 11], "temperature": 0.0, "avg_logprob": -0.16238189250864882, "compression_ratio": 1.6877828054298643, "no_speech_prob": 9.368113751406781e-06}, {"id": 380, "seek": 305652, "start": 3077.92, "end": 3081.04, "text": " then every point that a person ever walked by is going to be like", "tokens": [550, 633, 935, 300, 257, 954, 1562, 7628, 538, 307, 516, 281, 312, 411], "temperature": 0.0, "avg_logprob": -0.16238189250864882, "compression_ratio": 1.6877828054298643, "no_speech_prob": 9.368113751406781e-06}, {"id": 381, "seek": 308104, "start": 3081.04, "end": 3087.0, "text": " off a little bit so you would have a tiny error everywhere if you did that.", "tokens": [766, 257, 707, 857, 370, 291, 576, 362, 257, 5870, 6713, 5315, 498, 291, 630, 300, 13], "temperature": 0.0, "avg_logprob": -0.2760117693645198, "compression_ratio": 1.4516129032258065, "no_speech_prob": 2.902248297687038e-06}, {"id": 382, "seek": 308104, "start": 3087.0, "end": 3090.08, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2760117693645198, "compression_ratio": 1.4516129032258065, "no_speech_prob": 2.902248297687038e-06}, {"id": 383, "seek": 308104, "start": 3091.96, "end": 3094.2, "text": " Then also, stepping back,", "tokens": [1396, 611, 11, 16821, 646, 11], "temperature": 0.0, "avg_logprob": -0.2760117693645198, "compression_ratio": 1.4516129032258065, "no_speech_prob": 2.902248297687038e-06}, {"id": 384, "seek": 308104, "start": 3094.2, "end": 3097.44, "text": " I just wanted to say, so with data like this,", "tokens": [286, 445, 1415, 281, 584, 11, 370, 365, 1412, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.2760117693645198, "compression_ratio": 1.4516129032258065, "no_speech_prob": 2.902248297687038e-06}, {"id": 385, "seek": 308104, "start": 3097.44, "end": 3099.96, "text": " I don't put it on GitHub,", "tokens": [286, 500, 380, 829, 309, 322, 23331, 11], "temperature": 0.0, "avg_logprob": -0.2760117693645198, "compression_ratio": 1.4516129032258065, "no_speech_prob": 2.902248297687038e-06}, {"id": 386, "seek": 308104, "start": 3099.96, "end": 3103.08, "text": " but you can download the data yourself from,", "tokens": [457, 291, 393, 5484, 264, 1412, 1803, 490, 11], "temperature": 0.0, "avg_logprob": -0.2760117693645198, "compression_ratio": 1.4516129032258065, "no_speech_prob": 2.902248297687038e-06}, {"id": 387, "seek": 308104, "start": 3103.08, "end": 3105.36, "text": " let me go back to the link,", "tokens": [718, 385, 352, 646, 281, 264, 2113, 11], "temperature": 0.0, "avg_logprob": -0.2760117693645198, "compression_ratio": 1.4516129032258065, "no_speech_prob": 2.902248297687038e-06}, {"id": 388, "seek": 310536, "start": 3105.36, "end": 3110.32, "text": " up here.", "tokens": [493, 510, 13], "temperature": 0.0, "avg_logprob": -0.18783700322530356, "compression_ratio": 1.5408163265306123, "no_speech_prob": 2.930915798060596e-05}, {"id": 389, "seek": 310536, "start": 3110.32, "end": 3114.84, "text": " This is the link of where I got this from.", "tokens": [639, 307, 264, 2113, 295, 689, 286, 658, 341, 490, 13], "temperature": 0.0, "avg_logprob": -0.18783700322530356, "compression_ratio": 1.5408163265306123, "no_speech_prob": 2.930915798060596e-05}, {"id": 390, "seek": 310536, "start": 3114.84, "end": 3119.6400000000003, "text": " This is video 3 in the real videos.", "tokens": [639, 307, 960, 805, 294, 264, 957, 2145, 13], "temperature": 0.0, "avg_logprob": -0.18783700322530356, "compression_ratio": 1.5408163265306123, "no_speech_prob": 2.930915798060596e-05}, {"id": 391, "seek": 310536, "start": 3119.6400000000003, "end": 3121.8, "text": " Although I definitely, if you do run this on", "tokens": [5780, 286, 2138, 11, 498, 291, 360, 1190, 341, 322], "temperature": 0.0, "avg_logprob": -0.18783700322530356, "compression_ratio": 1.5408163265306123, "no_speech_prob": 2.930915798060596e-05}, {"id": 392, "seek": 310536, "start": 3121.8, "end": 3124.1200000000003, "text": " different videos and get interesting results,", "tokens": [819, 2145, 293, 483, 1880, 3542, 11], "temperature": 0.0, "avg_logprob": -0.18783700322530356, "compression_ratio": 1.5408163265306123, "no_speech_prob": 2.930915798060596e-05}, {"id": 393, "seek": 310536, "start": 3124.1200000000003, "end": 3127.6800000000003, "text": " please send them to me because I would be curious to see them.", "tokens": [1767, 2845, 552, 281, 385, 570, 286, 576, 312, 6369, 281, 536, 552, 13], "temperature": 0.0, "avg_logprob": -0.18783700322530356, "compression_ratio": 1.5408163265306123, "no_speech_prob": 2.930915798060596e-05}, {"id": 394, "seek": 310536, "start": 3128.84, "end": 3133.92, "text": " Then the other thing I probably should have done more of is,", "tokens": [1396, 264, 661, 551, 286, 1391, 820, 362, 1096, 544, 295, 307, 11], "temperature": 0.0, "avg_logprob": -0.18783700322530356, "compression_ratio": 1.5408163265306123, "no_speech_prob": 2.930915798060596e-05}, {"id": 395, "seek": 313392, "start": 3133.92, "end": 3140.16, "text": " here where I calculate the matrix and it's really slow to calculate,", "tokens": [510, 689, 286, 8873, 264, 8141, 293, 309, 311, 534, 2964, 281, 8873, 11], "temperature": 0.0, "avg_logprob": -0.14559151445116317, "compression_ratio": 1.60352422907489, "no_speech_prob": 5.475501893670298e-05}, {"id": 396, "seek": 313392, "start": 3140.16, "end": 3142.88, "text": " you don't want to have to do that every time you run this.", "tokens": [291, 500, 380, 528, 281, 362, 281, 360, 300, 633, 565, 291, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.14559151445116317, "compression_ratio": 1.60352422907489, "no_speech_prob": 5.475501893670298e-05}, {"id": 397, "seek": 313392, "start": 3142.88, "end": 3147.44, "text": " You can use np.save to save the NumPy array.", "tokens": [509, 393, 764, 33808, 13, 82, 946, 281, 3155, 264, 22592, 47, 88, 10225, 13], "temperature": 0.0, "avg_logprob": -0.14559151445116317, "compression_ratio": 1.60352422907489, "no_speech_prob": 5.475501893670298e-05}, {"id": 398, "seek": 313392, "start": 3147.44, "end": 3149.76, "text": " This was for the high-res version,", "tokens": [639, 390, 337, 264, 1090, 12, 495, 3037, 11], "temperature": 0.0, "avg_logprob": -0.14559151445116317, "compression_ratio": 1.60352422907489, "no_speech_prob": 5.475501893670298e-05}, {"id": 399, "seek": 313392, "start": 3149.76, "end": 3153.52, "text": " but you would probably want to do it as a low-res version.", "tokens": [457, 291, 576, 1391, 528, 281, 360, 309, 382, 257, 2295, 12, 495, 3037, 13], "temperature": 0.0, "avg_logprob": -0.14559151445116317, "compression_ratio": 1.60352422907489, "no_speech_prob": 5.475501893670298e-05}, {"id": 400, "seek": 313392, "start": 3153.52, "end": 3155.16, "text": " I'm just giving a file name,", "tokens": [286, 478, 445, 2902, 257, 3991, 1315, 11], "temperature": 0.0, "avg_logprob": -0.14559151445116317, "compression_ratio": 1.60352422907489, "no_speech_prob": 5.475501893670298e-05}, {"id": 401, "seek": 313392, "start": 3155.16, "end": 3157.8, "text": " passing in my matrix M.", "tokens": [8437, 294, 452, 8141, 376, 13], "temperature": 0.0, "avg_logprob": -0.14559151445116317, "compression_ratio": 1.60352422907489, "no_speech_prob": 5.475501893670298e-05}, {"id": 402, "seek": 313392, "start": 3158.04, "end": 3161.4, "text": " I can save it and then next time I run this,", "tokens": [286, 393, 3155, 309, 293, 550, 958, 565, 286, 1190, 341, 11], "temperature": 0.0, "avg_logprob": -0.14559151445116317, "compression_ratio": 1.60352422907489, "no_speech_prob": 5.475501893670298e-05}, {"id": 403, "seek": 316140, "start": 3161.4, "end": 3163.96, "text": " instead of creating data matrix video,", "tokens": [2602, 295, 4084, 1412, 8141, 960, 11], "temperature": 0.0, "avg_logprob": -0.4100914001464844, "compression_ratio": 1.4926829268292683, "no_speech_prob": 5.4220176934904885e-06}, {"id": 404, "seek": 316140, "start": 3163.96, "end": 3165.8, "text": " I could just do np.load.", "tokens": [286, 727, 445, 360, 33808, 13, 2907, 13], "temperature": 0.0, "avg_logprob": -0.4100914001464844, "compression_ratio": 1.4926829268292683, "no_speech_prob": 5.4220176934904885e-06}, {"id": 405, "seek": 316140, "start": 3165.8, "end": 3168.12, "text": " So that's a nice trick.", "tokens": [407, 300, 311, 257, 1481, 4282, 13], "temperature": 0.0, "avg_logprob": -0.4100914001464844, "compression_ratio": 1.4926829268292683, "no_speech_prob": 5.4220176934904885e-06}, {"id": 406, "seek": 316140, "start": 3170.2400000000002, "end": 3175.2400000000002, "text": " Any other questions about the rank 1 approximation?", "tokens": [2639, 661, 1651, 466, 264, 6181, 502, 28023, 30], "temperature": 0.0, "avg_logprob": -0.4100914001464844, "compression_ratio": 1.4926829268292683, "no_speech_prob": 5.4220176934904885e-06}, {"id": 407, "seek": 316140, "start": 3175.8, "end": 3180.12, "text": " Let's go. Yes. I'm past the microphone.", "tokens": [961, 311, 352, 13, 1079, 13, 286, 478, 1791, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.4100914001464844, "compression_ratio": 1.4926829268292683, "no_speech_prob": 5.4220176934904885e-06}, {"id": 408, "seek": 316140, "start": 3180.12, "end": 3184.6800000000003, "text": " I didn't hear, but did anything happen with the signpost?", "tokens": [286, 994, 380, 1568, 11, 457, 630, 1340, 1051, 365, 264, 1465, 23744, 30], "temperature": 0.0, "avg_logprob": -0.4100914001464844, "compression_ratio": 1.4926829268292683, "no_speech_prob": 5.4220176934904885e-06}, {"id": 409, "seek": 316140, "start": 3184.6800000000003, "end": 3186.6800000000003, "text": " Anything happened with what?", "tokens": [11998, 2011, 365, 437, 30], "temperature": 0.0, "avg_logprob": -0.4100914001464844, "compression_ratio": 1.4926829268292683, "no_speech_prob": 5.4220176934904885e-06}, {"id": 410, "seek": 316140, "start": 3186.6800000000003, "end": 3189.84, "text": " The signpost that would be in the line.", "tokens": [440, 1465, 23744, 300, 576, 312, 294, 264, 1622, 13], "temperature": 0.0, "avg_logprob": -0.4100914001464844, "compression_ratio": 1.4926829268292683, "no_speech_prob": 5.4220176934904885e-06}, {"id": 411, "seek": 318984, "start": 3189.84, "end": 3195.1600000000003, "text": " The signpost? I think more what's happening is you're", "tokens": [440, 1465, 23744, 30, 286, 519, 544, 437, 311, 2737, 307, 291, 434], "temperature": 0.0, "avg_logprob": -0.2893199012393043, "compression_ratio": 1.8518518518518519, "no_speech_prob": 2.9309812816791236e-05}, {"id": 412, "seek": 318984, "start": 3195.1600000000003, "end": 3200.6800000000003, "text": " getting some blurs of the people right behind the signpost.", "tokens": [1242, 512, 888, 2156, 295, 264, 561, 558, 2261, 264, 1465, 23744, 13], "temperature": 0.0, "avg_logprob": -0.2893199012393043, "compression_ratio": 1.8518518518518519, "no_speech_prob": 2.9309812816791236e-05}, {"id": 413, "seek": 318984, "start": 3200.6800000000003, "end": 3203.84, "text": " It's like having those blurs of the people or what", "tokens": [467, 311, 411, 1419, 729, 888, 2156, 295, 264, 561, 420, 437], "temperature": 0.0, "avg_logprob": -0.2893199012393043, "compression_ratio": 1.8518518518518519, "no_speech_prob": 2.9309812816791236e-05}, {"id": 414, "seek": 318984, "start": 3203.84, "end": 3209.28, "text": " let you giving you that artifact of the signpost.", "tokens": [718, 291, 2902, 291, 300, 34806, 295, 264, 1465, 23744, 13], "temperature": 0.0, "avg_logprob": -0.2893199012393043, "compression_ratio": 1.8518518518518519, "no_speech_prob": 2.9309812816791236e-05}, {"id": 415, "seek": 318984, "start": 3209.6400000000003, "end": 3210.96, "text": " Although-", "tokens": [5780, 12], "temperature": 0.0, "avg_logprob": -0.2893199012393043, "compression_ratio": 1.8518518518518519, "no_speech_prob": 2.9309812816791236e-05}, {"id": 416, "seek": 318984, "start": 3210.96, "end": 3212.44, "text": " It's hard to see in the low-res version.", "tokens": [467, 311, 1152, 281, 536, 294, 264, 2295, 12, 495, 3037, 13], "temperature": 0.0, "avg_logprob": -0.2893199012393043, "compression_ratio": 1.8518518518518519, "no_speech_prob": 2.9309812816791236e-05}, {"id": 417, "seek": 318984, "start": 3212.44, "end": 3214.28, "text": " It's hard to see in the low-res version.", "tokens": [467, 311, 1152, 281, 536, 294, 264, 2295, 12, 495, 3037, 13], "temperature": 0.0, "avg_logprob": -0.2893199012393043, "compression_ratio": 1.8518518518518519, "no_speech_prob": 2.9309812816791236e-05}, {"id": 418, "seek": 318984, "start": 3214.28, "end": 3217.1600000000003, "text": " Yeah, you have to use the high-res version.", "tokens": [865, 11, 291, 362, 281, 764, 264, 1090, 12, 495, 3037, 13], "temperature": 0.0, "avg_logprob": -0.2893199012393043, "compression_ratio": 1.8518518518518519, "no_speech_prob": 2.9309812816791236e-05}, {"id": 419, "seek": 321716, "start": 3217.16, "end": 3220.8799999999997, "text": " I have to see that.", "tokens": [286, 362, 281, 536, 300, 13], "temperature": 0.0, "avg_logprob": -0.38415701992540474, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.9021562113484833e-06}, {"id": 420, "seek": 321716, "start": 3220.8799999999997, "end": 3223.3599999999997, "text": " But yeah, so here's the,", "tokens": [583, 1338, 11, 370, 510, 311, 264, 11], "temperature": 0.0, "avg_logprob": -0.38415701992540474, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.9021562113484833e-06}, {"id": 421, "seek": 321716, "start": 3223.72, "end": 3228.3999999999996, "text": " which I just did for the rank 2 approximation up here.", "tokens": [597, 286, 445, 630, 337, 264, 6181, 568, 28023, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.38415701992540474, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.9021562113484833e-06}, {"id": 422, "seek": 321716, "start": 3228.3999999999996, "end": 3232.3999999999996, "text": " I think probably not so much that the signpost is showing up,", "tokens": [286, 519, 1391, 406, 370, 709, 300, 264, 1465, 23744, 307, 4099, 493, 11], "temperature": 0.0, "avg_logprob": -0.38415701992540474, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.9021562113484833e-06}, {"id": 423, "seek": 321716, "start": 3232.3999999999996, "end": 3234.16, "text": " is that you have these light blurs", "tokens": [307, 300, 291, 362, 613, 1442, 888, 2156], "temperature": 0.0, "avg_logprob": -0.38415701992540474, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.9021562113484833e-06}, {"id": 424, "seek": 321716, "start": 3234.16, "end": 3237.12, "text": " of people walking by behind it.", "tokens": [295, 561, 4494, 538, 2261, 309, 13], "temperature": 0.0, "avg_logprob": -0.38415701992540474, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.9021562113484833e-06}, {"id": 425, "seek": 321716, "start": 3239.48, "end": 3243.24, "text": " So principal component analysis.", "tokens": [407, 9716, 6542, 5215, 13], "temperature": 0.0, "avg_logprob": -0.38415701992540474, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.9021562113484833e-06}, {"id": 426, "seek": 321716, "start": 3243.24, "end": 3245.64, "text": " Go to full screen.", "tokens": [1037, 281, 1577, 2568, 13], "temperature": 0.0, "avg_logprob": -0.38415701992540474, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.9021562113484833e-06}, {"id": 427, "seek": 324564, "start": 3245.64, "end": 3249.52, "text": " So typically when you're dealing with a high-dimensional dataset,", "tokens": [407, 5850, 562, 291, 434, 6260, 365, 257, 1090, 12, 18759, 28872, 11], "temperature": 0.0, "avg_logprob": -0.14006159327051662, "compression_ratio": 1.7651821862348178, "no_speech_prob": 8.013056685740594e-06}, {"id": 428, "seek": 324564, "start": 3249.52, "end": 3251.0, "text": " you want to use the fact that", "tokens": [291, 528, 281, 764, 264, 1186, 300], "temperature": 0.0, "avg_logprob": -0.14006159327051662, "compression_ratio": 1.7651821862348178, "no_speech_prob": 8.013056685740594e-06}, {"id": 429, "seek": 324564, "start": 3251.0, "end": 3254.2, "text": " the data usually has a low intrinsic dimensionality.", "tokens": [264, 1412, 2673, 575, 257, 2295, 35698, 10139, 1860, 13], "temperature": 0.0, "avg_logprob": -0.14006159327051662, "compression_ratio": 1.7651821862348178, "no_speech_prob": 8.013056685740594e-06}, {"id": 430, "seek": 324564, "start": 3254.2, "end": 3257.3599999999997, "text": " So even though it's in a lot of dimensions,", "tokens": [407, 754, 1673, 309, 311, 294, 257, 688, 295, 12819, 11], "temperature": 0.0, "avg_logprob": -0.14006159327051662, "compression_ratio": 1.7651821862348178, "no_speech_prob": 8.013056685740594e-06}, {"id": 431, "seek": 324564, "start": 3258.8399999999997, "end": 3262.16, "text": " not all those dimensions have information.", "tokens": [406, 439, 729, 12819, 362, 1589, 13], "temperature": 0.0, "avg_logprob": -0.14006159327051662, "compression_ratio": 1.7651821862348178, "no_speech_prob": 8.013056685740594e-06}, {"id": 432, "seek": 324564, "start": 3262.16, "end": 3265.56, "text": " So you can think of that as lying on a lower dimensional manifold.", "tokens": [407, 291, 393, 519, 295, 300, 382, 8493, 322, 257, 3126, 18795, 47138, 13], "temperature": 0.0, "avg_logprob": -0.14006159327051662, "compression_ratio": 1.7651821862348178, "no_speech_prob": 8.013056685740594e-06}, {"id": 433, "seek": 324564, "start": 3265.56, "end": 3267.92, "text": " So this is even with the randomized SVD,", "tokens": [407, 341, 307, 754, 365, 264, 38513, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.14006159327051662, "compression_ratio": 1.7651821862348178, "no_speech_prob": 8.013056685740594e-06}, {"id": 434, "seek": 324564, "start": 3267.92, "end": 3270.92, "text": " the idea of A had all these columns,", "tokens": [264, 1558, 295, 316, 632, 439, 613, 13766, 11], "temperature": 0.0, "avg_logprob": -0.14006159327051662, "compression_ratio": 1.7651821862348178, "no_speech_prob": 8.013056685740594e-06}, {"id": 435, "seek": 324564, "start": 3270.92, "end": 3273.0, "text": " but really the information of the column space could be", "tokens": [457, 534, 264, 1589, 295, 264, 7738, 1901, 727, 312], "temperature": 0.0, "avg_logprob": -0.14006159327051662, "compression_ratio": 1.7651821862348178, "no_speech_prob": 8.013056685740594e-06}, {"id": 436, "seek": 327300, "start": 3273.0, "end": 3276.64, "text": " captured by something with a lot fewer columns.", "tokens": [11828, 538, 746, 365, 257, 688, 13366, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1876160430908203, "compression_ratio": 1.5222672064777327, "no_speech_prob": 1.1300298865535296e-05}, {"id": 437, "seek": 327300, "start": 3276.64, "end": 3281.92, "text": " So principal component analysis lets us eliminate dimensions.", "tokens": [407, 9716, 6542, 5215, 6653, 505, 13819, 12819, 13], "temperature": 0.0, "avg_logprob": -0.1876160430908203, "compression_ratio": 1.5222672064777327, "no_speech_prob": 1.1300298865535296e-05}, {"id": 438, "seek": 327300, "start": 3281.92, "end": 3288.4, "text": " So classical PCA is seeking the best rank K estimate L.", "tokens": [407, 13735, 6465, 32, 307, 11670, 264, 1151, 6181, 591, 12539, 441, 13], "temperature": 0.0, "avg_logprob": -0.1876160430908203, "compression_ratio": 1.5222672064777327, "no_speech_prob": 1.1300298865535296e-05}, {"id": 439, "seek": 327300, "start": 3288.4, "end": 3292.56, "text": " So I'm just calling it L because it's low rank of a matrix M.", "tokens": [407, 286, 478, 445, 5141, 309, 441, 570, 309, 311, 2295, 6181, 295, 257, 8141, 376, 13], "temperature": 0.0, "avg_logprob": -0.1876160430908203, "compression_ratio": 1.5222672064777327, "no_speech_prob": 1.1300298865535296e-05}, {"id": 440, "seek": 327300, "start": 3292.56, "end": 3296.92, "text": " So K is a parameter that you're specifying going into it.", "tokens": [407, 591, 307, 257, 13075, 300, 291, 434, 1608, 5489, 516, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.1876160430908203, "compression_ratio": 1.5222672064777327, "no_speech_prob": 1.1300298865535296e-05}, {"id": 441, "seek": 327300, "start": 3296.92, "end": 3300.04, "text": " You're saying, I want to see what the best.", "tokens": [509, 434, 1566, 11, 286, 528, 281, 536, 437, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.1876160430908203, "compression_ratio": 1.5222672064777327, "no_speech_prob": 1.1300298865535296e-05}, {"id": 442, "seek": 327300, "start": 3300.04, "end": 3302.64, "text": " We just previously looked at rank 2 and rank 1", "tokens": [492, 445, 8046, 2956, 412, 6181, 568, 293, 6181, 502], "temperature": 0.0, "avg_logprob": -0.1876160430908203, "compression_ratio": 1.5222672064777327, "no_speech_prob": 1.1300298865535296e-05}, {"id": 443, "seek": 330264, "start": 3302.64, "end": 3306.7599999999998, "text": " approximations, but you could choose any value for K.", "tokens": [8542, 763, 11, 457, 291, 727, 2826, 604, 2158, 337, 591, 13], "temperature": 0.0, "avg_logprob": -0.15311466119228265, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.356740646471735e-06}, {"id": 444, "seek": 330264, "start": 3306.7599999999998, "end": 3309.12, "text": " Then you're going to use an algorithm that's going to", "tokens": [1396, 291, 434, 516, 281, 764, 364, 9284, 300, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.15311466119228265, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.356740646471735e-06}, {"id": 445, "seek": 330264, "start": 3309.12, "end": 3312.2799999999997, "text": " minimize the difference between M minus", "tokens": [17522, 264, 2649, 1296, 376, 3175], "temperature": 0.0, "avg_logprob": -0.15311466119228265, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.356740646471735e-06}, {"id": 446, "seek": 330264, "start": 3312.2799999999997, "end": 3318.24, "text": " L for any possible L that has the rank you're interested in.", "tokens": [441, 337, 604, 1944, 441, 300, 575, 264, 6181, 291, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.15311466119228265, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.356740646471735e-06}, {"id": 447, "seek": 330264, "start": 3319.48, "end": 3326.52, "text": " So traditional PCA can handle noise in small magnitudes,", "tokens": [407, 5164, 6465, 32, 393, 4813, 5658, 294, 1359, 4944, 16451, 11], "temperature": 0.0, "avg_logprob": -0.15311466119228265, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.356740646471735e-06}, {"id": 448, "seek": 330264, "start": 3326.52, "end": 3328.8799999999997, "text": " but it's very brittle to having,", "tokens": [457, 309, 311, 588, 49325, 281, 1419, 11], "temperature": 0.0, "avg_logprob": -0.15311466119228265, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.356740646471735e-06}, {"id": 449, "seek": 332888, "start": 3328.88, "end": 3332.92, "text": " even if you just have one observation that's super wrong,", "tokens": [754, 498, 291, 445, 362, 472, 14816, 300, 311, 1687, 2085, 11], "temperature": 0.0, "avg_logprob": -0.17007017135620117, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.2028923265461344e-05}, {"id": 450, "seek": 332888, "start": 3332.92, "end": 3335.84, "text": " that can really throw it off.", "tokens": [300, 393, 534, 3507, 309, 766, 13], "temperature": 0.0, "avg_logprob": -0.17007017135620117, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.2028923265461344e-05}, {"id": 451, "seek": 332888, "start": 3337.04, "end": 3340.84, "text": " Which unfortunately in a lot of real-world datasets,", "tokens": [3013, 7015, 294, 257, 688, 295, 957, 12, 13217, 42856, 11], "temperature": 0.0, "avg_logprob": -0.17007017135620117, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.2028923265461344e-05}, {"id": 452, "seek": 332888, "start": 3340.84, "end": 3344.8, "text": " you can have some observations that are just very wrong.", "tokens": [291, 393, 362, 512, 18163, 300, 366, 445, 588, 2085, 13], "temperature": 0.0, "avg_logprob": -0.17007017135620117, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.2028923265461344e-05}, {"id": 453, "seek": 332888, "start": 3344.8, "end": 3348.0, "text": " So robust PCA is a way around this.", "tokens": [407, 13956, 6465, 32, 307, 257, 636, 926, 341, 13], "temperature": 0.0, "avg_logprob": -0.17007017135620117, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.2028923265461344e-05}, {"id": 454, "seek": 332888, "start": 3348.0, "end": 3353.12, "text": " Robust PCA factors a matrix into the sum of two matrices,", "tokens": [5424, 381, 6465, 32, 6771, 257, 8141, 666, 264, 2408, 295, 732, 32284, 11], "temperature": 0.0, "avg_logprob": -0.17007017135620117, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.2028923265461344e-05}, {"id": 455, "seek": 332888, "start": 3353.12, "end": 3357.92, "text": " low rank matrix L and a sparse matrix S.", "tokens": [2295, 6181, 8141, 441, 293, 257, 637, 11668, 8141, 318, 13], "temperature": 0.0, "avg_logprob": -0.17007017135620117, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.2028923265461344e-05}, {"id": 456, "seek": 335792, "start": 3357.92, "end": 3359.92, "text": " So next I'll ask you,", "tokens": [407, 958, 286, 603, 1029, 291, 11], "temperature": 0.0, "avg_logprob": -0.21295772899280896, "compression_ratio": 1.7794871794871794, "no_speech_prob": 4.092766630492406e-06}, {"id": 457, "seek": 335792, "start": 3359.92, "end": 3362.92, "text": " what does it mean for a matrix to be sparse?", "tokens": [437, 775, 309, 914, 337, 257, 8141, 281, 312, 637, 11668, 30], "temperature": 0.0, "avg_logprob": -0.21295772899280896, "compression_ratio": 1.7794871794871794, "no_speech_prob": 4.092766630492406e-06}, {"id": 458, "seek": 335792, "start": 3365.32, "end": 3367.8, "text": " Sam.", "tokens": [4832, 13], "temperature": 0.0, "avg_logprob": -0.21295772899280896, "compression_ratio": 1.7794871794871794, "no_speech_prob": 4.092766630492406e-06}, {"id": 459, "seek": 335792, "start": 3369.32, "end": 3372.12, "text": " It means that there are a lot of zero values.", "tokens": [467, 1355, 300, 456, 366, 257, 688, 295, 4018, 4190, 13], "temperature": 0.0, "avg_logprob": -0.21295772899280896, "compression_ratio": 1.7794871794871794, "no_speech_prob": 4.092766630492406e-06}, {"id": 460, "seek": 335792, "start": 3372.12, "end": 3375.44, "text": " Exactly. Yes. So there are a lot of zero values.", "tokens": [7587, 13, 1079, 13, 407, 456, 366, 257, 688, 295, 4018, 4190, 13], "temperature": 0.0, "avg_logprob": -0.21295772899280896, "compression_ratio": 1.7794871794871794, "no_speech_prob": 4.092766630492406e-06}, {"id": 461, "seek": 335792, "start": 3375.44, "end": 3379.36, "text": " So going back to our example of the people,", "tokens": [407, 516, 646, 281, 527, 1365, 295, 264, 561, 11], "temperature": 0.0, "avg_logprob": -0.21295772899280896, "compression_ratio": 1.7794871794871794, "no_speech_prob": 4.092766630492406e-06}, {"id": 462, "seek": 335792, "start": 3379.36, "end": 3381.12, "text": " there are a lot of zero values,", "tokens": [456, 366, 257, 688, 295, 4018, 4190, 11], "temperature": 0.0, "avg_logprob": -0.21295772899280896, "compression_ratio": 1.7794871794871794, "no_speech_prob": 4.092766630492406e-06}, {"id": 463, "seek": 335792, "start": 3381.12, "end": 3382.6800000000003, "text": " and here I'm actually zoomed in.", "tokens": [293, 510, 286, 478, 767, 8863, 292, 294, 13], "temperature": 0.0, "avg_logprob": -0.21295772899280896, "compression_ratio": 1.7794871794871794, "no_speech_prob": 4.092766630492406e-06}, {"id": 464, "seek": 335792, "start": 3382.6800000000003, "end": 3384.96, "text": " So if you think of all this gray background,", "tokens": [407, 498, 291, 519, 295, 439, 341, 10855, 3678, 11], "temperature": 0.0, "avg_logprob": -0.21295772899280896, "compression_ratio": 1.7794871794871794, "no_speech_prob": 4.092766630492406e-06}, {"id": 465, "seek": 335792, "start": 3384.96, "end": 3386.52, "text": " those are all zero values,", "tokens": [729, 366, 439, 4018, 4190, 11], "temperature": 0.0, "avg_logprob": -0.21295772899280896, "compression_ratio": 1.7794871794871794, "no_speech_prob": 4.092766630492406e-06}, {"id": 466, "seek": 338652, "start": 3386.52, "end": 3390.12, "text": " and they're actually even more when you zoom out to the full size.", "tokens": [293, 436, 434, 767, 754, 544, 562, 291, 8863, 484, 281, 264, 1577, 2744, 13], "temperature": 0.0, "avg_logprob": -0.17080818893563035, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.6441355910501443e-05}, {"id": 467, "seek": 338652, "start": 3390.12, "end": 3393.4, "text": " So we want to find a factorization.", "tokens": [407, 321, 528, 281, 915, 257, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.17080818893563035, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.6441355910501443e-05}, {"id": 468, "seek": 338652, "start": 3393.4, "end": 3397.72, "text": " This factorization is a little bit distinctive in that it's a sum.", "tokens": [639, 5952, 2144, 307, 257, 707, 857, 27766, 294, 300, 309, 311, 257, 2408, 13], "temperature": 0.0, "avg_logprob": -0.17080818893563035, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.6441355910501443e-05}, {"id": 469, "seek": 338652, "start": 3397.72, "end": 3399.52, "text": " Most of the factorizations, in fact,", "tokens": [4534, 295, 264, 5952, 14455, 11, 294, 1186, 11], "temperature": 0.0, "avg_logprob": -0.17080818893563035, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.6441355910501443e-05}, {"id": 470, "seek": 338652, "start": 3399.52, "end": 3404.24, "text": " I think all the other factorizations we'll learn in this course are multiplication.", "tokens": [286, 519, 439, 264, 661, 5952, 14455, 321, 603, 1466, 294, 341, 1164, 366, 27290, 13], "temperature": 0.0, "avg_logprob": -0.17080818893563035, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.6441355910501443e-05}, {"id": 471, "seek": 338652, "start": 3404.24, "end": 3406.6, "text": " But here we're factoring something into a sum,", "tokens": [583, 510, 321, 434, 1186, 3662, 746, 666, 257, 2408, 11], "temperature": 0.0, "avg_logprob": -0.17080818893563035, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.6441355910501443e-05}, {"id": 472, "seek": 338652, "start": 3406.6, "end": 3410.08, "text": " and it's the low rank matrix plus a sparse matrix.", "tokens": [293, 309, 311, 264, 2295, 6181, 8141, 1804, 257, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.17080818893563035, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.6441355910501443e-05}, {"id": 473, "seek": 338652, "start": 3410.08, "end": 3414.92, "text": " It's not factoring, it's a decomposition.", "tokens": [467, 311, 406, 1186, 3662, 11, 309, 311, 257, 48356, 13], "temperature": 0.0, "avg_logprob": -0.17080818893563035, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.6441355910501443e-05}, {"id": 474, "seek": 341492, "start": 3414.92, "end": 3418.2000000000003, "text": " We are decomposing it into two.", "tokens": [492, 366, 22867, 6110, 309, 666, 732, 13], "temperature": 0.0, "avg_logprob": -0.1999720660122958, "compression_ratio": 1.6243093922651934, "no_speech_prob": 1.723013338050805e-05}, {"id": 475, "seek": 341492, "start": 3418.2000000000003, "end": 3424.8, "text": " Most decompositions are factorizations because they're multiplying.", "tokens": [4534, 22867, 329, 2451, 366, 5952, 14455, 570, 436, 434, 30955, 13], "temperature": 0.0, "avg_logprob": -0.1999720660122958, "compression_ratio": 1.6243093922651934, "no_speech_prob": 1.723013338050805e-05}, {"id": 476, "seek": 341492, "start": 3427.88, "end": 3429.96, "text": " We'll see this in a moment,", "tokens": [492, 603, 536, 341, 294, 257, 1623, 11], "temperature": 0.0, "avg_logprob": -0.1999720660122958, "compression_ratio": 1.6243093922651934, "no_speech_prob": 1.723013338050805e-05}, {"id": 477, "seek": 341492, "start": 3429.96, "end": 3433.08, "text": " but in the problem of if your data was corrupted,", "tokens": [457, 294, 264, 1154, 295, 498, 428, 1412, 390, 39480, 11], "temperature": 0.0, "avg_logprob": -0.1999720660122958, "compression_ratio": 1.6243093922651934, "no_speech_prob": 1.723013338050805e-05}, {"id": 478, "seek": 341492, "start": 3433.08, "end": 3439.2400000000002, "text": " hopefully most of your data is not corrupted,", "tokens": [4696, 881, 295, 428, 1412, 307, 406, 39480, 11], "temperature": 0.0, "avg_logprob": -0.1999720660122958, "compression_ratio": 1.6243093922651934, "no_speech_prob": 1.723013338050805e-05}, {"id": 479, "seek": 343924, "start": 3439.24, "end": 3445.3999999999996, "text": " and so you can think of the corrupted data as being the sparse matrix.", "tokens": [293, 370, 291, 393, 519, 295, 264, 39480, 1412, 382, 885, 264, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2962047259012858, "compression_ratio": 1.509009009009009, "no_speech_prob": 4.93679635837907e-06}, {"id": 480, "seek": 343924, "start": 3445.3999999999996, "end": 3447.52, "text": " So now I have some pictures that I took.", "tokens": [407, 586, 286, 362, 512, 5242, 300, 286, 1890, 13], "temperature": 0.0, "avg_logprob": -0.2962047259012858, "compression_ratio": 1.509009009009009, "no_speech_prob": 4.93679635837907e-06}, {"id": 481, "seek": 343924, "start": 3447.52, "end": 3450.4799999999996, "text": " This is from a really nice blog post.", "tokens": [639, 307, 490, 257, 534, 1481, 6968, 2183, 13], "temperature": 0.0, "avg_logprob": -0.2962047259012858, "compression_ratio": 1.509009009009009, "no_speech_prob": 4.93679635837907e-06}, {"id": 482, "seek": 343924, "start": 3450.4799999999996, "end": 3452.68, "text": " Let me open it.", "tokens": [961, 385, 1269, 309, 13], "temperature": 0.0, "avg_logprob": -0.2962047259012858, "compression_ratio": 1.509009009009009, "no_speech_prob": 4.93679635837907e-06}, {"id": 483, "seek": 343924, "start": 3452.68, "end": 3456.4799999999996, "text": " It's called Rebus Tensor PCA with Tensorly.", "tokens": [467, 311, 1219, 1300, 21441, 34306, 6465, 32, 365, 34306, 356, 13], "temperature": 0.0, "avg_logprob": -0.2962047259012858, "compression_ratio": 1.509009009009009, "no_speech_prob": 4.93679635837907e-06}, {"id": 484, "seek": 343924, "start": 3456.4799999999996, "end": 3460.64, "text": " I haven't tried Tensorly, which is a specific library.", "tokens": [286, 2378, 380, 3031, 34306, 356, 11, 597, 307, 257, 2685, 6405, 13], "temperature": 0.0, "avg_logprob": -0.2962047259012858, "compression_ratio": 1.509009009009009, "no_speech_prob": 4.93679635837907e-06}, {"id": 485, "seek": 343924, "start": 3462.8399999999997, "end": 3466.4399999999996, "text": " But these are really great illustrations.", "tokens": [583, 613, 366, 534, 869, 34540, 13], "temperature": 0.0, "avg_logprob": -0.2962047259012858, "compression_ratio": 1.509009009009009, "no_speech_prob": 4.93679635837907e-06}, {"id": 486, "seek": 343924, "start": 3466.4399999999996, "end": 3468.6, "text": " So this is face recognition,", "tokens": [407, 341, 307, 1851, 11150, 11], "temperature": 0.0, "avg_logprob": -0.2962047259012858, "compression_ratio": 1.509009009009009, "no_speech_prob": 4.93679635837907e-06}, {"id": 487, "seek": 346860, "start": 3468.6, "end": 3474.92, "text": " and it was done on a dataset where you had multiple pictures of one person's face,", "tokens": [293, 309, 390, 1096, 322, 257, 28872, 689, 291, 632, 3866, 5242, 295, 472, 954, 311, 1851, 11], "temperature": 0.0, "avg_logprob": -0.1585906226679964, "compression_ratio": 1.8130434782608695, "no_speech_prob": 2.1780700990348123e-05}, {"id": 488, "seek": 346860, "start": 3474.92, "end": 3479.92, "text": " but from different angles where the light is coming from different angles.", "tokens": [457, 490, 819, 14708, 689, 264, 1442, 307, 1348, 490, 819, 14708, 13], "temperature": 0.0, "avg_logprob": -0.1585906226679964, "compression_ratio": 1.8130434782608695, "no_speech_prob": 2.1780700990348123e-05}, {"id": 489, "seek": 346860, "start": 3479.92, "end": 3482.7599999999998, "text": " So here there would have been a lot of pictures of", "tokens": [407, 510, 456, 576, 362, 668, 257, 688, 295, 5242, 295], "temperature": 0.0, "avg_logprob": -0.1585906226679964, "compression_ratio": 1.8130434782608695, "no_speech_prob": 2.1780700990348123e-05}, {"id": 490, "seek": 346860, "start": 3482.7599999999998, "end": 3486.3199999999997, "text": " this person's face but lit up from different angles.", "tokens": [341, 954, 311, 1851, 457, 7997, 493, 490, 819, 14708, 13], "temperature": 0.0, "avg_logprob": -0.1585906226679964, "compression_ratio": 1.8130434782608695, "no_speech_prob": 2.1780700990348123e-05}, {"id": 491, "seek": 346860, "start": 3486.3199999999997, "end": 3489.48, "text": " So here's the original. They've added some noise in,", "tokens": [407, 510, 311, 264, 3380, 13, 814, 600, 3869, 512, 5658, 294, 11], "temperature": 0.0, "avg_logprob": -0.1585906226679964, "compression_ratio": 1.8130434782608695, "no_speech_prob": 2.1780700990348123e-05}, {"id": 492, "seek": 346860, "start": 3489.48, "end": 3492.7599999999998, "text": " or you can say grossly corrupted entries.", "tokens": [420, 291, 393, 584, 11367, 356, 39480, 23041, 13], "temperature": 0.0, "avg_logprob": -0.1585906226679964, "compression_ratio": 1.8130434782608695, "no_speech_prob": 2.1780700990348123e-05}, {"id": 493, "seek": 346860, "start": 3492.7599999999998, "end": 3495.0, "text": " I think this is pretty amazing.", "tokens": [286, 519, 341, 307, 1238, 2243, 13], "temperature": 0.0, "avg_logprob": -0.1585906226679964, "compression_ratio": 1.8130434782608695, "no_speech_prob": 2.1780700990348123e-05}, {"id": 494, "seek": 346860, "start": 3495.0, "end": 3496.8399999999997, "text": " They get from this original,", "tokens": [814, 483, 490, 341, 3380, 11], "temperature": 0.0, "avg_logprob": -0.1585906226679964, "compression_ratio": 1.8130434782608695, "no_speech_prob": 2.1780700990348123e-05}, {"id": 495, "seek": 349684, "start": 3496.84, "end": 3500.04, "text": " this is the low-rank component and this is the sparse component.", "tokens": [341, 307, 264, 2295, 12, 20479, 6542, 293, 341, 307, 264, 637, 11668, 6542, 13], "temperature": 0.0, "avg_logprob": -0.13880843944377727, "compression_ratio": 1.9212962962962963, "no_speech_prob": 1.2029177014483139e-05}, {"id": 496, "seek": 349684, "start": 3500.04, "end": 3502.84, "text": " So the sparse component is picking out the noise,", "tokens": [407, 264, 637, 11668, 6542, 307, 8867, 484, 264, 5658, 11], "temperature": 0.0, "avg_logprob": -0.13880843944377727, "compression_ratio": 1.9212962962962963, "no_speech_prob": 1.2029177014483139e-05}, {"id": 497, "seek": 349684, "start": 3502.84, "end": 3506.0, "text": " and the low-rank component is picking out the face.", "tokens": [293, 264, 2295, 12, 20479, 6542, 307, 8867, 484, 264, 1851, 13], "temperature": 0.0, "avg_logprob": -0.13880843944377727, "compression_ratio": 1.9212962962962963, "no_speech_prob": 1.2029177014483139e-05}, {"id": 498, "seek": 349684, "start": 3506.0, "end": 3511.6000000000004, "text": " I think particularly this third one is amazing because the one on the left right here,", "tokens": [286, 519, 4098, 341, 2636, 472, 307, 2243, 570, 264, 472, 322, 264, 1411, 558, 510, 11], "temperature": 0.0, "avg_logprob": -0.13880843944377727, "compression_ratio": 1.9212962962962963, "no_speech_prob": 1.2029177014483139e-05}, {"id": 499, "seek": 349684, "start": 3511.6000000000004, "end": 3514.84, "text": " I can't even tell that this is a picture of a face,", "tokens": [286, 393, 380, 754, 980, 300, 341, 307, 257, 3036, 295, 257, 1851, 11], "temperature": 0.0, "avg_logprob": -0.13880843944377727, "compression_ratio": 1.9212962962962963, "no_speech_prob": 1.2029177014483139e-05}, {"id": 500, "seek": 349684, "start": 3514.84, "end": 3517.88, "text": " but they have picked out the face.", "tokens": [457, 436, 362, 6183, 484, 264, 1851, 13], "temperature": 0.0, "avg_logprob": -0.13880843944377727, "compression_ratio": 1.9212962962962963, "no_speech_prob": 1.2029177014483139e-05}, {"id": 501, "seek": 349684, "start": 3518.0, "end": 3523.36, "text": " So this is another application of this.", "tokens": [407, 341, 307, 1071, 3861, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.13880843944377727, "compression_ratio": 1.9212962962962963, "no_speech_prob": 1.2029177014483139e-05}, {"id": 502, "seek": 349684, "start": 3523.36, "end": 3525.6400000000003, "text": " So remember what we're looking at,", "tokens": [407, 1604, 437, 321, 434, 1237, 412, 11], "temperature": 0.0, "avg_logprob": -0.13880843944377727, "compression_ratio": 1.9212962962962963, "no_speech_prob": 1.2029177014483139e-05}, {"id": 503, "seek": 352564, "start": 3525.64, "end": 3530.08, "text": " the low-rank one will be the background and the sparse component will be the people.", "tokens": [264, 2295, 12, 20479, 472, 486, 312, 264, 3678, 293, 264, 637, 11668, 6542, 486, 312, 264, 561, 13], "temperature": 0.0, "avg_logprob": -0.14124781125551694, "compression_ratio": 1.7415730337078652, "no_speech_prob": 4.71064458906767e-06}, {"id": 504, "seek": 352564, "start": 3530.08, "end": 3537.12, "text": " Here the low-rank is the space and the sparse component is this really awful noise.", "tokens": [1692, 264, 2295, 12, 20479, 307, 264, 1901, 293, 264, 637, 11668, 6542, 307, 341, 534, 11232, 5658, 13], "temperature": 0.0, "avg_logprob": -0.14124781125551694, "compression_ratio": 1.7415730337078652, "no_speech_prob": 4.71064458906767e-06}, {"id": 505, "seek": 352564, "start": 3540.6, "end": 3545.4, "text": " Then here he takes it up even another level and graze out", "tokens": [1396, 510, 415, 2516, 309, 493, 754, 1071, 1496, 293, 1295, 1381, 484], "temperature": 0.0, "avg_logprob": -0.14124781125551694, "compression_ratio": 1.7415730337078652, "no_speech_prob": 4.71064458906767e-06}, {"id": 506, "seek": 354540, "start": 3545.4, "end": 3559.12, "text": " entire sections in this picture of the face and is still able to recover the faces. Yeah.", "tokens": [2302, 10863, 294, 341, 3036, 295, 264, 1851, 293, 307, 920, 1075, 281, 8114, 264, 8475, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.22703773570510577, "compression_ratio": 1.3586206896551725, "no_speech_prob": 6.642999323958065e-06}, {"id": 507, "seek": 354540, "start": 3559.12, "end": 3563.2000000000003, "text": " So it's very impressive.", "tokens": [407, 309, 311, 588, 8992, 13], "temperature": 0.0, "avg_logprob": -0.22703773570510577, "compression_ratio": 1.3586206896551725, "no_speech_prob": 6.642999323958065e-06}, {"id": 508, "seek": 354540, "start": 3565.6800000000003, "end": 3568.96, "text": " So other applications of this,", "tokens": [407, 661, 5821, 295, 341, 11], "temperature": 0.0, "avg_logprob": -0.22703773570510577, "compression_ratio": 1.3586206896551725, "no_speech_prob": 6.642999323958065e-06}, {"id": 509, "seek": 354540, "start": 3568.96, "end": 3573.2400000000002, "text": " latent semantic indexing, you could use robust PCA.", "tokens": [48994, 47982, 8186, 278, 11, 291, 727, 764, 13956, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.22703773570510577, "compression_ratio": 1.3586206896551725, "no_speech_prob": 6.642999323958065e-06}, {"id": 510, "seek": 357324, "start": 3573.24, "end": 3578.4799999999996, "text": " There the low-rank matrix would be the common words that show up in all documents.", "tokens": [821, 264, 2295, 12, 20479, 8141, 576, 312, 264, 2689, 2283, 300, 855, 493, 294, 439, 8512, 13], "temperature": 0.0, "avg_logprob": -0.16341924667358398, "compression_ratio": 1.704225352112676, "no_speech_prob": 8.938726750784554e-06}, {"id": 511, "seek": 357324, "start": 3578.4799999999996, "end": 3583.2799999999997, "text": " So this is and probably in all your documents.", "tokens": [407, 341, 307, 293, 1391, 294, 439, 428, 8512, 13], "temperature": 0.0, "avg_logprob": -0.16341924667358398, "compression_ratio": 1.704225352112676, "no_speech_prob": 8.938726750784554e-06}, {"id": 512, "seek": 357324, "start": 3583.2799999999997, "end": 3587.2799999999997, "text": " Then your sparse matrix could capture a few keywords", "tokens": [1396, 428, 637, 11668, 8141, 727, 7983, 257, 1326, 21009], "temperature": 0.0, "avg_logprob": -0.16341924667358398, "compression_ratio": 1.704225352112676, "no_speech_prob": 8.938726750784554e-06}, {"id": 513, "seek": 357324, "start": 3587.2799999999997, "end": 3592.16, "text": " from each document that make it different than others.", "tokens": [490, 1184, 4166, 300, 652, 309, 819, 813, 2357, 13], "temperature": 0.0, "avg_logprob": -0.16341924667358398, "compression_ratio": 1.704225352112676, "no_speech_prob": 8.938726750784554e-06}, {"id": 514, "seek": 357324, "start": 3592.16, "end": 3595.6, "text": " Then ranking and collaborative filtering.", "tokens": [1396, 17833, 293, 16555, 30822, 13], "temperature": 0.0, "avg_logprob": -0.16341924667358398, "compression_ratio": 1.704225352112676, "no_speech_prob": 8.938726750784554e-06}, {"id": 515, "seek": 359560, "start": 3595.6, "end": 3603.48, "text": " So this is an issue that Netflix has in terms of some of the data being really bad.", "tokens": [407, 341, 307, 364, 2734, 300, 12778, 575, 294, 2115, 295, 512, 295, 264, 1412, 885, 534, 1578, 13], "temperature": 0.0, "avg_logprob": -0.13634022799405185, "compression_ratio": 1.5727272727272728, "no_speech_prob": 1.9946612610510783e-06}, {"id": 516, "seek": 359560, "start": 3603.48, "end": 3606.24, "text": " I don't think they give examples of this,", "tokens": [286, 500, 380, 519, 436, 976, 5110, 295, 341, 11], "temperature": 0.0, "avg_logprob": -0.13634022799405185, "compression_ratio": 1.5727272727272728, "no_speech_prob": 1.9946612610510783e-06}, {"id": 517, "seek": 359560, "start": 3606.24, "end": 3608.16, "text": " but I feel like this could even be things of,", "tokens": [457, 286, 841, 411, 341, 727, 754, 312, 721, 295, 11], "temperature": 0.0, "avg_logprob": -0.13634022799405185, "compression_ratio": 1.5727272727272728, "no_speech_prob": 1.9946612610510783e-06}, {"id": 518, "seek": 359560, "start": 3608.16, "end": 3612.96, "text": " I don't know if a toddler accidentally enters some ratings for movies that are", "tokens": [286, 500, 380, 458, 498, 257, 44348, 15715, 18780, 512, 24603, 337, 6233, 300, 366], "temperature": 0.0, "avg_logprob": -0.13634022799405185, "compression_ratio": 1.5727272727272728, "no_speech_prob": 1.9946612610510783e-06}, {"id": 519, "seek": 359560, "start": 3612.96, "end": 3617.8399999999997, "text": " just totally have nothing to do with that person's actual preferences,", "tokens": [445, 3879, 362, 1825, 281, 360, 365, 300, 954, 311, 3539, 21910, 11], "temperature": 0.0, "avg_logprob": -0.13634022799405185, "compression_ratio": 1.5727272727272728, "no_speech_prob": 1.9946612610510783e-06}, {"id": 520, "seek": 361784, "start": 3617.84, "end": 3632.2400000000002, "text": " but they use robust PCA. You want to say more about this Jeremy?", "tokens": [457, 436, 764, 13956, 6465, 32, 13, 509, 528, 281, 584, 544, 466, 341, 17809, 30], "temperature": 0.0, "avg_logprob": -0.3296081119113498, "compression_ratio": 1.497584541062802, "no_speech_prob": 3.881666998495348e-05}, {"id": 521, "seek": 361784, "start": 3632.2400000000002, "end": 3634.44, "text": " There's a great example.", "tokens": [821, 311, 257, 869, 1365, 13], "temperature": 0.0, "avg_logprob": -0.3296081119113498, "compression_ratio": 1.497584541062802, "no_speech_prob": 3.881666998495348e-05}, {"id": 522, "seek": 361784, "start": 3634.44, "end": 3636.4, "text": " For a lot of you know about the Netflix prize,", "tokens": [1171, 257, 688, 295, 291, 458, 466, 264, 12778, 12818, 11], "temperature": 0.0, "avg_logprob": -0.3296081119113498, "compression_ratio": 1.497584541062802, "no_speech_prob": 3.881666998495348e-05}, {"id": 523, "seek": 361784, "start": 3636.4, "end": 3638.52, "text": " it's a $1 million machine learning prize.", "tokens": [309, 311, 257, 1848, 16, 2459, 3479, 2539, 12818, 13], "temperature": 0.0, "avg_logprob": -0.3296081119113498, "compression_ratio": 1.497584541062802, "no_speech_prob": 3.881666998495348e-05}, {"id": 524, "seek": 361784, "start": 3638.52, "end": 3640.6400000000003, "text": " There's a lot of great write-ups of that.", "tokens": [821, 311, 257, 688, 295, 869, 2464, 12, 7528, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.3296081119113498, "compression_ratio": 1.497584541062802, "no_speech_prob": 3.881666998495348e-05}, {"id": 525, "seek": 361784, "start": 3640.6400000000003, "end": 3643.28, "text": " The winners had to come up with some,", "tokens": [440, 17193, 632, 281, 808, 493, 365, 512, 11], "temperature": 0.0, "avg_logprob": -0.3296081119113498, "compression_ratio": 1.497584541062802, "no_speech_prob": 3.881666998495348e-05}, {"id": 526, "seek": 361784, "start": 3643.28, "end": 3645.32, "text": " well, interestingly, like before the Netflix prize,", "tokens": [731, 11, 25873, 11, 411, 949, 264, 12778, 12818, 11], "temperature": 0.0, "avg_logprob": -0.3296081119113498, "compression_ratio": 1.497584541062802, "no_speech_prob": 3.881666998495348e-05}, {"id": 527, "seek": 364532, "start": 3645.32, "end": 3650.52, "text": " people have forgotten about some of these robust decomposition techniques.", "tokens": [561, 362, 11832, 466, 512, 295, 613, 13956, 48356, 7512, 13], "temperature": 0.0, "avg_logprob": -0.265331789978549, "compression_ratio": 1.6051779935275081, "no_speech_prob": 5.826463529956527e-05}, {"id": 528, "seek": 364532, "start": 3650.52, "end": 3655.28, "text": " The winners really walk them back into vogue and they've had a lot of attention since then.", "tokens": [440, 17193, 534, 1792, 552, 646, 666, 371, 7213, 293, 436, 600, 632, 257, 688, 295, 3202, 1670, 550, 13], "temperature": 0.0, "avg_logprob": -0.265331789978549, "compression_ratio": 1.6051779935275081, "no_speech_prob": 5.826463529956527e-05}, {"id": 529, "seek": 364532, "start": 3655.28, "end": 3656.04, "text": " Actually, there was-", "tokens": [5135, 11, 456, 390, 12], "temperature": 0.0, "avg_logprob": -0.265331789978549, "compression_ratio": 1.6051779935275081, "no_speech_prob": 5.826463529956527e-05}, {"id": 530, "seek": 364532, "start": 3656.04, "end": 3658.52, "text": " I can read the papers from that because they're very accessible.", "tokens": [286, 393, 1401, 264, 10577, 490, 300, 570, 436, 434, 588, 9515, 13], "temperature": 0.0, "avg_logprob": -0.265331789978549, "compression_ratio": 1.6051779935275081, "no_speech_prob": 5.826463529956527e-05}, {"id": 531, "seek": 364532, "start": 3658.52, "end": 3663.6400000000003, "text": " There was a fun New York Times article on the Netflix prize about Napoleon Dynamite.", "tokens": [821, 390, 257, 1019, 1873, 3609, 11366, 7222, 322, 264, 12778, 12818, 466, 31694, 22947, 642, 13], "temperature": 0.0, "avg_logprob": -0.265331789978549, "compression_ratio": 1.6051779935275081, "no_speech_prob": 5.826463529956527e-05}, {"id": 532, "seek": 364532, "start": 3663.6400000000003, "end": 3665.2000000000003, "text": " Are you all familiar with this movie?", "tokens": [2014, 291, 439, 4963, 365, 341, 3169, 30], "temperature": 0.0, "avg_logprob": -0.265331789978549, "compression_ratio": 1.6051779935275081, "no_speech_prob": 5.826463529956527e-05}, {"id": 533, "seek": 364532, "start": 3665.2000000000003, "end": 3669.4, "text": " So apparently, Napoleon Dynamite was a hugely polarizing movie,", "tokens": [407, 7970, 11, 31694, 22947, 642, 390, 257, 27417, 12367, 3319, 3169, 11], "temperature": 0.0, "avg_logprob": -0.265331789978549, "compression_ratio": 1.6051779935275081, "no_speech_prob": 5.826463529956527e-05}, {"id": 534, "seek": 364532, "start": 3669.4, "end": 3672.6400000000003, "text": " and it also didn't relate to people's other preferences.", "tokens": [293, 309, 611, 994, 380, 10961, 281, 561, 311, 661, 21910, 13], "temperature": 0.0, "avg_logprob": -0.265331789978549, "compression_ratio": 1.6051779935275081, "no_speech_prob": 5.826463529956527e-05}, {"id": 535, "seek": 367264, "start": 3672.64, "end": 3676.8799999999997, "text": " So including Napoleon Dynamite in", "tokens": [407, 3009, 31694, 22947, 642, 294], "temperature": 0.0, "avg_logprob": -0.1966432744806463, "compression_ratio": 1.6729323308270676, "no_speech_prob": 5.223398329690099e-05}, {"id": 536, "seek": 367264, "start": 3676.8799999999997, "end": 3680.04, "text": " the Netflix problem makes it way more difficult because it's", "tokens": [264, 12778, 1154, 1669, 309, 636, 544, 2252, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.1966432744806463, "compression_ratio": 1.6729323308270676, "no_speech_prob": 5.223398329690099e-05}, {"id": 537, "seek": 367264, "start": 3680.04, "end": 3683.96, "text": " almost random how people will feel about the movie.", "tokens": [1920, 4974, 577, 561, 486, 841, 466, 264, 3169, 13], "temperature": 0.0, "avg_logprob": -0.1966432744806463, "compression_ratio": 1.6729323308270676, "no_speech_prob": 5.223398329690099e-05}, {"id": 538, "seek": 367264, "start": 3683.96, "end": 3688.92, "text": " So I thought that was a fun example of something that could really throw things off.", "tokens": [407, 286, 1194, 300, 390, 257, 1019, 1365, 295, 746, 300, 727, 534, 3507, 721, 766, 13], "temperature": 0.0, "avg_logprob": -0.1966432744806463, "compression_ratio": 1.6729323308270676, "no_speech_prob": 5.223398329690099e-05}, {"id": 539, "seek": 367264, "start": 3688.92, "end": 3691.64, "text": " I know I'm very biased as an ex-cattle person,", "tokens": [286, 458, 286, 478, 588, 28035, 382, 364, 454, 12, 66, 3327, 954, 11], "temperature": 0.0, "avg_logprob": -0.1966432744806463, "compression_ratio": 1.6729323308270676, "no_speech_prob": 5.223398329690099e-05}, {"id": 540, "seek": 367264, "start": 3691.64, "end": 3693.8399999999997, "text": " but I was going to say a lot of people complain that", "tokens": [457, 286, 390, 516, 281, 584, 257, 688, 295, 561, 11024, 300], "temperature": 0.0, "avg_logprob": -0.1966432744806463, "compression_ratio": 1.6729323308270676, "no_speech_prob": 5.223398329690099e-05}, {"id": 541, "seek": 367264, "start": 3693.8399999999997, "end": 3697.24, "text": " the Netflix prize didn't lead directly to results,", "tokens": [264, 12778, 12818, 994, 380, 1477, 3838, 281, 3542, 11], "temperature": 0.0, "avg_logprob": -0.1966432744806463, "compression_ratio": 1.6729323308270676, "no_speech_prob": 5.223398329690099e-05}, {"id": 542, "seek": 367264, "start": 3697.24, "end": 3700.7999999999997, "text": " but the truth is actually the results of the Netflix prize had", "tokens": [457, 264, 3494, 307, 767, 264, 3542, 295, 264, 12778, 12818, 632], "temperature": 0.0, "avg_logprob": -0.1966432744806463, "compression_ratio": 1.6729323308270676, "no_speech_prob": 5.223398329690099e-05}, {"id": 543, "seek": 370080, "start": 3700.8, "end": 3705.1200000000003, "text": " a huge impact on how people think about all kinds of areas of linear algebra,", "tokens": [257, 2603, 2712, 322, 577, 561, 519, 466, 439, 3685, 295, 3179, 295, 8213, 21989, 11], "temperature": 0.0, "avg_logprob": -0.5619165783836728, "compression_ratio": 1.5, "no_speech_prob": 2.04597399715567e-05}, {"id": 544, "seek": 370080, "start": 3705.1200000000003, "end": 3708.04, "text": " and collaborative filtering, and super valuables.", "tokens": [293, 16555, 30822, 11, 293, 1687, 7332, 2965, 13], "temperature": 0.0, "avg_logprob": -0.5619165783836728, "compression_ratio": 1.5, "no_speech_prob": 2.04597399715567e-05}, {"id": 545, "seek": 370080, "start": 3708.04, "end": 3711.92, "text": " Actually, yes, studying competition winning results is", "tokens": [5135, 11, 2086, 11, 7601, 6211, 8224, 3542, 307], "temperature": 0.0, "avg_logprob": -0.5619165783836728, "compression_ratio": 1.5, "no_speech_prob": 2.04597399715567e-05}, {"id": 546, "seek": 370080, "start": 3711.92, "end": 3715.2400000000002, "text": " a great way to further your machine learning and data science skills.", "tokens": [257, 869, 636, 281, 3052, 428, 3479, 2539, 293, 1412, 3497, 3942, 13], "temperature": 0.0, "avg_logprob": -0.5619165783836728, "compression_ratio": 1.5, "no_speech_prob": 2.04597399715567e-05}, {"id": 547, "seek": 370080, "start": 3715.2400000000002, "end": 3716.1600000000003, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.5619165783836728, "compression_ratio": 1.5, "no_speech_prob": 2.04597399715567e-05}, {"id": 548, "seek": 370080, "start": 3716.1600000000003, "end": 3719.48, "text": " I'm glad that you-", "tokens": [286, 478, 5404, 300, 291, 12], "temperature": 0.0, "avg_logprob": -0.5619165783836728, "compression_ratio": 1.5, "no_speech_prob": 2.04597399715567e-05}, {"id": 549, "seek": 370080, "start": 3719.48, "end": 3720.88, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.5619165783836728, "compression_ratio": 1.5, "no_speech_prob": 2.04597399715567e-05}, {"id": 550, "seek": 370080, "start": 3722.48, "end": 3726.5600000000004, "text": " Yes, so just- and actually,", "tokens": [1079, 11, 370, 445, 12, 293, 767, 11], "temperature": 0.0, "avg_logprob": -0.5619165783836728, "compression_ratio": 1.5, "no_speech_prob": 2.04597399715567e-05}, {"id": 551, "seek": 372656, "start": 3726.56, "end": 3734.12, "text": " to ask, raise your hand if you've seen the L1 norm inducing sparsity before.", "tokens": [281, 1029, 11, 5300, 428, 1011, 498, 291, 600, 1612, 264, 441, 16, 2026, 13716, 2175, 637, 685, 507, 949, 13], "temperature": 0.0, "avg_logprob": -0.21882918785358296, "compression_ratio": 1.4712041884816753, "no_speech_prob": 3.023843964911066e-05}, {"id": 552, "seek": 372656, "start": 3734.44, "end": 3738.08, "text": " We'll review the concept of it.", "tokens": [492, 603, 3131, 264, 3410, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.21882918785358296, "compression_ratio": 1.4712041884816753, "no_speech_prob": 3.023843964911066e-05}, {"id": 553, "seek": 372656, "start": 3738.08, "end": 3739.72, "text": " This is a good thing to be familiar with.", "tokens": [639, 307, 257, 665, 551, 281, 312, 4963, 365, 13], "temperature": 0.0, "avg_logprob": -0.21882918785358296, "compression_ratio": 1.4712041884816753, "no_speech_prob": 3.023843964911066e-05}, {"id": 554, "seek": 372656, "start": 3739.72, "end": 3743.7599999999998, "text": " I've had this show up in data scientist interviews that I've done.", "tokens": [286, 600, 632, 341, 855, 493, 294, 1412, 12662, 12318, 300, 286, 600, 1096, 13], "temperature": 0.0, "avg_logprob": -0.21882918785358296, "compression_ratio": 1.4712041884816753, "no_speech_prob": 3.023843964911066e-05}, {"id": 555, "seek": 372656, "start": 3743.7599999999998, "end": 3747.12, "text": " But the idea is that,", "tokens": [583, 264, 1558, 307, 300, 11], "temperature": 0.0, "avg_logprob": -0.21882918785358296, "compression_ratio": 1.4712041884816753, "no_speech_prob": 3.023843964911066e-05}, {"id": 556, "seek": 372656, "start": 3747.12, "end": 3749.4, "text": " so with the L1 norm,", "tokens": [370, 365, 264, 441, 16, 2026, 11], "temperature": 0.0, "avg_logprob": -0.21882918785358296, "compression_ratio": 1.4712041884816753, "no_speech_prob": 3.023843964911066e-05}, {"id": 557, "seek": 372656, "start": 3749.4, "end": 3755.44, "text": " which is just taking", "tokens": [597, 307, 445, 1940], "temperature": 0.0, "avg_logprob": -0.21882918785358296, "compression_ratio": 1.4712041884816753, "no_speech_prob": 3.023843964911066e-05}, {"id": 558, "seek": 375544, "start": 3755.44, "end": 3757.8, "text": " the absolute values of your entries,", "tokens": [264, 8236, 4190, 295, 428, 23041, 11], "temperature": 0.0, "avg_logprob": -0.20424052034870963, "compression_ratio": 1.6019900497512438, "no_speech_prob": 2.6270867238054052e-05}, {"id": 559, "seek": 375544, "start": 3757.8, "end": 3761.6, "text": " so what the unit ball looks like is a diamond.", "tokens": [370, 437, 264, 4985, 2594, 1542, 411, 307, 257, 16059, 13], "temperature": 0.0, "avg_logprob": -0.20424052034870963, "compression_ratio": 1.6019900497512438, "no_speech_prob": 2.6270867238054052e-05}, {"id": 560, "seek": 375544, "start": 3761.6, "end": 3763.2000000000003, "text": " Whereas with the L2 norm,", "tokens": [13813, 365, 264, 441, 17, 2026, 11], "temperature": 0.0, "avg_logprob": -0.20424052034870963, "compression_ratio": 1.6019900497512438, "no_speech_prob": 2.6270867238054052e-05}, {"id": 561, "seek": 375544, "start": 3763.2000000000003, "end": 3765.36, "text": " the unit ball is a circle.", "tokens": [264, 4985, 2594, 307, 257, 6329, 13], "temperature": 0.0, "avg_logprob": -0.20424052034870963, "compression_ratio": 1.6019900497512438, "no_speech_prob": 2.6270867238054052e-05}, {"id": 562, "seek": 375544, "start": 3765.36, "end": 3771.88, "text": " The idea here is where you're looking for where this is going to intersect,", "tokens": [440, 1558, 510, 307, 689, 291, 434, 1237, 337, 689, 341, 307, 516, 281, 27815, 11], "temperature": 0.0, "avg_logprob": -0.20424052034870963, "compression_ratio": 1.6019900497512438, "no_speech_prob": 2.6270867238054052e-05}, {"id": 563, "seek": 375544, "start": 3771.88, "end": 3775.76, "text": " with this other curve that you're optimizing.", "tokens": [365, 341, 661, 7605, 300, 291, 434, 40425, 13], "temperature": 0.0, "avg_logprob": -0.20424052034870963, "compression_ratio": 1.6019900497512438, "no_speech_prob": 2.6270867238054052e-05}, {"id": 564, "seek": 375544, "start": 3775.76, "end": 3782.12, "text": " With the L1 norm, that is way most likely to happen at corners.", "tokens": [2022, 264, 441, 16, 2026, 11, 300, 307, 636, 881, 3700, 281, 1051, 412, 12413, 13], "temperature": 0.0, "avg_logprob": -0.20424052034870963, "compression_ratio": 1.6019900497512438, "no_speech_prob": 2.6270867238054052e-05}, {"id": 565, "seek": 378212, "start": 3782.12, "end": 3786.68, "text": " Let me briefly, I guess,", "tokens": [961, 385, 10515, 11, 286, 2041, 11], "temperature": 0.0, "avg_logprob": -0.4149012565612793, "compression_ratio": 1.1714285714285715, "no_speech_prob": 9.817268619372044e-06}, {"id": 566, "seek": 378212, "start": 3786.68, "end": 3790.0, "text": " say the L1 norm,", "tokens": [584, 264, 441, 16, 2026, 11], "temperature": 0.0, "avg_logprob": -0.4149012565612793, "compression_ratio": 1.1714285714285715, "no_speech_prob": 9.817268619372044e-06}, {"id": 567, "seek": 378212, "start": 3790.6, "end": 3794.44, "text": " I'll write it on here.", "tokens": [286, 603, 2464, 309, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.4149012565612793, "compression_ratio": 1.1714285714285715, "no_speech_prob": 9.817268619372044e-06}, {"id": 568, "seek": 378212, "start": 3801.68, "end": 3811.3599999999997, "text": " L1, you're taking a summation of absolute values for each.", "tokens": [441, 16, 11, 291, 434, 1940, 257, 28811, 295, 8236, 4190, 337, 1184, 13], "temperature": 0.0, "avg_logprob": -0.4149012565612793, "compression_ratio": 1.1714285714285715, "no_speech_prob": 9.817268619372044e-06}, {"id": 569, "seek": 381136, "start": 3811.36, "end": 3812.84, "text": " If you have something in multiple dimensions,", "tokens": [759, 291, 362, 746, 294, 3866, 12819, 11], "temperature": 0.0, "avg_logprob": -0.1957894007364909, "compression_ratio": 1.6169154228855722, "no_speech_prob": 7.5276893767295405e-06}, {"id": 570, "seek": 381136, "start": 3812.84, "end": 3814.08, "text": " sorry, this should be an xi.", "tokens": [2597, 11, 341, 820, 312, 364, 36800, 13], "temperature": 0.0, "avg_logprob": -0.1957894007364909, "compression_ratio": 1.6169154228855722, "no_speech_prob": 7.5276893767295405e-06}, {"id": 571, "seek": 381136, "start": 3814.08, "end": 3818.76, "text": " So each of your x values for the L2 norm,", "tokens": [407, 1184, 295, 428, 2031, 4190, 337, 264, 441, 17, 2026, 11], "temperature": 0.0, "avg_logprob": -0.1957894007364909, "compression_ratio": 1.6169154228855722, "no_speech_prob": 7.5276893767295405e-06}, {"id": 572, "seek": 381136, "start": 3824.2400000000002, "end": 3828.88, "text": " you're summing the squares and then taking the square root of the whole thing.", "tokens": [291, 434, 2408, 2810, 264, 19368, 293, 550, 1940, 264, 3732, 5593, 295, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.1957894007364909, "compression_ratio": 1.6169154228855722, "no_speech_prob": 7.5276893767295405e-06}, {"id": 573, "seek": 381136, "start": 3828.88, "end": 3830.88, "text": " These are generalization.", "tokens": [1981, 366, 2674, 2144, 13], "temperature": 0.0, "avg_logprob": -0.1957894007364909, "compression_ratio": 1.6169154228855722, "no_speech_prob": 7.5276893767295405e-06}, {"id": 574, "seek": 381136, "start": 3830.88, "end": 3836.2400000000002, "text": " So actually, this you're taking the whole thing to the power of one,", "tokens": [407, 767, 11, 341, 291, 434, 1940, 264, 1379, 551, 281, 264, 1347, 295, 472, 11], "temperature": 0.0, "avg_logprob": -0.1957894007364909, "compression_ratio": 1.6169154228855722, "no_speech_prob": 7.5276893767295405e-06}, {"id": 575, "seek": 381136, "start": 3836.2400000000002, "end": 3839.92, "text": " which is nothing or has no effect.", "tokens": [597, 307, 1825, 420, 575, 572, 1802, 13], "temperature": 0.0, "avg_logprob": -0.1957894007364909, "compression_ratio": 1.6169154228855722, "no_speech_prob": 7.5276893767295405e-06}, {"id": 576, "seek": 383992, "start": 3839.92, "end": 3842.92, "text": " But this is generalization of this idea of the LP norm.", "tokens": [583, 341, 307, 2674, 2144, 295, 341, 1558, 295, 264, 38095, 2026, 13], "temperature": 0.0, "avg_logprob": -0.16430013221606873, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.785214175877627e-06}, {"id": 577, "seek": 383992, "start": 3842.92, "end": 3846.2400000000002, "text": " But L1 and L2 are the most common.", "tokens": [583, 441, 16, 293, 441, 17, 366, 264, 881, 2689, 13], "temperature": 0.0, "avg_logprob": -0.16430013221606873, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.785214175877627e-06}, {"id": 578, "seek": 383992, "start": 3847.36, "end": 3849.36, "text": " Sorry, for regularization,", "tokens": [4919, 11, 337, 3890, 2144, 11], "temperature": 0.0, "avg_logprob": -0.16430013221606873, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.785214175877627e-06}, {"id": 579, "seek": 383992, "start": 3849.36, "end": 3851.4, "text": " and this is the penalty term that you're", "tokens": [293, 341, 307, 264, 16263, 1433, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.16430013221606873, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.785214175877627e-06}, {"id": 580, "seek": 383992, "start": 3851.4, "end": 3854.88, "text": " adding when you're doing some optimization problem.", "tokens": [5127, 562, 291, 434, 884, 512, 19618, 1154, 13], "temperature": 0.0, "avg_logprob": -0.16430013221606873, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.785214175877627e-06}, {"id": 581, "seek": 383992, "start": 3854.88, "end": 3861.16, "text": " So the idea with any of these is that you don't want your weights to get too large,", "tokens": [407, 264, 1558, 365, 604, 295, 613, 307, 300, 291, 500, 380, 528, 428, 17443, 281, 483, 886, 2416, 11], "temperature": 0.0, "avg_logprob": -0.16430013221606873, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.785214175877627e-06}, {"id": 582, "seek": 383992, "start": 3861.16, "end": 3864.36, "text": " and so you're putting a penalty on the size of your weights.", "tokens": [293, 370, 291, 434, 3372, 257, 16263, 322, 264, 2744, 295, 428, 17443, 13], "temperature": 0.0, "avg_logprob": -0.16430013221606873, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.785214175877627e-06}, {"id": 583, "seek": 383992, "start": 3864.36, "end": 3868.2400000000002, "text": " But the type of penalty you put will affect what your answers are.", "tokens": [583, 264, 2010, 295, 16263, 291, 829, 486, 3345, 437, 428, 6338, 366, 13], "temperature": 0.0, "avg_logprob": -0.16430013221606873, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.785214175877627e-06}, {"id": 584, "seek": 386824, "start": 3868.24, "end": 3872.12, "text": " So with the L1, it ends up saying,", "tokens": [407, 365, 264, 441, 16, 11, 309, 5314, 493, 1566, 11], "temperature": 0.0, "avg_logprob": -0.12134159935845269, "compression_ratio": 1.6567796610169492, "no_speech_prob": 1.1659140909614507e-05}, {"id": 585, "seek": 386824, "start": 3872.12, "end": 3873.72, "text": " I want to keep this sparse,", "tokens": [286, 528, 281, 1066, 341, 637, 11668, 11], "temperature": 0.0, "avg_logprob": -0.12134159935845269, "compression_ratio": 1.6567796610169492, "no_speech_prob": 1.1659140909614507e-05}, {"id": 586, "seek": 386824, "start": 3873.72, "end": 3880.3199999999997, "text": " and so you're getting more of a penalty for making new things non-zero.", "tokens": [293, 370, 291, 434, 1242, 544, 295, 257, 16263, 337, 1455, 777, 721, 2107, 12, 32226, 13], "temperature": 0.0, "avg_logprob": -0.12134159935845269, "compression_ratio": 1.6567796610169492, "no_speech_prob": 1.1659140909614507e-05}, {"id": 587, "seek": 386824, "start": 3880.3199999999997, "end": 3881.9199999999996, "text": " Whereas the L2 norm,", "tokens": [13813, 264, 441, 17, 2026, 11], "temperature": 0.0, "avg_logprob": -0.12134159935845269, "compression_ratio": 1.6567796610169492, "no_speech_prob": 1.1659140909614507e-05}, {"id": 588, "seek": 386824, "start": 3881.9199999999996, "end": 3888.12, "text": " since you're trying to keep this whole square root of sum of squares to a small value,", "tokens": [1670, 291, 434, 1382, 281, 1066, 341, 1379, 3732, 5593, 295, 2408, 295, 19368, 281, 257, 1359, 2158, 11], "temperature": 0.0, "avg_logprob": -0.12134159935845269, "compression_ratio": 1.6567796610169492, "no_speech_prob": 1.1659140909614507e-05}, {"id": 589, "seek": 386824, "start": 3888.12, "end": 3892.16, "text": " you will have probably a lot of things with small values.", "tokens": [291, 486, 362, 1391, 257, 688, 295, 721, 365, 1359, 4190, 13], "temperature": 0.0, "avg_logprob": -0.12134159935845269, "compression_ratio": 1.6567796610169492, "no_speech_prob": 1.1659140909614507e-05}, {"id": 590, "seek": 389216, "start": 3892.16, "end": 3898.3599999999997, "text": " Whereas L1, it's better to maybe have one thing slightly bigger and the other weight zero.", "tokens": [13813, 441, 16, 11, 309, 311, 1101, 281, 1310, 362, 472, 551, 4748, 3801, 293, 264, 661, 3364, 4018, 13], "temperature": 0.0, "avg_logprob": -0.21862747485821063, "compression_ratio": 1.4972067039106145, "no_speech_prob": 1.0289067176927347e-05}, {"id": 591, "seek": 389216, "start": 3898.3599999999997, "end": 3902.44, "text": " So these are how those penalties work.", "tokens": [407, 613, 366, 577, 729, 35389, 589, 13], "temperature": 0.0, "avg_logprob": -0.21862747485821063, "compression_ratio": 1.4972067039106145, "no_speech_prob": 1.0289067176927347e-05}, {"id": 592, "seek": 389216, "start": 3910.48, "end": 3915.08, "text": " So these are some common pictures that people will show that if", "tokens": [407, 613, 366, 512, 2689, 5242, 300, 561, 486, 855, 300, 498], "temperature": 0.0, "avg_logprob": -0.21862747485821063, "compression_ratio": 1.4972067039106145, "no_speech_prob": 1.0289067176927347e-05}, {"id": 593, "seek": 389216, "start": 3915.08, "end": 3919.16, "text": " you have the problem you're trying to optimize and then also this penalty,", "tokens": [291, 362, 264, 1154, 291, 434, 1382, 281, 19719, 293, 550, 611, 341, 16263, 11], "temperature": 0.0, "avg_logprob": -0.21862747485821063, "compression_ratio": 1.4972067039106145, "no_speech_prob": 1.0289067176927347e-05}, {"id": 594, "seek": 391916, "start": 3919.16, "end": 3923.44, "text": " thinking about where they intersect for L1 norm,", "tokens": [1953, 466, 689, 436, 27815, 337, 441, 16, 2026, 11], "temperature": 0.0, "avg_logprob": -0.18303435525776426, "compression_ratio": 1.5075376884422111, "no_speech_prob": 2.1110028683324344e-05}, {"id": 595, "seek": 391916, "start": 3923.44, "end": 3929.64, "text": " it's going to be a corner which here the x-axis is zero and y is one.", "tokens": [309, 311, 516, 281, 312, 257, 4538, 597, 510, 264, 2031, 12, 24633, 307, 4018, 293, 288, 307, 472, 13], "temperature": 0.0, "avg_logprob": -0.18303435525776426, "compression_ratio": 1.5075376884422111, "no_speech_prob": 2.1110028683324344e-05}, {"id": 596, "seek": 391916, "start": 3929.64, "end": 3932.24, "text": " Whereas here it's going to be somewhere on the circle.", "tokens": [13813, 510, 309, 311, 516, 281, 312, 4079, 322, 264, 6329, 13], "temperature": 0.0, "avg_logprob": -0.18303435525776426, "compression_ratio": 1.5075376884422111, "no_speech_prob": 2.1110028683324344e-05}, {"id": 597, "seek": 391916, "start": 3932.24, "end": 3935.48, "text": " Brad, can you throw the microphone, Jeremy.", "tokens": [11895, 11, 393, 291, 3507, 264, 10952, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.18303435525776426, "compression_ratio": 1.5075376884422111, "no_speech_prob": 2.1110028683324344e-05}, {"id": 598, "seek": 391916, "start": 3937.0, "end": 3943.56, "text": " So one question I've always had with these pictures is that we're saying that with", "tokens": [407, 472, 1168, 286, 600, 1009, 632, 365, 613, 5242, 307, 300, 321, 434, 1566, 300, 365], "temperature": 0.0, "avg_logprob": -0.18303435525776426, "compression_ratio": 1.5075376884422111, "no_speech_prob": 2.1110028683324344e-05}, {"id": 599, "seek": 394356, "start": 3943.56, "end": 3950.52, "text": " the L1 penalty we can induce sparsity,", "tokens": [264, 441, 16, 16263, 321, 393, 41263, 637, 685, 507, 11], "temperature": 0.0, "avg_logprob": -0.2366256063634699, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.682847429648973e-05}, {"id": 600, "seek": 394356, "start": 3950.52, "end": 3954.84, "text": " but we can't with L2 even though when you look at the graph,", "tokens": [457, 321, 393, 380, 365, 441, 17, 754, 1673, 562, 291, 574, 412, 264, 4295, 11], "temperature": 0.0, "avg_logprob": -0.2366256063634699, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.682847429648973e-05}, {"id": 601, "seek": 394356, "start": 3954.84, "end": 3957.2799999999997, "text": " those points at zero are there.", "tokens": [729, 2793, 412, 4018, 366, 456, 13], "temperature": 0.0, "avg_logprob": -0.2366256063634699, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.682847429648973e-05}, {"id": 602, "seek": 394356, "start": 3957.2799999999997, "end": 3958.92, "text": " Is that because-", "tokens": [1119, 300, 570, 12], "temperature": 0.0, "avg_logprob": -0.2366256063634699, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.682847429648973e-05}, {"id": 603, "seek": 394356, "start": 3958.92, "end": 3961.68, "text": " Sorry, what points at zero?", "tokens": [4919, 11, 437, 2793, 412, 4018, 30], "temperature": 0.0, "avg_logprob": -0.2366256063634699, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.682847429648973e-05}, {"id": 604, "seek": 394356, "start": 3962.6, "end": 3967.84, "text": " At the north pole of the circle and at each pole, right?", "tokens": [1711, 264, 6830, 13208, 295, 264, 6329, 293, 412, 1184, 13208, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2366256063634699, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.682847429648973e-05}, {"id": 605, "seek": 394356, "start": 3967.84, "end": 3969.2, "text": " Like here?", "tokens": [1743, 510, 30], "temperature": 0.0, "avg_logprob": -0.2366256063634699, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.682847429648973e-05}, {"id": 606, "seek": 394356, "start": 3969.2, "end": 3972.32, "text": " Yeah, or in the graphs above.", "tokens": [865, 11, 420, 294, 264, 24877, 3673, 13], "temperature": 0.0, "avg_logprob": -0.2366256063634699, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.682847429648973e-05}, {"id": 607, "seek": 397232, "start": 3972.32, "end": 3973.56, "text": " Here?", "tokens": [1692, 30], "temperature": 0.0, "avg_logprob": -0.2937942001054872, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.3314888898748904e-05}, {"id": 608, "seek": 397232, "start": 3973.56, "end": 3978.28, "text": " You can technically see that you could hit those corners in the square there.", "tokens": [509, 393, 12120, 536, 300, 291, 727, 2045, 729, 12413, 294, 264, 3732, 456, 13], "temperature": 0.0, "avg_logprob": -0.2937942001054872, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.3314888898748904e-05}, {"id": 609, "seek": 397232, "start": 3978.28, "end": 3981.6000000000004, "text": " But is the reason why we don't because the circles are", "tokens": [583, 307, 264, 1778, 983, 321, 500, 380, 570, 264, 13040, 366], "temperature": 0.0, "avg_logprob": -0.2937942001054872, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.3314888898748904e-05}, {"id": 610, "seek": 397232, "start": 3981.6000000000004, "end": 3985.76, "text": " continuum and the probability of hitting any one of those points is zero?", "tokens": [36120, 293, 264, 8482, 295, 8850, 604, 472, 295, 729, 2793, 307, 4018, 30], "temperature": 0.0, "avg_logprob": -0.2937942001054872, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.3314888898748904e-05}, {"id": 611, "seek": 397232, "start": 3985.76, "end": 3990.7200000000003, "text": " I think it's more when you think about the structure of this.", "tokens": [286, 519, 309, 311, 544, 562, 291, 519, 466, 264, 3877, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.2937942001054872, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.3314888898748904e-05}, {"id": 612, "seek": 397232, "start": 3991.36, "end": 3996.1600000000003, "text": " I'll look at this one. So here, these are the contours of what,", "tokens": [286, 603, 574, 412, 341, 472, 13, 407, 510, 11, 613, 366, 264, 660, 5067, 295, 437, 11], "temperature": 0.0, "avg_logprob": -0.2937942001054872, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.3314888898748904e-05}, {"id": 613, "seek": 397232, "start": 3996.1600000000003, "end": 3998.0800000000004, "text": " in a lot of cases,", "tokens": [294, 257, 688, 295, 3331, 11], "temperature": 0.0, "avg_logprob": -0.2937942001054872, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.3314888898748904e-05}, {"id": 614, "seek": 397232, "start": 3998.0800000000004, "end": 4001.76, "text": " is the air of your model.", "tokens": [307, 264, 1988, 295, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2937942001054872, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.3314888898748904e-05}, {"id": 615, "seek": 400176, "start": 4001.76, "end": 4004.2400000000002, "text": " When you look at these,", "tokens": [1133, 291, 574, 412, 613, 11], "temperature": 0.0, "avg_logprob": -0.17618093944731214, "compression_ratio": 1.6483050847457628, "no_speech_prob": 3.321023905300535e-05}, {"id": 616, "seek": 400176, "start": 4004.2400000000002, "end": 4010.92, "text": " it's very unlikely that this would be perfectly aligned on the, I mean, it's possible.", "tokens": [309, 311, 588, 17518, 300, 341, 576, 312, 6239, 17962, 322, 264, 11, 286, 914, 11, 309, 311, 1944, 13], "temperature": 0.0, "avg_logprob": -0.17618093944731214, "compression_ratio": 1.6483050847457628, "no_speech_prob": 3.321023905300535e-05}, {"id": 617, "seek": 400176, "start": 4010.92, "end": 4016.48, "text": " You could have something that was perfectly here and lying on the x-axis,", "tokens": [509, 727, 362, 746, 300, 390, 6239, 510, 293, 8493, 322, 264, 2031, 12, 24633, 11], "temperature": 0.0, "avg_logprob": -0.17618093944731214, "compression_ratio": 1.6483050847457628, "no_speech_prob": 3.321023905300535e-05}, {"id": 618, "seek": 400176, "start": 4016.48, "end": 4018.5200000000004, "text": " but that would just be, to me,", "tokens": [457, 300, 576, 445, 312, 11, 281, 385, 11], "temperature": 0.0, "avg_logprob": -0.17618093944731214, "compression_ratio": 1.6483050847457628, "no_speech_prob": 3.321023905300535e-05}, {"id": 619, "seek": 400176, "start": 4018.5200000000004, "end": 4023.32, "text": " that implies you had this very artificial data set or something bizarre going on.", "tokens": [300, 18779, 291, 632, 341, 588, 11677, 1412, 992, 420, 746, 18265, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.17618093944731214, "compression_ratio": 1.6483050847457628, "no_speech_prob": 3.321023905300535e-05}, {"id": 620, "seek": 400176, "start": 4023.32, "end": 4027.1600000000003, "text": " So you're right, you could happen to hit those,", "tokens": [407, 291, 434, 558, 11, 291, 727, 1051, 281, 2045, 729, 11], "temperature": 0.0, "avg_logprob": -0.17618093944731214, "compression_ratio": 1.6483050847457628, "no_speech_prob": 3.321023905300535e-05}, {"id": 621, "seek": 400176, "start": 4027.1600000000003, "end": 4031.36, "text": " but yeah, the chance is so minimally small.", "tokens": [457, 1338, 11, 264, 2931, 307, 370, 4464, 379, 1359, 13], "temperature": 0.0, "avg_logprob": -0.17618093944731214, "compression_ratio": 1.6483050847457628, "no_speech_prob": 3.321023905300535e-05}, {"id": 622, "seek": 403136, "start": 4031.36, "end": 4036.2400000000002, "text": " It also to me implies that there was something very artificially", "tokens": [467, 611, 281, 385, 18779, 300, 456, 390, 746, 588, 39905, 2270], "temperature": 0.0, "avg_logprob": -0.2897788514482214, "compression_ratio": 1.6196581196581197, "no_speech_prob": 5.682117716787616e-06}, {"id": 623, "seek": 403136, "start": 4036.2400000000002, "end": 4042.08, "text": " constructed about the problem you are minimizing. Tim?", "tokens": [17083, 466, 264, 1154, 291, 366, 46608, 13, 7172, 30], "temperature": 0.0, "avg_logprob": -0.2897788514482214, "compression_ratio": 1.6196581196581197, "no_speech_prob": 5.682117716787616e-06}, {"id": 624, "seek": 403136, "start": 4042.4, "end": 4047.0, "text": " Yeah, I feel like the only way that the ellipse or circle would hit,", "tokens": [865, 11, 286, 841, 411, 264, 787, 636, 300, 264, 8284, 48041, 420, 6329, 576, 2045, 11], "temperature": 0.0, "avg_logprob": -0.2897788514482214, "compression_ratio": 1.6196581196581197, "no_speech_prob": 5.682117716787616e-06}, {"id": 625, "seek": 403136, "start": 4047.0, "end": 4050.6, "text": " like you'd actually have it hit zero exactly is if", "tokens": [411, 291, 1116, 767, 362, 309, 2045, 4018, 2293, 307, 498], "temperature": 0.0, "avg_logprob": -0.2897788514482214, "compression_ratio": 1.6196581196581197, "no_speech_prob": 5.682117716787616e-06}, {"id": 626, "seek": 403136, "start": 4050.6, "end": 4056.84, "text": " the actual minimal, the best solution was lied on one of the axes.", "tokens": [264, 3539, 13206, 11, 264, 1151, 3827, 390, 20101, 322, 472, 295, 264, 35387, 13], "temperature": 0.0, "avg_logprob": -0.2897788514482214, "compression_ratio": 1.6196581196581197, "no_speech_prob": 5.682117716787616e-06}, {"id": 627, "seek": 403136, "start": 4056.84, "end": 4059.88, "text": " That would mean that one of the coordinates was zero in the first place.", "tokens": [663, 576, 914, 300, 472, 295, 264, 21056, 390, 4018, 294, 264, 700, 1081, 13], "temperature": 0.0, "avg_logprob": -0.2897788514482214, "compression_ratio": 1.6196581196581197, "no_speech_prob": 5.682117716787616e-06}, {"id": 628, "seek": 405988, "start": 4059.88, "end": 4063.36, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.25097453955448035, "compression_ratio": 1.5318181818181817, "no_speech_prob": 5.771714313596021e-06}, {"id": 629, "seek": 405988, "start": 4063.36, "end": 4065.7200000000003, "text": " Not only that it was like one of those had to be zero,", "tokens": [1726, 787, 300, 309, 390, 411, 472, 295, 729, 632, 281, 312, 4018, 11], "temperature": 0.0, "avg_logprob": -0.25097453955448035, "compression_ratio": 1.5318181818181817, "no_speech_prob": 5.771714313596021e-06}, {"id": 630, "seek": 405988, "start": 4065.7200000000003, "end": 4068.4, "text": " but that implies there's no noise in your problem as well.", "tokens": [457, 300, 18779, 456, 311, 572, 5658, 294, 428, 1154, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.25097453955448035, "compression_ratio": 1.5318181818181817, "no_speech_prob": 5.771714313596021e-06}, {"id": 631, "seek": 405988, "start": 4068.4, "end": 4071.44, "text": " Because it's like even if you had a little bit of noise distorting it,", "tokens": [1436, 309, 311, 411, 754, 498, 291, 632, 257, 707, 857, 295, 5658, 37555, 278, 309, 11], "temperature": 0.0, "avg_logprob": -0.25097453955448035, "compression_ratio": 1.5318181818181817, "no_speech_prob": 5.771714313596021e-06}, {"id": 632, "seek": 405988, "start": 4071.44, "end": 4074.0, "text": " that's going to take it off the axis.", "tokens": [300, 311, 516, 281, 747, 309, 766, 264, 10298, 13], "temperature": 0.0, "avg_logprob": -0.25097453955448035, "compression_ratio": 1.5318181818181817, "no_speech_prob": 5.771714313596021e-06}, {"id": 633, "seek": 405988, "start": 4074.0, "end": 4077.84, "text": " Right. Yeah, thank you.", "tokens": [1779, 13, 865, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.25097453955448035, "compression_ratio": 1.5318181818181817, "no_speech_prob": 5.771714313596021e-06}, {"id": 634, "seek": 405988, "start": 4079.2000000000003, "end": 4082.4, "text": " Other questions?", "tokens": [5358, 1651, 30], "temperature": 0.0, "avg_logprob": -0.25097453955448035, "compression_ratio": 1.5318181818181817, "no_speech_prob": 5.771714313596021e-06}, {"id": 635, "seek": 405988, "start": 4083.56, "end": 4089.28, "text": " We're actually not going to get too much into the details of this,", "tokens": [492, 434, 767, 406, 516, 281, 483, 886, 709, 666, 264, 4365, 295, 341, 11], "temperature": 0.0, "avg_logprob": -0.25097453955448035, "compression_ratio": 1.5318181818181817, "no_speech_prob": 5.771714313596021e-06}, {"id": 636, "seek": 408928, "start": 4089.28, "end": 4095.0800000000004, "text": " but I wanted you to see how the robust PCA can be written as an optimization problem.", "tokens": [457, 286, 1415, 291, 281, 536, 577, 264, 13956, 6465, 32, 393, 312, 3720, 382, 364, 19618, 1154, 13], "temperature": 0.0, "avg_logprob": -0.24502397838391757, "compression_ratio": 1.439153439153439, "no_speech_prob": 2.5462632038397714e-05}, {"id": 637, "seek": 408928, "start": 4095.0800000000004, "end": 4102.0, "text": " There, you're minimizing the nuclear norm of L plus", "tokens": [821, 11, 291, 434, 46608, 264, 8179, 2026, 295, 441, 1804], "temperature": 0.0, "avg_logprob": -0.24502397838391757, "compression_ratio": 1.439153439153439, "no_speech_prob": 2.5462632038397714e-05}, {"id": 638, "seek": 408928, "start": 4102.0, "end": 4107.320000000001, "text": " some parameter lambda times the L1 norm of S.", "tokens": [512, 13075, 13607, 1413, 264, 441, 16, 2026, 295, 318, 13], "temperature": 0.0, "avg_logprob": -0.24502397838391757, "compression_ratio": 1.439153439153439, "no_speech_prob": 2.5462632038397714e-05}, {"id": 639, "seek": 408928, "start": 4107.320000000001, "end": 4112.08, "text": " Subject to you wanting L plus S to equal M.", "tokens": [8511, 1020, 281, 291, 7935, 441, 1804, 318, 281, 2681, 376, 13], "temperature": 0.0, "avg_logprob": -0.24502397838391757, "compression_ratio": 1.439153439153439, "no_speech_prob": 2.5462632038397714e-05}, {"id": 640, "seek": 408928, "start": 4112.08, "end": 4115.360000000001, "text": " So M is your matrix that you're decomposing.", "tokens": [407, 376, 307, 428, 8141, 300, 291, 434, 22867, 6110, 13], "temperature": 0.0, "avg_logprob": -0.24502397838391757, "compression_ratio": 1.439153439153439, "no_speech_prob": 2.5462632038397714e-05}, {"id": 641, "seek": 411536, "start": 4115.36, "end": 4122.88, "text": " This is basically just saying how do we describe a sparse and low-rank matrix using math?", "tokens": [639, 307, 1936, 445, 1566, 577, 360, 321, 6786, 257, 637, 11668, 293, 2295, 12, 20479, 8141, 1228, 5221, 30], "temperature": 0.0, "avg_logprob": -0.13777803605602634, "compression_ratio": 1.791044776119403, "no_speech_prob": 8.530106242687907e-06}, {"id": 642, "seek": 411536, "start": 4122.88, "end": 4128.759999999999, "text": " The way to describe a sparse matrix is to say we're minimizing the L1 norm.", "tokens": [440, 636, 281, 6786, 257, 637, 11668, 8141, 307, 281, 584, 321, 434, 46608, 264, 441, 16, 2026, 13], "temperature": 0.0, "avg_logprob": -0.13777803605602634, "compression_ratio": 1.791044776119403, "no_speech_prob": 8.530106242687907e-06}, {"id": 643, "seek": 411536, "start": 4128.759999999999, "end": 4133.759999999999, "text": " The way to describe a low-rank matrix is minimizing something called the nuclear norm,", "tokens": [440, 636, 281, 6786, 257, 2295, 12, 20479, 8141, 307, 46608, 746, 1219, 264, 8179, 2026, 11], "temperature": 0.0, "avg_logprob": -0.13777803605602634, "compression_ratio": 1.791044776119403, "no_speech_prob": 8.530106242687907e-06}, {"id": 644, "seek": 411536, "start": 4133.759999999999, "end": 4138.08, "text": " which is the L1 norm of the singular values.", "tokens": [597, 307, 264, 441, 16, 2026, 295, 264, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.13777803605602634, "compression_ratio": 1.791044776119403, "no_speech_prob": 8.530106242687907e-06}, {"id": 645, "seek": 411536, "start": 4138.08, "end": 4143.639999999999, "text": " So you can think of this as results in sparse singular values.", "tokens": [407, 291, 393, 519, 295, 341, 382, 3542, 294, 637, 11668, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.13777803605602634, "compression_ratio": 1.791044776119403, "no_speech_prob": 8.530106242687907e-06}, {"id": 646, "seek": 414364, "start": 4143.64, "end": 4146.08, "text": " If a lot of singular values are zero,", "tokens": [759, 257, 688, 295, 20010, 4190, 366, 4018, 11], "temperature": 0.0, "avg_logprob": -0.19013144549201516, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.5611568617023295e-06}, {"id": 647, "seek": 414364, "start": 4146.08, "end": 4149.04, "text": " that means you have a low-rank matrix.", "tokens": [300, 1355, 291, 362, 257, 2295, 12, 20479, 8141, 13], "temperature": 0.0, "avg_logprob": -0.19013144549201516, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.5611568617023295e-06}, {"id": 648, "seek": 414364, "start": 4151.280000000001, "end": 4155.88, "text": " So this is just how this is formally written out.", "tokens": [407, 341, 307, 445, 577, 341, 307, 25983, 3720, 484, 13], "temperature": 0.0, "avg_logprob": -0.19013144549201516, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.5611568617023295e-06}, {"id": 649, "seek": 414364, "start": 4155.88, "end": 4161.4800000000005, "text": " I should say we're definitely not going to get deep into optimization in this course,", "tokens": [286, 820, 584, 321, 434, 2138, 406, 516, 281, 483, 2452, 666, 19618, 294, 341, 1164, 11], "temperature": 0.0, "avg_logprob": -0.19013144549201516, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.5611568617023295e-06}, {"id": 650, "seek": 414364, "start": 4161.4800000000005, "end": 4163.26, "text": " but if this is a topic that interests you,", "tokens": [457, 498, 341, 307, 257, 4829, 300, 8847, 291, 11], "temperature": 0.0, "avg_logprob": -0.19013144549201516, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.5611568617023295e-06}, {"id": 651, "seek": 414364, "start": 4163.26, "end": 4166.360000000001, "text": " optimization is a huge field.", "tokens": [19618, 307, 257, 2603, 2519, 13], "temperature": 0.0, "avg_logprob": -0.19013144549201516, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.5611568617023295e-06}, {"id": 652, "seek": 414364, "start": 4166.360000000001, "end": 4168.320000000001, "text": " I linked to in particular,", "tokens": [286, 9408, 281, 294, 1729, 11], "temperature": 0.0, "avg_logprob": -0.19013144549201516, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.5611568617023295e-06}, {"id": 653, "seek": 416832, "start": 4168.32, "end": 4175.44, "text": " Steven Boyd, who's a professor at Stanford has an online class, videos through Open edX.", "tokens": [12754, 9486, 67, 11, 567, 311, 257, 8304, 412, 20374, 575, 364, 2950, 1508, 11, 2145, 807, 7238, 1257, 55, 13], "temperature": 0.0, "avg_logprob": -0.2756874173186546, "compression_ratio": 1.4195121951219511, "no_speech_prob": 1.5293505839508725e-06}, {"id": 654, "seek": 416832, "start": 4175.44, "end": 4177.799999999999, "text": " Also, I just found this last week,", "tokens": [2743, 11, 286, 445, 1352, 341, 1036, 1243, 11], "temperature": 0.0, "avg_logprob": -0.2756874173186546, "compression_ratio": 1.4195121951219511, "no_speech_prob": 1.5293505839508725e-06}, {"id": 655, "seek": 416832, "start": 4177.799999999999, "end": 4182.799999999999, "text": " he has a tutorial or a short course he gave using Jupiter Notebook,", "tokens": [415, 575, 257, 7073, 420, 257, 2099, 1164, 415, 2729, 1228, 24567, 11633, 2939, 11], "temperature": 0.0, "avg_logprob": -0.2756874173186546, "compression_ratio": 1.4195121951219511, "no_speech_prob": 1.5293505839508725e-06}, {"id": 656, "seek": 416832, "start": 4182.799999999999, "end": 4184.4, "text": " since I have links for that.", "tokens": [1670, 286, 362, 6123, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.2756874173186546, "compression_ratio": 1.4195121951219511, "no_speech_prob": 1.5293505839508725e-06}, {"id": 657, "seek": 416832, "start": 4184.4, "end": 4188.2, "text": " Yes. Yeah. The link is in here.", "tokens": [1079, 13, 865, 13, 440, 2113, 307, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.2756874173186546, "compression_ratio": 1.4195121951219511, "no_speech_prob": 1.5293505839508725e-06}, {"id": 658, "seek": 416832, "start": 4189.719999999999, "end": 4193.679999999999, "text": " Part of what I want to show with this,", "tokens": [4100, 295, 437, 286, 528, 281, 855, 365, 341, 11], "temperature": 0.0, "avg_logprob": -0.2756874173186546, "compression_ratio": 1.4195121951219511, "no_speech_prob": 1.5293505839508725e-06}, {"id": 659, "seek": 419368, "start": 4193.68, "end": 4199.320000000001, "text": " so we'll be looking at an algorithm called primary component pursuit,", "tokens": [370, 321, 603, 312, 1237, 412, 364, 9284, 1219, 6194, 6542, 23365, 11], "temperature": 0.0, "avg_logprob": -0.1323000689347585, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.529746992280707e-06}, {"id": 660, "seek": 419368, "start": 4199.320000000001, "end": 4202.76, "text": " which is one way of doing robust PCA.", "tokens": [597, 307, 472, 636, 295, 884, 13956, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.1323000689347585, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.529746992280707e-06}, {"id": 661, "seek": 419368, "start": 4202.76, "end": 4206.16, "text": " Again, robust PCA is this problem statement of", "tokens": [3764, 11, 13956, 6465, 32, 307, 341, 1154, 5629, 295], "temperature": 0.0, "avg_logprob": -0.1323000689347585, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.529746992280707e-06}, {"id": 662, "seek": 419368, "start": 4206.16, "end": 4211.56, "text": " decomposing a matrix into a sum of a low-rank matrix and a sparse matrix.", "tokens": [22867, 6110, 257, 8141, 666, 257, 2408, 295, 257, 2295, 12, 20479, 8141, 293, 257, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1323000689347585, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.529746992280707e-06}, {"id": 663, "seek": 419368, "start": 4211.56, "end": 4217.240000000001, "text": " This is an example of implementing an algorithm from a paper.", "tokens": [639, 307, 364, 1365, 295, 18114, 364, 9284, 490, 257, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1323000689347585, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.529746992280707e-06}, {"id": 664, "seek": 419368, "start": 4217.240000000001, "end": 4222.280000000001, "text": " It's okay, we're not going to get into all the math details of this.", "tokens": [467, 311, 1392, 11, 321, 434, 406, 516, 281, 483, 666, 439, 264, 5221, 4365, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.1323000689347585, "compression_ratio": 1.6026785714285714, "no_speech_prob": 8.529746992280707e-06}, {"id": 665, "seek": 422228, "start": 4222.28, "end": 4227.16, "text": " But I wanted to more show you some of the process of doing this.", "tokens": [583, 286, 1415, 281, 544, 855, 291, 512, 295, 264, 1399, 295, 884, 341, 13], "temperature": 0.0, "avg_logprob": -0.30417429483853853, "compression_ratio": 1.2953020134228188, "no_speech_prob": 5.6817502809280995e-06}, {"id": 666, "seek": 422228, "start": 4227.32, "end": 4234.0, "text": " The original paper is this one called robust PCA.", "tokens": [440, 3380, 3035, 307, 341, 472, 1219, 13956, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.30417429483853853, "compression_ratio": 1.2953020134228188, "no_speech_prob": 5.6817502809280995e-06}, {"id": 667, "seek": 422228, "start": 4238.2, "end": 4243.96, "text": " It's by collection of researchers from Stanford,", "tokens": [467, 311, 538, 5765, 295, 10309, 490, 20374, 11], "temperature": 0.0, "avg_logprob": -0.30417429483853853, "compression_ratio": 1.2953020134228188, "no_speech_prob": 5.6817502809280995e-06}, {"id": 668, "seek": 422228, "start": 4243.96, "end": 4247.88, "text": " UIUC, and Microsoft Research.", "tokens": [15682, 23967, 11, 293, 8116, 10303, 13], "temperature": 0.0, "avg_logprob": -0.30417429483853853, "compression_ratio": 1.2953020134228188, "no_speech_prob": 5.6817502809280995e-06}, {"id": 669, "seek": 424788, "start": 4247.88, "end": 4256.68, "text": " Then there's a second paper that gives more details that's also used.", "tokens": [1396, 456, 311, 257, 1150, 3035, 300, 2709, 544, 4365, 300, 311, 611, 1143, 13], "temperature": 0.0, "avg_logprob": -0.25203619247827774, "compression_ratio": 1.5649717514124293, "no_speech_prob": 3.4799330023815855e-05}, {"id": 670, "seek": 424788, "start": 4256.68, "end": 4258.28, "text": " I have a link to that lower down.", "tokens": [286, 362, 257, 2113, 281, 300, 3126, 760, 13], "temperature": 0.0, "avg_logprob": -0.25203619247827774, "compression_ratio": 1.5649717514124293, "no_speech_prob": 3.4799330023815855e-05}, {"id": 671, "seek": 424788, "start": 4258.28, "end": 4261.400000000001, "text": " I also, in encoding this,", "tokens": [286, 611, 11, 294, 43430, 341, 11], "temperature": 0.0, "avg_logprob": -0.25203619247827774, "compression_ratio": 1.5649717514124293, "no_speech_prob": 3.4799330023815855e-05}, {"id": 672, "seek": 424788, "start": 4261.400000000001, "end": 4263.6, "text": " I looked at those, I looked at", "tokens": [286, 2956, 412, 729, 11, 286, 2956, 412], "temperature": 0.0, "avg_logprob": -0.25203619247827774, "compression_ratio": 1.5649717514124293, "no_speech_prob": 3.4799330023815855e-05}, {"id": 673, "seek": 424788, "start": 4263.6, "end": 4269.84, "text": " two existing implementations that I found online and their links here.", "tokens": [732, 6741, 4445, 763, 300, 286, 1352, 2950, 293, 641, 6123, 510, 13], "temperature": 0.0, "avg_logprob": -0.25203619247827774, "compression_ratio": 1.5649717514124293, "no_speech_prob": 3.4799330023815855e-05}, {"id": 674, "seek": 424788, "start": 4270.2, "end": 4274.52, "text": " A key thing to know is you don't need to know", "tokens": [316, 2141, 551, 281, 458, 307, 291, 500, 380, 643, 281, 458], "temperature": 0.0, "avg_logprob": -0.25203619247827774, "compression_ratio": 1.5649717514124293, "no_speech_prob": 3.4799330023815855e-05}, {"id": 675, "seek": 427452, "start": 4274.52, "end": 4278.84, "text": " the math or understand the proofs to implement an algorithm from a paper.", "tokens": [264, 5221, 420, 1223, 264, 8177, 82, 281, 4445, 364, 9284, 490, 257, 3035, 13], "temperature": 0.0, "avg_logprob": -0.12642191224179025, "compression_ratio": 1.6884615384615385, "no_speech_prob": 3.5558191484597046e-06}, {"id": 676, "seek": 427452, "start": 4278.84, "end": 4283.280000000001, "text": " It can also be very distracting to try to read all the paper.", "tokens": [467, 393, 611, 312, 588, 36689, 281, 853, 281, 1401, 439, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.12642191224179025, "compression_ratio": 1.6884615384615385, "no_speech_prob": 3.5558191484597046e-06}, {"id": 677, "seek": 427452, "start": 4283.280000000001, "end": 4286.64, "text": " Some of this depends on your purposes and what you're trying to get out of it.", "tokens": [2188, 295, 341, 5946, 322, 428, 9932, 293, 437, 291, 434, 1382, 281, 483, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.12642191224179025, "compression_ratio": 1.6884615384615385, "no_speech_prob": 3.5558191484597046e-06}, {"id": 678, "seek": 427452, "start": 4286.64, "end": 4290.96, "text": " But I think if your goal is to implement it or write the code,", "tokens": [583, 286, 519, 498, 428, 3387, 307, 281, 4445, 309, 420, 2464, 264, 3089, 11], "temperature": 0.0, "avg_logprob": -0.12642191224179025, "compression_ratio": 1.6884615384615385, "no_speech_prob": 3.5558191484597046e-06}, {"id": 679, "seek": 427452, "start": 4290.96, "end": 4293.360000000001, "text": " I think it's good to try to get there quickly and you", "tokens": [286, 519, 309, 311, 665, 281, 853, 281, 483, 456, 2661, 293, 291], "temperature": 0.0, "avg_logprob": -0.12642191224179025, "compression_ratio": 1.6884615384615385, "no_speech_prob": 3.5558191484597046e-06}, {"id": 680, "seek": 427452, "start": 4293.360000000001, "end": 4296.4400000000005, "text": " don't need to go through all the theorems.", "tokens": [500, 380, 643, 281, 352, 807, 439, 264, 10299, 2592, 13], "temperature": 0.0, "avg_logprob": -0.12642191224179025, "compression_ratio": 1.6884615384615385, "no_speech_prob": 3.5558191484597046e-06}, {"id": 681, "seek": 429644, "start": 4296.44, "end": 4304.04, "text": " For example, let me go back to the Candice Lee Mawn write paper.", "tokens": [1171, 1365, 11, 718, 385, 352, 646, 281, 264, 20466, 573, 6957, 4042, 895, 2464, 3035, 13], "temperature": 0.0, "avg_logprob": -0.3678632833189883, "compression_ratio": 1.2679738562091503, "no_speech_prob": 9.132110108112101e-07}, {"id": 682, "seek": 429644, "start": 4309.799999999999, "end": 4312.719999999999, "text": " Okay, here it is.", "tokens": [1033, 11, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.3678632833189883, "compression_ratio": 1.2679738562091503, "no_speech_prob": 9.132110108112101e-07}, {"id": 683, "seek": 429644, "start": 4314.44, "end": 4317.839999999999, "text": " This is 39 pages.", "tokens": [639, 307, 15238, 7183, 13], "temperature": 0.0, "avg_logprob": -0.3678632833189883, "compression_ratio": 1.2679738562091503, "no_speech_prob": 9.132110108112101e-07}, {"id": 684, "seek": 429644, "start": 4317.839999999999, "end": 4321.639999999999, "text": " I think the introduction is really nicely written on this,", "tokens": [286, 519, 264, 9339, 307, 534, 9594, 3720, 322, 341, 11], "temperature": 0.0, "avg_logprob": -0.3678632833189883, "compression_ratio": 1.2679738562091503, "no_speech_prob": 9.132110108112101e-07}, {"id": 685, "seek": 429644, "start": 4321.639999999999, "end": 4326.12, "text": " and explaining what robust PCA is.", "tokens": [293, 13468, 437, 13956, 6465, 32, 307, 13], "temperature": 0.0, "avg_logprob": -0.3678632833189883, "compression_ratio": 1.2679738562091503, "no_speech_prob": 9.132110108112101e-07}, {"id": 686, "seek": 432612, "start": 4326.12, "end": 4328.04, "text": " It also has some nice,", "tokens": [467, 611, 575, 512, 1481, 11], "temperature": 0.0, "avg_logprob": -0.24370695140263807, "compression_ratio": 1.4310344827586208, "no_speech_prob": 4.565793915389804e-06}, {"id": 687, "seek": 432612, "start": 4328.04, "end": 4331.12, "text": " and you can see I've based my notes on some of this.", "tokens": [293, 291, 393, 536, 286, 600, 2361, 452, 5570, 322, 512, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.24370695140263807, "compression_ratio": 1.4310344827586208, "no_speech_prob": 4.565793915389804e-06}, {"id": 688, "seek": 432612, "start": 4331.12, "end": 4334.24, "text": " It gives several examples of applications.", "tokens": [467, 2709, 2940, 5110, 295, 5821, 13], "temperature": 0.0, "avg_logprob": -0.24370695140263807, "compression_ratio": 1.4310344827586208, "no_speech_prob": 4.565793915389804e-06}, {"id": 689, "seek": 432612, "start": 4334.24, "end": 4336.92, "text": " Number one is video surveillance.", "tokens": [5118, 472, 307, 960, 18475, 13], "temperature": 0.0, "avg_logprob": -0.24370695140263807, "compression_ratio": 1.4310344827586208, "no_speech_prob": 4.565793915389804e-06}, {"id": 690, "seek": 432612, "start": 4336.92, "end": 4339.5599999999995, "text": " This is great to see.", "tokens": [639, 307, 869, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.24370695140263807, "compression_ratio": 1.4310344827586208, "no_speech_prob": 4.565793915389804e-06}, {"id": 691, "seek": 432612, "start": 4341.28, "end": 4345.0, "text": " But then if you scroll through,", "tokens": [583, 550, 498, 291, 11369, 807, 11], "temperature": 0.0, "avg_logprob": -0.24370695140263807, "compression_ratio": 1.4310344827586208, "no_speech_prob": 4.565793915389804e-06}, {"id": 692, "seek": 432612, "start": 4351.28, "end": 4354.84, "text": " you'll see a lot of this is theorems about", "tokens": [291, 603, 536, 257, 688, 295, 341, 307, 10299, 2592, 466], "temperature": 0.0, "avg_logprob": -0.24370695140263807, "compression_ratio": 1.4310344827586208, "no_speech_prob": 4.565793915389804e-06}, {"id": 693, "seek": 435484, "start": 4354.84, "end": 4358.56, "text": " the correctness of your results,", "tokens": [264, 3006, 1287, 295, 428, 3542, 11], "temperature": 0.0, "avg_logprob": -0.31707774268256295, "compression_ratio": 1.434782608695652, "no_speech_prob": 1.4063103662920184e-05}, {"id": 694, "seek": 435484, "start": 4360.04, "end": 4364.2, "text": " which are going to be less relevant.", "tokens": [597, 366, 516, 281, 312, 1570, 7340, 13], "temperature": 0.0, "avg_logprob": -0.31707774268256295, "compression_ratio": 1.434782608695652, "no_speech_prob": 1.4063103662920184e-05}, {"id": 695, "seek": 435484, "start": 4364.2, "end": 4367.72, "text": " So you actually have to go all the way down to.", "tokens": [407, 291, 767, 362, 281, 352, 439, 264, 636, 760, 281, 13], "temperature": 0.0, "avg_logprob": -0.31707774268256295, "compression_ratio": 1.434782608695652, "no_speech_prob": 1.4063103662920184e-05}, {"id": 696, "seek": 435484, "start": 4368.360000000001, "end": 4375.4800000000005, "text": " It's telling you the architecture of the proof before you even get to the proof.", "tokens": [467, 311, 3585, 291, 264, 9482, 295, 264, 8177, 949, 291, 754, 483, 281, 264, 8177, 13], "temperature": 0.0, "avg_logprob": -0.31707774268256295, "compression_ratio": 1.434782608695652, "no_speech_prob": 1.4063103662920184e-05}, {"id": 697, "seek": 437548, "start": 4375.48, "end": 4385.24, "text": " So section 2 is the architecture of the proof,", "tokens": [407, 3541, 568, 307, 264, 9482, 295, 264, 8177, 11], "temperature": 0.0, "avg_logprob": -0.28576051143177766, "compression_ratio": 1.4878048780487805, "no_speech_prob": 2.2601936962018954e-06}, {"id": 698, "seek": 437548, "start": 4385.24, "end": 4387.36, "text": " section 3 is the proofs.", "tokens": [3541, 805, 307, 264, 8177, 82, 13], "temperature": 0.0, "avg_logprob": -0.28576051143177766, "compression_ratio": 1.4878048780487805, "no_speech_prob": 2.2601936962018954e-06}, {"id": 699, "seek": 437548, "start": 4387.36, "end": 4388.679999999999, "text": " Well, part of the proofs,", "tokens": [1042, 11, 644, 295, 264, 8177, 82, 11], "temperature": 0.0, "avg_logprob": -0.28576051143177766, "compression_ratio": 1.4878048780487805, "no_speech_prob": 2.2601936962018954e-06}, {"id": 700, "seek": 437548, "start": 4388.679999999999, "end": 4391.24, "text": " and I think section 4 is also.", "tokens": [293, 286, 519, 3541, 1017, 307, 611, 13], "temperature": 0.0, "avg_logprob": -0.28576051143177766, "compression_ratio": 1.4878048780487805, "no_speech_prob": 2.2601936962018954e-06}, {"id": 701, "seek": 437548, "start": 4391.24, "end": 4395.839999999999, "text": " No, section 4 is numerical experiments, which is good.", "tokens": [883, 11, 3541, 1017, 307, 29054, 12050, 11, 597, 307, 665, 13], "temperature": 0.0, "avg_logprob": -0.28576051143177766, "compression_ratio": 1.4878048780487805, "no_speech_prob": 2.2601936962018954e-06}, {"id": 702, "seek": 439584, "start": 4395.84, "end": 4407.12, "text": " Actually, this is nice that they do show some,", "tokens": [5135, 11, 341, 307, 1481, 300, 436, 360, 855, 512, 11], "temperature": 0.0, "avg_logprob": -0.35097921305689317, "compression_ratio": 1.403973509933775, "no_speech_prob": 1.7602338857614086e-06}, {"id": 703, "seek": 439584, "start": 4407.52, "end": 4412.8, "text": " using it on the surveillance videos here.", "tokens": [1228, 309, 322, 264, 18475, 2145, 510, 13], "temperature": 0.0, "avg_logprob": -0.35097921305689317, "compression_ratio": 1.403973509933775, "no_speech_prob": 1.7602338857614086e-06}, {"id": 704, "seek": 439584, "start": 4412.96, "end": 4420.08, "text": " Zoom back out. They also used it on faces.", "tokens": [13453, 646, 484, 13, 814, 611, 1143, 309, 322, 8475, 13], "temperature": 0.0, "avg_logprob": -0.35097921305689317, "compression_ratio": 1.403973509933775, "no_speech_prob": 1.7602338857614086e-06}, {"id": 705, "seek": 439584, "start": 4420.08, "end": 4421.92, "text": " Although I think the results,", "tokens": [5780, 286, 519, 264, 3542, 11], "temperature": 0.0, "avg_logprob": -0.35097921305689317, "compression_ratio": 1.403973509933775, "no_speech_prob": 1.7602338857614086e-06}, {"id": 706, "seek": 439584, "start": 4421.92, "end": 4425.32, "text": " this here they're removing shadows from the faces.", "tokens": [341, 510, 436, 434, 12720, 14740, 490, 264, 8475, 13], "temperature": 0.0, "avg_logprob": -0.35097921305689317, "compression_ratio": 1.403973509933775, "no_speech_prob": 1.7602338857614086e-06}, {"id": 707, "seek": 442532, "start": 4425.32, "end": 4428.599999999999, "text": " So it's a lot subtler to look for.", "tokens": [407, 309, 311, 257, 688, 7257, 1918, 281, 574, 337, 13], "temperature": 0.0, "avg_logprob": -0.20497167780158226, "compression_ratio": 1.5305164319248827, "no_speech_prob": 1.3844980458088685e-05}, {"id": 708, "seek": 442532, "start": 4428.599999999999, "end": 4431.719999999999, "text": " I think you can particularly see it.", "tokens": [286, 519, 291, 393, 4098, 536, 309, 13], "temperature": 0.0, "avg_logprob": -0.20497167780158226, "compression_ratio": 1.5305164319248827, "no_speech_prob": 1.3844980458088685e-05}, {"id": 709, "seek": 442532, "start": 4433.48, "end": 4438.32, "text": " Like on this one, there's a pretty pronounced shadow here coming from the nose,", "tokens": [1743, 322, 341, 472, 11, 456, 311, 257, 1238, 23155, 8576, 510, 1348, 490, 264, 6690, 11], "temperature": 0.0, "avg_logprob": -0.20497167780158226, "compression_ratio": 1.5305164319248827, "no_speech_prob": 1.3844980458088685e-05}, {"id": 710, "seek": 442532, "start": 4438.32, "end": 4440.84, "text": " and you can see that that's been removed.", "tokens": [293, 291, 393, 536, 300, 300, 311, 668, 7261, 13], "temperature": 0.0, "avg_logprob": -0.20497167780158226, "compression_ratio": 1.5305164319248827, "no_speech_prob": 1.3844980458088685e-05}, {"id": 711, "seek": 442532, "start": 4440.84, "end": 4446.48, "text": " Coming over here. Although you've also lost the gleams in the eyes.", "tokens": [12473, 670, 510, 13, 5780, 291, 600, 611, 2731, 264, 48956, 4070, 294, 264, 2575, 13], "temperature": 0.0, "avg_logprob": -0.20497167780158226, "compression_ratio": 1.5305164319248827, "no_speech_prob": 1.3844980458088685e-05}, {"id": 712, "seek": 442532, "start": 4448.5199999999995, "end": 4452.719999999999, "text": " But in terms of actually getting to the algorithm of this paper,", "tokens": [583, 294, 2115, 295, 767, 1242, 281, 264, 9284, 295, 341, 3035, 11], "temperature": 0.0, "avg_logprob": -0.20497167780158226, "compression_ratio": 1.5305164319248827, "no_speech_prob": 1.3844980458088685e-05}, {"id": 713, "seek": 445272, "start": 4452.72, "end": 4456.320000000001, "text": " it doesn't come till page 29.", "tokens": [309, 1177, 380, 808, 4288, 3028, 9413, 13], "temperature": 0.0, "avg_logprob": -0.2420268722727329, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.7716456467460375e-06}, {"id": 714, "seek": 445272, "start": 4456.4400000000005, "end": 4459.8, "text": " You'll find the algorithm.", "tokens": [509, 603, 915, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.2420268722727329, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.7716456467460375e-06}, {"id": 715, "seek": 445272, "start": 4459.8, "end": 4463.04, "text": " I'll go back to the notebook.", "tokens": [286, 603, 352, 646, 281, 264, 21060, 13], "temperature": 0.0, "avg_logprob": -0.2420268722727329, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.7716456467460375e-06}, {"id": 716, "seek": 445272, "start": 4463.04, "end": 4465.04, "text": " I just wanted to show you that I think it can be", "tokens": [286, 445, 1415, 281, 855, 291, 300, 286, 519, 309, 393, 312], "temperature": 0.0, "avg_logprob": -0.2420268722727329, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.7716456467460375e-06}, {"id": 717, "seek": 445272, "start": 4465.04, "end": 4469.08, "text": " intimidating the amount of proofs that come before the algorithm.", "tokens": [29714, 264, 2372, 295, 8177, 82, 300, 808, 949, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.2420268722727329, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.7716456467460375e-06}, {"id": 718, "seek": 445272, "start": 4469.08, "end": 4470.84, "text": " But if the algorithm is what you're interested in,", "tokens": [583, 498, 264, 9284, 307, 437, 291, 434, 3102, 294, 11], "temperature": 0.0, "avg_logprob": -0.2420268722727329, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.7716456467460375e-06}, {"id": 719, "seek": 445272, "start": 4470.84, "end": 4473.320000000001, "text": " please skip to the algorithm.", "tokens": [1767, 10023, 281, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.2420268722727329, "compression_ratio": 1.5932203389830508, "no_speech_prob": 5.7716456467460375e-06}, {"id": 720, "seek": 447332, "start": 4473.32, "end": 4483.04, "text": " Here we are.", "tokens": [1692, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.2810397148132324, "compression_ratio": 1.3021582733812949, "no_speech_prob": 1.0288541488989722e-05}, {"id": 721, "seek": 447332, "start": 4483.04, "end": 4485.12, "text": " Here on page 29,", "tokens": [1692, 322, 3028, 9413, 11], "temperature": 0.0, "avg_logprob": -0.2810397148132324, "compression_ratio": 1.3021582733812949, "no_speech_prob": 1.0288541488989722e-05}, {"id": 722, "seek": 447332, "start": 4485.12, "end": 4490.639999999999, "text": " we have the principal component pursuit by alternating directions.", "tokens": [321, 362, 264, 9716, 6542, 23365, 538, 40062, 11095, 13], "temperature": 0.0, "avg_logprob": -0.2810397148132324, "compression_ratio": 1.3021582733812949, "no_speech_prob": 1.0288541488989722e-05}, {"id": 723, "seek": 447332, "start": 4490.639999999999, "end": 4494.28, "text": " The basic idea is that they will be taking,", "tokens": [440, 3875, 1558, 307, 300, 436, 486, 312, 1940, 11], "temperature": 0.0, "avg_logprob": -0.2810397148132324, "compression_ratio": 1.3021582733812949, "no_speech_prob": 1.0288541488989722e-05}, {"id": 724, "seek": 447332, "start": 4494.28, "end": 4502.12, "text": " and they define this operator cursive D,", "tokens": [293, 436, 6964, 341, 12973, 13946, 488, 413, 11], "temperature": 0.0, "avg_logprob": -0.2810397148132324, "compression_ratio": 1.3021582733812949, "no_speech_prob": 1.0288541488989722e-05}, {"id": 725, "seek": 450212, "start": 4502.12, "end": 4507.92, "text": " which is the singular value thresholding.", "tokens": [597, 307, 264, 20010, 2158, 14678, 278, 13], "temperature": 0.0, "avg_logprob": -0.4012422561645508, "compression_ratio": 1.072289156626506, "no_speech_prob": 3.704669143189676e-05}, {"id": 726, "seek": 450792, "start": 4507.92, "end": 4534.0, "text": " Actually, I should just show their definitions.", "tokens": [5135, 11, 286, 820, 445, 855, 641, 21988, 13], "temperature": 0.0, "avg_logprob": -0.42977594074450043, "compression_ratio": 0.921875, "no_speech_prob": 3.167941758874804e-05}, {"id": 727, "seek": 450792, "start": 4534.0, "end": 4535.72, "text": " Here it is.", "tokens": [1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.42977594074450043, "compression_ratio": 0.921875, "no_speech_prob": 3.167941758874804e-05}, {"id": 728, "seek": 453572, "start": 4535.72, "end": 4538.92, "text": " You need to know the definitions of S,", "tokens": [509, 643, 281, 458, 264, 21988, 295, 318, 11], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 729, "seek": 453572, "start": 4538.92, "end": 4540.360000000001, "text": " which they call curly S,", "tokens": [597, 436, 818, 32066, 318, 11], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 730, "seek": 453572, "start": 4540.360000000001, "end": 4541.72, "text": " the shrinkage operator,", "tokens": [264, 23060, 609, 12973, 11], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 731, "seek": 453572, "start": 4541.72, "end": 4546.12, "text": " and cursive D, the singular value thresholding operator.", "tokens": [293, 13946, 488, 413, 11, 264, 20010, 2158, 14678, 278, 12973, 13], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 732, "seek": 453572, "start": 4546.12, "end": 4551.280000000001, "text": " The shrinkage operator is basically, here it is.", "tokens": [440, 23060, 609, 12973, 307, 1936, 11, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 733, "seek": 453572, "start": 4551.280000000001, "end": 4557.68, "text": " They take the singular values and subtract a value tau off of them.", "tokens": [814, 747, 264, 20010, 4190, 293, 16390, 257, 2158, 17842, 766, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 734, "seek": 453572, "start": 4557.68, "end": 4559.400000000001, "text": " So you have this parameter tau,", "tokens": [407, 291, 362, 341, 13075, 17842, 11], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 735, "seek": 453572, "start": 4559.400000000001, "end": 4560.72, "text": " you take your singular values,", "tokens": [291, 747, 428, 20010, 4190, 11], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 736, "seek": 453572, "start": 4560.72, "end": 4561.96, "text": " subtract that off.", "tokens": [16390, 300, 766, 13], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 737, "seek": 453572, "start": 4561.96, "end": 4563.76, "text": " If the values were less than tau,", "tokens": [759, 264, 4190, 645, 1570, 813, 17842, 11], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 738, "seek": 453572, "start": 4563.76, "end": 4565.2, "text": " you just round them to zero.", "tokens": [291, 445, 3098, 552, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1580865066663354, "compression_ratio": 1.8125, "no_speech_prob": 4.907996117253788e-05}, {"id": 739, "seek": 456520, "start": 4565.2, "end": 4567.44, "text": " So you don't want to be flipping the sign on your values,", "tokens": [407, 291, 500, 380, 528, 281, 312, 26886, 264, 1465, 322, 428, 4190, 11], "temperature": 0.0, "avg_logprob": -0.16680777072906494, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.546540054026991e-05}, {"id": 740, "seek": 456520, "start": 4567.44, "end": 4572.12, "text": " but you just want to make everything that's more than tau away from zero,", "tokens": [457, 291, 445, 528, 281, 652, 1203, 300, 311, 544, 813, 17842, 1314, 490, 4018, 11], "temperature": 0.0, "avg_logprob": -0.16680777072906494, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.546540054026991e-05}, {"id": 741, "seek": 456520, "start": 4572.12, "end": 4573.5599999999995, "text": " a little bit smaller.", "tokens": [257, 707, 857, 4356, 13], "temperature": 0.0, "avg_logprob": -0.16680777072906494, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.546540054026991e-05}, {"id": 742, "seek": 456520, "start": 4573.5599999999995, "end": 4575.32, "text": " Then if it was within tau of zero,", "tokens": [1396, 498, 309, 390, 1951, 17842, 295, 4018, 11], "temperature": 0.0, "avg_logprob": -0.16680777072906494, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.546540054026991e-05}, {"id": 743, "seek": 456520, "start": 4575.32, "end": 4577.04, "text": " just round it to zero.", "tokens": [445, 3098, 309, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.16680777072906494, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.546540054026991e-05}, {"id": 744, "seek": 456520, "start": 4577.04, "end": 4581.639999999999, "text": " You have this process of making the singular value smaller.", "tokens": [509, 362, 341, 1399, 295, 1455, 264, 20010, 2158, 4356, 13], "temperature": 0.0, "avg_logprob": -0.16680777072906494, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.546540054026991e-05}, {"id": 745, "seek": 456520, "start": 4581.639999999999, "end": 4584.76, "text": " Then for curly D,", "tokens": [1396, 337, 32066, 413, 11], "temperature": 0.0, "avg_logprob": -0.16680777072906494, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.546540054026991e-05}, {"id": 746, "seek": 456520, "start": 4584.76, "end": 4586.679999999999, "text": " let me find it here.", "tokens": [718, 385, 915, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.16680777072906494, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.546540054026991e-05}, {"id": 747, "seek": 456520, "start": 4586.679999999999, "end": 4589.639999999999, "text": " That's the singular value thresholding operator.", "tokens": [663, 311, 264, 20010, 2158, 14678, 278, 12973, 13], "temperature": 0.0, "avg_logprob": -0.16680777072906494, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.546540054026991e-05}, {"id": 748, "seek": 456520, "start": 4589.639999999999, "end": 4593.32, "text": " Basically, that's taking the SVD and", "tokens": [8537, 11, 300, 311, 1940, 264, 31910, 35, 293], "temperature": 0.0, "avg_logprob": -0.16680777072906494, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.546540054026991e-05}, {"id": 749, "seek": 459332, "start": 4593.32, "end": 4596.719999999999, "text": " then making your singular value smaller,", "tokens": [550, 1455, 428, 20010, 2158, 4356, 11], "temperature": 0.0, "avg_logprob": -0.16766422311055293, "compression_ratio": 1.7892156862745099, "no_speech_prob": 1.750252158672083e-05}, {"id": 750, "seek": 459332, "start": 4596.719999999999, "end": 4599.5199999999995, "text": " and then recomposing your matrix.", "tokens": [293, 550, 48000, 6110, 428, 8141, 13], "temperature": 0.0, "avg_logprob": -0.16766422311055293, "compression_ratio": 1.7892156862745099, "no_speech_prob": 1.750252158672083e-05}, {"id": 751, "seek": 459332, "start": 4599.5199999999995, "end": 4605.08, "text": " So you're both decreasing the magnitude a little bit,", "tokens": [407, 291, 434, 1293, 23223, 264, 15668, 257, 707, 857, 11], "temperature": 0.0, "avg_logprob": -0.16766422311055293, "compression_ratio": 1.7892156862745099, "no_speech_prob": 1.750252158672083e-05}, {"id": 752, "seek": 459332, "start": 4605.08, "end": 4609.32, "text": " and you're taking some of your singular values to zero is a huge part of this.", "tokens": [293, 291, 434, 1940, 512, 295, 428, 20010, 4190, 281, 4018, 307, 257, 2603, 644, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.16766422311055293, "compression_ratio": 1.7892156862745099, "no_speech_prob": 1.750252158672083e-05}, {"id": 753, "seek": 459332, "start": 4609.32, "end": 4611.639999999999, "text": " Is that anything that was within tau of zero,", "tokens": [1119, 300, 1340, 300, 390, 1951, 17842, 295, 4018, 11], "temperature": 0.0, "avg_logprob": -0.16766422311055293, "compression_ratio": 1.7892156862745099, "no_speech_prob": 1.750252158672083e-05}, {"id": 754, "seek": 459332, "start": 4611.639999999999, "end": 4614.16, "text": " you're just rounding to zero.", "tokens": [291, 434, 445, 48237, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.16766422311055293, "compression_ratio": 1.7892156862745099, "no_speech_prob": 1.750252158672083e-05}, {"id": 755, "seek": 459332, "start": 4614.16, "end": 4617.719999999999, "text": " So you're making your singular values more sparse,", "tokens": [407, 291, 434, 1455, 428, 20010, 4190, 544, 637, 11668, 11], "temperature": 0.0, "avg_logprob": -0.16766422311055293, "compression_ratio": 1.7892156862745099, "no_speech_prob": 1.750252158672083e-05}, {"id": 756, "seek": 459332, "start": 4617.719999999999, "end": 4619.759999999999, "text": " which I'll remember is a goal.", "tokens": [597, 286, 603, 1604, 307, 257, 3387, 13], "temperature": 0.0, "avg_logprob": -0.16766422311055293, "compression_ratio": 1.7892156862745099, "no_speech_prob": 1.750252158672083e-05}, {"id": 757, "seek": 461976, "start": 4619.76, "end": 4623.64, "text": " So it's really very similar to just doing a truncated SVD", "tokens": [407, 309, 311, 534, 588, 2531, 281, 445, 884, 257, 504, 409, 66, 770, 31910, 35], "temperature": 0.0, "avg_logprob": -0.1935727549534218, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9771985282422975e-05}, {"id": 758, "seek": 461976, "start": 4623.64, "end": 4627.12, "text": " and then modulating back up together again.", "tokens": [293, 550, 1072, 12162, 646, 493, 1214, 797, 13], "temperature": 0.0, "avg_logprob": -0.1935727549534218, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9771985282422975e-05}, {"id": 759, "seek": 461976, "start": 4627.12, "end": 4628.76, "text": " That's a great point. Yeah.", "tokens": [663, 311, 257, 869, 935, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.1935727549534218, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9771985282422975e-05}, {"id": 760, "seek": 461976, "start": 4628.76, "end": 4632.64, "text": " So Jeremy just said it's similar to doing a truncated SVD,", "tokens": [407, 17809, 445, 848, 309, 311, 2531, 281, 884, 257, 504, 409, 66, 770, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.1935727549534218, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9771985282422975e-05}, {"id": 761, "seek": 461976, "start": 4632.64, "end": 4637.84, "text": " in that you can think of if", "tokens": [294, 300, 291, 393, 519, 295, 498], "temperature": 0.0, "avg_logprob": -0.1935727549534218, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9771985282422975e-05}, {"id": 762, "seek": 461976, "start": 4637.84, "end": 4643.12, "text": " the columns that you were chopping off all had tau or less for their singular values,", "tokens": [264, 13766, 300, 291, 645, 35205, 766, 439, 632, 17842, 420, 1570, 337, 641, 20010, 4190, 11], "temperature": 0.0, "avg_logprob": -0.1935727549534218, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9771985282422975e-05}, {"id": 763, "seek": 461976, "start": 4643.12, "end": 4645.400000000001, "text": " that's what you've done here. You've chopped off", "tokens": [300, 311, 437, 291, 600, 1096, 510, 13, 509, 600, 16497, 766], "temperature": 0.0, "avg_logprob": -0.1935727549534218, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9771985282422975e-05}, {"id": 764, "seek": 464540, "start": 4645.4, "end": 4650.12, "text": " everything that was less than tau by setting it to zero.", "tokens": [1203, 300, 390, 1570, 813, 17842, 538, 3287, 309, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.22819857163862747, "compression_ratio": 1.5317073170731708, "no_speech_prob": 2.178177601308562e-05}, {"id": 765, "seek": 464540, "start": 4650.12, "end": 4654.36, "text": " So this is really a lot like a truncated SVD. Thank you.", "tokens": [407, 341, 307, 534, 257, 688, 411, 257, 504, 409, 66, 770, 31910, 35, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.22819857163862747, "compression_ratio": 1.5317073170731708, "no_speech_prob": 2.178177601308562e-05}, {"id": 766, "seek": 464540, "start": 4654.36, "end": 4657.839999999999, "text": " Which makes sense because we just find that that creates a low-rank matrix.", "tokens": [3013, 1669, 2020, 570, 321, 445, 915, 300, 300, 7829, 257, 2295, 12, 20479, 8141, 13], "temperature": 0.0, "avg_logprob": -0.22819857163862747, "compression_ratio": 1.5317073170731708, "no_speech_prob": 2.178177601308562e-05}, {"id": 767, "seek": 464540, "start": 4657.839999999999, "end": 4661.16, "text": " Yes. Yeah.", "tokens": [1079, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.22819857163862747, "compression_ratio": 1.5317073170731708, "no_speech_prob": 2.178177601308562e-05}, {"id": 768, "seek": 464540, "start": 4663.16, "end": 4665.679999999999, "text": " Going back to this algorithm,", "tokens": [10963, 646, 281, 341, 9284, 11], "temperature": 0.0, "avg_logprob": -0.22819857163862747, "compression_ratio": 1.5317073170731708, "no_speech_prob": 2.178177601308562e-05}, {"id": 769, "seek": 464540, "start": 4665.679999999999, "end": 4672.92, "text": " the idea is the low-rank matrix you're creating is the singular value thresholding,", "tokens": [264, 1558, 307, 264, 2295, 12, 20479, 8141, 291, 434, 4084, 307, 264, 20010, 2158, 14678, 278, 11], "temperature": 0.0, "avg_logprob": -0.22819857163862747, "compression_ratio": 1.5317073170731708, "no_speech_prob": 2.178177601308562e-05}, {"id": 770, "seek": 467292, "start": 4672.92, "end": 4678.24, "text": " where you've just chopped off a bunch of singular values because they were tau or less.", "tokens": [689, 291, 600, 445, 16497, 766, 257, 3840, 295, 20010, 4190, 570, 436, 645, 17842, 420, 1570, 13], "temperature": 0.0, "avg_logprob": -0.1951238439324197, "compression_ratio": 1.7626262626262625, "no_speech_prob": 2.355103788431734e-05}, {"id": 771, "seek": 467292, "start": 4678.24, "end": 4682.84, "text": " The sparse matrix, here,", "tokens": [440, 637, 11668, 8141, 11, 510, 11], "temperature": 0.0, "avg_logprob": -0.1951238439324197, "compression_ratio": 1.7626262626262625, "no_speech_prob": 2.355103788431734e-05}, {"id": 772, "seek": 467292, "start": 4682.84, "end": 4689.08, "text": " they're just looking at the air that still left from your original matrix minus your low-rank,", "tokens": [436, 434, 445, 1237, 412, 264, 1988, 300, 920, 1411, 490, 428, 3380, 8141, 3175, 428, 2295, 12, 20479, 11], "temperature": 0.0, "avg_logprob": -0.1951238439324197, "compression_ratio": 1.7626262626262625, "no_speech_prob": 2.355103788431734e-05}, {"id": 773, "seek": 467292, "start": 4689.08, "end": 4695.12, "text": " because remember, you want the sparse matrix to be equal to,", "tokens": [570, 1604, 11, 291, 528, 264, 637, 11668, 8141, 281, 312, 2681, 281, 11], "temperature": 0.0, "avg_logprob": -0.1951238439324197, "compression_ratio": 1.7626262626262625, "no_speech_prob": 2.355103788431734e-05}, {"id": 774, "seek": 467292, "start": 4695.12, "end": 4699.6, "text": " or you want the sparse plus the low-rank matrices to equal your original matrix.", "tokens": [420, 291, 528, 264, 637, 11668, 1804, 264, 2295, 12, 20479, 32284, 281, 2681, 428, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1951238439324197, "compression_ratio": 1.7626262626262625, "no_speech_prob": 2.355103788431734e-05}, {"id": 775, "seek": 469960, "start": 4699.6, "end": 4703.4800000000005, "text": " So you'll notice there's this alternating that's happening between,", "tokens": [407, 291, 603, 3449, 456, 311, 341, 40062, 300, 311, 2737, 1296, 11], "temperature": 0.0, "avg_logprob": -0.1490250825881958, "compression_ratio": 1.8157894736842106, "no_speech_prob": 1.1842711501230951e-05}, {"id": 776, "seek": 469960, "start": 4703.4800000000005, "end": 4705.68, "text": " for my low-rank matrix,", "tokens": [337, 452, 2295, 12, 20479, 8141, 11], "temperature": 0.0, "avg_logprob": -0.1490250825881958, "compression_ratio": 1.8157894736842106, "no_speech_prob": 1.1842711501230951e-05}, {"id": 777, "seek": 469960, "start": 4705.68, "end": 4712.56, "text": " that's approximating the original matrix minus the sparse one,", "tokens": [300, 311, 8542, 990, 264, 3380, 8141, 3175, 264, 637, 11668, 472, 11], "temperature": 0.0, "avg_logprob": -0.1490250825881958, "compression_ratio": 1.8157894736842106, "no_speech_prob": 1.1842711501230951e-05}, {"id": 778, "seek": 469960, "start": 4712.56, "end": 4719.280000000001, "text": " and the sparse matrix is approximating the original matrix minus the low-rank one.", "tokens": [293, 264, 637, 11668, 8141, 307, 8542, 990, 264, 3380, 8141, 3175, 264, 2295, 12, 20479, 472, 13], "temperature": 0.0, "avg_logprob": -0.1490250825881958, "compression_ratio": 1.8157894736842106, "no_speech_prob": 1.1842711501230951e-05}, {"id": 779, "seek": 469960, "start": 4719.280000000001, "end": 4726.52, "text": " Then this mu times yk is keeping track", "tokens": [1396, 341, 2992, 1413, 288, 74, 307, 5145, 2837], "temperature": 0.0, "avg_logprob": -0.1490250825881958, "compression_ratio": 1.8157894736842106, "no_speech_prob": 1.1842711501230951e-05}, {"id": 780, "seek": 472652, "start": 4726.52, "end": 4740.120000000001, "text": " of what you still have left to approximate.", "tokens": [295, 437, 291, 920, 362, 1411, 281, 30874, 13], "temperature": 0.0, "avg_logprob": -0.18028265496958856, "compression_ratio": 1.3488372093023255, "no_speech_prob": 8.529965271009132e-06}, {"id": 781, "seek": 472652, "start": 4744.84, "end": 4749.360000000001, "text": " So here, I really just want you to get the broad strokes out of this.", "tokens": [407, 510, 11, 286, 534, 445, 528, 291, 281, 483, 264, 4152, 24493, 484, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.18028265496958856, "compression_ratio": 1.3488372093023255, "no_speech_prob": 8.529965271009132e-06}, {"id": 782, "seek": 472652, "start": 4749.360000000001, "end": 4754.040000000001, "text": " I think this idea of the alternating back and forth between,", "tokens": [286, 519, 341, 1558, 295, 264, 40062, 646, 293, 5220, 1296, 11], "temperature": 0.0, "avg_logprob": -0.18028265496958856, "compression_ratio": 1.3488372093023255, "no_speech_prob": 8.529965271009132e-06}, {"id": 783, "seek": 475404, "start": 4754.04, "end": 4758.4, "text": " okay, let's try to improve our estimate for the low-rank matrix,", "tokens": [1392, 11, 718, 311, 853, 281, 3470, 527, 12539, 337, 264, 2295, 12, 20479, 8141, 11], "temperature": 0.0, "avg_logprob": -0.15626161177079756, "compression_ratio": 1.7761194029850746, "no_speech_prob": 5.338069058780093e-06}, {"id": 784, "seek": 475404, "start": 4758.4, "end": 4761.24, "text": " okay, let's improve our estimate for the sparse matrix,", "tokens": [1392, 11, 718, 311, 3470, 527, 12539, 337, 264, 637, 11668, 8141, 11], "temperature": 0.0, "avg_logprob": -0.15626161177079756, "compression_ratio": 1.7761194029850746, "no_speech_prob": 5.338069058780093e-06}, {"id": 785, "seek": 475404, "start": 4761.24, "end": 4764.96, "text": " and going back and forth iteratively is a really nice pattern.", "tokens": [293, 516, 646, 293, 5220, 17138, 19020, 307, 257, 534, 1481, 5102, 13], "temperature": 0.0, "avg_logprob": -0.15626161177079756, "compression_ratio": 1.7761194029850746, "no_speech_prob": 5.338069058780093e-06}, {"id": 786, "seek": 475404, "start": 4764.96, "end": 4768.4, "text": " Then this idea of the singular value thresholding being", "tokens": [1396, 341, 1558, 295, 264, 20010, 2158, 14678, 278, 885], "temperature": 0.0, "avg_logprob": -0.15626161177079756, "compression_ratio": 1.7761194029850746, "no_speech_prob": 5.338069058780093e-06}, {"id": 787, "seek": 475404, "start": 4768.4, "end": 4773.12, "text": " really similar to just truncating a bunch of our singular values.", "tokens": [534, 2531, 281, 445, 504, 409, 66, 990, 257, 3840, 295, 527, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.15626161177079756, "compression_ratio": 1.7761194029850746, "no_speech_prob": 5.338069058780093e-06}, {"id": 788, "seek": 477312, "start": 4773.12, "end": 4783.28, "text": " Then there's another paper that builds on that one.", "tokens": [1396, 456, 311, 1071, 3035, 300, 15182, 322, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.4266524910926819, "compression_ratio": 1.1363636363636365, "no_speech_prob": 1.3006785593461245e-05}, {"id": 789, "seek": 477312, "start": 4786.8, "end": 4790.12, "text": " I don't know what happened.", "tokens": [286, 500, 380, 458, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.4266524910926819, "compression_ratio": 1.1363636363636365, "no_speech_prob": 1.3006785593461245e-05}, {"id": 790, "seek": 477312, "start": 4795.8, "end": 4798.76, "text": " Let me go back down.", "tokens": [961, 385, 352, 646, 760, 13], "temperature": 0.0, "avg_logprob": -0.4266524910926819, "compression_ratio": 1.1363636363636365, "no_speech_prob": 1.3006785593461245e-05}, {"id": 791, "seek": 479876, "start": 4798.76, "end": 4808.52, "text": " There's another paper that builds on that one and gives additional detail of how to,", "tokens": [821, 311, 1071, 3035, 300, 15182, 322, 300, 472, 293, 2709, 4497, 2607, 295, 577, 281, 11], "temperature": 0.0, "avg_logprob": -0.22134507420551347, "compression_ratio": 1.5196078431372548, "no_speech_prob": 2.5465413273195736e-05}, {"id": 792, "seek": 479876, "start": 4809.56, "end": 4812.280000000001, "text": " well, has this is great,", "tokens": [731, 11, 575, 341, 307, 869, 11], "temperature": 0.0, "avg_logprob": -0.22134507420551347, "compression_ratio": 1.5196078431372548, "no_speech_prob": 2.5465413273195736e-05}, {"id": 793, "seek": 479876, "start": 4812.280000000001, "end": 4817.24, "text": " really takes advantage of actually calculating a truncated SVD.", "tokens": [534, 2516, 5002, 295, 767, 28258, 257, 504, 409, 66, 770, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.22134507420551347, "compression_ratio": 1.5196078431372548, "no_speech_prob": 2.5465413273195736e-05}, {"id": 794, "seek": 479876, "start": 4817.24, "end": 4821.56, "text": " Even though we're doing this truncation by throwing away a bunch of our singular values,", "tokens": [2754, 1673, 321, 434, 884, 341, 504, 409, 46252, 538, 10238, 1314, 257, 3840, 295, 527, 20010, 4190, 11], "temperature": 0.0, "avg_logprob": -0.22134507420551347, "compression_ratio": 1.5196078431372548, "no_speech_prob": 2.5465413273195736e-05}, {"id": 795, "seek": 479876, "start": 4821.56, "end": 4825.4800000000005, "text": " we don't want to have to do the full SVD first.", "tokens": [321, 500, 380, 528, 281, 362, 281, 360, 264, 1577, 31910, 35, 700, 13], "temperature": 0.0, "avg_logprob": -0.22134507420551347, "compression_ratio": 1.5196078431372548, "no_speech_prob": 2.5465413273195736e-05}, {"id": 796, "seek": 482548, "start": 4825.48, "end": 4829.4, "text": " The second paper gives us estimates of like,", "tokens": [440, 1150, 3035, 2709, 505, 20561, 295, 411, 11], "temperature": 0.0, "avg_logprob": -0.19106029139624703, "compression_ratio": 1.451086956521739, "no_speech_prob": 3.3930000427062623e-06}, {"id": 797, "seek": 482548, "start": 4829.4, "end": 4833.759999999999, "text": " okay, you can just calculate this many singular values each time,", "tokens": [1392, 11, 291, 393, 445, 8873, 341, 867, 20010, 4190, 1184, 565, 11], "temperature": 0.0, "avg_logprob": -0.19106029139624703, "compression_ratio": 1.451086956521739, "no_speech_prob": 3.3930000427062623e-06}, {"id": 798, "seek": 482548, "start": 4833.759999999999, "end": 4837.919999999999, "text": " although we'll still do the thresholding where we truncate.", "tokens": [4878, 321, 603, 920, 360, 264, 14678, 278, 689, 321, 504, 409, 66, 473, 13], "temperature": 0.0, "avg_logprob": -0.19106029139624703, "compression_ratio": 1.451086956521739, "no_speech_prob": 3.3930000427062623e-06}, {"id": 799, "seek": 483792, "start": 4837.92, "end": 4861.2, "text": " That was weird, I just tried to click on a link and I don't know why it took me back to the top.", "tokens": [663, 390, 3657, 11, 286, 445, 3031, 281, 2052, 322, 257, 2113, 293, 286, 500, 380, 458, 983, 309, 1890, 385, 646, 281, 264, 1192, 13], "temperature": 0.0, "avg_logprob": -0.26155017217000326, "compression_ratio": 1.0909090909090908, "no_speech_prob": 9.817632417252753e-06}, {"id": 800, "seek": 486120, "start": 4861.2, "end": 4870.0, "text": " I'll just show the clips from it.", "tokens": [286, 603, 445, 855, 264, 13117, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.3707498990572416, "compression_ratio": 1.4488636363636365, "no_speech_prob": 1.4968940377002582e-05}, {"id": 801, "seek": 486120, "start": 4870.0, "end": 4872.599999999999, "text": " Here in this second paper,", "tokens": [1692, 294, 341, 1150, 3035, 11], "temperature": 0.0, "avg_logprob": -0.3707498990572416, "compression_ratio": 1.4488636363636365, "no_speech_prob": 1.4968940377002582e-05}, {"id": 802, "seek": 486120, "start": 4872.599999999999, "end": 4876.88, "text": " our PCA stands for robust PCA.", "tokens": [527, 6465, 32, 7382, 337, 13956, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.3707498990572416, "compression_ratio": 1.4488636363636365, "no_speech_prob": 1.4968940377002582e-05}, {"id": 803, "seek": 486120, "start": 4877.32, "end": 4882.5599999999995, "text": " They're using this optimization technique,", "tokens": [814, 434, 1228, 341, 19618, 6532, 11], "temperature": 0.0, "avg_logprob": -0.3707498990572416, "compression_ratio": 1.4488636363636365, "no_speech_prob": 1.4968940377002582e-05}, {"id": 804, "seek": 486120, "start": 4882.5599999999995, "end": 4885.639999999999, "text": " alternating Lagrange multipliers.", "tokens": [40062, 24886, 14521, 12788, 4890, 13], "temperature": 0.0, "avg_logprob": -0.3707498990572416, "compression_ratio": 1.4488636363636365, "no_speech_prob": 1.4968940377002582e-05}, {"id": 805, "seek": 486120, "start": 4885.639999999999, "end": 4891.16, "text": " Here section four in this second paper has some really helpful implementation details.", "tokens": [1692, 3541, 1451, 294, 341, 1150, 3035, 575, 512, 534, 4961, 11420, 4365, 13], "temperature": 0.0, "avg_logprob": -0.3707498990572416, "compression_ratio": 1.4488636363636365, "no_speech_prob": 1.4968940377002582e-05}, {"id": 806, "seek": 489116, "start": 4891.16, "end": 4898.76, "text": " In particular, they give you a formula of how many singular values to calculate each time.", "tokens": [682, 1729, 11, 436, 976, 291, 257, 8513, 295, 577, 867, 20010, 4190, 281, 8873, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.20674807989775246, "compression_ratio": 1.425414364640884, "no_speech_prob": 2.1110003217472695e-05}, {"id": 807, "seek": 489116, "start": 4898.76, "end": 4906.44, "text": " Basically, if it's small enough,", "tokens": [8537, 11, 498, 309, 311, 1359, 1547, 11], "temperature": 0.0, "avg_logprob": -0.20674807989775246, "compression_ratio": 1.425414364640884, "no_speech_prob": 2.1110003217472695e-05}, {"id": 808, "seek": 489116, "start": 4906.44, "end": 4907.96, "text": " you're just incrementing,", "tokens": [291, 434, 445, 26200, 278, 11], "temperature": 0.0, "avg_logprob": -0.20674807989775246, "compression_ratio": 1.425414364640884, "no_speech_prob": 2.1110003217472695e-05}, {"id": 809, "seek": 489116, "start": 4907.96, "end": 4915.599999999999, "text": " otherwise, you're taking basically like 20 percent of your dimensionality,", "tokens": [5911, 11, 291, 434, 1940, 1936, 411, 945, 3043, 295, 428, 10139, 1860, 11], "temperature": 0.0, "avg_logprob": -0.20674807989775246, "compression_ratio": 1.425414364640884, "no_speech_prob": 2.1110003217472695e-05}, {"id": 810, "seek": 489116, "start": 4915.599999999999, "end": 4920.2, "text": " so doing the minimum of this SVP.", "tokens": [370, 884, 264, 7285, 295, 341, 31910, 47, 13], "temperature": 0.0, "avg_logprob": -0.20674807989775246, "compression_ratio": 1.425414364640884, "no_speech_prob": 2.1110003217472695e-05}, {"id": 811, "seek": 492020, "start": 4920.2, "end": 4926.88, "text": " You don't want to be calculating more than 20 percent of whatever the size of your matrix is.", "tokens": [509, 500, 380, 528, 281, 312, 28258, 544, 813, 945, 3043, 295, 2035, 264, 2744, 295, 428, 8141, 307, 13], "temperature": 0.0, "avg_logprob": -0.17939250809805735, "compression_ratio": 1.4394618834080717, "no_speech_prob": 1.9032917180084041e-06}, {"id": 812, "seek": 492020, "start": 4926.88, "end": 4932.44, "text": " In section four, and this is great when papers have this,", "tokens": [682, 3541, 1451, 11, 293, 341, 307, 869, 562, 10577, 362, 341, 11], "temperature": 0.0, "avg_logprob": -0.17939250809805735, "compression_ratio": 1.4394618834080717, "no_speech_prob": 1.9032917180084041e-06}, {"id": 813, "seek": 492020, "start": 4932.44, "end": 4937.16, "text": " it gives values for the parameters to start with.", "tokens": [309, 2709, 4190, 337, 264, 9834, 281, 722, 365, 13], "temperature": 0.0, "avg_logprob": -0.17939250809805735, "compression_ratio": 1.4394618834080717, "no_speech_prob": 1.9032917180084041e-06}, {"id": 814, "seek": 492020, "start": 4937.16, "end": 4939.679999999999, "text": " Let's get down to the code.", "tokens": [961, 311, 483, 760, 281, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.17939250809805735, "compression_ratio": 1.4394618834080717, "no_speech_prob": 1.9032917180084041e-06}, {"id": 815, "seek": 493968, "start": 4939.68, "end": 4954.280000000001, "text": " Here are the links I mentioned to Stephen Boyd's Open edX videos and his Jupyter Notebooks.", "tokens": [1692, 366, 264, 6123, 286, 2835, 281, 13391, 9486, 67, 311, 7238, 1257, 55, 2145, 293, 702, 22125, 88, 391, 11633, 15170, 13], "temperature": 0.0, "avg_logprob": -0.2579630027383061, "compression_ratio": 1.3373493975903614, "no_speech_prob": 6.143733571661869e-06}, {"id": 816, "seek": 493968, "start": 4954.280000000001, "end": 4957.76, "text": " I want to show, I guess in particular,", "tokens": [286, 528, 281, 855, 11, 286, 2041, 294, 1729, 11], "temperature": 0.0, "avg_logprob": -0.2579630027383061, "compression_ratio": 1.3373493975903614, "no_speech_prob": 6.143733571661869e-06}, {"id": 817, "seek": 493968, "start": 4957.76, "end": 4968.04, "text": " it's definitely good to try to have little methods and try to keep things somewhat modular.", "tokens": [309, 311, 2138, 665, 281, 853, 281, 362, 707, 7150, 293, 853, 281, 1066, 721, 8344, 31111, 13], "temperature": 0.0, "avg_logprob": -0.2579630027383061, "compression_ratio": 1.3373493975903614, "no_speech_prob": 6.143733571661869e-06}, {"id": 818, "seek": 496804, "start": 4968.04, "end": 4970.76, "text": " I think one of the implementations I found online,", "tokens": [286, 519, 472, 295, 264, 4445, 763, 286, 1352, 2950, 11], "temperature": 0.0, "avg_logprob": -0.1544419229030609, "compression_ratio": 1.6299559471365639, "no_speech_prob": 9.515476449450944e-06}, {"id": 819, "seek": 496804, "start": 4970.76, "end": 4975.4, "text": " it was all just in one giant method and really difficult to read.", "tokens": [309, 390, 439, 445, 294, 472, 7410, 3170, 293, 534, 2252, 281, 1401, 13], "temperature": 0.0, "avg_logprob": -0.1544419229030609, "compression_ratio": 1.6299559471365639, "no_speech_prob": 9.515476449450944e-06}, {"id": 820, "seek": 496804, "start": 4975.4, "end": 4982.92, "text": " But having here calling it like a shrinkage method for the shrinkage operator that they've described.", "tokens": [583, 1419, 510, 5141, 309, 411, 257, 23060, 609, 3170, 337, 264, 23060, 609, 12973, 300, 436, 600, 7619, 13], "temperature": 0.0, "avg_logprob": -0.1544419229030609, "compression_ratio": 1.6299559471365639, "no_speech_prob": 9.515476449450944e-06}, {"id": 821, "seek": 496804, "start": 4982.92, "end": 4986.88, "text": " Remember, this is the one that just subtracts tau.", "tokens": [5459, 11, 341, 307, 264, 472, 300, 445, 16390, 82, 17842, 13], "temperature": 0.0, "avg_logprob": -0.1544419229030609, "compression_ratio": 1.6299559471365639, "no_speech_prob": 9.515476449450944e-06}, {"id": 822, "seek": 496804, "start": 4986.88, "end": 4991.04, "text": " If it was less than tau away from zero,", "tokens": [759, 309, 390, 1570, 813, 17842, 1314, 490, 4018, 11], "temperature": 0.0, "avg_logprob": -0.1544419229030609, "compression_ratio": 1.6299559471365639, "no_speech_prob": 9.515476449450944e-06}, {"id": 823, "seek": 499104, "start": 4991.04, "end": 5005.2, "text": " you're setting it to zero and so that's what this is doing.", "tokens": [291, 434, 3287, 309, 281, 4018, 293, 370, 300, 311, 437, 341, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.19815975011781203, "compression_ratio": 1.265625, "no_speech_prob": 2.4299541109940037e-05}, {"id": 824, "seek": 499104, "start": 5005.2, "end": 5014.2, "text": " Actually, I guess it contains that of subtracting off from the singular values and then taking an SVD,", "tokens": [5135, 11, 286, 2041, 309, 8306, 300, 295, 16390, 278, 766, 490, 264, 20010, 4190, 293, 550, 1940, 364, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.19815975011781203, "compression_ratio": 1.265625, "no_speech_prob": 2.4299541109940037e-05}, {"id": 825, "seek": 501420, "start": 5014.2, "end": 5021.44, "text": " subtracting off from the singular values and then multiplying it back together to reconstruct.", "tokens": [16390, 278, 766, 490, 264, 20010, 4190, 293, 550, 30955, 309, 646, 1214, 281, 31499, 13], "temperature": 0.0, "avg_logprob": -0.24872057139873505, "compression_ratio": 1.3743016759776536, "no_speech_prob": 4.198211172479205e-05}, {"id": 826, "seek": 501420, "start": 5021.44, "end": 5031.0, "text": " Yes. You'll notice here we're just calling underscore SVD.", "tokens": [1079, 13, 509, 603, 3449, 510, 321, 434, 445, 5141, 37556, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.24872057139873505, "compression_ratio": 1.3743016759776536, "no_speech_prob": 4.198211172479205e-05}, {"id": 827, "seek": 501420, "start": 5031.0, "end": 5036.16, "text": " Here underscore SVD, we've set to use FBPCA,", "tokens": [1692, 37556, 31910, 35, 11, 321, 600, 992, 281, 764, 479, 33, 12986, 32, 11], "temperature": 0.0, "avg_logprob": -0.24872057139873505, "compression_ratio": 1.3743016759776536, "no_speech_prob": 4.198211172479205e-05}, {"id": 828, "seek": 503616, "start": 5036.16, "end": 5046.04, "text": " which is a Facebook library for randomized SVD. Let me pull it up.", "tokens": [597, 307, 257, 4384, 6405, 337, 38513, 31910, 35, 13, 961, 385, 2235, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.16749115551219268, "compression_ratio": 1.3922651933701657, "no_speech_prob": 9.51546917349333e-06}, {"id": 829, "seek": 503616, "start": 5047.2, "end": 5049.72, "text": " I should have saved timings for this,", "tokens": [286, 820, 362, 6624, 524, 1109, 337, 341, 11], "temperature": 0.0, "avg_logprob": -0.16749115551219268, "compression_ratio": 1.3922651933701657, "no_speech_prob": 9.51546917349333e-06}, {"id": 830, "seek": 503616, "start": 5049.72, "end": 5055.72, "text": " but basically we just tried this with several different SVD implementations", "tokens": [457, 1936, 321, 445, 3031, 341, 365, 2940, 819, 31910, 35, 4445, 763], "temperature": 0.0, "avg_logprob": -0.16749115551219268, "compression_ratio": 1.3922651933701657, "no_speech_prob": 9.51546917349333e-06}, {"id": 831, "seek": 503616, "start": 5055.72, "end": 5060.12, "text": " to try to find what was quickest and this was fast,", "tokens": [281, 853, 281, 915, 437, 390, 49403, 293, 341, 390, 2370, 11], "temperature": 0.0, "avg_logprob": -0.16749115551219268, "compression_ratio": 1.3922651933701657, "no_speech_prob": 9.51546917349333e-06}, {"id": 832, "seek": 503616, "start": 5060.12, "end": 5061.36, "text": " so we went with it.", "tokens": [370, 321, 1437, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.16749115551219268, "compression_ratio": 1.3922651933701657, "no_speech_prob": 9.51546917349333e-06}, {"id": 833, "seek": 506136, "start": 5061.36, "end": 5068.96, "text": " But you could still use Scikit-learns randomized SVD here if you wanted.", "tokens": [583, 291, 727, 920, 764, 16942, 22681, 12, 306, 1083, 82, 38513, 31910, 35, 510, 498, 291, 1415, 13], "temperature": 0.0, "avg_logprob": -0.21156648192742858, "compression_ratio": 1.568, "no_speech_prob": 1.4738053323526401e-05}, {"id": 834, "seek": 506136, "start": 5068.96, "end": 5075.36, "text": " This is also, I think, an interesting point in that the speed of", "tokens": [639, 307, 611, 11, 286, 519, 11, 364, 1880, 935, 294, 300, 264, 3073, 295], "temperature": 0.0, "avg_logprob": -0.21156648192742858, "compression_ratio": 1.568, "no_speech_prob": 1.4738053323526401e-05}, {"id": 835, "seek": 506136, "start": 5075.36, "end": 5079.04, "text": " your implementation with these things matters and that there are different options,", "tokens": [428, 11420, 365, 613, 721, 7001, 293, 300, 456, 366, 819, 3956, 11], "temperature": 0.0, "avg_logprob": -0.21156648192742858, "compression_ratio": 1.568, "no_speech_prob": 1.4738053323526401e-05}, {"id": 836, "seek": 506136, "start": 5079.04, "end": 5084.5599999999995, "text": " but some of them, I at one point started working with this library called PyPropac.", "tokens": [457, 512, 295, 552, 11, 286, 412, 472, 935, 1409, 1364, 365, 341, 6405, 1219, 9953, 47, 1513, 326, 13], "temperature": 0.0, "avg_logprob": -0.21156648192742858, "compression_ratio": 1.568, "no_speech_prob": 1.4738053323526401e-05}, {"id": 837, "seek": 506136, "start": 5085.0, "end": 5090.839999999999, "text": " I made a pull request to update it to Python 3 and the author of the library was like,", "tokens": [286, 1027, 257, 2235, 5308, 281, 5623, 309, 281, 15329, 805, 293, 264, 3793, 295, 264, 6405, 390, 411, 11], "temperature": 0.0, "avg_logprob": -0.21156648192742858, "compression_ratio": 1.568, "no_speech_prob": 1.4738053323526401e-05}, {"id": 838, "seek": 509084, "start": 5090.84, "end": 5093.400000000001, "text": " I no longer am supporting this.", "tokens": [286, 572, 2854, 669, 7231, 341, 13], "temperature": 0.0, "avg_logprob": -0.4315208164991531, "compression_ratio": 1.5037313432835822, "no_speech_prob": 1.450908439437626e-05}, {"id": 839, "seek": 509084, "start": 5093.400000000001, "end": 5096.32, "text": " It was all based off of this library ProPac,", "tokens": [467, 390, 439, 2361, 766, 295, 341, 6405, 1705, 47, 326, 11], "temperature": 0.0, "avg_logprob": -0.4315208164991531, "compression_ratio": 1.5037313432835822, "no_speech_prob": 1.450908439437626e-05}, {"id": 840, "seek": 509084, "start": 5096.32, "end": 5100.400000000001, "text": " which came out of someone's PhD thesis", "tokens": [597, 1361, 484, 295, 1580, 311, 14476, 22288], "temperature": 0.0, "avg_logprob": -0.4315208164991531, "compression_ratio": 1.5037313432835822, "no_speech_prob": 1.450908439437626e-05}, {"id": 841, "seek": 509084, "start": 5100.400000000001, "end": 5103.68, "text": " 10 or 15 years ago as this Fortran library.", "tokens": [1266, 420, 2119, 924, 2057, 382, 341, 11002, 4257, 6405, 13], "temperature": 0.0, "avg_logprob": -0.4315208164991531, "compression_ratio": 1.5037313432835822, "no_speech_prob": 1.450908439437626e-05}, {"id": 842, "seek": 509084, "start": 5103.68, "end": 5106.32, "text": " So you do sometimes do get into", "tokens": [407, 291, 360, 2171, 360, 483, 666], "temperature": 0.0, "avg_logprob": -0.4315208164991531, "compression_ratio": 1.5037313432835822, "no_speech_prob": 1.450908439437626e-05}, {"id": 843, "seek": 509084, "start": 5106.32, "end": 5110.24, "text": " these implementation specifics even in figuring out which libraries to use.", "tokens": [613, 11420, 28454, 754, 294, 15213, 484, 597, 15148, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.4315208164991531, "compression_ratio": 1.5037313432835822, "no_speech_prob": 1.450908439437626e-05}, {"id": 844, "seek": 509084, "start": 5110.24, "end": 5112.96, "text": " So Rachel is now the extra maintainer of PyPropac.", "tokens": [407, 14246, 307, 586, 264, 2857, 6909, 260, 295, 9953, 47, 1513, 326, 13], "temperature": 0.0, "avg_logprob": -0.4315208164991531, "compression_ratio": 1.5037313432835822, "no_speech_prob": 1.450908439437626e-05}, {"id": 845, "seek": 509084, "start": 5112.96, "end": 5114.4400000000005, "text": " Thanks for the introduction.", "tokens": [2561, 337, 264, 9339, 13], "temperature": 0.0, "avg_logprob": -0.4315208164991531, "compression_ratio": 1.5037313432835822, "no_speech_prob": 1.450908439437626e-05}, {"id": 846, "seek": 509084, "start": 5114.4400000000005, "end": 5116.16, "text": " Yes. He did merge my-", "tokens": [1079, 13, 634, 630, 22183, 452, 12], "temperature": 0.0, "avg_logprob": -0.4315208164991531, "compression_ratio": 1.5037313432835822, "no_speech_prob": 1.450908439437626e-05}, {"id": 847, "seek": 509084, "start": 5116.16, "end": 5117.76, "text": " Actually, he's a poor programmer.", "tokens": [5135, 11, 415, 311, 257, 4716, 32116, 13], "temperature": 0.0, "avg_logprob": -0.4315208164991531, "compression_ratio": 1.5037313432835822, "no_speech_prob": 1.450908439437626e-05}, {"id": 848, "seek": 511776, "start": 5117.76, "end": 5121.0, "text": " Yeah. He was like, if you want this library, you can take over maintaining it.", "tokens": [865, 13, 634, 390, 411, 11, 498, 291, 528, 341, 6405, 11, 291, 393, 747, 670, 14916, 309, 13], "temperature": 0.0, "avg_logprob": -0.20551084518432616, "compression_ratio": 1.5188284518828452, "no_speech_prob": 5.337377842806745e-06}, {"id": 849, "seek": 511776, "start": 5121.0, "end": 5126.360000000001, "text": " But although he actually didn't switch it off to me.", "tokens": [583, 4878, 415, 767, 994, 380, 3679, 309, 766, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.20551084518432616, "compression_ratio": 1.5188284518828452, "no_speech_prob": 5.337377842806745e-06}, {"id": 850, "seek": 511776, "start": 5126.360000000001, "end": 5130.2, "text": " So FVPCA was fast.", "tokens": [407, 479, 53, 12986, 32, 390, 2370, 13], "temperature": 0.0, "avg_logprob": -0.20551084518432616, "compression_ratio": 1.5188284518828452, "no_speech_prob": 5.337377842806745e-06}, {"id": 851, "seek": 511776, "start": 5130.2, "end": 5132.24, "text": " I'll just say personally,", "tokens": [286, 603, 445, 584, 5665, 11], "temperature": 0.0, "avg_logprob": -0.20551084518432616, "compression_ratio": 1.5188284518828452, "no_speech_prob": 5.337377842806745e-06}, {"id": 852, "seek": 511776, "start": 5132.24, "end": 5137.64, "text": " I do find it encouraging when you see that there's a big company supporting", "tokens": [286, 360, 915, 309, 14580, 562, 291, 536, 300, 456, 311, 257, 955, 2237, 7231], "temperature": 0.0, "avg_logprob": -0.20551084518432616, "compression_ratio": 1.5188284518828452, "no_speech_prob": 5.337377842806745e-06}, {"id": 853, "seek": 511776, "start": 5137.64, "end": 5142.4400000000005, "text": " a library because that does make me feel like it'll hopefully be maintained.", "tokens": [257, 6405, 570, 300, 775, 652, 385, 841, 411, 309, 603, 4696, 312, 17578, 13], "temperature": 0.0, "avg_logprob": -0.20551084518432616, "compression_ratio": 1.5188284518828452, "no_speech_prob": 5.337377842806745e-06}, {"id": 854, "seek": 511776, "start": 5142.8, "end": 5146.2, "text": " But that's what we're using here.", "tokens": [583, 300, 311, 437, 321, 434, 1228, 510, 13], "temperature": 0.0, "avg_logprob": -0.20551084518432616, "compression_ratio": 1.5188284518828452, "no_speech_prob": 5.337377842806745e-06}, {"id": 855, "seek": 514620, "start": 5146.2, "end": 5149.16, "text": " It's a little bit confusing because they've called the method PCA,", "tokens": [467, 311, 257, 707, 857, 13181, 570, 436, 600, 1219, 264, 3170, 6465, 32, 11], "temperature": 0.0, "avg_logprob": -0.22555499381207406, "compression_ratio": 1.4977578475336324, "no_speech_prob": 1.1476786312414333e-05}, {"id": 856, "seek": 514620, "start": 5149.16, "end": 5151.4, "text": " but it's an SVD.", "tokens": [457, 309, 311, 364, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.22555499381207406, "compression_ratio": 1.4977578475336324, "no_speech_prob": 1.1476786312414333e-05}, {"id": 857, "seek": 514620, "start": 5151.4, "end": 5154.8, "text": " I find that a fair amount that there are people that", "tokens": [286, 915, 300, 257, 3143, 2372, 300, 456, 366, 561, 300], "temperature": 0.0, "avg_logprob": -0.22555499381207406, "compression_ratio": 1.4977578475336324, "no_speech_prob": 1.1476786312414333e-05}, {"id": 858, "seek": 514620, "start": 5154.8, "end": 5158.96, "text": " almost use PCA and SVD somewhat interchangeably.", "tokens": [1920, 764, 6465, 32, 293, 31910, 35, 8344, 30358, 1188, 13], "temperature": 0.0, "avg_logprob": -0.22555499381207406, "compression_ratio": 1.4977578475336324, "no_speech_prob": 1.1476786312414333e-05}, {"id": 859, "seek": 514620, "start": 5162.72, "end": 5167.16, "text": " They're slightly different.", "tokens": [814, 434, 4748, 819, 13], "temperature": 0.0, "avg_logprob": -0.22555499381207406, "compression_ratio": 1.4977578475336324, "no_speech_prob": 1.1476786312414333e-05}, {"id": 860, "seek": 514620, "start": 5167.16, "end": 5171.28, "text": " Yeah, I'm not going to get into right now unless you wanted to talk about it.", "tokens": [865, 11, 286, 478, 406, 516, 281, 483, 666, 558, 586, 5969, 291, 1415, 281, 751, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.22555499381207406, "compression_ratio": 1.4977578475336324, "no_speech_prob": 1.1476786312414333e-05}, {"id": 861, "seek": 514620, "start": 5171.28, "end": 5174.88, "text": " I mean, they're basically the same almost.", "tokens": [286, 914, 11, 436, 434, 1936, 264, 912, 1920, 13], "temperature": 0.0, "avg_logprob": -0.22555499381207406, "compression_ratio": 1.4977578475336324, "no_speech_prob": 1.1476786312414333e-05}, {"id": 862, "seek": 517488, "start": 5174.88, "end": 5179.04, "text": " I think it's worth mentioning that PCA basically uses SVD.", "tokens": [286, 519, 309, 311, 3163, 18315, 300, 6465, 32, 1936, 4960, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.34994898709383876, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012926598719786853}, {"id": 863, "seek": 517488, "start": 5179.04, "end": 5180.88, "text": " Yes. PCA uses SVD.", "tokens": [1079, 13, 6465, 32, 4960, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.34994898709383876, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012926598719786853}, {"id": 864, "seek": 517488, "start": 5180.88, "end": 5186.72, "text": " PCA is typically you're multiplying the matrix by its transpose first.", "tokens": [6465, 32, 307, 5850, 291, 434, 30955, 264, 8141, 538, 1080, 25167, 700, 13], "temperature": 0.0, "avg_logprob": -0.34994898709383876, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012926598719786853}, {"id": 865, "seek": 517488, "start": 5188.32, "end": 5190.88, "text": " No, they are. Sorry.", "tokens": [883, 11, 436, 366, 13, 4919, 13], "temperature": 0.0, "avg_logprob": -0.34994898709383876, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012926598719786853}, {"id": 866, "seek": 517488, "start": 5190.88, "end": 5194.04, "text": " I did mention they both do that.", "tokens": [286, 630, 2152, 436, 1293, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.34994898709383876, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012926598719786853}, {"id": 867, "seek": 517488, "start": 5194.04, "end": 5197.6, "text": " The only difference as far as I'm aware is that PCA subtracts the main.", "tokens": [440, 787, 2649, 382, 1400, 382, 286, 478, 3650, 307, 300, 6465, 32, 16390, 82, 264, 2135, 13], "temperature": 0.0, "avg_logprob": -0.34994898709383876, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012926598719786853}, {"id": 868, "seek": 517488, "start": 5197.6, "end": 5200.6, "text": " That's right. PCA subtracts the main.", "tokens": [663, 311, 558, 13, 6465, 32, 16390, 82, 264, 2135, 13], "temperature": 0.0, "avg_logprob": -0.34994898709383876, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012926598719786853}, {"id": 869, "seek": 517488, "start": 5200.6, "end": 5203.36, "text": " When I want these, I want they with the same.", "tokens": [1133, 286, 528, 613, 11, 286, 528, 436, 365, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.34994898709383876, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.00012926598719786853}, {"id": 870, "seek": 520336, "start": 5203.36, "end": 5205.32, "text": " They're almost the same.", "tokens": [814, 434, 1920, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.21108443648726852, "compression_ratio": 1.558659217877095, "no_speech_prob": 1.1124939192086458e-05}, {"id": 871, "seek": 520336, "start": 5205.32, "end": 5210.5199999999995, "text": " Okay. Thank you. That is something to be careful of with subtracting the mean.", "tokens": [1033, 13, 1044, 291, 13, 663, 307, 746, 281, 312, 5026, 295, 365, 16390, 278, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.21108443648726852, "compression_ratio": 1.558659217877095, "no_speech_prob": 1.1124939192086458e-05}, {"id": 872, "seek": 520336, "start": 5210.5199999999995, "end": 5213.0, "text": " If you start off with a sparse matrix,", "tokens": [759, 291, 722, 766, 365, 257, 637, 11668, 8141, 11], "temperature": 0.0, "avg_logprob": -0.21108443648726852, "compression_ratio": 1.558659217877095, "no_speech_prob": 1.1124939192086458e-05}, {"id": 873, "seek": 520336, "start": 5213.0, "end": 5215.799999999999, "text": " subtracting the mean makes it no longer sparse,", "tokens": [16390, 278, 264, 914, 1669, 309, 572, 2854, 637, 11668, 11], "temperature": 0.0, "avg_logprob": -0.21108443648726852, "compression_ratio": 1.558659217877095, "no_speech_prob": 1.1124939192086458e-05}, {"id": 874, "seek": 520336, "start": 5215.799999999999, "end": 5218.32, "text": " so you don't want to do that.", "tokens": [370, 291, 500, 380, 528, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.21108443648726852, "compression_ratio": 1.558659217877095, "no_speech_prob": 1.1124939192086458e-05}, {"id": 875, "seek": 521832, "start": 5218.32, "end": 5234.4, "text": " Okay. Actually, let me see if I want to say anything else about this.", "tokens": [1033, 13, 5135, 11, 718, 385, 536, 498, 286, 528, 281, 584, 1340, 1646, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.2416756845289661, "compression_ratio": 1.1401869158878504, "no_speech_prob": 4.565759354591137e-06}, {"id": 876, "seek": 523440, "start": 5234.4, "end": 5248.48, "text": " This is the primary component pursuit method itself.", "tokens": [639, 307, 264, 6194, 6542, 23365, 3170, 2564, 13], "temperature": 0.0, "avg_logprob": -0.32638269662857056, "compression_ratio": 1.3828125, "no_speech_prob": 2.0783654690603726e-05}, {"id": 877, "seek": 523440, "start": 5248.48, "end": 5250.36, "text": " Transposes.", "tokens": [6531, 79, 4201, 13], "temperature": 0.0, "avg_logprob": -0.32638269662857056, "compression_ratio": 1.3828125, "no_speech_prob": 2.0783654690603726e-05}, {"id": 878, "seek": 523440, "start": 5250.36, "end": 5257.4, "text": " Yes. Here, trans just transposes the matrix because this is quicker.", "tokens": [1079, 13, 1692, 11, 1145, 445, 7132, 4201, 264, 8141, 570, 341, 307, 16255, 13], "temperature": 0.0, "avg_logprob": -0.32638269662857056, "compression_ratio": 1.3828125, "no_speech_prob": 2.0783654690603726e-05}, {"id": 879, "seek": 523440, "start": 5257.4, "end": 5263.2, "text": " You want this matrix to be tall and skinny.", "tokens": [509, 528, 341, 8141, 281, 312, 6764, 293, 25193, 13], "temperature": 0.0, "avg_logprob": -0.32638269662857056, "compression_ratio": 1.3828125, "no_speech_prob": 2.0783654690603726e-05}, {"id": 880, "seek": 526320, "start": 5263.2, "end": 5264.88, "text": " If you're given the reverse,", "tokens": [759, 291, 434, 2212, 264, 9943, 11], "temperature": 0.0, "avg_logprob": -0.1878028292404978, "compression_ratio": 1.4438202247191012, "no_speech_prob": 1.3418627531791572e-05}, {"id": 881, "seek": 526320, "start": 5264.88, "end": 5267.16, "text": " you need to transpose the matrix.", "tokens": [291, 643, 281, 25167, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1878028292404978, "compression_ratio": 1.4438202247191012, "no_speech_prob": 1.3418627531791572e-05}, {"id": 882, "seek": 526320, "start": 5267.16, "end": 5270.96, "text": " Do this, it'll be quicker and then go back.", "tokens": [1144, 341, 11, 309, 603, 312, 16255, 293, 550, 352, 646, 13], "temperature": 0.0, "avg_logprob": -0.1878028292404978, "compression_ratio": 1.4438202247191012, "no_speech_prob": 1.3418627531791572e-05}, {"id": 883, "seek": 526320, "start": 5280.72, "end": 5282.599999999999, "text": " Yes. I'll just say here,", "tokens": [1079, 13, 286, 603, 445, 584, 510, 11], "temperature": 0.0, "avg_logprob": -0.1878028292404978, "compression_ratio": 1.4438202247191012, "no_speech_prob": 1.3418627531791572e-05}, {"id": 884, "seek": 526320, "start": 5282.599999999999, "end": 5285.76, "text": " we've got this loop that we're going through.", "tokens": [321, 600, 658, 341, 6367, 300, 321, 434, 516, 807, 13], "temperature": 0.0, "avg_logprob": -0.1878028292404978, "compression_ratio": 1.4438202247191012, "no_speech_prob": 1.3418627531791572e-05}, {"id": 885, "seek": 526320, "start": 5285.76, "end": 5291.679999999999, "text": " We're getting a new estimate of our sparse matrix using the shrinkage operator.", "tokens": [492, 434, 1242, 257, 777, 12539, 295, 527, 637, 11668, 8141, 1228, 264, 23060, 609, 12973, 13], "temperature": 0.0, "avg_logprob": -0.1878028292404978, "compression_ratio": 1.4438202247191012, "no_speech_prob": 1.3418627531791572e-05}, {"id": 886, "seek": 529168, "start": 5291.68, "end": 5298.400000000001, "text": " Then we're getting our low-rank matrix by reconstructing the SVD.", "tokens": [1396, 321, 434, 1242, 527, 2295, 12, 20479, 8141, 538, 31499, 278, 264, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.2572098308139377, "compression_ratio": 1.4013157894736843, "no_speech_prob": 6.4389118961116765e-06}, {"id": 887, "seek": 529168, "start": 5300.6, "end": 5307.08, "text": " We're each time updating how many singular values we want to compute,", "tokens": [492, 434, 1184, 565, 25113, 577, 867, 20010, 4190, 321, 528, 281, 14722, 11], "temperature": 0.0, "avg_logprob": -0.2572098308139377, "compression_ratio": 1.4013157894736843, "no_speech_prob": 6.4389118961116765e-06}, {"id": 888, "seek": 530708, "start": 5307.08, "end": 5325.96, "text": " seeing what our error is and adding that onto Y where we keep this total. Yeah, that's it.", "tokens": [2577, 437, 527, 6713, 307, 293, 5127, 300, 3911, 398, 689, 321, 1066, 341, 3217, 13, 865, 11, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.2289166627106843, "compression_ratio": 1.4087591240875912, "no_speech_prob": 1.892429827421438e-05}, {"id": 889, "seek": 530708, "start": 5325.96, "end": 5331.6, "text": " Next, I want to show the results that we get from this.", "tokens": [3087, 11, 286, 528, 281, 855, 264, 3542, 300, 321, 483, 490, 341, 13], "temperature": 0.0, "avg_logprob": -0.2289166627106843, "compression_ratio": 1.4087591240875912, "no_speech_prob": 1.892429827421438e-05}, {"id": 890, "seek": 530708, "start": 5331.6, "end": 5336.0, "text": " Here, it's taking as inputs the maximum number", "tokens": [1692, 11, 309, 311, 1940, 382, 15743, 264, 6674, 1230], "temperature": 0.0, "avg_logprob": -0.2289166627106843, "compression_ratio": 1.4087591240875912, "no_speech_prob": 1.892429827421438e-05}, {"id": 891, "seek": 533600, "start": 5336.0, "end": 5341.0, "text": " of iterations you want to do as well as a hyperparameter.", "tokens": [295, 36540, 291, 528, 281, 360, 382, 731, 382, 257, 9848, 2181, 335, 2398, 13], "temperature": 0.0, "avg_logprob": -0.19583980073320104, "compression_ratio": 1.583710407239819, "no_speech_prob": 2.014496385527309e-05}, {"id": 892, "seek": 533600, "start": 5341.0, "end": 5345.76, "text": " That hyperparameter, I actually", "tokens": [663, 9848, 2181, 335, 2398, 11, 286, 767], "temperature": 0.0, "avg_logprob": -0.19583980073320104, "compression_ratio": 1.583710407239819, "no_speech_prob": 2.014496385527309e-05}, {"id": 893, "seek": 533600, "start": 5345.76, "end": 5348.4, "text": " added because I was trying this on different videos and found that", "tokens": [3869, 570, 286, 390, 1382, 341, 322, 819, 2145, 293, 1352, 300], "temperature": 0.0, "avg_logprob": -0.19583980073320104, "compression_ratio": 1.583710407239819, "no_speech_prob": 2.014496385527309e-05}, {"id": 894, "seek": 533600, "start": 5348.4, "end": 5353.52, "text": " it was needing different parameter values to converge.", "tokens": [309, 390, 18006, 819, 13075, 4190, 281, 41881, 13], "temperature": 0.0, "avg_logprob": -0.19583980073320104, "compression_ratio": 1.583710407239819, "no_speech_prob": 2.014496385527309e-05}, {"id": 895, "seek": 533600, "start": 5353.52, "end": 5358.32, "text": " But you can see the error each time of how it's doing.", "tokens": [583, 291, 393, 536, 264, 6713, 1184, 565, 295, 577, 309, 311, 884, 13], "temperature": 0.0, "avg_logprob": -0.19583980073320104, "compression_ratio": 1.583710407239819, "no_speech_prob": 2.014496385527309e-05}, {"id": 896, "seek": 533600, "start": 5358.68, "end": 5360.92, "text": " These are the results.", "tokens": [1981, 366, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.19583980073320104, "compression_ratio": 1.583710407239819, "no_speech_prob": 2.014496385527309e-05}, {"id": 897, "seek": 533600, "start": 5360.92, "end": 5365.72, "text": " We've gotten back our low-rank matrix and our sparse matrix,", "tokens": [492, 600, 5768, 646, 527, 2295, 12, 20479, 8141, 293, 527, 637, 11668, 8141, 11], "temperature": 0.0, "avg_logprob": -0.19583980073320104, "compression_ratio": 1.583710407239819, "no_speech_prob": 2.014496385527309e-05}, {"id": 898, "seek": 536572, "start": 5365.72, "end": 5367.04, "text": " and we can plot them.", "tokens": [293, 321, 393, 7542, 552, 13], "temperature": 0.0, "avg_logprob": -0.1417426739708852, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.5206360330921598e-05}, {"id": 899, "seek": 536572, "start": 5367.04, "end": 5371.04, "text": " So the original, the sparse, and the low-rank.", "tokens": [407, 264, 3380, 11, 264, 637, 11668, 11, 293, 264, 2295, 12, 20479, 13], "temperature": 0.0, "avg_logprob": -0.1417426739708852, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.5206360330921598e-05}, {"id": 900, "seek": 536572, "start": 5371.04, "end": 5374.68, "text": " You will see there's still some blurs of people", "tokens": [509, 486, 536, 456, 311, 920, 512, 888, 2156, 295, 561], "temperature": 0.0, "avg_logprob": -0.1417426739708852, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.5206360330921598e-05}, {"id": 901, "seek": 536572, "start": 5374.68, "end": 5377.68, "text": " showing up that haven't been fully removed.", "tokens": [4099, 493, 300, 2378, 380, 668, 4498, 7261, 13], "temperature": 0.0, "avg_logprob": -0.1417426739708852, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.5206360330921598e-05}, {"id": 902, "seek": 536572, "start": 5377.68, "end": 5381.320000000001, "text": " I wanted to note that extracting a little bit of", "tokens": [286, 1415, 281, 3637, 300, 49844, 257, 707, 857, 295], "temperature": 0.0, "avg_logprob": -0.1417426739708852, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.5206360330921598e-05}, {"id": 903, "seek": 536572, "start": 5381.320000000001, "end": 5385.8, "text": " the foreground is easier than fully identifying the background.", "tokens": [264, 32058, 307, 3571, 813, 4498, 16696, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.1417426739708852, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.5206360330921598e-05}, {"id": 904, "seek": 536572, "start": 5385.8, "end": 5387.400000000001, "text": " You'll notice the middle pictures,", "tokens": [509, 603, 3449, 264, 2808, 5242, 11], "temperature": 0.0, "avg_logprob": -0.1417426739708852, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.5206360330921598e-05}, {"id": 905, "seek": 536572, "start": 5387.400000000001, "end": 5391.16, "text": " I think, look better than the pictures on the far right.", "tokens": [286, 519, 11, 574, 1101, 813, 264, 5242, 322, 264, 1400, 558, 13], "temperature": 0.0, "avg_logprob": -0.1417426739708852, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.5206360330921598e-05}, {"id": 906, "seek": 536572, "start": 5391.16, "end": 5393.16, "text": " That's because, I don't know,", "tokens": [663, 311, 570, 11, 286, 500, 380, 458, 11], "temperature": 0.0, "avg_logprob": -0.1417426739708852, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.5206360330921598e-05}, {"id": 907, "seek": 536572, "start": 5393.16, "end": 5394.360000000001, "text": " as long as you're close,", "tokens": [382, 938, 382, 291, 434, 1998, 11], "temperature": 0.0, "avg_logprob": -0.1417426739708852, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.5206360330921598e-05}, {"id": 908, "seek": 539436, "start": 5394.36, "end": 5396.599999999999, "text": " it'll look like people even if you don't have", "tokens": [309, 603, 574, 411, 561, 754, 498, 291, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.17018640836079915, "compression_ratio": 1.4407894736842106, "no_speech_prob": 7.0706546466681175e-06}, {"id": 909, "seek": 539436, "start": 5396.599999999999, "end": 5399.4, "text": " every piece of the person to the right intensity.", "tokens": [633, 2522, 295, 264, 954, 281, 264, 558, 13749, 13], "temperature": 0.0, "avg_logprob": -0.17018640836079915, "compression_ratio": 1.4407894736842106, "no_speech_prob": 7.0706546466681175e-06}, {"id": 910, "seek": 539436, "start": 5399.4, "end": 5402.16, "text": " Whereas with the background,", "tokens": [13813, 365, 264, 3678, 11], "temperature": 0.0, "avg_logprob": -0.17018640836079915, "compression_ratio": 1.4407894736842106, "no_speech_prob": 7.0706546466681175e-06}, {"id": 911, "seek": 539436, "start": 5402.16, "end": 5404.32, "text": " if any of the persons remaining,", "tokens": [498, 604, 295, 264, 14453, 8877, 11], "temperature": 0.0, "avg_logprob": -0.17018640836079915, "compression_ratio": 1.4407894736842106, "no_speech_prob": 7.0706546466681175e-06}, {"id": 912, "seek": 539436, "start": 5404.32, "end": 5409.32, "text": " you do have this little smudge or ghost image.", "tokens": [291, 360, 362, 341, 707, 899, 16032, 420, 8359, 3256, 13], "temperature": 0.0, "avg_logprob": -0.17018640836079915, "compression_ratio": 1.4407894736842106, "no_speech_prob": 7.0706546466681175e-06}, {"id": 913, "seek": 540932, "start": 5409.32, "end": 5427.84, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2938724909073267, "compression_ratio": 1.1531531531531531, "no_speech_prob": 3.5906141420127824e-05}, {"id": 914, "seek": 540932, "start": 5427.84, "end": 5433.84, "text": " On that note, I may stop here before we start since", "tokens": [1282, 300, 3637, 11, 286, 815, 1590, 510, 949, 321, 722, 1670], "temperature": 0.0, "avg_logprob": -0.2938724909073267, "compression_ratio": 1.1531531531531531, "no_speech_prob": 3.5906141420127824e-05}, {"id": 915, "seek": 540932, "start": 5433.84, "end": 5438.759999999999, "text": " LU factorization is a big meaty topic that we'll get to next.", "tokens": [31851, 5952, 2144, 307, 257, 955, 4615, 88, 4829, 300, 321, 603, 483, 281, 958, 13], "temperature": 0.0, "avg_logprob": -0.2938724909073267, "compression_ratio": 1.1531531531531531, "no_speech_prob": 3.5906141420127824e-05}, {"id": 916, "seek": 543876, "start": 5438.76, "end": 5441.76, "text": " You'll remember LU factorization we saw was used in", "tokens": [509, 603, 1604, 31851, 5952, 2144, 321, 1866, 390, 1143, 294], "temperature": 0.0, "avg_logprob": -0.17383255801358066, "compression_ratio": 1.553921568627451, "no_speech_prob": 4.983160033589229e-05}, {"id": 917, "seek": 543876, "start": 5441.76, "end": 5445.96, "text": " the randomized SVD that we wrote in that middle for loop,", "tokens": [264, 38513, 31910, 35, 300, 321, 4114, 294, 300, 2808, 337, 6367, 11], "temperature": 0.0, "avg_logprob": -0.17383255801358066, "compression_ratio": 1.553921568627451, "no_speech_prob": 4.983160033589229e-05}, {"id": 918, "seek": 543876, "start": 5445.96, "end": 5448.24, "text": " and it's also used in Facebook.", "tokens": [293, 309, 311, 611, 1143, 294, 4384, 13], "temperature": 0.0, "avg_logprob": -0.17383255801358066, "compression_ratio": 1.553921568627451, "no_speech_prob": 4.983160033589229e-05}, {"id": 919, "seek": 543876, "start": 5448.24, "end": 5453.280000000001, "text": " The Facebook PCA randomized SVD that we were using has it.", "tokens": [440, 4384, 6465, 32, 38513, 31910, 35, 300, 321, 645, 1228, 575, 309, 13], "temperature": 0.0, "avg_logprob": -0.17383255801358066, "compression_ratio": 1.553921568627451, "no_speech_prob": 4.983160033589229e-05}, {"id": 920, "seek": 543876, "start": 5453.280000000001, "end": 5455.24, "text": " So we're going to dig in next time", "tokens": [407, 321, 434, 516, 281, 2528, 294, 958, 565], "temperature": 0.0, "avg_logprob": -0.17383255801358066, "compression_ratio": 1.553921568627451, "no_speech_prob": 4.983160033589229e-05}, {"id": 921, "seek": 543876, "start": 5455.24, "end": 5458.68, "text": " to how LU factorization works.", "tokens": [281, 577, 31851, 5952, 2144, 1985, 13], "temperature": 0.0, "avg_logprob": -0.17383255801358066, "compression_ratio": 1.553921568627451, "no_speech_prob": 4.983160033589229e-05}, {"id": 922, "seek": 545868, "start": 5458.68, "end": 5475.68, "text": " We'll just end, I think, five minutes early.", "tokens": [50364, 492, 603, 445, 917, 11, 286, 519, 11, 1732, 2077, 2440, 13, 51214], "temperature": 0.0, "avg_logprob": -0.362941042582194, "compression_ratio": 0.8461538461538461, "no_speech_prob": 2.9737107979599386e-05}], "language": "en"}