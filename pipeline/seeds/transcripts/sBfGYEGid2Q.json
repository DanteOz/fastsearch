{"text": " Thank you all for coming this evening. This should be a really interesting set of talks. It actually kind of coincidentally works out that they're very kind of complimentary in nature. You can learn how to do deep learning and then turn around and do it on MXNet. So it totally works. So first, before we kind of even get started, I'd like to kind of have a big hand for our host here, AWS, being very gracious for this. They're providing one of the talks and the food and the venue and everything, and it's really exciting stuff. Definitely thanks to them. And so the first talk is going to be Rachel from Fast AI, and she's going to be talking about the educational aspects of how to learn to do all this cool stuff. So it should be really interesting. All right. Thanks, Rachel. Thank you. All right. How to learn deep learning when you're not a computer science PhD or even if you are. So I'm Rachel Thomas. I'm on Twitter at math underscore Rachel, and I blog about data science at fast.ai. Briefly, my background is I studied math and computer science in undergrad and then did a PhD in pure math. I worked as a quant and energy trading for two years, and then I was a data scientist and backend engineer at Uber. I taught full stack software development at Hackbrite. I really loved teaching. I had taught calculus while I was in grad school, and I think I'll probably always return to teaching. One year ago, I started fast.ai together with Jeremy Howard, who's the previous president of Kaggle and founder of Enlytic, which was the first company to use deep learning for medicine. And our goal is to make deep learning more accessible and easier to use. So I'm going to kind of describe the gap that we saw in deep learning and kind of with technical teaching, how we addressed it with our course. So we have a course that we taught in person together with the University of San Francisco's Data Institute. It was open to the community. And then we released it for free online, and over 50,000 people have started it. Oh, sorry. And then I'll share some lessons that will hopefully be useful for you on your learning journey. I'm going to begin my story at the Bay Area Machine Learning Meetup back in 2013. I want to thank Dave that he's been organizing this for so long. It's a huge help to our community. I first got interested in deep learning in 2013, and so I was really excited to see that Ilya Suskovor was going to be speaking. At that point, there were not that many deep learning materials available online, and this was before any of the open source frameworks had been released. And I was really looking for practical info on how I could implement a neural net at home, and Ilya's talk was mostly theoretical. I asked a question at the end about how he had initialized his weights, and he said, that's part of a dirty bag of tricks that nobody publishes. And I was so disappointed, but that was the message I was getting a lot of places at that point that everyone doing deep learning had gotten their PhD with the same four advisors, and they weren't publishing the practical info. The field has come a long way since then, but there's still this problem with exclusivity. This is a comment from Hacker News that was voted to the top of its thread a few months ago, and I completely disagree with it, but it's saying that to get into machine learning, you first need to spend years taking quote boring math courses, and then you need to learn CUDA and MPI, and then like years later, you can start touching Theano and TensorFlow. So I disagree with this advice, but I think this attitude kind of keeps people out of the field and from trying it. So there's an essay that I really love called A Mathematician's Lament by Paul Lockhart. Paul Lockhart was a math professor at Brown before he quit to go teach K through 12, because he thought the state of math education in the US was such a disaster, and he describes this nightmare world where children are not allowed to sing songs or play instruments until graduate school, because they need to have over a decade of studying music notation and transcribing sheet music by hand, you know, before they can be trusted to sing. Hopefully this sounds horrifying to you. He says that's what we do with math. We kind of force students to learn dry disconnected notation and save the really fun and creative parts until after most of them have dropped out of the field. And sadly, we're carrying this over into deep learning as well often. This is how the most popular textbook on deep learning encourages you to develop intuition for back propagation through time. So what do you think? Is this pretty intuitive? This is Ian Goodfellow's book, and I think it's a useful resource, but ultimately it's a theoretical math textbook. And there's no code in the book. So to summarize, some of the problems that I see with technical teaching are it tends to be math-centric. I love math, however, if you're trying to build something, code is more useful. There's something that Harvard professor David Perkins calls elementitis, and that's feeling like you have to teach each individual element before years later you can assemble those components to build something cool. And he gives an analogy with baseball and says, you know, we don't say that little kids have to memorize all the rules for baseball and understand all the technical details before they're allowed to play. You know, they can play and get a general idea of the game even if they don't know all the details. And then of the more practical or code-focused tutorials or blog posts out there, many of them are kind of on simple toy problems and subtle for these good enough results that wouldn't be acceptable in the workplace. So fast.ai, we kind of wanted to address this, where our slogan is making neural nets uncool again. And that's because being cool is about being exclusive, which is the opposite of what we want. We're particularly interested in people using small or obscure data sets and people that don't have many resources and can't afford much computation time or GPU power. So we taught a course in partnership with the University of San Francisco's Data Institute. This was open to the community, and it was one evening a week for seven weeks, although many hours of homework outside of class to go along with that. And we had this hypothesis, can we teach deep learning to coders with no explicit math prerequisites? And we weren't even sure that it could be done, but it was a big success. And we've now released the course online completely for free. All the videos are on YouTube. The code is on GitHub. And 50,000 people have started the course. And some of the success stories we've heard, one of our students who has two years of coding experience was just accepted to the Google Brain residency program, which is very prestigious. We have another student who he does not even call himself a data scientist, but he came up with a new fraud detection technique for his company. And he said it's only like 10 lines of code, but it earned him a large bonus. We have several more students that have received job offers or internship offers, that have been able to switch teams at work. I know of one student that's contributed back to TensorFlow's code, and then four students that have won hackathons. I want to introduce you to a few of our students that really, I think, capture kind of our mission. On the bottom right is Samar Haider. He's a natural language researcher in Pakistan. Pakistan has 70 spoken languages, and none of them have that many resources, not even Urdu, which is the most popular language for translation to and from. Or other kind of other text. As part of our course, Samar put together the largest corpus of Urdu that's been assembled, and he trained word embeddings on that, and has found some really interesting connections in the language. And he's largely, for machine learning, he's largely self-taught using online courses. Above him are some pictures sent to me by Sahil Singla, a farm guide in India. So thousands of farmers commit suicide every year in India, because they're taking these very predatory loans from loan sharks who then kind of threaten them with violence and harass their families. And part of the problem is they can't prove how much land they own or what type of crops they're growing. And so Sahil is scraping data from Google Earth and then applying CNNs to identify their plots and use this so that they can qualify for better loans. Bottom left is Sarah Hooker and a team of data scientists from Delta Analytics. Delta Analytics partners nonprofits with teams of data scientists who volunteer to help work with them. And Rainforest Connection is a startup that is putting recycled cell phones in endangered rainforest and then streaming audio to identify chainsaw noises and send out an alarm when there's someone cutting down the forest. And so this is a team of data scientists. They were running some tests in Berkeley, but they're using deep learning to identify chainsaws. And then above that, top left is a picture of Tosin Maisha. She's a student in Bangladesh. She's done a project analyzing how the major Bangladeshi newspaper covers violence against women by doing a kind of visualization of having scraped the text. And she's blogged about the challenges of doing machine learning with intermittent electricity. She currently has some free AWS credits that she was gifted, but she doesn't know what she's going to do when they run out because the expense is going to be a lot. And so these are the type of problems and the type of students that we are really interested in and kind of thinking about people with few resources who have projects that they're really, really interested in. So most math classes are bottom up. You learn each building block you need, and then eventually you can put them together. And it's hard for a lot of students to maintain motivation with that. It's also hard to know what the big picture is or where you're going. And it's hard to know which pieces you'll actually need. So I sometimes talk to people that are kind of studying theory that they think they're going to need for deep learning but don't actually end up needing. So we try to get students using a neural net right away and six lines of code. We spend a lot of time refactoring our code to help make it concise and modular. We give students an Amazon machine image that's set up with everything they need. And then over time we do gradually peel back the layers and get to lower level stuff. And it's always motivated though by trying to improve our performance or to kind of tackle a harder problem. So eventually they are understanding the low level components. We've just kind of gone in the reverse order. And then we are huge fans of transfer learning. Transfer learning is the technique where you take a pre-trained network that was trained on a different data set and a different problem and apply it to your problem. And often you retrain the last layer or the last few layers. And it's really powerful because it lets people with smaller data sets and limited computational power kind of take advantage of these networks that have been trained elsewhere. So this is from lesson one. And it's six lines of code and it gets you better than 97% accuracy on dogs versus cats, which has been a Kaggle competition twice now. And here we're using VGG, which was the ImageNet winner in 2014, and then just retraining the last layer. Okay, so what does all this mean for you? What kind of lessons can you take away? So my very opinionated recommendation is to start with Keras. Keras is a Python library that sits on top of either Theano or TensorFlow. And the abstractions in Keras are just so well mapped to the abstractions of neural networks that I think it makes it a lot easier to learn both. I wrote a blog post several months ago where I said using TensorFlow makes me feel like I'm not smart enough to use TensorFlow, whereas using Keras makes me feel like neural networks are easier than I realized. This was a very popular post and a lot of people said they agreed. We are using, so we used Theano and Keras in part one of the course. We do use TensorFlow and PyTorch, which is also excellent in part two, but I still really recommend starting with Keras. And I should say I have not tried MXNet yet, and so I'm looking forward to hearing the next talk about that. Without read or watch tutorials, without taking time to code, it's by coding that you learn. And then hitting Shift Enter to go through somebody else's Jupyter notebook does not count as coding. You really need to type it out yourself. You need to modify the inputs and see how that impacts the outputs. And then have a programming project that you're interested in and let that drive your learning. Don't feel like you need to spend several months learning before you can tackle the project you're interested in. Kind of start right away, and then that will keep you always learning the most relevant things you need to tackle the problem you care about. And then finally, a lot of them, a lot of programmers are very snobby about Excel, but it's actually a very useful visual tool. And so Jeremy spent a lot of time thinking about how to implement stochastic gradient descent as well as other optimizers like RMSProp and AdaGrad in Excel. And it provides a really visual picture of which cells depend on which cells and how the calculations are updating. So I encourage you to check out those notebooks. And also if you're ever stumped on a math concept, try implementing it in Excel. And then once you have it down, put it in Python. So there are two tests of whether you understand something. And one is whether you can code it and build something with it. And the other is can you explain it to somebody else? And ways to practice explaining what you're learning. One is to write a blog post. There are two blog posts by our students that are, I thought they were really well written and they were also very accessible. And remember, you don't have to be an expert to start blogging. Your target audience is the person that's just one or two steps behind you and you're actually best positioned to help that person. The other place that you can practice explaining what you're learning is answering other people's questions. And so two places you can do that are on our forums, which are forums.fast.ai. And there are always interesting discussions happening related to deep learning. So here people are talking about the Kaggle fisheries competition, about average pooling instead of dense layers, or how to use persistent AWS spot instances. So definitely, yeah, check them out. Another place that you can ask or answer questions is the learn machine learning subreddit, which is different than the machine learning subreddit. Here's a quote from one of our students after part one of the course. I personally fell into the habit of watching the lectures too much and Googling definitions too much without running the code. At first, I thought that I should read the code quickly and then spend time researching the theory behind it. In retrospect, I should have spent the majority of my time on the actual code, running it and seeing what goes into it and what comes out of it. So this is excellent advice. Learn from this person's mistake. Stay focused on the code. And summary, some keys for learning or to have a practical coding project that you care about and let that kind of lead the way for you. Get something working as fast as possible. And it's okay if you don't understand all the components at first. And that's actually better. And then kind of as time goes on, you will learn about the components as you improve your performance and dig down. And then help others answer questions and write about what you're learning. I value altruism, but that's not why I recommend it. I recommend it because it's a key part to cementing your own understanding is to explain it. Then I wanted to leave you with a few resources. So there's our free course in our forums, the Learn Machine Learning subreddit. Some of my favorite deep learning blogs are Steven Marity, Chris Ola. And he doesn't update his old blog anymore, but you should still read his old post. He now spearheads distill.pub. And Andre Carpathy, I think the Stanford CNN course, which is freely available, is also excellent. And then, and I'll tweet a link to these slides probably tomorrow. There's a link to a blog post I wrote that links to like 15 blog posts I like in the area of deep learning. Yeah, so I can take some questions now. And again, I am at math underscore Rachel on Twitter and I blog at fast.ai. I also have an ask a data scientist advice column there. Thank you. That's a great question. Okay, so the question was, you know, this is very focused on deep learning. What do I think about other tools such as random forest? And we actually in part two of our course, we use a neural net on a structured data set that you would much more traditionally see with like a random forest or XGBoost. I definitely think random forests are still worth learning about. I think it's possible in the future though that deep learning is going to be used more and more on data sets that are still being solved with random forests today. Thank you. Okay, I mean, so I would recommend with not enough data, like I think a lot's being done with data augmentation to, like, combined with neural networks to try to deal with that use case. What? I'm not familiar with Smote. Like I'm thinking of kind of, you know, just like even, you know, flipping and random crops and applying like color tents, if you're using picture data, but ways to... Oh, yeah. Yeah, so I think there are a lot of techniques for augmenting the size of your data set. And then like I think, yeah, when you have imbalanced classes, I mean, you can do, you know, imbalanced or, you know, sampling where you're kind of taking more equal numbers from each class to try to counteract that. Yes? Oh, that's a good question. I mean, I think some of it may be just like search for an interesting data set that appeals to you. And so there are a lot... I mean, Kaggle is always a great place to find interesting data sets or other resources out there. Then there are like these organizations like Delta Analytics where you can, you know, volunteer to partner with a nonprofit. Yeah, and then I think another source might be like seeing if there are any like particular papers that appeal to you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.0, "text": " Thank you all for coming this evening.", "tokens": [1044, 291, 439, 337, 1348, 341, 5634, 13], "temperature": 0.0, "avg_logprob": -0.2762721668590199, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.047927550971508026}, {"id": 1, "seek": 0, "start": 5.0, "end": 7.44, "text": " This should be a really interesting set of talks.", "tokens": [639, 820, 312, 257, 534, 1880, 992, 295, 6686, 13], "temperature": 0.0, "avg_logprob": -0.2762721668590199, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.047927550971508026}, {"id": 2, "seek": 0, "start": 7.44, "end": 11.28, "text": " It actually kind of coincidentally works out that they're very kind of complimentary in nature.", "tokens": [467, 767, 733, 295, 13001, 36578, 1985, 484, 300, 436, 434, 588, 733, 295, 47162, 294, 3687, 13], "temperature": 0.0, "avg_logprob": -0.2762721668590199, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.047927550971508026}, {"id": 3, "seek": 0, "start": 11.28, "end": 14.8, "text": " You can learn how to do deep learning and then turn around and do it on MXNet.", "tokens": [509, 393, 1466, 577, 281, 360, 2452, 2539, 293, 550, 1261, 926, 293, 360, 309, 322, 47509, 31890, 13], "temperature": 0.0, "avg_logprob": -0.2762721668590199, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.047927550971508026}, {"id": 4, "seek": 0, "start": 14.8, "end": 16.6, "text": " So it totally works.", "tokens": [407, 309, 3879, 1985, 13], "temperature": 0.0, "avg_logprob": -0.2762721668590199, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.047927550971508026}, {"id": 5, "seek": 0, "start": 16.6, "end": 20.92, "text": " So first, before we kind of even get started, I'd like to kind of have a big hand", "tokens": [407, 700, 11, 949, 321, 733, 295, 754, 483, 1409, 11, 286, 1116, 411, 281, 733, 295, 362, 257, 955, 1011], "temperature": 0.0, "avg_logprob": -0.2762721668590199, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.047927550971508026}, {"id": 6, "seek": 0, "start": 20.92, "end": 23.6, "text": " for our host here, AWS, being very gracious for this.", "tokens": [337, 527, 3975, 510, 11, 17650, 11, 885, 588, 36113, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.2762721668590199, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.047927550971508026}, {"id": 7, "seek": 2360, "start": 23.6, "end": 32.0, "text": " They're providing one of the talks and the food and the venue and everything,", "tokens": [814, 434, 6530, 472, 295, 264, 6686, 293, 264, 1755, 293, 264, 21645, 293, 1203, 11], "temperature": 0.0, "avg_logprob": -0.22160569763183594, "compression_ratio": 1.752808988764045, "no_speech_prob": 4.603574416250922e-05}, {"id": 8, "seek": 2360, "start": 32.0, "end": 33.760000000000005, "text": " and it's really exciting stuff.", "tokens": [293, 309, 311, 534, 4670, 1507, 13], "temperature": 0.0, "avg_logprob": -0.22160569763183594, "compression_ratio": 1.752808988764045, "no_speech_prob": 4.603574416250922e-05}, {"id": 9, "seek": 2360, "start": 33.760000000000005, "end": 35.2, "text": " Definitely thanks to them.", "tokens": [12151, 3231, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.22160569763183594, "compression_ratio": 1.752808988764045, "no_speech_prob": 4.603574416250922e-05}, {"id": 10, "seek": 2360, "start": 35.2, "end": 39.96, "text": " And so the first talk is going to be Rachel from Fast AI, and she's going to be talking", "tokens": [400, 370, 264, 700, 751, 307, 516, 281, 312, 14246, 490, 15968, 7318, 11, 293, 750, 311, 516, 281, 312, 1417], "temperature": 0.0, "avg_logprob": -0.22160569763183594, "compression_ratio": 1.752808988764045, "no_speech_prob": 4.603574416250922e-05}, {"id": 11, "seek": 2360, "start": 39.96, "end": 43.16, "text": " about the educational aspects of how to learn to do all this cool stuff.", "tokens": [466, 264, 10189, 7270, 295, 577, 281, 1466, 281, 360, 439, 341, 1627, 1507, 13], "temperature": 0.0, "avg_logprob": -0.22160569763183594, "compression_ratio": 1.752808988764045, "no_speech_prob": 4.603574416250922e-05}, {"id": 12, "seek": 2360, "start": 43.16, "end": 44.760000000000005, "text": " So it should be really interesting.", "tokens": [407, 309, 820, 312, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22160569763183594, "compression_ratio": 1.752808988764045, "no_speech_prob": 4.603574416250922e-05}, {"id": 13, "seek": 2360, "start": 44.760000000000005, "end": 45.6, "text": " All right. Thanks, Rachel.", "tokens": [1057, 558, 13, 2561, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.22160569763183594, "compression_ratio": 1.752808988764045, "no_speech_prob": 4.603574416250922e-05}, {"id": 14, "seek": 2360, "start": 45.6, "end": 46.400000000000006, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.22160569763183594, "compression_ratio": 1.752808988764045, "no_speech_prob": 4.603574416250922e-05}, {"id": 15, "seek": 2360, "start": 46.400000000000006, "end": 47.6, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.22160569763183594, "compression_ratio": 1.752808988764045, "no_speech_prob": 4.603574416250922e-05}, {"id": 16, "seek": 2360, "start": 47.6, "end": 52.400000000000006, "text": " How to learn deep learning when you're not a computer science PhD or even if you are.", "tokens": [1012, 281, 1466, 2452, 2539, 562, 291, 434, 406, 257, 3820, 3497, 14476, 420, 754, 498, 291, 366, 13], "temperature": 0.0, "avg_logprob": -0.22160569763183594, "compression_ratio": 1.752808988764045, "no_speech_prob": 4.603574416250922e-05}, {"id": 17, "seek": 5240, "start": 52.4, "end": 56.199999999999996, "text": " So I'm Rachel Thomas.", "tokens": [407, 286, 478, 14246, 8500, 13], "temperature": 0.0, "avg_logprob": -0.16385803537920487, "compression_ratio": 1.6510791366906474, "no_speech_prob": 1.2407899703248404e-05}, {"id": 18, "seek": 5240, "start": 56.199999999999996, "end": 61.2, "text": " I'm on Twitter at math underscore Rachel, and I blog about data science at fast.ai.", "tokens": [286, 478, 322, 5794, 412, 5221, 37556, 14246, 11, 293, 286, 6968, 466, 1412, 3497, 412, 2370, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.16385803537920487, "compression_ratio": 1.6510791366906474, "no_speech_prob": 1.2407899703248404e-05}, {"id": 19, "seek": 5240, "start": 61.2, "end": 65.0, "text": " Briefly, my background is I studied math and computer science in undergrad", "tokens": [39805, 356, 11, 452, 3678, 307, 286, 9454, 5221, 293, 3820, 3497, 294, 14295], "temperature": 0.0, "avg_logprob": -0.16385803537920487, "compression_ratio": 1.6510791366906474, "no_speech_prob": 1.2407899703248404e-05}, {"id": 20, "seek": 5240, "start": 65.0, "end": 66.8, "text": " and then did a PhD in pure math.", "tokens": [293, 550, 630, 257, 14476, 294, 6075, 5221, 13], "temperature": 0.0, "avg_logprob": -0.16385803537920487, "compression_ratio": 1.6510791366906474, "no_speech_prob": 1.2407899703248404e-05}, {"id": 21, "seek": 5240, "start": 66.8, "end": 71.68, "text": " I worked as a quant and energy trading for two years, and then I was a data scientist", "tokens": [286, 2732, 382, 257, 4426, 293, 2281, 9529, 337, 732, 924, 11, 293, 550, 286, 390, 257, 1412, 12662], "temperature": 0.0, "avg_logprob": -0.16385803537920487, "compression_ratio": 1.6510791366906474, "no_speech_prob": 1.2407899703248404e-05}, {"id": 22, "seek": 5240, "start": 71.68, "end": 73.84, "text": " and backend engineer at Uber.", "tokens": [293, 38087, 11403, 412, 21839, 13], "temperature": 0.0, "avg_logprob": -0.16385803537920487, "compression_ratio": 1.6510791366906474, "no_speech_prob": 1.2407899703248404e-05}, {"id": 23, "seek": 5240, "start": 73.84, "end": 76.6, "text": " I taught full stack software development at Hackbrite.", "tokens": [286, 5928, 1577, 8630, 4722, 3250, 412, 35170, 1443, 642, 13], "temperature": 0.0, "avg_logprob": -0.16385803537920487, "compression_ratio": 1.6510791366906474, "no_speech_prob": 1.2407899703248404e-05}, {"id": 24, "seek": 5240, "start": 76.6, "end": 77.68, "text": " I really loved teaching.", "tokens": [286, 534, 4333, 4571, 13], "temperature": 0.0, "avg_logprob": -0.16385803537920487, "compression_ratio": 1.6510791366906474, "no_speech_prob": 1.2407899703248404e-05}, {"id": 25, "seek": 5240, "start": 77.68, "end": 79.6, "text": " I had taught calculus while I was in grad school,", "tokens": [286, 632, 5928, 33400, 1339, 286, 390, 294, 2771, 1395, 11], "temperature": 0.0, "avg_logprob": -0.16385803537920487, "compression_ratio": 1.6510791366906474, "no_speech_prob": 1.2407899703248404e-05}, {"id": 26, "seek": 7960, "start": 79.6, "end": 82.6, "text": " and I think I'll probably always return to teaching.", "tokens": [293, 286, 519, 286, 603, 1391, 1009, 2736, 281, 4571, 13], "temperature": 0.0, "avg_logprob": -0.12405881711414882, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8626278688316233e-05}, {"id": 27, "seek": 7960, "start": 82.6, "end": 88.88, "text": " One year ago, I started fast.ai together with Jeremy Howard, who's the previous president", "tokens": [1485, 1064, 2057, 11, 286, 1409, 2370, 13, 1301, 1214, 365, 17809, 17626, 11, 567, 311, 264, 3894, 3868], "temperature": 0.0, "avg_logprob": -0.12405881711414882, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8626278688316233e-05}, {"id": 28, "seek": 7960, "start": 88.88, "end": 95.08, "text": " of Kaggle and founder of Enlytic, which was the first company to use deep learning for medicine.", "tokens": [295, 48751, 22631, 293, 14917, 295, 2193, 356, 40907, 11, 597, 390, 264, 700, 2237, 281, 764, 2452, 2539, 337, 7195, 13], "temperature": 0.0, "avg_logprob": -0.12405881711414882, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8626278688316233e-05}, {"id": 29, "seek": 7960, "start": 95.08, "end": 99.39999999999999, "text": " And our goal is to make deep learning more accessible and easier to use.", "tokens": [400, 527, 3387, 307, 281, 652, 2452, 2539, 544, 9515, 293, 3571, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.12405881711414882, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8626278688316233e-05}, {"id": 30, "seek": 7960, "start": 99.39999999999999, "end": 104.8, "text": " So I'm going to kind of describe the gap that we saw in deep learning and kind", "tokens": [407, 286, 478, 516, 281, 733, 295, 6786, 264, 7417, 300, 321, 1866, 294, 2452, 2539, 293, 733], "temperature": 0.0, "avg_logprob": -0.12405881711414882, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8626278688316233e-05}, {"id": 31, "seek": 7960, "start": 104.8, "end": 107.88, "text": " of with technical teaching, how we addressed it with our course.", "tokens": [295, 365, 6191, 4571, 11, 577, 321, 13847, 309, 365, 527, 1164, 13], "temperature": 0.0, "avg_logprob": -0.12405881711414882, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8626278688316233e-05}, {"id": 32, "seek": 10788, "start": 107.88, "end": 111.08, "text": " So we have a course that we taught in person together with the University", "tokens": [407, 321, 362, 257, 1164, 300, 321, 5928, 294, 954, 1214, 365, 264, 3535], "temperature": 0.0, "avg_logprob": -0.159600694602895, "compression_ratio": 1.5092250922509225, "no_speech_prob": 1.1188308235432487e-06}, {"id": 33, "seek": 10788, "start": 111.08, "end": 112.8, "text": " of San Francisco's Data Institute.", "tokens": [295, 5271, 12279, 311, 11888, 9446, 13], "temperature": 0.0, "avg_logprob": -0.159600694602895, "compression_ratio": 1.5092250922509225, "no_speech_prob": 1.1188308235432487e-06}, {"id": 34, "seek": 10788, "start": 112.8, "end": 114.0, "text": " It was open to the community.", "tokens": [467, 390, 1269, 281, 264, 1768, 13], "temperature": 0.0, "avg_logprob": -0.159600694602895, "compression_ratio": 1.5092250922509225, "no_speech_prob": 1.1188308235432487e-06}, {"id": 35, "seek": 10788, "start": 114.0, "end": 121.84, "text": " And then we released it for free online, and over 50,000 people have started it.", "tokens": [400, 550, 321, 4736, 309, 337, 1737, 2950, 11, 293, 670, 2625, 11, 1360, 561, 362, 1409, 309, 13], "temperature": 0.0, "avg_logprob": -0.159600694602895, "compression_ratio": 1.5092250922509225, "no_speech_prob": 1.1188308235432487e-06}, {"id": 36, "seek": 10788, "start": 121.84, "end": 123.08, "text": " Oh, sorry.", "tokens": [876, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.159600694602895, "compression_ratio": 1.5092250922509225, "no_speech_prob": 1.1188308235432487e-06}, {"id": 37, "seek": 10788, "start": 123.08, "end": 125.6, "text": " And then I'll share some lessons that will hopefully be useful", "tokens": [400, 550, 286, 603, 2073, 512, 8820, 300, 486, 4696, 312, 4420], "temperature": 0.0, "avg_logprob": -0.159600694602895, "compression_ratio": 1.5092250922509225, "no_speech_prob": 1.1188308235432487e-06}, {"id": 38, "seek": 10788, "start": 125.6, "end": 127.6, "text": " for you on your learning journey.", "tokens": [337, 291, 322, 428, 2539, 4671, 13], "temperature": 0.0, "avg_logprob": -0.159600694602895, "compression_ratio": 1.5092250922509225, "no_speech_prob": 1.1188308235432487e-06}, {"id": 39, "seek": 10788, "start": 127.6, "end": 134.8, "text": " I'm going to begin my story at the Bay Area Machine Learning Meetup back in 2013.", "tokens": [286, 478, 516, 281, 1841, 452, 1657, 412, 264, 7840, 19405, 22155, 15205, 22963, 1010, 646, 294, 9012, 13], "temperature": 0.0, "avg_logprob": -0.159600694602895, "compression_ratio": 1.5092250922509225, "no_speech_prob": 1.1188308235432487e-06}, {"id": 40, "seek": 13480, "start": 134.8, "end": 138.4, "text": " I want to thank Dave that he's been organizing this for so long.", "tokens": [286, 528, 281, 1309, 11017, 300, 415, 311, 668, 17608, 341, 337, 370, 938, 13], "temperature": 0.0, "avg_logprob": -0.14747095916230799, "compression_ratio": 1.6288659793814433, "no_speech_prob": 9.076415153685957e-06}, {"id": 41, "seek": 13480, "start": 138.4, "end": 142.72, "text": " It's a huge help to our community.", "tokens": [467, 311, 257, 2603, 854, 281, 527, 1768, 13], "temperature": 0.0, "avg_logprob": -0.14747095916230799, "compression_ratio": 1.6288659793814433, "no_speech_prob": 9.076415153685957e-06}, {"id": 42, "seek": 13480, "start": 142.72, "end": 147.28, "text": " I first got interested in deep learning in 2013, and so I was really excited to see", "tokens": [286, 700, 658, 3102, 294, 2452, 2539, 294, 9012, 11, 293, 370, 286, 390, 534, 2919, 281, 536], "temperature": 0.0, "avg_logprob": -0.14747095916230799, "compression_ratio": 1.6288659793814433, "no_speech_prob": 9.076415153685957e-06}, {"id": 43, "seek": 13480, "start": 147.28, "end": 149.96, "text": " that Ilya Suskovor was going to be speaking.", "tokens": [300, 286, 45106, 9545, 4093, 8453, 390, 516, 281, 312, 4124, 13], "temperature": 0.0, "avg_logprob": -0.14747095916230799, "compression_ratio": 1.6288659793814433, "no_speech_prob": 9.076415153685957e-06}, {"id": 44, "seek": 13480, "start": 149.96, "end": 154.72000000000003, "text": " At that point, there were not that many deep learning materials available online,", "tokens": [1711, 300, 935, 11, 456, 645, 406, 300, 867, 2452, 2539, 5319, 2435, 2950, 11], "temperature": 0.0, "avg_logprob": -0.14747095916230799, "compression_ratio": 1.6288659793814433, "no_speech_prob": 9.076415153685957e-06}, {"id": 45, "seek": 13480, "start": 154.72000000000003, "end": 158.68, "text": " and this was before any of the open source frameworks had been released.", "tokens": [293, 341, 390, 949, 604, 295, 264, 1269, 4009, 29834, 632, 668, 4736, 13], "temperature": 0.0, "avg_logprob": -0.14747095916230799, "compression_ratio": 1.6288659793814433, "no_speech_prob": 9.076415153685957e-06}, {"id": 46, "seek": 13480, "start": 158.68, "end": 163.52, "text": " And I was really looking for practical info on how I could implement a neural net at home,", "tokens": [400, 286, 390, 534, 1237, 337, 8496, 13614, 322, 577, 286, 727, 4445, 257, 18161, 2533, 412, 1280, 11], "temperature": 0.0, "avg_logprob": -0.14747095916230799, "compression_ratio": 1.6288659793814433, "no_speech_prob": 9.076415153685957e-06}, {"id": 47, "seek": 16352, "start": 163.52, "end": 165.96, "text": " and Ilya's talk was mostly theoretical.", "tokens": [293, 286, 45106, 311, 751, 390, 5240, 20864, 13], "temperature": 0.0, "avg_logprob": -0.12473336342842348, "compression_ratio": 1.6491803278688524, "no_speech_prob": 6.853296326880809e-06}, {"id": 48, "seek": 16352, "start": 165.96, "end": 170.96, "text": " I asked a question at the end about how he had initialized his weights, and he said,", "tokens": [286, 2351, 257, 1168, 412, 264, 917, 466, 577, 415, 632, 5883, 1602, 702, 17443, 11, 293, 415, 848, 11], "temperature": 0.0, "avg_logprob": -0.12473336342842348, "compression_ratio": 1.6491803278688524, "no_speech_prob": 6.853296326880809e-06}, {"id": 49, "seek": 16352, "start": 170.96, "end": 174.4, "text": " that's part of a dirty bag of tricks that nobody publishes.", "tokens": [300, 311, 644, 295, 257, 9360, 3411, 295, 11733, 300, 5079, 11374, 279, 13], "temperature": 0.0, "avg_logprob": -0.12473336342842348, "compression_ratio": 1.6491803278688524, "no_speech_prob": 6.853296326880809e-06}, {"id": 50, "seek": 16352, "start": 174.4, "end": 178.08, "text": " And I was so disappointed, but that was the message I was getting a lot of places", "tokens": [400, 286, 390, 370, 13856, 11, 457, 300, 390, 264, 3636, 286, 390, 1242, 257, 688, 295, 3190], "temperature": 0.0, "avg_logprob": -0.12473336342842348, "compression_ratio": 1.6491803278688524, "no_speech_prob": 6.853296326880809e-06}, {"id": 51, "seek": 16352, "start": 178.08, "end": 181.52, "text": " at that point that everyone doing deep learning had gotten their PhD", "tokens": [412, 300, 935, 300, 1518, 884, 2452, 2539, 632, 5768, 641, 14476], "temperature": 0.0, "avg_logprob": -0.12473336342842348, "compression_ratio": 1.6491803278688524, "no_speech_prob": 6.853296326880809e-06}, {"id": 52, "seek": 16352, "start": 181.52, "end": 186.20000000000002, "text": " with the same four advisors, and they weren't publishing the practical info.", "tokens": [365, 264, 912, 1451, 29136, 11, 293, 436, 4999, 380, 17832, 264, 8496, 13614, 13], "temperature": 0.0, "avg_logprob": -0.12473336342842348, "compression_ratio": 1.6491803278688524, "no_speech_prob": 6.853296326880809e-06}, {"id": 53, "seek": 16352, "start": 186.20000000000002, "end": 191.12, "text": " The field has come a long way since then, but there's still this problem with exclusivity.", "tokens": [440, 2519, 575, 808, 257, 938, 636, 1670, 550, 11, 457, 456, 311, 920, 341, 1154, 365, 15085, 4253, 13], "temperature": 0.0, "avg_logprob": -0.12473336342842348, "compression_ratio": 1.6491803278688524, "no_speech_prob": 6.853296326880809e-06}, {"id": 54, "seek": 19112, "start": 191.12, "end": 196.68, "text": " This is a comment from Hacker News that was voted to the top of its thread a few months ago,", "tokens": [639, 307, 257, 2871, 490, 389, 23599, 7987, 300, 390, 13415, 281, 264, 1192, 295, 1080, 7207, 257, 1326, 2493, 2057, 11], "temperature": 0.0, "avg_logprob": -0.12456220785776774, "compression_ratio": 1.6445993031358885, "no_speech_prob": 2.9302897019078955e-05}, {"id": 55, "seek": 19112, "start": 196.68, "end": 202.0, "text": " and I completely disagree with it, but it's saying that to get into machine learning,", "tokens": [293, 286, 2584, 14091, 365, 309, 11, 457, 309, 311, 1566, 300, 281, 483, 666, 3479, 2539, 11], "temperature": 0.0, "avg_logprob": -0.12456220785776774, "compression_ratio": 1.6445993031358885, "no_speech_prob": 2.9302897019078955e-05}, {"id": 56, "seek": 19112, "start": 202.0, "end": 207.28, "text": " you first need to spend years taking quote boring math courses, and then you need", "tokens": [291, 700, 643, 281, 3496, 924, 1940, 6513, 9989, 5221, 7712, 11, 293, 550, 291, 643], "temperature": 0.0, "avg_logprob": -0.12456220785776774, "compression_ratio": 1.6445993031358885, "no_speech_prob": 2.9302897019078955e-05}, {"id": 57, "seek": 19112, "start": 207.28, "end": 210.36, "text": " to learn CUDA and MPI, and then like years later,", "tokens": [281, 1466, 29777, 7509, 293, 14146, 40, 11, 293, 550, 411, 924, 1780, 11], "temperature": 0.0, "avg_logprob": -0.12456220785776774, "compression_ratio": 1.6445993031358885, "no_speech_prob": 2.9302897019078955e-05}, {"id": 58, "seek": 19112, "start": 210.36, "end": 213.24, "text": " you can start touching Theano and TensorFlow.", "tokens": [291, 393, 722, 11175, 440, 3730, 293, 37624, 13], "temperature": 0.0, "avg_logprob": -0.12456220785776774, "compression_ratio": 1.6445993031358885, "no_speech_prob": 2.9302897019078955e-05}, {"id": 59, "seek": 19112, "start": 213.24, "end": 216.6, "text": " So I disagree with this advice, but I think this attitude kind of keeps people", "tokens": [407, 286, 14091, 365, 341, 5192, 11, 457, 286, 519, 341, 10157, 733, 295, 5965, 561], "temperature": 0.0, "avg_logprob": -0.12456220785776774, "compression_ratio": 1.6445993031358885, "no_speech_prob": 2.9302897019078955e-05}, {"id": 60, "seek": 19112, "start": 216.6, "end": 220.88, "text": " out of the field and from trying it.", "tokens": [484, 295, 264, 2519, 293, 490, 1382, 309, 13], "temperature": 0.0, "avg_logprob": -0.12456220785776774, "compression_ratio": 1.6445993031358885, "no_speech_prob": 2.9302897019078955e-05}, {"id": 61, "seek": 22088, "start": 220.88, "end": 225.76, "text": " So there's an essay that I really love called A Mathematician's Lament by Paul Lockhart.", "tokens": [407, 456, 311, 364, 16238, 300, 286, 534, 959, 1219, 316, 15776, 14911, 952, 311, 441, 2466, 538, 4552, 16736, 42535, 13], "temperature": 0.0, "avg_logprob": -0.07452945709228516, "compression_ratio": 1.6237623762376239, "no_speech_prob": 2.076807686535176e-05}, {"id": 62, "seek": 22088, "start": 225.76, "end": 230.68, "text": " Paul Lockhart was a math professor at Brown before he quit to go teach K through 12,", "tokens": [4552, 16736, 42535, 390, 257, 5221, 8304, 412, 8030, 949, 415, 10366, 281, 352, 2924, 591, 807, 2272, 11], "temperature": 0.0, "avg_logprob": -0.07452945709228516, "compression_ratio": 1.6237623762376239, "no_speech_prob": 2.076807686535176e-05}, {"id": 63, "seek": 22088, "start": 230.68, "end": 235.28, "text": " because he thought the state of math education in the US was such a disaster,", "tokens": [570, 415, 1194, 264, 1785, 295, 5221, 3309, 294, 264, 2546, 390, 1270, 257, 11293, 11], "temperature": 0.0, "avg_logprob": -0.07452945709228516, "compression_ratio": 1.6237623762376239, "no_speech_prob": 2.076807686535176e-05}, {"id": 64, "seek": 22088, "start": 235.28, "end": 240.88, "text": " and he describes this nightmare world where children are not allowed to sing songs", "tokens": [293, 415, 15626, 341, 18724, 1002, 689, 2227, 366, 406, 4350, 281, 1522, 5781], "temperature": 0.0, "avg_logprob": -0.07452945709228516, "compression_ratio": 1.6237623762376239, "no_speech_prob": 2.076807686535176e-05}, {"id": 65, "seek": 22088, "start": 240.88, "end": 245.16, "text": " or play instruments until graduate school, because they need to have over a decade", "tokens": [420, 862, 12190, 1826, 8080, 1395, 11, 570, 436, 643, 281, 362, 670, 257, 10378], "temperature": 0.0, "avg_logprob": -0.07452945709228516, "compression_ratio": 1.6237623762376239, "no_speech_prob": 2.076807686535176e-05}, {"id": 66, "seek": 22088, "start": 245.16, "end": 249.92, "text": " of studying music notation and transcribing sheet music by hand, you know,", "tokens": [295, 7601, 1318, 24657, 293, 1145, 39541, 8193, 1318, 538, 1011, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.07452945709228516, "compression_ratio": 1.6237623762376239, "no_speech_prob": 2.076807686535176e-05}, {"id": 67, "seek": 24992, "start": 249.92, "end": 252.6, "text": " before they can be trusted to sing.", "tokens": [949, 436, 393, 312, 16034, 281, 1522, 13], "temperature": 0.0, "avg_logprob": -0.15587806701660156, "compression_ratio": 1.6377358490566039, "no_speech_prob": 5.593211881205207e-06}, {"id": 68, "seek": 24992, "start": 252.6, "end": 255.16, "text": " Hopefully this sounds horrifying to you.", "tokens": [10429, 341, 3263, 40227, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.15587806701660156, "compression_ratio": 1.6377358490566039, "no_speech_prob": 5.593211881205207e-06}, {"id": 69, "seek": 24992, "start": 255.16, "end": 256.88, "text": " He says that's what we do with math.", "tokens": [634, 1619, 300, 311, 437, 321, 360, 365, 5221, 13], "temperature": 0.0, "avg_logprob": -0.15587806701660156, "compression_ratio": 1.6377358490566039, "no_speech_prob": 5.593211881205207e-06}, {"id": 70, "seek": 24992, "start": 256.88, "end": 262.03999999999996, "text": " We kind of force students to learn dry disconnected notation and save the really fun", "tokens": [492, 733, 295, 3464, 1731, 281, 1466, 4016, 29426, 24657, 293, 3155, 264, 534, 1019], "temperature": 0.0, "avg_logprob": -0.15587806701660156, "compression_ratio": 1.6377358490566039, "no_speech_prob": 5.593211881205207e-06}, {"id": 71, "seek": 24992, "start": 262.03999999999996, "end": 266.44, "text": " and creative parts until after most of them have dropped out of the field.", "tokens": [293, 5880, 3166, 1826, 934, 881, 295, 552, 362, 8119, 484, 295, 264, 2519, 13], "temperature": 0.0, "avg_logprob": -0.15587806701660156, "compression_ratio": 1.6377358490566039, "no_speech_prob": 5.593211881205207e-06}, {"id": 72, "seek": 24992, "start": 266.44, "end": 270.52, "text": " And sadly, we're carrying this over into deep learning as well often.", "tokens": [400, 22023, 11, 321, 434, 9792, 341, 670, 666, 2452, 2539, 382, 731, 2049, 13], "temperature": 0.0, "avg_logprob": -0.15587806701660156, "compression_ratio": 1.6377358490566039, "no_speech_prob": 5.593211881205207e-06}, {"id": 73, "seek": 24992, "start": 270.52, "end": 278.08, "text": " This is how the most popular textbook on deep learning encourages you to develop intuition", "tokens": [639, 307, 577, 264, 881, 3743, 25591, 322, 2452, 2539, 28071, 291, 281, 1499, 24002], "temperature": 0.0, "avg_logprob": -0.15587806701660156, "compression_ratio": 1.6377358490566039, "no_speech_prob": 5.593211881205207e-06}, {"id": 74, "seek": 27808, "start": 278.08, "end": 280.2, "text": " for back propagation through time.", "tokens": [337, 646, 38377, 807, 565, 13], "temperature": 0.0, "avg_logprob": -0.20901348279870074, "compression_ratio": 1.5795454545454546, "no_speech_prob": 4.565322342386935e-06}, {"id": 75, "seek": 27808, "start": 280.2, "end": 281.76, "text": " So what do you think?", "tokens": [407, 437, 360, 291, 519, 30], "temperature": 0.0, "avg_logprob": -0.20901348279870074, "compression_ratio": 1.5795454545454546, "no_speech_prob": 4.565322342386935e-06}, {"id": 76, "seek": 27808, "start": 281.76, "end": 285.28, "text": " Is this pretty intuitive?", "tokens": [1119, 341, 1238, 21769, 30], "temperature": 0.0, "avg_logprob": -0.20901348279870074, "compression_ratio": 1.5795454545454546, "no_speech_prob": 4.565322342386935e-06}, {"id": 77, "seek": 27808, "start": 285.28, "end": 288.24, "text": " This is Ian Goodfellow's book, and I think it's a useful resource,", "tokens": [639, 307, 19595, 2205, 69, 21348, 311, 1446, 11, 293, 286, 519, 309, 311, 257, 4420, 7684, 11], "temperature": 0.0, "avg_logprob": -0.20901348279870074, "compression_ratio": 1.5795454545454546, "no_speech_prob": 4.565322342386935e-06}, {"id": 78, "seek": 27808, "start": 288.24, "end": 291.24, "text": " but ultimately it's a theoretical math textbook.", "tokens": [457, 6284, 309, 311, 257, 20864, 5221, 25591, 13], "temperature": 0.0, "avg_logprob": -0.20901348279870074, "compression_ratio": 1.5795454545454546, "no_speech_prob": 4.565322342386935e-06}, {"id": 79, "seek": 27808, "start": 291.24, "end": 295.0, "text": " And there's no code in the book.", "tokens": [400, 456, 311, 572, 3089, 294, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.20901348279870074, "compression_ratio": 1.5795454545454546, "no_speech_prob": 4.565322342386935e-06}, {"id": 80, "seek": 27808, "start": 295.0, "end": 299.44, "text": " So to summarize, some of the problems that I see with technical teaching are it tends", "tokens": [407, 281, 20858, 11, 512, 295, 264, 2740, 300, 286, 536, 365, 6191, 4571, 366, 309, 12258], "temperature": 0.0, "avg_logprob": -0.20901348279870074, "compression_ratio": 1.5795454545454546, "no_speech_prob": 4.565322342386935e-06}, {"id": 81, "seek": 27808, "start": 299.44, "end": 300.56, "text": " to be math-centric.", "tokens": [281, 312, 5221, 12, 45300, 13], "temperature": 0.0, "avg_logprob": -0.20901348279870074, "compression_ratio": 1.5795454545454546, "no_speech_prob": 4.565322342386935e-06}, {"id": 82, "seek": 27808, "start": 300.56, "end": 307.0, "text": " I love math, however, if you're trying to build something, code is more useful.", "tokens": [286, 959, 5221, 11, 4461, 11, 498, 291, 434, 1382, 281, 1322, 746, 11, 3089, 307, 544, 4420, 13], "temperature": 0.0, "avg_logprob": -0.20901348279870074, "compression_ratio": 1.5795454545454546, "no_speech_prob": 4.565322342386935e-06}, {"id": 83, "seek": 30700, "start": 307.0, "end": 311.96, "text": " There's something that Harvard professor David Perkins calls elementitis, and that's feeling", "tokens": [821, 311, 746, 300, 13378, 8304, 4389, 3026, 10277, 5498, 4478, 16074, 11, 293, 300, 311, 2633], "temperature": 0.0, "avg_logprob": -0.1060266258302799, "compression_ratio": 1.74, "no_speech_prob": 1.320857791142771e-05}, {"id": 84, "seek": 30700, "start": 311.96, "end": 316.68, "text": " like you have to teach each individual element before years later you can assemble those", "tokens": [411, 291, 362, 281, 2924, 1184, 2609, 4478, 949, 924, 1780, 291, 393, 22364, 729], "temperature": 0.0, "avg_logprob": -0.1060266258302799, "compression_ratio": 1.74, "no_speech_prob": 1.320857791142771e-05}, {"id": 85, "seek": 30700, "start": 316.68, "end": 319.12, "text": " components to build something cool.", "tokens": [6677, 281, 1322, 746, 1627, 13], "temperature": 0.0, "avg_logprob": -0.1060266258302799, "compression_ratio": 1.74, "no_speech_prob": 1.320857791142771e-05}, {"id": 86, "seek": 30700, "start": 319.12, "end": 323.48, "text": " And he gives an analogy with baseball and says, you know, we don't say that little kids", "tokens": [400, 415, 2709, 364, 21663, 365, 14323, 293, 1619, 11, 291, 458, 11, 321, 500, 380, 584, 300, 707, 2301], "temperature": 0.0, "avg_logprob": -0.1060266258302799, "compression_ratio": 1.74, "no_speech_prob": 1.320857791142771e-05}, {"id": 87, "seek": 30700, "start": 323.48, "end": 327.64, "text": " have to memorize all the rules for baseball and understand all the technical details before", "tokens": [362, 281, 27478, 439, 264, 4474, 337, 14323, 293, 1223, 439, 264, 6191, 4365, 949], "temperature": 0.0, "avg_logprob": -0.1060266258302799, "compression_ratio": 1.74, "no_speech_prob": 1.320857791142771e-05}, {"id": 88, "seek": 30700, "start": 327.64, "end": 328.64, "text": " they're allowed to play.", "tokens": [436, 434, 4350, 281, 862, 13], "temperature": 0.0, "avg_logprob": -0.1060266258302799, "compression_ratio": 1.74, "no_speech_prob": 1.320857791142771e-05}, {"id": 89, "seek": 30700, "start": 328.64, "end": 332.92, "text": " You know, they can play and get a general idea of the game even if they don't know all", "tokens": [509, 458, 11, 436, 393, 862, 293, 483, 257, 2674, 1558, 295, 264, 1216, 754, 498, 436, 500, 380, 458, 439], "temperature": 0.0, "avg_logprob": -0.1060266258302799, "compression_ratio": 1.74, "no_speech_prob": 1.320857791142771e-05}, {"id": 90, "seek": 30700, "start": 332.92, "end": 334.72, "text": " the details.", "tokens": [264, 4365, 13], "temperature": 0.0, "avg_logprob": -0.1060266258302799, "compression_ratio": 1.74, "no_speech_prob": 1.320857791142771e-05}, {"id": 91, "seek": 33472, "start": 334.72, "end": 340.48, "text": " And then of the more practical or code-focused tutorials or blog posts out there, many of", "tokens": [400, 550, 295, 264, 544, 8496, 420, 3089, 12, 44062, 17616, 420, 6968, 12300, 484, 456, 11, 867, 295], "temperature": 0.0, "avg_logprob": -0.13577699198306187, "compression_ratio": 1.6181102362204725, "no_speech_prob": 5.173266345082084e-06}, {"id": 92, "seek": 33472, "start": 340.48, "end": 345.8, "text": " them are kind of on simple toy problems and subtle for these good enough results that", "tokens": [552, 366, 733, 295, 322, 2199, 12058, 2740, 293, 13743, 337, 613, 665, 1547, 3542, 300], "temperature": 0.0, "avg_logprob": -0.13577699198306187, "compression_ratio": 1.6181102362204725, "no_speech_prob": 5.173266345082084e-06}, {"id": 93, "seek": 33472, "start": 345.8, "end": 350.08000000000004, "text": " wouldn't be acceptable in the workplace.", "tokens": [2759, 380, 312, 15513, 294, 264, 15328, 13], "temperature": 0.0, "avg_logprob": -0.13577699198306187, "compression_ratio": 1.6181102362204725, "no_speech_prob": 5.173266345082084e-06}, {"id": 94, "seek": 33472, "start": 350.08000000000004, "end": 356.76000000000005, "text": " So fast.ai, we kind of wanted to address this, where our slogan is making neural nets uncool", "tokens": [407, 2370, 13, 1301, 11, 321, 733, 295, 1415, 281, 2985, 341, 11, 689, 527, 33052, 307, 1455, 18161, 36170, 6219, 1092], "temperature": 0.0, "avg_logprob": -0.13577699198306187, "compression_ratio": 1.6181102362204725, "no_speech_prob": 5.173266345082084e-06}, {"id": 95, "seek": 33472, "start": 356.76000000000005, "end": 357.76000000000005, "text": " again.", "tokens": [797, 13], "temperature": 0.0, "avg_logprob": -0.13577699198306187, "compression_ratio": 1.6181102362204725, "no_speech_prob": 5.173266345082084e-06}, {"id": 96, "seek": 33472, "start": 357.76000000000005, "end": 361.88000000000005, "text": " And that's because being cool is about being exclusive, which is the opposite of what we", "tokens": [400, 300, 311, 570, 885, 1627, 307, 466, 885, 13005, 11, 597, 307, 264, 6182, 295, 437, 321], "temperature": 0.0, "avg_logprob": -0.13577699198306187, "compression_ratio": 1.6181102362204725, "no_speech_prob": 5.173266345082084e-06}, {"id": 97, "seek": 33472, "start": 361.88000000000005, "end": 362.88000000000005, "text": " want.", "tokens": [528, 13], "temperature": 0.0, "avg_logprob": -0.13577699198306187, "compression_ratio": 1.6181102362204725, "no_speech_prob": 5.173266345082084e-06}, {"id": 98, "seek": 36288, "start": 362.88, "end": 369.08, "text": " We're particularly interested in people using small or obscure data sets and people that", "tokens": [492, 434, 4098, 3102, 294, 561, 1228, 1359, 420, 34443, 1412, 6352, 293, 561, 300], "temperature": 0.0, "avg_logprob": -0.13595833985701852, "compression_ratio": 1.55893536121673, "no_speech_prob": 1.6962520021479577e-05}, {"id": 99, "seek": 36288, "start": 369.08, "end": 376.4, "text": " don't have many resources and can't afford much computation time or GPU power.", "tokens": [500, 380, 362, 867, 3593, 293, 393, 380, 6157, 709, 24903, 565, 420, 18407, 1347, 13], "temperature": 0.0, "avg_logprob": -0.13595833985701852, "compression_ratio": 1.55893536121673, "no_speech_prob": 1.6962520021479577e-05}, {"id": 100, "seek": 36288, "start": 376.4, "end": 380.64, "text": " So we taught a course in partnership with the University of San Francisco's Data Institute.", "tokens": [407, 321, 5928, 257, 1164, 294, 9982, 365, 264, 3535, 295, 5271, 12279, 311, 11888, 9446, 13], "temperature": 0.0, "avg_logprob": -0.13595833985701852, "compression_ratio": 1.55893536121673, "no_speech_prob": 1.6962520021479577e-05}, {"id": 101, "seek": 36288, "start": 380.64, "end": 385.96, "text": " This was open to the community, and it was one evening a week for seven weeks, although", "tokens": [639, 390, 1269, 281, 264, 1768, 11, 293, 309, 390, 472, 5634, 257, 1243, 337, 3407, 3259, 11, 4878], "temperature": 0.0, "avg_logprob": -0.13595833985701852, "compression_ratio": 1.55893536121673, "no_speech_prob": 1.6962520021479577e-05}, {"id": 102, "seek": 36288, "start": 385.96, "end": 389.6, "text": " many hours of homework outside of class to go along with that.", "tokens": [867, 2496, 295, 14578, 2380, 295, 1508, 281, 352, 2051, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.13595833985701852, "compression_ratio": 1.55893536121673, "no_speech_prob": 1.6962520021479577e-05}, {"id": 103, "seek": 38960, "start": 389.6, "end": 396.72, "text": " And we had this hypothesis, can we teach deep learning to coders with no explicit math prerequisites?", "tokens": [400, 321, 632, 341, 17291, 11, 393, 321, 2924, 2452, 2539, 281, 17656, 433, 365, 572, 13691, 5221, 38333, 15398, 3324, 30], "temperature": 0.0, "avg_logprob": -0.10412159987858363, "compression_ratio": 1.6174242424242424, "no_speech_prob": 8.52904577186564e-06}, {"id": 104, "seek": 38960, "start": 396.72, "end": 400.64000000000004, "text": " And we weren't even sure that it could be done, but it was a big success.", "tokens": [400, 321, 4999, 380, 754, 988, 300, 309, 727, 312, 1096, 11, 457, 309, 390, 257, 955, 2245, 13], "temperature": 0.0, "avg_logprob": -0.10412159987858363, "compression_ratio": 1.6174242424242424, "no_speech_prob": 8.52904577186564e-06}, {"id": 105, "seek": 38960, "start": 400.64000000000004, "end": 403.6, "text": " And we've now released the course online completely for free.", "tokens": [400, 321, 600, 586, 4736, 264, 1164, 2950, 2584, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.10412159987858363, "compression_ratio": 1.6174242424242424, "no_speech_prob": 8.52904577186564e-06}, {"id": 106, "seek": 38960, "start": 403.6, "end": 405.08000000000004, "text": " All the videos are on YouTube.", "tokens": [1057, 264, 2145, 366, 322, 3088, 13], "temperature": 0.0, "avg_logprob": -0.10412159987858363, "compression_ratio": 1.6174242424242424, "no_speech_prob": 8.52904577186564e-06}, {"id": 107, "seek": 38960, "start": 405.08000000000004, "end": 407.44, "text": " The code is on GitHub.", "tokens": [440, 3089, 307, 322, 23331, 13], "temperature": 0.0, "avg_logprob": -0.10412159987858363, "compression_ratio": 1.6174242424242424, "no_speech_prob": 8.52904577186564e-06}, {"id": 108, "seek": 38960, "start": 407.44, "end": 410.40000000000003, "text": " And 50,000 people have started the course.", "tokens": [400, 2625, 11, 1360, 561, 362, 1409, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.10412159987858363, "compression_ratio": 1.6174242424242424, "no_speech_prob": 8.52904577186564e-06}, {"id": 109, "seek": 38960, "start": 410.40000000000003, "end": 414.72, "text": " And some of the success stories we've heard, one of our students who has two years of coding", "tokens": [400, 512, 295, 264, 2245, 3676, 321, 600, 2198, 11, 472, 295, 527, 1731, 567, 575, 732, 924, 295, 17720], "temperature": 0.0, "avg_logprob": -0.10412159987858363, "compression_ratio": 1.6174242424242424, "no_speech_prob": 8.52904577186564e-06}, {"id": 110, "seek": 41472, "start": 414.72, "end": 420.76000000000005, "text": " experience was just accepted to the Google Brain residency program, which is very prestigious.", "tokens": [1752, 390, 445, 9035, 281, 264, 3329, 29783, 34014, 1461, 11, 597, 307, 588, 33510, 13], "temperature": 0.0, "avg_logprob": -0.12914108535618457, "compression_ratio": 1.5971223021582734, "no_speech_prob": 2.8848817237303592e-05}, {"id": 111, "seek": 41472, "start": 420.76000000000005, "end": 424.92, "text": " We have another student who he does not even call himself a data scientist, but he came", "tokens": [492, 362, 1071, 3107, 567, 415, 775, 406, 754, 818, 3647, 257, 1412, 12662, 11, 457, 415, 1361], "temperature": 0.0, "avg_logprob": -0.12914108535618457, "compression_ratio": 1.5971223021582734, "no_speech_prob": 2.8848817237303592e-05}, {"id": 112, "seek": 41472, "start": 424.92, "end": 427.88000000000005, "text": " up with a new fraud detection technique for his company.", "tokens": [493, 365, 257, 777, 14560, 17784, 6532, 337, 702, 2237, 13], "temperature": 0.0, "avg_logprob": -0.12914108535618457, "compression_ratio": 1.5971223021582734, "no_speech_prob": 2.8848817237303592e-05}, {"id": 113, "seek": 41472, "start": 427.88000000000005, "end": 432.36, "text": " And he said it's only like 10 lines of code, but it earned him a large bonus.", "tokens": [400, 415, 848, 309, 311, 787, 411, 1266, 3876, 295, 3089, 11, 457, 309, 12283, 796, 257, 2416, 10882, 13], "temperature": 0.0, "avg_logprob": -0.12914108535618457, "compression_ratio": 1.5971223021582734, "no_speech_prob": 2.8848817237303592e-05}, {"id": 114, "seek": 41472, "start": 432.36, "end": 438.36, "text": " We have several more students that have received job offers or internship offers, that have", "tokens": [492, 362, 2940, 544, 1731, 300, 362, 4613, 1691, 7736, 420, 16861, 7736, 11, 300, 362], "temperature": 0.0, "avg_logprob": -0.12914108535618457, "compression_ratio": 1.5971223021582734, "no_speech_prob": 2.8848817237303592e-05}, {"id": 115, "seek": 41472, "start": 438.36, "end": 440.68, "text": " been able to switch teams at work.", "tokens": [668, 1075, 281, 3679, 5491, 412, 589, 13], "temperature": 0.0, "avg_logprob": -0.12914108535618457, "compression_ratio": 1.5971223021582734, "no_speech_prob": 2.8848817237303592e-05}, {"id": 116, "seek": 44068, "start": 440.68, "end": 445.6, "text": " I know of one student that's contributed back to TensorFlow's code, and then four students", "tokens": [286, 458, 295, 472, 3107, 300, 311, 18434, 646, 281, 37624, 311, 3089, 11, 293, 550, 1451, 1731], "temperature": 0.0, "avg_logprob": -0.1821863060323601, "compression_ratio": 1.6321428571428571, "no_speech_prob": 7.526485205744393e-06}, {"id": 117, "seek": 44068, "start": 445.6, "end": 446.6, "text": " that have won hackathons.", "tokens": [300, 362, 1582, 10339, 998, 892, 13], "temperature": 0.0, "avg_logprob": -0.1821863060323601, "compression_ratio": 1.6321428571428571, "no_speech_prob": 7.526485205744393e-06}, {"id": 118, "seek": 44068, "start": 446.6, "end": 452.8, "text": " I want to introduce you to a few of our students that really, I think, capture kind of our", "tokens": [286, 528, 281, 5366, 291, 281, 257, 1326, 295, 527, 1731, 300, 534, 11, 286, 519, 11, 7983, 733, 295, 527], "temperature": 0.0, "avg_logprob": -0.1821863060323601, "compression_ratio": 1.6321428571428571, "no_speech_prob": 7.526485205744393e-06}, {"id": 119, "seek": 44068, "start": 452.8, "end": 453.8, "text": " mission.", "tokens": [4447, 13], "temperature": 0.0, "avg_logprob": -0.1821863060323601, "compression_ratio": 1.6321428571428571, "no_speech_prob": 7.526485205744393e-06}, {"id": 120, "seek": 44068, "start": 453.8, "end": 456.52, "text": " On the bottom right is Samar Haider.", "tokens": [1282, 264, 2767, 558, 307, 4832, 289, 4064, 1438, 13], "temperature": 0.0, "avg_logprob": -0.1821863060323601, "compression_ratio": 1.6321428571428571, "no_speech_prob": 7.526485205744393e-06}, {"id": 121, "seek": 44068, "start": 456.52, "end": 459.8, "text": " He's a natural language researcher in Pakistan.", "tokens": [634, 311, 257, 3303, 2856, 21751, 294, 15985, 13], "temperature": 0.0, "avg_logprob": -0.1821863060323601, "compression_ratio": 1.6321428571428571, "no_speech_prob": 7.526485205744393e-06}, {"id": 122, "seek": 44068, "start": 459.8, "end": 466.08, "text": " Pakistan has 70 spoken languages, and none of them have that many resources, not even", "tokens": [15985, 575, 5285, 10759, 8650, 11, 293, 6022, 295, 552, 362, 300, 867, 3593, 11, 406, 754], "temperature": 0.0, "avg_logprob": -0.1821863060323601, "compression_ratio": 1.6321428571428571, "no_speech_prob": 7.526485205744393e-06}, {"id": 123, "seek": 44068, "start": 466.08, "end": 470.48, "text": " Urdu, which is the most popular language for translation to and from.", "tokens": [9533, 769, 11, 597, 307, 264, 881, 3743, 2856, 337, 12853, 281, 293, 490, 13], "temperature": 0.0, "avg_logprob": -0.1821863060323601, "compression_ratio": 1.6321428571428571, "no_speech_prob": 7.526485205744393e-06}, {"id": 124, "seek": 47048, "start": 470.48, "end": 472.98, "text": " Or other kind of other text.", "tokens": [1610, 661, 733, 295, 661, 2487, 13], "temperature": 0.0, "avg_logprob": -0.12959155563480598, "compression_ratio": 1.6506849315068493, "no_speech_prob": 1.951038939296268e-05}, {"id": 125, "seek": 47048, "start": 472.98, "end": 476.72, "text": " As part of our course, Samar put together the largest corpus of Urdu that's been assembled,", "tokens": [1018, 644, 295, 527, 1164, 11, 4832, 289, 829, 1214, 264, 6443, 1181, 31624, 295, 9533, 769, 300, 311, 668, 24204, 11], "temperature": 0.0, "avg_logprob": -0.12959155563480598, "compression_ratio": 1.6506849315068493, "no_speech_prob": 1.951038939296268e-05}, {"id": 126, "seek": 47048, "start": 476.72, "end": 481.04, "text": " and he trained word embeddings on that, and has found some really interesting connections", "tokens": [293, 415, 8895, 1349, 12240, 29432, 322, 300, 11, 293, 575, 1352, 512, 534, 1880, 9271], "temperature": 0.0, "avg_logprob": -0.12959155563480598, "compression_ratio": 1.6506849315068493, "no_speech_prob": 1.951038939296268e-05}, {"id": 127, "seek": 47048, "start": 481.04, "end": 482.88, "text": " in the language.", "tokens": [294, 264, 2856, 13], "temperature": 0.0, "avg_logprob": -0.12959155563480598, "compression_ratio": 1.6506849315068493, "no_speech_prob": 1.951038939296268e-05}, {"id": 128, "seek": 47048, "start": 482.88, "end": 488.96000000000004, "text": " And he's largely, for machine learning, he's largely self-taught using online courses.", "tokens": [400, 415, 311, 11611, 11, 337, 3479, 2539, 11, 415, 311, 11611, 2698, 12, 1328, 1599, 1228, 2950, 7712, 13], "temperature": 0.0, "avg_logprob": -0.12959155563480598, "compression_ratio": 1.6506849315068493, "no_speech_prob": 1.951038939296268e-05}, {"id": 129, "seek": 47048, "start": 488.96000000000004, "end": 494.02000000000004, "text": " Above him are some pictures sent to me by Sahil Singla, a farm guide in India.", "tokens": [32691, 796, 366, 512, 5242, 2279, 281, 385, 538, 18280, 388, 7474, 875, 11, 257, 5421, 5934, 294, 5282, 13], "temperature": 0.0, "avg_logprob": -0.12959155563480598, "compression_ratio": 1.6506849315068493, "no_speech_prob": 1.951038939296268e-05}, {"id": 130, "seek": 47048, "start": 494.02000000000004, "end": 499.34000000000003, "text": " So thousands of farmers commit suicide every year in India, because they're taking these", "tokens": [407, 5383, 295, 11339, 5599, 12308, 633, 1064, 294, 5282, 11, 570, 436, 434, 1940, 613], "temperature": 0.0, "avg_logprob": -0.12959155563480598, "compression_ratio": 1.6506849315068493, "no_speech_prob": 1.951038939296268e-05}, {"id": 131, "seek": 49934, "start": 499.34, "end": 503.28, "text": " very predatory loans from loan sharks who then kind of threaten them with violence and", "tokens": [588, 3852, 4745, 15443, 490, 10529, 26312, 567, 550, 733, 295, 29864, 552, 365, 6270, 293], "temperature": 0.0, "avg_logprob": -0.09425960968588, "compression_ratio": 1.651851851851852, "no_speech_prob": 4.464378071133979e-05}, {"id": 132, "seek": 49934, "start": 503.28, "end": 504.28, "text": " harass their families.", "tokens": [16910, 641, 4466, 13], "temperature": 0.0, "avg_logprob": -0.09425960968588, "compression_ratio": 1.651851851851852, "no_speech_prob": 4.464378071133979e-05}, {"id": 133, "seek": 49934, "start": 504.28, "end": 508.67999999999995, "text": " And part of the problem is they can't prove how much land they own or what type of crops", "tokens": [400, 644, 295, 264, 1154, 307, 436, 393, 380, 7081, 577, 709, 2117, 436, 1065, 420, 437, 2010, 295, 16829], "temperature": 0.0, "avg_logprob": -0.09425960968588, "compression_ratio": 1.651851851851852, "no_speech_prob": 4.464378071133979e-05}, {"id": 134, "seek": 49934, "start": 508.67999999999995, "end": 509.96, "text": " they're growing.", "tokens": [436, 434, 4194, 13], "temperature": 0.0, "avg_logprob": -0.09425960968588, "compression_ratio": 1.651851851851852, "no_speech_prob": 4.464378071133979e-05}, {"id": 135, "seek": 49934, "start": 509.96, "end": 516.0, "text": " And so Sahil is scraping data from Google Earth and then applying CNNs to identify their", "tokens": [400, 370, 18280, 388, 307, 43738, 1412, 490, 3329, 4755, 293, 550, 9275, 24859, 82, 281, 5876, 641], "temperature": 0.0, "avg_logprob": -0.09425960968588, "compression_ratio": 1.651851851851852, "no_speech_prob": 4.464378071133979e-05}, {"id": 136, "seek": 49934, "start": 516.0, "end": 520.78, "text": " plots and use this so that they can qualify for better loans.", "tokens": [28609, 293, 764, 341, 370, 300, 436, 393, 20276, 337, 1101, 15443, 13], "temperature": 0.0, "avg_logprob": -0.09425960968588, "compression_ratio": 1.651851851851852, "no_speech_prob": 4.464378071133979e-05}, {"id": 137, "seek": 49934, "start": 520.78, "end": 526.04, "text": " Bottom left is Sarah Hooker and a team of data scientists from Delta Analytics.", "tokens": [38289, 1411, 307, 9519, 33132, 260, 293, 257, 1469, 295, 1412, 7708, 490, 18183, 25944, 13], "temperature": 0.0, "avg_logprob": -0.09425960968588, "compression_ratio": 1.651851851851852, "no_speech_prob": 4.464378071133979e-05}, {"id": 138, "seek": 52604, "start": 526.04, "end": 531.8399999999999, "text": " Delta Analytics partners nonprofits with teams of data scientists who volunteer to help work", "tokens": [18183, 25944, 4462, 42851, 365, 5491, 295, 1412, 7708, 567, 13835, 281, 854, 589], "temperature": 0.0, "avg_logprob": -0.11935159895155165, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.5556418374180794e-05}, {"id": 139, "seek": 52604, "start": 531.8399999999999, "end": 533.3199999999999, "text": " with them.", "tokens": [365, 552, 13], "temperature": 0.0, "avg_logprob": -0.11935159895155165, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.5556418374180794e-05}, {"id": 140, "seek": 52604, "start": 533.3199999999999, "end": 538.5999999999999, "text": " And Rainforest Connection is a startup that is putting recycled cell phones in endangered", "tokens": [400, 14487, 36629, 11653, 313, 307, 257, 18578, 300, 307, 3372, 30674, 2815, 10216, 294, 37539], "temperature": 0.0, "avg_logprob": -0.11935159895155165, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.5556418374180794e-05}, {"id": 141, "seek": 52604, "start": 538.5999999999999, "end": 545.36, "text": " rainforest and then streaming audio to identify chainsaw noises and send out an alarm when", "tokens": [48531, 293, 550, 11791, 6278, 281, 5876, 12626, 1607, 14620, 293, 2845, 484, 364, 14183, 562], "temperature": 0.0, "avg_logprob": -0.11935159895155165, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.5556418374180794e-05}, {"id": 142, "seek": 52604, "start": 545.36, "end": 547.76, "text": " there's someone cutting down the forest.", "tokens": [456, 311, 1580, 6492, 760, 264, 6719, 13], "temperature": 0.0, "avg_logprob": -0.11935159895155165, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.5556418374180794e-05}, {"id": 143, "seek": 52604, "start": 547.76, "end": 550.3199999999999, "text": " And so this is a team of data scientists.", "tokens": [400, 370, 341, 307, 257, 1469, 295, 1412, 7708, 13], "temperature": 0.0, "avg_logprob": -0.11935159895155165, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.5556418374180794e-05}, {"id": 144, "seek": 52604, "start": 550.3199999999999, "end": 555.36, "text": " They were running some tests in Berkeley, but they're using deep learning to identify", "tokens": [814, 645, 2614, 512, 6921, 294, 23684, 11, 457, 436, 434, 1228, 2452, 2539, 281, 5876], "temperature": 0.0, "avg_logprob": -0.11935159895155165, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.5556418374180794e-05}, {"id": 145, "seek": 55536, "start": 555.36, "end": 556.84, "text": " chainsaws.", "tokens": [12626, 12282, 13], "temperature": 0.0, "avg_logprob": -0.08366954660861292, "compression_ratio": 1.5842293906810037, "no_speech_prob": 8.798736416792963e-06}, {"id": 146, "seek": 55536, "start": 556.84, "end": 561.12, "text": " And then above that, top left is a picture of Tosin Maisha.", "tokens": [400, 550, 3673, 300, 11, 1192, 1411, 307, 257, 3036, 295, 314, 329, 259, 4042, 16546, 13], "temperature": 0.0, "avg_logprob": -0.08366954660861292, "compression_ratio": 1.5842293906810037, "no_speech_prob": 8.798736416792963e-06}, {"id": 147, "seek": 55536, "start": 561.12, "end": 562.92, "text": " She's a student in Bangladesh.", "tokens": [1240, 311, 257, 3107, 294, 35260, 13], "temperature": 0.0, "avg_logprob": -0.08366954660861292, "compression_ratio": 1.5842293906810037, "no_speech_prob": 8.798736416792963e-06}, {"id": 148, "seek": 55536, "start": 562.92, "end": 568.24, "text": " She's done a project analyzing how the major Bangladeshi newspaper covers violence against", "tokens": [1240, 311, 1096, 257, 1716, 23663, 577, 264, 2563, 32123, 2977, 4954, 13669, 10538, 6270, 1970], "temperature": 0.0, "avg_logprob": -0.08366954660861292, "compression_ratio": 1.5842293906810037, "no_speech_prob": 8.798736416792963e-06}, {"id": 149, "seek": 55536, "start": 568.24, "end": 573.36, "text": " women by doing a kind of visualization of having scraped the text.", "tokens": [2266, 538, 884, 257, 733, 295, 25801, 295, 1419, 13943, 3452, 264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.08366954660861292, "compression_ratio": 1.5842293906810037, "no_speech_prob": 8.798736416792963e-06}, {"id": 150, "seek": 55536, "start": 573.36, "end": 578.12, "text": " And she's blogged about the challenges of doing machine learning with intermittent electricity.", "tokens": [400, 750, 311, 6968, 3004, 466, 264, 4759, 295, 884, 3479, 2539, 365, 44084, 10356, 13], "temperature": 0.0, "avg_logprob": -0.08366954660861292, "compression_ratio": 1.5842293906810037, "no_speech_prob": 8.798736416792963e-06}, {"id": 151, "seek": 55536, "start": 578.12, "end": 582.72, "text": " She currently has some free AWS credits that she was gifted, but she doesn't know what", "tokens": [1240, 4362, 575, 512, 1737, 17650, 16816, 300, 750, 390, 27104, 11, 457, 750, 1177, 380, 458, 437], "temperature": 0.0, "avg_logprob": -0.08366954660861292, "compression_ratio": 1.5842293906810037, "no_speech_prob": 8.798736416792963e-06}, {"id": 152, "seek": 58272, "start": 582.72, "end": 586.36, "text": " she's going to do when they run out because the expense is going to be a lot.", "tokens": [750, 311, 516, 281, 360, 562, 436, 1190, 484, 570, 264, 18406, 307, 516, 281, 312, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.1251131140667459, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.936588528740685e-06}, {"id": 153, "seek": 58272, "start": 586.36, "end": 590.0, "text": " And so these are the type of problems and the type of students that we are really interested", "tokens": [400, 370, 613, 366, 264, 2010, 295, 2740, 293, 264, 2010, 295, 1731, 300, 321, 366, 534, 3102], "temperature": 0.0, "avg_logprob": -0.1251131140667459, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.936588528740685e-06}, {"id": 154, "seek": 58272, "start": 590.0, "end": 594.88, "text": " in and kind of thinking about people with few resources who have projects that they're", "tokens": [294, 293, 733, 295, 1953, 466, 561, 365, 1326, 3593, 567, 362, 4455, 300, 436, 434], "temperature": 0.0, "avg_logprob": -0.1251131140667459, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.936588528740685e-06}, {"id": 155, "seek": 58272, "start": 594.88, "end": 598.4, "text": " really, really interested in.", "tokens": [534, 11, 534, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.1251131140667459, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.936588528740685e-06}, {"id": 156, "seek": 58272, "start": 598.4, "end": 600.48, "text": " So most math classes are bottom up.", "tokens": [407, 881, 5221, 5359, 366, 2767, 493, 13], "temperature": 0.0, "avg_logprob": -0.1251131140667459, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.936588528740685e-06}, {"id": 157, "seek": 58272, "start": 600.48, "end": 604.24, "text": " You learn each building block you need, and then eventually you can put them together.", "tokens": [509, 1466, 1184, 2390, 3461, 291, 643, 11, 293, 550, 4728, 291, 393, 829, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1251131140667459, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.936588528740685e-06}, {"id": 158, "seek": 58272, "start": 604.24, "end": 608.96, "text": " And it's hard for a lot of students to maintain motivation with that.", "tokens": [400, 309, 311, 1152, 337, 257, 688, 295, 1731, 281, 6909, 12335, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.1251131140667459, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.936588528740685e-06}, {"id": 159, "seek": 60896, "start": 608.96, "end": 612.8000000000001, "text": " It's also hard to know what the big picture is or where you're going.", "tokens": [467, 311, 611, 1152, 281, 458, 437, 264, 955, 3036, 307, 420, 689, 291, 434, 516, 13], "temperature": 0.0, "avg_logprob": -0.09159829799945537, "compression_ratio": 1.7227722772277227, "no_speech_prob": 4.156908744334942e-06}, {"id": 160, "seek": 60896, "start": 612.8000000000001, "end": 614.72, "text": " And it's hard to know which pieces you'll actually need.", "tokens": [400, 309, 311, 1152, 281, 458, 597, 3755, 291, 603, 767, 643, 13], "temperature": 0.0, "avg_logprob": -0.09159829799945537, "compression_ratio": 1.7227722772277227, "no_speech_prob": 4.156908744334942e-06}, {"id": 161, "seek": 60896, "start": 614.72, "end": 617.9200000000001, "text": " So I sometimes talk to people that are kind of studying theory that they think they're", "tokens": [407, 286, 2171, 751, 281, 561, 300, 366, 733, 295, 7601, 5261, 300, 436, 519, 436, 434], "temperature": 0.0, "avg_logprob": -0.09159829799945537, "compression_ratio": 1.7227722772277227, "no_speech_prob": 4.156908744334942e-06}, {"id": 162, "seek": 60896, "start": 617.9200000000001, "end": 622.32, "text": " going to need for deep learning but don't actually end up needing.", "tokens": [516, 281, 643, 337, 2452, 2539, 457, 500, 380, 767, 917, 493, 18006, 13], "temperature": 0.0, "avg_logprob": -0.09159829799945537, "compression_ratio": 1.7227722772277227, "no_speech_prob": 4.156908744334942e-06}, {"id": 163, "seek": 60896, "start": 622.32, "end": 626.48, "text": " So we try to get students using a neural net right away and six lines of code.", "tokens": [407, 321, 853, 281, 483, 1731, 1228, 257, 18161, 2533, 558, 1314, 293, 2309, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.09159829799945537, "compression_ratio": 1.7227722772277227, "no_speech_prob": 4.156908744334942e-06}, {"id": 164, "seek": 60896, "start": 626.48, "end": 631.76, "text": " We spend a lot of time refactoring our code to help make it concise and modular.", "tokens": [492, 3496, 257, 688, 295, 565, 1895, 578, 3662, 527, 3089, 281, 854, 652, 309, 44882, 293, 31111, 13], "temperature": 0.0, "avg_logprob": -0.09159829799945537, "compression_ratio": 1.7227722772277227, "no_speech_prob": 4.156908744334942e-06}, {"id": 165, "seek": 60896, "start": 631.76, "end": 636.36, "text": " We give students an Amazon machine image that's set up with everything they need.", "tokens": [492, 976, 1731, 364, 6795, 3479, 3256, 300, 311, 992, 493, 365, 1203, 436, 643, 13], "temperature": 0.0, "avg_logprob": -0.09159829799945537, "compression_ratio": 1.7227722772277227, "no_speech_prob": 4.156908744334942e-06}, {"id": 166, "seek": 63636, "start": 636.36, "end": 640.16, "text": " And then over time we do gradually peel back the layers and get to lower level stuff.", "tokens": [400, 550, 670, 565, 321, 360, 13145, 13889, 646, 264, 7914, 293, 483, 281, 3126, 1496, 1507, 13], "temperature": 0.0, "avg_logprob": -0.08971235177812785, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9019809062447166e-06}, {"id": 167, "seek": 63636, "start": 640.16, "end": 644.6, "text": " And it's always motivated though by trying to improve our performance or to kind of tackle", "tokens": [400, 309, 311, 1009, 14515, 1673, 538, 1382, 281, 3470, 527, 3389, 420, 281, 733, 295, 14896], "temperature": 0.0, "avg_logprob": -0.08971235177812785, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9019809062447166e-06}, {"id": 168, "seek": 63636, "start": 644.6, "end": 645.6, "text": " a harder problem.", "tokens": [257, 6081, 1154, 13], "temperature": 0.0, "avg_logprob": -0.08971235177812785, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9019809062447166e-06}, {"id": 169, "seek": 63636, "start": 645.6, "end": 648.36, "text": " So eventually they are understanding the low level components.", "tokens": [407, 4728, 436, 366, 3701, 264, 2295, 1496, 6677, 13], "temperature": 0.0, "avg_logprob": -0.08971235177812785, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9019809062447166e-06}, {"id": 170, "seek": 63636, "start": 648.36, "end": 651.08, "text": " We've just kind of gone in the reverse order.", "tokens": [492, 600, 445, 733, 295, 2780, 294, 264, 9943, 1668, 13], "temperature": 0.0, "avg_logprob": -0.08971235177812785, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9019809062447166e-06}, {"id": 171, "seek": 63636, "start": 651.08, "end": 654.2, "text": " And then we are huge fans of transfer learning.", "tokens": [400, 550, 321, 366, 2603, 4499, 295, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.08971235177812785, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9019809062447166e-06}, {"id": 172, "seek": 63636, "start": 654.2, "end": 657.48, "text": " Transfer learning is the technique where you take a pre-trained network that was trained", "tokens": [35025, 2539, 307, 264, 6532, 689, 291, 747, 257, 659, 12, 17227, 2001, 3209, 300, 390, 8895], "temperature": 0.0, "avg_logprob": -0.08971235177812785, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9019809062447166e-06}, {"id": 173, "seek": 63636, "start": 657.48, "end": 661.24, "text": " on a different data set and a different problem and apply it to your problem.", "tokens": [322, 257, 819, 1412, 992, 293, 257, 819, 1154, 293, 3079, 309, 281, 428, 1154, 13], "temperature": 0.0, "avg_logprob": -0.08971235177812785, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9019809062447166e-06}, {"id": 174, "seek": 63636, "start": 661.24, "end": 664.8000000000001, "text": " And often you retrain the last layer or the last few layers.", "tokens": [400, 2049, 291, 1533, 7146, 264, 1036, 4583, 420, 264, 1036, 1326, 7914, 13], "temperature": 0.0, "avg_logprob": -0.08971235177812785, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.9019809062447166e-06}, {"id": 175, "seek": 66480, "start": 664.8, "end": 668.4399999999999, "text": " And it's really powerful because it lets people with smaller data sets and limited computational", "tokens": [400, 309, 311, 534, 4005, 570, 309, 6653, 561, 365, 4356, 1412, 6352, 293, 5567, 28270], "temperature": 0.0, "avg_logprob": -0.12920563437721946, "compression_ratio": 1.5308219178082192, "no_speech_prob": 5.173186764295679e-06}, {"id": 176, "seek": 66480, "start": 668.4399999999999, "end": 673.9599999999999, "text": " power kind of take advantage of these networks that have been trained elsewhere.", "tokens": [1347, 733, 295, 747, 5002, 295, 613, 9590, 300, 362, 668, 8895, 14517, 13], "temperature": 0.0, "avg_logprob": -0.12920563437721946, "compression_ratio": 1.5308219178082192, "no_speech_prob": 5.173186764295679e-06}, {"id": 177, "seek": 66480, "start": 673.9599999999999, "end": 675.56, "text": " So this is from lesson one.", "tokens": [407, 341, 307, 490, 6898, 472, 13], "temperature": 0.0, "avg_logprob": -0.12920563437721946, "compression_ratio": 1.5308219178082192, "no_speech_prob": 5.173186764295679e-06}, {"id": 178, "seek": 66480, "start": 675.56, "end": 681.8399999999999, "text": " And it's six lines of code and it gets you better than 97% accuracy on dogs versus cats,", "tokens": [400, 309, 311, 2309, 3876, 295, 3089, 293, 309, 2170, 291, 1101, 813, 23399, 4, 14170, 322, 7197, 5717, 11111, 11], "temperature": 0.0, "avg_logprob": -0.12920563437721946, "compression_ratio": 1.5308219178082192, "no_speech_prob": 5.173186764295679e-06}, {"id": 179, "seek": 66480, "start": 681.8399999999999, "end": 686.4799999999999, "text": " which has been a Kaggle competition twice now.", "tokens": [597, 575, 668, 257, 48751, 22631, 6211, 6091, 586, 13], "temperature": 0.0, "avg_logprob": -0.12920563437721946, "compression_ratio": 1.5308219178082192, "no_speech_prob": 5.173186764295679e-06}, {"id": 180, "seek": 66480, "start": 686.4799999999999, "end": 691.68, "text": " And here we're using VGG, which was the ImageNet winner in 2014, and then just retraining the", "tokens": [400, 510, 321, 434, 1228, 691, 27561, 11, 597, 390, 264, 29903, 31890, 8507, 294, 8227, 11, 293, 550, 445, 49356, 1760, 264], "temperature": 0.0, "avg_logprob": -0.12920563437721946, "compression_ratio": 1.5308219178082192, "no_speech_prob": 5.173186764295679e-06}, {"id": 181, "seek": 66480, "start": 691.68, "end": 692.68, "text": " last layer.", "tokens": [1036, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12920563437721946, "compression_ratio": 1.5308219178082192, "no_speech_prob": 5.173186764295679e-06}, {"id": 182, "seek": 69268, "start": 692.68, "end": 696.12, "text": " Okay, so what does all this mean for you?", "tokens": [1033, 11, 370, 437, 775, 439, 341, 914, 337, 291, 30], "temperature": 0.0, "avg_logprob": -0.114947858064071, "compression_ratio": 1.6028368794326242, "no_speech_prob": 3.5556338389142184e-06}, {"id": 183, "seek": 69268, "start": 696.12, "end": 699.8399999999999, "text": " What kind of lessons can you take away?", "tokens": [708, 733, 295, 8820, 393, 291, 747, 1314, 30], "temperature": 0.0, "avg_logprob": -0.114947858064071, "compression_ratio": 1.6028368794326242, "no_speech_prob": 3.5556338389142184e-06}, {"id": 184, "seek": 69268, "start": 699.8399999999999, "end": 705.9599999999999, "text": " So my very opinionated recommendation is to start with Keras.", "tokens": [407, 452, 588, 4800, 770, 11879, 307, 281, 722, 365, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.114947858064071, "compression_ratio": 1.6028368794326242, "no_speech_prob": 3.5556338389142184e-06}, {"id": 185, "seek": 69268, "start": 705.9599999999999, "end": 710.52, "text": " Keras is a Python library that sits on top of either Theano or TensorFlow.", "tokens": [591, 6985, 307, 257, 15329, 6405, 300, 12696, 322, 1192, 295, 2139, 440, 3730, 420, 37624, 13], "temperature": 0.0, "avg_logprob": -0.114947858064071, "compression_ratio": 1.6028368794326242, "no_speech_prob": 3.5556338389142184e-06}, {"id": 186, "seek": 69268, "start": 710.52, "end": 714.1999999999999, "text": " And the abstractions in Keras are just so well mapped to the abstractions of neural", "tokens": [400, 264, 12649, 626, 294, 591, 6985, 366, 445, 370, 731, 33318, 281, 264, 12649, 626, 295, 18161], "temperature": 0.0, "avg_logprob": -0.114947858064071, "compression_ratio": 1.6028368794326242, "no_speech_prob": 3.5556338389142184e-06}, {"id": 187, "seek": 69268, "start": 714.1999999999999, "end": 718.0, "text": " networks that I think it makes it a lot easier to learn both.", "tokens": [9590, 300, 286, 519, 309, 1669, 309, 257, 688, 3571, 281, 1466, 1293, 13], "temperature": 0.0, "avg_logprob": -0.114947858064071, "compression_ratio": 1.6028368794326242, "no_speech_prob": 3.5556338389142184e-06}, {"id": 188, "seek": 69268, "start": 718.0, "end": 722.5999999999999, "text": " I wrote a blog post several months ago where I said using TensorFlow makes me feel like", "tokens": [286, 4114, 257, 6968, 2183, 2940, 2493, 2057, 689, 286, 848, 1228, 37624, 1669, 385, 841, 411], "temperature": 0.0, "avg_logprob": -0.114947858064071, "compression_ratio": 1.6028368794326242, "no_speech_prob": 3.5556338389142184e-06}, {"id": 189, "seek": 72260, "start": 722.6, "end": 727.2, "text": " I'm not smart enough to use TensorFlow, whereas using Keras makes me feel like neural networks", "tokens": [286, 478, 406, 4069, 1547, 281, 764, 37624, 11, 9735, 1228, 591, 6985, 1669, 385, 841, 411, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.092683995953043, "compression_ratio": 1.6, "no_speech_prob": 1.129914653574815e-05}, {"id": 190, "seek": 72260, "start": 727.2, "end": 729.28, "text": " are easier than I realized.", "tokens": [366, 3571, 813, 286, 5334, 13], "temperature": 0.0, "avg_logprob": -0.092683995953043, "compression_ratio": 1.6, "no_speech_prob": 1.129914653574815e-05}, {"id": 191, "seek": 72260, "start": 729.28, "end": 732.6800000000001, "text": " This was a very popular post and a lot of people said they agreed.", "tokens": [639, 390, 257, 588, 3743, 2183, 293, 257, 688, 295, 561, 848, 436, 9166, 13], "temperature": 0.0, "avg_logprob": -0.092683995953043, "compression_ratio": 1.6, "no_speech_prob": 1.129914653574815e-05}, {"id": 192, "seek": 72260, "start": 732.6800000000001, "end": 736.12, "text": " We are using, so we used Theano and Keras in part one of the course.", "tokens": [492, 366, 1228, 11, 370, 321, 1143, 440, 3730, 293, 591, 6985, 294, 644, 472, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.092683995953043, "compression_ratio": 1.6, "no_speech_prob": 1.129914653574815e-05}, {"id": 193, "seek": 72260, "start": 736.12, "end": 741.0600000000001, "text": " We do use TensorFlow and PyTorch, which is also excellent in part two, but I still really", "tokens": [492, 360, 764, 37624, 293, 9953, 51, 284, 339, 11, 597, 307, 611, 7103, 294, 644, 732, 11, 457, 286, 920, 534], "temperature": 0.0, "avg_logprob": -0.092683995953043, "compression_ratio": 1.6, "no_speech_prob": 1.129914653574815e-05}, {"id": 194, "seek": 72260, "start": 741.0600000000001, "end": 743.08, "text": " recommend starting with Keras.", "tokens": [2748, 2891, 365, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.092683995953043, "compression_ratio": 1.6, "no_speech_prob": 1.129914653574815e-05}, {"id": 195, "seek": 72260, "start": 743.08, "end": 747.38, "text": " And I should say I have not tried MXNet yet, and so I'm looking forward to hearing the", "tokens": [400, 286, 820, 584, 286, 362, 406, 3031, 47509, 31890, 1939, 11, 293, 370, 286, 478, 1237, 2128, 281, 4763, 264], "temperature": 0.0, "avg_logprob": -0.092683995953043, "compression_ratio": 1.6, "no_speech_prob": 1.129914653574815e-05}, {"id": 196, "seek": 72260, "start": 747.38, "end": 751.0400000000001, "text": " next talk about that.", "tokens": [958, 751, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.092683995953043, "compression_ratio": 1.6, "no_speech_prob": 1.129914653574815e-05}, {"id": 197, "seek": 75104, "start": 751.04, "end": 757.0, "text": " Without read or watch tutorials, without taking time to code, it's by coding that you learn.", "tokens": [9129, 1401, 420, 1159, 17616, 11, 1553, 1940, 565, 281, 3089, 11, 309, 311, 538, 17720, 300, 291, 1466, 13], "temperature": 0.0, "avg_logprob": -0.10676584869134621, "compression_ratio": 1.7750865051903115, "no_speech_prob": 0.00019700429402291775}, {"id": 198, "seek": 75104, "start": 757.0, "end": 760.64, "text": " And then hitting Shift Enter to go through somebody else's Jupyter notebook does not", "tokens": [400, 550, 8850, 28304, 10399, 281, 352, 807, 2618, 1646, 311, 22125, 88, 391, 21060, 775, 406], "temperature": 0.0, "avg_logprob": -0.10676584869134621, "compression_ratio": 1.7750865051903115, "no_speech_prob": 0.00019700429402291775}, {"id": 199, "seek": 75104, "start": 760.64, "end": 762.0999999999999, "text": " count as coding.", "tokens": [1207, 382, 17720, 13], "temperature": 0.0, "avg_logprob": -0.10676584869134621, "compression_ratio": 1.7750865051903115, "no_speech_prob": 0.00019700429402291775}, {"id": 200, "seek": 75104, "start": 762.0999999999999, "end": 764.16, "text": " You really need to type it out yourself.", "tokens": [509, 534, 643, 281, 2010, 309, 484, 1803, 13], "temperature": 0.0, "avg_logprob": -0.10676584869134621, "compression_ratio": 1.7750865051903115, "no_speech_prob": 0.00019700429402291775}, {"id": 201, "seek": 75104, "start": 764.16, "end": 769.0, "text": " You need to modify the inputs and see how that impacts the outputs.", "tokens": [509, 643, 281, 16927, 264, 15743, 293, 536, 577, 300, 11606, 264, 23930, 13], "temperature": 0.0, "avg_logprob": -0.10676584869134621, "compression_ratio": 1.7750865051903115, "no_speech_prob": 0.00019700429402291775}, {"id": 202, "seek": 75104, "start": 769.0, "end": 773.36, "text": " And then have a programming project that you're interested in and let that drive your learning.", "tokens": [400, 550, 362, 257, 9410, 1716, 300, 291, 434, 3102, 294, 293, 718, 300, 3332, 428, 2539, 13], "temperature": 0.0, "avg_logprob": -0.10676584869134621, "compression_ratio": 1.7750865051903115, "no_speech_prob": 0.00019700429402291775}, {"id": 203, "seek": 75104, "start": 773.36, "end": 776.52, "text": " Don't feel like you need to spend several months learning before you can tackle the", "tokens": [1468, 380, 841, 411, 291, 643, 281, 3496, 2940, 2493, 2539, 949, 291, 393, 14896, 264], "temperature": 0.0, "avg_logprob": -0.10676584869134621, "compression_ratio": 1.7750865051903115, "no_speech_prob": 0.00019700429402291775}, {"id": 204, "seek": 75104, "start": 776.52, "end": 778.0799999999999, "text": " project you're interested in.", "tokens": [1716, 291, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.10676584869134621, "compression_ratio": 1.7750865051903115, "no_speech_prob": 0.00019700429402291775}, {"id": 205, "seek": 77808, "start": 778.08, "end": 781.96, "text": " Kind of start right away, and then that will keep you always learning the most relevant", "tokens": [9242, 295, 722, 558, 1314, 11, 293, 550, 300, 486, 1066, 291, 1009, 2539, 264, 881, 7340], "temperature": 0.0, "avg_logprob": -0.12350065877118449, "compression_ratio": 1.697674418604651, "no_speech_prob": 2.0902930373267736e-06}, {"id": 206, "seek": 77808, "start": 781.96, "end": 786.32, "text": " things you need to tackle the problem you care about.", "tokens": [721, 291, 643, 281, 14896, 264, 1154, 291, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.12350065877118449, "compression_ratio": 1.697674418604651, "no_speech_prob": 2.0902930373267736e-06}, {"id": 207, "seek": 77808, "start": 786.32, "end": 790.84, "text": " And then finally, a lot of them, a lot of programmers are very snobby about Excel, but", "tokens": [400, 550, 2721, 11, 257, 688, 295, 552, 11, 257, 688, 295, 41504, 366, 588, 2406, 996, 2322, 466, 19060, 11, 457], "temperature": 0.0, "avg_logprob": -0.12350065877118449, "compression_ratio": 1.697674418604651, "no_speech_prob": 2.0902930373267736e-06}, {"id": 208, "seek": 77808, "start": 790.84, "end": 793.44, "text": " it's actually a very useful visual tool.", "tokens": [309, 311, 767, 257, 588, 4420, 5056, 2290, 13], "temperature": 0.0, "avg_logprob": -0.12350065877118449, "compression_ratio": 1.697674418604651, "no_speech_prob": 2.0902930373267736e-06}, {"id": 209, "seek": 77808, "start": 793.44, "end": 797.08, "text": " And so Jeremy spent a lot of time thinking about how to implement stochastic gradient", "tokens": [400, 370, 17809, 4418, 257, 688, 295, 565, 1953, 466, 577, 281, 4445, 342, 8997, 2750, 16235], "temperature": 0.0, "avg_logprob": -0.12350065877118449, "compression_ratio": 1.697674418604651, "no_speech_prob": 2.0902930373267736e-06}, {"id": 210, "seek": 77808, "start": 797.08, "end": 801.72, "text": " descent as well as other optimizers like RMSProp and AdaGrad in Excel.", "tokens": [23475, 382, 731, 382, 661, 5028, 22525, 411, 497, 10288, 47, 1513, 293, 32276, 38, 6206, 294, 19060, 13], "temperature": 0.0, "avg_logprob": -0.12350065877118449, "compression_ratio": 1.697674418604651, "no_speech_prob": 2.0902930373267736e-06}, {"id": 211, "seek": 77808, "start": 801.72, "end": 805.96, "text": " And it provides a really visual picture of which cells depend on which cells and how", "tokens": [400, 309, 6417, 257, 534, 5056, 3036, 295, 597, 5438, 5672, 322, 597, 5438, 293, 577], "temperature": 0.0, "avg_logprob": -0.12350065877118449, "compression_ratio": 1.697674418604651, "no_speech_prob": 2.0902930373267736e-06}, {"id": 212, "seek": 80596, "start": 805.96, "end": 808.24, "text": " the calculations are updating.", "tokens": [264, 20448, 366, 25113, 13], "temperature": 0.0, "avg_logprob": -0.11182980459244525, "compression_ratio": 1.7003610108303249, "no_speech_prob": 3.0413812055485323e-06}, {"id": 213, "seek": 80596, "start": 808.24, "end": 810.48, "text": " So I encourage you to check out those notebooks.", "tokens": [407, 286, 5373, 291, 281, 1520, 484, 729, 43782, 13], "temperature": 0.0, "avg_logprob": -0.11182980459244525, "compression_ratio": 1.7003610108303249, "no_speech_prob": 3.0413812055485323e-06}, {"id": 214, "seek": 80596, "start": 810.48, "end": 814.76, "text": " And also if you're ever stumped on a math concept, try implementing it in Excel.", "tokens": [400, 611, 498, 291, 434, 1562, 43164, 292, 322, 257, 5221, 3410, 11, 853, 18114, 309, 294, 19060, 13], "temperature": 0.0, "avg_logprob": -0.11182980459244525, "compression_ratio": 1.7003610108303249, "no_speech_prob": 3.0413812055485323e-06}, {"id": 215, "seek": 80596, "start": 814.76, "end": 819.5600000000001, "text": " And then once you have it down, put it in Python.", "tokens": [400, 550, 1564, 291, 362, 309, 760, 11, 829, 309, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.11182980459244525, "compression_ratio": 1.7003610108303249, "no_speech_prob": 3.0413812055485323e-06}, {"id": 216, "seek": 80596, "start": 819.5600000000001, "end": 822.72, "text": " So there are two tests of whether you understand something.", "tokens": [407, 456, 366, 732, 6921, 295, 1968, 291, 1223, 746, 13], "temperature": 0.0, "avg_logprob": -0.11182980459244525, "compression_ratio": 1.7003610108303249, "no_speech_prob": 3.0413812055485323e-06}, {"id": 217, "seek": 80596, "start": 822.72, "end": 825.72, "text": " And one is whether you can code it and build something with it.", "tokens": [400, 472, 307, 1968, 291, 393, 3089, 309, 293, 1322, 746, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.11182980459244525, "compression_ratio": 1.7003610108303249, "no_speech_prob": 3.0413812055485323e-06}, {"id": 218, "seek": 80596, "start": 825.72, "end": 828.62, "text": " And the other is can you explain it to somebody else?", "tokens": [400, 264, 661, 307, 393, 291, 2903, 309, 281, 2618, 1646, 30], "temperature": 0.0, "avg_logprob": -0.11182980459244525, "compression_ratio": 1.7003610108303249, "no_speech_prob": 3.0413812055485323e-06}, {"id": 219, "seek": 80596, "start": 828.62, "end": 831.72, "text": " And ways to practice explaining what you're learning.", "tokens": [400, 2098, 281, 3124, 13468, 437, 291, 434, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11182980459244525, "compression_ratio": 1.7003610108303249, "no_speech_prob": 3.0413812055485323e-06}, {"id": 220, "seek": 80596, "start": 831.72, "end": 833.32, "text": " One is to write a blog post.", "tokens": [1485, 307, 281, 2464, 257, 6968, 2183, 13], "temperature": 0.0, "avg_logprob": -0.11182980459244525, "compression_ratio": 1.7003610108303249, "no_speech_prob": 3.0413812055485323e-06}, {"id": 221, "seek": 83332, "start": 833.32, "end": 837.88, "text": " There are two blog posts by our students that are, I thought they were really well written", "tokens": [821, 366, 732, 6968, 12300, 538, 527, 1731, 300, 366, 11, 286, 1194, 436, 645, 534, 731, 3720], "temperature": 0.0, "avg_logprob": -0.12303029632568359, "compression_ratio": 1.7171717171717171, "no_speech_prob": 7.765137524984311e-06}, {"id": 222, "seek": 83332, "start": 837.88, "end": 840.5200000000001, "text": " and they were also very accessible.", "tokens": [293, 436, 645, 611, 588, 9515, 13], "temperature": 0.0, "avg_logprob": -0.12303029632568359, "compression_ratio": 1.7171717171717171, "no_speech_prob": 7.765137524984311e-06}, {"id": 223, "seek": 83332, "start": 840.5200000000001, "end": 843.98, "text": " And remember, you don't have to be an expert to start blogging.", "tokens": [400, 1604, 11, 291, 500, 380, 362, 281, 312, 364, 5844, 281, 722, 6968, 3249, 13], "temperature": 0.0, "avg_logprob": -0.12303029632568359, "compression_ratio": 1.7171717171717171, "no_speech_prob": 7.765137524984311e-06}, {"id": 224, "seek": 83332, "start": 843.98, "end": 847.44, "text": " Your target audience is the person that's just one or two steps behind you and you're", "tokens": [2260, 3779, 4034, 307, 264, 954, 300, 311, 445, 472, 420, 732, 4439, 2261, 291, 293, 291, 434], "temperature": 0.0, "avg_logprob": -0.12303029632568359, "compression_ratio": 1.7171717171717171, "no_speech_prob": 7.765137524984311e-06}, {"id": 225, "seek": 83332, "start": 847.44, "end": 851.0, "text": " actually best positioned to help that person.", "tokens": [767, 1151, 24889, 281, 854, 300, 954, 13], "temperature": 0.0, "avg_logprob": -0.12303029632568359, "compression_ratio": 1.7171717171717171, "no_speech_prob": 7.765137524984311e-06}, {"id": 226, "seek": 83332, "start": 851.0, "end": 855.6, "text": " The other place that you can practice explaining what you're learning is answering other people's", "tokens": [440, 661, 1081, 300, 291, 393, 3124, 13468, 437, 291, 434, 2539, 307, 13430, 661, 561, 311], "temperature": 0.0, "avg_logprob": -0.12303029632568359, "compression_ratio": 1.7171717171717171, "no_speech_prob": 7.765137524984311e-06}, {"id": 227, "seek": 83332, "start": 855.6, "end": 856.6, "text": " questions.", "tokens": [1651, 13], "temperature": 0.0, "avg_logprob": -0.12303029632568359, "compression_ratio": 1.7171717171717171, "no_speech_prob": 7.765137524984311e-06}, {"id": 228, "seek": 83332, "start": 856.6, "end": 860.96, "text": " And so two places you can do that are on our forums, which are forums.fast.ai.", "tokens": [400, 370, 732, 3190, 291, 393, 360, 300, 366, 322, 527, 26998, 11, 597, 366, 26998, 13, 7011, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.12303029632568359, "compression_ratio": 1.7171717171717171, "no_speech_prob": 7.765137524984311e-06}, {"id": 229, "seek": 86096, "start": 860.96, "end": 866.5600000000001, "text": " And there are always interesting discussions happening related to deep learning.", "tokens": [400, 456, 366, 1009, 1880, 11088, 2737, 4077, 281, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.14680703815660978, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.664207027526572e-06}, {"id": 230, "seek": 86096, "start": 866.5600000000001, "end": 871.6, "text": " So here people are talking about the Kaggle fisheries competition, about average pooling", "tokens": [407, 510, 561, 366, 1417, 466, 264, 48751, 22631, 20698, 530, 6211, 11, 466, 4274, 7005, 278], "temperature": 0.0, "avg_logprob": -0.14680703815660978, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.664207027526572e-06}, {"id": 231, "seek": 86096, "start": 871.6, "end": 875.84, "text": " instead of dense layers, or how to use persistent AWS spot instances.", "tokens": [2602, 295, 18011, 7914, 11, 420, 577, 281, 764, 24315, 17650, 4008, 14519, 13], "temperature": 0.0, "avg_logprob": -0.14680703815660978, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.664207027526572e-06}, {"id": 232, "seek": 86096, "start": 875.84, "end": 878.76, "text": " So definitely, yeah, check them out.", "tokens": [407, 2138, 11, 1338, 11, 1520, 552, 484, 13], "temperature": 0.0, "avg_logprob": -0.14680703815660978, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.664207027526572e-06}, {"id": 233, "seek": 86096, "start": 878.76, "end": 882.96, "text": " Another place that you can ask or answer questions is the learn machine learning subreddit, which", "tokens": [3996, 1081, 300, 291, 393, 1029, 420, 1867, 1651, 307, 264, 1466, 3479, 2539, 1422, 986, 17975, 11, 597], "temperature": 0.0, "avg_logprob": -0.14680703815660978, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.664207027526572e-06}, {"id": 234, "seek": 86096, "start": 882.96, "end": 887.0400000000001, "text": " is different than the machine learning subreddit.", "tokens": [307, 819, 813, 264, 3479, 2539, 1422, 986, 17975, 13], "temperature": 0.0, "avg_logprob": -0.14680703815660978, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.664207027526572e-06}, {"id": 235, "seek": 88704, "start": 887.04, "end": 891.0799999999999, "text": " Here's a quote from one of our students after part one of the course.", "tokens": [1692, 311, 257, 6513, 490, 472, 295, 527, 1731, 934, 644, 472, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.09759552516634502, "compression_ratio": 1.748299319727891, "no_speech_prob": 1.129853626480326e-05}, {"id": 236, "seek": 88704, "start": 891.0799999999999, "end": 895.52, "text": " I personally fell into the habit of watching the lectures too much and Googling definitions", "tokens": [286, 5665, 5696, 666, 264, 7164, 295, 1976, 264, 16564, 886, 709, 293, 45005, 1688, 21988], "temperature": 0.0, "avg_logprob": -0.09759552516634502, "compression_ratio": 1.748299319727891, "no_speech_prob": 1.129853626480326e-05}, {"id": 237, "seek": 88704, "start": 895.52, "end": 897.68, "text": " too much without running the code.", "tokens": [886, 709, 1553, 2614, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.09759552516634502, "compression_ratio": 1.748299319727891, "no_speech_prob": 1.129853626480326e-05}, {"id": 238, "seek": 88704, "start": 897.68, "end": 901.16, "text": " At first, I thought that I should read the code quickly and then spend time researching", "tokens": [1711, 700, 11, 286, 1194, 300, 286, 820, 1401, 264, 3089, 2661, 293, 550, 3496, 565, 24176], "temperature": 0.0, "avg_logprob": -0.09759552516634502, "compression_ratio": 1.748299319727891, "no_speech_prob": 1.129853626480326e-05}, {"id": 239, "seek": 88704, "start": 901.16, "end": 902.8399999999999, "text": " the theory behind it.", "tokens": [264, 5261, 2261, 309, 13], "temperature": 0.0, "avg_logprob": -0.09759552516634502, "compression_ratio": 1.748299319727891, "no_speech_prob": 1.129853626480326e-05}, {"id": 240, "seek": 88704, "start": 902.8399999999999, "end": 907.36, "text": " In retrospect, I should have spent the majority of my time on the actual code, running it", "tokens": [682, 34997, 11, 286, 820, 362, 4418, 264, 6286, 295, 452, 565, 322, 264, 3539, 3089, 11, 2614, 309], "temperature": 0.0, "avg_logprob": -0.09759552516634502, "compression_ratio": 1.748299319727891, "no_speech_prob": 1.129853626480326e-05}, {"id": 241, "seek": 88704, "start": 907.36, "end": 910.1999999999999, "text": " and seeing what goes into it and what comes out of it.", "tokens": [293, 2577, 437, 1709, 666, 309, 293, 437, 1487, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.09759552516634502, "compression_ratio": 1.748299319727891, "no_speech_prob": 1.129853626480326e-05}, {"id": 242, "seek": 88704, "start": 910.1999999999999, "end": 912.04, "text": " So this is excellent advice.", "tokens": [407, 341, 307, 7103, 5192, 13], "temperature": 0.0, "avg_logprob": -0.09759552516634502, "compression_ratio": 1.748299319727891, "no_speech_prob": 1.129853626480326e-05}, {"id": 243, "seek": 88704, "start": 912.04, "end": 914.0, "text": " Learn from this person's mistake.", "tokens": [17216, 490, 341, 954, 311, 6146, 13], "temperature": 0.0, "avg_logprob": -0.09759552516634502, "compression_ratio": 1.748299319727891, "no_speech_prob": 1.129853626480326e-05}, {"id": 244, "seek": 91400, "start": 914.0, "end": 917.96, "text": " Stay focused on the code.", "tokens": [8691, 5178, 322, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.14069599401755412, "compression_ratio": 1.7634408602150538, "no_speech_prob": 1.1123614967800677e-05}, {"id": 245, "seek": 91400, "start": 917.96, "end": 922.24, "text": " And summary, some keys for learning or to have a practical coding project that you care", "tokens": [400, 12691, 11, 512, 9317, 337, 2539, 420, 281, 362, 257, 8496, 17720, 1716, 300, 291, 1127], "temperature": 0.0, "avg_logprob": -0.14069599401755412, "compression_ratio": 1.7634408602150538, "no_speech_prob": 1.1123614967800677e-05}, {"id": 246, "seek": 91400, "start": 922.24, "end": 925.72, "text": " about and let that kind of lead the way for you.", "tokens": [466, 293, 718, 300, 733, 295, 1477, 264, 636, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.14069599401755412, "compression_ratio": 1.7634408602150538, "no_speech_prob": 1.1123614967800677e-05}, {"id": 247, "seek": 91400, "start": 925.72, "end": 928.0, "text": " Get something working as fast as possible.", "tokens": [3240, 746, 1364, 382, 2370, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.14069599401755412, "compression_ratio": 1.7634408602150538, "no_speech_prob": 1.1123614967800677e-05}, {"id": 248, "seek": 91400, "start": 928.0, "end": 930.96, "text": " And it's okay if you don't understand all the components at first.", "tokens": [400, 309, 311, 1392, 498, 291, 500, 380, 1223, 439, 264, 6677, 412, 700, 13], "temperature": 0.0, "avg_logprob": -0.14069599401755412, "compression_ratio": 1.7634408602150538, "no_speech_prob": 1.1123614967800677e-05}, {"id": 249, "seek": 91400, "start": 930.96, "end": 932.52, "text": " And that's actually better.", "tokens": [400, 300, 311, 767, 1101, 13], "temperature": 0.0, "avg_logprob": -0.14069599401755412, "compression_ratio": 1.7634408602150538, "no_speech_prob": 1.1123614967800677e-05}, {"id": 250, "seek": 91400, "start": 932.52, "end": 937.32, "text": " And then kind of as time goes on, you will learn about the components as you improve", "tokens": [400, 550, 733, 295, 382, 565, 1709, 322, 11, 291, 486, 1466, 466, 264, 6677, 382, 291, 3470], "temperature": 0.0, "avg_logprob": -0.14069599401755412, "compression_ratio": 1.7634408602150538, "no_speech_prob": 1.1123614967800677e-05}, {"id": 251, "seek": 91400, "start": 937.32, "end": 939.92, "text": " your performance and dig down.", "tokens": [428, 3389, 293, 2528, 760, 13], "temperature": 0.0, "avg_logprob": -0.14069599401755412, "compression_ratio": 1.7634408602150538, "no_speech_prob": 1.1123614967800677e-05}, {"id": 252, "seek": 91400, "start": 939.92, "end": 943.56, "text": " And then help others answer questions and write about what you're learning.", "tokens": [400, 550, 854, 2357, 1867, 1651, 293, 2464, 466, 437, 291, 434, 2539, 13], "temperature": 0.0, "avg_logprob": -0.14069599401755412, "compression_ratio": 1.7634408602150538, "no_speech_prob": 1.1123614967800677e-05}, {"id": 253, "seek": 94356, "start": 943.56, "end": 946.1999999999999, "text": " I value altruism, but that's not why I recommend it.", "tokens": [286, 2158, 4955, 894, 1434, 11, 457, 300, 311, 406, 983, 286, 2748, 309, 13], "temperature": 0.0, "avg_logprob": -0.15655158636138194, "compression_ratio": 1.59375, "no_speech_prob": 2.467560989316553e-05}, {"id": 254, "seek": 94356, "start": 946.1999999999999, "end": 950.8399999999999, "text": " I recommend it because it's a key part to cementing your own understanding is to explain", "tokens": [286, 2748, 309, 570, 309, 311, 257, 2141, 644, 281, 19729, 278, 428, 1065, 3701, 307, 281, 2903], "temperature": 0.0, "avg_logprob": -0.15655158636138194, "compression_ratio": 1.59375, "no_speech_prob": 2.467560989316553e-05}, {"id": 255, "seek": 94356, "start": 950.8399999999999, "end": 951.8399999999999, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.15655158636138194, "compression_ratio": 1.59375, "no_speech_prob": 2.467560989316553e-05}, {"id": 256, "seek": 94356, "start": 951.8399999999999, "end": 954.1999999999999, "text": " Then I wanted to leave you with a few resources.", "tokens": [1396, 286, 1415, 281, 1856, 291, 365, 257, 1326, 3593, 13], "temperature": 0.0, "avg_logprob": -0.15655158636138194, "compression_ratio": 1.59375, "no_speech_prob": 2.467560989316553e-05}, {"id": 257, "seek": 94356, "start": 954.1999999999999, "end": 959.1199999999999, "text": " So there's our free course in our forums, the Learn Machine Learning subreddit.", "tokens": [407, 456, 311, 527, 1737, 1164, 294, 527, 26998, 11, 264, 17216, 22155, 15205, 1422, 986, 17975, 13], "temperature": 0.0, "avg_logprob": -0.15655158636138194, "compression_ratio": 1.59375, "no_speech_prob": 2.467560989316553e-05}, {"id": 258, "seek": 94356, "start": 959.1199999999999, "end": 963.76, "text": " Some of my favorite deep learning blogs are Steven Marity, Chris Ola.", "tokens": [2188, 295, 452, 2954, 2452, 2539, 31038, 366, 12754, 2039, 507, 11, 6688, 422, 875, 13], "temperature": 0.0, "avg_logprob": -0.15655158636138194, "compression_ratio": 1.59375, "no_speech_prob": 2.467560989316553e-05}, {"id": 259, "seek": 94356, "start": 963.76, "end": 967.9599999999999, "text": " And he doesn't update his old blog anymore, but you should still read his old post.", "tokens": [400, 415, 1177, 380, 5623, 702, 1331, 6968, 3602, 11, 457, 291, 820, 920, 1401, 702, 1331, 2183, 13], "temperature": 0.0, "avg_logprob": -0.15655158636138194, "compression_ratio": 1.59375, "no_speech_prob": 2.467560989316553e-05}, {"id": 260, "seek": 94356, "start": 967.9599999999999, "end": 970.4799999999999, "text": " He now spearheads distill.pub.", "tokens": [634, 586, 26993, 29481, 1483, 373, 13, 79, 836, 13], "temperature": 0.0, "avg_logprob": -0.15655158636138194, "compression_ratio": 1.59375, "no_speech_prob": 2.467560989316553e-05}, {"id": 261, "seek": 97048, "start": 970.48, "end": 975.24, "text": " And Andre Carpathy, I think the Stanford CNN course, which is freely available, is also", "tokens": [400, 20667, 2741, 79, 9527, 11, 286, 519, 264, 20374, 24859, 1164, 11, 597, 307, 16433, 2435, 11, 307, 611], "temperature": 0.0, "avg_logprob": -0.21483669906366068, "compression_ratio": 1.5486111111111112, "no_speech_prob": 1.4736594494024757e-05}, {"id": 262, "seek": 97048, "start": 975.24, "end": 976.78, "text": " excellent.", "tokens": [7103, 13], "temperature": 0.0, "avg_logprob": -0.21483669906366068, "compression_ratio": 1.5486111111111112, "no_speech_prob": 1.4736594494024757e-05}, {"id": 263, "seek": 97048, "start": 976.78, "end": 981.28, "text": " And then, and I'll tweet a link to these slides probably tomorrow.", "tokens": [400, 550, 11, 293, 286, 603, 15258, 257, 2113, 281, 613, 9788, 1391, 4153, 13], "temperature": 0.0, "avg_logprob": -0.21483669906366068, "compression_ratio": 1.5486111111111112, "no_speech_prob": 1.4736594494024757e-05}, {"id": 264, "seek": 97048, "start": 981.28, "end": 986.96, "text": " There's a link to a blog post I wrote that links to like 15 blog posts I like in the", "tokens": [821, 311, 257, 2113, 281, 257, 6968, 2183, 286, 4114, 300, 6123, 281, 411, 2119, 6968, 12300, 286, 411, 294, 264], "temperature": 0.0, "avg_logprob": -0.21483669906366068, "compression_ratio": 1.5486111111111112, "no_speech_prob": 1.4736594494024757e-05}, {"id": 265, "seek": 97048, "start": 986.96, "end": 987.96, "text": " area of deep learning.", "tokens": [1859, 295, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.21483669906366068, "compression_ratio": 1.5486111111111112, "no_speech_prob": 1.4736594494024757e-05}, {"id": 266, "seek": 97048, "start": 987.96, "end": 990.84, "text": " Yeah, so I can take some questions now.", "tokens": [865, 11, 370, 286, 393, 747, 512, 1651, 586, 13], "temperature": 0.0, "avg_logprob": -0.21483669906366068, "compression_ratio": 1.5486111111111112, "no_speech_prob": 1.4736594494024757e-05}, {"id": 267, "seek": 97048, "start": 990.84, "end": 995.6800000000001, "text": " And again, I am at math underscore Rachel on Twitter and I blog at fast.ai.", "tokens": [400, 797, 11, 286, 669, 412, 5221, 37556, 14246, 322, 5794, 293, 286, 6968, 412, 2370, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.21483669906366068, "compression_ratio": 1.5486111111111112, "no_speech_prob": 1.4736594494024757e-05}, {"id": 268, "seek": 97048, "start": 995.6800000000001, "end": 999.52, "text": " I also have an ask a data scientist advice column there.", "tokens": [286, 611, 362, 364, 1029, 257, 1412, 12662, 5192, 7738, 456, 13], "temperature": 0.0, "avg_logprob": -0.21483669906366068, "compression_ratio": 1.5486111111111112, "no_speech_prob": 1.4736594494024757e-05}, {"id": 269, "seek": 99952, "start": 999.52, "end": 1003.48, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.7518816630045573, "compression_ratio": 0.813953488372093, "no_speech_prob": 4.129808803554624e-05}, {"id": 270, "seek": 99952, "start": 1003.48, "end": 1020.92, "text": " That's a great question.", "tokens": [663, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.7518816630045573, "compression_ratio": 0.813953488372093, "no_speech_prob": 4.129808803554624e-05}, {"id": 271, "seek": 102092, "start": 1020.92, "end": 1031.76, "text": " Okay, so the question was, you know, this is very focused on deep learning.", "tokens": [1033, 11, 370, 264, 1168, 390, 11, 291, 458, 11, 341, 307, 588, 5178, 322, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.14528373571542594, "compression_ratio": 1.4827586206896552, "no_speech_prob": 1.8625894881552085e-05}, {"id": 272, "seek": 102092, "start": 1031.76, "end": 1035.12, "text": " What do I think about other tools such as random forest?", "tokens": [708, 360, 286, 519, 466, 661, 3873, 1270, 382, 4974, 6719, 30], "temperature": 0.0, "avg_logprob": -0.14528373571542594, "compression_ratio": 1.4827586206896552, "no_speech_prob": 1.8625894881552085e-05}, {"id": 273, "seek": 102092, "start": 1035.12, "end": 1040.72, "text": " And we actually in part two of our course, we use a neural net on a structured data set", "tokens": [400, 321, 767, 294, 644, 732, 295, 527, 1164, 11, 321, 764, 257, 18161, 2533, 322, 257, 18519, 1412, 992], "temperature": 0.0, "avg_logprob": -0.14528373571542594, "compression_ratio": 1.4827586206896552, "no_speech_prob": 1.8625894881552085e-05}, {"id": 274, "seek": 102092, "start": 1040.72, "end": 1048.6399999999999, "text": " that you would much more traditionally see with like a random forest or XGBoost.", "tokens": [300, 291, 576, 709, 544, 19067, 536, 365, 411, 257, 4974, 6719, 420, 1783, 8769, 78, 555, 13], "temperature": 0.0, "avg_logprob": -0.14528373571542594, "compression_ratio": 1.4827586206896552, "no_speech_prob": 1.8625894881552085e-05}, {"id": 275, "seek": 104864, "start": 1048.64, "end": 1051.5600000000002, "text": " I definitely think random forests are still worth learning about.", "tokens": [286, 2138, 519, 4974, 21700, 366, 920, 3163, 2539, 466, 13], "temperature": 0.0, "avg_logprob": -0.20966777434715858, "compression_ratio": 1.5684931506849316, "no_speech_prob": 2.749614395725075e-05}, {"id": 276, "seek": 104864, "start": 1051.5600000000002, "end": 1055.8000000000002, "text": " I think it's possible in the future though that deep learning is going to be used more", "tokens": [286, 519, 309, 311, 1944, 294, 264, 2027, 1673, 300, 2452, 2539, 307, 516, 281, 312, 1143, 544], "temperature": 0.0, "avg_logprob": -0.20966777434715858, "compression_ratio": 1.5684931506849316, "no_speech_prob": 2.749614395725075e-05}, {"id": 277, "seek": 104864, "start": 1055.8000000000002, "end": 1063.92, "text": " and more on data sets that are still being solved with random forests today.", "tokens": [293, 544, 322, 1412, 6352, 300, 366, 920, 885, 13041, 365, 4974, 21700, 965, 13], "temperature": 0.0, "avg_logprob": -0.20966777434715858, "compression_ratio": 1.5684931506849316, "no_speech_prob": 2.749614395725075e-05}, {"id": 278, "seek": 106392, "start": 1063.92, "end": 1079.44, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51140], "temperature": 1.0, "avg_logprob": -1.2338696320851643, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0021946881897747517}, {"id": 279, "seek": 109392, "start": 1093.92, "end": 1105.72, "text": " Okay, I mean, so I would recommend with not enough data, like I think a lot's being done", "tokens": [1033, 11, 286, 914, 11, 370, 286, 576, 2748, 365, 406, 1547, 1412, 11, 411, 286, 519, 257, 688, 311, 885, 1096], "temperature": 0.0, "avg_logprob": -0.25556431459576895, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.004132916685193777}, {"id": 280, "seek": 109392, "start": 1105.72, "end": 1111.9, "text": " with data augmentation to, like, combined with neural networks to try to deal with that", "tokens": [365, 1412, 14501, 19631, 281, 11, 411, 11, 9354, 365, 18161, 9590, 281, 853, 281, 2028, 365, 300], "temperature": 0.0, "avg_logprob": -0.25556431459576895, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.004132916685193777}, {"id": 281, "seek": 109392, "start": 1111.9, "end": 1112.9, "text": " use case.", "tokens": [764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.25556431459576895, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.004132916685193777}, {"id": 282, "seek": 109392, "start": 1112.9, "end": 1113.9, "text": " What?", "tokens": [708, 30], "temperature": 0.0, "avg_logprob": -0.25556431459576895, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.004132916685193777}, {"id": 283, "seek": 109392, "start": 1113.9, "end": 1119.16, "text": " I'm not familiar with Smote.", "tokens": [286, 478, 406, 4963, 365, 3915, 1370, 13], "temperature": 0.0, "avg_logprob": -0.25556431459576895, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.004132916685193777}, {"id": 284, "seek": 109392, "start": 1119.16, "end": 1122.6000000000001, "text": " Like I'm thinking of kind of, you know, just like even, you know, flipping and random crops", "tokens": [1743, 286, 478, 1953, 295, 733, 295, 11, 291, 458, 11, 445, 411, 754, 11, 291, 458, 11, 26886, 293, 4974, 16829], "temperature": 0.0, "avg_logprob": -0.25556431459576895, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.004132916685193777}, {"id": 285, "seek": 112260, "start": 1122.6, "end": 1127.9199999999998, "text": " and applying like color tents, if you're using picture data, but ways to...", "tokens": [293, 9275, 411, 2017, 39283, 11, 498, 291, 434, 1228, 3036, 1412, 11, 457, 2098, 281, 485], "temperature": 0.0, "avg_logprob": -0.39752151489257814, "compression_ratio": 1.3082706766917294, "no_speech_prob": 5.305458398652263e-05}, {"id": 286, "seek": 112260, "start": 1127.9199999999998, "end": 1130.9199999999998, "text": " Oh, yeah.", "tokens": [876, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.39752151489257814, "compression_ratio": 1.3082706766917294, "no_speech_prob": 5.305458398652263e-05}, {"id": 287, "seek": 112260, "start": 1130.9199999999998, "end": 1151.7199999999998, "text": " Yeah, so I think there are a lot of techniques for augmenting the size of your data set.", "tokens": [865, 11, 370, 286, 519, 456, 366, 257, 688, 295, 7512, 337, 29919, 278, 264, 2744, 295, 428, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.39752151489257814, "compression_ratio": 1.3082706766917294, "no_speech_prob": 5.305458398652263e-05}, {"id": 288, "seek": 115172, "start": 1151.72, "end": 1156.64, "text": " And then like I think, yeah, when you have imbalanced classes, I mean, you can do, you", "tokens": [400, 550, 411, 286, 519, 11, 1338, 11, 562, 291, 362, 566, 40251, 5359, 11, 286, 914, 11, 291, 393, 360, 11, 291], "temperature": 0.0, "avg_logprob": -0.2725944519042969, "compression_ratio": 1.4910179640718564, "no_speech_prob": 4.859784894506447e-06}, {"id": 289, "seek": 115172, "start": 1156.64, "end": 1163.52, "text": " know, imbalanced or, you know, sampling where you're kind of taking more equal numbers from", "tokens": [458, 11, 566, 40251, 420, 11, 291, 458, 11, 21179, 689, 291, 434, 733, 295, 1940, 544, 2681, 3547, 490], "temperature": 0.0, "avg_logprob": -0.2725944519042969, "compression_ratio": 1.4910179640718564, "no_speech_prob": 4.859784894506447e-06}, {"id": 290, "seek": 115172, "start": 1163.52, "end": 1167.0, "text": " each class to try to counteract that.", "tokens": [1184, 1508, 281, 853, 281, 5682, 578, 300, 13], "temperature": 0.0, "avg_logprob": -0.2725944519042969, "compression_ratio": 1.4910179640718564, "no_speech_prob": 4.859784894506447e-06}, {"id": 291, "seek": 115172, "start": 1167.0, "end": 1169.0, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.2725944519042969, "compression_ratio": 1.4910179640718564, "no_speech_prob": 4.859784894506447e-06}, {"id": 292, "seek": 115172, "start": 1169.0, "end": 1175.0, "text": " Oh, that's a good question.", "tokens": [876, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2725944519042969, "compression_ratio": 1.4910179640718564, "no_speech_prob": 4.859784894506447e-06}, {"id": 293, "seek": 117500, "start": 1175.0, "end": 1182.68, "text": " I mean, I think some of it may be just like search for an interesting data set that appeals", "tokens": [286, 914, 11, 286, 519, 512, 295, 309, 815, 312, 445, 411, 3164, 337, 364, 1880, 1412, 992, 300, 32603], "temperature": 0.0, "avg_logprob": -0.1550352616743608, "compression_ratio": 1.7093023255813953, "no_speech_prob": 4.565554263535887e-06}, {"id": 294, "seek": 117500, "start": 1182.68, "end": 1183.68, "text": " to you.", "tokens": [281, 291, 13], "temperature": 0.0, "avg_logprob": -0.1550352616743608, "compression_ratio": 1.7093023255813953, "no_speech_prob": 4.565554263535887e-06}, {"id": 295, "seek": 117500, "start": 1183.68, "end": 1184.68, "text": " And so there are a lot...", "tokens": [400, 370, 456, 366, 257, 688, 485], "temperature": 0.0, "avg_logprob": -0.1550352616743608, "compression_ratio": 1.7093023255813953, "no_speech_prob": 4.565554263535887e-06}, {"id": 296, "seek": 117500, "start": 1184.68, "end": 1189.84, "text": " I mean, Kaggle is always a great place to find interesting data sets or other resources", "tokens": [286, 914, 11, 48751, 22631, 307, 1009, 257, 869, 1081, 281, 915, 1880, 1412, 6352, 420, 661, 3593], "temperature": 0.0, "avg_logprob": -0.1550352616743608, "compression_ratio": 1.7093023255813953, "no_speech_prob": 4.565554263535887e-06}, {"id": 297, "seek": 117500, "start": 1189.84, "end": 1191.16, "text": " out there.", "tokens": [484, 456, 13], "temperature": 0.0, "avg_logprob": -0.1550352616743608, "compression_ratio": 1.7093023255813953, "no_speech_prob": 4.565554263535887e-06}, {"id": 298, "seek": 117500, "start": 1191.16, "end": 1195.16, "text": " Then there are like these organizations like Delta Analytics where you can, you know, volunteer", "tokens": [1396, 456, 366, 411, 613, 6150, 411, 18183, 25944, 689, 291, 393, 11, 291, 458, 11, 13835], "temperature": 0.0, "avg_logprob": -0.1550352616743608, "compression_ratio": 1.7093023255813953, "no_speech_prob": 4.565554263535887e-06}, {"id": 299, "seek": 117500, "start": 1195.16, "end": 1197.16, "text": " to partner with a nonprofit.", "tokens": [281, 4975, 365, 257, 23348, 13], "temperature": 0.0, "avg_logprob": -0.1550352616743608, "compression_ratio": 1.7093023255813953, "no_speech_prob": 4.565554263535887e-06}, {"id": 300, "seek": 117500, "start": 1197.16, "end": 1202.04, "text": " Yeah, and then I think another source might be like seeing if there are any like particular", "tokens": [865, 11, 293, 550, 286, 519, 1071, 4009, 1062, 312, 411, 2577, 498, 456, 366, 604, 411, 1729], "temperature": 0.0, "avg_logprob": -0.1550352616743608, "compression_ratio": 1.7093023255813953, "no_speech_prob": 4.565554263535887e-06}, {"id": 301, "seek": 120204, "start": 1202.04, "end": 1206.6399999999999, "text": " papers that appeal to you.", "tokens": [50364, 10577, 300, 13668, 281, 291, 13, 50594], "temperature": 0.0, "avg_logprob": -0.36372062895033097, "compression_ratio": 0.7647058823529411, "no_speech_prob": 6.699252116959542e-05}], "language": "en"}