{"text": " to myself, to Paige, to Chris, to Ed, or others. Today we actually have a very short agenda and a very welcome guest. So with that, I'd like to hand it off to the man who needs no introduction, Jeremy, to talk about Fast AI v2. Jeremy Thanks, Brennan. So this actually comes out of my enthusiasm for when Adam presented a little bit of Haskell torch code a couple of weeks ago, which I thought was super cool. And so mainly my goal here is to is to kind of encourage other people to present cool things in other languages and libraries, because I think it's a great way for us all to learn what cool stuff you can do. But as tends to happen when you say, Can somebody please do X, somebody else says, Hey, why don't you do X first? So here I am doing X where X is telling you about the library that Sylvia and I have been working on. Basically, since Chris Latner and I finished our last Swift and Fast AI lesson, so for quite a while now, I will, it's a library for PyTorch called Fast AI. And it, I think there are things we can learn from it regarding stuff, cool stuff we can do in Swift. But I'm going to focus on trying to sell you on Fast AI rather than on the Swift bits, but where I think of Swifty things, I will mention them as we go. So Fast AI is a library, as I said, that sits on top of PyTorch. And a lot of people kind of think that a higher level API is this kind of like small little thing that you slap on top of the serious business of TensorFlow or PyTorch or whatever. But hopefully, you'll be convinced when I show you actually what's involved in a truly modern high-level API that there's actually quite a lot going on. If you want to check it out, I put a link to it in the meeting notes, and that will link you to the notebooks, the development notebooks. So that's the first weird thing. What the hell are development notebooks? Well, this is an example of what Fast AI is and what Fast AI v2 source code looks like. It's written, as you see, in notebooks. Really? We are just having a little trouble actually doing that seeing part. Okay, so that probably means I failed to present my screen. Shall I endeavor to do that? That would be great. Present your entire screen. Yeah, that explains a lot. There you go. There we go. Victory. All right. Sorry about that. So here is an example of what the Fast AI v2 source code looks like. It has links. It has titles. It has pictures. It has code. And this may seem like a painful way to develop because these are notebooks that are designed for interactive stuff, not for normal development. But actually, you'll find that also this pixel shuffle appears here in a standard layers.py module, which you can import in the usual way. So we've developed a new literate programming system that allows you to write code and have it automatically turned into nice modules, which even do things that most people don't bother to do because they're annoying if they're not automatic, like setting DUNDA all so it only exports the things that you want. Also coming out of that is automatically documentation. So all that gets turned into hyperlink documentation, including links directly to the source code and automatic doc strings and parameter lists. Also, you'll see tests. And the tests are used both to document the behavior expected. So if you're not sure what pixel shuffle is, this test is a very good description of exactly what it is. And also ensures that our code is working. And those tests can all be put in continuous integration and so forth. So that's the first interesting thing about Fast.ai V2, is it's the first truly literate programming based system I've worked on. And it's been an absolute delight. So we've written our own framework for every part of this, which is kind of a theme for Fast.ai V2. Basically, every time Shilva and I found something that didn't quite work the way we wanted it at any part of the stack, we wrote our own. So it's kind of like building something with no particular deadline and trying to do everything the very, very best we can. So the layered API of Fast.ai V2 starts at the applications layer, which is where most beginners will start. And it looks a lot like Fast.ai V1, which is the released version of the software that people have seen before. But V2, everything is rewritten from scratch. It's totally new. There's no code borrowed. But the top-level API looks quite similar. The idea is that in one, two, three, four lines of code, you can create a state-of-the-art computer vision classifier, including transfer learning, with nearly the same one, two, three, four lines of code, oh, five lines of code in this case, because we're also displaying, you can create a state-of-the-art segmentation model. And actually, like when I say state-of-the-art, like for example, this segmentation model is, to the best of my knowledge, still better than any published result on this particular Canvid data set. So like these five lines of code are super good five lines of code. And as you can see, it includes a line of code, which if you say show batch, it will display your data in an appropriate format, in this case, showing you segmentation, a picture, and the color-coded pixels overlaid on top of the picture. The same basic four lines of code will do text classification. So here's the basis of ULMfit, which is a system that we developed and wrote up along with Sebastian Ruda for transfer learning in natural language processing. And as you can see in here, this is working on IMDB, this is working on IMDB, on a single epoch in four minutes. The accuracy here is basically what was the state-of-the-art as of a couple of years ago. Tabular or time series analysis, same deal, basically a few lines of code, nearly exactly the same lines of code, and you'll get a great result from your tabular data and ditto for collaborative filtering. So the high-level API for Fast AIV2 is designed to be something where, you know, regardless of what application you're working on, you can get a great result from it using sensible defaults and carefully selected hyperparameters, which is automatically largely done for you for the most common kinds of problems that people look at. And that bit doesn't look that different to v1, but understanding how we get to that is kind of interesting and involves getting deeper and deeper. This approach, though, does work super well, and partly it's because this is based on quite a few years of research to figure out what are the best ways to solve various problems along the way. And when people actually try using Fast AI, they're often surprised. So this person posted on our forum that they've been working in TF2 for a while, and for some reason they couldn't figure out all of their models are suddenly working much better. And the answer is basically they're getting all these nice kind of curated best practices, and somebody else on Twitter saw that and said, yep, we found the same thing. We were trying TensorFlow, spent months tweaking, and then we switched to Fast AI. A couple of days later, we were getting better results. So these kind of carefully curated defaults and algorithms and high-level APIs that do things right for you the first time, even for experienced practitioners, can give you better results faster. But it's actually the other pieces that are more, I think, interesting for a Swift conversation, because the deeper we go into how we make that work, the more stuff you'll see which will be a great fit, I think, with Swift. So the mid-layer API is something which is largely new to Fast to, well, actually, I guess the foundation layer is new. So the mid-layer, I guess I'd say, is more rewritten for v1, and it contains some of the things that make those high-level APIs easy. One of the bits which is the most interesting is the training loop itself. And I thank Sylvain for the set of slides we have for the training loop. This is what a training loop looks like in PyTorch. We calculate some predictions, we get a loss, we do a backwards pass to get the gradients, we do an optimizer step, and then optionally, we, from time to time, we'll zero the gradients based on if we're doing, when we're accumulating. So this is what that loop looks like. Run the model, get the loss, do the gradients, step the optimizer, do that a bunch of times. But you want to do something interesting, you'll need to add something to the loop to do keeping track of your training statistics in TensorBoard or in Fast Progress or whatever. You might want to schedule various hyperparameters in various different ways. You might want to add various different types of categorization. You may want to do mixed precision training. You may want to do GANs. So this is a problem because either you have to write a new training loop for every time you want to add a different tweak. Making all those tweaks work together then becomes incredibly complicated. Or you try and write one training loop which does everything you can think of. This is the training loop for Fast AI 0.7 which only did a tiny subset of the things I just said but was still getting ridiculous. Or you can add callbacks at each step. Now the idea of callbacks has been around in deep learning for a long time, APIs. But what's very different about Fast AI is that every callback is actually a two-way callback. It can read absolutely everything. It can read gradients, parameters, data, so forth. And it can write them. So it can actually change anything at any time. So the callbacks, we say infinitely flexible. We feel pretty confident in that because the training loop in Fast AI has not needed to be modified to do any of the tweaks that I showed you before. So even the entirety of training GANs can be done in a callback. So basically we switched out a basic training loop and replaced it with one with the same five steps but callbacks between every step. So that means, for example, if you want to do a scheduler, you can define a batch begin that sets the optimizer's learning rate to some function. Or if you want to do early stopping, you can write an on epoch end that checks the metrics and stops training. Or you can do parallel training, set up data parallel, and if you're happy at the end of training, take data parallel off again. Gradient clipping, you have access to the parameters themselves. So you can click the gradient norms at the end of the backward step. And so forth. So all of these different things are all things that have been written with Fast AI callbacks, including, for example, mixed precision. All of NVIDIA's recommendations, mixed precision training will be added automatically if you just add a 2FP16 at the end of your learn call. And really importantly, you know, for example, all of those mixed precision things can be combined with multi-GPU and one cycle training and gradient accumulation and so forth. And so trying to, you know, create a state of the art model which involves combining state of the art regularization and mixed precision and distributed training and so forth is a really, really, really hard job. But with this approach, it's actually just a single extra line of code to add each feature and they all explicitly are designed to work with each other and are tested to work with each other. So, for instance, here is mixup data augmentation, which is an incredibly powerful data augmentation method that has powered lots of state of the art results. And as you can see, it's well under a screen of code. By comparison, here is the version of mixup from the paper. Not only is it far longer, but it only works with one particular data set and one particular optimizer and is full of all kinds of assumptions and only one particular kind of metric and so forth. So that's an example of these mid-tier APIs. Another one is the optimizer. It turns out that, you know, it looks like there's been lots and lots of different optimizers appearing in the last year or two. But actually, it turns out that they're all minor tweaks on each other. Most libraries don't write them this way. So, for example, AdamW, also known as decoupled weight decay Adam, was added to PyTorch quite recently in the last month or two. And it required writing a whole new class and a whole new step to implement. And it took, you know, it was like two or three years after the paper was released. On the other hand, FastAI's implementation, as you can see, involves a single extra function containing two lines of code and this little bit of gray here. So it's kind of like two and a half, three lines of code to implement the same thing. Because what we did was we realized, let's refactor the idea of an optimizer, see what's different for each of these, you know, state of the art optimizers that have appeared recently, and make it so that each of those things can be added and removed by just changing two things, stats and steppers. A stat is something that you measure during training, such as the gradients or the gradient squared, or you might use dampening or momentum or whatever. And then a stepper is something that uses those stats to change the weights in some way. And you can combine those things together. And by combining these, we've been able to implement all these different optimizers. So, for instance, the LAM optimizer, which came out of Google and was super cool at reducing BERT pre-training time from three days to 76 minutes. We were able to implement that in this tiny piece of code. And one of the nice things is that when you compare it to the math, it really looks almost line for line. Line for line, identical, except ours is a little bit nicer because we refactored some of the math. So, it makes it really easy to do research as well because you can kind of quite directly bring the equations across into your code. Then the last of the mid-tier APIs is the data block API, which is something we had in version one as well. But when we were porting that to Swift, we had an opportunity to rethink it. And actually, Alexis Gallagher in particular helped us to rethink it in a more idiomatically Swifty way. And it came out really nicely. And so, then we took the result of that and kind of ported it back into Python. And we ended up with something that was quite a bit nicer. So, there's been a kind of a nice interaction and interplay between fast AI in Python and Swift AI in Swift in terms of helping each other's APIs. But basically, the data block API is something where you define each of the key things that the program needs to know to flexibly get your data into a form you can put in a model. So, it needs to know what type of data do you have, how do you get that data, how do you split it into a training set and a validation set, and then put that all together into a data bunch, which is just a simple little class. It's literally, I think, four lines of code, which just has the validation set and the training set in one place. So, with a data block, you just say, okay, my types, I want to create a black and white pillow image for my X and a category for my Y. And to get the list of files for those, I need to use this function. And to split those files into training and validation, use this function, which is looking at the grandparent path directory name. And to get the labels, use this function, which is use the parent's path name. And so, with that, that's enough to give you MNIST, for instance. And so, once you've done this, you end up with a data bunch. And as I mentioned before, everything has a show batch. So, one of the nice things is it makes it very easy for you to look at your data, regardless of whether it's tabular or collaborative filtering or vision or text or even audio. If it was audio, it would show you a spectrogram and let you play the sound. So, you can do custom labeling with data blocks by using, for example, a regular expression labeler. You can get your labels from an external file or data frame, and they could be multi-labels. So, this thing here, it's a multi-label classification task. So, it's automatically put a semicolon between each label. Again, it's still basically just three lines of code to define the data block. So, here's a data block for segmentation. And you can see, really, the only thing I had to change here was that my dependent variable has been changed from category to pillow mask. And, again, automatically, I show batch works, and we can train a model from that straight away as well. You could do key points. So, here, I've just changed my dependent variable to tensor point. And so, now it knows how to behave with that. Object detection. So, now, I changed my dependent variable to bounding box. And you can see I've got my bounding boxes here. Text. And so forth. So, actually, going back, I have a couple questions if it's okay to... Anytime. Yeah. So, the code, you've got sort of the Xs and Ys. And these both, sounds like these different data types roughly conform to a protocol. Yep. We're going to get to that in a moment. Absolutely. Okay. Fantastic. It's well-plotted. Yep. That's an excellent way to think of it. And, actually, this is the way it looked about three weeks ago. Now it looks even more like a protocol. So, yes, this is where it all comes from, which is the foundation APIs. And this is the bit that I think is the most relevant to Swift. And a lot of this, I think, would be a lot easier to write in Swift. So, the first thing that we added to PyTorch was object-oriented tenses. For too long, we've all been satisfied with a data type called tensor, which has no semantics to it. And so, those tenses actually represent something like a sentence or a picture of a cat or a recording of somebody saying something. So, why can't I take one of those tenses and say dot flip or dot rotate or dot resample or dot translate to German? Well, the answer is you can't, because it's just a tensor without a type. So, we have added types to tensors. So, you can now have a tensor image, tensor point, tensor bounding box, and you can define a flip left, right for each. And so, this is some of the source code from we've written our own computer vision library. So, that now you can say flip LR, and it flips the puppy. And if it was a key points, it would flip the key points. If it was a bounding box, it would flip the bounding boxes and so forth. So, this is an example of how tensors which carry around semantics are nice. It's also nice that I didn't just say dot show, right? So, dot show is something that's defined for all fast AIV to tensor types, and it will just display that tensor. It could even be a tuple containing a tensor and some bounding boxes and some bounding box classes. Whatever it is, it will be able to display it. It will be able to convert it into batches for modeling and so forth. So, you know, with that, we can now create, for example, a random transformation called flip item, and we can say that the encoding of that random transformation is defined for a pillow image or any tensor type. And in each case, the implementation is simply to call x dot flip LR. Or we could do the dihedral symmetry transforms in the same way. Before we call, grab a random number between zero and seven to decide which of the eight transposes to do, and then encodes call x dot what's dihedral with that thing we just got. And so, now we can call that transform a bunch of times, and each time we'll get back a different random augmentation. So, a lot of these things become nice and easy. Hey, Jeremy, some Maxam asked, why isn't tensor a backing data structure for an image type? A tensor image is a tensor which is an image type. Why isn't, he says, why isn't tensor a backing, why not have a different type named image, I guess, that has a tensor inside of it? Do you mean why inherit rather than compose? Apparently, yes, that. Yeah. So, inheritance, I mean, you can do both, and you can create identical APIs. Inheritance just has the benefit that all the normal stuff you can do with a tensor, you can do with a tensor that happens to be an image. So, just because a tensor is an image doesn't mean you now don't want to be able to do fancy indexing to it or do an LUD composition of it or stack it with other tensors across that axis. So, basically, a tensor image ought to have all the behavior of a tensor plus additional behavior. So, that's why we used inheritance. We have a version that uses composition as well and it uses Python's nice get atra functionality to pass on all of the behavior of tensor, but it comes out more nicely in Python when you do inheritance. And, actually, the PyTorch team has decided to officially implement semantic tensor subtypes now. And so, hopefully, in the next version of PyTorch, you won't have to use the extremely ugly hacks that we had to use to make this work. You'll be able to use the real ones. And, hopefully, you'll see in TorchVision some of these ideas will be brought over there. Can I ask you, so, how does that the type propagate? So, if you do arithmetic on an image tensor, do you get an image tensor back? So, Chris and I had a conversation about this a few months ago and I said I'm banging my head around this issue of types not carrying around their behavior. And Chris casually mentioned, oh, yes, that thing is called higher kind of types. So, I went home and that was one of these phrases that I thought only functional programming dweebs talked about and I would never care about. It actually matters a lot. And it's basically the idea that if you have a tensor image and you add one to it, you want to get back a tensor image because it should be an image that's a bit brighter rather than something that loses its type. So, we implemented our own, again, hacky partial higher kind of type implementation in fast.aiv2. So, any of these things that you do to a tensor of a subtype, you will nearly always get back the correctly subtyped tensor. Yeah, I mean, I saw that PyTorch recently started talking about their named indexing extensions for their tensors as well. And I see they have a similar kind of challenge there where when you start doing arithmetic and other things like that on a tensor that has named dimensions, you want to propagate those along. Yeah, so we haven't started using that yet because it hasn't quite landed its table. But yeah, we talked to the PyTorch team at the DevCon and we certainly are planning to bring these ideas together. They're all Falcon Orbit related concerns. Yeah, I just mean that I assume that that feature has the same problem, the same challenge. I assume so, yeah. It would be interesting to see what they do. Yeah, yeah, it would. Yeah, so, you know, it's kind of nice. Not only do we get to be able to say dot show batch, but you can even go dot show results. And in this case, it knows what the independent variables type is, it knows what the dependent variables type is, and it even knows things like, hey, for a classification task, those two things should be the same. And if they're not, by default, I will highlight them in red. So these lower level foundations are the things that drive our ability to easily add this higher level functionality. So, you know, this is the kind of ugly stuff we wouldn't have to do in Swift. We had to write our own type dispatch system so that we can annotate things with types and those type annotations are actually semantic. And so we now have the joyfully modern idea of function overloading in Python, which has made life a lot easier. And we already have that. Do you have many users that are using this yet? It's still pre-released. It's not even alpha. But there is an enthusiastic early adopter community who is using it. So, for example, the user contributed audio library has already been ported to it. I've also built a medical imaging library on top of it and have written a series of five notebooks showing how to do CT scan analysis with it. So it's kind of like, it works. I was curious what your users think of it because there's this very strongly held conception that Python folks hate types. And you're kind of providing a little bit of typing. I'm curious how they react to that. The extremely biased subset of early adopter classed AI enthusiasts who are using it love it. And they tend to be people who have gone pretty deep in the past. So, for example, my friend Andrew Shaw who wrote something called music autobot, which is one of the coolest things in the world in case you haven't seen it yet. Which is something where you can generate music using a neural network. You can put in some melodies and some chords. And it will autocomplete some additional melodies and chords. Or you can put in a melody and it will automatically add chords. Or you can add chords that create melody. And so he had to write his own MIDI library. He rewrote it in V2. And he said it's just like so, so, so much easier thanks to those mid tier APIs. So, yeah. At this stage. I was just going to jump in quick. I've been helping with some of the audio stuff. And it's been really awesome. So, it makes things a lot more flexible than version one. So, that's probably my favorite thing about it is everything can be interchanged. Nothing is like, well, it's got to be this way because that's how it is. Yep. That's cool. Cool. Thanks. Another piece of the transform is of the foundation is the partially reversible composed function pipeline dispatched over collections. Which really rolls off the tongue. We call them transform and pipeline. Basically, the idea is that the way you kind of want function dispatch to work and function composition to work in deep learning is a little different to other places. There's a couple of things. The first is you often want to dispatch over tuples. And what I mean by that is if you have a function called flip left right and you have a tuple representing a mini batch where your independent variable is a picture and your dependent variable is a set of boundary variables and bounding boxes, if you say flip left right on that tuple, you would expect both the X and the Y to be flipped and to be flipped with the type appropriate method. So, our transforms will automatically send each element of a tuple to the function separately and will dispatch according to their types automatically. We've mentioned type retention. So, the kind of basic type stuff we need. One interesting thing is not only encoding, so, in other words, applying the function, you often need to be able to decode, which is to de-apply the function. So, for example, a categorization transform would take the word dog and convert it to the number one, perhaps, which is what you need for modeling. But then when your predictions come back, you need to know what one represents. So, you need to reverse that transform and turn one back into dog. Often, those transforms also need data-driven setup. For example, in that example of dog becoming one, there needs to be something that actually creates that vocab automatically, recognizing what are all the possible classes, so it can create a different index for each one and then apply that to the validation set. And quite often, these transforms also have some kind of state, such as the vocab. So, we built this bunch of stuff that builds on top of each other. At the lowest level is a class called transform, which is a callable, which also has a decode, does the type retention higher kind of type thing and does the dispatch over tuples by default. So, then a pipeline is something that does function composition over transforms. And it knows about, for example, setting up transforms. And, like, setting up transforms in a pipeline is a bit tricky, because you have to make sure that at each level of the pipeline, only the previous steps have been applied before you set up the next step. So, it does little things like that. And then we have something that applies a pipeline to a collection to give you an indexable, lazily transformed collection. And then you can do those in parallel to get back, you know, an independent variable, for instance. And then finally, we've built a data loader, which will apply these things in parallel in parallel and create collated batches. So, in the end, all this stuff makes a lot of things much easier. For example, the language model data loader in fast.ai v1 was, like, pages of code. In TensorFlow, it's pages of code. In fast.ai v2, it's less than a screen of code by leveraging these powerful abstractions and foundations. So, then finally, and, again, this is something I think Swift will be great for, we worked really hard to make everything extremely well optimized. So, for example, preprocessing and natural language processing, we created a parallel generator in Python, which you can then basically pass a class to that defines some setup and a call. And it can automatically parallelize that. So, for example, tokenization is done in parallel in a pretty memory-efficient way. Excuse me. But perhaps the thing I'm most excited about, both in Python and Swift, is the optimized pipeline running on the GPU. So, all of the, pretty much all of the transforms we've done can and by default do run on the GPU. So, for example, when you do the flip left-right I showed you earlier will actually run on the GPU, as will warp, as will zoom, as will even things like crop. So, one of the basics of this is the affine coordinate transform, which uses affine grid and grid sample, which are very powerful PyTorch functions, which would be great things to actually write in the script for TensorFlow's new metaprogramming, because they don't exist in TensorFlow, or at least not in any very complete way. But with these basic ideas, we can create this affine coordinate transform that lets us do a very wide range of data augmentations in parallel on the GPU. For those of you that know about the Dali library that we've created, this provides a lot of the same benefits as Dali. It's pretty similar in terms of its performance, but the nice thing is all the stuff you write, you write it in Python, not in CUDA. So, with Dali, if they don't have the exact transformation you want, and there's a pretty high chance that they won't, then you're stuck. Or else with fast.aiv2, you can write your own in a few lines of Python, you can test it out in a Jupyter notebook, it makes life super easy. So, this kind of stuff, I feel like, because Swift is a much faster, more hackable language than Python, or at least hackable in the sense of performance, I guess not as hackable in terms of its type system necessarily, I feel like we can kind of build even more powerful foundations and pipelines and a real Swift for TensorFlow computer vision library, leveraging the metaprogramming and leveraging Swift numerics, stuff like that I think would be super cool. So, that is the end of that. That was great. That was excellent. Thank you very much, Jeremy. My pleasure. So, just sort of thinking through, so as you're propagating along this self-type amongst the transformations, that seems relatively straightforward for Swift to handle. Are there other sorts of things that you think we should start thinking about now? Yeah, the thing I really want you to think about, and we've kind of been nagging you on and off since March, is the way that tensors are represented. Having them as a value type the way they are now makes some things hard or impossible. So, the generic optimizer is a thing that I really, really want you guys to look into and build properly. Currently, it uses ugly key path hacks and it's only partially doing what we need it to do. So, I've kind of talked to Alexis about this idea quite a bit. We kind of thought, you know, maybe there could be some type that represents the actual block of GPU memory in a way where we can easily share that. In practice, we've realized the vast majority of the time, we want to refer to that exact piece of memory on the GPU, not this idea of a tensor which may magically copy itself if I change something. And so, for example, with the generic optimizer, we need to be able to say, like, oh, this layer is part of this layer group and this layer group has these things that need to happen to it. So, I actually said to Ed, like, hey, you know, could you please have a look at the Swift AI generic optimizer because it looks it's trying to be a similar design to the fast AI V2 optimizer, but it's currently pretty unattractive. The second is I feel like creating a really good computer vision library is something which could be done now-ish. When I tried to do it, I was getting kind of race conditions and freezes inside Swift and I don't have the Swift skills to know where they were coming from or how to fix them. It would be nice if folks could, like, I think all of my answers is, like, go back to the stuff that we all built together back in March, April, May, and try to start using it in real life and build models with it and put them in production and see the bits where it hits where you get stuck because you'll find things like, oh, there's no grid sample and, oh, there's race conditions in the, you know, interaction of OpenCV and, you know, the optimizer doesn't quite work properly and that stuff. That makes sense. I think we're also trying to figure out right now what the right path is with the runtime. So we've historically been building on top of the TensorFlow runtime, which is great for a lot of reasons. It has a lot of functionality in the box. It does pretty much everything. On the other hand, the performance, particularly in eager mode, is not great. I think one of the things we're kicking around is the idea of going more directly into XLA. Yeah. Well, I think that's a thing that's been stopping us all from XLA being a stepping stone towards MLR in the future, which is also coming. I mean, I think that's the thing that's been stopping us all from using stuff like Swift AI to actually build models because the autodiff has memory leaks and the TensorFlow runtime is, I don't have to be polite, so not at Google, slow as molasses and, you know, implements everything in six different ways in six different places and so forth. So yeah, I think everybody's going to be thinking into these higher level APIs a lot more once the foundations are there. Yeah. And so, I mean, the trade-off there is if we go with that direction now, XLA doesn't provide all the things in the box, but I think that's probably fine. We haven't passed those languages so we can't put stuff if we need it. And so I think we're talking about that, trying to decide what to do there. We're also investing a lot in AD and finishing that off. Yeah. I mean, all the right work's being done. It's just, you know, it's just early days. Yes. Yeah, I think the challenge that we're really struggling with is this decision to stick with the TensorFlow runtime to move on to something else. And that I think is complicated, but I agree this is one of the major blockers for adoption and use. Yeah. I mean, especially if you want to take advantage of Swift, which we do, you need something where, you know, the kernel launch time is tiny or better still kind of non-existent because you can write everything in Swift. Otherwise, it's, yeah, you don't really get the benefits. Yeah. And one of the, so I'll answer your question a second, but one of the trade offs there is that XLA doesn't have really fast kernel launch time because it effectively jit compiles things before launching it. On the other hand, there are a lot of opportunities to do, for example, fusion and other things like that that cannot set it. And one of the nice hybrid models you get is this combination of tracing plus compilation, which I think could be really interesting. Yeah. I'm Sayid S. What's going on with MLNR? There's tons of stuff going on. It's really exciting. Just yesterday, there was a really fantastic talk from some folks at Intel talking about their code generation algorithm they're bringing over to MLNR, which I'm really, really, really excited about. And so there's tons of stuff going on. Getting the ideal code gen for NVIDIA GPUs, for example, is probably still six plus months away. And I don't know how much plus that is. But what I'm encouraging is the community to come together and collaborate instead of the different teams of the different companies being in front of me. And the Intel stuff that they presented yesterday is super, super impressive. So we'll see what happens with that. The other thing I might mention in terms of tails on the other side, what's life like in the Python world? Things that aren't working well over there. The kind of the answer to switch for TensorFlow in the PyTorch world is JIT. So it's basically to trace your Python code and attempt to figure out what it's doing and create what they call TorchScript, which is a dialect subset of Python. Or else to actually parse your Python code is also an option and turn it into TorchScript. It has reached the point now where it can actually be used for good. So one of our students created a bunch of our students actually have been working on a thing called Mesh. And including a young researcher who designed the original thing. It's a very nice activation function that's outperforming everything else that anybody's trying it on. And it was pretty slow. And when we just, you know, it took me half an hour to create a JIT version. And it ran at the same speed as somebody else's hand created CUDA code. So for like small things like that, where it's like two or three lines of code, that's working pretty well. Although for bigger things, like a new batch norm implementation we tried to do during the last course, the performance wasn't there. Or if we actually tried to take like one of the big problems at the moment, not the Python, but the whole world of non-Google people, is that the best computer vision models by far are largely those that have been coming out of Google, like Efficient Nets, Mixed Nets, like Kwok Lee's team. They run very slowly and with a lot of memory on GPUs. And so we tried wrapping an entire Efficient Net and Mixed Net into a JITed thing so it wouldn't be so slow. The Mixed Net didn't work at all and the Efficient Net was a little bit slower. So that's kind of the status of JIT in PyTorch is, you know, bits of it are useful. The way I look at this from the compiler code generation 8 pieces, that I think the MLAir pieces are all going the right direction, they're just going to take a while to get here. XLA, as far as I know, is state-of-the-art in code generation. For the things it does, it does quite well. The challenge with it though is that it does have certain limitations like static shapes and the number of offset supports. And so you kind of have to be within its world for it to be useful. But it has a very useful, it has a large subset of the world that it covers very well. It has a pretty useful world. TorchScript, my understanding is that the base model of TorchScript and the interpreter they have, I understand that's quite nice. But the kernel fusion piece is still fairly early on, I suppose, in element-wise operations, for example. I don't find them that quite nice. I mean, like simple things like, they can't, partly a limitation of the Python type system. So like, you want to be able to write things that can work with different numbers of channels while you're out of luck because they use Python type imitations which have no way of saying it's a tuple of size n. You have to say it's a tuple of size three. So you then have to hard code all these assumptions into your code. Lots of stuff I find pretty frustrating. I see. Interesting. Well, so I mean, I think there's other spaces that I'm eager to reevaluate as, I mean, this isn't the highest priority at this moment. But in terms of our APIs, there's still very legit questions around, should we encode D type in the static type system or should we just say tensor? And if you just say tensor, then you get rid of all the generics everywhere, cleans up tons of code at the cost of losing some of the checking. But then I think if you go with more semantic tensor types, as Jerry was pushing forward, you actually really don't even want the D type. What you want is the semantics and that you're actually in a better spot. Right. Like for big precision, we're switching stuff from one type to another all the time. Depending on whether you're doing a loss function or a gradient calculation or whatever, you need to be changing between half and single. So if we went that direction, I think that would be really interesting in terms of ergonomics, but also simplification, which I think would be great. It would also, like your point about the optimizers, the key path have all kinds of weirdness because you have multiple D types and you want to be generic over D type. And so that's really unpleasant right now. Yeah. I think also like for Swift wanting to bring over big world of Python using data scientists, they're definitely not going to be wanting to put lots and lots of verbose generic type annotations in their Jupyter notebooks. Yeah. So I don't know when we'll have cycles to re-evaluate those APIs, but I think we should go do a fresh take of this and combine it with an XLA based approach. It changes a lot of the trade-offs. Right. It would be really interesting. Yeah. I mean, I think in my mind, right, so a couple of weeks ago, I presented the layering proposal to separate out lib tensor from lib deep learning so that we can then get the freedom to then iterate at that level and have multiple explorations on top. So the progress update on there is I've started, we have the two different packages now in Swift DPI's so you can depend only on one as opposed to the other. And Dan helped fix all the issues that I caused while doing the initial move of the random number generators out of what will become lib deep learning. That said, it's still very early and I have a lot more code to move. Well, I think that Jeremy is like fundamentally right that we need to spend more time with software and the optimizer designs and re-evaluate the training loop callback systems and things like that. Yeah. As each of these variables change, like it affects other parts of the system and different trade-offs, I think should be re-evaluated as we do that. But I think that getting AD like bulletproof is like super important. And performance. Yeah. We have to get those two things right. Well, and upstream and integrate into Swift so that we can build on it and take it for granted. Quick question about tensor without D type. I wonder if they would add any type assertions and any functions. I think the Python model is to not check things and to let things crash at runtime, if I understand. So I don't know. I mean, I think that there's a couple of different options there. I don't know what the right answer is, but again, one of the things that PyTorch is doing is they're doing more coercions with D types. So if you take into account that into an N32, it will actually promote an 8 into an N32, for example. I mean, rocket science. But that's the kind of thing that is just very nice and it just eliminates a certain kind of error. On the other hand, it's kind of like broadcasting where it makes certain things just work at the cost of potentially getting surprising in some cases. So I don't know about that. I think if you do things that don't make sense, like you try to do a floating point operation on an integer, then you would want it to be a runtime error. I think that our model is trying towards a much more runtime-centric approach. I think ironically, Swift for Nintzo started out very static, but now it's going very dynamic. Yeah, for me, I'm realizing one of the major benefits of having FastFast languages, like dynamic is free. And so now you can have super dynamic abstractions that you can do these things in a nice way. And by Torch, you do get a pretty clear runtime error if there's a type mismatch. It doesn't just crash. It'll tell you what it expected and what it got. Yeah. And one of the nice things about EagerMode is that then you get a stack trace. I think there are other ways around, instead of encoding things into the static type system that you have to adhere to. I think Adam's work on integer splitting perfectly shows that you can still get a lot of benefits of static analysis without necessarily encoding into the type system. That said, I think it's still an open question as to how far we can really push that and where we end up landing. Yeah, I think it's just a really great opportunity to reevaluate these things as other pieces are coming together. Maxim asks, why is runtime checking preferable over static analysis? I think it's more that we're still trying to figure out what dimensions you want to be flexible on. And so doing things dynamically is sort of the ultimate in flexibility. And so as we're trying to iterate on the programming model, making sure that things are as dynamic as you want them to be is sometimes nice. And then we should think about how static analysis can help catch errors sooner. Yeah, exactly. And so this is just a spectrum. And it's not that one end of the spectrum is better than the other. It's about where in the spectrum you end up. Nicholas's question, Nicholas asks, how are MLR and XLA related? That is a super complicated question because we're actively re-implementing pieces of XLA in terms of MLR. So that's actually a lot more complicated than it sounds. I would just say that MLR is a broad scale compiler technology that solves lots of problems. XLA as a name is typically thought of as a thing in terms of sensors and deficient code. And so I wouldn't over-index on the letters, the number of letters, I guess. And once Swift has moved to the top of MLR, we'll still use XLA-targeted TPUs. Yeah. So I mean, this is internal work, but we're doing a lot to change and enhance the TPU software stack in XLA. And things in XLA are changing in their implementation as well. And so there's a big investment going on in all these pieces right now. And I think that more generally, again, if you ignore which letters get attached to them, the effort here culminates in a much more flexible cogeneration stack, support for dynamic shapes and custom ops and things like that. It's just that different pieces in this very complicated technology come together at different points. I don't know what the marketing, the crack compiler marketing team will end up labeling the resultant thing. Excellent. We're slightly over time. So I just wanted to, unless there's any pressing questions, thank everyone for joining and see you all next week. I think next week, Mark will be up talking about some of his work on testing the auditive system to ensure that it's really reliable. There's some pretty good things that Mark's been up to there. It's also exciting that Adi is getting upstream to master too, which is really cool. Thanks everyone. Have a great week and see you all next week. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.4, "text": " to myself, to Paige, to Chris, to Ed, or others.", "tokens": [281, 2059, 11, 281, 45177, 11, 281, 6688, 11, 281, 3977, 11, 420, 2357, 13], "temperature": 0.0, "avg_logprob": -0.19021807294903378, "compression_ratio": 1.3511904761904763, "no_speech_prob": 0.006283491384238005}, {"id": 1, "seek": 0, "start": 7.76, "end": 13.84, "text": " Today we actually have a very short agenda and a very welcome guest. So with that, I'd like to", "tokens": [2692, 321, 767, 362, 257, 588, 2099, 9829, 293, 257, 588, 2928, 8341, 13, 407, 365, 300, 11, 286, 1116, 411, 281], "temperature": 0.0, "avg_logprob": -0.19021807294903378, "compression_ratio": 1.3511904761904763, "no_speech_prob": 0.006283491384238005}, {"id": 2, "seek": 0, "start": 13.84, "end": 18.48, "text": " hand it off to the man who needs no introduction, Jeremy, to talk about Fast AI v2.", "tokens": [1011, 309, 766, 281, 264, 587, 567, 2203, 572, 9339, 11, 17809, 11, 281, 751, 466, 15968, 7318, 371, 17, 13], "temperature": 0.0, "avg_logprob": -0.19021807294903378, "compression_ratio": 1.3511904761904763, "no_speech_prob": 0.006283491384238005}, {"id": 3, "seek": 1848, "start": 18.48, "end": 29.520000000000003, "text": " Jeremy Thanks, Brennan. So this actually comes out of my enthusiasm for when", "tokens": [17809, 2561, 11, 31200, 17622, 13, 407, 341, 767, 1487, 484, 295, 452, 23417, 337, 562], "temperature": 0.0, "avg_logprob": -0.19165315050067325, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.00029505978454835713}, {"id": 4, "seek": 1848, "start": 30.08, "end": 36.72, "text": " Adam presented a little bit of Haskell torch code a couple of weeks ago, which I thought was super", "tokens": [7938, 8212, 257, 707, 857, 295, 8646, 43723, 27822, 3089, 257, 1916, 295, 3259, 2057, 11, 597, 286, 1194, 390, 1687], "temperature": 0.0, "avg_logprob": -0.19165315050067325, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.00029505978454835713}, {"id": 5, "seek": 1848, "start": 36.72, "end": 44.88, "text": " cool. And so mainly my goal here is to is to kind of encourage other people to present cool things", "tokens": [1627, 13, 400, 370, 8704, 452, 3387, 510, 307, 281, 307, 281, 733, 295, 5373, 661, 561, 281, 1974, 1627, 721], "temperature": 0.0, "avg_logprob": -0.19165315050067325, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.00029505978454835713}, {"id": 6, "seek": 4488, "start": 44.88, "end": 49.440000000000005, "text": " in other languages and libraries, because I think it's a great way for us all to learn what cool", "tokens": [294, 661, 8650, 293, 15148, 11, 570, 286, 519, 309, 311, 257, 869, 636, 337, 505, 439, 281, 1466, 437, 1627], "temperature": 0.0, "avg_logprob": -0.11276198387145996, "compression_ratio": 1.5298804780876494, "no_speech_prob": 0.0003092786646448076}, {"id": 7, "seek": 4488, "start": 49.440000000000005, "end": 55.6, "text": " stuff you can do. But as tends to happen when you say, Can somebody please do X, somebody else says,", "tokens": [1507, 291, 393, 360, 13, 583, 382, 12258, 281, 1051, 562, 291, 584, 11, 1664, 2618, 1767, 360, 1783, 11, 2618, 1646, 1619, 11], "temperature": 0.0, "avg_logprob": -0.11276198387145996, "compression_ratio": 1.5298804780876494, "no_speech_prob": 0.0003092786646448076}, {"id": 8, "seek": 4488, "start": 55.6, "end": 61.28, "text": " Hey, why don't you do X first? So here I am doing X where X is telling you about the library that", "tokens": [1911, 11, 983, 500, 380, 291, 360, 1783, 700, 30, 407, 510, 286, 669, 884, 1783, 689, 1783, 307, 3585, 291, 466, 264, 6405, 300], "temperature": 0.0, "avg_logprob": -0.11276198387145996, "compression_ratio": 1.5298804780876494, "no_speech_prob": 0.0003092786646448076}, {"id": 9, "seek": 4488, "start": 61.28, "end": 67.52000000000001, "text": " Sylvia and I have been working on. Basically, since Chris Latner and I finished our last", "tokens": [33349, 11617, 293, 286, 362, 668, 1364, 322, 13, 8537, 11, 1670, 6688, 7354, 1193, 293, 286, 4335, 527, 1036], "temperature": 0.0, "avg_logprob": -0.11276198387145996, "compression_ratio": 1.5298804780876494, "no_speech_prob": 0.0003092786646448076}, {"id": 10, "seek": 6752, "start": 67.52, "end": 78.96, "text": " Swift and Fast AI lesson, so for quite a while now, I will, it's a library for PyTorch called Fast AI.", "tokens": [25539, 293, 15968, 7318, 6898, 11, 370, 337, 1596, 257, 1339, 586, 11, 286, 486, 11, 309, 311, 257, 6405, 337, 9953, 51, 284, 339, 1219, 15968, 7318, 13], "temperature": 0.0, "avg_logprob": -0.24838581304440552, "compression_ratio": 1.5522388059701493, "no_speech_prob": 5.1411581807769835e-05}, {"id": 11, "seek": 6752, "start": 78.96, "end": 88.96, "text": " And it, I think there are things we can learn from it regarding stuff, cool stuff we can do in Swift.", "tokens": [400, 309, 11, 286, 519, 456, 366, 721, 321, 393, 1466, 490, 309, 8595, 1507, 11, 1627, 1507, 321, 393, 360, 294, 25539, 13], "temperature": 0.0, "avg_logprob": -0.24838581304440552, "compression_ratio": 1.5522388059701493, "no_speech_prob": 5.1411581807769835e-05}, {"id": 12, "seek": 6752, "start": 88.96, "end": 95.28, "text": " But I'm going to focus on trying to sell you on Fast AI rather than on the Swift bits, but where I think of", "tokens": [583, 286, 478, 516, 281, 1879, 322, 1382, 281, 3607, 291, 322, 15968, 7318, 2831, 813, 322, 264, 25539, 9239, 11, 457, 689, 286, 519, 295], "temperature": 0.0, "avg_logprob": -0.24838581304440552, "compression_ratio": 1.5522388059701493, "no_speech_prob": 5.1411581807769835e-05}, {"id": 13, "seek": 9528, "start": 95.28, "end": 104.64, "text": " Swifty things, I will mention them as we go. So Fast AI is a library, as I said, that sits on top of", "tokens": [25539, 88, 721, 11, 286, 486, 2152, 552, 382, 321, 352, 13, 407, 15968, 7318, 307, 257, 6405, 11, 382, 286, 848, 11, 300, 12696, 322, 1192, 295], "temperature": 0.0, "avg_logprob": -0.16510348092942012, "compression_ratio": 1.50990099009901, "no_speech_prob": 2.3909915398689918e-05}, {"id": 14, "seek": 9528, "start": 104.64, "end": 114.96000000000001, "text": " PyTorch. And a lot of people kind of think that a higher level API is this kind of like small little", "tokens": [9953, 51, 284, 339, 13, 400, 257, 688, 295, 561, 733, 295, 519, 300, 257, 2946, 1496, 9362, 307, 341, 733, 295, 411, 1359, 707], "temperature": 0.0, "avg_logprob": -0.16510348092942012, "compression_ratio": 1.50990099009901, "no_speech_prob": 2.3909915398689918e-05}, {"id": 15, "seek": 9528, "start": 114.96000000000001, "end": 120.48, "text": " thing that you slap on top of the serious business of TensorFlow or PyTorch or whatever. But hopefully,", "tokens": [551, 300, 291, 21075, 322, 1192, 295, 264, 3156, 1606, 295, 37624, 420, 9953, 51, 284, 339, 420, 2035, 13, 583, 4696, 11], "temperature": 0.0, "avg_logprob": -0.16510348092942012, "compression_ratio": 1.50990099009901, "no_speech_prob": 2.3909915398689918e-05}, {"id": 16, "seek": 12048, "start": 120.48, "end": 127.44, "text": " you'll be convinced when I show you actually what's involved in a truly modern high-level API that", "tokens": [291, 603, 312, 12561, 562, 286, 855, 291, 767, 437, 311, 3288, 294, 257, 4908, 4363, 1090, 12, 12418, 9362, 300], "temperature": 0.0, "avg_logprob": -0.17350451800287986, "compression_ratio": 1.6307053941908713, "no_speech_prob": 2.467573358444497e-05}, {"id": 17, "seek": 12048, "start": 127.44, "end": 134.08, "text": " there's actually quite a lot going on. If you want to check it out, I put a link to it in the meeting", "tokens": [456, 311, 767, 1596, 257, 688, 516, 322, 13, 759, 291, 528, 281, 1520, 309, 484, 11, 286, 829, 257, 2113, 281, 309, 294, 264, 3440], "temperature": 0.0, "avg_logprob": -0.17350451800287986, "compression_ratio": 1.6307053941908713, "no_speech_prob": 2.467573358444497e-05}, {"id": 18, "seek": 12048, "start": 134.08, "end": 142.8, "text": " notes, and that will link you to the notebooks, the development notebooks. So that's the first weird", "tokens": [5570, 11, 293, 300, 486, 2113, 291, 281, 264, 43782, 11, 264, 3250, 43782, 13, 407, 300, 311, 264, 700, 3657], "temperature": 0.0, "avg_logprob": -0.17350451800287986, "compression_ratio": 1.6307053941908713, "no_speech_prob": 2.467573358444497e-05}, {"id": 19, "seek": 12048, "start": 142.8, "end": 149.6, "text": " thing. What the hell are development notebooks? Well, this is an example of what Fast AI is", "tokens": [551, 13, 708, 264, 4921, 366, 3250, 43782, 30, 1042, 11, 341, 307, 364, 1365, 295, 437, 15968, 7318, 307], "temperature": 0.0, "avg_logprob": -0.17350451800287986, "compression_ratio": 1.6307053941908713, "no_speech_prob": 2.467573358444497e-05}, {"id": 20, "seek": 14960, "start": 149.6, "end": 156.07999999999998, "text": " and what Fast AI v2 source code looks like. It's written, as you see, in notebooks.", "tokens": [293, 437, 15968, 7318, 371, 17, 4009, 3089, 1542, 411, 13, 467, 311, 3720, 11, 382, 291, 536, 11, 294, 43782, 13], "temperature": 0.0, "avg_logprob": -0.2776406394110786, "compression_ratio": 1.4675324675324675, "no_speech_prob": 1.2804170182789676e-05}, {"id": 21, "seek": 14960, "start": 156.07999999999998, "end": 162.07999999999998, "text": " Really? We are just having a little trouble actually doing that seeing part.", "tokens": [4083, 30, 492, 366, 445, 1419, 257, 707, 5253, 767, 884, 300, 2577, 644, 13], "temperature": 0.0, "avg_logprob": -0.2776406394110786, "compression_ratio": 1.4675324675324675, "no_speech_prob": 1.2804170182789676e-05}, {"id": 22, "seek": 14960, "start": 162.07999999999998, "end": 169.68, "text": " Okay, so that probably means I failed to present my screen. Shall I endeavor to do that?", "tokens": [1033, 11, 370, 300, 1391, 1355, 286, 7612, 281, 1974, 452, 2568, 13, 12128, 286, 34975, 281, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.2776406394110786, "compression_ratio": 1.4675324675324675, "no_speech_prob": 1.2804170182789676e-05}, {"id": 23, "seek": 14960, "start": 169.68, "end": 170.88, "text": " That would be great.", "tokens": [663, 576, 312, 869, 13], "temperature": 0.0, "avg_logprob": -0.2776406394110786, "compression_ratio": 1.4675324675324675, "no_speech_prob": 1.2804170182789676e-05}, {"id": 24, "seek": 14960, "start": 170.88, "end": 176.88, "text": " Present your entire screen. Yeah, that explains a lot. There you go.", "tokens": [33253, 428, 2302, 2568, 13, 865, 11, 300, 13948, 257, 688, 13, 821, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.2776406394110786, "compression_ratio": 1.4675324675324675, "no_speech_prob": 1.2804170182789676e-05}, {"id": 25, "seek": 17688, "start": 176.88, "end": 181.44, "text": " There we go. Victory.", "tokens": [821, 321, 352, 13, 37976, 13], "temperature": 0.0, "avg_logprob": -0.1686685901798614, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.0951479453069624e-05}, {"id": 26, "seek": 17688, "start": 181.44, "end": 189.12, "text": " All right. Sorry about that. So here is an example of what the Fast AI v2 source code looks like.", "tokens": [1057, 558, 13, 4919, 466, 300, 13, 407, 510, 307, 364, 1365, 295, 437, 264, 15968, 7318, 371, 17, 4009, 3089, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1686685901798614, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.0951479453069624e-05}, {"id": 27, "seek": 17688, "start": 189.92, "end": 193.68, "text": " It has links. It has titles. It has pictures. It has code.", "tokens": [467, 575, 6123, 13, 467, 575, 12992, 13, 467, 575, 5242, 13, 467, 575, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1686685901798614, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.0951479453069624e-05}, {"id": 28, "seek": 17688, "start": 196.07999999999998, "end": 202.24, "text": " And this may seem like a painful way to develop because these are notebooks that are designed", "tokens": [400, 341, 815, 1643, 411, 257, 11697, 636, 281, 1499, 570, 613, 366, 43782, 300, 366, 4761], "temperature": 0.0, "avg_logprob": -0.1686685901798614, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.0951479453069624e-05}, {"id": 29, "seek": 20224, "start": 202.24, "end": 208.32000000000002, "text": " for interactive stuff, not for normal development. But actually, you'll find that also this pixel", "tokens": [337, 15141, 1507, 11, 406, 337, 2710, 3250, 13, 583, 767, 11, 291, 603, 915, 300, 611, 341, 19261], "temperature": 0.0, "avg_logprob": -0.08694335392543248, "compression_ratio": 1.4545454545454546, "no_speech_prob": 2.178105933126062e-05}, {"id": 30, "seek": 20224, "start": 208.32000000000002, "end": 223.60000000000002, "text": " shuffle appears here in a standard layers.py module, which you can import in the usual way.", "tokens": [39426, 7038, 510, 294, 257, 3832, 7914, 13, 8200, 10088, 11, 597, 291, 393, 974, 294, 264, 7713, 636, 13], "temperature": 0.0, "avg_logprob": -0.08694335392543248, "compression_ratio": 1.4545454545454546, "no_speech_prob": 2.178105933126062e-05}, {"id": 31, "seek": 20224, "start": 224.48000000000002, "end": 230.4, "text": " So we've developed a new literate programming system that allows you to write code", "tokens": [407, 321, 600, 4743, 257, 777, 2733, 473, 9410, 1185, 300, 4045, 291, 281, 2464, 3089], "temperature": 0.0, "avg_logprob": -0.08694335392543248, "compression_ratio": 1.4545454545454546, "no_speech_prob": 2.178105933126062e-05}, {"id": 32, "seek": 23040, "start": 230.4, "end": 237.52, "text": " and have it automatically turned into nice modules, which even do things that most people", "tokens": [293, 362, 309, 6772, 3574, 666, 1481, 16679, 11, 597, 754, 360, 721, 300, 881, 561], "temperature": 0.0, "avg_logprob": -0.20649335413803288, "compression_ratio": 1.6849315068493151, "no_speech_prob": 4.53914690297097e-05}, {"id": 33, "seek": 23040, "start": 238.24, "end": 242.24, "text": " don't bother to do because they're annoying if they're not automatic, like setting DUNDA all", "tokens": [500, 380, 8677, 281, 360, 570, 436, 434, 11304, 498, 436, 434, 406, 12509, 11, 411, 3287, 413, 3979, 7509, 439], "temperature": 0.0, "avg_logprob": -0.20649335413803288, "compression_ratio": 1.6849315068493151, "no_speech_prob": 4.53914690297097e-05}, {"id": 34, "seek": 23040, "start": 242.24, "end": 248.88, "text": " so it only exports the things that you want. Also coming out of that is automatically documentation.", "tokens": [370, 309, 787, 31428, 264, 721, 300, 291, 528, 13, 2743, 1348, 484, 295, 300, 307, 6772, 14333, 13], "temperature": 0.0, "avg_logprob": -0.20649335413803288, "compression_ratio": 1.6849315068493151, "no_speech_prob": 4.53914690297097e-05}, {"id": 35, "seek": 23040, "start": 248.88, "end": 255.20000000000002, "text": " So all that gets turned into hyperlink documentation, including links directly to the", "tokens": [407, 439, 300, 2170, 3574, 666, 9848, 22473, 14333, 11, 3009, 6123, 3838, 281, 264], "temperature": 0.0, "avg_logprob": -0.20649335413803288, "compression_ratio": 1.6849315068493151, "no_speech_prob": 4.53914690297097e-05}, {"id": 36, "seek": 25520, "start": 255.2, "end": 265.03999999999996, "text": " source code and automatic doc strings and parameter lists. Also, you'll see tests. And the tests are", "tokens": [4009, 3089, 293, 12509, 3211, 13985, 293, 13075, 14511, 13, 2743, 11, 291, 603, 536, 6921, 13, 400, 264, 6921, 366], "temperature": 0.0, "avg_logprob": -0.21507666084203828, "compression_ratio": 1.6331877729257642, "no_speech_prob": 8.397328201681376e-06}, {"id": 37, "seek": 25520, "start": 265.03999999999996, "end": 271.2, "text": " used both to document the behavior expected. So if you're not sure what pixel shuffle is,", "tokens": [1143, 1293, 281, 4166, 264, 5223, 5176, 13, 407, 498, 291, 434, 406, 988, 437, 19261, 39426, 307, 11], "temperature": 0.0, "avg_logprob": -0.21507666084203828, "compression_ratio": 1.6331877729257642, "no_speech_prob": 8.397328201681376e-06}, {"id": 38, "seek": 25520, "start": 271.2, "end": 276.48, "text": " this test is a very good description of exactly what it is. And also ensures that our code is", "tokens": [341, 1500, 307, 257, 588, 665, 3855, 295, 2293, 437, 309, 307, 13, 400, 611, 28111, 300, 527, 3089, 307], "temperature": 0.0, "avg_logprob": -0.21507666084203828, "compression_ratio": 1.6331877729257642, "no_speech_prob": 8.397328201681376e-06}, {"id": 39, "seek": 25520, "start": 276.48, "end": 282.88, "text": " working. And those tests can all be put in continuous integration and so forth. So that's", "tokens": [1364, 13, 400, 729, 6921, 393, 439, 312, 829, 294, 10957, 10980, 293, 370, 5220, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.21507666084203828, "compression_ratio": 1.6331877729257642, "no_speech_prob": 8.397328201681376e-06}, {"id": 40, "seek": 28288, "start": 282.88, "end": 289.2, "text": " the first interesting thing about Fast.ai V2, is it's the first truly literate programming", "tokens": [264, 700, 1880, 551, 466, 15968, 13, 1301, 691, 17, 11, 307, 309, 311, 264, 700, 4908, 2733, 473, 9410], "temperature": 0.0, "avg_logprob": -0.22842380253955571, "compression_ratio": 1.5625, "no_speech_prob": 3.218112760805525e-05}, {"id": 41, "seek": 28288, "start": 289.2, "end": 294.4, "text": " based system I've worked on. And it's been an absolute delight. So we've written our own", "tokens": [2361, 1185, 286, 600, 2732, 322, 13, 400, 309, 311, 668, 364, 8236, 11627, 13, 407, 321, 600, 3720, 527, 1065], "temperature": 0.0, "avg_logprob": -0.22842380253955571, "compression_ratio": 1.5625, "no_speech_prob": 3.218112760805525e-05}, {"id": 42, "seek": 28288, "start": 294.4, "end": 302.32, "text": " framework for every part of this, which is kind of a theme for Fast.ai V2. Basically, every time", "tokens": [8388, 337, 633, 644, 295, 341, 11, 597, 307, 733, 295, 257, 6314, 337, 15968, 13, 1301, 691, 17, 13, 8537, 11, 633, 565], "temperature": 0.0, "avg_logprob": -0.22842380253955571, "compression_ratio": 1.5625, "no_speech_prob": 3.218112760805525e-05}, {"id": 43, "seek": 28288, "start": 302.32, "end": 308.24, "text": " Shilva and I found something that didn't quite work the way we wanted it at any part of the stack,", "tokens": [1160, 388, 2757, 293, 286, 1352, 746, 300, 994, 380, 1596, 589, 264, 636, 321, 1415, 309, 412, 604, 644, 295, 264, 8630, 11], "temperature": 0.0, "avg_logprob": -0.22842380253955571, "compression_ratio": 1.5625, "no_speech_prob": 3.218112760805525e-05}, {"id": 44, "seek": 30824, "start": 308.24, "end": 312.72, "text": " we wrote our own. So it's kind of like building something with no particular deadline and trying", "tokens": [321, 4114, 527, 1065, 13, 407, 309, 311, 733, 295, 411, 2390, 746, 365, 572, 1729, 20615, 293, 1382], "temperature": 0.0, "avg_logprob": -0.10932900278191818, "compression_ratio": 1.5875, "no_speech_prob": 2.212035542470403e-05}, {"id": 45, "seek": 30824, "start": 312.72, "end": 322.72, "text": " to do everything the very, very best we can. So the layered API of Fast.ai V2 starts at the", "tokens": [281, 360, 1203, 264, 588, 11, 588, 1151, 321, 393, 13, 407, 264, 34666, 9362, 295, 15968, 13, 1301, 691, 17, 3719, 412, 264], "temperature": 0.0, "avg_logprob": -0.10932900278191818, "compression_ratio": 1.5875, "no_speech_prob": 2.212035542470403e-05}, {"id": 46, "seek": 30824, "start": 322.72, "end": 331.68, "text": " applications layer, which is where most beginners will start. And it looks a lot like Fast.ai V1,", "tokens": [5821, 4583, 11, 597, 307, 689, 881, 26992, 486, 722, 13, 400, 309, 1542, 257, 688, 411, 15968, 13, 1301, 691, 16, 11], "temperature": 0.0, "avg_logprob": -0.10932900278191818, "compression_ratio": 1.5875, "no_speech_prob": 2.212035542470403e-05}, {"id": 47, "seek": 30824, "start": 331.68, "end": 336.16, "text": " which is the released version of the software that people have seen before. But V2, everything", "tokens": [597, 307, 264, 4736, 3037, 295, 264, 4722, 300, 561, 362, 1612, 949, 13, 583, 691, 17, 11, 1203], "temperature": 0.0, "avg_logprob": -0.10932900278191818, "compression_ratio": 1.5875, "no_speech_prob": 2.212035542470403e-05}, {"id": 48, "seek": 33616, "start": 336.16, "end": 341.20000000000005, "text": " is rewritten from scratch. It's totally new. There's no code borrowed. But the top-level API", "tokens": [307, 319, 26859, 490, 8459, 13, 467, 311, 3879, 777, 13, 821, 311, 572, 3089, 26805, 13, 583, 264, 1192, 12, 12418, 9362], "temperature": 0.0, "avg_logprob": -0.17693770165536918, "compression_ratio": 1.644736842105263, "no_speech_prob": 2.7965474146185443e-05}, {"id": 49, "seek": 33616, "start": 341.20000000000005, "end": 349.20000000000005, "text": " looks quite similar. The idea is that in one, two, three, four lines of code, you can create a", "tokens": [1542, 1596, 2531, 13, 440, 1558, 307, 300, 294, 472, 11, 732, 11, 1045, 11, 1451, 3876, 295, 3089, 11, 291, 393, 1884, 257], "temperature": 0.0, "avg_logprob": -0.17693770165536918, "compression_ratio": 1.644736842105263, "no_speech_prob": 2.7965474146185443e-05}, {"id": 50, "seek": 33616, "start": 350.48, "end": 358.48, "text": " state-of-the-art computer vision classifier, including transfer learning, with nearly the", "tokens": [1785, 12, 2670, 12, 3322, 12, 446, 3820, 5201, 1508, 9902, 11, 3009, 5003, 2539, 11, 365, 6217, 264], "temperature": 0.0, "avg_logprob": -0.17693770165536918, "compression_ratio": 1.644736842105263, "no_speech_prob": 2.7965474146185443e-05}, {"id": 51, "seek": 33616, "start": 358.48, "end": 364.48, "text": " same one, two, three, four lines of code, oh, five lines of code in this case, because we're also", "tokens": [912, 472, 11, 732, 11, 1045, 11, 1451, 3876, 295, 3089, 11, 1954, 11, 1732, 3876, 295, 3089, 294, 341, 1389, 11, 570, 321, 434, 611], "temperature": 0.0, "avg_logprob": -0.17693770165536918, "compression_ratio": 1.644736842105263, "no_speech_prob": 2.7965474146185443e-05}, {"id": 52, "seek": 36448, "start": 364.48, "end": 369.6, "text": " displaying, you can create a state-of-the-art segmentation model. And actually, like when I", "tokens": [36834, 11, 291, 393, 1884, 257, 1785, 12, 2670, 12, 3322, 12, 446, 9469, 399, 2316, 13, 400, 767, 11, 411, 562, 286], "temperature": 0.0, "avg_logprob": -0.17192393493652344, "compression_ratio": 1.7752808988764044, "no_speech_prob": 1.4508624190057162e-05}, {"id": 53, "seek": 36448, "start": 369.6, "end": 373.28000000000003, "text": " say state-of-the-art, like for example, this segmentation model is, to the best of my knowledge,", "tokens": [584, 1785, 12, 2670, 12, 3322, 12, 446, 11, 411, 337, 1365, 11, 341, 9469, 399, 2316, 307, 11, 281, 264, 1151, 295, 452, 3601, 11], "temperature": 0.0, "avg_logprob": -0.17192393493652344, "compression_ratio": 1.7752808988764044, "no_speech_prob": 1.4508624190057162e-05}, {"id": 54, "seek": 36448, "start": 373.28000000000003, "end": 378.40000000000003, "text": " still better than any published result on this particular Canvid data set. So like these five", "tokens": [920, 1101, 813, 604, 6572, 1874, 322, 341, 1729, 1664, 85, 327, 1412, 992, 13, 407, 411, 613, 1732], "temperature": 0.0, "avg_logprob": -0.17192393493652344, "compression_ratio": 1.7752808988764044, "no_speech_prob": 1.4508624190057162e-05}, {"id": 55, "seek": 36448, "start": 378.40000000000003, "end": 384.48, "text": " lines of code are super good five lines of code. And as you can see, it includes a line of code,", "tokens": [3876, 295, 3089, 366, 1687, 665, 1732, 3876, 295, 3089, 13, 400, 382, 291, 393, 536, 11, 309, 5974, 257, 1622, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.17192393493652344, "compression_ratio": 1.7752808988764044, "no_speech_prob": 1.4508624190057162e-05}, {"id": 56, "seek": 36448, "start": 384.48, "end": 391.20000000000005, "text": " which if you say show batch, it will display your data in an appropriate format, in this case,", "tokens": [597, 498, 291, 584, 855, 15245, 11, 309, 486, 4674, 428, 1412, 294, 364, 6854, 7877, 11, 294, 341, 1389, 11], "temperature": 0.0, "avg_logprob": -0.17192393493652344, "compression_ratio": 1.7752808988764044, "no_speech_prob": 1.4508624190057162e-05}, {"id": 57, "seek": 39120, "start": 391.2, "end": 396.4, "text": " showing you segmentation, a picture, and the color-coded pixels overlaid on top of the picture.", "tokens": [4099, 291, 9469, 399, 11, 257, 3036, 11, 293, 264, 2017, 12, 66, 12340, 18668, 670, 875, 327, 322, 1192, 295, 264, 3036, 13], "temperature": 0.0, "avg_logprob": -0.23254390515779194, "compression_ratio": 1.4899598393574298, "no_speech_prob": 1.1841970263049006e-05}, {"id": 58, "seek": 39120, "start": 399.2, "end": 406.8, "text": " The same basic four lines of code will do text classification. So here's the basis of ULMfit,", "tokens": [440, 912, 3875, 1451, 3876, 295, 3089, 486, 360, 2487, 21538, 13, 407, 510, 311, 264, 5143, 295, 624, 43, 44, 6845, 11], "temperature": 0.0, "avg_logprob": -0.23254390515779194, "compression_ratio": 1.4899598393574298, "no_speech_prob": 1.1841970263049006e-05}, {"id": 59, "seek": 39120, "start": 406.8, "end": 413.03999999999996, "text": " which is a system that we developed and wrote up along with Sebastian Ruda for transfer", "tokens": [597, 307, 257, 1185, 300, 321, 4743, 293, 4114, 493, 2051, 365, 31102, 497, 11152, 337, 5003], "temperature": 0.0, "avg_logprob": -0.23254390515779194, "compression_ratio": 1.4899598393574298, "no_speech_prob": 1.1841970263049006e-05}, {"id": 60, "seek": 39120, "start": 413.03999999999996, "end": 418.8, "text": " learning in natural language processing. And as you can see in here, this is working on IMDB,", "tokens": [2539, 294, 3303, 2856, 9007, 13, 400, 382, 291, 393, 536, 294, 510, 11, 341, 307, 1364, 322, 21463, 27735, 11], "temperature": 0.0, "avg_logprob": -0.23254390515779194, "compression_ratio": 1.4899598393574298, "no_speech_prob": 1.1841970263049006e-05}, {"id": 61, "seek": 41880, "start": 418.8, "end": 425.92, "text": " this is working on IMDB, on a single epoch in four minutes. The accuracy here is basically what was", "tokens": [341, 307, 1364, 322, 21463, 27735, 11, 322, 257, 2167, 30992, 339, 294, 1451, 2077, 13, 440, 14170, 510, 307, 1936, 437, 390], "temperature": 0.0, "avg_logprob": -0.1334775136067317, "compression_ratio": 1.5252918287937742, "no_speech_prob": 9.367791790282354e-06}, {"id": 62, "seek": 41880, "start": 425.92, "end": 433.68, "text": " the state-of-the-art as of a couple of years ago. Tabular or time series analysis, same deal,", "tokens": [264, 1785, 12, 2670, 12, 3322, 12, 446, 382, 295, 257, 1916, 295, 924, 2057, 13, 14106, 1040, 420, 565, 2638, 5215, 11, 912, 2028, 11], "temperature": 0.0, "avg_logprob": -0.1334775136067317, "compression_ratio": 1.5252918287937742, "no_speech_prob": 9.367791790282354e-06}, {"id": 63, "seek": 41880, "start": 433.68, "end": 438.96000000000004, "text": " basically a few lines of code, nearly exactly the same lines of code, and you'll get a great result", "tokens": [1936, 257, 1326, 3876, 295, 3089, 11, 6217, 2293, 264, 912, 3876, 295, 3089, 11, 293, 291, 603, 483, 257, 869, 1874], "temperature": 0.0, "avg_logprob": -0.1334775136067317, "compression_ratio": 1.5252918287937742, "no_speech_prob": 9.367791790282354e-06}, {"id": 64, "seek": 43896, "start": 438.96, "end": 449.28, "text": " from your tabular data and ditto for collaborative filtering. So the high-level API for Fast AIV2", "tokens": [490, 428, 4421, 1040, 1412, 293, 274, 34924, 337, 16555, 30822, 13, 407, 264, 1090, 12, 12418, 9362, 337, 15968, 7318, 53, 17], "temperature": 0.0, "avg_logprob": -0.12885525521267666, "compression_ratio": 1.5019455252918288, "no_speech_prob": 1.0288496923749335e-05}, {"id": 65, "seek": 43896, "start": 449.28, "end": 453.84, "text": " is designed to be something where, you know, regardless of what application you're working on,", "tokens": [307, 4761, 281, 312, 746, 689, 11, 291, 458, 11, 10060, 295, 437, 3861, 291, 434, 1364, 322, 11], "temperature": 0.0, "avg_logprob": -0.12885525521267666, "compression_ratio": 1.5019455252918288, "no_speech_prob": 1.0288496923749335e-05}, {"id": 66, "seek": 43896, "start": 454.47999999999996, "end": 459.91999999999996, "text": " you can get a great result from it using sensible defaults and carefully selected hyperparameters,", "tokens": [291, 393, 483, 257, 869, 1874, 490, 309, 1228, 25380, 7576, 82, 293, 7500, 8209, 9848, 2181, 335, 6202, 11], "temperature": 0.0, "avg_logprob": -0.12885525521267666, "compression_ratio": 1.5019455252918288, "no_speech_prob": 1.0288496923749335e-05}, {"id": 67, "seek": 45992, "start": 459.92, "end": 468.08000000000004, "text": " which is automatically largely done for you for the most common kinds of problems that people look", "tokens": [597, 307, 6772, 11611, 1096, 337, 291, 337, 264, 881, 2689, 3685, 295, 2740, 300, 561, 574], "temperature": 0.0, "avg_logprob": -0.13130227843327308, "compression_ratio": 1.5364583333333333, "no_speech_prob": 1.6695159501978196e-05}, {"id": 68, "seek": 45992, "start": 468.08000000000004, "end": 475.04, "text": " at. And that bit doesn't look that different to v1, but understanding how we get to that is kind of", "tokens": [412, 13, 400, 300, 857, 1177, 380, 574, 300, 819, 281, 371, 16, 11, 457, 3701, 577, 321, 483, 281, 300, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.13130227843327308, "compression_ratio": 1.5364583333333333, "no_speech_prob": 1.6695159501978196e-05}, {"id": 69, "seek": 45992, "start": 476.48, "end": 485.36, "text": " interesting and involves getting deeper and deeper. This approach, though, does work super well,", "tokens": [1880, 293, 11626, 1242, 7731, 293, 7731, 13, 639, 3109, 11, 1673, 11, 775, 589, 1687, 731, 11], "temperature": 0.0, "avg_logprob": -0.13130227843327308, "compression_ratio": 1.5364583333333333, "no_speech_prob": 1.6695159501978196e-05}, {"id": 70, "seek": 48536, "start": 485.36, "end": 490.32, "text": " and partly it's because this is based on quite a few years of research to figure out what are the", "tokens": [293, 17031, 309, 311, 570, 341, 307, 2361, 322, 1596, 257, 1326, 924, 295, 2132, 281, 2573, 484, 437, 366, 264], "temperature": 0.0, "avg_logprob": -0.07412547724587577, "compression_ratio": 1.6486486486486487, "no_speech_prob": 3.3205371437361464e-05}, {"id": 71, "seek": 48536, "start": 490.32, "end": 495.68, "text": " best ways to solve various problems along the way. And when people actually try using Fast AI,", "tokens": [1151, 2098, 281, 5039, 3683, 2740, 2051, 264, 636, 13, 400, 562, 561, 767, 853, 1228, 15968, 7318, 11], "temperature": 0.0, "avg_logprob": -0.07412547724587577, "compression_ratio": 1.6486486486486487, "no_speech_prob": 3.3205371437361464e-05}, {"id": 72, "seek": 48536, "start": 495.68, "end": 501.36, "text": " they're often surprised. So this person posted on our forum that they've been working in TF2 for a", "tokens": [436, 434, 2049, 6100, 13, 407, 341, 954, 9437, 322, 527, 17542, 300, 436, 600, 668, 1364, 294, 40964, 17, 337, 257], "temperature": 0.0, "avg_logprob": -0.07412547724587577, "compression_ratio": 1.6486486486486487, "no_speech_prob": 3.3205371437361464e-05}, {"id": 73, "seek": 48536, "start": 501.36, "end": 506.32, "text": " while, and for some reason they couldn't figure out all of their models are suddenly working much", "tokens": [1339, 11, 293, 337, 512, 1778, 436, 2809, 380, 2573, 484, 439, 295, 641, 5245, 366, 5800, 1364, 709], "temperature": 0.0, "avg_logprob": -0.07412547724587577, "compression_ratio": 1.6486486486486487, "no_speech_prob": 3.3205371437361464e-05}, {"id": 74, "seek": 48536, "start": 506.32, "end": 512.32, "text": " better. And the answer is basically they're getting all these nice kind of curated best practices,", "tokens": [1101, 13, 400, 264, 1867, 307, 1936, 436, 434, 1242, 439, 613, 1481, 733, 295, 47851, 1151, 7525, 11], "temperature": 0.0, "avg_logprob": -0.07412547724587577, "compression_ratio": 1.6486486486486487, "no_speech_prob": 3.3205371437361464e-05}, {"id": 75, "seek": 51232, "start": 512.32, "end": 517.12, "text": " and somebody else on Twitter saw that and said, yep, we found the same thing. We were trying", "tokens": [293, 2618, 1646, 322, 5794, 1866, 300, 293, 848, 11, 18633, 11, 321, 1352, 264, 912, 551, 13, 492, 645, 1382], "temperature": 0.0, "avg_logprob": -0.12821304905521977, "compression_ratio": 1.6020408163265305, "no_speech_prob": 7.527238722104812e-06}, {"id": 76, "seek": 51232, "start": 517.12, "end": 521.6, "text": " TensorFlow, spent months tweaking, and then we switched to Fast AI. A couple of days later,", "tokens": [37624, 11, 4418, 2493, 6986, 2456, 11, 293, 550, 321, 16858, 281, 15968, 7318, 13, 316, 1916, 295, 1708, 1780, 11], "temperature": 0.0, "avg_logprob": -0.12821304905521977, "compression_ratio": 1.6020408163265305, "no_speech_prob": 7.527238722104812e-06}, {"id": 77, "seek": 51232, "start": 521.6, "end": 528.32, "text": " we were getting better results. So these kind of carefully curated defaults and algorithms and", "tokens": [321, 645, 1242, 1101, 3542, 13, 407, 613, 733, 295, 7500, 47851, 7576, 82, 293, 14642, 293], "temperature": 0.0, "avg_logprob": -0.12821304905521977, "compression_ratio": 1.6020408163265305, "no_speech_prob": 7.527238722104812e-06}, {"id": 78, "seek": 51232, "start": 528.32, "end": 533.2800000000001, "text": " high-level APIs that do things right for you the first time, even for experienced practitioners,", "tokens": [1090, 12, 12418, 21445, 300, 360, 721, 558, 337, 291, 264, 700, 565, 11, 754, 337, 6751, 25742, 11], "temperature": 0.0, "avg_logprob": -0.12821304905521977, "compression_ratio": 1.6020408163265305, "no_speech_prob": 7.527238722104812e-06}, {"id": 79, "seek": 51232, "start": 534.24, "end": 541.12, "text": " can give you better results faster. But it's actually the other pieces that are more, I think,", "tokens": [393, 976, 291, 1101, 3542, 4663, 13, 583, 309, 311, 767, 264, 661, 3755, 300, 366, 544, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.12821304905521977, "compression_ratio": 1.6020408163265305, "no_speech_prob": 7.527238722104812e-06}, {"id": 80, "seek": 54112, "start": 541.12, "end": 547.76, "text": " interesting for a Swift conversation, because the deeper we go into how we make that work,", "tokens": [1880, 337, 257, 25539, 3761, 11, 570, 264, 7731, 321, 352, 666, 577, 321, 652, 300, 589, 11], "temperature": 0.0, "avg_logprob": -0.23245427941763272, "compression_ratio": 1.574766355140187, "no_speech_prob": 3.26878325722646e-05}, {"id": 81, "seek": 54112, "start": 548.16, "end": 552.5600000000001, "text": " the more stuff you'll see which will be a great fit, I think, with Swift.", "tokens": [264, 544, 1507, 291, 603, 536, 597, 486, 312, 257, 869, 3318, 11, 286, 519, 11, 365, 25539, 13], "temperature": 0.0, "avg_logprob": -0.23245427941763272, "compression_ratio": 1.574766355140187, "no_speech_prob": 3.26878325722646e-05}, {"id": 82, "seek": 54112, "start": 554.88, "end": 562.4, "text": " So the mid-layer API is something which is largely new to Fast to, well, actually,", "tokens": [407, 264, 2062, 12, 8376, 260, 9362, 307, 746, 597, 307, 11611, 777, 281, 15968, 281, 11, 731, 11, 767, 11], "temperature": 0.0, "avg_logprob": -0.23245427941763272, "compression_ratio": 1.574766355140187, "no_speech_prob": 3.26878325722646e-05}, {"id": 83, "seek": 54112, "start": 562.4, "end": 566.0, "text": " I guess the foundation layer is new. So the mid-layer, I guess I'd say, is more rewritten", "tokens": [286, 2041, 264, 7030, 4583, 307, 777, 13, 407, 264, 2062, 12, 8376, 260, 11, 286, 2041, 286, 1116, 584, 11, 307, 544, 319, 26859], "temperature": 0.0, "avg_logprob": -0.23245427941763272, "compression_ratio": 1.574766355140187, "no_speech_prob": 3.26878325722646e-05}, {"id": 84, "seek": 56600, "start": 566.0, "end": 573.04, "text": " for v1, and it contains some of the things that make those high-level APIs easy.", "tokens": [337, 371, 16, 11, 293, 309, 8306, 512, 295, 264, 721, 300, 652, 729, 1090, 12, 12418, 21445, 1858, 13], "temperature": 0.0, "avg_logprob": -0.11456163918099753, "compression_ratio": 1.5472636815920398, "no_speech_prob": 8.939198778534774e-06}, {"id": 85, "seek": 56600, "start": 574.64, "end": 579.76, "text": " One of the bits which is the most interesting is the training loop itself.", "tokens": [1485, 295, 264, 9239, 597, 307, 264, 881, 1880, 307, 264, 3097, 6367, 2564, 13], "temperature": 0.0, "avg_logprob": -0.11456163918099753, "compression_ratio": 1.5472636815920398, "no_speech_prob": 8.939198778534774e-06}, {"id": 86, "seek": 56600, "start": 582.16, "end": 585.52, "text": " And I thank Sylvain for the set of slides we have for the training loop.", "tokens": [400, 286, 1309, 3902, 14574, 491, 337, 264, 992, 295, 9788, 321, 362, 337, 264, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.11456163918099753, "compression_ratio": 1.5472636815920398, "no_speech_prob": 8.939198778534774e-06}, {"id": 87, "seek": 56600, "start": 586.96, "end": 592.4, "text": " This is what a training loop looks like in PyTorch. We calculate some predictions,", "tokens": [639, 307, 437, 257, 3097, 6367, 1542, 411, 294, 9953, 51, 284, 339, 13, 492, 8873, 512, 21264, 11], "temperature": 0.0, "avg_logprob": -0.11456163918099753, "compression_ratio": 1.5472636815920398, "no_speech_prob": 8.939198778534774e-06}, {"id": 88, "seek": 59240, "start": 592.4, "end": 598.3199999999999, "text": " we get a loss, we do a backwards pass to get the gradients, we do an optimizer step,", "tokens": [321, 483, 257, 4470, 11, 321, 360, 257, 12204, 1320, 281, 483, 264, 2771, 2448, 11, 321, 360, 364, 5028, 6545, 1823, 11], "temperature": 0.0, "avg_logprob": -0.08774071473341721, "compression_ratio": 1.7451923076923077, "no_speech_prob": 3.70473790098913e-05}, {"id": 89, "seek": 59240, "start": 598.3199999999999, "end": 602.72, "text": " and then optionally, we, from time to time, we'll zero the gradients based on if we're doing,", "tokens": [293, 550, 3614, 379, 11, 321, 11, 490, 565, 281, 565, 11, 321, 603, 4018, 264, 2771, 2448, 2361, 322, 498, 321, 434, 884, 11], "temperature": 0.0, "avg_logprob": -0.08774071473341721, "compression_ratio": 1.7451923076923077, "no_speech_prob": 3.70473790098913e-05}, {"id": 90, "seek": 59240, "start": 603.6, "end": 610.16, "text": " when we're accumulating. So this is what that loop looks like. Run the model, get the loss,", "tokens": [562, 321, 434, 12989, 12162, 13, 407, 341, 307, 437, 300, 6367, 1542, 411, 13, 8950, 264, 2316, 11, 483, 264, 4470, 11], "temperature": 0.0, "avg_logprob": -0.08774071473341721, "compression_ratio": 1.7451923076923077, "no_speech_prob": 3.70473790098913e-05}, {"id": 91, "seek": 59240, "start": 610.16, "end": 618.4, "text": " do the gradients, step the optimizer, do that a bunch of times. But you want to do something", "tokens": [360, 264, 2771, 2448, 11, 1823, 264, 5028, 6545, 11, 360, 300, 257, 3840, 295, 1413, 13, 583, 291, 528, 281, 360, 746], "temperature": 0.0, "avg_logprob": -0.08774071473341721, "compression_ratio": 1.7451923076923077, "no_speech_prob": 3.70473790098913e-05}, {"id": 92, "seek": 61840, "start": 618.4, "end": 623.6, "text": " interesting, you'll need to add something to the loop to do keeping track of your training", "tokens": [1880, 11, 291, 603, 643, 281, 909, 746, 281, 264, 6367, 281, 360, 5145, 2837, 295, 428, 3097], "temperature": 0.0, "avg_logprob": -0.12988736769732306, "compression_ratio": 1.7320574162679425, "no_speech_prob": 3.0893868370185373e-06}, {"id": 93, "seek": 61840, "start": 623.6, "end": 630.0799999999999, "text": " statistics in TensorBoard or in Fast Progress or whatever. You might want to schedule various", "tokens": [12523, 294, 34306, 22493, 515, 420, 294, 15968, 32587, 420, 2035, 13, 509, 1062, 528, 281, 7567, 3683], "temperature": 0.0, "avg_logprob": -0.12988736769732306, "compression_ratio": 1.7320574162679425, "no_speech_prob": 3.0893868370185373e-06}, {"id": 94, "seek": 61840, "start": 630.0799999999999, "end": 634.56, "text": " hyperparameters in various different ways. You might want to add various different", "tokens": [9848, 2181, 335, 6202, 294, 3683, 819, 2098, 13, 509, 1062, 528, 281, 909, 3683, 819], "temperature": 0.0, "avg_logprob": -0.12988736769732306, "compression_ratio": 1.7320574162679425, "no_speech_prob": 3.0893868370185373e-06}, {"id": 95, "seek": 61840, "start": 634.56, "end": 641.84, "text": " types of categorization. You may want to do mixed precision training. You may want to do GANs.", "tokens": [3467, 295, 19250, 2144, 13, 509, 815, 528, 281, 360, 7467, 18356, 3097, 13, 509, 815, 528, 281, 360, 460, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.12988736769732306, "compression_ratio": 1.7320574162679425, "no_speech_prob": 3.0893868370185373e-06}, {"id": 96, "seek": 64184, "start": 641.84, "end": 648.48, "text": " So this is a problem because either you have to write a new training loop for every time you want", "tokens": [407, 341, 307, 257, 1154, 570, 2139, 291, 362, 281, 2464, 257, 777, 3097, 6367, 337, 633, 565, 291, 528], "temperature": 0.0, "avg_logprob": -0.2443519804212782, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.3320328637055354e-06}, {"id": 97, "seek": 64184, "start": 648.48, "end": 653.6800000000001, "text": " to add a different tweak. Making all those tweaks work together then becomes incredibly complicated.", "tokens": [281, 909, 257, 819, 29879, 13, 14595, 439, 729, 46664, 589, 1214, 550, 3643, 6252, 6179, 13], "temperature": 0.0, "avg_logprob": -0.2443519804212782, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.3320328637055354e-06}, {"id": 98, "seek": 64184, "start": 655.6800000000001, "end": 660.48, "text": " Or you try and write one training loop which does everything you can think of. This is the training", "tokens": [1610, 291, 853, 293, 2464, 472, 3097, 6367, 597, 775, 1203, 291, 393, 519, 295, 13, 639, 307, 264, 3097], "temperature": 0.0, "avg_logprob": -0.2443519804212782, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.3320328637055354e-06}, {"id": 99, "seek": 64184, "start": 660.48, "end": 666.0, "text": " loop for Fast AI 0.7 which only did a tiny subset of the things I just said but was still getting", "tokens": [6367, 337, 15968, 7318, 1958, 13, 22, 597, 787, 630, 257, 5870, 25993, 295, 264, 721, 286, 445, 848, 457, 390, 920, 1242], "temperature": 0.0, "avg_logprob": -0.2443519804212782, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.3320328637055354e-06}, {"id": 100, "seek": 66600, "start": 666.0, "end": 674.64, "text": " ridiculous. Or you can add callbacks at each step. Now the idea of callbacks has been around in", "tokens": [11083, 13, 1610, 291, 393, 909, 818, 17758, 412, 1184, 1823, 13, 823, 264, 1558, 295, 818, 17758, 575, 668, 926, 294], "temperature": 0.0, "avg_logprob": -0.2002470272103536, "compression_ratio": 1.6340425531914893, "no_speech_prob": 2.156753225790453e-06}, {"id": 101, "seek": 66600, "start": 675.28, "end": 682.24, "text": " deep learning for a long time, APIs. But what's very different about Fast AI is that every callback", "tokens": [2452, 2539, 337, 257, 938, 565, 11, 21445, 13, 583, 437, 311, 588, 819, 466, 15968, 7318, 307, 300, 633, 818, 3207], "temperature": 0.0, "avg_logprob": -0.2002470272103536, "compression_ratio": 1.6340425531914893, "no_speech_prob": 2.156753225790453e-06}, {"id": 102, "seek": 66600, "start": 682.24, "end": 687.76, "text": " is actually a two-way callback. It can read absolutely everything. It can read gradients,", "tokens": [307, 767, 257, 732, 12, 676, 818, 3207, 13, 467, 393, 1401, 3122, 1203, 13, 467, 393, 1401, 2771, 2448, 11], "temperature": 0.0, "avg_logprob": -0.2002470272103536, "compression_ratio": 1.6340425531914893, "no_speech_prob": 2.156753225790453e-06}, {"id": 103, "seek": 68776, "start": 687.76, "end": 696.3199999999999, "text": " parameters, data, so forth. And it can write them. So it can actually change anything at any time.", "tokens": [9834, 11, 1412, 11, 370, 5220, 13, 400, 309, 393, 2464, 552, 13, 407, 309, 393, 767, 1319, 1340, 412, 604, 565, 13], "temperature": 0.0, "avg_logprob": -0.10890027549531725, "compression_ratio": 1.5, "no_speech_prob": 3.0415008041018154e-06}, {"id": 104, "seek": 68776, "start": 696.96, "end": 705.76, "text": " So the callbacks, we say infinitely flexible. We feel pretty confident in that because the training", "tokens": [407, 264, 818, 17758, 11, 321, 584, 36227, 11358, 13, 492, 841, 1238, 6679, 294, 300, 570, 264, 3097], "temperature": 0.0, "avg_logprob": -0.10890027549531725, "compression_ratio": 1.5, "no_speech_prob": 3.0415008041018154e-06}, {"id": 105, "seek": 68776, "start": 705.76, "end": 712.88, "text": " loop in Fast AI has not needed to be modified to do any of the tweaks that I showed you before.", "tokens": [6367, 294, 15968, 7318, 575, 406, 2978, 281, 312, 15873, 281, 360, 604, 295, 264, 46664, 300, 286, 4712, 291, 949, 13], "temperature": 0.0, "avg_logprob": -0.10890027549531725, "compression_ratio": 1.5, "no_speech_prob": 3.0415008041018154e-06}, {"id": 106, "seek": 71288, "start": 712.88, "end": 719.92, "text": " So even the entirety of training GANs can be done in a callback. So basically we switched out a", "tokens": [407, 754, 264, 31557, 295, 3097, 460, 1770, 82, 393, 312, 1096, 294, 257, 818, 3207, 13, 407, 1936, 321, 16858, 484, 257], "temperature": 0.0, "avg_logprob": -0.0748455027739207, "compression_ratio": 1.6394849785407726, "no_speech_prob": 1.505656541667122e-06}, {"id": 107, "seek": 71288, "start": 719.92, "end": 727.92, "text": " basic training loop and replaced it with one with the same five steps but callbacks between every", "tokens": [3875, 3097, 6367, 293, 10772, 309, 365, 472, 365, 264, 912, 1732, 4439, 457, 818, 17758, 1296, 633], "temperature": 0.0, "avg_logprob": -0.0748455027739207, "compression_ratio": 1.6394849785407726, "no_speech_prob": 1.505656541667122e-06}, {"id": 108, "seek": 71288, "start": 727.92, "end": 736.48, "text": " step. So that means, for example, if you want to do a scheduler, you can define a batch begin that", "tokens": [1823, 13, 407, 300, 1355, 11, 337, 1365, 11, 498, 291, 528, 281, 360, 257, 12000, 260, 11, 291, 393, 6964, 257, 15245, 1841, 300], "temperature": 0.0, "avg_logprob": -0.0748455027739207, "compression_ratio": 1.6394849785407726, "no_speech_prob": 1.505656541667122e-06}, {"id": 109, "seek": 71288, "start": 736.48, "end": 741.36, "text": " sets the optimizer's learning rate to some function. Or if you want to do early stopping,", "tokens": [6352, 264, 5028, 6545, 311, 2539, 3314, 281, 512, 2445, 13, 1610, 498, 291, 528, 281, 360, 2440, 12767, 11], "temperature": 0.0, "avg_logprob": -0.0748455027739207, "compression_ratio": 1.6394849785407726, "no_speech_prob": 1.505656541667122e-06}, {"id": 110, "seek": 74136, "start": 741.36, "end": 749.52, "text": " you can write an on epoch end that checks the metrics and stops training. Or you can do parallel", "tokens": [291, 393, 2464, 364, 322, 30992, 339, 917, 300, 13834, 264, 16367, 293, 10094, 3097, 13, 1610, 291, 393, 360, 8952], "temperature": 0.0, "avg_logprob": -0.11399220383685568, "compression_ratio": 1.740909090909091, "no_speech_prob": 3.34033529725275e-06}, {"id": 111, "seek": 74136, "start": 749.52, "end": 755.76, "text": " training, set up data parallel, and if you're happy at the end of training, take data parallel", "tokens": [3097, 11, 992, 493, 1412, 8952, 11, 293, 498, 291, 434, 2055, 412, 264, 917, 295, 3097, 11, 747, 1412, 8952], "temperature": 0.0, "avg_logprob": -0.11399220383685568, "compression_ratio": 1.740909090909091, "no_speech_prob": 3.34033529725275e-06}, {"id": 112, "seek": 74136, "start": 755.76, "end": 762.32, "text": " off again. Gradient clipping, you have access to the parameters themselves. So you can click", "tokens": [766, 797, 13, 16710, 1196, 49320, 11, 291, 362, 2105, 281, 264, 9834, 2969, 13, 407, 291, 393, 2052], "temperature": 0.0, "avg_logprob": -0.11399220383685568, "compression_ratio": 1.740909090909091, "no_speech_prob": 3.34033529725275e-06}, {"id": 113, "seek": 74136, "start": 762.32, "end": 769.6800000000001, "text": " the gradient norms at the end of the backward step. And so forth. So all of these different things", "tokens": [264, 16235, 24357, 412, 264, 917, 295, 264, 23897, 1823, 13, 400, 370, 5220, 13, 407, 439, 295, 613, 819, 721], "temperature": 0.0, "avg_logprob": -0.11399220383685568, "compression_ratio": 1.740909090909091, "no_speech_prob": 3.34033529725275e-06}, {"id": 114, "seek": 76968, "start": 769.68, "end": 775.52, "text": " are all things that have been written with Fast AI callbacks, including, for example,", "tokens": [366, 439, 721, 300, 362, 668, 3720, 365, 15968, 7318, 818, 17758, 11, 3009, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.18746604475864145, "compression_ratio": 1.579646017699115, "no_speech_prob": 2.3550681362394243e-05}, {"id": 115, "seek": 76968, "start": 775.52, "end": 782.9599999999999, "text": " mixed precision. All of NVIDIA's recommendations, mixed precision training will be added", "tokens": [7467, 18356, 13, 1057, 295, 426, 3958, 6914, 311, 10434, 11, 7467, 18356, 3097, 486, 312, 3869], "temperature": 0.0, "avg_logprob": -0.18746604475864145, "compression_ratio": 1.579646017699115, "no_speech_prob": 2.3550681362394243e-05}, {"id": 116, "seek": 76968, "start": 782.9599999999999, "end": 792.0799999999999, "text": " automatically if you just add a 2FP16 at the end of your learn call. And really importantly,", "tokens": [6772, 498, 291, 445, 909, 257, 568, 45882, 6866, 412, 264, 917, 295, 428, 1466, 818, 13, 400, 534, 8906, 11], "temperature": 0.0, "avg_logprob": -0.18746604475864145, "compression_ratio": 1.579646017699115, "no_speech_prob": 2.3550681362394243e-05}, {"id": 117, "seek": 76968, "start": 792.0799999999999, "end": 797.28, "text": " you know, for example, all of those mixed precision things can be combined with multi-GPU", "tokens": [291, 458, 11, 337, 1365, 11, 439, 295, 729, 7467, 18356, 721, 393, 312, 9354, 365, 4825, 12, 38, 8115], "temperature": 0.0, "avg_logprob": -0.18746604475864145, "compression_ratio": 1.579646017699115, "no_speech_prob": 2.3550681362394243e-05}, {"id": 118, "seek": 79728, "start": 797.28, "end": 807.68, "text": " and one cycle training and gradient accumulation and so forth. And so trying to, you know, create", "tokens": [293, 472, 6586, 3097, 293, 16235, 35647, 293, 370, 5220, 13, 400, 370, 1382, 281, 11, 291, 458, 11, 1884], "temperature": 0.0, "avg_logprob": -0.1298039284619418, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.784852990269428e-06}, {"id": 119, "seek": 79728, "start": 807.68, "end": 814.16, "text": " a state of the art model which involves combining state of the art regularization and mixed precision", "tokens": [257, 1785, 295, 264, 1523, 2316, 597, 11626, 21928, 1785, 295, 264, 1523, 3890, 2144, 293, 7467, 18356], "temperature": 0.0, "avg_logprob": -0.1298039284619418, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.784852990269428e-06}, {"id": 120, "seek": 79728, "start": 814.16, "end": 820.4, "text": " and distributed training and so forth is a really, really, really hard job. But with this approach,", "tokens": [293, 12631, 3097, 293, 370, 5220, 307, 257, 534, 11, 534, 11, 534, 1152, 1691, 13, 583, 365, 341, 3109, 11], "temperature": 0.0, "avg_logprob": -0.1298039284619418, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.784852990269428e-06}, {"id": 121, "seek": 79728, "start": 821.04, "end": 826.64, "text": " it's actually just a single extra line of code to add each feature and they all explicitly are", "tokens": [309, 311, 767, 445, 257, 2167, 2857, 1622, 295, 3089, 281, 909, 1184, 4111, 293, 436, 439, 20803, 366], "temperature": 0.0, "avg_logprob": -0.1298039284619418, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.784852990269428e-06}, {"id": 122, "seek": 82664, "start": 826.64, "end": 833.04, "text": " designed to work with each other and are tested to work with each other. So, for instance, here is", "tokens": [4761, 281, 589, 365, 1184, 661, 293, 366, 8246, 281, 589, 365, 1184, 661, 13, 407, 11, 337, 5197, 11, 510, 307], "temperature": 0.0, "avg_logprob": -0.08776395007817432, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.722928209346719e-05}, {"id": 123, "seek": 82664, "start": 833.04, "end": 839.76, "text": " mixup data augmentation, which is an incredibly powerful data augmentation method that has powered", "tokens": [2890, 1010, 1412, 14501, 19631, 11, 597, 307, 364, 6252, 4005, 1412, 14501, 19631, 3170, 300, 575, 17786], "temperature": 0.0, "avg_logprob": -0.08776395007817432, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.722928209346719e-05}, {"id": 124, "seek": 82664, "start": 839.76, "end": 846.24, "text": " lots of state of the art results. And as you can see, it's well under a screen of code. By comparison,", "tokens": [3195, 295, 1785, 295, 264, 1523, 3542, 13, 400, 382, 291, 393, 536, 11, 309, 311, 731, 833, 257, 2568, 295, 3089, 13, 3146, 9660, 11], "temperature": 0.0, "avg_logprob": -0.08776395007817432, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.722928209346719e-05}, {"id": 125, "seek": 82664, "start": 847.4399999999999, "end": 853.76, "text": " here is the version of mixup from the paper. Not only is it far longer, but it only works with", "tokens": [510, 307, 264, 3037, 295, 2890, 1010, 490, 264, 3035, 13, 1726, 787, 307, 309, 1400, 2854, 11, 457, 309, 787, 1985, 365], "temperature": 0.0, "avg_logprob": -0.08776395007817432, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.722928209346719e-05}, {"id": 126, "seek": 85376, "start": 853.76, "end": 859.76, "text": " one particular data set and one particular optimizer and is full of all kinds of assumptions and only", "tokens": [472, 1729, 1412, 992, 293, 472, 1729, 5028, 6545, 293, 307, 1577, 295, 439, 3685, 295, 17695, 293, 787], "temperature": 0.0, "avg_logprob": -0.05572497340994821, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.3211074474384077e-05}, {"id": 127, "seek": 85376, "start": 859.76, "end": 867.6, "text": " one particular kind of metric and so forth. So that's an example of these mid-tier APIs.", "tokens": [472, 1729, 733, 295, 20678, 293, 370, 5220, 13, 407, 300, 311, 364, 1365, 295, 613, 2062, 12, 25402, 21445, 13], "temperature": 0.0, "avg_logprob": -0.05572497340994821, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.3211074474384077e-05}, {"id": 128, "seek": 85376, "start": 868.48, "end": 877.2, "text": " Another one is the optimizer. It turns out that, you know, it looks like there's been lots and", "tokens": [3996, 472, 307, 264, 5028, 6545, 13, 467, 4523, 484, 300, 11, 291, 458, 11, 309, 1542, 411, 456, 311, 668, 3195, 293], "temperature": 0.0, "avg_logprob": -0.05572497340994821, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.3211074474384077e-05}, {"id": 129, "seek": 87720, "start": 877.2, "end": 884.4000000000001, "text": " lots of different optimizers appearing in the last year or two. But actually, it turns out that they're", "tokens": [3195, 295, 819, 5028, 22525, 19870, 294, 264, 1036, 1064, 420, 732, 13, 583, 767, 11, 309, 4523, 484, 300, 436, 434], "temperature": 0.0, "avg_logprob": -0.11557858868649132, "compression_ratio": 1.438423645320197, "no_speech_prob": 4.196383815724403e-05}, {"id": 130, "seek": 87720, "start": 884.4000000000001, "end": 890.08, "text": " all minor tweaks on each other. Most libraries don't write them this way. So, for example,", "tokens": [439, 6696, 46664, 322, 1184, 661, 13, 4534, 15148, 500, 380, 2464, 552, 341, 636, 13, 407, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.11557858868649132, "compression_ratio": 1.438423645320197, "no_speech_prob": 4.196383815724403e-05}, {"id": 131, "seek": 87720, "start": 890.72, "end": 900.72, "text": " AdamW, also known as decoupled weight decay Adam, was added to PyTorch quite recently in the last", "tokens": [7938, 54, 11, 611, 2570, 382, 979, 263, 15551, 3364, 21039, 7938, 11, 390, 3869, 281, 9953, 51, 284, 339, 1596, 3938, 294, 264, 1036], "temperature": 0.0, "avg_logprob": -0.11557858868649132, "compression_ratio": 1.438423645320197, "no_speech_prob": 4.196383815724403e-05}, {"id": 132, "seek": 90072, "start": 900.72, "end": 910.4, "text": " month or two. And it required writing a whole new class and a whole new step to implement. And it", "tokens": [1618, 420, 732, 13, 400, 309, 4739, 3579, 257, 1379, 777, 1508, 293, 257, 1379, 777, 1823, 281, 4445, 13, 400, 309], "temperature": 0.0, "avg_logprob": -0.1147185637026417, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.028760198096279e-05}, {"id": 133, "seek": 90072, "start": 910.4, "end": 915.6, "text": " took, you know, it was like two or three years after the paper was released. On the other hand,", "tokens": [1890, 11, 291, 458, 11, 309, 390, 411, 732, 420, 1045, 924, 934, 264, 3035, 390, 4736, 13, 1282, 264, 661, 1011, 11], "temperature": 0.0, "avg_logprob": -0.1147185637026417, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.028760198096279e-05}, {"id": 134, "seek": 90072, "start": 916.72, "end": 923.2, "text": " FastAI's implementation, as you can see, involves a single extra function containing two lines of", "tokens": [15968, 48698, 311, 11420, 11, 382, 291, 393, 536, 11, 11626, 257, 2167, 2857, 2445, 19273, 732, 3876, 295], "temperature": 0.0, "avg_logprob": -0.1147185637026417, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.028760198096279e-05}, {"id": 135, "seek": 90072, "start": 923.2, "end": 928.64, "text": " code and this little bit of gray here. So it's kind of like two and a half, three lines of code", "tokens": [3089, 293, 341, 707, 857, 295, 10855, 510, 13, 407, 309, 311, 733, 295, 411, 732, 293, 257, 1922, 11, 1045, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.1147185637026417, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.028760198096279e-05}, {"id": 136, "seek": 92864, "start": 928.64, "end": 936.48, "text": " to implement the same thing. Because what we did was we realized, let's refactor the idea of an", "tokens": [281, 4445, 264, 912, 551, 13, 1436, 437, 321, 630, 390, 321, 5334, 11, 718, 311, 1895, 15104, 264, 1558, 295, 364], "temperature": 0.0, "avg_logprob": -0.080229209529029, "compression_ratio": 1.5856353591160222, "no_speech_prob": 5.255026280792663e-06}, {"id": 137, "seek": 92864, "start": 936.48, "end": 942.64, "text": " optimizer, see what's different for each of these, you know, state of the art optimizers that have", "tokens": [5028, 6545, 11, 536, 437, 311, 819, 337, 1184, 295, 613, 11, 291, 458, 11, 1785, 295, 264, 1523, 5028, 22525, 300, 362], "temperature": 0.0, "avg_logprob": -0.080229209529029, "compression_ratio": 1.5856353591160222, "no_speech_prob": 5.255026280792663e-06}, {"id": 138, "seek": 92864, "start": 942.64, "end": 949.68, "text": " appeared recently, and make it so that each of those things can be added and removed by just", "tokens": [8516, 3938, 11, 293, 652, 309, 370, 300, 1184, 295, 729, 721, 393, 312, 3869, 293, 7261, 538, 445], "temperature": 0.0, "avg_logprob": -0.080229209529029, "compression_ratio": 1.5856353591160222, "no_speech_prob": 5.255026280792663e-06}, {"id": 139, "seek": 94968, "start": 949.68, "end": 959.68, "text": " changing two things, stats and steppers. A stat is something that you measure during training,", "tokens": [4473, 732, 721, 11, 18152, 293, 2126, 15226, 13, 316, 2219, 307, 746, 300, 291, 3481, 1830, 3097, 11], "temperature": 0.0, "avg_logprob": -0.07646461508490822, "compression_ratio": 1.8, "no_speech_prob": 8.397270903515164e-06}, {"id": 140, "seek": 94968, "start": 959.68, "end": 964.16, "text": " such as the gradients or the gradient squared, or you might use dampening or momentum or whatever.", "tokens": [1270, 382, 264, 2771, 2448, 420, 264, 16235, 8889, 11, 420, 291, 1062, 764, 19498, 4559, 420, 11244, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.07646461508490822, "compression_ratio": 1.8, "no_speech_prob": 8.397270903515164e-06}, {"id": 141, "seek": 94968, "start": 964.8, "end": 972.0, "text": " And then a stepper is something that uses those stats to change the weights in some way. And you", "tokens": [400, 550, 257, 2126, 3717, 307, 746, 300, 4960, 729, 18152, 281, 1319, 264, 17443, 294, 512, 636, 13, 400, 291], "temperature": 0.0, "avg_logprob": -0.07646461508490822, "compression_ratio": 1.8, "no_speech_prob": 8.397270903515164e-06}, {"id": 142, "seek": 94968, "start": 972.0, "end": 975.92, "text": " can combine those things together. And by combining these, we've been able to implement", "tokens": [393, 10432, 729, 721, 1214, 13, 400, 538, 21928, 613, 11, 321, 600, 668, 1075, 281, 4445], "temperature": 0.0, "avg_logprob": -0.07646461508490822, "compression_ratio": 1.8, "no_speech_prob": 8.397270903515164e-06}, {"id": 143, "seek": 97592, "start": 975.92, "end": 984.24, "text": " all these different optimizers. So, for instance, the LAM optimizer,", "tokens": [439, 613, 819, 5028, 22525, 13, 407, 11, 337, 5197, 11, 264, 441, 2865, 5028, 6545, 11], "temperature": 0.0, "avg_logprob": -0.20056203777870435, "compression_ratio": 1.4848484848484849, "no_speech_prob": 1.0615588507789653e-05}, {"id": 144, "seek": 97592, "start": 986.0, "end": 991.36, "text": " which came out of Google and was super cool at reducing BERT pre-training time from three days", "tokens": [597, 1361, 484, 295, 3329, 293, 390, 1687, 1627, 412, 12245, 363, 31479, 659, 12, 17227, 1760, 565, 490, 1045, 1708], "temperature": 0.0, "avg_logprob": -0.20056203777870435, "compression_ratio": 1.4848484848484849, "no_speech_prob": 1.0615588507789653e-05}, {"id": 145, "seek": 97592, "start": 991.36, "end": 998.64, "text": " to 76 minutes. We were able to implement that in this tiny piece of code. And one of the nice", "tokens": [281, 24733, 2077, 13, 492, 645, 1075, 281, 4445, 300, 294, 341, 5870, 2522, 295, 3089, 13, 400, 472, 295, 264, 1481], "temperature": 0.0, "avg_logprob": -0.20056203777870435, "compression_ratio": 1.4848484848484849, "no_speech_prob": 1.0615588507789653e-05}, {"id": 146, "seek": 97592, "start": 998.64, "end": 1004.56, "text": " things is that when you compare it to the math, it really looks almost line for line.", "tokens": [721, 307, 300, 562, 291, 6794, 309, 281, 264, 5221, 11, 309, 534, 1542, 1920, 1622, 337, 1622, 13], "temperature": 0.0, "avg_logprob": -0.20056203777870435, "compression_ratio": 1.4848484848484849, "no_speech_prob": 1.0615588507789653e-05}, {"id": 147, "seek": 100456, "start": 1004.56, "end": 1010.2399999999999, "text": " Line for line, identical, except ours is a little bit nicer because we refactored some of the math.", "tokens": [14670, 337, 1622, 11, 14800, 11, 3993, 11896, 307, 257, 707, 857, 22842, 570, 321, 1895, 578, 2769, 512, 295, 264, 5221, 13], "temperature": 0.0, "avg_logprob": -0.13651253520578577, "compression_ratio": 1.46524064171123, "no_speech_prob": 5.421643436420709e-06}, {"id": 148, "seek": 100456, "start": 1011.52, "end": 1017.1199999999999, "text": " So, it makes it really easy to do research as well because you can kind of quite directly", "tokens": [407, 11, 309, 1669, 309, 534, 1858, 281, 360, 2132, 382, 731, 570, 291, 393, 733, 295, 1596, 3838], "temperature": 0.0, "avg_logprob": -0.13651253520578577, "compression_ratio": 1.46524064171123, "no_speech_prob": 5.421643436420709e-06}, {"id": 149, "seek": 100456, "start": 1017.1199999999999, "end": 1024.48, "text": " bring the equations across into your code. Then the last of the mid-tier APIs is the", "tokens": [1565, 264, 11787, 2108, 666, 428, 3089, 13, 1396, 264, 1036, 295, 264, 2062, 12, 25402, 21445, 307, 264], "temperature": 0.0, "avg_logprob": -0.13651253520578577, "compression_ratio": 1.46524064171123, "no_speech_prob": 5.421643436420709e-06}, {"id": 150, "seek": 102448, "start": 1024.48, "end": 1037.6, "text": " data block API, which is something we had in version one as well. But when we were porting that to", "tokens": [1412, 3461, 9362, 11, 597, 307, 746, 321, 632, 294, 3037, 472, 382, 731, 13, 583, 562, 321, 645, 2436, 278, 300, 281], "temperature": 0.0, "avg_logprob": -0.1352637079026964, "compression_ratio": 1.4946236559139785, "no_speech_prob": 1.3418011803878471e-05}, {"id": 151, "seek": 102448, "start": 1037.6, "end": 1046.88, "text": " Swift, we had an opportunity to rethink it. And actually, Alexis Gallagher in particular helped", "tokens": [25539, 11, 321, 632, 364, 2650, 281, 34595, 309, 13, 400, 767, 11, 39826, 14588, 559, 511, 294, 1729, 4254], "temperature": 0.0, "avg_logprob": -0.1352637079026964, "compression_ratio": 1.4946236559139785, "no_speech_prob": 1.3418011803878471e-05}, {"id": 152, "seek": 102448, "start": 1046.88, "end": 1052.88, "text": " us to rethink it in a more idiomatically Swifty way. And it came out really nicely.", "tokens": [505, 281, 34595, 309, 294, 257, 544, 18014, 298, 5030, 25539, 88, 636, 13, 400, 309, 1361, 484, 534, 9594, 13], "temperature": 0.0, "avg_logprob": -0.1352637079026964, "compression_ratio": 1.4946236559139785, "no_speech_prob": 1.3418011803878471e-05}, {"id": 153, "seek": 105288, "start": 1052.88, "end": 1057.5200000000002, "text": " And so, then we took the result of that and kind of ported it back into Python. And we ended up", "tokens": [400, 370, 11, 550, 321, 1890, 264, 1874, 295, 300, 293, 733, 295, 2436, 292, 309, 646, 666, 15329, 13, 400, 321, 4590, 493], "temperature": 0.0, "avg_logprob": -0.1008852796351656, "compression_ratio": 1.6535087719298245, "no_speech_prob": 1.2028132005070802e-05}, {"id": 154, "seek": 105288, "start": 1057.5200000000002, "end": 1061.92, "text": " with something that was quite a bit nicer. So, there's been a kind of a nice interaction and", "tokens": [365, 746, 300, 390, 1596, 257, 857, 22842, 13, 407, 11, 456, 311, 668, 257, 733, 295, 257, 1481, 9285, 293], "temperature": 0.0, "avg_logprob": -0.1008852796351656, "compression_ratio": 1.6535087719298245, "no_speech_prob": 1.2028132005070802e-05}, {"id": 155, "seek": 105288, "start": 1061.92, "end": 1069.68, "text": " interplay between fast AI in Python and Swift AI in Swift in terms of helping each other's APIs.", "tokens": [728, 2858, 1296, 2370, 7318, 294, 15329, 293, 25539, 7318, 294, 25539, 294, 2115, 295, 4315, 1184, 661, 311, 21445, 13], "temperature": 0.0, "avg_logprob": -0.1008852796351656, "compression_ratio": 1.6535087719298245, "no_speech_prob": 1.2028132005070802e-05}, {"id": 156, "seek": 105288, "start": 1070.4, "end": 1076.88, "text": " But basically, the data block API is something where you define each of the key things that", "tokens": [583, 1936, 11, 264, 1412, 3461, 9362, 307, 746, 689, 291, 6964, 1184, 295, 264, 2141, 721, 300], "temperature": 0.0, "avg_logprob": -0.1008852796351656, "compression_ratio": 1.6535087719298245, "no_speech_prob": 1.2028132005070802e-05}, {"id": 157, "seek": 107688, "start": 1076.88, "end": 1083.1200000000001, "text": " the program needs to know to flexibly get your data into a form you can put in a model.", "tokens": [264, 1461, 2203, 281, 458, 281, 5896, 3545, 483, 428, 1412, 666, 257, 1254, 291, 393, 829, 294, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.07080626487731934, "compression_ratio": 1.7983193277310925, "no_speech_prob": 3.647237826953642e-05}, {"id": 158, "seek": 107688, "start": 1084.0, "end": 1090.96, "text": " So, it needs to know what type of data do you have, how do you get that data, how do you split", "tokens": [407, 11, 309, 2203, 281, 458, 437, 2010, 295, 1412, 360, 291, 362, 11, 577, 360, 291, 483, 300, 1412, 11, 577, 360, 291, 7472], "temperature": 0.0, "avg_logprob": -0.07080626487731934, "compression_ratio": 1.7983193277310925, "no_speech_prob": 3.647237826953642e-05}, {"id": 159, "seek": 107688, "start": 1090.96, "end": 1095.68, "text": " it into a training set and a validation set, and then put that all together into a data bunch,", "tokens": [309, 666, 257, 3097, 992, 293, 257, 24071, 992, 11, 293, 550, 829, 300, 439, 1214, 666, 257, 1412, 3840, 11], "temperature": 0.0, "avg_logprob": -0.07080626487731934, "compression_ratio": 1.7983193277310925, "no_speech_prob": 3.647237826953642e-05}, {"id": 160, "seek": 107688, "start": 1095.68, "end": 1098.96, "text": " which is just a simple little class. It's literally, I think, four lines of code,", "tokens": [597, 307, 445, 257, 2199, 707, 1508, 13, 467, 311, 3736, 11, 286, 519, 11, 1451, 3876, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.07080626487731934, "compression_ratio": 1.7983193277310925, "no_speech_prob": 3.647237826953642e-05}, {"id": 161, "seek": 109896, "start": 1098.96, "end": 1107.52, "text": " which just has the validation set and the training set in one place. So, with a data block,", "tokens": [597, 445, 575, 264, 24071, 992, 293, 264, 3097, 992, 294, 472, 1081, 13, 407, 11, 365, 257, 1412, 3461, 11], "temperature": 0.0, "avg_logprob": -0.1493867039680481, "compression_ratio": 1.7209302325581395, "no_speech_prob": 4.425300176080782e-06}, {"id": 162, "seek": 109896, "start": 1109.1200000000001, "end": 1115.68, "text": " you just say, okay, my types, I want to create a black and white pillow image for my X and a", "tokens": [291, 445, 584, 11, 1392, 11, 452, 3467, 11, 286, 528, 281, 1884, 257, 2211, 293, 2418, 18581, 3256, 337, 452, 1783, 293, 257], "temperature": 0.0, "avg_logprob": -0.1493867039680481, "compression_ratio": 1.7209302325581395, "no_speech_prob": 4.425300176080782e-06}, {"id": 163, "seek": 109896, "start": 1115.68, "end": 1123.1200000000001, "text": " category for my Y. And to get the list of files for those, I need to use this function. And to", "tokens": [7719, 337, 452, 398, 13, 400, 281, 483, 264, 1329, 295, 7098, 337, 729, 11, 286, 643, 281, 764, 341, 2445, 13, 400, 281], "temperature": 0.0, "avg_logprob": -0.1493867039680481, "compression_ratio": 1.7209302325581395, "no_speech_prob": 4.425300176080782e-06}, {"id": 164, "seek": 109896, "start": 1123.1200000000001, "end": 1127.76, "text": " split those files into training and validation, use this function, which is looking at the", "tokens": [7472, 729, 7098, 666, 3097, 293, 24071, 11, 764, 341, 2445, 11, 597, 307, 1237, 412, 264], "temperature": 0.0, "avg_logprob": -0.1493867039680481, "compression_ratio": 1.7209302325581395, "no_speech_prob": 4.425300176080782e-06}, {"id": 165, "seek": 112776, "start": 1127.76, "end": 1134.96, "text": " grandparent path directory name. And to get the labels, use this function, which is use the", "tokens": [2697, 38321, 3100, 21120, 1315, 13, 400, 281, 483, 264, 16949, 11, 764, 341, 2445, 11, 597, 307, 764, 264], "temperature": 0.0, "avg_logprob": -0.11974849150731014, "compression_ratio": 1.6491228070175439, "no_speech_prob": 1.7501790352980606e-05}, {"id": 166, "seek": 112776, "start": 1134.96, "end": 1144.48, "text": " parent's path name. And so, with that, that's enough to give you MNIST, for instance. And so,", "tokens": [2596, 311, 3100, 1315, 13, 400, 370, 11, 365, 300, 11, 300, 311, 1547, 281, 976, 291, 376, 45, 19756, 11, 337, 5197, 13, 400, 370, 11], "temperature": 0.0, "avg_logprob": -0.11974849150731014, "compression_ratio": 1.6491228070175439, "no_speech_prob": 1.7501790352980606e-05}, {"id": 167, "seek": 112776, "start": 1144.48, "end": 1150.24, "text": " once you've done this, you end up with a data bunch. And as I mentioned before, everything has", "tokens": [1564, 291, 600, 1096, 341, 11, 291, 917, 493, 365, 257, 1412, 3840, 13, 400, 382, 286, 2835, 949, 11, 1203, 575], "temperature": 0.0, "avg_logprob": -0.11974849150731014, "compression_ratio": 1.6491228070175439, "no_speech_prob": 1.7501790352980606e-05}, {"id": 168, "seek": 112776, "start": 1150.24, "end": 1154.8799999999999, "text": " a show batch. So, one of the nice things is it makes it very easy for you to look at your data,", "tokens": [257, 855, 15245, 13, 407, 11, 472, 295, 264, 1481, 721, 307, 309, 1669, 309, 588, 1858, 337, 291, 281, 574, 412, 428, 1412, 11], "temperature": 0.0, "avg_logprob": -0.11974849150731014, "compression_ratio": 1.6491228070175439, "no_speech_prob": 1.7501790352980606e-05}, {"id": 169, "seek": 115488, "start": 1154.88, "end": 1160.88, "text": " regardless of whether it's tabular or collaborative filtering or vision or text or even audio. If it", "tokens": [10060, 295, 1968, 309, 311, 4421, 1040, 420, 16555, 30822, 420, 5201, 420, 2487, 420, 754, 6278, 13, 759, 309], "temperature": 0.0, "avg_logprob": -0.14295858266402264, "compression_ratio": 1.6083333333333334, "no_speech_prob": 4.637717211153358e-06}, {"id": 170, "seek": 115488, "start": 1160.88, "end": 1170.4, "text": " was audio, it would show you a spectrogram and let you play the sound. So, you can do custom", "tokens": [390, 6278, 11, 309, 576, 855, 291, 257, 6177, 340, 1342, 293, 718, 291, 862, 264, 1626, 13, 407, 11, 291, 393, 360, 2375], "temperature": 0.0, "avg_logprob": -0.14295858266402264, "compression_ratio": 1.6083333333333334, "no_speech_prob": 4.637717211153358e-06}, {"id": 171, "seek": 115488, "start": 1170.4, "end": 1178.0, "text": " labeling with data blocks by using, for example, a regular expression labeler. You can get your", "tokens": [40244, 365, 1412, 8474, 538, 1228, 11, 337, 1365, 11, 257, 3890, 6114, 2715, 6185, 13, 509, 393, 483, 428], "temperature": 0.0, "avg_logprob": -0.14295858266402264, "compression_ratio": 1.6083333333333334, "no_speech_prob": 4.637717211153358e-06}, {"id": 172, "seek": 115488, "start": 1178.0, "end": 1184.0, "text": " labels from an external file or data frame, and they could be multi-labels. So, this thing here,", "tokens": [16949, 490, 364, 8320, 3991, 420, 1412, 3920, 11, 293, 436, 727, 312, 4825, 12, 44990, 1625, 13, 407, 11, 341, 551, 510, 11], "temperature": 0.0, "avg_logprob": -0.14295858266402264, "compression_ratio": 1.6083333333333334, "no_speech_prob": 4.637717211153358e-06}, {"id": 173, "seek": 118400, "start": 1184.0, "end": 1190.32, "text": " it's a multi-label classification task. So, it's automatically put a semicolon between each label.", "tokens": [309, 311, 257, 4825, 12, 75, 18657, 21538, 5633, 13, 407, 11, 309, 311, 6772, 829, 257, 27515, 38780, 1296, 1184, 7645, 13], "temperature": 0.0, "avg_logprob": -0.15339032048764437, "compression_ratio": 1.686131386861314, "no_speech_prob": 7.527630714321276e-06}, {"id": 174, "seek": 118400, "start": 1191.76, "end": 1195.68, "text": " Again, it's still basically just three lines of code to define the data block.", "tokens": [3764, 11, 309, 311, 920, 1936, 445, 1045, 3876, 295, 3089, 281, 6964, 264, 1412, 3461, 13], "temperature": 0.0, "avg_logprob": -0.15339032048764437, "compression_ratio": 1.686131386861314, "no_speech_prob": 7.527630714321276e-06}, {"id": 175, "seek": 118400, "start": 1197.2, "end": 1201.92, "text": " So, here's a data block for segmentation. And you can see, really, the only thing I had to change", "tokens": [407, 11, 510, 311, 257, 1412, 3461, 337, 9469, 399, 13, 400, 291, 393, 536, 11, 534, 11, 264, 787, 551, 286, 632, 281, 1319], "temperature": 0.0, "avg_logprob": -0.15339032048764437, "compression_ratio": 1.686131386861314, "no_speech_prob": 7.527630714321276e-06}, {"id": 176, "seek": 118400, "start": 1201.92, "end": 1209.6, "text": " here was that my dependent variable has been changed from category to pillow mask. And,", "tokens": [510, 390, 300, 452, 12334, 7006, 575, 668, 3105, 490, 7719, 281, 18581, 6094, 13, 400, 11], "temperature": 0.0, "avg_logprob": -0.15339032048764437, "compression_ratio": 1.686131386861314, "no_speech_prob": 7.527630714321276e-06}, {"id": 177, "seek": 120960, "start": 1209.6, "end": 1213.9199999999998, "text": " again, automatically, I show batch works, and we can train a model from that straight away as well.", "tokens": [797, 11, 6772, 11, 286, 855, 15245, 1985, 11, 293, 321, 393, 3847, 257, 2316, 490, 300, 2997, 1314, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12681979944210242, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.3419481547316536e-05}, {"id": 178, "seek": 120960, "start": 1215.6799999999998, "end": 1220.56, "text": " You could do key points. So, here, I've just changed my dependent variable to tensor point.", "tokens": [509, 727, 360, 2141, 2793, 13, 407, 11, 510, 11, 286, 600, 445, 3105, 452, 12334, 7006, 281, 40863, 935, 13], "temperature": 0.0, "avg_logprob": -0.12681979944210242, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.3419481547316536e-05}, {"id": 179, "seek": 120960, "start": 1220.56, "end": 1225.9199999999998, "text": " And so, now it knows how to behave with that. Object detection. So, now, I changed my dependent", "tokens": [400, 370, 11, 586, 309, 3255, 577, 281, 15158, 365, 300, 13, 24753, 17784, 13, 407, 11, 586, 11, 286, 3105, 452, 12334], "temperature": 0.0, "avg_logprob": -0.12681979944210242, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.3419481547316536e-05}, {"id": 180, "seek": 120960, "start": 1225.9199999999998, "end": 1235.36, "text": " variable to bounding box. And you can see I've got my bounding boxes here. Text. And so forth.", "tokens": [7006, 281, 5472, 278, 2424, 13, 400, 291, 393, 536, 286, 600, 658, 452, 5472, 278, 9002, 510, 13, 18643, 13, 400, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12681979944210242, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.3419481547316536e-05}, {"id": 181, "seek": 123536, "start": 1235.36, "end": 1239.52, "text": " So, actually, going back, I have a couple questions if it's okay to...", "tokens": [407, 11, 767, 11, 516, 646, 11, 286, 362, 257, 1916, 1651, 498, 309, 311, 1392, 281, 485], "temperature": 0.0, "avg_logprob": -0.20954119148900954, "compression_ratio": 1.5214007782101167, "no_speech_prob": 5.4757179896114394e-05}, {"id": 182, "seek": 123536, "start": 1239.52, "end": 1240.32, "text": " Anytime.", "tokens": [39401, 13], "temperature": 0.0, "avg_logprob": -0.20954119148900954, "compression_ratio": 1.5214007782101167, "no_speech_prob": 5.4757179896114394e-05}, {"id": 183, "seek": 123536, "start": 1240.32, "end": 1248.24, "text": " Yeah. So, the code, you've got sort of the Xs and Ys. And these both,", "tokens": [865, 13, 407, 11, 264, 3089, 11, 291, 600, 658, 1333, 295, 264, 1783, 82, 293, 398, 82, 13, 400, 613, 1293, 11], "temperature": 0.0, "avg_logprob": -0.20954119148900954, "compression_ratio": 1.5214007782101167, "no_speech_prob": 5.4757179896114394e-05}, {"id": 184, "seek": 123536, "start": 1248.9599999999998, "end": 1252.32, "text": " sounds like these different data types roughly conform to a protocol.", "tokens": [3263, 411, 613, 819, 1412, 3467, 9810, 18975, 281, 257, 10336, 13], "temperature": 0.0, "avg_logprob": -0.20954119148900954, "compression_ratio": 1.5214007782101167, "no_speech_prob": 5.4757179896114394e-05}, {"id": 185, "seek": 123536, "start": 1253.36, "end": 1255.4399999999998, "text": " Yep. We're going to get to that in a moment. Absolutely.", "tokens": [7010, 13, 492, 434, 516, 281, 483, 281, 300, 294, 257, 1623, 13, 7021, 13], "temperature": 0.0, "avg_logprob": -0.20954119148900954, "compression_ratio": 1.5214007782101167, "no_speech_prob": 5.4757179896114394e-05}, {"id": 186, "seek": 123536, "start": 1255.4399999999998, "end": 1256.08, "text": " Okay. Fantastic.", "tokens": [1033, 13, 21320, 13], "temperature": 0.0, "avg_logprob": -0.20954119148900954, "compression_ratio": 1.5214007782101167, "no_speech_prob": 5.4757179896114394e-05}, {"id": 187, "seek": 123536, "start": 1256.08, "end": 1261.52, "text": " It's well-plotted. Yep. That's an excellent way to think of it. And, actually, this is the way it", "tokens": [467, 311, 731, 12, 564, 11252, 13, 7010, 13, 663, 311, 364, 7103, 636, 281, 519, 295, 309, 13, 400, 11, 767, 11, 341, 307, 264, 636, 309], "temperature": 0.0, "avg_logprob": -0.20954119148900954, "compression_ratio": 1.5214007782101167, "no_speech_prob": 5.4757179896114394e-05}, {"id": 188, "seek": 126152, "start": 1261.52, "end": 1269.28, "text": " looked about three weeks ago. Now it looks even more like a protocol. So, yes, this is where it", "tokens": [2956, 466, 1045, 3259, 2057, 13, 823, 309, 1542, 754, 544, 411, 257, 10336, 13, 407, 11, 2086, 11, 341, 307, 689, 309], "temperature": 0.0, "avg_logprob": -0.11149661540985108, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.392056012467947e-05}, {"id": 189, "seek": 126152, "start": 1269.28, "end": 1273.92, "text": " all comes from, which is the foundation APIs. And this is the bit that I think is the most relevant", "tokens": [439, 1487, 490, 11, 597, 307, 264, 7030, 21445, 13, 400, 341, 307, 264, 857, 300, 286, 519, 307, 264, 881, 7340], "temperature": 0.0, "avg_logprob": -0.11149661540985108, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.392056012467947e-05}, {"id": 190, "seek": 126152, "start": 1273.92, "end": 1282.8, "text": " to Swift. And a lot of this, I think, would be a lot easier to write in Swift. So, the first thing", "tokens": [281, 25539, 13, 400, 257, 688, 295, 341, 11, 286, 519, 11, 576, 312, 257, 688, 3571, 281, 2464, 294, 25539, 13, 407, 11, 264, 700, 551], "temperature": 0.0, "avg_logprob": -0.11149661540985108, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.392056012467947e-05}, {"id": 191, "seek": 128280, "start": 1282.8, "end": 1291.68, "text": " that we added to PyTorch was object-oriented tenses. For too long, we've all been satisfied", "tokens": [300, 321, 3869, 281, 9953, 51, 284, 339, 390, 2657, 12, 27414, 256, 9085, 13, 1171, 886, 938, 11, 321, 600, 439, 668, 11239], "temperature": 0.0, "avg_logprob": -0.08331038246692067, "compression_ratio": 1.478494623655914, "no_speech_prob": 9.515823876427021e-06}, {"id": 192, "seek": 128280, "start": 1291.68, "end": 1299.68, "text": " with a data type called tensor, which has no semantics to it. And so, those tenses actually", "tokens": [365, 257, 1412, 2010, 1219, 40863, 11, 597, 575, 572, 4361, 45298, 281, 309, 13, 400, 370, 11, 729, 256, 9085, 767], "temperature": 0.0, "avg_logprob": -0.08331038246692067, "compression_ratio": 1.478494623655914, "no_speech_prob": 9.515823876427021e-06}, {"id": 193, "seek": 128280, "start": 1299.68, "end": 1307.04, "text": " represent something like a sentence or a picture of a cat or a recording of somebody saying", "tokens": [2906, 746, 411, 257, 8174, 420, 257, 3036, 295, 257, 3857, 420, 257, 6613, 295, 2618, 1566], "temperature": 0.0, "avg_logprob": -0.08331038246692067, "compression_ratio": 1.478494623655914, "no_speech_prob": 9.515823876427021e-06}, {"id": 194, "seek": 130704, "start": 1307.04, "end": 1315.36, "text": " something. So, why can't I take one of those tenses and say dot flip or dot rotate or dot resample", "tokens": [746, 13, 407, 11, 983, 393, 380, 286, 747, 472, 295, 729, 256, 9085, 293, 584, 5893, 7929, 420, 5893, 13121, 420, 5893, 725, 335, 781], "temperature": 0.0, "avg_logprob": -0.08654586280264505, "compression_ratio": 1.6214689265536724, "no_speech_prob": 8.800203431746922e-06}, {"id": 195, "seek": 130704, "start": 1315.36, "end": 1322.6399999999999, "text": " or dot translate to German? Well, the answer is you can't, because it's just a tensor without a", "tokens": [420, 5893, 13799, 281, 6521, 30, 1042, 11, 264, 1867, 307, 291, 393, 380, 11, 570, 309, 311, 445, 257, 40863, 1553, 257], "temperature": 0.0, "avg_logprob": -0.08654586280264505, "compression_ratio": 1.6214689265536724, "no_speech_prob": 8.800203431746922e-06}, {"id": 196, "seek": 130704, "start": 1322.6399999999999, "end": 1331.84, "text": " type. So, we have added types to tensors. So, you can now have a tensor image, tensor point,", "tokens": [2010, 13, 407, 11, 321, 362, 3869, 3467, 281, 10688, 830, 13, 407, 11, 291, 393, 586, 362, 257, 40863, 3256, 11, 40863, 935, 11], "temperature": 0.0, "avg_logprob": -0.08654586280264505, "compression_ratio": 1.6214689265536724, "no_speech_prob": 8.800203431746922e-06}, {"id": 197, "seek": 133184, "start": 1331.84, "end": 1337.52, "text": " tensor bounding box, and you can define a flip left, right for each. And so, this is some of", "tokens": [40863, 5472, 278, 2424, 11, 293, 291, 393, 6964, 257, 7929, 1411, 11, 558, 337, 1184, 13, 400, 370, 11, 341, 307, 512, 295], "temperature": 0.0, "avg_logprob": -0.12510815206563697, "compression_ratio": 1.738532110091743, "no_speech_prob": 4.157198873144807e-06}, {"id": 198, "seek": 133184, "start": 1337.52, "end": 1342.32, "text": " the source code from we've written our own computer vision library. So, that now you can say", "tokens": [264, 4009, 3089, 490, 321, 600, 3720, 527, 1065, 3820, 5201, 6405, 13, 407, 11, 300, 586, 291, 393, 584], "temperature": 0.0, "avg_logprob": -0.12510815206563697, "compression_ratio": 1.738532110091743, "no_speech_prob": 4.157198873144807e-06}, {"id": 199, "seek": 133184, "start": 1343.1999999999998, "end": 1350.6399999999999, "text": " flip LR, and it flips the puppy. And if it was a key points, it would flip the key points. If it", "tokens": [7929, 441, 49, 11, 293, 309, 40249, 264, 18196, 13, 400, 498, 309, 390, 257, 2141, 2793, 11, 309, 576, 7929, 264, 2141, 2793, 13, 759, 309], "temperature": 0.0, "avg_logprob": -0.12510815206563697, "compression_ratio": 1.738532110091743, "no_speech_prob": 4.157198873144807e-06}, {"id": 200, "seek": 133184, "start": 1350.6399999999999, "end": 1356.6399999999999, "text": " was a bounding box, it would flip the bounding boxes and so forth. So, this is an example of how", "tokens": [390, 257, 5472, 278, 2424, 11, 309, 576, 7929, 264, 5472, 278, 9002, 293, 370, 5220, 13, 407, 11, 341, 307, 364, 1365, 295, 577], "temperature": 0.0, "avg_logprob": -0.12510815206563697, "compression_ratio": 1.738532110091743, "no_speech_prob": 4.157198873144807e-06}, {"id": 201, "seek": 135664, "start": 1356.64, "end": 1361.2800000000002, "text": " tensors which carry around semantics are nice. It's also nice that I didn't just say dot show,", "tokens": [10688, 830, 597, 3985, 926, 4361, 45298, 366, 1481, 13, 467, 311, 611, 1481, 300, 286, 994, 380, 445, 584, 5893, 855, 11], "temperature": 0.0, "avg_logprob": -0.16539213180541992, "compression_ratio": 1.7194570135746607, "no_speech_prob": 9.972240150091238e-06}, {"id": 202, "seek": 135664, "start": 1361.92, "end": 1369.44, "text": " right? So, dot show is something that's defined for all fast AIV to tensor types, and it will just", "tokens": [558, 30, 407, 11, 5893, 855, 307, 746, 300, 311, 7642, 337, 439, 2370, 7318, 53, 281, 40863, 3467, 11, 293, 309, 486, 445], "temperature": 0.0, "avg_logprob": -0.16539213180541992, "compression_ratio": 1.7194570135746607, "no_speech_prob": 9.972240150091238e-06}, {"id": 203, "seek": 135664, "start": 1369.44, "end": 1376.72, "text": " display that tensor. It could even be a tuple containing a tensor and some bounding boxes and", "tokens": [4674, 300, 40863, 13, 467, 727, 754, 312, 257, 2604, 781, 19273, 257, 40863, 293, 512, 5472, 278, 9002, 293], "temperature": 0.0, "avg_logprob": -0.16539213180541992, "compression_ratio": 1.7194570135746607, "no_speech_prob": 9.972240150091238e-06}, {"id": 204, "seek": 135664, "start": 1376.72, "end": 1382.48, "text": " some bounding box classes. Whatever it is, it will be able to display it. It will be able to", "tokens": [512, 5472, 278, 2424, 5359, 13, 8541, 309, 307, 11, 309, 486, 312, 1075, 281, 4674, 309, 13, 467, 486, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.16539213180541992, "compression_ratio": 1.7194570135746607, "no_speech_prob": 9.972240150091238e-06}, {"id": 205, "seek": 138248, "start": 1382.48, "end": 1393.28, "text": " convert it into batches for modeling and so forth. So, you know, with that, we can now create, for", "tokens": [7620, 309, 666, 15245, 279, 337, 15983, 293, 370, 5220, 13, 407, 11, 291, 458, 11, 365, 300, 11, 321, 393, 586, 1884, 11, 337], "temperature": 0.0, "avg_logprob": -0.14350004599127972, "compression_ratio": 1.633879781420765, "no_speech_prob": 8.664275810588151e-06}, {"id": 206, "seek": 138248, "start": 1393.28, "end": 1400.0, "text": " example, a random transformation called flip item, and we can say that the encoding of that random", "tokens": [1365, 11, 257, 4974, 9887, 1219, 7929, 3174, 11, 293, 321, 393, 584, 300, 264, 43430, 295, 300, 4974], "temperature": 0.0, "avg_logprob": -0.14350004599127972, "compression_ratio": 1.633879781420765, "no_speech_prob": 8.664275810588151e-06}, {"id": 207, "seek": 138248, "start": 1400.0, "end": 1408.0, "text": " transformation is defined for a pillow image or any tensor type. And in each case, the implementation", "tokens": [9887, 307, 7642, 337, 257, 18581, 3256, 420, 604, 40863, 2010, 13, 400, 294, 1184, 1389, 11, 264, 11420], "temperature": 0.0, "avg_logprob": -0.14350004599127972, "compression_ratio": 1.633879781420765, "no_speech_prob": 8.664275810588151e-06}, {"id": 208, "seek": 140800, "start": 1408.0, "end": 1414.96, "text": " is simply to call x dot flip LR. Or we could do the dihedral symmetry transforms in the same way.", "tokens": [307, 2935, 281, 818, 2031, 5893, 7929, 441, 49, 13, 1610, 321, 727, 360, 264, 1026, 71, 24764, 25440, 35592, 294, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.16182355880737304, "compression_ratio": 1.6304347826086956, "no_speech_prob": 2.1781526811537333e-05}, {"id": 209, "seek": 140800, "start": 1416.24, "end": 1421.84, "text": " Before we call, grab a random number between zero and seven to decide which of the eight", "tokens": [4546, 321, 818, 11, 4444, 257, 4974, 1230, 1296, 4018, 293, 3407, 281, 4536, 597, 295, 264, 3180], "temperature": 0.0, "avg_logprob": -0.16182355880737304, "compression_ratio": 1.6304347826086956, "no_speech_prob": 2.1781526811537333e-05}, {"id": 210, "seek": 140800, "start": 1424.08, "end": 1430.32, "text": " transposes to do, and then encodes call x dot what's dihedral with that thing we just got.", "tokens": [7132, 4201, 281, 360, 11, 293, 550, 2058, 4789, 818, 2031, 5893, 437, 311, 1026, 71, 24764, 365, 300, 551, 321, 445, 658, 13], "temperature": 0.0, "avg_logprob": -0.16182355880737304, "compression_ratio": 1.6304347826086956, "no_speech_prob": 2.1781526811537333e-05}, {"id": 211, "seek": 140800, "start": 1431.2, "end": 1437.2, "text": " And so, now we can call that transform a bunch of times, and each time we'll get back a different", "tokens": [400, 370, 11, 586, 321, 393, 818, 300, 4088, 257, 3840, 295, 1413, 11, 293, 1184, 565, 321, 603, 483, 646, 257, 819], "temperature": 0.0, "avg_logprob": -0.16182355880737304, "compression_ratio": 1.6304347826086956, "no_speech_prob": 2.1781526811537333e-05}, {"id": 212, "seek": 143720, "start": 1437.2, "end": 1443.2, "text": " random augmentation. So, a lot of these things become nice and easy. Hey, Jeremy, some Maxam", "tokens": [4974, 14501, 19631, 13, 407, 11, 257, 688, 295, 613, 721, 1813, 1481, 293, 1858, 13, 1911, 11, 17809, 11, 512, 7402, 335], "temperature": 0.0, "avg_logprob": -0.17936759299420296, "compression_ratio": 1.6650485436893203, "no_speech_prob": 2.8854647098341957e-05}, {"id": 213, "seek": 143720, "start": 1443.2, "end": 1446.32, "text": " asked, why isn't tensor a backing data structure for an image type?", "tokens": [2351, 11, 983, 1943, 380, 40863, 257, 19373, 1412, 3877, 337, 364, 3256, 2010, 30], "temperature": 0.0, "avg_logprob": -0.17936759299420296, "compression_ratio": 1.6650485436893203, "no_speech_prob": 2.8854647098341957e-05}, {"id": 214, "seek": 143720, "start": 1450.16, "end": 1457.44, "text": " A tensor image is a tensor which is an image type. Why isn't, he says, why isn't tensor a backing,", "tokens": [316, 40863, 3256, 307, 257, 40863, 597, 307, 364, 3256, 2010, 13, 1545, 1943, 380, 11, 415, 1619, 11, 983, 1943, 380, 40863, 257, 19373, 11], "temperature": 0.0, "avg_logprob": -0.17936759299420296, "compression_ratio": 1.6650485436893203, "no_speech_prob": 2.8854647098341957e-05}, {"id": 215, "seek": 143720, "start": 1458.16, "end": 1461.92, "text": " why not have a different type named image, I guess, that has a tensor inside of it?", "tokens": [983, 406, 362, 257, 819, 2010, 4926, 3256, 11, 286, 2041, 11, 300, 575, 257, 40863, 1854, 295, 309, 30], "temperature": 0.0, "avg_logprob": -0.17936759299420296, "compression_ratio": 1.6650485436893203, "no_speech_prob": 2.8854647098341957e-05}, {"id": 216, "seek": 146192, "start": 1461.92, "end": 1469.92, "text": " Do you mean why inherit rather than compose? Apparently, yes, that. Yeah.", "tokens": [1144, 291, 914, 983, 21389, 2831, 813, 35925, 30, 16755, 11, 2086, 11, 300, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.1080503138628873, "compression_ratio": 1.694300518134715, "no_speech_prob": 1.777655415935442e-05}, {"id": 217, "seek": 146192, "start": 1471.3600000000001, "end": 1477.68, "text": " So, inheritance, I mean, you can do both, and you can create identical APIs.", "tokens": [407, 11, 32122, 11, 286, 914, 11, 291, 393, 360, 1293, 11, 293, 291, 393, 1884, 14800, 21445, 13], "temperature": 0.0, "avg_logprob": -0.1080503138628873, "compression_ratio": 1.694300518134715, "no_speech_prob": 1.777655415935442e-05}, {"id": 218, "seek": 146192, "start": 1478.48, "end": 1482.5600000000002, "text": " Inheritance just has the benefit that all the normal stuff you can do with a tensor,", "tokens": [682, 511, 270, 719, 445, 575, 264, 5121, 300, 439, 264, 2710, 1507, 291, 393, 360, 365, 257, 40863, 11], "temperature": 0.0, "avg_logprob": -0.1080503138628873, "compression_ratio": 1.694300518134715, "no_speech_prob": 1.777655415935442e-05}, {"id": 219, "seek": 146192, "start": 1482.5600000000002, "end": 1486.48, "text": " you can do with a tensor that happens to be an image. So, just because a tensor is an image", "tokens": [291, 393, 360, 365, 257, 40863, 300, 2314, 281, 312, 364, 3256, 13, 407, 11, 445, 570, 257, 40863, 307, 364, 3256], "temperature": 0.0, "avg_logprob": -0.1080503138628873, "compression_ratio": 1.694300518134715, "no_speech_prob": 1.777655415935442e-05}, {"id": 220, "seek": 148648, "start": 1486.48, "end": 1493.04, "text": " doesn't mean you now don't want to be able to do fancy indexing to it or do an LUD composition of", "tokens": [1177, 380, 914, 291, 586, 500, 380, 528, 281, 312, 1075, 281, 360, 10247, 8186, 278, 281, 309, 420, 360, 364, 441, 9438, 12686, 295], "temperature": 0.0, "avg_logprob": -0.09099475439492759, "compression_ratio": 1.5392670157068062, "no_speech_prob": 6.2407325458480045e-06}, {"id": 221, "seek": 148648, "start": 1493.04, "end": 1503.76, "text": " it or stack it with other tensors across that axis. So, basically, a tensor image ought to have all", "tokens": [309, 420, 8630, 309, 365, 661, 10688, 830, 2108, 300, 10298, 13, 407, 11, 1936, 11, 257, 40863, 3256, 13416, 281, 362, 439], "temperature": 0.0, "avg_logprob": -0.09099475439492759, "compression_ratio": 1.5392670157068062, "no_speech_prob": 6.2407325458480045e-06}, {"id": 222, "seek": 148648, "start": 1503.76, "end": 1510.0, "text": " the behavior of a tensor plus additional behavior. So, that's why we used inheritance. We have a", "tokens": [264, 5223, 295, 257, 40863, 1804, 4497, 5223, 13, 407, 11, 300, 311, 983, 321, 1143, 32122, 13, 492, 362, 257], "temperature": 0.0, "avg_logprob": -0.09099475439492759, "compression_ratio": 1.5392670157068062, "no_speech_prob": 6.2407325458480045e-06}, {"id": 223, "seek": 151000, "start": 1510.0, "end": 1517.84, "text": " version that uses composition as well and it uses Python's nice get atra functionality to pass on", "tokens": [3037, 300, 4960, 12686, 382, 731, 293, 309, 4960, 15329, 311, 1481, 483, 412, 424, 14980, 281, 1320, 322], "temperature": 0.0, "avg_logprob": -0.16019802093505858, "compression_ratio": 1.6324786324786325, "no_speech_prob": 1.5205495401460212e-05}, {"id": 224, "seek": 151000, "start": 1517.84, "end": 1526.56, "text": " all of the behavior of tensor, but it comes out more nicely in Python when you do inheritance.", "tokens": [439, 295, 264, 5223, 295, 40863, 11, 457, 309, 1487, 484, 544, 9594, 294, 15329, 562, 291, 360, 32122, 13], "temperature": 0.0, "avg_logprob": -0.16019802093505858, "compression_ratio": 1.6324786324786325, "no_speech_prob": 1.5205495401460212e-05}, {"id": 225, "seek": 151000, "start": 1527.12, "end": 1535.04, "text": " And, actually, the PyTorch team has decided to officially implement semantic tensor subtypes now.", "tokens": [400, 11, 767, 11, 264, 9953, 51, 284, 339, 1469, 575, 3047, 281, 12053, 4445, 47982, 40863, 1422, 874, 5190, 586, 13], "temperature": 0.0, "avg_logprob": -0.16019802093505858, "compression_ratio": 1.6324786324786325, "no_speech_prob": 1.5205495401460212e-05}, {"id": 226, "seek": 151000, "start": 1535.04, "end": 1539.84, "text": " And so, hopefully, in the next version of PyTorch, you won't have to use the extremely ugly", "tokens": [400, 370, 11, 4696, 11, 294, 264, 958, 3037, 295, 9953, 51, 284, 339, 11, 291, 1582, 380, 362, 281, 764, 264, 4664, 12246], "temperature": 0.0, "avg_logprob": -0.16019802093505858, "compression_ratio": 1.6324786324786325, "no_speech_prob": 1.5205495401460212e-05}, {"id": 227, "seek": 153984, "start": 1539.84, "end": 1546.3999999999999, "text": " hacks that we had to use to make this work. You'll be able to use the real ones. And,", "tokens": [33617, 300, 321, 632, 281, 764, 281, 652, 341, 589, 13, 509, 603, 312, 1075, 281, 764, 264, 957, 2306, 13, 400, 11], "temperature": 0.0, "avg_logprob": -0.17181972581513075, "compression_ratio": 1.568888888888889, "no_speech_prob": 3.704640766954981e-05}, {"id": 228, "seek": 153984, "start": 1546.3999999999999, "end": 1550.56, "text": " hopefully, you'll see in TorchVision some of these ideas will be brought over there.", "tokens": [4696, 11, 291, 603, 536, 294, 7160, 339, 53, 1991, 512, 295, 613, 3487, 486, 312, 3038, 670, 456, 13], "temperature": 0.0, "avg_logprob": -0.17181972581513075, "compression_ratio": 1.568888888888889, "no_speech_prob": 3.704640766954981e-05}, {"id": 229, "seek": 153984, "start": 1551.6799999999998, "end": 1557.36, "text": " Can I ask you, so, how does that the type propagate? So, if you do arithmetic on an", "tokens": [1664, 286, 1029, 291, 11, 370, 11, 577, 775, 300, 264, 2010, 48256, 30, 407, 11, 498, 291, 360, 42973, 322, 364], "temperature": 0.0, "avg_logprob": -0.17181972581513075, "compression_ratio": 1.568888888888889, "no_speech_prob": 3.704640766954981e-05}, {"id": 230, "seek": 153984, "start": 1557.36, "end": 1564.56, "text": " image tensor, do you get an image tensor back? So, Chris and I had a conversation about this a few", "tokens": [3256, 40863, 11, 360, 291, 483, 364, 3256, 40863, 646, 30, 407, 11, 6688, 293, 286, 632, 257, 3761, 466, 341, 257, 1326], "temperature": 0.0, "avg_logprob": -0.17181972581513075, "compression_ratio": 1.568888888888889, "no_speech_prob": 3.704640766954981e-05}, {"id": 231, "seek": 156456, "start": 1564.56, "end": 1571.28, "text": " months ago and I said I'm banging my head around this issue of types not carrying around their", "tokens": [2493, 2057, 293, 286, 848, 286, 478, 36982, 452, 1378, 926, 341, 2734, 295, 3467, 406, 9792, 926, 641], "temperature": 0.0, "avg_logprob": -0.1483245224788271, "compression_ratio": 1.690391459074733, "no_speech_prob": 2.2122307200334035e-05}, {"id": 232, "seek": 156456, "start": 1571.28, "end": 1575.6799999999998, "text": " behavior. And Chris casually mentioned, oh, yes, that thing is called higher kind of types.", "tokens": [5223, 13, 400, 6688, 34872, 2835, 11, 1954, 11, 2086, 11, 300, 551, 307, 1219, 2946, 733, 295, 3467, 13], "temperature": 0.0, "avg_logprob": -0.1483245224788271, "compression_ratio": 1.690391459074733, "no_speech_prob": 2.2122307200334035e-05}, {"id": 233, "seek": 156456, "start": 1576.32, "end": 1581.2, "text": " So, I went home and that was one of these phrases that I thought only functional programming", "tokens": [407, 11, 286, 1437, 1280, 293, 300, 390, 472, 295, 613, 20312, 300, 286, 1194, 787, 11745, 9410], "temperature": 0.0, "avg_logprob": -0.1483245224788271, "compression_ratio": 1.690391459074733, "no_speech_prob": 2.2122307200334035e-05}, {"id": 234, "seek": 156456, "start": 1581.2, "end": 1588.0, "text": " dweebs talked about and I would never care about. It actually matters a lot. And it's basically the", "tokens": [274, 826, 68, 929, 2825, 466, 293, 286, 576, 1128, 1127, 466, 13, 467, 767, 7001, 257, 688, 13, 400, 309, 311, 1936, 264], "temperature": 0.0, "avg_logprob": -0.1483245224788271, "compression_ratio": 1.690391459074733, "no_speech_prob": 2.2122307200334035e-05}, {"id": 235, "seek": 156456, "start": 1588.0, "end": 1593.9199999999998, "text": " idea that if you have a tensor image and you add one to it, you want to get back a tensor image", "tokens": [1558, 300, 498, 291, 362, 257, 40863, 3256, 293, 291, 909, 472, 281, 309, 11, 291, 528, 281, 483, 646, 257, 40863, 3256], "temperature": 0.0, "avg_logprob": -0.1483245224788271, "compression_ratio": 1.690391459074733, "no_speech_prob": 2.2122307200334035e-05}, {"id": 236, "seek": 159392, "start": 1593.92, "end": 1597.92, "text": " because it should be an image that's a bit brighter rather than something that loses its type.", "tokens": [570, 309, 820, 312, 364, 3256, 300, 311, 257, 857, 19764, 2831, 813, 746, 300, 18293, 1080, 2010, 13], "temperature": 0.0, "avg_logprob": -0.15133510727480234, "compression_ratio": 1.5693069306930694, "no_speech_prob": 3.943428600905463e-05}, {"id": 237, "seek": 159392, "start": 1599.04, "end": 1606.24, "text": " So, we implemented our own, again, hacky partial higher kind of type implementation in", "tokens": [407, 11, 321, 12270, 527, 1065, 11, 797, 11, 10339, 88, 14641, 2946, 733, 295, 2010, 11420, 294], "temperature": 0.0, "avg_logprob": -0.15133510727480234, "compression_ratio": 1.5693069306930694, "no_speech_prob": 3.943428600905463e-05}, {"id": 238, "seek": 159392, "start": 1606.24, "end": 1613.8400000000001, "text": " fast.aiv2. So, any of these things that you do to a tensor of a subtype, you will", "tokens": [2370, 13, 1301, 85, 17, 13, 407, 11, 604, 295, 613, 721, 300, 291, 360, 281, 257, 40863, 295, 257, 1422, 20467, 11, 291, 486], "temperature": 0.0, "avg_logprob": -0.15133510727480234, "compression_ratio": 1.5693069306930694, "no_speech_prob": 3.943428600905463e-05}, {"id": 239, "seek": 159392, "start": 1615.04, "end": 1618.0, "text": " nearly always get back the correctly subtyped tensor.", "tokens": [6217, 1009, 483, 646, 264, 8944, 1422, 874, 3452, 40863, 13], "temperature": 0.0, "avg_logprob": -0.15133510727480234, "compression_ratio": 1.5693069306930694, "no_speech_prob": 3.943428600905463e-05}, {"id": 240, "seek": 161800, "start": 1618.0, "end": 1625.92, "text": " Yeah, I mean, I saw that PyTorch recently started talking about their named indexing extensions for", "tokens": [865, 11, 286, 914, 11, 286, 1866, 300, 9953, 51, 284, 339, 3938, 1409, 1417, 466, 641, 4926, 8186, 278, 25129, 337], "temperature": 0.0, "avg_logprob": -0.22354110630079246, "compression_ratio": 1.6255506607929515, "no_speech_prob": 6.30210415693e-05}, {"id": 241, "seek": 161800, "start": 1625.92, "end": 1630.32, "text": " their tensors as well. And I see they have a similar kind of challenge there where when you", "tokens": [641, 10688, 830, 382, 731, 13, 400, 286, 536, 436, 362, 257, 2531, 733, 295, 3430, 456, 689, 562, 291], "temperature": 0.0, "avg_logprob": -0.22354110630079246, "compression_ratio": 1.6255506607929515, "no_speech_prob": 6.30210415693e-05}, {"id": 242, "seek": 161800, "start": 1630.88, "end": 1635.76, "text": " start doing arithmetic and other things like that on a tensor that has named dimensions,", "tokens": [722, 884, 42973, 293, 661, 721, 411, 300, 322, 257, 40863, 300, 575, 4926, 12819, 11], "temperature": 0.0, "avg_logprob": -0.22354110630079246, "compression_ratio": 1.6255506607929515, "no_speech_prob": 6.30210415693e-05}, {"id": 243, "seek": 161800, "start": 1635.76, "end": 1642.72, "text": " you want to propagate those along. Yeah, so we haven't started using that yet because it", "tokens": [291, 528, 281, 48256, 729, 2051, 13, 865, 11, 370, 321, 2378, 380, 1409, 1228, 300, 1939, 570, 309], "temperature": 0.0, "avg_logprob": -0.22354110630079246, "compression_ratio": 1.6255506607929515, "no_speech_prob": 6.30210415693e-05}, {"id": 244, "seek": 164272, "start": 1642.72, "end": 1650.8, "text": " hasn't quite landed its table. But yeah, we talked to the PyTorch team at the DevCon and", "tokens": [6132, 380, 1596, 15336, 1080, 3199, 13, 583, 1338, 11, 321, 2825, 281, 264, 9953, 51, 284, 339, 1469, 412, 264, 9096, 9838, 293], "temperature": 0.0, "avg_logprob": -0.20649678779370856, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.48010167828761e-05}, {"id": 245, "seek": 164272, "start": 1650.8, "end": 1658.0, "text": " we certainly are planning to bring these ideas together. They're all Falcon Orbit related concerns.", "tokens": [321, 3297, 366, 5038, 281, 1565, 613, 3487, 1214, 13, 814, 434, 439, 31801, 1610, 5260, 4077, 7389, 13], "temperature": 0.0, "avg_logprob": -0.20649678779370856, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.48010167828761e-05}, {"id": 246, "seek": 164272, "start": 1658.0, "end": 1663.04, "text": " Yeah, I just mean that I assume that that feature has the same problem, the same challenge.", "tokens": [865, 11, 286, 445, 914, 300, 286, 6552, 300, 300, 4111, 575, 264, 912, 1154, 11, 264, 912, 3430, 13], "temperature": 0.0, "avg_logprob": -0.20649678779370856, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.48010167828761e-05}, {"id": 247, "seek": 164272, "start": 1663.04, "end": 1664.0, "text": " I assume so, yeah.", "tokens": [286, 6552, 370, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.20649678779370856, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.48010167828761e-05}, {"id": 248, "seek": 164272, "start": 1665.28, "end": 1666.8, "text": " It would be interesting to see what they do.", "tokens": [467, 576, 312, 1880, 281, 536, 437, 436, 360, 13], "temperature": 0.0, "avg_logprob": -0.20649678779370856, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.48010167828761e-05}, {"id": 249, "seek": 166680, "start": 1666.8, "end": 1676.72, "text": " Yeah, yeah, it would. Yeah, so, you know, it's kind of nice. Not only do we get to be able to say", "tokens": [865, 11, 1338, 11, 309, 576, 13, 865, 11, 370, 11, 291, 458, 11, 309, 311, 733, 295, 1481, 13, 1726, 787, 360, 321, 483, 281, 312, 1075, 281, 584], "temperature": 0.0, "avg_logprob": -0.11936768363503848, "compression_ratio": 1.7511520737327189, "no_speech_prob": 6.143346126918914e-06}, {"id": 250, "seek": 166680, "start": 1676.72, "end": 1683.9199999999998, "text": " dot show batch, but you can even go dot show results. And in this case, it knows what the", "tokens": [5893, 855, 15245, 11, 457, 291, 393, 754, 352, 5893, 855, 3542, 13, 400, 294, 341, 1389, 11, 309, 3255, 437, 264], "temperature": 0.0, "avg_logprob": -0.11936768363503848, "compression_ratio": 1.7511520737327189, "no_speech_prob": 6.143346126918914e-06}, {"id": 251, "seek": 166680, "start": 1683.9199999999998, "end": 1688.1599999999999, "text": " independent variables type is, it knows what the dependent variables type is, and it even", "tokens": [6695, 9102, 2010, 307, 11, 309, 3255, 437, 264, 12334, 9102, 2010, 307, 11, 293, 309, 754], "temperature": 0.0, "avg_logprob": -0.11936768363503848, "compression_ratio": 1.7511520737327189, "no_speech_prob": 6.143346126918914e-06}, {"id": 252, "seek": 166680, "start": 1688.1599999999999, "end": 1692.56, "text": " knows things like, hey, for a classification task, those two things should be the same. And if they're", "tokens": [3255, 721, 411, 11, 4177, 11, 337, 257, 21538, 5633, 11, 729, 732, 721, 820, 312, 264, 912, 13, 400, 498, 436, 434], "temperature": 0.0, "avg_logprob": -0.11936768363503848, "compression_ratio": 1.7511520737327189, "no_speech_prob": 6.143346126918914e-06}, {"id": 253, "seek": 169256, "start": 1692.56, "end": 1698.8, "text": " not, by default, I will highlight them in red. So these lower level foundations are the things that", "tokens": [406, 11, 538, 7576, 11, 286, 486, 5078, 552, 294, 2182, 13, 407, 613, 3126, 1496, 22467, 366, 264, 721, 300], "temperature": 0.0, "avg_logprob": -0.11756551530626085, "compression_ratio": 1.6336206896551724, "no_speech_prob": 7.295373507076874e-06}, {"id": 254, "seek": 169256, "start": 1698.8, "end": 1707.28, "text": " drive our ability to easily add this higher level functionality. So, you know, this is the kind of", "tokens": [3332, 527, 3485, 281, 3612, 909, 341, 2946, 1496, 14980, 13, 407, 11, 291, 458, 11, 341, 307, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.11756551530626085, "compression_ratio": 1.6336206896551724, "no_speech_prob": 7.295373507076874e-06}, {"id": 255, "seek": 169256, "start": 1707.28, "end": 1711.9199999999998, "text": " ugly stuff we wouldn't have to do in Swift. We had to write our own type dispatch system", "tokens": [12246, 1507, 321, 2759, 380, 362, 281, 360, 294, 25539, 13, 492, 632, 281, 2464, 527, 1065, 2010, 36729, 1185], "temperature": 0.0, "avg_logprob": -0.11756551530626085, "compression_ratio": 1.6336206896551724, "no_speech_prob": 7.295373507076874e-06}, {"id": 256, "seek": 169256, "start": 1713.76, "end": 1718.56, "text": " so that we can annotate things with types and those type annotations are actually semantic.", "tokens": [370, 300, 321, 393, 25339, 473, 721, 365, 3467, 293, 729, 2010, 25339, 763, 366, 767, 47982, 13], "temperature": 0.0, "avg_logprob": -0.11756551530626085, "compression_ratio": 1.6336206896551724, "no_speech_prob": 7.295373507076874e-06}, {"id": 257, "seek": 171856, "start": 1718.56, "end": 1726.24, "text": " And so we now have the joyfully modern idea of function overloading in Python,", "tokens": [400, 370, 321, 586, 362, 264, 6258, 2277, 4363, 1558, 295, 2445, 28777, 278, 294, 15329, 11], "temperature": 0.0, "avg_logprob": -0.14213287353515625, "compression_ratio": 1.4540816326530612, "no_speech_prob": 2.429073902021628e-05}, {"id": 258, "seek": 171856, "start": 1726.96, "end": 1729.36, "text": " which has made life a lot easier. And we already have that.", "tokens": [597, 575, 1027, 993, 257, 688, 3571, 13, 400, 321, 1217, 362, 300, 13], "temperature": 0.0, "avg_logprob": -0.14213287353515625, "compression_ratio": 1.4540816326530612, "no_speech_prob": 2.429073902021628e-05}, {"id": 259, "seek": 171856, "start": 1732.3999999999999, "end": 1735.2, "text": " Do you have many users that are using this yet?", "tokens": [1144, 291, 362, 867, 5022, 300, 366, 1228, 341, 1939, 30], "temperature": 0.0, "avg_logprob": -0.14213287353515625, "compression_ratio": 1.4540816326530612, "no_speech_prob": 2.429073902021628e-05}, {"id": 260, "seek": 171856, "start": 1736.72, "end": 1745.84, "text": " It's still pre-released. It's not even alpha. But there is an enthusiastic early adopter community", "tokens": [467, 311, 920, 659, 12, 265, 41087, 13, 467, 311, 406, 754, 8961, 13, 583, 456, 307, 364, 28574, 2440, 22486, 391, 1768], "temperature": 0.0, "avg_logprob": -0.14213287353515625, "compression_ratio": 1.4540816326530612, "no_speech_prob": 2.429073902021628e-05}, {"id": 261, "seek": 174584, "start": 1745.84, "end": 1752.9599999999998, "text": " who is using it. So, for example, the user contributed audio library has already been", "tokens": [567, 307, 1228, 309, 13, 407, 11, 337, 1365, 11, 264, 4195, 18434, 6278, 6405, 575, 1217, 668], "temperature": 0.0, "avg_logprob": -0.12426575136856294, "compression_ratio": 1.4574468085106382, "no_speech_prob": 1.6698704712325707e-05}, {"id": 262, "seek": 174584, "start": 1752.9599999999998, "end": 1758.8, "text": " ported to it. I've also built a medical imaging library on top of it and have written a series of", "tokens": [2436, 292, 281, 309, 13, 286, 600, 611, 3094, 257, 4625, 25036, 6405, 322, 1192, 295, 309, 293, 362, 3720, 257, 2638, 295], "temperature": 0.0, "avg_logprob": -0.12426575136856294, "compression_ratio": 1.4574468085106382, "no_speech_prob": 1.6698704712325707e-05}, {"id": 263, "seek": 174584, "start": 1758.8, "end": 1767.4399999999998, "text": " five notebooks showing how to do CT scan analysis with it. So it's kind of like, it works.", "tokens": [1732, 43782, 4099, 577, 281, 360, 19529, 11049, 5215, 365, 309, 13, 407, 309, 311, 733, 295, 411, 11, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.12426575136856294, "compression_ratio": 1.4574468085106382, "no_speech_prob": 1.6698704712325707e-05}, {"id": 264, "seek": 176744, "start": 1767.44, "end": 1776.72, "text": " I was curious what your users think of it because there's this very strongly held conception that", "tokens": [286, 390, 6369, 437, 428, 5022, 519, 295, 309, 570, 456, 311, 341, 588, 10613, 5167, 30698, 300], "temperature": 0.0, "avg_logprob": -0.1738323418490858, "compression_ratio": 1.5414847161572052, "no_speech_prob": 8.798946510069072e-06}, {"id": 265, "seek": 176744, "start": 1776.72, "end": 1782.16, "text": " Python folks hate types. And you're kind of providing a little bit of typing.", "tokens": [15329, 4024, 4700, 3467, 13, 400, 291, 434, 733, 295, 6530, 257, 707, 857, 295, 18444, 13], "temperature": 0.0, "avg_logprob": -0.1738323418490858, "compression_ratio": 1.5414847161572052, "no_speech_prob": 8.798946510069072e-06}, {"id": 266, "seek": 176744, "start": 1784.0, "end": 1788.56, "text": " I'm curious how they react to that. The extremely biased subset of early adopter", "tokens": [286, 478, 6369, 577, 436, 4515, 281, 300, 13, 440, 4664, 28035, 25993, 295, 2440, 22486, 391], "temperature": 0.0, "avg_logprob": -0.1738323418490858, "compression_ratio": 1.5414847161572052, "no_speech_prob": 8.798946510069072e-06}, {"id": 267, "seek": 176744, "start": 1789.28, "end": 1795.76, "text": " classed AI enthusiasts who are using it love it. And they tend to be people who have gone pretty", "tokens": [1508, 292, 7318, 45873, 567, 366, 1228, 309, 959, 309, 13, 400, 436, 3928, 281, 312, 561, 567, 362, 2780, 1238], "temperature": 0.0, "avg_logprob": -0.1738323418490858, "compression_ratio": 1.5414847161572052, "no_speech_prob": 8.798946510069072e-06}, {"id": 268, "seek": 179576, "start": 1795.76, "end": 1801.6, "text": " deep in the past. So, for example, my friend Andrew Shaw who wrote something called music autobot,", "tokens": [2452, 294, 264, 1791, 13, 407, 11, 337, 1365, 11, 452, 1277, 10110, 27132, 567, 4114, 746, 1219, 1318, 1476, 996, 310, 11], "temperature": 0.0, "avg_logprob": -0.14871868065425328, "compression_ratio": 1.8346456692913387, "no_speech_prob": 7.028316031210124e-05}, {"id": 269, "seek": 179576, "start": 1802.24, "end": 1806.32, "text": " which is one of the coolest things in the world in case you haven't seen it yet.", "tokens": [597, 307, 472, 295, 264, 22013, 721, 294, 264, 1002, 294, 1389, 291, 2378, 380, 1612, 309, 1939, 13], "temperature": 0.0, "avg_logprob": -0.14871868065425328, "compression_ratio": 1.8346456692913387, "no_speech_prob": 7.028316031210124e-05}, {"id": 270, "seek": 179576, "start": 1806.32, "end": 1811.76, "text": " Which is something where you can generate music using a neural network. You can put in some", "tokens": [3013, 307, 746, 689, 291, 393, 8460, 1318, 1228, 257, 18161, 3209, 13, 509, 393, 829, 294, 512], "temperature": 0.0, "avg_logprob": -0.14871868065425328, "compression_ratio": 1.8346456692913387, "no_speech_prob": 7.028316031210124e-05}, {"id": 271, "seek": 179576, "start": 1811.76, "end": 1816.8799999999999, "text": " melodies and some chords. And it will autocomplete some additional melodies and chords. Or you can", "tokens": [47085, 293, 512, 21733, 13, 400, 309, 486, 45833, 298, 17220, 512, 4497, 47085, 293, 21733, 13, 1610, 291, 393], "temperature": 0.0, "avg_logprob": -0.14871868065425328, "compression_ratio": 1.8346456692913387, "no_speech_prob": 7.028316031210124e-05}, {"id": 272, "seek": 179576, "start": 1816.8799999999999, "end": 1823.92, "text": " put in a melody and it will automatically add chords. Or you can add chords that create melody.", "tokens": [829, 294, 257, 17997, 293, 309, 486, 6772, 909, 21733, 13, 1610, 291, 393, 909, 21733, 300, 1884, 17997, 13], "temperature": 0.0, "avg_logprob": -0.14871868065425328, "compression_ratio": 1.8346456692913387, "no_speech_prob": 7.028316031210124e-05}, {"id": 273, "seek": 182392, "start": 1823.92, "end": 1832.16, "text": " And so he had to write his own MIDI library. He rewrote it in V2. And he said it's just like", "tokens": [400, 370, 415, 632, 281, 2464, 702, 1065, 41474, 6405, 13, 634, 319, 7449, 1370, 309, 294, 691, 17, 13, 400, 415, 848, 309, 311, 445, 411], "temperature": 0.0, "avg_logprob": -0.1728335160475511, "compression_ratio": 1.3650793650793651, "no_speech_prob": 5.306088860379532e-05}, {"id": 274, "seek": 182392, "start": 1833.1200000000001, "end": 1840.4, "text": " so, so, so much easier thanks to those mid tier APIs. So, yeah. At this stage.", "tokens": [370, 11, 370, 11, 370, 709, 3571, 3231, 281, 729, 2062, 12362, 21445, 13, 407, 11, 1338, 13, 1711, 341, 3233, 13], "temperature": 0.0, "avg_logprob": -0.1728335160475511, "compression_ratio": 1.3650793650793651, "no_speech_prob": 5.306088860379532e-05}, {"id": 275, "seek": 182392, "start": 1842.5600000000002, "end": 1847.28, "text": " I was just going to jump in quick. I've been helping with some of the audio stuff. And", "tokens": [286, 390, 445, 516, 281, 3012, 294, 1702, 13, 286, 600, 668, 4315, 365, 512, 295, 264, 6278, 1507, 13, 400], "temperature": 0.0, "avg_logprob": -0.1728335160475511, "compression_ratio": 1.3650793650793651, "no_speech_prob": 5.306088860379532e-05}, {"id": 276, "seek": 184728, "start": 1847.28, "end": 1856.72, "text": " it's been really awesome. So, it makes things a lot more flexible than version one. So, that's", "tokens": [309, 311, 668, 534, 3476, 13, 407, 11, 309, 1669, 721, 257, 688, 544, 11358, 813, 3037, 472, 13, 407, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.20053532835725066, "compression_ratio": 1.5106382978723405, "no_speech_prob": 9.459620923735201e-05}, {"id": 277, "seek": 184728, "start": 1856.72, "end": 1863.44, "text": " probably my favorite thing about it is everything can be interchanged. Nothing is like, well, it's", "tokens": [1391, 452, 2954, 551, 466, 309, 307, 1203, 393, 312, 728, 339, 10296, 13, 6693, 307, 411, 11, 731, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.20053532835725066, "compression_ratio": 1.5106382978723405, "no_speech_prob": 9.459620923735201e-05}, {"id": 278, "seek": 184728, "start": 1863.44, "end": 1870.96, "text": " got to be this way because that's how it is. Yep. That's cool. Cool. Thanks. Another piece", "tokens": [658, 281, 312, 341, 636, 570, 300, 311, 577, 309, 307, 13, 7010, 13, 663, 311, 1627, 13, 8561, 13, 2561, 13, 3996, 2522], "temperature": 0.0, "avg_logprob": -0.20053532835725066, "compression_ratio": 1.5106382978723405, "no_speech_prob": 9.459620923735201e-05}, {"id": 279, "seek": 187096, "start": 1870.96, "end": 1878.48, "text": " of the transform is of the foundation is the partially reversible composed function pipeline", "tokens": [295, 264, 4088, 307, 295, 264, 7030, 307, 264, 18886, 44788, 18204, 2445, 15517], "temperature": 0.0, "avg_logprob": -0.19034215023643092, "compression_ratio": 1.6484848484848484, "no_speech_prob": 1.3841337022313382e-05}, {"id": 280, "seek": 187096, "start": 1878.48, "end": 1883.6000000000001, "text": " dispatched over collections. Which really rolls off the tongue. We call them transform and pipeline.", "tokens": [4920, 24102, 670, 16641, 13, 3013, 534, 15767, 766, 264, 10601, 13, 492, 818, 552, 4088, 293, 15517, 13], "temperature": 0.0, "avg_logprob": -0.19034215023643092, "compression_ratio": 1.6484848484848484, "no_speech_prob": 1.3841337022313382e-05}, {"id": 281, "seek": 187096, "start": 1884.8, "end": 1894.16, "text": " Basically, the idea is that the way you kind of want function dispatch to work", "tokens": [8537, 11, 264, 1558, 307, 300, 264, 636, 291, 733, 295, 528, 2445, 36729, 281, 589], "temperature": 0.0, "avg_logprob": -0.19034215023643092, "compression_ratio": 1.6484848484848484, "no_speech_prob": 1.3841337022313382e-05}, {"id": 282, "seek": 189416, "start": 1894.16, "end": 1900.8000000000002, "text": " and function composition to work in deep learning is a little different to other places. There's a", "tokens": [293, 2445, 12686, 281, 589, 294, 2452, 2539, 307, 257, 707, 819, 281, 661, 3190, 13, 821, 311, 257], "temperature": 0.0, "avg_logprob": -0.2311936075037176, "compression_ratio": 1.738938053097345, "no_speech_prob": 2.178062277380377e-05}, {"id": 283, "seek": 189416, "start": 1900.8000000000002, "end": 1907.3600000000001, "text": " couple of things. The first is you often want to dispatch over tuples. And what I mean by that is", "tokens": [1916, 295, 721, 13, 440, 700, 307, 291, 2049, 528, 281, 36729, 670, 2604, 2622, 13, 400, 437, 286, 914, 538, 300, 307], "temperature": 0.0, "avg_logprob": -0.2311936075037176, "compression_ratio": 1.738938053097345, "no_speech_prob": 2.178062277380377e-05}, {"id": 284, "seek": 189416, "start": 1907.92, "end": 1918.72, "text": " if you have a function called flip left right and you have a tuple representing a mini batch where", "tokens": [498, 291, 362, 257, 2445, 1219, 7929, 1411, 558, 293, 291, 362, 257, 2604, 781, 13460, 257, 8382, 15245, 689], "temperature": 0.0, "avg_logprob": -0.2311936075037176, "compression_ratio": 1.738938053097345, "no_speech_prob": 2.178062277380377e-05}, {"id": 285, "seek": 189416, "start": 1918.72, "end": 1923.44, "text": " your independent variable is a picture and your dependent variable is a set of boundary variables", "tokens": [428, 6695, 7006, 307, 257, 3036, 293, 428, 12334, 7006, 307, 257, 992, 295, 12866, 9102], "temperature": 0.0, "avg_logprob": -0.2311936075037176, "compression_ratio": 1.738938053097345, "no_speech_prob": 2.178062277380377e-05}, {"id": 286, "seek": 192344, "start": 1923.44, "end": 1930.8, "text": " and bounding boxes, if you say flip left right on that tuple, you would expect both the X and the Y", "tokens": [293, 5472, 278, 9002, 11, 498, 291, 584, 7929, 1411, 558, 322, 300, 2604, 781, 11, 291, 576, 2066, 1293, 264, 1783, 293, 264, 398], "temperature": 0.0, "avg_logprob": -0.1358259374445135, "compression_ratio": 1.5303867403314917, "no_speech_prob": 3.96684481529519e-06}, {"id": 287, "seek": 192344, "start": 1931.6000000000001, "end": 1939.52, "text": " to be flipped and to be flipped with the type appropriate method. So, our transforms will", "tokens": [281, 312, 26273, 293, 281, 312, 26273, 365, 264, 2010, 6854, 3170, 13, 407, 11, 527, 35592, 486], "temperature": 0.0, "avg_logprob": -0.1358259374445135, "compression_ratio": 1.5303867403314917, "no_speech_prob": 3.96684481529519e-06}, {"id": 288, "seek": 192344, "start": 1939.52, "end": 1946.88, "text": " automatically send each element of a tuple to the function separately and will dispatch", "tokens": [6772, 2845, 1184, 4478, 295, 257, 2604, 781, 281, 264, 2445, 14759, 293, 486, 36729], "temperature": 0.0, "avg_logprob": -0.1358259374445135, "compression_ratio": 1.5303867403314917, "no_speech_prob": 3.96684481529519e-06}, {"id": 289, "seek": 194688, "start": 1946.88, "end": 1953.1200000000001, "text": " according to their types automatically. We've mentioned type retention. So, the kind of basic", "tokens": [4650, 281, 641, 3467, 6772, 13, 492, 600, 2835, 2010, 22871, 13, 407, 11, 264, 733, 295, 3875], "temperature": 0.0, "avg_logprob": -0.21953329789011103, "compression_ratio": 1.6297872340425532, "no_speech_prob": 1.6279907413263572e-06}, {"id": 290, "seek": 194688, "start": 1953.1200000000001, "end": 1962.48, "text": " type stuff we need. One interesting thing is not only encoding, so, in other words, applying the", "tokens": [2010, 1507, 321, 643, 13, 1485, 1880, 551, 307, 406, 787, 43430, 11, 370, 11, 294, 661, 2283, 11, 9275, 264], "temperature": 0.0, "avg_logprob": -0.21953329789011103, "compression_ratio": 1.6297872340425532, "no_speech_prob": 1.6279907413263572e-06}, {"id": 291, "seek": 194688, "start": 1962.48, "end": 1968.64, "text": " function, you often need to be able to decode, which is to de-apply the function. So, for example,", "tokens": [2445, 11, 291, 2049, 643, 281, 312, 1075, 281, 979, 1429, 11, 597, 307, 281, 368, 12, 1746, 356, 264, 2445, 13, 407, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.21953329789011103, "compression_ratio": 1.6297872340425532, "no_speech_prob": 1.6279907413263572e-06}, {"id": 292, "seek": 194688, "start": 1968.64, "end": 1975.6000000000001, "text": " a categorization transform would take the word dog and convert it to the number one, perhaps,", "tokens": [257, 19250, 2144, 4088, 576, 747, 264, 1349, 3000, 293, 7620, 309, 281, 264, 1230, 472, 11, 4317, 11], "temperature": 0.0, "avg_logprob": -0.21953329789011103, "compression_ratio": 1.6297872340425532, "no_speech_prob": 1.6279907413263572e-06}, {"id": 293, "seek": 197560, "start": 1975.6, "end": 1980.56, "text": " which is what you need for modeling. But then when your predictions come back, you need to know what", "tokens": [597, 307, 437, 291, 643, 337, 15983, 13, 583, 550, 562, 428, 21264, 808, 646, 11, 291, 643, 281, 458, 437], "temperature": 0.0, "avg_logprob": -0.16589046407628943, "compression_ratio": 1.7096774193548387, "no_speech_prob": 4.222615189064527e-06}, {"id": 294, "seek": 197560, "start": 1980.56, "end": 1987.76, "text": " one represents. So, you need to reverse that transform and turn one back into dog. Often,", "tokens": [472, 8855, 13, 407, 11, 291, 643, 281, 9943, 300, 4088, 293, 1261, 472, 646, 666, 3000, 13, 20043, 11], "temperature": 0.0, "avg_logprob": -0.16589046407628943, "compression_ratio": 1.7096774193548387, "no_speech_prob": 4.222615189064527e-06}, {"id": 295, "seek": 197560, "start": 1987.76, "end": 1994.8799999999999, "text": " those transforms also need data-driven setup. For example, in that example of dog becoming one,", "tokens": [729, 35592, 611, 643, 1412, 12, 25456, 8657, 13, 1171, 1365, 11, 294, 300, 1365, 295, 3000, 5617, 472, 11], "temperature": 0.0, "avg_logprob": -0.16589046407628943, "compression_ratio": 1.7096774193548387, "no_speech_prob": 4.222615189064527e-06}, {"id": 296, "seek": 197560, "start": 1994.8799999999999, "end": 1999.12, "text": " there needs to be something that actually creates that vocab automatically, recognizing what are", "tokens": [456, 2203, 281, 312, 746, 300, 767, 7829, 300, 2329, 455, 6772, 11, 18538, 437, 366], "temperature": 0.0, "avg_logprob": -0.16589046407628943, "compression_ratio": 1.7096774193548387, "no_speech_prob": 4.222615189064527e-06}, {"id": 297, "seek": 197560, "start": 1999.12, "end": 2004.56, "text": " all the possible classes, so it can create a different index for each one and then apply that", "tokens": [439, 264, 1944, 5359, 11, 370, 309, 393, 1884, 257, 819, 8186, 337, 1184, 472, 293, 550, 3079, 300], "temperature": 0.0, "avg_logprob": -0.16589046407628943, "compression_ratio": 1.7096774193548387, "no_speech_prob": 4.222615189064527e-06}, {"id": 298, "seek": 200456, "start": 2004.56, "end": 2012.56, "text": " to the validation set. And quite often, these transforms also have some kind of state, such as", "tokens": [281, 264, 24071, 992, 13, 400, 1596, 2049, 11, 613, 35592, 611, 362, 512, 733, 295, 1785, 11, 1270, 382], "temperature": 0.0, "avg_logprob": -0.18899798075358074, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.771664291387424e-06}, {"id": 299, "seek": 200456, "start": 2012.56, "end": 2018.96, "text": " the vocab. So, we built this bunch of stuff that builds on top of each other. At the lowest level", "tokens": [264, 2329, 455, 13, 407, 11, 321, 3094, 341, 3840, 295, 1507, 300, 15182, 322, 1192, 295, 1184, 661, 13, 1711, 264, 12437, 1496], "temperature": 0.0, "avg_logprob": -0.18899798075358074, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.771664291387424e-06}, {"id": 300, "seek": 200456, "start": 2018.96, "end": 2029.28, "text": " is a class called transform, which is a callable, which also has a decode, does the type retention", "tokens": [307, 257, 1508, 1219, 4088, 11, 597, 307, 257, 818, 712, 11, 597, 611, 575, 257, 979, 1429, 11, 775, 264, 2010, 22871], "temperature": 0.0, "avg_logprob": -0.18899798075358074, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.771664291387424e-06}, {"id": 301, "seek": 202928, "start": 2029.28, "end": 2035.04, "text": " higher kind of type thing and does the dispatch over tuples by default. So, then a pipeline is", "tokens": [2946, 733, 295, 2010, 551, 293, 775, 264, 36729, 670, 2604, 2622, 538, 7576, 13, 407, 11, 550, 257, 15517, 307], "temperature": 0.0, "avg_logprob": -0.18619814584421557, "compression_ratio": 1.7302325581395348, "no_speech_prob": 2.2472531782113947e-05}, {"id": 302, "seek": 202928, "start": 2035.04, "end": 2042.3999999999999, "text": " something that does function composition over transforms. And it knows about, for example,", "tokens": [746, 300, 775, 2445, 12686, 670, 35592, 13, 400, 309, 3255, 466, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.18619814584421557, "compression_ratio": 1.7302325581395348, "no_speech_prob": 2.2472531782113947e-05}, {"id": 303, "seek": 202928, "start": 2042.3999999999999, "end": 2049.12, "text": " setting up transforms. And, like, setting up transforms in a pipeline is a bit tricky,", "tokens": [3287, 493, 35592, 13, 400, 11, 411, 11, 3287, 493, 35592, 294, 257, 15517, 307, 257, 857, 12414, 11], "temperature": 0.0, "avg_logprob": -0.18619814584421557, "compression_ratio": 1.7302325581395348, "no_speech_prob": 2.2472531782113947e-05}, {"id": 304, "seek": 202928, "start": 2049.12, "end": 2055.2799999999997, "text": " because you have to make sure that at each level of the pipeline, only the previous steps have been", "tokens": [570, 291, 362, 281, 652, 988, 300, 412, 1184, 1496, 295, 264, 15517, 11, 787, 264, 3894, 4439, 362, 668], "temperature": 0.0, "avg_logprob": -0.18619814584421557, "compression_ratio": 1.7302325581395348, "no_speech_prob": 2.2472531782113947e-05}, {"id": 305, "seek": 205528, "start": 2055.28, "end": 2061.2000000000003, "text": " applied before you set up the next step. So, it does little things like that. And then we have", "tokens": [6456, 949, 291, 992, 493, 264, 958, 1823, 13, 407, 11, 309, 775, 707, 721, 411, 300, 13, 400, 550, 321, 362], "temperature": 0.0, "avg_logprob": -0.14901832867694156, "compression_ratio": 1.7092511013215859, "no_speech_prob": 2.586442860774696e-05}, {"id": 306, "seek": 205528, "start": 2061.2000000000003, "end": 2067.0400000000004, "text": " something that applies a pipeline to a collection to give you an indexable, lazily transformed", "tokens": [746, 300, 13165, 257, 15517, 281, 257, 5765, 281, 976, 291, 364, 8186, 712, 11, 19320, 953, 16894], "temperature": 0.0, "avg_logprob": -0.14901832867694156, "compression_ratio": 1.7092511013215859, "no_speech_prob": 2.586442860774696e-05}, {"id": 307, "seek": 205528, "start": 2067.0400000000004, "end": 2073.28, "text": " collection. And then you can do those in parallel to get back, you know, an independent variable,", "tokens": [5765, 13, 400, 550, 291, 393, 360, 729, 294, 8952, 281, 483, 646, 11, 291, 458, 11, 364, 6695, 7006, 11], "temperature": 0.0, "avg_logprob": -0.14901832867694156, "compression_ratio": 1.7092511013215859, "no_speech_prob": 2.586442860774696e-05}, {"id": 308, "seek": 205528, "start": 2073.28, "end": 2083.6000000000004, "text": " for instance. And then finally, we've built a data loader, which will apply these things in parallel", "tokens": [337, 5197, 13, 400, 550, 2721, 11, 321, 600, 3094, 257, 1412, 3677, 260, 11, 597, 486, 3079, 613, 721, 294, 8952], "temperature": 0.0, "avg_logprob": -0.14901832867694156, "compression_ratio": 1.7092511013215859, "no_speech_prob": 2.586442860774696e-05}, {"id": 309, "seek": 208360, "start": 2083.6, "end": 2094.48, "text": " in parallel and create collated batches. So, in the end, all this stuff makes a lot of things", "tokens": [294, 8952, 293, 1884, 1263, 770, 15245, 279, 13, 407, 11, 294, 264, 917, 11, 439, 341, 1507, 1669, 257, 688, 295, 721], "temperature": 0.0, "avg_logprob": -0.1229133031454431, "compression_ratio": 1.5294117647058822, "no_speech_prob": 4.494876520766411e-06}, {"id": 310, "seek": 208360, "start": 2094.48, "end": 2102.56, "text": " much easier. For example, the language model data loader in fast.ai v1 was, like, pages of code.", "tokens": [709, 3571, 13, 1171, 1365, 11, 264, 2856, 2316, 1412, 3677, 260, 294, 2370, 13, 1301, 371, 16, 390, 11, 411, 11, 7183, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1229133031454431, "compression_ratio": 1.5294117647058822, "no_speech_prob": 4.494876520766411e-06}, {"id": 311, "seek": 208360, "start": 2102.56, "end": 2109.2799999999997, "text": " In TensorFlow, it's pages of code. In fast.ai v2, it's less than a screen of code by leveraging", "tokens": [682, 37624, 11, 309, 311, 7183, 295, 3089, 13, 682, 2370, 13, 1301, 371, 17, 11, 309, 311, 1570, 813, 257, 2568, 295, 3089, 538, 32666], "temperature": 0.0, "avg_logprob": -0.1229133031454431, "compression_ratio": 1.5294117647058822, "no_speech_prob": 4.494876520766411e-06}, {"id": 312, "seek": 210928, "start": 2109.28, "end": 2119.36, "text": " these powerful abstractions and foundations. So, then finally, and, again, this is something I", "tokens": [613, 4005, 12649, 626, 293, 22467, 13, 407, 11, 550, 2721, 11, 293, 11, 797, 11, 341, 307, 746, 286], "temperature": 0.0, "avg_logprob": -0.12560462951660156, "compression_ratio": 1.5767634854771784, "no_speech_prob": 1.3630054127133917e-05}, {"id": 313, "seek": 210928, "start": 2119.36, "end": 2126.1600000000003, "text": " think Swift will be great for, we worked really hard to make everything extremely well optimized.", "tokens": [519, 25539, 486, 312, 869, 337, 11, 321, 2732, 534, 1152, 281, 652, 1203, 4664, 731, 26941, 13], "temperature": 0.0, "avg_logprob": -0.12560462951660156, "compression_ratio": 1.5767634854771784, "no_speech_prob": 1.3630054127133917e-05}, {"id": 314, "seek": 210928, "start": 2126.1600000000003, "end": 2132.1600000000003, "text": " So, for example, preprocessing and natural language processing, we created a parallel generator", "tokens": [407, 11, 337, 1365, 11, 2666, 340, 780, 278, 293, 3303, 2856, 9007, 11, 321, 2942, 257, 8952, 19265], "temperature": 0.0, "avg_logprob": -0.12560462951660156, "compression_ratio": 1.5767634854771784, "no_speech_prob": 1.3630054127133917e-05}, {"id": 315, "seek": 213216, "start": 2132.16, "end": 2138.72, "text": " in Python, which you can then basically pass a class to that defines some setup and a call.", "tokens": [294, 15329, 11, 597, 291, 393, 550, 1936, 1320, 257, 1508, 281, 300, 23122, 512, 8657, 293, 257, 818, 13], "temperature": 0.0, "avg_logprob": -0.22095750437842476, "compression_ratio": 1.4572864321608041, "no_speech_prob": 2.930896334873978e-05}, {"id": 316, "seek": 213216, "start": 2138.72, "end": 2148.0, "text": " And it can automatically parallelize that. So, for example, tokenization is done in parallel in a", "tokens": [400, 309, 393, 6772, 8952, 1125, 300, 13, 407, 11, 337, 1365, 11, 14862, 2144, 307, 1096, 294, 8952, 294, 257], "temperature": 0.0, "avg_logprob": -0.22095750437842476, "compression_ratio": 1.4572864321608041, "no_speech_prob": 2.930896334873978e-05}, {"id": 317, "seek": 213216, "start": 2148.0, "end": 2157.8399999999997, "text": " pretty memory-efficient way. Excuse me. But perhaps the thing I'm most excited about, both in Python", "tokens": [1238, 4675, 12, 68, 7816, 636, 13, 11359, 385, 13, 583, 4317, 264, 551, 286, 478, 881, 2919, 466, 11, 1293, 294, 15329], "temperature": 0.0, "avg_logprob": -0.22095750437842476, "compression_ratio": 1.4572864321608041, "no_speech_prob": 2.930896334873978e-05}, {"id": 318, "seek": 215784, "start": 2157.84, "end": 2167.36, "text": " and Swift, is the optimized pipeline running on the GPU. So, all of the, pretty much all of the", "tokens": [293, 25539, 11, 307, 264, 26941, 15517, 2614, 322, 264, 18407, 13, 407, 11, 439, 295, 264, 11, 1238, 709, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.21092746257781983, "compression_ratio": 1.6, "no_speech_prob": 1.1299754078208935e-05}, {"id": 319, "seek": 215784, "start": 2167.36, "end": 2175.84, "text": " transforms we've done can and by default do run on the GPU. So, for example, when you do the flip", "tokens": [35592, 321, 600, 1096, 393, 293, 538, 7576, 360, 1190, 322, 264, 18407, 13, 407, 11, 337, 1365, 11, 562, 291, 360, 264, 7929], "temperature": 0.0, "avg_logprob": -0.21092746257781983, "compression_ratio": 1.6, "no_speech_prob": 1.1299754078208935e-05}, {"id": 320, "seek": 215784, "start": 2175.84, "end": 2182.48, "text": " left-right I showed you earlier will actually run on the GPU, as will warp, as will zoom, as will even", "tokens": [1411, 12, 1938, 286, 4712, 291, 3071, 486, 767, 1190, 322, 264, 18407, 11, 382, 486, 36030, 11, 382, 486, 8863, 11, 382, 486, 754], "temperature": 0.0, "avg_logprob": -0.21092746257781983, "compression_ratio": 1.6, "no_speech_prob": 1.1299754078208935e-05}, {"id": 321, "seek": 218248, "start": 2182.48, "end": 2191.6, "text": " things like crop. So, one of the basics of this is the affine coordinate transform,", "tokens": [721, 411, 9086, 13, 407, 11, 472, 295, 264, 14688, 295, 341, 307, 264, 2096, 533, 15670, 4088, 11], "temperature": 0.0, "avg_logprob": -0.15259296306665393, "compression_ratio": 1.489247311827957, "no_speech_prob": 4.6376640057133045e-06}, {"id": 322, "seek": 218248, "start": 2193.28, "end": 2201.44, "text": " which uses affine grid and grid sample, which are very powerful PyTorch functions, which would be", "tokens": [597, 4960, 2096, 533, 10748, 293, 10748, 6889, 11, 597, 366, 588, 4005, 9953, 51, 284, 339, 6828, 11, 597, 576, 312], "temperature": 0.0, "avg_logprob": -0.15259296306665393, "compression_ratio": 1.489247311827957, "no_speech_prob": 4.6376640057133045e-06}, {"id": 323, "seek": 218248, "start": 2201.44, "end": 2207.28, "text": " great things to actually write in the script for TensorFlow's new metaprogramming, because they", "tokens": [869, 721, 281, 767, 2464, 294, 264, 5755, 337, 37624, 311, 777, 1131, 569, 340, 1342, 2810, 11, 570, 436], "temperature": 0.0, "avg_logprob": -0.15259296306665393, "compression_ratio": 1.489247311827957, "no_speech_prob": 4.6376640057133045e-06}, {"id": 324, "seek": 220728, "start": 2207.28, "end": 2215.36, "text": " don't exist in TensorFlow, or at least not in any very complete way. But with these basic ideas,", "tokens": [500, 380, 2514, 294, 37624, 11, 420, 412, 1935, 406, 294, 604, 588, 3566, 636, 13, 583, 365, 613, 3875, 3487, 11], "temperature": 0.0, "avg_logprob": -0.08103601475979419, "compression_ratio": 1.559670781893004, "no_speech_prob": 8.938967766880523e-06}, {"id": 325, "seek": 220728, "start": 2215.36, "end": 2220.88, "text": " we can create this affine coordinate transform that lets us do a very wide range of data", "tokens": [321, 393, 1884, 341, 2096, 533, 15670, 4088, 300, 6653, 505, 360, 257, 588, 4874, 3613, 295, 1412], "temperature": 0.0, "avg_logprob": -0.08103601475979419, "compression_ratio": 1.559670781893004, "no_speech_prob": 8.938967766880523e-06}, {"id": 326, "seek": 220728, "start": 2220.88, "end": 2227.0400000000004, "text": " augmentations in parallel on the GPU. For those of you that know about the Dali library that we've", "tokens": [29919, 763, 294, 8952, 322, 264, 18407, 13, 1171, 729, 295, 291, 300, 458, 466, 264, 413, 5103, 6405, 300, 321, 600], "temperature": 0.0, "avg_logprob": -0.08103601475979419, "compression_ratio": 1.559670781893004, "no_speech_prob": 8.938967766880523e-06}, {"id": 327, "seek": 220728, "start": 2227.0400000000004, "end": 2233.0400000000004, "text": " created, this provides a lot of the same benefits as Dali. It's pretty similar in terms of its", "tokens": [2942, 11, 341, 6417, 257, 688, 295, 264, 912, 5311, 382, 413, 5103, 13, 467, 311, 1238, 2531, 294, 2115, 295, 1080], "temperature": 0.0, "avg_logprob": -0.08103601475979419, "compression_ratio": 1.559670781893004, "no_speech_prob": 8.938967766880523e-06}, {"id": 328, "seek": 223304, "start": 2233.04, "end": 2239.68, "text": " performance, but the nice thing is all the stuff you write, you write it in Python, not in CUDA.", "tokens": [3389, 11, 457, 264, 1481, 551, 307, 439, 264, 1507, 291, 2464, 11, 291, 2464, 309, 294, 15329, 11, 406, 294, 29777, 7509, 13], "temperature": 0.0, "avg_logprob": -0.10965810757931148, "compression_ratio": 1.6, "no_speech_prob": 1.5935298506519757e-05}, {"id": 329, "seek": 223304, "start": 2239.68, "end": 2245.44, "text": " So, with Dali, if they don't have the exact transformation you want, and there's a pretty", "tokens": [407, 11, 365, 413, 5103, 11, 498, 436, 500, 380, 362, 264, 1900, 9887, 291, 528, 11, 293, 456, 311, 257, 1238], "temperature": 0.0, "avg_logprob": -0.10965810757931148, "compression_ratio": 1.6, "no_speech_prob": 1.5935298506519757e-05}, {"id": 330, "seek": 223304, "start": 2245.44, "end": 2252.48, "text": " high chance that they won't, then you're stuck. Or else with fast.aiv2, you can write your own", "tokens": [1090, 2931, 300, 436, 1582, 380, 11, 550, 291, 434, 5541, 13, 1610, 1646, 365, 2370, 13, 1301, 85, 17, 11, 291, 393, 2464, 428, 1065], "temperature": 0.0, "avg_logprob": -0.10965810757931148, "compression_ratio": 1.6, "no_speech_prob": 1.5935298506519757e-05}, {"id": 331, "seek": 223304, "start": 2253.12, "end": 2260.64, "text": " in a few lines of Python, you can test it out in a Jupyter notebook, it makes life super easy.", "tokens": [294, 257, 1326, 3876, 295, 15329, 11, 291, 393, 1500, 309, 484, 294, 257, 22125, 88, 391, 21060, 11, 309, 1669, 993, 1687, 1858, 13], "temperature": 0.0, "avg_logprob": -0.10965810757931148, "compression_ratio": 1.6, "no_speech_prob": 1.5935298506519757e-05}, {"id": 332, "seek": 226064, "start": 2260.64, "end": 2271.2, "text": " So, this kind of stuff, I feel like, because Swift is a much faster, more hackable language", "tokens": [407, 11, 341, 733, 295, 1507, 11, 286, 841, 411, 11, 570, 25539, 307, 257, 709, 4663, 11, 544, 10339, 712, 2856], "temperature": 0.0, "avg_logprob": -0.13057257788521903, "compression_ratio": 1.5722222222222222, "no_speech_prob": 6.74715920467861e-06}, {"id": 333, "seek": 226064, "start": 2271.2, "end": 2276.7999999999997, "text": " than Python, or at least hackable in the sense of performance, I guess not as hackable in terms", "tokens": [813, 15329, 11, 420, 412, 1935, 10339, 712, 294, 264, 2020, 295, 3389, 11, 286, 2041, 406, 382, 10339, 712, 294, 2115], "temperature": 0.0, "avg_logprob": -0.13057257788521903, "compression_ratio": 1.5722222222222222, "no_speech_prob": 6.74715920467861e-06}, {"id": 334, "seek": 226064, "start": 2276.7999999999997, "end": 2286.96, "text": " of its type system necessarily, I feel like we can kind of build even more powerful foundations", "tokens": [295, 1080, 2010, 1185, 4725, 11, 286, 841, 411, 321, 393, 733, 295, 1322, 754, 544, 4005, 22467], "temperature": 0.0, "avg_logprob": -0.13057257788521903, "compression_ratio": 1.5722222222222222, "no_speech_prob": 6.74715920467861e-06}, {"id": 335, "seek": 228696, "start": 2286.96, "end": 2295.68, "text": " and pipelines and a real Swift for TensorFlow computer vision library, leveraging the", "tokens": [293, 40168, 293, 257, 957, 25539, 337, 37624, 3820, 5201, 6405, 11, 32666, 264], "temperature": 0.0, "avg_logprob": -0.17961593296216882, "compression_ratio": 1.5055555555555555, "no_speech_prob": 3.0710900318808854e-05}, {"id": 336, "seek": 228696, "start": 2295.68, "end": 2303.2, "text": " metaprogramming and leveraging Swift numerics, stuff like that I think would be super cool.", "tokens": [1131, 569, 340, 1342, 2810, 293, 32666, 25539, 7866, 1167, 11, 1507, 411, 300, 286, 519, 576, 312, 1687, 1627, 13], "temperature": 0.0, "avg_logprob": -0.17961593296216882, "compression_ratio": 1.5055555555555555, "no_speech_prob": 3.0710900318808854e-05}, {"id": 337, "seek": 228696, "start": 2304.32, "end": 2306.32, "text": " So, that is the end of that.", "tokens": [407, 11, 300, 307, 264, 917, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.17961593296216882, "compression_ratio": 1.5055555555555555, "no_speech_prob": 3.0710900318808854e-05}, {"id": 338, "seek": 228696, "start": 2308.8, "end": 2311.76, "text": " That was great. That was excellent. Thank you very much, Jeremy.", "tokens": [663, 390, 869, 13, 663, 390, 7103, 13, 1044, 291, 588, 709, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.17961593296216882, "compression_ratio": 1.5055555555555555, "no_speech_prob": 3.0710900318808854e-05}, {"id": 339, "seek": 231176, "start": 2311.76, "end": 2319.84, "text": " My pleasure. So, just sort of thinking through, so as you're propagating along this self-type", "tokens": [1222, 6834, 13, 407, 11, 445, 1333, 295, 1953, 807, 11, 370, 382, 291, 434, 12425, 990, 2051, 341, 2698, 12, 20467], "temperature": 0.0, "avg_logprob": -0.12624113419476676, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.361004827544093e-05}, {"id": 340, "seek": 231176, "start": 2319.84, "end": 2324.32, "text": " amongst the transformations, that seems relatively straightforward for Swift to handle. Are there", "tokens": [12918, 264, 34852, 11, 300, 2544, 7226, 15325, 337, 25539, 281, 4813, 13, 2014, 456], "temperature": 0.0, "avg_logprob": -0.12624113419476676, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.361004827544093e-05}, {"id": 341, "seek": 231176, "start": 2324.32, "end": 2327.92, "text": " other sorts of things that you think we should start thinking about now?", "tokens": [661, 7527, 295, 721, 300, 291, 519, 321, 820, 722, 1953, 466, 586, 30], "temperature": 0.0, "avg_logprob": -0.12624113419476676, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.361004827544093e-05}, {"id": 342, "seek": 231176, "start": 2328.88, "end": 2332.7200000000003, "text": " Yeah, the thing I really want you to think about, and we've kind of been nagging you on and off", "tokens": [865, 11, 264, 551, 286, 534, 528, 291, 281, 519, 466, 11, 293, 321, 600, 733, 295, 668, 17096, 3249, 291, 322, 293, 766], "temperature": 0.0, "avg_logprob": -0.12624113419476676, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.361004827544093e-05}, {"id": 343, "seek": 233272, "start": 2332.72, "end": 2342.72, "text": " since March, is the way that tensors are represented. Having them as a value type the way they are now", "tokens": [1670, 6129, 11, 307, 264, 636, 300, 10688, 830, 366, 10379, 13, 10222, 552, 382, 257, 2158, 2010, 264, 636, 436, 366, 586], "temperature": 0.0, "avg_logprob": -0.09020956942909643, "compression_ratio": 1.5847457627118644, "no_speech_prob": 4.1946266719605774e-05}, {"id": 344, "seek": 233272, "start": 2342.72, "end": 2348.7999999999997, "text": " makes some things hard or impossible. So, the generic optimizer is a thing that I really,", "tokens": [1669, 512, 721, 1152, 420, 6243, 13, 407, 11, 264, 19577, 5028, 6545, 307, 257, 551, 300, 286, 534, 11], "temperature": 0.0, "avg_logprob": -0.09020956942909643, "compression_ratio": 1.5847457627118644, "no_speech_prob": 4.1946266719605774e-05}, {"id": 345, "seek": 233272, "start": 2348.7999999999997, "end": 2354.7999999999997, "text": " really want you guys to look into and build properly. Currently, it uses ugly key path", "tokens": [534, 528, 291, 1074, 281, 574, 666, 293, 1322, 6108, 13, 19964, 11, 309, 4960, 12246, 2141, 3100], "temperature": 0.0, "avg_logprob": -0.09020956942909643, "compression_ratio": 1.5847457627118644, "no_speech_prob": 4.1946266719605774e-05}, {"id": 346, "seek": 235480, "start": 2354.8, "end": 2364.32, "text": " hacks and it's only partially doing what we need it to do. So, I've kind of talked to Alexis about", "tokens": [33617, 293, 309, 311, 787, 18886, 884, 437, 321, 643, 309, 281, 360, 13, 407, 11, 286, 600, 733, 295, 2825, 281, 39826, 466], "temperature": 0.0, "avg_logprob": -0.17761093691775673, "compression_ratio": 1.4396135265700483, "no_speech_prob": 1.4282951269706246e-05}, {"id": 347, "seek": 235480, "start": 2364.32, "end": 2373.84, "text": " this idea quite a bit. We kind of thought, you know, maybe there could be some type that represents", "tokens": [341, 1558, 1596, 257, 857, 13, 492, 733, 295, 1194, 11, 291, 458, 11, 1310, 456, 727, 312, 512, 2010, 300, 8855], "temperature": 0.0, "avg_logprob": -0.17761093691775673, "compression_ratio": 1.4396135265700483, "no_speech_prob": 1.4282951269706246e-05}, {"id": 348, "seek": 235480, "start": 2373.84, "end": 2381.84, "text": " the actual block of GPU memory in a way where we can easily share that. In practice, we've realized", "tokens": [264, 3539, 3461, 295, 18407, 4675, 294, 257, 636, 689, 321, 393, 3612, 2073, 300, 13, 682, 3124, 11, 321, 600, 5334], "temperature": 0.0, "avg_logprob": -0.17761093691775673, "compression_ratio": 1.4396135265700483, "no_speech_prob": 1.4282951269706246e-05}, {"id": 349, "seek": 238184, "start": 2381.84, "end": 2389.2000000000003, "text": " the vast majority of the time, we want to refer to that exact piece of memory on the GPU, not", "tokens": [264, 8369, 6286, 295, 264, 565, 11, 321, 528, 281, 2864, 281, 300, 1900, 2522, 295, 4675, 322, 264, 18407, 11, 406], "temperature": 0.0, "avg_logprob": -0.18213268317798575, "compression_ratio": 1.6058091286307055, "no_speech_prob": 4.936585355608258e-06}, {"id": 350, "seek": 238184, "start": 2390.1600000000003, "end": 2396.7200000000003, "text": " this idea of a tensor which may magically copy itself if I change something. And so, for example,", "tokens": [341, 1558, 295, 257, 40863, 597, 815, 39763, 5055, 2564, 498, 286, 1319, 746, 13, 400, 370, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.18213268317798575, "compression_ratio": 1.6058091286307055, "no_speech_prob": 4.936585355608258e-06}, {"id": 351, "seek": 238184, "start": 2396.7200000000003, "end": 2402.2400000000002, "text": " with the generic optimizer, we need to be able to say, like, oh, this layer is part of this layer", "tokens": [365, 264, 19577, 5028, 6545, 11, 321, 643, 281, 312, 1075, 281, 584, 11, 411, 11, 1954, 11, 341, 4583, 307, 644, 295, 341, 4583], "temperature": 0.0, "avg_logprob": -0.18213268317798575, "compression_ratio": 1.6058091286307055, "no_speech_prob": 4.936585355608258e-06}, {"id": 352, "seek": 238184, "start": 2402.2400000000002, "end": 2410.1600000000003, "text": " group and this layer group has these things that need to happen to it. So, I actually said to Ed,", "tokens": [1594, 293, 341, 4583, 1594, 575, 613, 721, 300, 643, 281, 1051, 281, 309, 13, 407, 11, 286, 767, 848, 281, 3977, 11], "temperature": 0.0, "avg_logprob": -0.18213268317798575, "compression_ratio": 1.6058091286307055, "no_speech_prob": 4.936585355608258e-06}, {"id": 353, "seek": 241016, "start": 2410.16, "end": 2416.16, "text": " like, hey, you know, could you please have a look at the Swift AI generic optimizer because it looks", "tokens": [411, 11, 4177, 11, 291, 458, 11, 727, 291, 1767, 362, 257, 574, 412, 264, 25539, 7318, 19577, 5028, 6545, 570, 309, 1542], "temperature": 0.0, "avg_logprob": -0.21526295608944362, "compression_ratio": 1.4572864321608041, "no_speech_prob": 1.6181556929950602e-05}, {"id": 354, "seek": 241016, "start": 2416.16, "end": 2423.04, "text": " it's trying to be a similar design to the fast AI V2 optimizer, but it's currently pretty", "tokens": [309, 311, 1382, 281, 312, 257, 2531, 1715, 281, 264, 2370, 7318, 691, 17, 5028, 6545, 11, 457, 309, 311, 4362, 1238], "temperature": 0.0, "avg_logprob": -0.21526295608944362, "compression_ratio": 1.4572864321608041, "no_speech_prob": 1.6181556929950602e-05}, {"id": 355, "seek": 241016, "start": 2423.04, "end": 2431.52, "text": " unattractive. The second is I feel like creating a really good computer vision library is something", "tokens": [47316, 1897, 488, 13, 440, 1150, 307, 286, 841, 411, 4084, 257, 534, 665, 3820, 5201, 6405, 307, 746], "temperature": 0.0, "avg_logprob": -0.21526295608944362, "compression_ratio": 1.4572864321608041, "no_speech_prob": 1.6181556929950602e-05}, {"id": 356, "seek": 243152, "start": 2431.52, "end": 2441.6, "text": " which could be done now-ish. When I tried to do it, I was getting kind of race conditions and freezes", "tokens": [597, 727, 312, 1096, 586, 12, 742, 13, 1133, 286, 3031, 281, 360, 309, 11, 286, 390, 1242, 733, 295, 4569, 4487, 293, 1737, 12214], "temperature": 0.0, "avg_logprob": -0.1667878650483631, "compression_ratio": 1.5975609756097562, "no_speech_prob": 1.6182739273062907e-05}, {"id": 357, "seek": 243152, "start": 2441.6, "end": 2446.24, "text": " inside Swift and I don't have the Swift skills to know where they were coming from or how to fix", "tokens": [1854, 25539, 293, 286, 500, 380, 362, 264, 25539, 3942, 281, 458, 689, 436, 645, 1348, 490, 420, 577, 281, 3191], "temperature": 0.0, "avg_logprob": -0.1667878650483631, "compression_ratio": 1.5975609756097562, "no_speech_prob": 1.6182739273062907e-05}, {"id": 358, "seek": 243152, "start": 2446.24, "end": 2451.36, "text": " them. It would be nice if folks could, like, I think all of my answers is, like, go back to the", "tokens": [552, 13, 467, 576, 312, 1481, 498, 4024, 727, 11, 411, 11, 286, 519, 439, 295, 452, 6338, 307, 11, 411, 11, 352, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.1667878650483631, "compression_ratio": 1.5975609756097562, "no_speech_prob": 1.6182739273062907e-05}, {"id": 359, "seek": 243152, "start": 2451.36, "end": 2459.12, "text": " stuff that we all built together back in March, April, May, and try to start using it in real life", "tokens": [1507, 300, 321, 439, 3094, 1214, 646, 294, 6129, 11, 6929, 11, 1891, 11, 293, 853, 281, 722, 1228, 309, 294, 957, 993], "temperature": 0.0, "avg_logprob": -0.1667878650483631, "compression_ratio": 1.5975609756097562, "no_speech_prob": 1.6182739273062907e-05}, {"id": 360, "seek": 245912, "start": 2459.12, "end": 2467.12, "text": " and build models with it and put them in production and see the bits where it hits where you get stuck", "tokens": [293, 1322, 5245, 365, 309, 293, 829, 552, 294, 4265, 293, 536, 264, 9239, 689, 309, 8664, 689, 291, 483, 5541], "temperature": 0.0, "avg_logprob": -0.1360614676224558, "compression_ratio": 1.5913978494623655, "no_speech_prob": 1.3629017303173896e-05}, {"id": 361, "seek": 245912, "start": 2468.0, "end": 2473.7599999999998, "text": " because you'll find things like, oh, there's no grid sample and, oh, there's race conditions in the,", "tokens": [570, 291, 603, 915, 721, 411, 11, 1954, 11, 456, 311, 572, 10748, 6889, 293, 11, 1954, 11, 456, 311, 4569, 4487, 294, 264, 11], "temperature": 0.0, "avg_logprob": -0.1360614676224558, "compression_ratio": 1.5913978494623655, "no_speech_prob": 1.3629017303173896e-05}, {"id": 362, "seek": 245912, "start": 2475.12, "end": 2482.24, "text": " you know, interaction of OpenCV and, you know, the optimizer doesn't quite work properly and", "tokens": [291, 458, 11, 9285, 295, 7238, 34, 53, 293, 11, 291, 458, 11, 264, 5028, 6545, 1177, 380, 1596, 589, 6108, 293], "temperature": 0.0, "avg_logprob": -0.1360614676224558, "compression_ratio": 1.5913978494623655, "no_speech_prob": 1.3629017303173896e-05}, {"id": 363, "seek": 248224, "start": 2482.24, "end": 2491.2799999999997, "text": " that stuff. That makes sense. I think we're also trying to figure out right now what the right path is with the runtime.", "tokens": [300, 1507, 13, 663, 1669, 2020, 13, 286, 519, 321, 434, 611, 1382, 281, 2573, 484, 558, 586, 437, 264, 558, 3100, 307, 365, 264, 34474, 13], "temperature": 0.0, "avg_logprob": -0.20679797766343602, "compression_ratio": 1.6641509433962265, "no_speech_prob": 7.766328053548932e-06}, {"id": 364, "seek": 248224, "start": 2491.2799999999997, "end": 2497.52, "text": " So we've historically been building on top of the TensorFlow runtime, which is great for a lot of reasons.", "tokens": [407, 321, 600, 16180, 668, 2390, 322, 1192, 295, 264, 37624, 34474, 11, 597, 307, 869, 337, 257, 688, 295, 4112, 13], "temperature": 0.0, "avg_logprob": -0.20679797766343602, "compression_ratio": 1.6641509433962265, "no_speech_prob": 7.766328053548932e-06}, {"id": 365, "seek": 248224, "start": 2497.52, "end": 2504.4799999999996, "text": " It has a lot of functionality in the box. It does pretty much everything. On the other hand, the", "tokens": [467, 575, 257, 688, 295, 14980, 294, 264, 2424, 13, 467, 775, 1238, 709, 1203, 13, 1282, 264, 661, 1011, 11, 264], "temperature": 0.0, "avg_logprob": -0.20679797766343602, "compression_ratio": 1.6641509433962265, "no_speech_prob": 7.766328053548932e-06}, {"id": 366, "seek": 248224, "start": 2504.4799999999996, "end": 2510.16, "text": " performance, particularly in eager mode, is not great. I think one of the things we're kicking around is the idea of", "tokens": [3389, 11, 4098, 294, 18259, 4391, 11, 307, 406, 869, 13, 286, 519, 472, 295, 264, 721, 321, 434, 19137, 926, 307, 264, 1558, 295], "temperature": 0.0, "avg_logprob": -0.20679797766343602, "compression_ratio": 1.6641509433962265, "no_speech_prob": 7.766328053548932e-06}, {"id": 367, "seek": 251016, "start": 2510.16, "end": 2517.92, "text": " going more directly into XLA. Yeah. Well, I think that's a thing that's been stopping us all from", "tokens": [516, 544, 3838, 666, 1783, 11435, 13, 865, 13, 1042, 11, 286, 519, 300, 311, 257, 551, 300, 311, 668, 12767, 505, 439, 490], "temperature": 0.0, "avg_logprob": -0.21120168612553522, "compression_ratio": 1.6694214876033058, "no_speech_prob": 1.777639590727631e-05}, {"id": 368, "seek": 251016, "start": 2517.92, "end": 2523.44, "text": " XLA being a stepping stone towards MLR in the future, which is also coming. I mean, I think that's the thing", "tokens": [1783, 11435, 885, 257, 16821, 7581, 3030, 21601, 49, 294, 264, 2027, 11, 597, 307, 611, 1348, 13, 286, 914, 11, 286, 519, 300, 311, 264, 551], "temperature": 0.0, "avg_logprob": -0.21120168612553522, "compression_ratio": 1.6694214876033058, "no_speech_prob": 1.777639590727631e-05}, {"id": 369, "seek": 251016, "start": 2523.44, "end": 2528.72, "text": " that's been stopping us all from using stuff like Swift AI to actually build models because", "tokens": [300, 311, 668, 12767, 505, 439, 490, 1228, 1507, 411, 25539, 7318, 281, 767, 1322, 5245, 570], "temperature": 0.0, "avg_logprob": -0.21120168612553522, "compression_ratio": 1.6694214876033058, "no_speech_prob": 1.777639590727631e-05}, {"id": 370, "seek": 251016, "start": 2529.7599999999998, "end": 2536.0, "text": " the autodiff has memory leaks and the TensorFlow runtime is, I don't have to be polite, so not at Google,", "tokens": [264, 1476, 378, 3661, 575, 4675, 28885, 293, 264, 37624, 34474, 307, 11, 286, 500, 380, 362, 281, 312, 25171, 11, 370, 406, 412, 3329, 11], "temperature": 0.0, "avg_logprob": -0.21120168612553522, "compression_ratio": 1.6694214876033058, "no_speech_prob": 1.777639590727631e-05}, {"id": 371, "seek": 253600, "start": 2536.0, "end": 2541.76, "text": " slow as molasses and, you know, implements everything in six different ways in six different places and so", "tokens": [2964, 382, 8015, 26615, 293, 11, 291, 458, 11, 704, 17988, 1203, 294, 2309, 819, 2098, 294, 2309, 819, 3190, 293, 370], "temperature": 0.0, "avg_logprob": -0.1420968216244537, "compression_ratio": 1.6122448979591837, "no_speech_prob": 1.1299764082650654e-05}, {"id": 372, "seek": 253600, "start": 2541.76, "end": 2546.72, "text": " forth. So yeah, I think everybody's going to be thinking into these higher level APIs a lot more", "tokens": [5220, 13, 407, 1338, 11, 286, 519, 2201, 311, 516, 281, 312, 1953, 666, 613, 2946, 1496, 21445, 257, 688, 544], "temperature": 0.0, "avg_logprob": -0.1420968216244537, "compression_ratio": 1.6122448979591837, "no_speech_prob": 1.1299764082650654e-05}, {"id": 373, "seek": 253600, "start": 2547.6, "end": 2553.44, "text": " once the foundations are there. Yeah. And so, I mean, the trade-off there is if we go with that", "tokens": [1564, 264, 22467, 366, 456, 13, 865, 13, 400, 370, 11, 286, 914, 11, 264, 4923, 12, 4506, 456, 307, 498, 321, 352, 365, 300], "temperature": 0.0, "avg_logprob": -0.1420968216244537, "compression_ratio": 1.6122448979591837, "no_speech_prob": 1.1299764082650654e-05}, {"id": 374, "seek": 253600, "start": 2553.44, "end": 2559.68, "text": " direction now, XLA doesn't provide all the things in the box, but I think that's probably fine.", "tokens": [3513, 586, 11, 1783, 11435, 1177, 380, 2893, 439, 264, 721, 294, 264, 2424, 11, 457, 286, 519, 300, 311, 1391, 2489, 13], "temperature": 0.0, "avg_logprob": -0.1420968216244537, "compression_ratio": 1.6122448979591837, "no_speech_prob": 1.1299764082650654e-05}, {"id": 375, "seek": 255968, "start": 2559.68, "end": 2566.3999999999996, "text": " We haven't passed those languages so we can't put stuff if we need it. And so I think we're talking about that,", "tokens": [492, 2378, 380, 4678, 729, 8650, 370, 321, 393, 380, 829, 1507, 498, 321, 643, 309, 13, 400, 370, 286, 519, 321, 434, 1417, 466, 300, 11], "temperature": 0.0, "avg_logprob": -0.2356176300048828, "compression_ratio": 1.6597938144329898, "no_speech_prob": 3.7045610952191055e-05}, {"id": 376, "seek": 255968, "start": 2566.3999999999996, "end": 2571.04, "text": " trying to decide what to do there. We're also investing a lot in AD and finishing that off.", "tokens": [1382, 281, 4536, 437, 281, 360, 456, 13, 492, 434, 611, 10978, 257, 688, 294, 9135, 293, 12693, 300, 766, 13], "temperature": 0.0, "avg_logprob": -0.2356176300048828, "compression_ratio": 1.6597938144329898, "no_speech_prob": 3.7045610952191055e-05}, {"id": 377, "seek": 255968, "start": 2571.04, "end": 2577.2799999999997, "text": " Yeah. I mean, all the right work's being done. It's just, you know, it's just early days.", "tokens": [865, 13, 286, 914, 11, 439, 264, 558, 589, 311, 885, 1096, 13, 467, 311, 445, 11, 291, 458, 11, 309, 311, 445, 2440, 1708, 13], "temperature": 0.0, "avg_logprob": -0.2356176300048828, "compression_ratio": 1.6597938144329898, "no_speech_prob": 3.7045610952191055e-05}, {"id": 378, "seek": 255968, "start": 2577.2799999999997, "end": 2581.68, "text": " Yes. Yeah, I think the challenge that we're really struggling with is this decision to stick with", "tokens": [1079, 13, 865, 11, 286, 519, 264, 3430, 300, 321, 434, 534, 9314, 365, 307, 341, 3537, 281, 2897, 365], "temperature": 0.0, "avg_logprob": -0.2356176300048828, "compression_ratio": 1.6597938144329898, "no_speech_prob": 3.7045610952191055e-05}, {"id": 379, "seek": 255968, "start": 2581.68, "end": 2589.44, "text": " the TensorFlow runtime to move on to something else. And that I think is complicated, but I", "tokens": [264, 37624, 34474, 281, 1286, 322, 281, 746, 1646, 13, 400, 300, 286, 519, 307, 6179, 11, 457, 286], "temperature": 0.0, "avg_logprob": -0.2356176300048828, "compression_ratio": 1.6597938144329898, "no_speech_prob": 3.7045610952191055e-05}, {"id": 380, "seek": 258944, "start": 2589.44, "end": 2596.08, "text": " agree this is one of the major blockers for adoption and use. Yeah. I mean, especially if you", "tokens": [3986, 341, 307, 472, 295, 264, 2563, 3461, 433, 337, 19215, 293, 764, 13, 865, 13, 286, 914, 11, 2318, 498, 291], "temperature": 0.0, "avg_logprob": -0.15995038157761698, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.222898370353505e-06}, {"id": 381, "seek": 258944, "start": 2596.8, "end": 2605.2000000000003, "text": " want to take advantage of Swift, which we do, you need something where, you know, the kernel", "tokens": [528, 281, 747, 5002, 295, 25539, 11, 597, 321, 360, 11, 291, 643, 746, 689, 11, 291, 458, 11, 264, 28256], "temperature": 0.0, "avg_logprob": -0.15995038157761698, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.222898370353505e-06}, {"id": 382, "seek": 258944, "start": 2605.2000000000003, "end": 2609.92, "text": " launch time is tiny or better still kind of non-existent because you can write everything", "tokens": [4025, 565, 307, 5870, 420, 1101, 920, 733, 295, 2107, 12, 18217, 317, 570, 291, 393, 2464, 1203], "temperature": 0.0, "avg_logprob": -0.15995038157761698, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.222898370353505e-06}, {"id": 383, "seek": 258944, "start": 2609.92, "end": 2616.56, "text": " in Swift. Otherwise, it's, yeah, you don't really get the benefits. Yeah. And one of the, so I'll", "tokens": [294, 25539, 13, 10328, 11, 309, 311, 11, 1338, 11, 291, 500, 380, 534, 483, 264, 5311, 13, 865, 13, 400, 472, 295, 264, 11, 370, 286, 603], "temperature": 0.0, "avg_logprob": -0.15995038157761698, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.222898370353505e-06}, {"id": 384, "seek": 261656, "start": 2616.56, "end": 2621.52, "text": " answer your question a second, but one of the trade offs there is that XLA doesn't have", "tokens": [1867, 428, 1168, 257, 1150, 11, 457, 472, 295, 264, 4923, 39457, 456, 307, 300, 1783, 11435, 1177, 380, 362], "temperature": 0.0, "avg_logprob": -0.18811801763681266, "compression_ratio": 1.6531365313653137, "no_speech_prob": 5.7717020354175474e-06}, {"id": 385, "seek": 261656, "start": 2621.52, "end": 2626.32, "text": " really fast kernel launch time because it effectively jit compiles things before launching it.", "tokens": [534, 2370, 28256, 4025, 565, 570, 309, 8659, 361, 270, 715, 4680, 721, 949, 18354, 309, 13], "temperature": 0.0, "avg_logprob": -0.18811801763681266, "compression_ratio": 1.6531365313653137, "no_speech_prob": 5.7717020354175474e-06}, {"id": 386, "seek": 261656, "start": 2627.2, "end": 2633.7599999999998, "text": " On the other hand, there are a lot of opportunities to do, for example, fusion and other things like", "tokens": [1282, 264, 661, 1011, 11, 456, 366, 257, 688, 295, 4786, 281, 360, 11, 337, 1365, 11, 23100, 293, 661, 721, 411], "temperature": 0.0, "avg_logprob": -0.18811801763681266, "compression_ratio": 1.6531365313653137, "no_speech_prob": 5.7717020354175474e-06}, {"id": 387, "seek": 261656, "start": 2633.7599999999998, "end": 2639.52, "text": " that that cannot set it. And one of the nice hybrid models you get is this combination of tracing plus", "tokens": [300, 300, 2644, 992, 309, 13, 400, 472, 295, 264, 1481, 13051, 5245, 291, 483, 307, 341, 6562, 295, 25262, 1804], "temperature": 0.0, "avg_logprob": -0.18811801763681266, "compression_ratio": 1.6531365313653137, "no_speech_prob": 5.7717020354175474e-06}, {"id": 388, "seek": 261656, "start": 2641.12, "end": 2643.6, "text": " compilation, which I think could be really interesting. Yeah.", "tokens": [40261, 11, 597, 286, 519, 727, 312, 534, 1880, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.18811801763681266, "compression_ratio": 1.6531365313653137, "no_speech_prob": 5.7717020354175474e-06}, {"id": 389, "seek": 264360, "start": 2643.6, "end": 2649.2, "text": " I'm Sayid S. What's going on with MLNR? There's tons of stuff going on. It's really exciting.", "tokens": [286, 478, 6463, 327, 318, 13, 708, 311, 516, 322, 365, 21601, 45, 49, 30, 821, 311, 9131, 295, 1507, 516, 322, 13, 467, 311, 534, 4670, 13], "temperature": 0.0, "avg_logprob": -0.17850566679431545, "compression_ratio": 1.6341463414634145, "no_speech_prob": 9.367894563183654e-06}, {"id": 390, "seek": 264360, "start": 2649.7599999999998, "end": 2654.48, "text": " Just yesterday, there was a really fantastic talk from some folks at Intel talking about their", "tokens": [1449, 5186, 11, 456, 390, 257, 534, 5456, 751, 490, 512, 4024, 412, 19762, 1417, 466, 641], "temperature": 0.0, "avg_logprob": -0.17850566679431545, "compression_ratio": 1.6341463414634145, "no_speech_prob": 9.367894563183654e-06}, {"id": 391, "seek": 264360, "start": 2654.48, "end": 2658.4, "text": " code generation algorithm they're bringing over to MLNR, which I'm really, really, really excited", "tokens": [3089, 5125, 9284, 436, 434, 5062, 670, 281, 21601, 45, 49, 11, 597, 286, 478, 534, 11, 534, 11, 534, 2919], "temperature": 0.0, "avg_logprob": -0.17850566679431545, "compression_ratio": 1.6341463414634145, "no_speech_prob": 9.367894563183654e-06}, {"id": 392, "seek": 264360, "start": 2658.4, "end": 2666.3199999999997, "text": " about. And so there's tons of stuff going on. Getting the ideal code gen for NVIDIA GPUs,", "tokens": [466, 13, 400, 370, 456, 311, 9131, 295, 1507, 516, 322, 13, 13674, 264, 7157, 3089, 1049, 337, 426, 3958, 6914, 18407, 82, 11], "temperature": 0.0, "avg_logprob": -0.17850566679431545, "compression_ratio": 1.6341463414634145, "no_speech_prob": 9.367894563183654e-06}, {"id": 393, "seek": 264360, "start": 2666.3199999999997, "end": 2671.7599999999998, "text": " for example, is probably still six plus months away. And I don't know how much plus that is.", "tokens": [337, 1365, 11, 307, 1391, 920, 2309, 1804, 2493, 1314, 13, 400, 286, 500, 380, 458, 577, 709, 1804, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.17850566679431545, "compression_ratio": 1.6341463414634145, "no_speech_prob": 9.367894563183654e-06}, {"id": 394, "seek": 267176, "start": 2671.76, "end": 2677.6800000000003, "text": " But what I'm encouraging is the community to come together and collaborate instead of the different", "tokens": [583, 437, 286, 478, 14580, 307, 264, 1768, 281, 808, 1214, 293, 18338, 2602, 295, 264, 819], "temperature": 0.0, "avg_logprob": -0.2070596304284521, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.8054250176646747e-05}, {"id": 395, "seek": 267176, "start": 2677.6800000000003, "end": 2684.0800000000004, "text": " teams of the different companies being in front of me. And the Intel stuff that they presented", "tokens": [5491, 295, 264, 819, 3431, 885, 294, 1868, 295, 385, 13, 400, 264, 19762, 1507, 300, 436, 8212], "temperature": 0.0, "avg_logprob": -0.2070596304284521, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.8054250176646747e-05}, {"id": 396, "seek": 267176, "start": 2684.0800000000004, "end": 2689.36, "text": " yesterday is super, super impressive. So we'll see what happens with that.", "tokens": [5186, 307, 1687, 11, 1687, 8992, 13, 407, 321, 603, 536, 437, 2314, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.2070596304284521, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.8054250176646747e-05}, {"id": 397, "seek": 267176, "start": 2689.36, "end": 2697.5200000000004, "text": " The other thing I might mention in terms of tails on the other side, what's life like in the Python", "tokens": [440, 661, 551, 286, 1062, 2152, 294, 2115, 295, 28537, 322, 264, 661, 1252, 11, 437, 311, 993, 411, 294, 264, 15329], "temperature": 0.0, "avg_logprob": -0.2070596304284521, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.8054250176646747e-05}, {"id": 398, "seek": 269752, "start": 2697.52, "end": 2705.28, "text": " world? Things that aren't working well over there. The kind of the answer to switch for TensorFlow", "tokens": [1002, 30, 9514, 300, 3212, 380, 1364, 731, 670, 456, 13, 440, 733, 295, 264, 1867, 281, 3679, 337, 37624], "temperature": 0.0, "avg_logprob": -0.2469830314318339, "compression_ratio": 1.623931623931624, "no_speech_prob": 8.609912038082257e-05}, {"id": 399, "seek": 269752, "start": 2705.28, "end": 2715.04, "text": " in the PyTorch world is JIT. So it's basically to trace your Python code and attempt to figure out", "tokens": [294, 264, 9953, 51, 284, 339, 1002, 307, 508, 3927, 13, 407, 309, 311, 1936, 281, 13508, 428, 15329, 3089, 293, 5217, 281, 2573, 484], "temperature": 0.0, "avg_logprob": -0.2469830314318339, "compression_ratio": 1.623931623931624, "no_speech_prob": 8.609912038082257e-05}, {"id": 400, "seek": 269752, "start": 2715.04, "end": 2721.28, "text": " what it's doing and create what they call TorchScript, which is a dialect subset of Python.", "tokens": [437, 309, 311, 884, 293, 1884, 437, 436, 818, 7160, 339, 14237, 11, 597, 307, 257, 24652, 25993, 295, 15329, 13], "temperature": 0.0, "avg_logprob": -0.2469830314318339, "compression_ratio": 1.623931623931624, "no_speech_prob": 8.609912038082257e-05}, {"id": 401, "seek": 269752, "start": 2721.84, "end": 2725.84, "text": " Or else to actually parse your Python code is also an option and turn it into TorchScript.", "tokens": [1610, 1646, 281, 767, 48377, 428, 15329, 3089, 307, 611, 364, 3614, 293, 1261, 309, 666, 7160, 339, 14237, 13], "temperature": 0.0, "avg_logprob": -0.2469830314318339, "compression_ratio": 1.623931623931624, "no_speech_prob": 8.609912038082257e-05}, {"id": 402, "seek": 272584, "start": 2725.84, "end": 2735.6000000000004, "text": " It has reached the point now where it can actually be used for good. So one of our students", "tokens": [467, 575, 6488, 264, 935, 586, 689, 309, 393, 767, 312, 1143, 337, 665, 13, 407, 472, 295, 527, 1731], "temperature": 0.0, "avg_logprob": -0.12648802364573758, "compression_ratio": 1.6238938053097345, "no_speech_prob": 9.36659489525482e-06}, {"id": 403, "seek": 272584, "start": 2736.56, "end": 2741.84, "text": " created a bunch of our students actually have been working on a thing called Mesh.", "tokens": [2942, 257, 3840, 295, 527, 1731, 767, 362, 668, 1364, 322, 257, 551, 1219, 376, 14935, 13], "temperature": 0.0, "avg_logprob": -0.12648802364573758, "compression_ratio": 1.6238938053097345, "no_speech_prob": 9.36659489525482e-06}, {"id": 404, "seek": 272584, "start": 2742.88, "end": 2748.0, "text": " And including a young researcher who designed the original thing. It's a very nice activation", "tokens": [400, 3009, 257, 2037, 21751, 567, 4761, 264, 3380, 551, 13, 467, 311, 257, 588, 1481, 24433], "temperature": 0.0, "avg_logprob": -0.12648802364573758, "compression_ratio": 1.6238938053097345, "no_speech_prob": 9.36659489525482e-06}, {"id": 405, "seek": 272584, "start": 2748.0, "end": 2752.56, "text": " function that's outperforming everything else that anybody's trying it on. And it was pretty slow.", "tokens": [2445, 300, 311, 484, 26765, 278, 1203, 1646, 300, 4472, 311, 1382, 309, 322, 13, 400, 309, 390, 1238, 2964, 13], "temperature": 0.0, "avg_logprob": -0.12648802364573758, "compression_ratio": 1.6238938053097345, "no_speech_prob": 9.36659489525482e-06}, {"id": 406, "seek": 275256, "start": 2752.56, "end": 2758.16, "text": " And when we just, you know, it took me half an hour to create a JIT version. And it ran at the", "tokens": [400, 562, 321, 445, 11, 291, 458, 11, 309, 1890, 385, 1922, 364, 1773, 281, 1884, 257, 508, 3927, 3037, 13, 400, 309, 5872, 412, 264], "temperature": 0.0, "avg_logprob": -0.212603489557902, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.771234100393485e-06}, {"id": 407, "seek": 275256, "start": 2758.16, "end": 2764.0, "text": " same speed as somebody else's hand created CUDA code. So for like small things like that, where", "tokens": [912, 3073, 382, 2618, 1646, 311, 1011, 2942, 29777, 7509, 3089, 13, 407, 337, 411, 1359, 721, 411, 300, 11, 689], "temperature": 0.0, "avg_logprob": -0.212603489557902, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.771234100393485e-06}, {"id": 408, "seek": 275256, "start": 2764.0, "end": 2769.2799999999997, "text": " it's like two or three lines of code, that's working pretty well. Although for bigger things,", "tokens": [309, 311, 411, 732, 420, 1045, 3876, 295, 3089, 11, 300, 311, 1364, 1238, 731, 13, 5780, 337, 3801, 721, 11], "temperature": 0.0, "avg_logprob": -0.212603489557902, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.771234100393485e-06}, {"id": 409, "seek": 275256, "start": 2769.2799999999997, "end": 2775.36, "text": " like a new batch norm implementation we tried to do during the last course, the performance wasn't", "tokens": [411, 257, 777, 15245, 2026, 11420, 321, 3031, 281, 360, 1830, 264, 1036, 1164, 11, 264, 3389, 2067, 380], "temperature": 0.0, "avg_logprob": -0.212603489557902, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.771234100393485e-06}, {"id": 410, "seek": 275256, "start": 2775.36, "end": 2782.08, "text": " there. Or if we actually tried to take like one of the big problems at the moment, not the", "tokens": [456, 13, 1610, 498, 321, 767, 3031, 281, 747, 411, 472, 295, 264, 955, 2740, 412, 264, 1623, 11, 406, 264], "temperature": 0.0, "avg_logprob": -0.212603489557902, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.771234100393485e-06}, {"id": 411, "seek": 278208, "start": 2782.08, "end": 2789.04, "text": " Python, but the whole world of non-Google people, is that the best computer vision models by far", "tokens": [15329, 11, 457, 264, 1379, 1002, 295, 2107, 12, 12104, 3127, 561, 11, 307, 300, 264, 1151, 3820, 5201, 5245, 538, 1400], "temperature": 0.0, "avg_logprob": -0.2304928418502067, "compression_ratio": 1.5289256198347108, "no_speech_prob": 1.6439093087683432e-05}, {"id": 412, "seek": 278208, "start": 2789.04, "end": 2792.7999999999997, "text": " are largely those that have been coming out of Google, like Efficient Nets, Mixed Nets,", "tokens": [366, 11611, 729, 300, 362, 668, 1348, 484, 295, 3329, 11, 411, 462, 7816, 426, 1385, 11, 12769, 292, 426, 1385, 11], "temperature": 0.0, "avg_logprob": -0.2304928418502067, "compression_ratio": 1.5289256198347108, "no_speech_prob": 1.6439093087683432e-05}, {"id": 413, "seek": 278208, "start": 2792.7999999999997, "end": 2801.12, "text": " like Kwok Lee's team. They run very slowly and with a lot of memory on GPUs. And so we tried", "tokens": [411, 43432, 453, 6957, 311, 1469, 13, 814, 1190, 588, 5692, 293, 365, 257, 688, 295, 4675, 322, 18407, 82, 13, 400, 370, 321, 3031], "temperature": 0.0, "avg_logprob": -0.2304928418502067, "compression_ratio": 1.5289256198347108, "no_speech_prob": 1.6439093087683432e-05}, {"id": 414, "seek": 278208, "start": 2801.92, "end": 2808.0, "text": " wrapping an entire Efficient Net and Mixed Net into a JITed thing so it wouldn't be so slow.", "tokens": [21993, 364, 2302, 462, 7816, 6188, 293, 12769, 292, 6188, 666, 257, 508, 3927, 292, 551, 370, 309, 2759, 380, 312, 370, 2964, 13], "temperature": 0.0, "avg_logprob": -0.2304928418502067, "compression_ratio": 1.5289256198347108, "no_speech_prob": 1.6439093087683432e-05}, {"id": 415, "seek": 280800, "start": 2808.0, "end": 2812.56, "text": " The Mixed Net didn't work at all and the Efficient Net was a little bit slower.", "tokens": [440, 12769, 292, 6188, 994, 380, 589, 412, 439, 293, 264, 462, 7816, 6188, 390, 257, 707, 857, 14009, 13], "temperature": 0.0, "avg_logprob": -0.17458280898232498, "compression_ratio": 1.5957446808510638, "no_speech_prob": 5.862244051968446e-06}, {"id": 416, "seek": 280800, "start": 2813.84, "end": 2819.68, "text": " So that's kind of the status of JIT in PyTorch is, you know, bits of it are useful.", "tokens": [407, 300, 311, 733, 295, 264, 6558, 295, 508, 3927, 294, 9953, 51, 284, 339, 307, 11, 291, 458, 11, 9239, 295, 309, 366, 4420, 13], "temperature": 0.0, "avg_logprob": -0.17458280898232498, "compression_ratio": 1.5957446808510638, "no_speech_prob": 5.862244051968446e-06}, {"id": 417, "seek": 280800, "start": 2820.64, "end": 2825.76, "text": " The way I look at this from the compiler code generation 8 pieces, that I think the MLAir pieces", "tokens": [440, 636, 286, 574, 412, 341, 490, 264, 31958, 3089, 5125, 1649, 3755, 11, 300, 286, 519, 264, 376, 11435, 347, 3755], "temperature": 0.0, "avg_logprob": -0.17458280898232498, "compression_ratio": 1.5957446808510638, "no_speech_prob": 5.862244051968446e-06}, {"id": 418, "seek": 280800, "start": 2825.76, "end": 2831.04, "text": " are all going the right direction, they're just going to take a while to get here. XLA, as far as", "tokens": [366, 439, 516, 264, 558, 3513, 11, 436, 434, 445, 516, 281, 747, 257, 1339, 281, 483, 510, 13, 1783, 11435, 11, 382, 1400, 382], "temperature": 0.0, "avg_logprob": -0.17458280898232498, "compression_ratio": 1.5957446808510638, "no_speech_prob": 5.862244051968446e-06}, {"id": 419, "seek": 280800, "start": 2831.04, "end": 2836.16, "text": " I know, is state-of-the-art in code generation. For the things it does, it does quite well.", "tokens": [286, 458, 11, 307, 1785, 12, 2670, 12, 3322, 12, 446, 294, 3089, 5125, 13, 1171, 264, 721, 309, 775, 11, 309, 775, 1596, 731, 13], "temperature": 0.0, "avg_logprob": -0.17458280898232498, "compression_ratio": 1.5957446808510638, "no_speech_prob": 5.862244051968446e-06}, {"id": 420, "seek": 283616, "start": 2836.16, "end": 2839.8399999999997, "text": " The challenge with it though is that it does have certain limitations like static shapes", "tokens": [440, 3430, 365, 309, 1673, 307, 300, 309, 775, 362, 1629, 15705, 411, 13437, 10854], "temperature": 0.0, "avg_logprob": -0.23320444770481275, "compression_ratio": 1.6936936936936937, "no_speech_prob": 2.8407848731148988e-05}, {"id": 421, "seek": 283616, "start": 2839.8399999999997, "end": 2844.7999999999997, "text": " and the number of offset supports. And so you kind of have to be within its world for it to be useful.", "tokens": [293, 264, 1230, 295, 18687, 9346, 13, 400, 370, 291, 733, 295, 362, 281, 312, 1951, 1080, 1002, 337, 309, 281, 312, 4420, 13], "temperature": 0.0, "avg_logprob": -0.23320444770481275, "compression_ratio": 1.6936936936936937, "no_speech_prob": 2.8407848731148988e-05}, {"id": 422, "seek": 283616, "start": 2846.24, "end": 2849.7599999999998, "text": " But it has a very useful, it has a large subset of the world that it covers very well.", "tokens": [583, 309, 575, 257, 588, 4420, 11, 309, 575, 257, 2416, 25993, 295, 264, 1002, 300, 309, 10538, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.23320444770481275, "compression_ratio": 1.6936936936936937, "no_speech_prob": 2.8407848731148988e-05}, {"id": 423, "seek": 283616, "start": 2849.7599999999998, "end": 2859.2, "text": " It has a pretty useful world. TorchScript, my understanding is that the base model of TorchScript", "tokens": [467, 575, 257, 1238, 4420, 1002, 13, 7160, 339, 14237, 11, 452, 3701, 307, 300, 264, 3096, 2316, 295, 7160, 339, 14237], "temperature": 0.0, "avg_logprob": -0.23320444770481275, "compression_ratio": 1.6936936936936937, "no_speech_prob": 2.8407848731148988e-05}, {"id": 424, "seek": 285920, "start": 2859.2, "end": 2867.52, "text": " and the interpreter they have, I understand that's quite nice. But the kernel fusion piece", "tokens": [293, 264, 34132, 436, 362, 11, 286, 1223, 300, 311, 1596, 1481, 13, 583, 264, 28256, 23100, 2522], "temperature": 0.0, "avg_logprob": -0.22537164430360537, "compression_ratio": 1.6727272727272726, "no_speech_prob": 4.068979978910647e-05}, {"id": 425, "seek": 285920, "start": 2867.52, "end": 2870.7999999999997, "text": " is still fairly early on, I suppose, in element-wise operations, for example.", "tokens": [307, 920, 6457, 2440, 322, 11, 286, 7297, 11, 294, 4478, 12, 3711, 7705, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.22537164430360537, "compression_ratio": 1.6727272727272726, "no_speech_prob": 4.068979978910647e-05}, {"id": 426, "seek": 285920, "start": 2870.7999999999997, "end": 2876.72, "text": " I don't find them that quite nice. I mean, like simple things like, they can't, partly a limitation", "tokens": [286, 500, 380, 915, 552, 300, 1596, 1481, 13, 286, 914, 11, 411, 2199, 721, 411, 11, 436, 393, 380, 11, 17031, 257, 27432], "temperature": 0.0, "avg_logprob": -0.22537164430360537, "compression_ratio": 1.6727272727272726, "no_speech_prob": 4.068979978910647e-05}, {"id": 427, "seek": 285920, "start": 2876.72, "end": 2882.48, "text": " of the Python type system. So like, you want to be able to write things that can work with different", "tokens": [295, 264, 15329, 2010, 1185, 13, 407, 411, 11, 291, 528, 281, 312, 1075, 281, 2464, 721, 300, 393, 589, 365, 819], "temperature": 0.0, "avg_logprob": -0.22537164430360537, "compression_ratio": 1.6727272727272726, "no_speech_prob": 4.068979978910647e-05}, {"id": 428, "seek": 285920, "start": 2882.48, "end": 2887.3599999999997, "text": " numbers of channels while you're out of luck because they use Python type imitations which", "tokens": [3547, 295, 9235, 1339, 291, 434, 484, 295, 3668, 570, 436, 764, 15329, 2010, 566, 31265, 597], "temperature": 0.0, "avg_logprob": -0.22537164430360537, "compression_ratio": 1.6727272727272726, "no_speech_prob": 4.068979978910647e-05}, {"id": 429, "seek": 288736, "start": 2887.36, "end": 2892.56, "text": " have no way of saying it's a tuple of size n. You have to say it's a tuple of size three.", "tokens": [362, 572, 636, 295, 1566, 309, 311, 257, 2604, 781, 295, 2744, 297, 13, 509, 362, 281, 584, 309, 311, 257, 2604, 781, 295, 2744, 1045, 13], "temperature": 0.0, "avg_logprob": -0.13088995265209769, "compression_ratio": 1.660649819494585, "no_speech_prob": 5.422032245405717e-06}, {"id": 430, "seek": 288736, "start": 2892.56, "end": 2895.1200000000003, "text": " So you then have to hard code all these assumptions into your code.", "tokens": [407, 291, 550, 362, 281, 1152, 3089, 439, 613, 17695, 666, 428, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13088995265209769, "compression_ratio": 1.660649819494585, "no_speech_prob": 5.422032245405717e-06}, {"id": 431, "seek": 288736, "start": 2896.1600000000003, "end": 2898.7200000000003, "text": " Lots of stuff I find pretty frustrating.", "tokens": [15908, 295, 1507, 286, 915, 1238, 16522, 13], "temperature": 0.0, "avg_logprob": -0.13088995265209769, "compression_ratio": 1.660649819494585, "no_speech_prob": 5.422032245405717e-06}, {"id": 432, "seek": 288736, "start": 2898.7200000000003, "end": 2902.4, "text": " I see. Interesting. Well, so I mean, I think there's other spaces that I'm", "tokens": [286, 536, 13, 14711, 13, 1042, 11, 370, 286, 914, 11, 286, 519, 456, 311, 661, 7673, 300, 286, 478], "temperature": 0.0, "avg_logprob": -0.13088995265209769, "compression_ratio": 1.660649819494585, "no_speech_prob": 5.422032245405717e-06}, {"id": 433, "seek": 288736, "start": 2902.4, "end": 2908.56, "text": " eager to reevaluate as, I mean, this isn't the highest priority at this moment. But in terms of", "tokens": [18259, 281, 43060, 3337, 10107, 382, 11, 286, 914, 11, 341, 1943, 380, 264, 6343, 9365, 412, 341, 1623, 13, 583, 294, 2115, 295], "temperature": 0.0, "avg_logprob": -0.13088995265209769, "compression_ratio": 1.660649819494585, "no_speech_prob": 5.422032245405717e-06}, {"id": 434, "seek": 288736, "start": 2908.56, "end": 2913.6800000000003, "text": " our APIs, there's still very legit questions around, should we encode D type in the static", "tokens": [527, 21445, 11, 456, 311, 920, 588, 10275, 1651, 926, 11, 820, 321, 2058, 1429, 413, 2010, 294, 264, 13437], "temperature": 0.0, "avg_logprob": -0.13088995265209769, "compression_ratio": 1.660649819494585, "no_speech_prob": 5.422032245405717e-06}, {"id": 435, "seek": 291368, "start": 2913.68, "end": 2918.7999999999997, "text": " type system or should we just say tensor? And if you just say tensor, then you get rid of all the", "tokens": [2010, 1185, 420, 820, 321, 445, 584, 40863, 30, 400, 498, 291, 445, 584, 40863, 11, 550, 291, 483, 3973, 295, 439, 264], "temperature": 0.0, "avg_logprob": -0.13280447255010192, "compression_ratio": 1.6750902527075813, "no_speech_prob": 2.3551641788799316e-05}, {"id": 436, "seek": 291368, "start": 2918.7999999999997, "end": 2924.7999999999997, "text": " generics everywhere, cleans up tons of code at the cost of losing some of the checking.", "tokens": [1337, 1167, 5315, 11, 16912, 493, 9131, 295, 3089, 412, 264, 2063, 295, 7027, 512, 295, 264, 8568, 13], "temperature": 0.0, "avg_logprob": -0.13280447255010192, "compression_ratio": 1.6750902527075813, "no_speech_prob": 2.3551641788799316e-05}, {"id": 437, "seek": 291368, "start": 2926.0, "end": 2930.56, "text": " But then I think if you go with more semantic tensor types, as Jerry was pushing forward,", "tokens": [583, 550, 286, 519, 498, 291, 352, 365, 544, 47982, 40863, 3467, 11, 382, 17454, 390, 7380, 2128, 11], "temperature": 0.0, "avg_logprob": -0.13280447255010192, "compression_ratio": 1.6750902527075813, "no_speech_prob": 2.3551641788799316e-05}, {"id": 438, "seek": 291368, "start": 2930.56, "end": 2934.48, "text": " you actually really don't even want the D type. What you want is the semantics and that you're", "tokens": [291, 767, 534, 500, 380, 754, 528, 264, 413, 2010, 13, 708, 291, 528, 307, 264, 4361, 45298, 293, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.13280447255010192, "compression_ratio": 1.6750902527075813, "no_speech_prob": 2.3551641788799316e-05}, {"id": 439, "seek": 291368, "start": 2934.48, "end": 2939.2, "text": " actually in a better spot. Right. Like for big precision, we're switching stuff from one type", "tokens": [767, 294, 257, 1101, 4008, 13, 1779, 13, 1743, 337, 955, 18356, 11, 321, 434, 16493, 1507, 490, 472, 2010], "temperature": 0.0, "avg_logprob": -0.13280447255010192, "compression_ratio": 1.6750902527075813, "no_speech_prob": 2.3551641788799316e-05}, {"id": 440, "seek": 293920, "start": 2939.2, "end": 2943.52, "text": " to another all the time. Depending on whether you're doing a loss function or a gradient", "tokens": [281, 1071, 439, 264, 565, 13, 22539, 322, 1968, 291, 434, 884, 257, 4470, 2445, 420, 257, 16235], "temperature": 0.0, "avg_logprob": -0.11786537965138753, "compression_ratio": 1.6925675675675675, "no_speech_prob": 3.138035481242696e-06}, {"id": 441, "seek": 293920, "start": 2943.52, "end": 2947.12, "text": " calculation or whatever, you need to be changing between half and single.", "tokens": [17108, 420, 2035, 11, 291, 643, 281, 312, 4473, 1296, 1922, 293, 2167, 13], "temperature": 0.0, "avg_logprob": -0.11786537965138753, "compression_ratio": 1.6925675675675675, "no_speech_prob": 3.138035481242696e-06}, {"id": 442, "seek": 293920, "start": 2948.0, "end": 2951.3599999999997, "text": " So if we went that direction, I think that would be really interesting", "tokens": [407, 498, 321, 1437, 300, 3513, 11, 286, 519, 300, 576, 312, 534, 1880], "temperature": 0.0, "avg_logprob": -0.11786537965138753, "compression_ratio": 1.6925675675675675, "no_speech_prob": 3.138035481242696e-06}, {"id": 443, "seek": 293920, "start": 2952.3999999999996, "end": 2955.9199999999996, "text": " in terms of ergonomics, but also simplification, which I think would be great.", "tokens": [294, 2115, 295, 42735, 29884, 11, 457, 611, 6883, 3774, 11, 597, 286, 519, 576, 312, 869, 13], "temperature": 0.0, "avg_logprob": -0.11786537965138753, "compression_ratio": 1.6925675675675675, "no_speech_prob": 3.138035481242696e-06}, {"id": 444, "seek": 293920, "start": 2956.72, "end": 2961.12, "text": " It would also, like your point about the optimizers, the key path have all kinds of weirdness because", "tokens": [467, 576, 611, 11, 411, 428, 935, 466, 264, 5028, 22525, 11, 264, 2141, 3100, 362, 439, 3685, 295, 3657, 1287, 570], "temperature": 0.0, "avg_logprob": -0.11786537965138753, "compression_ratio": 1.6925675675675675, "no_speech_prob": 3.138035481242696e-06}, {"id": 445, "seek": 293920, "start": 2961.12, "end": 2965.6, "text": " you have multiple D types and you want to be generic over D type. And so that's really", "tokens": [291, 362, 3866, 413, 3467, 293, 291, 528, 281, 312, 19577, 670, 413, 2010, 13, 400, 370, 300, 311, 534], "temperature": 0.0, "avg_logprob": -0.11786537965138753, "compression_ratio": 1.6925675675675675, "no_speech_prob": 3.138035481242696e-06}, {"id": 446, "seek": 296560, "start": 2965.6, "end": 2972.4, "text": " unpleasant right now. Yeah. I think also like for Swift wanting to bring over", "tokens": [29128, 558, 586, 13, 865, 13, 286, 519, 611, 411, 337, 25539, 7935, 281, 1565, 670], "temperature": 0.0, "avg_logprob": -0.17013661258191948, "compression_ratio": 1.5, "no_speech_prob": 1.5444957170984708e-05}, {"id": 447, "seek": 296560, "start": 2973.52, "end": 2982.0, "text": " big world of Python using data scientists, they're definitely not going to be wanting to put", "tokens": [955, 1002, 295, 15329, 1228, 1412, 7708, 11, 436, 434, 2138, 406, 516, 281, 312, 7935, 281, 829], "temperature": 0.0, "avg_logprob": -0.17013661258191948, "compression_ratio": 1.5, "no_speech_prob": 1.5444957170984708e-05}, {"id": 448, "seek": 296560, "start": 2982.0, "end": 2987.12, "text": " lots and lots of verbose generic type annotations in their Jupyter notebooks.", "tokens": [3195, 293, 3195, 295, 9595, 541, 19577, 2010, 25339, 763, 294, 641, 22125, 88, 391, 43782, 13], "temperature": 0.0, "avg_logprob": -0.17013661258191948, "compression_ratio": 1.5, "no_speech_prob": 1.5444957170984708e-05}, {"id": 449, "seek": 296560, "start": 2987.68, "end": 2994.3199999999997, "text": " Yeah. So I don't know when we'll have cycles to re-evaluate those APIs, but", "tokens": [865, 13, 407, 286, 500, 380, 458, 562, 321, 603, 362, 17796, 281, 319, 12, 68, 3337, 10107, 729, 21445, 11, 457], "temperature": 0.0, "avg_logprob": -0.17013661258191948, "compression_ratio": 1.5, "no_speech_prob": 1.5444957170984708e-05}, {"id": 450, "seek": 299432, "start": 2994.32, "end": 3001.1200000000003, "text": " I think we should go do a fresh take of this and combine it with an XLA based approach.", "tokens": [286, 519, 321, 820, 352, 360, 257, 4451, 747, 295, 341, 293, 10432, 309, 365, 364, 1783, 11435, 2361, 3109, 13], "temperature": 0.0, "avg_logprob": -0.2570889920604472, "compression_ratio": 1.5738396624472575, "no_speech_prob": 6.540299636981217e-06}, {"id": 451, "seek": 299432, "start": 3001.1200000000003, "end": 3004.2400000000002, "text": " It changes a lot of the trade-offs. Right.", "tokens": [467, 2962, 257, 688, 295, 264, 4923, 12, 19231, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.2570889920604472, "compression_ratio": 1.5738396624472575, "no_speech_prob": 6.540299636981217e-06}, {"id": 452, "seek": 299432, "start": 3005.76, "end": 3008.96, "text": " It would be really interesting. Yeah. I mean, I think in my mind,", "tokens": [467, 576, 312, 534, 1880, 13, 865, 13, 286, 914, 11, 286, 519, 294, 452, 1575, 11], "temperature": 0.0, "avg_logprob": -0.2570889920604472, "compression_ratio": 1.5738396624472575, "no_speech_prob": 6.540299636981217e-06}, {"id": 453, "seek": 299432, "start": 3008.96, "end": 3013.36, "text": " right, so a couple of weeks ago, I presented the layering proposal to separate out lib", "tokens": [558, 11, 370, 257, 1916, 295, 3259, 2057, 11, 286, 8212, 264, 40754, 11494, 281, 4994, 484, 22854], "temperature": 0.0, "avg_logprob": -0.2570889920604472, "compression_ratio": 1.5738396624472575, "no_speech_prob": 6.540299636981217e-06}, {"id": 454, "seek": 299432, "start": 3013.36, "end": 3018.96, "text": " tensor from lib deep learning so that we can then get the freedom to then iterate at that", "tokens": [40863, 490, 22854, 2452, 2539, 370, 300, 321, 393, 550, 483, 264, 5645, 281, 550, 44497, 412, 300], "temperature": 0.0, "avg_logprob": -0.2570889920604472, "compression_ratio": 1.5738396624472575, "no_speech_prob": 6.540299636981217e-06}, {"id": 455, "seek": 301896, "start": 3018.96, "end": 3025.2, "text": " level and have multiple explorations on top. So the progress update on there is I've started,", "tokens": [1496, 293, 362, 3866, 24765, 763, 322, 1192, 13, 407, 264, 4205, 5623, 322, 456, 307, 286, 600, 1409, 11], "temperature": 0.0, "avg_logprob": -0.20453643798828125, "compression_ratio": 1.5752508361204014, "no_speech_prob": 3.2376867693528766e-06}, {"id": 456, "seek": 301896, "start": 3026.16, "end": 3032.48, "text": " we have the two different packages now in Swift DPI's so you can depend only on one as opposed", "tokens": [321, 362, 264, 732, 819, 17401, 586, 294, 25539, 413, 31701, 311, 370, 291, 393, 5672, 787, 322, 472, 382, 8851], "temperature": 0.0, "avg_logprob": -0.20453643798828125, "compression_ratio": 1.5752508361204014, "no_speech_prob": 3.2376867693528766e-06}, {"id": 457, "seek": 301896, "start": 3032.48, "end": 3037.68, "text": " to the other. And Dan helped fix all the issues that I caused while doing the initial move of", "tokens": [281, 264, 661, 13, 400, 3394, 4254, 3191, 439, 264, 2663, 300, 286, 7008, 1339, 884, 264, 5883, 1286, 295], "temperature": 0.0, "avg_logprob": -0.20453643798828125, "compression_ratio": 1.5752508361204014, "no_speech_prob": 3.2376867693528766e-06}, {"id": 458, "seek": 301896, "start": 3037.68, "end": 3043.68, "text": " the random number generators out of what will become lib deep learning. That said, it's still", "tokens": [264, 4974, 1230, 38662, 484, 295, 437, 486, 1813, 22854, 2452, 2539, 13, 663, 848, 11, 309, 311, 920], "temperature": 0.0, "avg_logprob": -0.20453643798828125, "compression_ratio": 1.5752508361204014, "no_speech_prob": 3.2376867693528766e-06}, {"id": 459, "seek": 301896, "start": 3043.68, "end": 3048.16, "text": " very early and I have a lot more code to move. Well, I think that Jeremy is like fundamentally", "tokens": [588, 2440, 293, 286, 362, 257, 688, 544, 3089, 281, 1286, 13, 1042, 11, 286, 519, 300, 17809, 307, 411, 17879], "temperature": 0.0, "avg_logprob": -0.20453643798828125, "compression_ratio": 1.5752508361204014, "no_speech_prob": 3.2376867693528766e-06}, {"id": 460, "seek": 304816, "start": 3048.16, "end": 3053.3599999999997, "text": " right that we need to spend more time with software and the optimizer designs and re-evaluate the", "tokens": [558, 300, 321, 643, 281, 3496, 544, 565, 365, 4722, 293, 264, 5028, 6545, 11347, 293, 319, 12, 68, 3337, 10107, 264], "temperature": 0.0, "avg_logprob": -0.23696815420728212, "compression_ratio": 1.661596958174905, "no_speech_prob": 7.570941420453892e-07}, {"id": 461, "seek": 304816, "start": 3053.3599999999997, "end": 3059.3599999999997, "text": " training loop callback systems and things like that. Yeah. As each of these variables change,", "tokens": [3097, 6367, 818, 3207, 3652, 293, 721, 411, 300, 13, 865, 13, 1018, 1184, 295, 613, 9102, 1319, 11], "temperature": 0.0, "avg_logprob": -0.23696815420728212, "compression_ratio": 1.661596958174905, "no_speech_prob": 7.570941420453892e-07}, {"id": 462, "seek": 304816, "start": 3060.72, "end": 3065.12, "text": " like it affects other parts of the system and different trade-offs, I think should be", "tokens": [411, 309, 11807, 661, 3166, 295, 264, 1185, 293, 819, 4923, 12, 19231, 11, 286, 519, 820, 312], "temperature": 0.0, "avg_logprob": -0.23696815420728212, "compression_ratio": 1.661596958174905, "no_speech_prob": 7.570941420453892e-07}, {"id": 463, "seek": 304816, "start": 3065.12, "end": 3072.3199999999997, "text": " re-evaluated as we do that. But I think that getting AD like bulletproof is like super important.", "tokens": [319, 12, 68, 3337, 27275, 382, 321, 360, 300, 13, 583, 286, 519, 300, 1242, 9135, 411, 11632, 15690, 307, 411, 1687, 1021, 13], "temperature": 0.0, "avg_logprob": -0.23696815420728212, "compression_ratio": 1.661596958174905, "no_speech_prob": 7.570941420453892e-07}, {"id": 464, "seek": 304816, "start": 3072.3199999999997, "end": 3075.6, "text": " And performance. Yeah. We have to get those two things right.", "tokens": [400, 3389, 13, 865, 13, 492, 362, 281, 483, 729, 732, 721, 558, 13], "temperature": 0.0, "avg_logprob": -0.23696815420728212, "compression_ratio": 1.661596958174905, "no_speech_prob": 7.570941420453892e-07}, {"id": 465, "seek": 307560, "start": 3075.6, "end": 3080.88, "text": " Well, and upstream and integrate into Swift so that we can build on it and take it for granted.", "tokens": [1042, 11, 293, 33915, 293, 13365, 666, 25539, 370, 300, 321, 393, 1322, 322, 309, 293, 747, 309, 337, 12344, 13], "temperature": 0.0, "avg_logprob": -0.2138657763721497, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.644148323975969e-05}, {"id": 466, "seek": 307560, "start": 3085.7599999999998, "end": 3091.36, "text": " Quick question about tensor without D type. I wonder if they would add any type assertions", "tokens": [12101, 1168, 466, 40863, 1553, 413, 2010, 13, 286, 2441, 498, 436, 576, 909, 604, 2010, 19810, 626], "temperature": 0.0, "avg_logprob": -0.2138657763721497, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.644148323975969e-05}, {"id": 467, "seek": 307560, "start": 3091.36, "end": 3095.92, "text": " and any functions. I think the Python model is to not check things and to let things crash at", "tokens": [293, 604, 6828, 13, 286, 519, 264, 15329, 2316, 307, 281, 406, 1520, 721, 293, 281, 718, 721, 8252, 412], "temperature": 0.0, "avg_logprob": -0.2138657763721497, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.644148323975969e-05}, {"id": 468, "seek": 307560, "start": 3095.92, "end": 3100.24, "text": " runtime, if I understand. So I don't know. I mean, I think that there's a couple of different options", "tokens": [34474, 11, 498, 286, 1223, 13, 407, 286, 500, 380, 458, 13, 286, 914, 11, 286, 519, 300, 456, 311, 257, 1916, 295, 819, 3956], "temperature": 0.0, "avg_logprob": -0.2138657763721497, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.644148323975969e-05}, {"id": 469, "seek": 307560, "start": 3100.24, "end": 3105.2799999999997, "text": " there. I don't know what the right answer is, but again, one of the things that PyTorch is doing is", "tokens": [456, 13, 286, 500, 380, 458, 437, 264, 558, 1867, 307, 11, 457, 797, 11, 472, 295, 264, 721, 300, 9953, 51, 284, 339, 307, 884, 307], "temperature": 0.0, "avg_logprob": -0.2138657763721497, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.644148323975969e-05}, {"id": 470, "seek": 310528, "start": 3105.28, "end": 3110.1600000000003, "text": " they're doing more coercions with D types. So if you take into account that into an N32, it will", "tokens": [436, 434, 884, 544, 49741, 626, 365, 413, 3467, 13, 407, 498, 291, 747, 666, 2696, 300, 666, 364, 426, 11440, 11, 309, 486], "temperature": 0.0, "avg_logprob": -0.21747929255167645, "compression_ratio": 1.7035714285714285, "no_speech_prob": 7.646408448636066e-06}, {"id": 471, "seek": 310528, "start": 3110.1600000000003, "end": 3116.88, "text": " actually promote an 8 into an N32, for example. I mean, rocket science. But that's the kind of", "tokens": [767, 9773, 364, 1649, 666, 364, 426, 11440, 11, 337, 1365, 13, 286, 914, 11, 13012, 3497, 13, 583, 300, 311, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.21747929255167645, "compression_ratio": 1.7035714285714285, "no_speech_prob": 7.646408448636066e-06}, {"id": 472, "seek": 310528, "start": 3116.88, "end": 3122.1600000000003, "text": " thing that is just very nice and it just eliminates a certain kind of error. On the other hand,", "tokens": [551, 300, 307, 445, 588, 1481, 293, 309, 445, 49893, 257, 1629, 733, 295, 6713, 13, 1282, 264, 661, 1011, 11], "temperature": 0.0, "avg_logprob": -0.21747929255167645, "compression_ratio": 1.7035714285714285, "no_speech_prob": 7.646408448636066e-06}, {"id": 473, "seek": 310528, "start": 3122.1600000000003, "end": 3127.2000000000003, "text": " it's kind of like broadcasting where it makes certain things just work at the cost of potentially", "tokens": [309, 311, 733, 295, 411, 30024, 689, 309, 1669, 1629, 721, 445, 589, 412, 264, 2063, 295, 7263], "temperature": 0.0, "avg_logprob": -0.21747929255167645, "compression_ratio": 1.7035714285714285, "no_speech_prob": 7.646408448636066e-06}, {"id": 474, "seek": 310528, "start": 3127.2000000000003, "end": 3132.2400000000002, "text": " getting surprising in some cases. So I don't know about that. I think if you do things that", "tokens": [1242, 8830, 294, 512, 3331, 13, 407, 286, 500, 380, 458, 466, 300, 13, 286, 519, 498, 291, 360, 721, 300], "temperature": 0.0, "avg_logprob": -0.21747929255167645, "compression_ratio": 1.7035714285714285, "no_speech_prob": 7.646408448636066e-06}, {"id": 475, "seek": 313224, "start": 3132.24, "end": 3139.2799999999997, "text": " don't make sense, like you try to do a floating point operation on an integer, then you would want", "tokens": [500, 380, 652, 2020, 11, 411, 291, 853, 281, 360, 257, 12607, 935, 6916, 322, 364, 24922, 11, 550, 291, 576, 528], "temperature": 0.0, "avg_logprob": -0.2690526431368798, "compression_ratio": 1.534136546184739, "no_speech_prob": 1.618714304640889e-05}, {"id": 476, "seek": 313224, "start": 3139.2799999999997, "end": 3144.4799999999996, "text": " it to be a runtime error. I think that our model is trying towards a much more runtime-centric", "tokens": [309, 281, 312, 257, 34474, 6713, 13, 286, 519, 300, 527, 2316, 307, 1382, 3030, 257, 709, 544, 34474, 12, 45300], "temperature": 0.0, "avg_logprob": -0.2690526431368798, "compression_ratio": 1.534136546184739, "no_speech_prob": 1.618714304640889e-05}, {"id": 477, "seek": 313224, "start": 3144.4799999999996, "end": 3150.7999999999997, "text": " approach. I think ironically, Swift for Nintzo started out very static, but now it's going very", "tokens": [3109, 13, 286, 519, 41082, 11, 25539, 337, 426, 686, 4765, 1409, 484, 588, 13437, 11, 457, 586, 309, 311, 516, 588], "temperature": 0.0, "avg_logprob": -0.2690526431368798, "compression_ratio": 1.534136546184739, "no_speech_prob": 1.618714304640889e-05}, {"id": 478, "seek": 313224, "start": 3150.7999999999997, "end": 3155.3599999999997, "text": " dynamic. Yeah, for me, I'm realizing one of the major benefits of having FastFast languages,", "tokens": [8546, 13, 865, 11, 337, 385, 11, 286, 478, 16734, 472, 295, 264, 2563, 5311, 295, 1419, 15968, 37, 525, 8650, 11], "temperature": 0.0, "avg_logprob": -0.2690526431368798, "compression_ratio": 1.534136546184739, "no_speech_prob": 1.618714304640889e-05}, {"id": 479, "seek": 315536, "start": 3155.36, "end": 3163.52, "text": " like dynamic is free. And so now you can have super dynamic abstractions that you can do these things in a nice way.", "tokens": [411, 8546, 307, 1737, 13, 400, 370, 586, 291, 393, 362, 1687, 8546, 12649, 626, 300, 291, 393, 360, 613, 721, 294, 257, 1481, 636, 13], "temperature": 0.0, "avg_logprob": -0.2375011444091797, "compression_ratio": 1.6496062992125984, "no_speech_prob": 6.643175311182858e-06}, {"id": 480, "seek": 315536, "start": 3163.52, "end": 3169.6800000000003, "text": " And by Torch, you do get a pretty clear runtime error if there's a type mismatch. It doesn't just crash.", "tokens": [400, 538, 7160, 339, 11, 291, 360, 483, 257, 1238, 1850, 34474, 6713, 498, 456, 311, 257, 2010, 23220, 852, 13, 467, 1177, 380, 445, 8252, 13], "temperature": 0.0, "avg_logprob": -0.2375011444091797, "compression_ratio": 1.6496062992125984, "no_speech_prob": 6.643175311182858e-06}, {"id": 481, "seek": 315536, "start": 3169.6800000000003, "end": 3174.32, "text": " It'll tell you what it expected and what it got. Yeah. And one of the nice things about EagerMode is that", "tokens": [467, 603, 980, 291, 437, 309, 5176, 293, 437, 309, 658, 13, 865, 13, 400, 472, 295, 264, 1481, 721, 466, 462, 3557, 44, 1429, 307, 300], "temperature": 0.0, "avg_logprob": -0.2375011444091797, "compression_ratio": 1.6496062992125984, "no_speech_prob": 6.643175311182858e-06}, {"id": 482, "seek": 315536, "start": 3174.32, "end": 3182.48, "text": " then you get a stack trace. I think there are other ways around, instead of encoding things", "tokens": [550, 291, 483, 257, 8630, 13508, 13, 286, 519, 456, 366, 661, 2098, 926, 11, 2602, 295, 43430, 721], "temperature": 0.0, "avg_logprob": -0.2375011444091797, "compression_ratio": 1.6496062992125984, "no_speech_prob": 6.643175311182858e-06}, {"id": 483, "seek": 318248, "start": 3182.48, "end": 3187.28, "text": " into the static type system that you have to adhere to. I think Adam's work on", "tokens": [666, 264, 13437, 2010, 1185, 300, 291, 362, 281, 33584, 281, 13, 286, 519, 7938, 311, 589, 322], "temperature": 0.0, "avg_logprob": -0.15432230631510416, "compression_ratio": 1.7121212121212122, "no_speech_prob": 8.80081734067062e-06}, {"id": 484, "seek": 318248, "start": 3187.28, "end": 3191.28, "text": " integer splitting perfectly shows that you can still get a lot of benefits of static analysis", "tokens": [24922, 30348, 6239, 3110, 300, 291, 393, 920, 483, 257, 688, 295, 5311, 295, 13437, 5215], "temperature": 0.0, "avg_logprob": -0.15432230631510416, "compression_ratio": 1.7121212121212122, "no_speech_prob": 8.80081734067062e-06}, {"id": 485, "seek": 318248, "start": 3191.28, "end": 3198.4, "text": " without necessarily encoding into the type system. That said, I think it's still an open question as to", "tokens": [1553, 4725, 43430, 666, 264, 2010, 1185, 13, 663, 848, 11, 286, 519, 309, 311, 920, 364, 1269, 1168, 382, 281], "temperature": 0.0, "avg_logprob": -0.15432230631510416, "compression_ratio": 1.7121212121212122, "no_speech_prob": 8.80081734067062e-06}, {"id": 486, "seek": 318248, "start": 3198.4, "end": 3204.48, "text": " how far we can really push that and where we end up landing. Yeah, I think it's just a really great", "tokens": [577, 1400, 321, 393, 534, 2944, 300, 293, 689, 321, 917, 493, 11202, 13, 865, 11, 286, 519, 309, 311, 445, 257, 534, 869], "temperature": 0.0, "avg_logprob": -0.15432230631510416, "compression_ratio": 1.7121212121212122, "no_speech_prob": 8.80081734067062e-06}, {"id": 487, "seek": 318248, "start": 3207.6, "end": 3210.08, "text": " opportunity to reevaluate these things as other pieces are coming together.", "tokens": [2650, 281, 43060, 3337, 10107, 613, 721, 382, 661, 3755, 366, 1348, 1214, 13], "temperature": 0.0, "avg_logprob": -0.15432230631510416, "compression_ratio": 1.7121212121212122, "no_speech_prob": 8.80081734067062e-06}, {"id": 488, "seek": 321008, "start": 3210.08, "end": 3216.88, "text": " Maxim asks, why is runtime checking preferable over static analysis? I think it's more that", "tokens": [29076, 8962, 11, 983, 307, 34474, 8568, 4382, 712, 670, 13437, 5215, 30, 286, 519, 309, 311, 544, 300], "temperature": 0.0, "avg_logprob": -0.11186171940394811, "compression_ratio": 1.75, "no_speech_prob": 5.093409981782315e-06}, {"id": 489, "seek": 321008, "start": 3217.7599999999998, "end": 3221.2, "text": " we're still trying to figure out what dimensions you want to be flexible on.", "tokens": [321, 434, 920, 1382, 281, 2573, 484, 437, 12819, 291, 528, 281, 312, 11358, 322, 13], "temperature": 0.0, "avg_logprob": -0.11186171940394811, "compression_ratio": 1.75, "no_speech_prob": 5.093409981782315e-06}, {"id": 490, "seek": 321008, "start": 3221.7599999999998, "end": 3228.0, "text": " And so doing things dynamically is sort of the ultimate in flexibility. And so as we're trying", "tokens": [400, 370, 884, 721, 43492, 307, 1333, 295, 264, 9705, 294, 12635, 13, 400, 370, 382, 321, 434, 1382], "temperature": 0.0, "avg_logprob": -0.11186171940394811, "compression_ratio": 1.75, "no_speech_prob": 5.093409981782315e-06}, {"id": 491, "seek": 321008, "start": 3228.0, "end": 3232.64, "text": " to iterate on the programming model, making sure that things are as dynamic as you want them to be", "tokens": [281, 44497, 322, 264, 9410, 2316, 11, 1455, 988, 300, 721, 366, 382, 8546, 382, 291, 528, 552, 281, 312], "temperature": 0.0, "avg_logprob": -0.11186171940394811, "compression_ratio": 1.75, "no_speech_prob": 5.093409981782315e-06}, {"id": 492, "seek": 321008, "start": 3233.44, "end": 3238.3199999999997, "text": " is sometimes nice. And then we should think about how static analysis can help catch errors sooner.", "tokens": [307, 2171, 1481, 13, 400, 550, 321, 820, 519, 466, 577, 13437, 5215, 393, 854, 3745, 13603, 15324, 13], "temperature": 0.0, "avg_logprob": -0.11186171940394811, "compression_ratio": 1.75, "no_speech_prob": 5.093409981782315e-06}, {"id": 493, "seek": 323832, "start": 3238.32, "end": 3242.7200000000003, "text": " Yeah, exactly. And so this is just a spectrum. And it's not that one end of the spectrum is", "tokens": [865, 11, 2293, 13, 400, 370, 341, 307, 445, 257, 11143, 13, 400, 309, 311, 406, 300, 472, 917, 295, 264, 11143, 307], "temperature": 0.0, "avg_logprob": -0.08723831176757812, "compression_ratio": 1.6008583690987124, "no_speech_prob": 3.119859320577234e-05}, {"id": 494, "seek": 323832, "start": 3242.7200000000003, "end": 3246.6400000000003, "text": " better than the other. It's about where in the spectrum you end up. Nicholas's question,", "tokens": [1101, 813, 264, 661, 13, 467, 311, 466, 689, 294, 264, 11143, 291, 917, 493, 13, 22924, 311, 1168, 11], "temperature": 0.0, "avg_logprob": -0.08723831176757812, "compression_ratio": 1.6008583690987124, "no_speech_prob": 3.119859320577234e-05}, {"id": 495, "seek": 323832, "start": 3246.6400000000003, "end": 3251.6000000000004, "text": " Nicholas asks, how are MLR and XLA related? That is a super complicated question because we're", "tokens": [22924, 8962, 11, 577, 366, 21601, 49, 293, 1783, 11435, 4077, 30, 663, 307, 257, 1687, 6179, 1168, 570, 321, 434], "temperature": 0.0, "avg_logprob": -0.08723831176757812, "compression_ratio": 1.6008583690987124, "no_speech_prob": 3.119859320577234e-05}, {"id": 496, "seek": 323832, "start": 3251.6000000000004, "end": 3257.76, "text": " actively re-implementing pieces of XLA in terms of MLR. So that's actually a lot more complicated", "tokens": [13022, 319, 12, 332, 43704, 278, 3755, 295, 1783, 11435, 294, 2115, 295, 21601, 49, 13, 407, 300, 311, 767, 257, 688, 544, 6179], "temperature": 0.0, "avg_logprob": -0.08723831176757812, "compression_ratio": 1.6008583690987124, "no_speech_prob": 3.119859320577234e-05}, {"id": 497, "seek": 325776, "start": 3257.76, "end": 3268.8, "text": " than it sounds. I would just say that MLR is a broad scale compiler technology that solves", "tokens": [813, 309, 3263, 13, 286, 576, 445, 584, 300, 21601, 49, 307, 257, 4152, 4373, 31958, 2899, 300, 39890], "temperature": 0.0, "avg_logprob": -0.11670539803700904, "compression_ratio": 1.46524064171123, "no_speech_prob": 2.5863584596663713e-05}, {"id": 498, "seek": 325776, "start": 3268.8, "end": 3274.0, "text": " lots of problems. XLA as a name is typically thought of as a thing in terms of sensors and", "tokens": [3195, 295, 2740, 13, 1783, 11435, 382, 257, 1315, 307, 5850, 1194, 295, 382, 257, 551, 294, 2115, 295, 14840, 293], "temperature": 0.0, "avg_logprob": -0.11670539803700904, "compression_ratio": 1.46524064171123, "no_speech_prob": 2.5863584596663713e-05}, {"id": 499, "seek": 325776, "start": 3274.0, "end": 3284.2400000000002, "text": " deficient code. And so I wouldn't over-index on the letters, the number of letters, I guess.", "tokens": [19248, 1196, 3089, 13, 400, 370, 286, 2759, 380, 670, 12, 471, 3121, 322, 264, 7825, 11, 264, 1230, 295, 7825, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.11670539803700904, "compression_ratio": 1.46524064171123, "no_speech_prob": 2.5863584596663713e-05}, {"id": 500, "seek": 328424, "start": 3284.24, "end": 3289.7599999999998, "text": " And once Swift has moved to the top of MLR, we'll still use XLA-targeted TPUs. Yeah. So", "tokens": [400, 1564, 25539, 575, 4259, 281, 264, 1192, 295, 21601, 49, 11, 321, 603, 920, 764, 1783, 11435, 12, 23480, 847, 292, 314, 8115, 82, 13, 865, 13, 407], "temperature": 0.0, "avg_logprob": -0.2588077245974073, "compression_ratio": 1.5083333333333333, "no_speech_prob": 6.14391865383368e-06}, {"id": 501, "seek": 328424, "start": 3290.64, "end": 3298.9599999999996, "text": " I mean, this is internal work, but we're doing a lot to change and enhance the TPU software stack", "tokens": [286, 914, 11, 341, 307, 6920, 589, 11, 457, 321, 434, 884, 257, 688, 281, 1319, 293, 11985, 264, 314, 8115, 4722, 8630], "temperature": 0.0, "avg_logprob": -0.2588077245974073, "compression_ratio": 1.5083333333333333, "no_speech_prob": 6.14391865383368e-06}, {"id": 502, "seek": 328424, "start": 3298.9599999999996, "end": 3307.12, "text": " in XLA. And things in XLA are changing in their implementation as well. And so there's a big", "tokens": [294, 1783, 11435, 13, 400, 721, 294, 1783, 11435, 366, 4473, 294, 641, 11420, 382, 731, 13, 400, 370, 456, 311, 257, 955], "temperature": 0.0, "avg_logprob": -0.2588077245974073, "compression_ratio": 1.5083333333333333, "no_speech_prob": 6.14391865383368e-06}, {"id": 503, "seek": 328424, "start": 3307.12, "end": 3313.12, "text": " investment going on in all these pieces right now. And I think that more generally,", "tokens": [6078, 516, 322, 294, 439, 613, 3755, 558, 586, 13, 400, 286, 519, 300, 544, 5101, 11], "temperature": 0.0, "avg_logprob": -0.2588077245974073, "compression_ratio": 1.5083333333333333, "no_speech_prob": 6.14391865383368e-06}, {"id": 504, "seek": 331312, "start": 3313.12, "end": 3317.7599999999998, "text": " again, if you ignore which letters get attached to them, the effort here culminates in a much more", "tokens": [797, 11, 498, 291, 11200, 597, 7825, 483, 8570, 281, 552, 11, 264, 4630, 510, 28583, 1024, 294, 257, 709, 544], "temperature": 0.0, "avg_logprob": -0.20223819778626223, "compression_ratio": 1.6218487394957983, "no_speech_prob": 4.7571567847626284e-05}, {"id": 505, "seek": 331312, "start": 3317.7599999999998, "end": 3324.0, "text": " flexible cogeneration stack, support for dynamic shapes and custom ops and things like that.", "tokens": [11358, 598, 30372, 8630, 11, 1406, 337, 8546, 10854, 293, 2375, 44663, 293, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.20223819778626223, "compression_ratio": 1.6218487394957983, "no_speech_prob": 4.7571567847626284e-05}, {"id": 506, "seek": 331312, "start": 3324.0, "end": 3329.6, "text": " It's just that different pieces in this very complicated technology come together at different", "tokens": [467, 311, 445, 300, 819, 3755, 294, 341, 588, 6179, 2899, 808, 1214, 412, 819], "temperature": 0.0, "avg_logprob": -0.20223819778626223, "compression_ratio": 1.6218487394957983, "no_speech_prob": 4.7571567847626284e-05}, {"id": 507, "seek": 331312, "start": 3329.6, "end": 3338.48, "text": " points. I don't know what the marketing, the crack compiler marketing team will end up labeling the", "tokens": [2793, 13, 286, 500, 380, 458, 437, 264, 6370, 11, 264, 6226, 31958, 6370, 1469, 486, 917, 493, 40244, 264], "temperature": 0.0, "avg_logprob": -0.20223819778626223, "compression_ratio": 1.6218487394957983, "no_speech_prob": 4.7571567847626284e-05}, {"id": 508, "seek": 333848, "start": 3338.48, "end": 3347.12, "text": " resultant thing. Excellent. We're slightly over time. So I just wanted to, unless there's any", "tokens": [1874, 394, 551, 13, 16723, 13, 492, 434, 4748, 670, 565, 13, 407, 286, 445, 1415, 281, 11, 5969, 456, 311, 604], "temperature": 0.0, "avg_logprob": -0.14604483368576213, "compression_ratio": 1.5528455284552845, "no_speech_prob": 4.264137533027679e-05}, {"id": 509, "seek": 333848, "start": 3347.12, "end": 3354.4, "text": " pressing questions, thank everyone for joining and see you all next week. I think next week,", "tokens": [12417, 1651, 11, 1309, 1518, 337, 5549, 293, 536, 291, 439, 958, 1243, 13, 286, 519, 958, 1243, 11], "temperature": 0.0, "avg_logprob": -0.14604483368576213, "compression_ratio": 1.5528455284552845, "no_speech_prob": 4.264137533027679e-05}, {"id": 510, "seek": 333848, "start": 3354.4, "end": 3360.64, "text": " Mark will be up talking about some of his work on testing the auditive system to ensure that it's", "tokens": [3934, 486, 312, 493, 1417, 466, 512, 295, 702, 589, 322, 4997, 264, 2379, 2187, 1185, 281, 5586, 300, 309, 311], "temperature": 0.0, "avg_logprob": -0.14604483368576213, "compression_ratio": 1.5528455284552845, "no_speech_prob": 4.264137533027679e-05}, {"id": 511, "seek": 333848, "start": 3360.64, "end": 3365.28, "text": " really reliable. There's some pretty good things that Mark's been up to there. It's also exciting", "tokens": [534, 12924, 13, 821, 311, 512, 1238, 665, 721, 300, 3934, 311, 668, 493, 281, 456, 13, 467, 311, 611, 4670], "temperature": 0.0, "avg_logprob": -0.14604483368576213, "compression_ratio": 1.5528455284552845, "no_speech_prob": 4.264137533027679e-05}, {"id": 512, "seek": 336528, "start": 3365.28, "end": 3371.44, "text": " that Adi is getting upstream to master too, which is really cool. Thanks everyone. Have a great week", "tokens": [300, 1999, 72, 307, 1242, 33915, 281, 4505, 886, 11, 597, 307, 534, 1627, 13, 2561, 1518, 13, 3560, 257, 869, 1243], "temperature": 0.0, "avg_logprob": -0.36349871351912216, "compression_ratio": 1.2545454545454546, "no_speech_prob": 0.0003978318418376148}, {"id": 513, "seek": 337144, "start": 3371.44, "end": 3399.44, "text": " and see you all next week. Thank you.", "tokens": [50364, 293, 536, 291, 439, 958, 1243, 13, 1044, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.4056787124046913, "compression_ratio": 0.8604651162790697, "no_speech_prob": 0.000936428492423147}], "language": "en"}