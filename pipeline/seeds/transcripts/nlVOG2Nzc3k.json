{"text": " Hi folks, thanks for joining me for lesson 18. We're going to start today in Microsoft Excel. You'll see there's an Excel folder actually in the course 22p2 repo. And in there there's a spreadsheet called graddesk as in gradient descent, which I guess we should zoom in a bit here. So there's some instructions here, but this is basically describing what's in each sheet. We're going to be looking at the various SGD accelerated approaches we saw last time, but done in a spreadsheet. We're going to do something very, very simple, which is to try to solve a linear regression. So the actual data was generated with y equals ax plus b, where a, which is the slope, was 2, and b, which is the intercept or constant, was 30. And so you can see we've got some random numbers here. And then over here we've got the ax plus b calculation. So then what I did is I copied and pasted as values, just one set of those random numbers into the next sheet called basic. This is the basic SGD sheet. So that's what x and y are. And so the idea is we're going to try to use SGD to learn that the intercept is 30 and the slope is 2. So the way we do SGD is we, so those are our weights or parameters. So the way we do SGD is we start out at some random kind of guess. So my random guess is going to be 1 and 1 for the intercept and slope. And so if we look at the very first data point, which is x is 14 and y is 58, the intercept and slope are both 1. Then we can make a prediction. And so our prediction is just equal to slope times x plus the intercept. So the prediction will be 15. Now actually the answer was 58. So we're a long way off. So we're going to use mean squared error. So the mean squared error is just the error, so the difference squared. Okay, so one way to calculate how much would the prediction, sorry, how much would the error change, so how much would the squared error, I should say change, if we changed the intercept, which is b, would be just to change b by a little bit, change the intercept by a little bit and see what the error is. So here that's what I've done is I've just added 0.01 to the intercept and then calculated y and then calculated the difference squared. And so this is what I mean by err b1. This is the error squared I get if I change b by 0.01. So it's made the error go down a little bit. So that suggests that we should probably increase b, increase the intercept. So we can calculate the estimated derivative by simply taking the change from when we use the actual intercept using the intercept plus 0.01. So that's the rise and we divide it by the run, which is as we said is 0.01. And that gives us the estimated derivative of the squared error with respect to b, the intercept. Okay, so it's about negative 86, 85.99. So we can do exactly the same thing for a, so change the slope by 0.01, calculate y, calculate the difference and square it. And we can calculate the estimated derivative in the same way, rise, which is the difference divided by run, which is 0.01. And that's quite a big number, minus 1200. In both cases, the estimated derivatives are negative. So that suggests we should increase the intercept and the slope. And we know that that's true because actually the intercept and the slope are both bigger than 1. The intercept is 30, should be 30 and the slope should be 2. So there's one way to calculate the derivatives. Another way is analytically. And the derivative of squared is 2 times. So here it is here. I've just written it down for you. So here's the analytic derivative. It's just 2 times the difference. And then the derivative for the slope is here. And you can see that the estimated version using the rise over run and the little 0.01 change and the actual, they're pretty similar. Okay. And same thing here. They're pretty similar. So anytime I calculate gradients kind of analytically, but by hand, I always like to test them against the actual rise over run calculation with some small number. And this is called using the finite differencing approach. We only use it for testing because it's slow, because you have to do a separate calculation for every single weight. But it's good for testing. We use analytic derivatives all the time in real life. Anyway, so however we calculate the derivatives, we can now calculate a new slope. And our new slope will be equal to the previous slope minus the derivative times the learning rate, which we just set here at 0.0001. And we can do the same thing for the intercept as you see. And so here's our new slope intercept. So we can use that for the second row of data. So the second row of data is x equals 86, y equals 202. So our intercept is not 1, 1 anymore. Intercept and slope are not 1, 1, but they're 1.01 and 1.12. So here's we're just using a formula just to point at the new intercept and slope. We can get a new prediction and squared error and derivatives. And then we can get another new slope and intercept. And so that was a pretty good one actually. It really helped our slope head in the right direction, although the intercept's moving pretty slowly. And so we can do that for every row of data. Now strictly speaking, this is not mini-batch gradient descent that we normally do in deep learning. It's a simpler version where every batch is a size 1. So I mean it's still stochastic gradient descent, it's just not, it's just a batch size of 1. I think sometimes it's called online gradient descent, if I remember correctly. So we go through every data point in our very small data set until we get to the very end. And so at the end of the first epoch, we've got an intercept of 1.06 and a slope of 2.57. And those indeed are better estimates than our starting estimates of 1, 1. So what I would do is I would copy our slope 2.57 up to here, 2.57, I'll just type it for now. And I'll copy our intercept up to here. And then it goes through the entire epoch again, and we get another intercept and slope. And so we could keep copying and pasting and copying and pasting again and again. And we can watch the root mean squared error going down. Now that's pretty boring doing that copying and pasting. So what we could do is fire up Visual Basic for applications. And sorry this might be a bit small, I'm not sure how to increase the font size. And what it shows here, so sorry this is a bit small, so you might want to just open it on your own computer to be able to see it clearly. But basically it shows I've created a little macro where if you click on the reset button, it's just going to set the slope and constant to 1 and calculate. And if you click the run button, it's going to go through five times, calling one step. And what one step's going to do is it's going to copy the slope, last slope, to the new slope and the last constant intercept to the new constant intercept. And also do the same for the RMSE. And it's actually going to paste it down to the bottom for reasons I'll show you in a moment. So if I now run this, I'll reset and then run. There we go. So it's running at five times and each time it's posted the RMSE. And here's a chart of it showing it going down. And so you can see the new slope is 2.57, the new intercept is 1.27. I could keep running at another five. So this is just doing copy paste, copy paste, copy paste five times. And you can see that the RMSE is very, very, very slowly going down. And the intercept and slope are very, very, very slowly getting closer to where they want to be. The big issue really is that the intercept is meant to be 30. It looks like it's going to take a very, very long time to get there. But it will get there eventually if you click run enough times or maybe set the VBA macro to loop more than five steps at a time. But you can see it's, it's very slowly. And importantly though, you can see like it's kind of taking this linear route every time these are increasing. So why not increase it by more and more and more. And so you'll remember from last week that that is what momentum does. So on the next sheet we show momentum. And so everything's exactly the same as the previous sheet. But this sheet we didn't bother with the finite differencing. We just have the analytic derivatives, which are exactly the same as last time. The data is the same as last time. The slope and intercept are the same starting points as last time. And this is the new B and new A that we get. But what we do this time is that we've added a momentum term, which we're calling beta. And so the beta is going to these cells here. And what are these cells? What these cells are is that they're, maybe it's most interesting to take this one here. What it's doing is it's taking the gradient and it's taking the gradient and it's using that to update the weights. But it's also taking the previous update. So you can see here the blue one, minus 25. So that is going to get multiplied by 0.9, the momentum. And then the derivative is then multiplied by 0.1. So this is momentum, which is getting a little bit of each. And so then what we do is we then use that instead of the derivative to multiply by our learning rate. So we keep doing that again and again and again as per usual. And so we've got one column which is calculating the next, which is calculating the momentum, you know, lerped version of the gradient for both B and for A. And so you can see that for this one it's the same thing. You look at what was the previous move and that's going to be 0.9 of what you're going to use for your momentum version gradient. And 0.1 is for this version, the momentum gradient. And so then that's again what we're going to use to multiply by the learning rate. And so you can see what happens is when you keep moving in the same direction, which here is we're saying the derivative is negative again and again and again, so it gets higher and higher and higher. And ditto here. And so particularly with this big jump we get, we keep getting big jumps because still we want to, there's still negative gradient, negative gradient, negative gradient. So if we, at, so at the end of this, our new, our B and our A have jumped ahead and so we can click run. And we can keep clicking it and you can see that it's moving, you know, not super fast but certainly faster than it was before. So if you haven't used VBA, Visual Basic for Applications, before you can hit alt, alt F11 or option F11 to open it. And you may need to go into your preferences and turn on the developer tools so that you can see it. You can also right-click and choose assign macro on a button and you can see what macro has been assigned. So if I hit alt F11 and I can just double, or I can just double click on the sheet name and it'll open it up. And you can see that this is exactly the same as the previous one. There's no difference here. One difference is that to keep track of momentum at the very, very end, so I've got my momentum values going all the way down, the very last momentum I copy back up to the top, each epoch, so that we don't lose track of our kind of optimizer state, if you like. Okay, so that's what momentum looks like. So yeah, if you're kind of a more of a visual person like me, you like to see everything laid out in front of you and like to be able to experiment, which I think is a good idea, this can be really helpful. So rmsprop, we've seen, and it's very similar to momentum, but in this case instead of keeping track of kind of a lerped moving average, an exponential moving average of gradients, we're keeping track of a moving average of gradient squared. And then rather than simply adding that, you know, using that as the gradient, what instead we're doing is we are dividing our gradient by the square root of that. And so remember the reason we were doing that is to say, you know, if there's very little variation, very little going on in your gradients, then you probably want to jump further. So that's rmsprop. And then finally atom, remember, was a combination of both. So in atom, we've got both the lerped version of the gradient, and we've got the lerped version of the gradient squared. And then we do both when we update. We're both dividing the gradient by the square root of the lerped, the moving exponentially weighted average, moving averages, and we're also using the momentumized version. And so again, we just go through that each time. And so if I reset, run, and so, oh wow, look at that. It jumped up there very quickly, because remember we wanted to get to 2 and 30. So just two sets, so that's 5, that's 10 epochs. Now if I keep running it, it's kind of now not getting closer. It's kind of jumping up and down between pretty much the same values. So probably what we need to do is decrease the learning rate at that point. And yeah, that's pretty good. And now it's jumping up and down between the same two values again. So maybe decrease the learning rate a little bit more. I kind of like playing around like this, because it gives me a really intuitive feeling for what training looks like. So I've got a question from our YouTube chat, which is how is J33 being initialized? So it's, it's, this is just, what happens is we take the very last cell, here, well there's actually all these last four cells, and we copy them to here as values. So this is what those looked like in the last epoch. So if I basically, we're going, we go copy, and then paste as values, and then they, this here just refers back to them as you see. And it's interesting that they're kind of, you can see how they're exact opposites of each other, which is really, you can really see how they're, it's, it's just fluctuating around the actual optimum at this point. Okay thank you to Sam Watkins, we've now got a nicer sized editor, that's great. Where are we, Adam? Okay so with, so with Adam, basically it all looks pretty much the same, except now we have to copy and paste our, both our momentums and our squared gradients, and of course the slopes and intercepts at the end of each step. But other than that it's just doing the same thing, and when we reset it, it just sets everything back to their default values. Now one thing that occurred to me, you know, when I first wrote this spreadsheet a few years ago, was that manually changing the learning rate seems pretty annoying. Now of course we can use a scheduler, but a scheduler is something we set up ahead of time, and I did wonder if it's possible to create an automatic scheduler. And so I created this Adam annealing tab, which honestly I've never really got back to experimenting with, so if anybody's interested they should check this out. What I did here was I used exactly the same spreadsheet as the Adam spreadsheet, but I added an extra, after I do this step, I added an extra thing, which is I automatically decreased the learning rate in a certain situation. And the situation in which I, in which I decreased it, was I kept track of the average of the squared gradients, and any time the average of the squared gradients decreased during an epoch, I stored it. So I basically kept track of the lowest squared gradients we had. And then what I did was if we got a, if that resulted in the gradients, the squared gradients average halving, then I would decrease the learning rate by, then I would decrease the learning rate by a factor of four. So I was keeping track of this gradient ratio. Now when you see a range like this, you can find what that's referring to by just clicking up here and finding gradient ratio. And there it is, and you can see that it's equal to the ratio between the average of the squared gradients versus the minimum that we've seen so far. So this is kind of like, my theory here was thinking that, yeah, basically as you train, you kind of get into flatter and more stable areas. And as you do that, that's a sign that, you know, you might want to decrease your learning rate. So yeah, if I try that, if I hit run, again it jumps straight to a pretty good value, but I'm not going to change the learning rate manually. I just press run, and you can see it's changed the learning rate automatically now. And if I keep hitting run without doing anything, look at that, it's got pretty good, hasn't it? And the learning rates got lower and lower, and we basically got almost exactly the right answer. So yeah, that's a little experiment I tried. So maybe some of you should try experiments around whether you can create an automatic annealer using the, using mini AI. I think that would be fun. So that is an excellent segue into our notebook, because we are going to talk about annealing now. So we've seen it manually before, where we've just decreased the learning rate in a notebook and like run a second cell. And we've seen something in Excel, but let's look at what we generally do in PyTorch. So we're still in the same notebook as last time, the accelerated SGD notebook. And now that we've re-implemented all the main optimizers that people tend to use most of the time from scratch, we can use PyTorch's of course. So let's see, look, look now at how we can do our own learning rate scheduling or annealing within the mini AI framework. Now we've seen when we implemented the learning rate finder, that, that we saw how to create something that adjusts the learning rate. So just to remind you, this was all we had to do. So we had to go through the optimizers, parameter groups, and in each group set the learning rate to times equals some multiplier. If we're just, that was for the learning rate finder. So since we know how to do that, we're not going to bother re-implementing all the schedulers from scratch, because we know the basic idea now. So instead, what we're going to have to do is have a look inside the torch.optim.lr scheduler module and see what's defined in there. So the lr scheduler module, you know, you can hit dot tab and see what's in there. But something that I quite like to do is to use dir, because dir, dir lr scheduler is a nice little function that tells you everything inside a Python object. And this particular object is a module object, and it tells you all the stuff in the module. When you use the dot version tab, it doesn't show you stuff that starts with an underscore, by the way, because that stuff's considered private. Where as dir does show you that stuff. Now I can kind of see from here that the things that start with a capital and then a small letter look like the things we care about. We probably don't care about this, we probably don't care about these. So we can just do a little list comprehension that checks that the first letter is an uppercase and the second letter is lowercase, and then join those all together with a space. And so here is a nice way to get a list of all of the schedulers that PyTorch has available. And actually, I couldn't find such a list on the PyTorch website in the documentation, so this is actually a handy thing to have available. So here's various schedulers we can use, and so I thought we might experiment with using cosine annealing. So before we do, we have to recognize that these PyTorch schedulers work with PyTorch optimizers, not with, of course, with our custom SGD class. And PyTorch optimizers have a slightly different API, and so we might learn how they work. So to learn how they work, we need an optimizer. So some, one easy way to just grab an optimizer would be to create a learner, just kind of pretty much any old random learner, and pass in that single batch callback that we created. Do you remember that single batch callback? Single batch. It just, after batch, it cancels the fit. So it literally just does one batch. And we could fit. And from that, we've now got a learner and an optimizer. And so we can do the same thing. We can do our optimizer to see what attributes it has. This is a nice way, or of course just read the documentation in PyTorch. This one is documented, I think, showing all the things it can do. As you would expect, it's got the step and the zero grad, like we're familiar with. Or you can just, if you just hit opt. So you can, the optimizers in PyTorch do actually have a repra, as it's called, which means you can just type it in and hit shift enter, and you can also see the information about it this way. Now an optimizer, it'll tell you what kind of optimizer it is. And so in this case, the default optimizer for a learner, when we created it, we decided was optim.sgd.sgd. So we've got an SGD optimizer. But it's got these things called parameter groups. What are parameter groups? Well, parameter groups are, as it suggests, they're groups of parameters. And in fact, we only have one parameter group here, which means all of our parameters are in this group. So let me kind of try and show you. It's a little bit confusing, but it's kind of quite neat. So let's grab all of our parameters. And that's actually a generator. So we have to turn that into an iterator and call next, and that will just give us our first parameter. Okay, now what we can do is we can then check the state of the optimizer. And the state is a dictionary, and the keys are parameter tensors. So this is kind of pretty interesting, because you might be, I'm sure you're familiar with dictionaries, I hope you're familiar with dictionaries, but normally you probably use numbers or strings as keys. But actually you can use tensors as keys, and indeed that's what happens here. If we look at param, it's a tensor, it's actually a parameter, which remember is a tensor, which it knows to require grad and to list in the parameters of the module. And so we're actually using that to index into the state. So if you look at up.state, it's a dictionary where the keys are parameters. Now what's this for? Well what we want to be able to do is, if you think back to this, we actually had each parameter, we have state for it. We have the average of the gradients, or the exponentially weight of moving average gradients, and of squared averages, and we actually stored them as attributes. So PyTorch does it a little bit differently, it doesn't store them as attributes, but instead it, the optimizer has a dictionary where you can look up, where you can index into it using a parameter, and that gives you the state. And so you can see here, it's got a, this is the exponentially weighted moving averages, and both because we haven't done any training yet, and because we're using non-momentum SGD, it's none, but that's how it would be stored. So this is really important to understand PyTorch optimizers. I quite liked our way of doing it, of just storing the state directly as attributes, but this works as well, and it's fine, you just have to know it's there. And then, as I said, rather than just having parameters, so we in SGD stored the parameters directly, but in PyTorch those parameters can be put into groups. And so, since we haven't put them into groups, the length of param groups is one, this is one group, so here is the param groups, and that group contains all of our parameters. Okay, so PG, just to clarify here what's going on, PG is a dictionary, it's a parameter group, and to get the keys from a dictionary you can just listify it, that gives you back the keys. And so this is one quick way of finding out all the keys in a dictionary, so you can see all the parameters in the group, and you can see all of the hyper parameters, the learning rate, the momentum, weight decay, and so forth. So that gives you some background about what's going on inside an optimizer. So Siva asks, isn't indexing by a tensor just like passing a tensor argument to a method? And no, it's not quite the same, because this is state, so this is how the optimizer stores state about the parameters, it has to be stored somewhere. For our homemade mini-ai version we stored it as attributes on the parameter, but in the PyTorch optimizers they store it as a dictionary. So it's just how it's stored. Okay so with that in mind let's look at how schedulers work. So let's create a cosine annealing scheduler. So a scheduler in PyTorch, you have to pass it the optimizer, and the reason for that is we want to be able to tell it to change the learning rates of our optimizer. So it needs to know what optimizer to change the learning rates of, so it can then do that for each set of parameters. And the reason that it does it by parameter group is that as we'll learn in a later lesson for things like transfer learning, we often want to adjust the learning rates of the later layers differently to the earlier layers that actually have different learning rates. And so that's why we can have different groups, and the different groups have the different learning rates, momentums, and so forth. Okay so we pass in the optimizer, and then if I hit shift tab a couple of times it'll tell me all of the things that you can pass in. And so it needs to know, t max, how many iterations you're going to do, and that's because it's trying to do one, you know, half a wave, if you like, of the cosine curve. So it needs to know how many iterations you're going to do, so it needs to know how far to step each time. So if we're going to do a hundred iterations. So the scheduler is going to store the base learning rate, and where did it get that from? It got it from our optimizer, which we set a learning rate. Okay so it's going to steal the optimizer's learning rate, and that's going to be the starting learning rate, the base learning rate. And it's a list because there could be a different one for each parameter group, we only have one parameter group. You can also get the most recent learning rate from a scheduler, which of course is the same. And so I couldn't find any method in PyTorch to actually plot a scheduler's learning rates, so I just made a tiny little thing that just created a list, set it to the last learning rate of the scheduler, which is going to start at 0.06, and then goes through however many steps you ask for, steps the optimizer, steps the scheduler. So this is the thing that causes the scheduler to adjust its learning rate, and then just append that new learning rate to a list of learning rates, and then plot it. So that's here's, and what I've done here is I've intentionally gone over a hundred, because I had told it I'm going to do a hundred, so I'm going over a hundred. And you can see the learning rate, if we did a hundred iterations, would start high for a while, it would then go down, and then it would stay low for a while. And if we intentionally go past the maximum, it's actually start going up again, because this is a cosine curve. So one of the main things I guess I wanted to show here is like what it looks like to really investigate in a REPL environment, like a notebook, how, you know, how an object behaves, you know, what's in it. And you know this is something I would always want to do when I'm using something from an API I'm not very familiar with. I really want to like see what's in it, see what they do, run it totally independently, plot anything I can plot. This is how I like to learn about the stuff I'm working with. You know data scientists don't spend all of their time just coding, you know, so that means we need, we can't just rely on using the same classes and APIs every day. So we have to be very good at exploring them and learning about them, and so that's why I think this is a really good approach. Okay so let's create a scheduler callback. So a scheduler callback is something we're going to pass in the scheduling class, but remember then when we, or the scheduling callable actually, and remember that when we create the scheduler we have to pass in the optimizer to schedule, and so before fit, that's the point at which we have an optimizer, we will create the scheduling object. I like this, it's very Australian. So the scheduling object we will create by passing the optimizer into the scheduler callable. Then when we do step, then we'll check if we're training, and if so we'll step. Okay so then what's going to call step is after batch. So after batch we'll call step, and that would be if you want your scheduler to update the learning rate every batch. We could also have an epoch scheduler callback, which we'll see later, and that's just going to be after epoch. Okay so in order to actually see what the scheduler is doing, we're going to need to create a new callback to keep track of what's going on in our learner, and I figured we could create a recorder callback, and what we're going to do is we're going to be passing in the name of the thing that we want to record, that we want to keep track of in each batch, and a function which is going to be responsible for grabbing the thing that we want. And so in this case the function here is going to grab from the callback, look up its param groups property, and grab the learning rate. Where does the PG property come from, retribute? Well before fit, the recorder callback is going to grab just the first parameter group, just so it's like you've got to pick some parameter group to track, so we'll just grab the first one. And so then also we're going to create a dictionary of all the things that we're recording, so we'll get all the names, so that's going to be in this case just LR, and initially it's just going to be an empty list, and then after batch we'll go through each of the items in that dictionary, which in this case is just LR is the key, and underscore LR function is the value, and we will append to that list, call that method, call that function or callable, and pass in this callback. And that's why this is going to get the callback. And so that's going to basically then have a whole bunch of, you know, dictionary of the results, you know, of each of these functions, after each batch, during training, so we'll just go through and plot them all. And so let me show you what that's going to look like. If we... let's create a cosine annealing callable. So we're going to have to use a partial to say that this callable is going to have Tmax equal to three times however many mini-batches we have in our data loader, that's because we're going to do three epochs. And then we will set it running, and we're passing in the batch scheduler with the scheduler callable, and we're also going to pass in our recorder callback, saying we want to track the learning rate using the underscore LR function. We're going to call fit, and oh this is actually a pretty good accuracy, we're getting, you know, close to 90% now in only three epochs, which is impressive, and so when we then call rec.plot, it's going to call... remember that rec is the recorder callback, so it plots the learning rate. Isn't that sweet? So we could, as I said, we would can do exactly the same thing but replace after batch with after epoch, and this will now become a scheduler which steps at the end of each epoch, rather than the end of each batch. So I can do exactly the same thing now using an epoch scheduler, so this time Tmax is three, because we're only going to be stepping three times. We're not stepping at the end of each batch, just at the end of each epoch. So that trains, and then we can call rec.plot after trains, and as you can see there, it's just stepping three times. So you can see here, we're really digging in deeply to understanding what's happening in everything in our models. What do all the activations look like? What do the losses look like? What do our learning rates look like? And we've built all this from scratch, so yeah, hopefully that gives you a sense that we can really, yeah, do a lot ourselves. Now, if you've done the fast.ai part one course, you'll be very aware of one cycle training, which was from a terrific paper by Leslie Smith, which I'm not sure it ever got published, actually. And one cycle training is, well, let's take a look at it. So we can just replace our scheduler with one cycle learning rate scheduler, so that's in PyTorch, and of course if it wasn't in PyTorch, we could very easily just write our own. We're going to make it a batch scheduler, and we're going to train, this time we're going to do five epochs. So we're going to train a bit longer, and so the first thing I'll point out is, hooray, we have got a new record for us, 90.6%, so that's great. And then B, you can see here's the plot, and now look, two things are being plotted, and that's because I've now passed into the recorder callback a plot of learning rates, and also a plot of momentums, and momentums is going to get the betas zero, because remember for atom, it's called beta zero and beta one, is momentum of the gradients, and the momentum of the gradient squared. And you can see what the one cycle is doing, is the learning rate is starting very low, and going up to high, and then down again, but the momentum is starting high, and then going down, and then up again. So what's the theory here? Well, the starting out at a low learning rate is particularly important if you have a not perfectly initialized model, which almost everybody, almost always does, even though we spend a lot of time learning to initialize models, you know, we use a lot of models that get more complicated, and after a while, people, after a while, people learn or figure out how to initialize more complex models properly. So for example, this is a very, very cool paper. In 2019, this team figured out how to initialize ResNets properly, we'll be looking at ResNets very shortly, and they discovered when they did, that they did not need batch norm, they could train networks of 10,000 layers, and they could get state-of-the-art performance with no batch norm, and there's actually been something similar for transformers, called tfixup, that does a similar kind of thing. But anyway, it is quite difficult to initialize models correctly, most people fail to, most people fail to realize that they generally don't need tricks like warmup and batch norm if they do initialize them correctly. In fact, tfixup explicitly looks at this, it looks at the difference between no warmup versus with warmup, with their correct initialization versus with normal initialization. And you can see these pictures they're showing are pretty similar actually, log scale histograms of gradients, they're very similar to the colorful dimension plots. I kind of like our colorful dimension plots better in some ways, because I think they're easier to read, although I think theirs are probably prettier. So there you go Stefano, there's something to inspire you if you want to try more things with our colorful dimension plots. I think it's interesting that some papers are actually starting to use a similar idea, I don't know if they got it from us or they came up with it independently, doesn't really matter. But so we do a warmup if our network's not quite initialized correctly, then starting at a very low learning rate means it's not going to jump off way outside the area where the weights even make sense. And so then you gradually increase them as the weights move into a part of the space that does make sense. And then during that time, while we have low learning rates, if they keep moving in the same direction, then with this very high momentum, they'll move more and more quickly. But if they keep moving in different directions, it's just the momentum is going to kind of look at the underlying direction they're moving. And then once you have got to a good part of the weight space, you can use a very high learning rate. And with a very high learning rate, you wouldn't want so much momentum. So that's why there's low momentum during the time when there's high learning rate. And then as we saw in our spreadsheet, which did this automatically, as you get closer to the optimal, you generally want to decrease the learning rate. And since we're decreasing it, again, we can increase the momentum. So you can see that starting from random weights, we've got a pretty good accuracy on fashion MNIST with a totally standard convolutional neural network, no ResNets, nothing else, everything built from scratch by hand, artisanal neural network training, and we've got 90.6% fashion MNIST. So there you go. All right, let's take a seven minute break, and I'll see you back shortly. I should warn you, you've got a lot more to cover, so I hope you're okay with a long lesson today. Okay, we're back. I just wanted to mention also something we skipped over here, which is this hasLearn callback. This is more important for the people doing the live course than the recordings. If you're doing the recording, you will have already seen this, but since I created Learner, actually Peter Zappler, I don't know how to pronounce your surname, sorry Peter, pointed out that there's actually kind of a nicer way of handling Learner. That previously we were putting the Learner object itself into self.learn in each callback. And that meant we were using self.learn.model and self.learn.opt and self.learn.all, there's you know all over the place, it was kind of ugly. So we've modified Learner this week to instead pass in when it calls the callback, when in runCBs, which is what it calls, Learner calls, you might remember, is it passes the Learner as a parameter to the method. So now the Learner no longer goes through the callbacks and sets their .learn attribute. But instead in your callbacks you have to put learn as a parameter in all of the callback methods. So for example, deviceCB has a before fit, so now it's got comma learn here. So now this is not self.learn, it's just learn. So it does make a lot of the code less yucky to not have all this self.learn.predsql, self.learn.model, self.learn.batch, it's now just learn. It also is good because you don't generally want to have both have the learner has a reference to the callbacks, but also the callbacks having a reference back to the learner, it creates something called a cycle. So there's a couple of benefits there. And that reminds me there's a few other little changes we've made to the code. And I want to show you a cool little trick. I want to show you a cool little trick for how I'm going to find quickly all of the changes that we've made to the code in the last week. So to do that we can go to the course repo, and on any repo you can add slash compare in GitHub, and then you can compare across, you know, all kinds of different things. One of the examples they've got here is to compare across different times. Look at the master branch now versus one day ago. So I actually want the master branch now versus seven days ago. So I just hit this, change this to seven. And there we go. There's all my commits, and I can immediately see the changes from last week. And so you can basically see what are the things I had to do when I change things. So for example you can see here all of my self.learns became learns. I added the YANILI, that's right, I added my augmentation. And so in learner I added an lrfind. Ah yes, I will show you that one. That's pretty fun. So here's the changes we made to runcbs, to fit. So this is a nice way I can quickly, yeah, find out what I've changed since last time, and make sure that I don't forget to tell you folks about any of them. Oh yes, clean up fit, I have to tell you about that as well. Okay that's a useful reminder. So the main other change to mention is that calling the learning rate finder is now easier, because I added what's called a patch to the learner. FastCoursePatchDecorator lets you take a function, and it will turn that function into a method of this class, of whatever class you put after the colon. So this has created a new method called lrfind, or learner.lrfind. And what it does is it calls self.fit, where self is a learner, passing in however many epochs you set as the maximum you want to check for your learning rate finder, what to start the learning rate at, and then it says to use as callbacks the learning rate finder callback. Now this is new as well, self.learn.fit didn't used to have a callbacks parameter. So that's very convenient, because what it does is it adds those callbacks just during the fit. So if you pass in callbacks, then it goes through each one and appends it to self.cbs, and when it's finished fitting, it removes them again. So these are callbacks that are just added for the period of this one fit, which is what we want for a learning rate finder. It should just be added for that one fit. So with this patch in place, it says this is all that's required to do the learning rate finder, is now to create your learner and call.lrfind, and there you go, bang. So patch is a very convenient thing. It's one of these things which, you know, Python has a lot of kind of like folk wisdom about what isn't considered Pythonic or good, and a lot of people really don't like. Patching in other languages, it's used very widely and is considered very good. So I don't tend to have strong opinions either way about what's good or what's bad. In fact, instead I just decide, you know, figure out what's useful in a particular situation. So in this situation, obviously it's very nice to be able to add in this additional functionality to our class. So that's what lrfind is. And then the only other thing we added to the learner this week was we added a few more parameters to fit. Fit used to just take the number of epochs, as well as the callbacks parameter. It now also has a learning rate parameter, and so you've always been able to provide a learning rate to the constructor, but you can override the learning rate for one fit. So if you pass in the learning rate, it will use it, if you pass it in. And if you don't, it'll use the learning rate passed into the constructor. And then I also added these two booleans to say when you fit, do you want to do the training loop, and do you want to do the validation loop. So by default it'll do both. And you can see here there's just an if train, do the training loop. If valid, do the validation loop. I'm not even going to talk about this, but if you're interested in testing your understanding of decorators, you might want to think about why it is that I didn't have to say with torch.nograd, but instead I called torch.nograd parentheses function. That will be a very, if you can get to a point that you understand why that works and what it does, you'll be on your way to understanding decorators better. Okay, so that is the end of Excel SGD. ResNets. Okay so we are up to 90 point, what was it, 3%? Let's keep track of this. Oh yeah, 90.6% is what we're up to. Okay so to remind you the model, actually so we're going to open 13 ResNet now, and we're going to do the usual import and setup initially. And the model that we've been using is the same one we've been using for a while, which is that it's a convolution and an activation and an optimal optional batch norm. And in our models we were using batch norm and applying our weight initialization, the kai-ming weight initialization. And then we've got comms that take the channels from 1 to 8 to 16 to 32 to 64, and each one's stride 2. And at the end we then do a flatten. And so that ended up with a 1 by 1, so that's been the model we've been using for a while. So the number of layers is 1, 2, 3, 4. So 4, 4 convolutional layers with a maximum of 64 channels in the last one. So can we beat 90 point, no, about 90 and a half, 90.6, can we beat 90.6%? So before we do a ResNet I thought well let's just see if we can improve the architecture thoughtfully. So generally speaking more depth and more channels gives the neural net more opportunity to learn. And since we're pretty good at initializing our neural nets and using batch norm, we should be able to handle deeper. So one thing we could do is we could, let's just remind ourselves of the previous version so we can compare, is we could have our, go up to 128 parameters. Now the way we do that is we could make our very first convolutional layer have a stride of 1. So that would be one that goes from the one input channel to 8 output channels, or 8 filters if you like. So if we make it a stride of 1, then that allows us to have one extra layer, and then that one extra layer could again double the number of channels and take us up to 128. So that would make it deeper and effectively wider as a result. So we can do our normal batch norm 2d and our new one cycle learning rate with our scheduler. And the callbacks we're going to use is the device callback, our metrics, our progress bar, and our activation stats looking for general values. And I won't have you watch them train because that would be kind of boring. But if I do this with this deeper and eventually wider network, this is pretty amazing. We get up to 91.7 percent. So that's like quite a big difference. And literally the only difference to our previous model is this one line of code which allowed us to take this, instead of going from 1 to 64, it goes from 8 to 128. So that's a very small change, but it massively improved. So the error rate's gone down by a temp, you know, about, well over 10 percent, relatively speaking, in terms of the error rate. So there's a huge impact we've already had. Again, five epochs. So now what we're going to do is we're going to make it deeper still. But it gets, there becomes a point, so Kaiming He noted that there comes a point where making neural nets deeper stops working well. And remember this is the guy who created the initializer that we know and love. And he pointed out that even with that good initialization, there comes a time where adding more layers becomes problematic. And he pointed out something particularly interesting. He said let's take a 20 layer neural network. This is in a paper called Deep Residual Learning for Image Recognition that introduced ResNets. So let's take a 20 layer network and train it for a few, what's that, tens of thousands of iterations, and track its test error. Okay and now let's do exactly the same thing on a 56 layer, otherwise identical, but deeper 56 layer network. And he pointed out that the 56 layer network had a worse error than the 20 layer. And it wasn't just a problem of generalization, because it was worse on the training set as well. Now the insight that he had is if you just set the additional 36 layers to just identity, you know, identity matrices, they should, they would do nothing at all. And so a 56 layer network is a superset of a 20 layer network. So it should be at least as good, but it's not, it's worse. So clearly the problem here is something about training it. And so him and his team came up with a really clever insight, which is can we create a 56 layer network which has the same training dynamics as a 20 layer network or even less. And they realized, yes you can. What you could do is you could add something called a shortcut connection. And basically the idea is that normally when we have, you know, our inputs coming into our convolution. So let's say that's, that was our inputs and here's our convolution and here's our outputs. Now if we do this 56 times, that's a lot of stacked up convolutions, which are effectively matrix multiplications, with a lot of opportunity for, you know, gradient explosions and all that fun stuff. So how could we make it so that we have convolutions but with the training dynamics of a much shallower network? And here's what he did. He said let's actually put two convs in here to make it twice as deep, because we are trying to make things deeper. But then let's add what's called a skip connection, where instead of just being out equals, so this is conv1, this is conv2. Instead of being out equals, and there's a, you know, assume that these include activation functions, equals conv2 of conv1 of in. Right, instead of just doing that, let's make it conv2 of conv1 of in plus in. Now if we initialize these at the first to have weights of zero, then initially this will do nothing at all. It will output zero, and therefore at first it you'll just get out equals in, which is exactly what we wanted, right? We actually want to, for it to be as if there is no extra layers. And so this way we actually end up with a network which can, which can be deep, but also at least when you start training behaves as if it's shallow. It's called a residual connection, because if we subtract in from both sides, now then we would get out minus in equals conv1 of conv2 of in. In other words, the difference between the end point and the starting point, which is the residual. And so another way of thinking about it is that this is calculating a residual. So there's a couple of ways of thinking about it. And so this, this thing here is called the res block or resnet block. Okay, so Sam Watkins has just pointed out the confusion here, which is that this only works if, let's put the minus in back and put it back over here. This only works if you can add these together. Now if conv1 and conv2 both have the same number of channels as in, or same number of filters, same number of filters, and they also have stride1, then that will work fine. You'll end up, that will be exactly the same output shape as the input shape, and you can add them together. But if they are not the same, then you're in a bit of trouble. So what do you do? And the answer which Kaiming He et al came up with is to add a conv on in as well, but to make it as simple as possible. We call this the identity conv. It's not really an identity anymore, but we're trying to make it as simple as possible so that we do as little to mess up these training dynamics as we can. And the simplest possible convolution is a one-by-one filter block, a one-by-one kernel, I guess we should call it, a one-by-one kernel size. And using that, and we can also add a stride or whatever if we want to. So let me show you the code. So we're going to create something called a conv block, okay, and the conv block is going to do the two convs. That's going to be a conv block, okay. So we've got some number of input filters, some number of output filters, some stride, some activation functions, possibly a normalization, and possibly, and some kernel shape, some kernel size. So the second conv is actually going to go from output filters to output filters because the first conv is going to be from input filters to output filters. So by the time we get to the second conv, it's going to be NF to NF. The first conv, we will set stride one, and then the second conv will have the requested stride. So that way the two convs back-to-back are going to overall have the requested stride. So this way the combination of these two convs is going to eventually, is going to take us from NI to NF in terms of the number of filters, and it's going to have the stride that we requested. So it's going to be a, the conv block is a sequential block consisting of a convolution followed by another convolution, each one with the requested kernel size, and the requested activation function, and the requested normalization layer. The second conv won't have an activation function. I'll explain why in a moment. And so I mentioned that one way to make this as if it didn't exist would be to set the convolutional weights to zero, and the biases to zero. But actually we would, we would like to have, you know, correctly randomly initialized weights. So instead what we can do is if you're using batch norm, we can initialize this conv2 one will be the batch norm layer. We can initialize the batch norm weights to zero. Now if you've forgotten what that means, go back and have a look at our implementation from scratch of batch norm, because the batch norm weights is the thing we multiply by. So do you remember the batch norm? We, we subtract the exponential moving average mean. We divide by the exponential moving average standard deviation, but then we add back the, the kind of the, the, the batch norms bias layer, and we multiply by the batch norms weights, well the other way around, multiply by weights first. So if we set the batch norm layers weights to zero, we're multiplying by zero. And so this will cause the initial conv block output to be just all zeros. And so that's going to give us what we wanted, is that nothing's happening here. So we just end up with the input with this possible id conv. So a res block is going to contain those convolutions in the convolution block. We just discussed, right? And then we're going to need this id conv. So the id conv is going to be a no op, so that's nothing at all, if the number of channels in is equal to the number of channels out. But otherwise we're going to use a convolution with a kernel size of one and a stride of one. And so that is going to, you know, is with as little work as possible, change the number of filters so that they match. Also what if the stride's not one? Well if the stride is two, actually this isn't going to work for any stride, this only works for a stride of two. If there's a stride of two, we will simply average using average pooling. So this is just saying take the mean of every set of two items in the grid. So we'll just take the mean. So we basically have here pool of id conv of in, if the stride is two and if the filtered number is changed. And so that's the minimal amount of work. So here it is, here is the forward pass. We get our input and on the identity connection we call pool and if stride is one, that's a no op. So do nothing at all. We do id conv and if the number of filters is not changed, that's also a no op. So this is, this is just the input in that situation. And then we add that to the result of the convs. And here's something interesting, we then apply the activation function to the whole thing. Okay, so that way, I wouldn't say this is like the only way you can do it, but this is a way that works pretty well, is to apply the activation function to the result of the whole, the whole resnet block. And that's why I didn't add activation function to the second conv. So that's a res block. So it's not a huge amount of code, right? And so now I've literally copied and pasted our get model, but everywhere that previously we had a conv, I've just replaced it with res block. In fact, let's have a look. Get model. Okay, so previously we started with conv 1 to 8, now we do res block 1 to 8, stride 1, stride 1. Then we added conv from number of filters i to number of filters i plus 1. Now it's res block from number of filters, number of filters i plus 1. Okay, so it's exactly the same. One change I have made though, is, I mean, it doesn't actually make any difference at all, I think it's mathematically identical, is previously the very last conv at the end went from the, you know, 128 channels down to the 10 channels, followed by flatten. But this conv is actually working on a 1 by 1 input. So you know, an alternate way, but I think makes it clearer, is flatten first and then use a linear layer. Because a conv on a 1 by 1 input is identical to a linear layer. And if that doesn't immediately make sense, that's totally fine, but this is one of those places where you should pause and have a little stop and think about why a conv on a 1 by 1 is the same, and maybe go back to the Excel spreadsheet if you like, or the Python from scratch conv we did, because this is a very important insight. So I think it's very useful with a more complex model like this to take a good old look at it, to see exactly what the inputs and outputs of each layer is. So here's a little function called print shape, which takes the things that a hook takes. And we will print out for each layer the name of the class, the shape of the input, and the shape of the output. So we can get our model, create our learner, and use our handy little hooks context manager we built in an earlier lesson, and call the print shape function. And then we will call fit for one epoch, just doing the evaluation or the training. And if we use the single batch callback, it'll just do a single batch, put, pass it through, and that hook will, as you see, print out each layer, the inputs shape, and the output shape. So you can see we're starting with an input of batch size of 1024, one channel, 28 by 28. This res block was stride one, so we still end up with 28 by 28, but now we've got eight channels. And then we gradually decrease the grid size to 14, to 7, to 4, to 2, to 1, as we gradually increase the number of channels. We then flatten it, which gets rid of that one by one, which allows us then to do linear, to go on to the 10. And then there's some discussion about whether you want a batch norm at the end or not. I was finding it quite useful in this case, so we've got a batch norm at the end. I think this is very useful, so I decided to create a patch for learner called summary that would do basically exactly the same thing, but it would do it as a markdown table. Okay, so if we create a train learner with our model and call dot summary, this method is now available because it's been patched, that method, into the learner. And it's going to do exactly the same thing as our print, but it does it more prettily by using a markdown table, if it's in a notebook, otherwise it'll just print it. So fastcore has a handy thing for keeping track if you're in a notebook, and in a notebook to make something markdown, you can just use ipython.display.markdown, as you see. And the other thing that I added, as well as the input and the output, is I thought let's also add in the number of parameters. So we can calculate that, as we've seen before, by summing up the number of elements for each parameter in that module. And so then I've kind of kept track of that as well, so that at the end I can also print out the total number of parameters. So we've got a 1.2 million parameter model, and you can see that there's very few parameters here in the input. Nearly all the parameters are actually in the last layer. Why is that? Well, you might want to go back to our Excel convolutional spreadsheet to see this. You have a parameter for every input channel. You have a set of parameters. They're all going to get added up across each of the 3x3 in the kernel, and then that's going to be done for every output filter, every output channel that you want. So that's why you're going to end up with, in fact let's take a look. Maybe let's create, let's just grab some particular one. So create our model, and so we'll just have a look at the sizes. And so you can see here there is this 256 by 256 by 3 by 3. So that's a lot of parameters. Okay so we can call lrfind on that, and get a sense of what kind of learning rate to use. So I chose 2e neg 2, so 0.02. This is our standard learning thing. You don't have to watch it train, I've just trained it. And so look at this, by using resnet we've gone up from 91.7, this is just keeps getting better, 92.2 in 5 epochs. So that's pretty nice. And you know this resnet is not anything fancy. It's the simplest possible res block, right. The model is literally copied and pasted from before, and replace each place it said conv with resblock. But we've just been thoughtful about it, you know. And here's something very interesting. We can actually try lots of other resnets by grabbing Tim. So that's Ross Whiteman's PyTorch image model library. And if you call Tim.listmodels star resnet star, there's a lot of resnets. And I tried quite a few of them. Now one thing that's interesting is if you actually look at the source code for Tim, you'll see that the various different resnets, like resnet 18, resnet 18d, resnet 10d, they're defined in a very nice way using this very elegant configuration. You can see exactly what's different. So there's basically only one line of code different between each different type of resnet for the main resnets. And so what I did was I tried all the Tim models I could find, and I even tried importing the underlying things and building my own resnets from those pieces. And the best I found was the resnet 18d. And if I train it in exactly the same way, I got to 92%. And so the interesting thing is you'll see that's less than our 92.2. And it's not like I tried lots of things to get here. This is the very first thing I tried. Where else this resnet 18d was after trying lots and lots of different Tim models. And so what this shows is that the just thoughtfully designed kind of basic architecture goes a very long way. It's actually better for this problem than any of the PyTorch image model models resnets that I could try, that I could find. So I think that's quite, quite amazing. Actually it's really cool, you know. And it shows that you can create a state-of-the-art architecture just by using some common sense, you know. So I hope that's, I hope that's, yeah, I hope that's encouraging. So anyway, so we're up to 92.2%. We're not done yet. Because we haven't even talked about data augmentation. All right, so let's keep going. So we're going to make everything the same as before. But before we do data augmentation, we're going to try to improve our model even further, if we can. So I said it was kind of not constructed with any great care and thought, really. Like in terms of like this resnet, we just took the convnet and replaced it with a resnet. So it's effectively twice as deep, because each conv block has two convolutions. But resnets train better than convnets. So surely we could go deeper and wider still. So I thought, okay, how could we go wider? And I thought, well, let's take our model. And previously we were going from 8 up to 256. What if we could get up to 512? And I thought, okay, well, one way to do that would be to make our very first res block not have a kernel size of 3, but a kernel size of 5. So that means that each grid is going to be 5 by 5. That's going to be 25 inputs. So I think it's fair enough then to have 16 outputs. So if I use a kernel size of 5, 16 outputs, then that means if I keep doubling as before, I'm going to end up at 512 rather than 256. Okay, so that's the only change I made, was to add k equals 5 here, and then change to double all the sizes. And so if I train that, wow, look at this, 92.7%. So we're getting better still. And again, it wasn't with lots of like trying and failing and whatever, it was just like saying well this just makes sense. And the first thing I tried, it just worked. You know, we're just trying to use these sensible, thoughtful approaches. Okay, the next thing I'm going to try isn't necessarily something to make it better, but it's something to make our ResNet more flexible. Our current ResNet is a bit awkward in that the number of stride two layers has to be exactly big enough that the last of them, that the last of them ends up with a one by one output. So you can flatten it and do the linear. So that's not very flexible because, you know, what if you've got something, you know, for different size? 28 by 28 is a pretty small image. So to kind of make that necessary, I've created a get model 2, which goes less far. It has one less layer. So it only goes up to 256 despite starting at 16. And so because it's got one less layer, that means that it's going to end up at the two by two, not the one by one. So what do we do? Well, we can do something very straightforward, which is we can take the mean over the two by two. And so if we take the mean over the two by two, that's going to give us a mean over the two by two. It's going to give us batch size by channels output, which is what we can then put into our linear layer. So this is called, this ridiculously simple thing, is called a global average pooling layer. That's the Keras term. In PyTorch, it's basically the same. It's called an adaptive average pooling layer. But in PyTorch, you can cause it to have an output other than one by one. But nobody ever really uses it that way. So they're basically the same thing. This is actually a little bit more convenient than the PyTorch version because you don't have to flatten it. So this is global average pooling. So you can see here, after our last res block, which gives us a two by two output, we have global average pool. And that's just going to take the mean. And then we can do the linear batch norm as usual. So I wanted to improve my summary patch to include not only the number of parameters, but also the approximate number of megaflops. So a flop is a floating operation per second, a floating point operation per second. I'm not going to promise my calculation is exactly right. I think the basic idea is right. I just basically actually calculated, it's not really a flop, so I actually counted the number of multiplications. So this is not perfectly accurate, but it's pretty indicative, I think. So this is the same summary I had before, but I added an extra thing which is a flops function where you pass in the weight matrix and the height and the width of your grid. Now if the number of dimensions of the weight matrix is less than three, then we're just doing like a linear layer or something. So actually just the number of elements is the number of flops, because it's just a matrix multiply. But if you're doing a convolution, so the dimension is four, then you actually do that matrix multiply for everything in the height by width grid. So that's how I calculate this kind of flops equivalent number. So okay, so if I run that on this model, we can now see our number of parameters compared to the ResNet model has gone from 1.2 million up to 4.9 million. And the reason why is because we've got this, we've got this res block that gets all the way up to 512. And the way we did this is we made that a stride one layer. So that's why you can see here it's gone 2.2 and it stayed at 2.2. So I wanted to make it as similar as possible to the last ones. It's got, you know, the same 512 final number of channels. And so most of the parameters are in that last block for the reason we just discussed. Interestingly though, it's not as clear for the megaflops, you know, it is the greatest of them. But, you know, in terms of number of parameters, I think this has more parameters than all the other ones added together by a lot. But that's not true of megaflops. And that's because this first layer has to be done 28 by 28 times, whereas this layer only has to be done 2 by 2 times. Anyway, so I tried training that and got pretty similar result, 92.6. And that kind of made me think, oh, let's fiddle around with this a little bit more to see like what kind of things would reduce the number of parameters and the megaflops. The reason you care about reducing the number of parameters is that it has lower memory requirements. And the reason you require, want to reduce the number of flops is it's less compute. So in this case, what I've done here is I've removed this line of code. So I've removed the line of code that takes it up to 512. So that means we don't have this layer anymore. And so the number of parameters has gone down from 4.9 million down to 1.2 million. Not a huge impact on the megaflops, but a huge impact on the parameters. We've reduced it by like two-thirds or three-quarters or something by getting rid of that. And you can see that the, if we take the very first ResNet block, the number of parameters is, you know, why is it this 5.3 megaflops? Because although the very first one starts with just one channel, the first conv, remember our ResNet blocks have two convs. So the second conv is going to be a 16 by 16 by 5 by 5. And again, I'm partly doing this to show you the actual details of this architecture, but I'm partly showing it so that you can see how to investigate exactly what's going on in your models. And I really want you to try these. So if we train that one, interestingly, even though it's only a quarter or something of the size, we get the same accuracy, 92.7. So that's interesting. Can we make it faster? Well at this point, this is the obvious place to look at, is this first ResNet block. Because that's where all the megaflops are. And as I said, the reason is because it's got two convs. The second one is 16 by 16 channels. 16 channels in, 16 channels out. And it's doing these 5 by 5 kernels. And it's having to do it across the whole 28 by 28 grid. So that's the bulk of the biggest compute. So what we could do is we could replace this Res block with just one convolution. And if we do that, then you'll see that we've now got rid of the 16 by 16 by 5 by 5. We just got the 16 by 1 by 5 by 5. So the number of megaflops has gone down from 18.3 to 13.3. The number of parameters hasn't really changed at all, right? Because the number of parameters was only 6800, right? So be very careful that when you see people talk about, oh my model has less parameters, that doesn't mean it's faster. Okay? Really, it doesn't necessarily, I mean, it doesn't mean that at all. There's no particular relationship between parameters and speed. Even counting megaflops doesn't always work that well, because it doesn't take account of the amount of things moving through memory. But, you know, it's not a bad approximation here. So here's one which has got much less megaflops, and in this case it's about the same accuracy as well. So I think this is really interesting. We've managed to build a model that has far less parameters and far less megaflops and has basically exactly the same accuracy. So I think that's a really important thing to keep in mind. And remember, this is still way better than the ResNet 18d from Tim. So we've built something that is fast, small, and accurate. So the obvious question is, what if we train for longer? And the answer is, if we train for longer, if we train for 20 epochs, I'm not going to have you wait for it. The training accuracy gets up to 0.999, but the validation accuracy is worse. It's 0.924. And the reason for that is that after 20 epochs, it's seen the same picture so many times, it's just memorizing them. And so once you start memorizing, things actually go downhill. So we need to regularize. Now something that we have claimed in the past can regularize is to use weight decay. But here's where I'm going to point out that weight decay doesn't regularize at all if you use batch norm. And it's fascinating. For years, people didn't even seem to notice this. And then somebody, I think, finally wrote a paper that pointed this out. And people were like, oh wow, that's weird. But it's really obvious when you think about it. A batch norm layer has a single set of coefficients which multiplies an entire layer. So that set of coefficients could just be the number 100 in every place. And that's going to multiply the entire previous weight matrix, or convolution kernel matrix, by 100. As far as weight decay is concerned, that's not much of an impact at all because the batch norm layer has very few weights. So it doesn't really have a huge impact on weight decay. But it massively increases the effective scale of the weight matrix. So batch norm basically lets the neural net cheat by increasing the coefficients, the parameters, even nearly as much as it wants indirectly just by changing the batch norm layers weights. So weight decay is not going to save us. And that's something really important to recognize. Weight decay is not... I mean, with batch norm layers, I don't see the point of it at all. It does have some... Like, there has been some studies of what it does. And it does have some weird kind of second-order effects on the learning rate. But I don't think you should rely on them. You should use a scheduler for changing the learning rate rather than weird second-order effects caused by weight decay. So instead we're going to do data augmentation, which is where we're going to modify every image a little bit by random change so that it doesn't see the same image each time. So there's not any particular reason to implement these from scratch, to be honest. We have implemented them all from scratch in Fast.ai, so you can certainly look them up if you're interested. But it's actually a little bit separate to what we're meant to be learning about, so I'm not going to go through it. But yeah, if you're interested, go into Fast.ai, vision, augment, and you'll be able to see, for example, how do we do flip. And you know, it's just like X dot transpose. Okay, which is not really... Yeah, it's not that interesting. Yeah, how do we do cropping and padding? How do we do random crops? So on and so forth. Okay, so we're just going to actually, you know, Fast.ai has probably got the best implementation of these, but TorchVisions are fine, so we'll just use them. And so we've created before a batch transform callback, and we used it for normalization, if you remember. So what we could do is we could create a transform batch function, which transforms the inputs and transforms the outputs using two different functions. So that would be an augmentation callback. And so then you would say, okay, for the transform batch function, for example, in this case, we want to transform our X's. And how do we want to transform our X's? And the answer is, we want to transform them using this module, which is a sequential module, first of all doing a random crop, and then a random horizontal flip. Now it seems weird to randomly crop a 28 by 28 image to get a 28 by 28 image, but we can add padding to it. And so effectively, it's going to randomly add padding on one or both sides to do this kind of random crop. One thing I did to change the batch transform callback, can't remember if I've mentioned this before, but something I changed slightly since we first wrote it, is I added this on train and on validate. So that it only does it if you said I want to do it on training and it's training, or I want to do it on validation and it's not training. And then this is, this is all the code is. So data augmentation, generally speaking, shouldn't be done on validation. We set on validation false. Okay, so what I'm going to do first of all, is I'm going to use our classic single batch CB trick and fit, in fact even better, oh yeah, fit one just doing training. And what I'm going to do then is after I fit, I can grab the batch out of the learner. And this is a way, this is quite cool, right? This is a way that I can see exactly what the model sees. Right? So this is not relying on, on any, you know, approximations. Remember when we fit, it puts it in the batch that it looks at into learn.batch. So if we fit for a single batch, we can then grab that batch back out of it, and we can call show images. And so here you can see this little crop it's added. Now something you'll notice is that every single image in this batch, now just grab the first 16, so I don't want to show you 1024, has exactly the same augmentation. And that makes sense, right? Because we're applying a batch transform. Now why is this good and why is it bad? It's good because this is running on the GPU. Right? Which is great, because nowadays very often it's really hard to get enough CPU to feed your fast GPU fast enough. Particularly if you use something like Kaggle or Colab that are really underpowered for CPU, particularly Kaggle. So this way all of our transformations, all of our augmentation is happening on the GPU. On the downside, it means that there's a little bit less variety. Every mini-batch has the same augmentation. I don't think the downside matters though, because it's going to see lots of mini-batches. So the fact that each mini-batch is going to have a different augmentation is actually all I care about. So we can see that if we run this multiple times, you can see it's got a different augmentation in each mini-batch. Okay, so I decided actually I'm just going to use one padding. So I'm just going to do a very, very small amount of data augmentation. And I'm going to do 20 epochs using one cycle learning rate. And so this takes quite a while to train, so you won't watch it. But check this out, we get to 93.8. That's pretty wild. Yeah, that's pretty wild. So I actually went on Twitter and I said to the entire world on Twitter, you know, which if you're watching this in 2023, if Twitter doesn't exist yet, ask somebody tell you about what Twitter used to be. Hopefully it still does. Can anybody beat this in 20 epochs? You can use any model you like, any library you like, and nobody's got anywhere close. So this is pretty amazing. And actually, you know, when I had a look at papers with code, there are, you know, Well, I mean, you can see it's right up there, right, with the kind of best models that are listed, certainly better than these ones. And the better models all use, you know, 250 or more epochs. So yeah, if anybody, I'm hoping that somebody watching this will find a way to beat this in 20 epochs, that would be really great. Because as you can see, we haven't really done anything very amazingly, weirdly clever. It's all very, very basic. And actually, we can go even a bit further than 93.8. Just before we do, I mentioned that since this is actually taking a while to train now, I can't remember, it takes like 10 to 15 seconds per epoch. So you know, you're waiting a few minutes, you may as well save it. So you can just call torch.save on a model, and then you can load that back later. So something that can make things even better is something called test time augmentation. I guess I should write this out properly here. Test text, test time augmentation. Now, test time augmentation actually does our batch transform callback on validation as well. And then what we're going to do is we're actually, in this case, we're going to do just a very, very, very simple test time augmentation, which is we're going to add a batch transform callback that runs on validate. And it's not random, but it actually just does a horizontal flip. Non-random, so it always does a horizontal flip. And so check this out. What we're going to do is we're going to create a new callback called capture preds. And after each batch, it's just going to append to a list the predictions, and it's going to append to a different list the targets. And that way we can just call learn.fit train equals false, and it will show us the accuracy. Okay, and this is just the same number that we saw before. But then what we can do is we can call the same thing, but this time with a different callback, which is with the horizontal flip callback. And that way it's going to do exactly the same thing as before, but in every time it's going to do a horizontal flip. And weirdly enough, that accuracy is slightly higher, which that's not the interesting bit. The interesting bit is that we've now got two sets of predictions. We've got the sets of predictions with the non-flipped version. We've got the set of predictions with the flipped version. And what we could do is we could stack those together and take the mean. So we're going to take the average of the flipped and unflipped predictions. And that gives us a better result still, 94.2%. So why is it better? It's because looking at the image from kind of like multiple different directions gives it more opportunities to try to understand what this is a picture of. And so in this case, I'm just giving it two different directions, which is the flipped and unflipped version, and then just taking their average. So yeah, this is like a really nice little trick. Sam's pointed out it's a bit like random forest, which is true. It's a kind of bagging that we're doing. We're kind of getting multiple predictions and bringing them together. And so we can actually, so 94.2% I think is my best 20 epoch result. And notice I didn't have to do any additional training. So it still counts as a 20 epoch result. You can do test time augmentation where you do, you know, a much wider range of different augmentations that you trained with. And then you can use them at test time as well. You know, more crops or rotations or warps or whatever. I want to show you one of my favorite data augmentation approaches, which is called random erasing. So random erasing, I'll show you what it's going to look like. Random erasing, we're going to add a little, we're going to basically delete a little bit of each picture. And we're going to replace it with some random Gaussian noise. Now in this case, we just got one patch, but eventually we're going to do more than one patch. So I wanted to implement this because remember we have to implement everything from scratch. This one's a bit less trivial than the previous transforms. So we should do it from scratch. And also I'm not sure there's that many good implementations. Ross Whiteman's team, I think has one. And so, and it's also a very good exercise to see how to implement this from scratch. So let's grab a batch out of the training set. And let's just grab the first 16 images. And so then let's grab the mean and standard deviation. And so what we want to do is we wanted to delete a patch from each image. But rather than deleting it, deleting it would change the statistics, right? If we set those orders zero, the mean and standard deviation are now not going to be zero one anymore. But if we replace them with exactly the same mean and standard deviation pixels that the picture has, or that our data set has, then it won't change the statistics. So that's why we've grabbed the mean and standard deviation. And so we could then try grabbing, let's say we want to delete 0.2, so 20% of the height and width. Then let's find out how big that size is. So 0.2 of the shape of the height and of the width, that's the size of the X and Y. And then the starting point, we're just going to randomly grab some starting point, right? So in this case, we've got the starting point for X is 14, starting point for Y is zero. And then it's going to be a 5 by 5 spot. And then we're going to do a Gaussian or normal initialization of our mini-batch, everything in the batch, every channel, for this X slice, this Y slice, and we're going to initialize it with this mean and standard deviation, normal random noise. And so that's what this is. So it's just that tiny little bit of code. So you'll see, I don't start by writing a function. I start by writing single lines of code that I can run independently and make sure that they all work, and that I look at the pictures and make sure it's working. Now one thing that's wrong here is that, do you see how the different, you know, this looks black and this looks gray? Now at first this was confusing me as to what's going on, what's it changed, because the original images didn't look like that. And I realized the problem is that the minimum and the maximum have changed. It used to be from negative 0.8 to 2, that was the previous min and max. Now it goes from negative 3 to 3. So the noise we've added has the same mean and standard deviation, but it doesn't have the same range, because the pixels were not normally distributed originally. So normally distributed noise actually is wrong. So to fix that, I created a new version, and I'm putting in a function now, does all the same stuff as before, as I just did before, but it clamps the random pixels to be between min and max. And so it's going to be exactly the same thing, but it's going to make sure that it doesn't change the range. That's really important, I think. Because changing the range really impacts your, you know, your activations quite a lot. So here's what that looks like. And so as you can see now, all of the backgrounds have that nice black, and it's still giving me random pixels. And I can check, and because I've done the clamping, you know, and stuff, the mean and standard deviation aren't quite 0, 1, but they're very, very close. So I'm going to call that good enough. And of course the min and max haven't changed, because I clamped them to ensure they didn't change. So that's my random erasing. So that randomly erases one block. And so I could create a random erase, which will randomly choose up to, in this case, four blocks. So with that function, oh that's annoying, it happened to be 0 this time. Okay, I'll just run it again. This time it's got 3, so that's good. So you can see it's got, oh maybe, no it's 4, 1, 2, 3, 4 blocks. Okay so that's what this data augmentation looks like. So we can create a class to do this data augmentation. So you'll pass in what percentage to do in each block, what the maximum number of blocks to have is, store that away, and then in the forward we're just going to call our random erase function, passing in the input and passing in the parameters. Great, so now we can use random crop, random flip, and random erase. Make sure it looks okay. And so now we're going to go all the way up to 50 epochs. And so if I run this for 50 epochs, I get 94.6. Isn't that crazy? So we're really right up there now, up, we're even above this one. So we're somewhere up here. And this is like stuff people write papers about from 2019, 2020. Oh look, here's the random erasing paper. That's cool. So they were way ahead of their time in 2017. But yeah, that would have trained for a lot longer. Now I was having a think, and I realized something. Which is like, why, like, how do we actually get the correct distribution, right? Like in some ways it shouldn't matter, but I was kind of like bothered by this thing of like, well we don't actually end up with 0, 1, and this kind of like clamping. It all feels a bit weird. Like how do we actually replace these pixels with something that is guaranteed to be the correct distribution? And I realized there's actually a very simple answer to this. Which is, we could copy another part of the picture over to here. If we copy part of the picture, we're guaranteed to have the correct distribution of pixels. And so it wouldn't exactly be random erasing anymore. That would be random copying. Now I'm sure somebody else has invented this. I mean, you know, I'm not saying this, nobody's ever thought of this before. So if anybody knows a paper that's done this, please tell me about it. But I, you know, I think it's a very sensible approach. And it's very, very easy to implement. So again, we're going to implement it all manually, right? So let's get our xminibatch, and let's get our, again, our size. And again, let's get the xy that we're going to be erasing. But this time we're not erasing, we're copying. So we'll then randomly get a different xy to copy from. And so now it's just, instead of in a random noise, we just say replace this slice of the batch with this slice of the batch. And we end up with, you know, you can see here it's kind of copied little bits across. Some of them you can't really see at all, and some of you can. Because I think some of them are black, and it's replaced black. But I guess it's knocked off the end of this shoe, added a little bit extra here, a little bit extra here. So we can now, again, we'll turn it into a function. Once I've tested it in the REPL, make sure the function works. And obviously this, in this case, it's copying it largely from something that's largely black for a lot of them. And then again, we can do the thing where we do it multiple times. And here we go. Now it's got a couple of random copies. And so again, turn that into a class. Create our transforms. And again we, okay, so again we can have a look at a batch to make sure it looks sensible. And do it for, just did it for 25 epochs here. And gets to 94%. Now why did I do it for 25 epochs? Because I was trying to think about how do I beat my 50 epoch record, which was 94.6. And I thought, well what I could do is I could train for 25 epochs. And then I'll train a whole new model for a different 25 epochs. And I'm going to put it a different learner, learn2. Right, that this one is 94.1. So one of the models was 94.1, one of them was 94. Maybe you can guess what we're going to do next. It's a bit like test time augmentation, but rather than that we're going to grab the predictions of our first learner, and grab the predictions of our second learner, and stack them up and take them in. And this is called ensembling. And not surprisingly the ensemble is better than either of the two individual models at 94.4. Although unfortunately I'm afraid to say we didn't beat our best. But it's a useful trick, and particularly useful trick in this case I was kind of like trying something a bit interesting to see if using the exact same number of epochs, can I get a better result by using ensembling instead of training for longer. And the answer was I couldn't. Maybe it's because the random copy is not as good, or maybe I'm using too much augmentation. Who knows, but it's something that you could experiment with. So Shaowen mentions in the chat that cutmix is similar to this, which is actually that's a good point. I'd forgotten cutmix, but cutmix, yes, copies it from different images rather than from the same image. But yeah, it's pretty much the same thing I guess-ish. Well, similar. Yeah, very similar. All right, so that brings us to the end of the lesson. And you know I am, yeah, so pumped and excited to share this with you. Because you know I don't know that this has ever been done before, you know, to be able to go from, I mean even in our previous courses, we've never done this before, go from scratch, step by step, to an absolute state-of-the-art model where we build everything ourselves and it runs this quickly. And we're even using our own custom ResNet and everything, you know, just using common sense at every stage. And so hopefully that shows that deep learning is not magic, you know, that we can actually build the pieces ourselves. And yeah, as you'll see, going up to larger datasets, absolutely nothing changes. And so it's exactly these techniques. And this is actually, I do 99% of my research on very small datasets. Because you can iterate much more quickly, you can understand them much better. And I don't think there's ever been a time where I've then gone up to a bigger dataset and my findings didn't continue to hold true. Now homework, what I would really like you to do is to actually do the thing that I didn't do. Which is to do the create your own schedulers that work with Python's optimizers. So I mean, it's the tricky bit will be making sure that you understand the PyTorch API well, which I've really laid out here. So study this carefully. So create your own cosine annealing scheduler from scratch. And then create your own one cycle scheduler from scratch. And make sure that they work correctly with this batch scheduler callback. This will be a very good exercise for you in, you know, hopefully getting extremely frustrated as things don't work the way you hoped they would. And being mystified for a while. And then working through it, you know, using this very step-by-step approach. Lots of experimentation, lots of exploration. And then figuring it out. That's the journey I'm hoping you have. If it's all super easy and you get it first go, then you know, you'll have to find something else to do. But yeah, I'm hoping you'll find it actually, you know, surprisingly tricky to get it all working properly. And in the process of doing so, you're going to have to do a lot of exploration and experimentation. But you'll realize that it requires no, like, prerequisite knowledge at all. Okay, so if it doesn't work first time, it's not because there's something that you didn't learn in graduate school, if only you had done a PhD, whatever. It's just that you need to dig through, you know, slowly and carefully to see how it all works. And you know, then see how neat and concise you can get it. Then the other homework is to try and beat me. I really, really want people to beat me. Try to beat me on the 5 epoch, or the 20 epoch, or the 50 epoch fashion MNIST. Ideally using many AI with things that you've added yourself. But you know, you can try grabbing other libraries if you like. Or ideally, if you do grab another library and you find you can beat my approach, try to reimplement that library. That way you are still within the spirit of the game. Okay, so in our next lesson, Jono and Tanishka and I are going to be putting this all together to create a diffusion model from scratch. And we're actually going to be taking a couple of lessons for this. Not just a diffusion model, but a variety of interesting generative approaches. So we've kind of starting to come full circle. So thank you so much for joining me on this very extensive journey. And I look forward to hearing what you come up with. Please do come and join us on forums.fast.ai and share your progress. Bye!", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.04, "text": " Hi folks, thanks for joining me for lesson 18.", "tokens": [50364, 2421, 4024, 11, 3231, 337, 5549, 385, 337, 6898, 2443, 13, 50616], "temperature": 0.0, "avg_logprob": -0.28183662117301644, "compression_ratio": 1.441025641025641, "no_speech_prob": 5.648923252010718e-05}, {"id": 1, "seek": 0, "start": 5.04, "end": 10.28, "text": " We're going to start today in Microsoft Excel.", "tokens": [50616, 492, 434, 516, 281, 722, 965, 294, 8116, 19060, 13, 50878], "temperature": 0.0, "avg_logprob": -0.28183662117301644, "compression_ratio": 1.441025641025641, "no_speech_prob": 5.648923252010718e-05}, {"id": 2, "seek": 0, "start": 10.28, "end": 17.04, "text": " You'll see there's an Excel folder actually in the course 22p2 repo.", "tokens": [50878, 509, 603, 536, 456, 311, 364, 19060, 10820, 767, 294, 264, 1164, 5853, 79, 17, 49040, 13, 51216], "temperature": 0.0, "avg_logprob": -0.28183662117301644, "compression_ratio": 1.441025641025641, "no_speech_prob": 5.648923252010718e-05}, {"id": 3, "seek": 0, "start": 17.04, "end": 23.88, "text": " And in there there's a spreadsheet called graddesk as in gradient descent, which I guess", "tokens": [51216, 400, 294, 456, 456, 311, 257, 27733, 1219, 2771, 14792, 74, 382, 294, 16235, 23475, 11, 597, 286, 2041, 51558], "temperature": 0.0, "avg_logprob": -0.28183662117301644, "compression_ratio": 1.441025641025641, "no_speech_prob": 5.648923252010718e-05}, {"id": 4, "seek": 0, "start": 23.88, "end": 29.080000000000002, "text": " we should zoom in a bit here.", "tokens": [51558, 321, 820, 8863, 294, 257, 857, 510, 13, 51818], "temperature": 0.0, "avg_logprob": -0.28183662117301644, "compression_ratio": 1.441025641025641, "no_speech_prob": 5.648923252010718e-05}, {"id": 5, "seek": 2908, "start": 29.08, "end": 40.32, "text": " So there's some instructions here, but this is basically describing what's in each sheet.", "tokens": [50364, 407, 456, 311, 512, 9415, 510, 11, 457, 341, 307, 1936, 16141, 437, 311, 294, 1184, 8193, 13, 50926], "temperature": 0.0, "avg_logprob": -0.22366586450028092, "compression_ratio": 1.5364583333333333, "no_speech_prob": 4.4693570089293644e-05}, {"id": 6, "seek": 2908, "start": 40.32, "end": 46.959999999999994, "text": " We're going to be looking at the various SGD accelerated approaches we saw last time, but", "tokens": [50926, 492, 434, 516, 281, 312, 1237, 412, 264, 3683, 34520, 35, 29763, 11587, 321, 1866, 1036, 565, 11, 457, 51258], "temperature": 0.0, "avg_logprob": -0.22366586450028092, "compression_ratio": 1.5364583333333333, "no_speech_prob": 4.4693570089293644e-05}, {"id": 7, "seek": 2908, "start": 46.959999999999994, "end": 51.04, "text": " done in a spreadsheet.", "tokens": [51258, 1096, 294, 257, 27733, 13, 51462], "temperature": 0.0, "avg_logprob": -0.22366586450028092, "compression_ratio": 1.5364583333333333, "no_speech_prob": 4.4693570089293644e-05}, {"id": 8, "seek": 2908, "start": 51.04, "end": 57.4, "text": " We're going to do something very, very simple, which is to try to solve a linear regression.", "tokens": [51462, 492, 434, 516, 281, 360, 746, 588, 11, 588, 2199, 11, 597, 307, 281, 853, 281, 5039, 257, 8213, 24590, 13, 51780], "temperature": 0.0, "avg_logprob": -0.22366586450028092, "compression_ratio": 1.5364583333333333, "no_speech_prob": 4.4693570089293644e-05}, {"id": 9, "seek": 5740, "start": 57.4, "end": 65.24, "text": " So the actual data was generated with y equals ax plus b, where a, which is the slope, was", "tokens": [50364, 407, 264, 3539, 1412, 390, 10833, 365, 288, 6915, 6360, 1804, 272, 11, 689, 257, 11, 597, 307, 264, 13525, 11, 390, 50756], "temperature": 0.0, "avg_logprob": -0.2771863608524717, "compression_ratio": 1.3819444444444444, "no_speech_prob": 5.829107612953521e-05}, {"id": 10, "seek": 5740, "start": 65.24, "end": 71.12, "text": " 2, and b, which is the intercept or constant, was 30.", "tokens": [50756, 568, 11, 293, 272, 11, 597, 307, 264, 24700, 420, 5754, 11, 390, 2217, 13, 51050], "temperature": 0.0, "avg_logprob": -0.2771863608524717, "compression_ratio": 1.3819444444444444, "no_speech_prob": 5.829107612953521e-05}, {"id": 11, "seek": 5740, "start": 71.12, "end": 77.92, "text": " And so you can see we've got some random numbers here.", "tokens": [51050, 400, 370, 291, 393, 536, 321, 600, 658, 512, 4974, 3547, 510, 13, 51390], "temperature": 0.0, "avg_logprob": -0.2771863608524717, "compression_ratio": 1.3819444444444444, "no_speech_prob": 5.829107612953521e-05}, {"id": 12, "seek": 7792, "start": 77.92, "end": 87.44, "text": " And then over here we've got the ax plus b calculation.", "tokens": [50364, 400, 550, 670, 510, 321, 600, 658, 264, 6360, 1804, 272, 17108, 13, 50840], "temperature": 0.0, "avg_logprob": -0.22069203513009208, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.00015597922902088612}, {"id": 13, "seek": 7792, "start": 87.44, "end": 92.96000000000001, "text": " So then what I did is I copied and pasted as values, just one set of those random numbers", "tokens": [50840, 407, 550, 437, 286, 630, 307, 286, 25365, 293, 1791, 292, 382, 4190, 11, 445, 472, 992, 295, 729, 4974, 3547, 51116], "temperature": 0.0, "avg_logprob": -0.22069203513009208, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.00015597922902088612}, {"id": 14, "seek": 7792, "start": 92.96000000000001, "end": 95.64, "text": " into the next sheet called basic.", "tokens": [51116, 666, 264, 958, 8193, 1219, 3875, 13, 51250], "temperature": 0.0, "avg_logprob": -0.22069203513009208, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.00015597922902088612}, {"id": 15, "seek": 7792, "start": 95.64, "end": 97.32000000000001, "text": " This is the basic SGD sheet.", "tokens": [51250, 639, 307, 264, 3875, 34520, 35, 8193, 13, 51334], "temperature": 0.0, "avg_logprob": -0.22069203513009208, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.00015597922902088612}, {"id": 16, "seek": 7792, "start": 97.32000000000001, "end": 100.04, "text": " So that's what x and y are.", "tokens": [51334, 407, 300, 311, 437, 2031, 293, 288, 366, 13, 51470], "temperature": 0.0, "avg_logprob": -0.22069203513009208, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.00015597922902088612}, {"id": 17, "seek": 10004, "start": 100.04, "end": 111.32000000000001, "text": " And so the idea is we're going to try to use SGD to learn that the intercept is 30 and", "tokens": [50364, 400, 370, 264, 1558, 307, 321, 434, 516, 281, 853, 281, 764, 34520, 35, 281, 1466, 300, 264, 24700, 307, 2217, 293, 50928], "temperature": 0.0, "avg_logprob": -0.2234397971111795, "compression_ratio": 1.7206703910614525, "no_speech_prob": 0.004681662656366825}, {"id": 18, "seek": 10004, "start": 111.32000000000001, "end": 115.46000000000001, "text": " the slope is 2.", "tokens": [50928, 264, 13525, 307, 568, 13, 51135], "temperature": 0.0, "avg_logprob": -0.2234397971111795, "compression_ratio": 1.7206703910614525, "no_speech_prob": 0.004681662656366825}, {"id": 19, "seek": 10004, "start": 115.46000000000001, "end": 120.96000000000001, "text": " So the way we do SGD is we, so those are our weights or parameters.", "tokens": [51135, 407, 264, 636, 321, 360, 34520, 35, 307, 321, 11, 370, 729, 366, 527, 17443, 420, 9834, 13, 51410], "temperature": 0.0, "avg_logprob": -0.2234397971111795, "compression_ratio": 1.7206703910614525, "no_speech_prob": 0.004681662656366825}, {"id": 20, "seek": 10004, "start": 120.96000000000001, "end": 124.08000000000001, "text": " So the way we do SGD is we start out at some random kind of guess.", "tokens": [51410, 407, 264, 636, 321, 360, 34520, 35, 307, 321, 722, 484, 412, 512, 4974, 733, 295, 2041, 13, 51566], "temperature": 0.0, "avg_logprob": -0.2234397971111795, "compression_ratio": 1.7206703910614525, "no_speech_prob": 0.004681662656366825}, {"id": 21, "seek": 10004, "start": 124.08000000000001, "end": 128.22, "text": " So my random guess is going to be 1 and 1 for the intercept and slope.", "tokens": [51566, 407, 452, 4974, 2041, 307, 516, 281, 312, 502, 293, 502, 337, 264, 24700, 293, 13525, 13, 51773], "temperature": 0.0, "avg_logprob": -0.2234397971111795, "compression_ratio": 1.7206703910614525, "no_speech_prob": 0.004681662656366825}, {"id": 22, "seek": 12822, "start": 128.22, "end": 135.58, "text": " And so if we look at the very first data point, which is x is 14 and y is 58, the intercept", "tokens": [50364, 400, 370, 498, 321, 574, 412, 264, 588, 700, 1412, 935, 11, 597, 307, 2031, 307, 3499, 293, 288, 307, 21786, 11, 264, 24700, 50732], "temperature": 0.0, "avg_logprob": -0.22181854248046876, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0005274750292301178}, {"id": 23, "seek": 12822, "start": 135.58, "end": 137.18, "text": " and slope are both 1.", "tokens": [50732, 293, 13525, 366, 1293, 502, 13, 50812], "temperature": 0.0, "avg_logprob": -0.22181854248046876, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0005274750292301178}, {"id": 24, "seek": 12822, "start": 137.18, "end": 139.18, "text": " Then we can make a prediction.", "tokens": [50812, 1396, 321, 393, 652, 257, 17630, 13, 50912], "temperature": 0.0, "avg_logprob": -0.22181854248046876, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0005274750292301178}, {"id": 25, "seek": 12822, "start": 139.18, "end": 149.9, "text": " And so our prediction is just equal to slope times x plus the intercept.", "tokens": [50912, 400, 370, 527, 17630, 307, 445, 2681, 281, 13525, 1413, 2031, 1804, 264, 24700, 13, 51448], "temperature": 0.0, "avg_logprob": -0.22181854248046876, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0005274750292301178}, {"id": 26, "seek": 12822, "start": 149.9, "end": 152.66, "text": " So the prediction will be 15.", "tokens": [51448, 407, 264, 17630, 486, 312, 2119, 13, 51586], "temperature": 0.0, "avg_logprob": -0.22181854248046876, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0005274750292301178}, {"id": 27, "seek": 12822, "start": 152.66, "end": 155.14, "text": " Now actually the answer was 58.", "tokens": [51586, 823, 767, 264, 1867, 390, 21786, 13, 51710], "temperature": 0.0, "avg_logprob": -0.22181854248046876, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0005274750292301178}, {"id": 28, "seek": 12822, "start": 155.14, "end": 156.76, "text": " So we're a long way off.", "tokens": [51710, 407, 321, 434, 257, 938, 636, 766, 13, 51791], "temperature": 0.0, "avg_logprob": -0.22181854248046876, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0005274750292301178}, {"id": 29, "seek": 15676, "start": 156.79999999999998, "end": 159.6, "text": " So we're going to use mean squared error.", "tokens": [50366, 407, 321, 434, 516, 281, 764, 914, 8889, 6713, 13, 50506], "temperature": 0.0, "avg_logprob": -0.3284984484110793, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.019418908283114433}, {"id": 30, "seek": 15676, "start": 159.6, "end": 167.16, "text": " So the mean squared error is just the error, so the difference squared.", "tokens": [50506, 407, 264, 914, 8889, 6713, 307, 445, 264, 6713, 11, 370, 264, 2649, 8889, 13, 50884], "temperature": 0.0, "avg_logprob": -0.3284984484110793, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.019418908283114433}, {"id": 31, "seek": 15676, "start": 167.16, "end": 175.07999999999998, "text": " Okay, so one way to calculate how much would the prediction, sorry, how much would the", "tokens": [50884, 1033, 11, 370, 472, 636, 281, 8873, 577, 709, 576, 264, 17630, 11, 2597, 11, 577, 709, 576, 264, 51280], "temperature": 0.0, "avg_logprob": -0.3284984484110793, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.019418908283114433}, {"id": 32, "seek": 15676, "start": 175.07999999999998, "end": 183.88, "text": " error change, so how much would the squared error, I should say change, if we changed", "tokens": [51280, 6713, 1319, 11, 370, 577, 709, 576, 264, 8889, 6713, 11, 286, 820, 584, 1319, 11, 498, 321, 3105, 51720], "temperature": 0.0, "avg_logprob": -0.3284984484110793, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.019418908283114433}, {"id": 33, "seek": 18388, "start": 183.88, "end": 189.76, "text": " the intercept, which is b, would be just to change b by a little bit, change the intercept", "tokens": [50364, 264, 24700, 11, 597, 307, 272, 11, 576, 312, 445, 281, 1319, 272, 538, 257, 707, 857, 11, 1319, 264, 24700, 50658], "temperature": 0.0, "avg_logprob": -0.2216935238595736, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.04813418164849281}, {"id": 34, "seek": 18388, "start": 189.76, "end": 191.84, "text": " by a little bit and see what the error is.", "tokens": [50658, 538, 257, 707, 857, 293, 536, 437, 264, 6713, 307, 13, 50762], "temperature": 0.0, "avg_logprob": -0.2216935238595736, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.04813418164849281}, {"id": 35, "seek": 18388, "start": 191.84, "end": 199.28, "text": " So here that's what I've done is I've just added 0.01 to the intercept and then calculated", "tokens": [50762, 407, 510, 300, 311, 437, 286, 600, 1096, 307, 286, 600, 445, 3869, 1958, 13, 10607, 281, 264, 24700, 293, 550, 15598, 51134], "temperature": 0.0, "avg_logprob": -0.2216935238595736, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.04813418164849281}, {"id": 36, "seek": 18388, "start": 199.28, "end": 203.56, "text": " y and then calculated the difference squared.", "tokens": [51134, 288, 293, 550, 15598, 264, 2649, 8889, 13, 51348], "temperature": 0.0, "avg_logprob": -0.2216935238595736, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.04813418164849281}, {"id": 37, "seek": 18388, "start": 203.56, "end": 205.92, "text": " And so this is what I mean by err b1.", "tokens": [51348, 400, 370, 341, 307, 437, 286, 914, 538, 45267, 272, 16, 13, 51466], "temperature": 0.0, "avg_logprob": -0.2216935238595736, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.04813418164849281}, {"id": 38, "seek": 18388, "start": 205.92, "end": 210.96, "text": " This is the error squared I get if I change b by 0.01.", "tokens": [51466, 639, 307, 264, 6713, 8889, 286, 483, 498, 286, 1319, 272, 538, 1958, 13, 10607, 13, 51718], "temperature": 0.0, "avg_logprob": -0.2216935238595736, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.04813418164849281}, {"id": 39, "seek": 18388, "start": 210.96, "end": 213.56, "text": " So it's made the error go down a little bit.", "tokens": [51718, 407, 309, 311, 1027, 264, 6713, 352, 760, 257, 707, 857, 13, 51848], "temperature": 0.0, "avg_logprob": -0.2216935238595736, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.04813418164849281}, {"id": 40, "seek": 21356, "start": 214.24, "end": 221.48, "text": " So that suggests that we should probably increase b, increase the intercept.", "tokens": [50398, 407, 300, 13409, 300, 321, 820, 1391, 3488, 272, 11, 3488, 264, 24700, 13, 50760], "temperature": 0.0, "avg_logprob": -0.20940060228914828, "compression_ratio": 1.6298342541436464, "no_speech_prob": 4.9086607759818435e-05}, {"id": 41, "seek": 21356, "start": 221.48, "end": 230.68, "text": " So we can calculate the estimated derivative by simply taking the change from when we use", "tokens": [50760, 407, 321, 393, 8873, 264, 14109, 13760, 538, 2935, 1940, 264, 1319, 490, 562, 321, 764, 51220], "temperature": 0.0, "avg_logprob": -0.20940060228914828, "compression_ratio": 1.6298342541436464, "no_speech_prob": 4.9086607759818435e-05}, {"id": 42, "seek": 21356, "start": 230.68, "end": 234.84, "text": " the actual intercept using the intercept plus 0.01.", "tokens": [51220, 264, 3539, 24700, 1228, 264, 24700, 1804, 1958, 13, 10607, 13, 51428], "temperature": 0.0, "avg_logprob": -0.20940060228914828, "compression_ratio": 1.6298342541436464, "no_speech_prob": 4.9086607759818435e-05}, {"id": 43, "seek": 21356, "start": 234.84, "end": 240.24, "text": " So that's the rise and we divide it by the run, which is as we said is 0.01.", "tokens": [51428, 407, 300, 311, 264, 6272, 293, 321, 9845, 309, 538, 264, 1190, 11, 597, 307, 382, 321, 848, 307, 1958, 13, 10607, 13, 51698], "temperature": 0.0, "avg_logprob": -0.20940060228914828, "compression_ratio": 1.6298342541436464, "no_speech_prob": 4.9086607759818435e-05}, {"id": 44, "seek": 24024, "start": 240.24, "end": 245.48000000000002, "text": " And that gives us the estimated derivative of the squared error with respect to b, the", "tokens": [50364, 400, 300, 2709, 505, 264, 14109, 13760, 295, 264, 8889, 6713, 365, 3104, 281, 272, 11, 264, 50626], "temperature": 0.0, "avg_logprob": -0.27288387917183543, "compression_ratio": 1.423913043478261, "no_speech_prob": 0.01065222267061472}, {"id": 45, "seek": 24024, "start": 245.48000000000002, "end": 246.48000000000002, "text": " intercept.", "tokens": [50626, 24700, 13, 50676], "temperature": 0.0, "avg_logprob": -0.27288387917183543, "compression_ratio": 1.423913043478261, "no_speech_prob": 0.01065222267061472}, {"id": 46, "seek": 24024, "start": 246.48000000000002, "end": 253.36, "text": " Okay, so it's about negative 86, 85.99.", "tokens": [50676, 1033, 11, 370, 309, 311, 466, 3671, 26687, 11, 14695, 13, 8494, 13, 51020], "temperature": 0.0, "avg_logprob": -0.27288387917183543, "compression_ratio": 1.423913043478261, "no_speech_prob": 0.01065222267061472}, {"id": 47, "seek": 24024, "start": 253.36, "end": 261.24, "text": " So we can do exactly the same thing for a, so change the slope by 0.01, calculate y,", "tokens": [51020, 407, 321, 393, 360, 2293, 264, 912, 551, 337, 257, 11, 370, 1319, 264, 13525, 538, 1958, 13, 10607, 11, 8873, 288, 11, 51414], "temperature": 0.0, "avg_logprob": -0.27288387917183543, "compression_ratio": 1.423913043478261, "no_speech_prob": 0.01065222267061472}, {"id": 48, "seek": 24024, "start": 261.24, "end": 263.96000000000004, "text": " calculate the difference and square it.", "tokens": [51414, 8873, 264, 2649, 293, 3732, 309, 13, 51550], "temperature": 0.0, "avg_logprob": -0.27288387917183543, "compression_ratio": 1.423913043478261, "no_speech_prob": 0.01065222267061472}, {"id": 49, "seek": 26396, "start": 263.96, "end": 270.29999999999995, "text": " And we can calculate the estimated derivative in the same way, rise, which is the difference", "tokens": [50364, 400, 321, 393, 8873, 264, 14109, 13760, 294, 264, 912, 636, 11, 6272, 11, 597, 307, 264, 2649, 50681], "temperature": 0.0, "avg_logprob": -0.2436432587473016, "compression_ratio": 1.8089430894308942, "no_speech_prob": 0.07695356011390686}, {"id": 50, "seek": 26396, "start": 270.29999999999995, "end": 273.47999999999996, "text": " divided by run, which is 0.01.", "tokens": [50681, 6666, 538, 1190, 11, 597, 307, 1958, 13, 10607, 13, 50840], "temperature": 0.0, "avg_logprob": -0.2436432587473016, "compression_ratio": 1.8089430894308942, "no_speech_prob": 0.07695356011390686}, {"id": 51, "seek": 26396, "start": 273.47999999999996, "end": 276.64, "text": " And that's quite a big number, minus 1200.", "tokens": [50840, 400, 300, 311, 1596, 257, 955, 1230, 11, 3175, 29139, 13, 50998], "temperature": 0.0, "avg_logprob": -0.2436432587473016, "compression_ratio": 1.8089430894308942, "no_speech_prob": 0.07695356011390686}, {"id": 52, "seek": 26396, "start": 276.64, "end": 280.88, "text": " In both cases, the estimated derivatives are negative.", "tokens": [50998, 682, 1293, 3331, 11, 264, 14109, 33733, 366, 3671, 13, 51210], "temperature": 0.0, "avg_logprob": -0.2436432587473016, "compression_ratio": 1.8089430894308942, "no_speech_prob": 0.07695356011390686}, {"id": 53, "seek": 26396, "start": 280.88, "end": 284.47999999999996, "text": " So that suggests we should increase the intercept and the slope.", "tokens": [51210, 407, 300, 13409, 321, 820, 3488, 264, 24700, 293, 264, 13525, 13, 51390], "temperature": 0.0, "avg_logprob": -0.2436432587473016, "compression_ratio": 1.8089430894308942, "no_speech_prob": 0.07695356011390686}, {"id": 54, "seek": 26396, "start": 284.47999999999996, "end": 287.74, "text": " And we know that that's true because actually the intercept and the slope are both bigger", "tokens": [51390, 400, 321, 458, 300, 300, 311, 2074, 570, 767, 264, 24700, 293, 264, 13525, 366, 1293, 3801, 51553], "temperature": 0.0, "avg_logprob": -0.2436432587473016, "compression_ratio": 1.8089430894308942, "no_speech_prob": 0.07695356011390686}, {"id": 55, "seek": 26396, "start": 287.74, "end": 289.0, "text": " than 1.", "tokens": [51553, 813, 502, 13, 51616], "temperature": 0.0, "avg_logprob": -0.2436432587473016, "compression_ratio": 1.8089430894308942, "no_speech_prob": 0.07695356011390686}, {"id": 56, "seek": 26396, "start": 289.0, "end": 293.24, "text": " The intercept is 30, should be 30 and the slope should be 2.", "tokens": [51616, 440, 24700, 307, 2217, 11, 820, 312, 2217, 293, 264, 13525, 820, 312, 568, 13, 51828], "temperature": 0.0, "avg_logprob": -0.2436432587473016, "compression_ratio": 1.8089430894308942, "no_speech_prob": 0.07695356011390686}, {"id": 57, "seek": 29324, "start": 293.52, "end": 296.32, "text": " So there's one way to calculate the derivatives.", "tokens": [50378, 407, 456, 311, 472, 636, 281, 8873, 264, 33733, 13, 50518], "temperature": 0.0, "avg_logprob": -0.3015416910950567, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0006986739463172853}, {"id": 58, "seek": 29324, "start": 296.32, "end": 299.32, "text": " Another way is analytically.", "tokens": [50518, 3996, 636, 307, 10783, 984, 13, 50668], "temperature": 0.0, "avg_logprob": -0.3015416910950567, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0006986739463172853}, {"id": 59, "seek": 29324, "start": 299.32, "end": 307.0, "text": " And the derivative of squared is 2 times.", "tokens": [50668, 400, 264, 13760, 295, 8889, 307, 568, 1413, 13, 51052], "temperature": 0.0, "avg_logprob": -0.3015416910950567, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0006986739463172853}, {"id": 60, "seek": 29324, "start": 307.0, "end": 309.12, "text": " So here it is here.", "tokens": [51052, 407, 510, 309, 307, 510, 13, 51158], "temperature": 0.0, "avg_logprob": -0.3015416910950567, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0006986739463172853}, {"id": 61, "seek": 29324, "start": 309.12, "end": 312.76, "text": " I've just written it down for you.", "tokens": [51158, 286, 600, 445, 3720, 309, 760, 337, 291, 13, 51340], "temperature": 0.0, "avg_logprob": -0.3015416910950567, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0006986739463172853}, {"id": 62, "seek": 29324, "start": 312.76, "end": 315.12, "text": " So here's the analytic derivative.", "tokens": [51340, 407, 510, 311, 264, 40358, 13760, 13, 51458], "temperature": 0.0, "avg_logprob": -0.3015416910950567, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0006986739463172853}, {"id": 63, "seek": 29324, "start": 315.12, "end": 320.1, "text": " It's just 2 times the difference.", "tokens": [51458, 467, 311, 445, 568, 1413, 264, 2649, 13, 51707], "temperature": 0.0, "avg_logprob": -0.3015416910950567, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0006986739463172853}, {"id": 64, "seek": 32010, "start": 320.1, "end": 327.26000000000005, "text": " And then the derivative for the slope is here.", "tokens": [50364, 400, 550, 264, 13760, 337, 264, 13525, 307, 510, 13, 50722], "temperature": 0.0, "avg_logprob": -0.25643361552377764, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.016402708366513252}, {"id": 65, "seek": 32010, "start": 327.26000000000005, "end": 332.1, "text": " And you can see that the estimated version using the rise over run and the little 0.01", "tokens": [50722, 400, 291, 393, 536, 300, 264, 14109, 3037, 1228, 264, 6272, 670, 1190, 293, 264, 707, 1958, 13, 10607, 50964], "temperature": 0.0, "avg_logprob": -0.25643361552377764, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.016402708366513252}, {"id": 66, "seek": 32010, "start": 332.1, "end": 335.42, "text": " change and the actual, they're pretty similar.", "tokens": [50964, 1319, 293, 264, 3539, 11, 436, 434, 1238, 2531, 13, 51130], "temperature": 0.0, "avg_logprob": -0.25643361552377764, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.016402708366513252}, {"id": 67, "seek": 32010, "start": 335.42, "end": 336.6, "text": " Okay.", "tokens": [51130, 1033, 13, 51189], "temperature": 0.0, "avg_logprob": -0.25643361552377764, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.016402708366513252}, {"id": 68, "seek": 32010, "start": 336.6, "end": 337.6, "text": " And same thing here.", "tokens": [51189, 400, 912, 551, 510, 13, 51239], "temperature": 0.0, "avg_logprob": -0.25643361552377764, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.016402708366513252}, {"id": 69, "seek": 32010, "start": 337.6, "end": 338.78000000000003, "text": " They're pretty similar.", "tokens": [51239, 814, 434, 1238, 2531, 13, 51298], "temperature": 0.0, "avg_logprob": -0.25643361552377764, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.016402708366513252}, {"id": 70, "seek": 32010, "start": 338.78000000000003, "end": 346.82000000000005, "text": " So anytime I calculate gradients kind of analytically, but by hand, I always like to test them against", "tokens": [51298, 407, 13038, 286, 8873, 2771, 2448, 733, 295, 10783, 984, 11, 457, 538, 1011, 11, 286, 1009, 411, 281, 1500, 552, 1970, 51700], "temperature": 0.0, "avg_logprob": -0.25643361552377764, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.016402708366513252}, {"id": 71, "seek": 34682, "start": 347.14, "end": 350.7, "text": " the actual rise over run calculation with some small number.", "tokens": [50380, 264, 3539, 6272, 670, 1190, 17108, 365, 512, 1359, 1230, 13, 50558], "temperature": 0.0, "avg_logprob": -0.27932928026336984, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.021287059411406517}, {"id": 72, "seek": 34682, "start": 350.7, "end": 353.7, "text": " And this is called using the finite differencing approach.", "tokens": [50558, 400, 341, 307, 1219, 1228, 264, 19362, 743, 13644, 3109, 13, 50708], "temperature": 0.0, "avg_logprob": -0.27932928026336984, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.021287059411406517}, {"id": 73, "seek": 34682, "start": 353.7, "end": 357.98, "text": " We only use it for testing because it's slow, because you have to do a separate calculation", "tokens": [50708, 492, 787, 764, 309, 337, 4997, 570, 309, 311, 2964, 11, 570, 291, 362, 281, 360, 257, 4994, 17108, 50922], "temperature": 0.0, "avg_logprob": -0.27932928026336984, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.021287059411406517}, {"id": 74, "seek": 34682, "start": 357.98, "end": 364.14, "text": " for every single weight.", "tokens": [50922, 337, 633, 2167, 3364, 13, 51230], "temperature": 0.0, "avg_logprob": -0.27932928026336984, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.021287059411406517}, {"id": 75, "seek": 34682, "start": 364.14, "end": 365.56, "text": " But it's good for testing.", "tokens": [51230, 583, 309, 311, 665, 337, 4997, 13, 51301], "temperature": 0.0, "avg_logprob": -0.27932928026336984, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.021287059411406517}, {"id": 76, "seek": 34682, "start": 365.56, "end": 368.46, "text": " We use analytic derivatives all the time in real life.", "tokens": [51301, 492, 764, 40358, 33733, 439, 264, 565, 294, 957, 993, 13, 51446], "temperature": 0.0, "avg_logprob": -0.27932928026336984, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.021287059411406517}, {"id": 77, "seek": 34682, "start": 368.46, "end": 374.9, "text": " Anyway, so however we calculate the derivatives, we can now calculate a new slope.", "tokens": [51446, 5684, 11, 370, 4461, 321, 8873, 264, 33733, 11, 321, 393, 586, 8873, 257, 777, 13525, 13, 51768], "temperature": 0.0, "avg_logprob": -0.27932928026336984, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.021287059411406517}, {"id": 78, "seek": 37490, "start": 374.9, "end": 382.26, "text": " And our new slope will be equal to the previous slope minus the derivative times the learning", "tokens": [50364, 400, 527, 777, 13525, 486, 312, 2681, 281, 264, 3894, 13525, 3175, 264, 13760, 1413, 264, 2539, 50732], "temperature": 0.0, "avg_logprob": -0.29609840606974663, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.011157942935824394}, {"id": 79, "seek": 37490, "start": 382.26, "end": 387.29999999999995, "text": " rate, which we just set here at 0.0001.", "tokens": [50732, 3314, 11, 597, 321, 445, 992, 510, 412, 1958, 13, 1360, 16, 13, 50984], "temperature": 0.0, "avg_logprob": -0.29609840606974663, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.011157942935824394}, {"id": 80, "seek": 37490, "start": 387.29999999999995, "end": 392.34, "text": " And we can do the same thing for the intercept as you see.", "tokens": [50984, 400, 321, 393, 360, 264, 912, 551, 337, 264, 24700, 382, 291, 536, 13, 51236], "temperature": 0.0, "avg_logprob": -0.29609840606974663, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.011157942935824394}, {"id": 81, "seek": 37490, "start": 392.34, "end": 395.41999999999996, "text": " And so here's our new slope intercept.", "tokens": [51236, 400, 370, 510, 311, 527, 777, 13525, 24700, 13, 51390], "temperature": 0.0, "avg_logprob": -0.29609840606974663, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.011157942935824394}, {"id": 82, "seek": 37490, "start": 395.41999999999996, "end": 398.06, "text": " So we can use that for the second row of data.", "tokens": [51390, 407, 321, 393, 764, 300, 337, 264, 1150, 5386, 295, 1412, 13, 51522], "temperature": 0.0, "avg_logprob": -0.29609840606974663, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.011157942935824394}, {"id": 83, "seek": 37490, "start": 398.06, "end": 401.56, "text": " So the second row of data is x equals 86, y equals 202.", "tokens": [51522, 407, 264, 1150, 5386, 295, 1412, 307, 2031, 6915, 26687, 11, 288, 6915, 945, 17, 13, 51697], "temperature": 0.0, "avg_logprob": -0.29609840606974663, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.011157942935824394}, {"id": 84, "seek": 37490, "start": 401.56, "end": 404.78, "text": " So our intercept is not 1, 1 anymore.", "tokens": [51697, 407, 527, 24700, 307, 406, 502, 11, 502, 3602, 13, 51858], "temperature": 0.0, "avg_logprob": -0.29609840606974663, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.011157942935824394}, {"id": 85, "seek": 40478, "start": 404.85999999999996, "end": 410.14, "text": " Intercept and slope are not 1, 1, but they're 1.01 and 1.12.", "tokens": [50368, 5751, 1336, 293, 13525, 366, 406, 502, 11, 502, 11, 457, 436, 434, 502, 13, 10607, 293, 502, 13, 4762, 13, 50632], "temperature": 0.0, "avg_logprob": -0.2671915839700138, "compression_ratio": 1.5947368421052632, "no_speech_prob": 0.0060975076630711555}, {"id": 86, "seek": 40478, "start": 410.14, "end": 417.46, "text": " So here's we're just using a formula just to point at the new intercept and slope.", "tokens": [50632, 407, 510, 311, 321, 434, 445, 1228, 257, 8513, 445, 281, 935, 412, 264, 777, 24700, 293, 13525, 13, 50998], "temperature": 0.0, "avg_logprob": -0.2671915839700138, "compression_ratio": 1.5947368421052632, "no_speech_prob": 0.0060975076630711555}, {"id": 87, "seek": 40478, "start": 417.46, "end": 424.7, "text": " We can get a new prediction and squared error and derivatives.", "tokens": [50998, 492, 393, 483, 257, 777, 17630, 293, 8889, 6713, 293, 33733, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2671915839700138, "compression_ratio": 1.5947368421052632, "no_speech_prob": 0.0060975076630711555}, {"id": 88, "seek": 40478, "start": 424.7, "end": 429.05999999999995, "text": " And then we can get another new slope and intercept.", "tokens": [51360, 400, 550, 321, 393, 483, 1071, 777, 13525, 293, 24700, 13, 51578], "temperature": 0.0, "avg_logprob": -0.2671915839700138, "compression_ratio": 1.5947368421052632, "no_speech_prob": 0.0060975076630711555}, {"id": 89, "seek": 40478, "start": 429.05999999999995, "end": 430.7, "text": " And so that was a pretty good one actually.", "tokens": [51578, 400, 370, 300, 390, 257, 1238, 665, 472, 767, 13, 51660], "temperature": 0.0, "avg_logprob": -0.2671915839700138, "compression_ratio": 1.5947368421052632, "no_speech_prob": 0.0060975076630711555}, {"id": 90, "seek": 43070, "start": 430.7, "end": 435.24, "text": " It really helped our slope head in the right direction, although the intercept's moving", "tokens": [50364, 467, 534, 4254, 527, 13525, 1378, 294, 264, 558, 3513, 11, 4878, 264, 24700, 311, 2684, 50591], "temperature": 0.0, "avg_logprob": -0.262893052858727, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.05920476093888283}, {"id": 91, "seek": 43070, "start": 435.24, "end": 437.14, "text": " pretty slowly.", "tokens": [50591, 1238, 5692, 13, 50686], "temperature": 0.0, "avg_logprob": -0.262893052858727, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.05920476093888283}, {"id": 92, "seek": 43070, "start": 437.14, "end": 440.97999999999996, "text": " And so we can do that for every row of data.", "tokens": [50686, 400, 370, 321, 393, 360, 300, 337, 633, 5386, 295, 1412, 13, 50878], "temperature": 0.0, "avg_logprob": -0.262893052858727, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.05920476093888283}, {"id": 93, "seek": 43070, "start": 440.97999999999996, "end": 447.58, "text": " Now strictly speaking, this is not mini-batch gradient descent that we normally do in deep", "tokens": [50878, 823, 20792, 4124, 11, 341, 307, 406, 8382, 12, 65, 852, 16235, 23475, 300, 321, 5646, 360, 294, 2452, 51208], "temperature": 0.0, "avg_logprob": -0.262893052858727, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.05920476093888283}, {"id": 94, "seek": 43070, "start": 447.58, "end": 448.58, "text": " learning.", "tokens": [51208, 2539, 13, 51258], "temperature": 0.0, "avg_logprob": -0.262893052858727, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.05920476093888283}, {"id": 95, "seek": 43070, "start": 448.58, "end": 451.98, "text": " It's a simpler version where every batch is a size 1.", "tokens": [51258, 467, 311, 257, 18587, 3037, 689, 633, 15245, 307, 257, 2744, 502, 13, 51428], "temperature": 0.0, "avg_logprob": -0.262893052858727, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.05920476093888283}, {"id": 96, "seek": 43070, "start": 451.98, "end": 457.53999999999996, "text": " So I mean it's still stochastic gradient descent, it's just not, it's just a batch size of 1.", "tokens": [51428, 407, 286, 914, 309, 311, 920, 342, 8997, 2750, 16235, 23475, 11, 309, 311, 445, 406, 11, 309, 311, 445, 257, 15245, 2744, 295, 502, 13, 51706], "temperature": 0.0, "avg_logprob": -0.262893052858727, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.05920476093888283}, {"id": 97, "seek": 45754, "start": 457.54, "end": 463.54, "text": " I think sometimes it's called online gradient descent, if I remember correctly.", "tokens": [50364, 286, 519, 2171, 309, 311, 1219, 2950, 16235, 23475, 11, 498, 286, 1604, 8944, 13, 50664], "temperature": 0.0, "avg_logprob": -0.21461825424365782, "compression_ratio": 1.543778801843318, "no_speech_prob": 0.0006263335235416889}, {"id": 98, "seek": 45754, "start": 463.54, "end": 469.38, "text": " So we go through every data point in our very small data set until we get to the very end.", "tokens": [50664, 407, 321, 352, 807, 633, 1412, 935, 294, 527, 588, 1359, 1412, 992, 1826, 321, 483, 281, 264, 588, 917, 13, 50956], "temperature": 0.0, "avg_logprob": -0.21461825424365782, "compression_ratio": 1.543778801843318, "no_speech_prob": 0.0006263335235416889}, {"id": 99, "seek": 45754, "start": 469.38, "end": 476.42, "text": " And so at the end of the first epoch, we've got an intercept of 1.06 and a slope of 2.57.", "tokens": [50956, 400, 370, 412, 264, 917, 295, 264, 700, 30992, 339, 11, 321, 600, 658, 364, 24700, 295, 502, 13, 12791, 293, 257, 13525, 295, 568, 13, 19004, 13, 51308], "temperature": 0.0, "avg_logprob": -0.21461825424365782, "compression_ratio": 1.543778801843318, "no_speech_prob": 0.0006263335235416889}, {"id": 100, "seek": 45754, "start": 476.42, "end": 481.48, "text": " And those indeed are better estimates than our starting estimates of 1, 1.", "tokens": [51308, 400, 729, 6451, 366, 1101, 20561, 813, 527, 2891, 20561, 295, 502, 11, 502, 13, 51561], "temperature": 0.0, "avg_logprob": -0.21461825424365782, "compression_ratio": 1.543778801843318, "no_speech_prob": 0.0006263335235416889}, {"id": 101, "seek": 48148, "start": 481.48, "end": 490.20000000000005, "text": " So what I would do is I would copy our slope 2.57 up to here, 2.57, I'll just type it for", "tokens": [50364, 407, 437, 286, 576, 360, 307, 286, 576, 5055, 527, 13525, 568, 13, 19004, 493, 281, 510, 11, 568, 13, 19004, 11, 286, 603, 445, 2010, 309, 337, 50800], "temperature": 0.0, "avg_logprob": -0.22626578941773834, "compression_ratio": 1.7528735632183907, "no_speech_prob": 0.07584913074970245}, {"id": 102, "seek": 48148, "start": 490.20000000000005, "end": 491.20000000000005, "text": " now.", "tokens": [50800, 586, 13, 50850], "temperature": 0.0, "avg_logprob": -0.22626578941773834, "compression_ratio": 1.7528735632183907, "no_speech_prob": 0.07584913074970245}, {"id": 103, "seek": 48148, "start": 491.20000000000005, "end": 496.96000000000004, "text": " And I'll copy our intercept up to here.", "tokens": [50850, 400, 286, 603, 5055, 527, 24700, 493, 281, 510, 13, 51138], "temperature": 0.0, "avg_logprob": -0.22626578941773834, "compression_ratio": 1.7528735632183907, "no_speech_prob": 0.07584913074970245}, {"id": 104, "seek": 48148, "start": 496.96000000000004, "end": 504.86, "text": " And then it goes through the entire epoch again, and we get another intercept and slope.", "tokens": [51138, 400, 550, 309, 1709, 807, 264, 2302, 30992, 339, 797, 11, 293, 321, 483, 1071, 24700, 293, 13525, 13, 51533], "temperature": 0.0, "avg_logprob": -0.22626578941773834, "compression_ratio": 1.7528735632183907, "no_speech_prob": 0.07584913074970245}, {"id": 105, "seek": 48148, "start": 504.86, "end": 509.86, "text": " And so we could keep copying and pasting and copying and pasting again and again.", "tokens": [51533, 400, 370, 321, 727, 1066, 27976, 293, 1791, 278, 293, 27976, 293, 1791, 278, 797, 293, 797, 13, 51783], "temperature": 0.0, "avg_logprob": -0.22626578941773834, "compression_ratio": 1.7528735632183907, "no_speech_prob": 0.07584913074970245}, {"id": 106, "seek": 50986, "start": 509.86, "end": 513.7, "text": " And we can watch the root mean squared error going down.", "tokens": [50364, 400, 321, 393, 1159, 264, 5593, 914, 8889, 6713, 516, 760, 13, 50556], "temperature": 0.0, "avg_logprob": -0.24967025642964377, "compression_ratio": 1.3989071038251366, "no_speech_prob": 0.05108137056231499}, {"id": 107, "seek": 50986, "start": 513.7, "end": 517.98, "text": " Now that's pretty boring doing that copying and pasting.", "tokens": [50556, 823, 300, 311, 1238, 9989, 884, 300, 27976, 293, 1791, 278, 13, 50770], "temperature": 0.0, "avg_logprob": -0.24967025642964377, "compression_ratio": 1.3989071038251366, "no_speech_prob": 0.05108137056231499}, {"id": 108, "seek": 50986, "start": 517.98, "end": 530.98, "text": " So what we could do is fire up Visual Basic for applications.", "tokens": [50770, 407, 437, 321, 727, 360, 307, 2610, 493, 23187, 31598, 337, 5821, 13, 51420], "temperature": 0.0, "avg_logprob": -0.24967025642964377, "compression_ratio": 1.3989071038251366, "no_speech_prob": 0.05108137056231499}, {"id": 109, "seek": 50986, "start": 530.98, "end": 536.94, "text": " And sorry this might be a bit small, I'm not sure how to increase the font size.", "tokens": [51420, 400, 2597, 341, 1062, 312, 257, 857, 1359, 11, 286, 478, 406, 988, 577, 281, 3488, 264, 10703, 2744, 13, 51718], "temperature": 0.0, "avg_logprob": -0.24967025642964377, "compression_ratio": 1.3989071038251366, "no_speech_prob": 0.05108137056231499}, {"id": 110, "seek": 53694, "start": 536.94, "end": 543.6600000000001, "text": " And what it shows here, so sorry this is a bit small, so you might want to just open", "tokens": [50364, 400, 437, 309, 3110, 510, 11, 370, 2597, 341, 307, 257, 857, 1359, 11, 370, 291, 1062, 528, 281, 445, 1269, 50700], "temperature": 0.0, "avg_logprob": -0.24974915532782527, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.03461696207523346}, {"id": 111, "seek": 53694, "start": 543.6600000000001, "end": 545.58, "text": " it on your own computer to be able to see it clearly.", "tokens": [50700, 309, 322, 428, 1065, 3820, 281, 312, 1075, 281, 536, 309, 4448, 13, 50796], "temperature": 0.0, "avg_logprob": -0.24974915532782527, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.03461696207523346}, {"id": 112, "seek": 53694, "start": 545.58, "end": 552.5, "text": " But basically it shows I've created a little macro where if you click on the reset button,", "tokens": [50796, 583, 1936, 309, 3110, 286, 600, 2942, 257, 707, 18887, 689, 498, 291, 2052, 322, 264, 14322, 2960, 11, 51142], "temperature": 0.0, "avg_logprob": -0.24974915532782527, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.03461696207523346}, {"id": 113, "seek": 53694, "start": 552.5, "end": 559.8000000000001, "text": " it's just going to set the slope and constant to 1 and calculate.", "tokens": [51142, 309, 311, 445, 516, 281, 992, 264, 13525, 293, 5754, 281, 502, 293, 8873, 13, 51507], "temperature": 0.0, "avg_logprob": -0.24974915532782527, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.03461696207523346}, {"id": 114, "seek": 53694, "start": 559.8000000000001, "end": 566.0, "text": " And if you click the run button, it's going to go through five times, calling one step.", "tokens": [51507, 400, 498, 291, 2052, 264, 1190, 2960, 11, 309, 311, 516, 281, 352, 807, 1732, 1413, 11, 5141, 472, 1823, 13, 51817], "temperature": 0.0, "avg_logprob": -0.24974915532782527, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.03461696207523346}, {"id": 115, "seek": 56600, "start": 566.06, "end": 572.26, "text": " And what one step's going to do is it's going to copy the slope, last slope, to the new", "tokens": [50367, 400, 437, 472, 1823, 311, 516, 281, 360, 307, 309, 311, 516, 281, 5055, 264, 13525, 11, 1036, 13525, 11, 281, 264, 777, 50677], "temperature": 0.0, "avg_logprob": -0.28044878138173923, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0011335627641528845}, {"id": 116, "seek": 56600, "start": 572.26, "end": 578.02, "text": " slope and the last constant intercept to the new constant intercept.", "tokens": [50677, 13525, 293, 264, 1036, 5754, 24700, 281, 264, 777, 5754, 24700, 13, 50965], "temperature": 0.0, "avg_logprob": -0.28044878138173923, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0011335627641528845}, {"id": 117, "seek": 56600, "start": 578.02, "end": 581.76, "text": " And also do the same for the RMSE.", "tokens": [50965, 400, 611, 360, 264, 912, 337, 264, 23790, 5879, 13, 51152], "temperature": 0.0, "avg_logprob": -0.28044878138173923, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0011335627641528845}, {"id": 118, "seek": 56600, "start": 581.76, "end": 584.22, "text": " And it's actually going to paste it down to the bottom for reasons I'll show you in a", "tokens": [51152, 400, 309, 311, 767, 516, 281, 9163, 309, 760, 281, 264, 2767, 337, 4112, 286, 603, 855, 291, 294, 257, 51275], "temperature": 0.0, "avg_logprob": -0.28044878138173923, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0011335627641528845}, {"id": 119, "seek": 56600, "start": 584.22, "end": 585.22, "text": " moment.", "tokens": [51275, 1623, 13, 51325], "temperature": 0.0, "avg_logprob": -0.28044878138173923, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0011335627641528845}, {"id": 120, "seek": 56600, "start": 585.22, "end": 594.14, "text": " So if I now run this, I'll reset and then run.", "tokens": [51325, 407, 498, 286, 586, 1190, 341, 11, 286, 603, 14322, 293, 550, 1190, 13, 51771], "temperature": 0.0, "avg_logprob": -0.28044878138173923, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0011335627641528845}, {"id": 121, "seek": 56600, "start": 594.14, "end": 595.14, "text": " There we go.", "tokens": [51771, 821, 321, 352, 13, 51821], "temperature": 0.0, "avg_logprob": -0.28044878138173923, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0011335627641528845}, {"id": 122, "seek": 59514, "start": 595.28, "end": 599.4, "text": " So it's running at five times and each time it's posted the RMSE.", "tokens": [50371, 407, 309, 311, 2614, 412, 1732, 1413, 293, 1184, 565, 309, 311, 9437, 264, 23790, 5879, 13, 50577], "temperature": 0.0, "avg_logprob": -0.29021168483122617, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.28452742099761963}, {"id": 123, "seek": 59514, "start": 599.4, "end": 602.04, "text": " And here's a chart of it showing it going down.", "tokens": [50577, 400, 510, 311, 257, 6927, 295, 309, 4099, 309, 516, 760, 13, 50709], "temperature": 0.0, "avg_logprob": -0.29021168483122617, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.28452742099761963}, {"id": 124, "seek": 59514, "start": 602.04, "end": 606.1999999999999, "text": " And so you can see the new slope is 2.57, the new intercept is 1.27.", "tokens": [50709, 400, 370, 291, 393, 536, 264, 777, 13525, 307, 568, 13, 19004, 11, 264, 777, 24700, 307, 502, 13, 10076, 13, 50917], "temperature": 0.0, "avg_logprob": -0.29021168483122617, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.28452742099761963}, {"id": 125, "seek": 59514, "start": 606.1999999999999, "end": 608.08, "text": " I could keep running at another five.", "tokens": [50917, 286, 727, 1066, 2614, 412, 1071, 1732, 13, 51011], "temperature": 0.0, "avg_logprob": -0.29021168483122617, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.28452742099761963}, {"id": 126, "seek": 59514, "start": 608.08, "end": 612.68, "text": " So this is just doing copy paste, copy paste, copy paste five times.", "tokens": [51011, 407, 341, 307, 445, 884, 5055, 9163, 11, 5055, 9163, 11, 5055, 9163, 1732, 1413, 13, 51241], "temperature": 0.0, "avg_logprob": -0.29021168483122617, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.28452742099761963}, {"id": 127, "seek": 59514, "start": 612.68, "end": 620.0, "text": " And you can see that the RMSE is very, very, very slowly going down.", "tokens": [51241, 400, 291, 393, 536, 300, 264, 23790, 5879, 307, 588, 11, 588, 11, 588, 5692, 516, 760, 13, 51607], "temperature": 0.0, "avg_logprob": -0.29021168483122617, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.28452742099761963}, {"id": 128, "seek": 59514, "start": 620.0, "end": 623.4399999999999, "text": " And the intercept and slope are very, very, very slowly getting closer to where they want", "tokens": [51607, 400, 264, 24700, 293, 13525, 366, 588, 11, 588, 11, 588, 5692, 1242, 4966, 281, 689, 436, 528, 51779], "temperature": 0.0, "avg_logprob": -0.29021168483122617, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.28452742099761963}, {"id": 129, "seek": 59514, "start": 623.4399999999999, "end": 624.4399999999999, "text": " to be.", "tokens": [51779, 281, 312, 13, 51829], "temperature": 0.0, "avg_logprob": -0.29021168483122617, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.28452742099761963}, {"id": 130, "seek": 62444, "start": 624.74, "end": 627.6600000000001, "text": " The big issue really is that the intercept is meant to be 30.", "tokens": [50379, 440, 955, 2734, 534, 307, 300, 264, 24700, 307, 4140, 281, 312, 2217, 13, 50525], "temperature": 0.0, "avg_logprob": -0.24775070190429688, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0009253833559341729}, {"id": 131, "seek": 62444, "start": 627.6600000000001, "end": 630.5200000000001, "text": " It looks like it's going to take a very, very long time to get there.", "tokens": [50525, 467, 1542, 411, 309, 311, 516, 281, 747, 257, 588, 11, 588, 938, 565, 281, 483, 456, 13, 50668], "temperature": 0.0, "avg_logprob": -0.24775070190429688, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0009253833559341729}, {"id": 132, "seek": 62444, "start": 630.5200000000001, "end": 636.46, "text": " But it will get there eventually if you click run enough times or maybe set the VBA macro", "tokens": [50668, 583, 309, 486, 483, 456, 4728, 498, 291, 2052, 1190, 1547, 1413, 420, 1310, 992, 264, 691, 9295, 18887, 50965], "temperature": 0.0, "avg_logprob": -0.24775070190429688, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0009253833559341729}, {"id": 133, "seek": 62444, "start": 636.46, "end": 639.2600000000001, "text": " to loop more than five steps at a time.", "tokens": [50965, 281, 6367, 544, 813, 1732, 4439, 412, 257, 565, 13, 51105], "temperature": 0.0, "avg_logprob": -0.24775070190429688, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0009253833559341729}, {"id": 134, "seek": 62444, "start": 639.2600000000001, "end": 642.0, "text": " But you can see it's, it's very slowly.", "tokens": [51105, 583, 291, 393, 536, 309, 311, 11, 309, 311, 588, 5692, 13, 51242], "temperature": 0.0, "avg_logprob": -0.24775070190429688, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0009253833559341729}, {"id": 135, "seek": 62444, "start": 642.0, "end": 647.8000000000001, "text": " And importantly though, you can see like it's kind of taking this linear route every time", "tokens": [51242, 400, 8906, 1673, 11, 291, 393, 536, 411, 309, 311, 733, 295, 1940, 341, 8213, 7955, 633, 565, 51532], "temperature": 0.0, "avg_logprob": -0.24775070190429688, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0009253833559341729}, {"id": 136, "seek": 62444, "start": 647.8000000000001, "end": 649.4200000000001, "text": " these are increasing.", "tokens": [51532, 613, 366, 5662, 13, 51613], "temperature": 0.0, "avg_logprob": -0.24775070190429688, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0009253833559341729}, {"id": 137, "seek": 62444, "start": 649.4200000000001, "end": 653.5400000000001, "text": " So why not increase it by more and more and more.", "tokens": [51613, 407, 983, 406, 3488, 309, 538, 544, 293, 544, 293, 544, 13, 51819], "temperature": 0.0, "avg_logprob": -0.24775070190429688, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0009253833559341729}, {"id": 138, "seek": 65354, "start": 653.64, "end": 656.9399999999999, "text": " And so you'll remember from last week that that is what momentum does.", "tokens": [50369, 400, 370, 291, 603, 1604, 490, 1036, 1243, 300, 300, 307, 437, 11244, 775, 13, 50534], "temperature": 0.0, "avg_logprob": -0.24477314254612598, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0014779132325202227}, {"id": 139, "seek": 65354, "start": 656.9399999999999, "end": 661.88, "text": " So on the next sheet we show momentum.", "tokens": [50534, 407, 322, 264, 958, 8193, 321, 855, 11244, 13, 50781], "temperature": 0.0, "avg_logprob": -0.24477314254612598, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0014779132325202227}, {"id": 140, "seek": 65354, "start": 661.88, "end": 664.64, "text": " And so everything's exactly the same as the previous sheet.", "tokens": [50781, 400, 370, 1203, 311, 2293, 264, 912, 382, 264, 3894, 8193, 13, 50919], "temperature": 0.0, "avg_logprob": -0.24477314254612598, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0014779132325202227}, {"id": 141, "seek": 65354, "start": 664.64, "end": 668.0, "text": " But this sheet we didn't bother with the finite differencing.", "tokens": [50919, 583, 341, 8193, 321, 994, 380, 8677, 365, 264, 19362, 743, 13644, 13, 51087], "temperature": 0.0, "avg_logprob": -0.24477314254612598, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0014779132325202227}, {"id": 142, "seek": 65354, "start": 668.0, "end": 671.74, "text": " We just have the analytic derivatives, which are exactly the same as last time.", "tokens": [51087, 492, 445, 362, 264, 40358, 33733, 11, 597, 366, 2293, 264, 912, 382, 1036, 565, 13, 51274], "temperature": 0.0, "avg_logprob": -0.24477314254612598, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0014779132325202227}, {"id": 143, "seek": 65354, "start": 671.74, "end": 673.88, "text": " The data is the same as last time.", "tokens": [51274, 440, 1412, 307, 264, 912, 382, 1036, 565, 13, 51381], "temperature": 0.0, "avg_logprob": -0.24477314254612598, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0014779132325202227}, {"id": 144, "seek": 65354, "start": 673.88, "end": 679.52, "text": " The slope and intercept are the same starting points as last time.", "tokens": [51381, 440, 13525, 293, 24700, 366, 264, 912, 2891, 2793, 382, 1036, 565, 13, 51663], "temperature": 0.0, "avg_logprob": -0.24477314254612598, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0014779132325202227}, {"id": 145, "seek": 67952, "start": 679.8199999999999, "end": 687.38, "text": " And this is the new B and new A that we get.", "tokens": [50379, 400, 341, 307, 264, 777, 363, 293, 777, 316, 300, 321, 483, 13, 50757], "temperature": 0.0, "avg_logprob": -0.23119841881518094, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.033083196729421616}, {"id": 146, "seek": 67952, "start": 687.38, "end": 699.74, "text": " But what we do this time is that we've added a momentum term, which we're calling beta.", "tokens": [50757, 583, 437, 321, 360, 341, 565, 307, 300, 321, 600, 3869, 257, 11244, 1433, 11, 597, 321, 434, 5141, 9861, 13, 51375], "temperature": 0.0, "avg_logprob": -0.23119841881518094, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.033083196729421616}, {"id": 147, "seek": 67952, "start": 699.74, "end": 709.02, "text": " And so the beta is going to these cells here.", "tokens": [51375, 400, 370, 264, 9861, 307, 516, 281, 613, 5438, 510, 13, 51839], "temperature": 0.0, "avg_logprob": -0.23119841881518094, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.033083196729421616}, {"id": 148, "seek": 70902, "start": 709.52, "end": 710.52, "text": " And what are these cells?", "tokens": [50389, 400, 437, 366, 613, 5438, 30, 50439], "temperature": 0.0, "avg_logprob": -0.2696705787412582, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001660384441493079}, {"id": 149, "seek": 70902, "start": 710.52, "end": 719.06, "text": " What these cells are is that they're, maybe it's most interesting to take this one here.", "tokens": [50439, 708, 613, 5438, 366, 307, 300, 436, 434, 11, 1310, 309, 311, 881, 1880, 281, 747, 341, 472, 510, 13, 50866], "temperature": 0.0, "avg_logprob": -0.2696705787412582, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001660384441493079}, {"id": 150, "seek": 70902, "start": 719.06, "end": 732.3, "text": " What it's doing is it's taking the gradient and it's taking the gradient and it's using", "tokens": [50866, 708, 309, 311, 884, 307, 309, 311, 1940, 264, 16235, 293, 309, 311, 1940, 264, 16235, 293, 309, 311, 1228, 51528], "temperature": 0.0, "avg_logprob": -0.2696705787412582, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001660384441493079}, {"id": 151, "seek": 70902, "start": 732.3, "end": 736.12, "text": " that to update the weights.", "tokens": [51528, 300, 281, 5623, 264, 17443, 13, 51719], "temperature": 0.0, "avg_logprob": -0.2696705787412582, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001660384441493079}, {"id": 152, "seek": 73612, "start": 736.22, "end": 740.78, "text": " But it's also taking the previous update.", "tokens": [50369, 583, 309, 311, 611, 1940, 264, 3894, 5623, 13, 50597], "temperature": 0.0, "avg_logprob": -0.2106130703075512, "compression_ratio": 1.505952380952381, "no_speech_prob": 0.007815727032721043}, {"id": 153, "seek": 73612, "start": 740.78, "end": 744.9, "text": " So you can see here the blue one, minus 25.", "tokens": [50597, 407, 291, 393, 536, 510, 264, 3344, 472, 11, 3175, 3552, 13, 50803], "temperature": 0.0, "avg_logprob": -0.2106130703075512, "compression_ratio": 1.505952380952381, "no_speech_prob": 0.007815727032721043}, {"id": 154, "seek": 73612, "start": 744.9, "end": 750.66, "text": " So that is going to get multiplied by 0.9, the momentum.", "tokens": [50803, 407, 300, 307, 516, 281, 483, 17207, 538, 1958, 13, 24, 11, 264, 11244, 13, 51091], "temperature": 0.0, "avg_logprob": -0.2106130703075512, "compression_ratio": 1.505952380952381, "no_speech_prob": 0.007815727032721043}, {"id": 155, "seek": 73612, "start": 750.66, "end": 756.32, "text": " And then the derivative is then multiplied by 0.1.", "tokens": [51091, 400, 550, 264, 13760, 307, 550, 17207, 538, 1958, 13, 16, 13, 51374], "temperature": 0.0, "avg_logprob": -0.2106130703075512, "compression_ratio": 1.505952380952381, "no_speech_prob": 0.007815727032721043}, {"id": 156, "seek": 73612, "start": 756.32, "end": 761.36, "text": " So this is momentum, which is getting a little bit of each.", "tokens": [51374, 407, 341, 307, 11244, 11, 597, 307, 1242, 257, 707, 857, 295, 1184, 13, 51626], "temperature": 0.0, "avg_logprob": -0.2106130703075512, "compression_ratio": 1.505952380952381, "no_speech_prob": 0.007815727032721043}, {"id": 157, "seek": 76136, "start": 761.58, "end": 768.54, "text": " And so then what we do is we then use that instead of the derivative to multiply by our", "tokens": [50375, 400, 370, 550, 437, 321, 360, 307, 321, 550, 764, 300, 2602, 295, 264, 13760, 281, 12972, 538, 527, 50723], "temperature": 0.0, "avg_logprob": -0.25173124313354495, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.009708215482532978}, {"id": 158, "seek": 76136, "start": 768.54, "end": 771.38, "text": " learning rate.", "tokens": [50723, 2539, 3314, 13, 50865], "temperature": 0.0, "avg_logprob": -0.25173124313354495, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.009708215482532978}, {"id": 159, "seek": 76136, "start": 771.38, "end": 778.5, "text": " So we keep doing that again and again and again as per usual.", "tokens": [50865, 407, 321, 1066, 884, 300, 797, 293, 797, 293, 797, 382, 680, 7713, 13, 51221], "temperature": 0.0, "avg_logprob": -0.25173124313354495, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.009708215482532978}, {"id": 160, "seek": 76136, "start": 778.5, "end": 783.46, "text": " And so we've got one column which is calculating the next, which is calculating the momentum,", "tokens": [51221, 400, 370, 321, 600, 658, 472, 7738, 597, 307, 28258, 264, 958, 11, 597, 307, 28258, 264, 11244, 11, 51469], "temperature": 0.0, "avg_logprob": -0.25173124313354495, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.009708215482532978}, {"id": 161, "seek": 76136, "start": 783.46, "end": 789.4200000000001, "text": " you know, lerped version of the gradient for both B and for A. And so you can see that", "tokens": [51469, 291, 458, 11, 32068, 3452, 3037, 295, 264, 16235, 337, 1293, 363, 293, 337, 316, 13, 400, 370, 291, 393, 536, 300, 51767], "temperature": 0.0, "avg_logprob": -0.25173124313354495, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.009708215482532978}, {"id": 162, "seek": 76136, "start": 789.4200000000001, "end": 790.74, "text": " for this one it's the same thing.", "tokens": [51767, 337, 341, 472, 309, 311, 264, 912, 551, 13, 51833], "temperature": 0.0, "avg_logprob": -0.25173124313354495, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.009708215482532978}, {"id": 163, "seek": 79074, "start": 791.12, "end": 798.3, "text": " You look at what was the previous move and that's going to be 0.9 of what you're going", "tokens": [50383, 509, 574, 412, 437, 390, 264, 3894, 1286, 293, 300, 311, 516, 281, 312, 1958, 13, 24, 295, 437, 291, 434, 516, 50742], "temperature": 0.0, "avg_logprob": -0.19687662842453166, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0017821796936914325}, {"id": 164, "seek": 79074, "start": 798.3, "end": 801.92, "text": " to use for your momentum version gradient.", "tokens": [50742, 281, 764, 337, 428, 11244, 3037, 16235, 13, 50923], "temperature": 0.0, "avg_logprob": -0.19687662842453166, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0017821796936914325}, {"id": 165, "seek": 79074, "start": 801.92, "end": 805.14, "text": " And 0.1 is for this version, the momentum gradient.", "tokens": [50923, 400, 1958, 13, 16, 307, 337, 341, 3037, 11, 264, 11244, 16235, 13, 51084], "temperature": 0.0, "avg_logprob": -0.19687662842453166, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0017821796936914325}, {"id": 166, "seek": 79074, "start": 805.14, "end": 811.8, "text": " And so then that's again what we're going to use to multiply by the learning rate.", "tokens": [51084, 400, 370, 550, 300, 311, 797, 437, 321, 434, 516, 281, 764, 281, 12972, 538, 264, 2539, 3314, 13, 51417], "temperature": 0.0, "avg_logprob": -0.19687662842453166, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0017821796936914325}, {"id": 167, "seek": 79074, "start": 811.8, "end": 816.42, "text": " And so you can see what happens is when you keep moving in the same direction, which here", "tokens": [51417, 400, 370, 291, 393, 536, 437, 2314, 307, 562, 291, 1066, 2684, 294, 264, 912, 3513, 11, 597, 510, 51648], "temperature": 0.0, "avg_logprob": -0.19687662842453166, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0017821796936914325}, {"id": 168, "seek": 81642, "start": 816.42, "end": 821.62, "text": " is we're saying the derivative is negative again and again and again, so it gets higher", "tokens": [50364, 307, 321, 434, 1566, 264, 13760, 307, 3671, 797, 293, 797, 293, 797, 11, 370, 309, 2170, 2946, 50624], "temperature": 0.0, "avg_logprob": -0.2730871649349437, "compression_ratio": 1.8647342995169083, "no_speech_prob": 0.15608814358711243}, {"id": 169, "seek": 81642, "start": 821.62, "end": 824.26, "text": " and higher and higher.", "tokens": [50624, 293, 2946, 293, 2946, 13, 50756], "temperature": 0.0, "avg_logprob": -0.2730871649349437, "compression_ratio": 1.8647342995169083, "no_speech_prob": 0.15608814358711243}, {"id": 170, "seek": 81642, "start": 824.26, "end": 826.8399999999999, "text": " And ditto here.", "tokens": [50756, 400, 274, 34924, 510, 13, 50885], "temperature": 0.0, "avg_logprob": -0.2730871649349437, "compression_ratio": 1.8647342995169083, "no_speech_prob": 0.15608814358711243}, {"id": 171, "seek": 81642, "start": 826.8399999999999, "end": 830.9799999999999, "text": " And so particularly with this big jump we get, we keep getting big jumps because still", "tokens": [50885, 400, 370, 4098, 365, 341, 955, 3012, 321, 483, 11, 321, 1066, 1242, 955, 16704, 570, 920, 51092], "temperature": 0.0, "avg_logprob": -0.2730871649349437, "compression_ratio": 1.8647342995169083, "no_speech_prob": 0.15608814358711243}, {"id": 172, "seek": 81642, "start": 830.9799999999999, "end": 835.02, "text": " we want to, there's still negative gradient, negative gradient, negative gradient.", "tokens": [51092, 321, 528, 281, 11, 456, 311, 920, 3671, 16235, 11, 3671, 16235, 11, 3671, 16235, 13, 51294], "temperature": 0.0, "avg_logprob": -0.2730871649349437, "compression_ratio": 1.8647342995169083, "no_speech_prob": 0.15608814358711243}, {"id": 173, "seek": 81642, "start": 835.02, "end": 844.3399999999999, "text": " So if we, at, so at the end of this, our new, our B and our A have jumped ahead and so we", "tokens": [51294, 407, 498, 321, 11, 412, 11, 370, 412, 264, 917, 295, 341, 11, 527, 777, 11, 527, 363, 293, 527, 316, 362, 13864, 2286, 293, 370, 321, 51760], "temperature": 0.0, "avg_logprob": -0.2730871649349437, "compression_ratio": 1.8647342995169083, "no_speech_prob": 0.15608814358711243}, {"id": 174, "seek": 84434, "start": 844.34, "end": 848.46, "text": " can click run.", "tokens": [50364, 393, 2052, 1190, 13, 50570], "temperature": 0.0, "avg_logprob": -0.27673363318810096, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.00034062640042975545}, {"id": 175, "seek": 84434, "start": 848.46, "end": 854.08, "text": " And we can keep clicking it and you can see that it's moving, you know, not super fast", "tokens": [50570, 400, 321, 393, 1066, 9697, 309, 293, 291, 393, 536, 300, 309, 311, 2684, 11, 291, 458, 11, 406, 1687, 2370, 50851], "temperature": 0.0, "avg_logprob": -0.27673363318810096, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.00034062640042975545}, {"id": 176, "seek": 84434, "start": 854.08, "end": 859.48, "text": " but certainly faster than it was before.", "tokens": [50851, 457, 3297, 4663, 813, 309, 390, 949, 13, 51121], "temperature": 0.0, "avg_logprob": -0.27673363318810096, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.00034062640042975545}, {"id": 177, "seek": 84434, "start": 859.48, "end": 866.74, "text": " So if you haven't used VBA, Visual Basic for Applications, before you can hit alt, alt", "tokens": [51121, 407, 498, 291, 2378, 380, 1143, 691, 9295, 11, 23187, 31598, 337, 26519, 763, 11, 949, 291, 393, 2045, 4955, 11, 4955, 51484], "temperature": 0.0, "avg_logprob": -0.27673363318810096, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.00034062640042975545}, {"id": 178, "seek": 86674, "start": 866.74, "end": 871.34, "text": " F11 or option F11 to open it.", "tokens": [50364, 479, 5348, 420, 3614, 479, 5348, 281, 1269, 309, 13, 50594], "temperature": 0.0, "avg_logprob": -0.2527741763902747, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.26279938220977783}, {"id": 179, "seek": 86674, "start": 871.34, "end": 877.26, "text": " And you may need to go into your preferences and turn on the developer tools so that you", "tokens": [50594, 400, 291, 815, 643, 281, 352, 666, 428, 21910, 293, 1261, 322, 264, 10754, 3873, 370, 300, 291, 50890], "temperature": 0.0, "avg_logprob": -0.2527741763902747, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.26279938220977783}, {"id": 180, "seek": 86674, "start": 877.26, "end": 880.7, "text": " can see it.", "tokens": [50890, 393, 536, 309, 13, 51062], "temperature": 0.0, "avg_logprob": -0.2527741763902747, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.26279938220977783}, {"id": 181, "seek": 86674, "start": 880.7, "end": 886.9, "text": " You can also right-click and choose assign macro on a button and you can see what macro", "tokens": [51062, 509, 393, 611, 558, 12, 18548, 293, 2826, 6269, 18887, 322, 257, 2960, 293, 291, 393, 536, 437, 18887, 51372], "temperature": 0.0, "avg_logprob": -0.2527741763902747, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.26279938220977783}, {"id": 182, "seek": 86674, "start": 886.9, "end": 890.82, "text": " has been assigned.", "tokens": [51372, 575, 668, 13279, 13, 51568], "temperature": 0.0, "avg_logprob": -0.2527741763902747, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.26279938220977783}, {"id": 183, "seek": 86674, "start": 890.82, "end": 895.54, "text": " So if I hit alt F11 and I can just double, or I can just double click on the sheet name", "tokens": [51568, 407, 498, 286, 2045, 4955, 479, 5348, 293, 286, 393, 445, 3834, 11, 420, 286, 393, 445, 3834, 2052, 322, 264, 8193, 1315, 51804], "temperature": 0.0, "avg_logprob": -0.2527741763902747, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.26279938220977783}, {"id": 184, "seek": 89554, "start": 895.66, "end": 897.18, "text": " and it'll open it up.", "tokens": [50370, 293, 309, 603, 1269, 309, 493, 13, 50446], "temperature": 0.0, "avg_logprob": -0.261210542566636, "compression_ratio": 1.5803108808290156, "no_speech_prob": 0.0070117455907166}, {"id": 185, "seek": 89554, "start": 897.18, "end": 902.86, "text": " And you can see that this is exactly the same as the previous one.", "tokens": [50446, 400, 291, 393, 536, 300, 341, 307, 2293, 264, 912, 382, 264, 3894, 472, 13, 50730], "temperature": 0.0, "avg_logprob": -0.261210542566636, "compression_ratio": 1.5803108808290156, "no_speech_prob": 0.0070117455907166}, {"id": 186, "seek": 89554, "start": 902.86, "end": 906.66, "text": " There's no difference here.", "tokens": [50730, 821, 311, 572, 2649, 510, 13, 50920], "temperature": 0.0, "avg_logprob": -0.261210542566636, "compression_ratio": 1.5803108808290156, "no_speech_prob": 0.0070117455907166}, {"id": 187, "seek": 89554, "start": 906.66, "end": 913.14, "text": " One difference is that to keep track of momentum at the very, very end, so I've got my momentum", "tokens": [50920, 1485, 2649, 307, 300, 281, 1066, 2837, 295, 11244, 412, 264, 588, 11, 588, 917, 11, 370, 286, 600, 658, 452, 11244, 51244], "temperature": 0.0, "avg_logprob": -0.261210542566636, "compression_ratio": 1.5803108808290156, "no_speech_prob": 0.0070117455907166}, {"id": 188, "seek": 89554, "start": 913.14, "end": 921.38, "text": " values going all the way down, the very last momentum I copy back up to the top, each epoch,", "tokens": [51244, 4190, 516, 439, 264, 636, 760, 11, 264, 588, 1036, 11244, 286, 5055, 646, 493, 281, 264, 1192, 11, 1184, 30992, 339, 11, 51656], "temperature": 0.0, "avg_logprob": -0.261210542566636, "compression_ratio": 1.5803108808290156, "no_speech_prob": 0.0070117455907166}, {"id": 189, "seek": 92138, "start": 921.38, "end": 925.38, "text": " so that we don't lose track of our kind of optimizer state, if you like.", "tokens": [50364, 370, 300, 321, 500, 380, 3624, 2837, 295, 527, 733, 295, 5028, 6545, 1785, 11, 498, 291, 411, 13, 50564], "temperature": 0.0, "avg_logprob": -0.22976553642143638, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.018832912668585777}, {"id": 190, "seek": 92138, "start": 925.38, "end": 927.78, "text": " Okay, so that's what momentum looks like.", "tokens": [50564, 1033, 11, 370, 300, 311, 437, 11244, 1542, 411, 13, 50684], "temperature": 0.0, "avg_logprob": -0.22976553642143638, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.018832912668585777}, {"id": 191, "seek": 92138, "start": 927.78, "end": 931.52, "text": " So yeah, if you're kind of a more of a visual person like me, you like to see everything", "tokens": [50684, 407, 1338, 11, 498, 291, 434, 733, 295, 257, 544, 295, 257, 5056, 954, 411, 385, 11, 291, 411, 281, 536, 1203, 50871], "temperature": 0.0, "avg_logprob": -0.22976553642143638, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.018832912668585777}, {"id": 192, "seek": 92138, "start": 931.52, "end": 934.9, "text": " laid out in front of you and like to be able to experiment, which I think is a good idea,", "tokens": [50871, 9897, 484, 294, 1868, 295, 291, 293, 411, 281, 312, 1075, 281, 5120, 11, 597, 286, 519, 307, 257, 665, 1558, 11, 51040], "temperature": 0.0, "avg_logprob": -0.22976553642143638, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.018832912668585777}, {"id": 193, "seek": 92138, "start": 934.9, "end": 937.78, "text": " this can be really helpful.", "tokens": [51040, 341, 393, 312, 534, 4961, 13, 51184], "temperature": 0.0, "avg_logprob": -0.22976553642143638, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.018832912668585777}, {"id": 194, "seek": 92138, "start": 937.78, "end": 948.9, "text": " So rmsprop, we've seen, and it's very similar to momentum, but in this case instead of keeping", "tokens": [51184, 407, 367, 2592, 79, 1513, 11, 321, 600, 1612, 11, 293, 309, 311, 588, 2531, 281, 11244, 11, 457, 294, 341, 1389, 2602, 295, 5145, 51740], "temperature": 0.0, "avg_logprob": -0.22976553642143638, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.018832912668585777}, {"id": 195, "seek": 94890, "start": 948.9, "end": 953.98, "text": " track of kind of a lerped moving average, an exponential moving average of gradients,", "tokens": [50364, 2837, 295, 733, 295, 257, 32068, 3452, 2684, 4274, 11, 364, 21510, 2684, 4274, 295, 2771, 2448, 11, 50618], "temperature": 0.0, "avg_logprob": -0.20212732950846354, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0008693652343936265}, {"id": 196, "seek": 94890, "start": 953.98, "end": 959.1, "text": " we're keeping track of a moving average of gradient squared.", "tokens": [50618, 321, 434, 5145, 2837, 295, 257, 2684, 4274, 295, 16235, 8889, 13, 50874], "temperature": 0.0, "avg_logprob": -0.20212732950846354, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0008693652343936265}, {"id": 197, "seek": 94890, "start": 959.1, "end": 967.02, "text": " And then rather than simply adding that, you know, using that as the gradient, what instead", "tokens": [50874, 400, 550, 2831, 813, 2935, 5127, 300, 11, 291, 458, 11, 1228, 300, 382, 264, 16235, 11, 437, 2602, 51270], "temperature": 0.0, "avg_logprob": -0.20212732950846354, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0008693652343936265}, {"id": 198, "seek": 94890, "start": 967.02, "end": 974.66, "text": " we're doing is we are dividing our gradient by the square root of that.", "tokens": [51270, 321, 434, 884, 307, 321, 366, 26764, 527, 16235, 538, 264, 3732, 5593, 295, 300, 13, 51652], "temperature": 0.0, "avg_logprob": -0.20212732950846354, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0008693652343936265}, {"id": 199, "seek": 97466, "start": 974.66, "end": 983.14, "text": " And so remember the reason we were doing that is to say, you know, if there's very", "tokens": [50364, 400, 370, 1604, 264, 1778, 321, 645, 884, 300, 307, 281, 584, 11, 291, 458, 11, 498, 456, 311, 588, 50788], "temperature": 0.0, "avg_logprob": -0.24812990998568601, "compression_ratio": 1.4885057471264367, "no_speech_prob": 0.0036499183624982834}, {"id": 200, "seek": 97466, "start": 983.14, "end": 987.9, "text": " little variation, very little going on in your gradients, then you probably want to", "tokens": [50788, 707, 12990, 11, 588, 707, 516, 322, 294, 428, 2771, 2448, 11, 550, 291, 1391, 528, 281, 51026], "temperature": 0.0, "avg_logprob": -0.24812990998568601, "compression_ratio": 1.4885057471264367, "no_speech_prob": 0.0036499183624982834}, {"id": 201, "seek": 97466, "start": 987.9, "end": 992.92, "text": " jump further.", "tokens": [51026, 3012, 3052, 13, 51277], "temperature": 0.0, "avg_logprob": -0.24812990998568601, "compression_ratio": 1.4885057471264367, "no_speech_prob": 0.0036499183624982834}, {"id": 202, "seek": 97466, "start": 992.92, "end": 995.06, "text": " So that's rmsprop.", "tokens": [51277, 407, 300, 311, 367, 2592, 79, 1513, 13, 51384], "temperature": 0.0, "avg_logprob": -0.24812990998568601, "compression_ratio": 1.4885057471264367, "no_speech_prob": 0.0036499183624982834}, {"id": 203, "seek": 97466, "start": 995.06, "end": 1001.74, "text": " And then finally atom, remember, was a combination of both.", "tokens": [51384, 400, 550, 2721, 12018, 11, 1604, 11, 390, 257, 6562, 295, 1293, 13, 51718], "temperature": 0.0, "avg_logprob": -0.24812990998568601, "compression_ratio": 1.4885057471264367, "no_speech_prob": 0.0036499183624982834}, {"id": 204, "seek": 100174, "start": 1001.82, "end": 1008.94, "text": " So in atom, we've got both the lerped version of the gradient, and we've got the lerped", "tokens": [50368, 407, 294, 12018, 11, 321, 600, 658, 1293, 264, 32068, 3452, 3037, 295, 264, 16235, 11, 293, 321, 600, 658, 264, 32068, 3452, 50724], "temperature": 0.0, "avg_logprob": -0.2757911971121123, "compression_ratio": 1.7588652482269505, "no_speech_prob": 0.006289798766374588}, {"id": 205, "seek": 100174, "start": 1008.94, "end": 1012.62, "text": " version of the gradient squared.", "tokens": [50724, 3037, 295, 264, 16235, 8889, 13, 50908], "temperature": 0.0, "avg_logprob": -0.2757911971121123, "compression_ratio": 1.7588652482269505, "no_speech_prob": 0.006289798766374588}, {"id": 206, "seek": 100174, "start": 1012.62, "end": 1019.14, "text": " And then we do both when we update.", "tokens": [50908, 400, 550, 321, 360, 1293, 562, 321, 5623, 13, 51234], "temperature": 0.0, "avg_logprob": -0.2757911971121123, "compression_ratio": 1.7588652482269505, "no_speech_prob": 0.006289798766374588}, {"id": 207, "seek": 100174, "start": 1019.14, "end": 1024.5, "text": " We're both dividing the gradient by the square root of the lerped, the moving exponentially", "tokens": [51234, 492, 434, 1293, 26764, 264, 16235, 538, 264, 3732, 5593, 295, 264, 32068, 3452, 11, 264, 2684, 37330, 51502], "temperature": 0.0, "avg_logprob": -0.2757911971121123, "compression_ratio": 1.7588652482269505, "no_speech_prob": 0.006289798766374588}, {"id": 208, "seek": 102450, "start": 1024.5, "end": 1033.34, "text": " weighted average, moving averages, and we're also using the momentumized version.", "tokens": [50364, 32807, 4274, 11, 2684, 42257, 11, 293, 321, 434, 611, 1228, 264, 11244, 1602, 3037, 13, 50806], "temperature": 0.0, "avg_logprob": -0.27390794496278503, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.3207726776599884}, {"id": 209, "seek": 102450, "start": 1033.34, "end": 1038.14, "text": " And so again, we just go through that each time.", "tokens": [50806, 400, 370, 797, 11, 321, 445, 352, 807, 300, 1184, 565, 13, 51046], "temperature": 0.0, "avg_logprob": -0.27390794496278503, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.3207726776599884}, {"id": 210, "seek": 102450, "start": 1038.14, "end": 1046.74, "text": " And so if I reset, run, and so, oh wow, look at that.", "tokens": [51046, 400, 370, 498, 286, 14322, 11, 1190, 11, 293, 370, 11, 1954, 6076, 11, 574, 412, 300, 13, 51476], "temperature": 0.0, "avg_logprob": -0.27390794496278503, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.3207726776599884}, {"id": 211, "seek": 102450, "start": 1046.74, "end": 1053.6, "text": " It jumped up there very quickly, because remember we wanted to get to 2 and 30.", "tokens": [51476, 467, 13864, 493, 456, 588, 2661, 11, 570, 1604, 321, 1415, 281, 483, 281, 568, 293, 2217, 13, 51819], "temperature": 0.0, "avg_logprob": -0.27390794496278503, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.3207726776599884}, {"id": 212, "seek": 105360, "start": 1053.6999999999998, "end": 1058.4399999999998, "text": " So just two sets, so that's 5, that's 10 epochs.", "tokens": [50369, 407, 445, 732, 6352, 11, 370, 300, 311, 1025, 11, 300, 311, 1266, 30992, 28346, 13, 50606], "temperature": 0.0, "avg_logprob": -0.23391308404703057, "compression_ratio": 1.774891774891775, "no_speech_prob": 0.008847307413816452}, {"id": 213, "seek": 105360, "start": 1058.4399999999998, "end": 1063.56, "text": " Now if I keep running it, it's kind of now not getting closer.", "tokens": [50606, 823, 498, 286, 1066, 2614, 309, 11, 309, 311, 733, 295, 586, 406, 1242, 4966, 13, 50862], "temperature": 0.0, "avg_logprob": -0.23391308404703057, "compression_ratio": 1.774891774891775, "no_speech_prob": 0.008847307413816452}, {"id": 214, "seek": 105360, "start": 1063.56, "end": 1067.6799999999998, "text": " It's kind of jumping up and down between pretty much the same values.", "tokens": [50862, 467, 311, 733, 295, 11233, 493, 293, 760, 1296, 1238, 709, 264, 912, 4190, 13, 51068], "temperature": 0.0, "avg_logprob": -0.23391308404703057, "compression_ratio": 1.774891774891775, "no_speech_prob": 0.008847307413816452}, {"id": 215, "seek": 105360, "start": 1067.6799999999998, "end": 1075.08, "text": " So probably what we need to do is decrease the learning rate at that point.", "tokens": [51068, 407, 1391, 437, 321, 643, 281, 360, 307, 11514, 264, 2539, 3314, 412, 300, 935, 13, 51438], "temperature": 0.0, "avg_logprob": -0.23391308404703057, "compression_ratio": 1.774891774891775, "no_speech_prob": 0.008847307413816452}, {"id": 216, "seek": 105360, "start": 1075.08, "end": 1077.1799999999998, "text": " And yeah, that's pretty good.", "tokens": [51438, 400, 1338, 11, 300, 311, 1238, 665, 13, 51543], "temperature": 0.0, "avg_logprob": -0.23391308404703057, "compression_ratio": 1.774891774891775, "no_speech_prob": 0.008847307413816452}, {"id": 217, "seek": 105360, "start": 1077.1799999999998, "end": 1079.6799999999998, "text": " And now it's jumping up and down between the same two values again.", "tokens": [51543, 400, 586, 309, 311, 11233, 493, 293, 760, 1296, 264, 912, 732, 4190, 797, 13, 51668], "temperature": 0.0, "avg_logprob": -0.23391308404703057, "compression_ratio": 1.774891774891775, "no_speech_prob": 0.008847307413816452}, {"id": 218, "seek": 105360, "start": 1079.6799999999998, "end": 1081.9199999999998, "text": " So maybe decrease the learning rate a little bit more.", "tokens": [51668, 407, 1310, 11514, 264, 2539, 3314, 257, 707, 857, 544, 13, 51780], "temperature": 0.0, "avg_logprob": -0.23391308404703057, "compression_ratio": 1.774891774891775, "no_speech_prob": 0.008847307413816452}, {"id": 219, "seek": 108192, "start": 1081.92, "end": 1085.98, "text": " I kind of like playing around like this, because it gives me a really intuitive feeling", "tokens": [50364, 286, 733, 295, 411, 2433, 926, 411, 341, 11, 570, 309, 2709, 385, 257, 534, 21769, 2633, 50567], "temperature": 0.0, "avg_logprob": -0.27311406816755024, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00267297332175076}, {"id": 220, "seek": 108192, "start": 1085.98, "end": 1091.6000000000001, "text": " for what training looks like.", "tokens": [50567, 337, 437, 3097, 1542, 411, 13, 50848], "temperature": 0.0, "avg_logprob": -0.27311406816755024, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00267297332175076}, {"id": 221, "seek": 108192, "start": 1091.6000000000001, "end": 1096.88, "text": " So I've got a question from our YouTube chat, which is how is J33 being initialized?", "tokens": [50848, 407, 286, 600, 658, 257, 1168, 490, 527, 3088, 5081, 11, 597, 307, 577, 307, 508, 10191, 885, 5883, 1602, 30, 51112], "temperature": 0.0, "avg_logprob": -0.27311406816755024, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00267297332175076}, {"id": 222, "seek": 108192, "start": 1096.88, "end": 1102.24, "text": " So it's, it's, this is just, what happens is we take the very last cell, here, well", "tokens": [51112, 407, 309, 311, 11, 309, 311, 11, 341, 307, 445, 11, 437, 2314, 307, 321, 747, 264, 588, 1036, 2815, 11, 510, 11, 731, 51380], "temperature": 0.0, "avg_logprob": -0.27311406816755024, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00267297332175076}, {"id": 223, "seek": 108192, "start": 1102.24, "end": 1108.0, "text": " there's actually all these last four cells, and we copy them to here as values.", "tokens": [51380, 456, 311, 767, 439, 613, 1036, 1451, 5438, 11, 293, 321, 5055, 552, 281, 510, 382, 4190, 13, 51668], "temperature": 0.0, "avg_logprob": -0.27311406816755024, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00267297332175076}, {"id": 224, "seek": 110800, "start": 1108.0, "end": 1113.84, "text": " So this is what those looked like in the last epoch.", "tokens": [50364, 407, 341, 307, 437, 729, 2956, 411, 294, 264, 1036, 30992, 339, 13, 50656], "temperature": 0.0, "avg_logprob": -0.25963961613642705, "compression_ratio": 1.5, "no_speech_prob": 0.0390474908053875}, {"id": 225, "seek": 110800, "start": 1113.84, "end": 1125.04, "text": " So if I basically, we're going, we go copy, and then paste as values, and then they, this", "tokens": [50656, 407, 498, 286, 1936, 11, 321, 434, 516, 11, 321, 352, 5055, 11, 293, 550, 9163, 382, 4190, 11, 293, 550, 436, 11, 341, 51216], "temperature": 0.0, "avg_logprob": -0.25963961613642705, "compression_ratio": 1.5, "no_speech_prob": 0.0390474908053875}, {"id": 226, "seek": 110800, "start": 1125.04, "end": 1132.92, "text": " here just refers back to them as you see.", "tokens": [51216, 510, 445, 14942, 646, 281, 552, 382, 291, 536, 13, 51610], "temperature": 0.0, "avg_logprob": -0.25963961613642705, "compression_ratio": 1.5, "no_speech_prob": 0.0390474908053875}, {"id": 227, "seek": 110800, "start": 1132.92, "end": 1135.9, "text": " And it's interesting that they're kind of, you can see how they're exact opposites of", "tokens": [51610, 400, 309, 311, 1880, 300, 436, 434, 733, 295, 11, 291, 393, 536, 577, 436, 434, 1900, 4665, 3324, 295, 51759], "temperature": 0.0, "avg_logprob": -0.25963961613642705, "compression_ratio": 1.5, "no_speech_prob": 0.0390474908053875}, {"id": 228, "seek": 113590, "start": 1135.9, "end": 1141.02, "text": " each other, which is really, you can really see how they're, it's, it's just fluctuating", "tokens": [50364, 1184, 661, 11, 597, 307, 534, 11, 291, 393, 534, 536, 577, 436, 434, 11, 309, 311, 11, 309, 311, 445, 23448, 32438, 50620], "temperature": 0.0, "avg_logprob": -0.3169851864085478, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.2508927881717682}, {"id": 229, "seek": 113590, "start": 1141.02, "end": 1147.42, "text": " around the actual optimum at this point.", "tokens": [50620, 926, 264, 3539, 39326, 412, 341, 935, 13, 50940], "temperature": 0.0, "avg_logprob": -0.3169851864085478, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.2508927881717682}, {"id": 230, "seek": 113590, "start": 1147.42, "end": 1156.7800000000002, "text": " Okay thank you to Sam Watkins, we've now got a nicer sized editor, that's great.", "tokens": [50940, 1033, 1309, 291, 281, 4832, 12593, 10277, 11, 321, 600, 586, 658, 257, 22842, 20004, 9839, 11, 300, 311, 869, 13, 51408], "temperature": 0.0, "avg_logprob": -0.3169851864085478, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.2508927881717682}, {"id": 231, "seek": 113590, "start": 1156.7800000000002, "end": 1162.0600000000002, "text": " Where are we, Adam?", "tokens": [51408, 2305, 366, 321, 11, 7938, 30, 51672], "temperature": 0.0, "avg_logprob": -0.3169851864085478, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.2508927881717682}, {"id": 232, "seek": 116206, "start": 1162.06, "end": 1167.86, "text": " Okay so with, so with Adam, basically it all looks pretty much the same, except now", "tokens": [50364, 1033, 370, 365, 11, 370, 365, 7938, 11, 1936, 309, 439, 1542, 1238, 709, 264, 912, 11, 3993, 586, 50654], "temperature": 0.0, "avg_logprob": -0.24852705001831055, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.05582057684659958}, {"id": 233, "seek": 116206, "start": 1167.86, "end": 1179.94, "text": " we have to copy and paste our, both our momentums and our squared gradients, and of course the", "tokens": [50654, 321, 362, 281, 5055, 293, 9163, 527, 11, 1293, 527, 1623, 8099, 293, 527, 8889, 2771, 2448, 11, 293, 295, 1164, 264, 51258], "temperature": 0.0, "avg_logprob": -0.24852705001831055, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.05582057684659958}, {"id": 234, "seek": 116206, "start": 1179.94, "end": 1183.22, "text": " slopes and intercepts at the end of each step.", "tokens": [51258, 37725, 293, 24700, 82, 412, 264, 917, 295, 1184, 1823, 13, 51422], "temperature": 0.0, "avg_logprob": -0.24852705001831055, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.05582057684659958}, {"id": 235, "seek": 116206, "start": 1183.22, "end": 1186.22, "text": " But other than that it's just doing the same thing, and when we reset it, it just sets", "tokens": [51422, 583, 661, 813, 300, 309, 311, 445, 884, 264, 912, 551, 11, 293, 562, 321, 14322, 309, 11, 309, 445, 6352, 51572], "temperature": 0.0, "avg_logprob": -0.24852705001831055, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.05582057684659958}, {"id": 236, "seek": 116206, "start": 1186.22, "end": 1189.8, "text": " everything back to their default values.", "tokens": [51572, 1203, 646, 281, 641, 7576, 4190, 13, 51751], "temperature": 0.0, "avg_logprob": -0.24852705001831055, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.05582057684659958}, {"id": 237, "seek": 118980, "start": 1189.8, "end": 1194.1599999999999, "text": " Now one thing that occurred to me, you know, when I first wrote this spreadsheet a few", "tokens": [50364, 823, 472, 551, 300, 11068, 281, 385, 11, 291, 458, 11, 562, 286, 700, 4114, 341, 27733, 257, 1326, 50582], "temperature": 0.0, "avg_logprob": -0.20929315774747642, "compression_ratio": 1.642570281124498, "no_speech_prob": 0.02096397802233696}, {"id": 238, "seek": 118980, "start": 1194.1599999999999, "end": 1202.48, "text": " years ago, was that manually changing the learning rate seems pretty annoying.", "tokens": [50582, 924, 2057, 11, 390, 300, 16945, 4473, 264, 2539, 3314, 2544, 1238, 11304, 13, 50998], "temperature": 0.0, "avg_logprob": -0.20929315774747642, "compression_ratio": 1.642570281124498, "no_speech_prob": 0.02096397802233696}, {"id": 239, "seek": 118980, "start": 1202.48, "end": 1206.0, "text": " Now of course we can use a scheduler, but a scheduler is something we set up ahead of", "tokens": [50998, 823, 295, 1164, 321, 393, 764, 257, 12000, 260, 11, 457, 257, 12000, 260, 307, 746, 321, 992, 493, 2286, 295, 51174], "temperature": 0.0, "avg_logprob": -0.20929315774747642, "compression_ratio": 1.642570281124498, "no_speech_prob": 0.02096397802233696}, {"id": 240, "seek": 118980, "start": 1206.0, "end": 1211.1, "text": " time, and I did wonder if it's possible to create an automatic scheduler.", "tokens": [51174, 565, 11, 293, 286, 630, 2441, 498, 309, 311, 1944, 281, 1884, 364, 12509, 12000, 260, 13, 51429], "temperature": 0.0, "avg_logprob": -0.20929315774747642, "compression_ratio": 1.642570281124498, "no_speech_prob": 0.02096397802233696}, {"id": 241, "seek": 118980, "start": 1211.1, "end": 1215.8799999999999, "text": " And so I created this Adam annealing tab, which honestly I've never really got back", "tokens": [51429, 400, 370, 286, 2942, 341, 7938, 22256, 4270, 4421, 11, 597, 6095, 286, 600, 1128, 534, 658, 646, 51668], "temperature": 0.0, "avg_logprob": -0.20929315774747642, "compression_ratio": 1.642570281124498, "no_speech_prob": 0.02096397802233696}, {"id": 242, "seek": 121588, "start": 1215.88, "end": 1220.24, "text": " to experimenting with, so if anybody's interested they should check this out.", "tokens": [50364, 281, 29070, 365, 11, 370, 498, 4472, 311, 3102, 436, 820, 1520, 341, 484, 13, 50582], "temperature": 0.0, "avg_logprob": -0.25549570719401044, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.11278466880321503}, {"id": 243, "seek": 121588, "start": 1220.24, "end": 1230.44, "text": " What I did here was I used exactly the same spreadsheet as the Adam spreadsheet, but I", "tokens": [50582, 708, 286, 630, 510, 390, 286, 1143, 2293, 264, 912, 27733, 382, 264, 7938, 27733, 11, 457, 286, 51092], "temperature": 0.0, "avg_logprob": -0.25549570719401044, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.11278466880321503}, {"id": 244, "seek": 121588, "start": 1230.44, "end": 1237.24, "text": " added an extra, after I do this step, I added an extra thing, which is I automatically decreased", "tokens": [51092, 3869, 364, 2857, 11, 934, 286, 360, 341, 1823, 11, 286, 3869, 364, 2857, 551, 11, 597, 307, 286, 6772, 24436, 51432], "temperature": 0.0, "avg_logprob": -0.25549570719401044, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.11278466880321503}, {"id": 245, "seek": 121588, "start": 1237.24, "end": 1240.3600000000001, "text": " the learning rate in a certain situation.", "tokens": [51432, 264, 2539, 3314, 294, 257, 1629, 2590, 13, 51588], "temperature": 0.0, "avg_logprob": -0.25549570719401044, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.11278466880321503}, {"id": 246, "seek": 124036, "start": 1240.36, "end": 1245.8, "text": " And the situation in which I, in which I decreased it, was I kept track of the average", "tokens": [50364, 400, 264, 2590, 294, 597, 286, 11, 294, 597, 286, 24436, 309, 11, 390, 286, 4305, 2837, 295, 264, 4274, 50636], "temperature": 0.0, "avg_logprob": -0.2302231924874442, "compression_ratio": 1.8356164383561644, "no_speech_prob": 0.007232522126287222}, {"id": 247, "seek": 124036, "start": 1245.8, "end": 1253.1399999999999, "text": " of the squared gradients, and any time the average of the squared gradients decreased", "tokens": [50636, 295, 264, 8889, 2771, 2448, 11, 293, 604, 565, 264, 4274, 295, 264, 8889, 2771, 2448, 24436, 51003], "temperature": 0.0, "avg_logprob": -0.2302231924874442, "compression_ratio": 1.8356164383561644, "no_speech_prob": 0.007232522126287222}, {"id": 248, "seek": 124036, "start": 1253.1399999999999, "end": 1255.9599999999998, "text": " during an epoch, I stored it.", "tokens": [51003, 1830, 364, 30992, 339, 11, 286, 12187, 309, 13, 51144], "temperature": 0.0, "avg_logprob": -0.2302231924874442, "compression_ratio": 1.8356164383561644, "no_speech_prob": 0.007232522126287222}, {"id": 249, "seek": 124036, "start": 1255.9599999999998, "end": 1262.9199999999998, "text": " So I basically kept track of the lowest squared gradients we had.", "tokens": [51144, 407, 286, 1936, 4305, 2837, 295, 264, 12437, 8889, 2771, 2448, 321, 632, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2302231924874442, "compression_ratio": 1.8356164383561644, "no_speech_prob": 0.007232522126287222}, {"id": 250, "seek": 126292, "start": 1262.92, "end": 1277.64, "text": " And then what I did was if we got a, if that resulted in the gradients, the squared gradients", "tokens": [50364, 400, 550, 437, 286, 630, 390, 498, 321, 658, 257, 11, 498, 300, 18753, 294, 264, 2771, 2448, 11, 264, 8889, 2771, 2448, 51100], "temperature": 0.0, "avg_logprob": -0.22627136163544237, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.0271685179322958}, {"id": 251, "seek": 126292, "start": 1277.64, "end": 1289.88, "text": " average halving, then I would decrease the learning rate by, then I would decrease the", "tokens": [51100, 4274, 7523, 798, 11, 550, 286, 576, 11514, 264, 2539, 3314, 538, 11, 550, 286, 576, 11514, 264, 51712], "temperature": 0.0, "avg_logprob": -0.22627136163544237, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.0271685179322958}, {"id": 252, "seek": 126292, "start": 1289.88, "end": 1292.5600000000002, "text": " learning rate by a factor of four.", "tokens": [51712, 2539, 3314, 538, 257, 5952, 295, 1451, 13, 51846], "temperature": 0.0, "avg_logprob": -0.22627136163544237, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.0271685179322958}, {"id": 253, "seek": 129256, "start": 1293.2, "end": 1294.84, "text": " So I was keeping track of this gradient ratio.", "tokens": [50396, 407, 286, 390, 5145, 2837, 295, 341, 16235, 8509, 13, 50478], "temperature": 0.0, "avg_logprob": -0.2120082119861281, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.003075362415984273}, {"id": 254, "seek": 129256, "start": 1294.84, "end": 1299.84, "text": " Now when you see a range like this, you can find what that's referring to by just clicking", "tokens": [50478, 823, 562, 291, 536, 257, 3613, 411, 341, 11, 291, 393, 915, 437, 300, 311, 13761, 281, 538, 445, 9697, 50728], "temperature": 0.0, "avg_logprob": -0.2120082119861281, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.003075362415984273}, {"id": 255, "seek": 129256, "start": 1299.84, "end": 1304.32, "text": " up here and finding gradient ratio.", "tokens": [50728, 493, 510, 293, 5006, 16235, 8509, 13, 50952], "temperature": 0.0, "avg_logprob": -0.2120082119861281, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.003075362415984273}, {"id": 256, "seek": 129256, "start": 1304.32, "end": 1310.3999999999999, "text": " And there it is, and you can see that it's equal to the ratio between the average of", "tokens": [50952, 400, 456, 309, 307, 11, 293, 291, 393, 536, 300, 309, 311, 2681, 281, 264, 8509, 1296, 264, 4274, 295, 51256], "temperature": 0.0, "avg_logprob": -0.2120082119861281, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.003075362415984273}, {"id": 257, "seek": 129256, "start": 1310.3999999999999, "end": 1317.32, "text": " the squared gradients versus the minimum that we've seen so far.", "tokens": [51256, 264, 8889, 2771, 2448, 5717, 264, 7285, 300, 321, 600, 1612, 370, 1400, 13, 51602], "temperature": 0.0, "avg_logprob": -0.2120082119861281, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.003075362415984273}, {"id": 258, "seek": 131732, "start": 1317.32, "end": 1324.84, "text": " So this is kind of like, my theory here was thinking that, yeah, basically as you train,", "tokens": [50364, 407, 341, 307, 733, 295, 411, 11, 452, 5261, 510, 390, 1953, 300, 11, 1338, 11, 1936, 382, 291, 3847, 11, 50740], "temperature": 0.0, "avg_logprob": -0.21510118756975447, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0009547107038088143}, {"id": 259, "seek": 131732, "start": 1324.84, "end": 1329.48, "text": " you kind of get into flatter and more stable areas.", "tokens": [50740, 291, 733, 295, 483, 666, 41247, 293, 544, 8351, 3179, 13, 50972], "temperature": 0.0, "avg_logprob": -0.21510118756975447, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0009547107038088143}, {"id": 260, "seek": 131732, "start": 1329.48, "end": 1335.0, "text": " And as you do that, that's a sign that, you know, you might want to decrease your learning", "tokens": [50972, 400, 382, 291, 360, 300, 11, 300, 311, 257, 1465, 300, 11, 291, 458, 11, 291, 1062, 528, 281, 11514, 428, 2539, 51248], "temperature": 0.0, "avg_logprob": -0.21510118756975447, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0009547107038088143}, {"id": 261, "seek": 131732, "start": 1335.0, "end": 1336.0, "text": " rate.", "tokens": [51248, 3314, 13, 51298], "temperature": 0.0, "avg_logprob": -0.21510118756975447, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0009547107038088143}, {"id": 262, "seek": 131732, "start": 1336.0, "end": 1343.04, "text": " So yeah, if I try that, if I hit run, again it jumps straight to a pretty good value,", "tokens": [51298, 407, 1338, 11, 498, 286, 853, 300, 11, 498, 286, 2045, 1190, 11, 797, 309, 16704, 2997, 281, 257, 1238, 665, 2158, 11, 51650], "temperature": 0.0, "avg_logprob": -0.21510118756975447, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0009547107038088143}, {"id": 263, "seek": 131732, "start": 1343.04, "end": 1344.8799999999999, "text": " but I'm not going to change the learning rate manually.", "tokens": [51650, 457, 286, 478, 406, 516, 281, 1319, 264, 2539, 3314, 16945, 13, 51742], "temperature": 0.0, "avg_logprob": -0.21510118756975447, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0009547107038088143}, {"id": 264, "seek": 134488, "start": 1344.88, "end": 1348.7600000000002, "text": " I just press run, and you can see it's changed the learning rate automatically now.", "tokens": [50364, 286, 445, 1886, 1190, 11, 293, 291, 393, 536, 309, 311, 3105, 264, 2539, 3314, 6772, 586, 13, 50558], "temperature": 0.0, "avg_logprob": -0.25501069744813787, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0004583112895488739}, {"id": 265, "seek": 134488, "start": 1348.7600000000002, "end": 1356.3600000000001, "text": " And if I keep hitting run without doing anything, look at that, it's got pretty good, hasn't", "tokens": [50558, 400, 498, 286, 1066, 8850, 1190, 1553, 884, 1340, 11, 574, 412, 300, 11, 309, 311, 658, 1238, 665, 11, 6132, 380, 50938], "temperature": 0.0, "avg_logprob": -0.25501069744813787, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0004583112895488739}, {"id": 266, "seek": 134488, "start": 1356.3600000000001, "end": 1357.3600000000001, "text": " it?", "tokens": [50938, 309, 30, 50988], "temperature": 0.0, "avg_logprob": -0.25501069744813787, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0004583112895488739}, {"id": 267, "seek": 134488, "start": 1357.3600000000001, "end": 1361.5200000000002, "text": " And the learning rates got lower and lower, and we basically got almost exactly the right", "tokens": [50988, 400, 264, 2539, 6846, 658, 3126, 293, 3126, 11, 293, 321, 1936, 658, 1920, 2293, 264, 558, 51196], "temperature": 0.0, "avg_logprob": -0.25501069744813787, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0004583112895488739}, {"id": 268, "seek": 134488, "start": 1361.5200000000002, "end": 1362.64, "text": " answer.", "tokens": [51196, 1867, 13, 51252], "temperature": 0.0, "avg_logprob": -0.25501069744813787, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0004583112895488739}, {"id": 269, "seek": 134488, "start": 1362.64, "end": 1365.0800000000002, "text": " So yeah, that's a little experiment I tried.", "tokens": [51252, 407, 1338, 11, 300, 311, 257, 707, 5120, 286, 3031, 13, 51374], "temperature": 0.0, "avg_logprob": -0.25501069744813787, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0004583112895488739}, {"id": 270, "seek": 134488, "start": 1365.0800000000002, "end": 1372.8400000000001, "text": " So maybe some of you should try experiments around whether you can create an automatic", "tokens": [51374, 407, 1310, 512, 295, 291, 820, 853, 12050, 926, 1968, 291, 393, 1884, 364, 12509, 51762], "temperature": 0.0, "avg_logprob": -0.25501069744813787, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0004583112895488739}, {"id": 271, "seek": 137284, "start": 1372.84, "end": 1377.84, "text": " annealer using the, using mini AI.", "tokens": [50364, 22256, 17148, 1228, 264, 11, 1228, 8382, 7318, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2825696375462916, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.0002913692151196301}, {"id": 272, "seek": 137284, "start": 1377.84, "end": 1382.5, "text": " I think that would be fun.", "tokens": [50614, 286, 519, 300, 576, 312, 1019, 13, 50847], "temperature": 0.0, "avg_logprob": -0.2825696375462916, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.0002913692151196301}, {"id": 273, "seek": 137284, "start": 1382.5, "end": 1387.32, "text": " So that is an excellent segue into our notebook, because we are going to talk about annealing", "tokens": [50847, 407, 300, 307, 364, 7103, 33850, 666, 527, 21060, 11, 570, 321, 366, 516, 281, 751, 466, 22256, 4270, 51088], "temperature": 0.0, "avg_logprob": -0.2825696375462916, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.0002913692151196301}, {"id": 274, "seek": 137284, "start": 1387.32, "end": 1390.48, "text": " now.", "tokens": [51088, 586, 13, 51246], "temperature": 0.0, "avg_logprob": -0.2825696375462916, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.0002913692151196301}, {"id": 275, "seek": 137284, "start": 1390.48, "end": 1398.6399999999999, "text": " So we've seen it manually before, where we've just decreased the learning rate in a notebook", "tokens": [51246, 407, 321, 600, 1612, 309, 16945, 949, 11, 689, 321, 600, 445, 24436, 264, 2539, 3314, 294, 257, 21060, 51654], "temperature": 0.0, "avg_logprob": -0.2825696375462916, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.0002913692151196301}, {"id": 276, "seek": 137284, "start": 1398.6399999999999, "end": 1401.6399999999999, "text": " and like run a second cell.", "tokens": [51654, 293, 411, 1190, 257, 1150, 2815, 13, 51804], "temperature": 0.0, "avg_logprob": -0.2825696375462916, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.0002913692151196301}, {"id": 277, "seek": 140164, "start": 1401.64, "end": 1410.44, "text": " And we've seen something in Excel, but let's look at what we generally do in PyTorch.", "tokens": [50364, 400, 321, 600, 1612, 746, 294, 19060, 11, 457, 718, 311, 574, 412, 437, 321, 5101, 360, 294, 9953, 51, 284, 339, 13, 50804], "temperature": 0.0, "avg_logprob": -0.22243861937790774, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0015978351002559066}, {"id": 278, "seek": 140164, "start": 1410.44, "end": 1417.2800000000002, "text": " So we're still in the same notebook as last time, the accelerated SGD notebook.", "tokens": [50804, 407, 321, 434, 920, 294, 264, 912, 21060, 382, 1036, 565, 11, 264, 29763, 34520, 35, 21060, 13, 51146], "temperature": 0.0, "avg_logprob": -0.22243861937790774, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0015978351002559066}, {"id": 279, "seek": 140164, "start": 1417.2800000000002, "end": 1422.0400000000002, "text": " And now that we've re-implemented all the main optimizers that people tend to use most", "tokens": [51146, 400, 586, 300, 321, 600, 319, 12, 332, 781, 14684, 439, 264, 2135, 5028, 22525, 300, 561, 3928, 281, 764, 881, 51384], "temperature": 0.0, "avg_logprob": -0.22243861937790774, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0015978351002559066}, {"id": 280, "seek": 140164, "start": 1422.0400000000002, "end": 1429.5400000000002, "text": " of the time from scratch, we can use PyTorch's of course.", "tokens": [51384, 295, 264, 565, 490, 8459, 11, 321, 393, 764, 9953, 51, 284, 339, 311, 295, 1164, 13, 51759], "temperature": 0.0, "avg_logprob": -0.22243861937790774, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0015978351002559066}, {"id": 281, "seek": 142954, "start": 1429.54, "end": 1438.58, "text": " So let's see, look, look now at how we can do our own learning rate scheduling or annealing", "tokens": [50364, 407, 718, 311, 536, 11, 574, 11, 574, 586, 412, 577, 321, 393, 360, 527, 1065, 2539, 3314, 29055, 420, 22256, 4270, 50816], "temperature": 0.0, "avg_logprob": -0.24445221223026872, "compression_ratio": 1.6721311475409837, "no_speech_prob": 7.48460806789808e-05}, {"id": 282, "seek": 142954, "start": 1438.58, "end": 1441.18, "text": " within the mini AI framework.", "tokens": [50816, 1951, 264, 8382, 7318, 8388, 13, 50946], "temperature": 0.0, "avg_logprob": -0.24445221223026872, "compression_ratio": 1.6721311475409837, "no_speech_prob": 7.48460806789808e-05}, {"id": 283, "seek": 142954, "start": 1441.18, "end": 1451.5, "text": " Now we've seen when we implemented the learning rate finder, that, that we saw how to create", "tokens": [50946, 823, 321, 600, 1612, 562, 321, 12270, 264, 2539, 3314, 915, 260, 11, 300, 11, 300, 321, 1866, 577, 281, 1884, 51462], "temperature": 0.0, "avg_logprob": -0.24445221223026872, "compression_ratio": 1.6721311475409837, "no_speech_prob": 7.48460806789808e-05}, {"id": 284, "seek": 142954, "start": 1451.5, "end": 1453.6599999999999, "text": " something that adjusts the learning rate.", "tokens": [51462, 746, 300, 4369, 82, 264, 2539, 3314, 13, 51570], "temperature": 0.0, "avg_logprob": -0.24445221223026872, "compression_ratio": 1.6721311475409837, "no_speech_prob": 7.48460806789808e-05}, {"id": 285, "seek": 142954, "start": 1453.6599999999999, "end": 1459.46, "text": " So just to remind you, this was all we had to do.", "tokens": [51570, 407, 445, 281, 4160, 291, 11, 341, 390, 439, 321, 632, 281, 360, 13, 51860], "temperature": 0.0, "avg_logprob": -0.24445221223026872, "compression_ratio": 1.6721311475409837, "no_speech_prob": 7.48460806789808e-05}, {"id": 286, "seek": 145946, "start": 1460.38, "end": 1465.22, "text": " So we had to go through the optimizers, parameter groups, and in each group set the learning", "tokens": [50410, 407, 321, 632, 281, 352, 807, 264, 5028, 22525, 11, 13075, 3935, 11, 293, 294, 1184, 1594, 992, 264, 2539, 50652], "temperature": 0.0, "avg_logprob": -0.24705165365467902, "compression_ratio": 1.7154471544715446, "no_speech_prob": 4.9859463615575805e-05}, {"id": 287, "seek": 145946, "start": 1465.22, "end": 1468.06, "text": " rate to times equals some multiplier.", "tokens": [50652, 3314, 281, 1413, 6915, 512, 44106, 13, 50794], "temperature": 0.0, "avg_logprob": -0.24705165365467902, "compression_ratio": 1.7154471544715446, "no_speech_prob": 4.9859463615575805e-05}, {"id": 288, "seek": 145946, "start": 1468.06, "end": 1474.08, "text": " If we're just, that was for the learning rate finder.", "tokens": [50794, 759, 321, 434, 445, 11, 300, 390, 337, 264, 2539, 3314, 915, 260, 13, 51095], "temperature": 0.0, "avg_logprob": -0.24705165365467902, "compression_ratio": 1.7154471544715446, "no_speech_prob": 4.9859463615575805e-05}, {"id": 289, "seek": 145946, "start": 1474.08, "end": 1477.7, "text": " So since we know how to do that, we're not going to bother re-implementing all the schedulers", "tokens": [51095, 407, 1670, 321, 458, 577, 281, 360, 300, 11, 321, 434, 406, 516, 281, 8677, 319, 12, 332, 43704, 278, 439, 264, 12000, 433, 51276], "temperature": 0.0, "avg_logprob": -0.24705165365467902, "compression_ratio": 1.7154471544715446, "no_speech_prob": 4.9859463615575805e-05}, {"id": 290, "seek": 145946, "start": 1477.7, "end": 1480.58, "text": " from scratch, because we know the basic idea now.", "tokens": [51276, 490, 8459, 11, 570, 321, 458, 264, 3875, 1558, 586, 13, 51420], "temperature": 0.0, "avg_logprob": -0.24705165365467902, "compression_ratio": 1.7154471544715446, "no_speech_prob": 4.9859463615575805e-05}, {"id": 291, "seek": 145946, "start": 1480.58, "end": 1486.02, "text": " So instead, what we're going to have to do is have a look inside the torch.optim.lr scheduler", "tokens": [51420, 407, 2602, 11, 437, 321, 434, 516, 281, 362, 281, 360, 307, 362, 257, 574, 1854, 264, 27822, 13, 5747, 332, 13, 40987, 12000, 260, 51692], "temperature": 0.0, "avg_logprob": -0.24705165365467902, "compression_ratio": 1.7154471544715446, "no_speech_prob": 4.9859463615575805e-05}, {"id": 292, "seek": 148602, "start": 1486.02, "end": 1490.74, "text": " module and see what's defined in there.", "tokens": [50364, 10088, 293, 536, 437, 311, 7642, 294, 456, 13, 50600], "temperature": 0.0, "avg_logprob": -0.2426216323654373, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.002672990318387747}, {"id": 293, "seek": 148602, "start": 1490.74, "end": 1498.54, "text": " So the lr scheduler module, you know, you can hit dot tab and see what's in there.", "tokens": [50600, 407, 264, 287, 81, 12000, 260, 10088, 11, 291, 458, 11, 291, 393, 2045, 5893, 4421, 293, 536, 437, 311, 294, 456, 13, 50990], "temperature": 0.0, "avg_logprob": -0.2426216323654373, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.002672990318387747}, {"id": 294, "seek": 148602, "start": 1498.54, "end": 1506.18, "text": " But something that I quite like to do is to use dir, because dir, dir lr scheduler is", "tokens": [50990, 583, 746, 300, 286, 1596, 411, 281, 360, 307, 281, 764, 4746, 11, 570, 4746, 11, 4746, 287, 81, 12000, 260, 307, 51372], "temperature": 0.0, "avg_logprob": -0.2426216323654373, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.002672990318387747}, {"id": 295, "seek": 148602, "start": 1506.18, "end": 1513.62, "text": " a nice little function that tells you everything inside a Python object.", "tokens": [51372, 257, 1481, 707, 2445, 300, 5112, 291, 1203, 1854, 257, 15329, 2657, 13, 51744], "temperature": 0.0, "avg_logprob": -0.2426216323654373, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.002672990318387747}, {"id": 296, "seek": 151362, "start": 1513.62, "end": 1519.6999999999998, "text": " And this particular object is a module object, and it tells you all the stuff in the module.", "tokens": [50364, 400, 341, 1729, 2657, 307, 257, 10088, 2657, 11, 293, 309, 5112, 291, 439, 264, 1507, 294, 264, 10088, 13, 50668], "temperature": 0.0, "avg_logprob": -0.2614217395624839, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0023231517989188433}, {"id": 297, "seek": 151362, "start": 1519.6999999999998, "end": 1525.82, "text": " When you use the dot version tab, it doesn't show you stuff that starts with an underscore,", "tokens": [50668, 1133, 291, 764, 264, 5893, 3037, 4421, 11, 309, 1177, 380, 855, 291, 1507, 300, 3719, 365, 364, 37556, 11, 50974], "temperature": 0.0, "avg_logprob": -0.2614217395624839, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0023231517989188433}, {"id": 298, "seek": 151362, "start": 1525.82, "end": 1527.82, "text": " by the way, because that stuff's considered private.", "tokens": [50974, 538, 264, 636, 11, 570, 300, 1507, 311, 4888, 4551, 13, 51074], "temperature": 0.0, "avg_logprob": -0.2614217395624839, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0023231517989188433}, {"id": 299, "seek": 151362, "start": 1527.82, "end": 1530.3, "text": " Where as dir does show you that stuff.", "tokens": [51074, 2305, 382, 4746, 775, 855, 291, 300, 1507, 13, 51198], "temperature": 0.0, "avg_logprob": -0.2614217395624839, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0023231517989188433}, {"id": 300, "seek": 151362, "start": 1530.3, "end": 1536.34, "text": " Now I can kind of see from here that the things that start with a capital and then a small", "tokens": [51198, 823, 286, 393, 733, 295, 536, 490, 510, 300, 264, 721, 300, 722, 365, 257, 4238, 293, 550, 257, 1359, 51500], "temperature": 0.0, "avg_logprob": -0.2614217395624839, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0023231517989188433}, {"id": 301, "seek": 151362, "start": 1536.34, "end": 1539.26, "text": " letter look like the things we care about.", "tokens": [51500, 5063, 574, 411, 264, 721, 321, 1127, 466, 13, 51646], "temperature": 0.0, "avg_logprob": -0.2614217395624839, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0023231517989188433}, {"id": 302, "seek": 151362, "start": 1539.26, "end": 1543.3999999999999, "text": " We probably don't care about this, we probably don't care about these.", "tokens": [51646, 492, 1391, 500, 380, 1127, 466, 341, 11, 321, 1391, 500, 380, 1127, 466, 613, 13, 51853], "temperature": 0.0, "avg_logprob": -0.2614217395624839, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0023231517989188433}, {"id": 303, "seek": 154340, "start": 1544.18, "end": 1547.38, "text": " So we can just do a little list comprehension that checks that the first letter is an uppercase", "tokens": [50403, 407, 321, 393, 445, 360, 257, 707, 1329, 44991, 300, 13834, 300, 264, 700, 5063, 307, 364, 11775, 2869, 651, 50563], "temperature": 0.0, "avg_logprob": -0.20013064090336594, "compression_ratio": 1.7125, "no_speech_prob": 0.001169503666460514}, {"id": 304, "seek": 154340, "start": 1547.38, "end": 1552.4, "text": " and the second letter is lowercase, and then join those all together with a space.", "tokens": [50563, 293, 264, 1150, 5063, 307, 3126, 9765, 11, 293, 550, 3917, 729, 439, 1214, 365, 257, 1901, 13, 50814], "temperature": 0.0, "avg_logprob": -0.20013064090336594, "compression_ratio": 1.7125, "no_speech_prob": 0.001169503666460514}, {"id": 305, "seek": 154340, "start": 1552.4, "end": 1557.26, "text": " And so here is a nice way to get a list of all of the schedulers that PyTorch has available.", "tokens": [50814, 400, 370, 510, 307, 257, 1481, 636, 281, 483, 257, 1329, 295, 439, 295, 264, 12000, 433, 300, 9953, 51, 284, 339, 575, 2435, 13, 51057], "temperature": 0.0, "avg_logprob": -0.20013064090336594, "compression_ratio": 1.7125, "no_speech_prob": 0.001169503666460514}, {"id": 306, "seek": 154340, "start": 1557.26, "end": 1564.1000000000001, "text": " And actually, I couldn't find such a list on the PyTorch website in the documentation,", "tokens": [51057, 400, 767, 11, 286, 2809, 380, 915, 1270, 257, 1329, 322, 264, 9953, 51, 284, 339, 3144, 294, 264, 14333, 11, 51399], "temperature": 0.0, "avg_logprob": -0.20013064090336594, "compression_ratio": 1.7125, "no_speech_prob": 0.001169503666460514}, {"id": 307, "seek": 154340, "start": 1564.1000000000001, "end": 1571.6000000000001, "text": " so this is actually a handy thing to have available.", "tokens": [51399, 370, 341, 307, 767, 257, 13239, 551, 281, 362, 2435, 13, 51774], "temperature": 0.0, "avg_logprob": -0.20013064090336594, "compression_ratio": 1.7125, "no_speech_prob": 0.001169503666460514}, {"id": 308, "seek": 157160, "start": 1571.6, "end": 1576.76, "text": " So here's various schedulers we can use, and so I thought we might experiment with", "tokens": [50364, 407, 510, 311, 3683, 12000, 433, 321, 393, 764, 11, 293, 370, 286, 1194, 321, 1062, 5120, 365, 50622], "temperature": 0.0, "avg_logprob": -0.20882062117258707, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.0003353481588419527}, {"id": 309, "seek": 157160, "start": 1576.76, "end": 1583.1599999999999, "text": " using cosine annealing.", "tokens": [50622, 1228, 23565, 22256, 4270, 13, 50942], "temperature": 0.0, "avg_logprob": -0.20882062117258707, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.0003353481588419527}, {"id": 310, "seek": 157160, "start": 1583.1599999999999, "end": 1590.3999999999999, "text": " So before we do, we have to recognize that these PyTorch schedulers work with PyTorch", "tokens": [50942, 407, 949, 321, 360, 11, 321, 362, 281, 5521, 300, 613, 9953, 51, 284, 339, 12000, 433, 589, 365, 9953, 51, 284, 339, 51304], "temperature": 0.0, "avg_logprob": -0.20882062117258707, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.0003353481588419527}, {"id": 311, "seek": 157160, "start": 1590.3999999999999, "end": 1594.3999999999999, "text": " optimizers, not with, of course, with our custom SGD class.", "tokens": [51304, 5028, 22525, 11, 406, 365, 11, 295, 1164, 11, 365, 527, 2375, 34520, 35, 1508, 13, 51504], "temperature": 0.0, "avg_logprob": -0.20882062117258707, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.0003353481588419527}, {"id": 312, "seek": 157160, "start": 1594.3999999999999, "end": 1599.8, "text": " And PyTorch optimizers have a slightly different API, and so we might learn how they work.", "tokens": [51504, 400, 9953, 51, 284, 339, 5028, 22525, 362, 257, 4748, 819, 9362, 11, 293, 370, 321, 1062, 1466, 577, 436, 589, 13, 51774], "temperature": 0.0, "avg_logprob": -0.20882062117258707, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.0003353481588419527}, {"id": 313, "seek": 159980, "start": 1599.8, "end": 1602.6, "text": " So to learn how they work, we need an optimizer.", "tokens": [50364, 407, 281, 1466, 577, 436, 589, 11, 321, 643, 364, 5028, 6545, 13, 50504], "temperature": 0.0, "avg_logprob": -0.262559023770419, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0008040816173888743}, {"id": 314, "seek": 159980, "start": 1602.6, "end": 1608.4199999999998, "text": " So some, one easy way to just grab an optimizer would be to create a learner, just kind of", "tokens": [50504, 407, 512, 11, 472, 1858, 636, 281, 445, 4444, 364, 5028, 6545, 576, 312, 281, 1884, 257, 33347, 11, 445, 733, 295, 50795], "temperature": 0.0, "avg_logprob": -0.262559023770419, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0008040816173888743}, {"id": 315, "seek": 159980, "start": 1608.4199999999998, "end": 1614.1599999999999, "text": " pretty much any old random learner, and pass in that single batch callback that we created.", "tokens": [50795, 1238, 709, 604, 1331, 4974, 33347, 11, 293, 1320, 294, 300, 2167, 15245, 818, 3207, 300, 321, 2942, 13, 51082], "temperature": 0.0, "avg_logprob": -0.262559023770419, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0008040816173888743}, {"id": 316, "seek": 159980, "start": 1614.1599999999999, "end": 1617.12, "text": " Do you remember that single batch callback?", "tokens": [51082, 1144, 291, 1604, 300, 2167, 15245, 818, 3207, 30, 51230], "temperature": 0.0, "avg_logprob": -0.262559023770419, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0008040816173888743}, {"id": 317, "seek": 159980, "start": 1617.12, "end": 1619.44, "text": " Single batch.", "tokens": [51230, 31248, 15245, 13, 51346], "temperature": 0.0, "avg_logprob": -0.262559023770419, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0008040816173888743}, {"id": 318, "seek": 159980, "start": 1619.44, "end": 1622.84, "text": " It just, after batch, it cancels the fit.", "tokens": [51346, 467, 445, 11, 934, 15245, 11, 309, 393, 66, 1625, 264, 3318, 13, 51516], "temperature": 0.0, "avg_logprob": -0.262559023770419, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0008040816173888743}, {"id": 319, "seek": 159980, "start": 1622.84, "end": 1627.28, "text": " So it literally just does one batch.", "tokens": [51516, 407, 309, 3736, 445, 775, 472, 15245, 13, 51738], "temperature": 0.0, "avg_logprob": -0.262559023770419, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0008040816173888743}, {"id": 320, "seek": 159980, "start": 1627.28, "end": 1629.04, "text": " And we could fit.", "tokens": [51738, 400, 321, 727, 3318, 13, 51826], "temperature": 0.0, "avg_logprob": -0.262559023770419, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0008040816173888743}, {"id": 321, "seek": 162904, "start": 1629.28, "end": 1633.32, "text": " And from that, we've now got a learner and an optimizer.", "tokens": [50376, 400, 490, 300, 11, 321, 600, 586, 658, 257, 33347, 293, 364, 5028, 6545, 13, 50578], "temperature": 0.0, "avg_logprob": -0.21940821011861164, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.0002653017290867865}, {"id": 322, "seek": 162904, "start": 1633.32, "end": 1634.6, "text": " And so we can do the same thing.", "tokens": [50578, 400, 370, 321, 393, 360, 264, 912, 551, 13, 50642], "temperature": 0.0, "avg_logprob": -0.21940821011861164, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.0002653017290867865}, {"id": 323, "seek": 162904, "start": 1634.6, "end": 1638.44, "text": " We can do our optimizer to see what attributes it has.", "tokens": [50642, 492, 393, 360, 527, 5028, 6545, 281, 536, 437, 17212, 309, 575, 13, 50834], "temperature": 0.0, "avg_logprob": -0.21940821011861164, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.0002653017290867865}, {"id": 324, "seek": 162904, "start": 1638.44, "end": 1641.3999999999999, "text": " This is a nice way, or of course just read the documentation in PyTorch.", "tokens": [50834, 639, 307, 257, 1481, 636, 11, 420, 295, 1164, 445, 1401, 264, 14333, 294, 9953, 51, 284, 339, 13, 50982], "temperature": 0.0, "avg_logprob": -0.21940821011861164, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.0002653017290867865}, {"id": 325, "seek": 162904, "start": 1641.3999999999999, "end": 1645.84, "text": " This one is documented, I think, showing all the things it can do.", "tokens": [50982, 639, 472, 307, 23007, 11, 286, 519, 11, 4099, 439, 264, 721, 309, 393, 360, 13, 51204], "temperature": 0.0, "avg_logprob": -0.21940821011861164, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.0002653017290867865}, {"id": 326, "seek": 162904, "start": 1645.84, "end": 1652.28, "text": " As you would expect, it's got the step and the zero grad, like we're familiar with.", "tokens": [51204, 1018, 291, 576, 2066, 11, 309, 311, 658, 264, 1823, 293, 264, 4018, 2771, 11, 411, 321, 434, 4963, 365, 13, 51526], "temperature": 0.0, "avg_logprob": -0.21940821011861164, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.0002653017290867865}, {"id": 327, "seek": 162904, "start": 1652.28, "end": 1657.52, "text": " Or you can just, if you just hit opt.", "tokens": [51526, 1610, 291, 393, 445, 11, 498, 291, 445, 2045, 2427, 13, 51788], "temperature": 0.0, "avg_logprob": -0.21940821011861164, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.0002653017290867865}, {"id": 328, "seek": 165752, "start": 1657.52, "end": 1662.8, "text": " So you can, the optimizers in PyTorch do actually have a repra, as it's called, which means", "tokens": [50364, 407, 291, 393, 11, 264, 5028, 22525, 294, 9953, 51, 284, 339, 360, 767, 362, 257, 1085, 424, 11, 382, 309, 311, 1219, 11, 597, 1355, 50628], "temperature": 0.0, "avg_logprob": -0.2326497802734375, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002571415388956666}, {"id": 329, "seek": 165752, "start": 1662.8, "end": 1666.6, "text": " you can just type it in and hit shift enter, and you can also see the information about", "tokens": [50628, 291, 393, 445, 2010, 309, 294, 293, 2045, 5513, 3242, 11, 293, 291, 393, 611, 536, 264, 1589, 466, 50818], "temperature": 0.0, "avg_logprob": -0.2326497802734375, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002571415388956666}, {"id": 330, "seek": 165752, "start": 1666.6, "end": 1668.08, "text": " it this way.", "tokens": [50818, 309, 341, 636, 13, 50892], "temperature": 0.0, "avg_logprob": -0.2326497802734375, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002571415388956666}, {"id": 331, "seek": 165752, "start": 1668.08, "end": 1671.6399999999999, "text": " Now an optimizer, it'll tell you what kind of optimizer it is.", "tokens": [50892, 823, 364, 5028, 6545, 11, 309, 603, 980, 291, 437, 733, 295, 5028, 6545, 309, 307, 13, 51070], "temperature": 0.0, "avg_logprob": -0.2326497802734375, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002571415388956666}, {"id": 332, "seek": 165752, "start": 1671.6399999999999, "end": 1677.8799999999999, "text": " And so in this case, the default optimizer for a learner, when we created it, we decided", "tokens": [51070, 400, 370, 294, 341, 1389, 11, 264, 7576, 5028, 6545, 337, 257, 33347, 11, 562, 321, 2942, 309, 11, 321, 3047, 51382], "temperature": 0.0, "avg_logprob": -0.2326497802734375, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002571415388956666}, {"id": 333, "seek": 165752, "start": 1677.8799999999999, "end": 1679.76, "text": " was optim.sgd.sgd.", "tokens": [51382, 390, 2427, 332, 13, 82, 70, 67, 13, 82, 70, 67, 13, 51476], "temperature": 0.0, "avg_logprob": -0.2326497802734375, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002571415388956666}, {"id": 334, "seek": 165752, "start": 1679.76, "end": 1684.48, "text": " So we've got an SGD optimizer.", "tokens": [51476, 407, 321, 600, 658, 364, 34520, 35, 5028, 6545, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2326497802734375, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002571415388956666}, {"id": 335, "seek": 168448, "start": 1684.48, "end": 1687.8, "text": " But it's got these things called parameter groups.", "tokens": [50364, 583, 309, 311, 658, 613, 721, 1219, 13075, 3935, 13, 50530], "temperature": 0.0, "avg_logprob": -0.24132378619650136, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.059205494821071625}, {"id": 336, "seek": 168448, "start": 1687.8, "end": 1689.08, "text": " What are parameter groups?", "tokens": [50530, 708, 366, 13075, 3935, 30, 50594], "temperature": 0.0, "avg_logprob": -0.24132378619650136, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.059205494821071625}, {"id": 337, "seek": 168448, "start": 1689.08, "end": 1694.52, "text": " Well, parameter groups are, as it suggests, they're groups of parameters.", "tokens": [50594, 1042, 11, 13075, 3935, 366, 11, 382, 309, 13409, 11, 436, 434, 3935, 295, 9834, 13, 50866], "temperature": 0.0, "avg_logprob": -0.24132378619650136, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.059205494821071625}, {"id": 338, "seek": 168448, "start": 1694.52, "end": 1700.76, "text": " And in fact, we only have one parameter group here, which means all of our parameters are", "tokens": [50866, 400, 294, 1186, 11, 321, 787, 362, 472, 13075, 1594, 510, 11, 597, 1355, 439, 295, 527, 9834, 366, 51178], "temperature": 0.0, "avg_logprob": -0.24132378619650136, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.059205494821071625}, {"id": 339, "seek": 168448, "start": 1700.76, "end": 1702.32, "text": " in this group.", "tokens": [51178, 294, 341, 1594, 13, 51256], "temperature": 0.0, "avg_logprob": -0.24132378619650136, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.059205494821071625}, {"id": 340, "seek": 168448, "start": 1702.32, "end": 1703.76, "text": " So let me kind of try and show you.", "tokens": [51256, 407, 718, 385, 733, 295, 853, 293, 855, 291, 13, 51328], "temperature": 0.0, "avg_logprob": -0.24132378619650136, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.059205494821071625}, {"id": 341, "seek": 168448, "start": 1703.76, "end": 1707.44, "text": " It's a little bit confusing, but it's kind of quite neat.", "tokens": [51328, 467, 311, 257, 707, 857, 13181, 11, 457, 309, 311, 733, 295, 1596, 10654, 13, 51512], "temperature": 0.0, "avg_logprob": -0.24132378619650136, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.059205494821071625}, {"id": 342, "seek": 168448, "start": 1707.44, "end": 1711.24, "text": " So let's grab all of our parameters.", "tokens": [51512, 407, 718, 311, 4444, 439, 295, 527, 9834, 13, 51702], "temperature": 0.0, "avg_logprob": -0.24132378619650136, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.059205494821071625}, {"id": 343, "seek": 168448, "start": 1711.24, "end": 1712.72, "text": " And that's actually a generator.", "tokens": [51702, 400, 300, 311, 767, 257, 19265, 13, 51776], "temperature": 0.0, "avg_logprob": -0.24132378619650136, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.059205494821071625}, {"id": 344, "seek": 171272, "start": 1712.76, "end": 1716.32, "text": " So we have to turn that into an iterator and call next, and that will just give us our", "tokens": [50366, 407, 321, 362, 281, 1261, 300, 666, 364, 17138, 1639, 293, 818, 958, 11, 293, 300, 486, 445, 976, 505, 527, 50544], "temperature": 0.0, "avg_logprob": -0.268560497038955, "compression_ratio": 1.7468879668049793, "no_speech_prob": 0.002434336580336094}, {"id": 345, "seek": 171272, "start": 1716.32, "end": 1718.1200000000001, "text": " first parameter.", "tokens": [50544, 700, 13075, 13, 50634], "temperature": 0.0, "avg_logprob": -0.268560497038955, "compression_ratio": 1.7468879668049793, "no_speech_prob": 0.002434336580336094}, {"id": 346, "seek": 171272, "start": 1718.1200000000001, "end": 1726.6000000000001, "text": " Okay, now what we can do is we can then check the state of the optimizer.", "tokens": [50634, 1033, 11, 586, 437, 321, 393, 360, 307, 321, 393, 550, 1520, 264, 1785, 295, 264, 5028, 6545, 13, 51058], "temperature": 0.0, "avg_logprob": -0.268560497038955, "compression_ratio": 1.7468879668049793, "no_speech_prob": 0.002434336580336094}, {"id": 347, "seek": 171272, "start": 1726.6000000000001, "end": 1733.48, "text": " And the state is a dictionary, and the keys are parameter tensors.", "tokens": [51058, 400, 264, 1785, 307, 257, 25890, 11, 293, 264, 9317, 366, 13075, 10688, 830, 13, 51402], "temperature": 0.0, "avg_logprob": -0.268560497038955, "compression_ratio": 1.7468879668049793, "no_speech_prob": 0.002434336580336094}, {"id": 348, "seek": 171272, "start": 1733.48, "end": 1736.64, "text": " So this is kind of pretty interesting, because you might be, I'm sure you're familiar with", "tokens": [51402, 407, 341, 307, 733, 295, 1238, 1880, 11, 570, 291, 1062, 312, 11, 286, 478, 988, 291, 434, 4963, 365, 51560], "temperature": 0.0, "avg_logprob": -0.268560497038955, "compression_ratio": 1.7468879668049793, "no_speech_prob": 0.002434336580336094}, {"id": 349, "seek": 171272, "start": 1736.64, "end": 1741.76, "text": " dictionaries, I hope you're familiar with dictionaries, but normally you probably use", "tokens": [51560, 22352, 4889, 11, 286, 1454, 291, 434, 4963, 365, 22352, 4889, 11, 457, 5646, 291, 1391, 764, 51816], "temperature": 0.0, "avg_logprob": -0.268560497038955, "compression_ratio": 1.7468879668049793, "no_speech_prob": 0.002434336580336094}, {"id": 350, "seek": 174176, "start": 1741.8, "end": 1744.44, "text": " numbers or strings as keys.", "tokens": [50366, 3547, 420, 13985, 382, 9317, 13, 50498], "temperature": 0.0, "avg_logprob": -0.26468578251925384, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.02800704725086689}, {"id": 351, "seek": 174176, "start": 1744.44, "end": 1748.36, "text": " But actually you can use tensors as keys, and indeed that's what happens here.", "tokens": [50498, 583, 767, 291, 393, 764, 10688, 830, 382, 9317, 11, 293, 6451, 300, 311, 437, 2314, 510, 13, 50694], "temperature": 0.0, "avg_logprob": -0.26468578251925384, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.02800704725086689}, {"id": 352, "seek": 174176, "start": 1748.36, "end": 1756.08, "text": " If we look at param, it's a tensor, it's actually a parameter, which remember is a tensor, which", "tokens": [50694, 759, 321, 574, 412, 6220, 11, 309, 311, 257, 40863, 11, 309, 311, 767, 257, 13075, 11, 597, 1604, 307, 257, 40863, 11, 597, 51080], "temperature": 0.0, "avg_logprob": -0.26468578251925384, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.02800704725086689}, {"id": 353, "seek": 174176, "start": 1756.08, "end": 1764.36, "text": " it knows to require grad and to list in the parameters of the module.", "tokens": [51080, 309, 3255, 281, 3651, 2771, 293, 281, 1329, 294, 264, 9834, 295, 264, 10088, 13, 51494], "temperature": 0.0, "avg_logprob": -0.26468578251925384, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.02800704725086689}, {"id": 354, "seek": 174176, "start": 1764.36, "end": 1767.8799999999999, "text": " And so we're actually using that to index into the state.", "tokens": [51494, 400, 370, 321, 434, 767, 1228, 300, 281, 8186, 666, 264, 1785, 13, 51670], "temperature": 0.0, "avg_logprob": -0.26468578251925384, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.02800704725086689}, {"id": 355, "seek": 176788, "start": 1768.0, "end": 1775.8400000000001, "text": " So if you look at up.state, it's a dictionary where the keys are parameters.", "tokens": [50370, 407, 498, 291, 574, 412, 493, 13, 15406, 11, 309, 311, 257, 25890, 689, 264, 9317, 366, 9834, 13, 50762], "temperature": 0.0, "avg_logprob": -0.24842369910514, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0035935617052018642}, {"id": 356, "seek": 176788, "start": 1775.8400000000001, "end": 1777.5200000000002, "text": " Now what's this for?", "tokens": [50762, 823, 437, 311, 341, 337, 30, 50846], "temperature": 0.0, "avg_logprob": -0.24842369910514, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0035935617052018642}, {"id": 357, "seek": 176788, "start": 1777.5200000000002, "end": 1784.72, "text": " Well what we want to be able to do is, if you think back to this, we actually had each", "tokens": [50846, 1042, 437, 321, 528, 281, 312, 1075, 281, 360, 307, 11, 498, 291, 519, 646, 281, 341, 11, 321, 767, 632, 1184, 51206], "temperature": 0.0, "avg_logprob": -0.24842369910514, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0035935617052018642}, {"id": 358, "seek": 176788, "start": 1784.72, "end": 1787.5200000000002, "text": " parameter, we have state for it.", "tokens": [51206, 13075, 11, 321, 362, 1785, 337, 309, 13, 51346], "temperature": 0.0, "avg_logprob": -0.24842369910514, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0035935617052018642}, {"id": 359, "seek": 176788, "start": 1787.5200000000002, "end": 1791.68, "text": " We have the average of the gradients, or the exponentially weight of moving average gradients,", "tokens": [51346, 492, 362, 264, 4274, 295, 264, 2771, 2448, 11, 420, 264, 37330, 3364, 295, 2684, 4274, 2771, 2448, 11, 51554], "temperature": 0.0, "avg_logprob": -0.24842369910514, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0035935617052018642}, {"id": 360, "seek": 176788, "start": 1791.68, "end": 1797.2, "text": " and of squared averages, and we actually stored them as attributes.", "tokens": [51554, 293, 295, 8889, 42257, 11, 293, 321, 767, 12187, 552, 382, 17212, 13, 51830], "temperature": 0.0, "avg_logprob": -0.24842369910514, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0035935617052018642}, {"id": 361, "seek": 179720, "start": 1797.52, "end": 1801.52, "text": " So PyTorch does it a little bit differently, it doesn't store them as attributes, but instead", "tokens": [50380, 407, 9953, 51, 284, 339, 775, 309, 257, 707, 857, 7614, 11, 309, 1177, 380, 3531, 552, 382, 17212, 11, 457, 2602, 50580], "temperature": 0.0, "avg_logprob": -0.2668684074677617, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.0003199978091288358}, {"id": 362, "seek": 179720, "start": 1801.52, "end": 1809.48, "text": " it, the optimizer has a dictionary where you can look up, where you can index into it using", "tokens": [50580, 309, 11, 264, 5028, 6545, 575, 257, 25890, 689, 291, 393, 574, 493, 11, 689, 291, 393, 8186, 666, 309, 1228, 50978], "temperature": 0.0, "avg_logprob": -0.2668684074677617, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.0003199978091288358}, {"id": 363, "seek": 179720, "start": 1809.48, "end": 1816.92, "text": " a parameter, and that gives you the state.", "tokens": [50978, 257, 13075, 11, 293, 300, 2709, 291, 264, 1785, 13, 51350], "temperature": 0.0, "avg_logprob": -0.2668684074677617, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.0003199978091288358}, {"id": 364, "seek": 179720, "start": 1816.92, "end": 1823.64, "text": " And so you can see here, it's got a, this is the exponentially weighted moving averages,", "tokens": [51350, 400, 370, 291, 393, 536, 510, 11, 309, 311, 658, 257, 11, 341, 307, 264, 37330, 32807, 2684, 42257, 11, 51686], "temperature": 0.0, "avg_logprob": -0.2668684074677617, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.0003199978091288358}, {"id": 365, "seek": 182364, "start": 1823.64, "end": 1827.48, "text": " and both because we haven't done any training yet, and because we're using non-momentum", "tokens": [50364, 293, 1293, 570, 321, 2378, 380, 1096, 604, 3097, 1939, 11, 293, 570, 321, 434, 1228, 2107, 12, 42544, 317, 449, 50556], "temperature": 0.0, "avg_logprob": -0.22139819305722075, "compression_ratio": 1.536480686695279, "no_speech_prob": 0.025957154110074043}, {"id": 366, "seek": 182364, "start": 1827.48, "end": 1832.1200000000001, "text": " SGD, it's none, but that's how it would be stored.", "tokens": [50556, 34520, 35, 11, 309, 311, 6022, 11, 457, 300, 311, 577, 309, 576, 312, 12187, 13, 50788], "temperature": 0.0, "avg_logprob": -0.22139819305722075, "compression_ratio": 1.536480686695279, "no_speech_prob": 0.025957154110074043}, {"id": 367, "seek": 182364, "start": 1832.1200000000001, "end": 1836.6000000000001, "text": " So this is really important to understand PyTorch optimizers.", "tokens": [50788, 407, 341, 307, 534, 1021, 281, 1223, 9953, 51, 284, 339, 5028, 22525, 13, 51012], "temperature": 0.0, "avg_logprob": -0.22139819305722075, "compression_ratio": 1.536480686695279, "no_speech_prob": 0.025957154110074043}, {"id": 368, "seek": 182364, "start": 1836.6000000000001, "end": 1842.96, "text": " I quite liked our way of doing it, of just storing the state directly as attributes,", "tokens": [51012, 286, 1596, 4501, 527, 636, 295, 884, 309, 11, 295, 445, 26085, 264, 1785, 3838, 382, 17212, 11, 51330], "temperature": 0.0, "avg_logprob": -0.22139819305722075, "compression_ratio": 1.536480686695279, "no_speech_prob": 0.025957154110074043}, {"id": 369, "seek": 182364, "start": 1842.96, "end": 1848.3600000000001, "text": " but this works as well, and it's fine, you just have to know it's there.", "tokens": [51330, 457, 341, 1985, 382, 731, 11, 293, 309, 311, 2489, 11, 291, 445, 362, 281, 458, 309, 311, 456, 13, 51600], "temperature": 0.0, "avg_logprob": -0.22139819305722075, "compression_ratio": 1.536480686695279, "no_speech_prob": 0.025957154110074043}, {"id": 370, "seek": 184836, "start": 1848.36, "end": 1860.9599999999998, "text": " And then, as I said, rather than just having parameters, so we in SGD stored the parameters", "tokens": [50364, 400, 550, 11, 382, 286, 848, 11, 2831, 813, 445, 1419, 9834, 11, 370, 321, 294, 34520, 35, 12187, 264, 9834, 50994], "temperature": 0.0, "avg_logprob": -0.30669070001858384, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.012820987962186337}, {"id": 371, "seek": 184836, "start": 1860.9599999999998, "end": 1867.6799999999998, "text": " directly, but in PyTorch those parameters can be put into groups.", "tokens": [50994, 3838, 11, 457, 294, 9953, 51, 284, 339, 729, 9834, 393, 312, 829, 666, 3935, 13, 51330], "temperature": 0.0, "avg_logprob": -0.30669070001858384, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.012820987962186337}, {"id": 372, "seek": 184836, "start": 1867.6799999999998, "end": 1872.4799999999998, "text": " And so, since we haven't put them into groups, the length of param groups is one, this is", "tokens": [51330, 400, 370, 11, 1670, 321, 2378, 380, 829, 552, 666, 3935, 11, 264, 4641, 295, 6220, 3935, 307, 472, 11, 341, 307, 51570], "temperature": 0.0, "avg_logprob": -0.30669070001858384, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.012820987962186337}, {"id": 373, "seek": 187248, "start": 1872.48, "end": 1881.48, "text": " one group, so here is the param groups, and that group contains all of our parameters.", "tokens": [50364, 472, 1594, 11, 370, 510, 307, 264, 6220, 3935, 11, 293, 300, 1594, 8306, 439, 295, 527, 9834, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2946843338012695, "compression_ratio": 1.4251968503937007, "no_speech_prob": 0.17552722990512848}, {"id": 374, "seek": 187248, "start": 1881.48, "end": 1897.48, "text": " Okay, so PG, just to clarify here what's going on, PG is a dictionary, it's a parameter group,", "tokens": [50814, 1033, 11, 370, 40975, 11, 445, 281, 17594, 510, 437, 311, 516, 322, 11, 40975, 307, 257, 25890, 11, 309, 311, 257, 13075, 1594, 11, 51614], "temperature": 0.0, "avg_logprob": -0.2946843338012695, "compression_ratio": 1.4251968503937007, "no_speech_prob": 0.17552722990512848}, {"id": 375, "seek": 189748, "start": 1897.48, "end": 1901.52, "text": " and to get the keys from a dictionary you can just listify it, that gives you back the", "tokens": [50364, 293, 281, 483, 264, 9317, 490, 257, 25890, 291, 393, 445, 1329, 2505, 309, 11, 300, 2709, 291, 646, 264, 50566], "temperature": 0.0, "avg_logprob": -0.24411314587260402, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.017711598426103592}, {"id": 376, "seek": 189748, "start": 1901.52, "end": 1902.52, "text": " keys.", "tokens": [50566, 9317, 13, 50616], "temperature": 0.0, "avg_logprob": -0.24411314587260402, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.017711598426103592}, {"id": 377, "seek": 189748, "start": 1902.52, "end": 1906.8, "text": " And so this is one quick way of finding out all the keys in a dictionary, so you can see", "tokens": [50616, 400, 370, 341, 307, 472, 1702, 636, 295, 5006, 484, 439, 264, 9317, 294, 257, 25890, 11, 370, 291, 393, 536, 50830], "temperature": 0.0, "avg_logprob": -0.24411314587260402, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.017711598426103592}, {"id": 378, "seek": 189748, "start": 1906.8, "end": 1912.32, "text": " all the parameters in the group, and you can see all of the hyper parameters, the learning", "tokens": [50830, 439, 264, 9834, 294, 264, 1594, 11, 293, 291, 393, 536, 439, 295, 264, 9848, 9834, 11, 264, 2539, 51106], "temperature": 0.0, "avg_logprob": -0.24411314587260402, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.017711598426103592}, {"id": 379, "seek": 189748, "start": 1912.32, "end": 1919.52, "text": " rate, the momentum, weight decay, and so forth.", "tokens": [51106, 3314, 11, 264, 11244, 11, 3364, 21039, 11, 293, 370, 5220, 13, 51466], "temperature": 0.0, "avg_logprob": -0.24411314587260402, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.017711598426103592}, {"id": 380, "seek": 191952, "start": 1919.52, "end": 1930.84, "text": " So that gives you some background about what's going on inside an optimizer.", "tokens": [50364, 407, 300, 2709, 291, 512, 3678, 466, 437, 311, 516, 322, 1854, 364, 5028, 6545, 13, 50930], "temperature": 0.0, "avg_logprob": -0.23031103438225345, "compression_ratio": 1.5176470588235293, "no_speech_prob": 0.003707108786329627}, {"id": 381, "seek": 191952, "start": 1930.84, "end": 1937.0, "text": " So Siva asks, isn't indexing by a tensor just like passing a tensor argument to a method?", "tokens": [50930, 407, 318, 5931, 8962, 11, 1943, 380, 8186, 278, 538, 257, 40863, 445, 411, 8437, 257, 40863, 6770, 281, 257, 3170, 30, 51238], "temperature": 0.0, "avg_logprob": -0.23031103438225345, "compression_ratio": 1.5176470588235293, "no_speech_prob": 0.003707108786329627}, {"id": 382, "seek": 191952, "start": 1937.0, "end": 1946.22, "text": " And no, it's not quite the same, because this is state, so this is how the optimizer stores", "tokens": [51238, 400, 572, 11, 309, 311, 406, 1596, 264, 912, 11, 570, 341, 307, 1785, 11, 370, 341, 307, 577, 264, 5028, 6545, 9512, 51699], "temperature": 0.0, "avg_logprob": -0.23031103438225345, "compression_ratio": 1.5176470588235293, "no_speech_prob": 0.003707108786329627}, {"id": 383, "seek": 194622, "start": 1946.38, "end": 1949.82, "text": " state about the parameters, it has to be stored somewhere.", "tokens": [50372, 1785, 466, 264, 9834, 11, 309, 575, 281, 312, 12187, 4079, 13, 50544], "temperature": 0.0, "avg_logprob": -0.2975266569404192, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.01883300021290779}, {"id": 384, "seek": 194622, "start": 1949.82, "end": 1956.06, "text": " For our homemade mini-ai version we stored it as attributes on the parameter, but in", "tokens": [50544, 1171, 527, 23336, 8382, 12, 1301, 3037, 321, 12187, 309, 382, 17212, 322, 264, 13075, 11, 457, 294, 50856], "temperature": 0.0, "avg_logprob": -0.2975266569404192, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.01883300021290779}, {"id": 385, "seek": 194622, "start": 1956.06, "end": 1961.78, "text": " the PyTorch optimizers they store it as a dictionary.", "tokens": [50856, 264, 9953, 51, 284, 339, 5028, 22525, 436, 3531, 309, 382, 257, 25890, 13, 51142], "temperature": 0.0, "avg_logprob": -0.2975266569404192, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.01883300021290779}, {"id": 386, "seek": 194622, "start": 1961.78, "end": 1964.22, "text": " So it's just how it's stored.", "tokens": [51142, 407, 309, 311, 445, 577, 309, 311, 12187, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2975266569404192, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.01883300021290779}, {"id": 387, "seek": 194622, "start": 1964.22, "end": 1967.3, "text": " Okay so with that in mind let's look at how schedulers work.", "tokens": [51264, 1033, 370, 365, 300, 294, 1575, 718, 311, 574, 412, 577, 12000, 433, 589, 13, 51418], "temperature": 0.0, "avg_logprob": -0.2975266569404192, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.01883300021290779}, {"id": 388, "seek": 194622, "start": 1967.3, "end": 1970.78, "text": " So let's create a cosine annealing scheduler.", "tokens": [51418, 407, 718, 311, 1884, 257, 23565, 22256, 4270, 12000, 260, 13, 51592], "temperature": 0.0, "avg_logprob": -0.2975266569404192, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.01883300021290779}, {"id": 389, "seek": 197078, "start": 1970.78, "end": 1976.34, "text": " So a scheduler in PyTorch, you have to pass it the optimizer, and the reason for that", "tokens": [50364, 407, 257, 12000, 260, 294, 9953, 51, 284, 339, 11, 291, 362, 281, 1320, 309, 264, 5028, 6545, 11, 293, 264, 1778, 337, 300, 50642], "temperature": 0.0, "avg_logprob": -0.21009033770600627, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.00020027293066959828}, {"id": 390, "seek": 197078, "start": 1976.34, "end": 1981.1, "text": " is we want to be able to tell it to change the learning rates of our optimizer.", "tokens": [50642, 307, 321, 528, 281, 312, 1075, 281, 980, 309, 281, 1319, 264, 2539, 6846, 295, 527, 5028, 6545, 13, 50880], "temperature": 0.0, "avg_logprob": -0.21009033770600627, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.00020027293066959828}, {"id": 391, "seek": 197078, "start": 1981.1, "end": 1985.5, "text": " So it needs to know what optimizer to change the learning rates of, so it can then do that", "tokens": [50880, 407, 309, 2203, 281, 458, 437, 5028, 6545, 281, 1319, 264, 2539, 6846, 295, 11, 370, 309, 393, 550, 360, 300, 51100], "temperature": 0.0, "avg_logprob": -0.21009033770600627, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.00020027293066959828}, {"id": 392, "seek": 197078, "start": 1985.5, "end": 1988.18, "text": " for each set of parameters.", "tokens": [51100, 337, 1184, 992, 295, 9834, 13, 51234], "temperature": 0.0, "avg_logprob": -0.21009033770600627, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.00020027293066959828}, {"id": 393, "seek": 197078, "start": 1988.18, "end": 1991.78, "text": " And the reason that it does it by parameter group is that as we'll learn in a later lesson", "tokens": [51234, 400, 264, 1778, 300, 309, 775, 309, 538, 13075, 1594, 307, 300, 382, 321, 603, 1466, 294, 257, 1780, 6898, 51414], "temperature": 0.0, "avg_logprob": -0.21009033770600627, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.00020027293066959828}, {"id": 394, "seek": 197078, "start": 1991.78, "end": 1998.04, "text": " for things like transfer learning, we often want to adjust the learning rates of the later", "tokens": [51414, 337, 721, 411, 5003, 2539, 11, 321, 2049, 528, 281, 4369, 264, 2539, 6846, 295, 264, 1780, 51727], "temperature": 0.0, "avg_logprob": -0.21009033770600627, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.00020027293066959828}, {"id": 395, "seek": 199804, "start": 1998.04, "end": 2004.24, "text": " layers differently to the earlier layers that actually have different learning rates.", "tokens": [50364, 7914, 7614, 281, 264, 3071, 7914, 300, 767, 362, 819, 2539, 6846, 13, 50674], "temperature": 0.0, "avg_logprob": -0.24395511024876645, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.1602427065372467}, {"id": 396, "seek": 199804, "start": 2004.24, "end": 2008.08, "text": " And so that's why we can have different groups, and the different groups have the different", "tokens": [50674, 400, 370, 300, 311, 983, 321, 393, 362, 819, 3935, 11, 293, 264, 819, 3935, 362, 264, 819, 50866], "temperature": 0.0, "avg_logprob": -0.24395511024876645, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.1602427065372467}, {"id": 397, "seek": 199804, "start": 2008.08, "end": 2011.3999999999999, "text": " learning rates, momentums, and so forth.", "tokens": [50866, 2539, 6846, 11, 1623, 8099, 11, 293, 370, 5220, 13, 51032], "temperature": 0.0, "avg_logprob": -0.24395511024876645, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.1602427065372467}, {"id": 398, "seek": 199804, "start": 2011.3999999999999, "end": 2017.36, "text": " Okay so we pass in the optimizer, and then if I hit shift tab a couple of times it'll", "tokens": [51032, 1033, 370, 321, 1320, 294, 264, 5028, 6545, 11, 293, 550, 498, 286, 2045, 5513, 4421, 257, 1916, 295, 1413, 309, 603, 51330], "temperature": 0.0, "avg_logprob": -0.24395511024876645, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.1602427065372467}, {"id": 399, "seek": 199804, "start": 2017.36, "end": 2019.8, "text": " tell me all of the things that you can pass in.", "tokens": [51330, 980, 385, 439, 295, 264, 721, 300, 291, 393, 1320, 294, 13, 51452], "temperature": 0.0, "avg_logprob": -0.24395511024876645, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.1602427065372467}, {"id": 400, "seek": 199804, "start": 2019.8, "end": 2026.96, "text": " And so it needs to know, t max, how many iterations you're going to do, and that's because it's", "tokens": [51452, 400, 370, 309, 2203, 281, 458, 11, 256, 11469, 11, 577, 867, 36540, 291, 434, 516, 281, 360, 11, 293, 300, 311, 570, 309, 311, 51810], "temperature": 0.0, "avg_logprob": -0.24395511024876645, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.1602427065372467}, {"id": 401, "seek": 202696, "start": 2026.96, "end": 2034.76, "text": " trying to do one, you know, half a wave, if you like, of the cosine curve.", "tokens": [50364, 1382, 281, 360, 472, 11, 291, 458, 11, 1922, 257, 5772, 11, 498, 291, 411, 11, 295, 264, 23565, 7605, 13, 50754], "temperature": 0.0, "avg_logprob": -0.23775791238855432, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0001376544387312606}, {"id": 402, "seek": 202696, "start": 2034.76, "end": 2037.8400000000001, "text": " So it needs to know how many iterations you're going to do, so it needs to know how far to", "tokens": [50754, 407, 309, 2203, 281, 458, 577, 867, 36540, 291, 434, 516, 281, 360, 11, 370, 309, 2203, 281, 458, 577, 1400, 281, 50908], "temperature": 0.0, "avg_logprob": -0.23775791238855432, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0001376544387312606}, {"id": 403, "seek": 202696, "start": 2037.8400000000001, "end": 2038.8400000000001, "text": " step each time.", "tokens": [50908, 1823, 1184, 565, 13, 50958], "temperature": 0.0, "avg_logprob": -0.23775791238855432, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0001376544387312606}, {"id": 404, "seek": 202696, "start": 2038.8400000000001, "end": 2041.06, "text": " So if we're going to do a hundred iterations.", "tokens": [50958, 407, 498, 321, 434, 516, 281, 360, 257, 3262, 36540, 13, 51069], "temperature": 0.0, "avg_logprob": -0.23775791238855432, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0001376544387312606}, {"id": 405, "seek": 202696, "start": 2041.06, "end": 2046.96, "text": " So the scheduler is going to store the base learning rate, and where did it get that from?", "tokens": [51069, 407, 264, 12000, 260, 307, 516, 281, 3531, 264, 3096, 2539, 3314, 11, 293, 689, 630, 309, 483, 300, 490, 30, 51364], "temperature": 0.0, "avg_logprob": -0.23775791238855432, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0001376544387312606}, {"id": 406, "seek": 202696, "start": 2046.96, "end": 2053.28, "text": " It got it from our optimizer, which we set a learning rate.", "tokens": [51364, 467, 658, 309, 490, 527, 5028, 6545, 11, 597, 321, 992, 257, 2539, 3314, 13, 51680], "temperature": 0.0, "avg_logprob": -0.23775791238855432, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0001376544387312606}, {"id": 407, "seek": 205328, "start": 2053.6000000000004, "end": 2058.48, "text": " Okay so it's going to steal the optimizer's learning rate, and that's going to be the", "tokens": [50380, 1033, 370, 309, 311, 516, 281, 11009, 264, 5028, 6545, 311, 2539, 3314, 11, 293, 300, 311, 516, 281, 312, 264, 50624], "temperature": 0.0, "avg_logprob": -0.19033361736096835, "compression_ratio": 1.8312236286919832, "no_speech_prob": 5.064462675363757e-05}, {"id": 408, "seek": 205328, "start": 2058.48, "end": 2061.0, "text": " starting learning rate, the base learning rate.", "tokens": [50624, 2891, 2539, 3314, 11, 264, 3096, 2539, 3314, 13, 50750], "temperature": 0.0, "avg_logprob": -0.19033361736096835, "compression_ratio": 1.8312236286919832, "no_speech_prob": 5.064462675363757e-05}, {"id": 409, "seek": 205328, "start": 2061.0, "end": 2064.1200000000003, "text": " And it's a list because there could be a different one for each parameter group, we only have", "tokens": [50750, 400, 309, 311, 257, 1329, 570, 456, 727, 312, 257, 819, 472, 337, 1184, 13075, 1594, 11, 321, 787, 362, 50906], "temperature": 0.0, "avg_logprob": -0.19033361736096835, "compression_ratio": 1.8312236286919832, "no_speech_prob": 5.064462675363757e-05}, {"id": 410, "seek": 205328, "start": 2064.1200000000003, "end": 2066.0, "text": " one parameter group.", "tokens": [50906, 472, 13075, 1594, 13, 51000], "temperature": 0.0, "avg_logprob": -0.19033361736096835, "compression_ratio": 1.8312236286919832, "no_speech_prob": 5.064462675363757e-05}, {"id": 411, "seek": 205328, "start": 2066.0, "end": 2070.96, "text": " You can also get the most recent learning rate from a scheduler, which of course is", "tokens": [51000, 509, 393, 611, 483, 264, 881, 5162, 2539, 3314, 490, 257, 12000, 260, 11, 597, 295, 1164, 307, 51248], "temperature": 0.0, "avg_logprob": -0.19033361736096835, "compression_ratio": 1.8312236286919832, "no_speech_prob": 5.064462675363757e-05}, {"id": 412, "seek": 205328, "start": 2070.96, "end": 2072.92, "text": " the same.", "tokens": [51248, 264, 912, 13, 51346], "temperature": 0.0, "avg_logprob": -0.19033361736096835, "compression_ratio": 1.8312236286919832, "no_speech_prob": 5.064462675363757e-05}, {"id": 413, "seek": 205328, "start": 2072.92, "end": 2080.1600000000003, "text": " And so I couldn't find any method in PyTorch to actually plot a scheduler's learning rates,", "tokens": [51346, 400, 370, 286, 2809, 380, 915, 604, 3170, 294, 9953, 51, 284, 339, 281, 767, 7542, 257, 12000, 260, 311, 2539, 6846, 11, 51708], "temperature": 0.0, "avg_logprob": -0.19033361736096835, "compression_ratio": 1.8312236286919832, "no_speech_prob": 5.064462675363757e-05}, {"id": 414, "seek": 208016, "start": 2080.16, "end": 2086.24, "text": " so I just made a tiny little thing that just created a list, set it to the last learning", "tokens": [50364, 370, 286, 445, 1027, 257, 5870, 707, 551, 300, 445, 2942, 257, 1329, 11, 992, 309, 281, 264, 1036, 2539, 50668], "temperature": 0.0, "avg_logprob": -0.21562601725260416, "compression_ratio": 1.8930232558139535, "no_speech_prob": 0.001187882386147976}, {"id": 415, "seek": 208016, "start": 2086.24, "end": 2090.7999999999997, "text": " rate of the scheduler, which is going to start at 0.06, and then goes through however many", "tokens": [50668, 3314, 295, 264, 12000, 260, 11, 597, 307, 516, 281, 722, 412, 1958, 13, 12791, 11, 293, 550, 1709, 807, 4461, 867, 50896], "temperature": 0.0, "avg_logprob": -0.21562601725260416, "compression_ratio": 1.8930232558139535, "no_speech_prob": 0.001187882386147976}, {"id": 416, "seek": 208016, "start": 2090.7999999999997, "end": 2095.64, "text": " steps you ask for, steps the optimizer, steps the scheduler.", "tokens": [50896, 4439, 291, 1029, 337, 11, 4439, 264, 5028, 6545, 11, 4439, 264, 12000, 260, 13, 51138], "temperature": 0.0, "avg_logprob": -0.21562601725260416, "compression_ratio": 1.8930232558139535, "no_speech_prob": 0.001187882386147976}, {"id": 417, "seek": 208016, "start": 2095.64, "end": 2101.2, "text": " So this is the thing that causes the scheduler to adjust its learning rate, and then just", "tokens": [51138, 407, 341, 307, 264, 551, 300, 7700, 264, 12000, 260, 281, 4369, 1080, 2539, 3314, 11, 293, 550, 445, 51416], "temperature": 0.0, "avg_logprob": -0.21562601725260416, "compression_ratio": 1.8930232558139535, "no_speech_prob": 0.001187882386147976}, {"id": 418, "seek": 208016, "start": 2101.2, "end": 2106.8799999999997, "text": " append that new learning rate to a list of learning rates, and then plot it.", "tokens": [51416, 34116, 300, 777, 2539, 3314, 281, 257, 1329, 295, 2539, 6846, 11, 293, 550, 7542, 309, 13, 51700], "temperature": 0.0, "avg_logprob": -0.21562601725260416, "compression_ratio": 1.8930232558139535, "no_speech_prob": 0.001187882386147976}, {"id": 419, "seek": 210688, "start": 2106.88, "end": 2111.44, "text": " So that's here's, and what I've done here is I've intentionally gone over a hundred,", "tokens": [50364, 407, 300, 311, 510, 311, 11, 293, 437, 286, 600, 1096, 510, 307, 286, 600, 22062, 2780, 670, 257, 3262, 11, 50592], "temperature": 0.0, "avg_logprob": -0.23245661137467724, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.012431222014129162}, {"id": 420, "seek": 210688, "start": 2111.44, "end": 2114.6, "text": " because I had told it I'm going to do a hundred, so I'm going over a hundred.", "tokens": [50592, 570, 286, 632, 1907, 309, 286, 478, 516, 281, 360, 257, 3262, 11, 370, 286, 478, 516, 670, 257, 3262, 13, 50750], "temperature": 0.0, "avg_logprob": -0.23245661137467724, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.012431222014129162}, {"id": 421, "seek": 210688, "start": 2114.6, "end": 2120.52, "text": " And you can see the learning rate, if we did a hundred iterations, would start high for", "tokens": [50750, 400, 291, 393, 536, 264, 2539, 3314, 11, 498, 321, 630, 257, 3262, 36540, 11, 576, 722, 1090, 337, 51046], "temperature": 0.0, "avg_logprob": -0.23245661137467724, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.012431222014129162}, {"id": 422, "seek": 210688, "start": 2120.52, "end": 2125.7200000000003, "text": " a while, it would then go down, and then it would stay low for a while.", "tokens": [51046, 257, 1339, 11, 309, 576, 550, 352, 760, 11, 293, 550, 309, 576, 1754, 2295, 337, 257, 1339, 13, 51306], "temperature": 0.0, "avg_logprob": -0.23245661137467724, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.012431222014129162}, {"id": 423, "seek": 210688, "start": 2125.7200000000003, "end": 2129.12, "text": " And if we intentionally go past the maximum, it's actually start going up again, because", "tokens": [51306, 400, 498, 321, 22062, 352, 1791, 264, 6674, 11, 309, 311, 767, 722, 516, 493, 797, 11, 570, 51476], "temperature": 0.0, "avg_logprob": -0.23245661137467724, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.012431222014129162}, {"id": 424, "seek": 210688, "start": 2129.12, "end": 2132.08, "text": " this is a cosine curve.", "tokens": [51476, 341, 307, 257, 23565, 7605, 13, 51624], "temperature": 0.0, "avg_logprob": -0.23245661137467724, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.012431222014129162}, {"id": 425, "seek": 213208, "start": 2132.08, "end": 2141.52, "text": " So one of the main things I guess I wanted to show here is like what it looks like to", "tokens": [50364, 407, 472, 295, 264, 2135, 721, 286, 2041, 286, 1415, 281, 855, 510, 307, 411, 437, 309, 1542, 411, 281, 50836], "temperature": 0.0, "avg_logprob": -0.27758649029309235, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.0038244316820055246}, {"id": 426, "seek": 213208, "start": 2141.52, "end": 2150.08, "text": " really investigate in a REPL environment, like a notebook, how, you know, how an object", "tokens": [50836, 534, 15013, 294, 257, 31511, 43, 2823, 11, 411, 257, 21060, 11, 577, 11, 291, 458, 11, 577, 364, 2657, 51264], "temperature": 0.0, "avg_logprob": -0.27758649029309235, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.0038244316820055246}, {"id": 427, "seek": 213208, "start": 2150.08, "end": 2153.2999999999997, "text": " behaves, you know, what's in it.", "tokens": [51264, 36896, 11, 291, 458, 11, 437, 311, 294, 309, 13, 51425], "temperature": 0.0, "avg_logprob": -0.27758649029309235, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.0038244316820055246}, {"id": 428, "seek": 213208, "start": 2153.2999999999997, "end": 2157.64, "text": " And you know this is something I would always want to do when I'm using something from an", "tokens": [51425, 400, 291, 458, 341, 307, 746, 286, 576, 1009, 528, 281, 360, 562, 286, 478, 1228, 746, 490, 364, 51642], "temperature": 0.0, "avg_logprob": -0.27758649029309235, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.0038244316820055246}, {"id": 429, "seek": 215764, "start": 2157.64, "end": 2159.0, "text": " API I'm not very familiar with.", "tokens": [50364, 9362, 286, 478, 406, 588, 4963, 365, 13, 50432], "temperature": 0.0, "avg_logprob": -0.2503305600013262, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.05834376811981201}, {"id": 430, "seek": 215764, "start": 2159.0, "end": 2165.56, "text": " I really want to like see what's in it, see what they do, run it totally independently,", "tokens": [50432, 286, 534, 528, 281, 411, 536, 437, 311, 294, 309, 11, 536, 437, 436, 360, 11, 1190, 309, 3879, 21761, 11, 50760], "temperature": 0.0, "avg_logprob": -0.2503305600013262, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.05834376811981201}, {"id": 431, "seek": 215764, "start": 2165.56, "end": 2167.7999999999997, "text": " plot anything I can plot.", "tokens": [50760, 7542, 1340, 286, 393, 7542, 13, 50872], "temperature": 0.0, "avg_logprob": -0.2503305600013262, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.05834376811981201}, {"id": 432, "seek": 215764, "start": 2167.7999999999997, "end": 2175.16, "text": " This is how I like to learn about the stuff I'm working with.", "tokens": [50872, 639, 307, 577, 286, 411, 281, 1466, 466, 264, 1507, 286, 478, 1364, 365, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2503305600013262, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.05834376811981201}, {"id": 433, "seek": 215764, "start": 2175.16, "end": 2180.8399999999997, "text": " You know data scientists don't spend all of their time just coding, you know, so that", "tokens": [51240, 509, 458, 1412, 7708, 500, 380, 3496, 439, 295, 641, 565, 445, 17720, 11, 291, 458, 11, 370, 300, 51524], "temperature": 0.0, "avg_logprob": -0.2503305600013262, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.05834376811981201}, {"id": 434, "seek": 218084, "start": 2180.84, "end": 2188.0, "text": " means we need, we can't just rely on using the same classes and APIs every day.", "tokens": [50364, 1355, 321, 643, 11, 321, 393, 380, 445, 10687, 322, 1228, 264, 912, 5359, 293, 21445, 633, 786, 13, 50722], "temperature": 0.0, "avg_logprob": -0.2231885274251302, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.14802800118923187}, {"id": 435, "seek": 218084, "start": 2188.0, "end": 2192.88, "text": " So we have to be very good at exploring them and learning about them, and so that's why", "tokens": [50722, 407, 321, 362, 281, 312, 588, 665, 412, 12736, 552, 293, 2539, 466, 552, 11, 293, 370, 300, 311, 983, 50966], "temperature": 0.0, "avg_logprob": -0.2231885274251302, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.14802800118923187}, {"id": 436, "seek": 218084, "start": 2192.88, "end": 2195.96, "text": " I think this is a really good approach.", "tokens": [50966, 286, 519, 341, 307, 257, 534, 665, 3109, 13, 51120], "temperature": 0.0, "avg_logprob": -0.2231885274251302, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.14802800118923187}, {"id": 437, "seek": 218084, "start": 2195.96, "end": 2201.32, "text": " Okay so let's create a scheduler callback.", "tokens": [51120, 1033, 370, 718, 311, 1884, 257, 12000, 260, 818, 3207, 13, 51388], "temperature": 0.0, "avg_logprob": -0.2231885274251302, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.14802800118923187}, {"id": 438, "seek": 218084, "start": 2201.32, "end": 2208.6800000000003, "text": " So a scheduler callback is something we're going to pass in the scheduling class, but", "tokens": [51388, 407, 257, 12000, 260, 818, 3207, 307, 746, 321, 434, 516, 281, 1320, 294, 264, 29055, 1508, 11, 457, 51756], "temperature": 0.0, "avg_logprob": -0.2231885274251302, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.14802800118923187}, {"id": 439, "seek": 220868, "start": 2209.08, "end": 2213.9199999999996, "text": " remember then when we, or the scheduling callable actually, and remember that when we create", "tokens": [50384, 1604, 550, 562, 321, 11, 420, 264, 29055, 818, 712, 767, 11, 293, 1604, 300, 562, 321, 1884, 50626], "temperature": 0.0, "avg_logprob": -0.2774919470151265, "compression_ratio": 2.0104166666666665, "no_speech_prob": 0.0004173154884483665}, {"id": 440, "seek": 220868, "start": 2213.9199999999996, "end": 2225.0, "text": " the scheduler we have to pass in the optimizer to schedule, and so before fit, that's the", "tokens": [50626, 264, 12000, 260, 321, 362, 281, 1320, 294, 264, 5028, 6545, 281, 7567, 11, 293, 370, 949, 3318, 11, 300, 311, 264, 51180], "temperature": 0.0, "avg_logprob": -0.2774919470151265, "compression_ratio": 2.0104166666666665, "no_speech_prob": 0.0004173154884483665}, {"id": 441, "seek": 220868, "start": 2225.0, "end": 2229.2, "text": " point at which we have an optimizer, we will create the scheduling object.", "tokens": [51180, 935, 412, 597, 321, 362, 364, 5028, 6545, 11, 321, 486, 1884, 264, 29055, 2657, 13, 51390], "temperature": 0.0, "avg_logprob": -0.2774919470151265, "compression_ratio": 2.0104166666666665, "no_speech_prob": 0.0004173154884483665}, {"id": 442, "seek": 220868, "start": 2229.2, "end": 2232.0, "text": " I like this, it's very Australian.", "tokens": [51390, 286, 411, 341, 11, 309, 311, 588, 13337, 13, 51530], "temperature": 0.0, "avg_logprob": -0.2774919470151265, "compression_ratio": 2.0104166666666665, "no_speech_prob": 0.0004173154884483665}, {"id": 443, "seek": 220868, "start": 2232.0, "end": 2238.2799999999997, "text": " So the scheduling object we will create by passing the optimizer into the scheduler callable.", "tokens": [51530, 407, 264, 29055, 2657, 321, 486, 1884, 538, 8437, 264, 5028, 6545, 666, 264, 12000, 260, 818, 712, 13, 51844], "temperature": 0.0, "avg_logprob": -0.2774919470151265, "compression_ratio": 2.0104166666666665, "no_speech_prob": 0.0004173154884483665}, {"id": 444, "seek": 223828, "start": 2238.88, "end": 2255.0, "text": " Then when we do step, then we'll check if we're training, and if so we'll step.", "tokens": [50394, 1396, 562, 321, 360, 1823, 11, 550, 321, 603, 1520, 498, 321, 434, 3097, 11, 293, 498, 370, 321, 603, 1823, 13, 51200], "temperature": 0.0, "avg_logprob": -0.24628116190433502, "compression_ratio": 1.5845070422535212, "no_speech_prob": 5.920907642575912e-05}, {"id": 445, "seek": 223828, "start": 2255.0, "end": 2259.28, "text": " Okay so then what's going to call step is after batch.", "tokens": [51200, 1033, 370, 550, 437, 311, 516, 281, 818, 1823, 307, 934, 15245, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24628116190433502, "compression_ratio": 1.5845070422535212, "no_speech_prob": 5.920907642575912e-05}, {"id": 446, "seek": 223828, "start": 2259.28, "end": 2266.44, "text": " So after batch we'll call step, and that would be if you want your scheduler to update the", "tokens": [51414, 407, 934, 15245, 321, 603, 818, 1823, 11, 293, 300, 576, 312, 498, 291, 528, 428, 12000, 260, 281, 5623, 264, 51772], "temperature": 0.0, "avg_logprob": -0.24628116190433502, "compression_ratio": 1.5845070422535212, "no_speech_prob": 5.920907642575912e-05}, {"id": 447, "seek": 226644, "start": 2266.44, "end": 2271.48, "text": " learning rate every batch.", "tokens": [50364, 2539, 3314, 633, 15245, 13, 50616], "temperature": 0.0, "avg_logprob": -0.25447380542755127, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.0005527700996026397}, {"id": 448, "seek": 226644, "start": 2271.48, "end": 2282.88, "text": " We could also have an epoch scheduler callback, which we'll see later, and that's just going", "tokens": [50616, 492, 727, 611, 362, 364, 30992, 339, 12000, 260, 818, 3207, 11, 597, 321, 603, 536, 1780, 11, 293, 300, 311, 445, 516, 51186], "temperature": 0.0, "avg_logprob": -0.25447380542755127, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.0005527700996026397}, {"id": 449, "seek": 226644, "start": 2282.88, "end": 2285.52, "text": " to be after epoch.", "tokens": [51186, 281, 312, 934, 30992, 339, 13, 51318], "temperature": 0.0, "avg_logprob": -0.25447380542755127, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.0005527700996026397}, {"id": 450, "seek": 226644, "start": 2285.52, "end": 2294.44, "text": " Okay so in order to actually see what the scheduler is doing, we're going to need to", "tokens": [51318, 1033, 370, 294, 1668, 281, 767, 536, 437, 264, 12000, 260, 307, 884, 11, 321, 434, 516, 281, 643, 281, 51764], "temperature": 0.0, "avg_logprob": -0.25447380542755127, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.0005527700996026397}, {"id": 451, "seek": 229444, "start": 2294.44, "end": 2299.48, "text": " create a new callback to keep track of what's going on in our learner, and I figured we", "tokens": [50364, 1884, 257, 777, 818, 3207, 281, 1066, 2837, 295, 437, 311, 516, 322, 294, 527, 33347, 11, 293, 286, 8932, 321, 50616], "temperature": 0.0, "avg_logprob": -0.19786973621534265, "compression_ratio": 1.913978494623656, "no_speech_prob": 0.10229436308145523}, {"id": 452, "seek": 229444, "start": 2299.48, "end": 2306.2000000000003, "text": " could create a recorder callback, and what we're going to do is we're going to be passing", "tokens": [50616, 727, 1884, 257, 37744, 818, 3207, 11, 293, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 312, 8437, 50952], "temperature": 0.0, "avg_logprob": -0.19786973621534265, "compression_ratio": 1.913978494623656, "no_speech_prob": 0.10229436308145523}, {"id": 453, "seek": 229444, "start": 2306.2000000000003, "end": 2314.36, "text": " in the name of the thing that we want to record, that we want to keep track of in each batch,", "tokens": [50952, 294, 264, 1315, 295, 264, 551, 300, 321, 528, 281, 2136, 11, 300, 321, 528, 281, 1066, 2837, 295, 294, 1184, 15245, 11, 51360], "temperature": 0.0, "avg_logprob": -0.19786973621534265, "compression_ratio": 1.913978494623656, "no_speech_prob": 0.10229436308145523}, {"id": 454, "seek": 229444, "start": 2314.36, "end": 2320.2200000000003, "text": " and a function which is going to be responsible for grabbing the thing that we want.", "tokens": [51360, 293, 257, 2445, 597, 307, 516, 281, 312, 6250, 337, 23771, 264, 551, 300, 321, 528, 13, 51653], "temperature": 0.0, "avg_logprob": -0.19786973621534265, "compression_ratio": 1.913978494623656, "no_speech_prob": 0.10229436308145523}, {"id": 455, "seek": 232022, "start": 2320.22, "end": 2326.74, "text": " And so in this case the function here is going to grab from the callback, look up its", "tokens": [50364, 400, 370, 294, 341, 1389, 264, 2445, 510, 307, 516, 281, 4444, 490, 264, 818, 3207, 11, 574, 493, 1080, 50690], "temperature": 0.0, "avg_logprob": -0.29900107720885616, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005469169467687607}, {"id": 456, "seek": 232022, "start": 2326.74, "end": 2331.3399999999997, "text": " param groups property, and grab the learning rate.", "tokens": [50690, 6220, 3935, 4707, 11, 293, 4444, 264, 2539, 3314, 13, 50920], "temperature": 0.0, "avg_logprob": -0.29900107720885616, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005469169467687607}, {"id": 457, "seek": 232022, "start": 2331.3399999999997, "end": 2334.3799999999997, "text": " Where does the PG property come from, retribute?", "tokens": [50920, 2305, 775, 264, 40975, 4707, 808, 490, 11, 1533, 2024, 1169, 30, 51072], "temperature": 0.0, "avg_logprob": -0.29900107720885616, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005469169467687607}, {"id": 458, "seek": 232022, "start": 2334.3799999999997, "end": 2341.8599999999997, "text": " Well before fit, the recorder callback is going to grab just the first parameter group,", "tokens": [51072, 1042, 949, 3318, 11, 264, 37744, 818, 3207, 307, 516, 281, 4444, 445, 264, 700, 13075, 1594, 11, 51446], "temperature": 0.0, "avg_logprob": -0.29900107720885616, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005469169467687607}, {"id": 459, "seek": 232022, "start": 2341.8599999999997, "end": 2344.7, "text": " just so it's like you've got to pick some parameter group to track, so we'll just grab", "tokens": [51446, 445, 370, 309, 311, 411, 291, 600, 658, 281, 1888, 512, 13075, 1594, 281, 2837, 11, 370, 321, 603, 445, 4444, 51588], "temperature": 0.0, "avg_logprob": -0.29900107720885616, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005469169467687607}, {"id": 460, "seek": 232022, "start": 2344.7, "end": 2346.98, "text": " the first one.", "tokens": [51588, 264, 700, 472, 13, 51702], "temperature": 0.0, "avg_logprob": -0.29900107720885616, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005469169467687607}, {"id": 461, "seek": 234698, "start": 2347.06, "end": 2353.26, "text": " And so then also we're going to create a dictionary of all the things that we're recording, so", "tokens": [50368, 400, 370, 550, 611, 321, 434, 516, 281, 1884, 257, 25890, 295, 439, 264, 721, 300, 321, 434, 6613, 11, 370, 50678], "temperature": 0.0, "avg_logprob": -0.22845953949226822, "compression_ratio": 1.9079497907949792, "no_speech_prob": 0.0007321779849007726}, {"id": 462, "seek": 234698, "start": 2353.26, "end": 2358.58, "text": " we'll get all the names, so that's going to be in this case just LR, and initially it's", "tokens": [50678, 321, 603, 483, 439, 264, 5288, 11, 370, 300, 311, 516, 281, 312, 294, 341, 1389, 445, 441, 49, 11, 293, 9105, 309, 311, 50944], "temperature": 0.0, "avg_logprob": -0.22845953949226822, "compression_ratio": 1.9079497907949792, "no_speech_prob": 0.0007321779849007726}, {"id": 463, "seek": 234698, "start": 2358.58, "end": 2364.06, "text": " just going to be an empty list, and then after batch we'll go through each of the items in", "tokens": [50944, 445, 516, 281, 312, 364, 6707, 1329, 11, 293, 550, 934, 15245, 321, 603, 352, 807, 1184, 295, 264, 4754, 294, 51218], "temperature": 0.0, "avg_logprob": -0.22845953949226822, "compression_ratio": 1.9079497907949792, "no_speech_prob": 0.0007321779849007726}, {"id": 464, "seek": 234698, "start": 2364.06, "end": 2368.54, "text": " that dictionary, which in this case is just LR is the key, and underscore LR function", "tokens": [51218, 300, 25890, 11, 597, 294, 341, 1389, 307, 445, 441, 49, 307, 264, 2141, 11, 293, 37556, 441, 49, 2445, 51442], "temperature": 0.0, "avg_logprob": -0.22845953949226822, "compression_ratio": 1.9079497907949792, "no_speech_prob": 0.0007321779849007726}, {"id": 465, "seek": 234698, "start": 2368.54, "end": 2376.8, "text": " is the value, and we will append to that list, call that method, call that function or callable,", "tokens": [51442, 307, 264, 2158, 11, 293, 321, 486, 34116, 281, 300, 1329, 11, 818, 300, 3170, 11, 818, 300, 2445, 420, 818, 712, 11, 51855], "temperature": 0.0, "avg_logprob": -0.22845953949226822, "compression_ratio": 1.9079497907949792, "no_speech_prob": 0.0007321779849007726}, {"id": 466, "seek": 237680, "start": 2377.1200000000003, "end": 2379.0, "text": " and pass in this callback.", "tokens": [50380, 293, 1320, 294, 341, 818, 3207, 13, 50474], "temperature": 0.0, "avg_logprob": -0.25381203639654465, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.00017952760390471667}, {"id": 467, "seek": 237680, "start": 2379.0, "end": 2382.36, "text": " And that's why this is going to get the callback.", "tokens": [50474, 400, 300, 311, 983, 341, 307, 516, 281, 483, 264, 818, 3207, 13, 50642], "temperature": 0.0, "avg_logprob": -0.25381203639654465, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.00017952760390471667}, {"id": 468, "seek": 237680, "start": 2382.36, "end": 2388.1600000000003, "text": " And so that's going to basically then have a whole bunch of, you know, dictionary of", "tokens": [50642, 400, 370, 300, 311, 516, 281, 1936, 550, 362, 257, 1379, 3840, 295, 11, 291, 458, 11, 25890, 295, 50932], "temperature": 0.0, "avg_logprob": -0.25381203639654465, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.00017952760390471667}, {"id": 469, "seek": 237680, "start": 2388.1600000000003, "end": 2397.1600000000003, "text": " the results, you know, of each of these functions, after each batch, during training, so we'll", "tokens": [50932, 264, 3542, 11, 291, 458, 11, 295, 1184, 295, 613, 6828, 11, 934, 1184, 15245, 11, 1830, 3097, 11, 370, 321, 603, 51382], "temperature": 0.0, "avg_logprob": -0.25381203639654465, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.00017952760390471667}, {"id": 470, "seek": 237680, "start": 2397.1600000000003, "end": 2400.0800000000004, "text": " just go through and plot them all.", "tokens": [51382, 445, 352, 807, 293, 7542, 552, 439, 13, 51528], "temperature": 0.0, "avg_logprob": -0.25381203639654465, "compression_ratio": 1.6348314606741574, "no_speech_prob": 0.00017952760390471667}, {"id": 471, "seek": 240008, "start": 2400.08, "end": 2403.4, "text": " And so let me show you what that's going to look like.", "tokens": [50364, 400, 370, 718, 385, 855, 291, 437, 300, 311, 516, 281, 574, 411, 13, 50530], "temperature": 0.0, "avg_logprob": -0.24140185895173447, "compression_ratio": 1.640625, "no_speech_prob": 0.004133765120059252}, {"id": 472, "seek": 240008, "start": 2403.4, "end": 2413.04, "text": " If we... let's create a cosine annealing callable.", "tokens": [50530, 759, 321, 485, 718, 311, 1884, 257, 23565, 22256, 4270, 818, 712, 13, 51012], "temperature": 0.0, "avg_logprob": -0.24140185895173447, "compression_ratio": 1.640625, "no_speech_prob": 0.004133765120059252}, {"id": 473, "seek": 240008, "start": 2413.04, "end": 2417.52, "text": " So we're going to have to use a partial to say that this callable is going to have Tmax", "tokens": [51012, 407, 321, 434, 516, 281, 362, 281, 764, 257, 14641, 281, 584, 300, 341, 818, 712, 307, 516, 281, 362, 314, 41167, 51236], "temperature": 0.0, "avg_logprob": -0.24140185895173447, "compression_ratio": 1.640625, "no_speech_prob": 0.004133765120059252}, {"id": 474, "seek": 240008, "start": 2417.52, "end": 2424.04, "text": " equal to three times however many mini-batches we have in our data loader, that's because", "tokens": [51236, 2681, 281, 1045, 1413, 4461, 867, 8382, 12, 65, 852, 279, 321, 362, 294, 527, 1412, 3677, 260, 11, 300, 311, 570, 51562], "temperature": 0.0, "avg_logprob": -0.24140185895173447, "compression_ratio": 1.640625, "no_speech_prob": 0.004133765120059252}, {"id": 475, "seek": 240008, "start": 2424.04, "end": 2428.72, "text": " we're going to do three epochs.", "tokens": [51562, 321, 434, 516, 281, 360, 1045, 30992, 28346, 13, 51796], "temperature": 0.0, "avg_logprob": -0.24140185895173447, "compression_ratio": 1.640625, "no_speech_prob": 0.004133765120059252}, {"id": 476, "seek": 242872, "start": 2428.72, "end": 2441.9599999999996, "text": " And then we will set it running, and we're passing in the batch scheduler with the scheduler", "tokens": [50364, 400, 550, 321, 486, 992, 309, 2614, 11, 293, 321, 434, 8437, 294, 264, 15245, 12000, 260, 365, 264, 12000, 260, 51026], "temperature": 0.0, "avg_logprob": -0.25965313684372676, "compression_ratio": 1.6581632653061225, "no_speech_prob": 0.00014653104881290346}, {"id": 477, "seek": 242872, "start": 2441.9599999999996, "end": 2448.7599999999998, "text": " callable, and we're also going to pass in our recorder callback, saying we want to track", "tokens": [51026, 818, 712, 11, 293, 321, 434, 611, 516, 281, 1320, 294, 527, 37744, 818, 3207, 11, 1566, 321, 528, 281, 2837, 51366], "temperature": 0.0, "avg_logprob": -0.25965313684372676, "compression_ratio": 1.6581632653061225, "no_speech_prob": 0.00014653104881290346}, {"id": 478, "seek": 242872, "start": 2448.7599999999998, "end": 2451.72, "text": " the learning rate using the underscore LR function.", "tokens": [51366, 264, 2539, 3314, 1228, 264, 37556, 441, 49, 2445, 13, 51514], "temperature": 0.0, "avg_logprob": -0.25965313684372676, "compression_ratio": 1.6581632653061225, "no_speech_prob": 0.00014653104881290346}, {"id": 479, "seek": 242872, "start": 2451.72, "end": 2456.9599999999996, "text": " We're going to call fit, and oh this is actually a pretty good accuracy, we're getting, you", "tokens": [51514, 492, 434, 516, 281, 818, 3318, 11, 293, 1954, 341, 307, 767, 257, 1238, 665, 14170, 11, 321, 434, 1242, 11, 291, 51776], "temperature": 0.0, "avg_logprob": -0.25965313684372676, "compression_ratio": 1.6581632653061225, "no_speech_prob": 0.00014653104881290346}, {"id": 480, "seek": 245696, "start": 2456.96, "end": 2462.6, "text": " know, close to 90% now in only three epochs, which is impressive, and so when we then", "tokens": [50364, 458, 11, 1998, 281, 4289, 4, 586, 294, 787, 1045, 30992, 28346, 11, 597, 307, 8992, 11, 293, 370, 562, 321, 550, 50646], "temperature": 0.0, "avg_logprob": -0.2675215547735041, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0040701208636164665}, {"id": 481, "seek": 245696, "start": 2462.6, "end": 2472.76, "text": " call rec.plot, it's going to call... remember that rec is the recorder callback, so it plots", "tokens": [50646, 818, 850, 13, 564, 310, 11, 309, 311, 516, 281, 818, 485, 1604, 300, 850, 307, 264, 37744, 818, 3207, 11, 370, 309, 28609, 51154], "temperature": 0.0, "avg_logprob": -0.2675215547735041, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0040701208636164665}, {"id": 482, "seek": 245696, "start": 2472.76, "end": 2473.76, "text": " the learning rate.", "tokens": [51154, 264, 2539, 3314, 13, 51204], "temperature": 0.0, "avg_logprob": -0.2675215547735041, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0040701208636164665}, {"id": 483, "seek": 245696, "start": 2473.76, "end": 2477.32, "text": " Isn't that sweet?", "tokens": [51204, 6998, 380, 300, 3844, 30, 51382], "temperature": 0.0, "avg_logprob": -0.2675215547735041, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0040701208636164665}, {"id": 484, "seek": 245696, "start": 2477.32, "end": 2481.42, "text": " So we could, as I said, we would can do exactly the same thing but replace after batch with", "tokens": [51382, 407, 321, 727, 11, 382, 286, 848, 11, 321, 576, 393, 360, 2293, 264, 912, 551, 457, 7406, 934, 15245, 365, 51587], "temperature": 0.0, "avg_logprob": -0.2675215547735041, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0040701208636164665}, {"id": 485, "seek": 248142, "start": 2481.42, "end": 2487.34, "text": " after epoch, and this will now become a scheduler which steps at the end of each epoch, rather", "tokens": [50364, 934, 30992, 339, 11, 293, 341, 486, 586, 1813, 257, 12000, 260, 597, 4439, 412, 264, 917, 295, 1184, 30992, 339, 11, 2831, 50660], "temperature": 0.0, "avg_logprob": -0.2114001121229798, "compression_ratio": 1.9828326180257512, "no_speech_prob": 0.03567592054605484}, {"id": 486, "seek": 248142, "start": 2487.34, "end": 2490.06, "text": " than the end of each batch.", "tokens": [50660, 813, 264, 917, 295, 1184, 15245, 13, 50796], "temperature": 0.0, "avg_logprob": -0.2114001121229798, "compression_ratio": 1.9828326180257512, "no_speech_prob": 0.03567592054605484}, {"id": 487, "seek": 248142, "start": 2490.06, "end": 2495.94, "text": " So I can do exactly the same thing now using an epoch scheduler, so this time Tmax is three,", "tokens": [50796, 407, 286, 393, 360, 2293, 264, 912, 551, 586, 1228, 364, 30992, 339, 12000, 260, 11, 370, 341, 565, 314, 41167, 307, 1045, 11, 51090], "temperature": 0.0, "avg_logprob": -0.2114001121229798, "compression_ratio": 1.9828326180257512, "no_speech_prob": 0.03567592054605484}, {"id": 488, "seek": 248142, "start": 2495.94, "end": 2497.7400000000002, "text": " because we're only going to be stepping three times.", "tokens": [51090, 570, 321, 434, 787, 516, 281, 312, 16821, 1045, 1413, 13, 51180], "temperature": 0.0, "avg_logprob": -0.2114001121229798, "compression_ratio": 1.9828326180257512, "no_speech_prob": 0.03567592054605484}, {"id": 489, "seek": 248142, "start": 2497.7400000000002, "end": 2502.54, "text": " We're not stepping at the end of each batch, just at the end of each epoch.", "tokens": [51180, 492, 434, 406, 16821, 412, 264, 917, 295, 1184, 15245, 11, 445, 412, 264, 917, 295, 1184, 30992, 339, 13, 51420], "temperature": 0.0, "avg_logprob": -0.2114001121229798, "compression_ratio": 1.9828326180257512, "no_speech_prob": 0.03567592054605484}, {"id": 490, "seek": 248142, "start": 2502.54, "end": 2508.62, "text": " So that trains, and then we can call rec.plot after trains, and as you can see there, it's", "tokens": [51420, 407, 300, 16329, 11, 293, 550, 321, 393, 818, 850, 13, 564, 310, 934, 16329, 11, 293, 382, 291, 393, 536, 456, 11, 309, 311, 51724], "temperature": 0.0, "avg_logprob": -0.2114001121229798, "compression_ratio": 1.9828326180257512, "no_speech_prob": 0.03567592054605484}, {"id": 491, "seek": 248142, "start": 2508.62, "end": 2511.34, "text": " just stepping three times.", "tokens": [51724, 445, 16821, 1045, 1413, 13, 51860], "temperature": 0.0, "avg_logprob": -0.2114001121229798, "compression_ratio": 1.9828326180257512, "no_speech_prob": 0.03567592054605484}, {"id": 492, "seek": 251134, "start": 2511.38, "end": 2520.38, "text": " So you can see here, we're really digging in deeply to understanding what's happening", "tokens": [50366, 407, 291, 393, 536, 510, 11, 321, 434, 534, 17343, 294, 8760, 281, 3701, 437, 311, 2737, 50816], "temperature": 0.0, "avg_logprob": -0.26788385452762725, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.8631715647643432e-05}, {"id": 493, "seek": 251134, "start": 2520.38, "end": 2522.2200000000003, "text": " in everything in our models.", "tokens": [50816, 294, 1203, 294, 527, 5245, 13, 50908], "temperature": 0.0, "avg_logprob": -0.26788385452762725, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.8631715647643432e-05}, {"id": 494, "seek": 251134, "start": 2522.2200000000003, "end": 2524.02, "text": " What do all the activations look like?", "tokens": [50908, 708, 360, 439, 264, 2430, 763, 574, 411, 30, 50998], "temperature": 0.0, "avg_logprob": -0.26788385452762725, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.8631715647643432e-05}, {"id": 495, "seek": 251134, "start": 2524.02, "end": 2526.1000000000004, "text": " What do the losses look like?", "tokens": [50998, 708, 360, 264, 15352, 574, 411, 30, 51102], "temperature": 0.0, "avg_logprob": -0.26788385452762725, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.8631715647643432e-05}, {"id": 496, "seek": 251134, "start": 2526.1000000000004, "end": 2529.34, "text": " What do our learning rates look like?", "tokens": [51102, 708, 360, 527, 2539, 6846, 574, 411, 30, 51264], "temperature": 0.0, "avg_logprob": -0.26788385452762725, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.8631715647643432e-05}, {"id": 497, "seek": 251134, "start": 2529.34, "end": 2534.9, "text": " And we've built all this from scratch, so yeah, hopefully that gives you a sense that", "tokens": [51264, 400, 321, 600, 3094, 439, 341, 490, 8459, 11, 370, 1338, 11, 4696, 300, 2709, 291, 257, 2020, 300, 51542], "temperature": 0.0, "avg_logprob": -0.26788385452762725, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.8631715647643432e-05}, {"id": 498, "seek": 251134, "start": 2534.9, "end": 2539.3, "text": " we can really, yeah, do a lot ourselves.", "tokens": [51542, 321, 393, 534, 11, 1338, 11, 360, 257, 688, 4175, 13, 51762], "temperature": 0.0, "avg_logprob": -0.26788385452762725, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.8631715647643432e-05}, {"id": 499, "seek": 253930, "start": 2540.26, "end": 2547.26, "text": " Now, if you've done the fast.ai part one course, you'll be very aware of one cycle training,", "tokens": [50412, 823, 11, 498, 291, 600, 1096, 264, 2370, 13, 1301, 644, 472, 1164, 11, 291, 603, 312, 588, 3650, 295, 472, 6586, 3097, 11, 50762], "temperature": 0.0, "avg_logprob": -0.3824711905585395, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.0011694993590936065}, {"id": 500, "seek": 253930, "start": 2547.26, "end": 2559.94, "text": " which was from a terrific paper by Leslie Smith, which I'm not sure it ever got published,", "tokens": [50762, 597, 390, 490, 257, 20899, 3035, 538, 28140, 8538, 11, 597, 286, 478, 406, 988, 309, 1562, 658, 6572, 11, 51396], "temperature": 0.0, "avg_logprob": -0.3824711905585395, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.0011694993590936065}, {"id": 501, "seek": 253930, "start": 2559.94, "end": 2562.26, "text": " actually.", "tokens": [51396, 767, 13, 51512], "temperature": 0.0, "avg_logprob": -0.3824711905585395, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.0011694993590936065}, {"id": 502, "seek": 256226, "start": 2562.3, "end": 2568.6600000000003, "text": " And one cycle training is, well, let's take a look at it.", "tokens": [50366, 400, 472, 6586, 3097, 307, 11, 731, 11, 718, 311, 747, 257, 574, 412, 309, 13, 50684], "temperature": 0.0, "avg_logprob": -0.28379262168452424, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.027168085798621178}, {"id": 503, "seek": 256226, "start": 2568.6600000000003, "end": 2576.9, "text": " So we can just replace our scheduler with one cycle learning rate scheduler, so that's", "tokens": [50684, 407, 321, 393, 445, 7406, 527, 12000, 260, 365, 472, 6586, 2539, 3314, 12000, 260, 11, 370, 300, 311, 51096], "temperature": 0.0, "avg_logprob": -0.28379262168452424, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.027168085798621178}, {"id": 504, "seek": 256226, "start": 2576.9, "end": 2580.6200000000003, "text": " in PyTorch, and of course if it wasn't in PyTorch, we could very easily just write our", "tokens": [51096, 294, 9953, 51, 284, 339, 11, 293, 295, 1164, 498, 309, 2067, 380, 294, 9953, 51, 284, 339, 11, 321, 727, 588, 3612, 445, 2464, 527, 51282], "temperature": 0.0, "avg_logprob": -0.28379262168452424, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.027168085798621178}, {"id": 505, "seek": 256226, "start": 2580.6200000000003, "end": 2581.6200000000003, "text": " own.", "tokens": [51282, 1065, 13, 51332], "temperature": 0.0, "avg_logprob": -0.28379262168452424, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.027168085798621178}, {"id": 506, "seek": 256226, "start": 2581.6200000000003, "end": 2587.1000000000004, "text": " We're going to make it a batch scheduler, and we're going to train, this time we're", "tokens": [51332, 492, 434, 516, 281, 652, 309, 257, 15245, 12000, 260, 11, 293, 321, 434, 516, 281, 3847, 11, 341, 565, 321, 434, 51606], "temperature": 0.0, "avg_logprob": -0.28379262168452424, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.027168085798621178}, {"id": 507, "seek": 256226, "start": 2587.1000000000004, "end": 2588.6600000000003, "text": " going to do five epochs.", "tokens": [51606, 516, 281, 360, 1732, 30992, 28346, 13, 51684], "temperature": 0.0, "avg_logprob": -0.28379262168452424, "compression_ratio": 1.6995073891625616, "no_speech_prob": 0.027168085798621178}, {"id": 508, "seek": 258866, "start": 2588.66, "end": 2594.2599999999998, "text": " So we're going to train a bit longer, and so the first thing I'll point out is, hooray,", "tokens": [50364, 407, 321, 434, 516, 281, 3847, 257, 857, 2854, 11, 293, 370, 264, 700, 551, 286, 603, 935, 484, 307, 11, 43330, 320, 11, 50644], "temperature": 0.0, "avg_logprob": -0.26600097397626454, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.05108088254928589}, {"id": 509, "seek": 258866, "start": 2594.2599999999998, "end": 2601.8599999999997, "text": " we have got a new record for us, 90.6%, so that's great.", "tokens": [50644, 321, 362, 658, 257, 777, 2136, 337, 505, 11, 4289, 13, 21, 8923, 370, 300, 311, 869, 13, 51024], "temperature": 0.0, "avg_logprob": -0.26600097397626454, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.05108088254928589}, {"id": 510, "seek": 258866, "start": 2601.8599999999997, "end": 2606.8199999999997, "text": " And then B, you can see here's the plot, and now look, two things are being plotted, and", "tokens": [51024, 400, 550, 363, 11, 291, 393, 536, 510, 311, 264, 7542, 11, 293, 586, 574, 11, 732, 721, 366, 885, 43288, 11, 293, 51272], "temperature": 0.0, "avg_logprob": -0.26600097397626454, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.05108088254928589}, {"id": 511, "seek": 258866, "start": 2606.8199999999997, "end": 2612.46, "text": " that's because I've now passed into the recorder callback a plot of learning rates, and also", "tokens": [51272, 300, 311, 570, 286, 600, 586, 4678, 666, 264, 37744, 818, 3207, 257, 7542, 295, 2539, 6846, 11, 293, 611, 51554], "temperature": 0.0, "avg_logprob": -0.26600097397626454, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.05108088254928589}, {"id": 512, "seek": 258866, "start": 2612.46, "end": 2618.2599999999998, "text": " a plot of momentums, and momentums is going to get the betas zero, because remember for", "tokens": [51554, 257, 7542, 295, 1623, 8099, 11, 293, 1623, 8099, 307, 516, 281, 483, 264, 778, 296, 4018, 11, 570, 1604, 337, 51844], "temperature": 0.0, "avg_logprob": -0.26600097397626454, "compression_ratio": 1.6693548387096775, "no_speech_prob": 0.05108088254928589}, {"id": 513, "seek": 261826, "start": 2618.86, "end": 2624.46, "text": " atom, it's called beta zero and beta one, is momentum of the gradients, and the momentum", "tokens": [50394, 12018, 11, 309, 311, 1219, 9861, 4018, 293, 9861, 472, 11, 307, 11244, 295, 264, 2771, 2448, 11, 293, 264, 11244, 50674], "temperature": 0.0, "avg_logprob": -0.23993143924446994, "compression_ratio": 1.8764705882352941, "no_speech_prob": 0.00026118976529687643}, {"id": 514, "seek": 261826, "start": 2624.46, "end": 2627.2200000000003, "text": " of the gradient squared.", "tokens": [50674, 295, 264, 16235, 8889, 13, 50812], "temperature": 0.0, "avg_logprob": -0.23993143924446994, "compression_ratio": 1.8764705882352941, "no_speech_prob": 0.00026118976529687643}, {"id": 515, "seek": 261826, "start": 2627.2200000000003, "end": 2637.38, "text": " And you can see what the one cycle is doing, is the learning rate is starting very low,", "tokens": [50812, 400, 291, 393, 536, 437, 264, 472, 6586, 307, 884, 11, 307, 264, 2539, 3314, 307, 2891, 588, 2295, 11, 51320], "temperature": 0.0, "avg_logprob": -0.23993143924446994, "compression_ratio": 1.8764705882352941, "no_speech_prob": 0.00026118976529687643}, {"id": 516, "seek": 261826, "start": 2637.38, "end": 2645.42, "text": " and going up to high, and then down again, but the momentum is starting high, and then", "tokens": [51320, 293, 516, 493, 281, 1090, 11, 293, 550, 760, 797, 11, 457, 264, 11244, 307, 2891, 1090, 11, 293, 550, 51722], "temperature": 0.0, "avg_logprob": -0.23993143924446994, "compression_ratio": 1.8764705882352941, "no_speech_prob": 0.00026118976529687643}, {"id": 517, "seek": 261826, "start": 2645.42, "end": 2648.0400000000004, "text": " going down, and then up again.", "tokens": [51722, 516, 760, 11, 293, 550, 493, 797, 13, 51853], "temperature": 0.0, "avg_logprob": -0.23993143924446994, "compression_ratio": 1.8764705882352941, "no_speech_prob": 0.00026118976529687643}, {"id": 518, "seek": 264804, "start": 2648.82, "end": 2649.82, "text": " So what's the theory here?", "tokens": [50403, 407, 437, 311, 264, 5261, 510, 30, 50453], "temperature": 0.0, "avg_logprob": -0.2584695551130507, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.00013552058953791857}, {"id": 519, "seek": 264804, "start": 2649.82, "end": 2658.36, "text": " Well, the starting out at a low learning rate is particularly important if you have a not", "tokens": [50453, 1042, 11, 264, 2891, 484, 412, 257, 2295, 2539, 3314, 307, 4098, 1021, 498, 291, 362, 257, 406, 50880], "temperature": 0.0, "avg_logprob": -0.2584695551130507, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.00013552058953791857}, {"id": 520, "seek": 264804, "start": 2658.36, "end": 2667.6, "text": " perfectly initialized model, which almost everybody, almost always does, even though", "tokens": [50880, 6239, 5883, 1602, 2316, 11, 597, 1920, 2201, 11, 1920, 1009, 775, 11, 754, 1673, 51342], "temperature": 0.0, "avg_logprob": -0.2584695551130507, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.00013552058953791857}, {"id": 521, "seek": 264804, "start": 2667.6, "end": 2673.68, "text": " we spend a lot of time learning to initialize models, you know, we use a lot of models that", "tokens": [51342, 321, 3496, 257, 688, 295, 565, 2539, 281, 5883, 1125, 5245, 11, 291, 458, 11, 321, 764, 257, 688, 295, 5245, 300, 51646], "temperature": 0.0, "avg_logprob": -0.2584695551130507, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.00013552058953791857}, {"id": 522, "seek": 267368, "start": 2673.68, "end": 2685.72, "text": " get more complicated, and after a while, people, after a while, people learn or figure", "tokens": [50364, 483, 544, 6179, 11, 293, 934, 257, 1339, 11, 561, 11, 934, 257, 1339, 11, 561, 1466, 420, 2573, 50966], "temperature": 0.0, "avg_logprob": -0.2564043747751336, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.07695181667804718}, {"id": 523, "seek": 267368, "start": 2685.72, "end": 2688.56, "text": " out how to initialize more complex models properly.", "tokens": [50966, 484, 577, 281, 5883, 1125, 544, 3997, 5245, 6108, 13, 51108], "temperature": 0.0, "avg_logprob": -0.2564043747751336, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.07695181667804718}, {"id": 524, "seek": 267368, "start": 2688.56, "end": 2693.48, "text": " So for example, this is a very, very cool paper.", "tokens": [51108, 407, 337, 1365, 11, 341, 307, 257, 588, 11, 588, 1627, 3035, 13, 51354], "temperature": 0.0, "avg_logprob": -0.2564043747751336, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.07695181667804718}, {"id": 525, "seek": 267368, "start": 2693.48, "end": 2702.7799999999997, "text": " In 2019, this team figured out how to initialize ResNets properly, we'll be looking at ResNets", "tokens": [51354, 682, 6071, 11, 341, 1469, 8932, 484, 577, 281, 5883, 1125, 5015, 45, 1385, 6108, 11, 321, 603, 312, 1237, 412, 5015, 45, 1385, 51819], "temperature": 0.0, "avg_logprob": -0.2564043747751336, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.07695181667804718}, {"id": 526, "seek": 270278, "start": 2702.88, "end": 2708.46, "text": " very shortly, and they discovered when they did, that they did not need batch norm, they", "tokens": [50369, 588, 13392, 11, 293, 436, 6941, 562, 436, 630, 11, 300, 436, 630, 406, 643, 15245, 2026, 11, 436, 50648], "temperature": 0.0, "avg_logprob": -0.27147627610426683, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.001345852855592966}, {"id": 527, "seek": 270278, "start": 2708.46, "end": 2714.78, "text": " could train networks of 10,000 layers, and they could get state-of-the-art performance", "tokens": [50648, 727, 3847, 9590, 295, 1266, 11, 1360, 7914, 11, 293, 436, 727, 483, 1785, 12, 2670, 12, 3322, 12, 446, 3389, 50964], "temperature": 0.0, "avg_logprob": -0.27147627610426683, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.001345852855592966}, {"id": 528, "seek": 270278, "start": 2714.78, "end": 2726.1400000000003, "text": " with no batch norm, and there's actually been something similar for transformers, called", "tokens": [50964, 365, 572, 15245, 2026, 11, 293, 456, 311, 767, 668, 746, 2531, 337, 4088, 433, 11, 1219, 51532], "temperature": 0.0, "avg_logprob": -0.27147627610426683, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.001345852855592966}, {"id": 529, "seek": 272614, "start": 2726.14, "end": 2731.2999999999997, "text": " tfixup, that does a similar kind of thing.", "tokens": [50364, 256, 69, 970, 1010, 11, 300, 775, 257, 2531, 733, 295, 551, 13, 50622], "temperature": 0.0, "avg_logprob": -0.2824200736151801, "compression_ratio": 1.6844660194174756, "no_speech_prob": 0.2017911970615387}, {"id": 530, "seek": 272614, "start": 2731.2999999999997, "end": 2738.58, "text": " But anyway, it is quite difficult to initialize models correctly, most people fail to, most", "tokens": [50622, 583, 4033, 11, 309, 307, 1596, 2252, 281, 5883, 1125, 5245, 8944, 11, 881, 561, 3061, 281, 11, 881, 50986], "temperature": 0.0, "avg_logprob": -0.2824200736151801, "compression_ratio": 1.6844660194174756, "no_speech_prob": 0.2017911970615387}, {"id": 531, "seek": 272614, "start": 2738.58, "end": 2745.18, "text": " people fail to realize that they generally don't need tricks like warmup and batch norm", "tokens": [50986, 561, 3061, 281, 4325, 300, 436, 5101, 500, 380, 643, 11733, 411, 4561, 1010, 293, 15245, 2026, 51316], "temperature": 0.0, "avg_logprob": -0.2824200736151801, "compression_ratio": 1.6844660194174756, "no_speech_prob": 0.2017911970615387}, {"id": 532, "seek": 272614, "start": 2745.18, "end": 2746.7799999999997, "text": " if they do initialize them correctly.", "tokens": [51316, 498, 436, 360, 5883, 1125, 552, 8944, 13, 51396], "temperature": 0.0, "avg_logprob": -0.2824200736151801, "compression_ratio": 1.6844660194174756, "no_speech_prob": 0.2017911970615387}, {"id": 533, "seek": 272614, "start": 2746.7799999999997, "end": 2751.52, "text": " In fact, tfixup explicitly looks at this, it looks at the difference between no warmup", "tokens": [51396, 682, 1186, 11, 256, 69, 970, 1010, 20803, 1542, 412, 341, 11, 309, 1542, 412, 264, 2649, 1296, 572, 4561, 1010, 51633], "temperature": 0.0, "avg_logprob": -0.2824200736151801, "compression_ratio": 1.6844660194174756, "no_speech_prob": 0.2017911970615387}, {"id": 534, "seek": 275152, "start": 2751.52, "end": 2757.2, "text": " versus with warmup, with their correct initialization versus with normal initialization.", "tokens": [50364, 5717, 365, 4561, 1010, 11, 365, 641, 3006, 5883, 2144, 5717, 365, 2710, 5883, 2144, 13, 50648], "temperature": 0.0, "avg_logprob": -0.2508851051330566, "compression_ratio": 1.8892857142857142, "no_speech_prob": 0.2845403254032135}, {"id": 535, "seek": 275152, "start": 2757.2, "end": 2761.28, "text": " And you can see these pictures they're showing are pretty similar actually, log scale histograms", "tokens": [50648, 400, 291, 393, 536, 613, 5242, 436, 434, 4099, 366, 1238, 2531, 767, 11, 3565, 4373, 49816, 82, 50852], "temperature": 0.0, "avg_logprob": -0.2508851051330566, "compression_ratio": 1.8892857142857142, "no_speech_prob": 0.2845403254032135}, {"id": 536, "seek": 275152, "start": 2761.28, "end": 2765.48, "text": " of gradients, they're very similar to the colorful dimension plots.", "tokens": [50852, 295, 2771, 2448, 11, 436, 434, 588, 2531, 281, 264, 18506, 10139, 28609, 13, 51062], "temperature": 0.0, "avg_logprob": -0.2508851051330566, "compression_ratio": 1.8892857142857142, "no_speech_prob": 0.2845403254032135}, {"id": 537, "seek": 275152, "start": 2765.48, "end": 2768.7599999999998, "text": " I kind of like our colorful dimension plots better in some ways, because I think they're", "tokens": [51062, 286, 733, 295, 411, 527, 18506, 10139, 28609, 1101, 294, 512, 2098, 11, 570, 286, 519, 436, 434, 51226], "temperature": 0.0, "avg_logprob": -0.2508851051330566, "compression_ratio": 1.8892857142857142, "no_speech_prob": 0.2845403254032135}, {"id": 538, "seek": 275152, "start": 2768.7599999999998, "end": 2772.12, "text": " easier to read, although I think theirs are probably prettier.", "tokens": [51226, 3571, 281, 1401, 11, 4878, 286, 519, 22760, 366, 1391, 36825, 13, 51394], "temperature": 0.0, "avg_logprob": -0.2508851051330566, "compression_ratio": 1.8892857142857142, "no_speech_prob": 0.2845403254032135}, {"id": 539, "seek": 275152, "start": 2772.12, "end": 2775.88, "text": " So there you go Stefano, there's something to inspire you if you want to try more things", "tokens": [51394, 407, 456, 291, 352, 43421, 3730, 11, 456, 311, 746, 281, 15638, 291, 498, 291, 528, 281, 853, 544, 721, 51582], "temperature": 0.0, "avg_logprob": -0.2508851051330566, "compression_ratio": 1.8892857142857142, "no_speech_prob": 0.2845403254032135}, {"id": 540, "seek": 275152, "start": 2775.88, "end": 2778.92, "text": " with our colorful dimension plots.", "tokens": [51582, 365, 527, 18506, 10139, 28609, 13, 51734], "temperature": 0.0, "avg_logprob": -0.2508851051330566, "compression_ratio": 1.8892857142857142, "no_speech_prob": 0.2845403254032135}, {"id": 541, "seek": 277892, "start": 2778.92, "end": 2784.84, "text": " I think it's interesting that some papers are actually starting to use a similar idea,", "tokens": [50364, 286, 519, 309, 311, 1880, 300, 512, 10577, 366, 767, 2891, 281, 764, 257, 2531, 1558, 11, 50660], "temperature": 0.0, "avg_logprob": -0.2868090259785555, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.00446834322065115}, {"id": 542, "seek": 277892, "start": 2784.84, "end": 2790.32, "text": " I don't know if they got it from us or they came up with it independently, doesn't really", "tokens": [50660, 286, 500, 380, 458, 498, 436, 658, 309, 490, 505, 420, 436, 1361, 493, 365, 309, 21761, 11, 1177, 380, 534, 50934], "temperature": 0.0, "avg_logprob": -0.2868090259785555, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.00446834322065115}, {"id": 543, "seek": 277892, "start": 2790.32, "end": 2791.32, "text": " matter.", "tokens": [50934, 1871, 13, 50984], "temperature": 0.0, "avg_logprob": -0.2868090259785555, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.00446834322065115}, {"id": 544, "seek": 277892, "start": 2791.32, "end": 2799.6800000000003, "text": " But so we do a warmup if our network's not quite initialized correctly, then starting", "tokens": [50984, 583, 370, 321, 360, 257, 4561, 1010, 498, 527, 3209, 311, 406, 1596, 5883, 1602, 8944, 11, 550, 2891, 51402], "temperature": 0.0, "avg_logprob": -0.2868090259785555, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.00446834322065115}, {"id": 545, "seek": 277892, "start": 2799.6800000000003, "end": 2804.96, "text": " at a very low learning rate means it's not going to jump off way outside the area where", "tokens": [51402, 412, 257, 588, 2295, 2539, 3314, 1355, 309, 311, 406, 516, 281, 3012, 766, 636, 2380, 264, 1859, 689, 51666], "temperature": 0.0, "avg_logprob": -0.2868090259785555, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.00446834322065115}, {"id": 546, "seek": 277892, "start": 2804.96, "end": 2807.0, "text": " the weights even make sense.", "tokens": [51666, 264, 17443, 754, 652, 2020, 13, 51768], "temperature": 0.0, "avg_logprob": -0.2868090259785555, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.00446834322065115}, {"id": 547, "seek": 280700, "start": 2807.08, "end": 2811.24, "text": " And so then you gradually increase them as the weights move into a part of the space", "tokens": [50368, 400, 370, 550, 291, 13145, 3488, 552, 382, 264, 17443, 1286, 666, 257, 644, 295, 264, 1901, 50576], "temperature": 0.0, "avg_logprob": -0.21779492593580677, "compression_ratio": 1.878228782287823, "no_speech_prob": 5.3075003961566836e-05}, {"id": 548, "seek": 280700, "start": 2811.24, "end": 2813.64, "text": " that does make sense.", "tokens": [50576, 300, 775, 652, 2020, 13, 50696], "temperature": 0.0, "avg_logprob": -0.21779492593580677, "compression_ratio": 1.878228782287823, "no_speech_prob": 5.3075003961566836e-05}, {"id": 549, "seek": 280700, "start": 2813.64, "end": 2818.48, "text": " And then during that time, while we have low learning rates, if they keep moving in the", "tokens": [50696, 400, 550, 1830, 300, 565, 11, 1339, 321, 362, 2295, 2539, 6846, 11, 498, 436, 1066, 2684, 294, 264, 50938], "temperature": 0.0, "avg_logprob": -0.21779492593580677, "compression_ratio": 1.878228782287823, "no_speech_prob": 5.3075003961566836e-05}, {"id": 550, "seek": 280700, "start": 2818.48, "end": 2822.92, "text": " same direction, then with this very high momentum, they'll move more and more quickly.", "tokens": [50938, 912, 3513, 11, 550, 365, 341, 588, 1090, 11244, 11, 436, 603, 1286, 544, 293, 544, 2661, 13, 51160], "temperature": 0.0, "avg_logprob": -0.21779492593580677, "compression_ratio": 1.878228782287823, "no_speech_prob": 5.3075003961566836e-05}, {"id": 551, "seek": 280700, "start": 2822.92, "end": 2827.0, "text": " But if they keep moving in different directions, it's just the momentum is going to kind of", "tokens": [51160, 583, 498, 436, 1066, 2684, 294, 819, 11095, 11, 309, 311, 445, 264, 11244, 307, 516, 281, 733, 295, 51364], "temperature": 0.0, "avg_logprob": -0.21779492593580677, "compression_ratio": 1.878228782287823, "no_speech_prob": 5.3075003961566836e-05}, {"id": 552, "seek": 280700, "start": 2827.0, "end": 2831.2, "text": " look at the underlying direction they're moving.", "tokens": [51364, 574, 412, 264, 14217, 3513, 436, 434, 2684, 13, 51574], "temperature": 0.0, "avg_logprob": -0.21779492593580677, "compression_ratio": 1.878228782287823, "no_speech_prob": 5.3075003961566836e-05}, {"id": 553, "seek": 280700, "start": 2831.2, "end": 2835.08, "text": " And then once you have got to a good part of the weight space, you can use a very high", "tokens": [51574, 400, 550, 1564, 291, 362, 658, 281, 257, 665, 644, 295, 264, 3364, 1901, 11, 291, 393, 764, 257, 588, 1090, 51768], "temperature": 0.0, "avg_logprob": -0.21779492593580677, "compression_ratio": 1.878228782287823, "no_speech_prob": 5.3075003961566836e-05}, {"id": 554, "seek": 283508, "start": 2835.16, "end": 2836.52, "text": " learning rate.", "tokens": [50368, 2539, 3314, 13, 50436], "temperature": 0.0, "avg_logprob": -0.21545835268699517, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.03209696710109711}, {"id": 555, "seek": 283508, "start": 2836.52, "end": 2839.3199999999997, "text": " And with a very high learning rate, you wouldn't want so much momentum.", "tokens": [50436, 400, 365, 257, 588, 1090, 2539, 3314, 11, 291, 2759, 380, 528, 370, 709, 11244, 13, 50576], "temperature": 0.0, "avg_logprob": -0.21545835268699517, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.03209696710109711}, {"id": 556, "seek": 283508, "start": 2839.3199999999997, "end": 2843.52, "text": " So that's why there's low momentum during the time when there's high learning rate.", "tokens": [50576, 407, 300, 311, 983, 456, 311, 2295, 11244, 1830, 264, 565, 562, 456, 311, 1090, 2539, 3314, 13, 50786], "temperature": 0.0, "avg_logprob": -0.21545835268699517, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.03209696710109711}, {"id": 557, "seek": 283508, "start": 2843.52, "end": 2849.7999999999997, "text": " And then as we saw in our spreadsheet, which did this automatically, as you get closer", "tokens": [50786, 400, 550, 382, 321, 1866, 294, 527, 27733, 11, 597, 630, 341, 6772, 11, 382, 291, 483, 4966, 51100], "temperature": 0.0, "avg_logprob": -0.21545835268699517, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.03209696710109711}, {"id": 558, "seek": 283508, "start": 2849.7999999999997, "end": 2854.6, "text": " to the optimal, you generally want to decrease the learning rate.", "tokens": [51100, 281, 264, 16252, 11, 291, 5101, 528, 281, 11514, 264, 2539, 3314, 13, 51340], "temperature": 0.0, "avg_logprob": -0.21545835268699517, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.03209696710109711}, {"id": 559, "seek": 283508, "start": 2854.6, "end": 2858.48, "text": " And since we're decreasing it, again, we can increase the momentum.", "tokens": [51340, 400, 1670, 321, 434, 23223, 309, 11, 797, 11, 321, 393, 3488, 264, 11244, 13, 51534], "temperature": 0.0, "avg_logprob": -0.21545835268699517, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.03209696710109711}, {"id": 560, "seek": 283508, "start": 2858.48, "end": 2863.48, "text": " So you can see that starting from random weights, we've got a pretty good accuracy on fashion", "tokens": [51534, 407, 291, 393, 536, 300, 2891, 490, 4974, 17443, 11, 321, 600, 658, 257, 1238, 665, 14170, 322, 6700, 51784], "temperature": 0.0, "avg_logprob": -0.21545835268699517, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.03209696710109711}, {"id": 561, "seek": 286348, "start": 2863.48, "end": 2869.52, "text": " MNIST with a totally standard convolutional neural network, no ResNets, nothing else,", "tokens": [50364, 376, 45, 19756, 365, 257, 3879, 3832, 45216, 304, 18161, 3209, 11, 572, 5015, 45, 1385, 11, 1825, 1646, 11, 50666], "temperature": 0.0, "avg_logprob": -0.3327122653823301, "compression_ratio": 1.4228855721393034, "no_speech_prob": 0.0018102035392075777}, {"id": 562, "seek": 286348, "start": 2869.52, "end": 2877.6, "text": " everything built from scratch by hand, artisanal neural network training, and we've got 90.6%", "tokens": [50666, 1203, 3094, 490, 8459, 538, 1011, 11, 1523, 14804, 304, 18161, 3209, 3097, 11, 293, 321, 600, 658, 4289, 13, 21, 4, 51070], "temperature": 0.0, "avg_logprob": -0.3327122653823301, "compression_ratio": 1.4228855721393034, "no_speech_prob": 0.0018102035392075777}, {"id": 563, "seek": 286348, "start": 2877.6, "end": 2880.32, "text": " fashion MNIST.", "tokens": [51070, 6700, 376, 45, 19756, 13, 51206], "temperature": 0.0, "avg_logprob": -0.3327122653823301, "compression_ratio": 1.4228855721393034, "no_speech_prob": 0.0018102035392075777}, {"id": 564, "seek": 286348, "start": 2880.32, "end": 2882.32, "text": " So there you go.", "tokens": [51206, 407, 456, 291, 352, 13, 51306], "temperature": 0.0, "avg_logprob": -0.3327122653823301, "compression_ratio": 1.4228855721393034, "no_speech_prob": 0.0018102035392075777}, {"id": 565, "seek": 286348, "start": 2882.32, "end": 2891.72, "text": " All right, let's take a seven minute break, and I'll see you back shortly.", "tokens": [51306, 1057, 558, 11, 718, 311, 747, 257, 3407, 3456, 1821, 11, 293, 286, 603, 536, 291, 646, 13392, 13, 51776], "temperature": 0.0, "avg_logprob": -0.3327122653823301, "compression_ratio": 1.4228855721393034, "no_speech_prob": 0.0018102035392075777}, {"id": 566, "seek": 289172, "start": 2891.9599999999996, "end": 2899.9599999999996, "text": " I should warn you, you've got a lot more to cover, so I hope you're okay with a long lesson today.", "tokens": [50376, 286, 820, 12286, 291, 11, 291, 600, 658, 257, 688, 544, 281, 2060, 11, 370, 286, 1454, 291, 434, 1392, 365, 257, 938, 6898, 965, 13, 50776], "temperature": 0.0, "avg_logprob": -0.3625178703894982, "compression_ratio": 1.489795918367347, "no_speech_prob": 0.06953706592321396}, {"id": 567, "seek": 289172, "start": 2902.9599999999996, "end": 2906.3199999999997, "text": " Okay, we're back.", "tokens": [50926, 1033, 11, 321, 434, 646, 13, 51094], "temperature": 0.0, "avg_logprob": -0.3625178703894982, "compression_ratio": 1.489795918367347, "no_speech_prob": 0.06953706592321396}, {"id": 568, "seek": 289172, "start": 2906.3199999999997, "end": 2916.2799999999997, "text": " I just wanted to mention also something we skipped over here, which is this hasLearn callback.", "tokens": [51094, 286, 445, 1415, 281, 2152, 611, 746, 321, 30193, 670, 510, 11, 597, 307, 341, 575, 11020, 1083, 818, 3207, 13, 51592], "temperature": 0.0, "avg_logprob": -0.3625178703894982, "compression_ratio": 1.489795918367347, "no_speech_prob": 0.06953706592321396}, {"id": 569, "seek": 289172, "start": 2916.2799999999997, "end": 2919.24, "text": " This is more important for the people doing the live course than the recordings.", "tokens": [51592, 639, 307, 544, 1021, 337, 264, 561, 884, 264, 1621, 1164, 813, 264, 25162, 13, 51740], "temperature": 0.0, "avg_logprob": -0.3625178703894982, "compression_ratio": 1.489795918367347, "no_speech_prob": 0.06953706592321396}, {"id": 570, "seek": 291924, "start": 2919.7599999999998, "end": 2924.16, "text": " If you're doing the recording, you will have already seen this, but since I created Learner,", "tokens": [50390, 759, 291, 434, 884, 264, 6613, 11, 291, 486, 362, 1217, 1612, 341, 11, 457, 1670, 286, 2942, 17216, 260, 11, 50610], "temperature": 0.0, "avg_logprob": -0.3384716551182634, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.023329416289925575}, {"id": 571, "seek": 291924, "start": 2924.16, "end": 2931.9599999999996, "text": " actually Peter Zappler, I don't know how to pronounce your surname, sorry Peter, pointed", "tokens": [50610, 767, 6508, 1176, 1746, 1918, 11, 286, 500, 380, 458, 577, 281, 19567, 428, 50152, 11, 2597, 6508, 11, 10932, 51000], "temperature": 0.0, "avg_logprob": -0.3384716551182634, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.023329416289925575}, {"id": 572, "seek": 291924, "start": 2931.9599999999996, "end": 2938.12, "text": " out that there's actually kind of a nicer way of handling Learner.", "tokens": [51000, 484, 300, 456, 311, 767, 733, 295, 257, 22842, 636, 295, 13175, 17216, 260, 13, 51308], "temperature": 0.0, "avg_logprob": -0.3384716551182634, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.023329416289925575}, {"id": 573, "seek": 291924, "start": 2938.12, "end": 2944.8399999999997, "text": " That previously we were putting the Learner object itself into self.learn in each callback.", "tokens": [51308, 663, 8046, 321, 645, 3372, 264, 17216, 260, 2657, 2564, 666, 2698, 13, 306, 1083, 294, 1184, 818, 3207, 13, 51644], "temperature": 0.0, "avg_logprob": -0.3384716551182634, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.023329416289925575}, {"id": 574, "seek": 291924, "start": 2944.8399999999997, "end": 2949.16, "text": " And that meant we were using self.learn.model and self.learn.opt and self.learn.all, there's", "tokens": [51644, 400, 300, 4140, 321, 645, 1228, 2698, 13, 306, 1083, 13, 8014, 338, 293, 2698, 13, 306, 1083, 13, 5747, 293, 2698, 13, 306, 1083, 13, 336, 11, 456, 311, 51860], "temperature": 0.0, "avg_logprob": -0.3384716551182634, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.023329416289925575}, {"id": 575, "seek": 294916, "start": 2949.16, "end": 2953.12, "text": " you know all over the place, it was kind of ugly.", "tokens": [50364, 291, 458, 439, 670, 264, 1081, 11, 309, 390, 733, 295, 12246, 13, 50562], "temperature": 0.0, "avg_logprob": -0.337115740776062, "compression_ratio": 1.25, "no_speech_prob": 0.002287180395796895}, {"id": 576, "seek": 294916, "start": 2953.12, "end": 2973.44, "text": " So we've modified Learner this week to instead pass in when it calls the callback, when in", "tokens": [50562, 407, 321, 600, 15873, 17216, 260, 341, 1243, 281, 2602, 1320, 294, 562, 309, 5498, 264, 818, 3207, 11, 562, 294, 51578], "temperature": 0.0, "avg_logprob": -0.337115740776062, "compression_ratio": 1.25, "no_speech_prob": 0.002287180395796895}, {"id": 577, "seek": 297344, "start": 2973.44, "end": 2980.88, "text": " runCBs, which is what it calls, Learner calls, you might remember, is it passes the Learner", "tokens": [50364, 1190, 34, 33, 82, 11, 597, 307, 437, 309, 5498, 11, 17216, 260, 5498, 11, 291, 1062, 1604, 11, 307, 309, 11335, 264, 17216, 260, 50736], "temperature": 0.0, "avg_logprob": -0.2641882563746253, "compression_ratio": 1.7457627118644068, "no_speech_prob": 0.10666058212518692}, {"id": 578, "seek": 297344, "start": 2980.88, "end": 2983.68, "text": " as a parameter to the method.", "tokens": [50736, 382, 257, 13075, 281, 264, 3170, 13, 50876], "temperature": 0.0, "avg_logprob": -0.2641882563746253, "compression_ratio": 1.7457627118644068, "no_speech_prob": 0.10666058212518692}, {"id": 579, "seek": 297344, "start": 2983.68, "end": 2992.08, "text": " So now the Learner no longer goes through the callbacks and sets their .learn attribute.", "tokens": [50876, 407, 586, 264, 17216, 260, 572, 2854, 1709, 807, 264, 818, 17758, 293, 6352, 641, 2411, 306, 1083, 19667, 13, 51296], "temperature": 0.0, "avg_logprob": -0.2641882563746253, "compression_ratio": 1.7457627118644068, "no_speech_prob": 0.10666058212518692}, {"id": 580, "seek": 297344, "start": 2992.08, "end": 3001.2000000000003, "text": " But instead in your callbacks you have to put learn as a parameter in all of the callback", "tokens": [51296, 583, 2602, 294, 428, 818, 17758, 291, 362, 281, 829, 1466, 382, 257, 13075, 294, 439, 295, 264, 818, 3207, 51752], "temperature": 0.0, "avg_logprob": -0.2641882563746253, "compression_ratio": 1.7457627118644068, "no_speech_prob": 0.10666058212518692}, {"id": 581, "seek": 297344, "start": 3001.2000000000003, "end": 3002.8, "text": " methods.", "tokens": [51752, 7150, 13, 51832], "temperature": 0.0, "avg_logprob": -0.2641882563746253, "compression_ratio": 1.7457627118644068, "no_speech_prob": 0.10666058212518692}, {"id": 582, "seek": 300280, "start": 3003.1600000000003, "end": 3011.6800000000003, "text": " So for example, deviceCB has a before fit, so now it's got comma learn here.", "tokens": [50382, 407, 337, 1365, 11, 4302, 34, 33, 575, 257, 949, 3318, 11, 370, 586, 309, 311, 658, 22117, 1466, 510, 13, 50808], "temperature": 0.0, "avg_logprob": -0.27510227759679157, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.0005274713621474802}, {"id": 583, "seek": 300280, "start": 3011.6800000000003, "end": 3017.2000000000003, "text": " So now this is not self.learn, it's just learn.", "tokens": [50808, 407, 586, 341, 307, 406, 2698, 13, 306, 1083, 11, 309, 311, 445, 1466, 13, 51084], "temperature": 0.0, "avg_logprob": -0.27510227759679157, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.0005274713621474802}, {"id": 584, "seek": 300280, "start": 3017.2000000000003, "end": 3025.76, "text": " So it does make a lot of the code less yucky to not have all this self.learn.predsql, self.learn.model,", "tokens": [51084, 407, 309, 775, 652, 257, 688, 295, 264, 3089, 1570, 288, 10616, 281, 406, 362, 439, 341, 2698, 13, 306, 1083, 13, 79, 986, 82, 80, 75, 11, 2698, 13, 306, 1083, 13, 8014, 338, 11, 51512], "temperature": 0.0, "avg_logprob": -0.27510227759679157, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.0005274713621474802}, {"id": 585, "seek": 300280, "start": 3025.76, "end": 3028.0800000000004, "text": " self.learn.batch, it's now just learn.", "tokens": [51512, 2698, 13, 306, 1083, 13, 65, 852, 11, 309, 311, 586, 445, 1466, 13, 51628], "temperature": 0.0, "avg_logprob": -0.27510227759679157, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.0005274713621474802}, {"id": 586, "seek": 302808, "start": 3028.08, "end": 3037.52, "text": " It also is good because you don't generally want to have both have the learner has a reference", "tokens": [50364, 467, 611, 307, 665, 570, 291, 500, 380, 5101, 528, 281, 362, 1293, 362, 264, 33347, 575, 257, 6408, 50836], "temperature": 0.0, "avg_logprob": -0.2497579965246729, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.0014778987970203161}, {"id": 587, "seek": 302808, "start": 3037.52, "end": 3043.2, "text": " to the callbacks, but also the callbacks having a reference back to the learner, it creates", "tokens": [50836, 281, 264, 818, 17758, 11, 457, 611, 264, 818, 17758, 1419, 257, 6408, 646, 281, 264, 33347, 11, 309, 7829, 51120], "temperature": 0.0, "avg_logprob": -0.2497579965246729, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.0014778987970203161}, {"id": 588, "seek": 302808, "start": 3043.2, "end": 3049.72, "text": " something called a cycle.", "tokens": [51120, 746, 1219, 257, 6586, 13, 51446], "temperature": 0.0, "avg_logprob": -0.2497579965246729, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.0014778987970203161}, {"id": 589, "seek": 302808, "start": 3049.72, "end": 3053.0, "text": " So there's a couple of benefits there.", "tokens": [51446, 407, 456, 311, 257, 1916, 295, 5311, 456, 13, 51610], "temperature": 0.0, "avg_logprob": -0.2497579965246729, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.0014778987970203161}, {"id": 590, "seek": 302808, "start": 3053.0, "end": 3057.48, "text": " And that reminds me there's a few other little changes we've made to the code.", "tokens": [51610, 400, 300, 12025, 385, 456, 311, 257, 1326, 661, 707, 2962, 321, 600, 1027, 281, 264, 3089, 13, 51834], "temperature": 0.0, "avg_logprob": -0.2497579965246729, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.0014778987970203161}, {"id": 591, "seek": 305748, "start": 3057.88, "end": 3061.48, "text": " And I want to show you a cool little trick.", "tokens": [50384, 400, 286, 528, 281, 855, 291, 257, 1627, 707, 4282, 13, 50564], "temperature": 0.0, "avg_logprob": -0.22518978516260782, "compression_ratio": 1.7512437810945274, "no_speech_prob": 0.0016229600878432393}, {"id": 592, "seek": 305748, "start": 3061.48, "end": 3066.36, "text": " I want to show you a cool little trick for how I'm going to find quickly all of the changes", "tokens": [50564, 286, 528, 281, 855, 291, 257, 1627, 707, 4282, 337, 577, 286, 478, 516, 281, 915, 2661, 439, 295, 264, 2962, 50808], "temperature": 0.0, "avg_logprob": -0.22518978516260782, "compression_ratio": 1.7512437810945274, "no_speech_prob": 0.0016229600878432393}, {"id": 593, "seek": 305748, "start": 3066.36, "end": 3070.52, "text": " that we've made to the code in the last week.", "tokens": [50808, 300, 321, 600, 1027, 281, 264, 3089, 294, 264, 1036, 1243, 13, 51016], "temperature": 0.0, "avg_logprob": -0.22518978516260782, "compression_ratio": 1.7512437810945274, "no_speech_prob": 0.0016229600878432393}, {"id": 594, "seek": 305748, "start": 3070.52, "end": 3077.48, "text": " So to do that we can go to the course repo, and on any repo you can add slash compare", "tokens": [51016, 407, 281, 360, 300, 321, 393, 352, 281, 264, 1164, 49040, 11, 293, 322, 604, 49040, 291, 393, 909, 17330, 6794, 51364], "temperature": 0.0, "avg_logprob": -0.22518978516260782, "compression_ratio": 1.7512437810945274, "no_speech_prob": 0.0016229600878432393}, {"id": 595, "seek": 305748, "start": 3077.48, "end": 3084.76, "text": " in GitHub, and then you can compare across, you know, all kinds of different things.", "tokens": [51364, 294, 23331, 11, 293, 550, 291, 393, 6794, 2108, 11, 291, 458, 11, 439, 3685, 295, 819, 721, 13, 51728], "temperature": 0.0, "avg_logprob": -0.22518978516260782, "compression_ratio": 1.7512437810945274, "no_speech_prob": 0.0016229600878432393}, {"id": 596, "seek": 308476, "start": 3084.88, "end": 3088.8, "text": " One of the examples they've got here is to compare across different times.", "tokens": [50370, 1485, 295, 264, 5110, 436, 600, 658, 510, 307, 281, 6794, 2108, 819, 1413, 13, 50566], "temperature": 0.0, "avg_logprob": -0.24622398447767596, "compression_ratio": 1.707112970711297, "no_speech_prob": 0.03514418005943298}, {"id": 597, "seek": 308476, "start": 3088.8, "end": 3092.28, "text": " Look at the master branch now versus one day ago.", "tokens": [50566, 2053, 412, 264, 4505, 9819, 586, 5717, 472, 786, 2057, 13, 50740], "temperature": 0.0, "avg_logprob": -0.24622398447767596, "compression_ratio": 1.707112970711297, "no_speech_prob": 0.03514418005943298}, {"id": 598, "seek": 308476, "start": 3092.28, "end": 3096.0400000000004, "text": " So I actually want the master branch now versus seven days ago.", "tokens": [50740, 407, 286, 767, 528, 264, 4505, 9819, 586, 5717, 3407, 1708, 2057, 13, 50928], "temperature": 0.0, "avg_logprob": -0.24622398447767596, "compression_ratio": 1.707112970711297, "no_speech_prob": 0.03514418005943298}, {"id": 599, "seek": 308476, "start": 3096.0400000000004, "end": 3101.6000000000004, "text": " So I just hit this, change this to seven.", "tokens": [50928, 407, 286, 445, 2045, 341, 11, 1319, 341, 281, 3407, 13, 51206], "temperature": 0.0, "avg_logprob": -0.24622398447767596, "compression_ratio": 1.707112970711297, "no_speech_prob": 0.03514418005943298}, {"id": 600, "seek": 308476, "start": 3101.6000000000004, "end": 3102.6000000000004, "text": " And there we go.", "tokens": [51206, 400, 456, 321, 352, 13, 51256], "temperature": 0.0, "avg_logprob": -0.24622398447767596, "compression_ratio": 1.707112970711297, "no_speech_prob": 0.03514418005943298}, {"id": 601, "seek": 308476, "start": 3102.6000000000004, "end": 3109.5600000000004, "text": " There's all my commits, and I can immediately see the changes from last week.", "tokens": [51256, 821, 311, 439, 452, 48311, 11, 293, 286, 393, 4258, 536, 264, 2962, 490, 1036, 1243, 13, 51604], "temperature": 0.0, "avg_logprob": -0.24622398447767596, "compression_ratio": 1.707112970711297, "no_speech_prob": 0.03514418005943298}, {"id": 602, "seek": 308476, "start": 3109.5600000000004, "end": 3113.2400000000002, "text": " And so you can basically see what are the things I had to do when I change things.", "tokens": [51604, 400, 370, 291, 393, 1936, 536, 437, 366, 264, 721, 286, 632, 281, 360, 562, 286, 1319, 721, 13, 51788], "temperature": 0.0, "avg_logprob": -0.24622398447767596, "compression_ratio": 1.707112970711297, "no_speech_prob": 0.03514418005943298}, {"id": 603, "seek": 311324, "start": 3113.24, "end": 3120.7999999999997, "text": " So for example you can see here all of my self.learns became learns.", "tokens": [50364, 407, 337, 1365, 291, 393, 536, 510, 439, 295, 452, 2698, 13, 306, 1083, 82, 3062, 27152, 13, 50742], "temperature": 0.0, "avg_logprob": -0.37139562658361486, "compression_ratio": 1.4, "no_speech_prob": 0.020963285118341446}, {"id": 604, "seek": 311324, "start": 3120.7999999999997, "end": 3130.2, "text": " I added the YANILI, that's right, I added my augmentation.", "tokens": [50742, 286, 3869, 264, 398, 1770, 4620, 40, 11, 300, 311, 558, 11, 286, 3869, 452, 14501, 19631, 13, 51212], "temperature": 0.0, "avg_logprob": -0.37139562658361486, "compression_ratio": 1.4, "no_speech_prob": 0.020963285118341446}, {"id": 605, "seek": 311324, "start": 3130.2, "end": 3132.2799999999997, "text": " And so in learner I added an lrfind.", "tokens": [51212, 400, 370, 294, 33347, 286, 3869, 364, 287, 81, 35072, 13, 51316], "temperature": 0.0, "avg_logprob": -0.37139562658361486, "compression_ratio": 1.4, "no_speech_prob": 0.020963285118341446}, {"id": 606, "seek": 311324, "start": 3132.2799999999997, "end": 3134.24, "text": " Ah yes, I will show you that one.", "tokens": [51316, 2438, 2086, 11, 286, 486, 855, 291, 300, 472, 13, 51414], "temperature": 0.0, "avg_logprob": -0.37139562658361486, "compression_ratio": 1.4, "no_speech_prob": 0.020963285118341446}, {"id": 607, "seek": 311324, "start": 3134.24, "end": 3136.52, "text": " That's pretty fun.", "tokens": [51414, 663, 311, 1238, 1019, 13, 51528], "temperature": 0.0, "avg_logprob": -0.37139562658361486, "compression_ratio": 1.4, "no_speech_prob": 0.020963285118341446}, {"id": 608, "seek": 313652, "start": 3136.52, "end": 3143.96, "text": " So here's the changes we made to runcbs, to fit.", "tokens": [50364, 407, 510, 311, 264, 2962, 321, 1027, 281, 1190, 66, 929, 11, 281, 3318, 13, 50736], "temperature": 0.0, "avg_logprob": -0.28167925940619576, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.009125306271016598}, {"id": 609, "seek": 313652, "start": 3143.96, "end": 3149.2, "text": " So this is a nice way I can quickly, yeah, find out what I've changed since last time,", "tokens": [50736, 407, 341, 307, 257, 1481, 636, 286, 393, 2661, 11, 1338, 11, 915, 484, 437, 286, 600, 3105, 1670, 1036, 565, 11, 50998], "temperature": 0.0, "avg_logprob": -0.28167925940619576, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.009125306271016598}, {"id": 610, "seek": 313652, "start": 3149.2, "end": 3152.28, "text": " and make sure that I don't forget to tell you folks about any of them.", "tokens": [50998, 293, 652, 988, 300, 286, 500, 380, 2870, 281, 980, 291, 4024, 466, 604, 295, 552, 13, 51152], "temperature": 0.0, "avg_logprob": -0.28167925940619576, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.009125306271016598}, {"id": 611, "seek": 313652, "start": 3152.28, "end": 3157.12, "text": " Oh yes, clean up fit, I have to tell you about that as well.", "tokens": [51152, 876, 2086, 11, 2541, 493, 3318, 11, 286, 362, 281, 980, 291, 466, 300, 382, 731, 13, 51394], "temperature": 0.0, "avg_logprob": -0.28167925940619576, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.009125306271016598}, {"id": 612, "seek": 313652, "start": 3157.12, "end": 3160.08, "text": " Okay that's a useful reminder.", "tokens": [51394, 1033, 300, 311, 257, 4420, 13548, 13, 51542], "temperature": 0.0, "avg_logprob": -0.28167925940619576, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.009125306271016598}, {"id": 613, "seek": 316008, "start": 3160.08, "end": 3169.4, "text": " So the main other change to mention is that calling the learning rate finder is now easier,", "tokens": [50364, 407, 264, 2135, 661, 1319, 281, 2152, 307, 300, 5141, 264, 2539, 3314, 915, 260, 307, 586, 3571, 11, 50830], "temperature": 0.0, "avg_logprob": -0.23542497096917567, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.002018990693613887}, {"id": 614, "seek": 316008, "start": 3169.4, "end": 3174.92, "text": " because I added what's called a patch to the learner.", "tokens": [50830, 570, 286, 3869, 437, 311, 1219, 257, 9972, 281, 264, 33347, 13, 51106], "temperature": 0.0, "avg_logprob": -0.23542497096917567, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.002018990693613887}, {"id": 615, "seek": 316008, "start": 3174.92, "end": 3183.12, "text": " FastCoursePatchDecorator lets you take a function, and it will turn that function into a method", "tokens": [51106, 15968, 34, 13656, 47, 852, 35, 3045, 284, 1639, 6653, 291, 747, 257, 2445, 11, 293, 309, 486, 1261, 300, 2445, 666, 257, 3170, 51516], "temperature": 0.0, "avg_logprob": -0.23542497096917567, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.002018990693613887}, {"id": 616, "seek": 316008, "start": 3183.12, "end": 3188.02, "text": " of this class, of whatever class you put after the colon.", "tokens": [51516, 295, 341, 1508, 11, 295, 2035, 1508, 291, 829, 934, 264, 8255, 13, 51761], "temperature": 0.0, "avg_logprob": -0.23542497096917567, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.002018990693613887}, {"id": 617, "seek": 318802, "start": 3188.02, "end": 3195.14, "text": " So this has created a new method called lrfind, or learner.lrfind.", "tokens": [50364, 407, 341, 575, 2942, 257, 777, 3170, 1219, 287, 81, 35072, 11, 420, 33347, 13, 40987, 35072, 13, 50720], "temperature": 0.0, "avg_logprob": -0.2360282248639046, "compression_ratio": 1.7135678391959799, "no_speech_prob": 0.02128659188747406}, {"id": 618, "seek": 318802, "start": 3195.14, "end": 3203.9, "text": " And what it does is it calls self.fit, where self is a learner, passing in however many", "tokens": [50720, 400, 437, 309, 775, 307, 309, 5498, 2698, 13, 6845, 11, 689, 2698, 307, 257, 33347, 11, 8437, 294, 4461, 867, 51158], "temperature": 0.0, "avg_logprob": -0.2360282248639046, "compression_ratio": 1.7135678391959799, "no_speech_prob": 0.02128659188747406}, {"id": 619, "seek": 318802, "start": 3203.9, "end": 3207.78, "text": " epochs you set as the maximum you want to check for your learning rate finder, what", "tokens": [51158, 30992, 28346, 291, 992, 382, 264, 6674, 291, 528, 281, 1520, 337, 428, 2539, 3314, 915, 260, 11, 437, 51352], "temperature": 0.0, "avg_logprob": -0.2360282248639046, "compression_ratio": 1.7135678391959799, "no_speech_prob": 0.02128659188747406}, {"id": 620, "seek": 318802, "start": 3207.78, "end": 3215.18, "text": " to start the learning rate at, and then it says to use as callbacks the learning rate", "tokens": [51352, 281, 722, 264, 2539, 3314, 412, 11, 293, 550, 309, 1619, 281, 764, 382, 818, 17758, 264, 2539, 3314, 51722], "temperature": 0.0, "avg_logprob": -0.2360282248639046, "compression_ratio": 1.7135678391959799, "no_speech_prob": 0.02128659188747406}, {"id": 621, "seek": 318802, "start": 3215.18, "end": 3216.84, "text": " finder callback.", "tokens": [51722, 915, 260, 818, 3207, 13, 51805], "temperature": 0.0, "avg_logprob": -0.2360282248639046, "compression_ratio": 1.7135678391959799, "no_speech_prob": 0.02128659188747406}, {"id": 622, "seek": 321684, "start": 3216.84, "end": 3225.6800000000003, "text": " Now this is new as well, self.learn.fit didn't used to have a callbacks parameter.", "tokens": [50364, 823, 341, 307, 777, 382, 731, 11, 2698, 13, 306, 1083, 13, 6845, 994, 380, 1143, 281, 362, 257, 818, 17758, 13075, 13, 50806], "temperature": 0.0, "avg_logprob": -0.22265813661658246, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0003101557958871126}, {"id": 623, "seek": 321684, "start": 3225.6800000000003, "end": 3231.1000000000004, "text": " So that's very convenient, because what it does is it adds those callbacks just during", "tokens": [50806, 407, 300, 311, 588, 10851, 11, 570, 437, 309, 775, 307, 309, 10860, 729, 818, 17758, 445, 1830, 51077], "temperature": 0.0, "avg_logprob": -0.22265813661658246, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0003101557958871126}, {"id": 624, "seek": 321684, "start": 3231.1000000000004, "end": 3232.1800000000003, "text": " the fit.", "tokens": [51077, 264, 3318, 13, 51131], "temperature": 0.0, "avg_logprob": -0.22265813661658246, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0003101557958871126}, {"id": 625, "seek": 321684, "start": 3232.1800000000003, "end": 3241.1000000000004, "text": " So if you pass in callbacks, then it goes through each one and appends it to self.cbs,", "tokens": [51131, 407, 498, 291, 1320, 294, 818, 17758, 11, 550, 309, 1709, 807, 1184, 472, 293, 724, 2581, 309, 281, 2698, 13, 66, 929, 11, 51577], "temperature": 0.0, "avg_logprob": -0.22265813661658246, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0003101557958871126}, {"id": 626, "seek": 321684, "start": 3241.1000000000004, "end": 3244.02, "text": " and when it's finished fitting, it removes them again.", "tokens": [51577, 293, 562, 309, 311, 4335, 15669, 11, 309, 30445, 552, 797, 13, 51723], "temperature": 0.0, "avg_logprob": -0.22265813661658246, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0003101557958871126}, {"id": 627, "seek": 324402, "start": 3244.2, "end": 3247.8, "text": " So these are callbacks that are just added for the period of this one fit, which is what", "tokens": [50373, 407, 613, 366, 818, 17758, 300, 366, 445, 3869, 337, 264, 2896, 295, 341, 472, 3318, 11, 597, 307, 437, 50553], "temperature": 0.0, "avg_logprob": -0.25865028899850195, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0010817102156579494}, {"id": 628, "seek": 324402, "start": 3247.8, "end": 3249.52, "text": " we want for a learning rate finder.", "tokens": [50553, 321, 528, 337, 257, 2539, 3314, 915, 260, 13, 50639], "temperature": 0.0, "avg_logprob": -0.25865028899850195, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0010817102156579494}, {"id": 629, "seek": 324402, "start": 3249.52, "end": 3253.06, "text": " It should just be added for that one fit.", "tokens": [50639, 467, 820, 445, 312, 3869, 337, 300, 472, 3318, 13, 50816], "temperature": 0.0, "avg_logprob": -0.25865028899850195, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0010817102156579494}, {"id": 630, "seek": 324402, "start": 3253.06, "end": 3258.24, "text": " So with this patch in place, it says this is all that's required to do the learning", "tokens": [50816, 407, 365, 341, 9972, 294, 1081, 11, 309, 1619, 341, 307, 439, 300, 311, 4739, 281, 360, 264, 2539, 51075], "temperature": 0.0, "avg_logprob": -0.25865028899850195, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0010817102156579494}, {"id": 631, "seek": 324402, "start": 3258.24, "end": 3264.98, "text": " rate finder, is now to create your learner and call.lrfind, and there you go, bang.", "tokens": [51075, 3314, 915, 260, 11, 307, 586, 281, 1884, 428, 33347, 293, 818, 13, 40987, 35072, 11, 293, 456, 291, 352, 11, 8550, 13, 51412], "temperature": 0.0, "avg_logprob": -0.25865028899850195, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0010817102156579494}, {"id": 632, "seek": 324402, "start": 3264.98, "end": 3267.9, "text": " So patch is a very convenient thing.", "tokens": [51412, 407, 9972, 307, 257, 588, 10851, 551, 13, 51558], "temperature": 0.0, "avg_logprob": -0.25865028899850195, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0010817102156579494}, {"id": 633, "seek": 326790, "start": 3267.9, "end": 3275.88, "text": " It's one of these things which, you know, Python has a lot of kind of like folk wisdom", "tokens": [50364, 467, 311, 472, 295, 613, 721, 597, 11, 291, 458, 11, 15329, 575, 257, 688, 295, 733, 295, 411, 15748, 10712, 50763], "temperature": 0.0, "avg_logprob": -0.2596951173932365, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.05920802056789398}, {"id": 634, "seek": 326790, "start": 3275.88, "end": 3284.6, "text": " about what isn't considered Pythonic or good, and a lot of people really don't like.", "tokens": [50763, 466, 437, 1943, 380, 4888, 9953, 392, 11630, 420, 665, 11, 293, 257, 688, 295, 561, 534, 500, 380, 411, 13, 51199], "temperature": 0.0, "avg_logprob": -0.2596951173932365, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.05920802056789398}, {"id": 635, "seek": 326790, "start": 3284.6, "end": 3289.2400000000002, "text": " Patching in other languages, it's used very widely and is considered very good.", "tokens": [51199, 430, 29569, 294, 661, 8650, 11, 309, 311, 1143, 588, 13371, 293, 307, 4888, 588, 665, 13, 51431], "temperature": 0.0, "avg_logprob": -0.2596951173932365, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.05920802056789398}, {"id": 636, "seek": 326790, "start": 3289.2400000000002, "end": 3294.92, "text": " So I don't tend to have strong opinions either way about what's good or what's bad.", "tokens": [51431, 407, 286, 500, 380, 3928, 281, 362, 2068, 11819, 2139, 636, 466, 437, 311, 665, 420, 437, 311, 1578, 13, 51715], "temperature": 0.0, "avg_logprob": -0.2596951173932365, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.05920802056789398}, {"id": 637, "seek": 329492, "start": 3294.94, "end": 3299.7400000000002, "text": " In fact, instead I just decide, you know, figure out what's useful in a particular situation.", "tokens": [50365, 682, 1186, 11, 2602, 286, 445, 4536, 11, 291, 458, 11, 2573, 484, 437, 311, 4420, 294, 257, 1729, 2590, 13, 50605], "temperature": 0.0, "avg_logprob": -0.2526971799833281, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.00015356214134953916}, {"id": 638, "seek": 329492, "start": 3299.7400000000002, "end": 3304.78, "text": " So in this situation, obviously it's very nice to be able to add in this additional", "tokens": [50605, 407, 294, 341, 2590, 11, 2745, 309, 311, 588, 1481, 281, 312, 1075, 281, 909, 294, 341, 4497, 50857], "temperature": 0.0, "avg_logprob": -0.2526971799833281, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.00015356214134953916}, {"id": 639, "seek": 329492, "start": 3304.78, "end": 3307.62, "text": " functionality to our class.", "tokens": [50857, 14980, 281, 527, 1508, 13, 50999], "temperature": 0.0, "avg_logprob": -0.2526971799833281, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.00015356214134953916}, {"id": 640, "seek": 329492, "start": 3307.62, "end": 3311.14, "text": " So that's what lrfind is.", "tokens": [50999, 407, 300, 311, 437, 287, 81, 35072, 307, 13, 51175], "temperature": 0.0, "avg_logprob": -0.2526971799833281, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.00015356214134953916}, {"id": 641, "seek": 329492, "start": 3311.14, "end": 3315.84, "text": " And then the only other thing we added to the learner this week was we added a few more", "tokens": [51175, 400, 550, 264, 787, 661, 551, 321, 3869, 281, 264, 33347, 341, 1243, 390, 321, 3869, 257, 1326, 544, 51410], "temperature": 0.0, "avg_logprob": -0.2526971799833281, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.00015356214134953916}, {"id": 642, "seek": 329492, "start": 3315.84, "end": 3317.5, "text": " parameters to fit.", "tokens": [51410, 9834, 281, 3318, 13, 51493], "temperature": 0.0, "avg_logprob": -0.2526971799833281, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.00015356214134953916}, {"id": 643, "seek": 329492, "start": 3317.5, "end": 3322.3, "text": " Fit used to just take the number of epochs, as well as the callbacks parameter.", "tokens": [51493, 29263, 1143, 281, 445, 747, 264, 1230, 295, 30992, 28346, 11, 382, 731, 382, 264, 818, 17758, 13075, 13, 51733], "temperature": 0.0, "avg_logprob": -0.2526971799833281, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.00015356214134953916}, {"id": 644, "seek": 332230, "start": 3322.3, "end": 3326.7200000000003, "text": " It now also has a learning rate parameter, and so you've always been able to provide", "tokens": [50364, 467, 586, 611, 575, 257, 2539, 3314, 13075, 11, 293, 370, 291, 600, 1009, 668, 1075, 281, 2893, 50585], "temperature": 0.0, "avg_logprob": -0.21895564043963398, "compression_ratio": 1.95260663507109, "no_speech_prob": 0.0012842948781326413}, {"id": 645, "seek": 332230, "start": 3326.7200000000003, "end": 3334.44, "text": " a learning rate to the constructor, but you can override the learning rate for one fit.", "tokens": [50585, 257, 2539, 3314, 281, 264, 47479, 11, 457, 291, 393, 42321, 264, 2539, 3314, 337, 472, 3318, 13, 50971], "temperature": 0.0, "avg_logprob": -0.21895564043963398, "compression_ratio": 1.95260663507109, "no_speech_prob": 0.0012842948781326413}, {"id": 646, "seek": 332230, "start": 3334.44, "end": 3340.28, "text": " So if you pass in the learning rate, it will use it, if you pass it in.", "tokens": [50971, 407, 498, 291, 1320, 294, 264, 2539, 3314, 11, 309, 486, 764, 309, 11, 498, 291, 1320, 309, 294, 13, 51263], "temperature": 0.0, "avg_logprob": -0.21895564043963398, "compression_ratio": 1.95260663507109, "no_speech_prob": 0.0012842948781326413}, {"id": 647, "seek": 332230, "start": 3340.28, "end": 3345.0800000000004, "text": " And if you don't, it'll use the learning rate passed into the constructor.", "tokens": [51263, 400, 498, 291, 500, 380, 11, 309, 603, 764, 264, 2539, 3314, 4678, 666, 264, 47479, 13, 51503], "temperature": 0.0, "avg_logprob": -0.21895564043963398, "compression_ratio": 1.95260663507109, "no_speech_prob": 0.0012842948781326413}, {"id": 648, "seek": 332230, "start": 3345.0800000000004, "end": 3351.2400000000002, "text": " And then I also added these two booleans to say when you fit, do you want to do the training", "tokens": [51503, 400, 550, 286, 611, 3869, 613, 732, 748, 4812, 599, 281, 584, 562, 291, 3318, 11, 360, 291, 528, 281, 360, 264, 3097, 51811], "temperature": 0.0, "avg_logprob": -0.21895564043963398, "compression_ratio": 1.95260663507109, "no_speech_prob": 0.0012842948781326413}, {"id": 649, "seek": 335124, "start": 3351.2599999999998, "end": 3353.2999999999997, "text": " loop, and do you want to do the validation loop.", "tokens": [50365, 6367, 11, 293, 360, 291, 528, 281, 360, 264, 24071, 6367, 13, 50467], "temperature": 0.0, "avg_logprob": -0.24175256039915966, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0018675505416467786}, {"id": 650, "seek": 335124, "start": 3353.2999999999997, "end": 3355.4599999999996, "text": " So by default it'll do both.", "tokens": [50467, 407, 538, 7576, 309, 603, 360, 1293, 13, 50575], "temperature": 0.0, "avg_logprob": -0.24175256039915966, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0018675505416467786}, {"id": 651, "seek": 335124, "start": 3355.4599999999996, "end": 3359.14, "text": " And you can see here there's just an if train, do the training loop.", "tokens": [50575, 400, 291, 393, 536, 510, 456, 311, 445, 364, 498, 3847, 11, 360, 264, 3097, 6367, 13, 50759], "temperature": 0.0, "avg_logprob": -0.24175256039915966, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0018675505416467786}, {"id": 652, "seek": 335124, "start": 3359.14, "end": 3363.8999999999996, "text": " If valid, do the validation loop.", "tokens": [50759, 759, 7363, 11, 360, 264, 24071, 6367, 13, 50997], "temperature": 0.0, "avg_logprob": -0.24175256039915966, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0018675505416467786}, {"id": 653, "seek": 335124, "start": 3363.8999999999996, "end": 3368.72, "text": " I'm not even going to talk about this, but if you're interested in testing your understanding", "tokens": [50997, 286, 478, 406, 754, 516, 281, 751, 466, 341, 11, 457, 498, 291, 434, 3102, 294, 4997, 428, 3701, 51238], "temperature": 0.0, "avg_logprob": -0.24175256039915966, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0018675505416467786}, {"id": 654, "seek": 335124, "start": 3368.72, "end": 3374.4799999999996, "text": " of decorators, you might want to think about why it is that I didn't have to say with torch.nograd,", "tokens": [51238, 295, 7919, 3391, 11, 291, 1062, 528, 281, 519, 466, 983, 309, 307, 300, 286, 994, 380, 362, 281, 584, 365, 27822, 13, 77, 664, 6206, 11, 51526], "temperature": 0.0, "avg_logprob": -0.24175256039915966, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0018675505416467786}, {"id": 655, "seek": 335124, "start": 3374.4799999999996, "end": 3379.24, "text": " but instead I called torch.nograd parentheses function.", "tokens": [51526, 457, 2602, 286, 1219, 27822, 13, 77, 664, 6206, 34153, 2445, 13, 51764], "temperature": 0.0, "avg_logprob": -0.24175256039915966, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0018675505416467786}, {"id": 656, "seek": 337924, "start": 3379.24, "end": 3382.8399999999997, "text": " That will be a very, if you can get to a point that you understand why that works and", "tokens": [50364, 663, 486, 312, 257, 588, 11, 498, 291, 393, 483, 281, 257, 935, 300, 291, 1223, 983, 300, 1985, 293, 50544], "temperature": 0.0, "avg_logprob": -0.3909619854342553, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.019718796014785767}, {"id": 657, "seek": 337924, "start": 3382.8399999999997, "end": 3388.24, "text": " what it does, you'll be on your way to understanding decorators better.", "tokens": [50544, 437, 309, 775, 11, 291, 603, 312, 322, 428, 636, 281, 3701, 7919, 3391, 1101, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3909619854342553, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.019718796014785767}, {"id": 658, "seek": 337924, "start": 3388.24, "end": 3398.04, "text": " Okay, so that is the end of Excel SGD.", "tokens": [50814, 1033, 11, 370, 300, 307, 264, 917, 295, 19060, 34520, 35, 13, 51304], "temperature": 0.0, "avg_logprob": -0.3909619854342553, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.019718796014785767}, {"id": 659, "seek": 337924, "start": 3398.04, "end": 3402.2, "text": " ResNets.", "tokens": [51304, 5015, 45, 1385, 13, 51512], "temperature": 0.0, "avg_logprob": -0.3909619854342553, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.019718796014785767}, {"id": 660, "seek": 340220, "start": 3402.2, "end": 3409.2, "text": " Okay so we are up to 90 point, what was it, 3%?", "tokens": [50364, 1033, 370, 321, 366, 493, 281, 4289, 935, 11, 437, 390, 309, 11, 805, 4, 30, 50714], "temperature": 0.0, "avg_logprob": -0.37213925754322724, "compression_ratio": 1.321917808219178, "no_speech_prob": 0.23649930953979492}, {"id": 661, "seek": 340220, "start": 3409.2, "end": 3411.2, "text": " Let's keep track of this.", "tokens": [50714, 961, 311, 1066, 2837, 295, 341, 13, 50814], "temperature": 0.0, "avg_logprob": -0.37213925754322724, "compression_ratio": 1.321917808219178, "no_speech_prob": 0.23649930953979492}, {"id": 662, "seek": 340220, "start": 3411.2, "end": 3418.3599999999997, "text": " Oh yeah, 90.6% is what we're up to.", "tokens": [50814, 876, 1338, 11, 4289, 13, 21, 4, 307, 437, 321, 434, 493, 281, 13, 51172], "temperature": 0.0, "avg_logprob": -0.37213925754322724, "compression_ratio": 1.321917808219178, "no_speech_prob": 0.23649930953979492}, {"id": 663, "seek": 340220, "start": 3418.3599999999997, "end": 3429.8799999999997, "text": " Okay so to remind you the model, actually so we're going to open 13 ResNet now, and", "tokens": [51172, 1033, 370, 281, 4160, 291, 264, 2316, 11, 767, 370, 321, 434, 516, 281, 1269, 3705, 5015, 31890, 586, 11, 293, 51748], "temperature": 0.0, "avg_logprob": -0.37213925754322724, "compression_ratio": 1.321917808219178, "no_speech_prob": 0.23649930953979492}, {"id": 664, "seek": 342988, "start": 3429.88, "end": 3436.44, "text": " we're going to do the usual import and setup initially.", "tokens": [50364, 321, 434, 516, 281, 360, 264, 7713, 974, 293, 8657, 9105, 13, 50692], "temperature": 0.0, "avg_logprob": -0.27171571631180613, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.11595643311738968}, {"id": 665, "seek": 342988, "start": 3436.44, "end": 3444.52, "text": " And the model that we've been using is the same one we've been using for a while, which", "tokens": [50692, 400, 264, 2316, 300, 321, 600, 668, 1228, 307, 264, 912, 472, 321, 600, 668, 1228, 337, 257, 1339, 11, 597, 51096], "temperature": 0.0, "avg_logprob": -0.27171571631180613, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.11595643311738968}, {"id": 666, "seek": 342988, "start": 3444.52, "end": 3455.04, "text": " is that it's a convolution and an activation and an optimal optional batch norm.", "tokens": [51096, 307, 300, 309, 311, 257, 45216, 293, 364, 24433, 293, 364, 16252, 17312, 15245, 2026, 13, 51622], "temperature": 0.0, "avg_logprob": -0.27171571631180613, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.11595643311738968}, {"id": 667, "seek": 345504, "start": 3455.04, "end": 3464.4, "text": " And in our models we were using batch norm and applying our weight initialization, the", "tokens": [50364, 400, 294, 527, 5245, 321, 645, 1228, 15245, 2026, 293, 9275, 527, 3364, 5883, 2144, 11, 264, 50832], "temperature": 0.0, "avg_logprob": -0.3055591950049767, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.134768545627594}, {"id": 668, "seek": 345504, "start": 3464.4, "end": 3467.12, "text": " kai-ming weight initialization.", "tokens": [50832, 350, 1301, 12, 2810, 3364, 5883, 2144, 13, 50968], "temperature": 0.0, "avg_logprob": -0.3055591950049767, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.134768545627594}, {"id": 669, "seek": 345504, "start": 3467.12, "end": 3473.24, "text": " And then we've got comms that take the channels from 1 to 8 to 16 to 32 to 64, and each one's", "tokens": [50968, 400, 550, 321, 600, 658, 800, 82, 300, 747, 264, 9235, 490, 502, 281, 1649, 281, 3165, 281, 8858, 281, 12145, 11, 293, 1184, 472, 311, 51274], "temperature": 0.0, "avg_logprob": -0.3055591950049767, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.134768545627594}, {"id": 670, "seek": 345504, "start": 3473.24, "end": 3475.6, "text": " stride 2.", "tokens": [51274, 1056, 482, 568, 13, 51392], "temperature": 0.0, "avg_logprob": -0.3055591950049767, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.134768545627594}, {"id": 671, "seek": 345504, "start": 3475.6, "end": 3477.5, "text": " And at the end we then do a flatten.", "tokens": [51392, 400, 412, 264, 917, 321, 550, 360, 257, 24183, 13, 51487], "temperature": 0.0, "avg_logprob": -0.3055591950049767, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.134768545627594}, {"id": 672, "seek": 345504, "start": 3477.5, "end": 3483.0, "text": " And so that ended up with a 1 by 1, so that's been the model we've been using for a while.", "tokens": [51487, 400, 370, 300, 4590, 493, 365, 257, 502, 538, 502, 11, 370, 300, 311, 668, 264, 2316, 321, 600, 668, 1228, 337, 257, 1339, 13, 51762], "temperature": 0.0, "avg_logprob": -0.3055591950049767, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.134768545627594}, {"id": 673, "seek": 348300, "start": 3483.0, "end": 3492.86, "text": " So the number of layers is 1, 2, 3, 4.", "tokens": [50364, 407, 264, 1230, 295, 7914, 307, 502, 11, 568, 11, 805, 11, 1017, 13, 50857], "temperature": 0.0, "avg_logprob": -0.2813121738718517, "compression_ratio": 1.35, "no_speech_prob": 0.002148924395442009}, {"id": 674, "seek": 348300, "start": 3492.86, "end": 3500.7, "text": " So 4, 4 convolutional layers with a maximum of 64 channels in the last one.", "tokens": [50857, 407, 1017, 11, 1017, 45216, 304, 7914, 365, 257, 6674, 295, 12145, 9235, 294, 264, 1036, 472, 13, 51249], "temperature": 0.0, "avg_logprob": -0.2813121738718517, "compression_ratio": 1.35, "no_speech_prob": 0.002148924395442009}, {"id": 675, "seek": 348300, "start": 3500.7, "end": 3511.22, "text": " So can we beat 90 point, no, about 90 and a half, 90.6, can we beat 90.6%?", "tokens": [51249, 407, 393, 321, 4224, 4289, 935, 11, 572, 11, 466, 4289, 293, 257, 1922, 11, 4289, 13, 21, 11, 393, 321, 4224, 4289, 13, 21, 4, 30, 51775], "temperature": 0.0, "avg_logprob": -0.2813121738718517, "compression_ratio": 1.35, "no_speech_prob": 0.002148924395442009}, {"id": 676, "seek": 351122, "start": 3511.22, "end": 3515.2599999999998, "text": " So before we do a ResNet I thought well let's just see if we can improve the architecture", "tokens": [50364, 407, 949, 321, 360, 257, 5015, 31890, 286, 1194, 731, 718, 311, 445, 536, 498, 321, 393, 3470, 264, 9482, 50566], "temperature": 0.0, "avg_logprob": -0.23479413382614714, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0030277990736067295}, {"id": 677, "seek": 351122, "start": 3515.2599999999998, "end": 3516.7999999999997, "text": " thoughtfully.", "tokens": [50566, 1194, 2277, 13, 50643], "temperature": 0.0, "avg_logprob": -0.23479413382614714, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0030277990736067295}, {"id": 678, "seek": 351122, "start": 3516.7999999999997, "end": 3524.98, "text": " So generally speaking more depth and more channels gives the neural net more opportunity", "tokens": [50643, 407, 5101, 4124, 544, 7161, 293, 544, 9235, 2709, 264, 18161, 2533, 544, 2650, 51052], "temperature": 0.0, "avg_logprob": -0.23479413382614714, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0030277990736067295}, {"id": 679, "seek": 351122, "start": 3524.98, "end": 3525.98, "text": " to learn.", "tokens": [51052, 281, 1466, 13, 51102], "temperature": 0.0, "avg_logprob": -0.23479413382614714, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0030277990736067295}, {"id": 680, "seek": 351122, "start": 3525.98, "end": 3530.1, "text": " And since we're pretty good at initializing our neural nets and using batch norm, we should", "tokens": [51102, 400, 1670, 321, 434, 1238, 665, 412, 5883, 3319, 527, 18161, 36170, 293, 1228, 15245, 2026, 11, 321, 820, 51308], "temperature": 0.0, "avg_logprob": -0.23479413382614714, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0030277990736067295}, {"id": 681, "seek": 351122, "start": 3530.1, "end": 3533.12, "text": " be able to handle deeper.", "tokens": [51308, 312, 1075, 281, 4813, 7731, 13, 51459], "temperature": 0.0, "avg_logprob": -0.23479413382614714, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0030277990736067295}, {"id": 682, "seek": 353312, "start": 3533.12, "end": 3543.9, "text": " So one thing we could do is we could, let's just remind ourselves of the previous version", "tokens": [50364, 407, 472, 551, 321, 727, 360, 307, 321, 727, 11, 718, 311, 445, 4160, 4175, 295, 264, 3894, 3037, 50903], "temperature": 0.0, "avg_logprob": -0.24199925363063812, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.003945296164602041}, {"id": 683, "seek": 353312, "start": 3543.9, "end": 3555.3199999999997, "text": " so we can compare, is we could have our, go up to 128 parameters.", "tokens": [50903, 370, 321, 393, 6794, 11, 307, 321, 727, 362, 527, 11, 352, 493, 281, 29810, 9834, 13, 51474], "temperature": 0.0, "avg_logprob": -0.24199925363063812, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.003945296164602041}, {"id": 684, "seek": 353312, "start": 3555.3199999999997, "end": 3561.54, "text": " Now the way we do that is we could make our very first convolutional layer have a stride", "tokens": [51474, 823, 264, 636, 321, 360, 300, 307, 321, 727, 652, 527, 588, 700, 45216, 304, 4583, 362, 257, 1056, 482, 51785], "temperature": 0.0, "avg_logprob": -0.24199925363063812, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.003945296164602041}, {"id": 685, "seek": 356154, "start": 3561.54, "end": 3563.8, "text": " of 1.", "tokens": [50364, 295, 502, 13, 50477], "temperature": 0.0, "avg_logprob": -0.24705916956851356, "compression_ratio": 1.743718592964824, "no_speech_prob": 0.0027149796951562166}, {"id": 686, "seek": 356154, "start": 3563.8, "end": 3571.58, "text": " So that would be one that goes from the one input channel to 8 output channels, or 8 filters", "tokens": [50477, 407, 300, 576, 312, 472, 300, 1709, 490, 264, 472, 4846, 2269, 281, 1649, 5598, 9235, 11, 420, 1649, 15995, 50866], "temperature": 0.0, "avg_logprob": -0.24705916956851356, "compression_ratio": 1.743718592964824, "no_speech_prob": 0.0027149796951562166}, {"id": 687, "seek": 356154, "start": 3571.58, "end": 3572.58, "text": " if you like.", "tokens": [50866, 498, 291, 411, 13, 50916], "temperature": 0.0, "avg_logprob": -0.24705916956851356, "compression_ratio": 1.743718592964824, "no_speech_prob": 0.0027149796951562166}, {"id": 688, "seek": 356154, "start": 3572.58, "end": 3578.58, "text": " So if we make it a stride of 1, then that allows us to have one extra layer, and then", "tokens": [50916, 407, 498, 321, 652, 309, 257, 1056, 482, 295, 502, 11, 550, 300, 4045, 505, 281, 362, 472, 2857, 4583, 11, 293, 550, 51216], "temperature": 0.0, "avg_logprob": -0.24705916956851356, "compression_ratio": 1.743718592964824, "no_speech_prob": 0.0027149796951562166}, {"id": 689, "seek": 356154, "start": 3578.58, "end": 3583.68, "text": " that one extra layer could again double the number of channels and take us up to 128.", "tokens": [51216, 300, 472, 2857, 4583, 727, 797, 3834, 264, 1230, 295, 9235, 293, 747, 505, 493, 281, 29810, 13, 51471], "temperature": 0.0, "avg_logprob": -0.24705916956851356, "compression_ratio": 1.743718592964824, "no_speech_prob": 0.0027149796951562166}, {"id": 690, "seek": 356154, "start": 3583.68, "end": 3590.24, "text": " So that would make it deeper and effectively wider as a result.", "tokens": [51471, 407, 300, 576, 652, 309, 7731, 293, 8659, 11842, 382, 257, 1874, 13, 51799], "temperature": 0.0, "avg_logprob": -0.24705916956851356, "compression_ratio": 1.743718592964824, "no_speech_prob": 0.0027149796951562166}, {"id": 691, "seek": 359024, "start": 3590.24, "end": 3599.12, "text": " So we can do our normal batch norm 2d and our new one cycle learning rate with our scheduler.", "tokens": [50364, 407, 321, 393, 360, 527, 2710, 15245, 2026, 568, 67, 293, 527, 777, 472, 6586, 2539, 3314, 365, 527, 12000, 260, 13, 50808], "temperature": 0.0, "avg_logprob": -0.24173601150512694, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00026118807727470994}, {"id": 692, "seek": 359024, "start": 3599.12, "end": 3603.3999999999996, "text": " And the callbacks we're going to use is the device callback, our metrics, our progress", "tokens": [50808, 400, 264, 818, 17758, 321, 434, 516, 281, 764, 307, 264, 4302, 818, 3207, 11, 527, 16367, 11, 527, 4205, 51022], "temperature": 0.0, "avg_logprob": -0.24173601150512694, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00026118807727470994}, {"id": 693, "seek": 359024, "start": 3603.3999999999996, "end": 3609.04, "text": " bar, and our activation stats looking for general values.", "tokens": [51022, 2159, 11, 293, 527, 24433, 18152, 1237, 337, 2674, 4190, 13, 51304], "temperature": 0.0, "avg_logprob": -0.24173601150512694, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00026118807727470994}, {"id": 694, "seek": 359024, "start": 3609.04, "end": 3613.2799999999997, "text": " And I won't have you watch them train because that would be kind of boring.", "tokens": [51304, 400, 286, 1582, 380, 362, 291, 1159, 552, 3847, 570, 300, 576, 312, 733, 295, 9989, 13, 51516], "temperature": 0.0, "avg_logprob": -0.24173601150512694, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00026118807727470994}, {"id": 695, "seek": 359024, "start": 3613.2799999999997, "end": 3619.62, "text": " But if I do this with this deeper and eventually wider network, this is pretty amazing.", "tokens": [51516, 583, 498, 286, 360, 341, 365, 341, 7731, 293, 4728, 11842, 3209, 11, 341, 307, 1238, 2243, 13, 51833], "temperature": 0.0, "avg_logprob": -0.24173601150512694, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00026118807727470994}, {"id": 696, "seek": 361962, "start": 3620.0, "end": 3622.2999999999997, "text": " We get up to 91.7 percent.", "tokens": [50383, 492, 483, 493, 281, 31064, 13, 22, 3043, 13, 50498], "temperature": 0.0, "avg_logprob": -0.30398161514945654, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010489033535122871}, {"id": 697, "seek": 361962, "start": 3622.2999999999997, "end": 3624.7599999999998, "text": " So that's like quite a big difference.", "tokens": [50498, 407, 300, 311, 411, 1596, 257, 955, 2649, 13, 50621], "temperature": 0.0, "avg_logprob": -0.30398161514945654, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010489033535122871}, {"id": 698, "seek": 361962, "start": 3624.7599999999998, "end": 3631.9, "text": " And literally the only difference to our previous model is this one line of code which allowed", "tokens": [50621, 400, 3736, 264, 787, 2649, 281, 527, 3894, 2316, 307, 341, 472, 1622, 295, 3089, 597, 4350, 50978], "temperature": 0.0, "avg_logprob": -0.30398161514945654, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010489033535122871}, {"id": 699, "seek": 361962, "start": 3631.9, "end": 3637.56, "text": " us to take this, instead of going from 1 to 64, it goes from 8 to 128.", "tokens": [50978, 505, 281, 747, 341, 11, 2602, 295, 516, 490, 502, 281, 12145, 11, 309, 1709, 490, 1649, 281, 29810, 13, 51261], "temperature": 0.0, "avg_logprob": -0.30398161514945654, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010489033535122871}, {"id": 700, "seek": 361962, "start": 3637.56, "end": 3640.5, "text": " So that's a very small change, but it massively improved.", "tokens": [51261, 407, 300, 311, 257, 588, 1359, 1319, 11, 457, 309, 29379, 9689, 13, 51408], "temperature": 0.0, "avg_logprob": -0.30398161514945654, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010489033535122871}, {"id": 701, "seek": 361962, "start": 3640.5, "end": 3645.38, "text": " So the error rate's gone down by a temp, you know, about, well over 10 percent, relatively", "tokens": [51408, 407, 264, 6713, 3314, 311, 2780, 760, 538, 257, 18274, 11, 291, 458, 11, 466, 11, 731, 670, 1266, 3043, 11, 7226, 51652], "temperature": 0.0, "avg_logprob": -0.30398161514945654, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010489033535122871}, {"id": 702, "seek": 361962, "start": 3645.38, "end": 3648.7999999999997, "text": " speaking, in terms of the error rate.", "tokens": [51652, 4124, 11, 294, 2115, 295, 264, 6713, 3314, 13, 51823], "temperature": 0.0, "avg_logprob": -0.30398161514945654, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.010489033535122871}, {"id": 703, "seek": 364880, "start": 3648.8, "end": 3651.7200000000003, "text": " So there's a huge impact we've already had.", "tokens": [50364, 407, 456, 311, 257, 2603, 2712, 321, 600, 1217, 632, 13, 50510], "temperature": 0.0, "avg_logprob": -0.32787295391685084, "compression_ratio": 1.5465116279069768, "no_speech_prob": 3.0241932108765468e-05}, {"id": 704, "seek": 364880, "start": 3651.7200000000003, "end": 3656.52, "text": " Again, five epochs.", "tokens": [50510, 3764, 11, 1732, 30992, 28346, 13, 50750], "temperature": 0.0, "avg_logprob": -0.32787295391685084, "compression_ratio": 1.5465116279069768, "no_speech_prob": 3.0241932108765468e-05}, {"id": 705, "seek": 364880, "start": 3656.52, "end": 3661.1600000000003, "text": " So now what we're going to do is we're going to make it deeper still.", "tokens": [50750, 407, 586, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 652, 309, 7731, 920, 13, 50982], "temperature": 0.0, "avg_logprob": -0.32787295391685084, "compression_ratio": 1.5465116279069768, "no_speech_prob": 3.0241932108765468e-05}, {"id": 706, "seek": 364880, "start": 3661.1600000000003, "end": 3670.0800000000004, "text": " But it gets, there becomes a point, so Kaiming He noted that there comes a point where making", "tokens": [50982, 583, 309, 2170, 11, 456, 3643, 257, 935, 11, 370, 10988, 332, 278, 634, 12964, 300, 456, 1487, 257, 935, 689, 1455, 51428], "temperature": 0.0, "avg_logprob": -0.32787295391685084, "compression_ratio": 1.5465116279069768, "no_speech_prob": 3.0241932108765468e-05}, {"id": 707, "seek": 364880, "start": 3670.0800000000004, "end": 3673.1600000000003, "text": " neural nets deeper stops working well.", "tokens": [51428, 18161, 36170, 7731, 10094, 1364, 731, 13, 51582], "temperature": 0.0, "avg_logprob": -0.32787295391685084, "compression_ratio": 1.5465116279069768, "no_speech_prob": 3.0241932108765468e-05}, {"id": 708, "seek": 367316, "start": 3673.16, "end": 3678.7999999999997, "text": " And remember this is the guy who created the initializer that we know and love.", "tokens": [50364, 400, 1604, 341, 307, 264, 2146, 567, 2942, 264, 5883, 6545, 300, 321, 458, 293, 959, 13, 50646], "temperature": 0.0, "avg_logprob": -0.26775546784096577, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.06853049248456955}, {"id": 709, "seek": 367316, "start": 3678.7999999999997, "end": 3686.0, "text": " And he pointed out that even with that good initialization, there comes a time where adding", "tokens": [50646, 400, 415, 10932, 484, 300, 754, 365, 300, 665, 5883, 2144, 11, 456, 1487, 257, 565, 689, 5127, 51006], "temperature": 0.0, "avg_logprob": -0.26775546784096577, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.06853049248456955}, {"id": 710, "seek": 367316, "start": 3686.0, "end": 3688.66, "text": " more layers becomes problematic.", "tokens": [51006, 544, 7914, 3643, 19011, 13, 51139], "temperature": 0.0, "avg_logprob": -0.26775546784096577, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.06853049248456955}, {"id": 711, "seek": 367316, "start": 3688.66, "end": 3691.3999999999996, "text": " And he pointed out something particularly interesting.", "tokens": [51139, 400, 415, 10932, 484, 746, 4098, 1880, 13, 51276], "temperature": 0.0, "avg_logprob": -0.26775546784096577, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.06853049248456955}, {"id": 712, "seek": 367316, "start": 3691.3999999999996, "end": 3695.64, "text": " He said let's take a 20 layer neural network.", "tokens": [51276, 634, 848, 718, 311, 747, 257, 945, 4583, 18161, 3209, 13, 51488], "temperature": 0.0, "avg_logprob": -0.26775546784096577, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.06853049248456955}, {"id": 713, "seek": 367316, "start": 3695.64, "end": 3700.12, "text": " This is in a paper called Deep Residual Learning for Image Recognition that introduced ResNets.", "tokens": [51488, 639, 307, 294, 257, 3035, 1219, 14895, 5015, 327, 901, 15205, 337, 29903, 44682, 849, 300, 7268, 5015, 45, 1385, 13, 51712], "temperature": 0.0, "avg_logprob": -0.26775546784096577, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.06853049248456955}, {"id": 714, "seek": 370012, "start": 3700.12, "end": 3708.3599999999997, "text": " So let's take a 20 layer network and train it for a few, what's that, tens of thousands", "tokens": [50364, 407, 718, 311, 747, 257, 945, 4583, 3209, 293, 3847, 309, 337, 257, 1326, 11, 437, 311, 300, 11, 10688, 295, 5383, 50776], "temperature": 0.0, "avg_logprob": -0.2797126056992005, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.0022169940639287233}, {"id": 715, "seek": 370012, "start": 3708.3599999999997, "end": 3712.8399999999997, "text": " of iterations, and track its test error.", "tokens": [50776, 295, 36540, 11, 293, 2837, 1080, 1500, 6713, 13, 51000], "temperature": 0.0, "avg_logprob": -0.2797126056992005, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.0022169940639287233}, {"id": 716, "seek": 370012, "start": 3712.8399999999997, "end": 3718.44, "text": " Okay and now let's do exactly the same thing on a 56 layer, otherwise identical, but deeper", "tokens": [51000, 1033, 293, 586, 718, 311, 360, 2293, 264, 912, 551, 322, 257, 19687, 4583, 11, 5911, 14800, 11, 457, 7731, 51280], "temperature": 0.0, "avg_logprob": -0.2797126056992005, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.0022169940639287233}, {"id": 717, "seek": 370012, "start": 3718.44, "end": 3720.94, "text": " 56 layer network.", "tokens": [51280, 19687, 4583, 3209, 13, 51405], "temperature": 0.0, "avg_logprob": -0.2797126056992005, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.0022169940639287233}, {"id": 718, "seek": 370012, "start": 3720.94, "end": 3726.08, "text": " And he pointed out that the 56 layer network had a worse error than the 20 layer.", "tokens": [51405, 400, 415, 10932, 484, 300, 264, 19687, 4583, 3209, 632, 257, 5324, 6713, 813, 264, 945, 4583, 13, 51662], "temperature": 0.0, "avg_logprob": -0.2797126056992005, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.0022169940639287233}, {"id": 719, "seek": 370012, "start": 3726.08, "end": 3729.6, "text": " And it wasn't just a problem of generalization, because it was worse on the training set as", "tokens": [51662, 400, 309, 2067, 380, 445, 257, 1154, 295, 2674, 2144, 11, 570, 309, 390, 5324, 322, 264, 3097, 992, 382, 51838], "temperature": 0.0, "avg_logprob": -0.2797126056992005, "compression_ratio": 1.7024793388429753, "no_speech_prob": 0.0022169940639287233}, {"id": 720, "seek": 372960, "start": 3729.6, "end": 3733.12, "text": " well.", "tokens": [50364, 731, 13, 50540], "temperature": 0.0, "avg_logprob": -0.20184388236394005, "compression_ratio": 1.5032679738562091, "no_speech_prob": 0.0008426375570707023}, {"id": 721, "seek": 372960, "start": 3733.12, "end": 3748.88, "text": " Now the insight that he had is if you just set the additional 36 layers to just identity,", "tokens": [50540, 823, 264, 11269, 300, 415, 632, 307, 498, 291, 445, 992, 264, 4497, 8652, 7914, 281, 445, 6575, 11, 51328], "temperature": 0.0, "avg_logprob": -0.20184388236394005, "compression_ratio": 1.5032679738562091, "no_speech_prob": 0.0008426375570707023}, {"id": 722, "seek": 372960, "start": 3748.88, "end": 3753.62, "text": " you know, identity matrices, they should, they would do nothing at all.", "tokens": [51328, 291, 458, 11, 6575, 32284, 11, 436, 820, 11, 436, 576, 360, 1825, 412, 439, 13, 51565], "temperature": 0.0, "avg_logprob": -0.20184388236394005, "compression_ratio": 1.5032679738562091, "no_speech_prob": 0.0008426375570707023}, {"id": 723, "seek": 372960, "start": 3753.62, "end": 3758.6, "text": " And so a 56 layer network is a superset of a 20 layer network.", "tokens": [51565, 400, 370, 257, 19687, 4583, 3209, 307, 257, 37906, 302, 295, 257, 945, 4583, 3209, 13, 51814], "temperature": 0.0, "avg_logprob": -0.20184388236394005, "compression_ratio": 1.5032679738562091, "no_speech_prob": 0.0008426375570707023}, {"id": 724, "seek": 375860, "start": 3758.6, "end": 3763.92, "text": " So it should be at least as good, but it's not, it's worse.", "tokens": [50364, 407, 309, 820, 312, 412, 1935, 382, 665, 11, 457, 309, 311, 406, 11, 309, 311, 5324, 13, 50630], "temperature": 0.0, "avg_logprob": -0.20370693949909954, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.0004373332194518298}, {"id": 725, "seek": 375860, "start": 3763.92, "end": 3770.44, "text": " So clearly the problem here is something about training it.", "tokens": [50630, 407, 4448, 264, 1154, 510, 307, 746, 466, 3097, 309, 13, 50956], "temperature": 0.0, "avg_logprob": -0.20370693949909954, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.0004373332194518298}, {"id": 726, "seek": 375860, "start": 3770.44, "end": 3780.24, "text": " And so him and his team came up with a really clever insight, which is can we create a 56", "tokens": [50956, 400, 370, 796, 293, 702, 1469, 1361, 493, 365, 257, 534, 13494, 11269, 11, 597, 307, 393, 321, 1884, 257, 19687, 51446], "temperature": 0.0, "avg_logprob": -0.20370693949909954, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.0004373332194518298}, {"id": 727, "seek": 375860, "start": 3780.24, "end": 3788.12, "text": " layer network which has the same training dynamics as a 20 layer network or even less.", "tokens": [51446, 4583, 3209, 597, 575, 264, 912, 3097, 15679, 382, 257, 945, 4583, 3209, 420, 754, 1570, 13, 51840], "temperature": 0.0, "avg_logprob": -0.20370693949909954, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.0004373332194518298}, {"id": 728, "seek": 378812, "start": 3788.64, "end": 3791.88, "text": " And they realized, yes you can.", "tokens": [50390, 400, 436, 5334, 11, 2086, 291, 393, 13, 50552], "temperature": 0.0, "avg_logprob": -0.2907256881395976, "compression_ratio": 1.4191176470588236, "no_speech_prob": 0.004331388045102358}, {"id": 729, "seek": 378812, "start": 3791.88, "end": 3799.56, "text": " What you could do is you could add something called a shortcut connection.", "tokens": [50552, 708, 291, 727, 360, 307, 291, 727, 909, 746, 1219, 257, 24822, 4984, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2907256881395976, "compression_ratio": 1.4191176470588236, "no_speech_prob": 0.004331388045102358}, {"id": 730, "seek": 378812, "start": 3799.56, "end": 3813.92, "text": " And basically the idea is that normally when we have, you know, our inputs coming into", "tokens": [50936, 400, 1936, 264, 1558, 307, 300, 5646, 562, 321, 362, 11, 291, 458, 11, 527, 15743, 1348, 666, 51654], "temperature": 0.0, "avg_logprob": -0.2907256881395976, "compression_ratio": 1.4191176470588236, "no_speech_prob": 0.004331388045102358}, {"id": 731, "seek": 381392, "start": 3813.92, "end": 3814.92, "text": " our convolution.", "tokens": [50364, 527, 45216, 13, 50414], "temperature": 0.0, "avg_logprob": -0.24221386909484863, "compression_ratio": 1.6210526315789473, "no_speech_prob": 0.13114267587661743}, {"id": 732, "seek": 381392, "start": 3814.92, "end": 3824.84, "text": " So let's say that's, that was our inputs and here's our convolution and here's our outputs.", "tokens": [50414, 407, 718, 311, 584, 300, 311, 11, 300, 390, 527, 15743, 293, 510, 311, 527, 45216, 293, 510, 311, 527, 23930, 13, 50910], "temperature": 0.0, "avg_logprob": -0.24221386909484863, "compression_ratio": 1.6210526315789473, "no_speech_prob": 0.13114267587661743}, {"id": 733, "seek": 381392, "start": 3824.84, "end": 3831.44, "text": " Now if we do this 56 times, that's a lot of stacked up convolutions, which are effectively", "tokens": [50910, 823, 498, 321, 360, 341, 19687, 1413, 11, 300, 311, 257, 688, 295, 28867, 493, 3754, 15892, 11, 597, 366, 8659, 51240], "temperature": 0.0, "avg_logprob": -0.24221386909484863, "compression_ratio": 1.6210526315789473, "no_speech_prob": 0.13114267587661743}, {"id": 734, "seek": 381392, "start": 3831.44, "end": 3836.6800000000003, "text": " matrix multiplications, with a lot of opportunity for, you know, gradient explosions and all", "tokens": [51240, 8141, 17596, 763, 11, 365, 257, 688, 295, 2650, 337, 11, 291, 458, 11, 16235, 36872, 293, 439, 51502], "temperature": 0.0, "avg_logprob": -0.24221386909484863, "compression_ratio": 1.6210526315789473, "no_speech_prob": 0.13114267587661743}, {"id": 735, "seek": 381392, "start": 3836.6800000000003, "end": 3839.52, "text": " that fun stuff.", "tokens": [51502, 300, 1019, 1507, 13, 51644], "temperature": 0.0, "avg_logprob": -0.24221386909484863, "compression_ratio": 1.6210526315789473, "no_speech_prob": 0.13114267587661743}, {"id": 736, "seek": 383952, "start": 3839.8, "end": 3849.04, "text": " So how could we make it so that we have convolutions but with the training dynamics of a much shallower", "tokens": [50378, 407, 577, 727, 321, 652, 309, 370, 300, 321, 362, 3754, 15892, 457, 365, 264, 3097, 15679, 295, 257, 709, 4393, 968, 50840], "temperature": 0.0, "avg_logprob": -0.30132205669696516, "compression_ratio": 1.2142857142857142, "no_speech_prob": 0.006003197748214006}, {"id": 737, "seek": 383952, "start": 3849.04, "end": 3850.36, "text": " network?", "tokens": [50840, 3209, 30, 50906], "temperature": 0.0, "avg_logprob": -0.30132205669696516, "compression_ratio": 1.2142857142857142, "no_speech_prob": 0.006003197748214006}, {"id": 738, "seek": 383952, "start": 3850.36, "end": 3861.92, "text": " And here's what he did.", "tokens": [50906, 400, 510, 311, 437, 415, 630, 13, 51484], "temperature": 0.0, "avg_logprob": -0.30132205669696516, "compression_ratio": 1.2142857142857142, "no_speech_prob": 0.006003197748214006}, {"id": 739, "seek": 386192, "start": 3861.92, "end": 3871.6, "text": " He said let's actually put two convs in here to make it twice as deep, because we are trying", "tokens": [50364, 634, 848, 718, 311, 767, 829, 732, 3754, 82, 294, 510, 281, 652, 309, 6091, 382, 2452, 11, 570, 321, 366, 1382, 50848], "temperature": 0.0, "avg_logprob": -0.2613589254657874, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.07262531667947769}, {"id": 740, "seek": 386192, "start": 3871.6, "end": 3873.7200000000003, "text": " to make things deeper.", "tokens": [50848, 281, 652, 721, 7731, 13, 50954], "temperature": 0.0, "avg_logprob": -0.2613589254657874, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.07262531667947769}, {"id": 741, "seek": 386192, "start": 3873.7200000000003, "end": 3883.2400000000002, "text": " But then let's add what's called a skip connection, where instead of just being out equals, so", "tokens": [50954, 583, 550, 718, 311, 909, 437, 311, 1219, 257, 10023, 4984, 11, 689, 2602, 295, 445, 885, 484, 6915, 11, 370, 51430], "temperature": 0.0, "avg_logprob": -0.2613589254657874, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.07262531667947769}, {"id": 742, "seek": 386192, "start": 3883.2400000000002, "end": 3886.16, "text": " this is conv1, this is conv2.", "tokens": [51430, 341, 307, 3754, 16, 11, 341, 307, 3754, 17, 13, 51576], "temperature": 0.0, "avg_logprob": -0.2613589254657874, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.07262531667947769}, {"id": 743, "seek": 386192, "start": 3886.16, "end": 3890.7200000000003, "text": " Instead of being out equals, and there's a, you know, assume that these include activation", "tokens": [51576, 7156, 295, 885, 484, 6915, 11, 293, 456, 311, 257, 11, 291, 458, 11, 6552, 300, 613, 4090, 24433, 51804], "temperature": 0.0, "avg_logprob": -0.2613589254657874, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.07262531667947769}, {"id": 744, "seek": 389072, "start": 3890.72, "end": 3899.6, "text": " functions, equals conv2 of conv1 of in.", "tokens": [50364, 6828, 11, 6915, 3754, 17, 295, 3754, 16, 295, 294, 13, 50808], "temperature": 0.0, "avg_logprob": -0.3196733670356946, "compression_ratio": 1.2967032967032968, "no_speech_prob": 0.002981027355417609}, {"id": 745, "seek": 389072, "start": 3899.6, "end": 3909.8399999999997, "text": " Right, instead of just doing that, let's make it conv2 of conv1 of in plus in.", "tokens": [50808, 1779, 11, 2602, 295, 445, 884, 300, 11, 718, 311, 652, 309, 3754, 17, 295, 3754, 16, 295, 294, 1804, 294, 13, 51320], "temperature": 0.0, "avg_logprob": -0.3196733670356946, "compression_ratio": 1.2967032967032968, "no_speech_prob": 0.002981027355417609}, {"id": 746, "seek": 390984, "start": 3909.84, "end": 3924.6400000000003, "text": " Now if we initialize these at the first to have weights of zero, then initially this", "tokens": [50364, 823, 498, 321, 5883, 1125, 613, 412, 264, 700, 281, 362, 17443, 295, 4018, 11, 550, 9105, 341, 51104], "temperature": 0.0, "avg_logprob": -0.2867461345234855, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.00884690135717392}, {"id": 747, "seek": 390984, "start": 3924.6400000000003, "end": 3926.4, "text": " will do nothing at all.", "tokens": [51104, 486, 360, 1825, 412, 439, 13, 51192], "temperature": 0.0, "avg_logprob": -0.2867461345234855, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.00884690135717392}, {"id": 748, "seek": 390984, "start": 3926.4, "end": 3934.52, "text": " It will output zero, and therefore at first it you'll just get out equals in, which is", "tokens": [51192, 467, 486, 5598, 4018, 11, 293, 4412, 412, 700, 309, 291, 603, 445, 483, 484, 6915, 294, 11, 597, 307, 51598], "temperature": 0.0, "avg_logprob": -0.2867461345234855, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.00884690135717392}, {"id": 749, "seek": 390984, "start": 3934.52, "end": 3936.7200000000003, "text": " exactly what we wanted, right?", "tokens": [51598, 2293, 437, 321, 1415, 11, 558, 30, 51708], "temperature": 0.0, "avg_logprob": -0.2867461345234855, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.00884690135717392}, {"id": 750, "seek": 393672, "start": 3936.72, "end": 3944.9399999999996, "text": " We actually want to, for it to be as if there is no extra layers.", "tokens": [50364, 492, 767, 528, 281, 11, 337, 309, 281, 312, 382, 498, 456, 307, 572, 2857, 7914, 13, 50775], "temperature": 0.0, "avg_logprob": -0.24470313120696505, "compression_ratio": 1.4625850340136055, "no_speech_prob": 0.01971670798957348}, {"id": 751, "seek": 393672, "start": 3944.9399999999996, "end": 3954.24, "text": " And so this way we actually end up with a network which can, which can be deep, but", "tokens": [50775, 400, 370, 341, 636, 321, 767, 917, 493, 365, 257, 3209, 597, 393, 11, 597, 393, 312, 2452, 11, 457, 51240], "temperature": 0.0, "avg_logprob": -0.24470313120696505, "compression_ratio": 1.4625850340136055, "no_speech_prob": 0.01971670798957348}, {"id": 752, "seek": 393672, "start": 3954.24, "end": 3959.68, "text": " also at least when you start training behaves as if it's shallow.", "tokens": [51240, 611, 412, 1935, 562, 291, 722, 3097, 36896, 382, 498, 309, 311, 20488, 13, 51512], "temperature": 0.0, "avg_logprob": -0.24470313120696505, "compression_ratio": 1.4625850340136055, "no_speech_prob": 0.01971670798957348}, {"id": 753, "seek": 395968, "start": 3959.68, "end": 3970.12, "text": " It's called a residual connection, because if we subtract in from both sides, now then", "tokens": [50364, 467, 311, 1219, 257, 27980, 4984, 11, 570, 498, 321, 16390, 294, 490, 1293, 4881, 11, 586, 550, 50886], "temperature": 0.0, "avg_logprob": -0.26185953049432664, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.0009110369137488306}, {"id": 754, "seek": 395968, "start": 3970.12, "end": 3976.2, "text": " we would get out minus in equals conv1 of conv2 of in.", "tokens": [50886, 321, 576, 483, 484, 3175, 294, 6915, 3754, 16, 295, 3754, 17, 295, 294, 13, 51190], "temperature": 0.0, "avg_logprob": -0.26185953049432664, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.0009110369137488306}, {"id": 755, "seek": 395968, "start": 3976.2, "end": 3983.52, "text": " In other words, the difference between the end point and the starting point, which is", "tokens": [51190, 682, 661, 2283, 11, 264, 2649, 1296, 264, 917, 935, 293, 264, 2891, 935, 11, 597, 307, 51556], "temperature": 0.0, "avg_logprob": -0.26185953049432664, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.0009110369137488306}, {"id": 756, "seek": 395968, "start": 3983.52, "end": 3985.3199999999997, "text": " the residual.", "tokens": [51556, 264, 27980, 13, 51646], "temperature": 0.0, "avg_logprob": -0.26185953049432664, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.0009110369137488306}, {"id": 757, "seek": 398532, "start": 3985.32, "end": 3995.1200000000003, "text": " And so another way of thinking about it is that this is calculating a residual.", "tokens": [50364, 400, 370, 1071, 636, 295, 1953, 466, 309, 307, 300, 341, 307, 28258, 257, 27980, 13, 50854], "temperature": 0.0, "avg_logprob": -0.284171554277528, "compression_ratio": 1.5669291338582678, "no_speech_prob": 0.00513948081061244}, {"id": 758, "seek": 398532, "start": 3995.1200000000003, "end": 3998.2400000000002, "text": " So there's a couple of ways of thinking about it.", "tokens": [50854, 407, 456, 311, 257, 1916, 295, 2098, 295, 1953, 466, 309, 13, 51010], "temperature": 0.0, "avg_logprob": -0.284171554277528, "compression_ratio": 1.5669291338582678, "no_speech_prob": 0.00513948081061244}, {"id": 759, "seek": 398532, "start": 3998.2400000000002, "end": 4008.04, "text": " And so this, this thing here is called the res block or resnet block.", "tokens": [51010, 400, 370, 341, 11, 341, 551, 510, 307, 1219, 264, 725, 3461, 420, 725, 7129, 3461, 13, 51500], "temperature": 0.0, "avg_logprob": -0.284171554277528, "compression_ratio": 1.5669291338582678, "no_speech_prob": 0.00513948081061244}, {"id": 760, "seek": 400804, "start": 4008.04, "end": 4018.24, "text": " Okay, so Sam Watkins has just pointed out the confusion here, which is that this only", "tokens": [50364, 1033, 11, 370, 4832, 12593, 10277, 575, 445, 10932, 484, 264, 15075, 510, 11, 597, 307, 300, 341, 787, 50874], "temperature": 0.0, "avg_logprob": -0.2535083912037037, "compression_ratio": 1.4485294117647058, "no_speech_prob": 0.018832815811038017}, {"id": 761, "seek": 400804, "start": 4018.24, "end": 4028.62, "text": " works if, let's put the minus in back and put it back over here.", "tokens": [50874, 1985, 498, 11, 718, 311, 829, 264, 3175, 294, 646, 293, 829, 309, 646, 670, 510, 13, 51393], "temperature": 0.0, "avg_logprob": -0.2535083912037037, "compression_ratio": 1.4485294117647058, "no_speech_prob": 0.018832815811038017}, {"id": 762, "seek": 400804, "start": 4028.62, "end": 4031.72, "text": " This only works if you can add these together.", "tokens": [51393, 639, 787, 1985, 498, 291, 393, 909, 613, 1214, 13, 51548], "temperature": 0.0, "avg_logprob": -0.2535083912037037, "compression_ratio": 1.4485294117647058, "no_speech_prob": 0.018832815811038017}, {"id": 763, "seek": 403172, "start": 4031.72, "end": 4040.3999999999996, "text": " Now if conv1 and conv2 both have the same number of channels as in, or same number of", "tokens": [50364, 823, 498, 3754, 16, 293, 3754, 17, 1293, 362, 264, 912, 1230, 295, 9235, 382, 294, 11, 420, 912, 1230, 295, 50798], "temperature": 0.0, "avg_logprob": -0.26673829710328734, "compression_ratio": 1.6826347305389222, "no_speech_prob": 0.003707176074385643}, {"id": 764, "seek": 403172, "start": 4040.3999999999996, "end": 4050.2799999999997, "text": " filters, same number of filters, and they also have stride1, then that will work fine.", "tokens": [50798, 15995, 11, 912, 1230, 295, 15995, 11, 293, 436, 611, 362, 1056, 482, 16, 11, 550, 300, 486, 589, 2489, 13, 51292], "temperature": 0.0, "avg_logprob": -0.26673829710328734, "compression_ratio": 1.6826347305389222, "no_speech_prob": 0.003707176074385643}, {"id": 765, "seek": 403172, "start": 4050.2799999999997, "end": 4054.7599999999998, "text": " You'll end up, that will be exactly the same output shape as the input shape, and you can", "tokens": [51292, 509, 603, 917, 493, 11, 300, 486, 312, 2293, 264, 912, 5598, 3909, 382, 264, 4846, 3909, 11, 293, 291, 393, 51516], "temperature": 0.0, "avg_logprob": -0.26673829710328734, "compression_ratio": 1.6826347305389222, "no_speech_prob": 0.003707176074385643}, {"id": 766, "seek": 403172, "start": 4054.7599999999998, "end": 4057.64, "text": " add them together.", "tokens": [51516, 909, 552, 1214, 13, 51660], "temperature": 0.0, "avg_logprob": -0.26673829710328734, "compression_ratio": 1.6826347305389222, "no_speech_prob": 0.003707176074385643}, {"id": 767, "seek": 405764, "start": 4057.64, "end": 4065.48, "text": " But if they are not the same, then you're in a bit of trouble.", "tokens": [50364, 583, 498, 436, 366, 406, 264, 912, 11, 550, 291, 434, 294, 257, 857, 295, 5253, 13, 50756], "temperature": 0.0, "avg_logprob": -0.2563984845135663, "compression_ratio": 1.4096385542168675, "no_speech_prob": 0.025562472641468048}, {"id": 768, "seek": 405764, "start": 4065.48, "end": 4067.2799999999997, "text": " So what do you do?", "tokens": [50756, 407, 437, 360, 291, 360, 30, 50846], "temperature": 0.0, "avg_logprob": -0.2563984845135663, "compression_ratio": 1.4096385542168675, "no_speech_prob": 0.025562472641468048}, {"id": 769, "seek": 405764, "start": 4067.2799999999997, "end": 4079.16, "text": " And the answer which Kaiming He et al came up with is to add a conv on in as well, but", "tokens": [50846, 400, 264, 1867, 597, 10988, 332, 278, 634, 1030, 419, 1361, 493, 365, 307, 281, 909, 257, 3754, 322, 294, 382, 731, 11, 457, 51440], "temperature": 0.0, "avg_logprob": -0.2563984845135663, "compression_ratio": 1.4096385542168675, "no_speech_prob": 0.025562472641468048}, {"id": 770, "seek": 405764, "start": 4079.16, "end": 4084.08, "text": " to make it as simple as possible.", "tokens": [51440, 281, 652, 309, 382, 2199, 382, 1944, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2563984845135663, "compression_ratio": 1.4096385542168675, "no_speech_prob": 0.025562472641468048}, {"id": 771, "seek": 405764, "start": 4084.08, "end": 4085.68, "text": " We call this the identity conv.", "tokens": [51686, 492, 818, 341, 264, 6575, 3754, 13, 51766], "temperature": 0.0, "avg_logprob": -0.2563984845135663, "compression_ratio": 1.4096385542168675, "no_speech_prob": 0.025562472641468048}, {"id": 772, "seek": 408568, "start": 4085.72, "end": 4090.0, "text": " It's not really an identity anymore, but we're trying to make it as simple as possible so", "tokens": [50366, 467, 311, 406, 534, 364, 6575, 3602, 11, 457, 321, 434, 1382, 281, 652, 309, 382, 2199, 382, 1944, 370, 50580], "temperature": 0.0, "avg_logprob": -0.20058048793247768, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0047551305033266544}, {"id": 773, "seek": 408568, "start": 4090.0, "end": 4095.12, "text": " that we do as little to mess up these training dynamics as we can.", "tokens": [50580, 300, 321, 360, 382, 707, 281, 2082, 493, 613, 3097, 15679, 382, 321, 393, 13, 50836], "temperature": 0.0, "avg_logprob": -0.20058048793247768, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0047551305033266544}, {"id": 774, "seek": 408568, "start": 4095.12, "end": 4102.68, "text": " And the simplest possible convolution is a one-by-one filter block, a one-by-one kernel,", "tokens": [50836, 400, 264, 22811, 1944, 45216, 307, 257, 472, 12, 2322, 12, 546, 6608, 3461, 11, 257, 472, 12, 2322, 12, 546, 28256, 11, 51214], "temperature": 0.0, "avg_logprob": -0.20058048793247768, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0047551305033266544}, {"id": 775, "seek": 408568, "start": 4102.68, "end": 4110.44, "text": " I guess we should call it, a one-by-one kernel size.", "tokens": [51214, 286, 2041, 321, 820, 818, 309, 11, 257, 472, 12, 2322, 12, 546, 28256, 2744, 13, 51602], "temperature": 0.0, "avg_logprob": -0.20058048793247768, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0047551305033266544}, {"id": 776, "seek": 408568, "start": 4110.44, "end": 4115.4, "text": " And using that, and we can also add a stride or whatever if we want to.", "tokens": [51602, 400, 1228, 300, 11, 293, 321, 393, 611, 909, 257, 1056, 482, 420, 2035, 498, 321, 528, 281, 13, 51850], "temperature": 0.0, "avg_logprob": -0.20058048793247768, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0047551305033266544}, {"id": 777, "seek": 411540, "start": 4116.12, "end": 4119.719999999999, "text": " So let me show you the code.", "tokens": [50400, 407, 718, 385, 855, 291, 264, 3089, 13, 50580], "temperature": 0.0, "avg_logprob": -0.23438751454256018, "compression_ratio": 1.8652849740932642, "no_speech_prob": 1.4064031347515993e-05}, {"id": 778, "seek": 411540, "start": 4119.719999999999, "end": 4125.12, "text": " So we're going to create something called a conv block, okay, and the conv block is", "tokens": [50580, 407, 321, 434, 516, 281, 1884, 746, 1219, 257, 3754, 3461, 11, 1392, 11, 293, 264, 3754, 3461, 307, 50850], "temperature": 0.0, "avg_logprob": -0.23438751454256018, "compression_ratio": 1.8652849740932642, "no_speech_prob": 1.4064031347515993e-05}, {"id": 779, "seek": 411540, "start": 4125.12, "end": 4128.12, "text": " going to do the two convs.", "tokens": [50850, 516, 281, 360, 264, 732, 3754, 82, 13, 51000], "temperature": 0.0, "avg_logprob": -0.23438751454256018, "compression_ratio": 1.8652849740932642, "no_speech_prob": 1.4064031347515993e-05}, {"id": 780, "seek": 411540, "start": 4128.12, "end": 4130.0599999999995, "text": " That's going to be a conv block, okay.", "tokens": [51000, 663, 311, 516, 281, 312, 257, 3754, 3461, 11, 1392, 13, 51097], "temperature": 0.0, "avg_logprob": -0.23438751454256018, "compression_ratio": 1.8652849740932642, "no_speech_prob": 1.4064031347515993e-05}, {"id": 781, "seek": 411540, "start": 4130.0599999999995, "end": 4135.16, "text": " So we've got some number of input filters, some number of output filters, some stride,", "tokens": [51097, 407, 321, 600, 658, 512, 1230, 295, 4846, 15995, 11, 512, 1230, 295, 5598, 15995, 11, 512, 1056, 482, 11, 51352], "temperature": 0.0, "avg_logprob": -0.23438751454256018, "compression_ratio": 1.8652849740932642, "no_speech_prob": 1.4064031347515993e-05}, {"id": 782, "seek": 411540, "start": 4135.16, "end": 4143.759999999999, "text": " some activation functions, possibly a normalization, and possibly, and some kernel shape, some", "tokens": [51352, 512, 24433, 6828, 11, 6264, 257, 2710, 2144, 11, 293, 6264, 11, 293, 512, 28256, 3909, 11, 512, 51782], "temperature": 0.0, "avg_logprob": -0.23438751454256018, "compression_ratio": 1.8652849740932642, "no_speech_prob": 1.4064031347515993e-05}, {"id": 783, "seek": 414376, "start": 4143.76, "end": 4145.8, "text": " kernel size.", "tokens": [50364, 28256, 2744, 13, 50466], "temperature": 0.0, "avg_logprob": -0.2579151789347331, "compression_ratio": 1.9252873563218391, "no_speech_prob": 0.0003250347508583218}, {"id": 784, "seek": 414376, "start": 4145.8, "end": 4154.56, "text": " So the second conv is actually going to go from output filters to output filters because", "tokens": [50466, 407, 264, 1150, 3754, 307, 767, 516, 281, 352, 490, 5598, 15995, 281, 5598, 15995, 570, 50904], "temperature": 0.0, "avg_logprob": -0.2579151789347331, "compression_ratio": 1.9252873563218391, "no_speech_prob": 0.0003250347508583218}, {"id": 785, "seek": 414376, "start": 4154.56, "end": 4160.1, "text": " the first conv is going to be from input filters to output filters.", "tokens": [50904, 264, 700, 3754, 307, 516, 281, 312, 490, 4846, 15995, 281, 5598, 15995, 13, 51181], "temperature": 0.0, "avg_logprob": -0.2579151789347331, "compression_ratio": 1.9252873563218391, "no_speech_prob": 0.0003250347508583218}, {"id": 786, "seek": 414376, "start": 4160.1, "end": 4165.52, "text": " So by the time we get to the second conv, it's going to be NF to NF.", "tokens": [51181, 407, 538, 264, 565, 321, 483, 281, 264, 1150, 3754, 11, 309, 311, 516, 281, 312, 13576, 281, 13576, 13, 51452], "temperature": 0.0, "avg_logprob": -0.2579151789347331, "compression_ratio": 1.9252873563218391, "no_speech_prob": 0.0003250347508583218}, {"id": 787, "seek": 414376, "start": 4165.52, "end": 4171.64, "text": " The first conv, we will set stride one, and then the second conv will have the requested", "tokens": [51452, 440, 700, 3754, 11, 321, 486, 992, 1056, 482, 472, 11, 293, 550, 264, 1150, 3754, 486, 362, 264, 16436, 51758], "temperature": 0.0, "avg_logprob": -0.2579151789347331, "compression_ratio": 1.9252873563218391, "no_speech_prob": 0.0003250347508583218}, {"id": 788, "seek": 414376, "start": 4171.64, "end": 4172.64, "text": " stride.", "tokens": [51758, 1056, 482, 13, 51808], "temperature": 0.0, "avg_logprob": -0.2579151789347331, "compression_ratio": 1.9252873563218391, "no_speech_prob": 0.0003250347508583218}, {"id": 789, "seek": 417264, "start": 4172.64, "end": 4177.240000000001, "text": " So that way the two convs back-to-back are going to overall have the requested stride.", "tokens": [50364, 407, 300, 636, 264, 732, 3754, 82, 646, 12, 1353, 12, 3207, 366, 516, 281, 4787, 362, 264, 16436, 1056, 482, 13, 50594], "temperature": 0.0, "avg_logprob": -0.26089354923793245, "compression_ratio": 1.8078817733990147, "no_speech_prob": 7.889260814408772e-06}, {"id": 790, "seek": 417264, "start": 4177.240000000001, "end": 4182.280000000001, "text": " So this way the combination of these two convs is going to eventually, is going to take us", "tokens": [50594, 407, 341, 636, 264, 6562, 295, 613, 732, 3754, 82, 307, 516, 281, 4728, 11, 307, 516, 281, 747, 505, 50846], "temperature": 0.0, "avg_logprob": -0.26089354923793245, "compression_ratio": 1.8078817733990147, "no_speech_prob": 7.889260814408772e-06}, {"id": 791, "seek": 417264, "start": 4182.280000000001, "end": 4186.76, "text": " from NI to NF in terms of the number of filters, and it's going to have the stride that we", "tokens": [50846, 490, 18482, 281, 13576, 294, 2115, 295, 264, 1230, 295, 15995, 11, 293, 309, 311, 516, 281, 362, 264, 1056, 482, 300, 321, 51070], "temperature": 0.0, "avg_logprob": -0.26089354923793245, "compression_ratio": 1.8078817733990147, "no_speech_prob": 7.889260814408772e-06}, {"id": 792, "seek": 417264, "start": 4186.76, "end": 4189.92, "text": " requested.", "tokens": [51070, 16436, 13, 51228], "temperature": 0.0, "avg_logprob": -0.26089354923793245, "compression_ratio": 1.8078817733990147, "no_speech_prob": 7.889260814408772e-06}, {"id": 793, "seek": 417264, "start": 4189.92, "end": 4194.96, "text": " So it's going to be a, the conv block is a sequential block consisting of a convolution", "tokens": [51228, 407, 309, 311, 516, 281, 312, 257, 11, 264, 3754, 3461, 307, 257, 42881, 3461, 33921, 295, 257, 45216, 51480], "temperature": 0.0, "avg_logprob": -0.26089354923793245, "compression_ratio": 1.8078817733990147, "no_speech_prob": 7.889260814408772e-06}, {"id": 794, "seek": 419496, "start": 4194.96, "end": 4203.24, "text": " followed by another convolution, each one with the requested kernel size, and the requested", "tokens": [50364, 6263, 538, 1071, 45216, 11, 1184, 472, 365, 264, 16436, 28256, 2744, 11, 293, 264, 16436, 50778], "temperature": 0.0, "avg_logprob": -0.23876118350338627, "compression_ratio": 1.6528497409326426, "no_speech_prob": 0.024796510115265846}, {"id": 795, "seek": 419496, "start": 4203.24, "end": 4207.6, "text": " activation function, and the requested normalization layer.", "tokens": [50778, 24433, 2445, 11, 293, 264, 16436, 2710, 2144, 4583, 13, 50996], "temperature": 0.0, "avg_logprob": -0.23876118350338627, "compression_ratio": 1.6528497409326426, "no_speech_prob": 0.024796510115265846}, {"id": 796, "seek": 419496, "start": 4207.6, "end": 4209.92, "text": " The second conv won't have an activation function.", "tokens": [50996, 440, 1150, 3754, 1582, 380, 362, 364, 24433, 2445, 13, 51112], "temperature": 0.0, "avg_logprob": -0.23876118350338627, "compression_ratio": 1.6528497409326426, "no_speech_prob": 0.024796510115265846}, {"id": 797, "seek": 419496, "start": 4209.92, "end": 4213.04, "text": " I'll explain why in a moment.", "tokens": [51112, 286, 603, 2903, 983, 294, 257, 1623, 13, 51268], "temperature": 0.0, "avg_logprob": -0.23876118350338627, "compression_ratio": 1.6528497409326426, "no_speech_prob": 0.024796510115265846}, {"id": 798, "seek": 419496, "start": 4213.04, "end": 4220.24, "text": " And so I mentioned that one way to make this as if it didn't exist would be to set the", "tokens": [51268, 400, 370, 286, 2835, 300, 472, 636, 281, 652, 341, 382, 498, 309, 994, 380, 2514, 576, 312, 281, 992, 264, 51628], "temperature": 0.0, "avg_logprob": -0.23876118350338627, "compression_ratio": 1.6528497409326426, "no_speech_prob": 0.024796510115265846}, {"id": 799, "seek": 422024, "start": 4220.24, "end": 4225.5599999999995, "text": " convolutional weights to zero, and the biases to zero.", "tokens": [50364, 45216, 304, 17443, 281, 4018, 11, 293, 264, 32152, 281, 4018, 13, 50630], "temperature": 0.0, "avg_logprob": -0.26160370386563814, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.020331712439656258}, {"id": 800, "seek": 422024, "start": 4225.5599999999995, "end": 4231.5599999999995, "text": " But actually we would, we would like to have, you know, correctly randomly initialized weights.", "tokens": [50630, 583, 767, 321, 576, 11, 321, 576, 411, 281, 362, 11, 291, 458, 11, 8944, 16979, 5883, 1602, 17443, 13, 50930], "temperature": 0.0, "avg_logprob": -0.26160370386563814, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.020331712439656258}, {"id": 801, "seek": 422024, "start": 4231.5599999999995, "end": 4239.24, "text": " So instead what we can do is if you're using batch norm, we can initialize this conv2 one", "tokens": [50930, 407, 2602, 437, 321, 393, 360, 307, 498, 291, 434, 1228, 15245, 2026, 11, 321, 393, 5883, 1125, 341, 3754, 17, 472, 51314], "temperature": 0.0, "avg_logprob": -0.26160370386563814, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.020331712439656258}, {"id": 802, "seek": 422024, "start": 4239.24, "end": 4240.74, "text": " will be the batch norm layer.", "tokens": [51314, 486, 312, 264, 15245, 2026, 4583, 13, 51389], "temperature": 0.0, "avg_logprob": -0.26160370386563814, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.020331712439656258}, {"id": 803, "seek": 422024, "start": 4240.74, "end": 4244.76, "text": " We can initialize the batch norm weights to zero.", "tokens": [51389, 492, 393, 5883, 1125, 264, 15245, 2026, 17443, 281, 4018, 13, 51590], "temperature": 0.0, "avg_logprob": -0.26160370386563814, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.020331712439656258}, {"id": 804, "seek": 422024, "start": 4244.76, "end": 4248.24, "text": " Now if you've forgotten what that means, go back and have a look at our implementation", "tokens": [51590, 823, 498, 291, 600, 11832, 437, 300, 1355, 11, 352, 646, 293, 362, 257, 574, 412, 527, 11420, 51764], "temperature": 0.0, "avg_logprob": -0.26160370386563814, "compression_ratio": 1.7543103448275863, "no_speech_prob": 0.020331712439656258}, {"id": 805, "seek": 424824, "start": 4248.24, "end": 4252.639999999999, "text": " from scratch of batch norm, because the batch norm weights is the thing we multiply by.", "tokens": [50364, 490, 8459, 295, 15245, 2026, 11, 570, 264, 15245, 2026, 17443, 307, 264, 551, 321, 12972, 538, 13, 50584], "temperature": 0.0, "avg_logprob": -0.3463690948486328, "compression_ratio": 1.9389671361502347, "no_speech_prob": 0.006488251034170389}, {"id": 806, "seek": 424824, "start": 4252.639999999999, "end": 4254.96, "text": " So do you remember the batch norm?", "tokens": [50584, 407, 360, 291, 1604, 264, 15245, 2026, 30, 50700], "temperature": 0.0, "avg_logprob": -0.3463690948486328, "compression_ratio": 1.9389671361502347, "no_speech_prob": 0.006488251034170389}, {"id": 807, "seek": 424824, "start": 4254.96, "end": 4261.42, "text": " We, we subtract the exponential moving average mean.", "tokens": [50700, 492, 11, 321, 16390, 264, 21510, 2684, 4274, 914, 13, 51023], "temperature": 0.0, "avg_logprob": -0.3463690948486328, "compression_ratio": 1.9389671361502347, "no_speech_prob": 0.006488251034170389}, {"id": 808, "seek": 424824, "start": 4261.42, "end": 4268.32, "text": " We divide by the exponential moving average standard deviation, but then we add back the,", "tokens": [51023, 492, 9845, 538, 264, 21510, 2684, 4274, 3832, 25163, 11, 457, 550, 321, 909, 646, 264, 11, 51368], "temperature": 0.0, "avg_logprob": -0.3463690948486328, "compression_ratio": 1.9389671361502347, "no_speech_prob": 0.006488251034170389}, {"id": 809, "seek": 424824, "start": 4268.32, "end": 4273.44, "text": " the kind of the, the, the batch norms bias layer, and we multiply by the batch norms", "tokens": [51368, 264, 733, 295, 264, 11, 264, 11, 264, 15245, 24357, 12577, 4583, 11, 293, 321, 12972, 538, 264, 15245, 24357, 51624], "temperature": 0.0, "avg_logprob": -0.3463690948486328, "compression_ratio": 1.9389671361502347, "no_speech_prob": 0.006488251034170389}, {"id": 810, "seek": 424824, "start": 4273.44, "end": 4277.139999999999, "text": " weights, well the other way around, multiply by weights first.", "tokens": [51624, 17443, 11, 731, 264, 661, 636, 926, 11, 12972, 538, 17443, 700, 13, 51809], "temperature": 0.0, "avg_logprob": -0.3463690948486328, "compression_ratio": 1.9389671361502347, "no_speech_prob": 0.006488251034170389}, {"id": 811, "seek": 427714, "start": 4277.14, "end": 4281.740000000001, "text": " So if we set the batch norm layers weights to zero, we're multiplying by zero.", "tokens": [50364, 407, 498, 321, 992, 264, 15245, 2026, 7914, 17443, 281, 4018, 11, 321, 434, 30955, 538, 4018, 13, 50594], "temperature": 0.0, "avg_logprob": -0.21058096634714227, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0001376545988023281}, {"id": 812, "seek": 427714, "start": 4281.740000000001, "end": 4288.54, "text": " And so this will cause the initial conv block output to be just all zeros.", "tokens": [50594, 400, 370, 341, 486, 3082, 264, 5883, 3754, 3461, 5598, 281, 312, 445, 439, 35193, 13, 50934], "temperature": 0.0, "avg_logprob": -0.21058096634714227, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0001376545988023281}, {"id": 813, "seek": 427714, "start": 4288.54, "end": 4294.46, "text": " And so that's going to give us what we wanted, is that nothing's happening here.", "tokens": [50934, 400, 370, 300, 311, 516, 281, 976, 505, 437, 321, 1415, 11, 307, 300, 1825, 311, 2737, 510, 13, 51230], "temperature": 0.0, "avg_logprob": -0.21058096634714227, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0001376545988023281}, {"id": 814, "seek": 427714, "start": 4294.46, "end": 4301.02, "text": " So we just end up with the input with this possible id conv.", "tokens": [51230, 407, 321, 445, 917, 493, 365, 264, 4846, 365, 341, 1944, 4496, 3754, 13, 51558], "temperature": 0.0, "avg_logprob": -0.21058096634714227, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0001376545988023281}, {"id": 815, "seek": 427714, "start": 4301.02, "end": 4307.0, "text": " So a res block is going to contain those convolutions in the convolution block.", "tokens": [51558, 407, 257, 725, 3461, 307, 516, 281, 5304, 729, 3754, 15892, 294, 264, 45216, 3461, 13, 51857], "temperature": 0.0, "avg_logprob": -0.21058096634714227, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0001376545988023281}, {"id": 816, "seek": 430700, "start": 4307.86, "end": 4309.18, "text": " We just discussed, right?", "tokens": [50407, 492, 445, 7152, 11, 558, 30, 50473], "temperature": 0.0, "avg_logprob": -0.24845286282626064, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0003406287869438529}, {"id": 817, "seek": 430700, "start": 4309.18, "end": 4311.56, "text": " And then we're going to need this id conv.", "tokens": [50473, 400, 550, 321, 434, 516, 281, 643, 341, 4496, 3754, 13, 50592], "temperature": 0.0, "avg_logprob": -0.24845286282626064, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0003406287869438529}, {"id": 818, "seek": 430700, "start": 4311.56, "end": 4318.52, "text": " So the id conv is going to be a no op, so that's nothing at all, if the number of channels", "tokens": [50592, 407, 264, 4496, 3754, 307, 516, 281, 312, 257, 572, 999, 11, 370, 300, 311, 1825, 412, 439, 11, 498, 264, 1230, 295, 9235, 50940], "temperature": 0.0, "avg_logprob": -0.24845286282626064, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0003406287869438529}, {"id": 819, "seek": 430700, "start": 4318.52, "end": 4321.14, "text": " in is equal to the number of channels out.", "tokens": [50940, 294, 307, 2681, 281, 264, 1230, 295, 9235, 484, 13, 51071], "temperature": 0.0, "avg_logprob": -0.24845286282626064, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0003406287869438529}, {"id": 820, "seek": 430700, "start": 4321.14, "end": 4325.8, "text": " But otherwise we're going to use a convolution with a kernel size of one and a stride of", "tokens": [51071, 583, 5911, 321, 434, 516, 281, 764, 257, 45216, 365, 257, 28256, 2744, 295, 472, 293, 257, 1056, 482, 295, 51304], "temperature": 0.0, "avg_logprob": -0.24845286282626064, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0003406287869438529}, {"id": 821, "seek": 430700, "start": 4325.8, "end": 4327.22, "text": " one.", "tokens": [51304, 472, 13, 51375], "temperature": 0.0, "avg_logprob": -0.24845286282626064, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0003406287869438529}, {"id": 822, "seek": 430700, "start": 4327.22, "end": 4335.2, "text": " And so that is going to, you know, is with as little work as possible, change the number", "tokens": [51375, 400, 370, 300, 307, 516, 281, 11, 291, 458, 11, 307, 365, 382, 707, 589, 382, 1944, 11, 1319, 264, 1230, 51774], "temperature": 0.0, "avg_logprob": -0.24845286282626064, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0003406287869438529}, {"id": 823, "seek": 433520, "start": 4335.36, "end": 4338.599999999999, "text": " of filters so that they match.", "tokens": [50372, 295, 15995, 370, 300, 436, 2995, 13, 50534], "temperature": 0.0, "avg_logprob": -0.26059068697635257, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.019718831405043602}, {"id": 824, "seek": 433520, "start": 4338.599999999999, "end": 4341.4, "text": " Also what if the stride's not one?", "tokens": [50534, 2743, 437, 498, 264, 1056, 482, 311, 406, 472, 30, 50674], "temperature": 0.0, "avg_logprob": -0.26059068697635257, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.019718831405043602}, {"id": 825, "seek": 433520, "start": 4341.4, "end": 4345.42, "text": " Well if the stride is two, actually this isn't going to work for any stride, this only works", "tokens": [50674, 1042, 498, 264, 1056, 482, 307, 732, 11, 767, 341, 1943, 380, 516, 281, 589, 337, 604, 1056, 482, 11, 341, 787, 1985, 50875], "temperature": 0.0, "avg_logprob": -0.26059068697635257, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.019718831405043602}, {"id": 826, "seek": 433520, "start": 4345.42, "end": 4346.92, "text": " for a stride of two.", "tokens": [50875, 337, 257, 1056, 482, 295, 732, 13, 50950], "temperature": 0.0, "avg_logprob": -0.26059068697635257, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.019718831405043602}, {"id": 827, "seek": 433520, "start": 4346.92, "end": 4351.58, "text": " If there's a stride of two, we will simply average using average pooling.", "tokens": [50950, 759, 456, 311, 257, 1056, 482, 295, 732, 11, 321, 486, 2935, 4274, 1228, 4274, 7005, 278, 13, 51183], "temperature": 0.0, "avg_logprob": -0.26059068697635257, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.019718831405043602}, {"id": 828, "seek": 433520, "start": 4351.58, "end": 4362.5199999999995, "text": " So this is just saying take the mean of every set of two items in the grid.", "tokens": [51183, 407, 341, 307, 445, 1566, 747, 264, 914, 295, 633, 992, 295, 732, 4754, 294, 264, 10748, 13, 51730], "temperature": 0.0, "avg_logprob": -0.26059068697635257, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.019718831405043602}, {"id": 829, "seek": 433520, "start": 4362.5199999999995, "end": 4364.66, "text": " So we'll just take the mean.", "tokens": [51730, 407, 321, 603, 445, 747, 264, 914, 13, 51837], "temperature": 0.0, "avg_logprob": -0.26059068697635257, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.019718831405043602}, {"id": 830, "seek": 436466, "start": 4364.66, "end": 4377.98, "text": " So we basically have here pool of id conv of in, if the stride is two and if the filtered", "tokens": [50364, 407, 321, 1936, 362, 510, 7005, 295, 4496, 3754, 295, 294, 11, 498, 264, 1056, 482, 307, 732, 293, 498, 264, 37111, 51030], "temperature": 0.0, "avg_logprob": -0.2832315578017124, "compression_ratio": 1.5846994535519126, "no_speech_prob": 8.220175368478522e-05}, {"id": 831, "seek": 436466, "start": 4377.98, "end": 4380.58, "text": " number is changed.", "tokens": [51030, 1230, 307, 3105, 13, 51160], "temperature": 0.0, "avg_logprob": -0.2832315578017124, "compression_ratio": 1.5846994535519126, "no_speech_prob": 8.220175368478522e-05}, {"id": 832, "seek": 436466, "start": 4380.58, "end": 4382.42, "text": " And so that's the minimal amount of work.", "tokens": [51160, 400, 370, 300, 311, 264, 13206, 2372, 295, 589, 13, 51252], "temperature": 0.0, "avg_logprob": -0.2832315578017124, "compression_ratio": 1.5846994535519126, "no_speech_prob": 8.220175368478522e-05}, {"id": 833, "seek": 436466, "start": 4382.42, "end": 4384.9, "text": " So here it is, here is the forward pass.", "tokens": [51252, 407, 510, 309, 307, 11, 510, 307, 264, 2128, 1320, 13, 51376], "temperature": 0.0, "avg_logprob": -0.2832315578017124, "compression_ratio": 1.5846994535519126, "no_speech_prob": 8.220175368478522e-05}, {"id": 834, "seek": 436466, "start": 4384.9, "end": 4392.86, "text": " We get our input and on the identity connection we call pool and if stride is one, that's", "tokens": [51376, 492, 483, 527, 4846, 293, 322, 264, 6575, 4984, 321, 818, 7005, 293, 498, 1056, 482, 307, 472, 11, 300, 311, 51774], "temperature": 0.0, "avg_logprob": -0.2832315578017124, "compression_ratio": 1.5846994535519126, "no_speech_prob": 8.220175368478522e-05}, {"id": 835, "seek": 436466, "start": 4392.86, "end": 4393.86, "text": " a no op.", "tokens": [51774, 257, 572, 999, 13, 51824], "temperature": 0.0, "avg_logprob": -0.2832315578017124, "compression_ratio": 1.5846994535519126, "no_speech_prob": 8.220175368478522e-05}, {"id": 836, "seek": 439386, "start": 4394.0599999999995, "end": 4395.62, "text": " So do nothing at all.", "tokens": [50374, 407, 360, 1825, 412, 439, 13, 50452], "temperature": 0.0, "avg_logprob": -0.2531979062141628, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.000209882840863429}, {"id": 837, "seek": 439386, "start": 4395.62, "end": 4400.839999999999, "text": " We do id conv and if the number of filters is not changed, that's also a no op.", "tokens": [50452, 492, 360, 4496, 3754, 293, 498, 264, 1230, 295, 15995, 307, 406, 3105, 11, 300, 311, 611, 257, 572, 999, 13, 50713], "temperature": 0.0, "avg_logprob": -0.2531979062141628, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.000209882840863429}, {"id": 838, "seek": 439386, "start": 4400.839999999999, "end": 4405.259999999999, "text": " So this is, this is just the input in that situation.", "tokens": [50713, 407, 341, 307, 11, 341, 307, 445, 264, 4846, 294, 300, 2590, 13, 50934], "temperature": 0.0, "avg_logprob": -0.2531979062141628, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.000209882840863429}, {"id": 839, "seek": 439386, "start": 4405.259999999999, "end": 4409.58, "text": " And then we add that to the result of the convs.", "tokens": [50934, 400, 550, 321, 909, 300, 281, 264, 1874, 295, 264, 3754, 82, 13, 51150], "temperature": 0.0, "avg_logprob": -0.2531979062141628, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.000209882840863429}, {"id": 840, "seek": 439386, "start": 4409.58, "end": 4414.38, "text": " And here's something interesting, we then apply the activation function to the whole", "tokens": [51150, 400, 510, 311, 746, 1880, 11, 321, 550, 3079, 264, 24433, 2445, 281, 264, 1379, 51390], "temperature": 0.0, "avg_logprob": -0.2531979062141628, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.000209882840863429}, {"id": 841, "seek": 439386, "start": 4414.38, "end": 4415.38, "text": " thing.", "tokens": [51390, 551, 13, 51440], "temperature": 0.0, "avg_logprob": -0.2531979062141628, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.000209882840863429}, {"id": 842, "seek": 439386, "start": 4415.38, "end": 4421.54, "text": " Okay, so that way, I wouldn't say this is like the only way you can do it, but this", "tokens": [51440, 1033, 11, 370, 300, 636, 11, 286, 2759, 380, 584, 341, 307, 411, 264, 787, 636, 291, 393, 360, 309, 11, 457, 341, 51748], "temperature": 0.0, "avg_logprob": -0.2531979062141628, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.000209882840863429}, {"id": 843, "seek": 442154, "start": 4422.1, "end": 4428.42, "text": " is a way that works pretty well, is to apply the activation function to the result of the", "tokens": [50392, 307, 257, 636, 300, 1985, 1238, 731, 11, 307, 281, 3079, 264, 24433, 2445, 281, 264, 1874, 295, 264, 50708], "temperature": 0.0, "avg_logprob": -0.22715344993017053, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.004982237238436937}, {"id": 844, "seek": 442154, "start": 4428.42, "end": 4432.62, "text": " whole, the whole resnet block.", "tokens": [50708, 1379, 11, 264, 1379, 725, 7129, 3461, 13, 50918], "temperature": 0.0, "avg_logprob": -0.22715344993017053, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.004982237238436937}, {"id": 845, "seek": 442154, "start": 4432.62, "end": 4437.1, "text": " And that's why I didn't add activation function to the second conv.", "tokens": [50918, 400, 300, 311, 983, 286, 994, 380, 909, 24433, 2445, 281, 264, 1150, 3754, 13, 51142], "temperature": 0.0, "avg_logprob": -0.22715344993017053, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.004982237238436937}, {"id": 846, "seek": 442154, "start": 4437.1, "end": 4438.1, "text": " So that's a res block.", "tokens": [51142, 407, 300, 311, 257, 725, 3461, 13, 51192], "temperature": 0.0, "avg_logprob": -0.22715344993017053, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.004982237238436937}, {"id": 847, "seek": 442154, "start": 4438.1, "end": 4443.48, "text": " So it's not a huge amount of code, right?", "tokens": [51192, 407, 309, 311, 406, 257, 2603, 2372, 295, 3089, 11, 558, 30, 51461], "temperature": 0.0, "avg_logprob": -0.22715344993017053, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.004982237238436937}, {"id": 848, "seek": 442154, "start": 4443.48, "end": 4448.3, "text": " And so now I've literally copied and pasted our get model, but everywhere that previously", "tokens": [51461, 400, 370, 586, 286, 600, 3736, 25365, 293, 1791, 292, 527, 483, 2316, 11, 457, 5315, 300, 8046, 51702], "temperature": 0.0, "avg_logprob": -0.22715344993017053, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.004982237238436937}, {"id": 849, "seek": 444830, "start": 4448.3, "end": 4452.68, "text": " we had a conv, I've just replaced it with res block.", "tokens": [50364, 321, 632, 257, 3754, 11, 286, 600, 445, 10772, 309, 365, 725, 3461, 13, 50583], "temperature": 0.0, "avg_logprob": -0.32907102274340255, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.04672333970665932}, {"id": 850, "seek": 444830, "start": 4452.68, "end": 4457.820000000001, "text": " In fact, let's have a look.", "tokens": [50583, 682, 1186, 11, 718, 311, 362, 257, 574, 13, 50840], "temperature": 0.0, "avg_logprob": -0.32907102274340255, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.04672333970665932}, {"id": 851, "seek": 444830, "start": 4457.820000000001, "end": 4461.42, "text": " Get model.", "tokens": [50840, 3240, 2316, 13, 51020], "temperature": 0.0, "avg_logprob": -0.32907102274340255, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.04672333970665932}, {"id": 852, "seek": 444830, "start": 4461.42, "end": 4471.62, "text": " Okay, so previously we started with conv 1 to 8, now we do res block 1 to 8, stride 1,", "tokens": [51020, 1033, 11, 370, 8046, 321, 1409, 365, 3754, 502, 281, 1649, 11, 586, 321, 360, 725, 3461, 502, 281, 1649, 11, 1056, 482, 502, 11, 51530], "temperature": 0.0, "avg_logprob": -0.32907102274340255, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.04672333970665932}, {"id": 853, "seek": 444830, "start": 4471.62, "end": 4472.62, "text": " stride 1.", "tokens": [51530, 1056, 482, 502, 13, 51580], "temperature": 0.0, "avg_logprob": -0.32907102274340255, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.04672333970665932}, {"id": 854, "seek": 444830, "start": 4472.62, "end": 4476.1, "text": " Then we added conv from number of filters i to number of filters i plus 1.", "tokens": [51580, 1396, 321, 3869, 3754, 490, 1230, 295, 15995, 741, 281, 1230, 295, 15995, 741, 1804, 502, 13, 51754], "temperature": 0.0, "avg_logprob": -0.32907102274340255, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.04672333970665932}, {"id": 855, "seek": 447610, "start": 4476.1, "end": 4479.22, "text": " Now it's res block from number of filters, number of filters i plus 1.", "tokens": [50364, 823, 309, 311, 725, 3461, 490, 1230, 295, 15995, 11, 1230, 295, 15995, 741, 1804, 502, 13, 50520], "temperature": 0.0, "avg_logprob": -0.2779709866172389, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.013636717572808266}, {"id": 856, "seek": 447610, "start": 4479.22, "end": 4486.860000000001, "text": " Okay, so it's exactly the same.", "tokens": [50520, 1033, 11, 370, 309, 311, 2293, 264, 912, 13, 50902], "temperature": 0.0, "avg_logprob": -0.2779709866172389, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.013636717572808266}, {"id": 857, "seek": 447610, "start": 4486.860000000001, "end": 4490.9800000000005, "text": " One change I have made though, is, I mean, it doesn't actually make any difference at", "tokens": [50902, 1485, 1319, 286, 362, 1027, 1673, 11, 307, 11, 286, 914, 11, 309, 1177, 380, 767, 652, 604, 2649, 412, 51108], "temperature": 0.0, "avg_logprob": -0.2779709866172389, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.013636717572808266}, {"id": 858, "seek": 447610, "start": 4490.9800000000005, "end": 4497.22, "text": " all, I think it's mathematically identical, is previously the very last conv at the end", "tokens": [51108, 439, 11, 286, 519, 309, 311, 44003, 14800, 11, 307, 8046, 264, 588, 1036, 3754, 412, 264, 917, 51420], "temperature": 0.0, "avg_logprob": -0.2779709866172389, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.013636717572808266}, {"id": 859, "seek": 447610, "start": 4497.22, "end": 4505.820000000001, "text": " went from the, you know, 128 channels down to the 10 channels, followed by flatten.", "tokens": [51420, 1437, 490, 264, 11, 291, 458, 11, 29810, 9235, 760, 281, 264, 1266, 9235, 11, 6263, 538, 24183, 13, 51850], "temperature": 0.0, "avg_logprob": -0.2779709866172389, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.013636717572808266}, {"id": 860, "seek": 450582, "start": 4506.54, "end": 4509.74, "text": " But this conv is actually working on a 1 by 1 input.", "tokens": [50400, 583, 341, 3754, 307, 767, 1364, 322, 257, 502, 538, 502, 4846, 13, 50560], "temperature": 0.0, "avg_logprob": -0.2285352283053928, "compression_ratio": 1.7046413502109705, "no_speech_prob": 0.0004878548497799784}, {"id": 861, "seek": 450582, "start": 4509.74, "end": 4514.66, "text": " So you know, an alternate way, but I think makes it clearer, is flatten first and then", "tokens": [50560, 407, 291, 458, 11, 364, 18873, 636, 11, 457, 286, 519, 1669, 309, 26131, 11, 307, 24183, 700, 293, 550, 50806], "temperature": 0.0, "avg_logprob": -0.2285352283053928, "compression_ratio": 1.7046413502109705, "no_speech_prob": 0.0004878548497799784}, {"id": 862, "seek": 450582, "start": 4514.66, "end": 4516.38, "text": " use a linear layer.", "tokens": [50806, 764, 257, 8213, 4583, 13, 50892], "temperature": 0.0, "avg_logprob": -0.2285352283053928, "compression_ratio": 1.7046413502109705, "no_speech_prob": 0.0004878548497799784}, {"id": 863, "seek": 450582, "start": 4516.38, "end": 4521.259999999999, "text": " Because a conv on a 1 by 1 input is identical to a linear layer.", "tokens": [50892, 1436, 257, 3754, 322, 257, 502, 538, 502, 4846, 307, 14800, 281, 257, 8213, 4583, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2285352283053928, "compression_ratio": 1.7046413502109705, "no_speech_prob": 0.0004878548497799784}, {"id": 864, "seek": 450582, "start": 4521.259999999999, "end": 4525.42, "text": " And if that doesn't immediately make sense, that's totally fine, but this is one of those", "tokens": [51136, 400, 498, 300, 1177, 380, 4258, 652, 2020, 11, 300, 311, 3879, 2489, 11, 457, 341, 307, 472, 295, 729, 51344], "temperature": 0.0, "avg_logprob": -0.2285352283053928, "compression_ratio": 1.7046413502109705, "no_speech_prob": 0.0004878548497799784}, {"id": 865, "seek": 450582, "start": 4525.42, "end": 4531.259999999999, "text": " places where you should pause and have a little stop and think about why a conv on a 1 by", "tokens": [51344, 3190, 689, 291, 820, 10465, 293, 362, 257, 707, 1590, 293, 519, 466, 983, 257, 3754, 322, 257, 502, 538, 51636], "temperature": 0.0, "avg_logprob": -0.2285352283053928, "compression_ratio": 1.7046413502109705, "no_speech_prob": 0.0004878548497799784}, {"id": 866, "seek": 453126, "start": 4531.26, "end": 4536.9800000000005, "text": " 1 is the same, and maybe go back to the Excel spreadsheet if you like, or the Python from", "tokens": [50364, 502, 307, 264, 912, 11, 293, 1310, 352, 646, 281, 264, 19060, 27733, 498, 291, 411, 11, 420, 264, 15329, 490, 50650], "temperature": 0.0, "avg_logprob": -0.24431713972941482, "compression_ratio": 1.5546875, "no_speech_prob": 0.5581719875335693}, {"id": 867, "seek": 453126, "start": 4536.9800000000005, "end": 4541.900000000001, "text": " scratch conv we did, because this is a very important insight.", "tokens": [50650, 8459, 3754, 321, 630, 11, 570, 341, 307, 257, 588, 1021, 11269, 13, 50896], "temperature": 0.0, "avg_logprob": -0.24431713972941482, "compression_ratio": 1.5546875, "no_speech_prob": 0.5581719875335693}, {"id": 868, "seek": 453126, "start": 4541.900000000001, "end": 4546.34, "text": " So I think it's very useful with a more complex model like this to take a good old look at", "tokens": [50896, 407, 286, 519, 309, 311, 588, 4420, 365, 257, 544, 3997, 2316, 411, 341, 281, 747, 257, 665, 1331, 574, 412, 51118], "temperature": 0.0, "avg_logprob": -0.24431713972941482, "compression_ratio": 1.5546875, "no_speech_prob": 0.5581719875335693}, {"id": 869, "seek": 453126, "start": 4546.34, "end": 4551.54, "text": " it, to see exactly what the inputs and outputs of each layer is.", "tokens": [51118, 309, 11, 281, 536, 2293, 437, 264, 15743, 293, 23930, 295, 1184, 4583, 307, 13, 51378], "temperature": 0.0, "avg_logprob": -0.24431713972941482, "compression_ratio": 1.5546875, "no_speech_prob": 0.5581719875335693}, {"id": 870, "seek": 453126, "start": 4551.54, "end": 4557.02, "text": " So here's a little function called print shape, which takes the things that a hook takes.", "tokens": [51378, 407, 510, 311, 257, 707, 2445, 1219, 4482, 3909, 11, 597, 2516, 264, 721, 300, 257, 6328, 2516, 13, 51652], "temperature": 0.0, "avg_logprob": -0.24431713972941482, "compression_ratio": 1.5546875, "no_speech_prob": 0.5581719875335693}, {"id": 871, "seek": 455702, "start": 4557.02, "end": 4564.42, "text": " And we will print out for each layer the name of the class, the shape of the input,", "tokens": [50364, 400, 321, 486, 4482, 484, 337, 1184, 4583, 264, 1315, 295, 264, 1508, 11, 264, 3909, 295, 264, 4846, 11, 50734], "temperature": 0.0, "avg_logprob": -0.23444163144289792, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.02161462977528572}, {"id": 872, "seek": 455702, "start": 4564.42, "end": 4565.88, "text": " and the shape of the output.", "tokens": [50734, 293, 264, 3909, 295, 264, 5598, 13, 50807], "temperature": 0.0, "avg_logprob": -0.23444163144289792, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.02161462977528572}, {"id": 873, "seek": 455702, "start": 4565.88, "end": 4572.3, "text": " So we can get our model, create our learner, and use our handy little hooks context manager", "tokens": [50807, 407, 321, 393, 483, 527, 2316, 11, 1884, 527, 33347, 11, 293, 764, 527, 13239, 707, 26485, 4319, 6598, 51128], "temperature": 0.0, "avg_logprob": -0.23444163144289792, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.02161462977528572}, {"id": 874, "seek": 455702, "start": 4572.3, "end": 4577.42, "text": " we built in an earlier lesson, and call the print shape function.", "tokens": [51128, 321, 3094, 294, 364, 3071, 6898, 11, 293, 818, 264, 4482, 3909, 2445, 13, 51384], "temperature": 0.0, "avg_logprob": -0.23444163144289792, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.02161462977528572}, {"id": 875, "seek": 455702, "start": 4577.42, "end": 4583.56, "text": " And then we will call fit for one epoch, just doing the evaluation or the training.", "tokens": [51384, 400, 550, 321, 486, 818, 3318, 337, 472, 30992, 339, 11, 445, 884, 264, 13344, 420, 264, 3097, 13, 51691], "temperature": 0.0, "avg_logprob": -0.23444163144289792, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.02161462977528572}, {"id": 876, "seek": 458356, "start": 4583.56, "end": 4588.160000000001, "text": " And if we use the single batch callback, it'll just do a single batch, put, pass it", "tokens": [50364, 400, 498, 321, 764, 264, 2167, 15245, 818, 3207, 11, 309, 603, 445, 360, 257, 2167, 15245, 11, 829, 11, 1320, 309, 50594], "temperature": 0.0, "avg_logprob": -0.282843991934535, "compression_ratio": 1.491891891891892, "no_speech_prob": 0.013020132668316364}, {"id": 877, "seek": 458356, "start": 4588.160000000001, "end": 4598.6, "text": " through, and that hook will, as you see, print out each layer, the inputs shape, and the", "tokens": [50594, 807, 11, 293, 300, 6328, 486, 11, 382, 291, 536, 11, 4482, 484, 1184, 4583, 11, 264, 15743, 3909, 11, 293, 264, 51116], "temperature": 0.0, "avg_logprob": -0.282843991934535, "compression_ratio": 1.491891891891892, "no_speech_prob": 0.013020132668316364}, {"id": 878, "seek": 458356, "start": 4598.6, "end": 4602.52, "text": " output shape.", "tokens": [51116, 5598, 3909, 13, 51312], "temperature": 0.0, "avg_logprob": -0.282843991934535, "compression_ratio": 1.491891891891892, "no_speech_prob": 0.013020132668316364}, {"id": 879, "seek": 458356, "start": 4602.52, "end": 4609.56, "text": " So you can see we're starting with an input of batch size of 1024, one channel, 28 by", "tokens": [51312, 407, 291, 393, 536, 321, 434, 2891, 365, 364, 4846, 295, 15245, 2744, 295, 1266, 7911, 11, 472, 2269, 11, 7562, 538, 51664], "temperature": 0.0, "avg_logprob": -0.282843991934535, "compression_ratio": 1.491891891891892, "no_speech_prob": 0.013020132668316364}, {"id": 880, "seek": 458356, "start": 4609.56, "end": 4610.56, "text": " 28.", "tokens": [51664, 7562, 13, 51714], "temperature": 0.0, "avg_logprob": -0.282843991934535, "compression_ratio": 1.491891891891892, "no_speech_prob": 0.013020132668316364}, {"id": 881, "seek": 461056, "start": 4610.56, "end": 4615.6, "text": " This res block was stride one, so we still end up with 28 by 28, but now we've got eight", "tokens": [50364, 639, 725, 3461, 390, 1056, 482, 472, 11, 370, 321, 920, 917, 493, 365, 7562, 538, 7562, 11, 457, 586, 321, 600, 658, 3180, 50616], "temperature": 0.0, "avg_logprob": -0.2647376485390238, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.019123779609799385}, {"id": 882, "seek": 461056, "start": 4615.6, "end": 4616.6, "text": " channels.", "tokens": [50616, 9235, 13, 50666], "temperature": 0.0, "avg_logprob": -0.2647376485390238, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.019123779609799385}, {"id": 883, "seek": 461056, "start": 4616.6, "end": 4623.240000000001, "text": " And then we gradually decrease the grid size to 14, to 7, to 4, to 2, to 1, as we gradually", "tokens": [50666, 400, 550, 321, 13145, 11514, 264, 10748, 2744, 281, 3499, 11, 281, 1614, 11, 281, 1017, 11, 281, 568, 11, 281, 502, 11, 382, 321, 13145, 50998], "temperature": 0.0, "avg_logprob": -0.2647376485390238, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.019123779609799385}, {"id": 884, "seek": 461056, "start": 4623.240000000001, "end": 4625.52, "text": " increase the number of channels.", "tokens": [50998, 3488, 264, 1230, 295, 9235, 13, 51112], "temperature": 0.0, "avg_logprob": -0.2647376485390238, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.019123779609799385}, {"id": 885, "seek": 461056, "start": 4625.52, "end": 4633.160000000001, "text": " We then flatten it, which gets rid of that one by one, which allows us then to do linear,", "tokens": [51112, 492, 550, 24183, 309, 11, 597, 2170, 3973, 295, 300, 472, 538, 472, 11, 597, 4045, 505, 550, 281, 360, 8213, 11, 51494], "temperature": 0.0, "avg_logprob": -0.2647376485390238, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.019123779609799385}, {"id": 886, "seek": 461056, "start": 4633.160000000001, "end": 4636.72, "text": " to go on to the 10.", "tokens": [51494, 281, 352, 322, 281, 264, 1266, 13, 51672], "temperature": 0.0, "avg_logprob": -0.2647376485390238, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.019123779609799385}, {"id": 887, "seek": 463672, "start": 4636.72, "end": 4642.2, "text": " And then there's some discussion about whether you want a batch norm at the end or not.", "tokens": [50364, 400, 550, 456, 311, 512, 5017, 466, 1968, 291, 528, 257, 15245, 2026, 412, 264, 917, 420, 406, 13, 50638], "temperature": 0.0, "avg_logprob": -0.2706424756483598, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.04672347381711006}, {"id": 888, "seek": 463672, "start": 4642.2, "end": 4648.2, "text": " I was finding it quite useful in this case, so we've got a batch norm at the end.", "tokens": [50638, 286, 390, 5006, 309, 1596, 4420, 294, 341, 1389, 11, 370, 321, 600, 658, 257, 15245, 2026, 412, 264, 917, 13, 50938], "temperature": 0.0, "avg_logprob": -0.2706424756483598, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.04672347381711006}, {"id": 889, "seek": 463672, "start": 4648.2, "end": 4656.0, "text": " I think this is very useful, so I decided to create a patch for learner called summary", "tokens": [50938, 286, 519, 341, 307, 588, 4420, 11, 370, 286, 3047, 281, 1884, 257, 9972, 337, 33347, 1219, 12691, 51328], "temperature": 0.0, "avg_logprob": -0.2706424756483598, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.04672347381711006}, {"id": 890, "seek": 463672, "start": 4656.0, "end": 4666.6, "text": " that would do basically exactly the same thing, but it would do it as a markdown table.", "tokens": [51328, 300, 576, 360, 1936, 2293, 264, 912, 551, 11, 457, 309, 576, 360, 309, 382, 257, 1491, 5093, 3199, 13, 51858], "temperature": 0.0, "avg_logprob": -0.2706424756483598, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.04672347381711006}, {"id": 891, "seek": 466660, "start": 4667.4800000000005, "end": 4677.6, "text": " Okay, so if we create a train learner with our model and call dot summary, this method", "tokens": [50408, 1033, 11, 370, 498, 321, 1884, 257, 3847, 33347, 365, 527, 2316, 293, 818, 5893, 12691, 11, 341, 3170, 50914], "temperature": 0.0, "avg_logprob": -0.23426643284884366, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.000732175656594336}, {"id": 892, "seek": 466660, "start": 4677.6, "end": 4683.96, "text": " is now available because it's been patched, that method, into the learner.", "tokens": [50914, 307, 586, 2435, 570, 309, 311, 668, 9972, 292, 11, 300, 3170, 11, 666, 264, 33347, 13, 51232], "temperature": 0.0, "avg_logprob": -0.23426643284884366, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.000732175656594336}, {"id": 893, "seek": 466660, "start": 4683.96, "end": 4687.52, "text": " And it's going to do exactly the same thing as our print, but it does it more prettily", "tokens": [51232, 400, 309, 311, 516, 281, 360, 2293, 264, 912, 551, 382, 527, 4482, 11, 457, 309, 775, 309, 544, 45421, 953, 51410], "temperature": 0.0, "avg_logprob": -0.23426643284884366, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.000732175656594336}, {"id": 894, "seek": 466660, "start": 4687.52, "end": 4693.68, "text": " by using a markdown table, if it's in a notebook, otherwise it'll just print it.", "tokens": [51410, 538, 1228, 257, 1491, 5093, 3199, 11, 498, 309, 311, 294, 257, 21060, 11, 5911, 309, 603, 445, 4482, 309, 13, 51718], "temperature": 0.0, "avg_logprob": -0.23426643284884366, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.000732175656594336}, {"id": 895, "seek": 469368, "start": 4693.76, "end": 4697.72, "text": " So fastcore has a handy thing for keeping track if you're in a notebook, and in a notebook", "tokens": [50368, 407, 2370, 12352, 575, 257, 13239, 551, 337, 5145, 2837, 498, 291, 434, 294, 257, 21060, 11, 293, 294, 257, 21060, 50566], "temperature": 0.0, "avg_logprob": -0.21451746069866678, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.002323131076991558}, {"id": 896, "seek": 469368, "start": 4697.72, "end": 4705.08, "text": " to make something markdown, you can just use ipython.display.markdown, as you see.", "tokens": [50566, 281, 652, 746, 1491, 5093, 11, 291, 393, 445, 764, 28501, 88, 11943, 13, 35238, 13, 5638, 5093, 11, 382, 291, 536, 13, 50934], "temperature": 0.0, "avg_logprob": -0.21451746069866678, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.002323131076991558}, {"id": 897, "seek": 469368, "start": 4705.08, "end": 4708.64, "text": " And the other thing that I added, as well as the input and the output, is I thought", "tokens": [50934, 400, 264, 661, 551, 300, 286, 3869, 11, 382, 731, 382, 264, 4846, 293, 264, 5598, 11, 307, 286, 1194, 51112], "temperature": 0.0, "avg_logprob": -0.21451746069866678, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.002323131076991558}, {"id": 898, "seek": 469368, "start": 4708.64, "end": 4711.52, "text": " let's also add in the number of parameters.", "tokens": [51112, 718, 311, 611, 909, 294, 264, 1230, 295, 9834, 13, 51256], "temperature": 0.0, "avg_logprob": -0.21451746069866678, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.002323131076991558}, {"id": 899, "seek": 469368, "start": 4711.52, "end": 4718.780000000001, "text": " So we can calculate that, as we've seen before, by summing up the number of elements for each", "tokens": [51256, 407, 321, 393, 8873, 300, 11, 382, 321, 600, 1612, 949, 11, 538, 2408, 2810, 493, 264, 1230, 295, 4959, 337, 1184, 51619], "temperature": 0.0, "avg_logprob": -0.21451746069866678, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.002323131076991558}, {"id": 900, "seek": 469368, "start": 4718.780000000001, "end": 4722.280000000001, "text": " parameter in that module.", "tokens": [51619, 13075, 294, 300, 10088, 13, 51794], "temperature": 0.0, "avg_logprob": -0.21451746069866678, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.002323131076991558}, {"id": 901, "seek": 472228, "start": 4722.28, "end": 4726.759999999999, "text": " And so then I've kind of kept track of that as well, so that at the end I can also print", "tokens": [50364, 400, 370, 550, 286, 600, 733, 295, 4305, 2837, 295, 300, 382, 731, 11, 370, 300, 412, 264, 917, 286, 393, 611, 4482, 50588], "temperature": 0.0, "avg_logprob": -0.19807934311200986, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0015487526543438435}, {"id": 902, "seek": 472228, "start": 4726.759999999999, "end": 4728.96, "text": " out the total number of parameters.", "tokens": [50588, 484, 264, 3217, 1230, 295, 9834, 13, 50698], "temperature": 0.0, "avg_logprob": -0.19807934311200986, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0015487526543438435}, {"id": 903, "seek": 472228, "start": 4728.96, "end": 4737.36, "text": " So we've got a 1.2 million parameter model, and you can see that there's very few parameters", "tokens": [50698, 407, 321, 600, 658, 257, 502, 13, 17, 2459, 13075, 2316, 11, 293, 291, 393, 536, 300, 456, 311, 588, 1326, 9834, 51118], "temperature": 0.0, "avg_logprob": -0.19807934311200986, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0015487526543438435}, {"id": 904, "seek": 472228, "start": 4737.36, "end": 4739.44, "text": " here in the input.", "tokens": [51118, 510, 294, 264, 4846, 13, 51222], "temperature": 0.0, "avg_logprob": -0.19807934311200986, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0015487526543438435}, {"id": 905, "seek": 472228, "start": 4739.44, "end": 4743.099999999999, "text": " Nearly all the parameters are actually in the last layer.", "tokens": [51222, 38000, 439, 264, 9834, 366, 767, 294, 264, 1036, 4583, 13, 51405], "temperature": 0.0, "avg_logprob": -0.19807934311200986, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0015487526543438435}, {"id": 906, "seek": 472228, "start": 4743.099999999999, "end": 4744.099999999999, "text": " Why is that?", "tokens": [51405, 1545, 307, 300, 30, 51455], "temperature": 0.0, "avg_logprob": -0.19807934311200986, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0015487526543438435}, {"id": 907, "seek": 472228, "start": 4744.099999999999, "end": 4748.219999999999, "text": " Well, you might want to go back to our Excel convolutional spreadsheet to see this.", "tokens": [51455, 1042, 11, 291, 1062, 528, 281, 352, 646, 281, 527, 19060, 45216, 304, 27733, 281, 536, 341, 13, 51661], "temperature": 0.0, "avg_logprob": -0.19807934311200986, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0015487526543438435}, {"id": 908, "seek": 474822, "start": 4748.22, "end": 4756.22, "text": " You have a parameter for every input channel.", "tokens": [50364, 509, 362, 257, 13075, 337, 633, 4846, 2269, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22758150834303637, "compression_ratio": 1.596026490066225, "no_speech_prob": 0.011868248693645}, {"id": 909, "seek": 474822, "start": 4756.22, "end": 4760.18, "text": " You have a set of parameters.", "tokens": [50764, 509, 362, 257, 992, 295, 9834, 13, 50962], "temperature": 0.0, "avg_logprob": -0.22758150834303637, "compression_ratio": 1.596026490066225, "no_speech_prob": 0.011868248693645}, {"id": 910, "seek": 474822, "start": 4760.18, "end": 4768.8, "text": " They're all going to get added up across each of the 3x3 in the kernel, and then that's", "tokens": [50962, 814, 434, 439, 516, 281, 483, 3869, 493, 2108, 1184, 295, 264, 805, 87, 18, 294, 264, 28256, 11, 293, 550, 300, 311, 51393], "temperature": 0.0, "avg_logprob": -0.22758150834303637, "compression_ratio": 1.596026490066225, "no_speech_prob": 0.011868248693645}, {"id": 911, "seek": 474822, "start": 4768.8, "end": 4773.76, "text": " going to be done for every output filter, every output channel that you want.", "tokens": [51393, 516, 281, 312, 1096, 337, 633, 5598, 6608, 11, 633, 5598, 2269, 300, 291, 528, 13, 51641], "temperature": 0.0, "avg_logprob": -0.22758150834303637, "compression_ratio": 1.596026490066225, "no_speech_prob": 0.011868248693645}, {"id": 912, "seek": 477376, "start": 4773.76, "end": 4785.12, "text": " So that's why you're going to end up with, in fact let's take a look.", "tokens": [50364, 407, 300, 311, 983, 291, 434, 516, 281, 917, 493, 365, 11, 294, 1186, 718, 311, 747, 257, 574, 13, 50932], "temperature": 0.0, "avg_logprob": -0.228512079287798, "compression_ratio": 1.2115384615384615, "no_speech_prob": 0.2750452756881714}, {"id": 913, "seek": 477376, "start": 4785.12, "end": 4791.4800000000005, "text": " Maybe let's create, let's just grab some particular one.", "tokens": [50932, 2704, 718, 311, 1884, 11, 718, 311, 445, 4444, 512, 1729, 472, 13, 51250], "temperature": 0.0, "avg_logprob": -0.228512079287798, "compression_ratio": 1.2115384615384615, "no_speech_prob": 0.2750452756881714}, {"id": 914, "seek": 479148, "start": 4791.48, "end": 4807.5199999999995, "text": " So create our model, and so we'll just have a look at the sizes.", "tokens": [50364, 407, 1884, 527, 2316, 11, 293, 370, 321, 603, 445, 362, 257, 574, 412, 264, 11602, 13, 51166], "temperature": 0.0, "avg_logprob": -0.3117901563644409, "compression_ratio": 1.203883495145631, "no_speech_prob": 0.01717533729970455}, {"id": 915, "seek": 479148, "start": 4807.5199999999995, "end": 4818.5599999999995, "text": " And so you can see here there is this 256 by 256 by 3 by 3.", "tokens": [51166, 400, 370, 291, 393, 536, 510, 456, 307, 341, 38882, 538, 38882, 538, 805, 538, 805, 13, 51718], "temperature": 0.0, "avg_logprob": -0.3117901563644409, "compression_ratio": 1.203883495145631, "no_speech_prob": 0.01717533729970455}, {"id": 916, "seek": 481856, "start": 4818.64, "end": 4822.240000000001, "text": " So that's a lot of parameters.", "tokens": [50368, 407, 300, 311, 257, 688, 295, 9834, 13, 50548], "temperature": 0.0, "avg_logprob": -0.32252941855901407, "compression_ratio": 1.4294117647058824, "no_speech_prob": 0.03021388314664364}, {"id": 917, "seek": 481856, "start": 4822.240000000001, "end": 4831.6, "text": " Okay so we can call lrfind on that, and get a sense of what kind of learning rate to use.", "tokens": [50548, 1033, 370, 321, 393, 818, 287, 81, 35072, 322, 300, 11, 293, 483, 257, 2020, 295, 437, 733, 295, 2539, 3314, 281, 764, 13, 51016], "temperature": 0.0, "avg_logprob": -0.32252941855901407, "compression_ratio": 1.4294117647058824, "no_speech_prob": 0.03021388314664364}, {"id": 918, "seek": 481856, "start": 4831.6, "end": 4835.120000000001, "text": " So I chose 2e neg 2, so 0.02.", "tokens": [51016, 407, 286, 5111, 568, 68, 2485, 568, 11, 370, 1958, 13, 12756, 13, 51192], "temperature": 0.0, "avg_logprob": -0.32252941855901407, "compression_ratio": 1.4294117647058824, "no_speech_prob": 0.03021388314664364}, {"id": 919, "seek": 481856, "start": 4835.120000000001, "end": 4837.360000000001, "text": " This is our standard learning thing.", "tokens": [51192, 639, 307, 527, 3832, 2539, 551, 13, 51304], "temperature": 0.0, "avg_logprob": -0.32252941855901407, "compression_ratio": 1.4294117647058824, "no_speech_prob": 0.03021388314664364}, {"id": 920, "seek": 481856, "start": 4837.360000000001, "end": 4840.84, "text": " You don't have to watch it train, I've just trained it.", "tokens": [51304, 509, 500, 380, 362, 281, 1159, 309, 3847, 11, 286, 600, 445, 8895, 309, 13, 51478], "temperature": 0.0, "avg_logprob": -0.32252941855901407, "compression_ratio": 1.4294117647058824, "no_speech_prob": 0.03021388314664364}, {"id": 921, "seek": 484084, "start": 4840.84, "end": 4849.32, "text": " And so look at this, by using resnet we've gone up from 91.7, this is just keeps getting", "tokens": [50364, 400, 370, 574, 412, 341, 11, 538, 1228, 725, 7129, 321, 600, 2780, 493, 490, 31064, 13, 22, 11, 341, 307, 445, 5965, 1242, 50788], "temperature": 0.0, "avg_logprob": -0.28475135885259156, "compression_ratio": 1.4414414414414414, "no_speech_prob": 0.0054692113772034645}, {"id": 922, "seek": 484084, "start": 4849.32, "end": 4853.58, "text": " better, 92.2 in 5 epochs.", "tokens": [50788, 1101, 11, 28225, 13, 17, 294, 1025, 30992, 28346, 13, 51001], "temperature": 0.0, "avg_logprob": -0.28475135885259156, "compression_ratio": 1.4414414414414414, "no_speech_prob": 0.0054692113772034645}, {"id": 923, "seek": 484084, "start": 4853.58, "end": 4857.08, "text": " So that's pretty nice.", "tokens": [51001, 407, 300, 311, 1238, 1481, 13, 51176], "temperature": 0.0, "avg_logprob": -0.28475135885259156, "compression_ratio": 1.4414414414414414, "no_speech_prob": 0.0054692113772034645}, {"id": 924, "seek": 484084, "start": 4857.08, "end": 4862.88, "text": " And you know this resnet is not anything fancy.", "tokens": [51176, 400, 291, 458, 341, 725, 7129, 307, 406, 1340, 10247, 13, 51466], "temperature": 0.0, "avg_logprob": -0.28475135885259156, "compression_ratio": 1.4414414414414414, "no_speech_prob": 0.0054692113772034645}, {"id": 925, "seek": 484084, "start": 4862.88, "end": 4865.88, "text": " It's the simplest possible res block, right.", "tokens": [51466, 467, 311, 264, 22811, 1944, 725, 3461, 11, 558, 13, 51616], "temperature": 0.0, "avg_logprob": -0.28475135885259156, "compression_ratio": 1.4414414414414414, "no_speech_prob": 0.0054692113772034645}, {"id": 926, "seek": 484084, "start": 4865.88, "end": 4870.52, "text": " The model is literally copied and pasted from before, and replace each place it said conv", "tokens": [51616, 440, 2316, 307, 3736, 25365, 293, 1791, 292, 490, 949, 11, 293, 7406, 1184, 1081, 309, 848, 3754, 51848], "temperature": 0.0, "avg_logprob": -0.28475135885259156, "compression_ratio": 1.4414414414414414, "no_speech_prob": 0.0054692113772034645}, {"id": 927, "seek": 487052, "start": 4871.200000000001, "end": 4872.56, "text": " with resblock.", "tokens": [50398, 365, 725, 28830, 13, 50466], "temperature": 0.0, "avg_logprob": -0.2531768035888672, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0005884072161279619}, {"id": 928, "seek": 487052, "start": 4872.56, "end": 4874.8, "text": " But we've just been thoughtful about it, you know.", "tokens": [50466, 583, 321, 600, 445, 668, 21566, 466, 309, 11, 291, 458, 13, 50578], "temperature": 0.0, "avg_logprob": -0.2531768035888672, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0005884072161279619}, {"id": 929, "seek": 487052, "start": 4874.8, "end": 4877.240000000001, "text": " And here's something very interesting.", "tokens": [50578, 400, 510, 311, 746, 588, 1880, 13, 50700], "temperature": 0.0, "avg_logprob": -0.2531768035888672, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0005884072161279619}, {"id": 930, "seek": 487052, "start": 4877.240000000001, "end": 4881.200000000001, "text": " We can actually try lots of other resnets by grabbing Tim.", "tokens": [50700, 492, 393, 767, 853, 3195, 295, 661, 725, 77, 1385, 538, 23771, 7172, 13, 50898], "temperature": 0.0, "avg_logprob": -0.2531768035888672, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0005884072161279619}, {"id": 931, "seek": 487052, "start": 4881.200000000001, "end": 4887.360000000001, "text": " So that's Ross Whiteman's PyTorch image model library.", "tokens": [50898, 407, 300, 311, 16140, 21693, 15023, 311, 9953, 51, 284, 339, 3256, 2316, 6405, 13, 51206], "temperature": 0.0, "avg_logprob": -0.2531768035888672, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0005884072161279619}, {"id": 932, "seek": 487052, "start": 4887.360000000001, "end": 4895.68, "text": " And if you call Tim.listmodels star resnet star, there's a lot of resnets.", "tokens": [51206, 400, 498, 291, 818, 7172, 13, 8264, 8014, 1625, 3543, 725, 7129, 3543, 11, 456, 311, 257, 688, 295, 725, 77, 1385, 13, 51622], "temperature": 0.0, "avg_logprob": -0.2531768035888672, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0005884072161279619}, {"id": 933, "seek": 487052, "start": 4895.68, "end": 4898.72, "text": " And I tried quite a few of them.", "tokens": [51622, 400, 286, 3031, 1596, 257, 1326, 295, 552, 13, 51774], "temperature": 0.0, "avg_logprob": -0.2531768035888672, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0005884072161279619}, {"id": 934, "seek": 489872, "start": 4898.72, "end": 4908.360000000001, "text": " Now one thing that's interesting is if you actually look at the source code for Tim,", "tokens": [50364, 823, 472, 551, 300, 311, 1880, 307, 498, 291, 767, 574, 412, 264, 4009, 3089, 337, 7172, 11, 50846], "temperature": 0.0, "avg_logprob": -0.25551949049297135, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.00025315600214526057}, {"id": 935, "seek": 489872, "start": 4908.360000000001, "end": 4917.64, "text": " you'll see that the various different resnets, like resnet 18, resnet 18d, resnet 10d, they're", "tokens": [50846, 291, 603, 536, 300, 264, 3683, 819, 725, 77, 1385, 11, 411, 725, 7129, 2443, 11, 725, 7129, 2443, 67, 11, 725, 7129, 1266, 67, 11, 436, 434, 51310], "temperature": 0.0, "avg_logprob": -0.25551949049297135, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.00025315600214526057}, {"id": 936, "seek": 489872, "start": 4917.64, "end": 4923.280000000001, "text": " defined in a very nice way using this very elegant configuration.", "tokens": [51310, 7642, 294, 257, 588, 1481, 636, 1228, 341, 588, 21117, 11694, 13, 51592], "temperature": 0.0, "avg_logprob": -0.25551949049297135, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.00025315600214526057}, {"id": 937, "seek": 489872, "start": 4923.280000000001, "end": 4924.76, "text": " You can see exactly what's different.", "tokens": [51592, 509, 393, 536, 2293, 437, 311, 819, 13, 51666], "temperature": 0.0, "avg_logprob": -0.25551949049297135, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.00025315600214526057}, {"id": 938, "seek": 492476, "start": 4924.8, "end": 4930.04, "text": " So there's basically only one line of code different between each different type of resnet", "tokens": [50366, 407, 456, 311, 1936, 787, 472, 1622, 295, 3089, 819, 1296, 1184, 819, 2010, 295, 725, 7129, 50628], "temperature": 0.0, "avg_logprob": -0.2389375847506236, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.0018385984003543854}, {"id": 939, "seek": 492476, "start": 4930.04, "end": 4931.92, "text": " for the main resnets.", "tokens": [50628, 337, 264, 2135, 725, 77, 1385, 13, 50722], "temperature": 0.0, "avg_logprob": -0.2389375847506236, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.0018385984003543854}, {"id": 940, "seek": 492476, "start": 4931.92, "end": 4940.16, "text": " And so what I did was I tried all the Tim models I could find, and I even tried importing", "tokens": [50722, 400, 370, 437, 286, 630, 390, 286, 3031, 439, 264, 7172, 5245, 286, 727, 915, 11, 293, 286, 754, 3031, 43866, 51134], "temperature": 0.0, "avg_logprob": -0.2389375847506236, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.0018385984003543854}, {"id": 941, "seek": 492476, "start": 4940.16, "end": 4947.280000000001, "text": " the underlying things and building my own resnets from those pieces.", "tokens": [51134, 264, 14217, 721, 293, 2390, 452, 1065, 725, 77, 1385, 490, 729, 3755, 13, 51490], "temperature": 0.0, "avg_logprob": -0.2389375847506236, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.0018385984003543854}, {"id": 942, "seek": 492476, "start": 4947.280000000001, "end": 4951.92, "text": " And the best I found was the resnet 18d.", "tokens": [51490, 400, 264, 1151, 286, 1352, 390, 264, 725, 7129, 2443, 67, 13, 51722], "temperature": 0.0, "avg_logprob": -0.2389375847506236, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.0018385984003543854}, {"id": 943, "seek": 495192, "start": 4951.92, "end": 4957.76, "text": " And if I train it in exactly the same way, I got to 92%.", "tokens": [50364, 400, 498, 286, 3847, 309, 294, 2293, 264, 912, 636, 11, 286, 658, 281, 28225, 6856, 50656], "temperature": 0.0, "avg_logprob": -0.2391137486010526, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00806188303977251}, {"id": 944, "seek": 495192, "start": 4957.76, "end": 4960.56, "text": " And so the interesting thing is you'll see that's less than our 92.2.", "tokens": [50656, 400, 370, 264, 1880, 551, 307, 291, 603, 536, 300, 311, 1570, 813, 527, 28225, 13, 17, 13, 50796], "temperature": 0.0, "avg_logprob": -0.2391137486010526, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00806188303977251}, {"id": 945, "seek": 495192, "start": 4960.56, "end": 4962.88, "text": " And it's not like I tried lots of things to get here.", "tokens": [50796, 400, 309, 311, 406, 411, 286, 3031, 3195, 295, 721, 281, 483, 510, 13, 50912], "temperature": 0.0, "avg_logprob": -0.2391137486010526, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00806188303977251}, {"id": 946, "seek": 495192, "start": 4962.88, "end": 4965.0, "text": " This is the very first thing I tried.", "tokens": [50912, 639, 307, 264, 588, 700, 551, 286, 3031, 13, 51018], "temperature": 0.0, "avg_logprob": -0.2391137486010526, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00806188303977251}, {"id": 947, "seek": 495192, "start": 4965.0, "end": 4970.28, "text": " Where else this resnet 18d was after trying lots and lots of different Tim models.", "tokens": [51018, 2305, 1646, 341, 725, 7129, 2443, 67, 390, 934, 1382, 3195, 293, 3195, 295, 819, 7172, 5245, 13, 51282], "temperature": 0.0, "avg_logprob": -0.2391137486010526, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00806188303977251}, {"id": 948, "seek": 495192, "start": 4970.28, "end": 4979.0, "text": " And so what this shows is that the just thoughtfully designed kind of basic architecture goes a", "tokens": [51282, 400, 370, 437, 341, 3110, 307, 300, 264, 445, 1194, 2277, 4761, 733, 295, 3875, 9482, 1709, 257, 51718], "temperature": 0.0, "avg_logprob": -0.2391137486010526, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00806188303977251}, {"id": 949, "seek": 495192, "start": 4979.0, "end": 4980.7, "text": " very long way.", "tokens": [51718, 588, 938, 636, 13, 51803], "temperature": 0.0, "avg_logprob": -0.2391137486010526, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00806188303977251}, {"id": 950, "seek": 498070, "start": 4980.7, "end": 4990.58, "text": " It's actually better for this problem than any of the PyTorch image model models resnets", "tokens": [50364, 467, 311, 767, 1101, 337, 341, 1154, 813, 604, 295, 264, 9953, 51, 284, 339, 3256, 2316, 5245, 725, 77, 1385, 50858], "temperature": 0.0, "avg_logprob": -0.24588021172417535, "compression_ratio": 1.55, "no_speech_prob": 0.001169507740996778}, {"id": 951, "seek": 498070, "start": 4990.58, "end": 4995.22, "text": " that I could try, that I could find.", "tokens": [50858, 300, 286, 727, 853, 11, 300, 286, 727, 915, 13, 51090], "temperature": 0.0, "avg_logprob": -0.24588021172417535, "compression_ratio": 1.55, "no_speech_prob": 0.001169507740996778}, {"id": 952, "seek": 498070, "start": 4995.22, "end": 4998.179999999999, "text": " So I think that's quite, quite amazing.", "tokens": [51090, 407, 286, 519, 300, 311, 1596, 11, 1596, 2243, 13, 51238], "temperature": 0.0, "avg_logprob": -0.24588021172417535, "compression_ratio": 1.55, "no_speech_prob": 0.001169507740996778}, {"id": 953, "seek": 498070, "start": 4998.179999999999, "end": 5000.0199999999995, "text": " Actually it's really cool, you know.", "tokens": [51238, 5135, 309, 311, 534, 1627, 11, 291, 458, 13, 51330], "temperature": 0.0, "avg_logprob": -0.24588021172417535, "compression_ratio": 1.55, "no_speech_prob": 0.001169507740996778}, {"id": 954, "seek": 498070, "start": 5000.0199999999995, "end": 5006.86, "text": " And it shows that you can create a state-of-the-art architecture just by using some common sense,", "tokens": [51330, 400, 309, 3110, 300, 291, 393, 1884, 257, 1785, 12, 2670, 12, 3322, 12, 446, 9482, 445, 538, 1228, 512, 2689, 2020, 11, 51672], "temperature": 0.0, "avg_logprob": -0.24588021172417535, "compression_ratio": 1.55, "no_speech_prob": 0.001169507740996778}, {"id": 955, "seek": 498070, "start": 5006.86, "end": 5007.86, "text": " you know.", "tokens": [51672, 291, 458, 13, 51722], "temperature": 0.0, "avg_logprob": -0.24588021172417535, "compression_ratio": 1.55, "no_speech_prob": 0.001169507740996778}, {"id": 956, "seek": 500786, "start": 5008.0199999999995, "end": 5014.099999999999, "text": " So I hope that's, I hope that's, yeah, I hope that's encouraging.", "tokens": [50372, 407, 286, 1454, 300, 311, 11, 286, 1454, 300, 311, 11, 1338, 11, 286, 1454, 300, 311, 14580, 13, 50676], "temperature": 0.0, "avg_logprob": -0.3212972204369235, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.03161788359284401}, {"id": 957, "seek": 500786, "start": 5014.099999999999, "end": 5018.98, "text": " So anyway, so we're up to 92.2%.", "tokens": [50676, 407, 4033, 11, 370, 321, 434, 493, 281, 28225, 13, 17, 6856, 50920], "temperature": 0.0, "avg_logprob": -0.3212972204369235, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.03161788359284401}, {"id": 958, "seek": 500786, "start": 5018.98, "end": 5025.62, "text": " We're not done yet.", "tokens": [50920, 492, 434, 406, 1096, 1939, 13, 51252], "temperature": 0.0, "avg_logprob": -0.3212972204369235, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.03161788359284401}, {"id": 959, "seek": 500786, "start": 5025.62, "end": 5029.62, "text": " Because we haven't even talked about data augmentation.", "tokens": [51252, 1436, 321, 2378, 380, 754, 2825, 466, 1412, 14501, 19631, 13, 51452], "temperature": 0.0, "avg_logprob": -0.3212972204369235, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.03161788359284401}, {"id": 960, "seek": 500786, "start": 5029.62, "end": 5034.98, "text": " All right, so let's keep going.", "tokens": [51452, 1057, 558, 11, 370, 718, 311, 1066, 516, 13, 51720], "temperature": 0.0, "avg_logprob": -0.3212972204369235, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.03161788359284401}, {"id": 961, "seek": 500786, "start": 5034.98, "end": 5037.299999999999, "text": " So we're going to make everything the same as before.", "tokens": [51720, 407, 321, 434, 516, 281, 652, 1203, 264, 912, 382, 949, 13, 51836], "temperature": 0.0, "avg_logprob": -0.3212972204369235, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.03161788359284401}, {"id": 962, "seek": 503730, "start": 5037.74, "end": 5043.9800000000005, "text": " But before we do data augmentation, we're going to try to improve our model even further,", "tokens": [50386, 583, 949, 321, 360, 1412, 14501, 19631, 11, 321, 434, 516, 281, 853, 281, 3470, 527, 2316, 754, 3052, 11, 50698], "temperature": 0.0, "avg_logprob": -0.19527333848019865, "compression_ratio": 1.5324675324675325, "no_speech_prob": 0.00013765435141976923}, {"id": 963, "seek": 503730, "start": 5043.9800000000005, "end": 5045.54, "text": " if we can.", "tokens": [50698, 498, 321, 393, 13, 50776], "temperature": 0.0, "avg_logprob": -0.19527333848019865, "compression_ratio": 1.5324675324675325, "no_speech_prob": 0.00013765435141976923}, {"id": 964, "seek": 503730, "start": 5045.54, "end": 5050.820000000001, "text": " So I said it was kind of not constructed with any great care and thought, really.", "tokens": [50776, 407, 286, 848, 309, 390, 733, 295, 406, 17083, 365, 604, 869, 1127, 293, 1194, 11, 534, 13, 51040], "temperature": 0.0, "avg_logprob": -0.19527333848019865, "compression_ratio": 1.5324675324675325, "no_speech_prob": 0.00013765435141976923}, {"id": 965, "seek": 503730, "start": 5050.820000000001, "end": 5056.8, "text": " Like in terms of like this resnet, we just took the convnet and replaced it with a resnet.", "tokens": [51040, 1743, 294, 2115, 295, 411, 341, 725, 7129, 11, 321, 445, 1890, 264, 3754, 7129, 293, 10772, 309, 365, 257, 725, 7129, 13, 51339], "temperature": 0.0, "avg_logprob": -0.19527333848019865, "compression_ratio": 1.5324675324675325, "no_speech_prob": 0.00013765435141976923}, {"id": 966, "seek": 503730, "start": 5056.8, "end": 5062.54, "text": " So it's effectively twice as deep, because each conv block has two convolutions.", "tokens": [51339, 407, 309, 311, 8659, 6091, 382, 2452, 11, 570, 1184, 3754, 3461, 575, 732, 3754, 15892, 13, 51626], "temperature": 0.0, "avg_logprob": -0.19527333848019865, "compression_ratio": 1.5324675324675325, "no_speech_prob": 0.00013765435141976923}, {"id": 967, "seek": 506254, "start": 5062.9, "end": 5067.86, "text": " But resnets train better than convnets.", "tokens": [50382, 583, 725, 77, 1385, 3847, 1101, 813, 3754, 77, 1385, 13, 50630], "temperature": 0.0, "avg_logprob": -0.212493634223938, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.010489008389413357}, {"id": 968, "seek": 506254, "start": 5067.86, "end": 5071.66, "text": " So surely we could go deeper and wider still.", "tokens": [50630, 407, 11468, 321, 727, 352, 7731, 293, 11842, 920, 13, 50820], "temperature": 0.0, "avg_logprob": -0.212493634223938, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.010489008389413357}, {"id": 969, "seek": 506254, "start": 5071.66, "end": 5076.5, "text": " So I thought, okay, how could we go wider?", "tokens": [50820, 407, 286, 1194, 11, 1392, 11, 577, 727, 321, 352, 11842, 30, 51062], "temperature": 0.0, "avg_logprob": -0.212493634223938, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.010489008389413357}, {"id": 970, "seek": 506254, "start": 5076.5, "end": 5083.48, "text": " And I thought, well, let's take our model.", "tokens": [51062, 400, 286, 1194, 11, 731, 11, 718, 311, 747, 527, 2316, 13, 51411], "temperature": 0.0, "avg_logprob": -0.212493634223938, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.010489008389413357}, {"id": 971, "seek": 506254, "start": 5083.48, "end": 5088.14, "text": " And previously we were going from 8 up to 256.", "tokens": [51411, 400, 8046, 321, 645, 516, 490, 1649, 493, 281, 38882, 13, 51644], "temperature": 0.0, "avg_logprob": -0.212493634223938, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.010489008389413357}, {"id": 972, "seek": 506254, "start": 5088.14, "end": 5092.1, "text": " What if we could get up to 512?", "tokens": [51644, 708, 498, 321, 727, 483, 493, 281, 1025, 4762, 30, 51842], "temperature": 0.0, "avg_logprob": -0.212493634223938, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.010489008389413357}, {"id": 973, "seek": 509210, "start": 5092.660000000001, "end": 5097.860000000001, "text": " And I thought, okay, well, one way to do that would be to make our very first res block", "tokens": [50392, 400, 286, 1194, 11, 1392, 11, 731, 11, 472, 636, 281, 360, 300, 576, 312, 281, 652, 527, 588, 700, 725, 3461, 50652], "temperature": 0.0, "avg_logprob": -0.234433270375663, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.00021654377633240074}, {"id": 974, "seek": 509210, "start": 5097.860000000001, "end": 5102.02, "text": " not have a kernel size of 3, but a kernel size of 5.", "tokens": [50652, 406, 362, 257, 28256, 2744, 295, 805, 11, 457, 257, 28256, 2744, 295, 1025, 13, 50860], "temperature": 0.0, "avg_logprob": -0.234433270375663, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.00021654377633240074}, {"id": 975, "seek": 509210, "start": 5102.02, "end": 5105.02, "text": " So that means that each grid is going to be 5 by 5.", "tokens": [50860, 407, 300, 1355, 300, 1184, 10748, 307, 516, 281, 312, 1025, 538, 1025, 13, 51010], "temperature": 0.0, "avg_logprob": -0.234433270375663, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.00021654377633240074}, {"id": 976, "seek": 509210, "start": 5105.02, "end": 5106.88, "text": " That's going to be 25 inputs.", "tokens": [51010, 663, 311, 516, 281, 312, 3552, 15743, 13, 51103], "temperature": 0.0, "avg_logprob": -0.234433270375663, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.00021654377633240074}, {"id": 977, "seek": 509210, "start": 5106.88, "end": 5110.9400000000005, "text": " So I think it's fair enough then to have 16 outputs.", "tokens": [51103, 407, 286, 519, 309, 311, 3143, 1547, 550, 281, 362, 3165, 23930, 13, 51306], "temperature": 0.0, "avg_logprob": -0.234433270375663, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.00021654377633240074}, {"id": 978, "seek": 509210, "start": 5110.9400000000005, "end": 5117.46, "text": " So if I use a kernel size of 5, 16 outputs, then that means if I keep doubling as before,", "tokens": [51306, 407, 498, 286, 764, 257, 28256, 2744, 295, 1025, 11, 3165, 23930, 11, 550, 300, 1355, 498, 286, 1066, 33651, 382, 949, 11, 51632], "temperature": 0.0, "avg_logprob": -0.234433270375663, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.00021654377633240074}, {"id": 979, "seek": 511746, "start": 5117.46, "end": 5123.34, "text": " I'm going to end up at 512 rather than 256.", "tokens": [50364, 286, 478, 516, 281, 917, 493, 412, 1025, 4762, 2831, 813, 38882, 13, 50658], "temperature": 0.0, "avg_logprob": -0.26710400861852307, "compression_ratio": 1.294871794871795, "no_speech_prob": 0.014728488400578499}, {"id": 980, "seek": 511746, "start": 5123.34, "end": 5132.18, "text": " Okay, so that's the only change I made, was to add k equals 5 here, and then change to", "tokens": [50658, 1033, 11, 370, 300, 311, 264, 787, 1319, 286, 1027, 11, 390, 281, 909, 350, 6915, 1025, 510, 11, 293, 550, 1319, 281, 51100], "temperature": 0.0, "avg_logprob": -0.26710400861852307, "compression_ratio": 1.294871794871795, "no_speech_prob": 0.014728488400578499}, {"id": 981, "seek": 511746, "start": 5132.18, "end": 5137.18, "text": " double all the sizes.", "tokens": [51100, 3834, 439, 264, 11602, 13, 51350], "temperature": 0.0, "avg_logprob": -0.26710400861852307, "compression_ratio": 1.294871794871795, "no_speech_prob": 0.014728488400578499}, {"id": 982, "seek": 511746, "start": 5137.18, "end": 5145.02, "text": " And so if I train that, wow, look at this, 92.7%.", "tokens": [51350, 400, 370, 498, 286, 3847, 300, 11, 6076, 11, 574, 412, 341, 11, 28225, 13, 22, 6856, 51742], "temperature": 0.0, "avg_logprob": -0.26710400861852307, "compression_ratio": 1.294871794871795, "no_speech_prob": 0.014728488400578499}, {"id": 983, "seek": 514502, "start": 5145.02, "end": 5148.42, "text": " So we're getting better still.", "tokens": [50364, 407, 321, 434, 1242, 1101, 920, 13, 50534], "temperature": 0.0, "avg_logprob": -0.3248946644435419, "compression_ratio": 1.7125, "no_speech_prob": 0.005469215102493763}, {"id": 984, "seek": 514502, "start": 5148.42, "end": 5152.38, "text": " And again, it wasn't with lots of like trying and failing and whatever, it was just like", "tokens": [50534, 400, 797, 11, 309, 2067, 380, 365, 3195, 295, 411, 1382, 293, 18223, 293, 2035, 11, 309, 390, 445, 411, 50732], "temperature": 0.0, "avg_logprob": -0.3248946644435419, "compression_ratio": 1.7125, "no_speech_prob": 0.005469215102493763}, {"id": 985, "seek": 514502, "start": 5152.38, "end": 5153.660000000001, "text": " saying well this just makes sense.", "tokens": [50732, 1566, 731, 341, 445, 1669, 2020, 13, 50796], "temperature": 0.0, "avg_logprob": -0.3248946644435419, "compression_ratio": 1.7125, "no_speech_prob": 0.005469215102493763}, {"id": 986, "seek": 514502, "start": 5153.660000000001, "end": 5156.3, "text": " And the first thing I tried, it just worked.", "tokens": [50796, 400, 264, 700, 551, 286, 3031, 11, 309, 445, 2732, 13, 50928], "temperature": 0.0, "avg_logprob": -0.3248946644435419, "compression_ratio": 1.7125, "no_speech_prob": 0.005469215102493763}, {"id": 987, "seek": 514502, "start": 5156.3, "end": 5161.42, "text": " You know, we're just trying to use these sensible, thoughtful approaches.", "tokens": [50928, 509, 458, 11, 321, 434, 445, 1382, 281, 764, 613, 25380, 11, 21566, 11587, 13, 51184], "temperature": 0.0, "avg_logprob": -0.3248946644435419, "compression_ratio": 1.7125, "no_speech_prob": 0.005469215102493763}, {"id": 988, "seek": 514502, "start": 5161.42, "end": 5165.46, "text": " Okay, the next thing I'm going to try isn't necessarily something to make it better, but", "tokens": [51184, 1033, 11, 264, 958, 551, 286, 478, 516, 281, 853, 1943, 380, 4725, 746, 281, 652, 309, 1101, 11, 457, 51386], "temperature": 0.0, "avg_logprob": -0.3248946644435419, "compression_ratio": 1.7125, "no_speech_prob": 0.005469215102493763}, {"id": 989, "seek": 514502, "start": 5165.46, "end": 5169.02, "text": " it's something to make our ResNet more flexible.", "tokens": [51386, 309, 311, 746, 281, 652, 527, 5015, 31890, 544, 11358, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3248946644435419, "compression_ratio": 1.7125, "no_speech_prob": 0.005469215102493763}, {"id": 990, "seek": 516902, "start": 5169.02, "end": 5176.700000000001, "text": " Our current ResNet is a bit awkward in that the number of stride two layers has to be", "tokens": [50364, 2621, 2190, 5015, 31890, 307, 257, 857, 11411, 294, 300, 264, 1230, 295, 1056, 482, 732, 7914, 575, 281, 312, 50748], "temperature": 0.0, "avg_logprob": -0.2773497830266538, "compression_ratio": 1.5837320574162679, "no_speech_prob": 0.0008426272543147206}, {"id": 991, "seek": 516902, "start": 5176.700000000001, "end": 5188.1, "text": " exactly big enough that the last of them, that the last of them ends up with a one by", "tokens": [50748, 2293, 955, 1547, 300, 264, 1036, 295, 552, 11, 300, 264, 1036, 295, 552, 5314, 493, 365, 257, 472, 538, 51318], "temperature": 0.0, "avg_logprob": -0.2773497830266538, "compression_ratio": 1.5837320574162679, "no_speech_prob": 0.0008426272543147206}, {"id": 992, "seek": 516902, "start": 5188.1, "end": 5189.1, "text": " one output.", "tokens": [51318, 472, 5598, 13, 51368], "temperature": 0.0, "avg_logprob": -0.2773497830266538, "compression_ratio": 1.5837320574162679, "no_speech_prob": 0.0008426272543147206}, {"id": 993, "seek": 516902, "start": 5189.1, "end": 5191.660000000001, "text": " So you can flatten it and do the linear.", "tokens": [51368, 407, 291, 393, 24183, 309, 293, 360, 264, 8213, 13, 51496], "temperature": 0.0, "avg_logprob": -0.2773497830266538, "compression_ratio": 1.5837320574162679, "no_speech_prob": 0.0008426272543147206}, {"id": 994, "seek": 516902, "start": 5191.660000000001, "end": 5195.3, "text": " So that's not very flexible because, you know, what if you've got something, you know, for", "tokens": [51496, 407, 300, 311, 406, 588, 11358, 570, 11, 291, 458, 11, 437, 498, 291, 600, 658, 746, 11, 291, 458, 11, 337, 51678], "temperature": 0.0, "avg_logprob": -0.2773497830266538, "compression_ratio": 1.5837320574162679, "no_speech_prob": 0.0008426272543147206}, {"id": 995, "seek": 516902, "start": 5195.3, "end": 5196.3, "text": " different size?", "tokens": [51678, 819, 2744, 30, 51728], "temperature": 0.0, "avg_logprob": -0.2773497830266538, "compression_ratio": 1.5837320574162679, "no_speech_prob": 0.0008426272543147206}, {"id": 996, "seek": 519630, "start": 5196.820000000001, "end": 5200.26, "text": " 28 by 28 is a pretty small image.", "tokens": [50390, 7562, 538, 7562, 307, 257, 1238, 1359, 3256, 13, 50562], "temperature": 0.0, "avg_logprob": -0.2574671565896214, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.016402756795287132}, {"id": 997, "seek": 519630, "start": 5200.26, "end": 5210.54, "text": " So to kind of make that necessary, I've created a get model 2, which goes less far.", "tokens": [50562, 407, 281, 733, 295, 652, 300, 4818, 11, 286, 600, 2942, 257, 483, 2316, 568, 11, 597, 1709, 1570, 1400, 13, 51076], "temperature": 0.0, "avg_logprob": -0.2574671565896214, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.016402756795287132}, {"id": 998, "seek": 519630, "start": 5210.54, "end": 5211.9800000000005, "text": " It has one less layer.", "tokens": [51076, 467, 575, 472, 1570, 4583, 13, 51148], "temperature": 0.0, "avg_logprob": -0.2574671565896214, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.016402756795287132}, {"id": 999, "seek": 519630, "start": 5211.9800000000005, "end": 5215.66, "text": " So it only goes up to 256 despite starting at 16.", "tokens": [51148, 407, 309, 787, 1709, 493, 281, 38882, 7228, 2891, 412, 3165, 13, 51332], "temperature": 0.0, "avg_logprob": -0.2574671565896214, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.016402756795287132}, {"id": 1000, "seek": 519630, "start": 5215.66, "end": 5220.820000000001, "text": " And so because it's got one less layer, that means that it's going to end up at the two", "tokens": [51332, 400, 370, 570, 309, 311, 658, 472, 1570, 4583, 11, 300, 1355, 300, 309, 311, 516, 281, 917, 493, 412, 264, 732, 51590], "temperature": 0.0, "avg_logprob": -0.2574671565896214, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.016402756795287132}, {"id": 1001, "seek": 519630, "start": 5220.820000000001, "end": 5223.58, "text": " by two, not the one by one.", "tokens": [51590, 538, 732, 11, 406, 264, 472, 538, 472, 13, 51728], "temperature": 0.0, "avg_logprob": -0.2574671565896214, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.016402756795287132}, {"id": 1002, "seek": 519630, "start": 5223.58, "end": 5226.06, "text": " So what do we do?", "tokens": [51728, 407, 437, 360, 321, 360, 30, 51852], "temperature": 0.0, "avg_logprob": -0.2574671565896214, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.016402756795287132}, {"id": 1003, "seek": 522606, "start": 5226.820000000001, "end": 5232.9400000000005, "text": " Well, we can do something very straightforward, which is we can take the mean over the two", "tokens": [50402, 1042, 11, 321, 393, 360, 746, 588, 15325, 11, 597, 307, 321, 393, 747, 264, 914, 670, 264, 732, 50708], "temperature": 0.0, "avg_logprob": -0.1907265229658647, "compression_ratio": 1.8815165876777251, "no_speech_prob": 6.108848901931196e-05}, {"id": 1004, "seek": 522606, "start": 5232.9400000000005, "end": 5234.780000000001, "text": " by two.", "tokens": [50708, 538, 732, 13, 50800], "temperature": 0.0, "avg_logprob": -0.1907265229658647, "compression_ratio": 1.8815165876777251, "no_speech_prob": 6.108848901931196e-05}, {"id": 1005, "seek": 522606, "start": 5234.780000000001, "end": 5240.42, "text": " And so if we take the mean over the two by two, that's going to give us a mean over the", "tokens": [50800, 400, 370, 498, 321, 747, 264, 914, 670, 264, 732, 538, 732, 11, 300, 311, 516, 281, 976, 505, 257, 914, 670, 264, 51082], "temperature": 0.0, "avg_logprob": -0.1907265229658647, "compression_ratio": 1.8815165876777251, "no_speech_prob": 6.108848901931196e-05}, {"id": 1006, "seek": 522606, "start": 5240.42, "end": 5241.42, "text": " two by two.", "tokens": [51082, 732, 538, 732, 13, 51132], "temperature": 0.0, "avg_logprob": -0.1907265229658647, "compression_ratio": 1.8815165876777251, "no_speech_prob": 6.108848901931196e-05}, {"id": 1007, "seek": 522606, "start": 5241.42, "end": 5245.700000000001, "text": " It's going to give us batch size by channels output, which is what we can then put into", "tokens": [51132, 467, 311, 516, 281, 976, 505, 15245, 2744, 538, 9235, 5598, 11, 597, 307, 437, 321, 393, 550, 829, 666, 51346], "temperature": 0.0, "avg_logprob": -0.1907265229658647, "compression_ratio": 1.8815165876777251, "no_speech_prob": 6.108848901931196e-05}, {"id": 1008, "seek": 522606, "start": 5245.700000000001, "end": 5248.360000000001, "text": " our linear layer.", "tokens": [51346, 527, 8213, 4583, 13, 51479], "temperature": 0.0, "avg_logprob": -0.1907265229658647, "compression_ratio": 1.8815165876777251, "no_speech_prob": 6.108848901931196e-05}, {"id": 1009, "seek": 522606, "start": 5248.360000000001, "end": 5253.900000000001, "text": " So this is called, this ridiculously simple thing, is called a global average pooling", "tokens": [51479, 407, 341, 307, 1219, 11, 341, 41358, 2199, 551, 11, 307, 1219, 257, 4338, 4274, 7005, 278, 51756], "temperature": 0.0, "avg_logprob": -0.1907265229658647, "compression_ratio": 1.8815165876777251, "no_speech_prob": 6.108848901931196e-05}, {"id": 1010, "seek": 522606, "start": 5253.900000000001, "end": 5254.900000000001, "text": " layer.", "tokens": [51756, 4583, 13, 51806], "temperature": 0.0, "avg_logprob": -0.1907265229658647, "compression_ratio": 1.8815165876777251, "no_speech_prob": 6.108848901931196e-05}, {"id": 1011, "seek": 525490, "start": 5255.259999999999, "end": 5257.379999999999, "text": " That's the Keras term.", "tokens": [50382, 663, 311, 264, 591, 6985, 1433, 13, 50488], "temperature": 0.0, "avg_logprob": -0.21903256007603236, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.009125507436692715}, {"id": 1012, "seek": 525490, "start": 5257.379999999999, "end": 5259.86, "text": " In PyTorch, it's basically the same.", "tokens": [50488, 682, 9953, 51, 284, 339, 11, 309, 311, 1936, 264, 912, 13, 50612], "temperature": 0.0, "avg_logprob": -0.21903256007603236, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.009125507436692715}, {"id": 1013, "seek": 525490, "start": 5259.86, "end": 5262.94, "text": " It's called an adaptive average pooling layer.", "tokens": [50612, 467, 311, 1219, 364, 27912, 4274, 7005, 278, 4583, 13, 50766], "temperature": 0.0, "avg_logprob": -0.21903256007603236, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.009125507436692715}, {"id": 1014, "seek": 525490, "start": 5262.94, "end": 5270.42, "text": " But in PyTorch, you can cause it to have an output other than one by one.", "tokens": [50766, 583, 294, 9953, 51, 284, 339, 11, 291, 393, 3082, 309, 281, 362, 364, 5598, 661, 813, 472, 538, 472, 13, 51140], "temperature": 0.0, "avg_logprob": -0.21903256007603236, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.009125507436692715}, {"id": 1015, "seek": 525490, "start": 5270.42, "end": 5272.74, "text": " But nobody ever really uses it that way.", "tokens": [51140, 583, 5079, 1562, 534, 4960, 309, 300, 636, 13, 51256], "temperature": 0.0, "avg_logprob": -0.21903256007603236, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.009125507436692715}, {"id": 1016, "seek": 525490, "start": 5272.74, "end": 5274.54, "text": " So they're basically the same thing.", "tokens": [51256, 407, 436, 434, 1936, 264, 912, 551, 13, 51346], "temperature": 0.0, "avg_logprob": -0.21903256007603236, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.009125507436692715}, {"id": 1017, "seek": 525490, "start": 5274.54, "end": 5278.58, "text": " This is actually a little bit more convenient than the PyTorch version because you don't", "tokens": [51346, 639, 307, 767, 257, 707, 857, 544, 10851, 813, 264, 9953, 51, 284, 339, 3037, 570, 291, 500, 380, 51548], "temperature": 0.0, "avg_logprob": -0.21903256007603236, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.009125507436692715}, {"id": 1018, "seek": 525490, "start": 5278.58, "end": 5280.74, "text": " have to flatten it.", "tokens": [51548, 362, 281, 24183, 309, 13, 51656], "temperature": 0.0, "avg_logprob": -0.21903256007603236, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.009125507436692715}, {"id": 1019, "seek": 525490, "start": 5280.74, "end": 5282.179999999999, "text": " So this is global average pooling.", "tokens": [51656, 407, 341, 307, 4338, 4274, 7005, 278, 13, 51728], "temperature": 0.0, "avg_logprob": -0.21903256007603236, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.009125507436692715}, {"id": 1020, "seek": 528218, "start": 5282.22, "end": 5287.46, "text": " So you can see here, after our last res block, which gives us a two by two output, we have", "tokens": [50366, 407, 291, 393, 536, 510, 11, 934, 527, 1036, 725, 3461, 11, 597, 2709, 505, 257, 732, 538, 732, 5598, 11, 321, 362, 50628], "temperature": 0.0, "avg_logprob": -0.21628033983838427, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.00024536839919164777}, {"id": 1021, "seek": 528218, "start": 5287.46, "end": 5289.18, "text": " global average pool.", "tokens": [50628, 4338, 4274, 7005, 13, 50714], "temperature": 0.0, "avg_logprob": -0.21628033983838427, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.00024536839919164777}, {"id": 1022, "seek": 528218, "start": 5289.18, "end": 5293.02, "text": " And that's just going to take the mean.", "tokens": [50714, 400, 300, 311, 445, 516, 281, 747, 264, 914, 13, 50906], "temperature": 0.0, "avg_logprob": -0.21628033983838427, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.00024536839919164777}, {"id": 1023, "seek": 528218, "start": 5293.02, "end": 5297.3, "text": " And then we can do the linear batch norm as usual.", "tokens": [50906, 400, 550, 321, 393, 360, 264, 8213, 15245, 2026, 382, 7713, 13, 51120], "temperature": 0.0, "avg_logprob": -0.21628033983838427, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.00024536839919164777}, {"id": 1024, "seek": 528218, "start": 5297.3, "end": 5307.38, "text": " So I wanted to improve my summary patch to include not only the number of parameters,", "tokens": [51120, 407, 286, 1415, 281, 3470, 452, 12691, 9972, 281, 4090, 406, 787, 264, 1230, 295, 9834, 11, 51624], "temperature": 0.0, "avg_logprob": -0.21628033983838427, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.00024536839919164777}, {"id": 1025, "seek": 528218, "start": 5307.38, "end": 5310.1, "text": " but also the approximate number of megaflops.", "tokens": [51624, 457, 611, 264, 30874, 1230, 295, 10816, 2792, 75, 3370, 13, 51760], "temperature": 0.0, "avg_logprob": -0.21628033983838427, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.00024536839919164777}, {"id": 1026, "seek": 531010, "start": 5310.1, "end": 5319.06, "text": " So a flop is a floating operation per second, a floating point operation per second.", "tokens": [50364, 407, 257, 25343, 307, 257, 12607, 6916, 680, 1150, 11, 257, 12607, 935, 6916, 680, 1150, 13, 50812], "temperature": 0.0, "avg_logprob": -0.27773022121853297, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.042085934430360794}, {"id": 1027, "seek": 531010, "start": 5319.06, "end": 5321.54, "text": " I'm not going to promise my calculation is exactly right.", "tokens": [50812, 286, 478, 406, 516, 281, 6228, 452, 17108, 307, 2293, 558, 13, 50936], "temperature": 0.0, "avg_logprob": -0.27773022121853297, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.042085934430360794}, {"id": 1028, "seek": 531010, "start": 5321.54, "end": 5324.9400000000005, "text": " I think the basic idea is right.", "tokens": [50936, 286, 519, 264, 3875, 1558, 307, 558, 13, 51106], "temperature": 0.0, "avg_logprob": -0.27773022121853297, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.042085934430360794}, {"id": 1029, "seek": 531010, "start": 5324.9400000000005, "end": 5328.5, "text": " I just basically actually calculated, it's not really a flop, so I actually counted the", "tokens": [51106, 286, 445, 1936, 767, 15598, 11, 309, 311, 406, 534, 257, 25343, 11, 370, 286, 767, 20150, 264, 51284], "temperature": 0.0, "avg_logprob": -0.27773022121853297, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.042085934430360794}, {"id": 1030, "seek": 531010, "start": 5328.5, "end": 5330.9800000000005, "text": " number of multiplications.", "tokens": [51284, 1230, 295, 17596, 763, 13, 51408], "temperature": 0.0, "avg_logprob": -0.27773022121853297, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.042085934430360794}, {"id": 1031, "seek": 531010, "start": 5330.9800000000005, "end": 5335.620000000001, "text": " So this is not perfectly accurate, but it's pretty indicative, I think.", "tokens": [51408, 407, 341, 307, 406, 6239, 8559, 11, 457, 309, 311, 1238, 47513, 11, 286, 519, 13, 51640], "temperature": 0.0, "avg_logprob": -0.27773022121853297, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.042085934430360794}, {"id": 1032, "seek": 533562, "start": 5335.62, "end": 5341.14, "text": " So this is the same summary I had before, but I added an extra thing which is a flops", "tokens": [50364, 407, 341, 307, 264, 912, 12691, 286, 632, 949, 11, 457, 286, 3869, 364, 2857, 551, 597, 307, 257, 932, 3370, 50640], "temperature": 0.0, "avg_logprob": -0.23589315601423674, "compression_ratio": 1.7148936170212765, "no_speech_prob": 0.006488073617219925}, {"id": 1033, "seek": 533562, "start": 5341.14, "end": 5348.0599999999995, "text": " function where you pass in the weight matrix and the height and the width of your grid.", "tokens": [50640, 2445, 689, 291, 1320, 294, 264, 3364, 8141, 293, 264, 6681, 293, 264, 11402, 295, 428, 10748, 13, 50986], "temperature": 0.0, "avg_logprob": -0.23589315601423674, "compression_ratio": 1.7148936170212765, "no_speech_prob": 0.006488073617219925}, {"id": 1034, "seek": 533562, "start": 5348.0599999999995, "end": 5354.5, "text": " Now if the number of dimensions of the weight matrix is less than three, then we're just", "tokens": [50986, 823, 498, 264, 1230, 295, 12819, 295, 264, 3364, 8141, 307, 1570, 813, 1045, 11, 550, 321, 434, 445, 51308], "temperature": 0.0, "avg_logprob": -0.23589315601423674, "compression_ratio": 1.7148936170212765, "no_speech_prob": 0.006488073617219925}, {"id": 1035, "seek": 533562, "start": 5354.5, "end": 5356.74, "text": " doing like a linear layer or something.", "tokens": [51308, 884, 411, 257, 8213, 4583, 420, 746, 13, 51420], "temperature": 0.0, "avg_logprob": -0.23589315601423674, "compression_ratio": 1.7148936170212765, "no_speech_prob": 0.006488073617219925}, {"id": 1036, "seek": 533562, "start": 5356.74, "end": 5362.38, "text": " So actually just the number of elements is the number of flops, because it's just a matrix", "tokens": [51420, 407, 767, 445, 264, 1230, 295, 4959, 307, 264, 1230, 295, 932, 3370, 11, 570, 309, 311, 445, 257, 8141, 51702], "temperature": 0.0, "avg_logprob": -0.23589315601423674, "compression_ratio": 1.7148936170212765, "no_speech_prob": 0.006488073617219925}, {"id": 1037, "seek": 533562, "start": 5362.38, "end": 5364.92, "text": " multiply.", "tokens": [51702, 12972, 13, 51829], "temperature": 0.0, "avg_logprob": -0.23589315601423674, "compression_ratio": 1.7148936170212765, "no_speech_prob": 0.006488073617219925}, {"id": 1038, "seek": 536492, "start": 5364.92, "end": 5368.88, "text": " But if you're doing a convolution, so the dimension is four, then you actually do that", "tokens": [50364, 583, 498, 291, 434, 884, 257, 45216, 11, 370, 264, 10139, 307, 1451, 11, 550, 291, 767, 360, 300, 50562], "temperature": 0.0, "avg_logprob": -0.20532756805419922, "compression_ratio": 1.5, "no_speech_prob": 0.00010071389988297597}, {"id": 1039, "seek": 536492, "start": 5368.88, "end": 5372.64, "text": " matrix multiply for everything in the height by width grid.", "tokens": [50562, 8141, 12972, 337, 1203, 294, 264, 6681, 538, 11402, 10748, 13, 50750], "temperature": 0.0, "avg_logprob": -0.20532756805419922, "compression_ratio": 1.5, "no_speech_prob": 0.00010071389988297597}, {"id": 1040, "seek": 536492, "start": 5372.64, "end": 5379.4800000000005, "text": " So that's how I calculate this kind of flops equivalent number.", "tokens": [50750, 407, 300, 311, 577, 286, 8873, 341, 733, 295, 932, 3370, 10344, 1230, 13, 51092], "temperature": 0.0, "avg_logprob": -0.20532756805419922, "compression_ratio": 1.5, "no_speech_prob": 0.00010071389988297597}, {"id": 1041, "seek": 536492, "start": 5379.4800000000005, "end": 5393.0, "text": " So okay, so if I run that on this model, we can now see our number of parameters compared", "tokens": [51092, 407, 1392, 11, 370, 498, 286, 1190, 300, 322, 341, 2316, 11, 321, 393, 586, 536, 527, 1230, 295, 9834, 5347, 51768], "temperature": 0.0, "avg_logprob": -0.20532756805419922, "compression_ratio": 1.5, "no_speech_prob": 0.00010071389988297597}, {"id": 1042, "seek": 539300, "start": 5393.0, "end": 5401.8, "text": " to the ResNet model has gone from 1.2 million up to 4.9 million.", "tokens": [50364, 281, 264, 5015, 31890, 2316, 575, 2780, 490, 502, 13, 17, 2459, 493, 281, 1017, 13, 24, 2459, 13, 50804], "temperature": 0.0, "avg_logprob": -0.24521234300401476, "compression_ratio": 1.4709677419354839, "no_speech_prob": 0.0059109218418598175}, {"id": 1043, "seek": 539300, "start": 5401.8, "end": 5413.2, "text": " And the reason why is because we've got this, we've got this res block that gets all the", "tokens": [50804, 400, 264, 1778, 983, 307, 570, 321, 600, 658, 341, 11, 321, 600, 658, 341, 725, 3461, 300, 2170, 439, 264, 51374], "temperature": 0.0, "avg_logprob": -0.24521234300401476, "compression_ratio": 1.4709677419354839, "no_speech_prob": 0.0059109218418598175}, {"id": 1044, "seek": 539300, "start": 5413.2, "end": 5415.52, "text": " way up to 512.", "tokens": [51374, 636, 493, 281, 1025, 4762, 13, 51490], "temperature": 0.0, "avg_logprob": -0.24521234300401476, "compression_ratio": 1.4709677419354839, "no_speech_prob": 0.0059109218418598175}, {"id": 1045, "seek": 539300, "start": 5415.52, "end": 5422.0, "text": " And the way we did this is we made that a stride one layer.", "tokens": [51490, 400, 264, 636, 321, 630, 341, 307, 321, 1027, 300, 257, 1056, 482, 472, 4583, 13, 51814], "temperature": 0.0, "avg_logprob": -0.24521234300401476, "compression_ratio": 1.4709677419354839, "no_speech_prob": 0.0059109218418598175}, {"id": 1046, "seek": 542200, "start": 5422.0, "end": 5426.12, "text": " So that's why you can see here it's gone 2.2 and it stayed at 2.2.", "tokens": [50364, 407, 300, 311, 983, 291, 393, 536, 510, 309, 311, 2780, 568, 13, 17, 293, 309, 9181, 412, 568, 13, 17, 13, 50570], "temperature": 0.0, "avg_logprob": -0.22976826176498877, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.000804083130788058}, {"id": 1047, "seek": 542200, "start": 5426.12, "end": 5428.24, "text": " So I wanted to make it as similar as possible to the last ones.", "tokens": [50570, 407, 286, 1415, 281, 652, 309, 382, 2531, 382, 1944, 281, 264, 1036, 2306, 13, 50676], "temperature": 0.0, "avg_logprob": -0.22976826176498877, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.000804083130788058}, {"id": 1048, "seek": 542200, "start": 5428.24, "end": 5432.98, "text": " It's got, you know, the same 512 final number of channels.", "tokens": [50676, 467, 311, 658, 11, 291, 458, 11, 264, 912, 1025, 4762, 2572, 1230, 295, 9235, 13, 50913], "temperature": 0.0, "avg_logprob": -0.22976826176498877, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.000804083130788058}, {"id": 1049, "seek": 542200, "start": 5432.98, "end": 5439.92, "text": " And so most of the parameters are in that last block for the reason we just discussed.", "tokens": [50913, 400, 370, 881, 295, 264, 9834, 366, 294, 300, 1036, 3461, 337, 264, 1778, 321, 445, 7152, 13, 51260], "temperature": 0.0, "avg_logprob": -0.22976826176498877, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.000804083130788058}, {"id": 1050, "seek": 542200, "start": 5439.92, "end": 5446.56, "text": " Interestingly though, it's not as clear for the megaflops, you know, it is the greatest", "tokens": [51260, 30564, 1673, 11, 309, 311, 406, 382, 1850, 337, 264, 10816, 2792, 75, 3370, 11, 291, 458, 11, 309, 307, 264, 6636, 51592], "temperature": 0.0, "avg_logprob": -0.22976826176498877, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.000804083130788058}, {"id": 1051, "seek": 542200, "start": 5446.56, "end": 5447.56, "text": " of them.", "tokens": [51592, 295, 552, 13, 51642], "temperature": 0.0, "avg_logprob": -0.22976826176498877, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.000804083130788058}, {"id": 1052, "seek": 542200, "start": 5447.56, "end": 5451.16, "text": " But, you know, in terms of number of parameters, I think this has more parameters than all", "tokens": [51642, 583, 11, 291, 458, 11, 294, 2115, 295, 1230, 295, 9834, 11, 286, 519, 341, 575, 544, 9834, 813, 439, 51822], "temperature": 0.0, "avg_logprob": -0.22976826176498877, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.000804083130788058}, {"id": 1053, "seek": 545116, "start": 5451.32, "end": 5453.44, "text": " the other ones added together by a lot.", "tokens": [50372, 264, 661, 2306, 3869, 1214, 538, 257, 688, 13, 50478], "temperature": 0.0, "avg_logprob": -0.2803451097928561, "compression_ratio": 1.5114942528735633, "no_speech_prob": 0.015906231477856636}, {"id": 1054, "seek": 545116, "start": 5453.44, "end": 5455.32, "text": " But that's not true of megaflops.", "tokens": [50478, 583, 300, 311, 406, 2074, 295, 10816, 2792, 75, 3370, 13, 50572], "temperature": 0.0, "avg_logprob": -0.2803451097928561, "compression_ratio": 1.5114942528735633, "no_speech_prob": 0.015906231477856636}, {"id": 1055, "seek": 545116, "start": 5455.32, "end": 5462.96, "text": " And that's because this first layer has to be done 28 by 28 times, whereas this layer", "tokens": [50572, 400, 300, 311, 570, 341, 700, 4583, 575, 281, 312, 1096, 7562, 538, 7562, 1413, 11, 9735, 341, 4583, 50954], "temperature": 0.0, "avg_logprob": -0.2803451097928561, "compression_ratio": 1.5114942528735633, "no_speech_prob": 0.015906231477856636}, {"id": 1056, "seek": 545116, "start": 5462.96, "end": 5466.16, "text": " only has to be done 2 by 2 times.", "tokens": [50954, 787, 575, 281, 312, 1096, 568, 538, 568, 1413, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2803451097928561, "compression_ratio": 1.5114942528735633, "no_speech_prob": 0.015906231477856636}, {"id": 1057, "seek": 545116, "start": 5466.16, "end": 5475.84, "text": " Anyway, so I tried training that and got pretty similar result, 92.6.", "tokens": [51114, 5684, 11, 370, 286, 3031, 3097, 300, 293, 658, 1238, 2531, 1874, 11, 28225, 13, 21, 13, 51598], "temperature": 0.0, "avg_logprob": -0.2803451097928561, "compression_ratio": 1.5114942528735633, "no_speech_prob": 0.015906231477856636}, {"id": 1058, "seek": 547584, "start": 5475.84, "end": 5480.96, "text": " And that kind of made me think, oh, let's fiddle around with this a little bit more", "tokens": [50364, 400, 300, 733, 295, 1027, 385, 519, 11, 1954, 11, 718, 311, 24553, 2285, 926, 365, 341, 257, 707, 857, 544, 50620], "temperature": 0.0, "avg_logprob": -0.23140863271859977, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.02161363698542118}, {"id": 1059, "seek": 547584, "start": 5480.96, "end": 5486.4800000000005, "text": " to see like what kind of things would reduce the number of parameters and the megaflops.", "tokens": [50620, 281, 536, 411, 437, 733, 295, 721, 576, 5407, 264, 1230, 295, 9834, 293, 264, 10816, 2792, 75, 3370, 13, 50896], "temperature": 0.0, "avg_logprob": -0.23140863271859977, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.02161363698542118}, {"id": 1060, "seek": 547584, "start": 5486.4800000000005, "end": 5490.4800000000005, "text": " The reason you care about reducing the number of parameters is that it has lower memory", "tokens": [50896, 440, 1778, 291, 1127, 466, 12245, 264, 1230, 295, 9834, 307, 300, 309, 575, 3126, 4675, 51096], "temperature": 0.0, "avg_logprob": -0.23140863271859977, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.02161363698542118}, {"id": 1061, "seek": 547584, "start": 5490.4800000000005, "end": 5491.84, "text": " requirements.", "tokens": [51096, 7728, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23140863271859977, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.02161363698542118}, {"id": 1062, "seek": 547584, "start": 5491.84, "end": 5500.42, "text": " And the reason you require, want to reduce the number of flops is it's less compute.", "tokens": [51164, 400, 264, 1778, 291, 3651, 11, 528, 281, 5407, 264, 1230, 295, 932, 3370, 307, 309, 311, 1570, 14722, 13, 51593], "temperature": 0.0, "avg_logprob": -0.23140863271859977, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.02161363698542118}, {"id": 1063, "seek": 550042, "start": 5500.42, "end": 5514.78, "text": " So in this case, what I've done here is I've removed this line of code.", "tokens": [50364, 407, 294, 341, 1389, 11, 437, 286, 600, 1096, 510, 307, 286, 600, 7261, 341, 1622, 295, 3089, 13, 51082], "temperature": 0.0, "avg_logprob": -0.17782883829884716, "compression_ratio": 1.562874251497006, "no_speech_prob": 0.00020662946917582303}, {"id": 1064, "seek": 550042, "start": 5514.78, "end": 5517.34, "text": " So I've removed the line of code that takes it up to 512.", "tokens": [51082, 407, 286, 600, 7261, 264, 1622, 295, 3089, 300, 2516, 309, 493, 281, 1025, 4762, 13, 51210], "temperature": 0.0, "avg_logprob": -0.17782883829884716, "compression_ratio": 1.562874251497006, "no_speech_prob": 0.00020662946917582303}, {"id": 1065, "seek": 550042, "start": 5517.34, "end": 5520.38, "text": " So that means we don't have this layer anymore.", "tokens": [51210, 407, 300, 1355, 321, 500, 380, 362, 341, 4583, 3602, 13, 51362], "temperature": 0.0, "avg_logprob": -0.17782883829884716, "compression_ratio": 1.562874251497006, "no_speech_prob": 0.00020662946917582303}, {"id": 1066, "seek": 550042, "start": 5520.38, "end": 5527.84, "text": " And so the number of parameters has gone down from 4.9 million down to 1.2 million.", "tokens": [51362, 400, 370, 264, 1230, 295, 9834, 575, 2780, 760, 490, 1017, 13, 24, 2459, 760, 281, 502, 13, 17, 2459, 13, 51735], "temperature": 0.0, "avg_logprob": -0.17782883829884716, "compression_ratio": 1.562874251497006, "no_speech_prob": 0.00020662946917582303}, {"id": 1067, "seek": 552784, "start": 5527.84, "end": 5531.16, "text": " Not a huge impact on the megaflops, but a huge impact on the parameters.", "tokens": [50364, 1726, 257, 2603, 2712, 322, 264, 10816, 2792, 75, 3370, 11, 457, 257, 2603, 2712, 322, 264, 9834, 13, 50530], "temperature": 0.0, "avg_logprob": -0.21185441450639206, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.0340992696583271}, {"id": 1068, "seek": 552784, "start": 5531.16, "end": 5538.04, "text": " We've reduced it by like two-thirds or three-quarters or something by getting rid of that.", "tokens": [50530, 492, 600, 9212, 309, 538, 411, 732, 12, 38507, 420, 1045, 12, 17313, 420, 746, 538, 1242, 3973, 295, 300, 13, 50874], "temperature": 0.0, "avg_logprob": -0.21185441450639206, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.0340992696583271}, {"id": 1069, "seek": 552784, "start": 5538.04, "end": 5550.4400000000005, "text": " And you can see that the, if we take the very first ResNet block, the number of parameters", "tokens": [50874, 400, 291, 393, 536, 300, 264, 11, 498, 321, 747, 264, 588, 700, 5015, 31890, 3461, 11, 264, 1230, 295, 9834, 51494], "temperature": 0.0, "avg_logprob": -0.21185441450639206, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.0340992696583271}, {"id": 1070, "seek": 552784, "start": 5550.4400000000005, "end": 5554.72, "text": " is, you know, why is it this 5.3 megaflops?", "tokens": [51494, 307, 11, 291, 458, 11, 983, 307, 309, 341, 1025, 13, 18, 10816, 2792, 75, 3370, 30, 51708], "temperature": 0.0, "avg_logprob": -0.21185441450639206, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.0340992696583271}, {"id": 1071, "seek": 555472, "start": 5554.72, "end": 5560.04, "text": " Because although the very first one starts with just one channel, the first conv, remember", "tokens": [50364, 1436, 4878, 264, 588, 700, 472, 3719, 365, 445, 472, 2269, 11, 264, 700, 3754, 11, 1604, 50630], "temperature": 0.0, "avg_logprob": -0.23115021591886467, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.015188273973762989}, {"id": 1072, "seek": 555472, "start": 5560.04, "end": 5561.92, "text": " our ResNet blocks have two convs.", "tokens": [50630, 527, 5015, 31890, 8474, 362, 732, 3754, 82, 13, 50724], "temperature": 0.0, "avg_logprob": -0.23115021591886467, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.015188273973762989}, {"id": 1073, "seek": 555472, "start": 5561.92, "end": 5566.26, "text": " So the second conv is going to be a 16 by 16 by 5 by 5.", "tokens": [50724, 407, 264, 1150, 3754, 307, 516, 281, 312, 257, 3165, 538, 3165, 538, 1025, 538, 1025, 13, 50941], "temperature": 0.0, "avg_logprob": -0.23115021591886467, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.015188273973762989}, {"id": 1074, "seek": 555472, "start": 5566.26, "end": 5569.88, "text": " And again, I'm partly doing this to show you the actual details of this architecture, but", "tokens": [50941, 400, 797, 11, 286, 478, 17031, 884, 341, 281, 855, 291, 264, 3539, 4365, 295, 341, 9482, 11, 457, 51122], "temperature": 0.0, "avg_logprob": -0.23115021591886467, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.015188273973762989}, {"id": 1075, "seek": 555472, "start": 5569.88, "end": 5574.240000000001, "text": " I'm partly showing it so that you can see how to investigate exactly what's going on", "tokens": [51122, 286, 478, 17031, 4099, 309, 370, 300, 291, 393, 536, 577, 281, 15013, 2293, 437, 311, 516, 322, 51340], "temperature": 0.0, "avg_logprob": -0.23115021591886467, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.015188273973762989}, {"id": 1076, "seek": 555472, "start": 5574.240000000001, "end": 5575.240000000001, "text": " in your models.", "tokens": [51340, 294, 428, 5245, 13, 51390], "temperature": 0.0, "avg_logprob": -0.23115021591886467, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.015188273973762989}, {"id": 1077, "seek": 555472, "start": 5575.240000000001, "end": 5578.56, "text": " And I really want you to try these.", "tokens": [51390, 400, 286, 534, 528, 291, 281, 853, 613, 13, 51556], "temperature": 0.0, "avg_logprob": -0.23115021591886467, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.015188273973762989}, {"id": 1078, "seek": 557856, "start": 5578.56, "end": 5585.0, "text": " So if we train that one, interestingly, even though it's only a quarter or something of", "tokens": [50364, 407, 498, 321, 3847, 300, 472, 11, 25873, 11, 754, 1673, 309, 311, 787, 257, 6555, 420, 746, 295, 50686], "temperature": 0.0, "avg_logprob": -0.2672655741373698, "compression_ratio": 1.4663461538461537, "no_speech_prob": 0.017985215410590172}, {"id": 1079, "seek": 557856, "start": 5585.0, "end": 5592.64, "text": " the size, we get the same accuracy, 92.7.", "tokens": [50686, 264, 2744, 11, 321, 483, 264, 912, 14170, 11, 28225, 13, 22, 13, 51068], "temperature": 0.0, "avg_logprob": -0.2672655741373698, "compression_ratio": 1.4663461538461537, "no_speech_prob": 0.017985215410590172}, {"id": 1080, "seek": 557856, "start": 5592.64, "end": 5595.360000000001, "text": " So that's interesting.", "tokens": [51068, 407, 300, 311, 1880, 13, 51204], "temperature": 0.0, "avg_logprob": -0.2672655741373698, "compression_ratio": 1.4663461538461537, "no_speech_prob": 0.017985215410590172}, {"id": 1081, "seek": 557856, "start": 5595.360000000001, "end": 5598.400000000001, "text": " Can we make it faster?", "tokens": [51204, 1664, 321, 652, 309, 4663, 30, 51356], "temperature": 0.0, "avg_logprob": -0.2672655741373698, "compression_ratio": 1.4663461538461537, "no_speech_prob": 0.017985215410590172}, {"id": 1082, "seek": 557856, "start": 5598.400000000001, "end": 5605.160000000001, "text": " Well at this point, this is the obvious place to look at, is this first ResNet block.", "tokens": [51356, 1042, 412, 341, 935, 11, 341, 307, 264, 6322, 1081, 281, 574, 412, 11, 307, 341, 700, 5015, 31890, 3461, 13, 51694], "temperature": 0.0, "avg_logprob": -0.2672655741373698, "compression_ratio": 1.4663461538461537, "no_speech_prob": 0.017985215410590172}, {"id": 1083, "seek": 557856, "start": 5605.160000000001, "end": 5606.4800000000005, "text": " Because that's where all the megaflops are.", "tokens": [51694, 1436, 300, 311, 689, 439, 264, 10816, 2792, 75, 3370, 366, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2672655741373698, "compression_ratio": 1.4663461538461537, "no_speech_prob": 0.017985215410590172}, {"id": 1084, "seek": 560648, "start": 5606.48, "end": 5609.5599999999995, "text": " And as I said, the reason is because it's got two convs.", "tokens": [50364, 400, 382, 286, 848, 11, 264, 1778, 307, 570, 309, 311, 658, 732, 3754, 82, 13, 50518], "temperature": 0.0, "avg_logprob": -0.22797536275472985, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0013885163934901357}, {"id": 1085, "seek": 560648, "start": 5609.5599999999995, "end": 5614.24, "text": " The second one is 16 by 16 channels.", "tokens": [50518, 440, 1150, 472, 307, 3165, 538, 3165, 9235, 13, 50752], "temperature": 0.0, "avg_logprob": -0.22797536275472985, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0013885163934901357}, {"id": 1086, "seek": 560648, "start": 5614.24, "end": 5617.879999999999, "text": " 16 channels in, 16 channels out.", "tokens": [50752, 3165, 9235, 294, 11, 3165, 9235, 484, 13, 50934], "temperature": 0.0, "avg_logprob": -0.22797536275472985, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0013885163934901357}, {"id": 1087, "seek": 560648, "start": 5617.879999999999, "end": 5622.719999999999, "text": " And it's doing these 5 by 5 kernels.", "tokens": [50934, 400, 309, 311, 884, 613, 1025, 538, 1025, 23434, 1625, 13, 51176], "temperature": 0.0, "avg_logprob": -0.22797536275472985, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0013885163934901357}, {"id": 1088, "seek": 560648, "start": 5622.719999999999, "end": 5626.24, "text": " And it's having to do it across the whole 28 by 28 grid.", "tokens": [51176, 400, 309, 311, 1419, 281, 360, 309, 2108, 264, 1379, 7562, 538, 7562, 10748, 13, 51352], "temperature": 0.0, "avg_logprob": -0.22797536275472985, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0013885163934901357}, {"id": 1089, "seek": 560648, "start": 5626.24, "end": 5630.44, "text": " So that's the bulk of the biggest compute.", "tokens": [51352, 407, 300, 311, 264, 16139, 295, 264, 3880, 14722, 13, 51562], "temperature": 0.0, "avg_logprob": -0.22797536275472985, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0013885163934901357}, {"id": 1090, "seek": 563044, "start": 5630.44, "end": 5640.24, "text": " So what we could do is we could replace this Res block with just one convolution.", "tokens": [50364, 407, 437, 321, 727, 360, 307, 321, 727, 7406, 341, 5015, 3461, 365, 445, 472, 45216, 13, 50854], "temperature": 0.0, "avg_logprob": -0.19511851142434514, "compression_ratio": 1.5086705202312138, "no_speech_prob": 0.011156869120895863}, {"id": 1091, "seek": 563044, "start": 5640.24, "end": 5650.839999999999, "text": " And if we do that, then you'll see that we've now got rid of the 16 by 16 by 5 by 5.", "tokens": [50854, 400, 498, 321, 360, 300, 11, 550, 291, 603, 536, 300, 321, 600, 586, 658, 3973, 295, 264, 3165, 538, 3165, 538, 1025, 538, 1025, 13, 51384], "temperature": 0.0, "avg_logprob": -0.19511851142434514, "compression_ratio": 1.5086705202312138, "no_speech_prob": 0.011156869120895863}, {"id": 1092, "seek": 563044, "start": 5650.839999999999, "end": 5652.839999999999, "text": " We just got the 16 by 1 by 5 by 5.", "tokens": [51384, 492, 445, 658, 264, 3165, 538, 502, 538, 1025, 538, 1025, 13, 51484], "temperature": 0.0, "avg_logprob": -0.19511851142434514, "compression_ratio": 1.5086705202312138, "no_speech_prob": 0.011156869120895863}, {"id": 1093, "seek": 563044, "start": 5652.839999999999, "end": 5660.04, "text": " So the number of megaflops has gone down from 18.3 to 13.3.", "tokens": [51484, 407, 264, 1230, 295, 10816, 2792, 75, 3370, 575, 2780, 760, 490, 2443, 13, 18, 281, 3705, 13, 18, 13, 51844], "temperature": 0.0, "avg_logprob": -0.19511851142434514, "compression_ratio": 1.5086705202312138, "no_speech_prob": 0.011156869120895863}, {"id": 1094, "seek": 566004, "start": 5660.64, "end": 5662.6, "text": " The number of parameters hasn't really changed at all, right?", "tokens": [50394, 440, 1230, 295, 9834, 6132, 380, 534, 3105, 412, 439, 11, 558, 30, 50492], "temperature": 0.0, "avg_logprob": -0.26939568399381236, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0006361902924254537}, {"id": 1095, "seek": 566004, "start": 5662.6, "end": 5666.92, "text": " Because the number of parameters was only 6800, right?", "tokens": [50492, 1436, 264, 1230, 295, 9834, 390, 787, 1386, 14423, 11, 558, 30, 50708], "temperature": 0.0, "avg_logprob": -0.26939568399381236, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0006361902924254537}, {"id": 1096, "seek": 566004, "start": 5666.92, "end": 5672.16, "text": " So be very careful that when you see people talk about, oh my model has less parameters,", "tokens": [50708, 407, 312, 588, 5026, 300, 562, 291, 536, 561, 751, 466, 11, 1954, 452, 2316, 575, 1570, 9834, 11, 50970], "temperature": 0.0, "avg_logprob": -0.26939568399381236, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0006361902924254537}, {"id": 1097, "seek": 566004, "start": 5672.16, "end": 5673.8, "text": " that doesn't mean it's faster.", "tokens": [50970, 300, 1177, 380, 914, 309, 311, 4663, 13, 51052], "temperature": 0.0, "avg_logprob": -0.26939568399381236, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0006361902924254537}, {"id": 1098, "seek": 566004, "start": 5673.8, "end": 5674.8, "text": " Okay?", "tokens": [51052, 1033, 30, 51102], "temperature": 0.0, "avg_logprob": -0.26939568399381236, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0006361902924254537}, {"id": 1099, "seek": 566004, "start": 5674.8, "end": 5677.56, "text": " Really, it doesn't necessarily, I mean, it doesn't mean that at all.", "tokens": [51102, 4083, 11, 309, 1177, 380, 4725, 11, 286, 914, 11, 309, 1177, 380, 914, 300, 412, 439, 13, 51240], "temperature": 0.0, "avg_logprob": -0.26939568399381236, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0006361902924254537}, {"id": 1100, "seek": 566004, "start": 5677.56, "end": 5682.12, "text": " There's no particular relationship between parameters and speed.", "tokens": [51240, 821, 311, 572, 1729, 2480, 1296, 9834, 293, 3073, 13, 51468], "temperature": 0.0, "avg_logprob": -0.26939568399381236, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0006361902924254537}, {"id": 1101, "seek": 566004, "start": 5682.12, "end": 5685.62, "text": " Even counting megaflops doesn't always work that well, because it doesn't take account", "tokens": [51468, 2754, 13251, 10816, 2792, 75, 3370, 1177, 380, 1009, 589, 300, 731, 11, 570, 309, 1177, 380, 747, 2696, 51643], "temperature": 0.0, "avg_logprob": -0.26939568399381236, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0006361902924254537}, {"id": 1102, "seek": 568562, "start": 5685.62, "end": 5689.94, "text": " of the amount of things moving through memory.", "tokens": [50364, 295, 264, 2372, 295, 721, 2684, 807, 4675, 13, 50580], "temperature": 0.0, "avg_logprob": -0.2188562456068102, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.04023703560233116}, {"id": 1103, "seek": 568562, "start": 5689.94, "end": 5696.54, "text": " But, you know, it's not a bad approximation here.", "tokens": [50580, 583, 11, 291, 458, 11, 309, 311, 406, 257, 1578, 28023, 510, 13, 50910], "temperature": 0.0, "avg_logprob": -0.2188562456068102, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.04023703560233116}, {"id": 1104, "seek": 568562, "start": 5696.54, "end": 5702.9, "text": " So here's one which has got much less megaflops, and in this case it's about the same accuracy", "tokens": [50910, 407, 510, 311, 472, 597, 575, 658, 709, 1570, 10816, 2792, 75, 3370, 11, 293, 294, 341, 1389, 309, 311, 466, 264, 912, 14170, 51228], "temperature": 0.0, "avg_logprob": -0.2188562456068102, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.04023703560233116}, {"id": 1105, "seek": 568562, "start": 5702.9, "end": 5703.94, "text": " as well.", "tokens": [51228, 382, 731, 13, 51280], "temperature": 0.0, "avg_logprob": -0.2188562456068102, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.04023703560233116}, {"id": 1106, "seek": 568562, "start": 5703.94, "end": 5705.3, "text": " So I think this is really interesting.", "tokens": [51280, 407, 286, 519, 341, 307, 534, 1880, 13, 51348], "temperature": 0.0, "avg_logprob": -0.2188562456068102, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.04023703560233116}, {"id": 1107, "seek": 568562, "start": 5705.3, "end": 5712.9, "text": " We've managed to build a model that has far less parameters and far less megaflops and", "tokens": [51348, 492, 600, 6453, 281, 1322, 257, 2316, 300, 575, 1400, 1570, 9834, 293, 1400, 1570, 10816, 2792, 75, 3370, 293, 51728], "temperature": 0.0, "avg_logprob": -0.2188562456068102, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.04023703560233116}, {"id": 1108, "seek": 571290, "start": 5712.9, "end": 5716.7, "text": " has basically exactly the same accuracy.", "tokens": [50364, 575, 1936, 2293, 264, 912, 14170, 13, 50554], "temperature": 0.0, "avg_logprob": -0.23376211147863887, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.001548751606605947}, {"id": 1109, "seek": 571290, "start": 5716.7, "end": 5719.839999999999, "text": " So I think that's a really important thing to keep in mind.", "tokens": [50554, 407, 286, 519, 300, 311, 257, 534, 1021, 551, 281, 1066, 294, 1575, 13, 50711], "temperature": 0.0, "avg_logprob": -0.23376211147863887, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.001548751606605947}, {"id": 1110, "seek": 571290, "start": 5719.839999999999, "end": 5727.82, "text": " And remember, this is still way better than the ResNet 18d from Tim.", "tokens": [50711, 400, 1604, 11, 341, 307, 920, 636, 1101, 813, 264, 5015, 31890, 2443, 67, 490, 7172, 13, 51110], "temperature": 0.0, "avg_logprob": -0.23376211147863887, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.001548751606605947}, {"id": 1111, "seek": 571290, "start": 5727.82, "end": 5734.5, "text": " So we've built something that is fast, small, and accurate.", "tokens": [51110, 407, 321, 600, 3094, 746, 300, 307, 2370, 11, 1359, 11, 293, 8559, 13, 51444], "temperature": 0.0, "avg_logprob": -0.23376211147863887, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.001548751606605947}, {"id": 1112, "seek": 571290, "start": 5734.5, "end": 5738.4, "text": " So the obvious question is, what if we train for longer?", "tokens": [51444, 407, 264, 6322, 1168, 307, 11, 437, 498, 321, 3847, 337, 2854, 30, 51639], "temperature": 0.0, "avg_logprob": -0.23376211147863887, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.001548751606605947}, {"id": 1113, "seek": 571290, "start": 5738.4, "end": 5742.299999999999, "text": " And the answer is, if we train for longer, if we train for 20 epochs, I'm not going to", "tokens": [51639, 400, 264, 1867, 307, 11, 498, 321, 3847, 337, 2854, 11, 498, 321, 3847, 337, 945, 30992, 28346, 11, 286, 478, 406, 516, 281, 51834], "temperature": 0.0, "avg_logprob": -0.23376211147863887, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.001548751606605947}, {"id": 1114, "seek": 574230, "start": 5742.7, "end": 5744.0, "text": " have you wait for it.", "tokens": [50384, 362, 291, 1699, 337, 309, 13, 50449], "temperature": 0.0, "avg_logprob": -0.22034055133198582, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0008830424048937857}, {"id": 1115, "seek": 574230, "start": 5744.0, "end": 5753.02, "text": " The training accuracy gets up to 0.999, but the validation accuracy is worse.", "tokens": [50449, 440, 3097, 14170, 2170, 493, 281, 1958, 13, 49017, 11, 457, 264, 24071, 14170, 307, 5324, 13, 50900], "temperature": 0.0, "avg_logprob": -0.22034055133198582, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0008830424048937857}, {"id": 1116, "seek": 574230, "start": 5753.02, "end": 5756.12, "text": " It's 0.924.", "tokens": [50900, 467, 311, 1958, 13, 24, 7911, 13, 51055], "temperature": 0.0, "avg_logprob": -0.22034055133198582, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0008830424048937857}, {"id": 1117, "seek": 574230, "start": 5756.12, "end": 5761.5, "text": " And the reason for that is that after 20 epochs, it's seen the same picture so many times,", "tokens": [51055, 400, 264, 1778, 337, 300, 307, 300, 934, 945, 30992, 28346, 11, 309, 311, 1612, 264, 912, 3036, 370, 867, 1413, 11, 51324], "temperature": 0.0, "avg_logprob": -0.22034055133198582, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0008830424048937857}, {"id": 1118, "seek": 574230, "start": 5761.5, "end": 5763.74, "text": " it's just memorizing them.", "tokens": [51324, 309, 311, 445, 10560, 3319, 552, 13, 51436], "temperature": 0.0, "avg_logprob": -0.22034055133198582, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0008830424048937857}, {"id": 1119, "seek": 574230, "start": 5763.74, "end": 5770.58, "text": " And so once you start memorizing, things actually go downhill.", "tokens": [51436, 400, 370, 1564, 291, 722, 10560, 3319, 11, 721, 767, 352, 29929, 13, 51778], "temperature": 0.0, "avg_logprob": -0.22034055133198582, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0008830424048937857}, {"id": 1120, "seek": 577058, "start": 5770.58, "end": 5772.34, "text": " So we need to regularize.", "tokens": [50364, 407, 321, 643, 281, 3890, 1125, 13, 50452], "temperature": 0.0, "avg_logprob": -0.24304435366675967, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0021156782750040293}, {"id": 1121, "seek": 577058, "start": 5772.34, "end": 5779.18, "text": " Now something that we have claimed in the past can regularize is to use weight decay.", "tokens": [50452, 823, 746, 300, 321, 362, 12941, 294, 264, 1791, 393, 3890, 1125, 307, 281, 764, 3364, 21039, 13, 50794], "temperature": 0.0, "avg_logprob": -0.24304435366675967, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0021156782750040293}, {"id": 1122, "seek": 577058, "start": 5779.18, "end": 5783.74, "text": " But here's where I'm going to point out that weight decay doesn't regularize at all if", "tokens": [50794, 583, 510, 311, 689, 286, 478, 516, 281, 935, 484, 300, 3364, 21039, 1177, 380, 3890, 1125, 412, 439, 498, 51022], "temperature": 0.0, "avg_logprob": -0.24304435366675967, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0021156782750040293}, {"id": 1123, "seek": 577058, "start": 5783.74, "end": 5786.78, "text": " you use batch norm.", "tokens": [51022, 291, 764, 15245, 2026, 13, 51174], "temperature": 0.0, "avg_logprob": -0.24304435366675967, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0021156782750040293}, {"id": 1124, "seek": 577058, "start": 5786.78, "end": 5787.78, "text": " And it's fascinating.", "tokens": [51174, 400, 309, 311, 10343, 13, 51224], "temperature": 0.0, "avg_logprob": -0.24304435366675967, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0021156782750040293}, {"id": 1125, "seek": 577058, "start": 5787.78, "end": 5789.46, "text": " For years, people didn't even seem to notice this.", "tokens": [51224, 1171, 924, 11, 561, 994, 380, 754, 1643, 281, 3449, 341, 13, 51308], "temperature": 0.0, "avg_logprob": -0.24304435366675967, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0021156782750040293}, {"id": 1126, "seek": 577058, "start": 5789.46, "end": 5792.66, "text": " And then somebody, I think, finally wrote a paper that pointed this out.", "tokens": [51308, 400, 550, 2618, 11, 286, 519, 11, 2721, 4114, 257, 3035, 300, 10932, 341, 484, 13, 51468], "temperature": 0.0, "avg_logprob": -0.24304435366675967, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0021156782750040293}, {"id": 1127, "seek": 577058, "start": 5792.66, "end": 5796.94, "text": " And people were like, oh wow, that's weird.", "tokens": [51468, 400, 561, 645, 411, 11, 1954, 6076, 11, 300, 311, 3657, 13, 51682], "temperature": 0.0, "avg_logprob": -0.24304435366675967, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0021156782750040293}, {"id": 1128, "seek": 577058, "start": 5796.94, "end": 5798.5, "text": " But it's really obvious when you think about it.", "tokens": [51682, 583, 309, 311, 534, 6322, 562, 291, 519, 466, 309, 13, 51760], "temperature": 0.0, "avg_logprob": -0.24304435366675967, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0021156782750040293}, {"id": 1129, "seek": 579850, "start": 5798.54, "end": 5807.22, "text": " A batch norm layer has a single set of coefficients which multiplies an entire layer.", "tokens": [50366, 316, 15245, 2026, 4583, 575, 257, 2167, 992, 295, 31994, 597, 12788, 530, 364, 2302, 4583, 13, 50800], "temperature": 0.0, "avg_logprob": -0.2647455987476167, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0003740948741324246}, {"id": 1130, "seek": 579850, "start": 5807.22, "end": 5814.12, "text": " So that set of coefficients could just be the number 100 in every place.", "tokens": [50800, 407, 300, 992, 295, 31994, 727, 445, 312, 264, 1230, 2319, 294, 633, 1081, 13, 51145], "temperature": 0.0, "avg_logprob": -0.2647455987476167, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0003740948741324246}, {"id": 1131, "seek": 579850, "start": 5814.12, "end": 5819.34, "text": " And that's going to multiply the entire previous weight matrix, or convolution kernel matrix,", "tokens": [51145, 400, 300, 311, 516, 281, 12972, 264, 2302, 3894, 3364, 8141, 11, 420, 45216, 28256, 8141, 11, 51406], "temperature": 0.0, "avg_logprob": -0.2647455987476167, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0003740948741324246}, {"id": 1132, "seek": 579850, "start": 5819.34, "end": 5821.42, "text": " by 100.", "tokens": [51406, 538, 2319, 13, 51510], "temperature": 0.0, "avg_logprob": -0.2647455987476167, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0003740948741324246}, {"id": 1133, "seek": 579850, "start": 5821.42, "end": 5827.94, "text": " As far as weight decay is concerned, that's not much of an impact at all because the batch", "tokens": [51510, 1018, 1400, 382, 3364, 21039, 307, 5922, 11, 300, 311, 406, 709, 295, 364, 2712, 412, 439, 570, 264, 15245, 51836], "temperature": 0.0, "avg_logprob": -0.2647455987476167, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0003740948741324246}, {"id": 1134, "seek": 582794, "start": 5828.379999999999, "end": 5831.7, "text": " norm layer has very few weights.", "tokens": [50386, 2026, 4583, 575, 588, 1326, 17443, 13, 50552], "temperature": 0.0, "avg_logprob": -0.28071528116861977, "compression_ratio": 1.6146341463414635, "no_speech_prob": 0.00293492479249835}, {"id": 1135, "seek": 582794, "start": 5831.7, "end": 5834.74, "text": " So it doesn't really have a huge impact on weight decay.", "tokens": [50552, 407, 309, 1177, 380, 534, 362, 257, 2603, 2712, 322, 3364, 21039, 13, 50704], "temperature": 0.0, "avg_logprob": -0.28071528116861977, "compression_ratio": 1.6146341463414635, "no_speech_prob": 0.00293492479249835}, {"id": 1136, "seek": 582794, "start": 5834.74, "end": 5840.96, "text": " But it massively increases the effective scale of the weight matrix.", "tokens": [50704, 583, 309, 29379, 8637, 264, 4942, 4373, 295, 264, 3364, 8141, 13, 51015], "temperature": 0.0, "avg_logprob": -0.28071528116861977, "compression_ratio": 1.6146341463414635, "no_speech_prob": 0.00293492479249835}, {"id": 1137, "seek": 582794, "start": 5840.96, "end": 5850.66, "text": " So batch norm basically lets the neural net cheat by increasing the coefficients, the", "tokens": [51015, 407, 15245, 2026, 1936, 6653, 264, 18161, 2533, 17470, 538, 5662, 264, 31994, 11, 264, 51500], "temperature": 0.0, "avg_logprob": -0.28071528116861977, "compression_ratio": 1.6146341463414635, "no_speech_prob": 0.00293492479249835}, {"id": 1138, "seek": 582794, "start": 5850.66, "end": 5855.7, "text": " parameters, even nearly as much as it wants indirectly just by changing the batch norm", "tokens": [51500, 9834, 11, 754, 6217, 382, 709, 382, 309, 2738, 37779, 445, 538, 4473, 264, 15245, 2026, 51752], "temperature": 0.0, "avg_logprob": -0.28071528116861977, "compression_ratio": 1.6146341463414635, "no_speech_prob": 0.00293492479249835}, {"id": 1139, "seek": 585570, "start": 5855.7, "end": 5857.98, "text": " layers weights.", "tokens": [50364, 7914, 17443, 13, 50478], "temperature": 0.0, "avg_logprob": -0.25028457641601565, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.04084519296884537}, {"id": 1140, "seek": 585570, "start": 5857.98, "end": 5863.78, "text": " So weight decay is not going to save us.", "tokens": [50478, 407, 3364, 21039, 307, 406, 516, 281, 3155, 505, 13, 50768], "temperature": 0.0, "avg_logprob": -0.25028457641601565, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.04084519296884537}, {"id": 1141, "seek": 585570, "start": 5863.78, "end": 5865.639999999999, "text": " And that's something really important to recognize.", "tokens": [50768, 400, 300, 311, 746, 534, 1021, 281, 5521, 13, 50861], "temperature": 0.0, "avg_logprob": -0.25028457641601565, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.04084519296884537}, {"id": 1142, "seek": 585570, "start": 5865.639999999999, "end": 5867.74, "text": " Weight decay is not...", "tokens": [50861, 44464, 21039, 307, 406, 485, 50966], "temperature": 0.0, "avg_logprob": -0.25028457641601565, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.04084519296884537}, {"id": 1143, "seek": 585570, "start": 5867.74, "end": 5872.0199999999995, "text": " I mean, with batch norm layers, I don't see the point of it at all.", "tokens": [50966, 286, 914, 11, 365, 15245, 2026, 7914, 11, 286, 500, 380, 536, 264, 935, 295, 309, 412, 439, 13, 51180], "temperature": 0.0, "avg_logprob": -0.25028457641601565, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.04084519296884537}, {"id": 1144, "seek": 585570, "start": 5872.0199999999995, "end": 5873.0199999999995, "text": " It does have some...", "tokens": [51180, 467, 775, 362, 512, 485, 51230], "temperature": 0.0, "avg_logprob": -0.25028457641601565, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.04084519296884537}, {"id": 1145, "seek": 585570, "start": 5873.0199999999995, "end": 5874.74, "text": " Like, there has been some studies of what it does.", "tokens": [51230, 1743, 11, 456, 575, 668, 512, 5313, 295, 437, 309, 775, 13, 51316], "temperature": 0.0, "avg_logprob": -0.25028457641601565, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.04084519296884537}, {"id": 1146, "seek": 585570, "start": 5874.74, "end": 5879.54, "text": " And it does have some weird kind of second-order effects on the learning rate.", "tokens": [51316, 400, 309, 775, 362, 512, 3657, 733, 295, 1150, 12, 4687, 5065, 322, 264, 2539, 3314, 13, 51556], "temperature": 0.0, "avg_logprob": -0.25028457641601565, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.04084519296884537}, {"id": 1147, "seek": 585570, "start": 5879.54, "end": 5880.94, "text": " But I don't think you should rely on them.", "tokens": [51556, 583, 286, 500, 380, 519, 291, 820, 10687, 322, 552, 13, 51626], "temperature": 0.0, "avg_logprob": -0.25028457641601565, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.04084519296884537}, {"id": 1148, "seek": 585570, "start": 5880.94, "end": 5884.46, "text": " You should use a scheduler for changing the learning rate rather than weird second-order", "tokens": [51626, 509, 820, 764, 257, 12000, 260, 337, 4473, 264, 2539, 3314, 2831, 813, 3657, 1150, 12, 4687, 51802], "temperature": 0.0, "avg_logprob": -0.25028457641601565, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.04084519296884537}, {"id": 1149, "seek": 588446, "start": 5884.46, "end": 5886.78, "text": " effects caused by weight decay.", "tokens": [50364, 5065, 7008, 538, 3364, 21039, 13, 50480], "temperature": 0.0, "avg_logprob": -0.24902293993079144, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.030674954876303673}, {"id": 1150, "seek": 588446, "start": 5886.78, "end": 5892.22, "text": " So instead we're going to do data augmentation, which is where we're going to modify every", "tokens": [50480, 407, 2602, 321, 434, 516, 281, 360, 1412, 14501, 19631, 11, 597, 307, 689, 321, 434, 516, 281, 16927, 633, 50752], "temperature": 0.0, "avg_logprob": -0.24902293993079144, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.030674954876303673}, {"id": 1151, "seek": 588446, "start": 5892.22, "end": 5900.58, "text": " image a little bit by random change so that it doesn't see the same image each time.", "tokens": [50752, 3256, 257, 707, 857, 538, 4974, 1319, 370, 300, 309, 1177, 380, 536, 264, 912, 3256, 1184, 565, 13, 51170], "temperature": 0.0, "avg_logprob": -0.24902293993079144, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.030674954876303673}, {"id": 1152, "seek": 588446, "start": 5900.58, "end": 5907.88, "text": " So there's not any particular reason to implement these from scratch, to be honest.", "tokens": [51170, 407, 456, 311, 406, 604, 1729, 1778, 281, 4445, 613, 490, 8459, 11, 281, 312, 3245, 13, 51535], "temperature": 0.0, "avg_logprob": -0.24902293993079144, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.030674954876303673}, {"id": 1153, "seek": 588446, "start": 5907.88, "end": 5911.88, "text": " We have implemented them all from scratch in Fast.ai, so you can certainly look them", "tokens": [51535, 492, 362, 12270, 552, 439, 490, 8459, 294, 15968, 13, 1301, 11, 370, 291, 393, 3297, 574, 552, 51735], "temperature": 0.0, "avg_logprob": -0.24902293993079144, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.030674954876303673}, {"id": 1154, "seek": 591188, "start": 5911.88, "end": 5915.4400000000005, "text": " up if you're interested.", "tokens": [50364, 493, 498, 291, 434, 3102, 13, 50542], "temperature": 0.0, "avg_logprob": -0.3365057064936711, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.17779065668582916}, {"id": 1155, "seek": 591188, "start": 5915.4400000000005, "end": 5918.64, "text": " But it's actually a little bit separate to what we're meant to be learning about, so", "tokens": [50542, 583, 309, 311, 767, 257, 707, 857, 4994, 281, 437, 321, 434, 4140, 281, 312, 2539, 466, 11, 370, 50702], "temperature": 0.0, "avg_logprob": -0.3365057064936711, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.17779065668582916}, {"id": 1156, "seek": 591188, "start": 5918.64, "end": 5921.16, "text": " I'm not going to go through it.", "tokens": [50702, 286, 478, 406, 516, 281, 352, 807, 309, 13, 50828], "temperature": 0.0, "avg_logprob": -0.3365057064936711, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.17779065668582916}, {"id": 1157, "seek": 591188, "start": 5921.16, "end": 5930.76, "text": " But yeah, if you're interested, go into Fast.ai, vision, augment, and you'll be able to see,", "tokens": [50828, 583, 1338, 11, 498, 291, 434, 3102, 11, 352, 666, 15968, 13, 1301, 11, 5201, 11, 29919, 11, 293, 291, 603, 312, 1075, 281, 536, 11, 51308], "temperature": 0.0, "avg_logprob": -0.3365057064936711, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.17779065668582916}, {"id": 1158, "seek": 591188, "start": 5930.76, "end": 5934.24, "text": " for example, how do we do flip.", "tokens": [51308, 337, 1365, 11, 577, 360, 321, 360, 7929, 13, 51482], "temperature": 0.0, "avg_logprob": -0.3365057064936711, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.17779065668582916}, {"id": 1159, "seek": 591188, "start": 5934.24, "end": 5936.52, "text": " And you know, it's just like X dot transpose.", "tokens": [51482, 400, 291, 458, 11, 309, 311, 445, 411, 1783, 5893, 25167, 13, 51596], "temperature": 0.0, "avg_logprob": -0.3365057064936711, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.17779065668582916}, {"id": 1160, "seek": 591188, "start": 5936.52, "end": 5939.2, "text": " Okay, which is not really...", "tokens": [51596, 1033, 11, 597, 307, 406, 534, 485, 51730], "temperature": 0.0, "avg_logprob": -0.3365057064936711, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.17779065668582916}, {"id": 1161, "seek": 593920, "start": 5939.2, "end": 5943.08, "text": " Yeah, it's not that interesting.", "tokens": [50364, 865, 11, 309, 311, 406, 300, 1880, 13, 50558], "temperature": 0.0, "avg_logprob": -0.32481138096299284, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.02333040162920952}, {"id": 1162, "seek": 593920, "start": 5943.08, "end": 5951.2, "text": " Yeah, how do we do cropping and padding?", "tokens": [50558, 865, 11, 577, 360, 321, 360, 4848, 3759, 293, 39562, 30, 50964], "temperature": 0.0, "avg_logprob": -0.32481138096299284, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.02333040162920952}, {"id": 1163, "seek": 593920, "start": 5951.2, "end": 5952.88, "text": " How do we do random crops?", "tokens": [50964, 1012, 360, 321, 360, 4974, 16829, 30, 51048], "temperature": 0.0, "avg_logprob": -0.32481138096299284, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.02333040162920952}, {"id": 1164, "seek": 593920, "start": 5952.88, "end": 5953.88, "text": " So on and so forth.", "tokens": [51048, 407, 322, 293, 370, 5220, 13, 51098], "temperature": 0.0, "avg_logprob": -0.32481138096299284, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.02333040162920952}, {"id": 1165, "seek": 593920, "start": 5953.88, "end": 5957.84, "text": " Okay, so we're just going to actually, you know, Fast.ai has probably got the best implementation", "tokens": [51098, 1033, 11, 370, 321, 434, 445, 516, 281, 767, 11, 291, 458, 11, 15968, 13, 1301, 575, 1391, 658, 264, 1151, 11420, 51296], "temperature": 0.0, "avg_logprob": -0.32481138096299284, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.02333040162920952}, {"id": 1166, "seek": 593920, "start": 5957.84, "end": 5967.599999999999, "text": " of these, but TorchVisions are fine, so we'll just use them.", "tokens": [51296, 295, 613, 11, 457, 7160, 339, 53, 4252, 366, 2489, 11, 370, 321, 603, 445, 764, 552, 13, 51784], "temperature": 0.0, "avg_logprob": -0.32481138096299284, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.02333040162920952}, {"id": 1167, "seek": 596760, "start": 5967.6, "end": 5979.240000000001, "text": " And so we've created before a batch transform callback, and we used it for normalization,", "tokens": [50364, 400, 370, 321, 600, 2942, 949, 257, 15245, 4088, 818, 3207, 11, 293, 321, 1143, 309, 337, 2710, 2144, 11, 50946], "temperature": 0.0, "avg_logprob": -0.24558725357055664, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0040072789415717125}, {"id": 1168, "seek": 596760, "start": 5979.240000000001, "end": 5980.240000000001, "text": " if you remember.", "tokens": [50946, 498, 291, 1604, 13, 50996], "temperature": 0.0, "avg_logprob": -0.24558725357055664, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0040072789415717125}, {"id": 1169, "seek": 596760, "start": 5980.240000000001, "end": 5993.400000000001, "text": " So what we could do is we could create a transform batch function, which transforms the inputs", "tokens": [50996, 407, 437, 321, 727, 360, 307, 321, 727, 1884, 257, 4088, 15245, 2445, 11, 597, 35592, 264, 15743, 51654], "temperature": 0.0, "avg_logprob": -0.24558725357055664, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0040072789415717125}, {"id": 1170, "seek": 599340, "start": 5993.4, "end": 6000.78, "text": " and transforms the outputs using two different functions.", "tokens": [50364, 293, 35592, 264, 23930, 1228, 732, 819, 6828, 13, 50733], "temperature": 0.0, "avg_logprob": -0.24430956739060422, "compression_ratio": 1.8040201005025125, "no_speech_prob": 0.003324428340420127}, {"id": 1171, "seek": 599340, "start": 6000.78, "end": 6003.5, "text": " So that would be an augmentation callback.", "tokens": [50733, 407, 300, 576, 312, 364, 14501, 19631, 818, 3207, 13, 50869], "temperature": 0.0, "avg_logprob": -0.24430956739060422, "compression_ratio": 1.8040201005025125, "no_speech_prob": 0.003324428340420127}, {"id": 1172, "seek": 599340, "start": 6003.5, "end": 6007.36, "text": " And so then you would say, okay, for the transform batch function, for example, in this case,", "tokens": [50869, 400, 370, 550, 291, 576, 584, 11, 1392, 11, 337, 264, 4088, 15245, 2445, 11, 337, 1365, 11, 294, 341, 1389, 11, 51062], "temperature": 0.0, "avg_logprob": -0.24430956739060422, "compression_ratio": 1.8040201005025125, "no_speech_prob": 0.003324428340420127}, {"id": 1173, "seek": 599340, "start": 6007.36, "end": 6009.839999999999, "text": " we want to transform our X's.", "tokens": [51062, 321, 528, 281, 4088, 527, 1783, 311, 13, 51186], "temperature": 0.0, "avg_logprob": -0.24430956739060422, "compression_ratio": 1.8040201005025125, "no_speech_prob": 0.003324428340420127}, {"id": 1174, "seek": 599340, "start": 6009.839999999999, "end": 6013.4, "text": " And how do we want to transform our X's?", "tokens": [51186, 400, 577, 360, 321, 528, 281, 4088, 527, 1783, 311, 30, 51364], "temperature": 0.0, "avg_logprob": -0.24430956739060422, "compression_ratio": 1.8040201005025125, "no_speech_prob": 0.003324428340420127}, {"id": 1175, "seek": 599340, "start": 6013.4, "end": 6021.759999999999, "text": " And the answer is, we want to transform them using this module, which is a sequential module,", "tokens": [51364, 400, 264, 1867, 307, 11, 321, 528, 281, 4088, 552, 1228, 341, 10088, 11, 597, 307, 257, 42881, 10088, 11, 51782], "temperature": 0.0, "avg_logprob": -0.24430956739060422, "compression_ratio": 1.8040201005025125, "no_speech_prob": 0.003324428340420127}, {"id": 1176, "seek": 602176, "start": 6021.76, "end": 6027.360000000001, "text": " first of all doing a random crop, and then a random horizontal flip.", "tokens": [50364, 700, 295, 439, 884, 257, 4974, 9086, 11, 293, 550, 257, 4974, 12750, 7929, 13, 50644], "temperature": 0.0, "avg_logprob": -0.25326152801513674, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0005976686952635646}, {"id": 1177, "seek": 602176, "start": 6027.360000000001, "end": 6032.88, "text": " Now it seems weird to randomly crop a 28 by 28 image to get a 28 by 28 image, but we can", "tokens": [50644, 823, 309, 2544, 3657, 281, 16979, 9086, 257, 7562, 538, 7562, 3256, 281, 483, 257, 7562, 538, 7562, 3256, 11, 457, 321, 393, 50920], "temperature": 0.0, "avg_logprob": -0.25326152801513674, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0005976686952635646}, {"id": 1178, "seek": 602176, "start": 6032.88, "end": 6034.24, "text": " add padding to it.", "tokens": [50920, 909, 39562, 281, 309, 13, 50988], "temperature": 0.0, "avg_logprob": -0.25326152801513674, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0005976686952635646}, {"id": 1179, "seek": 602176, "start": 6034.24, "end": 6038.4400000000005, "text": " And so effectively, it's going to randomly add padding on one or both sides to do this", "tokens": [50988, 400, 370, 8659, 11, 309, 311, 516, 281, 16979, 909, 39562, 322, 472, 420, 1293, 4881, 281, 360, 341, 51198], "temperature": 0.0, "avg_logprob": -0.25326152801513674, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0005976686952635646}, {"id": 1180, "seek": 602176, "start": 6038.4400000000005, "end": 6041.12, "text": " kind of random crop.", "tokens": [51198, 733, 295, 4974, 9086, 13, 51332], "temperature": 0.0, "avg_logprob": -0.25326152801513674, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0005976686952635646}, {"id": 1181, "seek": 602176, "start": 6041.12, "end": 6048.0, "text": " One thing I did to change the batch transform callback, can't remember if I've mentioned", "tokens": [51332, 1485, 551, 286, 630, 281, 1319, 264, 15245, 4088, 818, 3207, 11, 393, 380, 1604, 498, 286, 600, 2835, 51676], "temperature": 0.0, "avg_logprob": -0.25326152801513674, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0005976686952635646}, {"id": 1182, "seek": 604800, "start": 6048.0, "end": 6052.96, "text": " this before, but something I changed slightly since we first wrote it, is I added this on", "tokens": [50364, 341, 949, 11, 457, 746, 286, 3105, 4748, 1670, 321, 700, 4114, 309, 11, 307, 286, 3869, 341, 322, 50612], "temperature": 0.0, "avg_logprob": -0.24031316757202148, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.01744184084236622}, {"id": 1183, "seek": 604800, "start": 6052.96, "end": 6056.2, "text": " train and on validate.", "tokens": [50612, 3847, 293, 322, 29562, 13, 50774], "temperature": 0.0, "avg_logprob": -0.24031316757202148, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.01744184084236622}, {"id": 1184, "seek": 604800, "start": 6056.2, "end": 6061.8, "text": " So that it only does it if you said I want to do it on training and it's training, or", "tokens": [50774, 407, 300, 309, 787, 775, 309, 498, 291, 848, 286, 528, 281, 360, 309, 322, 3097, 293, 309, 311, 3097, 11, 420, 51054], "temperature": 0.0, "avg_logprob": -0.24031316757202148, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.01744184084236622}, {"id": 1185, "seek": 604800, "start": 6061.8, "end": 6065.04, "text": " I want to do it on validation and it's not training.", "tokens": [51054, 286, 528, 281, 360, 309, 322, 24071, 293, 309, 311, 406, 3097, 13, 51216], "temperature": 0.0, "avg_logprob": -0.24031316757202148, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.01744184084236622}, {"id": 1186, "seek": 604800, "start": 6065.04, "end": 6069.84, "text": " And then this is, this is all the code is.", "tokens": [51216, 400, 550, 341, 307, 11, 341, 307, 439, 264, 3089, 307, 13, 51456], "temperature": 0.0, "avg_logprob": -0.24031316757202148, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.01744184084236622}, {"id": 1187, "seek": 604800, "start": 6069.84, "end": 6075.48, "text": " So data augmentation, generally speaking, shouldn't be done on validation.", "tokens": [51456, 407, 1412, 14501, 19631, 11, 5101, 4124, 11, 4659, 380, 312, 1096, 322, 24071, 13, 51738], "temperature": 0.0, "avg_logprob": -0.24031316757202148, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.01744184084236622}, {"id": 1188, "seek": 607548, "start": 6075.48, "end": 6078.2, "text": " We set on validation false.", "tokens": [50364, 492, 992, 322, 24071, 7908, 13, 50500], "temperature": 0.0, "avg_logprob": -0.38512955393110004, "compression_ratio": 1.3591549295774648, "no_speech_prob": 0.035675931721925735}, {"id": 1189, "seek": 607548, "start": 6078.2, "end": 6088.12, "text": " Okay, so what I'm going to do first of all, is I'm going to use our classic single batch", "tokens": [50500, 1033, 11, 370, 437, 286, 478, 516, 281, 360, 700, 295, 439, 11, 307, 286, 478, 516, 281, 764, 527, 7230, 2167, 15245, 50996], "temperature": 0.0, "avg_logprob": -0.38512955393110004, "compression_ratio": 1.3591549295774648, "no_speech_prob": 0.035675931721925735}, {"id": 1190, "seek": 607548, "start": 6088.12, "end": 6098.599999999999, "text": " CB trick and fit, in fact even better, oh yeah, fit one just doing training.", "tokens": [50996, 18745, 4282, 293, 3318, 11, 294, 1186, 754, 1101, 11, 1954, 1338, 11, 3318, 472, 445, 884, 3097, 13, 51520], "temperature": 0.0, "avg_logprob": -0.38512955393110004, "compression_ratio": 1.3591549295774648, "no_speech_prob": 0.035675931721925735}, {"id": 1191, "seek": 609860, "start": 6098.6, "end": 6106.56, "text": " And what I'm going to do then is after I fit, I can grab the batch out of the learner.", "tokens": [50364, 400, 437, 286, 478, 516, 281, 360, 550, 307, 934, 286, 3318, 11, 286, 393, 4444, 264, 15245, 484, 295, 264, 33347, 13, 50762], "temperature": 0.0, "avg_logprob": -0.2548640167828902, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.014956426806747913}, {"id": 1192, "seek": 609860, "start": 6106.56, "end": 6108.400000000001, "text": " And this is a way, this is quite cool, right?", "tokens": [50762, 400, 341, 307, 257, 636, 11, 341, 307, 1596, 1627, 11, 558, 30, 50854], "temperature": 0.0, "avg_logprob": -0.2548640167828902, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.014956426806747913}, {"id": 1193, "seek": 609860, "start": 6108.400000000001, "end": 6112.320000000001, "text": " This is a way that I can see exactly what the model sees.", "tokens": [50854, 639, 307, 257, 636, 300, 286, 393, 536, 2293, 437, 264, 2316, 8194, 13, 51050], "temperature": 0.0, "avg_logprob": -0.2548640167828902, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.014956426806747913}, {"id": 1194, "seek": 609860, "start": 6112.320000000001, "end": 6113.360000000001, "text": " Right?", "tokens": [51050, 1779, 30, 51102], "temperature": 0.0, "avg_logprob": -0.2548640167828902, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.014956426806747913}, {"id": 1195, "seek": 609860, "start": 6113.360000000001, "end": 6119.76, "text": " So this is not relying on, on any, you know, approximations.", "tokens": [51102, 407, 341, 307, 406, 24140, 322, 11, 322, 604, 11, 291, 458, 11, 8542, 763, 13, 51422], "temperature": 0.0, "avg_logprob": -0.2548640167828902, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.014956426806747913}, {"id": 1196, "seek": 609860, "start": 6119.76, "end": 6124.06, "text": " Remember when we fit, it puts it in the batch that it looks at into learn.batch.", "tokens": [51422, 5459, 562, 321, 3318, 11, 309, 8137, 309, 294, 264, 15245, 300, 309, 1542, 412, 666, 1466, 13, 65, 852, 13, 51637], "temperature": 0.0, "avg_logprob": -0.2548640167828902, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.014956426806747913}, {"id": 1197, "seek": 612406, "start": 6124.06, "end": 6130.9400000000005, "text": " So if we fit for a single batch, we can then grab that batch back out of it, and we can", "tokens": [50364, 407, 498, 321, 3318, 337, 257, 2167, 15245, 11, 321, 393, 550, 4444, 300, 15245, 646, 484, 295, 309, 11, 293, 321, 393, 50708], "temperature": 0.0, "avg_logprob": -0.2505849901136461, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.004069925285875797}, {"id": 1198, "seek": 612406, "start": 6130.9400000000005, "end": 6132.700000000001, "text": " call show images.", "tokens": [50708, 818, 855, 5267, 13, 50796], "temperature": 0.0, "avg_logprob": -0.2505849901136461, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.004069925285875797}, {"id": 1199, "seek": 612406, "start": 6132.700000000001, "end": 6137.580000000001, "text": " And so here you can see this little crop it's added.", "tokens": [50796, 400, 370, 510, 291, 393, 536, 341, 707, 9086, 309, 311, 3869, 13, 51040], "temperature": 0.0, "avg_logprob": -0.2505849901136461, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.004069925285875797}, {"id": 1200, "seek": 612406, "start": 6137.580000000001, "end": 6140.700000000001, "text": " Now something you'll notice is that every single image in this batch, now just grab", "tokens": [51040, 823, 746, 291, 603, 3449, 307, 300, 633, 2167, 3256, 294, 341, 15245, 11, 586, 445, 4444, 51196], "temperature": 0.0, "avg_logprob": -0.2505849901136461, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.004069925285875797}, {"id": 1201, "seek": 612406, "start": 6140.700000000001, "end": 6147.42, "text": " the first 16, so I don't want to show you 1024, has exactly the same augmentation.", "tokens": [51196, 264, 700, 3165, 11, 370, 286, 500, 380, 528, 281, 855, 291, 1266, 7911, 11, 575, 2293, 264, 912, 14501, 19631, 13, 51532], "temperature": 0.0, "avg_logprob": -0.2505849901136461, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.004069925285875797}, {"id": 1202, "seek": 614742, "start": 6147.42, "end": 6150.14, "text": " And that makes sense, right?", "tokens": [50364, 400, 300, 1669, 2020, 11, 558, 30, 50500], "temperature": 0.0, "avg_logprob": -0.29708120957860407, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.009707866236567497}, {"id": 1203, "seek": 614742, "start": 6150.14, "end": 6154.14, "text": " Because we're applying a batch transform.", "tokens": [50500, 1436, 321, 434, 9275, 257, 15245, 4088, 13, 50700], "temperature": 0.0, "avg_logprob": -0.29708120957860407, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.009707866236567497}, {"id": 1204, "seek": 614742, "start": 6154.14, "end": 6156.5, "text": " Now why is this good and why is it bad?", "tokens": [50700, 823, 983, 307, 341, 665, 293, 983, 307, 309, 1578, 30, 50818], "temperature": 0.0, "avg_logprob": -0.29708120957860407, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.009707866236567497}, {"id": 1205, "seek": 614742, "start": 6156.5, "end": 6160.86, "text": " It's good because this is running on the GPU.", "tokens": [50818, 467, 311, 665, 570, 341, 307, 2614, 322, 264, 18407, 13, 51036], "temperature": 0.0, "avg_logprob": -0.29708120957860407, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.009707866236567497}, {"id": 1206, "seek": 614742, "start": 6160.86, "end": 6162.06, "text": " Right?", "tokens": [51036, 1779, 30, 51096], "temperature": 0.0, "avg_logprob": -0.29708120957860407, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.009707866236567497}, {"id": 1207, "seek": 614742, "start": 6162.06, "end": 6168.26, "text": " Which is great, because nowadays very often it's really hard to get enough CPU to feed", "tokens": [51096, 3013, 307, 869, 11, 570, 13434, 588, 2049, 309, 311, 534, 1152, 281, 483, 1547, 13199, 281, 3154, 51406], "temperature": 0.0, "avg_logprob": -0.29708120957860407, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.009707866236567497}, {"id": 1208, "seek": 614742, "start": 6168.26, "end": 6170.62, "text": " your fast GPU fast enough.", "tokens": [51406, 428, 2370, 18407, 2370, 1547, 13, 51524], "temperature": 0.0, "avg_logprob": -0.29708120957860407, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.009707866236567497}, {"id": 1209, "seek": 614742, "start": 6170.62, "end": 6174.5, "text": " Particularly if you use something like Kaggle or Colab that are really underpowered for", "tokens": [51524, 32281, 498, 291, 764, 746, 411, 48751, 22631, 420, 4004, 455, 300, 366, 534, 833, 27178, 337, 51718], "temperature": 0.0, "avg_logprob": -0.29708120957860407, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.009707866236567497}, {"id": 1210, "seek": 614742, "start": 6174.5, "end": 6176.74, "text": " CPU, particularly Kaggle.", "tokens": [51718, 13199, 11, 4098, 48751, 22631, 13, 51830], "temperature": 0.0, "avg_logprob": -0.29708120957860407, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.009707866236567497}, {"id": 1211, "seek": 617674, "start": 6177.0599999999995, "end": 6183.9, "text": " So this way all of our transformations, all of our augmentation is happening on the GPU.", "tokens": [50380, 407, 341, 636, 439, 295, 527, 34852, 11, 439, 295, 527, 14501, 19631, 307, 2737, 322, 264, 18407, 13, 50722], "temperature": 0.0, "avg_logprob": -0.19997765467717096, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.6187559594982304e-05}, {"id": 1212, "seek": 617674, "start": 6183.9, "end": 6187.82, "text": " On the downside, it means that there's a little bit less variety.", "tokens": [50722, 1282, 264, 25060, 11, 309, 1355, 300, 456, 311, 257, 707, 857, 1570, 5673, 13, 50918], "temperature": 0.0, "avg_logprob": -0.19997765467717096, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.6187559594982304e-05}, {"id": 1213, "seek": 617674, "start": 6187.82, "end": 6190.0199999999995, "text": " Every mini-batch has the same augmentation.", "tokens": [50918, 2048, 8382, 12, 65, 852, 575, 264, 912, 14501, 19631, 13, 51028], "temperature": 0.0, "avg_logprob": -0.19997765467717096, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.6187559594982304e-05}, {"id": 1214, "seek": 617674, "start": 6190.0199999999995, "end": 6193.86, "text": " I don't think the downside matters though, because it's going to see lots of mini-batches.", "tokens": [51028, 286, 500, 380, 519, 264, 25060, 7001, 1673, 11, 570, 309, 311, 516, 281, 536, 3195, 295, 8382, 12, 65, 852, 279, 13, 51220], "temperature": 0.0, "avg_logprob": -0.19997765467717096, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.6187559594982304e-05}, {"id": 1215, "seek": 617674, "start": 6193.86, "end": 6197.62, "text": " So the fact that each mini-batch is going to have a different augmentation is actually", "tokens": [51220, 407, 264, 1186, 300, 1184, 8382, 12, 65, 852, 307, 516, 281, 362, 257, 819, 14501, 19631, 307, 767, 51408], "temperature": 0.0, "avg_logprob": -0.19997765467717096, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.6187559594982304e-05}, {"id": 1216, "seek": 617674, "start": 6197.62, "end": 6199.42, "text": " all I care about.", "tokens": [51408, 439, 286, 1127, 466, 13, 51498], "temperature": 0.0, "avg_logprob": -0.19997765467717096, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.6187559594982304e-05}, {"id": 1217, "seek": 617674, "start": 6199.42, "end": 6206.219999999999, "text": " So we can see that if we run this multiple times, you can see it's got a different augmentation", "tokens": [51498, 407, 321, 393, 536, 300, 498, 321, 1190, 341, 3866, 1413, 11, 291, 393, 536, 309, 311, 658, 257, 819, 14501, 19631, 51838], "temperature": 0.0, "avg_logprob": -0.19997765467717096, "compression_ratio": 1.856060606060606, "no_speech_prob": 1.6187559594982304e-05}, {"id": 1218, "seek": 620622, "start": 6206.22, "end": 6207.22, "text": " in each mini-batch.", "tokens": [50364, 294, 1184, 8382, 12, 65, 852, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3050221488589332, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.010169414803385735}, {"id": 1219, "seek": 620622, "start": 6207.22, "end": 6216.820000000001, "text": " Okay, so I decided actually I'm just going to use one padding.", "tokens": [50414, 1033, 11, 370, 286, 3047, 767, 286, 478, 445, 516, 281, 764, 472, 39562, 13, 50894], "temperature": 0.0, "avg_logprob": -0.3050221488589332, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.010169414803385735}, {"id": 1220, "seek": 620622, "start": 6216.820000000001, "end": 6221.62, "text": " So I'm just going to do a very, very small amount of data augmentation.", "tokens": [50894, 407, 286, 478, 445, 516, 281, 360, 257, 588, 11, 588, 1359, 2372, 295, 1412, 14501, 19631, 13, 51134], "temperature": 0.0, "avg_logprob": -0.3050221488589332, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.010169414803385735}, {"id": 1221, "seek": 620622, "start": 6221.62, "end": 6228.58, "text": " And I'm going to do 20 epochs using one cycle learning rate.", "tokens": [51134, 400, 286, 478, 516, 281, 360, 945, 30992, 28346, 1228, 472, 6586, 2539, 3314, 13, 51482], "temperature": 0.0, "avg_logprob": -0.3050221488589332, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.010169414803385735}, {"id": 1222, "seek": 620622, "start": 6228.58, "end": 6232.780000000001, "text": " And so this takes quite a while to train, so you won't watch it.", "tokens": [51482, 400, 370, 341, 2516, 1596, 257, 1339, 281, 3847, 11, 370, 291, 1582, 380, 1159, 309, 13, 51692], "temperature": 0.0, "avg_logprob": -0.3050221488589332, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.010169414803385735}, {"id": 1223, "seek": 623278, "start": 6232.78, "end": 6240.46, "text": " But check this out, we get to 93.8.", "tokens": [50364, 583, 1520, 341, 484, 11, 321, 483, 281, 28876, 13, 23, 13, 50748], "temperature": 0.0, "avg_logprob": -0.28085688182285856, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.06953852623701096}, {"id": 1224, "seek": 623278, "start": 6240.46, "end": 6243.0599999999995, "text": " That's pretty wild.", "tokens": [50748, 663, 311, 1238, 4868, 13, 50878], "temperature": 0.0, "avg_logprob": -0.28085688182285856, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.06953852623701096}, {"id": 1225, "seek": 623278, "start": 6243.0599999999995, "end": 6247.139999999999, "text": " Yeah, that's pretty wild.", "tokens": [50878, 865, 11, 300, 311, 1238, 4868, 13, 51082], "temperature": 0.0, "avg_logprob": -0.28085688182285856, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.06953852623701096}, {"id": 1226, "seek": 623278, "start": 6247.139999999999, "end": 6255.34, "text": " So I actually went on Twitter and I said to the entire world on Twitter, you know, which", "tokens": [51082, 407, 286, 767, 1437, 322, 5794, 293, 286, 848, 281, 264, 2302, 1002, 322, 5794, 11, 291, 458, 11, 597, 51492], "temperature": 0.0, "avg_logprob": -0.28085688182285856, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.06953852623701096}, {"id": 1227, "seek": 623278, "start": 6255.34, "end": 6260.94, "text": " if you're watching this in 2023, if Twitter doesn't exist yet, ask somebody tell you about", "tokens": [51492, 498, 291, 434, 1976, 341, 294, 44377, 11, 498, 5794, 1177, 380, 2514, 1939, 11, 1029, 2618, 980, 291, 466, 51772], "temperature": 0.0, "avg_logprob": -0.28085688182285856, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.06953852623701096}, {"id": 1228, "seek": 623278, "start": 6260.94, "end": 6261.94, "text": " what Twitter used to be.", "tokens": [51772, 437, 5794, 1143, 281, 312, 13, 51822], "temperature": 0.0, "avg_logprob": -0.28085688182285856, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.06953852623701096}, {"id": 1229, "seek": 626194, "start": 6262.099999999999, "end": 6265.419999999999, "text": " Hopefully it still does.", "tokens": [50372, 10429, 309, 920, 775, 13, 50538], "temperature": 0.0, "avg_logprob": -0.24415565181422877, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.0006361896521411836}, {"id": 1230, "seek": 626194, "start": 6265.419999999999, "end": 6268.54, "text": " Can anybody beat this in 20 epochs?", "tokens": [50538, 1664, 4472, 4224, 341, 294, 945, 30992, 28346, 30, 50694], "temperature": 0.0, "avg_logprob": -0.24415565181422877, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.0006361896521411836}, {"id": 1231, "seek": 626194, "start": 6268.54, "end": 6277.86, "text": " You can use any model you like, any library you like, and nobody's got anywhere close.", "tokens": [50694, 509, 393, 764, 604, 2316, 291, 411, 11, 604, 6405, 291, 411, 11, 293, 5079, 311, 658, 4992, 1998, 13, 51160], "temperature": 0.0, "avg_logprob": -0.24415565181422877, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.0006361896521411836}, {"id": 1232, "seek": 626194, "start": 6277.86, "end": 6280.58, "text": " So this is pretty amazing.", "tokens": [51160, 407, 341, 307, 1238, 2243, 13, 51296], "temperature": 0.0, "avg_logprob": -0.24415565181422877, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.0006361896521411836}, {"id": 1233, "seek": 626194, "start": 6280.58, "end": 6288.5, "text": " And actually, you know, when I had a look at papers with code, there are, you know,", "tokens": [51296, 400, 767, 11, 291, 458, 11, 562, 286, 632, 257, 574, 412, 10577, 365, 3089, 11, 456, 366, 11, 291, 458, 11, 51692], "temperature": 0.0, "avg_logprob": -0.24415565181422877, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.0006361896521411836}, {"id": 1234, "seek": 628850, "start": 6288.78, "end": 6294.18, "text": " Well, I mean, you can see it's right up there, right, with the kind of best models that are", "tokens": [50378, 1042, 11, 286, 914, 11, 291, 393, 536, 309, 311, 558, 493, 456, 11, 558, 11, 365, 264, 733, 295, 1151, 5245, 300, 366, 50648], "temperature": 0.0, "avg_logprob": -0.27796975858918915, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0004044789238832891}, {"id": 1235, "seek": 628850, "start": 6294.18, "end": 6298.34, "text": " listed, certainly better than these ones.", "tokens": [50648, 10052, 11, 3297, 1101, 813, 613, 2306, 13, 50856], "temperature": 0.0, "avg_logprob": -0.27796975858918915, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0004044789238832891}, {"id": 1236, "seek": 628850, "start": 6298.34, "end": 6306.14, "text": " And the better models all use, you know, 250 or more epochs.", "tokens": [50856, 400, 264, 1101, 5245, 439, 764, 11, 291, 458, 11, 11650, 420, 544, 30992, 28346, 13, 51246], "temperature": 0.0, "avg_logprob": -0.27796975858918915, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0004044789238832891}, {"id": 1237, "seek": 628850, "start": 6306.14, "end": 6312.7, "text": " So yeah, if anybody, I'm hoping that somebody watching this will find a way to beat this", "tokens": [51246, 407, 1338, 11, 498, 4472, 11, 286, 478, 7159, 300, 2618, 1976, 341, 486, 915, 257, 636, 281, 4224, 341, 51574], "temperature": 0.0, "avg_logprob": -0.27796975858918915, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0004044789238832891}, {"id": 1238, "seek": 628850, "start": 6312.7, "end": 6314.74, "text": " in 20 epochs, that would be really great.", "tokens": [51574, 294, 945, 30992, 28346, 11, 300, 576, 312, 534, 869, 13, 51676], "temperature": 0.0, "avg_logprob": -0.27796975858918915, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0004044789238832891}, {"id": 1239, "seek": 631474, "start": 6315.5, "end": 6321.139999999999, "text": " Because as you can see, we haven't really done anything very amazingly, weirdly clever.", "tokens": [50402, 1436, 382, 291, 393, 536, 11, 321, 2378, 380, 534, 1096, 1340, 588, 31762, 11, 48931, 13494, 13, 50684], "temperature": 0.0, "avg_logprob": -0.22258956344039352, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.007232659962028265}, {"id": 1240, "seek": 631474, "start": 6321.139999999999, "end": 6323.78, "text": " It's all very, very basic.", "tokens": [50684, 467, 311, 439, 588, 11, 588, 3875, 13, 50816], "temperature": 0.0, "avg_logprob": -0.22258956344039352, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.007232659962028265}, {"id": 1241, "seek": 631474, "start": 6323.78, "end": 6330.94, "text": " And actually, we can go even a bit further than 93.8.", "tokens": [50816, 400, 767, 11, 321, 393, 352, 754, 257, 857, 3052, 813, 28876, 13, 23, 13, 51174], "temperature": 0.0, "avg_logprob": -0.22258956344039352, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.007232659962028265}, {"id": 1242, "seek": 631474, "start": 6330.94, "end": 6335.099999999999, "text": " Just before we do, I mentioned that since this is actually taking a while to train now,", "tokens": [51174, 1449, 949, 321, 360, 11, 286, 2835, 300, 1670, 341, 307, 767, 1940, 257, 1339, 281, 3847, 586, 11, 51382], "temperature": 0.0, "avg_logprob": -0.22258956344039352, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.007232659962028265}, {"id": 1243, "seek": 631474, "start": 6335.099999999999, "end": 6340.94, "text": " I can't remember, it takes like 10 to 15 seconds per epoch.", "tokens": [51382, 286, 393, 380, 1604, 11, 309, 2516, 411, 1266, 281, 2119, 3949, 680, 30992, 339, 13, 51674], "temperature": 0.0, "avg_logprob": -0.22258956344039352, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.007232659962028265}, {"id": 1244, "seek": 631474, "start": 6340.94, "end": 6344.7, "text": " So you know, you're waiting a few minutes, you may as well save it.", "tokens": [51674, 407, 291, 458, 11, 291, 434, 3806, 257, 1326, 2077, 11, 291, 815, 382, 731, 3155, 309, 13, 51862], "temperature": 0.0, "avg_logprob": -0.22258956344039352, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.007232659962028265}, {"id": 1245, "seek": 634470, "start": 6345.66, "end": 6356.74, "text": " So you can just call torch.save on a model, and then you can load that back later.", "tokens": [50412, 407, 291, 393, 445, 818, 27822, 13, 82, 946, 322, 257, 2316, 11, 293, 550, 291, 393, 3677, 300, 646, 1780, 13, 50966], "temperature": 0.0, "avg_logprob": -0.3633224602901574, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.0005976644461043179}, {"id": 1246, "seek": 634470, "start": 6356.74, "end": 6362.42, "text": " So something that can make things even better is something called test time augmentation.", "tokens": [50966, 407, 746, 300, 393, 652, 721, 754, 1101, 307, 746, 1219, 1500, 565, 14501, 19631, 13, 51250], "temperature": 0.0, "avg_logprob": -0.3633224602901574, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.0005976644461043179}, {"id": 1247, "seek": 634470, "start": 6362.42, "end": 6365.34, "text": " I guess I should write this out properly here.", "tokens": [51250, 286, 2041, 286, 820, 2464, 341, 484, 6108, 510, 13, 51396], "temperature": 0.0, "avg_logprob": -0.3633224602901574, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.0005976644461043179}, {"id": 1248, "seek": 634470, "start": 6365.34, "end": 6370.94, "text": " Test text, test time augmentation.", "tokens": [51396, 9279, 2487, 11, 1500, 565, 14501, 19631, 13, 51676], "temperature": 0.0, "avg_logprob": -0.3633224602901574, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.0005976644461043179}, {"id": 1249, "seek": 637094, "start": 6371.94, "end": 6382.299999999999, "text": " Now, test time augmentation actually does our batch transform callback on validation", "tokens": [50414, 823, 11, 1500, 565, 14501, 19631, 767, 775, 527, 15245, 4088, 818, 3207, 322, 24071, 50932], "temperature": 0.0, "avg_logprob": -0.25391296491231, "compression_ratio": 1.7987012987012987, "no_speech_prob": 0.0003799765545409173}, {"id": 1250, "seek": 637094, "start": 6382.299999999999, "end": 6384.0599999999995, "text": " as well.", "tokens": [50932, 382, 731, 13, 51020], "temperature": 0.0, "avg_logprob": -0.25391296491231, "compression_ratio": 1.7987012987012987, "no_speech_prob": 0.0003799765545409173}, {"id": 1251, "seek": 637094, "start": 6384.0599999999995, "end": 6389.46, "text": " And then what we're going to do is we're actually, in this case, we're going to do just a very,", "tokens": [51020, 400, 550, 437, 321, 434, 516, 281, 360, 307, 321, 434, 767, 11, 294, 341, 1389, 11, 321, 434, 516, 281, 360, 445, 257, 588, 11, 51290], "temperature": 0.0, "avg_logprob": -0.25391296491231, "compression_ratio": 1.7987012987012987, "no_speech_prob": 0.0003799765545409173}, {"id": 1252, "seek": 637094, "start": 6389.46, "end": 6396.419999999999, "text": " very, very simple test time augmentation, which is we're going to add a batch transform", "tokens": [51290, 588, 11, 588, 2199, 1500, 565, 14501, 19631, 11, 597, 307, 321, 434, 516, 281, 909, 257, 15245, 4088, 51638], "temperature": 0.0, "avg_logprob": -0.25391296491231, "compression_ratio": 1.7987012987012987, "no_speech_prob": 0.0003799765545409173}, {"id": 1253, "seek": 639642, "start": 6396.42, "end": 6398.38, "text": " callback that runs on validate.", "tokens": [50364, 818, 3207, 300, 6676, 322, 29562, 13, 50462], "temperature": 0.0, "avg_logprob": -0.2282067581459328, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.03963695466518402}, {"id": 1254, "seek": 639642, "start": 6398.38, "end": 6404.1, "text": " And it's not random, but it actually just does a horizontal flip.", "tokens": [50462, 400, 309, 311, 406, 4974, 11, 457, 309, 767, 445, 775, 257, 12750, 7929, 13, 50748], "temperature": 0.0, "avg_logprob": -0.2282067581459328, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.03963695466518402}, {"id": 1255, "seek": 639642, "start": 6404.1, "end": 6406.58, "text": " Non-random, so it always does a horizontal flip.", "tokens": [50748, 8774, 12, 3699, 298, 11, 370, 309, 1009, 775, 257, 12750, 7929, 13, 50872], "temperature": 0.0, "avg_logprob": -0.2282067581459328, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.03963695466518402}, {"id": 1256, "seek": 639642, "start": 6406.58, "end": 6407.58, "text": " And so check this out.", "tokens": [50872, 400, 370, 1520, 341, 484, 13, 50922], "temperature": 0.0, "avg_logprob": -0.2282067581459328, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.03963695466518402}, {"id": 1257, "seek": 639642, "start": 6407.58, "end": 6414.08, "text": " What we're going to do is we're going to create a new callback called capture preds.", "tokens": [50922, 708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1884, 257, 777, 818, 3207, 1219, 7983, 3852, 82, 13, 51247], "temperature": 0.0, "avg_logprob": -0.2282067581459328, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.03963695466518402}, {"id": 1258, "seek": 639642, "start": 6414.08, "end": 6420.4800000000005, "text": " And after each batch, it's just going to append to a list the predictions, and it's going", "tokens": [51247, 400, 934, 1184, 15245, 11, 309, 311, 445, 516, 281, 34116, 281, 257, 1329, 264, 21264, 11, 293, 309, 311, 516, 51567], "temperature": 0.0, "avg_logprob": -0.2282067581459328, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.03963695466518402}, {"id": 1259, "seek": 639642, "start": 6420.4800000000005, "end": 6424.34, "text": " to append to a different list the targets.", "tokens": [51567, 281, 34116, 281, 257, 819, 1329, 264, 12911, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2282067581459328, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.03963695466518402}, {"id": 1260, "seek": 642434, "start": 6424.34, "end": 6433.46, "text": " And that way we can just call learn.fit train equals false, and it will show us the accuracy.", "tokens": [50364, 400, 300, 636, 321, 393, 445, 818, 1466, 13, 6845, 3847, 6915, 7908, 11, 293, 309, 486, 855, 505, 264, 14170, 13, 50820], "temperature": 0.0, "avg_logprob": -0.25892185928797956, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.006797462701797485}, {"id": 1261, "seek": 642434, "start": 6433.46, "end": 6436.9800000000005, "text": " Okay, and this is just the same number that we saw before.", "tokens": [50820, 1033, 11, 293, 341, 307, 445, 264, 912, 1230, 300, 321, 1866, 949, 13, 50996], "temperature": 0.0, "avg_logprob": -0.25892185928797956, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.006797462701797485}, {"id": 1262, "seek": 642434, "start": 6436.9800000000005, "end": 6441.7, "text": " But then what we can do is we can call the same thing, but this time with a different", "tokens": [50996, 583, 550, 437, 321, 393, 360, 307, 321, 393, 818, 264, 912, 551, 11, 457, 341, 565, 365, 257, 819, 51232], "temperature": 0.0, "avg_logprob": -0.25892185928797956, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.006797462701797485}, {"id": 1263, "seek": 642434, "start": 6441.7, "end": 6450.7, "text": " callback, which is with the horizontal flip callback.", "tokens": [51232, 818, 3207, 11, 597, 307, 365, 264, 12750, 7929, 818, 3207, 13, 51682], "temperature": 0.0, "avg_logprob": -0.25892185928797956, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.006797462701797485}, {"id": 1264, "seek": 642434, "start": 6450.7, "end": 6453.9800000000005, "text": " And that way it's going to do exactly the same thing as before, but in every time it's", "tokens": [51682, 400, 300, 636, 309, 311, 516, 281, 360, 2293, 264, 912, 551, 382, 949, 11, 457, 294, 633, 565, 309, 311, 51846], "temperature": 0.0, "avg_logprob": -0.25892185928797956, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.006797462701797485}, {"id": 1265, "seek": 645398, "start": 6454.0199999999995, "end": 6456.139999999999, "text": " going to do a horizontal flip.", "tokens": [50366, 516, 281, 360, 257, 12750, 7929, 13, 50472], "temperature": 0.0, "avg_logprob": -0.25346144450079533, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0022518394980579615}, {"id": 1266, "seek": 645398, "start": 6456.139999999999, "end": 6460.459999999999, "text": " And weirdly enough, that accuracy is slightly higher, which that's not the interesting bit.", "tokens": [50472, 400, 48931, 1547, 11, 300, 14170, 307, 4748, 2946, 11, 597, 300, 311, 406, 264, 1880, 857, 13, 50688], "temperature": 0.0, "avg_logprob": -0.25346144450079533, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0022518394980579615}, {"id": 1267, "seek": 645398, "start": 6460.459999999999, "end": 6465.7, "text": " The interesting bit is that we've now got two sets of predictions.", "tokens": [50688, 440, 1880, 857, 307, 300, 321, 600, 586, 658, 732, 6352, 295, 21264, 13, 50950], "temperature": 0.0, "avg_logprob": -0.25346144450079533, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0022518394980579615}, {"id": 1268, "seek": 645398, "start": 6465.7, "end": 6468.959999999999, "text": " We've got the sets of predictions with the non-flipped version.", "tokens": [50950, 492, 600, 658, 264, 6352, 295, 21264, 365, 264, 2107, 12, 69, 2081, 3320, 3037, 13, 51113], "temperature": 0.0, "avg_logprob": -0.25346144450079533, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0022518394980579615}, {"id": 1269, "seek": 645398, "start": 6468.959999999999, "end": 6472.78, "text": " We've got the set of predictions with the flipped version.", "tokens": [51113, 492, 600, 658, 264, 992, 295, 21264, 365, 264, 26273, 3037, 13, 51304], "temperature": 0.0, "avg_logprob": -0.25346144450079533, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0022518394980579615}, {"id": 1270, "seek": 645398, "start": 6472.78, "end": 6479.62, "text": " And what we could do is we could stack those together and take the mean.", "tokens": [51304, 400, 437, 321, 727, 360, 307, 321, 727, 8630, 729, 1214, 293, 747, 264, 914, 13, 51646], "temperature": 0.0, "avg_logprob": -0.25346144450079533, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0022518394980579615}, {"id": 1271, "seek": 647962, "start": 6479.62, "end": 6485.18, "text": " So we're going to take the average of the flipped and unflipped predictions.", "tokens": [50364, 407, 321, 434, 516, 281, 747, 264, 4274, 295, 264, 26273, 293, 517, 3423, 5529, 21264, 13, 50642], "temperature": 0.0, "avg_logprob": -0.22619303878472777, "compression_ratio": 1.625, "no_speech_prob": 0.006903445348143578}, {"id": 1272, "seek": 647962, "start": 6485.18, "end": 6490.78, "text": " And that gives us a better result still, 94.2%.", "tokens": [50642, 400, 300, 2709, 505, 257, 1101, 1874, 920, 11, 30849, 13, 17, 6856, 50922], "temperature": 0.0, "avg_logprob": -0.22619303878472777, "compression_ratio": 1.625, "no_speech_prob": 0.006903445348143578}, {"id": 1273, "seek": 647962, "start": 6490.78, "end": 6492.22, "text": " So why is it better?", "tokens": [50922, 407, 983, 307, 309, 1101, 30, 50994], "temperature": 0.0, "avg_logprob": -0.22619303878472777, "compression_ratio": 1.625, "no_speech_prob": 0.006903445348143578}, {"id": 1274, "seek": 647962, "start": 6492.22, "end": 6498.0199999999995, "text": " It's because looking at the image from kind of like multiple different directions gives", "tokens": [50994, 467, 311, 570, 1237, 412, 264, 3256, 490, 733, 295, 411, 3866, 819, 11095, 2709, 51284], "temperature": 0.0, "avg_logprob": -0.22619303878472777, "compression_ratio": 1.625, "no_speech_prob": 0.006903445348143578}, {"id": 1275, "seek": 647962, "start": 6498.0199999999995, "end": 6501.5, "text": " it more opportunities to try to understand what this is a picture of.", "tokens": [51284, 309, 544, 4786, 281, 853, 281, 1223, 437, 341, 307, 257, 3036, 295, 13, 51458], "temperature": 0.0, "avg_logprob": -0.22619303878472777, "compression_ratio": 1.625, "no_speech_prob": 0.006903445348143578}, {"id": 1276, "seek": 647962, "start": 6501.5, "end": 6504.98, "text": " And so in this case, I'm just giving it two different directions, which is the flipped", "tokens": [51458, 400, 370, 294, 341, 1389, 11, 286, 478, 445, 2902, 309, 732, 819, 11095, 11, 597, 307, 264, 26273, 51632], "temperature": 0.0, "avg_logprob": -0.22619303878472777, "compression_ratio": 1.625, "no_speech_prob": 0.006903445348143578}, {"id": 1277, "seek": 650498, "start": 6504.98, "end": 6513.24, "text": " and unflipped version, and then just taking their average.", "tokens": [50364, 293, 517, 3423, 5529, 3037, 11, 293, 550, 445, 1940, 641, 4274, 13, 50777], "temperature": 0.0, "avg_logprob": -0.26829335596654325, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.13116438686847687}, {"id": 1278, "seek": 650498, "start": 6513.24, "end": 6521.459999999999, "text": " So yeah, this is like a really nice little trick.", "tokens": [50777, 407, 1338, 11, 341, 307, 411, 257, 534, 1481, 707, 4282, 13, 51188], "temperature": 0.0, "avg_logprob": -0.26829335596654325, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.13116438686847687}, {"id": 1279, "seek": 650498, "start": 6521.459999999999, "end": 6524.139999999999, "text": " Sam's pointed out it's a bit like random forest, which is true.", "tokens": [51188, 4832, 311, 10932, 484, 309, 311, 257, 857, 411, 4974, 6719, 11, 597, 307, 2074, 13, 51322], "temperature": 0.0, "avg_logprob": -0.26829335596654325, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.13116438686847687}, {"id": 1280, "seek": 650498, "start": 6524.139999999999, "end": 6525.78, "text": " It's a kind of bagging that we're doing.", "tokens": [51322, 467, 311, 257, 733, 295, 3411, 3249, 300, 321, 434, 884, 13, 51404], "temperature": 0.0, "avg_logprob": -0.26829335596654325, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.13116438686847687}, {"id": 1281, "seek": 650498, "start": 6525.78, "end": 6532.5599999999995, "text": " We're kind of getting multiple predictions and bringing them together.", "tokens": [51404, 492, 434, 733, 295, 1242, 3866, 21264, 293, 5062, 552, 1214, 13, 51743], "temperature": 0.0, "avg_logprob": -0.26829335596654325, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.13116438686847687}, {"id": 1282, "seek": 653256, "start": 6532.56, "end": 6543.56, "text": " And so we can actually, so 94.2% I think is my best 20 epoch result.", "tokens": [50364, 400, 370, 321, 393, 767, 11, 370, 30849, 13, 17, 4, 286, 519, 307, 452, 1151, 945, 30992, 339, 1874, 13, 50914], "temperature": 0.0, "avg_logprob": -0.26331439460675743, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.006692761555314064}, {"id": 1283, "seek": 653256, "start": 6543.56, "end": 6545.68, "text": " And notice I didn't have to do any additional training.", "tokens": [50914, 400, 3449, 286, 994, 380, 362, 281, 360, 604, 4497, 3097, 13, 51020], "temperature": 0.0, "avg_logprob": -0.26331439460675743, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.006692761555314064}, {"id": 1284, "seek": 653256, "start": 6545.68, "end": 6549.9400000000005, "text": " So it still counts as a 20 epoch result.", "tokens": [51020, 407, 309, 920, 14893, 382, 257, 945, 30992, 339, 1874, 13, 51233], "temperature": 0.0, "avg_logprob": -0.26331439460675743, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.006692761555314064}, {"id": 1285, "seek": 653256, "start": 6549.9400000000005, "end": 6553.96, "text": " You can do test time augmentation where you do, you know, a much wider range of different", "tokens": [51233, 509, 393, 360, 1500, 565, 14501, 19631, 689, 291, 360, 11, 291, 458, 11, 257, 709, 11842, 3613, 295, 819, 51434], "temperature": 0.0, "avg_logprob": -0.26331439460675743, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.006692761555314064}, {"id": 1286, "seek": 653256, "start": 6553.96, "end": 6555.64, "text": " augmentations that you trained with.", "tokens": [51434, 29919, 763, 300, 291, 8895, 365, 13, 51518], "temperature": 0.0, "avg_logprob": -0.26331439460675743, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.006692761555314064}, {"id": 1287, "seek": 653256, "start": 6555.64, "end": 6557.76, "text": " And then you can use them at test time as well.", "tokens": [51518, 400, 550, 291, 393, 764, 552, 412, 1500, 565, 382, 731, 13, 51624], "temperature": 0.0, "avg_logprob": -0.26331439460675743, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.006692761555314064}, {"id": 1288, "seek": 655776, "start": 6557.76, "end": 6563.04, "text": " You know, more crops or rotations or warps or whatever.", "tokens": [50364, 509, 458, 11, 544, 16829, 420, 44796, 420, 1516, 1878, 420, 2035, 13, 50628], "temperature": 0.0, "avg_logprob": -0.2244114769829644, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.15815791487693787}, {"id": 1289, "seek": 655776, "start": 6563.04, "end": 6567.84, "text": " I want to show you one of my favorite data augmentation approaches, which is called random", "tokens": [50628, 286, 528, 281, 855, 291, 472, 295, 452, 2954, 1412, 14501, 19631, 11587, 11, 597, 307, 1219, 4974, 50868], "temperature": 0.0, "avg_logprob": -0.2244114769829644, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.15815791487693787}, {"id": 1290, "seek": 655776, "start": 6567.84, "end": 6571.12, "text": " erasing.", "tokens": [50868, 1189, 3349, 13, 51032], "temperature": 0.0, "avg_logprob": -0.2244114769829644, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.15815791487693787}, {"id": 1291, "seek": 655776, "start": 6571.12, "end": 6575.84, "text": " So random erasing, I'll show you what it's going to look like.", "tokens": [51032, 407, 4974, 1189, 3349, 11, 286, 603, 855, 291, 437, 309, 311, 516, 281, 574, 411, 13, 51268], "temperature": 0.0, "avg_logprob": -0.2244114769829644, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.15815791487693787}, {"id": 1292, "seek": 655776, "start": 6575.84, "end": 6583.0, "text": " Random erasing, we're going to add a little, we're going to basically delete a little bit", "tokens": [51268, 37603, 1189, 3349, 11, 321, 434, 516, 281, 909, 257, 707, 11, 321, 434, 516, 281, 1936, 12097, 257, 707, 857, 51626], "temperature": 0.0, "avg_logprob": -0.2244114769829644, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.15815791487693787}, {"id": 1293, "seek": 655776, "start": 6583.0, "end": 6585.4800000000005, "text": " of each picture.", "tokens": [51626, 295, 1184, 3036, 13, 51750], "temperature": 0.0, "avg_logprob": -0.2244114769829644, "compression_ratio": 1.7015706806282722, "no_speech_prob": 0.15815791487693787}, {"id": 1294, "seek": 658548, "start": 6585.48, "end": 6589.04, "text": " And we're going to replace it with some random Gaussian noise.", "tokens": [50364, 400, 321, 434, 516, 281, 7406, 309, 365, 512, 4974, 39148, 5658, 13, 50542], "temperature": 0.0, "avg_logprob": -0.285566650167869, "compression_ratio": 1.7905405405405406, "no_speech_prob": 0.012431206181645393}, {"id": 1295, "seek": 658548, "start": 6589.04, "end": 6593.16, "text": " Now in this case, we just got one patch, but eventually we're going to do more than one", "tokens": [50542, 823, 294, 341, 1389, 11, 321, 445, 658, 472, 9972, 11, 457, 4728, 321, 434, 516, 281, 360, 544, 813, 472, 50748], "temperature": 0.0, "avg_logprob": -0.285566650167869, "compression_ratio": 1.7905405405405406, "no_speech_prob": 0.012431206181645393}, {"id": 1296, "seek": 658548, "start": 6593.16, "end": 6594.32, "text": " patch.", "tokens": [50748, 9972, 13, 50806], "temperature": 0.0, "avg_logprob": -0.285566650167869, "compression_ratio": 1.7905405405405406, "no_speech_prob": 0.012431206181645393}, {"id": 1297, "seek": 658548, "start": 6594.32, "end": 6599.12, "text": " So I wanted to implement this because remember we have to implement everything from scratch.", "tokens": [50806, 407, 286, 1415, 281, 4445, 341, 570, 1604, 321, 362, 281, 4445, 1203, 490, 8459, 13, 51046], "temperature": 0.0, "avg_logprob": -0.285566650167869, "compression_ratio": 1.7905405405405406, "no_speech_prob": 0.012431206181645393}, {"id": 1298, "seek": 658548, "start": 6599.12, "end": 6602.959999999999, "text": " This one's a bit less trivial than the previous transforms.", "tokens": [51046, 639, 472, 311, 257, 857, 1570, 26703, 813, 264, 3894, 35592, 13, 51238], "temperature": 0.0, "avg_logprob": -0.285566650167869, "compression_ratio": 1.7905405405405406, "no_speech_prob": 0.012431206181645393}, {"id": 1299, "seek": 658548, "start": 6602.959999999999, "end": 6604.74, "text": " So we should do it from scratch.", "tokens": [51238, 407, 321, 820, 360, 309, 490, 8459, 13, 51327], "temperature": 0.0, "avg_logprob": -0.285566650167869, "compression_ratio": 1.7905405405405406, "no_speech_prob": 0.012431206181645393}, {"id": 1300, "seek": 658548, "start": 6604.74, "end": 6607.28, "text": " And also I'm not sure there's that many good implementations.", "tokens": [51327, 400, 611, 286, 478, 406, 988, 456, 311, 300, 867, 665, 4445, 763, 13, 51454], "temperature": 0.0, "avg_logprob": -0.285566650167869, "compression_ratio": 1.7905405405405406, "no_speech_prob": 0.012431206181645393}, {"id": 1301, "seek": 658548, "start": 6607.28, "end": 6609.879999999999, "text": " Ross Whiteman's team, I think has one.", "tokens": [51454, 16140, 21693, 15023, 311, 1469, 11, 286, 519, 575, 472, 13, 51584], "temperature": 0.0, "avg_logprob": -0.285566650167869, "compression_ratio": 1.7905405405405406, "no_speech_prob": 0.012431206181645393}, {"id": 1302, "seek": 658548, "start": 6609.879999999999, "end": 6615.12, "text": " And so, and it's also a very good exercise to see how to implement this from scratch.", "tokens": [51584, 400, 370, 11, 293, 309, 311, 611, 257, 588, 665, 5380, 281, 536, 577, 281, 4445, 341, 490, 8459, 13, 51846], "temperature": 0.0, "avg_logprob": -0.285566650167869, "compression_ratio": 1.7905405405405406, "no_speech_prob": 0.012431206181645393}, {"id": 1303, "seek": 661512, "start": 6615.76, "end": 6621.5199999999995, "text": " So let's grab a batch out of the training set.", "tokens": [50396, 407, 718, 311, 4444, 257, 15245, 484, 295, 264, 3097, 992, 13, 50684], "temperature": 0.0, "avg_logprob": -0.2635163657272918, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0011513952631503344}, {"id": 1304, "seek": 661512, "start": 6621.5199999999995, "end": 6624.96, "text": " And let's just grab the first 16 images.", "tokens": [50684, 400, 718, 311, 445, 4444, 264, 700, 3165, 5267, 13, 50856], "temperature": 0.0, "avg_logprob": -0.2635163657272918, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0011513952631503344}, {"id": 1305, "seek": 661512, "start": 6624.96, "end": 6629.96, "text": " And so then let's grab the mean and standard deviation.", "tokens": [50856, 400, 370, 550, 718, 311, 4444, 264, 914, 293, 3832, 25163, 13, 51106], "temperature": 0.0, "avg_logprob": -0.2635163657272918, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0011513952631503344}, {"id": 1306, "seek": 661512, "start": 6629.96, "end": 6636.46, "text": " And so what we want to do is we wanted to delete a patch from each image.", "tokens": [51106, 400, 370, 437, 321, 528, 281, 360, 307, 321, 1415, 281, 12097, 257, 9972, 490, 1184, 3256, 13, 51431], "temperature": 0.0, "avg_logprob": -0.2635163657272918, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0011513952631503344}, {"id": 1307, "seek": 661512, "start": 6636.46, "end": 6641.8, "text": " But rather than deleting it, deleting it would change the statistics, right?", "tokens": [51431, 583, 2831, 813, 48946, 309, 11, 48946, 309, 576, 1319, 264, 12523, 11, 558, 30, 51698], "temperature": 0.0, "avg_logprob": -0.2635163657272918, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0011513952631503344}, {"id": 1308, "seek": 664180, "start": 6641.8, "end": 6646.360000000001, "text": " If we set those orders zero, the mean and standard deviation are now not going to be", "tokens": [50364, 759, 321, 992, 729, 9470, 4018, 11, 264, 914, 293, 3832, 25163, 366, 586, 406, 516, 281, 312, 50592], "temperature": 0.0, "avg_logprob": -0.2110905647277832, "compression_ratio": 1.7669491525423728, "no_speech_prob": 0.011330966837704182}, {"id": 1309, "seek": 664180, "start": 6646.360000000001, "end": 6648.4800000000005, "text": " zero one anymore.", "tokens": [50592, 4018, 472, 3602, 13, 50698], "temperature": 0.0, "avg_logprob": -0.2110905647277832, "compression_ratio": 1.7669491525423728, "no_speech_prob": 0.011330966837704182}, {"id": 1310, "seek": 664180, "start": 6648.4800000000005, "end": 6654.2, "text": " But if we replace them with exactly the same mean and standard deviation pixels that the", "tokens": [50698, 583, 498, 321, 7406, 552, 365, 2293, 264, 912, 914, 293, 3832, 25163, 18668, 300, 264, 50984], "temperature": 0.0, "avg_logprob": -0.2110905647277832, "compression_ratio": 1.7669491525423728, "no_speech_prob": 0.011330966837704182}, {"id": 1311, "seek": 664180, "start": 6654.2, "end": 6659.88, "text": " picture has, or that our data set has, then it won't change the statistics.", "tokens": [50984, 3036, 575, 11, 420, 300, 527, 1412, 992, 575, 11, 550, 309, 1582, 380, 1319, 264, 12523, 13, 51268], "temperature": 0.0, "avg_logprob": -0.2110905647277832, "compression_ratio": 1.7669491525423728, "no_speech_prob": 0.011330966837704182}, {"id": 1312, "seek": 664180, "start": 6659.88, "end": 6663.08, "text": " So that's why we've grabbed the mean and standard deviation.", "tokens": [51268, 407, 300, 311, 983, 321, 600, 18607, 264, 914, 293, 3832, 25163, 13, 51428], "temperature": 0.0, "avg_logprob": -0.2110905647277832, "compression_ratio": 1.7669491525423728, "no_speech_prob": 0.011330966837704182}, {"id": 1313, "seek": 664180, "start": 6663.08, "end": 6671.04, "text": " And so we could then try grabbing, let's say we want to delete 0.2, so 20% of the height", "tokens": [51428, 400, 370, 321, 727, 550, 853, 23771, 11, 718, 311, 584, 321, 528, 281, 12097, 1958, 13, 17, 11, 370, 945, 4, 295, 264, 6681, 51826], "temperature": 0.0, "avg_logprob": -0.2110905647277832, "compression_ratio": 1.7669491525423728, "no_speech_prob": 0.011330966837704182}, {"id": 1314, "seek": 667104, "start": 6671.04, "end": 6674.4, "text": " and width.", "tokens": [50364, 293, 11402, 13, 50532], "temperature": 0.0, "avg_logprob": -0.2464773036815502, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0002269330434501171}, {"id": 1315, "seek": 667104, "start": 6674.4, "end": 6676.5199999999995, "text": " Then let's find out how big that size is.", "tokens": [50532, 1396, 718, 311, 915, 484, 577, 955, 300, 2744, 307, 13, 50638], "temperature": 0.0, "avg_logprob": -0.2464773036815502, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0002269330434501171}, {"id": 1316, "seek": 667104, "start": 6676.5199999999995, "end": 6682.84, "text": " So 0.2 of the shape of the height and of the width, that's the size of the X and Y.", "tokens": [50638, 407, 1958, 13, 17, 295, 264, 3909, 295, 264, 6681, 293, 295, 264, 11402, 11, 300, 311, 264, 2744, 295, 264, 1783, 293, 398, 13, 50954], "temperature": 0.0, "avg_logprob": -0.2464773036815502, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0002269330434501171}, {"id": 1317, "seek": 667104, "start": 6682.84, "end": 6689.64, "text": " And then the starting point, we're just going to randomly grab some starting point, right?", "tokens": [50954, 400, 550, 264, 2891, 935, 11, 321, 434, 445, 516, 281, 16979, 4444, 512, 2891, 935, 11, 558, 30, 51294], "temperature": 0.0, "avg_logprob": -0.2464773036815502, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0002269330434501171}, {"id": 1318, "seek": 667104, "start": 6689.64, "end": 6695.48, "text": " So in this case, we've got the starting point for X is 14, starting point for Y is zero.", "tokens": [51294, 407, 294, 341, 1389, 11, 321, 600, 658, 264, 2891, 935, 337, 1783, 307, 3499, 11, 2891, 935, 337, 398, 307, 4018, 13, 51586], "temperature": 0.0, "avg_logprob": -0.2464773036815502, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0002269330434501171}, {"id": 1319, "seek": 667104, "start": 6695.48, "end": 6698.3, "text": " And then it's going to be a 5 by 5 spot.", "tokens": [51586, 400, 550, 309, 311, 516, 281, 312, 257, 1025, 538, 1025, 4008, 13, 51727], "temperature": 0.0, "avg_logprob": -0.2464773036815502, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0002269330434501171}, {"id": 1320, "seek": 669830, "start": 6698.3, "end": 6705.46, "text": " And then we're going to do a Gaussian or normal initialization of our mini-batch, everything", "tokens": [50364, 400, 550, 321, 434, 516, 281, 360, 257, 39148, 420, 2710, 5883, 2144, 295, 527, 8382, 12, 65, 852, 11, 1203, 50722], "temperature": 0.0, "avg_logprob": -0.20903004174944997, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.006097448989748955}, {"id": 1321, "seek": 669830, "start": 6705.46, "end": 6713.78, "text": " in the batch, every channel, for this X slice, this Y slice, and we're going to initialize", "tokens": [50722, 294, 264, 15245, 11, 633, 2269, 11, 337, 341, 1783, 13153, 11, 341, 398, 13153, 11, 293, 321, 434, 516, 281, 5883, 1125, 51138], "temperature": 0.0, "avg_logprob": -0.20903004174944997, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.006097448989748955}, {"id": 1322, "seek": 669830, "start": 6713.78, "end": 6719.820000000001, "text": " it with this mean and standard deviation, normal random noise.", "tokens": [51138, 309, 365, 341, 914, 293, 3832, 25163, 11, 2710, 4974, 5658, 13, 51440], "temperature": 0.0, "avg_logprob": -0.20903004174944997, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.006097448989748955}, {"id": 1323, "seek": 669830, "start": 6719.820000000001, "end": 6722.26, "text": " And so that's what this is.", "tokens": [51440, 400, 370, 300, 311, 437, 341, 307, 13, 51562], "temperature": 0.0, "avg_logprob": -0.20903004174944997, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.006097448989748955}, {"id": 1324, "seek": 669830, "start": 6722.26, "end": 6725.860000000001, "text": " So it's just that tiny little bit of code.", "tokens": [51562, 407, 309, 311, 445, 300, 5870, 707, 857, 295, 3089, 13, 51742], "temperature": 0.0, "avg_logprob": -0.20903004174944997, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.006097448989748955}, {"id": 1325, "seek": 672586, "start": 6725.86, "end": 6729.299999999999, "text": " So you'll see, I don't start by writing a function.", "tokens": [50364, 407, 291, 603, 536, 11, 286, 500, 380, 722, 538, 3579, 257, 2445, 13, 50536], "temperature": 0.0, "avg_logprob": -0.24430106183607803, "compression_ratio": 1.7993311036789297, "no_speech_prob": 0.001264408347196877}, {"id": 1326, "seek": 672586, "start": 6729.299999999999, "end": 6734.5, "text": " I start by writing single lines of code that I can run independently and make sure that", "tokens": [50536, 286, 722, 538, 3579, 2167, 3876, 295, 3089, 300, 286, 393, 1190, 21761, 293, 652, 988, 300, 50796], "temperature": 0.0, "avg_logprob": -0.24430106183607803, "compression_ratio": 1.7993311036789297, "no_speech_prob": 0.001264408347196877}, {"id": 1327, "seek": 672586, "start": 6734.5, "end": 6738.259999999999, "text": " they all work, and that I look at the pictures and make sure it's working.", "tokens": [50796, 436, 439, 589, 11, 293, 300, 286, 574, 412, 264, 5242, 293, 652, 988, 309, 311, 1364, 13, 50984], "temperature": 0.0, "avg_logprob": -0.24430106183607803, "compression_ratio": 1.7993311036789297, "no_speech_prob": 0.001264408347196877}, {"id": 1328, "seek": 672586, "start": 6738.259999999999, "end": 6742.0199999999995, "text": " Now one thing that's wrong here is that, do you see how the different, you know, this", "tokens": [50984, 823, 472, 551, 300, 311, 2085, 510, 307, 300, 11, 360, 291, 536, 577, 264, 819, 11, 291, 458, 11, 341, 51172], "temperature": 0.0, "avg_logprob": -0.24430106183607803, "compression_ratio": 1.7993311036789297, "no_speech_prob": 0.001264408347196877}, {"id": 1329, "seek": 672586, "start": 6742.0199999999995, "end": 6744.339999999999, "text": " looks black and this looks gray?", "tokens": [51172, 1542, 2211, 293, 341, 1542, 10855, 30, 51288], "temperature": 0.0, "avg_logprob": -0.24430106183607803, "compression_ratio": 1.7993311036789297, "no_speech_prob": 0.001264408347196877}, {"id": 1330, "seek": 672586, "start": 6744.339999999999, "end": 6748.38, "text": " Now at first this was confusing me as to what's going on, what's it changed, because the original", "tokens": [51288, 823, 412, 700, 341, 390, 13181, 385, 382, 281, 437, 311, 516, 322, 11, 437, 311, 309, 3105, 11, 570, 264, 3380, 51490], "temperature": 0.0, "avg_logprob": -0.24430106183607803, "compression_ratio": 1.7993311036789297, "no_speech_prob": 0.001264408347196877}, {"id": 1331, "seek": 672586, "start": 6748.38, "end": 6750.7, "text": " images didn't look like that.", "tokens": [51490, 5267, 994, 380, 574, 411, 300, 13, 51606], "temperature": 0.0, "avg_logprob": -0.24430106183607803, "compression_ratio": 1.7993311036789297, "no_speech_prob": 0.001264408347196877}, {"id": 1332, "seek": 672586, "start": 6750.7, "end": 6754.5, "text": " And I realized the problem is that the minimum and the maximum have changed.", "tokens": [51606, 400, 286, 5334, 264, 1154, 307, 300, 264, 7285, 293, 264, 6674, 362, 3105, 13, 51796], "temperature": 0.0, "avg_logprob": -0.24430106183607803, "compression_ratio": 1.7993311036789297, "no_speech_prob": 0.001264408347196877}, {"id": 1333, "seek": 675450, "start": 6754.5, "end": 6759.74, "text": " It used to be from negative 0.8 to 2, that was the previous min and max.", "tokens": [50364, 467, 1143, 281, 312, 490, 3671, 1958, 13, 23, 281, 568, 11, 300, 390, 264, 3894, 923, 293, 11469, 13, 50626], "temperature": 0.0, "avg_logprob": -0.18910927242702907, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.0006563670467585325}, {"id": 1334, "seek": 675450, "start": 6759.74, "end": 6763.98, "text": " Now it goes from negative 3 to 3.", "tokens": [50626, 823, 309, 1709, 490, 3671, 805, 281, 805, 13, 50838], "temperature": 0.0, "avg_logprob": -0.18910927242702907, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.0006563670467585325}, {"id": 1335, "seek": 675450, "start": 6763.98, "end": 6769.58, "text": " So the noise we've added has the same mean and standard deviation, but it doesn't have", "tokens": [50838, 407, 264, 5658, 321, 600, 3869, 575, 264, 912, 914, 293, 3832, 25163, 11, 457, 309, 1177, 380, 362, 51118], "temperature": 0.0, "avg_logprob": -0.18910927242702907, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.0006563670467585325}, {"id": 1336, "seek": 675450, "start": 6769.58, "end": 6774.9, "text": " the same range, because the pixels were not normally distributed originally.", "tokens": [51118, 264, 912, 3613, 11, 570, 264, 18668, 645, 406, 5646, 12631, 7993, 13, 51384], "temperature": 0.0, "avg_logprob": -0.18910927242702907, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.0006563670467585325}, {"id": 1337, "seek": 675450, "start": 6774.9, "end": 6778.94, "text": " So normally distributed noise actually is wrong.", "tokens": [51384, 407, 5646, 12631, 5658, 767, 307, 2085, 13, 51586], "temperature": 0.0, "avg_logprob": -0.18910927242702907, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.0006563670467585325}, {"id": 1338, "seek": 677894, "start": 6778.94, "end": 6785.219999999999, "text": " So to fix that, I created a new version, and I'm putting in a function now, does all", "tokens": [50364, 407, 281, 3191, 300, 11, 286, 2942, 257, 777, 3037, 11, 293, 286, 478, 3372, 294, 257, 2445, 586, 11, 775, 439, 50678], "temperature": 0.0, "avg_logprob": -0.2391516367594401, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0115062789991498}, {"id": 1339, "seek": 677894, "start": 6785.219999999999, "end": 6794.74, "text": " the same stuff as before, as I just did before, but it clamps the random pixels to be between", "tokens": [50678, 264, 912, 1507, 382, 949, 11, 382, 286, 445, 630, 949, 11, 457, 309, 44423, 264, 4974, 18668, 281, 312, 1296, 51154], "temperature": 0.0, "avg_logprob": -0.2391516367594401, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0115062789991498}, {"id": 1340, "seek": 677894, "start": 6794.74, "end": 6796.9, "text": " min and max.", "tokens": [51154, 923, 293, 11469, 13, 51262], "temperature": 0.0, "avg_logprob": -0.2391516367594401, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0115062789991498}, {"id": 1341, "seek": 677894, "start": 6796.9, "end": 6801.099999999999, "text": " And so it's going to be exactly the same thing, but it's going to make sure that it doesn't", "tokens": [51262, 400, 370, 309, 311, 516, 281, 312, 2293, 264, 912, 551, 11, 457, 309, 311, 516, 281, 652, 988, 300, 309, 1177, 380, 51472], "temperature": 0.0, "avg_logprob": -0.2391516367594401, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0115062789991498}, {"id": 1342, "seek": 677894, "start": 6801.099999999999, "end": 6803.419999999999, "text": " change the range.", "tokens": [51472, 1319, 264, 3613, 13, 51588], "temperature": 0.0, "avg_logprob": -0.2391516367594401, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0115062789991498}, {"id": 1343, "seek": 677894, "start": 6803.419999999999, "end": 6805.7, "text": " That's really important, I think.", "tokens": [51588, 663, 311, 534, 1021, 11, 286, 519, 13, 51702], "temperature": 0.0, "avg_logprob": -0.2391516367594401, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0115062789991498}, {"id": 1344, "seek": 680570, "start": 6805.7, "end": 6811.38, "text": " Because changing the range really impacts your, you know, your activations quite a lot.", "tokens": [50364, 1436, 4473, 264, 3613, 534, 11606, 428, 11, 291, 458, 11, 428, 2430, 763, 1596, 257, 688, 13, 50648], "temperature": 0.0, "avg_logprob": -0.2379723647545124, "compression_ratio": 1.75, "no_speech_prob": 0.020331570878624916}, {"id": 1345, "seek": 680570, "start": 6811.38, "end": 6813.22, "text": " So here's what that looks like.", "tokens": [50648, 407, 510, 311, 437, 300, 1542, 411, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2379723647545124, "compression_ratio": 1.75, "no_speech_prob": 0.020331570878624916}, {"id": 1346, "seek": 680570, "start": 6813.22, "end": 6817.5199999999995, "text": " And so as you can see now, all of the backgrounds have that nice black, and it's still giving", "tokens": [50740, 400, 370, 382, 291, 393, 536, 586, 11, 439, 295, 264, 17336, 362, 300, 1481, 2211, 11, 293, 309, 311, 920, 2902, 50955], "temperature": 0.0, "avg_logprob": -0.2379723647545124, "compression_ratio": 1.75, "no_speech_prob": 0.020331570878624916}, {"id": 1347, "seek": 680570, "start": 6817.5199999999995, "end": 6821.42, "text": " me random pixels.", "tokens": [50955, 385, 4974, 18668, 13, 51150], "temperature": 0.0, "avg_logprob": -0.2379723647545124, "compression_ratio": 1.75, "no_speech_prob": 0.020331570878624916}, {"id": 1348, "seek": 680570, "start": 6821.42, "end": 6825.36, "text": " And I can check, and because I've done the clamping, you know, and stuff, the mean and", "tokens": [51150, 400, 286, 393, 1520, 11, 293, 570, 286, 600, 1096, 264, 17690, 278, 11, 291, 458, 11, 293, 1507, 11, 264, 914, 293, 51347], "temperature": 0.0, "avg_logprob": -0.2379723647545124, "compression_ratio": 1.75, "no_speech_prob": 0.020331570878624916}, {"id": 1349, "seek": 680570, "start": 6825.36, "end": 6827.86, "text": " standard deviation aren't quite 0, 1, but they're very, very close.", "tokens": [51347, 3832, 25163, 3212, 380, 1596, 1958, 11, 502, 11, 457, 436, 434, 588, 11, 588, 1998, 13, 51472], "temperature": 0.0, "avg_logprob": -0.2379723647545124, "compression_ratio": 1.75, "no_speech_prob": 0.020331570878624916}, {"id": 1350, "seek": 680570, "start": 6827.86, "end": 6829.98, "text": " So I'm going to call that good enough.", "tokens": [51472, 407, 286, 478, 516, 281, 818, 300, 665, 1547, 13, 51578], "temperature": 0.0, "avg_logprob": -0.2379723647545124, "compression_ratio": 1.75, "no_speech_prob": 0.020331570878624916}, {"id": 1351, "seek": 680570, "start": 6829.98, "end": 6833.98, "text": " And of course the min and max haven't changed, because I clamped them to ensure they didn't", "tokens": [51578, 400, 295, 1164, 264, 923, 293, 11469, 2378, 380, 3105, 11, 570, 286, 17690, 292, 552, 281, 5586, 436, 994, 380, 51778], "temperature": 0.0, "avg_logprob": -0.2379723647545124, "compression_ratio": 1.75, "no_speech_prob": 0.020331570878624916}, {"id": 1352, "seek": 680570, "start": 6833.98, "end": 6834.98, "text": " change.", "tokens": [51778, 1319, 13, 51828], "temperature": 0.0, "avg_logprob": -0.2379723647545124, "compression_ratio": 1.75, "no_speech_prob": 0.020331570878624916}, {"id": 1353, "seek": 683498, "start": 6835.259999999999, "end": 6837.379999999999, "text": " So that's my random erasing.", "tokens": [50378, 407, 300, 311, 452, 4974, 1189, 3349, 13, 50484], "temperature": 0.0, "avg_logprob": -0.29980675717617605, "compression_ratio": 1.5670103092783505, "no_speech_prob": 0.025178559124469757}, {"id": 1354, "seek": 683498, "start": 6837.379999999999, "end": 6840.219999999999, "text": " So that randomly erases one block.", "tokens": [50484, 407, 300, 16979, 1189, 1957, 472, 3461, 13, 50626], "temperature": 0.0, "avg_logprob": -0.29980675717617605, "compression_ratio": 1.5670103092783505, "no_speech_prob": 0.025178559124469757}, {"id": 1355, "seek": 683498, "start": 6840.219999999999, "end": 6847.82, "text": " And so I could create a random erase, which will randomly choose up to, in this case,", "tokens": [50626, 400, 370, 286, 727, 1884, 257, 4974, 23525, 11, 597, 486, 16979, 2826, 493, 281, 11, 294, 341, 1389, 11, 51006], "temperature": 0.0, "avg_logprob": -0.29980675717617605, "compression_ratio": 1.5670103092783505, "no_speech_prob": 0.025178559124469757}, {"id": 1356, "seek": 683498, "start": 6847.82, "end": 6850.16, "text": " four blocks.", "tokens": [51006, 1451, 8474, 13, 51123], "temperature": 0.0, "avg_logprob": -0.29980675717617605, "compression_ratio": 1.5670103092783505, "no_speech_prob": 0.025178559124469757}, {"id": 1357, "seek": 683498, "start": 6850.16, "end": 6855.74, "text": " So with that function, oh that's annoying, it happened to be 0 this time.", "tokens": [51123, 407, 365, 300, 2445, 11, 1954, 300, 311, 11304, 11, 309, 2011, 281, 312, 1958, 341, 565, 13, 51402], "temperature": 0.0, "avg_logprob": -0.29980675717617605, "compression_ratio": 1.5670103092783505, "no_speech_prob": 0.025178559124469757}, {"id": 1358, "seek": 683498, "start": 6855.74, "end": 6860.0199999999995, "text": " Okay, I'll just run it again.", "tokens": [51402, 1033, 11, 286, 603, 445, 1190, 309, 797, 13, 51616], "temperature": 0.0, "avg_logprob": -0.29980675717617605, "compression_ratio": 1.5670103092783505, "no_speech_prob": 0.025178559124469757}, {"id": 1359, "seek": 683498, "start": 6860.0199999999995, "end": 6861.82, "text": " This time it's got 3, so that's good.", "tokens": [51616, 639, 565, 309, 311, 658, 805, 11, 370, 300, 311, 665, 13, 51706], "temperature": 0.0, "avg_logprob": -0.29980675717617605, "compression_ratio": 1.5670103092783505, "no_speech_prob": 0.025178559124469757}, {"id": 1360, "seek": 686182, "start": 6861.82, "end": 6867.82, "text": " So you can see it's got, oh maybe, no it's 4, 1, 2, 3, 4 blocks.", "tokens": [50364, 407, 291, 393, 536, 309, 311, 658, 11, 1954, 1310, 11, 572, 309, 311, 1017, 11, 502, 11, 568, 11, 805, 11, 1017, 8474, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2365570068359375, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.003222379367798567}, {"id": 1361, "seek": 686182, "start": 6867.82, "end": 6873.38, "text": " Okay so that's what this data augmentation looks like.", "tokens": [50664, 1033, 370, 300, 311, 437, 341, 1412, 14501, 19631, 1542, 411, 13, 50942], "temperature": 0.0, "avg_logprob": -0.2365570068359375, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.003222379367798567}, {"id": 1362, "seek": 686182, "start": 6873.38, "end": 6876.58, "text": " So we can create a class to do this data augmentation.", "tokens": [50942, 407, 321, 393, 1884, 257, 1508, 281, 360, 341, 1412, 14501, 19631, 13, 51102], "temperature": 0.0, "avg_logprob": -0.2365570068359375, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.003222379367798567}, {"id": 1363, "seek": 686182, "start": 6876.58, "end": 6882.0199999999995, "text": " So you'll pass in what percentage to do in each block, what the maximum number of blocks", "tokens": [51102, 407, 291, 603, 1320, 294, 437, 9668, 281, 360, 294, 1184, 3461, 11, 437, 264, 6674, 1230, 295, 8474, 51374], "temperature": 0.0, "avg_logprob": -0.2365570068359375, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.003222379367798567}, {"id": 1364, "seek": 686182, "start": 6882.0199999999995, "end": 6887.42, "text": " to have is, store that away, and then in the forward we're just going to call our random", "tokens": [51374, 281, 362, 307, 11, 3531, 300, 1314, 11, 293, 550, 294, 264, 2128, 321, 434, 445, 516, 281, 818, 527, 4974, 51644], "temperature": 0.0, "avg_logprob": -0.2365570068359375, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.003222379367798567}, {"id": 1365, "seek": 688742, "start": 6887.42, "end": 6892.74, "text": " erase function, passing in the input and passing in the parameters.", "tokens": [50364, 23525, 2445, 11, 8437, 294, 264, 4846, 293, 8437, 294, 264, 9834, 13, 50630], "temperature": 0.0, "avg_logprob": -0.28650653173053076, "compression_ratio": 1.4797297297297298, "no_speech_prob": 0.08268587291240692}, {"id": 1366, "seek": 688742, "start": 6892.74, "end": 6907.66, "text": " Great, so now we can use random crop, random flip, and random erase.", "tokens": [50630, 3769, 11, 370, 586, 321, 393, 764, 4974, 9086, 11, 4974, 7929, 11, 293, 4974, 23525, 13, 51376], "temperature": 0.0, "avg_logprob": -0.28650653173053076, "compression_ratio": 1.4797297297297298, "no_speech_prob": 0.08268587291240692}, {"id": 1367, "seek": 688742, "start": 6907.66, "end": 6909.3, "text": " Make sure it looks okay.", "tokens": [51376, 4387, 988, 309, 1542, 1392, 13, 51458], "temperature": 0.0, "avg_logprob": -0.28650653173053076, "compression_ratio": 1.4797297297297298, "no_speech_prob": 0.08268587291240692}, {"id": 1368, "seek": 688742, "start": 6909.3, "end": 6914.22, "text": " And so now we're going to go all the way up to 50 epochs.", "tokens": [51458, 400, 370, 586, 321, 434, 516, 281, 352, 439, 264, 636, 493, 281, 2625, 30992, 28346, 13, 51704], "temperature": 0.0, "avg_logprob": -0.28650653173053076, "compression_ratio": 1.4797297297297298, "no_speech_prob": 0.08268587291240692}, {"id": 1369, "seek": 691422, "start": 6914.22, "end": 6923.62, "text": " And so if I run this for 50 epochs, I get 94.6.", "tokens": [50364, 400, 370, 498, 286, 1190, 341, 337, 2625, 30992, 28346, 11, 286, 483, 30849, 13, 21, 13, 50834], "temperature": 0.0, "avg_logprob": -0.31570875921914743, "compression_ratio": 1.4031413612565444, "no_speech_prob": 0.03621792048215866}, {"id": 1370, "seek": 691422, "start": 6923.62, "end": 6926.52, "text": " Isn't that crazy?", "tokens": [50834, 6998, 380, 300, 3219, 30, 50979], "temperature": 0.0, "avg_logprob": -0.31570875921914743, "compression_ratio": 1.4031413612565444, "no_speech_prob": 0.03621792048215866}, {"id": 1371, "seek": 691422, "start": 6926.52, "end": 6932.18, "text": " So we're really right up there now, up, we're even above this one.", "tokens": [50979, 407, 321, 434, 534, 558, 493, 456, 586, 11, 493, 11, 321, 434, 754, 3673, 341, 472, 13, 51262], "temperature": 0.0, "avg_logprob": -0.31570875921914743, "compression_ratio": 1.4031413612565444, "no_speech_prob": 0.03621792048215866}, {"id": 1372, "seek": 691422, "start": 6932.18, "end": 6934.34, "text": " So we're somewhere up here.", "tokens": [51262, 407, 321, 434, 4079, 493, 510, 13, 51370], "temperature": 0.0, "avg_logprob": -0.31570875921914743, "compression_ratio": 1.4031413612565444, "no_speech_prob": 0.03621792048215866}, {"id": 1373, "seek": 691422, "start": 6934.34, "end": 6938.34, "text": " And this is like stuff people write papers about from 2019, 2020.", "tokens": [51370, 400, 341, 307, 411, 1507, 561, 2464, 10577, 466, 490, 6071, 11, 4808, 13, 51570], "temperature": 0.0, "avg_logprob": -0.31570875921914743, "compression_ratio": 1.4031413612565444, "no_speech_prob": 0.03621792048215866}, {"id": 1374, "seek": 691422, "start": 6938.34, "end": 6943.02, "text": " Oh look, here's the random erasing paper.", "tokens": [51570, 876, 574, 11, 510, 311, 264, 4974, 1189, 3349, 3035, 13, 51804], "temperature": 0.0, "avg_logprob": -0.31570875921914743, "compression_ratio": 1.4031413612565444, "no_speech_prob": 0.03621792048215866}, {"id": 1375, "seek": 694302, "start": 6943.02, "end": 6944.02, "text": " That's cool.", "tokens": [50364, 663, 311, 1627, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3074234134548313, "compression_ratio": 1.4429824561403508, "no_speech_prob": 0.016402943059802055}, {"id": 1376, "seek": 694302, "start": 6944.02, "end": 6947.9800000000005, "text": " So they were way ahead of their time in 2017.", "tokens": [50414, 407, 436, 645, 636, 2286, 295, 641, 565, 294, 6591, 13, 50612], "temperature": 0.0, "avg_logprob": -0.3074234134548313, "compression_ratio": 1.4429824561403508, "no_speech_prob": 0.016402943059802055}, {"id": 1377, "seek": 694302, "start": 6947.9800000000005, "end": 6955.860000000001, "text": " But yeah, that would have trained for a lot longer.", "tokens": [50612, 583, 1338, 11, 300, 576, 362, 8895, 337, 257, 688, 2854, 13, 51006], "temperature": 0.0, "avg_logprob": -0.3074234134548313, "compression_ratio": 1.4429824561403508, "no_speech_prob": 0.016402943059802055}, {"id": 1378, "seek": 694302, "start": 6955.860000000001, "end": 6961.18, "text": " Now I was having a think, and I realized something.", "tokens": [51006, 823, 286, 390, 1419, 257, 519, 11, 293, 286, 5334, 746, 13, 51272], "temperature": 0.0, "avg_logprob": -0.3074234134548313, "compression_ratio": 1.4429824561403508, "no_speech_prob": 0.016402943059802055}, {"id": 1379, "seek": 694302, "start": 6961.18, "end": 6967.700000000001, "text": " Which is like, why, like, how do we actually get the correct distribution, right?", "tokens": [51272, 3013, 307, 411, 11, 983, 11, 411, 11, 577, 360, 321, 767, 483, 264, 3006, 7316, 11, 558, 30, 51598], "temperature": 0.0, "avg_logprob": -0.3074234134548313, "compression_ratio": 1.4429824561403508, "no_speech_prob": 0.016402943059802055}, {"id": 1380, "seek": 694302, "start": 6967.700000000001, "end": 6971.3, "text": " Like in some ways it shouldn't matter, but I was kind of like bothered by this thing", "tokens": [51598, 1743, 294, 512, 2098, 309, 4659, 380, 1871, 11, 457, 286, 390, 733, 295, 411, 22996, 538, 341, 551, 51778], "temperature": 0.0, "avg_logprob": -0.3074234134548313, "compression_ratio": 1.4429824561403508, "no_speech_prob": 0.016402943059802055}, {"id": 1381, "seek": 697130, "start": 6971.38, "end": 6975.22, "text": " of like, well we don't actually end up with 0, 1, and this kind of like clamping.", "tokens": [50368, 295, 411, 11, 731, 321, 500, 380, 767, 917, 493, 365, 1958, 11, 502, 11, 293, 341, 733, 295, 411, 17690, 278, 13, 50560], "temperature": 0.0, "avg_logprob": -0.2307845675756061, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.007345687597990036}, {"id": 1382, "seek": 697130, "start": 6975.22, "end": 6976.46, "text": " It all feels a bit weird.", "tokens": [50560, 467, 439, 3417, 257, 857, 3657, 13, 50622], "temperature": 0.0, "avg_logprob": -0.2307845675756061, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.007345687597990036}, {"id": 1383, "seek": 697130, "start": 6976.46, "end": 6983.1, "text": " Like how do we actually replace these pixels with something that is guaranteed to be the", "tokens": [50622, 1743, 577, 360, 321, 767, 7406, 613, 18668, 365, 746, 300, 307, 18031, 281, 312, 264, 50954], "temperature": 0.0, "avg_logprob": -0.2307845675756061, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.007345687597990036}, {"id": 1384, "seek": 697130, "start": 6983.1, "end": 6985.34, "text": " correct distribution?", "tokens": [50954, 3006, 7316, 30, 51066], "temperature": 0.0, "avg_logprob": -0.2307845675756061, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.007345687597990036}, {"id": 1385, "seek": 697130, "start": 6985.34, "end": 6988.22, "text": " And I realized there's actually a very simple answer to this.", "tokens": [51066, 400, 286, 5334, 456, 311, 767, 257, 588, 2199, 1867, 281, 341, 13, 51210], "temperature": 0.0, "avg_logprob": -0.2307845675756061, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.007345687597990036}, {"id": 1386, "seek": 697130, "start": 6988.22, "end": 6993.66, "text": " Which is, we could copy another part of the picture over to here.", "tokens": [51210, 3013, 307, 11, 321, 727, 5055, 1071, 644, 295, 264, 3036, 670, 281, 510, 13, 51482], "temperature": 0.0, "avg_logprob": -0.2307845675756061, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.007345687597990036}, {"id": 1387, "seek": 697130, "start": 6993.66, "end": 6998.9800000000005, "text": " If we copy part of the picture, we're guaranteed to have the correct distribution of pixels.", "tokens": [51482, 759, 321, 5055, 644, 295, 264, 3036, 11, 321, 434, 18031, 281, 362, 264, 3006, 7316, 295, 18668, 13, 51748], "temperature": 0.0, "avg_logprob": -0.2307845675756061, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.007345687597990036}, {"id": 1388, "seek": 697130, "start": 6998.9800000000005, "end": 7001.1, "text": " And so it wouldn't exactly be random erasing anymore.", "tokens": [51748, 400, 370, 309, 2759, 380, 2293, 312, 4974, 1189, 3349, 3602, 13, 51854], "temperature": 0.0, "avg_logprob": -0.2307845675756061, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.007345687597990036}, {"id": 1389, "seek": 700110, "start": 7001.900000000001, "end": 7003.700000000001, "text": " That would be random copying.", "tokens": [50404, 663, 576, 312, 4974, 27976, 13, 50494], "temperature": 0.0, "avg_logprob": -0.28468794292873806, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0017821920337155461}, {"id": 1390, "seek": 700110, "start": 7003.700000000001, "end": 7007.46, "text": " Now I'm sure somebody else has invented this.", "tokens": [50494, 823, 286, 478, 988, 2618, 1646, 575, 14479, 341, 13, 50682], "temperature": 0.0, "avg_logprob": -0.28468794292873806, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0017821920337155461}, {"id": 1391, "seek": 700110, "start": 7007.46, "end": 7012.38, "text": " I mean, you know, I'm not saying this, nobody's ever thought of this before.", "tokens": [50682, 286, 914, 11, 291, 458, 11, 286, 478, 406, 1566, 341, 11, 5079, 311, 1562, 1194, 295, 341, 949, 13, 50928], "temperature": 0.0, "avg_logprob": -0.28468794292873806, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0017821920337155461}, {"id": 1392, "seek": 700110, "start": 7012.38, "end": 7015.820000000001, "text": " So if anybody knows a paper that's done this, please tell me about it.", "tokens": [50928, 407, 498, 4472, 3255, 257, 3035, 300, 311, 1096, 341, 11, 1767, 980, 385, 466, 309, 13, 51100], "temperature": 0.0, "avg_logprob": -0.28468794292873806, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0017821920337155461}, {"id": 1393, "seek": 700110, "start": 7015.820000000001, "end": 7024.14, "text": " But I, you know, I think it's a very sensible approach.", "tokens": [51100, 583, 286, 11, 291, 458, 11, 286, 519, 309, 311, 257, 588, 25380, 3109, 13, 51516], "temperature": 0.0, "avg_logprob": -0.28468794292873806, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0017821920337155461}, {"id": 1394, "seek": 700110, "start": 7024.14, "end": 7027.820000000001, "text": " And it's very, very easy to implement.", "tokens": [51516, 400, 309, 311, 588, 11, 588, 1858, 281, 4445, 13, 51700], "temperature": 0.0, "avg_logprob": -0.28468794292873806, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0017821920337155461}, {"id": 1395, "seek": 700110, "start": 7027.820000000001, "end": 7029.740000000001, "text": " So again, we're going to implement it all manually, right?", "tokens": [51700, 407, 797, 11, 321, 434, 516, 281, 4445, 309, 439, 16945, 11, 558, 30, 51796], "temperature": 0.0, "avg_logprob": -0.28468794292873806, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0017821920337155461}, {"id": 1396, "seek": 702974, "start": 7029.78, "end": 7035.94, "text": " So let's get our xminibatch, and let's get our, again, our size.", "tokens": [50366, 407, 718, 311, 483, 527, 2031, 2367, 897, 852, 11, 293, 718, 311, 483, 527, 11, 797, 11, 527, 2744, 13, 50674], "temperature": 0.0, "avg_logprob": -0.24808543812144887, "compression_ratio": 1.77, "no_speech_prob": 0.010328026488423347}, {"id": 1397, "seek": 702974, "start": 7035.94, "end": 7041.3, "text": " And again, let's get the xy that we're going to be erasing.", "tokens": [50674, 400, 797, 11, 718, 311, 483, 264, 2031, 88, 300, 321, 434, 516, 281, 312, 1189, 3349, 13, 50942], "temperature": 0.0, "avg_logprob": -0.24808543812144887, "compression_ratio": 1.77, "no_speech_prob": 0.010328026488423347}, {"id": 1398, "seek": 702974, "start": 7041.3, "end": 7043.0199999999995, "text": " But this time we're not erasing, we're copying.", "tokens": [50942, 583, 341, 565, 321, 434, 406, 1189, 3349, 11, 321, 434, 27976, 13, 51028], "temperature": 0.0, "avg_logprob": -0.24808543812144887, "compression_ratio": 1.77, "no_speech_prob": 0.010328026488423347}, {"id": 1399, "seek": 702974, "start": 7043.0199999999995, "end": 7046.179999999999, "text": " So we'll then randomly get a different xy to copy from.", "tokens": [51028, 407, 321, 603, 550, 16979, 483, 257, 819, 2031, 88, 281, 5055, 490, 13, 51186], "temperature": 0.0, "avg_logprob": -0.24808543812144887, "compression_ratio": 1.77, "no_speech_prob": 0.010328026488423347}, {"id": 1400, "seek": 702974, "start": 7046.179999999999, "end": 7052.7, "text": " And so now it's just, instead of in a random noise, we just say replace this slice of the", "tokens": [51186, 400, 370, 586, 309, 311, 445, 11, 2602, 295, 294, 257, 4974, 5658, 11, 321, 445, 584, 7406, 341, 13153, 295, 264, 51512], "temperature": 0.0, "avg_logprob": -0.24808543812144887, "compression_ratio": 1.77, "no_speech_prob": 0.010328026488423347}, {"id": 1401, "seek": 702974, "start": 7052.7, "end": 7057.76, "text": " batch with this slice of the batch.", "tokens": [51512, 15245, 365, 341, 13153, 295, 264, 15245, 13, 51765], "temperature": 0.0, "avg_logprob": -0.24808543812144887, "compression_ratio": 1.77, "no_speech_prob": 0.010328026488423347}, {"id": 1402, "seek": 705776, "start": 7057.780000000001, "end": 7065.2, "text": " And we end up with, you know, you can see here it's kind of copied little bits across.", "tokens": [50365, 400, 321, 917, 493, 365, 11, 291, 458, 11, 291, 393, 536, 510, 309, 311, 733, 295, 25365, 707, 9239, 2108, 13, 50736], "temperature": 0.0, "avg_logprob": -0.26865221759465735, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.011687138117849827}, {"id": 1403, "seek": 705776, "start": 7065.2, "end": 7067.04, "text": " Some of them you can't really see at all, and some of you can.", "tokens": [50736, 2188, 295, 552, 291, 393, 380, 534, 536, 412, 439, 11, 293, 512, 295, 291, 393, 13, 50828], "temperature": 0.0, "avg_logprob": -0.26865221759465735, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.011687138117849827}, {"id": 1404, "seek": 705776, "start": 7067.04, "end": 7068.84, "text": " Because I think some of them are black, and it's replaced black.", "tokens": [50828, 1436, 286, 519, 512, 295, 552, 366, 2211, 11, 293, 309, 311, 10772, 2211, 13, 50918], "temperature": 0.0, "avg_logprob": -0.26865221759465735, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.011687138117849827}, {"id": 1405, "seek": 705776, "start": 7068.84, "end": 7072.72, "text": " But I guess it's knocked off the end of this shoe, added a little bit extra here, a little", "tokens": [50918, 583, 286, 2041, 309, 311, 16914, 766, 264, 917, 295, 341, 12796, 11, 3869, 257, 707, 857, 2857, 510, 11, 257, 707, 51112], "temperature": 0.0, "avg_logprob": -0.26865221759465735, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.011687138117849827}, {"id": 1406, "seek": 705776, "start": 7072.72, "end": 7075.04, "text": " bit extra here.", "tokens": [51112, 857, 2857, 510, 13, 51228], "temperature": 0.0, "avg_logprob": -0.26865221759465735, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.011687138117849827}, {"id": 1407, "seek": 705776, "start": 7075.04, "end": 7078.68, "text": " So we can now, again, we'll turn it into a function.", "tokens": [51228, 407, 321, 393, 586, 11, 797, 11, 321, 603, 1261, 309, 666, 257, 2445, 13, 51410], "temperature": 0.0, "avg_logprob": -0.26865221759465735, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.011687138117849827}, {"id": 1408, "seek": 705776, "start": 7078.68, "end": 7083.780000000001, "text": " Once I've tested it in the REPL, make sure the function works.", "tokens": [51410, 3443, 286, 600, 8246, 309, 294, 264, 31511, 43, 11, 652, 988, 264, 2445, 1985, 13, 51665], "temperature": 0.0, "avg_logprob": -0.26865221759465735, "compression_ratio": 1.7341269841269842, "no_speech_prob": 0.011687138117849827}, {"id": 1409, "seek": 708378, "start": 7083.78, "end": 7088.42, "text": " And obviously this, in this case, it's copying it largely from something that's largely black", "tokens": [50364, 400, 2745, 341, 11, 294, 341, 1389, 11, 309, 311, 27976, 309, 11611, 490, 746, 300, 311, 11611, 2211, 50596], "temperature": 0.0, "avg_logprob": -0.24897300365359284, "compression_ratio": 1.6, "no_speech_prob": 0.045346979051828384}, {"id": 1410, "seek": 708378, "start": 7088.42, "end": 7090.3, "text": " for a lot of them.", "tokens": [50596, 337, 257, 688, 295, 552, 13, 50690], "temperature": 0.0, "avg_logprob": -0.24897300365359284, "compression_ratio": 1.6, "no_speech_prob": 0.045346979051828384}, {"id": 1411, "seek": 708378, "start": 7090.3, "end": 7096.96, "text": " And then again, we can do the thing where we do it multiple times.", "tokens": [50690, 400, 550, 797, 11, 321, 393, 360, 264, 551, 689, 321, 360, 309, 3866, 1413, 13, 51023], "temperature": 0.0, "avg_logprob": -0.24897300365359284, "compression_ratio": 1.6, "no_speech_prob": 0.045346979051828384}, {"id": 1412, "seek": 708378, "start": 7096.96, "end": 7097.96, "text": " And here we go.", "tokens": [51023, 400, 510, 321, 352, 13, 51073], "temperature": 0.0, "avg_logprob": -0.24897300365359284, "compression_ratio": 1.6, "no_speech_prob": 0.045346979051828384}, {"id": 1413, "seek": 708378, "start": 7097.96, "end": 7103.0599999999995, "text": " Now it's got a couple of random copies.", "tokens": [51073, 823, 309, 311, 658, 257, 1916, 295, 4974, 14341, 13, 51328], "temperature": 0.0, "avg_logprob": -0.24897300365359284, "compression_ratio": 1.6, "no_speech_prob": 0.045346979051828384}, {"id": 1414, "seek": 708378, "start": 7103.0599999999995, "end": 7107.94, "text": " And so again, turn that into a class.", "tokens": [51328, 400, 370, 797, 11, 1261, 300, 666, 257, 1508, 13, 51572], "temperature": 0.0, "avg_logprob": -0.24897300365359284, "compression_ratio": 1.6, "no_speech_prob": 0.045346979051828384}, {"id": 1415, "seek": 708378, "start": 7107.94, "end": 7110.0, "text": " Create our transforms.", "tokens": [51572, 20248, 527, 35592, 13, 51675], "temperature": 0.0, "avg_logprob": -0.24897300365359284, "compression_ratio": 1.6, "no_speech_prob": 0.045346979051828384}, {"id": 1416, "seek": 711000, "start": 7110.0, "end": 7120.72, "text": " And again we, okay, so again we can have a look at a batch to make sure it looks sensible.", "tokens": [50364, 400, 797, 321, 11, 1392, 11, 370, 797, 321, 393, 362, 257, 574, 412, 257, 15245, 281, 652, 988, 309, 1542, 25380, 13, 50900], "temperature": 0.0, "avg_logprob": -0.2326308053637308, "compression_ratio": 1.4104477611940298, "no_speech_prob": 0.03676609322428703}, {"id": 1417, "seek": 711000, "start": 7120.72, "end": 7126.0, "text": " And do it for, just did it for 25 epochs here.", "tokens": [50900, 400, 360, 309, 337, 11, 445, 630, 309, 337, 3552, 30992, 28346, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2326308053637308, "compression_ratio": 1.4104477611940298, "no_speech_prob": 0.03676609322428703}, {"id": 1418, "seek": 711000, "start": 7126.0, "end": 7132.38, "text": " And gets to 94%.", "tokens": [51164, 400, 2170, 281, 30849, 6856, 51483], "temperature": 0.0, "avg_logprob": -0.2326308053637308, "compression_ratio": 1.4104477611940298, "no_speech_prob": 0.03676609322428703}, {"id": 1419, "seek": 711000, "start": 7132.38, "end": 7134.52, "text": " Now why did I do it for 25 epochs?", "tokens": [51483, 823, 983, 630, 286, 360, 309, 337, 3552, 30992, 28346, 30, 51590], "temperature": 0.0, "avg_logprob": -0.2326308053637308, "compression_ratio": 1.4104477611940298, "no_speech_prob": 0.03676609322428703}, {"id": 1420, "seek": 713452, "start": 7134.52, "end": 7140.56, "text": " Because I was trying to think about how do I beat my 50 epoch record, which was 94.6.", "tokens": [50364, 1436, 286, 390, 1382, 281, 519, 466, 577, 360, 286, 4224, 452, 2625, 30992, 339, 2136, 11, 597, 390, 30849, 13, 21, 13, 50666], "temperature": 0.0, "avg_logprob": -0.29179949373812286, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.02843373641371727}, {"id": 1421, "seek": 713452, "start": 7140.56, "end": 7147.4400000000005, "text": " And I thought, well what I could do is I could train for 25 epochs.", "tokens": [50666, 400, 286, 1194, 11, 731, 437, 286, 727, 360, 307, 286, 727, 3847, 337, 3552, 30992, 28346, 13, 51010], "temperature": 0.0, "avg_logprob": -0.29179949373812286, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.02843373641371727}, {"id": 1422, "seek": 713452, "start": 7147.4400000000005, "end": 7152.68, "text": " And then I'll train a whole new model for a different 25 epochs.", "tokens": [51010, 400, 550, 286, 603, 3847, 257, 1379, 777, 2316, 337, 257, 819, 3552, 30992, 28346, 13, 51272], "temperature": 0.0, "avg_logprob": -0.29179949373812286, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.02843373641371727}, {"id": 1423, "seek": 713452, "start": 7152.68, "end": 7155.68, "text": " And I'm going to put it a different learner, learn2.", "tokens": [51272, 400, 286, 478, 516, 281, 829, 309, 257, 819, 33347, 11, 1466, 17, 13, 51422], "temperature": 0.0, "avg_logprob": -0.29179949373812286, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.02843373641371727}, {"id": 1424, "seek": 713452, "start": 7155.68, "end": 7158.64, "text": " Right, that this one is 94.1.", "tokens": [51422, 1779, 11, 300, 341, 472, 307, 30849, 13, 16, 13, 51570], "temperature": 0.0, "avg_logprob": -0.29179949373812286, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.02843373641371727}, {"id": 1425, "seek": 713452, "start": 7158.64, "end": 7163.200000000001, "text": " So one of the models was 94.1, one of them was 94.", "tokens": [51570, 407, 472, 295, 264, 5245, 390, 30849, 13, 16, 11, 472, 295, 552, 390, 30849, 13, 51798], "temperature": 0.0, "avg_logprob": -0.29179949373812286, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.02843373641371727}, {"id": 1426, "seek": 716320, "start": 7163.2, "end": 7165.639999999999, "text": " Maybe you can guess what we're going to do next.", "tokens": [50364, 2704, 291, 393, 2041, 437, 321, 434, 516, 281, 360, 958, 13, 50486], "temperature": 0.0, "avg_logprob": -0.2262485905697471, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.03962264955043793}, {"id": 1427, "seek": 716320, "start": 7165.639999999999, "end": 7173.84, "text": " It's a bit like test time augmentation, but rather than that we're going to grab the predictions", "tokens": [50486, 467, 311, 257, 857, 411, 1500, 565, 14501, 19631, 11, 457, 2831, 813, 300, 321, 434, 516, 281, 4444, 264, 21264, 50896], "temperature": 0.0, "avg_logprob": -0.2262485905697471, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.03962264955043793}, {"id": 1428, "seek": 716320, "start": 7173.84, "end": 7181.54, "text": " of our first learner, and grab the predictions of our second learner, and stack them up and", "tokens": [50896, 295, 527, 700, 33347, 11, 293, 4444, 264, 21264, 295, 527, 1150, 33347, 11, 293, 8630, 552, 493, 293, 51281], "temperature": 0.0, "avg_logprob": -0.2262485905697471, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.03962264955043793}, {"id": 1429, "seek": 716320, "start": 7181.54, "end": 7183.84, "text": " take them in.", "tokens": [51281, 747, 552, 294, 13, 51396], "temperature": 0.0, "avg_logprob": -0.2262485905697471, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.03962264955043793}, {"id": 1430, "seek": 716320, "start": 7183.84, "end": 7188.599999999999, "text": " And this is called ensembling.", "tokens": [51396, 400, 341, 307, 1219, 12567, 2504, 1688, 13, 51634], "temperature": 0.0, "avg_logprob": -0.2262485905697471, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.03962264955043793}, {"id": 1431, "seek": 718860, "start": 7188.6, "end": 7193.96, "text": " And not surprisingly the ensemble is better than either of the two individual models at", "tokens": [50364, 400, 406, 17600, 264, 19492, 307, 1101, 813, 2139, 295, 264, 732, 2609, 5245, 412, 50632], "temperature": 0.0, "avg_logprob": -0.2928609275817871, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.16450220346450806}, {"id": 1432, "seek": 718860, "start": 7193.96, "end": 7196.240000000001, "text": " 94.4.", "tokens": [50632, 30849, 13, 19, 13, 50746], "temperature": 0.0, "avg_logprob": -0.2928609275817871, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.16450220346450806}, {"id": 1433, "seek": 718860, "start": 7196.240000000001, "end": 7201.280000000001, "text": " Although unfortunately I'm afraid to say we didn't beat our best.", "tokens": [50746, 5780, 7015, 286, 478, 4638, 281, 584, 321, 994, 380, 4224, 527, 1151, 13, 50998], "temperature": 0.0, "avg_logprob": -0.2928609275817871, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.16450220346450806}, {"id": 1434, "seek": 718860, "start": 7201.280000000001, "end": 7208.04, "text": " But it's a useful trick, and particularly useful trick in this case I was kind of like", "tokens": [50998, 583, 309, 311, 257, 4420, 4282, 11, 293, 4098, 4420, 4282, 294, 341, 1389, 286, 390, 733, 295, 411, 51336], "temperature": 0.0, "avg_logprob": -0.2928609275817871, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.16450220346450806}, {"id": 1435, "seek": 718860, "start": 7208.04, "end": 7213.04, "text": " trying something a bit interesting to see if using the exact same number of epochs,", "tokens": [51336, 1382, 746, 257, 857, 1880, 281, 536, 498, 1228, 264, 1900, 912, 1230, 295, 30992, 28346, 11, 51586], "temperature": 0.0, "avg_logprob": -0.2928609275817871, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.16450220346450806}, {"id": 1436, "seek": 718860, "start": 7213.04, "end": 7218.0, "text": " can I get a better result by using ensembling instead of training for longer.", "tokens": [51586, 393, 286, 483, 257, 1101, 1874, 538, 1228, 12567, 2504, 1688, 2602, 295, 3097, 337, 2854, 13, 51834], "temperature": 0.0, "avg_logprob": -0.2928609275817871, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.16450220346450806}, {"id": 1437, "seek": 721800, "start": 7218.4, "end": 7219.48, "text": " And the answer was I couldn't.", "tokens": [50384, 400, 264, 1867, 390, 286, 2809, 380, 13, 50438], "temperature": 0.0, "avg_logprob": -0.28234670828054614, "compression_ratio": 1.6, "no_speech_prob": 0.04813506454229355}, {"id": 1438, "seek": 721800, "start": 7219.48, "end": 7223.52, "text": " Maybe it's because the random copy is not as good, or maybe I'm using too much augmentation.", "tokens": [50438, 2704, 309, 311, 570, 264, 4974, 5055, 307, 406, 382, 665, 11, 420, 1310, 286, 478, 1228, 886, 709, 14501, 19631, 13, 50640], "temperature": 0.0, "avg_logprob": -0.28234670828054614, "compression_ratio": 1.6, "no_speech_prob": 0.04813506454229355}, {"id": 1439, "seek": 721800, "start": 7223.52, "end": 7232.04, "text": " Who knows, but it's something that you could experiment with.", "tokens": [50640, 2102, 3255, 11, 457, 309, 311, 746, 300, 291, 727, 5120, 365, 13, 51066], "temperature": 0.0, "avg_logprob": -0.28234670828054614, "compression_ratio": 1.6, "no_speech_prob": 0.04813506454229355}, {"id": 1440, "seek": 721800, "start": 7232.04, "end": 7237.16, "text": " So Shaowen mentions in the chat that cutmix is similar to this, which is actually that's", "tokens": [51066, 407, 14944, 305, 268, 23844, 294, 264, 5081, 300, 1723, 76, 970, 307, 2531, 281, 341, 11, 597, 307, 767, 300, 311, 51322], "temperature": 0.0, "avg_logprob": -0.28234670828054614, "compression_ratio": 1.6, "no_speech_prob": 0.04813506454229355}, {"id": 1441, "seek": 721800, "start": 7237.16, "end": 7238.16, "text": " a good point.", "tokens": [51322, 257, 665, 935, 13, 51372], "temperature": 0.0, "avg_logprob": -0.28234670828054614, "compression_ratio": 1.6, "no_speech_prob": 0.04813506454229355}, {"id": 1442, "seek": 721800, "start": 7238.16, "end": 7242.8, "text": " I'd forgotten cutmix, but cutmix, yes, copies it from different images rather than from", "tokens": [51372, 286, 1116, 11832, 1723, 76, 970, 11, 457, 1723, 76, 970, 11, 2086, 11, 14341, 309, 490, 819, 5267, 2831, 813, 490, 51604], "temperature": 0.0, "avg_logprob": -0.28234670828054614, "compression_ratio": 1.6, "no_speech_prob": 0.04813506454229355}, {"id": 1443, "seek": 721800, "start": 7242.8, "end": 7243.8, "text": " the same image.", "tokens": [51604, 264, 912, 3256, 13, 51654], "temperature": 0.0, "avg_logprob": -0.28234670828054614, "compression_ratio": 1.6, "no_speech_prob": 0.04813506454229355}, {"id": 1444, "seek": 724380, "start": 7243.8, "end": 7247.52, "text": " But yeah, it's pretty much the same thing I guess-ish.", "tokens": [50364, 583, 1338, 11, 309, 311, 1238, 709, 264, 912, 551, 286, 2041, 12, 742, 13, 50550], "temperature": 0.0, "avg_logprob": -0.39163333767063013, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.07584694772958755}, {"id": 1445, "seek": 724380, "start": 7247.52, "end": 7249.92, "text": " Well, similar.", "tokens": [50550, 1042, 11, 2531, 13, 50670], "temperature": 0.0, "avg_logprob": -0.39163333767063013, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.07584694772958755}, {"id": 1446, "seek": 724380, "start": 7249.92, "end": 7253.8, "text": " Yeah, very similar.", "tokens": [50670, 865, 11, 588, 2531, 13, 50864], "temperature": 0.0, "avg_logprob": -0.39163333767063013, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.07584694772958755}, {"id": 1447, "seek": 724380, "start": 7253.8, "end": 7259.4800000000005, "text": " All right, so that brings us to the end of the lesson.", "tokens": [50864, 1057, 558, 11, 370, 300, 5607, 505, 281, 264, 917, 295, 264, 6898, 13, 51148], "temperature": 0.0, "avg_logprob": -0.39163333767063013, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.07584694772958755}, {"id": 1448, "seek": 724380, "start": 7259.4800000000005, "end": 7266.4800000000005, "text": " And you know I am, yeah, so pumped and excited to share this with you.", "tokens": [51148, 400, 291, 458, 286, 669, 11, 1338, 11, 370, 27774, 293, 2919, 281, 2073, 341, 365, 291, 13, 51498], "temperature": 0.0, "avg_logprob": -0.39163333767063013, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.07584694772958755}, {"id": 1449, "seek": 724380, "start": 7266.4800000000005, "end": 7270.68, "text": " Because you know I don't know that this has ever been done before, you know, to be able", "tokens": [51498, 1436, 291, 458, 286, 500, 380, 458, 300, 341, 575, 1562, 668, 1096, 949, 11, 291, 458, 11, 281, 312, 1075, 51708], "temperature": 0.0, "avg_logprob": -0.39163333767063013, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.07584694772958755}, {"id": 1450, "seek": 727068, "start": 7271.08, "end": 7277.08, "text": " to go from, I mean even in our previous courses, we've never done this before, go from scratch,", "tokens": [50384, 281, 352, 490, 11, 286, 914, 754, 294, 527, 3894, 7712, 11, 321, 600, 1128, 1096, 341, 949, 11, 352, 490, 8459, 11, 50684], "temperature": 0.0, "avg_logprob": -0.27099795621984146, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0047551547177135944}, {"id": 1451, "seek": 727068, "start": 7277.08, "end": 7282.84, "text": " step by step, to an absolute state-of-the-art model where we build everything ourselves", "tokens": [50684, 1823, 538, 1823, 11, 281, 364, 8236, 1785, 12, 2670, 12, 3322, 12, 446, 2316, 689, 321, 1322, 1203, 4175, 50972], "temperature": 0.0, "avg_logprob": -0.27099795621984146, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0047551547177135944}, {"id": 1452, "seek": 727068, "start": 7282.84, "end": 7286.08, "text": " and it runs this quickly.", "tokens": [50972, 293, 309, 6676, 341, 2661, 13, 51134], "temperature": 0.0, "avg_logprob": -0.27099795621984146, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0047551547177135944}, {"id": 1453, "seek": 727068, "start": 7286.08, "end": 7291.360000000001, "text": " And we're even using our own custom ResNet and everything, you know, just using common", "tokens": [51134, 400, 321, 434, 754, 1228, 527, 1065, 2375, 5015, 31890, 293, 1203, 11, 291, 458, 11, 445, 1228, 2689, 51398], "temperature": 0.0, "avg_logprob": -0.27099795621984146, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0047551547177135944}, {"id": 1454, "seek": 727068, "start": 7291.360000000001, "end": 7295.240000000001, "text": " sense at every stage.", "tokens": [51398, 2020, 412, 633, 3233, 13, 51592], "temperature": 0.0, "avg_logprob": -0.27099795621984146, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0047551547177135944}, {"id": 1455, "seek": 729524, "start": 7295.28, "end": 7303.48, "text": " And so hopefully that shows that deep learning is not magic, you know, that we can actually", "tokens": [50366, 400, 370, 4696, 300, 3110, 300, 2452, 2539, 307, 406, 5585, 11, 291, 458, 11, 300, 321, 393, 767, 50776], "temperature": 0.0, "avg_logprob": -0.2678434396091896, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0010322195012122393}, {"id": 1456, "seek": 729524, "start": 7303.48, "end": 7306.24, "text": " build the pieces ourselves.", "tokens": [50776, 1322, 264, 3755, 4175, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2678434396091896, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0010322195012122393}, {"id": 1457, "seek": 729524, "start": 7306.24, "end": 7315.4, "text": " And yeah, as you'll see, going up to larger datasets, absolutely nothing changes.", "tokens": [50914, 400, 1338, 11, 382, 291, 603, 536, 11, 516, 493, 281, 4833, 42856, 11, 3122, 1825, 2962, 13, 51372], "temperature": 0.0, "avg_logprob": -0.2678434396091896, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0010322195012122393}, {"id": 1458, "seek": 729524, "start": 7315.4, "end": 7317.4, "text": " And so it's exactly these techniques.", "tokens": [51372, 400, 370, 309, 311, 2293, 613, 7512, 13, 51472], "temperature": 0.0, "avg_logprob": -0.2678434396091896, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0010322195012122393}, {"id": 1459, "seek": 729524, "start": 7317.4, "end": 7323.639999999999, "text": " And this is actually, I do 99% of my research on very small datasets.", "tokens": [51472, 400, 341, 307, 767, 11, 286, 360, 11803, 4, 295, 452, 2132, 322, 588, 1359, 42856, 13, 51784], "temperature": 0.0, "avg_logprob": -0.2678434396091896, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0010322195012122393}, {"id": 1460, "seek": 732364, "start": 7323.64, "end": 7327.320000000001, "text": " Because you can iterate much more quickly, you can understand them much better.", "tokens": [50364, 1436, 291, 393, 44497, 709, 544, 2661, 11, 291, 393, 1223, 552, 709, 1101, 13, 50548], "temperature": 0.0, "avg_logprob": -0.239927550892771, "compression_ratio": 1.527363184079602, "no_speech_prob": 0.0017821850487962365}, {"id": 1461, "seek": 732364, "start": 7327.320000000001, "end": 7330.56, "text": " And I don't think there's ever been a time where I've then gone up to a bigger dataset", "tokens": [50548, 400, 286, 500, 380, 519, 456, 311, 1562, 668, 257, 565, 689, 286, 600, 550, 2780, 493, 281, 257, 3801, 28872, 50710], "temperature": 0.0, "avg_logprob": -0.239927550892771, "compression_ratio": 1.527363184079602, "no_speech_prob": 0.0017821850487962365}, {"id": 1462, "seek": 732364, "start": 7330.56, "end": 7334.56, "text": " and my findings didn't continue to hold true.", "tokens": [50710, 293, 452, 16483, 994, 380, 2354, 281, 1797, 2074, 13, 50910], "temperature": 0.0, "avg_logprob": -0.239927550892771, "compression_ratio": 1.527363184079602, "no_speech_prob": 0.0017821850487962365}, {"id": 1463, "seek": 732364, "start": 7334.56, "end": 7342.8, "text": " Now homework, what I would really like you to do is to actually do the thing that I didn't", "tokens": [50910, 823, 14578, 11, 437, 286, 576, 534, 411, 291, 281, 360, 307, 281, 767, 360, 264, 551, 300, 286, 994, 380, 51322], "temperature": 0.0, "avg_logprob": -0.239927550892771, "compression_ratio": 1.527363184079602, "no_speech_prob": 0.0017821850487962365}, {"id": 1464, "seek": 732364, "start": 7342.8, "end": 7343.84, "text": " do.", "tokens": [51322, 360, 13, 51374], "temperature": 0.0, "avg_logprob": -0.239927550892771, "compression_ratio": 1.527363184079602, "no_speech_prob": 0.0017821850487962365}, {"id": 1465, "seek": 734384, "start": 7343.84, "end": 7361.24, "text": " Which is to do the create your own schedulers that work with Python's optimizers.", "tokens": [50364, 3013, 307, 281, 360, 264, 1884, 428, 1065, 12000, 433, 300, 589, 365, 15329, 311, 5028, 22525, 13, 51234], "temperature": 0.0, "avg_logprob": -0.2689036149245042, "compression_ratio": 1.3647058823529412, "no_speech_prob": 0.03461452201008797}, {"id": 1466, "seek": 734384, "start": 7361.24, "end": 7368.76, "text": " So I mean, it's the tricky bit will be making sure that you understand the PyTorch API well,", "tokens": [51234, 407, 286, 914, 11, 309, 311, 264, 12414, 857, 486, 312, 1455, 988, 300, 291, 1223, 264, 9953, 51, 284, 339, 9362, 731, 11, 51610], "temperature": 0.0, "avg_logprob": -0.2689036149245042, "compression_ratio": 1.3647058823529412, "no_speech_prob": 0.03461452201008797}, {"id": 1467, "seek": 734384, "start": 7368.76, "end": 7370.18, "text": " which I've really laid out here.", "tokens": [51610, 597, 286, 600, 534, 9897, 484, 510, 13, 51681], "temperature": 0.0, "avg_logprob": -0.2689036149245042, "compression_ratio": 1.3647058823529412, "no_speech_prob": 0.03461452201008797}, {"id": 1468, "seek": 734384, "start": 7370.18, "end": 7371.7, "text": " So study this carefully.", "tokens": [51681, 407, 2979, 341, 7500, 13, 51757], "temperature": 0.0, "avg_logprob": -0.2689036149245042, "compression_ratio": 1.3647058823529412, "no_speech_prob": 0.03461452201008797}, {"id": 1469, "seek": 737170, "start": 7371.7, "end": 7378.54, "text": " So create your own cosine annealing scheduler from scratch.", "tokens": [50364, 407, 1884, 428, 1065, 23565, 22256, 4270, 12000, 260, 490, 8459, 13, 50706], "temperature": 0.0, "avg_logprob": -0.24029036724206174, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.0009253704338334501}, {"id": 1470, "seek": 737170, "start": 7378.54, "end": 7385.78, "text": " And then create your own one cycle scheduler from scratch.", "tokens": [50706, 400, 550, 1884, 428, 1065, 472, 6586, 12000, 260, 490, 8459, 13, 51068], "temperature": 0.0, "avg_logprob": -0.24029036724206174, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.0009253704338334501}, {"id": 1471, "seek": 737170, "start": 7385.78, "end": 7391.0199999999995, "text": " And make sure that they work correctly with this batch scheduler callback.", "tokens": [51068, 400, 652, 988, 300, 436, 589, 8944, 365, 341, 15245, 12000, 260, 818, 3207, 13, 51330], "temperature": 0.0, "avg_logprob": -0.24029036724206174, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.0009253704338334501}, {"id": 1472, "seek": 737170, "start": 7391.0199999999995, "end": 7397.98, "text": " This will be a very good exercise for you in, you know, hopefully getting extremely", "tokens": [51330, 639, 486, 312, 257, 588, 665, 5380, 337, 291, 294, 11, 291, 458, 11, 4696, 1242, 4664, 51678], "temperature": 0.0, "avg_logprob": -0.24029036724206174, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.0009253704338334501}, {"id": 1473, "seek": 739798, "start": 7397.98, "end": 7401.98, "text": " frustrated as things don't work the way you hoped they would.", "tokens": [50364, 15751, 382, 721, 500, 380, 589, 264, 636, 291, 19737, 436, 576, 13, 50564], "temperature": 0.0, "avg_logprob": -0.23735982562423846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.17551496624946594}, {"id": 1474, "seek": 739798, "start": 7401.98, "end": 7403.419999999999, "text": " And being mystified for a while.", "tokens": [50564, 400, 885, 9111, 2587, 337, 257, 1339, 13, 50636], "temperature": 0.0, "avg_logprob": -0.23735982562423846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.17551496624946594}, {"id": 1475, "seek": 739798, "start": 7403.419999999999, "end": 7407.7, "text": " And then working through it, you know, using this very step-by-step approach.", "tokens": [50636, 400, 550, 1364, 807, 309, 11, 291, 458, 11, 1228, 341, 588, 1823, 12, 2322, 12, 16792, 3109, 13, 50850], "temperature": 0.0, "avg_logprob": -0.23735982562423846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.17551496624946594}, {"id": 1476, "seek": 739798, "start": 7407.7, "end": 7410.0599999999995, "text": " Lots of experimentation, lots of exploration.", "tokens": [50850, 15908, 295, 37142, 11, 3195, 295, 16197, 13, 50968], "temperature": 0.0, "avg_logprob": -0.23735982562423846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.17551496624946594}, {"id": 1477, "seek": 739798, "start": 7410.0599999999995, "end": 7413.299999999999, "text": " And then figuring it out.", "tokens": [50968, 400, 550, 15213, 309, 484, 13, 51130], "temperature": 0.0, "avg_logprob": -0.23735982562423846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.17551496624946594}, {"id": 1478, "seek": 739798, "start": 7413.299999999999, "end": 7416.099999999999, "text": " That's the journey I'm hoping you have.", "tokens": [51130, 663, 311, 264, 4671, 286, 478, 7159, 291, 362, 13, 51270], "temperature": 0.0, "avg_logprob": -0.23735982562423846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.17551496624946594}, {"id": 1479, "seek": 739798, "start": 7416.099999999999, "end": 7421.66, "text": " If it's all super easy and you get it first go, then you know, you'll have to find something", "tokens": [51270, 759, 309, 311, 439, 1687, 1858, 293, 291, 483, 309, 700, 352, 11, 550, 291, 458, 11, 291, 603, 362, 281, 915, 746, 51548], "temperature": 0.0, "avg_logprob": -0.23735982562423846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.17551496624946594}, {"id": 1480, "seek": 739798, "start": 7421.66, "end": 7423.259999999999, "text": " else to do.", "tokens": [51548, 1646, 281, 360, 13, 51628], "temperature": 0.0, "avg_logprob": -0.23735982562423846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.17551496624946594}, {"id": 1481, "seek": 742326, "start": 7423.26, "end": 7430.34, "text": " But yeah, I'm hoping you'll find it actually, you know, surprisingly tricky to get it all", "tokens": [50364, 583, 1338, 11, 286, 478, 7159, 291, 603, 915, 309, 767, 11, 291, 458, 11, 17600, 12414, 281, 483, 309, 439, 50718], "temperature": 0.0, "avg_logprob": -0.22533328073066577, "compression_ratio": 1.6, "no_speech_prob": 0.014728395268321037}, {"id": 1482, "seek": 742326, "start": 7430.34, "end": 7431.38, "text": " working properly.", "tokens": [50718, 1364, 6108, 13, 50770], "temperature": 0.0, "avg_logprob": -0.22533328073066577, "compression_ratio": 1.6, "no_speech_prob": 0.014728395268321037}, {"id": 1483, "seek": 742326, "start": 7431.38, "end": 7435.5, "text": " And in the process of doing so, you're going to have to do a lot of exploration and experimentation.", "tokens": [50770, 400, 294, 264, 1399, 295, 884, 370, 11, 291, 434, 516, 281, 362, 281, 360, 257, 688, 295, 16197, 293, 37142, 13, 50976], "temperature": 0.0, "avg_logprob": -0.22533328073066577, "compression_ratio": 1.6, "no_speech_prob": 0.014728395268321037}, {"id": 1484, "seek": 742326, "start": 7435.5, "end": 7443.5, "text": " But you'll realize that it requires no, like, prerequisite knowledge at all.", "tokens": [50976, 583, 291, 603, 4325, 300, 309, 7029, 572, 11, 411, 11, 38333, 34152, 3601, 412, 439, 13, 51376], "temperature": 0.0, "avg_logprob": -0.22533328073066577, "compression_ratio": 1.6, "no_speech_prob": 0.014728395268321037}, {"id": 1485, "seek": 742326, "start": 7443.5, "end": 7448.54, "text": " Okay, so if it doesn't work first time, it's not because there's something that you didn't", "tokens": [51376, 1033, 11, 370, 498, 309, 1177, 380, 589, 700, 565, 11, 309, 311, 406, 570, 456, 311, 746, 300, 291, 994, 380, 51628], "temperature": 0.0, "avg_logprob": -0.22533328073066577, "compression_ratio": 1.6, "no_speech_prob": 0.014728395268321037}, {"id": 1486, "seek": 742326, "start": 7448.54, "end": 7451.76, "text": " learn in graduate school, if only you had done a PhD, whatever.", "tokens": [51628, 1466, 294, 8080, 1395, 11, 498, 787, 291, 632, 1096, 257, 14476, 11, 2035, 13, 51789], "temperature": 0.0, "avg_logprob": -0.22533328073066577, "compression_ratio": 1.6, "no_speech_prob": 0.014728395268321037}, {"id": 1487, "seek": 745176, "start": 7451.76, "end": 7456.04, "text": " It's just that you need to dig through, you know, slowly and carefully to see how it all", "tokens": [50364, 467, 311, 445, 300, 291, 643, 281, 2528, 807, 11, 291, 458, 11, 5692, 293, 7500, 281, 536, 577, 309, 439, 50578], "temperature": 0.0, "avg_logprob": -0.2346842216722893, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.013636264018714428}, {"id": 1488, "seek": 745176, "start": 7456.04, "end": 7458.24, "text": " works.", "tokens": [50578, 1985, 13, 50688], "temperature": 0.0, "avg_logprob": -0.2346842216722893, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.013636264018714428}, {"id": 1489, "seek": 745176, "start": 7458.24, "end": 7463.64, "text": " And you know, then see how neat and concise you can get it.", "tokens": [50688, 400, 291, 458, 11, 550, 536, 577, 10654, 293, 44882, 291, 393, 483, 309, 13, 50958], "temperature": 0.0, "avg_logprob": -0.2346842216722893, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.013636264018714428}, {"id": 1490, "seek": 745176, "start": 7463.64, "end": 7467.24, "text": " Then the other homework is to try and beat me.", "tokens": [50958, 1396, 264, 661, 14578, 307, 281, 853, 293, 4224, 385, 13, 51138], "temperature": 0.0, "avg_logprob": -0.2346842216722893, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.013636264018714428}, {"id": 1491, "seek": 745176, "start": 7467.24, "end": 7470.08, "text": " I really, really want people to beat me.", "tokens": [51138, 286, 534, 11, 534, 528, 561, 281, 4224, 385, 13, 51280], "temperature": 0.0, "avg_logprob": -0.2346842216722893, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.013636264018714428}, {"id": 1492, "seek": 745176, "start": 7470.08, "end": 7478.96, "text": " Try to beat me on the 5 epoch, or the 20 epoch, or the 50 epoch fashion MNIST.", "tokens": [51280, 6526, 281, 4224, 385, 322, 264, 1025, 30992, 339, 11, 420, 264, 945, 30992, 339, 11, 420, 264, 2625, 30992, 339, 6700, 376, 45, 19756, 13, 51724], "temperature": 0.0, "avg_logprob": -0.2346842216722893, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.013636264018714428}, {"id": 1493, "seek": 747896, "start": 7478.96, "end": 7489.0, "text": " Ideally using many AI with things that you've added yourself.", "tokens": [50364, 40817, 1228, 867, 7318, 365, 721, 300, 291, 600, 3869, 1803, 13, 50866], "temperature": 0.0, "avg_logprob": -0.32425839742024737, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.23648501932621002}, {"id": 1494, "seek": 747896, "start": 7489.0, "end": 7494.12, "text": " But you know, you can try grabbing other libraries if you like.", "tokens": [50866, 583, 291, 458, 11, 291, 393, 853, 23771, 661, 15148, 498, 291, 411, 13, 51122], "temperature": 0.0, "avg_logprob": -0.32425839742024737, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.23648501932621002}, {"id": 1495, "seek": 747896, "start": 7494.12, "end": 7498.72, "text": " Or ideally, if you do grab another library and you find you can beat my approach, try", "tokens": [51122, 1610, 22915, 11, 498, 291, 360, 4444, 1071, 6405, 293, 291, 915, 291, 393, 4224, 452, 3109, 11, 853, 51352], "temperature": 0.0, "avg_logprob": -0.32425839742024737, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.23648501932621002}, {"id": 1496, "seek": 747896, "start": 7498.72, "end": 7501.24, "text": " to reimplement that library.", "tokens": [51352, 281, 33433, 43704, 300, 6405, 13, 51478], "temperature": 0.0, "avg_logprob": -0.32425839742024737, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.23648501932621002}, {"id": 1497, "seek": 747896, "start": 7501.24, "end": 7506.84, "text": " That way you are still within the spirit of the game.", "tokens": [51478, 663, 636, 291, 366, 920, 1951, 264, 3797, 295, 264, 1216, 13, 51758], "temperature": 0.0, "avg_logprob": -0.32425839742024737, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.23648501932621002}, {"id": 1498, "seek": 750684, "start": 7507.24, "end": 7517.400000000001, "text": " Okay, so in our next lesson, Jono and Tanishka and I are going to be putting this all together", "tokens": [50384, 1033, 11, 370, 294, 527, 958, 6898, 11, 7745, 78, 293, 314, 7524, 2330, 293, 286, 366, 516, 281, 312, 3372, 341, 439, 1214, 50892], "temperature": 0.0, "avg_logprob": -0.30548350016276044, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.0004108442226424813}, {"id": 1499, "seek": 750684, "start": 7517.400000000001, "end": 7522.8, "text": " to create a diffusion model from scratch.", "tokens": [50892, 281, 1884, 257, 25242, 2316, 490, 8459, 13, 51162], "temperature": 0.0, "avg_logprob": -0.30548350016276044, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.0004108442226424813}, {"id": 1500, "seek": 750684, "start": 7522.8, "end": 7526.12, "text": " And we're actually going to be taking a couple of lessons for this.", "tokens": [51162, 400, 321, 434, 767, 516, 281, 312, 1940, 257, 1916, 295, 8820, 337, 341, 13, 51328], "temperature": 0.0, "avg_logprob": -0.30548350016276044, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.0004108442226424813}, {"id": 1501, "seek": 750684, "start": 7526.12, "end": 7532.64, "text": " Not just a diffusion model, but a variety of interesting generative approaches.", "tokens": [51328, 1726, 445, 257, 25242, 2316, 11, 457, 257, 5673, 295, 1880, 1337, 1166, 11587, 13, 51654], "temperature": 0.0, "avg_logprob": -0.30548350016276044, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.0004108442226424813}, {"id": 1502, "seek": 753264, "start": 7532.64, "end": 7537.160000000001, "text": " So we've kind of starting to come full circle.", "tokens": [50364, 407, 321, 600, 733, 295, 2891, 281, 808, 1577, 6329, 13, 50590], "temperature": 0.0, "avg_logprob": -0.3088435842030084, "compression_ratio": 1.423529411764706, "no_speech_prob": 0.003428923198953271}, {"id": 1503, "seek": 753264, "start": 7537.160000000001, "end": 7543.88, "text": " So thank you so much for joining me on this very extensive journey.", "tokens": [50590, 407, 1309, 291, 370, 709, 337, 5549, 385, 322, 341, 588, 13246, 4671, 13, 50926], "temperature": 0.0, "avg_logprob": -0.3088435842030084, "compression_ratio": 1.423529411764706, "no_speech_prob": 0.003428923198953271}, {"id": 1504, "seek": 753264, "start": 7543.88, "end": 7547.56, "text": " And I look forward to hearing what you come up with.", "tokens": [50926, 400, 286, 574, 2128, 281, 4763, 437, 291, 808, 493, 365, 13, 51110], "temperature": 0.0, "avg_logprob": -0.3088435842030084, "compression_ratio": 1.423529411764706, "no_speech_prob": 0.003428923198953271}, {"id": 1505, "seek": 753264, "start": 7547.56, "end": 7552.4800000000005, "text": " Please do come and join us on forums.fast.ai and share your progress.", "tokens": [51110, 2555, 360, 808, 293, 3917, 505, 322, 26998, 13, 7011, 13, 1301, 293, 2073, 428, 4205, 13, 51356], "temperature": 0.0, "avg_logprob": -0.3088435842030084, "compression_ratio": 1.423529411764706, "no_speech_prob": 0.003428923198953271}, {"id": 1506, "seek": 753264, "start": 7552.4800000000005, "end": 7553.64, "text": " Bye!", "tokens": [51356, 4621, 0, 51414], "temperature": 0.0, "avg_logprob": -0.3088435842030084, "compression_ratio": 1.423529411764706, "no_speech_prob": 0.003428923198953271}], "language": "en"}