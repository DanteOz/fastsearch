{"text": " Okay, I'm going to go ahead and get started. I wanted to just briefly return to the problem of the compressed sensing of reconstructing what the CT scan had detected, and so kind of thinking about Tim's question from last time of how would we represent this, so kind of as linear regression. So here if linear regression is matrix X of data times a vector beta of coefficients equals some vector Y. Vector X is the projection, they call it the projection operator, and it's this matrix that you can see is black and then has some white lines that seem to have a little bit of a pattern to them. Does anyone know where these are coming from? Tim? Where the lines hit through those circle things? Close, so you're actually getting a little bit ahead of this. So this is just the matrix X, but we'll get there soon. Matthew? You mean like the readings? So it's actually not the readings. The value of the, well I wanted to say readings, the value at a given angle and whatever step. So what you're getting at, I'll actually skip ahead, is the vector Y, so here, the density readings. Thank you. So yeah, this is kind of the measurements that the CT scan was recording, and what we do to get our vector Y, so this is the right hand side of the equation, the linear regression, is we're just unwrapping this, unraveling it, is the NumPy operation, to get this really one, and a tall vector that's 2000 by one. That's why, any other guesses of, I know this is kind of a tricky question, what this X could be? Matthew? Good catch. Is it the reading or are the results of the image from different angles? Not yet, so that's why, kind of the reading from the different angles. And Sam behind you has a guess. Is it the total intensity, like before when we looked at the projection operator at just one slice, there's a line going through a space, but now are we looking at, like, is the X dimension the position and Y is the angle and then the dot is like the length of the line almost? That's very close. Yeah. So let me kind of go back up to, I think it was this section where we had these pictures of lines at different angles in different locations. And so this was part of a, so this was originally part of a four dimensional matrix where we had the different angles and the positions, which is the vertical position of how high up the line is and then an X-Y coordinate for each. And so here we were kind of just showing pictures where we had index on the first and second dimensions, so the angle and a vertical position and then see the X-Y for all these different lines. And so this mystery matrix X that I was showing you is actually the collection of all of these. So this is kind of what the X-rays look like before they've intersected with anything. And here we've kind of unwrapped them, or not unwrapped, reshaped them. So we took this, this was in 40, and we said, look, okay, like, let's make, I think the X-axis is the angles and positions and then we've unwrapped each picture to be just a single long row. And so that's why it has the appearance that it does down here of like these do contain little lines, but they've been unwrapped. Yes? Linda? Oh, and could you throw the catch box, Sam? This might need an intermediate, oh, good throw and good catch. So what would be the Y-axis and the X-axis? So is the Y-axis unravel the angle and the vertical placed in the X-axis? Is the X and Y? Exactly, yeah. So the Y-axis here is the angle and the vertical position and then this one is the X and the Y, correct? Another question will be like, toDense.A, what does it do? So how do you unwrap, let's say, just the angle and the vertical motion? Well so what toDense is doing is we're storing this as a sparse matrix. So you just make it dense? Yes, yeah. And actually, kind of going off of that, why would it be a good idea to store this matrix as sparse? Anyone can answer this. Yeah, so Valentine said because it's sparse and that's correct, yes. So we see a lot of black here which represents zero, so the white is where there are ones and that's a relatively small portion. So we've got this stored as a sparse SciPy matrix and we're just converting it to dense so we can look at pictures of it. So to kind of actually do the math, we don't need it to be dense. It's more just to be able to look at these pictures. And I also just wanted to highlight that this matrix is a lot wider than it is tall and that's because the width is L by L which is 128 by 128 and we only had, so we have 128 vertical positions but only one seventh of that number of angles which is 18 so that's why it's much shorter is we have a kind of smaller number of angles. Any other questions about this X matrix? Okay, so let's move on to beta. And so beta is what we're solving for with the linear regression and that's the image. So we're trying to kind of recover the image from, you know, given knowing, just knowing what angle and what position the X-rays are coming from and knowing what the outputs are in terms of these kind of measurements of density that were picked up by the CT scan, we're trying to solve for this image which we wouldn't actually have. And so in order to do that or kind of to express that as our linear regression, we have raveled the image from a 2D array into a 1D array. So you'll see this is just this very tall over 16,000 length vector and it consists of just a little bit hard to see but it's mostly black with a few, some white spots which fits with this picture being mostly black background and just a little bit of white. And so you can see that how the kind of the measurements, how they were gathered were basically just by taking these X-rays and multiplying them by the image we were X-raying only as a vector. Any questions? Does this help to kind of have it as a, I guess kind of more standard linear regression right out of matrix times a vector equals a vector? Alright and then just to review what penalty for our linear regression gave us the best answer. And I hear mumblings but I couldn't hear what was said. Yes L1 is the correct answer and that's because the L1 norm induces sparsity and so we saw I'm just going to briefly show those again that when we did a linear regression with the L2 penalty we don't get a very good answer. However, doing it with the L1 penalty gives us almost exactly the right answer. Any final questions on kind of notebook four, compressed sensing? Okay, head back to notebook five. So we talked about this some last time but I want to review some of what we saw. So we're looking at this data set of diabetes data that and we've broken it into a training set and a test set which is really important so that you don't overfit. Our training data is 353 rows, those are the measurements, times 10 variables and these are often called features in machine learning. We did a linear regression in Scikit-learn just using, is this large enough? I should probably make this bigger. Yeah, so we just used Scikit-learn's linear regression. We get a prediction and so here kind of as a starting point our L2 error is 75, our L1 error is 60, so not a great fit. And we talked about one way to try to get a better fit would be to add polynomial features. And so kind of linear regression for, here I just did a problem where you had three features, X0, X1 and X2, you're trying to find the best coefficients, beta 0, beta 1, beta 2. And adding polynomial features just means taking, still have X0, X1, X2 and then adding X0 squared, X0, X1, X0, X2, X1 squared, X1 times X2 and X2 squared. Why do you think this is still a linear regression problem even though we have squared terms in here? Wait, Linda, can you throw that catch box to Connor? Because the coefficients are still linear? Exactly, the coefficients are still linear. So we're just solving for the betas and so we're now solving for eight coefficients. And so this is a way to kind of create a model that's going to capture a little bit more complexity in case there are interactions between X0 and X1 or if there's a squared relationship between how the dependent variable, which in this case is I think like a long term health outcome for the diabetes patient, how that depends on your inputs. But yeah, this still, you kind of calculate these additional features and then you still have a matrix of values that are linear with beta. You just have more coefficients you're finding. Questions about the setup of using polynomial features? All right. So we started off, scikit-learn has a polynomial features method that will create them for us. So we did that to get some additional training features, which it's printed out here. And in our case, since we had 10 features that we started with, we're getting back, we're now up to 65 features. And so it's just calculated all the possible combinations. So age squared, age times sex, age times BMI, blood pressure times serum level two, but just any possible combination. So this reduces our error when we run linear regression again with our kind of having more features. The only issue is that time is squared just to even create these features. And so this could possibly get slow as we have a larger and larger data set. So we saw Numba last time. And to make it clear, Numba is a compiler and it is a Python library that compiles code directly to C. I wanted to highlight, so I think Jake Vander Plaas has some really nice, he's got like a nice introduction. And he also has a later blog post. Let me open this. Where he tries to do a, or not tries, but does a non-trivial algorithm, which I thought was nice since often you kind of see these test cases on simpler algorithms, which is what I gave you. But so I definitely recommend checking out this blog post. And then I wanted to highlight a quote of his. This is from his kind of intro post. He says that, so for this problem he does, Numba is a thousand times faster than a pure Python implementation and only marginally slower than nearly identical Cython code. And Cython is another Python library slash compiler that compiles directly to C. And he says that he has years of experience with Cython and only an hour's experience with Numba. And he was using every optimization he knew for the Cython variation and just the basic vanilla syntax for Numba. So I think this is a really strong endorsement of Numba over Cython, that it took less knowledge. And I've only used Cython a little bit, but this was my experience that there was more to learn about Cython and it felt closer to C, whereas Numba is very simple to use. And I saw kind of similar results for the problems I've tried it on. And again, Cython and Numba are both ways to compile your Python to C for additional optimization. So I wanted to walk through kind of our example again that we saw last time. So we have this Python algorithm that is bit contrived. We'll be using it for polynomial feature generation though next. This is just as an introduction. So we're going through this for loop with kind of doing a number of operations with x and y to calculate a new vector z. We time it and it's 49 milliseconds. And then we use NumPy to vectorize it. So here we've taken out the for loop and are just acting on the vectors. So you can see this pure Python, got a for loop where we go through each element of the array. And then NumPy has this built in vectorization where we can just treat x and y as vectors and do our operations on them. And this is a lot faster. This is 35 microseconds. So again, we've gone from milliseconds to microseconds, which is a thousand times speed up using NumPy. So what's a downside to this NumPy approach? Any ideas? Sam? Oh wait, Connor throw the ketchup box. Thanks. Poor localization. Yes, poor localization. Do you want to say more about what that means? Yeah. Instead of doing one operation on a single value from the x vector and a single value from the y vector, and then writing once to z, it's going to read in all of x and all of y at every stage and process. Yes, exactly. So that I can get this back. Thank you. Yeah, so as Sam said, what's happening here is that it's going to read in all of x and y and if these are very long vectors, they might not all fit in cache. And so that would mean it's reading in the first part of x and the first part of y, doing this computation, then reading in a later part of x, later part of y, still on the same y, doing this x squared minus y times 55 computation, and then another section of x and y, and then it goes to the next line and it has to do all that again. It has to read in the first part of the vector x, the first part of the vector y, read in the second part, and so on, if the vector is large enough to not fit all in cache. And so, yeah, as Sam said, that's poor localization and can be more time consuming. So then we get Numba. Numba offers several different decorators and we're just going to talk about two of them. One is at JIT, which stands for Just in Time Compilation. It's a little bit confusing because I think all of Numba's compilers are just in time, but this is kind of the most general one. And then we'll also look at Vectorize, which doesn't require us to write a for loop. And so that's really useful when operating on vectors of the same size. And what Vectorize is doing is it's automatically writing something called a NumPy U-FUNC for us. So a NumPy U-FUNC, that's a, actually I'll pull up the page. It's a universal function. And that's something that's kind of like lower level within NumPy and more difficult to write yourself where you are writing in C to be creating those. And Numba will kind of create those for you if you use that Vectorize. We'll start though with at JIT. So you just put the decorator on the line before you define your method. We still have our for loop. We're going through. And now when we time it, it's six microseconds. So again, just to remember, the NumPy version was 36 microseconds. So this has been a six times speed up over NumPy. And NumPy was a thousand times speed up over pure Python. And then with at Vectorize, this is kind of the same as the at JIT version, only we don't have the for loop. And there we get 5.82 microseconds, which is kind of almost the same. Although I think this is cleaner to read because you don't have the for loop. However, if you were doing something where you weren't acting on vectors of the same size to kind of get back a vector of that size, at JIT would be good to use. So I just want to say NumPy, or sorry, Numba is amazing because this is really fast and fairly simple to write. Sam? Oh, yeah. Can you do functions, can you do broadcasting in this context? Yes, and so actually a Ufunc, so really what Vectorize is doing is creating this NumPy Ufunc, which is required to have broadcasting for you. So yeah, this would have broadcasting. And that's a benefit of Numba is you're not having to code the broadcasting yourself to create the Ufunc. So the function here, the Vectorize one, looks almost identical to the NumPy one. Correct. And what is it, like the locality of this one is better than the other one. Yes, and that's because Numba is kind of has smarter optimization than NumPy. So the issue with NumPy is it's not able to look ahead, like NumPy is still going kind of line by line through this algorithm. So it's basically kind of completing, you know, x squared minus y times 55 before it even looks at this like, oh, now I have to do something with x and y again. Because this decorator is like for the whole method, Numba is optimizing kind of that whole method as a unit. So it's got a little bit of, you know, this like bigger picture of like, hey, I'm going to be using x and y again on the next line. Any other questions about this? All right. So let's, oh, wait, wait for the microphone. Just curious. So it looks like Numba is a lot quicker and it's very easy to use. Is there any reason sometimes we still use NumPy instead of Numba? So I think that's a good question. NumPy is going to have a lot more specialized methods written, like NumPy is a much larger library. So I think it's possible that if you were doing kind of like some fancy specific NumPy method that Numba may not have that written. Right. Yes. Yeah. Yeah. And you can use them together. Yeah. Yeah. And I really feel like this is something that Numba just doesn't seem that widely known to exist, but I think it's a relatively simple tool that gives you a nice speed up. Are the arguments to this function NumPy arrays or can they be lists? Like when you use vectorize, does it automatically treat all of the lists like NumPy arrays? I'm not sure about that one. And the other, I want to say Numba has more functionality that I'm not getting into, but Numba gives you the option of explicitly passing types or like explicitly setting like this is what the types of the method will be. Okay. So let's go back to the problem we were interested in of polynomial features. So we're adding our at-jit decorator in here. And actually let me go to, this gives you different options. No Python disables the use of Py objects in Python API calls. And there's also, I'll show an example of this later, but there is also, you could pass the types in, I believe here, if you want to say explicitly what kind of types the method's taking. So I wanted to just kind of show the punchline of when we use this method. Our polynomial vectorization, the time is seven microseconds. And so to compare that to what we were getting above with the scikit-learn of polynomial features, we were getting 600 microseconds. So this is about a hundred times speed up, which is pretty impressive because everything in scikit-learn has been kind of written by experts and pretty heavily optimized. And so here, and I should maybe go through a little bit. Here basically what we're doing is just kind of looping through our features and taking all possible combinations, multiplying them together in order to increase our number of features. So this is just adding something, kind of adding all the squared terms. Hey, yeah, so a big speed up there. And then something I skipped over, let me go back to this, is I basically wanted to talk a little bit about row major versus column major storage. And I found there's a nice blog post on it. And so the idea of row major storage is you're storing things by rows. And column major, you're storing by columns. So here kind of first column, second column, third column. And so it's important to know, or it can be very important to know how your data is stored. You know, and in many ways it doesn't matter beyond just knowing because it makes it easier kind of to access your data. If you're using column major, it's easier to access it by column. If you're using row major, it's easier to access it by row. Actually, yeah, I should show. So here with row major storage, if you wanted to access the first column, you would have to kind of grab this element and then this element and then this element, you know, which are not contiguous in memory. Whereas column major storage, it's easy to get a particular column because they're all next to each other. And so it'll be quicker to access. But vice versa, that it's easier to grab a row from row major storage, whereas here if you wanted the second row, you have to kind of jump around to grab the elements. So somewhat unfortunately is that this is not consistent between languages. And so Fortran is column major layout and it was kind of the first major language to use column major. And then Matlab R and Julia also use column major. C, C++, Python, Pascal, and Mathematica all use row major. And this really just kind of matters when you're writing algorithms of if you're using column major, you probably want to loop by columns. If you're using row major, you want to loop by rows. And so NumPy gives you the functionality to switch between those. So there's this as Fortran array to convert it from row major to column major. And so we're using that here. That's kind of the logic behind that. Questions about row major versus column major? Okay. So here we've converted our train and test because that's what we need for this, our polynomial vectorization since we're taking columns of data and multiplying them by each other to create our new features. We want column major layout. And so again, this is giving us a big speed up over scikit-learn. And this is a kind of real problem we might want to solve to improve our linear regression. All right. And so then as a final step, we want to add some regularization to our model to reduce overfitting. So here, lasso regression is what uses an L1 penalty. We're going to try using that here. And I've linked to, there's a Coursera video on lasso regression if anyone wanted more. And at this point, we've already seen the L1 penalty a few times, or at least two times. Can anyone remember what the two times we've seen the L1 penalty is? All right. Someone raise their hand and get the microphone from Sam. Robust PCA and CT scan. Exactly. Yeah. So robust PCA for the background removal of the surveillance video and then for the CT scan, compressed sensing problem. Yeah. So here, scikit-learn has a lasso CV. We're not going to get too deep into, have you all seen cross validation in your other courses? Okay. Great. I see a lot of nodding heads. And that's just kind of comparing different parameters. And so here, so alpha is the weight of how much weight to put on the air. And this will compare 10 different alphas and tell us what the best one is. So we do that and we get an improvement in our regression, our metrics. So again, this is the L2 norm and the L1 norm. Those are now down to 50 and 40. And so kind of when we ran this lasso cross validation, we were getting back a model that had alpha set to what the optimal one was. And this does have additional parameters, like if you wanted to set yourself which alphas you want to try. And finally, we're going to add some noise to the data. So we do that here. This can be another way to try to kind of reduce our overfitting. Then I also just wanted to kind of briefly show you the Huber loss. And that's yet another loss function. And it's less sensitive to outliers than squared error loss. Basically what it is, is it's quadratic for small error values and then linear for large values. So this is what this method is doing. It's saying for x less than a certain value, this is going to be quadratic. For larger x, this will be linear. And the use of the one halves are to make sure that it's differentiable. So even where it meets up at point delta, it's still differentiable. So we try using that, or actually, so I guess here this is kind of illustrating, if we had this noise, our method is giving us worse error, but we can kind of improve it by using a better loss function. Any questions? Roger? And Tim, can you throw the microphone? So is x the actual data in that loss function? Or is that supposed to be the prediction? So here, oh, that's a great question. Really that would be the error. So that would be the difference between the prediction and the actual value. Good question. Any other questions? Matthew? And can you throw the microphone? Is there a rule of thumb for picking the Huber value, or is it just kind of? That's a good question. I think some of it also depends on your problem, like if you think that outliers are kind of less important. It could be good to use the Huber loss method. Yeah, choosing a loss method in general kind of involves some of your perspective on the problem. So like with the, going back to the CT scan, we could look at the pictures and know, OK, this L2 picture is not what we want, we know that we're supposed to be getting something that looks more like the picture we're getting from the L1. But yeah, I think that kind of requires some domain knowledge about your problem or what you're trying to find. Yeah, it's tough because each, like the loss method itself is what saying a good solution is, but you have to still choose which loss method. Thanks. Any other questions? OK, we're a little bit early, but I think this might be a good time to stop for our break and not start the next notebook until after that. Yeah, so let's meet back here in eight minutes. So that would be 11, 1156. OK, I'm going to go ahead and start back up. So yeah, I just wanted to kind of say the key takeaways from lesson five are kind of one, you know, polynomial features as relatively easy way to try to kind of improve performance while still using linear regression. And, you know, let you capture kind of this interaction of how features interact with each other. And then number is a great way to speed up your code. Any questions? Cool. Let's start on lesson six, how to implement linear regression. So we've been using scikit-learns implementation, and now we're kind of going to go a level deeper and think about how would we implement this ourselves. First, so we're still going to be working with the same data set as before, the diabetes data set. First, let's look at how scikit-learn did this. And so you can check the source code, and it's really handy. Actually, I didn't go there. But the SciPy documentation, so let me show this. Typically, when you search for something, you know, you're interested in scikit-learn, least squares. Actually, I should probably do SciPy. It's kind of nice on the documentation. There's always a link to the source code. And so I think this can be really helpful for getting more information about what you're doing. Let me just go back here to make sure. So here I wanted to look at SciPy's linear regression that we've been using and see kind of what is it doing. And there will be stuff you can kind of skim through more where they're just, you know, like maybe checking the inputs or creating an object. But here the interesting part to me was how to say, okay, how do they get the coefficients? And here they're handling the case if it's sparse or not. We were working with a matrix that wasn't sparse, and we see it's calling Lin-Aoj.least-square. So then we can go there and just confirm that they are importing Lin-Aoj from SciPy. So we want it... Actually, sorry. They had called it LSTQ, right? Yeah, Lin-Aoj.least-square. So come here to the documentation and then into the source code. And we scroll down. And so we're kind of getting a hint here that... So they're using a LawPack driver. Actually, can anyone remember what is LawPack? It's like... Wait, okay. The linear algebra package and also known as BPack and MKL, the most known name. So, close. Does anyone want to elaborate on that or clarify a little? Yeah, so LawPack is a low-level linear algebra library, and it's using BLAST, which is kind of even lower-level library for basic matrix computations. So BLAST is going to have things like this is how you do matrix vector multiplication, this is how you do matrix-matrix multiplication, and then LawPack is a layer above that and is kind of has these decompositions we've seen, like this is how you do SVD or QR. And this is something that has been heavily, heavily optimized. And LawPack, I believe, came out in the early 90s. So let me bring up the notebook just to make sure I get the facts right. LawPack has kind of been optimized for each computer, and so libraries like NumPy are calling LawPack, and pretty much any scientific library in any language is going to be calling LawPack at a lower level. I'll just briefly bring that up. Oh, yeah, the speed section. Maybe not. Okay, I'll come back to this. Sorry, this is somewhere in this lesson. I'll add a little bit more detail about it. Oh, you know what? It's up in the matrix computation section. Okay, sorry about that. I'll find that and show that next time. But yeah, LawPack, low-level library being called by pretty much all scientific computing libraries. So, yeah, we were back here in the SciPy source code. It'll become important that we're kind of taking one of these, it says LawPack drivers, GELSD, GELSY, GELSS, and it's going to call them, yeah, down here. It's kind of checking for the driver and then calling that from LawPack. And so the information we have is options, and this shows up in the comment at the top. The options are GELSD, GELSY, and GELSS. The default is GELSD. However, GELSY can be slightly faster. GELSS was used historically. It's slow but uses less memory. And so this can be kind of helpful to read, and this is something that we could override if we wanted, kind of still wanted to be using SciPy's least squares but wanted it to use a different method at this low level. And so this is also kind of, I think, interesting to see that SciPy has, you know, at least two main options. You know, it's not using the same algorithm necessarily all the time. And then just here, I did not get into the details of this at all. You can kind of just get like the very basic kind of, oh, this computes the minimum norm solution to a linear square problem using the SBD decomposition and a divide and conquer method. And so the key thing here is I didn't want you to get into the details at all, but I do want you to feel comfortable looking at SciPy's source code and, you know, particularly at least like reading the comments in there because they can be helpful. Yeah, just to show that there are these three different methods that could be happening under the hood. And then I glossed over this, but we saw that it was, you know, had this if statement, if it's sparse, call these methods. If not, call these other ones. We're not going to dwell on the sparse one at all, but I wanted to say that SciPy's, and then this was happening at Scikit-learn, it was calling SciPy sparse if it's sparse, the SciPy lenalge normal one if it's not, that it's using yet another method for the sparse ones. And so that's something that's typically hidden from you when you're just using Scikit-learn's linear regression that these different things could be happening. So lenalge.lease square. Here I've tried calling it and overriding, so passing in LAPACK driver GELSD, Y or S, and comparing the times on those. And so I got that GELSY was I guess about twice as fast as GELSD. And notice it defaults to GELSD, so that could, you know, in this case maybe you would want to override it and use GELSY, but it's only a 2x speedup. And I actually don't know what's going on here, but it's kind of interesting to me that GELSD is slower than GLSS, which is what it had said is like the older version that's generally slow. And these things would all depend on what problem you're doing, so if you did this on a different problem, you might get that a different method was faster. All right, so now we're going to kind of step back. So that was still using SciPy's implementation. We'll step back and kind of remember the definition of the least squares problem is we're wanting to minimize AX minus B and trying to find the best values for X to do that. Another way to think about this in linear algebra is that we're interested in where vector B is closest to the subspace spanned by A, which is called the range of A. And you can kind of think of this as the projection of B onto A, because if you think about all possible vectors X, multiplying those by A gives you the range of A, you know, any possible, yeah, any vector that could be in the range, and you're trying to find the one that's closest to B. And so that can be expressed as B minus, so B minus AX must be perpendicular to the subspace spanned by A. That's kind of getting from B to AX, since it's a projection, would be the perpendicular. Let me, maybe I should go to the drawing pad for that. Oh, I actually wanted the other one. So if we think of AX, really we've got a plane where this is A times X for all X's, would give us some, you know, two-dimensional plane. And then we've got some, so that was a bad angle, some vector B, we're trying to find the X to minimize AX minus B. And so that's the projection of B onto AX, and that would, yeah, we could write, oh, let's write this as AX hat is in that plane. And so then this is where, so this line here, you know, is B minus AX hat, and that must be perpendicular to A. This is the whole plane. And I'll show, I'm going to show another three blue, one brown video in a little while. So this is kind of getting back to a more geometric way of thinking about matrices and vectors. Any questions here? Okay, so going back to the notebook, you know, we end up with A transpose times B minus AX hat must be zero. And this is where the normal equations come from. This is kind of the closed form solution for linear regression least squares problem. And so we could, we could do this, I call this least squares naive. This is an approach that you would not actually want to use in practice because you're taking a matrix inverse, which you generally want to, really want to avoid doing. But I just kind of did this as a starting point of this is something that would work, assuming, I guess it would work assuming that A is not something we're going to end up dividing by a number close to zero to get its inverse. Matthew? And who has the microphone? Nice. Yeah, so it's both, it's both computationally expensive and it can also be unstable because you end up dividing by things. Yeah. Good question. So here this was just kind of a first stab at implementing something that would give us an answer for linear least squares regression. Alright, so then coming off of that, so these are called the normal equations, this equation we got above. And really this is, this equation has been rewritten and we're just taking A transpose B, or I guess we're taking A transpose AX to the other side. So we have A transpose AX equals A transpose B. And so if A is full rank, A transpose A is going to give us a, if A is square, well actually not even square, if A is full rank, we're going to get a square Hermitian positive definite matrix for A transpose A. And square Hermitian positive definite matrices have something called a Cholesky decomposition. And the Cholesky decomposition, or Cholesky factorization finds an upper triangular matrix R such that R transpose times R equals your matrix. And what would R, so if R is upper triangular, what would R transpose be? Yes, lower triangular, that's correct. Not all questions I ask are hard. So, so what we're doing, so in this case we're looking at A transpose A, we're going to take the Cholesky factorization and that'll give us back R such that A transpose A is R transpose times R. A transpose times A equals R transpose times R. Here I've just pre-computed A transpose A, since I'm going to be using it a few times, as well as A transpose B. And then I'm going to use SciPy's implementation of the Cholesky factorization as a warning. NumPy and SciPy default to different, one of them returns the upper triangular matrix and one returns the lower triangular matrix, which I discovered the hard way. I didn't realize I was switching between NumPy and SciPy and was like using this in other operations and getting different answers. Yeah, so we can, and so it's always good after you do this to kind of check that what you're doing makes sense. So I'm passing A transpose A in, and actually let's even look at, oh, okay, that's difficult to see. So remember, np.setprintoptions is super useful. You can do suppress equals true to not show zeros in this format with an exponential, 0.0000, which is not very readable. And then you can also set the number of digits you want to see. Although this matrix, I guess, is still kind of too large to be that viewable. But I can believe that this looks like an upper triangular matrix that I've gotten back. And then I'm just confirming that A transpose A minus r dot transpose times r is giving me something close to 0. So A transpose A is approximately equal to r dot transpose r. And so then the way that the Cholesky factorization is useful in solving the linear least squares problem is so we can plug it in. Now we have our r instead of A. Then we have r transpose, and here I'm calling rx, saying that that equals w, which you can see in this last line, setting that equals to A transpose B, and getting rx equals w. So why is this helpful? I've just kind of written this equation a few different ways. Why might this be better than just sticking with our A transpose A, x equals A transpose B? Oh wait, Matthew can throw it. So now it's much easier to solve the last equation, because the r is like a triangular matrix. Exactly. So that's it. Solving linear systems of equations for a triangular matrix is a lot easier, because you just start with the last entry, divide by one thing, then you plug into the second to last entry, and so on. So it's much simpler than solving a linear system where you have a non-triangular matrix. And so we've kind of just changed this into having to solve two triangular systems of equations. So we solve, I guess we would solve this one first, define w, and then we solve the one on the last line to find x, which is our original answer. This I think kind of illustrates a lot of numerical linear algebra is about getting these factorizations, and often it looks like, okay, you've just rewritten the problem, you know, you still have all these factors, but it's because of the properties of the matrices you've factored into, you've made things easier for yourself in some way. Any questions about this, or questions about why it's easier to solve a triangular system of equations? Okay. So yeah, this is just kind of rewritten, the least squares linear regression problem. So here kind of working our way, working our way towards x, first we're going to solve the triangular system, our transpose w equals a transpose b, which is what's happening in this line. And so SciPy, Lin-Aoj has a solve triangular method, even, where you give it a triangular matrix, and that has a transpose argument. So here we're letting it know this is our transpose. Again, we're kind of checking that our result is what we were hoping, and that our transpose times w minus a transpose b is close to zero. So we've successfully solved this third equation. And then from here we solve triangular r with w on the right in order to get x, which I called a coefficients Cholesky here. And we got that that works. And so I can put this all together in a method and say that solving least squares with the Cholesky decomposition takes in a and b, gets the Cholesky decomposition of a transpose times a, and then it solves those two triangular systems and returns the answer. Any questions? Kelsey, can you throw the microphone? Why was this slower than the naive way? Yeah, that's a good question. So I think here, and I have to look this up, that's probably the cost of calculating the Cholesky decomposition. And so I'll check on that with you. I'll put this down. I think it's also to remember the naive way you have this instability that comes with getting the inverse, so that's kind of less good in that regard. But yeah, I'll check about the time. Is there some sort of order of the size of your matrix? Yes, yeah, I'm sure that it, like this is small enough that I think constant terms matter more. So when you get to really huge matrices, it does come about, become more about the highest order term. But yeah, the size could matter. And the other question is, so this is not what Scikit was doing? Correct. And why? So we'll come back to kind of a comparison. So I'm going to kind of show you several different ways to do it, and then we'll like compare the different methods. Got good questions? Any other questions about this approach? Okay. And then I think I brought this up last week when I was looking at the history of Gaussian elimination, but the Cholesky decomposition is the one that Gauss first found that Cholesky got the credit for. Oh, yes. Yeah, you can always walk it too. Thank you. Yeah, so I see this slide. Let me back up. We actually get two different mp.linionlog.norm. We have different value here. One is 1.13. One is 6.86. So what has changed in these results? So these are so close to zero that it doesn't... Like here I'm mostly trying to confirm that this is a number close to zero, and I'm interested in the exponent and wanting it to be, you know, 10 to the negative 14th is close to zero. Although I guess that is a question. Why aren't we getting exactly zero if this is a way to decompose and get the answer? All right. I mean, does anyone have an answer to this on why we're not getting it? Yeah, because there's... So machine epsilon, because we get rounding errors and with floating point numbers, we're trying to represent these continuous numbers with discrete values. Another question is, suppose A is actually invertible. So we normally find the best coefficient by, for example, gradient descent, right? But here we are actually trying to solve a linear algebra solution. So which one is usually faster? So when you bring up gradient descent, are you talking about like a stochastic gradient descent approach? So stochastic, I actually originally have had stochastic gradient descent in this lesson with PyTorch, and it was actually much slower than any of these linear algebra approaches, and so I took it out because it just didn't look reasonable compared to them. A benefit, so yeah, SGD will often be slower than if you have, you know, kind of like a well-studied method for your particular problem. But a benefit of SGD is that it's so general, and so if you have a problem where you don't have a closed form solution or, you know, people haven't studied it to come up with this optimization method, you can use SGD there. So kind of the benefit of SGD is, yeah, that it's so general and that it'll work on problems where we don't have closed form solutions. Connor? And can you throw the microphone? Or pass the microphone? But I thought that if we have, say, like, say our dimensionality is really, really, really large, then the closed form solution takes a while in comparison. I don't know. I was just reading somewhere. Yeah, no, and that's quite possible too, yeah, that at a certain size it might be worth it. Yeah, and that's an important note, I think, with all of these that the answer of what algorithm is best can really depend on your problem. And we'll see that later. And so different things like both the size of your problem, kind of properties around the stability of your problem. Yeah, we'll see later two methods that kind of, or two particular matrices that don't work well for a particular method. Yeah, so there's a lot of variation with these. Yeah, these are good questions. Any other questions about Cholesky factorization before we go on to our next approach? All right, so next up is the QR factorization. And this, again, next week we will actually talk about how to code a QR factorization. We're still just using NumPy's. Can someone remind me what the QR decomposition gives you? No, although you have the right words in there. I think Kelsey has a guess. I think it's an upper triangular one and then one that's orthogonal column vectors that are orthogonal to normal as well, or just a box. I believe, so if it's square, they would be orthonormal. But yeah, I guess for the non-square case, I would have to check. But yeah, so Q is orthonormal, R is upper triangular. Thank you. And where did we see the QR factorization before? I'm not sure that we saw it with NMF. Yeah, so inside of randomized SVD, we saw it. Okay, so here, so kind of getting into these equations. And we, yeah, next week we'll talk about how to code a QR factorization ourselves. So AX equals B. We're going to get the QR factorization for A. So A equals QR, plug that in, and then say RX equals Q transpose B. And why was I able to go from Q on the left side to a Q transpose on the right side? Yeah, so a few people shouted this out. Q is orthonormal. So multiplying by Q transpose, we get the identity on the left. And that's also nice because you're not having to calculate an inverse, you're just transposing the columns. And then why is it nice to have RX equals Q transpose B? Oh, can you grab the microphone? You want to solve for X, R is upper triangular, so it's easier to solve. Exactly, yeah. So now we have a linear system of equations with a triangular matrix that'll be fast to solve. So here we do QR equals, using SciPy's QR factorization, and then we get a triangular system to solve. We solve this, and we're getting the same error kind of that we've been getting. Questions about this approach with the QR factorization? The question is, why is this one slower? Let's actually save the timing comparison ones for a moment, but yeah, we'll come back to that. Any other questions before we start our next approach? Okay, so now yet another approach. We're going to look at SBD. And so again, we're still trying to solve the same problem of AX equals B and finding the optimal X. Now we're going to use the singular value decomposition. Can someone remind us what the singular value decomposition is? Tim? So the full SVDs, you decompose the matrix into a product of U, sigma, V transpose, I think? Yes, yeah. So U and V are orthogonal matrices, and sigma contains a block of a square matrix where the diagonals are your singular values. Exactly, yeah. So sigma is a diagonal matrix. Everything's zero except the diagonals, and those are the singular values, and U and V are orthonormal. Sigma doesn't have to be square though, right? In the full SVD? So they talk about \u2013 I didn't cover this in class. They talk about like a full SVD and also a reduced SVD, and one of them \u2013 yeah, and one of them it's not square, but you're just adding a bunch of zeros to get the dimensions to work out. Yeah, and actually I guess \u2013 I think for the reduced one, really it's just that U has orthonormal rows and V has orthonormal columns, and then for the full version you're like making those \u2013 expanding them so that they're full bases. So I guess in the full U and V are square? Yes, yeah. In the full SVD, the U and the V are square matrices. And I'll bring \u2013 Trevathan has a nice picture of this. Yeah. Okay, I'll talk about that next time, full versus reduced for SVD. Okay, yeah, so that's the SVD, which we've seen a few times before. Here we plug that in for A, and then actually similar to what we did before, we can multiply each side by U transpose, since U transpose times U is the identity because U has orthonormal rows. We get sigma Vx equals U transpose b, then we have sigma W equals U transpose b, x equals V transpose w. And so this \u2013 I actually think you should kind of be seeing some common themes between these three different approaches. Well, actually, I guess first I should say what's nice about solving sigma W equals U transpose b? Does anyone else want to answer? And so here we've just said Vx equals w, and now we're going to solve sigma W equals U transpose b. Roger? Tim, can you pass the microphone? Thanks. So it's diagonal, so all you have to do is provide to get the w. Exactly, yeah. So solving a diagonal system of equations is even better than solving a triangular system because you just have a single non-zero coefficient for each equation, so you can just divide through. So that's, yeah, very quick to solve. And then we're left with, well, Vx equals w. Since V is orthonormal, we can rewrite that x equals V transpose w. And then this is that in equation form. So we're using psi-pi's SVD here, getting back U sigma, and I called it VH in this case. Then I see for w, notice I'm just dividing by sigma, dividing U transpose b by sigma. And here, I guess, U transpose is possibly larger than I need, so that's why I'm chopping off the end. And then I'm returning VH transpose times w. Questions about this approach? Yeah, some people are noticing that this is much slower. We've gone from microseconds to milliseconds, which is an order of thousand-time difference. Any questions or questions about the equations, kind of how we solved this? Okay, and then we had another technique that I'm not going to get into detail at all on, but wanted to let you know that it's out there, is that there's something called the random sketching technique for linear regression, which involves using a random matrix and computing SA equals SB, and then finding a solution to the regression SA x equals SB. So I just wanted to let you know that that exists as an example of a randomized approach. And this would be where you had a very large A and B. Okay, so now I think you will like this part. We're going to do a timing comparison and do it a little bit more formally. So we were just kind of printing out what the times were for the diabetes data set. What I'm going to do here is randomly generate some data, and M is going to be either 100, 1000, or 10,000. N is going to be 20, 100, or 1000. You can see down here is where I generate the data. Randomly, A has dimensions M by N, and I'm going to loop through all possible values of M and N. And then we're going to compare these different techniques. So we've got the naive solution with the normal equations, the Cholesky factorization, the QR factorization, SBD, and then also SciPy's least squares. Let me see if there's anything else about the setup. Oh, in SciPy's least squares, it returns several items, but we just wanted the first one. So that's why I've kind of wrapped it in this other method. And here I'm using, so I've got my row names, and then I have a dictionary that converts the name, kind of lists what the method is that we've defined for that. Who in here has used pandas before? Pandas, plural. Yeah. Okay. So it looks like everybody. And I'm just using it here because the data frame is kind of a nice way to store a table of information. And I'm using the multi-index, which can be handy to have M and N. So now I just have a nested for loop. And this is kind of formally for the linear least squares problem. You want M greater than or equal to N, like you typically have more data points than you have features. So I'm just looking at those cases. What I'm going to do is loop through them, randomly generate data of that size. So with M rows and N columns. And then for each method, I am going to get the function. I'm going to use Python's time it module to time it. And what this will do is run the decomposition five times and track the speed of each one and average them. And it's good to run it a few times just to kind of reduce variation some. Then for the data frame, I'm going to set the value of that kind of that method for that row and column. This is how long it took. Oh, and then I'm also I'm interested in the air, not just the speed. So I've got the coefficients back from this would be. And this is a little tricky. So function is a string. And so time it, you actually pass a string to here. We're using locals to kind of look up the actual variable with that name. Call that on A and B and then check what the air is using our metrics coefficients. And in this case, for simplicity, I decided just to look at the L2 air. So that's what I'm storing in my DF air data frame. So I've got this DF data frame that's going to hold the time. DF air is going to hold the air. And so I run that and that can be a little bit slow to run. So I'm just going to use the results I have here since it's doing kind of all these matrix decompositions and some of the matrices are larger. So let me see if this will fit on the screen when it's larger. Are you able to read this this table? All right. We should put them both up. So the top table. This like perfectly fits. Top table is the time it takes and the second table is the air. So I'm going to spend a moment just kind of looking at this and tell me what you notice of which methods are working best, kind of in which cases. So anyone want to share some observations? And then microphones between Aaron and Roger. Someone throw it to Connor. Like across the board in terms of timing, the Cholesky factorization does pretty well. It does. Yeah. Yeah. And the Cholesky factorization does, I thought, surprisingly well. And these are there are much larger matrices out there that we could have done this on. Any other observations? I have to say, I was I was personally surprised by how slow SVD is. What do you notice about the air? Yeah, it's the same in all of these. So that's reassuring. So here this problem is not not one where we're having to worry about kind of stability with different different ones. Matthew, can you throw the microphone? How would SVD get slower than that? Oh, that's a good question. Yeah, I'm not sure about that. I'll think about that more. That's a good question. So here I just used random uniform. Oh, yeah, maybe it was like an unlucky draw. Yeah, it's random for each one. And I also wanted to kind of share this as a because I do want to highlight that Cholesky is fast. Yeah, Cholesky is fast. A lot of these. I want to highlight this as a technique for comparing methods. I think that this can be kind of nice to do this kind of like looping through and getting getting data on how fast things are in a systematic manner. And it also I think makes it a lot easier to compare as opposed to looking, you know, having to scroll through like different places in the notebook when we've just output it one line at a time. So I think this is a good kind of good general purpose method for comparing comparing methods. And then I just wanted to note there's also an L1 regression where you're using the L1 norm for your your error instead. But we're not going to go into detail on that. I'm going to take a moment to talk about conditioning and stability. So condition number is a measure of how small changes in the input affect the output. Why would we care about what happens when you have small changes in the input? Matthew. That's true. I mean, so there is when you're using real world data. Yeah, there's going to be this noisiness. What's another reason that we're particularly interested in numerical in your algebra in this question? Sam. So if you have something that is. The thing that comes to mind is the almost the right answer to almost the right question. So if you do something to your matrix and it is slightly reconstructed slightly differently than you wanted to not have a huge effect on whatever you do next to it. That that is true. Yeah. And what's that? What kind of what's the cause, though, of why we would be getting something that's just almost the right answer to almost the right question? As opposed to having always exactly the right answer. Exactly, because of floating point errors. So since with computers kind of having machine epsilon and floating point error, you're almost never having exactly what you want there. You know, you have these tiny differences. It's important to know how do small small differences affect the output. And then as Sam was getting at this can also kind of propagate through your problem if you're doing lots of operations and you've got just a little bit of error each time that could really add up. So the relative condition number is defined as and this is just looking at the change in the output divided by change in the input for very tiny changes. And then Trevathan, I like that he put actual numbers on this on page 91 says a problem is well conditioned if kappa the condition number is small, such as 110 or 100 and ill conditioned if kappa is large, such as that's a lot tech error typo in my part, such as 10 to the sixth or 10 to the 16th. And then this is kind of just a technical definition conditioning relates to the problem. So just least squares where stability is what's used to talk about the particular algorithm. And so an example that we saw before was computing eigenvalues. Here we have two matrices, A is 1, 1000, 01, B is 1, 1000,.0011. So these are very close that they only differ in the bottom left entry, which is zero for A and.001 for B. When we get the eigenvalues, the eigenvalues for A are 1 and 1 and the eigenvalues for B are 2 and then this is practically zero. But 1, 1 and 2, 0. That's like a huge difference. And this is something that this isn't the computer's fault. This is something about the problem of finding an eigenvalue is that for relatively small change, we're getting a very different different answer. Are there questions about this idea of conditioning or of whether our problem is kind of well or ill conditioned? And then just even to kind of tie this back into this definition up here, the change in the solution here, we went from 1 to 2 and 1 to 0 for our two answers over the change in X, which was.001. That would be like 1 divided by.001, 1000. And that could actually get larger if we're doing smaller changes. We're going to have that this is kind of a poorly conditioned problem. Tim? Oh, Sam, can you pass the catch box to Tim? Oh, yeah. Okay, great. Okay, thank you. Yeah. Yeah. And that's like if you wanted to do an even more scientific comparison, you could probably randomly generate several different matrices in average. Yeah. Yeah. Thank you, Tim. That's helpful. Okay, so then in talking about this is another LaTeX typo. So the product norm of A times norm of A inverse comes up a lot when you're like it appears in several different theorems around conditioning of problems. And so it has its own name, the condition number of A. And so we've previously kind of talked about problems being well conditioned. Here we're talking about a matrix. But the condition number of A relates to it shows that both. And I'm not going to give the full theorems because they kind of have more technical details than I want to get into. But computing B given A and X and AX equals B or computing X given A and B, the condition number is related to how well conditioned that problem is. So this quantity of norm of A times norm of A inverse is something you'll see. I think we're about at time. We'll go over this kind of conditioning next time. And then as a reminder, I think everyone knows this, that homework two was due today as well as well as the draft of the blog post.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 17.16, "text": " Okay, I'm going to go ahead and get started.", "tokens": [1033, 11, 286, 478, 516, 281, 352, 2286, 293, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.26866699667537913, "compression_ratio": 1.2272727272727273, "no_speech_prob": 0.10361028462648392}, {"id": 1, "seek": 0, "start": 17.16, "end": 25.28, "text": " I wanted to just briefly return to the problem of the compressed sensing of reconstructing", "tokens": [286, 1415, 281, 445, 10515, 2736, 281, 264, 1154, 295, 264, 30353, 30654, 295, 31499, 278], "temperature": 0.0, "avg_logprob": -0.26866699667537913, "compression_ratio": 1.2272727272727273, "no_speech_prob": 0.10361028462648392}, {"id": 2, "seek": 2528, "start": 25.28, "end": 31.68, "text": " what the CT scan had detected, and so kind of thinking about Tim's question from last", "tokens": [437, 264, 19529, 11049, 632, 21896, 11, 293, 370, 733, 295, 1953, 466, 7172, 311, 1168, 490, 1036], "temperature": 0.0, "avg_logprob": -0.20112359523773193, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.00011955020454479381}, {"id": 3, "seek": 2528, "start": 31.68, "end": 38.72, "text": " time of how would we represent this, so kind of as linear regression.", "tokens": [565, 295, 577, 576, 321, 2906, 341, 11, 370, 733, 295, 382, 8213, 24590, 13], "temperature": 0.0, "avg_logprob": -0.20112359523773193, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.00011955020454479381}, {"id": 4, "seek": 2528, "start": 38.72, "end": 45.56, "text": " So here if linear regression is matrix X of data times a vector beta of coefficients equals", "tokens": [407, 510, 498, 8213, 24590, 307, 8141, 1783, 295, 1412, 1413, 257, 8062, 9861, 295, 31994, 6915], "temperature": 0.0, "avg_logprob": -0.20112359523773193, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.00011955020454479381}, {"id": 5, "seek": 2528, "start": 45.56, "end": 48.68000000000001, "text": " some vector Y.", "tokens": [512, 8062, 398, 13], "temperature": 0.0, "avg_logprob": -0.20112359523773193, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.00011955020454479381}, {"id": 6, "seek": 4868, "start": 48.68, "end": 55.08, "text": " Vector X is the projection, they call it the projection operator, and it's this matrix", "tokens": [691, 20814, 1783, 307, 264, 22743, 11, 436, 818, 309, 264, 22743, 12973, 11, 293, 309, 311, 341, 8141], "temperature": 0.0, "avg_logprob": -0.3073004768008277, "compression_ratio": 1.50625, "no_speech_prob": 2.3549919205834158e-05}, {"id": 7, "seek": 4868, "start": 55.08, "end": 61.8, "text": " that you can see is black and then has some white lines that seem to have a little bit", "tokens": [300, 291, 393, 536, 307, 2211, 293, 550, 575, 512, 2418, 3876, 300, 1643, 281, 362, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.3073004768008277, "compression_ratio": 1.50625, "no_speech_prob": 2.3549919205834158e-05}, {"id": 8, "seek": 4868, "start": 61.8, "end": 63.76, "text": " of a pattern to them.", "tokens": [295, 257, 5102, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.3073004768008277, "compression_ratio": 1.50625, "no_speech_prob": 2.3549919205834158e-05}, {"id": 9, "seek": 4868, "start": 63.76, "end": 70.03999999999999, "text": " Does anyone know where these are coming from?", "tokens": [4402, 2878, 458, 689, 613, 366, 1348, 490, 30], "temperature": 0.0, "avg_logprob": -0.3073004768008277, "compression_ratio": 1.50625, "no_speech_prob": 2.3549919205834158e-05}, {"id": 10, "seek": 7004, "start": 70.04, "end": 87.44000000000001, "text": " Tim?", "tokens": [7172, 30], "temperature": 0.0, "avg_logprob": -0.4560572017322887, "compression_ratio": 1.1274509803921569, "no_speech_prob": 2.4298904463648796e-05}, {"id": 11, "seek": 7004, "start": 87.44000000000001, "end": 90.64000000000001, "text": " Where the lines hit through those circle things?", "tokens": [2305, 264, 3876, 2045, 807, 729, 6329, 721, 30], "temperature": 0.0, "avg_logprob": -0.4560572017322887, "compression_ratio": 1.1274509803921569, "no_speech_prob": 2.4298904463648796e-05}, {"id": 12, "seek": 7004, "start": 90.64000000000001, "end": 95.44000000000001, "text": " Close, so you're actually getting a little bit ahead of this.", "tokens": [16346, 11, 370, 291, 434, 767, 1242, 257, 707, 857, 2286, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.4560572017322887, "compression_ratio": 1.1274509803921569, "no_speech_prob": 2.4298904463648796e-05}, {"id": 13, "seek": 9544, "start": 95.44, "end": 104.0, "text": " So this is just the matrix X, but we'll get there soon.", "tokens": [407, 341, 307, 445, 264, 8141, 1783, 11, 457, 321, 603, 483, 456, 2321, 13], "temperature": 0.0, "avg_logprob": -0.44685959234470274, "compression_ratio": 1.1981132075471699, "no_speech_prob": 1.5205734598566778e-05}, {"id": 14, "seek": 9544, "start": 104.0, "end": 106.08, "text": " Matthew?", "tokens": [12434, 30], "temperature": 0.0, "avg_logprob": -0.44685959234470274, "compression_ratio": 1.1981132075471699, "no_speech_prob": 1.5205734598566778e-05}, {"id": 15, "seek": 9544, "start": 106.08, "end": 113.8, "text": " You mean like the readings?", "tokens": [509, 914, 411, 264, 27319, 30], "temperature": 0.0, "avg_logprob": -0.44685959234470274, "compression_ratio": 1.1981132075471699, "no_speech_prob": 1.5205734598566778e-05}, {"id": 16, "seek": 9544, "start": 113.8, "end": 116.8, "text": " So it's actually not the readings.", "tokens": [407, 309, 311, 767, 406, 264, 27319, 13], "temperature": 0.0, "avg_logprob": -0.44685959234470274, "compression_ratio": 1.1981132075471699, "no_speech_prob": 1.5205734598566778e-05}, {"id": 17, "seek": 11680, "start": 116.8, "end": 133.68, "text": " The value of the, well I wanted to say readings, the value at a given angle and whatever step.", "tokens": [440, 2158, 295, 264, 11, 731, 286, 1415, 281, 584, 27319, 11, 264, 2158, 412, 257, 2212, 5802, 293, 2035, 1823, 13], "temperature": 0.0, "avg_logprob": -0.3248408317565918, "compression_ratio": 1.4109589041095891, "no_speech_prob": 7.766688213450834e-06}, {"id": 18, "seek": 11680, "start": 133.68, "end": 139.56, "text": " So what you're getting at, I'll actually skip ahead, is the vector Y, so here, the density", "tokens": [407, 437, 291, 434, 1242, 412, 11, 286, 603, 767, 10023, 2286, 11, 307, 264, 8062, 398, 11, 370, 510, 11, 264, 10305], "temperature": 0.0, "avg_logprob": -0.3248408317565918, "compression_ratio": 1.4109589041095891, "no_speech_prob": 7.766688213450834e-06}, {"id": 19, "seek": 11680, "start": 139.56, "end": 140.56, "text": " readings.", "tokens": [27319, 13], "temperature": 0.0, "avg_logprob": -0.3248408317565918, "compression_ratio": 1.4109589041095891, "no_speech_prob": 7.766688213450834e-06}, {"id": 20, "seek": 11680, "start": 140.56, "end": 141.56, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3248408317565918, "compression_ratio": 1.4109589041095891, "no_speech_prob": 7.766688213450834e-06}, {"id": 21, "seek": 14156, "start": 141.56, "end": 148.52, "text": " So yeah, this is kind of the measurements that the CT scan was recording, and what we", "tokens": [407, 1338, 11, 341, 307, 733, 295, 264, 15383, 300, 264, 19529, 11049, 390, 6613, 11, 293, 437, 321], "temperature": 0.0, "avg_logprob": -0.20899317286036037, "compression_ratio": 1.6736401673640167, "no_speech_prob": 2.4299397409777157e-05}, {"id": 22, "seek": 14156, "start": 148.52, "end": 153.4, "text": " do to get our vector Y, so this is the right hand side of the equation, the linear regression,", "tokens": [360, 281, 483, 527, 8062, 398, 11, 370, 341, 307, 264, 558, 1011, 1252, 295, 264, 5367, 11, 264, 8213, 24590, 11], "temperature": 0.0, "avg_logprob": -0.20899317286036037, "compression_ratio": 1.6736401673640167, "no_speech_prob": 2.4299397409777157e-05}, {"id": 23, "seek": 14156, "start": 153.4, "end": 159.32, "text": " is we're just unwrapping this, unraveling it, is the NumPy operation, to get this really", "tokens": [307, 321, 434, 445, 14853, 424, 3759, 341, 11, 40507, 278, 309, 11, 307, 264, 22592, 47, 88, 6916, 11, 281, 483, 341, 534], "temperature": 0.0, "avg_logprob": -0.20899317286036037, "compression_ratio": 1.6736401673640167, "no_speech_prob": 2.4299397409777157e-05}, {"id": 24, "seek": 14156, "start": 159.32, "end": 166.68, "text": " one, and a tall vector that's 2000 by one.", "tokens": [472, 11, 293, 257, 6764, 8062, 300, 311, 8132, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.20899317286036037, "compression_ratio": 1.6736401673640167, "no_speech_prob": 2.4299397409777157e-05}, {"id": 25, "seek": 14156, "start": 166.68, "end": 171.24, "text": " That's why, any other guesses of, I know this is kind of a tricky question, what this X", "tokens": [663, 311, 983, 11, 604, 661, 42703, 295, 11, 286, 458, 341, 307, 733, 295, 257, 12414, 1168, 11, 437, 341, 1783], "temperature": 0.0, "avg_logprob": -0.20899317286036037, "compression_ratio": 1.6736401673640167, "no_speech_prob": 2.4299397409777157e-05}, {"id": 26, "seek": 17124, "start": 171.24, "end": 172.24, "text": " could be?", "tokens": [727, 312, 30], "temperature": 0.0, "avg_logprob": -0.2878788363549017, "compression_ratio": 1.4206896551724137, "no_speech_prob": 0.00012147166125942022}, {"id": 27, "seek": 17124, "start": 172.24, "end": 173.24, "text": " Matthew?", "tokens": [12434, 30], "temperature": 0.0, "avg_logprob": -0.2878788363549017, "compression_ratio": 1.4206896551724137, "no_speech_prob": 0.00012147166125942022}, {"id": 28, "seek": 17124, "start": 173.24, "end": 174.24, "text": " Good catch.", "tokens": [2205, 3745, 13], "temperature": 0.0, "avg_logprob": -0.2878788363549017, "compression_ratio": 1.4206896551724137, "no_speech_prob": 0.00012147166125942022}, {"id": 29, "seek": 17124, "start": 174.24, "end": 188.04000000000002, "text": " Is it the reading or are the results of the image from different angles?", "tokens": [1119, 309, 264, 3760, 420, 366, 264, 3542, 295, 264, 3256, 490, 819, 14708, 30], "temperature": 0.0, "avg_logprob": -0.2878788363549017, "compression_ratio": 1.4206896551724137, "no_speech_prob": 0.00012147166125942022}, {"id": 30, "seek": 17124, "start": 188.04000000000002, "end": 193.96, "text": " Not yet, so that's why, kind of the reading from the different angles.", "tokens": [1726, 1939, 11, 370, 300, 311, 983, 11, 733, 295, 264, 3760, 490, 264, 819, 14708, 13], "temperature": 0.0, "avg_logprob": -0.2878788363549017, "compression_ratio": 1.4206896551724137, "no_speech_prob": 0.00012147166125942022}, {"id": 31, "seek": 17124, "start": 193.96, "end": 198.72, "text": " And Sam behind you has a guess.", "tokens": [400, 4832, 2261, 291, 575, 257, 2041, 13], "temperature": 0.0, "avg_logprob": -0.2878788363549017, "compression_ratio": 1.4206896551724137, "no_speech_prob": 0.00012147166125942022}, {"id": 32, "seek": 19872, "start": 198.72, "end": 205.08, "text": " Is it the total intensity, like before when we looked at the projection operator at just", "tokens": [1119, 309, 264, 3217, 13749, 11, 411, 949, 562, 321, 2956, 412, 264, 22743, 12973, 412, 445], "temperature": 0.0, "avg_logprob": -0.21905719803040286, "compression_ratio": 1.5729166666666667, "no_speech_prob": 3.480467785266228e-05}, {"id": 33, "seek": 19872, "start": 205.08, "end": 212.64, "text": " one slice, there's a line going through a space, but now are we looking at, like, is", "tokens": [472, 13153, 11, 456, 311, 257, 1622, 516, 807, 257, 1901, 11, 457, 586, 366, 321, 1237, 412, 11, 411, 11, 307], "temperature": 0.0, "avg_logprob": -0.21905719803040286, "compression_ratio": 1.5729166666666667, "no_speech_prob": 3.480467785266228e-05}, {"id": 34, "seek": 19872, "start": 212.64, "end": 219.76, "text": " the X dimension the position and Y is the angle and then the dot is like the length", "tokens": [264, 1783, 10139, 264, 2535, 293, 398, 307, 264, 5802, 293, 550, 264, 5893, 307, 411, 264, 4641], "temperature": 0.0, "avg_logprob": -0.21905719803040286, "compression_ratio": 1.5729166666666667, "no_speech_prob": 3.480467785266228e-05}, {"id": 35, "seek": 19872, "start": 219.76, "end": 221.36, "text": " of the line almost?", "tokens": [295, 264, 1622, 1920, 30], "temperature": 0.0, "avg_logprob": -0.21905719803040286, "compression_ratio": 1.5729166666666667, "no_speech_prob": 3.480467785266228e-05}, {"id": 36, "seek": 19872, "start": 221.36, "end": 222.36, "text": " That's very close.", "tokens": [663, 311, 588, 1998, 13], "temperature": 0.0, "avg_logprob": -0.21905719803040286, "compression_ratio": 1.5729166666666667, "no_speech_prob": 3.480467785266228e-05}, {"id": 37, "seek": 19872, "start": 222.36, "end": 223.36, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.21905719803040286, "compression_ratio": 1.5729166666666667, "no_speech_prob": 3.480467785266228e-05}, {"id": 38, "seek": 22336, "start": 223.36, "end": 234.64000000000001, "text": " So let me kind of go back up to, I think it was this section where we had these pictures", "tokens": [407, 718, 385, 733, 295, 352, 646, 493, 281, 11, 286, 519, 309, 390, 341, 3541, 689, 321, 632, 613, 5242], "temperature": 0.0, "avg_logprob": -0.1776724238144724, "compression_ratio": 1.7445652173913044, "no_speech_prob": 8.939538929553237e-06}, {"id": 39, "seek": 22336, "start": 234.64000000000001, "end": 239.8, "text": " of lines at different angles in different locations.", "tokens": [295, 3876, 412, 819, 14708, 294, 819, 9253, 13], "temperature": 0.0, "avg_logprob": -0.1776724238144724, "compression_ratio": 1.7445652173913044, "no_speech_prob": 8.939538929553237e-06}, {"id": 40, "seek": 22336, "start": 239.8, "end": 245.04000000000002, "text": " And so this was part of a, so this was originally part of a four dimensional matrix where we", "tokens": [400, 370, 341, 390, 644, 295, 257, 11, 370, 341, 390, 7993, 644, 295, 257, 1451, 18795, 8141, 689, 321], "temperature": 0.0, "avg_logprob": -0.1776724238144724, "compression_ratio": 1.7445652173913044, "no_speech_prob": 8.939538929553237e-06}, {"id": 41, "seek": 22336, "start": 245.04000000000002, "end": 250.68, "text": " had the different angles and the positions, which is the vertical position of how high", "tokens": [632, 264, 819, 14708, 293, 264, 8432, 11, 597, 307, 264, 9429, 2535, 295, 577, 1090], "temperature": 0.0, "avg_logprob": -0.1776724238144724, "compression_ratio": 1.7445652173913044, "no_speech_prob": 8.939538929553237e-06}, {"id": 42, "seek": 25068, "start": 250.68, "end": 255.04000000000002, "text": " up the line is and then an X-Y coordinate for each.", "tokens": [493, 264, 1622, 307, 293, 550, 364, 1783, 12, 56, 15670, 337, 1184, 13], "temperature": 0.0, "avg_logprob": -0.14828174415676074, "compression_ratio": 1.631578947368421, "no_speech_prob": 9.080224117496982e-06}, {"id": 43, "seek": 25068, "start": 255.04000000000002, "end": 260.48, "text": " And so here we were kind of just showing pictures where we had index on the first and second", "tokens": [400, 370, 510, 321, 645, 733, 295, 445, 4099, 5242, 689, 321, 632, 8186, 322, 264, 700, 293, 1150], "temperature": 0.0, "avg_logprob": -0.14828174415676074, "compression_ratio": 1.631578947368421, "no_speech_prob": 9.080224117496982e-06}, {"id": 44, "seek": 25068, "start": 260.48, "end": 266.52, "text": " dimensions, so the angle and a vertical position and then see the X-Y for all these different", "tokens": [12819, 11, 370, 264, 5802, 293, 257, 9429, 2535, 293, 550, 536, 264, 1783, 12, 56, 337, 439, 613, 819], "temperature": 0.0, "avg_logprob": -0.14828174415676074, "compression_ratio": 1.631578947368421, "no_speech_prob": 9.080224117496982e-06}, {"id": 45, "seek": 25068, "start": 266.52, "end": 267.52, "text": " lines.", "tokens": [3876, 13], "temperature": 0.0, "avg_logprob": -0.14828174415676074, "compression_ratio": 1.631578947368421, "no_speech_prob": 9.080224117496982e-06}, {"id": 46, "seek": 25068, "start": 267.52, "end": 275.0, "text": " And so this mystery matrix X that I was showing you is actually the collection of all of these.", "tokens": [400, 370, 341, 11422, 8141, 1783, 300, 286, 390, 4099, 291, 307, 767, 264, 5765, 295, 439, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.14828174415676074, "compression_ratio": 1.631578947368421, "no_speech_prob": 9.080224117496982e-06}, {"id": 47, "seek": 27500, "start": 275.0, "end": 281.84, "text": " So this is kind of what the X-rays look like before they've intersected with anything.", "tokens": [407, 341, 307, 733, 295, 437, 264, 1783, 12, 36212, 574, 411, 949, 436, 600, 27815, 292, 365, 1340, 13], "temperature": 0.0, "avg_logprob": -0.14298153823276735, "compression_ratio": 1.6009174311926606, "no_speech_prob": 8.800957402854692e-06}, {"id": 48, "seek": 27500, "start": 281.84, "end": 287.4, "text": " And here we've kind of unwrapped them, or not unwrapped, reshaped them.", "tokens": [400, 510, 321, 600, 733, 295, 14853, 424, 3320, 552, 11, 420, 406, 14853, 424, 3320, 11, 725, 71, 18653, 552, 13], "temperature": 0.0, "avg_logprob": -0.14298153823276735, "compression_ratio": 1.6009174311926606, "no_speech_prob": 8.800957402854692e-06}, {"id": 49, "seek": 27500, "start": 287.4, "end": 295.84, "text": " So we took this, this was in 40, and we said, look, okay, like, let's make, I think the", "tokens": [407, 321, 1890, 341, 11, 341, 390, 294, 3356, 11, 293, 321, 848, 11, 574, 11, 1392, 11, 411, 11, 718, 311, 652, 11, 286, 519, 264], "temperature": 0.0, "avg_logprob": -0.14298153823276735, "compression_ratio": 1.6009174311926606, "no_speech_prob": 8.800957402854692e-06}, {"id": 50, "seek": 27500, "start": 295.84, "end": 302.52, "text": " X-axis is the angles and positions and then we've unwrapped each picture to be just a", "tokens": [1783, 12, 24633, 307, 264, 14708, 293, 8432, 293, 550, 321, 600, 14853, 424, 3320, 1184, 3036, 281, 312, 445, 257], "temperature": 0.0, "avg_logprob": -0.14298153823276735, "compression_ratio": 1.6009174311926606, "no_speech_prob": 8.800957402854692e-06}, {"id": 51, "seek": 27500, "start": 302.52, "end": 304.68, "text": " single long row.", "tokens": [2167, 938, 5386, 13], "temperature": 0.0, "avg_logprob": -0.14298153823276735, "compression_ratio": 1.6009174311926606, "no_speech_prob": 8.800957402854692e-06}, {"id": 52, "seek": 30468, "start": 304.68, "end": 311.72, "text": " And so that's why it has the appearance that it does down here of like these do contain", "tokens": [400, 370, 300, 311, 983, 309, 575, 264, 8967, 300, 309, 775, 760, 510, 295, 411, 613, 360, 5304], "temperature": 0.0, "avg_logprob": -0.28243451505093964, "compression_ratio": 1.4067796610169492, "no_speech_prob": 1.5688687199144624e-05}, {"id": 53, "seek": 30468, "start": 311.72, "end": 314.36, "text": " little lines, but they've been unwrapped.", "tokens": [707, 3876, 11, 457, 436, 600, 668, 14853, 424, 3320, 13], "temperature": 0.0, "avg_logprob": -0.28243451505093964, "compression_ratio": 1.4067796610169492, "no_speech_prob": 1.5688687199144624e-05}, {"id": 54, "seek": 30468, "start": 314.36, "end": 315.36, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.28243451505093964, "compression_ratio": 1.4067796610169492, "no_speech_prob": 1.5688687199144624e-05}, {"id": 55, "seek": 30468, "start": 315.36, "end": 316.36, "text": " Linda?", "tokens": [20324, 30], "temperature": 0.0, "avg_logprob": -0.28243451505093964, "compression_ratio": 1.4067796610169492, "no_speech_prob": 1.5688687199144624e-05}, {"id": 56, "seek": 30468, "start": 316.36, "end": 323.08, "text": " Oh, and could you throw the catch box, Sam?", "tokens": [876, 11, 293, 727, 291, 3507, 264, 3745, 2424, 11, 4832, 30], "temperature": 0.0, "avg_logprob": -0.28243451505093964, "compression_ratio": 1.4067796610169492, "no_speech_prob": 1.5688687199144624e-05}, {"id": 57, "seek": 30468, "start": 323.08, "end": 329.64, "text": " This might need an intermediate, oh, good throw and good catch.", "tokens": [639, 1062, 643, 364, 19376, 11, 1954, 11, 665, 3507, 293, 665, 3745, 13], "temperature": 0.0, "avg_logprob": -0.28243451505093964, "compression_ratio": 1.4067796610169492, "no_speech_prob": 1.5688687199144624e-05}, {"id": 58, "seek": 32964, "start": 329.64, "end": 335.32, "text": " So what would be the Y-axis and the X-axis?", "tokens": [407, 437, 576, 312, 264, 398, 12, 24633, 293, 264, 1783, 12, 24633, 30], "temperature": 0.0, "avg_logprob": -0.27113357147613126, "compression_ratio": 1.8120300751879699, "no_speech_prob": 2.7534464607015252e-05}, {"id": 59, "seek": 32964, "start": 335.32, "end": 347.28, "text": " So is the Y-axis unravel the angle and the vertical placed in the X-axis?", "tokens": [407, 307, 264, 398, 12, 24633, 40507, 264, 5802, 293, 264, 9429, 7074, 294, 264, 1783, 12, 24633, 30], "temperature": 0.0, "avg_logprob": -0.27113357147613126, "compression_ratio": 1.8120300751879699, "no_speech_prob": 2.7534464607015252e-05}, {"id": 60, "seek": 32964, "start": 347.28, "end": 348.88, "text": " Is the X and Y?", "tokens": [1119, 264, 1783, 293, 398, 30], "temperature": 0.0, "avg_logprob": -0.27113357147613126, "compression_ratio": 1.8120300751879699, "no_speech_prob": 2.7534464607015252e-05}, {"id": 61, "seek": 32964, "start": 348.88, "end": 349.88, "text": " Exactly, yeah.", "tokens": [7587, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.27113357147613126, "compression_ratio": 1.8120300751879699, "no_speech_prob": 2.7534464607015252e-05}, {"id": 62, "seek": 32964, "start": 349.88, "end": 357.76, "text": " So the Y-axis here is the angle and the vertical position and then this one is the X and the", "tokens": [407, 264, 398, 12, 24633, 510, 307, 264, 5802, 293, 264, 9429, 2535, 293, 550, 341, 472, 307, 264, 1783, 293, 264], "temperature": 0.0, "avg_logprob": -0.27113357147613126, "compression_ratio": 1.8120300751879699, "no_speech_prob": 2.7534464607015252e-05}, {"id": 63, "seek": 35776, "start": 357.76, "end": 359.76, "text": " Y, correct?", "tokens": [398, 11, 3006, 30], "temperature": 0.0, "avg_logprob": -0.2912125038889657, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0289405508956406e-05}, {"id": 64, "seek": 35776, "start": 359.76, "end": 369.0, "text": " Another question will be like, toDense.A, what does it do?", "tokens": [3996, 1168, 486, 312, 411, 11, 281, 35, 1288, 13, 32, 11, 437, 775, 309, 360, 30], "temperature": 0.0, "avg_logprob": -0.2912125038889657, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0289405508956406e-05}, {"id": 65, "seek": 35776, "start": 369.0, "end": 374.44, "text": " So how do you unwrap, let's say, just the angle and the vertical motion?", "tokens": [407, 577, 360, 291, 14853, 4007, 11, 718, 311, 584, 11, 445, 264, 5802, 293, 264, 9429, 5394, 30], "temperature": 0.0, "avg_logprob": -0.2912125038889657, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0289405508956406e-05}, {"id": 66, "seek": 35776, "start": 374.44, "end": 378.15999999999997, "text": " Well so what toDense is doing is we're storing this as a sparse matrix.", "tokens": [1042, 370, 437, 281, 35, 1288, 307, 884, 307, 321, 434, 26085, 341, 382, 257, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2912125038889657, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0289405508956406e-05}, {"id": 67, "seek": 35776, "start": 378.15999999999997, "end": 381.15999999999997, "text": " So you just make it dense?", "tokens": [407, 291, 445, 652, 309, 18011, 30], "temperature": 0.0, "avg_logprob": -0.2912125038889657, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0289405508956406e-05}, {"id": 68, "seek": 35776, "start": 381.15999999999997, "end": 382.15999999999997, "text": " Yes, yeah.", "tokens": [1079, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2912125038889657, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0289405508956406e-05}, {"id": 69, "seek": 35776, "start": 382.15999999999997, "end": 386.03999999999996, "text": " And actually, kind of going off of that, why would it be a good idea to store this matrix", "tokens": [400, 767, 11, 733, 295, 516, 766, 295, 300, 11, 983, 576, 309, 312, 257, 665, 1558, 281, 3531, 341, 8141], "temperature": 0.0, "avg_logprob": -0.2912125038889657, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0289405508956406e-05}, {"id": 70, "seek": 35776, "start": 386.03999999999996, "end": 387.03999999999996, "text": " as sparse?", "tokens": [382, 637, 11668, 30], "temperature": 0.0, "avg_logprob": -0.2912125038889657, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0289405508956406e-05}, {"id": 71, "seek": 38704, "start": 387.04, "end": 388.04, "text": " Anyone can answer this.", "tokens": [14643, 393, 1867, 341, 13], "temperature": 0.0, "avg_logprob": -0.21863353126927426, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.5658684939553495e-06}, {"id": 72, "seek": 38704, "start": 388.04, "end": 397.6, "text": " Yeah, so Valentine said because it's sparse and that's correct, yes.", "tokens": [865, 11, 370, 24359, 848, 570, 309, 311, 637, 11668, 293, 300, 311, 3006, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.21863353126927426, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.5658684939553495e-06}, {"id": 73, "seek": 38704, "start": 397.6, "end": 404.28000000000003, "text": " So we see a lot of black here which represents zero, so the white is where there are ones", "tokens": [407, 321, 536, 257, 688, 295, 2211, 510, 597, 8855, 4018, 11, 370, 264, 2418, 307, 689, 456, 366, 2306], "temperature": 0.0, "avg_logprob": -0.21863353126927426, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.5658684939553495e-06}, {"id": 74, "seek": 38704, "start": 404.28000000000003, "end": 406.32000000000005, "text": " and that's a relatively small portion.", "tokens": [293, 300, 311, 257, 7226, 1359, 8044, 13], "temperature": 0.0, "avg_logprob": -0.21863353126927426, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.5658684939553495e-06}, {"id": 75, "seek": 38704, "start": 406.32000000000005, "end": 412.0, "text": " So we've got this stored as a sparse SciPy matrix and we're just converting it to dense", "tokens": [407, 321, 600, 658, 341, 12187, 382, 257, 637, 11668, 16942, 47, 88, 8141, 293, 321, 434, 445, 29942, 309, 281, 18011], "temperature": 0.0, "avg_logprob": -0.21863353126927426, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.5658684939553495e-06}, {"id": 76, "seek": 38704, "start": 412.0, "end": 413.96000000000004, "text": " so we can look at pictures of it.", "tokens": [370, 321, 393, 574, 412, 5242, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.21863353126927426, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.5658684939553495e-06}, {"id": 77, "seek": 41396, "start": 413.96, "end": 417.12, "text": " So to kind of actually do the math, we don't need it to be dense.", "tokens": [407, 281, 733, 295, 767, 360, 264, 5221, 11, 321, 500, 380, 643, 309, 281, 312, 18011, 13], "temperature": 0.0, "avg_logprob": -0.13622840054063912, "compression_ratio": 1.5340314136125655, "no_speech_prob": 3.785247599807917e-06}, {"id": 78, "seek": 41396, "start": 417.12, "end": 419.76, "text": " It's more just to be able to look at these pictures.", "tokens": [467, 311, 544, 445, 281, 312, 1075, 281, 574, 412, 613, 5242, 13], "temperature": 0.0, "avg_logprob": -0.13622840054063912, "compression_ratio": 1.5340314136125655, "no_speech_prob": 3.785247599807917e-06}, {"id": 79, "seek": 41396, "start": 419.76, "end": 428.88, "text": " And I also just wanted to highlight that this matrix is a lot wider than it is tall and", "tokens": [400, 286, 611, 445, 1415, 281, 5078, 300, 341, 8141, 307, 257, 688, 11842, 813, 309, 307, 6764, 293], "temperature": 0.0, "avg_logprob": -0.13622840054063912, "compression_ratio": 1.5340314136125655, "no_speech_prob": 3.785247599807917e-06}, {"id": 80, "seek": 41396, "start": 428.88, "end": 438.15999999999997, "text": " that's because the width is L by L which is 128 by 128 and we only had, so we have 128", "tokens": [300, 311, 570, 264, 11402, 307, 441, 538, 441, 597, 307, 29810, 538, 29810, 293, 321, 787, 632, 11, 370, 321, 362, 29810], "temperature": 0.0, "avg_logprob": -0.13622840054063912, "compression_ratio": 1.5340314136125655, "no_speech_prob": 3.785247599807917e-06}, {"id": 81, "seek": 43816, "start": 438.16, "end": 444.36, "text": " vertical positions but only one seventh of that number of angles which is 18 so that's", "tokens": [9429, 8432, 457, 787, 472, 17875, 295, 300, 1230, 295, 14708, 597, 307, 2443, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.3521823570376537, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.4063616617931984e-05}, {"id": 82, "seek": 43816, "start": 444.36, "end": 450.56, "text": " why it's much shorter is we have a kind of smaller number of angles.", "tokens": [983, 309, 311, 709, 11639, 307, 321, 362, 257, 733, 295, 4356, 1230, 295, 14708, 13], "temperature": 0.0, "avg_logprob": -0.3521823570376537, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.4063616617931984e-05}, {"id": 83, "seek": 43816, "start": 450.56, "end": 460.72, "text": " Any other questions about this X matrix?", "tokens": [2639, 661, 1651, 466, 341, 1783, 8141, 30], "temperature": 0.0, "avg_logprob": -0.3521823570376537, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.4063616617931984e-05}, {"id": 84, "seek": 43816, "start": 460.72, "end": 465.52000000000004, "text": " Okay, so let's move on to beta.", "tokens": [1033, 11, 370, 718, 311, 1286, 322, 281, 9861, 13], "temperature": 0.0, "avg_logprob": -0.3521823570376537, "compression_ratio": 1.4339622641509433, "no_speech_prob": 1.4063616617931984e-05}, {"id": 85, "seek": 46552, "start": 465.52, "end": 472.56, "text": " And so beta is what we're solving for with the linear regression and that's the image.", "tokens": [400, 370, 9861, 307, 437, 321, 434, 12606, 337, 365, 264, 8213, 24590, 293, 300, 311, 264, 3256, 13], "temperature": 0.0, "avg_logprob": -0.12428717893712661, "compression_ratio": 1.6431924882629108, "no_speech_prob": 4.3567351895035245e-06}, {"id": 86, "seek": 46552, "start": 472.56, "end": 477.64, "text": " So we're trying to kind of recover the image from, you know, given knowing, just knowing", "tokens": [407, 321, 434, 1382, 281, 733, 295, 8114, 264, 3256, 490, 11, 291, 458, 11, 2212, 5276, 11, 445, 5276], "temperature": 0.0, "avg_logprob": -0.12428717893712661, "compression_ratio": 1.6431924882629108, "no_speech_prob": 4.3567351895035245e-06}, {"id": 87, "seek": 46552, "start": 477.64, "end": 483.91999999999996, "text": " what angle and what position the X-rays are coming from and knowing what the outputs are", "tokens": [437, 5802, 293, 437, 2535, 264, 1783, 12, 36212, 366, 1348, 490, 293, 5276, 437, 264, 23930, 366], "temperature": 0.0, "avg_logprob": -0.12428717893712661, "compression_ratio": 1.6431924882629108, "no_speech_prob": 4.3567351895035245e-06}, {"id": 88, "seek": 46552, "start": 483.91999999999996, "end": 490.76, "text": " in terms of these kind of measurements of density that were picked up by the CT scan,", "tokens": [294, 2115, 295, 613, 733, 295, 15383, 295, 10305, 300, 645, 6183, 493, 538, 264, 19529, 11049, 11], "temperature": 0.0, "avg_logprob": -0.12428717893712661, "compression_ratio": 1.6431924882629108, "no_speech_prob": 4.3567351895035245e-06}, {"id": 89, "seek": 49076, "start": 490.76, "end": 497.68, "text": " we're trying to solve for this image which we wouldn't actually have.", "tokens": [321, 434, 1382, 281, 5039, 337, 341, 3256, 597, 321, 2759, 380, 767, 362, 13], "temperature": 0.0, "avg_logprob": -0.1308449387550354, "compression_ratio": 1.482233502538071, "no_speech_prob": 2.0904262783005834e-06}, {"id": 90, "seek": 49076, "start": 497.68, "end": 503.12, "text": " And so in order to do that or kind of to express that as our linear regression, we have raveled", "tokens": [400, 370, 294, 1668, 281, 360, 300, 420, 733, 295, 281, 5109, 300, 382, 527, 8213, 24590, 11, 321, 362, 3342, 779, 292], "temperature": 0.0, "avg_logprob": -0.1308449387550354, "compression_ratio": 1.482233502538071, "no_speech_prob": 2.0904262783005834e-06}, {"id": 91, "seek": 49076, "start": 503.12, "end": 507.15999999999997, "text": " the image from a 2D array into a 1D array.", "tokens": [264, 3256, 490, 257, 568, 35, 10225, 666, 257, 502, 35, 10225, 13], "temperature": 0.0, "avg_logprob": -0.1308449387550354, "compression_ratio": 1.482233502538071, "no_speech_prob": 2.0904262783005834e-06}, {"id": 92, "seek": 49076, "start": 507.15999999999997, "end": 516.0, "text": " So you'll see this is just this very tall over 16,000 length vector and it consists", "tokens": [407, 291, 603, 536, 341, 307, 445, 341, 588, 6764, 670, 3165, 11, 1360, 4641, 8062, 293, 309, 14689], "temperature": 0.0, "avg_logprob": -0.1308449387550354, "compression_ratio": 1.482233502538071, "no_speech_prob": 2.0904262783005834e-06}, {"id": 93, "seek": 51600, "start": 516.0, "end": 524.0, "text": " of just a little bit hard to see but it's mostly black with a few, some white spots", "tokens": [295, 445, 257, 707, 857, 1152, 281, 536, 457, 309, 311, 5240, 2211, 365, 257, 1326, 11, 512, 2418, 10681], "temperature": 0.0, "avg_logprob": -0.1550772740290715, "compression_ratio": 1.6049382716049383, "no_speech_prob": 2.4060502710199216e-06}, {"id": 94, "seek": 51600, "start": 524.0, "end": 528.36, "text": " which fits with this picture being mostly black background and just a little bit of", "tokens": [597, 9001, 365, 341, 3036, 885, 5240, 2211, 3678, 293, 445, 257, 707, 857, 295], "temperature": 0.0, "avg_logprob": -0.1550772740290715, "compression_ratio": 1.6049382716049383, "no_speech_prob": 2.4060502710199216e-06}, {"id": 95, "seek": 51600, "start": 528.36, "end": 532.64, "text": " white.", "tokens": [2418, 13], "temperature": 0.0, "avg_logprob": -0.1550772740290715, "compression_ratio": 1.6049382716049383, "no_speech_prob": 2.4060502710199216e-06}, {"id": 96, "seek": 51600, "start": 532.64, "end": 539.92, "text": " And so you can see that how the kind of the measurements, how they were gathered were", "tokens": [400, 370, 291, 393, 536, 300, 577, 264, 733, 295, 264, 15383, 11, 577, 436, 645, 13032, 645], "temperature": 0.0, "avg_logprob": -0.1550772740290715, "compression_ratio": 1.6049382716049383, "no_speech_prob": 2.4060502710199216e-06}, {"id": 97, "seek": 53992, "start": 539.92, "end": 548.36, "text": " basically just by taking these X-rays and multiplying them by the image we were X-raying", "tokens": [1936, 445, 538, 1940, 613, 1783, 12, 36212, 293, 30955, 552, 538, 264, 3256, 321, 645, 1783, 12, 3458, 278], "temperature": 0.0, "avg_logprob": -0.26413649982876247, "compression_ratio": 1.21, "no_speech_prob": 4.494916083785938e-06}, {"id": 98, "seek": 53992, "start": 548.36, "end": 552.0799999999999, "text": " only as a vector.", "tokens": [787, 382, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.26413649982876247, "compression_ratio": 1.21, "no_speech_prob": 4.494916083785938e-06}, {"id": 99, "seek": 53992, "start": 552.0799999999999, "end": 556.12, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.26413649982876247, "compression_ratio": 1.21, "no_speech_prob": 4.494916083785938e-06}, {"id": 100, "seek": 55612, "start": 556.12, "end": 571.48, "text": " Does this help to kind of have it as a, I guess kind of more standard linear regression", "tokens": [4402, 341, 854, 281, 733, 295, 362, 309, 382, 257, 11, 286, 2041, 733, 295, 544, 3832, 8213, 24590], "temperature": 0.0, "avg_logprob": -0.2403814262813992, "compression_ratio": 1.3113207547169812, "no_speech_prob": 6.853972081444226e-06}, {"id": 101, "seek": 55612, "start": 571.48, "end": 576.76, "text": " right out of matrix times a vector equals a vector?", "tokens": [558, 484, 295, 8141, 1413, 257, 8062, 6915, 257, 8062, 30], "temperature": 0.0, "avg_logprob": -0.2403814262813992, "compression_ratio": 1.3113207547169812, "no_speech_prob": 6.853972081444226e-06}, {"id": 102, "seek": 57676, "start": 576.76, "end": 586.4399999999999, "text": " Alright and then just to review what penalty for our linear regression gave us the best", "tokens": [2798, 293, 550, 445, 281, 3131, 437, 16263, 337, 527, 8213, 24590, 2729, 505, 264, 1151], "temperature": 0.0, "avg_logprob": -0.22590218729047634, "compression_ratio": 1.4578313253012047, "no_speech_prob": 5.507376954483334e-06}, {"id": 103, "seek": 57676, "start": 586.4399999999999, "end": 587.4399999999999, "text": " answer.", "tokens": [1867, 13], "temperature": 0.0, "avg_logprob": -0.22590218729047634, "compression_ratio": 1.4578313253012047, "no_speech_prob": 5.507376954483334e-06}, {"id": 104, "seek": 57676, "start": 587.4399999999999, "end": 599.4, "text": " And I hear mumblings but I couldn't hear what was said.", "tokens": [400, 286, 1568, 275, 2860, 20823, 457, 286, 2809, 380, 1568, 437, 390, 848, 13], "temperature": 0.0, "avg_logprob": -0.22590218729047634, "compression_ratio": 1.4578313253012047, "no_speech_prob": 5.507376954483334e-06}, {"id": 105, "seek": 57676, "start": 599.4, "end": 605.2, "text": " Yes L1 is the correct answer and that's because the L1 norm induces sparsity and so we saw", "tokens": [1079, 441, 16, 307, 264, 3006, 1867, 293, 300, 311, 570, 264, 441, 16, 2026, 13716, 887, 637, 685, 507, 293, 370, 321, 1866], "temperature": 0.0, "avg_logprob": -0.22590218729047634, "compression_ratio": 1.4578313253012047, "no_speech_prob": 5.507376954483334e-06}, {"id": 106, "seek": 60520, "start": 605.2, "end": 612.88, "text": " I'm just going to briefly show those again that when we did a linear regression with", "tokens": [286, 478, 445, 516, 281, 10515, 855, 729, 797, 300, 562, 321, 630, 257, 8213, 24590, 365], "temperature": 0.0, "avg_logprob": -0.2500208149785581, "compression_ratio": 1.4631578947368422, "no_speech_prob": 1.6027856872824486e-06}, {"id": 107, "seek": 60520, "start": 612.88, "end": 618.08, "text": " the L2 penalty we don't get a very good answer.", "tokens": [264, 441, 17, 16263, 321, 500, 380, 483, 257, 588, 665, 1867, 13], "temperature": 0.0, "avg_logprob": -0.2500208149785581, "compression_ratio": 1.4631578947368422, "no_speech_prob": 1.6027856872824486e-06}, {"id": 108, "seek": 60520, "start": 618.08, "end": 626.72, "text": " However, doing it with the L1 penalty gives us almost exactly the right answer.", "tokens": [2908, 11, 884, 309, 365, 264, 441, 16, 16263, 2709, 505, 1920, 2293, 264, 558, 1867, 13], "temperature": 0.0, "avg_logprob": -0.2500208149785581, "compression_ratio": 1.4631578947368422, "no_speech_prob": 1.6027856872824486e-06}, {"id": 109, "seek": 60520, "start": 626.72, "end": 633.5600000000001, "text": " Any final questions on kind of notebook four, compressed sensing?", "tokens": [2639, 2572, 1651, 322, 733, 295, 21060, 1451, 11, 30353, 30654, 30], "temperature": 0.0, "avg_logprob": -0.2500208149785581, "compression_ratio": 1.4631578947368422, "no_speech_prob": 1.6027856872824486e-06}, {"id": 110, "seek": 63356, "start": 633.56, "end": 653.1999999999999, "text": " Okay, head back to notebook five.", "tokens": [1033, 11, 1378, 646, 281, 21060, 1732, 13], "temperature": 0.0, "avg_logprob": -0.17029574423125296, "compression_ratio": 1.175257731958763, "no_speech_prob": 1.4063190974411555e-05}, {"id": 111, "seek": 63356, "start": 653.1999999999999, "end": 658.4, "text": " So we talked about this some last time but I want to review some of what we saw.", "tokens": [407, 321, 2825, 466, 341, 512, 1036, 565, 457, 286, 528, 281, 3131, 512, 295, 437, 321, 1866, 13], "temperature": 0.0, "avg_logprob": -0.17029574423125296, "compression_ratio": 1.175257731958763, "no_speech_prob": 1.4063190974411555e-05}, {"id": 112, "seek": 65840, "start": 658.4, "end": 665.1999999999999, "text": " So we're looking at this data set of diabetes data that and we've broken it into a training", "tokens": [407, 321, 434, 1237, 412, 341, 1412, 992, 295, 13881, 1412, 300, 293, 321, 600, 5463, 309, 666, 257, 3097], "temperature": 0.0, "avg_logprob": -0.12697709749822747, "compression_ratio": 1.528205128205128, "no_speech_prob": 5.014478119846899e-06}, {"id": 113, "seek": 65840, "start": 665.1999999999999, "end": 669.84, "text": " set and a test set which is really important so that you don't overfit.", "tokens": [992, 293, 257, 1500, 992, 597, 307, 534, 1021, 370, 300, 291, 500, 380, 670, 6845, 13], "temperature": 0.0, "avg_logprob": -0.12697709749822747, "compression_ratio": 1.528205128205128, "no_speech_prob": 5.014478119846899e-06}, {"id": 114, "seek": 65840, "start": 669.84, "end": 678.64, "text": " Our training data is 353 rows, those are the measurements, times 10 variables and these", "tokens": [2621, 3097, 1412, 307, 6976, 18, 13241, 11, 729, 366, 264, 15383, 11, 1413, 1266, 9102, 293, 613], "temperature": 0.0, "avg_logprob": -0.12697709749822747, "compression_ratio": 1.528205128205128, "no_speech_prob": 5.014478119846899e-06}, {"id": 115, "seek": 65840, "start": 678.64, "end": 682.92, "text": " are often called features in machine learning.", "tokens": [366, 2049, 1219, 4122, 294, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.12697709749822747, "compression_ratio": 1.528205128205128, "no_speech_prob": 5.014478119846899e-06}, {"id": 116, "seek": 68292, "start": 682.92, "end": 688.7199999999999, "text": " We did a linear regression in Scikit-learn just using, is this large enough?", "tokens": [492, 630, 257, 8213, 24590, 294, 16942, 22681, 12, 306, 1083, 445, 1228, 11, 307, 341, 2416, 1547, 30], "temperature": 0.0, "avg_logprob": -0.2791025831892684, "compression_ratio": 1.457142857142857, "no_speech_prob": 8.800784598861355e-06}, {"id": 117, "seek": 68292, "start": 688.7199999999999, "end": 691.7199999999999, "text": " I should probably make this bigger.", "tokens": [286, 820, 1391, 652, 341, 3801, 13], "temperature": 0.0, "avg_logprob": -0.2791025831892684, "compression_ratio": 1.457142857142857, "no_speech_prob": 8.800784598861355e-06}, {"id": 118, "seek": 68292, "start": 691.7199999999999, "end": 698.1999999999999, "text": " Yeah, so we just used Scikit-learn's linear regression.", "tokens": [865, 11, 370, 321, 445, 1143, 16942, 22681, 12, 306, 1083, 311, 8213, 24590, 13], "temperature": 0.0, "avg_logprob": -0.2791025831892684, "compression_ratio": 1.457142857142857, "no_speech_prob": 8.800784598861355e-06}, {"id": 119, "seek": 68292, "start": 698.1999999999999, "end": 707.7199999999999, "text": " We get a prediction and so here kind of as a starting point our L2 error is 75, our L1", "tokens": [492, 483, 257, 17630, 293, 370, 510, 733, 295, 382, 257, 2891, 935, 527, 441, 17, 6713, 307, 9562, 11, 527, 441, 16], "temperature": 0.0, "avg_logprob": -0.2791025831892684, "compression_ratio": 1.457142857142857, "no_speech_prob": 8.800784598861355e-06}, {"id": 120, "seek": 70772, "start": 707.72, "end": 714.0400000000001, "text": " error is 60, so not a great fit.", "tokens": [6713, 307, 4060, 11, 370, 406, 257, 869, 3318, 13], "temperature": 0.0, "avg_logprob": -0.19625216516955146, "compression_ratio": 1.5024875621890548, "no_speech_prob": 6.9621064540115185e-06}, {"id": 121, "seek": 70772, "start": 714.0400000000001, "end": 719.5600000000001, "text": " And we talked about one way to try to get a better fit would be to add polynomial features.", "tokens": [400, 321, 2825, 466, 472, 636, 281, 853, 281, 483, 257, 1101, 3318, 576, 312, 281, 909, 26110, 4122, 13], "temperature": 0.0, "avg_logprob": -0.19625216516955146, "compression_ratio": 1.5024875621890548, "no_speech_prob": 6.9621064540115185e-06}, {"id": 122, "seek": 70772, "start": 719.5600000000001, "end": 725.6, "text": " And so kind of linear regression for, here I just did a problem where you had three features,", "tokens": [400, 370, 733, 295, 8213, 24590, 337, 11, 510, 286, 445, 630, 257, 1154, 689, 291, 632, 1045, 4122, 11], "temperature": 0.0, "avg_logprob": -0.19625216516955146, "compression_ratio": 1.5024875621890548, "no_speech_prob": 6.9621064540115185e-06}, {"id": 123, "seek": 70772, "start": 725.6, "end": 733.1600000000001, "text": " X0, X1 and X2, you're trying to find the best coefficients, beta 0, beta 1, beta 2.", "tokens": [1783, 15, 11, 1783, 16, 293, 1783, 17, 11, 291, 434, 1382, 281, 915, 264, 1151, 31994, 11, 9861, 1958, 11, 9861, 502, 11, 9861, 568, 13], "temperature": 0.0, "avg_logprob": -0.19625216516955146, "compression_ratio": 1.5024875621890548, "no_speech_prob": 6.9621064540115185e-06}, {"id": 124, "seek": 73316, "start": 733.16, "end": 740.24, "text": " And adding polynomial features just means taking, still have X0, X1, X2 and then adding", "tokens": [400, 5127, 26110, 4122, 445, 1355, 1940, 11, 920, 362, 1783, 15, 11, 1783, 16, 11, 1783, 17, 293, 550, 5127], "temperature": 0.0, "avg_logprob": -0.13475106086260008, "compression_ratio": 1.5389221556886228, "no_speech_prob": 7.527731213485822e-06}, {"id": 125, "seek": 73316, "start": 740.24, "end": 750.68, "text": " X0 squared, X0, X1, X0, X2, X1 squared, X1 times X2 and X2 squared.", "tokens": [1783, 15, 8889, 11, 1783, 15, 11, 1783, 16, 11, 1783, 15, 11, 1783, 17, 11, 1783, 16, 8889, 11, 1783, 16, 1413, 1783, 17, 293, 1783, 17, 8889, 13], "temperature": 0.0, "avg_logprob": -0.13475106086260008, "compression_ratio": 1.5389221556886228, "no_speech_prob": 7.527731213485822e-06}, {"id": 126, "seek": 73316, "start": 750.68, "end": 755.8, "text": " Why do you think this is still a linear regression problem even though we have squared terms", "tokens": [1545, 360, 291, 519, 341, 307, 920, 257, 8213, 24590, 1154, 754, 1673, 321, 362, 8889, 2115], "temperature": 0.0, "avg_logprob": -0.13475106086260008, "compression_ratio": 1.5389221556886228, "no_speech_prob": 7.527731213485822e-06}, {"id": 127, "seek": 73316, "start": 755.8, "end": 756.8, "text": " in here?", "tokens": [294, 510, 30], "temperature": 0.0, "avg_logprob": -0.13475106086260008, "compression_ratio": 1.5389221556886228, "no_speech_prob": 7.527731213485822e-06}, {"id": 128, "seek": 75680, "start": 756.8, "end": 764.5999999999999, "text": " Wait, Linda, can you throw that catch box to Connor?", "tokens": [3802, 11, 20324, 11, 393, 291, 3507, 300, 3745, 2424, 281, 33133, 30], "temperature": 0.0, "avg_logprob": -0.22731709480285645, "compression_ratio": 1.638743455497382, "no_speech_prob": 9.223306733474601e-06}, {"id": 129, "seek": 75680, "start": 764.5999999999999, "end": 768.88, "text": " Because the coefficients are still linear?", "tokens": [1436, 264, 31994, 366, 920, 8213, 30], "temperature": 0.0, "avg_logprob": -0.22731709480285645, "compression_ratio": 1.638743455497382, "no_speech_prob": 9.223306733474601e-06}, {"id": 130, "seek": 75680, "start": 768.88, "end": 772.8, "text": " Exactly, the coefficients are still linear.", "tokens": [7587, 11, 264, 31994, 366, 920, 8213, 13], "temperature": 0.0, "avg_logprob": -0.22731709480285645, "compression_ratio": 1.638743455497382, "no_speech_prob": 9.223306733474601e-06}, {"id": 131, "seek": 75680, "start": 772.8, "end": 776.64, "text": " So we're just solving for the betas and so we're now solving for eight coefficients.", "tokens": [407, 321, 434, 445, 12606, 337, 264, 778, 296, 293, 370, 321, 434, 586, 12606, 337, 3180, 31994, 13], "temperature": 0.0, "avg_logprob": -0.22731709480285645, "compression_ratio": 1.638743455497382, "no_speech_prob": 9.223306733474601e-06}, {"id": 132, "seek": 75680, "start": 776.64, "end": 781.8399999999999, "text": " And so this is a way to kind of create a model that's going to capture a little bit more", "tokens": [400, 370, 341, 307, 257, 636, 281, 733, 295, 1884, 257, 2316, 300, 311, 516, 281, 7983, 257, 707, 857, 544], "temperature": 0.0, "avg_logprob": -0.22731709480285645, "compression_ratio": 1.638743455497382, "no_speech_prob": 9.223306733474601e-06}, {"id": 133, "seek": 78184, "start": 781.84, "end": 788.0, "text": " complexity in case there are interactions between X0 and X1 or if there's a squared", "tokens": [14024, 294, 1389, 456, 366, 13280, 1296, 1783, 15, 293, 1783, 16, 420, 498, 456, 311, 257, 8889], "temperature": 0.0, "avg_logprob": -0.1582980579800076, "compression_ratio": 1.5975609756097562, "no_speech_prob": 6.240825769054936e-06}, {"id": 134, "seek": 78184, "start": 788.0, "end": 793.84, "text": " relationship between how the dependent variable, which in this case is I think like a long", "tokens": [2480, 1296, 577, 264, 12334, 7006, 11, 597, 294, 341, 1389, 307, 286, 519, 411, 257, 938], "temperature": 0.0, "avg_logprob": -0.1582980579800076, "compression_ratio": 1.5975609756097562, "no_speech_prob": 6.240825769054936e-06}, {"id": 135, "seek": 78184, "start": 793.84, "end": 799.32, "text": " term health outcome for the diabetes patient, how that depends on your inputs.", "tokens": [1433, 1585, 9700, 337, 264, 13881, 4537, 11, 577, 300, 5946, 322, 428, 15743, 13], "temperature": 0.0, "avg_logprob": -0.1582980579800076, "compression_ratio": 1.5975609756097562, "no_speech_prob": 6.240825769054936e-06}, {"id": 136, "seek": 78184, "start": 799.32, "end": 805.64, "text": " But yeah, this still, you kind of calculate these additional features and then you still", "tokens": [583, 1338, 11, 341, 920, 11, 291, 733, 295, 8873, 613, 4497, 4122, 293, 550, 291, 920], "temperature": 0.0, "avg_logprob": -0.1582980579800076, "compression_ratio": 1.5975609756097562, "no_speech_prob": 6.240825769054936e-06}, {"id": 137, "seek": 78184, "start": 805.64, "end": 808.48, "text": " have a matrix of values that are linear with beta.", "tokens": [362, 257, 8141, 295, 4190, 300, 366, 8213, 365, 9861, 13], "temperature": 0.0, "avg_logprob": -0.1582980579800076, "compression_ratio": 1.5975609756097562, "no_speech_prob": 6.240825769054936e-06}, {"id": 138, "seek": 80848, "start": 808.48, "end": 813.9200000000001, "text": " You just have more coefficients you're finding.", "tokens": [509, 445, 362, 544, 31994, 291, 434, 5006, 13], "temperature": 0.0, "avg_logprob": -0.26913653287020595, "compression_ratio": 1.3933333333333333, "no_speech_prob": 6.144049621070735e-06}, {"id": 139, "seek": 80848, "start": 813.9200000000001, "end": 818.16, "text": " Questions about the setup of using polynomial features?", "tokens": [27738, 466, 264, 8657, 295, 1228, 26110, 4122, 30], "temperature": 0.0, "avg_logprob": -0.26913653287020595, "compression_ratio": 1.3933333333333333, "no_speech_prob": 6.144049621070735e-06}, {"id": 140, "seek": 80848, "start": 818.16, "end": 825.9200000000001, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.26913653287020595, "compression_ratio": 1.3933333333333333, "no_speech_prob": 6.144049621070735e-06}, {"id": 141, "seek": 80848, "start": 825.9200000000001, "end": 831.4, "text": " So we started off, scikit-learn has a polynomial features method that will create them for", "tokens": [407, 321, 1409, 766, 11, 2180, 22681, 12, 306, 1083, 575, 257, 26110, 4122, 3170, 300, 486, 1884, 552, 337], "temperature": 0.0, "avg_logprob": -0.26913653287020595, "compression_ratio": 1.3933333333333333, "no_speech_prob": 6.144049621070735e-06}, {"id": 142, "seek": 80848, "start": 831.4, "end": 832.84, "text": " us.", "tokens": [505, 13], "temperature": 0.0, "avg_logprob": -0.26913653287020595, "compression_ratio": 1.3933333333333333, "no_speech_prob": 6.144049621070735e-06}, {"id": 143, "seek": 83284, "start": 832.84, "end": 839.48, "text": " So we did that to get some additional training features, which it's printed out here.", "tokens": [407, 321, 630, 300, 281, 483, 512, 4497, 3097, 4122, 11, 597, 309, 311, 13567, 484, 510, 13], "temperature": 0.0, "avg_logprob": -0.1130585385792291, "compression_ratio": 1.511764705882353, "no_speech_prob": 3.7852585137443384e-06}, {"id": 144, "seek": 83284, "start": 839.48, "end": 847.8000000000001, "text": " And in our case, since we had 10 features that we started with, we're getting back,", "tokens": [400, 294, 527, 1389, 11, 1670, 321, 632, 1266, 4122, 300, 321, 1409, 365, 11, 321, 434, 1242, 646, 11], "temperature": 0.0, "avg_logprob": -0.1130585385792291, "compression_ratio": 1.511764705882353, "no_speech_prob": 3.7852585137443384e-06}, {"id": 145, "seek": 83284, "start": 847.8000000000001, "end": 852.64, "text": " we're now up to 65 features.", "tokens": [321, 434, 586, 493, 281, 11624, 4122, 13], "temperature": 0.0, "avg_logprob": -0.1130585385792291, "compression_ratio": 1.511764705882353, "no_speech_prob": 3.7852585137443384e-06}, {"id": 146, "seek": 83284, "start": 852.64, "end": 855.44, "text": " And so it's just calculated all the possible combinations.", "tokens": [400, 370, 309, 311, 445, 15598, 439, 264, 1944, 21267, 13], "temperature": 0.0, "avg_logprob": -0.1130585385792291, "compression_ratio": 1.511764705882353, "no_speech_prob": 3.7852585137443384e-06}, {"id": 147, "seek": 85544, "start": 855.44, "end": 863.0, "text": " So age squared, age times sex, age times BMI, blood pressure times serum level two, but", "tokens": [407, 3205, 8889, 11, 3205, 1413, 3260, 11, 3205, 1413, 363, 13808, 11, 3390, 3321, 1413, 32755, 1496, 732, 11, 457], "temperature": 0.0, "avg_logprob": -0.169310900900099, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.9033213902730495e-06}, {"id": 148, "seek": 85544, "start": 863.0, "end": 869.1600000000001, "text": " just any possible combination.", "tokens": [445, 604, 1944, 6562, 13], "temperature": 0.0, "avg_logprob": -0.169310900900099, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.9033213902730495e-06}, {"id": 149, "seek": 85544, "start": 869.1600000000001, "end": 875.44, "text": " So this reduces our error when we run linear regression again with our kind of having more", "tokens": [407, 341, 18081, 527, 6713, 562, 321, 1190, 8213, 24590, 797, 365, 527, 733, 295, 1419, 544], "temperature": 0.0, "avg_logprob": -0.169310900900099, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.9033213902730495e-06}, {"id": 150, "seek": 85544, "start": 875.44, "end": 878.2, "text": " features.", "tokens": [4122, 13], "temperature": 0.0, "avg_logprob": -0.169310900900099, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.9033213902730495e-06}, {"id": 151, "seek": 85544, "start": 878.2, "end": 884.5600000000001, "text": " The only issue is that time is squared just to even create these features.", "tokens": [440, 787, 2734, 307, 300, 565, 307, 8889, 445, 281, 754, 1884, 613, 4122, 13], "temperature": 0.0, "avg_logprob": -0.169310900900099, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.9033213902730495e-06}, {"id": 152, "seek": 88456, "start": 884.56, "end": 895.5999999999999, "text": " And so this could possibly get slow as we have a larger and larger data set.", "tokens": [400, 370, 341, 727, 6264, 483, 2964, 382, 321, 362, 257, 4833, 293, 4833, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.16447634107611153, "compression_ratio": 1.5, "no_speech_prob": 1.9333231193741085e-06}, {"id": 153, "seek": 88456, "start": 895.5999999999999, "end": 897.7199999999999, "text": " So we saw Numba last time.", "tokens": [407, 321, 1866, 426, 49353, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.16447634107611153, "compression_ratio": 1.5, "no_speech_prob": 1.9333231193741085e-06}, {"id": 154, "seek": 88456, "start": 897.7199999999999, "end": 905.5999999999999, "text": " And to make it clear, Numba is a compiler and it is a Python library that compiles code", "tokens": [400, 281, 652, 309, 1850, 11, 426, 49353, 307, 257, 31958, 293, 309, 307, 257, 15329, 6405, 300, 715, 4680, 3089], "temperature": 0.0, "avg_logprob": -0.16447634107611153, "compression_ratio": 1.5, "no_speech_prob": 1.9333231193741085e-06}, {"id": 155, "seek": 88456, "start": 905.5999999999999, "end": 906.5999999999999, "text": " directly to C.", "tokens": [3838, 281, 383, 13], "temperature": 0.0, "avg_logprob": -0.16447634107611153, "compression_ratio": 1.5, "no_speech_prob": 1.9333231193741085e-06}, {"id": 156, "seek": 88456, "start": 906.5999999999999, "end": 912.64, "text": " I wanted to highlight, so I think Jake Vander Plaas has some really nice, he's got like", "tokens": [286, 1415, 281, 5078, 11, 370, 286, 519, 15822, 46588, 19942, 296, 575, 512, 534, 1481, 11, 415, 311, 658, 411], "temperature": 0.0, "avg_logprob": -0.16447634107611153, "compression_ratio": 1.5, "no_speech_prob": 1.9333231193741085e-06}, {"id": 157, "seek": 88456, "start": 912.64, "end": 913.64, "text": " a nice introduction.", "tokens": [257, 1481, 9339, 13], "temperature": 0.0, "avg_logprob": -0.16447634107611153, "compression_ratio": 1.5, "no_speech_prob": 1.9333231193741085e-06}, {"id": 158, "seek": 91364, "start": 913.64, "end": 915.68, "text": " And he also has a later blog post.", "tokens": [400, 415, 611, 575, 257, 1780, 6968, 2183, 13], "temperature": 0.0, "avg_logprob": -0.18691970600801355, "compression_ratio": 1.51, "no_speech_prob": 7.527627076342469e-06}, {"id": 159, "seek": 91364, "start": 915.68, "end": 919.48, "text": " Let me open this.", "tokens": [961, 385, 1269, 341, 13], "temperature": 0.0, "avg_logprob": -0.18691970600801355, "compression_ratio": 1.51, "no_speech_prob": 7.527627076342469e-06}, {"id": 160, "seek": 91364, "start": 919.48, "end": 925.4, "text": " Where he tries to do a, or not tries, but does a non-trivial algorithm, which I thought", "tokens": [2305, 415, 9898, 281, 360, 257, 11, 420, 406, 9898, 11, 457, 775, 257, 2107, 12, 83, 470, 22640, 9284, 11, 597, 286, 1194], "temperature": 0.0, "avg_logprob": -0.18691970600801355, "compression_ratio": 1.51, "no_speech_prob": 7.527627076342469e-06}, {"id": 161, "seek": 91364, "start": 925.4, "end": 931.0, "text": " was nice since often you kind of see these test cases on simpler algorithms, which is", "tokens": [390, 1481, 1670, 2049, 291, 733, 295, 536, 613, 1500, 3331, 322, 18587, 14642, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.18691970600801355, "compression_ratio": 1.51, "no_speech_prob": 7.527627076342469e-06}, {"id": 162, "seek": 91364, "start": 931.0, "end": 932.0, "text": " what I gave you.", "tokens": [437, 286, 2729, 291, 13], "temperature": 0.0, "avg_logprob": -0.18691970600801355, "compression_ratio": 1.51, "no_speech_prob": 7.527627076342469e-06}, {"id": 163, "seek": 91364, "start": 932.0, "end": 942.76, "text": " But so I definitely recommend checking out this blog post.", "tokens": [583, 370, 286, 2138, 2748, 8568, 484, 341, 6968, 2183, 13], "temperature": 0.0, "avg_logprob": -0.18691970600801355, "compression_ratio": 1.51, "no_speech_prob": 7.527627076342469e-06}, {"id": 164, "seek": 94276, "start": 942.76, "end": 944.52, "text": " And then I wanted to highlight a quote of his.", "tokens": [400, 550, 286, 1415, 281, 5078, 257, 6513, 295, 702, 13], "temperature": 0.0, "avg_logprob": -0.11046210122764658, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.3418471098702867e-05}, {"id": 165, "seek": 94276, "start": 944.52, "end": 947.52, "text": " This is from his kind of intro post.", "tokens": [639, 307, 490, 702, 733, 295, 12897, 2183, 13], "temperature": 0.0, "avg_logprob": -0.11046210122764658, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.3418471098702867e-05}, {"id": 166, "seek": 94276, "start": 947.52, "end": 952.76, "text": " He says that, so for this problem he does, Numba is a thousand times faster than a pure", "tokens": [634, 1619, 300, 11, 370, 337, 341, 1154, 415, 775, 11, 426, 49353, 307, 257, 4714, 1413, 4663, 813, 257, 6075], "temperature": 0.0, "avg_logprob": -0.11046210122764658, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.3418471098702867e-05}, {"id": 167, "seek": 94276, "start": 952.76, "end": 958.4, "text": " Python implementation and only marginally slower than nearly identical Cython code.", "tokens": [15329, 11420, 293, 787, 10270, 379, 14009, 813, 6217, 14800, 10295, 11943, 3089, 13], "temperature": 0.0, "avg_logprob": -0.11046210122764658, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.3418471098702867e-05}, {"id": 168, "seek": 94276, "start": 958.4, "end": 964.52, "text": " And Cython is another Python library slash compiler that compiles directly to C. And", "tokens": [400, 10295, 11943, 307, 1071, 15329, 6405, 17330, 31958, 300, 715, 4680, 3838, 281, 383, 13, 400], "temperature": 0.0, "avg_logprob": -0.11046210122764658, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.3418471098702867e-05}, {"id": 169, "seek": 94276, "start": 964.52, "end": 968.28, "text": " he says that he has years of experience with Cython and only an hour's experience with", "tokens": [415, 1619, 300, 415, 575, 924, 295, 1752, 365, 10295, 11943, 293, 787, 364, 1773, 311, 1752, 365], "temperature": 0.0, "avg_logprob": -0.11046210122764658, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.3418471098702867e-05}, {"id": 170, "seek": 94276, "start": 968.28, "end": 969.28, "text": " Numba.", "tokens": [426, 49353, 13], "temperature": 0.0, "avg_logprob": -0.11046210122764658, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.3418471098702867e-05}, {"id": 171, "seek": 96928, "start": 969.28, "end": 973.8399999999999, "text": " And he was using every optimization he knew for the Cython variation and just the basic", "tokens": [400, 415, 390, 1228, 633, 19618, 415, 2586, 337, 264, 10295, 11943, 12990, 293, 445, 264, 3875], "temperature": 0.0, "avg_logprob": -0.11063496753422901, "compression_ratio": 1.5850622406639003, "no_speech_prob": 6.747708994225832e-06}, {"id": 172, "seek": 96928, "start": 973.8399999999999, "end": 976.04, "text": " vanilla syntax for Numba.", "tokens": [17528, 28431, 337, 426, 49353, 13], "temperature": 0.0, "avg_logprob": -0.11063496753422901, "compression_ratio": 1.5850622406639003, "no_speech_prob": 6.747708994225832e-06}, {"id": 173, "seek": 96928, "start": 976.04, "end": 982.3199999999999, "text": " So I think this is a really strong endorsement of Numba over Cython, that it took less knowledge.", "tokens": [407, 286, 519, 341, 307, 257, 534, 2068, 29228, 518, 295, 426, 49353, 670, 10295, 11943, 11, 300, 309, 1890, 1570, 3601, 13], "temperature": 0.0, "avg_logprob": -0.11063496753422901, "compression_ratio": 1.5850622406639003, "no_speech_prob": 6.747708994225832e-06}, {"id": 174, "seek": 96928, "start": 982.3199999999999, "end": 988.12, "text": " And I've only used Cython a little bit, but this was my experience that there was more", "tokens": [400, 286, 600, 787, 1143, 10295, 11943, 257, 707, 857, 11, 457, 341, 390, 452, 1752, 300, 456, 390, 544], "temperature": 0.0, "avg_logprob": -0.11063496753422901, "compression_ratio": 1.5850622406639003, "no_speech_prob": 6.747708994225832e-06}, {"id": 175, "seek": 96928, "start": 988.12, "end": 993.8399999999999, "text": " to learn about Cython and it felt closer to C, whereas Numba is very simple to use.", "tokens": [281, 1466, 466, 10295, 11943, 293, 309, 2762, 4966, 281, 383, 11, 9735, 426, 49353, 307, 588, 2199, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.11063496753422901, "compression_ratio": 1.5850622406639003, "no_speech_prob": 6.747708994225832e-06}, {"id": 176, "seek": 99384, "start": 993.84, "end": 1000.6800000000001, "text": " And I saw kind of similar results for the problems I've tried it on.", "tokens": [400, 286, 1866, 733, 295, 2531, 3542, 337, 264, 2740, 286, 600, 3031, 309, 322, 13], "temperature": 0.0, "avg_logprob": -0.1352997124195099, "compression_ratio": 1.3942857142857144, "no_speech_prob": 5.862700163561385e-06}, {"id": 177, "seek": 99384, "start": 1000.6800000000001, "end": 1008.0, "text": " And again, Cython and Numba are both ways to compile your Python to C for additional", "tokens": [400, 797, 11, 10295, 11943, 293, 426, 49353, 366, 1293, 2098, 281, 31413, 428, 15329, 281, 383, 337, 4497], "temperature": 0.0, "avg_logprob": -0.1352997124195099, "compression_ratio": 1.3942857142857144, "no_speech_prob": 5.862700163561385e-06}, {"id": 178, "seek": 99384, "start": 1008.0, "end": 1009.0, "text": " optimization.", "tokens": [19618, 13], "temperature": 0.0, "avg_logprob": -0.1352997124195099, "compression_ratio": 1.3942857142857144, "no_speech_prob": 5.862700163561385e-06}, {"id": 179, "seek": 99384, "start": 1009.0, "end": 1020.52, "text": " So I wanted to walk through kind of our example again that we saw last time.", "tokens": [407, 286, 1415, 281, 1792, 807, 733, 295, 527, 1365, 797, 300, 321, 1866, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.1352997124195099, "compression_ratio": 1.3942857142857144, "no_speech_prob": 5.862700163561385e-06}, {"id": 180, "seek": 102052, "start": 1020.52, "end": 1028.08, "text": " So we have this Python algorithm that is bit contrived.", "tokens": [407, 321, 362, 341, 15329, 9284, 300, 307, 857, 660, 470, 937, 13], "temperature": 0.0, "avg_logprob": -0.18111073098531583, "compression_ratio": 1.480952380952381, "no_speech_prob": 1.8924087271443568e-05}, {"id": 181, "seek": 102052, "start": 1028.08, "end": 1031.56, "text": " We'll be using it for polynomial feature generation though next.", "tokens": [492, 603, 312, 1228, 309, 337, 26110, 4111, 5125, 1673, 958, 13], "temperature": 0.0, "avg_logprob": -0.18111073098531583, "compression_ratio": 1.480952380952381, "no_speech_prob": 1.8924087271443568e-05}, {"id": 182, "seek": 102052, "start": 1031.56, "end": 1033.84, "text": " This is just as an introduction.", "tokens": [639, 307, 445, 382, 364, 9339, 13], "temperature": 0.0, "avg_logprob": -0.18111073098531583, "compression_ratio": 1.480952380952381, "no_speech_prob": 1.8924087271443568e-05}, {"id": 183, "seek": 102052, "start": 1033.84, "end": 1039.16, "text": " So we're going through this for loop with kind of doing a number of operations with", "tokens": [407, 321, 434, 516, 807, 341, 337, 6367, 365, 733, 295, 884, 257, 1230, 295, 7705, 365], "temperature": 0.0, "avg_logprob": -0.18111073098531583, "compression_ratio": 1.480952380952381, "no_speech_prob": 1.8924087271443568e-05}, {"id": 184, "seek": 102052, "start": 1039.16, "end": 1042.24, "text": " x and y to calculate a new vector z.", "tokens": [2031, 293, 288, 281, 8873, 257, 777, 8062, 710, 13], "temperature": 0.0, "avg_logprob": -0.18111073098531583, "compression_ratio": 1.480952380952381, "no_speech_prob": 1.8924087271443568e-05}, {"id": 185, "seek": 102052, "start": 1042.24, "end": 1048.78, "text": " We time it and it's 49 milliseconds.", "tokens": [492, 565, 309, 293, 309, 311, 16513, 34184, 13], "temperature": 0.0, "avg_logprob": -0.18111073098531583, "compression_ratio": 1.480952380952381, "no_speech_prob": 1.8924087271443568e-05}, {"id": 186, "seek": 104878, "start": 1048.78, "end": 1051.24, "text": " And then we use NumPy to vectorize it.", "tokens": [400, 550, 321, 764, 22592, 47, 88, 281, 8062, 1125, 309, 13], "temperature": 0.0, "avg_logprob": -0.11825861372389235, "compression_ratio": 1.6725663716814159, "no_speech_prob": 7.888994332461152e-06}, {"id": 187, "seek": 104878, "start": 1051.24, "end": 1056.6399999999999, "text": " So here we've taken out the for loop and are just acting on the vectors.", "tokens": [407, 510, 321, 600, 2726, 484, 264, 337, 6367, 293, 366, 445, 6577, 322, 264, 18875, 13], "temperature": 0.0, "avg_logprob": -0.11825861372389235, "compression_ratio": 1.6725663716814159, "no_speech_prob": 7.888994332461152e-06}, {"id": 188, "seek": 104878, "start": 1056.6399999999999, "end": 1062.48, "text": " So you can see this pure Python, got a for loop where we go through each element of the", "tokens": [407, 291, 393, 536, 341, 6075, 15329, 11, 658, 257, 337, 6367, 689, 321, 352, 807, 1184, 4478, 295, 264], "temperature": 0.0, "avg_logprob": -0.11825861372389235, "compression_ratio": 1.6725663716814159, "no_speech_prob": 7.888994332461152e-06}, {"id": 189, "seek": 104878, "start": 1062.48, "end": 1063.48, "text": " array.", "tokens": [10225, 13], "temperature": 0.0, "avg_logprob": -0.11825861372389235, "compression_ratio": 1.6725663716814159, "no_speech_prob": 7.888994332461152e-06}, {"id": 190, "seek": 104878, "start": 1063.48, "end": 1068.8799999999999, "text": " And then NumPy has this built in vectorization where we can just treat x and y as vectors", "tokens": [400, 550, 22592, 47, 88, 575, 341, 3094, 294, 8062, 2144, 689, 321, 393, 445, 2387, 2031, 293, 288, 382, 18875], "temperature": 0.0, "avg_logprob": -0.11825861372389235, "compression_ratio": 1.6725663716814159, "no_speech_prob": 7.888994332461152e-06}, {"id": 191, "seek": 104878, "start": 1068.8799999999999, "end": 1072.76, "text": " and do our operations on them.", "tokens": [293, 360, 527, 7705, 322, 552, 13], "temperature": 0.0, "avg_logprob": -0.11825861372389235, "compression_ratio": 1.6725663716814159, "no_speech_prob": 7.888994332461152e-06}, {"id": 192, "seek": 104878, "start": 1072.76, "end": 1073.76, "text": " And this is a lot faster.", "tokens": [400, 341, 307, 257, 688, 4663, 13], "temperature": 0.0, "avg_logprob": -0.11825861372389235, "compression_ratio": 1.6725663716814159, "no_speech_prob": 7.888994332461152e-06}, {"id": 193, "seek": 104878, "start": 1073.76, "end": 1076.54, "text": " This is 35 microseconds.", "tokens": [639, 307, 6976, 3123, 37841, 28750, 13], "temperature": 0.0, "avg_logprob": -0.11825861372389235, "compression_ratio": 1.6725663716814159, "no_speech_prob": 7.888994332461152e-06}, {"id": 194, "seek": 107654, "start": 1076.54, "end": 1080.8999999999999, "text": " So again, we've gone from milliseconds to microseconds, which is a thousand times speed", "tokens": [407, 797, 11, 321, 600, 2780, 490, 34184, 281, 3123, 37841, 28750, 11, 597, 307, 257, 4714, 1413, 3073], "temperature": 0.0, "avg_logprob": -0.3000520706176758, "compression_ratio": 1.2619047619047619, "no_speech_prob": 3.44645241057151e-06}, {"id": 195, "seek": 107654, "start": 1080.8999999999999, "end": 1084.12, "text": " up using NumPy.", "tokens": [493, 1228, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.3000520706176758, "compression_ratio": 1.2619047619047619, "no_speech_prob": 3.44645241057151e-06}, {"id": 196, "seek": 107654, "start": 1084.12, "end": 1091.44, "text": " So what's a downside to this NumPy approach?", "tokens": [407, 437, 311, 257, 25060, 281, 341, 22592, 47, 88, 3109, 30], "temperature": 0.0, "avg_logprob": -0.3000520706176758, "compression_ratio": 1.2619047619047619, "no_speech_prob": 3.44645241057151e-06}, {"id": 197, "seek": 107654, "start": 1091.44, "end": 1100.1599999999999, "text": " Any ideas?", "tokens": [2639, 3487, 30], "temperature": 0.0, "avg_logprob": -0.3000520706176758, "compression_ratio": 1.2619047619047619, "no_speech_prob": 3.44645241057151e-06}, {"id": 198, "seek": 110016, "start": 1100.16, "end": 1109.3200000000002, "text": " Sam?", "tokens": [4832, 30], "temperature": 0.0, "avg_logprob": -0.4159592373270384, "compression_ratio": 1.45625, "no_speech_prob": 2.5865994757623412e-05}, {"id": 199, "seek": 110016, "start": 1109.3200000000002, "end": 1113.68, "text": " Oh wait, Connor throw the ketchup box.", "tokens": [876, 1699, 11, 33133, 3507, 264, 29301, 2424, 13], "temperature": 0.0, "avg_logprob": -0.4159592373270384, "compression_ratio": 1.45625, "no_speech_prob": 2.5865994757623412e-05}, {"id": 200, "seek": 110016, "start": 1113.68, "end": 1114.68, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.4159592373270384, "compression_ratio": 1.45625, "no_speech_prob": 2.5865994757623412e-05}, {"id": 201, "seek": 110016, "start": 1114.68, "end": 1115.68, "text": " Poor localization.", "tokens": [23591, 2654, 2144, 13], "temperature": 0.0, "avg_logprob": -0.4159592373270384, "compression_ratio": 1.45625, "no_speech_prob": 2.5865994757623412e-05}, {"id": 202, "seek": 110016, "start": 1115.68, "end": 1116.68, "text": " Yes, poor localization.", "tokens": [1079, 11, 4716, 2654, 2144, 13], "temperature": 0.0, "avg_logprob": -0.4159592373270384, "compression_ratio": 1.45625, "no_speech_prob": 2.5865994757623412e-05}, {"id": 203, "seek": 110016, "start": 1116.68, "end": 1121.2, "text": " Do you want to say more about what that means?", "tokens": [1144, 291, 528, 281, 584, 544, 466, 437, 300, 1355, 30], "temperature": 0.0, "avg_logprob": -0.4159592373270384, "compression_ratio": 1.45625, "no_speech_prob": 2.5865994757623412e-05}, {"id": 204, "seek": 110016, "start": 1121.2, "end": 1122.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.4159592373270384, "compression_ratio": 1.45625, "no_speech_prob": 2.5865994757623412e-05}, {"id": 205, "seek": 110016, "start": 1122.2, "end": 1128.48, "text": " Instead of doing one operation on a single value from the x vector and a single value", "tokens": [7156, 295, 884, 472, 6916, 322, 257, 2167, 2158, 490, 264, 2031, 8062, 293, 257, 2167, 2158], "temperature": 0.0, "avg_logprob": -0.4159592373270384, "compression_ratio": 1.45625, "no_speech_prob": 2.5865994757623412e-05}, {"id": 206, "seek": 112848, "start": 1128.48, "end": 1135.16, "text": " from the y vector, and then writing once to z, it's going to read in all of x and all", "tokens": [490, 264, 288, 8062, 11, 293, 550, 3579, 1564, 281, 710, 11, 309, 311, 516, 281, 1401, 294, 439, 295, 2031, 293, 439], "temperature": 0.0, "avg_logprob": -0.31127389272054035, "compression_ratio": 1.65, "no_speech_prob": 1.7777907487470657e-05}, {"id": 207, "seek": 112848, "start": 1135.16, "end": 1138.64, "text": " of y at every stage and process.", "tokens": [295, 288, 412, 633, 3233, 293, 1399, 13], "temperature": 0.0, "avg_logprob": -0.31127389272054035, "compression_ratio": 1.65, "no_speech_prob": 1.7777907487470657e-05}, {"id": 208, "seek": 112848, "start": 1138.64, "end": 1140.76, "text": " Yes, exactly.", "tokens": [1079, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.31127389272054035, "compression_ratio": 1.65, "no_speech_prob": 1.7777907487470657e-05}, {"id": 209, "seek": 112848, "start": 1140.76, "end": 1144.64, "text": " So that I can get this back.", "tokens": [407, 300, 286, 393, 483, 341, 646, 13], "temperature": 0.0, "avg_logprob": -0.31127389272054035, "compression_ratio": 1.65, "no_speech_prob": 1.7777907487470657e-05}, {"id": 210, "seek": 112848, "start": 1144.64, "end": 1145.64, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.31127389272054035, "compression_ratio": 1.65, "no_speech_prob": 1.7777907487470657e-05}, {"id": 211, "seek": 112848, "start": 1145.64, "end": 1151.76, "text": " Yeah, so as Sam said, what's happening here is that it's going to read in all of x and", "tokens": [865, 11, 370, 382, 4832, 848, 11, 437, 311, 2737, 510, 307, 300, 309, 311, 516, 281, 1401, 294, 439, 295, 2031, 293], "temperature": 0.0, "avg_logprob": -0.31127389272054035, "compression_ratio": 1.65, "no_speech_prob": 1.7777907487470657e-05}, {"id": 212, "seek": 112848, "start": 1151.76, "end": 1155.56, "text": " y and if these are very long vectors, they might not all fit in cache.", "tokens": [288, 293, 498, 613, 366, 588, 938, 18875, 11, 436, 1062, 406, 439, 3318, 294, 19459, 13], "temperature": 0.0, "avg_logprob": -0.31127389272054035, "compression_ratio": 1.65, "no_speech_prob": 1.7777907487470657e-05}, {"id": 213, "seek": 115556, "start": 1155.56, "end": 1160.12, "text": " And so that would mean it's reading in the first part of x and the first part of y, doing", "tokens": [400, 370, 300, 576, 914, 309, 311, 3760, 294, 264, 700, 644, 295, 2031, 293, 264, 700, 644, 295, 288, 11, 884], "temperature": 0.0, "avg_logprob": -0.12728059702906117, "compression_ratio": 2.1144278606965172, "no_speech_prob": 2.0144580048508942e-05}, {"id": 214, "seek": 115556, "start": 1160.12, "end": 1166.2, "text": " this computation, then reading in a later part of x, later part of y, still on the same", "tokens": [341, 24903, 11, 550, 3760, 294, 257, 1780, 644, 295, 2031, 11, 1780, 644, 295, 288, 11, 920, 322, 264, 912], "temperature": 0.0, "avg_logprob": -0.12728059702906117, "compression_ratio": 2.1144278606965172, "no_speech_prob": 2.0144580048508942e-05}, {"id": 215, "seek": 115556, "start": 1166.2, "end": 1172.76, "text": " y, doing this x squared minus y times 55 computation, and then another section of x and y, and then", "tokens": [288, 11, 884, 341, 2031, 8889, 3175, 288, 1413, 12330, 24903, 11, 293, 550, 1071, 3541, 295, 2031, 293, 288, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.12728059702906117, "compression_ratio": 2.1144278606965172, "no_speech_prob": 2.0144580048508942e-05}, {"id": 216, "seek": 115556, "start": 1172.76, "end": 1175.24, "text": " it goes to the next line and it has to do all that again.", "tokens": [309, 1709, 281, 264, 958, 1622, 293, 309, 575, 281, 360, 439, 300, 797, 13], "temperature": 0.0, "avg_logprob": -0.12728059702906117, "compression_ratio": 2.1144278606965172, "no_speech_prob": 2.0144580048508942e-05}, {"id": 217, "seek": 115556, "start": 1175.24, "end": 1180.8799999999999, "text": " It has to read in the first part of the vector x, the first part of the vector y, read in", "tokens": [467, 575, 281, 1401, 294, 264, 700, 644, 295, 264, 8062, 2031, 11, 264, 700, 644, 295, 264, 8062, 288, 11, 1401, 294], "temperature": 0.0, "avg_logprob": -0.12728059702906117, "compression_ratio": 2.1144278606965172, "no_speech_prob": 2.0144580048508942e-05}, {"id": 218, "seek": 118088, "start": 1180.88, "end": 1186.5200000000002, "text": " the second part, and so on, if the vector is large enough to not fit all in cache.", "tokens": [264, 1150, 644, 11, 293, 370, 322, 11, 498, 264, 8062, 307, 2416, 1547, 281, 406, 3318, 439, 294, 19459, 13], "temperature": 0.0, "avg_logprob": -0.2086383976887182, "compression_ratio": 1.4801762114537445, "no_speech_prob": 1.6027950096031418e-06}, {"id": 219, "seek": 118088, "start": 1186.5200000000002, "end": 1195.5600000000002, "text": " And so, yeah, as Sam said, that's poor localization and can be more time consuming.", "tokens": [400, 370, 11, 1338, 11, 382, 4832, 848, 11, 300, 311, 4716, 2654, 2144, 293, 393, 312, 544, 565, 19867, 13], "temperature": 0.0, "avg_logprob": -0.2086383976887182, "compression_ratio": 1.4801762114537445, "no_speech_prob": 1.6027950096031418e-06}, {"id": 220, "seek": 118088, "start": 1195.5600000000002, "end": 1197.48, "text": " So then we get Numba.", "tokens": [407, 550, 321, 483, 426, 49353, 13], "temperature": 0.0, "avg_logprob": -0.2086383976887182, "compression_ratio": 1.4801762114537445, "no_speech_prob": 1.6027950096031418e-06}, {"id": 221, "seek": 118088, "start": 1197.48, "end": 1202.1200000000001, "text": " Numba offers several different decorators and we're just going to talk about two of", "tokens": [426, 49353, 7736, 2940, 819, 7919, 3391, 293, 321, 434, 445, 516, 281, 751, 466, 732, 295], "temperature": 0.0, "avg_logprob": -0.2086383976887182, "compression_ratio": 1.4801762114537445, "no_speech_prob": 1.6027950096031418e-06}, {"id": 222, "seek": 118088, "start": 1202.1200000000001, "end": 1203.16, "text": " them.", "tokens": [552, 13], "temperature": 0.0, "avg_logprob": -0.2086383976887182, "compression_ratio": 1.4801762114537445, "no_speech_prob": 1.6027950096031418e-06}, {"id": 223, "seek": 118088, "start": 1203.16, "end": 1208.24, "text": " One is at JIT, which stands for Just in Time Compilation.", "tokens": [1485, 307, 412, 508, 3927, 11, 597, 7382, 337, 1449, 294, 6161, 6620, 16067, 13], "temperature": 0.0, "avg_logprob": -0.2086383976887182, "compression_ratio": 1.4801762114537445, "no_speech_prob": 1.6027950096031418e-06}, {"id": 224, "seek": 120824, "start": 1208.24, "end": 1215.0, "text": " It's a little bit confusing because I think all of Numba's compilers are just in time,", "tokens": [467, 311, 257, 707, 857, 13181, 570, 286, 519, 439, 295, 426, 49353, 311, 715, 388, 433, 366, 445, 294, 565, 11], "temperature": 0.0, "avg_logprob": -0.13962603927752293, "compression_ratio": 1.538152610441767, "no_speech_prob": 2.9479792829079088e-06}, {"id": 225, "seek": 120824, "start": 1215.0, "end": 1218.92, "text": " but this is kind of the most general one.", "tokens": [457, 341, 307, 733, 295, 264, 881, 2674, 472, 13], "temperature": 0.0, "avg_logprob": -0.13962603927752293, "compression_ratio": 1.538152610441767, "no_speech_prob": 2.9479792829079088e-06}, {"id": 226, "seek": 120824, "start": 1218.92, "end": 1225.92, "text": " And then we'll also look at Vectorize, which doesn't require us to write a for loop.", "tokens": [400, 550, 321, 603, 611, 574, 412, 691, 20814, 1125, 11, 597, 1177, 380, 3651, 505, 281, 2464, 257, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.13962603927752293, "compression_ratio": 1.538152610441767, "no_speech_prob": 2.9479792829079088e-06}, {"id": 227, "seek": 120824, "start": 1225.92, "end": 1230.28, "text": " And so that's really useful when operating on vectors of the same size.", "tokens": [400, 370, 300, 311, 534, 4420, 562, 7447, 322, 18875, 295, 264, 912, 2744, 13], "temperature": 0.0, "avg_logprob": -0.13962603927752293, "compression_ratio": 1.538152610441767, "no_speech_prob": 2.9479792829079088e-06}, {"id": 228, "seek": 120824, "start": 1230.28, "end": 1236.0, "text": " And what Vectorize is doing is it's automatically writing something called a NumPy U-FUNC for", "tokens": [400, 437, 691, 20814, 1125, 307, 884, 307, 309, 311, 6772, 3579, 746, 1219, 257, 22592, 47, 88, 624, 12, 37, 3979, 34, 337], "temperature": 0.0, "avg_logprob": -0.13962603927752293, "compression_ratio": 1.538152610441767, "no_speech_prob": 2.9479792829079088e-06}, {"id": 229, "seek": 120824, "start": 1236.0, "end": 1237.0, "text": " us.", "tokens": [505, 13], "temperature": 0.0, "avg_logprob": -0.13962603927752293, "compression_ratio": 1.538152610441767, "no_speech_prob": 2.9479792829079088e-06}, {"id": 230, "seek": 123700, "start": 1237.0, "end": 1246.08, "text": " So a NumPy U-FUNC, that's a, actually I'll pull up the page.", "tokens": [407, 257, 22592, 47, 88, 624, 12, 37, 3979, 34, 11, 300, 311, 257, 11, 767, 286, 603, 2235, 493, 264, 3028, 13], "temperature": 0.0, "avg_logprob": -0.1963783452208613, "compression_ratio": 1.3976608187134503, "no_speech_prob": 7.646242011105642e-06}, {"id": 231, "seek": 123700, "start": 1246.08, "end": 1249.12, "text": " It's a universal function.", "tokens": [467, 311, 257, 11455, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1963783452208613, "compression_ratio": 1.3976608187134503, "no_speech_prob": 7.646242011105642e-06}, {"id": 232, "seek": 123700, "start": 1249.12, "end": 1256.44, "text": " And that's something that's kind of like lower level within NumPy and more difficult to write", "tokens": [400, 300, 311, 746, 300, 311, 733, 295, 411, 3126, 1496, 1951, 22592, 47, 88, 293, 544, 2252, 281, 2464], "temperature": 0.0, "avg_logprob": -0.1963783452208613, "compression_ratio": 1.3976608187134503, "no_speech_prob": 7.646242011105642e-06}, {"id": 233, "seek": 123700, "start": 1256.44, "end": 1259.96, "text": " yourself where you are writing in C to be creating those.", "tokens": [1803, 689, 291, 366, 3579, 294, 383, 281, 312, 4084, 729, 13], "temperature": 0.0, "avg_logprob": -0.1963783452208613, "compression_ratio": 1.3976608187134503, "no_speech_prob": 7.646242011105642e-06}, {"id": 234, "seek": 125996, "start": 1259.96, "end": 1268.68, "text": " And Numba will kind of create those for you if you use that Vectorize.", "tokens": [400, 426, 49353, 486, 733, 295, 1884, 729, 337, 291, 498, 291, 764, 300, 691, 20814, 1125, 13], "temperature": 0.0, "avg_logprob": -0.22286596642919335, "compression_ratio": 1.439153439153439, "no_speech_prob": 5.173830686544534e-06}, {"id": 235, "seek": 125996, "start": 1268.68, "end": 1271.88, "text": " We'll start though with at JIT.", "tokens": [492, 603, 722, 1673, 365, 412, 508, 3927, 13], "temperature": 0.0, "avg_logprob": -0.22286596642919335, "compression_ratio": 1.439153439153439, "no_speech_prob": 5.173830686544534e-06}, {"id": 236, "seek": 125996, "start": 1271.88, "end": 1279.32, "text": " So you just put the decorator on the line before you define your method.", "tokens": [407, 291, 445, 829, 264, 7919, 1639, 322, 264, 1622, 949, 291, 6964, 428, 3170, 13], "temperature": 0.0, "avg_logprob": -0.22286596642919335, "compression_ratio": 1.439153439153439, "no_speech_prob": 5.173830686544534e-06}, {"id": 237, "seek": 125996, "start": 1279.32, "end": 1281.52, "text": " We still have our for loop.", "tokens": [492, 920, 362, 527, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.22286596642919335, "compression_ratio": 1.439153439153439, "no_speech_prob": 5.173830686544534e-06}, {"id": 238, "seek": 125996, "start": 1281.52, "end": 1285.04, "text": " We're going through.", "tokens": [492, 434, 516, 807, 13], "temperature": 0.0, "avg_logprob": -0.22286596642919335, "compression_ratio": 1.439153439153439, "no_speech_prob": 5.173830686544534e-06}, {"id": 239, "seek": 125996, "start": 1285.04, "end": 1289.56, "text": " And now when we time it, it's six microseconds.", "tokens": [400, 586, 562, 321, 565, 309, 11, 309, 311, 2309, 3123, 37841, 28750, 13], "temperature": 0.0, "avg_logprob": -0.22286596642919335, "compression_ratio": 1.439153439153439, "no_speech_prob": 5.173830686544534e-06}, {"id": 240, "seek": 128956, "start": 1289.56, "end": 1294.3999999999999, "text": " So again, just to remember, the NumPy version was 36 microseconds.", "tokens": [407, 797, 11, 445, 281, 1604, 11, 264, 22592, 47, 88, 3037, 390, 8652, 3123, 37841, 28750, 13], "temperature": 0.0, "avg_logprob": -0.14174170927567917, "compression_ratio": 1.4816753926701571, "no_speech_prob": 7.889000698924065e-06}, {"id": 241, "seek": 128956, "start": 1294.3999999999999, "end": 1298.8, "text": " So this has been a six times speed up over NumPy.", "tokens": [407, 341, 575, 668, 257, 2309, 1413, 3073, 493, 670, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.14174170927567917, "compression_ratio": 1.4816753926701571, "no_speech_prob": 7.889000698924065e-06}, {"id": 242, "seek": 128956, "start": 1298.8, "end": 1308.04, "text": " And NumPy was a thousand times speed up over pure Python.", "tokens": [400, 22592, 47, 88, 390, 257, 4714, 1413, 3073, 493, 670, 6075, 15329, 13], "temperature": 0.0, "avg_logprob": -0.14174170927567917, "compression_ratio": 1.4816753926701571, "no_speech_prob": 7.889000698924065e-06}, {"id": 243, "seek": 128956, "start": 1308.04, "end": 1314.98, "text": " And then with at Vectorize, this is kind of the same as the at JIT version, only we don't", "tokens": [400, 550, 365, 412, 691, 20814, 1125, 11, 341, 307, 733, 295, 264, 912, 382, 264, 412, 508, 3927, 3037, 11, 787, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.14174170927567917, "compression_ratio": 1.4816753926701571, "no_speech_prob": 7.889000698924065e-06}, {"id": 244, "seek": 128956, "start": 1314.98, "end": 1317.2, "text": " have the for loop.", "tokens": [362, 264, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.14174170927567917, "compression_ratio": 1.4816753926701571, "no_speech_prob": 7.889000698924065e-06}, {"id": 245, "seek": 131720, "start": 1317.2, "end": 1322.92, "text": " And there we get 5.82 microseconds, which is kind of almost the same.", "tokens": [400, 456, 321, 483, 1025, 13, 32848, 3123, 37841, 28750, 11, 597, 307, 733, 295, 1920, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.18248003323872883, "compression_ratio": 1.58364312267658, "no_speech_prob": 3.288685320512741e-06}, {"id": 246, "seek": 131720, "start": 1322.92, "end": 1327.1200000000001, "text": " Although I think this is cleaner to read because you don't have the for loop.", "tokens": [5780, 286, 519, 341, 307, 16532, 281, 1401, 570, 291, 500, 380, 362, 264, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.18248003323872883, "compression_ratio": 1.58364312267658, "no_speech_prob": 3.288685320512741e-06}, {"id": 247, "seek": 131720, "start": 1327.1200000000001, "end": 1332.0, "text": " However, if you were doing something where you weren't acting on vectors of the same", "tokens": [2908, 11, 498, 291, 645, 884, 746, 689, 291, 4999, 380, 6577, 322, 18875, 295, 264, 912], "temperature": 0.0, "avg_logprob": -0.18248003323872883, "compression_ratio": 1.58364312267658, "no_speech_prob": 3.288685320512741e-06}, {"id": 248, "seek": 131720, "start": 1332.0, "end": 1336.48, "text": " size to kind of get back a vector of that size, at JIT would be good to use.", "tokens": [2744, 281, 733, 295, 483, 646, 257, 8062, 295, 300, 2744, 11, 412, 508, 3927, 576, 312, 665, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.18248003323872883, "compression_ratio": 1.58364312267658, "no_speech_prob": 3.288685320512741e-06}, {"id": 249, "seek": 131720, "start": 1336.48, "end": 1343.68, "text": " So I just want to say NumPy, or sorry, Numba is amazing because this is really fast and", "tokens": [407, 286, 445, 528, 281, 584, 22592, 47, 88, 11, 420, 2597, 11, 426, 49353, 307, 2243, 570, 341, 307, 534, 2370, 293], "temperature": 0.0, "avg_logprob": -0.18248003323872883, "compression_ratio": 1.58364312267658, "no_speech_prob": 3.288685320512741e-06}, {"id": 250, "seek": 131720, "start": 1343.68, "end": 1345.32, "text": " fairly simple to write.", "tokens": [6457, 2199, 281, 2464, 13], "temperature": 0.0, "avg_logprob": -0.18248003323872883, "compression_ratio": 1.58364312267658, "no_speech_prob": 3.288685320512741e-06}, {"id": 251, "seek": 131720, "start": 1345.32, "end": 1346.32, "text": " Sam?", "tokens": [4832, 30], "temperature": 0.0, "avg_logprob": -0.18248003323872883, "compression_ratio": 1.58364312267658, "no_speech_prob": 3.288685320512741e-06}, {"id": 252, "seek": 134632, "start": 1346.32, "end": 1347.32, "text": " Oh, yeah.", "tokens": [876, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.28335161711040296, "compression_ratio": 1.6553398058252426, "no_speech_prob": 2.8855272830696777e-05}, {"id": 253, "seek": 134632, "start": 1347.32, "end": 1356.32, "text": " Can you do functions, can you do broadcasting in this context?", "tokens": [1664, 291, 360, 6828, 11, 393, 291, 360, 30024, 294, 341, 4319, 30], "temperature": 0.0, "avg_logprob": -0.28335161711040296, "compression_ratio": 1.6553398058252426, "no_speech_prob": 2.8855272830696777e-05}, {"id": 254, "seek": 134632, "start": 1356.32, "end": 1364.04, "text": " Yes, and so actually a Ufunc, so really what Vectorize is doing is creating this NumPy", "tokens": [1079, 11, 293, 370, 767, 257, 624, 15930, 66, 11, 370, 534, 437, 691, 20814, 1125, 307, 884, 307, 4084, 341, 22592, 47, 88], "temperature": 0.0, "avg_logprob": -0.28335161711040296, "compression_ratio": 1.6553398058252426, "no_speech_prob": 2.8855272830696777e-05}, {"id": 255, "seek": 134632, "start": 1364.04, "end": 1367.04, "text": " Ufunc, which is required to have broadcasting for you.", "tokens": [624, 15930, 66, 11, 597, 307, 4739, 281, 362, 30024, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.28335161711040296, "compression_ratio": 1.6553398058252426, "no_speech_prob": 2.8855272830696777e-05}, {"id": 256, "seek": 134632, "start": 1367.04, "end": 1370.04, "text": " So yeah, this would have broadcasting.", "tokens": [407, 1338, 11, 341, 576, 362, 30024, 13], "temperature": 0.0, "avg_logprob": -0.28335161711040296, "compression_ratio": 1.6553398058252426, "no_speech_prob": 2.8855272830696777e-05}, {"id": 257, "seek": 134632, "start": 1370.04, "end": 1374.4199999999998, "text": " And that's a benefit of Numba is you're not having to code the broadcasting yourself to", "tokens": [400, 300, 311, 257, 5121, 295, 426, 49353, 307, 291, 434, 406, 1419, 281, 3089, 264, 30024, 1803, 281], "temperature": 0.0, "avg_logprob": -0.28335161711040296, "compression_ratio": 1.6553398058252426, "no_speech_prob": 2.8855272830696777e-05}, {"id": 258, "seek": 137442, "start": 1374.42, "end": 1377.68, "text": " create the Ufunc.", "tokens": [1884, 264, 624, 15930, 66, 13], "temperature": 0.0, "avg_logprob": -0.391949489198882, "compression_ratio": 1.3834586466165413, "no_speech_prob": 4.19765965489205e-05}, {"id": 259, "seek": 137442, "start": 1377.68, "end": 1394.68, "text": " So the function here, the Vectorize one, looks almost identical to the NumPy one.", "tokens": [407, 264, 2445, 510, 11, 264, 691, 20814, 1125, 472, 11, 1542, 1920, 14800, 281, 264, 22592, 47, 88, 472, 13], "temperature": 0.0, "avg_logprob": -0.391949489198882, "compression_ratio": 1.3834586466165413, "no_speech_prob": 4.19765965489205e-05}, {"id": 260, "seek": 137442, "start": 1394.68, "end": 1395.68, "text": " Correct.", "tokens": [12753, 13], "temperature": 0.0, "avg_logprob": -0.391949489198882, "compression_ratio": 1.3834586466165413, "no_speech_prob": 4.19765965489205e-05}, {"id": 261, "seek": 137442, "start": 1395.68, "end": 1402.68, "text": " And what is it, like the locality of this one is better than the other one.", "tokens": [400, 437, 307, 309, 11, 411, 264, 1628, 1860, 295, 341, 472, 307, 1101, 813, 264, 661, 472, 13], "temperature": 0.0, "avg_logprob": -0.391949489198882, "compression_ratio": 1.3834586466165413, "no_speech_prob": 4.19765965489205e-05}, {"id": 262, "seek": 140268, "start": 1402.68, "end": 1409.96, "text": " Yes, and that's because Numba is kind of has smarter optimization than NumPy.", "tokens": [1079, 11, 293, 300, 311, 570, 426, 49353, 307, 733, 295, 575, 20294, 19618, 813, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.11864957622453279, "compression_ratio": 1.58008658008658, "no_speech_prob": 6.962056431802921e-06}, {"id": 263, "seek": 140268, "start": 1409.96, "end": 1416.8, "text": " So the issue with NumPy is it's not able to look ahead, like NumPy is still going kind", "tokens": [407, 264, 2734, 365, 22592, 47, 88, 307, 309, 311, 406, 1075, 281, 574, 2286, 11, 411, 22592, 47, 88, 307, 920, 516, 733], "temperature": 0.0, "avg_logprob": -0.11864957622453279, "compression_ratio": 1.58008658008658, "no_speech_prob": 6.962056431802921e-06}, {"id": 264, "seek": 140268, "start": 1416.8, "end": 1420.28, "text": " of line by line through this algorithm.", "tokens": [295, 1622, 538, 1622, 807, 341, 9284, 13], "temperature": 0.0, "avg_logprob": -0.11864957622453279, "compression_ratio": 1.58008658008658, "no_speech_prob": 6.962056431802921e-06}, {"id": 265, "seek": 140268, "start": 1420.28, "end": 1426.44, "text": " So it's basically kind of completing, you know, x squared minus y times 55 before it", "tokens": [407, 309, 311, 1936, 733, 295, 19472, 11, 291, 458, 11, 2031, 8889, 3175, 288, 1413, 12330, 949, 309], "temperature": 0.0, "avg_logprob": -0.11864957622453279, "compression_ratio": 1.58008658008658, "no_speech_prob": 6.962056431802921e-06}, {"id": 266, "seek": 140268, "start": 1426.44, "end": 1431.3600000000001, "text": " even looks at this like, oh, now I have to do something with x and y again.", "tokens": [754, 1542, 412, 341, 411, 11, 1954, 11, 586, 286, 362, 281, 360, 746, 365, 2031, 293, 288, 797, 13], "temperature": 0.0, "avg_logprob": -0.11864957622453279, "compression_ratio": 1.58008658008658, "no_speech_prob": 6.962056431802921e-06}, {"id": 267, "seek": 143136, "start": 1431.36, "end": 1438.9199999999998, "text": " Because this decorator is like for the whole method, Numba is optimizing kind of that whole", "tokens": [1436, 341, 7919, 1639, 307, 411, 337, 264, 1379, 3170, 11, 426, 49353, 307, 40425, 733, 295, 300, 1379], "temperature": 0.0, "avg_logprob": -0.22860282506698218, "compression_ratio": 1.452127659574468, "no_speech_prob": 8.139515557559207e-06}, {"id": 268, "seek": 143136, "start": 1438.9199999999998, "end": 1439.9199999999998, "text": " method as a unit.", "tokens": [3170, 382, 257, 4985, 13], "temperature": 0.0, "avg_logprob": -0.22860282506698218, "compression_ratio": 1.452127659574468, "no_speech_prob": 8.139515557559207e-06}, {"id": 269, "seek": 143136, "start": 1439.9199999999998, "end": 1444.12, "text": " So it's got a little bit of, you know, this like bigger picture of like, hey, I'm going", "tokens": [407, 309, 311, 658, 257, 707, 857, 295, 11, 291, 458, 11, 341, 411, 3801, 3036, 295, 411, 11, 4177, 11, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.22860282506698218, "compression_ratio": 1.452127659574468, "no_speech_prob": 8.139515557559207e-06}, {"id": 270, "seek": 143136, "start": 1444.12, "end": 1448.1999999999998, "text": " to be using x and y again on the next line.", "tokens": [281, 312, 1228, 2031, 293, 288, 797, 322, 264, 958, 1622, 13], "temperature": 0.0, "avg_logprob": -0.22860282506698218, "compression_ratio": 1.452127659574468, "no_speech_prob": 8.139515557559207e-06}, {"id": 271, "seek": 143136, "start": 1448.1999999999998, "end": 1457.52, "text": " Any other questions about this?", "tokens": [2639, 661, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.22860282506698218, "compression_ratio": 1.452127659574468, "no_speech_prob": 8.139515557559207e-06}, {"id": 272, "seek": 145752, "start": 1457.52, "end": 1464.4, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.45749127314640925, "compression_ratio": 1.3355263157894737, "no_speech_prob": 6.921547173988074e-05}, {"id": 273, "seek": 145752, "start": 1464.4, "end": 1469.16, "text": " So let's, oh, wait, wait for the microphone.", "tokens": [407, 718, 311, 11, 1954, 11, 1699, 11, 1699, 337, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.45749127314640925, "compression_ratio": 1.3355263157894737, "no_speech_prob": 6.921547173988074e-05}, {"id": 274, "seek": 145752, "start": 1469.16, "end": 1472.16, "text": " Just curious.", "tokens": [1449, 6369, 13], "temperature": 0.0, "avg_logprob": -0.45749127314640925, "compression_ratio": 1.3355263157894737, "no_speech_prob": 6.921547173988074e-05}, {"id": 275, "seek": 145752, "start": 1472.16, "end": 1481.92, "text": " So it looks like Numba is a lot quicker and it's very easy to use.", "tokens": [407, 309, 1542, 411, 426, 49353, 307, 257, 688, 16255, 293, 309, 311, 588, 1858, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.45749127314640925, "compression_ratio": 1.3355263157894737, "no_speech_prob": 6.921547173988074e-05}, {"id": 276, "seek": 145752, "start": 1481.92, "end": 1486.72, "text": " Is there any reason sometimes we still use NumPy instead of Numba?", "tokens": [1119, 456, 604, 1778, 2171, 321, 920, 764, 22592, 47, 88, 2602, 295, 426, 49353, 30], "temperature": 0.0, "avg_logprob": -0.45749127314640925, "compression_ratio": 1.3355263157894737, "no_speech_prob": 6.921547173988074e-05}, {"id": 277, "seek": 148672, "start": 1486.72, "end": 1491.8, "text": " So I think that's a good question.", "tokens": [407, 286, 519, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2404724372612251, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.1125231139885727e-05}, {"id": 278, "seek": 148672, "start": 1491.8, "end": 1498.92, "text": " NumPy is going to have a lot more specialized methods written, like NumPy is a much larger", "tokens": [22592, 47, 88, 307, 516, 281, 362, 257, 688, 544, 19813, 7150, 3720, 11, 411, 22592, 47, 88, 307, 257, 709, 4833], "temperature": 0.0, "avg_logprob": -0.2404724372612251, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.1125231139885727e-05}, {"id": 279, "seek": 148672, "start": 1498.92, "end": 1500.96, "text": " library.", "tokens": [6405, 13], "temperature": 0.0, "avg_logprob": -0.2404724372612251, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.1125231139885727e-05}, {"id": 280, "seek": 148672, "start": 1500.96, "end": 1506.6000000000001, "text": " So I think it's possible that if you were doing kind of like some fancy specific NumPy", "tokens": [407, 286, 519, 309, 311, 1944, 300, 498, 291, 645, 884, 733, 295, 411, 512, 10247, 2685, 22592, 47, 88], "temperature": 0.0, "avg_logprob": -0.2404724372612251, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.1125231139885727e-05}, {"id": 281, "seek": 148672, "start": 1506.6000000000001, "end": 1510.76, "text": " method that Numba may not have that written.", "tokens": [3170, 300, 426, 49353, 815, 406, 362, 300, 3720, 13], "temperature": 0.0, "avg_logprob": -0.2404724372612251, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.1125231139885727e-05}, {"id": 282, "seek": 148672, "start": 1510.76, "end": 1511.76, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.2404724372612251, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.1125231139885727e-05}, {"id": 283, "seek": 148672, "start": 1511.76, "end": 1512.76, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.2404724372612251, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.1125231139885727e-05}, {"id": 284, "seek": 148672, "start": 1512.76, "end": 1513.76, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2404724372612251, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.1125231139885727e-05}, {"id": 285, "seek": 148672, "start": 1513.76, "end": 1514.76, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2404724372612251, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.1125231139885727e-05}, {"id": 286, "seek": 151476, "start": 1514.76, "end": 1517.76, "text": " And you can use them together.", "tokens": [400, 291, 393, 764, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.24235081281818327, "compression_ratio": 1.3806451612903226, "no_speech_prob": 1.7501984984846786e-05}, {"id": 287, "seek": 151476, "start": 1517.76, "end": 1518.76, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.24235081281818327, "compression_ratio": 1.3806451612903226, "no_speech_prob": 1.7501984984846786e-05}, {"id": 288, "seek": 151476, "start": 1518.76, "end": 1519.76, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.24235081281818327, "compression_ratio": 1.3806451612903226, "no_speech_prob": 1.7501984984846786e-05}, {"id": 289, "seek": 151476, "start": 1519.76, "end": 1525.4, "text": " And I really feel like this is something that Numba just doesn't seem that widely known", "tokens": [400, 286, 534, 841, 411, 341, 307, 746, 300, 426, 49353, 445, 1177, 380, 1643, 300, 13371, 2570], "temperature": 0.0, "avg_logprob": -0.24235081281818327, "compression_ratio": 1.3806451612903226, "no_speech_prob": 1.7501984984846786e-05}, {"id": 290, "seek": 151476, "start": 1525.4, "end": 1534.24, "text": " to exist, but I think it's a relatively simple tool that gives you a nice speed up.", "tokens": [281, 2514, 11, 457, 286, 519, 309, 311, 257, 7226, 2199, 2290, 300, 2709, 291, 257, 1481, 3073, 493, 13], "temperature": 0.0, "avg_logprob": -0.24235081281818327, "compression_ratio": 1.3806451612903226, "no_speech_prob": 1.7501984984846786e-05}, {"id": 291, "seek": 153424, "start": 1534.24, "end": 1545.16, "text": " Are the arguments to this function NumPy arrays or can they be lists?", "tokens": [2014, 264, 12869, 281, 341, 2445, 22592, 47, 88, 41011, 420, 393, 436, 312, 14511, 30], "temperature": 0.0, "avg_logprob": -0.2376368908171958, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.30065054690931e-05}, {"id": 292, "seek": 153424, "start": 1545.16, "end": 1549.6, "text": " Like when you use vectorize, does it automatically treat all of the lists like NumPy arrays?", "tokens": [1743, 562, 291, 764, 8062, 1125, 11, 775, 309, 6772, 2387, 439, 295, 264, 14511, 411, 22592, 47, 88, 41011, 30], "temperature": 0.0, "avg_logprob": -0.2376368908171958, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.30065054690931e-05}, {"id": 293, "seek": 153424, "start": 1549.6, "end": 1551.8, "text": " I'm not sure about that one.", "tokens": [286, 478, 406, 988, 466, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.2376368908171958, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.30065054690931e-05}, {"id": 294, "seek": 153424, "start": 1551.8, "end": 1556.92, "text": " And the other, I want to say Numba has more functionality that I'm not getting into, but", "tokens": [400, 264, 661, 11, 286, 528, 281, 584, 426, 49353, 575, 544, 14980, 300, 286, 478, 406, 1242, 666, 11, 457], "temperature": 0.0, "avg_logprob": -0.2376368908171958, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.30065054690931e-05}, {"id": 295, "seek": 153424, "start": 1556.92, "end": 1562.28, "text": " Numba gives you the option of explicitly passing types or like explicitly setting like this", "tokens": [426, 49353, 2709, 291, 264, 3614, 295, 20803, 8437, 3467, 420, 411, 20803, 3287, 411, 341], "temperature": 0.0, "avg_logprob": -0.2376368908171958, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.30065054690931e-05}, {"id": 296, "seek": 156228, "start": 1562.28, "end": 1565.92, "text": " is what the types of the method will be.", "tokens": [307, 437, 264, 3467, 295, 264, 3170, 486, 312, 13], "temperature": 0.0, "avg_logprob": -0.3377096469585712, "compression_ratio": 1.317829457364341, "no_speech_prob": 1.9832739781122655e-05}, {"id": 297, "seek": 156228, "start": 1565.92, "end": 1567.92, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3377096469585712, "compression_ratio": 1.317829457364341, "no_speech_prob": 1.9832739781122655e-05}, {"id": 298, "seek": 156228, "start": 1567.92, "end": 1579.96, "text": " So let's go back to the problem we were interested in of polynomial features.", "tokens": [407, 718, 311, 352, 646, 281, 264, 1154, 321, 645, 3102, 294, 295, 26110, 4122, 13], "temperature": 0.0, "avg_logprob": -0.3377096469585712, "compression_ratio": 1.317829457364341, "no_speech_prob": 1.9832739781122655e-05}, {"id": 299, "seek": 156228, "start": 1579.96, "end": 1584.72, "text": " So we're adding our at-jit decorator in here.", "tokens": [407, 321, 434, 5127, 527, 412, 12, 73, 270, 7919, 1639, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.3377096469585712, "compression_ratio": 1.317829457364341, "no_speech_prob": 1.9832739781122655e-05}, {"id": 300, "seek": 158472, "start": 1584.72, "end": 1596.1200000000001, "text": " And actually let me go to, this gives you different options.", "tokens": [400, 767, 718, 385, 352, 281, 11, 341, 2709, 291, 819, 3956, 13], "temperature": 0.0, "avg_logprob": -0.2992592146902373, "compression_ratio": 1.161904761904762, "no_speech_prob": 9.665726793173235e-06}, {"id": 301, "seek": 158472, "start": 1596.1200000000001, "end": 1611.28, "text": " No Python disables the use of Py objects in Python API calls.", "tokens": [883, 15329, 717, 2965, 264, 764, 295, 9953, 6565, 294, 15329, 9362, 5498, 13], "temperature": 0.0, "avg_logprob": -0.2992592146902373, "compression_ratio": 1.161904761904762, "no_speech_prob": 9.665726793173235e-06}, {"id": 302, "seek": 161128, "start": 1611.28, "end": 1616.0, "text": " And there's also, I'll show an example of this later, but there is also, you could pass", "tokens": [400, 456, 311, 611, 11, 286, 603, 855, 364, 1365, 295, 341, 1780, 11, 457, 456, 307, 611, 11, 291, 727, 1320], "temperature": 0.0, "avg_logprob": -0.21220429302894905, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.4284981261880603e-05}, {"id": 303, "seek": 161128, "start": 1616.0, "end": 1621.08, "text": " the types in, I believe here, if you want to say explicitly what kind of types the method's", "tokens": [264, 3467, 294, 11, 286, 1697, 510, 11, 498, 291, 528, 281, 584, 20803, 437, 733, 295, 3467, 264, 3170, 311], "temperature": 0.0, "avg_logprob": -0.21220429302894905, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.4284981261880603e-05}, {"id": 304, "seek": 161128, "start": 1621.08, "end": 1622.08, "text": " taking.", "tokens": [1940, 13], "temperature": 0.0, "avg_logprob": -0.21220429302894905, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.4284981261880603e-05}, {"id": 305, "seek": 161128, "start": 1622.08, "end": 1641.12, "text": " So I wanted to just kind of show the punchline of when we use this method.", "tokens": [407, 286, 1415, 281, 445, 733, 295, 855, 264, 8135, 1889, 295, 562, 321, 764, 341, 3170, 13], "temperature": 0.0, "avg_logprob": -0.21220429302894905, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.4284981261880603e-05}, {"id": 306, "seek": 164112, "start": 1641.12, "end": 1649.8799999999999, "text": " Our polynomial vectorization, the time is seven microseconds.", "tokens": [2621, 26110, 8062, 2144, 11, 264, 565, 307, 3407, 3123, 37841, 28750, 13], "temperature": 0.0, "avg_logprob": -0.1029302082704694, "compression_ratio": 1.6294642857142858, "no_speech_prob": 7.527771686000051e-06}, {"id": 307, "seek": 164112, "start": 1649.8799999999999, "end": 1655.1999999999998, "text": " And so to compare that to what we were getting above with the scikit-learn of polynomial", "tokens": [400, 370, 281, 6794, 300, 281, 437, 321, 645, 1242, 3673, 365, 264, 2180, 22681, 12, 306, 1083, 295, 26110], "temperature": 0.0, "avg_logprob": -0.1029302082704694, "compression_ratio": 1.6294642857142858, "no_speech_prob": 7.527771686000051e-06}, {"id": 308, "seek": 164112, "start": 1655.1999999999998, "end": 1659.56, "text": " features, we were getting 600 microseconds.", "tokens": [4122, 11, 321, 645, 1242, 11849, 3123, 37841, 28750, 13], "temperature": 0.0, "avg_logprob": -0.1029302082704694, "compression_ratio": 1.6294642857142858, "no_speech_prob": 7.527771686000051e-06}, {"id": 309, "seek": 164112, "start": 1659.56, "end": 1665.0, "text": " So this is about a hundred times speed up, which is pretty impressive because everything", "tokens": [407, 341, 307, 466, 257, 3262, 1413, 3073, 493, 11, 597, 307, 1238, 8992, 570, 1203], "temperature": 0.0, "avg_logprob": -0.1029302082704694, "compression_ratio": 1.6294642857142858, "no_speech_prob": 7.527771686000051e-06}, {"id": 310, "seek": 164112, "start": 1665.0, "end": 1670.08, "text": " in scikit-learn has been kind of written by experts and pretty heavily optimized.", "tokens": [294, 2180, 22681, 12, 306, 1083, 575, 668, 733, 295, 3720, 538, 8572, 293, 1238, 10950, 26941, 13], "temperature": 0.0, "avg_logprob": -0.1029302082704694, "compression_ratio": 1.6294642857142858, "no_speech_prob": 7.527771686000051e-06}, {"id": 311, "seek": 167008, "start": 1670.08, "end": 1675.1999999999998, "text": " And so here, and I should maybe go through a little bit.", "tokens": [400, 370, 510, 11, 293, 286, 820, 1310, 352, 807, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.11455243930482027, "compression_ratio": 1.475609756097561, "no_speech_prob": 1.2029207027808297e-05}, {"id": 312, "seek": 167008, "start": 1675.1999999999998, "end": 1683.32, "text": " Here basically what we're doing is just kind of looping through our features and taking", "tokens": [1692, 1936, 437, 321, 434, 884, 307, 445, 733, 295, 6367, 278, 807, 527, 4122, 293, 1940], "temperature": 0.0, "avg_logprob": -0.11455243930482027, "compression_ratio": 1.475609756097561, "no_speech_prob": 1.2029207027808297e-05}, {"id": 313, "seek": 167008, "start": 1683.32, "end": 1690.32, "text": " all possible combinations, multiplying them together in order to increase our number of", "tokens": [439, 1944, 21267, 11, 30955, 552, 1214, 294, 1668, 281, 3488, 527, 1230, 295], "temperature": 0.0, "avg_logprob": -0.11455243930482027, "compression_ratio": 1.475609756097561, "no_speech_prob": 1.2029207027808297e-05}, {"id": 314, "seek": 167008, "start": 1690.32, "end": 1691.32, "text": " features.", "tokens": [4122, 13], "temperature": 0.0, "avg_logprob": -0.11455243930482027, "compression_ratio": 1.475609756097561, "no_speech_prob": 1.2029207027808297e-05}, {"id": 315, "seek": 169132, "start": 1691.32, "end": 1706.32, "text": " So this is just adding something, kind of adding all the squared terms.", "tokens": [407, 341, 307, 445, 5127, 746, 11, 733, 295, 5127, 439, 264, 8889, 2115, 13], "temperature": 0.0, "avg_logprob": -0.2676593312677347, "compression_ratio": 1.3941605839416058, "no_speech_prob": 3.2886937333387323e-06}, {"id": 316, "seek": 169132, "start": 1706.32, "end": 1714.0, "text": " Hey, yeah, so a big speed up there.", "tokens": [1911, 11, 1338, 11, 370, 257, 955, 3073, 493, 456, 13], "temperature": 0.0, "avg_logprob": -0.2676593312677347, "compression_ratio": 1.3941605839416058, "no_speech_prob": 3.2886937333387323e-06}, {"id": 317, "seek": 169132, "start": 1714.0, "end": 1718.52, "text": " And then something I skipped over, let me go back to this, is I basically wanted to", "tokens": [400, 550, 746, 286, 30193, 670, 11, 718, 385, 352, 646, 281, 341, 11, 307, 286, 1936, 1415, 281], "temperature": 0.0, "avg_logprob": -0.2676593312677347, "compression_ratio": 1.3941605839416058, "no_speech_prob": 3.2886937333387323e-06}, {"id": 318, "seek": 171852, "start": 1718.52, "end": 1722.44, "text": " talk a little bit about row major versus column major storage.", "tokens": [751, 257, 707, 857, 466, 5386, 2563, 5717, 7738, 2563, 6725, 13], "temperature": 0.0, "avg_logprob": -0.13582730293273926, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.4285260476754047e-05}, {"id": 319, "seek": 171852, "start": 1722.44, "end": 1731.96, "text": " And I found there's a nice blog post on it.", "tokens": [400, 286, 1352, 456, 311, 257, 1481, 6968, 2183, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.13582730293273926, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.4285260476754047e-05}, {"id": 320, "seek": 171852, "start": 1731.96, "end": 1737.96, "text": " And so the idea of row major storage is you're storing things by rows.", "tokens": [400, 370, 264, 1558, 295, 5386, 2563, 6725, 307, 291, 434, 26085, 721, 538, 13241, 13], "temperature": 0.0, "avg_logprob": -0.13582730293273926, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.4285260476754047e-05}, {"id": 321, "seek": 171852, "start": 1737.96, "end": 1740.36, "text": " And column major, you're storing by columns.", "tokens": [400, 7738, 2563, 11, 291, 434, 26085, 538, 13766, 13], "temperature": 0.0, "avg_logprob": -0.13582730293273926, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.4285260476754047e-05}, {"id": 322, "seek": 174036, "start": 1740.36, "end": 1751.24, "text": " So here kind of first column, second column, third column.", "tokens": [407, 510, 733, 295, 700, 7738, 11, 1150, 7738, 11, 2636, 7738, 13], "temperature": 0.0, "avg_logprob": -0.14654481009151157, "compression_ratio": 1.693467336683417, "no_speech_prob": 1.6701074855518527e-05}, {"id": 323, "seek": 174036, "start": 1751.24, "end": 1756.32, "text": " And so it's important to know, or it can be very important to know how your data is stored.", "tokens": [400, 370, 309, 311, 1021, 281, 458, 11, 420, 309, 393, 312, 588, 1021, 281, 458, 577, 428, 1412, 307, 12187, 13], "temperature": 0.0, "avg_logprob": -0.14654481009151157, "compression_ratio": 1.693467336683417, "no_speech_prob": 1.6701074855518527e-05}, {"id": 324, "seek": 174036, "start": 1756.32, "end": 1762.76, "text": " You know, and in many ways it doesn't matter beyond just knowing because it makes it easier", "tokens": [509, 458, 11, 293, 294, 867, 2098, 309, 1177, 380, 1871, 4399, 445, 5276, 570, 309, 1669, 309, 3571], "temperature": 0.0, "avg_logprob": -0.14654481009151157, "compression_ratio": 1.693467336683417, "no_speech_prob": 1.6701074855518527e-05}, {"id": 325, "seek": 174036, "start": 1762.76, "end": 1763.76, "text": " kind of to access your data.", "tokens": [733, 295, 281, 2105, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.14654481009151157, "compression_ratio": 1.693467336683417, "no_speech_prob": 1.6701074855518527e-05}, {"id": 326, "seek": 174036, "start": 1763.76, "end": 1766.7199999999998, "text": " If you're using column major, it's easier to access it by column.", "tokens": [759, 291, 434, 1228, 7738, 2563, 11, 309, 311, 3571, 281, 2105, 309, 538, 7738, 13], "temperature": 0.0, "avg_logprob": -0.14654481009151157, "compression_ratio": 1.693467336683417, "no_speech_prob": 1.6701074855518527e-05}, {"id": 327, "seek": 176672, "start": 1766.72, "end": 1770.56, "text": " If you're using row major, it's easier to access it by row.", "tokens": [759, 291, 434, 1228, 5386, 2563, 11, 309, 311, 3571, 281, 2105, 309, 538, 5386, 13], "temperature": 0.0, "avg_logprob": -0.14946884219929324, "compression_ratio": 1.7926829268292683, "no_speech_prob": 1.5689067367929965e-05}, {"id": 328, "seek": 176672, "start": 1770.56, "end": 1772.0, "text": " Actually, yeah, I should show.", "tokens": [5135, 11, 1338, 11, 286, 820, 855, 13], "temperature": 0.0, "avg_logprob": -0.14946884219929324, "compression_ratio": 1.7926829268292683, "no_speech_prob": 1.5689067367929965e-05}, {"id": 329, "seek": 176672, "start": 1772.0, "end": 1778.3600000000001, "text": " So here with row major storage, if you wanted to access the first column, you would have", "tokens": [407, 510, 365, 5386, 2563, 6725, 11, 498, 291, 1415, 281, 2105, 264, 700, 7738, 11, 291, 576, 362], "temperature": 0.0, "avg_logprob": -0.14946884219929324, "compression_ratio": 1.7926829268292683, "no_speech_prob": 1.5689067367929965e-05}, {"id": 330, "seek": 176672, "start": 1778.3600000000001, "end": 1782.72, "text": " to kind of grab this element and then this element and then this element, you know, which", "tokens": [281, 733, 295, 4444, 341, 4478, 293, 550, 341, 4478, 293, 550, 341, 4478, 11, 291, 458, 11, 597], "temperature": 0.0, "avg_logprob": -0.14946884219929324, "compression_ratio": 1.7926829268292683, "no_speech_prob": 1.5689067367929965e-05}, {"id": 331, "seek": 176672, "start": 1782.72, "end": 1785.1200000000001, "text": " are not contiguous in memory.", "tokens": [366, 406, 660, 30525, 294, 4675, 13], "temperature": 0.0, "avg_logprob": -0.14946884219929324, "compression_ratio": 1.7926829268292683, "no_speech_prob": 1.5689067367929965e-05}, {"id": 332, "seek": 176672, "start": 1785.1200000000001, "end": 1789.04, "text": " Whereas column major storage, it's easy to get a particular column because they're all", "tokens": [13813, 7738, 2563, 6725, 11, 309, 311, 1858, 281, 483, 257, 1729, 7738, 570, 436, 434, 439], "temperature": 0.0, "avg_logprob": -0.14946884219929324, "compression_ratio": 1.7926829268292683, "no_speech_prob": 1.5689067367929965e-05}, {"id": 333, "seek": 176672, "start": 1789.04, "end": 1790.04, "text": " next to each other.", "tokens": [958, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.14946884219929324, "compression_ratio": 1.7926829268292683, "no_speech_prob": 1.5689067367929965e-05}, {"id": 334, "seek": 176672, "start": 1790.04, "end": 1792.72, "text": " And so it'll be quicker to access.", "tokens": [400, 370, 309, 603, 312, 16255, 281, 2105, 13], "temperature": 0.0, "avg_logprob": -0.14946884219929324, "compression_ratio": 1.7926829268292683, "no_speech_prob": 1.5689067367929965e-05}, {"id": 335, "seek": 179272, "start": 1792.72, "end": 1796.88, "text": " But vice versa, that it's easier to grab a row from row major storage, whereas here if", "tokens": [583, 11964, 25650, 11, 300, 309, 311, 3571, 281, 4444, 257, 5386, 490, 5386, 2563, 6725, 11, 9735, 510, 498], "temperature": 0.0, "avg_logprob": -0.18102431783870776, "compression_ratio": 1.6822033898305084, "no_speech_prob": 8.267700650321785e-06}, {"id": 336, "seek": 179272, "start": 1796.88, "end": 1803.76, "text": " you wanted the second row, you have to kind of jump around to grab the elements.", "tokens": [291, 1415, 264, 1150, 5386, 11, 291, 362, 281, 733, 295, 3012, 926, 281, 4444, 264, 4959, 13], "temperature": 0.0, "avg_logprob": -0.18102431783870776, "compression_ratio": 1.6822033898305084, "no_speech_prob": 8.267700650321785e-06}, {"id": 337, "seek": 179272, "start": 1803.76, "end": 1809.84, "text": " So somewhat unfortunately is that this is not consistent between languages.", "tokens": [407, 8344, 7015, 307, 300, 341, 307, 406, 8398, 1296, 8650, 13], "temperature": 0.0, "avg_logprob": -0.18102431783870776, "compression_ratio": 1.6822033898305084, "no_speech_prob": 8.267700650321785e-06}, {"id": 338, "seek": 179272, "start": 1809.84, "end": 1816.08, "text": " And so Fortran is column major layout and it was kind of the first major language to", "tokens": [400, 370, 11002, 4257, 307, 7738, 2563, 13333, 293, 309, 390, 733, 295, 264, 700, 2563, 2856, 281], "temperature": 0.0, "avg_logprob": -0.18102431783870776, "compression_ratio": 1.6822033898305084, "no_speech_prob": 8.267700650321785e-06}, {"id": 339, "seek": 179272, "start": 1816.08, "end": 1817.08, "text": " use column major.", "tokens": [764, 7738, 2563, 13], "temperature": 0.0, "avg_logprob": -0.18102431783870776, "compression_ratio": 1.6822033898305084, "no_speech_prob": 8.267700650321785e-06}, {"id": 340, "seek": 179272, "start": 1817.08, "end": 1821.48, "text": " And then Matlab R and Julia also use column major.", "tokens": [400, 550, 6789, 44990, 497, 293, 18551, 611, 764, 7738, 2563, 13], "temperature": 0.0, "avg_logprob": -0.18102431783870776, "compression_ratio": 1.6822033898305084, "no_speech_prob": 8.267700650321785e-06}, {"id": 341, "seek": 182148, "start": 1821.48, "end": 1827.4, "text": " C, C++, Python, Pascal, and Mathematica all use row major.", "tokens": [383, 11, 383, 25472, 11, 15329, 11, 41723, 11, 293, 15776, 8615, 2262, 439, 764, 5386, 2563, 13], "temperature": 0.0, "avg_logprob": -0.10495220940068083, "compression_ratio": 1.673728813559322, "no_speech_prob": 7.766633643768728e-06}, {"id": 342, "seek": 182148, "start": 1827.4, "end": 1831.28, "text": " And this really just kind of matters when you're writing algorithms of if you're using", "tokens": [400, 341, 534, 445, 733, 295, 7001, 562, 291, 434, 3579, 14642, 295, 498, 291, 434, 1228], "temperature": 0.0, "avg_logprob": -0.10495220940068083, "compression_ratio": 1.673728813559322, "no_speech_prob": 7.766633643768728e-06}, {"id": 343, "seek": 182148, "start": 1831.28, "end": 1833.56, "text": " column major, you probably want to loop by columns.", "tokens": [7738, 2563, 11, 291, 1391, 528, 281, 6367, 538, 13766, 13], "temperature": 0.0, "avg_logprob": -0.10495220940068083, "compression_ratio": 1.673728813559322, "no_speech_prob": 7.766633643768728e-06}, {"id": 344, "seek": 182148, "start": 1833.56, "end": 1837.44, "text": " If you're using row major, you want to loop by rows.", "tokens": [759, 291, 434, 1228, 5386, 2563, 11, 291, 528, 281, 6367, 538, 13241, 13], "temperature": 0.0, "avg_logprob": -0.10495220940068083, "compression_ratio": 1.673728813559322, "no_speech_prob": 7.766633643768728e-06}, {"id": 345, "seek": 182148, "start": 1837.44, "end": 1841.84, "text": " And so NumPy gives you the functionality to switch between those.", "tokens": [400, 370, 22592, 47, 88, 2709, 291, 264, 14980, 281, 3679, 1296, 729, 13], "temperature": 0.0, "avg_logprob": -0.10495220940068083, "compression_ratio": 1.673728813559322, "no_speech_prob": 7.766633643768728e-06}, {"id": 346, "seek": 182148, "start": 1841.84, "end": 1849.28, "text": " So there's this as Fortran array to convert it from row major to column major.", "tokens": [407, 456, 311, 341, 382, 11002, 4257, 10225, 281, 7620, 309, 490, 5386, 2563, 281, 7738, 2563, 13], "temperature": 0.0, "avg_logprob": -0.10495220940068083, "compression_ratio": 1.673728813559322, "no_speech_prob": 7.766633643768728e-06}, {"id": 347, "seek": 184928, "start": 1849.28, "end": 1852.6399999999999, "text": " And so we're using that here.", "tokens": [400, 370, 321, 434, 1228, 300, 510, 13], "temperature": 0.0, "avg_logprob": -0.23097480576613855, "compression_ratio": 1.3666666666666667, "no_speech_prob": 9.080314157472458e-06}, {"id": 348, "seek": 184928, "start": 1852.6399999999999, "end": 1855.68, "text": " That's kind of the logic behind that.", "tokens": [663, 311, 733, 295, 264, 9952, 2261, 300, 13], "temperature": 0.0, "avg_logprob": -0.23097480576613855, "compression_ratio": 1.3666666666666667, "no_speech_prob": 9.080314157472458e-06}, {"id": 349, "seek": 184928, "start": 1855.68, "end": 1860.0, "text": " Questions about row major versus column major?", "tokens": [27738, 466, 5386, 2563, 5717, 7738, 2563, 30], "temperature": 0.0, "avg_logprob": -0.23097480576613855, "compression_ratio": 1.3666666666666667, "no_speech_prob": 9.080314157472458e-06}, {"id": 350, "seek": 184928, "start": 1860.0, "end": 1864.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.23097480576613855, "compression_ratio": 1.3666666666666667, "no_speech_prob": 9.080314157472458e-06}, {"id": 351, "seek": 184928, "start": 1864.52, "end": 1875.12, "text": " So here we've converted our train and test because that's what we need for this, our", "tokens": [407, 510, 321, 600, 16424, 527, 3847, 293, 1500, 570, 300, 311, 437, 321, 643, 337, 341, 11, 527], "temperature": 0.0, "avg_logprob": -0.23097480576613855, "compression_ratio": 1.3666666666666667, "no_speech_prob": 9.080314157472458e-06}, {"id": 352, "seek": 187512, "start": 1875.12, "end": 1882.04, "text": " polynomial vectorization since we're taking columns of data and multiplying them by each", "tokens": [26110, 8062, 2144, 1670, 321, 434, 1940, 13766, 295, 1412, 293, 30955, 552, 538, 1184], "temperature": 0.0, "avg_logprob": -0.222228270310622, "compression_ratio": 1.5, "no_speech_prob": 9.665794095781166e-06}, {"id": 353, "seek": 187512, "start": 1882.04, "end": 1883.7199999999998, "text": " other to create our new features.", "tokens": [661, 281, 1884, 527, 777, 4122, 13], "temperature": 0.0, "avg_logprob": -0.222228270310622, "compression_ratio": 1.5, "no_speech_prob": 9.665794095781166e-06}, {"id": 354, "seek": 187512, "start": 1883.7199999999998, "end": 1888.1599999999999, "text": " We want column major layout.", "tokens": [492, 528, 7738, 2563, 13333, 13], "temperature": 0.0, "avg_logprob": -0.222228270310622, "compression_ratio": 1.5, "no_speech_prob": 9.665794095781166e-06}, {"id": 355, "seek": 187512, "start": 1888.1599999999999, "end": 1899.1999999999998, "text": " And so again, this is giving us a big speed up over scikit-learn.", "tokens": [400, 370, 797, 11, 341, 307, 2902, 505, 257, 955, 3073, 493, 670, 2180, 22681, 12, 306, 1083, 13], "temperature": 0.0, "avg_logprob": -0.222228270310622, "compression_ratio": 1.5, "no_speech_prob": 9.665794095781166e-06}, {"id": 356, "seek": 187512, "start": 1899.1999999999998, "end": 1904.6799999999998, "text": " And this is a kind of real problem we might want to solve to improve our linear regression.", "tokens": [400, 341, 307, 257, 733, 295, 957, 1154, 321, 1062, 528, 281, 5039, 281, 3470, 527, 8213, 24590, 13], "temperature": 0.0, "avg_logprob": -0.222228270310622, "compression_ratio": 1.5, "no_speech_prob": 9.665794095781166e-06}, {"id": 357, "seek": 190468, "start": 1904.68, "end": 1907.68, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.24728691962457472, "compression_ratio": 1.3605442176870748, "no_speech_prob": 1.5445826647919603e-05}, {"id": 358, "seek": 190468, "start": 1907.68, "end": 1920.44, "text": " And so then as a final step, we want to add some regularization to our model to reduce", "tokens": [400, 370, 550, 382, 257, 2572, 1823, 11, 321, 528, 281, 909, 512, 3890, 2144, 281, 527, 2316, 281, 5407], "temperature": 0.0, "avg_logprob": -0.24728691962457472, "compression_ratio": 1.3605442176870748, "no_speech_prob": 1.5445826647919603e-05}, {"id": 359, "seek": 190468, "start": 1920.44, "end": 1926.0800000000002, "text": " overfitting.", "tokens": [670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.24728691962457472, "compression_ratio": 1.3605442176870748, "no_speech_prob": 1.5445826647919603e-05}, {"id": 360, "seek": 190468, "start": 1926.0800000000002, "end": 1930.64, "text": " So here, lasso regression is what uses an L1 penalty.", "tokens": [407, 510, 11, 2439, 539, 24590, 307, 437, 4960, 364, 441, 16, 16263, 13], "temperature": 0.0, "avg_logprob": -0.24728691962457472, "compression_ratio": 1.3605442176870748, "no_speech_prob": 1.5445826647919603e-05}, {"id": 361, "seek": 190468, "start": 1930.64, "end": 1932.64, "text": " We're going to try using that here.", "tokens": [492, 434, 516, 281, 853, 1228, 300, 510, 13], "temperature": 0.0, "avg_logprob": -0.24728691962457472, "compression_ratio": 1.3605442176870748, "no_speech_prob": 1.5445826647919603e-05}, {"id": 362, "seek": 193264, "start": 1932.64, "end": 1937.72, "text": " And I've linked to, there's a Coursera video on lasso regression if anyone wanted more.", "tokens": [400, 286, 600, 9408, 281, 11, 456, 311, 257, 383, 5067, 1663, 960, 322, 2439, 539, 24590, 498, 2878, 1415, 544, 13], "temperature": 0.0, "avg_logprob": -0.27941263133081895, "compression_ratio": 1.5778894472361809, "no_speech_prob": 2.2826998247182928e-05}, {"id": 363, "seek": 193264, "start": 1937.72, "end": 1947.0400000000002, "text": " And at this point, we've already seen the L1 penalty a few times, or at least two times.", "tokens": [400, 412, 341, 935, 11, 321, 600, 1217, 1612, 264, 441, 16, 16263, 257, 1326, 1413, 11, 420, 412, 1935, 732, 1413, 13], "temperature": 0.0, "avg_logprob": -0.27941263133081895, "compression_ratio": 1.5778894472361809, "no_speech_prob": 2.2826998247182928e-05}, {"id": 364, "seek": 193264, "start": 1947.0400000000002, "end": 1952.2800000000002, "text": " Can anyone remember what the two times we've seen the L1 penalty is?", "tokens": [1664, 2878, 1604, 437, 264, 732, 1413, 321, 600, 1612, 264, 441, 16, 16263, 307, 30], "temperature": 0.0, "avg_logprob": -0.27941263133081895, "compression_ratio": 1.5778894472361809, "no_speech_prob": 2.2826998247182928e-05}, {"id": 365, "seek": 193264, "start": 1952.2800000000002, "end": 1954.24, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.27941263133081895, "compression_ratio": 1.5778894472361809, "no_speech_prob": 2.2826998247182928e-05}, {"id": 366, "seek": 193264, "start": 1954.24, "end": 1959.5200000000002, "text": " Someone raise their hand and get the microphone from Sam.", "tokens": [8734, 5300, 641, 1011, 293, 483, 264, 10952, 490, 4832, 13], "temperature": 0.0, "avg_logprob": -0.27941263133081895, "compression_ratio": 1.5778894472361809, "no_speech_prob": 2.2826998247182928e-05}, {"id": 367, "seek": 195952, "start": 1959.52, "end": 1963.16, "text": " Robust PCA and CT scan.", "tokens": [5424, 381, 6465, 32, 293, 19529, 11049, 13], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 368, "seek": 195952, "start": 1963.16, "end": 1964.16, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 369, "seek": 195952, "start": 1964.16, "end": 1965.16, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 370, "seek": 195952, "start": 1965.16, "end": 1970.08, "text": " So robust PCA for the background removal of the surveillance video and then for the CT", "tokens": [407, 13956, 6465, 32, 337, 264, 3678, 17933, 295, 264, 18475, 960, 293, 550, 337, 264, 19529], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 371, "seek": 195952, "start": 1970.08, "end": 1972.8799999999999, "text": " scan, compressed sensing problem.", "tokens": [11049, 11, 30353, 30654, 1154, 13], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 372, "seek": 195952, "start": 1972.8799999999999, "end": 1973.8799999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 373, "seek": 195952, "start": 1973.8799999999999, "end": 1978.44, "text": " So here, scikit-learn has a lasso CV.", "tokens": [407, 510, 11, 2180, 22681, 12, 306, 1083, 575, 257, 2439, 539, 22995, 13], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 374, "seek": 195952, "start": 1978.44, "end": 1983.44, "text": " We're not going to get too deep into, have you all seen cross validation in your other", "tokens": [492, 434, 406, 516, 281, 483, 886, 2452, 666, 11, 362, 291, 439, 1612, 3278, 24071, 294, 428, 661], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 375, "seek": 195952, "start": 1983.44, "end": 1984.44, "text": " courses?", "tokens": [7712, 30], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 376, "seek": 195952, "start": 1984.44, "end": 1985.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 377, "seek": 195952, "start": 1985.44, "end": 1986.44, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 378, "seek": 195952, "start": 1986.44, "end": 1987.44, "text": " I see a lot of nodding heads.", "tokens": [286, 536, 257, 688, 295, 15224, 3584, 8050, 13], "temperature": 0.0, "avg_logprob": -0.2532654667759801, "compression_ratio": 1.4869565217391305, "no_speech_prob": 1.0289073543390259e-05}, {"id": 379, "seek": 198744, "start": 1987.44, "end": 1992.56, "text": " And that's just kind of comparing different parameters.", "tokens": [400, 300, 311, 445, 733, 295, 15763, 819, 9834, 13], "temperature": 0.0, "avg_logprob": -0.17885162353515624, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048599061614368e-06}, {"id": 380, "seek": 198744, "start": 1992.56, "end": 1997.24, "text": " And so here, so alpha is the weight of how much weight to put on the air.", "tokens": [400, 370, 510, 11, 370, 8961, 307, 264, 3364, 295, 577, 709, 3364, 281, 829, 322, 264, 1988, 13], "temperature": 0.0, "avg_logprob": -0.17885162353515624, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048599061614368e-06}, {"id": 381, "seek": 198744, "start": 1997.24, "end": 2005.24, "text": " And this will compare 10 different alphas and tell us what the best one is.", "tokens": [400, 341, 486, 6794, 1266, 819, 419, 7485, 293, 980, 505, 437, 264, 1151, 472, 307, 13], "temperature": 0.0, "avg_logprob": -0.17885162353515624, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048599061614368e-06}, {"id": 382, "seek": 198744, "start": 2005.24, "end": 2009.3200000000002, "text": " So we do that and we get an improvement in our regression, our metrics.", "tokens": [407, 321, 360, 300, 293, 321, 483, 364, 10444, 294, 527, 24590, 11, 527, 16367, 13], "temperature": 0.0, "avg_logprob": -0.17885162353515624, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048599061614368e-06}, {"id": 383, "seek": 198744, "start": 2009.3200000000002, "end": 2012.48, "text": " So again, this is the L2 norm and the L1 norm.", "tokens": [407, 797, 11, 341, 307, 264, 441, 17, 2026, 293, 264, 441, 16, 2026, 13], "temperature": 0.0, "avg_logprob": -0.17885162353515624, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048599061614368e-06}, {"id": 384, "seek": 198744, "start": 2012.48, "end": 2017.3600000000001, "text": " Those are now down to 50 and 40.", "tokens": [3950, 366, 586, 760, 281, 2625, 293, 3356, 13], "temperature": 0.0, "avg_logprob": -0.17885162353515624, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048599061614368e-06}, {"id": 385, "seek": 201736, "start": 2017.36, "end": 2031.0, "text": " And so kind of when we ran this lasso cross validation, we were getting back a model that", "tokens": [400, 370, 733, 295, 562, 321, 5872, 341, 2439, 539, 3278, 24071, 11, 321, 645, 1242, 646, 257, 2316, 300], "temperature": 0.0, "avg_logprob": -0.15986104445023971, "compression_ratio": 1.4509803921568627, "no_speech_prob": 3.393095767023624e-06}, {"id": 386, "seek": 201736, "start": 2031.0, "end": 2037.04, "text": " had alpha set to what the optimal one was.", "tokens": [632, 8961, 992, 281, 437, 264, 16252, 472, 390, 13], "temperature": 0.0, "avg_logprob": -0.15986104445023971, "compression_ratio": 1.4509803921568627, "no_speech_prob": 3.393095767023624e-06}, {"id": 387, "seek": 201736, "start": 2037.04, "end": 2045.4399999999998, "text": " And this does have additional parameters, like if you wanted to set yourself which alphas", "tokens": [400, 341, 775, 362, 4497, 9834, 11, 411, 498, 291, 1415, 281, 992, 1803, 597, 419, 7485], "temperature": 0.0, "avg_logprob": -0.15986104445023971, "compression_ratio": 1.4509803921568627, "no_speech_prob": 3.393095767023624e-06}, {"id": 388, "seek": 204544, "start": 2045.44, "end": 2051.4, "text": " you want to try.", "tokens": [291, 528, 281, 853, 13], "temperature": 0.0, "avg_logprob": -0.26258529316295276, "compression_ratio": 1.0823529411764705, "no_speech_prob": 5.59410091227619e-06}, {"id": 389, "seek": 204544, "start": 2051.4, "end": 2066.48, "text": " And finally, we're going to add some noise to the data.", "tokens": [400, 2721, 11, 321, 434, 516, 281, 909, 512, 5658, 281, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.26258529316295276, "compression_ratio": 1.0823529411764705, "no_speech_prob": 5.59410091227619e-06}, {"id": 390, "seek": 204544, "start": 2066.48, "end": 2067.96, "text": " So we do that here.", "tokens": [407, 321, 360, 300, 510, 13], "temperature": 0.0, "avg_logprob": -0.26258529316295276, "compression_ratio": 1.0823529411764705, "no_speech_prob": 5.59410091227619e-06}, {"id": 391, "seek": 206796, "start": 2067.96, "end": 2076.96, "text": " This can be another way to try to kind of reduce our overfitting.", "tokens": [639, 393, 312, 1071, 636, 281, 853, 281, 733, 295, 5407, 527, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.1524184544881185, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.495072516874643e-06}, {"id": 392, "seek": 206796, "start": 2076.96, "end": 2082.44, "text": " Then I also just wanted to kind of briefly show you the Huber loss.", "tokens": [1396, 286, 611, 445, 1415, 281, 733, 295, 10515, 855, 291, 264, 389, 10261, 4470, 13], "temperature": 0.0, "avg_logprob": -0.1524184544881185, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.495072516874643e-06}, {"id": 393, "seek": 206796, "start": 2082.44, "end": 2086.64, "text": " And that's yet another loss function.", "tokens": [400, 300, 311, 1939, 1071, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1524184544881185, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.495072516874643e-06}, {"id": 394, "seek": 206796, "start": 2086.64, "end": 2090.96, "text": " And it's less sensitive to outliers than squared error loss.", "tokens": [400, 309, 311, 1570, 9477, 281, 484, 23646, 813, 8889, 6713, 4470, 13], "temperature": 0.0, "avg_logprob": -0.1524184544881185, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.495072516874643e-06}, {"id": 395, "seek": 206796, "start": 2090.96, "end": 2095.84, "text": " Basically what it is, is it's quadratic for small error values and then linear for large", "tokens": [8537, 437, 309, 307, 11, 307, 309, 311, 37262, 337, 1359, 6713, 4190, 293, 550, 8213, 337, 2416], "temperature": 0.0, "avg_logprob": -0.1524184544881185, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.495072516874643e-06}, {"id": 396, "seek": 206796, "start": 2095.84, "end": 2096.96, "text": " values.", "tokens": [4190, 13], "temperature": 0.0, "avg_logprob": -0.1524184544881185, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.495072516874643e-06}, {"id": 397, "seek": 209696, "start": 2096.96, "end": 2098.76, "text": " So this is what this method is doing.", "tokens": [407, 341, 307, 437, 341, 3170, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.1246969081737377, "compression_ratio": 1.5508021390374331, "no_speech_prob": 3.9669134821451735e-06}, {"id": 398, "seek": 209696, "start": 2098.76, "end": 2104.7200000000003, "text": " It's saying for x less than a certain value, this is going to be quadratic.", "tokens": [467, 311, 1566, 337, 2031, 1570, 813, 257, 1629, 2158, 11, 341, 307, 516, 281, 312, 37262, 13], "temperature": 0.0, "avg_logprob": -0.1246969081737377, "compression_ratio": 1.5508021390374331, "no_speech_prob": 3.9669134821451735e-06}, {"id": 399, "seek": 209696, "start": 2104.7200000000003, "end": 2107.28, "text": " For larger x, this will be linear.", "tokens": [1171, 4833, 2031, 11, 341, 486, 312, 8213, 13], "temperature": 0.0, "avg_logprob": -0.1246969081737377, "compression_ratio": 1.5508021390374331, "no_speech_prob": 3.9669134821451735e-06}, {"id": 400, "seek": 209696, "start": 2107.28, "end": 2114.58, "text": " And the use of the one halves are to make sure that it's differentiable.", "tokens": [400, 264, 764, 295, 264, 472, 38490, 366, 281, 652, 988, 300, 309, 311, 819, 9364, 13], "temperature": 0.0, "avg_logprob": -0.1246969081737377, "compression_ratio": 1.5508021390374331, "no_speech_prob": 3.9669134821451735e-06}, {"id": 401, "seek": 209696, "start": 2114.58, "end": 2120.7200000000003, "text": " So even where it meets up at point delta, it's still differentiable.", "tokens": [407, 754, 689, 309, 13961, 493, 412, 935, 8289, 11, 309, 311, 920, 819, 9364, 13], "temperature": 0.0, "avg_logprob": -0.1246969081737377, "compression_ratio": 1.5508021390374331, "no_speech_prob": 3.9669134821451735e-06}, {"id": 402, "seek": 212072, "start": 2120.72, "end": 2129.52, "text": " So we try using that, or actually, so I guess here this is kind of illustrating, if we had", "tokens": [407, 321, 853, 1228, 300, 11, 420, 767, 11, 370, 286, 2041, 510, 341, 307, 733, 295, 8490, 8754, 11, 498, 321, 632], "temperature": 0.0, "avg_logprob": -0.29665071623665945, "compression_ratio": 1.4225352112676057, "no_speech_prob": 6.9618740781152155e-06}, {"id": 403, "seek": 212072, "start": 2129.52, "end": 2137.6, "text": " this noise, our method is giving us worse error, but we can kind of improve it by using", "tokens": [341, 5658, 11, 527, 3170, 307, 2902, 505, 5324, 6713, 11, 457, 321, 393, 733, 295, 3470, 309, 538, 1228], "temperature": 0.0, "avg_logprob": -0.29665071623665945, "compression_ratio": 1.4225352112676057, "no_speech_prob": 6.9618740781152155e-06}, {"id": 404, "seek": 212072, "start": 2137.6, "end": 2144.6, "text": " a better loss function.", "tokens": [257, 1101, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.29665071623665945, "compression_ratio": 1.4225352112676057, "no_speech_prob": 6.9618740781152155e-06}, {"id": 405, "seek": 214460, "start": 2144.6, "end": 2150.88, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.5296713787576427, "compression_ratio": 1.2627118644067796, "no_speech_prob": 3.8825306546641514e-05}, {"id": 406, "seek": 214460, "start": 2150.88, "end": 2155.36, "text": " Roger?", "tokens": [17666, 30], "temperature": 0.0, "avg_logprob": -0.5296713787576427, "compression_ratio": 1.2627118644067796, "no_speech_prob": 3.8825306546641514e-05}, {"id": 407, "seek": 214460, "start": 2155.36, "end": 2163.08, "text": " And Tim, can you throw the microphone?", "tokens": [400, 7172, 11, 393, 291, 3507, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.5296713787576427, "compression_ratio": 1.2627118644067796, "no_speech_prob": 3.8825306546641514e-05}, {"id": 408, "seek": 214460, "start": 2163.08, "end": 2168.56, "text": " So is x the actual data in that loss function?", "tokens": [407, 307, 2031, 264, 3539, 1412, 294, 300, 4470, 2445, 30], "temperature": 0.0, "avg_logprob": -0.5296713787576427, "compression_ratio": 1.2627118644067796, "no_speech_prob": 3.8825306546641514e-05}, {"id": 409, "seek": 214460, "start": 2168.56, "end": 2171.4, "text": " Or is that supposed to be the prediction?", "tokens": [1610, 307, 300, 3442, 281, 312, 264, 17630, 30], "temperature": 0.0, "avg_logprob": -0.5296713787576427, "compression_ratio": 1.2627118644067796, "no_speech_prob": 3.8825306546641514e-05}, {"id": 410, "seek": 217140, "start": 2171.4, "end": 2175.52, "text": " So here, oh, that's a great question.", "tokens": [407, 510, 11, 1954, 11, 300, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.4294912164861506, "compression_ratio": 1.446969696969697, "no_speech_prob": 3.288623929620371e-06}, {"id": 411, "seek": 217140, "start": 2175.52, "end": 2177.2000000000003, "text": " Really that would be the error.", "tokens": [4083, 300, 576, 312, 264, 6713, 13], "temperature": 0.0, "avg_logprob": -0.4294912164861506, "compression_ratio": 1.446969696969697, "no_speech_prob": 3.288623929620371e-06}, {"id": 412, "seek": 217140, "start": 2177.2000000000003, "end": 2182.52, "text": " So that would be the difference between the prediction and the actual value.", "tokens": [407, 300, 576, 312, 264, 2649, 1296, 264, 17630, 293, 264, 3539, 2158, 13], "temperature": 0.0, "avg_logprob": -0.4294912164861506, "compression_ratio": 1.446969696969697, "no_speech_prob": 3.288623929620371e-06}, {"id": 413, "seek": 217140, "start": 2182.52, "end": 2185.52, "text": " Good question.", "tokens": [2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.4294912164861506, "compression_ratio": 1.446969696969697, "no_speech_prob": 3.288623929620371e-06}, {"id": 414, "seek": 217140, "start": 2185.52, "end": 2192.76, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.4294912164861506, "compression_ratio": 1.446969696969697, "no_speech_prob": 3.288623929620371e-06}, {"id": 415, "seek": 217140, "start": 2192.76, "end": 2195.76, "text": " Matthew?", "tokens": [12434, 30], "temperature": 0.0, "avg_logprob": -0.4294912164861506, "compression_ratio": 1.446969696969697, "no_speech_prob": 3.288623929620371e-06}, {"id": 416, "seek": 219576, "start": 2195.76, "end": 2202.76, "text": " And can you throw the microphone?", "tokens": [400, 393, 291, 3507, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.39265460502810595, "compression_ratio": 1.1551724137931034, "no_speech_prob": 1.11252684291685e-05}, {"id": 417, "seek": 219576, "start": 2202.76, "end": 2216.28, "text": " Is there a rule of thumb for picking the Huber value, or is it just kind of?", "tokens": [1119, 456, 257, 4978, 295, 9298, 337, 8867, 264, 389, 10261, 2158, 11, 420, 307, 309, 445, 733, 295, 30], "temperature": 0.0, "avg_logprob": -0.39265460502810595, "compression_ratio": 1.1551724137931034, "no_speech_prob": 1.11252684291685e-05}, {"id": 418, "seek": 219576, "start": 2216.28, "end": 2222.7200000000003, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.39265460502810595, "compression_ratio": 1.1551724137931034, "no_speech_prob": 1.11252684291685e-05}, {"id": 419, "seek": 222272, "start": 2222.72, "end": 2227.24, "text": " I think some of it also depends on your problem, like if you think that outliers are kind of", "tokens": [286, 519, 512, 295, 309, 611, 5946, 322, 428, 1154, 11, 411, 498, 291, 519, 300, 484, 23646, 366, 733, 295], "temperature": 0.0, "avg_logprob": -0.2251815173936927, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3630967259814497e-05}, {"id": 420, "seek": 222272, "start": 2227.24, "end": 2229.9199999999996, "text": " less important.", "tokens": [1570, 1021, 13], "temperature": 0.0, "avg_logprob": -0.2251815173936927, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3630967259814497e-05}, {"id": 421, "seek": 222272, "start": 2229.9199999999996, "end": 2233.7599999999998, "text": " It could be good to use the Huber loss method.", "tokens": [467, 727, 312, 665, 281, 764, 264, 389, 10261, 4470, 3170, 13], "temperature": 0.0, "avg_logprob": -0.2251815173936927, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3630967259814497e-05}, {"id": 422, "seek": 222272, "start": 2233.7599999999998, "end": 2243.6, "text": " Yeah, choosing a loss method in general kind of involves some of your perspective on the", "tokens": [865, 11, 10875, 257, 4470, 3170, 294, 2674, 733, 295, 11626, 512, 295, 428, 4585, 322, 264], "temperature": 0.0, "avg_logprob": -0.2251815173936927, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3630967259814497e-05}, {"id": 423, "seek": 222272, "start": 2243.6, "end": 2244.6, "text": " problem.", "tokens": [1154, 13], "temperature": 0.0, "avg_logprob": -0.2251815173936927, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3630967259814497e-05}, {"id": 424, "seek": 222272, "start": 2244.6, "end": 2249.7999999999997, "text": " So like with the, going back to the CT scan, we could look at the pictures and know, OK,", "tokens": [407, 411, 365, 264, 11, 516, 646, 281, 264, 19529, 11049, 11, 321, 727, 574, 412, 264, 5242, 293, 458, 11, 2264, 11], "temperature": 0.0, "avg_logprob": -0.2251815173936927, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3630967259814497e-05}, {"id": 425, "seek": 224980, "start": 2249.8, "end": 2254.76, "text": " this L2 picture is not what we want, we know that we're supposed to be getting something", "tokens": [341, 441, 17, 3036, 307, 406, 437, 321, 528, 11, 321, 458, 300, 321, 434, 3442, 281, 312, 1242, 746], "temperature": 0.0, "avg_logprob": -0.2160948398066502, "compression_ratio": 1.6611570247933884, "no_speech_prob": 1.078289369615959e-05}, {"id": 426, "seek": 224980, "start": 2254.76, "end": 2257.48, "text": " that looks more like the picture we're getting from the L1.", "tokens": [300, 1542, 544, 411, 264, 3036, 321, 434, 1242, 490, 264, 441, 16, 13], "temperature": 0.0, "avg_logprob": -0.2160948398066502, "compression_ratio": 1.6611570247933884, "no_speech_prob": 1.078289369615959e-05}, {"id": 427, "seek": 224980, "start": 2257.48, "end": 2264.6400000000003, "text": " But yeah, I think that kind of requires some domain knowledge about your problem or what", "tokens": [583, 1338, 11, 286, 519, 300, 733, 295, 7029, 512, 9274, 3601, 466, 428, 1154, 420, 437], "temperature": 0.0, "avg_logprob": -0.2160948398066502, "compression_ratio": 1.6611570247933884, "no_speech_prob": 1.078289369615959e-05}, {"id": 428, "seek": 224980, "start": 2264.6400000000003, "end": 2266.4, "text": " you're trying to find.", "tokens": [291, 434, 1382, 281, 915, 13], "temperature": 0.0, "avg_logprob": -0.2160948398066502, "compression_ratio": 1.6611570247933884, "no_speech_prob": 1.078289369615959e-05}, {"id": 429, "seek": 224980, "start": 2266.4, "end": 2273.4, "text": " Yeah, it's tough because each, like the loss method itself is what saying a good solution", "tokens": [865, 11, 309, 311, 4930, 570, 1184, 11, 411, 264, 4470, 3170, 2564, 307, 437, 1566, 257, 665, 3827], "temperature": 0.0, "avg_logprob": -0.2160948398066502, "compression_ratio": 1.6611570247933884, "no_speech_prob": 1.078289369615959e-05}, {"id": 430, "seek": 224980, "start": 2273.4, "end": 2278.36, "text": " is, but you have to still choose which loss method.", "tokens": [307, 11, 457, 291, 362, 281, 920, 2826, 597, 4470, 3170, 13], "temperature": 0.0, "avg_logprob": -0.2160948398066502, "compression_ratio": 1.6611570247933884, "no_speech_prob": 1.078289369615959e-05}, {"id": 431, "seek": 227836, "start": 2278.36, "end": 2280.08, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.3274407159714472, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.356667432148242e-06}, {"id": 432, "seek": 227836, "start": 2280.08, "end": 2284.08, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.3274407159714472, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.356667432148242e-06}, {"id": 433, "seek": 227836, "start": 2284.08, "end": 2293.52, "text": " OK, we're a little bit early, but I think this might be a good time to stop for our", "tokens": [2264, 11, 321, 434, 257, 707, 857, 2440, 11, 457, 286, 519, 341, 1062, 312, 257, 665, 565, 281, 1590, 337, 527], "temperature": 0.0, "avg_logprob": -0.3274407159714472, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.356667432148242e-06}, {"id": 434, "seek": 227836, "start": 2293.52, "end": 2299.56, "text": " break and not start the next notebook until after that.", "tokens": [1821, 293, 406, 722, 264, 958, 21060, 1826, 934, 300, 13], "temperature": 0.0, "avg_logprob": -0.3274407159714472, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.356667432148242e-06}, {"id": 435, "seek": 227836, "start": 2299.56, "end": 2303.1200000000003, "text": " Yeah, so let's meet back here in eight minutes.", "tokens": [865, 11, 370, 718, 311, 1677, 646, 510, 294, 3180, 2077, 13], "temperature": 0.0, "avg_logprob": -0.3274407159714472, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.356667432148242e-06}, {"id": 436, "seek": 230312, "start": 2303.12, "end": 2308.8399999999997, "text": " So that would be 11, 1156.", "tokens": [407, 300, 576, 312, 2975, 11, 2975, 18317, 13], "temperature": 0.0, "avg_logprob": -0.2793016309862013, "compression_ratio": 1.4306930693069306, "no_speech_prob": 4.1329232772113755e-05}, {"id": 437, "seek": 230312, "start": 2308.8399999999997, "end": 2311.48, "text": " OK, I'm going to go ahead and start back up.", "tokens": [2264, 11, 286, 478, 516, 281, 352, 2286, 293, 722, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.2793016309862013, "compression_ratio": 1.4306930693069306, "no_speech_prob": 4.1329232772113755e-05}, {"id": 438, "seek": 230312, "start": 2311.48, "end": 2319.3599999999997, "text": " So yeah, I just wanted to kind of say the key takeaways from lesson five are kind of", "tokens": [407, 1338, 11, 286, 445, 1415, 281, 733, 295, 584, 264, 2141, 45584, 490, 6898, 1732, 366, 733, 295], "temperature": 0.0, "avg_logprob": -0.2793016309862013, "compression_ratio": 1.4306930693069306, "no_speech_prob": 4.1329232772113755e-05}, {"id": 439, "seek": 230312, "start": 2319.3599999999997, "end": 2327.12, "text": " one, you know, polynomial features as relatively easy way to try to kind of improve performance", "tokens": [472, 11, 291, 458, 11, 26110, 4122, 382, 7226, 1858, 636, 281, 853, 281, 733, 295, 3470, 3389], "temperature": 0.0, "avg_logprob": -0.2793016309862013, "compression_ratio": 1.4306930693069306, "no_speech_prob": 4.1329232772113755e-05}, {"id": 440, "seek": 230312, "start": 2327.12, "end": 2329.12, "text": " while still using linear regression.", "tokens": [1339, 920, 1228, 8213, 24590, 13], "temperature": 0.0, "avg_logprob": -0.2793016309862013, "compression_ratio": 1.4306930693069306, "no_speech_prob": 4.1329232772113755e-05}, {"id": 441, "seek": 232912, "start": 2329.12, "end": 2335.72, "text": " And, you know, let you capture kind of this interaction of how features interact with", "tokens": [400, 11, 291, 458, 11, 718, 291, 7983, 733, 295, 341, 9285, 295, 577, 4122, 4648, 365], "temperature": 0.0, "avg_logprob": -0.20937168414776142, "compression_ratio": 1.4242424242424243, "no_speech_prob": 6.143935024738312e-06}, {"id": 442, "seek": 232912, "start": 2335.72, "end": 2337.96, "text": " each other.", "tokens": [1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.20937168414776142, "compression_ratio": 1.4242424242424243, "no_speech_prob": 6.143935024738312e-06}, {"id": 443, "seek": 232912, "start": 2337.96, "end": 2343.4, "text": " And then number is a great way to speed up your code.", "tokens": [400, 550, 1230, 307, 257, 869, 636, 281, 3073, 493, 428, 3089, 13], "temperature": 0.0, "avg_logprob": -0.20937168414776142, "compression_ratio": 1.4242424242424243, "no_speech_prob": 6.143935024738312e-06}, {"id": 444, "seek": 232912, "start": 2343.4, "end": 2346.8399999999997, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.20937168414776142, "compression_ratio": 1.4242424242424243, "no_speech_prob": 6.143935024738312e-06}, {"id": 445, "seek": 232912, "start": 2346.8399999999997, "end": 2350.16, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.20937168414776142, "compression_ratio": 1.4242424242424243, "no_speech_prob": 6.143935024738312e-06}, {"id": 446, "seek": 232912, "start": 2350.16, "end": 2357.8399999999997, "text": " Let's start on lesson six, how to implement linear regression.", "tokens": [961, 311, 722, 322, 6898, 2309, 11, 577, 281, 4445, 8213, 24590, 13], "temperature": 0.0, "avg_logprob": -0.20937168414776142, "compression_ratio": 1.4242424242424243, "no_speech_prob": 6.143935024738312e-06}, {"id": 447, "seek": 235784, "start": 2357.84, "end": 2363.2000000000003, "text": " So we've been using scikit-learns implementation, and now we're kind of going to go a level", "tokens": [407, 321, 600, 668, 1228, 2180, 22681, 12, 306, 1083, 82, 11420, 11, 293, 586, 321, 434, 733, 295, 516, 281, 352, 257, 1496], "temperature": 0.0, "avg_logprob": -0.2285607201712472, "compression_ratio": 1.6738197424892705, "no_speech_prob": 9.222793778462801e-06}, {"id": 448, "seek": 235784, "start": 2363.2000000000003, "end": 2365.8, "text": " deeper and think about how would we implement this ourselves.", "tokens": [7731, 293, 519, 466, 577, 576, 321, 4445, 341, 4175, 13], "temperature": 0.0, "avg_logprob": -0.2285607201712472, "compression_ratio": 1.6738197424892705, "no_speech_prob": 9.222793778462801e-06}, {"id": 449, "seek": 235784, "start": 2365.8, "end": 2371.96, "text": " First, so we're still going to be working with the same data set as before, the diabetes", "tokens": [2386, 11, 370, 321, 434, 920, 516, 281, 312, 1364, 365, 264, 912, 1412, 992, 382, 949, 11, 264, 13881], "temperature": 0.0, "avg_logprob": -0.2285607201712472, "compression_ratio": 1.6738197424892705, "no_speech_prob": 9.222793778462801e-06}, {"id": 450, "seek": 235784, "start": 2371.96, "end": 2372.96, "text": " data set.", "tokens": [1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.2285607201712472, "compression_ratio": 1.6738197424892705, "no_speech_prob": 9.222793778462801e-06}, {"id": 451, "seek": 235784, "start": 2372.96, "end": 2379.2000000000003, "text": " First, let's look at how scikit-learn did this.", "tokens": [2386, 11, 718, 311, 574, 412, 577, 2180, 22681, 12, 306, 1083, 630, 341, 13], "temperature": 0.0, "avg_logprob": -0.2285607201712472, "compression_ratio": 1.6738197424892705, "no_speech_prob": 9.222793778462801e-06}, {"id": 452, "seek": 235784, "start": 2379.2000000000003, "end": 2382.28, "text": " And so you can check the source code, and it's really handy.", "tokens": [400, 370, 291, 393, 1520, 264, 4009, 3089, 11, 293, 309, 311, 534, 13239, 13], "temperature": 0.0, "avg_logprob": -0.2285607201712472, "compression_ratio": 1.6738197424892705, "no_speech_prob": 9.222793778462801e-06}, {"id": 453, "seek": 235784, "start": 2382.28, "end": 2385.1600000000003, "text": " Actually, I didn't go there.", "tokens": [5135, 11, 286, 994, 380, 352, 456, 13], "temperature": 0.0, "avg_logprob": -0.2285607201712472, "compression_ratio": 1.6738197424892705, "no_speech_prob": 9.222793778462801e-06}, {"id": 454, "seek": 238516, "start": 2385.16, "end": 2389.16, "text": " But the SciPy documentation, so let me show this.", "tokens": [583, 264, 16942, 47, 88, 14333, 11, 370, 718, 385, 855, 341, 13], "temperature": 0.0, "avg_logprob": -0.31360673067862527, "compression_ratio": 1.303448275862069, "no_speech_prob": 1.805669307941571e-05}, {"id": 455, "seek": 238516, "start": 2389.16, "end": 2394.2799999999997, "text": " Typically, when you search for something, you know, you're interested in scikit-learn,", "tokens": [23129, 11, 562, 291, 3164, 337, 746, 11, 291, 458, 11, 291, 434, 3102, 294, 2180, 22681, 12, 306, 1083, 11], "temperature": 0.0, "avg_logprob": -0.31360673067862527, "compression_ratio": 1.303448275862069, "no_speech_prob": 1.805669307941571e-05}, {"id": 456, "seek": 238516, "start": 2394.2799999999997, "end": 2395.2799999999997, "text": " least squares.", "tokens": [1935, 19368, 13], "temperature": 0.0, "avg_logprob": -0.31360673067862527, "compression_ratio": 1.303448275862069, "no_speech_prob": 1.805669307941571e-05}, {"id": 457, "seek": 238516, "start": 2395.2799999999997, "end": 2409.2799999999997, "text": " Actually, I should probably do SciPy.", "tokens": [5135, 11, 286, 820, 1391, 360, 16942, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.31360673067862527, "compression_ratio": 1.303448275862069, "no_speech_prob": 1.805669307941571e-05}, {"id": 458, "seek": 240928, "start": 2409.28, "end": 2418.88, "text": " It's kind of nice on the documentation.", "tokens": [467, 311, 733, 295, 1481, 322, 264, 14333, 13], "temperature": 0.0, "avg_logprob": -0.1519256844578019, "compression_ratio": 1.4734299516908214, "no_speech_prob": 4.936837740388e-06}, {"id": 459, "seek": 240928, "start": 2418.88, "end": 2420.6400000000003, "text": " There's always a link to the source code.", "tokens": [821, 311, 1009, 257, 2113, 281, 264, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1519256844578019, "compression_ratio": 1.4734299516908214, "no_speech_prob": 4.936837740388e-06}, {"id": 460, "seek": 240928, "start": 2420.6400000000003, "end": 2426.28, "text": " And so I think this can be really helpful for getting more information about what you're", "tokens": [400, 370, 286, 519, 341, 393, 312, 534, 4961, 337, 1242, 544, 1589, 466, 437, 291, 434], "temperature": 0.0, "avg_logprob": -0.1519256844578019, "compression_ratio": 1.4734299516908214, "no_speech_prob": 4.936837740388e-06}, {"id": 461, "seek": 240928, "start": 2426.28, "end": 2427.28, "text": " doing.", "tokens": [884, 13], "temperature": 0.0, "avg_logprob": -0.1519256844578019, "compression_ratio": 1.4734299516908214, "no_speech_prob": 4.936837740388e-06}, {"id": 462, "seek": 240928, "start": 2427.28, "end": 2430.0400000000004, "text": " Let me just go back here to make sure.", "tokens": [961, 385, 445, 352, 646, 510, 281, 652, 988, 13], "temperature": 0.0, "avg_logprob": -0.1519256844578019, "compression_ratio": 1.4734299516908214, "no_speech_prob": 4.936837740388e-06}, {"id": 463, "seek": 240928, "start": 2430.0400000000004, "end": 2437.6800000000003, "text": " So here I wanted to look at SciPy's linear regression that we've been using and see kind", "tokens": [407, 510, 286, 1415, 281, 574, 412, 16942, 47, 88, 311, 8213, 24590, 300, 321, 600, 668, 1228, 293, 536, 733], "temperature": 0.0, "avg_logprob": -0.1519256844578019, "compression_ratio": 1.4734299516908214, "no_speech_prob": 4.936837740388e-06}, {"id": 464, "seek": 243768, "start": 2437.68, "end": 2440.56, "text": " of what is it doing.", "tokens": [295, 437, 307, 309, 884, 13], "temperature": 0.0, "avg_logprob": -0.3079757038344685, "compression_ratio": 1.632, "no_speech_prob": 2.0462277461774647e-05}, {"id": 465, "seek": 243768, "start": 2440.56, "end": 2444.64, "text": " And there will be stuff you can kind of skim through more where they're just, you know,", "tokens": [400, 456, 486, 312, 1507, 291, 393, 733, 295, 1110, 332, 807, 544, 689, 436, 434, 445, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.3079757038344685, "compression_ratio": 1.632, "no_speech_prob": 2.0462277461774647e-05}, {"id": 466, "seek": 243768, "start": 2444.64, "end": 2448.48, "text": " like maybe checking the inputs or creating an object.", "tokens": [411, 1310, 8568, 264, 15743, 420, 4084, 364, 2657, 13], "temperature": 0.0, "avg_logprob": -0.3079757038344685, "compression_ratio": 1.632, "no_speech_prob": 2.0462277461774647e-05}, {"id": 467, "seek": 243768, "start": 2448.48, "end": 2457.0, "text": " But here the interesting part to me was how to say, okay, how do they get the coefficients?", "tokens": [583, 510, 264, 1880, 644, 281, 385, 390, 577, 281, 584, 11, 1392, 11, 577, 360, 436, 483, 264, 31994, 30], "temperature": 0.0, "avg_logprob": -0.3079757038344685, "compression_ratio": 1.632, "no_speech_prob": 2.0462277461774647e-05}, {"id": 468, "seek": 243768, "start": 2457.0, "end": 2461.2599999999998, "text": " And here they're handling the case if it's sparse or not.", "tokens": [400, 510, 436, 434, 13175, 264, 1389, 498, 309, 311, 637, 11668, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.3079757038344685, "compression_ratio": 1.632, "no_speech_prob": 2.0462277461774647e-05}, {"id": 469, "seek": 243768, "start": 2461.2599999999998, "end": 2465.9199999999996, "text": " We were working with a matrix that wasn't sparse, and we see it's calling Lin-Aoj.least-square.", "tokens": [492, 645, 1364, 365, 257, 8141, 300, 2067, 380, 637, 11668, 11, 293, 321, 536, 309, 311, 5141, 9355, 12, 32, 78, 73, 13, 306, 525, 12, 33292, 543, 13], "temperature": 0.0, "avg_logprob": -0.3079757038344685, "compression_ratio": 1.632, "no_speech_prob": 2.0462277461774647e-05}, {"id": 470, "seek": 246592, "start": 2465.92, "end": 2485.04, "text": " So then we can go there and just confirm that they are importing Lin-Aoj from SciPy.", "tokens": [407, 550, 321, 393, 352, 456, 293, 445, 9064, 300, 436, 366, 43866, 9355, 12, 32, 78, 73, 490, 16942, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.17696746190388998, "compression_ratio": 1.024390243902439, "no_speech_prob": 3.78528238798026e-06}, {"id": 471, "seek": 248504, "start": 2485.04, "end": 2496.68, "text": " So we want it...", "tokens": [407, 321, 528, 309, 485], "temperature": 0.0, "avg_logprob": -0.6181819350631149, "compression_ratio": 0.9154929577464789, "no_speech_prob": 6.0488059716590215e-06}, {"id": 472, "seek": 248504, "start": 2496.68, "end": 2505.48, "text": " Actually, sorry.", "tokens": [5135, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.6181819350631149, "compression_ratio": 0.9154929577464789, "no_speech_prob": 6.0488059716590215e-06}, {"id": 473, "seek": 248504, "start": 2505.48, "end": 2509.96, "text": " They had called it LSTQ, right?", "tokens": [814, 632, 1219, 309, 441, 6840, 48, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.6181819350631149, "compression_ratio": 0.9154929577464789, "no_speech_prob": 6.0488059716590215e-06}, {"id": 474, "seek": 250996, "start": 2509.96, "end": 2516.0, "text": " Yeah, Lin-Aoj.least-square.", "tokens": [865, 11, 9355, 12, 32, 78, 73, 13, 306, 525, 12, 33292, 543, 13], "temperature": 0.0, "avg_logprob": -0.33197593688964844, "compression_ratio": 1.0823529411764705, "no_speech_prob": 4.860394255956635e-06}, {"id": 475, "seek": 250996, "start": 2516.0, "end": 2537.12, "text": " So come here to the documentation and then into the source code.", "tokens": [407, 808, 510, 281, 264, 14333, 293, 550, 666, 264, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.33197593688964844, "compression_ratio": 1.0823529411764705, "no_speech_prob": 4.860394255956635e-06}, {"id": 476, "seek": 253712, "start": 2537.12, "end": 2542.2, "text": " And we scroll down.", "tokens": [400, 321, 11369, 760, 13], "temperature": 0.0, "avg_logprob": -0.4475711822509766, "compression_ratio": 1.2773722627737227, "no_speech_prob": 1.593416709511075e-05}, {"id": 477, "seek": 253712, "start": 2542.2, "end": 2546.08, "text": " And so we're kind of getting a hint here that...", "tokens": [400, 370, 321, 434, 733, 295, 1242, 257, 12075, 510, 300, 485], "temperature": 0.0, "avg_logprob": -0.4475711822509766, "compression_ratio": 1.2773722627737227, "no_speech_prob": 1.593416709511075e-05}, {"id": 478, "seek": 253712, "start": 2546.08, "end": 2547.7599999999998, "text": " So they're using a LawPack driver.", "tokens": [407, 436, 434, 1228, 257, 7744, 47, 501, 6787, 13], "temperature": 0.0, "avg_logprob": -0.4475711822509766, "compression_ratio": 1.2773722627737227, "no_speech_prob": 1.593416709511075e-05}, {"id": 479, "seek": 253712, "start": 2547.7599999999998, "end": 2551.7599999999998, "text": " Actually, can anyone remember what is LawPack?", "tokens": [5135, 11, 393, 2878, 1604, 437, 307, 7744, 47, 501, 30], "temperature": 0.0, "avg_logprob": -0.4475711822509766, "compression_ratio": 1.2773722627737227, "no_speech_prob": 1.593416709511075e-05}, {"id": 480, "seek": 253712, "start": 2551.7599999999998, "end": 2555.92, "text": " It's like...", "tokens": [467, 311, 411, 485], "temperature": 0.0, "avg_logprob": -0.4475711822509766, "compression_ratio": 1.2773722627737227, "no_speech_prob": 1.593416709511075e-05}, {"id": 481, "seek": 253712, "start": 2555.92, "end": 2560.08, "text": " Wait, okay.", "tokens": [3802, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.4475711822509766, "compression_ratio": 1.2773722627737227, "no_speech_prob": 1.593416709511075e-05}, {"id": 482, "seek": 256008, "start": 2560.08, "end": 2576.52, "text": " The linear algebra package and also known as BPack and MKL, the most known name.", "tokens": [440, 8213, 21989, 7372, 293, 611, 2570, 382, 363, 47, 501, 293, 30770, 43, 11, 264, 881, 2570, 1315, 13], "temperature": 0.0, "avg_logprob": -0.43956071680242365, "compression_ratio": 1.2396694214876034, "no_speech_prob": 4.637748588720569e-06}, {"id": 483, "seek": 256008, "start": 2576.52, "end": 2578.24, "text": " So, close.", "tokens": [407, 11, 1998, 13], "temperature": 0.0, "avg_logprob": -0.43956071680242365, "compression_ratio": 1.2396694214876034, "no_speech_prob": 4.637748588720569e-06}, {"id": 484, "seek": 256008, "start": 2578.24, "end": 2585.84, "text": " Does anyone want to elaborate on that or clarify a little?", "tokens": [4402, 2878, 528, 281, 20945, 322, 300, 420, 17594, 257, 707, 30], "temperature": 0.0, "avg_logprob": -0.43956071680242365, "compression_ratio": 1.2396694214876034, "no_speech_prob": 4.637748588720569e-06}, {"id": 485, "seek": 258584, "start": 2585.84, "end": 2594.6000000000004, "text": " Yeah, so LawPack is a low-level linear algebra library, and it's using BLAST, which is kind", "tokens": [865, 11, 370, 7744, 47, 501, 307, 257, 2295, 12, 12418, 8213, 21989, 6405, 11, 293, 309, 311, 1228, 15132, 20398, 11, 597, 307, 733], "temperature": 0.0, "avg_logprob": -0.18567826282018904, "compression_ratio": 1.6616161616161615, "no_speech_prob": 2.5465205908403732e-05}, {"id": 486, "seek": 258584, "start": 2594.6000000000004, "end": 2599.1600000000003, "text": " of even lower-level library for basic matrix computations.", "tokens": [295, 754, 3126, 12, 12418, 6405, 337, 3875, 8141, 2807, 763, 13], "temperature": 0.0, "avg_logprob": -0.18567826282018904, "compression_ratio": 1.6616161616161615, "no_speech_prob": 2.5465205908403732e-05}, {"id": 487, "seek": 258584, "start": 2599.1600000000003, "end": 2603.1600000000003, "text": " So BLAST is going to have things like this is how you do matrix vector multiplication,", "tokens": [407, 15132, 20398, 307, 516, 281, 362, 721, 411, 341, 307, 577, 291, 360, 8141, 8062, 27290, 11], "temperature": 0.0, "avg_logprob": -0.18567826282018904, "compression_ratio": 1.6616161616161615, "no_speech_prob": 2.5465205908403732e-05}, {"id": 488, "seek": 258584, "start": 2603.1600000000003, "end": 2610.92, "text": " this is how you do matrix-matrix multiplication, and then LawPack is a layer above that and", "tokens": [341, 307, 577, 291, 360, 8141, 12, 15677, 6579, 27290, 11, 293, 550, 7744, 47, 501, 307, 257, 4583, 3673, 300, 293], "temperature": 0.0, "avg_logprob": -0.18567826282018904, "compression_ratio": 1.6616161616161615, "no_speech_prob": 2.5465205908403732e-05}, {"id": 489, "seek": 261092, "start": 2610.92, "end": 2619.04, "text": " is kind of has these decompositions we've seen, like this is how you do SVD or QR.", "tokens": [307, 733, 295, 575, 613, 22867, 329, 2451, 321, 600, 1612, 11, 411, 341, 307, 577, 291, 360, 31910, 35, 420, 32784, 13], "temperature": 0.0, "avg_logprob": -0.2268345661652394, "compression_ratio": 1.3917525773195876, "no_speech_prob": 3.7265385799400974e-06}, {"id": 490, "seek": 261092, "start": 2619.04, "end": 2624.96, "text": " And this is something that has been heavily, heavily optimized.", "tokens": [400, 341, 307, 746, 300, 575, 668, 10950, 11, 10950, 26941, 13], "temperature": 0.0, "avg_logprob": -0.2268345661652394, "compression_ratio": 1.3917525773195876, "no_speech_prob": 3.7265385799400974e-06}, {"id": 491, "seek": 261092, "start": 2624.96, "end": 2627.6800000000003, "text": " And LawPack, I believe, came out in the early 90s.", "tokens": [400, 7744, 47, 501, 11, 286, 1697, 11, 1361, 484, 294, 264, 2440, 4289, 82, 13], "temperature": 0.0, "avg_logprob": -0.2268345661652394, "compression_ratio": 1.3917525773195876, "no_speech_prob": 3.7265385799400974e-06}, {"id": 492, "seek": 261092, "start": 2627.6800000000003, "end": 2636.12, "text": " So let me bring up the notebook just to make sure I get the facts right.", "tokens": [407, 718, 385, 1565, 493, 264, 21060, 445, 281, 652, 988, 286, 483, 264, 9130, 558, 13], "temperature": 0.0, "avg_logprob": -0.2268345661652394, "compression_ratio": 1.3917525773195876, "no_speech_prob": 3.7265385799400974e-06}, {"id": 493, "seek": 263612, "start": 2636.12, "end": 2641.44, "text": " LawPack has kind of been optimized for each computer, and so libraries like NumPy are", "tokens": [7744, 47, 501, 575, 733, 295, 668, 26941, 337, 1184, 3820, 11, 293, 370, 15148, 411, 22592, 47, 88, 366], "temperature": 0.0, "avg_logprob": -0.2825144841120793, "compression_ratio": 1.4573170731707317, "no_speech_prob": 3.0894575502316e-06}, {"id": 494, "seek": 263612, "start": 2641.44, "end": 2646.12, "text": " calling LawPack, and pretty much any scientific library in any language is going to be calling", "tokens": [5141, 7744, 47, 501, 11, 293, 1238, 709, 604, 8134, 6405, 294, 604, 2856, 307, 516, 281, 312, 5141], "temperature": 0.0, "avg_logprob": -0.2825144841120793, "compression_ratio": 1.4573170731707317, "no_speech_prob": 3.0894575502316e-06}, {"id": 495, "seek": 263612, "start": 2646.12, "end": 2649.12, "text": " LawPack at a lower level.", "tokens": [7744, 47, 501, 412, 257, 3126, 1496, 13], "temperature": 0.0, "avg_logprob": -0.2825144841120793, "compression_ratio": 1.4573170731707317, "no_speech_prob": 3.0894575502316e-06}, {"id": 496, "seek": 263612, "start": 2649.12, "end": 2655.12, "text": " I'll just briefly bring that up.", "tokens": [286, 603, 445, 10515, 1565, 300, 493, 13], "temperature": 0.0, "avg_logprob": -0.2825144841120793, "compression_ratio": 1.4573170731707317, "no_speech_prob": 3.0894575502316e-06}, {"id": 497, "seek": 265512, "start": 2655.12, "end": 2667.12, "text": " Oh, yeah, the speed section.", "tokens": [876, 11, 1338, 11, 264, 3073, 3541, 13], "temperature": 0.0, "avg_logprob": -0.6432324250539144, "compression_ratio": 0.8235294117647058, "no_speech_prob": 0.0004949026624672115}, {"id": 498, "seek": 266712, "start": 2667.12, "end": 2687.12, "text": " Maybe not.", "tokens": [2704, 406, 13], "temperature": 0.0, "avg_logprob": -0.28371626755286905, "compression_ratio": 1.0253164556962024, "no_speech_prob": 1.3419054994301405e-05}, {"id": 499, "seek": 266712, "start": 2687.12, "end": 2689.12, "text": " Okay, I'll come back to this.", "tokens": [1033, 11, 286, 603, 808, 646, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.28371626755286905, "compression_ratio": 1.0253164556962024, "no_speech_prob": 1.3419054994301405e-05}, {"id": 500, "seek": 266712, "start": 2689.12, "end": 2693.12, "text": " Sorry, this is somewhere in this lesson.", "tokens": [4919, 11, 341, 307, 4079, 294, 341, 6898, 13], "temperature": 0.0, "avg_logprob": -0.28371626755286905, "compression_ratio": 1.0253164556962024, "no_speech_prob": 1.3419054994301405e-05}, {"id": 501, "seek": 269312, "start": 2693.12, "end": 2699.12, "text": " I'll add a little bit more detail about it.", "tokens": [286, 603, 909, 257, 707, 857, 544, 2607, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.1522064562197085, "compression_ratio": 1.2900763358778626, "no_speech_prob": 2.710821536311414e-05}, {"id": 502, "seek": 269312, "start": 2699.12, "end": 2700.12, "text": " Oh, you know what?", "tokens": [876, 11, 291, 458, 437, 30], "temperature": 0.0, "avg_logprob": -0.1522064562197085, "compression_ratio": 1.2900763358778626, "no_speech_prob": 2.710821536311414e-05}, {"id": 503, "seek": 269312, "start": 2700.12, "end": 2715.12, "text": " It's up in the matrix computation section.", "tokens": [467, 311, 493, 294, 264, 8141, 24903, 3541, 13], "temperature": 0.0, "avg_logprob": -0.1522064562197085, "compression_ratio": 1.2900763358778626, "no_speech_prob": 2.710821536311414e-05}, {"id": 504, "seek": 269312, "start": 2715.12, "end": 2717.12, "text": " Okay, sorry about that.", "tokens": [1033, 11, 2597, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.1522064562197085, "compression_ratio": 1.2900763358778626, "no_speech_prob": 2.710821536311414e-05}, {"id": 505, "seek": 269312, "start": 2717.12, "end": 2719.12, "text": " I'll find that and show that next time.", "tokens": [286, 603, 915, 300, 293, 855, 300, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.1522064562197085, "compression_ratio": 1.2900763358778626, "no_speech_prob": 2.710821536311414e-05}, {"id": 506, "seek": 271912, "start": 2719.12, "end": 2725.12, "text": " But yeah, LawPack, low-level library being called by pretty much all scientific computing", "tokens": [583, 1338, 11, 7744, 47, 501, 11, 2295, 12, 12418, 6405, 885, 1219, 538, 1238, 709, 439, 8134, 15866], "temperature": 0.0, "avg_logprob": -0.14470362663269043, "compression_ratio": 1.375, "no_speech_prob": 8.267667908512522e-06}, {"id": 507, "seek": 271912, "start": 2725.12, "end": 2726.12, "text": " libraries.", "tokens": [15148, 13], "temperature": 0.0, "avg_logprob": -0.14470362663269043, "compression_ratio": 1.375, "no_speech_prob": 8.267667908512522e-06}, {"id": 508, "seek": 271912, "start": 2726.12, "end": 2732.12, "text": " So, yeah, we were back here in the SciPy source code.", "tokens": [407, 11, 1338, 11, 321, 645, 646, 510, 294, 264, 16942, 47, 88, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.14470362663269043, "compression_ratio": 1.375, "no_speech_prob": 8.267667908512522e-06}, {"id": 509, "seek": 271912, "start": 2732.12, "end": 2740.12, "text": " It'll become important that we're kind of taking one of these, it says LawPack drivers,", "tokens": [467, 603, 1813, 1021, 300, 321, 434, 733, 295, 1940, 472, 295, 613, 11, 309, 1619, 7744, 47, 501, 11590, 11], "temperature": 0.0, "avg_logprob": -0.14470362663269043, "compression_ratio": 1.375, "no_speech_prob": 8.267667908512522e-06}, {"id": 510, "seek": 274012, "start": 2740.12, "end": 2751.12, "text": " GELSD, GELSY, GELSS, and it's going to call them, yeah, down here.", "tokens": [460, 3158, 23969, 11, 460, 3158, 50, 56, 11, 460, 3158, 21929, 11, 293, 309, 311, 516, 281, 818, 552, 11, 1338, 11, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.11265081754872497, "compression_ratio": 1.4099378881987579, "no_speech_prob": 1.644195981498342e-05}, {"id": 511, "seek": 274012, "start": 2751.12, "end": 2756.12, "text": " It's kind of checking for the driver and then calling that from LawPack.", "tokens": [467, 311, 733, 295, 8568, 337, 264, 6787, 293, 550, 5141, 300, 490, 7744, 47, 501, 13], "temperature": 0.0, "avg_logprob": -0.11265081754872497, "compression_ratio": 1.4099378881987579, "no_speech_prob": 1.644195981498342e-05}, {"id": 512, "seek": 274012, "start": 2756.12, "end": 2764.12, "text": " And so the information we have is options, and this shows up in the comment at the top.", "tokens": [400, 370, 264, 1589, 321, 362, 307, 3956, 11, 293, 341, 3110, 493, 294, 264, 2871, 412, 264, 1192, 13], "temperature": 0.0, "avg_logprob": -0.11265081754872497, "compression_ratio": 1.4099378881987579, "no_speech_prob": 1.644195981498342e-05}, {"id": 513, "seek": 276412, "start": 2764.12, "end": 2770.12, "text": " The options are GELSD, GELSY, and GELSS.", "tokens": [440, 3956, 366, 460, 3158, 23969, 11, 460, 3158, 50, 56, 11, 293, 460, 3158, 21929, 13], "temperature": 0.0, "avg_logprob": -0.07169166793171157, "compression_ratio": 1.5726141078838174, "no_speech_prob": 2.7967398636974394e-05}, {"id": 514, "seek": 276412, "start": 2770.12, "end": 2772.12, "text": " The default is GELSD.", "tokens": [440, 7576, 307, 460, 3158, 23969, 13], "temperature": 0.0, "avg_logprob": -0.07169166793171157, "compression_ratio": 1.5726141078838174, "no_speech_prob": 2.7967398636974394e-05}, {"id": 515, "seek": 276412, "start": 2772.12, "end": 2776.12, "text": " However, GELSY can be slightly faster.", "tokens": [2908, 11, 460, 3158, 50, 56, 393, 312, 4748, 4663, 13], "temperature": 0.0, "avg_logprob": -0.07169166793171157, "compression_ratio": 1.5726141078838174, "no_speech_prob": 2.7967398636974394e-05}, {"id": 516, "seek": 276412, "start": 2776.12, "end": 2778.12, "text": " GELSS was used historically.", "tokens": [460, 3158, 21929, 390, 1143, 16180, 13], "temperature": 0.0, "avg_logprob": -0.07169166793171157, "compression_ratio": 1.5726141078838174, "no_speech_prob": 2.7967398636974394e-05}, {"id": 517, "seek": 276412, "start": 2778.12, "end": 2781.12, "text": " It's slow but uses less memory.", "tokens": [467, 311, 2964, 457, 4960, 1570, 4675, 13], "temperature": 0.0, "avg_logprob": -0.07169166793171157, "compression_ratio": 1.5726141078838174, "no_speech_prob": 2.7967398636974394e-05}, {"id": 518, "seek": 276412, "start": 2781.12, "end": 2785.12, "text": " And so this can be kind of helpful to read, and this is something that we could override", "tokens": [400, 370, 341, 393, 312, 733, 295, 4961, 281, 1401, 11, 293, 341, 307, 746, 300, 321, 727, 42321], "temperature": 0.0, "avg_logprob": -0.07169166793171157, "compression_ratio": 1.5726141078838174, "no_speech_prob": 2.7967398636974394e-05}, {"id": 519, "seek": 276412, "start": 2785.12, "end": 2790.12, "text": " if we wanted, kind of still wanted to be using SciPy's least squares but wanted it to use", "tokens": [498, 321, 1415, 11, 733, 295, 920, 1415, 281, 312, 1228, 16942, 47, 88, 311, 1935, 19368, 457, 1415, 309, 281, 764], "temperature": 0.0, "avg_logprob": -0.07169166793171157, "compression_ratio": 1.5726141078838174, "no_speech_prob": 2.7967398636974394e-05}, {"id": 520, "seek": 276412, "start": 2790.12, "end": 2793.12, "text": " a different method at this low level.", "tokens": [257, 819, 3170, 412, 341, 2295, 1496, 13], "temperature": 0.0, "avg_logprob": -0.07169166793171157, "compression_ratio": 1.5726141078838174, "no_speech_prob": 2.7967398636974394e-05}, {"id": 521, "seek": 279312, "start": 2793.12, "end": 2798.12, "text": " And so this is also kind of, I think, interesting to see that SciPy has, you know, at least", "tokens": [400, 370, 341, 307, 611, 733, 295, 11, 286, 519, 11, 1880, 281, 536, 300, 16942, 47, 88, 575, 11, 291, 458, 11, 412, 1935], "temperature": 0.0, "avg_logprob": -0.09590827657821331, "compression_ratio": 1.5876777251184835, "no_speech_prob": 9.080245945369825e-06}, {"id": 522, "seek": 279312, "start": 2798.12, "end": 2800.12, "text": " two main options.", "tokens": [732, 2135, 3956, 13], "temperature": 0.0, "avg_logprob": -0.09590827657821331, "compression_ratio": 1.5876777251184835, "no_speech_prob": 9.080245945369825e-06}, {"id": 523, "seek": 279312, "start": 2800.12, "end": 2806.12, "text": " You know, it's not using the same algorithm necessarily all the time.", "tokens": [509, 458, 11, 309, 311, 406, 1228, 264, 912, 9284, 4725, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.09590827657821331, "compression_ratio": 1.5876777251184835, "no_speech_prob": 9.080245945369825e-06}, {"id": 524, "seek": 279312, "start": 2806.12, "end": 2811.12, "text": " And then just here, I did not get into the details of this at all.", "tokens": [400, 550, 445, 510, 11, 286, 630, 406, 483, 666, 264, 4365, 295, 341, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.09590827657821331, "compression_ratio": 1.5876777251184835, "no_speech_prob": 9.080245945369825e-06}, {"id": 525, "seek": 279312, "start": 2811.12, "end": 2818.12, "text": " You can kind of just get like the very basic kind of, oh, this computes the minimum norm", "tokens": [509, 393, 733, 295, 445, 483, 411, 264, 588, 3875, 733, 295, 11, 1954, 11, 341, 715, 1819, 264, 7285, 2026], "temperature": 0.0, "avg_logprob": -0.09590827657821331, "compression_ratio": 1.5876777251184835, "no_speech_prob": 9.080245945369825e-06}, {"id": 526, "seek": 281812, "start": 2818.12, "end": 2823.12, "text": " solution to a linear square problem using the SBD decomposition and a divide and conquer", "tokens": [3827, 281, 257, 8213, 3732, 1154, 1228, 264, 26944, 35, 48356, 293, 257, 9845, 293, 24136], "temperature": 0.0, "avg_logprob": -0.09526868300004439, "compression_ratio": 1.6209386281588447, "no_speech_prob": 1.0451239177200478e-05}, {"id": 527, "seek": 281812, "start": 2823.12, "end": 2825.12, "text": " method.", "tokens": [3170, 13], "temperature": 0.0, "avg_logprob": -0.09526868300004439, "compression_ratio": 1.6209386281588447, "no_speech_prob": 1.0451239177200478e-05}, {"id": 528, "seek": 281812, "start": 2825.12, "end": 2828.12, "text": " And so the key thing here is I didn't want you to get into the details at all, but I", "tokens": [400, 370, 264, 2141, 551, 510, 307, 286, 994, 380, 528, 291, 281, 483, 666, 264, 4365, 412, 439, 11, 457, 286], "temperature": 0.0, "avg_logprob": -0.09526868300004439, "compression_ratio": 1.6209386281588447, "no_speech_prob": 1.0451239177200478e-05}, {"id": 529, "seek": 281812, "start": 2828.12, "end": 2834.12, "text": " do want you to feel comfortable looking at SciPy's source code and, you know, particularly", "tokens": [360, 528, 291, 281, 841, 4619, 1237, 412, 16942, 47, 88, 311, 4009, 3089, 293, 11, 291, 458, 11, 4098], "temperature": 0.0, "avg_logprob": -0.09526868300004439, "compression_ratio": 1.6209386281588447, "no_speech_prob": 1.0451239177200478e-05}, {"id": 530, "seek": 281812, "start": 2834.12, "end": 2838.12, "text": " at least like reading the comments in there because they can be helpful.", "tokens": [412, 1935, 411, 3760, 264, 3053, 294, 456, 570, 436, 393, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.09526868300004439, "compression_ratio": 1.6209386281588447, "no_speech_prob": 1.0451239177200478e-05}, {"id": 531, "seek": 281812, "start": 2838.12, "end": 2842.12, "text": " Yeah, just to show that there are these three different methods that could be happening", "tokens": [865, 11, 445, 281, 855, 300, 456, 366, 613, 1045, 819, 7150, 300, 727, 312, 2737], "temperature": 0.0, "avg_logprob": -0.09526868300004439, "compression_ratio": 1.6209386281588447, "no_speech_prob": 1.0451239177200478e-05}, {"id": 532, "seek": 281812, "start": 2842.12, "end": 2847.12, "text": " under the hood.", "tokens": [833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.09526868300004439, "compression_ratio": 1.6209386281588447, "no_speech_prob": 1.0451239177200478e-05}, {"id": 533, "seek": 284712, "start": 2847.12, "end": 2853.12, "text": " And then I glossed over this, but we saw that it was, you know, had this if statement, if", "tokens": [400, 550, 286, 19574, 292, 670, 341, 11, 457, 321, 1866, 300, 309, 390, 11, 291, 458, 11, 632, 341, 498, 5629, 11, 498], "temperature": 0.0, "avg_logprob": -0.11459674391635628, "compression_ratio": 1.7835497835497836, "no_speech_prob": 3.426694456720725e-05}, {"id": 534, "seek": 284712, "start": 2853.12, "end": 2856.12, "text": " it's sparse, call these methods.", "tokens": [309, 311, 637, 11668, 11, 818, 613, 7150, 13], "temperature": 0.0, "avg_logprob": -0.11459674391635628, "compression_ratio": 1.7835497835497836, "no_speech_prob": 3.426694456720725e-05}, {"id": 535, "seek": 284712, "start": 2856.12, "end": 2858.12, "text": " If not, call these other ones.", "tokens": [759, 406, 11, 818, 613, 661, 2306, 13], "temperature": 0.0, "avg_logprob": -0.11459674391635628, "compression_ratio": 1.7835497835497836, "no_speech_prob": 3.426694456720725e-05}, {"id": 536, "seek": 284712, "start": 2858.12, "end": 2863.12, "text": " We're not going to dwell on the sparse one at all, but I wanted to say that SciPy's,", "tokens": [492, 434, 406, 516, 281, 24355, 322, 264, 637, 11668, 472, 412, 439, 11, 457, 286, 1415, 281, 584, 300, 16942, 47, 88, 311, 11], "temperature": 0.0, "avg_logprob": -0.11459674391635628, "compression_ratio": 1.7835497835497836, "no_speech_prob": 3.426694456720725e-05}, {"id": 537, "seek": 284712, "start": 2863.12, "end": 2870.12, "text": " and then this was happening at Scikit-learn, it was calling SciPy sparse if it's sparse,", "tokens": [293, 550, 341, 390, 2737, 412, 16942, 22681, 12, 306, 1083, 11, 309, 390, 5141, 16942, 47, 88, 637, 11668, 498, 309, 311, 637, 11668, 11], "temperature": 0.0, "avg_logprob": -0.11459674391635628, "compression_ratio": 1.7835497835497836, "no_speech_prob": 3.426694456720725e-05}, {"id": 538, "seek": 284712, "start": 2870.12, "end": 2876.12, "text": " the SciPy lenalge normal one if it's not, that it's using yet another method for the", "tokens": [264, 16942, 47, 88, 40116, 304, 432, 2710, 472, 498, 309, 311, 406, 11, 300, 309, 311, 1228, 1939, 1071, 3170, 337, 264], "temperature": 0.0, "avg_logprob": -0.11459674391635628, "compression_ratio": 1.7835497835497836, "no_speech_prob": 3.426694456720725e-05}, {"id": 539, "seek": 287612, "start": 2876.12, "end": 2881.12, "text": " sparse ones.", "tokens": [637, 11668, 2306, 13], "temperature": 0.0, "avg_logprob": -0.16121210534888578, "compression_ratio": 1.3853658536585365, "no_speech_prob": 5.507519745151512e-06}, {"id": 540, "seek": 287612, "start": 2881.12, "end": 2885.12, "text": " And so that's something that's typically hidden from you when you're just using Scikit-learn's", "tokens": [400, 370, 300, 311, 746, 300, 311, 5850, 7633, 490, 291, 562, 291, 434, 445, 1228, 16942, 22681, 12, 306, 1083, 311], "temperature": 0.0, "avg_logprob": -0.16121210534888578, "compression_ratio": 1.3853658536585365, "no_speech_prob": 5.507519745151512e-06}, {"id": 541, "seek": 287612, "start": 2885.12, "end": 2890.12, "text": " linear regression that these different things could be happening.", "tokens": [8213, 24590, 300, 613, 819, 721, 727, 312, 2737, 13], "temperature": 0.0, "avg_logprob": -0.16121210534888578, "compression_ratio": 1.3853658536585365, "no_speech_prob": 5.507519745151512e-06}, {"id": 542, "seek": 287612, "start": 2890.12, "end": 2893.12, "text": " So lenalge.lease square.", "tokens": [407, 40116, 304, 432, 13, 306, 651, 3732, 13], "temperature": 0.0, "avg_logprob": -0.16121210534888578, "compression_ratio": 1.3853658536585365, "no_speech_prob": 5.507519745151512e-06}, {"id": 543, "seek": 287612, "start": 2893.12, "end": 2904.12, "text": " Here I've tried calling it and overriding, so passing in LAPACK driver GELSD, Y or S,", "tokens": [1692, 286, 600, 3031, 5141, 309, 293, 670, 81, 2819, 11, 370, 8437, 294, 441, 4715, 11595, 6787, 460, 3158, 23969, 11, 398, 420, 318, 11], "temperature": 0.0, "avg_logprob": -0.16121210534888578, "compression_ratio": 1.3853658536585365, "no_speech_prob": 5.507519745151512e-06}, {"id": 544, "seek": 290412, "start": 2904.12, "end": 2907.12, "text": " and comparing the times on those.", "tokens": [293, 15763, 264, 1413, 322, 729, 13], "temperature": 0.0, "avg_logprob": -0.0998619624546596, "compression_ratio": 1.538812785388128, "no_speech_prob": 1.618714304640889e-05}, {"id": 545, "seek": 290412, "start": 2907.12, "end": 2914.12, "text": " And so I got that GELSY was I guess about twice as fast as GELSD.", "tokens": [400, 370, 286, 658, 300, 460, 3158, 50, 56, 390, 286, 2041, 466, 6091, 382, 2370, 382, 460, 3158, 23969, 13], "temperature": 0.0, "avg_logprob": -0.0998619624546596, "compression_ratio": 1.538812785388128, "no_speech_prob": 1.618714304640889e-05}, {"id": 546, "seek": 290412, "start": 2914.12, "end": 2920.12, "text": " And notice it defaults to GELSD, so that could, you know, in this case maybe you would want", "tokens": [400, 3449, 309, 7576, 82, 281, 460, 3158, 23969, 11, 370, 300, 727, 11, 291, 458, 11, 294, 341, 1389, 1310, 291, 576, 528], "temperature": 0.0, "avg_logprob": -0.0998619624546596, "compression_ratio": 1.538812785388128, "no_speech_prob": 1.618714304640889e-05}, {"id": 547, "seek": 290412, "start": 2920.12, "end": 2926.12, "text": " to override it and use GELSY, but it's only a 2x speedup.", "tokens": [281, 42321, 309, 293, 764, 460, 3158, 50, 56, 11, 457, 309, 311, 787, 257, 568, 87, 3073, 1010, 13], "temperature": 0.0, "avg_logprob": -0.0998619624546596, "compression_ratio": 1.538812785388128, "no_speech_prob": 1.618714304640889e-05}, {"id": 548, "seek": 290412, "start": 2926.12, "end": 2930.12, "text": " And I actually don't know what's going on here, but it's kind of interesting to me that", "tokens": [400, 286, 767, 500, 380, 458, 437, 311, 516, 322, 510, 11, 457, 309, 311, 733, 295, 1880, 281, 385, 300], "temperature": 0.0, "avg_logprob": -0.0998619624546596, "compression_ratio": 1.538812785388128, "no_speech_prob": 1.618714304640889e-05}, {"id": 549, "seek": 293012, "start": 2930.12, "end": 2938.12, "text": " GELSD is slower than GLSS, which is what it had said is like the older version that's", "tokens": [460, 3158, 23969, 307, 14009, 813, 460, 19198, 50, 11, 597, 307, 437, 309, 632, 848, 307, 411, 264, 4906, 3037, 300, 311], "temperature": 0.0, "avg_logprob": -0.07220140905941234, "compression_ratio": 1.55, "no_speech_prob": 1.9525421521393582e-05}, {"id": 550, "seek": 293012, "start": 2938.12, "end": 2942.12, "text": " generally slow.", "tokens": [5101, 2964, 13], "temperature": 0.0, "avg_logprob": -0.07220140905941234, "compression_ratio": 1.55, "no_speech_prob": 1.9525421521393582e-05}, {"id": 551, "seek": 293012, "start": 2942.12, "end": 2946.12, "text": " And these things would all depend on what problem you're doing, so if you did this on a", "tokens": [400, 613, 721, 576, 439, 5672, 322, 437, 1154, 291, 434, 884, 11, 370, 498, 291, 630, 341, 322, 257], "temperature": 0.0, "avg_logprob": -0.07220140905941234, "compression_ratio": 1.55, "no_speech_prob": 1.9525421521393582e-05}, {"id": 552, "seek": 293012, "start": 2946.12, "end": 2956.12, "text": " different problem, you might get that a different method was faster.", "tokens": [819, 1154, 11, 291, 1062, 483, 300, 257, 819, 3170, 390, 4663, 13], "temperature": 0.0, "avg_logprob": -0.07220140905941234, "compression_ratio": 1.55, "no_speech_prob": 1.9525421521393582e-05}, {"id": 553, "seek": 293012, "start": 2956.12, "end": 2958.12, "text": " All right, so now we're going to kind of step back.", "tokens": [1057, 558, 11, 370, 586, 321, 434, 516, 281, 733, 295, 1823, 646, 13], "temperature": 0.0, "avg_logprob": -0.07220140905941234, "compression_ratio": 1.55, "no_speech_prob": 1.9525421521393582e-05}, {"id": 554, "seek": 295812, "start": 2958.12, "end": 2962.12, "text": " So that was still using SciPy's implementation.", "tokens": [407, 300, 390, 920, 1228, 16942, 47, 88, 311, 11420, 13], "temperature": 0.0, "avg_logprob": -0.0801986821492513, "compression_ratio": 1.4950980392156863, "no_speech_prob": 8.267649718618486e-06}, {"id": 555, "seek": 295812, "start": 2962.12, "end": 2970.12, "text": " We'll step back and kind of remember the definition of the least squares problem is we're wanting", "tokens": [492, 603, 1823, 646, 293, 733, 295, 1604, 264, 7123, 295, 264, 1935, 19368, 1154, 307, 321, 434, 7935], "temperature": 0.0, "avg_logprob": -0.0801986821492513, "compression_ratio": 1.4950980392156863, "no_speech_prob": 8.267649718618486e-06}, {"id": 556, "seek": 295812, "start": 2970.12, "end": 2979.12, "text": " to minimize AX minus B and trying to find the best values for X to do that.", "tokens": [281, 17522, 316, 55, 3175, 363, 293, 1382, 281, 915, 264, 1151, 4190, 337, 1783, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.0801986821492513, "compression_ratio": 1.4950980392156863, "no_speech_prob": 8.267649718618486e-06}, {"id": 557, "seek": 295812, "start": 2979.12, "end": 2984.12, "text": " Another way to think about this in linear algebra is that we're interested in where", "tokens": [3996, 636, 281, 519, 466, 341, 294, 8213, 21989, 307, 300, 321, 434, 3102, 294, 689], "temperature": 0.0, "avg_logprob": -0.0801986821492513, "compression_ratio": 1.4950980392156863, "no_speech_prob": 8.267649718618486e-06}, {"id": 558, "seek": 298412, "start": 2984.12, "end": 2991.12, "text": " vector B is closest to the subspace spanned by A, which is called the range of A.", "tokens": [8062, 363, 307, 13699, 281, 264, 2090, 17940, 637, 5943, 538, 316, 11, 597, 307, 1219, 264, 3613, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.08651738119597482, "compression_ratio": 1.7289719626168225, "no_speech_prob": 7.183160505519481e-06}, {"id": 559, "seek": 298412, "start": 2991.12, "end": 2997.12, "text": " And you can kind of think of this as the projection of B onto A, because if you think about all", "tokens": [400, 291, 393, 733, 295, 519, 295, 341, 382, 264, 22743, 295, 363, 3911, 316, 11, 570, 498, 291, 519, 466, 439], "temperature": 0.0, "avg_logprob": -0.08651738119597482, "compression_ratio": 1.7289719626168225, "no_speech_prob": 7.183160505519481e-06}, {"id": 560, "seek": 298412, "start": 2997.12, "end": 3005.12, "text": " possible vectors X, multiplying those by A gives you the range of A, you know, any possible,", "tokens": [1944, 18875, 1783, 11, 30955, 729, 538, 316, 2709, 291, 264, 3613, 295, 316, 11, 291, 458, 11, 604, 1944, 11], "temperature": 0.0, "avg_logprob": -0.08651738119597482, "compression_ratio": 1.7289719626168225, "no_speech_prob": 7.183160505519481e-06}, {"id": 561, "seek": 298412, "start": 3005.12, "end": 3010.12, "text": " yeah, any vector that could be in the range, and you're trying to find the one that's closest", "tokens": [1338, 11, 604, 8062, 300, 727, 312, 294, 264, 3613, 11, 293, 291, 434, 1382, 281, 915, 264, 472, 300, 311, 13699], "temperature": 0.0, "avg_logprob": -0.08651738119597482, "compression_ratio": 1.7289719626168225, "no_speech_prob": 7.183160505519481e-06}, {"id": 562, "seek": 298412, "start": 3010.12, "end": 3012.12, "text": " to B.", "tokens": [281, 363, 13], "temperature": 0.0, "avg_logprob": -0.08651738119597482, "compression_ratio": 1.7289719626168225, "no_speech_prob": 7.183160505519481e-06}, {"id": 563, "seek": 301212, "start": 3012.12, "end": 3022.12, "text": " And so that can be expressed as B minus, so B minus AX must be perpendicular to the subspace", "tokens": [400, 370, 300, 393, 312, 12675, 382, 363, 3175, 11, 370, 363, 3175, 316, 55, 1633, 312, 26734, 281, 264, 2090, 17940], "temperature": 0.0, "avg_logprob": -0.10640858950680249, "compression_ratio": 1.4678362573099415, "no_speech_prob": 8.939526196627412e-06}, {"id": 564, "seek": 301212, "start": 3022.12, "end": 3027.12, "text": " spanned by A. That's kind of getting from B to AX, since it's a projection, would be", "tokens": [637, 5943, 538, 316, 13, 663, 311, 733, 295, 1242, 490, 363, 281, 316, 55, 11, 1670, 309, 311, 257, 22743, 11, 576, 312], "temperature": 0.0, "avg_logprob": -0.10640858950680249, "compression_ratio": 1.4678362573099415, "no_speech_prob": 8.939526196627412e-06}, {"id": 565, "seek": 301212, "start": 3027.12, "end": 3029.12, "text": " the perpendicular.", "tokens": [264, 26734, 13], "temperature": 0.0, "avg_logprob": -0.10640858950680249, "compression_ratio": 1.4678362573099415, "no_speech_prob": 8.939526196627412e-06}, {"id": 566, "seek": 301212, "start": 3029.12, "end": 3037.12, "text": " Let me, maybe I should go to the drawing pad for that.", "tokens": [961, 385, 11, 1310, 286, 820, 352, 281, 264, 6316, 6887, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.10640858950680249, "compression_ratio": 1.4678362573099415, "no_speech_prob": 8.939526196627412e-06}, {"id": 567, "seek": 303712, "start": 3037.12, "end": 3053.12, "text": " Oh, I actually wanted the other one.", "tokens": [876, 11, 286, 767, 1415, 264, 661, 472, 13], "temperature": 0.0, "avg_logprob": -0.15347748536330003, "compression_ratio": 0.8571428571428571, "no_speech_prob": 4.539580913842656e-05}, {"id": 568, "seek": 305312, "start": 3053.12, "end": 3068.12, "text": " So if we think of AX, really we've got a plane where this is A times X for all X's, would", "tokens": [407, 498, 321, 519, 295, 316, 55, 11, 534, 321, 600, 658, 257, 5720, 689, 341, 307, 316, 1413, 1783, 337, 439, 1783, 311, 11, 576], "temperature": 0.0, "avg_logprob": -0.1908859989859841, "compression_ratio": 1.2035398230088497, "no_speech_prob": 3.024122088390868e-05}, {"id": 569, "seek": 305312, "start": 3068.12, "end": 3071.12, "text": " give us some, you know, two-dimensional plane.", "tokens": [976, 505, 512, 11, 291, 458, 11, 732, 12, 18759, 5720, 13], "temperature": 0.0, "avg_logprob": -0.1908859989859841, "compression_ratio": 1.2035398230088497, "no_speech_prob": 3.024122088390868e-05}, {"id": 570, "seek": 307112, "start": 3071.12, "end": 3095.12, "text": " And then we've got some, so that was a bad angle, some vector B, we're trying to find", "tokens": [400, 550, 321, 600, 658, 512, 11, 370, 300, 390, 257, 1578, 5802, 11, 512, 8062, 363, 11, 321, 434, 1382, 281, 915], "temperature": 0.0, "avg_logprob": -0.11056528268037019, "compression_ratio": 1.0759493670886076, "no_speech_prob": 5.338104074326111e-06}, {"id": 571, "seek": 309512, "start": 3095.12, "end": 3109.12, "text": " the X to minimize AX minus B.", "tokens": [264, 1783, 281, 17522, 316, 55, 3175, 363, 13], "temperature": 0.0, "avg_logprob": -0.0997372998131646, "compression_ratio": 1.3140495867768596, "no_speech_prob": 1.3631108231493272e-05}, {"id": 572, "seek": 309512, "start": 3109.12, "end": 3116.12, "text": " And so that's the projection of B onto AX, and that would, yeah, we could write, oh,", "tokens": [400, 370, 300, 311, 264, 22743, 295, 363, 3911, 316, 55, 11, 293, 300, 576, 11, 1338, 11, 321, 727, 2464, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.0997372998131646, "compression_ratio": 1.3140495867768596, "no_speech_prob": 1.3631108231493272e-05}, {"id": 573, "seek": 309512, "start": 3116.12, "end": 3124.12, "text": " let's write this as AX hat is in that plane.", "tokens": [718, 311, 2464, 341, 382, 316, 55, 2385, 307, 294, 300, 5720, 13], "temperature": 0.0, "avg_logprob": -0.0997372998131646, "compression_ratio": 1.3140495867768596, "no_speech_prob": 1.3631108231493272e-05}, {"id": 574, "seek": 312412, "start": 3124.12, "end": 3133.12, "text": " And so then this is where, so this line here, you know, is B minus AX hat, and that must", "tokens": [400, 370, 550, 341, 307, 689, 11, 370, 341, 1622, 510, 11, 291, 458, 11, 307, 363, 3175, 316, 55, 2385, 11, 293, 300, 1633], "temperature": 0.0, "avg_logprob": -0.11471426848209265, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.028939732350409e-05}, {"id": 575, "seek": 312412, "start": 3133.12, "end": 3141.12, "text": " be perpendicular to A. This is the whole plane.", "tokens": [312, 26734, 281, 316, 13, 639, 307, 264, 1379, 5720, 13], "temperature": 0.0, "avg_logprob": -0.11471426848209265, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.028939732350409e-05}, {"id": 576, "seek": 312412, "start": 3141.12, "end": 3146.12, "text": " And I'll show, I'm going to show another three blue, one brown video in a little while.", "tokens": [400, 286, 603, 855, 11, 286, 478, 516, 281, 855, 1071, 1045, 3344, 11, 472, 6292, 960, 294, 257, 707, 1339, 13], "temperature": 0.0, "avg_logprob": -0.11471426848209265, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.028939732350409e-05}, {"id": 577, "seek": 314612, "start": 3146.12, "end": 3154.12, "text": " So this is kind of getting back to a more geometric way of thinking about matrices and", "tokens": [407, 341, 307, 733, 295, 1242, 646, 281, 257, 544, 33246, 636, 295, 1953, 466, 32284, 293], "temperature": 0.0, "avg_logprob": -0.09156988537500775, "compression_ratio": 1.391304347826087, "no_speech_prob": 4.1572939153411426e-06}, {"id": 578, "seek": 314612, "start": 3154.12, "end": 3155.12, "text": " vectors.", "tokens": [18875, 13], "temperature": 0.0, "avg_logprob": -0.09156988537500775, "compression_ratio": 1.391304347826087, "no_speech_prob": 4.1572939153411426e-06}, {"id": 579, "seek": 314612, "start": 3155.12, "end": 3158.12, "text": " Any questions here?", "tokens": [2639, 1651, 510, 30], "temperature": 0.0, "avg_logprob": -0.09156988537500775, "compression_ratio": 1.391304347826087, "no_speech_prob": 4.1572939153411426e-06}, {"id": 580, "seek": 314612, "start": 3158.12, "end": 3169.12, "text": " Okay, so going back to the notebook, you know, we end up with A transpose times B minus AX", "tokens": [1033, 11, 370, 516, 646, 281, 264, 21060, 11, 291, 458, 11, 321, 917, 493, 365, 316, 25167, 1413, 363, 3175, 316, 55], "temperature": 0.0, "avg_logprob": -0.09156988537500775, "compression_ratio": 1.391304347826087, "no_speech_prob": 4.1572939153411426e-06}, {"id": 581, "seek": 314612, "start": 3169.12, "end": 3172.12, "text": " hat must be zero.", "tokens": [2385, 1633, 312, 4018, 13], "temperature": 0.0, "avg_logprob": -0.09156988537500775, "compression_ratio": 1.391304347826087, "no_speech_prob": 4.1572939153411426e-06}, {"id": 582, "seek": 317212, "start": 3172.12, "end": 3177.12, "text": " And this is where the normal equations come from.", "tokens": [400, 341, 307, 689, 264, 2710, 11787, 808, 490, 13], "temperature": 0.0, "avg_logprob": -0.07743313637646762, "compression_ratio": 1.6832579185520362, "no_speech_prob": 1.7502576156402938e-05}, {"id": 583, "seek": 317212, "start": 3177.12, "end": 3188.12, "text": " This is kind of the closed form solution for linear regression least squares problem.", "tokens": [639, 307, 733, 295, 264, 5395, 1254, 3827, 337, 8213, 24590, 1935, 19368, 1154, 13], "temperature": 0.0, "avg_logprob": -0.07743313637646762, "compression_ratio": 1.6832579185520362, "no_speech_prob": 1.7502576156402938e-05}, {"id": 584, "seek": 317212, "start": 3188.12, "end": 3192.12, "text": " And so we could, we could do this, I call this least squares naive.", "tokens": [400, 370, 321, 727, 11, 321, 727, 360, 341, 11, 286, 818, 341, 1935, 19368, 29052, 13], "temperature": 0.0, "avg_logprob": -0.07743313637646762, "compression_ratio": 1.6832579185520362, "no_speech_prob": 1.7502576156402938e-05}, {"id": 585, "seek": 317212, "start": 3192.12, "end": 3196.12, "text": " This is an approach that you would not actually want to use in practice because you're taking", "tokens": [639, 307, 364, 3109, 300, 291, 576, 406, 767, 528, 281, 764, 294, 3124, 570, 291, 434, 1940], "temperature": 0.0, "avg_logprob": -0.07743313637646762, "compression_ratio": 1.6832579185520362, "no_speech_prob": 1.7502576156402938e-05}, {"id": 586, "seek": 317212, "start": 3196.12, "end": 3201.12, "text": " a matrix inverse, which you generally want to, really want to avoid doing.", "tokens": [257, 8141, 17340, 11, 597, 291, 5101, 528, 281, 11, 534, 528, 281, 5042, 884, 13], "temperature": 0.0, "avg_logprob": -0.07743313637646762, "compression_ratio": 1.6832579185520362, "no_speech_prob": 1.7502576156402938e-05}, {"id": 587, "seek": 320112, "start": 3201.12, "end": 3207.12, "text": " But I just kind of did this as a starting point of this is something that would work,", "tokens": [583, 286, 445, 733, 295, 630, 341, 382, 257, 2891, 935, 295, 341, 307, 746, 300, 576, 589, 11], "temperature": 0.0, "avg_logprob": -0.11219397106686153, "compression_ratio": 1.5, "no_speech_prob": 1.3006596418563277e-05}, {"id": 588, "seek": 320112, "start": 3207.12, "end": 3212.12, "text": " assuming, I guess it would work assuming that A is not something we're going to end up dividing", "tokens": [11926, 11, 286, 2041, 309, 576, 589, 11926, 300, 316, 307, 406, 746, 321, 434, 516, 281, 917, 493, 26764], "temperature": 0.0, "avg_logprob": -0.11219397106686153, "compression_ratio": 1.5, "no_speech_prob": 1.3006596418563277e-05}, {"id": 589, "seek": 320112, "start": 3212.12, "end": 3217.12, "text": " by a number close to zero to get its inverse.", "tokens": [538, 257, 1230, 1998, 281, 4018, 281, 483, 1080, 17340, 13], "temperature": 0.0, "avg_logprob": -0.11219397106686153, "compression_ratio": 1.5, "no_speech_prob": 1.3006596418563277e-05}, {"id": 590, "seek": 320112, "start": 3217.12, "end": 3218.12, "text": " Matthew?", "tokens": [12434, 30], "temperature": 0.0, "avg_logprob": -0.11219397106686153, "compression_ratio": 1.5, "no_speech_prob": 1.3006596418563277e-05}, {"id": 591, "seek": 320112, "start": 3218.12, "end": 3224.12, "text": " And who has the microphone?", "tokens": [400, 567, 575, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.11219397106686153, "compression_ratio": 1.5, "no_speech_prob": 1.3006596418563277e-05}, {"id": 592, "seek": 320112, "start": 3224.12, "end": 3228.12, "text": " Nice.", "tokens": [5490, 13], "temperature": 0.0, "avg_logprob": -0.11219397106686153, "compression_ratio": 1.5, "no_speech_prob": 1.3006596418563277e-05}, {"id": 593, "seek": 322812, "start": 3228.12, "end": 3237.12, "text": " Yeah, so it's both, it's both computationally expensive and it can also be unstable because", "tokens": [865, 11, 370, 309, 311, 1293, 11, 309, 311, 1293, 24903, 379, 5124, 293, 309, 393, 611, 312, 23742, 570], "temperature": 0.0, "avg_logprob": -0.14316514333089192, "compression_ratio": 1.4161490683229814, "no_speech_prob": 3.0894279916537926e-06}, {"id": 594, "seek": 322812, "start": 3237.12, "end": 3240.12, "text": " you end up dividing by things.", "tokens": [291, 917, 493, 26764, 538, 721, 13], "temperature": 0.0, "avg_logprob": -0.14316514333089192, "compression_ratio": 1.4161490683229814, "no_speech_prob": 3.0894279916537926e-06}, {"id": 595, "seek": 322812, "start": 3240.12, "end": 3241.12, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.14316514333089192, "compression_ratio": 1.4161490683229814, "no_speech_prob": 3.0894279916537926e-06}, {"id": 596, "seek": 322812, "start": 3241.12, "end": 3248.12, "text": " Good question.", "tokens": [2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.14316514333089192, "compression_ratio": 1.4161490683229814, "no_speech_prob": 3.0894279916537926e-06}, {"id": 597, "seek": 322812, "start": 3248.12, "end": 3253.12, "text": " So here this was just kind of a first stab at implementing something that would give", "tokens": [407, 510, 341, 390, 445, 733, 295, 257, 700, 16343, 412, 18114, 746, 300, 576, 976], "temperature": 0.0, "avg_logprob": -0.14316514333089192, "compression_ratio": 1.4161490683229814, "no_speech_prob": 3.0894279916537926e-06}, {"id": 598, "seek": 325312, "start": 3253.12, "end": 3266.12, "text": " us an answer for linear least squares regression.", "tokens": [505, 364, 1867, 337, 8213, 1935, 19368, 24590, 13], "temperature": 0.0, "avg_logprob": -0.15887646675109862, "compression_ratio": 1.5576923076923077, "no_speech_prob": 4.637761776393745e-06}, {"id": 599, "seek": 325312, "start": 3266.12, "end": 3274.12, "text": " Alright, so then coming off of that, so these are called the normal equations, this equation", "tokens": [2798, 11, 370, 550, 1348, 766, 295, 300, 11, 370, 613, 366, 1219, 264, 2710, 11787, 11, 341, 5367], "temperature": 0.0, "avg_logprob": -0.15887646675109862, "compression_ratio": 1.5576923076923077, "no_speech_prob": 4.637761776393745e-06}, {"id": 600, "seek": 325312, "start": 3274.12, "end": 3276.12, "text": " we got above.", "tokens": [321, 658, 3673, 13], "temperature": 0.0, "avg_logprob": -0.15887646675109862, "compression_ratio": 1.5576923076923077, "no_speech_prob": 4.637761776393745e-06}, {"id": 601, "seek": 325312, "start": 3276.12, "end": 3281.12, "text": " And really this is, this equation has been rewritten and we're just taking A transpose", "tokens": [400, 534, 341, 307, 11, 341, 5367, 575, 668, 319, 26859, 293, 321, 434, 445, 1940, 316, 25167], "temperature": 0.0, "avg_logprob": -0.15887646675109862, "compression_ratio": 1.5576923076923077, "no_speech_prob": 4.637761776393745e-06}, {"id": 602, "seek": 328112, "start": 3281.12, "end": 3284.12, "text": " B, or I guess we're taking A transpose AX to the other side.", "tokens": [363, 11, 420, 286, 2041, 321, 434, 1940, 316, 25167, 316, 55, 281, 264, 661, 1252, 13], "temperature": 0.0, "avg_logprob": -0.09645585126655046, "compression_ratio": 1.7544910179640718, "no_speech_prob": 2.546588257246185e-05}, {"id": 603, "seek": 328112, "start": 3284.12, "end": 3291.12, "text": " So we have A transpose AX equals A transpose B.", "tokens": [407, 321, 362, 316, 25167, 316, 55, 6915, 316, 25167, 363, 13], "temperature": 0.0, "avg_logprob": -0.09645585126655046, "compression_ratio": 1.7544910179640718, "no_speech_prob": 2.546588257246185e-05}, {"id": 604, "seek": 328112, "start": 3291.12, "end": 3303.12, "text": " And so if A is full rank, A transpose A is going to give us a, if A is square, well actually", "tokens": [400, 370, 498, 316, 307, 1577, 6181, 11, 316, 25167, 316, 307, 516, 281, 976, 505, 257, 11, 498, 316, 307, 3732, 11, 731, 767], "temperature": 0.0, "avg_logprob": -0.09645585126655046, "compression_ratio": 1.7544910179640718, "no_speech_prob": 2.546588257246185e-05}, {"id": 605, "seek": 328112, "start": 3303.12, "end": 3310.12, "text": " not even square, if A is full rank, we're going to get a square Hermitian positive definite", "tokens": [406, 754, 3732, 11, 498, 316, 307, 1577, 6181, 11, 321, 434, 516, 281, 483, 257, 3732, 21842, 270, 952, 3353, 25131], "temperature": 0.0, "avg_logprob": -0.09645585126655046, "compression_ratio": 1.7544910179640718, "no_speech_prob": 2.546588257246185e-05}, {"id": 606, "seek": 331012, "start": 3310.12, "end": 3318.12, "text": " matrix for A transpose A. And square Hermitian positive definite matrices have something", "tokens": [8141, 337, 316, 25167, 316, 13, 400, 3732, 21842, 270, 952, 3353, 25131, 32284, 362, 746], "temperature": 0.0, "avg_logprob": -0.09837277473941926, "compression_ratio": 1.5808383233532934, "no_speech_prob": 6.962191946513485e-06}, {"id": 607, "seek": 331012, "start": 3318.12, "end": 3320.12, "text": " called a Cholesky decomposition.", "tokens": [1219, 257, 761, 7456, 4133, 48356, 13], "temperature": 0.0, "avg_logprob": -0.09837277473941926, "compression_ratio": 1.5808383233532934, "no_speech_prob": 6.962191946513485e-06}, {"id": 608, "seek": 331012, "start": 3320.12, "end": 3327.12, "text": " And the Cholesky decomposition, or Cholesky factorization finds an upper triangular matrix", "tokens": [400, 264, 761, 7456, 4133, 48356, 11, 420, 761, 7456, 4133, 5952, 2144, 10704, 364, 6597, 38190, 8141], "temperature": 0.0, "avg_logprob": -0.09837277473941926, "compression_ratio": 1.5808383233532934, "no_speech_prob": 6.962191946513485e-06}, {"id": 609, "seek": 331012, "start": 3327.12, "end": 3335.12, "text": " R such that R transpose times R equals your matrix.", "tokens": [497, 1270, 300, 497, 25167, 1413, 497, 6915, 428, 8141, 13], "temperature": 0.0, "avg_logprob": -0.09837277473941926, "compression_ratio": 1.5808383233532934, "no_speech_prob": 6.962191946513485e-06}, {"id": 610, "seek": 333512, "start": 3335.12, "end": 3343.12, "text": " And what would R, so if R is upper triangular, what would R transpose be?", "tokens": [400, 437, 576, 497, 11, 370, 498, 497, 307, 6597, 38190, 11, 437, 576, 497, 25167, 312, 30], "temperature": 0.0, "avg_logprob": -0.1269174575805664, "compression_ratio": 1.551948051948052, "no_speech_prob": 2.9944171728857327e-06}, {"id": 611, "seek": 333512, "start": 3343.12, "end": 3351.12, "text": " Yes, lower triangular, that's correct.", "tokens": [1079, 11, 3126, 38190, 11, 300, 311, 3006, 13], "temperature": 0.0, "avg_logprob": -0.1269174575805664, "compression_ratio": 1.551948051948052, "no_speech_prob": 2.9944171728857327e-06}, {"id": 612, "seek": 333512, "start": 3351.12, "end": 3355.12, "text": " Not all questions I ask are hard.", "tokens": [1726, 439, 1651, 286, 1029, 366, 1152, 13], "temperature": 0.0, "avg_logprob": -0.1269174575805664, "compression_ratio": 1.551948051948052, "no_speech_prob": 2.9944171728857327e-06}, {"id": 613, "seek": 333512, "start": 3355.12, "end": 3360.12, "text": " So, so what we're doing, so in this case we're looking at A transpose A, we're going to take", "tokens": [407, 11, 370, 437, 321, 434, 884, 11, 370, 294, 341, 1389, 321, 434, 1237, 412, 316, 25167, 316, 11, 321, 434, 516, 281, 747], "temperature": 0.0, "avg_logprob": -0.1269174575805664, "compression_ratio": 1.551948051948052, "no_speech_prob": 2.9944171728857327e-06}, {"id": 614, "seek": 336012, "start": 3360.12, "end": 3368.12, "text": " the Cholesky factorization and that'll give us back R such that A transpose A is R transpose", "tokens": [264, 761, 7456, 4133, 5952, 2144, 293, 300, 603, 976, 505, 646, 497, 1270, 300, 316, 25167, 316, 307, 497, 25167], "temperature": 0.0, "avg_logprob": -0.09442994214486385, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.593623892404139e-05}, {"id": 615, "seek": 336012, "start": 3368.12, "end": 3370.12, "text": " times R.", "tokens": [1413, 497, 13], "temperature": 0.0, "avg_logprob": -0.09442994214486385, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.593623892404139e-05}, {"id": 616, "seek": 336012, "start": 3370.12, "end": 3383.12, "text": " A transpose times A equals R transpose times R.", "tokens": [316, 25167, 1413, 316, 6915, 497, 25167, 1413, 497, 13], "temperature": 0.0, "avg_logprob": -0.09442994214486385, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.593623892404139e-05}, {"id": 617, "seek": 336012, "start": 3383.12, "end": 3388.12, "text": " Here I've just pre-computed A transpose A, since I'm going to be using it a few times,", "tokens": [1692, 286, 600, 445, 659, 12, 1112, 2582, 292, 316, 25167, 316, 11, 1670, 286, 478, 516, 281, 312, 1228, 309, 257, 1326, 1413, 11], "temperature": 0.0, "avg_logprob": -0.09442994214486385, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.593623892404139e-05}, {"id": 618, "seek": 338812, "start": 3388.12, "end": 3393.12, "text": " as well as A transpose B.", "tokens": [382, 731, 382, 316, 25167, 363, 13], "temperature": 0.0, "avg_logprob": -0.06078941968022561, "compression_ratio": 1.603448275862069, "no_speech_prob": 1.6700892956578173e-05}, {"id": 619, "seek": 338812, "start": 3393.12, "end": 3400.12, "text": " And then I'm going to use SciPy's implementation of the Cholesky factorization as a warning.", "tokens": [400, 550, 286, 478, 516, 281, 764, 16942, 47, 88, 311, 11420, 295, 264, 761, 7456, 4133, 5952, 2144, 382, 257, 9164, 13], "temperature": 0.0, "avg_logprob": -0.06078941968022561, "compression_ratio": 1.603448275862069, "no_speech_prob": 1.6700892956578173e-05}, {"id": 620, "seek": 338812, "start": 3400.12, "end": 3405.12, "text": " NumPy and SciPy default to different, one of them returns the upper triangular matrix", "tokens": [22592, 47, 88, 293, 16942, 47, 88, 7576, 281, 819, 11, 472, 295, 552, 11247, 264, 6597, 38190, 8141], "temperature": 0.0, "avg_logprob": -0.06078941968022561, "compression_ratio": 1.603448275862069, "no_speech_prob": 1.6700892956578173e-05}, {"id": 621, "seek": 338812, "start": 3405.12, "end": 3409.12, "text": " and one returns the lower triangular matrix, which I discovered the hard way.", "tokens": [293, 472, 11247, 264, 3126, 38190, 8141, 11, 597, 286, 6941, 264, 1152, 636, 13], "temperature": 0.0, "avg_logprob": -0.06078941968022561, "compression_ratio": 1.603448275862069, "no_speech_prob": 1.6700892956578173e-05}, {"id": 622, "seek": 338812, "start": 3409.12, "end": 3413.12, "text": " I didn't realize I was switching between NumPy and SciPy and was like using this in other", "tokens": [286, 994, 380, 4325, 286, 390, 16493, 1296, 22592, 47, 88, 293, 16942, 47, 88, 293, 390, 411, 1228, 341, 294, 661], "temperature": 0.0, "avg_logprob": -0.06078941968022561, "compression_ratio": 1.603448275862069, "no_speech_prob": 1.6700892956578173e-05}, {"id": 623, "seek": 341312, "start": 3413.12, "end": 3418.12, "text": " operations and getting different answers.", "tokens": [7705, 293, 1242, 819, 6338, 13], "temperature": 0.0, "avg_logprob": -0.1399203328525319, "compression_ratio": 1.4137931034482758, "no_speech_prob": 9.368512110086158e-06}, {"id": 624, "seek": 341312, "start": 3418.12, "end": 3421.12, "text": " Yeah, so we can, and so it's always good after you do this to kind of check that what you're", "tokens": [865, 11, 370, 321, 393, 11, 293, 370, 309, 311, 1009, 665, 934, 291, 360, 341, 281, 733, 295, 1520, 300, 437, 291, 434], "temperature": 0.0, "avg_logprob": -0.1399203328525319, "compression_ratio": 1.4137931034482758, "no_speech_prob": 9.368512110086158e-06}, {"id": 625, "seek": 341312, "start": 3421.12, "end": 3423.12, "text": " doing makes sense.", "tokens": [884, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.1399203328525319, "compression_ratio": 1.4137931034482758, "no_speech_prob": 9.368512110086158e-06}, {"id": 626, "seek": 341312, "start": 3423.12, "end": 3434.12, "text": " So I'm passing A transpose A in, and actually let's even look at, oh, okay, that's difficult", "tokens": [407, 286, 478, 8437, 316, 25167, 316, 294, 11, 293, 767, 718, 311, 754, 574, 412, 11, 1954, 11, 1392, 11, 300, 311, 2252], "temperature": 0.0, "avg_logprob": -0.1399203328525319, "compression_ratio": 1.4137931034482758, "no_speech_prob": 9.368512110086158e-06}, {"id": 627, "seek": 343412, "start": 3434.12, "end": 3447.12, "text": " to see.", "tokens": [281, 536, 13], "temperature": 0.0, "avg_logprob": -0.11714165299027055, "compression_ratio": 1.3185185185185184, "no_speech_prob": 7.296206604223698e-06}, {"id": 628, "seek": 343412, "start": 3447.12, "end": 3451.12, "text": " So remember, np.setprintoptions is super useful.", "tokens": [407, 1604, 11, 33808, 13, 3854, 14030, 5747, 626, 307, 1687, 4420, 13], "temperature": 0.0, "avg_logprob": -0.11714165299027055, "compression_ratio": 1.3185185185185184, "no_speech_prob": 7.296206604223698e-06}, {"id": 629, "seek": 343412, "start": 3451.12, "end": 3458.12, "text": " You can do suppress equals true to not show zeros in this format with an exponential,", "tokens": [509, 393, 360, 26835, 6915, 2074, 281, 406, 855, 35193, 294, 341, 7877, 365, 364, 21510, 11], "temperature": 0.0, "avg_logprob": -0.11714165299027055, "compression_ratio": 1.3185185185185184, "no_speech_prob": 7.296206604223698e-06}, {"id": 630, "seek": 343412, "start": 3458.12, "end": 3463.12, "text": " 0.0000, which is not very readable.", "tokens": [1958, 13, 628, 628, 11, 597, 307, 406, 588, 49857, 13], "temperature": 0.0, "avg_logprob": -0.11714165299027055, "compression_ratio": 1.3185185185185184, "no_speech_prob": 7.296206604223698e-06}, {"id": 631, "seek": 346312, "start": 3463.12, "end": 3468.12, "text": " And then you can also set the number of digits you want to see.", "tokens": [400, 550, 291, 393, 611, 992, 264, 1230, 295, 27011, 291, 528, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.07222238453951749, "compression_ratio": 1.5852534562211982, "no_speech_prob": 7.296299372683279e-06}, {"id": 632, "seek": 346312, "start": 3468.12, "end": 3473.12, "text": " Although this matrix, I guess, is still kind of too large to be that viewable.", "tokens": [5780, 341, 8141, 11, 286, 2041, 11, 307, 920, 733, 295, 886, 2416, 281, 312, 300, 1910, 712, 13], "temperature": 0.0, "avg_logprob": -0.07222238453951749, "compression_ratio": 1.5852534562211982, "no_speech_prob": 7.296299372683279e-06}, {"id": 633, "seek": 346312, "start": 3473.12, "end": 3480.12, "text": " But I can believe that this looks like an upper triangular matrix that I've gotten back.", "tokens": [583, 286, 393, 1697, 300, 341, 1542, 411, 364, 6597, 38190, 8141, 300, 286, 600, 5768, 646, 13], "temperature": 0.0, "avg_logprob": -0.07222238453951749, "compression_ratio": 1.5852534562211982, "no_speech_prob": 7.296299372683279e-06}, {"id": 634, "seek": 346312, "start": 3480.12, "end": 3488.12, "text": " And then I'm just confirming that A transpose A minus r dot transpose times r is giving me", "tokens": [400, 550, 286, 478, 445, 42861, 300, 316, 25167, 316, 3175, 367, 5893, 25167, 1413, 367, 307, 2902, 385], "temperature": 0.0, "avg_logprob": -0.07222238453951749, "compression_ratio": 1.5852534562211982, "no_speech_prob": 7.296299372683279e-06}, {"id": 635, "seek": 346312, "start": 3488.12, "end": 3490.12, "text": " something close to 0.", "tokens": [746, 1998, 281, 1958, 13], "temperature": 0.0, "avg_logprob": -0.07222238453951749, "compression_ratio": 1.5852534562211982, "no_speech_prob": 7.296299372683279e-06}, {"id": 636, "seek": 349012, "start": 3490.12, "end": 3499.12, "text": " So A transpose A is approximately equal to r dot transpose r.", "tokens": [407, 316, 25167, 316, 307, 10447, 2681, 281, 367, 5893, 25167, 367, 13], "temperature": 0.0, "avg_logprob": -0.09575652448754561, "compression_ratio": 1.4764397905759161, "no_speech_prob": 7.29630482965149e-06}, {"id": 637, "seek": 349012, "start": 3499.12, "end": 3506.12, "text": " And so then the way that the Cholesky factorization is useful in solving the linear least squares", "tokens": [400, 370, 550, 264, 636, 300, 264, 761, 7456, 4133, 5952, 2144, 307, 4420, 294, 12606, 264, 8213, 1935, 19368], "temperature": 0.0, "avg_logprob": -0.09575652448754561, "compression_ratio": 1.4764397905759161, "no_speech_prob": 7.29630482965149e-06}, {"id": 638, "seek": 349012, "start": 3506.12, "end": 3510.12, "text": " problem is so we can plug it in.", "tokens": [1154, 307, 370, 321, 393, 5452, 309, 294, 13], "temperature": 0.0, "avg_logprob": -0.09575652448754561, "compression_ratio": 1.4764397905759161, "no_speech_prob": 7.29630482965149e-06}, {"id": 639, "seek": 349012, "start": 3510.12, "end": 3518.12, "text": " Now we have our r instead of A. Then we have r transpose, and here I'm calling rx, saying", "tokens": [823, 321, 362, 527, 367, 2602, 295, 316, 13, 1396, 321, 362, 367, 25167, 11, 293, 510, 286, 478, 5141, 367, 87, 11, 1566], "temperature": 0.0, "avg_logprob": -0.09575652448754561, "compression_ratio": 1.4764397905759161, "no_speech_prob": 7.29630482965149e-06}, {"id": 640, "seek": 351812, "start": 3518.12, "end": 3522.12, "text": " that that equals w, which you can see in this last line, setting that equals to A transpose", "tokens": [300, 300, 6915, 261, 11, 597, 291, 393, 536, 294, 341, 1036, 1622, 11, 3287, 300, 6915, 281, 316, 25167], "temperature": 0.0, "avg_logprob": -0.12065349481044671, "compression_ratio": 1.6243093922651934, "no_speech_prob": 6.502371979877353e-05}, {"id": 641, "seek": 351812, "start": 3522.12, "end": 3526.12, "text": " B, and getting rx equals w.", "tokens": [363, 11, 293, 1242, 367, 87, 6915, 261, 13], "temperature": 0.0, "avg_logprob": -0.12065349481044671, "compression_ratio": 1.6243093922651934, "no_speech_prob": 6.502371979877353e-05}, {"id": 642, "seek": 351812, "start": 3526.12, "end": 3528.12, "text": " So why is this helpful?", "tokens": [407, 983, 307, 341, 4961, 30], "temperature": 0.0, "avg_logprob": -0.12065349481044671, "compression_ratio": 1.6243093922651934, "no_speech_prob": 6.502371979877353e-05}, {"id": 643, "seek": 351812, "start": 3528.12, "end": 3532.12, "text": " I've just kind of written this equation a few different ways.", "tokens": [286, 600, 445, 733, 295, 3720, 341, 5367, 257, 1326, 819, 2098, 13], "temperature": 0.0, "avg_logprob": -0.12065349481044671, "compression_ratio": 1.6243093922651934, "no_speech_prob": 6.502371979877353e-05}, {"id": 644, "seek": 351812, "start": 3532.12, "end": 3538.12, "text": " Why might this be better than just sticking with our A transpose A, x equals A transpose", "tokens": [1545, 1062, 341, 312, 1101, 813, 445, 13465, 365, 527, 316, 25167, 316, 11, 2031, 6915, 316, 25167], "temperature": 0.0, "avg_logprob": -0.12065349481044671, "compression_ratio": 1.6243093922651934, "no_speech_prob": 6.502371979877353e-05}, {"id": 645, "seek": 353812, "start": 3538.12, "end": 3549.12, "text": " B?", "tokens": [363, 30], "temperature": 0.0, "avg_logprob": -0.16104967329237196, "compression_ratio": 1.191304347826087, "no_speech_prob": 3.535373616614379e-05}, {"id": 646, "seek": 353812, "start": 3549.12, "end": 3558.12, "text": " Oh wait, Matthew can throw it.", "tokens": [876, 1699, 11, 12434, 393, 3507, 309, 13], "temperature": 0.0, "avg_logprob": -0.16104967329237196, "compression_ratio": 1.191304347826087, "no_speech_prob": 3.535373616614379e-05}, {"id": 647, "seek": 353812, "start": 3558.12, "end": 3564.12, "text": " So now it's much easier to solve the last equation, because the r is like a triangular", "tokens": [407, 586, 309, 311, 709, 3571, 281, 5039, 264, 1036, 5367, 11, 570, 264, 367, 307, 411, 257, 38190], "temperature": 0.0, "avg_logprob": -0.16104967329237196, "compression_ratio": 1.191304347826087, "no_speech_prob": 3.535373616614379e-05}, {"id": 648, "seek": 353812, "start": 3564.12, "end": 3565.12, "text": " matrix.", "tokens": [8141, 13], "temperature": 0.0, "avg_logprob": -0.16104967329237196, "compression_ratio": 1.191304347826087, "no_speech_prob": 3.535373616614379e-05}, {"id": 649, "seek": 353812, "start": 3565.12, "end": 3566.12, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.16104967329237196, "compression_ratio": 1.191304347826087, "no_speech_prob": 3.535373616614379e-05}, {"id": 650, "seek": 356612, "start": 3566.12, "end": 3568.12, "text": " So that's it.", "tokens": [407, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.13087021432271817, "compression_ratio": 1.6358695652173914, "no_speech_prob": 5.338089067663532e-06}, {"id": 651, "seek": 356612, "start": 3568.12, "end": 3579.12, "text": " Solving linear systems of equations for a triangular matrix is a lot easier, because", "tokens": [7026, 798, 8213, 3652, 295, 11787, 337, 257, 38190, 8141, 307, 257, 688, 3571, 11, 570], "temperature": 0.0, "avg_logprob": -0.13087021432271817, "compression_ratio": 1.6358695652173914, "no_speech_prob": 5.338089067663532e-06}, {"id": 652, "seek": 356612, "start": 3579.12, "end": 3584.12, "text": " you just start with the last entry, divide by one thing, then you plug into the second", "tokens": [291, 445, 722, 365, 264, 1036, 8729, 11, 9845, 538, 472, 551, 11, 550, 291, 5452, 666, 264, 1150], "temperature": 0.0, "avg_logprob": -0.13087021432271817, "compression_ratio": 1.6358695652173914, "no_speech_prob": 5.338089067663532e-06}, {"id": 653, "seek": 356612, "start": 3584.12, "end": 3586.12, "text": " to last entry, and so on.", "tokens": [281, 1036, 8729, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.13087021432271817, "compression_ratio": 1.6358695652173914, "no_speech_prob": 5.338089067663532e-06}, {"id": 654, "seek": 356612, "start": 3586.12, "end": 3592.12, "text": " So it's much simpler than solving a linear system where you have a non-triangular matrix.", "tokens": [407, 309, 311, 709, 18587, 813, 12606, 257, 8213, 1185, 689, 291, 362, 257, 2107, 12, 83, 470, 656, 1040, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13087021432271817, "compression_ratio": 1.6358695652173914, "no_speech_prob": 5.338089067663532e-06}, {"id": 655, "seek": 359212, "start": 3592.12, "end": 3599.12, "text": " And so we've kind of just changed this into having to solve two triangular systems of", "tokens": [400, 370, 321, 600, 733, 295, 445, 3105, 341, 666, 1419, 281, 5039, 732, 38190, 3652, 295], "temperature": 0.0, "avg_logprob": -0.0946252753094929, "compression_ratio": 1.5748792270531402, "no_speech_prob": 3.726622935573687e-06}, {"id": 656, "seek": 359212, "start": 3599.12, "end": 3600.12, "text": " equations.", "tokens": [11787, 13], "temperature": 0.0, "avg_logprob": -0.0946252753094929, "compression_ratio": 1.5748792270531402, "no_speech_prob": 3.726622935573687e-06}, {"id": 657, "seek": 359212, "start": 3600.12, "end": 3609.12, "text": " So we solve, I guess we would solve this one first, define w, and then we solve the one", "tokens": [407, 321, 5039, 11, 286, 2041, 321, 576, 5039, 341, 472, 700, 11, 6964, 261, 11, 293, 550, 321, 5039, 264, 472], "temperature": 0.0, "avg_logprob": -0.0946252753094929, "compression_ratio": 1.5748792270531402, "no_speech_prob": 3.726622935573687e-06}, {"id": 658, "seek": 359212, "start": 3609.12, "end": 3613.12, "text": " on the last line to find x, which is our original answer.", "tokens": [322, 264, 1036, 1622, 281, 915, 2031, 11, 597, 307, 527, 3380, 1867, 13], "temperature": 0.0, "avg_logprob": -0.0946252753094929, "compression_ratio": 1.5748792270531402, "no_speech_prob": 3.726622935573687e-06}, {"id": 659, "seek": 359212, "start": 3613.12, "end": 3620.12, "text": " This I think kind of illustrates a lot of numerical linear algebra is about getting", "tokens": [639, 286, 519, 733, 295, 41718, 257, 688, 295, 29054, 8213, 21989, 307, 466, 1242], "temperature": 0.0, "avg_logprob": -0.0946252753094929, "compression_ratio": 1.5748792270531402, "no_speech_prob": 3.726622935573687e-06}, {"id": 660, "seek": 362012, "start": 3620.12, "end": 3624.12, "text": " these factorizations, and often it looks like, okay, you've just rewritten the problem, you", "tokens": [613, 5952, 14455, 11, 293, 2049, 309, 1542, 411, 11, 1392, 11, 291, 600, 445, 319, 26859, 264, 1154, 11, 291], "temperature": 0.0, "avg_logprob": -0.11046219897526567, "compression_ratio": 1.6636363636363636, "no_speech_prob": 6.540355570905376e-06}, {"id": 661, "seek": 362012, "start": 3624.12, "end": 3628.12, "text": " know, you still have all these factors, but it's because of the properties of the matrices", "tokens": [458, 11, 291, 920, 362, 439, 613, 6771, 11, 457, 309, 311, 570, 295, 264, 7221, 295, 264, 32284], "temperature": 0.0, "avg_logprob": -0.11046219897526567, "compression_ratio": 1.6636363636363636, "no_speech_prob": 6.540355570905376e-06}, {"id": 662, "seek": 362012, "start": 3628.12, "end": 3636.12, "text": " you've factored into, you've made things easier for yourself in some way.", "tokens": [291, 600, 1186, 2769, 666, 11, 291, 600, 1027, 721, 3571, 337, 1803, 294, 512, 636, 13], "temperature": 0.0, "avg_logprob": -0.11046219897526567, "compression_ratio": 1.6636363636363636, "no_speech_prob": 6.540355570905376e-06}, {"id": 663, "seek": 362012, "start": 3636.12, "end": 3640.12, "text": " Any questions about this, or questions about why it's easier to solve a triangular system", "tokens": [2639, 1651, 466, 341, 11, 420, 1651, 466, 983, 309, 311, 3571, 281, 5039, 257, 38190, 1185], "temperature": 0.0, "avg_logprob": -0.11046219897526567, "compression_ratio": 1.6636363636363636, "no_speech_prob": 6.540355570905376e-06}, {"id": 664, "seek": 362012, "start": 3640.12, "end": 3645.12, "text": " of equations?", "tokens": [295, 11787, 30], "temperature": 0.0, "avg_logprob": -0.11046219897526567, "compression_ratio": 1.6636363636363636, "no_speech_prob": 6.540355570905376e-06}, {"id": 665, "seek": 362012, "start": 3645.12, "end": 3646.12, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.11046219897526567, "compression_ratio": 1.6636363636363636, "no_speech_prob": 6.540355570905376e-06}, {"id": 666, "seek": 364612, "start": 3646.12, "end": 3655.12, "text": " So yeah, this is just kind of rewritten, the least squares linear regression problem.", "tokens": [407, 1338, 11, 341, 307, 445, 733, 295, 319, 26859, 11, 264, 1935, 19368, 8213, 24590, 1154, 13], "temperature": 0.0, "avg_logprob": -0.12287316806074502, "compression_ratio": 1.5511363636363635, "no_speech_prob": 1.147859074990265e-05}, {"id": 667, "seek": 364612, "start": 3655.12, "end": 3661.12, "text": " So here kind of working our way, working our way towards x, first we're going to solve", "tokens": [407, 510, 733, 295, 1364, 527, 636, 11, 1364, 527, 636, 3030, 2031, 11, 700, 321, 434, 516, 281, 5039], "temperature": 0.0, "avg_logprob": -0.12287316806074502, "compression_ratio": 1.5511363636363635, "no_speech_prob": 1.147859074990265e-05}, {"id": 668, "seek": 364612, "start": 3661.12, "end": 3670.12, "text": " the triangular system, our transpose w equals a transpose b, which is what's happening in", "tokens": [264, 38190, 1185, 11, 527, 25167, 261, 6915, 257, 25167, 272, 11, 597, 307, 437, 311, 2737, 294], "temperature": 0.0, "avg_logprob": -0.12287316806074502, "compression_ratio": 1.5511363636363635, "no_speech_prob": 1.147859074990265e-05}, {"id": 669, "seek": 364612, "start": 3670.12, "end": 3672.12, "text": " this line.", "tokens": [341, 1622, 13], "temperature": 0.0, "avg_logprob": -0.12287316806074502, "compression_ratio": 1.5511363636363635, "no_speech_prob": 1.147859074990265e-05}, {"id": 670, "seek": 367212, "start": 3672.12, "end": 3678.12, "text": " And so SciPy, Lin-Aoj has a solve triangular method, even, where you give it a triangular", "tokens": [400, 370, 16942, 47, 88, 11, 9355, 12, 32, 78, 73, 575, 257, 5039, 38190, 3170, 11, 754, 11, 689, 291, 976, 309, 257, 38190], "temperature": 0.0, "avg_logprob": -0.15898241996765136, "compression_ratio": 1.6696428571428572, "no_speech_prob": 8.139565579767805e-06}, {"id": 671, "seek": 367212, "start": 3678.12, "end": 3680.12, "text": " matrix, and that has a transpose argument.", "tokens": [8141, 11, 293, 300, 575, 257, 25167, 6770, 13], "temperature": 0.0, "avg_logprob": -0.15898241996765136, "compression_ratio": 1.6696428571428572, "no_speech_prob": 8.139565579767805e-06}, {"id": 672, "seek": 367212, "start": 3680.12, "end": 3683.12, "text": " So here we're letting it know this is our transpose.", "tokens": [407, 510, 321, 434, 8295, 309, 458, 341, 307, 527, 25167, 13], "temperature": 0.0, "avg_logprob": -0.15898241996765136, "compression_ratio": 1.6696428571428572, "no_speech_prob": 8.139565579767805e-06}, {"id": 673, "seek": 367212, "start": 3683.12, "end": 3690.12, "text": " Again, we're kind of checking that our result is what we were hoping, and that our transpose", "tokens": [3764, 11, 321, 434, 733, 295, 8568, 300, 527, 1874, 307, 437, 321, 645, 7159, 11, 293, 300, 527, 25167], "temperature": 0.0, "avg_logprob": -0.15898241996765136, "compression_ratio": 1.6696428571428572, "no_speech_prob": 8.139565579767805e-06}, {"id": 674, "seek": 367212, "start": 3690.12, "end": 3694.12, "text": " times w minus a transpose b is close to zero.", "tokens": [1413, 261, 3175, 257, 25167, 272, 307, 1998, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.15898241996765136, "compression_ratio": 1.6696428571428572, "no_speech_prob": 8.139565579767805e-06}, {"id": 675, "seek": 367212, "start": 3694.12, "end": 3700.12, "text": " So we've successfully solved this third equation.", "tokens": [407, 321, 600, 10727, 13041, 341, 2636, 5367, 13], "temperature": 0.0, "avg_logprob": -0.15898241996765136, "compression_ratio": 1.6696428571428572, "no_speech_prob": 8.139565579767805e-06}, {"id": 676, "seek": 370012, "start": 3700.12, "end": 3707.12, "text": " And then from here we solve triangular r with w on the right in order to get x, which I", "tokens": [400, 550, 490, 510, 321, 5039, 38190, 367, 365, 261, 322, 264, 558, 294, 1668, 281, 483, 2031, 11, 597, 286], "temperature": 0.0, "avg_logprob": -0.0949324230815089, "compression_ratio": 1.6802030456852792, "no_speech_prob": 1.2805073311028536e-05}, {"id": 677, "seek": 370012, "start": 3707.12, "end": 3710.12, "text": " called a coefficients Cholesky here.", "tokens": [1219, 257, 31994, 761, 7456, 4133, 510, 13], "temperature": 0.0, "avg_logprob": -0.0949324230815089, "compression_ratio": 1.6802030456852792, "no_speech_prob": 1.2805073311028536e-05}, {"id": 678, "seek": 370012, "start": 3710.12, "end": 3713.12, "text": " And we got that that works.", "tokens": [400, 321, 658, 300, 300, 1985, 13], "temperature": 0.0, "avg_logprob": -0.0949324230815089, "compression_ratio": 1.6802030456852792, "no_speech_prob": 1.2805073311028536e-05}, {"id": 679, "seek": 370012, "start": 3713.12, "end": 3718.12, "text": " And so I can put this all together in a method and say that solving least squares with the", "tokens": [400, 370, 286, 393, 829, 341, 439, 1214, 294, 257, 3170, 293, 584, 300, 12606, 1935, 19368, 365, 264], "temperature": 0.0, "avg_logprob": -0.0949324230815089, "compression_ratio": 1.6802030456852792, "no_speech_prob": 1.2805073311028536e-05}, {"id": 680, "seek": 370012, "start": 3718.12, "end": 3726.12, "text": " Cholesky decomposition takes in a and b, gets the Cholesky decomposition of a transpose", "tokens": [761, 7456, 4133, 48356, 2516, 294, 257, 293, 272, 11, 2170, 264, 761, 7456, 4133, 48356, 295, 257, 25167], "temperature": 0.0, "avg_logprob": -0.0949324230815089, "compression_ratio": 1.6802030456852792, "no_speech_prob": 1.2805073311028536e-05}, {"id": 681, "seek": 372612, "start": 3726.12, "end": 3734.12, "text": " times a, and then it solves those two triangular systems and returns the answer.", "tokens": [1413, 257, 11, 293, 550, 309, 39890, 729, 732, 38190, 3652, 293, 11247, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.20627520424979073, "compression_ratio": 1.2201834862385321, "no_speech_prob": 7.183049092418514e-06}, {"id": 682, "seek": 372612, "start": 3739.12, "end": 3741.12, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.20627520424979073, "compression_ratio": 1.2201834862385321, "no_speech_prob": 7.183049092418514e-06}, {"id": 683, "seek": 372612, "start": 3745.12, "end": 3748.12, "text": " Kelsey, can you throw the microphone?", "tokens": [44714, 11, 393, 291, 3507, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.20627520424979073, "compression_ratio": 1.2201834862385321, "no_speech_prob": 7.183049092418514e-06}, {"id": 684, "seek": 374812, "start": 3748.12, "end": 3758.12, "text": " Why was this slower than the naive way?", "tokens": [1545, 390, 341, 14009, 813, 264, 29052, 636, 30], "temperature": 0.0, "avg_logprob": -0.20444058638352613, "compression_ratio": 1.371069182389937, "no_speech_prob": 2.7105903427582234e-05}, {"id": 685, "seek": 374812, "start": 3758.12, "end": 3763.12, "text": " Yeah, that's a good question.", "tokens": [865, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.20444058638352613, "compression_ratio": 1.371069182389937, "no_speech_prob": 2.7105903427582234e-05}, {"id": 686, "seek": 374812, "start": 3763.12, "end": 3770.12, "text": " So I think here, and I have to look this up, that's probably the cost of calculating the", "tokens": [407, 286, 519, 510, 11, 293, 286, 362, 281, 574, 341, 493, 11, 300, 311, 1391, 264, 2063, 295, 28258, 264], "temperature": 0.0, "avg_logprob": -0.20444058638352613, "compression_ratio": 1.371069182389937, "no_speech_prob": 2.7105903427582234e-05}, {"id": 687, "seek": 374812, "start": 3770.12, "end": 3772.12, "text": " Cholesky decomposition.", "tokens": [761, 7456, 4133, 48356, 13], "temperature": 0.0, "avg_logprob": -0.20444058638352613, "compression_ratio": 1.371069182389937, "no_speech_prob": 2.7105903427582234e-05}, {"id": 688, "seek": 374812, "start": 3772.12, "end": 3775.12, "text": " And so I'll check on that with you.", "tokens": [400, 370, 286, 603, 1520, 322, 300, 365, 291, 13], "temperature": 0.0, "avg_logprob": -0.20444058638352613, "compression_ratio": 1.371069182389937, "no_speech_prob": 2.7105903427582234e-05}, {"id": 689, "seek": 377512, "start": 3775.12, "end": 3778.12, "text": " I'll put this down.", "tokens": [286, 603, 829, 341, 760, 13], "temperature": 0.0, "avg_logprob": -0.12129920104454303, "compression_ratio": 1.389261744966443, "no_speech_prob": 2.0784509615623392e-05}, {"id": 690, "seek": 377512, "start": 3787.12, "end": 3791.12, "text": " I think it's also to remember the naive way you have this instability that comes with", "tokens": [286, 519, 309, 311, 611, 281, 1604, 264, 29052, 636, 291, 362, 341, 34379, 300, 1487, 365], "temperature": 0.0, "avg_logprob": -0.12129920104454303, "compression_ratio": 1.389261744966443, "no_speech_prob": 2.0784509615623392e-05}, {"id": 691, "seek": 377512, "start": 3791.12, "end": 3795.12, "text": " getting the inverse, so that's kind of less good in that regard.", "tokens": [1242, 264, 17340, 11, 370, 300, 311, 733, 295, 1570, 665, 294, 300, 3843, 13], "temperature": 0.0, "avg_logprob": -0.12129920104454303, "compression_ratio": 1.389261744966443, "no_speech_prob": 2.0784509615623392e-05}, {"id": 692, "seek": 377512, "start": 3795.12, "end": 3798.12, "text": " But yeah, I'll check about the time.", "tokens": [583, 1338, 11, 286, 603, 1520, 466, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.12129920104454303, "compression_ratio": 1.389261744966443, "no_speech_prob": 2.0784509615623392e-05}, {"id": 693, "seek": 379812, "start": 3798.12, "end": 3806.12, "text": " Is there some sort of order of the size of your matrix?", "tokens": [1119, 456, 512, 1333, 295, 1668, 295, 264, 2744, 295, 428, 8141, 30], "temperature": 0.0, "avg_logprob": -0.20748960971832275, "compression_ratio": 1.5513513513513513, "no_speech_prob": 7.183029538282426e-06}, {"id": 694, "seek": 379812, "start": 3806.12, "end": 3813.12, "text": " Yes, yeah, I'm sure that it, like this is small enough that I think constant terms matter", "tokens": [1079, 11, 1338, 11, 286, 478, 988, 300, 309, 11, 411, 341, 307, 1359, 1547, 300, 286, 519, 5754, 2115, 1871], "temperature": 0.0, "avg_logprob": -0.20748960971832275, "compression_ratio": 1.5513513513513513, "no_speech_prob": 7.183029538282426e-06}, {"id": 695, "seek": 379812, "start": 3813.12, "end": 3814.12, "text": " more.", "tokens": [544, 13], "temperature": 0.0, "avg_logprob": -0.20748960971832275, "compression_ratio": 1.5513513513513513, "no_speech_prob": 7.183029538282426e-06}, {"id": 696, "seek": 379812, "start": 3814.12, "end": 3819.12, "text": " So when you get to really huge matrices, it does come about, become more about the highest", "tokens": [407, 562, 291, 483, 281, 534, 2603, 32284, 11, 309, 775, 808, 466, 11, 1813, 544, 466, 264, 6343], "temperature": 0.0, "avg_logprob": -0.20748960971832275, "compression_ratio": 1.5513513513513513, "no_speech_prob": 7.183029538282426e-06}, {"id": 697, "seek": 379812, "start": 3819.12, "end": 3820.12, "text": " order term.", "tokens": [1668, 1433, 13], "temperature": 0.0, "avg_logprob": -0.20748960971832275, "compression_ratio": 1.5513513513513513, "no_speech_prob": 7.183029538282426e-06}, {"id": 698, "seek": 379812, "start": 3820.12, "end": 3822.12, "text": " But yeah, the size could matter.", "tokens": [583, 1338, 11, 264, 2744, 727, 1871, 13], "temperature": 0.0, "avg_logprob": -0.20748960971832275, "compression_ratio": 1.5513513513513513, "no_speech_prob": 7.183029538282426e-06}, {"id": 699, "seek": 382212, "start": 3822.12, "end": 3828.12, "text": " And the other question is, so this is not what Scikit was doing?", "tokens": [400, 264, 661, 1168, 307, 11, 370, 341, 307, 406, 437, 16942, 22681, 390, 884, 30], "temperature": 0.0, "avg_logprob": -0.13341534414956735, "compression_ratio": 1.558974358974359, "no_speech_prob": 7.646098310942762e-06}, {"id": 700, "seek": 382212, "start": 3828.12, "end": 3829.12, "text": " Correct.", "tokens": [12753, 13], "temperature": 0.0, "avg_logprob": -0.13341534414956735, "compression_ratio": 1.558974358974359, "no_speech_prob": 7.646098310942762e-06}, {"id": 701, "seek": 382212, "start": 3829.12, "end": 3830.12, "text": " And why?", "tokens": [400, 983, 30], "temperature": 0.0, "avg_logprob": -0.13341534414956735, "compression_ratio": 1.558974358974359, "no_speech_prob": 7.646098310942762e-06}, {"id": 702, "seek": 382212, "start": 3830.12, "end": 3832.12, "text": " So we'll come back to kind of a comparison.", "tokens": [407, 321, 603, 808, 646, 281, 733, 295, 257, 9660, 13], "temperature": 0.0, "avg_logprob": -0.13341534414956735, "compression_ratio": 1.558974358974359, "no_speech_prob": 7.646098310942762e-06}, {"id": 703, "seek": 382212, "start": 3832.12, "end": 3836.12, "text": " So I'm going to kind of show you several different ways to do it, and then we'll like compare", "tokens": [407, 286, 478, 516, 281, 733, 295, 855, 291, 2940, 819, 2098, 281, 360, 309, 11, 293, 550, 321, 603, 411, 6794], "temperature": 0.0, "avg_logprob": -0.13341534414956735, "compression_ratio": 1.558974358974359, "no_speech_prob": 7.646098310942762e-06}, {"id": 704, "seek": 382212, "start": 3836.12, "end": 3837.12, "text": " the different methods.", "tokens": [264, 819, 7150, 13], "temperature": 0.0, "avg_logprob": -0.13341534414956735, "compression_ratio": 1.558974358974359, "no_speech_prob": 7.646098310942762e-06}, {"id": 705, "seek": 382212, "start": 3837.12, "end": 3839.12, "text": " Got good questions?", "tokens": [5803, 665, 1651, 30], "temperature": 0.0, "avg_logprob": -0.13341534414956735, "compression_ratio": 1.558974358974359, "no_speech_prob": 7.646098310942762e-06}, {"id": 706, "seek": 383912, "start": 3839.12, "end": 3852.12, "text": " Any other questions about this approach?", "tokens": [2639, 661, 1651, 466, 341, 3109, 30], "temperature": 0.0, "avg_logprob": -0.1388808163729581, "compression_ratio": 1.4821428571428572, "no_speech_prob": 2.840838351403363e-05}, {"id": 707, "seek": 383912, "start": 3852.12, "end": 3853.12, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1388808163729581, "compression_ratio": 1.4821428571428572, "no_speech_prob": 2.840838351403363e-05}, {"id": 708, "seek": 383912, "start": 3853.12, "end": 3858.12, "text": " And then I think I brought this up last week when I was looking at the history of Gaussian", "tokens": [400, 550, 286, 519, 286, 3038, 341, 493, 1036, 1243, 562, 286, 390, 1237, 412, 264, 2503, 295, 10384, 2023, 952], "temperature": 0.0, "avg_logprob": -0.1388808163729581, "compression_ratio": 1.4821428571428572, "no_speech_prob": 2.840838351403363e-05}, {"id": 709, "seek": 383912, "start": 3858.12, "end": 3863.12, "text": " elimination, but the Cholesky decomposition is the one that Gauss first found that Cholesky", "tokens": [29224, 11, 457, 264, 761, 7456, 4133, 48356, 307, 264, 472, 300, 10384, 2023, 700, 1352, 300, 761, 7456, 4133], "temperature": 0.0, "avg_logprob": -0.1388808163729581, "compression_ratio": 1.4821428571428572, "no_speech_prob": 2.840838351403363e-05}, {"id": 710, "seek": 386312, "start": 3863.12, "end": 3869.12, "text": " got the credit for.", "tokens": [658, 264, 5397, 337, 13], "temperature": 0.0, "avg_logprob": -0.27981793880462646, "compression_ratio": 1.2595419847328244, "no_speech_prob": 2.5465124053880572e-05}, {"id": 711, "seek": 386312, "start": 3869.12, "end": 3872.12, "text": " Oh, yes.", "tokens": [876, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.27981793880462646, "compression_ratio": 1.2595419847328244, "no_speech_prob": 2.5465124053880572e-05}, {"id": 712, "seek": 386312, "start": 3872.12, "end": 3878.12, "text": " Yeah, you can always walk it too.", "tokens": [865, 11, 291, 393, 1009, 1792, 309, 886, 13], "temperature": 0.0, "avg_logprob": -0.27981793880462646, "compression_ratio": 1.2595419847328244, "no_speech_prob": 2.5465124053880572e-05}, {"id": 713, "seek": 386312, "start": 3878.12, "end": 3881.12, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.27981793880462646, "compression_ratio": 1.2595419847328244, "no_speech_prob": 2.5465124053880572e-05}, {"id": 714, "seek": 386312, "start": 3881.12, "end": 3884.12, "text": " Yeah, so I see this slide.", "tokens": [865, 11, 370, 286, 536, 341, 4137, 13], "temperature": 0.0, "avg_logprob": -0.27981793880462646, "compression_ratio": 1.2595419847328244, "no_speech_prob": 2.5465124053880572e-05}, {"id": 715, "seek": 386312, "start": 3884.12, "end": 3886.12, "text": " Let me back up.", "tokens": [961, 385, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.27981793880462646, "compression_ratio": 1.2595419847328244, "no_speech_prob": 2.5465124053880572e-05}, {"id": 716, "seek": 386312, "start": 3886.12, "end": 3892.12, "text": " We actually get two different mp.linionlog.norm.", "tokens": [492, 767, 483, 732, 819, 275, 79, 13, 5045, 313, 4987, 13, 13403, 13], "temperature": 0.0, "avg_logprob": -0.27981793880462646, "compression_ratio": 1.2595419847328244, "no_speech_prob": 2.5465124053880572e-05}, {"id": 717, "seek": 389212, "start": 3892.12, "end": 3894.12, "text": " We have different value here.", "tokens": [492, 362, 819, 2158, 510, 13], "temperature": 0.0, "avg_logprob": -0.12403284072875977, "compression_ratio": 1.5424528301886793, "no_speech_prob": 5.143973612575792e-05}, {"id": 718, "seek": 389212, "start": 3894.12, "end": 3896.12, "text": " One is 1.13.", "tokens": [1485, 307, 502, 13, 7668, 13], "temperature": 0.0, "avg_logprob": -0.12403284072875977, "compression_ratio": 1.5424528301886793, "no_speech_prob": 5.143973612575792e-05}, {"id": 719, "seek": 389212, "start": 3896.12, "end": 3899.12, "text": " One is 6.86.", "tokens": [1485, 307, 1386, 13, 22193, 13], "temperature": 0.0, "avg_logprob": -0.12403284072875977, "compression_ratio": 1.5424528301886793, "no_speech_prob": 5.143973612575792e-05}, {"id": 720, "seek": 389212, "start": 3899.12, "end": 3904.12, "text": " So what has changed in these results?", "tokens": [407, 437, 575, 3105, 294, 613, 3542, 30], "temperature": 0.0, "avg_logprob": -0.12403284072875977, "compression_ratio": 1.5424528301886793, "no_speech_prob": 5.143973612575792e-05}, {"id": 721, "seek": 389212, "start": 3904.12, "end": 3912.12, "text": " So these are so close to zero that it doesn't...", "tokens": [407, 613, 366, 370, 1998, 281, 4018, 300, 309, 1177, 380, 485], "temperature": 0.0, "avg_logprob": -0.12403284072875977, "compression_ratio": 1.5424528301886793, "no_speech_prob": 5.143973612575792e-05}, {"id": 722, "seek": 389212, "start": 3912.12, "end": 3915.12, "text": " Like here I'm mostly trying to confirm that this is a number close to zero, and I'm interested", "tokens": [1743, 510, 286, 478, 5240, 1382, 281, 9064, 300, 341, 307, 257, 1230, 1998, 281, 4018, 11, 293, 286, 478, 3102], "temperature": 0.0, "avg_logprob": -0.12403284072875977, "compression_ratio": 1.5424528301886793, "no_speech_prob": 5.143973612575792e-05}, {"id": 723, "seek": 389212, "start": 3915.12, "end": 3920.12, "text": " in the exponent and wanting it to be, you know, 10 to the negative 14th is close to", "tokens": [294, 264, 37871, 293, 7935, 309, 281, 312, 11, 291, 458, 11, 1266, 281, 264, 3671, 3499, 392, 307, 1998, 281], "temperature": 0.0, "avg_logprob": -0.12403284072875977, "compression_ratio": 1.5424528301886793, "no_speech_prob": 5.143973612575792e-05}, {"id": 724, "seek": 389212, "start": 3920.12, "end": 3921.12, "text": " zero.", "tokens": [4018, 13], "temperature": 0.0, "avg_logprob": -0.12403284072875977, "compression_ratio": 1.5424528301886793, "no_speech_prob": 5.143973612575792e-05}, {"id": 725, "seek": 392112, "start": 3921.12, "end": 3922.12, "text": " Although I guess that is a question.", "tokens": [5780, 286, 2041, 300, 307, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.16379755596781886, "compression_ratio": 1.519047619047619, "no_speech_prob": 2.3550528567284346e-05}, {"id": 726, "seek": 392112, "start": 3922.12, "end": 3929.12, "text": " Why aren't we getting exactly zero if this is a way to decompose and get the answer?", "tokens": [1545, 3212, 380, 321, 1242, 2293, 4018, 498, 341, 307, 257, 636, 281, 22867, 541, 293, 483, 264, 1867, 30], "temperature": 0.0, "avg_logprob": -0.16379755596781886, "compression_ratio": 1.519047619047619, "no_speech_prob": 2.3550528567284346e-05}, {"id": 727, "seek": 392112, "start": 3929.12, "end": 3932.12, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.16379755596781886, "compression_ratio": 1.519047619047619, "no_speech_prob": 2.3550528567284346e-05}, {"id": 728, "seek": 392112, "start": 3932.12, "end": 3942.12, "text": " I mean, does anyone have an answer to this on why we're not getting it?", "tokens": [286, 914, 11, 775, 2878, 362, 364, 1867, 281, 341, 322, 983, 321, 434, 406, 1242, 309, 30], "temperature": 0.0, "avg_logprob": -0.16379755596781886, "compression_ratio": 1.519047619047619, "no_speech_prob": 2.3550528567284346e-05}, {"id": 729, "seek": 392112, "start": 3942.12, "end": 3944.12, "text": " Yeah, because there's...", "tokens": [865, 11, 570, 456, 311, 485], "temperature": 0.0, "avg_logprob": -0.16379755596781886, "compression_ratio": 1.519047619047619, "no_speech_prob": 2.3550528567284346e-05}, {"id": 730, "seek": 392112, "start": 3944.12, "end": 3949.12, "text": " So machine epsilon, because we get rounding errors and with floating point numbers, we're", "tokens": [407, 3479, 17889, 11, 570, 321, 483, 48237, 13603, 293, 365, 12607, 935, 3547, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.16379755596781886, "compression_ratio": 1.519047619047619, "no_speech_prob": 2.3550528567284346e-05}, {"id": 731, "seek": 394912, "start": 3949.12, "end": 3954.12, "text": " trying to represent these continuous numbers with discrete values.", "tokens": [1382, 281, 2906, 613, 10957, 3547, 365, 27706, 4190, 13], "temperature": 0.0, "avg_logprob": -0.15655423214561062, "compression_ratio": 1.6942148760330578, "no_speech_prob": 2.8408212529029697e-05}, {"id": 732, "seek": 394912, "start": 3954.12, "end": 3959.12, "text": " Another question is, suppose A is actually invertible.", "tokens": [3996, 1168, 307, 11, 7297, 316, 307, 767, 33966, 964, 13], "temperature": 0.0, "avg_logprob": -0.15655423214561062, "compression_ratio": 1.6942148760330578, "no_speech_prob": 2.8408212529029697e-05}, {"id": 733, "seek": 394912, "start": 3959.12, "end": 3964.12, "text": " So we normally find the best coefficient by, for example, gradient descent, right?", "tokens": [407, 321, 5646, 915, 264, 1151, 17619, 538, 11, 337, 1365, 11, 16235, 23475, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15655423214561062, "compression_ratio": 1.6942148760330578, "no_speech_prob": 2.8408212529029697e-05}, {"id": 734, "seek": 394912, "start": 3964.12, "end": 3968.12, "text": " But here we are actually trying to solve a linear algebra solution.", "tokens": [583, 510, 321, 366, 767, 1382, 281, 5039, 257, 8213, 21989, 3827, 13], "temperature": 0.0, "avg_logprob": -0.15655423214561062, "compression_ratio": 1.6942148760330578, "no_speech_prob": 2.8408212529029697e-05}, {"id": 735, "seek": 394912, "start": 3968.12, "end": 3971.12, "text": " So which one is usually faster?", "tokens": [407, 597, 472, 307, 2673, 4663, 30], "temperature": 0.0, "avg_logprob": -0.15655423214561062, "compression_ratio": 1.6942148760330578, "no_speech_prob": 2.8408212529029697e-05}, {"id": 736, "seek": 394912, "start": 3971.12, "end": 3975.12, "text": " So when you bring up gradient descent, are you talking about like a stochastic gradient", "tokens": [407, 562, 291, 1565, 493, 16235, 23475, 11, 366, 291, 1417, 466, 411, 257, 342, 8997, 2750, 16235], "temperature": 0.0, "avg_logprob": -0.15655423214561062, "compression_ratio": 1.6942148760330578, "no_speech_prob": 2.8408212529029697e-05}, {"id": 737, "seek": 394912, "start": 3975.12, "end": 3976.12, "text": " descent approach?", "tokens": [23475, 3109, 30], "temperature": 0.0, "avg_logprob": -0.15655423214561062, "compression_ratio": 1.6942148760330578, "no_speech_prob": 2.8408212529029697e-05}, {"id": 738, "seek": 397612, "start": 3976.12, "end": 3982.12, "text": " So stochastic, I actually originally have had stochastic gradient descent in this lesson", "tokens": [407, 342, 8997, 2750, 11, 286, 767, 7993, 362, 632, 342, 8997, 2750, 16235, 23475, 294, 341, 6898], "temperature": 0.0, "avg_logprob": -0.10562305639285853, "compression_ratio": 1.5573122529644268, "no_speech_prob": 5.7718539210327435e-06}, {"id": 739, "seek": 397612, "start": 3982.12, "end": 3987.12, "text": " with PyTorch, and it was actually much slower than any of these linear algebra approaches,", "tokens": [365, 9953, 51, 284, 339, 11, 293, 309, 390, 767, 709, 14009, 813, 604, 295, 613, 8213, 21989, 11587, 11], "temperature": 0.0, "avg_logprob": -0.10562305639285853, "compression_ratio": 1.5573122529644268, "no_speech_prob": 5.7718539210327435e-06}, {"id": 740, "seek": 397612, "start": 3987.12, "end": 3993.12, "text": " and so I took it out because it just didn't look reasonable compared to them.", "tokens": [293, 370, 286, 1890, 309, 484, 570, 309, 445, 994, 380, 574, 10585, 5347, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.10562305639285853, "compression_ratio": 1.5573122529644268, "no_speech_prob": 5.7718539210327435e-06}, {"id": 741, "seek": 397612, "start": 3993.12, "end": 4001.12, "text": " A benefit, so yeah, SGD will often be slower than if you have, you know, kind of like a", "tokens": [316, 5121, 11, 370, 1338, 11, 34520, 35, 486, 2049, 312, 14009, 813, 498, 291, 362, 11, 291, 458, 11, 733, 295, 411, 257], "temperature": 0.0, "avg_logprob": -0.10562305639285853, "compression_ratio": 1.5573122529644268, "no_speech_prob": 5.7718539210327435e-06}, {"id": 742, "seek": 397612, "start": 4001.12, "end": 4003.12, "text": " well-studied method for your particular problem.", "tokens": [731, 12, 28349, 1091, 3170, 337, 428, 1729, 1154, 13], "temperature": 0.0, "avg_logprob": -0.10562305639285853, "compression_ratio": 1.5573122529644268, "no_speech_prob": 5.7718539210327435e-06}, {"id": 743, "seek": 400312, "start": 4003.12, "end": 4009.12, "text": " But a benefit of SGD is that it's so general, and so if you have a problem where you don't", "tokens": [583, 257, 5121, 295, 34520, 35, 307, 300, 309, 311, 370, 2674, 11, 293, 370, 498, 291, 362, 257, 1154, 689, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.08787357807159424, "compression_ratio": 1.7901785714285714, "no_speech_prob": 6.747934548911871e-06}, {"id": 744, "seek": 400312, "start": 4009.12, "end": 4012.12, "text": " have a closed form solution or, you know, people haven't studied it to come up with", "tokens": [362, 257, 5395, 1254, 3827, 420, 11, 291, 458, 11, 561, 2378, 380, 9454, 309, 281, 808, 493, 365], "temperature": 0.0, "avg_logprob": -0.08787357807159424, "compression_ratio": 1.7901785714285714, "no_speech_prob": 6.747934548911871e-06}, {"id": 745, "seek": 400312, "start": 4012.12, "end": 4016.12, "text": " this optimization method, you can use SGD there.", "tokens": [341, 19618, 3170, 11, 291, 393, 764, 34520, 35, 456, 13], "temperature": 0.0, "avg_logprob": -0.08787357807159424, "compression_ratio": 1.7901785714285714, "no_speech_prob": 6.747934548911871e-06}, {"id": 746, "seek": 400312, "start": 4016.12, "end": 4022.12, "text": " So kind of the benefit of SGD is, yeah, that it's so general and that it'll work on problems", "tokens": [407, 733, 295, 264, 5121, 295, 34520, 35, 307, 11, 1338, 11, 300, 309, 311, 370, 2674, 293, 300, 309, 603, 589, 322, 2740], "temperature": 0.0, "avg_logprob": -0.08787357807159424, "compression_ratio": 1.7901785714285714, "no_speech_prob": 6.747934548911871e-06}, {"id": 747, "seek": 400312, "start": 4022.12, "end": 4026.12, "text": " where we don't have closed form solutions.", "tokens": [689, 321, 500, 380, 362, 5395, 1254, 6547, 13], "temperature": 0.0, "avg_logprob": -0.08787357807159424, "compression_ratio": 1.7901785714285714, "no_speech_prob": 6.747934548911871e-06}, {"id": 748, "seek": 400312, "start": 4026.12, "end": 4028.12, "text": " Connor?", "tokens": [33133, 30], "temperature": 0.0, "avg_logprob": -0.08787357807159424, "compression_ratio": 1.7901785714285714, "no_speech_prob": 6.747934548911871e-06}, {"id": 749, "seek": 400312, "start": 4028.12, "end": 4030.12, "text": " And can you throw the microphone?", "tokens": [400, 393, 291, 3507, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.08787357807159424, "compression_ratio": 1.7901785714285714, "no_speech_prob": 6.747934548911871e-06}, {"id": 750, "seek": 403012, "start": 4030.12, "end": 4034.12, "text": " Or pass the microphone?", "tokens": [1610, 1320, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.219924198107773, "compression_ratio": 1.490566037735849, "no_speech_prob": 4.6837099944241345e-05}, {"id": 751, "seek": 403012, "start": 4034.12, "end": 4041.12, "text": " But I thought that if we have, say, like, say our dimensionality is really, really,", "tokens": [583, 286, 1194, 300, 498, 321, 362, 11, 584, 11, 411, 11, 584, 527, 10139, 1860, 307, 534, 11, 534, 11], "temperature": 0.0, "avg_logprob": -0.219924198107773, "compression_ratio": 1.490566037735849, "no_speech_prob": 4.6837099944241345e-05}, {"id": 752, "seek": 403012, "start": 4041.12, "end": 4047.12, "text": " really large, then the closed form solution takes a while in comparison.", "tokens": [534, 2416, 11, 550, 264, 5395, 1254, 3827, 2516, 257, 1339, 294, 9660, 13], "temperature": 0.0, "avg_logprob": -0.219924198107773, "compression_ratio": 1.490566037735849, "no_speech_prob": 4.6837099944241345e-05}, {"id": 753, "seek": 403012, "start": 4047.12, "end": 4049.12, "text": " I don't know.", "tokens": [286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.219924198107773, "compression_ratio": 1.490566037735849, "no_speech_prob": 4.6837099944241345e-05}, {"id": 754, "seek": 403012, "start": 4049.12, "end": 4051.12, "text": " I was just reading somewhere.", "tokens": [286, 390, 445, 3760, 4079, 13], "temperature": 0.0, "avg_logprob": -0.219924198107773, "compression_ratio": 1.490566037735849, "no_speech_prob": 4.6837099944241345e-05}, {"id": 755, "seek": 403012, "start": 4051.12, "end": 4056.12, "text": " Yeah, no, and that's quite possible too, yeah, that at a certain size it might be worth it.", "tokens": [865, 11, 572, 11, 293, 300, 311, 1596, 1944, 886, 11, 1338, 11, 300, 412, 257, 1629, 2744, 309, 1062, 312, 3163, 309, 13], "temperature": 0.0, "avg_logprob": -0.219924198107773, "compression_ratio": 1.490566037735849, "no_speech_prob": 4.6837099944241345e-05}, {"id": 756, "seek": 405612, "start": 4056.12, "end": 4063.12, "text": " Yeah, and that's an important note, I think, with all of these that the answer of what", "tokens": [865, 11, 293, 300, 311, 364, 1021, 3637, 11, 286, 519, 11, 365, 439, 295, 613, 300, 264, 1867, 295, 437], "temperature": 0.0, "avg_logprob": -0.08994455898509306, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.0129745533049572e-05}, {"id": 757, "seek": 405612, "start": 4063.12, "end": 4066.12, "text": " algorithm is best can really depend on your problem.", "tokens": [9284, 307, 1151, 393, 534, 5672, 322, 428, 1154, 13], "temperature": 0.0, "avg_logprob": -0.08994455898509306, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.0129745533049572e-05}, {"id": 758, "seek": 405612, "start": 4066.12, "end": 4068.12, "text": " And we'll see that later.", "tokens": [400, 321, 603, 536, 300, 1780, 13], "temperature": 0.0, "avg_logprob": -0.08994455898509306, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.0129745533049572e-05}, {"id": 759, "seek": 405612, "start": 4068.12, "end": 4071.12, "text": " And so different things like both the size of your problem, kind of properties around", "tokens": [400, 370, 819, 721, 411, 1293, 264, 2744, 295, 428, 1154, 11, 733, 295, 7221, 926], "temperature": 0.0, "avg_logprob": -0.08994455898509306, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.0129745533049572e-05}, {"id": 760, "seek": 405612, "start": 4071.12, "end": 4074.12, "text": " the stability of your problem.", "tokens": [264, 11826, 295, 428, 1154, 13], "temperature": 0.0, "avg_logprob": -0.08994455898509306, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.0129745533049572e-05}, {"id": 761, "seek": 405612, "start": 4074.12, "end": 4080.12, "text": " Yeah, we'll see later two methods that kind of, or two particular matrices that don't work", "tokens": [865, 11, 321, 603, 536, 1780, 732, 7150, 300, 733, 295, 11, 420, 732, 1729, 32284, 300, 500, 380, 589], "temperature": 0.0, "avg_logprob": -0.08994455898509306, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.0129745533049572e-05}, {"id": 762, "seek": 405612, "start": 4080.12, "end": 4083.12, "text": " well for a particular method.", "tokens": [731, 337, 257, 1729, 3170, 13], "temperature": 0.0, "avg_logprob": -0.08994455898509306, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.0129745533049572e-05}, {"id": 763, "seek": 408312, "start": 4083.12, "end": 4088.12, "text": " Yeah, so there's a lot of variation with these.", "tokens": [865, 11, 370, 456, 311, 257, 688, 295, 12990, 365, 613, 13], "temperature": 0.0, "avg_logprob": -0.09819626404067218, "compression_ratio": 1.4489795918367347, "no_speech_prob": 4.198241367703304e-05}, {"id": 764, "seek": 408312, "start": 4088.12, "end": 4090.12, "text": " Yeah, these are good questions.", "tokens": [865, 11, 613, 366, 665, 1651, 13], "temperature": 0.0, "avg_logprob": -0.09819626404067218, "compression_ratio": 1.4489795918367347, "no_speech_prob": 4.198241367703304e-05}, {"id": 765, "seek": 408312, "start": 4090.12, "end": 4102.12, "text": " Any other questions about Cholesky factorization before we go on to our next approach?", "tokens": [2639, 661, 1651, 466, 761, 7456, 4133, 5952, 2144, 949, 321, 352, 322, 281, 527, 958, 3109, 30], "temperature": 0.0, "avg_logprob": -0.09819626404067218, "compression_ratio": 1.4489795918367347, "no_speech_prob": 4.198241367703304e-05}, {"id": 766, "seek": 408312, "start": 4102.12, "end": 4107.12, "text": " All right, so next up is the QR factorization.", "tokens": [1057, 558, 11, 370, 958, 493, 307, 264, 32784, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.09819626404067218, "compression_ratio": 1.4489795918367347, "no_speech_prob": 4.198241367703304e-05}, {"id": 767, "seek": 410712, "start": 4107.12, "end": 4113.12, "text": " And this, again, next week we will actually talk about how to code a QR factorization.", "tokens": [400, 341, 11, 797, 11, 958, 1243, 321, 486, 767, 751, 466, 577, 281, 3089, 257, 32784, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.0778601023615623, "compression_ratio": 1.2464788732394365, "no_speech_prob": 4.1326744394609705e-05}, {"id": 768, "seek": 410712, "start": 4113.12, "end": 4115.12, "text": " We're still just using NumPy's.", "tokens": [492, 434, 920, 445, 1228, 22592, 47, 88, 311, 13], "temperature": 0.0, "avg_logprob": -0.0778601023615623, "compression_ratio": 1.2464788732394365, "no_speech_prob": 4.1326744394609705e-05}, {"id": 769, "seek": 410712, "start": 4115.12, "end": 4135.12, "text": " Can someone remind me what the QR decomposition gives you?", "tokens": [1664, 1580, 4160, 385, 437, 264, 32784, 48356, 2709, 291, 30], "temperature": 0.0, "avg_logprob": -0.0778601023615623, "compression_ratio": 1.2464788732394365, "no_speech_prob": 4.1326744394609705e-05}, {"id": 770, "seek": 413512, "start": 4135.12, "end": 4147.12, "text": " No, although you have the right words in there.", "tokens": [883, 11, 4878, 291, 362, 264, 558, 2283, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.3033631245295207, "compression_ratio": 0.9868421052631579, "no_speech_prob": 3.590554479160346e-05}, {"id": 771, "seek": 413512, "start": 4147.12, "end": 4152.12, "text": " I think Kelsey has a guess.", "tokens": [286, 519, 44714, 575, 257, 2041, 13], "temperature": 0.0, "avg_logprob": -0.3033631245295207, "compression_ratio": 0.9868421052631579, "no_speech_prob": 3.590554479160346e-05}, {"id": 772, "seek": 415212, "start": 4152.12, "end": 4166.12, "text": " I think it's an upper triangular one and then one that's orthogonal column vectors that", "tokens": [286, 519, 309, 311, 364, 6597, 38190, 472, 293, 550, 472, 300, 311, 41488, 7738, 18875, 300], "temperature": 0.0, "avg_logprob": -0.2080250382423401, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1842265848827083e-05}, {"id": 773, "seek": 415212, "start": 4166.12, "end": 4167.12, "text": " are orthogonal to normal as well, or just a box.", "tokens": [366, 41488, 281, 2710, 382, 731, 11, 420, 445, 257, 2424, 13], "temperature": 0.0, "avg_logprob": -0.2080250382423401, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1842265848827083e-05}, {"id": 774, "seek": 415212, "start": 4167.12, "end": 4168.12, "text": " I believe, so if it's square, they would be orthonormal.", "tokens": [286, 1697, 11, 370, 498, 309, 311, 3732, 11, 436, 576, 312, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.2080250382423401, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1842265848827083e-05}, {"id": 775, "seek": 415212, "start": 4168.12, "end": 4170.12, "text": " But yeah, I guess for the non-square case, I would have to check.", "tokens": [583, 1338, 11, 286, 2041, 337, 264, 2107, 12, 33292, 543, 1389, 11, 286, 576, 362, 281, 1520, 13], "temperature": 0.0, "avg_logprob": -0.2080250382423401, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1842265848827083e-05}, {"id": 776, "seek": 415212, "start": 4170.12, "end": 4174.12, "text": " But yeah, so Q is orthonormal, R is upper triangular.", "tokens": [583, 1338, 11, 370, 1249, 307, 420, 11943, 24440, 11, 497, 307, 6597, 38190, 13], "temperature": 0.0, "avg_logprob": -0.2080250382423401, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1842265848827083e-05}, {"id": 777, "seek": 415212, "start": 4174.12, "end": 4176.12, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2080250382423401, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1842265848827083e-05}, {"id": 778, "seek": 417612, "start": 4176.12, "end": 4194.12, "text": " And where did we see the QR factorization before?", "tokens": [400, 689, 630, 321, 536, 264, 32784, 5952, 2144, 949, 30], "temperature": 0.0, "avg_logprob": -0.073569917678833, "compression_ratio": 1.0, "no_speech_prob": 3.269687658757903e-05}, {"id": 779, "seek": 417612, "start": 4194.12, "end": 4203.12, "text": " I'm not sure that we saw it with NMF.", "tokens": [286, 478, 406, 988, 300, 321, 1866, 309, 365, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.073569917678833, "compression_ratio": 1.0, "no_speech_prob": 3.269687658757903e-05}, {"id": 780, "seek": 420312, "start": 4203.12, "end": 4214.12, "text": " Yeah, so inside of randomized SVD, we saw it.", "tokens": [865, 11, 370, 1854, 295, 38513, 31910, 35, 11, 321, 1866, 309, 13], "temperature": 0.0, "avg_logprob": -0.1637913051404451, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1200037483358756e-05}, {"id": 781, "seek": 420312, "start": 4214.12, "end": 4217.12, "text": " Okay, so here, so kind of getting into these equations.", "tokens": [1033, 11, 370, 510, 11, 370, 733, 295, 1242, 666, 613, 11787, 13], "temperature": 0.0, "avg_logprob": -0.1637913051404451, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1200037483358756e-05}, {"id": 782, "seek": 420312, "start": 4217.12, "end": 4223.12, "text": " And we, yeah, next week we'll talk about how to code a QR factorization ourselves.", "tokens": [400, 321, 11, 1338, 11, 958, 1243, 321, 603, 751, 466, 577, 281, 3089, 257, 32784, 5952, 2144, 4175, 13], "temperature": 0.0, "avg_logprob": -0.1637913051404451, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1200037483358756e-05}, {"id": 783, "seek": 420312, "start": 4223.12, "end": 4225.12, "text": " So AX equals B.", "tokens": [407, 316, 55, 6915, 363, 13], "temperature": 0.0, "avg_logprob": -0.1637913051404451, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1200037483358756e-05}, {"id": 784, "seek": 420312, "start": 4225.12, "end": 4230.12, "text": " We're going to get the QR factorization for A.", "tokens": [492, 434, 516, 281, 483, 264, 32784, 5952, 2144, 337, 316, 13], "temperature": 0.0, "avg_logprob": -0.1637913051404451, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1200037483358756e-05}, {"id": 785, "seek": 423012, "start": 4230.12, "end": 4237.12, "text": " So A equals QR, plug that in, and then say RX equals Q transpose B.", "tokens": [407, 316, 6915, 32784, 11, 5452, 300, 294, 11, 293, 550, 584, 46197, 6915, 1249, 25167, 363, 13], "temperature": 0.0, "avg_logprob": -0.09948758381169016, "compression_ratio": 1.5657142857142856, "no_speech_prob": 2.355162905587349e-05}, {"id": 786, "seek": 423012, "start": 4237.12, "end": 4249.12, "text": " And why was I able to go from Q on the left side to a Q transpose on the right side?", "tokens": [400, 983, 390, 286, 1075, 281, 352, 490, 1249, 322, 264, 1411, 1252, 281, 257, 1249, 25167, 322, 264, 558, 1252, 30], "temperature": 0.0, "avg_logprob": -0.09948758381169016, "compression_ratio": 1.5657142857142856, "no_speech_prob": 2.355162905587349e-05}, {"id": 787, "seek": 423012, "start": 4249.12, "end": 4251.12, "text": " Yeah, so a few people shouted this out.", "tokens": [865, 11, 370, 257, 1326, 561, 37310, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.09948758381169016, "compression_ratio": 1.5657142857142856, "no_speech_prob": 2.355162905587349e-05}, {"id": 788, "seek": 423012, "start": 4251.12, "end": 4252.12, "text": " Q is orthonormal.", "tokens": [1249, 307, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.09948758381169016, "compression_ratio": 1.5657142857142856, "no_speech_prob": 2.355162905587349e-05}, {"id": 789, "seek": 423012, "start": 4252.12, "end": 4256.12, "text": " So multiplying by Q transpose, we get the identity on the left.", "tokens": [407, 30955, 538, 1249, 25167, 11, 321, 483, 264, 6575, 322, 264, 1411, 13], "temperature": 0.0, "avg_logprob": -0.09948758381169016, "compression_ratio": 1.5657142857142856, "no_speech_prob": 2.355162905587349e-05}, {"id": 790, "seek": 425612, "start": 4256.12, "end": 4262.12, "text": " And that's also nice because you're not having to calculate an inverse, you're just transposing the columns.", "tokens": [400, 300, 311, 611, 1481, 570, 291, 434, 406, 1419, 281, 8873, 364, 17340, 11, 291, 434, 445, 7132, 6110, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.11629872228585038, "compression_ratio": 1.5743801652892562, "no_speech_prob": 7.183182788139675e-06}, {"id": 791, "seek": 425612, "start": 4262.12, "end": 4269.12, "text": " And then why is it nice to have RX equals Q transpose B?", "tokens": [400, 550, 983, 307, 309, 1481, 281, 362, 46197, 6915, 1249, 25167, 363, 30], "temperature": 0.0, "avg_logprob": -0.11629872228585038, "compression_ratio": 1.5743801652892562, "no_speech_prob": 7.183182788139675e-06}, {"id": 792, "seek": 425612, "start": 4269.12, "end": 4273.12, "text": " Oh, can you grab the microphone?", "tokens": [876, 11, 393, 291, 4444, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.11629872228585038, "compression_ratio": 1.5743801652892562, "no_speech_prob": 7.183182788139675e-06}, {"id": 793, "seek": 425612, "start": 4273.12, "end": 4276.12, "text": " You want to solve for X, R is upper triangular, so it's easier to solve.", "tokens": [509, 528, 281, 5039, 337, 1783, 11, 497, 307, 6597, 38190, 11, 370, 309, 311, 3571, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.11629872228585038, "compression_ratio": 1.5743801652892562, "no_speech_prob": 7.183182788139675e-06}, {"id": 794, "seek": 425612, "start": 4276.12, "end": 4277.12, "text": " Exactly, yeah.", "tokens": [7587, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.11629872228585038, "compression_ratio": 1.5743801652892562, "no_speech_prob": 7.183182788139675e-06}, {"id": 795, "seek": 425612, "start": 4277.12, "end": 4284.12, "text": " So now we have a linear system of equations with a triangular matrix that'll be fast to solve.", "tokens": [407, 586, 321, 362, 257, 8213, 1185, 295, 11787, 365, 257, 38190, 8141, 300, 603, 312, 2370, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.11629872228585038, "compression_ratio": 1.5743801652892562, "no_speech_prob": 7.183182788139675e-06}, {"id": 796, "seek": 428412, "start": 4284.12, "end": 4295.12, "text": " So here we do QR equals, using SciPy's QR factorization, and then we get a triangular system to solve.", "tokens": [407, 510, 321, 360, 32784, 6915, 11, 1228, 16942, 47, 88, 311, 32784, 5952, 2144, 11, 293, 550, 321, 483, 257, 38190, 1185, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.10899008956609987, "compression_ratio": 1.3656716417910448, "no_speech_prob": 1.300684925809037e-05}, {"id": 797, "seek": 428412, "start": 4295.12, "end": 4308.12, "text": " We solve this, and we're getting the same error kind of that we've been getting.", "tokens": [492, 5039, 341, 11, 293, 321, 434, 1242, 264, 912, 6713, 733, 295, 300, 321, 600, 668, 1242, 13], "temperature": 0.0, "avg_logprob": -0.10899008956609987, "compression_ratio": 1.3656716417910448, "no_speech_prob": 1.300684925809037e-05}, {"id": 798, "seek": 430812, "start": 4308.12, "end": 4328.12, "text": " Questions about this approach with the QR factorization?", "tokens": [27738, 466, 341, 3109, 365, 264, 32784, 5952, 2144, 30], "temperature": 0.0, "avg_logprob": -0.12243455189924973, "compression_ratio": 1.1149425287356323, "no_speech_prob": 3.7634996260749176e-05}, {"id": 799, "seek": 430812, "start": 4328.12, "end": 4330.12, "text": " The question is, why is this one slower?", "tokens": [440, 1168, 307, 11, 983, 307, 341, 472, 14009, 30], "temperature": 0.0, "avg_logprob": -0.12243455189924973, "compression_ratio": 1.1149425287356323, "no_speech_prob": 3.7634996260749176e-05}, {"id": 800, "seek": 433012, "start": 4330.12, "end": 4339.12, "text": " Let's actually save the timing comparison ones for a moment, but yeah, we'll come back to that.", "tokens": [961, 311, 767, 3155, 264, 10822, 9660, 2306, 337, 257, 1623, 11, 457, 1338, 11, 321, 603, 808, 646, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.08717634280522664, "compression_ratio": 1.3214285714285714, "no_speech_prob": 9.81788252829574e-06}, {"id": 801, "seek": 433012, "start": 4339.12, "end": 4354.12, "text": " Any other questions before we start our next approach?", "tokens": [2639, 661, 1651, 949, 321, 722, 527, 958, 3109, 30], "temperature": 0.0, "avg_logprob": -0.08717634280522664, "compression_ratio": 1.3214285714285714, "no_speech_prob": 9.81788252829574e-06}, {"id": 802, "seek": 433012, "start": 4354.12, "end": 4358.12, "text": " Okay, so now yet another approach.", "tokens": [1033, 11, 370, 586, 1939, 1071, 3109, 13], "temperature": 0.0, "avg_logprob": -0.08717634280522664, "compression_ratio": 1.3214285714285714, "no_speech_prob": 9.81788252829574e-06}, {"id": 803, "seek": 435812, "start": 4358.12, "end": 4360.12, "text": " We're going to look at SBD.", "tokens": [492, 434, 516, 281, 574, 412, 26944, 35, 13], "temperature": 0.0, "avg_logprob": -0.07529911096545233, "compression_ratio": 1.5582822085889572, "no_speech_prob": 8.34885286167264e-05}, {"id": 804, "seek": 435812, "start": 4360.12, "end": 4366.12, "text": " And so again, we're still trying to solve the same problem of AX equals B and finding the optimal X.", "tokens": [400, 370, 797, 11, 321, 434, 920, 1382, 281, 5039, 264, 912, 1154, 295, 316, 55, 6915, 363, 293, 5006, 264, 16252, 1783, 13], "temperature": 0.0, "avg_logprob": -0.07529911096545233, "compression_ratio": 1.5582822085889572, "no_speech_prob": 8.34885286167264e-05}, {"id": 805, "seek": 435812, "start": 4366.12, "end": 4370.12, "text": " Now we're going to use the singular value decomposition.", "tokens": [823, 321, 434, 516, 281, 764, 264, 20010, 2158, 48356, 13], "temperature": 0.0, "avg_logprob": -0.07529911096545233, "compression_ratio": 1.5582822085889572, "no_speech_prob": 8.34885286167264e-05}, {"id": 806, "seek": 435812, "start": 4370.12, "end": 4382.12, "text": " Can someone remind us what the singular value decomposition is?", "tokens": [1664, 1580, 4160, 505, 437, 264, 20010, 2158, 48356, 307, 30], "temperature": 0.0, "avg_logprob": -0.07529911096545233, "compression_ratio": 1.5582822085889572, "no_speech_prob": 8.34885286167264e-05}, {"id": 807, "seek": 435812, "start": 4382.12, "end": 4385.12, "text": " Tim?", "tokens": [7172, 30], "temperature": 0.0, "avg_logprob": -0.07529911096545233, "compression_ratio": 1.5582822085889572, "no_speech_prob": 8.34885286167264e-05}, {"id": 808, "seek": 438512, "start": 4385.12, "end": 4391.12, "text": " So the full SVDs, you decompose the matrix into a product of U, sigma, V transpose, I think?", "tokens": [407, 264, 1577, 31910, 35, 82, 11, 291, 22867, 541, 264, 8141, 666, 257, 1674, 295, 624, 11, 12771, 11, 691, 25167, 11, 286, 519, 30], "temperature": 0.0, "avg_logprob": -0.22686985560825892, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.00031994329765439034}, {"id": 809, "seek": 438512, "start": 4391.12, "end": 4393.12, "text": " Yes, yeah.", "tokens": [1079, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.22686985560825892, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.00031994329765439034}, {"id": 810, "seek": 438512, "start": 4393.12, "end": 4404.12, "text": " So U and V are orthogonal matrices, and sigma contains a block of a square matrix where the diagonals are your singular values.", "tokens": [407, 624, 293, 691, 366, 41488, 32284, 11, 293, 12771, 8306, 257, 3461, 295, 257, 3732, 8141, 689, 264, 17405, 1124, 366, 428, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.22686985560825892, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.00031994329765439034}, {"id": 811, "seek": 438512, "start": 4404.12, "end": 4405.12, "text": " Exactly, yeah.", "tokens": [7587, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.22686985560825892, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.00031994329765439034}, {"id": 812, "seek": 438512, "start": 4405.12, "end": 4408.12, "text": " So sigma is a diagonal matrix.", "tokens": [407, 12771, 307, 257, 21539, 8141, 13], "temperature": 0.0, "avg_logprob": -0.22686985560825892, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.00031994329765439034}, {"id": 813, "seek": 438512, "start": 4408.12, "end": 4414.12, "text": " Everything's zero except the diagonals, and those are the singular values, and U and V are orthonormal.", "tokens": [5471, 311, 4018, 3993, 264, 17405, 1124, 11, 293, 729, 366, 264, 20010, 4190, 11, 293, 624, 293, 691, 366, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.22686985560825892, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.00031994329765439034}, {"id": 814, "seek": 441412, "start": 4414.12, "end": 4416.12, "text": " Sigma doesn't have to be square though, right?", "tokens": [36595, 1177, 380, 362, 281, 312, 3732, 1673, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11429451836480034, "compression_ratio": 1.5148514851485149, "no_speech_prob": 9.818198122957256e-06}, {"id": 815, "seek": 441412, "start": 4416.12, "end": 4420.12, "text": " In the full SVD?", "tokens": [682, 264, 1577, 31910, 35, 30], "temperature": 0.0, "avg_logprob": -0.11429451836480034, "compression_ratio": 1.5148514851485149, "no_speech_prob": 9.818198122957256e-06}, {"id": 816, "seek": 441412, "start": 4420.12, "end": 4423.12, "text": " So they talk about \u2013 I didn't cover this in class.", "tokens": [407, 436, 751, 466, 1662, 286, 994, 380, 2060, 341, 294, 1508, 13], "temperature": 0.0, "avg_logprob": -0.11429451836480034, "compression_ratio": 1.5148514851485149, "no_speech_prob": 9.818198122957256e-06}, {"id": 817, "seek": 441412, "start": 4423.12, "end": 4433.12, "text": " They talk about like a full SVD and also a reduced SVD, and one of them \u2013 yeah, and one of them it's not square,", "tokens": [814, 751, 466, 411, 257, 1577, 31910, 35, 293, 611, 257, 9212, 31910, 35, 11, 293, 472, 295, 552, 1662, 1338, 11, 293, 472, 295, 552, 309, 311, 406, 3732, 11], "temperature": 0.0, "avg_logprob": -0.11429451836480034, "compression_ratio": 1.5148514851485149, "no_speech_prob": 9.818198122957256e-06}, {"id": 818, "seek": 441412, "start": 4433.12, "end": 4439.12, "text": " but you're just adding a bunch of zeros to get the dimensions to work out.", "tokens": [457, 291, 434, 445, 5127, 257, 3840, 295, 35193, 281, 483, 264, 12819, 281, 589, 484, 13], "temperature": 0.0, "avg_logprob": -0.11429451836480034, "compression_ratio": 1.5148514851485149, "no_speech_prob": 9.818198122957256e-06}, {"id": 819, "seek": 443912, "start": 4439.12, "end": 4453.12, "text": " Yeah, and actually I guess \u2013 I think for the reduced one, really it's just that U has orthonormal rows and V has orthonormal columns,", "tokens": [865, 11, 293, 767, 286, 2041, 1662, 286, 519, 337, 264, 9212, 472, 11, 534, 309, 311, 445, 300, 624, 575, 420, 11943, 24440, 13241, 293, 691, 575, 420, 11943, 24440, 13766, 11], "temperature": 0.0, "avg_logprob": -0.12876029014587403, "compression_ratio": 1.5396825396825398, "no_speech_prob": 7.527862180722877e-06}, {"id": 820, "seek": 443912, "start": 4453.12, "end": 4460.12, "text": " and then for the full version you're like making those \u2013 expanding them so that they're full bases.", "tokens": [293, 550, 337, 264, 1577, 3037, 291, 434, 411, 1455, 729, 1662, 14702, 552, 370, 300, 436, 434, 1577, 17949, 13], "temperature": 0.0, "avg_logprob": -0.12876029014587403, "compression_ratio": 1.5396825396825398, "no_speech_prob": 7.527862180722877e-06}, {"id": 821, "seek": 443912, "start": 4460.12, "end": 4464.12, "text": " So I guess in the full U and V are square?", "tokens": [407, 286, 2041, 294, 264, 1577, 624, 293, 691, 366, 3732, 30], "temperature": 0.0, "avg_logprob": -0.12876029014587403, "compression_ratio": 1.5396825396825398, "no_speech_prob": 7.527862180722877e-06}, {"id": 822, "seek": 443912, "start": 4464.12, "end": 4465.12, "text": " Yes, yeah.", "tokens": [1079, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.12876029014587403, "compression_ratio": 1.5396825396825398, "no_speech_prob": 7.527862180722877e-06}, {"id": 823, "seek": 446512, "start": 4465.12, "end": 4471.12, "text": " In the full SVD, the U and the V are square matrices.", "tokens": [682, 264, 1577, 31910, 35, 11, 264, 624, 293, 264, 691, 366, 3732, 32284, 13], "temperature": 0.0, "avg_logprob": -0.14288501416222524, "compression_ratio": 1.22, "no_speech_prob": 1.4970489246479701e-05}, {"id": 824, "seek": 446512, "start": 4471.12, "end": 4477.12, "text": " And I'll bring \u2013 Trevathan has a nice picture of this.", "tokens": [400, 286, 603, 1565, 1662, 8648, 85, 9390, 575, 257, 1481, 3036, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.14288501416222524, "compression_ratio": 1.22, "no_speech_prob": 1.4970489246479701e-05}, {"id": 825, "seek": 446512, "start": 4477.12, "end": 4482.12, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.14288501416222524, "compression_ratio": 1.22, "no_speech_prob": 1.4970489246479701e-05}, {"id": 826, "seek": 446512, "start": 4482.12, "end": 4494.12, "text": " Okay, I'll talk about that next time, full versus reduced for SVD.", "tokens": [1033, 11, 286, 603, 751, 466, 300, 958, 565, 11, 1577, 5717, 9212, 337, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.14288501416222524, "compression_ratio": 1.22, "no_speech_prob": 1.4970489246479701e-05}, {"id": 827, "seek": 449412, "start": 4494.12, "end": 4500.12, "text": " Okay, yeah, so that's the SVD, which we've seen a few times before.", "tokens": [1033, 11, 1338, 11, 370, 300, 311, 264, 31910, 35, 11, 597, 321, 600, 1612, 257, 1326, 1413, 949, 13], "temperature": 0.0, "avg_logprob": -0.08827508335382166, "compression_ratio": 1.452513966480447, "no_speech_prob": 1.8631277271197177e-05}, {"id": 828, "seek": 449412, "start": 4500.12, "end": 4509.12, "text": " Here we plug that in for A, and then actually similar to what we did before, we can multiply each side by U transpose,", "tokens": [1692, 321, 5452, 300, 294, 337, 316, 11, 293, 550, 767, 2531, 281, 437, 321, 630, 949, 11, 321, 393, 12972, 1184, 1252, 538, 624, 25167, 11], "temperature": 0.0, "avg_logprob": -0.08827508335382166, "compression_ratio": 1.452513966480447, "no_speech_prob": 1.8631277271197177e-05}, {"id": 829, "seek": 449412, "start": 4509.12, "end": 4516.12, "text": " since U transpose times U is the identity because U has orthonormal rows.", "tokens": [1670, 624, 25167, 1413, 624, 307, 264, 6575, 570, 624, 575, 420, 11943, 24440, 13241, 13], "temperature": 0.0, "avg_logprob": -0.08827508335382166, "compression_ratio": 1.452513966480447, "no_speech_prob": 1.8631277271197177e-05}, {"id": 830, "seek": 451612, "start": 4516.12, "end": 4535.12, "text": " We get sigma Vx equals U transpose b, then we have sigma W equals U transpose b, x equals V transpose w.", "tokens": [492, 483, 12771, 691, 87, 6915, 624, 25167, 272, 11, 550, 321, 362, 12771, 343, 6915, 624, 25167, 272, 11, 2031, 6915, 691, 25167, 261, 13], "temperature": 0.0, "avg_logprob": -0.08559019477279098, "compression_ratio": 1.4645161290322581, "no_speech_prob": 2.6015875391749432e-06}, {"id": 831, "seek": 451612, "start": 4535.12, "end": 4542.12, "text": " And so this \u2013 I actually think you should kind of be seeing some common themes between these three different approaches.", "tokens": [400, 370, 341, 1662, 286, 767, 519, 291, 820, 733, 295, 312, 2577, 512, 2689, 13544, 1296, 613, 1045, 819, 11587, 13], "temperature": 0.0, "avg_logprob": -0.08559019477279098, "compression_ratio": 1.4645161290322581, "no_speech_prob": 2.6015875391749432e-06}, {"id": 832, "seek": 454212, "start": 4542.12, "end": 4555.12, "text": " Well, actually, I guess first I should say what's nice about solving sigma W equals U transpose b?", "tokens": [1042, 11, 767, 11, 286, 2041, 700, 286, 820, 584, 437, 311, 1481, 466, 12606, 12771, 343, 6915, 624, 25167, 272, 30], "temperature": 0.0, "avg_logprob": -0.07594517299107142, "compression_ratio": 1.1696428571428572, "no_speech_prob": 1.1842795174743515e-05}, {"id": 833, "seek": 454212, "start": 4555.12, "end": 4562.12, "text": " Does anyone else want to answer?", "tokens": [4402, 2878, 1646, 528, 281, 1867, 30], "temperature": 0.0, "avg_logprob": -0.07594517299107142, "compression_ratio": 1.1696428571428572, "no_speech_prob": 1.1842795174743515e-05}, {"id": 834, "seek": 456212, "start": 4562.12, "end": 4575.12, "text": " And so here we've just said Vx equals w, and now we're going to solve sigma W equals U transpose b.", "tokens": [400, 370, 510, 321, 600, 445, 848, 691, 87, 6915, 261, 11, 293, 586, 321, 434, 516, 281, 5039, 12771, 343, 6915, 624, 25167, 272, 13], "temperature": 0.0, "avg_logprob": -0.12976662814617157, "compression_ratio": 1.356687898089172, "no_speech_prob": 2.4299335564137436e-05}, {"id": 835, "seek": 456212, "start": 4575.12, "end": 4581.12, "text": " Roger? Tim, can you pass the microphone? Thanks.", "tokens": [17666, 30, 7172, 11, 393, 291, 1320, 264, 10952, 30, 2561, 13], "temperature": 0.0, "avg_logprob": -0.12976662814617157, "compression_ratio": 1.356687898089172, "no_speech_prob": 2.4299335564137436e-05}, {"id": 836, "seek": 456212, "start": 4581.12, "end": 4586.12, "text": " So it's diagonal, so all you have to do is provide to get the w.", "tokens": [407, 309, 311, 21539, 11, 370, 439, 291, 362, 281, 360, 307, 2893, 281, 483, 264, 261, 13], "temperature": 0.0, "avg_logprob": -0.12976662814617157, "compression_ratio": 1.356687898089172, "no_speech_prob": 2.4299335564137436e-05}, {"id": 837, "seek": 458612, "start": 4586.12, "end": 4593.12, "text": " Exactly, yeah. So solving a diagonal system of equations is even better than solving a triangular system", "tokens": [7587, 11, 1338, 13, 407, 12606, 257, 21539, 1185, 295, 11787, 307, 754, 1101, 813, 12606, 257, 38190, 1185], "temperature": 0.0, "avg_logprob": -0.10370755599716962, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.2408730627794284e-06}, {"id": 838, "seek": 458612, "start": 4593.12, "end": 4603.12, "text": " because you just have a single non-zero coefficient for each equation, so you can just divide through.", "tokens": [570, 291, 445, 362, 257, 2167, 2107, 12, 32226, 17619, 337, 1184, 5367, 11, 370, 291, 393, 445, 9845, 807, 13], "temperature": 0.0, "avg_logprob": -0.10370755599716962, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.2408730627794284e-06}, {"id": 839, "seek": 458612, "start": 4603.12, "end": 4607.12, "text": " So that's, yeah, very quick to solve.", "tokens": [407, 300, 311, 11, 1338, 11, 588, 1702, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.10370755599716962, "compression_ratio": 1.4848484848484849, "no_speech_prob": 6.2408730627794284e-06}, {"id": 840, "seek": 460712, "start": 4607.12, "end": 4624.12, "text": " And then we're left with, well, Vx equals w. Since V is orthonormal, we can rewrite that x equals V transpose w.", "tokens": [400, 550, 321, 434, 1411, 365, 11, 731, 11, 691, 87, 6915, 261, 13, 4162, 691, 307, 420, 11943, 24440, 11, 321, 393, 28132, 300, 2031, 6915, 691, 25167, 261, 13], "temperature": 0.0, "avg_logprob": -0.11930831618930983, "compression_ratio": 1.3217391304347825, "no_speech_prob": 1.6701018466847017e-05}, {"id": 841, "seek": 460712, "start": 4624.12, "end": 4628.12, "text": " And then this is that in equation form.", "tokens": [400, 550, 341, 307, 300, 294, 5367, 1254, 13], "temperature": 0.0, "avg_logprob": -0.11930831618930983, "compression_ratio": 1.3217391304347825, "no_speech_prob": 1.6701018466847017e-05}, {"id": 842, "seek": 462812, "start": 4628.12, "end": 4638.12, "text": " So we're using psi-pi's SVD here, getting back U sigma, and I called it VH in this case.", "tokens": [407, 321, 434, 1228, 20304, 12, 22630, 311, 31910, 35, 510, 11, 1242, 646, 624, 12771, 11, 293, 286, 1219, 309, 691, 39, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.17799660011574073, "compression_ratio": 1.3384615384615384, "no_speech_prob": 1.4285311408457346e-05}, {"id": 843, "seek": 462812, "start": 4638.12, "end": 4650.12, "text": " Then I see for w, notice I'm just dividing by sigma, dividing U transpose b by sigma.", "tokens": [1396, 286, 536, 337, 261, 11, 3449, 286, 478, 445, 26764, 538, 12771, 11, 26764, 624, 25167, 272, 538, 12771, 13], "temperature": 0.0, "avg_logprob": -0.17799660011574073, "compression_ratio": 1.3384615384615384, "no_speech_prob": 1.4285311408457346e-05}, {"id": 844, "seek": 465012, "start": 4650.12, "end": 4664.12, "text": " And here, I guess, U transpose is possibly larger than I need, so that's why I'm chopping off the end.", "tokens": [400, 510, 11, 286, 2041, 11, 624, 25167, 307, 6264, 4833, 813, 286, 643, 11, 370, 300, 311, 983, 286, 478, 35205, 766, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.06075615406036377, "compression_ratio": 1.289855072463768, "no_speech_prob": 2.123366812156746e-06}, {"id": 845, "seek": 465012, "start": 4664.12, "end": 4670.12, "text": " And then I'm returning VH transpose times w.", "tokens": [400, 550, 286, 478, 12678, 691, 39, 25167, 1413, 261, 13], "temperature": 0.0, "avg_logprob": -0.06075615406036377, "compression_ratio": 1.289855072463768, "no_speech_prob": 2.123366812156746e-06}, {"id": 846, "seek": 465012, "start": 4670.12, "end": 4679.12, "text": " Questions about this approach?", "tokens": [27738, 466, 341, 3109, 30], "temperature": 0.0, "avg_logprob": -0.06075615406036377, "compression_ratio": 1.289855072463768, "no_speech_prob": 2.123366812156746e-06}, {"id": 847, "seek": 467912, "start": 4679.12, "end": 4682.12, "text": " Yeah, some people are noticing that this is much slower.", "tokens": [865, 11, 512, 561, 366, 21814, 300, 341, 307, 709, 14009, 13], "temperature": 0.0, "avg_logprob": -0.10448044224789269, "compression_ratio": 1.2735042735042734, "no_speech_prob": 1.89244619832607e-05}, {"id": 848, "seek": 467912, "start": 4682.12, "end": 4698.12, "text": " We've gone from microseconds to milliseconds, which is an order of thousand-time difference.", "tokens": [492, 600, 2780, 490, 3123, 37841, 28750, 281, 34184, 11, 597, 307, 364, 1668, 295, 4714, 12, 3766, 2649, 13], "temperature": 0.0, "avg_logprob": -0.10448044224789269, "compression_ratio": 1.2735042735042734, "no_speech_prob": 1.89244619832607e-05}, {"id": 849, "seek": 469812, "start": 4698.12, "end": 4718.12, "text": " Any questions or questions about the equations, kind of how we solved this?", "tokens": [2639, 1651, 420, 1651, 466, 264, 11787, 11, 733, 295, 577, 321, 13041, 341, 30], "temperature": 0.0, "avg_logprob": -0.11930544035775321, "compression_ratio": 1.3442622950819672, "no_speech_prob": 7.889038897701539e-06}, {"id": 850, "seek": 469812, "start": 4718.12, "end": 4722.12, "text": " Okay, and then we had another technique that I'm not going to get into detail at all on,", "tokens": [1033, 11, 293, 550, 321, 632, 1071, 6532, 300, 286, 478, 406, 516, 281, 483, 666, 2607, 412, 439, 322, 11], "temperature": 0.0, "avg_logprob": -0.11930544035775321, "compression_ratio": 1.3442622950819672, "no_speech_prob": 7.889038897701539e-06}, {"id": 851, "seek": 472212, "start": 4722.12, "end": 4731.12, "text": " but wanted to let you know that it's out there, is that there's something called the random sketching technique for linear regression,", "tokens": [457, 1415, 281, 718, 291, 458, 300, 309, 311, 484, 456, 11, 307, 300, 456, 311, 746, 1219, 264, 4974, 12325, 278, 6532, 337, 8213, 24590, 11], "temperature": 0.0, "avg_logprob": -0.0914313944080208, "compression_ratio": 1.6794258373205742, "no_speech_prob": 2.0462131942622364e-05}, {"id": 852, "seek": 472212, "start": 4731.12, "end": 4744.12, "text": " which involves using a random matrix and computing SA equals SB, and then finding a solution to the regression SA x equals SB.", "tokens": [597, 11626, 1228, 257, 4974, 8141, 293, 15866, 16482, 6915, 26944, 11, 293, 550, 5006, 257, 3827, 281, 264, 24590, 16482, 2031, 6915, 26944, 13], "temperature": 0.0, "avg_logprob": -0.0914313944080208, "compression_ratio": 1.6794258373205742, "no_speech_prob": 2.0462131942622364e-05}, {"id": 853, "seek": 472212, "start": 4744.12, "end": 4751.12, "text": " So I just wanted to let you know that that exists as an example of a randomized approach.", "tokens": [407, 286, 445, 1415, 281, 718, 291, 458, 300, 300, 8198, 382, 364, 1365, 295, 257, 38513, 3109, 13], "temperature": 0.0, "avg_logprob": -0.0914313944080208, "compression_ratio": 1.6794258373205742, "no_speech_prob": 2.0462131942622364e-05}, {"id": 854, "seek": 475112, "start": 4751.12, "end": 4766.12, "text": " And this would be where you had a very large A and B.", "tokens": [400, 341, 576, 312, 689, 291, 632, 257, 588, 2416, 316, 293, 363, 13], "temperature": 0.0, "avg_logprob": -0.05867149315628351, "compression_ratio": 1.3059701492537314, "no_speech_prob": 6.854197181382915e-06}, {"id": 855, "seek": 475112, "start": 4766.12, "end": 4771.12, "text": " Okay, so now I think you will like this part.", "tokens": [1033, 11, 370, 586, 286, 519, 291, 486, 411, 341, 644, 13], "temperature": 0.0, "avg_logprob": -0.05867149315628351, "compression_ratio": 1.3059701492537314, "no_speech_prob": 6.854197181382915e-06}, {"id": 856, "seek": 475112, "start": 4771.12, "end": 4775.12, "text": " We're going to do a timing comparison and do it a little bit more formally.", "tokens": [492, 434, 516, 281, 360, 257, 10822, 9660, 293, 360, 309, 257, 707, 857, 544, 25983, 13], "temperature": 0.0, "avg_logprob": -0.05867149315628351, "compression_ratio": 1.3059701492537314, "no_speech_prob": 6.854197181382915e-06}, {"id": 857, "seek": 477512, "start": 4775.12, "end": 4781.12, "text": " So we were just kind of printing out what the times were for the diabetes data set.", "tokens": [407, 321, 645, 445, 733, 295, 14699, 484, 437, 264, 1413, 645, 337, 264, 13881, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.14426168983365284, "compression_ratio": 1.606936416184971, "no_speech_prob": 1.723106106510386e-05}, {"id": 858, "seek": 477512, "start": 4781.12, "end": 4790.12, "text": " What I'm going to do here is randomly generate some data, and M is going to be either 100, 1000, or 10,000.", "tokens": [708, 286, 478, 516, 281, 360, 510, 307, 16979, 8460, 512, 1412, 11, 293, 376, 307, 516, 281, 312, 2139, 2319, 11, 9714, 11, 420, 1266, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.14426168983365284, "compression_ratio": 1.606936416184971, "no_speech_prob": 1.723106106510386e-05}, {"id": 859, "seek": 477512, "start": 4790.12, "end": 4795.12, "text": " N is going to be 20, 100, or 1000.", "tokens": [426, 307, 516, 281, 312, 945, 11, 2319, 11, 420, 9714, 13], "temperature": 0.0, "avg_logprob": -0.14426168983365284, "compression_ratio": 1.606936416184971, "no_speech_prob": 1.723106106510386e-05}, {"id": 860, "seek": 477512, "start": 4795.12, "end": 4800.12, "text": " You can see down here is where I generate the data.", "tokens": [509, 393, 536, 760, 510, 307, 689, 286, 8460, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.14426168983365284, "compression_ratio": 1.606936416184971, "no_speech_prob": 1.723106106510386e-05}, {"id": 861, "seek": 480012, "start": 4800.12, "end": 4808.12, "text": " Randomly, A has dimensions M by N, and I'm going to loop through all possible values of M and N.", "tokens": [37603, 356, 11, 316, 575, 12819, 376, 538, 426, 11, 293, 286, 478, 516, 281, 6367, 807, 439, 1944, 4190, 295, 376, 293, 426, 13], "temperature": 0.0, "avg_logprob": -0.06422570881090667, "compression_ratio": 1.5450643776824033, "no_speech_prob": 8.664455890539102e-06}, {"id": 862, "seek": 480012, "start": 4808.12, "end": 4811.12, "text": " And then we're going to compare these different techniques.", "tokens": [400, 550, 321, 434, 516, 281, 6794, 613, 819, 7512, 13], "temperature": 0.0, "avg_logprob": -0.06422570881090667, "compression_ratio": 1.5450643776824033, "no_speech_prob": 8.664455890539102e-06}, {"id": 863, "seek": 480012, "start": 4811.12, "end": 4823.12, "text": " So we've got the naive solution with the normal equations, the Cholesky factorization, the QR factorization, SBD, and then also SciPy's least squares.", "tokens": [407, 321, 600, 658, 264, 29052, 3827, 365, 264, 2710, 11787, 11, 264, 761, 7456, 4133, 5952, 2144, 11, 264, 32784, 5952, 2144, 11, 26944, 35, 11, 293, 550, 611, 16942, 47, 88, 311, 1935, 19368, 13], "temperature": 0.0, "avg_logprob": -0.06422570881090667, "compression_ratio": 1.5450643776824033, "no_speech_prob": 8.664455890539102e-06}, {"id": 864, "seek": 480012, "start": 4823.12, "end": 4827.12, "text": " Let me see if there's anything else about the setup.", "tokens": [961, 385, 536, 498, 456, 311, 1340, 1646, 466, 264, 8657, 13], "temperature": 0.0, "avg_logprob": -0.06422570881090667, "compression_ratio": 1.5450643776824033, "no_speech_prob": 8.664455890539102e-06}, {"id": 865, "seek": 482712, "start": 4827.12, "end": 4833.12, "text": " Oh, in SciPy's least squares, it returns several items, but we just wanted the first one.", "tokens": [876, 11, 294, 16942, 47, 88, 311, 1935, 19368, 11, 309, 11247, 2940, 4754, 11, 457, 321, 445, 1415, 264, 700, 472, 13], "temperature": 0.0, "avg_logprob": -0.08931531257999753, "compression_ratio": 1.553648068669528, "no_speech_prob": 2.9022826311120298e-06}, {"id": 866, "seek": 482712, "start": 4833.12, "end": 4838.12, "text": " So that's why I've kind of wrapped it in this other method.", "tokens": [407, 300, 311, 983, 286, 600, 733, 295, 14226, 309, 294, 341, 661, 3170, 13], "temperature": 0.0, "avg_logprob": -0.08931531257999753, "compression_ratio": 1.553648068669528, "no_speech_prob": 2.9022826311120298e-06}, {"id": 867, "seek": 482712, "start": 4838.12, "end": 4850.12, "text": " And here I'm using, so I've got my row names, and then I have a dictionary that converts the name, kind of lists what the method is that we've defined for that.", "tokens": [400, 510, 286, 478, 1228, 11, 370, 286, 600, 658, 452, 5386, 5288, 11, 293, 550, 286, 362, 257, 25890, 300, 38874, 264, 1315, 11, 733, 295, 14511, 437, 264, 3170, 307, 300, 321, 600, 7642, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.08931531257999753, "compression_ratio": 1.553648068669528, "no_speech_prob": 2.9022826311120298e-06}, {"id": 868, "seek": 482712, "start": 4850.12, "end": 4854.12, "text": " Who in here has used pandas before?", "tokens": [2102, 294, 510, 575, 1143, 4565, 296, 949, 30], "temperature": 0.0, "avg_logprob": -0.08931531257999753, "compression_ratio": 1.553648068669528, "no_speech_prob": 2.9022826311120298e-06}, {"id": 869, "seek": 482712, "start": 4854.12, "end": 4856.12, "text": " Pandas, plural.", "tokens": [16995, 296, 11, 25377, 13], "temperature": 0.0, "avg_logprob": -0.08931531257999753, "compression_ratio": 1.553648068669528, "no_speech_prob": 2.9022826311120298e-06}, {"id": 870, "seek": 485612, "start": 4856.12, "end": 4858.12, "text": " Yeah. Okay. So it looks like everybody.", "tokens": [865, 13, 1033, 13, 407, 309, 1542, 411, 2201, 13], "temperature": 0.0, "avg_logprob": -0.10291577657063802, "compression_ratio": 1.4310344827586208, "no_speech_prob": 1.1478539818199351e-05}, {"id": 871, "seek": 485612, "start": 4858.12, "end": 4865.12, "text": " And I'm just using it here because the data frame is kind of a nice way to store a table of information.", "tokens": [400, 286, 478, 445, 1228, 309, 510, 570, 264, 1412, 3920, 307, 733, 295, 257, 1481, 636, 281, 3531, 257, 3199, 295, 1589, 13], "temperature": 0.0, "avg_logprob": -0.10291577657063802, "compression_ratio": 1.4310344827586208, "no_speech_prob": 1.1478539818199351e-05}, {"id": 872, "seek": 485612, "start": 4865.12, "end": 4872.12, "text": " And I'm using the multi-index, which can be handy to have M and N.", "tokens": [400, 286, 478, 1228, 264, 4825, 12, 471, 3121, 11, 597, 393, 312, 13239, 281, 362, 376, 293, 426, 13], "temperature": 0.0, "avg_logprob": -0.10291577657063802, "compression_ratio": 1.4310344827586208, "no_speech_prob": 1.1478539818199351e-05}, {"id": 873, "seek": 485612, "start": 4872.12, "end": 4877.12, "text": " So now I just have a nested for loop.", "tokens": [407, 586, 286, 445, 362, 257, 15646, 292, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.10291577657063802, "compression_ratio": 1.4310344827586208, "no_speech_prob": 1.1478539818199351e-05}, {"id": 874, "seek": 487712, "start": 4877.12, "end": 4890.12, "text": " And this is kind of formally for the linear least squares problem. You want M greater than or equal to N, like you typically have more data points than you have features.", "tokens": [400, 341, 307, 733, 295, 25983, 337, 264, 8213, 1935, 19368, 1154, 13, 509, 528, 376, 5044, 813, 420, 2681, 281, 426, 11, 411, 291, 5850, 362, 544, 1412, 2793, 813, 291, 362, 4122, 13], "temperature": 0.0, "avg_logprob": -0.08943688869476318, "compression_ratio": 1.4766355140186915, "no_speech_prob": 2.4060850591922645e-06}, {"id": 875, "seek": 487712, "start": 4890.12, "end": 4892.12, "text": " So I'm just looking at those cases.", "tokens": [407, 286, 478, 445, 1237, 412, 729, 3331, 13], "temperature": 0.0, "avg_logprob": -0.08943688869476318, "compression_ratio": 1.4766355140186915, "no_speech_prob": 2.4060850591922645e-06}, {"id": 876, "seek": 487712, "start": 4892.12, "end": 4898.12, "text": " What I'm going to do is loop through them, randomly generate data of that size.", "tokens": [708, 286, 478, 516, 281, 360, 307, 6367, 807, 552, 11, 16979, 8460, 1412, 295, 300, 2744, 13], "temperature": 0.0, "avg_logprob": -0.08943688869476318, "compression_ratio": 1.4766355140186915, "no_speech_prob": 2.4060850591922645e-06}, {"id": 877, "seek": 487712, "start": 4898.12, "end": 4902.12, "text": " So with M rows and N columns.", "tokens": [407, 365, 376, 13241, 293, 426, 13766, 13], "temperature": 0.0, "avg_logprob": -0.08943688869476318, "compression_ratio": 1.4766355140186915, "no_speech_prob": 2.4060850591922645e-06}, {"id": 878, "seek": 490212, "start": 4902.12, "end": 4907.12, "text": " And then for each method, I am going to get the function.", "tokens": [400, 550, 337, 1184, 3170, 11, 286, 669, 516, 281, 483, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.05753251833793444, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.0289271813235246e-05}, {"id": 879, "seek": 490212, "start": 4907.12, "end": 4911.12, "text": " I'm going to use Python's time it module to time it.", "tokens": [286, 478, 516, 281, 764, 15329, 311, 565, 309, 10088, 281, 565, 309, 13], "temperature": 0.0, "avg_logprob": -0.05753251833793444, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.0289271813235246e-05}, {"id": 880, "seek": 490212, "start": 4911.12, "end": 4918.12, "text": " And what this will do is run the decomposition five times and track the speed of each one and average them.", "tokens": [400, 437, 341, 486, 360, 307, 1190, 264, 48356, 1732, 1413, 293, 2837, 264, 3073, 295, 1184, 472, 293, 4274, 552, 13], "temperature": 0.0, "avg_logprob": -0.05753251833793444, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.0289271813235246e-05}, {"id": 881, "seek": 490212, "start": 4918.12, "end": 4927.12, "text": " And it's good to run it a few times just to kind of reduce variation some.", "tokens": [400, 309, 311, 665, 281, 1190, 309, 257, 1326, 1413, 445, 281, 733, 295, 5407, 12990, 512, 13], "temperature": 0.0, "avg_logprob": -0.05753251833793444, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.0289271813235246e-05}, {"id": 882, "seek": 492712, "start": 4927.12, "end": 4934.12, "text": " Then for the data frame, I'm going to set the value of that kind of that method for that row and column.", "tokens": [1396, 337, 264, 1412, 3920, 11, 286, 478, 516, 281, 992, 264, 2158, 295, 300, 733, 295, 300, 3170, 337, 300, 5386, 293, 7738, 13], "temperature": 0.0, "avg_logprob": -0.06311085007407448, "compression_ratio": 1.535, "no_speech_prob": 8.939482540881727e-06}, {"id": 883, "seek": 492712, "start": 4934.12, "end": 4938.12, "text": " This is how long it took.", "tokens": [639, 307, 577, 938, 309, 1890, 13], "temperature": 0.0, "avg_logprob": -0.06311085007407448, "compression_ratio": 1.535, "no_speech_prob": 8.939482540881727e-06}, {"id": 884, "seek": 492712, "start": 4938.12, "end": 4942.12, "text": " Oh, and then I'm also I'm interested in the air, not just the speed.", "tokens": [876, 11, 293, 550, 286, 478, 611, 286, 478, 3102, 294, 264, 1988, 11, 406, 445, 264, 3073, 13], "temperature": 0.0, "avg_logprob": -0.06311085007407448, "compression_ratio": 1.535, "no_speech_prob": 8.939482540881727e-06}, {"id": 885, "seek": 492712, "start": 4942.12, "end": 4948.12, "text": " So I've got the coefficients back from this would be.", "tokens": [407, 286, 600, 658, 264, 31994, 646, 490, 341, 576, 312, 13], "temperature": 0.0, "avg_logprob": -0.06311085007407448, "compression_ratio": 1.535, "no_speech_prob": 8.939482540881727e-06}, {"id": 886, "seek": 492712, "start": 4948.12, "end": 4952.12, "text": " And this is a little tricky. So function is a string.", "tokens": [400, 341, 307, 257, 707, 12414, 13, 407, 2445, 307, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.06311085007407448, "compression_ratio": 1.535, "no_speech_prob": 8.939482540881727e-06}, {"id": 887, "seek": 495212, "start": 4952.12, "end": 4957.12, "text": " And so time it, you actually pass a string to here.", "tokens": [400, 370, 565, 309, 11, 291, 767, 1320, 257, 6798, 281, 510, 13], "temperature": 0.0, "avg_logprob": -0.09224221580906918, "compression_ratio": 1.4789473684210526, "no_speech_prob": 4.565902600006666e-06}, {"id": 888, "seek": 495212, "start": 4957.12, "end": 4965.12, "text": " We're using locals to kind of look up the actual variable with that name.", "tokens": [492, 434, 1228, 23335, 281, 733, 295, 574, 493, 264, 3539, 7006, 365, 300, 1315, 13], "temperature": 0.0, "avg_logprob": -0.09224221580906918, "compression_ratio": 1.4789473684210526, "no_speech_prob": 4.565902600006666e-06}, {"id": 889, "seek": 495212, "start": 4965.12, "end": 4973.12, "text": " Call that on A and B and then check what the air is using our metrics coefficients.", "tokens": [7807, 300, 322, 316, 293, 363, 293, 550, 1520, 437, 264, 1988, 307, 1228, 527, 16367, 31994, 13], "temperature": 0.0, "avg_logprob": -0.09224221580906918, "compression_ratio": 1.4789473684210526, "no_speech_prob": 4.565902600006666e-06}, {"id": 890, "seek": 495212, "start": 4973.12, "end": 4977.12, "text": " And in this case, for simplicity, I decided just to look at the L2 air.", "tokens": [400, 294, 341, 1389, 11, 337, 25632, 11, 286, 3047, 445, 281, 574, 412, 264, 441, 17, 1988, 13], "temperature": 0.0, "avg_logprob": -0.09224221580906918, "compression_ratio": 1.4789473684210526, "no_speech_prob": 4.565902600006666e-06}, {"id": 891, "seek": 497712, "start": 4977.12, "end": 4982.12, "text": " So that's what I'm storing in my DF air data frame.", "tokens": [407, 300, 311, 437, 286, 478, 26085, 294, 452, 48336, 1988, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.08835330153956558, "compression_ratio": 1.7024390243902439, "no_speech_prob": 1.5689140127506107e-05}, {"id": 892, "seek": 497712, "start": 4982.12, "end": 4985.12, "text": " So I've got this DF data frame that's going to hold the time.", "tokens": [407, 286, 600, 658, 341, 48336, 1412, 3920, 300, 311, 516, 281, 1797, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.08835330153956558, "compression_ratio": 1.7024390243902439, "no_speech_prob": 1.5689140127506107e-05}, {"id": 893, "seek": 497712, "start": 4985.12, "end": 4990.12, "text": " DF air is going to hold the air.", "tokens": [48336, 1988, 307, 516, 281, 1797, 264, 1988, 13], "temperature": 0.0, "avg_logprob": -0.08835330153956558, "compression_ratio": 1.7024390243902439, "no_speech_prob": 1.5689140127506107e-05}, {"id": 894, "seek": 497712, "start": 4990.12, "end": 4993.12, "text": " And so I run that and that can be a little bit slow to run.", "tokens": [400, 370, 286, 1190, 300, 293, 300, 393, 312, 257, 707, 857, 2964, 281, 1190, 13], "temperature": 0.0, "avg_logprob": -0.08835330153956558, "compression_ratio": 1.7024390243902439, "no_speech_prob": 1.5689140127506107e-05}, {"id": 895, "seek": 497712, "start": 4993.12, "end": 5002.12, "text": " So I'm just going to use the results I have here since it's doing kind of all these matrix decompositions and some of the matrices are larger.", "tokens": [407, 286, 478, 445, 516, 281, 764, 264, 3542, 286, 362, 510, 1670, 309, 311, 884, 733, 295, 439, 613, 8141, 22867, 329, 2451, 293, 512, 295, 264, 32284, 366, 4833, 13], "temperature": 0.0, "avg_logprob": -0.08835330153956558, "compression_ratio": 1.7024390243902439, "no_speech_prob": 1.5689140127506107e-05}, {"id": 896, "seek": 500212, "start": 5002.12, "end": 5013.12, "text": " So let me see if this will fit on the screen when it's larger.", "tokens": [407, 718, 385, 536, 498, 341, 486, 3318, 322, 264, 2568, 562, 309, 311, 4833, 13], "temperature": 0.0, "avg_logprob": -0.1951601982116699, "compression_ratio": 1.3083333333333333, "no_speech_prob": 1.095287825592095e-05}, {"id": 897, "seek": 500212, "start": 5013.12, "end": 5017.12, "text": " Are you able to read this this table? All right.", "tokens": [2014, 291, 1075, 281, 1401, 341, 341, 3199, 30, 1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.1951601982116699, "compression_ratio": 1.3083333333333333, "no_speech_prob": 1.095287825592095e-05}, {"id": 898, "seek": 500212, "start": 5017.12, "end": 5020.12, "text": " We should put them both up.", "tokens": [492, 820, 829, 552, 1293, 493, 13], "temperature": 0.0, "avg_logprob": -0.1951601982116699, "compression_ratio": 1.3083333333333333, "no_speech_prob": 1.095287825592095e-05}, {"id": 899, "seek": 500212, "start": 5020.12, "end": 5025.12, "text": " So the top table.", "tokens": [407, 264, 1192, 3199, 13], "temperature": 0.0, "avg_logprob": -0.1951601982116699, "compression_ratio": 1.3083333333333333, "no_speech_prob": 1.095287825592095e-05}, {"id": 900, "seek": 502512, "start": 5025.12, "end": 5032.12, "text": " This like perfectly fits. Top table is the time it takes and the second table is the air.", "tokens": [639, 411, 6239, 9001, 13, 8840, 3199, 307, 264, 565, 309, 2516, 293, 264, 1150, 3199, 307, 264, 1988, 13], "temperature": 0.0, "avg_logprob": -0.24093528588612875, "compression_ratio": 1.1710526315789473, "no_speech_prob": 1.3629909517476335e-05}, {"id": 901, "seek": 503212, "start": 5032.12, "end": 5060.12, "text": " So I'm going to spend a moment just kind of looking at this and tell me what you notice of which methods are working best, kind of in which cases.", "tokens": [407, 286, 478, 516, 281, 3496, 257, 1623, 445, 733, 295, 1237, 412, 341, 293, 980, 385, 437, 291, 3449, 295, 597, 7150, 366, 1364, 1151, 11, 733, 295, 294, 597, 3331, 13], "temperature": 0.0, "avg_logprob": -0.25441226443728887, "compression_ratio": 1.3035714285714286, "no_speech_prob": 3.217642370145768e-05}, {"id": 902, "seek": 506012, "start": 5060.12, "end": 5068.12, "text": " So anyone want to share some observations?", "tokens": [407, 2878, 528, 281, 2073, 512, 18163, 30], "temperature": 0.0, "avg_logprob": -0.2629135356229894, "compression_ratio": 1.3533333333333333, "no_speech_prob": 1.147786679212004e-05}, {"id": 903, "seek": 506012, "start": 5068.12, "end": 5071.12, "text": " And then microphones between Aaron and Roger.", "tokens": [400, 550, 30495, 1296, 14018, 293, 17666, 13], "temperature": 0.0, "avg_logprob": -0.2629135356229894, "compression_ratio": 1.3533333333333333, "no_speech_prob": 1.147786679212004e-05}, {"id": 904, "seek": 506012, "start": 5071.12, "end": 5076.12, "text": " Someone throw it to Connor.", "tokens": [8734, 3507, 309, 281, 33133, 13], "temperature": 0.0, "avg_logprob": -0.2629135356229894, "compression_ratio": 1.3533333333333333, "no_speech_prob": 1.147786679212004e-05}, {"id": 905, "seek": 506012, "start": 5076.12, "end": 5082.12, "text": " Like across the board in terms of timing, the Cholesky factorization does pretty well.", "tokens": [1743, 2108, 264, 3150, 294, 2115, 295, 10822, 11, 264, 761, 7456, 4133, 5952, 2144, 775, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.2629135356229894, "compression_ratio": 1.3533333333333333, "no_speech_prob": 1.147786679212004e-05}, {"id": 906, "seek": 508212, "start": 5082.12, "end": 5090.12, "text": " It does. Yeah. Yeah. And the Cholesky factorization does, I thought, surprisingly well.", "tokens": [467, 775, 13, 865, 13, 865, 13, 400, 264, 761, 7456, 4133, 5952, 2144, 775, 11, 286, 1194, 11, 17600, 731, 13], "temperature": 0.0, "avg_logprob": -0.1443384315656579, "compression_ratio": 1.3461538461538463, "no_speech_prob": 3.500739012451959e-06}, {"id": 907, "seek": 508212, "start": 5090.12, "end": 5099.12, "text": " And these are there are much larger matrices out there that we could have done this on.", "tokens": [400, 613, 366, 456, 366, 709, 4833, 32284, 484, 456, 300, 321, 727, 362, 1096, 341, 322, 13], "temperature": 0.0, "avg_logprob": -0.1443384315656579, "compression_ratio": 1.3461538461538463, "no_speech_prob": 3.500739012451959e-06}, {"id": 908, "seek": 509912, "start": 5099.12, "end": 5116.12, "text": " Any other observations?", "tokens": [2639, 661, 18163, 30], "temperature": 0.0, "avg_logprob": -0.09968027898243495, "compression_ratio": 1.058139534883721, "no_speech_prob": 4.289198386686621e-06}, {"id": 909, "seek": 509912, "start": 5116.12, "end": 5126.12, "text": " I have to say, I was I was personally surprised by how slow SVD is.", "tokens": [286, 362, 281, 584, 11, 286, 390, 286, 390, 5665, 6100, 538, 577, 2964, 31910, 35, 307, 13], "temperature": 0.0, "avg_logprob": -0.09968027898243495, "compression_ratio": 1.058139534883721, "no_speech_prob": 4.289198386686621e-06}, {"id": 910, "seek": 512612, "start": 5126.12, "end": 5133.12, "text": " What do you notice about the air?", "tokens": [708, 360, 291, 3449, 466, 264, 1988, 30], "temperature": 0.0, "avg_logprob": -0.10614724592729048, "compression_ratio": 1.4792899408284024, "no_speech_prob": 6.643151664320612e-06}, {"id": 911, "seek": 512612, "start": 5133.12, "end": 5139.12, "text": " Yeah, it's the same in all of these. So that's reassuring.", "tokens": [865, 11, 309, 311, 264, 912, 294, 439, 295, 613, 13, 407, 300, 311, 19486, 1345, 13], "temperature": 0.0, "avg_logprob": -0.10614724592729048, "compression_ratio": 1.4792899408284024, "no_speech_prob": 6.643151664320612e-06}, {"id": 912, "seek": 512612, "start": 5139.12, "end": 5146.12, "text": " So here this problem is not not one where we're having to worry about kind of stability with different different ones.", "tokens": [407, 510, 341, 1154, 307, 406, 406, 472, 689, 321, 434, 1419, 281, 3292, 466, 733, 295, 11826, 365, 819, 819, 2306, 13], "temperature": 0.0, "avg_logprob": -0.10614724592729048, "compression_ratio": 1.4792899408284024, "no_speech_prob": 6.643151664320612e-06}, {"id": 913, "seek": 512612, "start": 5146.12, "end": 5153.12, "text": " Matthew, can you throw the microphone?", "tokens": [12434, 11, 393, 291, 3507, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.10614724592729048, "compression_ratio": 1.4792899408284024, "no_speech_prob": 6.643151664320612e-06}, {"id": 914, "seek": 515312, "start": 5153.12, "end": 5161.12, "text": " How would SVD get slower than that?", "tokens": [1012, 576, 31910, 35, 483, 14009, 813, 300, 30], "temperature": 0.0, "avg_logprob": -0.130125278379859, "compression_ratio": 1.1844660194174756, "no_speech_prob": 7.183046363934409e-06}, {"id": 915, "seek": 515312, "start": 5161.12, "end": 5169.12, "text": " Oh, that's a good question.", "tokens": [876, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.130125278379859, "compression_ratio": 1.1844660194174756, "no_speech_prob": 7.183046363934409e-06}, {"id": 916, "seek": 515312, "start": 5169.12, "end": 5180.12, "text": " Yeah, I'm not sure about that. I'll think about that more.", "tokens": [865, 11, 286, 478, 406, 988, 466, 300, 13, 286, 603, 519, 466, 300, 544, 13], "temperature": 0.0, "avg_logprob": -0.130125278379859, "compression_ratio": 1.1844660194174756, "no_speech_prob": 7.183046363934409e-06}, {"id": 917, "seek": 518012, "start": 5180.12, "end": 5187.12, "text": " That's a good question. So here I just used random uniform.", "tokens": [663, 311, 257, 665, 1168, 13, 407, 510, 286, 445, 1143, 4974, 9452, 13], "temperature": 0.0, "avg_logprob": -0.13387999423714572, "compression_ratio": 1.1929824561403508, "no_speech_prob": 1.7777947505237535e-05}, {"id": 918, "seek": 518012, "start": 5187.12, "end": 5195.12, "text": " Oh, yeah, maybe it was like an unlucky draw.", "tokens": [876, 11, 1338, 11, 1310, 309, 390, 411, 364, 38838, 2642, 13], "temperature": 0.0, "avg_logprob": -0.13387999423714572, "compression_ratio": 1.1929824561403508, "no_speech_prob": 1.7777947505237535e-05}, {"id": 919, "seek": 518012, "start": 5195.12, "end": 5203.12, "text": " Yeah, it's random for each one.", "tokens": [865, 11, 309, 311, 4974, 337, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.13387999423714572, "compression_ratio": 1.1929824561403508, "no_speech_prob": 1.7777947505237535e-05}, {"id": 920, "seek": 520312, "start": 5203.12, "end": 5213.12, "text": " And I also wanted to kind of share this as a because I do want to highlight that Cholesky is fast.", "tokens": [400, 286, 611, 1415, 281, 733, 295, 2073, 341, 382, 257, 570, 286, 360, 528, 281, 5078, 300, 761, 7456, 4133, 307, 2370, 13], "temperature": 0.0, "avg_logprob": -0.12633572043953362, "compression_ratio": 1.7376237623762376, "no_speech_prob": 6.240709353733109e-06}, {"id": 921, "seek": 520312, "start": 5213.12, "end": 5217.12, "text": " Yeah, Cholesky is fast.", "tokens": [865, 11, 761, 7456, 4133, 307, 2370, 13], "temperature": 0.0, "avg_logprob": -0.12633572043953362, "compression_ratio": 1.7376237623762376, "no_speech_prob": 6.240709353733109e-06}, {"id": 922, "seek": 520312, "start": 5217.12, "end": 5223.12, "text": " A lot of these. I want to highlight this as a technique for comparing methods.", "tokens": [316, 688, 295, 613, 13, 286, 528, 281, 5078, 341, 382, 257, 6532, 337, 15763, 7150, 13], "temperature": 0.0, "avg_logprob": -0.12633572043953362, "compression_ratio": 1.7376237623762376, "no_speech_prob": 6.240709353733109e-06}, {"id": 923, "seek": 520312, "start": 5223.12, "end": 5232.12, "text": " I think that this can be kind of nice to do this kind of like looping through and getting getting data on how fast things are in a systematic manner.", "tokens": [286, 519, 300, 341, 393, 312, 733, 295, 1481, 281, 360, 341, 733, 295, 411, 6367, 278, 807, 293, 1242, 1242, 1412, 322, 577, 2370, 721, 366, 294, 257, 27249, 9060, 13], "temperature": 0.0, "avg_logprob": -0.12633572043953362, "compression_ratio": 1.7376237623762376, "no_speech_prob": 6.240709353733109e-06}, {"id": 924, "seek": 523212, "start": 5232.12, "end": 5243.12, "text": " And it also I think makes it a lot easier to compare as opposed to looking, you know, having to scroll through like different places in the notebook when we've just output it one line at a time.", "tokens": [400, 309, 611, 286, 519, 1669, 309, 257, 688, 3571, 281, 6794, 382, 8851, 281, 1237, 11, 291, 458, 11, 1419, 281, 11369, 807, 411, 819, 3190, 294, 264, 21060, 562, 321, 600, 445, 5598, 309, 472, 1622, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.09720888282313492, "compression_ratio": 1.5372340425531914, "no_speech_prob": 2.0903535187244415e-06}, {"id": 925, "seek": 523212, "start": 5243.12, "end": 5261.12, "text": " So I think this is a good kind of good general purpose method for comparing comparing methods.", "tokens": [407, 286, 519, 341, 307, 257, 665, 733, 295, 665, 2674, 4334, 3170, 337, 15763, 15763, 7150, 13], "temperature": 0.0, "avg_logprob": -0.09720888282313492, "compression_ratio": 1.5372340425531914, "no_speech_prob": 2.0903535187244415e-06}, {"id": 926, "seek": 526112, "start": 5261.12, "end": 5271.12, "text": " And then I just wanted to note there's also an L1 regression where you're using the L1 norm for your your error instead.", "tokens": [400, 550, 286, 445, 1415, 281, 3637, 456, 311, 611, 364, 441, 16, 24590, 689, 291, 434, 1228, 264, 441, 16, 2026, 337, 428, 428, 6713, 2602, 13], "temperature": 0.0, "avg_logprob": -0.1096218478295111, "compression_ratio": 1.475, "no_speech_prob": 1.4509657376038376e-05}, {"id": 927, "seek": 526112, "start": 5271.12, "end": 5275.12, "text": " But we're not going to go into detail on that.", "tokens": [583, 321, 434, 406, 516, 281, 352, 666, 2607, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.1096218478295111, "compression_ratio": 1.475, "no_speech_prob": 1.4509657376038376e-05}, {"id": 928, "seek": 526112, "start": 5275.12, "end": 5286.12, "text": " I'm going to take a moment to talk about conditioning and stability.", "tokens": [286, 478, 516, 281, 747, 257, 1623, 281, 751, 466, 21901, 293, 11826, 13], "temperature": 0.0, "avg_logprob": -0.1096218478295111, "compression_ratio": 1.475, "no_speech_prob": 1.4509657376038376e-05}, {"id": 929, "seek": 528612, "start": 5286.12, "end": 5294.12, "text": " So condition number is a measure of how small changes in the input affect the output.", "tokens": [407, 4188, 1230, 307, 257, 3481, 295, 577, 1359, 2962, 294, 264, 4846, 3345, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.03025217545338166, "compression_ratio": 1.4137931034482758, "no_speech_prob": 6.338748335110722e-06}, {"id": 930, "seek": 528612, "start": 5294.12, "end": 5307.12, "text": " Why would we care about what happens when you have small changes in the input?", "tokens": [1545, 576, 321, 1127, 466, 437, 2314, 562, 291, 362, 1359, 2962, 294, 264, 4846, 30], "temperature": 0.0, "avg_logprob": -0.03025217545338166, "compression_ratio": 1.4137931034482758, "no_speech_prob": 6.338748335110722e-06}, {"id": 931, "seek": 530712, "start": 5307.12, "end": 5317.12, "text": " Matthew.", "tokens": [12434, 13], "temperature": 0.0, "avg_logprob": -0.17381401719718143, "compression_ratio": 1.3962264150943395, "no_speech_prob": 3.480360101093538e-05}, {"id": 932, "seek": 530712, "start": 5317.12, "end": 5321.12, "text": " That's true. I mean, so there is when you're using real world data. Yeah, there's going to be this noisiness.", "tokens": [663, 311, 2074, 13, 286, 914, 11, 370, 456, 307, 562, 291, 434, 1228, 957, 1002, 1412, 13, 865, 11, 456, 311, 516, 281, 312, 341, 572, 271, 1324, 13], "temperature": 0.0, "avg_logprob": -0.17381401719718143, "compression_ratio": 1.3962264150943395, "no_speech_prob": 3.480360101093538e-05}, {"id": 933, "seek": 530712, "start": 5321.12, "end": 5335.12, "text": " What's another reason that we're particularly interested in numerical in your algebra in this question?", "tokens": [708, 311, 1071, 1778, 300, 321, 434, 4098, 3102, 294, 29054, 294, 428, 21989, 294, 341, 1168, 30], "temperature": 0.0, "avg_logprob": -0.17381401719718143, "compression_ratio": 1.3962264150943395, "no_speech_prob": 3.480360101093538e-05}, {"id": 934, "seek": 533512, "start": 5335.12, "end": 5341.12, "text": " Sam.", "tokens": [4832, 13], "temperature": 0.0, "avg_logprob": -0.18659349282582602, "compression_ratio": 1.391304347826087, "no_speech_prob": 7.596513023599982e-05}, {"id": 935, "seek": 533512, "start": 5341.12, "end": 5348.12, "text": " So if you have something that is.", "tokens": [407, 498, 291, 362, 746, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.18659349282582602, "compression_ratio": 1.391304347826087, "no_speech_prob": 7.596513023599982e-05}, {"id": 936, "seek": 533512, "start": 5348.12, "end": 5354.12, "text": " The thing that comes to mind is the almost the right answer to almost the right question.", "tokens": [440, 551, 300, 1487, 281, 1575, 307, 264, 1920, 264, 558, 1867, 281, 1920, 264, 558, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18659349282582602, "compression_ratio": 1.391304347826087, "no_speech_prob": 7.596513023599982e-05}, {"id": 937, "seek": 535412, "start": 5354.12, "end": 5371.12, "text": " So if you do something to your matrix and it is slightly reconstructed slightly differently than you wanted to not have a huge effect on whatever you do next to it.", "tokens": [407, 498, 291, 360, 746, 281, 428, 8141, 293, 309, 307, 4748, 31499, 292, 4748, 7614, 813, 291, 1415, 281, 406, 362, 257, 2603, 1802, 322, 2035, 291, 360, 958, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.2101555789809629, "compression_ratio": 1.6571428571428573, "no_speech_prob": 8.938579412642866e-06}, {"id": 938, "seek": 535412, "start": 5371.12, "end": 5373.12, "text": " That that is true. Yeah.", "tokens": [663, 300, 307, 2074, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.2101555789809629, "compression_ratio": 1.6571428571428573, "no_speech_prob": 8.938579412642866e-06}, {"id": 939, "seek": 535412, "start": 5373.12, "end": 5382.12, "text": " And what's that? What kind of what's the cause, though, of why we would be getting something that's just almost the right answer to almost the right question?", "tokens": [400, 437, 311, 300, 30, 708, 733, 295, 437, 311, 264, 3082, 11, 1673, 11, 295, 983, 321, 576, 312, 1242, 746, 300, 311, 445, 1920, 264, 558, 1867, 281, 1920, 264, 558, 1168, 30], "temperature": 0.0, "avg_logprob": -0.2101555789809629, "compression_ratio": 1.6571428571428573, "no_speech_prob": 8.938579412642866e-06}, {"id": 940, "seek": 538212, "start": 5382.12, "end": 5388.12, "text": " As opposed to having always exactly the right answer.", "tokens": [1018, 8851, 281, 1419, 1009, 2293, 264, 558, 1867, 13], "temperature": 0.0, "avg_logprob": -0.15771865844726562, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.3845019566360861e-05}, {"id": 941, "seek": 538212, "start": 5388.12, "end": 5401.12, "text": " Exactly, because of floating point errors. So since with computers kind of having machine epsilon and floating point error, you're almost never having exactly what you want there.", "tokens": [7587, 11, 570, 295, 12607, 935, 13603, 13, 407, 1670, 365, 10807, 733, 295, 1419, 3479, 17889, 293, 12607, 935, 6713, 11, 291, 434, 1920, 1128, 1419, 2293, 437, 291, 528, 456, 13], "temperature": 0.0, "avg_logprob": -0.15771865844726562, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.3845019566360861e-05}, {"id": 942, "seek": 538212, "start": 5401.12, "end": 5404.12, "text": " You know, you have these tiny differences.", "tokens": [509, 458, 11, 291, 362, 613, 5870, 7300, 13], "temperature": 0.0, "avg_logprob": -0.15771865844726562, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.3845019566360861e-05}, {"id": 943, "seek": 538212, "start": 5404.12, "end": 5410.12, "text": " It's important to know how do small small differences affect the output.", "tokens": [467, 311, 1021, 281, 458, 577, 360, 1359, 1359, 7300, 3345, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.15771865844726562, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.3845019566360861e-05}, {"id": 944, "seek": 541012, "start": 5410.12, "end": 5424.12, "text": " And then as Sam was getting at this can also kind of propagate through your problem if you're doing lots of operations and you've got just a little bit of error each time that could really add up.", "tokens": [400, 550, 382, 4832, 390, 1242, 412, 341, 393, 611, 733, 295, 48256, 807, 428, 1154, 498, 291, 434, 884, 3195, 295, 7705, 293, 291, 600, 658, 445, 257, 707, 857, 295, 6713, 1184, 565, 300, 727, 534, 909, 493, 13], "temperature": 0.0, "avg_logprob": -0.12313543955485026, "compression_ratio": 1.4, "no_speech_prob": 3.84484565074672e-06}, {"id": 945, "seek": 542412, "start": 5424.12, "end": 5440.12, "text": " So the relative condition number is defined as and this is just looking at the change in the output divided by change in the input for very tiny changes.", "tokens": [407, 264, 4972, 4188, 1230, 307, 7642, 382, 293, 341, 307, 445, 1237, 412, 264, 1319, 294, 264, 5598, 6666, 538, 1319, 294, 264, 4846, 337, 588, 5870, 2962, 13], "temperature": 0.0, "avg_logprob": -0.07291013353011187, "compression_ratio": 1.3909090909090909, "no_speech_prob": 1.4822899174760096e-06}, {"id": 946, "seek": 544012, "start": 5440.12, "end": 5467.12, "text": " And then Trevathan, I like that he put actual numbers on this on page 91 says a problem is well conditioned if kappa the condition number is small, such as 110 or 100 and ill conditioned if kappa is large, such as that's a lot tech error typo in my part, such as 10 to the sixth or 10 to the 16th.", "tokens": [400, 550, 8648, 85, 9390, 11, 286, 411, 300, 415, 829, 3539, 3547, 322, 341, 322, 3028, 31064, 1619, 257, 1154, 307, 731, 35833, 498, 350, 25637, 264, 4188, 1230, 307, 1359, 11, 1270, 382, 20154, 420, 2319, 293, 3171, 35833, 498, 350, 25637, 307, 2416, 11, 1270, 382, 300, 311, 257, 688, 7553, 6713, 2125, 78, 294, 452, 644, 11, 1270, 382, 1266, 281, 264, 15102, 420, 1266, 281, 264, 3165, 392, 13], "temperature": 0.0, "avg_logprob": -0.24425809811323118, "compression_ratio": 1.6318681318681318, "no_speech_prob": 6.338974344544113e-06}, {"id": 947, "seek": 546712, "start": 5467.12, "end": 5473.12, "text": " And then this is kind of just a technical definition conditioning relates to the problem.", "tokens": [400, 550, 341, 307, 733, 295, 445, 257, 6191, 7123, 21901, 16155, 281, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.12919192843967015, "compression_ratio": 1.430232558139535, "no_speech_prob": 8.53003075462766e-06}, {"id": 948, "seek": 546712, "start": 5473.12, "end": 5483.12, "text": " So just least squares where stability is what's used to talk about the particular algorithm.", "tokens": [407, 445, 1935, 19368, 689, 11826, 307, 437, 311, 1143, 281, 751, 466, 264, 1729, 9284, 13], "temperature": 0.0, "avg_logprob": -0.12919192843967015, "compression_ratio": 1.430232558139535, "no_speech_prob": 8.53003075462766e-06}, {"id": 949, "seek": 546712, "start": 5483.12, "end": 5489.12, "text": " And so an example that we saw before was computing eigenvalues.", "tokens": [400, 370, 364, 1365, 300, 321, 1866, 949, 390, 15866, 10446, 46033, 13], "temperature": 0.0, "avg_logprob": -0.12919192843967015, "compression_ratio": 1.430232558139535, "no_speech_prob": 8.53003075462766e-06}, {"id": 950, "seek": 548912, "start": 5489.12, "end": 5498.12, "text": " Here we have two matrices, A is 1, 1000, 01, B is 1, 1000,.0011.", "tokens": [1692, 321, 362, 732, 32284, 11, 316, 307, 502, 11, 9714, 11, 23185, 11, 363, 307, 502, 11, 9714, 11, 2411, 628, 5348, 13], "temperature": 0.0, "avg_logprob": -0.22875320702268367, "compression_ratio": 1.3255813953488371, "no_speech_prob": 4.198271199129522e-05}, {"id": 951, "seek": 548912, "start": 5498.12, "end": 5508.12, "text": " So these are very close that they only differ in the bottom left entry, which is zero for A and.001 for B.", "tokens": [407, 613, 366, 588, 1998, 300, 436, 787, 743, 294, 264, 2767, 1411, 8729, 11, 597, 307, 4018, 337, 316, 293, 2411, 628, 16, 337, 363, 13], "temperature": 0.0, "avg_logprob": -0.22875320702268367, "compression_ratio": 1.3255813953488371, "no_speech_prob": 4.198271199129522e-05}, {"id": 952, "seek": 550812, "start": 5508.12, "end": 5522.12, "text": " When we get the eigenvalues, the eigenvalues for A are 1 and 1 and the eigenvalues for B are 2 and then this is practically zero. But 1, 1 and 2, 0. That's like a huge difference.", "tokens": [1133, 321, 483, 264, 10446, 46033, 11, 264, 10446, 46033, 337, 316, 366, 502, 293, 502, 293, 264, 10446, 46033, 337, 363, 366, 568, 293, 550, 341, 307, 15667, 4018, 13, 583, 502, 11, 502, 293, 568, 11, 1958, 13, 663, 311, 411, 257, 2603, 2649, 13], "temperature": 0.0, "avg_logprob": -0.17929684882070504, "compression_ratio": 1.4793388429752066, "no_speech_prob": 5.594210961135104e-06}, {"id": 953, "seek": 552212, "start": 5522.12, "end": 5545.12, "text": " And this is something that this isn't the computer's fault. This is something about the problem of finding an eigenvalue is that for relatively small change, we're getting a very different different answer.", "tokens": [400, 341, 307, 746, 300, 341, 1943, 380, 264, 3820, 311, 7441, 13, 639, 307, 746, 466, 264, 1154, 295, 5006, 364, 10446, 29155, 307, 300, 337, 7226, 1359, 1319, 11, 321, 434, 1242, 257, 588, 819, 819, 1867, 13], "temperature": 0.0, "avg_logprob": -0.06017162583091042, "compression_ratio": 1.4927536231884058, "no_speech_prob": 1.8738376184046501e-06}, {"id": 954, "seek": 554512, "start": 5545.12, "end": 5558.12, "text": " Are there questions about this idea of conditioning or of whether our problem is kind of well or ill conditioned?", "tokens": [2014, 456, 1651, 466, 341, 1558, 295, 21901, 420, 295, 1968, 527, 1154, 307, 733, 295, 731, 420, 3171, 35833, 30], "temperature": 0.0, "avg_logprob": -0.14247676849365234, "compression_ratio": 1.2840909090909092, "no_speech_prob": 8.939386134443339e-06}, {"id": 955, "seek": 555812, "start": 5558.12, "end": 5576.12, "text": " And then just even to kind of tie this back into this definition up here, the change in the solution here, we went from 1 to 2 and 1 to 0 for our two answers over the change in X, which was.001.", "tokens": [400, 550, 445, 754, 281, 733, 295, 7582, 341, 646, 666, 341, 7123, 493, 510, 11, 264, 1319, 294, 264, 3827, 510, 11, 321, 1437, 490, 502, 281, 568, 293, 502, 281, 1958, 337, 527, 732, 6338, 670, 264, 1319, 294, 1783, 11, 597, 390, 2411, 628, 16, 13], "temperature": 0.0, "avg_logprob": -0.0998565745803545, "compression_ratio": 1.4586466165413534, "no_speech_prob": 8.939361578086391e-06}, {"id": 956, "seek": 557612, "start": 5576.12, "end": 5594.12, "text": " That would be like 1 divided by.001, 1000. And that could actually get larger if we're doing smaller changes. We're going to have that this is kind of a poorly conditioned problem.", "tokens": [663, 576, 312, 411, 502, 6666, 538, 2411, 628, 16, 11, 9714, 13, 400, 300, 727, 767, 483, 4833, 498, 321, 434, 884, 4356, 2962, 13, 492, 434, 516, 281, 362, 300, 341, 307, 733, 295, 257, 22271, 35833, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1680470291449099, "compression_ratio": 1.2847222222222223, "no_speech_prob": 5.4220736274146475e-06}, {"id": 957, "seek": 557612, "start": 5594.12, "end": 5598.12, "text": " Tim?", "tokens": [7172, 30], "temperature": 0.0, "avg_logprob": -0.1680470291449099, "compression_ratio": 1.2847222222222223, "no_speech_prob": 5.4220736274146475e-06}, {"id": 958, "seek": 559812, "start": 5598.12, "end": 5607.12, "text": " Oh, Sam, can you pass the catch box to Tim?", "tokens": [876, 11, 4832, 11, 393, 291, 1320, 264, 3745, 2424, 281, 7172, 30], "temperature": 0.0, "avg_logprob": -0.2910994239475416, "compression_ratio": 0.8983050847457628, "no_speech_prob": 2.046110421360936e-05}, {"id": 959, "seek": 559812, "start": 5607.12, "end": 5612.12, "text": " Oh, yeah.", "tokens": [876, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2910994239475416, "compression_ratio": 0.8983050847457628, "no_speech_prob": 2.046110421360936e-05}, {"id": 960, "seek": 561212, "start": 5612.12, "end": 5628.12, "text": " Okay, great. Okay, thank you. Yeah. Yeah. And that's like if you wanted to do an even more scientific comparison, you could probably randomly generate several different matrices in average. Yeah.", "tokens": [1033, 11, 869, 13, 1033, 11, 1309, 291, 13, 865, 13, 865, 13, 400, 300, 311, 411, 498, 291, 1415, 281, 360, 364, 754, 544, 8134, 9660, 11, 291, 727, 1391, 16979, 8460, 2940, 819, 32284, 294, 4274, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.15190680273647966, "compression_ratio": 1.45625, "no_speech_prob": 3.726323484443128e-06}, {"id": 961, "seek": 561212, "start": 5628.12, "end": 5635.12, "text": " Yeah. Thank you, Tim. That's helpful.", "tokens": [865, 13, 1044, 291, 11, 7172, 13, 663, 311, 4961, 13], "temperature": 0.0, "avg_logprob": -0.15190680273647966, "compression_ratio": 1.45625, "no_speech_prob": 3.726323484443128e-06}, {"id": 962, "seek": 563512, "start": 5635.12, "end": 5651.12, "text": " Okay, so then in talking about this is another LaTeX typo.", "tokens": [1033, 11, 370, 550, 294, 1417, 466, 341, 307, 1071, 2369, 14233, 55, 2125, 78, 13], "temperature": 0.0, "avg_logprob": -0.2782170295715332, "compression_ratio": 0.9354838709677419, "no_speech_prob": 4.495011125982273e-06}, {"id": 963, "seek": 565112, "start": 5651.12, "end": 5666.12, "text": " So the product norm of A times norm of A inverse comes up a lot when you're like it appears in several different theorems around conditioning of problems. And so it has its own name, the condition number of A.", "tokens": [407, 264, 1674, 2026, 295, 316, 1413, 2026, 295, 316, 17340, 1487, 493, 257, 688, 562, 291, 434, 411, 309, 7038, 294, 2940, 819, 10299, 2592, 926, 21901, 295, 2740, 13, 400, 370, 309, 575, 1080, 1065, 1315, 11, 264, 4188, 1230, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.07410509732304787, "compression_ratio": 1.4315068493150684, "no_speech_prob": 6.048658633517334e-06}, {"id": 964, "seek": 566612, "start": 5666.12, "end": 5682.12, "text": " And so we've previously kind of talked about problems being well conditioned. Here we're talking about a matrix. But the condition number of A relates to it shows that both.", "tokens": [400, 370, 321, 600, 8046, 733, 295, 2825, 466, 2740, 885, 731, 35833, 13, 1692, 321, 434, 1417, 466, 257, 8141, 13, 583, 264, 4188, 1230, 295, 316, 16155, 281, 309, 3110, 300, 1293, 13], "temperature": 0.0, "avg_logprob": -0.1276833338615222, "compression_ratio": 1.3515625, "no_speech_prob": 4.71080875286134e-06}, {"id": 965, "seek": 568212, "start": 5682.12, "end": 5704.12, "text": " And I'm not going to give the full theorems because they kind of have more technical details than I want to get into. But computing B given A and X and AX equals B or computing X given A and B, the condition number is related to how well conditioned that problem is.", "tokens": [400, 286, 478, 406, 516, 281, 976, 264, 1577, 10299, 2592, 570, 436, 733, 295, 362, 544, 6191, 4365, 813, 286, 528, 281, 483, 666, 13, 583, 15866, 363, 2212, 316, 293, 1783, 293, 316, 55, 6915, 363, 420, 15866, 1783, 2212, 316, 293, 363, 11, 264, 4188, 1230, 307, 4077, 281, 577, 731, 35833, 300, 1154, 307, 13], "temperature": 0.0, "avg_logprob": -0.11497635311550564, "compression_ratio": 1.528735632183908, "no_speech_prob": 1.9946910470025614e-06}, {"id": 966, "seek": 570412, "start": 5704.12, "end": 5713.12, "text": " So this quantity of norm of A times norm of A inverse is something you'll see.", "tokens": [407, 341, 11275, 295, 2026, 295, 316, 1413, 2026, 295, 316, 17340, 307, 746, 291, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.05172280932581702, "compression_ratio": 1.3389830508474576, "no_speech_prob": 4.5658607632503845e-06}, {"id": 967, "seek": 570412, "start": 5713.12, "end": 5728.12, "text": " I think we're about at time. We'll go over this kind of conditioning next time.", "tokens": [286, 519, 321, 434, 466, 412, 565, 13, 492, 603, 352, 670, 341, 733, 295, 21901, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.05172280932581702, "compression_ratio": 1.3389830508474576, "no_speech_prob": 4.5658607632503845e-06}, {"id": 968, "seek": 572812, "start": 5728.12, "end": 5748.12, "text": " And then as a reminder, I think everyone knows this, that homework two was due today as well as well as the draft of the blog post.", "tokens": [50364, 400, 550, 382, 257, 13548, 11, 286, 519, 1518, 3255, 341, 11, 300, 14578, 732, 390, 3462, 965, 382, 731, 382, 731, 382, 264, 11206, 295, 264, 6968, 2183, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1663064667672822, "compression_ratio": 1.2596153846153846, "no_speech_prob": 9.160851186607033e-05}], "language": "en"}