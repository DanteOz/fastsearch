{"text": " Welcome back, and here is lesson four, which is where we get deep into the weeds of exactly what is going on when we are training a neural network. And we started looking at this in the previous lesson. We were looking at stochastic gradient descent. And so to remind you, we were looking at what Arthur Samuel said. Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment, or we would call it parameter assignment, in terms of actual performance, and provide a mechanism for altering the weight assignment, so as to maximize that performance. So we could make that entirely automatic, and a machine so programmed would learn from its experience. And that was our goal. So our initial attempt on the MNIST dataset was not really based on that. We didn't really have any parameters. So then last week we tried to figure out how we could parameterize it, how we could create a function that had parameters. And what we thought we could do would be to have something where the probability of being some particular number was expressed in terms of the pixels of that number, and some weights, and then we would just multiply them together and add them up. So we looked at how stochastic gradient descent worked last week, and the basic idea is that we start out by initializing the parameters randomly. We use them to make a prediction using a function such as this one. We then see how good that prediction is by measuring using a loss function. We then calculate the gradient, which is how much would the loss change if I changed one parameter by a little bit. We then use that to make a small step to change each of the parameters by a little bit, by multiplying the learning rate by the gradient to get a new set of predictions. And so we went round and round and round a few times until eventually we decided to stop. And so these are the basic seven steps that we went through. And so we did that for simple quadratic equation. And we had something which looked like this. And so by the end we had this nice sample of a curve getting closer and closer and closer. So I have a little summary at the start of this section summarizing gradient descent that Silva and I have in the notebooks in the book of what we just did. So you can review that and make sure it makes sense to you. So now let's use this to create our MNIST threes versus sevens model. And so to create a model, we're going to need to create something that we can pass into a function like, let's see where it was, pass into a function like this one. So we need just some pixels that are all lined up and some parameters that are all lined up and then we're going to sum them up. So our X's are going to be pixels. And so in this case, because we're just going to multiply each pixel by a parameter and add them up, the fact that they're laid out in a grid is not important. So let's reshape those those grids and turn them into vectors. The way we reshape things in PyTorch is by using the view method. And so the view method, you can pass to it how large you want each dimension to be. And so in this case, we want the number of columns to be equal to the total number of pixels in each picture, which is 28 times 28 because they're 28 by 28 images. And then the number of rows will be however many rows there are in the data. And so if you just use minus one when you call view, that means, you know, as many as there are in the data. So this will create something of the same with the same total number of elements that we had before. So we can grab all our threes, we can concatenate them, torch.cat with all of our sevens, and then reshape that into a matrix where each row is one image with all of the rows and columns of the image all lined up in a single vector. So then we're going to need labels. So that's our X. So we're going to need labels. Our labels will be a one for each of the threes and a zero for each of the sevens. So basically we're going to create an is3 model. So that's going to create a vector. We actually need it to be a matrix in in PyTorch. So unsqueeze will add an additional unit dimension to wherever I've asked for. So here in position one. So in other words, this is going to turn it from something which is a vector of 12,396 long into a matrix with 12,396 rows and one column. That's just what PyTorch expects to see. So now we're going to turn our X and Y into a data set and a data set is a very specific concept in PyTorch. It's something which we can index into using square brackets. And when we do so, it's expected to return a tuple. So here if we look at, we're going to create this data set and when we index into it, it's going to return a tuple containing our independent variable and a dependent variable for each for a particular row. And so to do that, we can use the Python zip function which takes one element of the first thing and combines it with, concatenates it with one element of the second thing and then it does that again and again and again. And so then if we create a list of those, it gives us a, it gives us a data set. It gives us a list which when we index into it, it's going to contain one image and one label. And so here you can see why there's my label and my image. I won't print out the whole thing, but it's a 784 long vector. So that's a really important concept. A data set is something that you can index into and get back a tuple. And here I am, this is called destructuring the tuple, which means I'm taking the two parts of the tuple and putting the first part in one variable and the second part in the other variable, which is something we do a lot in Python. It's pretty handy. A lot of other languages support that as well. Repeat the same three steps for a validation set. So we've now got a training data set and a validation data set. Right, so now we need to initialize our parameters. And so to do that, as we've discussed, we just do it randomly. So here is a variable function that given some size, some shape, if you like, we'll randomly initialize using a normal random number distribution in PyTorch. That's what randn does. And we can hit shift tab to see how that works. Okay. And it says here that it's going to have a variance of one. So I probably shouldn't call this standard deviation. I probably should call this variance actually. So multiply it by the variance to change its variance to whatever is requested, which will default to one. And then as we talked about when it comes to calculating our gradients, we have to tell PyTorch which things we want gradients for. And the way we do that is requires grad underscore. Remember this underscore at the end is a special magic symbol which tells Python, tells PyTorch that we want this function to actually change the thing that it's referring to. So this will change this tensor such that it requires gradients. So here's some weights. So our weights are going to need to be 28 by 28 by one shape, 28 by 28 because every pixel is going to need a weight and then one because we're going to need again, we're going to need to have that, that, that unit access to make it into a column. So that's what PyTorch expects. So there's our weights. Now just weights by pixels actually isn't going to be enough because weights by pixels were always equal zero when the pixels are equal to zero. It has a zero intercept. So we really want something where it's like wx plus b a line. So the b is we call the bias and so that's just going to be a single number. So let's grab a single number for our bias. So remember I told you there's a difference between the parameters and weights, sexually speaking. So here the weights are the w in this equation, the bias is b in this equation and the weights and bias together is the parameters of the function. They're all the things that we're going to change. They're all the things that have gradients that we're going to update. So there's an important bit of jargon for you. The weights and biases of the model are the parameters. So we can, yes question. What's the difference between gradient descent and stochastic gradient descent? So far we've only done gradient descent. We'll be doing stochastic gradient descent in a few minutes. So we can now create a calculated prediction for one image. So we can take an image such as the first one and multiply by the weights. We need to transpose them to make them line up in terms of the rows and columns and add it up and add the bias and there is a prediction. We want to do that for every image. We could do that with a for loop and that would be really, really slow. It wouldn't run on the GPU and it wouldn't run in optimized C code. So we actually want to use always to do kind of like looping over pixels, looping over images. You always need to try to make sure you're doing that without a Python for loop. In this case, doing this calculation for lots of rows and columns is a mathematical operation called matrix monoplay. So if you've forgotten your matrix multiplication or maybe never quite got around to it at high school, it would be a good idea to have a look at Khan Academy or something to learn about what it is, but it's actually, I'll give you the quick answer. This is from Wikipedia. If these are two matrices A and B, then this element here, 1, 2 in the output is going to be equal to the first bit here times the first bit here plus the second bit here times the second bit here. So it's going to be B12 times A11 plus B22 times A12. That's you can see the orange matches the orange. Ditto for over here. This would be equal to B13 times A31 plus B23 times A32 and so forth for every part. Here's a great picture of that in action. If you look at matrix multiplication.xyz, another way to think of it is we can kind of flip the second bit over on top and then multiply each bit together and add them up, multiply each bit together and add them up, and you can see always the second one here and ends up in the second spot and the first one ends up in the first spot. And that's what matrix multiplication is. So we can do our multiply and add up by using matrix multiplication and in Python and therefore PyTorch matrix multiplication is the at sign operator. So when you see at that means matrix multiply. So here is our 20.2336. If I do a matrix multiply of our training set by our weights and then we add the bias and here is our 20.336 for the first one and you can see though it's doing every single one. So that's really important is that matrix multiplication gives us an optimized way to do these simple linear functions for as many kind of rows and columns as we want. So this is one of the two fundamental equations of any neural network. Some rows of data, rows and columns of data, matrix multiply, some weights, add some bias and the second one which we'll see in a moment is an activation function. So that is some predictions from our randomly initialized model. So we can check how good our model is and so to do that we can decide that anything greater than zero we will call a three and anything less than zero we will call a seven. So preds greater than zero tells us whether or not something is predicted to be a three or not. Then turn that into a float so rather than true and false make it one and zero because that's what our training set contains and then check with our thresholded predictions are equal to our training set and this will return true every time a row is correctly predicted and false otherwise. So if we take all those trues and falses and turn them into floats so that'll be ones and zeros and then take their mean it's 0.49. So not surprisingly our randomly initialized model is right about half the time at predicting threes from sevens. I had one more method here which is dot item without dot item this would return a tensor it's a rank zero tensor it has no rows it has no columns it just it's just a number on its own but I actually wanted to unwrap it to create a normal Python scalar mainly just because I wanted to see the easily see the full set of decimal places and the reason for that is I want to show you how we're going to calculate the derivative on the accuracy by changing a parameter by a tiny bit. So let's take one parameter which will be weight zero and multiply it by 1.0001 and so that's going to make it a little bit bigger and then if I calculate how the the accuracy changes based on the change in that weight that will be the gradient of the accuracy with respect to that parameter. So I can do that by calculating my new set of predictions and then I can threshold them and then I can check whether they're equal to the training set and then take the mean and I get back exactly the same number. So remember that gradient is equal to rise over run if you remember back to your calculus or if you'd forgotten your calculus hopefully you've reviewed it on Khan Academy. So the change in the y so y new minus y old which is 0.4912 etc minus 0.4912 etc which is 0 divided by this change will give us 0. So at this point we have a problem our derivative is 0 so we have 0 gradients which means our step will be 0 which means our prediction will be unchanged. Okay so we have a problem and our problem is that our gradient is 0 and with a gradient of 0 we can't take a step and we can't get better predictions and so intuitively speaking the reason that our gradient is 0 is because when we change a single pixel by a tiny bit we might not ever in any way change an actual prediction to change from a 3 predicting a 3 to a 7 or vice versa because we have this we have this threshold. Okay and so in other words our our accuracy loss function here is is very bumpy it's like flat step flat step flat step so it's got this this 0 gradient all over the place. So what we need to do is use something other than accuracy as our loss function. So let's try and create a new function and what this new function is going to do is it's going to give us a better value kind of in much the same way that accuracy gives a better value. So this is the loss remember a small loss is better so to give us a lower loss when the accuracy is better but it won't have a 0 gradient. So that means that a slightly better prediction needs to have a slightly better loss. So let's have a look at an example let's say our targets our labels of like that is 3 oh there's just three rows three images here 1 0 1 okay and we've made some predictions from a neural net and those predictions gave us 0.9 0.4 0.2. So now consider this loss function a loss function we're going to use torch dot where which is basically the same as this list comprehension it's basically an if statement. So it's going to say for for where target equals 1 we're going to return 1 minus predictions so here target is 1 so it'll be 1 minus 0.9 and where target is not 1 it'll just be predictions. So for these examples here the first one target equals 1 will be 1 minus 0.9 just 0.1 the next one is target equals 0 so it'll best be the prediction just 0.4 and then for the third one it's a 1 for target so it'll be 1 minus prediction which is 0.8 and so you can see here when the prediction is correct correct in other words it's a num you know it's a high number when the target is 1 and a low number when the target is 0 these numbers are going to be smaller. So the worst one is when we predicted 0.2 so we're pretty we really thought that was actually a 0 but it's actually a 1 so we ended up with a 0.8 here because this is 1 minus prediction 1 minus 0.2 is 0.8 so we can then take the mean of all of these to calculate a loss so if you think about it this loss will be the smallest if the predictions are exactly right so if we did predictions is actually identical to the targets then this will be 0 0 0 okay where else if they were exactly wrong they say they were 1 minus then it's 1 1 1 so it's going to be the loss will be better ie smaller when the predictions are closer to the targets and so here we can now take the mean and when we do we get here 0.4 3 3 so let's say we change this last bad one this inaccurate prediction from 0.2 to 0.8 and the loss gets better from 0.43 to 0.23 so this is just this function is torch dot where dot mean so this is actually pretty good this is actually a loss function which pretty closely tracks accuracy was the accuracy is better the loss will be smaller but also it doesn't have these zero gradients because every time we change the prediction the loss changes because the prediction is literally part of the loss that's pretty neat isn't it one problem is this is only going to work well as long as the predictions are between 0 and 1 otherwise this 1 minus prediction thing is going to look a bit funny so we should try and find a way to ensure that the predictions are always between 0 and 1 and that's also going to just make a lot more intuitive sense because you know we like to be able to kind of think of these as if they're like probabilities or at least nicely scaled numbers so we need some function that can take our numbers have a look it's something which can take these big numbers and turn them all into numbers between 0 and 1 and it so happens that we have exactly the right function it's called the sigmoid function so the sigmoid function looks like this if you pass in a really small number you get a number very close to 0 if you pass in a big number you get a number very close to 1 it never gets past 1 and it never goes smaller than 0 and then it's kind of like the smooth curve between and in the middle it looks a lot like the y equals x line this is the definition of the sigmoid function it's 1 over 1 plus e to the minus x what is x x is just e to the power of something so if we look at e it's just a number like pi this is the simple it's just a number that has a particular value right so if we go e squared and we look at it's going to be a tensor use pi torch it could have float there we go and you can see that these are the same number so that's what torch.exp means okay so you know for me when I see these kinds of interesting functions I don't worry too much about the definition what I care about is the shape right so you can have a play around with graphing calculators or whatever to kind of see why it is that you end up with this shape from this particular equation but for me I just never think about that I it never really matters to me what's important is this sigmoid shape which is what we want it's something that squashes every number to be between naught and 1 so we can change MNIST loss to be exactly the same as it was before but first we can make everything into sigmoid first and then use torch.ware so that is a loss function that has all the properties we want it's it's something which is going to be have not have any of those nasty zero gradients and we've ensured that the input to the where is between naught and 1 so the reason we did this is because our our accuracy was kind of what we really care about is a good accuracy we can't use it to get our gradients just just to create our step to improve our parameters so we can change our our accuracy to another function that is similar in terms of it it's better when the accuracy is better but it also does not have these zero gradients and so you can see now where why we have a metric and a loss the metric is the thing we actually care about the loss is the thing that's similar to what we care about that has a nicely behaved gradient sometimes the thing you care about your metric does have a nicely defined gradient and you can use it directly as a loss for example we often use mean squared error but for classification unfortunately not so we need to now use this to to update the parameters and so there's a couple of ways we could do this one would be to loop through every image calculate a prediction for that image and then calculate a loss and then do a step and then step the parameters and then do that again for the next image and the next image and the next image that's going to be really slow because we're doing a single step for a single image so that would mean an epoch would take quite a while we could go much faster by doing every single image in the data set so a big matrix multiplication it can all be parallelized on the GPU and then so then we can we could then do a step based on the gradients looking at the entire data set but now that's going to be like a lot of work to just update the weights once and remember sometimes our data sets have millions or tens of millions of items so that's probably a bad idea too so why not compromise let's grab a few data items at a time to calculate our loss and our step if we grab a few data items at a time those two data items are called a mini-batch and a mini-batch just means a few pieces of data and so the size of your mini-batch is called surprisingly the batch size right so the bigger the batch size the closer you get to the full size of your data set the longer it's going to take to calculate a single set of losses a single step but the the more accurate it's going to be it's going to be like the gradients are going to be much closer to the true data set gradients and then the smaller the batch size the faster each step we'll be able to do but those steps will represent a smaller number of items and so they won't be such an accurate approximation of the real gradient of the whole data set. Is there a reason the mean of the loss is calculated over say doing a median since the median is less prone to getting influenced by outliers in the example you gave if the third point which was wrongly predicted as an outlier then the derivative would push the function away while doing SGD and a median could be better in that case. Honestly I've never tried using a median the problem with a median is it ends up really only caring about one number which is the number in the middle so it could end up really pretty much ignoring all of the things at each end in fact all it really cares about is the order of things so my guess is that you would end up with something that is only good at predicting one thing in the middle but I haven't tried it be interesting to see well I guess the other thing that would happen with a median is you would have a lot of zero gradients I think because it's picking the thing in the middle and you could you know change your values and the thing in the middle might well wouldn't be zero gradients but bumpy gradients the thing in the middle would suddenly jump to being a different item so it might not behave very well that's that's my guess you should try it. Okay so how do we ask for a few items at a time it turns out that PyTorch and fastai provide something to do that for you you can pass in any data set to this class called data loader and it will grab a few items from that data set at a time you can ask for how many by asking for a batch size and then you can as you can see it will grab a few items at a time until it's grabbed all of them so here I'm saying let's create a collection that just contains all the numbers from 0 to 14 let's pass that into a data loader with a batch size of 5 and then that's going to be something it's called an iterator in Python it's something that you can ask for one more thing from an iterator if you pass an iterator to list in Python it returns all of the things from the iterator so here are my three mini batches and you'll see here all the numbers from 0 to 15 appear they appear in a random order and they appear five at a time they appear in random order because shuffle equals true so normally in the training set we ask for things to be shuffled so it gives us a little bit more randomization more randomization is is good because it makes it harder for it to kind of learn what the data set looks like so that's what a data loader well that's how a data loader is created now remember though that our data sets actually return tuples and here I've just got single ints so let's actually create a tuple so if we enumerate all the letters of English then that means that returns 0 a 1 b 2 c better let's make that our data set so if we pass that to a data loader with a batch size of 6 and as you can see it returns tuples containing 6 of the first things and the associated 6 of the second things so this is like our independent variable and this is like our dependent variable okay and so and then at the end you know that with the batch size won't necessarily exactly divide nicely into the full size of the data set you might end up with a smaller batch so basically then we already have a data set remember and so we could pass it to a data loader and then we can basically say this an iterator in Python is something that you can actually loop through so when we say for in data loader it's going to return a tuple we can destructure it into the first bit and the second bit and so it's going to be our X and Y we can calculate our predictions we can calculate our loss from the predictions and the targets we can ask it to calculate our gradients and then we can update our parameters just like we did in our toy SGD example for the quadratic equation so let's reinitialize our weights and bias with the same two lines of code before let's create the data loader this time from our actual MNIST data set and create a nice big batch size so we do plenty of work each time and just to take a look let's just grab the first thing from the data loader first is a fast AI function which just grabs the first thing from an iterator it's just it's useful to look at you know kind of an arbitrary mini batch so here is the shape we're going to have the first mini batch is 256 rows of 784 long that's 28 by 28 so 256 flattened out images and 256 labels that are one long because that's just the number zero or the number one depending on whether it's a three or a seven do the same for the validation set so here's our validation data loader and so let's grab a batch here testing pass it into well why do we do that we should look yeah I guess yeah actually for our for our testing I'm going to just manually grab the first four things just so that we can make sure everything lines up so so let's grab just the first four things we'll call that a batch pass it into that linear function we created earlier or remember linear was just x batch at weights matrix multiply plus bias and so that's going to give us four results that's a prediction for each of those four images and so then we can calculate the loss using that loss function we just used and let's just grab the first four items of the training set and there's the loss okay and so now we can calculate the gradients and so the gradients are seven hundred and eighty four by one so in other words it's a column where every weight as a gradient it's what's the change in loss for a small change in that parameter and then the bias as a gradient that's a single number because the bias is just a single number so we can take those three steps and put it in a function so if you pass if you this is calculate gradient you pass it an x batch or y batch in some model then it's going to calculate the predictions calculate the loss and do the backward step and here we see calculate gradient and so we can get the just to take a look the mean of the weights gradient and the bias gradient and there it is if I call it a second time and look notice I have not done any step here this is exactly the same parameters I get a different value that's a concern you would expect to get the same gradient every time you called it with the same data why have the gradients changed that's because loss dot backward does not just calculate the gradients it calculates the gradients and adds them to the existing gradients the things in the dot grad attribute the reasons for that will come to you later but the for now the thing to know is just it does that so actually what we need to do is to call grad dot zero underscore so dot zero returns a tensor containing zeros and remember underscore does it in place so that updates the weights dot grad attribute which is a tensor to contain zeros so now if I do that and call it again I will get exactly the same number so here is how you train one epoch with SGD loop through the data loader grabbing the x batch and the y batch calculate the gradient prediction loss backward go through each of the parameters we're going to be passing those in so there's going to be the 768 weights and the one bias and then for each of those update the parameter to go minus equals gradient times learning rate that's our gradient descent step and then zero it out for the next time around the loop I'm not just saying P minus equals I'm saying P dot data minus equals and the reason for that is that remember pytorch keeps track of all of the calculations we do so that it can calculate the gradient well I don't want to calculate in the gradient of my gradient descent step that's like not part of the bottle right so dot data is a special attribute in pytorch where if you write to it it tells pytorch not to update the gradients using that calculation so this is your most basic standard SGD stochastic gradient descent loop so now we can answer that earlier question the difference between stochastic gradient descent and gradient descent is that gradient descent does not have this here that loops through each mini batch for gradient descent it does it on the whole data set each time around so train epoch or gradient descent would simply not have the for loop at all but instead it would calculate the gradient for the whole data set and update the parameters based on the whole data set which we never really do in practice we always use many batches of various sizes. Okay so we can take the function we had before where we compared the predictions to whether that well we used to be comparing the predictions to whether they were greater or less than zero right but now that we're doing the sigmoid remember the sigmoid will squish everything between naught and one so now we should compare the predictions to whether they're greater than 0.5 or not if they're greater than 0.5 just to look back at our sigmoid function though zero what used to be zero is now on the sigmoid is 0.5 okay so we need to just to make that slight change to our measure of accuracy. To calculate the accuracy for some x-batch and some y-batch this is actually assume this is actually the predictions then we take the sigmoid of the predictions we compare them to 0.5 to tell us whether it's a three or not we check what the actual target was to see which ones are correct and then we take the mean of those after converting the Boolean to floats so we can check that accuracy let's take our batch put it through our simple linear model compare it to the four items of the training set and there's the accuracy. So if we do that for every batch in the validation set then we can loop through with a list comprehension every batch in the validation set get the accuracy based on some model stack those all up together so that this is a list right if we want to turn that list into a tensor where the the items of the list of the tensor are the items of the list that's what stack does so we can stack up all those take the mean convert it to a standard Python scalar by calling dot item round it to four decimal places just for display and so here is our validation set accuracy as you would expect it's about 50% because it's random so we can now train for one epoch so we can say remember train epoch needed the parameters so our parameters in this case are the weights tensor and the bias tensor so train one epoch using the linear one model with the learning with the learning rate of one with these two parameters and then validate and look at that our accuracy is now 68.8% so we've we've trained an epoch so let's just repeat that 20 times train and validate and you can see the accuracy goes up and up and up and up and up to about 97% so that's cool we've built an SGD optimizer of a simple linear function that is getting about 97% on our simplified M list where this is just the threes and the sevens so a lot of steps there let's simplify this through some refactoring so the kind of simple refactoring we're going to do we're going to do a couple but the basic idea is we're going to create something called an optimizer class the first thing we'll do is we'll get rid of the linear one function remember the linear one function does x at w plus b there's actually a class in pytorch that does that equation for us so we may as well use it it's called nn.linear and nn.linear does two things it does that function for us and it also initializes the parameters for us so we don't have to do weights and bias in it params anymore we just create an nn.linear class and that's going to create a matrix of size 28 by 28 comma 1 and a bias of size 1 it will set requires great equals true for us it's all going to be encapsulated in this class and then when I call that as a function it's going to do my x at w plus b so to see the parameters in it we would expect it to contain 784 weights and one bias we can just call dot parameters and we can destructure it to w comma b and see yep it is 784 and one for the weights and bias so that's cool so this is just you could you know it could be an interesting exercise for you to create this class yourself from scratch you should be able to at this point so that you can confirm that you can recreate something that behaves exactly like an n.linear so now that we've got this object which contains our parameters in a parameters method we could now create an optimizer so for our optimizer we're going to pass it the parameters to optimize and a learning rate we'll store them away and we'll have something called step which goes through each parameter and does that thing we just saw p.data minus equals p.grad times learning rate and it's also going to have something called zero grad which goes through each parameter and zeros it out or we could even just set it to none so that's the thing we're going to call basic optimizer so those are exactly the same lines of code we've already seen wrapped up into a class so we can now create an optimizer passing in the parameters of the linear model for these and our learning rate and so now our training loop is look through each mini batch in the data loader calculate the gradient opt dot step opt dot zero grad that's it validation function doesn't have to change and so let's put our training loop into a function that's going to loop through a bunch of epochs call an epoch print validate epoch and then run it and it's the same we're getting a slightly different result here but much much the same idea okay so that's cool right we've now refactoring using you know creating our own optimizer and using faster pytorch is built in nn dot linear class and you know by the way we don't actually need to use our own basic optimum not surprisingly pytorch comes with something which does exactly this and not surprisingly it's called sgd so and actually this sgd is provided by fastai fastai and pytorch provides some overlapping functionality they work much the same way so you can pass to sgd your parameters and your learning rate just like basic optimum okay and train it and get the same result so as you can see these classes that are in fastai and pytorch are not mysterious they're just pretty you know thin wrappers around functionality that we've now written ourself so there's quite a few steps there and if you haven't done gradient descent before then there's a lot of unpacking so so this this lesson is kind of the key lesson it's the one where you know like we should you know really take a stop and a deep breath at this point and make sure you're comfortable what's a data set what's a data loader what's nn dot linear what's sgd and if you you know if what any any or all of those don't make sense go back to where we defined it from scratch using python code well the data loader we didn't define from scratch but it you know the functionality is not particularly interesting you could certainly create your own from scratch if you wanted to that would be another pretty good exercise let's refactor some more fastai has a data loaders class which is as we've mentioned before is a tiny class that just you pass it a bunch of data loaders and it just stores them away as a dot train and a dot valid even though it's a tiny class it's it's super handy because with that we now have a single object that knows all the data we have and so it can make sure that your training data loader is shuffled and your validation loader isn't shuffled you know make sure everything works properly so that's what the data loaders class is you can pass in the training and valid data loader and then the next thing we have in fastai is the learner class and the learner class is something where we're going to pass in our data loaders we're going to pass in our model we're going to pass in our optimization function we're going to pass in our loss function we're going to pass in our metrics so all the stuff we've just done manually that's all learner does is it's just going to do that for us so it's just going to call this train model and this train epoch it's just you know it's inside learner so now if we go learn dot fit you can see again it's doing the same thing getting the same result and it's got some nice functionality it's printing it out into a pretty table for us and it's showing us the losses and the accuracy and how long it takes but there's nothing magic right you've been able to do exactly the same thing by hand using python and pytorch okay so so these abstractions are here to like let you write less code and to save some time and to save some cognitive overhead but they're not doing anything you can't do yourself and that's important right because if the if if they're doing things you can't do yourself then you can't customize them you can't debug them you know you can't profile them so we want to make sure that the the the stuff we're using is stuff that we understand what it's doing so this is just a linear function is not great we want a neural network so how do we turn this into a neural network or remember this is a linear function x at w plus b to turn it into a neural network we have two linear functions exactly the same but with different weights and different biases and in between this magic line of code which takes the result of our first linear function and then does a max between that and zero so a max of res and zero is going to take any negative numbers and turn them into zeros so we're going to do a linear function we're going to replace the negatives with zero and then we're going to take that and put it through another linear function that believe it or not is a neural net so w1 and w2 will weight tensors b1 and b2 are bias tensors just like before so we can initialize them just like before and we could now call exactly the same training code that we did before to all these so res.max zero is called a rectified linear unit which you will always see referred to as relu and so here is and and in pytorch it already has this function it's called f.relu and so if we plot it you can see it's as you'd expect it's zero for all negative numbers and then it's y equals x for positive numbers so you know here's some jargon rectified linear unit sounds scary sounds complicated but it's actually this incredibly tiny line of code this incredibly simple function and this happens a lot in deep learning things that sound complicated and sophisticated and impressive turn out to be normally super simple frankly at least once you know what it is so why do we do linear layer value linear layer well if we got rid of the middle if we got rid of the middle value and just went linear layer linear layer then you could rewrite that as a single linear layer when you multiply things and add and then multiply things and add and you can just change the coefficients and make it into a single multiply and then add so no matter how many linear layers we stack on top of each other we can never make anything more kind of effective than a simple linear model but if you put a non-linearity between the linear layers then actually you have the opposite this is now where something called the universal approximation theorem holds which is that if the size of the weight and bias matrices are big enough this can actually approximate any arbitrary function including the function of how do I recognize threes from sevens or or whatever so that's kind of amazing right this tiny thing is actually a universal function approximator as long as you have w1 b1 w2 and b2 have the right numbers and we know how to make them the right numbers you use SGD could take a very long time could take a lot of memory but the basic idea is that there is some solution to any computable problem and this is one of the biggest challenges a lot of beginners have to deep learning is that there's nothing else to it like there's often this like okay how do I make a neural net oh that is the neural net well how do I do deep learning training where there's GD there's things to like make it train a bit faster there's you know things to mean you need a few less parameters but everything from here is just performance tweaks honestly right so this is you know this is the key understanding of of training a neural network okay we can simplify things a bit more we already know that we can use nn.linear to replace the weight and bias so let's do that for both of the linear layers and then since we're simply taking the result of one function and passing it into the next and take the result of that function pass it to the next and so forth and then return the end this is called function composition function composition is when you just take the result of one function pass it to a new one take a result of one function pass it to a new one and so every pretty much neural network is just doing function composition of linear layers and these are called activation functions or nonlinearities so pytorch provides something to do function composition for us and it's called nn.sequential so it's going to do a linear layer pass the result to a relu pass the result to a linear layer you'll see here I'm not using f.relu I'm using nn.relu this is identical returns exactly the same thing but this is a class rather than a function yes rachel by using the nonlinearity won't using a function that makes all negative output zero make many of the gradients in the network zero and stop the learning process due to many zero gradients well that's a fantastic question and the answer is yes it does but they won't be zero for every image and remember the mini batches are shuffled so even if it's zero for every image in one mini batch it won't be for the next mini batch and it won't be the next time around we go for another epoch so yes it can create zeros and if if the neural net ends up with a set of parameters such that lots and lots of inputs end up as zeros you can end up with whole mini batches that are zero and you can end up in a situation where some of the neurons remain inactive inactive means that they're zero and they're basically dead units and this is a huge problem it basically means you're wasting computation so there's a few tricks to avoid that which we'll be learning about a lot one simple trick is to not make this thing flat here but just make it a less steep line that's called a leaky value leaky rectified linear unit and that they help a bit as well learn though even better is to make sure that we just kind of initialize to sensible initial values that are not too big and not too small and step by sensible amounts that are particularly not too big and generally if we do that we can keep things in the zone where they're positive most of the time but we are going to learn about how to actually analyze inside a network and find out how many dead units we have how many of these zeros we have because as you point out they are they are bad news they don't do any work and they'll continue to not do any work if enough of the inputs end up being zero. Okay so now that we've got a neural net we can use exactly the same learner we had before but this time we're passing the simple net instead of the linear one everything else is the same and we can call fit just like before and generally as your models get deeper so here we've gone from one layer to and I'm only counting the parameterized layers as layers you could say it's three I'm just going to call it two there's two trainable layers so I've gone from one layer to two I've checked dropped my learning rate from one to zero point one because the deeper models you know tend to be kind of bumpier less nicely behaved so often you need to use lower learning rates and so we train it for a while okay and we can actually find out what that training looks like by looking inside our learner and there's an attribute we create for you called recorder and that's going to record well everything that appears in this table basically well these three things the training loss the validation loss and the accuracy or any metrics so recorder dot values contains that kind of table of results and so item number two of each row will be the accuracy and so the the capital L class which I'm using here as a nice little method called item got that will will get the second item from every row and then I can plot that to see how the training went and I can get the final accuracy like so by grabbing the last row of the table and grabbing the second index two zero one two and my final accuracy not bad ninety eight point three percent so this is pretty amazing we now have a function that can solve any problem to any level of accuracy if we can find the right parameters and we have a way to find hopefully the best or at least a very good set of parameters for any function so this is kind of the magic yes Rachel. How could we use what we're learning here to get an idea of what the network is learning along the way like xylar and fergus did more or less we will look at that later not in the full detail of their paper but basically you can look in the dot parameters to see the values of those parameters and at this point well I mean why don't you try it yourself right you've actually got now the parameters so if you want to grab the model you can actually see learn dot model so we can we can look inside learn dot model to see the actual model that we just trained and you can see it's got the three things in it the linear the value the linear and you know what I kind of like to do is to put that into a variable make it a bit easy to work with and you can grab one layer by indexing in you can look at the parameters and that just gives me a something called a generator it's something that will give me a list of the parameters when I ask for them so I can just go wait comma bias equals to destructure them and so the weight is 30 by 784 because that's what I asked for so one of the things to note here is that to create a neural net so something with more than one layer I actually have 30 outputs not just one right so I'm kind of generating lots of you can think of as generating lots of features so it's kind of like 30 different linear linear models here and then I combine those 30 back into one so you could look at one of those by having a look at yeah so there's there's the numbers in the first row we could reshape that into the original shape of the images and we could even have a look and there it is right so you can see this is something so this is cool right we can actually see here we've got something which is which is kind of learning to find things at the top and the bottom and the middle and so we could look at the second one okay no idea what that's showing and so some of them are kind of you know I've probably got far more than I need which is why they're not that obvious but you can see yeah here's another thing that's looking pretty similar here's something that's kind of looking for this little bit in the middle so yeah this is the basic idea to understand the features that are not the first layer but later layers you have to be a bit more sophisticated but yeah to see the first layer ones you can you can just plot them okay so then you know just to compare we could use the full fast AI toolkit so grab our data loaders by using data loaders from folder as we've done before and create a CNN learner and a resnet and fit it for a single epoch and whoa 99.7 right so we did 40 epochs and got 98.3 as I said using all the tricks you can really speed things up and make things a lot better and so by the end of this course or at least both parts of this course you'll be able to from scratch get this 99.7 in a single epoch. Alright so jargon so jargon just to remind us value function that returns zero for negatives many batch a few inputs and labels which optionally are randomly selected the forward pass is the bit where we calculate the predictions the loss is the function that we're going to take the derivative of and then the gradient is the derivative of the loss with respect to each parameter the backward pass is when we calculate those gradients gradient descent is that full thing of taking a step in the direction opposite to the gradients by after calculating the loss and then the learning rate is the size of the step that we take. Other things to know perhaps the two most important pieces of jargon are all of the numbers that are in a neural network the numbers that we're learning are called parameters and then the numbers that we're calculating so every value that's calculated every matrix multiplication element that's calculated they're called activations so activations and parameters are all of the numbers in the neural net and so be very careful when I say from here on in in these lessons activations or parameters you've got to make sure you know what those mean because that's that's the entire basically almost the entire set of numbers that exist inside a neural net so activations are calculated parameters are learned we're doing this stuff with tensors and tensors are just regularly shaped arrays rank zero tensors we call scalars rank one tensors we call vectors rank two tensors we call matrices and we continue on to rank three tensors rank four tensors and so forth and rank five tensors are very common in deep learning so don't be scared of going up to higher numbers of dimensions. Okay so let's have a break oh we've got a question okay. Is there a rule of thumb for what non-linearity to choose given that there are many? Yeah there are many non-linearities to choose from and it doesn't generally matter very much which you choose so just use ReLU or leaky ReLU or yeah whatever any anyone should work fine later on we'll we'll look at the minor differences between between them but it's not so much something that you pick on a per problem it's more like some take a little bit longer and a little bit more accurate and some are a bit faster and a little bit less accurate. That's a good question okay so before you move on it's really important that you finish the questionnaire for this chapter because there's a whole lot of concepts that we've just done so you know try to go through the questionnaire go back and relook at the notebook and please run the code do the experiments and make sure it makes sense. All right let's have a seven minute break see you back here in seven minutes time. Okay welcome back so now that we know how to create and train a neural net let's cycle back and look deeper at some applications and so we're going to try to kind of interpolate in from one end we've done the kind of from scratch version at the other end we've done the kind of four lines of code version and we're going to gradually nibble at each end until we find ourselves in the middle and we've we've we've touched on all of it. So let's go back up to the kind of the four lines of code version and and delve a little deeper so let's go back to pets and let's think though about like how do you actually you know start with a new data set and figure out how to use it. So it you know the the data sets we provide it's easy enough to untar them you just say untar that'll download it and untar it if it's a data set that you're getting you can just use the terminal or either or Python or whatever so let's assume we have a path that's pointing at something so initially you don't you don't know what that something is. So we can start by doing LS to have a look and see what's inside there so the pets data set that we saw in lesson one contains three things annotations images and models and you'll see we have this little trick here where we say path dot base path equals and then the path to our data and that's just does a little simple thing where when we print it out it just doesn't show us it just shows us relative to this path which is a bit convenient. So if you go and have a look at the readme for the original pets data set it tells you what these images and annotations folders are and not surprisingly the images path if we go path slash images that's how we use path lib to grab a subdirectory and then LS we can see here are the names that the paths to the images as it mentions here most functions and methods in fast AI which return a collection don't return a Python list but they return a capital L and a capital L as we briefly mentioned is basically an enhanced list one of the enhancements is the way it prints the representation of its starts by showing you how many items there are in the list in the collection so there's 7394 images and it if there's more than 10 things it truncates it and just says dot dot dot to avoid filling up your screen so there's a couple of little conveniences there and so we can see from this output that the file name as we mentioned in lesson one if the first letter is a capital it means it's a cat and if the first letter is lowercase it means it's a dog but this time we're going to do something a bit more complex a lot more complex which is figure out what breed it is and so you can see the breed is kind of everything up to after the in the file name it's everything up to the last underscore and before this number is the breed so we want to label everything with its breed so we're going to take advantage of this structure so the way I would do this is to use a regular expression a regular expression is something that looks at a string and basically lets you kind of pull it apart into its pieces in very flexible ways this kind of simple little language for doing that if you haven't used regular expressions before and please Google regular expression tutorial now and look it's going to be like one of the most useful tools you'll come across in your life I use them almost every day I will go to details about how to use them since there's so many great tutorials and there's also a lot of great like exercises you know there's regex regex is short for regular expression there's regex crosswords there's regex Q&A there's all kinds of cool regex things a lot of people like me love this tool in order to there's also a regex lesson in the fastai nlp course maybe even two regex lessons oh yeah I'm sorry for forgetting about the fastai nlp course what an excellent resource that is so regular expressions are a bit hard to get right the first time so the best thing to do is to get a sample string so a good way to do that would be to just grab one of the file names so let's pop it in F name and then you can experiment with regular expressions so re is the regular expression module in Python and find all we'll just grab all the parts of a regular expression that have parentheses around them so this regular expression and R is a special kind of string in Python which basically says don't treat backslash as special because normally in Python like backslash n means a new line so here's a string which I'm going to capture any letter one or more times followed by an underscore followed by a digit one or more times followed by anything I probably should have used backslash dot but that's fine followed by the letters JPG followed by the end of the string and so if I call that regular expression against my file names name oh looks good right so we kind of check it out so now that seems to work we can create a data block where the independent variables are images the dependent variables are categories just like before get items is going to be get image files we're going to split it randomly as per usual and then we're going to get the label by calling regex labeler which is a just a handy little fast a class which labels things with a regular expression we can't call the regular expression this particular regular expression directly on the path lib path object we actually want to call it on the name attribute and faster I has a nice little function called using atra using attribute which takes this function and changes it to a function which will be passed this attribute that's going to be using regex labeler on the name attribute and then from that data block we can create the data loaders as usual there's two interesting lines here resize and or transforms or transforms we have seen before in notebook 2 in the section called data augmentation and so or transforms was the thing which can zoom in and zoom out and warp and rotate and change contrast and change brightness and so forth and flip to kind of give us almost it's like giving us more data being generated synthetically from the data we already have and we also learned about random resize crop which is a kind of a really cool way of getting ensuring you get square images at the same time that you're augmenting the data here we have a resize to a really large image but you know by deep learning standards 460 by 460 is a really large image and then we're using or transforms with a size so that's actually going to use random resize crop to a smaller size why are we doing that this particular combination of two steps does something which I think is unique to fast AI which we call pre sizing and the best way is I will show you this beautiful example of some PowerPoint wizardry that I'm so excited about to show how pre sizing works what pre sizing does is that first step where we say resize to 460 by 460 is it grabs a square and it grabs it randomly if it's a kind of landscape orientation photo it'll grab it randomly sort of take the whole height and randomly grab somewhere from along the side if it's a portrait orientation then it'll grab it you know we'll take the full width and grab it grand and grab a random bit from top to bottom so then we take this area here and here it is right and so that's what the first resize does and then the second or transforms bit will grab a random warped crop possibly rotated from in here and will turn that into a square and so it does so there's two steps it's first of all resize to a square that's big and then the second step is do a kind of rotation and warping and zooming stage to something smaller in this case 224 by 224 because this first step creates something that's square and always is the same size the second step can happen on the GPU and because normally things like rotating and image warping actually pretty slow also normally doing a zoom and a rotate and a warp actually is really destructive to the image because each one of those things requires an interpolation step which it's not just slow it actually makes the image really quite low quality so we do it in a very special way in fast AI I think it's unique where we do all of the all of these kind of coordinate transforms like rotations and walks and zooms and so forth not on the actual pixels but instead we kind of keep track of the changing coordinate values in a in a non lossy way so the full floating point value and then once at the very end we then do the interpolation there is also quite striking here is what the difference looks like hopefully you can see this on on the video on the left is our pre sizing approach and on the right is the standard approach that other libraries use and you can see that the one on the right is a lot less nicely focused and it also has like weird things like this should be grass here but it's actually got its kind of bum sticking way out this has a little bit of weird distortions this has got loads of weird distortions so you can see the precise version really ends up way way better and I think we have a question Rachel. Are the blocks in the data block an ordered list? Do they specify the input and output structures respectively? Are there always two blocks or can there be more than two? For example if you wanted a segmentation model with the second block be something about segmentation? So so yeah this is an ordered list so the first item says I want to create an image and then the second item says I want to create a category so that's my independent and dependent variable you can have one thing here you can have three things here you can have any amount of things here you want obviously the vast majority of the time it'll be two normally there's an independent variable and a dependent variable we'll be seeing this in more detail later although if you go back to the earlier lesson when we introduced data blocks I do have a picture kind of showing how these pieces fit together. So after you've put together your data block created your data loaders you want to make sure it's working correctly so the obvious thing to do for a computer vision data block is show batch and show batch will show you the items and you can kind of just make sure they look sensible that looks like the labels are reasonable if you add a unique equals true then it's going to show you the same image with all the different augmentations this is a good way to make sure your augmentations work if you make a mistake in your data block in this example there's no resize so the different images are going to be different sizes so it would be impossible to collate them into a batch so if you call dot summary this is a really neat thing which will go through and tell you everything that's happening so I collecting the items how many did I find what happened when I split them what are the different variables independent dependent variables I'm creating let's try and create one of these here's each step create my image that categorize is what the first thing gave me an American bulldog is the final sample is this image this size this category and then eventually it says oh it's not possible to collect your items I tried to collect the zero index members of your tuples so in other words that's the independent variable and I got this was size 500 by 375 this was 375 by 500 oh I can't collect these into a tensor because they're different sizes so this is a super great debugging tool for debugging your data blocks I have a question how does the item transforms pre-size work if the resize is smaller than the image is a whole width or height still taken or is it just a random crop with the resize value so if you remember back to lesson two we looked at the different ways of creating these things you can use squish you can use pad or you can use crop so if your image is smaller than the precise value then squish will really be zoom so it will just small stretch it'll stretch it and then pattern crop will do much the same thing and so you'll just end up with a you know the same just looks like these but it'll be a kind of lower more pixelated lower resolution because it's having to zoom in a little bit okay so a lot of people say that you should do a hell of a lot of data cleaning before you model we don't we say model as soon as you can because remember what we found in in notebook 2 your your model can teach you about the problems in your data so as soon as I've got to a point where I have a data block that's working and I have data loaders I'm going to build a model and so here I'm you know it also tells me how I'm going so I'm getting 7% error wow that's actually really good or a pets model and so at this point now that I have a model I can do that stuff we learned about earlier in 02 the notebook 02 where we train our model and use it to clean the data so we can look at the classification a confusion matrix top losses the image cleaner widget you know so forth okay now one thing interesting here is in notebook 4 we included a loss function when we created a learner and here we don't pass in a loss function why is that that's because fast AI will try to automatically pick a somewhat sensible loss function for you and so for a image classification task it knows what loss function is the normal one to pick and it's done it for you but let's have a look and see what actually did pick so we could have a look at learn dot loss funk and we will see it is cross entropy loss what on earth is cross entropy loss I'm glad you asked let's find out cross entropy loss is really much the same as the MNIST lost we created with that with that sigmoid and the one minus predictions and predictions but it's it's a kind of extended version of that and the extended version of that is that torch dot where that we looked at in notebook for only works when you have a binary outcome in that case it was is it a three or not but in this case we've got which of the 37 pet breeds is it so we want to kind of create something just like that sigmoid and torch dot where that which also works nicely for more than two categories so let's see how we can do that so first of all let's grab a batch yes a question why do we want to build a model before cleaning the data I would think a clean data set would help in training yeah absolutely a clean data set helps in training but remember as we saw in notebook O2 an initial model helps you clean the data set so remember how plot top losses helped us identify mislabeled images and the confusion matrix helped us recognize which things we were getting confused and might need you know fixing and the image classifier cleaner actually let us find things like an image that contained two bears rather than one bear and clean it up so a model is just a fantastic way to help you zoom in on the data that matters which things seem to have the problems which things are most important stuff like that so you would go through a new cleaner with the model helping you and then you go back and train it again with the clean data thanks for that great question okay so in order to understand cross entropy loss let's grab a batch of data which we can use DLs dot one batch and that's going to grab a batch from the training set we could also go first DLs dot train and that's going to do exactly the same thing and so then we can destructure that into the independent independent variable and so the dependent variable shows us we've got a batch size of 64 that shows us the 64 categories and remember those numbers simply refer to the index of the vocab so for example 16 is a boxer and so that all happens for you automatically when we say show batch it shows us those strings so here's our first mini batch and so now we can view the predictions that is the activations of the final layer of the network by calling get preds and you can pass in a data loader and a data loader can really be anything that's going to return a sequence of many batches so we can just pass in a list containing our many batch as a data loader and so that's going to get the predictions for one mini batch so here's some predictions okay so the actual predictions if we go preds zero dot sum to grab the predictions for the first image and add them all up they add up to one and there are 37 of them so that makes sense right it's like the very first thing is what is the probability that that is a else vocab so the first thing is what's the probability it's an Abyssinian cat it's 10 to the negative 6 you see and so forth so it's basically like it's not this it's not this it's not this and you can look through and oh yeah this one here you know obviously what I think it is so how did it you know so we obviously want the probabilities to sum to one because it would be pretty weird if if they didn't it would say you know that the the probability of being one of these things is more than one or less than one which would be extremely odd so how do we go about creating these predictions where each one is between zero and one and they all add up to one to do that we use something called softmax softmax is basically an extension of sigmoid to handle more than two levels two categories so remember the sigmoid function look like this and we use that for our threes versus sevens model so what if we want 37 categories rather than two categories we need one activation for every category so actually the the threes and sevens model rather than thinking of that as an is three model we could actually say oh that has two categories so that's actually create two activations one representing how three like something is and one representing how seven like something is so let's say you know let's just say that we have six MNIST digits and these were the can I do this and this first column were the activations of my model for for one activation and the second column was for a second activation so my final layer actually has two activations now so this is like how much like a three is it and this is how much like a seven is it but this one is not at all like a three and it's slightly not like a seven this is very much like a three and not much like a seven and so forth so we can take that model and rather having rather than having one activation for like is three we can have two activations for how much like a three how much like a seven so if we take the sigmoid of that we get two numbers between naught and one but they don't add up to one so that doesn't make any sense it can't be point six six chance it's a three and point five six chance it's a seven because every digit in that data set is only one or the other so that's not going to work but what we could do is we could take the difference between this value and this value and say that's how likely it is to be a three so in other words this one here with a high number here and a low number here is very likely to be a three so we could basically say in the binary case these activations that what really matters is their relative confidence of being a three versus a seven so we could calculate the difference between column one and column two or column index zero and column index one right now here's the difference between the two columns there's that big difference and we could take the sigmoid of that right and so this is now giving us a single number between naught and one and so then since we wanted two columns we could make column index zero the sigmoid and column index one could be one minus that and now look these all add up to one so here's probability of three probability of seven for the second one probably three probably seven and so forth so like that's a way that we could go from having two activations for every image to creating two probabilities each of which is between naught and one and each pair of which adds to one. Great how do we extend that to more than two columns to extend it to more than two columns we use this function which is called softmax. So softmax is equal to e to the x divided by sum of e to the x. Just to show you if I go softmax on my activations I get 0.6025 0.3975 0.6025 0.3975 I get exactly the same thing right. So softmax in the binary case is identical to the sigmoid that we just looked at but in the multi category case we basically end up with something like this let's say we were doing the teddy bear grizzly bear brown bear and for that remember our neural net is going to have the final layer will have three activations so let's say it was point oh two negative two point four nine one point two five that I calculate softmax I first go e to the power of each of these three things so here's e to the power of point oh two e to the power of negative two point four nine e to the power of three point four eight of the power of one point two five okay then I add them up so there's the sum of the X's and then softmax will simply be one point oh two divided by four point six and then this one will be point oh eight divided by four point six and this one will be three point four nine divided by four point six so since each one of these represents each number divided by the sum that means that the total is one okay and because all of these are positive and each one is an item divided by the sum it means all of these must be between naught and one so this shows you that softmax always gives you numbers between naught and one and they always add up to one so to do that in practice you can just call torch dot softmax and it will give you this result of this this function so you should experiment with this in your own time you know write this out by hand and try putting in these numbers right and and see how that you get back the numbers I claim you're going to get back make sure this makes sense to you so one of the interesting points about softmax is remember I told you that X is e to the power of something and now what that means is that e to the power of something grows very very fast right so like x before is 54 x of eight is 292,980 right it grows super fast and what that means is that if you have one activation that's just a bit bigger than the others its softmax will be a lot bigger than the others so intuitively the softmax function really wants to pick one class among the others which is generally what you want right when you're trying to train a classifier to say which breed is it you kind of want it to to pick one and kind of go for it right and so that's what softmax does that's not what you always want so sometimes at inference time you want it to be a bit cautious and so you kind of got to remember that softmax isn't always the perfect approach but it's the default it's what we use most of the time and it works well on a lot of situations so that is softmax now in the binary case for the MNIST 3 versus 7s this was how we calculated MNIST loss we took the sigmoid and then we did either one minus that or that as our loss function which is fine as you saw it it worked right and so we could do this exactly the same thing we can't use torch.where anymore because targets aren't just zero or one targets could be any number from 0 to 36 so we could do that by replacing the torch.where with indexing so here's an example for the binary case let's say these are our targets 0 1 0 1 1 0 and these are our softmax activations which we calculated before they're just some random numbers just for a toy example so one way to do instead of doing torch.where is it we could instead have a look at this I could say I could grab all the numbers from not to 5 and if I index into here with all the numbers from 0 to 5 and then my targets 0 1 0 1 1 0 then what that's going to do is it's going to pick the row 0 it'll pick 0.6 and then for row 1 it'll pick 1.49 a row 2 it'll pick 0.13 row 4 it'll pick 1.003 and so forth so this is a super nifty indexing expression which you should definitely play with right and it's basically this trick of passing multiple things to the pie torch indexer the first thing says which rows should you return and the second thing says for each of those rows which column should you return so this is returning all the rows and these columns for each one and so this is actually identical to torch.where so isn't that tricky and so the nice thing is we can now use that for more than just two values and so here's here's the fully worked out thing so I've got my threes column I've got my sevens column here's that target is the indexes from not 1 2 3 4 5 and so here's 0 0.6 1 1.49 0 2.13 and so forth so yeah this works just as well with more than two columns so we can add you know for doing a full M list you know so all the digits from naught to 9 we could have 10 columns and we would just be indexing into the 10 so this thing we're doing where we're going minus our activations matrix all of the numbers from naught to n and then our targets is exactly the same as something that already exists in pie torch called f.nllloss as you can see exactly the same that's again we're kind of seeing that these things inside pie torch and fast AI are just little shortcuts for stuff we can write ourselves and nllloss stands for negative log likelihood again sounds complex but actually it's just this indexing expression rather confusingly there's no log in it we'll see why in a moment so let's talk about logs so this locks this loss function works quite well as we as we saw in the notebook 04 it's basically this it is exactly the same as we do in notebook 04 just a different way of expressing it but we can actually make it better because remember the probabilities we're looking at are between naught and 1 so they can't be smaller than 0 they can't be greater than 1 which means that if our model is trying to decide whether to predict 0.99 or 0.999 it's going to think that those numbers are very very close together but won't really care but actually if you think about the error you know if there's like a hundred thing a thousand things then this would like be ten things are wrong and this would be like one thing is wrong but this is really like ten times better than this so really what we'd like to do is to transform the numbers between 0 and 1 to instead between be between negative infinity and infinity and there's a function that does exactly that which is called logarithm. Okay so as the so the numbers we could have can be between 0 and 1 and as we get closer and closer to 0 it goes down to infinity and then at 1 it's going to be 0 and we can't go above 0 because our loss function we want to be negative. So this logarithm in case you forgot hopefully you vaguely remember what logarithm is from high school but that basically the definition is is this if you have some number that is y that is b to the power of a then logarithm is defined such that a equals the logarithm of y comma b in other words it tells you b to the power of what equals y which is not that interesting of itself but one of the really interesting things about logarithms is this very cool relationship which is that log of a times b equals log of a plus log of b and we use that all the time in deep learning and machine learning because this number here a times b can get very very big or very very small if you multiply things a lot of small things together you'll get a tiny number if you multiply a lot of big things together you'll get a huge number it can get so big or so small that the kind of the precision in your computer's floating point gets really bad. Where else this thing here adding is not going to get out of control so we really love using logarithms like particularly in a deep neural net where there's lots of layers we're kind of multiplying and adding many times so this kind of tends to come out quite nicely. So when we take the probabilities that we saw before the things that came out of this function and we take their logs and we take the mean that is called negative log likelihood and so this ends up being kind of a really nicely behaved number because of this property of the log that we described. So if you take the softmax and then take the log and then pass that to an LL loss because remember that didn't actually take the log at all despite the name that gives you cross entropy loss. So that leaves an obvious question of why doesn't an LL loss actually take the log and the reason for that is that it's more convenient computationally to actually take the log back at the softmax step. So PyTorch has a function called log softmax and so since it's actually easier to do the log at the softmax stage it's just faster and more accurate. PyTorch assumes that you use softlogmax and then pass that to NLL loss. So NLL loss does not do the log it assumes that you've done the log beforehand. So log softmax followed by NLL loss is the definition of cross entropy loss in PyTorch. So that's our loss function and so you can pass that some activations and some targets and get back a number and pretty much everything in in PyTorch every every one of these kinds of functions you can either use the NN version as a class like this and then call that object as if it's a function or you can just use f dot with the camel case name as a function directly and as you can see they're exactly the same number. People normally use the class version in the documentation in PyTorch you'll see it normally uses the class version so we'll tend to use the class version as well. You'll see that it's returning a single number and that's because it takes the mean because a loss needs to be as we've discussed the mean but if you want to see the underlying numbers before taking the mean you can just pass in reduction equals none and that shows you the individual cross entropy losses before taking the mean. Okay great so this is a good place to stop with our discussion of loss functions and such things. Rachel were there any questions about this? Why does the loss function need to be negative? Well I mean I guess it doesn't but it's we want something that the lower it is the better and we kind of need it to cut off somewhere. I have to think about this more during the week because I'm it's a bit tired. Yeah so let me let me refresh my memory when I'm awake. Okay now next week well note not for the video next week actually happened last week so the thing I'm about to say is actually referring to. So next week we're going to be talking about data ethics and I wanted to kind of segue into that by talking about how my week's gone because a week or two ago in I did a as part of a lesson I actually talked about the efficacy of masks and specifically wearing masks in public and I pointed out that the efficacy of masks seemed like it could be really high and maybe everybody should be wearing them and somehow I found myself as the face of a global advocacy campaign and so if you go to masksforall.co you will find a website talking about masks and I've been on you know TV shows in South Africa and the US and England and Australia and on radio and blah blah blah talking about masks. Why is this? Well it's because as a data scientist you know I noticed that the data around masks seem to be getting misunderstood and it seemed that that misunderstanding was costing possibly hundreds of thousands of lives you know literally in the places that were using masks it seemed to be associated with you know orders of magnitude fewer deaths and one of the things we'll talk about next week is like you know what's your role as a data scientist and you know I strongly believe that it's to understand the data and then do something about it and so nobody was talking about this so I ended up writing an article that appeared in the Washington Post that basically called on people to really consider wearing masks which is this article and you know I was I was lucky I managed to kind of get a huge team of brilliant not huge a pretty decent sized team of brilliant volunteers who helped you know kind of build this website and kind of some PR folks and stuff like that but what these came clear was and I was talking to politicians you know and it is buffers and what was becoming clear is that people weren't convinced by the science which is fair enough because it's it's hard to you know when the WHO and the CDC is saying you don't need to wear a mask and some random data scientist is saying that doesn't seem to be what the data is showing you know you've got half a brain you would pick them WHO and the CDC not the random data scientist so I really felt like I if I was going to be an effective advocate I needed sort the science out and it you know credentialism is strong and so it wouldn't be enough for me to say it I needed to find other people to say it so I put together a team of 19 scientists including you know a professor of sociology a professor of aerosol dynamics the founder of an African movement that's that kind of studied preventative methods for methods for tuberculosis a Stanford professor who studies mask disposal and cleaning methods a bunch of Chinese scientists who study epidemiology modeling a UCLA professor who is one of the top infectious disease epidemiologists experts and so forth so like this kind of all-star team of people from all around the world and I had never met any of these people before so well no not quite true I knew Austin a little bit and I knew Zainab a little bit I knew Lex a little bit but on the whole you know and well Reshma we all know she's awesome so it's great to actually have a fast AI community person there too and so but yeah I kind of tried to pull together people from you know as many geographies as possible and as many areas of expertise as possible and you know the kind of the global community helped me find papers about about everything about you know how different materials work about how droplets form about epidemiology about case studies of people infecting with and without masks blah blah blah and we ended up in the last week basically we wrote this paper it contains 84 citations and you know we basically worked around the clock on it as a team and it's out and it's been sent to a number of some of the earlier versions three or four days ago we sent us some governments so one of the things is I in this team I try to look for people who were you know working closely with government leaders not just that they're scientists and so this this went out to a number of government ministers and in the last few days I've heard that it was a very significant part of decisions by governments to change their to change their guidelines around masks and you know the fights not over by any means in particular the UK is a bit of a holdout but I'm going to be on ITV tomorrow and then BBC the next day you know it's it's kind of required stepping out to be a lot more than just a data scientist I've had to pull together you know politicians and staffers I've had to you know you know hustle with the media to try and get you know coverage and you know today I'm now starting to do a lot of work with unions to try to get unions to understand this you know it's really a case of like saying okay as a data scientist and income in conjunction with real scientists we've we've built this really strong understanding that masks you know are this simple but incredibly powerful tool that doesn't do anything unless I can effectively communicate this to decision makers so today I was you know on the phone to you know one of the top union leaders in the country explaining what this means basically it turns out that in buses in America they're kind of the air conditioning is set up so that it blows from the back to the front and there's actually case studies in the medical literature of how people that are seated and have downwind of an air conditioning unit in a restaurant ended up all getting sick with kovat 19 and so we can see why like bus drivers are dying because they're like they're right in the wrong spot here and their passengers aren't wearing masks so I kind of want to explain this science to union leaders so that they understand that to keep the workers safe it's not enough just for the driver to wear a mask but all the people on the bus needed to be wearing masks as well so you know all of this is basically to say you know as data scientists I think we have a responsibility to the study the data and then do something about it it's not just a research you know exercise it's not just a computation exercise you know what what's the point of doing things if it doesn't lead to anything so yeah so next week we'll be talking about this a lot more but I think you know this is a really to me kind of interesting example of how digging into the data can lead to really amazing things happening and and in this case I strongly believe and a lot of people are telling me they strongly believe that this kind of advocacy work that's come out of this data analysis is is already saving lives and so I hope this might help inspire you to to take your data analysis and to take it to places that it really makes a difference so thank you very much and I'll see you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.34, "text": " Welcome back, and here is lesson four, which is where we get deep into the weeds of exactly", "tokens": [4027, 646, 11, 293, 510, 307, 6898, 1451, 11, 597, 307, 689, 321, 483, 2452, 666, 264, 26370, 295, 2293], "temperature": 0.0, "avg_logprob": -0.14600549024694107, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.012816683389246464}, {"id": 1, "seek": 0, "start": 12.34, "end": 18.3, "text": " what is going on when we are training a neural network. And we started looking at this in", "tokens": [437, 307, 516, 322, 562, 321, 366, 3097, 257, 18161, 3209, 13, 400, 321, 1409, 1237, 412, 341, 294], "temperature": 0.0, "avg_logprob": -0.14600549024694107, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.012816683389246464}, {"id": 2, "seek": 0, "start": 18.3, "end": 25.2, "text": " the previous lesson. We were looking at stochastic gradient descent. And so to remind you, we", "tokens": [264, 3894, 6898, 13, 492, 645, 1237, 412, 342, 8997, 2750, 16235, 23475, 13, 400, 370, 281, 4160, 291, 11, 321], "temperature": 0.0, "avg_logprob": -0.14600549024694107, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.012816683389246464}, {"id": 3, "seek": 2520, "start": 25.2, "end": 31.6, "text": " were looking at what Arthur Samuel said. Suppose we arrange for some automatic means of testing", "tokens": [645, 1237, 412, 437, 19624, 23036, 848, 13, 21360, 321, 9424, 337, 512, 12509, 1355, 295, 4997], "temperature": 0.0, "avg_logprob": -0.1280412276585897, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3922244508867152e-05}, {"id": 4, "seek": 2520, "start": 31.6, "end": 37.76, "text": " the effectiveness of any current weight assignment, or we would call it parameter assignment,", "tokens": [264, 21208, 295, 604, 2190, 3364, 15187, 11, 420, 321, 576, 818, 309, 13075, 15187, 11], "temperature": 0.0, "avg_logprob": -0.1280412276585897, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3922244508867152e-05}, {"id": 5, "seek": 2520, "start": 37.76, "end": 43.0, "text": " in terms of actual performance, and provide a mechanism for altering the weight assignment,", "tokens": [294, 2115, 295, 3539, 3389, 11, 293, 2893, 257, 7513, 337, 11337, 278, 264, 3364, 15187, 11], "temperature": 0.0, "avg_logprob": -0.1280412276585897, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3922244508867152e-05}, {"id": 6, "seek": 2520, "start": 43.0, "end": 48.2, "text": " so as to maximize that performance. So we could make that entirely automatic, and a", "tokens": [370, 382, 281, 19874, 300, 3389, 13, 407, 321, 727, 652, 300, 7696, 12509, 11, 293, 257], "temperature": 0.0, "avg_logprob": -0.1280412276585897, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3922244508867152e-05}, {"id": 7, "seek": 2520, "start": 48.2, "end": 54.96, "text": " machine so programmed would learn from its experience. And that was our goal. So our", "tokens": [3479, 370, 31092, 576, 1466, 490, 1080, 1752, 13, 400, 300, 390, 527, 3387, 13, 407, 527], "temperature": 0.0, "avg_logprob": -0.1280412276585897, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.3922244508867152e-05}, {"id": 8, "seek": 5496, "start": 54.96, "end": 62.36, "text": " initial attempt on the MNIST dataset was not really based on that. We didn't really have", "tokens": [5883, 5217, 322, 264, 376, 45, 19756, 28872, 390, 406, 534, 2361, 322, 300, 13, 492, 994, 380, 534, 362], "temperature": 0.0, "avg_logprob": -0.10969600907291274, "compression_ratio": 1.6425339366515836, "no_speech_prob": 3.6688461477751844e-06}, {"id": 9, "seek": 5496, "start": 62.36, "end": 69.56, "text": " any parameters. So then last week we tried to figure out how we could parameterize it,", "tokens": [604, 9834, 13, 407, 550, 1036, 1243, 321, 3031, 281, 2573, 484, 577, 321, 727, 13075, 1125, 309, 11], "temperature": 0.0, "avg_logprob": -0.10969600907291274, "compression_ratio": 1.6425339366515836, "no_speech_prob": 3.6688461477751844e-06}, {"id": 10, "seek": 5496, "start": 69.56, "end": 75.16, "text": " how we could create a function that had parameters. And what we thought we could do would be to", "tokens": [577, 321, 727, 1884, 257, 2445, 300, 632, 9834, 13, 400, 437, 321, 1194, 321, 727, 360, 576, 312, 281], "temperature": 0.0, "avg_logprob": -0.10969600907291274, "compression_ratio": 1.6425339366515836, "no_speech_prob": 3.6688461477751844e-06}, {"id": 11, "seek": 5496, "start": 75.16, "end": 81.04, "text": " have something where the probability of being some particular number was expressed in terms", "tokens": [362, 746, 689, 264, 8482, 295, 885, 512, 1729, 1230, 390, 12675, 294, 2115], "temperature": 0.0, "avg_logprob": -0.10969600907291274, "compression_ratio": 1.6425339366515836, "no_speech_prob": 3.6688461477751844e-06}, {"id": 12, "seek": 8104, "start": 81.04, "end": 88.2, "text": " of the pixels of that number, and some weights, and then we would just multiply them together", "tokens": [295, 264, 18668, 295, 300, 1230, 11, 293, 512, 17443, 11, 293, 550, 321, 576, 445, 12972, 552, 1214], "temperature": 0.0, "avg_logprob": -0.09236638225726228, "compression_ratio": 1.5561797752808988, "no_speech_prob": 8.059428182605188e-07}, {"id": 13, "seek": 8104, "start": 88.2, "end": 100.08000000000001, "text": " and add them up. So we looked at how stochastic gradient descent worked last week, and the", "tokens": [293, 909, 552, 493, 13, 407, 321, 2956, 412, 577, 342, 8997, 2750, 16235, 23475, 2732, 1036, 1243, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.09236638225726228, "compression_ratio": 1.5561797752808988, "no_speech_prob": 8.059428182605188e-07}, {"id": 14, "seek": 8104, "start": 100.08000000000001, "end": 107.96000000000001, "text": " basic idea is that we start out by initializing the parameters randomly. We use them to make", "tokens": [3875, 1558, 307, 300, 321, 722, 484, 538, 5883, 3319, 264, 9834, 16979, 13, 492, 764, 552, 281, 652], "temperature": 0.0, "avg_logprob": -0.09236638225726228, "compression_ratio": 1.5561797752808988, "no_speech_prob": 8.059428182605188e-07}, {"id": 15, "seek": 10796, "start": 107.96, "end": 117.11999999999999, "text": " a prediction using a function such as this one. We then see how good that prediction", "tokens": [257, 17630, 1228, 257, 2445, 1270, 382, 341, 472, 13, 492, 550, 536, 577, 665, 300, 17630], "temperature": 0.0, "avg_logprob": -0.08119275246137454, "compression_ratio": 1.8, "no_speech_prob": 1.9333549516886706e-06}, {"id": 16, "seek": 10796, "start": 117.11999999999999, "end": 122.46, "text": " is by measuring using a loss function. We then calculate the gradient, which is how", "tokens": [307, 538, 13389, 1228, 257, 4470, 2445, 13, 492, 550, 8873, 264, 16235, 11, 597, 307, 577], "temperature": 0.0, "avg_logprob": -0.08119275246137454, "compression_ratio": 1.8, "no_speech_prob": 1.9333549516886706e-06}, {"id": 17, "seek": 10796, "start": 122.46, "end": 129.24, "text": " much would the loss change if I changed one parameter by a little bit. We then use that", "tokens": [709, 576, 264, 4470, 1319, 498, 286, 3105, 472, 13075, 538, 257, 707, 857, 13, 492, 550, 764, 300], "temperature": 0.0, "avg_logprob": -0.08119275246137454, "compression_ratio": 1.8, "no_speech_prob": 1.9333549516886706e-06}, {"id": 18, "seek": 10796, "start": 129.24, "end": 135.78, "text": " to make a small step to change each of the parameters by a little bit, by multiplying", "tokens": [281, 652, 257, 1359, 1823, 281, 1319, 1184, 295, 264, 9834, 538, 257, 707, 857, 11, 538, 30955], "temperature": 0.0, "avg_logprob": -0.08119275246137454, "compression_ratio": 1.8, "no_speech_prob": 1.9333549516886706e-06}, {"id": 19, "seek": 13578, "start": 135.78, "end": 139.92, "text": " the learning rate by the gradient to get a new set of predictions. And so we went round", "tokens": [264, 2539, 3314, 538, 264, 16235, 281, 483, 257, 777, 992, 295, 21264, 13, 400, 370, 321, 1437, 3098], "temperature": 0.0, "avg_logprob": -0.10700462047870342, "compression_ratio": 1.6625766871165644, "no_speech_prob": 1.6536848761461442e-06}, {"id": 20, "seek": 13578, "start": 139.92, "end": 147.32, "text": " and round and round a few times until eventually we decided to stop. And so these are the basic", "tokens": [293, 3098, 293, 3098, 257, 1326, 1413, 1826, 4728, 321, 3047, 281, 1590, 13, 400, 370, 613, 366, 264, 3875], "temperature": 0.0, "avg_logprob": -0.10700462047870342, "compression_ratio": 1.6625766871165644, "no_speech_prob": 1.6536848761461442e-06}, {"id": 21, "seek": 13578, "start": 147.32, "end": 159.48, "text": " seven steps that we went through. And so we did that for simple quadratic equation. And", "tokens": [3407, 4439, 300, 321, 1437, 807, 13, 400, 370, 321, 630, 300, 337, 2199, 37262, 5367, 13, 400], "temperature": 0.0, "avg_logprob": -0.10700462047870342, "compression_ratio": 1.6625766871165644, "no_speech_prob": 1.6536848761461442e-06}, {"id": 22, "seek": 15948, "start": 159.48, "end": 167.56, "text": " we had something which looked like this. And so by the end we had this nice sample of a", "tokens": [321, 632, 746, 597, 2956, 411, 341, 13, 400, 370, 538, 264, 917, 321, 632, 341, 1481, 6889, 295, 257], "temperature": 0.0, "avg_logprob": -0.15005741119384766, "compression_ratio": 1.6893203883495145, "no_speech_prob": 1.3081729548503063e-06}, {"id": 23, "seek": 15948, "start": 167.56, "end": 178.92, "text": " curve getting closer and closer and closer. So I have a little summary at the start of", "tokens": [7605, 1242, 4966, 293, 4966, 293, 4966, 13, 407, 286, 362, 257, 707, 12691, 412, 264, 722, 295], "temperature": 0.0, "avg_logprob": -0.15005741119384766, "compression_ratio": 1.6893203883495145, "no_speech_prob": 1.3081729548503063e-06}, {"id": 24, "seek": 15948, "start": 178.92, "end": 182.88, "text": " this section summarizing gradient descent that Silva and I have in the notebooks in", "tokens": [341, 3541, 14611, 3319, 16235, 23475, 300, 50171, 293, 286, 362, 294, 264, 43782, 294], "temperature": 0.0, "avg_logprob": -0.15005741119384766, "compression_ratio": 1.6893203883495145, "no_speech_prob": 1.3081729548503063e-06}, {"id": 25, "seek": 15948, "start": 182.88, "end": 189.2, "text": " the book of what we just did. So you can review that and make sure it makes sense to you.", "tokens": [264, 1446, 295, 437, 321, 445, 630, 13, 407, 291, 393, 3131, 300, 293, 652, 988, 309, 1669, 2020, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.15005741119384766, "compression_ratio": 1.6893203883495145, "no_speech_prob": 1.3081729548503063e-06}, {"id": 26, "seek": 18920, "start": 189.2, "end": 199.16, "text": " So now let's use this to create our MNIST threes versus sevens model. And so to create", "tokens": [407, 586, 718, 311, 764, 341, 281, 1884, 527, 376, 45, 19756, 258, 4856, 5717, 3407, 82, 2316, 13, 400, 370, 281, 1884], "temperature": 0.0, "avg_logprob": -0.1378742481799836, "compression_ratio": 1.8534031413612566, "no_speech_prob": 7.338191494454804e-07}, {"id": 27, "seek": 18920, "start": 199.16, "end": 206.79999999999998, "text": " a model, we're going to need to create something that we can pass into a function like, let's", "tokens": [257, 2316, 11, 321, 434, 516, 281, 643, 281, 1884, 746, 300, 321, 393, 1320, 666, 257, 2445, 411, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.1378742481799836, "compression_ratio": 1.8534031413612566, "no_speech_prob": 7.338191494454804e-07}, {"id": 28, "seek": 18920, "start": 206.79999999999998, "end": 213.6, "text": " see where it was, pass into a function like this one. So we need just some pixels that", "tokens": [536, 689, 309, 390, 11, 1320, 666, 257, 2445, 411, 341, 472, 13, 407, 321, 643, 445, 512, 18668, 300], "temperature": 0.0, "avg_logprob": -0.1378742481799836, "compression_ratio": 1.8534031413612566, "no_speech_prob": 7.338191494454804e-07}, {"id": 29, "seek": 18920, "start": 213.6, "end": 217.6, "text": " are all lined up and some parameters that are all lined up and then we're going to sum", "tokens": [366, 439, 17189, 493, 293, 512, 9834, 300, 366, 439, 17189, 493, 293, 550, 321, 434, 516, 281, 2408], "temperature": 0.0, "avg_logprob": -0.1378742481799836, "compression_ratio": 1.8534031413612566, "no_speech_prob": 7.338191494454804e-07}, {"id": 30, "seek": 21760, "start": 217.6, "end": 227.32, "text": " them up. So our X's are going to be pixels. And so in this case, because we're just going", "tokens": [552, 493, 13, 407, 527, 1783, 311, 366, 516, 281, 312, 18668, 13, 400, 370, 294, 341, 1389, 11, 570, 321, 434, 445, 516], "temperature": 0.0, "avg_logprob": -0.09611850242092185, "compression_ratio": 1.5113636363636365, "no_speech_prob": 9.874612487692502e-07}, {"id": 31, "seek": 21760, "start": 227.32, "end": 231.84, "text": " to multiply each pixel by a parameter and add them up, the fact that they're laid out", "tokens": [281, 12972, 1184, 19261, 538, 257, 13075, 293, 909, 552, 493, 11, 264, 1186, 300, 436, 434, 9897, 484], "temperature": 0.0, "avg_logprob": -0.09611850242092185, "compression_ratio": 1.5113636363636365, "no_speech_prob": 9.874612487692502e-07}, {"id": 32, "seek": 21760, "start": 231.84, "end": 241.28, "text": " in a grid is not important. So let's reshape those those grids and turn them into vectors.", "tokens": [294, 257, 10748, 307, 406, 1021, 13, 407, 718, 311, 725, 42406, 729, 729, 677, 3742, 293, 1261, 552, 666, 18875, 13], "temperature": 0.0, "avg_logprob": -0.09611850242092185, "compression_ratio": 1.5113636363636365, "no_speech_prob": 9.874612487692502e-07}, {"id": 33, "seek": 24128, "start": 241.28, "end": 247.96, "text": " The way we reshape things in PyTorch is by using the view method. And so the view method,", "tokens": [440, 636, 321, 725, 42406, 721, 294, 9953, 51, 284, 339, 307, 538, 1228, 264, 1910, 3170, 13, 400, 370, 264, 1910, 3170, 11], "temperature": 0.0, "avg_logprob": -0.10836170598080283, "compression_ratio": 1.6604651162790698, "no_speech_prob": 1.414473558725149e-06}, {"id": 34, "seek": 24128, "start": 247.96, "end": 255.44, "text": " you can pass to it how large you want each dimension to be. And so in this case, we want", "tokens": [291, 393, 1320, 281, 309, 577, 2416, 291, 528, 1184, 10139, 281, 312, 13, 400, 370, 294, 341, 1389, 11, 321, 528], "temperature": 0.0, "avg_logprob": -0.10836170598080283, "compression_ratio": 1.6604651162790698, "no_speech_prob": 1.414473558725149e-06}, {"id": 35, "seek": 24128, "start": 255.44, "end": 262.88, "text": " the number of columns to be equal to the total number of pixels in each picture, which is", "tokens": [264, 1230, 295, 13766, 281, 312, 2681, 281, 264, 3217, 1230, 295, 18668, 294, 1184, 3036, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.10836170598080283, "compression_ratio": 1.6604651162790698, "no_speech_prob": 1.414473558725149e-06}, {"id": 36, "seek": 24128, "start": 262.88, "end": 268.94, "text": " 28 times 28 because they're 28 by 28 images. And then the number of rows will be however", "tokens": [7562, 1413, 7562, 570, 436, 434, 7562, 538, 7562, 5267, 13, 400, 550, 264, 1230, 295, 13241, 486, 312, 4461], "temperature": 0.0, "avg_logprob": -0.10836170598080283, "compression_ratio": 1.6604651162790698, "no_speech_prob": 1.414473558725149e-06}, {"id": 37, "seek": 26894, "start": 268.94, "end": 274.4, "text": " many rows there are in the data. And so if you just use minus one when you call view,", "tokens": [867, 13241, 456, 366, 294, 264, 1412, 13, 400, 370, 498, 291, 445, 764, 3175, 472, 562, 291, 818, 1910, 11], "temperature": 0.0, "avg_logprob": -0.1122658523087649, "compression_ratio": 1.7019230769230769, "no_speech_prob": 8.31527017908229e-07}, {"id": 38, "seek": 26894, "start": 274.4, "end": 279.34, "text": " that means, you know, as many as there are in the data. So this will create something", "tokens": [300, 1355, 11, 291, 458, 11, 382, 867, 382, 456, 366, 294, 264, 1412, 13, 407, 341, 486, 1884, 746], "temperature": 0.0, "avg_logprob": -0.1122658523087649, "compression_ratio": 1.7019230769230769, "no_speech_prob": 8.31527017908229e-07}, {"id": 39, "seek": 26894, "start": 279.34, "end": 284.4, "text": " of the same with the same total number of elements that we had before. So we can grab", "tokens": [295, 264, 912, 365, 264, 912, 3217, 1230, 295, 4959, 300, 321, 632, 949, 13, 407, 321, 393, 4444], "temperature": 0.0, "avg_logprob": -0.1122658523087649, "compression_ratio": 1.7019230769230769, "no_speech_prob": 8.31527017908229e-07}, {"id": 40, "seek": 26894, "start": 284.4, "end": 292.1, "text": " all our threes, we can concatenate them, torch.cat with all of our sevens, and then reshape that", "tokens": [439, 527, 258, 4856, 11, 321, 393, 1588, 7186, 473, 552, 11, 27822, 13, 18035, 365, 439, 295, 527, 3407, 82, 11, 293, 550, 725, 42406, 300], "temperature": 0.0, "avg_logprob": -0.1122658523087649, "compression_ratio": 1.7019230769230769, "no_speech_prob": 8.31527017908229e-07}, {"id": 41, "seek": 29210, "start": 292.1, "end": 300.1, "text": " into a matrix where each row is one image with all of the rows and columns of the image", "tokens": [666, 257, 8141, 689, 1184, 5386, 307, 472, 3256, 365, 439, 295, 264, 13241, 293, 13766, 295, 264, 3256], "temperature": 0.0, "avg_logprob": -0.13175562449863978, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.002161623859138e-07}, {"id": 42, "seek": 29210, "start": 300.1, "end": 306.72, "text": " all lined up in a single vector. So then we're going to need labels. So that's our X. So", "tokens": [439, 17189, 493, 294, 257, 2167, 8062, 13, 407, 550, 321, 434, 516, 281, 643, 16949, 13, 407, 300, 311, 527, 1783, 13, 407], "temperature": 0.0, "avg_logprob": -0.13175562449863978, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.002161623859138e-07}, {"id": 43, "seek": 29210, "start": 306.72, "end": 312.70000000000005, "text": " we're going to need labels. Our labels will be a one for each of the threes and a zero", "tokens": [321, 434, 516, 281, 643, 16949, 13, 2621, 16949, 486, 312, 257, 472, 337, 1184, 295, 264, 258, 4856, 293, 257, 4018], "temperature": 0.0, "avg_logprob": -0.13175562449863978, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.002161623859138e-07}, {"id": 44, "seek": 29210, "start": 312.70000000000005, "end": 321.04, "text": " for each of the sevens. So basically we're going to create an is3 model. So that's going", "tokens": [337, 1184, 295, 264, 3407, 82, 13, 407, 1936, 321, 434, 516, 281, 1884, 364, 307, 18, 2316, 13, 407, 300, 311, 516], "temperature": 0.0, "avg_logprob": -0.13175562449863978, "compression_ratio": 1.8333333333333333, "no_speech_prob": 7.002161623859138e-07}, {"id": 45, "seek": 32104, "start": 321.04, "end": 331.56, "text": " to create a vector. We actually need it to be a matrix in in PyTorch. So unsqueeze will", "tokens": [281, 1884, 257, 8062, 13, 492, 767, 643, 309, 281, 312, 257, 8141, 294, 294, 9953, 51, 284, 339, 13, 407, 2693, 1077, 10670, 486], "temperature": 0.0, "avg_logprob": -0.13568516035337705, "compression_ratio": 1.402116402116402, "no_speech_prob": 3.35967627052014e-07}, {"id": 46, "seek": 32104, "start": 331.56, "end": 339.56, "text": " add an additional unit dimension to wherever I've asked for. So here in position one. So", "tokens": [909, 364, 4497, 4985, 10139, 281, 8660, 286, 600, 2351, 337, 13, 407, 510, 294, 2535, 472, 13, 407], "temperature": 0.0, "avg_logprob": -0.13568516035337705, "compression_ratio": 1.402116402116402, "no_speech_prob": 3.35967627052014e-07}, {"id": 47, "seek": 32104, "start": 339.56, "end": 347.08000000000004, "text": " in other words, this is going to turn it from something which is a vector of 12,396 long", "tokens": [294, 661, 2283, 11, 341, 307, 516, 281, 1261, 309, 490, 746, 597, 307, 257, 8062, 295, 2272, 11, 12493, 21, 938], "temperature": 0.0, "avg_logprob": -0.13568516035337705, "compression_ratio": 1.402116402116402, "no_speech_prob": 3.35967627052014e-07}, {"id": 48, "seek": 34708, "start": 347.08, "end": 357.68, "text": " into a matrix with 12,396 rows and one column. That's just what PyTorch expects to see. So", "tokens": [666, 257, 8141, 365, 2272, 11, 12493, 21, 13241, 293, 472, 7738, 13, 663, 311, 445, 437, 9953, 51, 284, 339, 33280, 281, 536, 13, 407], "temperature": 0.0, "avg_logprob": -0.09412891613809686, "compression_ratio": 1.4010695187165776, "no_speech_prob": 4.812511065210856e-07}, {"id": 49, "seek": 34708, "start": 357.68, "end": 364.47999999999996, "text": " now we're going to turn our X and Y into a data set and a data set is a very specific", "tokens": [586, 321, 434, 516, 281, 1261, 527, 1783, 293, 398, 666, 257, 1412, 992, 293, 257, 1412, 992, 307, 257, 588, 2685], "temperature": 0.0, "avg_logprob": -0.09412891613809686, "compression_ratio": 1.4010695187165776, "no_speech_prob": 4.812511065210856e-07}, {"id": 50, "seek": 34708, "start": 364.47999999999996, "end": 371.59999999999997, "text": " concept in PyTorch. It's something which we can index into using square brackets. And", "tokens": [3410, 294, 9953, 51, 284, 339, 13, 467, 311, 746, 597, 321, 393, 8186, 666, 1228, 3732, 26179, 13, 400], "temperature": 0.0, "avg_logprob": -0.09412891613809686, "compression_ratio": 1.4010695187165776, "no_speech_prob": 4.812511065210856e-07}, {"id": 51, "seek": 37160, "start": 371.6, "end": 384.16, "text": " when we do so, it's expected to return a tuple. So here if we look at, we're going to create", "tokens": [562, 321, 360, 370, 11, 309, 311, 5176, 281, 2736, 257, 2604, 781, 13, 407, 510, 498, 321, 574, 412, 11, 321, 434, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.13710041303892392, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.884608309410396e-07}, {"id": 52, "seek": 37160, "start": 384.16, "end": 391.16, "text": " this data set and when we index into it, it's going to return a tuple containing our independent", "tokens": [341, 1412, 992, 293, 562, 321, 8186, 666, 309, 11, 309, 311, 516, 281, 2736, 257, 2604, 781, 19273, 527, 6695], "temperature": 0.0, "avg_logprob": -0.13710041303892392, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.884608309410396e-07}, {"id": 53, "seek": 37160, "start": 391.16, "end": 398.24, "text": " variable and a dependent variable for each for a particular row. And so to do that, we", "tokens": [7006, 293, 257, 12334, 7006, 337, 1184, 337, 257, 1729, 5386, 13, 400, 370, 281, 360, 300, 11, 321], "temperature": 0.0, "avg_logprob": -0.13710041303892392, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.884608309410396e-07}, {"id": 54, "seek": 39824, "start": 398.24, "end": 407.0, "text": " can use the Python zip function which takes one element of the first thing and combines", "tokens": [393, 764, 264, 15329, 20730, 2445, 597, 2516, 472, 4478, 295, 264, 700, 551, 293, 29520], "temperature": 0.0, "avg_logprob": -0.14894097646077473, "compression_ratio": 1.7947368421052632, "no_speech_prob": 2.521565647839452e-06}, {"id": 55, "seek": 39824, "start": 407.0, "end": 410.72, "text": " it with, concatenates it with one element of the second thing and then it does that", "tokens": [309, 365, 11, 1588, 7186, 1024, 309, 365, 472, 4478, 295, 264, 1150, 551, 293, 550, 309, 775, 300], "temperature": 0.0, "avg_logprob": -0.14894097646077473, "compression_ratio": 1.7947368421052632, "no_speech_prob": 2.521565647839452e-06}, {"id": 56, "seek": 39824, "start": 410.72, "end": 416.92, "text": " again and again and again. And so then if we create a list of those, it gives us a,", "tokens": [797, 293, 797, 293, 797, 13, 400, 370, 550, 498, 321, 1884, 257, 1329, 295, 729, 11, 309, 2709, 505, 257, 11], "temperature": 0.0, "avg_logprob": -0.14894097646077473, "compression_ratio": 1.7947368421052632, "no_speech_prob": 2.521565647839452e-06}, {"id": 57, "seek": 39824, "start": 416.92, "end": 422.44, "text": " it gives us a data set. It gives us a list which when we index into it, it's going to", "tokens": [309, 2709, 505, 257, 1412, 992, 13, 467, 2709, 505, 257, 1329, 597, 562, 321, 8186, 666, 309, 11, 309, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.14894097646077473, "compression_ratio": 1.7947368421052632, "no_speech_prob": 2.521565647839452e-06}, {"id": 58, "seek": 42244, "start": 422.44, "end": 429.52, "text": " contain one image and one label. And so here you can see why there's my label and my image.", "tokens": [5304, 472, 3256, 293, 472, 7645, 13, 400, 370, 510, 291, 393, 536, 983, 456, 311, 452, 7645, 293, 452, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1408068561553955, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.5294098147933255e-06}, {"id": 59, "seek": 42244, "start": 429.52, "end": 436.04, "text": " I won't print out the whole thing, but it's a 784 long vector. So that's a really important", "tokens": [286, 1582, 380, 4482, 484, 264, 1379, 551, 11, 457, 309, 311, 257, 1614, 25494, 938, 8062, 13, 407, 300, 311, 257, 534, 1021], "temperature": 0.0, "avg_logprob": -0.1408068561553955, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.5294098147933255e-06}, {"id": 60, "seek": 42244, "start": 436.04, "end": 443.92, "text": " concept. A data set is something that you can index into and get back a tuple. And here", "tokens": [3410, 13, 316, 1412, 992, 307, 746, 300, 291, 393, 8186, 666, 293, 483, 646, 257, 2604, 781, 13, 400, 510], "temperature": 0.0, "avg_logprob": -0.1408068561553955, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.5294098147933255e-06}, {"id": 61, "seek": 42244, "start": 443.92, "end": 447.76, "text": " I am, this is called destructuring the tuple, which means I'm taking the two parts of the", "tokens": [286, 669, 11, 341, 307, 1219, 2677, 1757, 1345, 264, 2604, 781, 11, 597, 1355, 286, 478, 1940, 264, 732, 3166, 295, 264], "temperature": 0.0, "avg_logprob": -0.1408068561553955, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.5294098147933255e-06}, {"id": 62, "seek": 44776, "start": 447.76, "end": 452.52, "text": " tuple and putting the first part in one variable and the second part in the other variable,", "tokens": [2604, 781, 293, 3372, 264, 700, 644, 294, 472, 7006, 293, 264, 1150, 644, 294, 264, 661, 7006, 11], "temperature": 0.0, "avg_logprob": -0.15707684392514437, "compression_ratio": 1.6981132075471699, "no_speech_prob": 3.7479904335668834e-07}, {"id": 63, "seek": 44776, "start": 452.52, "end": 455.96, "text": " which is something we do a lot in Python. It's pretty handy. A lot of other languages", "tokens": [597, 307, 746, 321, 360, 257, 688, 294, 15329, 13, 467, 311, 1238, 13239, 13, 316, 688, 295, 661, 8650], "temperature": 0.0, "avg_logprob": -0.15707684392514437, "compression_ratio": 1.6981132075471699, "no_speech_prob": 3.7479904335668834e-07}, {"id": 64, "seek": 44776, "start": 455.96, "end": 462.8, "text": " support that as well. Repeat the same three steps for a validation set. So we've now got", "tokens": [1406, 300, 382, 731, 13, 28523, 264, 912, 1045, 4439, 337, 257, 24071, 992, 13, 407, 321, 600, 586, 658], "temperature": 0.0, "avg_logprob": -0.15707684392514437, "compression_ratio": 1.6981132075471699, "no_speech_prob": 3.7479904335668834e-07}, {"id": 65, "seek": 44776, "start": 462.8, "end": 471.96, "text": " a training data set and a validation data set. Right, so now we need to initialize our", "tokens": [257, 3097, 1412, 992, 293, 257, 24071, 1412, 992, 13, 1779, 11, 370, 586, 321, 643, 281, 5883, 1125, 527], "temperature": 0.0, "avg_logprob": -0.15707684392514437, "compression_ratio": 1.6981132075471699, "no_speech_prob": 3.7479904335668834e-07}, {"id": 66, "seek": 44776, "start": 471.96, "end": 477.71999999999997, "text": " parameters. And so to do that, as we've discussed, we just do it randomly. So here is a variable", "tokens": [9834, 13, 400, 370, 281, 360, 300, 11, 382, 321, 600, 7152, 11, 321, 445, 360, 309, 16979, 13, 407, 510, 307, 257, 7006], "temperature": 0.0, "avg_logprob": -0.15707684392514437, "compression_ratio": 1.6981132075471699, "no_speech_prob": 3.7479904335668834e-07}, {"id": 67, "seek": 47772, "start": 477.72, "end": 488.6, "text": " function that given some size, some shape, if you like, we'll randomly initialize using", "tokens": [2445, 300, 2212, 512, 2744, 11, 512, 3909, 11, 498, 291, 411, 11, 321, 603, 16979, 5883, 1125, 1228], "temperature": 0.0, "avg_logprob": -0.18223624033470676, "compression_ratio": 1.4725274725274726, "no_speech_prob": 2.026140691668843e-06}, {"id": 68, "seek": 47772, "start": 488.6, "end": 494.6, "text": " a normal random number distribution in PyTorch. That's what randn does. And we can hit shift", "tokens": [257, 2710, 4974, 1230, 7316, 294, 9953, 51, 284, 339, 13, 663, 311, 437, 367, 474, 77, 775, 13, 400, 321, 393, 2045, 5513], "temperature": 0.0, "avg_logprob": -0.18223624033470676, "compression_ratio": 1.4725274725274726, "no_speech_prob": 2.026140691668843e-06}, {"id": 69, "seek": 47772, "start": 494.6, "end": 506.04, "text": " tab to see how that works. Okay. And it says here that it's going to have a variance of", "tokens": [4421, 281, 536, 577, 300, 1985, 13, 1033, 13, 400, 309, 1619, 510, 300, 309, 311, 516, 281, 362, 257, 21977, 295], "temperature": 0.0, "avg_logprob": -0.18223624033470676, "compression_ratio": 1.4725274725274726, "no_speech_prob": 2.026140691668843e-06}, {"id": 70, "seek": 50604, "start": 506.04, "end": 510.92, "text": " one. So I probably shouldn't call this standard deviation. I probably should call this variance", "tokens": [472, 13, 407, 286, 1391, 4659, 380, 818, 341, 3832, 25163, 13, 286, 1391, 820, 818, 341, 21977], "temperature": 0.0, "avg_logprob": -0.11295404888334729, "compression_ratio": 1.754863813229572, "no_speech_prob": 3.576355993573088e-07}, {"id": 71, "seek": 50604, "start": 510.92, "end": 517.32, "text": " actually. So multiply it by the variance to change its variance to whatever is requested,", "tokens": [767, 13, 407, 12972, 309, 538, 264, 21977, 281, 1319, 1080, 21977, 281, 2035, 307, 16436, 11], "temperature": 0.0, "avg_logprob": -0.11295404888334729, "compression_ratio": 1.754863813229572, "no_speech_prob": 3.576355993573088e-07}, {"id": 72, "seek": 50604, "start": 517.32, "end": 523.2, "text": " which will default to one. And then as we talked about when it comes to calculating", "tokens": [597, 486, 7576, 281, 472, 13, 400, 550, 382, 321, 2825, 466, 562, 309, 1487, 281, 28258], "temperature": 0.0, "avg_logprob": -0.11295404888334729, "compression_ratio": 1.754863813229572, "no_speech_prob": 3.576355993573088e-07}, {"id": 73, "seek": 50604, "start": 523.2, "end": 529.16, "text": " our gradients, we have to tell PyTorch which things we want gradients for. And the way", "tokens": [527, 2771, 2448, 11, 321, 362, 281, 980, 9953, 51, 284, 339, 597, 721, 321, 528, 2771, 2448, 337, 13, 400, 264, 636], "temperature": 0.0, "avg_logprob": -0.11295404888334729, "compression_ratio": 1.754863813229572, "no_speech_prob": 3.576355993573088e-07}, {"id": 74, "seek": 50604, "start": 529.16, "end": 534.5600000000001, "text": " we do that is requires grad underscore. Remember this underscore at the end is a special magic", "tokens": [321, 360, 300, 307, 7029, 2771, 37556, 13, 5459, 341, 37556, 412, 264, 917, 307, 257, 2121, 5585], "temperature": 0.0, "avg_logprob": -0.11295404888334729, "compression_ratio": 1.754863813229572, "no_speech_prob": 3.576355993573088e-07}, {"id": 75, "seek": 53456, "start": 534.56, "end": 540.04, "text": " symbol which tells Python, tells PyTorch that we want this function to actually change the", "tokens": [5986, 597, 5112, 15329, 11, 5112, 9953, 51, 284, 339, 300, 321, 528, 341, 2445, 281, 767, 1319, 264], "temperature": 0.0, "avg_logprob": -0.169507068136464, "compression_ratio": 1.5941176470588236, "no_speech_prob": 2.8291319154050143e-07}, {"id": 76, "seek": 53456, "start": 540.04, "end": 549.8399999999999, "text": " thing that it's referring to. So this will change this tensor such that it requires gradients.", "tokens": [551, 300, 309, 311, 13761, 281, 13, 407, 341, 486, 1319, 341, 40863, 1270, 300, 309, 7029, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.169507068136464, "compression_ratio": 1.5941176470588236, "no_speech_prob": 2.8291319154050143e-07}, {"id": 77, "seek": 53456, "start": 549.8399999999999, "end": 557.7199999999999, "text": " So here's some weights. So our weights are going to need to be 28 by 28 by one shape,", "tokens": [407, 510, 311, 512, 17443, 13, 407, 527, 17443, 366, 516, 281, 643, 281, 312, 7562, 538, 7562, 538, 472, 3909, 11], "temperature": 0.0, "avg_logprob": -0.169507068136464, "compression_ratio": 1.5941176470588236, "no_speech_prob": 2.8291319154050143e-07}, {"id": 78, "seek": 55772, "start": 557.72, "end": 564.6800000000001, "text": " 28 by 28 because every pixel is going to need a weight and then one because we're going", "tokens": [7562, 538, 7562, 570, 633, 19261, 307, 516, 281, 643, 257, 3364, 293, 550, 472, 570, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.1166290555681501, "compression_ratio": 1.7623762376237624, "no_speech_prob": 6.786729613850184e-07}, {"id": 79, "seek": 55772, "start": 564.6800000000001, "end": 569.72, "text": " to need again, we're going to need to have that, that, that unit access to make it into", "tokens": [281, 643, 797, 11, 321, 434, 516, 281, 643, 281, 362, 300, 11, 300, 11, 300, 4985, 2105, 281, 652, 309, 666], "temperature": 0.0, "avg_logprob": -0.1166290555681501, "compression_ratio": 1.7623762376237624, "no_speech_prob": 6.786729613850184e-07}, {"id": 80, "seek": 55772, "start": 569.72, "end": 580.4200000000001, "text": " a column. So that's what PyTorch expects. So there's our weights. Now just weights by", "tokens": [257, 7738, 13, 407, 300, 311, 437, 9953, 51, 284, 339, 33280, 13, 407, 456, 311, 527, 17443, 13, 823, 445, 17443, 538], "temperature": 0.0, "avg_logprob": -0.1166290555681501, "compression_ratio": 1.7623762376237624, "no_speech_prob": 6.786729613850184e-07}, {"id": 81, "seek": 55772, "start": 580.4200000000001, "end": 585.96, "text": " pixels actually isn't going to be enough because weights by pixels were always equal zero when", "tokens": [18668, 767, 1943, 380, 516, 281, 312, 1547, 570, 17443, 538, 18668, 645, 1009, 2681, 4018, 562], "temperature": 0.0, "avg_logprob": -0.1166290555681501, "compression_ratio": 1.7623762376237624, "no_speech_prob": 6.786729613850184e-07}, {"id": 82, "seek": 58596, "start": 585.96, "end": 589.96, "text": " the pixels are equal to zero. It has a zero intercept. So we really want something where", "tokens": [264, 18668, 366, 2681, 281, 4018, 13, 467, 575, 257, 4018, 24700, 13, 407, 321, 534, 528, 746, 689], "temperature": 0.0, "avg_logprob": -0.16250911125769982, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.107193720097712e-07}, {"id": 83, "seek": 58596, "start": 589.96, "end": 597.0, "text": " it's like wx plus b a line. So the b is we call the bias and so that's just going to", "tokens": [309, 311, 411, 261, 87, 1804, 272, 257, 1622, 13, 407, 264, 272, 307, 321, 818, 264, 12577, 293, 370, 300, 311, 445, 516, 281], "temperature": 0.0, "avg_logprob": -0.16250911125769982, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.107193720097712e-07}, {"id": 84, "seek": 58596, "start": 597.0, "end": 604.4000000000001, "text": " be a single number. So let's grab a single number for our bias. So remember I told you", "tokens": [312, 257, 2167, 1230, 13, 407, 718, 311, 4444, 257, 2167, 1230, 337, 527, 12577, 13, 407, 1604, 286, 1907, 291], "temperature": 0.0, "avg_logprob": -0.16250911125769982, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.107193720097712e-07}, {"id": 85, "seek": 58596, "start": 604.4000000000001, "end": 608.6, "text": " there's a difference between the parameters and weights, sexually speaking. So here the", "tokens": [456, 311, 257, 2649, 1296, 264, 9834, 293, 17443, 11, 26791, 4124, 13, 407, 510, 264], "temperature": 0.0, "avg_logprob": -0.16250911125769982, "compression_ratio": 1.6415094339622642, "no_speech_prob": 3.107193720097712e-07}, {"id": 86, "seek": 60860, "start": 608.6, "end": 617.0400000000001, "text": " weights are the w in this equation, the bias is b in this equation and the weights and", "tokens": [17443, 366, 264, 261, 294, 341, 5367, 11, 264, 12577, 307, 272, 294, 341, 5367, 293, 264, 17443, 293], "temperature": 0.0, "avg_logprob": -0.10081224762991572, "compression_ratio": 2.0, "no_speech_prob": 7.856216655000026e-08}, {"id": 87, "seek": 60860, "start": 617.0400000000001, "end": 622.44, "text": " bias together is the parameters of the function. They're all the things that we're going to", "tokens": [12577, 1214, 307, 264, 9834, 295, 264, 2445, 13, 814, 434, 439, 264, 721, 300, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.10081224762991572, "compression_ratio": 2.0, "no_speech_prob": 7.856216655000026e-08}, {"id": 88, "seek": 60860, "start": 622.44, "end": 627.32, "text": " change. They're all the things that have gradients that we're going to update. So there's an", "tokens": [1319, 13, 814, 434, 439, 264, 721, 300, 362, 2771, 2448, 300, 321, 434, 516, 281, 5623, 13, 407, 456, 311, 364], "temperature": 0.0, "avg_logprob": -0.10081224762991572, "compression_ratio": 2.0, "no_speech_prob": 7.856216655000026e-08}, {"id": 89, "seek": 60860, "start": 627.32, "end": 634.28, "text": " important bit of jargon for you. The weights and biases of the model are the parameters.", "tokens": [1021, 857, 295, 15181, 10660, 337, 291, 13, 440, 17443, 293, 32152, 295, 264, 2316, 366, 264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.10081224762991572, "compression_ratio": 2.0, "no_speech_prob": 7.856216655000026e-08}, {"id": 90, "seek": 63428, "start": 634.28, "end": 643.12, "text": " So we can, yes question. What's the difference between gradient descent and stochastic gradient", "tokens": [407, 321, 393, 11, 2086, 1168, 13, 708, 311, 264, 2649, 1296, 16235, 23475, 293, 342, 8997, 2750, 16235], "temperature": 0.0, "avg_logprob": -0.12525946370671304, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.8738712697086157e-06}, {"id": 91, "seek": 63428, "start": 643.12, "end": 649.4399999999999, "text": " descent? So far we've only done gradient descent. We'll be doing stochastic gradient descent", "tokens": [23475, 30, 407, 1400, 321, 600, 787, 1096, 16235, 23475, 13, 492, 603, 312, 884, 342, 8997, 2750, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.12525946370671304, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.8738712697086157e-06}, {"id": 92, "seek": 63428, "start": 649.4399999999999, "end": 656.06, "text": " in a few minutes. So we can now create a calculated prediction for one image. So we can take an", "tokens": [294, 257, 1326, 2077, 13, 407, 321, 393, 586, 1884, 257, 15598, 17630, 337, 472, 3256, 13, 407, 321, 393, 747, 364], "temperature": 0.0, "avg_logprob": -0.12525946370671304, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.8738712697086157e-06}, {"id": 93, "seek": 63428, "start": 656.06, "end": 661.5, "text": " image such as the first one and multiply by the weights. We need to transpose them to", "tokens": [3256, 1270, 382, 264, 700, 472, 293, 12972, 538, 264, 17443, 13, 492, 643, 281, 25167, 552, 281], "temperature": 0.0, "avg_logprob": -0.12525946370671304, "compression_ratio": 1.7535545023696681, "no_speech_prob": 1.8738712697086157e-06}, {"id": 94, "seek": 66150, "start": 661.5, "end": 667.12, "text": " make them line up in terms of the rows and columns and add it up and add the bias and", "tokens": [652, 552, 1622, 493, 294, 2115, 295, 264, 13241, 293, 13766, 293, 909, 309, 493, 293, 909, 264, 12577, 293], "temperature": 0.0, "avg_logprob": -0.08357242871356267, "compression_ratio": 1.6587677725118484, "no_speech_prob": 1.7880595351016382e-06}, {"id": 95, "seek": 66150, "start": 667.12, "end": 675.8, "text": " there is a prediction. We want to do that for every image. We could do that with a for", "tokens": [456, 307, 257, 17630, 13, 492, 528, 281, 360, 300, 337, 633, 3256, 13, 492, 727, 360, 300, 365, 257, 337], "temperature": 0.0, "avg_logprob": -0.08357242871356267, "compression_ratio": 1.6587677725118484, "no_speech_prob": 1.7880595351016382e-06}, {"id": 96, "seek": 66150, "start": 675.8, "end": 681.78, "text": " loop and that would be really, really slow. It wouldn't run on the GPU and it wouldn't", "tokens": [6367, 293, 300, 576, 312, 534, 11, 534, 2964, 13, 467, 2759, 380, 1190, 322, 264, 18407, 293, 309, 2759, 380], "temperature": 0.0, "avg_logprob": -0.08357242871356267, "compression_ratio": 1.6587677725118484, "no_speech_prob": 1.7880595351016382e-06}, {"id": 97, "seek": 66150, "start": 681.78, "end": 687.8, "text": " run in optimized C code. So we actually want to use always to do kind of like looping over", "tokens": [1190, 294, 26941, 383, 3089, 13, 407, 321, 767, 528, 281, 764, 1009, 281, 360, 733, 295, 411, 6367, 278, 670], "temperature": 0.0, "avg_logprob": -0.08357242871356267, "compression_ratio": 1.6587677725118484, "no_speech_prob": 1.7880595351016382e-06}, {"id": 98, "seek": 68780, "start": 687.8, "end": 692.68, "text": " pixels, looping over images. You always need to try to make sure you're doing that without", "tokens": [18668, 11, 6367, 278, 670, 5267, 13, 509, 1009, 643, 281, 853, 281, 652, 988, 291, 434, 884, 300, 1553], "temperature": 0.0, "avg_logprob": -0.12400742234854863, "compression_ratio": 1.6079295154185023, "no_speech_prob": 8.851553729982697e-07}, {"id": 99, "seek": 68780, "start": 692.68, "end": 700.76, "text": " a Python for loop. In this case, doing this calculation for lots of rows and columns is", "tokens": [257, 15329, 337, 6367, 13, 682, 341, 1389, 11, 884, 341, 17108, 337, 3195, 295, 13241, 293, 13766, 307], "temperature": 0.0, "avg_logprob": -0.12400742234854863, "compression_ratio": 1.6079295154185023, "no_speech_prob": 8.851553729982697e-07}, {"id": 100, "seek": 68780, "start": 700.76, "end": 709.4399999999999, "text": " a mathematical operation called matrix monoplay. So if you've forgotten your matrix multiplication", "tokens": [257, 18894, 6916, 1219, 8141, 1108, 404, 8376, 13, 407, 498, 291, 600, 11832, 428, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.12400742234854863, "compression_ratio": 1.6079295154185023, "no_speech_prob": 8.851553729982697e-07}, {"id": 101, "seek": 68780, "start": 709.4399999999999, "end": 714.56, "text": " or maybe never quite got around to it at high school, it would be a good idea to have a", "tokens": [420, 1310, 1128, 1596, 658, 926, 281, 309, 412, 1090, 1395, 11, 309, 576, 312, 257, 665, 1558, 281, 362, 257], "temperature": 0.0, "avg_logprob": -0.12400742234854863, "compression_ratio": 1.6079295154185023, "no_speech_prob": 8.851553729982697e-07}, {"id": 102, "seek": 71456, "start": 714.56, "end": 720.28, "text": " look at Khan Academy or something to learn about what it is, but it's actually, I'll", "tokens": [574, 412, 18136, 11735, 420, 746, 281, 1466, 466, 437, 309, 307, 11, 457, 309, 311, 767, 11, 286, 603], "temperature": 0.0, "avg_logprob": -0.18116762261641653, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.5294115200958913e-06}, {"id": 103, "seek": 71456, "start": 720.28, "end": 728.2399999999999, "text": " give you the quick answer. This is from Wikipedia. If these are two matrices A and B, then this", "tokens": [976, 291, 264, 1702, 1867, 13, 639, 307, 490, 28999, 13, 759, 613, 366, 732, 32284, 316, 293, 363, 11, 550, 341], "temperature": 0.0, "avg_logprob": -0.18116762261641653, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.5294115200958913e-06}, {"id": 104, "seek": 71456, "start": 728.2399999999999, "end": 735.52, "text": " element here, 1, 2 in the output is going to be equal to the first bit here times the", "tokens": [4478, 510, 11, 502, 11, 568, 294, 264, 5598, 307, 516, 281, 312, 2681, 281, 264, 700, 857, 510, 1413, 264], "temperature": 0.0, "avg_logprob": -0.18116762261641653, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.5294115200958913e-06}, {"id": 105, "seek": 71456, "start": 735.52, "end": 742.0799999999999, "text": " first bit here plus the second bit here times the second bit here. So it's going to be B12", "tokens": [700, 857, 510, 1804, 264, 1150, 857, 510, 1413, 264, 1150, 857, 510, 13, 407, 309, 311, 516, 281, 312, 363, 4762], "temperature": 0.0, "avg_logprob": -0.18116762261641653, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.5294115200958913e-06}, {"id": 106, "seek": 74208, "start": 742.08, "end": 751.36, "text": " times A11 plus B22 times A12. That's you can see the orange matches the orange. Ditto for", "tokens": [1413, 316, 5348, 1804, 363, 7490, 1413, 316, 4762, 13, 663, 311, 291, 393, 536, 264, 7671, 10676, 264, 7671, 13, 413, 34924, 337], "temperature": 0.0, "avg_logprob": -0.14047502588342736, "compression_ratio": 1.432, "no_speech_prob": 1.3497003692464205e-06}, {"id": 107, "seek": 74208, "start": 751.36, "end": 758.8000000000001, "text": " over here. This would be equal to B13 times A31 plus B23 times A32 and so forth for every", "tokens": [670, 510, 13, 639, 576, 312, 2681, 281, 363, 7668, 1413, 316, 12967, 1804, 363, 9356, 1413, 316, 11440, 293, 370, 5220, 337, 633], "temperature": 0.0, "avg_logprob": -0.14047502588342736, "compression_ratio": 1.432, "no_speech_prob": 1.3497003692464205e-06}, {"id": 108, "seek": 75880, "start": 758.8, "end": 774.52, "text": " part. Here's a great picture of that in action. If you look at matrix multiplication.xyz,", "tokens": [644, 13, 1692, 311, 257, 869, 3036, 295, 300, 294, 3069, 13, 759, 291, 574, 412, 8141, 27290, 13, 12876, 89, 11], "temperature": 0.0, "avg_logprob": -0.13948416709899902, "compression_ratio": 1.685897435897436, "no_speech_prob": 7.11241966655507e-07}, {"id": 109, "seek": 75880, "start": 774.52, "end": 780.64, "text": " another way to think of it is we can kind of flip the second bit over on top and then", "tokens": [1071, 636, 281, 519, 295, 309, 307, 321, 393, 733, 295, 7929, 264, 1150, 857, 670, 322, 1192, 293, 550], "temperature": 0.0, "avg_logprob": -0.13948416709899902, "compression_ratio": 1.685897435897436, "no_speech_prob": 7.11241966655507e-07}, {"id": 110, "seek": 75880, "start": 780.64, "end": 785.52, "text": " multiply each bit together and add them up, multiply each bit together and add them up,", "tokens": [12972, 1184, 857, 1214, 293, 909, 552, 493, 11, 12972, 1184, 857, 1214, 293, 909, 552, 493, 11], "temperature": 0.0, "avg_logprob": -0.13948416709899902, "compression_ratio": 1.685897435897436, "no_speech_prob": 7.11241966655507e-07}, {"id": 111, "seek": 78552, "start": 785.52, "end": 789.04, "text": " and you can see always the second one here and ends up in the second spot and the first", "tokens": [293, 291, 393, 536, 1009, 264, 1150, 472, 510, 293, 5314, 493, 294, 264, 1150, 4008, 293, 264, 700], "temperature": 0.0, "avg_logprob": -0.13021957513057825, "compression_ratio": 1.7151898734177216, "no_speech_prob": 7.338199452533445e-07}, {"id": 112, "seek": 78552, "start": 789.04, "end": 800.68, "text": " one ends up in the first spot. And that's what matrix multiplication is. So we can do", "tokens": [472, 5314, 493, 294, 264, 700, 4008, 13, 400, 300, 311, 437, 8141, 27290, 307, 13, 407, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.13021957513057825, "compression_ratio": 1.7151898734177216, "no_speech_prob": 7.338199452533445e-07}, {"id": 113, "seek": 78552, "start": 800.68, "end": 810.28, "text": " our multiply and add up by using matrix multiplication and in Python and therefore PyTorch matrix", "tokens": [527, 12972, 293, 909, 493, 538, 1228, 8141, 27290, 293, 294, 15329, 293, 4412, 9953, 51, 284, 339, 8141], "temperature": 0.0, "avg_logprob": -0.13021957513057825, "compression_ratio": 1.7151898734177216, "no_speech_prob": 7.338199452533445e-07}, {"id": 114, "seek": 81028, "start": 810.28, "end": 818.16, "text": " multiplication is the at sign operator. So when you see at that means matrix multiply.", "tokens": [27290, 307, 264, 412, 1465, 12973, 13, 407, 562, 291, 536, 412, 300, 1355, 8141, 12972, 13], "temperature": 0.0, "avg_logprob": -0.11367466714647081, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.4367476524057565e-06}, {"id": 115, "seek": 81028, "start": 818.16, "end": 830.76, "text": " So here is our 20.2336. If I do a matrix multiply of our training set by our weights and then", "tokens": [407, 510, 307, 527, 945, 13, 9356, 11309, 13, 759, 286, 360, 257, 8141, 12972, 295, 527, 3097, 992, 538, 527, 17443, 293, 550], "temperature": 0.0, "avg_logprob": -0.11367466714647081, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.4367476524057565e-06}, {"id": 116, "seek": 81028, "start": 830.76, "end": 836.0799999999999, "text": " we add the bias and here is our 20.336 for the first one and you can see though it's", "tokens": [321, 909, 264, 12577, 293, 510, 307, 527, 945, 13, 10191, 21, 337, 264, 700, 472, 293, 291, 393, 536, 1673, 309, 311], "temperature": 0.0, "avg_logprob": -0.11367466714647081, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.4367476524057565e-06}, {"id": 117, "seek": 83608, "start": 836.08, "end": 842.96, "text": " doing every single one. So that's really important is that matrix multiplication gives us an", "tokens": [884, 633, 2167, 472, 13, 407, 300, 311, 534, 1021, 307, 300, 8141, 27290, 2709, 505, 364], "temperature": 0.0, "avg_logprob": -0.12363093073775129, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.089483698204276e-06}, {"id": 118, "seek": 83608, "start": 842.96, "end": 848.8000000000001, "text": " optimized way to do these simple linear functions for as many kind of rows and columns as we", "tokens": [26941, 636, 281, 360, 613, 2199, 8213, 6828, 337, 382, 867, 733, 295, 13241, 293, 13766, 382, 321], "temperature": 0.0, "avg_logprob": -0.12363093073775129, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.089483698204276e-06}, {"id": 119, "seek": 83608, "start": 848.8000000000001, "end": 858.5200000000001, "text": " want. So this is one of the two fundamental equations of any neural network. Some rows", "tokens": [528, 13, 407, 341, 307, 472, 295, 264, 732, 8088, 11787, 295, 604, 18161, 3209, 13, 2188, 13241], "temperature": 0.0, "avg_logprob": -0.12363093073775129, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.089483698204276e-06}, {"id": 120, "seek": 83608, "start": 858.5200000000001, "end": 864.0, "text": " of data, rows and columns of data, matrix multiply, some weights, add some bias and", "tokens": [295, 1412, 11, 13241, 293, 13766, 295, 1412, 11, 8141, 12972, 11, 512, 17443, 11, 909, 512, 12577, 293], "temperature": 0.0, "avg_logprob": -0.12363093073775129, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.089483698204276e-06}, {"id": 121, "seek": 86400, "start": 864.0, "end": 871.76, "text": " the second one which we'll see in a moment is an activation function. So that is some", "tokens": [264, 1150, 472, 597, 321, 603, 536, 294, 257, 1623, 307, 364, 24433, 2445, 13, 407, 300, 307, 512], "temperature": 0.0, "avg_logprob": -0.1045915058680943, "compression_ratio": 1.7908163265306123, "no_speech_prob": 3.089483698204276e-06}, {"id": 122, "seek": 86400, "start": 871.76, "end": 876.88, "text": " predictions from our randomly initialized model. So we can check how good our model", "tokens": [21264, 490, 527, 16979, 5883, 1602, 2316, 13, 407, 321, 393, 1520, 577, 665, 527, 2316], "temperature": 0.0, "avg_logprob": -0.1045915058680943, "compression_ratio": 1.7908163265306123, "no_speech_prob": 3.089483698204276e-06}, {"id": 123, "seek": 86400, "start": 876.88, "end": 885.96, "text": " is and so to do that we can decide that anything greater than zero we will call a three and", "tokens": [307, 293, 370, 281, 360, 300, 321, 393, 4536, 300, 1340, 5044, 813, 4018, 321, 486, 818, 257, 1045, 293], "temperature": 0.0, "avg_logprob": -0.1045915058680943, "compression_ratio": 1.7908163265306123, "no_speech_prob": 3.089483698204276e-06}, {"id": 124, "seek": 86400, "start": 885.96, "end": 893.76, "text": " anything less than zero we will call a seven. So preds greater than zero tells us whether", "tokens": [1340, 1570, 813, 4018, 321, 486, 818, 257, 3407, 13, 407, 3852, 82, 5044, 813, 4018, 5112, 505, 1968], "temperature": 0.0, "avg_logprob": -0.1045915058680943, "compression_ratio": 1.7908163265306123, "no_speech_prob": 3.089483698204276e-06}, {"id": 125, "seek": 89376, "start": 893.76, "end": 898.96, "text": " or not something is predicted to be a three or not. Then turn that into a float so rather", "tokens": [420, 406, 746, 307, 19147, 281, 312, 257, 1045, 420, 406, 13, 1396, 1261, 300, 666, 257, 15706, 370, 2831], "temperature": 0.0, "avg_logprob": -0.11669832009535569, "compression_ratio": 1.8429752066115703, "no_speech_prob": 1.4144744682198507e-06}, {"id": 126, "seek": 89376, "start": 898.96, "end": 903.48, "text": " than true and false make it one and zero because that's what our training set contains and", "tokens": [813, 2074, 293, 7908, 652, 309, 472, 293, 4018, 570, 300, 311, 437, 527, 3097, 992, 8306, 293], "temperature": 0.0, "avg_logprob": -0.11669832009535569, "compression_ratio": 1.8429752066115703, "no_speech_prob": 1.4144744682198507e-06}, {"id": 127, "seek": 89376, "start": 903.48, "end": 912.28, "text": " then check with our thresholded predictions are equal to our training set and this will", "tokens": [550, 1520, 365, 527, 14678, 292, 21264, 366, 2681, 281, 527, 3097, 992, 293, 341, 486], "temperature": 0.0, "avg_logprob": -0.11669832009535569, "compression_ratio": 1.8429752066115703, "no_speech_prob": 1.4144744682198507e-06}, {"id": 128, "seek": 89376, "start": 912.28, "end": 919.4399999999999, "text": " return true every time a row is correctly predicted and false otherwise. So if we take", "tokens": [2736, 2074, 633, 565, 257, 5386, 307, 8944, 19147, 293, 7908, 5911, 13, 407, 498, 321, 747], "temperature": 0.0, "avg_logprob": -0.11669832009535569, "compression_ratio": 1.8429752066115703, "no_speech_prob": 1.4144744682198507e-06}, {"id": 129, "seek": 89376, "start": 919.4399999999999, "end": 923.36, "text": " all those trues and falses and turn them into floats so that'll be ones and zeros and then", "tokens": [439, 729, 504, 1247, 293, 16720, 279, 293, 1261, 552, 666, 37878, 370, 300, 603, 312, 2306, 293, 35193, 293, 550], "temperature": 0.0, "avg_logprob": -0.11669832009535569, "compression_ratio": 1.8429752066115703, "no_speech_prob": 1.4144744682198507e-06}, {"id": 130, "seek": 92336, "start": 923.36, "end": 930.2, "text": " take their mean it's 0.49. So not surprisingly our randomly initialized model is right about", "tokens": [747, 641, 914, 309, 311, 1958, 13, 14938, 13, 407, 406, 17600, 527, 16979, 5883, 1602, 2316, 307, 558, 466], "temperature": 0.0, "avg_logprob": -0.14757134119669596, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.5779594377818285e-06}, {"id": 131, "seek": 92336, "start": 930.2, "end": 936.88, "text": " half the time at predicting threes from sevens. I had one more method here which is dot item", "tokens": [1922, 264, 565, 412, 32884, 258, 4856, 490, 3407, 82, 13, 286, 632, 472, 544, 3170, 510, 597, 307, 5893, 3174], "temperature": 0.0, "avg_logprob": -0.14757134119669596, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.5779594377818285e-06}, {"id": 132, "seek": 92336, "start": 936.88, "end": 944.32, "text": " without dot item this would return a tensor it's a rank zero tensor it has no rows it", "tokens": [1553, 5893, 3174, 341, 576, 2736, 257, 40863, 309, 311, 257, 6181, 4018, 40863, 309, 575, 572, 13241, 309], "temperature": 0.0, "avg_logprob": -0.14757134119669596, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.5779594377818285e-06}, {"id": 133, "seek": 92336, "start": 944.32, "end": 950.5, "text": " has no columns it just it's just a number on its own but I actually wanted to unwrap", "tokens": [575, 572, 13766, 309, 445, 309, 311, 445, 257, 1230, 322, 1080, 1065, 457, 286, 767, 1415, 281, 14853, 4007], "temperature": 0.0, "avg_logprob": -0.14757134119669596, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.5779594377818285e-06}, {"id": 134, "seek": 95050, "start": 950.5, "end": 954.4, "text": " it to create a normal Python scalar mainly just because I wanted to see the easily see", "tokens": [309, 281, 1884, 257, 2710, 15329, 39684, 8704, 445, 570, 286, 1415, 281, 536, 264, 3612, 536], "temperature": 0.0, "avg_logprob": -0.07636163974630422, "compression_ratio": 1.5822222222222222, "no_speech_prob": 8.579224299865018e-07}, {"id": 135, "seek": 95050, "start": 954.4, "end": 959.36, "text": " the full set of decimal places and the reason for that is I want to show you how we're going", "tokens": [264, 1577, 992, 295, 26601, 3190, 293, 264, 1778, 337, 300, 307, 286, 528, 281, 855, 291, 577, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.07636163974630422, "compression_ratio": 1.5822222222222222, "no_speech_prob": 8.579224299865018e-07}, {"id": 136, "seek": 95050, "start": 959.36, "end": 967.24, "text": " to calculate the derivative on the accuracy by changing a parameter by a tiny bit. So", "tokens": [281, 8873, 264, 13760, 322, 264, 14170, 538, 4473, 257, 13075, 538, 257, 5870, 857, 13, 407], "temperature": 0.0, "avg_logprob": -0.07636163974630422, "compression_ratio": 1.5822222222222222, "no_speech_prob": 8.579224299865018e-07}, {"id": 137, "seek": 95050, "start": 967.24, "end": 977.0, "text": " let's take one parameter which will be weight zero and multiply it by 1.0001 and so that's", "tokens": [718, 311, 747, 472, 13075, 597, 486, 312, 3364, 4018, 293, 12972, 309, 538, 502, 13, 1360, 16, 293, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.07636163974630422, "compression_ratio": 1.5822222222222222, "no_speech_prob": 8.579224299865018e-07}, {"id": 138, "seek": 97700, "start": 977.0, "end": 985.86, "text": " going to make it a little bit bigger and then if I calculate how the the accuracy changes", "tokens": [516, 281, 652, 309, 257, 707, 857, 3801, 293, 550, 498, 286, 8873, 577, 264, 264, 14170, 2962], "temperature": 0.0, "avg_logprob": -0.06177311284201486, "compression_ratio": 1.7990196078431373, "no_speech_prob": 2.742088724971836e-07}, {"id": 139, "seek": 97700, "start": 985.86, "end": 992.6, "text": " based on the change in that weight that will be the gradient of the accuracy with respect", "tokens": [2361, 322, 264, 1319, 294, 300, 3364, 300, 486, 312, 264, 16235, 295, 264, 14170, 365, 3104], "temperature": 0.0, "avg_logprob": -0.06177311284201486, "compression_ratio": 1.7990196078431373, "no_speech_prob": 2.742088724971836e-07}, {"id": 140, "seek": 97700, "start": 992.6, "end": 998.76, "text": " to that parameter. So I can do that by calculating my new set of predictions and then I can threshold", "tokens": [281, 300, 13075, 13, 407, 286, 393, 360, 300, 538, 28258, 452, 777, 992, 295, 21264, 293, 550, 286, 393, 14678], "temperature": 0.0, "avg_logprob": -0.06177311284201486, "compression_ratio": 1.7990196078431373, "no_speech_prob": 2.742088724971836e-07}, {"id": 141, "seek": 97700, "start": 998.76, "end": 1002.6, "text": " them and then I can check whether they're equal to the training set and then take the", "tokens": [552, 293, 550, 286, 393, 1520, 1968, 436, 434, 2681, 281, 264, 3097, 992, 293, 550, 747, 264], "temperature": 0.0, "avg_logprob": -0.06177311284201486, "compression_ratio": 1.7990196078431373, "no_speech_prob": 2.742088724971836e-07}, {"id": 142, "seek": 100260, "start": 1002.6, "end": 1016.16, "text": " mean and I get back exactly the same number. So remember that gradient is equal to rise", "tokens": [914, 293, 286, 483, 646, 2293, 264, 912, 1230, 13, 407, 1604, 300, 16235, 307, 2681, 281, 6272], "temperature": 0.0, "avg_logprob": -0.09336414188146591, "compression_ratio": 1.52, "no_speech_prob": 3.866963993459649e-07}, {"id": 143, "seek": 100260, "start": 1016.16, "end": 1020.44, "text": " over run if you remember back to your calculus or if you'd forgotten your calculus hopefully", "tokens": [670, 1190, 498, 291, 1604, 646, 281, 428, 33400, 420, 498, 291, 1116, 11832, 428, 33400, 4696], "temperature": 0.0, "avg_logprob": -0.09336414188146591, "compression_ratio": 1.52, "no_speech_prob": 3.866963993459649e-07}, {"id": 144, "seek": 100260, "start": 1020.44, "end": 1029.88, "text": " you've reviewed it on Khan Academy. So the change in the y so y new minus y old which", "tokens": [291, 600, 18429, 309, 322, 18136, 11735, 13, 407, 264, 1319, 294, 264, 288, 370, 288, 777, 3175, 288, 1331, 597], "temperature": 0.0, "avg_logprob": -0.09336414188146591, "compression_ratio": 1.52, "no_speech_prob": 3.866963993459649e-07}, {"id": 145, "seek": 102988, "start": 1029.88, "end": 1043.5200000000002, "text": " is 0.4912 etc minus 0.4912 etc which is 0 divided by this change will give us 0. So", "tokens": [307, 1958, 13, 14938, 4762, 5183, 3175, 1958, 13, 14938, 4762, 5183, 597, 307, 1958, 6666, 538, 341, 1319, 486, 976, 505, 1958, 13, 407], "temperature": 0.0, "avg_logprob": -0.16364961511948528, "compression_ratio": 1.4146341463414633, "no_speech_prob": 4.888292437499331e-07}, {"id": 146, "seek": 102988, "start": 1043.5200000000002, "end": 1050.88, "text": " at this point we have a problem our derivative is 0 so we have 0 gradients which means our", "tokens": [412, 341, 935, 321, 362, 257, 1154, 527, 13760, 307, 1958, 370, 321, 362, 1958, 2771, 2448, 597, 1355, 527], "temperature": 0.0, "avg_logprob": -0.16364961511948528, "compression_ratio": 1.4146341463414633, "no_speech_prob": 4.888292437499331e-07}, {"id": 147, "seek": 105088, "start": 1050.88, "end": 1061.92, "text": " step will be 0 which means our prediction will be unchanged. Okay so we have a problem", "tokens": [1823, 486, 312, 1958, 597, 1355, 527, 17630, 486, 312, 44553, 13, 1033, 370, 321, 362, 257, 1154], "temperature": 0.0, "avg_logprob": -0.08555494035993304, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.05901977526446e-07}, {"id": 148, "seek": 105088, "start": 1061.92, "end": 1070.24, "text": " and our problem is that our gradient is 0 and with a gradient of 0 we can't take a step", "tokens": [293, 527, 1154, 307, 300, 527, 16235, 307, 1958, 293, 365, 257, 16235, 295, 1958, 321, 393, 380, 747, 257, 1823], "temperature": 0.0, "avg_logprob": -0.08555494035993304, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.05901977526446e-07}, {"id": 149, "seek": 105088, "start": 1070.24, "end": 1075.92, "text": " and we can't get better predictions and so intuitively speaking the reason that our gradient", "tokens": [293, 321, 393, 380, 483, 1101, 21264, 293, 370, 46506, 4124, 264, 1778, 300, 527, 16235], "temperature": 0.0, "avg_logprob": -0.08555494035993304, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.05901977526446e-07}, {"id": 150, "seek": 107592, "start": 1075.92, "end": 1084.5600000000002, "text": " is 0 is because when we change a single pixel by a tiny bit we might not ever in any way", "tokens": [307, 1958, 307, 570, 562, 321, 1319, 257, 2167, 19261, 538, 257, 5870, 857, 321, 1062, 406, 1562, 294, 604, 636], "temperature": 0.0, "avg_logprob": -0.08283520467353589, "compression_ratio": 1.5773809523809523, "no_speech_prob": 1.9333504042151617e-06}, {"id": 151, "seek": 107592, "start": 1084.5600000000002, "end": 1091.6000000000001, "text": " change an actual prediction to change from a 3 predicting a 3 to a 7 or vice versa because", "tokens": [1319, 364, 3539, 17630, 281, 1319, 490, 257, 805, 32884, 257, 805, 281, 257, 1614, 420, 11964, 25650, 570], "temperature": 0.0, "avg_logprob": -0.08283520467353589, "compression_ratio": 1.5773809523809523, "no_speech_prob": 1.9333504042151617e-06}, {"id": 152, "seek": 107592, "start": 1091.6000000000001, "end": 1102.72, "text": " we have this we have this threshold. Okay and so in other words our our accuracy loss", "tokens": [321, 362, 341, 321, 362, 341, 14678, 13, 1033, 293, 370, 294, 661, 2283, 527, 527, 14170, 4470], "temperature": 0.0, "avg_logprob": -0.08283520467353589, "compression_ratio": 1.5773809523809523, "no_speech_prob": 1.9333504042151617e-06}, {"id": 153, "seek": 110272, "start": 1102.72, "end": 1108.48, "text": " function here is is very bumpy it's like flat step flat step flat step so it's got this", "tokens": [2445, 510, 307, 307, 588, 49400, 309, 311, 411, 4962, 1823, 4962, 1823, 4962, 1823, 370, 309, 311, 658, 341], "temperature": 0.0, "avg_logprob": -0.12300102638475822, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.690368306612072e-07}, {"id": 154, "seek": 110272, "start": 1108.48, "end": 1116.4, "text": " this 0 gradient all over the place. So what we need to do is use something other than", "tokens": [341, 1958, 16235, 439, 670, 264, 1081, 13, 407, 437, 321, 643, 281, 360, 307, 764, 746, 661, 813], "temperature": 0.0, "avg_logprob": -0.12300102638475822, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.690368306612072e-07}, {"id": 155, "seek": 110272, "start": 1116.4, "end": 1126.16, "text": " accuracy as our loss function. So let's try and create a new function and what this new", "tokens": [14170, 382, 527, 4470, 2445, 13, 407, 718, 311, 853, 293, 1884, 257, 777, 2445, 293, 437, 341, 777], "temperature": 0.0, "avg_logprob": -0.12300102638475822, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.690368306612072e-07}, {"id": 156, "seek": 112616, "start": 1126.16, "end": 1134.72, "text": " function is going to do is it's going to give us a better value kind of in much the same", "tokens": [2445, 307, 516, 281, 360, 307, 309, 311, 516, 281, 976, 505, 257, 1101, 2158, 733, 295, 294, 709, 264, 912], "temperature": 0.0, "avg_logprob": -0.10804322145987248, "compression_ratio": 1.7115384615384615, "no_speech_prob": 4.381843439205113e-07}, {"id": 157, "seek": 112616, "start": 1134.72, "end": 1139.44, "text": " way that accuracy gives a better value. So this is the loss remember a small loss is", "tokens": [636, 300, 14170, 2709, 257, 1101, 2158, 13, 407, 341, 307, 264, 4470, 1604, 257, 1359, 4470, 307], "temperature": 0.0, "avg_logprob": -0.10804322145987248, "compression_ratio": 1.7115384615384615, "no_speech_prob": 4.381843439205113e-07}, {"id": 158, "seek": 112616, "start": 1139.44, "end": 1150.1200000000001, "text": " better so to give us a lower loss when the accuracy is better but it won't have a 0 gradient.", "tokens": [1101, 370, 281, 976, 505, 257, 3126, 4470, 562, 264, 14170, 307, 1101, 457, 309, 1582, 380, 362, 257, 1958, 16235, 13], "temperature": 0.0, "avg_logprob": -0.10804322145987248, "compression_ratio": 1.7115384615384615, "no_speech_prob": 4.381843439205113e-07}, {"id": 159, "seek": 115012, "start": 1150.12, "end": 1158.1599999999999, "text": " So that means that a slightly better prediction needs to have a slightly better loss. So let's", "tokens": [407, 300, 1355, 300, 257, 4748, 1101, 17630, 2203, 281, 362, 257, 4748, 1101, 4470, 13, 407, 718, 311], "temperature": 0.0, "avg_logprob": -0.1622278441244097, "compression_ratio": 1.5964912280701755, "no_speech_prob": 4.520935021901096e-07}, {"id": 160, "seek": 115012, "start": 1158.1599999999999, "end": 1165.6399999999999, "text": " have a look at an example let's say our targets our labels of like that is 3 oh there's just", "tokens": [362, 257, 574, 412, 364, 1365, 718, 311, 584, 527, 12911, 527, 16949, 295, 411, 300, 307, 805, 1954, 456, 311, 445], "temperature": 0.0, "avg_logprob": -0.1622278441244097, "compression_ratio": 1.5964912280701755, "no_speech_prob": 4.520935021901096e-07}, {"id": 161, "seek": 115012, "start": 1165.6399999999999, "end": 1172.56, "text": " three rows three images here 1 0 1 okay and we've made some predictions from a neural", "tokens": [1045, 13241, 1045, 5267, 510, 502, 1958, 502, 1392, 293, 321, 600, 1027, 512, 21264, 490, 257, 18161], "temperature": 0.0, "avg_logprob": -0.1622278441244097, "compression_ratio": 1.5964912280701755, "no_speech_prob": 4.520935021901096e-07}, {"id": 162, "seek": 117256, "start": 1172.56, "end": 1182.96, "text": " net and those predictions gave us 0.9 0.4 0.2. So now consider this loss function a loss", "tokens": [2533, 293, 729, 21264, 2729, 505, 1958, 13, 24, 1958, 13, 19, 1958, 13, 17, 13, 407, 586, 1949, 341, 4470, 2445, 257, 4470], "temperature": 0.0, "avg_logprob": -0.12505567577523244, "compression_ratio": 1.5862068965517242, "no_speech_prob": 7.571138667117339e-07}, {"id": 163, "seek": 117256, "start": 1182.96, "end": 1189.12, "text": " function we're going to use torch dot where which is basically the same as this list comprehension", "tokens": [2445, 321, 434, 516, 281, 764, 27822, 5893, 689, 597, 307, 1936, 264, 912, 382, 341, 1329, 44991], "temperature": 0.0, "avg_logprob": -0.12505567577523244, "compression_ratio": 1.5862068965517242, "no_speech_prob": 7.571138667117339e-07}, {"id": 164, "seek": 117256, "start": 1189.12, "end": 1196.76, "text": " it's basically an if statement. So it's going to say for for where target equals 1 we're", "tokens": [309, 311, 1936, 364, 498, 5629, 13, 407, 309, 311, 516, 281, 584, 337, 337, 689, 3779, 6915, 502, 321, 434], "temperature": 0.0, "avg_logprob": -0.12505567577523244, "compression_ratio": 1.5862068965517242, "no_speech_prob": 7.571138667117339e-07}, {"id": 165, "seek": 119676, "start": 1196.76, "end": 1203.04, "text": " going to return 1 minus predictions so here target is 1 so it'll be 1 minus 0.9 and where", "tokens": [516, 281, 2736, 502, 3175, 21264, 370, 510, 3779, 307, 502, 370, 309, 603, 312, 502, 3175, 1958, 13, 24, 293, 689], "temperature": 0.0, "avg_logprob": -0.12278471907524213, "compression_ratio": 1.8082191780821917, "no_speech_prob": 3.059018922613177e-07}, {"id": 166, "seek": 119676, "start": 1203.04, "end": 1211.8799999999999, "text": " target is not 1 it'll just be predictions. So for these examples here the first one target", "tokens": [3779, 307, 406, 502, 309, 603, 445, 312, 21264, 13, 407, 337, 613, 5110, 510, 264, 700, 472, 3779], "temperature": 0.0, "avg_logprob": -0.12278471907524213, "compression_ratio": 1.8082191780821917, "no_speech_prob": 3.059018922613177e-07}, {"id": 167, "seek": 119676, "start": 1211.8799999999999, "end": 1222.36, "text": " equals 1 will be 1 minus 0.9 just 0.1 the next one is target equals 0 so it'll best", "tokens": [6915, 502, 486, 312, 502, 3175, 1958, 13, 24, 445, 1958, 13, 16, 264, 958, 472, 307, 3779, 6915, 1958, 370, 309, 603, 1151], "temperature": 0.0, "avg_logprob": -0.12278471907524213, "compression_ratio": 1.8082191780821917, "no_speech_prob": 3.059018922613177e-07}, {"id": 168, "seek": 122236, "start": 1222.36, "end": 1228.9199999999998, "text": " be the prediction just 0.4 and then for the third one it's a 1 for target so it'll be", "tokens": [312, 264, 17630, 445, 1958, 13, 19, 293, 550, 337, 264, 2636, 472, 309, 311, 257, 502, 337, 3779, 370, 309, 603, 312], "temperature": 0.0, "avg_logprob": -0.10071770862866473, "compression_ratio": 1.7989690721649485, "no_speech_prob": 2.169170159049827e-07}, {"id": 169, "seek": 122236, "start": 1228.9199999999998, "end": 1236.9199999999998, "text": " 1 minus prediction which is 0.8 and so you can see here when the prediction is correct", "tokens": [502, 3175, 17630, 597, 307, 1958, 13, 23, 293, 370, 291, 393, 536, 510, 562, 264, 17630, 307, 3006], "temperature": 0.0, "avg_logprob": -0.10071770862866473, "compression_ratio": 1.7989690721649485, "no_speech_prob": 2.169170159049827e-07}, {"id": 170, "seek": 122236, "start": 1236.9199999999998, "end": 1241.84, "text": " correct in other words it's a num you know it's a high number when the target is 1 and", "tokens": [3006, 294, 661, 2283, 309, 311, 257, 1031, 291, 458, 309, 311, 257, 1090, 1230, 562, 264, 3779, 307, 502, 293], "temperature": 0.0, "avg_logprob": -0.10071770862866473, "compression_ratio": 1.7989690721649485, "no_speech_prob": 2.169170159049827e-07}, {"id": 171, "seek": 122236, "start": 1241.84, "end": 1249.12, "text": " a low number when the target is 0 these numbers are going to be smaller. So the worst one", "tokens": [257, 2295, 1230, 562, 264, 3779, 307, 1958, 613, 3547, 366, 516, 281, 312, 4356, 13, 407, 264, 5855, 472], "temperature": 0.0, "avg_logprob": -0.10071770862866473, "compression_ratio": 1.7989690721649485, "no_speech_prob": 2.169170159049827e-07}, {"id": 172, "seek": 124912, "start": 1249.12, "end": 1255.1599999999999, "text": " is when we predicted 0.2 so we're pretty we really thought that was actually a 0 but it's", "tokens": [307, 562, 321, 19147, 1958, 13, 17, 370, 321, 434, 1238, 321, 534, 1194, 300, 390, 767, 257, 1958, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.086717731074283, "compression_ratio": 1.587878787878788, "no_speech_prob": 6.276700332819019e-07}, {"id": 173, "seek": 124912, "start": 1255.1599999999999, "end": 1262.0, "text": " actually a 1 so we ended up with a 0.8 here because this is 1 minus prediction 1 minus", "tokens": [767, 257, 502, 370, 321, 4590, 493, 365, 257, 1958, 13, 23, 510, 570, 341, 307, 502, 3175, 17630, 502, 3175], "temperature": 0.0, "avg_logprob": -0.086717731074283, "compression_ratio": 1.587878787878788, "no_speech_prob": 6.276700332819019e-07}, {"id": 174, "seek": 124912, "start": 1262.0, "end": 1271.8799999999999, "text": " 0.2 is 0.8 so we can then take the mean of all of these to calculate a loss so if you", "tokens": [1958, 13, 17, 307, 1958, 13, 23, 370, 321, 393, 550, 747, 264, 914, 295, 439, 295, 613, 281, 8873, 257, 4470, 370, 498, 291], "temperature": 0.0, "avg_logprob": -0.086717731074283, "compression_ratio": 1.587878787878788, "no_speech_prob": 6.276700332819019e-07}, {"id": 175, "seek": 127188, "start": 1271.88, "end": 1281.0400000000002, "text": " think about it this loss will be the smallest if the predictions are exactly right so if", "tokens": [519, 466, 309, 341, 4470, 486, 312, 264, 16998, 498, 264, 21264, 366, 2293, 558, 370, 498], "temperature": 0.0, "avg_logprob": -0.14804469085321192, "compression_ratio": 1.4793388429752066, "no_speech_prob": 5.453292146739841e-07}, {"id": 176, "seek": 127188, "start": 1281.0400000000002, "end": 1298.68, "text": " we did predictions is actually identical to the targets then this will be 0 0 0 okay where", "tokens": [321, 630, 21264, 307, 767, 14800, 281, 264, 12911, 550, 341, 486, 312, 1958, 1958, 1958, 1392, 689], "temperature": 0.0, "avg_logprob": -0.14804469085321192, "compression_ratio": 1.4793388429752066, "no_speech_prob": 5.453292146739841e-07}, {"id": 177, "seek": 129868, "start": 1298.68, "end": 1309.04, "text": " else if they were exactly wrong they say they were 1 minus then it's 1 1 1 so it's going", "tokens": [1646, 498, 436, 645, 2293, 2085, 436, 584, 436, 645, 502, 3175, 550, 309, 311, 502, 502, 502, 370, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.11273269130759044, "compression_ratio": 1.639751552795031, "no_speech_prob": 3.989710819496395e-07}, {"id": 178, "seek": 129868, "start": 1309.04, "end": 1317.96, "text": " to be the loss will be better ie smaller when the predictions are closer to the targets", "tokens": [281, 312, 264, 4470, 486, 312, 1101, 43203, 4356, 562, 264, 21264, 366, 4966, 281, 264, 12911], "temperature": 0.0, "avg_logprob": -0.11273269130759044, "compression_ratio": 1.639751552795031, "no_speech_prob": 3.989710819496395e-07}, {"id": 179, "seek": 129868, "start": 1317.96, "end": 1327.44, "text": " and so here we can now take the mean and when we do we get here 0.4 3 3 so let's say we", "tokens": [293, 370, 510, 321, 393, 586, 747, 264, 914, 293, 562, 321, 360, 321, 483, 510, 1958, 13, 19, 805, 805, 370, 718, 311, 584, 321], "temperature": 0.0, "avg_logprob": -0.11273269130759044, "compression_ratio": 1.639751552795031, "no_speech_prob": 3.989710819496395e-07}, {"id": 180, "seek": 132744, "start": 1327.44, "end": 1340.04, "text": " change this last bad one this inaccurate prediction from 0.2 to 0.8 and the loss gets better from", "tokens": [1319, 341, 1036, 1578, 472, 341, 46443, 17630, 490, 1958, 13, 17, 281, 1958, 13, 23, 293, 264, 4470, 2170, 1101, 490], "temperature": 0.0, "avg_logprob": -0.13966644511503332, "compression_ratio": 1.705521472392638, "no_speech_prob": 4.116356819849898e-07}, {"id": 181, "seek": 132744, "start": 1340.04, "end": 1347.66, "text": " 0.43 to 0.23 so this is just this function is torch dot where dot mean so this is actually", "tokens": [1958, 13, 17201, 281, 1958, 13, 9356, 370, 341, 307, 445, 341, 2445, 307, 27822, 5893, 689, 5893, 914, 370, 341, 307, 767], "temperature": 0.0, "avg_logprob": -0.13966644511503332, "compression_ratio": 1.705521472392638, "no_speech_prob": 4.116356819849898e-07}, {"id": 182, "seek": 132744, "start": 1347.66, "end": 1353.0800000000002, "text": " pretty good this is actually a loss function which pretty closely tracks accuracy was the", "tokens": [1238, 665, 341, 307, 767, 257, 4470, 2445, 597, 1238, 8185, 10218, 14170, 390, 264], "temperature": 0.0, "avg_logprob": -0.13966644511503332, "compression_ratio": 1.705521472392638, "no_speech_prob": 4.116356819849898e-07}, {"id": 183, "seek": 135308, "start": 1353.08, "end": 1358.3999999999999, "text": " accuracy is better the loss will be smaller but also it doesn't have these zero gradients", "tokens": [14170, 307, 1101, 264, 4470, 486, 312, 4356, 457, 611, 309, 1177, 380, 362, 613, 4018, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.06191478991040997, "compression_ratio": 1.8577235772357723, "no_speech_prob": 1.7603351807338186e-06}, {"id": 184, "seek": 135308, "start": 1358.3999999999999, "end": 1362.8799999999999, "text": " because every time we change the prediction the loss changes because the prediction is", "tokens": [570, 633, 565, 321, 1319, 264, 17630, 264, 4470, 2962, 570, 264, 17630, 307], "temperature": 0.0, "avg_logprob": -0.06191478991040997, "compression_ratio": 1.8577235772357723, "no_speech_prob": 1.7603351807338186e-06}, {"id": 185, "seek": 135308, "start": 1362.8799999999999, "end": 1369.3799999999999, "text": " literally part of the loss that's pretty neat isn't it one problem is this is only going", "tokens": [3736, 644, 295, 264, 4470, 300, 311, 1238, 10654, 1943, 380, 309, 472, 1154, 307, 341, 307, 787, 516], "temperature": 0.0, "avg_logprob": -0.06191478991040997, "compression_ratio": 1.8577235772357723, "no_speech_prob": 1.7603351807338186e-06}, {"id": 186, "seek": 135308, "start": 1369.3799999999999, "end": 1374.9399999999998, "text": " to work well as long as the predictions are between 0 and 1 otherwise this 1 minus prediction", "tokens": [281, 589, 731, 382, 938, 382, 264, 21264, 366, 1296, 1958, 293, 502, 5911, 341, 502, 3175, 17630], "temperature": 0.0, "avg_logprob": -0.06191478991040997, "compression_ratio": 1.8577235772357723, "no_speech_prob": 1.7603351807338186e-06}, {"id": 187, "seek": 135308, "start": 1374.9399999999998, "end": 1381.08, "text": " thing is going to look a bit funny so we should try and find a way to ensure that the predictions", "tokens": [551, 307, 516, 281, 574, 257, 857, 4074, 370, 321, 820, 853, 293, 915, 257, 636, 281, 5586, 300, 264, 21264], "temperature": 0.0, "avg_logprob": -0.06191478991040997, "compression_ratio": 1.8577235772357723, "no_speech_prob": 1.7603351807338186e-06}, {"id": 188, "seek": 138108, "start": 1381.08, "end": 1386.28, "text": " are always between 0 and 1 and that's also going to just make a lot more intuitive sense", "tokens": [366, 1009, 1296, 1958, 293, 502, 293, 300, 311, 611, 516, 281, 445, 652, 257, 688, 544, 21769, 2020], "temperature": 0.0, "avg_logprob": -0.10619794317038662, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.965274342794146e-07}, {"id": 189, "seek": 138108, "start": 1386.28, "end": 1390.4399999999998, "text": " because you know we like to be able to kind of think of these as if they're like probabilities", "tokens": [570, 291, 458, 321, 411, 281, 312, 1075, 281, 733, 295, 519, 295, 613, 382, 498, 436, 434, 411, 33783], "temperature": 0.0, "avg_logprob": -0.10619794317038662, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.965274342794146e-07}, {"id": 190, "seek": 138108, "start": 1390.4399999999998, "end": 1400.1999999999998, "text": " or at least nicely scaled numbers so we need some function that can take our numbers have", "tokens": [420, 412, 1935, 9594, 36039, 3547, 370, 321, 643, 512, 2445, 300, 393, 747, 527, 3547, 362], "temperature": 0.0, "avg_logprob": -0.10619794317038662, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.965274342794146e-07}, {"id": 191, "seek": 138108, "start": 1400.1999999999998, "end": 1407.76, "text": " a look it's something which can take these big numbers and turn them all into numbers", "tokens": [257, 574, 309, 311, 746, 597, 393, 747, 613, 955, 3547, 293, 1261, 552, 439, 666, 3547], "temperature": 0.0, "avg_logprob": -0.10619794317038662, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.965274342794146e-07}, {"id": 192, "seek": 140776, "start": 1407.76, "end": 1416.28, "text": " between 0 and 1 and it so happens that we have exactly the right function it's called", "tokens": [1296, 1958, 293, 502, 293, 309, 370, 2314, 300, 321, 362, 2293, 264, 558, 2445, 309, 311, 1219], "temperature": 0.0, "avg_logprob": -0.07616864310370551, "compression_ratio": 1.9886363636363635, "no_speech_prob": 8.059431024776131e-07}, {"id": 193, "seek": 140776, "start": 1416.28, "end": 1421.84, "text": " the sigmoid function so the sigmoid function looks like this if you pass in a really small", "tokens": [264, 4556, 3280, 327, 2445, 370, 264, 4556, 3280, 327, 2445, 1542, 411, 341, 498, 291, 1320, 294, 257, 534, 1359], "temperature": 0.0, "avg_logprob": -0.07616864310370551, "compression_ratio": 1.9886363636363635, "no_speech_prob": 8.059431024776131e-07}, {"id": 194, "seek": 140776, "start": 1421.84, "end": 1427.04, "text": " number you get a number very close to 0 if you pass in a big number you get a number", "tokens": [1230, 291, 483, 257, 1230, 588, 1998, 281, 1958, 498, 291, 1320, 294, 257, 955, 1230, 291, 483, 257, 1230], "temperature": 0.0, "avg_logprob": -0.07616864310370551, "compression_ratio": 1.9886363636363635, "no_speech_prob": 8.059431024776131e-07}, {"id": 195, "seek": 140776, "start": 1427.04, "end": 1434.46, "text": " very close to 1 it never gets past 1 and it never goes smaller than 0 and then it's kind", "tokens": [588, 1998, 281, 502, 309, 1128, 2170, 1791, 502, 293, 309, 1128, 1709, 4356, 813, 1958, 293, 550, 309, 311, 733], "temperature": 0.0, "avg_logprob": -0.07616864310370551, "compression_ratio": 1.9886363636363635, "no_speech_prob": 8.059431024776131e-07}, {"id": 196, "seek": 143446, "start": 1434.46, "end": 1439.06, "text": " of like the smooth curve between and in the middle it looks a lot like the y equals x", "tokens": [295, 411, 264, 5508, 7605, 1296, 293, 294, 264, 2808, 309, 1542, 257, 688, 411, 264, 288, 6915, 2031], "temperature": 0.0, "avg_logprob": -0.13909789790277896, "compression_ratio": 1.425, "no_speech_prob": 5.122889206177206e-07}, {"id": 197, "seek": 143446, "start": 1439.06, "end": 1448.56, "text": " line this is the definition of the sigmoid function it's 1 over 1 plus e to the minus", "tokens": [1622, 341, 307, 264, 7123, 295, 264, 4556, 3280, 327, 2445, 309, 311, 502, 670, 502, 1804, 308, 281, 264, 3175], "temperature": 0.0, "avg_logprob": -0.13909789790277896, "compression_ratio": 1.425, "no_speech_prob": 5.122889206177206e-07}, {"id": 198, "seek": 144856, "start": 1448.56, "end": 1465.0, "text": " x what is x x is just e to the power of something so if we look at e it's just a number like", "tokens": [2031, 437, 307, 2031, 2031, 307, 445, 308, 281, 264, 1347, 295, 746, 370, 498, 321, 574, 412, 308, 309, 311, 445, 257, 1230, 411], "temperature": 0.0, "avg_logprob": -0.1637572875389686, "compression_ratio": 1.5338983050847457, "no_speech_prob": 3.059018069961894e-07}, {"id": 199, "seek": 144856, "start": 1465.0, "end": 1472.28, "text": " pi this is the simple it's just a number that has a particular value right so if we go e", "tokens": [3895, 341, 307, 264, 2199, 309, 311, 445, 257, 1230, 300, 575, 257, 1729, 2158, 558, 370, 498, 321, 352, 308], "temperature": 0.0, "avg_logprob": -0.1637572875389686, "compression_ratio": 1.5338983050847457, "no_speech_prob": 3.059018069961894e-07}, {"id": 200, "seek": 147228, "start": 1472.28, "end": 1489.04, "text": " squared and we look at it's going to be a tensor use pi torch it could have float there", "tokens": [8889, 293, 321, 574, 412, 309, 311, 516, 281, 312, 257, 40863, 764, 3895, 27822, 309, 727, 362, 15706, 456], "temperature": 0.0, "avg_logprob": -0.23536542121400225, "compression_ratio": 1.4193548387096775, "no_speech_prob": 3.28873875332647e-06}, {"id": 201, "seek": 147228, "start": 1489.04, "end": 1499.8799999999999, "text": " we go and you can see that these are the same number so that's what torch.exp means okay", "tokens": [321, 352, 293, 291, 393, 536, 300, 613, 366, 264, 912, 1230, 370, 300, 311, 437, 27822, 13, 15952, 1355, 1392], "temperature": 0.0, "avg_logprob": -0.23536542121400225, "compression_ratio": 1.4193548387096775, "no_speech_prob": 3.28873875332647e-06}, {"id": 202, "seek": 149988, "start": 1499.88, "end": 1504.7600000000002, "text": " so you know for me when I see these kinds of interesting functions I don't worry too", "tokens": [370, 291, 458, 337, 385, 562, 286, 536, 613, 3685, 295, 1880, 6828, 286, 500, 380, 3292, 886], "temperature": 0.0, "avg_logprob": -0.06082096099853516, "compression_ratio": 1.7651821862348178, "no_speech_prob": 1.3925416624260833e-06}, {"id": 203, "seek": 149988, "start": 1504.7600000000002, "end": 1511.42, "text": " much about the definition what I care about is the shape right so you can have a play", "tokens": [709, 466, 264, 7123, 437, 286, 1127, 466, 307, 264, 3909, 558, 370, 291, 393, 362, 257, 862], "temperature": 0.0, "avg_logprob": -0.06082096099853516, "compression_ratio": 1.7651821862348178, "no_speech_prob": 1.3925416624260833e-06}, {"id": 204, "seek": 149988, "start": 1511.42, "end": 1515.88, "text": " around with graphing calculators or whatever to kind of see why it is that you end up with", "tokens": [926, 365, 1295, 79, 571, 4322, 3391, 420, 2035, 281, 733, 295, 536, 983, 309, 307, 300, 291, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.06082096099853516, "compression_ratio": 1.7651821862348178, "no_speech_prob": 1.3925416624260833e-06}, {"id": 205, "seek": 149988, "start": 1515.88, "end": 1522.8000000000002, "text": " this shape from this particular equation but for me I just never think about that I it", "tokens": [341, 3909, 490, 341, 1729, 5367, 457, 337, 385, 286, 445, 1128, 519, 466, 300, 286, 309], "temperature": 0.0, "avg_logprob": -0.06082096099853516, "compression_ratio": 1.7651821862348178, "no_speech_prob": 1.3925416624260833e-06}, {"id": 206, "seek": 149988, "start": 1522.8000000000002, "end": 1528.96, "text": " never really matters to me what's important is this sigmoid shape which is what we want", "tokens": [1128, 534, 7001, 281, 385, 437, 311, 1021, 307, 341, 4556, 3280, 327, 3909, 597, 307, 437, 321, 528], "temperature": 0.0, "avg_logprob": -0.06082096099853516, "compression_ratio": 1.7651821862348178, "no_speech_prob": 1.3925416624260833e-06}, {"id": 207, "seek": 152896, "start": 1528.96, "end": 1537.76, "text": " it's something that squashes every number to be between naught and 1 so we can change", "tokens": [309, 311, 746, 300, 2339, 12808, 633, 1230, 281, 312, 1296, 13138, 293, 502, 370, 321, 393, 1319], "temperature": 0.0, "avg_logprob": -0.09993245628442657, "compression_ratio": 1.5945945945945945, "no_speech_prob": 2.601608912300435e-06}, {"id": 208, "seek": 152896, "start": 1537.76, "end": 1543.4, "text": " MNIST loss to be exactly the same as it was before but first we can make everything into", "tokens": [376, 45, 19756, 4470, 281, 312, 2293, 264, 912, 382, 309, 390, 949, 457, 700, 321, 393, 652, 1203, 666], "temperature": 0.0, "avg_logprob": -0.09993245628442657, "compression_ratio": 1.5945945945945945, "no_speech_prob": 2.601608912300435e-06}, {"id": 209, "seek": 152896, "start": 1543.4, "end": 1550.64, "text": " sigmoid first and then use torch.ware so that is a loss function that has all the properties", "tokens": [4556, 3280, 327, 700, 293, 550, 764, 27822, 13, 3039, 370, 300, 307, 257, 4470, 2445, 300, 575, 439, 264, 7221], "temperature": 0.0, "avg_logprob": -0.09993245628442657, "compression_ratio": 1.5945945945945945, "no_speech_prob": 2.601608912300435e-06}, {"id": 210, "seek": 152896, "start": 1550.64, "end": 1557.48, "text": " we want it's it's something which is going to be have not have any of those nasty zero", "tokens": [321, 528, 309, 311, 309, 311, 746, 597, 307, 516, 281, 312, 362, 406, 362, 604, 295, 729, 17923, 4018], "temperature": 0.0, "avg_logprob": -0.09993245628442657, "compression_ratio": 1.5945945945945945, "no_speech_prob": 2.601608912300435e-06}, {"id": 211, "seek": 155748, "start": 1557.48, "end": 1568.2, "text": " gradients and we've ensured that the input to the where is between naught and 1 so the", "tokens": [2771, 2448, 293, 321, 600, 3489, 3831, 300, 264, 4846, 281, 264, 689, 307, 1296, 13138, 293, 502, 370, 264], "temperature": 0.0, "avg_logprob": -0.12512587098514333, "compression_ratio": 1.6424242424242423, "no_speech_prob": 2.785267838589789e-07}, {"id": 212, "seek": 155748, "start": 1568.2, "end": 1577.84, "text": " reason we did this is because our our accuracy was kind of what we really care about is a", "tokens": [1778, 321, 630, 341, 307, 570, 527, 527, 14170, 390, 733, 295, 437, 321, 534, 1127, 466, 307, 257], "temperature": 0.0, "avg_logprob": -0.12512587098514333, "compression_ratio": 1.6424242424242423, "no_speech_prob": 2.785267838589789e-07}, {"id": 213, "seek": 155748, "start": 1577.84, "end": 1584.76, "text": " good accuracy we can't use it to get our gradients just just to create our step to improve our", "tokens": [665, 14170, 321, 393, 380, 764, 309, 281, 483, 527, 2771, 2448, 445, 445, 281, 1884, 527, 1823, 281, 3470, 527], "temperature": 0.0, "avg_logprob": -0.12512587098514333, "compression_ratio": 1.6424242424242423, "no_speech_prob": 2.785267838589789e-07}, {"id": 214, "seek": 158476, "start": 1584.76, "end": 1596.48, "text": " parameters so we can change our our accuracy to another function that is similar in terms", "tokens": [9834, 370, 321, 393, 1319, 527, 527, 14170, 281, 1071, 2445, 300, 307, 2531, 294, 2115], "temperature": 0.0, "avg_logprob": -0.08246166365487236, "compression_ratio": 1.7878787878787878, "no_speech_prob": 3.927853242657875e-07}, {"id": 215, "seek": 158476, "start": 1596.48, "end": 1603.52, "text": " of it it's better when the accuracy is better but it also does not have these zero gradients", "tokens": [295, 309, 309, 311, 1101, 562, 264, 14170, 307, 1101, 457, 309, 611, 775, 406, 362, 613, 4018, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.08246166365487236, "compression_ratio": 1.7878787878787878, "no_speech_prob": 3.927853242657875e-07}, {"id": 216, "seek": 158476, "start": 1603.52, "end": 1607.8, "text": " and so you can see now where why we have a metric and a loss the metric is the thing", "tokens": [293, 370, 291, 393, 536, 586, 689, 983, 321, 362, 257, 20678, 293, 257, 4470, 264, 20678, 307, 264, 551], "temperature": 0.0, "avg_logprob": -0.08246166365487236, "compression_ratio": 1.7878787878787878, "no_speech_prob": 3.927853242657875e-07}, {"id": 217, "seek": 158476, "start": 1607.8, "end": 1613.08, "text": " we actually care about the loss is the thing that's similar to what we care about that", "tokens": [321, 767, 1127, 466, 264, 4470, 307, 264, 551, 300, 311, 2531, 281, 437, 321, 1127, 466, 300], "temperature": 0.0, "avg_logprob": -0.08246166365487236, "compression_ratio": 1.7878787878787878, "no_speech_prob": 3.927853242657875e-07}, {"id": 218, "seek": 161308, "start": 1613.08, "end": 1621.32, "text": " has a nicely behaved gradient sometimes the thing you care about your metric does have", "tokens": [575, 257, 9594, 48249, 16235, 2171, 264, 551, 291, 1127, 466, 428, 20678, 775, 362], "temperature": 0.0, "avg_logprob": -0.07765509401048933, "compression_ratio": 1.617283950617284, "no_speech_prob": 9.276329819840612e-07}, {"id": 219, "seek": 161308, "start": 1621.32, "end": 1625.48, "text": " a nicely defined gradient and you can use it directly as a loss for example we often", "tokens": [257, 9594, 7642, 16235, 293, 291, 393, 764, 309, 3838, 382, 257, 4470, 337, 1365, 321, 2049], "temperature": 0.0, "avg_logprob": -0.07765509401048933, "compression_ratio": 1.617283950617284, "no_speech_prob": 9.276329819840612e-07}, {"id": 220, "seek": 161308, "start": 1625.48, "end": 1635.4399999999998, "text": " use mean squared error but for classification unfortunately not so we need to now use this", "tokens": [764, 914, 8889, 6713, 457, 337, 21538, 7015, 406, 370, 321, 643, 281, 586, 764, 341], "temperature": 0.0, "avg_logprob": -0.07765509401048933, "compression_ratio": 1.617283950617284, "no_speech_prob": 9.276329819840612e-07}, {"id": 221, "seek": 163544, "start": 1635.44, "end": 1643.96, "text": " to to update the parameters and so there's a couple of ways we could do this one would", "tokens": [281, 281, 5623, 264, 9834, 293, 370, 456, 311, 257, 1916, 295, 2098, 321, 727, 360, 341, 472, 576], "temperature": 0.0, "avg_logprob": -0.09211794535319011, "compression_ratio": 2.0, "no_speech_prob": 1.1911058663827134e-06}, {"id": 222, "seek": 163544, "start": 1643.96, "end": 1653.0, "text": " be to loop through every image calculate a prediction for that image and then calculate", "tokens": [312, 281, 6367, 807, 633, 3256, 8873, 257, 17630, 337, 300, 3256, 293, 550, 8873], "temperature": 0.0, "avg_logprob": -0.09211794535319011, "compression_ratio": 2.0, "no_speech_prob": 1.1911058663827134e-06}, {"id": 223, "seek": 163544, "start": 1653.0, "end": 1659.48, "text": " a loss and then do a step and then step the parameters and then do that again for the", "tokens": [257, 4470, 293, 550, 360, 257, 1823, 293, 550, 1823, 264, 9834, 293, 550, 360, 300, 797, 337, 264], "temperature": 0.0, "avg_logprob": -0.09211794535319011, "compression_ratio": 2.0, "no_speech_prob": 1.1911058663827134e-06}, {"id": 224, "seek": 163544, "start": 1659.48, "end": 1664.4, "text": " next image and the next image and the next image that's going to be really slow because", "tokens": [958, 3256, 293, 264, 958, 3256, 293, 264, 958, 3256, 300, 311, 516, 281, 312, 534, 2964, 570], "temperature": 0.0, "avg_logprob": -0.09211794535319011, "compression_ratio": 2.0, "no_speech_prob": 1.1911058663827134e-06}, {"id": 225, "seek": 166440, "start": 1664.4, "end": 1671.0400000000002, "text": " we're doing a single step for a single image so that would mean an epoch would take quite", "tokens": [321, 434, 884, 257, 2167, 1823, 337, 257, 2167, 3256, 370, 300, 576, 914, 364, 30992, 339, 576, 747, 1596], "temperature": 0.0, "avg_logprob": -0.07464503158222545, "compression_ratio": 1.733009708737864, "no_speech_prob": 7.934482937344001e-07}, {"id": 226, "seek": 166440, "start": 1671.0400000000002, "end": 1679.8000000000002, "text": " a while we could go much faster by doing every single image in the data set so a big matrix", "tokens": [257, 1339, 321, 727, 352, 709, 4663, 538, 884, 633, 2167, 3256, 294, 264, 1412, 992, 370, 257, 955, 8141], "temperature": 0.0, "avg_logprob": -0.07464503158222545, "compression_ratio": 1.733009708737864, "no_speech_prob": 7.934482937344001e-07}, {"id": 227, "seek": 166440, "start": 1679.8000000000002, "end": 1685.48, "text": " multiplication it can all be parallelized on the GPU and then so then we can we could", "tokens": [27290, 309, 393, 439, 312, 8952, 1602, 322, 264, 18407, 293, 550, 370, 550, 321, 393, 321, 727], "temperature": 0.0, "avg_logprob": -0.07464503158222545, "compression_ratio": 1.733009708737864, "no_speech_prob": 7.934482937344001e-07}, {"id": 228, "seek": 166440, "start": 1685.48, "end": 1693.76, "text": " then do a step based on the gradients looking at the entire data set but now that's going", "tokens": [550, 360, 257, 1823, 2361, 322, 264, 2771, 2448, 1237, 412, 264, 2302, 1412, 992, 457, 586, 300, 311, 516], "temperature": 0.0, "avg_logprob": -0.07464503158222545, "compression_ratio": 1.733009708737864, "no_speech_prob": 7.934482937344001e-07}, {"id": 229, "seek": 169376, "start": 1693.76, "end": 1701.04, "text": " to be like a lot of work to just update the weights once and remember sometimes our data", "tokens": [281, 312, 411, 257, 688, 295, 589, 281, 445, 5623, 264, 17443, 1564, 293, 1604, 2171, 527, 1412], "temperature": 0.0, "avg_logprob": -0.10793632810766046, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.2679251994995866e-06}, {"id": 230, "seek": 169376, "start": 1701.04, "end": 1708.32, "text": " sets have millions or tens of millions of items so that's probably a bad idea too so", "tokens": [6352, 362, 6803, 420, 10688, 295, 6803, 295, 4754, 370, 300, 311, 1391, 257, 1578, 1558, 886, 370], "temperature": 0.0, "avg_logprob": -0.10793632810766046, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.2679251994995866e-06}, {"id": 231, "seek": 169376, "start": 1708.32, "end": 1716.6, "text": " why not compromise let's grab a few data items at a time to calculate our loss and our step", "tokens": [983, 406, 18577, 718, 311, 4444, 257, 1326, 1412, 4754, 412, 257, 565, 281, 8873, 527, 4470, 293, 527, 1823], "temperature": 0.0, "avg_logprob": -0.10793632810766046, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.2679251994995866e-06}, {"id": 232, "seek": 169376, "start": 1716.6, "end": 1721.28, "text": " if we grab a few data items at a time those two data items are called a mini-batch and", "tokens": [498, 321, 4444, 257, 1326, 1412, 4754, 412, 257, 565, 729, 732, 1412, 4754, 366, 1219, 257, 8382, 12, 65, 852, 293], "temperature": 0.0, "avg_logprob": -0.10793632810766046, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.2679251994995866e-06}, {"id": 233, "seek": 172128, "start": 1721.28, "end": 1729.2, "text": " a mini-batch just means a few pieces of data and so the size of your mini-batch is called", "tokens": [257, 8382, 12, 65, 852, 445, 1355, 257, 1326, 3755, 295, 1412, 293, 370, 264, 2744, 295, 428, 8382, 12, 65, 852, 307, 1219], "temperature": 0.0, "avg_logprob": -0.07934271904730028, "compression_ratio": 1.9259259259259258, "no_speech_prob": 1.136562218562176e-06}, {"id": 234, "seek": 172128, "start": 1729.2, "end": 1734.16, "text": " surprisingly the batch size right so the bigger the batch size the closer you get to the full", "tokens": [17600, 264, 15245, 2744, 558, 370, 264, 3801, 264, 15245, 2744, 264, 4966, 291, 483, 281, 264, 1577], "temperature": 0.0, "avg_logprob": -0.07934271904730028, "compression_ratio": 1.9259259259259258, "no_speech_prob": 1.136562218562176e-06}, {"id": 235, "seek": 172128, "start": 1734.16, "end": 1740.6399999999999, "text": " size of your data set the longer it's going to take to calculate a single set of losses", "tokens": [2744, 295, 428, 1412, 992, 264, 2854, 309, 311, 516, 281, 747, 281, 8873, 257, 2167, 992, 295, 15352], "temperature": 0.0, "avg_logprob": -0.07934271904730028, "compression_ratio": 1.9259259259259258, "no_speech_prob": 1.136562218562176e-06}, {"id": 236, "seek": 172128, "start": 1740.6399999999999, "end": 1745.92, "text": " a single step but the the more accurate it's going to be it's going to be like the gradients", "tokens": [257, 2167, 1823, 457, 264, 264, 544, 8559, 309, 311, 516, 281, 312, 309, 311, 516, 281, 312, 411, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.07934271904730028, "compression_ratio": 1.9259259259259258, "no_speech_prob": 1.136562218562176e-06}, {"id": 237, "seek": 174592, "start": 1745.92, "end": 1751.52, "text": " are going to be much closer to the true data set gradients and then the smaller the batch", "tokens": [366, 516, 281, 312, 709, 4966, 281, 264, 2074, 1412, 992, 2771, 2448, 293, 550, 264, 4356, 264, 15245], "temperature": 0.0, "avg_logprob": -0.08304728931850858, "compression_ratio": 1.728110599078341, "no_speech_prob": 8.186352715711109e-07}, {"id": 238, "seek": 174592, "start": 1751.52, "end": 1758.04, "text": " size the faster each step we'll be able to do but those steps will represent a smaller", "tokens": [2744, 264, 4663, 1184, 1823, 321, 603, 312, 1075, 281, 360, 457, 729, 4439, 486, 2906, 257, 4356], "temperature": 0.0, "avg_logprob": -0.08304728931850858, "compression_ratio": 1.728110599078341, "no_speech_prob": 8.186352715711109e-07}, {"id": 239, "seek": 174592, "start": 1758.04, "end": 1763.0800000000002, "text": " number of items and so they won't be such an accurate approximation of the real gradient", "tokens": [1230, 295, 4754, 293, 370, 436, 1582, 380, 312, 1270, 364, 8559, 28023, 295, 264, 957, 16235], "temperature": 0.0, "avg_logprob": -0.08304728931850858, "compression_ratio": 1.728110599078341, "no_speech_prob": 8.186352715711109e-07}, {"id": 240, "seek": 174592, "start": 1763.0800000000002, "end": 1768.0, "text": " of the whole data set.", "tokens": [295, 264, 1379, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.08304728931850858, "compression_ratio": 1.728110599078341, "no_speech_prob": 8.186352715711109e-07}, {"id": 241, "seek": 174592, "start": 1768.0, "end": 1772.8000000000002, "text": " Is there a reason the mean of the loss is calculated over say doing a median since the", "tokens": [1119, 456, 257, 1778, 264, 914, 295, 264, 4470, 307, 15598, 670, 584, 884, 257, 26779, 1670, 264], "temperature": 0.0, "avg_logprob": -0.08304728931850858, "compression_ratio": 1.728110599078341, "no_speech_prob": 8.186352715711109e-07}, {"id": 242, "seek": 177280, "start": 1772.8, "end": 1779.76, "text": " median is less prone to getting influenced by outliers in the example you gave if the", "tokens": [26779, 307, 1570, 25806, 281, 1242, 15269, 538, 484, 23646, 294, 264, 1365, 291, 2729, 498, 264], "temperature": 0.0, "avg_logprob": -0.10362413601997571, "compression_ratio": 1.580188679245283, "no_speech_prob": 5.862775196874281e-06}, {"id": 243, "seek": 177280, "start": 1779.76, "end": 1784.84, "text": " third point which was wrongly predicted as an outlier then the derivative would push", "tokens": [2636, 935, 597, 390, 2085, 356, 19147, 382, 364, 484, 2753, 550, 264, 13760, 576, 2944], "temperature": 0.0, "avg_logprob": -0.10362413601997571, "compression_ratio": 1.580188679245283, "no_speech_prob": 5.862775196874281e-06}, {"id": 244, "seek": 177280, "start": 1784.84, "end": 1792.8, "text": " the function away while doing SGD and a median could be better in that case.", "tokens": [264, 2445, 1314, 1339, 884, 34520, 35, 293, 257, 26779, 727, 312, 1101, 294, 300, 1389, 13], "temperature": 0.0, "avg_logprob": -0.10362413601997571, "compression_ratio": 1.580188679245283, "no_speech_prob": 5.862775196874281e-06}, {"id": 245, "seek": 177280, "start": 1792.8, "end": 1798.6399999999999, "text": " Honestly I've never tried using a median the problem with a median is it ends up really", "tokens": [12348, 286, 600, 1128, 3031, 1228, 257, 26779, 264, 1154, 365, 257, 26779, 307, 309, 5314, 493, 534], "temperature": 0.0, "avg_logprob": -0.10362413601997571, "compression_ratio": 1.580188679245283, "no_speech_prob": 5.862775196874281e-06}, {"id": 246, "seek": 179864, "start": 1798.64, "end": 1805.68, "text": " only caring about one number which is the number in the middle so it could end up really", "tokens": [787, 15365, 466, 472, 1230, 597, 307, 264, 1230, 294, 264, 2808, 370, 309, 727, 917, 493, 534], "temperature": 0.0, "avg_logprob": -0.09150917713458721, "compression_ratio": 1.9051724137931034, "no_speech_prob": 1.5779547766214819e-06}, {"id": 247, "seek": 179864, "start": 1805.68, "end": 1810.5600000000002, "text": " pretty much ignoring all of the things at each end in fact all it really cares about", "tokens": [1238, 709, 26258, 439, 295, 264, 721, 412, 1184, 917, 294, 1186, 439, 309, 534, 12310, 466], "temperature": 0.0, "avg_logprob": -0.09150917713458721, "compression_ratio": 1.9051724137931034, "no_speech_prob": 1.5779547766214819e-06}, {"id": 248, "seek": 179864, "start": 1810.5600000000002, "end": 1815.74, "text": " is the order of things so my guess is that you would end up with something that is only", "tokens": [307, 264, 1668, 295, 721, 370, 452, 2041, 307, 300, 291, 576, 917, 493, 365, 746, 300, 307, 787], "temperature": 0.0, "avg_logprob": -0.09150917713458721, "compression_ratio": 1.9051724137931034, "no_speech_prob": 1.5779547766214819e-06}, {"id": 249, "seek": 179864, "start": 1815.74, "end": 1822.6000000000001, "text": " good at predicting one thing in the middle but I haven't tried it be interesting to see", "tokens": [665, 412, 32884, 472, 551, 294, 264, 2808, 457, 286, 2378, 380, 3031, 309, 312, 1880, 281, 536], "temperature": 0.0, "avg_logprob": -0.09150917713458721, "compression_ratio": 1.9051724137931034, "no_speech_prob": 1.5779547766214819e-06}, {"id": 250, "seek": 179864, "start": 1822.6000000000001, "end": 1828.24, "text": " well I guess the other thing that would happen with a median is you would have a lot of zero", "tokens": [731, 286, 2041, 264, 661, 551, 300, 576, 1051, 365, 257, 26779, 307, 291, 576, 362, 257, 688, 295, 4018], "temperature": 0.0, "avg_logprob": -0.09150917713458721, "compression_ratio": 1.9051724137931034, "no_speech_prob": 1.5779547766214819e-06}, {"id": 251, "seek": 182824, "start": 1828.24, "end": 1833.1, "text": " gradients I think because it's picking the thing in the middle and you could you know", "tokens": [2771, 2448, 286, 519, 570, 309, 311, 8867, 264, 551, 294, 264, 2808, 293, 291, 727, 291, 458], "temperature": 0.0, "avg_logprob": -0.12683439254760742, "compression_ratio": 1.8206521739130435, "no_speech_prob": 3.393134193174774e-06}, {"id": 252, "seek": 182824, "start": 1833.1, "end": 1838.8, "text": " change your values and the thing in the middle might well wouldn't be zero gradients but", "tokens": [1319, 428, 4190, 293, 264, 551, 294, 264, 2808, 1062, 731, 2759, 380, 312, 4018, 2771, 2448, 457], "temperature": 0.0, "avg_logprob": -0.12683439254760742, "compression_ratio": 1.8206521739130435, "no_speech_prob": 3.393134193174774e-06}, {"id": 253, "seek": 182824, "start": 1838.8, "end": 1843.2, "text": " bumpy gradients the thing in the middle would suddenly jump to being a different item so", "tokens": [49400, 2771, 2448, 264, 551, 294, 264, 2808, 576, 5800, 3012, 281, 885, 257, 819, 3174, 370], "temperature": 0.0, "avg_logprob": -0.12683439254760742, "compression_ratio": 1.8206521739130435, "no_speech_prob": 3.393134193174774e-06}, {"id": 254, "seek": 182824, "start": 1843.2, "end": 1851.36, "text": " it might not behave very well that's that's my guess you should try it.", "tokens": [309, 1062, 406, 15158, 588, 731, 300, 311, 300, 311, 452, 2041, 291, 820, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.12683439254760742, "compression_ratio": 1.8206521739130435, "no_speech_prob": 3.393134193174774e-06}, {"id": 255, "seek": 185136, "start": 1851.36, "end": 1859.3999999999999, "text": " Okay so how do we ask for a few items at a time it turns out that PyTorch and fastai", "tokens": [1033, 370, 577, 360, 321, 1029, 337, 257, 1326, 4754, 412, 257, 565, 309, 4523, 484, 300, 9953, 51, 284, 339, 293, 2370, 1301], "temperature": 0.0, "avg_logprob": -0.11094722747802735, "compression_ratio": 1.8617021276595744, "no_speech_prob": 1.191104843201174e-06}, {"id": 256, "seek": 185136, "start": 1859.3999999999999, "end": 1867.36, "text": " provide something to do that for you you can pass in any data set to this class called", "tokens": [2893, 746, 281, 360, 300, 337, 291, 291, 393, 1320, 294, 604, 1412, 992, 281, 341, 1508, 1219], "temperature": 0.0, "avg_logprob": -0.11094722747802735, "compression_ratio": 1.8617021276595744, "no_speech_prob": 1.191104843201174e-06}, {"id": 257, "seek": 185136, "start": 1867.36, "end": 1873.24, "text": " data loader and it will grab a few items from that data set at a time you can ask for how", "tokens": [1412, 3677, 260, 293, 309, 486, 4444, 257, 1326, 4754, 490, 300, 1412, 992, 412, 257, 565, 291, 393, 1029, 337, 577], "temperature": 0.0, "avg_logprob": -0.11094722747802735, "compression_ratio": 1.8617021276595744, "no_speech_prob": 1.191104843201174e-06}, {"id": 258, "seek": 185136, "start": 1873.24, "end": 1881.1599999999999, "text": " many by asking for a batch size and then you can as you can see it will grab a few items", "tokens": [867, 538, 3365, 337, 257, 15245, 2744, 293, 550, 291, 393, 382, 291, 393, 536, 309, 486, 4444, 257, 1326, 4754], "temperature": 0.0, "avg_logprob": -0.11094722747802735, "compression_ratio": 1.8617021276595744, "no_speech_prob": 1.191104843201174e-06}, {"id": 259, "seek": 188116, "start": 1881.16, "end": 1885.6000000000001, "text": " at a time until it's grabbed all of them so here I'm saying let's create a collection", "tokens": [412, 257, 565, 1826, 309, 311, 18607, 439, 295, 552, 370, 510, 286, 478, 1566, 718, 311, 1884, 257, 5765], "temperature": 0.0, "avg_logprob": -0.06772588010419879, "compression_ratio": 1.8353909465020577, "no_speech_prob": 4.785082182934275e-06}, {"id": 260, "seek": 188116, "start": 1885.6000000000001, "end": 1891.4, "text": " that just contains all the numbers from 0 to 14 let's pass that into a data loader with", "tokens": [300, 445, 8306, 439, 264, 3547, 490, 1958, 281, 3499, 718, 311, 1320, 300, 666, 257, 1412, 3677, 260, 365], "temperature": 0.0, "avg_logprob": -0.06772588010419879, "compression_ratio": 1.8353909465020577, "no_speech_prob": 4.785082182934275e-06}, {"id": 261, "seek": 188116, "start": 1891.4, "end": 1897.1200000000001, "text": " a batch size of 5 and then that's going to be something it's called an iterator in Python", "tokens": [257, 15245, 2744, 295, 1025, 293, 550, 300, 311, 516, 281, 312, 746, 309, 311, 1219, 364, 17138, 1639, 294, 15329], "temperature": 0.0, "avg_logprob": -0.06772588010419879, "compression_ratio": 1.8353909465020577, "no_speech_prob": 4.785082182934275e-06}, {"id": 262, "seek": 188116, "start": 1897.1200000000001, "end": 1901.3200000000002, "text": " it's something that you can ask for one more thing from an iterator if you pass an iterator", "tokens": [309, 311, 746, 300, 291, 393, 1029, 337, 472, 544, 551, 490, 364, 17138, 1639, 498, 291, 1320, 364, 17138, 1639], "temperature": 0.0, "avg_logprob": -0.06772588010419879, "compression_ratio": 1.8353909465020577, "no_speech_prob": 4.785082182934275e-06}, {"id": 263, "seek": 188116, "start": 1901.3200000000002, "end": 1907.98, "text": " to list in Python it returns all of the things from the iterator so here are my three mini", "tokens": [281, 1329, 294, 15329, 309, 11247, 439, 295, 264, 721, 490, 264, 17138, 1639, 370, 510, 366, 452, 1045, 8382], "temperature": 0.0, "avg_logprob": -0.06772588010419879, "compression_ratio": 1.8353909465020577, "no_speech_prob": 4.785082182934275e-06}, {"id": 264, "seek": 190798, "start": 1907.98, "end": 1912.08, "text": " batches and you'll see here all the numbers from 0 to 15 appear they appear in a random", "tokens": [15245, 279, 293, 291, 603, 536, 510, 439, 264, 3547, 490, 1958, 281, 2119, 4204, 436, 4204, 294, 257, 4974], "temperature": 0.0, "avg_logprob": -0.09438004003506954, "compression_ratio": 1.8305785123966942, "no_speech_prob": 7.338196041928313e-07}, {"id": 265, "seek": 190798, "start": 1912.08, "end": 1916.68, "text": " order and they appear five at a time they appear in random order because shuffle equals", "tokens": [1668, 293, 436, 4204, 1732, 412, 257, 565, 436, 4204, 294, 4974, 1668, 570, 39426, 6915], "temperature": 0.0, "avg_logprob": -0.09438004003506954, "compression_ratio": 1.8305785123966942, "no_speech_prob": 7.338196041928313e-07}, {"id": 266, "seek": 190798, "start": 1916.68, "end": 1922.04, "text": " true so normally in the training set we ask for things to be shuffled so it gives us a", "tokens": [2074, 370, 5646, 294, 264, 3097, 992, 321, 1029, 337, 721, 281, 312, 402, 33974, 370, 309, 2709, 505, 257], "temperature": 0.0, "avg_logprob": -0.09438004003506954, "compression_ratio": 1.8305785123966942, "no_speech_prob": 7.338196041928313e-07}, {"id": 267, "seek": 190798, "start": 1922.04, "end": 1927.52, "text": " little bit more randomization more randomization is is good because it makes it harder for", "tokens": [707, 857, 544, 4974, 2144, 544, 4974, 2144, 307, 307, 665, 570, 309, 1669, 309, 6081, 337], "temperature": 0.0, "avg_logprob": -0.09438004003506954, "compression_ratio": 1.8305785123966942, "no_speech_prob": 7.338196041928313e-07}, {"id": 268, "seek": 190798, "start": 1927.52, "end": 1934.24, "text": " it to kind of learn what the data set looks like so that's what a data loader well that's", "tokens": [309, 281, 733, 295, 1466, 437, 264, 1412, 992, 1542, 411, 370, 300, 311, 437, 257, 1412, 3677, 260, 731, 300, 311], "temperature": 0.0, "avg_logprob": -0.09438004003506954, "compression_ratio": 1.8305785123966942, "no_speech_prob": 7.338196041928313e-07}, {"id": 269, "seek": 193424, "start": 1934.24, "end": 1944.32, "text": " how a data loader is created now remember though that our data sets actually return", "tokens": [577, 257, 1412, 3677, 260, 307, 2942, 586, 1604, 1673, 300, 527, 1412, 6352, 767, 2736], "temperature": 0.0, "avg_logprob": -0.12105454820575136, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.00301247130119e-06}, {"id": 270, "seek": 193424, "start": 1944.32, "end": 1949.72, "text": " tuples and here I've just got single ints so let's actually create a tuple so if we", "tokens": [2604, 2622, 293, 510, 286, 600, 445, 658, 2167, 560, 82, 370, 718, 311, 767, 1884, 257, 2604, 781, 370, 498, 321], "temperature": 0.0, "avg_logprob": -0.12105454820575136, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.00301247130119e-06}, {"id": 271, "seek": 193424, "start": 1949.72, "end": 1957.96, "text": " enumerate all the letters of English then that means that returns 0 a 1 b 2 c better", "tokens": [465, 15583, 473, 439, 264, 7825, 295, 3669, 550, 300, 1355, 300, 11247, 1958, 257, 502, 272, 568, 269, 1101], "temperature": 0.0, "avg_logprob": -0.12105454820575136, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.00301247130119e-06}, {"id": 272, "seek": 195796, "start": 1957.96, "end": 1964.3600000000001, "text": " let's make that our data set so if we pass that to a data loader with a batch size of", "tokens": [718, 311, 652, 300, 527, 1412, 992, 370, 498, 321, 1320, 300, 281, 257, 1412, 3677, 260, 365, 257, 15245, 2744, 295], "temperature": 0.0, "avg_logprob": -0.10354019613826976, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.3925432540418115e-06}, {"id": 273, "seek": 195796, "start": 1964.3600000000001, "end": 1975.0, "text": " 6 and as you can see it returns tuples containing 6 of the first things and the associated 6", "tokens": [1386, 293, 382, 291, 393, 536, 309, 11247, 2604, 2622, 19273, 1386, 295, 264, 700, 721, 293, 264, 6615, 1386], "temperature": 0.0, "avg_logprob": -0.10354019613826976, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.3925432540418115e-06}, {"id": 274, "seek": 195796, "start": 1975.0, "end": 1982.92, "text": " of the second things so this is like our independent variable and this is like our dependent variable", "tokens": [295, 264, 1150, 721, 370, 341, 307, 411, 527, 6695, 7006, 293, 341, 307, 411, 527, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.10354019613826976, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.3925432540418115e-06}, {"id": 275, "seek": 198292, "start": 1982.92, "end": 1990.2, "text": " okay and so and then at the end you know that with the batch size won't necessarily exactly", "tokens": [1392, 293, 370, 293, 550, 412, 264, 917, 291, 458, 300, 365, 264, 15245, 2744, 1582, 380, 4725, 2293], "temperature": 0.0, "avg_logprob": -0.07529940491630918, "compression_ratio": 1.7688442211055277, "no_speech_prob": 1.3081732959108194e-06}, {"id": 276, "seek": 198292, "start": 1990.2, "end": 1999.68, "text": " divide nicely into the full size of the data set you might end up with a smaller batch", "tokens": [9845, 9594, 666, 264, 1577, 2744, 295, 264, 1412, 992, 291, 1062, 917, 493, 365, 257, 4356, 15245], "temperature": 0.0, "avg_logprob": -0.07529940491630918, "compression_ratio": 1.7688442211055277, "no_speech_prob": 1.3081732959108194e-06}, {"id": 277, "seek": 198292, "start": 1999.68, "end": 2006.52, "text": " so basically then we already have a data set remember and so we could pass it to a data", "tokens": [370, 1936, 550, 321, 1217, 362, 257, 1412, 992, 1604, 293, 370, 321, 727, 1320, 309, 281, 257, 1412], "temperature": 0.0, "avg_logprob": -0.07529940491630918, "compression_ratio": 1.7688442211055277, "no_speech_prob": 1.3081732959108194e-06}, {"id": 278, "seek": 198292, "start": 2006.52, "end": 2011.28, "text": " loader and then we can basically say this an iterator in Python is something that you", "tokens": [3677, 260, 293, 550, 321, 393, 1936, 584, 341, 364, 17138, 1639, 294, 15329, 307, 746, 300, 291], "temperature": 0.0, "avg_logprob": -0.07529940491630918, "compression_ratio": 1.7688442211055277, "no_speech_prob": 1.3081732959108194e-06}, {"id": 279, "seek": 201128, "start": 2011.28, "end": 2018.3999999999999, "text": " can actually loop through so when we say for in data loader it's going to return a tuple", "tokens": [393, 767, 6367, 807, 370, 562, 321, 584, 337, 294, 1412, 3677, 260, 309, 311, 516, 281, 2736, 257, 2604, 781], "temperature": 0.0, "avg_logprob": -0.10405324519365683, "compression_ratio": 1.951086956521739, "no_speech_prob": 7.57113809868315e-07}, {"id": 280, "seek": 201128, "start": 2018.3999999999999, "end": 2024.6, "text": " we can destructure it into the first bit and the second bit and so it's going to be our", "tokens": [321, 393, 2677, 2885, 309, 666, 264, 700, 857, 293, 264, 1150, 857, 293, 370, 309, 311, 516, 281, 312, 527], "temperature": 0.0, "avg_logprob": -0.10405324519365683, "compression_ratio": 1.951086956521739, "no_speech_prob": 7.57113809868315e-07}, {"id": 281, "seek": 201128, "start": 2024.6, "end": 2030.04, "text": " X and Y we can calculate our predictions we can calculate our loss from the predictions", "tokens": [1783, 293, 398, 321, 393, 8873, 527, 21264, 321, 393, 8873, 527, 4470, 490, 264, 21264], "temperature": 0.0, "avg_logprob": -0.10405324519365683, "compression_ratio": 1.951086956521739, "no_speech_prob": 7.57113809868315e-07}, {"id": 282, "seek": 201128, "start": 2030.04, "end": 2037.84, "text": " and the targets we can ask it to calculate our gradients and then we can update our parameters", "tokens": [293, 264, 12911, 321, 393, 1029, 309, 281, 8873, 527, 2771, 2448, 293, 550, 321, 393, 5623, 527, 9834], "temperature": 0.0, "avg_logprob": -0.10405324519365683, "compression_ratio": 1.951086956521739, "no_speech_prob": 7.57113809868315e-07}, {"id": 283, "seek": 203784, "start": 2037.84, "end": 2045.28, "text": " just like we did in our toy SGD example for the quadratic equation so let's reinitialize", "tokens": [445, 411, 321, 630, 294, 527, 12058, 34520, 35, 1365, 337, 264, 37262, 5367, 370, 718, 311, 6561, 270, 831, 1125], "temperature": 0.0, "avg_logprob": -0.10119228869412852, "compression_ratio": 1.73828125, "no_speech_prob": 1.1726388038368896e-06}, {"id": 284, "seek": 203784, "start": 2045.28, "end": 2050.16, "text": " our weights and bias with the same two lines of code before let's create the data loader", "tokens": [527, 17443, 293, 12577, 365, 264, 912, 732, 3876, 295, 3089, 949, 718, 311, 1884, 264, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.10119228869412852, "compression_ratio": 1.73828125, "no_speech_prob": 1.1726388038368896e-06}, {"id": 285, "seek": 203784, "start": 2050.16, "end": 2055.7599999999998, "text": " this time from our actual MNIST data set and create a nice big batch size so we do plenty", "tokens": [341, 565, 490, 527, 3539, 376, 45, 19756, 1412, 992, 293, 1884, 257, 1481, 955, 15245, 2744, 370, 321, 360, 7140], "temperature": 0.0, "avg_logprob": -0.10119228869412852, "compression_ratio": 1.73828125, "no_speech_prob": 1.1726388038368896e-06}, {"id": 286, "seek": 203784, "start": 2055.7599999999998, "end": 2061.0, "text": " of work each time and just to take a look let's just grab the first thing from the data", "tokens": [295, 589, 1184, 565, 293, 445, 281, 747, 257, 574, 718, 311, 445, 4444, 264, 700, 551, 490, 264, 1412], "temperature": 0.0, "avg_logprob": -0.10119228869412852, "compression_ratio": 1.73828125, "no_speech_prob": 1.1726388038368896e-06}, {"id": 287, "seek": 203784, "start": 2061.0, "end": 2066.6, "text": " loader first is a fast AI function which just grabs the first thing from an iterator it's", "tokens": [3677, 260, 700, 307, 257, 2370, 7318, 2445, 597, 445, 30028, 264, 700, 551, 490, 364, 17138, 1639, 309, 311], "temperature": 0.0, "avg_logprob": -0.10119228869412852, "compression_ratio": 1.73828125, "no_speech_prob": 1.1726388038368896e-06}, {"id": 288, "seek": 206660, "start": 2066.6, "end": 2073.08, "text": " just it's useful to look at you know kind of an arbitrary mini batch so here is the", "tokens": [445, 309, 311, 4420, 281, 574, 412, 291, 458, 733, 295, 364, 23211, 8382, 15245, 370, 510, 307, 264], "temperature": 0.0, "avg_logprob": -0.14271587795681423, "compression_ratio": 1.6355140186915889, "no_speech_prob": 1.2289159485590062e-06}, {"id": 289, "seek": 206660, "start": 2073.08, "end": 2080.72, "text": " shape we're going to have the first mini batch is 256 rows of 784 long that's 28 by 28 so", "tokens": [3909, 321, 434, 516, 281, 362, 264, 700, 8382, 15245, 307, 38882, 13241, 295, 1614, 25494, 938, 300, 311, 7562, 538, 7562, 370], "temperature": 0.0, "avg_logprob": -0.14271587795681423, "compression_ratio": 1.6355140186915889, "no_speech_prob": 1.2289159485590062e-06}, {"id": 290, "seek": 206660, "start": 2080.72, "end": 2088.4, "text": " 256 flattened out images and 256 labels that are one long because that's just the number", "tokens": [38882, 24183, 292, 484, 5267, 293, 38882, 16949, 300, 366, 472, 938, 570, 300, 311, 445, 264, 1230], "temperature": 0.0, "avg_logprob": -0.14271587795681423, "compression_ratio": 1.6355140186915889, "no_speech_prob": 1.2289159485590062e-06}, {"id": 291, "seek": 206660, "start": 2088.4, "end": 2093.64, "text": " zero or the number one depending on whether it's a three or a seven do the same for the", "tokens": [4018, 420, 264, 1230, 472, 5413, 322, 1968, 309, 311, 257, 1045, 420, 257, 3407, 360, 264, 912, 337, 264], "temperature": 0.0, "avg_logprob": -0.14271587795681423, "compression_ratio": 1.6355140186915889, "no_speech_prob": 1.2289159485590062e-06}, {"id": 292, "seek": 209364, "start": 2093.64, "end": 2108.04, "text": " validation set so here's our validation data loader and so let's grab a batch here testing", "tokens": [24071, 992, 370, 510, 311, 527, 24071, 1412, 3677, 260, 293, 370, 718, 311, 4444, 257, 15245, 510, 4997], "temperature": 0.0, "avg_logprob": -0.14698681301540797, "compression_ratio": 1.4634146341463414, "no_speech_prob": 1.392543822476e-06}, {"id": 293, "seek": 209364, "start": 2108.04, "end": 2119.08, "text": " pass it into well why do we do that we should look yeah I guess yeah actually for our for", "tokens": [1320, 309, 666, 731, 983, 360, 321, 360, 300, 321, 820, 574, 1338, 286, 2041, 1338, 767, 337, 527, 337], "temperature": 0.0, "avg_logprob": -0.14698681301540797, "compression_ratio": 1.4634146341463414, "no_speech_prob": 1.392543822476e-06}, {"id": 294, "seek": 211908, "start": 2119.08, "end": 2123.7999999999997, "text": " our testing I'm going to just manually grab the first four things just so that we can", "tokens": [527, 4997, 286, 478, 516, 281, 445, 16945, 4444, 264, 700, 1451, 721, 445, 370, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.08142226537068685, "compression_ratio": 1.6602564102564104, "no_speech_prob": 1.975050309965809e-07}, {"id": 295, "seek": 211908, "start": 2123.7999999999997, "end": 2127.7999999999997, "text": " make sure everything lines up so so let's grab just the first four things we'll call", "tokens": [652, 988, 1203, 3876, 493, 370, 370, 718, 311, 4444, 445, 264, 700, 1451, 721, 321, 603, 818], "temperature": 0.0, "avg_logprob": -0.08142226537068685, "compression_ratio": 1.6602564102564104, "no_speech_prob": 1.975050309965809e-07}, {"id": 296, "seek": 211908, "start": 2127.7999999999997, "end": 2140.16, "text": " that a batch pass it into that linear function we created earlier or remember linear was", "tokens": [300, 257, 15245, 1320, 309, 666, 300, 8213, 2445, 321, 2942, 3071, 420, 1604, 8213, 390], "temperature": 0.0, "avg_logprob": -0.08142226537068685, "compression_ratio": 1.6602564102564104, "no_speech_prob": 1.975050309965809e-07}, {"id": 297, "seek": 214016, "start": 2140.16, "end": 2152.3599999999997, "text": " just x batch at weights matrix multiply plus bias", "tokens": [445, 2031, 15245, 412, 17443, 8141, 12972, 1804, 12577], "temperature": 0.0, "avg_logprob": -0.10675910356882456, "compression_ratio": 1.6939890710382515, "no_speech_prob": 1.586999331948391e-07}, {"id": 298, "seek": 214016, "start": 2152.3599999999997, "end": 2157.48, "text": " and so that's going to give us four results that's a prediction for each of those four", "tokens": [293, 370, 300, 311, 516, 281, 976, 505, 1451, 3542, 300, 311, 257, 17630, 337, 1184, 295, 729, 1451], "temperature": 0.0, "avg_logprob": -0.10675910356882456, "compression_ratio": 1.6939890710382515, "no_speech_prob": 1.586999331948391e-07}, {"id": 299, "seek": 214016, "start": 2157.48, "end": 2162.6, "text": " images and so then we can calculate the loss using that loss function we just used and", "tokens": [5267, 293, 370, 550, 321, 393, 8873, 264, 4470, 1228, 300, 4470, 2445, 321, 445, 1143, 293], "temperature": 0.0, "avg_logprob": -0.10675910356882456, "compression_ratio": 1.6939890710382515, "no_speech_prob": 1.586999331948391e-07}, {"id": 300, "seek": 214016, "start": 2162.6, "end": 2169.3999999999996, "text": " let's just grab the first four items of the training set and there's the loss okay and", "tokens": [718, 311, 445, 4444, 264, 700, 1451, 4754, 295, 264, 3097, 992, 293, 456, 311, 264, 4470, 1392, 293], "temperature": 0.0, "avg_logprob": -0.10675910356882456, "compression_ratio": 1.6939890710382515, "no_speech_prob": 1.586999331948391e-07}, {"id": 301, "seek": 216940, "start": 2169.4, "end": 2176.2400000000002, "text": " so now we can calculate the gradients and so the gradients are seven hundred and eighty", "tokens": [370, 586, 321, 393, 8873, 264, 2771, 2448, 293, 370, 264, 2771, 2448, 366, 3407, 3262, 293, 26348], "temperature": 0.0, "avg_logprob": -0.08375351849724265, "compression_ratio": 1.8368421052631578, "no_speech_prob": 6.179386105031881e-07}, {"id": 302, "seek": 216940, "start": 2176.2400000000002, "end": 2182.48, "text": " four by one so in other words it's a column where every weight as a gradient it's what's", "tokens": [1451, 538, 472, 370, 294, 661, 2283, 309, 311, 257, 7738, 689, 633, 3364, 382, 257, 16235, 309, 311, 437, 311], "temperature": 0.0, "avg_logprob": -0.08375351849724265, "compression_ratio": 1.8368421052631578, "no_speech_prob": 6.179386105031881e-07}, {"id": 303, "seek": 216940, "start": 2182.48, "end": 2190.8, "text": " the change in loss for a small change in that parameter and then the bias as a gradient", "tokens": [264, 1319, 294, 4470, 337, 257, 1359, 1319, 294, 300, 13075, 293, 550, 264, 12577, 382, 257, 16235], "temperature": 0.0, "avg_logprob": -0.08375351849724265, "compression_ratio": 1.8368421052631578, "no_speech_prob": 6.179386105031881e-07}, {"id": 304, "seek": 216940, "start": 2190.8, "end": 2197.96, "text": " that's a single number because the bias is just a single number so we can take those", "tokens": [300, 311, 257, 2167, 1230, 570, 264, 12577, 307, 445, 257, 2167, 1230, 370, 321, 393, 747, 729], "temperature": 0.0, "avg_logprob": -0.08375351849724265, "compression_ratio": 1.8368421052631578, "no_speech_prob": 6.179386105031881e-07}, {"id": 305, "seek": 219796, "start": 2197.96, "end": 2203.56, "text": " three steps and put it in a function so if you pass if you this is calculate gradient", "tokens": [1045, 4439, 293, 829, 309, 294, 257, 2445, 370, 498, 291, 1320, 498, 291, 341, 307, 8873, 16235], "temperature": 0.0, "avg_logprob": -0.12488404239516661, "compression_ratio": 1.893048128342246, "no_speech_prob": 8.714327464076632e-07}, {"id": 306, "seek": 219796, "start": 2203.56, "end": 2209.12, "text": " you pass it an x batch or y batch in some model then it's going to calculate the predictions", "tokens": [291, 1320, 309, 364, 2031, 15245, 420, 288, 15245, 294, 512, 2316, 550, 309, 311, 516, 281, 8873, 264, 21264], "temperature": 0.0, "avg_logprob": -0.12488404239516661, "compression_ratio": 1.893048128342246, "no_speech_prob": 8.714327464076632e-07}, {"id": 307, "seek": 219796, "start": 2209.12, "end": 2215.44, "text": " calculate the loss and do the backward step and here we see calculate gradient and so", "tokens": [8873, 264, 4470, 293, 360, 264, 23897, 1823, 293, 510, 321, 536, 8873, 16235, 293, 370], "temperature": 0.0, "avg_logprob": -0.12488404239516661, "compression_ratio": 1.893048128342246, "no_speech_prob": 8.714327464076632e-07}, {"id": 308, "seek": 219796, "start": 2215.44, "end": 2221.2, "text": " we can get the just to take a look the mean of the weights gradient and the bias gradient", "tokens": [321, 393, 483, 264, 445, 281, 747, 257, 574, 264, 914, 295, 264, 17443, 16235, 293, 264, 12577, 16235], "temperature": 0.0, "avg_logprob": -0.12488404239516661, "compression_ratio": 1.893048128342246, "no_speech_prob": 8.714327464076632e-07}, {"id": 309, "seek": 222120, "start": 2221.2, "end": 2228.9199999999996, "text": " and there it is if I call it a second time and look notice I have not done any step here", "tokens": [293, 456, 309, 307, 498, 286, 818, 309, 257, 1150, 565, 293, 574, 3449, 286, 362, 406, 1096, 604, 1823, 510], "temperature": 0.0, "avg_logprob": -0.06961533499927056, "compression_ratio": 1.7303921568627452, "no_speech_prob": 9.132537570621935e-07}, {"id": 310, "seek": 222120, "start": 2228.9199999999996, "end": 2235.52, "text": " this is exactly the same parameters I get a different value that's a concern you would", "tokens": [341, 307, 2293, 264, 912, 9834, 286, 483, 257, 819, 2158, 300, 311, 257, 3136, 291, 576], "temperature": 0.0, "avg_logprob": -0.06961533499927056, "compression_ratio": 1.7303921568627452, "no_speech_prob": 9.132537570621935e-07}, {"id": 311, "seek": 222120, "start": 2235.52, "end": 2240.72, "text": " expect to get the same gradient every time you called it with the same data why have", "tokens": [2066, 281, 483, 264, 912, 16235, 633, 565, 291, 1219, 309, 365, 264, 912, 1412, 983, 362], "temperature": 0.0, "avg_logprob": -0.06961533499927056, "compression_ratio": 1.7303921568627452, "no_speech_prob": 9.132537570621935e-07}, {"id": 312, "seek": 222120, "start": 2240.72, "end": 2248.96, "text": " the gradients changed that's because loss dot backward does not just calculate the gradients", "tokens": [264, 2771, 2448, 3105, 300, 311, 570, 4470, 5893, 23897, 775, 406, 445, 8873, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.06961533499927056, "compression_ratio": 1.7303921568627452, "no_speech_prob": 9.132537570621935e-07}, {"id": 313, "seek": 224896, "start": 2248.96, "end": 2254.96, "text": " it calculates the gradients and adds them to the existing gradients the things in the", "tokens": [309, 4322, 1024, 264, 2771, 2448, 293, 10860, 552, 281, 264, 6741, 2771, 2448, 264, 721, 294, 264], "temperature": 0.0, "avg_logprob": -0.09298911152115788, "compression_ratio": 1.8274111675126903, "no_speech_prob": 1.3925429129812983e-06}, {"id": 314, "seek": 224896, "start": 2254.96, "end": 2260.96, "text": " dot grad attribute the reasons for that will come to you later but the for now the thing", "tokens": [5893, 2771, 19667, 264, 4112, 337, 300, 486, 808, 281, 291, 1780, 457, 264, 337, 586, 264, 551], "temperature": 0.0, "avg_logprob": -0.09298911152115788, "compression_ratio": 1.8274111675126903, "no_speech_prob": 1.3925429129812983e-06}, {"id": 315, "seek": 224896, "start": 2260.96, "end": 2269.08, "text": " to know is just it does that so actually what we need to do is to call grad dot zero underscore", "tokens": [281, 458, 307, 445, 309, 775, 300, 370, 767, 437, 321, 643, 281, 360, 307, 281, 818, 2771, 5893, 4018, 37556], "temperature": 0.0, "avg_logprob": -0.09298911152115788, "compression_ratio": 1.8274111675126903, "no_speech_prob": 1.3925429129812983e-06}, {"id": 316, "seek": 224896, "start": 2269.08, "end": 2275.92, "text": " so dot zero returns a tensor containing zeros and remember underscore does it in place so", "tokens": [370, 5893, 4018, 11247, 257, 40863, 19273, 35193, 293, 1604, 37556, 775, 309, 294, 1081, 370], "temperature": 0.0, "avg_logprob": -0.09298911152115788, "compression_ratio": 1.8274111675126903, "no_speech_prob": 1.3925429129812983e-06}, {"id": 317, "seek": 227592, "start": 2275.92, "end": 2283.36, "text": " that updates the weights dot grad attribute which is a tensor to contain zeros so now", "tokens": [300, 9205, 264, 17443, 5893, 2771, 19667, 597, 307, 257, 40863, 281, 5304, 35193, 370, 586], "temperature": 0.0, "avg_logprob": -0.10269731283187866, "compression_ratio": 1.4912280701754386, "no_speech_prob": 1.8266251800014288e-07}, {"id": 318, "seek": 227592, "start": 2283.36, "end": 2291.4, "text": " if I do that and call it again I will get exactly the same number so here is how you", "tokens": [498, 286, 360, 300, 293, 818, 309, 797, 286, 486, 483, 2293, 264, 912, 1230, 370, 510, 307, 577, 291], "temperature": 0.0, "avg_logprob": -0.10269731283187866, "compression_ratio": 1.4912280701754386, "no_speech_prob": 1.8266251800014288e-07}, {"id": 319, "seek": 227592, "start": 2291.4, "end": 2297.12, "text": " train one epoch with SGD loop through the data loader grabbing the x batch and the y", "tokens": [3847, 472, 30992, 339, 365, 34520, 35, 6367, 807, 264, 1412, 3677, 260, 23771, 264, 2031, 15245, 293, 264, 288], "temperature": 0.0, "avg_logprob": -0.10269731283187866, "compression_ratio": 1.4912280701754386, "no_speech_prob": 1.8266251800014288e-07}, {"id": 320, "seek": 229712, "start": 2297.12, "end": 2306.8399999999997, "text": " batch calculate the gradient prediction loss backward go through each of the parameters", "tokens": [15245, 8873, 264, 16235, 17630, 4470, 23897, 352, 807, 1184, 295, 264, 9834], "temperature": 0.0, "avg_logprob": -0.11515478360450875, "compression_ratio": 1.6604938271604939, "no_speech_prob": 5.89641103942995e-07}, {"id": 321, "seek": 229712, "start": 2306.8399999999997, "end": 2313.0, "text": " we're going to be passing those in so there's going to be the 768 weights and the one bias", "tokens": [321, 434, 516, 281, 312, 8437, 729, 294, 370, 456, 311, 516, 281, 312, 264, 24733, 23, 17443, 293, 264, 472, 12577], "temperature": 0.0, "avg_logprob": -0.11515478360450875, "compression_ratio": 1.6604938271604939, "no_speech_prob": 5.89641103942995e-07}, {"id": 322, "seek": 229712, "start": 2313.0, "end": 2320.18, "text": " and then for each of those update the parameter to go minus equals gradient times learning", "tokens": [293, 550, 337, 1184, 295, 729, 5623, 264, 13075, 281, 352, 3175, 6915, 16235, 1413, 2539], "temperature": 0.0, "avg_logprob": -0.11515478360450875, "compression_ratio": 1.6604938271604939, "no_speech_prob": 5.89641103942995e-07}, {"id": 323, "seek": 232018, "start": 2320.18, "end": 2327.3999999999996, "text": " rate that's our gradient descent step and then zero it out for the next time around", "tokens": [3314, 300, 311, 527, 16235, 23475, 1823, 293, 550, 4018, 309, 484, 337, 264, 958, 565, 926], "temperature": 0.0, "avg_logprob": -0.09385358856385012, "compression_ratio": 1.801047120418848, "no_speech_prob": 1.798306357159163e-07}, {"id": 324, "seek": 232018, "start": 2327.3999999999996, "end": 2335.68, "text": " the loop I'm not just saying P minus equals I'm saying P dot data minus equals and the", "tokens": [264, 6367, 286, 478, 406, 445, 1566, 430, 3175, 6915, 286, 478, 1566, 430, 5893, 1412, 3175, 6915, 293, 264], "temperature": 0.0, "avg_logprob": -0.09385358856385012, "compression_ratio": 1.801047120418848, "no_speech_prob": 1.798306357159163e-07}, {"id": 325, "seek": 232018, "start": 2335.68, "end": 2342.0, "text": " reason for that is that remember pytorch keeps track of all of the calculations we do so", "tokens": [1778, 337, 300, 307, 300, 1604, 25878, 284, 339, 5965, 2837, 295, 439, 295, 264, 20448, 321, 360, 370], "temperature": 0.0, "avg_logprob": -0.09385358856385012, "compression_ratio": 1.801047120418848, "no_speech_prob": 1.798306357159163e-07}, {"id": 326, "seek": 232018, "start": 2342.0, "end": 2347.04, "text": " that it can calculate the gradient well I don't want to calculate in the gradient of", "tokens": [300, 309, 393, 8873, 264, 16235, 731, 286, 500, 380, 528, 281, 8873, 294, 264, 16235, 295], "temperature": 0.0, "avg_logprob": -0.09385358856385012, "compression_ratio": 1.801047120418848, "no_speech_prob": 1.798306357159163e-07}, {"id": 327, "seek": 234704, "start": 2347.04, "end": 2354.16, "text": " my gradient descent step that's like not part of the bottle right so dot data is a special", "tokens": [452, 16235, 23475, 1823, 300, 311, 411, 406, 644, 295, 264, 7817, 558, 370, 5893, 1412, 307, 257, 2121], "temperature": 0.0, "avg_logprob": -0.07790027345929827, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.893604336255521e-07}, {"id": 328, "seek": 234704, "start": 2354.16, "end": 2362.7599999999998, "text": " attribute in pytorch where if you write to it it tells pytorch not to update the gradients", "tokens": [19667, 294, 25878, 284, 339, 689, 498, 291, 2464, 281, 309, 309, 5112, 25878, 284, 339, 406, 281, 5623, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.07790027345929827, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.893604336255521e-07}, {"id": 329, "seek": 234704, "start": 2362.7599999999998, "end": 2370.72, "text": " using that calculation so this is your most basic standard SGD stochastic gradient descent", "tokens": [1228, 300, 17108, 370, 341, 307, 428, 881, 3875, 3832, 34520, 35, 342, 8997, 2750, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.07790027345929827, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.893604336255521e-07}, {"id": 330, "seek": 234704, "start": 2370.72, "end": 2375.16, "text": " loop so now we can answer that earlier question the difference between stochastic gradient", "tokens": [6367, 370, 586, 321, 393, 1867, 300, 3071, 1168, 264, 2649, 1296, 342, 8997, 2750, 16235], "temperature": 0.0, "avg_logprob": -0.07790027345929827, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.893604336255521e-07}, {"id": 331, "seek": 237516, "start": 2375.16, "end": 2383.52, "text": " descent and gradient descent is that gradient descent does not have this here that loops", "tokens": [23475, 293, 16235, 23475, 307, 300, 16235, 23475, 775, 406, 362, 341, 510, 300, 16121], "temperature": 0.0, "avg_logprob": -0.11082433064778646, "compression_ratio": 2.064327485380117, "no_speech_prob": 9.570819656801177e-07}, {"id": 332, "seek": 237516, "start": 2383.52, "end": 2389.6, "text": " through each mini batch for gradient descent it does it on the whole data set each time", "tokens": [807, 1184, 8382, 15245, 337, 16235, 23475, 309, 775, 309, 322, 264, 1379, 1412, 992, 1184, 565], "temperature": 0.0, "avg_logprob": -0.11082433064778646, "compression_ratio": 2.064327485380117, "no_speech_prob": 9.570819656801177e-07}, {"id": 333, "seek": 237516, "start": 2389.6, "end": 2397.24, "text": " around so train epoch or gradient descent would simply not have the for loop at all", "tokens": [926, 370, 3847, 30992, 339, 420, 16235, 23475, 576, 2935, 406, 362, 264, 337, 6367, 412, 439], "temperature": 0.0, "avg_logprob": -0.11082433064778646, "compression_ratio": 2.064327485380117, "no_speech_prob": 9.570819656801177e-07}, {"id": 334, "seek": 237516, "start": 2397.24, "end": 2401.48, "text": " but instead it would calculate the gradient for the whole data set and update the parameters", "tokens": [457, 2602, 309, 576, 8873, 264, 16235, 337, 264, 1379, 1412, 992, 293, 5623, 264, 9834], "temperature": 0.0, "avg_logprob": -0.11082433064778646, "compression_ratio": 2.064327485380117, "no_speech_prob": 9.570819656801177e-07}, {"id": 335, "seek": 240148, "start": 2401.48, "end": 2407.48, "text": " based on the whole data set which we never really do in practice we always use many batches", "tokens": [2361, 322, 264, 1379, 1412, 992, 597, 321, 1128, 534, 360, 294, 3124, 321, 1009, 764, 867, 15245, 279], "temperature": 0.0, "avg_logprob": -0.16814573487239098, "compression_ratio": 1.6742857142857144, "no_speech_prob": 6.179386105031881e-07}, {"id": 336, "seek": 240148, "start": 2407.48, "end": 2416.4, "text": " of various sizes.", "tokens": [295, 3683, 11602, 13], "temperature": 0.0, "avg_logprob": -0.16814573487239098, "compression_ratio": 1.6742857142857144, "no_speech_prob": 6.179386105031881e-07}, {"id": 337, "seek": 240148, "start": 2416.4, "end": 2423.6, "text": " Okay so we can take the function we had before where we compared the predictions to whether", "tokens": [1033, 370, 321, 393, 747, 264, 2445, 321, 632, 949, 689, 321, 5347, 264, 21264, 281, 1968], "temperature": 0.0, "avg_logprob": -0.16814573487239098, "compression_ratio": 1.6742857142857144, "no_speech_prob": 6.179386105031881e-07}, {"id": 338, "seek": 240148, "start": 2423.6, "end": 2426.64, "text": " that well we used to be comparing the predictions to whether they were greater or less than", "tokens": [300, 731, 321, 1143, 281, 312, 15763, 264, 21264, 281, 1968, 436, 645, 5044, 420, 1570, 813], "temperature": 0.0, "avg_logprob": -0.16814573487239098, "compression_ratio": 1.6742857142857144, "no_speech_prob": 6.179386105031881e-07}, {"id": 339, "seek": 242664, "start": 2426.64, "end": 2432.3399999999997, "text": " zero right but now that we're doing the sigmoid remember the sigmoid will squish everything", "tokens": [4018, 558, 457, 586, 300, 321, 434, 884, 264, 4556, 3280, 327, 1604, 264, 4556, 3280, 327, 486, 31379, 1203], "temperature": 0.0, "avg_logprob": -0.10441423984284097, "compression_ratio": 1.7524752475247525, "no_speech_prob": 1.994724243559176e-06}, {"id": 340, "seek": 242664, "start": 2432.3399999999997, "end": 2436.3199999999997, "text": " between naught and one so now we should compare the predictions to whether they're greater", "tokens": [1296, 13138, 293, 472, 370, 586, 321, 820, 6794, 264, 21264, 281, 1968, 436, 434, 5044], "temperature": 0.0, "avg_logprob": -0.10441423984284097, "compression_ratio": 1.7524752475247525, "no_speech_prob": 1.994724243559176e-06}, {"id": 341, "seek": 242664, "start": 2436.3199999999997, "end": 2441.56, "text": " than 0.5 or not if they're greater than 0.5 just to look back at our sigmoid function", "tokens": [813, 1958, 13, 20, 420, 406, 498, 436, 434, 5044, 813, 1958, 13, 20, 445, 281, 574, 646, 412, 527, 4556, 3280, 327, 2445], "temperature": 0.0, "avg_logprob": -0.10441423984284097, "compression_ratio": 1.7524752475247525, "no_speech_prob": 1.994724243559176e-06}, {"id": 342, "seek": 242664, "start": 2441.56, "end": 2452.8399999999997, "text": " though zero what used to be zero is now on the sigmoid is 0.5 okay so we need to just", "tokens": [1673, 4018, 437, 1143, 281, 312, 4018, 307, 586, 322, 264, 4556, 3280, 327, 307, 1958, 13, 20, 1392, 370, 321, 643, 281, 445], "temperature": 0.0, "avg_logprob": -0.10441423984284097, "compression_ratio": 1.7524752475247525, "no_speech_prob": 1.994724243559176e-06}, {"id": 343, "seek": 245284, "start": 2452.84, "end": 2462.44, "text": " to make that slight change to our measure of accuracy.", "tokens": [281, 652, 300, 4036, 1319, 281, 527, 3481, 295, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1391836213476864, "compression_ratio": 1.6631578947368422, "no_speech_prob": 2.419883173843118e-07}, {"id": 344, "seek": 245284, "start": 2462.44, "end": 2467.8, "text": " To calculate the accuracy for some x-batch and some y-batch this is actually assume this", "tokens": [1407, 8873, 264, 14170, 337, 512, 2031, 12, 65, 852, 293, 512, 288, 12, 65, 852, 341, 307, 767, 6552, 341], "temperature": 0.0, "avg_logprob": -0.1391836213476864, "compression_ratio": 1.6631578947368422, "no_speech_prob": 2.419883173843118e-07}, {"id": 345, "seek": 245284, "start": 2467.8, "end": 2473.32, "text": " is actually the predictions then we take the sigmoid of the predictions we compare them", "tokens": [307, 767, 264, 21264, 550, 321, 747, 264, 4556, 3280, 327, 295, 264, 21264, 321, 6794, 552], "temperature": 0.0, "avg_logprob": -0.1391836213476864, "compression_ratio": 1.6631578947368422, "no_speech_prob": 2.419883173843118e-07}, {"id": 346, "seek": 245284, "start": 2473.32, "end": 2478.2400000000002, "text": " to 0.5 to tell us whether it's a three or not we check what the actual target was to", "tokens": [281, 1958, 13, 20, 281, 980, 505, 1968, 309, 311, 257, 1045, 420, 406, 321, 1520, 437, 264, 3539, 3779, 390, 281], "temperature": 0.0, "avg_logprob": -0.1391836213476864, "compression_ratio": 1.6631578947368422, "no_speech_prob": 2.419883173843118e-07}, {"id": 347, "seek": 247824, "start": 2478.24, "end": 2484.12, "text": " see which ones are correct and then we take the mean of those after converting the Boolean", "tokens": [536, 597, 2306, 366, 3006, 293, 550, 321, 747, 264, 914, 295, 729, 934, 29942, 264, 23351, 28499], "temperature": 0.0, "avg_logprob": -0.11006998174330768, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.0188077794737183e-06}, {"id": 348, "seek": 247824, "start": 2484.12, "end": 2491.64, "text": " to floats so we can check that accuracy let's take our batch put it through our simple linear", "tokens": [281, 37878, 370, 321, 393, 1520, 300, 14170, 718, 311, 747, 527, 15245, 829, 309, 807, 527, 2199, 8213], "temperature": 0.0, "avg_logprob": -0.11006998174330768, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.0188077794737183e-06}, {"id": 349, "seek": 247824, "start": 2491.64, "end": 2498.8599999999997, "text": " model compare it to the four items of the training set and there's the accuracy.", "tokens": [2316, 6794, 309, 281, 264, 1451, 4754, 295, 264, 3097, 992, 293, 456, 311, 264, 14170, 13], "temperature": 0.0, "avg_logprob": -0.11006998174330768, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.0188077794737183e-06}, {"id": 350, "seek": 247824, "start": 2498.8599999999997, "end": 2505.4799999999996, "text": " So if we do that for every batch in the validation set then we can loop through with a list comprehension", "tokens": [407, 498, 321, 360, 300, 337, 633, 15245, 294, 264, 24071, 992, 550, 321, 393, 6367, 807, 365, 257, 1329, 44991], "temperature": 0.0, "avg_logprob": -0.11006998174330768, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.0188077794737183e-06}, {"id": 351, "seek": 250548, "start": 2505.48, "end": 2515.4, "text": " every batch in the validation set get the accuracy based on some model stack those all", "tokens": [633, 15245, 294, 264, 24071, 992, 483, 264, 14170, 2361, 322, 512, 2316, 8630, 729, 439], "temperature": 0.0, "avg_logprob": -0.13126777467273532, "compression_ratio": 1.794871794871795, "no_speech_prob": 1.4144748092803638e-06}, {"id": 352, "seek": 250548, "start": 2515.4, "end": 2520.72, "text": " up together so that this is a list right if we want to turn that list into a tensor where", "tokens": [493, 1214, 370, 300, 341, 307, 257, 1329, 558, 498, 321, 528, 281, 1261, 300, 1329, 666, 257, 40863, 689], "temperature": 0.0, "avg_logprob": -0.13126777467273532, "compression_ratio": 1.794871794871795, "no_speech_prob": 1.4144748092803638e-06}, {"id": 353, "seek": 250548, "start": 2520.72, "end": 2525.2, "text": " the the items of the list of the tensor are the items of the list that's what stack does", "tokens": [264, 264, 4754, 295, 264, 1329, 295, 264, 40863, 366, 264, 4754, 295, 264, 1329, 300, 311, 437, 8630, 775], "temperature": 0.0, "avg_logprob": -0.13126777467273532, "compression_ratio": 1.794871794871795, "no_speech_prob": 1.4144748092803638e-06}, {"id": 354, "seek": 250548, "start": 2525.2, "end": 2533.92, "text": " so we can stack up all those take the mean convert it to a standard Python scalar by", "tokens": [370, 321, 393, 8630, 493, 439, 729, 747, 264, 914, 7620, 309, 281, 257, 3832, 15329, 39684, 538], "temperature": 0.0, "avg_logprob": -0.13126777467273532, "compression_ratio": 1.794871794871795, "no_speech_prob": 1.4144748092803638e-06}, {"id": 355, "seek": 253392, "start": 2533.92, "end": 2541.4, "text": " calling dot item round it to four decimal places just for display and so here is our", "tokens": [5141, 5893, 3174, 3098, 309, 281, 1451, 26601, 3190, 445, 337, 4674, 293, 370, 510, 307, 527], "temperature": 0.0, "avg_logprob": -0.08160316199064255, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.4144720807962585e-06}, {"id": 356, "seek": 253392, "start": 2541.4, "end": 2548.08, "text": " validation set accuracy as you would expect it's about 50% because it's random so we can", "tokens": [24071, 992, 14170, 382, 291, 576, 2066, 309, 311, 466, 2625, 4, 570, 309, 311, 4974, 370, 321, 393], "temperature": 0.0, "avg_logprob": -0.08160316199064255, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.4144720807962585e-06}, {"id": 357, "seek": 253392, "start": 2548.08, "end": 2557.44, "text": " now train for one epoch so we can say remember train epoch needed the parameters so our parameters", "tokens": [586, 3847, 337, 472, 30992, 339, 370, 321, 393, 584, 1604, 3847, 30992, 339, 2978, 264, 9834, 370, 527, 9834], "temperature": 0.0, "avg_logprob": -0.08160316199064255, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.4144720807962585e-06}, {"id": 358, "seek": 255744, "start": 2557.44, "end": 2564.68, "text": " in this case are the weights tensor and the bias tensor so train one epoch using the linear", "tokens": [294, 341, 1389, 366, 264, 17443, 40863, 293, 264, 12577, 40863, 370, 3847, 472, 30992, 339, 1228, 264, 8213], "temperature": 0.0, "avg_logprob": -0.13078933773618756, "compression_ratio": 1.6645962732919255, "no_speech_prob": 6.475938789662905e-07}, {"id": 359, "seek": 255744, "start": 2564.68, "end": 2570.88, "text": " one model with the learning with the learning rate of one with these two parameters and", "tokens": [472, 2316, 365, 264, 2539, 365, 264, 2539, 3314, 295, 472, 365, 613, 732, 9834, 293], "temperature": 0.0, "avg_logprob": -0.13078933773618756, "compression_ratio": 1.6645962732919255, "no_speech_prob": 6.475938789662905e-07}, {"id": 360, "seek": 255744, "start": 2570.88, "end": 2581.7200000000003, "text": " then validate and look at that our accuracy is now 68.8% so we've we've trained an epoch", "tokens": [550, 29562, 293, 574, 412, 300, 527, 14170, 307, 586, 23317, 13, 23, 4, 370, 321, 600, 321, 600, 8895, 364, 30992, 339], "temperature": 0.0, "avg_logprob": -0.13078933773618756, "compression_ratio": 1.6645962732919255, "no_speech_prob": 6.475938789662905e-07}, {"id": 361, "seek": 258172, "start": 2581.72, "end": 2589.0, "text": " so let's just repeat that 20 times train and validate and you can see the accuracy goes", "tokens": [370, 718, 311, 445, 7149, 300, 945, 1413, 3847, 293, 29562, 293, 291, 393, 536, 264, 14170, 1709], "temperature": 0.0, "avg_logprob": -0.12143802642822266, "compression_ratio": 1.5229885057471264, "no_speech_prob": 9.57081056185416e-07}, {"id": 362, "seek": 258172, "start": 2589.0, "end": 2600.3999999999996, "text": " up and up and up and up and up to about 97% so that's cool we've built an SGD optimizer", "tokens": [493, 293, 493, 293, 493, 293, 493, 293, 493, 281, 466, 23399, 4, 370, 300, 311, 1627, 321, 600, 3094, 364, 34520, 35, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.12143802642822266, "compression_ratio": 1.5229885057471264, "no_speech_prob": 9.57081056185416e-07}, {"id": 363, "seek": 258172, "start": 2600.3999999999996, "end": 2607.8399999999997, "text": " of a simple linear function that is getting about 97% on our simplified M list where this", "tokens": [295, 257, 2199, 8213, 2445, 300, 307, 1242, 466, 23399, 4, 322, 527, 26335, 376, 1329, 689, 341], "temperature": 0.0, "avg_logprob": -0.12143802642822266, "compression_ratio": 1.5229885057471264, "no_speech_prob": 9.57081056185416e-07}, {"id": 364, "seek": 260784, "start": 2607.84, "end": 2615.4, "text": " is just the threes and the sevens so a lot of steps there let's simplify this through", "tokens": [307, 445, 264, 258, 4856, 293, 264, 3407, 82, 370, 257, 688, 295, 4439, 456, 718, 311, 20460, 341, 807], "temperature": 0.0, "avg_logprob": -0.10025900536841088, "compression_ratio": 1.8808290155440415, "no_speech_prob": 1.933349949467811e-06}, {"id": 365, "seek": 260784, "start": 2615.4, "end": 2620.6400000000003, "text": " some refactoring so the kind of simple refactoring we're going to do we're going to do a couple", "tokens": [512, 1895, 578, 3662, 370, 264, 733, 295, 2199, 1895, 578, 3662, 321, 434, 516, 281, 360, 321, 434, 516, 281, 360, 257, 1916], "temperature": 0.0, "avg_logprob": -0.10025900536841088, "compression_ratio": 1.8808290155440415, "no_speech_prob": 1.933349949467811e-06}, {"id": 366, "seek": 260784, "start": 2620.6400000000003, "end": 2625.44, "text": " but the basic idea is we're going to create something called an optimizer class the first", "tokens": [457, 264, 3875, 1558, 307, 321, 434, 516, 281, 1884, 746, 1219, 364, 5028, 6545, 1508, 264, 700], "temperature": 0.0, "avg_logprob": -0.10025900536841088, "compression_ratio": 1.8808290155440415, "no_speech_prob": 1.933349949467811e-06}, {"id": 367, "seek": 260784, "start": 2625.44, "end": 2633.52, "text": " thing we'll do is we'll get rid of the linear one function remember the linear one function", "tokens": [551, 321, 603, 360, 307, 321, 603, 483, 3973, 295, 264, 8213, 472, 2445, 1604, 264, 8213, 472, 2445], "temperature": 0.0, "avg_logprob": -0.10025900536841088, "compression_ratio": 1.8808290155440415, "no_speech_prob": 1.933349949467811e-06}, {"id": 368, "seek": 263352, "start": 2633.52, "end": 2643.7599999999998, "text": " does x at w plus b there's actually a class in pytorch that does that equation for us", "tokens": [775, 2031, 412, 261, 1804, 272, 456, 311, 767, 257, 1508, 294, 25878, 284, 339, 300, 775, 300, 5367, 337, 505], "temperature": 0.0, "avg_logprob": -0.1097178974667111, "compression_ratio": 1.675, "no_speech_prob": 1.154456299445883e-06}, {"id": 369, "seek": 263352, "start": 2643.7599999999998, "end": 2651.48, "text": " so we may as well use it it's called nn.linear and nn.linear does two things it does that", "tokens": [370, 321, 815, 382, 731, 764, 309, 309, 311, 1219, 297, 77, 13, 28263, 293, 297, 77, 13, 28263, 775, 732, 721, 309, 775, 300], "temperature": 0.0, "avg_logprob": -0.1097178974667111, "compression_ratio": 1.675, "no_speech_prob": 1.154456299445883e-06}, {"id": 370, "seek": 263352, "start": 2651.48, "end": 2659.7599999999998, "text": " function for us and it also initializes the parameters for us so we don't have to do weights", "tokens": [2445, 337, 505, 293, 309, 611, 5883, 5660, 264, 9834, 337, 505, 370, 321, 500, 380, 362, 281, 360, 17443], "temperature": 0.0, "avg_logprob": -0.1097178974667111, "compression_ratio": 1.675, "no_speech_prob": 1.154456299445883e-06}, {"id": 371, "seek": 265976, "start": 2659.76, "end": 2667.84, "text": " and bias in it params anymore we just create an nn.linear class and that's going to create", "tokens": [293, 12577, 294, 309, 971, 4070, 3602, 321, 445, 1884, 364, 297, 77, 13, 28263, 1508, 293, 300, 311, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.10654789871639675, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.457991001847404e-07}, {"id": 372, "seek": 265976, "start": 2667.84, "end": 2676.32, "text": " a matrix of size 28 by 28 comma 1 and a bias of size 1 it will set requires great equals", "tokens": [257, 8141, 295, 2744, 7562, 538, 7562, 22117, 502, 293, 257, 12577, 295, 2744, 502, 309, 486, 992, 7029, 869, 6915], "temperature": 0.0, "avg_logprob": -0.10654789871639675, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.457991001847404e-07}, {"id": 373, "seek": 265976, "start": 2676.32, "end": 2682.2000000000003, "text": " true for us it's all going to be encapsulated in this class and then when I call that as", "tokens": [2074, 337, 505, 309, 311, 439, 516, 281, 312, 38745, 6987, 294, 341, 1508, 293, 550, 562, 286, 818, 300, 382], "temperature": 0.0, "avg_logprob": -0.10654789871639675, "compression_ratio": 1.576470588235294, "no_speech_prob": 2.457991001847404e-07}, {"id": 374, "seek": 268220, "start": 2682.2, "end": 2692.7999999999997, "text": " a function it's going to do my x at w plus b so to see the parameters in it we would", "tokens": [257, 2445, 309, 311, 516, 281, 360, 452, 2031, 412, 261, 1804, 272, 370, 281, 536, 264, 9834, 294, 309, 321, 576], "temperature": 0.0, "avg_logprob": -0.12088539230991417, "compression_ratio": 1.628930817610063, "no_speech_prob": 1.637375675045405e-07}, {"id": 375, "seek": 268220, "start": 2692.7999999999997, "end": 2699.2, "text": " expect it to contain 784 weights and one bias we can just call dot parameters and we can", "tokens": [2066, 309, 281, 5304, 1614, 25494, 17443, 293, 472, 12577, 321, 393, 445, 818, 5893, 9834, 293, 321, 393], "temperature": 0.0, "avg_logprob": -0.12088539230991417, "compression_ratio": 1.628930817610063, "no_speech_prob": 1.637375675045405e-07}, {"id": 376, "seek": 268220, "start": 2699.2, "end": 2708.6, "text": " destructure it to w comma b and see yep it is 784 and one for the weights and bias so", "tokens": [2677, 2885, 309, 281, 261, 22117, 272, 293, 536, 18633, 309, 307, 1614, 25494, 293, 472, 337, 264, 17443, 293, 12577, 370], "temperature": 0.0, "avg_logprob": -0.12088539230991417, "compression_ratio": 1.628930817610063, "no_speech_prob": 1.637375675045405e-07}, {"id": 377, "seek": 270860, "start": 2708.6, "end": 2713.0, "text": " that's cool so this is just you could you know it could be an interesting exercise for", "tokens": [300, 311, 1627, 370, 341, 307, 445, 291, 727, 291, 458, 309, 727, 312, 364, 1880, 5380, 337], "temperature": 0.0, "avg_logprob": -0.09788603782653808, "compression_ratio": 1.7772277227722773, "no_speech_prob": 3.9897122405818664e-07}, {"id": 378, "seek": 270860, "start": 2713.0, "end": 2719.6, "text": " you to create this class yourself from scratch you should be able to at this point so that", "tokens": [291, 281, 1884, 341, 1508, 1803, 490, 8459, 291, 820, 312, 1075, 281, 412, 341, 935, 370, 300], "temperature": 0.0, "avg_logprob": -0.09788603782653808, "compression_ratio": 1.7772277227722773, "no_speech_prob": 3.9897122405818664e-07}, {"id": 379, "seek": 270860, "start": 2719.6, "end": 2726.7, "text": " you can confirm that you can recreate something that behaves exactly like an n.linear so now", "tokens": [291, 393, 9064, 300, 291, 393, 25833, 746, 300, 36896, 2293, 411, 364, 297, 13, 28263, 370, 586], "temperature": 0.0, "avg_logprob": -0.09788603782653808, "compression_ratio": 1.7772277227722773, "no_speech_prob": 3.9897122405818664e-07}, {"id": 380, "seek": 270860, "start": 2726.7, "end": 2733.92, "text": " that we've got this object which contains our parameters in a parameters method we could", "tokens": [300, 321, 600, 658, 341, 2657, 597, 8306, 527, 9834, 294, 257, 9834, 3170, 321, 727], "temperature": 0.0, "avg_logprob": -0.09788603782653808, "compression_ratio": 1.7772277227722773, "no_speech_prob": 3.9897122405818664e-07}, {"id": 381, "seek": 273392, "start": 2733.92, "end": 2739.42, "text": " now create an optimizer so for our optimizer we're going to pass it the parameters to optimize", "tokens": [586, 1884, 364, 5028, 6545, 370, 337, 527, 5028, 6545, 321, 434, 516, 281, 1320, 309, 264, 9834, 281, 19719], "temperature": 0.0, "avg_logprob": -0.08635152303255521, "compression_ratio": 1.9954545454545454, "no_speech_prob": 8.446208994428162e-07}, {"id": 382, "seek": 273392, "start": 2739.42, "end": 2745.64, "text": " and a learning rate we'll store them away and we'll have something called step which", "tokens": [293, 257, 2539, 3314, 321, 603, 3531, 552, 1314, 293, 321, 603, 362, 746, 1219, 1823, 597], "temperature": 0.0, "avg_logprob": -0.08635152303255521, "compression_ratio": 1.9954545454545454, "no_speech_prob": 8.446208994428162e-07}, {"id": 383, "seek": 273392, "start": 2745.64, "end": 2751.44, "text": " goes through each parameter and does that thing we just saw p.data minus equals p.grad", "tokens": [1709, 807, 1184, 13075, 293, 775, 300, 551, 321, 445, 1866, 280, 13, 67, 3274, 3175, 6915, 280, 13, 7165], "temperature": 0.0, "avg_logprob": -0.08635152303255521, "compression_ratio": 1.9954545454545454, "no_speech_prob": 8.446208994428162e-07}, {"id": 384, "seek": 273392, "start": 2751.44, "end": 2756.06, "text": " times learning rate and it's also going to have something called zero grad which goes", "tokens": [1413, 2539, 3314, 293, 309, 311, 611, 516, 281, 362, 746, 1219, 4018, 2771, 597, 1709], "temperature": 0.0, "avg_logprob": -0.08635152303255521, "compression_ratio": 1.9954545454545454, "no_speech_prob": 8.446208994428162e-07}, {"id": 385, "seek": 273392, "start": 2756.06, "end": 2761.96, "text": " through each parameter and zeros it out or we could even just set it to none so that's", "tokens": [807, 1184, 13075, 293, 35193, 309, 484, 420, 321, 727, 754, 445, 992, 309, 281, 6022, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.08635152303255521, "compression_ratio": 1.9954545454545454, "no_speech_prob": 8.446208994428162e-07}, {"id": 386, "seek": 276196, "start": 2761.96, "end": 2766.36, "text": " the thing we're going to call basic optimizer so those are exactly the same lines of code", "tokens": [264, 551, 321, 434, 516, 281, 818, 3875, 5028, 6545, 370, 729, 366, 2293, 264, 912, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.08861993551254273, "compression_ratio": 1.6763285024154588, "no_speech_prob": 5.122890343045583e-07}, {"id": 387, "seek": 276196, "start": 2766.36, "end": 2771.96, "text": " we've already seen wrapped up into a class so we can now create an optimizer passing", "tokens": [321, 600, 1217, 1612, 14226, 493, 666, 257, 1508, 370, 321, 393, 586, 1884, 364, 5028, 6545, 8437], "temperature": 0.0, "avg_logprob": -0.08861993551254273, "compression_ratio": 1.6763285024154588, "no_speech_prob": 5.122890343045583e-07}, {"id": 388, "seek": 276196, "start": 2771.96, "end": 2778.44, "text": " in the parameters of the linear model for these and our learning rate and so now our", "tokens": [294, 264, 9834, 295, 264, 8213, 2316, 337, 613, 293, 527, 2539, 3314, 293, 370, 586, 527], "temperature": 0.0, "avg_logprob": -0.08861993551254273, "compression_ratio": 1.6763285024154588, "no_speech_prob": 5.122890343045583e-07}, {"id": 389, "seek": 276196, "start": 2778.44, "end": 2784.78, "text": " training loop is look through each mini batch in the data loader calculate the gradient", "tokens": [3097, 6367, 307, 574, 807, 1184, 8382, 15245, 294, 264, 1412, 3677, 260, 8873, 264, 16235], "temperature": 0.0, "avg_logprob": -0.08861993551254273, "compression_ratio": 1.6763285024154588, "no_speech_prob": 5.122890343045583e-07}, {"id": 390, "seek": 278478, "start": 2784.78, "end": 2796.0800000000004, "text": " opt dot step opt dot zero grad that's it validation function doesn't have to change and so let's", "tokens": [2427, 5893, 1823, 2427, 5893, 4018, 2771, 300, 311, 309, 24071, 2445, 1177, 380, 362, 281, 1319, 293, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.08415719046108965, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0348513796998304e-06}, {"id": 391, "seek": 278478, "start": 2796.0800000000004, "end": 2800.96, "text": " put our training loop into a function that's going to loop through a bunch of epochs call", "tokens": [829, 527, 3097, 6367, 666, 257, 2445, 300, 311, 516, 281, 6367, 807, 257, 3840, 295, 30992, 28346, 818], "temperature": 0.0, "avg_logprob": -0.08415719046108965, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0348513796998304e-06}, {"id": 392, "seek": 278478, "start": 2800.96, "end": 2810.2400000000002, "text": " an epoch print validate epoch and then run it and it's the same we're getting a slightly", "tokens": [364, 30992, 339, 4482, 29562, 30992, 339, 293, 550, 1190, 309, 293, 309, 311, 264, 912, 321, 434, 1242, 257, 4748], "temperature": 0.0, "avg_logprob": -0.08415719046108965, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0348513796998304e-06}, {"id": 393, "seek": 281024, "start": 2810.24, "end": 2822.2799999999997, "text": " different result here but much much the same idea okay so that's cool right we've now refactoring", "tokens": [819, 1874, 510, 457, 709, 709, 264, 912, 1558, 1392, 370, 300, 311, 1627, 558, 321, 600, 586, 1895, 578, 3662], "temperature": 0.0, "avg_logprob": -0.16280496821683996, "compression_ratio": 1.5813953488372092, "no_speech_prob": 2.76939772447804e-06}, {"id": 394, "seek": 281024, "start": 2822.2799999999997, "end": 2828.7599999999998, "text": " using you know creating our own optimizer and using faster pytorch is built in nn dot", "tokens": [1228, 291, 458, 4084, 527, 1065, 5028, 6545, 293, 1228, 4663, 25878, 284, 339, 307, 3094, 294, 297, 77, 5893], "temperature": 0.0, "avg_logprob": -0.16280496821683996, "compression_ratio": 1.5813953488372092, "no_speech_prob": 2.76939772447804e-06}, {"id": 395, "seek": 281024, "start": 2828.7599999999998, "end": 2835.52, "text": " linear class and you know by the way we don't actually need to use our own basic optimum", "tokens": [8213, 1508, 293, 291, 458, 538, 264, 636, 321, 500, 380, 767, 643, 281, 764, 527, 1065, 3875, 39326], "temperature": 0.0, "avg_logprob": -0.16280496821683996, "compression_ratio": 1.5813953488372092, "no_speech_prob": 2.76939772447804e-06}, {"id": 396, "seek": 283552, "start": 2835.52, "end": 2840.92, "text": " not surprisingly pytorch comes with something which does exactly this and not surprisingly", "tokens": [406, 17600, 25878, 284, 339, 1487, 365, 746, 597, 775, 2293, 341, 293, 406, 17600], "temperature": 0.0, "avg_logprob": -0.07964096512905387, "compression_ratio": 1.7607655502392345, "no_speech_prob": 4.3818383232974156e-07}, {"id": 397, "seek": 283552, "start": 2840.92, "end": 2847.68, "text": " it's called sgd so and actually this sgd is provided by fastai fastai and pytorch provides", "tokens": [309, 311, 1219, 262, 70, 67, 370, 293, 767, 341, 262, 70, 67, 307, 5649, 538, 2370, 1301, 2370, 1301, 293, 25878, 284, 339, 6417], "temperature": 0.0, "avg_logprob": -0.07964096512905387, "compression_ratio": 1.7607655502392345, "no_speech_prob": 4.3818383232974156e-07}, {"id": 398, "seek": 283552, "start": 2847.68, "end": 2855.92, "text": " some overlapping functionality they work much the same way so you can pass to sgd your parameters", "tokens": [512, 33535, 14980, 436, 589, 709, 264, 912, 636, 370, 291, 393, 1320, 281, 262, 70, 67, 428, 9834], "temperature": 0.0, "avg_logprob": -0.07964096512905387, "compression_ratio": 1.7607655502392345, "no_speech_prob": 4.3818383232974156e-07}, {"id": 399, "seek": 283552, "start": 2855.92, "end": 2864.78, "text": " and your learning rate just like basic optimum okay and train it and get the same result", "tokens": [293, 428, 2539, 3314, 445, 411, 3875, 39326, 1392, 293, 3847, 309, 293, 483, 264, 912, 1874], "temperature": 0.0, "avg_logprob": -0.07964096512905387, "compression_ratio": 1.7607655502392345, "no_speech_prob": 4.3818383232974156e-07}, {"id": 400, "seek": 286478, "start": 2864.78, "end": 2871.6400000000003, "text": " so as you can see these classes that are in fastai and pytorch are not mysterious they're", "tokens": [370, 382, 291, 393, 536, 613, 5359, 300, 366, 294, 2370, 1301, 293, 25878, 284, 339, 366, 406, 13831, 436, 434], "temperature": 0.0, "avg_logprob": -0.08440751253172409, "compression_ratio": 1.6859903381642511, "no_speech_prob": 1.5056975826155394e-06}, {"id": 401, "seek": 286478, "start": 2871.6400000000003, "end": 2880.36, "text": " just pretty you know thin wrappers around functionality that we've now written ourself", "tokens": [445, 1238, 291, 458, 5862, 7843, 15226, 926, 14980, 300, 321, 600, 586, 3720, 527, 927], "temperature": 0.0, "avg_logprob": -0.08440751253172409, "compression_ratio": 1.6859903381642511, "no_speech_prob": 1.5056975826155394e-06}, {"id": 402, "seek": 286478, "start": 2880.36, "end": 2885.94, "text": " so there's quite a few steps there and if you haven't done gradient descent before then", "tokens": [370, 456, 311, 1596, 257, 1326, 4439, 456, 293, 498, 291, 2378, 380, 1096, 16235, 23475, 949, 550], "temperature": 0.0, "avg_logprob": -0.08440751253172409, "compression_ratio": 1.6859903381642511, "no_speech_prob": 1.5056975826155394e-06}, {"id": 403, "seek": 286478, "start": 2885.94, "end": 2893.88, "text": " there's a lot of unpacking so so this this lesson is kind of the key lesson it's the", "tokens": [456, 311, 257, 688, 295, 26699, 278, 370, 370, 341, 341, 6898, 307, 733, 295, 264, 2141, 6898, 309, 311, 264], "temperature": 0.0, "avg_logprob": -0.08440751253172409, "compression_ratio": 1.6859903381642511, "no_speech_prob": 1.5056975826155394e-06}, {"id": 404, "seek": 289388, "start": 2893.88, "end": 2899.52, "text": " one where you know like we should you know really take a stop and a deep breath at this", "tokens": [472, 689, 291, 458, 411, 321, 820, 291, 458, 534, 747, 257, 1590, 293, 257, 2452, 6045, 412, 341], "temperature": 0.0, "avg_logprob": -0.09656390936478325, "compression_ratio": 1.7897435897435898, "no_speech_prob": 3.393109636817826e-06}, {"id": 405, "seek": 289388, "start": 2899.52, "end": 2905.7200000000003, "text": " point and make sure you're comfortable what's a data set what's a data loader what's nn", "tokens": [935, 293, 652, 988, 291, 434, 4619, 437, 311, 257, 1412, 992, 437, 311, 257, 1412, 3677, 260, 437, 311, 297, 77], "temperature": 0.0, "avg_logprob": -0.09656390936478325, "compression_ratio": 1.7897435897435898, "no_speech_prob": 3.393109636817826e-06}, {"id": 406, "seek": 289388, "start": 2905.7200000000003, "end": 2911.84, "text": " dot linear what's sgd and if you you know if what any any or all of those don't make", "tokens": [5893, 8213, 437, 311, 262, 70, 67, 293, 498, 291, 291, 458, 498, 437, 604, 604, 420, 439, 295, 729, 500, 380, 652], "temperature": 0.0, "avg_logprob": -0.09656390936478325, "compression_ratio": 1.7897435897435898, "no_speech_prob": 3.393109636817826e-06}, {"id": 407, "seek": 289388, "start": 2911.84, "end": 2918.6, "text": " sense go back to where we defined it from scratch using python code well the data loader", "tokens": [2020, 352, 646, 281, 689, 321, 7642, 309, 490, 8459, 1228, 38797, 3089, 731, 264, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.09656390936478325, "compression_ratio": 1.7897435897435898, "no_speech_prob": 3.393109636817826e-06}, {"id": 408, "seek": 291860, "start": 2918.6, "end": 2924.24, "text": " we didn't define from scratch but it you know the functionality is not particularly interesting", "tokens": [321, 994, 380, 6964, 490, 8459, 457, 309, 291, 458, 264, 14980, 307, 406, 4098, 1880], "temperature": 0.0, "avg_logprob": -0.07522341142217796, "compression_ratio": 1.705607476635514, "no_speech_prob": 1.4593671266993624e-06}, {"id": 409, "seek": 291860, "start": 2924.24, "end": 2928.64, "text": " you could certainly create your own from scratch if you wanted to that would be another pretty", "tokens": [291, 727, 3297, 1884, 428, 1065, 490, 8459, 498, 291, 1415, 281, 300, 576, 312, 1071, 1238], "temperature": 0.0, "avg_logprob": -0.07522341142217796, "compression_ratio": 1.705607476635514, "no_speech_prob": 1.4593671266993624e-06}, {"id": 410, "seek": 291860, "start": 2928.64, "end": 2939.6, "text": " good exercise let's refactor some more fastai has a data loaders class which is as we've", "tokens": [665, 5380, 718, 311, 1895, 15104, 512, 544, 2370, 1301, 575, 257, 1412, 3677, 433, 1508, 597, 307, 382, 321, 600], "temperature": 0.0, "avg_logprob": -0.07522341142217796, "compression_ratio": 1.705607476635514, "no_speech_prob": 1.4593671266993624e-06}, {"id": 411, "seek": 291860, "start": 2939.6, "end": 2945.72, "text": " mentioned before is a tiny class that just you pass it a bunch of data loaders and it", "tokens": [2835, 949, 307, 257, 5870, 1508, 300, 445, 291, 1320, 309, 257, 3840, 295, 1412, 3677, 433, 293, 309], "temperature": 0.0, "avg_logprob": -0.07522341142217796, "compression_ratio": 1.705607476635514, "no_speech_prob": 1.4593671266993624e-06}, {"id": 412, "seek": 294572, "start": 2945.72, "end": 2951.3199999999997, "text": " just stores them away as a dot train and a dot valid even though it's a tiny class it's", "tokens": [445, 9512, 552, 1314, 382, 257, 5893, 3847, 293, 257, 5893, 7363, 754, 1673, 309, 311, 257, 5870, 1508, 309, 311], "temperature": 0.0, "avg_logprob": -0.07286080324424887, "compression_ratio": 1.9285714285714286, "no_speech_prob": 6.681493687210605e-07}, {"id": 413, "seek": 294572, "start": 2951.3199999999997, "end": 2958.12, "text": " it's super handy because with that we now have a single object that knows all the data", "tokens": [309, 311, 1687, 13239, 570, 365, 300, 321, 586, 362, 257, 2167, 2657, 300, 3255, 439, 264, 1412], "temperature": 0.0, "avg_logprob": -0.07286080324424887, "compression_ratio": 1.9285714285714286, "no_speech_prob": 6.681493687210605e-07}, {"id": 414, "seek": 294572, "start": 2958.12, "end": 2963.04, "text": " we have and so it can make sure that your training data loader is shuffled and your", "tokens": [321, 362, 293, 370, 309, 393, 652, 988, 300, 428, 3097, 1412, 3677, 260, 307, 402, 33974, 293, 428], "temperature": 0.0, "avg_logprob": -0.07286080324424887, "compression_ratio": 1.9285714285714286, "no_speech_prob": 6.681493687210605e-07}, {"id": 415, "seek": 294572, "start": 2963.04, "end": 2968.64, "text": " validation loader isn't shuffled you know make sure everything works properly so that's", "tokens": [24071, 3677, 260, 1943, 380, 402, 33974, 291, 458, 652, 988, 1203, 1985, 6108, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.07286080324424887, "compression_ratio": 1.9285714285714286, "no_speech_prob": 6.681493687210605e-07}, {"id": 416, "seek": 294572, "start": 2968.64, "end": 2973.8799999999997, "text": " what the data loaders class is you can pass in the training and valid data loader and", "tokens": [437, 264, 1412, 3677, 433, 1508, 307, 291, 393, 1320, 294, 264, 3097, 293, 7363, 1412, 3677, 260, 293], "temperature": 0.0, "avg_logprob": -0.07286080324424887, "compression_ratio": 1.9285714285714286, "no_speech_prob": 6.681493687210605e-07}, {"id": 417, "seek": 297388, "start": 2973.88, "end": 2979.08, "text": " then the next thing we have in fastai is the learner class and the learner class is something", "tokens": [550, 264, 958, 551, 321, 362, 294, 2370, 1301, 307, 264, 33347, 1508, 293, 264, 33347, 1508, 307, 746], "temperature": 0.0, "avg_logprob": -0.04987919872457331, "compression_ratio": 2.2546583850931676, "no_speech_prob": 2.902305368479574e-06}, {"id": 418, "seek": 297388, "start": 2979.08, "end": 2985.84, "text": " where we're going to pass in our data loaders we're going to pass in our model we're going", "tokens": [689, 321, 434, 516, 281, 1320, 294, 527, 1412, 3677, 433, 321, 434, 516, 281, 1320, 294, 527, 2316, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.04987919872457331, "compression_ratio": 2.2546583850931676, "no_speech_prob": 2.902305368479574e-06}, {"id": 419, "seek": 297388, "start": 2985.84, "end": 2992.08, "text": " to pass in our optimization function we're going to pass in our loss function we're going", "tokens": [281, 1320, 294, 527, 19618, 2445, 321, 434, 516, 281, 1320, 294, 527, 4470, 2445, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.04987919872457331, "compression_ratio": 2.2546583850931676, "no_speech_prob": 2.902305368479574e-06}, {"id": 420, "seek": 297388, "start": 2992.08, "end": 2999.84, "text": " to pass in our metrics so all the stuff we've just done manually that's all learner does", "tokens": [281, 1320, 294, 527, 16367, 370, 439, 264, 1507, 321, 600, 445, 1096, 16945, 300, 311, 439, 33347, 775], "temperature": 0.0, "avg_logprob": -0.04987919872457331, "compression_ratio": 2.2546583850931676, "no_speech_prob": 2.902305368479574e-06}, {"id": 421, "seek": 299984, "start": 2999.84, "end": 3005.84, "text": " is it's just going to do that for us so it's just going to call this train model and this", "tokens": [307, 309, 311, 445, 516, 281, 360, 300, 337, 505, 370, 309, 311, 445, 516, 281, 818, 341, 3847, 2316, 293, 341], "temperature": 0.0, "avg_logprob": -0.07022535695438892, "compression_ratio": 1.9310344827586208, "no_speech_prob": 7.934484642646566e-07}, {"id": 422, "seek": 299984, "start": 3005.84, "end": 3014.6800000000003, "text": " train epoch it's just you know it's inside learner so now if we go learn dot fit you", "tokens": [3847, 30992, 339, 309, 311, 445, 291, 458, 309, 311, 1854, 33347, 370, 586, 498, 321, 352, 1466, 5893, 3318, 291], "temperature": 0.0, "avg_logprob": -0.07022535695438892, "compression_ratio": 1.9310344827586208, "no_speech_prob": 7.934484642646566e-07}, {"id": 423, "seek": 299984, "start": 3014.6800000000003, "end": 3021.1200000000003, "text": " can see again it's doing the same thing getting the same result and it's got some nice functionality", "tokens": [393, 536, 797, 309, 311, 884, 264, 912, 551, 1242, 264, 912, 1874, 293, 309, 311, 658, 512, 1481, 14980], "temperature": 0.0, "avg_logprob": -0.07022535695438892, "compression_ratio": 1.9310344827586208, "no_speech_prob": 7.934484642646566e-07}, {"id": 424, "seek": 299984, "start": 3021.1200000000003, "end": 3024.7200000000003, "text": " it's printing it out into a pretty table for us and it's showing us the losses and the", "tokens": [309, 311, 14699, 309, 484, 666, 257, 1238, 3199, 337, 505, 293, 309, 311, 4099, 505, 264, 15352, 293, 264], "temperature": 0.0, "avg_logprob": -0.07022535695438892, "compression_ratio": 1.9310344827586208, "no_speech_prob": 7.934484642646566e-07}, {"id": 425, "seek": 299984, "start": 3024.7200000000003, "end": 3029.82, "text": " accuracy and how long it takes but there's nothing magic right you've been able to do", "tokens": [14170, 293, 577, 938, 309, 2516, 457, 456, 311, 1825, 5585, 558, 291, 600, 668, 1075, 281, 360], "temperature": 0.0, "avg_logprob": -0.07022535695438892, "compression_ratio": 1.9310344827586208, "no_speech_prob": 7.934484642646566e-07}, {"id": 426, "seek": 302982, "start": 3029.82, "end": 3038.0, "text": " exactly the same thing by hand using python and pytorch okay so so these abstractions", "tokens": [2293, 264, 912, 551, 538, 1011, 1228, 38797, 293, 25878, 284, 339, 1392, 370, 370, 613, 12649, 626], "temperature": 0.0, "avg_logprob": -0.10045750169868929, "compression_ratio": 1.8010204081632653, "no_speech_prob": 4.637834990717238e-06}, {"id": 427, "seek": 302982, "start": 3038.0, "end": 3043.88, "text": " are here to like let you write less code and to save some time and to save some cognitive", "tokens": [366, 510, 281, 411, 718, 291, 2464, 1570, 3089, 293, 281, 3155, 512, 565, 293, 281, 3155, 512, 15605], "temperature": 0.0, "avg_logprob": -0.10045750169868929, "compression_ratio": 1.8010204081632653, "no_speech_prob": 4.637834990717238e-06}, {"id": 428, "seek": 302982, "start": 3043.88, "end": 3050.48, "text": " overhead but they're not doing anything you can't do yourself and that's important right", "tokens": [19922, 457, 436, 434, 406, 884, 1340, 291, 393, 380, 360, 1803, 293, 300, 311, 1021, 558], "temperature": 0.0, "avg_logprob": -0.10045750169868929, "compression_ratio": 1.8010204081632653, "no_speech_prob": 4.637834990717238e-06}, {"id": 429, "seek": 302982, "start": 3050.48, "end": 3057.32, "text": " because if the if if they're doing things you can't do yourself then you can't customize", "tokens": [570, 498, 264, 498, 498, 436, 434, 884, 721, 291, 393, 380, 360, 1803, 550, 291, 393, 380, 19734], "temperature": 0.0, "avg_logprob": -0.10045750169868929, "compression_ratio": 1.8010204081632653, "no_speech_prob": 4.637834990717238e-06}, {"id": 430, "seek": 305732, "start": 3057.32, "end": 3063.2000000000003, "text": " them you can't debug them you know you can't profile them so we want to make sure that", "tokens": [552, 291, 393, 380, 24083, 552, 291, 458, 291, 393, 380, 7964, 552, 370, 321, 528, 281, 652, 988, 300], "temperature": 0.0, "avg_logprob": -0.08114770580740537, "compression_ratio": 1.7025316455696202, "no_speech_prob": 5.285506290420017e-07}, {"id": 431, "seek": 305732, "start": 3063.2000000000003, "end": 3071.8, "text": " the the the stuff we're using is stuff that we understand what it's doing so this is just", "tokens": [264, 264, 264, 1507, 321, 434, 1228, 307, 1507, 300, 321, 1223, 437, 309, 311, 884, 370, 341, 307, 445], "temperature": 0.0, "avg_logprob": -0.08114770580740537, "compression_ratio": 1.7025316455696202, "no_speech_prob": 5.285506290420017e-07}, {"id": 432, "seek": 305732, "start": 3071.8, "end": 3079.6800000000003, "text": " a linear function is not great we want a neural network so how do we turn this into a neural", "tokens": [257, 8213, 2445, 307, 406, 869, 321, 528, 257, 18161, 3209, 370, 577, 360, 321, 1261, 341, 666, 257, 18161], "temperature": 0.0, "avg_logprob": -0.08114770580740537, "compression_ratio": 1.7025316455696202, "no_speech_prob": 5.285506290420017e-07}, {"id": 433, "seek": 307968, "start": 3079.68, "end": 3089.3999999999996, "text": " network or remember this is a linear function x at w plus b to turn it into a neural network", "tokens": [3209, 420, 1604, 341, 307, 257, 8213, 2445, 2031, 412, 261, 1804, 272, 281, 1261, 309, 666, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.07983494599660237, "compression_ratio": 1.7098765432098766, "no_speech_prob": 7.934482937344001e-07}, {"id": 434, "seek": 307968, "start": 3089.3999999999996, "end": 3095.2799999999997, "text": " we have two linear functions exactly the same but with different weights and different biases", "tokens": [321, 362, 732, 8213, 6828, 2293, 264, 912, 457, 365, 819, 17443, 293, 819, 32152], "temperature": 0.0, "avg_logprob": -0.07983494599660237, "compression_ratio": 1.7098765432098766, "no_speech_prob": 7.934482937344001e-07}, {"id": 435, "seek": 307968, "start": 3095.2799999999997, "end": 3102.8399999999997, "text": " and in between this magic line of code which takes the result of our first linear function", "tokens": [293, 294, 1296, 341, 5585, 1622, 295, 3089, 597, 2516, 264, 1874, 295, 527, 700, 8213, 2445], "temperature": 0.0, "avg_logprob": -0.07983494599660237, "compression_ratio": 1.7098765432098766, "no_speech_prob": 7.934482937344001e-07}, {"id": 436, "seek": 310284, "start": 3102.84, "end": 3110.7200000000003, "text": " and then does a max between that and zero so a max of res and zero is going to take", "tokens": [293, 550, 775, 257, 11469, 1296, 300, 293, 4018, 370, 257, 11469, 295, 725, 293, 4018, 307, 516, 281, 747], "temperature": 0.0, "avg_logprob": -0.09430582956834273, "compression_ratio": 1.8835978835978835, "no_speech_prob": 4.88829698497284e-07}, {"id": 437, "seek": 310284, "start": 3110.7200000000003, "end": 3117.1600000000003, "text": " any negative numbers and turn them into zeros so we're going to do a linear function we're", "tokens": [604, 3671, 3547, 293, 1261, 552, 666, 35193, 370, 321, 434, 516, 281, 360, 257, 8213, 2445, 321, 434], "temperature": 0.0, "avg_logprob": -0.09430582956834273, "compression_ratio": 1.8835978835978835, "no_speech_prob": 4.88829698497284e-07}, {"id": 438, "seek": 310284, "start": 3117.1600000000003, "end": 3120.96, "text": " going to replace the negatives with zero and then we're going to take that and put it through", "tokens": [516, 281, 7406, 264, 40019, 365, 4018, 293, 550, 321, 434, 516, 281, 747, 300, 293, 829, 309, 807], "temperature": 0.0, "avg_logprob": -0.09430582956834273, "compression_ratio": 1.8835978835978835, "no_speech_prob": 4.88829698497284e-07}, {"id": 439, "seek": 310284, "start": 3120.96, "end": 3130.04, "text": " another linear function that believe it or not is a neural net so w1 and w2 will weight", "tokens": [1071, 8213, 2445, 300, 1697, 309, 420, 406, 307, 257, 18161, 2533, 370, 261, 16, 293, 261, 17, 486, 3364], "temperature": 0.0, "avg_logprob": -0.09430582956834273, "compression_ratio": 1.8835978835978835, "no_speech_prob": 4.88829698497284e-07}, {"id": 440, "seek": 313004, "start": 3130.04, "end": 3135.52, "text": " tensors b1 and b2 are bias tensors just like before so we can initialize them just like", "tokens": [10688, 830, 272, 16, 293, 272, 17, 366, 12577, 10688, 830, 445, 411, 949, 370, 321, 393, 5883, 1125, 552, 445, 411], "temperature": 0.0, "avg_logprob": -0.11112271493940211, "compression_ratio": 1.6181818181818182, "no_speech_prob": 6.179383831295127e-07}, {"id": 441, "seek": 313004, "start": 3135.52, "end": 3145.68, "text": " before and we could now call exactly the same training code that we did before to all these", "tokens": [949, 293, 321, 727, 586, 818, 2293, 264, 912, 3097, 3089, 300, 321, 630, 949, 281, 439, 613], "temperature": 0.0, "avg_logprob": -0.11112271493940211, "compression_ratio": 1.6181818181818182, "no_speech_prob": 6.179383831295127e-07}, {"id": 442, "seek": 313004, "start": 3145.68, "end": 3154.8, "text": " so res.max zero is called a rectified linear unit which you will always see referred to", "tokens": [370, 725, 13, 41167, 4018, 307, 1219, 257, 11048, 2587, 8213, 4985, 597, 291, 486, 1009, 536, 10839, 281], "temperature": 0.0, "avg_logprob": -0.11112271493940211, "compression_ratio": 1.6181818181818182, "no_speech_prob": 6.179383831295127e-07}, {"id": 443, "seek": 315480, "start": 3154.8, "end": 3165.1600000000003, "text": " as relu and so here is and and in pytorch it already has this function it's called f.relu", "tokens": [382, 1039, 84, 293, 370, 510, 307, 293, 293, 294, 25878, 284, 339, 309, 1217, 575, 341, 2445, 309, 311, 1219, 283, 13, 265, 2781], "temperature": 0.0, "avg_logprob": -0.133563242460552, "compression_ratio": 1.6094674556213018, "no_speech_prob": 2.3823668016120791e-07}, {"id": 444, "seek": 315480, "start": 3165.1600000000003, "end": 3171.1600000000003, "text": " and so if we plot it you can see it's as you'd expect it's zero for all negative numbers", "tokens": [293, 370, 498, 321, 7542, 309, 291, 393, 536, 309, 311, 382, 291, 1116, 2066, 309, 311, 4018, 337, 439, 3671, 3547], "temperature": 0.0, "avg_logprob": -0.133563242460552, "compression_ratio": 1.6094674556213018, "no_speech_prob": 2.3823668016120791e-07}, {"id": 445, "seek": 315480, "start": 3171.1600000000003, "end": 3181.6000000000004, "text": " and then it's y equals x for positive numbers so you know here's some jargon rectified linear", "tokens": [293, 550, 309, 311, 288, 6915, 2031, 337, 3353, 3547, 370, 291, 458, 510, 311, 512, 15181, 10660, 11048, 2587, 8213], "temperature": 0.0, "avg_logprob": -0.133563242460552, "compression_ratio": 1.6094674556213018, "no_speech_prob": 2.3823668016120791e-07}, {"id": 446, "seek": 318160, "start": 3181.6, "end": 3188.96, "text": " unit sounds scary sounds complicated but it's actually this incredibly tiny line of code", "tokens": [4985, 3263, 6958, 3263, 6179, 457, 309, 311, 767, 341, 6252, 5870, 1622, 295, 3089], "temperature": 0.0, "avg_logprob": -0.07294457813478866, "compression_ratio": 1.720496894409938, "no_speech_prob": 5.453288736134709e-07}, {"id": 447, "seek": 318160, "start": 3188.96, "end": 3195.68, "text": " this incredibly simple function and this happens a lot in deep learning things that sound complicated", "tokens": [341, 6252, 2199, 2445, 293, 341, 2314, 257, 688, 294, 2452, 2539, 721, 300, 1626, 6179], "temperature": 0.0, "avg_logprob": -0.07294457813478866, "compression_ratio": 1.720496894409938, "no_speech_prob": 5.453288736134709e-07}, {"id": 448, "seek": 318160, "start": 3195.68, "end": 3202.64, "text": " and sophisticated and impressive turn out to be normally super simple frankly at least", "tokens": [293, 16950, 293, 8992, 1261, 484, 281, 312, 5646, 1687, 2199, 11939, 412, 1935], "temperature": 0.0, "avg_logprob": -0.07294457813478866, "compression_ratio": 1.720496894409938, "no_speech_prob": 5.453288736134709e-07}, {"id": 449, "seek": 320264, "start": 3202.64, "end": 3212.7999999999997, "text": " once you know what it is so why do we do linear layer value linear layer well if we got rid", "tokens": [1564, 291, 458, 437, 309, 307, 370, 983, 360, 321, 360, 8213, 4583, 2158, 8213, 4583, 731, 498, 321, 658, 3973], "temperature": 0.0, "avg_logprob": -0.10002684593200684, "compression_ratio": 1.9635036496350364, "no_speech_prob": 1.4367469702847302e-06}, {"id": 450, "seek": 320264, "start": 3212.7999999999997, "end": 3224.94, "text": " of the middle if we got rid of the middle value and just went linear layer linear layer", "tokens": [295, 264, 2808, 498, 321, 658, 3973, 295, 264, 2808, 2158, 293, 445, 1437, 8213, 4583, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.10002684593200684, "compression_ratio": 1.9635036496350364, "no_speech_prob": 1.4367469702847302e-06}, {"id": 451, "seek": 320264, "start": 3224.94, "end": 3231.12, "text": " then you could rewrite that as a single linear layer when you multiply things and add and", "tokens": [550, 291, 727, 28132, 300, 382, 257, 2167, 8213, 4583, 562, 291, 12972, 721, 293, 909, 293], "temperature": 0.0, "avg_logprob": -0.10002684593200684, "compression_ratio": 1.9635036496350364, "no_speech_prob": 1.4367469702847302e-06}, {"id": 452, "seek": 323112, "start": 3231.12, "end": 3235.04, "text": " then multiply things and add and you can just change the coefficients and make it into a", "tokens": [550, 12972, 721, 293, 909, 293, 291, 393, 445, 1319, 264, 31994, 293, 652, 309, 666, 257], "temperature": 0.0, "avg_logprob": -0.08164104662443462, "compression_ratio": 1.7480314960629921, "no_speech_prob": 3.7266136132529937e-06}, {"id": 453, "seek": 323112, "start": 3235.04, "end": 3239.4, "text": " single multiply and then add so no matter how many linear layers we stack on top of", "tokens": [2167, 12972, 293, 550, 909, 370, 572, 1871, 577, 867, 8213, 7914, 321, 8630, 322, 1192, 295], "temperature": 0.0, "avg_logprob": -0.08164104662443462, "compression_ratio": 1.7480314960629921, "no_speech_prob": 3.7266136132529937e-06}, {"id": 454, "seek": 323112, "start": 3239.4, "end": 3248.04, "text": " each other we can never make anything more kind of effective than a simple linear model", "tokens": [1184, 661, 321, 393, 1128, 652, 1340, 544, 733, 295, 4942, 813, 257, 2199, 8213, 2316], "temperature": 0.0, "avg_logprob": -0.08164104662443462, "compression_ratio": 1.7480314960629921, "no_speech_prob": 3.7266136132529937e-06}, {"id": 455, "seek": 323112, "start": 3248.04, "end": 3253.96, "text": " but if you put a non-linearity between the linear layers then actually you have the opposite", "tokens": [457, 498, 291, 829, 257, 2107, 12, 1889, 17409, 1296, 264, 8213, 7914, 550, 767, 291, 362, 264, 6182], "temperature": 0.0, "avg_logprob": -0.08164104662443462, "compression_ratio": 1.7480314960629921, "no_speech_prob": 3.7266136132529937e-06}, {"id": 456, "seek": 323112, "start": 3253.96, "end": 3260.12, "text": " this is now where something called the universal approximation theorem holds which is that", "tokens": [341, 307, 586, 689, 746, 1219, 264, 11455, 28023, 20904, 9190, 597, 307, 300], "temperature": 0.0, "avg_logprob": -0.08164104662443462, "compression_ratio": 1.7480314960629921, "no_speech_prob": 3.7266136132529937e-06}, {"id": 457, "seek": 326012, "start": 3260.12, "end": 3266.88, "text": " if the size of the weight and bias matrices are big enough this can actually approximate", "tokens": [498, 264, 2744, 295, 264, 3364, 293, 12577, 32284, 366, 955, 1547, 341, 393, 767, 30874], "temperature": 0.0, "avg_logprob": -0.08033175640795605, "compression_ratio": 1.658878504672897, "no_speech_prob": 3.9054675653460436e-06}, {"id": 458, "seek": 326012, "start": 3266.88, "end": 3274.3199999999997, "text": " any arbitrary function including the function of how do I recognize threes from sevens or", "tokens": [604, 23211, 2445, 3009, 264, 2445, 295, 577, 360, 286, 5521, 258, 4856, 490, 3407, 82, 420], "temperature": 0.0, "avg_logprob": -0.08033175640795605, "compression_ratio": 1.658878504672897, "no_speech_prob": 3.9054675653460436e-06}, {"id": 459, "seek": 326012, "start": 3274.3199999999997, "end": 3282.08, "text": " or whatever so that's kind of amazing right this tiny thing is actually a universal function", "tokens": [420, 2035, 370, 300, 311, 733, 295, 2243, 558, 341, 5870, 551, 307, 767, 257, 11455, 2445], "temperature": 0.0, "avg_logprob": -0.08033175640795605, "compression_ratio": 1.658878504672897, "no_speech_prob": 3.9054675653460436e-06}, {"id": 460, "seek": 326012, "start": 3282.08, "end": 3289.2799999999997, "text": " approximator as long as you have w1 b1 w2 and b2 have the right numbers and we know", "tokens": [8542, 1639, 382, 938, 382, 291, 362, 261, 16, 272, 16, 261, 17, 293, 272, 17, 362, 264, 558, 3547, 293, 321, 458], "temperature": 0.0, "avg_logprob": -0.08033175640795605, "compression_ratio": 1.658878504672897, "no_speech_prob": 3.9054675653460436e-06}, {"id": 461, "seek": 328928, "start": 3289.28, "end": 3295.0400000000004, "text": " how to make them the right numbers you use SGD could take a very long time could take", "tokens": [577, 281, 652, 552, 264, 558, 3547, 291, 764, 34520, 35, 727, 747, 257, 588, 938, 565, 727, 747], "temperature": 0.0, "avg_logprob": -0.08347413680132698, "compression_ratio": 1.6761904761904762, "no_speech_prob": 4.247026197390369e-07}, {"id": 462, "seek": 328928, "start": 3295.0400000000004, "end": 3305.0800000000004, "text": " a lot of memory but the basic idea is that there is some solution to any computable problem", "tokens": [257, 688, 295, 4675, 457, 264, 3875, 1558, 307, 300, 456, 307, 512, 3827, 281, 604, 2807, 712, 1154], "temperature": 0.0, "avg_logprob": -0.08347413680132698, "compression_ratio": 1.6761904761904762, "no_speech_prob": 4.247026197390369e-07}, {"id": 463, "seek": 328928, "start": 3305.0800000000004, "end": 3311.6400000000003, "text": " and this is one of the biggest challenges a lot of beginners have to deep learning is", "tokens": [293, 341, 307, 472, 295, 264, 3880, 4759, 257, 688, 295, 26992, 362, 281, 2452, 2539, 307], "temperature": 0.0, "avg_logprob": -0.08347413680132698, "compression_ratio": 1.6761904761904762, "no_speech_prob": 4.247026197390369e-07}, {"id": 464, "seek": 328928, "start": 3311.6400000000003, "end": 3318.0400000000004, "text": " that there's nothing else to it like there's often this like okay how do I make a neural", "tokens": [300, 456, 311, 1825, 1646, 281, 309, 411, 456, 311, 2049, 341, 411, 1392, 577, 360, 286, 652, 257, 18161], "temperature": 0.0, "avg_logprob": -0.08347413680132698, "compression_ratio": 1.6761904761904762, "no_speech_prob": 4.247026197390369e-07}, {"id": 465, "seek": 331804, "start": 3318.04, "end": 3324.4, "text": " net oh that is the neural net well how do I do deep learning training where there's", "tokens": [2533, 1954, 300, 307, 264, 18161, 2533, 731, 577, 360, 286, 360, 2452, 2539, 3097, 689, 456, 311], "temperature": 0.0, "avg_logprob": -0.14682105330170178, "compression_ratio": 1.5853658536585367, "no_speech_prob": 1.2098635124857537e-06}, {"id": 466, "seek": 331804, "start": 3324.4, "end": 3331.48, "text": " GD there's things to like make it train a bit faster there's you know things to mean", "tokens": [460, 35, 456, 311, 721, 281, 411, 652, 309, 3847, 257, 857, 4663, 456, 311, 291, 458, 721, 281, 914], "temperature": 0.0, "avg_logprob": -0.14682105330170178, "compression_ratio": 1.5853658536585367, "no_speech_prob": 1.2098635124857537e-06}, {"id": 467, "seek": 331804, "start": 3331.48, "end": 3342.08, "text": " you need a few less parameters but everything from here is just performance tweaks honestly", "tokens": [291, 643, 257, 1326, 1570, 9834, 457, 1203, 490, 510, 307, 445, 3389, 46664, 6095], "temperature": 0.0, "avg_logprob": -0.14682105330170178, "compression_ratio": 1.5853658536585367, "no_speech_prob": 1.2098635124857537e-06}, {"id": 468, "seek": 334208, "start": 3342.08, "end": 3350.84, "text": " right so this is you know this is the key understanding of of training a neural network", "tokens": [558, 370, 341, 307, 291, 458, 341, 307, 264, 2141, 3701, 295, 295, 3097, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.12230577319860458, "compression_ratio": 1.6012269938650308, "no_speech_prob": 8.446207289125596e-07}, {"id": 469, "seek": 334208, "start": 3350.84, "end": 3360.12, "text": " okay we can simplify things a bit more we already know that we can use nn.linear to", "tokens": [1392, 321, 393, 20460, 721, 257, 857, 544, 321, 1217, 458, 300, 321, 393, 764, 297, 77, 13, 28263, 281], "temperature": 0.0, "avg_logprob": -0.12230577319860458, "compression_ratio": 1.6012269938650308, "no_speech_prob": 8.446207289125596e-07}, {"id": 470, "seek": 334208, "start": 3360.12, "end": 3369.12, "text": " replace the weight and bias so let's do that for both of the linear layers and then since", "tokens": [7406, 264, 3364, 293, 12577, 370, 718, 311, 360, 300, 337, 1293, 295, 264, 8213, 7914, 293, 550, 1670], "temperature": 0.0, "avg_logprob": -0.12230577319860458, "compression_ratio": 1.6012269938650308, "no_speech_prob": 8.446207289125596e-07}, {"id": 471, "seek": 336912, "start": 3369.12, "end": 3379.64, "text": " we're simply taking the result of one function and passing it into the next and take the", "tokens": [321, 434, 2935, 1940, 264, 1874, 295, 472, 2445, 293, 8437, 309, 666, 264, 958, 293, 747, 264], "temperature": 0.0, "avg_logprob": -0.07029149618493505, "compression_ratio": 2.3529411764705883, "no_speech_prob": 2.4824657884892076e-06}, {"id": 472, "seek": 336912, "start": 3379.64, "end": 3384.0, "text": " result of that function pass it to the next and so forth and then return the end this", "tokens": [1874, 295, 300, 2445, 1320, 309, 281, 264, 958, 293, 370, 5220, 293, 550, 2736, 264, 917, 341], "temperature": 0.0, "avg_logprob": -0.07029149618493505, "compression_ratio": 2.3529411764705883, "no_speech_prob": 2.4824657884892076e-06}, {"id": 473, "seek": 336912, "start": 3384.0, "end": 3389.16, "text": " is called function composition function composition is when you just take the result of one function", "tokens": [307, 1219, 2445, 12686, 2445, 12686, 307, 562, 291, 445, 747, 264, 1874, 295, 472, 2445], "temperature": 0.0, "avg_logprob": -0.07029149618493505, "compression_ratio": 2.3529411764705883, "no_speech_prob": 2.4824657884892076e-06}, {"id": 474, "seek": 336912, "start": 3389.16, "end": 3394.8399999999997, "text": " pass it to a new one take a result of one function pass it to a new one and so every", "tokens": [1320, 309, 281, 257, 777, 472, 747, 257, 1874, 295, 472, 2445, 1320, 309, 281, 257, 777, 472, 293, 370, 633], "temperature": 0.0, "avg_logprob": -0.07029149618493505, "compression_ratio": 2.3529411764705883, "no_speech_prob": 2.4824657884892076e-06}, {"id": 475, "seek": 339484, "start": 3394.84, "end": 3402.1600000000003, "text": " pretty much neural network is just doing function composition of linear layers and these are", "tokens": [1238, 709, 18161, 3209, 307, 445, 884, 2445, 12686, 295, 8213, 7914, 293, 613, 366], "temperature": 0.0, "avg_logprob": -0.1266838854009455, "compression_ratio": 1.8477157360406091, "no_speech_prob": 1.101593738894735e-06}, {"id": 476, "seek": 339484, "start": 3402.1600000000003, "end": 3408.36, "text": " called activation functions or nonlinearities so pytorch provides something to do function", "tokens": [1219, 24433, 6828, 420, 2107, 28263, 1088, 370, 25878, 284, 339, 6417, 746, 281, 360, 2445], "temperature": 0.0, "avg_logprob": -0.1266838854009455, "compression_ratio": 1.8477157360406091, "no_speech_prob": 1.101593738894735e-06}, {"id": 477, "seek": 339484, "start": 3408.36, "end": 3414.84, "text": " composition for us and it's called nn.sequential so it's going to do a linear layer pass the", "tokens": [12686, 337, 505, 293, 309, 311, 1219, 297, 77, 13, 11834, 2549, 370, 309, 311, 516, 281, 360, 257, 8213, 4583, 1320, 264], "temperature": 0.0, "avg_logprob": -0.1266838854009455, "compression_ratio": 1.8477157360406091, "no_speech_prob": 1.101593738894735e-06}, {"id": 478, "seek": 339484, "start": 3414.84, "end": 3420.96, "text": " result to a relu pass the result to a linear layer you'll see here I'm not using f.relu", "tokens": [1874, 281, 257, 1039, 84, 1320, 264, 1874, 281, 257, 8213, 4583, 291, 603, 536, 510, 286, 478, 406, 1228, 283, 13, 265, 2781], "temperature": 0.0, "avg_logprob": -0.1266838854009455, "compression_ratio": 1.8477157360406091, "no_speech_prob": 1.101593738894735e-06}, {"id": 479, "seek": 342096, "start": 3420.96, "end": 3427.28, "text": " I'm using nn.relu this is identical returns exactly the same thing but this is a class", "tokens": [286, 478, 1228, 297, 77, 13, 265, 2781, 341, 307, 14800, 11247, 2293, 264, 912, 551, 457, 341, 307, 257, 1508], "temperature": 0.0, "avg_logprob": -0.11954325507668888, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.7880578297990724e-06}, {"id": 480, "seek": 342096, "start": 3427.28, "end": 3436.32, "text": " rather than a function yes rachel by using the nonlinearity won't using a function that", "tokens": [2831, 813, 257, 2445, 2086, 367, 8188, 538, 1228, 264, 2107, 1889, 17409, 1582, 380, 1228, 257, 2445, 300], "temperature": 0.0, "avg_logprob": -0.11954325507668888, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.7880578297990724e-06}, {"id": 481, "seek": 342096, "start": 3436.32, "end": 3441.0, "text": " makes all negative output zero make many of the gradients in the network zero and stop", "tokens": [1669, 439, 3671, 5598, 4018, 652, 867, 295, 264, 2771, 2448, 294, 264, 3209, 4018, 293, 1590], "temperature": 0.0, "avg_logprob": -0.11954325507668888, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.7880578297990724e-06}, {"id": 482, "seek": 342096, "start": 3441.0, "end": 3448.96, "text": " the learning process due to many zero gradients well that's a fantastic question and the answer", "tokens": [264, 2539, 1399, 3462, 281, 867, 4018, 2771, 2448, 731, 300, 311, 257, 5456, 1168, 293, 264, 1867], "temperature": 0.0, "avg_logprob": -0.11954325507668888, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.7880578297990724e-06}, {"id": 483, "seek": 344896, "start": 3448.96, "end": 3456.7200000000003, "text": " is yes it does but they won't be zero for every image and remember the mini batches", "tokens": [307, 2086, 309, 775, 457, 436, 1582, 380, 312, 4018, 337, 633, 3256, 293, 1604, 264, 8382, 15245, 279], "temperature": 0.0, "avg_logprob": -0.08772012689611414, "compression_ratio": 1.86096256684492, "no_speech_prob": 1.5534850490439567e-06}, {"id": 484, "seek": 344896, "start": 3456.7200000000003, "end": 3462.4, "text": " are shuffled so even if it's zero for every image in one mini batch it won't be for the", "tokens": [366, 402, 33974, 370, 754, 498, 309, 311, 4018, 337, 633, 3256, 294, 472, 8382, 15245, 309, 1582, 380, 312, 337, 264], "temperature": 0.0, "avg_logprob": -0.08772012689611414, "compression_ratio": 1.86096256684492, "no_speech_prob": 1.5534850490439567e-06}, {"id": 485, "seek": 344896, "start": 3462.4, "end": 3468.4, "text": " next mini batch and it won't be the next time around we go for another epoch so yes it can", "tokens": [958, 8382, 15245, 293, 309, 1582, 380, 312, 264, 958, 565, 926, 321, 352, 337, 1071, 30992, 339, 370, 2086, 309, 393], "temperature": 0.0, "avg_logprob": -0.08772012689611414, "compression_ratio": 1.86096256684492, "no_speech_prob": 1.5534850490439567e-06}, {"id": 486, "seek": 344896, "start": 3468.4, "end": 3476.48, "text": " create zeros and if if the neural net ends up with a set of parameters such that lots", "tokens": [1884, 35193, 293, 498, 498, 264, 18161, 2533, 5314, 493, 365, 257, 992, 295, 9834, 1270, 300, 3195], "temperature": 0.0, "avg_logprob": -0.08772012689611414, "compression_ratio": 1.86096256684492, "no_speech_prob": 1.5534850490439567e-06}, {"id": 487, "seek": 347648, "start": 3476.48, "end": 3484.68, "text": " and lots of inputs end up as zeros you can end up with whole mini batches that are zero", "tokens": [293, 3195, 295, 15743, 917, 493, 382, 35193, 291, 393, 917, 493, 365, 1379, 8382, 15245, 279, 300, 366, 4018], "temperature": 0.0, "avg_logprob": -0.12742922856257513, "compression_ratio": 1.7467532467532467, "no_speech_prob": 2.9648953159266966e-07}, {"id": 488, "seek": 347648, "start": 3484.68, "end": 3494.52, "text": " and you can end up in a situation where some of the neurons remain inactive inactive means", "tokens": [293, 291, 393, 917, 493, 294, 257, 2590, 689, 512, 295, 264, 22027, 6222, 294, 12596, 294, 12596, 1355], "temperature": 0.0, "avg_logprob": -0.12742922856257513, "compression_ratio": 1.7467532467532467, "no_speech_prob": 2.9648953159266966e-07}, {"id": 489, "seek": 347648, "start": 3494.52, "end": 3501.84, "text": " that they're zero and they're basically dead units and this is a huge problem it basically", "tokens": [300, 436, 434, 4018, 293, 436, 434, 1936, 3116, 6815, 293, 341, 307, 257, 2603, 1154, 309, 1936], "temperature": 0.0, "avg_logprob": -0.12742922856257513, "compression_ratio": 1.7467532467532467, "no_speech_prob": 2.9648953159266966e-07}, {"id": 490, "seek": 350184, "start": 3501.84, "end": 3507.1600000000003, "text": " means you're wasting computation so there's a few tricks to avoid that which we'll be", "tokens": [1355, 291, 434, 20457, 24903, 370, 456, 311, 257, 1326, 11733, 281, 5042, 300, 597, 321, 603, 312], "temperature": 0.0, "avg_logprob": -0.11024004992316751, "compression_ratio": 1.6995073891625616, "no_speech_prob": 1.051146796271496e-06}, {"id": 491, "seek": 350184, "start": 3507.1600000000003, "end": 3514.7200000000003, "text": " learning about a lot one simple trick is to not make this thing flat here but just make", "tokens": [2539, 466, 257, 688, 472, 2199, 4282, 307, 281, 406, 652, 341, 551, 4962, 510, 457, 445, 652], "temperature": 0.0, "avg_logprob": -0.11024004992316751, "compression_ratio": 1.6995073891625616, "no_speech_prob": 1.051146796271496e-06}, {"id": 492, "seek": 350184, "start": 3514.7200000000003, "end": 3523.6000000000004, "text": " it a less steep line that's called a leaky value leaky rectified linear unit and that", "tokens": [309, 257, 1570, 16841, 1622, 300, 311, 1219, 257, 476, 15681, 2158, 476, 15681, 11048, 2587, 8213, 4985, 293, 300], "temperature": 0.0, "avg_logprob": -0.11024004992316751, "compression_ratio": 1.6995073891625616, "no_speech_prob": 1.051146796271496e-06}, {"id": 493, "seek": 350184, "start": 3523.6000000000004, "end": 3528.7400000000002, "text": " they help a bit as well learn though even better is to make sure that we just kind of", "tokens": [436, 854, 257, 857, 382, 731, 1466, 1673, 754, 1101, 307, 281, 652, 988, 300, 321, 445, 733, 295], "temperature": 0.0, "avg_logprob": -0.11024004992316751, "compression_ratio": 1.6995073891625616, "no_speech_prob": 1.051146796271496e-06}, {"id": 494, "seek": 352874, "start": 3528.74, "end": 3534.8399999999997, "text": " initialize to sensible initial values that are not too big and not too small and step", "tokens": [5883, 1125, 281, 25380, 5883, 4190, 300, 366, 406, 886, 955, 293, 406, 886, 1359, 293, 1823], "temperature": 0.0, "avg_logprob": -0.06104416370391846, "compression_ratio": 1.8632478632478633, "no_speech_prob": 4.116357672501181e-07}, {"id": 495, "seek": 352874, "start": 3534.8399999999997, "end": 3540.3999999999996, "text": " by sensible amounts that are particularly not too big and generally if we do that we", "tokens": [538, 25380, 11663, 300, 366, 4098, 406, 886, 955, 293, 5101, 498, 321, 360, 300, 321], "temperature": 0.0, "avg_logprob": -0.06104416370391846, "compression_ratio": 1.8632478632478633, "no_speech_prob": 4.116357672501181e-07}, {"id": 496, "seek": 352874, "start": 3540.3999999999996, "end": 3545.24, "text": " can keep things in the zone where they're positive most of the time but we are going", "tokens": [393, 1066, 721, 294, 264, 6668, 689, 436, 434, 3353, 881, 295, 264, 565, 457, 321, 366, 516], "temperature": 0.0, "avg_logprob": -0.06104416370391846, "compression_ratio": 1.8632478632478633, "no_speech_prob": 4.116357672501181e-07}, {"id": 497, "seek": 352874, "start": 3545.24, "end": 3549.7999999999997, "text": " to learn about how to actually analyze inside a network and find out how many dead units", "tokens": [281, 1466, 466, 577, 281, 767, 12477, 1854, 257, 3209, 293, 915, 484, 577, 867, 3116, 6815], "temperature": 0.0, "avg_logprob": -0.06104416370391846, "compression_ratio": 1.8632478632478633, "no_speech_prob": 4.116357672501181e-07}, {"id": 498, "seek": 352874, "start": 3549.7999999999997, "end": 3555.8399999999997, "text": " we have how many of these zeros we have because as you point out they are they are bad news", "tokens": [321, 362, 577, 867, 295, 613, 35193, 321, 362, 570, 382, 291, 935, 484, 436, 366, 436, 366, 1578, 2583], "temperature": 0.0, "avg_logprob": -0.06104416370391846, "compression_ratio": 1.8632478632478633, "no_speech_prob": 4.116357672501181e-07}, {"id": 499, "seek": 355584, "start": 3555.84, "end": 3562.92, "text": " they don't do any work and they'll continue to not do any work if enough of the inputs", "tokens": [436, 500, 380, 360, 604, 589, 293, 436, 603, 2354, 281, 406, 360, 604, 589, 498, 1547, 295, 264, 15743], "temperature": 0.0, "avg_logprob": -0.10957922357501405, "compression_ratio": 1.5476190476190477, "no_speech_prob": 3.8070083974162117e-07}, {"id": 500, "seek": 355584, "start": 3562.92, "end": 3574.96, "text": " end up being zero. Okay so now that we've got a neural net we can use exactly the same", "tokens": [917, 493, 885, 4018, 13, 1033, 370, 586, 300, 321, 600, 658, 257, 18161, 2533, 321, 393, 764, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.10957922357501405, "compression_ratio": 1.5476190476190477, "no_speech_prob": 3.8070083974162117e-07}, {"id": 501, "seek": 355584, "start": 3574.96, "end": 3580.44, "text": " learner we had before but this time we're passing the simple net instead of the linear", "tokens": [33347, 321, 632, 949, 457, 341, 565, 321, 434, 8437, 264, 2199, 2533, 2602, 295, 264, 8213], "temperature": 0.0, "avg_logprob": -0.10957922357501405, "compression_ratio": 1.5476190476190477, "no_speech_prob": 3.8070083974162117e-07}, {"id": 502, "seek": 358044, "start": 3580.44, "end": 3587.12, "text": " one everything else is the same and we can call fit just like before and generally as", "tokens": [472, 1203, 1646, 307, 264, 912, 293, 321, 393, 818, 3318, 445, 411, 949, 293, 5101, 382], "temperature": 0.0, "avg_logprob": -0.1162741409157807, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.2878922461823095e-06}, {"id": 503, "seek": 358044, "start": 3587.12, "end": 3593.06, "text": " your models get deeper so here we've gone from one layer to and I'm only counting the", "tokens": [428, 5245, 483, 7731, 370, 510, 321, 600, 2780, 490, 472, 4583, 281, 293, 286, 478, 787, 13251, 264], "temperature": 0.0, "avg_logprob": -0.1162741409157807, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.2878922461823095e-06}, {"id": 504, "seek": 358044, "start": 3593.06, "end": 3597.36, "text": " parameterized layers as layers you could say it's three I'm just going to call it two there's", "tokens": [13075, 1602, 7914, 382, 7914, 291, 727, 584, 309, 311, 1045, 286, 478, 445, 516, 281, 818, 309, 732, 456, 311], "temperature": 0.0, "avg_logprob": -0.1162741409157807, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.2878922461823095e-06}, {"id": 505, "seek": 358044, "start": 3597.36, "end": 3602.6, "text": " two trainable layers so I've gone from one layer to two I've checked dropped my learning", "tokens": [732, 3847, 712, 7914, 370, 286, 600, 2780, 490, 472, 4583, 281, 732, 286, 600, 10033, 8119, 452, 2539], "temperature": 0.0, "avg_logprob": -0.1162741409157807, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.2878922461823095e-06}, {"id": 506, "seek": 358044, "start": 3602.6, "end": 3609.0, "text": " rate from one to zero point one because the deeper models you know tend to be kind of", "tokens": [3314, 490, 472, 281, 4018, 935, 472, 570, 264, 7731, 5245, 291, 458, 3928, 281, 312, 733, 295], "temperature": 0.0, "avg_logprob": -0.1162741409157807, "compression_ratio": 1.8181818181818181, "no_speech_prob": 1.2878922461823095e-06}, {"id": 507, "seek": 360900, "start": 3609.0, "end": 3614.24, "text": " bumpier less nicely behaved so often you need to use lower learning rates and so we train", "tokens": [9961, 811, 1570, 9594, 48249, 370, 2049, 291, 643, 281, 764, 3126, 2539, 6846, 293, 370, 321, 3847], "temperature": 0.0, "avg_logprob": -0.07727807607406224, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.228915607498493e-06}, {"id": 508, "seek": 360900, "start": 3614.24, "end": 3622.4, "text": " it for a while okay and we can actually find out what that training looks like by looking", "tokens": [309, 337, 257, 1339, 1392, 293, 321, 393, 767, 915, 484, 437, 300, 3097, 1542, 411, 538, 1237], "temperature": 0.0, "avg_logprob": -0.07727807607406224, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.228915607498493e-06}, {"id": 509, "seek": 360900, "start": 3622.4, "end": 3626.94, "text": " inside our learner and there's an attribute we create for you called recorder and that's", "tokens": [1854, 527, 33347, 293, 456, 311, 364, 19667, 321, 1884, 337, 291, 1219, 37744, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.07727807607406224, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.228915607498493e-06}, {"id": 510, "seek": 360900, "start": 3626.94, "end": 3633.12, "text": " going to record well everything that appears in this table basically well these three things", "tokens": [516, 281, 2136, 731, 1203, 300, 7038, 294, 341, 3199, 1936, 731, 613, 1045, 721], "temperature": 0.0, "avg_logprob": -0.07727807607406224, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.228915607498493e-06}, {"id": 511, "seek": 363312, "start": 3633.12, "end": 3640.44, "text": " the training loss the validation loss and the accuracy or any metrics so recorder dot values", "tokens": [264, 3097, 4470, 264, 24071, 4470, 293, 264, 14170, 420, 604, 16367, 370, 37744, 5893, 4190], "temperature": 0.0, "avg_logprob": -0.08869061704541815, "compression_ratio": 1.569767441860465, "no_speech_prob": 6.893599788782012e-07}, {"id": 512, "seek": 363312, "start": 3640.44, "end": 3648.7599999999998, "text": " contains that kind of table of results and so item number two of each row will be the", "tokens": [8306, 300, 733, 295, 3199, 295, 3542, 293, 370, 3174, 1230, 732, 295, 1184, 5386, 486, 312, 264], "temperature": 0.0, "avg_logprob": -0.08869061704541815, "compression_ratio": 1.569767441860465, "no_speech_prob": 6.893599788782012e-07}, {"id": 513, "seek": 363312, "start": 3648.7599999999998, "end": 3659.68, "text": " accuracy and so the the capital L class which I'm using here as a nice little method called", "tokens": [14170, 293, 370, 264, 264, 4238, 441, 1508, 597, 286, 478, 1228, 510, 382, 257, 1481, 707, 3170, 1219], "temperature": 0.0, "avg_logprob": -0.08869061704541815, "compression_ratio": 1.569767441860465, "no_speech_prob": 6.893599788782012e-07}, {"id": 514, "seek": 365968, "start": 3659.68, "end": 3670.52, "text": " item got that will will get the second item from every row and then I can plot that to", "tokens": [3174, 658, 300, 486, 486, 483, 264, 1150, 3174, 490, 633, 5386, 293, 550, 286, 393, 7542, 300, 281], "temperature": 0.0, "avg_logprob": -0.13851958986312624, "compression_ratio": 1.76, "no_speech_prob": 2.7693999982147943e-06}, {"id": 515, "seek": 365968, "start": 3670.52, "end": 3677.7599999999998, "text": " see how the training went and I can get the final accuracy like so by grabbing the last", "tokens": [536, 577, 264, 3097, 1437, 293, 286, 393, 483, 264, 2572, 14170, 411, 370, 538, 23771, 264, 1036], "temperature": 0.0, "avg_logprob": -0.13851958986312624, "compression_ratio": 1.76, "no_speech_prob": 2.7693999982147943e-06}, {"id": 516, "seek": 365968, "start": 3677.7599999999998, "end": 3686.24, "text": " row of the table and grabbing the second index two zero one two and my final accuracy not", "tokens": [5386, 295, 264, 3199, 293, 23771, 264, 1150, 8186, 732, 4018, 472, 732, 293, 452, 2572, 14170, 406], "temperature": 0.0, "avg_logprob": -0.13851958986312624, "compression_ratio": 1.76, "no_speech_prob": 2.7693999982147943e-06}, {"id": 517, "seek": 368624, "start": 3686.24, "end": 3694.64, "text": " bad ninety eight point three percent so this is pretty amazing we now have a function that", "tokens": [1578, 25063, 3180, 935, 1045, 3043, 370, 341, 307, 1238, 2243, 321, 586, 362, 257, 2445, 300], "temperature": 0.0, "avg_logprob": -0.11962135314941406, "compression_ratio": 1.6476683937823835, "no_speech_prob": 5.285508564156771e-07}, {"id": 518, "seek": 368624, "start": 3694.64, "end": 3700.72, "text": " can solve any problem to any level of accuracy if we can find the right parameters and we", "tokens": [393, 5039, 604, 1154, 281, 604, 1496, 295, 14170, 498, 321, 393, 915, 264, 558, 9834, 293, 321], "temperature": 0.0, "avg_logprob": -0.11962135314941406, "compression_ratio": 1.6476683937823835, "no_speech_prob": 5.285508564156771e-07}, {"id": 519, "seek": 368624, "start": 3700.72, "end": 3705.9199999999996, "text": " have a way to find hopefully the best or at least a very good set of parameters for any", "tokens": [362, 257, 636, 281, 915, 4696, 264, 1151, 420, 412, 1935, 257, 588, 665, 992, 295, 9834, 337, 604], "temperature": 0.0, "avg_logprob": -0.11962135314941406, "compression_ratio": 1.6476683937823835, "no_speech_prob": 5.285508564156771e-07}, {"id": 520, "seek": 368624, "start": 3705.9199999999996, "end": 3714.8799999999997, "text": " function so this is kind of the magic yes Rachel.", "tokens": [2445, 370, 341, 307, 733, 295, 264, 5585, 2086, 14246, 13], "temperature": 0.0, "avg_logprob": -0.11962135314941406, "compression_ratio": 1.6476683937823835, "no_speech_prob": 5.285508564156771e-07}, {"id": 521, "seek": 371488, "start": 3714.88, "end": 3718.32, "text": " How could we use what we're learning here to get an idea of what the network is learning", "tokens": [1012, 727, 321, 764, 437, 321, 434, 2539, 510, 281, 483, 364, 1558, 295, 437, 264, 3209, 307, 2539], "temperature": 0.0, "avg_logprob": -0.11844711742181888, "compression_ratio": 1.6650943396226414, "no_speech_prob": 4.425451152201276e-06}, {"id": 522, "seek": 371488, "start": 3718.32, "end": 3727.76, "text": " along the way like xylar and fergus did more or less we will look at that later not in", "tokens": [2051, 264, 636, 411, 2031, 88, 2200, 293, 7202, 21956, 630, 544, 420, 1570, 321, 486, 574, 412, 300, 1780, 406, 294], "temperature": 0.0, "avg_logprob": -0.11844711742181888, "compression_ratio": 1.6650943396226414, "no_speech_prob": 4.425451152201276e-06}, {"id": 523, "seek": 371488, "start": 3727.76, "end": 3735.2400000000002, "text": " the full detail of their paper but basically you can look in the dot parameters to see", "tokens": [264, 1577, 2607, 295, 641, 3035, 457, 1936, 291, 393, 574, 294, 264, 5893, 9834, 281, 536], "temperature": 0.0, "avg_logprob": -0.11844711742181888, "compression_ratio": 1.6650943396226414, "no_speech_prob": 4.425451152201276e-06}, {"id": 524, "seek": 371488, "start": 3735.2400000000002, "end": 3741.84, "text": " the values of those parameters and at this point well I mean why don't you try it yourself", "tokens": [264, 4190, 295, 729, 9834, 293, 412, 341, 935, 731, 286, 914, 983, 500, 380, 291, 853, 309, 1803], "temperature": 0.0, "avg_logprob": -0.11844711742181888, "compression_ratio": 1.6650943396226414, "no_speech_prob": 4.425451152201276e-06}, {"id": 525, "seek": 374184, "start": 3741.84, "end": 3750.44, "text": " right you've actually got now the parameters so if you want to grab the model you can actually", "tokens": [558, 291, 600, 767, 658, 586, 264, 9834, 370, 498, 291, 528, 281, 4444, 264, 2316, 291, 393, 767], "temperature": 0.0, "avg_logprob": -0.09982914557823767, "compression_ratio": 1.756578947368421, "no_speech_prob": 4.157352123002056e-06}, {"id": 526, "seek": 374184, "start": 3750.44, "end": 3759.96, "text": " see learn dot model so we can we can look inside learn dot model to see the actual model", "tokens": [536, 1466, 5893, 2316, 370, 321, 393, 321, 393, 574, 1854, 1466, 5893, 2316, 281, 536, 264, 3539, 2316], "temperature": 0.0, "avg_logprob": -0.09982914557823767, "compression_ratio": 1.756578947368421, "no_speech_prob": 4.157352123002056e-06}, {"id": 527, "seek": 374184, "start": 3759.96, "end": 3768.6800000000003, "text": " that we just trained and you can see it's got the three things in it the linear the", "tokens": [300, 321, 445, 8895, 293, 291, 393, 536, 309, 311, 658, 264, 1045, 721, 294, 309, 264, 8213, 264], "temperature": 0.0, "avg_logprob": -0.09982914557823767, "compression_ratio": 1.756578947368421, "no_speech_prob": 4.157352123002056e-06}, {"id": 528, "seek": 376868, "start": 3768.68, "end": 3773.8799999999997, "text": " value the linear and you know what I kind of like to do is to put that into a variable", "tokens": [2158, 264, 8213, 293, 291, 458, 437, 286, 733, 295, 411, 281, 360, 307, 281, 829, 300, 666, 257, 7006], "temperature": 0.0, "avg_logprob": -0.14975347190067687, "compression_ratio": 1.6911764705882353, "no_speech_prob": 4.888293005933519e-07}, {"id": 529, "seek": 376868, "start": 3773.8799999999997, "end": 3783.2799999999997, "text": " make it a bit easy to work with and you can grab one layer by indexing in you can look", "tokens": [652, 309, 257, 857, 1858, 281, 589, 365, 293, 291, 393, 4444, 472, 4583, 538, 8186, 278, 294, 291, 393, 574], "temperature": 0.0, "avg_logprob": -0.14975347190067687, "compression_ratio": 1.6911764705882353, "no_speech_prob": 4.888293005933519e-07}, {"id": 530, "seek": 376868, "start": 3783.2799999999997, "end": 3791.0, "text": " at the parameters and that just gives me a something called a generator it's something", "tokens": [412, 264, 9834, 293, 300, 445, 2709, 385, 257, 746, 1219, 257, 19265, 309, 311, 746], "temperature": 0.0, "avg_logprob": -0.14975347190067687, "compression_ratio": 1.6911764705882353, "no_speech_prob": 4.888293005933519e-07}, {"id": 531, "seek": 376868, "start": 3791.0, "end": 3795.04, "text": " that will give me a list of the parameters when I ask for them so I can just go wait", "tokens": [300, 486, 976, 385, 257, 1329, 295, 264, 9834, 562, 286, 1029, 337, 552, 370, 286, 393, 445, 352, 1699], "temperature": 0.0, "avg_logprob": -0.14975347190067687, "compression_ratio": 1.6911764705882353, "no_speech_prob": 4.888293005933519e-07}, {"id": 532, "seek": 379504, "start": 3795.04, "end": 3810.4, "text": " comma bias equals to destructure them and so the weight is 30 by 784 because that's", "tokens": [22117, 12577, 6915, 281, 2677, 2885, 552, 293, 370, 264, 3364, 307, 2217, 538, 1614, 25494, 570, 300, 311], "temperature": 0.0, "avg_logprob": -0.12068227063054623, "compression_ratio": 1.3587786259541985, "no_speech_prob": 9.132532454714237e-07}, {"id": 533, "seek": 379504, "start": 3810.4, "end": 3821.0, "text": " what I asked for so one of the things to note here is that to create a neural net so something", "tokens": [437, 286, 2351, 337, 370, 472, 295, 264, 721, 281, 3637, 510, 307, 300, 281, 1884, 257, 18161, 2533, 370, 746], "temperature": 0.0, "avg_logprob": -0.12068227063054623, "compression_ratio": 1.3587786259541985, "no_speech_prob": 9.132532454714237e-07}, {"id": 534, "seek": 382100, "start": 3821.0, "end": 3827.52, "text": " with more than one layer I actually have 30 outputs not just one right so I'm kind of", "tokens": [365, 544, 813, 472, 4583, 286, 767, 362, 2217, 23930, 406, 445, 472, 558, 370, 286, 478, 733, 295], "temperature": 0.0, "avg_logprob": -0.11237861926739033, "compression_ratio": 1.5988372093023255, "no_speech_prob": 1.6028078562158043e-06}, {"id": 535, "seek": 382100, "start": 3827.52, "end": 3831.64, "text": " generating lots of you can think of as generating lots of features so it's kind of like 30 different", "tokens": [17746, 3195, 295, 291, 393, 519, 295, 382, 17746, 3195, 295, 4122, 370, 309, 311, 733, 295, 411, 2217, 819], "temperature": 0.0, "avg_logprob": -0.11237861926739033, "compression_ratio": 1.5988372093023255, "no_speech_prob": 1.6028078562158043e-06}, {"id": 536, "seek": 382100, "start": 3831.64, "end": 3841.4, "text": " linear linear models here and then I combine those 30 back into one so you could look at", "tokens": [8213, 8213, 5245, 510, 293, 550, 286, 10432, 729, 2217, 646, 666, 472, 370, 291, 727, 574, 412], "temperature": 0.0, "avg_logprob": -0.11237861926739033, "compression_ratio": 1.5988372093023255, "no_speech_prob": 1.6028078562158043e-06}, {"id": 537, "seek": 384140, "start": 3841.4, "end": 3852.0, "text": " one of those by having a look at yeah so there's there's the numbers in the first row we could", "tokens": [472, 295, 729, 538, 1419, 257, 574, 412, 1338, 370, 456, 311, 456, 311, 264, 3547, 294, 264, 700, 5386, 321, 727], "temperature": 0.0, "avg_logprob": -0.10442987772134635, "compression_ratio": 1.205128205128205, "no_speech_prob": 2.3090683498594444e-07}, {"id": 538, "seek": 385200, "start": 3852.0, "end": 3871.56, "text": " reshape that into the original shape of the images and we could even have a look", "tokens": [725, 42406, 300, 666, 264, 3380, 3909, 295, 264, 5267, 293, 321, 727, 754, 362, 257, 574], "temperature": 0.0, "avg_logprob": -0.09606062798272996, "compression_ratio": 1.490909090909091, "no_speech_prob": 4.1163573882840865e-07}, {"id": 539, "seek": 385200, "start": 3871.56, "end": 3875.6, "text": " and there it is right so you can see this is something so this is cool right we can", "tokens": [293, 456, 309, 307, 558, 370, 291, 393, 536, 341, 307, 746, 370, 341, 307, 1627, 558, 321, 393], "temperature": 0.0, "avg_logprob": -0.09606062798272996, "compression_ratio": 1.490909090909091, "no_speech_prob": 4.1163573882840865e-07}, {"id": 540, "seek": 387560, "start": 3875.6, "end": 3886.54, "text": " actually see here we've got something which is which is kind of learning to find things", "tokens": [767, 536, 510, 321, 600, 658, 746, 597, 307, 597, 307, 733, 295, 2539, 281, 915, 721], "temperature": 0.0, "avg_logprob": -0.05116402417763896, "compression_ratio": 1.7524752475247525, "no_speech_prob": 2.6016045922006015e-06}, {"id": 541, "seek": 387560, "start": 3886.54, "end": 3894.2999999999997, "text": " at the top and the bottom and the middle and so we could look at the second one okay no", "tokens": [412, 264, 1192, 293, 264, 2767, 293, 264, 2808, 293, 370, 321, 727, 574, 412, 264, 1150, 472, 1392, 572], "temperature": 0.0, "avg_logprob": -0.05116402417763896, "compression_ratio": 1.7524752475247525, "no_speech_prob": 2.6016045922006015e-06}, {"id": 542, "seek": 387560, "start": 3894.2999999999997, "end": 3900.0, "text": " idea what that's showing and so some of them are kind of you know I've probably got far", "tokens": [1558, 437, 300, 311, 4099, 293, 370, 512, 295, 552, 366, 733, 295, 291, 458, 286, 600, 1391, 658, 1400], "temperature": 0.0, "avg_logprob": -0.05116402417763896, "compression_ratio": 1.7524752475247525, "no_speech_prob": 2.6016045922006015e-06}, {"id": 543, "seek": 387560, "start": 3900.0, "end": 3905.2799999999997, "text": " more than I need which is why they're not that obvious but you can see yeah here's another", "tokens": [544, 813, 286, 643, 597, 307, 983, 436, 434, 406, 300, 6322, 457, 291, 393, 536, 1338, 510, 311, 1071], "temperature": 0.0, "avg_logprob": -0.05116402417763896, "compression_ratio": 1.7524752475247525, "no_speech_prob": 2.6016045922006015e-06}, {"id": 544, "seek": 390528, "start": 3905.28, "end": 3910.0800000000004, "text": " thing that's looking pretty similar here's something that's kind of looking for this", "tokens": [551, 300, 311, 1237, 1238, 2531, 510, 311, 746, 300, 311, 733, 295, 1237, 337, 341], "temperature": 0.0, "avg_logprob": -0.09145966172218323, "compression_ratio": 1.8219895287958114, "no_speech_prob": 4.93694005854195e-06}, {"id": 545, "seek": 390528, "start": 3910.0800000000004, "end": 3918.4, "text": " little bit in the middle so yeah this is the basic idea to understand the features that", "tokens": [707, 857, 294, 264, 2808, 370, 1338, 341, 307, 264, 3875, 1558, 281, 1223, 264, 4122, 300], "temperature": 0.0, "avg_logprob": -0.09145966172218323, "compression_ratio": 1.8219895287958114, "no_speech_prob": 4.93694005854195e-06}, {"id": 546, "seek": 390528, "start": 3918.4, "end": 3924.1600000000003, "text": " are not the first layer but later layers you have to be a bit more sophisticated but yeah", "tokens": [366, 406, 264, 700, 4583, 457, 1780, 7914, 291, 362, 281, 312, 257, 857, 544, 16950, 457, 1338], "temperature": 0.0, "avg_logprob": -0.09145966172218323, "compression_ratio": 1.8219895287958114, "no_speech_prob": 4.93694005854195e-06}, {"id": 547, "seek": 390528, "start": 3924.1600000000003, "end": 3933.32, "text": " to see the first layer ones you can you can just plot them okay so then you know just", "tokens": [281, 536, 264, 700, 4583, 2306, 291, 393, 291, 393, 445, 7542, 552, 1392, 370, 550, 291, 458, 445], "temperature": 0.0, "avg_logprob": -0.09145966172218323, "compression_ratio": 1.8219895287958114, "no_speech_prob": 4.93694005854195e-06}, {"id": 548, "seek": 393332, "start": 3933.32, "end": 3941.2400000000002, "text": " to compare we could use the full fast AI toolkit so grab our data loaders by using data loaders", "tokens": [281, 6794, 321, 727, 764, 264, 1577, 2370, 7318, 40167, 370, 4444, 527, 1412, 3677, 433, 538, 1228, 1412, 3677, 433], "temperature": 0.0, "avg_logprob": -0.08821941375732421, "compression_ratio": 1.4619565217391304, "no_speech_prob": 1.4823519904894056e-06}, {"id": 549, "seek": 393332, "start": 3941.2400000000002, "end": 3947.6000000000004, "text": " from folder as we've done before and create a CNN learner and a resnet and fit it for", "tokens": [490, 10820, 382, 321, 600, 1096, 949, 293, 1884, 257, 24859, 33347, 293, 257, 725, 7129, 293, 3318, 309, 337], "temperature": 0.0, "avg_logprob": -0.08821941375732421, "compression_ratio": 1.4619565217391304, "no_speech_prob": 1.4823519904894056e-06}, {"id": 550, "seek": 393332, "start": 3947.6000000000004, "end": 3959.6000000000004, "text": " a single epoch and whoa 99.7 right so we did 40 epochs and got 98.3 as I said using all", "tokens": [257, 2167, 30992, 339, 293, 13310, 11803, 13, 22, 558, 370, 321, 630, 3356, 30992, 28346, 293, 658, 20860, 13, 18, 382, 286, 848, 1228, 439], "temperature": 0.0, "avg_logprob": -0.08821941375732421, "compression_ratio": 1.4619565217391304, "no_speech_prob": 1.4823519904894056e-06}, {"id": 551, "seek": 395960, "start": 3959.6, "end": 3965.68, "text": " the tricks you can really speed things up and make things a lot better and so by the", "tokens": [264, 11733, 291, 393, 534, 3073, 721, 493, 293, 652, 721, 257, 688, 1101, 293, 370, 538, 264], "temperature": 0.0, "avg_logprob": -0.12619676760264806, "compression_ratio": 1.4963503649635037, "no_speech_prob": 8.446208994428162e-07}, {"id": 552, "seek": 395960, "start": 3965.68, "end": 3973.96, "text": " end of this course or at least both parts of this course you'll be able to from scratch", "tokens": [917, 295, 341, 1164, 420, 412, 1935, 1293, 3166, 295, 341, 1164, 291, 603, 312, 1075, 281, 490, 8459], "temperature": 0.0, "avg_logprob": -0.12619676760264806, "compression_ratio": 1.4963503649635037, "no_speech_prob": 8.446208994428162e-07}, {"id": 553, "seek": 395960, "start": 3973.96, "end": 3980.64, "text": " get this 99.7 in a single epoch.", "tokens": [483, 341, 11803, 13, 22, 294, 257, 2167, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.12619676760264806, "compression_ratio": 1.4963503649635037, "no_speech_prob": 8.446208994428162e-07}, {"id": 554, "seek": 398064, "start": 3980.64, "end": 3992.4, "text": " Alright so jargon so jargon just to remind us value function that returns zero for negatives", "tokens": [2798, 370, 15181, 10660, 370, 15181, 10660, 445, 281, 4160, 505, 2158, 2445, 300, 11247, 4018, 337, 40019], "temperature": 0.0, "avg_logprob": -0.09782804586948493, "compression_ratio": 1.748792270531401, "no_speech_prob": 1.3496942301571835e-06}, {"id": 555, "seek": 398064, "start": 3992.4, "end": 4001.52, "text": " many batch a few inputs and labels which optionally are randomly selected the forward pass is", "tokens": [867, 15245, 257, 1326, 15743, 293, 16949, 597, 3614, 379, 366, 16979, 8209, 264, 2128, 1320, 307], "temperature": 0.0, "avg_logprob": -0.09782804586948493, "compression_ratio": 1.748792270531401, "no_speech_prob": 1.3496942301571835e-06}, {"id": 556, "seek": 398064, "start": 4001.52, "end": 4005.3599999999997, "text": " the bit where we calculate the predictions the loss is the function that we're going", "tokens": [264, 857, 689, 321, 8873, 264, 21264, 264, 4470, 307, 264, 2445, 300, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.09782804586948493, "compression_ratio": 1.748792270531401, "no_speech_prob": 1.3496942301571835e-06}, {"id": 557, "seek": 398064, "start": 4005.3599999999997, "end": 4010.4, "text": " to take the derivative of and then the gradient is the derivative of the loss with respect", "tokens": [281, 747, 264, 13760, 295, 293, 550, 264, 16235, 307, 264, 13760, 295, 264, 4470, 365, 3104], "temperature": 0.0, "avg_logprob": -0.09782804586948493, "compression_ratio": 1.748792270531401, "no_speech_prob": 1.3496942301571835e-06}, {"id": 558, "seek": 401040, "start": 4010.4, "end": 4018.0, "text": " to each parameter the backward pass is when we calculate those gradients gradient descent", "tokens": [281, 1184, 13075, 264, 23897, 1320, 307, 562, 321, 8873, 729, 2771, 2448, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.09692276130288334, "compression_ratio": 1.6645569620253164, "no_speech_prob": 6.577922135875269e-07}, {"id": 559, "seek": 401040, "start": 4018.0, "end": 4022.76, "text": " is that full thing of taking a step in the direction opposite to the gradients by after", "tokens": [307, 300, 1577, 551, 295, 1940, 257, 1823, 294, 264, 3513, 6182, 281, 264, 2771, 2448, 538, 934], "temperature": 0.0, "avg_logprob": -0.09692276130288334, "compression_ratio": 1.6645569620253164, "no_speech_prob": 6.577922135875269e-07}, {"id": 560, "seek": 401040, "start": 4022.76, "end": 4034.82, "text": " calculating the loss and then the learning rate is the size of the step that we take.", "tokens": [28258, 264, 4470, 293, 550, 264, 2539, 3314, 307, 264, 2744, 295, 264, 1823, 300, 321, 747, 13], "temperature": 0.0, "avg_logprob": -0.09692276130288334, "compression_ratio": 1.6645569620253164, "no_speech_prob": 6.577922135875269e-07}, {"id": 561, "seek": 403482, "start": 4034.82, "end": 4040.52, "text": " Other things to know perhaps the two most important pieces of jargon are all of the", "tokens": [5358, 721, 281, 458, 4317, 264, 732, 881, 1021, 3755, 295, 15181, 10660, 366, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.09344159590231406, "compression_ratio": 1.9153439153439153, "no_speech_prob": 3.86695944598614e-07}, {"id": 562, "seek": 403482, "start": 4040.52, "end": 4047.54, "text": " numbers that are in a neural network the numbers that we're learning are called parameters", "tokens": [3547, 300, 366, 294, 257, 18161, 3209, 264, 3547, 300, 321, 434, 2539, 366, 1219, 9834], "temperature": 0.0, "avg_logprob": -0.09344159590231406, "compression_ratio": 1.9153439153439153, "no_speech_prob": 3.86695944598614e-07}, {"id": 563, "seek": 403482, "start": 4047.54, "end": 4052.96, "text": " and then the numbers that we're calculating so every value that's calculated every matrix", "tokens": [293, 550, 264, 3547, 300, 321, 434, 28258, 370, 633, 2158, 300, 311, 15598, 633, 8141], "temperature": 0.0, "avg_logprob": -0.09344159590231406, "compression_ratio": 1.9153439153439153, "no_speech_prob": 3.86695944598614e-07}, {"id": 564, "seek": 403482, "start": 4052.96, "end": 4059.0800000000004, "text": " multiplication element that's calculated they're called activations so activations and parameters", "tokens": [27290, 4478, 300, 311, 15598, 436, 434, 1219, 2430, 763, 370, 2430, 763, 293, 9834], "temperature": 0.0, "avg_logprob": -0.09344159590231406, "compression_ratio": 1.9153439153439153, "no_speech_prob": 3.86695944598614e-07}, {"id": 565, "seek": 405908, "start": 4059.08, "end": 4064.84, "text": " are all of the numbers in the neural net and so be very careful when I say from here on", "tokens": [366, 439, 295, 264, 3547, 294, 264, 18161, 2533, 293, 370, 312, 588, 5026, 562, 286, 584, 490, 510, 322], "temperature": 0.0, "avg_logprob": -0.091576651829045, "compression_ratio": 1.7548076923076923, "no_speech_prob": 4.247027334258746e-07}, {"id": 566, "seek": 405908, "start": 4064.84, "end": 4070.48, "text": " in in these lessons activations or parameters you've got to make sure you know what those", "tokens": [294, 294, 613, 8820, 2430, 763, 420, 9834, 291, 600, 658, 281, 652, 988, 291, 458, 437, 729], "temperature": 0.0, "avg_logprob": -0.091576651829045, "compression_ratio": 1.7548076923076923, "no_speech_prob": 4.247027334258746e-07}, {"id": 567, "seek": 405908, "start": 4070.48, "end": 4076.18, "text": " mean because that's that's the entire basically almost the entire set of numbers that exist", "tokens": [914, 570, 300, 311, 300, 311, 264, 2302, 1936, 1920, 264, 2302, 992, 295, 3547, 300, 2514], "temperature": 0.0, "avg_logprob": -0.091576651829045, "compression_ratio": 1.7548076923076923, "no_speech_prob": 4.247027334258746e-07}, {"id": 568, "seek": 405908, "start": 4076.18, "end": 4084.72, "text": " inside a neural net so activations are calculated parameters are learned we're doing this stuff", "tokens": [1854, 257, 18161, 2533, 370, 2430, 763, 366, 15598, 9834, 366, 3264, 321, 434, 884, 341, 1507], "temperature": 0.0, "avg_logprob": -0.091576651829045, "compression_ratio": 1.7548076923076923, "no_speech_prob": 4.247027334258746e-07}, {"id": 569, "seek": 408472, "start": 4084.72, "end": 4091.64, "text": " with tensors and tensors are just regularly shaped arrays rank zero tensors we call scalars", "tokens": [365, 10688, 830, 293, 10688, 830, 366, 445, 11672, 13475, 41011, 6181, 4018, 10688, 830, 321, 818, 15664, 685], "temperature": 0.0, "avg_logprob": -0.09499216647375197, "compression_ratio": 1.8864864864864865, "no_speech_prob": 9.132515970122768e-07}, {"id": 570, "seek": 408472, "start": 4091.64, "end": 4097.179999999999, "text": " rank one tensors we call vectors rank two tensors we call matrices and we continue on", "tokens": [6181, 472, 10688, 830, 321, 818, 18875, 6181, 732, 10688, 830, 321, 818, 32284, 293, 321, 2354, 322], "temperature": 0.0, "avg_logprob": -0.09499216647375197, "compression_ratio": 1.8864864864864865, "no_speech_prob": 9.132515970122768e-07}, {"id": 571, "seek": 408472, "start": 4097.179999999999, "end": 4103.679999999999, "text": " to rank three tensors rank four tensors and so forth and rank five tensors are very common", "tokens": [281, 6181, 1045, 10688, 830, 6181, 1451, 10688, 830, 293, 370, 5220, 293, 6181, 1732, 10688, 830, 366, 588, 2689], "temperature": 0.0, "avg_logprob": -0.09499216647375197, "compression_ratio": 1.8864864864864865, "no_speech_prob": 9.132515970122768e-07}, {"id": 572, "seek": 408472, "start": 4103.679999999999, "end": 4109.88, "text": " in deep learning so don't be scared of going up to higher numbers of dimensions.", "tokens": [294, 2452, 2539, 370, 500, 380, 312, 5338, 295, 516, 493, 281, 2946, 3547, 295, 12819, 13], "temperature": 0.0, "avg_logprob": -0.09499216647375197, "compression_ratio": 1.8864864864864865, "no_speech_prob": 9.132515970122768e-07}, {"id": 573, "seek": 410988, "start": 4109.88, "end": 4115.24, "text": " Okay so let's have a break oh we've got a question okay.", "tokens": [1033, 370, 718, 311, 362, 257, 1821, 1954, 321, 600, 658, 257, 1168, 1392, 13], "temperature": 0.0, "avg_logprob": -0.15181604273178997, "compression_ratio": 1.592964824120603, "no_speech_prob": 1.202940347866388e-05}, {"id": 574, "seek": 410988, "start": 4115.24, "end": 4121.08, "text": " Is there a rule of thumb for what non-linearity to choose given that there are many?", "tokens": [1119, 456, 257, 4978, 295, 9298, 337, 437, 2107, 12, 1889, 17409, 281, 2826, 2212, 300, 456, 366, 867, 30], "temperature": 0.0, "avg_logprob": -0.15181604273178997, "compression_ratio": 1.592964824120603, "no_speech_prob": 1.202940347866388e-05}, {"id": 575, "seek": 410988, "start": 4121.08, "end": 4125.16, "text": " Yeah there are many non-linearities to choose from and it doesn't generally matter very", "tokens": [865, 456, 366, 867, 2107, 12, 28263, 1088, 281, 2826, 490, 293, 309, 1177, 380, 5101, 1871, 588], "temperature": 0.0, "avg_logprob": -0.15181604273178997, "compression_ratio": 1.592964824120603, "no_speech_prob": 1.202940347866388e-05}, {"id": 576, "seek": 410988, "start": 4125.16, "end": 4135.0, "text": " much which you choose so just use ReLU or leaky ReLU or yeah whatever any anyone should", "tokens": [709, 597, 291, 2826, 370, 445, 764, 1300, 43, 52, 420, 476, 15681, 1300, 43, 52, 420, 1338, 2035, 604, 2878, 820], "temperature": 0.0, "avg_logprob": -0.15181604273178997, "compression_ratio": 1.592964824120603, "no_speech_prob": 1.202940347866388e-05}, {"id": 577, "seek": 413500, "start": 4135.0, "end": 4142.16, "text": " work fine later on we'll we'll look at the minor differences between between them but", "tokens": [589, 2489, 1780, 322, 321, 603, 321, 603, 574, 412, 264, 6696, 7300, 1296, 1296, 552, 457], "temperature": 0.0, "avg_logprob": -0.11618562502281687, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.1381282497022767e-06}, {"id": 578, "seek": 413500, "start": 4142.16, "end": 4146.88, "text": " it's not so much something that you pick on a per problem it's more like some take a little", "tokens": [309, 311, 406, 370, 709, 746, 300, 291, 1888, 322, 257, 680, 1154, 309, 311, 544, 411, 512, 747, 257, 707], "temperature": 0.0, "avg_logprob": -0.11618562502281687, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.1381282497022767e-06}, {"id": 579, "seek": 413500, "start": 4146.88, "end": 4150.26, "text": " bit longer and a little bit more accurate and some are a bit faster and a little bit", "tokens": [857, 2854, 293, 257, 707, 857, 544, 8559, 293, 512, 366, 257, 857, 4663, 293, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.11618562502281687, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.1381282497022767e-06}, {"id": 580, "seek": 413500, "start": 4150.26, "end": 4151.26, "text": " less accurate.", "tokens": [1570, 8559, 13], "temperature": 0.0, "avg_logprob": -0.11618562502281687, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.1381282497022767e-06}, {"id": 581, "seek": 413500, "start": 4151.26, "end": 4157.56, "text": " That's a good question okay so before you move on it's really important that you finish", "tokens": [663, 311, 257, 665, 1168, 1392, 370, 949, 291, 1286, 322, 309, 311, 534, 1021, 300, 291, 2413], "temperature": 0.0, "avg_logprob": -0.11618562502281687, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.1381282497022767e-06}, {"id": 582, "seek": 413500, "start": 4157.56, "end": 4162.56, "text": " the questionnaire for this chapter because there's a whole lot of concepts that we've", "tokens": [264, 44702, 337, 341, 7187, 570, 456, 311, 257, 1379, 688, 295, 10392, 300, 321, 600], "temperature": 0.0, "avg_logprob": -0.11618562502281687, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.1381282497022767e-06}, {"id": 583, "seek": 416256, "start": 4162.56, "end": 4169.360000000001, "text": " just done so you know try to go through the questionnaire go back and relook at the notebook", "tokens": [445, 1096, 370, 291, 458, 853, 281, 352, 807, 264, 44702, 352, 646, 293, 319, 12747, 412, 264, 21060], "temperature": 0.0, "avg_logprob": -0.18139222462972004, "compression_ratio": 1.5796178343949046, "no_speech_prob": 1.0845124052139e-06}, {"id": 584, "seek": 416256, "start": 4169.360000000001, "end": 4173.4400000000005, "text": " and please run the code do the experiments and make sure it makes sense.", "tokens": [293, 1767, 1190, 264, 3089, 360, 264, 12050, 293, 652, 988, 309, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.18139222462972004, "compression_ratio": 1.5796178343949046, "no_speech_prob": 1.0845124052139e-06}, {"id": 585, "seek": 416256, "start": 4173.4400000000005, "end": 4185.400000000001, "text": " All right let's have a seven minute break see you back here in seven minutes time.", "tokens": [1057, 558, 718, 311, 362, 257, 3407, 3456, 1821, 536, 291, 646, 510, 294, 3407, 2077, 565, 13], "temperature": 0.0, "avg_logprob": -0.18139222462972004, "compression_ratio": 1.5796178343949046, "no_speech_prob": 1.0845124052139e-06}, {"id": 586, "seek": 418540, "start": 4185.4, "end": 4194.92, "text": " Okay welcome back so now that we know how to create and train a neural net let's cycle", "tokens": [1033, 2928, 646, 370, 586, 300, 321, 458, 577, 281, 1884, 293, 3847, 257, 18161, 2533, 718, 311, 6586], "temperature": 0.0, "avg_logprob": -0.07796715593886101, "compression_ratio": 1.7959183673469388, "no_speech_prob": 1.7061727248801617e-06}, {"id": 587, "seek": 418540, "start": 4194.92, "end": 4202.44, "text": " back and look deeper at some applications and so we're going to try to kind of interpolate", "tokens": [646, 293, 574, 7731, 412, 512, 5821, 293, 370, 321, 434, 516, 281, 853, 281, 733, 295, 44902, 473], "temperature": 0.0, "avg_logprob": -0.07796715593886101, "compression_ratio": 1.7959183673469388, "no_speech_prob": 1.7061727248801617e-06}, {"id": 588, "seek": 418540, "start": 4202.44, "end": 4207.879999999999, "text": " in from one end we've done the kind of from scratch version at the other end we've done", "tokens": [294, 490, 472, 917, 321, 600, 1096, 264, 733, 295, 490, 8459, 3037, 412, 264, 661, 917, 321, 600, 1096], "temperature": 0.0, "avg_logprob": -0.07796715593886101, "compression_ratio": 1.7959183673469388, "no_speech_prob": 1.7061727248801617e-06}, {"id": 589, "seek": 418540, "start": 4207.879999999999, "end": 4212.32, "text": " the kind of four lines of code version and we're going to gradually nibble at each end", "tokens": [264, 733, 295, 1451, 3876, 295, 3089, 3037, 293, 321, 434, 516, 281, 13145, 38956, 638, 412, 1184, 917], "temperature": 0.0, "avg_logprob": -0.07796715593886101, "compression_ratio": 1.7959183673469388, "no_speech_prob": 1.7061727248801617e-06}, {"id": 590, "seek": 421232, "start": 4212.32, "end": 4218.36, "text": " until we find ourselves in the middle and we've we've we've touched on all of it.", "tokens": [1826, 321, 915, 4175, 294, 264, 2808, 293, 321, 600, 321, 600, 321, 600, 9828, 322, 439, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.11719000339508057, "compression_ratio": 1.5900621118012421, "no_speech_prob": 1.994724243559176e-06}, {"id": 591, "seek": 421232, "start": 4218.36, "end": 4223.04, "text": " So let's go back up to the kind of the four lines of code version and and delve a little", "tokens": [407, 718, 311, 352, 646, 493, 281, 264, 733, 295, 264, 1451, 3876, 295, 3089, 3037, 293, 293, 43098, 257, 707], "temperature": 0.0, "avg_logprob": -0.11719000339508057, "compression_ratio": 1.5900621118012421, "no_speech_prob": 1.994724243559176e-06}, {"id": 592, "seek": 421232, "start": 4223.04, "end": 4236.099999999999, "text": " deeper so let's go back to pets and let's think though about like how do you actually", "tokens": [7731, 370, 718, 311, 352, 646, 281, 19897, 293, 718, 311, 519, 1673, 466, 411, 577, 360, 291, 767], "temperature": 0.0, "avg_logprob": -0.11719000339508057, "compression_ratio": 1.5900621118012421, "no_speech_prob": 1.994724243559176e-06}, {"id": 593, "seek": 423610, "start": 4236.1, "end": 4245.0, "text": " you know start with a new data set and figure out how to use it.", "tokens": [291, 458, 722, 365, 257, 777, 1412, 992, 293, 2573, 484, 577, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.1486019814151457, "compression_ratio": 1.6822916666666667, "no_speech_prob": 4.0294285099662375e-06}, {"id": 594, "seek": 423610, "start": 4245.0, "end": 4249.96, "text": " So it you know the the data sets we provide it's easy enough to untar them you just say", "tokens": [407, 309, 291, 458, 264, 264, 1412, 6352, 321, 2893, 309, 311, 1858, 1547, 281, 1701, 289, 552, 291, 445, 584], "temperature": 0.0, "avg_logprob": -0.1486019814151457, "compression_ratio": 1.6822916666666667, "no_speech_prob": 4.0294285099662375e-06}, {"id": 595, "seek": 423610, "start": 4249.96, "end": 4256.4400000000005, "text": " untar that'll download it and untar it if it's a data set that you're getting you can", "tokens": [1701, 289, 300, 603, 5484, 309, 293, 1701, 289, 309, 498, 309, 311, 257, 1412, 992, 300, 291, 434, 1242, 291, 393], "temperature": 0.0, "avg_logprob": -0.1486019814151457, "compression_ratio": 1.6822916666666667, "no_speech_prob": 4.0294285099662375e-06}, {"id": 596, "seek": 423610, "start": 4256.4400000000005, "end": 4264.42, "text": " just use the terminal or either or Python or whatever so let's assume we have a path", "tokens": [445, 764, 264, 14709, 420, 2139, 420, 15329, 420, 2035, 370, 718, 311, 6552, 321, 362, 257, 3100], "temperature": 0.0, "avg_logprob": -0.1486019814151457, "compression_ratio": 1.6822916666666667, "no_speech_prob": 4.0294285099662375e-06}, {"id": 597, "seek": 426442, "start": 4264.42, "end": 4269.4400000000005, "text": " that's pointing at something so initially you don't you don't know what that something", "tokens": [300, 311, 12166, 412, 746, 370, 9105, 291, 500, 380, 291, 500, 380, 458, 437, 300, 746], "temperature": 0.0, "avg_logprob": -0.12718938697468152, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.1911059800695512e-06}, {"id": 598, "seek": 426442, "start": 4269.4400000000005, "end": 4271.92, "text": " is.", "tokens": [307, 13], "temperature": 0.0, "avg_logprob": -0.12718938697468152, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.1911059800695512e-06}, {"id": 599, "seek": 426442, "start": 4271.92, "end": 4277.12, "text": " So we can start by doing LS to have a look and see what's inside there so the pets data", "tokens": [407, 321, 393, 722, 538, 884, 36657, 281, 362, 257, 574, 293, 536, 437, 311, 1854, 456, 370, 264, 19897, 1412], "temperature": 0.0, "avg_logprob": -0.12718938697468152, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.1911059800695512e-06}, {"id": 600, "seek": 426442, "start": 4277.12, "end": 4284.64, "text": " set that we saw in lesson one contains three things annotations images and models and you'll", "tokens": [992, 300, 321, 1866, 294, 6898, 472, 8306, 1045, 721, 25339, 763, 5267, 293, 5245, 293, 291, 603], "temperature": 0.0, "avg_logprob": -0.12718938697468152, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.1911059800695512e-06}, {"id": 601, "seek": 426442, "start": 4284.64, "end": 4290.12, "text": " see we have this little trick here where we say path dot base path equals and then the", "tokens": [536, 321, 362, 341, 707, 4282, 510, 689, 321, 584, 3100, 5893, 3096, 3100, 6915, 293, 550, 264], "temperature": 0.0, "avg_logprob": -0.12718938697468152, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.1911059800695512e-06}, {"id": 602, "seek": 429012, "start": 4290.12, "end": 4295.16, "text": " path to our data and that's just does a little simple thing where when we print it out it", "tokens": [3100, 281, 527, 1412, 293, 300, 311, 445, 775, 257, 707, 2199, 551, 689, 562, 321, 4482, 309, 484, 309], "temperature": 0.0, "avg_logprob": -0.09656842175651999, "compression_ratio": 1.6698564593301435, "no_speech_prob": 4.737900951568008e-07}, {"id": 603, "seek": 429012, "start": 4295.16, "end": 4301.86, "text": " just doesn't show us it just shows us relative to this path which is a bit convenient.", "tokens": [445, 1177, 380, 855, 505, 309, 445, 3110, 505, 4972, 281, 341, 3100, 597, 307, 257, 857, 10851, 13], "temperature": 0.0, "avg_logprob": -0.09656842175651999, "compression_ratio": 1.6698564593301435, "no_speech_prob": 4.737900951568008e-07}, {"id": 604, "seek": 429012, "start": 4301.86, "end": 4310.5199999999995, "text": " So if you go and have a look at the readme for the original pets data set it tells you", "tokens": [407, 498, 291, 352, 293, 362, 257, 574, 412, 264, 1401, 1398, 337, 264, 3380, 19897, 1412, 992, 309, 5112, 291], "temperature": 0.0, "avg_logprob": -0.09656842175651999, "compression_ratio": 1.6698564593301435, "no_speech_prob": 4.737900951568008e-07}, {"id": 605, "seek": 429012, "start": 4310.5199999999995, "end": 4316.28, "text": " what these images and annotations folders are and not surprisingly the images path if", "tokens": [437, 613, 5267, 293, 25339, 763, 31082, 366, 293, 406, 17600, 264, 5267, 3100, 498], "temperature": 0.0, "avg_logprob": -0.09656842175651999, "compression_ratio": 1.6698564593301435, "no_speech_prob": 4.737900951568008e-07}, {"id": 606, "seek": 431628, "start": 4316.28, "end": 4323.48, "text": " we go path slash images that's how we use path lib to grab a subdirectory and then LS", "tokens": [321, 352, 3100, 17330, 5267, 300, 311, 577, 321, 764, 3100, 22854, 281, 4444, 257, 31662, 11890, 827, 293, 550, 36657], "temperature": 0.0, "avg_logprob": -0.11146181891946232, "compression_ratio": 1.6919431279620853, "no_speech_prob": 1.1979295777564403e-07}, {"id": 607, "seek": 431628, "start": 4323.48, "end": 4331.36, "text": " we can see here are the names that the paths to the images as it mentions here most functions", "tokens": [321, 393, 536, 510, 366, 264, 5288, 300, 264, 14518, 281, 264, 5267, 382, 309, 23844, 510, 881, 6828], "temperature": 0.0, "avg_logprob": -0.11146181891946232, "compression_ratio": 1.6919431279620853, "no_speech_prob": 1.1979295777564403e-07}, {"id": 608, "seek": 431628, "start": 4331.36, "end": 4337.88, "text": " and methods in fast AI which return a collection don't return a Python list but they return", "tokens": [293, 7150, 294, 2370, 7318, 597, 2736, 257, 5765, 500, 380, 2736, 257, 15329, 1329, 457, 436, 2736], "temperature": 0.0, "avg_logprob": -0.11146181891946232, "compression_ratio": 1.6919431279620853, "no_speech_prob": 1.1979295777564403e-07}, {"id": 609, "seek": 431628, "start": 4337.88, "end": 4345.099999999999, "text": " a capital L and a capital L as we briefly mentioned is basically an enhanced list one", "tokens": [257, 4238, 441, 293, 257, 4238, 441, 382, 321, 10515, 2835, 307, 1936, 364, 21191, 1329, 472], "temperature": 0.0, "avg_logprob": -0.11146181891946232, "compression_ratio": 1.6919431279620853, "no_speech_prob": 1.1979295777564403e-07}, {"id": 610, "seek": 434510, "start": 4345.1, "end": 4349.700000000001, "text": " of the enhancements is the way it prints the representation of its starts by showing you", "tokens": [295, 264, 11985, 1117, 307, 264, 636, 309, 22305, 264, 10290, 295, 1080, 3719, 538, 4099, 291], "temperature": 0.0, "avg_logprob": -0.13169347156177869, "compression_ratio": 1.674641148325359, "no_speech_prob": 7.979932803436895e-08}, {"id": 611, "seek": 434510, "start": 4349.700000000001, "end": 4358.68, "text": " how many items there are in the list in the collection so there's 7394 images and it if", "tokens": [577, 867, 4754, 456, 366, 294, 264, 1329, 294, 264, 5765, 370, 456, 311, 1614, 12493, 19, 5267, 293, 309, 498], "temperature": 0.0, "avg_logprob": -0.13169347156177869, "compression_ratio": 1.674641148325359, "no_speech_prob": 7.979932803436895e-08}, {"id": 612, "seek": 434510, "start": 4358.68, "end": 4364.42, "text": " there's more than 10 things it truncates it and just says dot dot dot to avoid filling", "tokens": [456, 311, 544, 813, 1266, 721, 309, 504, 409, 66, 1024, 309, 293, 445, 1619, 5893, 5893, 5893, 281, 5042, 10623], "temperature": 0.0, "avg_logprob": -0.13169347156177869, "compression_ratio": 1.674641148325359, "no_speech_prob": 7.979932803436895e-08}, {"id": 613, "seek": 434510, "start": 4364.42, "end": 4372.26, "text": " up your screen so there's a couple of little conveniences there and so we can see from", "tokens": [493, 428, 2568, 370, 456, 311, 257, 1916, 295, 707, 7158, 14004, 456, 293, 370, 321, 393, 536, 490], "temperature": 0.0, "avg_logprob": -0.13169347156177869, "compression_ratio": 1.674641148325359, "no_speech_prob": 7.979932803436895e-08}, {"id": 614, "seek": 437226, "start": 4372.26, "end": 4381.9800000000005, "text": " this output that the file name as we mentioned in lesson one if the first letter is a capital", "tokens": [341, 5598, 300, 264, 3991, 1315, 382, 321, 2835, 294, 6898, 472, 498, 264, 700, 5063, 307, 257, 4238], "temperature": 0.0, "avg_logprob": -0.07081432021066045, "compression_ratio": 1.8153846153846154, "no_speech_prob": 5.771884843852604e-06}, {"id": 615, "seek": 437226, "start": 4381.9800000000005, "end": 4388.52, "text": " it means it's a cat and if the first letter is lowercase it means it's a dog but this", "tokens": [309, 1355, 309, 311, 257, 3857, 293, 498, 264, 700, 5063, 307, 3126, 9765, 309, 1355, 309, 311, 257, 3000, 457, 341], "temperature": 0.0, "avg_logprob": -0.07081432021066045, "compression_ratio": 1.8153846153846154, "no_speech_prob": 5.771884843852604e-06}, {"id": 616, "seek": 437226, "start": 4388.52, "end": 4392.02, "text": " time we're going to do something a bit more complex a lot more complex which is figure", "tokens": [565, 321, 434, 516, 281, 360, 746, 257, 857, 544, 3997, 257, 688, 544, 3997, 597, 307, 2573], "temperature": 0.0, "avg_logprob": -0.07081432021066045, "compression_ratio": 1.8153846153846154, "no_speech_prob": 5.771884843852604e-06}, {"id": 617, "seek": 437226, "start": 4392.02, "end": 4398.54, "text": " out what breed it is and so you can see the breed is kind of everything up to after the", "tokens": [484, 437, 18971, 309, 307, 293, 370, 291, 393, 536, 264, 18971, 307, 733, 295, 1203, 493, 281, 934, 264], "temperature": 0.0, "avg_logprob": -0.07081432021066045, "compression_ratio": 1.8153846153846154, "no_speech_prob": 5.771884843852604e-06}, {"id": 618, "seek": 439854, "start": 4398.54, "end": 4403.82, "text": " in the file name it's everything up to the last underscore and before this number is", "tokens": [294, 264, 3991, 1315, 309, 311, 1203, 493, 281, 264, 1036, 37556, 293, 949, 341, 1230, 307], "temperature": 0.0, "avg_logprob": -0.09988993214022729, "compression_ratio": 1.6981132075471699, "no_speech_prob": 4.116357672501181e-07}, {"id": 619, "seek": 439854, "start": 4403.82, "end": 4411.48, "text": " the breed so we want to label everything with its breed so we're going to take advantage", "tokens": [264, 18971, 370, 321, 528, 281, 7645, 1203, 365, 1080, 18971, 370, 321, 434, 516, 281, 747, 5002], "temperature": 0.0, "avg_logprob": -0.09988993214022729, "compression_ratio": 1.6981132075471699, "no_speech_prob": 4.116357672501181e-07}, {"id": 620, "seek": 439854, "start": 4411.48, "end": 4423.8, "text": " of this structure so the way I would do this is to use a regular expression a regular expression", "tokens": [295, 341, 3877, 370, 264, 636, 286, 576, 360, 341, 307, 281, 764, 257, 3890, 6114, 257, 3890, 6114], "temperature": 0.0, "avg_logprob": -0.09988993214022729, "compression_ratio": 1.6981132075471699, "no_speech_prob": 4.116357672501181e-07}, {"id": 621, "seek": 442380, "start": 4423.8, "end": 4429.5, "text": " is something that looks at a string and basically lets you kind of pull it apart into its pieces", "tokens": [307, 746, 300, 1542, 412, 257, 6798, 293, 1936, 6653, 291, 733, 295, 2235, 309, 4936, 666, 1080, 3755], "temperature": 0.0, "avg_logprob": -0.09406191759770459, "compression_ratio": 1.748062015503876, "no_speech_prob": 1.1911042747669853e-06}, {"id": 622, "seek": 442380, "start": 4429.5, "end": 4435.66, "text": " in very flexible ways this kind of simple little language for doing that if you haven't", "tokens": [294, 588, 11358, 2098, 341, 733, 295, 2199, 707, 2856, 337, 884, 300, 498, 291, 2378, 380], "temperature": 0.0, "avg_logprob": -0.09406191759770459, "compression_ratio": 1.748062015503876, "no_speech_prob": 1.1911042747669853e-06}, {"id": 623, "seek": 442380, "start": 4435.66, "end": 4440.9800000000005, "text": " used regular expressions before and please Google regular expression tutorial now and", "tokens": [1143, 3890, 15277, 949, 293, 1767, 3329, 3890, 6114, 7073, 586, 293], "temperature": 0.0, "avg_logprob": -0.09406191759770459, "compression_ratio": 1.748062015503876, "no_speech_prob": 1.1911042747669853e-06}, {"id": 624, "seek": 442380, "start": 4440.9800000000005, "end": 4444.46, "text": " look it's going to be like one of the most useful tools you'll come across in your life", "tokens": [574, 309, 311, 516, 281, 312, 411, 472, 295, 264, 881, 4420, 3873, 291, 603, 808, 2108, 294, 428, 993], "temperature": 0.0, "avg_logprob": -0.09406191759770459, "compression_ratio": 1.748062015503876, "no_speech_prob": 1.1911042747669853e-06}, {"id": 625, "seek": 442380, "start": 4444.46, "end": 4451.4800000000005, "text": " I use them almost every day I will go to details about how to use them since there's so many", "tokens": [286, 764, 552, 1920, 633, 786, 286, 486, 352, 281, 4365, 466, 577, 281, 764, 552, 1670, 456, 311, 370, 867], "temperature": 0.0, "avg_logprob": -0.09406191759770459, "compression_ratio": 1.748062015503876, "no_speech_prob": 1.1911042747669853e-06}, {"id": 626, "seek": 445148, "start": 4451.48, "end": 4456.259999999999, "text": " great tutorials and there's also a lot of great like exercises you know there's regex", "tokens": [869, 17616, 293, 456, 311, 611, 257, 688, 295, 869, 411, 11900, 291, 458, 456, 311, 319, 432, 87], "temperature": 0.0, "avg_logprob": -0.1083065668741862, "compression_ratio": 1.8071065989847717, "no_speech_prob": 5.338117716746638e-06}, {"id": 627, "seek": 445148, "start": 4456.259999999999, "end": 4462.12, "text": " regex is short for regular expression there's regex crosswords there's regex Q&A there's", "tokens": [319, 432, 87, 307, 2099, 337, 3890, 6114, 456, 311, 319, 432, 87, 3278, 13832, 456, 311, 319, 432, 87, 1249, 5, 32, 456, 311], "temperature": 0.0, "avg_logprob": -0.1083065668741862, "compression_ratio": 1.8071065989847717, "no_speech_prob": 5.338117716746638e-06}, {"id": 628, "seek": 445148, "start": 4462.12, "end": 4469.5199999999995, "text": " all kinds of cool regex things a lot of people like me love this tool in order to there's", "tokens": [439, 3685, 295, 1627, 319, 432, 87, 721, 257, 688, 295, 561, 411, 385, 959, 341, 2290, 294, 1668, 281, 456, 311], "temperature": 0.0, "avg_logprob": -0.1083065668741862, "compression_ratio": 1.8071065989847717, "no_speech_prob": 5.338117716746638e-06}, {"id": 629, "seek": 445148, "start": 4469.5199999999995, "end": 4476.66, "text": " also a regex lesson in the fastai nlp course maybe even two regex lessons oh yeah I'm sorry", "tokens": [611, 257, 319, 432, 87, 6898, 294, 264, 2370, 1301, 297, 75, 79, 1164, 1310, 754, 732, 319, 432, 87, 8820, 1954, 1338, 286, 478, 2597], "temperature": 0.0, "avg_logprob": -0.1083065668741862, "compression_ratio": 1.8071065989847717, "no_speech_prob": 5.338117716746638e-06}, {"id": 630, "seek": 447666, "start": 4476.66, "end": 4488.86, "text": " for forgetting about the fastai nlp course what an excellent resource that is so regular", "tokens": [337, 25428, 466, 264, 2370, 1301, 297, 75, 79, 1164, 437, 364, 7103, 7684, 300, 307, 370, 3890], "temperature": 0.0, "avg_logprob": -0.106364029041235, "compression_ratio": 1.6325301204819278, "no_speech_prob": 2.0904406028421363e-06}, {"id": 631, "seek": 447666, "start": 4488.86, "end": 4491.98, "text": " expressions are a bit hard to get right the first time so the best thing to do is to get", "tokens": [15277, 366, 257, 857, 1152, 281, 483, 558, 264, 700, 565, 370, 264, 1151, 551, 281, 360, 307, 281, 483], "temperature": 0.0, "avg_logprob": -0.106364029041235, "compression_ratio": 1.6325301204819278, "no_speech_prob": 2.0904406028421363e-06}, {"id": 632, "seek": 447666, "start": 4491.98, "end": 4497.46, "text": " a sample string so a good way to do that would be to just grab one of the file names so let's", "tokens": [257, 6889, 6798, 370, 257, 665, 636, 281, 360, 300, 576, 312, 281, 445, 4444, 472, 295, 264, 3991, 5288, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.106364029041235, "compression_ratio": 1.6325301204819278, "no_speech_prob": 2.0904406028421363e-06}, {"id": 633, "seek": 449746, "start": 4497.46, "end": 4508.02, "text": " pop it in F name and then you can experiment with regular expressions so re is the regular", "tokens": [1665, 309, 294, 479, 1315, 293, 550, 291, 393, 5120, 365, 3890, 15277, 370, 319, 307, 264, 3890], "temperature": 0.0, "avg_logprob": -0.10878547032674153, "compression_ratio": 1.7871287128712872, "no_speech_prob": 2.3320594664255623e-06}, {"id": 634, "seek": 449746, "start": 4508.02, "end": 4515.18, "text": " expression module in Python and find all we'll just grab all the parts of a regular expression", "tokens": [6114, 10088, 294, 15329, 293, 915, 439, 321, 603, 445, 4444, 439, 264, 3166, 295, 257, 3890, 6114], "temperature": 0.0, "avg_logprob": -0.10878547032674153, "compression_ratio": 1.7871287128712872, "no_speech_prob": 2.3320594664255623e-06}, {"id": 635, "seek": 449746, "start": 4515.18, "end": 4519.74, "text": " that have parentheses around them so this regular expression and R is a special kind", "tokens": [300, 362, 34153, 926, 552, 370, 341, 3890, 6114, 293, 497, 307, 257, 2121, 733], "temperature": 0.0, "avg_logprob": -0.10878547032674153, "compression_ratio": 1.7871287128712872, "no_speech_prob": 2.3320594664255623e-06}, {"id": 636, "seek": 449746, "start": 4519.74, "end": 4525.06, "text": " of string in Python which basically says don't treat backslash as special because normally", "tokens": [295, 6798, 294, 15329, 597, 1936, 1619, 500, 380, 2387, 646, 10418, 1299, 382, 2121, 570, 5646], "temperature": 0.0, "avg_logprob": -0.10878547032674153, "compression_ratio": 1.7871287128712872, "no_speech_prob": 2.3320594664255623e-06}, {"id": 637, "seek": 452506, "start": 4525.06, "end": 4535.38, "text": " in Python like backslash n means a new line so here's a string which I'm going to capture", "tokens": [294, 15329, 411, 646, 10418, 1299, 297, 1355, 257, 777, 1622, 370, 510, 311, 257, 6798, 597, 286, 478, 516, 281, 7983], "temperature": 0.0, "avg_logprob": -0.09929007016695462, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.2098611250621616e-06}, {"id": 638, "seek": 452506, "start": 4535.38, "end": 4543.02, "text": " any letter one or more times followed by an underscore followed by a digit one or more", "tokens": [604, 5063, 472, 420, 544, 1413, 6263, 538, 364, 37556, 6263, 538, 257, 14293, 472, 420, 544], "temperature": 0.0, "avg_logprob": -0.09929007016695462, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.2098611250621616e-06}, {"id": 639, "seek": 452506, "start": 4543.02, "end": 4549.54, "text": " times followed by anything I probably should have used backslash dot but that's fine followed", "tokens": [1413, 6263, 538, 1340, 286, 1391, 820, 362, 1143, 646, 10418, 1299, 5893, 457, 300, 311, 2489, 6263], "temperature": 0.0, "avg_logprob": -0.09929007016695462, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.2098611250621616e-06}, {"id": 640, "seek": 454954, "start": 4549.54, "end": 4557.22, "text": " by the letters JPG followed by the end of the string and so if I call that regular expression", "tokens": [538, 264, 7825, 34336, 38, 6263, 538, 264, 917, 295, 264, 6798, 293, 370, 498, 286, 818, 300, 3890, 6114], "temperature": 0.0, "avg_logprob": -0.11633574244487717, "compression_ratio": 1.6712328767123288, "no_speech_prob": 6.893600357216201e-07}, {"id": 641, "seek": 454954, "start": 4557.22, "end": 4566.14, "text": " against my file names name oh looks good right so we kind of check it out so now that seems", "tokens": [1970, 452, 3991, 5288, 1315, 1954, 1542, 665, 558, 370, 321, 733, 295, 1520, 309, 484, 370, 586, 300, 2544], "temperature": 0.0, "avg_logprob": -0.11633574244487717, "compression_ratio": 1.6712328767123288, "no_speech_prob": 6.893600357216201e-07}, {"id": 642, "seek": 454954, "start": 4566.14, "end": 4572.26, "text": " to work we can create a data block where the independent variables are images the dependent", "tokens": [281, 589, 321, 393, 1884, 257, 1412, 3461, 689, 264, 6695, 9102, 366, 5267, 264, 12334], "temperature": 0.0, "avg_logprob": -0.11633574244487717, "compression_ratio": 1.6712328767123288, "no_speech_prob": 6.893600357216201e-07}, {"id": 643, "seek": 454954, "start": 4572.26, "end": 4578.98, "text": " variables are categories just like before get items is going to be get image files we're", "tokens": [9102, 366, 10479, 445, 411, 949, 483, 4754, 307, 516, 281, 312, 483, 3256, 7098, 321, 434], "temperature": 0.0, "avg_logprob": -0.11633574244487717, "compression_ratio": 1.6712328767123288, "no_speech_prob": 6.893600357216201e-07}, {"id": 644, "seek": 457898, "start": 4578.98, "end": 4586.86, "text": " going to split it randomly as per usual and then we're going to get the label by calling", "tokens": [516, 281, 7472, 309, 16979, 382, 680, 7713, 293, 550, 321, 434, 516, 281, 483, 264, 7645, 538, 5141], "temperature": 0.0, "avg_logprob": -0.14832790986991223, "compression_ratio": 1.8282828282828283, "no_speech_prob": 8.186349873540166e-07}, {"id": 645, "seek": 457898, "start": 4586.86, "end": 4597.0199999999995, "text": " regex labeler which is a just a handy little fast a class which labels things with a regular", "tokens": [319, 432, 87, 7645, 260, 597, 307, 257, 445, 257, 13239, 707, 2370, 257, 1508, 597, 16949, 721, 365, 257, 3890], "temperature": 0.0, "avg_logprob": -0.14832790986991223, "compression_ratio": 1.8282828282828283, "no_speech_prob": 8.186349873540166e-07}, {"id": 646, "seek": 457898, "start": 4597.0199999999995, "end": 4601.7, "text": " expression we can't call the regular expression this particular regular expression directly", "tokens": [6114, 321, 393, 380, 818, 264, 3890, 6114, 341, 1729, 3890, 6114, 3838], "temperature": 0.0, "avg_logprob": -0.14832790986991223, "compression_ratio": 1.8282828282828283, "no_speech_prob": 8.186349873540166e-07}, {"id": 647, "seek": 457898, "start": 4601.7, "end": 4608.339999999999, "text": " on the path lib path object we actually want to call it on the name attribute and faster", "tokens": [322, 264, 3100, 22854, 3100, 2657, 321, 767, 528, 281, 818, 309, 322, 264, 1315, 19667, 293, 4663], "temperature": 0.0, "avg_logprob": -0.14832790986991223, "compression_ratio": 1.8282828282828283, "no_speech_prob": 8.186349873540166e-07}, {"id": 648, "seek": 460834, "start": 4608.34, "end": 4615.22, "text": " I has a nice little function called using atra using attribute which takes this function", "tokens": [286, 575, 257, 1481, 707, 2445, 1219, 1228, 412, 424, 1228, 19667, 597, 2516, 341, 2445], "temperature": 0.0, "avg_logprob": -0.1369126227594191, "compression_ratio": 1.7115384615384615, "no_speech_prob": 9.570819656801177e-07}, {"id": 649, "seek": 460834, "start": 4615.22, "end": 4620.66, "text": " and changes it to a function which will be passed this attribute that's going to be using", "tokens": [293, 2962, 309, 281, 257, 2445, 597, 486, 312, 4678, 341, 19667, 300, 311, 516, 281, 312, 1228], "temperature": 0.0, "avg_logprob": -0.1369126227594191, "compression_ratio": 1.7115384615384615, "no_speech_prob": 9.570819656801177e-07}, {"id": 650, "seek": 460834, "start": 4620.66, "end": 4629.66, "text": " regex labeler on the name attribute and then from that data block we can create the data", "tokens": [319, 432, 87, 7645, 260, 322, 264, 1315, 19667, 293, 550, 490, 300, 1412, 3461, 321, 393, 1884, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1369126227594191, "compression_ratio": 1.7115384615384615, "no_speech_prob": 9.570819656801177e-07}, {"id": 651, "seek": 462966, "start": 4629.66, "end": 4640.139999999999, "text": " loaders as usual there's two interesting lines here resize and or transforms or transforms", "tokens": [3677, 433, 382, 7713, 456, 311, 732, 1880, 3876, 510, 50069, 293, 420, 35592, 420, 35592], "temperature": 0.0, "avg_logprob": -0.1278827985127767, "compression_ratio": 1.7018633540372672, "no_speech_prob": 1.1544606195457163e-06}, {"id": 652, "seek": 462966, "start": 4640.139999999999, "end": 4652.74, "text": " we have seen before in notebook 2 in the section called data augmentation and so or transforms", "tokens": [321, 362, 1612, 949, 294, 21060, 568, 294, 264, 3541, 1219, 1412, 14501, 19631, 293, 370, 420, 35592], "temperature": 0.0, "avg_logprob": -0.1278827985127767, "compression_ratio": 1.7018633540372672, "no_speech_prob": 1.1544606195457163e-06}, {"id": 653, "seek": 462966, "start": 4652.74, "end": 4658.84, "text": " was the thing which can zoom in and zoom out and warp and rotate and change contrast and", "tokens": [390, 264, 551, 597, 393, 8863, 294, 293, 8863, 484, 293, 36030, 293, 13121, 293, 1319, 8712, 293], "temperature": 0.0, "avg_logprob": -0.1278827985127767, "compression_ratio": 1.7018633540372672, "no_speech_prob": 1.1544606195457163e-06}, {"id": 654, "seek": 465884, "start": 4658.84, "end": 4665.32, "text": " change brightness and so forth and flip to kind of give us almost it's like giving us", "tokens": [1319, 21367, 293, 370, 5220, 293, 7929, 281, 733, 295, 976, 505, 1920, 309, 311, 411, 2902, 505], "temperature": 0.0, "avg_logprob": -0.06671085600125587, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.0188044825554243e-06}, {"id": 655, "seek": 465884, "start": 4665.32, "end": 4675.54, "text": " more data being generated synthetically from the data we already have and we also learned", "tokens": [544, 1412, 885, 10833, 10657, 22652, 490, 264, 1412, 321, 1217, 362, 293, 321, 611, 3264], "temperature": 0.0, "avg_logprob": -0.06671085600125587, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.0188044825554243e-06}, {"id": 656, "seek": 465884, "start": 4675.54, "end": 4683.38, "text": " about random resize crop which is a kind of a really cool way of getting ensuring you", "tokens": [466, 4974, 50069, 9086, 597, 307, 257, 733, 295, 257, 534, 1627, 636, 295, 1242, 16882, 291], "temperature": 0.0, "avg_logprob": -0.06671085600125587, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.0188044825554243e-06}, {"id": 657, "seek": 468338, "start": 4683.38, "end": 4694.42, "text": " get square images at the same time that you're augmenting the data here we have a resize", "tokens": [483, 3732, 5267, 412, 264, 912, 565, 300, 291, 434, 29919, 278, 264, 1412, 510, 321, 362, 257, 50069], "temperature": 0.0, "avg_logprob": -0.09465348359310266, "compression_ratio": 1.6219512195121952, "no_speech_prob": 2.0377449061470543e-07}, {"id": 658, "seek": 468338, "start": 4694.42, "end": 4699.38, "text": " to a really large image but you know by deep learning standards 460 by 460 is a really", "tokens": [281, 257, 534, 2416, 3256, 457, 291, 458, 538, 2452, 2539, 7787, 1017, 4550, 538, 1017, 4550, 307, 257, 534], "temperature": 0.0, "avg_logprob": -0.09465348359310266, "compression_ratio": 1.6219512195121952, "no_speech_prob": 2.0377449061470543e-07}, {"id": 659, "seek": 468338, "start": 4699.38, "end": 4706.04, "text": " large image and then we're using or transforms with a size so that's actually going to use", "tokens": [2416, 3256, 293, 550, 321, 434, 1228, 420, 35592, 365, 257, 2744, 370, 300, 311, 767, 516, 281, 764], "temperature": 0.0, "avg_logprob": -0.09465348359310266, "compression_ratio": 1.6219512195121952, "no_speech_prob": 2.0377449061470543e-07}, {"id": 660, "seek": 470604, "start": 4706.04, "end": 4716.38, "text": " random resize crop to a smaller size why are we doing that", "tokens": [4974, 50069, 9086, 281, 257, 4356, 2744, 983, 366, 321, 884, 300], "temperature": 0.0, "avg_logprob": -0.1330784851650022, "compression_ratio": 1.484076433121019, "no_speech_prob": 7.224428486551915e-07}, {"id": 661, "seek": 470604, "start": 4716.38, "end": 4720.38, "text": " this particular combination of two steps does something which I think is unique to fast", "tokens": [341, 1729, 6562, 295, 732, 4439, 775, 746, 597, 286, 519, 307, 3845, 281, 2370], "temperature": 0.0, "avg_logprob": -0.1330784851650022, "compression_ratio": 1.484076433121019, "no_speech_prob": 7.224428486551915e-07}, {"id": 662, "seek": 470604, "start": 4720.38, "end": 4728.8, "text": " AI which we call pre sizing and the best way is I will show you this beautiful example", "tokens": [7318, 597, 321, 818, 659, 45435, 293, 264, 1151, 636, 307, 286, 486, 855, 291, 341, 2238, 1365], "temperature": 0.0, "avg_logprob": -0.1330784851650022, "compression_ratio": 1.484076433121019, "no_speech_prob": 7.224428486551915e-07}, {"id": 663, "seek": 472880, "start": 4728.8, "end": 4736.4800000000005, "text": " of some PowerPoint wizardry that I'm so excited about to show how pre sizing works what pre", "tokens": [295, 512, 25584, 25807, 627, 300, 286, 478, 370, 2919, 466, 281, 855, 577, 659, 45435, 1985, 437, 659], "temperature": 0.0, "avg_logprob": -0.09945669518895896, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.994719468711992e-06}, {"id": 664, "seek": 472880, "start": 4736.4800000000005, "end": 4745.2, "text": " sizing does is that first step where we say resize to 460 by 460 is it grabs a square", "tokens": [45435, 775, 307, 300, 700, 1823, 689, 321, 584, 50069, 281, 1017, 4550, 538, 1017, 4550, 307, 309, 30028, 257, 3732], "temperature": 0.0, "avg_logprob": -0.09945669518895896, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.994719468711992e-06}, {"id": 665, "seek": 472880, "start": 4745.2, "end": 4750.5, "text": " and it grabs it randomly if it's a kind of landscape orientation photo it'll grab it", "tokens": [293, 309, 30028, 309, 16979, 498, 309, 311, 257, 733, 295, 9661, 14764, 5052, 309, 603, 4444, 309], "temperature": 0.0, "avg_logprob": -0.09945669518895896, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.994719468711992e-06}, {"id": 666, "seek": 472880, "start": 4750.5, "end": 4755.9800000000005, "text": " randomly sort of take the whole height and randomly grab somewhere from along the side", "tokens": [16979, 1333, 295, 747, 264, 1379, 6681, 293, 16979, 4444, 4079, 490, 2051, 264, 1252], "temperature": 0.0, "avg_logprob": -0.09945669518895896, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.994719468711992e-06}, {"id": 667, "seek": 475598, "start": 4755.98, "end": 4760.62, "text": " if it's a portrait orientation then it'll grab it you know we'll take the full width", "tokens": [498, 309, 311, 257, 17126, 14764, 550, 309, 603, 4444, 309, 291, 458, 321, 603, 747, 264, 1577, 11402], "temperature": 0.0, "avg_logprob": -0.13708599134423266, "compression_ratio": 1.841025641025641, "no_speech_prob": 5.626386609947076e-07}, {"id": 668, "seek": 475598, "start": 4760.62, "end": 4767.339999999999, "text": " and grab it grand and grab a random bit from top to bottom so then we take this area here", "tokens": [293, 4444, 309, 2697, 293, 4444, 257, 4974, 857, 490, 1192, 281, 2767, 370, 550, 321, 747, 341, 1859, 510], "temperature": 0.0, "avg_logprob": -0.13708599134423266, "compression_ratio": 1.841025641025641, "no_speech_prob": 5.626386609947076e-07}, {"id": 669, "seek": 475598, "start": 4767.339999999999, "end": 4773.7, "text": " and here it is right and so that's what the first resize does and then the second or transforms", "tokens": [293, 510, 309, 307, 558, 293, 370, 300, 311, 437, 264, 700, 50069, 775, 293, 550, 264, 1150, 420, 35592], "temperature": 0.0, "avg_logprob": -0.13708599134423266, "compression_ratio": 1.841025641025641, "no_speech_prob": 5.626386609947076e-07}, {"id": 670, "seek": 475598, "start": 4773.7, "end": 4784.98, "text": " bit will grab a random warped crop possibly rotated from in here and will turn that into", "tokens": [857, 486, 4444, 257, 4974, 1516, 3452, 9086, 6264, 42146, 490, 294, 510, 293, 486, 1261, 300, 666], "temperature": 0.0, "avg_logprob": -0.13708599134423266, "compression_ratio": 1.841025641025641, "no_speech_prob": 5.626386609947076e-07}, {"id": 671, "seek": 478498, "start": 4784.98, "end": 4791.82, "text": " a square and so it does so there's two steps it's first of all resize to a square that's", "tokens": [257, 3732, 293, 370, 309, 775, 370, 456, 311, 732, 4439, 309, 311, 700, 295, 439, 50069, 281, 257, 3732, 300, 311], "temperature": 0.0, "avg_logprob": -0.0819435344022863, "compression_ratio": 1.7411167512690355, "no_speech_prob": 1.0511473647056846e-06}, {"id": 672, "seek": 478498, "start": 4791.82, "end": 4799.5, "text": " big and then the second step is do a kind of rotation and warping and zooming stage", "tokens": [955, 293, 550, 264, 1150, 1823, 307, 360, 257, 733, 295, 12447, 293, 1516, 3381, 293, 48226, 3233], "temperature": 0.0, "avg_logprob": -0.0819435344022863, "compression_ratio": 1.7411167512690355, "no_speech_prob": 1.0511473647056846e-06}, {"id": 673, "seek": 478498, "start": 4799.5, "end": 4806.86, "text": " to something smaller in this case 224 by 224 because this first step creates something", "tokens": [281, 746, 4356, 294, 341, 1389, 5853, 19, 538, 5853, 19, 570, 341, 700, 1823, 7829, 746], "temperature": 0.0, "avg_logprob": -0.0819435344022863, "compression_ratio": 1.7411167512690355, "no_speech_prob": 1.0511473647056846e-06}, {"id": 674, "seek": 478498, "start": 4806.86, "end": 4813.74, "text": " that's square and always is the same size the second step can happen on the GPU and", "tokens": [300, 311, 3732, 293, 1009, 307, 264, 912, 2744, 264, 1150, 1823, 393, 1051, 322, 264, 18407, 293], "temperature": 0.0, "avg_logprob": -0.0819435344022863, "compression_ratio": 1.7411167512690355, "no_speech_prob": 1.0511473647056846e-06}, {"id": 675, "seek": 481374, "start": 4813.74, "end": 4819.84, "text": " because normally things like rotating and image warping actually pretty slow also normally", "tokens": [570, 5646, 721, 411, 19627, 293, 3256, 1516, 3381, 767, 1238, 2964, 611, 5646], "temperature": 0.0, "avg_logprob": -0.11551995155138847, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.2098599881937844e-06}, {"id": 676, "seek": 481374, "start": 4819.84, "end": 4828.4, "text": " doing a zoom and a rotate and a warp actually is really destructive to the image because", "tokens": [884, 257, 8863, 293, 257, 13121, 293, 257, 36030, 767, 307, 534, 26960, 281, 264, 3256, 570], "temperature": 0.0, "avg_logprob": -0.11551995155138847, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.2098599881937844e-06}, {"id": 677, "seek": 481374, "start": 4828.4, "end": 4834.34, "text": " each one of those things requires an interpolation step which it's not just slow it actually", "tokens": [1184, 472, 295, 729, 721, 7029, 364, 44902, 399, 1823, 597, 309, 311, 406, 445, 2964, 309, 767], "temperature": 0.0, "avg_logprob": -0.11551995155138847, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.2098599881937844e-06}, {"id": 678, "seek": 481374, "start": 4834.34, "end": 4841.099999999999, "text": " makes the image really quite low quality so we do it in a very special way in fast AI", "tokens": [1669, 264, 3256, 534, 1596, 2295, 3125, 370, 321, 360, 309, 294, 257, 588, 2121, 636, 294, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.11551995155138847, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.2098599881937844e-06}, {"id": 679, "seek": 484110, "start": 4841.1, "end": 4847.58, "text": " I think it's unique where we do all of the all of these kind of coordinate transforms", "tokens": [286, 519, 309, 311, 3845, 689, 321, 360, 439, 295, 264, 439, 295, 613, 733, 295, 15670, 35592], "temperature": 0.0, "avg_logprob": -0.13351427790630294, "compression_ratio": 1.74, "no_speech_prob": 8.059418519223982e-07}, {"id": 680, "seek": 484110, "start": 4847.58, "end": 4855.0, "text": " like rotations and walks and zooms and so forth not on the actual pixels but instead", "tokens": [411, 44796, 293, 12896, 293, 5721, 4785, 293, 370, 5220, 406, 322, 264, 3539, 18668, 457, 2602], "temperature": 0.0, "avg_logprob": -0.13351427790630294, "compression_ratio": 1.74, "no_speech_prob": 8.059418519223982e-07}, {"id": 681, "seek": 484110, "start": 4855.0, "end": 4861.22, "text": " we kind of keep track of the changing coordinate values in a in a non lossy way so the full", "tokens": [321, 733, 295, 1066, 2837, 295, 264, 4473, 15670, 4190, 294, 257, 294, 257, 2107, 4470, 88, 636, 370, 264, 1577], "temperature": 0.0, "avg_logprob": -0.13351427790630294, "compression_ratio": 1.74, "no_speech_prob": 8.059418519223982e-07}, {"id": 682, "seek": 484110, "start": 4861.22, "end": 4868.9400000000005, "text": " floating point value and then once at the very end we then do the interpolation there", "tokens": [12607, 935, 2158, 293, 550, 1564, 412, 264, 588, 917, 321, 550, 360, 264, 44902, 399, 456], "temperature": 0.0, "avg_logprob": -0.13351427790630294, "compression_ratio": 1.74, "no_speech_prob": 8.059418519223982e-07}, {"id": 683, "seek": 486894, "start": 4868.94, "end": 4877.74, "text": " is also quite striking here is what the difference looks like hopefully you can see this on on", "tokens": [307, 611, 1596, 18559, 510, 307, 437, 264, 2649, 1542, 411, 4696, 291, 393, 536, 341, 322, 322], "temperature": 0.0, "avg_logprob": -0.08912453019475362, "compression_ratio": 1.7843137254901962, "no_speech_prob": 1.6280450836347882e-06}, {"id": 684, "seek": 486894, "start": 4877.74, "end": 4884.839999999999, "text": " the video on the left is our pre sizing approach and on the right is the standard approach", "tokens": [264, 960, 322, 264, 1411, 307, 527, 659, 45435, 3109, 293, 322, 264, 558, 307, 264, 3832, 3109], "temperature": 0.0, "avg_logprob": -0.08912453019475362, "compression_ratio": 1.7843137254901962, "no_speech_prob": 1.6280450836347882e-06}, {"id": 685, "seek": 486894, "start": 4884.839999999999, "end": 4892.08, "text": " that other libraries use and you can see that the one on the right is a lot less nicely", "tokens": [300, 661, 15148, 764, 293, 291, 393, 536, 300, 264, 472, 322, 264, 558, 307, 257, 688, 1570, 9594], "temperature": 0.0, "avg_logprob": -0.08912453019475362, "compression_ratio": 1.7843137254901962, "no_speech_prob": 1.6280450836347882e-06}, {"id": 686, "seek": 486894, "start": 4892.08, "end": 4897.0199999999995, "text": " focused and it also has like weird things like this should be grass here but it's actually", "tokens": [5178, 293, 309, 611, 575, 411, 3657, 721, 411, 341, 820, 312, 8054, 510, 457, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.08912453019475362, "compression_ratio": 1.7843137254901962, "no_speech_prob": 1.6280450836347882e-06}, {"id": 687, "seek": 489702, "start": 4897.02, "end": 4901.740000000001, "text": " got its kind of bum sticking way out this has a little bit of weird distortions this", "tokens": [658, 1080, 733, 295, 13309, 13465, 636, 484, 341, 575, 257, 707, 857, 295, 3657, 37555, 626, 341], "temperature": 0.0, "avg_logprob": -0.17906678852282074, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.6119886936394323e-07}, {"id": 688, "seek": 489702, "start": 4901.740000000001, "end": 4907.740000000001, "text": " has got loads of weird distortions so you can see the precise version really ends up", "tokens": [575, 658, 12668, 295, 3657, 37555, 626, 370, 291, 393, 536, 264, 13600, 3037, 534, 5314, 493], "temperature": 0.0, "avg_logprob": -0.17906678852282074, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.6119886936394323e-07}, {"id": 689, "seek": 489702, "start": 4907.740000000001, "end": 4913.9400000000005, "text": " way way better and I think we have a question Rachel.", "tokens": [636, 636, 1101, 293, 286, 519, 321, 362, 257, 1168, 14246, 13], "temperature": 0.0, "avg_logprob": -0.17906678852282074, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.6119886936394323e-07}, {"id": 690, "seek": 489702, "start": 4913.9400000000005, "end": 4916.540000000001, "text": " Are the blocks in the data block an ordered list?", "tokens": [2014, 264, 8474, 294, 264, 1412, 3461, 364, 8866, 1329, 30], "temperature": 0.0, "avg_logprob": -0.17906678852282074, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.6119886936394323e-07}, {"id": 691, "seek": 489702, "start": 4916.540000000001, "end": 4920.660000000001, "text": " Do they specify the input and output structures respectively?", "tokens": [1144, 436, 16500, 264, 4846, 293, 5598, 9227, 25009, 30], "temperature": 0.0, "avg_logprob": -0.17906678852282074, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.6119886936394323e-07}, {"id": 692, "seek": 489702, "start": 4920.660000000001, "end": 4923.860000000001, "text": " Are there always two blocks or can there be more than two?", "tokens": [2014, 456, 1009, 732, 8474, 420, 393, 456, 312, 544, 813, 732, 30], "temperature": 0.0, "avg_logprob": -0.17906678852282074, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.6119886936394323e-07}, {"id": 693, "seek": 492386, "start": 4923.86, "end": 4932.5, "text": " For example if you wanted a segmentation model with the second block be something about segmentation?", "tokens": [1171, 1365, 498, 291, 1415, 257, 9469, 399, 2316, 365, 264, 1150, 3461, 312, 746, 466, 9469, 399, 30], "temperature": 0.0, "avg_logprob": -0.1289206101344182, "compression_ratio": 1.9087136929460582, "no_speech_prob": 3.6119597552897176e-06}, {"id": 694, "seek": 492386, "start": 4932.5, "end": 4939.5, "text": " So so yeah this is an ordered list so the first item says I want to create an image", "tokens": [407, 370, 1338, 341, 307, 364, 8866, 1329, 370, 264, 700, 3174, 1619, 286, 528, 281, 1884, 364, 3256], "temperature": 0.0, "avg_logprob": -0.1289206101344182, "compression_ratio": 1.9087136929460582, "no_speech_prob": 3.6119597552897176e-06}, {"id": 695, "seek": 492386, "start": 4939.5, "end": 4943.74, "text": " and then the second item says I want to create a category so that's my independent and dependent", "tokens": [293, 550, 264, 1150, 3174, 1619, 286, 528, 281, 1884, 257, 7719, 370, 300, 311, 452, 6695, 293, 12334], "temperature": 0.0, "avg_logprob": -0.1289206101344182, "compression_ratio": 1.9087136929460582, "no_speech_prob": 3.6119597552897176e-06}, {"id": 696, "seek": 492386, "start": 4943.74, "end": 4949.0599999999995, "text": " variable you can have one thing here you can have three things here you can have any amount", "tokens": [7006, 291, 393, 362, 472, 551, 510, 291, 393, 362, 1045, 721, 510, 291, 393, 362, 604, 2372], "temperature": 0.0, "avg_logprob": -0.1289206101344182, "compression_ratio": 1.9087136929460582, "no_speech_prob": 3.6119597552897176e-06}, {"id": 697, "seek": 492386, "start": 4949.0599999999995, "end": 4953.38, "text": " of things here you want obviously the vast majority of the time it'll be two normally", "tokens": [295, 721, 510, 291, 528, 2745, 264, 8369, 6286, 295, 264, 565, 309, 603, 312, 732, 5646], "temperature": 0.0, "avg_logprob": -0.1289206101344182, "compression_ratio": 1.9087136929460582, "no_speech_prob": 3.6119597552897176e-06}, {"id": 698, "seek": 495338, "start": 4953.38, "end": 4957.9800000000005, "text": " there's an independent variable and a dependent variable we'll be seeing this in more detail", "tokens": [456, 311, 364, 6695, 7006, 293, 257, 12334, 7006, 321, 603, 312, 2577, 341, 294, 544, 2607], "temperature": 0.0, "avg_logprob": -0.10933255636563866, "compression_ratio": 1.7061224489795919, "no_speech_prob": 6.719780287767207e-08}, {"id": 699, "seek": 495338, "start": 4957.9800000000005, "end": 4961.86, "text": " later although if you go back to the earlier lesson when we introduced data blocks I do", "tokens": [1780, 4878, 498, 291, 352, 646, 281, 264, 3071, 6898, 562, 321, 7268, 1412, 8474, 286, 360], "temperature": 0.0, "avg_logprob": -0.10933255636563866, "compression_ratio": 1.7061224489795919, "no_speech_prob": 6.719780287767207e-08}, {"id": 700, "seek": 495338, "start": 4961.86, "end": 4973.900000000001, "text": " have a picture kind of showing how these pieces fit together.", "tokens": [362, 257, 3036, 733, 295, 4099, 577, 613, 3755, 3318, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10933255636563866, "compression_ratio": 1.7061224489795919, "no_speech_prob": 6.719780287767207e-08}, {"id": 701, "seek": 495338, "start": 4973.900000000001, "end": 4978.26, "text": " So after you've put together your data block created your data loaders you want to make", "tokens": [407, 934, 291, 600, 829, 1214, 428, 1412, 3461, 2942, 428, 1412, 3677, 433, 291, 528, 281, 652], "temperature": 0.0, "avg_logprob": -0.10933255636563866, "compression_ratio": 1.7061224489795919, "no_speech_prob": 6.719780287767207e-08}, {"id": 702, "seek": 495338, "start": 4978.26, "end": 4983.14, "text": " sure it's working correctly so the obvious thing to do for a computer vision data block", "tokens": [988, 309, 311, 1364, 8944, 370, 264, 6322, 551, 281, 360, 337, 257, 3820, 5201, 1412, 3461], "temperature": 0.0, "avg_logprob": -0.10933255636563866, "compression_ratio": 1.7061224489795919, "no_speech_prob": 6.719780287767207e-08}, {"id": 703, "seek": 498314, "start": 4983.14, "end": 4992.54, "text": " is show batch and show batch will show you the items and you can kind of just make sure", "tokens": [307, 855, 15245, 293, 855, 15245, 486, 855, 291, 264, 4754, 293, 291, 393, 733, 295, 445, 652, 988], "temperature": 0.0, "avg_logprob": -0.09653966972626836, "compression_ratio": 1.803030303030303, "no_speech_prob": 6.276699195950641e-07}, {"id": 704, "seek": 498314, "start": 4992.54, "end": 4997.38, "text": " they look sensible that looks like the labels are reasonable if you add a unique equals", "tokens": [436, 574, 25380, 300, 1542, 411, 264, 16949, 366, 10585, 498, 291, 909, 257, 3845, 6915], "temperature": 0.0, "avg_logprob": -0.09653966972626836, "compression_ratio": 1.803030303030303, "no_speech_prob": 6.276699195950641e-07}, {"id": 705, "seek": 498314, "start": 4997.38, "end": 5002.06, "text": " true then it's going to show you the same image with all the different augmentations", "tokens": [2074, 550, 309, 311, 516, 281, 855, 291, 264, 912, 3256, 365, 439, 264, 819, 29919, 763], "temperature": 0.0, "avg_logprob": -0.09653966972626836, "compression_ratio": 1.803030303030303, "no_speech_prob": 6.276699195950641e-07}, {"id": 706, "seek": 498314, "start": 5002.06, "end": 5006.820000000001, "text": " this is a good way to make sure your augmentations work if you make a mistake in your data block", "tokens": [341, 307, 257, 665, 636, 281, 652, 988, 428, 29919, 763, 589, 498, 291, 652, 257, 6146, 294, 428, 1412, 3461], "temperature": 0.0, "avg_logprob": -0.09653966972626836, "compression_ratio": 1.803030303030303, "no_speech_prob": 6.276699195950641e-07}, {"id": 707, "seek": 500682, "start": 5006.82, "end": 5013.58, "text": " in this example there's no resize so the different images are going to be different sizes so", "tokens": [294, 341, 1365, 456, 311, 572, 50069, 370, 264, 819, 5267, 366, 516, 281, 312, 819, 11602, 370], "temperature": 0.0, "avg_logprob": -0.12980993294421536, "compression_ratio": 1.6971153846153846, "no_speech_prob": 4.812508223039913e-07}, {"id": 708, "seek": 500682, "start": 5013.58, "end": 5021.58, "text": " it would be impossible to collate them into a batch so if you call dot summary this is", "tokens": [309, 576, 312, 6243, 281, 1263, 473, 552, 666, 257, 15245, 370, 498, 291, 818, 5893, 12691, 341, 307], "temperature": 0.0, "avg_logprob": -0.12980993294421536, "compression_ratio": 1.6971153846153846, "no_speech_prob": 4.812508223039913e-07}, {"id": 709, "seek": 500682, "start": 5021.58, "end": 5027.0199999999995, "text": " a really neat thing which will go through and tell you everything that's happening so", "tokens": [257, 534, 10654, 551, 597, 486, 352, 807, 293, 980, 291, 1203, 300, 311, 2737, 370], "temperature": 0.0, "avg_logprob": -0.12980993294421536, "compression_ratio": 1.6971153846153846, "no_speech_prob": 4.812508223039913e-07}, {"id": 710, "seek": 500682, "start": 5027.0199999999995, "end": 5032.58, "text": " I collecting the items how many did I find what happened when I split them what are the", "tokens": [286, 12510, 264, 4754, 577, 867, 630, 286, 915, 437, 2011, 562, 286, 7472, 552, 437, 366, 264], "temperature": 0.0, "avg_logprob": -0.12980993294421536, "compression_ratio": 1.6971153846153846, "no_speech_prob": 4.812508223039913e-07}, {"id": 711, "seek": 503258, "start": 5032.58, "end": 5038.18, "text": " different variables independent dependent variables I'm creating let's try and create", "tokens": [819, 9102, 6695, 12334, 9102, 286, 478, 4084, 718, 311, 853, 293, 1884], "temperature": 0.0, "avg_logprob": -0.1472386818427544, "compression_ratio": 1.729064039408867, "no_speech_prob": 1.9033769831366953e-06}, {"id": 712, "seek": 503258, "start": 5038.18, "end": 5047.7, "text": " one of these here's each step create my image that categorize is what the first thing gave", "tokens": [472, 295, 613, 510, 311, 1184, 1823, 1884, 452, 3256, 300, 19250, 1125, 307, 437, 264, 700, 551, 2729], "temperature": 0.0, "avg_logprob": -0.1472386818427544, "compression_ratio": 1.729064039408867, "no_speech_prob": 1.9033769831366953e-06}, {"id": 713, "seek": 503258, "start": 5047.7, "end": 5056.14, "text": " me an American bulldog is the final sample is this image this size this category and", "tokens": [385, 364, 2665, 4693, 14833, 307, 264, 2572, 6889, 307, 341, 3256, 341, 2744, 341, 7719, 293], "temperature": 0.0, "avg_logprob": -0.1472386818427544, "compression_ratio": 1.729064039408867, "no_speech_prob": 1.9033769831366953e-06}, {"id": 714, "seek": 503258, "start": 5056.14, "end": 5061.12, "text": " then eventually it says oh it's not possible to collect your items I tried to collect the", "tokens": [550, 4728, 309, 1619, 1954, 309, 311, 406, 1944, 281, 2500, 428, 4754, 286, 3031, 281, 2500, 264], "temperature": 0.0, "avg_logprob": -0.1472386818427544, "compression_ratio": 1.729064039408867, "no_speech_prob": 1.9033769831366953e-06}, {"id": 715, "seek": 506112, "start": 5061.12, "end": 5066.04, "text": " zero index members of your tuples so in other words that's the independent variable and", "tokens": [4018, 8186, 2679, 295, 428, 2604, 2622, 370, 294, 661, 2283, 300, 311, 264, 6695, 7006, 293], "temperature": 0.0, "avg_logprob": -0.1034650578218348, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.340514240335324e-06}, {"id": 716, "seek": 506112, "start": 5066.04, "end": 5073.36, "text": " I got this was size 500 by 375 this was 375 by 500 oh I can't collect these into a tensor", "tokens": [286, 658, 341, 390, 2744, 5923, 538, 805, 11901, 341, 390, 805, 11901, 538, 5923, 1954, 286, 393, 380, 2500, 613, 666, 257, 40863], "temperature": 0.0, "avg_logprob": -0.1034650578218348, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.340514240335324e-06}, {"id": 717, "seek": 506112, "start": 5073.36, "end": 5078.18, "text": " because they're different sizes so this is a super great debugging tool for debugging", "tokens": [570, 436, 434, 819, 11602, 370, 341, 307, 257, 1687, 869, 45592, 2290, 337, 45592], "temperature": 0.0, "avg_logprob": -0.1034650578218348, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.340514240335324e-06}, {"id": 718, "seek": 506112, "start": 5078.18, "end": 5087.18, "text": " your data blocks I have a question how does the item transforms pre-size work if the resize", "tokens": [428, 1412, 8474, 286, 362, 257, 1168, 577, 775, 264, 3174, 35592, 659, 12, 27553, 589, 498, 264, 50069], "temperature": 0.0, "avg_logprob": -0.1034650578218348, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.340514240335324e-06}, {"id": 719, "seek": 508718, "start": 5087.18, "end": 5092.54, "text": " is smaller than the image is a whole width or height still taken or is it just a random", "tokens": [307, 4356, 813, 264, 3256, 307, 257, 1379, 11402, 420, 6681, 920, 2726, 420, 307, 309, 445, 257, 4974], "temperature": 0.0, "avg_logprob": -0.09340568572755843, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.7061724975064863e-06}, {"id": 720, "seek": 508718, "start": 5092.54, "end": 5104.780000000001, "text": " crop with the resize value so if you remember back to lesson two we looked at the different", "tokens": [9086, 365, 264, 50069, 2158, 370, 498, 291, 1604, 646, 281, 6898, 732, 321, 2956, 412, 264, 819], "temperature": 0.0, "avg_logprob": -0.09340568572755843, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.7061724975064863e-06}, {"id": 721, "seek": 508718, "start": 5104.780000000001, "end": 5116.9400000000005, "text": " ways of creating these things you can use squish you can use pad or you can use crop", "tokens": [2098, 295, 4084, 613, 721, 291, 393, 764, 31379, 291, 393, 764, 6887, 420, 291, 393, 764, 9086], "temperature": 0.0, "avg_logprob": -0.09340568572755843, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.7061724975064863e-06}, {"id": 722, "seek": 511694, "start": 5116.94, "end": 5125.86, "text": " so if your image is smaller than the precise value then squish will really be zoom so it", "tokens": [370, 498, 428, 3256, 307, 4356, 813, 264, 13600, 2158, 550, 31379, 486, 534, 312, 8863, 370, 309], "temperature": 0.0, "avg_logprob": -0.12711956334668537, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.2058204624263453e-07}, {"id": 723, "seek": 511694, "start": 5125.86, "end": 5133.5599999999995, "text": " will just small stretch it'll stretch it and then pattern crop will do much the same thing", "tokens": [486, 445, 1359, 5985, 309, 603, 5985, 309, 293, 550, 5102, 9086, 486, 360, 709, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.12711956334668537, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.2058204624263453e-07}, {"id": 724, "seek": 511694, "start": 5133.5599999999995, "end": 5137.74, "text": " and so you'll just end up with a you know the same just looks like these but it'll be", "tokens": [293, 370, 291, 603, 445, 917, 493, 365, 257, 291, 458, 264, 912, 445, 1542, 411, 613, 457, 309, 603, 312], "temperature": 0.0, "avg_logprob": -0.12711956334668537, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.2058204624263453e-07}, {"id": 725, "seek": 511694, "start": 5137.74, "end": 5142.86, "text": " a kind of lower more pixelated lower resolution because it's having to zoom in a little bit", "tokens": [257, 733, 295, 3126, 544, 19261, 770, 3126, 8669, 570, 309, 311, 1419, 281, 8863, 294, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.12711956334668537, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.2058204624263453e-07}, {"id": 726, "seek": 514286, "start": 5142.86, "end": 5152.78, "text": " okay so a lot of people say that you should do a hell of a lot of data cleaning before", "tokens": [1392, 370, 257, 688, 295, 561, 584, 300, 291, 820, 360, 257, 4921, 295, 257, 688, 295, 1412, 8924, 949], "temperature": 0.0, "avg_logprob": -0.07412101305448092, "compression_ratio": 1.643312101910828, "no_speech_prob": 6.681507329631131e-07}, {"id": 727, "seek": 514286, "start": 5152.78, "end": 5160.38, "text": " you model we don't we say model as soon as you can because remember what we found in", "tokens": [291, 2316, 321, 500, 380, 321, 584, 2316, 382, 2321, 382, 291, 393, 570, 1604, 437, 321, 1352, 294], "temperature": 0.0, "avg_logprob": -0.07412101305448092, "compression_ratio": 1.643312101910828, "no_speech_prob": 6.681507329631131e-07}, {"id": 728, "seek": 514286, "start": 5160.38, "end": 5168.98, "text": " in notebook 2 your your model can teach you about the problems in your data so as soon", "tokens": [294, 21060, 568, 428, 428, 2316, 393, 2924, 291, 466, 264, 2740, 294, 428, 1412, 370, 382, 2321], "temperature": 0.0, "avg_logprob": -0.07412101305448092, "compression_ratio": 1.643312101910828, "no_speech_prob": 6.681507329631131e-07}, {"id": 729, "seek": 516898, "start": 5168.98, "end": 5173.58, "text": " as I've got to a point where I have a data block that's working and I have data loaders", "tokens": [382, 286, 600, 658, 281, 257, 935, 689, 286, 362, 257, 1412, 3461, 300, 311, 1364, 293, 286, 362, 1412, 3677, 433], "temperature": 0.0, "avg_logprob": -0.12312046279255141, "compression_ratio": 1.8089430894308942, "no_speech_prob": 3.689879974899668e-07}, {"id": 730, "seek": 516898, "start": 5173.58, "end": 5178.099999999999, "text": " I'm going to build a model and so here I'm you know it also tells me how I'm going so", "tokens": [286, 478, 516, 281, 1322, 257, 2316, 293, 370, 510, 286, 478, 291, 458, 309, 611, 5112, 385, 577, 286, 478, 516, 370], "temperature": 0.0, "avg_logprob": -0.12312046279255141, "compression_ratio": 1.8089430894308942, "no_speech_prob": 3.689879974899668e-07}, {"id": 731, "seek": 516898, "start": 5178.099999999999, "end": 5185.0599999999995, "text": " I'm getting 7% error wow that's actually really good or a pets model and so at this point", "tokens": [286, 478, 1242, 1614, 4, 6713, 6076, 300, 311, 767, 534, 665, 420, 257, 19897, 2316, 293, 370, 412, 341, 935], "temperature": 0.0, "avg_logprob": -0.12312046279255141, "compression_ratio": 1.8089430894308942, "no_speech_prob": 3.689879974899668e-07}, {"id": 732, "seek": 516898, "start": 5185.0599999999995, "end": 5189.62, "text": " now that I have a model I can do that stuff we learned about earlier in 02 the notebook", "tokens": [586, 300, 286, 362, 257, 2316, 286, 393, 360, 300, 1507, 321, 3264, 466, 3071, 294, 37202, 264, 21060], "temperature": 0.0, "avg_logprob": -0.12312046279255141, "compression_ratio": 1.8089430894308942, "no_speech_prob": 3.689879974899668e-07}, {"id": 733, "seek": 516898, "start": 5189.62, "end": 5195.219999999999, "text": " 02 where we train our model and use it to clean the data so we can look at the classification", "tokens": [37202, 689, 321, 3847, 527, 2316, 293, 764, 309, 281, 2541, 264, 1412, 370, 321, 393, 574, 412, 264, 21538], "temperature": 0.0, "avg_logprob": -0.12312046279255141, "compression_ratio": 1.8089430894308942, "no_speech_prob": 3.689879974899668e-07}, {"id": 734, "seek": 519522, "start": 5195.22, "end": 5209.740000000001, "text": " a confusion matrix top losses the image cleaner widget you know so forth okay now one thing", "tokens": [257, 15075, 8141, 1192, 15352, 264, 3256, 16532, 34047, 291, 458, 370, 5220, 1392, 586, 472, 551], "temperature": 0.0, "avg_logprob": -0.14263251570404553, "compression_ratio": 1.5680473372781065, "no_speech_prob": 1.1189403039679746e-06}, {"id": 735, "seek": 519522, "start": 5209.740000000001, "end": 5218.14, "text": " interesting here is in notebook 4 we included a loss function when we created a learner", "tokens": [1880, 510, 307, 294, 21060, 1017, 321, 5556, 257, 4470, 2445, 562, 321, 2942, 257, 33347], "temperature": 0.0, "avg_logprob": -0.14263251570404553, "compression_ratio": 1.5680473372781065, "no_speech_prob": 1.1189403039679746e-06}, {"id": 736, "seek": 519522, "start": 5218.14, "end": 5225.0, "text": " and here we don't pass in a loss function why is that that's because fast AI will try", "tokens": [293, 510, 321, 500, 380, 1320, 294, 257, 4470, 2445, 983, 307, 300, 300, 311, 570, 2370, 7318, 486, 853], "temperature": 0.0, "avg_logprob": -0.14263251570404553, "compression_ratio": 1.5680473372781065, "no_speech_prob": 1.1189403039679746e-06}, {"id": 737, "seek": 522500, "start": 5225.0, "end": 5234.1, "text": " to automatically pick a somewhat sensible loss function for you and so for a image classification", "tokens": [281, 6772, 1888, 257, 8344, 25380, 4470, 2445, 337, 291, 293, 370, 337, 257, 3256, 21538], "temperature": 0.0, "avg_logprob": -0.06951752575961026, "compression_ratio": 1.528, "no_speech_prob": 1.3496970723281265e-06}, {"id": 738, "seek": 522500, "start": 5234.1, "end": 5240.32, "text": " task it knows what loss function is the normal one to pick and it's done it for you but let's", "tokens": [5633, 309, 3255, 437, 4470, 2445, 307, 264, 2710, 472, 281, 1888, 293, 309, 311, 1096, 309, 337, 291, 457, 718, 311], "temperature": 0.0, "avg_logprob": -0.06951752575961026, "compression_ratio": 1.528, "no_speech_prob": 1.3496970723281265e-06}, {"id": 739, "seek": 524032, "start": 5240.32, "end": 5254.599999999999, "text": " have a look and see what actually did pick so we could have a look at learn dot loss", "tokens": [362, 257, 574, 293, 536, 437, 767, 630, 1888, 370, 321, 727, 362, 257, 574, 412, 1466, 5893, 4470], "temperature": 0.0, "avg_logprob": -0.16079827414618597, "compression_ratio": 1.5486725663716814, "no_speech_prob": 3.011592752955039e-07}, {"id": 740, "seek": 524032, "start": 5254.599999999999, "end": 5261.46, "text": " funk and we will see it is cross entropy loss what on earth is cross entropy loss I'm glad", "tokens": [26476, 293, 321, 486, 536, 309, 307, 3278, 30867, 4470, 437, 322, 4120, 307, 3278, 30867, 4470, 286, 478, 5404], "temperature": 0.0, "avg_logprob": -0.16079827414618597, "compression_ratio": 1.5486725663716814, "no_speech_prob": 3.011592752955039e-07}, {"id": 741, "seek": 526146, "start": 5261.46, "end": 5271.18, "text": " you asked let's find out cross entropy loss is really much the same as the MNIST lost", "tokens": [291, 2351, 718, 311, 915, 484, 3278, 30867, 4470, 307, 534, 709, 264, 912, 382, 264, 376, 45, 19756, 2731], "temperature": 0.0, "avg_logprob": -0.13682392239570618, "compression_ratio": 1.7218543046357615, "no_speech_prob": 7.69034045333683e-07}, {"id": 742, "seek": 526146, "start": 5271.18, "end": 5278.86, "text": " we created with that with that sigmoid and the one minus predictions and predictions", "tokens": [321, 2942, 365, 300, 365, 300, 4556, 3280, 327, 293, 264, 472, 3175, 21264, 293, 21264], "temperature": 0.0, "avg_logprob": -0.13682392239570618, "compression_ratio": 1.7218543046357615, "no_speech_prob": 7.69034045333683e-07}, {"id": 743, "seek": 526146, "start": 5278.86, "end": 5286.5, "text": " but it's it's a kind of extended version of that and the extended version of that is that", "tokens": [457, 309, 311, 309, 311, 257, 733, 295, 10913, 3037, 295, 300, 293, 264, 10913, 3037, 295, 300, 307, 300], "temperature": 0.0, "avg_logprob": -0.13682392239570618, "compression_ratio": 1.7218543046357615, "no_speech_prob": 7.69034045333683e-07}, {"id": 744, "seek": 528650, "start": 5286.5, "end": 5294.06, "text": " torch dot where that we looked at in notebook for only works when you have a binary outcome", "tokens": [27822, 5893, 689, 300, 321, 2956, 412, 294, 21060, 337, 787, 1985, 562, 291, 362, 257, 17434, 9700], "temperature": 0.0, "avg_logprob": -0.10038626194000244, "compression_ratio": 1.5529411764705883, "no_speech_prob": 1.2482664715207648e-06}, {"id": 745, "seek": 528650, "start": 5294.06, "end": 5301.02, "text": " in that case it was is it a three or not but in this case we've got which of the 37 pet", "tokens": [294, 300, 1389, 309, 390, 307, 309, 257, 1045, 420, 406, 457, 294, 341, 1389, 321, 600, 658, 597, 295, 264, 13435, 3817], "temperature": 0.0, "avg_logprob": -0.10038626194000244, "compression_ratio": 1.5529411764705883, "no_speech_prob": 1.2482664715207648e-06}, {"id": 746, "seek": 528650, "start": 5301.02, "end": 5308.98, "text": " breeds is it so we want to kind of create something just like that sigmoid and torch", "tokens": [41609, 307, 309, 370, 321, 528, 281, 733, 295, 1884, 746, 445, 411, 300, 4556, 3280, 327, 293, 27822], "temperature": 0.0, "avg_logprob": -0.10038626194000244, "compression_ratio": 1.5529411764705883, "no_speech_prob": 1.2482664715207648e-06}, {"id": 747, "seek": 530898, "start": 5308.98, "end": 5318.82, "text": " dot where that which also works nicely for more than two categories so let's see how", "tokens": [5893, 689, 300, 597, 611, 1985, 9594, 337, 544, 813, 732, 10479, 370, 718, 311, 536, 577], "temperature": 0.0, "avg_logprob": -0.11972214625431941, "compression_ratio": 1.5773809523809523, "no_speech_prob": 9.422400921721419e-07}, {"id": 748, "seek": 530898, "start": 5318.82, "end": 5330.12, "text": " we can do that so first of all let's grab a batch yes a question why do we want to build", "tokens": [321, 393, 360, 300, 370, 700, 295, 439, 718, 311, 4444, 257, 15245, 2086, 257, 1168, 983, 360, 321, 528, 281, 1322], "temperature": 0.0, "avg_logprob": -0.11972214625431941, "compression_ratio": 1.5773809523809523, "no_speech_prob": 9.422400921721419e-07}, {"id": 749, "seek": 530898, "start": 5330.12, "end": 5337.66, "text": " a model before cleaning the data I would think a clean data set would help in training yeah", "tokens": [257, 2316, 949, 8924, 264, 1412, 286, 576, 519, 257, 2541, 1412, 992, 576, 854, 294, 3097, 1338], "temperature": 0.0, "avg_logprob": -0.11972214625431941, "compression_ratio": 1.5773809523809523, "no_speech_prob": 9.422400921721419e-07}, {"id": 750, "seek": 533766, "start": 5337.66, "end": 5346.58, "text": " absolutely a clean data set helps in training but remember as we saw in notebook O2 an initial", "tokens": [3122, 257, 2541, 1412, 992, 3665, 294, 3097, 457, 1604, 382, 321, 1866, 294, 21060, 422, 17, 364, 5883], "temperature": 0.0, "avg_logprob": -0.10468463897705078, "compression_ratio": 1.7031963470319635, "no_speech_prob": 1.9637900550151244e-06}, {"id": 751, "seek": 533766, "start": 5346.58, "end": 5352.82, "text": " model helps you clean the data set so remember how plot top losses helped us identify mislabeled", "tokens": [2316, 3665, 291, 2541, 264, 1412, 992, 370, 1604, 577, 7542, 1192, 15352, 4254, 505, 5876, 3346, 75, 18657, 292], "temperature": 0.0, "avg_logprob": -0.10468463897705078, "compression_ratio": 1.7031963470319635, "no_speech_prob": 1.9637900550151244e-06}, {"id": 752, "seek": 533766, "start": 5352.82, "end": 5359.66, "text": " images and the confusion matrix helped us recognize which things we were getting confused", "tokens": [5267, 293, 264, 15075, 8141, 4254, 505, 5521, 597, 721, 321, 645, 1242, 9019], "temperature": 0.0, "avg_logprob": -0.10468463897705078, "compression_ratio": 1.7031963470319635, "no_speech_prob": 1.9637900550151244e-06}, {"id": 753, "seek": 533766, "start": 5359.66, "end": 5365.5599999999995, "text": " and might need you know fixing and the image classifier cleaner actually let us find things", "tokens": [293, 1062, 643, 291, 458, 19442, 293, 264, 3256, 1508, 9902, 16532, 767, 718, 505, 915, 721], "temperature": 0.0, "avg_logprob": -0.10468463897705078, "compression_ratio": 1.7031963470319635, "no_speech_prob": 1.9637900550151244e-06}, {"id": 754, "seek": 536556, "start": 5365.56, "end": 5371.820000000001, "text": " like an image that contained two bears rather than one bear and clean it up so a model is", "tokens": [411, 364, 3256, 300, 16212, 732, 17276, 2831, 813, 472, 6155, 293, 2541, 309, 493, 370, 257, 2316, 307], "temperature": 0.0, "avg_logprob": -0.09382625323970144, "compression_ratio": 1.7450980392156863, "no_speech_prob": 1.1015905556632788e-06}, {"id": 755, "seek": 536556, "start": 5371.820000000001, "end": 5377.8, "text": " just a fantastic way to help you zoom in on the data that matters which things seem to", "tokens": [445, 257, 5456, 636, 281, 854, 291, 8863, 294, 322, 264, 1412, 300, 7001, 597, 721, 1643, 281], "temperature": 0.0, "avg_logprob": -0.09382625323970144, "compression_ratio": 1.7450980392156863, "no_speech_prob": 1.1015905556632788e-06}, {"id": 756, "seek": 536556, "start": 5377.8, "end": 5382.9800000000005, "text": " have the problems which things are most important stuff like that so you would go through a", "tokens": [362, 264, 2740, 597, 721, 366, 881, 1021, 1507, 411, 300, 370, 291, 576, 352, 807, 257], "temperature": 0.0, "avg_logprob": -0.09382625323970144, "compression_ratio": 1.7450980392156863, "no_speech_prob": 1.1015905556632788e-06}, {"id": 757, "seek": 536556, "start": 5382.9800000000005, "end": 5388.580000000001, "text": " new cleaner with the model helping you and then you go back and train it again with the", "tokens": [777, 16532, 365, 264, 2316, 4315, 291, 293, 550, 291, 352, 646, 293, 3847, 309, 797, 365, 264], "temperature": 0.0, "avg_logprob": -0.09382625323970144, "compression_ratio": 1.7450980392156863, "no_speech_prob": 1.1015905556632788e-06}, {"id": 758, "seek": 538858, "start": 5388.58, "end": 5399.1, "text": " clean data thanks for that great question okay so in order to understand cross entropy", "tokens": [2541, 1412, 3231, 337, 300, 869, 1168, 1392, 370, 294, 1668, 281, 1223, 3278, 30867], "temperature": 0.0, "avg_logprob": -0.15980015939740994, "compression_ratio": 1.6687898089171975, "no_speech_prob": 5.714996405004058e-07}, {"id": 759, "seek": 538858, "start": 5399.1, "end": 5407.78, "text": " loss let's grab a batch of data which we can use DLs dot one batch and that's going to", "tokens": [4470, 718, 311, 4444, 257, 15245, 295, 1412, 597, 321, 393, 764, 413, 43, 82, 5893, 472, 15245, 293, 300, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.15980015939740994, "compression_ratio": 1.6687898089171975, "no_speech_prob": 5.714996405004058e-07}, {"id": 760, "seek": 538858, "start": 5407.78, "end": 5417.18, "text": " grab a batch from the training set we could also go first DLs dot train and that's going", "tokens": [4444, 257, 15245, 490, 264, 3097, 992, 321, 727, 611, 352, 700, 413, 43, 82, 5893, 3847, 293, 300, 311, 516], "temperature": 0.0, "avg_logprob": -0.15980015939740994, "compression_ratio": 1.6687898089171975, "no_speech_prob": 5.714996405004058e-07}, {"id": 761, "seek": 541718, "start": 5417.18, "end": 5423.780000000001, "text": " to do exactly the same thing and so then we can destructure that into the independent", "tokens": [281, 360, 2293, 264, 912, 551, 293, 370, 550, 321, 393, 2677, 2885, 300, 666, 264, 6695], "temperature": 0.0, "avg_logprob": -0.1281901556870033, "compression_ratio": 1.7124183006535947, "no_speech_prob": 1.482351422055217e-06}, {"id": 762, "seek": 541718, "start": 5423.780000000001, "end": 5428.860000000001, "text": " independent variable and so the dependent variable shows us we've got a batch size of", "tokens": [6695, 7006, 293, 370, 264, 12334, 7006, 3110, 505, 321, 600, 658, 257, 15245, 2744, 295], "temperature": 0.0, "avg_logprob": -0.1281901556870033, "compression_ratio": 1.7124183006535947, "no_speech_prob": 1.482351422055217e-06}, {"id": 763, "seek": 541718, "start": 5428.860000000001, "end": 5445.820000000001, "text": " 64 that shows us the 64 categories and remember those numbers simply refer to the index of", "tokens": [12145, 300, 3110, 505, 264, 12145, 10479, 293, 1604, 729, 3547, 2935, 2864, 281, 264, 8186, 295], "temperature": 0.0, "avg_logprob": -0.1281901556870033, "compression_ratio": 1.7124183006535947, "no_speech_prob": 1.482351422055217e-06}, {"id": 764, "seek": 544582, "start": 5445.82, "end": 5453.54, "text": " the vocab so for example 16 is a boxer and so that all happens for you automatically", "tokens": [264, 2329, 455, 370, 337, 1365, 3165, 307, 257, 47252, 293, 370, 300, 439, 2314, 337, 291, 6772], "temperature": 0.0, "avg_logprob": -0.13777923583984375, "compression_ratio": 1.5813953488372092, "no_speech_prob": 8.714318369129614e-07}, {"id": 765, "seek": 544582, "start": 5453.54, "end": 5462.5599999999995, "text": " when we say show batch it shows us those strings so here's our first mini batch and so now", "tokens": [562, 321, 584, 855, 15245, 309, 3110, 505, 729, 13985, 370, 510, 311, 527, 700, 8382, 15245, 293, 370, 586], "temperature": 0.0, "avg_logprob": -0.13777923583984375, "compression_ratio": 1.5813953488372092, "no_speech_prob": 8.714318369129614e-07}, {"id": 766, "seek": 544582, "start": 5462.5599999999995, "end": 5468.92, "text": " we can view the predictions that is the activations of the final layer of the network by calling", "tokens": [321, 393, 1910, 264, 21264, 300, 307, 264, 2430, 763, 295, 264, 2572, 4583, 295, 264, 3209, 538, 5141], "temperature": 0.0, "avg_logprob": -0.13777923583984375, "compression_ratio": 1.5813953488372092, "no_speech_prob": 8.714318369129614e-07}, {"id": 767, "seek": 546892, "start": 5468.92, "end": 5477.02, "text": " get preds and you can pass in a data loader and a data loader can really be anything that's", "tokens": [483, 3852, 82, 293, 291, 393, 1320, 294, 257, 1412, 3677, 260, 293, 257, 1412, 3677, 260, 393, 534, 312, 1340, 300, 311], "temperature": 0.0, "avg_logprob": -0.09609171273051828, "compression_ratio": 1.813793103448276, "no_speech_prob": 3.689880827550951e-07}, {"id": 768, "seek": 546892, "start": 5477.02, "end": 5485.18, "text": " going to return a sequence of many batches so we can just pass in a list containing our", "tokens": [516, 281, 2736, 257, 8310, 295, 867, 15245, 279, 370, 321, 393, 445, 1320, 294, 257, 1329, 19273, 527], "temperature": 0.0, "avg_logprob": -0.09609171273051828, "compression_ratio": 1.813793103448276, "no_speech_prob": 3.689880827550951e-07}, {"id": 769, "seek": 546892, "start": 5485.18, "end": 5490.06, "text": " many batch as a data loader and so that's going to get the predictions for one mini", "tokens": [867, 15245, 382, 257, 1412, 3677, 260, 293, 370, 300, 311, 516, 281, 483, 264, 21264, 337, 472, 8382], "temperature": 0.0, "avg_logprob": -0.09609171273051828, "compression_ratio": 1.813793103448276, "no_speech_prob": 3.689880827550951e-07}, {"id": 770, "seek": 549006, "start": 5490.06, "end": 5499.9400000000005, "text": " batch so here's some predictions okay so the actual predictions if we go preds zero dot", "tokens": [15245, 370, 510, 311, 512, 21264, 1392, 370, 264, 3539, 21264, 498, 321, 352, 3852, 82, 4018, 5893], "temperature": 0.0, "avg_logprob": -0.09370068770188551, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.8291276521486e-07}, {"id": 771, "seek": 549006, "start": 5499.9400000000005, "end": 5507.06, "text": " sum to grab the predictions for the first image and add them all up they add up to one", "tokens": [2408, 281, 4444, 264, 21264, 337, 264, 700, 3256, 293, 909, 552, 439, 493, 436, 909, 493, 281, 472], "temperature": 0.0, "avg_logprob": -0.09370068770188551, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.8291276521486e-07}, {"id": 772, "seek": 549006, "start": 5507.06, "end": 5513.580000000001, "text": " and there are 37 of them so that makes sense right it's like the very first thing is what", "tokens": [293, 456, 366, 13435, 295, 552, 370, 300, 1669, 2020, 558, 309, 311, 411, 264, 588, 700, 551, 307, 437], "temperature": 0.0, "avg_logprob": -0.09370068770188551, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.8291276521486e-07}, {"id": 773, "seek": 551358, "start": 5513.58, "end": 5521.92, "text": " is the probability that that is a else vocab so the first thing is what's the probability", "tokens": [307, 264, 8482, 300, 300, 307, 257, 1646, 2329, 455, 370, 264, 700, 551, 307, 437, 311, 264, 8482], "temperature": 0.0, "avg_logprob": -0.12494447383474797, "compression_ratio": 1.8624338624338623, "no_speech_prob": 1.084513996829628e-06}, {"id": 774, "seek": 551358, "start": 5521.92, "end": 5529.46, "text": " it's an Abyssinian cat it's 10 to the negative 6 you see and so forth so it's basically like", "tokens": [309, 311, 364, 2847, 749, 19767, 952, 3857, 309, 311, 1266, 281, 264, 3671, 1386, 291, 536, 293, 370, 5220, 370, 309, 311, 1936, 411], "temperature": 0.0, "avg_logprob": -0.12494447383474797, "compression_ratio": 1.8624338624338623, "no_speech_prob": 1.084513996829628e-06}, {"id": 775, "seek": 551358, "start": 5529.46, "end": 5533.46, "text": " it's not this it's not this it's not this and you can look through and oh yeah this", "tokens": [309, 311, 406, 341, 309, 311, 406, 341, 309, 311, 406, 341, 293, 291, 393, 574, 807, 293, 1954, 1338, 341], "temperature": 0.0, "avg_logprob": -0.12494447383474797, "compression_ratio": 1.8624338624338623, "no_speech_prob": 1.084513996829628e-06}, {"id": 776, "seek": 551358, "start": 5533.46, "end": 5542.9, "text": " one here you know obviously what I think it is so how did it you know so we obviously", "tokens": [472, 510, 291, 458, 2745, 437, 286, 519, 309, 307, 370, 577, 630, 309, 291, 458, 370, 321, 2745], "temperature": 0.0, "avg_logprob": -0.12494447383474797, "compression_ratio": 1.8624338624338623, "no_speech_prob": 1.084513996829628e-06}, {"id": 777, "seek": 554290, "start": 5542.9, "end": 5548.0199999999995, "text": " want the probabilities to sum to one because it would be pretty weird if if they didn't", "tokens": [528, 264, 33783, 281, 2408, 281, 472, 570, 309, 576, 312, 1238, 3657, 498, 498, 436, 994, 380], "temperature": 0.0, "avg_logprob": -0.0748930199201717, "compression_ratio": 1.7912621359223302, "no_speech_prob": 4.737898677831254e-07}, {"id": 778, "seek": 554290, "start": 5548.0199999999995, "end": 5552.839999999999, "text": " it would say you know that the the probability of being one of these things is more than", "tokens": [309, 576, 584, 291, 458, 300, 264, 264, 8482, 295, 885, 472, 295, 613, 721, 307, 544, 813], "temperature": 0.0, "avg_logprob": -0.0748930199201717, "compression_ratio": 1.7912621359223302, "no_speech_prob": 4.737898677831254e-07}, {"id": 779, "seek": 554290, "start": 5552.839999999999, "end": 5562.78, "text": " one or less than one which would be extremely odd so how do we go about creating these predictions", "tokens": [472, 420, 1570, 813, 472, 597, 576, 312, 4664, 7401, 370, 577, 360, 321, 352, 466, 4084, 613, 21264], "temperature": 0.0, "avg_logprob": -0.0748930199201717, "compression_ratio": 1.7912621359223302, "no_speech_prob": 4.737898677831254e-07}, {"id": 780, "seek": 554290, "start": 5562.78, "end": 5568.54, "text": " where each one is between zero and one and they all add up to one to do that we use something", "tokens": [689, 1184, 472, 307, 1296, 4018, 293, 472, 293, 436, 439, 909, 493, 281, 472, 281, 360, 300, 321, 764, 746], "temperature": 0.0, "avg_logprob": -0.0748930199201717, "compression_ratio": 1.7912621359223302, "no_speech_prob": 4.737898677831254e-07}, {"id": 781, "seek": 556854, "start": 5568.54, "end": 5578.78, "text": " called softmax softmax is basically an extension of sigmoid to handle more than two levels", "tokens": [1219, 2787, 41167, 2787, 41167, 307, 1936, 364, 10320, 295, 4556, 3280, 327, 281, 4813, 544, 813, 732, 4358], "temperature": 0.0, "avg_logprob": -0.11854881793260574, "compression_ratio": 1.6790123456790123, "no_speech_prob": 8.714318937563803e-07}, {"id": 782, "seek": 556854, "start": 5578.78, "end": 5585.82, "text": " two categories so remember the sigmoid function look like this and we use that for our threes", "tokens": [732, 10479, 370, 1604, 264, 4556, 3280, 327, 2445, 574, 411, 341, 293, 321, 764, 300, 337, 527, 258, 4856], "temperature": 0.0, "avg_logprob": -0.11854881793260574, "compression_ratio": 1.6790123456790123, "no_speech_prob": 8.714318937563803e-07}, {"id": 783, "seek": 556854, "start": 5585.82, "end": 5594.42, "text": " versus sevens model so what if we want 37 categories rather than two categories we need", "tokens": [5717, 3407, 82, 2316, 370, 437, 498, 321, 528, 13435, 10479, 2831, 813, 732, 10479, 321, 643], "temperature": 0.0, "avg_logprob": -0.11854881793260574, "compression_ratio": 1.6790123456790123, "no_speech_prob": 8.714318937563803e-07}, {"id": 784, "seek": 559442, "start": 5594.42, "end": 5603.42, "text": " one activation for every category so actually the the threes and sevens model rather than", "tokens": [472, 24433, 337, 633, 7719, 370, 767, 264, 264, 258, 4856, 293, 3407, 82, 2316, 2831, 813], "temperature": 0.0, "avg_logprob": -0.0867323936560215, "compression_ratio": 1.9666666666666666, "no_speech_prob": 4.5921311198071635e-07}, {"id": 785, "seek": 559442, "start": 5603.42, "end": 5609.34, "text": " thinking of that as an is three model we could actually say oh that has two categories so", "tokens": [1953, 295, 300, 382, 364, 307, 1045, 2316, 321, 727, 767, 584, 1954, 300, 575, 732, 10479, 370], "temperature": 0.0, "avg_logprob": -0.0867323936560215, "compression_ratio": 1.9666666666666666, "no_speech_prob": 4.5921311198071635e-07}, {"id": 786, "seek": 559442, "start": 5609.34, "end": 5614.1, "text": " that's actually create two activations one representing how three like something is and", "tokens": [300, 311, 767, 1884, 732, 2430, 763, 472, 13460, 577, 1045, 411, 746, 307, 293], "temperature": 0.0, "avg_logprob": -0.0867323936560215, "compression_ratio": 1.9666666666666666, "no_speech_prob": 4.5921311198071635e-07}, {"id": 787, "seek": 559442, "start": 5614.1, "end": 5623.7, "text": " one representing how seven like something is so let's say you know let's just say that", "tokens": [472, 13460, 577, 3407, 411, 746, 307, 370, 718, 311, 584, 291, 458, 718, 311, 445, 584, 300], "temperature": 0.0, "avg_logprob": -0.0867323936560215, "compression_ratio": 1.9666666666666666, "no_speech_prob": 4.5921311198071635e-07}, {"id": 788, "seek": 562370, "start": 5623.7, "end": 5636.58, "text": " we have six MNIST digits and these were the can I do this and this first column were the", "tokens": [321, 362, 2309, 376, 45, 19756, 27011, 293, 613, 645, 264, 393, 286, 360, 341, 293, 341, 700, 7738, 645, 264], "temperature": 0.0, "avg_logprob": -0.1026532794490005, "compression_ratio": 1.7452229299363058, "no_speech_prob": 1.3081736369713326e-06}, {"id": 789, "seek": 562370, "start": 5636.58, "end": 5646.82, "text": " activations of my model for for one activation and the second column was for a second activation", "tokens": [2430, 763, 295, 452, 2316, 337, 337, 472, 24433, 293, 264, 1150, 7738, 390, 337, 257, 1150, 24433], "temperature": 0.0, "avg_logprob": -0.1026532794490005, "compression_ratio": 1.7452229299363058, "no_speech_prob": 1.3081736369713326e-06}, {"id": 790, "seek": 562370, "start": 5646.82, "end": 5651.34, "text": " so my final layer actually has two activations now so this is like how much like a three", "tokens": [370, 452, 2572, 4583, 767, 575, 732, 2430, 763, 586, 370, 341, 307, 411, 577, 709, 411, 257, 1045], "temperature": 0.0, "avg_logprob": -0.1026532794490005, "compression_ratio": 1.7452229299363058, "no_speech_prob": 1.3081736369713326e-06}, {"id": 791, "seek": 565134, "start": 5651.34, "end": 5657.02, "text": " is it and this is how much like a seven is it but this one is not at all like a three", "tokens": [307, 309, 293, 341, 307, 577, 709, 411, 257, 3407, 307, 309, 457, 341, 472, 307, 406, 412, 439, 411, 257, 1045], "temperature": 0.0, "avg_logprob": -0.08627623981899685, "compression_ratio": 2.1317365269461077, "no_speech_prob": 5.368743813960464e-07}, {"id": 792, "seek": 565134, "start": 5657.02, "end": 5662.58, "text": " and it's slightly not like a seven this is very much like a three and not much like a", "tokens": [293, 309, 311, 4748, 406, 411, 257, 3407, 341, 307, 588, 709, 411, 257, 1045, 293, 406, 709, 411, 257], "temperature": 0.0, "avg_logprob": -0.08627623981899685, "compression_ratio": 2.1317365269461077, "no_speech_prob": 5.368743813960464e-07}, {"id": 793, "seek": 565134, "start": 5662.58, "end": 5667.42, "text": " seven and so forth so we can take that model and rather having rather than having one activation", "tokens": [3407, 293, 370, 5220, 370, 321, 393, 747, 300, 2316, 293, 2831, 1419, 2831, 813, 1419, 472, 24433], "temperature": 0.0, "avg_logprob": -0.08627623981899685, "compression_ratio": 2.1317365269461077, "no_speech_prob": 5.368743813960464e-07}, {"id": 794, "seek": 565134, "start": 5667.42, "end": 5672.62, "text": " for like is three we can have two activations for how much like a three how much like a", "tokens": [337, 411, 307, 1045, 321, 393, 362, 732, 2430, 763, 337, 577, 709, 411, 257, 1045, 577, 709, 411, 257], "temperature": 0.0, "avg_logprob": -0.08627623981899685, "compression_ratio": 2.1317365269461077, "no_speech_prob": 5.368743813960464e-07}, {"id": 795, "seek": 567262, "start": 5672.62, "end": 5682.0599999999995, "text": " seven so if we take the sigmoid of that we get two numbers between naught and one but", "tokens": [3407, 370, 498, 321, 747, 264, 4556, 3280, 327, 295, 300, 321, 483, 732, 3547, 1296, 13138, 293, 472, 457], "temperature": 0.0, "avg_logprob": -0.0850736232514077, "compression_ratio": 1.77, "no_speech_prob": 1.6028078562158043e-06}, {"id": 796, "seek": 567262, "start": 5682.0599999999995, "end": 5688.78, "text": " they don't add up to one so that doesn't make any sense it can't be point six six chance", "tokens": [436, 500, 380, 909, 493, 281, 472, 370, 300, 1177, 380, 652, 604, 2020, 309, 393, 380, 312, 935, 2309, 2309, 2931], "temperature": 0.0, "avg_logprob": -0.0850736232514077, "compression_ratio": 1.77, "no_speech_prob": 1.6028078562158043e-06}, {"id": 797, "seek": 567262, "start": 5688.78, "end": 5693.46, "text": " it's a three and point five six chance it's a seven because every digit in that data set", "tokens": [309, 311, 257, 1045, 293, 935, 1732, 2309, 2931, 309, 311, 257, 3407, 570, 633, 14293, 294, 300, 1412, 992], "temperature": 0.0, "avg_logprob": -0.0850736232514077, "compression_ratio": 1.77, "no_speech_prob": 1.6028078562158043e-06}, {"id": 798, "seek": 567262, "start": 5693.46, "end": 5700.9, "text": " is only one or the other so that's not going to work but what we could do is we could take", "tokens": [307, 787, 472, 420, 264, 661, 370, 300, 311, 406, 516, 281, 589, 457, 437, 321, 727, 360, 307, 321, 727, 747], "temperature": 0.0, "avg_logprob": -0.0850736232514077, "compression_ratio": 1.77, "no_speech_prob": 1.6028078562158043e-06}, {"id": 799, "seek": 570090, "start": 5700.9, "end": 5706.98, "text": " the difference between this value and this value and say that's how likely it is to be", "tokens": [264, 2649, 1296, 341, 2158, 293, 341, 2158, 293, 584, 300, 311, 577, 3700, 309, 307, 281, 312], "temperature": 0.0, "avg_logprob": -0.0740296273004441, "compression_ratio": 1.8375634517766497, "no_speech_prob": 5.04346530760813e-07}, {"id": 800, "seek": 570090, "start": 5706.98, "end": 5712.0599999999995, "text": " a three so in other words this one here with a high number here and a low number here is", "tokens": [257, 1045, 370, 294, 661, 2283, 341, 472, 510, 365, 257, 1090, 1230, 510, 293, 257, 2295, 1230, 510, 307], "temperature": 0.0, "avg_logprob": -0.0740296273004441, "compression_ratio": 1.8375634517766497, "no_speech_prob": 5.04346530760813e-07}, {"id": 801, "seek": 570090, "start": 5712.0599999999995, "end": 5721.78, "text": " very likely to be a three so we could basically say in the binary case these activations that", "tokens": [588, 3700, 281, 312, 257, 1045, 370, 321, 727, 1936, 584, 294, 264, 17434, 1389, 613, 2430, 763, 300], "temperature": 0.0, "avg_logprob": -0.0740296273004441, "compression_ratio": 1.8375634517766497, "no_speech_prob": 5.04346530760813e-07}, {"id": 802, "seek": 570090, "start": 5721.78, "end": 5727.0199999999995, "text": " what really matters is their relative confidence of being a three versus a seven so we could", "tokens": [437, 534, 7001, 307, 641, 4972, 6687, 295, 885, 257, 1045, 5717, 257, 3407, 370, 321, 727], "temperature": 0.0, "avg_logprob": -0.0740296273004441, "compression_ratio": 1.8375634517766497, "no_speech_prob": 5.04346530760813e-07}, {"id": 803, "seek": 572702, "start": 5727.02, "end": 5732.1, "text": " calculate the difference between column one and column two or column index zero and column", "tokens": [8873, 264, 2649, 1296, 7738, 472, 293, 7738, 732, 420, 7738, 8186, 4018, 293, 7738], "temperature": 0.0, "avg_logprob": -0.09203519225120545, "compression_ratio": 2.0625, "no_speech_prob": 3.011594742474699e-07}, {"id": 804, "seek": 572702, "start": 5732.1, "end": 5738.240000000001, "text": " index one right now here's the difference between the two columns there's that big difference", "tokens": [8186, 472, 558, 586, 510, 311, 264, 2649, 1296, 264, 732, 13766, 456, 311, 300, 955, 2649], "temperature": 0.0, "avg_logprob": -0.09203519225120545, "compression_ratio": 2.0625, "no_speech_prob": 3.011594742474699e-07}, {"id": 805, "seek": 572702, "start": 5738.240000000001, "end": 5747.580000000001, "text": " and we could take the sigmoid of that right and so this is now giving us a single number", "tokens": [293, 321, 727, 747, 264, 4556, 3280, 327, 295, 300, 558, 293, 370, 341, 307, 586, 2902, 505, 257, 2167, 1230], "temperature": 0.0, "avg_logprob": -0.09203519225120545, "compression_ratio": 2.0625, "no_speech_prob": 3.011594742474699e-07}, {"id": 806, "seek": 572702, "start": 5747.580000000001, "end": 5755.120000000001, "text": " between naught and one and so then since we wanted two columns we could make column index", "tokens": [1296, 13138, 293, 472, 293, 370, 550, 1670, 321, 1415, 732, 13766, 321, 727, 652, 7738, 8186], "temperature": 0.0, "avg_logprob": -0.09203519225120545, "compression_ratio": 2.0625, "no_speech_prob": 3.011594742474699e-07}, {"id": 807, "seek": 575512, "start": 5755.12, "end": 5762.5, "text": " zero the sigmoid and column index one could be one minus that and now look these all add", "tokens": [4018, 264, 4556, 3280, 327, 293, 7738, 8186, 472, 727, 312, 472, 3175, 300, 293, 586, 574, 613, 439, 909], "temperature": 0.0, "avg_logprob": -0.10373161055824974, "compression_ratio": 1.6951219512195121, "no_speech_prob": 2.964903842439526e-07}, {"id": 808, "seek": 575512, "start": 5762.5, "end": 5769.42, "text": " up to one so here's probability of three probability of seven for the second one probably three", "tokens": [493, 281, 472, 370, 510, 311, 8482, 295, 1045, 8482, 295, 3407, 337, 264, 1150, 472, 1391, 1045], "temperature": 0.0, "avg_logprob": -0.10373161055824974, "compression_ratio": 1.6951219512195121, "no_speech_prob": 2.964903842439526e-07}, {"id": 809, "seek": 575512, "start": 5769.42, "end": 5781.94, "text": " probably seven and so forth so like that's a way that we could go from having two activations", "tokens": [1391, 3407, 293, 370, 5220, 370, 411, 300, 311, 257, 636, 300, 321, 727, 352, 490, 1419, 732, 2430, 763], "temperature": 0.0, "avg_logprob": -0.10373161055824974, "compression_ratio": 1.6951219512195121, "no_speech_prob": 2.964903842439526e-07}, {"id": 810, "seek": 578194, "start": 5781.94, "end": 5790.299999999999, "text": " for every image to creating two probabilities each of which is between naught and one and", "tokens": [337, 633, 3256, 281, 4084, 732, 33783, 1184, 295, 597, 307, 1296, 13138, 293, 472, 293], "temperature": 0.0, "avg_logprob": -0.18239081290460402, "compression_ratio": 1.6624203821656052, "no_speech_prob": 1.2359565459973965e-07}, {"id": 811, "seek": 578194, "start": 5790.299999999999, "end": 5799.82, "text": " each pair of which adds to one. Great how do we extend that to more than two columns", "tokens": [1184, 6119, 295, 597, 10860, 281, 472, 13, 3769, 577, 360, 321, 10101, 300, 281, 544, 813, 732, 13766], "temperature": 0.0, "avg_logprob": -0.18239081290460402, "compression_ratio": 1.6624203821656052, "no_speech_prob": 1.2359565459973965e-07}, {"id": 812, "seek": 578194, "start": 5799.82, "end": 5806.82, "text": " to extend it to more than two columns we use this function which is called softmax. So", "tokens": [281, 10101, 309, 281, 544, 813, 732, 13766, 321, 764, 341, 2445, 597, 307, 1219, 2787, 41167, 13, 407], "temperature": 0.0, "avg_logprob": -0.18239081290460402, "compression_ratio": 1.6624203821656052, "no_speech_prob": 1.2359565459973965e-07}, {"id": 813, "seek": 580682, "start": 5806.82, "end": 5820.38, "text": " softmax is equal to e to the x divided by sum of e to the x. Just to show you if I go", "tokens": [2787, 41167, 307, 2681, 281, 308, 281, 264, 2031, 6666, 538, 2408, 295, 308, 281, 264, 2031, 13, 1449, 281, 855, 291, 498, 286, 352], "temperature": 0.0, "avg_logprob": -0.1278764615293409, "compression_ratio": 1.392, "no_speech_prob": 3.359677123171423e-07}, {"id": 814, "seek": 580682, "start": 5820.38, "end": 5831.299999999999, "text": " softmax on my activations I get 0.6025 0.3975 0.6025 0.3975 I get exactly the same thing", "tokens": [2787, 41167, 322, 452, 2430, 763, 286, 483, 1958, 13, 4550, 6074, 1958, 13, 12493, 11901, 1958, 13, 4550, 6074, 1958, 13, 12493, 11901, 286, 483, 2293, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.1278764615293409, "compression_ratio": 1.392, "no_speech_prob": 3.359677123171423e-07}, {"id": 815, "seek": 583130, "start": 5831.3, "end": 5844.7, "text": " right. So softmax in the binary case is identical to the sigmoid that we just looked at but", "tokens": [558, 13, 407, 2787, 41167, 294, 264, 17434, 1389, 307, 14800, 281, 264, 4556, 3280, 327, 300, 321, 445, 2956, 412, 457], "temperature": 0.0, "avg_logprob": -0.13404837619052845, "compression_ratio": 1.641255605381166, "no_speech_prob": 4.888298690275406e-07}, {"id": 816, "seek": 583130, "start": 5844.7, "end": 5848.9800000000005, "text": " in the multi category case we basically end up with something like this let's say we were", "tokens": [294, 264, 4825, 7719, 1389, 321, 1936, 917, 493, 365, 746, 411, 341, 718, 311, 584, 321, 645], "temperature": 0.0, "avg_logprob": -0.13404837619052845, "compression_ratio": 1.641255605381166, "no_speech_prob": 4.888298690275406e-07}, {"id": 817, "seek": 583130, "start": 5848.9800000000005, "end": 5854.9800000000005, "text": " doing the teddy bear grizzly bear brown bear and for that remember our neural net is going", "tokens": [884, 264, 45116, 6155, 17865, 4313, 356, 6155, 6292, 6155, 293, 337, 300, 1604, 527, 18161, 2533, 307, 516], "temperature": 0.0, "avg_logprob": -0.13404837619052845, "compression_ratio": 1.641255605381166, "no_speech_prob": 4.888298690275406e-07}, {"id": 818, "seek": 583130, "start": 5854.9800000000005, "end": 5860.34, "text": " to have the final layer will have three activations so let's say it was point oh two negative", "tokens": [281, 362, 264, 2572, 4583, 486, 362, 1045, 2430, 763, 370, 718, 311, 584, 309, 390, 935, 1954, 732, 3671], "temperature": 0.0, "avg_logprob": -0.13404837619052845, "compression_ratio": 1.641255605381166, "no_speech_prob": 4.888298690275406e-07}, {"id": 819, "seek": 586034, "start": 5860.34, "end": 5865.9400000000005, "text": " two point four nine one point two five that I calculate softmax I first go e to the power", "tokens": [732, 935, 1451, 4949, 472, 935, 732, 1732, 300, 286, 8873, 2787, 41167, 286, 700, 352, 308, 281, 264, 1347], "temperature": 0.0, "avg_logprob": -0.15444932904159814, "compression_ratio": 2.187192118226601, "no_speech_prob": 1.191107230624766e-06}, {"id": 820, "seek": 586034, "start": 5865.9400000000005, "end": 5871.22, "text": " of each of these three things so here's e to the power of point oh two e to the power", "tokens": [295, 1184, 295, 613, 1045, 721, 370, 510, 311, 308, 281, 264, 1347, 295, 935, 1954, 732, 308, 281, 264, 1347], "temperature": 0.0, "avg_logprob": -0.15444932904159814, "compression_ratio": 2.187192118226601, "no_speech_prob": 1.191107230624766e-06}, {"id": 821, "seek": 586034, "start": 5871.22, "end": 5875.54, "text": " of negative two point four nine e to the power of three point four eight of the power of", "tokens": [295, 3671, 732, 935, 1451, 4949, 308, 281, 264, 1347, 295, 1045, 935, 1451, 3180, 295, 264, 1347, 295], "temperature": 0.0, "avg_logprob": -0.15444932904159814, "compression_ratio": 2.187192118226601, "no_speech_prob": 1.191107230624766e-06}, {"id": 822, "seek": 586034, "start": 5875.54, "end": 5882.02, "text": " one point two five okay then I add them up so there's the sum of the X's and then softmax", "tokens": [472, 935, 732, 1732, 1392, 550, 286, 909, 552, 493, 370, 456, 311, 264, 2408, 295, 264, 1783, 311, 293, 550, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.15444932904159814, "compression_ratio": 2.187192118226601, "no_speech_prob": 1.191107230624766e-06}, {"id": 823, "seek": 586034, "start": 5882.02, "end": 5887.54, "text": " will simply be one point oh two divided by four point six and then this one will be point", "tokens": [486, 2935, 312, 472, 935, 1954, 732, 6666, 538, 1451, 935, 2309, 293, 550, 341, 472, 486, 312, 935], "temperature": 0.0, "avg_logprob": -0.15444932904159814, "compression_ratio": 2.187192118226601, "no_speech_prob": 1.191107230624766e-06}, {"id": 824, "seek": 588754, "start": 5887.54, "end": 5891.06, "text": " oh eight divided by four point six and this one will be three point four nine divided", "tokens": [1954, 3180, 6666, 538, 1451, 935, 2309, 293, 341, 472, 486, 312, 1045, 935, 1451, 4949, 6666], "temperature": 0.0, "avg_logprob": -0.07384691334734059, "compression_ratio": 2.0970873786407767, "no_speech_prob": 1.5294112927222159e-06}, {"id": 825, "seek": 588754, "start": 5891.06, "end": 5897.62, "text": " by four point six so since each one of these represents each number divided by the sum", "tokens": [538, 1451, 935, 2309, 370, 1670, 1184, 472, 295, 613, 8855, 1184, 1230, 6666, 538, 264, 2408], "temperature": 0.0, "avg_logprob": -0.07384691334734059, "compression_ratio": 2.0970873786407767, "no_speech_prob": 1.5294112927222159e-06}, {"id": 826, "seek": 588754, "start": 5897.62, "end": 5905.16, "text": " that means that the total is one okay and because all of these are positive and each", "tokens": [300, 1355, 300, 264, 3217, 307, 472, 1392, 293, 570, 439, 295, 613, 366, 3353, 293, 1184], "temperature": 0.0, "avg_logprob": -0.07384691334734059, "compression_ratio": 2.0970873786407767, "no_speech_prob": 1.5294112927222159e-06}, {"id": 827, "seek": 588754, "start": 5905.16, "end": 5910.42, "text": " one is an item divided by the sum it means all of these must be between naught and one", "tokens": [472, 307, 364, 3174, 6666, 538, 264, 2408, 309, 1355, 439, 295, 613, 1633, 312, 1296, 13138, 293, 472], "temperature": 0.0, "avg_logprob": -0.07384691334734059, "compression_ratio": 2.0970873786407767, "no_speech_prob": 1.5294112927222159e-06}, {"id": 828, "seek": 588754, "start": 5910.42, "end": 5916.62, "text": " so this shows you that softmax always gives you numbers between naught and one and they", "tokens": [370, 341, 3110, 291, 300, 2787, 41167, 1009, 2709, 291, 3547, 1296, 13138, 293, 472, 293, 436], "temperature": 0.0, "avg_logprob": -0.07384691334734059, "compression_ratio": 2.0970873786407767, "no_speech_prob": 1.5294112927222159e-06}, {"id": 829, "seek": 591662, "start": 5916.62, "end": 5925.099999999999, "text": " always add up to one so to do that in practice you can just call torch dot softmax and it", "tokens": [1009, 909, 493, 281, 472, 370, 281, 360, 300, 294, 3124, 291, 393, 445, 818, 27822, 5893, 2787, 41167, 293, 309], "temperature": 0.0, "avg_logprob": -0.0960730840993482, "compression_ratio": 1.6981132075471699, "no_speech_prob": 4.181181907370046e-07}, {"id": 830, "seek": 591662, "start": 5925.099999999999, "end": 5933.74, "text": " will give you this result of this this function so you should experiment with this in your", "tokens": [486, 976, 291, 341, 1874, 295, 341, 341, 2445, 370, 291, 820, 5120, 365, 341, 294, 428], "temperature": 0.0, "avg_logprob": -0.0960730840993482, "compression_ratio": 1.6981132075471699, "no_speech_prob": 4.181181907370046e-07}, {"id": 831, "seek": 591662, "start": 5933.74, "end": 5941.22, "text": " own time you know write this out by hand and try putting in these numbers right and and", "tokens": [1065, 565, 291, 458, 2464, 341, 484, 538, 1011, 293, 853, 3372, 294, 613, 3547, 558, 293, 293], "temperature": 0.0, "avg_logprob": -0.0960730840993482, "compression_ratio": 1.6981132075471699, "no_speech_prob": 4.181181907370046e-07}, {"id": 832, "seek": 591662, "start": 5941.22, "end": 5945.18, "text": " see how that you get back the numbers I claim you're going to get back make sure this makes", "tokens": [536, 577, 300, 291, 483, 646, 264, 3547, 286, 3932, 291, 434, 516, 281, 483, 646, 652, 988, 341, 1669], "temperature": 0.0, "avg_logprob": -0.0960730840993482, "compression_ratio": 1.6981132075471699, "no_speech_prob": 4.181181907370046e-07}, {"id": 833, "seek": 594518, "start": 5945.18, "end": 5952.66, "text": " sense to you so one of the interesting points about softmax is remember I told you that", "tokens": [2020, 281, 291, 370, 472, 295, 264, 1880, 2793, 466, 2787, 41167, 307, 1604, 286, 1907, 291, 300], "temperature": 0.0, "avg_logprob": -0.11659650802612305, "compression_ratio": 1.6071428571428572, "no_speech_prob": 6.083582775318064e-07}, {"id": 834, "seek": 594518, "start": 5952.66, "end": 5960.780000000001, "text": " X is e to the power of something and now what that means is that e to the power of something", "tokens": [1783, 307, 308, 281, 264, 1347, 295, 746, 293, 586, 437, 300, 1355, 307, 300, 308, 281, 264, 1347, 295, 746], "temperature": 0.0, "avg_logprob": -0.11659650802612305, "compression_ratio": 1.6071428571428572, "no_speech_prob": 6.083582775318064e-07}, {"id": 835, "seek": 596078, "start": 5960.78, "end": 5981.94, "text": " grows very very fast right so like x before is 54 x of eight is 292,980 right it grows", "tokens": [13156, 588, 588, 2370, 558, 370, 411, 2031, 949, 307, 20793, 2031, 295, 3180, 307, 9413, 17, 11, 24, 4702, 558, 309, 13156], "temperature": 0.0, "avg_logprob": -0.20491078559388506, "compression_ratio": 1.3709677419354838, "no_speech_prob": 7.811468094587326e-07}, {"id": 836, "seek": 596078, "start": 5981.94, "end": 5988.0599999999995, "text": " super fast and what that means is that if you have one activation that's just a bit", "tokens": [1687, 2370, 293, 437, 300, 1355, 307, 300, 498, 291, 362, 472, 24433, 300, 311, 445, 257, 857], "temperature": 0.0, "avg_logprob": -0.20491078559388506, "compression_ratio": 1.3709677419354838, "no_speech_prob": 7.811468094587326e-07}, {"id": 837, "seek": 598806, "start": 5988.06, "end": 5994.3, "text": " bigger than the others its softmax will be a lot bigger than the others so intuitively", "tokens": [3801, 813, 264, 2357, 1080, 2787, 41167, 486, 312, 257, 688, 3801, 813, 264, 2357, 370, 46506], "temperature": 0.0, "avg_logprob": -0.0682102876551011, "compression_ratio": 1.8617021276595744, "no_speech_prob": 2.406094608886633e-06}, {"id": 838, "seek": 598806, "start": 5994.3, "end": 6002.18, "text": " the softmax function really wants to pick one class among the others which is generally", "tokens": [264, 2787, 41167, 2445, 534, 2738, 281, 1888, 472, 1508, 3654, 264, 2357, 597, 307, 5101], "temperature": 0.0, "avg_logprob": -0.0682102876551011, "compression_ratio": 1.8617021276595744, "no_speech_prob": 2.406094608886633e-06}, {"id": 839, "seek": 598806, "start": 6002.18, "end": 6007.860000000001, "text": " what you want right when you're trying to train a classifier to say which breed is it", "tokens": [437, 291, 528, 558, 562, 291, 434, 1382, 281, 3847, 257, 1508, 9902, 281, 584, 597, 18971, 307, 309], "temperature": 0.0, "avg_logprob": -0.0682102876551011, "compression_ratio": 1.8617021276595744, "no_speech_prob": 2.406094608886633e-06}, {"id": 840, "seek": 598806, "start": 6007.860000000001, "end": 6013.620000000001, "text": " you kind of want it to to pick one and kind of go for it right and so that's what softmax", "tokens": [291, 733, 295, 528, 309, 281, 281, 1888, 472, 293, 733, 295, 352, 337, 309, 558, 293, 370, 300, 311, 437, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.0682102876551011, "compression_ratio": 1.8617021276595744, "no_speech_prob": 2.406094608886633e-06}, {"id": 841, "seek": 601362, "start": 6013.62, "end": 6021.18, "text": " does that's not what you always want so sometimes at inference time you want it to be a bit", "tokens": [775, 300, 311, 406, 437, 291, 1009, 528, 370, 2171, 412, 38253, 565, 291, 528, 309, 281, 312, 257, 857], "temperature": 0.0, "avg_logprob": -0.12432933110062794, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.026130005106097e-06}, {"id": 842, "seek": 601362, "start": 6021.18, "end": 6027.3, "text": " cautious and so you kind of got to remember that softmax isn't always the perfect approach", "tokens": [25278, 293, 370, 291, 733, 295, 658, 281, 1604, 300, 2787, 41167, 1943, 380, 1009, 264, 2176, 3109], "temperature": 0.0, "avg_logprob": -0.12432933110062794, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.026130005106097e-06}, {"id": 843, "seek": 601362, "start": 6027.3, "end": 6031.38, "text": " but it's the default it's what we use most of the time and it works well on a lot of", "tokens": [457, 309, 311, 264, 7576, 309, 311, 437, 321, 764, 881, 295, 264, 565, 293, 309, 1985, 731, 322, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.12432933110062794, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.026130005106097e-06}, {"id": 844, "seek": 601362, "start": 6031.38, "end": 6043.38, "text": " situations so that is softmax now in the binary case for the MNIST 3 versus 7s this was how", "tokens": [6851, 370, 300, 307, 2787, 41167, 586, 294, 264, 17434, 1389, 337, 264, 376, 45, 19756, 805, 5717, 1614, 82, 341, 390, 577], "temperature": 0.0, "avg_logprob": -0.12432933110062794, "compression_ratio": 1.617117117117117, "no_speech_prob": 2.026130005106097e-06}, {"id": 845, "seek": 604338, "start": 6043.38, "end": 6049.24, "text": " we calculated MNIST loss we took the sigmoid and then we did either one minus that or that", "tokens": [321, 15598, 376, 45, 19756, 4470, 321, 1890, 264, 4556, 3280, 327, 293, 550, 321, 630, 2139, 472, 3175, 300, 420, 300], "temperature": 0.0, "avg_logprob": -0.1347516515980596, "compression_ratio": 1.4972375690607735, "no_speech_prob": 5.539164362744486e-07}, {"id": 846, "seek": 604338, "start": 6049.24, "end": 6059.9800000000005, "text": " as our loss function which is fine as you saw it it worked right and so we could do", "tokens": [382, 527, 4470, 2445, 597, 307, 2489, 382, 291, 1866, 309, 309, 2732, 558, 293, 370, 321, 727, 360], "temperature": 0.0, "avg_logprob": -0.1347516515980596, "compression_ratio": 1.4972375690607735, "no_speech_prob": 5.539164362744486e-07}, {"id": 847, "seek": 604338, "start": 6059.9800000000005, "end": 6066.34, "text": " this exactly the same thing we can't use torch.where anymore because targets aren't just zero or", "tokens": [341, 2293, 264, 912, 551, 321, 393, 380, 764, 27822, 13, 1992, 3602, 570, 12911, 3212, 380, 445, 4018, 420], "temperature": 0.0, "avg_logprob": -0.1347516515980596, "compression_ratio": 1.4972375690607735, "no_speech_prob": 5.539164362744486e-07}, {"id": 848, "seek": 606634, "start": 6066.34, "end": 6074.62, "text": " one targets could be any number from 0 to 36 so we could do that by replacing the torch.where", "tokens": [472, 12911, 727, 312, 604, 1230, 490, 1958, 281, 8652, 370, 321, 727, 360, 300, 538, 19139, 264, 27822, 13, 1992], "temperature": 0.0, "avg_logprob": -0.13623131762494098, "compression_ratio": 1.6605504587155964, "no_speech_prob": 1.5143248788263008e-07}, {"id": 849, "seek": 606634, "start": 6074.62, "end": 6081.18, "text": " with indexing so here's an example for the binary case let's say these are our targets", "tokens": [365, 8186, 278, 370, 510, 311, 364, 1365, 337, 264, 17434, 1389, 718, 311, 584, 613, 366, 527, 12911], "temperature": 0.0, "avg_logprob": -0.13623131762494098, "compression_ratio": 1.6605504587155964, "no_speech_prob": 1.5143248788263008e-07}, {"id": 850, "seek": 606634, "start": 6081.18, "end": 6088.900000000001, "text": " 0 1 0 1 1 0 and these are our softmax activations which we calculated before they're just some", "tokens": [1958, 502, 1958, 502, 502, 1958, 293, 613, 366, 527, 2787, 41167, 2430, 763, 597, 321, 15598, 949, 436, 434, 445, 512], "temperature": 0.0, "avg_logprob": -0.13623131762494098, "compression_ratio": 1.6605504587155964, "no_speech_prob": 1.5143248788263008e-07}, {"id": 851, "seek": 606634, "start": 6088.900000000001, "end": 6096.3, "text": " random numbers just for a toy example so one way to do instead of doing torch.where is", "tokens": [4974, 3547, 445, 337, 257, 12058, 1365, 370, 472, 636, 281, 360, 2602, 295, 884, 27822, 13, 1992, 307], "temperature": 0.0, "avg_logprob": -0.13623131762494098, "compression_ratio": 1.6605504587155964, "no_speech_prob": 1.5143248788263008e-07}, {"id": 852, "seek": 609630, "start": 6096.3, "end": 6103.26, "text": " it we could instead have a look at this I could say I could grab all the numbers from", "tokens": [309, 321, 727, 2602, 362, 257, 574, 412, 341, 286, 727, 584, 286, 727, 4444, 439, 264, 3547, 490], "temperature": 0.0, "avg_logprob": -0.15054501985248767, "compression_ratio": 1.6560509554140128, "no_speech_prob": 3.966973963542841e-06}, {"id": 853, "seek": 609630, "start": 6103.26, "end": 6111.820000000001, "text": " not to 5 and if I index into here with all the numbers from 0 to 5 and then my targets", "tokens": [406, 281, 1025, 293, 498, 286, 8186, 666, 510, 365, 439, 264, 3547, 490, 1958, 281, 1025, 293, 550, 452, 12911], "temperature": 0.0, "avg_logprob": -0.15054501985248767, "compression_ratio": 1.6560509554140128, "no_speech_prob": 3.966973963542841e-06}, {"id": 854, "seek": 609630, "start": 6111.820000000001, "end": 6124.92, "text": " 0 1 0 1 1 0 then what that's going to do is it's going to pick the row 0 it'll pick 0.6", "tokens": [1958, 502, 1958, 502, 502, 1958, 550, 437, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 1888, 264, 5386, 1958, 309, 603, 1888, 1958, 13, 21], "temperature": 0.0, "avg_logprob": -0.15054501985248767, "compression_ratio": 1.6560509554140128, "no_speech_prob": 3.966973963542841e-06}, {"id": 855, "seek": 612492, "start": 6124.92, "end": 6139.22, "text": " and then for row 1 it'll pick 1.49 a row 2 it'll pick 0.13 row 4 it'll pick 1.003 and", "tokens": [293, 550, 337, 5386, 502, 309, 603, 1888, 502, 13, 14938, 257, 5386, 568, 309, 603, 1888, 1958, 13, 7668, 5386, 1017, 309, 603, 1888, 502, 13, 628, 18, 293], "temperature": 0.0, "avg_logprob": -0.14808040775664866, "compression_ratio": 1.5317919075144508, "no_speech_prob": 2.918935990692262e-07}, {"id": 856, "seek": 612492, "start": 6139.22, "end": 6148.38, "text": " so forth so this is a super nifty indexing expression which you should definitely play", "tokens": [370, 5220, 370, 341, 307, 257, 1687, 297, 37177, 8186, 278, 6114, 597, 291, 820, 2138, 862], "temperature": 0.0, "avg_logprob": -0.14808040775664866, "compression_ratio": 1.5317919075144508, "no_speech_prob": 2.918935990692262e-07}, {"id": 857, "seek": 612492, "start": 6148.38, "end": 6154.82, "text": " with right and it's basically this trick of passing multiple things to the pie torch indexer", "tokens": [365, 558, 293, 309, 311, 1936, 341, 4282, 295, 8437, 3866, 721, 281, 264, 1730, 27822, 8186, 260], "temperature": 0.0, "avg_logprob": -0.14808040775664866, "compression_ratio": 1.5317919075144508, "no_speech_prob": 2.918935990692262e-07}, {"id": 858, "seek": 615482, "start": 6154.82, "end": 6160.04, "text": " the first thing says which rows should you return and the second thing says for each", "tokens": [264, 700, 551, 1619, 597, 13241, 820, 291, 2736, 293, 264, 1150, 551, 1619, 337, 1184], "temperature": 0.0, "avg_logprob": -0.08055807749430338, "compression_ratio": 1.7837837837837838, "no_speech_prob": 4.5921294145045977e-07}, {"id": 859, "seek": 615482, "start": 6160.04, "end": 6166.98, "text": " of those rows which column should you return so this is returning all the rows and these", "tokens": [295, 729, 13241, 597, 7738, 820, 291, 2736, 370, 341, 307, 12678, 439, 264, 13241, 293, 613], "temperature": 0.0, "avg_logprob": -0.08055807749430338, "compression_ratio": 1.7837837837837838, "no_speech_prob": 4.5921294145045977e-07}, {"id": 860, "seek": 615482, "start": 6166.98, "end": 6177.0599999999995, "text": " columns for each one and so this is actually identical to torch.where so isn't that tricky", "tokens": [13766, 337, 1184, 472, 293, 370, 341, 307, 767, 14800, 281, 27822, 13, 1992, 370, 1943, 380, 300, 12414], "temperature": 0.0, "avg_logprob": -0.08055807749430338, "compression_ratio": 1.7837837837837838, "no_speech_prob": 4.5921294145045977e-07}, {"id": 861, "seek": 617706, "start": 6177.06, "end": 6186.38, "text": " and so the nice thing is we can now use that for more than just two values and so here's", "tokens": [293, 370, 264, 1481, 551, 307, 321, 393, 586, 764, 300, 337, 544, 813, 445, 732, 4190, 293, 370, 510, 311], "temperature": 0.0, "avg_logprob": -0.133166475993831, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.577915883099195e-07}, {"id": 862, "seek": 617706, "start": 6186.38, "end": 6191.22, "text": " here's the fully worked out thing so I've got my threes column I've got my sevens column", "tokens": [510, 311, 264, 4498, 2732, 484, 551, 370, 286, 600, 658, 452, 258, 4856, 7738, 286, 600, 658, 452, 3407, 82, 7738], "temperature": 0.0, "avg_logprob": -0.133166475993831, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.577915883099195e-07}, {"id": 863, "seek": 617706, "start": 6191.22, "end": 6203.660000000001, "text": " here's that target is the indexes from not 1 2 3 4 5 and so here's 0 0.6 1 1.49 0 2.13", "tokens": [510, 311, 300, 3779, 307, 264, 8186, 279, 490, 406, 502, 568, 805, 1017, 1025, 293, 370, 510, 311, 1958, 1958, 13, 21, 502, 502, 13, 14938, 1958, 568, 13, 7668], "temperature": 0.0, "avg_logprob": -0.133166475993831, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.577915883099195e-07}, {"id": 864, "seek": 620366, "start": 6203.66, "end": 6213.74, "text": " and so forth so yeah this works just as well with more than two columns so we can add you", "tokens": [293, 370, 5220, 370, 1338, 341, 1985, 445, 382, 731, 365, 544, 813, 732, 13766, 370, 321, 393, 909, 291], "temperature": 0.0, "avg_logprob": -0.10590826946756114, "compression_ratio": 1.5705882352941176, "no_speech_prob": 1.101592943086871e-06}, {"id": 865, "seek": 620366, "start": 6213.74, "end": 6218.3, "text": " know for doing a full M list you know so all the digits from naught to 9 we could have", "tokens": [458, 337, 884, 257, 1577, 376, 1329, 291, 458, 370, 439, 264, 27011, 490, 13138, 281, 1722, 321, 727, 362], "temperature": 0.0, "avg_logprob": -0.10590826946756114, "compression_ratio": 1.5705882352941176, "no_speech_prob": 1.101592943086871e-06}, {"id": 866, "seek": 620366, "start": 6218.3, "end": 6227.3, "text": " 10 columns and we would just be indexing into the 10 so this thing we're doing where we're", "tokens": [1266, 13766, 293, 321, 576, 445, 312, 8186, 278, 666, 264, 1266, 370, 341, 551, 321, 434, 884, 689, 321, 434], "temperature": 0.0, "avg_logprob": -0.10590826946756114, "compression_ratio": 1.5705882352941176, "no_speech_prob": 1.101592943086871e-06}, {"id": 867, "seek": 622730, "start": 6227.3, "end": 6235.14, "text": " going minus our activations matrix all of the numbers from naught to n and then our", "tokens": [516, 3175, 527, 2430, 763, 8141, 439, 295, 264, 3547, 490, 13138, 281, 297, 293, 550, 527], "temperature": 0.0, "avg_logprob": -0.18808928201364916, "compression_ratio": 1.6651162790697673, "no_speech_prob": 1.1365608543201233e-06}, {"id": 868, "seek": 622730, "start": 6235.14, "end": 6242.9400000000005, "text": " targets is exactly the same as something that already exists in pie torch called f.nllloss", "tokens": [12911, 307, 2293, 264, 912, 382, 746, 300, 1217, 8198, 294, 1730, 27822, 1219, 283, 13, 77, 285, 75, 772], "temperature": 0.0, "avg_logprob": -0.18808928201364916, "compression_ratio": 1.6651162790697673, "no_speech_prob": 1.1365608543201233e-06}, {"id": 869, "seek": 622730, "start": 6242.9400000000005, "end": 6247.9400000000005, "text": " as you can see exactly the same that's again we're kind of seeing that these things inside", "tokens": [382, 291, 393, 536, 2293, 264, 912, 300, 311, 797, 321, 434, 733, 295, 2577, 300, 613, 721, 1854], "temperature": 0.0, "avg_logprob": -0.18808928201364916, "compression_ratio": 1.6651162790697673, "no_speech_prob": 1.1365608543201233e-06}, {"id": 870, "seek": 622730, "start": 6247.9400000000005, "end": 6256.22, "text": " pie torch and fast AI are just little shortcuts for stuff we can write ourselves and nllloss", "tokens": [1730, 27822, 293, 2370, 7318, 366, 445, 707, 34620, 337, 1507, 321, 393, 2464, 4175, 293, 297, 285, 75, 772], "temperature": 0.0, "avg_logprob": -0.18808928201364916, "compression_ratio": 1.6651162790697673, "no_speech_prob": 1.1365608543201233e-06}, {"id": 871, "seek": 625622, "start": 6256.22, "end": 6262.18, "text": " stands for negative log likelihood again sounds complex but actually it's just this indexing", "tokens": [7382, 337, 3671, 3565, 22119, 797, 3263, 3997, 457, 767, 309, 311, 445, 341, 8186, 278], "temperature": 0.0, "avg_logprob": -0.11601762473583221, "compression_ratio": 1.5593220338983051, "no_speech_prob": 1.7330486343780649e-06}, {"id": 872, "seek": 625622, "start": 6262.18, "end": 6274.26, "text": " expression rather confusingly there's no log in it we'll see why in a moment so let's talk", "tokens": [6114, 2831, 13181, 356, 456, 311, 572, 3565, 294, 309, 321, 603, 536, 983, 294, 257, 1623, 370, 718, 311, 751], "temperature": 0.0, "avg_logprob": -0.11601762473583221, "compression_ratio": 1.5593220338983051, "no_speech_prob": 1.7330486343780649e-06}, {"id": 873, "seek": 625622, "start": 6274.26, "end": 6283.46, "text": " about logs so this locks this loss function works quite well as we as we saw in the notebook", "tokens": [466, 20820, 370, 341, 20703, 341, 4470, 2445, 1985, 1596, 731, 382, 321, 382, 321, 1866, 294, 264, 21060], "temperature": 0.0, "avg_logprob": -0.11601762473583221, "compression_ratio": 1.5593220338983051, "no_speech_prob": 1.7330486343780649e-06}, {"id": 874, "seek": 628346, "start": 6283.46, "end": 6288.54, "text": " 04 it's basically this it is exactly the same as we do in notebook 04 just a different way", "tokens": [50022, 309, 311, 1936, 341, 309, 307, 2293, 264, 912, 382, 321, 360, 294, 21060, 50022, 445, 257, 819, 636], "temperature": 0.0, "avg_logprob": -0.1377936399207925, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.338188083849673e-07}, {"id": 875, "seek": 628346, "start": 6288.54, "end": 6295.14, "text": " of expressing it but we can actually make it better because remember the probabilities", "tokens": [295, 22171, 309, 457, 321, 393, 767, 652, 309, 1101, 570, 1604, 264, 33783], "temperature": 0.0, "avg_logprob": -0.1377936399207925, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.338188083849673e-07}, {"id": 876, "seek": 628346, "start": 6295.14, "end": 6299.74, "text": " we're looking at are between naught and 1 so they can't be smaller than 0 they can't", "tokens": [321, 434, 1237, 412, 366, 1296, 13138, 293, 502, 370, 436, 393, 380, 312, 4356, 813, 1958, 436, 393, 380], "temperature": 0.0, "avg_logprob": -0.1377936399207925, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.338188083849673e-07}, {"id": 877, "seek": 628346, "start": 6299.74, "end": 6304.46, "text": " be greater than 1 which means that if our model is trying to decide whether to predict", "tokens": [312, 5044, 813, 502, 597, 1355, 300, 498, 527, 2316, 307, 1382, 281, 4536, 1968, 281, 6069], "temperature": 0.0, "avg_logprob": -0.1377936399207925, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.338188083849673e-07}, {"id": 878, "seek": 628346, "start": 6304.46, "end": 6311.5, "text": " 0.99 or 0.999 it's going to think that those numbers are very very close together but won't", "tokens": [1958, 13, 8494, 420, 1958, 13, 49017, 309, 311, 516, 281, 519, 300, 729, 3547, 366, 588, 588, 1998, 1214, 457, 1582, 380], "temperature": 0.0, "avg_logprob": -0.1377936399207925, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.338188083849673e-07}, {"id": 879, "seek": 631150, "start": 6311.5, "end": 6317.82, "text": " really care but actually if you think about the error you know if there's like a hundred", "tokens": [534, 1127, 457, 767, 498, 291, 519, 466, 264, 6713, 291, 458, 498, 456, 311, 411, 257, 3262], "temperature": 0.0, "avg_logprob": -0.11579882226339201, "compression_ratio": 1.8783068783068784, "no_speech_prob": 1.4593715604860336e-06}, {"id": 880, "seek": 631150, "start": 6317.82, "end": 6324.02, "text": " thing a thousand things then this would like be ten things are wrong and this would be", "tokens": [551, 257, 4714, 721, 550, 341, 576, 411, 312, 2064, 721, 366, 2085, 293, 341, 576, 312], "temperature": 0.0, "avg_logprob": -0.11579882226339201, "compression_ratio": 1.8783068783068784, "no_speech_prob": 1.4593715604860336e-06}, {"id": 881, "seek": 631150, "start": 6324.02, "end": 6331.46, "text": " like one thing is wrong but this is really like ten times better than this so really", "tokens": [411, 472, 551, 307, 2085, 457, 341, 307, 534, 411, 2064, 1413, 1101, 813, 341, 370, 534], "temperature": 0.0, "avg_logprob": -0.11579882226339201, "compression_ratio": 1.8783068783068784, "no_speech_prob": 1.4593715604860336e-06}, {"id": 882, "seek": 631150, "start": 6331.46, "end": 6337.46, "text": " what we'd like to do is to transform the numbers between 0 and 1 to instead between be between", "tokens": [437, 321, 1116, 411, 281, 360, 307, 281, 4088, 264, 3547, 1296, 1958, 293, 502, 281, 2602, 1296, 312, 1296], "temperature": 0.0, "avg_logprob": -0.11579882226339201, "compression_ratio": 1.8783068783068784, "no_speech_prob": 1.4593715604860336e-06}, {"id": 883, "seek": 633746, "start": 6337.46, "end": 6342.46, "text": " negative infinity and infinity and there's a function that does exactly that which is", "tokens": [3671, 13202, 293, 13202, 293, 456, 311, 257, 2445, 300, 775, 2293, 300, 597, 307], "temperature": 0.0, "avg_logprob": -0.10058381340720436, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.248268858944357e-06}, {"id": 884, "seek": 633746, "start": 6342.46, "end": 6354.66, "text": " called logarithm. Okay so as the so the numbers we could have can be between 0 and 1 and as", "tokens": [1219, 41473, 32674, 13, 1033, 370, 382, 264, 370, 264, 3547, 321, 727, 362, 393, 312, 1296, 1958, 293, 502, 293, 382], "temperature": 0.0, "avg_logprob": -0.10058381340720436, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.248268858944357e-06}, {"id": 885, "seek": 633746, "start": 6354.66, "end": 6365.26, "text": " we get closer and closer to 0 it goes down to infinity and then at 1 it's going to be", "tokens": [321, 483, 4966, 293, 4966, 281, 1958, 309, 1709, 760, 281, 13202, 293, 550, 412, 502, 309, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.10058381340720436, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.248268858944357e-06}, {"id": 886, "seek": 636526, "start": 6365.26, "end": 6378.54, "text": " 0 and we can't go above 0 because our loss function we want to be negative. So this logarithm", "tokens": [1958, 293, 321, 393, 380, 352, 3673, 1958, 570, 527, 4470, 2445, 321, 528, 281, 312, 3671, 13, 407, 341, 41473, 32674], "temperature": 0.0, "avg_logprob": -0.15396021373236357, "compression_ratio": 1.5480225988700564, "no_speech_prob": 1.436747083971568e-06}, {"id": 887, "seek": 636526, "start": 6378.54, "end": 6383.66, "text": " in case you forgot hopefully you vaguely remember what logarithm is from high school but that", "tokens": [294, 1389, 291, 5298, 4696, 291, 13501, 48863, 1604, 437, 41473, 32674, 307, 490, 1090, 1395, 457, 300], "temperature": 0.0, "avg_logprob": -0.15396021373236357, "compression_ratio": 1.5480225988700564, "no_speech_prob": 1.436747083971568e-06}, {"id": 888, "seek": 636526, "start": 6383.66, "end": 6389.9800000000005, "text": " basically the definition is is this if you have some number that is y that is b to the", "tokens": [1936, 264, 7123, 307, 307, 341, 498, 291, 362, 512, 1230, 300, 307, 288, 300, 307, 272, 281, 264], "temperature": 0.0, "avg_logprob": -0.15396021373236357, "compression_ratio": 1.5480225988700564, "no_speech_prob": 1.436747083971568e-06}, {"id": 889, "seek": 638998, "start": 6389.98, "end": 6399.299999999999, "text": " power of a then logarithm is defined such that a equals the logarithm of y comma b in", "tokens": [1347, 295, 257, 550, 41473, 32674, 307, 7642, 1270, 300, 257, 6915, 264, 41473, 32674, 295, 288, 22117, 272, 294], "temperature": 0.0, "avg_logprob": -0.0943465308537559, "compression_ratio": 1.6973684210526316, "no_speech_prob": 1.084514792637492e-06}, {"id": 890, "seek": 638998, "start": 6399.299999999999, "end": 6412.099999999999, "text": " other words it tells you b to the power of what equals y which is not that interesting", "tokens": [661, 2283, 309, 5112, 291, 272, 281, 264, 1347, 295, 437, 6915, 288, 597, 307, 406, 300, 1880], "temperature": 0.0, "avg_logprob": -0.0943465308537559, "compression_ratio": 1.6973684210526316, "no_speech_prob": 1.084514792637492e-06}, {"id": 891, "seek": 638998, "start": 6412.099999999999, "end": 6417.62, "text": " of itself but one of the really interesting things about logarithms is this very cool", "tokens": [295, 2564, 457, 472, 295, 264, 534, 1880, 721, 466, 41473, 355, 2592, 307, 341, 588, 1627], "temperature": 0.0, "avg_logprob": -0.0943465308537559, "compression_ratio": 1.6973684210526316, "no_speech_prob": 1.084514792637492e-06}, {"id": 892, "seek": 641762, "start": 6417.62, "end": 6425.099999999999, "text": " relationship which is that log of a times b equals log of a plus log of b and we use", "tokens": [2480, 597, 307, 300, 3565, 295, 257, 1413, 272, 6915, 3565, 295, 257, 1804, 3565, 295, 272, 293, 321, 764], "temperature": 0.0, "avg_logprob": -0.0712467025308048, "compression_ratio": 2.0171428571428573, "no_speech_prob": 1.5056988331707544e-06}, {"id": 893, "seek": 641762, "start": 6425.099999999999, "end": 6433.099999999999, "text": " that all the time in deep learning and machine learning because this number here a times", "tokens": [300, 439, 264, 565, 294, 2452, 2539, 293, 3479, 2539, 570, 341, 1230, 510, 257, 1413], "temperature": 0.0, "avg_logprob": -0.0712467025308048, "compression_ratio": 2.0171428571428573, "no_speech_prob": 1.5056988331707544e-06}, {"id": 894, "seek": 641762, "start": 6433.099999999999, "end": 6438.3, "text": " b can get very very big or very very small if you multiply things a lot of small things", "tokens": [272, 393, 483, 588, 588, 955, 420, 588, 588, 1359, 498, 291, 12972, 721, 257, 688, 295, 1359, 721], "temperature": 0.0, "avg_logprob": -0.0712467025308048, "compression_ratio": 2.0171428571428573, "no_speech_prob": 1.5056988331707544e-06}, {"id": 895, "seek": 641762, "start": 6438.3, "end": 6442.26, "text": " together you'll get a tiny number if you multiply a lot of big things together you'll get a", "tokens": [1214, 291, 603, 483, 257, 5870, 1230, 498, 291, 12972, 257, 688, 295, 955, 721, 1214, 291, 603, 483, 257], "temperature": 0.0, "avg_logprob": -0.0712467025308048, "compression_ratio": 2.0171428571428573, "no_speech_prob": 1.5056988331707544e-06}, {"id": 896, "seek": 644226, "start": 6442.26, "end": 6448.54, "text": " huge number it can get so big or so small that the kind of the precision in your computer's", "tokens": [2603, 1230, 309, 393, 483, 370, 955, 420, 370, 1359, 300, 264, 733, 295, 264, 18356, 294, 428, 3820, 311], "temperature": 0.0, "avg_logprob": -0.08581894145292394, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8266237589159573e-07}, {"id": 897, "seek": 644226, "start": 6448.54, "end": 6455.42, "text": " floating point gets really bad. Where else this thing here adding is not going to get", "tokens": [12607, 935, 2170, 534, 1578, 13, 2305, 1646, 341, 551, 510, 5127, 307, 406, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.08581894145292394, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8266237589159573e-07}, {"id": 898, "seek": 644226, "start": 6455.42, "end": 6462.6, "text": " out of control so we really love using logarithms like particularly in a deep neural net where", "tokens": [484, 295, 1969, 370, 321, 534, 959, 1228, 41473, 355, 2592, 411, 4098, 294, 257, 2452, 18161, 2533, 689], "temperature": 0.0, "avg_logprob": -0.08581894145292394, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8266237589159573e-07}, {"id": 899, "seek": 644226, "start": 6462.6, "end": 6467.76, "text": " there's lots of layers we're kind of multiplying and adding many times so this kind of tends", "tokens": [456, 311, 3195, 295, 7914, 321, 434, 733, 295, 30955, 293, 5127, 867, 1413, 370, 341, 733, 295, 12258], "temperature": 0.0, "avg_logprob": -0.08581894145292394, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8266237589159573e-07}, {"id": 900, "seek": 646776, "start": 6467.76, "end": 6485.22, "text": " to come out quite nicely. So when we take the probabilities that we saw before the things", "tokens": [281, 808, 484, 1596, 9594, 13, 407, 562, 321, 747, 264, 33783, 300, 321, 1866, 949, 264, 721], "temperature": 0.0, "avg_logprob": -0.0992740697638933, "compression_ratio": 1.5299145299145298, "no_speech_prob": 2.156809159714612e-06}, {"id": 901, "seek": 646776, "start": 6485.22, "end": 6496.14, "text": " that came out of this function and we take their logs and we take the mean that is called", "tokens": [300, 1361, 484, 295, 341, 2445, 293, 321, 747, 641, 20820, 293, 321, 747, 264, 914, 300, 307, 1219], "temperature": 0.0, "avg_logprob": -0.0992740697638933, "compression_ratio": 1.5299145299145298, "no_speech_prob": 2.156809159714612e-06}, {"id": 902, "seek": 649614, "start": 6496.14, "end": 6503.700000000001, "text": " negative log likelihood and so this ends up being kind of a really nicely behaved number", "tokens": [3671, 3565, 22119, 293, 370, 341, 5314, 493, 885, 733, 295, 257, 534, 9594, 48249, 1230], "temperature": 0.0, "avg_logprob": -0.09724596238905384, "compression_ratio": 1.5654761904761905, "no_speech_prob": 1.994719468711992e-06}, {"id": 903, "seek": 649614, "start": 6503.700000000001, "end": 6511.18, "text": " because of this property of the log that we described. So if you take the softmax and", "tokens": [570, 295, 341, 4707, 295, 264, 3565, 300, 321, 7619, 13, 407, 498, 291, 747, 264, 2787, 41167, 293], "temperature": 0.0, "avg_logprob": -0.09724596238905384, "compression_ratio": 1.5654761904761905, "no_speech_prob": 1.994719468711992e-06}, {"id": 904, "seek": 649614, "start": 6511.18, "end": 6517.58, "text": " then take the log and then pass that to an LL loss because remember that didn't actually", "tokens": [550, 747, 264, 3565, 293, 550, 1320, 300, 281, 364, 441, 43, 4470, 570, 1604, 300, 994, 380, 767], "temperature": 0.0, "avg_logprob": -0.09724596238905384, "compression_ratio": 1.5654761904761905, "no_speech_prob": 1.994719468711992e-06}, {"id": 905, "seek": 651758, "start": 6517.58, "end": 6526.78, "text": " take the log at all despite the name that gives you cross entropy loss. So that leaves", "tokens": [747, 264, 3565, 412, 439, 7228, 264, 1315, 300, 2709, 291, 3278, 30867, 4470, 13, 407, 300, 5510], "temperature": 0.0, "avg_logprob": -0.08428113907575607, "compression_ratio": 1.6645962732919255, "no_speech_prob": 6.083579364712932e-07}, {"id": 906, "seek": 651758, "start": 6526.78, "end": 6534.82, "text": " an obvious question of why doesn't an LL loss actually take the log and the reason for that", "tokens": [364, 6322, 1168, 295, 983, 1177, 380, 364, 441, 43, 4470, 767, 747, 264, 3565, 293, 264, 1778, 337, 300], "temperature": 0.0, "avg_logprob": -0.08428113907575607, "compression_ratio": 1.6645962732919255, "no_speech_prob": 6.083579364712932e-07}, {"id": 907, "seek": 651758, "start": 6534.82, "end": 6540.3, "text": " is that it's more convenient computationally to actually take the log back at the softmax", "tokens": [307, 300, 309, 311, 544, 10851, 24903, 379, 281, 767, 747, 264, 3565, 646, 412, 264, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.08428113907575607, "compression_ratio": 1.6645962732919255, "no_speech_prob": 6.083579364712932e-07}, {"id": 908, "seek": 654030, "start": 6540.3, "end": 6550.900000000001, "text": " step. So PyTorch has a function called log softmax and so since it's actually easier", "tokens": [1823, 13, 407, 9953, 51, 284, 339, 575, 257, 2445, 1219, 3565, 2787, 41167, 293, 370, 1670, 309, 311, 767, 3571], "temperature": 0.0, "avg_logprob": -0.14161633472053373, "compression_ratio": 1.7766497461928934, "no_speech_prob": 9.721530886963592e-07}, {"id": 909, "seek": 654030, "start": 6550.900000000001, "end": 6556.54, "text": " to do the log at the softmax stage it's just faster and more accurate. PyTorch assumes", "tokens": [281, 360, 264, 3565, 412, 264, 2787, 41167, 3233, 309, 311, 445, 4663, 293, 544, 8559, 13, 9953, 51, 284, 339, 37808], "temperature": 0.0, "avg_logprob": -0.14161633472053373, "compression_ratio": 1.7766497461928934, "no_speech_prob": 9.721530886963592e-07}, {"id": 910, "seek": 654030, "start": 6556.54, "end": 6564.28, "text": " that you use softlogmax and then pass that to NLL loss. So NLL loss does not do the log", "tokens": [300, 291, 764, 2787, 4987, 41167, 293, 550, 1320, 300, 281, 426, 24010, 4470, 13, 407, 426, 24010, 4470, 775, 406, 360, 264, 3565], "temperature": 0.0, "avg_logprob": -0.14161633472053373, "compression_ratio": 1.7766497461928934, "no_speech_prob": 9.721530886963592e-07}, {"id": 911, "seek": 654030, "start": 6564.28, "end": 6569.820000000001, "text": " it assumes that you've done the log beforehand. So log softmax followed by NLL loss is the", "tokens": [309, 37808, 300, 291, 600, 1096, 264, 3565, 22893, 13, 407, 3565, 2787, 41167, 6263, 538, 426, 24010, 4470, 307, 264], "temperature": 0.0, "avg_logprob": -0.14161633472053373, "compression_ratio": 1.7766497461928934, "no_speech_prob": 9.721530886963592e-07}, {"id": 912, "seek": 656982, "start": 6569.82, "end": 6576.86, "text": " definition of cross entropy loss in PyTorch. So that's our loss function and so you can", "tokens": [7123, 295, 3278, 30867, 4470, 294, 9953, 51, 284, 339, 13, 407, 300, 311, 527, 4470, 2445, 293, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.09187421693906679, "compression_ratio": 1.7355769230769231, "no_speech_prob": 1.2482690863180324e-06}, {"id": 913, "seek": 656982, "start": 6576.86, "end": 6582.62, "text": " pass that some activations and some targets and get back a number and pretty much everything", "tokens": [1320, 300, 512, 2430, 763, 293, 512, 12911, 293, 483, 646, 257, 1230, 293, 1238, 709, 1203], "temperature": 0.0, "avg_logprob": -0.09187421693906679, "compression_ratio": 1.7355769230769231, "no_speech_prob": 1.2482690863180324e-06}, {"id": 914, "seek": 656982, "start": 6582.62, "end": 6589.0199999999995, "text": " in in PyTorch every every one of these kinds of functions you can either use the NN version", "tokens": [294, 294, 9953, 51, 284, 339, 633, 633, 472, 295, 613, 3685, 295, 6828, 291, 393, 2139, 764, 264, 426, 45, 3037], "temperature": 0.0, "avg_logprob": -0.09187421693906679, "compression_ratio": 1.7355769230769231, "no_speech_prob": 1.2482690863180324e-06}, {"id": 915, "seek": 656982, "start": 6589.0199999999995, "end": 6595.0199999999995, "text": " as a class like this and then call that object as if it's a function or you can just use", "tokens": [382, 257, 1508, 411, 341, 293, 550, 818, 300, 2657, 382, 498, 309, 311, 257, 2445, 420, 291, 393, 445, 764], "temperature": 0.0, "avg_logprob": -0.09187421693906679, "compression_ratio": 1.7355769230769231, "no_speech_prob": 1.2482690863180324e-06}, {"id": 916, "seek": 659502, "start": 6595.02, "end": 6600.540000000001, "text": " f dot with the camel case name as a function directly and as you can see they're exactly", "tokens": [283, 5893, 365, 264, 37755, 1389, 1315, 382, 257, 2445, 3838, 293, 382, 291, 393, 536, 436, 434, 2293], "temperature": 0.0, "avg_logprob": -0.11441765708484869, "compression_ratio": 1.7960199004975124, "no_speech_prob": 1.4144727629172849e-06}, {"id": 917, "seek": 659502, "start": 6600.540000000001, "end": 6610.820000000001, "text": " the same number. People normally use the class version in the documentation in PyTorch you'll", "tokens": [264, 912, 1230, 13, 3432, 5646, 764, 264, 1508, 3037, 294, 264, 14333, 294, 9953, 51, 284, 339, 291, 603], "temperature": 0.0, "avg_logprob": -0.11441765708484869, "compression_ratio": 1.7960199004975124, "no_speech_prob": 1.4144727629172849e-06}, {"id": 918, "seek": 659502, "start": 6610.820000000001, "end": 6617.3, "text": " see it normally uses the class version so we'll tend to use the class version as well.", "tokens": [536, 309, 5646, 4960, 264, 1508, 3037, 370, 321, 603, 3928, 281, 764, 264, 1508, 3037, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11441765708484869, "compression_ratio": 1.7960199004975124, "no_speech_prob": 1.4144727629172849e-06}, {"id": 919, "seek": 659502, "start": 6617.3, "end": 6621.22, "text": " You'll see that it's returning a single number and that's because it takes the mean because", "tokens": [509, 603, 536, 300, 309, 311, 12678, 257, 2167, 1230, 293, 300, 311, 570, 309, 2516, 264, 914, 570], "temperature": 0.0, "avg_logprob": -0.11441765708484869, "compression_ratio": 1.7960199004975124, "no_speech_prob": 1.4144727629172849e-06}, {"id": 920, "seek": 662122, "start": 6621.22, "end": 6626.42, "text": " a loss needs to be as we've discussed the mean but if you want to see the underlying", "tokens": [257, 4470, 2203, 281, 312, 382, 321, 600, 7152, 264, 914, 457, 498, 291, 528, 281, 536, 264, 14217], "temperature": 0.0, "avg_logprob": -0.13975700625666865, "compression_ratio": 1.5906040268456376, "no_speech_prob": 6.276684416661737e-07}, {"id": 921, "seek": 662122, "start": 6626.42, "end": 6632.280000000001, "text": " numbers before taking the mean you can just pass in reduction equals none and that shows", "tokens": [3547, 949, 1940, 264, 914, 291, 393, 445, 1320, 294, 11004, 6915, 6022, 293, 300, 3110], "temperature": 0.0, "avg_logprob": -0.13975700625666865, "compression_ratio": 1.5906040268456376, "no_speech_prob": 6.276684416661737e-07}, {"id": 922, "seek": 662122, "start": 6632.280000000001, "end": 6642.14, "text": " you the individual cross entropy losses before taking the mean.", "tokens": [291, 264, 2609, 3278, 30867, 15352, 949, 1940, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.13975700625666865, "compression_ratio": 1.5906040268456376, "no_speech_prob": 6.276684416661737e-07}, {"id": 923, "seek": 664214, "start": 6642.14, "end": 6659.820000000001, "text": " Okay great so this is a good place to stop with our discussion of loss functions and", "tokens": [1033, 869, 370, 341, 307, 257, 665, 1081, 281, 1590, 365, 527, 5017, 295, 4470, 6828, 293], "temperature": 0.0, "avg_logprob": -0.13623269399007162, "compression_ratio": 1.105263157894737, "no_speech_prob": 4.812488327843312e-07}, {"id": 924, "seek": 665982, "start": 6659.82, "end": 6673.5, "text": " such things. Rachel were there any questions about this? Why does the loss function need", "tokens": [1270, 721, 13, 14246, 645, 456, 604, 1651, 466, 341, 30, 1545, 775, 264, 4470, 2445, 643], "temperature": 0.0, "avg_logprob": -0.14232109893452038, "compression_ratio": 1.3511450381679388, "no_speech_prob": 1.529390374344075e-06}, {"id": 925, "seek": 665982, "start": 6673.5, "end": 6685.099999999999, "text": " to be negative? Well I mean I guess it doesn't but it's we want something that the lower", "tokens": [281, 312, 3671, 30, 1042, 286, 914, 286, 2041, 309, 1177, 380, 457, 309, 311, 321, 528, 746, 300, 264, 3126], "temperature": 0.0, "avg_logprob": -0.14232109893452038, "compression_ratio": 1.3511450381679388, "no_speech_prob": 1.529390374344075e-06}, {"id": 926, "seek": 668510, "start": 6685.1, "end": 6696.22, "text": " it is the better and we kind of need it to cut off somewhere. I have to think about this", "tokens": [309, 307, 264, 1101, 293, 321, 733, 295, 643, 309, 281, 1723, 766, 4079, 13, 286, 362, 281, 519, 466, 341], "temperature": 0.0, "avg_logprob": -0.1693048671800263, "compression_ratio": 1.376923076923077, "no_speech_prob": 2.8129959446232533e-06}, {"id": 927, "seek": 668510, "start": 6696.22, "end": 6704.780000000001, "text": " more during the week because I'm it's a bit tired. Yeah so let me let me refresh my memory", "tokens": [544, 1830, 264, 1243, 570, 286, 478, 309, 311, 257, 857, 5868, 13, 865, 370, 718, 385, 718, 385, 15134, 452, 4675], "temperature": 0.0, "avg_logprob": -0.1693048671800263, "compression_ratio": 1.376923076923077, "no_speech_prob": 2.8129959446232533e-06}, {"id": 928, "seek": 670478, "start": 6704.78, "end": 6719.34, "text": " when I'm awake. Okay now next week well note not for the video next week actually happened", "tokens": [562, 286, 478, 15994, 13, 1033, 586, 958, 1243, 731, 3637, 406, 337, 264, 960, 958, 1243, 767, 2011], "temperature": 0.0, "avg_logprob": -0.11299744385939378, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.7330402215520735e-06}, {"id": 929, "seek": 670478, "start": 6719.34, "end": 6726.62, "text": " last week so the thing I'm about to say is actually referring to. So next week we're", "tokens": [1036, 1243, 370, 264, 551, 286, 478, 466, 281, 584, 307, 767, 13761, 281, 13, 407, 958, 1243, 321, 434], "temperature": 0.0, "avg_logprob": -0.11299744385939378, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.7330402215520735e-06}, {"id": 930, "seek": 670478, "start": 6726.62, "end": 6732.3, "text": " going to be talking about data ethics and I wanted to kind of segue into that by talking", "tokens": [516, 281, 312, 1417, 466, 1412, 19769, 293, 286, 1415, 281, 733, 295, 33850, 666, 300, 538, 1417], "temperature": 0.0, "avg_logprob": -0.11299744385939378, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.7330402215520735e-06}, {"id": 931, "seek": 673230, "start": 6732.3, "end": 6743.820000000001, "text": " about how my week's gone because a week or two ago in I did a as part of a lesson I actually", "tokens": [466, 577, 452, 1243, 311, 2780, 570, 257, 1243, 420, 732, 2057, 294, 286, 630, 257, 382, 644, 295, 257, 6898, 286, 767], "temperature": 0.0, "avg_logprob": -0.09134894325619652, "compression_ratio": 1.652694610778443, "no_speech_prob": 2.813002993207192e-06}, {"id": 932, "seek": 673230, "start": 6743.820000000001, "end": 6752.62, "text": " talked about the efficacy of masks and specifically wearing masks in public and I pointed out", "tokens": [2825, 466, 264, 33492, 295, 11830, 293, 4682, 4769, 11830, 294, 1908, 293, 286, 10932, 484], "temperature": 0.0, "avg_logprob": -0.09134894325619652, "compression_ratio": 1.652694610778443, "no_speech_prob": 2.813002993207192e-06}, {"id": 933, "seek": 673230, "start": 6752.62, "end": 6758.46, "text": " that the efficacy of masks seemed like it could be really high and maybe everybody should", "tokens": [300, 264, 33492, 295, 11830, 6576, 411, 309, 727, 312, 534, 1090, 293, 1310, 2201, 820], "temperature": 0.0, "avg_logprob": -0.09134894325619652, "compression_ratio": 1.652694610778443, "no_speech_prob": 2.813002993207192e-06}, {"id": 934, "seek": 675846, "start": 6758.46, "end": 6772.22, "text": " be wearing them and somehow I found myself as the face of a global advocacy campaign", "tokens": [312, 4769, 552, 293, 6063, 286, 1352, 2059, 382, 264, 1851, 295, 257, 4338, 22011, 5129], "temperature": 0.0, "avg_logprob": -0.1256173610687256, "compression_ratio": 1.105263157894737, "no_speech_prob": 2.190766508647357e-06}, {"id": 935, "seek": 677222, "start": 6772.22, "end": 6789.58, "text": " and so if you go to masksforall.co you will find a website talking about masks and I've", "tokens": [293, 370, 498, 291, 352, 281, 11830, 2994, 336, 13, 1291, 291, 486, 915, 257, 3144, 1417, 466, 11830, 293, 286, 600], "temperature": 0.0, "avg_logprob": -0.08335514172263768, "compression_ratio": 1.3206106870229009, "no_speech_prob": 3.18750517180888e-06}, {"id": 936, "seek": 677222, "start": 6789.58, "end": 6796.9800000000005, "text": " been on you know TV shows in South Africa and the US and England and Australia and on", "tokens": [668, 322, 291, 458, 3558, 3110, 294, 4242, 7349, 293, 264, 2546, 293, 8196, 293, 7060, 293, 322], "temperature": 0.0, "avg_logprob": -0.08335514172263768, "compression_ratio": 1.3206106870229009, "no_speech_prob": 3.18750517180888e-06}, {"id": 937, "seek": 679698, "start": 6796.98, "end": 6808.86, "text": " radio and blah blah blah talking about masks. Why is this? Well it's because as a data scientist", "tokens": [6477, 293, 12288, 12288, 12288, 1417, 466, 11830, 13, 1545, 307, 341, 30, 1042, 309, 311, 570, 382, 257, 1412, 12662], "temperature": 0.0, "avg_logprob": -0.08567109902699789, "compression_ratio": 1.6306818181818181, "no_speech_prob": 8.397672900173347e-06}, {"id": 938, "seek": 679698, "start": 6808.86, "end": 6816.58, "text": " you know I noticed that the data around masks seem to be getting misunderstood and it seemed", "tokens": [291, 458, 286, 5694, 300, 264, 1412, 926, 11830, 1643, 281, 312, 1242, 33870, 293, 309, 6576], "temperature": 0.0, "avg_logprob": -0.08567109902699789, "compression_ratio": 1.6306818181818181, "no_speech_prob": 8.397672900173347e-06}, {"id": 939, "seek": 679698, "start": 6816.58, "end": 6823.7, "text": " that that misunderstanding was costing possibly hundreds of thousands of lives you know literally", "tokens": [300, 300, 29227, 390, 37917, 6264, 6779, 295, 5383, 295, 2909, 291, 458, 3736], "temperature": 0.0, "avg_logprob": -0.08567109902699789, "compression_ratio": 1.6306818181818181, "no_speech_prob": 8.397672900173347e-06}, {"id": 940, "seek": 682370, "start": 6823.7, "end": 6829.74, "text": " in the places that were using masks it seemed to be associated with you know orders of magnitude", "tokens": [294, 264, 3190, 300, 645, 1228, 11830, 309, 6576, 281, 312, 6615, 365, 291, 458, 9470, 295, 15668], "temperature": 0.0, "avg_logprob": -0.07415584676405963, "compression_ratio": 1.7370892018779343, "no_speech_prob": 6.540145022881916e-06}, {"id": 941, "seek": 682370, "start": 6829.74, "end": 6836.5, "text": " fewer deaths and one of the things we'll talk about next week is like you know what's your", "tokens": [13366, 13027, 293, 472, 295, 264, 721, 321, 603, 751, 466, 958, 1243, 307, 411, 291, 458, 437, 311, 428], "temperature": 0.0, "avg_logprob": -0.07415584676405963, "compression_ratio": 1.7370892018779343, "no_speech_prob": 6.540145022881916e-06}, {"id": 942, "seek": 682370, "start": 6836.5, "end": 6843.94, "text": " role as a data scientist and you know I strongly believe that it's to understand the data and", "tokens": [3090, 382, 257, 1412, 12662, 293, 291, 458, 286, 10613, 1697, 300, 309, 311, 281, 1223, 264, 1412, 293], "temperature": 0.0, "avg_logprob": -0.07415584676405963, "compression_ratio": 1.7370892018779343, "no_speech_prob": 6.540145022881916e-06}, {"id": 943, "seek": 682370, "start": 6843.94, "end": 6852.34, "text": " then do something about it and so nobody was talking about this so I ended up writing an", "tokens": [550, 360, 746, 466, 309, 293, 370, 5079, 390, 1417, 466, 341, 370, 286, 4590, 493, 3579, 364], "temperature": 0.0, "avg_logprob": -0.07415584676405963, "compression_ratio": 1.7370892018779343, "no_speech_prob": 6.540145022881916e-06}, {"id": 944, "seek": 685234, "start": 6852.34, "end": 6858.82, "text": " article that appeared in the Washington Post that basically called on people to really", "tokens": [7222, 300, 8516, 294, 264, 6149, 10223, 300, 1936, 1219, 322, 561, 281, 534], "temperature": 0.0, "avg_logprob": -0.14462924825734105, "compression_ratio": 1.6, "no_speech_prob": 1.6963769667199813e-05}, {"id": 945, "seek": 685234, "start": 6858.82, "end": 6872.7, "text": " consider wearing masks which is this article and you know I was I was lucky I managed to", "tokens": [1949, 4769, 11830, 597, 307, 341, 7222, 293, 291, 458, 286, 390, 286, 390, 6356, 286, 6453, 281], "temperature": 0.0, "avg_logprob": -0.14462924825734105, "compression_ratio": 1.6, "no_speech_prob": 1.6963769667199813e-05}, {"id": 946, "seek": 685234, "start": 6872.7, "end": 6879.2, "text": " kind of get a huge team of brilliant not huge a pretty decent sized team of brilliant volunteers", "tokens": [733, 295, 483, 257, 2603, 1469, 295, 10248, 406, 2603, 257, 1238, 8681, 20004, 1469, 295, 10248, 14352], "temperature": 0.0, "avg_logprob": -0.14462924825734105, "compression_ratio": 1.6, "no_speech_prob": 1.6963769667199813e-05}, {"id": 947, "seek": 687920, "start": 6879.2, "end": 6885.139999999999, "text": " who helped you know kind of build this website and kind of some PR folks and stuff like that", "tokens": [567, 4254, 291, 458, 733, 295, 1322, 341, 3144, 293, 733, 295, 512, 11568, 4024, 293, 1507, 411, 300], "temperature": 0.0, "avg_logprob": -0.1773817879813058, "compression_ratio": 1.6342592592592593, "no_speech_prob": 4.157217063038843e-06}, {"id": 948, "seek": 687920, "start": 6885.139999999999, "end": 6894.74, "text": " but what these came clear was and I was talking to politicians you know and it is buffers", "tokens": [457, 437, 613, 1361, 1850, 390, 293, 286, 390, 1417, 281, 14756, 291, 458, 293, 309, 307, 9204, 433], "temperature": 0.0, "avg_logprob": -0.1773817879813058, "compression_ratio": 1.6342592592592593, "no_speech_prob": 4.157217063038843e-06}, {"id": 949, "seek": 687920, "start": 6894.74, "end": 6900.82, "text": " and what was becoming clear is that people weren't convinced by the science which is", "tokens": [293, 437, 390, 5617, 1850, 307, 300, 561, 4999, 380, 12561, 538, 264, 3497, 597, 307], "temperature": 0.0, "avg_logprob": -0.1773817879813058, "compression_ratio": 1.6342592592592593, "no_speech_prob": 4.157217063038843e-06}, {"id": 950, "seek": 687920, "start": 6900.82, "end": 6907.7, "text": " fair enough because it's it's hard to you know when the WHO and the CDC is saying you", "tokens": [3143, 1547, 570, 309, 311, 309, 311, 1152, 281, 291, 458, 562, 264, 23256, 293, 264, 17133, 307, 1566, 291], "temperature": 0.0, "avg_logprob": -0.1773817879813058, "compression_ratio": 1.6342592592592593, "no_speech_prob": 4.157217063038843e-06}, {"id": 951, "seek": 690770, "start": 6907.7, "end": 6912.38, "text": " don't need to wear a mask and some random data scientist is saying that doesn't seem", "tokens": [500, 380, 643, 281, 3728, 257, 6094, 293, 512, 4974, 1412, 12662, 307, 1566, 300, 1177, 380, 1643], "temperature": 0.0, "avg_logprob": -0.09511164382652, "compression_ratio": 1.7620967741935485, "no_speech_prob": 3.0894611882104073e-06}, {"id": 952, "seek": 690770, "start": 6912.38, "end": 6916.94, "text": " to be what the data is showing you know you've got half a brain you would pick them WHO and", "tokens": [281, 312, 437, 264, 1412, 307, 4099, 291, 458, 291, 600, 658, 1922, 257, 3567, 291, 576, 1888, 552, 23256, 293], "temperature": 0.0, "avg_logprob": -0.09511164382652, "compression_ratio": 1.7620967741935485, "no_speech_prob": 3.0894611882104073e-06}, {"id": 953, "seek": 690770, "start": 6916.94, "end": 6922.46, "text": " the CDC not the random data scientist so I really felt like I if I was going to be an", "tokens": [264, 17133, 406, 264, 4974, 1412, 12662, 370, 286, 534, 2762, 411, 286, 498, 286, 390, 516, 281, 312, 364], "temperature": 0.0, "avg_logprob": -0.09511164382652, "compression_ratio": 1.7620967741935485, "no_speech_prob": 3.0894611882104073e-06}, {"id": 954, "seek": 690770, "start": 6922.46, "end": 6930.78, "text": " effective advocate I needed sort the science out and it you know credentialism is strong", "tokens": [4942, 14608, 286, 2978, 1333, 264, 3497, 484, 293, 309, 291, 458, 22034, 1434, 307, 2068], "temperature": 0.0, "avg_logprob": -0.09511164382652, "compression_ratio": 1.7620967741935485, "no_speech_prob": 3.0894611882104073e-06}, {"id": 955, "seek": 690770, "start": 6930.78, "end": 6933.98, "text": " and so it wouldn't be enough for me to say it I needed to find other people to say it", "tokens": [293, 370, 309, 2759, 380, 312, 1547, 337, 385, 281, 584, 309, 286, 2978, 281, 915, 661, 561, 281, 584, 309], "temperature": 0.0, "avg_logprob": -0.09511164382652, "compression_ratio": 1.7620967741935485, "no_speech_prob": 3.0894611882104073e-06}, {"id": 956, "seek": 693398, "start": 6933.98, "end": 6944.839999999999, "text": " so I put together a team of 19 scientists including you know a professor of sociology", "tokens": [370, 286, 829, 1214, 257, 1469, 295, 1294, 7708, 3009, 291, 458, 257, 8304, 295, 41744], "temperature": 0.0, "avg_logprob": -0.1788626057761056, "compression_ratio": 1.5562130177514792, "no_speech_prob": 7.766610906401183e-06}, {"id": 957, "seek": 693398, "start": 6944.839999999999, "end": 6951.98, "text": " a professor of aerosol dynamics the founder of an African movement that's that kind of", "tokens": [257, 8304, 295, 11207, 329, 401, 15679, 264, 14917, 295, 364, 7312, 3963, 300, 311, 300, 733, 295], "temperature": 0.0, "avg_logprob": -0.1788626057761056, "compression_ratio": 1.5562130177514792, "no_speech_prob": 7.766610906401183e-06}, {"id": 958, "seek": 693398, "start": 6951.98, "end": 6962.099999999999, "text": " studied preventative methods for methods for tuberculosis a Stanford professor who studies", "tokens": [9454, 4871, 1166, 7150, 337, 7150, 337, 39847, 49379, 257, 20374, 8304, 567, 5313], "temperature": 0.0, "avg_logprob": -0.1788626057761056, "compression_ratio": 1.5562130177514792, "no_speech_prob": 7.766610906401183e-06}, {"id": 959, "seek": 696210, "start": 6962.1, "end": 6969.900000000001, "text": " mask disposal and cleaning methods a bunch of Chinese scientists who study epidemiology", "tokens": [6094, 26400, 293, 8924, 7150, 257, 3840, 295, 4649, 7708, 567, 2979, 35761, 1793], "temperature": 0.0, "avg_logprob": -0.12336108559056332, "compression_ratio": 1.4943820224719102, "no_speech_prob": 8.397832061746158e-06}, {"id": 960, "seek": 696210, "start": 6969.900000000001, "end": 6982.58, "text": " modeling a UCLA professor who is one of the top infectious disease epidemiologists experts", "tokens": [15983, 257, 42862, 8304, 567, 307, 472, 295, 264, 1192, 26780, 4752, 35761, 12256, 8572], "temperature": 0.0, "avg_logprob": -0.12336108559056332, "compression_ratio": 1.4943820224719102, "no_speech_prob": 8.397832061746158e-06}, {"id": 961, "seek": 696210, "start": 6982.58, "end": 6988.22, "text": " and so forth so like this kind of all-star team of people from all around the world and", "tokens": [293, 370, 5220, 370, 411, 341, 733, 295, 439, 12, 9710, 1469, 295, 561, 490, 439, 926, 264, 1002, 293], "temperature": 0.0, "avg_logprob": -0.12336108559056332, "compression_ratio": 1.4943820224719102, "no_speech_prob": 8.397832061746158e-06}, {"id": 962, "seek": 698822, "start": 6988.22, "end": 6992.5, "text": " I had never met any of these people before so well no not quite true I knew Austin a", "tokens": [286, 632, 1128, 1131, 604, 295, 613, 561, 949, 370, 731, 572, 406, 1596, 2074, 286, 2586, 15356, 257], "temperature": 0.0, "avg_logprob": -0.12241290165827824, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.092843937542057e-06}, {"id": 963, "seek": 698822, "start": 6992.5, "end": 7000.34, "text": " little bit and I knew Zainab a little bit I knew Lex a little bit but on the whole you", "tokens": [707, 857, 293, 286, 2586, 1176, 491, 455, 257, 707, 857, 286, 2586, 24086, 257, 707, 857, 457, 322, 264, 1379, 291], "temperature": 0.0, "avg_logprob": -0.12241290165827824, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.092843937542057e-06}, {"id": 964, "seek": 698822, "start": 7000.34, "end": 7006.46, "text": " know and well Reshma we all know she's awesome so it's great to actually have a fast AI community", "tokens": [458, 293, 731, 5015, 22061, 321, 439, 458, 750, 311, 3476, 370, 309, 311, 869, 281, 767, 362, 257, 2370, 7318, 1768], "temperature": 0.0, "avg_logprob": -0.12241290165827824, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.092843937542057e-06}, {"id": 965, "seek": 698822, "start": 7006.46, "end": 7015.18, "text": " person there too and so but yeah I kind of tried to pull together people from you know", "tokens": [954, 456, 886, 293, 370, 457, 1338, 286, 733, 295, 3031, 281, 2235, 1214, 561, 490, 291, 458], "temperature": 0.0, "avg_logprob": -0.12241290165827824, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.092843937542057e-06}, {"id": 966, "seek": 701518, "start": 7015.18, "end": 7021.58, "text": " as many geographies as possible and as many areas of expertise as possible and you know", "tokens": [382, 867, 25435, 530, 382, 1944, 293, 382, 867, 3179, 295, 11769, 382, 1944, 293, 291, 458], "temperature": 0.0, "avg_logprob": -0.07561637737132886, "compression_ratio": 1.685897435897436, "no_speech_prob": 1.7603219930606429e-06}, {"id": 967, "seek": 701518, "start": 7021.58, "end": 7030.740000000001, "text": " the kind of the global community helped me find papers about about everything about you", "tokens": [264, 733, 295, 264, 4338, 1768, 4254, 385, 915, 10577, 466, 466, 1203, 466, 291], "temperature": 0.0, "avg_logprob": -0.07561637737132886, "compression_ratio": 1.685897435897436, "no_speech_prob": 1.7603219930606429e-06}, {"id": 968, "seek": 701518, "start": 7030.740000000001, "end": 7040.34, "text": " know how different materials work about how droplets form about epidemiology about case", "tokens": [458, 577, 819, 5319, 589, 466, 577, 41573, 1254, 466, 35761, 1793, 466, 1389], "temperature": 0.0, "avg_logprob": -0.07561637737132886, "compression_ratio": 1.685897435897436, "no_speech_prob": 1.7603219930606429e-06}, {"id": 969, "seek": 704034, "start": 7040.34, "end": 7047.26, "text": " studies of people infecting with and without masks blah blah blah and we ended up in the", "tokens": [5313, 295, 561, 5888, 278, 365, 293, 1553, 11830, 12288, 12288, 12288, 293, 321, 4590, 493, 294, 264], "temperature": 0.0, "avg_logprob": -0.07363537274874174, "compression_ratio": 1.592814371257485, "no_speech_prob": 6.540259164466988e-06}, {"id": 970, "seek": 704034, "start": 7047.26, "end": 7059.34, "text": " last week basically we wrote this paper it contains 84 citations and you know we basically", "tokens": [1036, 1243, 1936, 321, 4114, 341, 3035, 309, 8306, 29018, 4814, 763, 293, 291, 458, 321, 1936], "temperature": 0.0, "avg_logprob": -0.07363537274874174, "compression_ratio": 1.592814371257485, "no_speech_prob": 6.540259164466988e-06}, {"id": 971, "seek": 704034, "start": 7059.34, "end": 7068.5, "text": " worked around the clock on it as a team and it's out and it's been sent to a number of", "tokens": [2732, 926, 264, 7830, 322, 309, 382, 257, 1469, 293, 309, 311, 484, 293, 309, 311, 668, 2279, 281, 257, 1230, 295], "temperature": 0.0, "avg_logprob": -0.07363537274874174, "compression_ratio": 1.592814371257485, "no_speech_prob": 6.540259164466988e-06}, {"id": 972, "seek": 706850, "start": 7068.5, "end": 7074.1, "text": " some of the earlier versions three or four days ago we sent us some governments so one", "tokens": [512, 295, 264, 3071, 9606, 1045, 420, 1451, 1708, 2057, 321, 2279, 505, 512, 11280, 370, 472], "temperature": 0.0, "avg_logprob": -0.10293204609940691, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.922036062751431e-05}, {"id": 973, "seek": 706850, "start": 7074.1, "end": 7079.22, "text": " of the things is I in this team I try to look for people who were you know working closely", "tokens": [295, 264, 721, 307, 286, 294, 341, 1469, 286, 853, 281, 574, 337, 561, 567, 645, 291, 458, 1364, 8185], "temperature": 0.0, "avg_logprob": -0.10293204609940691, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.922036062751431e-05}, {"id": 974, "seek": 706850, "start": 7079.22, "end": 7083.78, "text": " with government leaders not just that they're scientists and so this this went out to a", "tokens": [365, 2463, 3523, 406, 445, 300, 436, 434, 7708, 293, 370, 341, 341, 1437, 484, 281, 257], "temperature": 0.0, "avg_logprob": -0.10293204609940691, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.922036062751431e-05}, {"id": 975, "seek": 706850, "start": 7083.78, "end": 7091.86, "text": " number of government ministers and in the last few days I've heard that it was a very", "tokens": [1230, 295, 2463, 26220, 293, 294, 264, 1036, 1326, 1708, 286, 600, 2198, 300, 309, 390, 257, 588], "temperature": 0.0, "avg_logprob": -0.10293204609940691, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.922036062751431e-05}, {"id": 976, "seek": 709186, "start": 7091.86, "end": 7100.7, "text": " significant part of decisions by governments to change their to change their guidelines", "tokens": [4776, 644, 295, 5327, 538, 11280, 281, 1319, 641, 281, 1319, 641, 12470], "temperature": 0.0, "avg_logprob": -0.07238902467669862, "compression_ratio": 1.542857142857143, "no_speech_prob": 2.260281689814292e-06}, {"id": 977, "seek": 709186, "start": 7100.7, "end": 7108.42, "text": " around masks and you know the fights not over by any means in particular the UK is a bit", "tokens": [926, 11830, 293, 291, 458, 264, 14512, 406, 670, 538, 604, 1355, 294, 1729, 264, 7051, 307, 257, 857], "temperature": 0.0, "avg_logprob": -0.07238902467669862, "compression_ratio": 1.542857142857143, "no_speech_prob": 2.260281689814292e-06}, {"id": 978, "seek": 709186, "start": 7108.42, "end": 7118.219999999999, "text": " of a holdout but I'm going to be on ITV tomorrow and then BBC the next day you know it's it's", "tokens": [295, 257, 1797, 346, 457, 286, 478, 516, 281, 312, 322, 6783, 53, 4153, 293, 550, 22669, 264, 958, 786, 291, 458, 309, 311, 309, 311], "temperature": 0.0, "avg_logprob": -0.07238902467669862, "compression_ratio": 1.542857142857143, "no_speech_prob": 2.260281689814292e-06}, {"id": 979, "seek": 711822, "start": 7118.22, "end": 7122.9800000000005, "text": " kind of required stepping out to be a lot more than just a data scientist I've had to", "tokens": [733, 295, 4739, 16821, 484, 281, 312, 257, 688, 544, 813, 445, 257, 1412, 12662, 286, 600, 632, 281], "temperature": 0.0, "avg_logprob": -0.10038439127115104, "compression_ratio": 1.8846153846153846, "no_speech_prob": 7.527291927544866e-06}, {"id": 980, "seek": 711822, "start": 7122.9800000000005, "end": 7130.860000000001, "text": " pull together you know politicians and staffers I've had to you know you know hustle with", "tokens": [2235, 1214, 291, 458, 14756, 293, 3525, 433, 286, 600, 632, 281, 291, 458, 291, 458, 34639, 365], "temperature": 0.0, "avg_logprob": -0.10038439127115104, "compression_ratio": 1.8846153846153846, "no_speech_prob": 7.527291927544866e-06}, {"id": 981, "seek": 711822, "start": 7130.860000000001, "end": 7136.1, "text": " the media to try and get you know coverage and you know today I'm now starting to do", "tokens": [264, 3021, 281, 853, 293, 483, 291, 458, 9645, 293, 291, 458, 965, 286, 478, 586, 2891, 281, 360], "temperature": 0.0, "avg_logprob": -0.10038439127115104, "compression_ratio": 1.8846153846153846, "no_speech_prob": 7.527291927544866e-06}, {"id": 982, "seek": 711822, "start": 7136.1, "end": 7139.820000000001, "text": " a lot of work with unions to try to get unions to understand this you know it's really a", "tokens": [257, 688, 295, 589, 365, 24914, 281, 853, 281, 483, 24914, 281, 1223, 341, 291, 458, 309, 311, 534, 257], "temperature": 0.0, "avg_logprob": -0.10038439127115104, "compression_ratio": 1.8846153846153846, "no_speech_prob": 7.527291927544866e-06}, {"id": 983, "seek": 711822, "start": 7139.820000000001, "end": 7147.3, "text": " case of like saying okay as a data scientist and income in conjunction with real scientists", "tokens": [1389, 295, 411, 1566, 1392, 382, 257, 1412, 12662, 293, 5742, 294, 27482, 365, 957, 7708], "temperature": 0.0, "avg_logprob": -0.10038439127115104, "compression_ratio": 1.8846153846153846, "no_speech_prob": 7.527291927544866e-06}, {"id": 984, "seek": 714730, "start": 7147.3, "end": 7155.5, "text": " we've we've built this really strong understanding that masks you know are this simple but incredibly", "tokens": [321, 600, 321, 600, 3094, 341, 534, 2068, 3701, 300, 11830, 291, 458, 366, 341, 2199, 457, 6252], "temperature": 0.0, "avg_logprob": -0.0717157781124115, "compression_ratio": 1.668141592920354, "no_speech_prob": 2.684127593965968e-06}, {"id": 985, "seek": 714730, "start": 7155.5, "end": 7161.5, "text": " powerful tool that doesn't do anything unless I can effectively communicate this to decision", "tokens": [4005, 2290, 300, 1177, 380, 360, 1340, 5969, 286, 393, 8659, 7890, 341, 281, 3537], "temperature": 0.0, "avg_logprob": -0.0717157781124115, "compression_ratio": 1.668141592920354, "no_speech_prob": 2.684127593965968e-06}, {"id": 986, "seek": 714730, "start": 7161.5, "end": 7168.5, "text": " makers so today I was you know on the phone to you know one of the top union leaders in", "tokens": [19323, 370, 965, 286, 390, 291, 458, 322, 264, 2593, 281, 291, 458, 472, 295, 264, 1192, 11671, 3523, 294], "temperature": 0.0, "avg_logprob": -0.0717157781124115, "compression_ratio": 1.668141592920354, "no_speech_prob": 2.684127593965968e-06}, {"id": 987, "seek": 714730, "start": 7168.5, "end": 7176.22, "text": " the country explaining what this means basically it turns out that in buses in America they're", "tokens": [264, 1941, 13468, 437, 341, 1355, 1936, 309, 4523, 484, 300, 294, 20519, 294, 3374, 436, 434], "temperature": 0.0, "avg_logprob": -0.0717157781124115, "compression_ratio": 1.668141592920354, "no_speech_prob": 2.684127593965968e-06}, {"id": 988, "seek": 717622, "start": 7176.22, "end": 7180.26, "text": " kind of the air conditioning is set up so that it blows from the back to the front and", "tokens": [733, 295, 264, 1988, 21901, 307, 992, 493, 370, 300, 309, 18458, 490, 264, 646, 281, 264, 1868, 293], "temperature": 0.0, "avg_logprob": -0.15809994477492112, "compression_ratio": 1.734375, "no_speech_prob": 4.092772996955318e-06}, {"id": 989, "seek": 717622, "start": 7180.26, "end": 7186.3, "text": " there's actually case studies in the medical literature of how people that are seated and", "tokens": [456, 311, 767, 1389, 5313, 294, 264, 4625, 10394, 295, 577, 561, 300, 366, 20959, 293], "temperature": 0.0, "avg_logprob": -0.15809994477492112, "compression_ratio": 1.734375, "no_speech_prob": 4.092772996955318e-06}, {"id": 990, "seek": 717622, "start": 7186.3, "end": 7192.1, "text": " have downwind of an air conditioning unit in a restaurant ended up all getting sick", "tokens": [362, 760, 12199, 295, 364, 1988, 21901, 4985, 294, 257, 6383, 4590, 493, 439, 1242, 4998], "temperature": 0.0, "avg_logprob": -0.15809994477492112, "compression_ratio": 1.734375, "no_speech_prob": 4.092772996955318e-06}, {"id": 991, "seek": 717622, "start": 7192.1, "end": 7200.62, "text": " with kovat 19 and so we can see why like bus drivers are dying because they're like they're", "tokens": [365, 350, 5179, 267, 1294, 293, 370, 321, 393, 536, 983, 411, 1255, 11590, 366, 8639, 570, 436, 434, 411, 436, 434], "temperature": 0.0, "avg_logprob": -0.15809994477492112, "compression_ratio": 1.734375, "no_speech_prob": 4.092772996955318e-06}, {"id": 992, "seek": 717622, "start": 7200.62, "end": 7205.02, "text": " right in the wrong spot here and their passengers aren't wearing masks so I kind of want to", "tokens": [558, 294, 264, 2085, 4008, 510, 293, 641, 18436, 3212, 380, 4769, 11830, 370, 286, 733, 295, 528, 281], "temperature": 0.0, "avg_logprob": -0.15809994477492112, "compression_ratio": 1.734375, "no_speech_prob": 4.092772996955318e-06}, {"id": 993, "seek": 720502, "start": 7205.02, "end": 7214.14, "text": " explain this science to union leaders so that they understand that to keep the workers safe", "tokens": [2903, 341, 3497, 281, 11671, 3523, 370, 300, 436, 1223, 300, 281, 1066, 264, 5600, 3273], "temperature": 0.0, "avg_logprob": -0.11375405639410019, "compression_ratio": 1.6144578313253013, "no_speech_prob": 2.406079602224054e-06}, {"id": 994, "seek": 720502, "start": 7214.14, "end": 7218.820000000001, "text": " it's not enough just for the driver to wear a mask but all the people on the bus needed", "tokens": [309, 311, 406, 1547, 445, 337, 264, 6787, 281, 3728, 257, 6094, 457, 439, 264, 561, 322, 264, 1255, 2978], "temperature": 0.0, "avg_logprob": -0.11375405639410019, "compression_ratio": 1.6144578313253013, "no_speech_prob": 2.406079602224054e-06}, {"id": 995, "seek": 720502, "start": 7218.820000000001, "end": 7228.900000000001, "text": " to be wearing masks as well so you know all of this is basically to say you know as data", "tokens": [281, 312, 4769, 11830, 382, 731, 370, 291, 458, 439, 295, 341, 307, 1936, 281, 584, 291, 458, 382, 1412], "temperature": 0.0, "avg_logprob": -0.11375405639410019, "compression_ratio": 1.6144578313253013, "no_speech_prob": 2.406079602224054e-06}, {"id": 996, "seek": 722890, "start": 7228.9, "end": 7235.62, "text": " scientists I think we have a responsibility to the study the data and then do something", "tokens": [7708, 286, 519, 321, 362, 257, 6357, 281, 264, 2979, 264, 1412, 293, 550, 360, 746], "temperature": 0.0, "avg_logprob": -0.10487731041446809, "compression_ratio": 1.6358024691358024, "no_speech_prob": 1.9947178770962637e-06}, {"id": 997, "seek": 722890, "start": 7235.62, "end": 7242.7, "text": " about it it's not just a research you know exercise it's not just a computation exercise", "tokens": [466, 309, 309, 311, 406, 445, 257, 2132, 291, 458, 5380, 309, 311, 406, 445, 257, 24903, 5380], "temperature": 0.0, "avg_logprob": -0.10487731041446809, "compression_ratio": 1.6358024691358024, "no_speech_prob": 1.9947178770962637e-06}, {"id": 998, "seek": 722890, "start": 7242.7, "end": 7254.46, "text": " you know what what's the point of doing things if it doesn't lead to anything so yeah so", "tokens": [291, 458, 437, 437, 311, 264, 935, 295, 884, 721, 498, 309, 1177, 380, 1477, 281, 1340, 370, 1338, 370], "temperature": 0.0, "avg_logprob": -0.10487731041446809, "compression_ratio": 1.6358024691358024, "no_speech_prob": 1.9947178770962637e-06}, {"id": 999, "seek": 725446, "start": 7254.46, "end": 7259.46, "text": " next week we'll be talking about this a lot more but I think you know this is a really", "tokens": [958, 1243, 321, 603, 312, 1417, 466, 341, 257, 688, 544, 457, 286, 519, 291, 458, 341, 307, 257, 534], "temperature": 0.0, "avg_logprob": -0.08185271518986399, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.7264459276921116e-06}, {"id": 1000, "seek": 725446, "start": 7259.46, "end": 7269.22, "text": " to me kind of interesting example of how digging into the data can lead to really amazing things", "tokens": [281, 385, 733, 295, 1880, 1365, 295, 577, 17343, 666, 264, 1412, 393, 1477, 281, 534, 2243, 721], "temperature": 0.0, "avg_logprob": -0.08185271518986399, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.7264459276921116e-06}, {"id": 1001, "seek": 725446, "start": 7269.22, "end": 7274.54, "text": " happening and and in this case I strongly believe and a lot of people are telling me", "tokens": [2737, 293, 293, 294, 341, 1389, 286, 10613, 1697, 293, 257, 688, 295, 561, 366, 3585, 385], "temperature": 0.0, "avg_logprob": -0.08185271518986399, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.7264459276921116e-06}, {"id": 1002, "seek": 725446, "start": 7274.54, "end": 7280.3, "text": " they strongly believe that this kind of advocacy work that's come out of this data analysis", "tokens": [436, 10613, 1697, 300, 341, 733, 295, 22011, 589, 300, 311, 808, 484, 295, 341, 1412, 5215], "temperature": 0.0, "avg_logprob": -0.08185271518986399, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.7264459276921116e-06}, {"id": 1003, "seek": 728030, "start": 7280.3, "end": 7287.820000000001, "text": " is is already saving lives and so I hope this might help inspire you to to take your data", "tokens": [307, 307, 1217, 6816, 2909, 293, 370, 286, 1454, 341, 1062, 854, 15638, 291, 281, 281, 747, 428, 1412], "temperature": 0.0, "avg_logprob": -0.10649491273439847, "compression_ratio": 1.4647887323943662, "no_speech_prob": 6.240477432584157e-06}, {"id": 1004, "seek": 728030, "start": 7287.820000000001, "end": 7292.9400000000005, "text": " analysis and to take it to places that it really makes a difference so thank you very", "tokens": [5215, 293, 281, 747, 309, 281, 3190, 300, 309, 534, 1669, 257, 2649, 370, 1309, 291, 588], "temperature": 0.0, "avg_logprob": -0.10649491273439847, "compression_ratio": 1.4647887323943662, "no_speech_prob": 6.240477432584157e-06}, {"id": 1005, "seek": 729294, "start": 7292.94, "end": 7310.54, "text": " much and I'll see you next week.", "tokens": [50364, 709, 293, 286, 603, 536, 291, 958, 1243, 13, 51244], "temperature": 0.0, "avg_logprob": -0.4050849676132202, "compression_ratio": 0.8, "no_speech_prob": 5.300679549691267e-05}], "language": "en"}