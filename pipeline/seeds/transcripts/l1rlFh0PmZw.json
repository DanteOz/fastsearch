{"text": " So we're going to be continuing with this notebook on predicting the English word version of numbers using an RNN. And this was something that we started last time. I also I added the PowerPoint slides that we'll be going through. Those are in the GitHub repo as well. So just to kind of remember where we are previously with ULM fit and doing the sentiment classification, we were using RNNs kind of underneath the hood, but we weren't directly having to deal with them. And now we want to dive more into kind of what RNNs are doing and how they work. And so to do that, we're going to look at the synthetic data set of counting with numbers written out in English, 8001, 8002, 8003, and trying to predict what comes next in this sequence. We talked last time about how there are two different types of numbers in deep learning parameters and activations, and that pretty much everything in your neural network will kind of fit into either being a parameter or being an activation. And so the hidden state, which we're going to talk about in a little bit, is just a set of activations. So here we're reading in and our training data of this list of numbers. And the nice thing about this is that this is a data set we can understand well. It'll kind of be easy to see if things are working. We know what the expected behavior is. This is a relatively small data set, so we only have three batches. So we can even kind of look at our batches in particular to see how things line up. So BPTT is back propagation through time, and that's kind of how many time steps of history we're going to be using in this. We're going to start with 70, and I think we'll reduce that perhaps later on. Notice this is something where you need some history, because if you just got the number one, that's not enough to tell what comes next. You know, in this case, it's comma and then eight. You need to know, really, you need to know that this had 8001 comma to predict that the next thing will be 8002. And really, I should say the next three tokens will be 8002. So this is kind of why you want some history, and it's not enough just to have kind of the most recent token beforehand. And this was true in our language generation as well. Like you don't just need to know the preceding word, you need to know several words that preceded what you're going to predict next. So here, the way these batches were created, we have 70 tokens per line and kind of 64 lines of text in each batch. So X1 is kind of our independent values and Y is the dependent values. And here, these are basically just the same, but offset by one. And that's because after beginning of string, you want to predict eight. After eight, you need to predict thousand. After thousand, you need to predict one. And so Y is basically just X shifted over by one. And we can see that these kind of line up across batches. So the zero, this is the kind of zeroth item in X1, which is batch one. And the zeroth item in batch two continues where that left off with 8,017, 8,018, 8,019. And later on, we'll kind of see why that's significant because we'll want to be able to kind of use that information we learned on batch one to keep going with batch two. This is another way of looking at the batches using data.showBatch. And so here we're looking at. So I'm not sure which batch. Oh, this is the validation set. So we're looking at our validation set and seeing what are the first few items. Are there any questions just about kind of how this data set is set up and how our batches are? All right. So let's switch back to the slides. And I mentioned last time there was the first ones weren't color coded. Now they are. So the thing to know here is that each color arrow is a separate matrix. And so here we have two matrices. These diagrams are kind of very rich in information because the color is going to have meaning, the shape has meaning. Here the shape is distinguishing between inputs, hidden layers and outputs. So this is kind of one of the most basic neural networks you can imagine that just has a single hidden layer. We're going to build from here and consider, OK, what if we wanted to predict word three from words one and two? We multiply word one by a matrix of weights. We get this set of activations and then we're going to multiply word two by the same matrix of weights that we multiplied word one with. And we want a way to combine it with our activations from word one. So those are being multiplied by a different set of weights and put together for our activations after word two. And our goal is to to output word three. And so and these are it's both multiplying by the weights and then going through a non-linearity. Then we can extend this. So word one we multiply by the green matrix, do a non-linearity, multiply by the orange matrix and combine that with word two, having been multiplied by the green matrix. Then we want to bring word three in. So word three will be multiplied by the green matrix. Our set of activations will be multiplied by an orange matrix. OK, sorry about that. Let's see if this stays. So combining word three through the green matrix, these activations from the combination of word one and word two by the orange matrix, put those together to get some activations and then we can predict the fourth word. Questions about what's going on here? Yes? I'm wondering how do we combine the previous hidden layers with the new words? That's a good question. We'll see that in a moment. Let me see. Yeah. So the next code, this is a good lead into the next. How are we combining those? So here you'll see we're just doing a plus equals. So this is kind of how we would write this in code. This is the same picture from before, but showing how we could have this in a neural network. This is using PyTorches and n.module. And you'll see that, yeah, we just do plus equals. And so here, and actually this will be helpful maybe to talk through. So the green arrow corresponds to our embedding matrix of how we're kind of getting the tokens in. And so I guess I shouldn't have said it's multiplied by, but it's kind of picking out the embedding. Then I would call it orange. The orange arrow is going through this linear layer from hidden to hidden. The blue arrow is going to be a linear layer from hidden to output. We'll also be using batch norm. And kind of how this works is a forward step is to take a batch norm of a relu of input to hidden, which is the embedding layer on your zero-th word. Then if it's long enough that you have a first word, then you'll do that hidden state plus equals the embedding layer of the first word. Then we'll take a batch norm and a relu of hidden to hidden of H. So actually, I guess we're combining these and then applying the orange arrow to them. Oh, yeah, so that's what takes it to here. Then we keep going. If it's long enough that we have a word number three, we do the same thing. Are there questions about this? Yes? What is batch norm? So batch norm, we're not going to go into too much detail in it, but it's a way of, it learns a, it's a way to normalize your kind of weights as you go along. And so it learns a bias term and a multiplier. So we normalize the weights, so we normalize the power of those ones. Jeremy, can you say a little bit about batch norm? Do you want the catch box? So we cover that a lot in the deep learning course, so we should definitely check that out for details. It's perhaps the most important development in neural networks in the last few years. And basically, it's a key thing that allows us to train deeper networks without the numbers getting really, really big or really, really small. As Rachel mentioned, the way it does that is by normalizing the output of each layer. So it's normalizing the activations, not the weights. There is also something called weight normalization that does something similar to the weights. That batch normalization normalizes the activations. It, at each layer, it subtracts the, or at each layer that has batch norm, it subtracts the mean of the activations of each channel and divides by the standard deviation. And then, as Rachel mentioned, it has two lots of learnable parameters. One lot of learnable parameters that gets multiplied by that normalized weights, and one set of learnable parameters that gets added to the normalized weights. But those details are terribly important. What's really important, I think, is to recognize that nowadays, most of the time, when somebody says a layer of a neural network, what they mean is what you see on the board reading from inside to out. Do an affine transformation, so in this case, a matrix multiply, and then a non-linearity, in this case, a ReLU, and then a normalization, in this case, batch norm. But that set of three things is what we always try to use when we talk about a layer nowadays. Great, thank you, Jeremy. Although it's really hard to do batch norm, as you'll see in recurrent neural networks, so it's nice that we've got it in this way. Yes, I misspoke. It's normalizing the activations for each layer, and the purpose of this is to keep your activations from getting too giant or too tiny as you go, both of which could cause problems. Thank you, Jeremy, for highlighting that typically a layer is going to involve this linear multiplication, a non-linearity, in this case, we're using ReLU, and then a batch norm. Any other questions on this part? You'll notice a key thing here is, in practice, we would not actually want to write this out. We have an if statement for each of our layers, and so if you're reading through a sentence, you don't really want to be, okay, if there's a word, word two, let's do another one. If there's a word three, if there's a word four, if there's word five. So in practice, since we're considering 70 steps of history, we wouldn't really want 70 if statements and to be writing the same code each time as we go through. This is kind of an ideal place. Actually, I'll ask you, what should we be using instead? A loop, yeah, I saw someone say it. This is kind of a perfect case to use a loop. The reason I've showed it like this is because it becomes harder to understand when you have the for loop, and so sometimes people refer to this as an unrolled RNN, and we'll kind of draw these diagrams. But again, kind of with the repetition as well, that you know these green arrows are actually the same, these orange arrows are the same, we could draw this differently as a loop. So this is the exact same thing as before, but in its rolled up form, written with a loop. And here in the diagram, the dotted line indicates what's inside the loop. So that's everything kind of outlined in this dotted box. And we'll pay attention to that, and that'll be significant later. But so here what we're doing is putting in word, word one, and then combining that with words two through n minus one, oops, and then at the end getting out an output. So this is how we could write that with a loop. Let me go back to the slides so you can see the code for that. And so here this code is written out. This was the same code that was on the slide earlier for the single fully connected model. Here we had it written with the if statements. We can try training that. We're getting an accuracy of, I guess, about 45% after five epochs, which remember with something where you're predicting what's next in a sequence, in some ways that's much harder than predicting a class because you have, I guess in this case we only had maybe 20 tokens, but you often have far more tokens than you had classes. This was only using two pieces of history though. Oops. So now we're refactoring it to use a loop, and this is what the code looks like. So the kind of attributes we're initializing are the same in terms of having the embedding as our green arrow, the hidden linear layer is the orange arrow, and a blue arrow is another linear layer. And here inside our loop we have for i and range x.shape, h equals h plus, put the kind of the next input through the input to hidden embedding. Then we take a relu and a batch norm and continue. And so this is the difference between kind of expressing an RNN and its unrolled form versus its rolled form, which is where you're using the loop. And if you run that you can see that the accuracy is about the same, which is what you expect since this is the same thing just written differently. Any questions about this? All right. So the kind of next way that we can try to improve our accuracy on this is so before we were just predicting the last word in a line of text. So it was basically we had 70, 70 words or 70 tokens as input and we're trying to we were trying to predict the 70 first. But that was actually throwing away a lot of data, right? So really we could be trying to predict token two from token one and then we could use those to predict token three, use that to predict token four. And we could be getting 70 predictions for each line, which would result in a lot more a lot more training data. And so we'll modify our model to do that. And we'll do that by kind of keeping track of our results at each state. So we have this result. We're appending. The batch norm and hidden to output. So this also means that we're so before we were just applying hidden to output at the end, which was the last linear layer. So you see that here hidden to output linear layer just shows up at the very end. We're applying that to our hidden state. Now we've moved that inside the loop and we are keeping track of those from each state. So what that looks like kind of returning to the slideshow. So notice this is what we had before with the for loop. Now we've moved our output inside the for loop because we're taking an output after each step because we want to check our accuracy after each step. Questions about this. So what would you expect this to do to our accuracy? I see some people pointing up. So in some ways this is going to have an effect of improving our accuracy because we've added a lot more data. Although in some ways we've also have a harder task because when we're predicting word for we only have three pieces of history. Whereas previously we always had 70 steps of history before we predict predicted the 70 first. And so if you look at this the accuracy has actually gone down since it's we have more data which was a positive. But then we're doing something harder. So that's that's reduced our accuracy. Basically for any word K for K less than 70 we have less history than we did before. Fortunately this is solvable. What we can do is save our history. And so I mentioned before how the first the first line on batch two is basically just a continuation of the first line from batch one. So we can use use that history. So we'll need to keep the hidden state from the previous line of text so that we're not starting over from scratch. So instead of saying you know each new line of text we're totally starting from the beginning because we've been smart about how we constructed our batches. We can say oh OK with each new line of text that's really just a continuation of the previous line. And so what that involves is kind of stacking up these results as we go. So we're continuing to kind of append them to our result. And then we are setting self dot H to to that hidden state. So we'll have it for the next time we go through the forward forward step. And we can see now our accuracy. So previously kind of I guess in the initial one we were getting like 45 percent accuracy. Then when we started made our task harder by predicting it each time set a step it dropped to 33 percent accuracy. And now that we're saying saving this history we're up to 58 percent accuracy. So we're kind of using our data and we're also using it smartly and kind of holding on to it. Let me see if I have. No. So questions questions about this. And so this is the picture of what we've what we've been doing of kind of recording that output within the loop. And then I guess this is not pictured but knowing that when you start over again you've state saved your state from the previous time. All right. So it's a it's possible to improve this even more. So first step first we're going to refactor using pie torches RNN. And this is something where up here you typically wouldn't wouldn't be writing this from scratch. But I wanted you to see kind of what pie torches RNN or whatever the library of your choices what's going on inside because it's you know it sounds fancy. It's really kind of a for loop just combining these you know adding your hidden state from the previous step to your your new input and then taking a linear transformation of that a rel you and a batch norm. So kind of the building blocks of neural nets. So we'll rewrite this with and RNN from pie torch. So now that that handles it for us. And if we go through this is even oh I guess we ran it for more time steps. So we're still getting the same accuracy after 10 time steps but I ran it for 20 and it improved slightly to 60 61 percent. Jeremy. Let me give you the microphone. I was just going to say one difference is that RNN RNN doesn't have batch norm in the loop. Thank you. The batch norm now you're only applying to the end. That's a good point. Yes. And so here you can see us apply it at the end but it's not inside the loop. The other thing I wanted to mention is the loop version is a lot slower because RNN RNN writes that loop in CUDA C so it runs on the GPU. Where else the Python version has to like say to the GPU run one step of the loop and another step of the loop and another step of the loop and each of those takes a lot of time. So that's one of the really annoying things about working with RNN is that they're not really fast enough to write them in pie torch loops like that. So you kind of have to work with the existing machinery that's out there. Well thanks. Yeah, so this previous for loop version is not something that you would want to be doing in practice, but it is is helpful to understand. So these. There are many things that are tricky about RNN and dealing with RNN. And one is that when you have longer time scales and deeper networks that can become very difficult to train. And so one way to address this is to add kind of mini neural net that will keep track of how much of the green arrow and how much of the orange arrow to keep. So kind of going back to the picture briefly, kind of deciding at each step how much from this part and how much from this part do you want. And the kind of the mini neural nets that are commonly used for this are called either GRUs or LSTMs are kind of the two most common types. And we'll go in a I think in a later lesson we'll talk a little bit more about what those are doing under the under the hood. But for now we'll use pie torches implementation of a GRU and add that. And what that actually. So. Adding a GRU. Kind of in the place of where we had just our general RNN before. Kind of the same the same forward step. And this improves our accuracy to 80 percent. So that's kind of a huge improvement from before where we were getting around 60 percent. And so kind of looking at the picture what this is doing. Is kind of adding this other. Stacked RNN. So kind of tying this back into what you saw in the previous lesson about ULM fit there we were basically swapping out the hidden to output with a classifier in order to do classification. So kind of here and all these our last step has been going hit into output which has been a linear layer. To kind of get which token we're predicting comes next whereas previously we were doing classification with kind of the sentiment analysis on IMDB. And so RNNs are just a refactored fully connected neural network to show one kind of in its. These are whoops unrolled stacked RNNs. You can see look like this where you're kind of feeding in. So this is another way of kind of redrawing this picture. And the same approach so here we're just looking at translation but there are a number of sequence to sequence tasks such as sequence labeling such as part of speech identifying if words should be kind of removed because they're sensitive material that you don't want to share. So there are other next I'll give a list of some other sequence to sequence task. A little bit later. Yes. Sorry for a complicated architecture like what. I'm not sure what would you say Jeremy. Yes, do you want to say more about that. I mean I say like another big issue with RNNs in general is just training them can be can be tricky. Yeah, it's really important. And the thing we talked about last week was the AWD LSTM which has all these different kinds of dropout in it. So the way we deal with overfitting one of the best ways to deal with overfitting is through regularization. And so AWD LSTM provides five times five types of dropout and we didn't talk about it but it also has two types of what's called activity regularization which is basically kind of L2 regularization. So yeah, you you can just dial up the amount of regularization when you see overfitting that anytime you're training in your own network that your approach should always be to first of all overfit and then gradually increase the amount of regularization. So models that don't have kind of nuanced regularization capabilities are really hard to use in practice because you can't use that basic kind of training approach. So that's one of the reasons why AWD LSTM and ULM fit so widely used in practice because they are they have these great controls for regularization. Thank you. Any other questions about this notebook before we go on to the next one? Yes. Can you explain more details about how does like GRU that add meaning and then to decide like to make the model more simpler to user train. So the question was to explain more about what the GRU is doing and how it makes the model simpler. I mean I'd say I don't think it makes the model simpler but it's giving you a way to kind of balance how much weight you want to put on the new each new word versus the history that you have stored in your hidden state. For the most part though we're going to kind of treat the GRU as a black box for now and we'll hopefully come back to it in a later lesson. I'm going to just kind of think of it as a little neural network that is learning so kind of before we go back to the slides. Before this was kind of simpler about what you had to learn like you were just kind of learning this matrix of green error of kind of your embedding and then the orange matrix. The linear layer from hidden to hidden you're learning these two things and adding them together to get this activation. We want to do something kind of even more sophisticated which is kind of like learning the relationship or the balance between them. And so that's what the GRU is letting us do so in that way it's kind of more sophisticated what it allows but for now we're going to treat the GRU as a black box and we'll come back to it. Yes. So the question was if we could go over how to do regularization for neural networks with text data. So that's a good request. I am not sure if we'll have time for it in this class. I'll make a note of that. Jeremy have you covered the regularization used in AWD LSTM in the deep learning class. Probably in details. It's just drop out. So it's Jeremy for the microphone. Jeremy just said it's just drop out on everything and they kind of all have different names where you're applying drop out to you'll see different names for it and I think it's even been independently invented sometimes with different names. The where the dropouts applied. So I'll make a note of that. I don't know that we'll have time for it but that is a good question because it's an important topic and I would I would say definitely I think like reading the AWD LSTM paper by Stephen Meridy would have more info. Okay. So Jeremy I'm saying saying this for the microphone. Jeremy said each each color arrow has its own drop out is a way to think about it.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.0, "text": " So we're going to be continuing with this notebook on predicting the English word version of numbers using an RNN.", "tokens": [407, 321, 434, 516, 281, 312, 9289, 365, 341, 21060, 322, 32884, 264, 3669, 1349, 3037, 295, 3547, 1228, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.18976231743307675, "compression_ratio": 1.516260162601626, "no_speech_prob": 0.08618975430727005}, {"id": 1, "seek": 0, "start": 9.0, "end": 16.0, "text": " And this was something that we started last time. I also I added the PowerPoint slides that we'll be going through.", "tokens": [400, 341, 390, 746, 300, 321, 1409, 1036, 565, 13, 286, 611, 286, 3869, 264, 25584, 9788, 300, 321, 603, 312, 516, 807, 13], "temperature": 0.0, "avg_logprob": -0.18976231743307675, "compression_ratio": 1.516260162601626, "no_speech_prob": 0.08618975430727005}, {"id": 2, "seek": 0, "start": 16.0, "end": 26.0, "text": " Those are in the GitHub repo as well. So just to kind of remember where we are previously with ULM fit and doing the sentiment classification,", "tokens": [3950, 366, 294, 264, 23331, 49040, 382, 731, 13, 407, 445, 281, 733, 295, 1604, 689, 321, 366, 8046, 365, 624, 43, 44, 3318, 293, 884, 264, 16149, 21538, 11], "temperature": 0.0, "avg_logprob": -0.18976231743307675, "compression_ratio": 1.516260162601626, "no_speech_prob": 0.08618975430727005}, {"id": 3, "seek": 2600, "start": 26.0, "end": 31.0, "text": " we were using RNNs kind of underneath the hood, but we weren't directly having to deal with them.", "tokens": [321, 645, 1228, 45702, 45, 82, 733, 295, 7223, 264, 13376, 11, 457, 321, 4999, 380, 3838, 1419, 281, 2028, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.07687309053209093, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.107788067311049e-05}, {"id": 4, "seek": 2600, "start": 31.0, "end": 37.0, "text": " And now we want to dive more into kind of what RNNs are doing and how they work.", "tokens": [400, 586, 321, 528, 281, 9192, 544, 666, 733, 295, 437, 45702, 45, 82, 366, 884, 293, 577, 436, 589, 13], "temperature": 0.0, "avg_logprob": -0.07687309053209093, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.107788067311049e-05}, {"id": 5, "seek": 2600, "start": 37.0, "end": 44.0, "text": " And so to do that, we're going to look at the synthetic data set of counting with numbers written out in English,", "tokens": [400, 370, 281, 360, 300, 11, 321, 434, 516, 281, 574, 412, 264, 23420, 1412, 992, 295, 13251, 365, 3547, 3720, 484, 294, 3669, 11], "temperature": 0.0, "avg_logprob": -0.07687309053209093, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.107788067311049e-05}, {"id": 6, "seek": 2600, "start": 44.0, "end": 53.0, "text": " 8001, 8002, 8003, and trying to predict what comes next in this sequence.", "tokens": [13083, 16, 11, 13083, 17, 11, 13083, 18, 11, 293, 1382, 281, 6069, 437, 1487, 958, 294, 341, 8310, 13], "temperature": 0.0, "avg_logprob": -0.07687309053209093, "compression_ratio": 1.5913043478260869, "no_speech_prob": 6.107788067311049e-05}, {"id": 7, "seek": 5300, "start": 53.0, "end": 60.0, "text": " We talked last time about how there are two different types of numbers in deep learning parameters and activations,", "tokens": [492, 2825, 1036, 565, 466, 577, 456, 366, 732, 819, 3467, 295, 3547, 294, 2452, 2539, 9834, 293, 2430, 763, 11], "temperature": 0.0, "avg_logprob": -0.05229844191135504, "compression_ratio": 1.619718309859155, "no_speech_prob": 6.604005466215312e-05}, {"id": 8, "seek": 5300, "start": 60.0, "end": 68.0, "text": " and that pretty much everything in your neural network will kind of fit into either being a parameter or being an activation.", "tokens": [293, 300, 1238, 709, 1203, 294, 428, 18161, 3209, 486, 733, 295, 3318, 666, 2139, 885, 257, 13075, 420, 885, 364, 24433, 13], "temperature": 0.0, "avg_logprob": -0.05229844191135504, "compression_ratio": 1.619718309859155, "no_speech_prob": 6.604005466215312e-05}, {"id": 9, "seek": 5300, "start": 68.0, "end": 78.0, "text": " And so the hidden state, which we're going to talk about in a little bit, is just a set of activations.", "tokens": [400, 370, 264, 7633, 1785, 11, 597, 321, 434, 516, 281, 751, 466, 294, 257, 707, 857, 11, 307, 445, 257, 992, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.05229844191135504, "compression_ratio": 1.619718309859155, "no_speech_prob": 6.604005466215312e-05}, {"id": 10, "seek": 7800, "start": 78.0, "end": 84.0, "text": " So here we're reading in and our training data of this list of numbers.", "tokens": [407, 510, 321, 434, 3760, 294, 293, 527, 3097, 1412, 295, 341, 1329, 295, 3547, 13], "temperature": 0.0, "avg_logprob": -0.0659090523580903, "compression_ratio": 1.673728813559322, "no_speech_prob": 4.539308065432124e-05}, {"id": 11, "seek": 7800, "start": 84.0, "end": 88.0, "text": " And the nice thing about this is that this is a data set we can understand well.", "tokens": [400, 264, 1481, 551, 466, 341, 307, 300, 341, 307, 257, 1412, 992, 321, 393, 1223, 731, 13], "temperature": 0.0, "avg_logprob": -0.0659090523580903, "compression_ratio": 1.673728813559322, "no_speech_prob": 4.539308065432124e-05}, {"id": 12, "seek": 7800, "start": 88.0, "end": 93.0, "text": " It'll kind of be easy to see if things are working. We know what the expected behavior is.", "tokens": [467, 603, 733, 295, 312, 1858, 281, 536, 498, 721, 366, 1364, 13, 492, 458, 437, 264, 5176, 5223, 307, 13], "temperature": 0.0, "avg_logprob": -0.0659090523580903, "compression_ratio": 1.673728813559322, "no_speech_prob": 4.539308065432124e-05}, {"id": 13, "seek": 7800, "start": 93.0, "end": 96.0, "text": " This is a relatively small data set, so we only have three batches.", "tokens": [639, 307, 257, 7226, 1359, 1412, 992, 11, 370, 321, 787, 362, 1045, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.0659090523580903, "compression_ratio": 1.673728813559322, "no_speech_prob": 4.539308065432124e-05}, {"id": 14, "seek": 7800, "start": 96.0, "end": 104.0, "text": " So we can even kind of look at our batches in particular to see how things line up.", "tokens": [407, 321, 393, 754, 733, 295, 574, 412, 527, 15245, 279, 294, 1729, 281, 536, 577, 721, 1622, 493, 13], "temperature": 0.0, "avg_logprob": -0.0659090523580903, "compression_ratio": 1.673728813559322, "no_speech_prob": 4.539308065432124e-05}, {"id": 15, "seek": 10400, "start": 104.0, "end": 113.0, "text": " So BPTT is back propagation through time, and that's kind of how many time steps of history we're going to be using in this.", "tokens": [407, 363, 47, 28178, 307, 646, 38377, 807, 565, 11, 293, 300, 311, 733, 295, 577, 867, 565, 4439, 295, 2503, 321, 434, 516, 281, 312, 1228, 294, 341, 13], "temperature": 0.0, "avg_logprob": -0.10267848746721135, "compression_ratio": 1.555045871559633, "no_speech_prob": 7.84310104791075e-05}, {"id": 16, "seek": 10400, "start": 113.0, "end": 118.0, "text": " We're going to start with 70, and I think we'll reduce that perhaps later on.", "tokens": [492, 434, 516, 281, 722, 365, 5285, 11, 293, 286, 519, 321, 603, 5407, 300, 4317, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.10267848746721135, "compression_ratio": 1.555045871559633, "no_speech_prob": 7.84310104791075e-05}, {"id": 17, "seek": 10400, "start": 118.0, "end": 129.0, "text": " Notice this is something where you need some history, because if you just got the number one, that's not enough to tell what comes next.", "tokens": [13428, 341, 307, 746, 689, 291, 643, 512, 2503, 11, 570, 498, 291, 445, 658, 264, 1230, 472, 11, 300, 311, 406, 1547, 281, 980, 437, 1487, 958, 13], "temperature": 0.0, "avg_logprob": -0.10267848746721135, "compression_ratio": 1.555045871559633, "no_speech_prob": 7.84310104791075e-05}, {"id": 18, "seek": 12900, "start": 129.0, "end": 141.0, "text": " You know, in this case, it's comma and then eight. You need to know, really, you need to know that this had 8001 comma to predict that the next thing will be 8002.", "tokens": [509, 458, 11, 294, 341, 1389, 11, 309, 311, 22117, 293, 550, 3180, 13, 509, 643, 281, 458, 11, 534, 11, 291, 643, 281, 458, 300, 341, 632, 13083, 16, 22117, 281, 6069, 300, 264, 958, 551, 486, 312, 13083, 17, 13], "temperature": 0.0, "avg_logprob": -0.10417297908238002, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.247307747893501e-05}, {"id": 19, "seek": 12900, "start": 141.0, "end": 145.0, "text": " And really, I should say the next three tokens will be 8002.", "tokens": [400, 534, 11, 286, 820, 584, 264, 958, 1045, 22667, 486, 312, 13083, 17, 13], "temperature": 0.0, "avg_logprob": -0.10417297908238002, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.247307747893501e-05}, {"id": 20, "seek": 12900, "start": 145.0, "end": 153.0, "text": " So this is kind of why you want some history, and it's not enough just to have kind of the most recent token beforehand.", "tokens": [407, 341, 307, 733, 295, 983, 291, 528, 512, 2503, 11, 293, 309, 311, 406, 1547, 445, 281, 362, 733, 295, 264, 881, 5162, 14862, 22893, 13], "temperature": 0.0, "avg_logprob": -0.10417297908238002, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.247307747893501e-05}, {"id": 21, "seek": 12900, "start": 153.0, "end": 155.0, "text": " And this was true in our language generation as well.", "tokens": [400, 341, 390, 2074, 294, 527, 2856, 5125, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10417297908238002, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.247307747893501e-05}, {"id": 22, "seek": 15500, "start": 155.0, "end": 164.0, "text": " Like you don't just need to know the preceding word, you need to know several words that preceded what you're going to predict next.", "tokens": [1743, 291, 500, 380, 445, 643, 281, 458, 264, 16969, 278, 1349, 11, 291, 643, 281, 458, 2940, 2283, 300, 16969, 292, 437, 291, 434, 516, 281, 6069, 958, 13], "temperature": 0.0, "avg_logprob": -0.07883663475513458, "compression_ratio": 1.5121951219512195, "no_speech_prob": 4.683598308474757e-05}, {"id": 23, "seek": 15500, "start": 164.0, "end": 180.0, "text": " So here, the way these batches were created, we have 70 tokens per line and kind of 64 lines of text in each batch.", "tokens": [407, 510, 11, 264, 636, 613, 15245, 279, 645, 2942, 11, 321, 362, 5285, 22667, 680, 1622, 293, 733, 295, 12145, 3876, 295, 2487, 294, 1184, 15245, 13], "temperature": 0.0, "avg_logprob": -0.07883663475513458, "compression_ratio": 1.5121951219512195, "no_speech_prob": 4.683598308474757e-05}, {"id": 24, "seek": 18000, "start": 180.0, "end": 187.0, "text": " So X1 is kind of our independent values and Y is the dependent values.", "tokens": [407, 1783, 16, 307, 733, 295, 527, 6695, 4190, 293, 398, 307, 264, 12334, 4190, 13], "temperature": 0.0, "avg_logprob": -0.0937958847392689, "compression_ratio": 1.8138297872340425, "no_speech_prob": 3.3404417081328575e-06}, {"id": 25, "seek": 18000, "start": 187.0, "end": 191.0, "text": " And here, these are basically just the same, but offset by one.", "tokens": [400, 510, 11, 613, 366, 1936, 445, 264, 912, 11, 457, 18687, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.0937958847392689, "compression_ratio": 1.8138297872340425, "no_speech_prob": 3.3404417081328575e-06}, {"id": 26, "seek": 18000, "start": 191.0, "end": 195.0, "text": " And that's because after beginning of string, you want to predict eight.", "tokens": [400, 300, 311, 570, 934, 2863, 295, 6798, 11, 291, 528, 281, 6069, 3180, 13], "temperature": 0.0, "avg_logprob": -0.0937958847392689, "compression_ratio": 1.8138297872340425, "no_speech_prob": 3.3404417081328575e-06}, {"id": 27, "seek": 18000, "start": 195.0, "end": 200.0, "text": " After eight, you need to predict thousand. After thousand, you need to predict one.", "tokens": [2381, 3180, 11, 291, 643, 281, 6069, 4714, 13, 2381, 4714, 11, 291, 643, 281, 6069, 472, 13], "temperature": 0.0, "avg_logprob": -0.0937958847392689, "compression_ratio": 1.8138297872340425, "no_speech_prob": 3.3404417081328575e-06}, {"id": 28, "seek": 18000, "start": 200.0, "end": 205.0, "text": " And so Y is basically just X shifted over by one.", "tokens": [400, 370, 398, 307, 1936, 445, 1783, 18892, 670, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.0937958847392689, "compression_ratio": 1.8138297872340425, "no_speech_prob": 3.3404417081328575e-06}, {"id": 29, "seek": 20500, "start": 205.0, "end": 215.0, "text": " And we can see that these kind of line up across batches. So the zero, this is the kind of zeroth item in X1, which is batch one.", "tokens": [400, 321, 393, 536, 300, 613, 733, 295, 1622, 493, 2108, 15245, 279, 13, 407, 264, 4018, 11, 341, 307, 264, 733, 295, 44746, 900, 3174, 294, 1783, 16, 11, 597, 307, 15245, 472, 13], "temperature": 0.0, "avg_logprob": -0.10988664627075195, "compression_ratio": 1.7212389380530972, "no_speech_prob": 4.539510337053798e-05}, {"id": 30, "seek": 20500, "start": 215.0, "end": 224.0, "text": " And the zeroth item in batch two continues where that left off with 8,017, 8,018, 8,019.", "tokens": [400, 264, 44746, 900, 3174, 294, 15245, 732, 6515, 689, 300, 1411, 766, 365, 1649, 11, 15, 7773, 11, 1649, 11, 15, 6494, 11, 1649, 11, 15, 3405, 13], "temperature": 0.0, "avg_logprob": -0.10988664627075195, "compression_ratio": 1.7212389380530972, "no_speech_prob": 4.539510337053798e-05}, {"id": 31, "seek": 20500, "start": 224.0, "end": 233.0, "text": " And later on, we'll kind of see why that's significant because we'll want to be able to kind of use that information we learned on batch one to keep going with batch two.", "tokens": [400, 1780, 322, 11, 321, 603, 733, 295, 536, 983, 300, 311, 4776, 570, 321, 603, 528, 281, 312, 1075, 281, 733, 295, 764, 300, 1589, 321, 3264, 322, 15245, 472, 281, 1066, 516, 365, 15245, 732, 13], "temperature": 0.0, "avg_logprob": -0.10988664627075195, "compression_ratio": 1.7212389380530972, "no_speech_prob": 4.539510337053798e-05}, {"id": 32, "seek": 23300, "start": 233.0, "end": 245.0, "text": " This is another way of looking at the batches using data.showBatch.", "tokens": [639, 307, 1071, 636, 295, 1237, 412, 264, 15245, 279, 1228, 1412, 13, 34436, 33, 852, 13], "temperature": 0.0, "avg_logprob": -0.1137824130780769, "compression_ratio": 1.576158940397351, "no_speech_prob": 1.7230498997378163e-05}, {"id": 33, "seek": 23300, "start": 245.0, "end": 254.0, "text": " And so here we're looking at. So I'm not sure which batch.", "tokens": [400, 370, 510, 321, 434, 1237, 412, 13, 407, 286, 478, 406, 988, 597, 15245, 13], "temperature": 0.0, "avg_logprob": -0.1137824130780769, "compression_ratio": 1.576158940397351, "no_speech_prob": 1.7230498997378163e-05}, {"id": 34, "seek": 23300, "start": 254.0, "end": 261.0, "text": " Oh, this is the validation set. So we're looking at our validation set and seeing what are the first few items.", "tokens": [876, 11, 341, 307, 264, 24071, 992, 13, 407, 321, 434, 1237, 412, 527, 24071, 992, 293, 2577, 437, 366, 264, 700, 1326, 4754, 13], "temperature": 0.0, "avg_logprob": -0.1137824130780769, "compression_ratio": 1.576158940397351, "no_speech_prob": 1.7230498997378163e-05}, {"id": 35, "seek": 26100, "start": 261.0, "end": 270.0, "text": " Are there any questions just about kind of how this data set is set up and how our batches are?", "tokens": [2014, 456, 604, 1651, 445, 466, 733, 295, 577, 341, 1412, 992, 307, 992, 493, 293, 577, 527, 15245, 279, 366, 30], "temperature": 0.0, "avg_logprob": -0.08870448341852502, "compression_ratio": 1.52020202020202, "no_speech_prob": 1.4970387383073103e-05}, {"id": 36, "seek": 26100, "start": 270.0, "end": 274.0, "text": " All right. So let's switch back to the slides.", "tokens": [1057, 558, 13, 407, 718, 311, 3679, 646, 281, 264, 9788, 13], "temperature": 0.0, "avg_logprob": -0.08870448341852502, "compression_ratio": 1.52020202020202, "no_speech_prob": 1.4970387383073103e-05}, {"id": 37, "seek": 26100, "start": 274.0, "end": 279.0, "text": " And I mentioned last time there was the first ones weren't color coded. Now they are.", "tokens": [400, 286, 2835, 1036, 565, 456, 390, 264, 700, 2306, 4999, 380, 2017, 34874, 13, 823, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.08870448341852502, "compression_ratio": 1.52020202020202, "no_speech_prob": 1.4970387383073103e-05}, {"id": 38, "seek": 26100, "start": 279.0, "end": 285.0, "text": " So the thing to know here is that each color arrow is a separate matrix.", "tokens": [407, 264, 551, 281, 458, 510, 307, 300, 1184, 2017, 11610, 307, 257, 4994, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08870448341852502, "compression_ratio": 1.52020202020202, "no_speech_prob": 1.4970387383073103e-05}, {"id": 39, "seek": 28500, "start": 285.0, "end": 296.0, "text": " And so here we have two matrices. These diagrams are kind of very rich in information because the color is going to have meaning, the shape has meaning.", "tokens": [400, 370, 510, 321, 362, 732, 32284, 13, 1981, 36709, 366, 733, 295, 588, 4593, 294, 1589, 570, 264, 2017, 307, 516, 281, 362, 3620, 11, 264, 3909, 575, 3620, 13], "temperature": 0.0, "avg_logprob": -0.09064651790418123, "compression_ratio": 1.6018957345971565, "no_speech_prob": 1.018794364426867e-06}, {"id": 40, "seek": 28500, "start": 296.0, "end": 301.0, "text": " Here the shape is distinguishing between inputs, hidden layers and outputs.", "tokens": [1692, 264, 3909, 307, 11365, 3807, 1296, 15743, 11, 7633, 7914, 293, 23930, 13], "temperature": 0.0, "avg_logprob": -0.09064651790418123, "compression_ratio": 1.6018957345971565, "no_speech_prob": 1.018794364426867e-06}, {"id": 41, "seek": 28500, "start": 301.0, "end": 308.0, "text": " So this is kind of one of the most basic neural networks you can imagine that just has a single hidden layer.", "tokens": [407, 341, 307, 733, 295, 472, 295, 264, 881, 3875, 18161, 9590, 291, 393, 3811, 300, 445, 575, 257, 2167, 7633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.09064651790418123, "compression_ratio": 1.6018957345971565, "no_speech_prob": 1.018794364426867e-06}, {"id": 42, "seek": 30800, "start": 308.0, "end": 317.0, "text": " We're going to build from here and consider, OK, what if we wanted to predict word three from words one and two?", "tokens": [492, 434, 516, 281, 1322, 490, 510, 293, 1949, 11, 2264, 11, 437, 498, 321, 1415, 281, 6069, 1349, 1045, 490, 2283, 472, 293, 732, 30], "temperature": 0.0, "avg_logprob": -0.06781861022278503, "compression_ratio": 1.7920792079207921, "no_speech_prob": 1.3006441804463975e-05}, {"id": 43, "seek": 30800, "start": 317.0, "end": 322.0, "text": " We multiply word one by a matrix of weights.", "tokens": [492, 12972, 1349, 472, 538, 257, 8141, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.06781861022278503, "compression_ratio": 1.7920792079207921, "no_speech_prob": 1.3006441804463975e-05}, {"id": 44, "seek": 30800, "start": 322.0, "end": 330.0, "text": " We get this set of activations and then we're going to multiply word two by the same matrix of weights that we multiplied word one with.", "tokens": [492, 483, 341, 992, 295, 2430, 763, 293, 550, 321, 434, 516, 281, 12972, 1349, 732, 538, 264, 912, 8141, 295, 17443, 300, 321, 17207, 1349, 472, 365, 13], "temperature": 0.0, "avg_logprob": -0.06781861022278503, "compression_ratio": 1.7920792079207921, "no_speech_prob": 1.3006441804463975e-05}, {"id": 45, "seek": 30800, "start": 330.0, "end": 334.0, "text": " And we want a way to combine it with our activations from word one.", "tokens": [400, 321, 528, 257, 636, 281, 10432, 309, 365, 527, 2430, 763, 490, 1349, 472, 13], "temperature": 0.0, "avg_logprob": -0.06781861022278503, "compression_ratio": 1.7920792079207921, "no_speech_prob": 1.3006441804463975e-05}, {"id": 46, "seek": 33400, "start": 334.0, "end": 342.0, "text": " So those are being multiplied by a different set of weights and put together for our activations after word two.", "tokens": [407, 729, 366, 885, 17207, 538, 257, 819, 992, 295, 17443, 293, 829, 1214, 337, 527, 2430, 763, 934, 1349, 732, 13], "temperature": 0.0, "avg_logprob": -0.08265889031546457, "compression_ratio": 1.541899441340782, "no_speech_prob": 6.143937753222417e-06}, {"id": 47, "seek": 33400, "start": 342.0, "end": 346.0, "text": " And our goal is to to output word three.", "tokens": [400, 527, 3387, 307, 281, 281, 5598, 1349, 1045, 13], "temperature": 0.0, "avg_logprob": -0.08265889031546457, "compression_ratio": 1.541899441340782, "no_speech_prob": 6.143937753222417e-06}, {"id": 48, "seek": 33400, "start": 346.0, "end": 354.0, "text": " And so and these are it's both multiplying by the weights and then going through a non-linearity.", "tokens": [400, 370, 293, 613, 366, 309, 311, 1293, 30955, 538, 264, 17443, 293, 550, 516, 807, 257, 2107, 12, 1889, 17409, 13], "temperature": 0.0, "avg_logprob": -0.08265889031546457, "compression_ratio": 1.541899441340782, "no_speech_prob": 6.143937753222417e-06}, {"id": 49, "seek": 33400, "start": 354.0, "end": 357.0, "text": " Then we can extend this.", "tokens": [1396, 321, 393, 10101, 341, 13], "temperature": 0.0, "avg_logprob": -0.08265889031546457, "compression_ratio": 1.541899441340782, "no_speech_prob": 6.143937753222417e-06}, {"id": 50, "seek": 35700, "start": 357.0, "end": 373.0, "text": " So word one we multiply by the green matrix, do a non-linearity, multiply by the orange matrix and combine that with word two, having been multiplied by the green matrix.", "tokens": [407, 1349, 472, 321, 12972, 538, 264, 3092, 8141, 11, 360, 257, 2107, 12, 1889, 17409, 11, 12972, 538, 264, 7671, 8141, 293, 10432, 300, 365, 1349, 732, 11, 1419, 668, 17207, 538, 264, 3092, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13081477238581732, "compression_ratio": 1.8642857142857143, "no_speech_prob": 8.397674719162751e-06}, {"id": 51, "seek": 35700, "start": 373.0, "end": 376.0, "text": " Then we want to bring word three in.", "tokens": [1396, 321, 528, 281, 1565, 1349, 1045, 294, 13], "temperature": 0.0, "avg_logprob": -0.13081477238581732, "compression_ratio": 1.8642857142857143, "no_speech_prob": 8.397674719162751e-06}, {"id": 52, "seek": 35700, "start": 376.0, "end": 379.0, "text": " So word three will be multiplied by the green matrix.", "tokens": [407, 1349, 1045, 486, 312, 17207, 538, 264, 3092, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13081477238581732, "compression_ratio": 1.8642857142857143, "no_speech_prob": 8.397674719162751e-06}, {"id": 53, "seek": 37900, "start": 379.0, "end": 387.0, "text": " Our set of activations will be multiplied by an orange matrix.", "tokens": [2621, 992, 295, 2430, 763, 486, 312, 17207, 538, 364, 7671, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13135425506099577, "compression_ratio": 1.5506329113924051, "no_speech_prob": 8.397804776905105e-06}, {"id": 54, "seek": 37900, "start": 387.0, "end": 391.0, "text": " OK, sorry about that.", "tokens": [2264, 11, 2597, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.13135425506099577, "compression_ratio": 1.5506329113924051, "no_speech_prob": 8.397804776905105e-06}, {"id": 55, "seek": 37900, "start": 391.0, "end": 394.0, "text": " Let's see if this stays.", "tokens": [961, 311, 536, 498, 341, 10834, 13], "temperature": 0.0, "avg_logprob": -0.13135425506099577, "compression_ratio": 1.5506329113924051, "no_speech_prob": 8.397804776905105e-06}, {"id": 56, "seek": 37900, "start": 394.0, "end": 403.0, "text": " So combining word three through the green matrix, these activations from the combination of word one and word two by the orange matrix,", "tokens": [407, 21928, 1349, 1045, 807, 264, 3092, 8141, 11, 613, 2430, 763, 490, 264, 6562, 295, 1349, 472, 293, 1349, 732, 538, 264, 7671, 8141, 11], "temperature": 0.0, "avg_logprob": -0.13135425506099577, "compression_ratio": 1.5506329113924051, "no_speech_prob": 8.397804776905105e-06}, {"id": 57, "seek": 40300, "start": 403.0, "end": 409.0, "text": " put those together to get some activations and then we can predict the fourth word.", "tokens": [829, 729, 1214, 281, 483, 512, 2430, 763, 293, 550, 321, 393, 6069, 264, 6409, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1421198026098386, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.704948539962061e-05}, {"id": 58, "seek": 40300, "start": 409.0, "end": 414.0, "text": " Questions about what's going on here?", "tokens": [27738, 466, 437, 311, 516, 322, 510, 30], "temperature": 0.0, "avg_logprob": -0.1421198026098386, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.704948539962061e-05}, {"id": 59, "seek": 40300, "start": 414.0, "end": 416.0, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.1421198026098386, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.704948539962061e-05}, {"id": 60, "seek": 40300, "start": 416.0, "end": 422.0, "text": " I'm wondering how do we combine the previous hidden layers with the new words?", "tokens": [286, 478, 6359, 577, 360, 321, 10432, 264, 3894, 7633, 7914, 365, 264, 777, 2283, 30], "temperature": 0.0, "avg_logprob": -0.1421198026098386, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.704948539962061e-05}, {"id": 61, "seek": 40300, "start": 422.0, "end": 424.0, "text": " That's a good question. We'll see that in a moment.", "tokens": [663, 311, 257, 665, 1168, 13, 492, 603, 536, 300, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1421198026098386, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.704948539962061e-05}, {"id": 62, "seek": 40300, "start": 424.0, "end": 428.0, "text": " Let me see. Yeah. So the next code, this is a good lead into the next.", "tokens": [961, 385, 536, 13, 865, 13, 407, 264, 958, 3089, 11, 341, 307, 257, 665, 1477, 666, 264, 958, 13], "temperature": 0.0, "avg_logprob": -0.1421198026098386, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.704948539962061e-05}, {"id": 63, "seek": 40300, "start": 428.0, "end": 430.0, "text": " How are we combining those?", "tokens": [1012, 366, 321, 21928, 729, 30], "temperature": 0.0, "avg_logprob": -0.1421198026098386, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.704948539962061e-05}, {"id": 64, "seek": 43000, "start": 430.0, "end": 436.0, "text": " So here you'll see we're just doing a plus equals.", "tokens": [407, 510, 291, 603, 536, 321, 434, 445, 884, 257, 1804, 6915, 13], "temperature": 0.0, "avg_logprob": -0.11516486897188075, "compression_ratio": 1.574585635359116, "no_speech_prob": 1.6186235370696522e-05}, {"id": 65, "seek": 43000, "start": 436.0, "end": 439.0, "text": " So this is kind of how we would write this in code.", "tokens": [407, 341, 307, 733, 295, 577, 321, 576, 2464, 341, 294, 3089, 13], "temperature": 0.0, "avg_logprob": -0.11516486897188075, "compression_ratio": 1.574585635359116, "no_speech_prob": 1.6186235370696522e-05}, {"id": 66, "seek": 43000, "start": 439.0, "end": 447.0, "text": " This is the same picture from before, but showing how we could have this in a neural network.", "tokens": [639, 307, 264, 912, 3036, 490, 949, 11, 457, 4099, 577, 321, 727, 362, 341, 294, 257, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.11516486897188075, "compression_ratio": 1.574585635359116, "no_speech_prob": 1.6186235370696522e-05}, {"id": 67, "seek": 43000, "start": 447.0, "end": 450.0, "text": " This is using PyTorches and n.module.", "tokens": [639, 307, 1228, 9953, 51, 284, 3781, 293, 297, 13, 8014, 2271, 13], "temperature": 0.0, "avg_logprob": -0.11516486897188075, "compression_ratio": 1.574585635359116, "no_speech_prob": 1.6186235370696522e-05}, {"id": 68, "seek": 43000, "start": 450.0, "end": 456.0, "text": " And you'll see that, yeah, we just do plus equals.", "tokens": [400, 291, 603, 536, 300, 11, 1338, 11, 321, 445, 360, 1804, 6915, 13], "temperature": 0.0, "avg_logprob": -0.11516486897188075, "compression_ratio": 1.574585635359116, "no_speech_prob": 1.6186235370696522e-05}, {"id": 69, "seek": 45600, "start": 456.0, "end": 460.0, "text": " And so here, and actually this will be helpful maybe to talk through.", "tokens": [400, 370, 510, 11, 293, 767, 341, 486, 312, 4961, 1310, 281, 751, 807, 13], "temperature": 0.0, "avg_logprob": -0.05876761801699375, "compression_ratio": 1.5940170940170941, "no_speech_prob": 2.586602022347506e-05}, {"id": 70, "seek": 45600, "start": 460.0, "end": 467.0, "text": " So the green arrow corresponds to our embedding matrix of how we're kind of getting the tokens in.", "tokens": [407, 264, 3092, 11610, 23249, 281, 527, 12240, 3584, 8141, 295, 577, 321, 434, 733, 295, 1242, 264, 22667, 294, 13], "temperature": 0.0, "avg_logprob": -0.05876761801699375, "compression_ratio": 1.5940170940170941, "no_speech_prob": 2.586602022347506e-05}, {"id": 71, "seek": 45600, "start": 467.0, "end": 474.0, "text": " And so I guess I shouldn't have said it's multiplied by, but it's kind of picking out the embedding.", "tokens": [400, 370, 286, 2041, 286, 4659, 380, 362, 848, 309, 311, 17207, 538, 11, 457, 309, 311, 733, 295, 8867, 484, 264, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.05876761801699375, "compression_ratio": 1.5940170940170941, "no_speech_prob": 2.586602022347506e-05}, {"id": 72, "seek": 45600, "start": 474.0, "end": 477.0, "text": " Then I would call it orange.", "tokens": [1396, 286, 576, 818, 309, 7671, 13], "temperature": 0.0, "avg_logprob": -0.05876761801699375, "compression_ratio": 1.5940170940170941, "no_speech_prob": 2.586602022347506e-05}, {"id": 73, "seek": 45600, "start": 477.0, "end": 482.0, "text": " The orange arrow is going through this linear layer from hidden to hidden.", "tokens": [440, 7671, 11610, 307, 516, 807, 341, 8213, 4583, 490, 7633, 281, 7633, 13], "temperature": 0.0, "avg_logprob": -0.05876761801699375, "compression_ratio": 1.5940170940170941, "no_speech_prob": 2.586602022347506e-05}, {"id": 74, "seek": 48200, "start": 482.0, "end": 487.0, "text": " The blue arrow is going to be a linear layer from hidden to output.", "tokens": [440, 3344, 11610, 307, 516, 281, 312, 257, 8213, 4583, 490, 7633, 281, 5598, 13], "temperature": 0.0, "avg_logprob": -0.08727383613586426, "compression_ratio": 1.5252525252525253, "no_speech_prob": 7.76672732172301e-06}, {"id": 75, "seek": 48200, "start": 487.0, "end": 489.0, "text": " We'll also be using batch norm.", "tokens": [492, 603, 611, 312, 1228, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.08727383613586426, "compression_ratio": 1.5252525252525253, "no_speech_prob": 7.76672732172301e-06}, {"id": 76, "seek": 48200, "start": 489.0, "end": 498.0, "text": " And kind of how this works is a forward step is to take a batch norm of a relu of input to hidden,", "tokens": [400, 733, 295, 577, 341, 1985, 307, 257, 2128, 1823, 307, 281, 747, 257, 15245, 2026, 295, 257, 1039, 84, 295, 4846, 281, 7633, 11], "temperature": 0.0, "avg_logprob": -0.08727383613586426, "compression_ratio": 1.5252525252525253, "no_speech_prob": 7.76672732172301e-06}, {"id": 77, "seek": 48200, "start": 498.0, "end": 503.0, "text": " which is the embedding layer on your zero-th word.", "tokens": [597, 307, 264, 12240, 3584, 4583, 322, 428, 4018, 12, 392, 1349, 13], "temperature": 0.0, "avg_logprob": -0.08727383613586426, "compression_ratio": 1.5252525252525253, "no_speech_prob": 7.76672732172301e-06}, {"id": 78, "seek": 48200, "start": 503.0, "end": 508.0, "text": " Then if it's long enough that you have a first word,", "tokens": [1396, 498, 309, 311, 938, 1547, 300, 291, 362, 257, 700, 1349, 11], "temperature": 0.0, "avg_logprob": -0.08727383613586426, "compression_ratio": 1.5252525252525253, "no_speech_prob": 7.76672732172301e-06}, {"id": 79, "seek": 50800, "start": 508.0, "end": 515.0, "text": " then you'll do that hidden state plus equals the embedding layer of the first word.", "tokens": [550, 291, 603, 360, 300, 7633, 1785, 1804, 6915, 264, 12240, 3584, 4583, 295, 264, 700, 1349, 13], "temperature": 0.0, "avg_logprob": -0.06565546060537363, "compression_ratio": 1.516304347826087, "no_speech_prob": 4.785032615473028e-06}, {"id": 80, "seek": 50800, "start": 515.0, "end": 523.0, "text": " Then we'll take a batch norm and a relu of hidden to hidden of H.", "tokens": [1396, 321, 603, 747, 257, 15245, 2026, 293, 257, 1039, 84, 295, 7633, 281, 7633, 295, 389, 13], "temperature": 0.0, "avg_logprob": -0.06565546060537363, "compression_ratio": 1.516304347826087, "no_speech_prob": 4.785032615473028e-06}, {"id": 81, "seek": 50800, "start": 523.0, "end": 532.0, "text": " So actually, I guess we're combining these and then applying the orange arrow to them.", "tokens": [407, 767, 11, 286, 2041, 321, 434, 21928, 613, 293, 550, 9275, 264, 7671, 11610, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.06565546060537363, "compression_ratio": 1.516304347826087, "no_speech_prob": 4.785032615473028e-06}, {"id": 82, "seek": 50800, "start": 532.0, "end": 535.0, "text": " Oh, yeah, so that's what takes it to here.", "tokens": [876, 11, 1338, 11, 370, 300, 311, 437, 2516, 309, 281, 510, 13], "temperature": 0.0, "avg_logprob": -0.06565546060537363, "compression_ratio": 1.516304347826087, "no_speech_prob": 4.785032615473028e-06}, {"id": 83, "seek": 53500, "start": 535.0, "end": 542.0, "text": " Then we keep going. If it's long enough that we have a word number three, we do the same thing.", "tokens": [1396, 321, 1066, 516, 13, 759, 309, 311, 938, 1547, 300, 321, 362, 257, 1349, 1230, 1045, 11, 321, 360, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.09622968372545744, "compression_ratio": 1.5472636815920398, "no_speech_prob": 2.885628055082634e-05}, {"id": 84, "seek": 53500, "start": 542.0, "end": 546.0, "text": " Are there questions about this?", "tokens": [2014, 456, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.09622968372545744, "compression_ratio": 1.5472636815920398, "no_speech_prob": 2.885628055082634e-05}, {"id": 85, "seek": 53500, "start": 546.0, "end": 547.0, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.09622968372545744, "compression_ratio": 1.5472636815920398, "no_speech_prob": 2.885628055082634e-05}, {"id": 86, "seek": 53500, "start": 547.0, "end": 550.0, "text": " What is batch norm?", "tokens": [708, 307, 15245, 2026, 30], "temperature": 0.0, "avg_logprob": -0.09622968372545744, "compression_ratio": 1.5472636815920398, "no_speech_prob": 2.885628055082634e-05}, {"id": 87, "seek": 53500, "start": 550.0, "end": 555.0, "text": " So batch norm, we're not going to go into too much detail in it, but it's a way of, it learns a,", "tokens": [407, 15245, 2026, 11, 321, 434, 406, 516, 281, 352, 666, 886, 709, 2607, 294, 309, 11, 457, 309, 311, 257, 636, 295, 11, 309, 27152, 257, 11], "temperature": 0.0, "avg_logprob": -0.09622968372545744, "compression_ratio": 1.5472636815920398, "no_speech_prob": 2.885628055082634e-05}, {"id": 88, "seek": 53500, "start": 555.0, "end": 559.0, "text": " it's a way to normalize your kind of weights as you go along.", "tokens": [309, 311, 257, 636, 281, 2710, 1125, 428, 733, 295, 17443, 382, 291, 352, 2051, 13], "temperature": 0.0, "avg_logprob": -0.09622968372545744, "compression_ratio": 1.5472636815920398, "no_speech_prob": 2.885628055082634e-05}, {"id": 89, "seek": 55900, "start": 559.0, "end": 567.0, "text": " And so it learns a bias term and a multiplier.", "tokens": [400, 370, 309, 27152, 257, 12577, 1433, 293, 257, 44106, 13], "temperature": 0.0, "avg_logprob": -0.23047182852761788, "compression_ratio": 1.4057971014492754, "no_speech_prob": 2.4298047719639726e-05}, {"id": 90, "seek": 55900, "start": 567.0, "end": 573.0, "text": " So we normalize the weights, so we normalize the power of those ones.", "tokens": [407, 321, 2710, 1125, 264, 17443, 11, 370, 321, 2710, 1125, 264, 1347, 295, 729, 2306, 13], "temperature": 0.0, "avg_logprob": -0.23047182852761788, "compression_ratio": 1.4057971014492754, "no_speech_prob": 2.4298047719639726e-05}, {"id": 91, "seek": 55900, "start": 573.0, "end": 578.0, "text": " Jeremy, can you say a little bit about batch norm?", "tokens": [17809, 11, 393, 291, 584, 257, 707, 857, 466, 15245, 2026, 30], "temperature": 0.0, "avg_logprob": -0.23047182852761788, "compression_ratio": 1.4057971014492754, "no_speech_prob": 2.4298047719639726e-05}, {"id": 92, "seek": 55900, "start": 578.0, "end": 587.0, "text": " Do you want the catch box?", "tokens": [1144, 291, 528, 264, 3745, 2424, 30], "temperature": 0.0, "avg_logprob": -0.23047182852761788, "compression_ratio": 1.4057971014492754, "no_speech_prob": 2.4298047719639726e-05}, {"id": 93, "seek": 58700, "start": 587.0, "end": 594.0, "text": " So we cover that a lot in the deep learning course, so we should definitely check that out for details.", "tokens": [407, 321, 2060, 300, 257, 688, 294, 264, 2452, 2539, 1164, 11, 370, 321, 820, 2138, 1520, 300, 484, 337, 4365, 13], "temperature": 0.0, "avg_logprob": -0.09739154964298397, "compression_ratio": 1.6009615384615385, "no_speech_prob": 5.3903826483292505e-05}, {"id": 94, "seek": 58700, "start": 594.0, "end": 604.0, "text": " It's perhaps the most important development in neural networks in the last few years.", "tokens": [467, 311, 4317, 264, 881, 1021, 3250, 294, 18161, 9590, 294, 264, 1036, 1326, 924, 13], "temperature": 0.0, "avg_logprob": -0.09739154964298397, "compression_ratio": 1.6009615384615385, "no_speech_prob": 5.3903826483292505e-05}, {"id": 95, "seek": 58700, "start": 604.0, "end": 613.0, "text": " And basically, it's a key thing that allows us to train deeper networks without the numbers getting really,", "tokens": [400, 1936, 11, 309, 311, 257, 2141, 551, 300, 4045, 505, 281, 3847, 7731, 9590, 1553, 264, 3547, 1242, 534, 11], "temperature": 0.0, "avg_logprob": -0.09739154964298397, "compression_ratio": 1.6009615384615385, "no_speech_prob": 5.3903826483292505e-05}, {"id": 96, "seek": 58700, "start": 613.0, "end": 615.0, "text": " really big or really, really small.", "tokens": [534, 955, 420, 534, 11, 534, 1359, 13], "temperature": 0.0, "avg_logprob": -0.09739154964298397, "compression_ratio": 1.6009615384615385, "no_speech_prob": 5.3903826483292505e-05}, {"id": 97, "seek": 61500, "start": 615.0, "end": 623.0, "text": " As Rachel mentioned, the way it does that is by normalizing the output of each layer.", "tokens": [1018, 14246, 2835, 11, 264, 636, 309, 775, 300, 307, 538, 2710, 3319, 264, 5598, 295, 1184, 4583, 13], "temperature": 0.0, "avg_logprob": -0.08404456774393718, "compression_ratio": 1.9308510638297873, "no_speech_prob": 5.06352043885272e-05}, {"id": 98, "seek": 61500, "start": 623.0, "end": 627.0, "text": " So it's normalizing the activations, not the weights.", "tokens": [407, 309, 311, 2710, 3319, 264, 2430, 763, 11, 406, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.08404456774393718, "compression_ratio": 1.9308510638297873, "no_speech_prob": 5.06352043885272e-05}, {"id": 99, "seek": 61500, "start": 627.0, "end": 630.0, "text": " There is also something called weight normalization that does something similar to the weights.", "tokens": [821, 307, 611, 746, 1219, 3364, 2710, 2144, 300, 775, 746, 2531, 281, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.08404456774393718, "compression_ratio": 1.9308510638297873, "no_speech_prob": 5.06352043885272e-05}, {"id": 100, "seek": 61500, "start": 630.0, "end": 634.0, "text": " That batch normalization normalizes the activations.", "tokens": [663, 15245, 2710, 2144, 2710, 5660, 264, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.08404456774393718, "compression_ratio": 1.9308510638297873, "no_speech_prob": 5.06352043885272e-05}, {"id": 101, "seek": 61500, "start": 634.0, "end": 639.0, "text": " It, at each layer, it subtracts the, or at each layer that has batch norm,", "tokens": [467, 11, 412, 1184, 4583, 11, 309, 16390, 82, 264, 11, 420, 412, 1184, 4583, 300, 575, 15245, 2026, 11], "temperature": 0.0, "avg_logprob": -0.08404456774393718, "compression_ratio": 1.9308510638297873, "no_speech_prob": 5.06352043885272e-05}, {"id": 102, "seek": 63900, "start": 639.0, "end": 648.0, "text": " it subtracts the mean of the activations of each channel and divides by the standard deviation.", "tokens": [309, 16390, 82, 264, 914, 295, 264, 2430, 763, 295, 1184, 2269, 293, 41347, 538, 264, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.08312017208821065, "compression_ratio": 1.847457627118644, "no_speech_prob": 2.429847518214956e-05}, {"id": 103, "seek": 63900, "start": 648.0, "end": 654.0, "text": " And then, as Rachel mentioned, it has two lots of learnable parameters.", "tokens": [400, 550, 11, 382, 14246, 2835, 11, 309, 575, 732, 3195, 295, 1466, 712, 9834, 13], "temperature": 0.0, "avg_logprob": -0.08312017208821065, "compression_ratio": 1.847457627118644, "no_speech_prob": 2.429847518214956e-05}, {"id": 104, "seek": 63900, "start": 654.0, "end": 659.0, "text": " One lot of learnable parameters that gets multiplied by that normalized weights,", "tokens": [1485, 688, 295, 1466, 712, 9834, 300, 2170, 17207, 538, 300, 48704, 17443, 11], "temperature": 0.0, "avg_logprob": -0.08312017208821065, "compression_ratio": 1.847457627118644, "no_speech_prob": 2.429847518214956e-05}, {"id": 105, "seek": 63900, "start": 659.0, "end": 665.0, "text": " and one set of learnable parameters that gets added to the normalized weights.", "tokens": [293, 472, 992, 295, 1466, 712, 9834, 300, 2170, 3869, 281, 264, 48704, 17443, 13], "temperature": 0.0, "avg_logprob": -0.08312017208821065, "compression_ratio": 1.847457627118644, "no_speech_prob": 2.429847518214956e-05}, {"id": 106, "seek": 66500, "start": 665.0, "end": 669.0, "text": " But those details are terribly important.", "tokens": [583, 729, 4365, 366, 22903, 1021, 13], "temperature": 0.0, "avg_logprob": -0.1258305628365333, "compression_ratio": 1.6693877551020408, "no_speech_prob": 0.00012929784134030342}, {"id": 107, "seek": 66500, "start": 669.0, "end": 674.0, "text": " What's really important, I think, is to recognize that nowadays, most of the time,", "tokens": [708, 311, 534, 1021, 11, 286, 519, 11, 307, 281, 5521, 300, 13434, 11, 881, 295, 264, 565, 11], "temperature": 0.0, "avg_logprob": -0.1258305628365333, "compression_ratio": 1.6693877551020408, "no_speech_prob": 0.00012929784134030342}, {"id": 108, "seek": 66500, "start": 674.0, "end": 680.0, "text": " when somebody says a layer of a neural network, what they mean is what you see on the board reading from inside to out.", "tokens": [562, 2618, 1619, 257, 4583, 295, 257, 18161, 3209, 11, 437, 436, 914, 307, 437, 291, 536, 322, 264, 3150, 3760, 490, 1854, 281, 484, 13], "temperature": 0.0, "avg_logprob": -0.1258305628365333, "compression_ratio": 1.6693877551020408, "no_speech_prob": 0.00012929784134030342}, {"id": 109, "seek": 66500, "start": 680.0, "end": 687.0, "text": " Do an affine transformation, so in this case, a matrix multiply, and then a non-linearity, in this case, a ReLU,", "tokens": [1144, 364, 2096, 533, 9887, 11, 370, 294, 341, 1389, 11, 257, 8141, 12972, 11, 293, 550, 257, 2107, 12, 1889, 17409, 11, 294, 341, 1389, 11, 257, 1300, 43, 52, 11], "temperature": 0.0, "avg_logprob": -0.1258305628365333, "compression_ratio": 1.6693877551020408, "no_speech_prob": 0.00012929784134030342}, {"id": 110, "seek": 66500, "start": 687.0, "end": 691.0, "text": " and then a normalization, in this case, batch norm.", "tokens": [293, 550, 257, 2710, 2144, 11, 294, 341, 1389, 11, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.1258305628365333, "compression_ratio": 1.6693877551020408, "no_speech_prob": 0.00012929784134030342}, {"id": 111, "seek": 69100, "start": 691.0, "end": 698.0, "text": " But that set of three things is what we always try to use when we talk about a layer nowadays.", "tokens": [583, 300, 992, 295, 1045, 721, 307, 437, 321, 1009, 853, 281, 764, 562, 321, 751, 466, 257, 4583, 13434, 13], "temperature": 0.0, "avg_logprob": -0.0714088992068642, "compression_ratio": 1.6, "no_speech_prob": 0.00011959140829276294}, {"id": 112, "seek": 69100, "start": 698.0, "end": 700.0, "text": " Great, thank you, Jeremy.", "tokens": [3769, 11, 1309, 291, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.0714088992068642, "compression_ratio": 1.6, "no_speech_prob": 0.00011959140829276294}, {"id": 113, "seek": 69100, "start": 700.0, "end": 704.0, "text": " Although it's really hard to do batch norm, as you'll see in recurrent neural networks,", "tokens": [5780, 309, 311, 534, 1152, 281, 360, 15245, 2026, 11, 382, 291, 603, 536, 294, 18680, 1753, 18161, 9590, 11], "temperature": 0.0, "avg_logprob": -0.0714088992068642, "compression_ratio": 1.6, "no_speech_prob": 0.00011959140829276294}, {"id": 114, "seek": 69100, "start": 704.0, "end": 708.0, "text": " so it's nice that we've got it in this way.", "tokens": [370, 309, 311, 1481, 300, 321, 600, 658, 309, 294, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.0714088992068642, "compression_ratio": 1.6, "no_speech_prob": 0.00011959140829276294}, {"id": 115, "seek": 69100, "start": 708.0, "end": 713.0, "text": " Yes, I misspoke. It's normalizing the activations for each layer,", "tokens": [1079, 11, 286, 1713, 48776, 13, 467, 311, 2710, 3319, 264, 2430, 763, 337, 1184, 4583, 11], "temperature": 0.0, "avg_logprob": -0.0714088992068642, "compression_ratio": 1.6, "no_speech_prob": 0.00011959140829276294}, {"id": 116, "seek": 69100, "start": 713.0, "end": 719.0, "text": " and the purpose of this is to keep your activations from getting too giant or too tiny as you go,", "tokens": [293, 264, 4334, 295, 341, 307, 281, 1066, 428, 2430, 763, 490, 1242, 886, 7410, 420, 886, 5870, 382, 291, 352, 11], "temperature": 0.0, "avg_logprob": -0.0714088992068642, "compression_ratio": 1.6, "no_speech_prob": 0.00011959140829276294}, {"id": 117, "seek": 71900, "start": 719.0, "end": 722.0, "text": " both of which could cause problems.", "tokens": [1293, 295, 597, 727, 3082, 2740, 13], "temperature": 0.0, "avg_logprob": -0.08814864688449436, "compression_ratio": 1.5, "no_speech_prob": 4.005989103461616e-05}, {"id": 118, "seek": 71900, "start": 722.0, "end": 728.0, "text": " Thank you, Jeremy, for highlighting that typically a layer is going to involve this linear multiplication,", "tokens": [1044, 291, 11, 17809, 11, 337, 26551, 300, 5850, 257, 4583, 307, 516, 281, 9494, 341, 8213, 27290, 11], "temperature": 0.0, "avg_logprob": -0.08814864688449436, "compression_ratio": 1.5, "no_speech_prob": 4.005989103461616e-05}, {"id": 119, "seek": 71900, "start": 728.0, "end": 734.0, "text": " a non-linearity, in this case, we're using ReLU, and then a batch norm.", "tokens": [257, 2107, 12, 1889, 17409, 11, 294, 341, 1389, 11, 321, 434, 1228, 1300, 43, 52, 11, 293, 550, 257, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.08814864688449436, "compression_ratio": 1.5, "no_speech_prob": 4.005989103461616e-05}, {"id": 120, "seek": 71900, "start": 734.0, "end": 741.0, "text": " Any other questions on this part?", "tokens": [2639, 661, 1651, 322, 341, 644, 30], "temperature": 0.0, "avg_logprob": -0.08814864688449436, "compression_ratio": 1.5, "no_speech_prob": 4.005989103461616e-05}, {"id": 121, "seek": 71900, "start": 741.0, "end": 748.0, "text": " You'll notice a key thing here is, in practice, we would not actually want to write this out.", "tokens": [509, 603, 3449, 257, 2141, 551, 510, 307, 11, 294, 3124, 11, 321, 576, 406, 767, 528, 281, 2464, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.08814864688449436, "compression_ratio": 1.5, "no_speech_prob": 4.005989103461616e-05}, {"id": 122, "seek": 74800, "start": 748.0, "end": 753.0, "text": " We have an if statement for each of our layers,", "tokens": [492, 362, 364, 498, 5629, 337, 1184, 295, 527, 7914, 11], "temperature": 0.0, "avg_logprob": -0.1079951576564623, "compression_ratio": 1.771551724137931, "no_speech_prob": 9.7582524176687e-05}, {"id": 123, "seek": 74800, "start": 753.0, "end": 758.0, "text": " and so if you're reading through a sentence, you don't really want to be,", "tokens": [293, 370, 498, 291, 434, 3760, 807, 257, 8174, 11, 291, 500, 380, 534, 528, 281, 312, 11], "temperature": 0.0, "avg_logprob": -0.1079951576564623, "compression_ratio": 1.771551724137931, "no_speech_prob": 9.7582524176687e-05}, {"id": 124, "seek": 74800, "start": 758.0, "end": 762.0, "text": " okay, if there's a word, word two, let's do another one.", "tokens": [1392, 11, 498, 456, 311, 257, 1349, 11, 1349, 732, 11, 718, 311, 360, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.1079951576564623, "compression_ratio": 1.771551724137931, "no_speech_prob": 9.7582524176687e-05}, {"id": 125, "seek": 74800, "start": 762.0, "end": 766.0, "text": " If there's a word three, if there's a word four, if there's word five.", "tokens": [759, 456, 311, 257, 1349, 1045, 11, 498, 456, 311, 257, 1349, 1451, 11, 498, 456, 311, 1349, 1732, 13], "temperature": 0.0, "avg_logprob": -0.1079951576564623, "compression_ratio": 1.771551724137931, "no_speech_prob": 9.7582524176687e-05}, {"id": 126, "seek": 74800, "start": 766.0, "end": 771.0, "text": " So in practice, since we're considering 70 steps of history,", "tokens": [407, 294, 3124, 11, 1670, 321, 434, 8079, 5285, 4439, 295, 2503, 11], "temperature": 0.0, "avg_logprob": -0.1079951576564623, "compression_ratio": 1.771551724137931, "no_speech_prob": 9.7582524176687e-05}, {"id": 127, "seek": 74800, "start": 771.0, "end": 777.0, "text": " we wouldn't really want 70 if statements and to be writing the same code each time as we go through.", "tokens": [321, 2759, 380, 534, 528, 5285, 498, 12363, 293, 281, 312, 3579, 264, 912, 3089, 1184, 565, 382, 321, 352, 807, 13], "temperature": 0.0, "avg_logprob": -0.1079951576564623, "compression_ratio": 1.771551724137931, "no_speech_prob": 9.7582524176687e-05}, {"id": 128, "seek": 77700, "start": 777.0, "end": 780.0, "text": " This is kind of an ideal place.", "tokens": [639, 307, 733, 295, 364, 7157, 1081, 13], "temperature": 0.0, "avg_logprob": -0.10568349063396454, "compression_ratio": 1.6655052264808363, "no_speech_prob": 7.141061360016465e-05}, {"id": 129, "seek": 77700, "start": 780.0, "end": 783.0, "text": " Actually, I'll ask you, what should we be using instead?", "tokens": [5135, 11, 286, 603, 1029, 291, 11, 437, 820, 321, 312, 1228, 2602, 30], "temperature": 0.0, "avg_logprob": -0.10568349063396454, "compression_ratio": 1.6655052264808363, "no_speech_prob": 7.141061360016465e-05}, {"id": 130, "seek": 77700, "start": 783.0, "end": 787.0, "text": " A loop, yeah, I saw someone say it.", "tokens": [316, 6367, 11, 1338, 11, 286, 1866, 1580, 584, 309, 13], "temperature": 0.0, "avg_logprob": -0.10568349063396454, "compression_ratio": 1.6655052264808363, "no_speech_prob": 7.141061360016465e-05}, {"id": 131, "seek": 77700, "start": 787.0, "end": 790.0, "text": " This is kind of a perfect case to use a loop.", "tokens": [639, 307, 733, 295, 257, 2176, 1389, 281, 764, 257, 6367, 13], "temperature": 0.0, "avg_logprob": -0.10568349063396454, "compression_ratio": 1.6655052264808363, "no_speech_prob": 7.141061360016465e-05}, {"id": 132, "seek": 77700, "start": 790.0, "end": 794.0, "text": " The reason I've showed it like this is because it becomes harder to understand when you have the for loop,", "tokens": [440, 1778, 286, 600, 4712, 309, 411, 341, 307, 570, 309, 3643, 6081, 281, 1223, 562, 291, 362, 264, 337, 6367, 11], "temperature": 0.0, "avg_logprob": -0.10568349063396454, "compression_ratio": 1.6655052264808363, "no_speech_prob": 7.141061360016465e-05}, {"id": 133, "seek": 77700, "start": 794.0, "end": 801.0, "text": " and so sometimes people refer to this as an unrolled RNN, and we'll kind of draw these diagrams.", "tokens": [293, 370, 2171, 561, 2864, 281, 341, 382, 364, 517, 28850, 45702, 45, 11, 293, 321, 603, 733, 295, 2642, 613, 36709, 13], "temperature": 0.0, "avg_logprob": -0.10568349063396454, "compression_ratio": 1.6655052264808363, "no_speech_prob": 7.141061360016465e-05}, {"id": 134, "seek": 77700, "start": 801.0, "end": 806.0, "text": " But again, kind of with the repetition as well, that you know these green arrows are actually the same,", "tokens": [583, 797, 11, 733, 295, 365, 264, 30432, 382, 731, 11, 300, 291, 458, 613, 3092, 19669, 366, 767, 264, 912, 11], "temperature": 0.0, "avg_logprob": -0.10568349063396454, "compression_ratio": 1.6655052264808363, "no_speech_prob": 7.141061360016465e-05}, {"id": 135, "seek": 80600, "start": 806.0, "end": 811.0, "text": " these orange arrows are the same, we could draw this differently as a loop.", "tokens": [613, 7671, 19669, 366, 264, 912, 11, 321, 727, 2642, 341, 7614, 382, 257, 6367, 13], "temperature": 0.0, "avg_logprob": -0.08046779837659611, "compression_ratio": 1.6561085972850678, "no_speech_prob": 1.172630845758249e-06}, {"id": 136, "seek": 80600, "start": 811.0, "end": 819.0, "text": " So this is the exact same thing as before, but in its rolled up form, written with a loop.", "tokens": [407, 341, 307, 264, 1900, 912, 551, 382, 949, 11, 457, 294, 1080, 14306, 493, 1254, 11, 3720, 365, 257, 6367, 13], "temperature": 0.0, "avg_logprob": -0.08046779837659611, "compression_ratio": 1.6561085972850678, "no_speech_prob": 1.172630845758249e-06}, {"id": 137, "seek": 80600, "start": 819.0, "end": 825.0, "text": " And here in the diagram, the dotted line indicates what's inside the loop.", "tokens": [400, 510, 294, 264, 10686, 11, 264, 37459, 1622, 16203, 437, 311, 1854, 264, 6367, 13], "temperature": 0.0, "avg_logprob": -0.08046779837659611, "compression_ratio": 1.6561085972850678, "no_speech_prob": 1.172630845758249e-06}, {"id": 138, "seek": 80600, "start": 825.0, "end": 830.0, "text": " So that's everything kind of outlined in this dotted box.", "tokens": [407, 300, 311, 1203, 733, 295, 27412, 294, 341, 37459, 2424, 13], "temperature": 0.0, "avg_logprob": -0.08046779837659611, "compression_ratio": 1.6561085972850678, "no_speech_prob": 1.172630845758249e-06}, {"id": 139, "seek": 80600, "start": 830.0, "end": 834.0, "text": " And we'll pay attention to that, and that'll be significant later.", "tokens": [400, 321, 603, 1689, 3202, 281, 300, 11, 293, 300, 603, 312, 4776, 1780, 13], "temperature": 0.0, "avg_logprob": -0.08046779837659611, "compression_ratio": 1.6561085972850678, "no_speech_prob": 1.172630845758249e-06}, {"id": 140, "seek": 83400, "start": 834.0, "end": 839.0, "text": " But so here what we're doing is putting in word, word one,", "tokens": [583, 370, 510, 437, 321, 434, 884, 307, 3372, 294, 1349, 11, 1349, 472, 11], "temperature": 0.0, "avg_logprob": -0.10490809546576606, "compression_ratio": 1.6544502617801047, "no_speech_prob": 9.609522385289893e-05}, {"id": 141, "seek": 83400, "start": 839.0, "end": 846.0, "text": " and then combining that with words two through n minus one, oops,", "tokens": [293, 550, 21928, 300, 365, 2283, 732, 807, 297, 3175, 472, 11, 34166, 11], "temperature": 0.0, "avg_logprob": -0.10490809546576606, "compression_ratio": 1.6544502617801047, "no_speech_prob": 9.609522385289893e-05}, {"id": 142, "seek": 83400, "start": 846.0, "end": 849.0, "text": " and then at the end getting out an output.", "tokens": [293, 550, 412, 264, 917, 1242, 484, 364, 5598, 13], "temperature": 0.0, "avg_logprob": -0.10490809546576606, "compression_ratio": 1.6544502617801047, "no_speech_prob": 9.609522385289893e-05}, {"id": 143, "seek": 83400, "start": 849.0, "end": 851.0, "text": " So this is how we could write that with a loop.", "tokens": [407, 341, 307, 577, 321, 727, 2464, 300, 365, 257, 6367, 13], "temperature": 0.0, "avg_logprob": -0.10490809546576606, "compression_ratio": 1.6544502617801047, "no_speech_prob": 9.609522385289893e-05}, {"id": 144, "seek": 83400, "start": 851.0, "end": 857.0, "text": " Let me go back to the slides so you can see the code for that.", "tokens": [961, 385, 352, 646, 281, 264, 9788, 370, 291, 393, 536, 264, 3089, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.10490809546576606, "compression_ratio": 1.6544502617801047, "no_speech_prob": 9.609522385289893e-05}, {"id": 145, "seek": 83400, "start": 857.0, "end": 859.0, "text": " And so here this code is written out.", "tokens": [400, 370, 510, 341, 3089, 307, 3720, 484, 13], "temperature": 0.0, "avg_logprob": -0.10490809546576606, "compression_ratio": 1.6544502617801047, "no_speech_prob": 9.609522385289893e-05}, {"id": 146, "seek": 85900, "start": 859.0, "end": 866.0, "text": " This was the same code that was on the slide earlier for the single fully connected model.", "tokens": [639, 390, 264, 912, 3089, 300, 390, 322, 264, 4137, 3071, 337, 264, 2167, 4498, 4582, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08653574352022968, "compression_ratio": 1.5, "no_speech_prob": 2.0784753360203467e-05}, {"id": 147, "seek": 85900, "start": 866.0, "end": 870.0, "text": " Here we had it written with the if statements.", "tokens": [1692, 321, 632, 309, 3720, 365, 264, 498, 12363, 13], "temperature": 0.0, "avg_logprob": -0.08653574352022968, "compression_ratio": 1.5, "no_speech_prob": 2.0784753360203467e-05}, {"id": 148, "seek": 85900, "start": 870.0, "end": 873.0, "text": " We can try training that.", "tokens": [492, 393, 853, 3097, 300, 13], "temperature": 0.0, "avg_logprob": -0.08653574352022968, "compression_ratio": 1.5, "no_speech_prob": 2.0784753360203467e-05}, {"id": 149, "seek": 85900, "start": 873.0, "end": 880.0, "text": " We're getting an accuracy of, I guess, about 45% after five epochs,", "tokens": [492, 434, 1242, 364, 14170, 295, 11, 286, 2041, 11, 466, 6905, 4, 934, 1732, 30992, 28346, 11], "temperature": 0.0, "avg_logprob": -0.08653574352022968, "compression_ratio": 1.5, "no_speech_prob": 2.0784753360203467e-05}, {"id": 150, "seek": 85900, "start": 880.0, "end": 885.0, "text": " which remember with something where you're predicting what's next in a sequence,", "tokens": [597, 1604, 365, 746, 689, 291, 434, 32884, 437, 311, 958, 294, 257, 8310, 11], "temperature": 0.0, "avg_logprob": -0.08653574352022968, "compression_ratio": 1.5, "no_speech_prob": 2.0784753360203467e-05}, {"id": 151, "seek": 88500, "start": 885.0, "end": 889.0, "text": " in some ways that's much harder than predicting a class because you have,", "tokens": [294, 512, 2098, 300, 311, 709, 6081, 813, 32884, 257, 1508, 570, 291, 362, 11], "temperature": 0.0, "avg_logprob": -0.0710765082260658, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.092871677130461e-06}, {"id": 152, "seek": 88500, "start": 889.0, "end": 891.0, "text": " I guess in this case we only had maybe 20 tokens,", "tokens": [286, 2041, 294, 341, 1389, 321, 787, 632, 1310, 945, 22667, 11], "temperature": 0.0, "avg_logprob": -0.0710765082260658, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.092871677130461e-06}, {"id": 153, "seek": 88500, "start": 891.0, "end": 896.0, "text": " but you often have far more tokens than you had classes.", "tokens": [457, 291, 2049, 362, 1400, 544, 22667, 813, 291, 632, 5359, 13], "temperature": 0.0, "avg_logprob": -0.0710765082260658, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.092871677130461e-06}, {"id": 154, "seek": 88500, "start": 896.0, "end": 901.0, "text": " This was only using two pieces of history though.", "tokens": [639, 390, 787, 1228, 732, 3755, 295, 2503, 1673, 13], "temperature": 0.0, "avg_logprob": -0.0710765082260658, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.092871677130461e-06}, {"id": 155, "seek": 88500, "start": 901.0, "end": 906.0, "text": " Oops.", "tokens": [21726, 13], "temperature": 0.0, "avg_logprob": -0.0710765082260658, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.092871677130461e-06}, {"id": 156, "seek": 88500, "start": 906.0, "end": 912.0, "text": " So now we're refactoring it to use a loop, and this is what the code looks like.", "tokens": [407, 586, 321, 434, 1895, 578, 3662, 309, 281, 764, 257, 6367, 11, 293, 341, 307, 437, 264, 3089, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.0710765082260658, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.092871677130461e-06}, {"id": 157, "seek": 91200, "start": 912.0, "end": 919.0, "text": " So the kind of attributes we're initializing are the same in terms of having the embedding as our green arrow,", "tokens": [407, 264, 733, 295, 17212, 321, 434, 5883, 3319, 366, 264, 912, 294, 2115, 295, 1419, 264, 12240, 3584, 382, 527, 3092, 11610, 11], "temperature": 0.0, "avg_logprob": -0.16883209389700016, "compression_ratio": 1.5813953488372092, "no_speech_prob": 6.962128736631712e-06}, {"id": 158, "seek": 91200, "start": 919.0, "end": 930.0, "text": " the hidden linear layer is the orange arrow, and a blue arrow is another linear layer.", "tokens": [264, 7633, 8213, 4583, 307, 264, 7671, 11610, 11, 293, 257, 3344, 11610, 307, 1071, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.16883209389700016, "compression_ratio": 1.5813953488372092, "no_speech_prob": 6.962128736631712e-06}, {"id": 159, "seek": 91200, "start": 930.0, "end": 938.0, "text": " And here inside our loop we have for i and range x.shape, h equals h plus,", "tokens": [400, 510, 1854, 527, 6367, 321, 362, 337, 741, 293, 3613, 2031, 13, 82, 42406, 11, 276, 6915, 276, 1804, 11], "temperature": 0.0, "avg_logprob": -0.16883209389700016, "compression_ratio": 1.5813953488372092, "no_speech_prob": 6.962128736631712e-06}, {"id": 160, "seek": 93800, "start": 938.0, "end": 944.0, "text": " put the kind of the next input through the input to hidden embedding.", "tokens": [829, 264, 733, 295, 264, 958, 4846, 807, 264, 4846, 281, 7633, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.1014700072152274, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.2411254829203244e-05}, {"id": 161, "seek": 93800, "start": 944.0, "end": 950.0, "text": " Then we take a relu and a batch norm and continue.", "tokens": [1396, 321, 747, 257, 1039, 84, 293, 257, 15245, 2026, 293, 2354, 13], "temperature": 0.0, "avg_logprob": -0.1014700072152274, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.2411254829203244e-05}, {"id": 162, "seek": 93800, "start": 950.0, "end": 958.0, "text": " And so this is the difference between kind of expressing an RNN and its unrolled form versus its rolled form,", "tokens": [400, 370, 341, 307, 264, 2649, 1296, 733, 295, 22171, 364, 45702, 45, 293, 1080, 517, 28850, 1254, 5717, 1080, 14306, 1254, 11], "temperature": 0.0, "avg_logprob": -0.1014700072152274, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.2411254829203244e-05}, {"id": 163, "seek": 93800, "start": 958.0, "end": 962.0, "text": " which is where you're using the loop.", "tokens": [597, 307, 689, 291, 434, 1228, 264, 6367, 13], "temperature": 0.0, "avg_logprob": -0.1014700072152274, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.2411254829203244e-05}, {"id": 164, "seek": 96200, "start": 962.0, "end": 971.0, "text": " And if you run that you can see that the accuracy is about the same, which is what you expect since this is the same thing just written differently.", "tokens": [400, 498, 291, 1190, 300, 291, 393, 536, 300, 264, 14170, 307, 466, 264, 912, 11, 597, 307, 437, 291, 2066, 1670, 341, 307, 264, 912, 551, 445, 3720, 7614, 13], "temperature": 0.0, "avg_logprob": -0.09745813551403228, "compression_ratio": 1.45, "no_speech_prob": 1.2218772099004127e-05}, {"id": 165, "seek": 96200, "start": 971.0, "end": 985.0, "text": " Any questions about this?", "tokens": [2639, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.09745813551403228, "compression_ratio": 1.45, "no_speech_prob": 1.2218772099004127e-05}, {"id": 166, "seek": 98500, "start": 985.0, "end": 996.0, "text": " All right. So the kind of next way that we can try to improve our accuracy on this is so before we were just predicting the last word in a line of text.", "tokens": [1057, 558, 13, 407, 264, 733, 295, 958, 636, 300, 321, 393, 853, 281, 3470, 527, 14170, 322, 341, 307, 370, 949, 321, 645, 445, 32884, 264, 1036, 1349, 294, 257, 1622, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.1503427729887121, "compression_ratio": 1.5734597156398105, "no_speech_prob": 1.2410976523824502e-05}, {"id": 167, "seek": 98500, "start": 996.0, "end": 1006.0, "text": " So it was basically we had 70, 70 words or 70 tokens as input and we're trying to we were trying to predict the 70 first.", "tokens": [407, 309, 390, 1936, 321, 632, 5285, 11, 5285, 2283, 420, 5285, 22667, 382, 4846, 293, 321, 434, 1382, 281, 321, 645, 1382, 281, 6069, 264, 5285, 700, 13], "temperature": 0.0, "avg_logprob": -0.1503427729887121, "compression_ratio": 1.5734597156398105, "no_speech_prob": 1.2410976523824502e-05}, {"id": 168, "seek": 98500, "start": 1006.0, "end": 1009.0, "text": " But that was actually throwing away a lot of data, right?", "tokens": [583, 300, 390, 767, 10238, 1314, 257, 688, 295, 1412, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1503427729887121, "compression_ratio": 1.5734597156398105, "no_speech_prob": 1.2410976523824502e-05}, {"id": 169, "seek": 100900, "start": 1009.0, "end": 1017.0, "text": " So really we could be trying to predict token two from token one and then we could use those to predict token three, use that to predict token four.", "tokens": [407, 534, 321, 727, 312, 1382, 281, 6069, 14862, 732, 490, 14862, 472, 293, 550, 321, 727, 764, 729, 281, 6069, 14862, 1045, 11, 764, 300, 281, 6069, 14862, 1451, 13], "temperature": 0.0, "avg_logprob": -0.06957963717881069, "compression_ratio": 1.8431372549019607, "no_speech_prob": 1.544583574286662e-05}, {"id": 170, "seek": 100900, "start": 1017.0, "end": 1024.0, "text": " And we could be getting 70 predictions for each line, which would result in a lot more a lot more training data.", "tokens": [400, 321, 727, 312, 1242, 5285, 21264, 337, 1184, 1622, 11, 597, 576, 1874, 294, 257, 688, 544, 257, 688, 544, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.06957963717881069, "compression_ratio": 1.8431372549019607, "no_speech_prob": 1.544583574286662e-05}, {"id": 171, "seek": 100900, "start": 1024.0, "end": 1030.0, "text": " And so we'll modify our model to do that.", "tokens": [400, 370, 321, 603, 16927, 527, 2316, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.06957963717881069, "compression_ratio": 1.8431372549019607, "no_speech_prob": 1.544583574286662e-05}, {"id": 172, "seek": 100900, "start": 1030.0, "end": 1037.0, "text": " And we'll do that by kind of keeping track of our results at each state.", "tokens": [400, 321, 603, 360, 300, 538, 733, 295, 5145, 2837, 295, 527, 3542, 412, 1184, 1785, 13], "temperature": 0.0, "avg_logprob": -0.06957963717881069, "compression_ratio": 1.8431372549019607, "no_speech_prob": 1.544583574286662e-05}, {"id": 173, "seek": 103700, "start": 1037.0, "end": 1042.0, "text": " So we have this result. We're appending.", "tokens": [407, 321, 362, 341, 1874, 13, 492, 434, 724, 2029, 13], "temperature": 0.0, "avg_logprob": -0.10597371480551111, "compression_ratio": 1.8, "no_speech_prob": 1.8631282728165388e-05}, {"id": 174, "seek": 103700, "start": 1042.0, "end": 1053.0, "text": " The batch norm and hidden to output. So this also means that we're so before we were just applying hidden to output at the end, which was the last linear layer.", "tokens": [440, 15245, 2026, 293, 7633, 281, 5598, 13, 407, 341, 611, 1355, 300, 321, 434, 370, 949, 321, 645, 445, 9275, 7633, 281, 5598, 412, 264, 917, 11, 597, 390, 264, 1036, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.10597371480551111, "compression_ratio": 1.8, "no_speech_prob": 1.8631282728165388e-05}, {"id": 175, "seek": 103700, "start": 1053.0, "end": 1059.0, "text": " So you see that here hidden to output linear layer just shows up at the very end.", "tokens": [407, 291, 536, 300, 510, 7633, 281, 5598, 8213, 4583, 445, 3110, 493, 412, 264, 588, 917, 13], "temperature": 0.0, "avg_logprob": -0.10597371480551111, "compression_ratio": 1.8, "no_speech_prob": 1.8631282728165388e-05}, {"id": 176, "seek": 103700, "start": 1059.0, "end": 1061.0, "text": " We're applying that to our hidden state.", "tokens": [492, 434, 9275, 300, 281, 527, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.10597371480551111, "compression_ratio": 1.8, "no_speech_prob": 1.8631282728165388e-05}, {"id": 177, "seek": 106100, "start": 1061.0, "end": 1071.0, "text": " Now we've moved that inside the loop and we are keeping track of those from each state.", "tokens": [823, 321, 600, 4259, 300, 1854, 264, 6367, 293, 321, 366, 5145, 2837, 295, 729, 490, 1184, 1785, 13], "temperature": 0.0, "avg_logprob": -0.08271399983819926, "compression_ratio": 1.460431654676259, "no_speech_prob": 4.222774350637337e-06}, {"id": 178, "seek": 106100, "start": 1071.0, "end": 1074.0, "text": " So what that looks like kind of returning to the slideshow.", "tokens": [407, 437, 300, 1542, 411, 733, 295, 12678, 281, 264, 9788, 4286, 13], "temperature": 0.0, "avg_logprob": -0.08271399983819926, "compression_ratio": 1.460431654676259, "no_speech_prob": 4.222774350637337e-06}, {"id": 179, "seek": 106100, "start": 1074.0, "end": 1078.0, "text": " So notice this is what we had before with the for loop.", "tokens": [407, 3449, 341, 307, 437, 321, 632, 949, 365, 264, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.08271399983819926, "compression_ratio": 1.460431654676259, "no_speech_prob": 4.222774350637337e-06}, {"id": 180, "seek": 107800, "start": 1078.0, "end": 1091.0, "text": " Now we've moved our output inside the for loop because we're taking an output after each step because we want to check our accuracy after each step.", "tokens": [823, 321, 600, 4259, 527, 5598, 1854, 264, 337, 6367, 570, 321, 434, 1940, 364, 5598, 934, 1184, 1823, 570, 321, 528, 281, 1520, 527, 14170, 934, 1184, 1823, 13], "temperature": 0.0, "avg_logprob": -0.08666165669759114, "compression_ratio": 1.5379310344827586, "no_speech_prob": 7.527860816480825e-06}, {"id": 181, "seek": 107800, "start": 1091.0, "end": 1098.0, "text": " Questions about this.", "tokens": [27738, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.08666165669759114, "compression_ratio": 1.5379310344827586, "no_speech_prob": 7.527860816480825e-06}, {"id": 182, "seek": 107800, "start": 1098.0, "end": 1105.0, "text": " So what would you expect this to do to our accuracy?", "tokens": [407, 437, 576, 291, 2066, 341, 281, 360, 281, 527, 14170, 30], "temperature": 0.0, "avg_logprob": -0.08666165669759114, "compression_ratio": 1.5379310344827586, "no_speech_prob": 7.527860816480825e-06}, {"id": 183, "seek": 110500, "start": 1105.0, "end": 1114.0, "text": " I see some people pointing up. So in some ways this is going to have an effect of improving our accuracy because we've added a lot more data.", "tokens": [286, 536, 512, 561, 12166, 493, 13, 407, 294, 512, 2098, 341, 307, 516, 281, 362, 364, 1802, 295, 11470, 527, 14170, 570, 321, 600, 3869, 257, 688, 544, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08423516226977837, "compression_ratio": 1.6898148148148149, "no_speech_prob": 8.219567098421976e-05}, {"id": 184, "seek": 110500, "start": 1114.0, "end": 1122.0, "text": " Although in some ways we've also have a harder task because when we're predicting word for we only have three pieces of history.", "tokens": [5780, 294, 512, 2098, 321, 600, 611, 362, 257, 6081, 5633, 570, 562, 321, 434, 32884, 1349, 337, 321, 787, 362, 1045, 3755, 295, 2503, 13], "temperature": 0.0, "avg_logprob": -0.08423516226977837, "compression_ratio": 1.6898148148148149, "no_speech_prob": 8.219567098421976e-05}, {"id": 185, "seek": 110500, "start": 1122.0, "end": 1131.0, "text": " Whereas previously we always had 70 steps of history before we predict predicted the 70 first.", "tokens": [13813, 8046, 321, 1009, 632, 5285, 4439, 295, 2503, 949, 321, 6069, 19147, 264, 5285, 700, 13], "temperature": 0.0, "avg_logprob": -0.08423516226977837, "compression_ratio": 1.6898148148148149, "no_speech_prob": 8.219567098421976e-05}, {"id": 186, "seek": 113100, "start": 1131.0, "end": 1138.0, "text": " And so if you look at this the accuracy has actually gone down since it's we have more data which was a positive.", "tokens": [400, 370, 498, 291, 574, 412, 341, 264, 14170, 575, 767, 2780, 760, 1670, 309, 311, 321, 362, 544, 1412, 597, 390, 257, 3353, 13], "temperature": 0.0, "avg_logprob": -0.07867423323697822, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.24730210902635e-05}, {"id": 187, "seek": 113100, "start": 1138.0, "end": 1143.0, "text": " But then we're doing something harder. So that's that's reduced our accuracy.", "tokens": [583, 550, 321, 434, 884, 746, 6081, 13, 407, 300, 311, 300, 311, 9212, 527, 14170, 13], "temperature": 0.0, "avg_logprob": -0.07867423323697822, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.24730210902635e-05}, {"id": 188, "seek": 113100, "start": 1143.0, "end": 1151.0, "text": " Basically for any word K for K less than 70 we have less history than we did before.", "tokens": [8537, 337, 604, 1349, 591, 337, 591, 1570, 813, 5285, 321, 362, 1570, 2503, 813, 321, 630, 949, 13], "temperature": 0.0, "avg_logprob": -0.07867423323697822, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.24730210902635e-05}, {"id": 189, "seek": 113100, "start": 1151.0, "end": 1156.0, "text": " Fortunately this is solvable. What we can do is save our history.", "tokens": [20652, 341, 307, 1404, 17915, 13, 708, 321, 393, 360, 307, 3155, 527, 2503, 13], "temperature": 0.0, "avg_logprob": -0.07867423323697822, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.24730210902635e-05}, {"id": 190, "seek": 115600, "start": 1156.0, "end": 1165.0, "text": " And so I mentioned before how the first the first line on batch two is basically just a continuation of the first line from batch one.", "tokens": [400, 370, 286, 2835, 949, 577, 264, 700, 264, 700, 1622, 322, 15245, 732, 307, 1936, 445, 257, 29357, 295, 264, 700, 1622, 490, 15245, 472, 13], "temperature": 0.0, "avg_logprob": -0.05337533083829013, "compression_ratio": 1.7918367346938775, "no_speech_prob": 6.30201684543863e-05}, {"id": 191, "seek": 115600, "start": 1165.0, "end": 1176.0, "text": " So we can use use that history. So we'll need to keep the hidden state from the previous line of text so that we're not starting over from scratch.", "tokens": [407, 321, 393, 764, 764, 300, 2503, 13, 407, 321, 603, 643, 281, 1066, 264, 7633, 1785, 490, 264, 3894, 1622, 295, 2487, 370, 300, 321, 434, 406, 2891, 670, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.05337533083829013, "compression_ratio": 1.7918367346938775, "no_speech_prob": 6.30201684543863e-05}, {"id": 192, "seek": 115600, "start": 1176.0, "end": 1185.0, "text": " So instead of saying you know each new line of text we're totally starting from the beginning because we've been smart about how we constructed our batches.", "tokens": [407, 2602, 295, 1566, 291, 458, 1184, 777, 1622, 295, 2487, 321, 434, 3879, 2891, 490, 264, 2863, 570, 321, 600, 668, 4069, 466, 577, 321, 17083, 527, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.05337533083829013, "compression_ratio": 1.7918367346938775, "no_speech_prob": 6.30201684543863e-05}, {"id": 193, "seek": 118500, "start": 1185.0, "end": 1192.0, "text": " We can say oh OK with each new line of text that's really just a continuation of the previous line.", "tokens": [492, 393, 584, 1954, 2264, 365, 1184, 777, 1622, 295, 2487, 300, 311, 534, 445, 257, 29357, 295, 264, 3894, 1622, 13], "temperature": 0.0, "avg_logprob": -0.06610105805477853, "compression_ratio": 1.4743589743589745, "no_speech_prob": 1.4063662092667073e-05}, {"id": 194, "seek": 118500, "start": 1192.0, "end": 1199.0, "text": " And so what that involves is kind of stacking up these results as we go.", "tokens": [400, 370, 437, 300, 11626, 307, 733, 295, 41376, 493, 613, 3542, 382, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.06610105805477853, "compression_ratio": 1.4743589743589745, "no_speech_prob": 1.4063662092667073e-05}, {"id": 195, "seek": 118500, "start": 1199.0, "end": 1205.0, "text": " So we're continuing to kind of append them to our result.", "tokens": [407, 321, 434, 9289, 281, 733, 295, 34116, 552, 281, 527, 1874, 13], "temperature": 0.0, "avg_logprob": -0.06610105805477853, "compression_ratio": 1.4743589743589745, "no_speech_prob": 1.4063662092667073e-05}, {"id": 196, "seek": 120500, "start": 1205.0, "end": 1223.0, "text": " And then we are setting self dot H to to that hidden state. So we'll have it for the next time we go through the forward forward step.", "tokens": [400, 550, 321, 366, 3287, 2698, 5893, 389, 281, 281, 300, 7633, 1785, 13, 407, 321, 603, 362, 309, 337, 264, 958, 565, 321, 352, 807, 264, 2128, 2128, 1823, 13], "temperature": 0.0, "avg_logprob": -0.08224198174855066, "compression_ratio": 1.4742857142857142, "no_speech_prob": 5.954968401056249e-06}, {"id": 197, "seek": 120500, "start": 1223.0, "end": 1233.0, "text": " And we can see now our accuracy. So previously kind of I guess in the initial one we were getting like 45 percent accuracy.", "tokens": [400, 321, 393, 536, 586, 527, 14170, 13, 407, 8046, 733, 295, 286, 2041, 294, 264, 5883, 472, 321, 645, 1242, 411, 6905, 3043, 14170, 13], "temperature": 0.0, "avg_logprob": -0.08224198174855066, "compression_ratio": 1.4742857142857142, "no_speech_prob": 5.954968401056249e-06}, {"id": 198, "seek": 123300, "start": 1233.0, "end": 1241.0, "text": " Then when we started made our task harder by predicting it each time set a step it dropped to 33 percent accuracy.", "tokens": [1396, 562, 321, 1409, 1027, 527, 5633, 6081, 538, 32884, 309, 1184, 565, 992, 257, 1823, 309, 8119, 281, 11816, 3043, 14170, 13], "temperature": 0.0, "avg_logprob": -0.08680220592169115, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.406364026479423e-05}, {"id": 199, "seek": 123300, "start": 1241.0, "end": 1246.0, "text": " And now that we're saying saving this history we're up to 58 percent accuracy.", "tokens": [400, 586, 300, 321, 434, 1566, 6816, 341, 2503, 321, 434, 493, 281, 21786, 3043, 14170, 13], "temperature": 0.0, "avg_logprob": -0.08680220592169115, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.406364026479423e-05}, {"id": 200, "seek": 123300, "start": 1246.0, "end": 1253.0, "text": " So we're kind of using our data and we're also using it smartly and kind of holding on to it.", "tokens": [407, 321, 434, 733, 295, 1228, 527, 1412, 293, 321, 434, 611, 1228, 309, 4069, 356, 293, 733, 295, 5061, 322, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.08680220592169115, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.406364026479423e-05}, {"id": 201, "seek": 123300, "start": 1253.0, "end": 1257.0, "text": " Let me see if I have.", "tokens": [961, 385, 536, 498, 286, 362, 13], "temperature": 0.0, "avg_logprob": -0.08680220592169115, "compression_ratio": 1.5685279187817258, "no_speech_prob": 1.406364026479423e-05}, {"id": 202, "seek": 125700, "start": 1257.0, "end": 1264.0, "text": " No. So questions questions about this.", "tokens": [883, 13, 407, 1651, 1651, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.051395503913655, "compression_ratio": 1.6342857142857143, "no_speech_prob": 1.7230899175046943e-05}, {"id": 203, "seek": 125700, "start": 1264.0, "end": 1273.0, "text": " And so this is the picture of what we've what we've been doing of kind of recording that output within the loop.", "tokens": [400, 370, 341, 307, 264, 3036, 295, 437, 321, 600, 437, 321, 600, 668, 884, 295, 733, 295, 6613, 300, 5598, 1951, 264, 6367, 13], "temperature": 0.0, "avg_logprob": -0.051395503913655, "compression_ratio": 1.6342857142857143, "no_speech_prob": 1.7230899175046943e-05}, {"id": 204, "seek": 125700, "start": 1273.0, "end": 1285.0, "text": " And then I guess this is not pictured but knowing that when you start over again you've state saved your state from the previous time.", "tokens": [400, 550, 286, 2041, 341, 307, 406, 49896, 457, 5276, 300, 562, 291, 722, 670, 797, 291, 600, 1785, 6624, 428, 1785, 490, 264, 3894, 565, 13], "temperature": 0.0, "avg_logprob": -0.051395503913655, "compression_ratio": 1.6342857142857143, "no_speech_prob": 1.7230899175046943e-05}, {"id": 205, "seek": 128500, "start": 1285.0, "end": 1290.0, "text": " All right. So it's a it's possible to improve this even more.", "tokens": [1057, 558, 13, 407, 309, 311, 257, 309, 311, 1944, 281, 3470, 341, 754, 544, 13], "temperature": 0.0, "avg_logprob": -0.10736977685358107, "compression_ratio": 1.6212765957446809, "no_speech_prob": 2.1111178284627385e-05}, {"id": 206, "seek": 128500, "start": 1290.0, "end": 1295.0, "text": " So first step first we're going to refactor using pie torches RNN.", "tokens": [407, 700, 1823, 700, 321, 434, 516, 281, 1895, 15104, 1228, 1730, 3930, 3781, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.10736977685358107, "compression_ratio": 1.6212765957446809, "no_speech_prob": 2.1111178284627385e-05}, {"id": 207, "seek": 128500, "start": 1295.0, "end": 1300.0, "text": " And this is something where up here you typically wouldn't wouldn't be writing this from scratch.", "tokens": [400, 341, 307, 746, 689, 493, 510, 291, 5850, 2759, 380, 2759, 380, 312, 3579, 341, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.10736977685358107, "compression_ratio": 1.6212765957446809, "no_speech_prob": 2.1111178284627385e-05}, {"id": 208, "seek": 128500, "start": 1300.0, "end": 1312.0, "text": " But I wanted you to see kind of what pie torches RNN or whatever the library of your choices what's going on inside because it's you know it sounds fancy.", "tokens": [583, 286, 1415, 291, 281, 536, 733, 295, 437, 1730, 3930, 3781, 45702, 45, 420, 2035, 264, 6405, 295, 428, 7994, 437, 311, 516, 322, 1854, 570, 309, 311, 291, 458, 309, 3263, 10247, 13], "temperature": 0.0, "avg_logprob": -0.10736977685358107, "compression_ratio": 1.6212765957446809, "no_speech_prob": 2.1111178284627385e-05}, {"id": 209, "seek": 131200, "start": 1312.0, "end": 1328.0, "text": " It's really kind of a for loop just combining these you know adding your hidden state from the previous step to your your new input and then taking a linear transformation of that a rel you and a batch norm.", "tokens": [467, 311, 534, 733, 295, 257, 337, 6367, 445, 21928, 613, 291, 458, 5127, 428, 7633, 1785, 490, 264, 3894, 1823, 281, 428, 428, 777, 4846, 293, 550, 1940, 257, 8213, 9887, 295, 300, 257, 1039, 291, 293, 257, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.15070541590860445, "compression_ratio": 1.5326633165829147, "no_speech_prob": 8.93942251423141e-06}, {"id": 210, "seek": 131200, "start": 1328.0, "end": 1332.0, "text": " So kind of the building blocks of neural nets.", "tokens": [407, 733, 295, 264, 2390, 8474, 295, 18161, 36170, 13], "temperature": 0.0, "avg_logprob": -0.15070541590860445, "compression_ratio": 1.5326633165829147, "no_speech_prob": 8.93942251423141e-06}, {"id": 211, "seek": 131200, "start": 1332.0, "end": 1340.0, "text": " So we'll rewrite this with and RNN from pie torch.", "tokens": [407, 321, 603, 28132, 341, 365, 293, 45702, 45, 490, 1730, 27822, 13], "temperature": 0.0, "avg_logprob": -0.15070541590860445, "compression_ratio": 1.5326633165829147, "no_speech_prob": 8.93942251423141e-06}, {"id": 212, "seek": 134000, "start": 1340.0, "end": 1346.0, "text": " So now that that handles it for us.", "tokens": [407, 586, 300, 300, 18722, 309, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.14119924398568962, "compression_ratio": 1.4464285714285714, "no_speech_prob": 1.952415914274752e-05}, {"id": 213, "seek": 134000, "start": 1346.0, "end": 1359.0, "text": " And if we go through this is even oh I guess we ran it for more time steps. So we're still getting the same accuracy after 10 time steps but I ran it for 20 and it improved slightly to 60 61 percent.", "tokens": [400, 498, 321, 352, 807, 341, 307, 754, 1954, 286, 2041, 321, 5872, 309, 337, 544, 565, 4439, 13, 407, 321, 434, 920, 1242, 264, 912, 14170, 934, 1266, 565, 4439, 457, 286, 5872, 309, 337, 945, 293, 309, 9689, 4748, 281, 4060, 28294, 3043, 13], "temperature": 0.0, "avg_logprob": -0.14119924398568962, "compression_ratio": 1.4464285714285714, "no_speech_prob": 1.952415914274752e-05}, {"id": 214, "seek": 134000, "start": 1359.0, "end": 1361.0, "text": " Jeremy.", "tokens": [17809, 13], "temperature": 0.0, "avg_logprob": -0.14119924398568962, "compression_ratio": 1.4464285714285714, "no_speech_prob": 1.952415914274752e-05}, {"id": 215, "seek": 136100, "start": 1361.0, "end": 1370.0, "text": " Let me give you the microphone.", "tokens": [961, 385, 976, 291, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.125714391820571, "compression_ratio": 1.5372340425531914, "no_speech_prob": 1.4280486539064441e-05}, {"id": 216, "seek": 136100, "start": 1370.0, "end": 1376.0, "text": " I was just going to say one difference is that RNN RNN doesn't have batch norm in the loop.", "tokens": [286, 390, 445, 516, 281, 584, 472, 2649, 307, 300, 45702, 45, 45702, 45, 1177, 380, 362, 15245, 2026, 294, 264, 6367, 13], "temperature": 0.0, "avg_logprob": -0.125714391820571, "compression_ratio": 1.5372340425531914, "no_speech_prob": 1.4280486539064441e-05}, {"id": 217, "seek": 136100, "start": 1376.0, "end": 1377.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.125714391820571, "compression_ratio": 1.5372340425531914, "no_speech_prob": 1.4280486539064441e-05}, {"id": 218, "seek": 136100, "start": 1377.0, "end": 1381.0, "text": " The batch norm now you're only applying to the end.", "tokens": [440, 15245, 2026, 586, 291, 434, 787, 9275, 281, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.125714391820571, "compression_ratio": 1.5372340425531914, "no_speech_prob": 1.4280486539064441e-05}, {"id": 219, "seek": 136100, "start": 1381.0, "end": 1387.0, "text": " That's a good point. Yes. And so here you can see us apply it at the end but it's not inside the loop.", "tokens": [663, 311, 257, 665, 935, 13, 1079, 13, 400, 370, 510, 291, 393, 536, 505, 3079, 309, 412, 264, 917, 457, 309, 311, 406, 1854, 264, 6367, 13], "temperature": 0.0, "avg_logprob": -0.125714391820571, "compression_ratio": 1.5372340425531914, "no_speech_prob": 1.4280486539064441e-05}, {"id": 220, "seek": 138700, "start": 1387.0, "end": 1406.0, "text": " The other thing I wanted to mention is the loop version is a lot slower because RNN RNN writes that loop in CUDA C so it runs on the GPU.", "tokens": [440, 661, 551, 286, 1415, 281, 2152, 307, 264, 6367, 3037, 307, 257, 688, 14009, 570, 45702, 45, 45702, 45, 13657, 300, 6367, 294, 29777, 7509, 383, 370, 309, 6676, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.17977591564780787, "compression_ratio": 1.2123893805309736, "no_speech_prob": 3.269482112955302e-05}, {"id": 221, "seek": 140600, "start": 1406.0, "end": 1417.0, "text": " Where else the Python version has to like say to the GPU run one step of the loop and another step of the loop and another step of the loop and each of those takes a lot of time.", "tokens": [2305, 1646, 264, 15329, 3037, 575, 281, 411, 584, 281, 264, 18407, 1190, 472, 1823, 295, 264, 6367, 293, 1071, 1823, 295, 264, 6367, 293, 1071, 1823, 295, 264, 6367, 293, 1184, 295, 729, 2516, 257, 688, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.11531068881352742, "compression_ratio": 1.8272727272727274, "no_speech_prob": 5.057869202573784e-05}, {"id": 222, "seek": 140600, "start": 1417.0, "end": 1427.0, "text": " So that's one of the really annoying things about working with RNN is that they're not really fast enough to write them in pie torch loops like that.", "tokens": [407, 300, 311, 472, 295, 264, 534, 11304, 721, 466, 1364, 365, 45702, 45, 307, 300, 436, 434, 406, 534, 2370, 1547, 281, 2464, 552, 294, 1730, 27822, 16121, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.11531068881352742, "compression_ratio": 1.8272727272727274, "no_speech_prob": 5.057869202573784e-05}, {"id": 223, "seek": 140600, "start": 1427.0, "end": 1433.0, "text": " So you kind of have to work with the existing machinery that's out there.", "tokens": [407, 291, 733, 295, 362, 281, 589, 365, 264, 6741, 27302, 300, 311, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.11531068881352742, "compression_ratio": 1.8272727272727274, "no_speech_prob": 5.057869202573784e-05}, {"id": 224, "seek": 143300, "start": 1433.0, "end": 1436.0, "text": " Well thanks.", "tokens": [1042, 3231, 13], "temperature": 0.0, "avg_logprob": -0.16293125465267994, "compression_ratio": 1.4110429447852761, "no_speech_prob": 4.3309733882779256e-05}, {"id": 225, "seek": 143300, "start": 1436.0, "end": 1449.0, "text": " Yeah, so this previous for loop version is not something that you would want to be doing in practice, but it is is helpful to understand.", "tokens": [865, 11, 370, 341, 3894, 337, 6367, 3037, 307, 406, 746, 300, 291, 576, 528, 281, 312, 884, 294, 3124, 11, 457, 309, 307, 307, 4961, 281, 1223, 13], "temperature": 0.0, "avg_logprob": -0.16293125465267994, "compression_ratio": 1.4110429447852761, "no_speech_prob": 4.3309733882779256e-05}, {"id": 226, "seek": 143300, "start": 1449.0, "end": 1452.0, "text": " So these.", "tokens": [407, 613, 13], "temperature": 0.0, "avg_logprob": -0.16293125465267994, "compression_ratio": 1.4110429447852761, "no_speech_prob": 4.3309733882779256e-05}, {"id": 227, "seek": 143300, "start": 1452.0, "end": 1456.0, "text": " There are many things that are tricky about RNN and dealing with RNN.", "tokens": [821, 366, 867, 721, 300, 366, 12414, 466, 45702, 45, 293, 6260, 365, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.16293125465267994, "compression_ratio": 1.4110429447852761, "no_speech_prob": 4.3309733882779256e-05}, {"id": 228, "seek": 145600, "start": 1456.0, "end": 1463.0, "text": " And one is that when you have longer time scales and deeper networks that can become very difficult to train.", "tokens": [400, 472, 307, 300, 562, 291, 362, 2854, 565, 17408, 293, 7731, 9590, 300, 393, 1813, 588, 2252, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.08373597264289856, "compression_ratio": 1.8097345132743363, "no_speech_prob": 2.014398887695279e-05}, {"id": 229, "seek": 145600, "start": 1463.0, "end": 1473.0, "text": " And so one way to address this is to add kind of mini neural net that will keep track of how much of the green arrow and how much of the orange arrow to keep.", "tokens": [400, 370, 472, 636, 281, 2985, 341, 307, 281, 909, 733, 295, 8382, 18161, 2533, 300, 486, 1066, 2837, 295, 577, 709, 295, 264, 3092, 11610, 293, 577, 709, 295, 264, 7671, 11610, 281, 1066, 13], "temperature": 0.0, "avg_logprob": -0.08373597264289856, "compression_ratio": 1.8097345132743363, "no_speech_prob": 2.014398887695279e-05}, {"id": 230, "seek": 145600, "start": 1473.0, "end": 1485.0, "text": " So kind of going back to the picture briefly, kind of deciding at each step how much from this part and how much from this part do you want.", "tokens": [407, 733, 295, 516, 646, 281, 264, 3036, 10515, 11, 733, 295, 17990, 412, 1184, 1823, 577, 709, 490, 341, 644, 293, 577, 709, 490, 341, 644, 360, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.08373597264289856, "compression_ratio": 1.8097345132743363, "no_speech_prob": 2.014398887695279e-05}, {"id": 231, "seek": 148500, "start": 1485.0, "end": 1493.0, "text": " And the kind of the mini neural nets that are commonly used for this are called either GRUs or LSTMs are kind of the two most common types.", "tokens": [400, 264, 733, 295, 264, 8382, 18161, 36170, 300, 366, 12719, 1143, 337, 341, 366, 1219, 2139, 10903, 29211, 420, 441, 6840, 26386, 366, 733, 295, 264, 732, 881, 2689, 3467, 13], "temperature": 0.0, "avg_logprob": -0.1414368369362571, "compression_ratio": 1.5896226415094339, "no_speech_prob": 5.474884528666735e-05}, {"id": 232, "seek": 148500, "start": 1493.0, "end": 1499.0, "text": " And we'll go in a I think in a later lesson we'll talk a little bit more about what those are doing under the under the hood.", "tokens": [400, 321, 603, 352, 294, 257, 286, 519, 294, 257, 1780, 6898, 321, 603, 751, 257, 707, 857, 544, 466, 437, 729, 366, 884, 833, 264, 833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.1414368369362571, "compression_ratio": 1.5896226415094339, "no_speech_prob": 5.474884528666735e-05}, {"id": 233, "seek": 148500, "start": 1499.0, "end": 1507.0, "text": " But for now we'll use pie torches implementation of a GRU and add that.", "tokens": [583, 337, 586, 321, 603, 764, 1730, 3930, 3781, 11420, 295, 257, 10903, 52, 293, 909, 300, 13], "temperature": 0.0, "avg_logprob": -0.1414368369362571, "compression_ratio": 1.5896226415094339, "no_speech_prob": 5.474884528666735e-05}, {"id": 234, "seek": 150700, "start": 1507.0, "end": 1515.0, "text": " And what that actually.", "tokens": [400, 437, 300, 767, 13], "temperature": 0.0, "avg_logprob": -0.13793063163757324, "compression_ratio": 1.0808080808080809, "no_speech_prob": 2.668570050445851e-05}, {"id": 235, "seek": 150700, "start": 1515.0, "end": 1517.0, "text": " So.", "tokens": [407, 13], "temperature": 0.0, "avg_logprob": -0.13793063163757324, "compression_ratio": 1.0808080808080809, "no_speech_prob": 2.668570050445851e-05}, {"id": 236, "seek": 150700, "start": 1517.0, "end": 1526.0, "text": " Adding a GRU.", "tokens": [31204, 257, 10903, 52, 13], "temperature": 0.0, "avg_logprob": -0.13793063163757324, "compression_ratio": 1.0808080808080809, "no_speech_prob": 2.668570050445851e-05}, {"id": 237, "seek": 150700, "start": 1526.0, "end": 1535.0, "text": " Kind of in the place of where we had just our general RNN before.", "tokens": [9242, 295, 294, 264, 1081, 295, 689, 321, 632, 445, 527, 2674, 45702, 45, 949, 13], "temperature": 0.0, "avg_logprob": -0.13793063163757324, "compression_ratio": 1.0808080808080809, "no_speech_prob": 2.668570050445851e-05}, {"id": 238, "seek": 153500, "start": 1535.0, "end": 1538.0, "text": " Kind of the same the same forward step.", "tokens": [9242, 295, 264, 912, 264, 912, 2128, 1823, 13], "temperature": 0.0, "avg_logprob": -0.1324018947148727, "compression_ratio": 1.5129870129870129, "no_speech_prob": 2.2123209419078194e-05}, {"id": 239, "seek": 153500, "start": 1538.0, "end": 1542.0, "text": " And this improves our accuracy to 80 percent.", "tokens": [400, 341, 24771, 527, 14170, 281, 4688, 3043, 13], "temperature": 0.0, "avg_logprob": -0.1324018947148727, "compression_ratio": 1.5129870129870129, "no_speech_prob": 2.2123209419078194e-05}, {"id": 240, "seek": 153500, "start": 1542.0, "end": 1550.0, "text": " So that's kind of a huge improvement from before where we were getting around 60 percent.", "tokens": [407, 300, 311, 733, 295, 257, 2603, 10444, 490, 949, 689, 321, 645, 1242, 926, 4060, 3043, 13], "temperature": 0.0, "avg_logprob": -0.1324018947148727, "compression_ratio": 1.5129870129870129, "no_speech_prob": 2.2123209419078194e-05}, {"id": 241, "seek": 153500, "start": 1550.0, "end": 1556.0, "text": " And so kind of looking at the picture what this is doing.", "tokens": [400, 370, 733, 295, 1237, 412, 264, 3036, 437, 341, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.1324018947148727, "compression_ratio": 1.5129870129870129, "no_speech_prob": 2.2123209419078194e-05}, {"id": 242, "seek": 155600, "start": 1556.0, "end": 1565.0, "text": " Is kind of adding this other.", "tokens": [1119, 733, 295, 5127, 341, 661, 13], "temperature": 0.0, "avg_logprob": -0.14197305115786465, "compression_ratio": 0.7837837837837838, "no_speech_prob": 2.2825390260550193e-05}, {"id": 243, "seek": 156500, "start": 1565.0, "end": 1587.0, "text": " Stacked RNN.", "tokens": [745, 25949, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.23679653803507486, "compression_ratio": 0.6, "no_speech_prob": 6.853872946521733e-06}, {"id": 244, "seek": 158700, "start": 1587.0, "end": 1600.0, "text": " So kind of tying this back into what you saw in the previous lesson about ULM fit there we were basically swapping out the hidden to output with a classifier in order to do classification.", "tokens": [407, 733, 295, 32405, 341, 646, 666, 437, 291, 1866, 294, 264, 3894, 6898, 466, 624, 43, 44, 3318, 456, 321, 645, 1936, 1693, 10534, 484, 264, 7633, 281, 5598, 365, 257, 1508, 9902, 294, 1668, 281, 360, 21538, 13], "temperature": 0.0, "avg_logprob": -0.12891629813373953, "compression_ratio": 1.5978260869565217, "no_speech_prob": 9.222596418112516e-06}, {"id": 245, "seek": 158700, "start": 1600.0, "end": 1609.0, "text": " So kind of here and all these our last step has been going hit into output which has been a linear layer.", "tokens": [407, 733, 295, 510, 293, 439, 613, 527, 1036, 1823, 575, 668, 516, 2045, 666, 5598, 597, 575, 668, 257, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12891629813373953, "compression_ratio": 1.5978260869565217, "no_speech_prob": 9.222596418112516e-06}, {"id": 246, "seek": 160900, "start": 1609.0, "end": 1625.0, "text": " To kind of get which token we're predicting comes next whereas previously we were doing classification with kind of the sentiment analysis on IMDB.", "tokens": [1407, 733, 295, 483, 597, 14862, 321, 434, 32884, 1487, 958, 9735, 8046, 321, 645, 884, 21538, 365, 733, 295, 264, 16149, 5215, 322, 21463, 27735, 13], "temperature": 0.0, "avg_logprob": -0.11061263935906547, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.6700105334166437e-05}, {"id": 247, "seek": 160900, "start": 1625.0, "end": 1633.0, "text": " And so RNNs are just a refactored fully connected neural network to show one kind of in its.", "tokens": [400, 370, 45702, 45, 82, 366, 445, 257, 1895, 578, 2769, 4498, 4582, 18161, 3209, 281, 855, 472, 733, 295, 294, 1080, 13], "temperature": 0.0, "avg_logprob": -0.11061263935906547, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.6700105334166437e-05}, {"id": 248, "seek": 163300, "start": 1633.0, "end": 1639.0, "text": " These are whoops unrolled stacked RNNs.", "tokens": [1981, 366, 567, 3370, 517, 28850, 28867, 45702, 45, 82, 13], "temperature": 0.0, "avg_logprob": -0.16240878815346577, "compression_ratio": 1.3083333333333333, "no_speech_prob": 1.4970223674026784e-05}, {"id": 249, "seek": 163300, "start": 1639.0, "end": 1642.0, "text": " You can see look like this where you're kind of feeding in.", "tokens": [509, 393, 536, 574, 411, 341, 689, 291, 434, 733, 295, 12919, 294, 13], "temperature": 0.0, "avg_logprob": -0.16240878815346577, "compression_ratio": 1.3083333333333333, "no_speech_prob": 1.4970223674026784e-05}, {"id": 250, "seek": 163300, "start": 1642.0, "end": 1650.0, "text": " So this is another way of kind of redrawing this picture.", "tokens": [407, 341, 307, 1071, 636, 295, 733, 295, 2182, 5131, 278, 341, 3036, 13], "temperature": 0.0, "avg_logprob": -0.16240878815346577, "compression_ratio": 1.3083333333333333, "no_speech_prob": 1.4970223674026784e-05}, {"id": 251, "seek": 165000, "start": 1650.0, "end": 1670.0, "text": " And the same approach so here we're just looking at translation but there are a number of sequence to sequence tasks such as sequence labeling such as part of speech identifying if words should be kind of removed because they're sensitive material that you don't want to share.", "tokens": [400, 264, 912, 3109, 370, 510, 321, 434, 445, 1237, 412, 12853, 457, 456, 366, 257, 1230, 295, 8310, 281, 8310, 9608, 1270, 382, 8310, 40244, 1270, 382, 644, 295, 6218, 16696, 498, 2283, 820, 312, 733, 295, 7261, 570, 436, 434, 9477, 2527, 300, 291, 500, 380, 528, 281, 2073, 13], "temperature": 0.0, "avg_logprob": -0.15631614233318128, "compression_ratio": 1.7259615384615385, "no_speech_prob": 1.5441948562511243e-05}, {"id": 252, "seek": 165000, "start": 1670.0, "end": 1675.0, "text": " So there are other next I'll give a list of some other sequence to sequence task.", "tokens": [407, 456, 366, 661, 958, 286, 603, 976, 257, 1329, 295, 512, 661, 8310, 281, 8310, 5633, 13], "temperature": 0.0, "avg_logprob": -0.15631614233318128, "compression_ratio": 1.7259615384615385, "no_speech_prob": 1.5441948562511243e-05}, {"id": 253, "seek": 167500, "start": 1675.0, "end": 1681.0, "text": " A little bit later. Yes.", "tokens": [316, 707, 857, 1780, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.1881647317305855, "compression_ratio": 1.2966101694915255, "no_speech_prob": 6.106503133196384e-05}, {"id": 254, "seek": 167500, "start": 1681.0, "end": 1694.0, "text": " Sorry for a complicated architecture like what.", "tokens": [4919, 337, 257, 6179, 9482, 411, 437, 13], "temperature": 0.0, "avg_logprob": -0.1881647317305855, "compression_ratio": 1.2966101694915255, "no_speech_prob": 6.106503133196384e-05}, {"id": 255, "seek": 167500, "start": 1694.0, "end": 1698.0, "text": " I'm not sure what would you say Jeremy.", "tokens": [286, 478, 406, 988, 437, 576, 291, 584, 17809, 13], "temperature": 0.0, "avg_logprob": -0.1881647317305855, "compression_ratio": 1.2966101694915255, "no_speech_prob": 6.106503133196384e-05}, {"id": 256, "seek": 167500, "start": 1698.0, "end": 1701.0, "text": " Yes, do you want to say more about that.", "tokens": [1079, 11, 360, 291, 528, 281, 584, 544, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.1881647317305855, "compression_ratio": 1.2966101694915255, "no_speech_prob": 6.106503133196384e-05}, {"id": 257, "seek": 170100, "start": 1701.0, "end": 1710.0, "text": " I mean I say like another big issue with RNNs in general is just training them can be can be tricky.", "tokens": [286, 914, 286, 584, 411, 1071, 955, 2734, 365, 45702, 45, 82, 294, 2674, 307, 445, 3097, 552, 393, 312, 393, 312, 12414, 13], "temperature": 0.0, "avg_logprob": -0.16383493307865027, "compression_ratio": 1.3259668508287292, "no_speech_prob": 2.3186299586086534e-05}, {"id": 258, "seek": 170100, "start": 1710.0, "end": 1712.0, "text": " Yeah, it's really important.", "tokens": [865, 11, 309, 311, 534, 1021, 13], "temperature": 0.0, "avg_logprob": -0.16383493307865027, "compression_ratio": 1.3259668508287292, "no_speech_prob": 2.3186299586086534e-05}, {"id": 259, "seek": 170100, "start": 1712.0, "end": 1725.0, "text": " And the thing we talked about last week was the AWD LSTM which has all these different kinds of dropout in it.", "tokens": [400, 264, 551, 321, 2825, 466, 1036, 1243, 390, 264, 25815, 35, 441, 6840, 44, 597, 575, 439, 613, 819, 3685, 295, 3270, 346, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.16383493307865027, "compression_ratio": 1.3259668508287292, "no_speech_prob": 2.3186299586086534e-05}, {"id": 260, "seek": 172500, "start": 1725.0, "end": 1731.0, "text": " So the way we deal with overfitting one of the best ways to deal with overfitting is through regularization.", "tokens": [407, 264, 636, 321, 2028, 365, 670, 69, 2414, 472, 295, 264, 1151, 2098, 281, 2028, 365, 670, 69, 2414, 307, 807, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.15999726245277807, "compression_ratio": 1.6243386243386244, "no_speech_prob": 7.1407099312637e-05}, {"id": 261, "seek": 172500, "start": 1731.0, "end": 1744.0, "text": " And so AWD LSTM provides five times five types of dropout and we didn't talk about it but it also has two types of what's called activity regularization which is basically kind of L2 regularization.", "tokens": [400, 370, 25815, 35, 441, 6840, 44, 6417, 1732, 1413, 1732, 3467, 295, 3270, 346, 293, 321, 994, 380, 751, 466, 309, 457, 309, 611, 575, 732, 3467, 295, 437, 311, 1219, 5191, 3890, 2144, 597, 307, 1936, 733, 295, 441, 17, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.15999726245277807, "compression_ratio": 1.6243386243386244, "no_speech_prob": 7.1407099312637e-05}, {"id": 262, "seek": 174400, "start": 1744.0, "end": 1761.0, "text": " So yeah, you you can just dial up the amount of regularization when you see overfitting that anytime you're training in your own network that your approach should always be to first of all overfit and then gradually increase the amount of regularization.", "tokens": [407, 1338, 11, 291, 291, 393, 445, 5502, 493, 264, 2372, 295, 3890, 2144, 562, 291, 536, 670, 69, 2414, 300, 13038, 291, 434, 3097, 294, 428, 1065, 3209, 300, 428, 3109, 820, 1009, 312, 281, 700, 295, 439, 670, 6845, 293, 550, 13145, 3488, 264, 2372, 295, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.14438167918812145, "compression_ratio": 1.6178343949044587, "no_speech_prob": 1.2804273865185678e-05}, {"id": 263, "seek": 176100, "start": 1761.0, "end": 1774.0, "text": " So models that don't have kind of nuanced regularization capabilities are really hard to use in practice because you can't use that basic kind of training approach.", "tokens": [407, 5245, 300, 500, 380, 362, 733, 295, 45115, 3890, 2144, 10862, 366, 534, 1152, 281, 764, 294, 3124, 570, 291, 393, 380, 764, 300, 3875, 733, 295, 3097, 3109, 13], "temperature": 0.0, "avg_logprob": -0.0659863484370244, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.3178759875008836e-05}, {"id": 264, "seek": 176100, "start": 1774.0, "end": 1787.0, "text": " So that's one of the reasons why AWD LSTM and ULM fit so widely used in practice because they are they have these great controls for regularization.", "tokens": [407, 300, 311, 472, 295, 264, 4112, 983, 25815, 35, 441, 6840, 44, 293, 624, 43, 44, 3318, 370, 13371, 1143, 294, 3124, 570, 436, 366, 436, 362, 613, 869, 9003, 337, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.0659863484370244, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.3178759875008836e-05}, {"id": 265, "seek": 176100, "start": 1787.0, "end": 1789.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.0659863484370244, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.3178759875008836e-05}, {"id": 266, "seek": 178900, "start": 1789.0, "end": 1794.0, "text": " Any other questions about this notebook before we go on to the next one?", "tokens": [2639, 661, 1651, 466, 341, 21060, 949, 321, 352, 322, 281, 264, 958, 472, 30], "temperature": 0.0, "avg_logprob": -0.2391203366793119, "compression_ratio": 1.6424870466321244, "no_speech_prob": 0.00023765282821841538}, {"id": 267, "seek": 178900, "start": 1794.0, "end": 1796.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.2391203366793119, "compression_ratio": 1.6424870466321244, "no_speech_prob": 0.00023765282821841538}, {"id": 268, "seek": 178900, "start": 1796.0, "end": 1812.0, "text": " Can you explain more details about how does like GRU that add meaning and then to decide like to make the model more simpler to user train.", "tokens": [1664, 291, 2903, 544, 4365, 466, 577, 775, 411, 10903, 52, 300, 909, 3620, 293, 550, 281, 4536, 411, 281, 652, 264, 2316, 544, 18587, 281, 4195, 3847, 13], "temperature": 0.0, "avg_logprob": -0.2391203366793119, "compression_ratio": 1.6424870466321244, "no_speech_prob": 0.00023765282821841538}, {"id": 269, "seek": 178900, "start": 1812.0, "end": 1818.0, "text": " So the question was to explain more about what the GRU is doing and how it makes the model simpler.", "tokens": [407, 264, 1168, 390, 281, 2903, 544, 466, 437, 264, 10903, 52, 307, 884, 293, 577, 309, 1669, 264, 2316, 18587, 13], "temperature": 0.0, "avg_logprob": -0.2391203366793119, "compression_ratio": 1.6424870466321244, "no_speech_prob": 0.00023765282821841538}, {"id": 270, "seek": 181800, "start": 1818.0, "end": 1833.0, "text": " I mean I'd say I don't think it makes the model simpler but it's giving you a way to kind of balance how much weight you want to put on the new each new word versus the history that you have stored in your hidden state.", "tokens": [286, 914, 286, 1116, 584, 286, 500, 380, 519, 309, 1669, 264, 2316, 18587, 457, 309, 311, 2902, 291, 257, 636, 281, 733, 295, 4772, 577, 709, 3364, 291, 528, 281, 829, 322, 264, 777, 1184, 777, 1349, 5717, 264, 2503, 300, 291, 362, 12187, 294, 428, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.08507849905225966, "compression_ratio": 1.5541125541125542, "no_speech_prob": 3.9437069062842056e-05}, {"id": 271, "seek": 181800, "start": 1833.0, "end": 1841.0, "text": " For the most part though we're going to kind of treat the GRU as a black box for now and we'll hopefully come back to it in a later lesson.", "tokens": [1171, 264, 881, 644, 1673, 321, 434, 516, 281, 733, 295, 2387, 264, 10903, 52, 382, 257, 2211, 2424, 337, 586, 293, 321, 603, 4696, 808, 646, 281, 309, 294, 257, 1780, 6898, 13], "temperature": 0.0, "avg_logprob": -0.08507849905225966, "compression_ratio": 1.5541125541125542, "no_speech_prob": 3.9437069062842056e-05}, {"id": 272, "seek": 184100, "start": 1841.0, "end": 1852.0, "text": " I'm going to just kind of think of it as a little neural network that is learning so kind of before we go back to the slides.", "tokens": [286, 478, 516, 281, 445, 733, 295, 519, 295, 309, 382, 257, 707, 18161, 3209, 300, 307, 2539, 370, 733, 295, 949, 321, 352, 646, 281, 264, 9788, 13], "temperature": 0.0, "avg_logprob": -0.11666203888369278, "compression_ratio": 1.6777777777777778, "no_speech_prob": 2.75324364338303e-05}, {"id": 273, "seek": 184100, "start": 1852.0, "end": 1865.0, "text": " Before this was kind of simpler about what you had to learn like you were just kind of learning this matrix of green error of kind of your embedding and then the orange matrix.", "tokens": [4546, 341, 390, 733, 295, 18587, 466, 437, 291, 632, 281, 1466, 411, 291, 645, 445, 733, 295, 2539, 341, 8141, 295, 3092, 6713, 295, 733, 295, 428, 12240, 3584, 293, 550, 264, 7671, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11666203888369278, "compression_ratio": 1.6777777777777778, "no_speech_prob": 2.75324364338303e-05}, {"id": 274, "seek": 186500, "start": 1865.0, "end": 1874.0, "text": " The linear layer from hidden to hidden you're learning these two things and adding them together to get this activation.", "tokens": [440, 8213, 4583, 490, 7633, 281, 7633, 291, 434, 2539, 613, 732, 721, 293, 5127, 552, 1214, 281, 483, 341, 24433, 13], "temperature": 0.0, "avg_logprob": -0.08237905685718243, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.5464387363172136e-05}, {"id": 275, "seek": 186500, "start": 1874.0, "end": 1882.0, "text": " We want to do something kind of even more sophisticated which is kind of like learning the relationship or the balance between them.", "tokens": [492, 528, 281, 360, 746, 733, 295, 754, 544, 16950, 597, 307, 733, 295, 411, 2539, 264, 2480, 420, 264, 4772, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.08237905685718243, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.5464387363172136e-05}, {"id": 276, "seek": 188200, "start": 1882.0, "end": 1897.0, "text": " And so that's what the GRU is letting us do so in that way it's kind of more sophisticated what it allows but for now we're going to treat the GRU as a black box and we'll come back to it.", "tokens": [400, 370, 300, 311, 437, 264, 10903, 52, 307, 8295, 505, 360, 370, 294, 300, 636, 309, 311, 733, 295, 544, 16950, 437, 309, 4045, 457, 337, 586, 321, 434, 516, 281, 2387, 264, 10903, 52, 382, 257, 2211, 2424, 293, 321, 603, 808, 646, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.09695498432431902, "compression_ratio": 1.3785714285714286, "no_speech_prob": 4.069112765137106e-05}, {"id": 277, "seek": 188200, "start": 1897.0, "end": 1899.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.09695498432431902, "compression_ratio": 1.3785714285714286, "no_speech_prob": 4.069112765137106e-05}, {"id": 278, "seek": 189900, "start": 1899.0, "end": 1923.0, "text": " So the question was if we could go over how to do regularization for neural networks with text data.", "tokens": [407, 264, 1168, 390, 498, 321, 727, 352, 670, 577, 281, 360, 3890, 2144, 337, 18161, 9590, 365, 2487, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13304842948913576, "compression_ratio": 1.1627906976744187, "no_speech_prob": 2.885124195017852e-05}, {"id": 279, "seek": 192300, "start": 1923.0, "end": 1929.0, "text": " So that's a good request. I am not sure if we'll have time for it in this class. I'll make a note of that.", "tokens": [407, 300, 311, 257, 665, 5308, 13, 286, 669, 406, 988, 498, 321, 603, 362, 565, 337, 309, 294, 341, 1508, 13, 286, 603, 652, 257, 3637, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.18996327263968332, "compression_ratio": 1.3742690058479532, "no_speech_prob": 2.4297476556967013e-05}, {"id": 280, "seek": 192300, "start": 1929.0, "end": 1940.0, "text": " Jeremy have you covered the regularization used in AWD LSTM in the deep learning class.", "tokens": [17809, 362, 291, 5343, 264, 3890, 2144, 1143, 294, 25815, 35, 441, 6840, 44, 294, 264, 2452, 2539, 1508, 13], "temperature": 0.0, "avg_logprob": -0.18996327263968332, "compression_ratio": 1.3742690058479532, "no_speech_prob": 2.4297476556967013e-05}, {"id": 281, "seek": 192300, "start": 1940.0, "end": 1943.0, "text": " Probably in details.", "tokens": [9210, 294, 4365, 13], "temperature": 0.0, "avg_logprob": -0.18996327263968332, "compression_ratio": 1.3742690058479532, "no_speech_prob": 2.4297476556967013e-05}, {"id": 282, "seek": 192300, "start": 1943.0, "end": 1949.0, "text": " It's just drop out.", "tokens": [467, 311, 445, 3270, 484, 13], "temperature": 0.0, "avg_logprob": -0.18996327263968332, "compression_ratio": 1.3742690058479532, "no_speech_prob": 2.4297476556967013e-05}, {"id": 283, "seek": 194900, "start": 1949.0, "end": 1965.0, "text": " So it's Jeremy for the microphone. Jeremy just said it's just drop out on everything and they kind of all have different names where you're applying drop out to you'll see different names for it and I think it's even been independently invented sometimes with different names.", "tokens": [407, 309, 311, 17809, 337, 264, 10952, 13, 17809, 445, 848, 309, 311, 445, 3270, 484, 322, 1203, 293, 436, 733, 295, 439, 362, 819, 5288, 689, 291, 434, 9275, 3270, 484, 281, 291, 603, 536, 819, 5288, 337, 309, 293, 286, 519, 309, 311, 754, 668, 21761, 14479, 2171, 365, 819, 5288, 13], "temperature": 0.0, "avg_logprob": -0.15644775587936927, "compression_ratio": 1.6932515337423313, "no_speech_prob": 4.028838247904787e-06}, {"id": 284, "seek": 196500, "start": 1965.0, "end": 1985.0, "text": " The where the dropouts applied. So I'll make a note of that. I don't know that we'll have time for it but that is a good question because it's an important topic and I would I would say definitely I think like reading the AWD LSTM paper by Stephen Meridy would have more info.", "tokens": [440, 689, 264, 3270, 7711, 6456, 13, 407, 286, 603, 652, 257, 3637, 295, 300, 13, 286, 500, 380, 458, 300, 321, 603, 362, 565, 337, 309, 457, 300, 307, 257, 665, 1168, 570, 309, 311, 364, 1021, 4829, 293, 286, 576, 286, 576, 584, 2138, 286, 519, 411, 3760, 264, 25815, 35, 441, 6840, 44, 3035, 538, 13391, 6124, 38836, 576, 362, 544, 13614, 13], "temperature": 0.0, "avg_logprob": -0.14494070325578962, "compression_ratio": 1.422680412371134, "no_speech_prob": 6.239937647478655e-06}, {"id": 285, "seek": 198500, "start": 1985.0, "end": 1996.0, "text": " Okay. So Jeremy I'm saying saying this for the microphone. Jeremy said each each color arrow has its own drop out is a way to think about it.", "tokens": [50364, 1033, 13, 407, 17809, 286, 478, 1566, 1566, 341, 337, 264, 10952, 13, 17809, 848, 1184, 1184, 2017, 11610, 575, 1080, 1065, 3270, 484, 307, 257, 636, 281, 519, 466, 309, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16894786017281668, "compression_ratio": 1.2702702702702702, "no_speech_prob": 8.132744369504508e-06}], "language": "en"}