{"text": " All right, I'm really excited to announce our guest speaker today, Nikhil Garg. Nikhil is a PhD candidate in electrical engineering at Stanford, and he is a part of both the Stanford CrowdSource Democracy Team and the Society and Algorithms Lab. He has an NSF graduate research fellowship, and he has spent time at Uber, NASA, Microsoft, the Texas Senate, and IEEE's Policy Arm, which I think is a really amazing diversity of places. His work has been covered in the New York Times, Science Magazine, Smithsonian Magazine, Stanford Engineering Magazine, Stanford News, and other places. So with that, we'll welcome Nikhil, and he invites questions, and I'm going to try to use the catch box to catch these for the questions. Yeah, thank you, Nikhil. Thanks for having me. And just to emphasize, I'm going to make this more of a classroom-style talk as opposed to a research talk, so feel free to interrupt and so on. And I think that would motivate some of how I structured this and what we'll talk about at the end, hopefully. So this is some recent work on how to use word embeddings to quantify gender and ethnic stereotypes, and more generally, I think, unsupervised machine learning outputs sort of have great promise as a lens on our society. And so that's, I think, the framework for this talk. So just a quick primer. I know you all are now experts in data science and natural language processing, so I probably don't need to do this, but I'll just give a quick primer on word embeddings, which their numeric representations of words and what I deem is a low dimensional space, low because it's, I think, somewhat surprising or inaccurate to say that 300 dimensions can capture all the complexities of language. And so as I said, there are models of the world, incomplete models of the world, and such that similar words will have similar numeric representations. And a lot of prior work has shown that word embeddings can encode a whole host of things about language, semantics and tax, relationships between words, and so on. And they're primarily trained through unsupervised methods using extremely large data sets, such as all the Wikipedia, all the Google News, a corpus of every American book, or so on. And they're, of course, a building block of modern natural language processing. So prior work before ours, there's, of course, a bunch, but two that I would like to highlight, really, I think, emphasize that word embeddings can encode biases or stereotypes in language or in our society. So the first one, through a bunch of analogy type tasks, focused on gender bias and then talked about how to debias word embeddings. And then the second paper, I think it was in Science, showed that word embeddings basically replicate every human bias on the implicit association test. So gender, ethnic, what have you. Basically all of those are replicated. And what I would emphasize here is that embeddings are thus a model or represent language as it is used, not just language as we may wish it to use or sort of some formal notion of our language. And the idea of our work was that this is, of course, harmful in many contexts, and we'll get to that at the end, but it's also useful. In particular, documenting and auditing historical beliefs is something that's important to do just as studying or knowing about how beliefs have changed over time, but it's also very hard to do. There's a limited set, a very limited set of historical survey, consistent historical surveys on what did people think about certain topics over time, but they're a little sparse. And so the idea is to augment some of those historical works with the word embeddings. And here, of course, showing, sort of taking advantage of the fact that the representational power of embeddings is useful. Okay, so sort of a very high level outline of the talk is that first I'll just summarize the methodology at a very high level, and then I'll walk through a bunch of examples, on confirming that we can actually do this, because there's, of course, a bunch of potential pitfalls. I'll present some exploratory findings, but then I would actually like to spend a substantial amount of time on limitations, both in this work and this type of work in general. And especially there, I'd be happy to have a discussion. Okay, so the high level framework in the paper is actually pretty simple. We start with some embeddings, so just train embeddings on some data set. And the assumption here is that the data set is a proxy for some society of interest. And then we compile two lists of words. First is what we call group words that will, quote unquote, represent populations of interest. The ones that we use in this work are for gender, we just use a bunch of pronouns and like relationships for gender, and then for ethnic groups, we use common last names derived from the US census. And then also compile a list of what we call neutral words that represent potential characteristics that people can have, so either adjectives or occupations or what have you, whatever is of interest. Then just from the embeddings of these words, just calculate the distance between groups and neutral words, and then compare distances across groups. So if a certain characteristic is closer to one group than the other, then we would say that the embeddings are biased toward that group over the other group. And then in some cases, we can compare these relative distances to outside data just to confirm that these are capturing something real. There's a bunch of assumptions in everything I just said from the embeddings representing societies to group words and so on. I'm going to save that till the end, talking about how these assumptions are present in this work and also a lot of natural language processing in similar domains. Okay, so the first example, which also appeared in other work, so this isn't novel or anything, is look at embeddings that are trained on Google News. These are available online and just have two sets of group words, one set of group words representing men and another for women, and then the neutral words are a set of occupations that you can come up with from the census, for example. The additional data that we use to compare is a breakdown of what percentage of that occupation in the United States is each gender, and this is through census data. And then we augment that with a Mechanical Turk Survey that was actually provided to us by the authors of the first paper, the Baluk Basi paper that I put up. And here, what the authors there did was run a Mechanical Turk Survey on what workers characterize as stereotypes of each occupation. And so this is trying to see, and yeah, I'll get into this. So what this plot shows is on the Y axis is the relative distances of each occupation to either the group, to the set of words representing men or the set of words representing women in the embeddings. And then the X axis is the occupation difference by percentage in the US census, I think in 2010. So it's somewhat noisy, but there's a clear pattern that embeddings are as a whole capturing the trend in how skewed the occupations are toward one gender or the other. And we highlighted just a few sort of outlier occupations. And then we can augment this with the Mechanical Turk Survey. So what it turns out is both human stereotypes and the stereotypes in the embeddings don't actually reflect reality. Sort of here I'm defining reality as the occupation percentages. And human stereotypes tend to be sort of in addition to that sometimes. And it turns out that the embeddings capture that difference. So here, if you would see the residuals on the plots, the residuals on the plots are one noise, but also variance that's unexplained by the occupation percentage. So biases in the embeddings that are somewhat additional to the skewness and the occupations. And it turns out that these, the residuals here correlate with the mechanical Turk, with the additional residuals on the Mechanical Turk Survey as well. So sort of showing that this embeddings technique can capture both, again, I want to put underlying reality in very big air quotes, represented by the occupational data, but then also additional potential biases or stereotypes by human workers and mechanical Turk. I have a question. So with the, I guess like the distance from the line, for instance, is nurse, is this showing that kind of both the embeddings and people expect way more women to be nurses than fits the occupational data of more women are nurses, but not to the extent that the embeddings reflect? Exactly. Yes. Okay. So it's kind of like the bias has been amplified or the stereotype has been amplified. And so I don't want to make that specific claim in that at least we didn't find any like systematic pattern of like certain types of occupations is amplified or so on. It's just that the residual in either direction is explained. It's somewhat explained by the embeddings. And then so with secretary, the embedding, I guess kind of captured secretary as being more masculine than the, the occupational data. Yes. And I think the secretary one in particular might've actually been just because the disambiguation of secretary into maybe cabinet secretary in the United States and other types of secretaries. So yes, and not all of these were not all of these residuals would be captured by human stereotypes. Some of these are of course due to other technical limitations. Well, thank you. Any other questions or? Okay. So once we have this general framework, we want to sort of verify it further. And in particular, what the interest of our work was, does it, can we do this consistently over time? So not just on one embedding in a snapshot setting, but can we consistently look at biases embeddings and say 1940 versus 1960, and then say something meaningful about the differences in those biases. So in particularly, we want to verify that embeddings are calibrated to changes and whatever else biases of historical beliefs. And so one thing that we do, for example, is continuing this trend of using census data. We using the corpus of historical American English, which is essentially a gender or not gender balance, it's a sort of genre balanced collection of books of American English written throughout the 20th century. So using this corpus, we can train embeddings for every decade. And then we can capture the, as we did in the previous slide, we can do the average bias in the consistent set of occupations over time. And then we can compare that to the mean of women in those occupations throughout the 20th century as well. And sort of what we just, there's of course some noise, but what I want to emphasize here is that there's some hope for calibration is that there's a consistent mapping, roughly consistent mapping from, let's see what the bias was in 1920 to what that maps to in terms of occupation percentage. And then the same thing in 1980. We do this for sort of a few other settings where we have historical data. So there are a few sets of historical surveys that were repeated over time. So there's the famous Princeton trilogy, which a researcher in 1933, 51, and one other date that I'm forgetting, asked the set of Princeton undergraduates, their stereotypes of a whole bunch of different ethnicities. And yeah, those are somewhat disturbing and fascinating to go through. But yeah, so this researcher asked consistently very similar questions on stereotypes for ethnicities. And we can essentially just replicate that the differences in the stereotypes between 1933 and 51 are also reflected in the differences in the beddings from those decades. And of course, we use the Kinabaturk survey in the previous slide. Okay, so now I'm just going to do a fairly quick run through a bunch of various stereotypes that we quantified in the paper. These are, I want to emphasize, exploratory in the sense that we can't validate each of these against some fixed outside measurement because a lot of those outside measurements those don't exist. Also want to give the warning that I'm going to present some stereotypes in the slides. So they're going to be gender and ethnic, so if you're uncomfortable with that. So this first slide shows Asian stereotypes in the corpus of historical American English in 1910, 1950, and 1990. And so how we generated these is sort of the same high level methodology of collecting a group of, so a set of group words, which are just common Asian last names, and then a very large list of adjectives found online. And then we found which adjectives are most similar toward Asians and dissimilar toward white last names and the embeddings in 1910, 1590. And sort of there's a progression I think that you can see where 1910 is very much an outsider type of stereotype. This was before a lot of the initial waves of Asian immigration in the United States or sort of before what people considered integration. And then in 1990, these outsider stereotypes at least have been replaced with other, I mean, stereotypes. This one is just the average association between Islam and terrorism in the New York Times between 1988 and 2005. So same high level methodology here is we collected a bunch of words that we considered to represent Islam and also a bunch of words that we considered to represent terrorism and just looked at the differences between sort of how close those words were to words representing Islam versus words representing Christianity in the New York Times. And so there's some peaks after, for example, the World Trade Center bombings in 1993, as well as of course 9-11. And it would be, I think, interesting to extend this past 2005 with a larger New York Times data set. This one's, I think, a little tougher to explain. So what this is is a heat map across decades. So we have in both the X and Y axis, we have from 1910 to 1990. And these are just the correlations and how similar language is in those two decades with respect to adjectives and gender differences. So sort of the more blue the cell is, the more similarly biased or skewed adjectives are as a whole in terms of gender. And this is one that are... So one of the collaborators on the paper is actually a historian of gender and science. And this is one, I think, that she particularly found interesting is sort of there's a somewhat marked shift in the 60s and 70s when the women's movement in the United States really brought a lot of issues on how are women talked about in media or books or what have you. That at least we found somewhat of a quantification for that actually had an effect in, positive effect, I would say, in the language. Okay. So that was just a very quick run through for the types of analysis that we did for quantifying various changes over time. Now for the rest of the talk, I'm going to sort of... I promise that there's a bunch of limitations in this sort of work and I'll just discuss a few of them. So here I brought back our high level framework and now I'm just emphasizing a few words that are, I think, indicative of the assumptions that we made. So we train embeddings on some data set that we claimed represents a society of interest. So for example, we trade books from 1910 and we're claiming that these are indicative of American societal thoughts or what have you in 1910. We compiled list of words that represent categories and then claim that these words are good enough to represent those categories. And then we calculated the distance between groups of words and then we claimed or at least assumed that the distance is meaningful. Each of these steps is baked with some assumptions that I'll go through now and I think at least a few of these assumptions are pretty common or I think problematic assumptions are pretty common in a lot of this type of work. And it's something to be aware of as maybe you're pursuing similar types. Okay, so for example, we used an existing corpus of text for American English books and we claim that they represent historical American beliefs. But there's a bunch of people that this doesn't include, right? This doesn't include Americans who didn't publish books at that time. This doesn't include Americans who published books in other languages or we compiled the distance into a single metric from 1910. But what about minority ideological opinions at that time? These are all washed away in some mean of distance. And then there's a whole bunch of, I think, power or whatever issues and who created the data set and what decided what text would go into that corpus. None of these decisions are sort of very, or at least some of them who just read the paper are obvious. I, for example, don't know sort of the decisions that went into making the corpus. And then sort of the second, I think, more technical issue that there is more headway on is that embeddings themselves are black box algorithms. So there's a bunch of different ways to train embeddings, some that are ever getting improved from GloVe to Word2Vect to BERT or what have you. And then we just calculated the Euclidean distance between two vectors and we claim that that represents something about these embeddings. This one I say that there's more progress on because at least in our paper, we sort of in the appendix, we do a bunch of robustness checks that the distance function really doesn't matter, the algorithm used for the embeddings doesn't matter, and so on. And there's follow on work on, I think, using much nicer, fine grained embeddings that instead of just training different embeddings per decade, you can actually sort of have embeddings tuned and changed over time in a very smooth way. Like, not just year by year, but sort of with additional data. So I think this one, this second issue is more of a solvable problem technically, but I think the first issue is something that there's never going to be a technical, or not never, that there isn't a technical solution for at the moment. Similarly, there's, we claim that words represent a category, and in particular, for example, we use lists of words to represent binary gender, and then we use common last names to represent ethnicities. But similar set of questions. This basically forgets about non-binary or multi-ethnic people, or people whose last names may not reflect their sort of self-defined ethnicity. It makes use of historical census classifications of ethnicity as well as occupation, and sort of one very concrete limitation of the methodology is that we couldn't distinguish between white and black Americans, because it turns out, at least in the US, the overlap and the distribution of last names are pretty similar. There's of course a few exceptions, but as a whole, the last name distribution is similar between white and black Americans. So yeah, okay. So sort of stepping back to similar challenges that are present in NLP or data science work in general is that language is messy, and language as a whole can probably take care of a lot of issues that I talked about. But in computation, when we start using language as data, we by definition have to simplify, and that involves categorizing and summarizing this messy language. And this categorization and summarization process is not objective, and you can never hope to make it objective, or at least in my view. That there's all sorts of issues on who's creating the categories on how you're categorizing. So for convenience, we use binary gender because it was hard in the paper to, for example, construct groups of words that historically would be able to represent any other. And this of course ignores those who fall between categories, or maybe the categories aren't discrete enough or granular enough or aren't well defined. And so that's about categorization. And then on summarization is, again, we in the paper prefer plots or to describe things you need a single number, maybe a single number representing an entire decade of data. And this is a summary of some kind. And in summaries and means, the minority opinions tend to be washed away or not accounted for in the mean properly. And then a more general question is, a lot of these word embedding models are trained on of course large corporate data like all the Wikipedia, and then they're summarized and used to train models that are going to apply to everyone, including potentially minority opinions that were washed away in the creation of the embeddings. And that, I think a whole host of literature in the past few years has described how this is problematic in a bunch of applications. And so stepping back even further, embeddings and unsupervised learning models or just machine learning models in particular often construct incomplete models of the sometimes problematic world. And this is not always a bad thing in the sense that our work couldn't have existed if embeddings didn't capture this sort of structure. But in other applications, these stereotypes embeddings may be undesirable. And it's a question for when is it undesirable, who decides if it is, and then how do you devias and should you devias. And these are all things that are something that you should think carefully about. And something that I want to emphasize is you thinking carefully about it is probably not enough in that I'm not an expert on these issues, you probably aren't either. And you should probably collaborate with and talk to people who are. For example, this work benefited pretty greatly from talking to and collaborating with Londa, who is a historian on these issues. Yes, that's sort of all the prepared talks that I had. Definitely open to questions or discussion on these issues and links to the paper. Probably easier to Google around and then try to follow the link. Any other questions? Hi, thank you for the talk. Am I using this correctly? Okay, thanks. I'm just wondering, did you mention how were the, so the example you gave for the Muslim and terrorism, you said you select a list of words that are associated with those two and then you compare how they're the correlation of them along with time. So my first question would be, how did you pick those words that are associated with the term? And secondly, are they updated in different time periods? Great questions. Thank you. Yeah, so you described the methodology correctly, I think is pick groups of words that represent Islam and terrorism and Christianity and then look at the relative distances between the group words and the words describing terrorism over time. So I think the two questions were, and correct me if I'm wrong, is one is who picked the words and how are the words picked and were they the same over time? And I think this is exactly the type of issues that I was talking about earlier is that these are important questions that are often glossed over on like how sensitive or sort of who defines these categories. And for the most part throughout the paper, we tried to just find as most general and large lists that other people had compiled online as possible for, I believe the religious and terrorism words, we started with lists found online and then sort of like removed overlaps in some sort that would sort of skew the results. So I think like various lists online sort of, for example, use madrasa as a word describing terrorism, but that also has a more benign meaning, I think. And so like little issues like that we had to hand remove. And yeah, so it's definitely not a perfect process. And then the second question is sort of are the words consistent or the same over time? And yes, essentially, there's sort of the words as a whole, I think, sort of how we did it was to be started with these original large lists and then looked from that filtered down to just the words that appear enough over time so that you're not going to have really high variance estimates in any particular decade and then use that smaller list that appears over time. I have one follow up question to that and then a different one. So on that process, and I was curious about the same thing, there's, you know, the problem with keeping a consistent list is that it's the good thing is it's consistent. And then the bad thing is that it's consistent in that word use and language selection varies massively over time. So keeping that list can be just a measurement of when a word gets popular on that list and when it isn't. And so have you guys thought about controlling for that? That just came up. But more importantly, I think now that it seems very clear that the sort of embeddings and algorithms are getting there in terms of really extracting really good context, is anyone thinking about conducting, as it already been done, this main issue, which is like if you use 100 years of white authors, it's going to reflect that. But actually go in there and actually try to fix that problem in a systematic way, you know, with like replacement of these books or consensus corpus, but then not just get a consensus from these external partners, but then test whether the consensus does improve things, change things, or if there is something more general despite the lack of representation in our language. Okay. Great question. So I'll tackle the second one because that's the one I remember currently. So I don't want to pick on the corpus of historical American. It's a great data size compiled by, I think, social science experts and for, I think, going through that process in the first place. So they genre balance and they did a lot of things to make sure that the books were comparable over time. So in some sense, that is the output of a process like the one you described. But sort of, no, I guess I haven't seen a lot of work on sort of compiling the right data sets to train embeddings like this. I think I have heard of, I forget the citation, but I have heard of some work on how you can, during the training process, for example, you can flip genders in sentences in the training data and so on. So as opposed to actually compile representative sources, which may be hard, do training process fixes. So the first question, I think, was on our consistent word list themselves a problem and can we correct for that? So that certainly is. And so, for example, one thing that we thought about doing in this work is, so our methods for distinguishing between groups of people was using last names, which has the benefit of being somewhat consistent in the census historically and sort of last names and like first names are decently consistent in describing ethnic groups. And so one thing to, but it of course fails to distinguish between white and black Americans. Other work has used first names for that purpose. And we thought about doing that, but then we would have to do exactly what you said is correct for first name frequencies differ by generation quite a bit. I imagine it can be done. I haven't seen work that corrects it exactly in context like this, but definitely interesting avenue to explore. I just wanted to kind of make a shout out to the paper NLP statements by Emily Bender and Batia Friedman, which is I think a relatively recent paper. But I love the list of questions you had about kind of who creates the categories and who was included versus excluded. And just the idea of the NLP statements is to record these things about any data set. And so that doesn't resolve the fact that people are probably still being excluded or included, but it would at least provide kind of a record of these are the people that created this. And they give a few examples in the paper of kind of if you're using Amazon Mechanical Turk workers, even to say kind of like what were their ages, what countries were they from, what variants of even within English, are you using kind of British English versus Australian English, these variations. And so just to even have a way of kind of like documenting how these data sets were constructed. And that was an interesting paper recently. I was just going to ask about the choice of using Euclidean distance versus cosine distance, because I know sometimes in higher dimensional spaces, people use cosine distance. Yes, I think part of the intention of... Yeah, so yes, definitely that was a choice. And I think part of why we did it is Euclidean distance is just easier to explain and visualize. And so I think we show in the appendix that it really doesn't matter, that sort of it's... Yeah, the noise is different maybe, but really for a whole host of these things, it's like within the noise of everything else. And so regardless of distance metrics, some essentially identical patterns are going to emerge. But yeah, another I think danger of work like this is I think... Well, first, not abusing the research degrees of freedom that you have, but then to documenting that you didn't do that and providing as much robustness checks as you can as is feasible or appropriate in the appendix. Other questions? I noticed something that I think Rachel's planning to cover in a future lesson, but there was a paper last year I think called FRAGE, F-A-R-G-E, which showed that the kind of principal component of word vectors from word to vector and glove is actually word frequency rather than word meaning. I guess when David asked his question, it made me think, oh, is that going to be particularly an issue here when you look for similarity between two words that it's going to particularly show change in frequency because that's such a major component? Or did you actually kind of try to remove the frequency eigenvector out of the representations? Yeah. So I'll answer this last question first. So no, we didn't do any preprocessing, like removing principal components. But concerns like this of there's just shift in the embeddings for all sorts of reasons. There's no reason to believe that embeddings trained on one data set and then trained on another data set, even if the data sets were meant to be comparable. There's no reason to believe that the resulting embeddings are comparable for all sorts of reasons like this. So I think that challenge in particular motivated two of the things that we did. The first one, which I'm not a huge fan of in general, but I think for challenges like this we need to is although all the results in the paper are sort of comparisons. So they're not just what's the average distance between this group and the set of neutral words, but rather it's what's the average distance between this group and these neutral words minus the average distance between another group and the same set of words. The hope is that this sort of normalizes away some of these issues. But if that's not satisfactory, that's I think the purpose of the validation in the paper. So we're definitely concerned of things like this. But the sort of the various validations that we did with available historical data shows that while that may be a problem, it's not at least first order as far as we can detect that the method as a whole is decently appropriate for comparing over time or across data sets. Thank you. Question over here. Hey. So I guess my question for you is goal-wise or mission-wise, kind of what was the difference between goal-wise or mission-wise, kind of what was the goal of doing this kind of research? Is it to be kind of prescriptive, to maybe implement new rules and new understandings of language, or is it just to kind of be like, okay, if you're going to do NLP work, here are some things you can consider? Because for me, if you're doing research on gender and ethnic stereotypes, at some point you have to define genders and you have to define ethnicities. And I think that gets really hairy when you start thinking about ethnic minorities in a lot of different countries. Even if you try to even define what black means in America, too, you get into some hairy things. I mean, you mentioned, too, it's really important to bring in experts from other fields, and there definitely is a lot of research being done in a lot of the humanities spaces. Yeah, great question. So at least our goal here was purely descriptive. I'm not an expert in any of the prescriptions or any of the very complex issues that you talked about. And so I don't feel comfortable being prescriptive at all in what to do about bias in a particular application. And I think the unsatisfactory answer is the right thing to do is going to depend on this very specific context, like on this data set, in this specific application today, what is the right thing to do? And there are definitely general rules that other people that are far more experts than I am have come up with. But no, the purpose of this paper was purely as descriptive of you can use these embeddings to say something about historical beliefs. I was just going to ask if there are any kind of particular future directions that you either want to continue with this work or that you'd like to see other people take this work in? Yeah, great question. I think there definitely has been a lot of work in this line, very exciting work in this line. And I think a few directions that I want to highlight is one is on the technical side, there's a bunch of decisions that we made just for simplicity and to reduce our degrees of freedom. So one includes correcting for these frequencies. Another includes exactly the algorithm that we use that there's work on instead of black block embeddings, can you use embeddings where the dimensions have a little more interpretation or embeddings that are smoother over time. So instead of just training differently on different data sets, can you sort of actually track the trajectory of a word as you add more data that's more and more recent? So I think on a technical angle, there's all sorts of improvements you can make to reduce the noise in the measurements. One sort of the application setting as well is that we sort of sprayed on a bunch of, I don't want to say random, but arbitrary applications using somewhat ad hoc ways of constructing, for example, word lists and so on. In my opinion, principled in trying to find the most general list online, but I'm not an expert in any of the domains that we apply the methodology to. So on an applied sense, I would love to have more domain experts formalize. These are specific things we would want to look at. So for example, on the Asian stereotypes over time, I think there's a lot of work to be done on how narratives rise and fall and how competing narratives may play out in language. And I think that's something for an expert in that domain to explore. I know you mentioned in the last answer to my previous question that part of the answer to can you improve the process or the calculation is, well, it's already pretty good. So given that, from your last answer, it also sounds like you're also open to continuing to improve it further. I guess I have a question about the use of word embeddings in social sciences and these kind of things more generally. It's a super convenient and, as we say, fairly effective tool. But we also know that these kinds of word embeddings are a bit of a blunt instrument. They're linear models of co-occurrence matrices. They don't really capture the nuance of language. Do you know, you or anybody else have looked at whether it's possible to do similar kinds of social science analyses using the richer representations in a full nonlinear language model type representation? And would that be even useful? Definitely useful. I have seen some recent work. I'm going to mess up the citation, so I'll send it to you offline instead of saying it out of a video. Yeah, so I think my two-part answer to this is, yes, they're pretty good in that I don't think we're missing a zeroth order thing that completely invalidates the methodology. But there's a lot of improvements to make it precise to validate on different datasets to use better approaches. And I could just, I guess, make a kind of shout out to a few papers, several of which you already mentioned, related to this question of where to debias embeddings. And I was glad you raised that question. So kind of the two papers from 2016, the Balakbasi one was proposing a method to debias embeddings that used SBD as part of it. I always like to see shout outs to SBD. And then the Kaliskin-Bryson and Nuranjan paper said that they thought that the debiasing should occur at the point of action. And so typically, you know, we're embedding sort of this kind of more basic building block in an application. And so the idea was like, oh, if you debias then, bias can still seep into your application elsewhere. And perhaps even also, I think what Jeremy was getting at with kind of having complex interactions, and so you should debias at the point of action. And there was a paper earlier this year from Hila Gonin and Joav Goldberg. I don't know if you read this one, lipstick on a pig. Debiasing kind of covers up the structural biases, but doesn't remove them. And so that was kind of more evidence for the point of debiasing at the point of action, not like the embeddings. So I just wanted to share those kind of perspectives. I don't know that there's a consensus though on how this should be dealt with. Yeah. No, I think these are all great. And the consensus or maybe there will be a consensus. I'm sort of my view again, this is a very unexpert view and that I haven't looked closely at any of these methods beyond just like cursory implementing a few of them is that I think it's nice and good if you can debias at another stage, for example, at the point of action. But unfortunately, a lot of these language models are very end to end and there might not be a convenient place where you can say this is where the action is. And so yeah, whether the point of fixing some of these issues are at your model of the language or at some other downstream and at the encoder, sort of at the decoder instead of the encoder, then yeah, I think that's still an open question. But yeah, that's my very small take. Well, thank you. Any final questions? All right. Well, thank you so much. We are so glad to have you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.08, "text": " All right, I'm really excited to announce our guest speaker today, Nikhil Garg.", "tokens": [1057, 558, 11, 286, 478, 534, 2919, 281, 7478, 527, 8341, 8145, 965, 11, 13969, 42829, 7995, 70, 13], "temperature": 0.0, "avg_logprob": -0.2055151097914752, "compression_ratio": 1.4588744588744589, "no_speech_prob": 0.010005530901253223}, {"id": 1, "seek": 0, "start": 6.08, "end": 13.36, "text": " Nikhil is a PhD candidate in electrical engineering at Stanford, and he is a part of both the", "tokens": [13969, 42829, 307, 257, 14476, 11532, 294, 12147, 7043, 412, 20374, 11, 293, 415, 307, 257, 644, 295, 1293, 264], "temperature": 0.0, "avg_logprob": -0.2055151097914752, "compression_ratio": 1.4588744588744589, "no_speech_prob": 0.010005530901253223}, {"id": 2, "seek": 0, "start": 13.36, "end": 18.52, "text": " Stanford CrowdSource Democracy Team and the Society and Algorithms Lab.", "tokens": [20374, 40110, 50, 2948, 43062, 7606, 293, 264, 13742, 293, 35014, 6819, 2592, 10137, 13], "temperature": 0.0, "avg_logprob": -0.2055151097914752, "compression_ratio": 1.4588744588744589, "no_speech_prob": 0.010005530901253223}, {"id": 3, "seek": 0, "start": 18.52, "end": 25.16, "text": " He has an NSF graduate research fellowship, and he has spent time at Uber, NASA, Microsoft,", "tokens": [634, 575, 364, 15943, 37, 8080, 2132, 24989, 11, 293, 415, 575, 4418, 565, 412, 21839, 11, 12077, 11, 8116, 11], "temperature": 0.0, "avg_logprob": -0.2055151097914752, "compression_ratio": 1.4588744588744589, "no_speech_prob": 0.010005530901253223}, {"id": 4, "seek": 2516, "start": 25.16, "end": 30.240000000000002, "text": " the Texas Senate, and IEEE's Policy Arm, which I think is a really amazing diversity", "tokens": [264, 7885, 9867, 11, 293, 286, 7258, 36, 311, 21708, 11893, 11, 597, 286, 519, 307, 257, 534, 2243, 8811], "temperature": 0.0, "avg_logprob": -0.2069585058424208, "compression_ratio": 1.5930232558139534, "no_speech_prob": 4.9790414777817205e-05}, {"id": 5, "seek": 2516, "start": 30.240000000000002, "end": 31.78, "text": " of places.", "tokens": [295, 3190, 13], "temperature": 0.0, "avg_logprob": -0.2069585058424208, "compression_ratio": 1.5930232558139534, "no_speech_prob": 4.9790414777817205e-05}, {"id": 6, "seek": 2516, "start": 31.78, "end": 37.519999999999996, "text": " His work has been covered in the New York Times, Science Magazine, Smithsonian Magazine,", "tokens": [2812, 589, 575, 668, 5343, 294, 264, 1873, 3609, 11366, 11, 8976, 27618, 11, 46013, 27618, 11], "temperature": 0.0, "avg_logprob": -0.2069585058424208, "compression_ratio": 1.5930232558139534, "no_speech_prob": 4.9790414777817205e-05}, {"id": 7, "seek": 2516, "start": 37.519999999999996, "end": 41.82, "text": " Stanford Engineering Magazine, Stanford News, and other places.", "tokens": [20374, 16215, 27618, 11, 20374, 7987, 11, 293, 661, 3190, 13], "temperature": 0.0, "avg_logprob": -0.2069585058424208, "compression_ratio": 1.5930232558139534, "no_speech_prob": 4.9790414777817205e-05}, {"id": 8, "seek": 2516, "start": 41.82, "end": 46.2, "text": " So with that, we'll welcome Nikhil, and he invites questions, and I'm going to try to", "tokens": [407, 365, 300, 11, 321, 603, 2928, 13969, 42829, 11, 293, 415, 35719, 1651, 11, 293, 286, 478, 516, 281, 853, 281], "temperature": 0.0, "avg_logprob": -0.2069585058424208, "compression_ratio": 1.5930232558139534, "no_speech_prob": 4.9790414777817205e-05}, {"id": 9, "seek": 2516, "start": 46.2, "end": 49.08, "text": " use the catch box to catch these for the questions.", "tokens": [764, 264, 3745, 2424, 281, 3745, 613, 337, 264, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2069585058424208, "compression_ratio": 1.5930232558139534, "no_speech_prob": 4.9790414777817205e-05}, {"id": 10, "seek": 2516, "start": 49.08, "end": 52.08, "text": " Yeah, thank you, Nikhil.", "tokens": [865, 11, 1309, 291, 11, 13969, 42829, 13], "temperature": 0.0, "avg_logprob": -0.2069585058424208, "compression_ratio": 1.5930232558139534, "no_speech_prob": 4.9790414777817205e-05}, {"id": 11, "seek": 5208, "start": 52.08, "end": 58.48, "text": " Thanks for having me.", "tokens": [2561, 337, 1419, 385, 13], "temperature": 0.0, "avg_logprob": -0.17825243631998697, "compression_ratio": 1.4578947368421054, "no_speech_prob": 9.970533938030712e-06}, {"id": 12, "seek": 5208, "start": 58.48, "end": 64.48, "text": " And just to emphasize, I'm going to make this more of a classroom-style talk as opposed", "tokens": [400, 445, 281, 16078, 11, 286, 478, 516, 281, 652, 341, 544, 295, 257, 7419, 12, 15014, 751, 382, 8851], "temperature": 0.0, "avg_logprob": -0.17825243631998697, "compression_ratio": 1.4578947368421054, "no_speech_prob": 9.970533938030712e-06}, {"id": 13, "seek": 5208, "start": 64.48, "end": 69.24, "text": " to a research talk, so feel free to interrupt and so on.", "tokens": [281, 257, 2132, 751, 11, 370, 841, 1737, 281, 12729, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.17825243631998697, "compression_ratio": 1.4578947368421054, "no_speech_prob": 9.970533938030712e-06}, {"id": 14, "seek": 5208, "start": 69.24, "end": 73.12, "text": " And I think that would motivate some of how I structured this and what we'll talk about", "tokens": [400, 286, 519, 300, 576, 28497, 512, 295, 577, 286, 18519, 341, 293, 437, 321, 603, 751, 466], "temperature": 0.0, "avg_logprob": -0.17825243631998697, "compression_ratio": 1.4578947368421054, "no_speech_prob": 9.970533938030712e-06}, {"id": 15, "seek": 5208, "start": 73.12, "end": 75.2, "text": " at the end, hopefully.", "tokens": [412, 264, 917, 11, 4696, 13], "temperature": 0.0, "avg_logprob": -0.17825243631998697, "compression_ratio": 1.4578947368421054, "no_speech_prob": 9.970533938030712e-06}, {"id": 16, "seek": 7520, "start": 75.2, "end": 82.44, "text": " So this is some recent work on how to use word embeddings to quantify gender and ethnic", "tokens": [407, 341, 307, 512, 5162, 589, 322, 577, 281, 764, 1349, 12240, 29432, 281, 40421, 7898, 293, 14363], "temperature": 0.0, "avg_logprob": -0.11939934602717764, "compression_ratio": 1.5577689243027888, "no_speech_prob": 1.2409239388944115e-05}, {"id": 17, "seek": 7520, "start": 82.44, "end": 87.60000000000001, "text": " stereotypes, and more generally, I think, unsupervised machine learning outputs sort", "tokens": [30853, 11, 293, 544, 5101, 11, 286, 519, 11, 2693, 12879, 24420, 3479, 2539, 23930, 1333], "temperature": 0.0, "avg_logprob": -0.11939934602717764, "compression_ratio": 1.5577689243027888, "no_speech_prob": 1.2409239388944115e-05}, {"id": 18, "seek": 7520, "start": 87.60000000000001, "end": 92.80000000000001, "text": " of have great promise as a lens on our society.", "tokens": [295, 362, 869, 6228, 382, 257, 6765, 322, 527, 4086, 13], "temperature": 0.0, "avg_logprob": -0.11939934602717764, "compression_ratio": 1.5577689243027888, "no_speech_prob": 1.2409239388944115e-05}, {"id": 19, "seek": 7520, "start": 92.80000000000001, "end": 97.2, "text": " And so that's, I think, the framework for this talk.", "tokens": [400, 370, 300, 311, 11, 286, 519, 11, 264, 8388, 337, 341, 751, 13], "temperature": 0.0, "avg_logprob": -0.11939934602717764, "compression_ratio": 1.5577689243027888, "no_speech_prob": 1.2409239388944115e-05}, {"id": 20, "seek": 7520, "start": 97.2, "end": 98.4, "text": " So just a quick primer.", "tokens": [407, 445, 257, 1702, 12595, 13], "temperature": 0.0, "avg_logprob": -0.11939934602717764, "compression_ratio": 1.5577689243027888, "no_speech_prob": 1.2409239388944115e-05}, {"id": 21, "seek": 7520, "start": 98.4, "end": 102.60000000000001, "text": " I know you all are now experts in data science and natural language processing, so I probably", "tokens": [286, 458, 291, 439, 366, 586, 8572, 294, 1412, 3497, 293, 3303, 2856, 9007, 11, 370, 286, 1391], "temperature": 0.0, "avg_logprob": -0.11939934602717764, "compression_ratio": 1.5577689243027888, "no_speech_prob": 1.2409239388944115e-05}, {"id": 22, "seek": 10260, "start": 102.6, "end": 107.8, "text": " don't need to do this, but I'll just give a quick primer on word embeddings, which their", "tokens": [500, 380, 643, 281, 360, 341, 11, 457, 286, 603, 445, 976, 257, 1702, 12595, 322, 1349, 12240, 29432, 11, 597, 641], "temperature": 0.0, "avg_logprob": -0.13903810601485403, "compression_ratio": 1.6340425531914893, "no_speech_prob": 3.8219492125790566e-05}, {"id": 23, "seek": 10260, "start": 107.8, "end": 114.72, "text": " numeric representations of words and what I deem is a low dimensional space, low because", "tokens": [7866, 299, 33358, 295, 2283, 293, 437, 286, 368, 443, 307, 257, 2295, 18795, 1901, 11, 2295, 570], "temperature": 0.0, "avg_logprob": -0.13903810601485403, "compression_ratio": 1.6340425531914893, "no_speech_prob": 3.8219492125790566e-05}, {"id": 24, "seek": 10260, "start": 114.72, "end": 120.52, "text": " it's, I think, somewhat surprising or inaccurate to say that 300 dimensions can capture all", "tokens": [309, 311, 11, 286, 519, 11, 8344, 8830, 420, 46443, 281, 584, 300, 6641, 12819, 393, 7983, 439], "temperature": 0.0, "avg_logprob": -0.13903810601485403, "compression_ratio": 1.6340425531914893, "no_speech_prob": 3.8219492125790566e-05}, {"id": 25, "seek": 10260, "start": 120.52, "end": 124.03999999999999, "text": " the complexities of language.", "tokens": [264, 48705, 295, 2856, 13], "temperature": 0.0, "avg_logprob": -0.13903810601485403, "compression_ratio": 1.6340425531914893, "no_speech_prob": 3.8219492125790566e-05}, {"id": 26, "seek": 10260, "start": 124.03999999999999, "end": 129.68, "text": " And so as I said, there are models of the world, incomplete models of the world, and", "tokens": [400, 370, 382, 286, 848, 11, 456, 366, 5245, 295, 264, 1002, 11, 31709, 5245, 295, 264, 1002, 11, 293], "temperature": 0.0, "avg_logprob": -0.13903810601485403, "compression_ratio": 1.6340425531914893, "no_speech_prob": 3.8219492125790566e-05}, {"id": 27, "seek": 12968, "start": 129.68, "end": 134.24, "text": " such that similar words will have similar numeric representations.", "tokens": [1270, 300, 2531, 2283, 486, 362, 2531, 7866, 299, 33358, 13], "temperature": 0.0, "avg_logprob": -0.1084262615925557, "compression_ratio": 1.5645933014354068, "no_speech_prob": 8.26638461148832e-06}, {"id": 28, "seek": 12968, "start": 134.24, "end": 139.72, "text": " And a lot of prior work has shown that word embeddings can encode a whole host of things", "tokens": [400, 257, 688, 295, 4059, 589, 575, 4898, 300, 1349, 12240, 29432, 393, 2058, 1429, 257, 1379, 3975, 295, 721], "temperature": 0.0, "avg_logprob": -0.1084262615925557, "compression_ratio": 1.5645933014354068, "no_speech_prob": 8.26638461148832e-06}, {"id": 29, "seek": 12968, "start": 139.72, "end": 146.9, "text": " about language, semantics and tax, relationships between words, and so on.", "tokens": [466, 2856, 11, 4361, 45298, 293, 3366, 11, 6159, 1296, 2283, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1084262615925557, "compression_ratio": 1.5645933014354068, "no_speech_prob": 8.26638461148832e-06}, {"id": 30, "seek": 12968, "start": 146.9, "end": 152.12, "text": " And they're primarily trained through unsupervised methods using extremely large data sets, such", "tokens": [400, 436, 434, 10029, 8895, 807, 2693, 12879, 24420, 7150, 1228, 4664, 2416, 1412, 6352, 11, 1270], "temperature": 0.0, "avg_logprob": -0.1084262615925557, "compression_ratio": 1.5645933014354068, "no_speech_prob": 8.26638461148832e-06}, {"id": 31, "seek": 15212, "start": 152.12, "end": 161.52, "text": " as all the Wikipedia, all the Google News, a corpus of every American book, or so on.", "tokens": [382, 439, 264, 28999, 11, 439, 264, 3329, 7987, 11, 257, 1181, 31624, 295, 633, 2665, 1446, 11, 420, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.13049827151828342, "compression_ratio": 1.5458515283842795, "no_speech_prob": 7.071360414556693e-06}, {"id": 32, "seek": 15212, "start": 161.52, "end": 167.16, "text": " And they're, of course, a building block of modern natural language processing.", "tokens": [400, 436, 434, 11, 295, 1164, 11, 257, 2390, 3461, 295, 4363, 3303, 2856, 9007, 13], "temperature": 0.0, "avg_logprob": -0.13049827151828342, "compression_ratio": 1.5458515283842795, "no_speech_prob": 7.071360414556693e-06}, {"id": 33, "seek": 15212, "start": 167.16, "end": 173.24, "text": " So prior work before ours, there's, of course, a bunch, but two that I would like to highlight,", "tokens": [407, 4059, 589, 949, 11896, 11, 456, 311, 11, 295, 1164, 11, 257, 3840, 11, 457, 732, 300, 286, 576, 411, 281, 5078, 11], "temperature": 0.0, "avg_logprob": -0.13049827151828342, "compression_ratio": 1.5458515283842795, "no_speech_prob": 7.071360414556693e-06}, {"id": 34, "seek": 15212, "start": 173.24, "end": 181.48000000000002, "text": " really, I think, emphasize that word embeddings can encode biases or stereotypes in language", "tokens": [534, 11, 286, 519, 11, 16078, 300, 1349, 12240, 29432, 393, 2058, 1429, 32152, 420, 30853, 294, 2856], "temperature": 0.0, "avg_logprob": -0.13049827151828342, "compression_ratio": 1.5458515283842795, "no_speech_prob": 7.071360414556693e-06}, {"id": 35, "seek": 18148, "start": 181.48, "end": 182.84, "text": " or in our society.", "tokens": [420, 294, 527, 4086, 13], "temperature": 0.0, "avg_logprob": -0.15020325017529865, "compression_ratio": 1.5868544600938967, "no_speech_prob": 1.7500680769444443e-05}, {"id": 36, "seek": 18148, "start": 182.84, "end": 190.79999999999998, "text": " So the first one, through a bunch of analogy type tasks, focused on gender bias and then", "tokens": [407, 264, 700, 472, 11, 807, 257, 3840, 295, 21663, 2010, 9608, 11, 5178, 322, 7898, 12577, 293, 550], "temperature": 0.0, "avg_logprob": -0.15020325017529865, "compression_ratio": 1.5868544600938967, "no_speech_prob": 1.7500680769444443e-05}, {"id": 37, "seek": 18148, "start": 190.79999999999998, "end": 193.83999999999997, "text": " talked about how to debias word embeddings.", "tokens": [2825, 466, 577, 281, 3001, 4609, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.15020325017529865, "compression_ratio": 1.5868544600938967, "no_speech_prob": 1.7500680769444443e-05}, {"id": 38, "seek": 18148, "start": 193.83999999999997, "end": 201.07999999999998, "text": " And then the second paper, I think it was in Science, showed that word embeddings basically", "tokens": [400, 550, 264, 1150, 3035, 11, 286, 519, 309, 390, 294, 8976, 11, 4712, 300, 1349, 12240, 29432, 1936], "temperature": 0.0, "avg_logprob": -0.15020325017529865, "compression_ratio": 1.5868544600938967, "no_speech_prob": 1.7500680769444443e-05}, {"id": 39, "seek": 18148, "start": 201.07999999999998, "end": 205.79999999999998, "text": " replicate every human bias on the implicit association test.", "tokens": [25356, 633, 1952, 12577, 322, 264, 26947, 14598, 1500, 13], "temperature": 0.0, "avg_logprob": -0.15020325017529865, "compression_ratio": 1.5868544600938967, "no_speech_prob": 1.7500680769444443e-05}, {"id": 40, "seek": 18148, "start": 205.79999999999998, "end": 209.72, "text": " So gender, ethnic, what have you.", "tokens": [407, 7898, 11, 14363, 11, 437, 362, 291, 13], "temperature": 0.0, "avg_logprob": -0.15020325017529865, "compression_ratio": 1.5868544600938967, "no_speech_prob": 1.7500680769444443e-05}, {"id": 41, "seek": 20972, "start": 209.72, "end": 213.12, "text": " Basically all of those are replicated.", "tokens": [8537, 439, 295, 729, 366, 46365, 13], "temperature": 0.0, "avg_logprob": -0.10349669748423052, "compression_ratio": 1.6255506607929515, "no_speech_prob": 7.76635806687409e-06}, {"id": 42, "seek": 20972, "start": 213.12, "end": 218.8, "text": " And what I would emphasize here is that embeddings are thus a model or represent language as", "tokens": [400, 437, 286, 576, 16078, 510, 307, 300, 12240, 29432, 366, 8807, 257, 2316, 420, 2906, 2856, 382], "temperature": 0.0, "avg_logprob": -0.10349669748423052, "compression_ratio": 1.6255506607929515, "no_speech_prob": 7.76635806687409e-06}, {"id": 43, "seek": 20972, "start": 218.8, "end": 225.36, "text": " it is used, not just language as we may wish it to use or sort of some formal notion of", "tokens": [309, 307, 1143, 11, 406, 445, 2856, 382, 321, 815, 3172, 309, 281, 764, 420, 1333, 295, 512, 9860, 10710, 295], "temperature": 0.0, "avg_logprob": -0.10349669748423052, "compression_ratio": 1.6255506607929515, "no_speech_prob": 7.76635806687409e-06}, {"id": 44, "seek": 20972, "start": 225.36, "end": 228.84, "text": " our language.", "tokens": [527, 2856, 13], "temperature": 0.0, "avg_logprob": -0.10349669748423052, "compression_ratio": 1.6255506607929515, "no_speech_prob": 7.76635806687409e-06}, {"id": 45, "seek": 20972, "start": 228.84, "end": 233.52, "text": " And the idea of our work was that this is, of course, harmful in many contexts, and we'll", "tokens": [400, 264, 1558, 295, 527, 589, 390, 300, 341, 307, 11, 295, 1164, 11, 19727, 294, 867, 30628, 11, 293, 321, 603], "temperature": 0.0, "avg_logprob": -0.10349669748423052, "compression_ratio": 1.6255506607929515, "no_speech_prob": 7.76635806687409e-06}, {"id": 46, "seek": 20972, "start": 233.52, "end": 236.72, "text": " get to that at the end, but it's also useful.", "tokens": [483, 281, 300, 412, 264, 917, 11, 457, 309, 311, 611, 4420, 13], "temperature": 0.0, "avg_logprob": -0.10349669748423052, "compression_ratio": 1.6255506607929515, "no_speech_prob": 7.76635806687409e-06}, {"id": 47, "seek": 23672, "start": 236.72, "end": 242.52, "text": " In particular, documenting and auditing historical beliefs is something that's important to do", "tokens": [682, 1729, 11, 42360, 293, 2379, 1748, 8584, 13585, 307, 746, 300, 311, 1021, 281, 360], "temperature": 0.0, "avg_logprob": -0.09655101909193882, "compression_ratio": 1.728110599078341, "no_speech_prob": 6.438187483581714e-06}, {"id": 48, "seek": 23672, "start": 242.52, "end": 249.92, "text": " just as studying or knowing about how beliefs have changed over time, but it's also very", "tokens": [445, 382, 7601, 420, 5276, 466, 577, 13585, 362, 3105, 670, 565, 11, 457, 309, 311, 611, 588], "temperature": 0.0, "avg_logprob": -0.09655101909193882, "compression_ratio": 1.728110599078341, "no_speech_prob": 6.438187483581714e-06}, {"id": 49, "seek": 23672, "start": 249.92, "end": 251.76, "text": " hard to do.", "tokens": [1152, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.09655101909193882, "compression_ratio": 1.728110599078341, "no_speech_prob": 6.438187483581714e-06}, {"id": 50, "seek": 23672, "start": 251.76, "end": 257.36, "text": " There's a limited set, a very limited set of historical survey, consistent historical", "tokens": [821, 311, 257, 5567, 992, 11, 257, 588, 5567, 992, 295, 8584, 8984, 11, 8398, 8584], "temperature": 0.0, "avg_logprob": -0.09655101909193882, "compression_ratio": 1.728110599078341, "no_speech_prob": 6.438187483581714e-06}, {"id": 51, "seek": 23672, "start": 257.36, "end": 264.88, "text": " surveys on what did people think about certain topics over time, but they're a little sparse.", "tokens": [22711, 322, 437, 630, 561, 519, 466, 1629, 8378, 670, 565, 11, 457, 436, 434, 257, 707, 637, 11668, 13], "temperature": 0.0, "avg_logprob": -0.09655101909193882, "compression_ratio": 1.728110599078341, "no_speech_prob": 6.438187483581714e-06}, {"id": 52, "seek": 26488, "start": 264.88, "end": 272.76, "text": " And so the idea is to augment some of those historical works with the word embeddings.", "tokens": [400, 370, 264, 1558, 307, 281, 29919, 512, 295, 729, 8584, 1985, 365, 264, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.12775811647114002, "compression_ratio": 1.6452991452991452, "no_speech_prob": 3.90527611671132e-06}, {"id": 53, "seek": 26488, "start": 272.76, "end": 277.28, "text": " And here, of course, showing, sort of taking advantage of the fact that the representational", "tokens": [400, 510, 11, 295, 1164, 11, 4099, 11, 1333, 295, 1940, 5002, 295, 264, 1186, 300, 264, 2906, 1478], "temperature": 0.0, "avg_logprob": -0.12775811647114002, "compression_ratio": 1.6452991452991452, "no_speech_prob": 3.90527611671132e-06}, {"id": 54, "seek": 26488, "start": 277.28, "end": 280.32, "text": " power of embeddings is useful.", "tokens": [1347, 295, 12240, 29432, 307, 4420, 13], "temperature": 0.0, "avg_logprob": -0.12775811647114002, "compression_ratio": 1.6452991452991452, "no_speech_prob": 3.90527611671132e-06}, {"id": 55, "seek": 26488, "start": 280.32, "end": 288.15999999999997, "text": " Okay, so sort of a very high level outline of the talk is that first I'll just summarize", "tokens": [1033, 11, 370, 1333, 295, 257, 588, 1090, 1496, 16387, 295, 264, 751, 307, 300, 700, 286, 603, 445, 20858], "temperature": 0.0, "avg_logprob": -0.12775811647114002, "compression_ratio": 1.6452991452991452, "no_speech_prob": 3.90527611671132e-06}, {"id": 56, "seek": 26488, "start": 288.15999999999997, "end": 294.24, "text": " the methodology at a very high level, and then I'll walk through a bunch of examples,", "tokens": [264, 24850, 412, 257, 588, 1090, 1496, 11, 293, 550, 286, 603, 1792, 807, 257, 3840, 295, 5110, 11], "temperature": 0.0, "avg_logprob": -0.12775811647114002, "compression_ratio": 1.6452991452991452, "no_speech_prob": 3.90527611671132e-06}, {"id": 57, "seek": 29424, "start": 294.24, "end": 297.88, "text": " on confirming that we can actually do this, because there's, of course, a bunch of potential", "tokens": [322, 42861, 300, 321, 393, 767, 360, 341, 11, 570, 456, 311, 11, 295, 1164, 11, 257, 3840, 295, 3995], "temperature": 0.0, "avg_logprob": -0.1351674806953657, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.6700330888852477e-05}, {"id": 58, "seek": 29424, "start": 297.88, "end": 298.88, "text": " pitfalls.", "tokens": [10147, 18542, 13], "temperature": 0.0, "avg_logprob": -0.1351674806953657, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.6700330888852477e-05}, {"id": 59, "seek": 29424, "start": 298.88, "end": 305.76, "text": " I'll present some exploratory findings, but then I would actually like to spend a substantial", "tokens": [286, 603, 1974, 512, 24765, 4745, 16483, 11, 457, 550, 286, 576, 767, 411, 281, 3496, 257, 16726], "temperature": 0.0, "avg_logprob": -0.1351674806953657, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.6700330888852477e-05}, {"id": 60, "seek": 29424, "start": 305.76, "end": 311.16, "text": " amount of time on limitations, both in this work and this type of work in general.", "tokens": [2372, 295, 565, 322, 15705, 11, 1293, 294, 341, 589, 293, 341, 2010, 295, 589, 294, 2674, 13], "temperature": 0.0, "avg_logprob": -0.1351674806953657, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.6700330888852477e-05}, {"id": 61, "seek": 29424, "start": 311.16, "end": 316.56, "text": " And especially there, I'd be happy to have a discussion.", "tokens": [400, 2318, 456, 11, 286, 1116, 312, 2055, 281, 362, 257, 5017, 13], "temperature": 0.0, "avg_logprob": -0.1351674806953657, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.6700330888852477e-05}, {"id": 62, "seek": 29424, "start": 316.56, "end": 322.40000000000003, "text": " Okay, so the high level framework in the paper is actually pretty simple.", "tokens": [1033, 11, 370, 264, 1090, 1496, 8388, 294, 264, 3035, 307, 767, 1238, 2199, 13], "temperature": 0.0, "avg_logprob": -0.1351674806953657, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.6700330888852477e-05}, {"id": 63, "seek": 32240, "start": 322.4, "end": 327.44, "text": " We start with some embeddings, so just train embeddings on some data set.", "tokens": [492, 722, 365, 512, 12240, 29432, 11, 370, 445, 3847, 12240, 29432, 322, 512, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1160879135131836, "compression_ratio": 1.6964285714285714, "no_speech_prob": 3.2187523174798116e-05}, {"id": 64, "seek": 32240, "start": 327.44, "end": 334.28, "text": " And the assumption here is that the data set is a proxy for some society of interest.", "tokens": [400, 264, 15302, 510, 307, 300, 264, 1412, 992, 307, 257, 29690, 337, 512, 4086, 295, 1179, 13], "temperature": 0.0, "avg_logprob": -0.1160879135131836, "compression_ratio": 1.6964285714285714, "no_speech_prob": 3.2187523174798116e-05}, {"id": 65, "seek": 32240, "start": 334.28, "end": 337.0, "text": " And then we compile two lists of words.", "tokens": [400, 550, 321, 31413, 732, 14511, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1160879135131836, "compression_ratio": 1.6964285714285714, "no_speech_prob": 3.2187523174798116e-05}, {"id": 66, "seek": 32240, "start": 337.0, "end": 344.28, "text": " First is what we call group words that will, quote unquote, represent populations of interest.", "tokens": [2386, 307, 437, 321, 818, 1594, 2283, 300, 486, 11, 6513, 37557, 11, 2906, 12822, 295, 1179, 13], "temperature": 0.0, "avg_logprob": -0.1160879135131836, "compression_ratio": 1.6964285714285714, "no_speech_prob": 3.2187523174798116e-05}, {"id": 67, "seek": 32240, "start": 344.28, "end": 350.03999999999996, "text": " The ones that we use in this work are for gender, we just use a bunch of pronouns and", "tokens": [440, 2306, 300, 321, 764, 294, 341, 589, 366, 337, 7898, 11, 321, 445, 764, 257, 3840, 295, 35883, 293], "temperature": 0.0, "avg_logprob": -0.1160879135131836, "compression_ratio": 1.6964285714285714, "no_speech_prob": 3.2187523174798116e-05}, {"id": 68, "seek": 35004, "start": 350.04, "end": 357.08000000000004, "text": " like relationships for gender, and then for ethnic groups, we use common last names derived", "tokens": [411, 6159, 337, 7898, 11, 293, 550, 337, 14363, 3935, 11, 321, 764, 2689, 1036, 5288, 18949], "temperature": 0.0, "avg_logprob": -0.12091431087917752, "compression_ratio": 1.6434426229508197, "no_speech_prob": 1.384494680678472e-05}, {"id": 69, "seek": 35004, "start": 357.08000000000004, "end": 360.48, "text": " from the US census.", "tokens": [490, 264, 2546, 23725, 13], "temperature": 0.0, "avg_logprob": -0.12091431087917752, "compression_ratio": 1.6434426229508197, "no_speech_prob": 1.384494680678472e-05}, {"id": 70, "seek": 35004, "start": 360.48, "end": 366.96000000000004, "text": " And then also compile a list of what we call neutral words that represent potential characteristics", "tokens": [400, 550, 611, 31413, 257, 1329, 295, 437, 321, 818, 10598, 2283, 300, 2906, 3995, 10891], "temperature": 0.0, "avg_logprob": -0.12091431087917752, "compression_ratio": 1.6434426229508197, "no_speech_prob": 1.384494680678472e-05}, {"id": 71, "seek": 35004, "start": 366.96000000000004, "end": 371.36, "text": " that people can have, so either adjectives or occupations or what have you, whatever", "tokens": [300, 561, 393, 362, 11, 370, 2139, 29378, 1539, 420, 8073, 763, 420, 437, 362, 291, 11, 2035], "temperature": 0.0, "avg_logprob": -0.12091431087917752, "compression_ratio": 1.6434426229508197, "no_speech_prob": 1.384494680678472e-05}, {"id": 72, "seek": 35004, "start": 371.36, "end": 374.76, "text": " is of interest.", "tokens": [307, 295, 1179, 13], "temperature": 0.0, "avg_logprob": -0.12091431087917752, "compression_ratio": 1.6434426229508197, "no_speech_prob": 1.384494680678472e-05}, {"id": 73, "seek": 35004, "start": 374.76, "end": 379.68, "text": " Then just from the embeddings of these words, just calculate the distance between groups", "tokens": [1396, 445, 490, 264, 12240, 29432, 295, 613, 2283, 11, 445, 8873, 264, 4560, 1296, 3935], "temperature": 0.0, "avg_logprob": -0.12091431087917752, "compression_ratio": 1.6434426229508197, "no_speech_prob": 1.384494680678472e-05}, {"id": 74, "seek": 37968, "start": 379.68, "end": 384.96, "text": " and neutral words, and then compare distances across groups.", "tokens": [293, 10598, 2283, 11, 293, 550, 6794, 22182, 2108, 3935, 13], "temperature": 0.0, "avg_logprob": -0.061886382102966306, "compression_ratio": 1.7281553398058251, "no_speech_prob": 7.527577963628573e-06}, {"id": 75, "seek": 37968, "start": 384.96, "end": 389.96, "text": " So if a certain characteristic is closer to one group than the other, then we would say", "tokens": [407, 498, 257, 1629, 16282, 307, 4966, 281, 472, 1594, 813, 264, 661, 11, 550, 321, 576, 584], "temperature": 0.0, "avg_logprob": -0.061886382102966306, "compression_ratio": 1.7281553398058251, "no_speech_prob": 7.527577963628573e-06}, {"id": 76, "seek": 37968, "start": 389.96, "end": 395.68, "text": " that the embeddings are biased toward that group over the other group.", "tokens": [300, 264, 12240, 29432, 366, 28035, 7361, 300, 1594, 670, 264, 661, 1594, 13], "temperature": 0.0, "avg_logprob": -0.061886382102966306, "compression_ratio": 1.7281553398058251, "no_speech_prob": 7.527577963628573e-06}, {"id": 77, "seek": 37968, "start": 395.68, "end": 400.52, "text": " And then in some cases, we can compare these relative distances to outside data just to", "tokens": [400, 550, 294, 512, 3331, 11, 321, 393, 6794, 613, 4972, 22182, 281, 2380, 1412, 445, 281], "temperature": 0.0, "avg_logprob": -0.061886382102966306, "compression_ratio": 1.7281553398058251, "no_speech_prob": 7.527577963628573e-06}, {"id": 78, "seek": 37968, "start": 400.52, "end": 405.0, "text": " confirm that these are capturing something real.", "tokens": [9064, 300, 613, 366, 23384, 746, 957, 13], "temperature": 0.0, "avg_logprob": -0.061886382102966306, "compression_ratio": 1.7281553398058251, "no_speech_prob": 7.527577963628573e-06}, {"id": 79, "seek": 40500, "start": 405.0, "end": 411.36, "text": " There's a bunch of assumptions in everything I just said from the embeddings representing", "tokens": [821, 311, 257, 3840, 295, 17695, 294, 1203, 286, 445, 848, 490, 264, 12240, 29432, 13460], "temperature": 0.0, "avg_logprob": -0.14177265167236328, "compression_ratio": 1.6176470588235294, "no_speech_prob": 8.139050805766601e-06}, {"id": 80, "seek": 40500, "start": 411.36, "end": 413.88, "text": " societies to group words and so on.", "tokens": [19329, 281, 1594, 2283, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.14177265167236328, "compression_ratio": 1.6176470588235294, "no_speech_prob": 8.139050805766601e-06}, {"id": 81, "seek": 40500, "start": 413.88, "end": 418.04, "text": " I'm going to save that till the end, talking about how these assumptions are present in", "tokens": [286, 478, 516, 281, 3155, 300, 4288, 264, 917, 11, 1417, 466, 577, 613, 17695, 366, 1974, 294], "temperature": 0.0, "avg_logprob": -0.14177265167236328, "compression_ratio": 1.6176470588235294, "no_speech_prob": 8.139050805766601e-06}, {"id": 82, "seek": 40500, "start": 418.04, "end": 424.12, "text": " this work and also a lot of natural language processing in similar domains.", "tokens": [341, 589, 293, 611, 257, 688, 295, 3303, 2856, 9007, 294, 2531, 25514, 13], "temperature": 0.0, "avg_logprob": -0.14177265167236328, "compression_ratio": 1.6176470588235294, "no_speech_prob": 8.139050805766601e-06}, {"id": 83, "seek": 40500, "start": 424.12, "end": 432.56, "text": " Okay, so the first example, which also appeared in other work, so this isn't novel or anything,", "tokens": [1033, 11, 370, 264, 700, 1365, 11, 597, 611, 8516, 294, 661, 589, 11, 370, 341, 1943, 380, 7613, 420, 1340, 11], "temperature": 0.0, "avg_logprob": -0.14177265167236328, "compression_ratio": 1.6176470588235294, "no_speech_prob": 8.139050805766601e-06}, {"id": 84, "seek": 43256, "start": 432.56, "end": 437.84, "text": " is look at embeddings that are trained on Google News.", "tokens": [307, 574, 412, 12240, 29432, 300, 366, 8895, 322, 3329, 7987, 13], "temperature": 0.0, "avg_logprob": -0.11435780900247981, "compression_ratio": 1.663716814159292, "no_speech_prob": 1.241042627952993e-05}, {"id": 85, "seek": 43256, "start": 437.84, "end": 443.36, "text": " These are available online and just have two sets of group words, one set of group words", "tokens": [1981, 366, 2435, 2950, 293, 445, 362, 732, 6352, 295, 1594, 2283, 11, 472, 992, 295, 1594, 2283], "temperature": 0.0, "avg_logprob": -0.11435780900247981, "compression_ratio": 1.663716814159292, "no_speech_prob": 1.241042627952993e-05}, {"id": 86, "seek": 43256, "start": 443.36, "end": 449.52, "text": " representing men and another for women, and then the neutral words are a set of occupations", "tokens": [13460, 1706, 293, 1071, 337, 2266, 11, 293, 550, 264, 10598, 2283, 366, 257, 992, 295, 8073, 763], "temperature": 0.0, "avg_logprob": -0.11435780900247981, "compression_ratio": 1.663716814159292, "no_speech_prob": 1.241042627952993e-05}, {"id": 87, "seek": 43256, "start": 449.52, "end": 454.68, "text": " that you can come up with from the census, for example.", "tokens": [300, 291, 393, 808, 493, 365, 490, 264, 23725, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.11435780900247981, "compression_ratio": 1.663716814159292, "no_speech_prob": 1.241042627952993e-05}, {"id": 88, "seek": 43256, "start": 454.68, "end": 462.2, "text": " The additional data that we use to compare is a breakdown of what percentage of that", "tokens": [440, 4497, 1412, 300, 321, 764, 281, 6794, 307, 257, 18188, 295, 437, 9668, 295, 300], "temperature": 0.0, "avg_logprob": -0.11435780900247981, "compression_ratio": 1.663716814159292, "no_speech_prob": 1.241042627952993e-05}, {"id": 89, "seek": 46220, "start": 462.2, "end": 468.2, "text": " occupation in the United States is each gender, and this is through census data.", "tokens": [24482, 294, 264, 2824, 3040, 307, 1184, 7898, 11, 293, 341, 307, 807, 23725, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1520613981096932, "compression_ratio": 1.6801801801801801, "no_speech_prob": 1.4284180906543043e-05}, {"id": 90, "seek": 46220, "start": 468.2, "end": 474.59999999999997, "text": " And then we augment that with a Mechanical Turk Survey that was actually provided to", "tokens": [400, 550, 321, 29919, 300, 365, 257, 30175, 804, 15714, 33365, 300, 390, 767, 5649, 281], "temperature": 0.0, "avg_logprob": -0.1520613981096932, "compression_ratio": 1.6801801801801801, "no_speech_prob": 1.4284180906543043e-05}, {"id": 91, "seek": 46220, "start": 474.59999999999997, "end": 480.36, "text": " us by the authors of the first paper, the Baluk Basi paper that I put up.", "tokens": [505, 538, 264, 16552, 295, 264, 700, 3035, 11, 264, 13140, 2034, 5859, 72, 3035, 300, 286, 829, 493, 13], "temperature": 0.0, "avg_logprob": -0.1520613981096932, "compression_ratio": 1.6801801801801801, "no_speech_prob": 1.4284180906543043e-05}, {"id": 92, "seek": 46220, "start": 480.36, "end": 488.08, "text": " And here, what the authors there did was run a Mechanical Turk Survey on what workers characterize", "tokens": [400, 510, 11, 437, 264, 16552, 456, 630, 390, 1190, 257, 30175, 804, 15714, 33365, 322, 437, 5600, 38463], "temperature": 0.0, "avg_logprob": -0.1520613981096932, "compression_ratio": 1.6801801801801801, "no_speech_prob": 1.4284180906543043e-05}, {"id": 93, "seek": 46220, "start": 488.08, "end": 491.38, "text": " as stereotypes of each occupation.", "tokens": [382, 30853, 295, 1184, 24482, 13], "temperature": 0.0, "avg_logprob": -0.1520613981096932, "compression_ratio": 1.6801801801801801, "no_speech_prob": 1.4284180906543043e-05}, {"id": 94, "seek": 49138, "start": 491.38, "end": 495.64, "text": " And so this is trying to see, and yeah, I'll get into this.", "tokens": [400, 370, 341, 307, 1382, 281, 536, 11, 293, 1338, 11, 286, 603, 483, 666, 341, 13], "temperature": 0.0, "avg_logprob": -0.1585025377171014, "compression_ratio": 1.7014218009478672, "no_speech_prob": 1.862929275375791e-05}, {"id": 95, "seek": 49138, "start": 495.64, "end": 504.1, "text": " So what this plot shows is on the Y axis is the relative distances of each occupation", "tokens": [407, 437, 341, 7542, 3110, 307, 322, 264, 398, 10298, 307, 264, 4972, 22182, 295, 1184, 24482], "temperature": 0.0, "avg_logprob": -0.1585025377171014, "compression_ratio": 1.7014218009478672, "no_speech_prob": 1.862929275375791e-05}, {"id": 96, "seek": 49138, "start": 504.1, "end": 509.48, "text": " to either the group, to the set of words representing men or the set of words representing women", "tokens": [281, 2139, 264, 1594, 11, 281, 264, 992, 295, 2283, 13460, 1706, 420, 264, 992, 295, 2283, 13460, 2266], "temperature": 0.0, "avg_logprob": -0.1585025377171014, "compression_ratio": 1.7014218009478672, "no_speech_prob": 1.862929275375791e-05}, {"id": 97, "seek": 49138, "start": 509.48, "end": 511.0, "text": " in the embeddings.", "tokens": [294, 264, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.1585025377171014, "compression_ratio": 1.7014218009478672, "no_speech_prob": 1.862929275375791e-05}, {"id": 98, "seek": 49138, "start": 511.0, "end": 520.16, "text": " And then the X axis is the occupation difference by percentage in the US census, I think in", "tokens": [400, 550, 264, 1783, 10298, 307, 264, 24482, 2649, 538, 9668, 294, 264, 2546, 23725, 11, 286, 519, 294], "temperature": 0.0, "avg_logprob": -0.1585025377171014, "compression_ratio": 1.7014218009478672, "no_speech_prob": 1.862929275375791e-05}, {"id": 99, "seek": 49138, "start": 520.16, "end": 521.22, "text": " 2010.", "tokens": [9657, 13], "temperature": 0.0, "avg_logprob": -0.1585025377171014, "compression_ratio": 1.7014218009478672, "no_speech_prob": 1.862929275375791e-05}, {"id": 100, "seek": 52122, "start": 521.22, "end": 527.9200000000001, "text": " So it's somewhat noisy, but there's a clear pattern that embeddings are as a whole capturing", "tokens": [407, 309, 311, 8344, 24518, 11, 457, 456, 311, 257, 1850, 5102, 300, 12240, 29432, 366, 382, 257, 1379, 23384], "temperature": 0.0, "avg_logprob": -0.06144276057204155, "compression_ratio": 1.513089005235602, "no_speech_prob": 3.905208359356038e-06}, {"id": 101, "seek": 52122, "start": 527.9200000000001, "end": 538.2, "text": " the trend in how skewed the occupations are toward one gender or the other.", "tokens": [264, 6028, 294, 577, 8756, 26896, 264, 8073, 763, 366, 7361, 472, 7898, 420, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.06144276057204155, "compression_ratio": 1.513089005235602, "no_speech_prob": 3.905208359356038e-06}, {"id": 102, "seek": 52122, "start": 538.2, "end": 544.48, "text": " And we highlighted just a few sort of outlier occupations.", "tokens": [400, 321, 17173, 445, 257, 1326, 1333, 295, 484, 2753, 8073, 763, 13], "temperature": 0.0, "avg_logprob": -0.06144276057204155, "compression_ratio": 1.513089005235602, "no_speech_prob": 3.905208359356038e-06}, {"id": 103, "seek": 52122, "start": 544.48, "end": 547.6800000000001, "text": " And then we can augment this with the Mechanical Turk Survey.", "tokens": [400, 550, 321, 393, 29919, 341, 365, 264, 30175, 804, 15714, 33365, 13], "temperature": 0.0, "avg_logprob": -0.06144276057204155, "compression_ratio": 1.513089005235602, "no_speech_prob": 3.905208359356038e-06}, {"id": 104, "seek": 54768, "start": 547.68, "end": 555.56, "text": " So what it turns out is both human stereotypes and the stereotypes in the embeddings don't", "tokens": [407, 437, 309, 4523, 484, 307, 1293, 1952, 30853, 293, 264, 30853, 294, 264, 12240, 29432, 500, 380], "temperature": 0.0, "avg_logprob": -0.10714123374537418, "compression_ratio": 1.8518518518518519, "no_speech_prob": 3.0238510589697398e-05}, {"id": 105, "seek": 54768, "start": 555.56, "end": 558.4, "text": " actually reflect reality.", "tokens": [767, 5031, 4103, 13], "temperature": 0.0, "avg_logprob": -0.10714123374537418, "compression_ratio": 1.8518518518518519, "no_speech_prob": 3.0238510589697398e-05}, {"id": 106, "seek": 54768, "start": 558.4, "end": 562.06, "text": " Sort of here I'm defining reality as the occupation percentages.", "tokens": [26149, 295, 510, 286, 478, 17827, 4103, 382, 264, 24482, 42270, 13], "temperature": 0.0, "avg_logprob": -0.10714123374537418, "compression_ratio": 1.8518518518518519, "no_speech_prob": 3.0238510589697398e-05}, {"id": 107, "seek": 54768, "start": 562.06, "end": 568.4799999999999, "text": " And human stereotypes tend to be sort of in addition to that sometimes.", "tokens": [400, 1952, 30853, 3928, 281, 312, 1333, 295, 294, 4500, 281, 300, 2171, 13], "temperature": 0.0, "avg_logprob": -0.10714123374537418, "compression_ratio": 1.8518518518518519, "no_speech_prob": 3.0238510589697398e-05}, {"id": 108, "seek": 54768, "start": 568.4799999999999, "end": 570.68, "text": " And it turns out that the embeddings capture that difference.", "tokens": [400, 309, 4523, 484, 300, 264, 12240, 29432, 7983, 300, 2649, 13], "temperature": 0.0, "avg_logprob": -0.10714123374537418, "compression_ratio": 1.8518518518518519, "no_speech_prob": 3.0238510589697398e-05}, {"id": 109, "seek": 54768, "start": 570.68, "end": 576.24, "text": " So here, if you would see the residuals on the plots, the residuals on the plots are", "tokens": [407, 510, 11, 498, 291, 576, 536, 264, 27980, 82, 322, 264, 28609, 11, 264, 27980, 82, 322, 264, 28609, 366], "temperature": 0.0, "avg_logprob": -0.10714123374537418, "compression_ratio": 1.8518518518518519, "no_speech_prob": 3.0238510589697398e-05}, {"id": 110, "seek": 57624, "start": 576.24, "end": 583.24, "text": " one noise, but also variance that's unexplained by the occupation percentage.", "tokens": [472, 5658, 11, 457, 611, 21977, 300, 311, 11572, 564, 3563, 538, 264, 24482, 9668, 13], "temperature": 0.0, "avg_logprob": -0.1208853411984134, "compression_ratio": 1.6701030927835052, "no_speech_prob": 1.1658392395474948e-05}, {"id": 111, "seek": 57624, "start": 583.24, "end": 591.64, "text": " So biases in the embeddings that are somewhat additional to the skewness and the occupations.", "tokens": [407, 32152, 294, 264, 12240, 29432, 300, 366, 8344, 4497, 281, 264, 8756, 895, 442, 293, 264, 8073, 763, 13], "temperature": 0.0, "avg_logprob": -0.1208853411984134, "compression_ratio": 1.6701030927835052, "no_speech_prob": 1.1658392395474948e-05}, {"id": 112, "seek": 57624, "start": 591.64, "end": 597.48, "text": " And it turns out that these, the residuals here correlate with the mechanical Turk, with", "tokens": [400, 309, 4523, 484, 300, 613, 11, 264, 27980, 82, 510, 48742, 365, 264, 12070, 15714, 11, 365], "temperature": 0.0, "avg_logprob": -0.1208853411984134, "compression_ratio": 1.6701030927835052, "no_speech_prob": 1.1658392395474948e-05}, {"id": 113, "seek": 57624, "start": 597.48, "end": 601.26, "text": " the additional residuals on the Mechanical Turk Survey as well.", "tokens": [264, 4497, 27980, 82, 322, 264, 30175, 804, 15714, 33365, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1208853411984134, "compression_ratio": 1.6701030927835052, "no_speech_prob": 1.1658392395474948e-05}, {"id": 114, "seek": 60126, "start": 601.26, "end": 608.0, "text": " So sort of showing that this embeddings technique can capture both, again, I want to put underlying", "tokens": [407, 1333, 295, 4099, 300, 341, 12240, 29432, 6532, 393, 7983, 1293, 11, 797, 11, 286, 528, 281, 829, 14217], "temperature": 0.0, "avg_logprob": -0.19801767482313998, "compression_ratio": 1.550420168067227, "no_speech_prob": 4.592100992795167e-07}, {"id": 115, "seek": 60126, "start": 608.0, "end": 615.2, "text": " reality in very big air quotes, represented by the occupational data, but then also additional", "tokens": [4103, 294, 588, 955, 1988, 19963, 11, 10379, 538, 264, 43544, 1412, 11, 457, 550, 611, 4497], "temperature": 0.0, "avg_logprob": -0.19801767482313998, "compression_ratio": 1.550420168067227, "no_speech_prob": 4.592100992795167e-07}, {"id": 116, "seek": 60126, "start": 615.2, "end": 620.8, "text": " potential biases or stereotypes by human workers and mechanical Turk.", "tokens": [3995, 32152, 420, 30853, 538, 1952, 5600, 293, 12070, 15714, 13], "temperature": 0.0, "avg_logprob": -0.19801767482313998, "compression_ratio": 1.550420168067227, "no_speech_prob": 4.592100992795167e-07}, {"id": 117, "seek": 60126, "start": 620.8, "end": 622.92, "text": " I have a question.", "tokens": [286, 362, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19801767482313998, "compression_ratio": 1.550420168067227, "no_speech_prob": 4.592100992795167e-07}, {"id": 118, "seek": 60126, "start": 622.92, "end": 629.12, "text": " So with the, I guess like the distance from the line, for instance, is nurse, is this", "tokens": [407, 365, 264, 11, 286, 2041, 411, 264, 4560, 490, 264, 1622, 11, 337, 5197, 11, 307, 14012, 11, 307, 341], "temperature": 0.0, "avg_logprob": -0.19801767482313998, "compression_ratio": 1.550420168067227, "no_speech_prob": 4.592100992795167e-07}, {"id": 119, "seek": 62912, "start": 629.12, "end": 635.92, "text": " showing that kind of both the embeddings and people expect way more women to be nurses", "tokens": [4099, 300, 733, 295, 1293, 264, 12240, 29432, 293, 561, 2066, 636, 544, 2266, 281, 312, 17446], "temperature": 0.0, "avg_logprob": -0.1332156265838237, "compression_ratio": 1.6236559139784945, "no_speech_prob": 2.9943867048132233e-06}, {"id": 120, "seek": 62912, "start": 635.92, "end": 643.8, "text": " than fits the occupational data of more women are nurses, but not to the extent that the", "tokens": [813, 9001, 264, 43544, 1412, 295, 544, 2266, 366, 17446, 11, 457, 406, 281, 264, 8396, 300, 264], "temperature": 0.0, "avg_logprob": -0.1332156265838237, "compression_ratio": 1.6236559139784945, "no_speech_prob": 2.9943867048132233e-06}, {"id": 121, "seek": 62912, "start": 643.8, "end": 644.8, "text": " embeddings reflect?", "tokens": [12240, 29432, 5031, 30], "temperature": 0.0, "avg_logprob": -0.1332156265838237, "compression_ratio": 1.6236559139784945, "no_speech_prob": 2.9943867048132233e-06}, {"id": 122, "seek": 62912, "start": 644.8, "end": 645.8, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.1332156265838237, "compression_ratio": 1.6236559139784945, "no_speech_prob": 2.9943867048132233e-06}, {"id": 123, "seek": 62912, "start": 645.8, "end": 646.8, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.1332156265838237, "compression_ratio": 1.6236559139784945, "no_speech_prob": 2.9943867048132233e-06}, {"id": 124, "seek": 62912, "start": 646.8, "end": 647.8, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1332156265838237, "compression_ratio": 1.6236559139784945, "no_speech_prob": 2.9943867048132233e-06}, {"id": 125, "seek": 62912, "start": 647.8, "end": 651.76, "text": " So it's kind of like the bias has been amplified or the stereotype has been amplified.", "tokens": [407, 309, 311, 733, 295, 411, 264, 12577, 575, 668, 49237, 420, 264, 38229, 575, 668, 49237, 13], "temperature": 0.0, "avg_logprob": -0.1332156265838237, "compression_ratio": 1.6236559139784945, "no_speech_prob": 2.9943867048132233e-06}, {"id": 126, "seek": 65176, "start": 651.76, "end": 659.56, "text": " And so I don't want to make that specific claim in that at least we didn't find any", "tokens": [400, 370, 286, 500, 380, 528, 281, 652, 300, 2685, 3932, 294, 300, 412, 1935, 321, 994, 380, 915, 604], "temperature": 0.0, "avg_logprob": -0.171182237822434, "compression_ratio": 1.6318181818181818, "no_speech_prob": 5.804944294141023e-07}, {"id": 127, "seek": 65176, "start": 659.56, "end": 666.24, "text": " like systematic pattern of like certain types of occupations is amplified or so on.", "tokens": [411, 27249, 5102, 295, 411, 1629, 3467, 295, 8073, 763, 307, 49237, 420, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.171182237822434, "compression_ratio": 1.6318181818181818, "no_speech_prob": 5.804944294141023e-07}, {"id": 128, "seek": 65176, "start": 666.24, "end": 670.08, "text": " It's just that the residual in either direction is explained.", "tokens": [467, 311, 445, 300, 264, 27980, 294, 2139, 3513, 307, 8825, 13], "temperature": 0.0, "avg_logprob": -0.171182237822434, "compression_ratio": 1.6318181818181818, "no_speech_prob": 5.804944294141023e-07}, {"id": 129, "seek": 65176, "start": 670.08, "end": 672.6, "text": " It's somewhat explained by the embeddings.", "tokens": [467, 311, 8344, 8825, 538, 264, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.171182237822434, "compression_ratio": 1.6318181818181818, "no_speech_prob": 5.804944294141023e-07}, {"id": 130, "seek": 65176, "start": 672.6, "end": 678.8, "text": " And then so with secretary, the embedding, I guess kind of captured secretary as being", "tokens": [400, 550, 370, 365, 15691, 11, 264, 12240, 3584, 11, 286, 2041, 733, 295, 11828, 15691, 382, 885], "temperature": 0.0, "avg_logprob": -0.171182237822434, "compression_ratio": 1.6318181818181818, "no_speech_prob": 5.804944294141023e-07}, {"id": 131, "seek": 67880, "start": 678.8, "end": 685.88, "text": " more masculine than the, the occupational data.", "tokens": [544, 28992, 813, 264, 11, 264, 43544, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1855496868644793, "compression_ratio": 1.7125, "no_speech_prob": 1.1015803238478838e-06}, {"id": 132, "seek": 67880, "start": 685.88, "end": 686.88, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.1855496868644793, "compression_ratio": 1.7125, "no_speech_prob": 1.1015803238478838e-06}, {"id": 133, "seek": 67880, "start": 686.88, "end": 692.24, "text": " And I think the secretary one in particular might've actually been just because the disambiguation", "tokens": [400, 286, 519, 264, 15691, 472, 294, 1729, 1062, 600, 767, 668, 445, 570, 264, 717, 2173, 328, 16073], "temperature": 0.0, "avg_logprob": -0.1855496868644793, "compression_ratio": 1.7125, "no_speech_prob": 1.1015803238478838e-06}, {"id": 134, "seek": 67880, "start": 692.24, "end": 698.7199999999999, "text": " of secretary into maybe cabinet secretary in the United States and other types of secretaries.", "tokens": [295, 15691, 666, 1310, 15188, 15691, 294, 264, 2824, 3040, 293, 661, 3467, 295, 4054, 4889, 13], "temperature": 0.0, "avg_logprob": -0.1855496868644793, "compression_ratio": 1.7125, "no_speech_prob": 1.1015803238478838e-06}, {"id": 135, "seek": 67880, "start": 698.7199999999999, "end": 703.92, "text": " So yes, and not all of these were not all of these residuals would be captured by human", "tokens": [407, 2086, 11, 293, 406, 439, 295, 613, 645, 406, 439, 295, 613, 27980, 82, 576, 312, 11828, 538, 1952], "temperature": 0.0, "avg_logprob": -0.1855496868644793, "compression_ratio": 1.7125, "no_speech_prob": 1.1015803238478838e-06}, {"id": 136, "seek": 67880, "start": 703.92, "end": 705.4399999999999, "text": " stereotypes.", "tokens": [30853, 13], "temperature": 0.0, "avg_logprob": -0.1855496868644793, "compression_ratio": 1.7125, "no_speech_prob": 1.1015803238478838e-06}, {"id": 137, "seek": 67880, "start": 705.4399999999999, "end": 707.92, "text": " Some of these are of course due to other technical limitations.", "tokens": [2188, 295, 613, 366, 295, 1164, 3462, 281, 661, 6191, 15705, 13], "temperature": 0.0, "avg_logprob": -0.1855496868644793, "compression_ratio": 1.7125, "no_speech_prob": 1.1015803238478838e-06}, {"id": 138, "seek": 70792, "start": 707.92, "end": 711.28, "text": " Well, thank you.", "tokens": [1042, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.16610886710030692, "compression_ratio": 1.553030303030303, "no_speech_prob": 2.947898337879451e-06}, {"id": 139, "seek": 70792, "start": 711.28, "end": 713.4, "text": " Any other questions or?", "tokens": [2639, 661, 1651, 420, 30], "temperature": 0.0, "avg_logprob": -0.16610886710030692, "compression_ratio": 1.553030303030303, "no_speech_prob": 2.947898337879451e-06}, {"id": 140, "seek": 70792, "start": 713.4, "end": 714.9399999999999, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.16610886710030692, "compression_ratio": 1.553030303030303, "no_speech_prob": 2.947898337879451e-06}, {"id": 141, "seek": 70792, "start": 714.9399999999999, "end": 720.52, "text": " So once we have this general framework, we want to sort of verify it further.", "tokens": [407, 1564, 321, 362, 341, 2674, 8388, 11, 321, 528, 281, 1333, 295, 16888, 309, 3052, 13], "temperature": 0.0, "avg_logprob": -0.16610886710030692, "compression_ratio": 1.553030303030303, "no_speech_prob": 2.947898337879451e-06}, {"id": 142, "seek": 70792, "start": 720.52, "end": 725.04, "text": " And in particular, what the interest of our work was, does it, can we do this consistently", "tokens": [400, 294, 1729, 11, 437, 264, 1179, 295, 527, 589, 390, 11, 775, 309, 11, 393, 321, 360, 341, 14961], "temperature": 0.0, "avg_logprob": -0.16610886710030692, "compression_ratio": 1.553030303030303, "no_speech_prob": 2.947898337879451e-06}, {"id": 143, "seek": 70792, "start": 725.04, "end": 726.04, "text": " over time?", "tokens": [670, 565, 30], "temperature": 0.0, "avg_logprob": -0.16610886710030692, "compression_ratio": 1.553030303030303, "no_speech_prob": 2.947898337879451e-06}, {"id": 144, "seek": 70792, "start": 726.04, "end": 731.8, "text": " So not just on one embedding in a snapshot setting, but can we consistently look at biases", "tokens": [407, 406, 445, 322, 472, 12240, 3584, 294, 257, 30163, 3287, 11, 457, 393, 321, 14961, 574, 412, 32152], "temperature": 0.0, "avg_logprob": -0.16610886710030692, "compression_ratio": 1.553030303030303, "no_speech_prob": 2.947898337879451e-06}, {"id": 145, "seek": 70792, "start": 731.8, "end": 737.62, "text": " embeddings and say 1940 versus 1960, and then say something meaningful about the differences", "tokens": [12240, 29432, 293, 584, 24158, 5717, 16157, 11, 293, 550, 584, 746, 10995, 466, 264, 7300], "temperature": 0.0, "avg_logprob": -0.16610886710030692, "compression_ratio": 1.553030303030303, "no_speech_prob": 2.947898337879451e-06}, {"id": 146, "seek": 73762, "start": 737.62, "end": 740.5600000000001, "text": " in those biases.", "tokens": [294, 729, 32152, 13], "temperature": 0.0, "avg_logprob": -0.1414468664872019, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.753204717009794e-05}, {"id": 147, "seek": 73762, "start": 740.5600000000001, "end": 748.2, "text": " So in particularly, we want to verify that embeddings are calibrated to changes and whatever", "tokens": [407, 294, 4098, 11, 321, 528, 281, 16888, 300, 12240, 29432, 366, 21583, 5468, 281, 2962, 293, 2035], "temperature": 0.0, "avg_logprob": -0.1414468664872019, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.753204717009794e-05}, {"id": 148, "seek": 73762, "start": 748.2, "end": 751.76, "text": " else biases of historical beliefs.", "tokens": [1646, 32152, 295, 8584, 13585, 13], "temperature": 0.0, "avg_logprob": -0.1414468664872019, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.753204717009794e-05}, {"id": 149, "seek": 73762, "start": 751.76, "end": 756.68, "text": " And so one thing that we do, for example, is continuing this trend of using census data.", "tokens": [400, 370, 472, 551, 300, 321, 360, 11, 337, 1365, 11, 307, 9289, 341, 6028, 295, 1228, 23725, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1414468664872019, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.753204717009794e-05}, {"id": 150, "seek": 73762, "start": 756.68, "end": 761.6800000000001, "text": " We using the corpus of historical American English, which is essentially a gender or", "tokens": [492, 1228, 264, 1181, 31624, 295, 8584, 2665, 3669, 11, 597, 307, 4476, 257, 7898, 420], "temperature": 0.0, "avg_logprob": -0.1414468664872019, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.753204717009794e-05}, {"id": 151, "seek": 76168, "start": 761.68, "end": 771.1999999999999, "text": " not gender balance, it's a sort of genre balanced collection of books of American English written", "tokens": [406, 7898, 4772, 11, 309, 311, 257, 1333, 295, 11022, 13902, 5765, 295, 3642, 295, 2665, 3669, 3720], "temperature": 0.0, "avg_logprob": -0.11869706177129978, "compression_ratio": 1.5497630331753554, "no_speech_prob": 8.937726306612603e-06}, {"id": 152, "seek": 76168, "start": 771.1999999999999, "end": 773.3399999999999, "text": " throughout the 20th century.", "tokens": [3710, 264, 945, 392, 4901, 13], "temperature": 0.0, "avg_logprob": -0.11869706177129978, "compression_ratio": 1.5497630331753554, "no_speech_prob": 8.937726306612603e-06}, {"id": 153, "seek": 76168, "start": 773.3399999999999, "end": 777.9599999999999, "text": " So using this corpus, we can train embeddings for every decade.", "tokens": [407, 1228, 341, 1181, 31624, 11, 321, 393, 3847, 12240, 29432, 337, 633, 10378, 13], "temperature": 0.0, "avg_logprob": -0.11869706177129978, "compression_ratio": 1.5497630331753554, "no_speech_prob": 8.937726306612603e-06}, {"id": 154, "seek": 76168, "start": 777.9599999999999, "end": 785.3199999999999, "text": " And then we can capture the, as we did in the previous slide, we can do the average", "tokens": [400, 550, 321, 393, 7983, 264, 11, 382, 321, 630, 294, 264, 3894, 4137, 11, 321, 393, 360, 264, 4274], "temperature": 0.0, "avg_logprob": -0.11869706177129978, "compression_ratio": 1.5497630331753554, "no_speech_prob": 8.937726306612603e-06}, {"id": 155, "seek": 76168, "start": 785.3199999999999, "end": 789.92, "text": " bias in the consistent set of occupations over time.", "tokens": [12577, 294, 264, 8398, 992, 295, 8073, 763, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.11869706177129978, "compression_ratio": 1.5497630331753554, "no_speech_prob": 8.937726306612603e-06}, {"id": 156, "seek": 78992, "start": 789.92, "end": 795.4399999999999, "text": " And then we can compare that to the mean of women in those occupations throughout the", "tokens": [400, 550, 321, 393, 6794, 300, 281, 264, 914, 295, 2266, 294, 729, 8073, 763, 3710, 264], "temperature": 0.0, "avg_logprob": -0.10946453164476867, "compression_ratio": 1.7682926829268293, "no_speech_prob": 1.1841531886602752e-05}, {"id": 157, "seek": 78992, "start": 795.4399999999999, "end": 797.56, "text": " 20th century as well.", "tokens": [945, 392, 4901, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10946453164476867, "compression_ratio": 1.7682926829268293, "no_speech_prob": 1.1841531886602752e-05}, {"id": 158, "seek": 78992, "start": 797.56, "end": 802.4799999999999, "text": " And sort of what we just, there's of course some noise, but what I want to emphasize here", "tokens": [400, 1333, 295, 437, 321, 445, 11, 456, 311, 295, 1164, 512, 5658, 11, 457, 437, 286, 528, 281, 16078, 510], "temperature": 0.0, "avg_logprob": -0.10946453164476867, "compression_ratio": 1.7682926829268293, "no_speech_prob": 1.1841531886602752e-05}, {"id": 159, "seek": 78992, "start": 802.4799999999999, "end": 808.36, "text": " is that there's some hope for calibration is that there's a consistent mapping, roughly", "tokens": [307, 300, 456, 311, 512, 1454, 337, 38732, 307, 300, 456, 311, 257, 8398, 18350, 11, 9810], "temperature": 0.0, "avg_logprob": -0.10946453164476867, "compression_ratio": 1.7682926829268293, "no_speech_prob": 1.1841531886602752e-05}, {"id": 160, "seek": 78992, "start": 808.36, "end": 813.5999999999999, "text": " consistent mapping from, let's see what the bias was in 1920 to what that maps to in terms", "tokens": [8398, 18350, 490, 11, 718, 311, 536, 437, 264, 12577, 390, 294, 22003, 281, 437, 300, 11317, 281, 294, 2115], "temperature": 0.0, "avg_logprob": -0.10946453164476867, "compression_ratio": 1.7682926829268293, "no_speech_prob": 1.1841531886602752e-05}, {"id": 161, "seek": 78992, "start": 813.5999999999999, "end": 815.26, "text": " of occupation percentage.", "tokens": [295, 24482, 9668, 13], "temperature": 0.0, "avg_logprob": -0.10946453164476867, "compression_ratio": 1.7682926829268293, "no_speech_prob": 1.1841531886602752e-05}, {"id": 162, "seek": 78992, "start": 815.26, "end": 819.74, "text": " And then the same thing in 1980.", "tokens": [400, 550, 264, 912, 551, 294, 13626, 13], "temperature": 0.0, "avg_logprob": -0.10946453164476867, "compression_ratio": 1.7682926829268293, "no_speech_prob": 1.1841531886602752e-05}, {"id": 163, "seek": 81974, "start": 819.74, "end": 824.1, "text": " We do this for sort of a few other settings where we have historical data.", "tokens": [492, 360, 341, 337, 1333, 295, 257, 1326, 661, 6257, 689, 321, 362, 8584, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11599651771255687, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.1841051673400216e-05}, {"id": 164, "seek": 81974, "start": 824.1, "end": 831.2, "text": " So there are a few sets of historical surveys that were repeated over time.", "tokens": [407, 456, 366, 257, 1326, 6352, 295, 8584, 22711, 300, 645, 10477, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.11599651771255687, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.1841051673400216e-05}, {"id": 165, "seek": 81974, "start": 831.2, "end": 840.72, "text": " So there's the famous Princeton trilogy, which a researcher in 1933, 51, and one other date", "tokens": [407, 456, 311, 264, 4618, 36592, 34030, 11, 597, 257, 21751, 294, 48390, 11, 18485, 11, 293, 472, 661, 4002], "temperature": 0.0, "avg_logprob": -0.11599651771255687, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.1841051673400216e-05}, {"id": 166, "seek": 81974, "start": 840.72, "end": 848.16, "text": " that I'm forgetting, asked the set of Princeton undergraduates, their stereotypes of a whole", "tokens": [300, 286, 478, 25428, 11, 2351, 264, 992, 295, 36592, 14295, 27710, 11, 641, 30853, 295, 257, 1379], "temperature": 0.0, "avg_logprob": -0.11599651771255687, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.1841051673400216e-05}, {"id": 167, "seek": 84816, "start": 848.16, "end": 849.92, "text": " bunch of different ethnicities.", "tokens": [3840, 295, 819, 14363, 1088, 13], "temperature": 0.0, "avg_logprob": -0.19321194887161255, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.2218292795296293e-05}, {"id": 168, "seek": 84816, "start": 849.92, "end": 855.88, "text": " And yeah, those are somewhat disturbing and fascinating to go through.", "tokens": [400, 1338, 11, 729, 366, 8344, 21903, 293, 10343, 281, 352, 807, 13], "temperature": 0.0, "avg_logprob": -0.19321194887161255, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.2218292795296293e-05}, {"id": 169, "seek": 84816, "start": 855.88, "end": 862.36, "text": " But yeah, so this researcher asked consistently very similar questions on stereotypes for", "tokens": [583, 1338, 11, 370, 341, 21751, 2351, 14961, 588, 2531, 1651, 322, 30853, 337], "temperature": 0.0, "avg_logprob": -0.19321194887161255, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.2218292795296293e-05}, {"id": 170, "seek": 84816, "start": 862.36, "end": 863.36, "text": " ethnicities.", "tokens": [14363, 1088, 13], "temperature": 0.0, "avg_logprob": -0.19321194887161255, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.2218292795296293e-05}, {"id": 171, "seek": 84816, "start": 863.36, "end": 868.48, "text": " And we can essentially just replicate that the differences in the stereotypes between", "tokens": [400, 321, 393, 4476, 445, 25356, 300, 264, 7300, 294, 264, 30853, 1296], "temperature": 0.0, "avg_logprob": -0.19321194887161255, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.2218292795296293e-05}, {"id": 172, "seek": 84816, "start": 868.48, "end": 874.24, "text": " 1933 and 51 are also reflected in the differences in the beddings from those decades.", "tokens": [48390, 293, 18485, 366, 611, 15502, 294, 264, 7300, 294, 264, 2901, 29432, 490, 729, 7878, 13], "temperature": 0.0, "avg_logprob": -0.19321194887161255, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.2218292795296293e-05}, {"id": 173, "seek": 87424, "start": 874.24, "end": 880.48, "text": " And of course, we use the Kinabaturk survey in the previous slide.", "tokens": [400, 295, 1164, 11, 321, 764, 264, 27950, 455, 267, 374, 74, 8984, 294, 264, 3894, 4137, 13], "temperature": 0.0, "avg_logprob": -0.2237587184696407, "compression_ratio": 1.5862068965517242, "no_speech_prob": 1.3630031389766373e-05}, {"id": 174, "seek": 87424, "start": 880.48, "end": 888.08, "text": " Okay, so now I'm just going to do a fairly quick run through a bunch of various stereotypes", "tokens": [1033, 11, 370, 586, 286, 478, 445, 516, 281, 360, 257, 6457, 1702, 1190, 807, 257, 3840, 295, 3683, 30853], "temperature": 0.0, "avg_logprob": -0.2237587184696407, "compression_ratio": 1.5862068965517242, "no_speech_prob": 1.3630031389766373e-05}, {"id": 175, "seek": 87424, "start": 888.08, "end": 891.04, "text": " that we quantified in the paper.", "tokens": [300, 321, 4426, 2587, 294, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.2237587184696407, "compression_ratio": 1.5862068965517242, "no_speech_prob": 1.3630031389766373e-05}, {"id": 176, "seek": 87424, "start": 891.04, "end": 896.5600000000001, "text": " These are, I want to emphasize, exploratory in the sense that we can't validate each of", "tokens": [1981, 366, 11, 286, 528, 281, 16078, 11, 24765, 4745, 294, 264, 2020, 300, 321, 393, 380, 29562, 1184, 295], "temperature": 0.0, "avg_logprob": -0.2237587184696407, "compression_ratio": 1.5862068965517242, "no_speech_prob": 1.3630031389766373e-05}, {"id": 177, "seek": 87424, "start": 896.5600000000001, "end": 902.72, "text": " these against some fixed outside measurement because a lot of those outside measurements", "tokens": [613, 1970, 512, 6806, 2380, 13160, 570, 257, 688, 295, 729, 2380, 15383], "temperature": 0.0, "avg_logprob": -0.2237587184696407, "compression_ratio": 1.5862068965517242, "no_speech_prob": 1.3630031389766373e-05}, {"id": 178, "seek": 90272, "start": 902.72, "end": 905.0400000000001, "text": " those don't exist.", "tokens": [729, 500, 380, 2514, 13], "temperature": 0.0, "avg_logprob": -0.1821273456920277, "compression_ratio": 1.5228426395939085, "no_speech_prob": 9.07954563444946e-06}, {"id": 179, "seek": 90272, "start": 905.0400000000001, "end": 909.8000000000001, "text": " Also want to give the warning that I'm going to present some stereotypes in the slides.", "tokens": [2743, 528, 281, 976, 264, 9164, 300, 286, 478, 516, 281, 1974, 512, 30853, 294, 264, 9788, 13], "temperature": 0.0, "avg_logprob": -0.1821273456920277, "compression_ratio": 1.5228426395939085, "no_speech_prob": 9.07954563444946e-06}, {"id": 180, "seek": 90272, "start": 909.8000000000001, "end": 917.0400000000001, "text": " So they're going to be gender and ethnic, so if you're uncomfortable with that.", "tokens": [407, 436, 434, 516, 281, 312, 7898, 293, 14363, 11, 370, 498, 291, 434, 10532, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.1821273456920277, "compression_ratio": 1.5228426395939085, "no_speech_prob": 9.07954563444946e-06}, {"id": 181, "seek": 90272, "start": 917.0400000000001, "end": 924.48, "text": " So this first slide shows Asian stereotypes in the corpus of historical American English", "tokens": [407, 341, 700, 4137, 3110, 10645, 30853, 294, 264, 1181, 31624, 295, 8584, 2665, 3669], "temperature": 0.0, "avg_logprob": -0.1821273456920277, "compression_ratio": 1.5228426395939085, "no_speech_prob": 9.07954563444946e-06}, {"id": 182, "seek": 90272, "start": 924.48, "end": 927.84, "text": " in 1910, 1950, and 1990.", "tokens": [294, 1294, 3279, 11, 18141, 11, 293, 13384, 13], "temperature": 0.0, "avg_logprob": -0.1821273456920277, "compression_ratio": 1.5228426395939085, "no_speech_prob": 9.07954563444946e-06}, {"id": 183, "seek": 92784, "start": 927.84, "end": 934.76, "text": " And so how we generated these is sort of the same high level methodology of collecting", "tokens": [400, 370, 577, 321, 10833, 613, 307, 1333, 295, 264, 912, 1090, 1496, 24850, 295, 12510], "temperature": 0.0, "avg_logprob": -0.16516015336320206, "compression_ratio": 1.6721311475409837, "no_speech_prob": 2.586007394711487e-05}, {"id": 184, "seek": 92784, "start": 934.76, "end": 942.2800000000001, "text": " a group of, so a set of group words, which are just common Asian last names, and then", "tokens": [257, 1594, 295, 11, 370, 257, 992, 295, 1594, 2283, 11, 597, 366, 445, 2689, 10645, 1036, 5288, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.16516015336320206, "compression_ratio": 1.6721311475409837, "no_speech_prob": 2.586007394711487e-05}, {"id": 185, "seek": 92784, "start": 942.2800000000001, "end": 946.4, "text": " a very large list of adjectives found online.", "tokens": [257, 588, 2416, 1329, 295, 29378, 1539, 1352, 2950, 13], "temperature": 0.0, "avg_logprob": -0.16516015336320206, "compression_ratio": 1.6721311475409837, "no_speech_prob": 2.586007394711487e-05}, {"id": 186, "seek": 92784, "start": 946.4, "end": 953.2, "text": " And then we found which adjectives are most similar toward Asians and dissimilar toward", "tokens": [400, 550, 321, 1352, 597, 29378, 1539, 366, 881, 2531, 7361, 47724, 293, 7802, 332, 2202, 7361], "temperature": 0.0, "avg_logprob": -0.16516015336320206, "compression_ratio": 1.6721311475409837, "no_speech_prob": 2.586007394711487e-05}, {"id": 187, "seek": 95320, "start": 953.2, "end": 957.88, "text": " white last names and the embeddings in 1910, 1590.", "tokens": [2418, 1036, 5288, 293, 264, 12240, 29432, 294, 1294, 3279, 11, 2119, 7771, 13], "temperature": 0.0, "avg_logprob": -0.18260660673442639, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.4283940799941774e-05}, {"id": 188, "seek": 95320, "start": 957.88, "end": 963.48, "text": " And sort of there's a progression I think that you can see where 1910 is very much an", "tokens": [400, 1333, 295, 456, 311, 257, 18733, 286, 519, 300, 291, 393, 536, 689, 1294, 3279, 307, 588, 709, 364], "temperature": 0.0, "avg_logprob": -0.18260660673442639, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.4283940799941774e-05}, {"id": 189, "seek": 95320, "start": 963.48, "end": 965.9200000000001, "text": " outsider type of stereotype.", "tokens": [40484, 2010, 295, 38229, 13], "temperature": 0.0, "avg_logprob": -0.18260660673442639, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.4283940799941774e-05}, {"id": 190, "seek": 95320, "start": 965.9200000000001, "end": 971.5200000000001, "text": " This was before a lot of the initial waves of Asian immigration in the United States", "tokens": [639, 390, 949, 257, 688, 295, 264, 5883, 9417, 295, 10645, 13554, 294, 264, 2824, 3040], "temperature": 0.0, "avg_logprob": -0.18260660673442639, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.4283940799941774e-05}, {"id": 191, "seek": 95320, "start": 971.5200000000001, "end": 977.0400000000001, "text": " or sort of before what people considered integration.", "tokens": [420, 1333, 295, 949, 437, 561, 4888, 10980, 13], "temperature": 0.0, "avg_logprob": -0.18260660673442639, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.4283940799941774e-05}, {"id": 192, "seek": 95320, "start": 977.0400000000001, "end": 983.1600000000001, "text": " And then in 1990, these outsider stereotypes at least have been replaced with other, I", "tokens": [400, 550, 294, 13384, 11, 613, 40484, 30853, 412, 1935, 362, 668, 10772, 365, 661, 11, 286], "temperature": 0.0, "avg_logprob": -0.18260660673442639, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.4283940799941774e-05}, {"id": 193, "seek": 98316, "start": 983.16, "end": 989.0, "text": " mean, stereotypes.", "tokens": [914, 11, 30853, 13], "temperature": 0.0, "avg_logprob": -0.12131448353038114, "compression_ratio": 1.6684782608695652, "no_speech_prob": 2.6271636670571752e-05}, {"id": 194, "seek": 98316, "start": 989.0, "end": 995.4, "text": " This one is just the average association between Islam and terrorism in the New York Times", "tokens": [639, 472, 307, 445, 264, 4274, 14598, 1296, 8571, 293, 23917, 294, 264, 1873, 3609, 11366], "temperature": 0.0, "avg_logprob": -0.12131448353038114, "compression_ratio": 1.6684782608695652, "no_speech_prob": 2.6271636670571752e-05}, {"id": 195, "seek": 98316, "start": 995.4, "end": 998.88, "text": " between 1988 and 2005.", "tokens": [1296, 27816, 293, 14394, 13], "temperature": 0.0, "avg_logprob": -0.12131448353038114, "compression_ratio": 1.6684782608695652, "no_speech_prob": 2.6271636670571752e-05}, {"id": 196, "seek": 98316, "start": 998.88, "end": 1006.0799999999999, "text": " So same high level methodology here is we collected a bunch of words that we considered", "tokens": [407, 912, 1090, 1496, 24850, 510, 307, 321, 11087, 257, 3840, 295, 2283, 300, 321, 4888], "temperature": 0.0, "avg_logprob": -0.12131448353038114, "compression_ratio": 1.6684782608695652, "no_speech_prob": 2.6271636670571752e-05}, {"id": 197, "seek": 98316, "start": 1006.0799999999999, "end": 1011.3199999999999, "text": " to represent Islam and also a bunch of words that we considered to represent terrorism", "tokens": [281, 2906, 8571, 293, 611, 257, 3840, 295, 2283, 300, 321, 4888, 281, 2906, 23917], "temperature": 0.0, "avg_logprob": -0.12131448353038114, "compression_ratio": 1.6684782608695652, "no_speech_prob": 2.6271636670571752e-05}, {"id": 198, "seek": 101132, "start": 1011.32, "end": 1017.8000000000001, "text": " and just looked at the differences between sort of how close those words were to words", "tokens": [293, 445, 2956, 412, 264, 7300, 1296, 1333, 295, 577, 1998, 729, 2283, 645, 281, 2283], "temperature": 0.0, "avg_logprob": -0.14063938627851771, "compression_ratio": 1.538152610441767, "no_speech_prob": 1.280331525777001e-05}, {"id": 199, "seek": 101132, "start": 1017.8000000000001, "end": 1022.3000000000001, "text": " representing Islam versus words representing Christianity in the New York Times.", "tokens": [13460, 8571, 5717, 2283, 13460, 17326, 294, 264, 1873, 3609, 11366, 13], "temperature": 0.0, "avg_logprob": -0.14063938627851771, "compression_ratio": 1.538152610441767, "no_speech_prob": 1.280331525777001e-05}, {"id": 200, "seek": 101132, "start": 1022.3000000000001, "end": 1029.8, "text": " And so there's some peaks after, for example, the World Trade Center bombings in 1993, as", "tokens": [400, 370, 456, 311, 512, 26897, 934, 11, 337, 1365, 11, 264, 3937, 23923, 5169, 7851, 1109, 294, 25137, 11, 382], "temperature": 0.0, "avg_logprob": -0.14063938627851771, "compression_ratio": 1.538152610441767, "no_speech_prob": 1.280331525777001e-05}, {"id": 201, "seek": 101132, "start": 1029.8, "end": 1032.1200000000001, "text": " well as of course 9-11.", "tokens": [731, 382, 295, 1164, 1722, 12, 5348, 13], "temperature": 0.0, "avg_logprob": -0.14063938627851771, "compression_ratio": 1.538152610441767, "no_speech_prob": 1.280331525777001e-05}, {"id": 202, "seek": 101132, "start": 1032.1200000000001, "end": 1037.0, "text": " And it would be, I think, interesting to extend this past 2005 with a larger New York Times", "tokens": [400, 309, 576, 312, 11, 286, 519, 11, 1880, 281, 10101, 341, 1791, 14394, 365, 257, 4833, 1873, 3609, 11366], "temperature": 0.0, "avg_logprob": -0.14063938627851771, "compression_ratio": 1.538152610441767, "no_speech_prob": 1.280331525777001e-05}, {"id": 203, "seek": 101132, "start": 1037.0, "end": 1040.66, "text": " data set.", "tokens": [1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.14063938627851771, "compression_ratio": 1.538152610441767, "no_speech_prob": 1.280331525777001e-05}, {"id": 204, "seek": 104066, "start": 1040.66, "end": 1044.76, "text": " This one's, I think, a little tougher to explain.", "tokens": [639, 472, 311, 11, 286, 519, 11, 257, 707, 30298, 281, 2903, 13], "temperature": 0.0, "avg_logprob": -0.13062500953674316, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.341777988272952e-05}, {"id": 205, "seek": 104066, "start": 1044.76, "end": 1047.8000000000002, "text": " So what this is is a heat map across decades.", "tokens": [407, 437, 341, 307, 307, 257, 3738, 4471, 2108, 7878, 13], "temperature": 0.0, "avg_logprob": -0.13062500953674316, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.341777988272952e-05}, {"id": 206, "seek": 104066, "start": 1047.8000000000002, "end": 1054.0800000000002, "text": " So we have in both the X and Y axis, we have from 1910 to 1990.", "tokens": [407, 321, 362, 294, 1293, 264, 1783, 293, 398, 10298, 11, 321, 362, 490, 1294, 3279, 281, 13384, 13], "temperature": 0.0, "avg_logprob": -0.13062500953674316, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.341777988272952e-05}, {"id": 207, "seek": 104066, "start": 1054.0800000000002, "end": 1061.68, "text": " And these are just the correlations and how similar language is in those two decades with", "tokens": [400, 613, 366, 445, 264, 13983, 763, 293, 577, 2531, 2856, 307, 294, 729, 732, 7878, 365], "temperature": 0.0, "avg_logprob": -0.13062500953674316, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.341777988272952e-05}, {"id": 208, "seek": 104066, "start": 1061.68, "end": 1066.46, "text": " respect to adjectives and gender differences.", "tokens": [3104, 281, 29378, 1539, 293, 7898, 7300, 13], "temperature": 0.0, "avg_logprob": -0.13062500953674316, "compression_ratio": 1.4676616915422886, "no_speech_prob": 1.341777988272952e-05}, {"id": 209, "seek": 106646, "start": 1066.46, "end": 1074.2, "text": " So sort of the more blue the cell is, the more similarly biased or skewed adjectives", "tokens": [407, 1333, 295, 264, 544, 3344, 264, 2815, 307, 11, 264, 544, 14138, 28035, 420, 8756, 26896, 29378, 1539], "temperature": 0.0, "avg_logprob": -0.12896893137977236, "compression_ratio": 1.655, "no_speech_prob": 1.0615264727675822e-05}, {"id": 210, "seek": 106646, "start": 1074.2, "end": 1077.52, "text": " are as a whole in terms of gender.", "tokens": [366, 382, 257, 1379, 294, 2115, 295, 7898, 13], "temperature": 0.0, "avg_logprob": -0.12896893137977236, "compression_ratio": 1.655, "no_speech_prob": 1.0615264727675822e-05}, {"id": 211, "seek": 106646, "start": 1077.52, "end": 1080.2, "text": " And this is one that are...", "tokens": [400, 341, 307, 472, 300, 366, 485], "temperature": 0.0, "avg_logprob": -0.12896893137977236, "compression_ratio": 1.655, "no_speech_prob": 1.0615264727675822e-05}, {"id": 212, "seek": 106646, "start": 1080.2, "end": 1086.04, "text": " So one of the collaborators on the paper is actually a historian of gender and science.", "tokens": [407, 472, 295, 264, 39789, 322, 264, 3035, 307, 767, 257, 25139, 295, 7898, 293, 3497, 13], "temperature": 0.0, "avg_logprob": -0.12896893137977236, "compression_ratio": 1.655, "no_speech_prob": 1.0615264727675822e-05}, {"id": 213, "seek": 106646, "start": 1086.04, "end": 1090.32, "text": " And this is one, I think, that she particularly found interesting is sort of there's a somewhat", "tokens": [400, 341, 307, 472, 11, 286, 519, 11, 300, 750, 4098, 1352, 1880, 307, 1333, 295, 456, 311, 257, 8344], "temperature": 0.0, "avg_logprob": -0.12896893137977236, "compression_ratio": 1.655, "no_speech_prob": 1.0615264727675822e-05}, {"id": 214, "seek": 109032, "start": 1090.32, "end": 1097.56, "text": " marked shift in the 60s and 70s when the women's movement in the United States really brought", "tokens": [12658, 5513, 294, 264, 4060, 82, 293, 5285, 82, 562, 264, 2266, 311, 3963, 294, 264, 2824, 3040, 534, 3038], "temperature": 0.0, "avg_logprob": -0.201905971620141, "compression_ratio": 1.5047619047619047, "no_speech_prob": 9.079822120838799e-06}, {"id": 215, "seek": 109032, "start": 1097.56, "end": 1105.04, "text": " a lot of issues on how are women talked about in media or books or what have you.", "tokens": [257, 688, 295, 2663, 322, 577, 366, 2266, 2825, 466, 294, 3021, 420, 3642, 420, 437, 362, 291, 13], "temperature": 0.0, "avg_logprob": -0.201905971620141, "compression_ratio": 1.5047619047619047, "no_speech_prob": 9.079822120838799e-06}, {"id": 216, "seek": 109032, "start": 1105.04, "end": 1112.3999999999999, "text": " That at least we found somewhat of a quantification for that actually had an effect in, positive", "tokens": [663, 412, 1935, 321, 1352, 8344, 295, 257, 4426, 3774, 337, 300, 767, 632, 364, 1802, 294, 11, 3353], "temperature": 0.0, "avg_logprob": -0.201905971620141, "compression_ratio": 1.5047619047619047, "no_speech_prob": 9.079822120838799e-06}, {"id": 217, "seek": 109032, "start": 1112.3999999999999, "end": 1116.6, "text": " effect, I would say, in the language.", "tokens": [1802, 11, 286, 576, 584, 11, 294, 264, 2856, 13], "temperature": 0.0, "avg_logprob": -0.201905971620141, "compression_ratio": 1.5047619047619047, "no_speech_prob": 9.079822120838799e-06}, {"id": 218, "seek": 109032, "start": 1116.6, "end": 1119.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.201905971620141, "compression_ratio": 1.5047619047619047, "no_speech_prob": 9.079822120838799e-06}, {"id": 219, "seek": 111972, "start": 1119.72, "end": 1126.24, "text": " So that was just a very quick run through for the types of analysis that we did for", "tokens": [407, 300, 390, 445, 257, 588, 1702, 1190, 807, 337, 264, 3467, 295, 5215, 300, 321, 630, 337], "temperature": 0.0, "avg_logprob": -0.14118166068165572, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.9219400201109238e-05}, {"id": 220, "seek": 111972, "start": 1126.24, "end": 1130.32, "text": " quantifying various changes over time.", "tokens": [4426, 5489, 3683, 2962, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.14118166068165572, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.9219400201109238e-05}, {"id": 221, "seek": 111972, "start": 1130.32, "end": 1132.32, "text": " Now for the rest of the talk, I'm going to sort of...", "tokens": [823, 337, 264, 1472, 295, 264, 751, 11, 286, 478, 516, 281, 1333, 295, 485], "temperature": 0.0, "avg_logprob": -0.14118166068165572, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.9219400201109238e-05}, {"id": 222, "seek": 111972, "start": 1132.32, "end": 1138.1000000000001, "text": " I promise that there's a bunch of limitations in this sort of work and I'll just discuss", "tokens": [286, 6228, 300, 456, 311, 257, 3840, 295, 15705, 294, 341, 1333, 295, 589, 293, 286, 603, 445, 2248], "temperature": 0.0, "avg_logprob": -0.14118166068165572, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.9219400201109238e-05}, {"id": 223, "seek": 111972, "start": 1138.1000000000001, "end": 1143.22, "text": " a few of them.", "tokens": [257, 1326, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.14118166068165572, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.9219400201109238e-05}, {"id": 224, "seek": 111972, "start": 1143.22, "end": 1149.66, "text": " So here I brought back our high level framework and now I'm just emphasizing a few words that", "tokens": [407, 510, 286, 3038, 646, 527, 1090, 1496, 8388, 293, 586, 286, 478, 445, 45550, 257, 1326, 2283, 300], "temperature": 0.0, "avg_logprob": -0.14118166068165572, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.9219400201109238e-05}, {"id": 225, "seek": 114966, "start": 1149.66, "end": 1153.8600000000001, "text": " are, I think, indicative of the assumptions that we made.", "tokens": [366, 11, 286, 519, 11, 47513, 295, 264, 17695, 300, 321, 1027, 13], "temperature": 0.0, "avg_logprob": -0.12594838672214084, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.0288673365721479e-05}, {"id": 226, "seek": 114966, "start": 1153.8600000000001, "end": 1159.28, "text": " So we train embeddings on some data set that we claimed represents a society of interest.", "tokens": [407, 321, 3847, 12240, 29432, 322, 512, 1412, 992, 300, 321, 12941, 8855, 257, 4086, 295, 1179, 13], "temperature": 0.0, "avg_logprob": -0.12594838672214084, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.0288673365721479e-05}, {"id": 227, "seek": 114966, "start": 1159.28, "end": 1167.3200000000002, "text": " So for example, we trade books from 1910 and we're claiming that these are indicative of", "tokens": [407, 337, 1365, 11, 321, 4923, 3642, 490, 1294, 3279, 293, 321, 434, 19232, 300, 613, 366, 47513, 295], "temperature": 0.0, "avg_logprob": -0.12594838672214084, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.0288673365721479e-05}, {"id": 228, "seek": 114966, "start": 1167.3200000000002, "end": 1172.8400000000001, "text": " American societal thoughts or what have you in 1910.", "tokens": [2665, 33472, 4598, 420, 437, 362, 291, 294, 1294, 3279, 13], "temperature": 0.0, "avg_logprob": -0.12594838672214084, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.0288673365721479e-05}, {"id": 229, "seek": 114966, "start": 1172.8400000000001, "end": 1178.16, "text": " We compiled list of words that represent categories and then claim that these words are good enough", "tokens": [492, 36548, 1329, 295, 2283, 300, 2906, 10479, 293, 550, 3932, 300, 613, 2283, 366, 665, 1547], "temperature": 0.0, "avg_logprob": -0.12594838672214084, "compression_ratio": 1.691304347826087, "no_speech_prob": 1.0288673365721479e-05}, {"id": 230, "seek": 117816, "start": 1178.16, "end": 1181.0, "text": " to represent those categories.", "tokens": [281, 2906, 729, 10479, 13], "temperature": 0.0, "avg_logprob": -0.10192420265891335, "compression_ratio": 1.8436018957345972, "no_speech_prob": 6.0487336668302305e-06}, {"id": 231, "seek": 117816, "start": 1181.0, "end": 1185.96, "text": " And then we calculated the distance between groups of words and then we claimed or at", "tokens": [400, 550, 321, 15598, 264, 4560, 1296, 3935, 295, 2283, 293, 550, 321, 12941, 420, 412], "temperature": 0.0, "avg_logprob": -0.10192420265891335, "compression_ratio": 1.8436018957345972, "no_speech_prob": 6.0487336668302305e-06}, {"id": 232, "seek": 117816, "start": 1185.96, "end": 1191.52, "text": " least assumed that the distance is meaningful.", "tokens": [1935, 15895, 300, 264, 4560, 307, 10995, 13], "temperature": 0.0, "avg_logprob": -0.10192420265891335, "compression_ratio": 1.8436018957345972, "no_speech_prob": 6.0487336668302305e-06}, {"id": 233, "seek": 117816, "start": 1191.52, "end": 1198.0800000000002, "text": " Each of these steps is baked with some assumptions that I'll go through now and I think at least", "tokens": [6947, 295, 613, 4439, 307, 19453, 365, 512, 17695, 300, 286, 603, 352, 807, 586, 293, 286, 519, 412, 1935], "temperature": 0.0, "avg_logprob": -0.10192420265891335, "compression_ratio": 1.8436018957345972, "no_speech_prob": 6.0487336668302305e-06}, {"id": 234, "seek": 117816, "start": 1198.0800000000002, "end": 1202.1000000000001, "text": " a few of these assumptions are pretty common or I think problematic assumptions are pretty", "tokens": [257, 1326, 295, 613, 17695, 366, 1238, 2689, 420, 286, 519, 19011, 17695, 366, 1238], "temperature": 0.0, "avg_logprob": -0.10192420265891335, "compression_ratio": 1.8436018957345972, "no_speech_prob": 6.0487336668302305e-06}, {"id": 235, "seek": 117816, "start": 1202.1000000000001, "end": 1204.48, "text": " common in a lot of this type of work.", "tokens": [2689, 294, 257, 688, 295, 341, 2010, 295, 589, 13], "temperature": 0.0, "avg_logprob": -0.10192420265891335, "compression_ratio": 1.8436018957345972, "no_speech_prob": 6.0487336668302305e-06}, {"id": 236, "seek": 120448, "start": 1204.48, "end": 1210.04, "text": " And it's something to be aware of as maybe you're pursuing similar types.", "tokens": [400, 309, 311, 746, 281, 312, 3650, 295, 382, 1310, 291, 434, 20222, 2531, 3467, 13], "temperature": 0.0, "avg_logprob": -0.11694965362548829, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.6686779165174812e-05}, {"id": 237, "seek": 120448, "start": 1210.04, "end": 1217.16, "text": " Okay, so for example, we used an existing corpus of text for American English books", "tokens": [1033, 11, 370, 337, 1365, 11, 321, 1143, 364, 6741, 1181, 31624, 295, 2487, 337, 2665, 3669, 3642], "temperature": 0.0, "avg_logprob": -0.11694965362548829, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.6686779165174812e-05}, {"id": 238, "seek": 120448, "start": 1217.16, "end": 1221.16, "text": " and we claim that they represent historical American beliefs.", "tokens": [293, 321, 3932, 300, 436, 2906, 8584, 2665, 13585, 13], "temperature": 0.0, "avg_logprob": -0.11694965362548829, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.6686779165174812e-05}, {"id": 239, "seek": 120448, "start": 1221.16, "end": 1224.44, "text": " But there's a bunch of people that this doesn't include, right?", "tokens": [583, 456, 311, 257, 3840, 295, 561, 300, 341, 1177, 380, 4090, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11694965362548829, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.6686779165174812e-05}, {"id": 240, "seek": 120448, "start": 1224.44, "end": 1228.2, "text": " This doesn't include Americans who didn't publish books at that time.", "tokens": [639, 1177, 380, 4090, 6280, 567, 994, 380, 11374, 3642, 412, 300, 565, 13], "temperature": 0.0, "avg_logprob": -0.11694965362548829, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.6686779165174812e-05}, {"id": 241, "seek": 122820, "start": 1228.2, "end": 1236.52, "text": " This doesn't include Americans who published books in other languages or we compiled the", "tokens": [639, 1177, 380, 4090, 6280, 567, 6572, 3642, 294, 661, 8650, 420, 321, 36548, 264], "temperature": 0.0, "avg_logprob": -0.13412252649084314, "compression_ratio": 1.46875, "no_speech_prob": 8.185954811779084e-07}, {"id": 242, "seek": 122820, "start": 1236.52, "end": 1239.3600000000001, "text": " distance into a single metric from 1910.", "tokens": [4560, 666, 257, 2167, 20678, 490, 1294, 3279, 13], "temperature": 0.0, "avg_logprob": -0.13412252649084314, "compression_ratio": 1.46875, "no_speech_prob": 8.185954811779084e-07}, {"id": 243, "seek": 122820, "start": 1239.3600000000001, "end": 1243.56, "text": " But what about minority ideological opinions at that time?", "tokens": [583, 437, 466, 16166, 35341, 11819, 412, 300, 565, 30], "temperature": 0.0, "avg_logprob": -0.13412252649084314, "compression_ratio": 1.46875, "no_speech_prob": 8.185954811779084e-07}, {"id": 244, "seek": 122820, "start": 1243.56, "end": 1248.38, "text": " These are all washed away in some mean of distance.", "tokens": [1981, 366, 439, 16300, 1314, 294, 512, 914, 295, 4560, 13], "temperature": 0.0, "avg_logprob": -0.13412252649084314, "compression_ratio": 1.46875, "no_speech_prob": 8.185954811779084e-07}, {"id": 245, "seek": 122820, "start": 1248.38, "end": 1254.68, "text": " And then there's a whole bunch of, I think, power or whatever issues and who created the", "tokens": [400, 550, 456, 311, 257, 1379, 3840, 295, 11, 286, 519, 11, 1347, 420, 2035, 2663, 293, 567, 2942, 264], "temperature": 0.0, "avg_logprob": -0.13412252649084314, "compression_ratio": 1.46875, "no_speech_prob": 8.185954811779084e-07}, {"id": 246, "seek": 125468, "start": 1254.68, "end": 1259.92, "text": " data set and what decided what text would go into that corpus.", "tokens": [1412, 992, 293, 437, 3047, 437, 2487, 576, 352, 666, 300, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.1431756255650284, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.4968951290939003e-05}, {"id": 247, "seek": 125468, "start": 1259.92, "end": 1265.4, "text": " None of these decisions are sort of very, or at least some of them who just read the", "tokens": [14492, 295, 613, 5327, 366, 1333, 295, 588, 11, 420, 412, 1935, 512, 295, 552, 567, 445, 1401, 264], "temperature": 0.0, "avg_logprob": -0.1431756255650284, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.4968951290939003e-05}, {"id": 248, "seek": 125468, "start": 1265.4, "end": 1267.3200000000002, "text": " paper are obvious.", "tokens": [3035, 366, 6322, 13], "temperature": 0.0, "avg_logprob": -0.1431756255650284, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.4968951290939003e-05}, {"id": 249, "seek": 125468, "start": 1267.3200000000002, "end": 1275.44, "text": " I, for example, don't know sort of the decisions that went into making the corpus.", "tokens": [286, 11, 337, 1365, 11, 500, 380, 458, 1333, 295, 264, 5327, 300, 1437, 666, 1455, 264, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.1431756255650284, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.4968951290939003e-05}, {"id": 250, "seek": 125468, "start": 1275.44, "end": 1279.68, "text": " And then sort of the second, I think, more technical issue that there is more headway", "tokens": [400, 550, 1333, 295, 264, 1150, 11, 286, 519, 11, 544, 6191, 2734, 300, 456, 307, 544, 1378, 676], "temperature": 0.0, "avg_logprob": -0.1431756255650284, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.4968951290939003e-05}, {"id": 251, "seek": 125468, "start": 1279.68, "end": 1283.64, "text": " on is that embeddings themselves are black box algorithms.", "tokens": [322, 307, 300, 12240, 29432, 2969, 366, 2211, 2424, 14642, 13], "temperature": 0.0, "avg_logprob": -0.1431756255650284, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.4968951290939003e-05}, {"id": 252, "seek": 128364, "start": 1283.64, "end": 1290.0, "text": " So there's a bunch of different ways to train embeddings, some that are ever getting improved", "tokens": [407, 456, 311, 257, 3840, 295, 819, 2098, 281, 3847, 12240, 29432, 11, 512, 300, 366, 1562, 1242, 9689], "temperature": 0.0, "avg_logprob": -0.126448680614603, "compression_ratio": 1.618881118881119, "no_speech_prob": 6.04863589614979e-06}, {"id": 253, "seek": 128364, "start": 1290.0, "end": 1294.3200000000002, "text": " from GloVe to Word2Vect to BERT or what have you.", "tokens": [490, 10786, 53, 68, 281, 8725, 17, 53, 557, 281, 363, 31479, 420, 437, 362, 291, 13], "temperature": 0.0, "avg_logprob": -0.126448680614603, "compression_ratio": 1.618881118881119, "no_speech_prob": 6.04863589614979e-06}, {"id": 254, "seek": 128364, "start": 1294.3200000000002, "end": 1299.92, "text": " And then we just calculated the Euclidean distance between two vectors and we claim", "tokens": [400, 550, 321, 445, 15598, 264, 462, 1311, 31264, 282, 4560, 1296, 732, 18875, 293, 321, 3932], "temperature": 0.0, "avg_logprob": -0.126448680614603, "compression_ratio": 1.618881118881119, "no_speech_prob": 6.04863589614979e-06}, {"id": 255, "seek": 128364, "start": 1299.92, "end": 1304.68, "text": " that that represents something about these embeddings.", "tokens": [300, 300, 8855, 746, 466, 613, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.126448680614603, "compression_ratio": 1.618881118881119, "no_speech_prob": 6.04863589614979e-06}, {"id": 256, "seek": 128364, "start": 1304.68, "end": 1307.98, "text": " This one I say that there's more progress on because at least in our paper, we sort", "tokens": [639, 472, 286, 584, 300, 456, 311, 544, 4205, 322, 570, 412, 1935, 294, 527, 3035, 11, 321, 1333], "temperature": 0.0, "avg_logprob": -0.126448680614603, "compression_ratio": 1.618881118881119, "no_speech_prob": 6.04863589614979e-06}, {"id": 257, "seek": 128364, "start": 1307.98, "end": 1312.24, "text": " of in the appendix, we do a bunch of robustness checks that the distance function really doesn't", "tokens": [295, 294, 264, 34116, 970, 11, 321, 360, 257, 3840, 295, 13956, 1287, 13834, 300, 264, 4560, 2445, 534, 1177, 380], "temperature": 0.0, "avg_logprob": -0.126448680614603, "compression_ratio": 1.618881118881119, "no_speech_prob": 6.04863589614979e-06}, {"id": 258, "seek": 131224, "start": 1312.24, "end": 1317.08, "text": " matter, the algorithm used for the embeddings doesn't matter, and so on.", "tokens": [1871, 11, 264, 9284, 1143, 337, 264, 12240, 29432, 1177, 380, 1871, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.14909806150071164, "compression_ratio": 1.6415929203539823, "no_speech_prob": 7.766219823679421e-06}, {"id": 259, "seek": 131224, "start": 1317.08, "end": 1324.52, "text": " And there's follow on work on, I think, using much nicer, fine grained embeddings that instead", "tokens": [400, 456, 311, 1524, 322, 589, 322, 11, 286, 519, 11, 1228, 709, 22842, 11, 2489, 1295, 2001, 12240, 29432, 300, 2602], "temperature": 0.0, "avg_logprob": -0.14909806150071164, "compression_ratio": 1.6415929203539823, "no_speech_prob": 7.766219823679421e-06}, {"id": 260, "seek": 131224, "start": 1324.52, "end": 1330.1200000000001, "text": " of just training different embeddings per decade, you can actually sort of have embeddings", "tokens": [295, 445, 3097, 819, 12240, 29432, 680, 10378, 11, 291, 393, 767, 1333, 295, 362, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.14909806150071164, "compression_ratio": 1.6415929203539823, "no_speech_prob": 7.766219823679421e-06}, {"id": 261, "seek": 131224, "start": 1330.1200000000001, "end": 1333.28, "text": " tuned and changed over time in a very smooth way.", "tokens": [10870, 293, 3105, 670, 565, 294, 257, 588, 5508, 636, 13], "temperature": 0.0, "avg_logprob": -0.14909806150071164, "compression_ratio": 1.6415929203539823, "no_speech_prob": 7.766219823679421e-06}, {"id": 262, "seek": 131224, "start": 1333.28, "end": 1338.56, "text": " Like, not just year by year, but sort of with additional data.", "tokens": [1743, 11, 406, 445, 1064, 538, 1064, 11, 457, 1333, 295, 365, 4497, 1412, 13], "temperature": 0.0, "avg_logprob": -0.14909806150071164, "compression_ratio": 1.6415929203539823, "no_speech_prob": 7.766219823679421e-06}, {"id": 263, "seek": 133856, "start": 1338.56, "end": 1343.8799999999999, "text": " So I think this one, this second issue is more of a solvable problem technically, but", "tokens": [407, 286, 519, 341, 472, 11, 341, 1150, 2734, 307, 544, 295, 257, 1404, 17915, 1154, 12120, 11, 457], "temperature": 0.0, "avg_logprob": -0.18863874011569554, "compression_ratio": 1.822314049586777, "no_speech_prob": 2.9306958822417073e-05}, {"id": 264, "seek": 133856, "start": 1343.8799999999999, "end": 1347.52, "text": " I think the first issue is something that there's never going to be a technical, or", "tokens": [286, 519, 264, 700, 2734, 307, 746, 300, 456, 311, 1128, 516, 281, 312, 257, 6191, 11, 420], "temperature": 0.0, "avg_logprob": -0.18863874011569554, "compression_ratio": 1.822314049586777, "no_speech_prob": 2.9306958822417073e-05}, {"id": 265, "seek": 133856, "start": 1347.52, "end": 1353.1599999999999, "text": " not never, that there isn't a technical solution for at the moment.", "tokens": [406, 1128, 11, 300, 456, 1943, 380, 257, 6191, 3827, 337, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.18863874011569554, "compression_ratio": 1.822314049586777, "no_speech_prob": 2.9306958822417073e-05}, {"id": 266, "seek": 133856, "start": 1353.1599999999999, "end": 1359.56, "text": " Similarly, there's, we claim that words represent a category, and in particular, for example,", "tokens": [13157, 11, 456, 311, 11, 321, 3932, 300, 2283, 2906, 257, 7719, 11, 293, 294, 1729, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.18863874011569554, "compression_ratio": 1.822314049586777, "no_speech_prob": 2.9306958822417073e-05}, {"id": 267, "seek": 133856, "start": 1359.56, "end": 1366.0, "text": " we use lists of words to represent binary gender, and then we use common last names", "tokens": [321, 764, 14511, 295, 2283, 281, 2906, 17434, 7898, 11, 293, 550, 321, 764, 2689, 1036, 5288], "temperature": 0.0, "avg_logprob": -0.18863874011569554, "compression_ratio": 1.822314049586777, "no_speech_prob": 2.9306958822417073e-05}, {"id": 268, "seek": 133856, "start": 1366.0, "end": 1368.32, "text": " to represent ethnicities.", "tokens": [281, 2906, 14363, 1088, 13], "temperature": 0.0, "avg_logprob": -0.18863874011569554, "compression_ratio": 1.822314049586777, "no_speech_prob": 2.9306958822417073e-05}, {"id": 269, "seek": 136832, "start": 1368.32, "end": 1370.0, "text": " But similar set of questions.", "tokens": [583, 2531, 992, 295, 1651, 13], "temperature": 0.0, "avg_logprob": -0.13026397984202315, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.8340559108764865e-05}, {"id": 270, "seek": 136832, "start": 1370.0, "end": 1375.76, "text": " This basically forgets about non-binary or multi-ethnic people, or people whose last", "tokens": [639, 1936, 2870, 82, 466, 2107, 12, 48621, 420, 4825, 12, 3293, 7692, 561, 11, 420, 561, 6104, 1036], "temperature": 0.0, "avg_logprob": -0.13026397984202315, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.8340559108764865e-05}, {"id": 271, "seek": 136832, "start": 1375.76, "end": 1382.4399999999998, "text": " names may not reflect their sort of self-defined ethnicity.", "tokens": [5288, 815, 406, 5031, 641, 1333, 295, 2698, 12, 37716, 33774, 13], "temperature": 0.0, "avg_logprob": -0.13026397984202315, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.8340559108764865e-05}, {"id": 272, "seek": 136832, "start": 1382.4399999999998, "end": 1392.48, "text": " It makes use of historical census classifications of ethnicity as well as occupation, and sort", "tokens": [467, 1669, 764, 295, 8584, 23725, 1508, 7833, 295, 33774, 382, 731, 382, 24482, 11, 293, 1333], "temperature": 0.0, "avg_logprob": -0.13026397984202315, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.8340559108764865e-05}, {"id": 273, "seek": 136832, "start": 1392.48, "end": 1398.1599999999999, "text": " of one very concrete limitation of the methodology is that we couldn't distinguish between white", "tokens": [295, 472, 588, 9859, 27432, 295, 264, 24850, 307, 300, 321, 2809, 380, 20206, 1296, 2418], "temperature": 0.0, "avg_logprob": -0.13026397984202315, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.8340559108764865e-05}, {"id": 274, "seek": 139816, "start": 1398.16, "end": 1403.64, "text": " and black Americans, because it turns out, at least in the US, the overlap and the distribution", "tokens": [293, 2211, 6280, 11, 570, 309, 4523, 484, 11, 412, 1935, 294, 264, 2546, 11, 264, 19959, 293, 264, 7316], "temperature": 0.0, "avg_logprob": -0.20062830828238223, "compression_ratio": 1.5549132947976878, "no_speech_prob": 3.269299486419186e-05}, {"id": 275, "seek": 139816, "start": 1403.64, "end": 1406.4, "text": " of last names are pretty similar.", "tokens": [295, 1036, 5288, 366, 1238, 2531, 13], "temperature": 0.0, "avg_logprob": -0.20062830828238223, "compression_ratio": 1.5549132947976878, "no_speech_prob": 3.269299486419186e-05}, {"id": 276, "seek": 139816, "start": 1406.4, "end": 1412.3600000000001, "text": " There's of course a few exceptions, but as a whole, the last name distribution is similar", "tokens": [821, 311, 295, 1164, 257, 1326, 22847, 11, 457, 382, 257, 1379, 11, 264, 1036, 1315, 7316, 307, 2531], "temperature": 0.0, "avg_logprob": -0.20062830828238223, "compression_ratio": 1.5549132947976878, "no_speech_prob": 3.269299486419186e-05}, {"id": 277, "seek": 139816, "start": 1412.3600000000001, "end": 1417.28, "text": " between white and black Americans.", "tokens": [1296, 2418, 293, 2211, 6280, 13], "temperature": 0.0, "avg_logprob": -0.20062830828238223, "compression_ratio": 1.5549132947976878, "no_speech_prob": 3.269299486419186e-05}, {"id": 278, "seek": 139816, "start": 1417.28, "end": 1422.24, "text": " So yeah, okay.", "tokens": [407, 1338, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.20062830828238223, "compression_ratio": 1.5549132947976878, "no_speech_prob": 3.269299486419186e-05}, {"id": 279, "seek": 142224, "start": 1422.24, "end": 1429.1200000000001, "text": " So sort of stepping back to similar challenges that are present in NLP or data science work", "tokens": [407, 1333, 295, 16821, 646, 281, 2531, 4759, 300, 366, 1974, 294, 426, 45196, 420, 1412, 3497, 589], "temperature": 0.0, "avg_logprob": -0.10283525570018871, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.5056067468321999e-06}, {"id": 280, "seek": 142224, "start": 1429.1200000000001, "end": 1436.48, "text": " in general is that language is messy, and language as a whole can probably take care", "tokens": [294, 2674, 307, 300, 2856, 307, 16191, 11, 293, 2856, 382, 257, 1379, 393, 1391, 747, 1127], "temperature": 0.0, "avg_logprob": -0.10283525570018871, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.5056067468321999e-06}, {"id": 281, "seek": 142224, "start": 1436.48, "end": 1439.88, "text": " of a lot of issues that I talked about.", "tokens": [295, 257, 688, 295, 2663, 300, 286, 2825, 466, 13], "temperature": 0.0, "avg_logprob": -0.10283525570018871, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.5056067468321999e-06}, {"id": 282, "seek": 142224, "start": 1439.88, "end": 1448.2, "text": " But in computation, when we start using language as data, we by definition have to simplify,", "tokens": [583, 294, 24903, 11, 562, 321, 722, 1228, 2856, 382, 1412, 11, 321, 538, 7123, 362, 281, 20460, 11], "temperature": 0.0, "avg_logprob": -0.10283525570018871, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.5056067468321999e-06}, {"id": 283, "seek": 144820, "start": 1448.2, "end": 1454.4, "text": " and that involves categorizing and summarizing this messy language.", "tokens": [293, 300, 11626, 19250, 3319, 293, 14611, 3319, 341, 16191, 2856, 13], "temperature": 0.0, "avg_logprob": -0.10891622112643334, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.5688276107539423e-05}, {"id": 284, "seek": 144820, "start": 1454.4, "end": 1458.6000000000001, "text": " And this categorization and summarization process is not objective, and you can never", "tokens": [400, 341, 19250, 2144, 293, 14611, 2144, 1399, 307, 406, 10024, 11, 293, 291, 393, 1128], "temperature": 0.0, "avg_logprob": -0.10891622112643334, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.5688276107539423e-05}, {"id": 285, "seek": 144820, "start": 1458.6000000000001, "end": 1464.04, "text": " hope to make it objective, or at least in my view.", "tokens": [1454, 281, 652, 309, 10024, 11, 420, 412, 1935, 294, 452, 1910, 13], "temperature": 0.0, "avg_logprob": -0.10891622112643334, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.5688276107539423e-05}, {"id": 286, "seek": 144820, "start": 1464.04, "end": 1469.64, "text": " That there's all sorts of issues on who's creating the categories on how you're categorizing.", "tokens": [663, 456, 311, 439, 7527, 295, 2663, 322, 567, 311, 4084, 264, 10479, 322, 577, 291, 434, 19250, 3319, 13], "temperature": 0.0, "avg_logprob": -0.10891622112643334, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.5688276107539423e-05}, {"id": 287, "seek": 144820, "start": 1469.64, "end": 1478.16, "text": " So for convenience, we use binary gender because it was hard in the paper to, for example,", "tokens": [407, 337, 19283, 11, 321, 764, 17434, 7898, 570, 309, 390, 1152, 294, 264, 3035, 281, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.10891622112643334, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.5688276107539423e-05}, {"id": 288, "seek": 147816, "start": 1478.16, "end": 1484.2, "text": " construct groups of words that historically would be able to represent any other.", "tokens": [7690, 3935, 295, 2283, 300, 16180, 576, 312, 1075, 281, 2906, 604, 661, 13], "temperature": 0.0, "avg_logprob": -0.1533946707697198, "compression_ratio": 1.7689243027888446, "no_speech_prob": 1.4969908079365268e-05}, {"id": 289, "seek": 147816, "start": 1484.2, "end": 1488.6000000000001, "text": " And this of course ignores those who fall between categories, or maybe the categories", "tokens": [400, 341, 295, 1164, 5335, 2706, 729, 567, 2100, 1296, 10479, 11, 420, 1310, 264, 10479], "temperature": 0.0, "avg_logprob": -0.1533946707697198, "compression_ratio": 1.7689243027888446, "no_speech_prob": 1.4969908079365268e-05}, {"id": 290, "seek": 147816, "start": 1488.6000000000001, "end": 1493.2, "text": " aren't discrete enough or granular enough or aren't well defined.", "tokens": [3212, 380, 27706, 1547, 420, 39962, 1547, 420, 3212, 380, 731, 7642, 13], "temperature": 0.0, "avg_logprob": -0.1533946707697198, "compression_ratio": 1.7689243027888446, "no_speech_prob": 1.4969908079365268e-05}, {"id": 291, "seek": 147816, "start": 1493.2, "end": 1495.3600000000001, "text": " And so that's about categorization.", "tokens": [400, 370, 300, 311, 466, 19250, 2144, 13], "temperature": 0.0, "avg_logprob": -0.1533946707697198, "compression_ratio": 1.7689243027888446, "no_speech_prob": 1.4969908079365268e-05}, {"id": 292, "seek": 147816, "start": 1495.3600000000001, "end": 1501.24, "text": " And then on summarization is, again, we in the paper prefer plots or to describe things", "tokens": [400, 550, 322, 14611, 2144, 307, 11, 797, 11, 321, 294, 264, 3035, 4382, 28609, 420, 281, 6786, 721], "temperature": 0.0, "avg_logprob": -0.1533946707697198, "compression_ratio": 1.7689243027888446, "no_speech_prob": 1.4969908079365268e-05}, {"id": 293, "seek": 147816, "start": 1501.24, "end": 1506.4, "text": " you need a single number, maybe a single number representing an entire decade of data.", "tokens": [291, 643, 257, 2167, 1230, 11, 1310, 257, 2167, 1230, 13460, 364, 2302, 10378, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1533946707697198, "compression_ratio": 1.7689243027888446, "no_speech_prob": 1.4969908079365268e-05}, {"id": 294, "seek": 150640, "start": 1506.4, "end": 1509.0800000000002, "text": " And this is a summary of some kind.", "tokens": [400, 341, 307, 257, 12691, 295, 512, 733, 13], "temperature": 0.0, "avg_logprob": -0.16921256780624389, "compression_ratio": 1.5679611650485437, "no_speech_prob": 6.143804512248607e-06}, {"id": 295, "seek": 150640, "start": 1509.0800000000002, "end": 1516.3200000000002, "text": " And in summaries and means, the minority opinions tend to be washed away or not accounted for", "tokens": [400, 294, 8367, 4889, 293, 1355, 11, 264, 16166, 11819, 3928, 281, 312, 16300, 1314, 420, 406, 43138, 337], "temperature": 0.0, "avg_logprob": -0.16921256780624389, "compression_ratio": 1.5679611650485437, "no_speech_prob": 6.143804512248607e-06}, {"id": 296, "seek": 150640, "start": 1516.3200000000002, "end": 1519.5600000000002, "text": " in the mean properly.", "tokens": [294, 264, 914, 6108, 13], "temperature": 0.0, "avg_logprob": -0.16921256780624389, "compression_ratio": 1.5679611650485437, "no_speech_prob": 6.143804512248607e-06}, {"id": 297, "seek": 150640, "start": 1519.5600000000002, "end": 1523.92, "text": " And then a more general question is, a lot of these word embedding models are trained", "tokens": [400, 550, 257, 544, 2674, 1168, 307, 11, 257, 688, 295, 613, 1349, 12240, 3584, 5245, 366, 8895], "temperature": 0.0, "avg_logprob": -0.16921256780624389, "compression_ratio": 1.5679611650485437, "no_speech_prob": 6.143804512248607e-06}, {"id": 298, "seek": 150640, "start": 1523.92, "end": 1530.3200000000002, "text": " on of course large corporate data like all the Wikipedia, and then they're summarized", "tokens": [322, 295, 1164, 2416, 10896, 1412, 411, 439, 264, 28999, 11, 293, 550, 436, 434, 14611, 1602], "temperature": 0.0, "avg_logprob": -0.16921256780624389, "compression_ratio": 1.5679611650485437, "no_speech_prob": 6.143804512248607e-06}, {"id": 299, "seek": 153032, "start": 1530.32, "end": 1536.6, "text": " and used to train models that are going to apply to everyone, including potentially minority", "tokens": [293, 1143, 281, 3847, 5245, 300, 366, 516, 281, 3079, 281, 1518, 11, 3009, 7263, 16166], "temperature": 0.0, "avg_logprob": -0.12349778552388035, "compression_ratio": 1.628691983122363, "no_speech_prob": 2.642447043399443e-06}, {"id": 300, "seek": 153032, "start": 1536.6, "end": 1540.24, "text": " opinions that were washed away in the creation of the embeddings.", "tokens": [11819, 300, 645, 16300, 1314, 294, 264, 8016, 295, 264, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.12349778552388035, "compression_ratio": 1.628691983122363, "no_speech_prob": 2.642447043399443e-06}, {"id": 301, "seek": 153032, "start": 1540.24, "end": 1545.3999999999999, "text": " And that, I think a whole host of literature in the past few years has described how this", "tokens": [400, 300, 11, 286, 519, 257, 1379, 3975, 295, 10394, 294, 264, 1791, 1326, 924, 575, 7619, 577, 341], "temperature": 0.0, "avg_logprob": -0.12349778552388035, "compression_ratio": 1.628691983122363, "no_speech_prob": 2.642447043399443e-06}, {"id": 302, "seek": 153032, "start": 1545.3999999999999, "end": 1552.72, "text": " is problematic in a bunch of applications.", "tokens": [307, 19011, 294, 257, 3840, 295, 5821, 13], "temperature": 0.0, "avg_logprob": -0.12349778552388035, "compression_ratio": 1.628691983122363, "no_speech_prob": 2.642447043399443e-06}, {"id": 303, "seek": 153032, "start": 1552.72, "end": 1559.48, "text": " And so stepping back even further, embeddings and unsupervised learning models or just machine", "tokens": [400, 370, 16821, 646, 754, 3052, 11, 12240, 29432, 293, 2693, 12879, 24420, 2539, 5245, 420, 445, 3479], "temperature": 0.0, "avg_logprob": -0.12349778552388035, "compression_ratio": 1.628691983122363, "no_speech_prob": 2.642447043399443e-06}, {"id": 304, "seek": 155948, "start": 1559.48, "end": 1567.16, "text": " learning models in particular often construct incomplete models of the sometimes problematic", "tokens": [2539, 5245, 294, 1729, 2049, 7690, 31709, 5245, 295, 264, 2171, 19011], "temperature": 0.0, "avg_logprob": -0.14496325064396512, "compression_ratio": 1.56, "no_speech_prob": 7.295436716958648e-06}, {"id": 305, "seek": 155948, "start": 1567.16, "end": 1568.56, "text": " world.", "tokens": [1002, 13], "temperature": 0.0, "avg_logprob": -0.14496325064396512, "compression_ratio": 1.56, "no_speech_prob": 7.295436716958648e-06}, {"id": 306, "seek": 155948, "start": 1568.56, "end": 1575.72, "text": " And this is not always a bad thing in the sense that our work couldn't have existed", "tokens": [400, 341, 307, 406, 1009, 257, 1578, 551, 294, 264, 2020, 300, 527, 589, 2809, 380, 362, 13135], "temperature": 0.0, "avg_logprob": -0.14496325064396512, "compression_ratio": 1.56, "no_speech_prob": 7.295436716958648e-06}, {"id": 307, "seek": 155948, "start": 1575.72, "end": 1580.2, "text": " if embeddings didn't capture this sort of structure.", "tokens": [498, 12240, 29432, 994, 380, 7983, 341, 1333, 295, 3877, 13], "temperature": 0.0, "avg_logprob": -0.14496325064396512, "compression_ratio": 1.56, "no_speech_prob": 7.295436716958648e-06}, {"id": 308, "seek": 155948, "start": 1580.2, "end": 1586.32, "text": " But in other applications, these stereotypes embeddings may be undesirable.", "tokens": [583, 294, 661, 5821, 11, 613, 30853, 12240, 29432, 815, 312, 45667, 21493, 13], "temperature": 0.0, "avg_logprob": -0.14496325064396512, "compression_ratio": 1.56, "no_speech_prob": 7.295436716958648e-06}, {"id": 309, "seek": 158632, "start": 1586.32, "end": 1591.72, "text": " And it's a question for when is it undesirable, who decides if it is, and then how do you", "tokens": [400, 309, 311, 257, 1168, 337, 562, 307, 309, 45667, 21493, 11, 567, 14898, 498, 309, 307, 11, 293, 550, 577, 360, 291], "temperature": 0.0, "avg_logprob": -0.11302478291163935, "compression_ratio": 1.8523206751054853, "no_speech_prob": 1.805549800337758e-05}, {"id": 310, "seek": 158632, "start": 1591.72, "end": 1596.3999999999999, "text": " devias and should you devias.", "tokens": [1905, 4609, 293, 820, 291, 1905, 4609, 13], "temperature": 0.0, "avg_logprob": -0.11302478291163935, "compression_ratio": 1.8523206751054853, "no_speech_prob": 1.805549800337758e-05}, {"id": 311, "seek": 158632, "start": 1596.3999999999999, "end": 1601.28, "text": " And these are all things that are something that you should think carefully about.", "tokens": [400, 613, 366, 439, 721, 300, 366, 746, 300, 291, 820, 519, 7500, 466, 13], "temperature": 0.0, "avg_logprob": -0.11302478291163935, "compression_ratio": 1.8523206751054853, "no_speech_prob": 1.805549800337758e-05}, {"id": 312, "seek": 158632, "start": 1601.28, "end": 1604.24, "text": " And something that I want to emphasize is you thinking carefully about it is probably", "tokens": [400, 746, 300, 286, 528, 281, 16078, 307, 291, 1953, 7500, 466, 309, 307, 1391], "temperature": 0.0, "avg_logprob": -0.11302478291163935, "compression_ratio": 1.8523206751054853, "no_speech_prob": 1.805549800337758e-05}, {"id": 313, "seek": 158632, "start": 1604.24, "end": 1609.1599999999999, "text": " not enough in that I'm not an expert on these issues, you probably aren't either.", "tokens": [406, 1547, 294, 300, 286, 478, 406, 364, 5844, 322, 613, 2663, 11, 291, 1391, 3212, 380, 2139, 13], "temperature": 0.0, "avg_logprob": -0.11302478291163935, "compression_ratio": 1.8523206751054853, "no_speech_prob": 1.805549800337758e-05}, {"id": 314, "seek": 158632, "start": 1609.1599999999999, "end": 1612.9199999999998, "text": " And you should probably collaborate with and talk to people who are.", "tokens": [400, 291, 820, 1391, 18338, 365, 293, 751, 281, 561, 567, 366, 13], "temperature": 0.0, "avg_logprob": -0.11302478291163935, "compression_ratio": 1.8523206751054853, "no_speech_prob": 1.805549800337758e-05}, {"id": 315, "seek": 161292, "start": 1612.92, "end": 1618.8400000000001, "text": " For example, this work benefited pretty greatly from talking to and collaborating with Londa,", "tokens": [1171, 1365, 11, 341, 589, 33605, 1238, 14147, 490, 1417, 281, 293, 30188, 365, 6735, 64, 11], "temperature": 0.0, "avg_logprob": -0.2573520805262312, "compression_ratio": 1.5560747663551402, "no_speech_prob": 7.295329851331189e-06}, {"id": 316, "seek": 161292, "start": 1618.8400000000001, "end": 1623.64, "text": " who is a historian on these issues.", "tokens": [567, 307, 257, 25139, 322, 613, 2663, 13], "temperature": 0.0, "avg_logprob": -0.2573520805262312, "compression_ratio": 1.5560747663551402, "no_speech_prob": 7.295329851331189e-06}, {"id": 317, "seek": 161292, "start": 1623.64, "end": 1630.64, "text": " Yes, that's sort of all the prepared talks that I had.", "tokens": [1079, 11, 300, 311, 1333, 295, 439, 264, 4927, 6686, 300, 286, 632, 13], "temperature": 0.0, "avg_logprob": -0.2573520805262312, "compression_ratio": 1.5560747663551402, "no_speech_prob": 7.295329851331189e-06}, {"id": 318, "seek": 161292, "start": 1630.64, "end": 1635.5600000000002, "text": " Definitely open to questions or discussion on these issues and links to the paper.", "tokens": [12151, 1269, 281, 1651, 420, 5017, 322, 613, 2663, 293, 6123, 281, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.2573520805262312, "compression_ratio": 1.5560747663551402, "no_speech_prob": 7.295329851331189e-06}, {"id": 319, "seek": 161292, "start": 1635.5600000000002, "end": 1642.04, "text": " Probably easier to Google around and then try to follow the link.", "tokens": [9210, 3571, 281, 3329, 926, 293, 550, 853, 281, 1524, 264, 2113, 13], "temperature": 0.0, "avg_logprob": -0.2573520805262312, "compression_ratio": 1.5560747663551402, "no_speech_prob": 7.295329851331189e-06}, {"id": 320, "seek": 164204, "start": 1642.04, "end": 1647.76, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.3584413528442383, "compression_ratio": 1.289855072463768, "no_speech_prob": 5.138957203598693e-05}, {"id": 321, "seek": 164204, "start": 1647.76, "end": 1655.32, "text": " Hi, thank you for the talk.", "tokens": [2421, 11, 1309, 291, 337, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.3584413528442383, "compression_ratio": 1.289855072463768, "no_speech_prob": 5.138957203598693e-05}, {"id": 322, "seek": 164204, "start": 1655.32, "end": 1657.32, "text": " Am I using this correctly?", "tokens": [2012, 286, 1228, 341, 8944, 30], "temperature": 0.0, "avg_logprob": -0.3584413528442383, "compression_ratio": 1.289855072463768, "no_speech_prob": 5.138957203598693e-05}, {"id": 323, "seek": 164204, "start": 1657.32, "end": 1658.32, "text": " Okay, thanks.", "tokens": [1033, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.3584413528442383, "compression_ratio": 1.289855072463768, "no_speech_prob": 5.138957203598693e-05}, {"id": 324, "seek": 164204, "start": 1658.32, "end": 1666.32, "text": " I'm just wondering, did you mention how were the, so the example you gave for the Muslim", "tokens": [286, 478, 445, 6359, 11, 630, 291, 2152, 577, 645, 264, 11, 370, 264, 1365, 291, 2729, 337, 264, 8178], "temperature": 0.0, "avg_logprob": -0.3584413528442383, "compression_ratio": 1.289855072463768, "no_speech_prob": 5.138957203598693e-05}, {"id": 325, "seek": 166632, "start": 1666.32, "end": 1673.8, "text": " and terrorism, you said you select a list of words that are associated with those two", "tokens": [293, 23917, 11, 291, 848, 291, 3048, 257, 1329, 295, 2283, 300, 366, 6615, 365, 729, 732], "temperature": 0.0, "avg_logprob": -0.1825378978953642, "compression_ratio": 1.7, "no_speech_prob": 5.141136352904141e-05}, {"id": 326, "seek": 166632, "start": 1673.8, "end": 1681.8, "text": " and then you compare how they're the correlation of them along with time.", "tokens": [293, 550, 291, 6794, 577, 436, 434, 264, 20009, 295, 552, 2051, 365, 565, 13], "temperature": 0.0, "avg_logprob": -0.1825378978953642, "compression_ratio": 1.7, "no_speech_prob": 5.141136352904141e-05}, {"id": 327, "seek": 166632, "start": 1681.8, "end": 1686.72, "text": " So my first question would be, how did you pick those words that are associated with", "tokens": [407, 452, 700, 1168, 576, 312, 11, 577, 630, 291, 1888, 729, 2283, 300, 366, 6615, 365], "temperature": 0.0, "avg_logprob": -0.1825378978953642, "compression_ratio": 1.7, "no_speech_prob": 5.141136352904141e-05}, {"id": 328, "seek": 166632, "start": 1686.72, "end": 1687.96, "text": " the term?", "tokens": [264, 1433, 30], "temperature": 0.0, "avg_logprob": -0.1825378978953642, "compression_ratio": 1.7, "no_speech_prob": 5.141136352904141e-05}, {"id": 329, "seek": 166632, "start": 1687.96, "end": 1693.84, "text": " And secondly, are they updated in different time periods?", "tokens": [400, 26246, 11, 366, 436, 10588, 294, 819, 565, 13804, 30], "temperature": 0.0, "avg_logprob": -0.1825378978953642, "compression_ratio": 1.7, "no_speech_prob": 5.141136352904141e-05}, {"id": 330, "seek": 166632, "start": 1693.84, "end": 1694.84, "text": " Great questions.", "tokens": [3769, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1825378978953642, "compression_ratio": 1.7, "no_speech_prob": 5.141136352904141e-05}, {"id": 331, "seek": 166632, "start": 1694.84, "end": 1695.84, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.1825378978953642, "compression_ratio": 1.7, "no_speech_prob": 5.141136352904141e-05}, {"id": 332, "seek": 169584, "start": 1695.84, "end": 1702.9199999999998, "text": " Yeah, so you described the methodology correctly, I think is pick groups of words that represent", "tokens": [865, 11, 370, 291, 7619, 264, 24850, 8944, 11, 286, 519, 307, 1888, 3935, 295, 2283, 300, 2906], "temperature": 0.0, "avg_logprob": -0.15127776219294622, "compression_ratio": 1.7510917030567685, "no_speech_prob": 8.527502359356731e-06}, {"id": 333, "seek": 169584, "start": 1702.9199999999998, "end": 1708.3999999999999, "text": " Islam and terrorism and Christianity and then look at the relative distances between the", "tokens": [8571, 293, 23917, 293, 17326, 293, 550, 574, 412, 264, 4972, 22182, 1296, 264], "temperature": 0.0, "avg_logprob": -0.15127776219294622, "compression_ratio": 1.7510917030567685, "no_speech_prob": 8.527502359356731e-06}, {"id": 334, "seek": 169584, "start": 1708.3999999999999, "end": 1712.1999999999998, "text": " group words and the words describing terrorism over time.", "tokens": [1594, 2283, 293, 264, 2283, 16141, 23917, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.15127776219294622, "compression_ratio": 1.7510917030567685, "no_speech_prob": 8.527502359356731e-06}, {"id": 335, "seek": 169584, "start": 1712.1999999999998, "end": 1716.56, "text": " So I think the two questions were, and correct me if I'm wrong, is one is who picked the", "tokens": [407, 286, 519, 264, 732, 1651, 645, 11, 293, 3006, 385, 498, 286, 478, 2085, 11, 307, 472, 307, 567, 6183, 264], "temperature": 0.0, "avg_logprob": -0.15127776219294622, "compression_ratio": 1.7510917030567685, "no_speech_prob": 8.527502359356731e-06}, {"id": 336, "seek": 169584, "start": 1716.56, "end": 1724.32, "text": " words and how are the words picked and were they the same over time?", "tokens": [2283, 293, 577, 366, 264, 2283, 6183, 293, 645, 436, 264, 912, 670, 565, 30], "temperature": 0.0, "avg_logprob": -0.15127776219294622, "compression_ratio": 1.7510917030567685, "no_speech_prob": 8.527502359356731e-06}, {"id": 337, "seek": 172432, "start": 1724.32, "end": 1729.48, "text": " And I think this is exactly the type of issues that I was talking about earlier is that these", "tokens": [400, 286, 519, 341, 307, 2293, 264, 2010, 295, 2663, 300, 286, 390, 1417, 466, 3071, 307, 300, 613], "temperature": 0.0, "avg_logprob": -0.13118937958118526, "compression_ratio": 1.5975103734439835, "no_speech_prob": 3.668305225801305e-06}, {"id": 338, "seek": 172432, "start": 1729.48, "end": 1735.6399999999999, "text": " are important questions that are often glossed over on like how sensitive or sort of who", "tokens": [366, 1021, 1651, 300, 366, 2049, 19574, 292, 670, 322, 411, 577, 9477, 420, 1333, 295, 567], "temperature": 0.0, "avg_logprob": -0.13118937958118526, "compression_ratio": 1.5975103734439835, "no_speech_prob": 3.668305225801305e-06}, {"id": 339, "seek": 172432, "start": 1735.6399999999999, "end": 1737.6799999999998, "text": " defines these categories.", "tokens": [23122, 613, 10479, 13], "temperature": 0.0, "avg_logprob": -0.13118937958118526, "compression_ratio": 1.5975103734439835, "no_speech_prob": 3.668305225801305e-06}, {"id": 340, "seek": 172432, "start": 1737.6799999999998, "end": 1744.12, "text": " And for the most part throughout the paper, we tried to just find as most general and", "tokens": [400, 337, 264, 881, 644, 3710, 264, 3035, 11, 321, 3031, 281, 445, 915, 382, 881, 2674, 293], "temperature": 0.0, "avg_logprob": -0.13118937958118526, "compression_ratio": 1.5975103734439835, "no_speech_prob": 3.668305225801305e-06}, {"id": 341, "seek": 172432, "start": 1744.12, "end": 1752.4399999999998, "text": " large lists that other people had compiled online as possible for, I believe the religious", "tokens": [2416, 14511, 300, 661, 561, 632, 36548, 2950, 382, 1944, 337, 11, 286, 1697, 264, 7185], "temperature": 0.0, "avg_logprob": -0.13118937958118526, "compression_ratio": 1.5975103734439835, "no_speech_prob": 3.668305225801305e-06}, {"id": 342, "seek": 175244, "start": 1752.44, "end": 1760.8400000000001, "text": " and terrorism words, we started with lists found online and then sort of like removed", "tokens": [293, 23917, 2283, 11, 321, 1409, 365, 14511, 1352, 2950, 293, 550, 1333, 295, 411, 7261], "temperature": 0.0, "avg_logprob": -0.15017059871128627, "compression_ratio": 1.6952380952380952, "no_speech_prob": 1.593329943716526e-05}, {"id": 343, "seek": 175244, "start": 1760.8400000000001, "end": 1764.44, "text": " overlaps in some sort that would sort of skew the results.", "tokens": [15986, 2382, 294, 512, 1333, 300, 576, 1333, 295, 8756, 86, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.15017059871128627, "compression_ratio": 1.6952380952380952, "no_speech_prob": 1.593329943716526e-05}, {"id": 344, "seek": 175244, "start": 1764.44, "end": 1771.8400000000001, "text": " So I think like various lists online sort of, for example, use madrasa as a word describing", "tokens": [407, 286, 519, 411, 3683, 14511, 2950, 1333, 295, 11, 337, 1365, 11, 764, 5244, 3906, 64, 382, 257, 1349, 16141], "temperature": 0.0, "avg_logprob": -0.15017059871128627, "compression_ratio": 1.6952380952380952, "no_speech_prob": 1.593329943716526e-05}, {"id": 345, "seek": 175244, "start": 1771.8400000000001, "end": 1775.98, "text": " terrorism, but that also has a more benign meaning, I think.", "tokens": [23917, 11, 457, 300, 611, 575, 257, 544, 3271, 788, 3620, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.15017059871128627, "compression_ratio": 1.6952380952380952, "no_speech_prob": 1.593329943716526e-05}, {"id": 346, "seek": 175244, "start": 1775.98, "end": 1781.0, "text": " And so like little issues like that we had to hand remove.", "tokens": [400, 370, 411, 707, 2663, 411, 300, 321, 632, 281, 1011, 4159, 13], "temperature": 0.0, "avg_logprob": -0.15017059871128627, "compression_ratio": 1.6952380952380952, "no_speech_prob": 1.593329943716526e-05}, {"id": 347, "seek": 178100, "start": 1781.0, "end": 1784.54, "text": " And yeah, so it's definitely not a perfect process.", "tokens": [400, 1338, 11, 370, 309, 311, 2138, 406, 257, 2176, 1399, 13], "temperature": 0.0, "avg_logprob": -0.15850904923451098, "compression_ratio": 1.5829145728643217, "no_speech_prob": 4.7570065362378955e-05}, {"id": 348, "seek": 178100, "start": 1784.54, "end": 1794.04, "text": " And then the second question is sort of are the words consistent or the same over time?", "tokens": [400, 550, 264, 1150, 1168, 307, 1333, 295, 366, 264, 2283, 8398, 420, 264, 912, 670, 565, 30], "temperature": 0.0, "avg_logprob": -0.15850904923451098, "compression_ratio": 1.5829145728643217, "no_speech_prob": 4.7570065362378955e-05}, {"id": 349, "seek": 178100, "start": 1794.04, "end": 1801.0, "text": " And yes, essentially, there's sort of the words as a whole, I think, sort of how we", "tokens": [400, 2086, 11, 4476, 11, 456, 311, 1333, 295, 264, 2283, 382, 257, 1379, 11, 286, 519, 11, 1333, 295, 577, 321], "temperature": 0.0, "avg_logprob": -0.15850904923451098, "compression_ratio": 1.5829145728643217, "no_speech_prob": 4.7570065362378955e-05}, {"id": 350, "seek": 178100, "start": 1801.0, "end": 1807.42, "text": " did it was to be started with these original large lists and then looked from that filtered", "tokens": [630, 309, 390, 281, 312, 1409, 365, 613, 3380, 2416, 14511, 293, 550, 2956, 490, 300, 37111], "temperature": 0.0, "avg_logprob": -0.15850904923451098, "compression_ratio": 1.5829145728643217, "no_speech_prob": 4.7570065362378955e-05}, {"id": 351, "seek": 180742, "start": 1807.42, "end": 1812.1200000000001, "text": " down to just the words that appear enough over time so that you're not going to have", "tokens": [760, 281, 445, 264, 2283, 300, 4204, 1547, 670, 565, 370, 300, 291, 434, 406, 516, 281, 362], "temperature": 0.0, "avg_logprob": -0.17960431025578424, "compression_ratio": 1.7510204081632652, "no_speech_prob": 3.218739220756106e-05}, {"id": 352, "seek": 180742, "start": 1812.1200000000001, "end": 1817.88, "text": " really high variance estimates in any particular decade and then use that smaller list that", "tokens": [534, 1090, 21977, 20561, 294, 604, 1729, 10378, 293, 550, 764, 300, 4356, 1329, 300], "temperature": 0.0, "avg_logprob": -0.17960431025578424, "compression_ratio": 1.7510204081632652, "no_speech_prob": 3.218739220756106e-05}, {"id": 353, "seek": 180742, "start": 1817.88, "end": 1819.88, "text": " appears over time.", "tokens": [7038, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.17960431025578424, "compression_ratio": 1.7510204081632652, "no_speech_prob": 3.218739220756106e-05}, {"id": 354, "seek": 180742, "start": 1819.88, "end": 1824.64, "text": " I have one follow up question to that and then a different one.", "tokens": [286, 362, 472, 1524, 493, 1168, 281, 300, 293, 550, 257, 819, 472, 13], "temperature": 0.0, "avg_logprob": -0.17960431025578424, "compression_ratio": 1.7510204081632652, "no_speech_prob": 3.218739220756106e-05}, {"id": 355, "seek": 180742, "start": 1824.64, "end": 1830.04, "text": " So on that process, and I was curious about the same thing, there's, you know, the problem", "tokens": [407, 322, 300, 1399, 11, 293, 286, 390, 6369, 466, 264, 912, 551, 11, 456, 311, 11, 291, 458, 11, 264, 1154], "temperature": 0.0, "avg_logprob": -0.17960431025578424, "compression_ratio": 1.7510204081632652, "no_speech_prob": 3.218739220756106e-05}, {"id": 356, "seek": 180742, "start": 1830.04, "end": 1833.64, "text": " with keeping a consistent list is that it's the good thing is it's consistent.", "tokens": [365, 5145, 257, 8398, 1329, 307, 300, 309, 311, 264, 665, 551, 307, 309, 311, 8398, 13], "temperature": 0.0, "avg_logprob": -0.17960431025578424, "compression_ratio": 1.7510204081632652, "no_speech_prob": 3.218739220756106e-05}, {"id": 357, "seek": 183364, "start": 1833.64, "end": 1838.24, "text": " And then the bad thing is that it's consistent in that word use and language selection varies", "tokens": [400, 550, 264, 1578, 551, 307, 300, 309, 311, 8398, 294, 300, 1349, 764, 293, 2856, 9450, 21716], "temperature": 0.0, "avg_logprob": -0.09732264280319214, "compression_ratio": 1.6, "no_speech_prob": 7.645905498065986e-06}, {"id": 358, "seek": 183364, "start": 1838.24, "end": 1840.0800000000002, "text": " massively over time.", "tokens": [29379, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.09732264280319214, "compression_ratio": 1.6, "no_speech_prob": 7.645905498065986e-06}, {"id": 359, "seek": 183364, "start": 1840.0800000000002, "end": 1847.0400000000002, "text": " So keeping that list can be just a measurement of when a word gets popular on that list and", "tokens": [407, 5145, 300, 1329, 393, 312, 445, 257, 13160, 295, 562, 257, 1349, 2170, 3743, 322, 300, 1329, 293], "temperature": 0.0, "avg_logprob": -0.09732264280319214, "compression_ratio": 1.6, "no_speech_prob": 7.645905498065986e-06}, {"id": 360, "seek": 183364, "start": 1847.0400000000002, "end": 1848.0400000000002, "text": " when it isn't.", "tokens": [562, 309, 1943, 380, 13], "temperature": 0.0, "avg_logprob": -0.09732264280319214, "compression_ratio": 1.6, "no_speech_prob": 7.645905498065986e-06}, {"id": 361, "seek": 183364, "start": 1848.0400000000002, "end": 1850.6000000000001, "text": " And so have you guys thought about controlling for that?", "tokens": [400, 370, 362, 291, 1074, 1194, 466, 14905, 337, 300, 30], "temperature": 0.0, "avg_logprob": -0.09732264280319214, "compression_ratio": 1.6, "no_speech_prob": 7.645905498065986e-06}, {"id": 362, "seek": 183364, "start": 1850.6000000000001, "end": 1852.88, "text": " That just came up.", "tokens": [663, 445, 1361, 493, 13], "temperature": 0.0, "avg_logprob": -0.09732264280319214, "compression_ratio": 1.6, "no_speech_prob": 7.645905498065986e-06}, {"id": 363, "seek": 183364, "start": 1852.88, "end": 1857.3200000000002, "text": " But more importantly, I think now that it seems very clear that the sort of embeddings", "tokens": [583, 544, 8906, 11, 286, 519, 586, 300, 309, 2544, 588, 1850, 300, 264, 1333, 295, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.09732264280319214, "compression_ratio": 1.6, "no_speech_prob": 7.645905498065986e-06}, {"id": 364, "seek": 185732, "start": 1857.32, "end": 1864.8799999999999, "text": " and algorithms are getting there in terms of really extracting really good context,", "tokens": [293, 14642, 366, 1242, 456, 294, 2115, 295, 534, 49844, 534, 665, 4319, 11], "temperature": 0.0, "avg_logprob": -0.16227966977148942, "compression_ratio": 1.6313725490196078, "no_speech_prob": 4.0293562051374465e-06}, {"id": 365, "seek": 185732, "start": 1864.8799999999999, "end": 1869.56, "text": " is anyone thinking about conducting, as it already been done, this main issue, which", "tokens": [307, 2878, 1953, 466, 21749, 11, 382, 309, 1217, 668, 1096, 11, 341, 2135, 2734, 11, 597], "temperature": 0.0, "avg_logprob": -0.16227966977148942, "compression_ratio": 1.6313725490196078, "no_speech_prob": 4.0293562051374465e-06}, {"id": 366, "seek": 185732, "start": 1869.56, "end": 1876.0, "text": " is like if you use 100 years of white authors, it's going to reflect that.", "tokens": [307, 411, 498, 291, 764, 2319, 924, 295, 2418, 16552, 11, 309, 311, 516, 281, 5031, 300, 13], "temperature": 0.0, "avg_logprob": -0.16227966977148942, "compression_ratio": 1.6313725490196078, "no_speech_prob": 4.0293562051374465e-06}, {"id": 367, "seek": 185732, "start": 1876.0, "end": 1880.8799999999999, "text": " But actually go in there and actually try to fix that problem in a systematic way, you", "tokens": [583, 767, 352, 294, 456, 293, 767, 853, 281, 3191, 300, 1154, 294, 257, 27249, 636, 11, 291], "temperature": 0.0, "avg_logprob": -0.16227966977148942, "compression_ratio": 1.6313725490196078, "no_speech_prob": 4.0293562051374465e-06}, {"id": 368, "seek": 185732, "start": 1880.8799999999999, "end": 1886.4199999999998, "text": " know, with like replacement of these books or consensus corpus, but then not just get", "tokens": [458, 11, 365, 411, 14419, 295, 613, 3642, 420, 19115, 1181, 31624, 11, 457, 550, 406, 445, 483], "temperature": 0.0, "avg_logprob": -0.16227966977148942, "compression_ratio": 1.6313725490196078, "no_speech_prob": 4.0293562051374465e-06}, {"id": 369, "seek": 188642, "start": 1886.42, "end": 1891.88, "text": " a consensus from these external partners, but then test whether the consensus does improve", "tokens": [257, 19115, 490, 613, 8320, 4462, 11, 457, 550, 1500, 1968, 264, 19115, 775, 3470], "temperature": 0.0, "avg_logprob": -0.23029181996329887, "compression_ratio": 1.6555183946488294, "no_speech_prob": 2.3920209059724584e-05}, {"id": 370, "seek": 188642, "start": 1891.88, "end": 1898.52, "text": " things, change things, or if there is something more general despite the lack of representation", "tokens": [721, 11, 1319, 721, 11, 420, 498, 456, 307, 746, 544, 2674, 7228, 264, 5011, 295, 10290], "temperature": 0.0, "avg_logprob": -0.23029181996329887, "compression_ratio": 1.6555183946488294, "no_speech_prob": 2.3920209059724584e-05}, {"id": 371, "seek": 188642, "start": 1898.52, "end": 1899.52, "text": " in our language.", "tokens": [294, 527, 2856, 13], "temperature": 0.0, "avg_logprob": -0.23029181996329887, "compression_ratio": 1.6555183946488294, "no_speech_prob": 2.3920209059724584e-05}, {"id": 372, "seek": 188642, "start": 1899.52, "end": 1900.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.23029181996329887, "compression_ratio": 1.6555183946488294, "no_speech_prob": 2.3920209059724584e-05}, {"id": 373, "seek": 188642, "start": 1900.52, "end": 1901.52, "text": " Great question.", "tokens": [3769, 1168, 13], "temperature": 0.0, "avg_logprob": -0.23029181996329887, "compression_ratio": 1.6555183946488294, "no_speech_prob": 2.3920209059724584e-05}, {"id": 374, "seek": 188642, "start": 1901.52, "end": 1906.0, "text": " So I'll tackle the second one because that's the one I remember currently.", "tokens": [407, 286, 603, 14896, 264, 1150, 472, 570, 300, 311, 264, 472, 286, 1604, 4362, 13], "temperature": 0.0, "avg_logprob": -0.23029181996329887, "compression_ratio": 1.6555183946488294, "no_speech_prob": 2.3920209059724584e-05}, {"id": 375, "seek": 188642, "start": 1906.0, "end": 1908.5600000000002, "text": " So I don't want to pick on the corpus of historical American.", "tokens": [407, 286, 500, 380, 528, 281, 1888, 322, 264, 1181, 31624, 295, 8584, 2665, 13], "temperature": 0.0, "avg_logprob": -0.23029181996329887, "compression_ratio": 1.6555183946488294, "no_speech_prob": 2.3920209059724584e-05}, {"id": 376, "seek": 188642, "start": 1908.5600000000002, "end": 1914.04, "text": " It's a great data size compiled by, I think, social science experts and for, I think,", "tokens": [467, 311, 257, 869, 1412, 2744, 36548, 538, 11, 286, 519, 11, 2093, 3497, 8572, 293, 337, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.23029181996329887, "compression_ratio": 1.6555183946488294, "no_speech_prob": 2.3920209059724584e-05}, {"id": 377, "seek": 188642, "start": 1914.04, "end": 1916.18, "text": " going through that process in the first place.", "tokens": [516, 807, 300, 1399, 294, 264, 700, 1081, 13], "temperature": 0.0, "avg_logprob": -0.23029181996329887, "compression_ratio": 1.6555183946488294, "no_speech_prob": 2.3920209059724584e-05}, {"id": 378, "seek": 191618, "start": 1916.18, "end": 1921.5800000000002, "text": " So they genre balance and they did a lot of things to make sure that the books were comparable", "tokens": [407, 436, 11022, 4772, 293, 436, 630, 257, 688, 295, 721, 281, 652, 988, 300, 264, 3642, 645, 25323], "temperature": 0.0, "avg_logprob": -0.11892100742885045, "compression_ratio": 1.5846153846153845, "no_speech_prob": 7.071209438436199e-06}, {"id": 379, "seek": 191618, "start": 1921.5800000000002, "end": 1923.02, "text": " over time.", "tokens": [670, 565, 13], "temperature": 0.0, "avg_logprob": -0.11892100742885045, "compression_ratio": 1.5846153846153845, "no_speech_prob": 7.071209438436199e-06}, {"id": 380, "seek": 191618, "start": 1923.02, "end": 1929.24, "text": " So in some sense, that is the output of a process like the one you described.", "tokens": [407, 294, 512, 2020, 11, 300, 307, 264, 5598, 295, 257, 1399, 411, 264, 472, 291, 7619, 13], "temperature": 0.0, "avg_logprob": -0.11892100742885045, "compression_ratio": 1.5846153846153845, "no_speech_prob": 7.071209438436199e-06}, {"id": 381, "seek": 191618, "start": 1929.24, "end": 1939.1200000000001, "text": " But sort of, no, I guess I haven't seen a lot of work on sort of compiling the right", "tokens": [583, 1333, 295, 11, 572, 11, 286, 2041, 286, 2378, 380, 1612, 257, 688, 295, 589, 322, 1333, 295, 715, 4883, 264, 558], "temperature": 0.0, "avg_logprob": -0.11892100742885045, "compression_ratio": 1.5846153846153845, "no_speech_prob": 7.071209438436199e-06}, {"id": 382, "seek": 191618, "start": 1939.1200000000001, "end": 1944.3600000000001, "text": " data sets to train embeddings like this.", "tokens": [1412, 6352, 281, 3847, 12240, 29432, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11892100742885045, "compression_ratio": 1.5846153846153845, "no_speech_prob": 7.071209438436199e-06}, {"id": 383, "seek": 194436, "start": 1944.36, "end": 1950.8, "text": " I think I have heard of, I forget the citation, but I have heard of some work on how you can,", "tokens": [286, 519, 286, 362, 2198, 295, 11, 286, 2870, 264, 45590, 11, 457, 286, 362, 2198, 295, 512, 589, 322, 577, 291, 393, 11], "temperature": 0.0, "avg_logprob": -0.12782030839186448, "compression_ratio": 1.6137566137566137, "no_speech_prob": 2.753308945102617e-05}, {"id": 384, "seek": 194436, "start": 1950.8, "end": 1958.8799999999999, "text": " during the training process, for example, you can flip genders in sentences in the training", "tokens": [1830, 264, 3097, 1399, 11, 337, 1365, 11, 291, 393, 7929, 290, 16292, 294, 16579, 294, 264, 3097], "temperature": 0.0, "avg_logprob": -0.12782030839186448, "compression_ratio": 1.6137566137566137, "no_speech_prob": 2.753308945102617e-05}, {"id": 385, "seek": 194436, "start": 1958.8799999999999, "end": 1960.12, "text": " data and so on.", "tokens": [1412, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.12782030839186448, "compression_ratio": 1.6137566137566137, "no_speech_prob": 2.753308945102617e-05}, {"id": 386, "seek": 194436, "start": 1960.12, "end": 1965.8999999999999, "text": " So as opposed to actually compile representative sources, which may be hard, do training process", "tokens": [407, 382, 8851, 281, 767, 31413, 12424, 7139, 11, 597, 815, 312, 1152, 11, 360, 3097, 1399], "temperature": 0.0, "avg_logprob": -0.12782030839186448, "compression_ratio": 1.6137566137566137, "no_speech_prob": 2.753308945102617e-05}, {"id": 387, "seek": 194436, "start": 1965.8999999999999, "end": 1968.6, "text": " fixes.", "tokens": [32539, 13], "temperature": 0.0, "avg_logprob": -0.12782030839186448, "compression_ratio": 1.6137566137566137, "no_speech_prob": 2.753308945102617e-05}, {"id": 388, "seek": 196860, "start": 1968.6, "end": 1975.0, "text": " So the first question, I think, was on our consistent word list themselves a problem", "tokens": [407, 264, 700, 1168, 11, 286, 519, 11, 390, 322, 527, 8398, 1349, 1329, 2969, 257, 1154], "temperature": 0.0, "avg_logprob": -0.12033826876909305, "compression_ratio": 1.5242718446601942, "no_speech_prob": 4.859907676291186e-06}, {"id": 389, "seek": 196860, "start": 1975.0, "end": 1977.0, "text": " and can we correct for that?", "tokens": [293, 393, 321, 3006, 337, 300, 30], "temperature": 0.0, "avg_logprob": -0.12033826876909305, "compression_ratio": 1.5242718446601942, "no_speech_prob": 4.859907676291186e-06}, {"id": 390, "seek": 196860, "start": 1977.0, "end": 1979.7199999999998, "text": " So that certainly is.", "tokens": [407, 300, 3297, 307, 13], "temperature": 0.0, "avg_logprob": -0.12033826876909305, "compression_ratio": 1.5242718446601942, "no_speech_prob": 4.859907676291186e-06}, {"id": 391, "seek": 196860, "start": 1979.7199999999998, "end": 1987.24, "text": " And so, for example, one thing that we thought about doing in this work is, so our methods", "tokens": [400, 370, 11, 337, 1365, 11, 472, 551, 300, 321, 1194, 466, 884, 294, 341, 589, 307, 11, 370, 527, 7150], "temperature": 0.0, "avg_logprob": -0.12033826876909305, "compression_ratio": 1.5242718446601942, "no_speech_prob": 4.859907676291186e-06}, {"id": 392, "seek": 196860, "start": 1987.24, "end": 1993.52, "text": " for distinguishing between groups of people was using last names, which has the benefit", "tokens": [337, 11365, 3807, 1296, 3935, 295, 561, 390, 1228, 1036, 5288, 11, 597, 575, 264, 5121], "temperature": 0.0, "avg_logprob": -0.12033826876909305, "compression_ratio": 1.5242718446601942, "no_speech_prob": 4.859907676291186e-06}, {"id": 393, "seek": 199352, "start": 1993.52, "end": 1998.68, "text": " of being somewhat consistent in the census historically and sort of last names and like", "tokens": [295, 885, 8344, 8398, 294, 264, 23725, 16180, 293, 1333, 295, 1036, 5288, 293, 411], "temperature": 0.0, "avg_logprob": -0.1552018221686868, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.2803362551494502e-05}, {"id": 394, "seek": 199352, "start": 1998.68, "end": 2003.2, "text": " first names are decently consistent in describing ethnic groups.", "tokens": [700, 5288, 366, 979, 2276, 8398, 294, 16141, 14363, 3935, 13], "temperature": 0.0, "avg_logprob": -0.1552018221686868, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.2803362551494502e-05}, {"id": 395, "seek": 199352, "start": 2003.2, "end": 2010.6, "text": " And so one thing to, but it of course fails to distinguish between white and black Americans.", "tokens": [400, 370, 472, 551, 281, 11, 457, 309, 295, 1164, 18199, 281, 20206, 1296, 2418, 293, 2211, 6280, 13], "temperature": 0.0, "avg_logprob": -0.1552018221686868, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.2803362551494502e-05}, {"id": 396, "seek": 199352, "start": 2010.6, "end": 2013.56, "text": " Other work has used first names for that purpose.", "tokens": [5358, 589, 575, 1143, 700, 5288, 337, 300, 4334, 13], "temperature": 0.0, "avg_logprob": -0.1552018221686868, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.2803362551494502e-05}, {"id": 397, "seek": 199352, "start": 2013.56, "end": 2017.16, "text": " And we thought about doing that, but then we would have to do exactly what you said", "tokens": [400, 321, 1194, 466, 884, 300, 11, 457, 550, 321, 576, 362, 281, 360, 2293, 437, 291, 848], "temperature": 0.0, "avg_logprob": -0.1552018221686868, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.2803362551494502e-05}, {"id": 398, "seek": 201716, "start": 2017.16, "end": 2024.28, "text": " is correct for first name frequencies differ by generation quite a bit.", "tokens": [307, 3006, 337, 700, 1315, 20250, 743, 538, 5125, 1596, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.16092658042907715, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.321063518844312e-05}, {"id": 399, "seek": 201716, "start": 2024.28, "end": 2025.72, "text": " I imagine it can be done.", "tokens": [286, 3811, 309, 393, 312, 1096, 13], "temperature": 0.0, "avg_logprob": -0.16092658042907715, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.321063518844312e-05}, {"id": 400, "seek": 201716, "start": 2025.72, "end": 2033.52, "text": " I haven't seen work that corrects it exactly in context like this, but definitely interesting", "tokens": [286, 2378, 380, 1612, 589, 300, 3006, 82, 309, 2293, 294, 4319, 411, 341, 11, 457, 2138, 1880], "temperature": 0.0, "avg_logprob": -0.16092658042907715, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.321063518844312e-05}, {"id": 401, "seek": 201716, "start": 2033.52, "end": 2043.8400000000001, "text": " avenue to explore.", "tokens": [39230, 281, 6839, 13], "temperature": 0.0, "avg_logprob": -0.16092658042907715, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.321063518844312e-05}, {"id": 402, "seek": 204384, "start": 2043.84, "end": 2049.2799999999997, "text": " I just wanted to kind of make a shout out to the paper NLP statements by Emily Bender", "tokens": [286, 445, 1415, 281, 733, 295, 652, 257, 8043, 484, 281, 264, 3035, 426, 45196, 12363, 538, 15034, 363, 3216], "temperature": 0.0, "avg_logprob": -0.19612556528822284, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.5444131349795498e-05}, {"id": 403, "seek": 204384, "start": 2049.2799999999997, "end": 2052.56, "text": " and Batia Friedman, which is I think a relatively recent paper.", "tokens": [293, 10066, 654, 17605, 1601, 11, 597, 307, 286, 519, 257, 7226, 5162, 3035, 13], "temperature": 0.0, "avg_logprob": -0.19612556528822284, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.5444131349795498e-05}, {"id": 404, "seek": 204384, "start": 2052.56, "end": 2056.64, "text": " But I love the list of questions you had about kind of who creates the categories and who", "tokens": [583, 286, 959, 264, 1329, 295, 1651, 291, 632, 466, 733, 295, 567, 7829, 264, 10479, 293, 567], "temperature": 0.0, "avg_logprob": -0.19612556528822284, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.5444131349795498e-05}, {"id": 405, "seek": 204384, "start": 2056.64, "end": 2058.44, "text": " was included versus excluded.", "tokens": [390, 5556, 5717, 29486, 13], "temperature": 0.0, "avg_logprob": -0.19612556528822284, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.5444131349795498e-05}, {"id": 406, "seek": 204384, "start": 2058.44, "end": 2064.7999999999997, "text": " And just the idea of the NLP statements is to record these things about any data set.", "tokens": [400, 445, 264, 1558, 295, 264, 426, 45196, 12363, 307, 281, 2136, 613, 721, 466, 604, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.19612556528822284, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.5444131349795498e-05}, {"id": 407, "seek": 204384, "start": 2064.7999999999997, "end": 2069.7999999999997, "text": " And so that doesn't resolve the fact that people are probably still being excluded or", "tokens": [400, 370, 300, 1177, 380, 14151, 264, 1186, 300, 561, 366, 1391, 920, 885, 29486, 420], "temperature": 0.0, "avg_logprob": -0.19612556528822284, "compression_ratio": 1.6333333333333333, "no_speech_prob": 1.5444131349795498e-05}, {"id": 408, "seek": 206980, "start": 2069.8, "end": 2074.48, "text": " included, but it would at least provide kind of a record of these are the people that created", "tokens": [5556, 11, 457, 309, 576, 412, 1935, 2893, 733, 295, 257, 2136, 295, 613, 366, 264, 561, 300, 2942], "temperature": 0.0, "avg_logprob": -0.16360722068978958, "compression_ratio": 1.7588652482269505, "no_speech_prob": 1.9221190086682327e-05}, {"id": 409, "seek": 206980, "start": 2074.48, "end": 2075.48, "text": " this.", "tokens": [341, 13], "temperature": 0.0, "avg_logprob": -0.16360722068978958, "compression_ratio": 1.7588652482269505, "no_speech_prob": 1.9221190086682327e-05}, {"id": 410, "seek": 206980, "start": 2075.48, "end": 2079.84, "text": " And they give a few examples in the paper of kind of if you're using Amazon Mechanical", "tokens": [400, 436, 976, 257, 1326, 5110, 294, 264, 3035, 295, 733, 295, 498, 291, 434, 1228, 6795, 30175, 804], "temperature": 0.0, "avg_logprob": -0.16360722068978958, "compression_ratio": 1.7588652482269505, "no_speech_prob": 1.9221190086682327e-05}, {"id": 411, "seek": 206980, "start": 2079.84, "end": 2083.52, "text": " Turk workers, even to say kind of like what were their ages, what countries were they", "tokens": [15714, 5600, 11, 754, 281, 584, 733, 295, 411, 437, 645, 641, 12357, 11, 437, 3517, 645, 436], "temperature": 0.0, "avg_logprob": -0.16360722068978958, "compression_ratio": 1.7588652482269505, "no_speech_prob": 1.9221190086682327e-05}, {"id": 412, "seek": 206980, "start": 2083.52, "end": 2090.96, "text": " from, what variants of even within English, are you using kind of British English versus", "tokens": [490, 11, 437, 21669, 295, 754, 1951, 3669, 11, 366, 291, 1228, 733, 295, 6221, 3669, 5717], "temperature": 0.0, "avg_logprob": -0.16360722068978958, "compression_ratio": 1.7588652482269505, "no_speech_prob": 1.9221190086682327e-05}, {"id": 413, "seek": 206980, "start": 2090.96, "end": 2093.88, "text": " Australian English, these variations.", "tokens": [13337, 3669, 11, 613, 17840, 13], "temperature": 0.0, "avg_logprob": -0.16360722068978958, "compression_ratio": 1.7588652482269505, "no_speech_prob": 1.9221190086682327e-05}, {"id": 414, "seek": 206980, "start": 2093.88, "end": 2097.0800000000004, "text": " And so just to even have a way of kind of like documenting how these data sets were", "tokens": [400, 370, 445, 281, 754, 362, 257, 636, 295, 733, 295, 411, 42360, 577, 613, 1412, 6352, 645], "temperature": 0.0, "avg_logprob": -0.16360722068978958, "compression_ratio": 1.7588652482269505, "no_speech_prob": 1.9221190086682327e-05}, {"id": 415, "seek": 206980, "start": 2097.0800000000004, "end": 2098.0800000000004, "text": " constructed.", "tokens": [17083, 13], "temperature": 0.0, "avg_logprob": -0.16360722068978958, "compression_ratio": 1.7588652482269505, "no_speech_prob": 1.9221190086682327e-05}, {"id": 416, "seek": 209808, "start": 2098.08, "end": 2100.0, "text": " And that was an interesting paper recently.", "tokens": [400, 300, 390, 364, 1880, 3035, 3938, 13], "temperature": 0.0, "avg_logprob": -0.25816490173339846, "compression_ratio": 1.7063829787234042, "no_speech_prob": 1.0951795047731139e-05}, {"id": 417, "seek": 209808, "start": 2100.0, "end": 2106.44, "text": " I was just going to ask about the choice of using Euclidean distance versus cosine distance,", "tokens": [286, 390, 445, 516, 281, 1029, 466, 264, 3922, 295, 1228, 462, 1311, 31264, 282, 4560, 5717, 23565, 4560, 11], "temperature": 0.0, "avg_logprob": -0.25816490173339846, "compression_ratio": 1.7063829787234042, "no_speech_prob": 1.0951795047731139e-05}, {"id": 418, "seek": 209808, "start": 2106.44, "end": 2110.7999999999997, "text": " because I know sometimes in higher dimensional spaces, people use cosine distance.", "tokens": [570, 286, 458, 2171, 294, 2946, 18795, 7673, 11, 561, 764, 23565, 4560, 13], "temperature": 0.0, "avg_logprob": -0.25816490173339846, "compression_ratio": 1.7063829787234042, "no_speech_prob": 1.0951795047731139e-05}, {"id": 419, "seek": 209808, "start": 2110.7999999999997, "end": 2117.24, "text": " Yes, I think part of the intention of...", "tokens": [1079, 11, 286, 519, 644, 295, 264, 7789, 295, 485], "temperature": 0.0, "avg_logprob": -0.25816490173339846, "compression_ratio": 1.7063829787234042, "no_speech_prob": 1.0951795047731139e-05}, {"id": 420, "seek": 209808, "start": 2117.24, "end": 2121.08, "text": " Yeah, so yes, definitely that was a choice.", "tokens": [865, 11, 370, 2086, 11, 2138, 300, 390, 257, 3922, 13], "temperature": 0.0, "avg_logprob": -0.25816490173339846, "compression_ratio": 1.7063829787234042, "no_speech_prob": 1.0951795047731139e-05}, {"id": 421, "seek": 209808, "start": 2121.08, "end": 2127.0, "text": " And I think part of why we did it is Euclidean distance is just easier to explain and visualize.", "tokens": [400, 286, 519, 644, 295, 983, 321, 630, 309, 307, 462, 1311, 31264, 282, 4560, 307, 445, 3571, 281, 2903, 293, 23273, 13], "temperature": 0.0, "avg_logprob": -0.25816490173339846, "compression_ratio": 1.7063829787234042, "no_speech_prob": 1.0951795047731139e-05}, {"id": 422, "seek": 212700, "start": 2127.0, "end": 2134.96, "text": " And so I think we show in the appendix that it really doesn't matter, that sort of it's...", "tokens": [400, 370, 286, 519, 321, 855, 294, 264, 34116, 970, 300, 309, 534, 1177, 380, 1871, 11, 300, 1333, 295, 309, 311, 485], "temperature": 0.0, "avg_logprob": -0.23610817631588707, "compression_ratio": 1.5778894472361809, "no_speech_prob": 1.3630177818413358e-05}, {"id": 423, "seek": 212700, "start": 2134.96, "end": 2141.4, "text": " Yeah, the noise is different maybe, but really for a whole host of these things, it's like", "tokens": [865, 11, 264, 5658, 307, 819, 1310, 11, 457, 534, 337, 257, 1379, 3975, 295, 613, 721, 11, 309, 311, 411], "temperature": 0.0, "avg_logprob": -0.23610817631588707, "compression_ratio": 1.5778894472361809, "no_speech_prob": 1.3630177818413358e-05}, {"id": 424, "seek": 212700, "start": 2141.4, "end": 2143.56, "text": " within the noise of everything else.", "tokens": [1951, 264, 5658, 295, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.23610817631588707, "compression_ratio": 1.5778894472361809, "no_speech_prob": 1.3630177818413358e-05}, {"id": 425, "seek": 212700, "start": 2143.56, "end": 2150.64, "text": " And so regardless of distance metrics, some essentially identical patterns are going to", "tokens": [400, 370, 10060, 295, 4560, 16367, 11, 512, 4476, 14800, 8294, 366, 516, 281], "temperature": 0.0, "avg_logprob": -0.23610817631588707, "compression_ratio": 1.5778894472361809, "no_speech_prob": 1.3630177818413358e-05}, {"id": 426, "seek": 212700, "start": 2150.64, "end": 2151.64, "text": " emerge.", "tokens": [21511, 13], "temperature": 0.0, "avg_logprob": -0.23610817631588707, "compression_ratio": 1.5778894472361809, "no_speech_prob": 1.3630177818413358e-05}, {"id": 427, "seek": 215164, "start": 2151.64, "end": 2158.3199999999997, "text": " But yeah, another I think danger of work like this is I think...", "tokens": [583, 1338, 11, 1071, 286, 519, 4330, 295, 589, 411, 341, 307, 286, 519, 485], "temperature": 0.0, "avg_logprob": -0.2958695512068899, "compression_ratio": 1.4875621890547264, "no_speech_prob": 3.3929582059499808e-06}, {"id": 428, "seek": 215164, "start": 2158.3199999999997, "end": 2166.12, "text": " Well, first, not abusing the research degrees of freedom that you have, but then to documenting", "tokens": [1042, 11, 700, 11, 406, 410, 7981, 264, 2132, 5310, 295, 5645, 300, 291, 362, 11, 457, 550, 281, 42360], "temperature": 0.0, "avg_logprob": -0.2958695512068899, "compression_ratio": 1.4875621890547264, "no_speech_prob": 3.3929582059499808e-06}, {"id": 429, "seek": 215164, "start": 2166.12, "end": 2173.12, "text": " that you didn't do that and providing as much robustness checks as you can as is feasible", "tokens": [300, 291, 994, 380, 360, 300, 293, 6530, 382, 709, 13956, 1287, 13834, 382, 291, 393, 382, 307, 26648], "temperature": 0.0, "avg_logprob": -0.2958695512068899, "compression_ratio": 1.4875621890547264, "no_speech_prob": 3.3929582059499808e-06}, {"id": 430, "seek": 215164, "start": 2173.12, "end": 2175.72, "text": " or appropriate in the appendix.", "tokens": [420, 6854, 294, 264, 34116, 970, 13], "temperature": 0.0, "avg_logprob": -0.2958695512068899, "compression_ratio": 1.4875621890547264, "no_speech_prob": 3.3929582059499808e-06}, {"id": 431, "seek": 215164, "start": 2175.72, "end": 2178.52, "text": " Other questions?", "tokens": [5358, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2958695512068899, "compression_ratio": 1.4875621890547264, "no_speech_prob": 3.3929582059499808e-06}, {"id": 432, "seek": 217852, "start": 2178.52, "end": 2190.64, "text": " I noticed something that I think Rachel's planning to cover in a future lesson, but", "tokens": [286, 5694, 746, 300, 286, 519, 14246, 311, 5038, 281, 2060, 294, 257, 2027, 6898, 11, 457], "temperature": 0.0, "avg_logprob": -0.21525812149047852, "compression_ratio": 1.465, "no_speech_prob": 6.643064352829242e-06}, {"id": 433, "seek": 217852, "start": 2190.64, "end": 2199.56, "text": " there was a paper last year I think called FRAGE, F-A-R-G-E, which showed that the kind", "tokens": [456, 390, 257, 3035, 1036, 1064, 286, 519, 1219, 479, 3750, 9177, 11, 479, 12, 32, 12, 49, 12, 38, 12, 36, 11, 597, 4712, 300, 264, 733], "temperature": 0.0, "avg_logprob": -0.21525812149047852, "compression_ratio": 1.465, "no_speech_prob": 6.643064352829242e-06}, {"id": 434, "seek": 217852, "start": 2199.56, "end": 2204.12, "text": " of principal component of word vectors from word to vector and glove is actually word", "tokens": [295, 9716, 6542, 295, 1349, 18875, 490, 1349, 281, 8062, 293, 26928, 307, 767, 1349], "temperature": 0.0, "avg_logprob": -0.21525812149047852, "compression_ratio": 1.465, "no_speech_prob": 6.643064352829242e-06}, {"id": 435, "seek": 217852, "start": 2204.12, "end": 2206.24, "text": " frequency rather than word meaning.", "tokens": [7893, 2831, 813, 1349, 3620, 13], "temperature": 0.0, "avg_logprob": -0.21525812149047852, "compression_ratio": 1.465, "no_speech_prob": 6.643064352829242e-06}, {"id": 436, "seek": 220624, "start": 2206.24, "end": 2211.4799999999996, "text": " I guess when David asked his question, it made me think, oh, is that going to be particularly", "tokens": [286, 2041, 562, 4389, 2351, 702, 1168, 11, 309, 1027, 385, 519, 11, 1954, 11, 307, 300, 516, 281, 312, 4098], "temperature": 0.0, "avg_logprob": -0.17367170212116648, "compression_ratio": 1.591093117408907, "no_speech_prob": 4.784729298989987e-06}, {"id": 437, "seek": 220624, "start": 2211.4799999999996, "end": 2217.4399999999996, "text": " an issue here when you look for similarity between two words that it's going to particularly", "tokens": [364, 2734, 510, 562, 291, 574, 337, 32194, 1296, 732, 2283, 300, 309, 311, 516, 281, 4098], "temperature": 0.0, "avg_logprob": -0.17367170212116648, "compression_ratio": 1.591093117408907, "no_speech_prob": 4.784729298989987e-06}, {"id": 438, "seek": 220624, "start": 2217.4399999999996, "end": 2223.3999999999996, "text": " show change in frequency because that's such a major component?", "tokens": [855, 1319, 294, 7893, 570, 300, 311, 1270, 257, 2563, 6542, 30], "temperature": 0.0, "avg_logprob": -0.17367170212116648, "compression_ratio": 1.591093117408907, "no_speech_prob": 4.784729298989987e-06}, {"id": 439, "seek": 220624, "start": 2223.3999999999996, "end": 2230.24, "text": " Or did you actually kind of try to remove the frequency eigenvector out of the representations?", "tokens": [1610, 630, 291, 767, 733, 295, 853, 281, 4159, 264, 7893, 10446, 303, 1672, 484, 295, 264, 33358, 30], "temperature": 0.0, "avg_logprob": -0.17367170212116648, "compression_ratio": 1.591093117408907, "no_speech_prob": 4.784729298989987e-06}, {"id": 440, "seek": 220624, "start": 2230.24, "end": 2231.24, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.17367170212116648, "compression_ratio": 1.591093117408907, "no_speech_prob": 4.784729298989987e-06}, {"id": 441, "seek": 220624, "start": 2231.24, "end": 2233.3199999999997, "text": " So I'll answer this last question first.", "tokens": [407, 286, 603, 1867, 341, 1036, 1168, 700, 13], "temperature": 0.0, "avg_logprob": -0.17367170212116648, "compression_ratio": 1.591093117408907, "no_speech_prob": 4.784729298989987e-06}, {"id": 442, "seek": 223332, "start": 2233.32, "end": 2239.36, "text": " So no, we didn't do any preprocessing, like removing principal components.", "tokens": [407, 572, 11, 321, 994, 380, 360, 604, 2666, 340, 780, 278, 11, 411, 12720, 9716, 6677, 13], "temperature": 0.0, "avg_logprob": -0.10484148661295573, "compression_ratio": 2.0046511627906978, "no_speech_prob": 3.168957118759863e-05}, {"id": 443, "seek": 223332, "start": 2239.36, "end": 2245.96, "text": " But concerns like this of there's just shift in the embeddings for all sorts of reasons.", "tokens": [583, 7389, 411, 341, 295, 456, 311, 445, 5513, 294, 264, 12240, 29432, 337, 439, 7527, 295, 4112, 13], "temperature": 0.0, "avg_logprob": -0.10484148661295573, "compression_ratio": 2.0046511627906978, "no_speech_prob": 3.168957118759863e-05}, {"id": 444, "seek": 223332, "start": 2245.96, "end": 2251.6400000000003, "text": " There's no reason to believe that embeddings trained on one data set and then trained on", "tokens": [821, 311, 572, 1778, 281, 1697, 300, 12240, 29432, 8895, 322, 472, 1412, 992, 293, 550, 8895, 322], "temperature": 0.0, "avg_logprob": -0.10484148661295573, "compression_ratio": 2.0046511627906978, "no_speech_prob": 3.168957118759863e-05}, {"id": 445, "seek": 223332, "start": 2251.6400000000003, "end": 2255.48, "text": " another data set, even if the data sets were meant to be comparable.", "tokens": [1071, 1412, 992, 11, 754, 498, 264, 1412, 6352, 645, 4140, 281, 312, 25323, 13], "temperature": 0.0, "avg_logprob": -0.10484148661295573, "compression_ratio": 2.0046511627906978, "no_speech_prob": 3.168957118759863e-05}, {"id": 446, "seek": 223332, "start": 2255.48, "end": 2258.92, "text": " There's no reason to believe that the resulting embeddings are comparable for all sorts of", "tokens": [821, 311, 572, 1778, 281, 1697, 300, 264, 16505, 12240, 29432, 366, 25323, 337, 439, 7527, 295], "temperature": 0.0, "avg_logprob": -0.10484148661295573, "compression_ratio": 2.0046511627906978, "no_speech_prob": 3.168957118759863e-05}, {"id": 447, "seek": 223332, "start": 2258.92, "end": 2260.76, "text": " reasons like this.", "tokens": [4112, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10484148661295573, "compression_ratio": 2.0046511627906978, "no_speech_prob": 3.168957118759863e-05}, {"id": 448, "seek": 226076, "start": 2260.76, "end": 2266.84, "text": " So I think that challenge in particular motivated two of the things that we did.", "tokens": [407, 286, 519, 300, 3430, 294, 1729, 14515, 732, 295, 264, 721, 300, 321, 630, 13], "temperature": 0.0, "avg_logprob": -0.11828214815347501, "compression_ratio": 1.832618025751073, "no_speech_prob": 4.00562203139998e-05}, {"id": 449, "seek": 226076, "start": 2266.84, "end": 2273.1200000000003, "text": " The first one, which I'm not a huge fan of in general, but I think for challenges like", "tokens": [440, 700, 472, 11, 597, 286, 478, 406, 257, 2603, 3429, 295, 294, 2674, 11, 457, 286, 519, 337, 4759, 411], "temperature": 0.0, "avg_logprob": -0.11828214815347501, "compression_ratio": 1.832618025751073, "no_speech_prob": 4.00562203139998e-05}, {"id": 450, "seek": 226076, "start": 2273.1200000000003, "end": 2279.44, "text": " this we need to is although all the results in the paper are sort of comparisons.", "tokens": [341, 321, 643, 281, 307, 4878, 439, 264, 3542, 294, 264, 3035, 366, 1333, 295, 33157, 13], "temperature": 0.0, "avg_logprob": -0.11828214815347501, "compression_ratio": 1.832618025751073, "no_speech_prob": 4.00562203139998e-05}, {"id": 451, "seek": 226076, "start": 2279.44, "end": 2284.6000000000004, "text": " So they're not just what's the average distance between this group and the set of neutral", "tokens": [407, 436, 434, 406, 445, 437, 311, 264, 4274, 4560, 1296, 341, 1594, 293, 264, 992, 295, 10598], "temperature": 0.0, "avg_logprob": -0.11828214815347501, "compression_ratio": 1.832618025751073, "no_speech_prob": 4.00562203139998e-05}, {"id": 452, "seek": 226076, "start": 2284.6000000000004, "end": 2288.6800000000003, "text": " words, but rather it's what's the average distance between this group and these neutral", "tokens": [2283, 11, 457, 2831, 309, 311, 437, 311, 264, 4274, 4560, 1296, 341, 1594, 293, 613, 10598], "temperature": 0.0, "avg_logprob": -0.11828214815347501, "compression_ratio": 1.832618025751073, "no_speech_prob": 4.00562203139998e-05}, {"id": 453, "seek": 228868, "start": 2288.68, "end": 2294.7999999999997, "text": " words minus the average distance between another group and the same set of words.", "tokens": [2283, 3175, 264, 4274, 4560, 1296, 1071, 1594, 293, 264, 912, 992, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.10477761788801714, "compression_ratio": 1.645021645021645, "no_speech_prob": 1.1841713785543106e-05}, {"id": 454, "seek": 228868, "start": 2294.7999999999997, "end": 2301.44, "text": " The hope is that this sort of normalizes away some of these issues.", "tokens": [440, 1454, 307, 300, 341, 1333, 295, 2710, 5660, 1314, 512, 295, 613, 2663, 13], "temperature": 0.0, "avg_logprob": -0.10477761788801714, "compression_ratio": 1.645021645021645, "no_speech_prob": 1.1841713785543106e-05}, {"id": 455, "seek": 228868, "start": 2301.44, "end": 2308.14, "text": " But if that's not satisfactory, that's I think the purpose of the validation in the paper.", "tokens": [583, 498, 300, 311, 406, 48614, 11, 300, 311, 286, 519, 264, 4334, 295, 264, 24071, 294, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.10477761788801714, "compression_ratio": 1.645021645021645, "no_speech_prob": 1.1841713785543106e-05}, {"id": 456, "seek": 228868, "start": 2308.14, "end": 2311.8399999999997, "text": " So we're definitely concerned of things like this.", "tokens": [407, 321, 434, 2138, 5922, 295, 721, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10477761788801714, "compression_ratio": 1.645021645021645, "no_speech_prob": 1.1841713785543106e-05}, {"id": 457, "seek": 228868, "start": 2311.8399999999997, "end": 2318.2, "text": " But the sort of the various validations that we did with available historical data shows", "tokens": [583, 264, 1333, 295, 264, 3683, 7363, 763, 300, 321, 630, 365, 2435, 8584, 1412, 3110], "temperature": 0.0, "avg_logprob": -0.10477761788801714, "compression_ratio": 1.645021645021645, "no_speech_prob": 1.1841713785543106e-05}, {"id": 458, "seek": 231820, "start": 2318.2, "end": 2323.24, "text": " that while that may be a problem, it's not at least first order as far as we can detect", "tokens": [300, 1339, 300, 815, 312, 257, 1154, 11, 309, 311, 406, 412, 1935, 700, 1668, 382, 1400, 382, 321, 393, 5531], "temperature": 0.0, "avg_logprob": -0.24399420794318705, "compression_ratio": 1.5, "no_speech_prob": 1.952412821992766e-05}, {"id": 459, "seek": 231820, "start": 2323.24, "end": 2330.3599999999997, "text": " that the method as a whole is decently appropriate for comparing over time or across data sets.", "tokens": [300, 264, 3170, 382, 257, 1379, 307, 979, 2276, 6854, 337, 15763, 670, 565, 420, 2108, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.24399420794318705, "compression_ratio": 1.5, "no_speech_prob": 1.952412821992766e-05}, {"id": 460, "seek": 231820, "start": 2330.3599999999997, "end": 2331.3599999999997, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.24399420794318705, "compression_ratio": 1.5, "no_speech_prob": 1.952412821992766e-05}, {"id": 461, "seek": 231820, "start": 2331.3599999999997, "end": 2332.3599999999997, "text": " Question over here.", "tokens": [14464, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.24399420794318705, "compression_ratio": 1.5, "no_speech_prob": 1.952412821992766e-05}, {"id": 462, "seek": 231820, "start": 2332.3599999999997, "end": 2333.3599999999997, "text": " Hey.", "tokens": [1911, 13], "temperature": 0.0, "avg_logprob": -0.24399420794318705, "compression_ratio": 1.5, "no_speech_prob": 1.952412821992766e-05}, {"id": 463, "seek": 231820, "start": 2333.3599999999997, "end": 2346.3599999999997, "text": " So I guess my question for you is goal-wise or mission-wise, kind of what was the difference", "tokens": [407, 286, 2041, 452, 1168, 337, 291, 307, 3387, 12, 3711, 420, 4447, 12, 3711, 11, 733, 295, 437, 390, 264, 2649], "temperature": 0.0, "avg_logprob": -0.24399420794318705, "compression_ratio": 1.5, "no_speech_prob": 1.952412821992766e-05}, {"id": 464, "seek": 234636, "start": 2346.36, "end": 2351.1200000000003, "text": " between goal-wise or mission-wise, kind of what was the goal of doing this kind of research?", "tokens": [1296, 3387, 12, 3711, 420, 4447, 12, 3711, 11, 733, 295, 437, 390, 264, 3387, 295, 884, 341, 733, 295, 2132, 30], "temperature": 0.0, "avg_logprob": -0.21251174663675243, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.00017942130216397345}, {"id": 465, "seek": 234636, "start": 2351.1200000000003, "end": 2355.7200000000003, "text": " Is it to be kind of prescriptive, to maybe implement new rules and new understandings", "tokens": [1119, 309, 281, 312, 733, 295, 1183, 5944, 488, 11, 281, 1310, 4445, 777, 4474, 293, 777, 1223, 1109], "temperature": 0.0, "avg_logprob": -0.21251174663675243, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.00017942130216397345}, {"id": 466, "seek": 234636, "start": 2355.7200000000003, "end": 2360.2000000000003, "text": " of language, or is it just to kind of be like, okay, if you're going to do NLP work, here", "tokens": [295, 2856, 11, 420, 307, 309, 445, 281, 733, 295, 312, 411, 11, 1392, 11, 498, 291, 434, 516, 281, 360, 426, 45196, 589, 11, 510], "temperature": 0.0, "avg_logprob": -0.21251174663675243, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.00017942130216397345}, {"id": 467, "seek": 234636, "start": 2360.2000000000003, "end": 2362.0, "text": " are some things you can consider?", "tokens": [366, 512, 721, 291, 393, 1949, 30], "temperature": 0.0, "avg_logprob": -0.21251174663675243, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.00017942130216397345}, {"id": 468, "seek": 234636, "start": 2362.0, "end": 2366.48, "text": " Because for me, if you're doing research on gender and ethnic stereotypes, at some point", "tokens": [1436, 337, 385, 11, 498, 291, 434, 884, 2132, 322, 7898, 293, 14363, 30853, 11, 412, 512, 935], "temperature": 0.0, "avg_logprob": -0.21251174663675243, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.00017942130216397345}, {"id": 469, "seek": 234636, "start": 2366.48, "end": 2369.1200000000003, "text": " you have to define genders and you have to define ethnicities.", "tokens": [291, 362, 281, 6964, 290, 16292, 293, 291, 362, 281, 6964, 14363, 1088, 13], "temperature": 0.0, "avg_logprob": -0.21251174663675243, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.00017942130216397345}, {"id": 470, "seek": 234636, "start": 2369.1200000000003, "end": 2372.84, "text": " And I think that gets really hairy when you start thinking about ethnic minorities in", "tokens": [400, 286, 519, 300, 2170, 534, 42346, 562, 291, 722, 1953, 466, 14363, 30373, 294], "temperature": 0.0, "avg_logprob": -0.21251174663675243, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.00017942130216397345}, {"id": 471, "seek": 234636, "start": 2372.84, "end": 2375.32, "text": " a lot of different countries.", "tokens": [257, 688, 295, 819, 3517, 13], "temperature": 0.0, "avg_logprob": -0.21251174663675243, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.00017942130216397345}, {"id": 472, "seek": 237532, "start": 2375.32, "end": 2379.88, "text": " Even if you try to even define what black means in America, too, you get into some hairy", "tokens": [2754, 498, 291, 853, 281, 754, 6964, 437, 2211, 1355, 294, 3374, 11, 886, 11, 291, 483, 666, 512, 42346], "temperature": 0.0, "avg_logprob": -0.20799947607106176, "compression_ratio": 1.6592592592592592, "no_speech_prob": 4.9072608817368746e-05}, {"id": 473, "seek": 237532, "start": 2379.88, "end": 2380.88, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.20799947607106176, "compression_ratio": 1.6592592592592592, "no_speech_prob": 4.9072608817368746e-05}, {"id": 474, "seek": 237532, "start": 2380.88, "end": 2386.52, "text": " I mean, you mentioned, too, it's really important to bring in experts from other fields, and", "tokens": [286, 914, 11, 291, 2835, 11, 886, 11, 309, 311, 534, 1021, 281, 1565, 294, 8572, 490, 661, 7909, 11, 293], "temperature": 0.0, "avg_logprob": -0.20799947607106176, "compression_ratio": 1.6592592592592592, "no_speech_prob": 4.9072608817368746e-05}, {"id": 475, "seek": 237532, "start": 2386.52, "end": 2391.6000000000004, "text": " there definitely is a lot of research being done in a lot of the humanities spaces.", "tokens": [456, 2138, 307, 257, 688, 295, 2132, 885, 1096, 294, 257, 688, 295, 264, 36140, 7673, 13], "temperature": 0.0, "avg_logprob": -0.20799947607106176, "compression_ratio": 1.6592592592592592, "no_speech_prob": 4.9072608817368746e-05}, {"id": 476, "seek": 237532, "start": 2391.6000000000004, "end": 2393.88, "text": " Yeah, great question.", "tokens": [865, 11, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.20799947607106176, "compression_ratio": 1.6592592592592592, "no_speech_prob": 4.9072608817368746e-05}, {"id": 477, "seek": 237532, "start": 2393.88, "end": 2398.4, "text": " So at least our goal here was purely descriptive.", "tokens": [407, 412, 1935, 527, 3387, 510, 390, 17491, 42585, 13], "temperature": 0.0, "avg_logprob": -0.20799947607106176, "compression_ratio": 1.6592592592592592, "no_speech_prob": 4.9072608817368746e-05}, {"id": 478, "seek": 237532, "start": 2398.4, "end": 2403.6000000000004, "text": " I'm not an expert in any of the prescriptions or any of the very complex issues that you", "tokens": [286, 478, 406, 364, 5844, 294, 604, 295, 264, 1183, 34173, 420, 604, 295, 264, 588, 3997, 2663, 300, 291], "temperature": 0.0, "avg_logprob": -0.20799947607106176, "compression_ratio": 1.6592592592592592, "no_speech_prob": 4.9072608817368746e-05}, {"id": 479, "seek": 237532, "start": 2403.6000000000004, "end": 2404.6000000000004, "text": " talked about.", "tokens": [2825, 466, 13], "temperature": 0.0, "avg_logprob": -0.20799947607106176, "compression_ratio": 1.6592592592592592, "no_speech_prob": 4.9072608817368746e-05}, {"id": 480, "seek": 240460, "start": 2404.6, "end": 2410.2, "text": " And so I don't feel comfortable being prescriptive at all in what to do about bias in a particular", "tokens": [400, 370, 286, 500, 380, 841, 4619, 885, 1183, 5944, 488, 412, 439, 294, 437, 281, 360, 466, 12577, 294, 257, 1729], "temperature": 0.0, "avg_logprob": -0.17045476301661078, "compression_ratio": 1.7108433734939759, "no_speech_prob": 1.112402696890058e-05}, {"id": 481, "seek": 240460, "start": 2410.2, "end": 2411.2, "text": " application.", "tokens": [3861, 13], "temperature": 0.0, "avg_logprob": -0.17045476301661078, "compression_ratio": 1.7108433734939759, "no_speech_prob": 1.112402696890058e-05}, {"id": 482, "seek": 240460, "start": 2411.2, "end": 2417.2, "text": " And I think the unsatisfactory answer is the right thing to do is going to depend on this", "tokens": [400, 286, 519, 264, 2693, 25239, 21840, 1867, 307, 264, 558, 551, 281, 360, 307, 516, 281, 5672, 322, 341], "temperature": 0.0, "avg_logprob": -0.17045476301661078, "compression_ratio": 1.7108433734939759, "no_speech_prob": 1.112402696890058e-05}, {"id": 483, "seek": 240460, "start": 2417.2, "end": 2424.72, "text": " very specific context, like on this data set, in this specific application today, what is", "tokens": [588, 2685, 4319, 11, 411, 322, 341, 1412, 992, 11, 294, 341, 2685, 3861, 965, 11, 437, 307], "temperature": 0.0, "avg_logprob": -0.17045476301661078, "compression_ratio": 1.7108433734939759, "no_speech_prob": 1.112402696890058e-05}, {"id": 484, "seek": 240460, "start": 2424.72, "end": 2427.56, "text": " the right thing to do?", "tokens": [264, 558, 551, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.17045476301661078, "compression_ratio": 1.7108433734939759, "no_speech_prob": 1.112402696890058e-05}, {"id": 485, "seek": 240460, "start": 2427.56, "end": 2432.6, "text": " And there are definitely general rules that other people that are far more experts than", "tokens": [400, 456, 366, 2138, 2674, 4474, 300, 661, 561, 300, 366, 1400, 544, 8572, 813], "temperature": 0.0, "avg_logprob": -0.17045476301661078, "compression_ratio": 1.7108433734939759, "no_speech_prob": 1.112402696890058e-05}, {"id": 486, "seek": 240460, "start": 2432.6, "end": 2433.7599999999998, "text": " I am have come up with.", "tokens": [286, 669, 362, 808, 493, 365, 13], "temperature": 0.0, "avg_logprob": -0.17045476301661078, "compression_ratio": 1.7108433734939759, "no_speech_prob": 1.112402696890058e-05}, {"id": 487, "seek": 243376, "start": 2433.76, "end": 2440.88, "text": " But no, the purpose of this paper was purely as descriptive of you can use these embeddings", "tokens": [583, 572, 11, 264, 4334, 295, 341, 3035, 390, 17491, 382, 42585, 295, 291, 393, 764, 613, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.1562371703813661, "compression_ratio": 1.686046511627907, "no_speech_prob": 1.1124457159894519e-05}, {"id": 488, "seek": 243376, "start": 2440.88, "end": 2443.92, "text": " to say something about historical beliefs.", "tokens": [281, 584, 746, 466, 8584, 13585, 13], "temperature": 0.0, "avg_logprob": -0.1562371703813661, "compression_ratio": 1.686046511627907, "no_speech_prob": 1.1124457159894519e-05}, {"id": 489, "seek": 243376, "start": 2443.92, "end": 2449.36, "text": " I was just going to ask if there are any kind of particular future directions that you either", "tokens": [286, 390, 445, 516, 281, 1029, 498, 456, 366, 604, 733, 295, 1729, 2027, 11095, 300, 291, 2139], "temperature": 0.0, "avg_logprob": -0.1562371703813661, "compression_ratio": 1.686046511627907, "no_speech_prob": 1.1124457159894519e-05}, {"id": 490, "seek": 243376, "start": 2449.36, "end": 2453.0, "text": " want to continue with this work or that you'd like to see other people take this work in?", "tokens": [528, 281, 2354, 365, 341, 589, 420, 300, 291, 1116, 411, 281, 536, 661, 561, 747, 341, 589, 294, 30], "temperature": 0.0, "avg_logprob": -0.1562371703813661, "compression_ratio": 1.686046511627907, "no_speech_prob": 1.1124457159894519e-05}, {"id": 491, "seek": 243376, "start": 2453.0, "end": 2454.6800000000003, "text": " Yeah, great question.", "tokens": [865, 11, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1562371703813661, "compression_ratio": 1.686046511627907, "no_speech_prob": 1.1124457159894519e-05}, {"id": 492, "seek": 243376, "start": 2454.6800000000003, "end": 2462.0, "text": " I think there definitely has been a lot of work in this line, very exciting work in this", "tokens": [286, 519, 456, 2138, 575, 668, 257, 688, 295, 589, 294, 341, 1622, 11, 588, 4670, 589, 294, 341], "temperature": 0.0, "avg_logprob": -0.1562371703813661, "compression_ratio": 1.686046511627907, "no_speech_prob": 1.1124457159894519e-05}, {"id": 493, "seek": 243376, "start": 2462.0, "end": 2463.0, "text": " line.", "tokens": [1622, 13], "temperature": 0.0, "avg_logprob": -0.1562371703813661, "compression_ratio": 1.686046511627907, "no_speech_prob": 1.1124457159894519e-05}, {"id": 494, "seek": 246300, "start": 2463.0, "end": 2469.52, "text": " And I think a few directions that I want to highlight is one is on the technical side,", "tokens": [400, 286, 519, 257, 1326, 11095, 300, 286, 528, 281, 5078, 307, 472, 307, 322, 264, 6191, 1252, 11], "temperature": 0.0, "avg_logprob": -0.15815487313777843, "compression_ratio": 1.6772908366533865, "no_speech_prob": 2.246947042294778e-05}, {"id": 495, "seek": 246300, "start": 2469.52, "end": 2475.92, "text": " there's a bunch of decisions that we made just for simplicity and to reduce our degrees", "tokens": [456, 311, 257, 3840, 295, 5327, 300, 321, 1027, 445, 337, 25632, 293, 281, 5407, 527, 5310], "temperature": 0.0, "avg_logprob": -0.15815487313777843, "compression_ratio": 1.6772908366533865, "no_speech_prob": 2.246947042294778e-05}, {"id": 496, "seek": 246300, "start": 2475.92, "end": 2476.92, "text": " of freedom.", "tokens": [295, 5645, 13], "temperature": 0.0, "avg_logprob": -0.15815487313777843, "compression_ratio": 1.6772908366533865, "no_speech_prob": 2.246947042294778e-05}, {"id": 497, "seek": 246300, "start": 2476.92, "end": 2479.68, "text": " So one includes correcting for these frequencies.", "tokens": [407, 472, 5974, 47032, 337, 613, 20250, 13], "temperature": 0.0, "avg_logprob": -0.15815487313777843, "compression_ratio": 1.6772908366533865, "no_speech_prob": 2.246947042294778e-05}, {"id": 498, "seek": 246300, "start": 2479.68, "end": 2485.52, "text": " Another includes exactly the algorithm that we use that there's work on instead of black", "tokens": [3996, 5974, 2293, 264, 9284, 300, 321, 764, 300, 456, 311, 589, 322, 2602, 295, 2211], "temperature": 0.0, "avg_logprob": -0.15815487313777843, "compression_ratio": 1.6772908366533865, "no_speech_prob": 2.246947042294778e-05}, {"id": 499, "seek": 246300, "start": 2485.52, "end": 2491.72, "text": " block embeddings, can you use embeddings where the dimensions have a little more interpretation", "tokens": [3461, 12240, 29432, 11, 393, 291, 764, 12240, 29432, 689, 264, 12819, 362, 257, 707, 544, 14174], "temperature": 0.0, "avg_logprob": -0.15815487313777843, "compression_ratio": 1.6772908366533865, "no_speech_prob": 2.246947042294778e-05}, {"id": 500, "seek": 249172, "start": 2491.72, "end": 2493.8399999999997, "text": " or embeddings that are smoother over time.", "tokens": [420, 12240, 29432, 300, 366, 28640, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.17037969827651978, "compression_ratio": 1.5933014354066986, "no_speech_prob": 3.320632822578773e-05}, {"id": 501, "seek": 249172, "start": 2493.8399999999997, "end": 2498.68, "text": " So instead of just training differently on different data sets, can you sort of actually", "tokens": [407, 2602, 295, 445, 3097, 7614, 322, 819, 1412, 6352, 11, 393, 291, 1333, 295, 767], "temperature": 0.0, "avg_logprob": -0.17037969827651978, "compression_ratio": 1.5933014354066986, "no_speech_prob": 3.320632822578773e-05}, {"id": 502, "seek": 249172, "start": 2498.68, "end": 2503.8599999999997, "text": " track the trajectory of a word as you add more data that's more and more recent?", "tokens": [2837, 264, 21512, 295, 257, 1349, 382, 291, 909, 544, 1412, 300, 311, 544, 293, 544, 5162, 30], "temperature": 0.0, "avg_logprob": -0.17037969827651978, "compression_ratio": 1.5933014354066986, "no_speech_prob": 3.320632822578773e-05}, {"id": 503, "seek": 249172, "start": 2503.8599999999997, "end": 2510.3599999999997, "text": " So I think on a technical angle, there's all sorts of improvements you can make to reduce", "tokens": [407, 286, 519, 322, 257, 6191, 5802, 11, 456, 311, 439, 7527, 295, 13797, 291, 393, 652, 281, 5407], "temperature": 0.0, "avg_logprob": -0.17037969827651978, "compression_ratio": 1.5933014354066986, "no_speech_prob": 3.320632822578773e-05}, {"id": 504, "seek": 249172, "start": 2510.3599999999997, "end": 2514.52, "text": " the noise in the measurements.", "tokens": [264, 5658, 294, 264, 15383, 13], "temperature": 0.0, "avg_logprob": -0.17037969827651978, "compression_ratio": 1.5933014354066986, "no_speech_prob": 3.320632822578773e-05}, {"id": 505, "seek": 251452, "start": 2514.52, "end": 2523.92, "text": " One sort of the application setting as well is that we sort of sprayed on a bunch of,", "tokens": [1485, 1333, 295, 264, 3861, 3287, 382, 731, 307, 300, 321, 1333, 295, 40330, 322, 257, 3840, 295, 11], "temperature": 0.0, "avg_logprob": -0.15392907873376624, "compression_ratio": 1.5561224489795917, "no_speech_prob": 7.888421350799035e-06}, {"id": 506, "seek": 251452, "start": 2523.92, "end": 2531.64, "text": " I don't want to say random, but arbitrary applications using somewhat ad hoc ways of", "tokens": [286, 500, 380, 528, 281, 584, 4974, 11, 457, 23211, 5821, 1228, 8344, 614, 16708, 2098, 295], "temperature": 0.0, "avg_logprob": -0.15392907873376624, "compression_ratio": 1.5561224489795917, "no_speech_prob": 7.888421350799035e-06}, {"id": 507, "seek": 251452, "start": 2531.64, "end": 2537.44, "text": " constructing, for example, word lists and so on.", "tokens": [39969, 11, 337, 1365, 11, 1349, 14511, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.15392907873376624, "compression_ratio": 1.5561224489795917, "no_speech_prob": 7.888421350799035e-06}, {"id": 508, "seek": 251452, "start": 2537.44, "end": 2543.16, "text": " In my opinion, principled in trying to find the most general list online, but I'm not", "tokens": [682, 452, 4800, 11, 3681, 15551, 294, 1382, 281, 915, 264, 881, 2674, 1329, 2950, 11, 457, 286, 478, 406], "temperature": 0.0, "avg_logprob": -0.15392907873376624, "compression_ratio": 1.5561224489795917, "no_speech_prob": 7.888421350799035e-06}, {"id": 509, "seek": 254316, "start": 2543.16, "end": 2548.7999999999997, "text": " an expert in any of the domains that we apply the methodology to.", "tokens": [364, 5844, 294, 604, 295, 264, 25514, 300, 321, 3079, 264, 24850, 281, 13], "temperature": 0.0, "avg_logprob": -0.07463410626287045, "compression_ratio": 1.6147186147186148, "no_speech_prob": 8.13880433270242e-06}, {"id": 510, "seek": 254316, "start": 2548.7999999999997, "end": 2554.68, "text": " So on an applied sense, I would love to have more domain experts formalize.", "tokens": [407, 322, 364, 6456, 2020, 11, 286, 576, 959, 281, 362, 544, 9274, 8572, 9860, 1125, 13], "temperature": 0.0, "avg_logprob": -0.07463410626287045, "compression_ratio": 1.6147186147186148, "no_speech_prob": 8.13880433270242e-06}, {"id": 511, "seek": 254316, "start": 2554.68, "end": 2557.3199999999997, "text": " These are specific things we would want to look at.", "tokens": [1981, 366, 2685, 721, 321, 576, 528, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.07463410626287045, "compression_ratio": 1.6147186147186148, "no_speech_prob": 8.13880433270242e-06}, {"id": 512, "seek": 254316, "start": 2557.3199999999997, "end": 2563.04, "text": " So for example, on the Asian stereotypes over time, I think there's a lot of work to be", "tokens": [407, 337, 1365, 11, 322, 264, 10645, 30853, 670, 565, 11, 286, 519, 456, 311, 257, 688, 295, 589, 281, 312], "temperature": 0.0, "avg_logprob": -0.07463410626287045, "compression_ratio": 1.6147186147186148, "no_speech_prob": 8.13880433270242e-06}, {"id": 513, "seek": 254316, "start": 2563.04, "end": 2569.7599999999998, "text": " done on how narratives rise and fall and how competing narratives may play out in language.", "tokens": [1096, 322, 577, 28016, 6272, 293, 2100, 293, 577, 15439, 28016, 815, 862, 484, 294, 2856, 13], "temperature": 0.0, "avg_logprob": -0.07463410626287045, "compression_ratio": 1.6147186147186148, "no_speech_prob": 8.13880433270242e-06}, {"id": 514, "seek": 256976, "start": 2569.76, "end": 2574.7200000000003, "text": " And I think that's something for an expert in that domain to explore.", "tokens": [400, 286, 519, 300, 311, 746, 337, 364, 5844, 294, 300, 9274, 281, 6839, 13], "temperature": 0.0, "avg_logprob": -0.10054339649521302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.4284716598922387e-05}, {"id": 515, "seek": 256976, "start": 2574.7200000000003, "end": 2579.76, "text": " I know you mentioned in the last answer to my previous question that part of the answer", "tokens": [286, 458, 291, 2835, 294, 264, 1036, 1867, 281, 452, 3894, 1168, 300, 644, 295, 264, 1867], "temperature": 0.0, "avg_logprob": -0.10054339649521302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.4284716598922387e-05}, {"id": 516, "seek": 256976, "start": 2579.76, "end": 2584.88, "text": " to can you improve the process or the calculation is, well, it's already pretty good.", "tokens": [281, 393, 291, 3470, 264, 1399, 420, 264, 17108, 307, 11, 731, 11, 309, 311, 1217, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.10054339649521302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.4284716598922387e-05}, {"id": 517, "seek": 256976, "start": 2584.88, "end": 2589.8, "text": " So given that, from your last answer, it also sounds like you're also open to continuing", "tokens": [407, 2212, 300, 11, 490, 428, 1036, 1867, 11, 309, 611, 3263, 411, 291, 434, 611, 1269, 281, 9289], "temperature": 0.0, "avg_logprob": -0.10054339649521302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.4284716598922387e-05}, {"id": 518, "seek": 256976, "start": 2589.8, "end": 2591.7200000000003, "text": " to improve it further.", "tokens": [281, 3470, 309, 3052, 13], "temperature": 0.0, "avg_logprob": -0.10054339649521302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.4284716598922387e-05}, {"id": 519, "seek": 256976, "start": 2591.7200000000003, "end": 2598.48, "text": " I guess I have a question about the use of word embeddings in social sciences and these", "tokens": [286, 2041, 286, 362, 257, 1168, 466, 264, 764, 295, 1349, 12240, 29432, 294, 2093, 17677, 293, 613], "temperature": 0.0, "avg_logprob": -0.10054339649521302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 1.4284716598922387e-05}, {"id": 520, "seek": 259848, "start": 2598.48, "end": 2600.88, "text": " kind of things more generally.", "tokens": [733, 295, 721, 544, 5101, 13], "temperature": 0.0, "avg_logprob": -0.13546526055586966, "compression_ratio": 1.51440329218107, "no_speech_prob": 2.178096110583283e-05}, {"id": 521, "seek": 259848, "start": 2600.88, "end": 2606.64, "text": " It's a super convenient and, as we say, fairly effective tool.", "tokens": [467, 311, 257, 1687, 10851, 293, 11, 382, 321, 584, 11, 6457, 4942, 2290, 13], "temperature": 0.0, "avg_logprob": -0.13546526055586966, "compression_ratio": 1.51440329218107, "no_speech_prob": 2.178096110583283e-05}, {"id": 522, "seek": 259848, "start": 2606.64, "end": 2614.16, "text": " But we also know that these kinds of word embeddings are a bit of a blunt instrument.", "tokens": [583, 321, 611, 458, 300, 613, 3685, 295, 1349, 12240, 29432, 366, 257, 857, 295, 257, 32246, 7198, 13], "temperature": 0.0, "avg_logprob": -0.13546526055586966, "compression_ratio": 1.51440329218107, "no_speech_prob": 2.178096110583283e-05}, {"id": 523, "seek": 259848, "start": 2614.16, "end": 2618.16, "text": " They're linear models of co-occurrence matrices.", "tokens": [814, 434, 8213, 5245, 295, 598, 12, 905, 14112, 10760, 32284, 13], "temperature": 0.0, "avg_logprob": -0.13546526055586966, "compression_ratio": 1.51440329218107, "no_speech_prob": 2.178096110583283e-05}, {"id": 524, "seek": 259848, "start": 2618.16, "end": 2623.44, "text": " They don't really capture the nuance of language.", "tokens": [814, 500, 380, 534, 7983, 264, 42625, 295, 2856, 13], "temperature": 0.0, "avg_logprob": -0.13546526055586966, "compression_ratio": 1.51440329218107, "no_speech_prob": 2.178096110583283e-05}, {"id": 525, "seek": 259848, "start": 2623.44, "end": 2628.32, "text": " Do you know, you or anybody else have looked at whether it's possible to do similar kinds", "tokens": [1144, 291, 458, 11, 291, 420, 4472, 1646, 362, 2956, 412, 1968, 309, 311, 1944, 281, 360, 2531, 3685], "temperature": 0.0, "avg_logprob": -0.13546526055586966, "compression_ratio": 1.51440329218107, "no_speech_prob": 2.178096110583283e-05}, {"id": 526, "seek": 262832, "start": 2628.32, "end": 2637.92, "text": " of social science analyses using the richer representations in a full nonlinear language", "tokens": [295, 2093, 3497, 37560, 1228, 264, 29021, 33358, 294, 257, 1577, 2107, 28263, 2856], "temperature": 0.0, "avg_logprob": -0.24819448396757052, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.726354634636664e-06}, {"id": 527, "seek": 262832, "start": 2637.92, "end": 2640.48, "text": " model type representation?", "tokens": [2316, 2010, 10290, 30], "temperature": 0.0, "avg_logprob": -0.24819448396757052, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.726354634636664e-06}, {"id": 528, "seek": 262832, "start": 2640.48, "end": 2644.44, "text": " And would that be even useful?", "tokens": [400, 576, 300, 312, 754, 4420, 30], "temperature": 0.0, "avg_logprob": -0.24819448396757052, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.726354634636664e-06}, {"id": 529, "seek": 262832, "start": 2644.44, "end": 2646.88, "text": " Definitely useful.", "tokens": [12151, 4420, 13], "temperature": 0.0, "avg_logprob": -0.24819448396757052, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.726354634636664e-06}, {"id": 530, "seek": 262832, "start": 2646.88, "end": 2648.92, "text": " I have seen some recent work.", "tokens": [286, 362, 1612, 512, 5162, 589, 13], "temperature": 0.0, "avg_logprob": -0.24819448396757052, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.726354634636664e-06}, {"id": 531, "seek": 262832, "start": 2648.92, "end": 2652.96, "text": " I'm going to mess up the citation, so I'll send it to you offline instead of saying it", "tokens": [286, 478, 516, 281, 2082, 493, 264, 45590, 11, 370, 286, 603, 2845, 309, 281, 291, 21857, 2602, 295, 1566, 309], "temperature": 0.0, "avg_logprob": -0.24819448396757052, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.726354634636664e-06}, {"id": 532, "seek": 262832, "start": 2652.96, "end": 2655.96, "text": " out of a video.", "tokens": [484, 295, 257, 960, 13], "temperature": 0.0, "avg_logprob": -0.24819448396757052, "compression_ratio": 1.5360824742268042, "no_speech_prob": 3.726354634636664e-06}, {"id": 533, "seek": 265596, "start": 2655.96, "end": 2664.92, "text": " Yeah, so I think my two-part answer to this is, yes, they're pretty good in that I don't", "tokens": [865, 11, 370, 286, 519, 452, 732, 12, 6971, 1867, 281, 341, 307, 11, 2086, 11, 436, 434, 1238, 665, 294, 300, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.1818944745593601, "compression_ratio": 1.4870466321243523, "no_speech_prob": 4.78459242003737e-06}, {"id": 534, "seek": 265596, "start": 2664.92, "end": 2671.04, "text": " think we're missing a zeroth order thing that completely invalidates the methodology.", "tokens": [519, 321, 434, 5361, 257, 44746, 900, 1668, 551, 300, 2584, 34702, 1024, 264, 24850, 13], "temperature": 0.0, "avg_logprob": -0.1818944745593601, "compression_ratio": 1.4870466321243523, "no_speech_prob": 4.78459242003737e-06}, {"id": 535, "seek": 265596, "start": 2671.04, "end": 2677.16, "text": " But there's a lot of improvements to make it precise to validate on different datasets", "tokens": [583, 456, 311, 257, 688, 295, 13797, 281, 652, 309, 13600, 281, 29562, 322, 819, 42856], "temperature": 0.0, "avg_logprob": -0.1818944745593601, "compression_ratio": 1.4870466321243523, "no_speech_prob": 4.78459242003737e-06}, {"id": 536, "seek": 265596, "start": 2677.16, "end": 2681.2400000000002, "text": " to use better approaches.", "tokens": [281, 764, 1101, 11587, 13], "temperature": 0.0, "avg_logprob": -0.1818944745593601, "compression_ratio": 1.4870466321243523, "no_speech_prob": 4.78459242003737e-06}, {"id": 537, "seek": 268124, "start": 2681.24, "end": 2686.2, "text": " And I could just, I guess, make a kind of shout out to a few papers, several of which", "tokens": [400, 286, 727, 445, 11, 286, 2041, 11, 652, 257, 733, 295, 8043, 484, 281, 257, 1326, 10577, 11, 2940, 295, 597], "temperature": 0.0, "avg_logprob": -0.1975942884172712, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.095186780730728e-05}, {"id": 538, "seek": 268124, "start": 2686.2, "end": 2689.68, "text": " you already mentioned, related to this question of where to debias embeddings.", "tokens": [291, 1217, 2835, 11, 4077, 281, 341, 1168, 295, 689, 281, 3001, 4609, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.1975942884172712, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.095186780730728e-05}, {"id": 539, "seek": 268124, "start": 2689.68, "end": 2693.08, "text": " And I was glad you raised that question.", "tokens": [400, 286, 390, 5404, 291, 6005, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1975942884172712, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.095186780730728e-05}, {"id": 540, "seek": 268124, "start": 2693.08, "end": 2701.12, "text": " So kind of the two papers from 2016, the Balakbasi one was proposing a method to debias embeddings", "tokens": [407, 733, 295, 264, 732, 10577, 490, 6549, 11, 264, 13140, 514, 65, 8483, 472, 390, 29939, 257, 3170, 281, 3001, 4609, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.1975942884172712, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.095186780730728e-05}, {"id": 541, "seek": 268124, "start": 2701.12, "end": 2703.3999999999996, "text": " that used SBD as part of it.", "tokens": [300, 1143, 26944, 35, 382, 644, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1975942884172712, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.095186780730728e-05}, {"id": 542, "seek": 268124, "start": 2703.3999999999996, "end": 2707.52, "text": " I always like to see shout outs to SBD.", "tokens": [286, 1009, 411, 281, 536, 8043, 14758, 281, 26944, 35, 13], "temperature": 0.0, "avg_logprob": -0.1975942884172712, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.095186780730728e-05}, {"id": 543, "seek": 270752, "start": 2707.52, "end": 2715.28, "text": " And then the Kaliskin-Bryson and Nuranjan paper said that they thought that the debiasing", "tokens": [400, 550, 264, 12655, 271, 5843, 12, 33, 627, 3015, 293, 426, 12125, 14763, 3035, 848, 300, 436, 1194, 300, 264, 3001, 4609, 278], "temperature": 0.0, "avg_logprob": -0.2183262507120768, "compression_ratio": 1.66015625, "no_speech_prob": 9.817693353397772e-06}, {"id": 544, "seek": 270752, "start": 2715.28, "end": 2717.7599999999998, "text": " should occur at the point of action.", "tokens": [820, 5160, 412, 264, 935, 295, 3069, 13], "temperature": 0.0, "avg_logprob": -0.2183262507120768, "compression_ratio": 1.66015625, "no_speech_prob": 9.817693353397772e-06}, {"id": 545, "seek": 270752, "start": 2717.7599999999998, "end": 2720.88, "text": " And so typically, you know, we're embedding sort of this kind of more basic building block", "tokens": [400, 370, 5850, 11, 291, 458, 11, 321, 434, 12240, 3584, 1333, 295, 341, 733, 295, 544, 3875, 2390, 3461], "temperature": 0.0, "avg_logprob": -0.2183262507120768, "compression_ratio": 1.66015625, "no_speech_prob": 9.817693353397772e-06}, {"id": 546, "seek": 270752, "start": 2720.88, "end": 2722.4, "text": " in an application.", "tokens": [294, 364, 3861, 13], "temperature": 0.0, "avg_logprob": -0.2183262507120768, "compression_ratio": 1.66015625, "no_speech_prob": 9.817693353397772e-06}, {"id": 547, "seek": 270752, "start": 2722.4, "end": 2730.96, "text": " And so the idea was like, oh, if you debias then, bias can still seep into your application", "tokens": [400, 370, 264, 1558, 390, 411, 11, 1954, 11, 498, 291, 3001, 4609, 550, 11, 12577, 393, 920, 536, 79, 666, 428, 3861], "temperature": 0.0, "avg_logprob": -0.2183262507120768, "compression_ratio": 1.66015625, "no_speech_prob": 9.817693353397772e-06}, {"id": 548, "seek": 270752, "start": 2730.96, "end": 2732.28, "text": " elsewhere.", "tokens": [14517, 13], "temperature": 0.0, "avg_logprob": -0.2183262507120768, "compression_ratio": 1.66015625, "no_speech_prob": 9.817693353397772e-06}, {"id": 549, "seek": 270752, "start": 2732.28, "end": 2735.88, "text": " And perhaps even also, I think what Jeremy was getting at with kind of having complex", "tokens": [400, 4317, 754, 611, 11, 286, 519, 437, 17809, 390, 1242, 412, 365, 733, 295, 1419, 3997], "temperature": 0.0, "avg_logprob": -0.2183262507120768, "compression_ratio": 1.66015625, "no_speech_prob": 9.817693353397772e-06}, {"id": 550, "seek": 273588, "start": 2735.88, "end": 2738.56, "text": " interactions, and so you should debias at the point of action.", "tokens": [13280, 11, 293, 370, 291, 820, 3001, 4609, 412, 264, 935, 295, 3069, 13], "temperature": 0.0, "avg_logprob": -0.21179189601866136, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8129325073678046e-06}, {"id": 551, "seek": 273588, "start": 2738.56, "end": 2744.52, "text": " And there was a paper earlier this year from Hila Gonin and Joav Goldberg.", "tokens": [400, 456, 390, 257, 3035, 3071, 341, 1064, 490, 389, 7371, 47403, 259, 293, 3139, 706, 6731, 6873, 13], "temperature": 0.0, "avg_logprob": -0.21179189601866136, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8129325073678046e-06}, {"id": 552, "seek": 273588, "start": 2744.52, "end": 2749.92, "text": " I don't know if you read this one, lipstick on a pig.", "tokens": [286, 500, 380, 458, 498, 291, 1401, 341, 472, 11, 22543, 322, 257, 8120, 13], "temperature": 0.0, "avg_logprob": -0.21179189601866136, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8129325073678046e-06}, {"id": 553, "seek": 273588, "start": 2749.92, "end": 2755.92, "text": " Debiasing kind of covers up the structural biases, but doesn't remove them.", "tokens": [1346, 5614, 3349, 733, 295, 10538, 493, 264, 15067, 32152, 11, 457, 1177, 380, 4159, 552, 13], "temperature": 0.0, "avg_logprob": -0.21179189601866136, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8129325073678046e-06}, {"id": 554, "seek": 273588, "start": 2755.92, "end": 2760.8, "text": " And so that was kind of more evidence for the point of debiasing at the point of action,", "tokens": [400, 370, 300, 390, 733, 295, 544, 4467, 337, 264, 935, 295, 3001, 4609, 278, 412, 264, 935, 295, 3069, 11], "temperature": 0.0, "avg_logprob": -0.21179189601866136, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8129325073678046e-06}, {"id": 555, "seek": 273588, "start": 2760.8, "end": 2761.8, "text": " not like the embeddings.", "tokens": [406, 411, 264, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.21179189601866136, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8129325073678046e-06}, {"id": 556, "seek": 273588, "start": 2761.8, "end": 2765.4, "text": " So I just wanted to share those kind of perspectives.", "tokens": [407, 286, 445, 1415, 281, 2073, 729, 733, 295, 16766, 13], "temperature": 0.0, "avg_logprob": -0.21179189601866136, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8129325073678046e-06}, {"id": 557, "seek": 276540, "start": 2765.4, "end": 2769.32, "text": " I don't know that there's a consensus though on how this should be dealt with.", "tokens": [286, 500, 380, 458, 300, 456, 311, 257, 19115, 1673, 322, 577, 341, 820, 312, 15991, 365, 13], "temperature": 0.0, "avg_logprob": -0.212238613764445, "compression_ratio": 1.654275092936803, "no_speech_prob": 2.111045250785537e-05}, {"id": 558, "seek": 276540, "start": 2769.32, "end": 2770.32, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.212238613764445, "compression_ratio": 1.654275092936803, "no_speech_prob": 2.111045250785537e-05}, {"id": 559, "seek": 276540, "start": 2770.32, "end": 2771.92, "text": " No, I think these are all great.", "tokens": [883, 11, 286, 519, 613, 366, 439, 869, 13], "temperature": 0.0, "avg_logprob": -0.212238613764445, "compression_ratio": 1.654275092936803, "no_speech_prob": 2.111045250785537e-05}, {"id": 560, "seek": 276540, "start": 2771.92, "end": 2776.76, "text": " And the consensus or maybe there will be a consensus.", "tokens": [400, 264, 19115, 420, 1310, 456, 486, 312, 257, 19115, 13], "temperature": 0.0, "avg_logprob": -0.212238613764445, "compression_ratio": 1.654275092936803, "no_speech_prob": 2.111045250785537e-05}, {"id": 561, "seek": 276540, "start": 2776.76, "end": 2781.1600000000003, "text": " I'm sort of my view again, this is a very unexpert view and that I haven't looked closely", "tokens": [286, 478, 1333, 295, 452, 1910, 797, 11, 341, 307, 257, 588, 11572, 15346, 1910, 293, 300, 286, 2378, 380, 2956, 8185], "temperature": 0.0, "avg_logprob": -0.212238613764445, "compression_ratio": 1.654275092936803, "no_speech_prob": 2.111045250785537e-05}, {"id": 562, "seek": 276540, "start": 2781.1600000000003, "end": 2788.6, "text": " at any of these methods beyond just like cursory implementing a few of them is that I think", "tokens": [412, 604, 295, 613, 7150, 4399, 445, 411, 13946, 827, 18114, 257, 1326, 295, 552, 307, 300, 286, 519], "temperature": 0.0, "avg_logprob": -0.212238613764445, "compression_ratio": 1.654275092936803, "no_speech_prob": 2.111045250785537e-05}, {"id": 563, "seek": 276540, "start": 2788.6, "end": 2795.28, "text": " it's nice and good if you can debias at another stage, for example, at the point of action.", "tokens": [309, 311, 1481, 293, 665, 498, 291, 393, 3001, 4609, 412, 1071, 3233, 11, 337, 1365, 11, 412, 264, 935, 295, 3069, 13], "temperature": 0.0, "avg_logprob": -0.212238613764445, "compression_ratio": 1.654275092936803, "no_speech_prob": 2.111045250785537e-05}, {"id": 564, "seek": 279528, "start": 2795.28, "end": 2801.96, "text": " But unfortunately, a lot of these language models are very end to end and there might", "tokens": [583, 7015, 11, 257, 688, 295, 613, 2856, 5245, 366, 588, 917, 281, 917, 293, 456, 1062], "temperature": 0.0, "avg_logprob": -0.1534597969055176, "compression_ratio": 1.75, "no_speech_prob": 1.618565511307679e-05}, {"id": 565, "seek": 279528, "start": 2801.96, "end": 2806.32, "text": " not be a convenient place where you can say this is where the action is.", "tokens": [406, 312, 257, 10851, 1081, 689, 291, 393, 584, 341, 307, 689, 264, 3069, 307, 13], "temperature": 0.0, "avg_logprob": -0.1534597969055176, "compression_ratio": 1.75, "no_speech_prob": 1.618565511307679e-05}, {"id": 566, "seek": 279528, "start": 2806.32, "end": 2813.0400000000004, "text": " And so yeah, whether the point of fixing some of these issues are at your model of the language", "tokens": [400, 370, 1338, 11, 1968, 264, 935, 295, 19442, 512, 295, 613, 2663, 366, 412, 428, 2316, 295, 264, 2856], "temperature": 0.0, "avg_logprob": -0.1534597969055176, "compression_ratio": 1.75, "no_speech_prob": 1.618565511307679e-05}, {"id": 567, "seek": 279528, "start": 2813.0400000000004, "end": 2818.8, "text": " or at some other downstream and at the encoder, sort of at the decoder instead of the encoder,", "tokens": [420, 412, 512, 661, 30621, 293, 412, 264, 2058, 19866, 11, 1333, 295, 412, 264, 979, 19866, 2602, 295, 264, 2058, 19866, 11], "temperature": 0.0, "avg_logprob": -0.1534597969055176, "compression_ratio": 1.75, "no_speech_prob": 1.618565511307679e-05}, {"id": 568, "seek": 279528, "start": 2818.8, "end": 2822.8, "text": " then yeah, I think that's still an open question.", "tokens": [550, 1338, 11, 286, 519, 300, 311, 920, 364, 1269, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1534597969055176, "compression_ratio": 1.75, "no_speech_prob": 1.618565511307679e-05}, {"id": 569, "seek": 282280, "start": 2822.8, "end": 2825.7200000000003, "text": " But yeah, that's my very small take.", "tokens": [583, 1338, 11, 300, 311, 452, 588, 1359, 747, 13], "temperature": 0.0, "avg_logprob": -0.38875156749378553, "compression_ratio": 1.3185840707964602, "no_speech_prob": 1.9818166038021445e-05}, {"id": 570, "seek": 282280, "start": 2825.7200000000003, "end": 2827.84, "text": " Well, thank you.", "tokens": [1042, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.38875156749378553, "compression_ratio": 1.3185840707964602, "no_speech_prob": 1.9818166038021445e-05}, {"id": 571, "seek": 282280, "start": 2827.84, "end": 2830.32, "text": " Any final questions?", "tokens": [2639, 2572, 1651, 30], "temperature": 0.0, "avg_logprob": -0.38875156749378553, "compression_ratio": 1.3185840707964602, "no_speech_prob": 1.9818166038021445e-05}, {"id": 572, "seek": 282280, "start": 2830.32, "end": 2831.32, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.38875156749378553, "compression_ratio": 1.3185840707964602, "no_speech_prob": 1.9818166038021445e-05}, {"id": 573, "seek": 282280, "start": 2831.32, "end": 2832.6000000000004, "text": " Well, thank you so much.", "tokens": [1042, 11, 1309, 291, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.38875156749378553, "compression_ratio": 1.3185840707964602, "no_speech_prob": 1.9818166038021445e-05}, {"id": 574, "seek": 282280, "start": 2832.6000000000004, "end": 2833.6000000000004, "text": " We are so glad to have you.", "tokens": [492, 366, 370, 5404, 281, 362, 291, 13], "temperature": 0.0, "avg_logprob": -0.38875156749378553, "compression_ratio": 1.3185840707964602, "no_speech_prob": 1.9818166038021445e-05}, {"id": 575, "seek": 283360, "start": 2833.6, "end": 2857.6, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51564], "temperature": 0.0, "avg_logprob": -0.8989671866099039, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0010769120417535305}], "language": "en"}