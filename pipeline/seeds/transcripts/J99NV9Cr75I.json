{"text": " Welcome back So we had a busy lesson last week and I Was really thrilled to see actually one of our master's students here at USF actually Actually took what we learned Took what we learned with structured deep learning and turned it into a Blog post which as I suspected has been incredibly popular because it's just something People didn't know about and so it actually ended up getting picked up by the towards data science publication Which I quite like actually if you're interested in keeping up with what's going on in data science It's quite good medium publication and so Karam talked about Structured deep learning and basically introduced you know the the basic ideas that we learned about last week and It got picked up quite quite widely one of the one of the things I was pleased to see actually Sebastian Ruda who actually mentioned in last week's class as being one of my favorite researchers Tweeted it and then somebody from stitch fix said oh, yeah, we've actually been doing that for ages, which is kind of cute I Kind of know that this is happening in industry a lot and I've been telling people this is happening in industry a lot But nobody's been talking about it and now that Karen's kind of published a blog saying hey check out this cool thing and now stitch Fix is like yeah doing that already so So that's been great Great to see and I think there's still a lot more that can be dug into with this structured deep learning stuff You know to build on top of Karen's post would be that maybe like Experiment with some different data sets maybe find some old Kaggle competitions and see like Now there's some competitions that you could now win with this or some which doesn't work for would be equally interesting and also like just Experimenting a bit with different amounts of dropout different layer sizes, you know Because nobody much has written about this. I don't think there's been any blog posts about this before that I've seen anywhere There's a lot of unexplored territory So I think there's a lot we could we could build on top of here And there's definitely a lot of interest I saw one person on Twitter saying this is what I've been looking for ages Another thing which I was pleased to see is Nikhil who we saw his cricket versus baseball Predictor as well as his currency predictor after lesson one Went on to Download something a bit bigger which was to download a couple of hundred of images of actors and he manually Went through and checked which well I think first of all he like used Google to try and find ones with glasses and ones without then he manually went through And checked that they had been put in the right spot And this is a good example of one where? Vanilla resonant didn't do so well with just the last layer And so what Nikhil did was he went through and tried unfreezing the layers and using differential learning rates and got up to 100% accuracy and The thing I like about these things that Nikhil is doing is the way he's he's not downloading a Kaggle data set He's like deciding on a problem that he's going to try and solve he's going from scratch from Google And he's actually got a link here even to a suggested way to help you download images from Google So I think this is great, and I actually gave a talk Just this afternoon at Singularity University to an executive team of one of the world's largest Telecommunications companies and actually show them this post Because the the folks there were telling me that that all the vendors that come to them and tell them they need like Millions of images and huge data centers full of hardware, and you know they have to buy a special software that only these vendors can provide and I said like Actually, this person's been doing a course for three weeks now and look at what he's just done With a computer that cost him 60 cents an hour and they were like they were so happy to hear that like okay There you know this actually is in the reach of normal people I'm assuming Nikhil's a normal person. I haven't actually If you're proudly abnormal Nikhil I apologize I actually went and actually had a look at his cricket Classifier and I was really pleased to see that his code actually is the exact same code That we used in lesson one. I was hoping that would be the case you know the only thing he changed was The number of epochs I guess So this idea that we can take those four lines of code and reuse it to do other things It's definitely turned out to be true and so these are good things to show like at your organization if you're anything like the executives of this big company I spoke to today, there'll be a certain amount of like Not just surprise, but almost like pushback with like if this was true somebody You know they basically said that this is true somebody would have told us so like why isn't everybody doing this already? So like I think you might have to actually show them you know maybe you can build your own With some internal data you've got at work or something like here. It is you know didn't cost me anything It's all finished um Vitaly or vitally I don't know how to pronounce his name correctly has done another very nice post on Just an introductory post on how we train neural networks, and I wanted to point this one out as being like I think This is one of the participants in this course who's just got a particular knack for Technical communication, and I think we can all learn from you know from his posts about about good technical writing and What I really like particularly is that he? He assumes almost nothing like he has a kind of a very chatty tone and describes everything But he also assumes that the reader is intelligent But you know so like he's not afraid to kind of say he has a paper or here's an equation or or whatever But then he's going to go through and tell you exactly what that equation means So it's kind of like this nice mix of like writing for a respectfully for an intelligent audience, but also not assuming any particular background knowledge So then I made the mistake earlier this week of posting a picture of my first placing on the Kaggle seedlings Competition at which point five other fast AI students posted their pictures of them passing me over the next few days so This is the current leaderboard for the Kaggle plant seedlings competition I believe the product top six are all fast AI students or in the worst of those teachers And so I think this is like a really oh Look James has just passed he was first This is a really good example of like What you can do this is? I'm trying to think it was like a small number of thousands of images and Most of the images were only were less than a hundred pixels by a hundred pixels And yet we you know I bet my approach was basically to say let's just run through the notebook We have pretty much default took me. I don't know an hour and I'm I think the other students doing a little bit more than that, but not a hell of a lot more and basically What this is saying is yeah these these techniques Work pretty reliably to a point where people that aren't using the fast AI libraries are You know literally really struggling I Suspect all these are fast AI students you might have to go down quite a way So I thought that was very interesting and really really cool So today we're going to Start what I would kind of call like the second half of this course so the first half of this course has been like getting through Like these are the applications that we can use this for Here's kind of the code you have to write Here's a fairly high level ish description of what it's doing and We're kind of we're kind of done for that bit and what we're now going to do is go in reverse We're going to go back over all of those exact same things again But this time we're going to dig into the detail of everyone and we're going to look inside the source code of the fast AI Library to see what it's doing and try to replicate that So in a sense like there's not going to be a lot more Best practices to show you like I've kind of shown you the best best practices I know But I feel like for us to now build on top of those to debug those models To come back to part two where we're going to kind of try out some new things You know it really helps to understand what's going on behind the scenes, okay? so the goal here today is we're going to try and create a pretty effective collaborative filtering model Almost entirely from scratch, so we'll use the kind of we'll use pytorch as a Automatic differentiation tool and as a GPU programming tool and not very much else. We'll try not to use its neural net features We'll try not to use Fast AI library any more than necessary so that's the goal so Let's go back and you know we only very quickly looked at collaborative filtering last time So let's let's go back and have a look at collaborative filtering and so we're going to look at this movie lens data set so the movie lens data set Basically is a list of ratings It's got a bunch of different users that are represented by some ID and a bunch of movies that are represented by some ID and Rating it also has a timestamp. I haven't actually ever tried to use this I guess this is just like what what time did that person rate that movie? So that's all we're going to use for modeling is Three columns user ID movie ID and rating and so thinking of that in kind of Structured data terms user ID and movie ID would be categorical variables We have two of them and rating would be a would be an independent variable We're not going to use this for modeling, but we can use it for looking at stuff later We can grab a list of the names of the movies as well And you could use this genre information. I haven't tried to be interested if during the week anybody tries it and finds it helpful My guess is you might not find it helpful. We'll see So in order to kind of look at this better. I just grabbed the Users that have watched the most movies and the movies that have been the most watched And made a cross tab of it right so this is exactly the same data But it's a subset and now rather than being user movie rating we've got user movie rating And so some users haven't watched some of these movies. That's why some of these are not a number, okay? Then I copied that into Excel And you'll see there's a thing called collab filter dot XLS If you don't see it there now, I'll make sure I've got it there by tomorrow and Here is where I've copied That table okay, so as I go through this like Set up of the problem and kind of how it's described and stuff if you're ever feeling lost feel free to Ask either directly or through the forum If you ask through the forum and somebody answers there, I want you to answer it here but if somebody else asks a question you would like answered of course just like it and Your net will keep an eye out for that Because kind of as we're digging in to the details of what's going on behind the scenes It's kind of important that at each stage you feel like okay. I can see What's going on? Okay Okay, so we're actually not going to build a neural net to start with Instead we're going to do something called a matrix factorization The reason we're not going to build a neural net to start with is that it so happens. There's a really really simple Kind of way of solving these kinds of problems which I'm going to show you and so if I scroll down I've basically what I've got here is the same the same thing but this time these are my predictions Rather than my actuals, and I'm going to show you how I created these predictions. Okay, so here are my actuals right here are my predictions and then down here we have our Score which is the sum of the difference squared Red average square root okay, so this is RMSE down here, okay, so on average we're Randomly initialized model is out by 2.8 So let me show you what this model is and I'm going to show you by saying how do we guess? How much user ID number 14? likes movie ID number 27 and The prediction here. This is just at this stage. This is still random is 0.91 So how are we calculating 0.91 and the answer is we're taking it as This vector here Dot product with this vector here, so dot product means point seven one times point one nine plus 0.81 times point six three plus point seven four plus point three one and so forth and In you know linear algebra speak because one of them is a column and one of them is a row This is the same as a matrix product so you can see here. I've used the Excel function matrix multiplier And that's my prediction Having said that if the original Rating doesn't exist at all Then I'm just going to set this to zero right because like there's no error in predicting something that hasn't happened Okay, so what I'm going to do is I'm basically going to say all right every one of my rate rate my predictions is not Going to be a neural net. It's going to be a single matrix multiplication Now the matrix multiplication that it's doing is basically in practice is between like this matrix and this Matrix right so each one of these is a single part of that So I randomly initialize these these are just random numbers That I've just pasted in here So I've basically started off with two Random matrices, and I've said let's assume for the time being that every rating can be represented as the the matrix product of those two So then in Excel you can actually do gradient descent You have to go to your options to the add-in section and and check the box to say turn it on and once you do You'll see there's something there called solver, and if I go solver it says okay. What's your? Objective function, and you just choose the cell so in this case we chose the cell that contains our rip-need-square error and Then it says okay. What do you want to? Change and you can see here We've selected this matrix and this matrix and so it's going to do a gradient descent For us by changing these matrices to try and in this case minimize this is min minimize this excel Cell right grg nonlinear is a gradient descent method so I say solve and you'll see it starts at 2.8 and Then down here. You'll see that numbers going down It's not actually showing us what it's doing, but we can see that the numbers going down, so this is kind of got a Neural netty feel to it in that we're doing like a matrix product, and we're doing a gradient descent But we don't have a nonlinear Layer, and we don't have a second Linear layer on top of that so we don't get to call this deep learning so things where people do like deep learning is things where they have kind of Metrics products and gradient descents, but it's not deep people tend to just call that shallow learning okay, so we're doing shallow learning here Right so I'm just going to go ahead and press escape to stop it because I'm sick of waiting And so you can see we've now got down to the 0.39 All right, so for example It guessed that movie 72 for sorry movie 27 for user 72 would get 4.44 rating 27 72 it actually got a 4 rating so you can see like it's it's it's doing something quite useful So Why is it doing something quite useful? I mean something to note here is The number of things we're trying to predict here is there's 225 of them right and The number of things we're using to predict is that times 2 so 150 of them So it's not like we can just exactly fit we actually have to do some kind of machine learning here so basically what this is saying is that there does seem to be some way of Making predictions in this way and so for those of you that have done some linear algebra This is actually a matrix decomposition normally in linear algebra You would do this using an analytical technique or using some techniques that are specifically designed for this purpose But the nice thing is that we can use Gradient descent to solve pretty much everything including this I don't like to so much think of it from a linear algebra point of view Though I think from an intuitive point of view which is this let's say movie. Sorry let's say movie ID 27 is Lord of the Rings part one and Let's say Movie and so let's say we're trying to make that prediction for user 72 are they going to like Lord of the Rings part one and so conceptually That particular movie. Maybe there's like this for sorry there's five numbers here And we could say like well what if the first one was like how much is it sci-fi and fantasy and the second one is like How recent a movie and how much special effects is there you know and the one at the top might be like how dialogue driven? Right like let's say those kind of five these five numbers represented particular things about the movie And so if that was the case then we could have the same five numbers for the user saying like okay How much does the user like sci-fi and fantasy? How much does the user like? modern Modern CGI driven movies how much does the does this user like dialogue driven movies? And so if you then took that cross product You would expect to have a good model right you expect to have a reasonable rating now the problem is We don't have this information for each user. We don't have the information for each movie So we're just going to like assume That this is a reasonable Kind of way of thinking about this system, and let's and let's stochastic gradient descent try and find these numbers Right so so in other words these these factors we call these things factors these factors And we call them factors because you can multiply them together to create this But they're factors in a linear algebra sense these factors we call them latent factors because they're not actually this is not actually a Vector that we've like named and understood and like entered in manually we've kind of assumed That we can think of movie ratings this way we've assumed that we can think of them as a dot product of Some particular features about a movie and some particular features of to look what users like those kinds of movies right and then we've used gradient descent To just say okay try and find some numbers that work So that's that's basically the technique right and it's kind of The and the entirety is in this spreadsheet right so that is collaborative filtering using what we call probabilistic matrix factorization And as you can see the whole thing is easy to do in an Excel spreadsheet and the entirety of it Really is this single thing which is a single matrix multiplication? plus randomly initializing We would like to know if it would be better to cap this to 0 and 5 maybe yeah Yeah, we're going to do that later right. There's a whole lot of stuff. We can do to improve this. This is like our our Simplest possible starting point right so so what we're going to do now is we're going to try and implement this in Python And run it on the whole data set no question is how do you figure out how many? You know how it's clear how long are the matrix why is this five yeah, yeah? So something to think about Given that this is like movie 49 right and We're looking at a rating for movie 49 Think about this. This is actually an embedding matrix and So this length is actually The size of the embedding matrix. I'm not saying this is an analogy. I'm saying it literally this is literally an embedding matrix We could have a one hot encoding where 72 Where a 1 is in the 72nd position and so we'd like to look it up and it would return This list of five numbers so the question is actually How do we decide on the dimensionality of our embedding vectors? And the answer to that question is we have no idea We have to try a few things and see what works the underlying concept is you need to pick an embedding dimensionality which is enough to reflect the kind of true complexity of this causal system But not so big that you Have too many parameters that could take forever to run or even with regularization it might overfit So what does it mean when the factor is negative that The factor being negative in the movie case would mean like this is not dialogue driven in fact it's like the opposite dialogue here is terrible a negative for the user would be like I actually dislike modern CGI movies so it's not from zero to whatever it's the range of Score would be negative is that range of score even like no no maximum. No there's no constraints at all here These are just standard embedding matrices Thanks Couple of questions so first question is why do what why can we trust this embeddings? Because like if you take a number six it can be expressed as one into six or like six into one or two into three And three into two also you're saying like we could like reorder these five numbers in some other different order Or like the value itself might be different as long as the product is something well But you see we're using gradient descent to find the best numbers So like once we found a good minimum the idea is like Yeah, there are other numbers, but they don't give you as good an objective value And of course we should be checking that on a validation set really which we'll be doing in the Python version Okay, and the second question is when we have a new movie or a new user do we have to retrain the model? That is a really good question, and there isn't a straightforward answer to that Time permitting will come back to it But basically you would need to have like a kind of a new user Model or a new movie model that you would use initially And then over time yes, you would then have to retrain the model so like I don't know if they still do it But Netflix used to have this thing when you were first on boarded onto Netflix It would say like what movies do you like? And you'd have to go through and like say a bunch of movies you like and it would then like train its model Could you could you just find the nearest movie to the movie that you're trying to do the new movie that you're trying to add Yeah, you could use nearest neighbors for sure But the thing is initially at least in this case we have no Columns to describe a movie so if you had something about like the movies Genre release date who was in it or something you could have some kind of non collaborative filtering model And that was kind of what I meant by like a new movie model. You'd have to have some some kind of predictors Okay, so a Lot of this is going to look familiar and and the way I'm going to do this is again It's kind of this top-down approach. We're going to start using a Few features of pytorch and fast AI and gradually we're going to redo it a few times in a few different ways Kind of doing a little bit deeper each time Regardless we do need a validation set so we can use our standard cross validation indexes approach to grab a random set of IDs This is something called weight decay Which we'll talk about later in the course for those of you that have done some machine learning its L2 regularization basically And this is where we choose how big a embedding matrix do we want okay? So again, you know here's where we get our model data object from CSV Passing in that ratings file which remember Looks like that Okay, so you'll see like stuff Tends to look pretty familiar after a while And then you just have to pass in The What are your rows effectively? What are your columns effectively and what are your values effectively right so any any collaborative filtering? Recommendation system approach. There's basically a concept of like You know a user and an item Now they might not be users and items like if you're doing the yet that Ecuadorian groceries competition There are stores and items and you're trying to predict how many things are you going to sell? At this store of this type But generally speaking just this idea of like you've got a couple of kind of high cardinality Categorical variables and something that you're measuring and you're kind of conceptualizing it as saying okay, we could predict The rating we can predict the value by doing this this dot product Interestingly this is kind of relevant to that that last question or suggestion an Identical way to think about this or to express this is to say when we're deciding Whether user 72 will like movie 27 It's basically saying which other users liked movies that 72 liked and which other movies were liked by people like User 72 it turns out that these are basically two ways of saying the exact same thing So basically what collaborative filtering is doing? You know kind of conceptually is to say okay this movie and this user which other movies are similar to it in terms of like Similar people enjoyed them and which people are similar to this person based on people that like the same kind of movies so that's kind of the underlying Structure at any time there's an underlying structure like this that kind of collaborative filtering approach is likely to be useful Okay, so So you yeah, so there's basically two parts the two bits of your thing that you're factoring and then the value the dependent variable So as per usual we can take our model data and ask for a learner from it And we need to tell it what size embedding matrix to use How many sorry what validation set indexes to use what batch size to use and what optimizer? To use and we're going to be talking more about optimizers Shortly, we won't do Adam today, but we'll do Adam Next week or the week after And then we can go ahead and say fit All right, and it all looks pretty similar interesting as usual interestingly I only had to do three epochs like this kind of model seemed to train super quickly You can use the learning rate finder as per usual all the stuff you're familiar with will work fine And that was it so this took you know about two seconds the train. There's no pre trained anything's here This is from random from scratch So This is our validation set and we can compare it we have this is a mean squared error Not a root mean squared error, so we can take the square root So that last time I ran it was point seven seven six and that's point eight eight and there's some benchmarks available for this data set And when I scrolled through and found the bench the best benchmark I could find here from this recommendation system specific library They had point nine one so we've got a better loss in two seconds Already so that's good So that's basically how you can do collaborative filtering with the fast AI library without Thinking too much, but so now we're going to dig in and try and rebuild that we'll try and get to the point that we're Getting something around point seven seven point seven eight from scratch But if you want to do this yourself at home, you know without worrying about the detail That's you know those three lines of code is all you need Okay, so we can get the predictions in the usual way and you know we could for example plot SNS is seaborn seaborn is a really great plotting library it sits on top of matplotlib It actually leverages matplotlib so anything we learn about matplotlib will help you with seaborn It's got a few like nice little plots like this joint plot Here is undoing predictions against Against actuals so these are my actuals these are my predictions and you can kind of see the The shape here is that as we predict higher numbers They actually are higher numbers and you can also see the histogram of the predictions and a histogram of the actions So I was just kind of plotting that just to show you another interesting visualization Could you please explain the n factors Why it's set to 50? It's set to 50 because I tried a few things that's in the world. That's all It's this it's the dimensionality of the embedding matrix Or to think of it in another way. It's like how you know rather than five it's 50 Jeremy I have a question about suppose that your Recommendation system is more implicit so you have zeros or ones instead of just actual numbers right so basically we would then Need to use a classifier instead of a regressor Have to sample the negative for something like that, so if you don't have it We just have once let's say like just kind of implicit feedback. Oh I'm not sure we'll get to that one in this class But what I will say is like in the case that you're just doing classification rather than regression We haven't actually built that in the library yet, maybe somebody this week wants to try adding it there would only be a small number of lines of code you basically have to change the activation function to be a sigmoid and you would have to change the Criterion or the loss function to be cross entropy rather than RMSC and that will give you a Classifier rather than a regressor those are the only things you'd have to change so hopefully somebody this week We'll take up that challenge and by the time we come back next week. We will have that working Okay So I said that we're basically doing a dot product right or no a dot product is kind of the vector version I guess if this matrix product So we're basically doing each of these things times each of these things and then add it together So that's a dot product, so let's just have a look at how we do that in pi torch So we can create a tensor in pi torch just using this little capital T thing You can just say that's the fast AI version the full version is torched up from non-pi or something But I've got it set up so you can possibly pass in even a list of lists So this is going to create a torch tensor with one two three four And then here's a torch tensor with two two ten ten okay, so here are two Torch tensors I didn't say dot Cuda, so they're not on the GPU. They're sitting on the CPU just FYI We can multiply them together All right, and so anytime you have a mathematical operator between tensors in non-pi or pi torch It will do element wise Assuming that they're the same dimensionality which they are they're both two by two Okay, and so here we've got two by two is four Three by ten is thirty and so forth okay, so there's a times B So if you think about basically what we want to do here is we want to take Okay, so I've got one Times two is two two times two is four Two plus four is six and so that is actually the dot product between one two and two four and Then here we've got three by ten is thirty four by forty sorry four by ten is forty thirty and forty is seventy So in other words a times B dot sum along the first dimension So that's summing up the columns in other words across a row okay this thing here is doing the dot product of each of these rows with each of these rows So make sense and obviously we could do that with You know some kind of matrix modification approach, but I'm trying to really do things with as little special case stuff as possible Okay, so that's what we're going to use for our dot products from now on so basically all we need to do now is Is remember we have the data we have is not in that crosstab format so in Excel We've got it in this crosstab format, but we've got it here in this Listed format user movie rating user movie rating So conceptually we want to be like looking up this user Into our embedding matrix to find their 50 factors looking up that movie to find their 50 factors and then take the dot product of those two 50 long vectors So Let's do that To do it we're going to build a layer our own custom Neural net layer that's not going to be a neural net right so the the the more generic Vocabulary we call this is we're going to build a pie torch module Okay, so a pie torch module is a very specific thing It's something that you can use as a layer in a neural net once you've created your own pie torch module You can throw it into a neural net And a module works by assuming we've already got one say called model You can pass in some things in parentheses, and it will calculate it right so assuming that we already have a module called dot product We can instantiate it like so to create our dot product Object and we can basically now treat that like a function All right, but the thing is it's not just a function Because we'll be able to do things like take derivatives of it stack them up together into a big Stack of neural network layers blah blah blah, right, so it's basically a function that we can kind of compose very conveniently So here how do we define a module which as you can see here returns a dot product well We have to create a Python class And so if you haven't done Python OO before you're going to have to learn Because all pie torch modules are written in Python OO and it's one of the things I really like about pie torch is that it doesn't Reinvent totally new ways of doing things like tensorflow does all the time in pie torch They you know really tend to use Pythonic ways to do things so in this case How do you create you know some kind of new behavior you create a Python class? So Jeremy suppose that you have a lot of data Not just a little bit of data you can have in memory. Will you be able to use fast AI to solve color refiltering? Yes, absolutely It's it uses Mini batch stochastic gradient descent which does it a batch at a time the This particular version is going to create a Pandas data frame and a pandas data frame has to live in memory Having said that you can get easily five twelve gig You know instances on Amazon so like if you had a CSV that was bigger than 512 gig You know that would be impressive if that did happen I guess you would have to instead save that as a beak holes array and Create a slightly different version that reads from a beak holes array to streaming in or maybe from a desk data frame which also so It would be easy to do. I don't think I've seen real-world situations where you have 512 gigabyte of collaborative filtering matrices, but yeah We can do it Okay now This is pipe torch specific this next bit is that when you define like the actual work to be done, which is here return user times movie dot sum You have to put it in a special method called forward Okay, and this is this idea that like it's very likely you're pretty neural net right in a neural net the thing where you calculate the next Set of activations is called the forward pass and so that's doing a forward calculation The gradients is called the backward calculation We don't have to do that because pie torch calculates that automatically so we just have to define Forward right so we create a new class we define forward and here we write in our definition of dot product Okay, so that's it so now that we've created this class definition We can instantiate our model right and we can call our model and get back the numbers we expected Okay, so that's it. That's how we create a custom pie torch layer And if you compare that to like any other other Library around pretty much. This is way easier Basically, I guess because we're leveraging What's already in Python? So let's go ahead and now create a more complex Module and we're going to basically do the same thing. We're going to have a forward again We're going to have our users times movies dot sum But we're going to do one more thing beforehand, which is we're going to create two Embedding matrices, and then we're going to look up our users and our movies in those embedding matrices So let's go through and and do that so the first thing to realize is that The users the user IDs and the movie IDs may not be contiguous You know like they maybe they start at a million and go to a million one thousand say right so if we just used Those IDs directly to look up into an embedding matrix We would have to create an embedding matrix of size 1 million 1,000, right? Which we don't want to do so the first thing I do is to get a list of the unique user IDs and then I create a mapping from every user ID to a contiguous integer this thing I've done here where I've created a Dictionary which maps from every unique thing to a unique index is well worth studying During the week because like it's super super handy is something you very very often Have to do in all kinds of machine learning right and so I will go through it here It's easy enough to figure out if you can't figure it out. Just ask on the forum Anyway, so once we've got the mapping from user to a contiguous index we then can say Let's now replace the user ID column with that contiguous index Right so pandas dot apply applies an arbitrary function In Python lambda is how you create an anonymous function on the fly and this anonymous function simply returns The index do the same thing for movies and so after that we now have the same ratings table we had before but our IDs have been mapped to contiguous Integers and therefore there are things that we can look up into an embedding matrix So let's get the count of our users in our movies and let's now go ahead and try and create our Python version of this Okay, so Earlier on when we created our simplest possible PyTorch module there was no like State we didn't need a constructor Because we weren't like saying how many users are there or how many movies are there or how many factors do we want or whatever? right anytime we want to do something like This where we're passing in and saying we want to construct our Module with this number of users and this number of movies then we need a constructor For our class and you create a constructor in Python by defining a Dunder in it underscore underscore in it underscore underscore it special name, so this just creates a Constructor again if you haven't done no over for You wanted to do some study during the week, but it's a pretty simple idea This is just the thing that when we create this object. This is what gets around okay again special Python thing when you Create your own constructor you have to call the parent class constructor And if you want to have all of the cool behavior of a pipe watch module you get that by inheriting From an end module neural net module okay, so basically by inheriting here and calling the super class constructor We now have a fully functioning pie torch layer, okay, so now we have to give it some behavior and So we give it some behavior by storing some things in it right so here. We're going to create something called Self dot you users and that is going to be an embedding layer Number of rows is and users number of columns is n factors So that is exactly this right the number of rows is n users number of columns is n factors And then we'll have to do the same thing for movies All right, so that's going to go ahead and create these two randomly initialized arrays However when you randomly initialize over an array it's important to randomly initialize it to a Reasonable set of numbers like a reasonable scale If we randomly initialize them from like naught to a million Then we would start out and you know these things would start out being like You know billions and billions of size rating and that's going to be very hard to do gradient descent on So I just kind of manually figured here like okay about what size? Numbers and are going to give me about the right ratings And so we know we know we need ratings between about 0 and 5 So if we start out with stuff between about 0 and 0.05 then we're going to get ratings of about the right level You can easily enough like that calculate that in neural nets there are standard algorithms for basically doing doing that calculation and the basic the key algorithm is Something called her initialization from climbing her and the basic idea Is that you take the Here you basically set the weights equal to a normal distribution With a standard deviation which is basically inversely proportional to the number of things in the previous layer And so in our previous layer So in this case we basically If you basically take that naught to 0.05 and multiply it by the fact that you've got 40 things I wasn't 40 or 50 things coming out of it 50 50 things coming out of it, then you're going to get something of about the right size My torch has already has like her initialization Class there like we don't in normally in real life have to think about this we can just call the existing initialization Functions, but we're trying to do this all like from scratch here. Okay without any special stuff going on So there's quite a bit of pytorch notation here, so self dot you we've already set to an instance of the embedding class It has a dot weight Attribute which contains the actual the actual embedding matrix So that contains this The actual embedding matrix is not a tensor it's a variable a Variable is exactly the same as a tensor in other words it supports the exact same operations as a tensor But it also Does automatic differentiation? That's all a variable is basically To pull the tensor out of a variable you get its data attribute, okay, so this is so this is now the tensor of the weight matrix of the self dot you embedding and Then something that's really handy to know is that all of the tensor functions in pytorch You can stick an underscore at the end and that means do it in place Right so this is say create a random uniform random number of an appropriate size For this tensor and don't return it, but actually fill in that matrix In place okay, so that's a super handy thing to know about I mean it wouldn't be rocket science Otherwise we would have to have gone Okay There's the non in place version, that's what saves us some typing saves us some screen noise, that's all Okay So now we've got our randomly initialized embedding weight matrices And so now the forward I'm actually going to use the same columnar model data that we used for Rossman and so it's actually going to be passed both categorical variables and continuous variables And in this case there are no Continuous variables, so I'm just going to grab the Zero column out of the categorical variables and call it users and the first column and call it movies Okay, so I'm just kind of too lazy to create my own I'm not so much too lazy that we do have a special class for this But I'm trying to avoid creating a special class, so we're just going to leverage this columnar model data class Okay, so we can basically grab our user and movies Many batches right and remember. This is not a single user in a single movie This is going to be a whole mini batch of them We can now look up that mini batch of users in our embedding matrix you and the movies in our embedding matrix M right so this is like exactly the same as just doing an array lookup to grab the user ID numbered Value, but we're doing it a whole mini batch at a time Right and so it's because pytorch can do a whole mini batch at a time with pretty much everything that we can get really easy Speed up we don't have to write any loops on the whole to do everything through our mini batch And in fact if you do ever loop through your mini batch manually you don't get GPU acceleration That's really important to know right so you never want to loop Have a for loop going through your mini batch you always want to do things in this kind of like whole mini batch at a time But pretty much everything in pytorch does things a whole mini batch at a time, so you shouldn't have to worry about it And then here's our dot product just like before all right so having defined That I'm now going to Go ahead and say all right my x values is Everything except the rating and the timestamp in my ratings table my y is my rating and then I can just say okay, let's Grab a model data from a data frame I'm using that X and that Y and here is our list of categorical variables Okay And then so let's now instantiate That pytorch object alright, so we've now created that from scratch And then the next thing we need to do is to create an optimizer, so this is part of pytorch The only fast AI thing here is this line right because I don't think showing you How to build data sets and data loaders is interesting enough really we might do that in part two of the course And it's actually so straightforward like a lot of you are already doing it on the forums So I'm not going to show you that in this part I mean if you're interested feel free to talk on the forums about it But I'm just going to basically take the the thing that feeds us data as a given particularly because these things are so flexible Right you know if you've got stuff in a data frame. You can just use this you don't have to rewrite it So that's the only fast AI thing we're using so this is a pytorch thing and so Optim is the thing in pytorch that gives us an optimizer. We'll be learning about that very shortly So it's actually the thing that's going to update our weights pytorch Calls them the parameters of the model so earlier on we said model equals embedding dot blah blah blah right and because embedding dot Derives from nn dot module we get all of the pytorch module behavior and one of the things we got for free Is the ability to say dot parameters? So that's pretty that's pretty handy right that's the thing that basically is going to automatically Give us a list of all of the weights in our model that have to be updated and so that's what gets passed to the optimizer We also passed the optimizer the learning rate The weight decay which we'll talk about later and momentum that we'll talk about later Okay one other thing that I'm not going to do right now but we will do later is to write a training loop so the training loop is a thing that loops through each mini-batch and Updates the weight to subtract the gradient times the length right? There's a function in fast AI which is the training loop and it's It's It's pretty simple Here it is right for epoch in epochs This is just the thing that shows a progress bar so ignore this for X comma Y in my training data loader calculate the loss Print out the loss in our in our progress bar Call any callbacks you have and at the end Call the call the metrics on the validation right so this is just for each epoch go through each mini-batch And do one step of our optimizer step is Basically going to take advantage of this optimizer, but we'll be writing that from scratch shortly So this is what notice we're not using a learner Okay, we're just using a pipe watch module so this this fit thing although. It's past it part of fast AI It's like lower down the layers of abstraction now. This is the thing that takes a regular high torch model, so if you ever want to like skip as much Fast AI stuff as possible like you've got some high torch model. You've got some code on the internet you basically want to run it But you don't want to write your own training loop Then this is this is what you want to do you want to call fast AI's fit function? And so what you'll find is like the library is designed so that you can kind of dig in at any layer of abstraction You like right and so at this layer of abstraction you're not going to get things like Stochastic gradient descent with restarts. You're not going to get like differential learning rates like all that stuff That's in the learner like you could do it, but you'd have to write it all by by hand yourself Right and that's the downside of kind of going down to this level of abstraction The upside is that as you saw the code for this is very simple. It's just a simple training loop It takes a standard high torch model So this is like this is a good thing for us to use here We can we just call it and it looks exactly like what we're we're used to saying right we get our validation and training loss for the three E plus now you'll notice that We wanted something around point seven six So we're not there so in other words the the default fast AI collaborative filtering algorithm is doing something smarter than this So we're going to try and do that One thing that we can do since we're calling our you know this lower level fit function There's no learning rate annealing we could do our own learning rate annealing so you can hear it see here There's a fast AI function called set learning rates You can pass in a standard high torch optimizer and pass in your new learning rate and then call fit again And so this is how we can like manually do a learning rate schedule And so you can see we've got a little bit better 1.13 We still got a long way to go Okay so I think what we might do is we might have a Seven minute break, and then we're going to come back and try and improve this score a bit For Those who are interested somebody was asking me at the break for a kind of a quick Walk through so this is totally optional, but if you go into the fast AI library. There's a model.py file and And that's where fit is which we're just looking at which goes through each epoch in Epochs and then goes through each X and Y in the mini batch and then it calls this Step function so the step function Is Here and you can see the key thing is it calculates the output from the model models quarter m right and so if you remember our dot product We didn't actually call model dot forward We just called model parentheses, and that's because the NN dot module Automatically you know when you call it as if it's a function it passes it along to forward okay So that's that's what that's doing there, right and then the rest of this will learn about shortly. She's basically doing the the loss function and then the Backward pass okay, so for those who are interested That's that's kind of gives you a bit of a sense of how the code is structured if you want to look at it and as I say like the the fast AI code is designed to both be world-class performance, but also Pretty easy to read so like feel free like take a look at it And if you want to know what's going on just ask on the forums and if you know if you think there's anything that could be clearer Let us know Because yeah, the code is definitely you know we're going to be digging into the code or and more Okay, so let's try and improve this a little bit and let's start off by improving it in Excel So you might have noticed here that we've kind of got the idea that user 72 You know like sci-fi modern movies with special effects You know whatever and movie number 27 is sci-fi and has special effects and not much dialogue But we're missing an important case which is like user 72 is Pretty enthusiastic on the whole and on average rates things highly highly you know and movie 27 You know it's just a popular movie You know which just on average it's higher so what we'd really like is to add a Constant for the user and a constant for the movie and remember in neural network terms we call that a bias That's where you want to add a bias So we could easily do that and if we go into the bias tab here We've got the same data as before and we've got the same Latent factors as before and I've just got one extra Row here and one extra column here, and you won't be surprised here that we now Take the same matrix multiplication as before and we add in That and we add in that That Okay, so that's our bias So other than that we've got exactly the same loss function over here And so just like before we can now go ahead and solve that and now our changing variables include the Bias and we can say solve and if we leave that for a little while it will come to a better result than we had before Okay, so that's the first thing we're going to do to improve our model and There's really very little to show Just to make the code a bit shorter I've defined a function called get embedding which takes a number of inputs and a number of factors so the number of rows and the embedding matrix and I'm positive but creates the embedding and then Randomly initializes it. I don't know why I'm doing negative to positive here and zero last time Honestly, it doesn't matter much as long as it's in the right ballpark And then we return that initialized embedding So now we need not just our users by factors, which are chuck into you Movies by factors which are chuck into M, but we also need users by one Which will put into you be user bias and movies by one which will put into movie bias, okay So this is just doing a list comprehension going through each of the tuples creating embedding for each of them and putting them into These things okay, so now our forward is exactly the same as before You times M dot sum This is actually a little confusing because we're doing it in two two steps Maybe to make it a bit easier. Let's pull this out put it up here Put this in parentheses Okay, so maybe that looks a little bit more familiar All right you times M dot sum that's the same dot product and then here we're just going to add in our user bias and our movie bias Dot squeeze is The pie torch thing that adds an additional unit axis on That's not going to make any sense if you haven't done broadcasting before I'm not going to do broadcasting in this course because we've already done it and we're doing it in the machine learning course But basically in short Broadcasting is what happens when you do something like this where UM is a matrix You be self dot you be users is a is a vector How do you add a vector to a matrix and basically what it does is it duplicates? the vector So that it makes it the same size as the matrix and the particular way whether it duplicates it across columns or down rows Or how it does it is called broadcasting the broadcasting rules are the same as numpy I thought it didn't actually used to support broadcasting so I was actually the guy who first added Broadcasting to pie torch using an ugly hack and then the pie torch authors did an awesome job of supporting it Actually inside the language so now you can use the same Broadcasting operations in pie torches numpy if you haven't dealt with this before it's really important to learn it Because like it's it's kind of the most important fundamental way to do computations Quickly in numpy and pie torture. It's the thing that lets you not have to do loops I could you imagine here if I had to loop through every row of this matrix and add each You know this vector to every row it would be slow. It would be you know a lot more code And the idea of broadcasting it actually goes all the way back to APL which was a language designed in the 50s by an extraordinary guy called Ken Iverson APL was originally a designed or written out as a new type of mathematical notation. He has this great essay called notation as a tool for thought and the idea was that like really good notation could actually make you Think of better things and part of that notation is this idea of forecasting I'm incredibly enthusiastic about it, and we're going to use it plenty so either watch the machine learning lesson or You know Google numpy broadcasting for information Anyway, so basically it works reasonably intuitively we can add on we can add the Vectors to the matrix All right Having done that we're now going to do one more trick which is I think it was your net asked earlier about could we Squish the ratings to be between 1 and 5 and the answer is We could right and specifically what we could do is We could Put it through a sigmoid function All right, so to remind you a sigmoid function Looks like that right and this is that's one Right we could put it through a sigmoid function So we could take like four point nine six and put it through a sigmoid function and like that You know that's kind of high so it kind of be over here somewhere, right? And then we could multiply that sigmoid like the result of that by five For example right and in this case we want it to be between 1 and 5 right so maybe we might multiply it by 4 And add 1 instance. That's a basic idea and So here is that trick we take the result so the result is basically the thing that comes straight out of the Dot product plus the addition of the biases and put it through a sigmoid function now in pytorch Basically all of the functions you can do the tensors are available inside This thing called capital F, and this is like totally standard in pytorch It's actually called torch dot and end up functional, but everybody including all of the pytorch docs Import torch dot and end up functional as capital F All right, so capital F dot sigmoid means a function called sigmoid that is coming from torches Functional module right and so that's going to apply a sigmoid function to the result So squish them all between 0 and 1 using that nice little shape and then I can multiply that by 5 minus 1 plus 4 Right and then add on 1 and that's going to give me something between 1 and 5 okay, so Like there's no need to do this I could comment it out, and it will still work Right but now it has to come up with a set of calculations that are always between 1 and 5 right where else if I leave this in then it's like makes it really easy It's basically like oh if you think this is a really good movie just calculate a really high number It's a really crappy movie capital really low number, and I'll make sure it's in the right region So even though this isn't a neural network It's still a good example of this kind of like if you're doing any kind of parameter fitting Try and make it so that the thing that you want your function to return It's like it's easy for it to return that okay, so that's why we do that that function squishing So we call this embedding dot bias So we can create that in the same way as before You'll see here I'm calling dot CUDA to put it on the GPU because we're not using any learner stuff normally that'll all happen for you But we have to manually say put it on the GPU This is the same as before create our optimizer Fit exactly the same as before and these numbers are looking good alright and again. We'll do a little Change to our learning rate and learning rate schedule, and we're down to point eight, so we're actually pretty close pretty close so So that's the key steps and This is how This is how most collaborative filtering is done and You're not reminded me of an important point which is that this is not strictly speaking a Matrix factorization because strictly speaking a matrix factorization would take that matrix by that matrix to create this matrix and remembering Anywhere that this is empty like here or here We're putting in a zero Right we're saying if the original was empty put in a zero Right now normally You can't do that with normal matrix factorization with normal matrix factorization it creates the whole Matrix and so it was a real problem actually When people used to try and use traditional linear algebra for this because when you have these sparse matrices like in practice This matrix is not doesn't have many gaps because we picked the users that watch the most movies And the movies that are the most watched but if you look at the whole matrix It's it's mainly empty and so traditional techniques treated empty as zero And so like you basically have to predict a zero as if the fact that I haven't watched a movie means I don't like the movie that gives terrible answers So this probabilistic matrix factorization approach Takes advantage of the fact that our data structure Actually looks like this Rather than that crosstab right and so it's only calculating the loss for the user ID movie ID combinations that actually appear That's it's actually like user ID one movie ID one or two nine should be three It's actually three and a half so our loss is point five like there's nothing here. That's ever going to calculate a Prediction or a loss for a user movie combination that doesn't appear in this table But by definition the only stuff that we can appear in a mini batch is what's in this table And like a lot of this happened interestingly enough actually in the Netflix prize So before the Netflix prize came along This probabilistic matrix factorization it had actually already been invented, but nobody noticed All right, and then in the first year of the Netflix prize someone wrote this like really really famous blog post But it basically said like hey check this out Incredibly simple technique works incredibly well and suddenly all the Netflix leaderboard entries were like much much better And so you know that's quite a few years ago now, and this is like now Every collaborative filtering approach does this not every collaborative filtering approach adds this sigmoid thing by the way. It's not like Rocket science this is this is not like the NLP thing we saw last week Which is like hey, this is a new state-of-the-art like this is you know not particularly uncommon But there are still people that don't do this and it definitely helps a lot right to have this and so Actually, you know what we could do is maybe now's a good time to have a look at the definition of this right so the column data module Contains all these definitions and we can now compare this to the thing we originally used which was Whatever came out of collab filter data set right so let's go to Co lab Filter data set here it is and we called Get learner all right so we can go down to get learner and that created a collab filter learner Passing in the model from get model is get model so it created an embedding bias and So here is embedding bias and You can see here here. It is like it's the same thing. There's the embedding for each of the things Here's our forward that does the u times i dot sum plus plus sigmoid so in fact We have just actually rebuilt What's in the fast air library literally okay? It's a little shorter and easier because we're taking advantage of the fact that there's a special collaborative filtering data set So we can actually we're getting passed in the users and the items and we don't have to pull them out of cats and cons But other than that this is exactly the same so hopefully you can see like the fast AI library is not some inscrutable code Containing concepts you can never understand we've actually just built up this entire thing from scratch ourselves And so why did we get? 0.76 rather than 0.8 You know I think it's simply because we used a stochastic gradient descent with restarts and a cycle multiplier and an atom optimizer You know like a few little training tricks So I'm looking at this and thinking that is we could totally improve this model, but maybe Looking at the date and doing some tricks with the date because this is kind of a just a regular Kind of model in a way yeah, you can add more features. Yeah, exactly exactly so like now that you've seen this You could now you know even if you didn't have Embedding bias in a notebook that you've written yourself some other model That's in fast AI you could look at it in fast AI and be like oh That does most of the things that I'd want to do, but it doesn't deal with time and so you could just go Oh, okay, let's grab it copy it. You know pop it into my notebook and Let's create you know the better version Right and then you can start playing right and you can now create your own model class from the open source code here and so Yeah, you know that's mentioning a couple things we could do we could try incorporating time stamps, so we could assume that maybe Well, maybe there's just like some For a particular user over time users tend to get more or less positive about movies Also remember there was the list of genres for each movie. Maybe we could incorporate that So one problem is it's a little bit difficult to incorporate that stuff Into this embedding bias model because it's kind of it's pretty custom right so what we're going to do next is we're going to try to create a Neural net version of this right so the basic idea here is We're going to Take exactly the same thing as we had before here's our list of users right and here is our embeddings right and here's our list of movies and here is our Embeddings right and so as you can see I've just kind of transposed The movie ones so that so that they're all in the same orientation and Here is our user movie rating But D crosstabbed okay, so in the original format so each row is a user movie rating Okay So the first thing I do is I need to replace user 14 with that users Contiguous index right and so I can do that in Excel Using this match that basically says what you know how far down this list you have to go and it said User 14 was the first thing in that list okay user 29 was the second thing in that list so forth okay? So this is the same as that thing that we did In our Python code where we basically created a dictionary to map this stuff So now we can for this particular user movie rating Combination we can look up the appropriate embedding Right and so you can see here what it's doing is it's saying all right. Let's basically offset From the start of this list and the number of rows we're going to go down is equal to the user index And the number of columns we're going to go across is One two three four or five okay, and so you can see what it does is it creates point one nine point six three point three one Here it is point one nine point three okay, so so this is literally Modern embedding does but remember this is exactly the same as doing a one hot encoding right because if instead this was a vector Containing one zero zero zero zero zero right and we multiplied that by this matrix Then the only row it's going to return Would be the first one okay, so So it's really useful to remember that embedding actually just is a matrix product The only reason it exists the only reason it exists is because this is an optimization You know this lets pytorch know like okay This is just a matrix multiply, but I guarantee you that you know this thing is one hot encoded Therefore you don't have to actually do the matrix multiply you can just do a direct lookup Okay, so that's literally all an embedding is is it is a computational performance Thing for a particular kind of matrix multiply all right so that looks up that uses user And then we can look up that users movie right so here is movie ID Movie ID 417 which apparently is index number 14 here it is here, so it should have been point seven five point four seven Yes, it is point seven five point four seven okay, so we've now got the user embedding and the movie embedding and rather than doing a dot product of Those two Right which is what we do normally Instead what if we concatenate the two together into a single vector of length Ten and then feed that into a neural net Right and so anytime we've got you know a a tensor of input activations or in this case a tensor of Actually, this is a tensor of output activations. This is coming out of an embedding layer We can chuck it in a neural net because neural nets we now know can calculate anything Okay, including hopefully collaborative filtering, so let's try that So here is our embedding net so This time I have not bothered to create a separate Bias because instead the linear layer in pie torch Already has a bias in it right so when we go and in dot linear, right? Let's kind of draw this out So we've got our you Matrix right and this is the number of users and This is the number of factors right and we've got our M matrix Right so here's our number of movies, and here's our again number of factors and so remember we look up a Single user We look up a single movie and let's grab them and concatenate them together All right, so here's like the user part. Here's the movie part, and then let's put that Through a matrix product All right, so that number of rows here is going to have to be the number of users plus the number of movies Because that's how long that is and Then the number of columns Can be anything we want? Because we're going to take that so in this case we're going to pick 10 apparently so it's picked 10 and Then we're going to stick that through a value And then stick that through another Matrix which obviously needs to be of size 10 here, and then the number of columns is of size one Because we want to predict a single rating okay? And so that's our kind of flow chart of what's going on right it is a standard I would call it a one hidden layer neural net it depends how you think of it like there's kind of an embedding layer But because this is linear, and this is linear the two together is really one linear layer right? It's just a computational convenience So it's really got one hidden layer because it's just got one layer before this nonlinear activation So in order to create a Linear layer with some number of rows and some number of columns you just go and end up linear In the machine learning class this week We learned how to create a linear layer from scratch by creating our own weight matrix and our own biases So if you want to check that out you can do so there right, but it's the same basic technique. We've already seen So we create our embeddings we create our two linear layers That's all the stuff that we need to start with You know really if I wanted to make this more general. I would have had another parameter here called like num hidden You know equals Equals 10, and then this would be a parameter And then you could like more easily play around with different numbers of activations So when we say like okay in this layer, I'm going to create a layer with this many activations all I mean Assuming it's a fully connected layer is My linear layer has how many columns in its white matrix. That's how many activations it creates All right, so we grab our users and movies we put them through our embedding matrix, and then we can catenate them together Okay, so torch dot cat Concatenates them together on the first dimension so in other words we can catenate the columns together to create longer rows Okay, so that's concatenating on dimension one Dropout will come back to in a moment. We've looked at that briefly So then having done that we'll put it through that linear layer we had We'll do our value and you'll notice that value is again inside our capital F And end up functional right is just a function so remember Activation functions are basically things that take one activation in and spit one activation out in this case taking something that can have negatives or positives and Truncate the negatives to zero that's all really does And then here's our sigmoid So that's that that is now a genuine Neural network, I don't know if we get to call it deep It's only got one hidden layer, but it's definitely a neural network right and so we can now construct it We can put it on the GPU We can create an optimizer for it, and we can fit it Now you'll notice there's one other thing. I've been passing to fit which is What loss function are we trying to minimize? Okay, and this is the mean squared error loss and again. It's inside F Okay, pretty much all the functions are inside it okay so One of the things that you have to pass fit is something saying like how do you score this what counts as good or bad? So Jeremy now that we have a real neural net do we have to use the same number of embeddings for users and That's a great question. You don't know it's absolutely right you don't and so like we've got a lot of benefits here right because if we You know think about you Know We're grabbing a user Embedding or concatenating it with a movie embedding which maybe is like I don't know some different size But then also perhaps we looked up the genre of the movie and like you know there's actually a Embedding matrix of like number of genres by I don't know three or something and so like we could then concatenate like a genre embedding and Then maybe the timestamp is in here as a continuous number right and so then that whole thing we can then feed into You know Our neural net right and then at the end Remember our final non-linearity with a sigmoid right so we can now recognize that this thing We did where we did sigmoid times max rating minus min rating plus blah blah blah Is actually just another? non-linear Activation function right and remember in our last layer We use generally different kinds of activation functions So as we said we don't need any activation function at all right we could just do That right but by not having any nonlinear activation function We're just making it harder, so that's why we put the sigmoid in there as well, okay? so we can then fit it in the usual way and There we go you know interestingly we actually got a better score than we did with our This model So it'll be interesting to try training this with stochastic gradient descent with restarts and see if it's actually better You know maybe you can play around with the number of hidden layers and the dropout and whatever else and see if you can come up with you know get a better answer than Point Seven six ish Okay, so so general so this is like if you were going Deep into collaborative filtering at your workplace or whatever this wouldn't be a bad way to go I could like I'd start out with like oh, okay. Here's like a club for the data set 30 and fast AI Get learner. There's you know not much I can send it basically number of factors is about the only thing that I pass in I can learn for a while Maybe try a few different approaches, and then you're like okay. There's like That's how I go if I use the defaults Okay How do I make it better and then I'd be like digging into the code and saying like okay What would Jeremy actually do here? This is actually what I want you know and So one of the nice things about the neural net approach Is that you know as you net mentioned? We can have different numbers of embeddings We can choose how many hidden and we can also choose dropout right so So what we're actually doing is we haven't just got really you that we're also going like okay, let's Let's delete a few things at random Right that's dropout. That's when this case we were deleting After the first linear layer 75% of them Right and then after the second linear layer 75% of them so we can add a whole lot of regularization Yeah, so you know this it kind of feels like the this this embedding net You know you could you could change this again we could like have it so that we could pass into the constructor Well if you want to make it look as much as possible like what we had before we could pass in peas peas equals 0.75 0.75 I'm not sure this is the best API, but it's not terrible Probably what since we've only got exactly two layers we could say P1 equals 0.75 Five P two equals point five and So then this will be P1 this will be P2 You know where we go and like if you wanted to go further You could make it look more like our Structured data learner you could actually have a thing this number of hidden You know maybe you could make a list and so then rather than creating exactly one Hidden layer and one output layer this could be a little loop that creates And hidden layers each one of the size you want so like this is all stuff you can play with during the during the week If you want to and I feel like if you've got like a much smaller collaborative filtering data set You know maybe you'd need like more regularization or whatever. It's a much bigger one Maybe more layers would help. I don't know you know I haven't seen Much discussion of this kind of neural network approach to collaborative filtering, but I'm not a collaborative filtering expert So maybe it's maybe it's around, but that'd be interesting thing to try So the next thing I wanted to do Was to talk about? The training loop so what's actually happening? inside the training loop So at the moment we're basically passing off the actual updating of the weights to pie torches optimizer But what I want to do is like understand What that optimizer is is actually doing and we're also I also want to understand what this momentum term is doing so You'll find we have a Spreadsheet called grad desk gradient descent And it's kind of designed to be read left to right sorry right to left worksheet wise So the rightmost worksheet Is some data right and we're going to implement gradient descent in excel because obviously Everybody wants to do deep learning in itself, and we've done collaborative filtering in Excel. We've done Convolutions in Excel so now we need SGD in Excel so we can replace Python once and for all okay, so Let's start by creating some data right and so here's You know here's some Independent you know I've got one column of X's you know and one column Of Y's and these are actually directly linearly related, so this is this is random right and this one here is equal to X times 2 plus 30 Okay, so Let's try and use Excel to take that data and try and learn those parameters Okay, that's going to be a call So let's start with the most basic version Of SGD and so the first thing I'm going to do is I'm going to run a macro So you can see what this looks like so I hit run and it does five epochs I do another five epochs another five epochs Okay, so The first one was pretty terrible. It's hard to see so I just delete that first one get better scaling all right So you can see actually it's pretty constantly improving the loss right this is the loss per epoch All right, so how do we do that so let's reset it? So here is my X's and my Y's and And what I do is I start out by assuming some intercept and some slope right so this is my Randomly initialized weights, so I have randomly initialized them both to one You could pick a different random number if you like, but I promise that I randomly picked the number one Twice there you go It was a random number between one and one so Here is my intercept and slope. I'm just going to copy them over here right so you can literally see this is just equal C1 Here is equals C2 okay, so I'm going to start with my very first row of data X equals 14 Y equals 58 and my goal is to come up After I look at this piece of data. I want to come up with a slightly better intercept and a slightly better slope Okay So to do that I need to first of all basically figure out Which direction is is down in other words if I make my intercept a little bit higher? Or a little bit lower would it make my error a little bit better a little bit worse So let's start out by calculating the error so to calculate the error the first thing we need is a prediction So the prediction is equal to the intercept plus X times flow So that is our Zero hidden layer neural network okay? And so here is our error. It's equal to our prediction minus our actual square So we could like play around with this. I don't want my error to be 1849. I'd like it to be lower So what if we set the intercept to? 1.1 1849 because to 1840 okay, so a higher intercept would be better Okay, what about the slope if I increase that? It goes from 1849 To 1730 okay a higher slope would be better as well Not surprising because we know Actually that they should be 30 and 2 So one way to figure that out You know in code and especially it is to do literally what I just did is to add a little bit to the intercept And the slope and see what happens and that's called finding the derivative through finite differencing Right and so let's go ahead and do that So here is the value of my error if I add 0.01 to My intercept right so it's c4 plus open 0.1, and then I just put that into my linear function, and then I subtract my actual all squared Right and so that causes my error to go down a bit that's increasing my Which one is that increasing c4 increasing the intercept a little bit has caused my error to go down So what's the derivative well the derivative is equal to how much the dependent variable changed by? Divided by how much the independent variable changed by right and so there it is right our Dependent variable changed by that minus that Right and our independent variable we changed by point. Oh one So there is the estimated value of? the error DB All right, so remember when people talking about derivatives This is this is all they're doing is they're saying what's this value, but as we make this number Smaller and smaller and smaller and smaller as it limits to zero I'm not smart enough to think in terms of like derivatives and integrals and stuff like that so whenever I think about this I always think about you know an actual like plus 0.01 divided by 0.01 because like I just find that easier Just like I never think about probability density functions. I always think about actual probabilities. I like toss a coin Something happens three times You know so I always think like remember. It's it's totally fair to do this because a computer is Discrete it's not continuous like a computer can't do anything infinitely small anyway, right? So it's actually got to be calculating things at some level of precision right and our brains kind of need that as well So this is like my version of Jeffrey Hinton's like to visualize things in more than two dimensions You just like say 12 dimensions really quickly while visualizing it in two dimensions This is my equivalent you know to to think about derivatives just think about Division and like although all the mathematicians say no you can't do that You actually can like if you think of DXTY as being literally you know change in X over change in Y like The division actually like the calculations still work like all the time so Okay, so let's do the same thing now with changing My slope by a little bit and so here's the same thing right and so you can see both of these are negative Okay, so that's saying if I increase my intercept my loss goes down if I increase my slope My loss goes down right and so my derivative of my error with respect to my slope is is actually pretty high and that's not surprising because It's actually You know the constant term is just being added whereas the slope is being multiplied by 40 Okay now Finite differencing is all very well and good, but there's a big problem with finite differencing in in high dimensional spaces and the problem is this right and this is like You don't need to learn How to calculate derivatives or integrals, but you need to learn how to think about them spatially right and so remember We have some Vector very high dimensional vector. It's got like a million items in it right and It's going through Some weight matrix right of size like 1 million by size 100,000 Or whatever and it's spitting out something of size 100,000 and So you need to realize like there isn't like a gradient here But it's like for every one of these things in this vector Right there's a gradient in every direction. You know in every part of the output Right so it actually has Not a single gradient number not even a gradient Vector but a gradient matrix And so this This is a lot to calculate right I would literally have to like add a little bit to this and see what happens to all Of these add a little bit to this see what happens to all of these right to fill in one column of this at a time, so that's going to be Horrendously slow like that that so that's why like if you're ever thinking like oh we can just do this with finite differencing Just remember like okay. We we're dealing in the with these very high dimensional vectors where You know this this kind of Matrix calculus like all the concepts are identical But when you actually draw it out like this you suddenly realize like okay for each number I could change There's a whole bunch of numbers that impacts, and I have this whole matrix of things to compute right and so Your gradient calculations can take up a lot of memory and they can take up a lot of time So we want to find some way to do this more quickly, okay? And it's definitely well worth like spending time kind of studying these ideas of like You know the idea of like the gradients like look up things like Jacobian and Hessian They're the things that you want to search for to start Unfortunately people normally write about them with you know lots of Greek letters and Blah blah blah is right, but there are some there are some nice You know intuitive explanations out there, and hopefully you can share them on the forum if you find them because this is stuff You really need to Really need to understand in here You know because you're trying to train something and it's not working properly and like later on we'll learn how to like look inside Pi torch to like actually get the values of the gradients, and you need to know like okay. Well how would I like plot the gradients? You know what would I consider unusual like you know these are the things that turn you into a really awesome Deep learning practitioner is when you can like debug your problems by like grabbing the gradients and doing histograms of them and like knowing You know that you could like plot that all each layer my average gradients getting worse, or you know bigger or you know whatever Okay, so the trick to doing this more quickly is to do it analytically rather than through finite differencing and So analytically is basically there is a list you probably all learned it at high school There is a literally a list of rules that for every Mathematical function there's a like this is the derivative of that function right so You probably remember a few of them for example X squared 2 X right and so we actually have here an x squared So here is our 2 times right now the one that I actually want you to know is Not any of the individual rules, but I want you to know the chain rule right which is You've got some function Of some function of something why is this important? I? Don't know that's a linear layer. That's a relu right and Then we can kind of keep going backwards Etc but a neural net Is just a function of a function of a function of a function where the innermost is you know it's basically linear relu linear relu dot dot dot linear sigmoid or softmax All right, and so it's a function of a function of a function and so therefore to calculate the derivative of the weights in your model The loss of your model with respect to the weights of your model you're going to need to use the chain rule and Specifically whatever layer it is that you're up to like I want to calculate the derivative here I'm going to need to use all of these All of these ones because that's all that's that's the function that's being applied right, and that's why they call this back propagation because the value of the derivative of that is equal to that derivative Now basically you can do it like this you can say let's call you Is this right let's call that you right then it's simply equal to The derivative of that times Derivative of that right you just multiply them together and So that's what back propagation is like it's not that back propagation is a new thing for you to learn It's not a new algorithm It is literally Take the derivative of every one of your layers and Multiply them all together So like it doesn't deserve a new name Right apply the chain rule to my layers Does not deserve a new lame, but it gets one because us neural networks folk really need to seem as clever as possible It's really important, but everybody else thinks that we are way outside of their capabilities Right so the fact that you're here means that we failed because you guys somehow think that you're capable right so remember It's really important when you talk to other people that you say back propagation and rectified linear unit rather than like multiply the layers gradients or Replace negatives with zeros okay, so so here we go so here is so I've just gone ahead and Grabbed the derivative unfortunately there is no automatic differentiation in Excel yet So I did the alternative which is to paste the formula into wolfram alpha and got back the derivatives So there's the first derivative, and there's the second derivative Analytically we only have one layer in this Infinite tiny small neural network, so we don't have to worry about the chain rule And we should see that this analytical derivative is pretty close to our estimated derivative from the finite differencing and Indeed it is right and we should see that these ones are pretty similar as well And indeed they are right and if you're you know back when I? Implemented my own neural nets 20 years ago I you know had to actually calculate the derivatives and so I always would write like had something that would check the Derivatives using finite differencing and so for those poor people that do have to write these things by hand You'll still see that they have like a finite differencing checker, so if you ever do have to implement a derivative by hand Please make sure that you Have a finite differencing checker so that you can test it All right So there's our derivatives So we know that if we increase B, then we're going to get a slightly better loss So let's increase B by a bit how much should we increase it by? Well, we'll increase it by some multiple of this and the multiple we're going to choose is called a learning rate And so here's our learning rate, so here's 1e neg 4 Okay, so our new value Is equal to whatever it was before Minus our Derivative times our learning rate okay, so we've gone from 1 to 1 point 0 1 and then a We've done the same thing so it's gone from 1 to 1 point 1 2 So this is a special kind of mini batch. It's a mini batch of size 1 okay, so we call this online gradient descent It just means mini batch of size 1 so then we can go on to the next one X is 86 Y is 202 right this is my intercept and slope copied across from the last row Okay So here's my new y prediction. Here's my new error Here are my derivatives Here are my new a and b All right, so we keep doing that for every mini batch of one and two eventually We react run out the end of the knee pop Okay, and so then at the end of an epoch we would grab our intercept and slope and Paste them back over here as our new values There we are and we can now continue again right so we're now starting with oops you guys either in the wrong spot It should be paste special transpose values All right Okay, so there's our new intercept. There's only slow possibly. I've got those the wrong way around But anyway you get the idea and then we continue okay, so I recorded the world's tiniest macro which literally just Copies the final slope and puts it into the new slope copies the final intercept puts it into the new intercept and does that Five times and after each time it grabs the root means greater error and pastes it into the next Spare area and that is attached to this run button and so it's going to go ahead and do that five times Okay So that's stochastic gradient descent in Excel So it to turn this into a CNN right you would just replace This error function right and therefore this prediction with the output of that convolutional example spreadsheet Okay, and that then would be in a CNN being trained with with SGD Okay Now the problem is that you'll see When I run this It's kind of going very slowly right we know that we need to get to a slope of 2 and an intercept of 30 And you can kind of see at this rate It's going to take a very long time Right and specifically It's like it keeps going the same direction So it's like come on take a hint. That's a good direction So the come on take a hint. That's a good direction. Please keep doing that but more is called momentum Right so on our next spreadsheet We're going to implement momentum Okay, so What momentum does is? The same thing and what to simplify this spreadsheet I've removed the finite differencing columns Okay, other than that this is just the same right so we still got our X's our Y's A's and B's our predictions Our error is now over here Okay And here's our derivatives, okay? our new calculation for let's grab a particular row Our new calculation here for our new a term just like before is it's equal to whatever a was before Minus Okay now this time I'm not taking the derivative, but I'm minusing some other number Times the line rate, so what's this other number? Okay, so this other number is equal to the derivative Times What's this K 1? 0.02 plus 0.98 times The thing just above it Okay, so this is a linear interpolation between this rows derivative or this mini batches derivative and Whatever direction we went last time All right, so in other words keep going the same direction as you were before Right then update it a little bit right and so in our Spread in our Python just before we had a momentum of point nine Okay, so you can see what tends to happen is that our Negative kind of gets more and more negative right all the way up to like 2000 Whereas with our standard SGD approach Our derivatives are kind of all over the place Right sometimes there's 700 sometimes negative 700 positive 100 You know so this is basically saying like yeah, if you've been going down For quite a while keep doing that until finally here. It's like okay That's that seems to be far enough so that's getting less and less and less negative Right and still we start going positive again, so you can kind of see why it's caught momentum It's like once you start traveling in a particular direction for a particular weight You kind of the wheel starts spinning and then once the gradient turns around the other way. It's like oh slow down We've got this kind of a momentum and then finally turn back around right so when we do it this way All right, we can do exactly the same thing right and after five iterations. We're at 89 Where else before? after five iterations, we're at 104 right and After a few more let's do maybe 15 okay? Seconds it's 102 for us here It's going right, so it's it's it's a bit better. It's not hips better. You can still see like these numbers. They're not Zipping along right, but it's definitely an improvement, and it also gives us something else to tune Which is nice like so if this is kind of a well-behaved error surface right in other words like Although it might be bumpy along the way there's kind of some overall Direction like imagine you're going down a hill right and there's like bumps on it right so the moment momentum you get up We're going to skipping over the tops right so we could say like okay. Let's increase our beta up to 0.98 Right and see if that like allows us to train a little faster and well look at that suddenly went straight to 82 Right so one nice thing about things like momentum is it's like another parameter that you can chew to try and make your model Train better in practice Basically everybody does this every like you look at any like image net winner or whatever they all use momentum Okay and so Back over here when we said use SGD that basically means use the the basic tab of our excel spreadsheet But then momentum equals point nine means add in Put a point nine over here, okay? And so that that's kind of your like default starting point So let's keep going and talk about Adam So Adam is something which I Actually was not right earlier on in this course. I said we've been using Adam by default. We actually haven't we've actually been I've noticed We've actually been using SGD with momentum by default and the reason is that Adam Has had it's much faster as you'll see it's much much faster to learn with but there's been some problems Which is people haven't been getting quite as good like final answers with Adam as they have with SGD with momentum And that's why you'll see like all the you know image net winning solutions and so forth and all the academic papers always use SGD with momentum and Adam seems to be a particular problem in NLP people really haven't got Adam working at all well The good news is this was I built it looks like this was solved two weeks ago It basically it turned out That the way people were dealing with a combination of weight decay and Adam had a nasty kind of bug in it basically and that's that's kind of carried through to every single library and one of our students and then Sahara has actually just Completed a prototype of adding is this new version of Adam is called Adam W into fast AI And he's confirmed that he's getting the much faster both the faster Performance and also the the better accuracy so hopefully we'll have this Adam W in fast AI Ideally before next week. We'll see how we go very very soon so so it is worth telling you about about Adam So let's talk about it. It's actually incredibly simple But again, you know make sure you make it sound really complicated when you tell people so that you can look like So here's the same spreadsheet again right and here's our Randomly selected a and B again somehow it's still one is a prediction is our derivatives Okay, so now how we calculating our new a and our new B You can immediately see it's looking pretty hopeful because even by like row 10 We're like we're seeing the numbers move a lot more right so this is looking pretty encouraging So how are we calculating this? It's equal to our previous value of B Minus j8 okay, so we're going to have to find out what that is times our learning rate Divided by the square root of L8 okay, so we're going to have to dig it and see what's going on One thing to notice here is that my learning rate is way higher than it used to be But then we're dividing it by this Big number okay, so let's start out by looking and seeing what this j8 thing is Okay j8 is identical to what we had before j8 is equal to the linear interpolation of the derivative and the previous direction Okay, so that was easy So one part of Adam is to use momentum in the way we just defined okay? The second piece was to divide by square root L8. What is that? Square root L8 okay is another linear interpolation of something and something else and specifically, it's a linear interpolation of F8 squared Okay, it's a linear interpolation of the derivative squared Along with the derivative squared last time okay, so in other words. We've got two pieces of momentum going on here one is calculating the momentum version of the gradient the other is calculating the momentum version of the gradient squared and and we often refer to this idea as a Exponentially weighted moving average in other words It's basically equal to the average of this one and the last one and the last one in the last one that we're like multiplicatively Decreasing the previous ones because we're multiplying it by point nine times point nine times point nine times point nine And so you actually see that for instance in the fast AI code? If you look at fit We don't just calculate the average loss right because What I actually want we certainly don't just report the loss for every mini-batch because that just bounces around so much So instead I say average loss is equal to whatever the average was was last time times 0.98 Plus the loss this time times point oh two Right so in other words the fast AI library the thing that it's actually when you do like the learning rate finder or plot loss It's actually showing you the exponentially weighted moving average of the loss Okay, so it's like a really handy concept it appears quite a lot All right the other a handy concept to know about it's this idea of like you've got two numbers One of them is multiplied by some value the other is multiplied by one minus that value So this is a linear interpolation with two values. You'll see it all the time and for some reason Deep learning people nearly always use the value alpha when they do this so like keep an eye out if you're reading a paper Or something and you see like alpha times blah blah blah blah blah plus one minus alpha Times some other blah blah blah blah right immediately like when people read papers None of us like read every thing in the equation we look at it. We go. Oh linear interpolation Right and I said something else just talking to Rachel about yesterday It's like whether we could start trying to find like a a new way of writing papers where we literally refactor them All right, like it'd be so much better to have written like linear interpolate Blah blah blah blah blah blah blah right because then you don't have to have that pattern recognition right but until we Convince the world to change how they write papers This is what you have to do is you have to look you know know what to look for right and once you do suddenly the Huge page width formulas aren't aren't bad at all like you often notice like for example The two things in here like they might be totally identical But this might be a time t and this might be at like time t minus 1 or something Right like it's very often these big ugly formulas turn out to be Really really simple if only they repack them, okay? So what are we doing with this gradient squared? So? What we were doing with the gradient squared is We were taking the square root, and then we were adjusting the learning rate by dividing the learning rate by that Okay, so gradient squared is always positive right and We're taking the exponentially waiting move a moving average of a bunch of things that are always positive And then we're taking the square root of that Right so when is this number going to be high? It's going to be particularly high if there's like one big You know if the gradients got a lot of variation, right? So there's a high variance of gradient then this g squared thing is going to be a really high number Where else if it's like a constant? Amount right it's going to be smaller that because when you add things that are squared The squared slight jump out much bigger or else if there wasn't if there wasn't much change That it's not going to be as big so basically This number at the bottom here It's going to be high If that gradient is changing a lot now. What do you want to do? If you've got something which is like first negative and then positive and then small and then high right Well, you probably want to be more careful right you probably don't want to take a big step because you can't really trust it Right so when the when the variance of the gradient is high We're going to divide our learning rate by a big number Where else if our learning rate is? Very similar kind of size all the time then we probably feel pretty good about the step So we're dividing it by a small amount And so this is called an adaptive learning right now and fit like a lot of people will have this confusion about Adam I've seen it on the forum actually where people are like Isn't there some kind of adaptive learning rate where somehow you're like setting different? Learning rates for different layers or something. It's like no not really Right all we're doing is we're just saying like just keep track of the average of the squares of the gradients and use that To adjust the learning rate, so there's still one learning rate Okay, in this case. It's one Okay, but effectively every parameter at every epoch Is being kind of like getting a bigger jump if the learning rate if the gradient's been pretty constant for that weight and a smaller jump Otherwise okay, and that's Adam. That's the entirety of Adam in In Excel right so there's now no reason at all why you can't train ImageNet in Excel because you've got you've got access to All of the pieces you need And so let's try this out run Okay, that's not bad right five and we're straight up to 29 and two right so the difference between like You know standard SGD and this is is is huge and basically that you know the key difference was that it figured out That we need to be you know moving this number much faster okay, and so and so it did and so you can see we've now got like Two different parameters one is kind of the momentum for the gradient piece the other is the momentum for the gradient squared piece and They I think they're called like I think there's just a couple of beta I think when you when you want to change it in PyTorch says I think what beta which is just a couple of two numbers You can change Jeremy so So you said the yeah, I think I understand this concept of you know when they When a gradient is it goes up and down then you're not really sure Which direction should it should go so you said should kind of slow things down therefore you subtract that You know radiant from the learning rate So but how do you implement that how far do you go? I guess maybe I missed something in early on you you set a number somewhere we divide Yeah, we divide the learning rate Divided by the square root of the moving average gradient squared, so that's where we use it. Oh I'm sorry can you be a little more sure so D2 is the learning rate, which is one Yeah, M27 is our moving average of the square gradients So we just go D2 divided by square root M27 That's it Okay, thanks I Have one question yeah, so The new method that you just mentioned which is in the process of getting implemented in yes Adam W Adam W How different is it from here? Okay? I can let's do that so To understand Adam W. We have to understand weight decay And maybe we'll learn more about that later. Let's see how we go now with weight decay so the idea is that when you have Lots and lots of parameters like we do with you know most of the neural nets we train You very often have like more parameters and data points, or you know like regularization becomes important And we've learned how to avoid over fitting by using dropout right which randomly deletes some activations In the hope that it's going to learn some kind of more resilient set of weights There's another kind of regularization we can use called weight decay or L2 regularization And it's actually comes kind of kind of classic statistical technique and the idea is that we take our loss function right, so we take our like Error squared loss function, and we add an additional piece to it Let's add weight decay right now The additional piece we add is to basically add the square of the weights So we'd say plus B squared Plus a squared Okay That is now Weight decay or L2 regularization and so the idea is that now The The loss function wants to keep the weight small right because increasing the weights makes the loss worse and So it's only going to increase the weights If the loss improves by more than the amount of that penalty and in fact to make this weight to catch a proper weight decay We then need some Modifier here right so if you remember back in our here we said weight decay equals WD 5 e neg 4 okay, so to actually use the same weight decay. I would have to multiply by 0.0005 All right, so that's actually now the same weight decay so If You have a really high weight decay then it's going to set all the parameters to zero So it'll never over fit right because it can't set any parameter to anything All right, and so as you gradually decrease the weight decay a Few more weights can actually be used right but the ones that don't help much. It's still going to leave it zero Or close to zero, right? So that's what that's what weight decay is is literally to change the loss function to add in this sum of squares of weights times some parameter some some hyper parameter you should see The problem is that if you put that into the loss function as I have here Then it ends up in the moving average of gradients and the moving average of squares of gradients For Adam right and so basically we end up When there's a lot of variation we end up decreasing the amount of weight decay And if there's very little variation we end up increasing the amount of weight decay so we end up basically saying penalize parameters, you know weights that are really high Unless their gradient varies a lot which is never what we intended right? That's just not not the plan at all so the trick we're trying to do is Well so the trick with Adam W is we basically remove Weight decay from here So it's not in the loss function It's not in the G not in the G squared and we move it so that instead it It's added directly to the When we update with the learning rate, it's added there instead so in other words it would be We would put the weight decay or actually the gradient with the weight decay in here when we calculate the new a and you be So it never ends up in our G and G squared So that was like a super fast Description which will probably only make sense if you listen to it three or four times on the video and then talk about it on the forum Yeah, but if you're interested let me know and we can also look at an an's code that's implemented this and You know the the idea of using weight decay is it's a really helpful regularizer Because it's basically this way that we can kind of say like You know please don't Increase any of the weight values unless the you know improvement in the loss Is worth it and And so generally speaking pretty much all state-of-the-art models have both dropout and weight decay And I don't claim to know like how to set each one and how much of each to use I've to say like you it's worth trying both To go back to the idea of embeddings is there any way to interpret the finals with user embeddings like absolutely We're going to look at that next week. It's super fun It turns out that you know we'll learn what some of the worst movies of all time It's like um it's a John Travolta Scientology ones like Battleship Earth or something I think that was like the worst movie of all time according to our embeddings Do you have any recommendations for scaling the L2 penalty or is that kind of based on how how wide the nodes or how many notes? No, I have no suggestion at all like I I kind of look for like Papers or cackle competitions or whatever similar and try to set it frankly the same it seems like in a particular area like computer vision object recognition It's like somewhere between 1 in neck 4 or 1 in neck 5 seems to work. You know actually in the Adam W paper the the authors point out that with this new approach it actually becomes like it seems to be much more stable as to what the Right weight decay amounts are so hopefully now when we start playing with it. We'll be able to have some definitive recommendations by the time we get to part 2 Alright well, that's 9 o'clock so this week You know practice the thing that you're least familiar with so if it's like Jacobians and Hessians read about those if it's broadcasting read about those if it's understanding Python Oh, oh read about that. You know try and implement your own custom layers Read the fast AI layers. You know and and talk on the forum about anything that you find Weird or confusing alright. See you next week", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.0, "text": " Welcome back", "tokens": [4027, 646], "temperature": 0.0, "avg_logprob": -0.3025611090281653, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0010483238147571683}, {"id": 1, "seek": 0, "start": 3.18, "end": 6.48, "text": " So we had a busy lesson last week and", "tokens": [407, 321, 632, 257, 5856, 6898, 1036, 1243, 293], "temperature": 0.0, "avg_logprob": -0.3025611090281653, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0010483238147571683}, {"id": 2, "seek": 0, "start": 9.0, "end": 10.56, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.3025611090281653, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0010483238147571683}, {"id": 3, "seek": 0, "start": 10.56, "end": 15.44, "text": " Was really thrilled to see actually one of our master's students here at USF actually", "tokens": [3027, 534, 18744, 281, 536, 767, 472, 295, 527, 4505, 311, 1731, 510, 412, 2546, 37, 767], "temperature": 0.0, "avg_logprob": -0.3025611090281653, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0010483238147571683}, {"id": 4, "seek": 0, "start": 17.28, "end": 19.28, "text": " Actually took what we learned", "tokens": [5135, 1890, 437, 321, 3264], "temperature": 0.0, "avg_logprob": -0.3025611090281653, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0010483238147571683}, {"id": 5, "seek": 0, "start": 20.76, "end": 22.76, "text": " Took what we learned", "tokens": [38288, 437, 321, 3264], "temperature": 0.0, "avg_logprob": -0.3025611090281653, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0010483238147571683}, {"id": 6, "seek": 0, "start": 22.8, "end": 25.32, "text": " with structured deep learning and turned it into a", "tokens": [365, 18519, 2452, 2539, 293, 3574, 309, 666, 257], "temperature": 0.0, "avg_logprob": -0.3025611090281653, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0010483238147571683}, {"id": 7, "seek": 2532, "start": 25.32, "end": 31.16, "text": " Blog post which as I suspected has been incredibly popular because it's just something", "tokens": [46693, 2183, 597, 382, 286, 26439, 575, 668, 6252, 3743, 570, 309, 311, 445, 746], "temperature": 0.0, "avg_logprob": -0.21270418167114258, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.144321297237184e-05}, {"id": 8, "seek": 2532, "start": 32.22, "end": 38.96, "text": " People didn't know about and so it actually ended up getting picked up by the towards data science publication", "tokens": [3432, 994, 380, 458, 466, 293, 370, 309, 767, 4590, 493, 1242, 6183, 493, 538, 264, 3030, 1412, 3497, 19953], "temperature": 0.0, "avg_logprob": -0.21270418167114258, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.144321297237184e-05}, {"id": 9, "seek": 2532, "start": 38.96, "end": 43.0, "text": " Which I quite like actually if you're interested in keeping up with what's going on in data science", "tokens": [3013, 286, 1596, 411, 767, 498, 291, 434, 3102, 294, 5145, 493, 365, 437, 311, 516, 322, 294, 1412, 3497], "temperature": 0.0, "avg_logprob": -0.21270418167114258, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.144321297237184e-05}, {"id": 10, "seek": 2532, "start": 43.120000000000005, "end": 46.879999999999995, "text": " It's quite good medium publication and so Karam", "tokens": [467, 311, 1596, 665, 6399, 19953, 293, 370, 8009, 335], "temperature": 0.0, "avg_logprob": -0.21270418167114258, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.144321297237184e-05}, {"id": 11, "seek": 2532, "start": 47.72, "end": 49.480000000000004, "text": " talked about", "tokens": [2825, 466], "temperature": 0.0, "avg_logprob": -0.21270418167114258, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.144321297237184e-05}, {"id": 12, "seek": 2532, "start": 49.480000000000004, "end": 51.760000000000005, "text": " Structured deep learning and basically introduced", "tokens": [745, 46847, 2452, 2539, 293, 1936, 7268], "temperature": 0.0, "avg_logprob": -0.21270418167114258, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.144321297237184e-05}, {"id": 13, "seek": 5176, "start": 51.76, "end": 56.519999999999996, "text": " you know the the basic ideas that we learned about last week and", "tokens": [291, 458, 264, 264, 3875, 3487, 300, 321, 3264, 466, 1036, 1243, 293], "temperature": 0.0, "avg_logprob": -0.17994151395909927, "compression_ratio": 1.818840579710145, "no_speech_prob": 7.2961342993949074e-06}, {"id": 14, "seek": 5176, "start": 57.92, "end": 62.839999999999996, "text": " It got picked up quite quite widely one of the one of the things I was pleased to see actually", "tokens": [467, 658, 6183, 493, 1596, 1596, 13371, 472, 295, 264, 472, 295, 264, 721, 286, 390, 10587, 281, 536, 767], "temperature": 0.0, "avg_logprob": -0.17994151395909927, "compression_ratio": 1.818840579710145, "no_speech_prob": 7.2961342993949074e-06}, {"id": 15, "seek": 5176, "start": 63.199999999999996, "end": 67.28, "text": " Sebastian Ruda who actually mentioned in last week's class as being one of my favorite researchers", "tokens": [31102, 497, 11152, 567, 767, 2835, 294, 1036, 1243, 311, 1508, 382, 885, 472, 295, 452, 2954, 10309], "temperature": 0.0, "avg_logprob": -0.17994151395909927, "compression_ratio": 1.818840579710145, "no_speech_prob": 7.2961342993949074e-06}, {"id": 16, "seek": 5176, "start": 68.72, "end": 74.92, "text": " Tweeted it and then somebody from stitch fix said oh, yeah, we've actually been doing that for ages, which is kind of cute", "tokens": [314, 10354, 292, 309, 293, 550, 2618, 490, 5635, 3191, 848, 1954, 11, 1338, 11, 321, 600, 767, 668, 884, 300, 337, 12357, 11, 597, 307, 733, 295, 4052], "temperature": 0.0, "avg_logprob": -0.17994151395909927, "compression_ratio": 1.818840579710145, "no_speech_prob": 7.2961342993949074e-06}, {"id": 17, "seek": 5176, "start": 75.6, "end": 76.72, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.17994151395909927, "compression_ratio": 1.818840579710145, "no_speech_prob": 7.2961342993949074e-06}, {"id": 18, "seek": 5176, "start": 76.72, "end": 81.64, "text": " Kind of know that this is happening in industry a lot and I've been telling people this is happening in industry a lot", "tokens": [9242, 295, 458, 300, 341, 307, 2737, 294, 3518, 257, 688, 293, 286, 600, 668, 3585, 561, 341, 307, 2737, 294, 3518, 257, 688], "temperature": 0.0, "avg_logprob": -0.17994151395909927, "compression_ratio": 1.818840579710145, "no_speech_prob": 7.2961342993949074e-06}, {"id": 19, "seek": 8164, "start": 81.64, "end": 87.62, "text": " But nobody's been talking about it and now that Karen's kind of published a blog saying hey check out this cool thing and now stitch", "tokens": [583, 5079, 311, 668, 1417, 466, 309, 293, 586, 300, 14834, 311, 733, 295, 6572, 257, 6968, 1566, 4177, 1520, 484, 341, 1627, 551, 293, 586, 5635], "temperature": 0.0, "avg_logprob": -0.20029770886456524, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.184280336019583e-05}, {"id": 20, "seek": 8164, "start": 87.62, "end": 89.82, "text": " Fix is like yeah doing that already so", "tokens": [25538, 307, 411, 1338, 884, 300, 1217, 370], "temperature": 0.0, "avg_logprob": -0.20029770886456524, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.184280336019583e-05}, {"id": 21, "seek": 8164, "start": 91.28, "end": 93.28, "text": " So that's been great", "tokens": [407, 300, 311, 668, 869], "temperature": 0.0, "avg_logprob": -0.20029770886456524, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.184280336019583e-05}, {"id": 22, "seek": 8164, "start": 93.4, "end": 100.78, "text": " Great to see and I think there's still a lot more that can be dug into with this structured deep learning stuff", "tokens": [3769, 281, 536, 293, 286, 519, 456, 311, 920, 257, 688, 544, 300, 393, 312, 22954, 666, 365, 341, 18519, 2452, 2539, 1507], "temperature": 0.0, "avg_logprob": -0.20029770886456524, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.184280336019583e-05}, {"id": 23, "seek": 8164, "start": 100.78, "end": 104.2, "text": " You know to build on top of Karen's post would be that maybe like", "tokens": [509, 458, 281, 1322, 322, 1192, 295, 14834, 311, 2183, 576, 312, 300, 1310, 411], "temperature": 0.0, "avg_logprob": -0.20029770886456524, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.184280336019583e-05}, {"id": 24, "seek": 8164, "start": 104.76, "end": 109.16, "text": " Experiment with some different data sets maybe find some old Kaggle competitions and see like", "tokens": [37933, 365, 512, 819, 1412, 6352, 1310, 915, 512, 1331, 48751, 22631, 26185, 293, 536, 411], "temperature": 0.0, "avg_logprob": -0.20029770886456524, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.184280336019583e-05}, {"id": 25, "seek": 10916, "start": 109.16, "end": 115.78, "text": " Now there's some competitions that you could now win with this or some which doesn't work for would be equally interesting", "tokens": [823, 456, 311, 512, 26185, 300, 291, 727, 586, 1942, 365, 341, 420, 512, 597, 1177, 380, 589, 337, 576, 312, 12309, 1880], "temperature": 0.0, "avg_logprob": -0.14994654832062898, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.4367411722560064e-06}, {"id": 26, "seek": 10916, "start": 116.47999999999999, "end": 118.47999999999999, "text": " and also like just", "tokens": [293, 611, 411, 445], "temperature": 0.0, "avg_logprob": -0.14994654832062898, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.4367411722560064e-06}, {"id": 27, "seek": 10916, "start": 118.64, "end": 122.52, "text": " Experimenting a bit with different amounts of dropout different layer sizes, you know", "tokens": [37933, 278, 257, 857, 365, 819, 11663, 295, 3270, 346, 819, 4583, 11602, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.14994654832062898, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.4367411722560064e-06}, {"id": 28, "seek": 10916, "start": 123.6, "end": 129.84, "text": " Because nobody much has written about this. I don't think there's been any blog posts about this before that I've seen anywhere", "tokens": [1436, 5079, 709, 575, 3720, 466, 341, 13, 286, 500, 380, 519, 456, 311, 668, 604, 6968, 12300, 466, 341, 949, 300, 286, 600, 1612, 4992], "temperature": 0.0, "avg_logprob": -0.14994654832062898, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.4367411722560064e-06}, {"id": 29, "seek": 10916, "start": 131.2, "end": 133.2, "text": " There's a lot of unexplored territory", "tokens": [821, 311, 257, 688, 295, 11572, 564, 2769, 11360], "temperature": 0.0, "avg_logprob": -0.14994654832062898, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.4367411722560064e-06}, {"id": 30, "seek": 10916, "start": 133.44, "end": 137.44, "text": " So I think there's a lot we could we could build on top of here", "tokens": [407, 286, 519, 456, 311, 257, 688, 321, 727, 321, 727, 1322, 322, 1192, 295, 510], "temperature": 0.0, "avg_logprob": -0.14994654832062898, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.4367411722560064e-06}, {"id": 31, "seek": 13744, "start": 137.44, "end": 143.42, "text": " And there's definitely a lot of interest I saw one person on Twitter saying this is what I've been looking for ages", "tokens": [400, 456, 311, 2138, 257, 688, 295, 1179, 286, 1866, 472, 954, 322, 5794, 1566, 341, 307, 437, 286, 600, 668, 1237, 337, 12357], "temperature": 0.0, "avg_logprob": -0.19783350216445103, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.507525656867074e-06}, {"id": 32, "seek": 13744, "start": 144.8, "end": 147.24, "text": " Another thing which I was pleased to see is", "tokens": [3996, 551, 597, 286, 390, 10587, 281, 536, 307], "temperature": 0.0, "avg_logprob": -0.19783350216445103, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.507525656867074e-06}, {"id": 33, "seek": 13744, "start": 148.28, "end": 150.28, "text": " Nikhil who we saw his", "tokens": [13969, 42829, 567, 321, 1866, 702], "temperature": 0.0, "avg_logprob": -0.19783350216445103, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.507525656867074e-06}, {"id": 34, "seek": 13744, "start": 150.35999999999999, "end": 152.32, "text": " cricket versus baseball", "tokens": [31626, 5717, 14323], "temperature": 0.0, "avg_logprob": -0.19783350216445103, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.507525656867074e-06}, {"id": 35, "seek": 13744, "start": 152.32, "end": 155.92, "text": " Predictor as well as his currency predictor after lesson one", "tokens": [430, 24945, 284, 382, 731, 382, 702, 13346, 6069, 284, 934, 6898, 472], "temperature": 0.0, "avg_logprob": -0.19783350216445103, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.507525656867074e-06}, {"id": 36, "seek": 13744, "start": 157.44, "end": 159.44, "text": " Went on to", "tokens": [31809, 322, 281], "temperature": 0.0, "avg_logprob": -0.19783350216445103, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.507525656867074e-06}, {"id": 37, "seek": 15944, "start": 159.44, "end": 166.88, "text": " Download something a bit bigger which was to download a couple of hundred of images of actors and he manually", "tokens": [32282, 746, 257, 857, 3801, 597, 390, 281, 5484, 257, 1916, 295, 3262, 295, 5267, 295, 10037, 293, 415, 16945], "temperature": 0.0, "avg_logprob": -0.16858257231165152, "compression_ratio": 1.785953177257525, "no_speech_prob": 1.4593700825571432e-06}, {"id": 38, "seek": 15944, "start": 167.28, "end": 169.12, "text": " Went through and checked which well", "tokens": [31809, 807, 293, 10033, 597, 731], "temperature": 0.0, "avg_logprob": -0.16858257231165152, "compression_ratio": 1.785953177257525, "no_speech_prob": 1.4593700825571432e-06}, {"id": 39, "seek": 15944, "start": 169.12, "end": 173.92, "text": " I think first of all he like used Google to try and find ones with glasses and ones without then he manually went through", "tokens": [286, 519, 700, 295, 439, 415, 411, 1143, 3329, 281, 853, 293, 915, 2306, 365, 10812, 293, 2306, 1553, 550, 415, 16945, 1437, 807], "temperature": 0.0, "avg_logprob": -0.16858257231165152, "compression_ratio": 1.785953177257525, "no_speech_prob": 1.4593700825571432e-06}, {"id": 40, "seek": 15944, "start": 173.92, "end": 175.92, "text": " And checked that they had been put in the right spot", "tokens": [400, 10033, 300, 436, 632, 668, 829, 294, 264, 558, 4008], "temperature": 0.0, "avg_logprob": -0.16858257231165152, "compression_ratio": 1.785953177257525, "no_speech_prob": 1.4593700825571432e-06}, {"id": 41, "seek": 15944, "start": 176.24, "end": 178.24, "text": " And this is a good example of one where?", "tokens": [400, 341, 307, 257, 665, 1365, 295, 472, 689, 30], "temperature": 0.0, "avg_logprob": -0.16858257231165152, "compression_ratio": 1.785953177257525, "no_speech_prob": 1.4593700825571432e-06}, {"id": 42, "seek": 15944, "start": 179.2, "end": 182.28, "text": " Vanilla resonant didn't do so well with just the last layer", "tokens": [8979, 5291, 12544, 394, 994, 380, 360, 370, 731, 365, 445, 264, 1036, 4583], "temperature": 0.0, "avg_logprob": -0.16858257231165152, "compression_ratio": 1.785953177257525, "no_speech_prob": 1.4593700825571432e-06}, {"id": 43, "seek": 18228, "start": 182.28, "end": 190.28, "text": " And so what Nikhil did was he went through and tried unfreezing the layers and using differential learning rates and got up to", "tokens": [400, 370, 437, 13969, 42829, 630, 390, 415, 1437, 807, 293, 3031, 3971, 701, 8781, 264, 7914, 293, 1228, 15756, 2539, 6846, 293, 658, 493, 281], "temperature": 0.0, "avg_logprob": -0.1354300932450728, "compression_ratio": 1.7169811320754718, "no_speech_prob": 1.9947187865909655e-06}, {"id": 44, "seek": 18228, "start": 190.56, "end": 192.56, "text": " 100% accuracy and", "tokens": [2319, 4, 14170, 293], "temperature": 0.0, "avg_logprob": -0.1354300932450728, "compression_ratio": 1.7169811320754718, "no_speech_prob": 1.9947187865909655e-06}, {"id": 45, "seek": 18228, "start": 192.88, "end": 198.64, "text": " The thing I like about these things that Nikhil is doing is the way he's he's not downloading a Kaggle data set", "tokens": [440, 551, 286, 411, 466, 613, 721, 300, 13969, 42829, 307, 884, 307, 264, 636, 415, 311, 415, 311, 406, 32529, 257, 48751, 22631, 1412, 992], "temperature": 0.0, "avg_logprob": -0.1354300932450728, "compression_ratio": 1.7169811320754718, "no_speech_prob": 1.9947187865909655e-06}, {"id": 46, "seek": 18228, "start": 198.64, "end": 203.6, "text": " He's like deciding on a problem that he's going to try and solve he's going from scratch from Google", "tokens": [634, 311, 411, 17990, 322, 257, 1154, 300, 415, 311, 516, 281, 853, 293, 5039, 415, 311, 516, 490, 8459, 490, 3329], "temperature": 0.0, "avg_logprob": -0.1354300932450728, "compression_ratio": 1.7169811320754718, "no_speech_prob": 1.9947187865909655e-06}, {"id": 47, "seek": 18228, "start": 203.96, "end": 209.14, "text": " And he's actually got a link here even to a suggested way to help you download images from Google", "tokens": [400, 415, 311, 767, 658, 257, 2113, 510, 754, 281, 257, 10945, 636, 281, 854, 291, 5484, 5267, 490, 3329], "temperature": 0.0, "avg_logprob": -0.1354300932450728, "compression_ratio": 1.7169811320754718, "no_speech_prob": 1.9947187865909655e-06}, {"id": 48, "seek": 20914, "start": 209.14, "end": 212.38, "text": " So I think this is great, and I actually gave a talk", "tokens": [407, 286, 519, 341, 307, 869, 11, 293, 286, 767, 2729, 257, 751], "temperature": 0.0, "avg_logprob": -0.19948064064492985, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.8448104128765408e-06}, {"id": 49, "seek": 20914, "start": 213.05999999999997, "end": 219.01999999999998, "text": " Just this afternoon at Singularity University to an executive team of one of the world's largest", "tokens": [1449, 341, 6499, 412, 7474, 1040, 507, 3535, 281, 364, 10140, 1469, 295, 472, 295, 264, 1002, 311, 6443], "temperature": 0.0, "avg_logprob": -0.19948064064492985, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.8448104128765408e-06}, {"id": 50, "seek": 20914, "start": 219.45999999999998, "end": 222.05999999999997, "text": " Telecommunications companies and actually show them this post", "tokens": [14889, 25451, 24847, 3431, 293, 767, 855, 552, 341, 2183], "temperature": 0.0, "avg_logprob": -0.19948064064492985, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.8448104128765408e-06}, {"id": 51, "seek": 20914, "start": 223.05999999999997, "end": 228.89999999999998, "text": " Because the the folks there were telling me that that all the vendors that come to them and tell them they need like", "tokens": [1436, 264, 264, 4024, 456, 645, 3585, 385, 300, 300, 439, 264, 22056, 300, 808, 281, 552, 293, 980, 552, 436, 643, 411], "temperature": 0.0, "avg_logprob": -0.19948064064492985, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.8448104128765408e-06}, {"id": 52, "seek": 20914, "start": 229.57999999999998, "end": 233.94, "text": " Millions of images and huge data centers full of hardware, and you know they have to buy a special", "tokens": [7190, 626, 295, 5267, 293, 2603, 1412, 10898, 1577, 295, 8837, 11, 293, 291, 458, 436, 362, 281, 2256, 257, 2121], "temperature": 0.0, "avg_logprob": -0.19948064064492985, "compression_ratio": 1.6550387596899225, "no_speech_prob": 1.8448104128765408e-06}, {"id": 53, "seek": 23394, "start": 233.94, "end": 237.94, "text": " software that only these vendors can provide and I said like", "tokens": [4722, 300, 787, 613, 22056, 393, 2893, 293, 286, 848, 411], "temperature": 0.0, "avg_logprob": -0.2327275826380803, "compression_ratio": 1.6349809885931559, "no_speech_prob": 5.014695489080623e-06}, {"id": 54, "seek": 23394, "start": 238.42, "end": 242.3, "text": " Actually, this person's been doing a course for three weeks now and look at what he's just done", "tokens": [5135, 11, 341, 954, 311, 668, 884, 257, 1164, 337, 1045, 3259, 586, 293, 574, 412, 437, 415, 311, 445, 1096], "temperature": 0.0, "avg_logprob": -0.2327275826380803, "compression_ratio": 1.6349809885931559, "no_speech_prob": 5.014695489080623e-06}, {"id": 55, "seek": 23394, "start": 242.54, "end": 249.38, "text": " With a computer that cost him 60 cents an hour and they were like they were so happy to hear that like okay", "tokens": [2022, 257, 3820, 300, 2063, 796, 4060, 14941, 364, 1773, 293, 436, 645, 411, 436, 645, 370, 2055, 281, 1568, 300, 411, 1392], "temperature": 0.0, "avg_logprob": -0.2327275826380803, "compression_ratio": 1.6349809885931559, "no_speech_prob": 5.014695489080623e-06}, {"id": 56, "seek": 23394, "start": 249.38, "end": 252.06, "text": " There you know this actually is in the reach of normal people", "tokens": [821, 291, 458, 341, 767, 307, 294, 264, 2524, 295, 2710, 561], "temperature": 0.0, "avg_logprob": -0.2327275826380803, "compression_ratio": 1.6349809885931559, "no_speech_prob": 5.014695489080623e-06}, {"id": 57, "seek": 23394, "start": 253.3, "end": 255.7, "text": " I'm assuming Nikhil's a normal person. I haven't actually", "tokens": [286, 478, 11926, 13969, 42829, 311, 257, 2710, 954, 13, 286, 2378, 380, 767], "temperature": 0.0, "avg_logprob": -0.2327275826380803, "compression_ratio": 1.6349809885931559, "no_speech_prob": 5.014695489080623e-06}, {"id": 58, "seek": 23394, "start": 257.9, "end": 260.54, "text": " If you're proudly abnormal Nikhil I apologize", "tokens": [759, 291, 434, 33522, 32847, 13969, 42829, 286, 12328], "temperature": 0.0, "avg_logprob": -0.2327275826380803, "compression_ratio": 1.6349809885931559, "no_speech_prob": 5.014695489080623e-06}, {"id": 59, "seek": 26054, "start": 260.54, "end": 264.82, "text": " I actually went and actually had a look at his cricket", "tokens": [286, 767, 1437, 293, 767, 632, 257, 574, 412, 702, 31626], "temperature": 0.0, "avg_logprob": -0.16437060492379324, "compression_ratio": 1.753787878787879, "no_speech_prob": 7.296266630874015e-06}, {"id": 60, "seek": 26054, "start": 265.46000000000004, "end": 269.94, "text": " Classifier and I was really pleased to see that his code actually is the exact same code", "tokens": [9471, 9902, 293, 286, 390, 534, 10587, 281, 536, 300, 702, 3089, 767, 307, 264, 1900, 912, 3089], "temperature": 0.0, "avg_logprob": -0.16437060492379324, "compression_ratio": 1.753787878787879, "no_speech_prob": 7.296266630874015e-06}, {"id": 61, "seek": 26054, "start": 270.34000000000003, "end": 274.48, "text": " That we used in lesson one. I was hoping that would be the case you know the only thing he changed was", "tokens": [663, 321, 1143, 294, 6898, 472, 13, 286, 390, 7159, 300, 576, 312, 264, 1389, 291, 458, 264, 787, 551, 415, 3105, 390], "temperature": 0.0, "avg_logprob": -0.16437060492379324, "compression_ratio": 1.753787878787879, "no_speech_prob": 7.296266630874015e-06}, {"id": 62, "seek": 26054, "start": 275.18, "end": 277.18, "text": " The number of epochs I guess", "tokens": [440, 1230, 295, 30992, 28346, 286, 2041], "temperature": 0.0, "avg_logprob": -0.16437060492379324, "compression_ratio": 1.753787878787879, "no_speech_prob": 7.296266630874015e-06}, {"id": 63, "seek": 26054, "start": 277.54, "end": 281.44, "text": " So this idea that we can take those four lines of code and reuse it to do other things", "tokens": [407, 341, 1558, 300, 321, 393, 747, 729, 1451, 3876, 295, 3089, 293, 26225, 309, 281, 360, 661, 721], "temperature": 0.0, "avg_logprob": -0.16437060492379324, "compression_ratio": 1.753787878787879, "no_speech_prob": 7.296266630874015e-06}, {"id": 64, "seek": 26054, "start": 281.44, "end": 287.90000000000003, "text": " It's definitely turned out to be true and so these are good things to show like at your organization", "tokens": [467, 311, 2138, 3574, 484, 281, 312, 2074, 293, 370, 613, 366, 665, 721, 281, 855, 411, 412, 428, 4475], "temperature": 0.0, "avg_logprob": -0.16437060492379324, "compression_ratio": 1.753787878787879, "no_speech_prob": 7.296266630874015e-06}, {"id": 65, "seek": 28790, "start": 287.9, "end": 293.62, "text": " if you're anything like the executives of this big company I spoke to today, there'll be a certain amount of like", "tokens": [498, 291, 434, 1340, 411, 264, 28485, 295, 341, 955, 2237, 286, 7179, 281, 965, 11, 456, 603, 312, 257, 1629, 2372, 295, 411], "temperature": 0.0, "avg_logprob": -0.21901455233173986, "compression_ratio": 1.729641693811075, "no_speech_prob": 3.1381259759655222e-06}, {"id": 66, "seek": 28790, "start": 294.14, "end": 299.21999999999997, "text": " Not just surprise, but almost like pushback with like if this was true somebody", "tokens": [1726, 445, 6365, 11, 457, 1920, 411, 2944, 3207, 365, 411, 498, 341, 390, 2074, 2618], "temperature": 0.0, "avg_logprob": -0.21901455233173986, "compression_ratio": 1.729641693811075, "no_speech_prob": 3.1381259759655222e-06}, {"id": 67, "seek": 28790, "start": 299.21999999999997, "end": 304.21999999999997, "text": " You know they basically said that this is true somebody would have told us so like why isn't everybody doing this already?", "tokens": [509, 458, 436, 1936, 848, 300, 341, 307, 2074, 2618, 576, 362, 1907, 505, 370, 411, 983, 1943, 380, 2201, 884, 341, 1217, 30], "temperature": 0.0, "avg_logprob": -0.21901455233173986, "compression_ratio": 1.729641693811075, "no_speech_prob": 3.1381259759655222e-06}, {"id": 68, "seek": 28790, "start": 304.29999999999995, "end": 308.62, "text": " So like I think you might have to actually show them you know maybe you can build your own", "tokens": [407, 411, 286, 519, 291, 1062, 362, 281, 767, 855, 552, 291, 458, 1310, 291, 393, 1322, 428, 1065], "temperature": 0.0, "avg_logprob": -0.21901455233173986, "compression_ratio": 1.729641693811075, "no_speech_prob": 3.1381259759655222e-06}, {"id": 69, "seek": 28790, "start": 309.06, "end": 314.73999999999995, "text": " With some internal data you've got at work or something like here. It is you know didn't cost me anything", "tokens": [2022, 512, 6920, 1412, 291, 600, 658, 412, 589, 420, 746, 411, 510, 13, 467, 307, 291, 458, 994, 380, 2063, 385, 1340], "temperature": 0.0, "avg_logprob": -0.21901455233173986, "compression_ratio": 1.729641693811075, "no_speech_prob": 3.1381259759655222e-06}, {"id": 70, "seek": 28790, "start": 315.29999999999995, "end": 317.29999999999995, "text": " It's all finished", "tokens": [467, 311, 439, 4335], "temperature": 0.0, "avg_logprob": -0.21901455233173986, "compression_ratio": 1.729641693811075, "no_speech_prob": 3.1381259759655222e-06}, {"id": 71, "seek": 31730, "start": 317.3, "end": 319.3, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.21565322875976561, "compression_ratio": 1.718978102189781, "no_speech_prob": 3.90544028050499e-06}, {"id": 72, "seek": 31730, "start": 319.90000000000003, "end": 323.90000000000003, "text": " Vitaly or vitally I don't know how to pronounce his name correctly has done another very nice post on", "tokens": [48307, 88, 420, 9467, 379, 286, 500, 380, 458, 577, 281, 19567, 702, 1315, 8944, 575, 1096, 1071, 588, 1481, 2183, 322], "temperature": 0.0, "avg_logprob": -0.21565322875976561, "compression_ratio": 1.718978102189781, "no_speech_prob": 3.90544028050499e-06}, {"id": 73, "seek": 31730, "start": 324.82, "end": 330.98, "text": " Just an introductory post on how we train neural networks, and I wanted to point this one out as being like I think", "tokens": [1449, 364, 39048, 2183, 322, 577, 321, 3847, 18161, 9590, 11, 293, 286, 1415, 281, 935, 341, 472, 484, 382, 885, 411, 286, 519], "temperature": 0.0, "avg_logprob": -0.21565322875976561, "compression_ratio": 1.718978102189781, "no_speech_prob": 3.90544028050499e-06}, {"id": 74, "seek": 31730, "start": 332.18, "end": 335.54, "text": " This is one of the participants in this course who's just got a particular knack for", "tokens": [639, 307, 472, 295, 264, 10503, 294, 341, 1164, 567, 311, 445, 658, 257, 1729, 444, 501, 337], "temperature": 0.0, "avg_logprob": -0.21565322875976561, "compression_ratio": 1.718978102189781, "no_speech_prob": 3.90544028050499e-06}, {"id": 75, "seek": 31730, "start": 335.94, "end": 341.98, "text": " Technical communication, and I think we can all learn from you know from his posts about about good technical writing and", "tokens": [35512, 6101, 11, 293, 286, 519, 321, 393, 439, 1466, 490, 291, 458, 490, 702, 12300, 466, 466, 665, 6191, 3579, 293], "temperature": 0.0, "avg_logprob": -0.21565322875976561, "compression_ratio": 1.718978102189781, "no_speech_prob": 3.90544028050499e-06}, {"id": 76, "seek": 31730, "start": 342.74, "end": 345.1, "text": " What I really like particularly is that he?", "tokens": [708, 286, 534, 411, 4098, 307, 300, 415, 30], "temperature": 0.0, "avg_logprob": -0.21565322875976561, "compression_ratio": 1.718978102189781, "no_speech_prob": 3.90544028050499e-06}, {"id": 77, "seek": 34510, "start": 345.1, "end": 350.54, "text": " He assumes almost nothing like he has a kind of a very chatty tone and describes everything", "tokens": [634, 37808, 1920, 1825, 411, 415, 575, 257, 733, 295, 257, 588, 5081, 874, 8027, 293, 15626, 1203], "temperature": 0.0, "avg_logprob": -0.16966462791512865, "compression_ratio": 1.7814814814814814, "no_speech_prob": 4.425425231602276e-06}, {"id": 78, "seek": 34510, "start": 350.86, "end": 353.12, "text": " But he also assumes that the reader is intelligent", "tokens": [583, 415, 611, 37808, 300, 264, 15149, 307, 13232], "temperature": 0.0, "avg_logprob": -0.16966462791512865, "compression_ratio": 1.7814814814814814, "no_speech_prob": 4.425425231602276e-06}, {"id": 79, "seek": 34510, "start": 353.42, "end": 358.46000000000004, "text": " But you know so like he's not afraid to kind of say he has a paper or here's an equation or or whatever", "tokens": [583, 291, 458, 370, 411, 415, 311, 406, 4638, 281, 733, 295, 584, 415, 575, 257, 3035, 420, 510, 311, 364, 5367, 420, 420, 2035], "temperature": 0.0, "avg_logprob": -0.16966462791512865, "compression_ratio": 1.7814814814814814, "no_speech_prob": 4.425425231602276e-06}, {"id": 80, "seek": 34510, "start": 358.46000000000004, "end": 361.58000000000004, "text": " But then he's going to go through and tell you exactly what that equation means", "tokens": [583, 550, 415, 311, 516, 281, 352, 807, 293, 980, 291, 2293, 437, 300, 5367, 1355], "temperature": 0.0, "avg_logprob": -0.16966462791512865, "compression_ratio": 1.7814814814814814, "no_speech_prob": 4.425425231602276e-06}, {"id": 81, "seek": 34510, "start": 362.54, "end": 365.92, "text": " So it's kind of like this nice mix of like writing for", "tokens": [407, 309, 311, 733, 295, 411, 341, 1481, 2890, 295, 411, 3579, 337], "temperature": 0.0, "avg_logprob": -0.16966462791512865, "compression_ratio": 1.7814814814814814, "no_speech_prob": 4.425425231602276e-06}, {"id": 82, "seek": 36592, "start": 365.92, "end": 374.52000000000004, "text": " a respectfully for an intelligent audience, but also not assuming any particular background knowledge", "tokens": [257, 45201, 337, 364, 13232, 4034, 11, 457, 611, 406, 11926, 604, 1729, 3678, 3601], "temperature": 0.0, "avg_logprob": -0.1904996917361305, "compression_ratio": 1.632, "no_speech_prob": 3.966882104577962e-06}, {"id": 83, "seek": 36592, "start": 375.64000000000004, "end": 382.84000000000003, "text": " So then I made the mistake earlier this week of posting a picture of my first placing on the Kaggle seedlings", "tokens": [407, 550, 286, 1027, 264, 6146, 3071, 341, 1243, 295, 15978, 257, 3036, 295, 452, 700, 17221, 322, 264, 48751, 22631, 8871, 20823], "temperature": 0.0, "avg_logprob": -0.1904996917361305, "compression_ratio": 1.632, "no_speech_prob": 3.966882104577962e-06}, {"id": 84, "seek": 36592, "start": 383.0, "end": 390.44, "text": " Competition at which point five other fast AI students posted their pictures of them passing me over the next few days so", "tokens": [43634, 412, 597, 935, 1732, 661, 2370, 7318, 1731, 9437, 641, 5242, 295, 552, 8437, 385, 670, 264, 958, 1326, 1708, 370], "temperature": 0.0, "avg_logprob": -0.1904996917361305, "compression_ratio": 1.632, "no_speech_prob": 3.966882104577962e-06}, {"id": 85, "seek": 36592, "start": 390.96000000000004, "end": 394.56, "text": " This is the current leaderboard for the Kaggle plant seedlings competition", "tokens": [639, 307, 264, 2190, 5263, 3787, 337, 264, 48751, 22631, 3709, 8871, 20823, 6211], "temperature": 0.0, "avg_logprob": -0.1904996917361305, "compression_ratio": 1.632, "no_speech_prob": 3.966882104577962e-06}, {"id": 86, "seek": 39456, "start": 394.56, "end": 400.42, "text": " I believe the product top six are all fast AI students or in the worst of those teachers", "tokens": [286, 1697, 264, 1674, 1192, 2309, 366, 439, 2370, 7318, 1731, 420, 294, 264, 5855, 295, 729, 6023], "temperature": 0.0, "avg_logprob": -0.2865234150606043, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.5294060631276807e-06}, {"id": 87, "seek": 39456, "start": 401.52, "end": 404.36, "text": " And so I think this is like a really oh", "tokens": [400, 370, 286, 519, 341, 307, 411, 257, 534, 1954], "temperature": 0.0, "avg_logprob": -0.2865234150606043, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.5294060631276807e-06}, {"id": 88, "seek": 39456, "start": 405.0, "end": 407.08, "text": " Look James has just passed he was first", "tokens": [2053, 5678, 575, 445, 4678, 415, 390, 700], "temperature": 0.0, "avg_logprob": -0.2865234150606043, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.5294060631276807e-06}, {"id": 89, "seek": 39456, "start": 407.72, "end": 410.06, "text": " This is a really good example of like", "tokens": [639, 307, 257, 534, 665, 1365, 295, 411], "temperature": 0.0, "avg_logprob": -0.2865234150606043, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.5294060631276807e-06}, {"id": 90, "seek": 39456, "start": 411.2, "end": 413.48, "text": " What you can do this is?", "tokens": [708, 291, 393, 360, 341, 307, 30], "temperature": 0.0, "avg_logprob": -0.2865234150606043, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.5294060631276807e-06}, {"id": 91, "seek": 39456, "start": 414.2, "end": 416.2, "text": " I'm trying to think it was like a", "tokens": [286, 478, 1382, 281, 519, 309, 390, 411, 257], "temperature": 0.0, "avg_logprob": -0.2865234150606043, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.5294060631276807e-06}, {"id": 92, "seek": 39456, "start": 416.96, "end": 418.96, "text": " small number of thousands of images", "tokens": [1359, 1230, 295, 5383, 295, 5267], "temperature": 0.0, "avg_logprob": -0.2865234150606043, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.5294060631276807e-06}, {"id": 93, "seek": 39456, "start": 420.12, "end": 421.32, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2865234150606043, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.5294060631276807e-06}, {"id": 94, "seek": 42132, "start": 421.32, "end": 425.14, "text": " Most of the images were only were less than a hundred pixels by a hundred pixels", "tokens": [4534, 295, 264, 5267, 645, 787, 645, 1570, 813, 257, 3262, 18668, 538, 257, 3262, 18668], "temperature": 0.0, "avg_logprob": -0.21086656204377763, "compression_ratio": 1.6270491803278688, "no_speech_prob": 4.425453425938031e-06}, {"id": 95, "seek": 42132, "start": 428.0, "end": 432.9, "text": " And yet we you know I bet my approach was basically to say let's just run through the notebook", "tokens": [400, 1939, 321, 291, 458, 286, 778, 452, 3109, 390, 1936, 281, 584, 718, 311, 445, 1190, 807, 264, 21060], "temperature": 0.0, "avg_logprob": -0.21086656204377763, "compression_ratio": 1.6270491803278688, "no_speech_prob": 4.425453425938031e-06}, {"id": 96, "seek": 42132, "start": 432.9, "end": 435.71999999999997, "text": " We have pretty much default took me. I don't know an hour", "tokens": [492, 362, 1238, 709, 7576, 1890, 385, 13, 286, 500, 380, 458, 364, 1773], "temperature": 0.0, "avg_logprob": -0.21086656204377763, "compression_ratio": 1.6270491803278688, "no_speech_prob": 4.425453425938031e-06}, {"id": 97, "seek": 42132, "start": 436.71999999999997, "end": 437.71999999999997, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.21086656204377763, "compression_ratio": 1.6270491803278688, "no_speech_prob": 4.425453425938031e-06}, {"id": 98, "seek": 42132, "start": 437.71999999999997, "end": 443.68, "text": " I'm I think the other students doing a little bit more than that, but not a hell of a lot more and basically", "tokens": [286, 478, 286, 519, 264, 661, 1731, 884, 257, 707, 857, 544, 813, 300, 11, 457, 406, 257, 4921, 295, 257, 688, 544, 293, 1936], "temperature": 0.0, "avg_logprob": -0.21086656204377763, "compression_ratio": 1.6270491803278688, "no_speech_prob": 4.425453425938031e-06}, {"id": 99, "seek": 42132, "start": 444.2, "end": 446.8, "text": " What this is saying is yeah these these techniques", "tokens": [708, 341, 307, 1566, 307, 1338, 613, 613, 7512], "temperature": 0.0, "avg_logprob": -0.21086656204377763, "compression_ratio": 1.6270491803278688, "no_speech_prob": 4.425453425938031e-06}, {"id": 100, "seek": 44680, "start": 446.8, "end": 452.72, "text": " Work pretty reliably to a point where people that aren't using the fast AI libraries are", "tokens": [6603, 1238, 49927, 281, 257, 935, 689, 561, 300, 3212, 380, 1228, 264, 2370, 7318, 15148, 366], "temperature": 0.0, "avg_logprob": -0.23370792752220518, "compression_ratio": 1.584070796460177, "no_speech_prob": 1.0030113344328129e-06}, {"id": 101, "seek": 44680, "start": 453.96000000000004, "end": 456.96000000000004, "text": " You know literally really struggling I", "tokens": [509, 458, 3736, 534, 9314, 286], "temperature": 0.0, "avg_logprob": -0.23370792752220518, "compression_ratio": 1.584070796460177, "no_speech_prob": 1.0030113344328129e-06}, {"id": 102, "seek": 44680, "start": 458.08, "end": 461.76, "text": " Suspect all these are fast AI students you might have to go down quite a way", "tokens": [9545, 1043, 439, 613, 366, 2370, 7318, 1731, 291, 1062, 362, 281, 352, 760, 1596, 257, 636], "temperature": 0.0, "avg_logprob": -0.23370792752220518, "compression_ratio": 1.584070796460177, "no_speech_prob": 1.0030113344328129e-06}, {"id": 103, "seek": 44680, "start": 462.84000000000003, "end": 465.76, "text": " So I thought that was very interesting and really really cool", "tokens": [407, 286, 1194, 300, 390, 588, 1880, 293, 534, 534, 1627], "temperature": 0.0, "avg_logprob": -0.23370792752220518, "compression_ratio": 1.584070796460177, "no_speech_prob": 1.0030113344328129e-06}, {"id": 104, "seek": 44680, "start": 468.0, "end": 470.0, "text": " So today we're going to", "tokens": [407, 965, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.23370792752220518, "compression_ratio": 1.584070796460177, "no_speech_prob": 1.0030113344328129e-06}, {"id": 105, "seek": 47000, "start": 470.0, "end": 476.36, "text": " Start what I would kind of call like the second half of this course", "tokens": [6481, 437, 286, 576, 733, 295, 818, 411, 264, 1150, 1922, 295, 341, 1164], "temperature": 0.0, "avg_logprob": -0.20201843212812376, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.813003447954543e-06}, {"id": 106, "seek": 47000, "start": 476.76, "end": 479.52, "text": " so the first half of this course has been like", "tokens": [370, 264, 700, 1922, 295, 341, 1164, 575, 668, 411], "temperature": 0.0, "avg_logprob": -0.20201843212812376, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.813003447954543e-06}, {"id": 107, "seek": 47000, "start": 480.28, "end": 481.84, "text": " getting through", "tokens": [1242, 807], "temperature": 0.0, "avg_logprob": -0.20201843212812376, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.813003447954543e-06}, {"id": 108, "seek": 47000, "start": 481.84, "end": 485.36, "text": " Like these are the applications that we can use this for", "tokens": [1743, 613, 366, 264, 5821, 300, 321, 393, 764, 341, 337], "temperature": 0.0, "avg_logprob": -0.20201843212812376, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.813003447954543e-06}, {"id": 109, "seek": 47000, "start": 486.48, "end": 488.48, "text": " Here's kind of the code you have to write", "tokens": [1692, 311, 733, 295, 264, 3089, 291, 362, 281, 2464], "temperature": 0.0, "avg_logprob": -0.20201843212812376, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.813003447954543e-06}, {"id": 110, "seek": 47000, "start": 489.64, "end": 493.0, "text": " Here's a fairly high level ish description of what it's doing", "tokens": [1692, 311, 257, 6457, 1090, 1496, 307, 71, 3855, 295, 437, 309, 311, 884], "temperature": 0.0, "avg_logprob": -0.20201843212812376, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.813003447954543e-06}, {"id": 111, "seek": 47000, "start": 493.92, "end": 495.32, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.20201843212812376, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.813003447954543e-06}, {"id": 112, "seek": 49532, "start": 495.32, "end": 500.48, "text": " We're kind of we're kind of done for that bit and what we're now going to do is go in reverse", "tokens": [492, 434, 733, 295, 321, 434, 733, 295, 1096, 337, 300, 857, 293, 437, 321, 434, 586, 516, 281, 360, 307, 352, 294, 9943], "temperature": 0.0, "avg_logprob": -0.16603834969656808, "compression_ratio": 1.798165137614679, "no_speech_prob": 3.041574018425308e-06}, {"id": 113, "seek": 49532, "start": 500.68, "end": 503.92, "text": " We're going to go back over all of those exact same things again", "tokens": [492, 434, 516, 281, 352, 646, 670, 439, 295, 729, 1900, 912, 721, 797], "temperature": 0.0, "avg_logprob": -0.16603834969656808, "compression_ratio": 1.798165137614679, "no_speech_prob": 3.041574018425308e-06}, {"id": 114, "seek": 49532, "start": 504.15999999999997, "end": 509.54, "text": " But this time we're going to dig into the detail of everyone and we're going to look inside the source code of the fast AI", "tokens": [583, 341, 565, 321, 434, 516, 281, 2528, 666, 264, 2607, 295, 1518, 293, 321, 434, 516, 281, 574, 1854, 264, 4009, 3089, 295, 264, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.16603834969656808, "compression_ratio": 1.798165137614679, "no_speech_prob": 3.041574018425308e-06}, {"id": 115, "seek": 49532, "start": 509.56, "end": 512.2, "text": " Library to see what it's doing and try to replicate", "tokens": [12806, 281, 536, 437, 309, 311, 884, 293, 853, 281, 25356], "temperature": 0.0, "avg_logprob": -0.16603834969656808, "compression_ratio": 1.798165137614679, "no_speech_prob": 3.041574018425308e-06}, {"id": 116, "seek": 49532, "start": 513.0, "end": 514.12, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.16603834969656808, "compression_ratio": 1.798165137614679, "no_speech_prob": 3.041574018425308e-06}, {"id": 117, "seek": 49532, "start": 514.12, "end": 518.68, "text": " So in a sense like there's not going to be a lot more", "tokens": [407, 294, 257, 2020, 411, 456, 311, 406, 516, 281, 312, 257, 688, 544], "temperature": 0.0, "avg_logprob": -0.16603834969656808, "compression_ratio": 1.798165137614679, "no_speech_prob": 3.041574018425308e-06}, {"id": 118, "seek": 51868, "start": 518.68, "end": 525.88, "text": " Best practices to show you like I've kind of shown you the best best practices", "tokens": [9752, 7525, 281, 855, 291, 411, 286, 600, 733, 295, 4898, 291, 264, 1151, 1151, 7525], "temperature": 0.0, "avg_logprob": -0.1580501519716703, "compression_ratio": 1.6947791164658634, "no_speech_prob": 2.123363174177939e-06}, {"id": 119, "seek": 51868, "start": 525.88, "end": 526.56, "text": " I know", "tokens": [286, 458], "temperature": 0.0, "avg_logprob": -0.1580501519716703, "compression_ratio": 1.6947791164658634, "no_speech_prob": 2.123363174177939e-06}, {"id": 120, "seek": 51868, "start": 526.56, "end": 530.8, "text": " But I feel like for us to now build on top of those to debug those models", "tokens": [583, 286, 841, 411, 337, 505, 281, 586, 1322, 322, 1192, 295, 729, 281, 24083, 729, 5245], "temperature": 0.0, "avg_logprob": -0.1580501519716703, "compression_ratio": 1.6947791164658634, "no_speech_prob": 2.123363174177939e-06}, {"id": 121, "seek": 51868, "start": 530.92, "end": 534.9, "text": " To come back to part two where we're going to kind of try out some new things", "tokens": [1407, 808, 646, 281, 644, 732, 689, 321, 434, 516, 281, 733, 295, 853, 484, 512, 777, 721], "temperature": 0.0, "avg_logprob": -0.1580501519716703, "compression_ratio": 1.6947791164658634, "no_speech_prob": 2.123363174177939e-06}, {"id": 122, "seek": 51868, "start": 535.1999999999999, "end": 539.92, "text": " You know it really helps to understand what's going on behind the scenes, okay?", "tokens": [509, 458, 309, 534, 3665, 281, 1223, 437, 311, 516, 322, 2261, 264, 8026, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1580501519716703, "compression_ratio": 1.6947791164658634, "no_speech_prob": 2.123363174177939e-06}, {"id": 123, "seek": 51868, "start": 539.92, "end": 544.62, "text": " so the goal here today is we're going to try and create a", "tokens": [370, 264, 3387, 510, 965, 307, 321, 434, 516, 281, 853, 293, 1884, 257], "temperature": 0.0, "avg_logprob": -0.1580501519716703, "compression_ratio": 1.6947791164658634, "no_speech_prob": 2.123363174177939e-06}, {"id": 124, "seek": 54462, "start": 544.62, "end": 547.94, "text": " pretty effective collaborative filtering model", "tokens": [1238, 4942, 16555, 30822, 2316], "temperature": 0.0, "avg_logprob": -0.20237594900779354, "compression_ratio": 1.7091633466135459, "no_speech_prob": 2.0904490156681277e-06}, {"id": 125, "seek": 54462, "start": 550.46, "end": 555.14, "text": " Almost entirely from scratch, so we'll use the kind of we'll use pytorch as a", "tokens": [12627, 7696, 490, 8459, 11, 370, 321, 603, 764, 264, 733, 295, 321, 603, 764, 25878, 284, 339, 382, 257], "temperature": 0.0, "avg_logprob": -0.20237594900779354, "compression_ratio": 1.7091633466135459, "no_speech_prob": 2.0904490156681277e-06}, {"id": 126, "seek": 54462, "start": 555.9, "end": 562.22, "text": " Automatic differentiation tool and as a GPU programming tool and not very much else. We'll try not to use its neural net features", "tokens": [6049, 13143, 38902, 2290, 293, 382, 257, 18407, 9410, 2290, 293, 406, 588, 709, 1646, 13, 492, 603, 853, 406, 281, 764, 1080, 18161, 2533, 4122], "temperature": 0.0, "avg_logprob": -0.20237594900779354, "compression_ratio": 1.7091633466135459, "no_speech_prob": 2.0904490156681277e-06}, {"id": 127, "seek": 54462, "start": 562.22, "end": 563.78, "text": " We'll try not to use", "tokens": [492, 603, 853, 406, 281, 764], "temperature": 0.0, "avg_logprob": -0.20237594900779354, "compression_ratio": 1.7091633466135459, "no_speech_prob": 2.0904490156681277e-06}, {"id": 128, "seek": 54462, "start": 563.78, "end": 567.34, "text": " Fast AI library any more than necessary so that's the goal", "tokens": [15968, 7318, 6405, 604, 544, 813, 4818, 370, 300, 311, 264, 3387], "temperature": 0.0, "avg_logprob": -0.20237594900779354, "compression_ratio": 1.7091633466135459, "no_speech_prob": 2.0904490156681277e-06}, {"id": 129, "seek": 54462, "start": 567.98, "end": 569.34, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.20237594900779354, "compression_ratio": 1.7091633466135459, "no_speech_prob": 2.0904490156681277e-06}, {"id": 130, "seek": 54462, "start": 569.34, "end": 573.44, "text": " Let's go back and you know we only very quickly looked at collaborative filtering last time", "tokens": [961, 311, 352, 646, 293, 291, 458, 321, 787, 588, 2661, 2956, 412, 16555, 30822, 1036, 565], "temperature": 0.0, "avg_logprob": -0.20237594900779354, "compression_ratio": 1.7091633466135459, "no_speech_prob": 2.0904490156681277e-06}, {"id": 131, "seek": 57344, "start": 573.44, "end": 578.1, "text": " So let's let's go back and have a look at collaborative filtering and so we're going to look at this", "tokens": [407, 718, 311, 718, 311, 352, 646, 293, 362, 257, 574, 412, 16555, 30822, 293, 370, 321, 434, 516, 281, 574, 412, 341], "temperature": 0.0, "avg_logprob": -0.14866039969704367, "compression_ratio": 1.7914691943127963, "no_speech_prob": 1.1015932841473841e-06}, {"id": 132, "seek": 57344, "start": 579.1, "end": 581.1, "text": " movie lens data set", "tokens": [3169, 6765, 1412, 992], "temperature": 0.0, "avg_logprob": -0.14866039969704367, "compression_ratio": 1.7914691943127963, "no_speech_prob": 1.1015932841473841e-06}, {"id": 133, "seek": 57344, "start": 582.1, "end": 583.5400000000001, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.14866039969704367, "compression_ratio": 1.7914691943127963, "no_speech_prob": 1.1015932841473841e-06}, {"id": 134, "seek": 57344, "start": 583.5400000000001, "end": 585.5400000000001, "text": " the movie lens data set", "tokens": [264, 3169, 6765, 1412, 992], "temperature": 0.0, "avg_logprob": -0.14866039969704367, "compression_ratio": 1.7914691943127963, "no_speech_prob": 1.1015932841473841e-06}, {"id": 135, "seek": 57344, "start": 586.82, "end": 588.58, "text": " Basically is a list of ratings", "tokens": [8537, 307, 257, 1329, 295, 24603], "temperature": 0.0, "avg_logprob": -0.14866039969704367, "compression_ratio": 1.7914691943127963, "no_speech_prob": 1.1015932841473841e-06}, {"id": 136, "seek": 57344, "start": 588.58, "end": 595.86, "text": " It's got a bunch of different users that are represented by some ID and a bunch of movies that are represented by some ID and", "tokens": [467, 311, 658, 257, 3840, 295, 819, 5022, 300, 366, 10379, 538, 512, 7348, 293, 257, 3840, 295, 6233, 300, 366, 10379, 538, 512, 7348, 293], "temperature": 0.0, "avg_logprob": -0.14866039969704367, "compression_ratio": 1.7914691943127963, "no_speech_prob": 1.1015932841473841e-06}, {"id": 137, "seek": 57344, "start": 596.5400000000001, "end": 600.5400000000001, "text": " Rating it also has a timestamp. I haven't actually ever tried to use this", "tokens": [497, 990, 309, 611, 575, 257, 49108, 1215, 13, 286, 2378, 380, 767, 1562, 3031, 281, 764, 341], "temperature": 0.0, "avg_logprob": -0.14866039969704367, "compression_ratio": 1.7914691943127963, "no_speech_prob": 1.1015932841473841e-06}, {"id": 138, "seek": 60054, "start": 600.54, "end": 604.66, "text": " I guess this is just like what what time did that person rate that movie?", "tokens": [286, 2041, 341, 307, 445, 411, 437, 437, 565, 630, 300, 954, 3314, 300, 3169, 30], "temperature": 0.0, "avg_logprob": -0.17608422672047336, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.3709504855796695e-06}, {"id": 139, "seek": 60054, "start": 606.9399999999999, "end": 609.5799999999999, "text": " So that's all we're going to use for modeling is", "tokens": [407, 300, 311, 439, 321, 434, 516, 281, 764, 337, 15983, 307], "temperature": 0.0, "avg_logprob": -0.17608422672047336, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.3709504855796695e-06}, {"id": 140, "seek": 60054, "start": 610.6999999999999, "end": 616.6999999999999, "text": " Three columns user ID movie ID and rating and so thinking of that in kind of", "tokens": [6244, 13766, 4195, 7348, 3169, 7348, 293, 10990, 293, 370, 1953, 295, 300, 294, 733, 295], "temperature": 0.0, "avg_logprob": -0.17608422672047336, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.3709504855796695e-06}, {"id": 141, "seek": 60054, "start": 617.02, "end": 620.8, "text": " Structured data terms user ID and movie ID would be categorical variables", "tokens": [745, 46847, 1412, 2115, 4195, 7348, 293, 3169, 7348, 576, 312, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.17608422672047336, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.3709504855796695e-06}, {"id": 142, "seek": 60054, "start": 621.5799999999999, "end": 626.5, "text": " We have two of them and rating would be a would be an independent variable", "tokens": [492, 362, 732, 295, 552, 293, 10990, 576, 312, 257, 576, 312, 364, 6695, 7006], "temperature": 0.0, "avg_logprob": -0.17608422672047336, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.3709504855796695e-06}, {"id": 143, "seek": 62650, "start": 626.5, "end": 633.02, "text": " We're not going to use this for modeling, but we can use it for looking at stuff later", "tokens": [492, 434, 406, 516, 281, 764, 341, 337, 15983, 11, 457, 321, 393, 764, 309, 337, 1237, 412, 1507, 1780], "temperature": 0.0, "avg_logprob": -0.1776979212858239, "compression_ratio": 1.6134453781512605, "no_speech_prob": 2.0580375803547213e-06}, {"id": 144, "seek": 62650, "start": 633.02, "end": 635.98, "text": " We can grab a list of the names of the movies as well", "tokens": [492, 393, 4444, 257, 1329, 295, 264, 5288, 295, 264, 6233, 382, 731], "temperature": 0.0, "avg_logprob": -0.1776979212858239, "compression_ratio": 1.6134453781512605, "no_speech_prob": 2.0580375803547213e-06}, {"id": 145, "seek": 62650, "start": 636.62, "end": 643.18, "text": " And you could use this genre information. I haven't tried to be interested if during the week anybody tries it and finds it helpful", "tokens": [400, 291, 727, 764, 341, 11022, 1589, 13, 286, 2378, 380, 3031, 281, 312, 3102, 498, 1830, 264, 1243, 4472, 9898, 309, 293, 10704, 309, 4961], "temperature": 0.0, "avg_logprob": -0.1776979212858239, "compression_ratio": 1.6134453781512605, "no_speech_prob": 2.0580375803547213e-06}, {"id": 146, "seek": 62650, "start": 643.98, "end": 646.48, "text": " My guess is you might not find it helpful. We'll see", "tokens": [1222, 2041, 307, 291, 1062, 406, 915, 309, 4961, 13, 492, 603, 536], "temperature": 0.0, "avg_logprob": -0.1776979212858239, "compression_ratio": 1.6134453781512605, "no_speech_prob": 2.0580375803547213e-06}, {"id": 147, "seek": 62650, "start": 648.42, "end": 653.54, "text": " So in order to kind of look at this better. I just grabbed", "tokens": [407, 294, 1668, 281, 733, 295, 574, 412, 341, 1101, 13, 286, 445, 18607], "temperature": 0.0, "avg_logprob": -0.1776979212858239, "compression_ratio": 1.6134453781512605, "no_speech_prob": 2.0580375803547213e-06}, {"id": 148, "seek": 65354, "start": 653.54, "end": 655.54, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.17305945313495139, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.555970124580199e-06}, {"id": 149, "seek": 65354, "start": 656.66, "end": 660.3399999999999, "text": " Users that have watched the most movies and the movies that have been the most watched", "tokens": [47092, 300, 362, 6337, 264, 881, 6233, 293, 264, 6233, 300, 362, 668, 264, 881, 6337], "temperature": 0.0, "avg_logprob": -0.17305945313495139, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.555970124580199e-06}, {"id": 150, "seek": 65354, "start": 661.2199999999999, "end": 665.62, "text": " And made a cross tab of it right so this is exactly the same data", "tokens": [400, 1027, 257, 3278, 4421, 295, 309, 558, 370, 341, 307, 2293, 264, 912, 1412], "temperature": 0.0, "avg_logprob": -0.17305945313495139, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.555970124580199e-06}, {"id": 151, "seek": 65354, "start": 665.78, "end": 670.9399999999999, "text": " But it's a subset and now rather than being user movie rating we've got user", "tokens": [583, 309, 311, 257, 25993, 293, 586, 2831, 813, 885, 4195, 3169, 10990, 321, 600, 658, 4195], "temperature": 0.0, "avg_logprob": -0.17305945313495139, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.555970124580199e-06}, {"id": 152, "seek": 65354, "start": 671.9, "end": 673.06, "text": " movie", "tokens": [3169], "temperature": 0.0, "avg_logprob": -0.17305945313495139, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.555970124580199e-06}, {"id": 153, "seek": 65354, "start": 673.06, "end": 674.5, "text": " rating", "tokens": [10990], "temperature": 0.0, "avg_logprob": -0.17305945313495139, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.555970124580199e-06}, {"id": 154, "seek": 65354, "start": 674.5, "end": 681.42, "text": " And so some users haven't watched some of these movies. That's why some of these are not a number, okay?", "tokens": [400, 370, 512, 5022, 2378, 380, 6337, 512, 295, 613, 6233, 13, 663, 311, 983, 512, 295, 613, 366, 406, 257, 1230, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.17305945313495139, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.555970124580199e-06}, {"id": 155, "seek": 68142, "start": 681.42, "end": 684.4599999999999, "text": " Then I copied that into Excel", "tokens": [1396, 286, 25365, 300, 666, 19060], "temperature": 0.0, "avg_logprob": -0.2804536317524157, "compression_ratio": 1.5069124423963134, "no_speech_prob": 1.5294058357540052e-06}, {"id": 156, "seek": 68142, "start": 685.78, "end": 690.38, "text": " And you'll see there's a thing called collab filter dot XLS", "tokens": [400, 291, 603, 536, 456, 311, 257, 551, 1219, 44228, 6608, 5893, 1783, 19198], "temperature": 0.0, "avg_logprob": -0.2804536317524157, "compression_ratio": 1.5069124423963134, "no_speech_prob": 1.5294058357540052e-06}, {"id": 157, "seek": 68142, "start": 690.9399999999999, "end": 693.9, "text": " If you don't see it there now, I'll make sure I've got it there by tomorrow", "tokens": [759, 291, 500, 380, 536, 309, 456, 586, 11, 286, 603, 652, 988, 286, 600, 658, 309, 456, 538, 4153], "temperature": 0.0, "avg_logprob": -0.2804536317524157, "compression_ratio": 1.5069124423963134, "no_speech_prob": 1.5294058357540052e-06}, {"id": 158, "seek": 68142, "start": 695.26, "end": 697.18, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2804536317524157, "compression_ratio": 1.5069124423963134, "no_speech_prob": 1.5294058357540052e-06}, {"id": 159, "seek": 68142, "start": 697.18, "end": 699.06, "text": " Here is where I've", "tokens": [1692, 307, 689, 286, 600], "temperature": 0.0, "avg_logprob": -0.2804536317524157, "compression_ratio": 1.5069124423963134, "no_speech_prob": 1.5294058357540052e-06}, {"id": 160, "seek": 68142, "start": 699.06, "end": 700.42, "text": " copied", "tokens": [25365], "temperature": 0.0, "avg_logprob": -0.2804536317524157, "compression_ratio": 1.5069124423963134, "no_speech_prob": 1.5294058357540052e-06}, {"id": 161, "seek": 68142, "start": 700.42, "end": 704.3399999999999, "text": " That table okay, so as I go through this like", "tokens": [663, 3199, 1392, 11, 370, 382, 286, 352, 807, 341, 411], "temperature": 0.0, "avg_logprob": -0.2804536317524157, "compression_ratio": 1.5069124423963134, "no_speech_prob": 1.5294058357540052e-06}, {"id": 162, "seek": 68142, "start": 705.02, "end": 709.86, "text": " Set up of the problem and kind of how it's described and stuff if you're ever feeling", "tokens": [8928, 493, 295, 264, 1154, 293, 733, 295, 577, 309, 311, 7619, 293, 1507, 498, 291, 434, 1562, 2633], "temperature": 0.0, "avg_logprob": -0.2804536317524157, "compression_ratio": 1.5069124423963134, "no_speech_prob": 1.5294058357540052e-06}, {"id": 163, "seek": 70986, "start": 709.86, "end": 711.86, "text": " lost feel free to", "tokens": [2731, 841, 1737, 281], "temperature": 0.0, "avg_logprob": -0.1801830760219641, "compression_ratio": 1.7164750957854407, "no_speech_prob": 1.5294052673198166e-06}, {"id": 164, "seek": 70986, "start": 712.22, "end": 715.38, "text": " Ask either directly or through the forum", "tokens": [12320, 2139, 3838, 420, 807, 264, 17542], "temperature": 0.0, "avg_logprob": -0.1801830760219641, "compression_ratio": 1.7164750957854407, "no_speech_prob": 1.5294052673198166e-06}, {"id": 165, "seek": 70986, "start": 715.9, "end": 720.38, "text": " If you ask through the forum and somebody answers there, I want you to answer it here", "tokens": [759, 291, 1029, 807, 264, 17542, 293, 2618, 6338, 456, 11, 286, 528, 291, 281, 1867, 309, 510], "temperature": 0.0, "avg_logprob": -0.1801830760219641, "compression_ratio": 1.7164750957854407, "no_speech_prob": 1.5294052673198166e-06}, {"id": 166, "seek": 70986, "start": 721.22, "end": 725.46, "text": " but if somebody else asks a question you would like answered of course just like it and", "tokens": [457, 498, 2618, 1646, 8962, 257, 1168, 291, 576, 411, 10103, 295, 1164, 445, 411, 309, 293], "temperature": 0.0, "avg_logprob": -0.1801830760219641, "compression_ratio": 1.7164750957854407, "no_speech_prob": 1.5294052673198166e-06}, {"id": 167, "seek": 70986, "start": 726.94, "end": 728.94, "text": " Your net will keep an eye out for that", "tokens": [2260, 2533, 486, 1066, 364, 3313, 484, 337, 300], "temperature": 0.0, "avg_logprob": -0.1801830760219641, "compression_ratio": 1.7164750957854407, "no_speech_prob": 1.5294052673198166e-06}, {"id": 168, "seek": 70986, "start": 729.38, "end": 733.62, "text": " Because kind of as we're digging in to the details of what's going on behind the scenes", "tokens": [1436, 733, 295, 382, 321, 434, 17343, 294, 281, 264, 4365, 295, 437, 311, 516, 322, 2261, 264, 8026], "temperature": 0.0, "avg_logprob": -0.1801830760219641, "compression_ratio": 1.7164750957854407, "no_speech_prob": 1.5294052673198166e-06}, {"id": 169, "seek": 70986, "start": 733.62, "end": 736.58, "text": " It's kind of important that at each stage you feel like okay. I can see", "tokens": [467, 311, 733, 295, 1021, 300, 412, 1184, 3233, 291, 841, 411, 1392, 13, 286, 393, 536], "temperature": 0.0, "avg_logprob": -0.1801830760219641, "compression_ratio": 1.7164750957854407, "no_speech_prob": 1.5294052673198166e-06}, {"id": 170, "seek": 70986, "start": 737.1, "end": 739.1, "text": " What's going on?", "tokens": [708, 311, 516, 322, 30], "temperature": 0.0, "avg_logprob": -0.1801830760219641, "compression_ratio": 1.7164750957854407, "no_speech_prob": 1.5294052673198166e-06}, {"id": 171, "seek": 73910, "start": 739.1, "end": 741.1, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.15594442774740497, "compression_ratio": 1.7198067632850242, "no_speech_prob": 2.0261334157112287e-06}, {"id": 172, "seek": 73910, "start": 743.82, "end": 750.02, "text": " Okay, so we're actually not going to build a neural net to start with", "tokens": [1033, 11, 370, 321, 434, 767, 406, 516, 281, 1322, 257, 18161, 2533, 281, 722, 365], "temperature": 0.0, "avg_logprob": -0.15594442774740497, "compression_ratio": 1.7198067632850242, "no_speech_prob": 2.0261334157112287e-06}, {"id": 173, "seek": 73910, "start": 751.26, "end": 753.78, "text": " Instead we're going to do something called a matrix factorization", "tokens": [7156, 321, 434, 516, 281, 360, 746, 1219, 257, 8141, 5952, 2144], "temperature": 0.0, "avg_logprob": -0.15594442774740497, "compression_ratio": 1.7198067632850242, "no_speech_prob": 2.0261334157112287e-06}, {"id": 174, "seek": 73910, "start": 756.0600000000001, "end": 761.14, "text": " The reason we're not going to build a neural net to start with is that it so happens. There's a really really simple", "tokens": [440, 1778, 321, 434, 406, 516, 281, 1322, 257, 18161, 2533, 281, 722, 365, 307, 300, 309, 370, 2314, 13, 821, 311, 257, 534, 534, 2199], "temperature": 0.0, "avg_logprob": -0.15594442774740497, "compression_ratio": 1.7198067632850242, "no_speech_prob": 2.0261334157112287e-06}, {"id": 175, "seek": 73910, "start": 761.98, "end": 767.5, "text": " Kind of way of solving these kinds of problems which I'm going to show you and so if I scroll down", "tokens": [9242, 295, 636, 295, 12606, 613, 3685, 295, 2740, 597, 286, 478, 516, 281, 855, 291, 293, 370, 498, 286, 11369, 760], "temperature": 0.0, "avg_logprob": -0.15594442774740497, "compression_ratio": 1.7198067632850242, "no_speech_prob": 2.0261334157112287e-06}, {"id": 176, "seek": 76750, "start": 767.5, "end": 774.7, "text": " I've basically what I've got here is the same the same thing but this time these are my predictions", "tokens": [286, 600, 1936, 437, 286, 600, 658, 510, 307, 264, 912, 264, 912, 551, 457, 341, 565, 613, 366, 452, 21264], "temperature": 0.0, "avg_logprob": -0.24648602803548178, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.7264570690022083e-06}, {"id": 177, "seek": 76750, "start": 775.14, "end": 783.1, "text": " Rather than my actuals, and I'm going to show you how I created these predictions. Okay, so here are my actuals right here are my predictions and", "tokens": [16571, 813, 452, 3539, 82, 11, 293, 286, 478, 516, 281, 855, 291, 577, 286, 2942, 613, 21264, 13, 1033, 11, 370, 510, 366, 452, 3539, 82, 558, 510, 366, 452, 21264, 293], "temperature": 0.0, "avg_logprob": -0.24648602803548178, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.7264570690022083e-06}, {"id": 178, "seek": 76750, "start": 783.94, "end": 785.94, "text": " then down here", "tokens": [550, 760, 510], "temperature": 0.0, "avg_logprob": -0.24648602803548178, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.7264570690022083e-06}, {"id": 179, "seek": 76750, "start": 786.06, "end": 787.74, "text": " we have", "tokens": [321, 362], "temperature": 0.0, "avg_logprob": -0.24648602803548178, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.7264570690022083e-06}, {"id": 180, "seek": 76750, "start": 787.74, "end": 788.98, "text": " our", "tokens": [527], "temperature": 0.0, "avg_logprob": -0.24648602803548178, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.7264570690022083e-06}, {"id": 181, "seek": 76750, "start": 788.98, "end": 792.66, "text": " Score which is the sum of the difference squared", "tokens": [47901, 597, 307, 264, 2408, 295, 264, 2649, 8889], "temperature": 0.0, "avg_logprob": -0.24648602803548178, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.7264570690022083e-06}, {"id": 182, "seek": 79266, "start": 792.66, "end": 799.9, "text": " Red average square root okay, so this is RMSE down here, okay, so on average we're", "tokens": [4477, 4274, 3732, 5593, 1392, 11, 370, 341, 307, 23790, 5879, 760, 510, 11, 1392, 11, 370, 322, 4274, 321, 434], "temperature": 0.0, "avg_logprob": -0.27179243126694036, "compression_ratio": 1.5495495495495495, "no_speech_prob": 3.9054680200933944e-06}, {"id": 183, "seek": 79266, "start": 801.14, "end": 804.3, "text": " Randomly initialized model is out by 2.8", "tokens": [37603, 356, 5883, 1602, 2316, 307, 484, 538, 568, 13, 23], "temperature": 0.0, "avg_logprob": -0.27179243126694036, "compression_ratio": 1.5495495495495495, "no_speech_prob": 3.9054680200933944e-06}, {"id": 184, "seek": 79266, "start": 804.4599999999999, "end": 809.5, "text": " So let me show you what this model is and I'm going to show you by saying how do we guess?", "tokens": [407, 718, 385, 855, 291, 437, 341, 2316, 307, 293, 286, 478, 516, 281, 855, 291, 538, 1566, 577, 360, 321, 2041, 30], "temperature": 0.0, "avg_logprob": -0.27179243126694036, "compression_ratio": 1.5495495495495495, "no_speech_prob": 3.9054680200933944e-06}, {"id": 185, "seek": 79266, "start": 809.98, "end": 813.3399999999999, "text": " How much user ID number 14?", "tokens": [1012, 709, 4195, 7348, 1230, 3499, 30], "temperature": 0.0, "avg_logprob": -0.27179243126694036, "compression_ratio": 1.5495495495495495, "no_speech_prob": 3.9054680200933944e-06}, {"id": 186, "seek": 79266, "start": 814.02, "end": 816.54, "text": " likes movie ID number 27 and", "tokens": [5902, 3169, 7348, 1230, 7634, 293], "temperature": 0.0, "avg_logprob": -0.27179243126694036, "compression_ratio": 1.5495495495495495, "no_speech_prob": 3.9054680200933944e-06}, {"id": 187, "seek": 79266, "start": 817.06, "end": 820.78, "text": " The prediction here. This is just at this stage. This is still random is", "tokens": [440, 17630, 510, 13, 639, 307, 445, 412, 341, 3233, 13, 639, 307, 920, 4974, 307], "temperature": 0.0, "avg_logprob": -0.27179243126694036, "compression_ratio": 1.5495495495495495, "no_speech_prob": 3.9054680200933944e-06}, {"id": 188, "seek": 82078, "start": 820.78, "end": 822.78, "text": " 0.91", "tokens": [1958, 13, 29925], "temperature": 0.0, "avg_logprob": -0.22724233928479645, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.0845138831427903e-06}, {"id": 189, "seek": 82078, "start": 823.78, "end": 828.66, "text": " So how are we calculating 0.91 and the answer is we're taking it as", "tokens": [407, 577, 366, 321, 28258, 1958, 13, 29925, 293, 264, 1867, 307, 321, 434, 1940, 309, 382], "temperature": 0.0, "avg_logprob": -0.22724233928479645, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.0845138831427903e-06}, {"id": 190, "seek": 82078, "start": 830.06, "end": 832.06, "text": " This vector here", "tokens": [639, 8062, 510], "temperature": 0.0, "avg_logprob": -0.22724233928479645, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.0845138831427903e-06}, {"id": 191, "seek": 82078, "start": 832.8199999999999, "end": 840.3399999999999, "text": " Dot product with this vector here, so dot product means point seven one times point one nine plus", "tokens": [38753, 1674, 365, 341, 8062, 510, 11, 370, 5893, 1674, 1355, 935, 3407, 472, 1413, 935, 472, 4949, 1804], "temperature": 0.0, "avg_logprob": -0.22724233928479645, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.0845138831427903e-06}, {"id": 192, "seek": 82078, "start": 840.86, "end": 844.9, "text": " 0.81 times point six three plus point seven four plus point three one and so forth and", "tokens": [1958, 13, 32875, 1413, 935, 2309, 1045, 1804, 935, 3407, 1451, 1804, 935, 1045, 472, 293, 370, 5220, 293], "temperature": 0.0, "avg_logprob": -0.22724233928479645, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.0845138831427903e-06}, {"id": 193, "seek": 82078, "start": 845.54, "end": 849.9399999999999, "text": " In you know linear algebra speak because one of them is a column and one of them is a row", "tokens": [682, 291, 458, 8213, 21989, 1710, 570, 472, 295, 552, 307, 257, 7738, 293, 472, 295, 552, 307, 257, 5386], "temperature": 0.0, "avg_logprob": -0.22724233928479645, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.0845138831427903e-06}, {"id": 194, "seek": 84994, "start": 849.94, "end": 855.3800000000001, "text": " This is the same as a matrix product so you can see here. I've used the Excel function matrix multiplier", "tokens": [639, 307, 264, 912, 382, 257, 8141, 1674, 370, 291, 393, 536, 510, 13, 286, 600, 1143, 264, 19060, 2445, 8141, 44106], "temperature": 0.0, "avg_logprob": -0.17132034301757812, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.1189406450284878e-06}, {"id": 195, "seek": 84994, "start": 856.5400000000001, "end": 858.5400000000001, "text": " And that's my prediction", "tokens": [400, 300, 311, 452, 17630], "temperature": 0.0, "avg_logprob": -0.17132034301757812, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.1189406450284878e-06}, {"id": 196, "seek": 84994, "start": 860.1400000000001, "end": 863.3800000000001, "text": " Having said that if the original", "tokens": [10222, 848, 300, 498, 264, 3380], "temperature": 0.0, "avg_logprob": -0.17132034301757812, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.1189406450284878e-06}, {"id": 197, "seek": 84994, "start": 865.2600000000001, "end": 867.2600000000001, "text": " Rating doesn't exist at all", "tokens": [497, 990, 1177, 380, 2514, 412, 439], "temperature": 0.0, "avg_logprob": -0.17132034301757812, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.1189406450284878e-06}, {"id": 198, "seek": 84994, "start": 868.62, "end": 874.6600000000001, "text": " Then I'm just going to set this to zero right because like there's no error in predicting something that hasn't happened", "tokens": [1396, 286, 478, 445, 516, 281, 992, 341, 281, 4018, 558, 570, 411, 456, 311, 572, 6713, 294, 32884, 746, 300, 6132, 380, 2011], "temperature": 0.0, "avg_logprob": -0.17132034301757812, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.1189406450284878e-06}, {"id": 199, "seek": 87466, "start": 874.66, "end": 881.54, "text": " Okay, so what I'm going to do is I'm basically going to say all right every one of my rate rate my predictions is not", "tokens": [1033, 11, 370, 437, 286, 478, 516, 281, 360, 307, 286, 478, 1936, 516, 281, 584, 439, 558, 633, 472, 295, 452, 3314, 3314, 452, 21264, 307, 406], "temperature": 0.0, "avg_logprob": -0.20002507639455272, "compression_ratio": 1.7989949748743719, "no_speech_prob": 9.276349715037213e-07}, {"id": 200, "seek": 87466, "start": 881.54, "end": 884.38, "text": " Going to be a neural net. It's going to be a single", "tokens": [10963, 281, 312, 257, 18161, 2533, 13, 467, 311, 516, 281, 312, 257, 2167], "temperature": 0.0, "avg_logprob": -0.20002507639455272, "compression_ratio": 1.7989949748743719, "no_speech_prob": 9.276349715037213e-07}, {"id": 201, "seek": 87466, "start": 885.02, "end": 887.02, "text": " matrix multiplication", "tokens": [8141, 27290], "temperature": 0.0, "avg_logprob": -0.20002507639455272, "compression_ratio": 1.7989949748743719, "no_speech_prob": 9.276349715037213e-07}, {"id": 202, "seek": 87466, "start": 887.98, "end": 895.66, "text": " Now the matrix multiplication that it's doing is basically in practice is between like this", "tokens": [823, 264, 8141, 27290, 300, 309, 311, 884, 307, 1936, 294, 3124, 307, 1296, 411, 341], "temperature": 0.0, "avg_logprob": -0.20002507639455272, "compression_ratio": 1.7989949748743719, "no_speech_prob": 9.276349715037213e-07}, {"id": 203, "seek": 87466, "start": 896.38, "end": 898.38, "text": " matrix and", "tokens": [8141, 293], "temperature": 0.0, "avg_logprob": -0.20002507639455272, "compression_ratio": 1.7989949748743719, "no_speech_prob": 9.276349715037213e-07}, {"id": 204, "seek": 87466, "start": 898.8199999999999, "end": 900.14, "text": " this", "tokens": [341], "temperature": 0.0, "avg_logprob": -0.20002507639455272, "compression_ratio": 1.7989949748743719, "no_speech_prob": 9.276349715037213e-07}, {"id": 205, "seek": 90014, "start": 900.14, "end": 904.6, "text": " Matrix right so each one of these is a single part of that", "tokens": [36274, 558, 370, 1184, 472, 295, 613, 307, 257, 2167, 644, 295, 300], "temperature": 0.0, "avg_logprob": -0.18644299919222607, "compression_ratio": 1.618811881188119, "no_speech_prob": 1.3081734095976572e-06}, {"id": 206, "seek": 90014, "start": 908.66, "end": 912.36, "text": " So I randomly initialize these these are just random numbers", "tokens": [407, 286, 16979, 5883, 1125, 613, 613, 366, 445, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.18644299919222607, "compression_ratio": 1.618811881188119, "no_speech_prob": 1.3081734095976572e-06}, {"id": 207, "seek": 90014, "start": 913.42, "end": 915.42, "text": " That I've just pasted in here", "tokens": [663, 286, 600, 445, 1791, 292, 294, 510], "temperature": 0.0, "avg_logprob": -0.18644299919222607, "compression_ratio": 1.618811881188119, "no_speech_prob": 1.3081734095976572e-06}, {"id": 208, "seek": 90014, "start": 915.8199999999999, "end": 917.9399999999999, "text": " So I've basically started off with two", "tokens": [407, 286, 600, 1936, 1409, 766, 365, 732], "temperature": 0.0, "avg_logprob": -0.18644299919222607, "compression_ratio": 1.618811881188119, "no_speech_prob": 1.3081734095976572e-06}, {"id": 209, "seek": 90014, "start": 918.74, "end": 926.1, "text": " Random matrices, and I've said let's assume for the time being that every rating can be represented as", "tokens": [37603, 32284, 11, 293, 286, 600, 848, 718, 311, 6552, 337, 264, 565, 885, 300, 633, 10990, 393, 312, 10379, 382], "temperature": 0.0, "avg_logprob": -0.18644299919222607, "compression_ratio": 1.618811881188119, "no_speech_prob": 1.3081734095976572e-06}, {"id": 210, "seek": 92610, "start": 926.1, "end": 929.4200000000001, "text": " the the matrix product of those two", "tokens": [264, 264, 8141, 1674, 295, 729, 732], "temperature": 0.0, "avg_logprob": -0.17170945393670464, "compression_ratio": 1.5793991416309012, "no_speech_prob": 1.1015935115210596e-06}, {"id": 211, "seek": 92610, "start": 930.58, "end": 934.22, "text": " So then in Excel you can actually do gradient descent", "tokens": [407, 550, 294, 19060, 291, 393, 767, 360, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.17170945393670464, "compression_ratio": 1.5793991416309012, "no_speech_prob": 1.1015935115210596e-06}, {"id": 212, "seek": 92610, "start": 937.22, "end": 942.94, "text": " You have to go to your options to the add-in section and and check the box to say turn it on and once you do", "tokens": [509, 362, 281, 352, 281, 428, 3956, 281, 264, 909, 12, 259, 3541, 293, 293, 1520, 264, 2424, 281, 584, 1261, 309, 322, 293, 1564, 291, 360], "temperature": 0.0, "avg_logprob": -0.17170945393670464, "compression_ratio": 1.5793991416309012, "no_speech_prob": 1.1015935115210596e-06}, {"id": 213, "seek": 92610, "start": 942.94, "end": 948.26, "text": " You'll see there's something there called solver, and if I go solver it says okay. What's your?", "tokens": [509, 603, 536, 456, 311, 746, 456, 1219, 1404, 331, 11, 293, 498, 286, 352, 1404, 331, 309, 1619, 1392, 13, 708, 311, 428, 30], "temperature": 0.0, "avg_logprob": -0.17170945393670464, "compression_ratio": 1.5793991416309012, "no_speech_prob": 1.1015935115210596e-06}, {"id": 214, "seek": 92610, "start": 949.22, "end": 954.02, "text": " Objective function, and you just choose the cell so in this case we chose", "tokens": [24753, 488, 2445, 11, 293, 291, 445, 2826, 264, 2815, 370, 294, 341, 1389, 321, 5111], "temperature": 0.0, "avg_logprob": -0.17170945393670464, "compression_ratio": 1.5793991416309012, "no_speech_prob": 1.1015935115210596e-06}, {"id": 215, "seek": 95402, "start": 954.02, "end": 956.9, "text": " the cell that contains our rip-need-square error and", "tokens": [264, 2815, 300, 8306, 527, 12782, 12, 716, 292, 12, 33292, 543, 6713, 293], "temperature": 0.0, "avg_logprob": -0.35916314073788225, "compression_ratio": 1.6854460093896713, "no_speech_prob": 3.1381364351545926e-06}, {"id": 216, "seek": 95402, "start": 957.6999999999999, "end": 960.5799999999999, "text": " Then it says okay. What do you want to?", "tokens": [1396, 309, 1619, 1392, 13, 708, 360, 291, 528, 281, 30], "temperature": 0.0, "avg_logprob": -0.35916314073788225, "compression_ratio": 1.6854460093896713, "no_speech_prob": 3.1381364351545926e-06}, {"id": 217, "seek": 95402, "start": 961.3, "end": 963.14, "text": " Change and you can see here", "tokens": [15060, 293, 291, 393, 536, 510], "temperature": 0.0, "avg_logprob": -0.35916314073788225, "compression_ratio": 1.6854460093896713, "no_speech_prob": 3.1381364351545926e-06}, {"id": 218, "seek": 95402, "start": 963.14, "end": 967.78, "text": " We've selected this matrix and this matrix and so it's going to do a gradient descent", "tokens": [492, 600, 8209, 341, 8141, 293, 341, 8141, 293, 370, 309, 311, 516, 281, 360, 257, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.35916314073788225, "compression_ratio": 1.6854460093896713, "no_speech_prob": 3.1381364351545926e-06}, {"id": 219, "seek": 95402, "start": 968.22, "end": 975.46, "text": " For us by changing these matrices to try and in this case minimize this is min minimize this excel", "tokens": [1171, 505, 538, 4473, 613, 32284, 281, 853, 293, 294, 341, 1389, 17522, 341, 307, 923, 17522, 341, 24015], "temperature": 0.0, "avg_logprob": -0.35916314073788225, "compression_ratio": 1.6854460093896713, "no_speech_prob": 3.1381364351545926e-06}, {"id": 220, "seek": 95402, "start": 975.9, "end": 981.36, "text": " Cell right grg nonlinear is a gradient descent method", "tokens": [28859, 558, 677, 70, 2107, 28263, 307, 257, 16235, 23475, 3170], "temperature": 0.0, "avg_logprob": -0.35916314073788225, "compression_ratio": 1.6854460093896713, "no_speech_prob": 3.1381364351545926e-06}, {"id": 221, "seek": 98136, "start": 981.36, "end": 984.48, "text": " so I say solve and you'll see it starts at 2.8 and", "tokens": [370, 286, 584, 5039, 293, 291, 603, 536, 309, 3719, 412, 568, 13, 23, 293], "temperature": 0.0, "avg_logprob": -0.2184038763647681, "compression_ratio": 1.6828193832599119, "no_speech_prob": 1.8448203036314226e-06}, {"id": 222, "seek": 98136, "start": 985.6800000000001, "end": 988.24, "text": " Then down here. You'll see that numbers going down", "tokens": [1396, 760, 510, 13, 509, 603, 536, 300, 3547, 516, 760], "temperature": 0.0, "avg_logprob": -0.2184038763647681, "compression_ratio": 1.6828193832599119, "no_speech_prob": 1.8448203036314226e-06}, {"id": 223, "seek": 98136, "start": 988.8000000000001, "end": 993.0, "text": " It's not actually showing us what it's doing, but we can see that the numbers going down, so", "tokens": [467, 311, 406, 767, 4099, 505, 437, 309, 311, 884, 11, 457, 321, 393, 536, 300, 264, 3547, 516, 760, 11, 370], "temperature": 0.0, "avg_logprob": -0.2184038763647681, "compression_ratio": 1.6828193832599119, "no_speech_prob": 1.8448203036314226e-06}, {"id": 224, "seek": 98136, "start": 994.04, "end": 996.04, "text": " this is kind of got a", "tokens": [341, 307, 733, 295, 658, 257], "temperature": 0.0, "avg_logprob": -0.2184038763647681, "compression_ratio": 1.6828193832599119, "no_speech_prob": 1.8448203036314226e-06}, {"id": 225, "seek": 98136, "start": 996.72, "end": 1001.48, "text": " Neural netty feel to it in that we're doing like a matrix product, and we're doing a gradient descent", "tokens": [1734, 1807, 2533, 874, 841, 281, 309, 294, 300, 321, 434, 884, 411, 257, 8141, 1674, 11, 293, 321, 434, 884, 257, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.2184038763647681, "compression_ratio": 1.6828193832599119, "no_speech_prob": 1.8448203036314226e-06}, {"id": 226, "seek": 98136, "start": 1001.72, "end": 1004.2, "text": " But we don't have a nonlinear", "tokens": [583, 321, 500, 380, 362, 257, 2107, 28263], "temperature": 0.0, "avg_logprob": -0.2184038763647681, "compression_ratio": 1.6828193832599119, "no_speech_prob": 1.8448203036314226e-06}, {"id": 227, "seek": 98136, "start": 1005.0, "end": 1007.3000000000001, "text": " Layer, and we don't have a second", "tokens": [35166, 11, 293, 321, 500, 380, 362, 257, 1150], "temperature": 0.0, "avg_logprob": -0.2184038763647681, "compression_ratio": 1.6828193832599119, "no_speech_prob": 1.8448203036314226e-06}, {"id": 228, "seek": 100730, "start": 1007.3, "end": 1014.06, "text": " Linear layer on top of that so we don't get to call this deep learning so things where people do like deep learning is", "tokens": [14670, 289, 4583, 322, 1192, 295, 300, 370, 321, 500, 380, 483, 281, 818, 341, 2452, 2539, 370, 721, 689, 561, 360, 411, 2452, 2539, 307], "temperature": 0.0, "avg_logprob": -0.18213594610040837, "compression_ratio": 1.71484375, "no_speech_prob": 5.338131813914515e-06}, {"id": 229, "seek": 100730, "start": 1014.06, "end": 1016.06, "text": " things where they have kind of", "tokens": [721, 689, 436, 362, 733, 295], "temperature": 0.0, "avg_logprob": -0.18213594610040837, "compression_ratio": 1.71484375, "no_speech_prob": 5.338131813914515e-06}, {"id": 230, "seek": 100730, "start": 1016.66, "end": 1023.4599999999999, "text": " Metrics products and gradient descents, but it's not deep people tend to just call that shallow learning okay, so we're doing shallow learning here", "tokens": [6377, 10716, 3383, 293, 16235, 7471, 791, 11, 457, 309, 311, 406, 2452, 561, 3928, 281, 445, 818, 300, 20488, 2539, 1392, 11, 370, 321, 434, 884, 20488, 2539, 510], "temperature": 0.0, "avg_logprob": -0.18213594610040837, "compression_ratio": 1.71484375, "no_speech_prob": 5.338131813914515e-06}, {"id": 231, "seek": 100730, "start": 1024.74, "end": 1028.26, "text": " Right so I'm just going to go ahead and press escape to stop it because I'm sick of waiting", "tokens": [1779, 370, 286, 478, 445, 516, 281, 352, 2286, 293, 1886, 7615, 281, 1590, 309, 570, 286, 478, 4998, 295, 3806], "temperature": 0.0, "avg_logprob": -0.18213594610040837, "compression_ratio": 1.71484375, "no_speech_prob": 5.338131813914515e-06}, {"id": 232, "seek": 100730, "start": 1029.54, "end": 1035.3, "text": " And so you can see we've now got down to the 0.39", "tokens": [400, 370, 291, 393, 536, 321, 600, 586, 658, 760, 281, 264, 1958, 13, 12493], "temperature": 0.0, "avg_logprob": -0.18213594610040837, "compression_ratio": 1.71484375, "no_speech_prob": 5.338131813914515e-06}, {"id": 233, "seek": 103530, "start": 1035.3, "end": 1037.3, "text": " All right, so for example", "tokens": [1057, 558, 11, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.2584603373209635, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.459373379475437e-06}, {"id": 234, "seek": 103530, "start": 1039.18, "end": 1047.26, "text": " It guessed that movie 72 for sorry movie 27 for user 72 would get 4.44 rating", "tokens": [467, 21852, 300, 3169, 18731, 337, 2597, 3169, 7634, 337, 4195, 18731, 576, 483, 1017, 13, 13912, 10990], "temperature": 0.0, "avg_logprob": -0.2584603373209635, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.459373379475437e-06}, {"id": 235, "seek": 103530, "start": 1049.18, "end": 1054.86, "text": " 27 72 it actually got a 4 rating so you can see like it's it's it's doing something quite useful", "tokens": [7634, 18731, 309, 767, 658, 257, 1017, 10990, 370, 291, 393, 536, 411, 309, 311, 309, 311, 309, 311, 884, 746, 1596, 4420], "temperature": 0.0, "avg_logprob": -0.2584603373209635, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.459373379475437e-06}, {"id": 236, "seek": 103530, "start": 1057.18, "end": 1058.46, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.2584603373209635, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.459373379475437e-06}, {"id": 237, "seek": 103530, "start": 1058.46, "end": 1062.68, "text": " Why is it doing something quite useful? I mean something to note here is", "tokens": [1545, 307, 309, 884, 746, 1596, 4420, 30, 286, 914, 746, 281, 3637, 510, 307], "temperature": 0.0, "avg_logprob": -0.2584603373209635, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.459373379475437e-06}, {"id": 238, "seek": 106268, "start": 1062.68, "end": 1067.96, "text": " The number of things we're trying to predict here is there's 225 of them", "tokens": [440, 1230, 295, 721, 321, 434, 1382, 281, 6069, 510, 307, 456, 311, 5853, 20, 295, 552], "temperature": 0.0, "avg_logprob": -0.1774674229238225, "compression_ratio": 1.6974358974358974, "no_speech_prob": 2.2603123852604767e-06}, {"id": 239, "seek": 106268, "start": 1068.64, "end": 1070.0800000000002, "text": " right and", "tokens": [558, 293], "temperature": 0.0, "avg_logprob": -0.1774674229238225, "compression_ratio": 1.6974358974358974, "no_speech_prob": 2.2603123852604767e-06}, {"id": 240, "seek": 106268, "start": 1070.0800000000002, "end": 1075.14, "text": " The number of things we're using to predict is that times 2 so 150 of them", "tokens": [440, 1230, 295, 721, 321, 434, 1228, 281, 6069, 307, 300, 1413, 568, 370, 8451, 295, 552], "temperature": 0.0, "avg_logprob": -0.1774674229238225, "compression_ratio": 1.6974358974358974, "no_speech_prob": 2.2603123852604767e-06}, {"id": 241, "seek": 106268, "start": 1075.24, "end": 1078.8, "text": " So it's not like we can just exactly fit we actually have to do some kind of", "tokens": [407, 309, 311, 406, 411, 321, 393, 445, 2293, 3318, 321, 767, 362, 281, 360, 512, 733, 295], "temperature": 0.0, "avg_logprob": -0.1774674229238225, "compression_ratio": 1.6974358974358974, "no_speech_prob": 2.2603123852604767e-06}, {"id": 242, "seek": 106268, "start": 1079.5600000000002, "end": 1081.4, "text": " machine learning here", "tokens": [3479, 2539, 510], "temperature": 0.0, "avg_logprob": -0.1774674229238225, "compression_ratio": 1.6974358974358974, "no_speech_prob": 2.2603123852604767e-06}, {"id": 243, "seek": 106268, "start": 1081.4, "end": 1086.5600000000002, "text": " so basically what this is saying is that there does seem to be some way of", "tokens": [370, 1936, 437, 341, 307, 1566, 307, 300, 456, 775, 1643, 281, 312, 512, 636, 295], "temperature": 0.0, "avg_logprob": -0.1774674229238225, "compression_ratio": 1.6974358974358974, "no_speech_prob": 2.2603123852604767e-06}, {"id": 244, "seek": 108656, "start": 1086.56, "end": 1093.08, "text": " Making predictions in this way and so for those of you that have done some linear algebra", "tokens": [14595, 21264, 294, 341, 636, 293, 370, 337, 729, 295, 291, 300, 362, 1096, 512, 8213, 21989], "temperature": 0.0, "avg_logprob": -0.14229551274725732, "compression_ratio": 1.7346153846153847, "no_speech_prob": 1.1365592627043952e-06}, {"id": 245, "seek": 108656, "start": 1093.36, "end": 1097.6799999999998, "text": " This is actually a matrix decomposition normally in linear algebra", "tokens": [639, 307, 767, 257, 8141, 48356, 5646, 294, 8213, 21989], "temperature": 0.0, "avg_logprob": -0.14229551274725732, "compression_ratio": 1.7346153846153847, "no_speech_prob": 1.1365592627043952e-06}, {"id": 246, "seek": 108656, "start": 1097.6799999999998, "end": 1104.34, "text": " You would do this using an analytical technique or using some techniques that are specifically designed for this purpose", "tokens": [509, 576, 360, 341, 1228, 364, 29579, 6532, 420, 1228, 512, 7512, 300, 366, 4682, 4761, 337, 341, 4334], "temperature": 0.0, "avg_logprob": -0.14229551274725732, "compression_ratio": 1.7346153846153847, "no_speech_prob": 1.1365592627043952e-06}, {"id": 247, "seek": 108656, "start": 1104.6399999999999, "end": 1106.6399999999999, "text": " But the nice thing is that we can use", "tokens": [583, 264, 1481, 551, 307, 300, 321, 393, 764], "temperature": 0.0, "avg_logprob": -0.14229551274725732, "compression_ratio": 1.7346153846153847, "no_speech_prob": 1.1365592627043952e-06}, {"id": 248, "seek": 108656, "start": 1107.28, "end": 1114.1599999999999, "text": " Gradient descent to solve pretty much everything including this I don't like to so much think of it from a linear algebra point of view", "tokens": [16710, 1196, 23475, 281, 5039, 1238, 709, 1203, 3009, 341, 286, 500, 380, 411, 281, 370, 709, 519, 295, 309, 490, 257, 8213, 21989, 935, 295, 1910], "temperature": 0.0, "avg_logprob": -0.14229551274725732, "compression_ratio": 1.7346153846153847, "no_speech_prob": 1.1365592627043952e-06}, {"id": 249, "seek": 111416, "start": 1114.16, "end": 1121.3600000000001, "text": " Though I think from an intuitive point of view which is this let's say movie. Sorry let's say movie ID 27 is", "tokens": [10404, 286, 519, 490, 364, 21769, 935, 295, 1910, 597, 307, 341, 718, 311, 584, 3169, 13, 4919, 718, 311, 584, 3169, 7348, 7634, 307], "temperature": 0.0, "avg_logprob": -0.25210816860198976, "compression_ratio": 1.6348314606741574, "no_speech_prob": 1.679724164205254e-06}, {"id": 250, "seek": 111416, "start": 1122.52, "end": 1124.16, "text": " Lord of the Rings", "tokens": [3257, 295, 264, 38543], "temperature": 0.0, "avg_logprob": -0.25210816860198976, "compression_ratio": 1.6348314606741574, "no_speech_prob": 1.679724164205254e-06}, {"id": 251, "seek": 111416, "start": 1124.16, "end": 1126.16, "text": " part one and", "tokens": [644, 472, 293], "temperature": 0.0, "avg_logprob": -0.25210816860198976, "compression_ratio": 1.6348314606741574, "no_speech_prob": 1.679724164205254e-06}, {"id": 252, "seek": 111416, "start": 1126.76, "end": 1128.76, "text": " Let's say", "tokens": [961, 311, 584], "temperature": 0.0, "avg_logprob": -0.25210816860198976, "compression_ratio": 1.6348314606741574, "no_speech_prob": 1.679724164205254e-06}, {"id": 253, "seek": 111416, "start": 1129.5600000000002, "end": 1133.5600000000002, "text": " Movie and so let's say we're trying to make that prediction for user", "tokens": [28766, 293, 370, 718, 311, 584, 321, 434, 1382, 281, 652, 300, 17630, 337, 4195], "temperature": 0.0, "avg_logprob": -0.25210816860198976, "compression_ratio": 1.6348314606741574, "no_speech_prob": 1.679724164205254e-06}, {"id": 254, "seek": 111416, "start": 1134.16, "end": 1139.24, "text": " 72 are they going to like Lord of the Rings part one and so conceptually", "tokens": [18731, 366, 436, 516, 281, 411, 3257, 295, 264, 38543, 644, 472, 293, 370, 3410, 671], "temperature": 0.0, "avg_logprob": -0.25210816860198976, "compression_ratio": 1.6348314606741574, "no_speech_prob": 1.679724164205254e-06}, {"id": 255, "seek": 113924, "start": 1139.24, "end": 1144.78, "text": " That particular movie. Maybe there's like this for sorry there's five", "tokens": [663, 1729, 3169, 13, 2704, 456, 311, 411, 341, 337, 2597, 456, 311, 1732], "temperature": 0.0, "avg_logprob": -0.20583150863647462, "compression_ratio": 1.7408906882591093, "no_speech_prob": 5.368742108657898e-07}, {"id": 256, "seek": 113924, "start": 1145.72, "end": 1146.52, "text": " numbers here", "tokens": [3547, 510], "temperature": 0.0, "avg_logprob": -0.20583150863647462, "compression_ratio": 1.7408906882591093, "no_speech_prob": 5.368742108657898e-07}, {"id": 257, "seek": 113924, "start": 1146.52, "end": 1153.9, "text": " And we could say like well what if the first one was like how much is it sci-fi and fantasy and the second one is like", "tokens": [400, 321, 727, 584, 411, 731, 437, 498, 264, 700, 472, 390, 411, 577, 709, 307, 309, 2180, 12, 13325, 293, 13861, 293, 264, 1150, 472, 307, 411], "temperature": 0.0, "avg_logprob": -0.20583150863647462, "compression_ratio": 1.7408906882591093, "no_speech_prob": 5.368742108657898e-07}, {"id": 258, "seek": 113924, "start": 1154.36, "end": 1161.04, "text": " How recent a movie and how much special effects is there you know and the one at the top might be like how dialogue driven?", "tokens": [1012, 5162, 257, 3169, 293, 577, 709, 2121, 5065, 307, 456, 291, 458, 293, 264, 472, 412, 264, 1192, 1062, 312, 411, 577, 10221, 9555, 30], "temperature": 0.0, "avg_logprob": -0.20583150863647462, "compression_ratio": 1.7408906882591093, "no_speech_prob": 5.368742108657898e-07}, {"id": 259, "seek": 113924, "start": 1161.56, "end": 1166.66, "text": " Right like let's say those kind of five these five numbers represented particular things about the movie", "tokens": [1779, 411, 718, 311, 584, 729, 733, 295, 1732, 613, 1732, 3547, 10379, 1729, 721, 466, 264, 3169], "temperature": 0.0, "avg_logprob": -0.20583150863647462, "compression_ratio": 1.7408906882591093, "no_speech_prob": 5.368742108657898e-07}, {"id": 260, "seek": 116666, "start": 1166.66, "end": 1173.64, "text": " And so if that was the case then we could have the same five numbers for the user saying like okay", "tokens": [400, 370, 498, 300, 390, 264, 1389, 550, 321, 727, 362, 264, 912, 1732, 3547, 337, 264, 4195, 1566, 411, 1392], "temperature": 0.0, "avg_logprob": -0.20744618330851639, "compression_ratio": 1.8565022421524664, "no_speech_prob": 9.132527907240728e-07}, {"id": 261, "seek": 116666, "start": 1173.64, "end": 1175.64, "text": " How much does the user like sci-fi and fantasy?", "tokens": [1012, 709, 775, 264, 4195, 411, 2180, 12, 13325, 293, 13861, 30], "temperature": 0.0, "avg_logprob": -0.20744618330851639, "compression_ratio": 1.8565022421524664, "no_speech_prob": 9.132527907240728e-07}, {"id": 262, "seek": 116666, "start": 1176.3200000000002, "end": 1178.3200000000002, "text": " How much does the user like?", "tokens": [1012, 709, 775, 264, 4195, 411, 30], "temperature": 0.0, "avg_logprob": -0.20744618330851639, "compression_ratio": 1.8565022421524664, "no_speech_prob": 9.132527907240728e-07}, {"id": 263, "seek": 116666, "start": 1178.76, "end": 1180.76, "text": " modern", "tokens": [4363], "temperature": 0.0, "avg_logprob": -0.20744618330851639, "compression_ratio": 1.8565022421524664, "no_speech_prob": 9.132527907240728e-07}, {"id": 264, "seek": 116666, "start": 1180.96, "end": 1187.3600000000001, "text": " Modern CGI driven movies how much does the does this user like dialogue driven movies?", "tokens": [19814, 48448, 9555, 6233, 577, 709, 775, 264, 775, 341, 4195, 411, 10221, 9555, 6233, 30], "temperature": 0.0, "avg_logprob": -0.20744618330851639, "compression_ratio": 1.8565022421524664, "no_speech_prob": 9.132527907240728e-07}, {"id": 265, "seek": 116666, "start": 1187.3600000000001, "end": 1189.3600000000001, "text": " And so if you then took that cross product", "tokens": [400, 370, 498, 291, 550, 1890, 300, 3278, 1674], "temperature": 0.0, "avg_logprob": -0.20744618330851639, "compression_ratio": 1.8565022421524664, "no_speech_prob": 9.132527907240728e-07}, {"id": 266, "seek": 118936, "start": 1189.36, "end": 1195.9599999999998, "text": " You would expect to have a good model right you expect to have a reasonable rating now the problem is", "tokens": [509, 576, 2066, 281, 362, 257, 665, 2316, 558, 291, 2066, 281, 362, 257, 10585, 10990, 586, 264, 1154, 307], "temperature": 0.0, "avg_logprob": -0.19327395430235106, "compression_ratio": 1.8663967611336032, "no_speech_prob": 3.632673042375245e-07}, {"id": 267, "seek": 118936, "start": 1196.52, "end": 1201.4399999999998, "text": " We don't have this information for each user. We don't have the information for each movie", "tokens": [492, 500, 380, 362, 341, 1589, 337, 1184, 4195, 13, 492, 500, 380, 362, 264, 1589, 337, 1184, 3169], "temperature": 0.0, "avg_logprob": -0.19327395430235106, "compression_ratio": 1.8663967611336032, "no_speech_prob": 3.632673042375245e-07}, {"id": 268, "seek": 118936, "start": 1201.76, "end": 1203.76, "text": " So we're just going to like assume", "tokens": [407, 321, 434, 445, 516, 281, 411, 6552], "temperature": 0.0, "avg_logprob": -0.19327395430235106, "compression_ratio": 1.8663967611336032, "no_speech_prob": 3.632673042375245e-07}, {"id": 269, "seek": 118936, "start": 1204.08, "end": 1206.08, "text": " That this is a reasonable", "tokens": [663, 341, 307, 257, 10585], "temperature": 0.0, "avg_logprob": -0.19327395430235106, "compression_ratio": 1.8663967611336032, "no_speech_prob": 3.632673042375245e-07}, {"id": 270, "seek": 118936, "start": 1206.08, "end": 1211.5, "text": " Kind of way of thinking about this system, and let's and let's stochastic gradient descent try and find these numbers", "tokens": [9242, 295, 636, 295, 1953, 466, 341, 1185, 11, 293, 718, 311, 293, 718, 311, 342, 8997, 2750, 16235, 23475, 853, 293, 915, 613, 3547], "temperature": 0.0, "avg_logprob": -0.19327395430235106, "compression_ratio": 1.8663967611336032, "no_speech_prob": 3.632673042375245e-07}, {"id": 271, "seek": 118936, "start": 1211.8799999999999, "end": 1218.56, "text": " Right so so in other words these these factors we call these things factors these factors", "tokens": [1779, 370, 370, 294, 661, 2283, 613, 613, 6771, 321, 818, 613, 721, 6771, 613, 6771], "temperature": 0.0, "avg_logprob": -0.19327395430235106, "compression_ratio": 1.8663967611336032, "no_speech_prob": 3.632673042375245e-07}, {"id": 272, "seek": 121856, "start": 1218.56, "end": 1222.52, "text": " And we call them factors because you can multiply them together to create this", "tokens": [400, 321, 818, 552, 6771, 570, 291, 393, 12972, 552, 1214, 281, 1884, 341], "temperature": 0.0, "avg_logprob": -0.21088198820749918, "compression_ratio": 1.9216589861751152, "no_speech_prob": 2.964903274005337e-07}, {"id": 273, "seek": 121856, "start": 1222.9199999999998, "end": 1228.06, "text": " But they're factors in a linear algebra sense these factors we call them latent factors", "tokens": [583, 436, 434, 6771, 294, 257, 8213, 21989, 2020, 613, 6771, 321, 818, 552, 48994, 6771], "temperature": 0.0, "avg_logprob": -0.21088198820749918, "compression_ratio": 1.9216589861751152, "no_speech_prob": 2.964903274005337e-07}, {"id": 274, "seek": 121856, "start": 1228.48, "end": 1232.12, "text": " because they're not actually this is not actually a", "tokens": [570, 436, 434, 406, 767, 341, 307, 406, 767, 257], "temperature": 0.0, "avg_logprob": -0.21088198820749918, "compression_ratio": 1.9216589861751152, "no_speech_prob": 2.964903274005337e-07}, {"id": 275, "seek": 121856, "start": 1233.1599999999999, "end": 1238.2, "text": " Vector that we've like named and understood and like entered in manually we've kind of", "tokens": [691, 20814, 300, 321, 600, 411, 4926, 293, 7320, 293, 411, 9065, 294, 16945, 321, 600, 733, 295], "temperature": 0.0, "avg_logprob": -0.21088198820749918, "compression_ratio": 1.9216589861751152, "no_speech_prob": 2.964903274005337e-07}, {"id": 276, "seek": 121856, "start": 1238.8, "end": 1240.04, "text": " assumed", "tokens": [15895], "temperature": 0.0, "avg_logprob": -0.21088198820749918, "compression_ratio": 1.9216589861751152, "no_speech_prob": 2.964903274005337e-07}, {"id": 277, "seek": 121856, "start": 1240.04, "end": 1246.76, "text": " That we can think of movie ratings this way we've assumed that we can think of them as a dot product of", "tokens": [663, 321, 393, 519, 295, 3169, 24603, 341, 636, 321, 600, 15895, 300, 321, 393, 519, 295, 552, 382, 257, 5893, 1674, 295], "temperature": 0.0, "avg_logprob": -0.21088198820749918, "compression_ratio": 1.9216589861751152, "no_speech_prob": 2.964903274005337e-07}, {"id": 278, "seek": 124676, "start": 1246.76, "end": 1255.16, "text": " Some particular features about a movie and some particular features of to look what users like those kinds of movies right and then we've used", "tokens": [2188, 1729, 4122, 466, 257, 3169, 293, 512, 1729, 4122, 295, 281, 574, 437, 5022, 411, 729, 3685, 295, 6233, 558, 293, 550, 321, 600, 1143], "temperature": 0.0, "avg_logprob": -0.21276966008273038, "compression_ratio": 1.7436974789915967, "no_speech_prob": 5.896405923522252e-07}, {"id": 279, "seek": 124676, "start": 1255.16, "end": 1256.48, "text": " gradient descent", "tokens": [16235, 23475], "temperature": 0.0, "avg_logprob": -0.21276966008273038, "compression_ratio": 1.7436974789915967, "no_speech_prob": 5.896405923522252e-07}, {"id": 280, "seek": 124676, "start": 1256.48, "end": 1260.02, "text": " To just say okay try and find some numbers that work", "tokens": [1407, 445, 584, 1392, 853, 293, 915, 512, 3547, 300, 589], "temperature": 0.0, "avg_logprob": -0.21276966008273038, "compression_ratio": 1.7436974789915967, "no_speech_prob": 5.896405923522252e-07}, {"id": 281, "seek": 124676, "start": 1261.96, "end": 1265.68, "text": " So that's that's basically the technique right and it's kind of", "tokens": [407, 300, 311, 300, 311, 1936, 264, 6532, 558, 293, 309, 311, 733, 295], "temperature": 0.0, "avg_logprob": -0.21276966008273038, "compression_ratio": 1.7436974789915967, "no_speech_prob": 5.896405923522252e-07}, {"id": 282, "seek": 124676, "start": 1266.96, "end": 1274.5, "text": " The and the entirety is in this spreadsheet right so that is collaborative filtering using what we call probabilistic matrix factorization", "tokens": [440, 293, 264, 31557, 307, 294, 341, 27733, 558, 370, 300, 307, 16555, 30822, 1228, 437, 321, 818, 31959, 3142, 8141, 5952, 2144], "temperature": 0.0, "avg_logprob": -0.21276966008273038, "compression_ratio": 1.7436974789915967, "no_speech_prob": 5.896405923522252e-07}, {"id": 283, "seek": 127450, "start": 1274.5, "end": 1280.14, "text": " And as you can see the whole thing is easy to do in an Excel spreadsheet and the entirety of it", "tokens": [400, 382, 291, 393, 536, 264, 1379, 551, 307, 1858, 281, 360, 294, 364, 19060, 27733, 293, 264, 31557, 295, 309], "temperature": 0.0, "avg_logprob": -0.2650263455449318, "compression_ratio": 1.5778688524590163, "no_speech_prob": 3.2887307952478295e-06}, {"id": 284, "seek": 127450, "start": 1280.14, "end": 1283.74, "text": " Really is this single thing which is a single matrix multiplication?", "tokens": [4083, 307, 341, 2167, 551, 597, 307, 257, 2167, 8141, 27290, 30], "temperature": 0.0, "avg_logprob": -0.2650263455449318, "compression_ratio": 1.5778688524590163, "no_speech_prob": 3.2887307952478295e-06}, {"id": 285, "seek": 127450, "start": 1284.86, "end": 1287.18, "text": " plus randomly initializing", "tokens": [1804, 16979, 5883, 3319], "temperature": 0.0, "avg_logprob": -0.2650263455449318, "compression_ratio": 1.5778688524590163, "no_speech_prob": 3.2887307952478295e-06}, {"id": 286, "seek": 127450, "start": 1289.38, "end": 1296.34, "text": " We would like to know if it would be better to cap this to 0 and 5 maybe yeah", "tokens": [492, 576, 411, 281, 458, 498, 309, 576, 312, 1101, 281, 1410, 341, 281, 1958, 293, 1025, 1310, 1338], "temperature": 0.0, "avg_logprob": -0.2650263455449318, "compression_ratio": 1.5778688524590163, "no_speech_prob": 3.2887307952478295e-06}, {"id": 287, "seek": 127450, "start": 1297.42, "end": 1302.36, "text": " Yeah, we're going to do that later right. There's a whole lot of stuff. We can do to improve this. This is like our", "tokens": [865, 11, 321, 434, 516, 281, 360, 300, 1780, 558, 13, 821, 311, 257, 1379, 688, 295, 1507, 13, 492, 393, 360, 281, 3470, 341, 13, 639, 307, 411, 527], "temperature": 0.0, "avg_logprob": -0.2650263455449318, "compression_ratio": 1.5778688524590163, "no_speech_prob": 3.2887307952478295e-06}, {"id": 288, "seek": 130236, "start": 1302.36, "end": 1303.6399999999999, "text": " our", "tokens": [527], "temperature": 0.0, "avg_logprob": -0.2803027303595292, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.1875460990704596e-06}, {"id": 289, "seek": 130236, "start": 1303.6399999999999, "end": 1309.1999999999998, "text": " Simplest possible starting point right so so what we're going to do now is we're going to try and implement this", "tokens": [3998, 564, 377, 1944, 2891, 935, 558, 370, 370, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 853, 293, 4445, 341], "temperature": 0.0, "avg_logprob": -0.2803027303595292, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.1875460990704596e-06}, {"id": 290, "seek": 130236, "start": 1310.36, "end": 1312.36, "text": " in Python", "tokens": [294, 15329], "temperature": 0.0, "avg_logprob": -0.2803027303595292, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.1875460990704596e-06}, {"id": 291, "seek": 130236, "start": 1312.3999999999999, "end": 1317.7199999999998, "text": " And run it on the whole data set no question is how do you figure out how many?", "tokens": [400, 1190, 309, 322, 264, 1379, 1412, 992, 572, 1168, 307, 577, 360, 291, 2573, 484, 577, 867, 30], "temperature": 0.0, "avg_logprob": -0.2803027303595292, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.1875460990704596e-06}, {"id": 292, "seek": 130236, "start": 1318.36, "end": 1323.8799999999999, "text": " You know how it's clear how long are the matrix why is this five yeah, yeah?", "tokens": [509, 458, 577, 309, 311, 1850, 577, 938, 366, 264, 8141, 983, 307, 341, 1732, 1338, 11, 1338, 30], "temperature": 0.0, "avg_logprob": -0.2803027303595292, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.1875460990704596e-06}, {"id": 293, "seek": 130236, "start": 1324.8799999999999, "end": 1326.8799999999999, "text": " So something to think about", "tokens": [407, 746, 281, 519, 466], "temperature": 0.0, "avg_logprob": -0.2803027303595292, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.1875460990704596e-06}, {"id": 294, "seek": 130236, "start": 1327.28, "end": 1329.9599999999998, "text": " Given that this is like movie 49 right", "tokens": [18600, 300, 341, 307, 411, 3169, 16513, 558], "temperature": 0.0, "avg_logprob": -0.2803027303595292, "compression_ratio": 1.5765765765765767, "no_speech_prob": 3.1875460990704596e-06}, {"id": 295, "seek": 132996, "start": 1329.96, "end": 1331.6000000000001, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2384506418735166, "compression_ratio": 1.6910112359550562, "no_speech_prob": 4.6644444751109404e-07}, {"id": 296, "seek": 132996, "start": 1331.6000000000001, "end": 1333.74, "text": " We're looking at a rating for movie 49", "tokens": [492, 434, 1237, 412, 257, 10990, 337, 3169, 16513], "temperature": 0.0, "avg_logprob": -0.2384506418735166, "compression_ratio": 1.6910112359550562, "no_speech_prob": 4.6644444751109404e-07}, {"id": 297, "seek": 132996, "start": 1334.72, "end": 1339.04, "text": " Think about this. This is actually an embedding matrix and", "tokens": [6557, 466, 341, 13, 639, 307, 767, 364, 12240, 3584, 8141, 293], "temperature": 0.0, "avg_logprob": -0.2384506418735166, "compression_ratio": 1.6910112359550562, "no_speech_prob": 4.6644444751109404e-07}, {"id": 298, "seek": 132996, "start": 1340.6000000000001, "end": 1342.6000000000001, "text": " So this length is actually", "tokens": [407, 341, 4641, 307, 767], "temperature": 0.0, "avg_logprob": -0.2384506418735166, "compression_ratio": 1.6910112359550562, "no_speech_prob": 4.6644444751109404e-07}, {"id": 299, "seek": 132996, "start": 1343.32, "end": 1350.68, "text": " The size of the embedding matrix. I'm not saying this is an analogy. I'm saying it literally this is literally an embedding matrix", "tokens": [440, 2744, 295, 264, 12240, 3584, 8141, 13, 286, 478, 406, 1566, 341, 307, 364, 21663, 13, 286, 478, 1566, 309, 3736, 341, 307, 3736, 364, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.2384506418735166, "compression_ratio": 1.6910112359550562, "no_speech_prob": 4.6644444751109404e-07}, {"id": 300, "seek": 132996, "start": 1351.04, "end": 1354.44, "text": " We could have a one hot encoding where 72", "tokens": [492, 727, 362, 257, 472, 2368, 43430, 689, 18731], "temperature": 0.0, "avg_logprob": -0.2384506418735166, "compression_ratio": 1.6910112359550562, "no_speech_prob": 4.6644444751109404e-07}, {"id": 301, "seek": 135444, "start": 1354.44, "end": 1360.26, "text": " Where a 1 is in the 72nd position and so we'd like to look it up and it would return", "tokens": [2305, 257, 502, 307, 294, 264, 18731, 273, 2535, 293, 370, 321, 1116, 411, 281, 574, 309, 493, 293, 309, 576, 2736], "temperature": 0.0, "avg_logprob": -0.21479995804603655, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.851551456245943e-07}, {"id": 302, "seek": 135444, "start": 1360.6000000000001, "end": 1363.92, "text": " This list of five numbers so the question is actually", "tokens": [639, 1329, 295, 1732, 3547, 370, 264, 1168, 307, 767], "temperature": 0.0, "avg_logprob": -0.21479995804603655, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.851551456245943e-07}, {"id": 303, "seek": 135444, "start": 1364.64, "end": 1367.7, "text": " How do we decide on the dimensionality of our embedding vectors?", "tokens": [1012, 360, 321, 4536, 322, 264, 10139, 1860, 295, 527, 12240, 3584, 18875, 30], "temperature": 0.0, "avg_logprob": -0.21479995804603655, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.851551456245943e-07}, {"id": 304, "seek": 135444, "start": 1368.6000000000001, "end": 1372.06, "text": " And the answer to that question is we have no idea", "tokens": [400, 264, 1867, 281, 300, 1168, 307, 321, 362, 572, 1558], "temperature": 0.0, "avg_logprob": -0.21479995804603655, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.851551456245943e-07}, {"id": 305, "seek": 135444, "start": 1372.56, "end": 1375.3600000000001, "text": " We have to try a few things and see what works", "tokens": [492, 362, 281, 853, 257, 1326, 721, 293, 536, 437, 1985], "temperature": 0.0, "avg_logprob": -0.21479995804603655, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.851551456245943e-07}, {"id": 306, "seek": 135444, "start": 1376.3200000000002, "end": 1378.48, "text": " the underlying concept is", "tokens": [264, 14217, 3410, 307], "temperature": 0.0, "avg_logprob": -0.21479995804603655, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.851551456245943e-07}, {"id": 307, "seek": 135444, "start": 1379.16, "end": 1382.6000000000001, "text": " you need to pick an embedding dimensionality which is", "tokens": [291, 643, 281, 1888, 364, 12240, 3584, 10139, 1860, 597, 307], "temperature": 0.0, "avg_logprob": -0.21479995804603655, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.851551456245943e-07}, {"id": 308, "seek": 138260, "start": 1382.6, "end": 1388.7199999999998, "text": " enough to reflect the kind of true complexity of this causal system", "tokens": [1547, 281, 5031, 264, 733, 295, 2074, 14024, 295, 341, 38755, 1185], "temperature": 0.0, "avg_logprob": -0.19755949118198493, "compression_ratio": 1.6208530805687205, "no_speech_prob": 1.4367452649821644e-06}, {"id": 309, "seek": 138260, "start": 1389.4399999999998, "end": 1391.76, "text": " But not so big that you", "tokens": [583, 406, 370, 955, 300, 291], "temperature": 0.0, "avg_logprob": -0.19755949118198493, "compression_ratio": 1.6208530805687205, "no_speech_prob": 1.4367452649821644e-06}, {"id": 310, "seek": 138260, "start": 1392.6, "end": 1397.84, "text": " Have too many parameters that could take forever to run or even with regularization it might overfit", "tokens": [3560, 886, 867, 9834, 300, 727, 747, 5680, 281, 1190, 420, 754, 365, 3890, 2144, 309, 1062, 670, 6845], "temperature": 0.0, "avg_logprob": -0.19755949118198493, "compression_ratio": 1.6208530805687205, "no_speech_prob": 1.4367452649821644e-06}, {"id": 311, "seek": 138260, "start": 1400.36, "end": 1404.84, "text": " So what does it mean when the factor is negative that", "tokens": [407, 437, 775, 309, 914, 562, 264, 5952, 307, 3671, 300], "temperature": 0.0, "avg_logprob": -0.19755949118198493, "compression_ratio": 1.6208530805687205, "no_speech_prob": 1.4367452649821644e-06}, {"id": 312, "seek": 138260, "start": 1405.6399999999999, "end": 1411.9599999999998, "text": " The factor being negative in the movie case would mean like this is not dialogue driven in fact", "tokens": [440, 5952, 885, 3671, 294, 264, 3169, 1389, 576, 914, 411, 341, 307, 406, 10221, 9555, 294, 1186], "temperature": 0.0, "avg_logprob": -0.19755949118198493, "compression_ratio": 1.6208530805687205, "no_speech_prob": 1.4367452649821644e-06}, {"id": 313, "seek": 141196, "start": 1411.96, "end": 1418.1200000000001, "text": " it's like the opposite dialogue here is terrible a negative for the user would be like I actually", "tokens": [309, 311, 411, 264, 6182, 10221, 510, 307, 6237, 257, 3671, 337, 264, 4195, 576, 312, 411, 286, 767], "temperature": 0.0, "avg_logprob": -0.2733926410916485, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.0348494470235892e-06}, {"id": 314, "seek": 141196, "start": 1418.56, "end": 1424.68, "text": " dislike modern CGI movies so it's not from zero to whatever it's the range of", "tokens": [26006, 4363, 48448, 6233, 370, 309, 311, 406, 490, 4018, 281, 2035, 309, 311, 264, 3613, 295], "temperature": 0.0, "avg_logprob": -0.2733926410916485, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.0348494470235892e-06}, {"id": 315, "seek": 141196, "start": 1425.2, "end": 1431.8, "text": " Score would be negative is that range of score even like no no maximum. No there's no constraints at all here", "tokens": [47901, 576, 312, 3671, 307, 300, 3613, 295, 6175, 754, 411, 572, 572, 6674, 13, 883, 456, 311, 572, 18491, 412, 439, 510], "temperature": 0.0, "avg_logprob": -0.2733926410916485, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.0348494470235892e-06}, {"id": 316, "seek": 141196, "start": 1432.48, "end": 1434.72, "text": " These are just standard embedding matrices", "tokens": [1981, 366, 445, 3832, 12240, 3584, 32284], "temperature": 0.0, "avg_logprob": -0.2733926410916485, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.0348494470235892e-06}, {"id": 317, "seek": 141196, "start": 1437.72, "end": 1439.72, "text": " Thanks", "tokens": [2561], "temperature": 0.0, "avg_logprob": -0.2733926410916485, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.0348494470235892e-06}, {"id": 318, "seek": 143972, "start": 1439.72, "end": 1445.88, "text": " Couple of questions so first question is why do what why can we trust this embeddings?", "tokens": [38266, 295, 1651, 370, 700, 1168, 307, 983, 360, 437, 983, 393, 321, 3361, 341, 12240, 29432, 30], "temperature": 0.0, "avg_logprob": -0.19797449291877026, "compression_ratio": 1.7575757575757576, "no_speech_prob": 5.594263257080456e-06}, {"id": 319, "seek": 143972, "start": 1445.88, "end": 1451.28, "text": " Because like if you take a number six it can be expressed as one into six or like six into one or two into three", "tokens": [1436, 411, 498, 291, 747, 257, 1230, 2309, 309, 393, 312, 12675, 382, 472, 666, 2309, 420, 411, 2309, 666, 472, 420, 732, 666, 1045], "temperature": 0.0, "avg_logprob": -0.19797449291877026, "compression_ratio": 1.7575757575757576, "no_speech_prob": 5.594263257080456e-06}, {"id": 320, "seek": 143972, "start": 1451.28, "end": 1457.44, "text": " And three into two also you're saying like we could like reorder these five numbers in some other different order", "tokens": [400, 1045, 666, 732, 611, 291, 434, 1566, 411, 321, 727, 411, 319, 4687, 613, 1732, 3547, 294, 512, 661, 819, 1668], "temperature": 0.0, "avg_logprob": -0.19797449291877026, "compression_ratio": 1.7575757575757576, "no_speech_prob": 5.594263257080456e-06}, {"id": 321, "seek": 143972, "start": 1457.44, "end": 1462.2, "text": " Or like the value itself might be different as long as the product is something well", "tokens": [1610, 411, 264, 2158, 2564, 1062, 312, 819, 382, 938, 382, 264, 1674, 307, 746, 731], "temperature": 0.0, "avg_logprob": -0.19797449291877026, "compression_ratio": 1.7575757575757576, "no_speech_prob": 5.594263257080456e-06}, {"id": 322, "seek": 143972, "start": 1462.24, "end": 1466.68, "text": " But you see we're using gradient descent to find the best numbers", "tokens": [583, 291, 536, 321, 434, 1228, 16235, 23475, 281, 915, 264, 1151, 3547], "temperature": 0.0, "avg_logprob": -0.19797449291877026, "compression_ratio": 1.7575757575757576, "no_speech_prob": 5.594263257080456e-06}, {"id": 323, "seek": 146668, "start": 1466.68, "end": 1471.88, "text": " So like once we found a good minimum the idea is like", "tokens": [407, 411, 1564, 321, 1352, 257, 665, 7285, 264, 1558, 307, 411], "temperature": 0.0, "avg_logprob": -0.17265629318525205, "compression_ratio": 1.6488549618320612, "no_speech_prob": 3.1381259759655222e-06}, {"id": 324, "seek": 146668, "start": 1472.6000000000001, "end": 1476.92, "text": " Yeah, there are other numbers, but they don't give you as good an objective value", "tokens": [865, 11, 456, 366, 661, 3547, 11, 457, 436, 500, 380, 976, 291, 382, 665, 364, 10024, 2158], "temperature": 0.0, "avg_logprob": -0.17265629318525205, "compression_ratio": 1.6488549618320612, "no_speech_prob": 3.1381259759655222e-06}, {"id": 325, "seek": 146668, "start": 1478.68, "end": 1483.48, "text": " And of course we should be checking that on a validation set really which we'll be doing in the Python version", "tokens": [400, 295, 1164, 321, 820, 312, 8568, 300, 322, 257, 24071, 992, 534, 597, 321, 603, 312, 884, 294, 264, 15329, 3037], "temperature": 0.0, "avg_logprob": -0.17265629318525205, "compression_ratio": 1.6488549618320612, "no_speech_prob": 3.1381259759655222e-06}, {"id": 326, "seek": 146668, "start": 1483.48, "end": 1489.16, "text": " Okay, and the second question is when we have a new movie or a new user do we have to retrain the model?", "tokens": [1033, 11, 293, 264, 1150, 1168, 307, 562, 321, 362, 257, 777, 3169, 420, 257, 777, 4195, 360, 321, 362, 281, 1533, 7146, 264, 2316, 30], "temperature": 0.0, "avg_logprob": -0.17265629318525205, "compression_ratio": 1.6488549618320612, "no_speech_prob": 3.1381259759655222e-06}, {"id": 327, "seek": 146668, "start": 1489.72, "end": 1493.54, "text": " That is a really good question, and there isn't a straightforward answer to that", "tokens": [663, 307, 257, 534, 665, 1168, 11, 293, 456, 1943, 380, 257, 15325, 1867, 281, 300], "temperature": 0.0, "avg_logprob": -0.17265629318525205, "compression_ratio": 1.6488549618320612, "no_speech_prob": 3.1381259759655222e-06}, {"id": 328, "seek": 149354, "start": 1493.54, "end": 1496.22, "text": " Time permitting will come back to it", "tokens": [6161, 4784, 2414, 486, 808, 646, 281, 309], "temperature": 0.0, "avg_logprob": -0.21374093032464747, "compression_ratio": 1.853846153846154, "no_speech_prob": 2.6015968614956364e-06}, {"id": 329, "seek": 149354, "start": 1496.34, "end": 1499.8999999999999, "text": " But basically you would need to have like a kind of a new user", "tokens": [583, 1936, 291, 576, 643, 281, 362, 411, 257, 733, 295, 257, 777, 4195], "temperature": 0.0, "avg_logprob": -0.21374093032464747, "compression_ratio": 1.853846153846154, "no_speech_prob": 2.6015968614956364e-06}, {"id": 330, "seek": 149354, "start": 1500.1399999999999, "end": 1504.46, "text": " Model or a new movie model that you would use initially", "tokens": [17105, 420, 257, 777, 3169, 2316, 300, 291, 576, 764, 9105], "temperature": 0.0, "avg_logprob": -0.21374093032464747, "compression_ratio": 1.853846153846154, "no_speech_prob": 2.6015968614956364e-06}, {"id": 331, "seek": 149354, "start": 1505.1399999999999, "end": 1511.82, "text": " And then over time yes, you would then have to retrain the model so like I don't know if they still do it", "tokens": [400, 550, 670, 565, 2086, 11, 291, 576, 550, 362, 281, 1533, 7146, 264, 2316, 370, 411, 286, 500, 380, 458, 498, 436, 920, 360, 309], "temperature": 0.0, "avg_logprob": -0.21374093032464747, "compression_ratio": 1.853846153846154, "no_speech_prob": 2.6015968614956364e-06}, {"id": 332, "seek": 149354, "start": 1511.82, "end": 1515.34, "text": " But Netflix used to have this thing when you were first on boarded onto Netflix", "tokens": [583, 12778, 1143, 281, 362, 341, 551, 562, 291, 645, 700, 322, 3150, 292, 3911, 12778], "temperature": 0.0, "avg_logprob": -0.21374093032464747, "compression_ratio": 1.853846153846154, "no_speech_prob": 2.6015968614956364e-06}, {"id": 333, "seek": 149354, "start": 1515.3799999999999, "end": 1517.3799999999999, "text": " It would say like what movies do you like?", "tokens": [467, 576, 584, 411, 437, 6233, 360, 291, 411, 30], "temperature": 0.0, "avg_logprob": -0.21374093032464747, "compression_ratio": 1.853846153846154, "no_speech_prob": 2.6015968614956364e-06}, {"id": 334, "seek": 149354, "start": 1517.98, "end": 1522.6599999999999, "text": " And you'd have to go through and like say a bunch of movies you like and it would then like train", "tokens": [400, 291, 1116, 362, 281, 352, 807, 293, 411, 584, 257, 3840, 295, 6233, 291, 411, 293, 309, 576, 550, 411, 3847], "temperature": 0.0, "avg_logprob": -0.21374093032464747, "compression_ratio": 1.853846153846154, "no_speech_prob": 2.6015968614956364e-06}, {"id": 335, "seek": 152266, "start": 1522.66, "end": 1524.66, "text": " its model", "tokens": [1080, 2316], "temperature": 0.0, "avg_logprob": -0.17484899110431912, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.6536819202883635e-06}, {"id": 336, "seek": 152266, "start": 1528.18, "end": 1533.42, "text": " Could you could you just find the nearest movie to the movie that you're trying to do the new movie that you're trying to add", "tokens": [7497, 291, 727, 291, 445, 915, 264, 23831, 3169, 281, 264, 3169, 300, 291, 434, 1382, 281, 360, 264, 777, 3169, 300, 291, 434, 1382, 281, 909], "temperature": 0.0, "avg_logprob": -0.17484899110431912, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.6536819202883635e-06}, {"id": 337, "seek": 152266, "start": 1533.6200000000001, "end": 1535.7, "text": " Yeah, you could use nearest neighbors for sure", "tokens": [865, 11, 291, 727, 764, 23831, 12512, 337, 988], "temperature": 0.0, "avg_logprob": -0.17484899110431912, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.6536819202883635e-06}, {"id": 338, "seek": 152266, "start": 1538.42, "end": 1543.66, "text": " But the thing is initially at least in this case we have no", "tokens": [583, 264, 551, 307, 9105, 412, 1935, 294, 341, 1389, 321, 362, 572], "temperature": 0.0, "avg_logprob": -0.17484899110431912, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.6536819202883635e-06}, {"id": 339, "seek": 152266, "start": 1545.22, "end": 1548.72, "text": " Columns to describe a movie so if you had something about like the movies", "tokens": [4004, 449, 3695, 281, 6786, 257, 3169, 370, 498, 291, 632, 746, 466, 411, 264, 6233], "temperature": 0.0, "avg_logprob": -0.17484899110431912, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.6536819202883635e-06}, {"id": 340, "seek": 154872, "start": 1548.72, "end": 1555.04, "text": " Genre release date who was in it or something you could have some kind of non collaborative filtering model", "tokens": [3632, 265, 4374, 4002, 567, 390, 294, 309, 420, 746, 291, 727, 362, 512, 733, 295, 2107, 16555, 30822, 2316], "temperature": 0.0, "avg_logprob": -0.1465870924670287, "compression_ratio": 1.6607929515418502, "no_speech_prob": 2.332065378141124e-06}, {"id": 341, "seek": 154872, "start": 1555.04, "end": 1560.56, "text": " And that was kind of what I meant by like a new movie model. You'd have to have some some kind of predictors", "tokens": [400, 300, 390, 733, 295, 437, 286, 4140, 538, 411, 257, 777, 3169, 2316, 13, 509, 1116, 362, 281, 362, 512, 512, 733, 295, 6069, 830], "temperature": 0.0, "avg_logprob": -0.1465870924670287, "compression_ratio": 1.6607929515418502, "no_speech_prob": 2.332065378141124e-06}, {"id": 342, "seek": 154872, "start": 1562.68, "end": 1564.68, "text": " Okay, so a", "tokens": [1033, 11, 370, 257], "temperature": 0.0, "avg_logprob": -0.1465870924670287, "compression_ratio": 1.6607929515418502, "no_speech_prob": 2.332065378141124e-06}, {"id": 343, "seek": 154872, "start": 1568.16, "end": 1571.96, "text": " Lot of this is going to look familiar and and the way I'm going to do this is again", "tokens": [20131, 295, 341, 307, 516, 281, 574, 4963, 293, 293, 264, 636, 286, 478, 516, 281, 360, 341, 307, 797], "temperature": 0.0, "avg_logprob": -0.1465870924670287, "compression_ratio": 1.6607929515418502, "no_speech_prob": 2.332065378141124e-06}, {"id": 344, "seek": 154872, "start": 1571.96, "end": 1575.08, "text": " It's kind of this top-down approach. We're going to start using a", "tokens": [467, 311, 733, 295, 341, 1192, 12, 5093, 3109, 13, 492, 434, 516, 281, 722, 1228, 257], "temperature": 0.0, "avg_logprob": -0.1465870924670287, "compression_ratio": 1.6607929515418502, "no_speech_prob": 2.332065378141124e-06}, {"id": 345, "seek": 157508, "start": 1575.08, "end": 1582.8999999999999, "text": " Few features of pytorch and fast AI and gradually we're going to redo it a few times in a few different ways", "tokens": [33468, 4122, 295, 25878, 284, 339, 293, 2370, 7318, 293, 13145, 321, 434, 516, 281, 29956, 309, 257, 1326, 1413, 294, 257, 1326, 819, 2098], "temperature": 0.0, "avg_logprob": -0.19972503959358512, "compression_ratio": 1.5288461538461537, "no_speech_prob": 3.187544280081056e-06}, {"id": 346, "seek": 157508, "start": 1583.6, "end": 1585.6, "text": " Kind of doing a little bit deeper each time", "tokens": [9242, 295, 884, 257, 707, 857, 7731, 1184, 565], "temperature": 0.0, "avg_logprob": -0.19972503959358512, "compression_ratio": 1.5288461538461537, "no_speech_prob": 3.187544280081056e-06}, {"id": 347, "seek": 157508, "start": 1587.96, "end": 1593.1999999999998, "text": " Regardless we do need a validation set so we can use our standard cross validation", "tokens": [25148, 321, 360, 643, 257, 24071, 992, 370, 321, 393, 764, 527, 3832, 3278, 24071], "temperature": 0.0, "avg_logprob": -0.19972503959358512, "compression_ratio": 1.5288461538461537, "no_speech_prob": 3.187544280081056e-06}, {"id": 348, "seek": 157508, "start": 1593.76, "end": 1596.52, "text": " indexes approach to grab a random set of IDs", "tokens": [8186, 279, 3109, 281, 4444, 257, 4974, 992, 295, 48212], "temperature": 0.0, "avg_logprob": -0.19972503959358512, "compression_ratio": 1.5288461538461537, "no_speech_prob": 3.187544280081056e-06}, {"id": 349, "seek": 157508, "start": 1599.1599999999999, "end": 1600.8, "text": " This is something called weight decay", "tokens": [639, 307, 746, 1219, 3364, 21039], "temperature": 0.0, "avg_logprob": -0.19972503959358512, "compression_ratio": 1.5288461538461537, "no_speech_prob": 3.187544280081056e-06}, {"id": 350, "seek": 160080, "start": 1600.8, "end": 1608.04, "text": " Which we'll talk about later in the course for those of you that have done some machine learning its L2 regularization basically", "tokens": [3013, 321, 603, 751, 466, 1780, 294, 264, 1164, 337, 729, 295, 291, 300, 362, 1096, 512, 3479, 2539, 1080, 441, 17, 3890, 2144, 1936], "temperature": 0.0, "avg_logprob": -0.18560316533218196, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.6903569379283e-07}, {"id": 351, "seek": 160080, "start": 1608.76, "end": 1613.36, "text": " And this is where we choose how big a embedding matrix do we want okay?", "tokens": [400, 341, 307, 689, 321, 2826, 577, 955, 257, 12240, 3584, 8141, 360, 321, 528, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18560316533218196, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.6903569379283e-07}, {"id": 352, "seek": 160080, "start": 1615.32, "end": 1620.6, "text": " So again, you know here's where we get our model data object from CSV", "tokens": [407, 797, 11, 291, 458, 510, 311, 689, 321, 483, 527, 2316, 1412, 2657, 490, 48814], "temperature": 0.0, "avg_logprob": -0.18560316533218196, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.6903569379283e-07}, {"id": 353, "seek": 160080, "start": 1621.9199999999998, "end": 1625.3999999999999, "text": " Passing in that ratings file which remember", "tokens": [10319, 278, 294, 300, 24603, 3991, 597, 1604], "temperature": 0.0, "avg_logprob": -0.18560316533218196, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.6903569379283e-07}, {"id": 354, "seek": 160080, "start": 1627.2, "end": 1628.8799999999999, "text": " Looks like that", "tokens": [10027, 411, 300], "temperature": 0.0, "avg_logprob": -0.18560316533218196, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.6903569379283e-07}, {"id": 355, "seek": 162888, "start": 1628.88, "end": 1630.88, "text": " Okay, so you'll see like stuff", "tokens": [1033, 11, 370, 291, 603, 536, 411, 1507], "temperature": 0.0, "avg_logprob": -0.1874859610269236, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.7061765902326442e-06}, {"id": 356, "seek": 162888, "start": 1631.5200000000002, "end": 1633.68, "text": " Tends to look pretty familiar after a while", "tokens": [314, 2581, 281, 574, 1238, 4963, 934, 257, 1339], "temperature": 0.0, "avg_logprob": -0.1874859610269236, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.7061765902326442e-06}, {"id": 357, "seek": 162888, "start": 1635.8400000000001, "end": 1637.8400000000001, "text": " And then you just have to pass in", "tokens": [400, 550, 291, 445, 362, 281, 1320, 294], "temperature": 0.0, "avg_logprob": -0.1874859610269236, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.7061765902326442e-06}, {"id": 358, "seek": 162888, "start": 1639.92, "end": 1641.88, "text": " The", "tokens": [440], "temperature": 0.0, "avg_logprob": -0.1874859610269236, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.7061765902326442e-06}, {"id": 359, "seek": 162888, "start": 1641.88, "end": 1649.64, "text": " What are your rows effectively? What are your columns effectively and what are your values effectively right so any any collaborative filtering?", "tokens": [708, 366, 428, 13241, 8659, 30, 708, 366, 428, 13766, 8659, 293, 437, 366, 428, 4190, 8659, 558, 370, 604, 604, 16555, 30822, 30], "temperature": 0.0, "avg_logprob": -0.1874859610269236, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.7061765902326442e-06}, {"id": 360, "seek": 162888, "start": 1650.64, "end": 1653.5, "text": " Recommendation system approach. There's basically a concept of like", "tokens": [49545, 521, 399, 1185, 3109, 13, 821, 311, 1936, 257, 3410, 295, 411], "temperature": 0.0, "avg_logprob": -0.1874859610269236, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.7061765902326442e-06}, {"id": 361, "seek": 162888, "start": 1654.2, "end": 1656.2, "text": " You know a user and an item", "tokens": [509, 458, 257, 4195, 293, 364, 3174], "temperature": 0.0, "avg_logprob": -0.1874859610269236, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.7061765902326442e-06}, {"id": 362, "seek": 165620, "start": 1656.2, "end": 1662.72, "text": " Now they might not be users and items like if you're doing the yet that Ecuadorian groceries competition", "tokens": [823, 436, 1062, 406, 312, 5022, 293, 4754, 411, 498, 291, 434, 884, 264, 1939, 300, 41558, 952, 31391, 6211], "temperature": 0.0, "avg_logprob": -0.20293848331157976, "compression_ratio": 1.713740458015267, "no_speech_prob": 3.059018069961894e-07}, {"id": 363, "seek": 165620, "start": 1662.76, "end": 1667.4, "text": " There are stores and items and you're trying to predict how many things are you going to sell?", "tokens": [821, 366, 9512, 293, 4754, 293, 291, 434, 1382, 281, 6069, 577, 867, 721, 366, 291, 516, 281, 3607, 30], "temperature": 0.0, "avg_logprob": -0.20293848331157976, "compression_ratio": 1.713740458015267, "no_speech_prob": 3.059018069961894e-07}, {"id": 364, "seek": 165620, "start": 1668.28, "end": 1670.88, "text": " At this store of this type", "tokens": [1711, 341, 3531, 295, 341, 2010], "temperature": 0.0, "avg_logprob": -0.20293848331157976, "compression_ratio": 1.713740458015267, "no_speech_prob": 3.059018069961894e-07}, {"id": 365, "seek": 165620, "start": 1673.68, "end": 1677.52, "text": " But generally speaking just this idea of like you've got a couple of kind of high cardinality", "tokens": [583, 5101, 4124, 445, 341, 1558, 295, 411, 291, 600, 658, 257, 1916, 295, 733, 295, 1090, 2920, 259, 1860], "temperature": 0.0, "avg_logprob": -0.20293848331157976, "compression_ratio": 1.713740458015267, "no_speech_prob": 3.059018069961894e-07}, {"id": 366, "seek": 165620, "start": 1678.04, "end": 1684.28, "text": " Categorical variables and something that you're measuring and you're kind of conceptualizing it as saying okay, we could predict", "tokens": [383, 2968, 284, 804, 9102, 293, 746, 300, 291, 434, 13389, 293, 291, 434, 733, 295, 24106, 3319, 309, 382, 1566, 1392, 11, 321, 727, 6069], "temperature": 0.0, "avg_logprob": -0.20293848331157976, "compression_ratio": 1.713740458015267, "no_speech_prob": 3.059018069961894e-07}, {"id": 367, "seek": 168428, "start": 1684.28, "end": 1689.16, "text": " The rating we can predict the value by doing this this dot product", "tokens": [440, 10990, 321, 393, 6069, 264, 2158, 538, 884, 341, 341, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.2819892732720626, "compression_ratio": 1.565, "no_speech_prob": 2.902299229390337e-06}, {"id": 368, "seek": 168428, "start": 1690.72, "end": 1695.3999999999999, "text": " Interestingly this is kind of relevant to that that last question or suggestion an", "tokens": [30564, 341, 307, 733, 295, 7340, 281, 300, 300, 1036, 1168, 420, 16541, 364], "temperature": 0.0, "avg_logprob": -0.2819892732720626, "compression_ratio": 1.565, "no_speech_prob": 2.902299229390337e-06}, {"id": 369, "seek": 168428, "start": 1696.68, "end": 1700.5, "text": " Identical way to think about this or to express this is to say", "tokens": [25905, 804, 636, 281, 519, 466, 341, 420, 281, 5109, 341, 307, 281, 584], "temperature": 0.0, "avg_logprob": -0.2819892732720626, "compression_ratio": 1.565, "no_speech_prob": 2.902299229390337e-06}, {"id": 370, "seek": 168428, "start": 1701.2, "end": 1703.2, "text": " when we're deciding", "tokens": [562, 321, 434, 17990], "temperature": 0.0, "avg_logprob": -0.2819892732720626, "compression_ratio": 1.565, "no_speech_prob": 2.902299229390337e-06}, {"id": 371, "seek": 168428, "start": 1703.68, "end": 1707.46, "text": " Whether user 72 will like movie 27", "tokens": [8503, 4195, 18731, 486, 411, 3169, 7634], "temperature": 0.0, "avg_logprob": -0.2819892732720626, "compression_ratio": 1.565, "no_speech_prob": 2.902299229390337e-06}, {"id": 372, "seek": 168428, "start": 1708.16, "end": 1711.0, "text": " It's basically saying which other", "tokens": [467, 311, 1936, 1566, 597, 661], "temperature": 0.0, "avg_logprob": -0.2819892732720626, "compression_ratio": 1.565, "no_speech_prob": 2.902299229390337e-06}, {"id": 373, "seek": 168428, "start": 1711.96, "end": 1713.6399999999999, "text": " users liked", "tokens": [5022, 4501], "temperature": 0.0, "avg_logprob": -0.2819892732720626, "compression_ratio": 1.565, "no_speech_prob": 2.902299229390337e-06}, {"id": 374, "seek": 171364, "start": 1713.64, "end": 1716.6000000000001, "text": " movies that 72 liked and", "tokens": [6233, 300, 18731, 4501, 293], "temperature": 0.0, "avg_logprob": -0.18870187417054787, "compression_ratio": 1.655, "no_speech_prob": 1.7880603309095022e-06}, {"id": 375, "seek": 171364, "start": 1717.5200000000002, "end": 1721.72, "text": " which other movies were liked by people like", "tokens": [597, 661, 6233, 645, 4501, 538, 561, 411], "temperature": 0.0, "avg_logprob": -0.18870187417054787, "compression_ratio": 1.655, "no_speech_prob": 1.7880603309095022e-06}, {"id": 376, "seek": 171364, "start": 1723.2, "end": 1730.16, "text": " User 72 it turns out that these are basically two ways of saying the exact same thing", "tokens": [32127, 18731, 309, 4523, 484, 300, 613, 366, 1936, 732, 2098, 295, 1566, 264, 1900, 912, 551], "temperature": 0.0, "avg_logprob": -0.18870187417054787, "compression_ratio": 1.655, "no_speech_prob": 1.7880603309095022e-06}, {"id": 377, "seek": 171364, "start": 1730.16, "end": 1732.3200000000002, "text": " So basically what collaborative filtering is doing?", "tokens": [407, 1936, 437, 16555, 30822, 307, 884, 30], "temperature": 0.0, "avg_logprob": -0.18870187417054787, "compression_ratio": 1.655, "no_speech_prob": 1.7880603309095022e-06}, {"id": 378, "seek": 171364, "start": 1733.0400000000002, "end": 1741.1200000000001, "text": " You know kind of conceptually is to say okay this movie and this user which other movies are similar to it in", "tokens": [509, 458, 733, 295, 3410, 671, 307, 281, 584, 1392, 341, 3169, 293, 341, 4195, 597, 661, 6233, 366, 2531, 281, 309, 294], "temperature": 0.0, "avg_logprob": -0.18870187417054787, "compression_ratio": 1.655, "no_speech_prob": 1.7880603309095022e-06}, {"id": 379, "seek": 171364, "start": 1741.3200000000002, "end": 1743.0400000000002, "text": " terms of like", "tokens": [2115, 295, 411], "temperature": 0.0, "avg_logprob": -0.18870187417054787, "compression_ratio": 1.655, "no_speech_prob": 1.7880603309095022e-06}, {"id": 380, "seek": 174304, "start": 1743.04, "end": 1749.32, "text": " Similar people enjoyed them and which people are similar to this person based on people that like the same kind of movies", "tokens": [10905, 561, 4626, 552, 293, 597, 561, 366, 2531, 281, 341, 954, 2361, 322, 561, 300, 411, 264, 912, 733, 295, 6233], "temperature": 0.0, "avg_logprob": -0.15016489913783124, "compression_ratio": 1.7736625514403292, "no_speech_prob": 2.9944217203592416e-06}, {"id": 381, "seek": 174304, "start": 1749.44, "end": 1751.44, "text": " so that's kind of the", "tokens": [370, 300, 311, 733, 295, 264], "temperature": 0.0, "avg_logprob": -0.15016489913783124, "compression_ratio": 1.7736625514403292, "no_speech_prob": 2.9944217203592416e-06}, {"id": 382, "seek": 174304, "start": 1751.56, "end": 1752.96, "text": " underlying", "tokens": [14217], "temperature": 0.0, "avg_logprob": -0.15016489913783124, "compression_ratio": 1.7736625514403292, "no_speech_prob": 2.9944217203592416e-06}, {"id": 383, "seek": 174304, "start": 1752.96, "end": 1758.68, "text": " Structure at any time there's an underlying structure like this that kind of collaborative filtering approach is likely to be useful", "tokens": [745, 2885, 412, 604, 565, 456, 311, 364, 14217, 3877, 411, 341, 300, 733, 295, 16555, 30822, 3109, 307, 3700, 281, 312, 4420], "temperature": 0.0, "avg_logprob": -0.15016489913783124, "compression_ratio": 1.7736625514403292, "no_speech_prob": 2.9944217203592416e-06}, {"id": 384, "seek": 174304, "start": 1759.92, "end": 1761.92, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.15016489913783124, "compression_ratio": 1.7736625514403292, "no_speech_prob": 2.9944217203592416e-06}, {"id": 385, "seek": 174304, "start": 1762.2, "end": 1769.1599999999999, "text": " So you yeah, so there's basically two parts the two bits of your thing that you're factoring and then the value the dependent variable", "tokens": [407, 291, 1338, 11, 370, 456, 311, 1936, 732, 3166, 264, 732, 9239, 295, 428, 551, 300, 291, 434, 1186, 3662, 293, 550, 264, 2158, 264, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.15016489913783124, "compression_ratio": 1.7736625514403292, "no_speech_prob": 2.9944217203592416e-06}, {"id": 386, "seek": 176916, "start": 1769.16, "end": 1775.44, "text": " So as per usual we can take our model data and ask for a learner from it", "tokens": [407, 382, 680, 7713, 321, 393, 747, 527, 2316, 1412, 293, 1029, 337, 257, 33347, 490, 309], "temperature": 0.0, "avg_logprob": -0.17753544560185186, "compression_ratio": 1.6610878661087867, "no_speech_prob": 3.844908860628493e-06}, {"id": 387, "seek": 176916, "start": 1775.44, "end": 1777.88, "text": " And we need to tell it what size embedding matrix to use", "tokens": [400, 321, 643, 281, 980, 309, 437, 2744, 12240, 3584, 8141, 281, 764], "temperature": 0.0, "avg_logprob": -0.17753544560185186, "compression_ratio": 1.6610878661087867, "no_speech_prob": 3.844908860628493e-06}, {"id": 388, "seek": 176916, "start": 1779.0, "end": 1785.8000000000002, "text": " How many sorry what validation set indexes to use what batch size to use and what optimizer?", "tokens": [1012, 867, 2597, 437, 24071, 992, 8186, 279, 281, 764, 437, 15245, 2744, 281, 764, 293, 437, 5028, 6545, 30], "temperature": 0.0, "avg_logprob": -0.17753544560185186, "compression_ratio": 1.6610878661087867, "no_speech_prob": 3.844908860628493e-06}, {"id": 389, "seek": 176916, "start": 1786.24, "end": 1789.14, "text": " To use and we're going to be talking more about optimizers", "tokens": [1407, 764, 293, 321, 434, 516, 281, 312, 1417, 544, 466, 5028, 22525], "temperature": 0.0, "avg_logprob": -0.17753544560185186, "compression_ratio": 1.6610878661087867, "no_speech_prob": 3.844908860628493e-06}, {"id": 390, "seek": 176916, "start": 1789.8400000000001, "end": 1793.18, "text": " Shortly, we won't do Adam today, but we'll do Adam", "tokens": [40109, 11, 321, 1582, 380, 360, 7938, 965, 11, 457, 321, 603, 360, 7938], "temperature": 0.0, "avg_logprob": -0.17753544560185186, "compression_ratio": 1.6610878661087867, "no_speech_prob": 3.844908860628493e-06}, {"id": 391, "seek": 176916, "start": 1793.8400000000001, "end": 1795.8400000000001, "text": " Next week or the week after", "tokens": [3087, 1243, 420, 264, 1243, 934], "temperature": 0.0, "avg_logprob": -0.17753544560185186, "compression_ratio": 1.6610878661087867, "no_speech_prob": 3.844908860628493e-06}, {"id": 392, "seek": 176916, "start": 1796.1200000000001, "end": 1798.1200000000001, "text": " And then we can go ahead and say fit", "tokens": [400, 550, 321, 393, 352, 2286, 293, 584, 3318], "temperature": 0.0, "avg_logprob": -0.17753544560185186, "compression_ratio": 1.6610878661087867, "no_speech_prob": 3.844908860628493e-06}, {"id": 393, "seek": 179812, "start": 1798.12, "end": 1804.08, "text": " All right, and it all looks pretty similar interesting as usual interestingly", "tokens": [1057, 558, 11, 293, 309, 439, 1542, 1238, 2531, 1880, 382, 7713, 25873], "temperature": 0.0, "avg_logprob": -0.20914469483078166, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.989710819496395e-07}, {"id": 394, "seek": 179812, "start": 1804.08, "end": 1809.52, "text": " I only had to do three epochs like this kind of model seemed to train super quickly", "tokens": [286, 787, 632, 281, 360, 1045, 30992, 28346, 411, 341, 733, 295, 2316, 6576, 281, 3847, 1687, 2661], "temperature": 0.0, "avg_logprob": -0.20914469483078166, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.989710819496395e-07}, {"id": 395, "seek": 179812, "start": 1810.2399999999998, "end": 1814.8, "text": " You can use the learning rate finder as per usual all the stuff you're familiar with will work fine", "tokens": [509, 393, 764, 264, 2539, 3314, 915, 260, 382, 680, 7713, 439, 264, 1507, 291, 434, 4963, 365, 486, 589, 2489], "temperature": 0.0, "avg_logprob": -0.20914469483078166, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.989710819496395e-07}, {"id": 396, "seek": 179812, "start": 1815.4399999999998, "end": 1822.3799999999999, "text": " And that was it so this took you know about two seconds the train. There's no pre trained anything's here", "tokens": [400, 300, 390, 309, 370, 341, 1890, 291, 458, 466, 732, 3949, 264, 3847, 13, 821, 311, 572, 659, 8895, 1340, 311, 510], "temperature": 0.0, "avg_logprob": -0.20914469483078166, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.989710819496395e-07}, {"id": 397, "seek": 179812, "start": 1822.3799999999999, "end": 1824.3799999999999, "text": " This is from random from scratch", "tokens": [639, 307, 490, 4974, 490, 8459], "temperature": 0.0, "avg_logprob": -0.20914469483078166, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.989710819496395e-07}, {"id": 398, "seek": 182438, "start": 1824.38, "end": 1826.38, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.19226685348822145, "compression_ratio": 1.780590717299578, "no_speech_prob": 3.5559653497330146e-06}, {"id": 399, "seek": 182438, "start": 1826.46, "end": 1831.22, "text": " This is our validation set and we can compare it we have this is a mean squared error", "tokens": [639, 307, 527, 24071, 992, 293, 321, 393, 6794, 309, 321, 362, 341, 307, 257, 914, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.19226685348822145, "compression_ratio": 1.780590717299578, "no_speech_prob": 3.5559653497330146e-06}, {"id": 400, "seek": 182438, "start": 1831.22, "end": 1833.9, "text": " Not a root mean squared error, so we can take the square root", "tokens": [1726, 257, 5593, 914, 8889, 6713, 11, 370, 321, 393, 747, 264, 3732, 5593], "temperature": 0.0, "avg_logprob": -0.19226685348822145, "compression_ratio": 1.780590717299578, "no_speech_prob": 3.5559653497330146e-06}, {"id": 401, "seek": 182438, "start": 1835.7800000000002, "end": 1841.2600000000002, "text": " So that last time I ran it was point seven seven six and that's point eight eight and there's some", "tokens": [407, 300, 1036, 565, 286, 5872, 309, 390, 935, 3407, 3407, 2309, 293, 300, 311, 935, 3180, 3180, 293, 456, 311, 512], "temperature": 0.0, "avg_logprob": -0.19226685348822145, "compression_ratio": 1.780590717299578, "no_speech_prob": 3.5559653497330146e-06}, {"id": 402, "seek": 182438, "start": 1841.66, "end": 1844.0200000000002, "text": " benchmarks available for this data set", "tokens": [43751, 2435, 337, 341, 1412, 992], "temperature": 0.0, "avg_logprob": -0.19226685348822145, "compression_ratio": 1.780590717299578, "no_speech_prob": 3.5559653497330146e-06}, {"id": 403, "seek": 182438, "start": 1844.5400000000002, "end": 1851.46, "text": " And when I scrolled through and found the bench the best benchmark I could find here from this recommendation system specific library", "tokens": [400, 562, 286, 11369, 292, 807, 293, 1352, 264, 10638, 264, 1151, 18927, 286, 727, 915, 510, 490, 341, 11879, 1185, 2685, 6405], "temperature": 0.0, "avg_logprob": -0.19226685348822145, "compression_ratio": 1.780590717299578, "no_speech_prob": 3.5559653497330146e-06}, {"id": 404, "seek": 185146, "start": 1851.46, "end": 1857.26, "text": " They had point nine one so we've got a better loss in two seconds", "tokens": [814, 632, 935, 4949, 472, 370, 321, 600, 658, 257, 1101, 4470, 294, 732, 3949], "temperature": 0.0, "avg_logprob": -0.21244136938888036, "compression_ratio": 1.6577777777777778, "no_speech_prob": 1.9750514468341862e-07}, {"id": 405, "seek": 185146, "start": 1859.42, "end": 1861.42, "text": " Already so that's good", "tokens": [23741, 370, 300, 311, 665], "temperature": 0.0, "avg_logprob": -0.21244136938888036, "compression_ratio": 1.6577777777777778, "no_speech_prob": 1.9750514468341862e-07}, {"id": 406, "seek": 185146, "start": 1863.3400000000001, "end": 1866.1000000000001, "text": " So that's basically how you can do collaborative filtering", "tokens": [407, 300, 311, 1936, 577, 291, 393, 360, 16555, 30822], "temperature": 0.0, "avg_logprob": -0.21244136938888036, "compression_ratio": 1.6577777777777778, "no_speech_prob": 1.9750514468341862e-07}, {"id": 407, "seek": 185146, "start": 1867.22, "end": 1869.74, "text": " with the fast AI library without", "tokens": [365, 264, 2370, 7318, 6405, 1553], "temperature": 0.0, "avg_logprob": -0.21244136938888036, "compression_ratio": 1.6577777777777778, "no_speech_prob": 1.9750514468341862e-07}, {"id": 408, "seek": 185146, "start": 1870.46, "end": 1876.8400000000001, "text": " Thinking too much, but so now we're going to dig in and try and rebuild that we'll try and get to the point that we're", "tokens": [24460, 886, 709, 11, 457, 370, 586, 321, 434, 516, 281, 2528, 294, 293, 853, 293, 16877, 300, 321, 603, 853, 293, 483, 281, 264, 935, 300, 321, 434], "temperature": 0.0, "avg_logprob": -0.21244136938888036, "compression_ratio": 1.6577777777777778, "no_speech_prob": 1.9750514468341862e-07}, {"id": 409, "seek": 187684, "start": 1876.84, "end": 1881.28, "text": " Getting something around point seven seven point seven eight from scratch", "tokens": [13674, 746, 926, 935, 3407, 3407, 935, 3407, 3180, 490, 8459], "temperature": 0.0, "avg_logprob": -0.16650466529690489, "compression_ratio": 1.6583333333333334, "no_speech_prob": 3.2058227361631e-07}, {"id": 410, "seek": 187684, "start": 1882.9599999999998, "end": 1887.6999999999998, "text": " But if you want to do this yourself at home, you know without worrying about the detail", "tokens": [583, 498, 291, 528, 281, 360, 341, 1803, 412, 1280, 11, 291, 458, 1553, 18788, 466, 264, 2607], "temperature": 0.0, "avg_logprob": -0.16650466529690489, "compression_ratio": 1.6583333333333334, "no_speech_prob": 3.2058227361631e-07}, {"id": 411, "seek": 187684, "start": 1888.34, "end": 1891.4599999999998, "text": " That's you know those three lines of code is all you need", "tokens": [663, 311, 291, 458, 729, 1045, 3876, 295, 3089, 307, 439, 291, 643], "temperature": 0.0, "avg_logprob": -0.16650466529690489, "compression_ratio": 1.6583333333333334, "no_speech_prob": 3.2058227361631e-07}, {"id": 412, "seek": 187684, "start": 1892.62, "end": 1897.56, "text": " Okay, so we can get the predictions in the usual way and you know we could for example plot", "tokens": [1033, 11, 370, 321, 393, 483, 264, 21264, 294, 264, 7713, 636, 293, 291, 458, 321, 727, 337, 1365, 7542], "temperature": 0.0, "avg_logprob": -0.16650466529690489, "compression_ratio": 1.6583333333333334, "no_speech_prob": 3.2058227361631e-07}, {"id": 413, "seek": 187684, "start": 1898.5, "end": 1903.3799999999999, "text": " SNS is seaborn seaborn is a really great plotting library it sits on top of matplotlib", "tokens": [13955, 50, 307, 369, 455, 1865, 369, 455, 1865, 307, 257, 534, 869, 41178, 6405, 309, 12696, 322, 1192, 295, 3803, 564, 310, 38270], "temperature": 0.0, "avg_logprob": -0.16650466529690489, "compression_ratio": 1.6583333333333334, "no_speech_prob": 3.2058227361631e-07}, {"id": 414, "seek": 190338, "start": 1903.38, "end": 1908.8600000000001, "text": " It actually leverages matplotlib so anything we learn about matplotlib will help you with seaborn", "tokens": [467, 767, 12451, 1660, 3803, 564, 310, 38270, 370, 1340, 321, 1466, 466, 3803, 564, 310, 38270, 486, 854, 291, 365, 369, 455, 1865], "temperature": 0.0, "avg_logprob": -0.16627112456730433, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.7061732933143503e-06}, {"id": 415, "seek": 190338, "start": 1908.8600000000001, "end": 1911.3600000000001, "text": " It's got a few like nice little plots like this joint plot", "tokens": [467, 311, 658, 257, 1326, 411, 1481, 707, 28609, 411, 341, 7225, 7542], "temperature": 0.0, "avg_logprob": -0.16627112456730433, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.7061732933143503e-06}, {"id": 416, "seek": 190338, "start": 1912.14, "end": 1914.14, "text": " Here is undoing", "tokens": [1692, 307, 23779, 278], "temperature": 0.0, "avg_logprob": -0.16627112456730433, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.7061732933143503e-06}, {"id": 417, "seek": 190338, "start": 1914.74, "end": 1916.0600000000002, "text": " predictions", "tokens": [21264], "temperature": 0.0, "avg_logprob": -0.16627112456730433, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.7061732933143503e-06}, {"id": 418, "seek": 190338, "start": 1916.0600000000002, "end": 1917.3400000000001, "text": " against", "tokens": [1970], "temperature": 0.0, "avg_logprob": -0.16627112456730433, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.7061732933143503e-06}, {"id": 419, "seek": 190338, "start": 1917.3400000000001, "end": 1922.3400000000001, "text": " Against actuals so these are my actuals these are my predictions and you can kind of see the", "tokens": [29995, 3539, 82, 370, 613, 366, 452, 3539, 82, 613, 366, 452, 21264, 293, 291, 393, 733, 295, 536, 264], "temperature": 0.0, "avg_logprob": -0.16627112456730433, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.7061732933143503e-06}, {"id": 420, "seek": 190338, "start": 1922.5800000000002, "end": 1925.18, "text": " The shape here is that as we predict higher numbers", "tokens": [440, 3909, 510, 307, 300, 382, 321, 6069, 2946, 3547], "temperature": 0.0, "avg_logprob": -0.16627112456730433, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.7061732933143503e-06}, {"id": 421, "seek": 190338, "start": 1925.18, "end": 1931.6200000000001, "text": " They actually are higher numbers and you can also see the histogram of the predictions and a histogram of the actions", "tokens": [814, 767, 366, 2946, 3547, 293, 291, 393, 611, 536, 264, 49816, 295, 264, 21264, 293, 257, 49816, 295, 264, 5909], "temperature": 0.0, "avg_logprob": -0.16627112456730433, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.7061732933143503e-06}, {"id": 422, "seek": 193162, "start": 1931.62, "end": 1936.06, "text": " So I was just kind of plotting that just to show you another interesting visualization", "tokens": [407, 286, 390, 445, 733, 295, 41178, 300, 445, 281, 855, 291, 1071, 1880, 25801], "temperature": 0.0, "avg_logprob": -0.2853598142925062, "compression_ratio": 1.6096491228070176, "no_speech_prob": 3.785293756664032e-06}, {"id": 423, "seek": 193162, "start": 1937.82, "end": 1940.82, "text": " Could you please explain the n factors", "tokens": [7497, 291, 1767, 2903, 264, 297, 6771], "temperature": 0.0, "avg_logprob": -0.2853598142925062, "compression_ratio": 1.6096491228070176, "no_speech_prob": 3.785293756664032e-06}, {"id": 424, "seek": 193162, "start": 1941.5, "end": 1947.2199999999998, "text": " Why it's set to 50? It's set to 50 because I tried a few things that's in the world. That's all", "tokens": [1545, 309, 311, 992, 281, 2625, 30, 467, 311, 992, 281, 2625, 570, 286, 3031, 257, 1326, 721, 300, 311, 294, 264, 1002, 13, 663, 311, 439], "temperature": 0.0, "avg_logprob": -0.2853598142925062, "compression_ratio": 1.6096491228070176, "no_speech_prob": 3.785293756664032e-06}, {"id": 425, "seek": 193162, "start": 1948.2199999999998, "end": 1951.1, "text": " It's this it's the dimensionality of the embedding matrix", "tokens": [467, 311, 341, 309, 311, 264, 10139, 1860, 295, 264, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.2853598142925062, "compression_ratio": 1.6096491228070176, "no_speech_prob": 3.785293756664032e-06}, {"id": 426, "seek": 195110, "start": 1951.1, "end": 1960.86, "text": " Or to think of it in another way. It's like how you know rather than five it's 50 Jeremy", "tokens": [1610, 281, 519, 295, 309, 294, 1071, 636, 13, 467, 311, 411, 577, 291, 458, 2831, 813, 1732, 309, 311, 2625, 17809], "temperature": 0.0, "avg_logprob": -0.3366064727306366, "compression_ratio": 1.4153005464480874, "no_speech_prob": 5.173838417249499e-06}, {"id": 427, "seek": 195110, "start": 1962.86, "end": 1966.3, "text": " I have a question about suppose that your", "tokens": [286, 362, 257, 1168, 466, 7297, 300, 428], "temperature": 0.0, "avg_logprob": -0.3366064727306366, "compression_ratio": 1.4153005464480874, "no_speech_prob": 5.173838417249499e-06}, {"id": 428, "seek": 195110, "start": 1968.62, "end": 1974.08, "text": " Recommendation system is more implicit so you have zeros or ones instead of just", "tokens": [49545, 521, 399, 1185, 307, 544, 26947, 370, 291, 362, 35193, 420, 2306, 2602, 295, 445], "temperature": 0.0, "avg_logprob": -0.3366064727306366, "compression_ratio": 1.4153005464480874, "no_speech_prob": 5.173838417249499e-06}, {"id": 429, "seek": 195110, "start": 1975.1399999999999, "end": 1977.74, "text": " actual numbers right so basically we would then", "tokens": [3539, 3547, 558, 370, 1936, 321, 576, 550], "temperature": 0.0, "avg_logprob": -0.3366064727306366, "compression_ratio": 1.4153005464480874, "no_speech_prob": 5.173838417249499e-06}, {"id": 430, "seek": 197774, "start": 1977.74, "end": 1981.3, "text": " Need to use a classifier instead of a regressor", "tokens": [16984, 281, 764, 257, 1508, 9902, 2602, 295, 257, 1121, 735, 284], "temperature": 0.0, "avg_logprob": -0.16991003209894354, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.2887278393900488e-06}, {"id": 431, "seek": 197774, "start": 1983.78, "end": 1987.1, "text": " Have to sample the negative for something like that, so if you don't have it", "tokens": [3560, 281, 6889, 264, 3671, 337, 746, 411, 300, 11, 370, 498, 291, 500, 380, 362, 309], "temperature": 0.0, "avg_logprob": -0.16991003209894354, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.2887278393900488e-06}, {"id": 432, "seek": 197774, "start": 1987.1, "end": 1991.42, "text": " We just have once let's say like just kind of implicit feedback. Oh", "tokens": [492, 445, 362, 1564, 718, 311, 584, 411, 445, 733, 295, 26947, 5824, 13, 876], "temperature": 0.0, "avg_logprob": -0.16991003209894354, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.2887278393900488e-06}, {"id": 433, "seek": 197774, "start": 1992.22, "end": 1994.3, "text": " I'm not sure we'll get to that one in this class", "tokens": [286, 478, 406, 988, 321, 603, 483, 281, 300, 472, 294, 341, 1508], "temperature": 0.0, "avg_logprob": -0.16991003209894354, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.2887278393900488e-06}, {"id": 434, "seek": 197774, "start": 1994.3, "end": 1998.78, "text": " But what I will say is like in the case that you're just doing classification rather than regression", "tokens": [583, 437, 286, 486, 584, 307, 411, 294, 264, 1389, 300, 291, 434, 445, 884, 21538, 2831, 813, 24590], "temperature": 0.0, "avg_logprob": -0.16991003209894354, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.2887278393900488e-06}, {"id": 435, "seek": 197774, "start": 2000.22, "end": 2004.3, "text": " We haven't actually built that in the library yet, maybe somebody this week wants to try adding it", "tokens": [492, 2378, 380, 767, 3094, 300, 294, 264, 6405, 1939, 11, 1310, 2618, 341, 1243, 2738, 281, 853, 5127, 309], "temperature": 0.0, "avg_logprob": -0.16991003209894354, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.2887278393900488e-06}, {"id": 436, "seek": 200430, "start": 2004.3, "end": 2007.82, "text": " there would only be a small number of lines of code you basically have to change the", "tokens": [456, 576, 787, 312, 257, 1359, 1230, 295, 3876, 295, 3089, 291, 1936, 362, 281, 1319, 264], "temperature": 0.0, "avg_logprob": -0.19267868577388295, "compression_ratio": 1.8127490039840637, "no_speech_prob": 3.2887317047425313e-06}, {"id": 437, "seek": 200430, "start": 2008.7, "end": 2012.4199999999998, "text": " activation function to be a sigmoid and you would have to change the", "tokens": [24433, 2445, 281, 312, 257, 4556, 3280, 327, 293, 291, 576, 362, 281, 1319, 264], "temperature": 0.0, "avg_logprob": -0.19267868577388295, "compression_ratio": 1.8127490039840637, "no_speech_prob": 3.2887317047425313e-06}, {"id": 438, "seek": 200430, "start": 2013.58, "end": 2017.02, "text": " Criterion or the loss function to be cross entropy", "tokens": [4779, 1681, 313, 420, 264, 4470, 2445, 281, 312, 3278, 30867], "temperature": 0.0, "avg_logprob": -0.19267868577388295, "compression_ratio": 1.8127490039840637, "no_speech_prob": 3.2887317047425313e-06}, {"id": 439, "seek": 200430, "start": 2017.54, "end": 2019.54, "text": " rather than", "tokens": [2831, 813], "temperature": 0.0, "avg_logprob": -0.19267868577388295, "compression_ratio": 1.8127490039840637, "no_speech_prob": 3.2887317047425313e-06}, {"id": 440, "seek": 200430, "start": 2019.58, "end": 2021.96, "text": " RMSC and that will give you a", "tokens": [497, 10288, 34, 293, 300, 486, 976, 291, 257], "temperature": 0.0, "avg_logprob": -0.19267868577388295, "compression_ratio": 1.8127490039840637, "no_speech_prob": 3.2887317047425313e-06}, {"id": 441, "seek": 200430, "start": 2023.02, "end": 2027.74, "text": " Classifier rather than a regressor those are the only things you'd have to change so hopefully somebody this week", "tokens": [9471, 9902, 2831, 813, 257, 1121, 735, 284, 729, 366, 264, 787, 721, 291, 1116, 362, 281, 1319, 370, 4696, 2618, 341, 1243], "temperature": 0.0, "avg_logprob": -0.19267868577388295, "compression_ratio": 1.8127490039840637, "no_speech_prob": 3.2887317047425313e-06}, {"id": 442, "seek": 200430, "start": 2027.74, "end": 2031.72, "text": " We'll take up that challenge and by the time we come back next week. We will have that working", "tokens": [492, 603, 747, 493, 300, 3430, 293, 538, 264, 565, 321, 808, 646, 958, 1243, 13, 492, 486, 362, 300, 1364], "temperature": 0.0, "avg_logprob": -0.19267868577388295, "compression_ratio": 1.8127490039840637, "no_speech_prob": 3.2887317047425313e-06}, {"id": 443, "seek": 203172, "start": 2031.72, "end": 2033.72, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.21082124365381447, "compression_ratio": 1.803370786516854, "no_speech_prob": 6.14410328125814e-06}, {"id": 444, "seek": 203172, "start": 2035.08, "end": 2043.5, "text": " So I said that we're basically doing a dot product right or no a dot product is kind of the vector version", "tokens": [407, 286, 848, 300, 321, 434, 1936, 884, 257, 5893, 1674, 558, 420, 572, 257, 5893, 1674, 307, 733, 295, 264, 8062, 3037], "temperature": 0.0, "avg_logprob": -0.21082124365381447, "compression_ratio": 1.803370786516854, "no_speech_prob": 6.14410328125814e-06}, {"id": 445, "seek": 203172, "start": 2043.5, "end": 2045.64, "text": " I guess if this matrix product", "tokens": [286, 2041, 498, 341, 8141, 1674], "temperature": 0.0, "avg_logprob": -0.21082124365381447, "compression_ratio": 1.803370786516854, "no_speech_prob": 6.14410328125814e-06}, {"id": 446, "seek": 203172, "start": 2046.8, "end": 2051.76, "text": " So we're basically doing each of these things times each of these things and then add it together", "tokens": [407, 321, 434, 1936, 884, 1184, 295, 613, 721, 1413, 1184, 295, 613, 721, 293, 550, 909, 309, 1214], "temperature": 0.0, "avg_logprob": -0.21082124365381447, "compression_ratio": 1.803370786516854, "no_speech_prob": 6.14410328125814e-06}, {"id": 447, "seek": 203172, "start": 2052.04, "end": 2057.06, "text": " So that's a dot product, so let's just have a look at how we do that in pi torch", "tokens": [407, 300, 311, 257, 5893, 1674, 11, 370, 718, 311, 445, 362, 257, 574, 412, 577, 321, 360, 300, 294, 3895, 27822], "temperature": 0.0, "avg_logprob": -0.21082124365381447, "compression_ratio": 1.803370786516854, "no_speech_prob": 6.14410328125814e-06}, {"id": 448, "seek": 205706, "start": 2057.06, "end": 2062.42, "text": " So we can create a tensor in pi torch just using this little capital T thing", "tokens": [407, 321, 393, 1884, 257, 40863, 294, 3895, 27822, 445, 1228, 341, 707, 4238, 314, 551], "temperature": 0.0, "avg_logprob": -0.19449349205092628, "compression_ratio": 1.7168141592920354, "no_speech_prob": 5.453285325529578e-07}, {"id": 449, "seek": 205706, "start": 2062.82, "end": 2068.86, "text": " You can just say that's the fast AI version the full version is torched up from non-pi or something", "tokens": [509, 393, 445, 584, 300, 311, 264, 2370, 7318, 3037, 264, 1577, 3037, 307, 3930, 19318, 493, 490, 2107, 12, 22630, 420, 746], "temperature": 0.0, "avg_logprob": -0.19449349205092628, "compression_ratio": 1.7168141592920354, "no_speech_prob": 5.453285325529578e-07}, {"id": 450, "seek": 205706, "start": 2069.1, "end": 2073.32, "text": " But I've got it set up so you can possibly pass in even a list of lists", "tokens": [583, 286, 600, 658, 309, 992, 493, 370, 291, 393, 6264, 1320, 294, 754, 257, 1329, 295, 14511], "temperature": 0.0, "avg_logprob": -0.19449349205092628, "compression_ratio": 1.7168141592920354, "no_speech_prob": 5.453285325529578e-07}, {"id": 451, "seek": 205706, "start": 2073.44, "end": 2078.38, "text": " So this is going to create a torch tensor with one two three four", "tokens": [407, 341, 307, 516, 281, 1884, 257, 27822, 40863, 365, 472, 732, 1045, 1451], "temperature": 0.0, "avg_logprob": -0.19449349205092628, "compression_ratio": 1.7168141592920354, "no_speech_prob": 5.453285325529578e-07}, {"id": 452, "seek": 205706, "start": 2078.38, "end": 2083.58, "text": " And then here's a torch tensor with two two ten ten okay, so here are two", "tokens": [400, 550, 510, 311, 257, 27822, 40863, 365, 732, 732, 2064, 2064, 1392, 11, 370, 510, 366, 732], "temperature": 0.0, "avg_logprob": -0.19449349205092628, "compression_ratio": 1.7168141592920354, "no_speech_prob": 5.453285325529578e-07}, {"id": 453, "seek": 208358, "start": 2083.58, "end": 2091.7, "text": " Torch tensors I didn't say dot Cuda, so they're not on the GPU. They're sitting on the CPU just FYI", "tokens": [7160, 339, 10688, 830, 286, 994, 380, 584, 5893, 383, 11152, 11, 370, 436, 434, 406, 322, 264, 18407, 13, 814, 434, 3798, 322, 264, 13199, 445, 42730, 40], "temperature": 0.0, "avg_logprob": -0.21462169647216797, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.4060957457550103e-06}, {"id": 454, "seek": 208358, "start": 2092.9, "end": 2094.9, "text": " We can multiply them together", "tokens": [492, 393, 12972, 552, 1214], "temperature": 0.0, "avg_logprob": -0.21462169647216797, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.4060957457550103e-06}, {"id": 455, "seek": 208358, "start": 2095.1, "end": 2102.46, "text": " All right, and so anytime you have a mathematical operator between tensors in non-pi or pi torch", "tokens": [1057, 558, 11, 293, 370, 13038, 291, 362, 257, 18894, 12973, 1296, 10688, 830, 294, 2107, 12, 22630, 420, 3895, 27822], "temperature": 0.0, "avg_logprob": -0.21462169647216797, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.4060957457550103e-06}, {"id": 456, "seek": 208358, "start": 2103.18, "end": 2105.18, "text": " It will do element wise", "tokens": [467, 486, 360, 4478, 10829], "temperature": 0.0, "avg_logprob": -0.21462169647216797, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.4060957457550103e-06}, {"id": 457, "seek": 208358, "start": 2105.34, "end": 2108.58, "text": " Assuming that they're the same dimensionality which they are they're both two by two", "tokens": [6281, 24919, 300, 436, 434, 264, 912, 10139, 1860, 597, 436, 366, 436, 434, 1293, 732, 538, 732], "temperature": 0.0, "avg_logprob": -0.21462169647216797, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.4060957457550103e-06}, {"id": 458, "seek": 208358, "start": 2109.5, "end": 2111.5, "text": " Okay, and so here we've got", "tokens": [1033, 11, 293, 370, 510, 321, 600, 658], "temperature": 0.0, "avg_logprob": -0.21462169647216797, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.4060957457550103e-06}, {"id": 459, "seek": 211150, "start": 2111.5, "end": 2113.5, "text": " two by two is four", "tokens": [732, 538, 732, 307, 1451], "temperature": 0.0, "avg_logprob": -0.16123238476839932, "compression_ratio": 1.5174825174825175, "no_speech_prob": 1.0030122439275146e-06}, {"id": 460, "seek": 211150, "start": 2114.14, "end": 2117.98, "text": " Three by ten is thirty and so forth okay, so there's a times B", "tokens": [6244, 538, 2064, 307, 11790, 293, 370, 5220, 1392, 11, 370, 456, 311, 257, 1413, 363], "temperature": 0.0, "avg_logprob": -0.16123238476839932, "compression_ratio": 1.5174825174825175, "no_speech_prob": 1.0030122439275146e-06}, {"id": 461, "seek": 211150, "start": 2118.94, "end": 2123.58, "text": " So if you think about basically what we want to do here is we want to take", "tokens": [407, 498, 291, 519, 466, 1936, 437, 321, 528, 281, 360, 510, 307, 321, 528, 281, 747], "temperature": 0.0, "avg_logprob": -0.16123238476839932, "compression_ratio": 1.5174825174825175, "no_speech_prob": 1.0030122439275146e-06}, {"id": 462, "seek": 211150, "start": 2129.62, "end": 2131.62, "text": " Okay, so I've got one", "tokens": [1033, 11, 370, 286, 600, 658, 472], "temperature": 0.0, "avg_logprob": -0.16123238476839932, "compression_ratio": 1.5174825174825175, "no_speech_prob": 1.0030122439275146e-06}, {"id": 463, "seek": 211150, "start": 2133.14, "end": 2136.54, "text": " Times two is two two times two is four", "tokens": [11366, 732, 307, 732, 732, 1413, 732, 307, 1451], "temperature": 0.0, "avg_logprob": -0.16123238476839932, "compression_ratio": 1.5174825174825175, "no_speech_prob": 1.0030122439275146e-06}, {"id": 464, "seek": 213654, "start": 2136.54, "end": 2144.42, "text": " Two plus four is six and so that is actually the dot product between one two and two four and", "tokens": [4453, 1804, 1451, 307, 2309, 293, 370, 300, 307, 767, 264, 5893, 1674, 1296, 472, 732, 293, 732, 1451, 293], "temperature": 0.0, "avg_logprob": -0.19340768751207288, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.0677011914594914e-06}, {"id": 465, "seek": 213654, "start": 2144.7, "end": 2152.2599999999998, "text": " Then here we've got three by ten is thirty four by forty sorry four by ten is forty thirty and forty is seventy", "tokens": [1396, 510, 321, 600, 658, 1045, 538, 2064, 307, 11790, 1451, 538, 15815, 2597, 1451, 538, 2064, 307, 15815, 11790, 293, 15815, 307, 25662], "temperature": 0.0, "avg_logprob": -0.19340768751207288, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.0677011914594914e-06}, {"id": 466, "seek": 213654, "start": 2152.62, "end": 2158.2, "text": " So in other words a times B dot sum along the first dimension", "tokens": [407, 294, 661, 2283, 257, 1413, 363, 5893, 2408, 2051, 264, 700, 10139], "temperature": 0.0, "avg_logprob": -0.19340768751207288, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.0677011914594914e-06}, {"id": 467, "seek": 213654, "start": 2158.2, "end": 2165.54, "text": " So that's summing up the columns in other words across a row okay this thing here is doing the dot product of", "tokens": [407, 300, 311, 2408, 2810, 493, 264, 13766, 294, 661, 2283, 2108, 257, 5386, 1392, 341, 551, 510, 307, 884, 264, 5893, 1674, 295], "temperature": 0.0, "avg_logprob": -0.19340768751207288, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.0677011914594914e-06}, {"id": 468, "seek": 216554, "start": 2165.54, "end": 2169.7599999999998, "text": " each of these rows with each of these rows", "tokens": [1184, 295, 613, 13241, 365, 1184, 295, 613, 13241], "temperature": 0.0, "avg_logprob": -0.17115427017211915, "compression_ratio": 1.6209677419354838, "no_speech_prob": 1.248268404197006e-06}, {"id": 469, "seek": 216554, "start": 2170.86, "end": 2173.54, "text": " So make sense and obviously we could do that with", "tokens": [407, 652, 2020, 293, 2745, 321, 727, 360, 300, 365], "temperature": 0.0, "avg_logprob": -0.17115427017211915, "compression_ratio": 1.6209677419354838, "no_speech_prob": 1.248268404197006e-06}, {"id": 470, "seek": 216554, "start": 2174.7799999999997, "end": 2179.88, "text": " You know some kind of matrix modification approach, but I'm trying to really do things with as little", "tokens": [509, 458, 512, 733, 295, 8141, 26747, 3109, 11, 457, 286, 478, 1382, 281, 534, 360, 721, 365, 382, 707], "temperature": 0.0, "avg_logprob": -0.17115427017211915, "compression_ratio": 1.6209677419354838, "no_speech_prob": 1.248268404197006e-06}, {"id": 471, "seek": 216554, "start": 2180.82, "end": 2182.82, "text": " special case stuff as possible", "tokens": [2121, 1389, 1507, 382, 1944], "temperature": 0.0, "avg_logprob": -0.17115427017211915, "compression_ratio": 1.6209677419354838, "no_speech_prob": 1.248268404197006e-06}, {"id": 472, "seek": 216554, "start": 2183.14, "end": 2189.66, "text": " Okay, so that's what we're going to use for our dot products from now on so basically all we need to do now is", "tokens": [1033, 11, 370, 300, 311, 437, 321, 434, 516, 281, 764, 337, 527, 5893, 3383, 490, 586, 322, 370, 1936, 439, 321, 643, 281, 360, 586, 307], "temperature": 0.0, "avg_logprob": -0.17115427017211915, "compression_ratio": 1.6209677419354838, "no_speech_prob": 1.248268404197006e-06}, {"id": 473, "seek": 218966, "start": 2189.66, "end": 2196.46, "text": " Is remember we have the data we have is not in that crosstab format so in Excel", "tokens": [1119, 1604, 321, 362, 264, 1412, 321, 362, 307, 406, 294, 300, 28108, 372, 455, 7877, 370, 294, 19060], "temperature": 0.0, "avg_logprob": -0.22278785705566406, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.1015928294000332e-06}, {"id": 474, "seek": 218966, "start": 2196.46, "end": 2199.98, "text": " We've got it in this crosstab format, but we've got it here in this", "tokens": [492, 600, 658, 309, 294, 341, 28108, 372, 455, 7877, 11, 457, 321, 600, 658, 309, 510, 294, 341], "temperature": 0.0, "avg_logprob": -0.22278785705566406, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.1015928294000332e-06}, {"id": 475, "seek": 218966, "start": 2200.8199999999997, "end": 2203.58, "text": " Listed format user movie rating user movie rating", "tokens": [17668, 292, 7877, 4195, 3169, 10990, 4195, 3169, 10990], "temperature": 0.0, "avg_logprob": -0.22278785705566406, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.1015928294000332e-06}, {"id": 476, "seek": 218966, "start": 2204.14, "end": 2207.2999999999997, "text": " So conceptually we want to be like looking up this user", "tokens": [407, 3410, 671, 321, 528, 281, 312, 411, 1237, 493, 341, 4195], "temperature": 0.0, "avg_logprob": -0.22278785705566406, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.1015928294000332e-06}, {"id": 477, "seek": 218966, "start": 2207.8599999999997, "end": 2214.8199999999997, "text": " Into our embedding matrix to find their 50 factors looking up that movie to find their 50 factors and then take the dot product", "tokens": [23373, 527, 12240, 3584, 8141, 281, 915, 641, 2625, 6771, 1237, 493, 300, 3169, 281, 915, 641, 2625, 6771, 293, 550, 747, 264, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.22278785705566406, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.1015928294000332e-06}, {"id": 478, "seek": 218966, "start": 2215.06, "end": 2217.54, "text": " of those two 50 long vectors", "tokens": [295, 729, 732, 2625, 938, 18875], "temperature": 0.0, "avg_logprob": -0.22278785705566406, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.1015928294000332e-06}, {"id": 479, "seek": 221754, "start": 2217.54, "end": 2219.54, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.23445761958255043, "compression_ratio": 1.7037037037037037, "no_speech_prob": 5.714998110306624e-07}, {"id": 480, "seek": 221754, "start": 2220.38, "end": 2222.38, "text": " Let's do that", "tokens": [961, 311, 360, 300], "temperature": 0.0, "avg_logprob": -0.23445761958255043, "compression_ratio": 1.7037037037037037, "no_speech_prob": 5.714998110306624e-07}, {"id": 481, "seek": 221754, "start": 2224.22, "end": 2229.14, "text": " To do it we're going to build a layer our own custom", "tokens": [1407, 360, 309, 321, 434, 516, 281, 1322, 257, 4583, 527, 1065, 2375], "temperature": 0.0, "avg_logprob": -0.23445761958255043, "compression_ratio": 1.7037037037037037, "no_speech_prob": 5.714998110306624e-07}, {"id": 482, "seek": 221754, "start": 2229.94, "end": 2234.5, "text": " Neural net layer that's not going to be a neural net right so the the the more generic", "tokens": [1734, 1807, 2533, 4583, 300, 311, 406, 516, 281, 312, 257, 18161, 2533, 558, 370, 264, 264, 264, 544, 19577], "temperature": 0.0, "avg_logprob": -0.23445761958255043, "compression_ratio": 1.7037037037037037, "no_speech_prob": 5.714998110306624e-07}, {"id": 483, "seek": 221754, "start": 2235.14, "end": 2239.02, "text": " Vocabulary we call this is we're going to build a pie torch module", "tokens": [8993, 19189, 321, 818, 341, 307, 321, 434, 516, 281, 1322, 257, 1730, 27822, 10088], "temperature": 0.0, "avg_logprob": -0.23445761958255043, "compression_ratio": 1.7037037037037037, "no_speech_prob": 5.714998110306624e-07}, {"id": 484, "seek": 221754, "start": 2239.62, "end": 2243.54, "text": " Okay, so a pie torch module is a very specific thing", "tokens": [1033, 11, 370, 257, 1730, 27822, 10088, 307, 257, 588, 2685, 551], "temperature": 0.0, "avg_logprob": -0.23445761958255043, "compression_ratio": 1.7037037037037037, "no_speech_prob": 5.714998110306624e-07}, {"id": 485, "seek": 224354, "start": 2243.54, "end": 2249.62, "text": " It's something that you can use as a layer in a neural net once you've created your own pie torch module", "tokens": [467, 311, 746, 300, 291, 393, 764, 382, 257, 4583, 294, 257, 18161, 2533, 1564, 291, 600, 2942, 428, 1065, 1730, 27822, 10088], "temperature": 0.0, "avg_logprob": -0.15124580540607885, "compression_ratio": 1.7743362831858407, "no_speech_prob": 9.422428206562472e-07}, {"id": 486, "seek": 224354, "start": 2249.62, "end": 2251.62, "text": " You can throw it into a neural net", "tokens": [509, 393, 3507, 309, 666, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.15124580540607885, "compression_ratio": 1.7743362831858407, "no_speech_prob": 9.422428206562472e-07}, {"id": 487, "seek": 224354, "start": 2251.7, "end": 2256.96, "text": " And a module works by assuming we've already got one say called model", "tokens": [400, 257, 10088, 1985, 538, 11926, 321, 600, 1217, 658, 472, 584, 1219, 2316], "temperature": 0.0, "avg_logprob": -0.15124580540607885, "compression_ratio": 1.7743362831858407, "no_speech_prob": 9.422428206562472e-07}, {"id": 488, "seek": 224354, "start": 2256.96, "end": 2265.5, "text": " You can pass in some things in parentheses, and it will calculate it right so assuming that we already have a module called dot product", "tokens": [509, 393, 1320, 294, 512, 721, 294, 34153, 11, 293, 309, 486, 8873, 309, 558, 370, 11926, 300, 321, 1217, 362, 257, 10088, 1219, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.15124580540607885, "compression_ratio": 1.7743362831858407, "no_speech_prob": 9.422428206562472e-07}, {"id": 489, "seek": 224354, "start": 2266.7799999999997, "end": 2268.94, "text": " We can instantiate it like so", "tokens": [492, 393, 9836, 13024, 309, 411, 370], "temperature": 0.0, "avg_logprob": -0.15124580540607885, "compression_ratio": 1.7743362831858407, "no_speech_prob": 9.422428206562472e-07}, {"id": 490, "seek": 224354, "start": 2269.62, "end": 2271.62, "text": " to create our dot product", "tokens": [281, 1884, 527, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.15124580540607885, "compression_ratio": 1.7743362831858407, "no_speech_prob": 9.422428206562472e-07}, {"id": 491, "seek": 227162, "start": 2271.62, "end": 2275.52, "text": " Object and we can basically now treat that like a function", "tokens": [24753, 293, 321, 393, 1936, 586, 2387, 300, 411, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1854008674621582, "compression_ratio": 1.6825396825396826, "no_speech_prob": 1.4144734450383112e-06}, {"id": 492, "seek": 227162, "start": 2276.1, "end": 2278.98, "text": " All right, but the thing is it's not just a function", "tokens": [1057, 558, 11, 457, 264, 551, 307, 309, 311, 406, 445, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1854008674621582, "compression_ratio": 1.6825396825396826, "no_speech_prob": 1.4144734450383112e-06}, {"id": 493, "seek": 227162, "start": 2279.58, "end": 2285.06, "text": " Because we'll be able to do things like take derivatives of it stack them up together into a big", "tokens": [1436, 321, 603, 312, 1075, 281, 360, 721, 411, 747, 33733, 295, 309, 8630, 552, 493, 1214, 666, 257, 955], "temperature": 0.0, "avg_logprob": -0.1854008674621582, "compression_ratio": 1.6825396825396826, "no_speech_prob": 1.4144734450383112e-06}, {"id": 494, "seek": 227162, "start": 2285.8599999999997, "end": 2293.54, "text": " Stack of neural network layers blah blah blah, right, so it's basically a function that we can kind of compose very conveniently", "tokens": [37649, 295, 18161, 3209, 7914, 12288, 12288, 12288, 11, 558, 11, 370, 309, 311, 1936, 257, 2445, 300, 321, 393, 733, 295, 35925, 588, 44375], "temperature": 0.0, "avg_logprob": -0.1854008674621582, "compression_ratio": 1.6825396825396826, "no_speech_prob": 1.4144734450383112e-06}, {"id": 495, "seek": 227162, "start": 2294.2599999999998, "end": 2300.74, "text": " So here how do we define a module which as you can see here returns a dot product well", "tokens": [407, 510, 577, 360, 321, 6964, 257, 10088, 597, 382, 291, 393, 536, 510, 11247, 257, 5893, 1674, 731], "temperature": 0.0, "avg_logprob": -0.1854008674621582, "compression_ratio": 1.6825396825396826, "no_speech_prob": 1.4144734450383112e-06}, {"id": 496, "seek": 230074, "start": 2300.74, "end": 2303.02, "text": " We have to create a Python class", "tokens": [492, 362, 281, 1884, 257, 15329, 1508], "temperature": 0.0, "avg_logprob": -0.21984098507807806, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.4823552874076995e-06}, {"id": 497, "seek": 230074, "start": 2303.62, "end": 2308.06, "text": " And so if you haven't done Python OO before you're going to have to learn", "tokens": [400, 370, 498, 291, 2378, 380, 1096, 15329, 422, 46, 949, 291, 434, 516, 281, 362, 281, 1466], "temperature": 0.0, "avg_logprob": -0.21984098507807806, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.4823552874076995e-06}, {"id": 498, "seek": 230074, "start": 2308.5, "end": 2315.9199999999996, "text": " Because all pie torch modules are written in Python OO and it's one of the things I really like about pie torch is that it doesn't", "tokens": [1436, 439, 1730, 27822, 16679, 366, 3720, 294, 15329, 422, 46, 293, 309, 311, 472, 295, 264, 721, 286, 534, 411, 466, 1730, 27822, 307, 300, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.21984098507807806, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.4823552874076995e-06}, {"id": 499, "seek": 230074, "start": 2316.8999999999996, "end": 2322.22, "text": " Reinvent totally new ways of doing things like tensorflow does all the time in pie torch", "tokens": [42116, 2475, 3879, 777, 2098, 295, 884, 721, 411, 40863, 10565, 775, 439, 264, 565, 294, 1730, 27822], "temperature": 0.0, "avg_logprob": -0.21984098507807806, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.4823552874076995e-06}, {"id": 500, "seek": 230074, "start": 2322.22, "end": 2324.22, "text": " They you know really tend to use", "tokens": [814, 291, 458, 534, 3928, 281, 764], "temperature": 0.0, "avg_logprob": -0.21984098507807806, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.4823552874076995e-06}, {"id": 501, "seek": 230074, "start": 2324.8199999999997, "end": 2326.7799999999997, "text": " Pythonic ways to do things so in this case", "tokens": [9953, 392, 11630, 2098, 281, 360, 721, 370, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.21984098507807806, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.4823552874076995e-06}, {"id": 502, "seek": 232678, "start": 2326.78, "end": 2332.26, "text": " How do you create you know some kind of new behavior you create a Python class?", "tokens": [1012, 360, 291, 1884, 291, 458, 512, 733, 295, 777, 5223, 291, 1884, 257, 15329, 1508, 30], "temperature": 0.0, "avg_logprob": -0.2684959720920872, "compression_ratio": 1.4565217391304348, "no_speech_prob": 6.681497097815736e-07}, {"id": 503, "seek": 232678, "start": 2334.5400000000004, "end": 2338.6200000000003, "text": " So Jeremy suppose that you have a lot of data", "tokens": [407, 17809, 7297, 300, 291, 362, 257, 688, 295, 1412], "temperature": 0.0, "avg_logprob": -0.2684959720920872, "compression_ratio": 1.4565217391304348, "no_speech_prob": 6.681497097815736e-07}, {"id": 504, "seek": 232678, "start": 2339.3, "end": 2345.78, "text": " Not just a little bit of data you can have in memory. Will you be able to use fast AI to solve color refiltering?", "tokens": [1726, 445, 257, 707, 857, 295, 1412, 291, 393, 362, 294, 4675, 13, 3099, 291, 312, 1075, 281, 764, 2370, 7318, 281, 5039, 2017, 1895, 388, 34200, 30], "temperature": 0.0, "avg_logprob": -0.2684959720920872, "compression_ratio": 1.4565217391304348, "no_speech_prob": 6.681497097815736e-07}, {"id": 505, "seek": 232678, "start": 2346.5, "end": 2348.5, "text": " Yes, absolutely", "tokens": [1079, 11, 3122], "temperature": 0.0, "avg_logprob": -0.2684959720920872, "compression_ratio": 1.4565217391304348, "no_speech_prob": 6.681497097815736e-07}, {"id": 506, "seek": 232678, "start": 2348.6600000000003, "end": 2350.6600000000003, "text": " It's it uses", "tokens": [467, 311, 309, 4960], "temperature": 0.0, "avg_logprob": -0.2684959720920872, "compression_ratio": 1.4565217391304348, "no_speech_prob": 6.681497097815736e-07}, {"id": 507, "seek": 235066, "start": 2350.66, "end": 2356.98, "text": " Mini batch stochastic gradient descent which does it a batch at a time", "tokens": [18239, 15245, 342, 8997, 2750, 16235, 23475, 597, 775, 309, 257, 15245, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.24501881833936348, "compression_ratio": 1.525974025974026, "no_speech_prob": 2.419882889626024e-07}, {"id": 508, "seek": 235066, "start": 2358.7, "end": 2360.7, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.24501881833936348, "compression_ratio": 1.525974025974026, "no_speech_prob": 2.419882889626024e-07}, {"id": 509, "seek": 235066, "start": 2364.98, "end": 2367.7599999999998, "text": " This particular version is going to create a", "tokens": [639, 1729, 3037, 307, 516, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.24501881833936348, "compression_ratio": 1.525974025974026, "no_speech_prob": 2.419882889626024e-07}, {"id": 510, "seek": 235066, "start": 2368.8599999999997, "end": 2372.58, "text": " Pandas data frame and a pandas data frame has to live in memory", "tokens": [16995, 296, 1412, 3920, 293, 257, 4565, 296, 1412, 3920, 575, 281, 1621, 294, 4675], "temperature": 0.0, "avg_logprob": -0.24501881833936348, "compression_ratio": 1.525974025974026, "no_speech_prob": 2.419882889626024e-07}, {"id": 511, "seek": 235066, "start": 2374.18, "end": 2377.8999999999996, "text": " Having said that you can get easily five twelve gig", "tokens": [10222, 848, 300, 291, 393, 483, 3612, 1732, 14390, 8741], "temperature": 0.0, "avg_logprob": -0.24501881833936348, "compression_ratio": 1.525974025974026, "no_speech_prob": 2.419882889626024e-07}, {"id": 512, "seek": 237790, "start": 2377.9, "end": 2383.7000000000003, "text": " You know instances on Amazon so like if you had a CSV that was bigger than 512 gig", "tokens": [509, 458, 14519, 322, 6795, 370, 411, 498, 291, 632, 257, 48814, 300, 390, 3801, 813, 1025, 4762, 8741], "temperature": 0.0, "avg_logprob": -0.21780584276336984, "compression_ratio": 1.6016597510373445, "no_speech_prob": 5.255350060906494e-06}, {"id": 513, "seek": 237790, "start": 2385.34, "end": 2387.98, "text": " You know that would be impressive if that did happen", "tokens": [509, 458, 300, 576, 312, 8992, 498, 300, 630, 1051], "temperature": 0.0, "avg_logprob": -0.21780584276336984, "compression_ratio": 1.6016597510373445, "no_speech_prob": 5.255350060906494e-06}, {"id": 514, "seek": 237790, "start": 2387.98, "end": 2391.82, "text": " I guess you would have to instead save that as a beak holes array and", "tokens": [286, 2041, 291, 576, 362, 281, 2602, 3155, 300, 382, 257, 48663, 8118, 10225, 293], "temperature": 0.0, "avg_logprob": -0.21780584276336984, "compression_ratio": 1.6016597510373445, "no_speech_prob": 5.255350060906494e-06}, {"id": 515, "seek": 237790, "start": 2392.26, "end": 2398.42, "text": " Create a slightly different version that reads from a beak holes array to streaming in or maybe from a desk", "tokens": [20248, 257, 4748, 819, 3037, 300, 15700, 490, 257, 48663, 8118, 10225, 281, 11791, 294, 420, 1310, 490, 257, 10026], "temperature": 0.0, "avg_logprob": -0.21780584276336984, "compression_ratio": 1.6016597510373445, "no_speech_prob": 5.255350060906494e-06}, {"id": 516, "seek": 237790, "start": 2399.1, "end": 2401.1, "text": " data frame which also so", "tokens": [1412, 3920, 597, 611, 370], "temperature": 0.0, "avg_logprob": -0.21780584276336984, "compression_ratio": 1.6016597510373445, "no_speech_prob": 5.255350060906494e-06}, {"id": 517, "seek": 237790, "start": 2402.38, "end": 2404.58, "text": " It would be easy to do. I don't think I've seen", "tokens": [467, 576, 312, 1858, 281, 360, 13, 286, 500, 380, 519, 286, 600, 1612], "temperature": 0.0, "avg_logprob": -0.21780584276336984, "compression_ratio": 1.6016597510373445, "no_speech_prob": 5.255350060906494e-06}, {"id": 518, "seek": 240458, "start": 2404.58, "end": 2407.38, "text": " real-world situations where you have", "tokens": [957, 12, 13217, 6851, 689, 291, 362], "temperature": 0.0, "avg_logprob": -0.2501234831633391, "compression_ratio": 1.462962962962963, "no_speech_prob": 2.4439800654363353e-06}, {"id": 519, "seek": 240458, "start": 2408.1, "end": 2411.54, "text": " 512 gigabyte of collaborative filtering matrices, but yeah", "tokens": [1025, 4762, 8741, 34529, 295, 16555, 30822, 32284, 11, 457, 1338], "temperature": 0.0, "avg_logprob": -0.2501234831633391, "compression_ratio": 1.462962962962963, "no_speech_prob": 2.4439800654363353e-06}, {"id": 520, "seek": 240458, "start": 2412.14, "end": 2414.14, "text": " We can do it", "tokens": [492, 393, 360, 309], "temperature": 0.0, "avg_logprob": -0.2501234831633391, "compression_ratio": 1.462962962962963, "no_speech_prob": 2.4439800654363353e-06}, {"id": 521, "seek": 240458, "start": 2414.58, "end": 2416.58, "text": " Okay now", "tokens": [1033, 586], "temperature": 0.0, "avg_logprob": -0.2501234831633391, "compression_ratio": 1.462962962962963, "no_speech_prob": 2.4439800654363353e-06}, {"id": 522, "seek": 240458, "start": 2417.1, "end": 2424.7999999999997, "text": " This is pipe torch specific this next bit is that when you define like the actual work to be done, which is here return", "tokens": [639, 307, 11240, 27822, 2685, 341, 958, 857, 307, 300, 562, 291, 6964, 411, 264, 3539, 589, 281, 312, 1096, 11, 597, 307, 510, 2736], "temperature": 0.0, "avg_logprob": -0.2501234831633391, "compression_ratio": 1.462962962962963, "no_speech_prob": 2.4439800654363353e-06}, {"id": 523, "seek": 240458, "start": 2425.34, "end": 2427.34, "text": " user times movie dot sum", "tokens": [4195, 1413, 3169, 5893, 2408], "temperature": 0.0, "avg_logprob": -0.2501234831633391, "compression_ratio": 1.462962962962963, "no_speech_prob": 2.4439800654363353e-06}, {"id": 524, "seek": 240458, "start": 2428.14, "end": 2431.62, "text": " You have to put it in a special method called forward", "tokens": [509, 362, 281, 829, 309, 294, 257, 2121, 3170, 1219, 2128], "temperature": 0.0, "avg_logprob": -0.2501234831633391, "compression_ratio": 1.462962962962963, "no_speech_prob": 2.4439800654363353e-06}, {"id": 525, "seek": 243162, "start": 2431.62, "end": 2438.14, "text": " Okay, and this is this idea that like it's very likely you're pretty neural net right in a neural net the thing where you calculate", "tokens": [1033, 11, 293, 341, 307, 341, 1558, 300, 411, 309, 311, 588, 3700, 291, 434, 1238, 18161, 2533, 558, 294, 257, 18161, 2533, 264, 551, 689, 291, 8873], "temperature": 0.0, "avg_logprob": -0.1643288988585866, "compression_ratio": 1.8687258687258688, "no_speech_prob": 4.936963705404196e-06}, {"id": 526, "seek": 243162, "start": 2438.58, "end": 2440.42, "text": " the next", "tokens": [264, 958], "temperature": 0.0, "avg_logprob": -0.1643288988585866, "compression_ratio": 1.8687258687258688, "no_speech_prob": 4.936963705404196e-06}, {"id": 527, "seek": 243162, "start": 2440.42, "end": 2445.98, "text": " Set of activations is called the forward pass and so that's doing a forward calculation", "tokens": [8928, 295, 2430, 763, 307, 1219, 264, 2128, 1320, 293, 370, 300, 311, 884, 257, 2128, 17108], "temperature": 0.0, "avg_logprob": -0.1643288988585866, "compression_ratio": 1.8687258687258688, "no_speech_prob": 4.936963705404196e-06}, {"id": 528, "seek": 243162, "start": 2446.66, "end": 2449.66, "text": " The gradients is called the backward calculation", "tokens": [440, 2771, 2448, 307, 1219, 264, 23897, 17108], "temperature": 0.0, "avg_logprob": -0.1643288988585866, "compression_ratio": 1.8687258687258688, "no_speech_prob": 4.936963705404196e-06}, {"id": 529, "seek": 243162, "start": 2449.66, "end": 2454.54, "text": " We don't have to do that because pie torch calculates that automatically so we just have to define", "tokens": [492, 500, 380, 362, 281, 360, 300, 570, 1730, 27822, 4322, 1024, 300, 6772, 370, 321, 445, 362, 281, 6964], "temperature": 0.0, "avg_logprob": -0.1643288988585866, "compression_ratio": 1.8687258687258688, "no_speech_prob": 4.936963705404196e-06}, {"id": 530, "seek": 245454, "start": 2454.54, "end": 2461.54, "text": " Forward right so we create a new class we define forward and here we write in our definition of dot product", "tokens": [35524, 558, 370, 321, 1884, 257, 777, 1508, 321, 6964, 2128, 293, 510, 321, 2464, 294, 527, 7123, 295, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.18850582838058472, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.9669694160693325e-06}, {"id": 531, "seek": 245454, "start": 2461.82, "end": 2467.86, "text": " Okay, so that's it so now that we've created this class definition", "tokens": [1033, 11, 370, 300, 311, 309, 370, 586, 300, 321, 600, 2942, 341, 1508, 7123], "temperature": 0.0, "avg_logprob": -0.18850582838058472, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.9669694160693325e-06}, {"id": 532, "seek": 245454, "start": 2467.86, "end": 2474.52, "text": " We can instantiate our model right and we can call our model and get back the numbers we expected", "tokens": [492, 393, 9836, 13024, 527, 2316, 558, 293, 321, 393, 818, 527, 2316, 293, 483, 646, 264, 3547, 321, 5176], "temperature": 0.0, "avg_logprob": -0.18850582838058472, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.9669694160693325e-06}, {"id": 533, "seek": 245454, "start": 2474.74, "end": 2477.82, "text": " Okay, so that's it. That's how we create a custom", "tokens": [1033, 11, 370, 300, 311, 309, 13, 663, 311, 577, 321, 1884, 257, 2375], "temperature": 0.0, "avg_logprob": -0.18850582838058472, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.9669694160693325e-06}, {"id": 534, "seek": 245454, "start": 2478.34, "end": 2479.94, "text": " pie torch layer", "tokens": [1730, 27822, 4583], "temperature": 0.0, "avg_logprob": -0.18850582838058472, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.9669694160693325e-06}, {"id": 535, "seek": 245454, "start": 2479.94, "end": 2483.12, "text": " And if you compare that to like any other", "tokens": [400, 498, 291, 6794, 300, 281, 411, 604, 661], "temperature": 0.0, "avg_logprob": -0.18850582838058472, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.9669694160693325e-06}, {"id": 536, "seek": 248312, "start": 2483.12, "end": 2484.12, "text": " other", "tokens": [661], "temperature": 0.0, "avg_logprob": -0.1797356096286218, "compression_ratio": 1.6779661016949152, "no_speech_prob": 1.7330488617517403e-06}, {"id": 537, "seek": 248312, "start": 2484.12, "end": 2486.12, "text": " Library around pretty much. This is way easier", "tokens": [12806, 926, 1238, 709, 13, 639, 307, 636, 3571], "temperature": 0.0, "avg_logprob": -0.1797356096286218, "compression_ratio": 1.6779661016949152, "no_speech_prob": 1.7330488617517403e-06}, {"id": 538, "seek": 248312, "start": 2487.04, "end": 2489.04, "text": " Basically, I guess because we're leveraging", "tokens": [8537, 11, 286, 2041, 570, 321, 434, 32666], "temperature": 0.0, "avg_logprob": -0.1797356096286218, "compression_ratio": 1.6779661016949152, "no_speech_prob": 1.7330488617517403e-06}, {"id": 539, "seek": 248312, "start": 2489.44, "end": 2491.44, "text": " What's already in Python?", "tokens": [708, 311, 1217, 294, 15329, 30], "temperature": 0.0, "avg_logprob": -0.1797356096286218, "compression_ratio": 1.6779661016949152, "no_speech_prob": 1.7330488617517403e-06}, {"id": 540, "seek": 248312, "start": 2491.7599999999998, "end": 2495.12, "text": " So let's go ahead and now create a more complex", "tokens": [407, 718, 311, 352, 2286, 293, 586, 1884, 257, 544, 3997], "temperature": 0.0, "avg_logprob": -0.1797356096286218, "compression_ratio": 1.6779661016949152, "no_speech_prob": 1.7330488617517403e-06}, {"id": 541, "seek": 248312, "start": 2496.2799999999997, "end": 2501.7599999999998, "text": " Module and we're going to basically do the same thing. We're going to have a forward again", "tokens": [48251, 293, 321, 434, 516, 281, 1936, 360, 264, 912, 551, 13, 492, 434, 516, 281, 362, 257, 2128, 797], "temperature": 0.0, "avg_logprob": -0.1797356096286218, "compression_ratio": 1.6779661016949152, "no_speech_prob": 1.7330488617517403e-06}, {"id": 542, "seek": 248312, "start": 2502.44, "end": 2505.02, "text": " We're going to have our users times movies dot sum", "tokens": [492, 434, 516, 281, 362, 527, 5022, 1413, 6233, 5893, 2408], "temperature": 0.0, "avg_logprob": -0.1797356096286218, "compression_ratio": 1.6779661016949152, "no_speech_prob": 1.7330488617517403e-06}, {"id": 543, "seek": 248312, "start": 2505.72, "end": 2509.6, "text": " But we're going to do one more thing beforehand, which is we're going to create two", "tokens": [583, 321, 434, 516, 281, 360, 472, 544, 551, 22893, 11, 597, 307, 321, 434, 516, 281, 1884, 732], "temperature": 0.0, "avg_logprob": -0.1797356096286218, "compression_ratio": 1.6779661016949152, "no_speech_prob": 1.7330488617517403e-06}, {"id": 544, "seek": 250960, "start": 2509.6, "end": 2516.4, "text": " Embedding matrices, and then we're going to look up our users and our movies in those embedding matrices", "tokens": [24234, 292, 3584, 32284, 11, 293, 550, 321, 434, 516, 281, 574, 493, 527, 5022, 293, 527, 6233, 294, 729, 12240, 3584, 32284], "temperature": 0.0, "avg_logprob": -0.17037076950073243, "compression_ratio": 1.7004830917874396, "no_speech_prob": 1.816216695260664e-06}, {"id": 545, "seek": 250960, "start": 2516.62, "end": 2519.48, "text": " So let's go through and and do that", "tokens": [407, 718, 311, 352, 807, 293, 293, 360, 300], "temperature": 0.0, "avg_logprob": -0.17037076950073243, "compression_ratio": 1.7004830917874396, "no_speech_prob": 1.816216695260664e-06}, {"id": 546, "seek": 250960, "start": 2520.68, "end": 2522.72, "text": " so the first thing to realize is that", "tokens": [370, 264, 700, 551, 281, 4325, 307, 300], "temperature": 0.0, "avg_logprob": -0.17037076950073243, "compression_ratio": 1.7004830917874396, "no_speech_prob": 1.816216695260664e-06}, {"id": 547, "seek": 250960, "start": 2524.24, "end": 2529.98, "text": " The users the user IDs and the movie IDs may not be contiguous", "tokens": [440, 5022, 264, 4195, 48212, 293, 264, 3169, 48212, 815, 406, 312, 660, 30525], "temperature": 0.0, "avg_logprob": -0.17037076950073243, "compression_ratio": 1.7004830917874396, "no_speech_prob": 1.816216695260664e-06}, {"id": 548, "seek": 250960, "start": 2530.16, "end": 2538.3199999999997, "text": " You know like they maybe they start at a million and go to a million one thousand say right so if we just used", "tokens": [509, 458, 411, 436, 1310, 436, 722, 412, 257, 2459, 293, 352, 281, 257, 2459, 472, 4714, 584, 558, 370, 498, 321, 445, 1143], "temperature": 0.0, "avg_logprob": -0.17037076950073243, "compression_ratio": 1.7004830917874396, "no_speech_prob": 1.816216695260664e-06}, {"id": 549, "seek": 253832, "start": 2538.32, "end": 2543.0800000000004, "text": " Those IDs directly to look up into an embedding matrix", "tokens": [3950, 48212, 3838, 281, 574, 493, 666, 364, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.17806732013661375, "compression_ratio": 1.5943396226415094, "no_speech_prob": 8.851553729982697e-07}, {"id": 550, "seek": 253832, "start": 2543.0800000000004, "end": 2547.04, "text": " We would have to create an embedding matrix of size 1 million 1,000, right?", "tokens": [492, 576, 362, 281, 1884, 364, 12240, 3584, 8141, 295, 2744, 502, 2459, 502, 11, 1360, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17806732013661375, "compression_ratio": 1.5943396226415094, "no_speech_prob": 8.851553729982697e-07}, {"id": 551, "seek": 253832, "start": 2547.04, "end": 2551.86, "text": " Which we don't want to do so the first thing I do is to get a list of the", "tokens": [3013, 321, 500, 380, 528, 281, 360, 370, 264, 700, 551, 286, 360, 307, 281, 483, 257, 1329, 295, 264], "temperature": 0.0, "avg_logprob": -0.17806732013661375, "compression_ratio": 1.5943396226415094, "no_speech_prob": 8.851553729982697e-07}, {"id": 552, "seek": 253832, "start": 2552.56, "end": 2554.56, "text": " unique user IDs and", "tokens": [3845, 4195, 48212, 293], "temperature": 0.0, "avg_logprob": -0.17806732013661375, "compression_ratio": 1.5943396226415094, "no_speech_prob": 8.851553729982697e-07}, {"id": 553, "seek": 253832, "start": 2555.32, "end": 2559.4, "text": " then I create a mapping from every user ID to a", "tokens": [550, 286, 1884, 257, 18350, 490, 633, 4195, 7348, 281, 257], "temperature": 0.0, "avg_logprob": -0.17806732013661375, "compression_ratio": 1.5943396226415094, "no_speech_prob": 8.851553729982697e-07}, {"id": 554, "seek": 253832, "start": 2560.6800000000003, "end": 2564.92, "text": " contiguous integer this thing I've done here where I've created a", "tokens": [660, 30525, 24922, 341, 551, 286, 600, 1096, 510, 689, 286, 600, 2942, 257], "temperature": 0.0, "avg_logprob": -0.17806732013661375, "compression_ratio": 1.5943396226415094, "no_speech_prob": 8.851553729982697e-07}, {"id": 555, "seek": 256492, "start": 2564.92, "end": 2572.32, "text": " Dictionary which maps from every unique thing to a unique index is well worth studying", "tokens": [413, 4105, 822, 597, 11317, 490, 633, 3845, 551, 281, 257, 3845, 8186, 307, 731, 3163, 7601], "temperature": 0.0, "avg_logprob": -0.19098320748042136, "compression_ratio": 1.6381322957198443, "no_speech_prob": 3.96696304960642e-06}, {"id": 556, "seek": 256492, "start": 2572.56, "end": 2577.2000000000003, "text": " During the week because like it's super super handy is something you very very often", "tokens": [6842, 264, 1243, 570, 411, 309, 311, 1687, 1687, 13239, 307, 746, 291, 588, 588, 2049], "temperature": 0.0, "avg_logprob": -0.19098320748042136, "compression_ratio": 1.6381322957198443, "no_speech_prob": 3.96696304960642e-06}, {"id": 557, "seek": 256492, "start": 2577.48, "end": 2581.7200000000003, "text": " Have to do in all kinds of machine learning right and so I will go through it here", "tokens": [3560, 281, 360, 294, 439, 3685, 295, 3479, 2539, 558, 293, 370, 286, 486, 352, 807, 309, 510], "temperature": 0.0, "avg_logprob": -0.19098320748042136, "compression_ratio": 1.6381322957198443, "no_speech_prob": 3.96696304960642e-06}, {"id": 558, "seek": 256492, "start": 2581.7200000000003, "end": 2584.98, "text": " It's easy enough to figure out if you can't figure it out. Just ask on the forum", "tokens": [467, 311, 1858, 1547, 281, 2573, 484, 498, 291, 393, 380, 2573, 309, 484, 13, 1449, 1029, 322, 264, 17542], "temperature": 0.0, "avg_logprob": -0.19098320748042136, "compression_ratio": 1.6381322957198443, "no_speech_prob": 3.96696304960642e-06}, {"id": 559, "seek": 256492, "start": 2586.16, "end": 2593.96, "text": " Anyway, so once we've got the mapping from user to a contiguous index we then can say", "tokens": [5684, 11, 370, 1564, 321, 600, 658, 264, 18350, 490, 4195, 281, 257, 660, 30525, 8186, 321, 550, 393, 584], "temperature": 0.0, "avg_logprob": -0.19098320748042136, "compression_ratio": 1.6381322957198443, "no_speech_prob": 3.96696304960642e-06}, {"id": 560, "seek": 259396, "start": 2593.96, "end": 2600.32, "text": " Let's now replace the user ID column with that contiguous index", "tokens": [961, 311, 586, 7406, 264, 4195, 7348, 7738, 365, 300, 660, 30525, 8186], "temperature": 0.0, "avg_logprob": -0.22956060521742877, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.934477253002115e-07}, {"id": 561, "seek": 259396, "start": 2600.44, "end": 2604.3, "text": " Right so pandas dot apply applies an arbitrary function", "tokens": [1779, 370, 4565, 296, 5893, 3079, 13165, 364, 23211, 2445], "temperature": 0.0, "avg_logprob": -0.22956060521742877, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.934477253002115e-07}, {"id": 562, "seek": 259396, "start": 2604.68, "end": 2611.46, "text": " In Python lambda is how you create an anonymous function on the fly and this anonymous function simply returns", "tokens": [682, 15329, 13607, 307, 577, 291, 1884, 364, 24932, 2445, 322, 264, 3603, 293, 341, 24932, 2445, 2935, 11247], "temperature": 0.0, "avg_logprob": -0.22956060521742877, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.934477253002115e-07}, {"id": 563, "seek": 259396, "start": 2611.76, "end": 2619.7200000000003, "text": " The index do the same thing for movies and so after that we now have the same ratings table we had before but our", "tokens": [440, 8186, 360, 264, 912, 551, 337, 6233, 293, 370, 934, 300, 321, 586, 362, 264, 912, 24603, 3199, 321, 632, 949, 457, 527], "temperature": 0.0, "avg_logprob": -0.22956060521742877, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.934477253002115e-07}, {"id": 564, "seek": 259396, "start": 2620.6, "end": 2622.7200000000003, "text": " IDs have been mapped to contiguous", "tokens": [48212, 362, 668, 33318, 281, 660, 30525], "temperature": 0.0, "avg_logprob": -0.22956060521742877, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.934477253002115e-07}, {"id": 565, "seek": 262272, "start": 2622.72, "end": 2627.48, "text": " Integers and therefore there are things that we can look up into an embedding matrix", "tokens": [5681, 1146, 433, 293, 4412, 456, 366, 721, 300, 321, 393, 574, 493, 666, 364, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.22271986568675323, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.1544594826773391e-06}, {"id": 566, "seek": 262272, "start": 2628.72, "end": 2635.52, "text": " So let's get the count of our users in our movies and let's now go ahead and try and create our", "tokens": [407, 718, 311, 483, 264, 1207, 295, 527, 5022, 294, 527, 6233, 293, 718, 311, 586, 352, 2286, 293, 853, 293, 1884, 527], "temperature": 0.0, "avg_logprob": -0.22271986568675323, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.1544594826773391e-06}, {"id": 567, "seek": 262272, "start": 2636.6, "end": 2638.7999999999997, "text": " Python version of this", "tokens": [15329, 3037, 295, 341], "temperature": 0.0, "avg_logprob": -0.22271986568675323, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.1544594826773391e-06}, {"id": 568, "seek": 262272, "start": 2639.9199999999996, "end": 2641.9199999999996, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.22271986568675323, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.1544594826773391e-06}, {"id": 569, "seek": 262272, "start": 2643.16, "end": 2646.8399999999997, "text": " Earlier on when we created our simplest possible", "tokens": [24552, 322, 562, 321, 2942, 527, 22811, 1944], "temperature": 0.0, "avg_logprob": -0.22271986568675323, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.1544594826773391e-06}, {"id": 570, "seek": 264684, "start": 2646.84, "end": 2652.28, "text": " PyTorch module there was no like", "tokens": [9953, 51, 284, 339, 10088, 456, 390, 572, 411], "temperature": 0.0, "avg_logprob": -0.21691713835063733, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.6028066056605894e-06}, {"id": 571, "seek": 264684, "start": 2653.04, "end": 2655.04, "text": " State we didn't need a constructor", "tokens": [4533, 321, 994, 380, 643, 257, 47479], "temperature": 0.0, "avg_logprob": -0.21691713835063733, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.6028066056605894e-06}, {"id": 572, "seek": 264684, "start": 2655.7200000000003, "end": 2661.2400000000002, "text": " Because we weren't like saying how many users are there or how many movies are there or how many factors do we want or whatever?", "tokens": [1436, 321, 4999, 380, 411, 1566, 577, 867, 5022, 366, 456, 420, 577, 867, 6233, 366, 456, 420, 577, 867, 6771, 360, 321, 528, 420, 2035, 30], "temperature": 0.0, "avg_logprob": -0.21691713835063733, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.6028066056605894e-06}, {"id": 573, "seek": 264684, "start": 2661.2400000000002, "end": 2664.44, "text": " right anytime we want to do something like", "tokens": [558, 13038, 321, 528, 281, 360, 746, 411], "temperature": 0.0, "avg_logprob": -0.21691713835063733, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.6028066056605894e-06}, {"id": 574, "seek": 264684, "start": 2666.2400000000002, "end": 2669.84, "text": " This where we're passing in and saying we want to construct our", "tokens": [639, 689, 321, 434, 8437, 294, 293, 1566, 321, 528, 281, 7690, 527], "temperature": 0.0, "avg_logprob": -0.21691713835063733, "compression_ratio": 1.7413793103448276, "no_speech_prob": 1.6028066056605894e-06}, {"id": 575, "seek": 266984, "start": 2669.84, "end": 2676.76, "text": " Module with this number of users and this number of movies then we need a constructor", "tokens": [48251, 365, 341, 1230, 295, 5022, 293, 341, 1230, 295, 6233, 550, 321, 643, 257, 47479], "temperature": 0.0, "avg_logprob": -0.2869199406016957, "compression_ratio": 1.7511520737327189, "no_speech_prob": 2.8573003874043934e-06}, {"id": 576, "seek": 266984, "start": 2677.36, "end": 2682.6400000000003, "text": " For our class and you create a constructor in Python by defining a", "tokens": [1171, 527, 1508, 293, 291, 1884, 257, 47479, 294, 15329, 538, 17827, 257], "temperature": 0.0, "avg_logprob": -0.2869199406016957, "compression_ratio": 1.7511520737327189, "no_speech_prob": 2.8573003874043934e-06}, {"id": 577, "seek": 266984, "start": 2683.28, "end": 2689.1200000000003, "text": " Dunder in it underscore underscore in it underscore underscore it special name, so this just creates a", "tokens": [413, 6617, 294, 309, 37556, 37556, 294, 309, 37556, 37556, 309, 2121, 1315, 11, 370, 341, 445, 7829, 257], "temperature": 0.0, "avg_logprob": -0.2869199406016957, "compression_ratio": 1.7511520737327189, "no_speech_prob": 2.8573003874043934e-06}, {"id": 578, "seek": 266984, "start": 2689.92, "end": 2691.92, "text": " Constructor again if you haven't done no over for", "tokens": [8574, 14535, 797, 498, 291, 2378, 380, 1096, 572, 670, 337], "temperature": 0.0, "avg_logprob": -0.2869199406016957, "compression_ratio": 1.7511520737327189, "no_speech_prob": 2.8573003874043934e-06}, {"id": 579, "seek": 266984, "start": 2692.88, "end": 2697.1600000000003, "text": " You wanted to do some study during the week, but it's a pretty simple idea", "tokens": [509, 1415, 281, 360, 512, 2979, 1830, 264, 1243, 11, 457, 309, 311, 257, 1238, 2199, 1558], "temperature": 0.0, "avg_logprob": -0.2869199406016957, "compression_ratio": 1.7511520737327189, "no_speech_prob": 2.8573003874043934e-06}, {"id": 580, "seek": 269716, "start": 2697.16, "end": 2704.7999999999997, "text": " This is just the thing that when we create this object. This is what gets around okay again special Python thing when you", "tokens": [639, 307, 445, 264, 551, 300, 562, 321, 1884, 341, 2657, 13, 639, 307, 437, 2170, 926, 1392, 797, 2121, 15329, 551, 562, 291], "temperature": 0.0, "avg_logprob": -0.17933951602892928, "compression_ratio": 1.7782608695652173, "no_speech_prob": 3.1875424610916525e-06}, {"id": 581, "seek": 269716, "start": 2705.3199999999997, "end": 2708.8399999999997, "text": " Create your own constructor you have to call the parent class constructor", "tokens": [20248, 428, 1065, 47479, 291, 362, 281, 818, 264, 2596, 1508, 47479], "temperature": 0.0, "avg_logprob": -0.17933951602892928, "compression_ratio": 1.7782608695652173, "no_speech_prob": 3.1875424610916525e-06}, {"id": 582, "seek": 269716, "start": 2708.8399999999997, "end": 2715.1, "text": " And if you want to have all of the cool behavior of a pipe watch module you get that by inheriting", "tokens": [400, 498, 291, 528, 281, 362, 439, 295, 264, 1627, 5223, 295, 257, 11240, 1159, 10088, 291, 483, 300, 538, 9484, 1748], "temperature": 0.0, "avg_logprob": -0.17933951602892928, "compression_ratio": 1.7782608695652173, "no_speech_prob": 3.1875424610916525e-06}, {"id": 583, "seek": 269716, "start": 2715.6, "end": 2723.6, "text": " From an end module neural net module okay, so basically by inheriting here and calling the super class constructor", "tokens": [3358, 364, 917, 10088, 18161, 2533, 10088, 1392, 11, 370, 1936, 538, 9484, 1748, 510, 293, 5141, 264, 1687, 1508, 47479], "temperature": 0.0, "avg_logprob": -0.17933951602892928, "compression_ratio": 1.7782608695652173, "no_speech_prob": 3.1875424610916525e-06}, {"id": 584, "seek": 272360, "start": 2723.6, "end": 2730.0, "text": " We now have a fully functioning pie torch layer, okay, so now we have to give it some behavior and", "tokens": [492, 586, 362, 257, 4498, 18483, 1730, 27822, 4583, 11, 1392, 11, 370, 586, 321, 362, 281, 976, 309, 512, 5223, 293], "temperature": 0.0, "avg_logprob": -0.2272859267246576, "compression_ratio": 1.6923076923076923, "no_speech_prob": 6.577920999006892e-07}, {"id": 585, "seek": 272360, "start": 2730.68, "end": 2737.48, "text": " So we give it some behavior by storing some things in it right so here. We're going to create something called", "tokens": [407, 321, 976, 309, 512, 5223, 538, 26085, 512, 721, 294, 309, 558, 370, 510, 13, 492, 434, 516, 281, 1884, 746, 1219], "temperature": 0.0, "avg_logprob": -0.2272859267246576, "compression_ratio": 1.6923076923076923, "no_speech_prob": 6.577920999006892e-07}, {"id": 586, "seek": 272360, "start": 2738.64, "end": 2743.44, "text": " Self dot you users and that is going to be an embedding layer", "tokens": [16348, 5893, 291, 5022, 293, 300, 307, 516, 281, 312, 364, 12240, 3584, 4583], "temperature": 0.0, "avg_logprob": -0.2272859267246576, "compression_ratio": 1.6923076923076923, "no_speech_prob": 6.577920999006892e-07}, {"id": 587, "seek": 272360, "start": 2744.16, "end": 2748.42, "text": " Number of rows is and users number of columns is n factors", "tokens": [5118, 295, 13241, 307, 293, 5022, 1230, 295, 13766, 307, 297, 6771], "temperature": 0.0, "avg_logprob": -0.2272859267246576, "compression_ratio": 1.6923076923076923, "no_speech_prob": 6.577920999006892e-07}, {"id": 588, "seek": 274842, "start": 2748.42, "end": 2756.92, "text": " So that is exactly this right the number of rows is n users number of columns is n factors", "tokens": [407, 300, 307, 2293, 341, 558, 264, 1230, 295, 13241, 307, 297, 5022, 1230, 295, 13766, 307, 297, 6771], "temperature": 0.0, "avg_logprob": -0.2773681879043579, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.034850697578804e-06}, {"id": 589, "seek": 274842, "start": 2757.42, "end": 2760.12, "text": " And then we'll have to do the same thing for movies", "tokens": [400, 550, 321, 603, 362, 281, 360, 264, 912, 551, 337, 6233], "temperature": 0.0, "avg_logprob": -0.2773681879043579, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.034850697578804e-06}, {"id": 590, "seek": 274842, "start": 2760.9, "end": 2764.78, "text": " All right, so that's going to go ahead and create these two", "tokens": [1057, 558, 11, 370, 300, 311, 516, 281, 352, 2286, 293, 1884, 613, 732], "temperature": 0.0, "avg_logprob": -0.2773681879043579, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.034850697578804e-06}, {"id": 591, "seek": 274842, "start": 2766.34, "end": 2768.34, "text": " randomly initialized arrays", "tokens": [16979, 5883, 1602, 41011], "temperature": 0.0, "avg_logprob": -0.2773681879043579, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.034850697578804e-06}, {"id": 592, "seek": 274842, "start": 2770.42, "end": 2776.1, "text": " However when you randomly initialize over an array it's important to randomly initialize it to a", "tokens": [2908, 562, 291, 16979, 5883, 1125, 670, 364, 10225, 309, 311, 1021, 281, 16979, 5883, 1125, 309, 281, 257], "temperature": 0.0, "avg_logprob": -0.2773681879043579, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.034850697578804e-06}, {"id": 593, "seek": 277610, "start": 2776.1, "end": 2779.5, "text": " Reasonable set of numbers like a reasonable scale", "tokens": [1300, 36169, 992, 295, 3547, 411, 257, 10585, 4373], "temperature": 0.0, "avg_logprob": -0.16725856741679082, "compression_ratio": 1.71900826446281, "no_speech_prob": 1.4367464018505416e-06}, {"id": 594, "seek": 277610, "start": 2780.1, "end": 2783.06, "text": " If we randomly initialize them from like naught to a million", "tokens": [759, 321, 16979, 5883, 1125, 552, 490, 411, 13138, 281, 257, 2459], "temperature": 0.0, "avg_logprob": -0.16725856741679082, "compression_ratio": 1.71900826446281, "no_speech_prob": 1.4367464018505416e-06}, {"id": 595, "seek": 277610, "start": 2783.66, "end": 2787.74, "text": " Then we would start out and you know these things would start out being like", "tokens": [1396, 321, 576, 722, 484, 293, 291, 458, 613, 721, 576, 722, 484, 885, 411], "temperature": 0.0, "avg_logprob": -0.16725856741679082, "compression_ratio": 1.71900826446281, "no_speech_prob": 1.4367464018505416e-06}, {"id": 596, "seek": 277610, "start": 2788.2599999999998, "end": 2793.62, "text": " You know billions and billions of size rating and that's going to be very hard to do gradient descent on", "tokens": [509, 458, 17375, 293, 17375, 295, 2744, 10990, 293, 300, 311, 516, 281, 312, 588, 1152, 281, 360, 16235, 23475, 322], "temperature": 0.0, "avg_logprob": -0.16725856741679082, "compression_ratio": 1.71900826446281, "no_speech_prob": 1.4367464018505416e-06}, {"id": 597, "seek": 277610, "start": 2794.58, "end": 2798.66, "text": " So I just kind of manually figured here like okay about what size?", "tokens": [407, 286, 445, 733, 295, 16945, 8932, 510, 411, 1392, 466, 437, 2744, 30], "temperature": 0.0, "avg_logprob": -0.16725856741679082, "compression_ratio": 1.71900826446281, "no_speech_prob": 1.4367464018505416e-06}, {"id": 598, "seek": 277610, "start": 2799.86, "end": 2803.2599999999998, "text": " Numbers and are going to give me about the right ratings", "tokens": [22592, 1616, 293, 366, 516, 281, 976, 385, 466, 264, 558, 24603], "temperature": 0.0, "avg_logprob": -0.16725856741679082, "compression_ratio": 1.71900826446281, "no_speech_prob": 1.4367464018505416e-06}, {"id": 599, "seek": 280326, "start": 2803.26, "end": 2805.98, "text": " And so we know we know we need ratings between about 0 and 5", "tokens": [400, 370, 321, 458, 321, 458, 321, 643, 24603, 1296, 466, 1958, 293, 1025], "temperature": 0.0, "avg_logprob": -0.24526381218570403, "compression_ratio": 1.7433628318584071, "no_speech_prob": 2.156809159714612e-06}, {"id": 600, "seek": 280326, "start": 2806.7400000000002, "end": 2814.0600000000004, "text": " So if we start out with stuff between about 0 and 0.05 then we're going to get ratings of about the right level", "tokens": [407, 498, 321, 722, 484, 365, 1507, 1296, 466, 1958, 293, 1958, 13, 13328, 550, 321, 434, 516, 281, 483, 24603, 295, 466, 264, 558, 1496], "temperature": 0.0, "avg_logprob": -0.24526381218570403, "compression_ratio": 1.7433628318584071, "no_speech_prob": 2.156809159714612e-06}, {"id": 601, "seek": 280326, "start": 2815.3, "end": 2822.5, "text": " You can easily enough like that calculate that in neural nets there are standard algorithms for", "tokens": [509, 393, 3612, 1547, 411, 300, 8873, 300, 294, 18161, 36170, 456, 366, 3832, 14642, 337], "temperature": 0.0, "avg_logprob": -0.24526381218570403, "compression_ratio": 1.7433628318584071, "no_speech_prob": 2.156809159714612e-06}, {"id": 602, "seek": 280326, "start": 2823.3, "end": 2827.9, "text": " basically doing doing that calculation and the basic the key algorithm is", "tokens": [1936, 884, 884, 300, 17108, 293, 264, 3875, 264, 2141, 9284, 307], "temperature": 0.0, "avg_logprob": -0.24526381218570403, "compression_ratio": 1.7433628318584071, "no_speech_prob": 2.156809159714612e-06}, {"id": 603, "seek": 282790, "start": 2827.9, "end": 2835.42, "text": " Something called her initialization from climbing her and the basic idea", "tokens": [6595, 1219, 720, 5883, 2144, 490, 14780, 720, 293, 264, 3875, 1558], "temperature": 0.0, "avg_logprob": -0.3344063949584961, "compression_ratio": 1.4968152866242037, "no_speech_prob": 3.340509692861815e-06}, {"id": 604, "seek": 282790, "start": 2837.6600000000003, "end": 2840.58, "text": " Is that you take the", "tokens": [1119, 300, 291, 747, 264], "temperature": 0.0, "avg_logprob": -0.3344063949584961, "compression_ratio": 1.4968152866242037, "no_speech_prob": 3.340509692861815e-06}, {"id": 605, "seek": 282790, "start": 2843.34, "end": 2847.86, "text": " Here you basically set the weights equal to a normal distribution", "tokens": [1692, 291, 1936, 992, 264, 17443, 2681, 281, 257, 2710, 7316], "temperature": 0.0, "avg_logprob": -0.3344063949584961, "compression_ratio": 1.4968152866242037, "no_speech_prob": 3.340509692861815e-06}, {"id": 606, "seek": 284786, "start": 2847.86, "end": 2860.6600000000003, "text": " With a standard deviation which is basically inversely proportional to the number of things in the previous layer", "tokens": [2022, 257, 3832, 25163, 597, 307, 1936, 21378, 736, 24969, 281, 264, 1230, 295, 721, 294, 264, 3894, 4583], "temperature": 0.0, "avg_logprob": -0.3010761073378266, "compression_ratio": 1.5389221556886228, "no_speech_prob": 5.285504585117451e-07}, {"id": 607, "seek": 284786, "start": 2861.46, "end": 2864.1400000000003, "text": " And so in our previous layer", "tokens": [400, 370, 294, 527, 3894, 4583], "temperature": 0.0, "avg_logprob": -0.3010761073378266, "compression_ratio": 1.5389221556886228, "no_speech_prob": 5.285504585117451e-07}, {"id": 608, "seek": 284786, "start": 2868.1400000000003, "end": 2870.1400000000003, "text": " So in this case we basically", "tokens": [407, 294, 341, 1389, 321, 1936], "temperature": 0.0, "avg_logprob": -0.3010761073378266, "compression_ratio": 1.5389221556886228, "no_speech_prob": 5.285504585117451e-07}, {"id": 609, "seek": 284786, "start": 2870.26, "end": 2875.7400000000002, "text": " If you basically take that naught to 0.05 and multiply it by the fact that you've got", "tokens": [759, 291, 1936, 747, 300, 13138, 281, 1958, 13, 13328, 293, 12972, 309, 538, 264, 1186, 300, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.3010761073378266, "compression_ratio": 1.5389221556886228, "no_speech_prob": 5.285504585117451e-07}, {"id": 610, "seek": 287574, "start": 2875.74, "end": 2879.8599999999997, "text": " 40 things I wasn't 40 or 50 things coming out of it", "tokens": [3356, 721, 286, 2067, 380, 3356, 420, 2625, 721, 1348, 484, 295, 309], "temperature": 0.0, "avg_logprob": -0.2319867832144511, "compression_ratio": 1.7025862068965518, "no_speech_prob": 3.726622026078985e-06}, {"id": 611, "seek": 287574, "start": 2881.3799999999997, "end": 2885.4599999999996, "text": " 50 50 things coming out of it, then you're going to get something of about the right size", "tokens": [2625, 2625, 721, 1348, 484, 295, 309, 11, 550, 291, 434, 516, 281, 483, 746, 295, 466, 264, 558, 2744], "temperature": 0.0, "avg_logprob": -0.2319867832144511, "compression_ratio": 1.7025862068965518, "no_speech_prob": 3.726622026078985e-06}, {"id": 612, "seek": 287574, "start": 2886.8199999999997, "end": 2890.7, "text": " My torch has already has like her initialization", "tokens": [1222, 27822, 575, 1217, 575, 411, 720, 5883, 2144], "temperature": 0.0, "avg_logprob": -0.2319867832144511, "compression_ratio": 1.7025862068965518, "no_speech_prob": 3.726622026078985e-06}, {"id": 613, "seek": 287574, "start": 2891.54, "end": 2897.58, "text": " Class there like we don't in normally in real life have to think about this we can just call the existing initialization", "tokens": [9471, 456, 411, 321, 500, 380, 294, 5646, 294, 957, 993, 362, 281, 519, 466, 341, 321, 393, 445, 818, 264, 6741, 5883, 2144], "temperature": 0.0, "avg_logprob": -0.2319867832144511, "compression_ratio": 1.7025862068965518, "no_speech_prob": 3.726622026078985e-06}, {"id": 614, "seek": 287574, "start": 2898.18, "end": 2903.7799999999997, "text": " Functions, but we're trying to do this all like from scratch here. Okay without any", "tokens": [11166, 3916, 11, 457, 321, 434, 1382, 281, 360, 341, 439, 411, 490, 8459, 510, 13, 1033, 1553, 604], "temperature": 0.0, "avg_logprob": -0.2319867832144511, "compression_ratio": 1.7025862068965518, "no_speech_prob": 3.726622026078985e-06}, {"id": 615, "seek": 290378, "start": 2903.78, "end": 2905.78, "text": " special stuff going on", "tokens": [2121, 1507, 516, 322], "temperature": 0.0, "avg_logprob": -0.20100693021501814, "compression_ratio": 1.5276073619631902, "no_speech_prob": 2.4060911982815014e-06}, {"id": 616, "seek": 290378, "start": 2907.2200000000003, "end": 2912.9, "text": " So there's quite a bit of pytorch notation here, so self dot you we've already set to", "tokens": [407, 456, 311, 1596, 257, 857, 295, 25878, 284, 339, 24657, 510, 11, 370, 2698, 5893, 291, 321, 600, 1217, 992, 281], "temperature": 0.0, "avg_logprob": -0.20100693021501814, "compression_ratio": 1.5276073619631902, "no_speech_prob": 2.4060911982815014e-06}, {"id": 617, "seek": 290378, "start": 2913.78, "end": 2915.78, "text": " an instance of the embedding class", "tokens": [364, 5197, 295, 264, 12240, 3584, 1508], "temperature": 0.0, "avg_logprob": -0.20100693021501814, "compression_ratio": 1.5276073619631902, "no_speech_prob": 2.4060911982815014e-06}, {"id": 618, "seek": 290378, "start": 2917.6200000000003, "end": 2919.6200000000003, "text": " It has a dot weight", "tokens": [467, 575, 257, 5893, 3364], "temperature": 0.0, "avg_logprob": -0.20100693021501814, "compression_ratio": 1.5276073619631902, "no_speech_prob": 2.4060911982815014e-06}, {"id": 619, "seek": 290378, "start": 2919.98, "end": 2923.94, "text": " Attribute which contains the actual the actual embedding matrix", "tokens": [7298, 2024, 1169, 597, 8306, 264, 3539, 264, 3539, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.20100693021501814, "compression_ratio": 1.5276073619631902, "no_speech_prob": 2.4060911982815014e-06}, {"id": 620, "seek": 290378, "start": 2924.7400000000002, "end": 2926.7400000000002, "text": " So that contains", "tokens": [407, 300, 8306], "temperature": 0.0, "avg_logprob": -0.20100693021501814, "compression_ratio": 1.5276073619631902, "no_speech_prob": 2.4060911982815014e-06}, {"id": 621, "seek": 290378, "start": 2926.7400000000002, "end": 2928.7400000000002, "text": " this", "tokens": [341], "temperature": 0.0, "avg_logprob": -0.20100693021501814, "compression_ratio": 1.5276073619631902, "no_speech_prob": 2.4060911982815014e-06}, {"id": 622, "seek": 292874, "start": 2928.74, "end": 2934.7799999999997, "text": " The actual embedding matrix is not a tensor it's a variable a", "tokens": [440, 3539, 12240, 3584, 8141, 307, 406, 257, 40863, 309, 311, 257, 7006, 257], "temperature": 0.0, "avg_logprob": -0.1845712325152229, "compression_ratio": 1.6896551724137931, "no_speech_prob": 1.9637989225884667e-06}, {"id": 623, "seek": 292874, "start": 2935.5, "end": 2942.4599999999996, "text": " Variable is exactly the same as a tensor in other words it supports the exact same operations as a tensor", "tokens": [32511, 712, 307, 2293, 264, 912, 382, 257, 40863, 294, 661, 2283, 309, 9346, 264, 1900, 912, 7705, 382, 257, 40863], "temperature": 0.0, "avg_logprob": -0.1845712325152229, "compression_ratio": 1.6896551724137931, "no_speech_prob": 1.9637989225884667e-06}, {"id": 624, "seek": 292874, "start": 2942.8999999999996, "end": 2944.54, "text": " But it also", "tokens": [583, 309, 611], "temperature": 0.0, "avg_logprob": -0.1845712325152229, "compression_ratio": 1.6896551724137931, "no_speech_prob": 1.9637989225884667e-06}, {"id": 625, "seek": 292874, "start": 2944.54, "end": 2946.54, "text": " Does automatic differentiation?", "tokens": [4402, 12509, 38902, 30], "temperature": 0.0, "avg_logprob": -0.1845712325152229, "compression_ratio": 1.6896551724137931, "no_speech_prob": 1.9637989225884667e-06}, {"id": 626, "seek": 292874, "start": 2946.8599999999997, "end": 2949.22, "text": " That's all a variable is basically", "tokens": [663, 311, 439, 257, 7006, 307, 1936], "temperature": 0.0, "avg_logprob": -0.1845712325152229, "compression_ratio": 1.6896551724137931, "no_speech_prob": 1.9637989225884667e-06}, {"id": 627, "seek": 294922, "start": 2949.22, "end": 2957.8599999999997, "text": " To pull the tensor out of a variable you get its data attribute, okay, so this is so this is now", "tokens": [1407, 2235, 264, 40863, 484, 295, 257, 7006, 291, 483, 1080, 1412, 19667, 11, 1392, 11, 370, 341, 307, 370, 341, 307, 586], "temperature": 0.0, "avg_logprob": -0.1656673925894278, "compression_ratio": 1.6294416243654823, "no_speech_prob": 4.5209338850327185e-07}, {"id": 628, "seek": 294922, "start": 2958.3799999999997, "end": 2963.18, "text": " the tensor of the weight matrix of the self dot you embedding and", "tokens": [264, 40863, 295, 264, 3364, 8141, 295, 264, 2698, 5893, 291, 12240, 3584, 293], "temperature": 0.0, "avg_logprob": -0.1656673925894278, "compression_ratio": 1.6294416243654823, "no_speech_prob": 4.5209338850327185e-07}, {"id": 629, "seek": 294922, "start": 2964.3799999999997, "end": 2970.22, "text": " Then something that's really handy to know is that all of the tensor functions in pytorch", "tokens": [1396, 746, 300, 311, 534, 13239, 281, 458, 307, 300, 439, 295, 264, 40863, 6828, 294, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.1656673925894278, "compression_ratio": 1.6294416243654823, "no_speech_prob": 4.5209338850327185e-07}, {"id": 630, "seek": 294922, "start": 2970.3799999999997, "end": 2974.5, "text": " You can stick an underscore at the end and that means do it in place", "tokens": [509, 393, 2897, 364, 37556, 412, 264, 917, 293, 300, 1355, 360, 309, 294, 1081], "temperature": 0.0, "avg_logprob": -0.1656673925894278, "compression_ratio": 1.6294416243654823, "no_speech_prob": 4.5209338850327185e-07}, {"id": 631, "seek": 297450, "start": 2974.5, "end": 2980.9, "text": " Right so this is say create a random uniform random number of an appropriate size", "tokens": [1779, 370, 341, 307, 584, 1884, 257, 4974, 9452, 4974, 1230, 295, 364, 6854, 2744], "temperature": 0.0, "avg_logprob": -0.15995994512585626, "compression_ratio": 1.471502590673575, "no_speech_prob": 2.726462071223068e-06}, {"id": 632, "seek": 297450, "start": 2981.34, "end": 2986.9, "text": " For this tensor and don't return it, but actually fill in that matrix", "tokens": [1171, 341, 40863, 293, 500, 380, 2736, 309, 11, 457, 767, 2836, 294, 300, 8141], "temperature": 0.0, "avg_logprob": -0.15995994512585626, "compression_ratio": 1.471502590673575, "no_speech_prob": 2.726462071223068e-06}, {"id": 633, "seek": 297450, "start": 2987.66, "end": 2994.02, "text": " In place okay, so that's a super handy thing to know about I mean it wouldn't be rocket science", "tokens": [682, 1081, 1392, 11, 370, 300, 311, 257, 1687, 13239, 551, 281, 458, 466, 286, 914, 309, 2759, 380, 312, 13012, 3497], "temperature": 0.0, "avg_logprob": -0.15995994512585626, "compression_ratio": 1.471502590673575, "no_speech_prob": 2.726462071223068e-06}, {"id": 634, "seek": 297450, "start": 2994.02, "end": 2996.02, "text": " Otherwise we would have to have gone", "tokens": [10328, 321, 576, 362, 281, 362, 2780], "temperature": 0.0, "avg_logprob": -0.15995994512585626, "compression_ratio": 1.471502590673575, "no_speech_prob": 2.726462071223068e-06}, {"id": 635, "seek": 299602, "start": 2996.02, "end": 2998.02, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.207899842943464, "compression_ratio": 1.3918918918918919, "no_speech_prob": 1.5779562545503723e-06}, {"id": 636, "seek": 299602, "start": 3003.66, "end": 3009.9, "text": " There's the non in place version, that's what saves us some typing saves us some screen noise, that's all", "tokens": [821, 311, 264, 2107, 294, 1081, 3037, 11, 300, 311, 437, 19155, 505, 512, 18444, 19155, 505, 512, 2568, 5658, 11, 300, 311, 439], "temperature": 0.0, "avg_logprob": -0.207899842943464, "compression_ratio": 1.3918918918918919, "no_speech_prob": 1.5779562545503723e-06}, {"id": 637, "seek": 299602, "start": 3013.46, "end": 3014.98, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.207899842943464, "compression_ratio": 1.3918918918918919, "no_speech_prob": 1.5779562545503723e-06}, {"id": 638, "seek": 299602, "start": 3014.98, "end": 3019.9, "text": " So now we've got our randomly initialized embedding weight matrices", "tokens": [407, 586, 321, 600, 658, 527, 16979, 5883, 1602, 12240, 3584, 3364, 32284], "temperature": 0.0, "avg_logprob": -0.207899842943464, "compression_ratio": 1.3918918918918919, "no_speech_prob": 1.5779562545503723e-06}, {"id": 639, "seek": 299602, "start": 3020.5, "end": 3022.5, "text": " And so now the forward", "tokens": [400, 370, 586, 264, 2128], "temperature": 0.0, "avg_logprob": -0.207899842943464, "compression_ratio": 1.3918918918918919, "no_speech_prob": 1.5779562545503723e-06}, {"id": 640, "seek": 302250, "start": 3022.5, "end": 3027.74, "text": " I'm actually going to use the same columnar model data that we used for", "tokens": [286, 478, 767, 516, 281, 764, 264, 912, 7738, 289, 2316, 1412, 300, 321, 1143, 337], "temperature": 0.0, "avg_logprob": -0.1716040809555809, "compression_ratio": 1.8976744186046512, "no_speech_prob": 1.93335199583089e-06}, {"id": 641, "seek": 302250, "start": 3028.46, "end": 3034.14, "text": " Rossman and so it's actually going to be passed both categorical variables and continuous variables", "tokens": [16140, 1601, 293, 370, 309, 311, 767, 516, 281, 312, 4678, 1293, 19250, 804, 9102, 293, 10957, 9102], "temperature": 0.0, "avg_logprob": -0.1716040809555809, "compression_ratio": 1.8976744186046512, "no_speech_prob": 1.93335199583089e-06}, {"id": 642, "seek": 302250, "start": 3034.66, "end": 3036.66, "text": " And in this case there are no", "tokens": [400, 294, 341, 1389, 456, 366, 572], "temperature": 0.0, "avg_logprob": -0.1716040809555809, "compression_ratio": 1.8976744186046512, "no_speech_prob": 1.93335199583089e-06}, {"id": 643, "seek": 302250, "start": 3037.54, "end": 3040.22, "text": " Continuous variables, so I'm just going to grab the", "tokens": [14674, 12549, 9102, 11, 370, 286, 478, 445, 516, 281, 4444, 264], "temperature": 0.0, "avg_logprob": -0.1716040809555809, "compression_ratio": 1.8976744186046512, "no_speech_prob": 1.93335199583089e-06}, {"id": 644, "seek": 302250, "start": 3040.82, "end": 3047.02, "text": " Zero column out of the categorical variables and call it users and the first column and call it movies", "tokens": [17182, 7738, 484, 295, 264, 19250, 804, 9102, 293, 818, 309, 5022, 293, 264, 700, 7738, 293, 818, 309, 6233], "temperature": 0.0, "avg_logprob": -0.1716040809555809, "compression_ratio": 1.8976744186046512, "no_speech_prob": 1.93335199583089e-06}, {"id": 645, "seek": 302250, "start": 3047.14, "end": 3050.18, "text": " Okay, so I'm just kind of too lazy to create my own", "tokens": [1033, 11, 370, 286, 478, 445, 733, 295, 886, 14847, 281, 1884, 452, 1065], "temperature": 0.0, "avg_logprob": -0.1716040809555809, "compression_ratio": 1.8976744186046512, "no_speech_prob": 1.93335199583089e-06}, {"id": 646, "seek": 305018, "start": 3050.18, "end": 3053.3399999999997, "text": " I'm not so much too lazy that we do have a special class for this", "tokens": [286, 478, 406, 370, 709, 886, 14847, 300, 321, 360, 362, 257, 2121, 1508, 337, 341], "temperature": 0.0, "avg_logprob": -0.15528830166520743, "compression_ratio": 1.8054474708171206, "no_speech_prob": 1.209862148243701e-06}, {"id": 647, "seek": 305018, "start": 3053.3399999999997, "end": 3058.94, "text": " But I'm trying to avoid creating a special class, so we're just going to leverage this columnar model data class", "tokens": [583, 286, 478, 1382, 281, 5042, 4084, 257, 2121, 1508, 11, 370, 321, 434, 445, 516, 281, 13982, 341, 7738, 289, 2316, 1412, 1508], "temperature": 0.0, "avg_logprob": -0.15528830166520743, "compression_ratio": 1.8054474708171206, "no_speech_prob": 1.209862148243701e-06}, {"id": 648, "seek": 305018, "start": 3059.22, "end": 3063.04, "text": " Okay, so we can basically grab our user and movies", "tokens": [1033, 11, 370, 321, 393, 1936, 4444, 527, 4195, 293, 6233], "temperature": 0.0, "avg_logprob": -0.15528830166520743, "compression_ratio": 1.8054474708171206, "no_speech_prob": 1.209862148243701e-06}, {"id": 649, "seek": 305018, "start": 3063.7, "end": 3068.1, "text": " Many batches right and remember. This is not a single user in a single movie", "tokens": [5126, 15245, 279, 558, 293, 1604, 13, 639, 307, 406, 257, 2167, 4195, 294, 257, 2167, 3169], "temperature": 0.0, "avg_logprob": -0.15528830166520743, "compression_ratio": 1.8054474708171206, "no_speech_prob": 1.209862148243701e-06}, {"id": 650, "seek": 305018, "start": 3068.1, "end": 3071.3399999999997, "text": " This is going to be a whole mini batch of them", "tokens": [639, 307, 516, 281, 312, 257, 1379, 8382, 15245, 295, 552], "temperature": 0.0, "avg_logprob": -0.15528830166520743, "compression_ratio": 1.8054474708171206, "no_speech_prob": 1.209862148243701e-06}, {"id": 651, "seek": 307134, "start": 3071.34, "end": 3079.34, "text": " We can now look up that mini batch of users in our embedding matrix you and the movies in our embedding matrix", "tokens": [492, 393, 586, 574, 493, 300, 8382, 15245, 295, 5022, 294, 527, 12240, 3584, 8141, 291, 293, 264, 6233, 294, 527, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.25351280212402344, "compression_ratio": 1.7533632286995515, "no_speech_prob": 8.186351010408544e-07}, {"id": 652, "seek": 307134, "start": 3079.6600000000003, "end": 3084.84, "text": " M right so this is like exactly the same as just doing an array lookup to grab the", "tokens": [376, 558, 370, 341, 307, 411, 2293, 264, 912, 382, 445, 884, 364, 10225, 574, 1010, 281, 4444, 264], "temperature": 0.0, "avg_logprob": -0.25351280212402344, "compression_ratio": 1.7533632286995515, "no_speech_prob": 8.186351010408544e-07}, {"id": 653, "seek": 307134, "start": 3085.46, "end": 3087.46, "text": " user ID numbered", "tokens": [4195, 7348, 40936], "temperature": 0.0, "avg_logprob": -0.25351280212402344, "compression_ratio": 1.7533632286995515, "no_speech_prob": 8.186351010408544e-07}, {"id": 654, "seek": 307134, "start": 3087.58, "end": 3090.02, "text": " Value, but we're doing it a whole mini batch at a time", "tokens": [39352, 11, 457, 321, 434, 884, 309, 257, 1379, 8382, 15245, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.25351280212402344, "compression_ratio": 1.7533632286995515, "no_speech_prob": 8.186351010408544e-07}, {"id": 655, "seek": 307134, "start": 3090.42, "end": 3096.82, "text": " Right and so it's because pytorch can do a whole mini batch at a time with pretty much everything that we can get really easy", "tokens": [1779, 293, 370, 309, 311, 570, 25878, 284, 339, 393, 360, 257, 1379, 8382, 15245, 412, 257, 565, 365, 1238, 709, 1203, 300, 321, 393, 483, 534, 1858], "temperature": 0.0, "avg_logprob": -0.25351280212402344, "compression_ratio": 1.7533632286995515, "no_speech_prob": 8.186351010408544e-07}, {"id": 656, "seek": 309682, "start": 3096.82, "end": 3102.5, "text": " Speed up we don't have to write any loops on the whole to do everything through our mini batch", "tokens": [18774, 493, 321, 500, 380, 362, 281, 2464, 604, 16121, 322, 264, 1379, 281, 360, 1203, 807, 527, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.14828589023687902, "compression_ratio": 1.9566929133858268, "no_speech_prob": 3.3405249268980697e-06}, {"id": 657, "seek": 309682, "start": 3102.78, "end": 3108.26, "text": " And in fact if you do ever loop through your mini batch manually you don't get GPU acceleration", "tokens": [400, 294, 1186, 498, 291, 360, 1562, 6367, 807, 428, 8382, 15245, 16945, 291, 500, 380, 483, 18407, 17162], "temperature": 0.0, "avg_logprob": -0.14828589023687902, "compression_ratio": 1.9566929133858268, "no_speech_prob": 3.3405249268980697e-06}, {"id": 658, "seek": 309682, "start": 3108.5, "end": 3111.34, "text": " That's really important to know right so you never want to loop", "tokens": [663, 311, 534, 1021, 281, 458, 558, 370, 291, 1128, 528, 281, 6367], "temperature": 0.0, "avg_logprob": -0.14828589023687902, "compression_ratio": 1.9566929133858268, "no_speech_prob": 3.3405249268980697e-06}, {"id": 659, "seek": 309682, "start": 3111.7400000000002, "end": 3118.1800000000003, "text": " Have a for loop going through your mini batch you always want to do things in this kind of like whole mini batch at a time", "tokens": [3560, 257, 337, 6367, 516, 807, 428, 8382, 15245, 291, 1009, 528, 281, 360, 721, 294, 341, 733, 295, 411, 1379, 8382, 15245, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.14828589023687902, "compression_ratio": 1.9566929133858268, "no_speech_prob": 3.3405249268980697e-06}, {"id": 660, "seek": 309682, "start": 3118.6600000000003, "end": 3124.1000000000004, "text": " But pretty much everything in pytorch does things a whole mini batch at a time, so you shouldn't have to worry about it", "tokens": [583, 1238, 709, 1203, 294, 25878, 284, 339, 775, 721, 257, 1379, 8382, 15245, 412, 257, 565, 11, 370, 291, 4659, 380, 362, 281, 3292, 466, 309], "temperature": 0.0, "avg_logprob": -0.14828589023687902, "compression_ratio": 1.9566929133858268, "no_speech_prob": 3.3405249268980697e-06}, {"id": 661, "seek": 312410, "start": 3124.1, "end": 3130.66, "text": " And then here's our dot product just like before all right so having defined", "tokens": [400, 550, 510, 311, 527, 5893, 1674, 445, 411, 949, 439, 558, 370, 1419, 7642], "temperature": 0.0, "avg_logprob": -0.24007851038223657, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.2878917914349586e-06}, {"id": 662, "seek": 312410, "start": 3133.18, "end": 3135.5, "text": " That I'm now going to", "tokens": [663, 286, 478, 586, 516, 281], "temperature": 0.0, "avg_logprob": -0.24007851038223657, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.2878917914349586e-06}, {"id": 663, "seek": 312410, "start": 3137.02, "end": 3139.58, "text": " Go ahead and say all right my x values is", "tokens": [1037, 2286, 293, 584, 439, 558, 452, 2031, 4190, 307], "temperature": 0.0, "avg_logprob": -0.24007851038223657, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.2878917914349586e-06}, {"id": 664, "seek": 312410, "start": 3140.86, "end": 3147.98, "text": " Everything except the rating and the timestamp in my ratings table my y is my rating and then I can just say okay, let's", "tokens": [5471, 3993, 264, 10990, 293, 264, 49108, 1215, 294, 452, 24603, 3199, 452, 288, 307, 452, 10990, 293, 550, 286, 393, 445, 584, 1392, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.24007851038223657, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.2878917914349586e-06}, {"id": 665, "seek": 312410, "start": 3148.86, "end": 3151.38, "text": " Grab a model data from a data frame", "tokens": [20357, 257, 2316, 1412, 490, 257, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.24007851038223657, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.2878917914349586e-06}, {"id": 666, "seek": 315138, "start": 3151.38, "end": 3155.42, "text": " I'm using that X and that Y and here is our list of", "tokens": [286, 478, 1228, 300, 1783, 293, 300, 398, 293, 510, 307, 527, 1329, 295], "temperature": 0.0, "avg_logprob": -0.2061587466469294, "compression_ratio": 1.5195530726256983, "no_speech_prob": 1.067701305146329e-06}, {"id": 667, "seek": 315138, "start": 3156.5, "end": 3158.2200000000003, "text": " categorical variables", "tokens": [19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.2061587466469294, "compression_ratio": 1.5195530726256983, "no_speech_prob": 1.067701305146329e-06}, {"id": 668, "seek": 315138, "start": 3158.2200000000003, "end": 3160.2200000000003, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2061587466469294, "compression_ratio": 1.5195530726256983, "no_speech_prob": 1.067701305146329e-06}, {"id": 669, "seek": 315138, "start": 3161.62, "end": 3163.62, "text": " And then so let's now instantiate", "tokens": [400, 550, 370, 718, 311, 586, 9836, 13024], "temperature": 0.0, "avg_logprob": -0.2061587466469294, "compression_ratio": 1.5195530726256983, "no_speech_prob": 1.067701305146329e-06}, {"id": 670, "seek": 315138, "start": 3164.62, "end": 3169.26, "text": " That pytorch object alright, so we've now created that from scratch", "tokens": [663, 25878, 284, 339, 2657, 5845, 11, 370, 321, 600, 586, 2942, 300, 490, 8459], "temperature": 0.0, "avg_logprob": -0.2061587466469294, "compression_ratio": 1.5195530726256983, "no_speech_prob": 1.067701305146329e-06}, {"id": 671, "seek": 315138, "start": 3170.06, "end": 3176.46, "text": " And then the next thing we need to do is to create an optimizer, so this is part of pytorch", "tokens": [400, 550, 264, 958, 551, 321, 643, 281, 360, 307, 281, 1884, 364, 5028, 6545, 11, 370, 341, 307, 644, 295, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.2061587466469294, "compression_ratio": 1.5195530726256983, "no_speech_prob": 1.067701305146329e-06}, {"id": 672, "seek": 317646, "start": 3176.46, "end": 3183.7, "text": " The only fast AI thing here is this line right because I don't think showing you", "tokens": [440, 787, 2370, 7318, 551, 510, 307, 341, 1622, 558, 570, 286, 500, 380, 519, 4099, 291], "temperature": 0.0, "avg_logprob": -0.18669165778405888, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.5215601908712415e-06}, {"id": 673, "seek": 317646, "start": 3184.34, "end": 3190.1, "text": " How to build data sets and data loaders is interesting enough really we might do that in part two of the course", "tokens": [1012, 281, 1322, 1412, 6352, 293, 1412, 3677, 433, 307, 1880, 1547, 534, 321, 1062, 360, 300, 294, 644, 732, 295, 264, 1164], "temperature": 0.0, "avg_logprob": -0.18669165778405888, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.5215601908712415e-06}, {"id": 674, "seek": 317646, "start": 3190.7400000000002, "end": 3194.82, "text": " And it's actually so straightforward like a lot of you are already doing it on the forums", "tokens": [400, 309, 311, 767, 370, 15325, 411, 257, 688, 295, 291, 366, 1217, 884, 309, 322, 264, 26998], "temperature": 0.0, "avg_logprob": -0.18669165778405888, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.5215601908712415e-06}, {"id": 675, "seek": 317646, "start": 3195.78, "end": 3197.9, "text": " So I'm not going to show you that in this part", "tokens": [407, 286, 478, 406, 516, 281, 855, 291, 300, 294, 341, 644], "temperature": 0.0, "avg_logprob": -0.18669165778405888, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.5215601908712415e-06}, {"id": 676, "seek": 317646, "start": 3197.9, "end": 3201.98, "text": " I mean if you're interested feel free to talk on the forums about it", "tokens": [286, 914, 498, 291, 434, 3102, 841, 1737, 281, 751, 322, 264, 26998, 466, 309], "temperature": 0.0, "avg_logprob": -0.18669165778405888, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.5215601908712415e-06}, {"id": 677, "seek": 320198, "start": 3201.98, "end": 3208.14, "text": " But I'm just going to basically take the the thing that feeds us data as a given particularly because these things are so flexible", "tokens": [583, 286, 478, 445, 516, 281, 1936, 747, 264, 264, 551, 300, 23712, 505, 1412, 382, 257, 2212, 4098, 570, 613, 721, 366, 370, 11358], "temperature": 0.0, "avg_logprob": -0.17023200988769532, "compression_ratio": 1.6989247311827957, "no_speech_prob": 3.5008322356588906e-06}, {"id": 678, "seek": 320198, "start": 3208.34, "end": 3212.92, "text": " Right you know if you've got stuff in a data frame. You can just use this you don't have to rewrite it", "tokens": [1779, 291, 458, 498, 291, 600, 658, 1507, 294, 257, 1412, 3920, 13, 509, 393, 445, 764, 341, 291, 500, 380, 362, 281, 28132, 309], "temperature": 0.0, "avg_logprob": -0.17023200988769532, "compression_ratio": 1.6989247311827957, "no_speech_prob": 3.5008322356588906e-06}, {"id": 679, "seek": 320198, "start": 3214.14, "end": 3219.1, "text": " So that's the only fast AI thing we're using so this is a pytorch thing and so", "tokens": [407, 300, 311, 264, 787, 2370, 7318, 551, 321, 434, 1228, 370, 341, 307, 257, 25878, 284, 339, 551, 293, 370], "temperature": 0.0, "avg_logprob": -0.17023200988769532, "compression_ratio": 1.6989247311827957, "no_speech_prob": 3.5008322356588906e-06}, {"id": 680, "seek": 320198, "start": 3219.7400000000002, "end": 3226.44, "text": " Optim is the thing in pytorch that gives us an optimizer. We'll be learning about that very shortly", "tokens": [21455, 332, 307, 264, 551, 294, 25878, 284, 339, 300, 2709, 505, 364, 5028, 6545, 13, 492, 603, 312, 2539, 466, 300, 588, 13392], "temperature": 0.0, "avg_logprob": -0.17023200988769532, "compression_ratio": 1.6989247311827957, "no_speech_prob": 3.5008322356588906e-06}, {"id": 681, "seek": 320198, "start": 3227.5, "end": 3230.26, "text": " So it's actually the thing that's going to update our weights", "tokens": [407, 309, 311, 767, 264, 551, 300, 311, 516, 281, 5623, 527, 17443], "temperature": 0.0, "avg_logprob": -0.17023200988769532, "compression_ratio": 1.6989247311827957, "no_speech_prob": 3.5008322356588906e-06}, {"id": 682, "seek": 323026, "start": 3230.26, "end": 3232.26, "text": " pytorch", "tokens": [25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.2821851027639289, "compression_ratio": 1.688235294117647, "no_speech_prob": 3.3405192425561836e-06}, {"id": 683, "seek": 323026, "start": 3233.78, "end": 3241.98, "text": " Calls them the parameters of the model so earlier on we said model equals embedding dot blah blah blah right and", "tokens": [7807, 82, 552, 264, 9834, 295, 264, 2316, 370, 3071, 322, 321, 848, 2316, 6915, 12240, 3584, 5893, 12288, 12288, 12288, 558, 293], "temperature": 0.0, "avg_logprob": -0.2821851027639289, "compression_ratio": 1.688235294117647, "no_speech_prob": 3.3405192425561836e-06}, {"id": 684, "seek": 323026, "start": 3242.46, "end": 3244.3, "text": " because embedding dot", "tokens": [570, 12240, 3584, 5893], "temperature": 0.0, "avg_logprob": -0.2821851027639289, "compression_ratio": 1.688235294117647, "no_speech_prob": 3.3405192425561836e-06}, {"id": 685, "seek": 323026, "start": 3244.3, "end": 3251.1800000000003, "text": " Derives from nn dot module we get all of the pytorch module behavior and one of the things we got for free", "tokens": [5618, 1539, 490, 297, 77, 5893, 10088, 321, 483, 439, 295, 264, 25878, 284, 339, 10088, 5223, 293, 472, 295, 264, 721, 321, 658, 337, 1737], "temperature": 0.0, "avg_logprob": -0.2821851027639289, "compression_ratio": 1.688235294117647, "no_speech_prob": 3.3405192425561836e-06}, {"id": 686, "seek": 323026, "start": 3251.3, "end": 3253.94, "text": " Is the ability to say dot parameters?", "tokens": [1119, 264, 3485, 281, 584, 5893, 9834, 30], "temperature": 0.0, "avg_logprob": -0.2821851027639289, "compression_ratio": 1.688235294117647, "no_speech_prob": 3.3405192425561836e-06}, {"id": 687, "seek": 325394, "start": 3253.94, "end": 3260.78, "text": " So that's pretty that's pretty handy right that's the thing that basically is going to automatically", "tokens": [407, 300, 311, 1238, 300, 311, 1238, 13239, 558, 300, 311, 264, 551, 300, 1936, 307, 516, 281, 6772], "temperature": 0.0, "avg_logprob": -0.16722065569406533, "compression_ratio": 1.816326530612245, "no_speech_prob": 1.0188060741711524e-06}, {"id": 688, "seek": 325394, "start": 3261.7000000000003, "end": 3268.94, "text": " Give us a list of all of the weights in our model that have to be updated and so that's what gets passed to the optimizer", "tokens": [5303, 505, 257, 1329, 295, 439, 295, 264, 17443, 294, 527, 2316, 300, 362, 281, 312, 10588, 293, 370, 300, 311, 437, 2170, 4678, 281, 264, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.16722065569406533, "compression_ratio": 1.816326530612245, "no_speech_prob": 1.0188060741711524e-06}, {"id": 689, "seek": 325394, "start": 3269.58, "end": 3271.88, "text": " We also passed the optimizer the learning rate", "tokens": [492, 611, 4678, 264, 5028, 6545, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.16722065569406533, "compression_ratio": 1.816326530612245, "no_speech_prob": 1.0188060741711524e-06}, {"id": 690, "seek": 325394, "start": 3273.38, "end": 3277.62, "text": " The weight decay which we'll talk about later and momentum that we'll talk about later", "tokens": [440, 3364, 21039, 597, 321, 603, 751, 466, 1780, 293, 11244, 300, 321, 603, 751, 466, 1780], "temperature": 0.0, "avg_logprob": -0.16722065569406533, "compression_ratio": 1.816326530612245, "no_speech_prob": 1.0188060741711524e-06}, {"id": 691, "seek": 327762, "start": 3277.62, "end": 3284.1, "text": " Okay one other thing that I'm not going to do right now", "tokens": [1033, 472, 661, 551, 300, 286, 478, 406, 516, 281, 360, 558, 586], "temperature": 0.0, "avg_logprob": -0.21754363820522646, "compression_ratio": 1.6436170212765957, "no_speech_prob": 1.3287705087350332e-06}, {"id": 692, "seek": 327762, "start": 3284.1, "end": 3290.06, "text": " but we will do later is to write a training loop so the training loop is a thing that loops through each mini-batch and", "tokens": [457, 321, 486, 360, 1780, 307, 281, 2464, 257, 3097, 6367, 370, 264, 3097, 6367, 307, 257, 551, 300, 16121, 807, 1184, 8382, 12, 65, 852, 293], "temperature": 0.0, "avg_logprob": -0.21754363820522646, "compression_ratio": 1.6436170212765957, "no_speech_prob": 1.3287705087350332e-06}, {"id": 693, "seek": 327762, "start": 3291.66, "end": 3296.3399999999997, "text": " Updates the weight to subtract the gradient times the length right?", "tokens": [5858, 67, 1024, 264, 3364, 281, 16390, 264, 16235, 1413, 264, 4641, 558, 30], "temperature": 0.0, "avg_logprob": -0.21754363820522646, "compression_ratio": 1.6436170212765957, "no_speech_prob": 1.3287705087350332e-06}, {"id": 694, "seek": 327762, "start": 3297.62, "end": 3303.06, "text": " There's a function in fast AI which is the training loop and it's", "tokens": [821, 311, 257, 2445, 294, 2370, 7318, 597, 307, 264, 3097, 6367, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.21754363820522646, "compression_ratio": 1.6436170212765957, "no_speech_prob": 1.3287705087350332e-06}, {"id": 695, "seek": 330306, "start": 3303.06, "end": 3305.06, "text": " It's", "tokens": [467, 311], "temperature": 0.0, "avg_logprob": -0.21494223854758523, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.733046588014986e-06}, {"id": 696, "seek": 330306, "start": 3307.18, "end": 3309.18, "text": " It's pretty simple", "tokens": [467, 311, 1238, 2199], "temperature": 0.0, "avg_logprob": -0.21494223854758523, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.733046588014986e-06}, {"id": 697, "seek": 330306, "start": 3310.18, "end": 3313.24, "text": " Here it is right for epoch in epochs", "tokens": [1692, 309, 307, 558, 337, 30992, 339, 294, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.21494223854758523, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.733046588014986e-06}, {"id": 698, "seek": 330306, "start": 3314.74, "end": 3320.98, "text": " This is just the thing that shows a progress bar so ignore this for X comma Y in my training data loader", "tokens": [639, 307, 445, 264, 551, 300, 3110, 257, 4205, 2159, 370, 11200, 341, 337, 1783, 22117, 398, 294, 452, 3097, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.21494223854758523, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.733046588014986e-06}, {"id": 699, "seek": 330306, "start": 3322.2599999999998, "end": 3324.2599999999998, "text": " calculate the loss", "tokens": [8873, 264, 4470], "temperature": 0.0, "avg_logprob": -0.21494223854758523, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.733046588014986e-06}, {"id": 700, "seek": 330306, "start": 3325.98, "end": 3329.46, "text": " Print out the loss in our in our progress bar", "tokens": [34439, 484, 264, 4470, 294, 527, 294, 527, 4205, 2159], "temperature": 0.0, "avg_logprob": -0.21494223854758523, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.733046588014986e-06}, {"id": 701, "seek": 332946, "start": 3329.46, "end": 3333.38, "text": " Call any callbacks you have and at the end", "tokens": [7807, 604, 818, 17758, 291, 362, 293, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.2358773897771966, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.994720605580369e-06}, {"id": 702, "seek": 332946, "start": 3335.94, "end": 3343.98, "text": " Call the call the metrics on the validation right so this is just for each epoch go through each mini-batch", "tokens": [7807, 264, 818, 264, 16367, 322, 264, 24071, 558, 370, 341, 307, 445, 337, 1184, 30992, 339, 352, 807, 1184, 8382, 12, 65, 852], "temperature": 0.0, "avg_logprob": -0.2358773897771966, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.994720605580369e-06}, {"id": 703, "seek": 332946, "start": 3344.9, "end": 3348.7400000000002, "text": " And do one step of our optimizer step is", "tokens": [400, 360, 472, 1823, 295, 527, 5028, 6545, 1823, 307], "temperature": 0.0, "avg_logprob": -0.2358773897771966, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.994720605580369e-06}, {"id": 704, "seek": 332946, "start": 3350.34, "end": 3354.82, "text": " Basically going to take advantage of this optimizer, but we'll be writing that from scratch shortly", "tokens": [8537, 516, 281, 747, 5002, 295, 341, 5028, 6545, 11, 457, 321, 603, 312, 3579, 300, 490, 8459, 13392], "temperature": 0.0, "avg_logprob": -0.2358773897771966, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.994720605580369e-06}, {"id": 705, "seek": 335482, "start": 3354.82, "end": 3359.02, "text": " So this is what notice we're not using a learner", "tokens": [407, 341, 307, 437, 3449, 321, 434, 406, 1228, 257, 33347], "temperature": 0.0, "avg_logprob": -0.24373092651367187, "compression_ratio": 1.7131147540983607, "no_speech_prob": 8.579214636483812e-07}, {"id": 706, "seek": 335482, "start": 3359.94, "end": 3366.42, "text": " Okay, we're just using a pipe watch module so this this fit thing although. It's past it part of fast AI", "tokens": [1033, 11, 321, 434, 445, 1228, 257, 11240, 1159, 10088, 370, 341, 341, 3318, 551, 4878, 13, 467, 311, 1791, 309, 644, 295, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.24373092651367187, "compression_ratio": 1.7131147540983607, "no_speech_prob": 8.579214636483812e-07}, {"id": 707, "seek": 335482, "start": 3366.7000000000003, "end": 3371.5800000000004, "text": " It's like lower down the layers of abstraction now. This is the thing that takes a", "tokens": [467, 311, 411, 3126, 760, 264, 7914, 295, 37765, 586, 13, 639, 307, 264, 551, 300, 2516, 257], "temperature": 0.0, "avg_logprob": -0.24373092651367187, "compression_ratio": 1.7131147540983607, "no_speech_prob": 8.579214636483812e-07}, {"id": 708, "seek": 335482, "start": 3372.1400000000003, "end": 3375.94, "text": " regular high torch model, so if you ever want to like", "tokens": [3890, 1090, 27822, 2316, 11, 370, 498, 291, 1562, 528, 281, 411], "temperature": 0.0, "avg_logprob": -0.24373092651367187, "compression_ratio": 1.7131147540983607, "no_speech_prob": 8.579214636483812e-07}, {"id": 709, "seek": 335482, "start": 3377.2200000000003, "end": 3379.2200000000003, "text": " skip as much", "tokens": [10023, 382, 709], "temperature": 0.0, "avg_logprob": -0.24373092651367187, "compression_ratio": 1.7131147540983607, "no_speech_prob": 8.579214636483812e-07}, {"id": 710, "seek": 337922, "start": 3379.22, "end": 3385.4199999999996, "text": " Fast AI stuff as possible like you've got some high torch model. You've got some code on the internet you basically want to run it", "tokens": [15968, 7318, 1507, 382, 1944, 411, 291, 600, 658, 512, 1090, 27822, 2316, 13, 509, 600, 658, 512, 3089, 322, 264, 4705, 291, 1936, 528, 281, 1190, 309], "temperature": 0.0, "avg_logprob": -0.18296596460175096, "compression_ratio": 1.8235294117647058, "no_speech_prob": 4.565937160805333e-06}, {"id": 711, "seek": 337922, "start": 3386.02, "end": 3388.18, "text": " But you don't want to write your own training loop", "tokens": [583, 291, 500, 380, 528, 281, 2464, 428, 1065, 3097, 6367], "temperature": 0.0, "avg_logprob": -0.18296596460175096, "compression_ratio": 1.8235294117647058, "no_speech_prob": 4.565937160805333e-06}, {"id": 712, "seek": 337922, "start": 3388.58, "end": 3392.58, "text": " Then this is this is what you want to do you want to call fast AI's fit function?", "tokens": [1396, 341, 307, 341, 307, 437, 291, 528, 281, 360, 291, 528, 281, 818, 2370, 7318, 311, 3318, 2445, 30], "temperature": 0.0, "avg_logprob": -0.18296596460175096, "compression_ratio": 1.8235294117647058, "no_speech_prob": 4.565937160805333e-06}, {"id": 713, "seek": 337922, "start": 3392.58, "end": 3399.14, "text": " And so what you'll find is like the library is designed so that you can kind of dig in at any layer of abstraction", "tokens": [400, 370, 437, 291, 603, 915, 307, 411, 264, 6405, 307, 4761, 370, 300, 291, 393, 733, 295, 2528, 294, 412, 604, 4583, 295, 37765], "temperature": 0.0, "avg_logprob": -0.18296596460175096, "compression_ratio": 1.8235294117647058, "no_speech_prob": 4.565937160805333e-06}, {"id": 714, "seek": 337922, "start": 3399.8199999999997, "end": 3405.4599999999996, "text": " You like right and so at this layer of abstraction you're not going to get things like", "tokens": [509, 411, 558, 293, 370, 412, 341, 4583, 295, 37765, 291, 434, 406, 516, 281, 483, 721, 411], "temperature": 0.0, "avg_logprob": -0.18296596460175096, "compression_ratio": 1.8235294117647058, "no_speech_prob": 4.565937160805333e-06}, {"id": 715, "seek": 340546, "start": 3405.46, "end": 3412.18, "text": " Stochastic gradient descent with restarts. You're not going to get like differential learning rates like all that stuff", "tokens": [745, 8997, 2750, 16235, 23475, 365, 1472, 11814, 13, 509, 434, 406, 516, 281, 483, 411, 15756, 2539, 6846, 411, 439, 300, 1507], "temperature": 0.0, "avg_logprob": -0.14384986172203257, "compression_ratio": 1.7077464788732395, "no_speech_prob": 3.9054557419149205e-06}, {"id": 716, "seek": 340546, "start": 3412.18, "end": 3416.66, "text": " That's in the learner like you could do it, but you'd have to write it all by by hand yourself", "tokens": [663, 311, 294, 264, 33347, 411, 291, 727, 360, 309, 11, 457, 291, 1116, 362, 281, 2464, 309, 439, 538, 538, 1011, 1803], "temperature": 0.0, "avg_logprob": -0.14384986172203257, "compression_ratio": 1.7077464788732395, "no_speech_prob": 3.9054557419149205e-06}, {"id": 717, "seek": 340546, "start": 3416.98, "end": 3420.94, "text": " Right and that's the downside of kind of going down to this level of abstraction", "tokens": [1779, 293, 300, 311, 264, 25060, 295, 733, 295, 516, 760, 281, 341, 1496, 295, 37765], "temperature": 0.0, "avg_logprob": -0.14384986172203257, "compression_ratio": 1.7077464788732395, "no_speech_prob": 3.9054557419149205e-06}, {"id": 718, "seek": 340546, "start": 3421.62, "end": 3427.1, "text": " The upside is that as you saw the code for this is very simple. It's just a simple training loop", "tokens": [440, 14119, 307, 300, 382, 291, 1866, 264, 3089, 337, 341, 307, 588, 2199, 13, 467, 311, 445, 257, 2199, 3097, 6367], "temperature": 0.0, "avg_logprob": -0.14384986172203257, "compression_ratio": 1.7077464788732395, "no_speech_prob": 3.9054557419149205e-06}, {"id": 719, "seek": 340546, "start": 3427.1, "end": 3429.1, "text": " It takes a standard high torch model", "tokens": [467, 2516, 257, 3832, 1090, 27822, 2316], "temperature": 0.0, "avg_logprob": -0.14384986172203257, "compression_ratio": 1.7077464788732395, "no_speech_prob": 3.9054557419149205e-06}, {"id": 720, "seek": 340546, "start": 3429.7, "end": 3432.5, "text": " So this is like this is a good thing for us to use here", "tokens": [407, 341, 307, 411, 341, 307, 257, 665, 551, 337, 505, 281, 764, 510], "temperature": 0.0, "avg_logprob": -0.14384986172203257, "compression_ratio": 1.7077464788732395, "no_speech_prob": 3.9054557419149205e-06}, {"id": 721, "seek": 343250, "start": 3432.5, "end": 3438.38, "text": " We can we just call it and it looks exactly like what we're we're used to saying right we get our", "tokens": [492, 393, 321, 445, 818, 309, 293, 309, 1542, 2293, 411, 437, 321, 434, 321, 434, 1143, 281, 1566, 558, 321, 483, 527], "temperature": 0.0, "avg_logprob": -0.2783237507468776, "compression_ratio": 1.5571428571428572, "no_speech_prob": 3.6688441014121054e-06}, {"id": 722, "seek": 343250, "start": 3439.58, "end": 3443.26, "text": " validation and training loss for the three E plus", "tokens": [24071, 293, 3097, 4470, 337, 264, 1045, 462, 1804], "temperature": 0.0, "avg_logprob": -0.2783237507468776, "compression_ratio": 1.5571428571428572, "no_speech_prob": 3.6688441014121054e-06}, {"id": 723, "seek": 343250, "start": 3444.26, "end": 3446.26, "text": " now you'll notice that", "tokens": [586, 291, 603, 3449, 300], "temperature": 0.0, "avg_logprob": -0.2783237507468776, "compression_ratio": 1.5571428571428572, "no_speech_prob": 3.6688441014121054e-06}, {"id": 724, "seek": 343250, "start": 3448.66, "end": 3451.3, "text": " We wanted something around point seven six", "tokens": [492, 1415, 746, 926, 935, 3407, 2309], "temperature": 0.0, "avg_logprob": -0.2783237507468776, "compression_ratio": 1.5571428571428572, "no_speech_prob": 3.6688441014121054e-06}, {"id": 725, "seek": 343250, "start": 3452.98, "end": 3459.62, "text": " So we're not there so in other words the the default fast AI collaborative filtering algorithm is doing something", "tokens": [407, 321, 434, 406, 456, 370, 294, 661, 2283, 264, 264, 7576, 2370, 7318, 16555, 30822, 9284, 307, 884, 746], "temperature": 0.0, "avg_logprob": -0.2783237507468776, "compression_ratio": 1.5571428571428572, "no_speech_prob": 3.6688441014121054e-06}, {"id": 726, "seek": 345962, "start": 3459.62, "end": 3461.62, "text": " smarter than this", "tokens": [20294, 813, 341], "temperature": 0.0, "avg_logprob": -0.17118988037109376, "compression_ratio": 1.880952380952381, "no_speech_prob": 3.0894814244675217e-06}, {"id": 727, "seek": 345962, "start": 3462.18, "end": 3464.38, "text": " So we're going to try and do that", "tokens": [407, 321, 434, 516, 281, 853, 293, 360, 300], "temperature": 0.0, "avg_logprob": -0.17118988037109376, "compression_ratio": 1.880952380952381, "no_speech_prob": 3.0894814244675217e-06}, {"id": 728, "seek": 345962, "start": 3465.06, "end": 3469.8199999999997, "text": " One thing that we can do since we're calling our you know this lower level fit function", "tokens": [1485, 551, 300, 321, 393, 360, 1670, 321, 434, 5141, 527, 291, 458, 341, 3126, 1496, 3318, 2445], "temperature": 0.0, "avg_logprob": -0.17118988037109376, "compression_ratio": 1.880952380952381, "no_speech_prob": 3.0894814244675217e-06}, {"id": 729, "seek": 345962, "start": 3469.8199999999997, "end": 3474.66, "text": " There's no learning rate annealing we could do our own learning rate annealing so you can hear it see here", "tokens": [821, 311, 572, 2539, 3314, 22256, 4270, 321, 727, 360, 527, 1065, 2539, 3314, 22256, 4270, 370, 291, 393, 1568, 309, 536, 510], "temperature": 0.0, "avg_logprob": -0.17118988037109376, "compression_ratio": 1.880952380952381, "no_speech_prob": 3.0894814244675217e-06}, {"id": 730, "seek": 345962, "start": 3474.8599999999997, "end": 3477.2599999999998, "text": " There's a fast AI function called set learning rates", "tokens": [821, 311, 257, 2370, 7318, 2445, 1219, 992, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.17118988037109376, "compression_ratio": 1.880952380952381, "no_speech_prob": 3.0894814244675217e-06}, {"id": 731, "seek": 345962, "start": 3477.46, "end": 3484.4, "text": " You can pass in a standard high torch optimizer and pass in your new learning rate and then call fit again", "tokens": [509, 393, 1320, 294, 257, 3832, 1090, 27822, 5028, 6545, 293, 1320, 294, 428, 777, 2539, 3314, 293, 550, 818, 3318, 797], "temperature": 0.0, "avg_logprob": -0.17118988037109376, "compression_ratio": 1.880952380952381, "no_speech_prob": 3.0894814244675217e-06}, {"id": 732, "seek": 345962, "start": 3484.4, "end": 3489.14, "text": " And so this is how we can like manually do a learning rate schedule", "tokens": [400, 370, 341, 307, 577, 321, 393, 411, 16945, 360, 257, 2539, 3314, 7567], "temperature": 0.0, "avg_logprob": -0.17118988037109376, "compression_ratio": 1.880952380952381, "no_speech_prob": 3.0894814244675217e-06}, {"id": 733, "seek": 348914, "start": 3489.14, "end": 3491.14, "text": " And so you can see we've got a little bit better", "tokens": [400, 370, 291, 393, 536, 321, 600, 658, 257, 707, 857, 1101], "temperature": 0.0, "avg_logprob": -0.20221162523542133, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.844910679617897e-06}, {"id": 734, "seek": 348914, "start": 3491.74, "end": 3493.18, "text": " 1.13", "tokens": [502, 13, 7668], "temperature": 0.0, "avg_logprob": -0.20221162523542133, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.844910679617897e-06}, {"id": 735, "seek": 348914, "start": 3493.18, "end": 3495.62, "text": " We still got a long way to go", "tokens": [492, 920, 658, 257, 938, 636, 281, 352], "temperature": 0.0, "avg_logprob": -0.20221162523542133, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.844910679617897e-06}, {"id": 736, "seek": 348914, "start": 3497.1, "end": 3498.3799999999997, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20221162523542133, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.844910679617897e-06}, {"id": 737, "seek": 348914, "start": 3498.3799999999997, "end": 3500.7799999999997, "text": " so I think what we might do is we might have a", "tokens": [370, 286, 519, 437, 321, 1062, 360, 307, 321, 1062, 362, 257], "temperature": 0.0, "avg_logprob": -0.20221162523542133, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.844910679617897e-06}, {"id": 738, "seek": 348914, "start": 3502.3399999999997, "end": 3508.2799999999997, "text": " Seven minute break, and then we're going to come back and try and improve this score a bit", "tokens": [14868, 3456, 1821, 11, 293, 550, 321, 434, 516, 281, 808, 646, 293, 853, 293, 3470, 341, 6175, 257, 857], "temperature": 0.0, "avg_logprob": -0.20221162523542133, "compression_ratio": 1.4037267080745341, "no_speech_prob": 3.844910679617897e-06}, {"id": 739, "seek": 350828, "start": 3508.28, "end": 3510.28, "text": " For", "tokens": [1171], "temperature": 0.0, "avg_logprob": -0.28724191806934496, "compression_ratio": 1.2980132450331126, "no_speech_prob": 2.994397391375969e-06}, {"id": 740, "seek": 350828, "start": 3517.7200000000003, "end": 3520.8, "text": " Those who are interested somebody was asking me at the break for a kind of a quick", "tokens": [3950, 567, 366, 3102, 2618, 390, 3365, 385, 412, 264, 1821, 337, 257, 733, 295, 257, 1702], "temperature": 0.0, "avg_logprob": -0.28724191806934496, "compression_ratio": 1.2980132450331126, "no_speech_prob": 2.994397391375969e-06}, {"id": 741, "seek": 350828, "start": 3521.84, "end": 3529.1200000000003, "text": " Walk through so this is totally optional, but if you go into the fast AI library. There's a model.py file", "tokens": [10818, 807, 370, 341, 307, 3879, 17312, 11, 457, 498, 291, 352, 666, 264, 2370, 7318, 6405, 13, 821, 311, 257, 2316, 13, 8200, 3991], "temperature": 0.0, "avg_logprob": -0.28724191806934496, "compression_ratio": 1.2980132450331126, "no_speech_prob": 2.994397391375969e-06}, {"id": 742, "seek": 350828, "start": 3530.0400000000004, "end": 3532.0400000000004, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.28724191806934496, "compression_ratio": 1.2980132450331126, "no_speech_prob": 2.994397391375969e-06}, {"id": 743, "seek": 353204, "start": 3532.04, "end": 3539.12, "text": " And that's where fit is which we're just looking at which goes through each epoch in", "tokens": [400, 300, 311, 689, 3318, 307, 597, 321, 434, 445, 1237, 412, 597, 1709, 807, 1184, 30992, 339, 294], "temperature": 0.0, "avg_logprob": -0.2981045586722238, "compression_ratio": 1.6700507614213198, "no_speech_prob": 3.966944859712385e-06}, {"id": 744, "seek": 353204, "start": 3539.56, "end": 3545.56, "text": " Epochs and then goes through each X and Y in the mini batch and then it calls this", "tokens": [462, 2259, 28346, 293, 550, 1709, 807, 1184, 1783, 293, 398, 294, 264, 8382, 15245, 293, 550, 309, 5498, 341], "temperature": 0.0, "avg_logprob": -0.2981045586722238, "compression_ratio": 1.6700507614213198, "no_speech_prob": 3.966944859712385e-06}, {"id": 745, "seek": 353204, "start": 3546.96, "end": 3550.04, "text": " Step function so the step function", "tokens": [5470, 2445, 370, 264, 1823, 2445], "temperature": 0.0, "avg_logprob": -0.2981045586722238, "compression_ratio": 1.6700507614213198, "no_speech_prob": 3.966944859712385e-06}, {"id": 746, "seek": 353204, "start": 3553.32, "end": 3554.92, "text": " Is", "tokens": [1119], "temperature": 0.0, "avg_logprob": -0.2981045586722238, "compression_ratio": 1.6700507614213198, "no_speech_prob": 3.966944859712385e-06}, {"id": 747, "seek": 353204, "start": 3554.92, "end": 3561.12, "text": " Here and you can see the key thing is it calculates the output from the model models quarter m right and so if you remember", "tokens": [1692, 293, 291, 393, 536, 264, 2141, 551, 307, 309, 4322, 1024, 264, 5598, 490, 264, 2316, 5245, 6555, 275, 558, 293, 370, 498, 291, 1604], "temperature": 0.0, "avg_logprob": -0.2981045586722238, "compression_ratio": 1.6700507614213198, "no_speech_prob": 3.966944859712385e-06}, {"id": 748, "seek": 356112, "start": 3561.12, "end": 3563.12, "text": " our dot product", "tokens": [527, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.21614057562324437, "compression_ratio": 1.6635071090047393, "no_speech_prob": 4.4951225390832406e-06}, {"id": 749, "seek": 356112, "start": 3565.44, "end": 3568.3199999999997, "text": " We didn't actually call model dot forward", "tokens": [492, 994, 380, 767, 818, 2316, 5893, 2128], "temperature": 0.0, "avg_logprob": -0.21614057562324437, "compression_ratio": 1.6635071090047393, "no_speech_prob": 4.4951225390832406e-06}, {"id": 750, "seek": 356112, "start": 3568.3199999999997, "end": 3571.3199999999997, "text": " We just called model parentheses, and that's because the", "tokens": [492, 445, 1219, 2316, 34153, 11, 293, 300, 311, 570, 264], "temperature": 0.0, "avg_logprob": -0.21614057562324437, "compression_ratio": 1.6635071090047393, "no_speech_prob": 4.4951225390832406e-06}, {"id": 751, "seek": 356112, "start": 3572.12, "end": 3574.12, "text": " NN dot module", "tokens": [426, 45, 5893, 10088], "temperature": 0.0, "avg_logprob": -0.21614057562324437, "compression_ratio": 1.6635071090047393, "no_speech_prob": 4.4951225390832406e-06}, {"id": 752, "seek": 356112, "start": 3574.4, "end": 3580.16, "text": " Automatically you know when you call it as if it's a function it passes it along to forward okay", "tokens": [24619, 5030, 291, 458, 562, 291, 818, 309, 382, 498, 309, 311, 257, 2445, 309, 11335, 309, 2051, 281, 2128, 1392], "temperature": 0.0, "avg_logprob": -0.21614057562324437, "compression_ratio": 1.6635071090047393, "no_speech_prob": 4.4951225390832406e-06}, {"id": 753, "seek": 356112, "start": 3580.16, "end": 3586.88, "text": " So that's that's what that's doing there, right and then the rest of this will learn about shortly. She's basically doing the", "tokens": [407, 300, 311, 300, 311, 437, 300, 311, 884, 456, 11, 558, 293, 550, 264, 1472, 295, 341, 486, 1466, 466, 13392, 13, 1240, 311, 1936, 884, 264], "temperature": 0.0, "avg_logprob": -0.21614057562324437, "compression_ratio": 1.6635071090047393, "no_speech_prob": 4.4951225390832406e-06}, {"id": 754, "seek": 358688, "start": 3586.88, "end": 3590.76, "text": " the loss function and then the", "tokens": [264, 4470, 2445, 293, 550, 264], "temperature": 0.0, "avg_logprob": -0.23486651370399877, "compression_ratio": 1.5707762557077625, "no_speech_prob": 3.90546074413578e-06}, {"id": 755, "seek": 358688, "start": 3591.28, "end": 3594.08, "text": " Backward pass okay, so for those who are interested", "tokens": [5833, 1007, 1320, 1392, 11, 370, 337, 729, 567, 366, 3102], "temperature": 0.0, "avg_logprob": -0.23486651370399877, "compression_ratio": 1.5707762557077625, "no_speech_prob": 3.90546074413578e-06}, {"id": 756, "seek": 358688, "start": 3594.4, "end": 3599.1600000000003, "text": " That's that's kind of gives you a bit of a sense of how the code is structured if you want to look at it", "tokens": [663, 311, 300, 311, 733, 295, 2709, 291, 257, 857, 295, 257, 2020, 295, 577, 264, 3089, 307, 18519, 498, 291, 528, 281, 574, 412, 309], "temperature": 0.0, "avg_logprob": -0.23486651370399877, "compression_ratio": 1.5707762557077625, "no_speech_prob": 3.90546074413578e-06}, {"id": 757, "seek": 358688, "start": 3599.96, "end": 3601.96, "text": " and as I say like the", "tokens": [293, 382, 286, 584, 411, 264], "temperature": 0.0, "avg_logprob": -0.23486651370399877, "compression_ratio": 1.5707762557077625, "no_speech_prob": 3.90546074413578e-06}, {"id": 758, "seek": 358688, "start": 3601.96, "end": 3604.44, "text": " the fast AI code is designed to", "tokens": [264, 2370, 7318, 3089, 307, 4761, 281], "temperature": 0.0, "avg_logprob": -0.23486651370399877, "compression_ratio": 1.5707762557077625, "no_speech_prob": 3.90546074413578e-06}, {"id": 759, "seek": 358688, "start": 3605.2400000000002, "end": 3607.76, "text": " both be world-class performance, but also", "tokens": [1293, 312, 1002, 12, 11665, 3389, 11, 457, 611], "temperature": 0.0, "avg_logprob": -0.23486651370399877, "compression_ratio": 1.5707762557077625, "no_speech_prob": 3.90546074413578e-06}, {"id": 760, "seek": 358688, "start": 3608.6800000000003, "end": 3613.44, "text": " Pretty easy to read so like feel free like take a look at it", "tokens": [10693, 1858, 281, 1401, 370, 411, 841, 1737, 411, 747, 257, 574, 412, 309], "temperature": 0.0, "avg_logprob": -0.23486651370399877, "compression_ratio": 1.5707762557077625, "no_speech_prob": 3.90546074413578e-06}, {"id": 761, "seek": 361344, "start": 3613.44, "end": 3619.4, "text": " And if you want to know what's going on just ask on the forums and if you know if you think there's anything that could be", "tokens": [400, 498, 291, 528, 281, 458, 437, 311, 516, 322, 445, 1029, 322, 264, 26998, 293, 498, 291, 458, 498, 291, 519, 456, 311, 1340, 300, 727, 312], "temperature": 0.0, "avg_logprob": -0.17036668185530038, "compression_ratio": 1.645320197044335, "no_speech_prob": 6.786724497942487e-07}, {"id": 762, "seek": 361344, "start": 3620.16, "end": 3621.84, "text": " clearer", "tokens": [26131], "temperature": 0.0, "avg_logprob": -0.17036668185530038, "compression_ratio": 1.645320197044335, "no_speech_prob": 6.786724497942487e-07}, {"id": 763, "seek": 361344, "start": 3621.84, "end": 3623.84, "text": " Let us know", "tokens": [961, 505, 458], "temperature": 0.0, "avg_logprob": -0.17036668185530038, "compression_ratio": 1.645320197044335, "no_speech_prob": 6.786724497942487e-07}, {"id": 764, "seek": 361344, "start": 3624.56, "end": 3628.88, "text": " Because yeah, the code is definitely you know we're going to be digging into the code or and more", "tokens": [1436, 1338, 11, 264, 3089, 307, 2138, 291, 458, 321, 434, 516, 281, 312, 17343, 666, 264, 3089, 420, 293, 544], "temperature": 0.0, "avg_logprob": -0.17036668185530038, "compression_ratio": 1.645320197044335, "no_speech_prob": 6.786724497942487e-07}, {"id": 765, "seek": 361344, "start": 3631.2400000000002, "end": 3636.56, "text": " Okay, so let's try and improve this a little bit and let's start off by improving it in Excel", "tokens": [1033, 11, 370, 718, 311, 853, 293, 3470, 341, 257, 707, 857, 293, 718, 311, 722, 766, 538, 11470, 309, 294, 19060], "temperature": 0.0, "avg_logprob": -0.17036668185530038, "compression_ratio": 1.645320197044335, "no_speech_prob": 6.786724497942487e-07}, {"id": 766, "seek": 363656, "start": 3636.56, "end": 3642.36, "text": " So you might have noticed here that we've kind of got the idea that", "tokens": [407, 291, 1062, 362, 5694, 510, 300, 321, 600, 733, 295, 658, 264, 1558, 300], "temperature": 0.0, "avg_logprob": -0.24881856034441693, "compression_ratio": 1.5870646766169154, "no_speech_prob": 8.990900823846459e-07}, {"id": 767, "seek": 363656, "start": 3643.12, "end": 3644.92, "text": " user 72", "tokens": [4195, 18731], "temperature": 0.0, "avg_logprob": -0.24881856034441693, "compression_ratio": 1.5870646766169154, "no_speech_prob": 8.990900823846459e-07}, {"id": 768, "seek": 363656, "start": 3644.92, "end": 3649.0, "text": " You know like sci-fi modern movies with special effects", "tokens": [509, 458, 411, 2180, 12, 13325, 4363, 6233, 365, 2121, 5065], "temperature": 0.0, "avg_logprob": -0.24881856034441693, "compression_ratio": 1.5870646766169154, "no_speech_prob": 8.990900823846459e-07}, {"id": 769, "seek": 363656, "start": 3649.16, "end": 3655.54, "text": " You know whatever and movie number 27 is sci-fi and has special effects and not much dialogue", "tokens": [509, 458, 2035, 293, 3169, 1230, 7634, 307, 2180, 12, 13325, 293, 575, 2121, 5065, 293, 406, 709, 10221], "temperature": 0.0, "avg_logprob": -0.24881856034441693, "compression_ratio": 1.5870646766169154, "no_speech_prob": 8.990900823846459e-07}, {"id": 770, "seek": 363656, "start": 3656.64, "end": 3658.92, "text": " But we're missing an important case", "tokens": [583, 321, 434, 5361, 364, 1021, 1389], "temperature": 0.0, "avg_logprob": -0.24881856034441693, "compression_ratio": 1.5870646766169154, "no_speech_prob": 8.990900823846459e-07}, {"id": 771, "seek": 363656, "start": 3659.48, "end": 3661.48, "text": " which is like", "tokens": [597, 307, 411], "temperature": 0.0, "avg_logprob": -0.24881856034441693, "compression_ratio": 1.5870646766169154, "no_speech_prob": 8.990900823846459e-07}, {"id": 772, "seek": 363656, "start": 3661.52, "end": 3663.52, "text": " user 72 is", "tokens": [4195, 18731, 307], "temperature": 0.0, "avg_logprob": -0.24881856034441693, "compression_ratio": 1.5870646766169154, "no_speech_prob": 8.990900823846459e-07}, {"id": 773, "seek": 366352, "start": 3663.52, "end": 3671.56, "text": " Pretty enthusiastic on the whole and on average rates things highly highly you know and movie 27", "tokens": [10693, 28574, 322, 264, 1379, 293, 322, 4274, 6846, 721, 5405, 5405, 291, 458, 293, 3169, 7634], "temperature": 0.0, "avg_logprob": -0.19649221680381082, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.203560817790276e-07}, {"id": 774, "seek": 366352, "start": 3672.12, "end": 3674.36, "text": " You know it's just a popular movie", "tokens": [509, 458, 309, 311, 445, 257, 3743, 3169], "temperature": 0.0, "avg_logprob": -0.19649221680381082, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.203560817790276e-07}, {"id": 775, "seek": 366352, "start": 3674.92, "end": 3677.04, "text": " You know which just on average it's higher", "tokens": [509, 458, 597, 445, 322, 4274, 309, 311, 2946], "temperature": 0.0, "avg_logprob": -0.19649221680381082, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.203560817790276e-07}, {"id": 776, "seek": 366352, "start": 3677.12, "end": 3680.04, "text": " so what we'd really like is to add a", "tokens": [370, 437, 321, 1116, 534, 411, 307, 281, 909, 257], "temperature": 0.0, "avg_logprob": -0.19649221680381082, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.203560817790276e-07}, {"id": 777, "seek": 366352, "start": 3680.72, "end": 3688.3, "text": " Constant for the user and a constant for the movie and remember in neural network terms we call that a bias", "tokens": [37413, 337, 264, 4195, 293, 257, 5754, 337, 264, 3169, 293, 1604, 294, 18161, 3209, 2115, 321, 818, 300, 257, 12577], "temperature": 0.0, "avg_logprob": -0.19649221680381082, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.203560817790276e-07}, {"id": 778, "seek": 366352, "start": 3688.92, "end": 3690.84, "text": " That's where you want to add a bias", "tokens": [663, 311, 689, 291, 528, 281, 909, 257, 12577], "temperature": 0.0, "avg_logprob": -0.19649221680381082, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.203560817790276e-07}, {"id": 779, "seek": 369084, "start": 3690.84, "end": 3695.04, "text": " So we could easily do that and if we go into the bias tab here", "tokens": [407, 321, 727, 3612, 360, 300, 293, 498, 321, 352, 666, 264, 12577, 4421, 510], "temperature": 0.0, "avg_logprob": -0.15482270982530383, "compression_ratio": 1.7872340425531914, "no_speech_prob": 1.3287733509059763e-06}, {"id": 780, "seek": 369084, "start": 3695.04, "end": 3698.56, "text": " We've got the same data as before and we've got the same", "tokens": [492, 600, 658, 264, 912, 1412, 382, 949, 293, 321, 600, 658, 264, 912], "temperature": 0.0, "avg_logprob": -0.15482270982530383, "compression_ratio": 1.7872340425531914, "no_speech_prob": 1.3287733509059763e-06}, {"id": 781, "seek": 369084, "start": 3700.08, "end": 3703.76, "text": " Latent factors as before and I've just got one extra", "tokens": [7354, 317, 6771, 382, 949, 293, 286, 600, 445, 658, 472, 2857], "temperature": 0.0, "avg_logprob": -0.15482270982530383, "compression_ratio": 1.7872340425531914, "no_speech_prob": 1.3287733509059763e-06}, {"id": 782, "seek": 369084, "start": 3704.48, "end": 3710.84, "text": " Row here and one extra column here, and you won't be surprised here that we now", "tokens": [20309, 510, 293, 472, 2857, 7738, 510, 11, 293, 291, 1582, 380, 312, 6100, 510, 300, 321, 586], "temperature": 0.0, "avg_logprob": -0.15482270982530383, "compression_ratio": 1.7872340425531914, "no_speech_prob": 1.3287733509059763e-06}, {"id": 783, "seek": 369084, "start": 3711.32, "end": 3715.46, "text": " Take the same matrix multiplication as before and we add in", "tokens": [3664, 264, 912, 8141, 27290, 382, 949, 293, 321, 909, 294], "temperature": 0.0, "avg_logprob": -0.15482270982530383, "compression_ratio": 1.7872340425531914, "no_speech_prob": 1.3287733509059763e-06}, {"id": 784, "seek": 369084, "start": 3716.2000000000003, "end": 3718.6800000000003, "text": " That and we add in that", "tokens": [663, 293, 321, 909, 294, 300], "temperature": 0.0, "avg_logprob": -0.15482270982530383, "compression_ratio": 1.7872340425531914, "no_speech_prob": 1.3287733509059763e-06}, {"id": 785, "seek": 371868, "start": 3718.68, "end": 3720.16, "text": " That", "tokens": [663], "temperature": 0.0, "avg_logprob": -0.2039110030250988, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.101590441976441e-06}, {"id": 786, "seek": 371868, "start": 3720.16, "end": 3722.66, "text": " Okay, so that's our bias", "tokens": [1033, 11, 370, 300, 311, 527, 12577], "temperature": 0.0, "avg_logprob": -0.2039110030250988, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.101590441976441e-06}, {"id": 787, "seek": 371868, "start": 3723.72, "end": 3727.7599999999998, "text": " So other than that we've got exactly the same loss function over here", "tokens": [407, 661, 813, 300, 321, 600, 658, 2293, 264, 912, 4470, 2445, 670, 510], "temperature": 0.0, "avg_logprob": -0.2039110030250988, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.101590441976441e-06}, {"id": 788, "seek": 371868, "start": 3727.7599999999998, "end": 3732.8799999999997, "text": " And so just like before we can now go ahead and solve that", "tokens": [400, 370, 445, 411, 949, 321, 393, 586, 352, 2286, 293, 5039, 300], "temperature": 0.0, "avg_logprob": -0.2039110030250988, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.101590441976441e-06}, {"id": 789, "seek": 371868, "start": 3733.3199999999997, "end": 3736.64, "text": " and now our changing variables include the", "tokens": [293, 586, 527, 4473, 9102, 4090, 264], "temperature": 0.0, "avg_logprob": -0.2039110030250988, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.101590441976441e-06}, {"id": 790, "seek": 371868, "start": 3737.2, "end": 3743.16, "text": " Bias and we can say solve and if we leave that for a little while it will come to a", "tokens": [363, 4609, 293, 321, 393, 584, 5039, 293, 498, 321, 1856, 300, 337, 257, 707, 1339, 309, 486, 808, 281, 257], "temperature": 0.0, "avg_logprob": -0.2039110030250988, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.101590441976441e-06}, {"id": 791, "seek": 371868, "start": 3744.3199999999997, "end": 3745.64, "text": " better", "tokens": [1101], "temperature": 0.0, "avg_logprob": -0.2039110030250988, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.101590441976441e-06}, {"id": 792, "seek": 371868, "start": 3745.64, "end": 3747.64, "text": " result than we had before", "tokens": [1874, 813, 321, 632, 949], "temperature": 0.0, "avg_logprob": -0.2039110030250988, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.101590441976441e-06}, {"id": 793, "seek": 374764, "start": 3747.64, "end": 3752.08, "text": " Okay, so that's the first thing we're going to do to improve our model and", "tokens": [1033, 11, 370, 300, 311, 264, 700, 551, 321, 434, 516, 281, 360, 281, 3470, 527, 2316, 293], "temperature": 0.0, "avg_logprob": -0.23622351752387152, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.5779575051055872e-06}, {"id": 794, "seek": 374764, "start": 3753.08, "end": 3755.08, "text": " There's really very little to show", "tokens": [821, 311, 534, 588, 707, 281, 855], "temperature": 0.0, "avg_logprob": -0.23622351752387152, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.5779575051055872e-06}, {"id": 795, "seek": 374764, "start": 3757.0, "end": 3760.48, "text": " Just to make the code a bit shorter", "tokens": [1449, 281, 652, 264, 3089, 257, 857, 11639], "temperature": 0.0, "avg_logprob": -0.23622351752387152, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.5779575051055872e-06}, {"id": 796, "seek": 374764, "start": 3760.48, "end": 3767.7599999999998, "text": " I've defined a function called get embedding which takes a number of inputs and a number of factors", "tokens": [286, 600, 7642, 257, 2445, 1219, 483, 12240, 3584, 597, 2516, 257, 1230, 295, 15743, 293, 257, 1230, 295, 6771], "temperature": 0.0, "avg_logprob": -0.23622351752387152, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.5779575051055872e-06}, {"id": 797, "seek": 374764, "start": 3767.7599999999998, "end": 3770.64, "text": " so the number of rows and the embedding matrix and I'm positive but", "tokens": [370, 264, 1230, 295, 13241, 293, 264, 12240, 3584, 8141, 293, 286, 478, 3353, 457], "temperature": 0.0, "avg_logprob": -0.23622351752387152, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.5779575051055872e-06}, {"id": 798, "seek": 374764, "start": 3771.4, "end": 3773.4, "text": " creates the embedding and", "tokens": [7829, 264, 12240, 3584, 293], "temperature": 0.0, "avg_logprob": -0.23622351752387152, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.5779575051055872e-06}, {"id": 799, "seek": 374764, "start": 3773.6, "end": 3775.0, "text": " then", "tokens": [550], "temperature": 0.0, "avg_logprob": -0.23622351752387152, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.5779575051055872e-06}, {"id": 800, "seek": 377500, "start": 3775.0, "end": 3780.48, "text": " Randomly initializes it. I don't know why I'm doing negative to positive here and zero last time", "tokens": [37603, 356, 5883, 5660, 309, 13, 286, 500, 380, 458, 983, 286, 478, 884, 3671, 281, 3353, 510, 293, 4018, 1036, 565], "temperature": 0.0, "avg_logprob": -0.21463262507345823, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.1233745428617112e-06}, {"id": 801, "seek": 377500, "start": 3780.48, "end": 3783.32, "text": " Honestly, it doesn't matter much as long as it's in the right ballpark", "tokens": [12348, 11, 309, 1177, 380, 1871, 709, 382, 938, 382, 309, 311, 294, 264, 558, 2594, 31239], "temperature": 0.0, "avg_logprob": -0.21463262507345823, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.1233745428617112e-06}, {"id": 802, "seek": 377500, "start": 3784.16, "end": 3786.8, "text": " And then we return that initialized embedding", "tokens": [400, 550, 321, 2736, 300, 5883, 1602, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.21463262507345823, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.1233745428617112e-06}, {"id": 803, "seek": 377500, "start": 3788.0, "end": 3792.68, "text": " So now we need not just our users by factors, which are chuck into you", "tokens": [407, 586, 321, 643, 406, 445, 527, 5022, 538, 6771, 11, 597, 366, 20870, 666, 291], "temperature": 0.0, "avg_logprob": -0.21463262507345823, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.1233745428617112e-06}, {"id": 804, "seek": 377500, "start": 3793.52, "end": 3797.96, "text": " Movies by factors which are chuck into M, but we also need users by one", "tokens": [43756, 530, 538, 6771, 597, 366, 20870, 666, 376, 11, 457, 321, 611, 643, 5022, 538, 472], "temperature": 0.0, "avg_logprob": -0.21463262507345823, "compression_ratio": 1.7509881422924902, "no_speech_prob": 2.1233745428617112e-06}, {"id": 805, "seek": 379796, "start": 3797.96, "end": 3805.2400000000002, "text": " Which will put into you be user bias and movies by one which will put into movie bias, okay", "tokens": [3013, 486, 829, 666, 291, 312, 4195, 12577, 293, 6233, 538, 472, 597, 486, 829, 666, 3169, 12577, 11, 1392], "temperature": 0.0, "avg_logprob": -0.23191184997558595, "compression_ratio": 1.6478260869565218, "no_speech_prob": 1.8162188553105807e-06}, {"id": 806, "seek": 379796, "start": 3805.2400000000002, "end": 3812.0, "text": " So this is just doing a list comprehension going through each of the tuples creating embedding for each of them and putting them into", "tokens": [407, 341, 307, 445, 884, 257, 1329, 44991, 516, 807, 1184, 295, 264, 2604, 2622, 4084, 12240, 3584, 337, 1184, 295, 552, 293, 3372, 552, 666], "temperature": 0.0, "avg_logprob": -0.23191184997558595, "compression_ratio": 1.6478260869565218, "no_speech_prob": 1.8162188553105807e-06}, {"id": 807, "seek": 379796, "start": 3812.0, "end": 3818.04, "text": " These things okay, so now our forward is exactly the same as before", "tokens": [1981, 721, 1392, 11, 370, 586, 527, 2128, 307, 2293, 264, 912, 382, 949], "temperature": 0.0, "avg_logprob": -0.23191184997558595, "compression_ratio": 1.6478260869565218, "no_speech_prob": 1.8162188553105807e-06}, {"id": 808, "seek": 379796, "start": 3820.0, "end": 3822.36, "text": " You times M dot sum", "tokens": [509, 1413, 376, 5893, 2408], "temperature": 0.0, "avg_logprob": -0.23191184997558595, "compression_ratio": 1.6478260869565218, "no_speech_prob": 1.8162188553105807e-06}, {"id": 809, "seek": 379796, "start": 3823.28, "end": 3825.88, "text": " This is actually a little confusing because we're doing it in two", "tokens": [639, 307, 767, 257, 707, 13181, 570, 321, 434, 884, 309, 294, 732], "temperature": 0.0, "avg_logprob": -0.23191184997558595, "compression_ratio": 1.6478260869565218, "no_speech_prob": 1.8162188553105807e-06}, {"id": 810, "seek": 382588, "start": 3825.88, "end": 3827.88, "text": " two steps", "tokens": [732, 4439], "temperature": 0.0, "avg_logprob": -0.2435629511454019, "compression_ratio": 1.495, "no_speech_prob": 4.356858426035615e-06}, {"id": 811, "seek": 382588, "start": 3829.2400000000002, "end": 3833.44, "text": " Maybe to make it a bit easier. Let's pull this out put it up here", "tokens": [2704, 281, 652, 309, 257, 857, 3571, 13, 961, 311, 2235, 341, 484, 829, 309, 493, 510], "temperature": 0.0, "avg_logprob": -0.2435629511454019, "compression_ratio": 1.495, "no_speech_prob": 4.356858426035615e-06}, {"id": 812, "seek": 382588, "start": 3835.0, "end": 3837.0, "text": " Put this in parentheses", "tokens": [4935, 341, 294, 34153], "temperature": 0.0, "avg_logprob": -0.2435629511454019, "compression_ratio": 1.495, "no_speech_prob": 4.356858426035615e-06}, {"id": 813, "seek": 382588, "start": 3838.96, "end": 3840.96, "text": " Okay, so maybe that looks a little bit more familiar", "tokens": [1033, 11, 370, 1310, 300, 1542, 257, 707, 857, 544, 4963], "temperature": 0.0, "avg_logprob": -0.2435629511454019, "compression_ratio": 1.495, "no_speech_prob": 4.356858426035615e-06}, {"id": 814, "seek": 382588, "start": 3841.2400000000002, "end": 3848.76, "text": " All right you times M dot sum that's the same dot product and then here we're just going to add in our user bias and our", "tokens": [1057, 558, 291, 1413, 376, 5893, 2408, 300, 311, 264, 912, 5893, 1674, 293, 550, 510, 321, 434, 445, 516, 281, 909, 294, 527, 4195, 12577, 293, 527], "temperature": 0.0, "avg_logprob": -0.2435629511454019, "compression_ratio": 1.495, "no_speech_prob": 4.356858426035615e-06}, {"id": 815, "seek": 382588, "start": 3849.08, "end": 3851.08, "text": " movie bias", "tokens": [3169, 12577], "temperature": 0.0, "avg_logprob": -0.2435629511454019, "compression_ratio": 1.495, "no_speech_prob": 4.356858426035615e-06}, {"id": 816, "seek": 382588, "start": 3851.4, "end": 3853.28, "text": " Dot squeeze is", "tokens": [38753, 13578, 307], "temperature": 0.0, "avg_logprob": -0.2435629511454019, "compression_ratio": 1.495, "no_speech_prob": 4.356858426035615e-06}, {"id": 817, "seek": 385328, "start": 3853.28, "end": 3859.1200000000003, "text": " The pie torch thing that adds an additional unit axis on", "tokens": [440, 1730, 27822, 551, 300, 10860, 364, 4497, 4985, 10298, 322], "temperature": 0.0, "avg_logprob": -0.20112612133934385, "compression_ratio": 1.6774193548387097, "no_speech_prob": 3.340524472150719e-06}, {"id": 818, "seek": 385328, "start": 3859.84, "end": 3863.0800000000004, "text": " That's not going to make any sense if you haven't done broadcasting before", "tokens": [663, 311, 406, 516, 281, 652, 604, 2020, 498, 291, 2378, 380, 1096, 30024, 949], "temperature": 0.0, "avg_logprob": -0.20112612133934385, "compression_ratio": 1.6774193548387097, "no_speech_prob": 3.340524472150719e-06}, {"id": 819, "seek": 385328, "start": 3863.84, "end": 3870.0, "text": " I'm not going to do broadcasting in this course because we've already done it and we're doing it in the machine learning course", "tokens": [286, 478, 406, 516, 281, 360, 30024, 294, 341, 1164, 570, 321, 600, 1217, 1096, 309, 293, 321, 434, 884, 309, 294, 264, 3479, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.20112612133934385, "compression_ratio": 1.6774193548387097, "no_speech_prob": 3.340524472150719e-06}, {"id": 820, "seek": 385328, "start": 3870.2000000000003, "end": 3872.2000000000003, "text": " But basically in short", "tokens": [583, 1936, 294, 2099], "temperature": 0.0, "avg_logprob": -0.20112612133934385, "compression_ratio": 1.6774193548387097, "no_speech_prob": 3.340524472150719e-06}, {"id": 821, "seek": 385328, "start": 3873.0, "end": 3877.28, "text": " Broadcasting is what happens when you do something like this where UM is a matrix", "tokens": [14074, 48860, 307, 437, 2314, 562, 291, 360, 746, 411, 341, 689, 31335, 307, 257, 8141], "temperature": 0.0, "avg_logprob": -0.20112612133934385, "compression_ratio": 1.6774193548387097, "no_speech_prob": 3.340524472150719e-06}, {"id": 822, "seek": 387728, "start": 3877.28, "end": 3882.2400000000002, "text": " You be self dot you be users is a is a vector", "tokens": [509, 312, 2698, 5893, 291, 312, 5022, 307, 257, 307, 257, 8062], "temperature": 0.0, "avg_logprob": -0.1660604476928711, "compression_ratio": 1.788659793814433, "no_speech_prob": 3.8669617197228945e-07}, {"id": 823, "seek": 387728, "start": 3882.44, "end": 3888.32, "text": " How do you add a vector to a matrix and basically what it does is it duplicates?", "tokens": [1012, 360, 291, 909, 257, 8062, 281, 257, 8141, 293, 1936, 437, 309, 775, 307, 309, 17154, 1024, 30], "temperature": 0.0, "avg_logprob": -0.1660604476928711, "compression_ratio": 1.788659793814433, "no_speech_prob": 3.8669617197228945e-07}, {"id": 824, "seek": 387728, "start": 3889.0800000000004, "end": 3890.44, "text": " the vector", "tokens": [264, 8062], "temperature": 0.0, "avg_logprob": -0.1660604476928711, "compression_ratio": 1.788659793814433, "no_speech_prob": 3.8669617197228945e-07}, {"id": 825, "seek": 387728, "start": 3890.44, "end": 3897.28, "text": " So that it makes it the same size as the matrix and the particular way whether it duplicates it across columns or down rows", "tokens": [407, 300, 309, 1669, 309, 264, 912, 2744, 382, 264, 8141, 293, 264, 1729, 636, 1968, 309, 17154, 1024, 309, 2108, 13766, 420, 760, 13241], "temperature": 0.0, "avg_logprob": -0.1660604476928711, "compression_ratio": 1.788659793814433, "no_speech_prob": 3.8669617197228945e-07}, {"id": 826, "seek": 387728, "start": 3897.28, "end": 3902.7200000000003, "text": " Or how it does it is called broadcasting the broadcasting rules are the same as numpy", "tokens": [1610, 577, 309, 775, 309, 307, 1219, 30024, 264, 30024, 4474, 366, 264, 912, 382, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.1660604476928711, "compression_ratio": 1.788659793814433, "no_speech_prob": 3.8669617197228945e-07}, {"id": 827, "seek": 390272, "start": 3902.72, "end": 3908.48, "text": " I thought it didn't actually used to support broadcasting so I was actually the guy who first added", "tokens": [286, 1194, 309, 994, 380, 767, 1143, 281, 1406, 30024, 370, 286, 390, 767, 264, 2146, 567, 700, 3869], "temperature": 0.0, "avg_logprob": -0.20228323569664589, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.090451744152233e-06}, {"id": 828, "seek": 390272, "start": 3908.64, "end": 3913.9199999999996, "text": " Broadcasting to pie torch using an ugly hack and then the pie torch authors did an awesome job of supporting it", "tokens": [14074, 48860, 281, 1730, 27822, 1228, 364, 12246, 10339, 293, 550, 264, 1730, 27822, 16552, 630, 364, 3476, 1691, 295, 7231, 309], "temperature": 0.0, "avg_logprob": -0.20228323569664589, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.090451744152233e-06}, {"id": 829, "seek": 390272, "start": 3914.48, "end": 3917.9599999999996, "text": " Actually inside the language so now you can use the same", "tokens": [5135, 1854, 264, 2856, 370, 586, 291, 393, 764, 264, 912], "temperature": 0.0, "avg_logprob": -0.20228323569664589, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.090451744152233e-06}, {"id": 830, "seek": 390272, "start": 3918.68, "end": 3926.56, "text": " Broadcasting operations in pie torches numpy if you haven't dealt with this before it's really important to learn it", "tokens": [14074, 48860, 7705, 294, 1730, 3930, 3781, 1031, 8200, 498, 291, 2378, 380, 15991, 365, 341, 949, 309, 311, 534, 1021, 281, 1466, 309], "temperature": 0.0, "avg_logprob": -0.20228323569664589, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.090451744152233e-06}, {"id": 831, "seek": 392656, "start": 3926.56, "end": 3932.2599999999998, "text": " Because like it's it's kind of the most important fundamental way to do computations", "tokens": [1436, 411, 309, 311, 309, 311, 733, 295, 264, 881, 1021, 8088, 636, 281, 360, 2807, 763], "temperature": 0.0, "avg_logprob": -0.190549794365378, "compression_ratio": 1.630952380952381, "no_speech_prob": 6.276686690398492e-07}, {"id": 832, "seek": 392656, "start": 3932.72, "end": 3937.14, "text": " Quickly in numpy and pie torture. It's the thing that lets you not have to do loops", "tokens": [31800, 294, 1031, 8200, 293, 1730, 20711, 13, 467, 311, 264, 551, 300, 6653, 291, 406, 362, 281, 360, 16121], "temperature": 0.0, "avg_logprob": -0.190549794365378, "compression_ratio": 1.630952380952381, "no_speech_prob": 6.276686690398492e-07}, {"id": 833, "seek": 392656, "start": 3937.14, "end": 3942.16, "text": " I could you imagine here if I had to loop through every row of this matrix and add each", "tokens": [286, 727, 291, 3811, 510, 498, 286, 632, 281, 6367, 807, 633, 5386, 295, 341, 8141, 293, 909, 1184], "temperature": 0.0, "avg_logprob": -0.190549794365378, "compression_ratio": 1.630952380952381, "no_speech_prob": 6.276686690398492e-07}, {"id": 834, "seek": 392656, "start": 3942.56, "end": 3947.66, "text": " You know this vector to every row it would be slow. It would be you know a lot more code", "tokens": [509, 458, 341, 8062, 281, 633, 5386, 309, 576, 312, 2964, 13, 467, 576, 312, 291, 458, 257, 688, 544, 3089], "temperature": 0.0, "avg_logprob": -0.190549794365378, "compression_ratio": 1.630952380952381, "no_speech_prob": 6.276686690398492e-07}, {"id": 835, "seek": 392656, "start": 3949.56, "end": 3952.64, "text": " And the idea of broadcasting it actually goes all the way back to", "tokens": [400, 264, 1558, 295, 30024, 309, 767, 1709, 439, 264, 636, 646, 281], "temperature": 0.0, "avg_logprob": -0.190549794365378, "compression_ratio": 1.630952380952381, "no_speech_prob": 6.276686690398492e-07}, {"id": 836, "seek": 395264, "start": 3952.64, "end": 3958.14, "text": " APL which was a language designed in the 50s by an extraordinary guy called Ken Iverson", "tokens": [5372, 43, 597, 390, 257, 2856, 4761, 294, 264, 2625, 82, 538, 364, 10581, 2146, 1219, 8273, 286, 840, 266], "temperature": 0.0, "avg_logprob": -0.18515146610348723, "compression_ratio": 1.6508620689655173, "no_speech_prob": 5.5075588534236886e-06}, {"id": 837, "seek": 395264, "start": 3959.08, "end": 3966.64, "text": " APL was originally a designed or written out as a new type of mathematical notation. He has this great essay called", "tokens": [5372, 43, 390, 7993, 257, 4761, 420, 3720, 484, 382, 257, 777, 2010, 295, 18894, 24657, 13, 634, 575, 341, 869, 16238, 1219], "temperature": 0.0, "avg_logprob": -0.18515146610348723, "compression_ratio": 1.6508620689655173, "no_speech_prob": 5.5075588534236886e-06}, {"id": 838, "seek": 395264, "start": 3967.48, "end": 3972.72, "text": " notation as a tool for thought and the idea was that like really good notation could actually make you", "tokens": [24657, 382, 257, 2290, 337, 1194, 293, 264, 1558, 390, 300, 411, 534, 665, 24657, 727, 767, 652, 291], "temperature": 0.0, "avg_logprob": -0.18515146610348723, "compression_ratio": 1.6508620689655173, "no_speech_prob": 5.5075588534236886e-06}, {"id": 839, "seek": 395264, "start": 3973.16, "end": 3978.3199999999997, "text": " Think of better things and part of that notation is this idea of forecasting", "tokens": [6557, 295, 1101, 721, 293, 644, 295, 300, 24657, 307, 341, 1558, 295, 44331], "temperature": 0.0, "avg_logprob": -0.18515146610348723, "compression_ratio": 1.6508620689655173, "no_speech_prob": 5.5075588534236886e-06}, {"id": 840, "seek": 397832, "start": 3978.32, "end": 3982.84, "text": " I'm incredibly enthusiastic about it, and we're going to use it plenty", "tokens": [286, 478, 6252, 28574, 466, 309, 11, 293, 321, 434, 516, 281, 764, 309, 7140], "temperature": 0.0, "avg_logprob": -0.23141012057452134, "compression_ratio": 1.461139896373057, "no_speech_prob": 1.8448174614604795e-06}, {"id": 841, "seek": 397832, "start": 3983.6000000000004, "end": 3985.1200000000003, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.23141012057452134, "compression_ratio": 1.461139896373057, "no_speech_prob": 1.8448174614604795e-06}, {"id": 842, "seek": 397832, "start": 3985.1200000000003, "end": 3989.4, "text": " either watch the machine learning lesson or", "tokens": [2139, 1159, 264, 3479, 2539, 6898, 420], "temperature": 0.0, "avg_logprob": -0.23141012057452134, "compression_ratio": 1.461139896373057, "no_speech_prob": 1.8448174614604795e-06}, {"id": 843, "seek": 397832, "start": 3990.4, "end": 3994.42, "text": " You know Google numpy broadcasting for information", "tokens": [509, 458, 3329, 1031, 8200, 30024, 337, 1589], "temperature": 0.0, "avg_logprob": -0.23141012057452134, "compression_ratio": 1.461139896373057, "no_speech_prob": 1.8448174614604795e-06}, {"id": 844, "seek": 397832, "start": 3995.56, "end": 4000.84, "text": " Anyway, so basically it works reasonably intuitively we can add on we can add the", "tokens": [5684, 11, 370, 1936, 309, 1985, 23551, 46506, 321, 393, 909, 322, 321, 393, 909, 264], "temperature": 0.0, "avg_logprob": -0.23141012057452134, "compression_ratio": 1.461139896373057, "no_speech_prob": 1.8448174614604795e-06}, {"id": 845, "seek": 397832, "start": 4001.6000000000004, "end": 4003.6000000000004, "text": " Vectors to the matrix", "tokens": [691, 557, 830, 281, 264, 8141], "temperature": 0.0, "avg_logprob": -0.23141012057452134, "compression_ratio": 1.461139896373057, "no_speech_prob": 1.8448174614604795e-06}, {"id": 846, "seek": 397832, "start": 4005.6000000000004, "end": 4007.4, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.23141012057452134, "compression_ratio": 1.461139896373057, "no_speech_prob": 1.8448174614604795e-06}, {"id": 847, "seek": 400740, "start": 4007.4, "end": 4015.86, "text": " Having done that we're now going to do one more trick which is I think it was your net asked earlier about could we", "tokens": [10222, 1096, 300, 321, 434, 586, 516, 281, 360, 472, 544, 4282, 597, 307, 286, 519, 309, 390, 428, 2533, 2351, 3071, 466, 727, 321], "temperature": 0.0, "avg_logprob": -0.21810832088940765, "compression_ratio": 1.4861878453038675, "no_speech_prob": 1.5779560271766968e-06}, {"id": 848, "seek": 400740, "start": 4016.7200000000003, "end": 4020.2400000000002, "text": " Squish the ratings to be between 1 and 5", "tokens": [8683, 742, 264, 24603, 281, 312, 1296, 502, 293, 1025], "temperature": 0.0, "avg_logprob": -0.21810832088940765, "compression_ratio": 1.4861878453038675, "no_speech_prob": 1.5779560271766968e-06}, {"id": 849, "seek": 400740, "start": 4021.52, "end": 4023.4, "text": " and the answer is", "tokens": [293, 264, 1867, 307], "temperature": 0.0, "avg_logprob": -0.21810832088940765, "compression_ratio": 1.4861878453038675, "no_speech_prob": 1.5779560271766968e-06}, {"id": 850, "seek": 400740, "start": 4023.4, "end": 4027.7200000000003, "text": " We could right and specifically what we could do is", "tokens": [492, 727, 558, 293, 4682, 437, 321, 727, 360, 307], "temperature": 0.0, "avg_logprob": -0.21810832088940765, "compression_ratio": 1.4861878453038675, "no_speech_prob": 1.5779560271766968e-06}, {"id": 851, "seek": 400740, "start": 4028.6, "end": 4030.6, "text": " We could", "tokens": [492, 727], "temperature": 0.0, "avg_logprob": -0.21810832088940765, "compression_ratio": 1.4861878453038675, "no_speech_prob": 1.5779560271766968e-06}, {"id": 852, "seek": 400740, "start": 4030.6, "end": 4032.7200000000003, "text": " Put it through a sigmoid function", "tokens": [4935, 309, 807, 257, 4556, 3280, 327, 2445], "temperature": 0.0, "avg_logprob": -0.21810832088940765, "compression_ratio": 1.4861878453038675, "no_speech_prob": 1.5779560271766968e-06}, {"id": 853, "seek": 403272, "start": 4032.72, "end": 4037.04, "text": " All right, so to remind you a sigmoid function", "tokens": [1057, 558, 11, 370, 281, 4160, 291, 257, 4556, 3280, 327, 2445], "temperature": 0.0, "avg_logprob": -0.21605796399323837, "compression_ratio": 1.8085106382978724, "no_speech_prob": 1.0348508112656418e-06}, {"id": 854, "seek": 403272, "start": 4038.8399999999997, "end": 4044.2, "text": " Looks like that right and this is that's one", "tokens": [10027, 411, 300, 558, 293, 341, 307, 300, 311, 472], "temperature": 0.0, "avg_logprob": -0.21605796399323837, "compression_ratio": 1.8085106382978724, "no_speech_prob": 1.0348508112656418e-06}, {"id": 855, "seek": 403272, "start": 4045.4399999999996, "end": 4048.08, "text": " Right we could put it through a sigmoid function", "tokens": [1779, 321, 727, 829, 309, 807, 257, 4556, 3280, 327, 2445], "temperature": 0.0, "avg_logprob": -0.21605796399323837, "compression_ratio": 1.8085106382978724, "no_speech_prob": 1.0348508112656418e-06}, {"id": 856, "seek": 403272, "start": 4048.08, "end": 4053.2799999999997, "text": " So we could take like four point nine six and put it through a sigmoid function and like that", "tokens": [407, 321, 727, 747, 411, 1451, 935, 4949, 2309, 293, 829, 309, 807, 257, 4556, 3280, 327, 2445, 293, 411, 300], "temperature": 0.0, "avg_logprob": -0.21605796399323837, "compression_ratio": 1.8085106382978724, "no_speech_prob": 1.0348508112656418e-06}, {"id": 857, "seek": 403272, "start": 4053.2799999999997, "end": 4056.72, "text": " You know that's kind of high so it kind of be over here somewhere, right?", "tokens": [509, 458, 300, 311, 733, 295, 1090, 370, 309, 733, 295, 312, 670, 510, 4079, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21605796399323837, "compression_ratio": 1.8085106382978724, "no_speech_prob": 1.0348508112656418e-06}, {"id": 858, "seek": 403272, "start": 4057.12, "end": 4059.68, "text": " And then we could multiply that", "tokens": [400, 550, 321, 727, 12972, 300], "temperature": 0.0, "avg_logprob": -0.21605796399323837, "compression_ratio": 1.8085106382978724, "no_speech_prob": 1.0348508112656418e-06}, {"id": 859, "seek": 405968, "start": 4059.68, "end": 4062.56, "text": " sigmoid like the result of that by five", "tokens": [4556, 3280, 327, 411, 264, 1874, 295, 300, 538, 1732], "temperature": 0.0, "avg_logprob": -0.2441596214217369, "compression_ratio": 1.6724890829694323, "no_speech_prob": 1.7061781818483723e-06}, {"id": 860, "seek": 405968, "start": 4064.3199999999997, "end": 4070.72, "text": " For example right and in this case we want it to be between 1 and 5 right so maybe we might multiply it by 4", "tokens": [1171, 1365, 558, 293, 294, 341, 1389, 321, 528, 309, 281, 312, 1296, 502, 293, 1025, 558, 370, 1310, 321, 1062, 12972, 309, 538, 1017], "temperature": 0.0, "avg_logprob": -0.2441596214217369, "compression_ratio": 1.6724890829694323, "no_speech_prob": 1.7061781818483723e-06}, {"id": 861, "seek": 405968, "start": 4070.72, "end": 4074.0, "text": " And add 1 instance. That's a basic idea", "tokens": [400, 909, 502, 5197, 13, 663, 311, 257, 3875, 1558], "temperature": 0.0, "avg_logprob": -0.2441596214217369, "compression_ratio": 1.6724890829694323, "no_speech_prob": 1.7061781818483723e-06}, {"id": 862, "seek": 405968, "start": 4074.7599999999998, "end": 4076.0, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2441596214217369, "compression_ratio": 1.6724890829694323, "no_speech_prob": 1.7061781818483723e-06}, {"id": 863, "seek": 405968, "start": 4076.0, "end": 4082.7599999999998, "text": " So here is that trick we take the result so the result is basically the thing that comes straight out of the", "tokens": [407, 510, 307, 300, 4282, 321, 747, 264, 1874, 370, 264, 1874, 307, 1936, 264, 551, 300, 1487, 2997, 484, 295, 264], "temperature": 0.0, "avg_logprob": -0.2441596214217369, "compression_ratio": 1.6724890829694323, "no_speech_prob": 1.7061781818483723e-06}, {"id": 864, "seek": 405968, "start": 4083.44, "end": 4088.72, "text": " Dot product plus the addition of the biases and put it through a sigmoid function", "tokens": [38753, 1674, 1804, 264, 4500, 295, 264, 32152, 293, 829, 309, 807, 257, 4556, 3280, 327, 2445], "temperature": 0.0, "avg_logprob": -0.2441596214217369, "compression_ratio": 1.6724890829694323, "no_speech_prob": 1.7061781818483723e-06}, {"id": 865, "seek": 408872, "start": 4088.72, "end": 4090.72, "text": " now in pytorch", "tokens": [586, 294, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.2720892735016652, "compression_ratio": 1.7513812154696133, "no_speech_prob": 2.521566557334154e-06}, {"id": 866, "seek": 408872, "start": 4092.7599999999998, "end": 4097.28, "text": " Basically all of the functions you can do the tensors are available inside", "tokens": [8537, 439, 295, 264, 6828, 291, 393, 360, 264, 10688, 830, 366, 2435, 1854], "temperature": 0.0, "avg_logprob": -0.2720892735016652, "compression_ratio": 1.7513812154696133, "no_speech_prob": 2.521566557334154e-06}, {"id": 867, "seek": 408872, "start": 4098.2, "end": 4102.5599999999995, "text": " This thing called capital F, and this is like totally standard in pytorch", "tokens": [639, 551, 1219, 4238, 479, 11, 293, 341, 307, 411, 3879, 3832, 294, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.2720892735016652, "compression_ratio": 1.7513812154696133, "no_speech_prob": 2.521566557334154e-06}, {"id": 868, "seek": 408872, "start": 4103.08, "end": 4108.8, "text": " It's actually called torch dot and end up functional, but everybody including all of the pytorch docs", "tokens": [467, 311, 767, 1219, 27822, 5893, 293, 917, 493, 11745, 11, 457, 2201, 3009, 439, 295, 264, 25878, 284, 339, 45623], "temperature": 0.0, "avg_logprob": -0.2720892735016652, "compression_ratio": 1.7513812154696133, "no_speech_prob": 2.521566557334154e-06}, {"id": 869, "seek": 408872, "start": 4109.32, "end": 4112.24, "text": " Import torch dot and end up functional as capital F", "tokens": [26391, 27822, 5893, 293, 917, 493, 11745, 382, 4238, 479], "temperature": 0.0, "avg_logprob": -0.2720892735016652, "compression_ratio": 1.7513812154696133, "no_speech_prob": 2.521566557334154e-06}, {"id": 870, "seek": 411224, "start": 4112.24, "end": 4118.599999999999, "text": " All right, so capital F dot sigmoid means a function called sigmoid that is coming from", "tokens": [1057, 558, 11, 370, 4238, 479, 5893, 4556, 3280, 327, 1355, 257, 2445, 1219, 4556, 3280, 327, 300, 307, 1348, 490], "temperature": 0.0, "avg_logprob": -0.1869317971023859, "compression_ratio": 1.6858407079646018, "no_speech_prob": 5.122892616782337e-07}, {"id": 871, "seek": 411224, "start": 4119.24, "end": 4120.599999999999, "text": " torches", "tokens": [3930, 3781], "temperature": 0.0, "avg_logprob": -0.1869317971023859, "compression_ratio": 1.6858407079646018, "no_speech_prob": 5.122892616782337e-07}, {"id": 872, "seek": 411224, "start": 4120.599999999999, "end": 4125.5599999999995, "text": " Functional module right and so that's going to apply a sigmoid function to the result", "tokens": [11166, 41048, 10088, 558, 293, 370, 300, 311, 516, 281, 3079, 257, 4556, 3280, 327, 2445, 281, 264, 1874], "temperature": 0.0, "avg_logprob": -0.1869317971023859, "compression_ratio": 1.6858407079646018, "no_speech_prob": 5.122892616782337e-07}, {"id": 873, "seek": 411224, "start": 4125.599999999999, "end": 4131.0, "text": " So squish them all between 0 and 1 using that nice little shape and then I can multiply that by", "tokens": [407, 31379, 552, 439, 1296, 1958, 293, 502, 1228, 300, 1481, 707, 3909, 293, 550, 286, 393, 12972, 300, 538], "temperature": 0.0, "avg_logprob": -0.1869317971023859, "compression_ratio": 1.6858407079646018, "no_speech_prob": 5.122892616782337e-07}, {"id": 874, "seek": 411224, "start": 4132.08, "end": 4134.08, "text": " 5 minus 1 plus 4", "tokens": [1025, 3175, 502, 1804, 1017], "temperature": 0.0, "avg_logprob": -0.1869317971023859, "compression_ratio": 1.6858407079646018, "no_speech_prob": 5.122892616782337e-07}, {"id": 875, "seek": 411224, "start": 4134.48, "end": 4139.599999999999, "text": " Right and then add on 1 and that's going to give me something between 1 and 5 okay, so", "tokens": [1779, 293, 550, 909, 322, 502, 293, 300, 311, 516, 281, 976, 385, 746, 1296, 502, 293, 1025, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.1869317971023859, "compression_ratio": 1.6858407079646018, "no_speech_prob": 5.122892616782337e-07}, {"id": 876, "seek": 413960, "start": 4139.6, "end": 4145.0, "text": " Like there's no need to do this I could comment it out, and it will still work", "tokens": [1743, 456, 311, 572, 643, 281, 360, 341, 286, 727, 2871, 309, 484, 11, 293, 309, 486, 920, 589], "temperature": 0.0, "avg_logprob": -0.18730653127034505, "compression_ratio": 1.7338129496402879, "no_speech_prob": 5.896412744732515e-07}, {"id": 877, "seek": 413960, "start": 4145.64, "end": 4150.6, "text": " Right but now it has to come up with a set of calculations that are always between", "tokens": [1779, 457, 586, 309, 575, 281, 808, 493, 365, 257, 992, 295, 20448, 300, 366, 1009, 1296], "temperature": 0.0, "avg_logprob": -0.18730653127034505, "compression_ratio": 1.7338129496402879, "no_speech_prob": 5.896412744732515e-07}, {"id": 878, "seek": 413960, "start": 4151.160000000001, "end": 4156.0, "text": " 1 and 5 right where else if I leave this in then it's like makes it really easy", "tokens": [502, 293, 1025, 558, 689, 1646, 498, 286, 1856, 341, 294, 550, 309, 311, 411, 1669, 309, 534, 1858], "temperature": 0.0, "avg_logprob": -0.18730653127034505, "compression_ratio": 1.7338129496402879, "no_speech_prob": 5.896412744732515e-07}, {"id": 879, "seek": 413960, "start": 4156.0, "end": 4160.4800000000005, "text": " It's basically like oh if you think this is a really good movie just calculate a really high number", "tokens": [467, 311, 1936, 411, 1954, 498, 291, 519, 341, 307, 257, 534, 665, 3169, 445, 8873, 257, 534, 1090, 1230], "temperature": 0.0, "avg_logprob": -0.18730653127034505, "compression_ratio": 1.7338129496402879, "no_speech_prob": 5.896412744732515e-07}, {"id": 880, "seek": 413960, "start": 4160.4800000000005, "end": 4165.160000000001, "text": " It's a really crappy movie capital really low number, and I'll make sure it's in the right region", "tokens": [467, 311, 257, 534, 36531, 3169, 4238, 534, 2295, 1230, 11, 293, 286, 603, 652, 988, 309, 311, 294, 264, 558, 4458], "temperature": 0.0, "avg_logprob": -0.18730653127034505, "compression_ratio": 1.7338129496402879, "no_speech_prob": 5.896412744732515e-07}, {"id": 881, "seek": 413960, "start": 4165.160000000001, "end": 4167.160000000001, "text": " So even though this isn't a neural network", "tokens": [407, 754, 1673, 341, 1943, 380, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.18730653127034505, "compression_ratio": 1.7338129496402879, "no_speech_prob": 5.896412744732515e-07}, {"id": 882, "seek": 416716, "start": 4167.16, "end": 4172.0, "text": " It's still a good example of this kind of like if you're doing any kind of parameter fitting", "tokens": [467, 311, 920, 257, 665, 1365, 295, 341, 733, 295, 411, 498, 291, 434, 884, 604, 733, 295, 13075, 15669], "temperature": 0.0, "avg_logprob": -0.1599678602374968, "compression_ratio": 1.728937728937729, "no_speech_prob": 2.5215679215762066e-06}, {"id": 883, "seek": 416716, "start": 4172.36, "end": 4176.26, "text": " Try and make it so that the thing that you want your function to return", "tokens": [6526, 293, 652, 309, 370, 300, 264, 551, 300, 291, 528, 428, 2445, 281, 2736], "temperature": 0.0, "avg_logprob": -0.1599678602374968, "compression_ratio": 1.728937728937729, "no_speech_prob": 2.5215679215762066e-06}, {"id": 884, "seek": 416716, "start": 4176.26, "end": 4182.34, "text": " It's like it's easy for it to return that okay, so that's why we do that that function squishing", "tokens": [467, 311, 411, 309, 311, 1858, 337, 309, 281, 2736, 300, 1392, 11, 370, 300, 311, 983, 321, 360, 300, 300, 2445, 2339, 3807], "temperature": 0.0, "avg_logprob": -0.1599678602374968, "compression_ratio": 1.728937728937729, "no_speech_prob": 2.5215679215762066e-06}, {"id": 885, "seek": 416716, "start": 4184.08, "end": 4186.24, "text": " So we call this embedding dot bias", "tokens": [407, 321, 818, 341, 12240, 3584, 5893, 12577], "temperature": 0.0, "avg_logprob": -0.1599678602374968, "compression_ratio": 1.728937728937729, "no_speech_prob": 2.5215679215762066e-06}, {"id": 886, "seek": 416716, "start": 4187.32, "end": 4189.5199999999995, "text": " So we can create that in the same way as before", "tokens": [407, 321, 393, 1884, 300, 294, 264, 912, 636, 382, 949], "temperature": 0.0, "avg_logprob": -0.1599678602374968, "compression_ratio": 1.728937728937729, "no_speech_prob": 2.5215679215762066e-06}, {"id": 887, "seek": 416716, "start": 4189.88, "end": 4191.0, "text": " You'll see here", "tokens": [509, 603, 536, 510], "temperature": 0.0, "avg_logprob": -0.1599678602374968, "compression_ratio": 1.728937728937729, "no_speech_prob": 2.5215679215762066e-06}, {"id": 888, "seek": 419100, "start": 4191.0, "end": 4197.44, "text": " I'm calling dot CUDA to put it on the GPU because we're not using any learner stuff normally that'll all happen for you", "tokens": [286, 478, 5141, 5893, 29777, 7509, 281, 829, 309, 322, 264, 18407, 570, 321, 434, 406, 1228, 604, 33347, 1507, 5646, 300, 603, 439, 1051, 337, 291], "temperature": 0.0, "avg_logprob": -0.21045861550427358, "compression_ratio": 1.7192307692307693, "no_speech_prob": 2.7693961328623118e-06}, {"id": 889, "seek": 419100, "start": 4197.44, "end": 4199.8, "text": " But we have to manually say put it on the GPU", "tokens": [583, 321, 362, 281, 16945, 584, 829, 309, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.21045861550427358, "compression_ratio": 1.7192307692307693, "no_speech_prob": 2.7693961328623118e-06}, {"id": 890, "seek": 419100, "start": 4200.44, "end": 4202.68, "text": " This is the same as before create our optimizer", "tokens": [639, 307, 264, 912, 382, 949, 1884, 527, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.21045861550427358, "compression_ratio": 1.7192307692307693, "no_speech_prob": 2.7693961328623118e-06}, {"id": 891, "seek": 419100, "start": 4203.16, "end": 4209.76, "text": " Fit exactly the same as before and these numbers are looking good alright and again. We'll do a little", "tokens": [29263, 2293, 264, 912, 382, 949, 293, 613, 3547, 366, 1237, 665, 5845, 293, 797, 13, 492, 603, 360, 257, 707], "temperature": 0.0, "avg_logprob": -0.21045861550427358, "compression_ratio": 1.7192307692307693, "no_speech_prob": 2.7693961328623118e-06}, {"id": 892, "seek": 419100, "start": 4210.76, "end": 4217.28, "text": " Change to our learning rate and learning rate schedule, and we're down to point eight, so we're actually pretty close", "tokens": [15060, 281, 527, 2539, 3314, 293, 2539, 3314, 7567, 11, 293, 321, 434, 760, 281, 935, 3180, 11, 370, 321, 434, 767, 1238, 1998], "temperature": 0.0, "avg_logprob": -0.21045861550427358, "compression_ratio": 1.7192307692307693, "no_speech_prob": 2.7693961328623118e-06}, {"id": 893, "seek": 419100, "start": 4218.52, "end": 4220.52, "text": " pretty close", "tokens": [1238, 1998], "temperature": 0.0, "avg_logprob": -0.21045861550427358, "compression_ratio": 1.7192307692307693, "no_speech_prob": 2.7693961328623118e-06}, {"id": 894, "seek": 422052, "start": 4220.52, "end": 4222.52, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.3873650687081473, "compression_ratio": 1.4307692307692308, "no_speech_prob": 7.112425919331145e-07}, {"id": 895, "seek": 422052, "start": 4222.56, "end": 4224.92, "text": " So that's the key steps", "tokens": [407, 300, 311, 264, 2141, 4439], "temperature": 0.0, "avg_logprob": -0.3873650687081473, "compression_ratio": 1.4307692307692308, "no_speech_prob": 7.112425919331145e-07}, {"id": 896, "seek": 422052, "start": 4226.200000000001, "end": 4228.080000000001, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.3873650687081473, "compression_ratio": 1.4307692307692308, "no_speech_prob": 7.112425919331145e-07}, {"id": 897, "seek": 422052, "start": 4228.080000000001, "end": 4230.080000000001, "text": " This is how", "tokens": [639, 307, 577], "temperature": 0.0, "avg_logprob": -0.3873650687081473, "compression_ratio": 1.4307692307692308, "no_speech_prob": 7.112425919331145e-07}, {"id": 898, "seek": 422052, "start": 4230.76, "end": 4232.76, "text": " This is how most", "tokens": [639, 307, 577, 881], "temperature": 0.0, "avg_logprob": -0.3873650687081473, "compression_ratio": 1.4307692307692308, "no_speech_prob": 7.112425919331145e-07}, {"id": 899, "seek": 422052, "start": 4233.4400000000005, "end": 4235.4400000000005, "text": " collaborative filtering is done and", "tokens": [16555, 30822, 307, 1096, 293], "temperature": 0.0, "avg_logprob": -0.3873650687081473, "compression_ratio": 1.4307692307692308, "no_speech_prob": 7.112425919331145e-07}, {"id": 900, "seek": 422052, "start": 4236.320000000001, "end": 4240.88, "text": " You're not reminded me of an important point which is that this is not", "tokens": [509, 434, 406, 15920, 385, 295, 364, 1021, 935, 597, 307, 300, 341, 307, 406], "temperature": 0.0, "avg_logprob": -0.3873650687081473, "compression_ratio": 1.4307692307692308, "no_speech_prob": 7.112425919331145e-07}, {"id": 901, "seek": 422052, "start": 4242.120000000001, "end": 4244.120000000001, "text": " strictly speaking a", "tokens": [20792, 4124, 257], "temperature": 0.0, "avg_logprob": -0.3873650687081473, "compression_ratio": 1.4307692307692308, "no_speech_prob": 7.112425919331145e-07}, {"id": 902, "seek": 424412, "start": 4244.12, "end": 4251.44, "text": " Matrix factorization because strictly speaking a matrix factorization would take that matrix by that matrix", "tokens": [36274, 5952, 2144, 570, 20792, 4124, 257, 8141, 5952, 2144, 576, 747, 300, 8141, 538, 300, 8141], "temperature": 0.0, "avg_logprob": -0.19293684329626695, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.3925413213655702e-06}, {"id": 903, "seek": 424412, "start": 4251.96, "end": 4253.599999999999, "text": " to create", "tokens": [281, 1884], "temperature": 0.0, "avg_logprob": -0.19293684329626695, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.3925413213655702e-06}, {"id": 904, "seek": 424412, "start": 4253.599999999999, "end": 4255.599999999999, "text": " this matrix and", "tokens": [341, 8141, 293], "temperature": 0.0, "avg_logprob": -0.19293684329626695, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.3925413213655702e-06}, {"id": 905, "seek": 424412, "start": 4256.599999999999, "end": 4258.599999999999, "text": " remembering", "tokens": [20719], "temperature": 0.0, "avg_logprob": -0.19293684329626695, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.3925413213655702e-06}, {"id": 906, "seek": 424412, "start": 4262.4, "end": 4266.64, "text": " Anywhere that this is empty like here or here", "tokens": [2639, 1992, 300, 341, 307, 6707, 411, 510, 420, 510], "temperature": 0.0, "avg_logprob": -0.19293684329626695, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.3925413213655702e-06}, {"id": 907, "seek": 424412, "start": 4268.12, "end": 4270.12, "text": " We're putting in a zero", "tokens": [492, 434, 3372, 294, 257, 4018], "temperature": 0.0, "avg_logprob": -0.19293684329626695, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.3925413213655702e-06}, {"id": 908, "seek": 427012, "start": 4270.12, "end": 4274.4, "text": " Right we're saying if the original was empty put in a zero", "tokens": [1779, 321, 434, 1566, 498, 264, 3380, 390, 6707, 829, 294, 257, 4018], "temperature": 0.0, "avg_logprob": -0.18605371475219726, "compression_ratio": 1.755813953488372, "no_speech_prob": 9.422423659088963e-07}, {"id": 909, "seek": 427012, "start": 4274.92, "end": 4276.92, "text": " Right now normally", "tokens": [1779, 586, 5646], "temperature": 0.0, "avg_logprob": -0.18605371475219726, "compression_ratio": 1.755813953488372, "no_speech_prob": 9.422423659088963e-07}, {"id": 910, "seek": 427012, "start": 4276.92, "end": 4282.48, "text": " You can't do that with normal matrix factorization with normal matrix factorization it creates the whole", "tokens": [509, 393, 380, 360, 300, 365, 2710, 8141, 5952, 2144, 365, 2710, 8141, 5952, 2144, 309, 7829, 264, 1379], "temperature": 0.0, "avg_logprob": -0.18605371475219726, "compression_ratio": 1.755813953488372, "no_speech_prob": 9.422423659088963e-07}, {"id": 911, "seek": 427012, "start": 4282.96, "end": 4285.68, "text": " Matrix and so it was a real problem actually", "tokens": [36274, 293, 370, 309, 390, 257, 957, 1154, 767], "temperature": 0.0, "avg_logprob": -0.18605371475219726, "compression_ratio": 1.755813953488372, "no_speech_prob": 9.422423659088963e-07}, {"id": 912, "seek": 427012, "start": 4286.12, "end": 4293.42, "text": " When people used to try and use traditional linear algebra for this because when you have these sparse matrices like in practice", "tokens": [1133, 561, 1143, 281, 853, 293, 764, 5164, 8213, 21989, 337, 341, 570, 562, 291, 362, 613, 637, 11668, 32284, 411, 294, 3124], "temperature": 0.0, "avg_logprob": -0.18605371475219726, "compression_ratio": 1.755813953488372, "no_speech_prob": 9.422423659088963e-07}, {"id": 913, "seek": 427012, "start": 4293.72, "end": 4299.58, "text": " This matrix is not doesn't have many gaps because we picked the users that watch the most movies", "tokens": [639, 8141, 307, 406, 1177, 380, 362, 867, 15031, 570, 321, 6183, 264, 5022, 300, 1159, 264, 881, 6233], "temperature": 0.0, "avg_logprob": -0.18605371475219726, "compression_ratio": 1.755813953488372, "no_speech_prob": 9.422423659088963e-07}, {"id": 914, "seek": 429958, "start": 4299.58, "end": 4303.12, "text": " And the movies that are the most watched but if you look at the whole matrix", "tokens": [400, 264, 6233, 300, 366, 264, 881, 6337, 457, 498, 291, 574, 412, 264, 1379, 8141], "temperature": 0.0, "avg_logprob": -0.174382568687521, "compression_ratio": 1.6958333333333333, "no_speech_prob": 3.1875567856332054e-06}, {"id": 915, "seek": 429958, "start": 4303.12, "end": 4309.04, "text": " It's it's mainly empty and so traditional techniques treated empty as zero", "tokens": [467, 311, 309, 311, 8704, 6707, 293, 370, 5164, 7512, 8668, 6707, 382, 4018], "temperature": 0.0, "avg_logprob": -0.174382568687521, "compression_ratio": 1.6958333333333333, "no_speech_prob": 3.1875567856332054e-06}, {"id": 916, "seek": 429958, "start": 4309.04, "end": 4315.08, "text": " And so like you basically have to predict a zero as if the fact that I haven't watched a movie means", "tokens": [400, 370, 411, 291, 1936, 362, 281, 6069, 257, 4018, 382, 498, 264, 1186, 300, 286, 2378, 380, 6337, 257, 3169, 1355], "temperature": 0.0, "avg_logprob": -0.174382568687521, "compression_ratio": 1.6958333333333333, "no_speech_prob": 3.1875567856332054e-06}, {"id": 917, "seek": 429958, "start": 4315.08, "end": 4317.76, "text": " I don't like the movie that gives terrible answers", "tokens": [286, 500, 380, 411, 264, 3169, 300, 2709, 6237, 6338], "temperature": 0.0, "avg_logprob": -0.174382568687521, "compression_ratio": 1.6958333333333333, "no_speech_prob": 3.1875567856332054e-06}, {"id": 918, "seek": 429958, "start": 4318.48, "end": 4322.28, "text": " So this probabilistic matrix factorization approach", "tokens": [407, 341, 31959, 3142, 8141, 5952, 2144, 3109], "temperature": 0.0, "avg_logprob": -0.174382568687521, "compression_ratio": 1.6958333333333333, "no_speech_prob": 3.1875567856332054e-06}, {"id": 919, "seek": 429958, "start": 4324.28, "end": 4326.98, "text": " Takes advantage of the fact that our data structure", "tokens": [44347, 5002, 295, 264, 1186, 300, 527, 1412, 3877], "temperature": 0.0, "avg_logprob": -0.174382568687521, "compression_ratio": 1.6958333333333333, "no_speech_prob": 3.1875567856332054e-06}, {"id": 920, "seek": 432698, "start": 4326.98, "end": 4328.98, "text": " Actually looks like this", "tokens": [5135, 1542, 411, 341], "temperature": 0.0, "avg_logprob": -0.19095953773049748, "compression_ratio": 1.8347457627118644, "no_speech_prob": 1.328773464592814e-06}, {"id": 921, "seek": 432698, "start": 4329.74, "end": 4337.74, "text": " Rather than that crosstab right and so it's only calculating the loss for the user ID movie ID combinations that actually appear", "tokens": [16571, 813, 300, 28108, 372, 455, 558, 293, 370, 309, 311, 787, 28258, 264, 4470, 337, 264, 4195, 7348, 3169, 7348, 21267, 300, 767, 4204], "temperature": 0.0, "avg_logprob": -0.19095953773049748, "compression_ratio": 1.8347457627118644, "no_speech_prob": 1.328773464592814e-06}, {"id": 922, "seek": 432698, "start": 4337.74, "end": 4341.9, "text": " That's it's actually like user ID one movie ID one or two nine should be three", "tokens": [663, 311, 309, 311, 767, 411, 4195, 7348, 472, 3169, 7348, 472, 420, 732, 4949, 820, 312, 1045], "temperature": 0.0, "avg_logprob": -0.19095953773049748, "compression_ratio": 1.8347457627118644, "no_speech_prob": 1.328773464592814e-06}, {"id": 923, "seek": 432698, "start": 4341.9, "end": 4348.259999999999, "text": " It's actually three and a half so our loss is point five like there's nothing here. That's ever going to calculate a", "tokens": [467, 311, 767, 1045, 293, 257, 1922, 370, 527, 4470, 307, 935, 1732, 411, 456, 311, 1825, 510, 13, 663, 311, 1562, 516, 281, 8873, 257], "temperature": 0.0, "avg_logprob": -0.19095953773049748, "compression_ratio": 1.8347457627118644, "no_speech_prob": 1.328773464592814e-06}, {"id": 924, "seek": 432698, "start": 4349.299999999999, "end": 4353.879999999999, "text": " Prediction or a loss for a user movie combination that doesn't appear in this table", "tokens": [32969, 4105, 420, 257, 4470, 337, 257, 4195, 3169, 6562, 300, 1177, 380, 4204, 294, 341, 3199], "temperature": 0.0, "avg_logprob": -0.19095953773049748, "compression_ratio": 1.8347457627118644, "no_speech_prob": 1.328773464592814e-06}, {"id": 925, "seek": 435388, "start": 4353.88, "end": 4359.76, "text": " But by definition the only stuff that we can appear in a mini batch is what's in this table", "tokens": [583, 538, 7123, 264, 787, 1507, 300, 321, 393, 4204, 294, 257, 8382, 15245, 307, 437, 311, 294, 341, 3199], "temperature": 0.0, "avg_logprob": -0.2185043790447178, "compression_ratio": 1.5628140703517588, "no_speech_prob": 3.989712809016055e-07}, {"id": 926, "seek": 435388, "start": 4364.84, "end": 4368.68, "text": " And like a lot of this happened interestingly enough actually in the Netflix prize", "tokens": [400, 411, 257, 688, 295, 341, 2011, 25873, 1547, 767, 294, 264, 12778, 12818], "temperature": 0.0, "avg_logprob": -0.2185043790447178, "compression_ratio": 1.5628140703517588, "no_speech_prob": 3.989712809016055e-07}, {"id": 927, "seek": 435388, "start": 4369.36, "end": 4372.36, "text": " So before the Netflix prize came along", "tokens": [407, 949, 264, 12778, 12818, 1361, 2051], "temperature": 0.0, "avg_logprob": -0.2185043790447178, "compression_ratio": 1.5628140703517588, "no_speech_prob": 3.989712809016055e-07}, {"id": 928, "seek": 435388, "start": 4373.2, "end": 4379.16, "text": " This probabilistic matrix factorization it had actually already been invented, but nobody noticed", "tokens": [639, 31959, 3142, 8141, 5952, 2144, 309, 632, 767, 1217, 668, 14479, 11, 457, 5079, 5694], "temperature": 0.0, "avg_logprob": -0.2185043790447178, "compression_ratio": 1.5628140703517588, "no_speech_prob": 3.989712809016055e-07}, {"id": 929, "seek": 437916, "start": 4379.16, "end": 4385.46, "text": " All right, and then in the first year of the Netflix prize someone wrote this like really really famous blog post", "tokens": [1057, 558, 11, 293, 550, 294, 264, 700, 1064, 295, 264, 12778, 12818, 1580, 4114, 341, 411, 534, 534, 4618, 6968, 2183], "temperature": 0.0, "avg_logprob": -0.19389094398135232, "compression_ratio": 1.79136690647482, "no_speech_prob": 6.375537964231626e-07}, {"id": 930, "seek": 437916, "start": 4385.46, "end": 4388.099999999999, "text": " But it basically said like hey check this out", "tokens": [583, 309, 1936, 848, 411, 4177, 1520, 341, 484], "temperature": 0.0, "avg_logprob": -0.19389094398135232, "compression_ratio": 1.79136690647482, "no_speech_prob": 6.375537964231626e-07}, {"id": 931, "seek": 437916, "start": 4388.88, "end": 4395.82, "text": " Incredibly simple technique works incredibly well and suddenly all the Netflix leaderboard entries were like much much better", "tokens": [27792, 3545, 2199, 6532, 1985, 6252, 731, 293, 5800, 439, 264, 12778, 5263, 3787, 23041, 645, 411, 709, 709, 1101], "temperature": 0.0, "avg_logprob": -0.19389094398135232, "compression_ratio": 1.79136690647482, "no_speech_prob": 6.375537964231626e-07}, {"id": 932, "seek": 437916, "start": 4396.5599999999995, "end": 4400.0, "text": " And so you know that's quite a few years ago now, and this is like now", "tokens": [400, 370, 291, 458, 300, 311, 1596, 257, 1326, 924, 2057, 586, 11, 293, 341, 307, 411, 586], "temperature": 0.0, "avg_logprob": -0.19389094398135232, "compression_ratio": 1.79136690647482, "no_speech_prob": 6.375537964231626e-07}, {"id": 933, "seek": 440000, "start": 4400.0, "end": 4409.0, "text": " Every collaborative filtering approach does this not every collaborative filtering approach adds this sigmoid thing by the way. It's not like", "tokens": [2048, 16555, 30822, 3109, 775, 341, 406, 633, 16555, 30822, 3109, 10860, 341, 4556, 3280, 327, 551, 538, 264, 636, 13, 467, 311, 406, 411], "temperature": 0.0, "avg_logprob": -0.17015815993486824, "compression_ratio": 1.8081180811808117, "no_speech_prob": 1.0953033779514953e-05}, {"id": 934, "seek": 440000, "start": 4409.24, "end": 4412.8, "text": " Rocket science this is this is not like the NLP thing we saw last week", "tokens": [29651, 3497, 341, 307, 341, 307, 406, 411, 264, 426, 45196, 551, 321, 1866, 1036, 1243], "temperature": 0.0, "avg_logprob": -0.17015815993486824, "compression_ratio": 1.8081180811808117, "no_speech_prob": 1.0953033779514953e-05}, {"id": 935, "seek": 440000, "start": 4412.8, "end": 4417.16, "text": " Which is like hey, this is a new state-of-the-art like this is you know not particularly uncommon", "tokens": [3013, 307, 411, 4177, 11, 341, 307, 257, 777, 1785, 12, 2670, 12, 3322, 12, 446, 411, 341, 307, 291, 458, 406, 4098, 29289], "temperature": 0.0, "avg_logprob": -0.17015815993486824, "compression_ratio": 1.8081180811808117, "no_speech_prob": 1.0953033779514953e-05}, {"id": 936, "seek": 440000, "start": 4417.16, "end": 4422.64, "text": " But there are still people that don't do this and it definitely helps a lot right to have this and so", "tokens": [583, 456, 366, 920, 561, 300, 500, 380, 360, 341, 293, 309, 2138, 3665, 257, 688, 558, 281, 362, 341, 293, 370], "temperature": 0.0, "avg_logprob": -0.17015815993486824, "compression_ratio": 1.8081180811808117, "no_speech_prob": 1.0953033779514953e-05}, {"id": 937, "seek": 440000, "start": 4424.4, "end": 4428.24, "text": " Actually, you know what we could do is maybe now's a good time to have a look", "tokens": [5135, 11, 291, 458, 437, 321, 727, 360, 307, 1310, 586, 311, 257, 665, 565, 281, 362, 257, 574], "temperature": 0.0, "avg_logprob": -0.17015815993486824, "compression_ratio": 1.8081180811808117, "no_speech_prob": 1.0953033779514953e-05}, {"id": 938, "seek": 442824, "start": 4428.24, "end": 4431.44, "text": " at the definition of this right so", "tokens": [412, 264, 7123, 295, 341, 558, 370], "temperature": 0.0, "avg_logprob": -0.3879229426383972, "compression_ratio": 1.5436241610738255, "no_speech_prob": 1.2029527169943321e-05}, {"id": 939, "seek": 442824, "start": 4432.44, "end": 4434.44, "text": " the column data", "tokens": [264, 7738, 1412], "temperature": 0.0, "avg_logprob": -0.3879229426383972, "compression_ratio": 1.5436241610738255, "no_speech_prob": 1.2029527169943321e-05}, {"id": 940, "seek": 442824, "start": 4434.44, "end": 4436.08, "text": " module", "tokens": [10088], "temperature": 0.0, "avg_logprob": -0.3879229426383972, "compression_ratio": 1.5436241610738255, "no_speech_prob": 1.2029527169943321e-05}, {"id": 941, "seek": 442824, "start": 4436.08, "end": 4438.08, "text": " Contains all these definitions", "tokens": [4839, 2315, 439, 613, 21988], "temperature": 0.0, "avg_logprob": -0.3879229426383972, "compression_ratio": 1.5436241610738255, "no_speech_prob": 1.2029527169943321e-05}, {"id": 942, "seek": 442824, "start": 4438.76, "end": 4441.639999999999, "text": " and we can now compare this to", "tokens": [293, 321, 393, 586, 6794, 341, 281], "temperature": 0.0, "avg_logprob": -0.3879229426383972, "compression_ratio": 1.5436241610738255, "no_speech_prob": 1.2029527169943321e-05}, {"id": 943, "seek": 442824, "start": 4442.32, "end": 4444.599999999999, "text": " the thing we originally used which was", "tokens": [264, 551, 321, 7993, 1143, 597, 390], "temperature": 0.0, "avg_logprob": -0.3879229426383972, "compression_ratio": 1.5436241610738255, "no_speech_prob": 1.2029527169943321e-05}, {"id": 944, "seek": 442824, "start": 4446.16, "end": 4450.099999999999, "text": " Whatever came out of collab filter data set right so let's go to", "tokens": [8541, 1361, 484, 295, 44228, 6608, 1412, 992, 558, 370, 718, 311, 352, 281], "temperature": 0.0, "avg_logprob": -0.3879229426383972, "compression_ratio": 1.5436241610738255, "no_speech_prob": 1.2029527169943321e-05}, {"id": 945, "seek": 442824, "start": 4451.5199999999995, "end": 4453.5199999999995, "text": " Co lab", "tokens": [3066, 2715], "temperature": 0.0, "avg_logprob": -0.3879229426383972, "compression_ratio": 1.5436241610738255, "no_speech_prob": 1.2029527169943321e-05}, {"id": 946, "seek": 445352, "start": 4453.52, "end": 4457.68, "text": " Filter data set here it is and we called", "tokens": [39592, 1412, 992, 510, 309, 307, 293, 321, 1219], "temperature": 0.0, "avg_logprob": -0.23806287977430557, "compression_ratio": 1.8186528497409327, "no_speech_prob": 9.570799193170387e-07}, {"id": 947, "seek": 445352, "start": 4458.280000000001, "end": 4465.4800000000005, "text": " Get learner all right so we can go down to get learner and that created a collab filter learner", "tokens": [3240, 33347, 439, 558, 370, 321, 393, 352, 760, 281, 483, 33347, 293, 300, 2942, 257, 44228, 6608, 33347], "temperature": 0.0, "avg_logprob": -0.23806287977430557, "compression_ratio": 1.8186528497409327, "no_speech_prob": 9.570799193170387e-07}, {"id": 948, "seek": 445352, "start": 4467.84, "end": 4473.160000000001, "text": " Passing in the model from get model is get model so it created an embedding bias and", "tokens": [10319, 278, 294, 264, 2316, 490, 483, 2316, 307, 483, 2316, 370, 309, 2942, 364, 12240, 3584, 12577, 293], "temperature": 0.0, "avg_logprob": -0.23806287977430557, "compression_ratio": 1.8186528497409327, "no_speech_prob": 9.570799193170387e-07}, {"id": 949, "seek": 445352, "start": 4474.68, "end": 4477.080000000001, "text": " So here is embedding bias and", "tokens": [407, 510, 307, 12240, 3584, 12577, 293], "temperature": 0.0, "avg_logprob": -0.23806287977430557, "compression_ratio": 1.8186528497409327, "no_speech_prob": 9.570799193170387e-07}, {"id": 950, "seek": 447708, "start": 4477.08, "end": 4483.08, "text": " You can see here here. It is like it's the same thing. There's the embedding for each of the things", "tokens": [509, 393, 536, 510, 510, 13, 467, 307, 411, 309, 311, 264, 912, 551, 13, 821, 311, 264, 12240, 3584, 337, 1184, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.23035579919815063, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.933349039973109e-06}, {"id": 951, "seek": 447708, "start": 4483.72, "end": 4487.92, "text": " Here's our forward that does the u times i dot sum", "tokens": [1692, 311, 527, 2128, 300, 775, 264, 344, 1413, 741, 5893, 2408], "temperature": 0.0, "avg_logprob": -0.23035579919815063, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.933349039973109e-06}, {"id": 952, "seek": 447708, "start": 4488.88, "end": 4490.0199999999995, "text": " plus plus", "tokens": [1804, 1804], "temperature": 0.0, "avg_logprob": -0.23035579919815063, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.933349039973109e-06}, {"id": 953, "seek": 447708, "start": 4490.0199999999995, "end": 4491.96, "text": " sigmoid so in fact", "tokens": [4556, 3280, 327, 370, 294, 1186], "temperature": 0.0, "avg_logprob": -0.23035579919815063, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.933349039973109e-06}, {"id": 954, "seek": 447708, "start": 4491.96, "end": 4493.96, "text": " We have just actually rebuilt", "tokens": [492, 362, 445, 767, 38532], "temperature": 0.0, "avg_logprob": -0.23035579919815063, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.933349039973109e-06}, {"id": 955, "seek": 447708, "start": 4494.76, "end": 4497.6, "text": " What's in the fast air library literally okay?", "tokens": [708, 311, 294, 264, 2370, 1988, 6405, 3736, 1392, 30], "temperature": 0.0, "avg_logprob": -0.23035579919815063, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.933349039973109e-06}, {"id": 956, "seek": 447708, "start": 4499.48, "end": 4505.32, "text": " It's a little shorter and easier because we're taking advantage of the fact that there's a special", "tokens": [467, 311, 257, 707, 11639, 293, 3571, 570, 321, 434, 1940, 5002, 295, 264, 1186, 300, 456, 311, 257, 2121], "temperature": 0.0, "avg_logprob": -0.23035579919815063, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.933349039973109e-06}, {"id": 957, "seek": 450532, "start": 4505.32, "end": 4508.28, "text": " collaborative filtering data set", "tokens": [16555, 30822, 1412, 992], "temperature": 0.0, "avg_logprob": -0.21523531863563938, "compression_ratio": 1.61328125, "no_speech_prob": 2.482471700204769e-06}, {"id": 958, "seek": 450532, "start": 4509.32, "end": 4514.36, "text": " So we can actually we're getting passed in the users and the items and we don't have to pull them out of cats and cons", "tokens": [407, 321, 393, 767, 321, 434, 1242, 4678, 294, 264, 5022, 293, 264, 4754, 293, 321, 500, 380, 362, 281, 2235, 552, 484, 295, 11111, 293, 1014], "temperature": 0.0, "avg_logprob": -0.21523531863563938, "compression_ratio": 1.61328125, "no_speech_prob": 2.482471700204769e-06}, {"id": 959, "seek": 450532, "start": 4515.48, "end": 4522.12, "text": " But other than that this is exactly the same so hopefully you can see like the fast AI library is not some inscrutable code", "tokens": [583, 661, 813, 300, 341, 307, 2293, 264, 912, 370, 4696, 291, 393, 536, 411, 264, 2370, 7318, 6405, 307, 406, 512, 1028, 10757, 32148, 3089], "temperature": 0.0, "avg_logprob": -0.21523531863563938, "compression_ratio": 1.61328125, "no_speech_prob": 2.482471700204769e-06}, {"id": 960, "seek": 450532, "start": 4522.24, "end": 4528.88, "text": " Containing concepts you can never understand we've actually just built up this entire thing from scratch ourselves", "tokens": [4839, 3686, 10392, 291, 393, 1128, 1223, 321, 600, 767, 445, 3094, 493, 341, 2302, 551, 490, 8459, 4175], "temperature": 0.0, "avg_logprob": -0.21523531863563938, "compression_ratio": 1.61328125, "no_speech_prob": 2.482471700204769e-06}, {"id": 961, "seek": 450532, "start": 4528.88, "end": 4531.639999999999, "text": " And so why did we get?", "tokens": [400, 370, 983, 630, 321, 483, 30], "temperature": 0.0, "avg_logprob": -0.21523531863563938, "compression_ratio": 1.61328125, "no_speech_prob": 2.482471700204769e-06}, {"id": 962, "seek": 453164, "start": 4531.64, "end": 4535.4400000000005, "text": " 0.76 rather than 0.8", "tokens": [1958, 13, 25026, 2831, 813, 1958, 13, 23], "temperature": 0.0, "avg_logprob": -0.24944590283678725, "compression_ratio": 1.4619289340101522, "no_speech_prob": 5.594280992227141e-06}, {"id": 963, "seek": 453164, "start": 4537.56, "end": 4545.14, "text": " You know I think it's simply because we used a stochastic gradient descent with restarts and a cycle multiplier and an atom optimizer", "tokens": [509, 458, 286, 519, 309, 311, 2935, 570, 321, 1143, 257, 342, 8997, 2750, 16235, 23475, 365, 1472, 11814, 293, 257, 6586, 44106, 293, 364, 12018, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.24944590283678725, "compression_ratio": 1.4619289340101522, "no_speech_prob": 5.594280992227141e-06}, {"id": 964, "seek": 453164, "start": 4545.14, "end": 4547.14, "text": " You know like a few little", "tokens": [509, 458, 411, 257, 1326, 707], "temperature": 0.0, "avg_logprob": -0.24944590283678725, "compression_ratio": 1.4619289340101522, "no_speech_prob": 5.594280992227141e-06}, {"id": 965, "seek": 453164, "start": 4547.320000000001, "end": 4549.320000000001, "text": " training tricks", "tokens": [3097, 11733], "temperature": 0.0, "avg_logprob": -0.24944590283678725, "compression_ratio": 1.4619289340101522, "no_speech_prob": 5.594280992227141e-06}, {"id": 966, "seek": 453164, "start": 4550.52, "end": 4555.64, "text": " So I'm looking at this and thinking that is we could totally", "tokens": [407, 286, 478, 1237, 412, 341, 293, 1953, 300, 307, 321, 727, 3879], "temperature": 0.0, "avg_logprob": -0.24944590283678725, "compression_ratio": 1.4619289340101522, "no_speech_prob": 5.594280992227141e-06}, {"id": 967, "seek": 453164, "start": 4556.360000000001, "end": 4558.360000000001, "text": " improve this model, but maybe", "tokens": [3470, 341, 2316, 11, 457, 1310], "temperature": 0.0, "avg_logprob": -0.24944590283678725, "compression_ratio": 1.4619289340101522, "no_speech_prob": 5.594280992227141e-06}, {"id": 968, "seek": 455836, "start": 4558.36, "end": 4564.2, "text": " Looking at the date and doing some tricks with the date because this is kind of a just a regular", "tokens": [11053, 412, 264, 4002, 293, 884, 512, 11733, 365, 264, 4002, 570, 341, 307, 733, 295, 257, 445, 257, 3890], "temperature": 0.0, "avg_logprob": -0.24901441064211402, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.422381026524818e-07}, {"id": 969, "seek": 455836, "start": 4565.0, "end": 4571.799999999999, "text": " Kind of model in a way yeah, you can add more features. Yeah, exactly exactly so like now that you've seen this", "tokens": [9242, 295, 2316, 294, 257, 636, 1338, 11, 291, 393, 909, 544, 4122, 13, 865, 11, 2293, 2293, 370, 411, 586, 300, 291, 600, 1612, 341], "temperature": 0.0, "avg_logprob": -0.24901441064211402, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.422381026524818e-07}, {"id": 970, "seek": 455836, "start": 4572.28, "end": 4575.36, "text": " You could now you know even if you didn't have", "tokens": [509, 727, 586, 291, 458, 754, 498, 291, 994, 380, 362], "temperature": 0.0, "avg_logprob": -0.24901441064211402, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.422381026524818e-07}, {"id": 971, "seek": 455836, "start": 4576.24, "end": 4580.12, "text": " Embedding bias in a notebook that you've written yourself some other model", "tokens": [24234, 292, 3584, 12577, 294, 257, 21060, 300, 291, 600, 3720, 1803, 512, 661, 2316], "temperature": 0.0, "avg_logprob": -0.24901441064211402, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.422381026524818e-07}, {"id": 972, "seek": 455836, "start": 4580.12, "end": 4582.92, "text": " That's in fast AI you could look at it in fast AI and be like oh", "tokens": [663, 311, 294, 2370, 7318, 291, 727, 574, 412, 309, 294, 2370, 7318, 293, 312, 411, 1954], "temperature": 0.0, "avg_logprob": -0.24901441064211402, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.422381026524818e-07}, {"id": 973, "seek": 458292, "start": 4582.92, "end": 4588.7, "text": " That does most of the things that I'd want to do, but it doesn't deal with time and so you could just go", "tokens": [663, 775, 881, 295, 264, 721, 300, 286, 1116, 528, 281, 360, 11, 457, 309, 1177, 380, 2028, 365, 565, 293, 370, 291, 727, 445, 352], "temperature": 0.0, "avg_logprob": -0.20649310816889224, "compression_ratio": 1.596244131455399, "no_speech_prob": 4.356861609267071e-06}, {"id": 974, "seek": 458292, "start": 4588.7, "end": 4593.6, "text": " Oh, okay, let's grab it copy it. You know pop it into my notebook and", "tokens": [876, 11, 1392, 11, 718, 311, 4444, 309, 5055, 309, 13, 509, 458, 1665, 309, 666, 452, 21060, 293], "temperature": 0.0, "avg_logprob": -0.20649310816889224, "compression_ratio": 1.596244131455399, "no_speech_prob": 4.356861609267071e-06}, {"id": 975, "seek": 458292, "start": 4594.2, "end": 4596.72, "text": " Let's create you know the better version", "tokens": [961, 311, 1884, 291, 458, 264, 1101, 3037], "temperature": 0.0, "avg_logprob": -0.20649310816889224, "compression_ratio": 1.596244131455399, "no_speech_prob": 4.356861609267071e-06}, {"id": 976, "seek": 458292, "start": 4597.4400000000005, "end": 4601.96, "text": " Right and then you can start playing right and you can now create your own", "tokens": [1779, 293, 550, 291, 393, 722, 2433, 558, 293, 291, 393, 586, 1884, 428, 1065], "temperature": 0.0, "avg_logprob": -0.20649310816889224, "compression_ratio": 1.596244131455399, "no_speech_prob": 4.356861609267071e-06}, {"id": 977, "seek": 458292, "start": 4603.04, "end": 4605.04, "text": " model class", "tokens": [2316, 1508], "temperature": 0.0, "avg_logprob": -0.20649310816889224, "compression_ratio": 1.596244131455399, "no_speech_prob": 4.356861609267071e-06}, {"id": 978, "seek": 458292, "start": 4605.08, "end": 4608.12, "text": " from the open source code here and so", "tokens": [490, 264, 1269, 4009, 3089, 510, 293, 370], "temperature": 0.0, "avg_logprob": -0.20649310816889224, "compression_ratio": 1.596244131455399, "no_speech_prob": 4.356861609267071e-06}, {"id": 979, "seek": 460812, "start": 4608.12, "end": 4614.04, "text": " Yeah, you know that's mentioning a couple things we could do we could try incorporating time stamps, so we could assume that maybe", "tokens": [865, 11, 291, 458, 300, 311, 18315, 257, 1916, 721, 321, 727, 360, 321, 727, 853, 33613, 565, 30800, 11, 370, 321, 727, 6552, 300, 1310], "temperature": 0.0, "avg_logprob": -0.19245787589780747, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.2289142432564404e-06}, {"id": 980, "seek": 460812, "start": 4615.04, "end": 4617.04, "text": " Well, maybe there's just like some", "tokens": [1042, 11, 1310, 456, 311, 445, 411, 512], "temperature": 0.0, "avg_logprob": -0.19245787589780747, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.2289142432564404e-06}, {"id": 981, "seek": 460812, "start": 4617.48, "end": 4622.7, "text": " For a particular user over time users tend to get more or less positive about movies", "tokens": [1171, 257, 1729, 4195, 670, 565, 5022, 3928, 281, 483, 544, 420, 1570, 3353, 466, 6233], "temperature": 0.0, "avg_logprob": -0.19245787589780747, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.2289142432564404e-06}, {"id": 982, "seek": 460812, "start": 4624.32, "end": 4629.68, "text": " Also remember there was the list of genres for each movie. Maybe we could incorporate that", "tokens": [2743, 1604, 456, 390, 264, 1329, 295, 30057, 337, 1184, 3169, 13, 2704, 321, 727, 16091, 300], "temperature": 0.0, "avg_logprob": -0.19245787589780747, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.2289142432564404e-06}, {"id": 983, "seek": 460812, "start": 4630.8, "end": 4634.72, "text": " So one problem is it's a little bit difficult to incorporate that stuff", "tokens": [407, 472, 1154, 307, 309, 311, 257, 707, 857, 2252, 281, 16091, 300, 1507], "temperature": 0.0, "avg_logprob": -0.19245787589780747, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.2289142432564404e-06}, {"id": 984, "seek": 463472, "start": 4634.72, "end": 4642.64, "text": " Into this embedding bias model because it's kind of it's pretty custom right so what we're going to do next is we're going", "tokens": [23373, 341, 12240, 3584, 12577, 2316, 570, 309, 311, 733, 295, 309, 311, 1238, 2375, 558, 370, 437, 321, 434, 516, 281, 360, 958, 307, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.22412031347101385, "compression_ratio": 1.6736842105263159, "no_speech_prob": 2.3823662331778905e-07}, {"id": 985, "seek": 463472, "start": 4642.64, "end": 4644.64, "text": " to try to create a", "tokens": [281, 853, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.22412031347101385, "compression_ratio": 1.6736842105263159, "no_speech_prob": 2.3823662331778905e-07}, {"id": 986, "seek": 463472, "start": 4645.88, "end": 4652.04, "text": " Neural net version of this right so the basic idea here is", "tokens": [1734, 1807, 2533, 3037, 295, 341, 558, 370, 264, 3875, 1558, 510, 307], "temperature": 0.0, "avg_logprob": -0.22412031347101385, "compression_ratio": 1.6736842105263159, "no_speech_prob": 2.3823662331778905e-07}, {"id": 987, "seek": 463472, "start": 4653.68, "end": 4655.52, "text": " We're going to", "tokens": [492, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.22412031347101385, "compression_ratio": 1.6736842105263159, "no_speech_prob": 2.3823662331778905e-07}, {"id": 988, "seek": 463472, "start": 4655.52, "end": 4658.8, "text": " Take exactly the same thing as we had before here's our list of users", "tokens": [3664, 2293, 264, 912, 551, 382, 321, 632, 949, 510, 311, 527, 1329, 295, 5022], "temperature": 0.0, "avg_logprob": -0.22412031347101385, "compression_ratio": 1.6736842105263159, "no_speech_prob": 2.3823662331778905e-07}, {"id": 989, "seek": 463472, "start": 4659.56, "end": 4661.56, "text": " right and here is", "tokens": [558, 293, 510, 307], "temperature": 0.0, "avg_logprob": -0.22412031347101385, "compression_ratio": 1.6736842105263159, "no_speech_prob": 2.3823662331778905e-07}, {"id": 990, "seek": 463472, "start": 4662.4400000000005, "end": 4663.96, "text": " our embeddings", "tokens": [527, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.22412031347101385, "compression_ratio": 1.6736842105263159, "no_speech_prob": 2.3823662331778905e-07}, {"id": 991, "seek": 466396, "start": 4663.96, "end": 4666.52, "text": " right and here's our list of movies and", "tokens": [558, 293, 510, 311, 527, 1329, 295, 6233, 293], "temperature": 0.0, "avg_logprob": -0.23244365692138672, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1015928294000332e-06}, {"id": 992, "seek": 466396, "start": 4667.36, "end": 4669.24, "text": " here is our", "tokens": [510, 307, 527], "temperature": 0.0, "avg_logprob": -0.23244365692138672, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1015928294000332e-06}, {"id": 993, "seek": 466396, "start": 4669.24, "end": 4672.64, "text": " Embeddings right and so as you can see I've just kind of transposed", "tokens": [24234, 292, 29432, 558, 293, 370, 382, 291, 393, 536, 286, 600, 445, 733, 295, 7132, 1744], "temperature": 0.0, "avg_logprob": -0.23244365692138672, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1015928294000332e-06}, {"id": 994, "seek": 466396, "start": 4673.56, "end": 4677.52, "text": " The movie ones so that so that they're all in the same orientation and", "tokens": [440, 3169, 2306, 370, 300, 370, 300, 436, 434, 439, 294, 264, 912, 14764, 293], "temperature": 0.0, "avg_logprob": -0.23244365692138672, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1015928294000332e-06}, {"id": 995, "seek": 466396, "start": 4678.2, "end": 4680.74, "text": " Here is our user movie rating", "tokens": [1692, 307, 527, 4195, 3169, 10990], "temperature": 0.0, "avg_logprob": -0.23244365692138672, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1015928294000332e-06}, {"id": 996, "seek": 466396, "start": 4681.4800000000005, "end": 4687.4, "text": " But D crosstabbed okay, so in the original format so each row is a user movie rating", "tokens": [583, 413, 28108, 372, 455, 2883, 1392, 11, 370, 294, 264, 3380, 7877, 370, 1184, 5386, 307, 257, 4195, 3169, 10990], "temperature": 0.0, "avg_logprob": -0.23244365692138672, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1015928294000332e-06}, {"id": 997, "seek": 466396, "start": 4688.08, "end": 4689.72, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.23244365692138672, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1015928294000332e-06}, {"id": 998, "seek": 468972, "start": 4689.72, "end": 4695.42, "text": " So the first thing I do is I need to replace user 14", "tokens": [407, 264, 700, 551, 286, 360, 307, 286, 643, 281, 7406, 4195, 3499], "temperature": 0.0, "avg_logprob": -0.15915735562642416, "compression_ratio": 1.6414141414141414, "no_speech_prob": 3.1381298413180048e-06}, {"id": 999, "seek": 468972, "start": 4696.08, "end": 4698.08, "text": " with that users", "tokens": [365, 300, 5022], "temperature": 0.0, "avg_logprob": -0.15915735562642416, "compression_ratio": 1.6414141414141414, "no_speech_prob": 3.1381298413180048e-06}, {"id": 1000, "seek": 468972, "start": 4698.280000000001, "end": 4702.2, "text": " Contiguous index right and so I can do that in Excel", "tokens": [4839, 30525, 8186, 558, 293, 370, 286, 393, 360, 300, 294, 19060], "temperature": 0.0, "avg_logprob": -0.15915735562642416, "compression_ratio": 1.6414141414141414, "no_speech_prob": 3.1381298413180048e-06}, {"id": 1001, "seek": 468972, "start": 4702.68, "end": 4709.360000000001, "text": " Using this match that basically says what you know how far down this list you have to go and it said", "tokens": [11142, 341, 2995, 300, 1936, 1619, 437, 291, 458, 577, 1400, 760, 341, 1329, 291, 362, 281, 352, 293, 309, 848], "temperature": 0.0, "avg_logprob": -0.15915735562642416, "compression_ratio": 1.6414141414141414, "no_speech_prob": 3.1381298413180048e-06}, {"id": 1002, "seek": 468972, "start": 4710.360000000001, "end": 4717.96, "text": " User 14 was the first thing in that list okay user 29 was the second thing in that list so forth okay?", "tokens": [32127, 3499, 390, 264, 700, 551, 294, 300, 1329, 1392, 4195, 9413, 390, 264, 1150, 551, 294, 300, 1329, 370, 5220, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15915735562642416, "compression_ratio": 1.6414141414141414, "no_speech_prob": 3.1381298413180048e-06}, {"id": 1003, "seek": 471796, "start": 4717.96, "end": 4721.44, "text": " So this is the same as that thing that we did", "tokens": [407, 341, 307, 264, 912, 382, 300, 551, 300, 321, 630], "temperature": 0.0, "avg_logprob": -0.2405575838955966, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.737898109397065e-07}, {"id": 1004, "seek": 471796, "start": 4722.08, "end": 4726.68, "text": " In our Python code where we basically created a dictionary to map this stuff", "tokens": [682, 527, 15329, 3089, 689, 321, 1936, 2942, 257, 25890, 281, 4471, 341, 1507], "temperature": 0.0, "avg_logprob": -0.2405575838955966, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.737898109397065e-07}, {"id": 1005, "seek": 471796, "start": 4727.68, "end": 4731.58, "text": " So now we can for this particular user movie rating", "tokens": [407, 586, 321, 393, 337, 341, 1729, 4195, 3169, 10990], "temperature": 0.0, "avg_logprob": -0.2405575838955966, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.737898109397065e-07}, {"id": 1006, "seek": 471796, "start": 4732.68, "end": 4736.5, "text": " Combination we can look up the appropriate embedding", "tokens": [25939, 2486, 321, 393, 574, 493, 264, 6854, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.2405575838955966, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.737898109397065e-07}, {"id": 1007, "seek": 471796, "start": 4737.36, "end": 4742.68, "text": " Right and so you can see here what it's doing is it's saying all right. Let's basically", "tokens": [1779, 293, 370, 291, 393, 536, 510, 437, 309, 311, 884, 307, 309, 311, 1566, 439, 558, 13, 961, 311, 1936], "temperature": 0.0, "avg_logprob": -0.2405575838955966, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.737898109397065e-07}, {"id": 1008, "seek": 471796, "start": 4743.52, "end": 4744.6, "text": " offset", "tokens": [18687], "temperature": 0.0, "avg_logprob": -0.2405575838955966, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.737898109397065e-07}, {"id": 1009, "seek": 474460, "start": 4744.6, "end": 4750.68, "text": " From the start of this list and the number of rows we're going to go down is equal to the user index", "tokens": [3358, 264, 722, 295, 341, 1329, 293, 264, 1230, 295, 13241, 321, 434, 516, 281, 352, 760, 307, 2681, 281, 264, 4195, 8186], "temperature": 0.0, "avg_logprob": -0.238828996641446, "compression_ratio": 1.847457627118644, "no_speech_prob": 6.577917019967572e-07}, {"id": 1010, "seek": 474460, "start": 4751.08, "end": 4753.280000000001, "text": " And the number of columns we're going to go across is", "tokens": [400, 264, 1230, 295, 13766, 321, 434, 516, 281, 352, 2108, 307], "temperature": 0.0, "avg_logprob": -0.238828996641446, "compression_ratio": 1.847457627118644, "no_speech_prob": 6.577917019967572e-07}, {"id": 1011, "seek": 474460, "start": 4754.0, "end": 4760.0, "text": " One two three four or five okay, and so you can see what it does is it creates point one nine point six three point three one", "tokens": [1485, 732, 1045, 1451, 420, 1732, 1392, 11, 293, 370, 291, 393, 536, 437, 309, 775, 307, 309, 7829, 935, 472, 4949, 935, 2309, 1045, 935, 1045, 472], "temperature": 0.0, "avg_logprob": -0.238828996641446, "compression_ratio": 1.847457627118644, "no_speech_prob": 6.577917019967572e-07}, {"id": 1012, "seek": 474460, "start": 4760.0, "end": 4764.120000000001, "text": " Here it is point one nine point three okay, so so this is literally", "tokens": [1692, 309, 307, 935, 472, 4949, 935, 1045, 1392, 11, 370, 370, 341, 307, 3736], "temperature": 0.0, "avg_logprob": -0.238828996641446, "compression_ratio": 1.847457627118644, "no_speech_prob": 6.577917019967572e-07}, {"id": 1013, "seek": 474460, "start": 4765.0, "end": 4767.320000000001, "text": " Modern embedding does but remember", "tokens": [19814, 12240, 3584, 775, 457, 1604], "temperature": 0.0, "avg_logprob": -0.238828996641446, "compression_ratio": 1.847457627118644, "no_speech_prob": 6.577917019967572e-07}, {"id": 1014, "seek": 474460, "start": 4767.88, "end": 4770.160000000001, "text": " this is exactly the same as", "tokens": [341, 307, 2293, 264, 912, 382], "temperature": 0.0, "avg_logprob": -0.238828996641446, "compression_ratio": 1.847457627118644, "no_speech_prob": 6.577917019967572e-07}, {"id": 1015, "seek": 474460, "start": 4771.0, "end": 4772.240000000001, "text": " doing a", "tokens": [884, 257], "temperature": 0.0, "avg_logprob": -0.238828996641446, "compression_ratio": 1.847457627118644, "no_speech_prob": 6.577917019967572e-07}, {"id": 1016, "seek": 474460, "start": 4772.240000000001, "end": 4773.84, "text": " one hot encoding", "tokens": [472, 2368, 43430], "temperature": 0.0, "avg_logprob": -0.238828996641446, "compression_ratio": 1.847457627118644, "no_speech_prob": 6.577917019967572e-07}, {"id": 1017, "seek": 477384, "start": 4773.84, "end": 4776.52, "text": " right because if instead this was a", "tokens": [558, 570, 498, 2602, 341, 390, 257], "temperature": 0.0, "avg_logprob": -0.24490519320027213, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.247026197390369e-07}, {"id": 1018, "seek": 477384, "start": 4777.32, "end": 4778.400000000001, "text": " vector", "tokens": [8062], "temperature": 0.0, "avg_logprob": -0.24490519320027213, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.247026197390369e-07}, {"id": 1019, "seek": 477384, "start": 4778.400000000001, "end": 4784.72, "text": " Containing one zero zero zero zero zero right and we multiplied that by this matrix", "tokens": [4839, 3686, 472, 4018, 4018, 4018, 4018, 4018, 558, 293, 321, 17207, 300, 538, 341, 8141], "temperature": 0.0, "avg_logprob": -0.24490519320027213, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.247026197390369e-07}, {"id": 1020, "seek": 477384, "start": 4785.08, "end": 4787.08, "text": " Then the only row it's going to return", "tokens": [1396, 264, 787, 5386, 309, 311, 516, 281, 2736], "temperature": 0.0, "avg_logprob": -0.24490519320027213, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.247026197390369e-07}, {"id": 1021, "seek": 477384, "start": 4787.56, "end": 4790.02, "text": " Would be the first one okay, so", "tokens": [6068, 312, 264, 700, 472, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.24490519320027213, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.247026197390369e-07}, {"id": 1022, "seek": 477384, "start": 4791.32, "end": 4796.6, "text": " So it's really useful to remember that embedding actually just is a matrix product", "tokens": [407, 309, 311, 534, 4420, 281, 1604, 300, 12240, 3584, 767, 445, 307, 257, 8141, 1674], "temperature": 0.0, "avg_logprob": -0.24490519320027213, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.247026197390369e-07}, {"id": 1023, "seek": 477384, "start": 4797.12, "end": 4802.84, "text": " The only reason it exists the only reason it exists is because this is an optimization", "tokens": [440, 787, 1778, 309, 8198, 264, 787, 1778, 309, 8198, 307, 570, 341, 307, 364, 19618], "temperature": 0.0, "avg_logprob": -0.24490519320027213, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.247026197390369e-07}, {"id": 1024, "seek": 480284, "start": 4802.84, "end": 4805.72, "text": " You know this lets pytorch know like okay", "tokens": [509, 458, 341, 6653, 25878, 284, 339, 458, 411, 1392], "temperature": 0.0, "avg_logprob": -0.22795799255371094, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.5763554251388996e-07}, {"id": 1025, "seek": 480284, "start": 4805.88, "end": 4813.360000000001, "text": " This is just a matrix multiply, but I guarantee you that you know this thing is one hot encoded", "tokens": [639, 307, 445, 257, 8141, 12972, 11, 457, 286, 10815, 291, 300, 291, 458, 341, 551, 307, 472, 2368, 2058, 12340], "temperature": 0.0, "avg_logprob": -0.22795799255371094, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.5763554251388996e-07}, {"id": 1026, "seek": 480284, "start": 4813.76, "end": 4817.52, "text": " Therefore you don't have to actually do the matrix multiply you can just do a direct lookup", "tokens": [7504, 291, 500, 380, 362, 281, 767, 360, 264, 8141, 12972, 291, 393, 445, 360, 257, 2047, 574, 1010], "temperature": 0.0, "avg_logprob": -0.22795799255371094, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.5763554251388996e-07}, {"id": 1027, "seek": 480284, "start": 4818.12, "end": 4821.6, "text": " Okay, so that's literally all an embedding is is it is a", "tokens": [1033, 11, 370, 300, 311, 3736, 439, 364, 12240, 3584, 307, 307, 309, 307, 257], "temperature": 0.0, "avg_logprob": -0.22795799255371094, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.5763554251388996e-07}, {"id": 1028, "seek": 480284, "start": 4822.4400000000005, "end": 4824.16, "text": " computational", "tokens": [28270], "temperature": 0.0, "avg_logprob": -0.22795799255371094, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.5763554251388996e-07}, {"id": 1029, "seek": 480284, "start": 4824.16, "end": 4825.24, "text": " performance", "tokens": [3389], "temperature": 0.0, "avg_logprob": -0.22795799255371094, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.5763554251388996e-07}, {"id": 1030, "seek": 480284, "start": 4825.24, "end": 4831.32, "text": " Thing for a particular kind of matrix multiply all right so that looks up that uses user", "tokens": [30902, 337, 257, 1729, 733, 295, 8141, 12972, 439, 558, 370, 300, 1542, 493, 300, 4960, 4195], "temperature": 0.0, "avg_logprob": -0.22795799255371094, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.5763554251388996e-07}, {"id": 1031, "seek": 483132, "start": 4831.32, "end": 4836.679999999999, "text": " And then we can look up that users movie right so here is movie ID", "tokens": [400, 550, 321, 393, 574, 493, 300, 5022, 3169, 558, 370, 510, 307, 3169, 7348], "temperature": 0.0, "avg_logprob": -0.21145331993531646, "compression_ratio": 1.7384615384615385, "no_speech_prob": 1.3496992323780432e-06}, {"id": 1032, "seek": 483132, "start": 4837.5599999999995, "end": 4839.36, "text": " Movie ID", "tokens": [28766, 7348], "temperature": 0.0, "avg_logprob": -0.21145331993531646, "compression_ratio": 1.7384615384615385, "no_speech_prob": 1.3496992323780432e-06}, {"id": 1033, "seek": 483132, "start": 4839.36, "end": 4846.24, "text": " 417 which apparently is index number 14 here it is here, so it should have been point seven five point four seven", "tokens": [1017, 7773, 597, 7970, 307, 8186, 1230, 3499, 510, 309, 307, 510, 11, 370, 309, 820, 362, 668, 935, 3407, 1732, 935, 1451, 3407], "temperature": 0.0, "avg_logprob": -0.21145331993531646, "compression_ratio": 1.7384615384615385, "no_speech_prob": 1.3496992323780432e-06}, {"id": 1034, "seek": 483132, "start": 4846.599999999999, "end": 4853.179999999999, "text": " Yes, it is point seven five point four seven okay, so we've now got the user embedding and the movie embedding", "tokens": [1079, 11, 309, 307, 935, 3407, 1732, 935, 1451, 3407, 1392, 11, 370, 321, 600, 586, 658, 264, 4195, 12240, 3584, 293, 264, 3169, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.21145331993531646, "compression_ratio": 1.7384615384615385, "no_speech_prob": 1.3496992323780432e-06}, {"id": 1035, "seek": 483132, "start": 4853.88, "end": 4855.88, "text": " and rather than doing a", "tokens": [293, 2831, 813, 884, 257], "temperature": 0.0, "avg_logprob": -0.21145331993531646, "compression_ratio": 1.7384615384615385, "no_speech_prob": 1.3496992323780432e-06}, {"id": 1036, "seek": 483132, "start": 4856.92, "end": 4858.92, "text": " dot product of", "tokens": [5893, 1674, 295], "temperature": 0.0, "avg_logprob": -0.21145331993531646, "compression_ratio": 1.7384615384615385, "no_speech_prob": 1.3496992323780432e-06}, {"id": 1037, "seek": 485892, "start": 4858.92, "end": 4860.92, "text": " Those two", "tokens": [3950, 732], "temperature": 0.0, "avg_logprob": -0.28723675864083426, "compression_ratio": 1.4714285714285715, "no_speech_prob": 3.0894855171936797e-06}, {"id": 1038, "seek": 485892, "start": 4861.92, "end": 4863.92, "text": " Right which is what we do normally", "tokens": [1779, 597, 307, 437, 321, 360, 5646], "temperature": 0.0, "avg_logprob": -0.28723675864083426, "compression_ratio": 1.4714285714285715, "no_speech_prob": 3.0894855171936797e-06}, {"id": 1039, "seek": 485892, "start": 4865.56, "end": 4872.68, "text": " Instead what if we concatenate the two together into a single vector of length", "tokens": [7156, 437, 498, 321, 1588, 7186, 473, 264, 732, 1214, 666, 257, 2167, 8062, 295, 4641], "temperature": 0.0, "avg_logprob": -0.28723675864083426, "compression_ratio": 1.4714285714285715, "no_speech_prob": 3.0894855171936797e-06}, {"id": 1040, "seek": 485892, "start": 4874.24, "end": 4877.42, "text": " Ten and then feed that into a neural net", "tokens": [9380, 293, 550, 3154, 300, 666, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.28723675864083426, "compression_ratio": 1.4714285714285715, "no_speech_prob": 3.0894855171936797e-06}, {"id": 1041, "seek": 485892, "start": 4879.36, "end": 4883.36, "text": " Right and so anytime we've got you know a", "tokens": [1779, 293, 370, 13038, 321, 600, 658, 291, 458, 257], "temperature": 0.0, "avg_logprob": -0.28723675864083426, "compression_ratio": 1.4714285714285715, "no_speech_prob": 3.0894855171936797e-06}, {"id": 1042, "seek": 488336, "start": 4883.36, "end": 4888.88, "text": " a tensor of input activations or in this case a tensor of", "tokens": [257, 40863, 295, 4846, 2430, 763, 420, 294, 341, 1389, 257, 40863, 295], "temperature": 0.0, "avg_logprob": -0.21125312078566777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.285509701025148e-07}, {"id": 1043, "seek": 488336, "start": 4889.36, "end": 4892.92, "text": " Actually, this is a tensor of output activations. This is coming out of an embedding layer", "tokens": [5135, 11, 341, 307, 257, 40863, 295, 5598, 2430, 763, 13, 639, 307, 1348, 484, 295, 364, 12240, 3584, 4583], "temperature": 0.0, "avg_logprob": -0.21125312078566777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.285509701025148e-07}, {"id": 1044, "seek": 488336, "start": 4893.44, "end": 4899.32, "text": " We can chuck it in a neural net because neural nets we now know can calculate anything", "tokens": [492, 393, 20870, 309, 294, 257, 18161, 2533, 570, 18161, 36170, 321, 586, 458, 393, 8873, 1340], "temperature": 0.0, "avg_logprob": -0.21125312078566777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.285509701025148e-07}, {"id": 1045, "seek": 488336, "start": 4900.639999999999, "end": 4904.96, "text": " Okay, including hopefully collaborative filtering, so let's try that", "tokens": [1033, 11, 3009, 4696, 16555, 30822, 11, 370, 718, 311, 853, 300], "temperature": 0.0, "avg_logprob": -0.21125312078566777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.285509701025148e-07}, {"id": 1046, "seek": 488336, "start": 4906.0, "end": 4908.639999999999, "text": " So here is our embedding net", "tokens": [407, 510, 307, 527, 12240, 3584, 2533], "temperature": 0.0, "avg_logprob": -0.21125312078566777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.285509701025148e-07}, {"id": 1047, "seek": 488336, "start": 4909.92, "end": 4911.92, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.21125312078566777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.285509701025148e-07}, {"id": 1048, "seek": 491192, "start": 4911.92, "end": 4916.08, "text": " This time I have not bothered to create a separate", "tokens": [639, 565, 286, 362, 406, 22996, 281, 1884, 257, 4994], "temperature": 0.0, "avg_logprob": -0.2664345029800657, "compression_ratio": 1.4, "no_speech_prob": 5.539159815270978e-07}, {"id": 1049, "seek": 491192, "start": 4918.04, "end": 4919.56, "text": " Bias", "tokens": [363, 4609], "temperature": 0.0, "avg_logprob": -0.2664345029800657, "compression_ratio": 1.4, "no_speech_prob": 5.539159815270978e-07}, {"id": 1050, "seek": 491192, "start": 4919.56, "end": 4923.96, "text": " because instead the linear layer in pie torch", "tokens": [570, 2602, 264, 8213, 4583, 294, 1730, 27822], "temperature": 0.0, "avg_logprob": -0.2664345029800657, "compression_ratio": 1.4, "no_speech_prob": 5.539159815270978e-07}, {"id": 1051, "seek": 491192, "start": 4924.68, "end": 4931.76, "text": " Already has a bias in it right so when we go and in dot linear, right?", "tokens": [23741, 575, 257, 12577, 294, 309, 558, 370, 562, 321, 352, 293, 294, 5893, 8213, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2664345029800657, "compression_ratio": 1.4, "no_speech_prob": 5.539159815270978e-07}, {"id": 1052, "seek": 491192, "start": 4934.4800000000005, "end": 4936.4800000000005, "text": " Let's kind of draw this out", "tokens": [961, 311, 733, 295, 2642, 341, 484], "temperature": 0.0, "avg_logprob": -0.2664345029800657, "compression_ratio": 1.4, "no_speech_prob": 5.539159815270978e-07}, {"id": 1053, "seek": 493648, "start": 4936.48, "end": 4940.0, "text": " So we've got our you", "tokens": [407, 321, 600, 658, 527, 291], "temperature": 0.0, "avg_logprob": -0.27791740048316216, "compression_ratio": 1.8076923076923077, "no_speech_prob": 1.5056948541314341e-06}, {"id": 1054, "seek": 493648, "start": 4942.36, "end": 4945.44, "text": " Matrix right and this is the number of users and", "tokens": [36274, 558, 293, 341, 307, 264, 1230, 295, 5022, 293], "temperature": 0.0, "avg_logprob": -0.27791740048316216, "compression_ratio": 1.8076923076923077, "no_speech_prob": 1.5056948541314341e-06}, {"id": 1055, "seek": 493648, "start": 4946.5199999999995, "end": 4952.48, "text": " This is the number of factors right and we've got our M matrix", "tokens": [639, 307, 264, 1230, 295, 6771, 558, 293, 321, 600, 658, 527, 376, 8141], "temperature": 0.0, "avg_logprob": -0.27791740048316216, "compression_ratio": 1.8076923076923077, "no_speech_prob": 1.5056948541314341e-06}, {"id": 1056, "seek": 495248, "start": 4952.48, "end": 4963.24, "text": " Right so here's our number of movies, and here's our again number of factors and so remember we look up a", "tokens": [1779, 370, 510, 311, 527, 1230, 295, 6233, 11, 293, 510, 311, 527, 797, 1230, 295, 6771, 293, 370, 1604, 321, 574, 493, 257], "temperature": 0.0, "avg_logprob": -0.23218714047784675, "compression_ratio": 1.7375, "no_speech_prob": 1.3925427992944606e-06}, {"id": 1057, "seek": 495248, "start": 4966.599999999999, "end": 4968.44, "text": " Single user", "tokens": [31248, 4195], "temperature": 0.0, "avg_logprob": -0.23218714047784675, "compression_ratio": 1.7375, "no_speech_prob": 1.3925427992944606e-06}, {"id": 1058, "seek": 495248, "start": 4968.44, "end": 4974.0, "text": " We look up a single movie and let's grab them and concatenate them together", "tokens": [492, 574, 493, 257, 2167, 3169, 293, 718, 311, 4444, 552, 293, 1588, 7186, 473, 552, 1214], "temperature": 0.0, "avg_logprob": -0.23218714047784675, "compression_ratio": 1.7375, "no_speech_prob": 1.3925427992944606e-06}, {"id": 1059, "seek": 497400, "start": 4974.0, "end": 4981.36, "text": " All right, so here's like the user part. Here's the movie part, and then let's put that", "tokens": [1057, 558, 11, 370, 510, 311, 411, 264, 4195, 644, 13, 1692, 311, 264, 3169, 644, 11, 293, 550, 718, 311, 829, 300], "temperature": 0.0, "avg_logprob": -0.1907658576965332, "compression_ratio": 1.673913043478261, "no_speech_prob": 7.934485779514944e-07}, {"id": 1060, "seek": 497400, "start": 4982.64, "end": 4984.64, "text": " Through a matrix product", "tokens": [8927, 257, 8141, 1674], "temperature": 0.0, "avg_logprob": -0.1907658576965332, "compression_ratio": 1.673913043478261, "no_speech_prob": 7.934485779514944e-07}, {"id": 1061, "seek": 497400, "start": 4985.52, "end": 4990.96, "text": " All right, so that number of rows here is going to have to be the number of users plus the number of movies", "tokens": [1057, 558, 11, 370, 300, 1230, 295, 13241, 510, 307, 516, 281, 362, 281, 312, 264, 1230, 295, 5022, 1804, 264, 1230, 295, 6233], "temperature": 0.0, "avg_logprob": -0.1907658576965332, "compression_ratio": 1.673913043478261, "no_speech_prob": 7.934485779514944e-07}, {"id": 1062, "seek": 497400, "start": 4991.6, "end": 4993.6, "text": " Because that's how long that is and", "tokens": [1436, 300, 311, 577, 938, 300, 307, 293], "temperature": 0.0, "avg_logprob": -0.1907658576965332, "compression_ratio": 1.673913043478261, "no_speech_prob": 7.934485779514944e-07}, {"id": 1063, "seek": 497400, "start": 4994.24, "end": 4996.24, "text": " Then the number of columns", "tokens": [1396, 264, 1230, 295, 13766], "temperature": 0.0, "avg_logprob": -0.1907658576965332, "compression_ratio": 1.673913043478261, "no_speech_prob": 7.934485779514944e-07}, {"id": 1064, "seek": 497400, "start": 4996.84, "end": 4998.84, "text": " Can be anything we want?", "tokens": [1664, 312, 1340, 321, 528, 30], "temperature": 0.0, "avg_logprob": -0.1907658576965332, "compression_ratio": 1.673913043478261, "no_speech_prob": 7.934485779514944e-07}, {"id": 1065, "seek": 499884, "start": 4998.84, "end": 5006.68, "text": " Because we're going to take that so in this case we're going to pick 10 apparently so it's picked 10 and", "tokens": [1436, 321, 434, 516, 281, 747, 300, 370, 294, 341, 1389, 321, 434, 516, 281, 1888, 1266, 7970, 370, 309, 311, 6183, 1266, 293], "temperature": 0.0, "avg_logprob": -0.2778445685782084, "compression_ratio": 1.690721649484536, "no_speech_prob": 2.0261384179320885e-06}, {"id": 1066, "seek": 499884, "start": 5007.16, "end": 5009.16, "text": " Then we're going to stick that through a", "tokens": [1396, 321, 434, 516, 281, 2897, 300, 807, 257], "temperature": 0.0, "avg_logprob": -0.2778445685782084, "compression_ratio": 1.690721649484536, "no_speech_prob": 2.0261384179320885e-06}, {"id": 1067, "seek": 499884, "start": 5010.0, "end": 5012.0, "text": " value", "tokens": [2158], "temperature": 0.0, "avg_logprob": -0.2778445685782084, "compression_ratio": 1.690721649484536, "no_speech_prob": 2.0261384179320885e-06}, {"id": 1068, "seek": 499884, "start": 5012.0, "end": 5014.52, "text": " And then stick that through another", "tokens": [400, 550, 2897, 300, 807, 1071], "temperature": 0.0, "avg_logprob": -0.2778445685782084, "compression_ratio": 1.690721649484536, "no_speech_prob": 2.0261384179320885e-06}, {"id": 1069, "seek": 499884, "start": 5015.4400000000005, "end": 5022.4800000000005, "text": " Matrix which obviously needs to be of size 10 here, and then the number of columns is of size one", "tokens": [36274, 597, 2745, 2203, 281, 312, 295, 2744, 1266, 510, 11, 293, 550, 264, 1230, 295, 13766, 307, 295, 2744, 472], "temperature": 0.0, "avg_logprob": -0.2778445685782084, "compression_ratio": 1.690721649484536, "no_speech_prob": 2.0261384179320885e-06}, {"id": 1070, "seek": 502248, "start": 5022.48, "end": 5028.759999999999, "text": " Because we want to predict a single rating okay?", "tokens": [1436, 321, 528, 281, 6069, 257, 2167, 10990, 1392, 30], "temperature": 0.0, "avg_logprob": -0.22855587005615235, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.0261388726794394e-06}, {"id": 1071, "seek": 502248, "start": 5029.799999999999, "end": 5036.959999999999, "text": " And so that's our kind of flow chart of what's going on right it is a standard", "tokens": [400, 370, 300, 311, 527, 733, 295, 3095, 6927, 295, 437, 311, 516, 322, 558, 309, 307, 257, 3832], "temperature": 0.0, "avg_logprob": -0.22855587005615235, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.0261388726794394e-06}, {"id": 1072, "seek": 502248, "start": 5038.08, "end": 5043.86, "text": " I would call it a one hidden layer neural net it depends how you think of it like there's kind of an embedding layer", "tokens": [286, 576, 818, 309, 257, 472, 7633, 4583, 18161, 2533, 309, 5946, 577, 291, 519, 295, 309, 411, 456, 311, 733, 295, 364, 12240, 3584, 4583], "temperature": 0.0, "avg_logprob": -0.22855587005615235, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.0261388726794394e-06}, {"id": 1073, "seek": 504386, "start": 5043.86, "end": 5051.62, "text": " But because this is linear, and this is linear the two together is really one linear layer right? It's just a computational convenience", "tokens": [583, 570, 341, 307, 8213, 11, 293, 341, 307, 8213, 264, 732, 1214, 307, 534, 472, 8213, 4583, 558, 30, 467, 311, 445, 257, 28270, 19283], "temperature": 0.0, "avg_logprob": -0.20350642320586415, "compression_ratio": 1.8205128205128205, "no_speech_prob": 9.87462158263952e-07}, {"id": 1074, "seek": 504386, "start": 5052.46, "end": 5058.12, "text": " So it's really got one hidden layer because it's just got one layer before this nonlinear activation", "tokens": [407, 309, 311, 534, 658, 472, 7633, 4583, 570, 309, 311, 445, 658, 472, 4583, 949, 341, 2107, 28263, 24433], "temperature": 0.0, "avg_logprob": -0.20350642320586415, "compression_ratio": 1.8205128205128205, "no_speech_prob": 9.87462158263952e-07}, {"id": 1075, "seek": 504386, "start": 5060.42, "end": 5062.42, "text": " So in order to create a", "tokens": [407, 294, 1668, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.20350642320586415, "compression_ratio": 1.8205128205128205, "no_speech_prob": 9.87462158263952e-07}, {"id": 1076, "seek": 504386, "start": 5064.74, "end": 5069.42, "text": " Linear layer with some number of rows and some number of columns you just go and end up linear", "tokens": [14670, 289, 4583, 365, 512, 1230, 295, 13241, 293, 512, 1230, 295, 13766, 291, 445, 352, 293, 917, 493, 8213], "temperature": 0.0, "avg_logprob": -0.20350642320586415, "compression_ratio": 1.8205128205128205, "no_speech_prob": 9.87462158263952e-07}, {"id": 1077, "seek": 506942, "start": 5069.42, "end": 5073.58, "text": " In the machine learning class this week", "tokens": [682, 264, 3479, 2539, 1508, 341, 1243], "temperature": 0.0, "avg_logprob": -0.15594298116276772, "compression_ratio": 1.660633484162896, "no_speech_prob": 1.6797247326394427e-06}, {"id": 1078, "seek": 506942, "start": 5073.86, "end": 5080.68, "text": " We learned how to create a linear layer from scratch by creating our own weight matrix and our own biases", "tokens": [492, 3264, 577, 281, 1884, 257, 8213, 4583, 490, 8459, 538, 4084, 527, 1065, 3364, 8141, 293, 527, 1065, 32152], "temperature": 0.0, "avg_logprob": -0.15594298116276772, "compression_ratio": 1.660633484162896, "no_speech_prob": 1.6797247326394427e-06}, {"id": 1079, "seek": 506942, "start": 5080.86, "end": 5087.24, "text": " So if you want to check that out you can do so there right, but it's the same basic technique. We've already seen", "tokens": [407, 498, 291, 528, 281, 1520, 300, 484, 291, 393, 360, 370, 456, 558, 11, 457, 309, 311, 264, 912, 3875, 6532, 13, 492, 600, 1217, 1612], "temperature": 0.0, "avg_logprob": -0.15594298116276772, "compression_ratio": 1.660633484162896, "no_speech_prob": 1.6797247326394427e-06}, {"id": 1080, "seek": 506942, "start": 5090.38, "end": 5093.26, "text": " So we create our embeddings we create our two linear layers", "tokens": [407, 321, 1884, 527, 12240, 29432, 321, 1884, 527, 732, 8213, 7914], "temperature": 0.0, "avg_logprob": -0.15594298116276772, "compression_ratio": 1.660633484162896, "no_speech_prob": 1.6797247326394427e-06}, {"id": 1081, "seek": 506942, "start": 5093.9, "end": 5096.02, "text": " That's all the stuff that we need to start with", "tokens": [663, 311, 439, 264, 1507, 300, 321, 643, 281, 722, 365], "temperature": 0.0, "avg_logprob": -0.15594298116276772, "compression_ratio": 1.660633484162896, "no_speech_prob": 1.6797247326394427e-06}, {"id": 1082, "seek": 509602, "start": 5096.02, "end": 5102.46, "text": " You know really if I wanted to make this more general. I would have had another parameter here called like", "tokens": [509, 458, 534, 498, 286, 1415, 281, 652, 341, 544, 2674, 13, 286, 576, 362, 632, 1071, 13075, 510, 1219, 411], "temperature": 0.0, "avg_logprob": -0.22424542772900927, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1083, "seek": 509602, "start": 5103.9800000000005, "end": 5105.700000000001, "text": " num hidden", "tokens": [1031, 7633], "temperature": 0.0, "avg_logprob": -0.22424542772900927, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1084, "seek": 509602, "start": 5105.700000000001, "end": 5107.700000000001, "text": " You know equals", "tokens": [509, 458, 6915], "temperature": 0.0, "avg_logprob": -0.22424542772900927, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1085, "seek": 509602, "start": 5108.540000000001, "end": 5111.540000000001, "text": " Equals 10, and then this would be a parameter", "tokens": [15624, 1124, 1266, 11, 293, 550, 341, 576, 312, 257, 13075], "temperature": 0.0, "avg_logprob": -0.22424542772900927, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1086, "seek": 509602, "start": 5112.26, "end": 5117.4400000000005, "text": " And then you could like more easily play around with different numbers of activations", "tokens": [400, 550, 291, 727, 411, 544, 3612, 862, 926, 365, 819, 3547, 295, 2430, 763], "temperature": 0.0, "avg_logprob": -0.22424542772900927, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1087, "seek": 509602, "start": 5117.46, "end": 5123.740000000001, "text": " So when we say like okay in this layer, I'm going to create a layer with this many activations all I mean", "tokens": [407, 562, 321, 584, 411, 1392, 294, 341, 4583, 11, 286, 478, 516, 281, 1884, 257, 4583, 365, 341, 867, 2430, 763, 439, 286, 914], "temperature": 0.0, "avg_logprob": -0.22424542772900927, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1088, "seek": 512374, "start": 5123.74, "end": 5126.179999999999, "text": " Assuming it's a fully connected layer is", "tokens": [6281, 24919, 309, 311, 257, 4498, 4582, 4583, 307], "temperature": 0.0, "avg_logprob": -0.21323492050170897, "compression_ratio": 1.748936170212766, "no_speech_prob": 1.760337454470573e-06}, {"id": 1089, "seek": 512374, "start": 5126.82, "end": 5133.08, "text": " My linear layer has how many columns in its white matrix. That's how many activations it creates", "tokens": [1222, 8213, 4583, 575, 577, 867, 13766, 294, 1080, 2418, 8141, 13, 663, 311, 577, 867, 2430, 763, 309, 7829], "temperature": 0.0, "avg_logprob": -0.21323492050170897, "compression_ratio": 1.748936170212766, "no_speech_prob": 1.760337454470573e-06}, {"id": 1090, "seek": 512374, "start": 5134.3, "end": 5141.4, "text": " All right, so we grab our users and movies we put them through our embedding matrix, and then we can catenate them together", "tokens": [1057, 558, 11, 370, 321, 4444, 527, 5022, 293, 6233, 321, 829, 552, 807, 527, 12240, 3584, 8141, 11, 293, 550, 321, 393, 3857, 268, 473, 552, 1214], "temperature": 0.0, "avg_logprob": -0.21323492050170897, "compression_ratio": 1.748936170212766, "no_speech_prob": 1.760337454470573e-06}, {"id": 1091, "seek": 512374, "start": 5141.62, "end": 5143.62, "text": " Okay, so torch dot cat", "tokens": [1033, 11, 370, 27822, 5893, 3857], "temperature": 0.0, "avg_logprob": -0.21323492050170897, "compression_ratio": 1.748936170212766, "no_speech_prob": 1.760337454470573e-06}, {"id": 1092, "seek": 512374, "start": 5144.0199999999995, "end": 5150.88, "text": " Concatenates them together on the first dimension so in other words we can catenate the columns together to create longer rows", "tokens": [18200, 7186, 1024, 552, 1214, 322, 264, 700, 10139, 370, 294, 661, 2283, 321, 393, 3857, 268, 473, 264, 13766, 1214, 281, 1884, 2854, 13241], "temperature": 0.0, "avg_logprob": -0.21323492050170897, "compression_ratio": 1.748936170212766, "no_speech_prob": 1.760337454470573e-06}, {"id": 1093, "seek": 515088, "start": 5150.88, "end": 5153.9800000000005, "text": " Okay, so that's concatenating on dimension one", "tokens": [1033, 11, 370, 300, 311, 1588, 7186, 990, 322, 10139, 472], "temperature": 0.0, "avg_logprob": -0.27786582853735947, "compression_ratio": 1.5260663507109005, "no_speech_prob": 8.990942887976416e-07}, {"id": 1094, "seek": 515088, "start": 5156.96, "end": 5159.88, "text": " Dropout will come back to in a moment. We've looked at that briefly", "tokens": [17675, 346, 486, 808, 646, 281, 294, 257, 1623, 13, 492, 600, 2956, 412, 300, 10515], "temperature": 0.0, "avg_logprob": -0.27786582853735947, "compression_ratio": 1.5260663507109005, "no_speech_prob": 8.990942887976416e-07}, {"id": 1095, "seek": 515088, "start": 5162.08, "end": 5166.64, "text": " So then having done that we'll put it through that linear layer we had", "tokens": [407, 550, 1419, 1096, 300, 321, 603, 829, 309, 807, 300, 8213, 4583, 321, 632], "temperature": 0.0, "avg_logprob": -0.27786582853735947, "compression_ratio": 1.5260663507109005, "no_speech_prob": 8.990942887976416e-07}, {"id": 1096, "seek": 515088, "start": 5167.4800000000005, "end": 5173.32, "text": " We'll do our value and you'll notice that value is again inside our capital F", "tokens": [492, 603, 360, 527, 2158, 293, 291, 603, 3449, 300, 2158, 307, 797, 1854, 527, 4238, 479], "temperature": 0.0, "avg_logprob": -0.27786582853735947, "compression_ratio": 1.5260663507109005, "no_speech_prob": 8.990942887976416e-07}, {"id": 1097, "seek": 515088, "start": 5173.68, "end": 5177.28, "text": " And end up functional right is just a function so remember", "tokens": [400, 917, 493, 11745, 558, 307, 445, 257, 2445, 370, 1604], "temperature": 0.0, "avg_logprob": -0.27786582853735947, "compression_ratio": 1.5260663507109005, "no_speech_prob": 8.990942887976416e-07}, {"id": 1098, "seek": 517728, "start": 5177.28, "end": 5184.599999999999, "text": " Activation functions are basically things that take one activation in and spit one activation out in this case", "tokens": [28550, 399, 6828, 366, 1936, 721, 300, 747, 472, 24433, 294, 293, 22127, 472, 24433, 484, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.2238330107468825, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.7330457922071218e-06}, {"id": 1099, "seek": 517728, "start": 5184.96, "end": 5187.48, "text": " taking something that can have negatives or positives and", "tokens": [1940, 746, 300, 393, 362, 40019, 420, 35127, 293], "temperature": 0.0, "avg_logprob": -0.2238330107468825, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.7330457922071218e-06}, {"id": 1100, "seek": 517728, "start": 5188.719999999999, "end": 5191.44, "text": " Truncate the negatives to zero that's all really does", "tokens": [1765, 409, 66, 473, 264, 40019, 281, 4018, 300, 311, 439, 534, 775], "temperature": 0.0, "avg_logprob": -0.2238330107468825, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.7330457922071218e-06}, {"id": 1101, "seek": 517728, "start": 5193.92, "end": 5195.92, "text": " And then here's our sigmoid", "tokens": [400, 550, 510, 311, 527, 4556, 3280, 327], "temperature": 0.0, "avg_logprob": -0.2238330107468825, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.7330457922071218e-06}, {"id": 1102, "seek": 517728, "start": 5196.679999999999, "end": 5200.28, "text": " So that's that that is now a genuine", "tokens": [407, 300, 311, 300, 300, 307, 586, 257, 16699], "temperature": 0.0, "avg_logprob": -0.2238330107468825, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.7330457922071218e-06}, {"id": 1103, "seek": 517728, "start": 5201.32, "end": 5203.32, "text": " Neural network, I don't know if we get to call it deep", "tokens": [1734, 1807, 3209, 11, 286, 500, 380, 458, 498, 321, 483, 281, 818, 309, 2452], "temperature": 0.0, "avg_logprob": -0.2238330107468825, "compression_ratio": 1.7231404958677685, "no_speech_prob": 1.7330457922071218e-06}, {"id": 1104, "seek": 520332, "start": 5203.32, "end": 5208.799999999999, "text": " It's only got one hidden layer, but it's definitely a neural network right and so we can now construct it", "tokens": [467, 311, 787, 658, 472, 7633, 4583, 11, 457, 309, 311, 2138, 257, 18161, 3209, 558, 293, 370, 321, 393, 586, 7690, 309], "temperature": 0.0, "avg_logprob": -0.19558898333845467, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.2959115995035972e-06}, {"id": 1105, "seek": 520332, "start": 5209.0, "end": 5211.0, "text": " We can put it on the GPU", "tokens": [492, 393, 829, 309, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.19558898333845467, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.2959115995035972e-06}, {"id": 1106, "seek": 520332, "start": 5211.0, "end": 5214.38, "text": " We can create an optimizer for it, and we can fit it", "tokens": [492, 393, 1884, 364, 5028, 6545, 337, 309, 11, 293, 321, 393, 3318, 309], "temperature": 0.0, "avg_logprob": -0.19558898333845467, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.2959115995035972e-06}, {"id": 1107, "seek": 520332, "start": 5215.44, "end": 5219.48, "text": " Now you'll notice there's one other thing. I've been passing to fit which is", "tokens": [823, 291, 603, 3449, 456, 311, 472, 661, 551, 13, 286, 600, 668, 8437, 281, 3318, 597, 307], "temperature": 0.0, "avg_logprob": -0.19558898333845467, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.2959115995035972e-06}, {"id": 1108, "seek": 520332, "start": 5220.0, "end": 5226.299999999999, "text": " What loss function are we trying to minimize? Okay, and this is the mean squared error loss and again. It's inside F", "tokens": [708, 4470, 2445, 366, 321, 1382, 281, 17522, 30, 1033, 11, 293, 341, 307, 264, 914, 8889, 6713, 4470, 293, 797, 13, 467, 311, 1854, 479], "temperature": 0.0, "avg_logprob": -0.19558898333845467, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.2959115995035972e-06}, {"id": 1109, "seek": 520332, "start": 5226.96, "end": 5229.66, "text": " Okay, pretty much all the functions are inside it okay", "tokens": [1033, 11, 1238, 709, 439, 264, 6828, 366, 1854, 309, 1392], "temperature": 0.0, "avg_logprob": -0.19558898333845467, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.2959115995035972e-06}, {"id": 1110, "seek": 520332, "start": 5230.5599999999995, "end": 5231.759999999999, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.19558898333845467, "compression_ratio": 1.6415094339622642, "no_speech_prob": 2.2959115995035972e-06}, {"id": 1111, "seek": 523176, "start": 5231.76, "end": 5238.7, "text": " One of the things that you have to pass fit is something saying like how do you score this what counts as good or bad?", "tokens": [1485, 295, 264, 721, 300, 291, 362, 281, 1320, 3318, 307, 746, 1566, 411, 577, 360, 291, 6175, 341, 437, 14893, 382, 665, 420, 1578, 30], "temperature": 0.0, "avg_logprob": -0.1536704414769223, "compression_ratio": 1.6508620689655173, "no_speech_prob": 4.710854227596428e-06}, {"id": 1112, "seek": 523176, "start": 5239.84, "end": 5247.820000000001, "text": " So Jeremy now that we have a real neural net do we have to use the same number of embeddings for users and", "tokens": [407, 17809, 586, 300, 321, 362, 257, 957, 18161, 2533, 360, 321, 362, 281, 764, 264, 912, 1230, 295, 12240, 29432, 337, 5022, 293], "temperature": 0.0, "avg_logprob": -0.1536704414769223, "compression_ratio": 1.6508620689655173, "no_speech_prob": 4.710854227596428e-06}, {"id": 1113, "seek": 523176, "start": 5248.280000000001, "end": 5256.08, "text": " That's a great question. You don't know it's absolutely right you don't and so like we've got a lot of benefits here right because if we", "tokens": [663, 311, 257, 869, 1168, 13, 509, 500, 380, 458, 309, 311, 3122, 558, 291, 500, 380, 293, 370, 411, 321, 600, 658, 257, 688, 295, 5311, 510, 558, 570, 498, 321], "temperature": 0.0, "avg_logprob": -0.1536704414769223, "compression_ratio": 1.6508620689655173, "no_speech_prob": 4.710854227596428e-06}, {"id": 1114, "seek": 525608, "start": 5256.08, "end": 5260.84, "text": " You know think about you", "tokens": [509, 458, 519, 466, 291], "temperature": 0.0, "avg_logprob": -0.21120467552771935, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0348502428314532e-06}, {"id": 1115, "seek": 525608, "start": 5263.68, "end": 5264.72, "text": " Know", "tokens": [10265], "temperature": 0.0, "avg_logprob": -0.21120467552771935, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0348502428314532e-06}, {"id": 1116, "seek": 525608, "start": 5264.72, "end": 5266.72, "text": " We're grabbing a user", "tokens": [492, 434, 23771, 257, 4195], "temperature": 0.0, "avg_logprob": -0.21120467552771935, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0348502428314532e-06}, {"id": 1117, "seek": 525608, "start": 5266.88, "end": 5272.0, "text": " Embedding or concatenating it with a movie embedding which maybe is like I don't know some different size", "tokens": [24234, 292, 3584, 420, 1588, 7186, 990, 309, 365, 257, 3169, 12240, 3584, 597, 1310, 307, 411, 286, 500, 380, 458, 512, 819, 2744], "temperature": 0.0, "avg_logprob": -0.21120467552771935, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0348502428314532e-06}, {"id": 1118, "seek": 525608, "start": 5273.76, "end": 5280.04, "text": " But then also perhaps we looked up the genre of the movie and like you know there's actually a", "tokens": [583, 550, 611, 4317, 321, 2956, 493, 264, 11022, 295, 264, 3169, 293, 411, 291, 458, 456, 311, 767, 257], "temperature": 0.0, "avg_logprob": -0.21120467552771935, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0348502428314532e-06}, {"id": 1119, "seek": 525608, "start": 5280.5599999999995, "end": 5283.0, "text": " Embedding matrix of like number of genres", "tokens": [24234, 292, 3584, 8141, 295, 411, 1230, 295, 30057], "temperature": 0.0, "avg_logprob": -0.21120467552771935, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0348502428314532e-06}, {"id": 1120, "seek": 528300, "start": 5283.0, "end": 5289.76, "text": " by I don't know three or something and so like we could then concatenate like a genre embedding and", "tokens": [538, 286, 500, 380, 458, 1045, 420, 746, 293, 370, 411, 321, 727, 550, 1588, 7186, 473, 411, 257, 11022, 12240, 3584, 293], "temperature": 0.0, "avg_logprob": -0.1942497295337719, "compression_ratio": 1.662037037037037, "no_speech_prob": 2.1907769678364275e-06}, {"id": 1121, "seek": 528300, "start": 5289.92, "end": 5297.12, "text": " Then maybe the timestamp is in here as a continuous number right and so then that whole thing we can then feed into", "tokens": [1396, 1310, 264, 49108, 1215, 307, 294, 510, 382, 257, 10957, 1230, 558, 293, 370, 550, 300, 1379, 551, 321, 393, 550, 3154, 666], "temperature": 0.0, "avg_logprob": -0.1942497295337719, "compression_ratio": 1.662037037037037, "no_speech_prob": 2.1907769678364275e-06}, {"id": 1122, "seek": 528300, "start": 5298.08, "end": 5300.08, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.1942497295337719, "compression_ratio": 1.662037037037037, "no_speech_prob": 2.1907769678364275e-06}, {"id": 1123, "seek": 528300, "start": 5301.88, "end": 5304.76, "text": " Our neural net right and then at the end", "tokens": [2621, 18161, 2533, 558, 293, 550, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.1942497295337719, "compression_ratio": 1.662037037037037, "no_speech_prob": 2.1907769678364275e-06}, {"id": 1124, "seek": 528300, "start": 5307.2, "end": 5312.76, "text": " Remember our final non-linearity with a sigmoid right so we can now recognize that this thing", "tokens": [5459, 527, 2572, 2107, 12, 1889, 17409, 365, 257, 4556, 3280, 327, 558, 370, 321, 393, 586, 5521, 300, 341, 551], "temperature": 0.0, "avg_logprob": -0.1942497295337719, "compression_ratio": 1.662037037037037, "no_speech_prob": 2.1907769678364275e-06}, {"id": 1125, "seek": 531276, "start": 5312.76, "end": 5316.64, "text": " We did where we did sigmoid times max rating minus min rating plus blah blah blah", "tokens": [492, 630, 689, 321, 630, 4556, 3280, 327, 1413, 11469, 10990, 3175, 923, 10990, 1804, 12288, 12288, 12288], "temperature": 0.0, "avg_logprob": -0.2257494122794505, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.041584704988054e-06}, {"id": 1126, "seek": 531276, "start": 5317.4400000000005, "end": 5319.4400000000005, "text": " Is actually just another?", "tokens": [1119, 767, 445, 1071, 30], "temperature": 0.0, "avg_logprob": -0.2257494122794505, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.041584704988054e-06}, {"id": 1127, "seek": 531276, "start": 5320.24, "end": 5321.4800000000005, "text": " non-linear", "tokens": [2107, 12, 28263], "temperature": 0.0, "avg_logprob": -0.2257494122794505, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.041584704988054e-06}, {"id": 1128, "seek": 531276, "start": 5321.4800000000005, "end": 5324.56, "text": " Activation function right and remember in our last layer", "tokens": [28550, 399, 2445, 558, 293, 1604, 294, 527, 1036, 4583], "temperature": 0.0, "avg_logprob": -0.2257494122794505, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.041584704988054e-06}, {"id": 1129, "seek": 531276, "start": 5325.12, "end": 5328.26, "text": " We use generally different kinds of activation functions", "tokens": [492, 764, 5101, 819, 3685, 295, 24433, 6828], "temperature": 0.0, "avg_logprob": -0.2257494122794505, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.041584704988054e-06}, {"id": 1130, "seek": 531276, "start": 5328.88, "end": 5334.38, "text": " So as we said we don't need any activation function at all right we could just do", "tokens": [407, 382, 321, 848, 321, 500, 380, 643, 604, 24433, 2445, 412, 439, 558, 321, 727, 445, 360], "temperature": 0.0, "avg_logprob": -0.2257494122794505, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.041584704988054e-06}, {"id": 1131, "seek": 531276, "start": 5337.08, "end": 5341.22, "text": " That right but by not having any nonlinear activation function", "tokens": [663, 558, 457, 538, 406, 1419, 604, 2107, 28263, 24433, 2445], "temperature": 0.0, "avg_logprob": -0.2257494122794505, "compression_ratio": 1.7453703703703705, "no_speech_prob": 3.041584704988054e-06}, {"id": 1132, "seek": 534122, "start": 5341.22, "end": 5347.2, "text": " We're just making it harder, so that's why we put the sigmoid in there as well, okay?", "tokens": [492, 434, 445, 1455, 309, 6081, 11, 370, 300, 311, 983, 321, 829, 264, 4556, 3280, 327, 294, 456, 382, 731, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15562452588762557, "compression_ratio": 1.650943396226415, "no_speech_prob": 5.338125447451603e-06}, {"id": 1133, "seek": 534122, "start": 5347.96, "end": 5351.0, "text": " so we can then fit it in the usual way and", "tokens": [370, 321, 393, 550, 3318, 309, 294, 264, 7713, 636, 293], "temperature": 0.0, "avg_logprob": -0.15562452588762557, "compression_ratio": 1.650943396226415, "no_speech_prob": 5.338125447451603e-06}, {"id": 1134, "seek": 534122, "start": 5353.08, "end": 5357.84, "text": " There we go you know interestingly we actually got a better score than we did with our", "tokens": [821, 321, 352, 291, 458, 25873, 321, 767, 658, 257, 1101, 6175, 813, 321, 630, 365, 527], "temperature": 0.0, "avg_logprob": -0.15562452588762557, "compression_ratio": 1.650943396226415, "no_speech_prob": 5.338125447451603e-06}, {"id": 1135, "seek": 534122, "start": 5359.96, "end": 5361.96, "text": " This model", "tokens": [639, 2316], "temperature": 0.0, "avg_logprob": -0.15562452588762557, "compression_ratio": 1.650943396226415, "no_speech_prob": 5.338125447451603e-06}, {"id": 1136, "seek": 534122, "start": 5362.04, "end": 5367.52, "text": " So it'll be interesting to try training this with stochastic gradient descent with restarts and see if it's actually better", "tokens": [407, 309, 603, 312, 1880, 281, 853, 3097, 341, 365, 342, 8997, 2750, 16235, 23475, 365, 1472, 11814, 293, 536, 498, 309, 311, 767, 1101], "temperature": 0.0, "avg_logprob": -0.15562452588762557, "compression_ratio": 1.650943396226415, "no_speech_prob": 5.338125447451603e-06}, {"id": 1137, "seek": 536752, "start": 5367.52, "end": 5374.4400000000005, "text": " You know maybe you can play around with the number of hidden layers and the dropout and whatever else and see if you can come", "tokens": [509, 458, 1310, 291, 393, 862, 926, 365, 264, 1230, 295, 7633, 7914, 293, 264, 3270, 346, 293, 2035, 1646, 293, 536, 498, 291, 393, 808], "temperature": 0.0, "avg_logprob": -0.25008286696213944, "compression_ratio": 1.50625, "no_speech_prob": 1.6797273474367103e-06}, {"id": 1138, "seek": 536752, "start": 5374.4400000000005, "end": 5377.8, "text": " up with you know get a better answer than", "tokens": [493, 365, 291, 458, 483, 257, 1101, 1867, 813], "temperature": 0.0, "avg_logprob": -0.25008286696213944, "compression_ratio": 1.50625, "no_speech_prob": 1.6797273474367103e-06}, {"id": 1139, "seek": 536752, "start": 5382.88, "end": 5384.88, "text": " Point", "tokens": [12387], "temperature": 0.0, "avg_logprob": -0.25008286696213944, "compression_ratio": 1.50625, "no_speech_prob": 1.6797273474367103e-06}, {"id": 1140, "seek": 536752, "start": 5385.360000000001, "end": 5387.360000000001, "text": " Seven six ish", "tokens": [14868, 2309, 307, 71], "temperature": 0.0, "avg_logprob": -0.25008286696213944, "compression_ratio": 1.50625, "no_speech_prob": 1.6797273474367103e-06}, {"id": 1141, "seek": 536752, "start": 5387.88, "end": 5392.6, "text": " Okay, so so general so this is like if you were going", "tokens": [1033, 11, 370, 370, 2674, 370, 341, 307, 411, 498, 291, 645, 516], "temperature": 0.0, "avg_logprob": -0.25008286696213944, "compression_ratio": 1.50625, "no_speech_prob": 1.6797273474367103e-06}, {"id": 1142, "seek": 539260, "start": 5392.6, "end": 5397.56, "text": " Deep into collaborative filtering at your workplace or whatever this wouldn't be a bad way to go", "tokens": [14895, 666, 16555, 30822, 412, 428, 15328, 420, 2035, 341, 2759, 380, 312, 257, 1578, 636, 281, 352], "temperature": 0.0, "avg_logprob": -0.1606892246310994, "compression_ratio": 1.5874125874125875, "no_speech_prob": 6.786718813600601e-07}, {"id": 1143, "seek": 539260, "start": 5397.56, "end": 5402.84, "text": " I could like I'd start out with like oh, okay. Here's like a club for the data set 30 and fast AI", "tokens": [286, 727, 411, 286, 1116, 722, 484, 365, 411, 1954, 11, 1392, 13, 1692, 311, 411, 257, 6482, 337, 264, 1412, 992, 2217, 293, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.1606892246310994, "compression_ratio": 1.5874125874125875, "no_speech_prob": 6.786718813600601e-07}, {"id": 1144, "seek": 539260, "start": 5403.52, "end": 5405.56, "text": " Get learner. There's you know not much", "tokens": [3240, 33347, 13, 821, 311, 291, 458, 406, 709], "temperature": 0.0, "avg_logprob": -0.1606892246310994, "compression_ratio": 1.5874125874125875, "no_speech_prob": 6.786718813600601e-07}, {"id": 1145, "seek": 539260, "start": 5405.56, "end": 5411.88, "text": " I can send it basically number of factors is about the only thing that I pass in I can learn for a while", "tokens": [286, 393, 2845, 309, 1936, 1230, 295, 6771, 307, 466, 264, 787, 551, 300, 286, 1320, 294, 286, 393, 1466, 337, 257, 1339], "temperature": 0.0, "avg_logprob": -0.1606892246310994, "compression_ratio": 1.5874125874125875, "no_speech_prob": 6.786718813600601e-07}, {"id": 1146, "seek": 539260, "start": 5412.240000000001, "end": 5416.4400000000005, "text": " Maybe try a few different approaches, and then you're like okay. There's like", "tokens": [2704, 853, 257, 1326, 819, 11587, 11, 293, 550, 291, 434, 411, 1392, 13, 821, 311, 411], "temperature": 0.0, "avg_logprob": -0.1606892246310994, "compression_ratio": 1.5874125874125875, "no_speech_prob": 6.786718813600601e-07}, {"id": 1147, "seek": 539260, "start": 5417.08, "end": 5419.08, "text": " That's how I go if I use the defaults", "tokens": [663, 311, 577, 286, 352, 498, 286, 764, 264, 7576, 82], "temperature": 0.0, "avg_logprob": -0.1606892246310994, "compression_ratio": 1.5874125874125875, "no_speech_prob": 6.786718813600601e-07}, {"id": 1148, "seek": 541908, "start": 5419.08, "end": 5421.08, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2674471063816801, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.338120899978094e-06}, {"id": 1149, "seek": 541908, "start": 5421.08, "end": 5424.64, "text": " How do I make it better and then I'd be like digging into the code and saying like okay", "tokens": [1012, 360, 286, 652, 309, 1101, 293, 550, 286, 1116, 312, 411, 17343, 666, 264, 3089, 293, 1566, 411, 1392], "temperature": 0.0, "avg_logprob": -0.2674471063816801, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.338120899978094e-06}, {"id": 1150, "seek": 541908, "start": 5424.64, "end": 5428.5199999999995, "text": " What would Jeremy actually do here? This is actually what I want you know and", "tokens": [708, 576, 17809, 767, 360, 510, 30, 639, 307, 767, 437, 286, 528, 291, 458, 293], "temperature": 0.0, "avg_logprob": -0.2674471063816801, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.338120899978094e-06}, {"id": 1151, "seek": 541908, "start": 5430.04, "end": 5433.14, "text": " So one of the nice things about the neural net approach", "tokens": [407, 472, 295, 264, 1481, 721, 466, 264, 18161, 2533, 3109], "temperature": 0.0, "avg_logprob": -0.2674471063816801, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.338120899978094e-06}, {"id": 1152, "seek": 541908, "start": 5434.36, "end": 5436.76, "text": " Is that you know as you net mentioned?", "tokens": [1119, 300, 291, 458, 382, 291, 2533, 2835, 30], "temperature": 0.0, "avg_logprob": -0.2674471063816801, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.338120899978094e-06}, {"id": 1153, "seek": 541908, "start": 5437.44, "end": 5439.44, "text": " We can have different numbers of", "tokens": [492, 393, 362, 819, 3547, 295], "temperature": 0.0, "avg_logprob": -0.2674471063816801, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.338120899978094e-06}, {"id": 1154, "seek": 541908, "start": 5440.2, "end": 5441.68, "text": " embeddings", "tokens": [12240, 29432], "temperature": 0.0, "avg_logprob": -0.2674471063816801, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.338120899978094e-06}, {"id": 1155, "seek": 541908, "start": 5441.68, "end": 5445.44, "text": " We can choose how many hidden and we can also choose", "tokens": [492, 393, 2826, 577, 867, 7633, 293, 321, 393, 611, 2826], "temperature": 0.0, "avg_logprob": -0.2674471063816801, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.338120899978094e-06}, {"id": 1156, "seek": 544544, "start": 5445.44, "end": 5448.44, "text": " dropout right so", "tokens": [3270, 346, 558, 370], "temperature": 0.0, "avg_logprob": -0.3017000226832148, "compression_ratio": 1.4969325153374233, "no_speech_prob": 1.4144719671094208e-06}, {"id": 1157, "seek": 544544, "start": 5450.0, "end": 5455.599999999999, "text": " So what we're actually doing is we haven't just got really you that we're also going like okay, let's", "tokens": [407, 437, 321, 434, 767, 884, 307, 321, 2378, 380, 445, 658, 534, 291, 300, 321, 434, 611, 516, 411, 1392, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.3017000226832148, "compression_ratio": 1.4969325153374233, "no_speech_prob": 1.4144719671094208e-06}, {"id": 1158, "seek": 544544, "start": 5461.28, "end": 5463.879999999999, "text": " Let's delete a few things at random", "tokens": [961, 311, 12097, 257, 1326, 721, 412, 4974], "temperature": 0.0, "avg_logprob": -0.3017000226832148, "compression_ratio": 1.4969325153374233, "no_speech_prob": 1.4144719671094208e-06}, {"id": 1159, "seek": 544544, "start": 5465.679999999999, "end": 5469.32, "text": " Right that's dropout. That's when this case we were deleting", "tokens": [1779, 300, 311, 3270, 346, 13, 663, 311, 562, 341, 1389, 321, 645, 48946], "temperature": 0.0, "avg_logprob": -0.3017000226832148, "compression_ratio": 1.4969325153374233, "no_speech_prob": 1.4144719671094208e-06}, {"id": 1160, "seek": 544544, "start": 5470.839999999999, "end": 5472.919999999999, "text": " After the first linear layer", "tokens": [2381, 264, 700, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.3017000226832148, "compression_ratio": 1.4969325153374233, "no_speech_prob": 1.4144719671094208e-06}, {"id": 1161, "seek": 547292, "start": 5472.92, "end": 5474.92, "text": " 75% of them", "tokens": [9562, 4, 295, 552], "temperature": 0.0, "avg_logprob": -0.1956790384620127, "compression_ratio": 1.7217391304347827, "no_speech_prob": 2.3687887278356357e-06}, {"id": 1162, "seek": 547292, "start": 5475.68, "end": 5479.96, "text": " Right and then after the second linear layer 75% of them so we can add a whole lot of regularization", "tokens": [1779, 293, 550, 934, 264, 1150, 8213, 4583, 9562, 4, 295, 552, 370, 321, 393, 909, 257, 1379, 688, 295, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.1956790384620127, "compression_ratio": 1.7217391304347827, "no_speech_prob": 2.3687887278356357e-06}, {"id": 1163, "seek": 547292, "start": 5480.52, "end": 5485.12, "text": " Yeah, so you know this it kind of feels like the this this embedding net", "tokens": [865, 11, 370, 291, 458, 341, 309, 733, 295, 3417, 411, 264, 341, 341, 12240, 3584, 2533], "temperature": 0.0, "avg_logprob": -0.1956790384620127, "compression_ratio": 1.7217391304347827, "no_speech_prob": 2.3687887278356357e-06}, {"id": 1164, "seek": 547292, "start": 5486.96, "end": 5492.4, "text": " You know you could you could change this again we could like have it so that we could pass into the constructor", "tokens": [509, 458, 291, 727, 291, 727, 1319, 341, 797, 321, 727, 411, 362, 309, 370, 300, 321, 727, 1320, 666, 264, 47479], "temperature": 0.0, "avg_logprob": -0.1956790384620127, "compression_ratio": 1.7217391304347827, "no_speech_prob": 2.3687887278356357e-06}, {"id": 1165, "seek": 547292, "start": 5496.4400000000005, "end": 5500.88, "text": " Well if you want to make it look as much as possible like what we had before we could pass in peas", "tokens": [1042, 498, 291, 528, 281, 652, 309, 574, 382, 709, 382, 1944, 411, 437, 321, 632, 949, 321, 727, 1320, 294, 24494], "temperature": 0.0, "avg_logprob": -0.1956790384620127, "compression_ratio": 1.7217391304347827, "no_speech_prob": 2.3687887278356357e-06}, {"id": 1166, "seek": 550088, "start": 5500.88, "end": 5502.28, "text": " peas", "tokens": [24494], "temperature": 0.0, "avg_logprob": -0.4023710015701921, "compression_ratio": 1.3717948717948718, "no_speech_prob": 6.962212410144275e-06}, {"id": 1167, "seek": 550088, "start": 5502.28, "end": 5504.28, "text": " equals", "tokens": [6915], "temperature": 0.0, "avg_logprob": -0.4023710015701921, "compression_ratio": 1.3717948717948718, "no_speech_prob": 6.962212410144275e-06}, {"id": 1168, "seek": 550088, "start": 5504.28, "end": 5506.28, "text": " 0.75", "tokens": [1958, 13, 11901], "temperature": 0.0, "avg_logprob": -0.4023710015701921, "compression_ratio": 1.3717948717948718, "no_speech_prob": 6.962212410144275e-06}, {"id": 1169, "seek": 550088, "start": 5506.28, "end": 5507.8, "text": " 0.75", "tokens": [1958, 13, 11901], "temperature": 0.0, "avg_logprob": -0.4023710015701921, "compression_ratio": 1.3717948717948718, "no_speech_prob": 6.962212410144275e-06}, {"id": 1170, "seek": 550088, "start": 5507.8, "end": 5510.74, "text": " I'm not sure this is the best API, but it's not terrible", "tokens": [286, 478, 406, 988, 341, 307, 264, 1151, 9362, 11, 457, 309, 311, 406, 6237], "temperature": 0.0, "avg_logprob": -0.4023710015701921, "compression_ratio": 1.3717948717948718, "no_speech_prob": 6.962212410144275e-06}, {"id": 1171, "seek": 550088, "start": 5511.92, "end": 5515.88, "text": " Probably what since we've only got exactly two layers we could say P1 equals", "tokens": [9210, 437, 1670, 321, 600, 787, 658, 2293, 732, 7914, 321, 727, 584, 430, 16, 6915], "temperature": 0.0, "avg_logprob": -0.4023710015701921, "compression_ratio": 1.3717948717948718, "no_speech_prob": 6.962212410144275e-06}, {"id": 1172, "seek": 550088, "start": 5516.88, "end": 5518.88, "text": " 0.75", "tokens": [1958, 13, 11901], "temperature": 0.0, "avg_logprob": -0.4023710015701921, "compression_ratio": 1.3717948717948718, "no_speech_prob": 6.962212410144275e-06}, {"id": 1173, "seek": 550088, "start": 5520.64, "end": 5525.400000000001, "text": " Five P two equals point five and", "tokens": [9436, 430, 732, 6915, 935, 1732, 293], "temperature": 0.0, "avg_logprob": -0.4023710015701921, "compression_ratio": 1.3717948717948718, "no_speech_prob": 6.962212410144275e-06}, {"id": 1174, "seek": 552540, "start": 5525.4, "end": 5529.0, "text": " So then this will be", "tokens": [407, 550, 341, 486, 312], "temperature": 0.0, "avg_logprob": -0.2566331175507092, "compression_ratio": 1.4722222222222223, "no_speech_prob": 7.338196041928313e-07}, {"id": 1175, "seek": 552540, "start": 5531.879999999999, "end": 5534.48, "text": " P1 this will be", "tokens": [430, 16, 341, 486, 312], "temperature": 0.0, "avg_logprob": -0.2566331175507092, "compression_ratio": 1.4722222222222223, "no_speech_prob": 7.338196041928313e-07}, {"id": 1176, "seek": 552540, "start": 5535.5599999999995, "end": 5537.0, "text": " P2", "tokens": [430, 17], "temperature": 0.0, "avg_logprob": -0.2566331175507092, "compression_ratio": 1.4722222222222223, "no_speech_prob": 7.338196041928313e-07}, {"id": 1177, "seek": 552540, "start": 5537.0, "end": 5541.24, "text": " You know where we go and like if you wanted to go further", "tokens": [509, 458, 689, 321, 352, 293, 411, 498, 291, 1415, 281, 352, 3052], "temperature": 0.0, "avg_logprob": -0.2566331175507092, "compression_ratio": 1.4722222222222223, "no_speech_prob": 7.338196041928313e-07}, {"id": 1178, "seek": 552540, "start": 5543.28, "end": 5545.28, "text": " You could make it look more like our", "tokens": [509, 727, 652, 309, 574, 544, 411, 527], "temperature": 0.0, "avg_logprob": -0.2566331175507092, "compression_ratio": 1.4722222222222223, "no_speech_prob": 7.338196041928313e-07}, {"id": 1179, "seek": 552540, "start": 5547.36, "end": 5551.799999999999, "text": " Structured data learner you could actually have a thing this number of hidden", "tokens": [745, 46847, 1412, 33347, 291, 727, 767, 362, 257, 551, 341, 1230, 295, 7633], "temperature": 0.0, "avg_logprob": -0.2566331175507092, "compression_ratio": 1.4722222222222223, "no_speech_prob": 7.338196041928313e-07}, {"id": 1180, "seek": 555180, "start": 5551.8, "end": 5558.400000000001, "text": " You know maybe you could make a list and so then rather than creating exactly one", "tokens": [509, 458, 1310, 291, 727, 652, 257, 1329, 293, 370, 550, 2831, 813, 4084, 2293, 472], "temperature": 0.0, "avg_logprob": -0.17033929644890553, "compression_ratio": 1.7557251908396947, "no_speech_prob": 6.083581070015498e-07}, {"id": 1181, "seek": 555180, "start": 5558.84, "end": 5563.1, "text": " Hidden layer and one output layer this could be a little loop that creates", "tokens": [41156, 4583, 293, 472, 5598, 4583, 341, 727, 312, 257, 707, 6367, 300, 7829], "temperature": 0.0, "avg_logprob": -0.17033929644890553, "compression_ratio": 1.7557251908396947, "no_speech_prob": 6.083581070015498e-07}, {"id": 1182, "seek": 555180, "start": 5563.56, "end": 5569.16, "text": " And hidden layers each one of the size you want so like this is all stuff you can play with during the during the week", "tokens": [400, 7633, 7914, 1184, 472, 295, 264, 2744, 291, 528, 370, 411, 341, 307, 439, 1507, 291, 393, 862, 365, 1830, 264, 1830, 264, 1243], "temperature": 0.0, "avg_logprob": -0.17033929644890553, "compression_ratio": 1.7557251908396947, "no_speech_prob": 6.083581070015498e-07}, {"id": 1183, "seek": 555180, "start": 5569.320000000001, "end": 5575.68, "text": " If you want to and I feel like if you've got like a much smaller collaborative filtering data set", "tokens": [759, 291, 528, 281, 293, 286, 841, 411, 498, 291, 600, 658, 411, 257, 709, 4356, 16555, 30822, 1412, 992], "temperature": 0.0, "avg_logprob": -0.17033929644890553, "compression_ratio": 1.7557251908396947, "no_speech_prob": 6.083581070015498e-07}, {"id": 1184, "seek": 555180, "start": 5575.68, "end": 5580.6, "text": " You know maybe you'd need like more regularization or whatever. It's a much bigger one", "tokens": [509, 458, 1310, 291, 1116, 643, 411, 544, 3890, 2144, 420, 2035, 13, 467, 311, 257, 709, 3801, 472], "temperature": 0.0, "avg_logprob": -0.17033929644890553, "compression_ratio": 1.7557251908396947, "no_speech_prob": 6.083581070015498e-07}, {"id": 1185, "seek": 558060, "start": 5580.6, "end": 5585.76, "text": " Maybe more layers would help. I don't know you know I haven't seen", "tokens": [2704, 544, 7914, 576, 854, 13, 286, 500, 380, 458, 291, 458, 286, 2378, 380, 1612], "temperature": 0.0, "avg_logprob": -0.15790017043487936, "compression_ratio": 1.595, "no_speech_prob": 1.4823541505393223e-06}, {"id": 1186, "seek": 558060, "start": 5587.280000000001, "end": 5593.08, "text": " Much discussion of this kind of neural network approach to collaborative filtering, but I'm not a collaborative filtering expert", "tokens": [12313, 5017, 295, 341, 733, 295, 18161, 3209, 3109, 281, 16555, 30822, 11, 457, 286, 478, 406, 257, 16555, 30822, 5844], "temperature": 0.0, "avg_logprob": -0.15790017043487936, "compression_ratio": 1.595, "no_speech_prob": 1.4823541505393223e-06}, {"id": 1187, "seek": 558060, "start": 5593.08, "end": 5596.68, "text": " So maybe it's maybe it's around, but that'd be interesting thing to try", "tokens": [407, 1310, 309, 311, 1310, 309, 311, 926, 11, 457, 300, 1116, 312, 1880, 551, 281, 853], "temperature": 0.0, "avg_logprob": -0.15790017043487936, "compression_ratio": 1.595, "no_speech_prob": 1.4823541505393223e-06}, {"id": 1188, "seek": 558060, "start": 5601.6, "end": 5603.6, "text": " So the next thing I wanted to do", "tokens": [407, 264, 958, 551, 286, 1415, 281, 360], "temperature": 0.0, "avg_logprob": -0.15790017043487936, "compression_ratio": 1.595, "no_speech_prob": 1.4823541505393223e-06}, {"id": 1189, "seek": 558060, "start": 5604.72, "end": 5606.72, "text": " Was to talk about?", "tokens": [3027, 281, 751, 466, 30], "temperature": 0.0, "avg_logprob": -0.15790017043487936, "compression_ratio": 1.595, "no_speech_prob": 1.4823541505393223e-06}, {"id": 1190, "seek": 560672, "start": 5606.72, "end": 5610.6, "text": " The training loop so what's actually happening?", "tokens": [440, 3097, 6367, 370, 437, 311, 767, 2737, 30], "temperature": 0.0, "avg_logprob": -0.23846354135652867, "compression_ratio": 1.7526315789473683, "no_speech_prob": 3.90545028494671e-06}, {"id": 1191, "seek": 560672, "start": 5611.6, "end": 5613.6, "text": " inside the training loop", "tokens": [1854, 264, 3097, 6367], "temperature": 0.0, "avg_logprob": -0.23846354135652867, "compression_ratio": 1.7526315789473683, "no_speech_prob": 3.90545028494671e-06}, {"id": 1192, "seek": 560672, "start": 5614.16, "end": 5617.6, "text": " So at the moment we're basically passing off", "tokens": [407, 412, 264, 1623, 321, 434, 1936, 8437, 766], "temperature": 0.0, "avg_logprob": -0.23846354135652867, "compression_ratio": 1.7526315789473683, "no_speech_prob": 3.90545028494671e-06}, {"id": 1193, "seek": 560672, "start": 5618.400000000001, "end": 5622.76, "text": " the actual updating of the weights to pie torches", "tokens": [264, 3539, 25113, 295, 264, 17443, 281, 1730, 3930, 3781], "temperature": 0.0, "avg_logprob": -0.23846354135652867, "compression_ratio": 1.7526315789473683, "no_speech_prob": 3.90545028494671e-06}, {"id": 1194, "seek": 560672, "start": 5623.400000000001, "end": 5625.320000000001, "text": " optimizer", "tokens": [5028, 6545], "temperature": 0.0, "avg_logprob": -0.23846354135652867, "compression_ratio": 1.7526315789473683, "no_speech_prob": 3.90545028494671e-06}, {"id": 1195, "seek": 560672, "start": 5625.320000000001, "end": 5627.68, "text": " But what I want to do is like understand", "tokens": [583, 437, 286, 528, 281, 360, 307, 411, 1223], "temperature": 0.0, "avg_logprob": -0.23846354135652867, "compression_ratio": 1.7526315789473683, "no_speech_prob": 3.90545028494671e-06}, {"id": 1196, "seek": 560672, "start": 5628.68, "end": 5635.9800000000005, "text": " What that optimizer is is actually doing and we're also I also want to understand what this momentum term is doing", "tokens": [708, 300, 5028, 6545, 307, 307, 767, 884, 293, 321, 434, 611, 286, 611, 528, 281, 1223, 437, 341, 11244, 1433, 307, 884], "temperature": 0.0, "avg_logprob": -0.23846354135652867, "compression_ratio": 1.7526315789473683, "no_speech_prob": 3.90545028494671e-06}, {"id": 1197, "seek": 563598, "start": 5635.98, "end": 5637.54, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2596102494459886, "compression_ratio": 1.646551724137931, "no_speech_prob": 6.893596378176881e-07}, {"id": 1198, "seek": 563598, "start": 5637.54, "end": 5639.54, "text": " You'll find we have a", "tokens": [509, 603, 915, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.2596102494459886, "compression_ratio": 1.646551724137931, "no_speech_prob": 6.893596378176881e-07}, {"id": 1199, "seek": 563598, "start": 5641.459999999999, "end": 5644.58, "text": " Spreadsheet called grad desk gradient descent", "tokens": [30308, 9611, 302, 1219, 2771, 10026, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.2596102494459886, "compression_ratio": 1.646551724137931, "no_speech_prob": 6.893596378176881e-07}, {"id": 1200, "seek": 563598, "start": 5645.219999999999, "end": 5649.62, "text": " And it's kind of designed to be read left to right sorry right to left worksheet wise", "tokens": [400, 309, 311, 733, 295, 4761, 281, 312, 1401, 1411, 281, 558, 2597, 558, 281, 1411, 49890, 10829], "temperature": 0.0, "avg_logprob": -0.2596102494459886, "compression_ratio": 1.646551724137931, "no_speech_prob": 6.893596378176881e-07}, {"id": 1201, "seek": 563598, "start": 5650.58, "end": 5652.58, "text": " So the rightmost worksheet", "tokens": [407, 264, 558, 1761, 49890], "temperature": 0.0, "avg_logprob": -0.2596102494459886, "compression_ratio": 1.646551724137931, "no_speech_prob": 6.893596378176881e-07}, {"id": 1202, "seek": 563598, "start": 5653.0199999999995, "end": 5658.54, "text": " Is some data right and we're going to implement gradient descent in excel because obviously", "tokens": [1119, 512, 1412, 558, 293, 321, 434, 516, 281, 4445, 16235, 23475, 294, 24015, 570, 2745], "temperature": 0.0, "avg_logprob": -0.2596102494459886, "compression_ratio": 1.646551724137931, "no_speech_prob": 6.893596378176881e-07}, {"id": 1203, "seek": 563598, "start": 5658.98, "end": 5663.74, "text": " Everybody wants to do deep learning in itself, and we've done collaborative filtering in Excel. We've done", "tokens": [7646, 2738, 281, 360, 2452, 2539, 294, 2564, 11, 293, 321, 600, 1096, 16555, 30822, 294, 19060, 13, 492, 600, 1096], "temperature": 0.0, "avg_logprob": -0.2596102494459886, "compression_ratio": 1.646551724137931, "no_speech_prob": 6.893596378176881e-07}, {"id": 1204, "seek": 566374, "start": 5663.74, "end": 5671.38, "text": " Convolutions in Excel so now we need SGD in Excel so we can replace Python once and for all okay, so", "tokens": [2656, 85, 15892, 294, 19060, 370, 586, 321, 643, 34520, 35, 294, 19060, 370, 321, 393, 7406, 15329, 1564, 293, 337, 439, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.2023845760301612, "compression_ratio": 1.5639810426540284, "no_speech_prob": 2.406093472018256e-06}, {"id": 1205, "seek": 566374, "start": 5672.0599999999995, "end": 5675.5, "text": " Let's start by creating some data right and so here's", "tokens": [961, 311, 722, 538, 4084, 512, 1412, 558, 293, 370, 510, 311], "temperature": 0.0, "avg_logprob": -0.2023845760301612, "compression_ratio": 1.5639810426540284, "no_speech_prob": 2.406093472018256e-06}, {"id": 1206, "seek": 566374, "start": 5676.62, "end": 5678.62, "text": " You know here's some", "tokens": [509, 458, 510, 311, 512], "temperature": 0.0, "avg_logprob": -0.2023845760301612, "compression_ratio": 1.5639810426540284, "no_speech_prob": 2.406093472018256e-06}, {"id": 1207, "seek": 566374, "start": 5679.0199999999995, "end": 5684.94, "text": " Independent you know I've got one column of X's you know and one column", "tokens": [40310, 291, 458, 286, 600, 658, 472, 7738, 295, 1783, 311, 291, 458, 293, 472, 7738], "temperature": 0.0, "avg_logprob": -0.2023845760301612, "compression_ratio": 1.5639810426540284, "no_speech_prob": 2.406093472018256e-06}, {"id": 1208, "seek": 566374, "start": 5685.54, "end": 5690.86, "text": " Of Y's and these are actually directly linearly related, so this is this is random", "tokens": [2720, 398, 311, 293, 613, 366, 767, 3838, 43586, 4077, 11, 370, 341, 307, 341, 307, 4974], "temperature": 0.0, "avg_logprob": -0.2023845760301612, "compression_ratio": 1.5639810426540284, "no_speech_prob": 2.406093472018256e-06}, {"id": 1209, "seek": 569086, "start": 5690.86, "end": 5694.9, "text": " right and this one here is equal to X", "tokens": [558, 293, 341, 472, 510, 307, 2681, 281, 1783], "temperature": 0.0, "avg_logprob": -0.2795032716133225, "compression_ratio": 1.4078947368421053, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1210, "seek": 569086, "start": 5695.7, "end": 5697.7, "text": " times 2", "tokens": [1413, 568], "temperature": 0.0, "avg_logprob": -0.2795032716133225, "compression_ratio": 1.4078947368421053, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1211, "seek": 569086, "start": 5698.139999999999, "end": 5700.139999999999, "text": " plus 30", "tokens": [1804, 2217], "temperature": 0.0, "avg_logprob": -0.2795032716133225, "compression_ratio": 1.4078947368421053, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1212, "seek": 569086, "start": 5700.38, "end": 5702.38, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.2795032716133225, "compression_ratio": 1.4078947368421053, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1213, "seek": 569086, "start": 5702.66, "end": 5704.66, "text": " Let's try and use Excel", "tokens": [961, 311, 853, 293, 764, 19060], "temperature": 0.0, "avg_logprob": -0.2795032716133225, "compression_ratio": 1.4078947368421053, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1214, "seek": 569086, "start": 5704.78, "end": 5706.58, "text": " to take", "tokens": [281, 747], "temperature": 0.0, "avg_logprob": -0.2795032716133225, "compression_ratio": 1.4078947368421053, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1215, "seek": 569086, "start": 5706.58, "end": 5709.54, "text": " that data and try and learn", "tokens": [300, 1412, 293, 853, 293, 1466], "temperature": 0.0, "avg_logprob": -0.2795032716133225, "compression_ratio": 1.4078947368421053, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1216, "seek": 569086, "start": 5710.62, "end": 5712.62, "text": " those parameters", "tokens": [729, 9834], "temperature": 0.0, "avg_logprob": -0.2795032716133225, "compression_ratio": 1.4078947368421053, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1217, "seek": 569086, "start": 5712.78, "end": 5714.78, "text": " Okay, that's going to be a call", "tokens": [1033, 11, 300, 311, 516, 281, 312, 257, 818], "temperature": 0.0, "avg_logprob": -0.2795032716133225, "compression_ratio": 1.4078947368421053, "no_speech_prob": 1.2289151527511422e-06}, {"id": 1218, "seek": 571478, "start": 5714.78, "end": 5720.099999999999, "text": " So let's start with the most basic version", "tokens": [407, 718, 311, 722, 365, 264, 881, 3875, 3037], "temperature": 0.0, "avg_logprob": -0.16887893279393515, "compression_ratio": 1.6476190476190475, "no_speech_prob": 3.34051492245635e-06}, {"id": 1219, "seek": 571478, "start": 5720.82, "end": 5724.46, "text": " Of SGD and so the first thing I'm going to do is I'm going to run a macro", "tokens": [2720, 34520, 35, 293, 370, 264, 700, 551, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 1190, 257, 18887], "temperature": 0.0, "avg_logprob": -0.16887893279393515, "compression_ratio": 1.6476190476190475, "no_speech_prob": 3.34051492245635e-06}, {"id": 1220, "seek": 571478, "start": 5724.78, "end": 5729.98, "text": " So you can see what this looks like so I hit run and it does five epochs", "tokens": [407, 291, 393, 536, 437, 341, 1542, 411, 370, 286, 2045, 1190, 293, 309, 775, 1732, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.16887893279393515, "compression_ratio": 1.6476190476190475, "no_speech_prob": 3.34051492245635e-06}, {"id": 1221, "seek": 571478, "start": 5729.98, "end": 5733.7, "text": " I do another five epochs another five epochs", "tokens": [286, 360, 1071, 1732, 30992, 28346, 1071, 1732, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.16887893279393515, "compression_ratio": 1.6476190476190475, "no_speech_prob": 3.34051492245635e-06}, {"id": 1222, "seek": 571478, "start": 5734.42, "end": 5736.34, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.16887893279393515, "compression_ratio": 1.6476190476190475, "no_speech_prob": 3.34051492245635e-06}, {"id": 1223, "seek": 573634, "start": 5736.34, "end": 5745.1, "text": " The first one was pretty terrible. It's hard to see so I just delete that first one get better scaling all right", "tokens": [440, 700, 472, 390, 1238, 6237, 13, 467, 311, 1152, 281, 536, 370, 286, 445, 12097, 300, 700, 472, 483, 1101, 21589, 439, 558], "temperature": 0.0, "avg_logprob": -0.23107893874005572, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.003012130240677e-06}, {"id": 1224, "seek": 573634, "start": 5745.1, "end": 5752.38, "text": " So you can see actually it's pretty constantly improving the loss right this is the loss per epoch", "tokens": [407, 291, 393, 536, 767, 309, 311, 1238, 6460, 11470, 264, 4470, 558, 341, 307, 264, 4470, 680, 30992, 339], "temperature": 0.0, "avg_logprob": -0.23107893874005572, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.003012130240677e-06}, {"id": 1225, "seek": 573634, "start": 5752.38, "end": 5755.06, "text": " All right, so how do we do that so let's reset it?", "tokens": [1057, 558, 11, 370, 577, 360, 321, 360, 300, 370, 718, 311, 14322, 309, 30], "temperature": 0.0, "avg_logprob": -0.23107893874005572, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.003012130240677e-06}, {"id": 1226, "seek": 573634, "start": 5758.14, "end": 5760.14, "text": " So here is my", "tokens": [407, 510, 307, 452], "temperature": 0.0, "avg_logprob": -0.23107893874005572, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.003012130240677e-06}, {"id": 1227, "seek": 573634, "start": 5760.18, "end": 5762.18, "text": " X's and my Y's and", "tokens": [1783, 311, 293, 452, 398, 311, 293], "temperature": 0.0, "avg_logprob": -0.23107893874005572, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.003012130240677e-06}, {"id": 1228, "seek": 576218, "start": 5762.18, "end": 5770.5, "text": " And what I do is I start out by assuming some intercept and some slope right so this is my", "tokens": [400, 437, 286, 360, 307, 286, 722, 484, 538, 11926, 512, 24700, 293, 512, 13525, 558, 370, 341, 307, 452], "temperature": 0.0, "avg_logprob": -0.18444506714983686, "compression_ratio": 1.6551724137931034, "no_speech_prob": 8.714321211300557e-07}, {"id": 1229, "seek": 576218, "start": 5771.18, "end": 5776.14, "text": " Randomly initialized weights, so I have randomly initialized them both to one", "tokens": [37603, 356, 5883, 1602, 17443, 11, 370, 286, 362, 16979, 5883, 1602, 552, 1293, 281, 472], "temperature": 0.0, "avg_logprob": -0.18444506714983686, "compression_ratio": 1.6551724137931034, "no_speech_prob": 8.714321211300557e-07}, {"id": 1230, "seek": 576218, "start": 5776.9400000000005, "end": 5783.42, "text": " You could pick a different random number if you like, but I promise that I randomly picked the number one", "tokens": [509, 727, 1888, 257, 819, 4974, 1230, 498, 291, 411, 11, 457, 286, 6228, 300, 286, 16979, 6183, 264, 1230, 472], "temperature": 0.0, "avg_logprob": -0.18444506714983686, "compression_ratio": 1.6551724137931034, "no_speech_prob": 8.714321211300557e-07}, {"id": 1231, "seek": 576218, "start": 5784.1, "end": 5786.1, "text": " Twice there you go", "tokens": [46964, 456, 291, 352], "temperature": 0.0, "avg_logprob": -0.18444506714983686, "compression_ratio": 1.6551724137931034, "no_speech_prob": 8.714321211300557e-07}, {"id": 1232, "seek": 576218, "start": 5788.38, "end": 5790.58, "text": " It was a random number between one and one", "tokens": [467, 390, 257, 4974, 1230, 1296, 472, 293, 472], "temperature": 0.0, "avg_logprob": -0.18444506714983686, "compression_ratio": 1.6551724137931034, "no_speech_prob": 8.714321211300557e-07}, {"id": 1233, "seek": 579058, "start": 5790.58, "end": 5792.58, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.17313743817924274, "compression_ratio": 1.6681614349775784, "no_speech_prob": 6.681505624328565e-07}, {"id": 1234, "seek": 579058, "start": 5792.98, "end": 5799.1, "text": " Here is my intercept and slope. I'm just going to copy them over here right so you can literally see this is just equal C1", "tokens": [1692, 307, 452, 24700, 293, 13525, 13, 286, 478, 445, 516, 281, 5055, 552, 670, 510, 558, 370, 291, 393, 3736, 536, 341, 307, 445, 2681, 383, 16], "temperature": 0.0, "avg_logprob": -0.17313743817924274, "compression_ratio": 1.6681614349775784, "no_speech_prob": 6.681505624328565e-07}, {"id": 1235, "seek": 579058, "start": 5799.78, "end": 5806.7, "text": " Here is equals C2 okay, so I'm going to start with my very first row of data X equals 14", "tokens": [1692, 307, 6915, 383, 17, 1392, 11, 370, 286, 478, 516, 281, 722, 365, 452, 588, 700, 5386, 295, 1412, 1783, 6915, 3499], "temperature": 0.0, "avg_logprob": -0.17313743817924274, "compression_ratio": 1.6681614349775784, "no_speech_prob": 6.681505624328565e-07}, {"id": 1236, "seek": 579058, "start": 5806.7, "end": 5810.42, "text": " Y equals 58 and my goal is to come up", "tokens": [398, 6915, 21786, 293, 452, 3387, 307, 281, 808, 493], "temperature": 0.0, "avg_logprob": -0.17313743817924274, "compression_ratio": 1.6681614349775784, "no_speech_prob": 6.681505624328565e-07}, {"id": 1237, "seek": 579058, "start": 5811.18, "end": 5816.62, "text": " After I look at this piece of data. I want to come up with a slightly better intercept and a slightly better slope", "tokens": [2381, 286, 574, 412, 341, 2522, 295, 1412, 13, 286, 528, 281, 808, 493, 365, 257, 4748, 1101, 24700, 293, 257, 4748, 1101, 13525], "temperature": 0.0, "avg_logprob": -0.17313743817924274, "compression_ratio": 1.6681614349775784, "no_speech_prob": 6.681505624328565e-07}, {"id": 1238, "seek": 579058, "start": 5817.18, "end": 5818.82, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.17313743817924274, "compression_ratio": 1.6681614349775784, "no_speech_prob": 6.681505624328565e-07}, {"id": 1239, "seek": 581882, "start": 5818.82, "end": 5824.16, "text": " So to do that I need to first of all basically figure out", "tokens": [407, 281, 360, 300, 286, 643, 281, 700, 295, 439, 1936, 2573, 484], "temperature": 0.0, "avg_logprob": -0.16157775920826, "compression_ratio": 1.8229665071770336, "no_speech_prob": 1.436747083971568e-06}, {"id": 1240, "seek": 581882, "start": 5824.9, "end": 5831.38, "text": " Which direction is is down in other words if I make my intercept a little bit higher?", "tokens": [3013, 3513, 307, 307, 760, 294, 661, 2283, 498, 286, 652, 452, 24700, 257, 707, 857, 2946, 30], "temperature": 0.0, "avg_logprob": -0.16157775920826, "compression_ratio": 1.8229665071770336, "no_speech_prob": 1.436747083971568e-06}, {"id": 1241, "seek": 581882, "start": 5831.7, "end": 5836.2, "text": " Or a little bit lower would it make my error a little bit better a little bit worse", "tokens": [1610, 257, 707, 857, 3126, 576, 309, 652, 452, 6713, 257, 707, 857, 1101, 257, 707, 857, 5324], "temperature": 0.0, "avg_logprob": -0.16157775920826, "compression_ratio": 1.8229665071770336, "no_speech_prob": 1.436747083971568e-06}, {"id": 1242, "seek": 581882, "start": 5836.78, "end": 5842.42, "text": " So let's start out by calculating the error so to calculate the error the first thing we need is a prediction", "tokens": [407, 718, 311, 722, 484, 538, 28258, 264, 6713, 370, 281, 8873, 264, 6713, 264, 700, 551, 321, 643, 307, 257, 17630], "temperature": 0.0, "avg_logprob": -0.16157775920826, "compression_ratio": 1.8229665071770336, "no_speech_prob": 1.436747083971568e-06}, {"id": 1243, "seek": 581882, "start": 5843.46, "end": 5846.679999999999, "text": " So the prediction is equal to the intercept", "tokens": [407, 264, 17630, 307, 2681, 281, 264, 24700], "temperature": 0.0, "avg_logprob": -0.16157775920826, "compression_ratio": 1.8229665071770336, "no_speech_prob": 1.436747083971568e-06}, {"id": 1244, "seek": 584668, "start": 5846.68, "end": 5849.280000000001, "text": " plus X times flow", "tokens": [1804, 1783, 1413, 3095], "temperature": 0.0, "avg_logprob": -0.3054371976304328, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.3497000281859073e-06}, {"id": 1245, "seek": 584668, "start": 5850.0, "end": 5852.0, "text": " So that is our", "tokens": [407, 300, 307, 527], "temperature": 0.0, "avg_logprob": -0.3054371976304328, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.3497000281859073e-06}, {"id": 1246, "seek": 584668, "start": 5852.56, "end": 5855.56, "text": " Zero hidden layer neural network okay?", "tokens": [17182, 7633, 4583, 18161, 3209, 1392, 30], "temperature": 0.0, "avg_logprob": -0.3054371976304328, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.3497000281859073e-06}, {"id": 1247, "seek": 584668, "start": 5856.52, "end": 5861.84, "text": " And so here is our error. It's equal to our prediction minus our actual square", "tokens": [400, 370, 510, 307, 527, 6713, 13, 467, 311, 2681, 281, 527, 17630, 3175, 527, 3539, 3732], "temperature": 0.0, "avg_logprob": -0.3054371976304328, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.3497000281859073e-06}, {"id": 1248, "seek": 584668, "start": 5863.240000000001, "end": 5868.240000000001, "text": " So we could like play around with this. I don't want my error to be 1849. I'd like it to be lower", "tokens": [407, 321, 727, 411, 862, 926, 365, 341, 13, 286, 500, 380, 528, 452, 6713, 281, 312, 2443, 14938, 13, 286, 1116, 411, 309, 281, 312, 3126], "temperature": 0.0, "avg_logprob": -0.3054371976304328, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.3497000281859073e-06}, {"id": 1249, "seek": 584668, "start": 5868.400000000001, "end": 5870.400000000001, "text": " So what if we set the intercept to?", "tokens": [407, 437, 498, 321, 992, 264, 24700, 281, 30], "temperature": 0.0, "avg_logprob": -0.3054371976304328, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.3497000281859073e-06}, {"id": 1250, "seek": 584668, "start": 5871.360000000001, "end": 5872.96, "text": " 1.1", "tokens": [502, 13, 16], "temperature": 0.0, "avg_logprob": -0.3054371976304328, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.3497000281859073e-06}, {"id": 1251, "seek": 587296, "start": 5872.96, "end": 5877.36, "text": " 1849 because to 1840 okay, so a higher intercept would be better", "tokens": [2443, 14938, 570, 281, 2443, 5254, 1392, 11, 370, 257, 2946, 24700, 576, 312, 1101], "temperature": 0.0, "avg_logprob": -0.21837782558006577, "compression_ratio": 1.5135135135135136, "no_speech_prob": 6.276699195950641e-07}, {"id": 1252, "seek": 587296, "start": 5877.8, "end": 5880.92, "text": " Okay, what about the slope if I increase that?", "tokens": [1033, 11, 437, 466, 264, 13525, 498, 286, 3488, 300, 30], "temperature": 0.0, "avg_logprob": -0.21837782558006577, "compression_ratio": 1.5135135135135136, "no_speech_prob": 6.276699195950641e-07}, {"id": 1253, "seek": 587296, "start": 5881.72, "end": 5883.72, "text": " It goes from 1849", "tokens": [467, 1709, 490, 2443, 14938], "temperature": 0.0, "avg_logprob": -0.21837782558006577, "compression_ratio": 1.5135135135135136, "no_speech_prob": 6.276699195950641e-07}, {"id": 1254, "seek": 587296, "start": 5884.6, "end": 5887.56, "text": " To 1730 okay a higher slope would be better as well", "tokens": [1407, 3282, 3446, 1392, 257, 2946, 13525, 576, 312, 1101, 382, 731], "temperature": 0.0, "avg_logprob": -0.21837782558006577, "compression_ratio": 1.5135135135135136, "no_speech_prob": 6.276699195950641e-07}, {"id": 1255, "seek": 587296, "start": 5888.16, "end": 5890.16, "text": " Not surprising because we know", "tokens": [1726, 8830, 570, 321, 458], "temperature": 0.0, "avg_logprob": -0.21837782558006577, "compression_ratio": 1.5135135135135136, "no_speech_prob": 6.276699195950641e-07}, {"id": 1256, "seek": 587296, "start": 5890.4800000000005, "end": 5892.4800000000005, "text": " Actually that they should be 30 and 2", "tokens": [5135, 300, 436, 820, 312, 2217, 293, 568], "temperature": 0.0, "avg_logprob": -0.21837782558006577, "compression_ratio": 1.5135135135135136, "no_speech_prob": 6.276699195950641e-07}, {"id": 1257, "seek": 587296, "start": 5894.16, "end": 5897.36, "text": " So one way to figure that out", "tokens": [407, 472, 636, 281, 2573, 300, 484], "temperature": 0.0, "avg_logprob": -0.21837782558006577, "compression_ratio": 1.5135135135135136, "no_speech_prob": 6.276699195950641e-07}, {"id": 1258, "seek": 589736, "start": 5897.36, "end": 5904.28, "text": " You know in code and especially it is to do literally what I just did is to add a little bit to the intercept", "tokens": [509, 458, 294, 3089, 293, 2318, 309, 307, 281, 360, 3736, 437, 286, 445, 630, 307, 281, 909, 257, 707, 857, 281, 264, 24700], "temperature": 0.0, "avg_logprob": -0.22768028833532847, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.1544600511115277e-06}, {"id": 1259, "seek": 589736, "start": 5904.28, "end": 5908.9, "text": " And the slope and see what happens and that's called finding the derivative through finite differencing", "tokens": [400, 264, 13525, 293, 536, 437, 2314, 293, 300, 311, 1219, 5006, 264, 13760, 807, 19362, 743, 13644], "temperature": 0.0, "avg_logprob": -0.22768028833532847, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.1544600511115277e-06}, {"id": 1260, "seek": 589736, "start": 5909.16, "end": 5911.48, "text": " Right and so let's go ahead and do that", "tokens": [1779, 293, 370, 718, 311, 352, 2286, 293, 360, 300], "temperature": 0.0, "avg_logprob": -0.22768028833532847, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.1544600511115277e-06}, {"id": 1261, "seek": 589736, "start": 5911.96, "end": 5916.599999999999, "text": " So here is the value of my error if I add", "tokens": [407, 510, 307, 264, 2158, 295, 452, 6713, 498, 286, 909], "temperature": 0.0, "avg_logprob": -0.22768028833532847, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.1544600511115277e-06}, {"id": 1262, "seek": 589736, "start": 5917.92, "end": 5919.16, "text": " 0.01", "tokens": [1958, 13, 10607], "temperature": 0.0, "avg_logprob": -0.22768028833532847, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.1544600511115277e-06}, {"id": 1263, "seek": 589736, "start": 5919.16, "end": 5920.48, "text": " to", "tokens": [281], "temperature": 0.0, "avg_logprob": -0.22768028833532847, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.1544600511115277e-06}, {"id": 1264, "seek": 589736, "start": 5920.48, "end": 5923.759999999999, "text": " My intercept right so it's c4 plus open", "tokens": [1222, 24700, 558, 370, 309, 311, 269, 19, 1804, 1269], "temperature": 0.0, "avg_logprob": -0.22768028833532847, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.1544600511115277e-06}, {"id": 1265, "seek": 592376, "start": 5923.76, "end": 5929.16, "text": " 0.1, and then I just put that into my linear function, and then I subtract my actual all squared", "tokens": [1958, 13, 16, 11, 293, 550, 286, 445, 829, 300, 666, 452, 8213, 2445, 11, 293, 550, 286, 16390, 452, 3539, 439, 8889], "temperature": 0.0, "avg_logprob": -0.19832487106323243, "compression_ratio": 1.8933333333333333, "no_speech_prob": 3.7266304389049765e-06}, {"id": 1266, "seek": 592376, "start": 5929.320000000001, "end": 5934.64, "text": " Right and so that causes my error to go down a bit that's increasing", "tokens": [1779, 293, 370, 300, 7700, 452, 6713, 281, 352, 760, 257, 857, 300, 311, 5662], "temperature": 0.0, "avg_logprob": -0.19832487106323243, "compression_ratio": 1.8933333333333333, "no_speech_prob": 3.7266304389049765e-06}, {"id": 1267, "seek": 592376, "start": 5935.24, "end": 5937.24, "text": " my", "tokens": [452], "temperature": 0.0, "avg_logprob": -0.19832487106323243, "compression_ratio": 1.8933333333333333, "no_speech_prob": 3.7266304389049765e-06}, {"id": 1268, "seek": 592376, "start": 5938.0, "end": 5943.92, "text": " Which one is that increasing c4 increasing the intercept a little bit has caused my error to go down", "tokens": [3013, 472, 307, 300, 5662, 269, 19, 5662, 264, 24700, 257, 707, 857, 575, 7008, 452, 6713, 281, 352, 760], "temperature": 0.0, "avg_logprob": -0.19832487106323243, "compression_ratio": 1.8933333333333333, "no_speech_prob": 3.7266304389049765e-06}, {"id": 1269, "seek": 592376, "start": 5944.96, "end": 5950.12, "text": " So what's the derivative well the derivative is equal to how much the dependent variable changed by?", "tokens": [407, 437, 311, 264, 13760, 731, 264, 13760, 307, 2681, 281, 577, 709, 264, 12334, 7006, 3105, 538, 30], "temperature": 0.0, "avg_logprob": -0.19832487106323243, "compression_ratio": 1.8933333333333333, "no_speech_prob": 3.7266304389049765e-06}, {"id": 1270, "seek": 595012, "start": 5950.12, "end": 5956.08, "text": " Divided by how much the independent variable changed by right and so there it is right our", "tokens": [413, 1843, 292, 538, 577, 709, 264, 6695, 7006, 3105, 538, 558, 293, 370, 456, 309, 307, 558, 527], "temperature": 0.0, "avg_logprob": -0.2615954399108887, "compression_ratio": 1.808888888888889, "no_speech_prob": 1.1911060937563889e-06}, {"id": 1271, "seek": 595012, "start": 5956.599999999999, "end": 5958.599999999999, "text": " Dependent variable changed by that minus that", "tokens": [4056, 521, 317, 7006, 3105, 538, 300, 3175, 300], "temperature": 0.0, "avg_logprob": -0.2615954399108887, "compression_ratio": 1.808888888888889, "no_speech_prob": 1.1911060937563889e-06}, {"id": 1272, "seek": 595012, "start": 5959.32, "end": 5962.28, "text": " Right and our independent variable we changed by point. Oh one", "tokens": [1779, 293, 527, 6695, 7006, 321, 3105, 538, 935, 13, 876, 472], "temperature": 0.0, "avg_logprob": -0.2615954399108887, "compression_ratio": 1.808888888888889, "no_speech_prob": 1.1911060937563889e-06}, {"id": 1273, "seek": 595012, "start": 5962.88, "end": 5965.24, "text": " So there is the estimated value of?", "tokens": [407, 456, 307, 264, 14109, 2158, 295, 30], "temperature": 0.0, "avg_logprob": -0.2615954399108887, "compression_ratio": 1.808888888888889, "no_speech_prob": 1.1911060937563889e-06}, {"id": 1274, "seek": 595012, "start": 5966.12, "end": 5968.12, "text": " the error DB", "tokens": [264, 6713, 26754], "temperature": 0.0, "avg_logprob": -0.2615954399108887, "compression_ratio": 1.808888888888889, "no_speech_prob": 1.1911060937563889e-06}, {"id": 1275, "seek": 595012, "start": 5968.16, "end": 5971.28, "text": " All right, so remember when people talking about derivatives", "tokens": [1057, 558, 11, 370, 1604, 562, 561, 1417, 466, 33733], "temperature": 0.0, "avg_logprob": -0.2615954399108887, "compression_ratio": 1.808888888888889, "no_speech_prob": 1.1911060937563889e-06}, {"id": 1276, "seek": 595012, "start": 5971.68, "end": 5977.16, "text": " This is this is all they're doing is they're saying what's this value, but as we make this number", "tokens": [639, 307, 341, 307, 439, 436, 434, 884, 307, 436, 434, 1566, 437, 311, 341, 2158, 11, 457, 382, 321, 652, 341, 1230], "temperature": 0.0, "avg_logprob": -0.2615954399108887, "compression_ratio": 1.808888888888889, "no_speech_prob": 1.1911060937563889e-06}, {"id": 1277, "seek": 597716, "start": 5977.16, "end": 5982.76, "text": " Smaller and smaller and smaller and smaller as it limits to zero", "tokens": [15287, 260, 293, 4356, 293, 4356, 293, 4356, 382, 309, 10406, 281, 4018], "temperature": 0.0, "avg_logprob": -0.18601532737807472, "compression_ratio": 1.8708333333333333, "no_speech_prob": 1.9638007415778702e-06}, {"id": 1278, "seek": 597716, "start": 5983.36, "end": 5989.4, "text": " I'm not smart enough to think in terms of like derivatives and integrals and stuff like that so whenever I think about this", "tokens": [286, 478, 406, 4069, 1547, 281, 519, 294, 2115, 295, 411, 33733, 293, 3572, 1124, 293, 1507, 411, 300, 370, 5699, 286, 519, 466, 341], "temperature": 0.0, "avg_logprob": -0.18601532737807472, "compression_ratio": 1.8708333333333333, "no_speech_prob": 1.9638007415778702e-06}, {"id": 1279, "seek": 597716, "start": 5989.4, "end": 5996.24, "text": " I always think about you know an actual like plus 0.01 divided by 0.01 because like I just find that easier", "tokens": [286, 1009, 519, 466, 291, 458, 364, 3539, 411, 1804, 1958, 13, 10607, 6666, 538, 1958, 13, 10607, 570, 411, 286, 445, 915, 300, 3571], "temperature": 0.0, "avg_logprob": -0.18601532737807472, "compression_ratio": 1.8708333333333333, "no_speech_prob": 1.9638007415778702e-06}, {"id": 1280, "seek": 597716, "start": 5996.68, "end": 6002.9, "text": " Just like I never think about probability density functions. I always think about actual probabilities. I like toss a coin", "tokens": [1449, 411, 286, 1128, 519, 466, 8482, 10305, 6828, 13, 286, 1009, 519, 466, 3539, 33783, 13, 286, 411, 14432, 257, 11464], "temperature": 0.0, "avg_logprob": -0.18601532737807472, "compression_ratio": 1.8708333333333333, "no_speech_prob": 1.9638007415778702e-06}, {"id": 1281, "seek": 597716, "start": 6003.48, "end": 6005.48, "text": " Something happens three times", "tokens": [6595, 2314, 1045, 1413], "temperature": 0.0, "avg_logprob": -0.18601532737807472, "compression_ratio": 1.8708333333333333, "no_speech_prob": 1.9638007415778702e-06}, {"id": 1282, "seek": 600548, "start": 6005.48, "end": 6011.299999999999, "text": " You know so I always think like remember. It's it's totally fair to do this because a computer is", "tokens": [509, 458, 370, 286, 1009, 519, 411, 1604, 13, 467, 311, 309, 311, 3879, 3143, 281, 360, 341, 570, 257, 3820, 307], "temperature": 0.0, "avg_logprob": -0.14730642218338816, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.9638002868305193e-06}, {"id": 1283, "seek": 600548, "start": 6012.719999999999, "end": 6017.919999999999, "text": " Discrete it's not continuous like a computer can't do anything infinitely small anyway, right?", "tokens": [19839, 7600, 309, 311, 406, 10957, 411, 257, 3820, 393, 380, 360, 1340, 36227, 1359, 4033, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14730642218338816, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.9638002868305193e-06}, {"id": 1284, "seek": 600548, "start": 6017.919999999999, "end": 6025.5599999999995, "text": " So it's actually got to be calculating things at some level of precision right and our brains kind of need that as well", "tokens": [407, 309, 311, 767, 658, 281, 312, 28258, 721, 412, 512, 1496, 295, 18356, 558, 293, 527, 15442, 733, 295, 643, 300, 382, 731], "temperature": 0.0, "avg_logprob": -0.14730642218338816, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.9638002868305193e-06}, {"id": 1285, "seek": 600548, "start": 6026.0, "end": 6032.04, "text": " So this is like my version of Jeffrey Hinton's like to visualize things in more than two dimensions", "tokens": [407, 341, 307, 411, 452, 3037, 295, 28721, 389, 12442, 311, 411, 281, 23273, 721, 294, 544, 813, 732, 12819], "temperature": 0.0, "avg_logprob": -0.14730642218338816, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.9638002868305193e-06}, {"id": 1286, "seek": 603204, "start": 6032.04, "end": 6035.88, "text": " You just like say 12 dimensions really quickly while visualizing it in two dimensions", "tokens": [509, 445, 411, 584, 2272, 12819, 534, 2661, 1339, 5056, 3319, 309, 294, 732, 12819], "temperature": 0.0, "avg_logprob": -0.21803010028341543, "compression_ratio": 1.7531380753138075, "no_speech_prob": 2.443985522404546e-06}, {"id": 1287, "seek": 603204, "start": 6036.0, "end": 6040.86, "text": " This is my equivalent you know to to think about derivatives just think about", "tokens": [639, 307, 452, 10344, 291, 458, 281, 281, 519, 466, 33733, 445, 519, 466], "temperature": 0.0, "avg_logprob": -0.21803010028341543, "compression_ratio": 1.7531380753138075, "no_speech_prob": 2.443985522404546e-06}, {"id": 1288, "seek": 603204, "start": 6041.44, "end": 6046.14, "text": " Division and like although all the mathematicians say no you can't do that", "tokens": [17183, 293, 411, 4878, 439, 264, 32811, 2567, 584, 572, 291, 393, 380, 360, 300], "temperature": 0.0, "avg_logprob": -0.21803010028341543, "compression_ratio": 1.7531380753138075, "no_speech_prob": 2.443985522404546e-06}, {"id": 1289, "seek": 603204, "start": 6046.8, "end": 6053.0, "text": " You actually can like if you think of DXTY as being literally you know change in X over change in Y like", "tokens": [509, 767, 393, 411, 498, 291, 519, 295, 48817, 23433, 382, 885, 3736, 291, 458, 1319, 294, 1783, 670, 1319, 294, 398, 411], "temperature": 0.0, "avg_logprob": -0.21803010028341543, "compression_ratio": 1.7531380753138075, "no_speech_prob": 2.443985522404546e-06}, {"id": 1290, "seek": 603204, "start": 6054.28, "end": 6059.16, "text": " The division actually like the calculations still work like all the time so", "tokens": [440, 10044, 767, 411, 264, 20448, 920, 589, 411, 439, 264, 565, 370], "temperature": 0.0, "avg_logprob": -0.21803010028341543, "compression_ratio": 1.7531380753138075, "no_speech_prob": 2.443985522404546e-06}, {"id": 1291, "seek": 605916, "start": 6059.16, "end": 6063.5599999999995, "text": " Okay, so let's do the same thing now with changing", "tokens": [1033, 11, 370, 718, 311, 360, 264, 912, 551, 586, 365, 4473], "temperature": 0.0, "avg_logprob": -0.1894770702683782, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.2603162506129593e-06}, {"id": 1292, "seek": 605916, "start": 6064.2, "end": 6070.62, "text": " My slope by a little bit and so here's the same thing right and so you can see both of these are negative", "tokens": [1222, 13525, 538, 257, 707, 857, 293, 370, 510, 311, 264, 912, 551, 558, 293, 370, 291, 393, 536, 1293, 295, 613, 366, 3671], "temperature": 0.0, "avg_logprob": -0.1894770702683782, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.2603162506129593e-06}, {"id": 1293, "seek": 605916, "start": 6071.44, "end": 6078.44, "text": " Okay, so that's saying if I increase my intercept my loss goes down if I increase my slope", "tokens": [1033, 11, 370, 300, 311, 1566, 498, 286, 3488, 452, 24700, 452, 4470, 1709, 760, 498, 286, 3488, 452, 13525], "temperature": 0.0, "avg_logprob": -0.1894770702683782, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.2603162506129593e-06}, {"id": 1294, "seek": 605916, "start": 6078.76, "end": 6084.2, "text": " My loss goes down right and so my derivative of my error", "tokens": [1222, 4470, 1709, 760, 558, 293, 370, 452, 13760, 295, 452, 6713], "temperature": 0.0, "avg_logprob": -0.1894770702683782, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.2603162506129593e-06}, {"id": 1295, "seek": 605916, "start": 6084.8, "end": 6086.599999999999, "text": " with respect to", "tokens": [365, 3104, 281], "temperature": 0.0, "avg_logprob": -0.1894770702683782, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.2603162506129593e-06}, {"id": 1296, "seek": 608660, "start": 6086.6, "end": 6092.280000000001, "text": " my slope is is actually pretty high and that's not surprising because", "tokens": [452, 13525, 307, 307, 767, 1238, 1090, 293, 300, 311, 406, 8830, 570], "temperature": 0.0, "avg_logprob": -0.19853826882182687, "compression_ratio": 1.5268817204301075, "no_speech_prob": 6.276700332819019e-07}, {"id": 1297, "seek": 608660, "start": 6093.4400000000005, "end": 6095.4400000000005, "text": " It's actually", "tokens": [467, 311, 767], "temperature": 0.0, "avg_logprob": -0.19853826882182687, "compression_ratio": 1.5268817204301075, "no_speech_prob": 6.276700332819019e-07}, {"id": 1298, "seek": 608660, "start": 6095.52, "end": 6100.04, "text": " You know the constant term is just being added whereas the slope is being multiplied by 40", "tokens": [509, 458, 264, 5754, 1433, 307, 445, 885, 3869, 9735, 264, 13525, 307, 885, 17207, 538, 3356], "temperature": 0.0, "avg_logprob": -0.19853826882182687, "compression_ratio": 1.5268817204301075, "no_speech_prob": 6.276700332819019e-07}, {"id": 1299, "seek": 608660, "start": 6103.56, "end": 6105.56, "text": " Okay now", "tokens": [1033, 586], "temperature": 0.0, "avg_logprob": -0.19853826882182687, "compression_ratio": 1.5268817204301075, "no_speech_prob": 6.276700332819019e-07}, {"id": 1300, "seek": 608660, "start": 6106.84, "end": 6111.92, "text": " Finite differencing is all very well and good, but there's a big problem with finite differencing in", "tokens": [3773, 642, 743, 13644, 307, 439, 588, 731, 293, 665, 11, 457, 456, 311, 257, 955, 1154, 365, 19362, 743, 13644, 294], "temperature": 0.0, "avg_logprob": -0.19853826882182687, "compression_ratio": 1.5268817204301075, "no_speech_prob": 6.276700332819019e-07}, {"id": 1301, "seek": 611192, "start": 6111.92, "end": 6116.74, "text": " in high dimensional spaces and the problem is this right and this is like", "tokens": [294, 1090, 18795, 7673, 293, 264, 1154, 307, 341, 558, 293, 341, 307, 411], "temperature": 0.0, "avg_logprob": -0.18267393686685218, "compression_ratio": 1.6733668341708543, "no_speech_prob": 6.179387810334447e-07}, {"id": 1302, "seek": 611192, "start": 6118.96, "end": 6120.84, "text": " You don't need to learn", "tokens": [509, 500, 380, 643, 281, 1466], "temperature": 0.0, "avg_logprob": -0.18267393686685218, "compression_ratio": 1.6733668341708543, "no_speech_prob": 6.179387810334447e-07}, {"id": 1303, "seek": 611192, "start": 6120.84, "end": 6127.6, "text": " How to calculate derivatives or integrals, but you need to learn how to think about them spatially right and so remember", "tokens": [1012, 281, 8873, 33733, 420, 3572, 1124, 11, 457, 291, 643, 281, 1466, 577, 281, 519, 466, 552, 15000, 2270, 558, 293, 370, 1604], "temperature": 0.0, "avg_logprob": -0.18267393686685218, "compression_ratio": 1.6733668341708543, "no_speech_prob": 6.179387810334447e-07}, {"id": 1304, "seek": 611192, "start": 6128.2, "end": 6130.2, "text": " We have some", "tokens": [492, 362, 512], "temperature": 0.0, "avg_logprob": -0.18267393686685218, "compression_ratio": 1.6733668341708543, "no_speech_prob": 6.179387810334447e-07}, {"id": 1305, "seek": 611192, "start": 6131.16, "end": 6135.4800000000005, "text": " Vector very high dimensional vector. It's got like a million items in it right and", "tokens": [691, 20814, 588, 1090, 18795, 8062, 13, 467, 311, 658, 411, 257, 2459, 4754, 294, 309, 558, 293], "temperature": 0.0, "avg_logprob": -0.18267393686685218, "compression_ratio": 1.6733668341708543, "no_speech_prob": 6.179387810334447e-07}, {"id": 1306, "seek": 611192, "start": 6137.36, "end": 6139.36, "text": " It's going through", "tokens": [467, 311, 516, 807], "temperature": 0.0, "avg_logprob": -0.18267393686685218, "compression_ratio": 1.6733668341708543, "no_speech_prob": 6.179387810334447e-07}, {"id": 1307, "seek": 613936, "start": 6139.36, "end": 6144.639999999999, "text": " Some weight matrix right of size like 1 million by size 100,000", "tokens": [2188, 3364, 8141, 558, 295, 2744, 411, 502, 2459, 538, 2744, 2319, 11, 1360], "temperature": 0.0, "avg_logprob": -0.26625988095305686, "compression_ratio": 1.645, "no_speech_prob": 2.1568118881987175e-06}, {"id": 1308, "seek": 613936, "start": 6145.04, "end": 6148.719999999999, "text": " Or whatever and it's spitting out something of size 100,000", "tokens": [1610, 2035, 293, 309, 311, 637, 2414, 484, 746, 295, 2744, 2319, 11, 1360], "temperature": 0.0, "avg_logprob": -0.26625988095305686, "compression_ratio": 1.645, "no_speech_prob": 2.1568118881987175e-06}, {"id": 1309, "seek": 613936, "start": 6149.48, "end": 6150.599999999999, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.26625988095305686, "compression_ratio": 1.645, "no_speech_prob": 2.1568118881987175e-06}, {"id": 1310, "seek": 613936, "start": 6150.599999999999, "end": 6155.62, "text": " So you need to realize like there isn't like a gradient here", "tokens": [407, 291, 643, 281, 4325, 411, 456, 1943, 380, 411, 257, 16235, 510], "temperature": 0.0, "avg_logprob": -0.26625988095305686, "compression_ratio": 1.645, "no_speech_prob": 2.1568118881987175e-06}, {"id": 1311, "seek": 613936, "start": 6155.799999999999, "end": 6158.86, "text": " But it's like for every one of these things in this vector", "tokens": [583, 309, 311, 411, 337, 633, 472, 295, 613, 721, 294, 341, 8062], "temperature": 0.0, "avg_logprob": -0.26625988095305686, "compression_ratio": 1.645, "no_speech_prob": 2.1568118881987175e-06}, {"id": 1312, "seek": 613936, "start": 6159.46, "end": 6166.4, "text": " Right there's a gradient in every direction. You know in every part of the output", "tokens": [1779, 456, 311, 257, 16235, 294, 633, 3513, 13, 509, 458, 294, 633, 644, 295, 264, 5598], "temperature": 0.0, "avg_logprob": -0.26625988095305686, "compression_ratio": 1.645, "no_speech_prob": 2.1568118881987175e-06}, {"id": 1313, "seek": 616640, "start": 6166.4, "end": 6169.08, "text": " Right so it actually has", "tokens": [1779, 370, 309, 767, 575], "temperature": 0.0, "avg_logprob": -0.18700005095682026, "compression_ratio": 1.8, "no_speech_prob": 4.5921299829387863e-07}, {"id": 1314, "seek": 616640, "start": 6169.92, "end": 6173.16, "text": " Not a single gradient number not even a gradient", "tokens": [1726, 257, 2167, 16235, 1230, 406, 754, 257, 16235], "temperature": 0.0, "avg_logprob": -0.18700005095682026, "compression_ratio": 1.8, "no_speech_prob": 4.5921299829387863e-07}, {"id": 1315, "seek": 616640, "start": 6174.879999999999, "end": 6176.96, "text": " Vector but a gradient matrix", "tokens": [691, 20814, 457, 257, 16235, 8141], "temperature": 0.0, "avg_logprob": -0.18700005095682026, "compression_ratio": 1.8, "no_speech_prob": 4.5921299829387863e-07}, {"id": 1316, "seek": 616640, "start": 6178.879999999999, "end": 6180.879999999999, "text": " And so this", "tokens": [400, 370, 341], "temperature": 0.0, "avg_logprob": -0.18700005095682026, "compression_ratio": 1.8, "no_speech_prob": 4.5921299829387863e-07}, {"id": 1317, "seek": 616640, "start": 6181.32, "end": 6188.679999999999, "text": " This is a lot to calculate right I would literally have to like add a little bit to this and see what happens to all", "tokens": [639, 307, 257, 688, 281, 8873, 558, 286, 576, 3736, 362, 281, 411, 909, 257, 707, 857, 281, 341, 293, 536, 437, 2314, 281, 439], "temperature": 0.0, "avg_logprob": -0.18700005095682026, "compression_ratio": 1.8, "no_speech_prob": 4.5921299829387863e-07}, {"id": 1318, "seek": 616640, "start": 6188.679999999999, "end": 6193.4, "text": " Of these add a little bit to this see what happens to all of these right to fill in", "tokens": [2720, 613, 909, 257, 707, 857, 281, 341, 536, 437, 2314, 281, 439, 295, 613, 558, 281, 2836, 294], "temperature": 0.0, "avg_logprob": -0.18700005095682026, "compression_ratio": 1.8, "no_speech_prob": 4.5921299829387863e-07}, {"id": 1319, "seek": 619340, "start": 6193.4, "end": 6197.679999999999, "text": " one column of this at a time, so that's going to be", "tokens": [472, 7738, 295, 341, 412, 257, 565, 11, 370, 300, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.20405574177586755, "compression_ratio": 1.5919282511210762, "no_speech_prob": 9.422419111615454e-07}, {"id": 1320, "seek": 619340, "start": 6198.92, "end": 6204.719999999999, "text": " Horrendously slow like that that so that's why like if you're ever thinking like oh we can just do this with finite differencing", "tokens": [10691, 4542, 5098, 2964, 411, 300, 300, 370, 300, 311, 983, 411, 498, 291, 434, 1562, 1953, 411, 1954, 321, 393, 445, 360, 341, 365, 19362, 743, 13644], "temperature": 0.0, "avg_logprob": -0.20405574177586755, "compression_ratio": 1.5919282511210762, "no_speech_prob": 9.422419111615454e-07}, {"id": 1321, "seek": 619340, "start": 6204.839999999999, "end": 6210.599999999999, "text": " Just remember like okay. We we're dealing in the with these very high dimensional vectors where", "tokens": [1449, 1604, 411, 1392, 13, 492, 321, 434, 6260, 294, 264, 365, 613, 588, 1090, 18795, 18875, 689], "temperature": 0.0, "avg_logprob": -0.20405574177586755, "compression_ratio": 1.5919282511210762, "no_speech_prob": 9.422419111615454e-07}, {"id": 1322, "seek": 619340, "start": 6211.92, "end": 6213.92, "text": " You know this this kind of", "tokens": [509, 458, 341, 341, 733, 295], "temperature": 0.0, "avg_logprob": -0.20405574177586755, "compression_ratio": 1.5919282511210762, "no_speech_prob": 9.422419111615454e-07}, {"id": 1323, "seek": 619340, "start": 6215.879999999999, "end": 6219.799999999999, "text": " Matrix calculus like all the concepts are identical", "tokens": [36274, 33400, 411, 439, 264, 10392, 366, 14800], "temperature": 0.0, "avg_logprob": -0.20405574177586755, "compression_ratio": 1.5919282511210762, "no_speech_prob": 9.422419111615454e-07}, {"id": 1324, "seek": 621980, "start": 6219.8, "end": 6225.8, "text": " But when you actually draw it out like this you suddenly realize like okay for each number I could change", "tokens": [583, 562, 291, 767, 2642, 309, 484, 411, 341, 291, 5800, 4325, 411, 1392, 337, 1184, 1230, 286, 727, 1319], "temperature": 0.0, "avg_logprob": -0.14079166918384786, "compression_ratio": 1.656, "no_speech_prob": 7.112431603673031e-07}, {"id": 1325, "seek": 621980, "start": 6226.4400000000005, "end": 6232.08, "text": " There's a whole bunch of numbers that impacts, and I have this whole matrix of things to compute right and so", "tokens": [821, 311, 257, 1379, 3840, 295, 3547, 300, 11606, 11, 293, 286, 362, 341, 1379, 8141, 295, 721, 281, 14722, 558, 293, 370], "temperature": 0.0, "avg_logprob": -0.14079166918384786, "compression_ratio": 1.656, "no_speech_prob": 7.112431603673031e-07}, {"id": 1326, "seek": 621980, "start": 6232.64, "end": 6238.08, "text": " Your gradient calculations can take up a lot of memory and they can take up a lot of time", "tokens": [2260, 16235, 20448, 393, 747, 493, 257, 688, 295, 4675, 293, 436, 393, 747, 493, 257, 688, 295, 565], "temperature": 0.0, "avg_logprob": -0.14079166918384786, "compression_ratio": 1.656, "no_speech_prob": 7.112431603673031e-07}, {"id": 1327, "seek": 621980, "start": 6238.24, "end": 6243.16, "text": " So we want to find some way to do this more quickly, okay?", "tokens": [407, 321, 528, 281, 915, 512, 636, 281, 360, 341, 544, 2661, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.14079166918384786, "compression_ratio": 1.656, "no_speech_prob": 7.112431603673031e-07}, {"id": 1328, "seek": 621980, "start": 6245.2, "end": 6247.76, "text": " And it's definitely well worth like spending time", "tokens": [400, 309, 311, 2138, 731, 3163, 411, 6434, 565], "temperature": 0.0, "avg_logprob": -0.14079166918384786, "compression_ratio": 1.656, "no_speech_prob": 7.112431603673031e-07}, {"id": 1329, "seek": 624776, "start": 6247.76, "end": 6251.360000000001, "text": " kind of studying these ideas of like", "tokens": [733, 295, 7601, 613, 3487, 295, 411], "temperature": 0.0, "avg_logprob": -0.24799413979053497, "compression_ratio": 1.5497076023391814, "no_speech_prob": 3.393128054085537e-06}, {"id": 1330, "seek": 624776, "start": 6252.5, "end": 6257.8, "text": " You know the idea of like the gradients like look up things like Jacobian and", "tokens": [509, 458, 264, 1558, 295, 411, 264, 2771, 2448, 411, 574, 493, 721, 411, 14117, 952, 293], "temperature": 0.0, "avg_logprob": -0.24799413979053497, "compression_ratio": 1.5497076023391814, "no_speech_prob": 3.393128054085537e-06}, {"id": 1331, "seek": 624776, "start": 6260.8, "end": 6262.8, "text": " Hessian", "tokens": [35960, 952], "temperature": 0.0, "avg_logprob": -0.24799413979053497, "compression_ratio": 1.5497076023391814, "no_speech_prob": 3.393128054085537e-06}, {"id": 1332, "seek": 624776, "start": 6263.8, "end": 6266.4800000000005, "text": " They're the things that you want to search for to start", "tokens": [814, 434, 264, 721, 300, 291, 528, 281, 3164, 337, 281, 722], "temperature": 0.0, "avg_logprob": -0.24799413979053497, "compression_ratio": 1.5497076023391814, "no_speech_prob": 3.393128054085537e-06}, {"id": 1333, "seek": 624776, "start": 6267.72, "end": 6273.360000000001, "text": " Unfortunately people normally write about them with you know lots of Greek letters and", "tokens": [8590, 561, 5646, 2464, 466, 552, 365, 291, 458, 3195, 295, 10281, 7825, 293], "temperature": 0.0, "avg_logprob": -0.24799413979053497, "compression_ratio": 1.5497076023391814, "no_speech_prob": 3.393128054085537e-06}, {"id": 1334, "seek": 627336, "start": 6273.36, "end": 6278.24, "text": " Blah blah blah is right, but there are some there are some nice", "tokens": [2177, 545, 12288, 12288, 307, 558, 11, 457, 456, 366, 512, 456, 366, 512, 1481], "temperature": 0.0, "avg_logprob": -0.18410877747969193, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.785296712521813e-06}, {"id": 1335, "seek": 627336, "start": 6279.12, "end": 6284.799999999999, "text": " You know intuitive explanations out there, and hopefully you can share them on the forum if you find them because this is stuff", "tokens": [509, 458, 21769, 28708, 484, 456, 11, 293, 4696, 291, 393, 2073, 552, 322, 264, 17542, 498, 291, 915, 552, 570, 341, 307, 1507], "temperature": 0.0, "avg_logprob": -0.18410877747969193, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.785296712521813e-06}, {"id": 1336, "seek": 627336, "start": 6284.799999999999, "end": 6286.799999999999, "text": " You really need to", "tokens": [509, 534, 643, 281], "temperature": 0.0, "avg_logprob": -0.18410877747969193, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.785296712521813e-06}, {"id": 1337, "seek": 627336, "start": 6287.16, "end": 6289.16, "text": " Really need to understand in here", "tokens": [4083, 643, 281, 1223, 294, 510], "temperature": 0.0, "avg_logprob": -0.18410877747969193, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.785296712521813e-06}, {"id": 1338, "seek": 627336, "start": 6291.2, "end": 6298.0, "text": " You know because you're trying to train something and it's not working properly and like later on we'll learn how to like look inside", "tokens": [509, 458, 570, 291, 434, 1382, 281, 3847, 746, 293, 309, 311, 406, 1364, 6108, 293, 411, 1780, 322, 321, 603, 1466, 577, 281, 411, 574, 1854], "temperature": 0.0, "avg_logprob": -0.18410877747969193, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.785296712521813e-06}, {"id": 1339, "seek": 629800, "start": 6298.0, "end": 6304.8, "text": " Pi torch to like actually get the values of the gradients, and you need to know like okay. Well how would I like plot the gradients?", "tokens": [17741, 27822, 281, 411, 767, 483, 264, 4190, 295, 264, 2771, 2448, 11, 293, 291, 643, 281, 458, 411, 1392, 13, 1042, 577, 576, 286, 411, 7542, 264, 2771, 2448, 30], "temperature": 0.0, "avg_logprob": -0.16198561484353585, "compression_ratio": 1.8731884057971016, "no_speech_prob": 3.9669575926382095e-06}, {"id": 1340, "seek": 629800, "start": 6305.04, "end": 6311.08, "text": " You know what would I consider unusual like you know these are the things that turn you into a really awesome", "tokens": [509, 458, 437, 576, 286, 1949, 10901, 411, 291, 458, 613, 366, 264, 721, 300, 1261, 291, 666, 257, 534, 3476], "temperature": 0.0, "avg_logprob": -0.16198561484353585, "compression_ratio": 1.8731884057971016, "no_speech_prob": 3.9669575926382095e-06}, {"id": 1341, "seek": 629800, "start": 6311.8, "end": 6319.08, "text": " Deep learning practitioner is when you can like debug your problems by like grabbing the gradients and doing histograms of them and like knowing", "tokens": [14895, 2539, 32125, 307, 562, 291, 393, 411, 24083, 428, 2740, 538, 411, 23771, 264, 2771, 2448, 293, 884, 49816, 82, 295, 552, 293, 411, 5276], "temperature": 0.0, "avg_logprob": -0.16198561484353585, "compression_ratio": 1.8731884057971016, "no_speech_prob": 3.9669575926382095e-06}, {"id": 1342, "seek": 629800, "start": 6319.2, "end": 6326.36, "text": " You know that you could like plot that all each layer my average gradients getting worse, or you know bigger or you know whatever", "tokens": [509, 458, 300, 291, 727, 411, 7542, 300, 439, 1184, 4583, 452, 4274, 2771, 2448, 1242, 5324, 11, 420, 291, 458, 3801, 420, 291, 458, 2035], "temperature": 0.0, "avg_logprob": -0.16198561484353585, "compression_ratio": 1.8731884057971016, "no_speech_prob": 3.9669575926382095e-06}, {"id": 1343, "seek": 632636, "start": 6326.36, "end": 6331.0199999999995, "text": " Okay, so the trick to doing this more quickly is to do it", "tokens": [1033, 11, 370, 264, 4282, 281, 884, 341, 544, 2661, 307, 281, 360, 309], "temperature": 0.0, "avg_logprob": -0.20722364069341304, "compression_ratio": 1.724770642201835, "no_speech_prob": 1.7061745438695652e-06}, {"id": 1344, "seek": 632636, "start": 6332.04, "end": 6333.24, "text": " analytically", "tokens": [10783, 984], "temperature": 0.0, "avg_logprob": -0.20722364069341304, "compression_ratio": 1.724770642201835, "no_speech_prob": 1.7061745438695652e-06}, {"id": 1345, "seek": 632636, "start": 6333.24, "end": 6335.24, "text": " rather than through finite differencing and", "tokens": [2831, 813, 807, 19362, 743, 13644, 293], "temperature": 0.0, "avg_logprob": -0.20722364069341304, "compression_ratio": 1.724770642201835, "no_speech_prob": 1.7061745438695652e-06}, {"id": 1346, "seek": 632636, "start": 6336.0, "end": 6341.48, "text": " So analytically is basically there is a list you probably all learned it at high school", "tokens": [407, 10783, 984, 307, 1936, 456, 307, 257, 1329, 291, 1391, 439, 3264, 309, 412, 1090, 1395], "temperature": 0.0, "avg_logprob": -0.20722364069341304, "compression_ratio": 1.724770642201835, "no_speech_prob": 1.7061745438695652e-06}, {"id": 1347, "seek": 632636, "start": 6341.48, "end": 6344.4, "text": " There is a literally a list of rules that for every", "tokens": [821, 307, 257, 3736, 257, 1329, 295, 4474, 300, 337, 633], "temperature": 0.0, "avg_logprob": -0.20722364069341304, "compression_ratio": 1.724770642201835, "no_speech_prob": 1.7061745438695652e-06}, {"id": 1348, "seek": 632636, "start": 6345.28, "end": 6350.44, "text": " Mathematical function there's a like this is the derivative of that function right so", "tokens": [15776, 8615, 804, 2445, 456, 311, 257, 411, 341, 307, 264, 13760, 295, 300, 2445, 558, 370], "temperature": 0.0, "avg_logprob": -0.20722364069341304, "compression_ratio": 1.724770642201835, "no_speech_prob": 1.7061745438695652e-06}, {"id": 1349, "seek": 632636, "start": 6351.48, "end": 6353.679999999999, "text": " You probably remember a few of them", "tokens": [509, 1391, 1604, 257, 1326, 295, 552], "temperature": 0.0, "avg_logprob": -0.20722364069341304, "compression_ratio": 1.724770642201835, "no_speech_prob": 1.7061745438695652e-06}, {"id": 1350, "seek": 635368, "start": 6353.68, "end": 6355.68, "text": " for example", "tokens": [337, 1365], "temperature": 0.0, "avg_logprob": -0.2646162033081055, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.7880605582831777e-06}, {"id": 1351, "seek": 635368, "start": 6357.240000000001, "end": 6359.240000000001, "text": " X squared", "tokens": [1783, 8889], "temperature": 0.0, "avg_logprob": -0.2646162033081055, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.7880605582831777e-06}, {"id": 1352, "seek": 635368, "start": 6359.52, "end": 6363.88, "text": " 2 X right and so we actually have here an x squared", "tokens": [568, 1783, 558, 293, 370, 321, 767, 362, 510, 364, 2031, 8889], "temperature": 0.0, "avg_logprob": -0.2646162033081055, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.7880605582831777e-06}, {"id": 1353, "seek": 635368, "start": 6364.4400000000005, "end": 6368.820000000001, "text": " So here is our 2 times right now the one that I actually want you", "tokens": [407, 510, 307, 527, 568, 1413, 558, 586, 264, 472, 300, 286, 767, 528, 291], "temperature": 0.0, "avg_logprob": -0.2646162033081055, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.7880605582831777e-06}, {"id": 1354, "seek": 635368, "start": 6369.96, "end": 6371.96, "text": " to know is", "tokens": [281, 458, 307], "temperature": 0.0, "avg_logprob": -0.2646162033081055, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.7880605582831777e-06}, {"id": 1355, "seek": 635368, "start": 6372.200000000001, "end": 6377.08, "text": " Not any of the individual rules, but I want you to know the chain rule", "tokens": [1726, 604, 295, 264, 2609, 4474, 11, 457, 286, 528, 291, 281, 458, 264, 5021, 4978], "temperature": 0.0, "avg_logprob": -0.2646162033081055, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.7880605582831777e-06}, {"id": 1356, "seek": 635368, "start": 6377.68, "end": 6379.56, "text": " right which is", "tokens": [558, 597, 307], "temperature": 0.0, "avg_logprob": -0.2646162033081055, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.7880605582831777e-06}, {"id": 1357, "seek": 635368, "start": 6379.56, "end": 6381.56, "text": " You've got some function", "tokens": [509, 600, 658, 512, 2445], "temperature": 0.0, "avg_logprob": -0.2646162033081055, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.7880605582831777e-06}, {"id": 1358, "seek": 638156, "start": 6381.56, "end": 6386.400000000001, "text": " Of some function of something why is this important? I?", "tokens": [2720, 512, 2445, 295, 746, 983, 307, 341, 1021, 30, 286, 30], "temperature": 0.0, "avg_logprob": -0.2502952617603344, "compression_ratio": 1.6961325966850829, "no_speech_prob": 7.5279576776665635e-06}, {"id": 1359, "seek": 638156, "start": 6387.0, "end": 6389.92, "text": " Don't know that's a linear layer. That's a relu", "tokens": [1468, 380, 458, 300, 311, 257, 8213, 4583, 13, 663, 311, 257, 1039, 84], "temperature": 0.0, "avg_logprob": -0.2502952617603344, "compression_ratio": 1.6961325966850829, "no_speech_prob": 7.5279576776665635e-06}, {"id": 1360, "seek": 638156, "start": 6390.56, "end": 6392.56, "text": " right and", "tokens": [558, 293], "temperature": 0.0, "avg_logprob": -0.2502952617603344, "compression_ratio": 1.6961325966850829, "no_speech_prob": 7.5279576776665635e-06}, {"id": 1361, "seek": 638156, "start": 6392.88, "end": 6394.88, "text": " Then we can kind of keep going backwards", "tokens": [1396, 321, 393, 733, 295, 1066, 516, 12204], "temperature": 0.0, "avg_logprob": -0.2502952617603344, "compression_ratio": 1.6961325966850829, "no_speech_prob": 7.5279576776665635e-06}, {"id": 1362, "seek": 638156, "start": 6397.64, "end": 6399.88, "text": " Etc but a neural net", "tokens": [3790, 66, 457, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.2502952617603344, "compression_ratio": 1.6961325966850829, "no_speech_prob": 7.5279576776665635e-06}, {"id": 1363, "seek": 638156, "start": 6400.72, "end": 6405.8, "text": " Is just a function of a function of a function of a function where the innermost is you know it's basically linear", "tokens": [1119, 445, 257, 2445, 295, 257, 2445, 295, 257, 2445, 295, 257, 2445, 689, 264, 7714, 966, 555, 307, 291, 458, 309, 311, 1936, 8213], "temperature": 0.0, "avg_logprob": -0.2502952617603344, "compression_ratio": 1.6961325966850829, "no_speech_prob": 7.5279576776665635e-06}, {"id": 1364, "seek": 638156, "start": 6406.360000000001, "end": 6407.88, "text": " relu", "tokens": [1039, 84], "temperature": 0.0, "avg_logprob": -0.2502952617603344, "compression_ratio": 1.6961325966850829, "no_speech_prob": 7.5279576776665635e-06}, {"id": 1365, "seek": 638156, "start": 6407.88, "end": 6409.080000000001, "text": " linear", "tokens": [8213], "temperature": 0.0, "avg_logprob": -0.2502952617603344, "compression_ratio": 1.6961325966850829, "no_speech_prob": 7.5279576776665635e-06}, {"id": 1366, "seek": 640908, "start": 6409.08, "end": 6412.08, "text": " relu", "tokens": [1039, 84], "temperature": 0.0, "avg_logprob": -0.2816305894118089, "compression_ratio": 1.7791411042944785, "no_speech_prob": 4.936959612678038e-06}, {"id": 1367, "seek": 640908, "start": 6412.08, "end": 6414.5599999999995, "text": " dot dot dot linear", "tokens": [5893, 5893, 5893, 8213], "temperature": 0.0, "avg_logprob": -0.2816305894118089, "compression_ratio": 1.7791411042944785, "no_speech_prob": 4.936959612678038e-06}, {"id": 1368, "seek": 640908, "start": 6416.32, "end": 6418.4, "text": " sigmoid or softmax", "tokens": [4556, 3280, 327, 420, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.2816305894118089, "compression_ratio": 1.7791411042944785, "no_speech_prob": 4.936959612678038e-06}, {"id": 1369, "seek": 640908, "start": 6420.5599999999995, "end": 6425.72, "text": " All right, and so it's a function of a function of a function and so therefore to calculate the derivative of", "tokens": [1057, 558, 11, 293, 370, 309, 311, 257, 2445, 295, 257, 2445, 295, 257, 2445, 293, 370, 4412, 281, 8873, 264, 13760, 295], "temperature": 0.0, "avg_logprob": -0.2816305894118089, "compression_ratio": 1.7791411042944785, "no_speech_prob": 4.936959612678038e-06}, {"id": 1370, "seek": 640908, "start": 6426.44, "end": 6428.5599999999995, "text": " the weights in your model", "tokens": [264, 17443, 294, 428, 2316], "temperature": 0.0, "avg_logprob": -0.2816305894118089, "compression_ratio": 1.7791411042944785, "no_speech_prob": 4.936959612678038e-06}, {"id": 1371, "seek": 640908, "start": 6429.68, "end": 6434.24, "text": " The loss of your model with respect to the weights of your model you're going to need to use the chain rule and", "tokens": [440, 4470, 295, 428, 2316, 365, 3104, 281, 264, 17443, 295, 428, 2316, 291, 434, 516, 281, 643, 281, 764, 264, 5021, 4978, 293], "temperature": 0.0, "avg_logprob": -0.2816305894118089, "compression_ratio": 1.7791411042944785, "no_speech_prob": 4.936959612678038e-06}, {"id": 1372, "seek": 643424, "start": 6434.24, "end": 6439.36, "text": " Specifically whatever layer it is that you're up to like I want to calculate the derivative here", "tokens": [26058, 2035, 4583, 309, 307, 300, 291, 434, 493, 281, 411, 286, 528, 281, 8873, 264, 13760, 510], "temperature": 0.0, "avg_logprob": -0.17859179302326683, "compression_ratio": 1.8222222222222222, "no_speech_prob": 1.228915607498493e-06}, {"id": 1373, "seek": 643424, "start": 6439.36, "end": 6441.84, "text": " I'm going to need to use all of these", "tokens": [286, 478, 516, 281, 643, 281, 764, 439, 295, 613], "temperature": 0.0, "avg_logprob": -0.17859179302326683, "compression_ratio": 1.8222222222222222, "no_speech_prob": 1.228915607498493e-06}, {"id": 1374, "seek": 643424, "start": 6442.599999999999, "end": 6445.719999999999, "text": " All of these ones because that's all that's that's the function that's being applied", "tokens": [1057, 295, 613, 2306, 570, 300, 311, 439, 300, 311, 300, 311, 264, 2445, 300, 311, 885, 6456], "temperature": 0.0, "avg_logprob": -0.17859179302326683, "compression_ratio": 1.8222222222222222, "no_speech_prob": 1.228915607498493e-06}, {"id": 1375, "seek": 643424, "start": 6446.04, "end": 6450.8, "text": " right, and that's why they call this back propagation because the value of the derivative of", "tokens": [558, 11, 293, 300, 311, 983, 436, 818, 341, 646, 38377, 570, 264, 2158, 295, 264, 13760, 295], "temperature": 0.0, "avg_logprob": -0.17859179302326683, "compression_ratio": 1.8222222222222222, "no_speech_prob": 1.228915607498493e-06}, {"id": 1376, "seek": 643424, "start": 6452.0, "end": 6454.0, "text": " that is", "tokens": [300, 307], "temperature": 0.0, "avg_logprob": -0.17859179302326683, "compression_ratio": 1.8222222222222222, "no_speech_prob": 1.228915607498493e-06}, {"id": 1377, "seek": 643424, "start": 6454.04, "end": 6455.84, "text": " equal to", "tokens": [2681, 281], "temperature": 0.0, "avg_logprob": -0.17859179302326683, "compression_ratio": 1.8222222222222222, "no_speech_prob": 1.228915607498493e-06}, {"id": 1378, "seek": 643424, "start": 6455.84, "end": 6457.84, "text": " that derivative", "tokens": [300, 13760], "temperature": 0.0, "avg_logprob": -0.17859179302326683, "compression_ratio": 1.8222222222222222, "no_speech_prob": 1.228915607498493e-06}, {"id": 1379, "seek": 643424, "start": 6457.84, "end": 6461.5199999999995, "text": " Now basically you can do it like this you can say let's call you", "tokens": [823, 1936, 291, 393, 360, 309, 411, 341, 291, 393, 584, 718, 311, 818, 291], "temperature": 0.0, "avg_logprob": -0.17859179302326683, "compression_ratio": 1.8222222222222222, "no_speech_prob": 1.228915607498493e-06}, {"id": 1380, "seek": 646152, "start": 6461.52, "end": 6467.8, "text": " Is this right let's call that you right then it's simply equal to", "tokens": [1119, 341, 558, 718, 311, 818, 300, 291, 558, 550, 309, 311, 2935, 2681, 281], "temperature": 0.0, "avg_logprob": -0.22515304367263594, "compression_ratio": 1.7283236994219653, "no_speech_prob": 6.786724497942487e-07}, {"id": 1381, "seek": 646152, "start": 6469.92, "end": 6472.820000000001, "text": " The derivative of that times", "tokens": [440, 13760, 295, 300, 1413], "temperature": 0.0, "avg_logprob": -0.22515304367263594, "compression_ratio": 1.7283236994219653, "no_speech_prob": 6.786724497942487e-07}, {"id": 1382, "seek": 646152, "start": 6474.4800000000005, "end": 6478.040000000001, "text": " Derivative of that right you just multiply them together and", "tokens": [5618, 592, 1166, 295, 300, 558, 291, 445, 12972, 552, 1214, 293], "temperature": 0.0, "avg_logprob": -0.22515304367263594, "compression_ratio": 1.7283236994219653, "no_speech_prob": 6.786724497942487e-07}, {"id": 1383, "seek": 646152, "start": 6478.68, "end": 6484.84, "text": " So that's what back propagation is like it's not that back propagation is a new thing for you to learn", "tokens": [407, 300, 311, 437, 646, 38377, 307, 411, 309, 311, 406, 300, 646, 38377, 307, 257, 777, 551, 337, 291, 281, 1466], "temperature": 0.0, "avg_logprob": -0.22515304367263594, "compression_ratio": 1.7283236994219653, "no_speech_prob": 6.786724497942487e-07}, {"id": 1384, "seek": 646152, "start": 6485.120000000001, "end": 6487.120000000001, "text": " It's not a new algorithm", "tokens": [467, 311, 406, 257, 777, 9284], "temperature": 0.0, "avg_logprob": -0.22515304367263594, "compression_ratio": 1.7283236994219653, "no_speech_prob": 6.786724497942487e-07}, {"id": 1385, "seek": 646152, "start": 6487.4400000000005, "end": 6489.320000000001, "text": " It is literally", "tokens": [467, 307, 3736], "temperature": 0.0, "avg_logprob": -0.22515304367263594, "compression_ratio": 1.7283236994219653, "no_speech_prob": 6.786724497942487e-07}, {"id": 1386, "seek": 648932, "start": 6489.32, "end": 6492.639999999999, "text": " Take the derivative of every one of your layers and", "tokens": [3664, 264, 13760, 295, 633, 472, 295, 428, 7914, 293], "temperature": 0.0, "avg_logprob": -0.1934992472330729, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.3287746014611912e-06}, {"id": 1387, "seek": 648932, "start": 6493.58, "end": 6495.58, "text": " Multiply them all together", "tokens": [31150, 356, 552, 439, 1214], "temperature": 0.0, "avg_logprob": -0.1934992472330729, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.3287746014611912e-06}, {"id": 1388, "seek": 648932, "start": 6495.58, "end": 6497.84, "text": " So like it doesn't deserve a new name", "tokens": [407, 411, 309, 1177, 380, 9948, 257, 777, 1315], "temperature": 0.0, "avg_logprob": -0.1934992472330729, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.3287746014611912e-06}, {"id": 1389, "seek": 648932, "start": 6498.4, "end": 6501.92, "text": " Right apply the chain rule to my layers", "tokens": [1779, 3079, 264, 5021, 4978, 281, 452, 7914], "temperature": 0.0, "avg_logprob": -0.1934992472330729, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.3287746014611912e-06}, {"id": 1390, "seek": 648932, "start": 6502.5599999999995, "end": 6509.16, "text": " Does not deserve a new lame, but it gets one because us neural networks folk really need to seem as clever as possible", "tokens": [4402, 406, 9948, 257, 777, 27635, 11, 457, 309, 2170, 472, 570, 505, 18161, 9590, 15748, 534, 643, 281, 1643, 382, 13494, 382, 1944], "temperature": 0.0, "avg_logprob": -0.1934992472330729, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.3287746014611912e-06}, {"id": 1391, "seek": 648932, "start": 6509.44, "end": 6514.92, "text": " It's really important, but everybody else thinks that we are way outside of their capabilities", "tokens": [467, 311, 534, 1021, 11, 457, 2201, 1646, 7309, 300, 321, 366, 636, 2380, 295, 641, 10862], "temperature": 0.0, "avg_logprob": -0.1934992472330729, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.3287746014611912e-06}, {"id": 1392, "seek": 651492, "start": 6514.92, "end": 6522.6, "text": " Right so the fact that you're here means that we failed because you guys somehow think that you're capable right so remember", "tokens": [1779, 370, 264, 1186, 300, 291, 434, 510, 1355, 300, 321, 7612, 570, 291, 1074, 6063, 519, 300, 291, 434, 8189, 558, 370, 1604], "temperature": 0.0, "avg_logprob": -0.16651517495341683, "compression_ratio": 1.6077586206896552, "no_speech_prob": 2.0904499251628295e-06}, {"id": 1393, "seek": 651492, "start": 6522.6, "end": 6529.8, "text": " It's really important when you talk to other people that you say back propagation and rectified linear unit rather than like", "tokens": [467, 311, 534, 1021, 562, 291, 751, 281, 661, 561, 300, 291, 584, 646, 38377, 293, 11048, 2587, 8213, 4985, 2831, 813, 411], "temperature": 0.0, "avg_logprob": -0.16651517495341683, "compression_ratio": 1.6077586206896552, "no_speech_prob": 2.0904499251628295e-06}, {"id": 1394, "seek": 651492, "start": 6530.4800000000005, "end": 6532.16, "text": " multiply the layers", "tokens": [12972, 264, 7914], "temperature": 0.0, "avg_logprob": -0.16651517495341683, "compression_ratio": 1.6077586206896552, "no_speech_prob": 2.0904499251628295e-06}, {"id": 1395, "seek": 651492, "start": 6532.16, "end": 6533.4, "text": " gradients or", "tokens": [2771, 2448, 420], "temperature": 0.0, "avg_logprob": -0.16651517495341683, "compression_ratio": 1.6077586206896552, "no_speech_prob": 2.0904499251628295e-06}, {"id": 1396, "seek": 651492, "start": 6533.4, "end": 6539.56, "text": " Replace negatives with zeros okay, so so here we go so here is so I've just gone ahead and", "tokens": [1300, 6742, 40019, 365, 35193, 1392, 11, 370, 370, 510, 321, 352, 370, 510, 307, 370, 286, 600, 445, 2780, 2286, 293], "temperature": 0.0, "avg_logprob": -0.16651517495341683, "compression_ratio": 1.6077586206896552, "no_speech_prob": 2.0904499251628295e-06}, {"id": 1397, "seek": 653956, "start": 6539.56, "end": 6545.8, "text": " Grabbed the derivative unfortunately there is no automatic differentiation in Excel yet", "tokens": [20357, 2883, 264, 13760, 7015, 456, 307, 572, 12509, 38902, 294, 19060, 1939], "temperature": 0.0, "avg_logprob": -0.22908301580519902, "compression_ratio": 1.8303249097472925, "no_speech_prob": 2.9944253583380487e-06}, {"id": 1398, "seek": 653956, "start": 6545.96, "end": 6552.120000000001, "text": " So I did the alternative which is to paste the formula into wolfram alpha and got back the derivatives", "tokens": [407, 286, 630, 264, 8535, 597, 307, 281, 9163, 264, 8513, 666, 19216, 2356, 8961, 293, 658, 646, 264, 33733], "temperature": 0.0, "avg_logprob": -0.22908301580519902, "compression_ratio": 1.8303249097472925, "no_speech_prob": 2.9944253583380487e-06}, {"id": 1399, "seek": 653956, "start": 6552.120000000001, "end": 6554.6, "text": " So there's the first derivative, and there's the second derivative", "tokens": [407, 456, 311, 264, 700, 13760, 11, 293, 456, 311, 264, 1150, 13760], "temperature": 0.0, "avg_logprob": -0.22908301580519902, "compression_ratio": 1.8303249097472925, "no_speech_prob": 2.9944253583380487e-06}, {"id": 1400, "seek": 653956, "start": 6555.320000000001, "end": 6558.240000000001, "text": " Analytically we only have one layer in this", "tokens": [23688, 984, 321, 787, 362, 472, 4583, 294, 341], "temperature": 0.0, "avg_logprob": -0.22908301580519902, "compression_ratio": 1.8303249097472925, "no_speech_prob": 2.9944253583380487e-06}, {"id": 1401, "seek": 653956, "start": 6559.120000000001, "end": 6562.88, "text": " Infinite tiny small neural network, so we don't have to worry about the chain rule", "tokens": [43368, 5870, 1359, 18161, 3209, 11, 370, 321, 500, 380, 362, 281, 3292, 466, 264, 5021, 4978], "temperature": 0.0, "avg_logprob": -0.22908301580519902, "compression_ratio": 1.8303249097472925, "no_speech_prob": 2.9944253583380487e-06}, {"id": 1402, "seek": 656288, "start": 6562.88, "end": 6569.28, "text": " And we should see that this analytical derivative is pretty close to our estimated derivative from the finite differencing and", "tokens": [400, 321, 820, 536, 300, 341, 29579, 13760, 307, 1238, 1998, 281, 527, 14109, 13760, 490, 264, 19362, 743, 13644, 293], "temperature": 0.0, "avg_logprob": -0.17516237497329712, "compression_ratio": 1.732, "no_speech_prob": 3.6119654396316037e-06}, {"id": 1403, "seek": 656288, "start": 6569.92, "end": 6574.52, "text": " Indeed it is right and we should see that these ones are pretty similar as well", "tokens": [15061, 309, 307, 558, 293, 321, 820, 536, 300, 613, 2306, 366, 1238, 2531, 382, 731], "temperature": 0.0, "avg_logprob": -0.17516237497329712, "compression_ratio": 1.732, "no_speech_prob": 3.6119654396316037e-06}, {"id": 1404, "seek": 656288, "start": 6574.52, "end": 6578.68, "text": " And indeed they are right and if you're you know back when I?", "tokens": [400, 6451, 436, 366, 558, 293, 498, 291, 434, 291, 458, 646, 562, 286, 30], "temperature": 0.0, "avg_logprob": -0.17516237497329712, "compression_ratio": 1.732, "no_speech_prob": 3.6119654396316037e-06}, {"id": 1405, "seek": 656288, "start": 6579.4400000000005, "end": 6582.24, "text": " Implemented my own neural nets 20 years ago", "tokens": [4331, 781, 14684, 452, 1065, 18161, 36170, 945, 924, 2057], "temperature": 0.0, "avg_logprob": -0.17516237497329712, "compression_ratio": 1.732, "no_speech_prob": 3.6119654396316037e-06}, {"id": 1406, "seek": 656288, "start": 6582.24, "end": 6589.04, "text": " I you know had to actually calculate the derivatives and so I always would write like had something that would check the", "tokens": [286, 291, 458, 632, 281, 767, 8873, 264, 33733, 293, 370, 286, 1009, 576, 2464, 411, 632, 746, 300, 576, 1520, 264], "temperature": 0.0, "avg_logprob": -0.17516237497329712, "compression_ratio": 1.732, "no_speech_prob": 3.6119654396316037e-06}, {"id": 1407, "seek": 658904, "start": 6589.04, "end": 6594.08, "text": " Derivatives using finite differencing and so for those poor people that do have to write these things by hand", "tokens": [5618, 592, 4884, 1228, 19362, 743, 13644, 293, 370, 337, 729, 4716, 561, 300, 360, 362, 281, 2464, 613, 721, 538, 1011], "temperature": 0.0, "avg_logprob": -0.16553462947811093, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.7266247545630904e-06}, {"id": 1408, "seek": 658904, "start": 6594.24, "end": 6601.72, "text": " You'll still see that they have like a finite differencing checker, so if you ever do have to implement a derivative by hand", "tokens": [509, 603, 920, 536, 300, 436, 362, 411, 257, 19362, 743, 13644, 1520, 260, 11, 370, 498, 291, 1562, 360, 362, 281, 4445, 257, 13760, 538, 1011], "temperature": 0.0, "avg_logprob": -0.16553462947811093, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.7266247545630904e-06}, {"id": 1409, "seek": 658904, "start": 6602.16, "end": 6604.16, "text": " Please make sure that you", "tokens": [2555, 652, 988, 300, 291], "temperature": 0.0, "avg_logprob": -0.16553462947811093, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.7266247545630904e-06}, {"id": 1410, "seek": 658904, "start": 6604.6, "end": 6607.5199999999995, "text": " Have a finite differencing checker so that you can test it", "tokens": [3560, 257, 19362, 743, 13644, 1520, 260, 370, 300, 291, 393, 1500, 309], "temperature": 0.0, "avg_logprob": -0.16553462947811093, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.7266247545630904e-06}, {"id": 1411, "seek": 658904, "start": 6608.16, "end": 6609.5199999999995, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.16553462947811093, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.7266247545630904e-06}, {"id": 1412, "seek": 658904, "start": 6609.5199999999995, "end": 6611.5199999999995, "text": " So there's our derivatives", "tokens": [407, 456, 311, 527, 33733], "temperature": 0.0, "avg_logprob": -0.16553462947811093, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.7266247545630904e-06}, {"id": 1413, "seek": 658904, "start": 6611.96, "end": 6614.12, "text": " So we know that if we increase", "tokens": [407, 321, 458, 300, 498, 321, 3488], "temperature": 0.0, "avg_logprob": -0.16553462947811093, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.7266247545630904e-06}, {"id": 1414, "seek": 658904, "start": 6615.08, "end": 6617.96, "text": " B, then we're going to get a slightly better loss", "tokens": [363, 11, 550, 321, 434, 516, 281, 483, 257, 4748, 1101, 4470], "temperature": 0.0, "avg_logprob": -0.16553462947811093, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.7266247545630904e-06}, {"id": 1415, "seek": 661796, "start": 6617.96, "end": 6622.36, "text": " So let's increase B by a bit how much should we increase it by?", "tokens": [407, 718, 311, 3488, 363, 538, 257, 857, 577, 709, 820, 321, 3488, 309, 538, 30], "temperature": 0.0, "avg_logprob": -0.2808583427877987, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1416, "seek": 661796, "start": 6622.88, "end": 6627.32, "text": " Well, we'll increase it by some multiple of this and the multiple we're going to choose is called a learning rate", "tokens": [1042, 11, 321, 603, 3488, 309, 538, 512, 3866, 295, 341, 293, 264, 3866, 321, 434, 516, 281, 2826, 307, 1219, 257, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.2808583427877987, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1417, "seek": 661796, "start": 6627.32, "end": 6630.38, "text": " And so here's our learning rate, so here's 1e neg 4", "tokens": [400, 370, 510, 311, 527, 2539, 3314, 11, 370, 510, 311, 502, 68, 2485, 1017], "temperature": 0.0, "avg_logprob": -0.2808583427877987, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1418, "seek": 661796, "start": 6630.92, "end": 6633.54, "text": " Okay, so our new value", "tokens": [1033, 11, 370, 527, 777, 2158], "temperature": 0.0, "avg_logprob": -0.2808583427877987, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1419, "seek": 661796, "start": 6635.04, "end": 6638.0, "text": " Is equal to whatever it was before", "tokens": [1119, 2681, 281, 2035, 309, 390, 949], "temperature": 0.0, "avg_logprob": -0.2808583427877987, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1420, "seek": 661796, "start": 6640.08, "end": 6642.08, "text": " Minus our", "tokens": [2829, 301, 527], "temperature": 0.0, "avg_logprob": -0.2808583427877987, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.5534943713646499e-06}, {"id": 1421, "seek": 664208, "start": 6642.08, "end": 6648.48, "text": " Derivative times our learning rate okay, so we've gone from 1 to 1 point 0 1 and", "tokens": [5618, 592, 1166, 1413, 527, 2539, 3314, 1392, 11, 370, 321, 600, 2780, 490, 502, 281, 502, 935, 1958, 502, 293], "temperature": 0.0, "avg_logprob": -0.22727960643201772, "compression_ratio": 1.7064676616915422, "no_speech_prob": 8.446214110335859e-07}, {"id": 1422, "seek": 664208, "start": 6649.6, "end": 6651.32, "text": " then a", "tokens": [550, 257], "temperature": 0.0, "avg_logprob": -0.22727960643201772, "compression_ratio": 1.7064676616915422, "no_speech_prob": 8.446214110335859e-07}, {"id": 1423, "seek": 664208, "start": 6651.32, "end": 6654.72, "text": " We've done the same thing so it's gone from 1 to", "tokens": [492, 600, 1096, 264, 912, 551, 370, 309, 311, 2780, 490, 502, 281], "temperature": 0.0, "avg_logprob": -0.22727960643201772, "compression_ratio": 1.7064676616915422, "no_speech_prob": 8.446214110335859e-07}, {"id": 1424, "seek": 664208, "start": 6655.64, "end": 6657.64, "text": " 1 point 1 2", "tokens": [502, 935, 502, 568], "temperature": 0.0, "avg_logprob": -0.22727960643201772, "compression_ratio": 1.7064676616915422, "no_speech_prob": 8.446214110335859e-07}, {"id": 1425, "seek": 664208, "start": 6658.96, "end": 6665.12, "text": " So this is a special kind of mini batch. It's a mini batch of size 1 okay, so we call this online gradient descent", "tokens": [407, 341, 307, 257, 2121, 733, 295, 8382, 15245, 13, 467, 311, 257, 8382, 15245, 295, 2744, 502, 1392, 11, 370, 321, 818, 341, 2950, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.22727960643201772, "compression_ratio": 1.7064676616915422, "no_speech_prob": 8.446214110335859e-07}, {"id": 1426, "seek": 666512, "start": 6665.12, "end": 6671.48, "text": " It just means mini batch of size 1 so then we can go on to the next one X is 86", "tokens": [467, 445, 1355, 8382, 15245, 295, 2744, 502, 370, 550, 321, 393, 352, 322, 281, 264, 958, 472, 1783, 307, 26687], "temperature": 0.0, "avg_logprob": -0.20071712616951234, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.338197747230879e-07}, {"id": 1427, "seek": 666512, "start": 6672.08, "end": 6677.92, "text": " Y is 202 right this is my intercept and slope copied across from the last row", "tokens": [398, 307, 945, 17, 558, 341, 307, 452, 24700, 293, 13525, 25365, 2108, 490, 264, 1036, 5386], "temperature": 0.0, "avg_logprob": -0.20071712616951234, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.338197747230879e-07}, {"id": 1428, "seek": 666512, "start": 6679.88, "end": 6681.12, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20071712616951234, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.338197747230879e-07}, {"id": 1429, "seek": 666512, "start": 6681.12, "end": 6684.5199999999995, "text": " So here's my new y prediction. Here's my new error", "tokens": [407, 510, 311, 452, 777, 288, 17630, 13, 1692, 311, 452, 777, 6713], "temperature": 0.0, "avg_logprob": -0.20071712616951234, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.338197747230879e-07}, {"id": 1430, "seek": 666512, "start": 6685.44, "end": 6687.44, "text": " Here are my derivatives", "tokens": [1692, 366, 452, 33733], "temperature": 0.0, "avg_logprob": -0.20071712616951234, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.338197747230879e-07}, {"id": 1431, "seek": 666512, "start": 6687.5199999999995, "end": 6689.5199999999995, "text": " Here are my new a and b", "tokens": [1692, 366, 452, 777, 257, 293, 272], "temperature": 0.0, "avg_logprob": -0.20071712616951234, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.338197747230879e-07}, {"id": 1432, "seek": 666512, "start": 6690.0, "end": 6693.16, "text": " All right, so we keep doing that for every mini batch of one", "tokens": [1057, 558, 11, 370, 321, 1066, 884, 300, 337, 633, 8382, 15245, 295, 472], "temperature": 0.0, "avg_logprob": -0.20071712616951234, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.338197747230879e-07}, {"id": 1433, "seek": 669316, "start": 6693.16, "end": 6694.88, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.3104279139270521, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.8448174614604795e-06}, {"id": 1434, "seek": 669316, "start": 6694.88, "end": 6696.599999999999, "text": " two eventually", "tokens": [732, 4728], "temperature": 0.0, "avg_logprob": -0.3104279139270521, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.8448174614604795e-06}, {"id": 1435, "seek": 669316, "start": 6696.599999999999, "end": 6699.32, "text": " We react run out the end of the knee pop", "tokens": [492, 4515, 1190, 484, 264, 917, 295, 264, 9434, 1665], "temperature": 0.0, "avg_logprob": -0.3104279139270521, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.8448174614604795e-06}, {"id": 1436, "seek": 669316, "start": 6700.04, "end": 6703.92, "text": " Okay, and so then at the end of an epoch we would grab our", "tokens": [1033, 11, 293, 370, 550, 412, 264, 917, 295, 364, 30992, 339, 321, 576, 4444, 527], "temperature": 0.0, "avg_logprob": -0.3104279139270521, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.8448174614604795e-06}, {"id": 1437, "seek": 669316, "start": 6705.16, "end": 6707.16, "text": " intercept and slope and", "tokens": [24700, 293, 13525, 293], "temperature": 0.0, "avg_logprob": -0.3104279139270521, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.8448174614604795e-06}, {"id": 1438, "seek": 669316, "start": 6708.36, "end": 6712.28, "text": " Paste them back over here as our new values", "tokens": [43827, 552, 646, 670, 510, 382, 527, 777, 4190], "temperature": 0.0, "avg_logprob": -0.3104279139270521, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.8448174614604795e-06}, {"id": 1439, "seek": 671228, "start": 6712.28, "end": 6722.36, "text": " There we are and we can now continue again right so we're now starting with oops you guys either in the wrong spot", "tokens": [821, 321, 366, 293, 321, 393, 586, 2354, 797, 558, 370, 321, 434, 586, 2891, 365, 34166, 291, 1074, 2139, 294, 264, 2085, 4008], "temperature": 0.0, "avg_logprob": -0.28733311528744904, "compression_ratio": 1.6061946902654867, "no_speech_prob": 5.507582500285935e-06}, {"id": 1440, "seek": 671228, "start": 6722.679999999999, "end": 6724.679999999999, "text": " It should be", "tokens": [467, 820, 312], "temperature": 0.0, "avg_logprob": -0.28733311528744904, "compression_ratio": 1.6061946902654867, "no_speech_prob": 5.507582500285935e-06}, {"id": 1441, "seek": 671228, "start": 6724.759999999999, "end": 6727.44, "text": " paste special transpose values", "tokens": [9163, 2121, 25167, 4190], "temperature": 0.0, "avg_logprob": -0.28733311528744904, "compression_ratio": 1.6061946902654867, "no_speech_prob": 5.507582500285935e-06}, {"id": 1442, "seek": 671228, "start": 6728.04, "end": 6729.719999999999, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.28733311528744904, "compression_ratio": 1.6061946902654867, "no_speech_prob": 5.507582500285935e-06}, {"id": 1443, "seek": 671228, "start": 6729.719999999999, "end": 6733.92, "text": " Okay, so there's our new intercept. There's only slow possibly. I've got those the wrong way around", "tokens": [1033, 11, 370, 456, 311, 527, 777, 24700, 13, 821, 311, 787, 2964, 6264, 13, 286, 600, 658, 729, 264, 2085, 636, 926], "temperature": 0.0, "avg_logprob": -0.28733311528744904, "compression_ratio": 1.6061946902654867, "no_speech_prob": 5.507582500285935e-06}, {"id": 1444, "seek": 671228, "start": 6733.92, "end": 6740.74, "text": " But anyway you get the idea and then we continue okay, so I recorded the world's tiniest macro", "tokens": [583, 4033, 291, 483, 264, 1558, 293, 550, 321, 2354, 1392, 11, 370, 286, 8287, 264, 1002, 311, 256, 3812, 377, 18887], "temperature": 0.0, "avg_logprob": -0.28733311528744904, "compression_ratio": 1.6061946902654867, "no_speech_prob": 5.507582500285935e-06}, {"id": 1445, "seek": 674074, "start": 6740.74, "end": 6743.38, "text": " which literally just", "tokens": [597, 3736, 445], "temperature": 0.0, "avg_logprob": -0.2954051653544108, "compression_ratio": 1.8134328358208955, "no_speech_prob": 4.495147095440188e-06}, {"id": 1446, "seek": 674074, "start": 6746.099999999999, "end": 6751.66, "text": " Copies the final slope and puts it into the new slope copies the final", "tokens": [11579, 530, 264, 2572, 13525, 293, 8137, 309, 666, 264, 777, 13525, 14341, 264, 2572], "temperature": 0.0, "avg_logprob": -0.2954051653544108, "compression_ratio": 1.8134328358208955, "no_speech_prob": 4.495147095440188e-06}, {"id": 1447, "seek": 674074, "start": 6752.54, "end": 6755.34, "text": " intercept puts it into the new intercept and", "tokens": [24700, 8137, 309, 666, 264, 777, 24700, 293], "temperature": 0.0, "avg_logprob": -0.2954051653544108, "compression_ratio": 1.8134328358208955, "no_speech_prob": 4.495147095440188e-06}, {"id": 1448, "seek": 674074, "start": 6756.42, "end": 6757.74, "text": " does that", "tokens": [775, 300], "temperature": 0.0, "avg_logprob": -0.2954051653544108, "compression_ratio": 1.8134328358208955, "no_speech_prob": 4.495147095440188e-06}, {"id": 1449, "seek": 674074, "start": 6757.74, "end": 6764.0199999999995, "text": " Five times and after each time it grabs the root means greater error and pastes it into the next", "tokens": [9436, 1413, 293, 934, 1184, 565, 309, 30028, 264, 5593, 1355, 5044, 6713, 293, 1791, 279, 309, 666, 264, 958], "temperature": 0.0, "avg_logprob": -0.2954051653544108, "compression_ratio": 1.8134328358208955, "no_speech_prob": 4.495147095440188e-06}, {"id": 1450, "seek": 676402, "start": 6764.02, "end": 6770.320000000001, "text": " Spare area and that is attached to this run button and so it's going to go ahead and do that five times", "tokens": [1738, 543, 1859, 293, 300, 307, 8570, 281, 341, 1190, 2960, 293, 370, 309, 311, 516, 281, 352, 2286, 293, 360, 300, 1732, 1413], "temperature": 0.0, "avg_logprob": -0.19846148430546628, "compression_ratio": 1.5922330097087378, "no_speech_prob": 8.059432161644509e-07}, {"id": 1451, "seek": 676402, "start": 6771.22, "end": 6772.38, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.19846148430546628, "compression_ratio": 1.5922330097087378, "no_speech_prob": 8.059432161644509e-07}, {"id": 1452, "seek": 676402, "start": 6772.38, "end": 6775.3, "text": " So that's stochastic gradient descent in Excel", "tokens": [407, 300, 311, 342, 8997, 2750, 16235, 23475, 294, 19060], "temperature": 0.0, "avg_logprob": -0.19846148430546628, "compression_ratio": 1.5922330097087378, "no_speech_prob": 8.059432161644509e-07}, {"id": 1453, "seek": 676402, "start": 6776.26, "end": 6780.700000000001, "text": " So it to turn this into a CNN right you would just replace", "tokens": [407, 309, 281, 1261, 341, 666, 257, 24859, 558, 291, 576, 445, 7406], "temperature": 0.0, "avg_logprob": -0.19846148430546628, "compression_ratio": 1.5922330097087378, "no_speech_prob": 8.059432161644509e-07}, {"id": 1454, "seek": 676402, "start": 6782.1, "end": 6788.14, "text": " This error function right and therefore this prediction with the output of that", "tokens": [639, 6713, 2445, 558, 293, 4412, 341, 17630, 365, 264, 5598, 295, 300], "temperature": 0.0, "avg_logprob": -0.19846148430546628, "compression_ratio": 1.5922330097087378, "no_speech_prob": 8.059432161644509e-07}, {"id": 1455, "seek": 676402, "start": 6789.02, "end": 6791.02, "text": " convolutional example spreadsheet", "tokens": [45216, 304, 1365, 27733], "temperature": 0.0, "avg_logprob": -0.19846148430546628, "compression_ratio": 1.5922330097087378, "no_speech_prob": 8.059432161644509e-07}, {"id": 1456, "seek": 679102, "start": 6791.02, "end": 6797.34, "text": " Okay, and that then would be in a CNN being trained with with SGD", "tokens": [1033, 11, 293, 300, 550, 576, 312, 294, 257, 24859, 885, 8895, 365, 365, 34520, 35], "temperature": 0.0, "avg_logprob": -0.16840654132009922, "compression_ratio": 1.4974874371859297, "no_speech_prob": 1.467734449533964e-07}, {"id": 1457, "seek": 679102, "start": 6797.860000000001, "end": 6799.860000000001, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.16840654132009922, "compression_ratio": 1.4974874371859297, "no_speech_prob": 1.467734449533964e-07}, {"id": 1458, "seek": 679102, "start": 6799.900000000001, "end": 6802.5, "text": " Now the problem is that you'll see", "tokens": [823, 264, 1154, 307, 300, 291, 603, 536], "temperature": 0.0, "avg_logprob": -0.16840654132009922, "compression_ratio": 1.4974874371859297, "no_speech_prob": 1.467734449533964e-07}, {"id": 1459, "seek": 679102, "start": 6803.860000000001, "end": 6805.860000000001, "text": " When I run this", "tokens": [1133, 286, 1190, 341], "temperature": 0.0, "avg_logprob": -0.16840654132009922, "compression_ratio": 1.4974874371859297, "no_speech_prob": 1.467734449533964e-07}, {"id": 1460, "seek": 679102, "start": 6809.38, "end": 6815.700000000001, "text": " It's kind of going very slowly right we know that we need to get to a slope of 2 and an intercept of 30", "tokens": [467, 311, 733, 295, 516, 588, 5692, 558, 321, 458, 300, 321, 643, 281, 483, 281, 257, 13525, 295, 568, 293, 364, 24700, 295, 2217], "temperature": 0.0, "avg_logprob": -0.16840654132009922, "compression_ratio": 1.4974874371859297, "no_speech_prob": 1.467734449533964e-07}, {"id": 1461, "seek": 679102, "start": 6816.18, "end": 6818.18, "text": " And you can kind of see at this rate", "tokens": [400, 291, 393, 733, 295, 536, 412, 341, 3314], "temperature": 0.0, "avg_logprob": -0.16840654132009922, "compression_ratio": 1.4974874371859297, "no_speech_prob": 1.467734449533964e-07}, {"id": 1462, "seek": 681818, "start": 6818.18, "end": 6820.18, "text": " It's going to take a very long time", "tokens": [467, 311, 516, 281, 747, 257, 588, 938, 565], "temperature": 0.0, "avg_logprob": -0.18745743951132132, "compression_ratio": 1.8089887640449438, "no_speech_prob": 1.1726395996447536e-06}, {"id": 1463, "seek": 681818, "start": 6821.700000000001, "end": 6823.700000000001, "text": " Right and specifically", "tokens": [1779, 293, 4682], "temperature": 0.0, "avg_logprob": -0.18745743951132132, "compression_ratio": 1.8089887640449438, "no_speech_prob": 1.1726395996447536e-06}, {"id": 1464, "seek": 681818, "start": 6828.3, "end": 6830.3, "text": " It's like it keeps going the same direction", "tokens": [467, 311, 411, 309, 5965, 516, 264, 912, 3513], "temperature": 0.0, "avg_logprob": -0.18745743951132132, "compression_ratio": 1.8089887640449438, "no_speech_prob": 1.1726395996447536e-06}, {"id": 1465, "seek": 681818, "start": 6831.14, "end": 6833.9400000000005, "text": " So it's like come on take a hint. That's a good direction", "tokens": [407, 309, 311, 411, 808, 322, 747, 257, 12075, 13, 663, 311, 257, 665, 3513], "temperature": 0.0, "avg_logprob": -0.18745743951132132, "compression_ratio": 1.8089887640449438, "no_speech_prob": 1.1726395996447536e-06}, {"id": 1466, "seek": 681818, "start": 6835.14, "end": 6840.9800000000005, "text": " So the come on take a hint. That's a good direction. Please keep doing that but more is called momentum", "tokens": [407, 264, 808, 322, 747, 257, 12075, 13, 663, 311, 257, 665, 3513, 13, 2555, 1066, 884, 300, 457, 544, 307, 1219, 11244], "temperature": 0.0, "avg_logprob": -0.18745743951132132, "compression_ratio": 1.8089887640449438, "no_speech_prob": 1.1726395996447536e-06}, {"id": 1467, "seek": 681818, "start": 6841.740000000001, "end": 6843.740000000001, "text": " Right so on our next spreadsheet", "tokens": [1779, 370, 322, 527, 958, 27733], "temperature": 0.0, "avg_logprob": -0.18745743951132132, "compression_ratio": 1.8089887640449438, "no_speech_prob": 1.1726395996447536e-06}, {"id": 1468, "seek": 681818, "start": 6844.9800000000005, "end": 6846.9400000000005, "text": " We're going to implement", "tokens": [492, 434, 516, 281, 4445], "temperature": 0.0, "avg_logprob": -0.18745743951132132, "compression_ratio": 1.8089887640449438, "no_speech_prob": 1.1726395996447536e-06}, {"id": 1469, "seek": 684694, "start": 6846.94, "end": 6848.379999999999, "text": " momentum", "tokens": [11244], "temperature": 0.0, "avg_logprob": -0.31839334295037086, "compression_ratio": 1.5467980295566504, "no_speech_prob": 3.340515831951052e-06}, {"id": 1470, "seek": 684694, "start": 6848.379999999999, "end": 6850.379999999999, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.31839334295037086, "compression_ratio": 1.5467980295566504, "no_speech_prob": 3.340515831951052e-06}, {"id": 1471, "seek": 684694, "start": 6850.86, "end": 6852.86, "text": " What momentum does is?", "tokens": [708, 11244, 775, 307, 30], "temperature": 0.0, "avg_logprob": -0.31839334295037086, "compression_ratio": 1.5467980295566504, "no_speech_prob": 3.340515831951052e-06}, {"id": 1472, "seek": 684694, "start": 6853.299999999999, "end": 6859.12, "text": " The same thing and what to simplify this spreadsheet I've removed the finite differencing columns", "tokens": [440, 912, 551, 293, 437, 281, 20460, 341, 27733, 286, 600, 7261, 264, 19362, 743, 13644, 13766], "temperature": 0.0, "avg_logprob": -0.31839334295037086, "compression_ratio": 1.5467980295566504, "no_speech_prob": 3.340515831951052e-06}, {"id": 1473, "seek": 684694, "start": 6859.339999999999, "end": 6863.639999999999, "text": " Okay, other than that this is just the same right so we still got our X's our Y's", "tokens": [1033, 11, 661, 813, 300, 341, 307, 445, 264, 912, 558, 370, 321, 920, 658, 527, 1783, 311, 527, 398, 311], "temperature": 0.0, "avg_logprob": -0.31839334295037086, "compression_ratio": 1.5467980295566504, "no_speech_prob": 3.340515831951052e-06}, {"id": 1474, "seek": 684694, "start": 6864.66, "end": 6867.339999999999, "text": " A's and B's our predictions", "tokens": [316, 311, 293, 363, 311, 527, 21264], "temperature": 0.0, "avg_logprob": -0.31839334295037086, "compression_ratio": 1.5467980295566504, "no_speech_prob": 3.340515831951052e-06}, {"id": 1475, "seek": 684694, "start": 6868.099999999999, "end": 6870.099999999999, "text": " Our error is now over here", "tokens": [2621, 6713, 307, 586, 670, 510], "temperature": 0.0, "avg_logprob": -0.31839334295037086, "compression_ratio": 1.5467980295566504, "no_speech_prob": 3.340515831951052e-06}, {"id": 1476, "seek": 684694, "start": 6870.099999999999, "end": 6871.379999999999, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.31839334295037086, "compression_ratio": 1.5467980295566504, "no_speech_prob": 3.340515831951052e-06}, {"id": 1477, "seek": 684694, "start": 6871.379999999999, "end": 6874.48, "text": " And here's our derivatives, okay?", "tokens": [400, 510, 311, 527, 33733, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.31839334295037086, "compression_ratio": 1.5467980295566504, "no_speech_prob": 3.340515831951052e-06}, {"id": 1478, "seek": 687448, "start": 6874.48, "end": 6880.919999999999, "text": " our new calculation for let's grab a particular row", "tokens": [527, 777, 17108, 337, 718, 311, 4444, 257, 1729, 5386], "temperature": 0.0, "avg_logprob": -0.2261114581938713, "compression_ratio": 1.5612903225806452, "no_speech_prob": 9.276345167563704e-07}, {"id": 1479, "seek": 687448, "start": 6884.28, "end": 6892.04, "text": " Our new calculation here for our new a term just like before is it's equal to whatever a was before", "tokens": [2621, 777, 17108, 510, 337, 527, 777, 257, 1433, 445, 411, 949, 307, 309, 311, 2681, 281, 2035, 257, 390, 949], "temperature": 0.0, "avg_logprob": -0.2261114581938713, "compression_ratio": 1.5612903225806452, "no_speech_prob": 9.276345167563704e-07}, {"id": 1480, "seek": 687448, "start": 6894.879999999999, "end": 6896.879999999999, "text": " Minus", "tokens": [2829, 301], "temperature": 0.0, "avg_logprob": -0.2261114581938713, "compression_ratio": 1.5612903225806452, "no_speech_prob": 9.276345167563704e-07}, {"id": 1481, "seek": 687448, "start": 6897.12, "end": 6901.36, "text": " Okay now this time I'm not taking the derivative, but I'm minusing some other number", "tokens": [1033, 586, 341, 565, 286, 478, 406, 1940, 264, 13760, 11, 457, 286, 478, 3175, 278, 512, 661, 1230], "temperature": 0.0, "avg_logprob": -0.2261114581938713, "compression_ratio": 1.5612903225806452, "no_speech_prob": 9.276345167563704e-07}, {"id": 1482, "seek": 690136, "start": 6901.36, "end": 6904.259999999999, "text": " Times the line rate, so what's this other number?", "tokens": [11366, 264, 1622, 3314, 11, 370, 437, 311, 341, 661, 1230, 30], "temperature": 0.0, "avg_logprob": -0.38346881336636013, "compression_ratio": 1.4859154929577465, "no_speech_prob": 1.6631608446004975e-07}, {"id": 1483, "seek": 690136, "start": 6906.759999999999, "end": 6911.5599999999995, "text": " Okay, so this other number is equal to the derivative", "tokens": [1033, 11, 370, 341, 661, 1230, 307, 2681, 281, 264, 13760], "temperature": 0.0, "avg_logprob": -0.38346881336636013, "compression_ratio": 1.4859154929577465, "no_speech_prob": 1.6631608446004975e-07}, {"id": 1484, "seek": 690136, "start": 6913.5199999999995, "end": 6915.2, "text": " Times", "tokens": [11366], "temperature": 0.0, "avg_logprob": -0.38346881336636013, "compression_ratio": 1.4859154929577465, "no_speech_prob": 1.6631608446004975e-07}, {"id": 1485, "seek": 690136, "start": 6915.2, "end": 6917.2, "text": " What's this K 1?", "tokens": [708, 311, 341, 591, 502, 30], "temperature": 0.0, "avg_logprob": -0.38346881336636013, "compression_ratio": 1.4859154929577465, "no_speech_prob": 1.6631608446004975e-07}, {"id": 1486, "seek": 690136, "start": 6918.0, "end": 6920.0, "text": " 0.02", "tokens": [1958, 13, 12756], "temperature": 0.0, "avg_logprob": -0.38346881336636013, "compression_ratio": 1.4859154929577465, "no_speech_prob": 1.6631608446004975e-07}, {"id": 1487, "seek": 690136, "start": 6920.0, "end": 6921.719999999999, "text": " plus", "tokens": [1804], "temperature": 0.0, "avg_logprob": -0.38346881336636013, "compression_ratio": 1.4859154929577465, "no_speech_prob": 1.6631608446004975e-07}, {"id": 1488, "seek": 690136, "start": 6921.719999999999, "end": 6923.44, "text": " 0.98 times", "tokens": [1958, 13, 22516, 1413], "temperature": 0.0, "avg_logprob": -0.38346881336636013, "compression_ratio": 1.4859154929577465, "no_speech_prob": 1.6631608446004975e-07}, {"id": 1489, "seek": 690136, "start": 6923.44, "end": 6925.44, "text": " The thing just above it", "tokens": [440, 551, 445, 3673, 309], "temperature": 0.0, "avg_logprob": -0.38346881336636013, "compression_ratio": 1.4859154929577465, "no_speech_prob": 1.6631608446004975e-07}, {"id": 1490, "seek": 690136, "start": 6925.679999999999, "end": 6928.4, "text": " Okay, so this is a linear interpolation", "tokens": [1033, 11, 370, 341, 307, 257, 8213, 44902, 399], "temperature": 0.0, "avg_logprob": -0.38346881336636013, "compression_ratio": 1.4859154929577465, "no_speech_prob": 1.6631608446004975e-07}, {"id": 1491, "seek": 692840, "start": 6928.4, "end": 6930.32, "text": " between", "tokens": [1296], "temperature": 0.0, "avg_logprob": -0.26074539290534127, "compression_ratio": 1.568421052631579, "no_speech_prob": 1.6028087657105061e-06}, {"id": 1492, "seek": 692840, "start": 6930.32, "end": 6933.5599999999995, "text": " this rows derivative or this mini batches derivative and", "tokens": [341, 13241, 13760, 420, 341, 8382, 15245, 279, 13760, 293], "temperature": 0.0, "avg_logprob": -0.26074539290534127, "compression_ratio": 1.568421052631579, "no_speech_prob": 1.6028087657105061e-06}, {"id": 1493, "seek": 692840, "start": 6934.639999999999, "end": 6936.639999999999, "text": " Whatever direction we went last time", "tokens": [8541, 3513, 321, 1437, 1036, 565], "temperature": 0.0, "avg_logprob": -0.26074539290534127, "compression_ratio": 1.568421052631579, "no_speech_prob": 1.6028087657105061e-06}, {"id": 1494, "seek": 692840, "start": 6937.719999999999, "end": 6942.12, "text": " All right, so in other words keep going the same direction as you were before", "tokens": [1057, 558, 11, 370, 294, 661, 2283, 1066, 516, 264, 912, 3513, 382, 291, 645, 949], "temperature": 0.0, "avg_logprob": -0.26074539290534127, "compression_ratio": 1.568421052631579, "no_speech_prob": 1.6028087657105061e-06}, {"id": 1495, "seek": 692840, "start": 6942.839999999999, "end": 6947.2, "text": " Right then update it a little bit right and so in our", "tokens": [1779, 550, 5623, 309, 257, 707, 857, 558, 293, 370, 294, 527], "temperature": 0.0, "avg_logprob": -0.26074539290534127, "compression_ratio": 1.568421052631579, "no_speech_prob": 1.6028087657105061e-06}, {"id": 1496, "seek": 692840, "start": 6948.0, "end": 6952.12, "text": " Spread in our Python just before we had a momentum of point nine", "tokens": [30308, 294, 527, 15329, 445, 949, 321, 632, 257, 11244, 295, 935, 4949], "temperature": 0.0, "avg_logprob": -0.26074539290534127, "compression_ratio": 1.568421052631579, "no_speech_prob": 1.6028087657105061e-06}, {"id": 1497, "seek": 695212, "start": 6952.12, "end": 6957.84, "text": " Okay, so you can see what tends to happen is that our", "tokens": [1033, 11, 370, 291, 393, 536, 437, 12258, 281, 1051, 307, 300, 527], "temperature": 0.0, "avg_logprob": -0.2679859988660698, "compression_ratio": 1.5462555066079295, "no_speech_prob": 2.058043492070283e-06}, {"id": 1498, "seek": 695212, "start": 6959.16, "end": 6964.5, "text": " Negative kind of gets more and more negative right all the way up to like 2000", "tokens": [43230, 733, 295, 2170, 544, 293, 544, 3671, 558, 439, 264, 636, 493, 281, 411, 8132], "temperature": 0.0, "avg_logprob": -0.2679859988660698, "compression_ratio": 1.5462555066079295, "no_speech_prob": 2.058043492070283e-06}, {"id": 1499, "seek": 695212, "start": 6967.16, "end": 6969.54, "text": " Whereas with our standard SGD approach", "tokens": [13813, 365, 527, 3832, 34520, 35, 3109], "temperature": 0.0, "avg_logprob": -0.2679859988660698, "compression_ratio": 1.5462555066079295, "no_speech_prob": 2.058043492070283e-06}, {"id": 1500, "seek": 695212, "start": 6970.88, "end": 6973.0199999999995, "text": " Our derivatives are kind of all over the place", "tokens": [2621, 33733, 366, 733, 295, 439, 670, 264, 1081], "temperature": 0.0, "avg_logprob": -0.2679859988660698, "compression_ratio": 1.5462555066079295, "no_speech_prob": 2.058043492070283e-06}, {"id": 1501, "seek": 695212, "start": 6973.68, "end": 6977.68, "text": " Right sometimes there's 700 sometimes negative 700 positive 100", "tokens": [1779, 2171, 456, 311, 15204, 2171, 3671, 15204, 3353, 2319], "temperature": 0.0, "avg_logprob": -0.2679859988660698, "compression_ratio": 1.5462555066079295, "no_speech_prob": 2.058043492070283e-06}, {"id": 1502, "seek": 697768, "start": 6977.68, "end": 6982.400000000001, "text": " You know so this is basically saying like yeah, if you've been going down", "tokens": [509, 458, 370, 341, 307, 1936, 1566, 411, 1338, 11, 498, 291, 600, 668, 516, 760], "temperature": 0.0, "avg_logprob": -0.16315388288654264, "compression_ratio": 1.7849829351535835, "no_speech_prob": 3.1381352982862154e-06}, {"id": 1503, "seek": 697768, "start": 6982.84, "end": 6986.84, "text": " For quite a while keep doing that until finally here. It's like okay", "tokens": [1171, 1596, 257, 1339, 1066, 884, 300, 1826, 2721, 510, 13, 467, 311, 411, 1392], "temperature": 0.0, "avg_logprob": -0.16315388288654264, "compression_ratio": 1.7849829351535835, "no_speech_prob": 3.1381352982862154e-06}, {"id": 1504, "seek": 697768, "start": 6986.84, "end": 6990.320000000001, "text": " That's that seems to be far enough so that's getting less and less and less negative", "tokens": [663, 311, 300, 2544, 281, 312, 1400, 1547, 370, 300, 311, 1242, 1570, 293, 1570, 293, 1570, 3671], "temperature": 0.0, "avg_logprob": -0.16315388288654264, "compression_ratio": 1.7849829351535835, "no_speech_prob": 3.1381352982862154e-06}, {"id": 1505, "seek": 697768, "start": 6990.76, "end": 6995.200000000001, "text": " Right and still we start going positive again, so you can kind of see why it's caught momentum", "tokens": [1779, 293, 920, 321, 722, 516, 3353, 797, 11, 370, 291, 393, 733, 295, 536, 983, 309, 311, 5415, 11244], "temperature": 0.0, "avg_logprob": -0.16315388288654264, "compression_ratio": 1.7849829351535835, "no_speech_prob": 3.1381352982862154e-06}, {"id": 1506, "seek": 697768, "start": 6995.200000000001, "end": 6999.12, "text": " It's like once you start traveling in a particular direction for a particular weight", "tokens": [467, 311, 411, 1564, 291, 722, 9712, 294, 257, 1729, 3513, 337, 257, 1729, 3364], "temperature": 0.0, "avg_logprob": -0.16315388288654264, "compression_ratio": 1.7849829351535835, "no_speech_prob": 3.1381352982862154e-06}, {"id": 1507, "seek": 697768, "start": 6999.72, "end": 7006.52, "text": " You kind of the wheel starts spinning and then once the gradient turns around the other way. It's like oh slow down", "tokens": [509, 733, 295, 264, 5589, 3719, 15640, 293, 550, 1564, 264, 16235, 4523, 926, 264, 661, 636, 13, 467, 311, 411, 1954, 2964, 760], "temperature": 0.0, "avg_logprob": -0.16315388288654264, "compression_ratio": 1.7849829351535835, "no_speech_prob": 3.1381352982862154e-06}, {"id": 1508, "seek": 700652, "start": 7006.52, "end": 7012.56, "text": " We've got this kind of a momentum and then finally turn back around right so when we do it this way", "tokens": [492, 600, 658, 341, 733, 295, 257, 11244, 293, 550, 2721, 1261, 646, 926, 558, 370, 562, 321, 360, 309, 341, 636], "temperature": 0.0, "avg_logprob": -0.26269807313617904, "compression_ratio": 1.5567567567567568, "no_speech_prob": 1.4823546052866732e-06}, {"id": 1509, "seek": 700652, "start": 7016.4800000000005, "end": 7022.0, "text": " All right, we can do exactly the same thing right and after five iterations. We're at 89", "tokens": [1057, 558, 11, 321, 393, 360, 2293, 264, 912, 551, 558, 293, 934, 1732, 36540, 13, 492, 434, 412, 31877], "temperature": 0.0, "avg_logprob": -0.26269807313617904, "compression_ratio": 1.5567567567567568, "no_speech_prob": 1.4823546052866732e-06}, {"id": 1510, "seek": 700652, "start": 7023.68, "end": 7025.56, "text": " Where else before?", "tokens": [2305, 1646, 949, 30], "temperature": 0.0, "avg_logprob": -0.26269807313617904, "compression_ratio": 1.5567567567567568, "no_speech_prob": 1.4823546052866732e-06}, {"id": 1511, "seek": 700652, "start": 7025.56, "end": 7029.160000000001, "text": " after five iterations, we're at 104 right and", "tokens": [934, 1732, 36540, 11, 321, 434, 412, 47757, 558, 293], "temperature": 0.0, "avg_logprob": -0.26269807313617904, "compression_ratio": 1.5567567567567568, "no_speech_prob": 1.4823546052866732e-06}, {"id": 1512, "seek": 702916, "start": 7029.16, "end": 7036.599999999999, "text": " After a few more let's do maybe 15 okay?", "tokens": [2381, 257, 1326, 544, 718, 311, 360, 1310, 2119, 1392, 30], "temperature": 0.0, "avg_logprob": -0.35496835708618163, "compression_ratio": 1.3356643356643356, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1513, "seek": 702916, "start": 7037.4, "end": 7040.599999999999, "text": " Seconds it's 102 for us here", "tokens": [5736, 82, 309, 311, 45937, 337, 505, 510], "temperature": 0.0, "avg_logprob": -0.35496835708618163, "compression_ratio": 1.3356643356643356, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1514, "seek": 702916, "start": 7046.639999999999, "end": 7054.5599999999995, "text": " It's going right, so it's it's it's a bit better. It's not hips better. You can still see like these numbers. They're not", "tokens": [467, 311, 516, 558, 11, 370, 309, 311, 309, 311, 309, 311, 257, 857, 1101, 13, 467, 311, 406, 15233, 1101, 13, 509, 393, 920, 536, 411, 613, 3547, 13, 814, 434, 406], "temperature": 0.0, "avg_logprob": -0.35496835708618163, "compression_ratio": 1.3356643356643356, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1515, "seek": 705456, "start": 7054.56, "end": 7060.96, "text": " Zipping along right, but it's definitely an improvement, and it also gives us something else to tune", "tokens": [1176, 6297, 2051, 558, 11, 457, 309, 311, 2138, 364, 10444, 11, 293, 309, 611, 2709, 505, 746, 1646, 281, 10864], "temperature": 0.0, "avg_logprob": -0.1818168104187516, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.7330480659438763e-06}, {"id": 1516, "seek": 705456, "start": 7060.96, "end": 7066.52, "text": " Which is nice like so if this is kind of a well-behaved error surface right in other words like", "tokens": [3013, 307, 1481, 411, 370, 498, 341, 307, 733, 295, 257, 731, 12, 29437, 12865, 6713, 3753, 558, 294, 661, 2283, 411], "temperature": 0.0, "avg_logprob": -0.1818168104187516, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.7330480659438763e-06}, {"id": 1517, "seek": 705456, "start": 7067.68, "end": 7071.4400000000005, "text": " Although it might be bumpy along the way there's kind of some overall", "tokens": [5780, 309, 1062, 312, 49400, 2051, 264, 636, 456, 311, 733, 295, 512, 4787], "temperature": 0.0, "avg_logprob": -0.1818168104187516, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.7330480659438763e-06}, {"id": 1518, "seek": 705456, "start": 7071.96, "end": 7078.320000000001, "text": " Direction like imagine you're going down a hill right and there's like bumps on it right so the moment momentum you get up", "tokens": [5822, 882, 411, 3811, 291, 434, 516, 760, 257, 10997, 558, 293, 456, 311, 411, 27719, 322, 309, 558, 370, 264, 1623, 11244, 291, 483, 493], "temperature": 0.0, "avg_logprob": -0.1818168104187516, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.7330480659438763e-06}, {"id": 1519, "seek": 705456, "start": 7078.320000000001, "end": 7083.04, "text": " We're going to skipping over the tops right so we could say like okay. Let's increase our beta up to 0.98", "tokens": [492, 434, 516, 281, 31533, 670, 264, 22836, 558, 370, 321, 727, 584, 411, 1392, 13, 961, 311, 3488, 527, 9861, 493, 281, 1958, 13, 22516], "temperature": 0.0, "avg_logprob": -0.1818168104187516, "compression_ratio": 1.6952054794520548, "no_speech_prob": 1.7330480659438763e-06}, {"id": 1520, "seek": 708304, "start": 7083.04, "end": 7089.64, "text": " Right and see if that like allows us to train a little faster and well look at that suddenly went straight to 82", "tokens": [1779, 293, 536, 498, 300, 411, 4045, 505, 281, 3847, 257, 707, 4663, 293, 731, 574, 412, 300, 5800, 1437, 2997, 281, 29097], "temperature": 0.0, "avg_logprob": -0.25905600361440373, "compression_ratio": 1.668122270742358, "no_speech_prob": 2.2959075067774393e-06}, {"id": 1521, "seek": 708304, "start": 7089.76, "end": 7096.76, "text": " Right so one nice thing about things like momentum is it's like another parameter that you can chew to try and make your model", "tokens": [1779, 370, 472, 1481, 551, 466, 721, 411, 11244, 307, 309, 311, 411, 1071, 13075, 300, 291, 393, 21200, 281, 853, 293, 652, 428, 2316], "temperature": 0.0, "avg_logprob": -0.25905600361440373, "compression_ratio": 1.668122270742358, "no_speech_prob": 2.2959075067774393e-06}, {"id": 1522, "seek": 708304, "start": 7096.76, "end": 7098.86, "text": " Train better in practice", "tokens": [28029, 1101, 294, 3124], "temperature": 0.0, "avg_logprob": -0.25905600361440373, "compression_ratio": 1.668122270742358, "no_speech_prob": 2.2959075067774393e-06}, {"id": 1523, "seek": 708304, "start": 7101.16, "end": 7107.4, "text": " Basically everybody does this every like you look at any like image net winner or whatever they all use", "tokens": [8537, 2201, 775, 341, 633, 411, 291, 574, 412, 604, 411, 3256, 2533, 8507, 420, 2035, 436, 439, 764], "temperature": 0.0, "avg_logprob": -0.25905600361440373, "compression_ratio": 1.668122270742358, "no_speech_prob": 2.2959075067774393e-06}, {"id": 1524, "seek": 708304, "start": 7108.44, "end": 7110.04, "text": " momentum", "tokens": [11244], "temperature": 0.0, "avg_logprob": -0.25905600361440373, "compression_ratio": 1.668122270742358, "no_speech_prob": 2.2959075067774393e-06}, {"id": 1525, "seek": 708304, "start": 7110.04, "end": 7112.04, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.25905600361440373, "compression_ratio": 1.668122270742358, "no_speech_prob": 2.2959075067774393e-06}, {"id": 1526, "seek": 711204, "start": 7112.04, "end": 7114.04, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.26376424516950336, "compression_ratio": 1.4795321637426901, "no_speech_prob": 1.6028062646000762e-06}, {"id": 1527, "seek": 711204, "start": 7114.24, "end": 7116.24, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.26376424516950336, "compression_ratio": 1.4795321637426901, "no_speech_prob": 1.6028062646000762e-06}, {"id": 1528, "seek": 711204, "start": 7117.44, "end": 7125.56, "text": " Back over here when we said use SGD that basically means use the the basic tab of our excel spreadsheet", "tokens": [5833, 670, 510, 562, 321, 848, 764, 34520, 35, 300, 1936, 1355, 764, 264, 264, 3875, 4421, 295, 527, 24015, 27733], "temperature": 0.0, "avg_logprob": -0.26376424516950336, "compression_ratio": 1.4795321637426901, "no_speech_prob": 1.6028062646000762e-06}, {"id": 1529, "seek": 711204, "start": 7125.8, "end": 7128.12, "text": " But then momentum equals point nine", "tokens": [583, 550, 11244, 6915, 935, 4949], "temperature": 0.0, "avg_logprob": -0.26376424516950336, "compression_ratio": 1.4795321637426901, "no_speech_prob": 1.6028062646000762e-06}, {"id": 1530, "seek": 711204, "start": 7128.84, "end": 7130.84, "text": " means add in", "tokens": [1355, 909, 294], "temperature": 0.0, "avg_logprob": -0.26376424516950336, "compression_ratio": 1.4795321637426901, "no_speech_prob": 1.6028062646000762e-06}, {"id": 1531, "seek": 711204, "start": 7131.76, "end": 7134.32, "text": " Put a point nine over here, okay?", "tokens": [4935, 257, 935, 4949, 670, 510, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.26376424516950336, "compression_ratio": 1.4795321637426901, "no_speech_prob": 1.6028062646000762e-06}, {"id": 1532, "seek": 711204, "start": 7135.16, "end": 7139.0, "text": " And so that that's kind of your like", "tokens": [400, 370, 300, 300, 311, 733, 295, 428, 411], "temperature": 0.0, "avg_logprob": -0.26376424516950336, "compression_ratio": 1.4795321637426901, "no_speech_prob": 1.6028062646000762e-06}, {"id": 1533, "seek": 713900, "start": 7139.0, "end": 7141.0, "text": " default starting point", "tokens": [7576, 2891, 935], "temperature": 0.0, "avg_logprob": -0.3078535111224065, "compression_ratio": 1.4873417721518987, "no_speech_prob": 4.289307071303483e-06}, {"id": 1534, "seek": 713900, "start": 7143.88, "end": 7146.6, "text": " So let's keep going and talk about", "tokens": [407, 718, 311, 1066, 516, 293, 751, 466], "temperature": 0.0, "avg_logprob": -0.3078535111224065, "compression_ratio": 1.4873417721518987, "no_speech_prob": 4.289307071303483e-06}, {"id": 1535, "seek": 713900, "start": 7148.44, "end": 7150.44, "text": " Adam", "tokens": [7938], "temperature": 0.0, "avg_logprob": -0.3078535111224065, "compression_ratio": 1.4873417721518987, "no_speech_prob": 4.289307071303483e-06}, {"id": 1536, "seek": 713900, "start": 7151.6, "end": 7155.92, "text": " So Adam is something which I", "tokens": [407, 7938, 307, 746, 597, 286], "temperature": 0.0, "avg_logprob": -0.3078535111224065, "compression_ratio": 1.4873417721518987, "no_speech_prob": 4.289307071303483e-06}, {"id": 1537, "seek": 713900, "start": 7157.36, "end": 7164.88, "text": " Actually was not right earlier on in this course. I said we've been using Adam by default. We actually haven't we've actually been I've noticed", "tokens": [5135, 390, 406, 558, 3071, 322, 294, 341, 1164, 13, 286, 848, 321, 600, 668, 1228, 7938, 538, 7576, 13, 492, 767, 2378, 380, 321, 600, 767, 668, 286, 600, 5694], "temperature": 0.0, "avg_logprob": -0.3078535111224065, "compression_ratio": 1.4873417721518987, "no_speech_prob": 4.289307071303483e-06}, {"id": 1538, "seek": 716488, "start": 7164.88, "end": 7170.2, "text": " We've actually been using SGD with momentum by default and the reason is that", "tokens": [492, 600, 767, 668, 1228, 34520, 35, 365, 11244, 538, 7576, 293, 264, 1778, 307, 300], "temperature": 0.0, "avg_logprob": -0.15088838797349197, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.482466470610234e-06}, {"id": 1539, "seek": 716488, "start": 7171.400000000001, "end": 7173.400000000001, "text": " Adam", "tokens": [7938], "temperature": 0.0, "avg_logprob": -0.15088838797349197, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.482466470610234e-06}, {"id": 1540, "seek": 716488, "start": 7173.56, "end": 7179.12, "text": " Has had it's much faster as you'll see it's much much faster to learn with but there's been some problems", "tokens": [8646, 632, 309, 311, 709, 4663, 382, 291, 603, 536, 309, 311, 709, 709, 4663, 281, 1466, 365, 457, 456, 311, 668, 512, 2740], "temperature": 0.0, "avg_logprob": -0.15088838797349197, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.482466470610234e-06}, {"id": 1541, "seek": 716488, "start": 7179.12, "end": 7185.72, "text": " Which is people haven't been getting quite as good like final answers with Adam as they have with SGD with momentum", "tokens": [3013, 307, 561, 2378, 380, 668, 1242, 1596, 382, 665, 411, 2572, 6338, 365, 7938, 382, 436, 362, 365, 34520, 35, 365, 11244], "temperature": 0.0, "avg_logprob": -0.15088838797349197, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.482466470610234e-06}, {"id": 1542, "seek": 716488, "start": 7185.72, "end": 7188.72, "text": " And that's why you'll see like all the you know image net winning", "tokens": [400, 300, 311, 983, 291, 603, 536, 411, 439, 264, 291, 458, 3256, 2533, 8224], "temperature": 0.0, "avg_logprob": -0.15088838797349197, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.482466470610234e-06}, {"id": 1543, "seek": 716488, "start": 7189.32, "end": 7193.12, "text": " solutions and so forth and all the academic papers always use", "tokens": [6547, 293, 370, 5220, 293, 439, 264, 7778, 10577, 1009, 764], "temperature": 0.0, "avg_logprob": -0.15088838797349197, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.482466470610234e-06}, {"id": 1544, "seek": 719312, "start": 7193.12, "end": 7200.5, "text": " SGD with momentum and Adam seems to be a particular problem in NLP people really haven't got Adam working at all well", "tokens": [34520, 35, 365, 11244, 293, 7938, 2544, 281, 312, 257, 1729, 1154, 294, 426, 45196, 561, 534, 2378, 380, 658, 7938, 1364, 412, 439, 731], "temperature": 0.0, "avg_logprob": -0.17050938606262206, "compression_ratio": 1.555045871559633, "no_speech_prob": 1.172637439594837e-06}, {"id": 1545, "seek": 719312, "start": 7202.68, "end": 7208.76, "text": " The good news is this was I built it looks like this was solved two weeks ago", "tokens": [440, 665, 2583, 307, 341, 390, 286, 3094, 309, 1542, 411, 341, 390, 13041, 732, 3259, 2057], "temperature": 0.0, "avg_logprob": -0.17050938606262206, "compression_ratio": 1.555045871559633, "no_speech_prob": 1.172637439594837e-06}, {"id": 1546, "seek": 719312, "start": 7209.88, "end": 7211.88, "text": " It basically it turned out", "tokens": [467, 1936, 309, 3574, 484], "temperature": 0.0, "avg_logprob": -0.17050938606262206, "compression_ratio": 1.555045871559633, "no_speech_prob": 1.172637439594837e-06}, {"id": 1547, "seek": 719312, "start": 7212.28, "end": 7219.5599999999995, "text": " That the way people were dealing with a combination of weight decay and Adam had a nasty kind of bug in it basically", "tokens": [663, 264, 636, 561, 645, 6260, 365, 257, 6562, 295, 3364, 21039, 293, 7938, 632, 257, 17923, 733, 295, 7426, 294, 309, 1936], "temperature": 0.0, "avg_logprob": -0.17050938606262206, "compression_ratio": 1.555045871559633, "no_speech_prob": 1.172637439594837e-06}, {"id": 1548, "seek": 721956, "start": 7219.56, "end": 7223.240000000001, "text": " and that's that's kind of carried through to every single library and", "tokens": [293, 300, 311, 300, 311, 733, 295, 9094, 807, 281, 633, 2167, 6405, 293], "temperature": 0.0, "avg_logprob": -0.2092867981303822, "compression_ratio": 1.6475770925110131, "no_speech_prob": 1.5779517070768634e-06}, {"id": 1549, "seek": 721956, "start": 7224.04, "end": 7227.6, "text": " one of our students and then Sahara has actually just", "tokens": [472, 295, 527, 1731, 293, 550, 18280, 2419, 575, 767, 445], "temperature": 0.0, "avg_logprob": -0.2092867981303822, "compression_ratio": 1.6475770925110131, "no_speech_prob": 1.5779517070768634e-06}, {"id": 1550, "seek": 721956, "start": 7228.52, "end": 7234.4400000000005, "text": " Completed a prototype of adding is this new version of Adam is called Adam W into fast AI", "tokens": [33736, 10993, 257, 19475, 295, 5127, 307, 341, 777, 3037, 295, 7938, 307, 1219, 7938, 343, 666, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.2092867981303822, "compression_ratio": 1.6475770925110131, "no_speech_prob": 1.5779517070768634e-06}, {"id": 1551, "seek": 721956, "start": 7234.4400000000005, "end": 7239.8, "text": " And he's confirmed that he's getting the much faster both the faster", "tokens": [400, 415, 311, 11341, 300, 415, 311, 1242, 264, 709, 4663, 1293, 264, 4663], "temperature": 0.0, "avg_logprob": -0.2092867981303822, "compression_ratio": 1.6475770925110131, "no_speech_prob": 1.5779517070768634e-06}, {"id": 1552, "seek": 721956, "start": 7240.96, "end": 7247.8, "text": " Performance and also the the better accuracy so hopefully we'll have this Adam W in fast AI", "tokens": [25047, 293, 611, 264, 264, 1101, 14170, 370, 4696, 321, 603, 362, 341, 7938, 343, 294, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.2092867981303822, "compression_ratio": 1.6475770925110131, "no_speech_prob": 1.5779517070768634e-06}, {"id": 1553, "seek": 724780, "start": 7247.8, "end": 7253.400000000001, "text": " Ideally before next week. We'll see how we go very very soon so so it is worth telling you about", "tokens": [40817, 949, 958, 1243, 13, 492, 603, 536, 577, 321, 352, 588, 588, 2321, 370, 370, 309, 307, 3163, 3585, 291, 466], "temperature": 0.0, "avg_logprob": -0.1977325347532709, "compression_ratio": 1.580188679245283, "no_speech_prob": 2.3320540094573516e-06}, {"id": 1554, "seek": 724780, "start": 7254.16, "end": 7256.16, "text": " about Adam", "tokens": [466, 7938], "temperature": 0.0, "avg_logprob": -0.1977325347532709, "compression_ratio": 1.580188679245283, "no_speech_prob": 2.3320540094573516e-06}, {"id": 1555, "seek": 724780, "start": 7256.88, "end": 7260.76, "text": " So let's talk about it. It's actually incredibly simple", "tokens": [407, 718, 311, 751, 466, 309, 13, 467, 311, 767, 6252, 2199], "temperature": 0.0, "avg_logprob": -0.1977325347532709, "compression_ratio": 1.580188679245283, "no_speech_prob": 2.3320540094573516e-06}, {"id": 1556, "seek": 724780, "start": 7261.92, "end": 7267.12, "text": " But again, you know make sure you make it sound really complicated when you tell people so that you can look like", "tokens": [583, 797, 11, 291, 458, 652, 988, 291, 652, 309, 1626, 534, 6179, 562, 291, 980, 561, 370, 300, 291, 393, 574, 411], "temperature": 0.0, "avg_logprob": -0.1977325347532709, "compression_ratio": 1.580188679245283, "no_speech_prob": 2.3320540094573516e-06}, {"id": 1557, "seek": 724780, "start": 7268.400000000001, "end": 7272.6, "text": " So here's the same spreadsheet again right and here's our", "tokens": [407, 510, 311, 264, 912, 27733, 797, 558, 293, 510, 311, 527], "temperature": 0.0, "avg_logprob": -0.1977325347532709, "compression_ratio": 1.580188679245283, "no_speech_prob": 2.3320540094573516e-06}, {"id": 1558, "seek": 727260, "start": 7272.6, "end": 7279.52, "text": " Randomly selected a and B again somehow it's still one is a prediction is our derivatives", "tokens": [37603, 356, 8209, 257, 293, 363, 797, 6063, 309, 311, 920, 472, 307, 257, 17630, 307, 527, 33733], "temperature": 0.0, "avg_logprob": -0.2537847277761876, "compression_ratio": 1.6267281105990783, "no_speech_prob": 1.7603319975023624e-06}, {"id": 1559, "seek": 727260, "start": 7280.04, "end": 7283.8, "text": " Okay, so now how we calculating our new a and our new B", "tokens": [1033, 11, 370, 586, 577, 321, 28258, 527, 777, 257, 293, 527, 777, 363], "temperature": 0.0, "avg_logprob": -0.2537847277761876, "compression_ratio": 1.6267281105990783, "no_speech_prob": 1.7603319975023624e-06}, {"id": 1560, "seek": 727260, "start": 7284.4400000000005, "end": 7290.4800000000005, "text": " You can immediately see it's looking pretty hopeful because even by like row 10", "tokens": [509, 393, 4258, 536, 309, 311, 1237, 1238, 20531, 570, 754, 538, 411, 5386, 1266], "temperature": 0.0, "avg_logprob": -0.2537847277761876, "compression_ratio": 1.6267281105990783, "no_speech_prob": 1.7603319975023624e-06}, {"id": 1561, "seek": 727260, "start": 7291.04, "end": 7294.76, "text": " We're like we're seeing the numbers move a lot more right so this is looking", "tokens": [492, 434, 411, 321, 434, 2577, 264, 3547, 1286, 257, 688, 544, 558, 370, 341, 307, 1237], "temperature": 0.0, "avg_logprob": -0.2537847277761876, "compression_ratio": 1.6267281105990783, "no_speech_prob": 1.7603319975023624e-06}, {"id": 1562, "seek": 727260, "start": 7295.4800000000005, "end": 7297.4800000000005, "text": " pretty encouraging", "tokens": [1238, 14580], "temperature": 0.0, "avg_logprob": -0.2537847277761876, "compression_ratio": 1.6267281105990783, "no_speech_prob": 1.7603319975023624e-06}, {"id": 1563, "seek": 727260, "start": 7298.320000000001, "end": 7300.320000000001, "text": " So how are we calculating this?", "tokens": [407, 577, 366, 321, 28258, 341, 30], "temperature": 0.0, "avg_logprob": -0.2537847277761876, "compression_ratio": 1.6267281105990783, "no_speech_prob": 1.7603319975023624e-06}, {"id": 1564, "seek": 730032, "start": 7300.32, "end": 7304.24, "text": " It's equal to our previous value of B", "tokens": [467, 311, 2681, 281, 527, 3894, 2158, 295, 363], "temperature": 0.0, "avg_logprob": -0.21675079345703124, "compression_ratio": 1.69, "no_speech_prob": 2.3320646960200975e-06}, {"id": 1565, "seek": 730032, "start": 7305.92, "end": 7310.08, "text": " Minus j8 okay, so we're going to have to find out what that is", "tokens": [2829, 301, 361, 23, 1392, 11, 370, 321, 434, 516, 281, 362, 281, 915, 484, 437, 300, 307], "temperature": 0.0, "avg_logprob": -0.21675079345703124, "compression_ratio": 1.69, "no_speech_prob": 2.3320646960200975e-06}, {"id": 1566, "seek": 730032, "start": 7311.599999999999, "end": 7313.599999999999, "text": " times", "tokens": [1413], "temperature": 0.0, "avg_logprob": -0.21675079345703124, "compression_ratio": 1.69, "no_speech_prob": 2.3320646960200975e-06}, {"id": 1567, "seek": 730032, "start": 7313.639999999999, "end": 7315.639999999999, "text": " our learning rate", "tokens": [527, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.21675079345703124, "compression_ratio": 1.69, "no_speech_prob": 2.3320646960200975e-06}, {"id": 1568, "seek": 730032, "start": 7316.2, "end": 7320.36, "text": " Divided by the square root of L8 okay, so we're going to have to dig it and see what's going on", "tokens": [413, 1843, 292, 538, 264, 3732, 5593, 295, 441, 23, 1392, 11, 370, 321, 434, 516, 281, 362, 281, 2528, 309, 293, 536, 437, 311, 516, 322], "temperature": 0.0, "avg_logprob": -0.21675079345703124, "compression_ratio": 1.69, "no_speech_prob": 2.3320646960200975e-06}, {"id": 1569, "seek": 730032, "start": 7320.599999999999, "end": 7325.799999999999, "text": " One thing to notice here is that my learning rate is way higher than it used to be", "tokens": [1485, 551, 281, 3449, 510, 307, 300, 452, 2539, 3314, 307, 636, 2946, 813, 309, 1143, 281, 312], "temperature": 0.0, "avg_logprob": -0.21675079345703124, "compression_ratio": 1.69, "no_speech_prob": 2.3320646960200975e-06}, {"id": 1570, "seek": 730032, "start": 7326.5599999999995, "end": 7328.5599999999995, "text": " But then we're dividing it by this", "tokens": [583, 550, 321, 434, 26764, 309, 538, 341], "temperature": 0.0, "avg_logprob": -0.21675079345703124, "compression_ratio": 1.69, "no_speech_prob": 2.3320646960200975e-06}, {"id": 1571, "seek": 732856, "start": 7328.56, "end": 7334.96, "text": " Big number okay, so let's start out by looking and seeing what this j8 thing is", "tokens": [5429, 1230, 1392, 11, 370, 718, 311, 722, 484, 538, 1237, 293, 2577, 437, 341, 361, 23, 551, 307], "temperature": 0.0, "avg_logprob": -0.18825001052663295, "compression_ratio": 1.5336787564766838, "no_speech_prob": 2.090450607283856e-06}, {"id": 1572, "seek": 732856, "start": 7338.6, "end": 7341.8, "text": " Okay j8 is identical to what we had before", "tokens": [1033, 361, 23, 307, 14800, 281, 437, 321, 632, 949], "temperature": 0.0, "avg_logprob": -0.18825001052663295, "compression_ratio": 1.5336787564766838, "no_speech_prob": 2.090450607283856e-06}, {"id": 1573, "seek": 732856, "start": 7342.6, "end": 7346.6, "text": " j8 is equal to the linear interpolation of the derivative and", "tokens": [361, 23, 307, 2681, 281, 264, 8213, 44902, 399, 295, 264, 13760, 293], "temperature": 0.0, "avg_logprob": -0.18825001052663295, "compression_ratio": 1.5336787564766838, "no_speech_prob": 2.090450607283856e-06}, {"id": 1574, "seek": 732856, "start": 7347.8, "end": 7349.8, "text": " the previous direction", "tokens": [264, 3894, 3513], "temperature": 0.0, "avg_logprob": -0.18825001052663295, "compression_ratio": 1.5336787564766838, "no_speech_prob": 2.090450607283856e-06}, {"id": 1575, "seek": 732856, "start": 7350.320000000001, "end": 7352.320000000001, "text": " Okay, so that was easy", "tokens": [1033, 11, 370, 300, 390, 1858], "temperature": 0.0, "avg_logprob": -0.18825001052663295, "compression_ratio": 1.5336787564766838, "no_speech_prob": 2.090450607283856e-06}, {"id": 1576, "seek": 735232, "start": 7352.32, "end": 7358.28, "text": " So one part of Adam is to use momentum in the way we just defined okay?", "tokens": [407, 472, 644, 295, 7938, 307, 281, 764, 11244, 294, 264, 636, 321, 445, 7642, 1392, 30], "temperature": 0.0, "avg_logprob": -0.24876304626464843, "compression_ratio": 1.5852272727272727, "no_speech_prob": 3.0115933213892276e-07}, {"id": 1577, "seek": 735232, "start": 7359.88, "end": 7363.179999999999, "text": " The second piece was to divide by square root L8. What is that?", "tokens": [440, 1150, 2522, 390, 281, 9845, 538, 3732, 5593, 441, 23, 13, 708, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.24876304626464843, "compression_ratio": 1.5852272727272727, "no_speech_prob": 3.0115933213892276e-07}, {"id": 1578, "seek": 735232, "start": 7365.0, "end": 7370.32, "text": " Square root L8 okay is another linear interpolation of something and", "tokens": [16463, 5593, 441, 23, 1392, 307, 1071, 8213, 44902, 399, 295, 746, 293], "temperature": 0.0, "avg_logprob": -0.24876304626464843, "compression_ratio": 1.5852272727272727, "no_speech_prob": 3.0115933213892276e-07}, {"id": 1579, "seek": 735232, "start": 7371.32, "end": 7373.32, "text": " something else and", "tokens": [746, 1646, 293], "temperature": 0.0, "avg_logprob": -0.24876304626464843, "compression_ratio": 1.5852272727272727, "no_speech_prob": 3.0115933213892276e-07}, {"id": 1580, "seek": 735232, "start": 7373.4, "end": 7375.4, "text": " specifically, it's a linear interpolation of", "tokens": [4682, 11, 309, 311, 257, 8213, 44902, 399, 295], "temperature": 0.0, "avg_logprob": -0.24876304626464843, "compression_ratio": 1.5852272727272727, "no_speech_prob": 3.0115933213892276e-07}, {"id": 1581, "seek": 735232, "start": 7377.679999999999, "end": 7379.679999999999, "text": " F8 squared", "tokens": [479, 23, 8889], "temperature": 0.0, "avg_logprob": -0.24876304626464843, "compression_ratio": 1.5852272727272727, "no_speech_prob": 3.0115933213892276e-07}, {"id": 1582, "seek": 737968, "start": 7379.68, "end": 7382.72, "text": " Okay, it's a linear interpolation of the derivative squared", "tokens": [1033, 11, 309, 311, 257, 8213, 44902, 399, 295, 264, 13760, 8889], "temperature": 0.0, "avg_logprob": -0.21944242128184144, "compression_ratio": 1.8944099378881987, "no_speech_prob": 2.994435362779768e-06}, {"id": 1583, "seek": 737968, "start": 7384.400000000001, "end": 7391.0, "text": " Along with the derivative squared last time okay, so in other words. We've got two pieces of", "tokens": [17457, 365, 264, 13760, 8889, 1036, 565, 1392, 11, 370, 294, 661, 2283, 13, 492, 600, 658, 732, 3755, 295], "temperature": 0.0, "avg_logprob": -0.21944242128184144, "compression_ratio": 1.8944099378881987, "no_speech_prob": 2.994435362779768e-06}, {"id": 1584, "seek": 737968, "start": 7391.92, "end": 7394.240000000001, "text": " momentum going on here one is", "tokens": [11244, 516, 322, 510, 472, 307], "temperature": 0.0, "avg_logprob": -0.21944242128184144, "compression_ratio": 1.8944099378881987, "no_speech_prob": 2.994435362779768e-06}, {"id": 1585, "seek": 737968, "start": 7395.200000000001, "end": 7397.200000000001, "text": " calculating the", "tokens": [28258, 264], "temperature": 0.0, "avg_logprob": -0.21944242128184144, "compression_ratio": 1.8944099378881987, "no_speech_prob": 2.994435362779768e-06}, {"id": 1586, "seek": 737968, "start": 7397.200000000001, "end": 7398.72, "text": " momentum", "tokens": [11244], "temperature": 0.0, "avg_logprob": -0.21944242128184144, "compression_ratio": 1.8944099378881987, "no_speech_prob": 2.994435362779768e-06}, {"id": 1587, "seek": 737968, "start": 7398.72, "end": 7405.12, "text": " version of the gradient the other is calculating the momentum version of the gradient squared and", "tokens": [3037, 295, 264, 16235, 264, 661, 307, 28258, 264, 11244, 3037, 295, 264, 16235, 8889, 293], "temperature": 0.0, "avg_logprob": -0.21944242128184144, "compression_ratio": 1.8944099378881987, "no_speech_prob": 2.994435362779768e-06}, {"id": 1588, "seek": 740512, "start": 7405.12, "end": 7409.4, "text": " and we often refer to this idea as a", "tokens": [293, 321, 2049, 2864, 281, 341, 1558, 382, 257], "temperature": 0.0, "avg_logprob": -0.2351997977808902, "compression_ratio": 1.8837209302325582, "no_speech_prob": 1.4144727629172849e-06}, {"id": 1589, "seek": 740512, "start": 7410.72, "end": 7413.5199999999995, "text": " Exponentially weighted moving average in other words", "tokens": [21391, 266, 3137, 32807, 2684, 4274, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.2351997977808902, "compression_ratio": 1.8837209302325582, "no_speech_prob": 1.4144727629172849e-06}, {"id": 1590, "seek": 740512, "start": 7413.5199999999995, "end": 7418.48, "text": " It's basically equal to the average of this one and the last one and the last one in the last one that we're like", "tokens": [467, 311, 1936, 2681, 281, 264, 4274, 295, 341, 472, 293, 264, 1036, 472, 293, 264, 1036, 472, 294, 264, 1036, 472, 300, 321, 434, 411], "temperature": 0.0, "avg_logprob": -0.2351997977808902, "compression_ratio": 1.8837209302325582, "no_speech_prob": 1.4144727629172849e-06}, {"id": 1591, "seek": 740512, "start": 7418.92, "end": 7420.4, "text": " multiplicatively", "tokens": [17596, 19020], "temperature": 0.0, "avg_logprob": -0.2351997977808902, "compression_ratio": 1.8837209302325582, "no_speech_prob": 1.4144727629172849e-06}, {"id": 1592, "seek": 740512, "start": 7420.4, "end": 7426.08, "text": " Decreasing the previous ones because we're multiplying it by point nine times point nine times point nine times point nine", "tokens": [12427, 265, 3349, 264, 3894, 2306, 570, 321, 434, 30955, 309, 538, 935, 4949, 1413, 935, 4949, 1413, 935, 4949, 1413, 935, 4949], "temperature": 0.0, "avg_logprob": -0.2351997977808902, "compression_ratio": 1.8837209302325582, "no_speech_prob": 1.4144727629172849e-06}, {"id": 1593, "seek": 742608, "start": 7426.08, "end": 7435.2, "text": " And so you actually see that for instance in the fast AI code?", "tokens": [400, 370, 291, 767, 536, 300, 337, 5197, 294, 264, 2370, 7318, 3089, 30], "temperature": 0.0, "avg_logprob": -0.2710440709040715, "compression_ratio": 1.2363636363636363, "no_speech_prob": 9.874631814454915e-07}, {"id": 1594, "seek": 742608, "start": 7440.72, "end": 7442.72, "text": " If you look at fit", "tokens": [759, 291, 574, 412, 3318], "temperature": 0.0, "avg_logprob": -0.2710440709040715, "compression_ratio": 1.2363636363636363, "no_speech_prob": 9.874631814454915e-07}, {"id": 1595, "seek": 742608, "start": 7443.72, "end": 7448.08, "text": " We don't just calculate the average loss right", "tokens": [492, 500, 380, 445, 8873, 264, 4274, 4470, 558], "temperature": 0.0, "avg_logprob": -0.2710440709040715, "compression_ratio": 1.2363636363636363, "no_speech_prob": 9.874631814454915e-07}, {"id": 1596, "seek": 742608, "start": 7449.68, "end": 7450.88, "text": " because", "tokens": [570], "temperature": 0.0, "avg_logprob": -0.2710440709040715, "compression_ratio": 1.2363636363636363, "no_speech_prob": 9.874631814454915e-07}, {"id": 1597, "seek": 745088, "start": 7450.88, "end": 7456.74, "text": " What I actually want we certainly don't just report the loss for every mini-batch because that just bounces around so much", "tokens": [708, 286, 767, 528, 321, 3297, 500, 380, 445, 2275, 264, 4470, 337, 633, 8382, 12, 65, 852, 570, 300, 445, 46901, 926, 370, 709], "temperature": 0.0, "avg_logprob": -0.21373650400262129, "compression_ratio": 1.63135593220339, "no_speech_prob": 4.9652766165309e-07}, {"id": 1598, "seek": 745088, "start": 7457.08, "end": 7463.08, "text": " So instead I say average loss is equal to whatever the average was was last time", "tokens": [407, 2602, 286, 584, 4274, 4470, 307, 2681, 281, 2035, 264, 4274, 390, 390, 1036, 565], "temperature": 0.0, "avg_logprob": -0.21373650400262129, "compression_ratio": 1.63135593220339, "no_speech_prob": 4.9652766165309e-07}, {"id": 1599, "seek": 745088, "start": 7463.6, "end": 7465.28, "text": " times", "tokens": [1413], "temperature": 0.0, "avg_logprob": -0.21373650400262129, "compression_ratio": 1.63135593220339, "no_speech_prob": 4.9652766165309e-07}, {"id": 1600, "seek": 745088, "start": 7465.28, "end": 7466.52, "text": " 0.98", "tokens": [1958, 13, 22516], "temperature": 0.0, "avg_logprob": -0.21373650400262129, "compression_ratio": 1.63135593220339, "no_speech_prob": 4.9652766165309e-07}, {"id": 1601, "seek": 745088, "start": 7466.52, "end": 7470.16, "text": " Plus the loss this time times point oh two", "tokens": [7721, 264, 4470, 341, 565, 1413, 935, 1954, 732], "temperature": 0.0, "avg_logprob": -0.21373650400262129, "compression_ratio": 1.63135593220339, "no_speech_prob": 4.9652766165309e-07}, {"id": 1602, "seek": 745088, "start": 7470.76, "end": 7478.26, "text": " Right so in other words the fast AI library the thing that it's actually when you do like the learning rate finder or plot loss", "tokens": [1779, 370, 294, 661, 2283, 264, 2370, 7318, 6405, 264, 551, 300, 309, 311, 767, 562, 291, 360, 411, 264, 2539, 3314, 915, 260, 420, 7542, 4470], "temperature": 0.0, "avg_logprob": -0.21373650400262129, "compression_ratio": 1.63135593220339, "no_speech_prob": 4.9652766165309e-07}, {"id": 1603, "seek": 747826, "start": 7478.26, "end": 7483.24, "text": " It's actually showing you the exponentially weighted moving average of the loss", "tokens": [467, 311, 767, 4099, 291, 264, 37330, 32807, 2684, 4274, 295, 264, 4470], "temperature": 0.0, "avg_logprob": -0.1779544754783706, "compression_ratio": 1.697211155378486, "no_speech_prob": 7.338198884099256e-07}, {"id": 1604, "seek": 747826, "start": 7483.76, "end": 7488.18, "text": " Okay, so it's like a really handy concept it appears quite a lot", "tokens": [1033, 11, 370, 309, 311, 411, 257, 534, 13239, 3410, 309, 7038, 1596, 257, 688], "temperature": 0.0, "avg_logprob": -0.1779544754783706, "compression_ratio": 1.697211155378486, "no_speech_prob": 7.338198884099256e-07}, {"id": 1605, "seek": 747826, "start": 7488.42, "end": 7494.5, "text": " All right the other a handy concept to know about it's this idea of like you've got two numbers", "tokens": [1057, 558, 264, 661, 257, 13239, 3410, 281, 458, 466, 309, 311, 341, 1558, 295, 411, 291, 600, 658, 732, 3547], "temperature": 0.0, "avg_logprob": -0.1779544754783706, "compression_ratio": 1.697211155378486, "no_speech_prob": 7.338198884099256e-07}, {"id": 1606, "seek": 747826, "start": 7495.02, "end": 7499.9800000000005, "text": " One of them is multiplied by some value the other is multiplied by one minus that value", "tokens": [1485, 295, 552, 307, 17207, 538, 512, 2158, 264, 661, 307, 17207, 538, 472, 3175, 300, 2158], "temperature": 0.0, "avg_logprob": -0.1779544754783706, "compression_ratio": 1.697211155378486, "no_speech_prob": 7.338198884099256e-07}, {"id": 1607, "seek": 747826, "start": 7499.9800000000005, "end": 7505.280000000001, "text": " So this is a linear interpolation with two values. You'll see it all the time and", "tokens": [407, 341, 307, 257, 8213, 44902, 399, 365, 732, 4190, 13, 509, 603, 536, 309, 439, 264, 565, 293], "temperature": 0.0, "avg_logprob": -0.1779544754783706, "compression_ratio": 1.697211155378486, "no_speech_prob": 7.338198884099256e-07}, {"id": 1608, "seek": 750528, "start": 7505.28, "end": 7507.28, "text": " for some reason", "tokens": [337, 512, 1778], "temperature": 0.0, "avg_logprob": -0.23723072252775493, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.8738688822850236e-06}, {"id": 1609, "seek": 750528, "start": 7507.96, "end": 7514.599999999999, "text": " Deep learning people nearly always use the value alpha when they do this so like keep an eye out if you're reading a paper", "tokens": [14895, 2539, 561, 6217, 1009, 764, 264, 2158, 8961, 562, 436, 360, 341, 370, 411, 1066, 364, 3313, 484, 498, 291, 434, 3760, 257, 3035], "temperature": 0.0, "avg_logprob": -0.23723072252775493, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.8738688822850236e-06}, {"id": 1610, "seek": 750528, "start": 7514.599999999999, "end": 7518.88, "text": " Or something and you see like alpha times blah blah blah blah blah plus", "tokens": [1610, 746, 293, 291, 536, 411, 8961, 1413, 12288, 12288, 12288, 12288, 12288, 1804], "temperature": 0.0, "avg_logprob": -0.23723072252775493, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.8738688822850236e-06}, {"id": 1611, "seek": 750528, "start": 7519.4, "end": 7521.4, "text": " one minus alpha", "tokens": [472, 3175, 8961], "temperature": 0.0, "avg_logprob": -0.23723072252775493, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.8738688822850236e-06}, {"id": 1612, "seek": 750528, "start": 7521.599999999999, "end": 7526.9, "text": " Times some other blah blah blah blah right immediately like when people read papers", "tokens": [11366, 512, 661, 12288, 12288, 12288, 12288, 558, 4258, 411, 562, 561, 1401, 10577], "temperature": 0.0, "avg_logprob": -0.23723072252775493, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.8738688822850236e-06}, {"id": 1613, "seek": 750528, "start": 7527.44, "end": 7533.36, "text": " None of us like read every thing in the equation we look at it. We go. Oh linear interpolation", "tokens": [14492, 295, 505, 411, 1401, 633, 551, 294, 264, 5367, 321, 574, 412, 309, 13, 492, 352, 13, 876, 8213, 44902, 399], "temperature": 0.0, "avg_logprob": -0.23723072252775493, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.8738688822850236e-06}, {"id": 1614, "seek": 753336, "start": 7533.36, "end": 7537.32, "text": " Right and I said something else just talking to Rachel about yesterday", "tokens": [1779, 293, 286, 848, 746, 1646, 445, 1417, 281, 14246, 466, 5186], "temperature": 0.0, "avg_logprob": -0.20364288330078126, "compression_ratio": 1.8041958041958042, "no_speech_prob": 9.874610213955748e-07}, {"id": 1615, "seek": 753336, "start": 7537.32, "end": 7543.5199999999995, "text": " It's like whether we could start trying to find like a a new way of writing papers where we literally refactor them", "tokens": [467, 311, 411, 1968, 321, 727, 722, 1382, 281, 915, 411, 257, 257, 777, 636, 295, 3579, 10577, 689, 321, 3736, 1895, 15104, 552], "temperature": 0.0, "avg_logprob": -0.20364288330078126, "compression_ratio": 1.8041958041958042, "no_speech_prob": 9.874610213955748e-07}, {"id": 1616, "seek": 753336, "start": 7543.5199999999995, "end": 7546.32, "text": " All right, like it'd be so much better to have written like", "tokens": [1057, 558, 11, 411, 309, 1116, 312, 370, 709, 1101, 281, 362, 3720, 411], "temperature": 0.0, "avg_logprob": -0.20364288330078126, "compression_ratio": 1.8041958041958042, "no_speech_prob": 9.874610213955748e-07}, {"id": 1617, "seek": 753336, "start": 7547.04, "end": 7548.48, "text": " linear", "tokens": [8213], "temperature": 0.0, "avg_logprob": -0.20364288330078126, "compression_ratio": 1.8041958041958042, "no_speech_prob": 9.874610213955748e-07}, {"id": 1618, "seek": 753336, "start": 7548.48, "end": 7549.96, "text": " interpolate", "tokens": [44902, 473], "temperature": 0.0, "avg_logprob": -0.20364288330078126, "compression_ratio": 1.8041958041958042, "no_speech_prob": 9.874610213955748e-07}, {"id": 1619, "seek": 753336, "start": 7549.96, "end": 7555.94, "text": " Blah blah blah blah blah blah blah right because then you don't have to have that pattern recognition right but until we", "tokens": [2177, 545, 12288, 12288, 12288, 12288, 12288, 12288, 558, 570, 550, 291, 500, 380, 362, 281, 362, 300, 5102, 11150, 558, 457, 1826, 321], "temperature": 0.0, "avg_logprob": -0.20364288330078126, "compression_ratio": 1.8041958041958042, "no_speech_prob": 9.874610213955748e-07}, {"id": 1620, "seek": 753336, "start": 7556.5599999999995, "end": 7558.5599999999995, "text": " Convince the world to change how they write papers", "tokens": [2656, 85, 1236, 264, 1002, 281, 1319, 577, 436, 2464, 10577], "temperature": 0.0, "avg_logprob": -0.20364288330078126, "compression_ratio": 1.8041958041958042, "no_speech_prob": 9.874610213955748e-07}, {"id": 1621, "seek": 753336, "start": 7558.759999999999, "end": 7562.32, "text": " This is what you have to do is you have to look you know know what to look for", "tokens": [639, 307, 437, 291, 362, 281, 360, 307, 291, 362, 281, 574, 291, 458, 458, 437, 281, 574, 337], "temperature": 0.0, "avg_logprob": -0.20364288330078126, "compression_ratio": 1.8041958041958042, "no_speech_prob": 9.874610213955748e-07}, {"id": 1622, "seek": 756232, "start": 7562.32, "end": 7564.88, "text": " right and once you do suddenly the", "tokens": [558, 293, 1564, 291, 360, 5800, 264], "temperature": 0.0, "avg_logprob": -0.23888303648750736, "compression_ratio": 1.7075098814229248, "no_speech_prob": 1.8738714970822912e-06}, {"id": 1623, "seek": 756232, "start": 7565.599999999999, "end": 7571.28, "text": " Huge page width formulas aren't aren't bad at all like you often notice like for example", "tokens": [37043, 3028, 11402, 30546, 3212, 380, 3212, 380, 1578, 412, 439, 411, 291, 2049, 3449, 411, 337, 1365], "temperature": 0.0, "avg_logprob": -0.23888303648750736, "compression_ratio": 1.7075098814229248, "no_speech_prob": 1.8738714970822912e-06}, {"id": 1624, "seek": 756232, "start": 7571.639999999999, "end": 7574.96, "text": " The two things in here like they might be totally identical", "tokens": [440, 732, 721, 294, 510, 411, 436, 1062, 312, 3879, 14800], "temperature": 0.0, "avg_logprob": -0.23888303648750736, "compression_ratio": 1.7075098814229248, "no_speech_prob": 1.8738714970822912e-06}, {"id": 1625, "seek": 756232, "start": 7574.96, "end": 7579.08, "text": " But this might be a time t and this might be at like time t minus 1 or something", "tokens": [583, 341, 1062, 312, 257, 565, 256, 293, 341, 1062, 312, 412, 411, 565, 256, 3175, 502, 420, 746], "temperature": 0.0, "avg_logprob": -0.23888303648750736, "compression_ratio": 1.7075098814229248, "no_speech_prob": 1.8738714970822912e-06}, {"id": 1626, "seek": 756232, "start": 7579.16, "end": 7583.139999999999, "text": " Right like it's very often these big ugly formulas turn out to be", "tokens": [1779, 411, 309, 311, 588, 2049, 613, 955, 12246, 30546, 1261, 484, 281, 312], "temperature": 0.0, "avg_logprob": -0.23888303648750736, "compression_ratio": 1.7075098814229248, "no_speech_prob": 1.8738714970822912e-06}, {"id": 1627, "seek": 756232, "start": 7583.88, "end": 7587.759999999999, "text": " Really really simple if only they repack them, okay?", "tokens": [4083, 534, 2199, 498, 787, 436, 1085, 501, 552, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.23888303648750736, "compression_ratio": 1.7075098814229248, "no_speech_prob": 1.8738714970822912e-06}, {"id": 1628, "seek": 758776, "start": 7587.76, "end": 7591.4400000000005, "text": " So what are we doing with this gradient squared? So?", "tokens": [407, 437, 366, 321, 884, 365, 341, 16235, 8889, 30, 407, 30], "temperature": 0.0, "avg_logprob": -0.23179360323174056, "compression_ratio": 1.943298969072165, "no_speech_prob": 1.5534932344962726e-06}, {"id": 1629, "seek": 758776, "start": 7592.860000000001, "end": 7595.88, "text": " What we were doing with the gradient squared is", "tokens": [708, 321, 645, 884, 365, 264, 16235, 8889, 307], "temperature": 0.0, "avg_logprob": -0.23179360323174056, "compression_ratio": 1.943298969072165, "no_speech_prob": 1.5534932344962726e-06}, {"id": 1630, "seek": 758776, "start": 7596.76, "end": 7603.76, "text": " We were taking the square root, and then we were adjusting the learning rate by dividing the learning rate by that", "tokens": [492, 645, 1940, 264, 3732, 5593, 11, 293, 550, 321, 645, 23559, 264, 2539, 3314, 538, 26764, 264, 2539, 3314, 538, 300], "temperature": 0.0, "avg_logprob": -0.23179360323174056, "compression_ratio": 1.943298969072165, "no_speech_prob": 1.5534932344962726e-06}, {"id": 1631, "seek": 758776, "start": 7604.64, "end": 7609.04, "text": " Okay, so gradient squared is always positive", "tokens": [1033, 11, 370, 16235, 8889, 307, 1009, 3353], "temperature": 0.0, "avg_logprob": -0.23179360323174056, "compression_ratio": 1.943298969072165, "no_speech_prob": 1.5534932344962726e-06}, {"id": 1632, "seek": 758776, "start": 7609.8, "end": 7611.8, "text": " right and", "tokens": [558, 293], "temperature": 0.0, "avg_logprob": -0.23179360323174056, "compression_ratio": 1.943298969072165, "no_speech_prob": 1.5534932344962726e-06}, {"id": 1633, "seek": 758776, "start": 7612.12, "end": 7617.2, "text": " We're taking the exponentially waiting move a moving average of a bunch of things that are always positive", "tokens": [492, 434, 1940, 264, 37330, 3806, 1286, 257, 2684, 4274, 295, 257, 3840, 295, 721, 300, 366, 1009, 3353], "temperature": 0.0, "avg_logprob": -0.23179360323174056, "compression_ratio": 1.943298969072165, "no_speech_prob": 1.5534932344962726e-06}, {"id": 1634, "seek": 761720, "start": 7617.2, "end": 7619.2, "text": " And then we're taking the square root of that", "tokens": [400, 550, 321, 434, 1940, 264, 3732, 5593, 295, 300], "temperature": 0.0, "avg_logprob": -0.17141386440822057, "compression_ratio": 1.8504273504273505, "no_speech_prob": 2.6425786927575245e-06}, {"id": 1635, "seek": 761720, "start": 7619.5199999999995, "end": 7621.82, "text": " Right so when is this number going to be high?", "tokens": [1779, 370, 562, 307, 341, 1230, 516, 281, 312, 1090, 30], "temperature": 0.0, "avg_logprob": -0.17141386440822057, "compression_ratio": 1.8504273504273505, "no_speech_prob": 2.6425786927575245e-06}, {"id": 1636, "seek": 761720, "start": 7622.84, "end": 7625.88, "text": " It's going to be particularly high if there's like one big", "tokens": [467, 311, 516, 281, 312, 4098, 1090, 498, 456, 311, 411, 472, 955], "temperature": 0.0, "avg_logprob": -0.17141386440822057, "compression_ratio": 1.8504273504273505, "no_speech_prob": 2.6425786927575245e-06}, {"id": 1637, "seek": 761720, "start": 7626.36, "end": 7630.0, "text": " You know if the gradients got a lot of variation, right?", "tokens": [509, 458, 498, 264, 2771, 2448, 658, 257, 688, 295, 12990, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17141386440822057, "compression_ratio": 1.8504273504273505, "no_speech_prob": 2.6425786927575245e-06}, {"id": 1638, "seek": 761720, "start": 7630.0, "end": 7635.8, "text": " So there's a high variance of gradient then this g squared thing is going to be a really high number", "tokens": [407, 456, 311, 257, 1090, 21977, 295, 16235, 550, 341, 290, 8889, 551, 307, 516, 281, 312, 257, 534, 1090, 1230], "temperature": 0.0, "avg_logprob": -0.17141386440822057, "compression_ratio": 1.8504273504273505, "no_speech_prob": 2.6425786927575245e-06}, {"id": 1639, "seek": 761720, "start": 7636.32, "end": 7638.32, "text": " Where else if it's like a constant?", "tokens": [2305, 1646, 498, 309, 311, 411, 257, 5754, 30], "temperature": 0.0, "avg_logprob": -0.17141386440822057, "compression_ratio": 1.8504273504273505, "no_speech_prob": 2.6425786927575245e-06}, {"id": 1640, "seek": 761720, "start": 7639.36, "end": 7643.88, "text": " Amount right it's going to be smaller that because when you add things that are squared", "tokens": [2012, 792, 558, 309, 311, 516, 281, 312, 4356, 300, 570, 562, 291, 909, 721, 300, 366, 8889], "temperature": 0.0, "avg_logprob": -0.17141386440822057, "compression_ratio": 1.8504273504273505, "no_speech_prob": 2.6425786927575245e-06}, {"id": 1641, "seek": 764388, "start": 7643.88, "end": 7648.88, "text": " The squared slight jump out much bigger or else if there wasn't if there wasn't much change", "tokens": [440, 8889, 4036, 3012, 484, 709, 3801, 420, 1646, 498, 456, 2067, 380, 498, 456, 2067, 380, 709, 1319], "temperature": 0.0, "avg_logprob": -0.24656084750561005, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.6165190547544626e-07}, {"id": 1642, "seek": 764388, "start": 7649.36, "end": 7652.04, "text": " That it's not going to be as big so basically", "tokens": [663, 309, 311, 406, 516, 281, 312, 382, 955, 370, 1936], "temperature": 0.0, "avg_logprob": -0.24656084750561005, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.6165190547544626e-07}, {"id": 1643, "seek": 764388, "start": 7652.76, "end": 7654.76, "text": " This number at the bottom here", "tokens": [639, 1230, 412, 264, 2767, 510], "temperature": 0.0, "avg_logprob": -0.24656084750561005, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.6165190547544626e-07}, {"id": 1644, "seek": 764388, "start": 7655.68, "end": 7657.68, "text": " It's going to be high", "tokens": [467, 311, 516, 281, 312, 1090], "temperature": 0.0, "avg_logprob": -0.24656084750561005, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.6165190547544626e-07}, {"id": 1645, "seek": 764388, "start": 7657.68, "end": 7661.54, "text": " If that gradient is changing a lot now. What do you want to do?", "tokens": [759, 300, 16235, 307, 4473, 257, 688, 586, 13, 708, 360, 291, 528, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.24656084750561005, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.6165190547544626e-07}, {"id": 1646, "seek": 764388, "start": 7662.32, "end": 7667.32, "text": " If you've got something which is like first negative and then positive and then small and then high", "tokens": [759, 291, 600, 658, 746, 597, 307, 411, 700, 3671, 293, 550, 3353, 293, 550, 1359, 293, 550, 1090], "temperature": 0.0, "avg_logprob": -0.24656084750561005, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.6165190547544626e-07}, {"id": 1647, "seek": 764388, "start": 7668.36, "end": 7669.68, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.24656084750561005, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.6165190547544626e-07}, {"id": 1648, "seek": 766968, "start": 7669.68, "end": 7676.66, "text": " Well, you probably want to be more careful right you probably don't want to take a big step because you can't really trust it", "tokens": [1042, 11, 291, 1391, 528, 281, 312, 544, 5026, 558, 291, 1391, 500, 380, 528, 281, 747, 257, 955, 1823, 570, 291, 393, 380, 534, 3361, 309], "temperature": 0.0, "avg_logprob": -0.1719898840393683, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.313907595587807e-07}, {"id": 1649, "seek": 766968, "start": 7676.88, "end": 7680.400000000001, "text": " Right so when the when the variance of the gradient is high", "tokens": [1779, 370, 562, 264, 562, 264, 21977, 295, 264, 16235, 307, 1090], "temperature": 0.0, "avg_logprob": -0.1719898840393683, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.313907595587807e-07}, {"id": 1650, "seek": 766968, "start": 7680.88, "end": 7683.200000000001, "text": " We're going to divide our learning rate by a big number", "tokens": [492, 434, 516, 281, 9845, 527, 2539, 3314, 538, 257, 955, 1230], "temperature": 0.0, "avg_logprob": -0.1719898840393683, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.313907595587807e-07}, {"id": 1651, "seek": 766968, "start": 7684.08, "end": 7686.08, "text": " Where else if our learning rate is?", "tokens": [2305, 1646, 498, 527, 2539, 3314, 307, 30], "temperature": 0.0, "avg_logprob": -0.1719898840393683, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.313907595587807e-07}, {"id": 1652, "seek": 766968, "start": 7686.280000000001, "end": 7691.400000000001, "text": " Very similar kind of size all the time then we probably feel pretty good about the step", "tokens": [4372, 2531, 733, 295, 2744, 439, 264, 565, 550, 321, 1391, 841, 1238, 665, 466, 264, 1823], "temperature": 0.0, "avg_logprob": -0.1719898840393683, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.313907595587807e-07}, {"id": 1653, "seek": 766968, "start": 7691.52, "end": 7693.52, "text": " So we're dividing it by a small amount", "tokens": [407, 321, 434, 26764, 309, 538, 257, 1359, 2372], "temperature": 0.0, "avg_logprob": -0.1719898840393683, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.313907595587807e-07}, {"id": 1654, "seek": 769352, "start": 7693.52, "end": 7700.320000000001, "text": " And so this is called an adaptive learning right now and fit like a lot of people will have this confusion about Adam", "tokens": [400, 370, 341, 307, 1219, 364, 27912, 2539, 558, 586, 293, 3318, 411, 257, 688, 295, 561, 486, 362, 341, 15075, 466, 7938], "temperature": 0.0, "avg_logprob": -0.22732830955868677, "compression_ratio": 1.7547169811320755, "no_speech_prob": 5.368743813960464e-07}, {"id": 1655, "seek": 769352, "start": 7700.320000000001, "end": 7702.580000000001, "text": " I've seen it on the forum actually where people are like", "tokens": [286, 600, 1612, 309, 322, 264, 17542, 767, 689, 561, 366, 411], "temperature": 0.0, "avg_logprob": -0.22732830955868677, "compression_ratio": 1.7547169811320755, "no_speech_prob": 5.368743813960464e-07}, {"id": 1656, "seek": 769352, "start": 7703.160000000001, "end": 7707.240000000001, "text": " Isn't there some kind of adaptive learning rate where somehow you're like setting different?", "tokens": [6998, 380, 456, 512, 733, 295, 27912, 2539, 3314, 689, 6063, 291, 434, 411, 3287, 819, 30], "temperature": 0.0, "avg_logprob": -0.22732830955868677, "compression_ratio": 1.7547169811320755, "no_speech_prob": 5.368743813960464e-07}, {"id": 1657, "seek": 769352, "start": 7708.64, "end": 7712.56, "text": " Learning rates for different layers or something. It's like no not really", "tokens": [15205, 6846, 337, 819, 7914, 420, 746, 13, 467, 311, 411, 572, 406, 534], "temperature": 0.0, "avg_logprob": -0.22732830955868677, "compression_ratio": 1.7547169811320755, "no_speech_prob": 5.368743813960464e-07}, {"id": 1658, "seek": 769352, "start": 7713.160000000001, "end": 7720.64, "text": " Right all we're doing is we're just saying like just keep track of the average of the squares of the gradients and use that", "tokens": [1779, 439, 321, 434, 884, 307, 321, 434, 445, 1566, 411, 445, 1066, 2837, 295, 264, 4274, 295, 264, 19368, 295, 264, 2771, 2448, 293, 764, 300], "temperature": 0.0, "avg_logprob": -0.22732830955868677, "compression_ratio": 1.7547169811320755, "no_speech_prob": 5.368743813960464e-07}, {"id": 1659, "seek": 772064, "start": 7720.64, "end": 7724.200000000001, "text": " To adjust the learning rate, so there's still one learning rate", "tokens": [1407, 4369, 264, 2539, 3314, 11, 370, 456, 311, 920, 472, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.22259532573611238, "compression_ratio": 1.6650717703349283, "no_speech_prob": 9.874624993244652e-07}, {"id": 1660, "seek": 772064, "start": 7725.160000000001, "end": 7727.160000000001, "text": " Okay, in this case. It's one", "tokens": [1033, 11, 294, 341, 1389, 13, 467, 311, 472], "temperature": 0.0, "avg_logprob": -0.22259532573611238, "compression_ratio": 1.6650717703349283, "no_speech_prob": 9.874624993244652e-07}, {"id": 1661, "seek": 772064, "start": 7727.360000000001, "end": 7732.08, "text": " Okay, but effectively every parameter at every epoch", "tokens": [1033, 11, 457, 8659, 633, 13075, 412, 633, 30992, 339], "temperature": 0.0, "avg_logprob": -0.22259532573611238, "compression_ratio": 1.6650717703349283, "no_speech_prob": 9.874624993244652e-07}, {"id": 1662, "seek": 772064, "start": 7732.68, "end": 7740.76, "text": " Is being kind of like getting a bigger jump if the learning rate if the gradient's been pretty constant for that weight and a smaller jump", "tokens": [1119, 885, 733, 295, 411, 1242, 257, 3801, 3012, 498, 264, 2539, 3314, 498, 264, 16235, 311, 668, 1238, 5754, 337, 300, 3364, 293, 257, 4356, 3012], "temperature": 0.0, "avg_logprob": -0.22259532573611238, "compression_ratio": 1.6650717703349283, "no_speech_prob": 9.874624993244652e-07}, {"id": 1663, "seek": 772064, "start": 7741.52, "end": 7745.240000000001, "text": " Otherwise okay, and that's Adam. That's the entirety of Adam in", "tokens": [10328, 1392, 11, 293, 300, 311, 7938, 13, 663, 311, 264, 31557, 295, 7938, 294], "temperature": 0.0, "avg_logprob": -0.22259532573611238, "compression_ratio": 1.6650717703349283, "no_speech_prob": 9.874624993244652e-07}, {"id": 1664, "seek": 774524, "start": 7745.24, "end": 7752.44, "text": " In Excel right so there's now no reason at all why you can't train ImageNet in Excel because you've got you've got access to", "tokens": [682, 19060, 558, 370, 456, 311, 586, 572, 1778, 412, 439, 983, 291, 393, 380, 3847, 29903, 31890, 294, 19060, 570, 291, 600, 658, 291, 600, 658, 2105, 281], "temperature": 0.0, "avg_logprob": -0.1753698797786937, "compression_ratio": 1.6475409836065573, "no_speech_prob": 2.2252750113693764e-06}, {"id": 1665, "seek": 774524, "start": 7752.44, "end": 7754.44, "text": " All of the pieces you need", "tokens": [1057, 295, 264, 3755, 291, 643], "temperature": 0.0, "avg_logprob": -0.1753698797786937, "compression_ratio": 1.6475409836065573, "no_speech_prob": 2.2252750113693764e-06}, {"id": 1666, "seek": 774524, "start": 7754.639999999999, "end": 7756.639999999999, "text": " And so let's try this out run", "tokens": [400, 370, 718, 311, 853, 341, 484, 1190], "temperature": 0.0, "avg_logprob": -0.1753698797786937, "compression_ratio": 1.6475409836065573, "no_speech_prob": 2.2252750113693764e-06}, {"id": 1667, "seek": 774524, "start": 7760.36, "end": 7766.8, "text": " Okay, that's not bad right five and we're straight up to 29 and two right so the difference between like", "tokens": [1033, 11, 300, 311, 406, 1578, 558, 1732, 293, 321, 434, 2997, 493, 281, 9413, 293, 732, 558, 370, 264, 2649, 1296, 411], "temperature": 0.0, "avg_logprob": -0.1753698797786937, "compression_ratio": 1.6475409836065573, "no_speech_prob": 2.2252750113693764e-06}, {"id": 1668, "seek": 774524, "start": 7767.92, "end": 7774.36, "text": " You know standard SGD and this is is is huge and basically that you know the key difference was that it figured out", "tokens": [509, 458, 3832, 34520, 35, 293, 341, 307, 307, 307, 2603, 293, 1936, 300, 291, 458, 264, 2141, 2649, 390, 300, 309, 8932, 484], "temperature": 0.0, "avg_logprob": -0.1753698797786937, "compression_ratio": 1.6475409836065573, "no_speech_prob": 2.2252750113693764e-06}, {"id": 1669, "seek": 777436, "start": 7774.36, "end": 7776.36, "text": " That we need to be you know", "tokens": [663, 321, 643, 281, 312, 291, 458], "temperature": 0.0, "avg_logprob": -0.23086299452670786, "compression_ratio": 1.6802030456852792, "no_speech_prob": 2.561272140155779e-06}, {"id": 1670, "seek": 777436, "start": 7777.12, "end": 7778.92, "text": " moving this number", "tokens": [2684, 341, 1230], "temperature": 0.0, "avg_logprob": -0.23086299452670786, "compression_ratio": 1.6802030456852792, "no_speech_prob": 2.561272140155779e-06}, {"id": 1671, "seek": 777436, "start": 7778.92, "end": 7780.28, "text": " much faster", "tokens": [709, 4663], "temperature": 0.0, "avg_logprob": -0.23086299452670786, "compression_ratio": 1.6802030456852792, "no_speech_prob": 2.561272140155779e-06}, {"id": 1672, "seek": 777436, "start": 7780.28, "end": 7782.5599999999995, "text": " okay, and so and so it did and", "tokens": [1392, 11, 293, 370, 293, 370, 309, 630, 293], "temperature": 0.0, "avg_logprob": -0.23086299452670786, "compression_ratio": 1.6802030456852792, "no_speech_prob": 2.561272140155779e-06}, {"id": 1673, "seek": 777436, "start": 7783.48, "end": 7785.48, "text": " so you can see we've now got like", "tokens": [370, 291, 393, 536, 321, 600, 586, 658, 411], "temperature": 0.0, "avg_logprob": -0.23086299452670786, "compression_ratio": 1.6802030456852792, "no_speech_prob": 2.561272140155779e-06}, {"id": 1674, "seek": 777436, "start": 7786.08, "end": 7793.599999999999, "text": " Two different parameters one is kind of the momentum for the gradient piece the other is the momentum for the gradient squared piece and", "tokens": [4453, 819, 9834, 472, 307, 733, 295, 264, 11244, 337, 264, 16235, 2522, 264, 661, 307, 264, 11244, 337, 264, 16235, 8889, 2522, 293], "temperature": 0.0, "avg_logprob": -0.23086299452670786, "compression_ratio": 1.6802030456852792, "no_speech_prob": 2.561272140155779e-06}, {"id": 1675, "seek": 777436, "start": 7795.16, "end": 7799.32, "text": " They I think they're called like I think there's just a couple of beta", "tokens": [814, 286, 519, 436, 434, 1219, 411, 286, 519, 456, 311, 445, 257, 1916, 295, 9861], "temperature": 0.0, "avg_logprob": -0.23086299452670786, "compression_ratio": 1.6802030456852792, "no_speech_prob": 2.561272140155779e-06}, {"id": 1676, "seek": 779932, "start": 7799.32, "end": 7804.44, "text": " I think when you when you want to change it in PyTorch says I think what beta which is just a couple of two numbers", "tokens": [286, 519, 562, 291, 562, 291, 528, 281, 1319, 309, 294, 9953, 51, 284, 339, 1619, 286, 519, 437, 9861, 597, 307, 445, 257, 1916, 295, 732, 3547], "temperature": 0.0, "avg_logprob": -0.25502485275268555, "compression_ratio": 1.6866952789699572, "no_speech_prob": 8.663771041028667e-06}, {"id": 1677, "seek": 779932, "start": 7804.44, "end": 7806.44, "text": " You can change", "tokens": [509, 393, 1319], "temperature": 0.0, "avg_logprob": -0.25502485275268555, "compression_ratio": 1.6866952789699572, "no_speech_prob": 8.663771041028667e-06}, {"id": 1678, "seek": 779932, "start": 7808.759999999999, "end": 7810.759999999999, "text": " Jeremy so", "tokens": [17809, 370], "temperature": 0.0, "avg_logprob": -0.25502485275268555, "compression_ratio": 1.6866952789699572, "no_speech_prob": 8.663771041028667e-06}, {"id": 1679, "seek": 779932, "start": 7811.679999999999, "end": 7813.679999999999, "text": " So you said the", "tokens": [407, 291, 848, 264], "temperature": 0.0, "avg_logprob": -0.25502485275268555, "compression_ratio": 1.6866952789699572, "no_speech_prob": 8.663771041028667e-06}, {"id": 1680, "seek": 779932, "start": 7813.679999999999, "end": 7817.7, "text": " yeah, I think I understand this concept of you know when they", "tokens": [1338, 11, 286, 519, 286, 1223, 341, 3410, 295, 291, 458, 562, 436], "temperature": 0.0, "avg_logprob": -0.25502485275268555, "compression_ratio": 1.6866952789699572, "no_speech_prob": 8.663771041028667e-06}, {"id": 1681, "seek": 779932, "start": 7818.4, "end": 7822.759999999999, "text": " When a gradient is it goes up and down then you're not really sure", "tokens": [1133, 257, 16235, 307, 309, 1709, 493, 293, 760, 550, 291, 434, 406, 534, 988], "temperature": 0.0, "avg_logprob": -0.25502485275268555, "compression_ratio": 1.6866952789699572, "no_speech_prob": 8.663771041028667e-06}, {"id": 1682, "seek": 779932, "start": 7823.84, "end": 7828.679999999999, "text": " Which direction should it should go so you said should kind of slow things down therefore you subtract that", "tokens": [3013, 3513, 820, 309, 820, 352, 370, 291, 848, 820, 733, 295, 2964, 721, 760, 4412, 291, 16390, 300], "temperature": 0.0, "avg_logprob": -0.25502485275268555, "compression_ratio": 1.6866952789699572, "no_speech_prob": 8.663771041028667e-06}, {"id": 1683, "seek": 782868, "start": 7828.68, "end": 7830.68, "text": " You know radiant from the learning rate", "tokens": [509, 458, 49430, 490, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.27346794203956526, "compression_ratio": 1.6986899563318778, "no_speech_prob": 3.905458925146377e-06}, {"id": 1684, "seek": 782868, "start": 7830.76, "end": 7834.6, "text": " So but how do you implement that how far do you go?", "tokens": [407, 457, 577, 360, 291, 4445, 300, 577, 1400, 360, 291, 352, 30], "temperature": 0.0, "avg_logprob": -0.27346794203956526, "compression_ratio": 1.6986899563318778, "no_speech_prob": 3.905458925146377e-06}, {"id": 1685, "seek": 782868, "start": 7834.6, "end": 7840.56, "text": " I guess maybe I missed something in early on you you set a number somewhere we divide", "tokens": [286, 2041, 1310, 286, 6721, 746, 294, 2440, 322, 291, 291, 992, 257, 1230, 4079, 321, 9845], "temperature": 0.0, "avg_logprob": -0.27346794203956526, "compression_ratio": 1.6986899563318778, "no_speech_prob": 3.905458925146377e-06}, {"id": 1686, "seek": 782868, "start": 7841.4800000000005, "end": 7843.4800000000005, "text": " Yeah, we divide the learning rate", "tokens": [865, 11, 321, 9845, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.27346794203956526, "compression_ratio": 1.6986899563318778, "no_speech_prob": 3.905458925146377e-06}, {"id": 1687, "seek": 782868, "start": 7844.72, "end": 7850.92, "text": " Divided by the square root of the moving average gradient squared, so that's where we use it. Oh", "tokens": [413, 1843, 292, 538, 264, 3732, 5593, 295, 264, 2684, 4274, 16235, 8889, 11, 370, 300, 311, 689, 321, 764, 309, 13, 876], "temperature": 0.0, "avg_logprob": -0.27346794203956526, "compression_ratio": 1.6986899563318778, "no_speech_prob": 3.905458925146377e-06}, {"id": 1688, "seek": 785092, "start": 7850.92, "end": 7857.96, "text": " I'm sorry can you be a little more sure so D2 is the learning rate, which is one", "tokens": [286, 478, 2597, 393, 291, 312, 257, 707, 544, 988, 370, 413, 17, 307, 264, 2539, 3314, 11, 597, 307, 472], "temperature": 0.0, "avg_logprob": -0.2866045808138913, "compression_ratio": 1.3727810650887573, "no_speech_prob": 7.527922662120545e-06}, {"id": 1689, "seek": 785092, "start": 7858.4400000000005, "end": 7863.32, "text": " Yeah, M27 is our moving average of the square gradients", "tokens": [865, 11, 376, 10076, 307, 527, 2684, 4274, 295, 264, 3732, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.2866045808138913, "compression_ratio": 1.3727810650887573, "no_speech_prob": 7.527922662120545e-06}, {"id": 1690, "seek": 785092, "start": 7864.08, "end": 7868.08, "text": " So we just go D2 divided by square root M27", "tokens": [407, 321, 445, 352, 413, 17, 6666, 538, 3732, 5593, 376, 10076], "temperature": 0.0, "avg_logprob": -0.2866045808138913, "compression_ratio": 1.3727810650887573, "no_speech_prob": 7.527922662120545e-06}, {"id": 1691, "seek": 785092, "start": 7869.4400000000005, "end": 7871.4400000000005, "text": " That's it", "tokens": [663, 311, 309], "temperature": 0.0, "avg_logprob": -0.2866045808138913, "compression_ratio": 1.3727810650887573, "no_speech_prob": 7.527922662120545e-06}, {"id": 1692, "seek": 785092, "start": 7872.24, "end": 7874.24, "text": " Okay, thanks I", "tokens": [1033, 11, 3231, 286], "temperature": 0.0, "avg_logprob": -0.2866045808138913, "compression_ratio": 1.3727810650887573, "no_speech_prob": 7.527922662120545e-06}, {"id": 1693, "seek": 785092, "start": 7875.16, "end": 7877.4, "text": " Have one question yeah, so", "tokens": [3560, 472, 1168, 1338, 11, 370], "temperature": 0.0, "avg_logprob": -0.2866045808138913, "compression_ratio": 1.3727810650887573, "no_speech_prob": 7.527922662120545e-06}, {"id": 1694, "seek": 787740, "start": 7877.4, "end": 7885.08, "text": " The new method that you just mentioned which is in the process of getting implemented in yes Adam W Adam W", "tokens": [440, 777, 3170, 300, 291, 445, 2835, 597, 307, 294, 264, 1399, 295, 1242, 12270, 294, 2086, 7938, 343, 7938, 343], "temperature": 0.0, "avg_logprob": -0.22892420942133124, "compression_ratio": 1.5779816513761469, "no_speech_prob": 7.411160822812235e-06}, {"id": 1695, "seek": 787740, "start": 7886.32, "end": 7891.12, "text": " How different is it from here? Okay? I can let's do that so", "tokens": [1012, 819, 307, 309, 490, 510, 30, 1033, 30, 286, 393, 718, 311, 360, 300, 370], "temperature": 0.0, "avg_logprob": -0.22892420942133124, "compression_ratio": 1.5779816513761469, "no_speech_prob": 7.411160822812235e-06}, {"id": 1696, "seek": 787740, "start": 7892.5599999999995, "end": 7895.28, "text": " To understand Adam W. We have to understand weight decay", "tokens": [1407, 1223, 7938, 343, 13, 492, 362, 281, 1223, 3364, 21039], "temperature": 0.0, "avg_logprob": -0.22892420942133124, "compression_ratio": 1.5779816513761469, "no_speech_prob": 7.411160822812235e-06}, {"id": 1697, "seek": 787740, "start": 7896.16, "end": 7902.04, "text": " And maybe we'll learn more about that later. Let's see how we go now with weight decay so the idea is that", "tokens": [400, 1310, 321, 603, 1466, 544, 466, 300, 1780, 13, 961, 311, 536, 577, 321, 352, 586, 365, 3364, 21039, 370, 264, 1558, 307, 300], "temperature": 0.0, "avg_logprob": -0.22892420942133124, "compression_ratio": 1.5779816513761469, "no_speech_prob": 7.411160822812235e-06}, {"id": 1698, "seek": 787740, "start": 7903.12, "end": 7905.12, "text": " when you have", "tokens": [562, 291, 362], "temperature": 0.0, "avg_logprob": -0.22892420942133124, "compression_ratio": 1.5779816513761469, "no_speech_prob": 7.411160822812235e-06}, {"id": 1699, "seek": 790512, "start": 7905.12, "end": 7910.599999999999, "text": " Lots and lots of parameters like we do with you know most of the neural nets we train", "tokens": [15908, 293, 3195, 295, 9834, 411, 321, 360, 365, 291, 458, 881, 295, 264, 18161, 36170, 321, 3847], "temperature": 0.0, "avg_logprob": -0.15508193969726564, "compression_ratio": 1.6170212765957446, "no_speech_prob": 1.2679245173785603e-06}, {"id": 1700, "seek": 790512, "start": 7911.76, "end": 7917.96, "text": " You very often have like more parameters and data points, or you know like regularization becomes important", "tokens": [509, 588, 2049, 362, 411, 544, 9834, 293, 1412, 2793, 11, 420, 291, 458, 411, 3890, 2144, 3643, 1021], "temperature": 0.0, "avg_logprob": -0.15508193969726564, "compression_ratio": 1.6170212765957446, "no_speech_prob": 1.2679245173785603e-06}, {"id": 1701, "seek": 790512, "start": 7917.96, "end": 7925.98, "text": " And we've learned how to avoid over fitting by using dropout right which randomly deletes some activations", "tokens": [400, 321, 600, 3264, 577, 281, 5042, 670, 15669, 538, 1228, 3270, 346, 558, 597, 16979, 1103, 37996, 512, 2430, 763], "temperature": 0.0, "avg_logprob": -0.15508193969726564, "compression_ratio": 1.6170212765957446, "no_speech_prob": 1.2679245173785603e-06}, {"id": 1702, "seek": 790512, "start": 7926.5199999999995, "end": 7931.04, "text": " In the hope that it's going to learn some kind of more resilient set of weights", "tokens": [682, 264, 1454, 300, 309, 311, 516, 281, 1466, 512, 733, 295, 544, 23699, 992, 295, 17443], "temperature": 0.0, "avg_logprob": -0.15508193969726564, "compression_ratio": 1.6170212765957446, "no_speech_prob": 1.2679245173785603e-06}, {"id": 1703, "seek": 793104, "start": 7931.04, "end": 7937.76, "text": " There's another kind of regularization we can use called weight decay or L2 regularization", "tokens": [821, 311, 1071, 733, 295, 3890, 2144, 321, 393, 764, 1219, 3364, 21039, 420, 441, 17, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.19079923127826892, "compression_ratio": 1.85, "no_speech_prob": 1.2098623756173765e-06}, {"id": 1704, "seek": 793104, "start": 7937.76, "end": 7944.34, "text": " And it's actually comes kind of kind of classic statistical technique and the idea is that we take our loss function", "tokens": [400, 309, 311, 767, 1487, 733, 295, 733, 295, 7230, 22820, 6532, 293, 264, 1558, 307, 300, 321, 747, 527, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.19079923127826892, "compression_ratio": 1.85, "no_speech_prob": 1.2098623756173765e-06}, {"id": 1705, "seek": 793104, "start": 7944.68, "end": 7946.68, "text": " right, so we take our like", "tokens": [558, 11, 370, 321, 747, 527, 411], "temperature": 0.0, "avg_logprob": -0.19079923127826892, "compression_ratio": 1.85, "no_speech_prob": 1.2098623756173765e-06}, {"id": 1706, "seek": 793104, "start": 7947.24, "end": 7951.24, "text": " Error squared loss function, and we add an additional piece to it", "tokens": [3300, 2874, 8889, 4470, 2445, 11, 293, 321, 909, 364, 4497, 2522, 281, 309], "temperature": 0.0, "avg_logprob": -0.19079923127826892, "compression_ratio": 1.85, "no_speech_prob": 1.2098623756173765e-06}, {"id": 1707, "seek": 793104, "start": 7952.16, "end": 7954.16, "text": " Let's add weight decay right now", "tokens": [961, 311, 909, 3364, 21039, 558, 586], "temperature": 0.0, "avg_logprob": -0.19079923127826892, "compression_ratio": 1.85, "no_speech_prob": 1.2098623756173765e-06}, {"id": 1708, "seek": 795416, "start": 7954.16, "end": 7960.84, "text": " The additional piece we add is to basically add the square of the weights", "tokens": [440, 4497, 2522, 321, 909, 307, 281, 1936, 909, 264, 3732, 295, 264, 17443], "temperature": 0.0, "avg_logprob": -0.23998653477635876, "compression_ratio": 1.4264705882352942, "no_speech_prob": 8.186344757632469e-07}, {"id": 1709, "seek": 795416, "start": 7961.32, "end": 7963.32, "text": " So we'd say plus", "tokens": [407, 321, 1116, 584, 1804], "temperature": 0.0, "avg_logprob": -0.23998653477635876, "compression_ratio": 1.4264705882352942, "no_speech_prob": 8.186344757632469e-07}, {"id": 1710, "seek": 795416, "start": 7963.5599999999995, "end": 7965.5599999999995, "text": " B squared", "tokens": [363, 8889], "temperature": 0.0, "avg_logprob": -0.23998653477635876, "compression_ratio": 1.4264705882352942, "no_speech_prob": 8.186344757632469e-07}, {"id": 1711, "seek": 795416, "start": 7966.24, "end": 7968.599999999999, "text": " Plus a squared", "tokens": [7721, 257, 8889], "temperature": 0.0, "avg_logprob": -0.23998653477635876, "compression_ratio": 1.4264705882352942, "no_speech_prob": 8.186344757632469e-07}, {"id": 1712, "seek": 795416, "start": 7970.12, "end": 7971.599999999999, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.23998653477635876, "compression_ratio": 1.4264705882352942, "no_speech_prob": 8.186344757632469e-07}, {"id": 1713, "seek": 795416, "start": 7971.599999999999, "end": 7973.599999999999, "text": " That is now", "tokens": [663, 307, 586], "temperature": 0.0, "avg_logprob": -0.23998653477635876, "compression_ratio": 1.4264705882352942, "no_speech_prob": 8.186344757632469e-07}, {"id": 1714, "seek": 795416, "start": 7974.0, "end": 7979.2, "text": " Weight decay or L2 regularization and so the idea is that now", "tokens": [44464, 21039, 420, 441, 17, 3890, 2144, 293, 370, 264, 1558, 307, 300, 586], "temperature": 0.0, "avg_logprob": -0.23998653477635876, "compression_ratio": 1.4264705882352942, "no_speech_prob": 8.186344757632469e-07}, {"id": 1715, "seek": 797920, "start": 7979.2, "end": 7981.2, "text": " The", "tokens": [440], "temperature": 0.0, "avg_logprob": -0.26097316212124294, "compression_ratio": 1.689265536723164, "no_speech_prob": 1.5779585282871267e-06}, {"id": 1716, "seek": 797920, "start": 7982.599999999999, "end": 7985.76, "text": " The loss function wants to keep the weight small", "tokens": [440, 4470, 2445, 2738, 281, 1066, 264, 3364, 1359], "temperature": 0.0, "avg_logprob": -0.26097316212124294, "compression_ratio": 1.689265536723164, "no_speech_prob": 1.5779585282871267e-06}, {"id": 1717, "seek": 797920, "start": 7986.16, "end": 7989.44, "text": " right because increasing the weights makes the loss worse and", "tokens": [558, 570, 5662, 264, 17443, 1669, 264, 4470, 5324, 293], "temperature": 0.0, "avg_logprob": -0.26097316212124294, "compression_ratio": 1.689265536723164, "no_speech_prob": 1.5779585282871267e-06}, {"id": 1718, "seek": 797920, "start": 7990.08, "end": 7992.16, "text": " So it's only going to increase the weights", "tokens": [407, 309, 311, 787, 516, 281, 3488, 264, 17443], "temperature": 0.0, "avg_logprob": -0.26097316212124294, "compression_ratio": 1.689265536723164, "no_speech_prob": 1.5779585282871267e-06}, {"id": 1719, "seek": 797920, "start": 7992.76, "end": 7999.94, "text": " If the loss improves by more than the amount of that penalty and in fact to make this weight to catch a proper weight decay", "tokens": [759, 264, 4470, 24771, 538, 544, 813, 264, 2372, 295, 300, 16263, 293, 294, 1186, 281, 652, 341, 3364, 281, 3745, 257, 2296, 3364, 21039], "temperature": 0.0, "avg_logprob": -0.26097316212124294, "compression_ratio": 1.689265536723164, "no_speech_prob": 1.5779585282871267e-06}, {"id": 1720, "seek": 797920, "start": 7999.96, "end": 8001.96, "text": " We then need some", "tokens": [492, 550, 643, 512], "temperature": 0.0, "avg_logprob": -0.26097316212124294, "compression_ratio": 1.689265536723164, "no_speech_prob": 1.5779585282871267e-06}, {"id": 1721, "seek": 800196, "start": 8001.96, "end": 8009.8, "text": " Modifier here right so if you remember back in our here we said weight decay equals WD", "tokens": [6583, 9902, 510, 558, 370, 498, 291, 1604, 646, 294, 527, 510, 321, 848, 3364, 21039, 6915, 343, 35], "temperature": 0.0, "avg_logprob": -0.2797859615749783, "compression_ratio": 1.635, "no_speech_prob": 9.570800330038765e-07}, {"id": 1722, "seek": 800196, "start": 8010.24, "end": 8015.04, "text": " 5 e neg 4 okay, so to actually use the same weight decay. I would have to multiply by", "tokens": [1025, 308, 2485, 1017, 1392, 11, 370, 281, 767, 764, 264, 912, 3364, 21039, 13, 286, 576, 362, 281, 12972, 538], "temperature": 0.0, "avg_logprob": -0.2797859615749783, "compression_ratio": 1.635, "no_speech_prob": 9.570800330038765e-07}, {"id": 1723, "seek": 800196, "start": 8017.32, "end": 8018.76, "text": " 0.0005", "tokens": [1958, 13, 1360, 20], "temperature": 0.0, "avg_logprob": -0.2797859615749783, "compression_ratio": 1.635, "no_speech_prob": 9.570800330038765e-07}, {"id": 1724, "seek": 800196, "start": 8018.76, "end": 8022.88, "text": " All right, so that's actually now the same weight decay so", "tokens": [1057, 558, 11, 370, 300, 311, 767, 586, 264, 912, 3364, 21039, 370], "temperature": 0.0, "avg_logprob": -0.2797859615749783, "compression_ratio": 1.635, "no_speech_prob": 9.570800330038765e-07}, {"id": 1725, "seek": 800196, "start": 8025.16, "end": 8026.56, "text": " If", "tokens": [759], "temperature": 0.0, "avg_logprob": -0.2797859615749783, "compression_ratio": 1.635, "no_speech_prob": 9.570800330038765e-07}, {"id": 1726, "seek": 800196, "start": 8026.56, "end": 8030.52, "text": " You have a really high weight decay then it's going to set all the parameters to zero", "tokens": [509, 362, 257, 534, 1090, 3364, 21039, 550, 309, 311, 516, 281, 992, 439, 264, 9834, 281, 4018], "temperature": 0.0, "avg_logprob": -0.2797859615749783, "compression_ratio": 1.635, "no_speech_prob": 9.570800330038765e-07}, {"id": 1727, "seek": 803052, "start": 8030.52, "end": 8035.92, "text": " So it'll never over fit right because it can't set any parameter to anything", "tokens": [407, 309, 603, 1128, 670, 3318, 558, 570, 309, 393, 380, 992, 604, 13075, 281, 1340], "temperature": 0.0, "avg_logprob": -0.1749419963106196, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.0677011914594914e-06}, {"id": 1728, "seek": 803052, "start": 8035.92, "end": 8039.4800000000005, "text": " All right, and so as you gradually decrease the weight decay a", "tokens": [1057, 558, 11, 293, 370, 382, 291, 13145, 11514, 264, 3364, 21039, 257], "temperature": 0.0, "avg_logprob": -0.1749419963106196, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.0677011914594914e-06}, {"id": 1729, "seek": 803052, "start": 8040.160000000001, "end": 8046.92, "text": " Few more weights can actually be used right but the ones that don't help much. It's still going to leave it zero", "tokens": [33468, 544, 17443, 393, 767, 312, 1143, 558, 457, 264, 2306, 300, 500, 380, 854, 709, 13, 467, 311, 920, 516, 281, 1856, 309, 4018], "temperature": 0.0, "avg_logprob": -0.1749419963106196, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.0677011914594914e-06}, {"id": 1730, "seek": 803052, "start": 8047.4400000000005, "end": 8049.4400000000005, "text": " Or close to zero, right?", "tokens": [1610, 1998, 281, 4018, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1749419963106196, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.0677011914594914e-06}, {"id": 1731, "seek": 803052, "start": 8050.080000000001, "end": 8057.42, "text": " So that's what that's what weight decay is is literally to change the loss function to add in this", "tokens": [407, 300, 311, 437, 300, 311, 437, 3364, 21039, 307, 307, 3736, 281, 1319, 264, 4470, 2445, 281, 909, 294, 341], "temperature": 0.0, "avg_logprob": -0.1749419963106196, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.0677011914594914e-06}, {"id": 1732, "seek": 805742, "start": 8057.42, "end": 8060.1, "text": " sum of squares of weights", "tokens": [2408, 295, 19368, 295, 17443], "temperature": 0.0, "avg_logprob": -0.47291008201805323, "compression_ratio": 1.7005649717514124, "no_speech_prob": 7.934486347949132e-07}, {"id": 1733, "seek": 805742, "start": 8060.82, "end": 8061.82, "text": " times", "tokens": [1413], "temperature": 0.0, "avg_logprob": -0.47291008201805323, "compression_ratio": 1.7005649717514124, "no_speech_prob": 7.934486347949132e-07}, {"id": 1734, "seek": 805742, "start": 8061.82, "end": 8065.14, "text": " some parameter some some hyper parameter you should see", "tokens": [512, 13075, 512, 512, 9848, 13075, 291, 820, 536], "temperature": 0.0, "avg_logprob": -0.47291008201805323, "compression_ratio": 1.7005649717514124, "no_speech_prob": 7.934486347949132e-07}, {"id": 1735, "seek": 805742, "start": 8066.54, "end": 8072.38, "text": " The problem is that if you put that into the loss function as I have here", "tokens": [440, 1154, 307, 300, 498, 291, 829, 300, 666, 264, 4470, 2445, 382, 286, 362, 510], "temperature": 0.0, "avg_logprob": -0.47291008201805323, "compression_ratio": 1.7005649717514124, "no_speech_prob": 7.934486347949132e-07}, {"id": 1736, "seek": 805742, "start": 8073.26, "end": 8078.74, "text": " Then it ends up in the moving average of gradients and the moving average of squares of gradients", "tokens": [1396, 309, 5314, 493, 294, 264, 2684, 4274, 295, 2771, 2448, 293, 264, 2684, 4274, 295, 19368, 295, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.47291008201805323, "compression_ratio": 1.7005649717514124, "no_speech_prob": 7.934486347949132e-07}, {"id": 1737, "seek": 805742, "start": 8079.38, "end": 8083.5, "text": " For Adam right and so basically we end up", "tokens": [1171, 7938, 558, 293, 370, 1936, 321, 917, 493], "temperature": 0.0, "avg_logprob": -0.47291008201805323, "compression_ratio": 1.7005649717514124, "no_speech_prob": 7.934486347949132e-07}, {"id": 1738, "seek": 808350, "start": 8083.5, "end": 8087.54, "text": " When there's a lot of variation we end up", "tokens": [1133, 456, 311, 257, 688, 295, 12990, 321, 917, 493], "temperature": 0.0, "avg_logprob": -0.47491153917814555, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.1561211244479637e-07}, {"id": 1739, "seek": 808350, "start": 8088.54, "end": 8090.54, "text": " decreasing the amount of weight decay", "tokens": [23223, 264, 2372, 295, 3364, 21039], "temperature": 0.0, "avg_logprob": -0.47491153917814555, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.1561211244479637e-07}, {"id": 1740, "seek": 808350, "start": 8090.7, "end": 8096.78, "text": " And if there's very little variation we end up increasing the amount of weight decay so we end up basically saying", "tokens": [400, 498, 456, 311, 588, 707, 12990, 321, 917, 493, 5662, 264, 2372, 295, 3364, 21039, 370, 321, 917, 493, 1936, 1566], "temperature": 0.0, "avg_logprob": -0.47491153917814555, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.1561211244479637e-07}, {"id": 1741, "seek": 808350, "start": 8097.74, "end": 8101.42, "text": " penalize parameters, you know weights that are really high", "tokens": [13661, 1125, 9834, 11, 291, 458, 17443, 300, 366, 534, 1090], "temperature": 0.0, "avg_logprob": -0.47491153917814555, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.1561211244479637e-07}, {"id": 1742, "seek": 808350, "start": 8102.46, "end": 8107.38, "text": " Unless their gradient varies a lot which is never what we intended right?", "tokens": [16581, 641, 16235, 21716, 257, 688, 597, 307, 1128, 437, 321, 10226, 558, 30], "temperature": 0.0, "avg_logprob": -0.47491153917814555, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.1561211244479637e-07}, {"id": 1743, "seek": 808350, "start": 8107.38, "end": 8112.02, "text": " That's just not not the plan at all so the trick we're trying to do is", "tokens": [663, 311, 445, 406, 406, 264, 1393, 412, 439, 370, 264, 4282, 321, 434, 1382, 281, 360, 307], "temperature": 0.0, "avg_logprob": -0.47491153917814555, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.1561211244479637e-07}, {"id": 1744, "seek": 811202, "start": 8112.02, "end": 8116.240000000001, "text": " Well so the trick with Adam W is we basically remove", "tokens": [1042, 370, 264, 4282, 365, 7938, 343, 307, 321, 1936, 4159], "temperature": 0.0, "avg_logprob": -0.2566628102903013, "compression_ratio": 1.8347826086956522, "no_speech_prob": 4.888295279670274e-07}, {"id": 1745, "seek": 811202, "start": 8117.02, "end": 8119.02, "text": " Weight decay from here", "tokens": [44464, 21039, 490, 510], "temperature": 0.0, "avg_logprob": -0.2566628102903013, "compression_ratio": 1.8347826086956522, "no_speech_prob": 4.888295279670274e-07}, {"id": 1746, "seek": 811202, "start": 8119.06, "end": 8121.06, "text": " So it's not in the loss function", "tokens": [407, 309, 311, 406, 294, 264, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.2566628102903013, "compression_ratio": 1.8347826086956522, "no_speech_prob": 4.888295279670274e-07}, {"id": 1747, "seek": 811202, "start": 8121.06, "end": 8126.64, "text": " It's not in the G not in the G squared and we move it so that instead it", "tokens": [467, 311, 406, 294, 264, 460, 406, 294, 264, 460, 8889, 293, 321, 1286, 309, 370, 300, 2602, 309], "temperature": 0.0, "avg_logprob": -0.2566628102903013, "compression_ratio": 1.8347826086956522, "no_speech_prob": 4.888295279670274e-07}, {"id": 1748, "seek": 811202, "start": 8127.26, "end": 8129.64, "text": " It's added directly to the", "tokens": [467, 311, 3869, 3838, 281, 264], "temperature": 0.0, "avg_logprob": -0.2566628102903013, "compression_ratio": 1.8347826086956522, "no_speech_prob": 4.888295279670274e-07}, {"id": 1749, "seek": 811202, "start": 8130.3, "end": 8134.820000000001, "text": " When we update with the learning rate, it's added there instead so in other words it would be", "tokens": [1133, 321, 5623, 365, 264, 2539, 3314, 11, 309, 311, 3869, 456, 2602, 370, 294, 661, 2283, 309, 576, 312], "temperature": 0.0, "avg_logprob": -0.2566628102903013, "compression_ratio": 1.8347826086956522, "no_speech_prob": 4.888295279670274e-07}, {"id": 1750, "seek": 813482, "start": 8134.82, "end": 8141.759999999999, "text": " We would put the weight decay or actually the gradient with the weight decay in here when we calculate the new a and you be", "tokens": [492, 576, 829, 264, 3364, 21039, 420, 767, 264, 16235, 365, 264, 3364, 21039, 294, 510, 562, 321, 8873, 264, 777, 257, 293, 291, 312], "temperature": 0.0, "avg_logprob": -0.22763236509550602, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1544581184352865e-06}, {"id": 1751, "seek": 813482, "start": 8141.9, "end": 8145.98, "text": " So it never ends up in our G and G squared", "tokens": [407, 309, 1128, 5314, 493, 294, 527, 460, 293, 460, 8889], "temperature": 0.0, "avg_logprob": -0.22763236509550602, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1544581184352865e-06}, {"id": 1752, "seek": 813482, "start": 8146.66, "end": 8148.66, "text": " So that was like a super fast", "tokens": [407, 300, 390, 411, 257, 1687, 2370], "temperature": 0.0, "avg_logprob": -0.22763236509550602, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1544581184352865e-06}, {"id": 1753, "seek": 813482, "start": 8148.74, "end": 8154.86, "text": " Description which will probably only make sense if you listen to it three or four times on the video and then talk about it", "tokens": [3885, 12432, 597, 486, 1391, 787, 652, 2020, 498, 291, 2140, 281, 309, 1045, 420, 1451, 1413, 322, 264, 960, 293, 550, 751, 466, 309], "temperature": 0.0, "avg_logprob": -0.22763236509550602, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1544581184352865e-06}, {"id": 1754, "seek": 813482, "start": 8154.86, "end": 8156.86, "text": " on the forum", "tokens": [322, 264, 17542], "temperature": 0.0, "avg_logprob": -0.22763236509550602, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1544581184352865e-06}, {"id": 1755, "seek": 813482, "start": 8156.86, "end": 8163.66, "text": " Yeah, but if you're interested let me know and we can also look at an an's code that's implemented this", "tokens": [865, 11, 457, 498, 291, 434, 3102, 718, 385, 458, 293, 321, 393, 611, 574, 412, 364, 364, 311, 3089, 300, 311, 12270, 341], "temperature": 0.0, "avg_logprob": -0.22763236509550602, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1544581184352865e-06}, {"id": 1756, "seek": 816366, "start": 8163.66, "end": 8165.66, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.23720158321756712, "compression_ratio": 1.535294117647059, "no_speech_prob": 1.6536781686227187e-06}, {"id": 1757, "seek": 816366, "start": 8167.54, "end": 8171.98, "text": " You know the the idea of using weight decay is it's a really helpful", "tokens": [509, 458, 264, 264, 1558, 295, 1228, 3364, 21039, 307, 309, 311, 257, 534, 4961], "temperature": 0.0, "avg_logprob": -0.23720158321756712, "compression_ratio": 1.535294117647059, "no_speech_prob": 1.6536781686227187e-06}, {"id": 1758, "seek": 816366, "start": 8173.26, "end": 8175.0599999999995, "text": " regularizer", "tokens": [3890, 6545], "temperature": 0.0, "avg_logprob": -0.23720158321756712, "compression_ratio": 1.535294117647059, "no_speech_prob": 1.6536781686227187e-06}, {"id": 1759, "seek": 816366, "start": 8175.0599999999995, "end": 8177.94, "text": " Because it's basically this way that we can kind of say like", "tokens": [1436, 309, 311, 1936, 341, 636, 300, 321, 393, 733, 295, 584, 411], "temperature": 0.0, "avg_logprob": -0.23720158321756712, "compression_ratio": 1.535294117647059, "no_speech_prob": 1.6536781686227187e-06}, {"id": 1760, "seek": 816366, "start": 8179.7, "end": 8181.86, "text": " You know please don't", "tokens": [509, 458, 1767, 500, 380], "temperature": 0.0, "avg_logprob": -0.23720158321756712, "compression_ratio": 1.535294117647059, "no_speech_prob": 1.6536781686227187e-06}, {"id": 1761, "seek": 816366, "start": 8182.7, "end": 8188.26, "text": " Increase any of the weight values unless the you know improvement in the loss", "tokens": [30367, 651, 604, 295, 264, 3364, 4190, 5969, 264, 291, 458, 10444, 294, 264, 4470], "temperature": 0.0, "avg_logprob": -0.23720158321756712, "compression_ratio": 1.535294117647059, "no_speech_prob": 1.6536781686227187e-06}, {"id": 1762, "seek": 816366, "start": 8189.7, "end": 8191.7, "text": " Is worth it and", "tokens": [1119, 3163, 309, 293], "temperature": 0.0, "avg_logprob": -0.23720158321756712, "compression_ratio": 1.535294117647059, "no_speech_prob": 1.6536781686227187e-06}, {"id": 1763, "seek": 819170, "start": 8191.7, "end": 8198.26, "text": " And so generally speaking pretty much all state-of-the-art models have both dropout and weight decay", "tokens": [400, 370, 5101, 4124, 1238, 709, 439, 1785, 12, 2670, 12, 3322, 12, 446, 5245, 362, 1293, 3270, 346, 293, 3364, 21039], "temperature": 0.0, "avg_logprob": -0.200532314824123, "compression_ratio": 1.5934959349593496, "no_speech_prob": 2.7693831725628115e-06}, {"id": 1764, "seek": 819170, "start": 8199.3, "end": 8204.36, "text": " And I don't claim to know like how to set each one and how much of each to use", "tokens": [400, 286, 500, 380, 3932, 281, 458, 411, 577, 281, 992, 1184, 472, 293, 577, 709, 295, 1184, 281, 764], "temperature": 0.0, "avg_logprob": -0.200532314824123, "compression_ratio": 1.5934959349593496, "no_speech_prob": 2.7693831725628115e-06}, {"id": 1765, "seek": 819170, "start": 8204.9, "end": 8207.96, "text": " I've to say like you it's worth trying both", "tokens": [286, 600, 281, 584, 411, 291, 309, 311, 3163, 1382, 1293], "temperature": 0.0, "avg_logprob": -0.200532314824123, "compression_ratio": 1.5934959349593496, "no_speech_prob": 2.7693831725628115e-06}, {"id": 1766, "seek": 819170, "start": 8209.9, "end": 8217.22, "text": " To go back to the idea of embeddings is there any way to interpret the finals with user embeddings like absolutely", "tokens": [1407, 352, 646, 281, 264, 1558, 295, 12240, 29432, 307, 456, 604, 636, 281, 7302, 264, 25526, 365, 4195, 12240, 29432, 411, 3122], "temperature": 0.0, "avg_logprob": -0.200532314824123, "compression_ratio": 1.5934959349593496, "no_speech_prob": 2.7693831725628115e-06}, {"id": 1767, "seek": 819170, "start": 8217.22, "end": 8219.34, "text": " We're going to look at that next week. It's super fun", "tokens": [492, 434, 516, 281, 574, 412, 300, 958, 1243, 13, 467, 311, 1687, 1019], "temperature": 0.0, "avg_logprob": -0.200532314824123, "compression_ratio": 1.5934959349593496, "no_speech_prob": 2.7693831725628115e-06}, {"id": 1768, "seek": 821934, "start": 8219.34, "end": 8223.56, "text": " It turns out that you know we'll learn what some of the worst movies of all time", "tokens": [467, 4523, 484, 300, 291, 458, 321, 603, 1466, 437, 512, 295, 264, 5855, 6233, 295, 439, 565], "temperature": 0.0, "avg_logprob": -0.23870122809159128, "compression_ratio": 1.6217391304347826, "no_speech_prob": 2.406087332929019e-06}, {"id": 1769, "seek": 821934, "start": 8227.1, "end": 8231.82, "text": " It's like um it's a John Travolta Scientology ones like Battleship Earth or something", "tokens": [467, 311, 411, 1105, 309, 311, 257, 2619, 5403, 9646, 1328, 18944, 1793, 2306, 411, 29439, 904, 1210, 4755, 420, 746], "temperature": 0.0, "avg_logprob": -0.23870122809159128, "compression_ratio": 1.6217391304347826, "no_speech_prob": 2.406087332929019e-06}, {"id": 1770, "seek": 821934, "start": 8231.82, "end": 8234.62, "text": " I think that was like the worst movie of all time according to our embeddings", "tokens": [286, 519, 300, 390, 411, 264, 5855, 3169, 295, 439, 565, 4650, 281, 527, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.23870122809159128, "compression_ratio": 1.6217391304347826, "no_speech_prob": 2.406087332929019e-06}, {"id": 1771, "seek": 821934, "start": 8239.42, "end": 8241.68, "text": " Do you have any recommendations for scaling the", "tokens": [1144, 291, 362, 604, 10434, 337, 21589, 264], "temperature": 0.0, "avg_logprob": -0.23870122809159128, "compression_ratio": 1.6217391304347826, "no_speech_prob": 2.406087332929019e-06}, {"id": 1772, "seek": 821934, "start": 8242.66, "end": 8247.26, "text": " L2 penalty or is that kind of based on how how wide the nodes or how many notes?", "tokens": [441, 17, 16263, 420, 307, 300, 733, 295, 2361, 322, 577, 577, 4874, 264, 13891, 420, 577, 867, 5570, 30], "temperature": 0.0, "avg_logprob": -0.23870122809159128, "compression_ratio": 1.6217391304347826, "no_speech_prob": 2.406087332929019e-06}, {"id": 1773, "seek": 824726, "start": 8247.26, "end": 8252.66, "text": " No, I have no suggestion at all like I I kind of look for like", "tokens": [883, 11, 286, 362, 572, 16541, 412, 439, 411, 286, 286, 733, 295, 574, 337, 411], "temperature": 0.0, "avg_logprob": -0.32575001436121326, "compression_ratio": 1.5251141552511416, "no_speech_prob": 2.857300160030718e-06}, {"id": 1774, "seek": 824726, "start": 8253.1, "end": 8258.94, "text": " Papers or cackle competitions or whatever similar and try to set it frankly the same it seems like", "tokens": [430, 14441, 420, 269, 501, 306, 26185, 420, 2035, 2531, 293, 853, 281, 992, 309, 11939, 264, 912, 309, 2544, 411], "temperature": 0.0, "avg_logprob": -0.32575001436121326, "compression_ratio": 1.5251141552511416, "no_speech_prob": 2.857300160030718e-06}, {"id": 1775, "seek": 824726, "start": 8259.66, "end": 8264.1, "text": " in a particular area like computer vision object recognition", "tokens": [294, 257, 1729, 1859, 411, 3820, 5201, 2657, 11150], "temperature": 0.0, "avg_logprob": -0.32575001436121326, "compression_ratio": 1.5251141552511416, "no_speech_prob": 2.857300160030718e-06}, {"id": 1776, "seek": 824726, "start": 8264.66, "end": 8269.62, "text": " It's like somewhere between 1 in neck 4 or 1 in neck 5 seems to work. You know", "tokens": [467, 311, 411, 4079, 1296, 502, 294, 6189, 1017, 420, 502, 294, 6189, 1025, 2544, 281, 589, 13, 509, 458], "temperature": 0.0, "avg_logprob": -0.32575001436121326, "compression_ratio": 1.5251141552511416, "no_speech_prob": 2.857300160030718e-06}, {"id": 1777, "seek": 824726, "start": 8270.7, "end": 8273.44, "text": " actually in the Adam W paper the", "tokens": [767, 294, 264, 7938, 343, 3035, 264], "temperature": 0.0, "avg_logprob": -0.32575001436121326, "compression_ratio": 1.5251141552511416, "no_speech_prob": 2.857300160030718e-06}, {"id": 1778, "seek": 827344, "start": 8273.44, "end": 8280.36, "text": " the authors point out that with this new approach it actually becomes like it seems to be much more stable as to what the", "tokens": [264, 16552, 935, 484, 300, 365, 341, 777, 3109, 309, 767, 3643, 411, 309, 2544, 281, 312, 709, 544, 8351, 382, 281, 437, 264], "temperature": 0.0, "avg_logprob": -0.20297356041110293, "compression_ratio": 1.6055776892430278, "no_speech_prob": 3.0894675546733197e-06}, {"id": 1779, "seek": 827344, "start": 8280.36, "end": 8285.04, "text": " Right weight decay amounts are so hopefully now when we start playing with it. We'll be able to have some", "tokens": [1779, 3364, 21039, 11663, 366, 370, 4696, 586, 562, 321, 722, 2433, 365, 309, 13, 492, 603, 312, 1075, 281, 362, 512], "temperature": 0.0, "avg_logprob": -0.20297356041110293, "compression_ratio": 1.6055776892430278, "no_speech_prob": 3.0894675546733197e-06}, {"id": 1780, "seek": 827344, "start": 8285.84, "end": 8288.12, "text": " definitive recommendations by the time we get to part 2", "tokens": [28152, 10434, 538, 264, 565, 321, 483, 281, 644, 568], "temperature": 0.0, "avg_logprob": -0.20297356041110293, "compression_ratio": 1.6055776892430278, "no_speech_prob": 3.0894675546733197e-06}, {"id": 1781, "seek": 827344, "start": 8289.52, "end": 8291.52, "text": " Alright well, that's 9 o'clock so", "tokens": [2798, 731, 11, 300, 311, 1722, 277, 6, 9023, 370], "temperature": 0.0, "avg_logprob": -0.20297356041110293, "compression_ratio": 1.6055776892430278, "no_speech_prob": 3.0894675546733197e-06}, {"id": 1782, "seek": 827344, "start": 8292.36, "end": 8294.36, "text": " this week", "tokens": [341, 1243], "temperature": 0.0, "avg_logprob": -0.20297356041110293, "compression_ratio": 1.6055776892430278, "no_speech_prob": 3.0894675546733197e-06}, {"id": 1783, "seek": 827344, "start": 8294.44, "end": 8298.640000000001, "text": " You know practice the thing that you're least familiar with so if it's like", "tokens": [509, 458, 3124, 264, 551, 300, 291, 434, 1935, 4963, 365, 370, 498, 309, 311, 411], "temperature": 0.0, "avg_logprob": -0.20297356041110293, "compression_ratio": 1.6055776892430278, "no_speech_prob": 3.0894675546733197e-06}, {"id": 1784, "seek": 829864, "start": 8298.64, "end": 8304.32, "text": " Jacobians and Hessians read about those if it's broadcasting read about those if it's understanding Python", "tokens": [14117, 2567, 293, 35960, 2567, 1401, 466, 729, 498, 309, 311, 30024, 1401, 466, 729, 498, 309, 311, 3701, 15329], "temperature": 0.0, "avg_logprob": -0.19708912993130617, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.0580037016770802e-06}, {"id": 1785, "seek": 829864, "start": 8304.32, "end": 8307.599999999999, "text": " Oh, oh read about that. You know try and implement your own custom layers", "tokens": [876, 11, 1954, 1401, 466, 300, 13, 509, 458, 853, 293, 4445, 428, 1065, 2375, 7914], "temperature": 0.0, "avg_logprob": -0.19708912993130617, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.0580037016770802e-06}, {"id": 1786, "seek": 829864, "start": 8308.16, "end": 8313.88, "text": " Read the fast AI layers. You know and and talk on the forum about anything that you find", "tokens": [17604, 264, 2370, 7318, 7914, 13, 509, 458, 293, 293, 751, 322, 264, 17542, 466, 1340, 300, 291, 915], "temperature": 0.0, "avg_logprob": -0.19708912993130617, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.0580037016770802e-06}, {"id": 1787, "seek": 831388, "start": 8313.88, "end": 8328.439999999999, "text": " Weird or confusing alright. See you next week", "tokens": [50364, 32033, 420, 13181, 5845, 13, 3008, 291, 958, 1243, 51092], "temperature": 0.0, "avg_logprob": -0.42048899332682294, "compression_ratio": 0.8490566037735849, "no_speech_prob": 4.637590336642461e-06}], "language": "en"}