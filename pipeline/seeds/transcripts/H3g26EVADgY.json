{"text": " Last class of part 1, I guess the theme of part 1 is classification and regression with deep learning. And specifically it's about identifying and learning the best practices classification and regression. And we started out with the kind of here are 3 lines of code to do image classification. And gradually we've been, well the first 4 lessons were then kind of going through NLP structured data, collaborative filtering and kind of understanding some of the key pieces and most importantly understanding how to actually make these things work well in practice. And then the last 3 lessons are then kind of going back over all of those topics in kind of reverse order to understand more detail about what was going on and understanding what the code looks like behind the scenes and learning to write them from scratch. Part 2 of the course will move from a focus on classification and regression, which is kind of predicting a thing like a number or at most a small number of things like a small number of labels. And we'll focus more on generative modeling. Generative modeling means predicting kind of lots of things. For example, creating a sentence such as in neural translation or image captioning or question answering or creating an image such as in style transfer, super resolution, segmentation and so forth. And then in part 2, it'll move away from being just here are some best practices, established best practices either through people that have written papers or through research that Fast AI has done and kind of got convinced that these are best practices to some stuff which will be a little bit more speculative. Some stuff which is maybe recent papers that haven't been fully tested yet and sometimes in part 2 papers will come out in the middle of the course and we'll change direction with the course and study that paper because it's just interesting. And so if you're interested in kind of learning a bit more about how to read a paper and how to implement it from scratch and so forth, then that's another good reason to do part 2. It still doesn't assume any particular math background, beyond kind of high school, but it does assume that you're prepared to spend time digging through the notation and understanding it and converting it to code and so forth. So where we're up to is RNNs at the moment. I think one of the issues I find most with teaching RNNs is trying to ensure that people understand they're not in any way different or unusual or magical. They're just a standard, fully connected network. And so let's go back to the standard, fully connected network which looks like this. So to remind you, the arrows represent one or more layer operations, generally speaking a linear followed by a non-linear function. In this case they're matrix multiplications followed by ReLU or THAM. And the arrows of the same color represent exactly the same weight matrix being used. And so one thing which was just slightly different from previous fully connected networks we've seen is that we have an input coming in not just at the first layer, but also at the second layer and also at the third layer. And we tried a couple of approaches. One was concatenating the inputs and one was adding the inputs. But there was nothing at all conceptually different about this. So that code looked like this. We had a model where we basically defined the 3 arrows colors we had as 3 different weight matrices. And by using the linear class we got actually both the weight matrix and the bias vector wrapped up for free for us. And then we went through and we did each of our embeddings, put it through our first linear layer, and then we did each of our, we call them, hidden, I think they were orange arrows. And in order to avoid the fact that there's no orange arrow coming into the first one, we decided to kind of invent an empty matrix and that way every one of these rows looked the same. And so then we did exactly the same thing except we used a loop just to refactor the code. It was just a code refactoring. There was no change of anything conceptually. And since we were doing a refactoring, we took advantage of that to increase the number of characters to 8 because I was too lazy to type 8 linear layers, but I'm quite happy to change the loop index to 8. So this now loops through this exact same thing, but we had 8 of these rather than 3. So then we refactored that again by taking advantage of nn.rnn, which basically puts that loop together for us and keeps track of this h as it goes along for us. And so by using that, we were able to replace the loop with a single colon. And so again, that's just a refactoring doing exactly the same thing. So then we looked at something which was mainly designed to save some training time, which was previously, if we had a big piece of text, so we've got like a movie review, we were basically splitting it up into 8-character segments and we'd grab segment number 1 and use that to predict the next character. But in order to make sure that we used all of the data, we didn't just split it up like that. We actually said, here's our whole thing, let's grab, the first will be to grab this section, the second will be to grab that section, then that section, then that section, and each time we're predicting the next one character along. And so I was a bit concerned that that seems pretty wasteful because as we calculate this section, nearly all of it overlaps with the previous section. So instead, what we did was we said, what if we actually did split it into non-overlapping pieces, and we said let's grab this section here and use it to predict every one of the characters one along. And then let's grab this section here and use it to predict every one of the characters one along. So after we look at the first character in, we try to predict the second character. And then after we look at the second character, we try to predict the third character, and so forth. So that's where we got to. And then one of you perceptive folks asked a really interesting question or expressed a concern, which was, hey, after we got through the first point here, after we got through the first point here, we kind of threw away our H activations and started a new one, which meant that when it was trying to use character one to predict character two, it's got nothing to go on. It's only done one linear layer, and so that seems like a problem, which indeed it is. So we're going to do the obvious thing, which is let's not throw away H. So let's not throw away that matrix at all. So in code, the big problem is here. Every time we call forward, so in other words, every time we do a new minibatch, we're creating our hidden state, which remember is the orange circles. We're resetting it back to a bunch of zeros. And so as we go to the next non-overlapping section, we're saying, forget everything that's come before. But in fact, the whole point is we know exactly where we are. We're at the end of the previous section and about to start the next contiguous section, so let's not throw it away. So instead, the idea would be to cut this out, move it up to here, store it away in self, and then kind of keep updating it. So we're going to do that. And there's going to be some minor details to get right. So let's start by looking at the model. So here's the model. It's nearly identical. Here's the model. It's nearly identical, but I've got as expected one more line in my constructor where I call something called init hidden. And as expected, init hidden sets self.h to be a bunch of zeros. So that's entirely unsurprising. And then as you can see, RNN now takes in self.h and it as before spits out our new hidden activations. And so now the trick is to now store that away inside self.h. And so here's wrinkle number one. If you think about it, if I was to simply do it like that, and now I train this on a document that's a million characters long, then the size of this unrolled RNN has a million zeros. And so that's fine going forwards. But when I finally get to the end and I say here's my character, and actually remember we're doing multi-output now. So multi-output looks like this. Or if we were to draw the unrolled version of multi-output, we would have a triangle coming off at every point. So the problem is that then when we do backpropagation, we're calculating how much does the error at character 1 impact the final answer. How much does the error at character 2 impact the final answer, and so forth. And so we need to go back through and say how do we have to update our weights based on all of those errors. And so if there are a million characters, my unrolled RNN is a million layers long, I have a 1 million layer fully connected network. And I didn't have to write the million layers because I have the for loop, and the for loop is hidden away behind the self.rn, but it's still there. So this is actually a 1 million layer fully connected network. And so the problem with that is it's going to be very memory intensive because in order to do the chain rule, I have to be able to multiply at every step like f-u times g-x. So I have to remember those values u, the value of every set of layers, so I'm going to have to remember all those million layers. And I'm going to have to do a million multiplications, and I'm going to have to do that every batch. So that would be bad. So to avoid that, we basically say, from time to time, I want you to forget your history. So we can still remember the state, which is to remember what's the actual values in our hidden matrix, but we can remember the state without remembering everything about how we got there. So there's a little function called repackage variable, which literally is just this. It just simply says grab the tensor out of it, because remember the tensor itself doesn't have any concept of history, and create a new variable out of that. And so this variable is going to have the same value, but no history of operations, and therefore when it tries to backpropagate, it'll stop there. So basically what we're going to do then is we're going to call this in our forward. So that means it's going to do 8 characters, it's going to backpropagate through 8 layers, it's going to keep track of the actual values in our hidden state, but it's going to throw away at the end of those 8 its history of operations. So this approach is called backprop through time. When you read about it online, people make it sound like a different algorithm or some big insight or something, but it's not at all. It's just saying, hey, after our for loop, just throw away your history of operations and start afresh. So we're keeping our hidden state, but we're not keeping our hidden state's history. So that's wrinkle number 1, that's what this repackage bar is doing. So when you see bptt, that's referring to backprop through time, and you might remember we saw that in our original RNN lesson. We had a variable called bptt equals 70. So when we set that, they're actually saying how many layers to backprop through. Another good reason not to backprop through too many layers is if you have any kind of gradient instability like gradient explosion or gradient spanishing, the more layers you have, the harder the network gets to train. So slower and less resilient. On the other hand, a longer value for bptt means that you're able to explicitly capture a longer memory, more state. So that's something that you get to tune when you create your RNN. Wrinkle number 2 is how are we going to put the data into this. It's all very well the way I described it just now where we said we could do this, then we can first of all look at this section, then this section, then this section. But we want to do a mini-batch at a time. We want to do a bunch at a time. So in other words, we want to say let's do it like this. So mini-batch number 1 would say let's look at this section and predict that section. And at the same time in parallel, let's look at this totally different section and predict this. And at the same time in parallel, let's look at this totally different section and predict this. And so then, because remember in our hidden state, we have a vector of hidden state for everything in our mini-batch. So it's going to keep track of at the end of this there's going to be a vector here, a vector here, a vector here. And then we can move across to the next one and say, for this part of the mini-batch, use this to predict that, and use this to predict that, and use this to predict that. So you can see that we're moving, we've got like a number of totally separate bits of our text that we're moving through in parallel. So hopefully this is going to ring a few bells for you, because what happened was, back when we started looking at TorchText for the first time, we started talking about how it creates these mini-batches. And I said what happened was we took our whole big long document consisting of like the entire works of Nietzsche, or all of the IMDB reviews concatenated together, or whatever. And a lot of you, not surprisingly, this is really weird at first, a lot of you didn't quite hear what I said correctly. What I said was, we split this into 64 equal-sized chunks. And a lot of your brains went, Jeremy just said we split this into chunks of size 64. But that's not what Jeremy said. Jeremy said we split it into 64 equal-sized chunks. So if this whole thing was length 64 million, which would be a reasonable-sized corpus, not an unusual-sized corpus, then each of our 64 chunks would have been of length 1 million. And so then what we did was we took the first chunk of 1 million and we put it here. And then we took the second chunk of 1 million and we put it here. The third chunk of 1 million and we put it here. And so forth to create 64 chunks. And then each mini-batch consisted of us going, let's split this down here and here and here. And each of these is of size B, P, T, T, which I think we had something like 70. And so what happened was we said, alright, let's look at our first mini-batch as all of these. So we do all of those at once and predict everything offset by 1. And then at the end of that first mini-batch, we went to the second chunk and used each one of these to predict the next one offset by 1. So that's why we did that slightly weird thing, is that we wanted to have a bunch of things we can look through in parallel, each of which hopefully are far enough away from each other that we don't have to worry about the fact that the truth is the start of this million characters was actually in the middle of a sentence. But who cares, it only happens once every million characters. Question from the audience. I was wondering if you could talk a little bit more about augmentation for this kind of dataset. Data augmentation for this kind of dataset? No I can't because I don't really know a good way. It's one of the things I'm going to be studying between now and part 2. There have been some recent developments, particularly something we talked about in the machine learning course and I think we briefly mentioned here, which was somebody for a recent Kaggle competition won it by doing data augmentation by randomly inserting parts of different rows basically. Something like that may be useful here and I've seen some papers that do something like that. But I haven't seen any recent-ish state-of-the-art NLP papers that are doing this kind of data augmentation. So it's something we're planning to work on. How do you choose a BPTT? So there's a couple of things to think about when you pick your BPTT. The first is that you'll note that the matrix size for a mini-batch has a BPTT by batch size. So one issue is your GPU RAM needs to be able to fit that by your embedding matrix. Every one of these is going to be of length embedding length plus all of the hidden state. So one thing is if you get a CUDA out of memory error, you need to reduce one of those. If you're finding your training is very unstable, like your loss is shooting off to NAN suddenly, then you could try decreasing your BPTT because you've got less layers to gradient explode through. If it's too slow, you could try decreasing your BPTT because it's got to do one of those steps at a time. Like that for loop can't be parallelized. I say that there's a recent thing called QRNN, which we'll hopefully talk about in part 2, which kind of does paralyze it, but the versions we're looking at don't paralyze it. So those would be the main issues, look at performance, look at memory, look at stability, and try and find a number that's as high as you can make it, but all of those things work for you. So trying to get all that chunking and lining up and everything to work is more code than I want to write. So for this section, we're going to go back and use TorchText again. So when you're using APIs like FastAI and TorchText, which in this case these two APIs are designed to work together, you often have a choice which is like, this API has a number of methods that expect the data in this kind of format, and you can either change your data to fit that format, or you can write your own data set subclass to handle the format that your data is already in. I've noticed on the forum, a lot of you are spending a lot of time writing your own data set classes, whereas I am way lazier than you, and I spend my time instead changing my data to fit the data set classes I have. Either is fine, and if you realize there's a kind of a format of data that me and other people are likely to be seeing quite often and it's not in the FastAI library, then by all means, write the data set subclass, submit it as a PR and then everybody can benefit. But in this case, I just kind of thought I want to have some niche data fed into TorchText. I'm just going to put it in the format that TorchText already supports. So TorchText already has, or at least the FastAI wrapper around TorchText, already has something where you can have a training path and a validation path and one or more text files in each path containing a bunch of stuff that's concatenated together for your language model. So in this case, all I did was I made a copy of my Nietzsche file, copied it into training, made another copy, stuck it into the validation, and then in one of them, in the training set, I deleted the last 20% of rows, and in the validation set, I deleted all except for the last 20% of rows. And I was done. So in this case, I found that easier than writing a custom data set class. The other benefit of doing it that way was that I felt like it was more realistic to have a validation set that wasn't a random shuffled set of rows of text, but was like a totally separate part of the corpus. Because I feel like in practice, you're very often going to be saying, I've got these books or these authors I'm learning from and then I want to apply it to these different books and these different authors. So I felt like for getting a more realistic validation of my Nietzsche model, I should use a whole separate piece of the text. So in this case, it was the last 20% of the rows of the corpus. So I haven't created this for you intentionally, because this is the kind of stuff I want you practicing, is making sure that you're familiar enough, comfortable enough with bash or whatever that you can create these and that you understand what they need to look like and so forth. So in this case, you can see I've now got a train and a validation here. So you can see I've literally just got one file in it. Because when you're doing a language model, i.e. predicting the next character or predicting the next word, you don't really need separate files. It's fine if you do have separate files, but they just get concatenated together anyway. So that's my source data. And so here is the same lines of code that we've seen before. Let's go over them again, because it's a couple of lessons ago. So in TorchText, we create this thing called a field. And a field initially is just a description of how to go about preprocessing the text. In this case, I'm going to say, lowercase it. Now I think about it, there's no particular reason to have done this lowercase. Uppercase would work fine too. And then how do I tokenize it? And so you might remember last time we used a tokenization function which largely spit on whitespace and tried to do clever things with punctuation. And that gave us the word model. In this case, I want a character model, so I actually want every character put into a separate token. So I could just use the function list in Python, because list in Python does that. So this is where you can kind of see understanding how libraries like TorchText and FastAI are designed to be extended can make your life a lot easier. So when you realize that very often both of these libraries kind of expect you to pass a function that does something, and then you realize, oh I can write any function I like. So this is now going to mean that each mini-batch is going to contain a list of characters. And so here's where we get to define all our different parameters. And so to make it the same as previous sections of this notebook, I'm going to use the same batch size, the same number of characters, I'm now going to rename it to be ptt since we know what that means, the size of the embedding, and the size of our hidden state. Remembering that size of our hidden state simply means, going all the way back to the start, and hidden simply means the size of the state that's created by each of those orange arrows. So it's the size of each of those circles here. So having done that, we can then create a little dictionary saying what's our training, validation and test set. In this case I don't have a separate test set, so I'll just use the same thing. And then I can say, alright I want a language model data subclass of model data, I'm going to grab it from text files, and this is my path, and this is my field, which I defined earlier, and these are my files, and these are my hyperparameters. Infrac's not going to do anything actually in this case because I don't think there's going to be any character that appears less than 3 times. So that's probably redundant. So at the end of that, it says there's going to be 963 batches to go through. And so if you think about it, that should be equal to the number of tokens divided by the batch size divided by BPTT, because that's like the size of each of those rectangles. You'll find that in practice it's not exactly that. And the reason it's not exactly that is that the authors of Torch Text did something pretty smart, which I think we briefly mentioned this before. They said, we can't shuffle the data, like with images we like to shuffle the order, so every time we see them in a different order, so a bit more randomness. We can't shuffle because we need to be contiguous. But what we could do is randomize the length, basically randomize BPTT a little bit each time. And so that's what PyTorch does. It's not always going to give us exactly 8 characters long. 5% of the time it will actually cut it in half, and then it's going to add on a small little standard deviation to make it slightly bigger or smaller than 4 or 8. So it's going to be slightly different to 8 on average. Question asked, Is it going to be constant per minibatch? Yes, exactly, that's right. So a minibatch has to kind of do a matrix multiplication. And the minibatch size has to remain constant because we've got this h weight matrix that has to line up in size with the size of the minibatch. But the number of the sequence length can change, no problem. So that's why we have 963. So the length of a data loader is how many minibatches. In this case it's a little bit approximate. Number of tokens is how many unique things are in the vocabulary. And remember, after we run this line, text now does not just contain a description of what we want, but it also contains an extra attribute called vocab, which contains stuff like a list of all of the unique items in the vocabulary and a reverse mapping from each item to its number. So that text object is now an important thing to keep track of. So let's now try this. So we started out by looking at the class. So the class is exactly the same as the class we've had before. The only key difference is to call init hidden, which sets out. So h is not a variable anymore, it's now an attribute. self.h is a variable containing a bunch of zeros. Now I mentioned that batch size remains constant each time, but unfortunately when I said that I lied to you. And the way that I lied to you is that the very last minibatch will be shorter. The very last minibatch is actually going to have less than 64. It might be exactly the right size if it so happens that this dataset is exactly divisible by bptt times batch size, but it probably isn't. So the last batch will probably have a little bit less. And so that's why I do a little check here that says let's check that the batch size inside self.h. And so self.h is going to be the height, the height is going to be the number of activations and the width is going to be the minibatch size. Check that that's equal to the actual batch size length that we've received. And if they're not the same, then set it back to zeros again. So this is just a minor little wrinkle that basically at the end of each epoch, it's going to do like a little mini-minibatch. And so then as soon as it starts the next epoch, it's going to see that they're not the same again and it'll reinitialize it to the correct full batch size. So that's why if you're wondering, there's an init hidden not just in the constructor but also inside forward, it's to handle this end of each epoch, start of each epoch difference. Not an important point by any means, but potentially confusing when you see it. So the last wrinkle. The last wrinkle is something which I think is something that slightly sucks about PyTorch and maybe somebody can be nice enough to try and fix it with a PR if anybody feels like it, which is that the loss functions such as softmax are not happy receiving a rank 3 tensor. Remember a rank 3 tensor is just another way of saying a dimension 3 array. There's no particular reason they ought to not be happy receiving a rank 3 tensor, like somebody could write some code to say a rank 3 tensor is probably a sequence length by batch size by results thing, and so you should just do it for each of the two initial axes. But no one's done that, so it expects it to be a rank 2 tensor. Funnily enough, it can handle rank 2 or rank 4, but not rank 3. So we've got a rank 2 tensor containing for each time period, I can't remember which way around the axes are, but for each time period, for each batch, we've got our predictions. And then we've got our actuals for each time period, for each batch, we've got our predictions, and we've got our actuals. And so we just want to check whether they're the same. And so in an ideal world, a lost function would check item 1,1, then item 1,2, and then item 1,3, but since that hasn't been written, we just have to flatten them both out. We can literally just flatten them out, put rows to rows. And so that's why here I have to use.view. And so.view says the number of columns will be equal to the size of the vocab, because remember we're going to end up with a probability for each letter, and then the number of rows is however big is necessary, which will be equal to batch size times bptt. And then you may be wondering where I do that for the target, and the answer is TorchText knows that the target needs to look like that, so TorchText has already done that for us. So TorchText automatically changes the target to be flattened out. As you might actually remember, if you go back to lesson 4, when we actually looked at a mini-batch that spat out of TorchText, we noticed actually that it was flattened, and I said we'll learn about why later, and so later it has now arrived. So there are the 3 wrinkles. Get rid of the history, I guess 4 wrinkles. Recreate the hidden state if the batch size changes. Flatten out and then use TorchText to create mini-batches that line up nicely. So once we do those things, we can then create our model, create our optimizer with that model's parameters, and fit it. One thing to be careful of here is that Softmax now, as of PyTorch 0.3, requires that we pass in a number here saying which axis do we want to do the Softmax over. So at this point, this is a 3-dimensional tensor, so we want to do the Softmax over the final axis. So when I say which axis do we do the Softmax over, remember we divide by, so we go e to the xi divided by the sum of e to the xi, so it's saying which axis do we sum over. So which axis do we want to sum to 1. And so in this case, clearly we want to do it over the last axis because the last axis is the one that contains the probability per letter of the alphabet. And we want all of those probabilities to sum to 1. So therefore, to run this notebook, you're going to need PyTorch 0.3 which just came out this week. So if you're doing this on the MOOC, you're fine. I'm sure you've got at least 0.3 or later. Where else are the students here? If you just go conda env update, it will automatically update you to 0.3. The really great news is that 0.3, although it does not yet officially support Windows, it does in practice. I successfully installed 0.3 from conda yesterday by typing conda install PyTorch in Windows. I then attempted to use the entirety of lesson 1 and every single part worked. So I actually ran it on this very laptop. So for those who are interested in doing deep learning on their laptop, I can definitely recommend the new Surface Book. The new Surface Book 15-inch has a GTX 1060 6GB GPU in it. And I was getting it, it was running about 3 times slower than my 1080 Ti, which I think means it's about the same speed as an AWS P2 instance. And as you can see, it's also a nice convertible tablet that you can write on, and it's thin and light. I've never seen such a good deep learning box. Also I successfully installed Linux on it, and all of the fastai stuff worked on the Linux as well. So really good option if you're interested in a laptop that can run deep learning stuff. So that's something to be aware of with this tm=\"-1\". So then we can go ahead and construct this and we can call fit. We're basically going to get pretty similar results to what we got before. So then we can go a bit further with our RNN by just kind of unpacking it a bit more. And so this is now again exactly the same thing, gives exactly the same answers, but I have removed the call to RNN. So I've got rid of this self.rn. And so this is just something I won't spend time on it, but you can check it out. So instead I've now defined RNN as RNN cell, and I've copied and pasted the code above. Don't run it, this is just for your reference from PyTorch. This is the definition of RNN cell in PyTorch. And I want you to see that you can now read PyTorch source code and understand it. Not only that, you'll recognize it as being something we've done before. It's a matrix multiplication of the weights by the inputs plus biases. So F.linear simply does a matrix product followed by an addition. And interestingly you'll see they do not concatenate the input bit and the hidden bit. They sum them together, which is our first approach. As I said, you can do either, neither one's right or wrong, but it's interesting to see this is the definition here. Question. Can you give us some insight about what are they using that particular activation function? I think we might have briefly covered this last week, but very happy to do it again if I did. Basically, Than looks like that. So in other words, it's a sigmoid function, double the height, minus 1. It's a nice function in that it's forcing it to be no smaller than minus 1, no bigger than plus 1. And since we're multiplying by this weight matrix again and again and again and again, we might worry that a ReLU, because it's unbounded, might have more of a gradient explosion problem. That's basically the theory. Having said that, you can actually ask PyTorch for an RNN cell which uses a different non-linearity. So you can see by default it uses Than, but you can ask for a ReLU as well. But most people seem to pretty much everybody still seems to use Than as far as I can tell. So you can basically see here, this is all the same except now I've got an RNN cell, which means now I need to put my for loop back. And you can see every time I call my little linear function, I just append the result onto my list. And at the end, the result is that all stacked up together. So I'm just trying to show you how nothing inside PyTorch is mysterious. You should find you get basically the same answer from this as the previous one. In practice, you would never write it like this, but what you may well find in practice is that somebody will come up with a new kind of RNN cell or a different way of keeping track of things over time or a different way of doing regularization. And so inside FastAI's code, you will find that we do this exactly this basically. We have this by hand because we use some regularization approaches that aren't supported by PyTorch. So then another thing I'm not going to spend much time on, but I'll mention briefly, is that nobody really uses this RNN cell in practice. And the reason we don't use that RNN cell in practice is even though the THAN is here, you do tend to find gradient explosions are still a problem. And so we have to use pretty low learning rates to get these to train and pretty small values for BPTT to get them to train. So what we do instead is we replace the RNN cell with something like this. This is called a GRU cell. And a GRU cell, there's a picture of it, and there's the equations for it. So basically, I'll show you both quickly, but we'll talk about it much more in part 2. We've got our input, and our input normally goes straight in, gets multiplied by a weight matrix to create our new activations. That's not what happens, and then of course we add it to the existing activations. That's not what happens here. In this case, our input goes into this H tilde temporary thing. And it doesn't just get added to our previous activations, but our previous activations get multiplied by this value R. And R stands for reset. It's a reset gate. And how do we calculate this value? It goes between 0 and 1 in our reset gate. Well the answer is, it's simply equal to a matrix product between some weight matrix and the concatenation of our previous hidden state and our new input. In other words, this is a little one hidden layer neural net. And in particular, it's a one hidden layer neural net because we then put it through the sigmoid function. One of the things I hate about mathematical notation is symbols are overloaded a lot. Sometimes when you see sigma, it means standard deviation. When you see it next to a parenthesis like this, it means the sigmoid function. So in other words, that which looks like that. So this is like a little mini-neural net with no hidden layers. So to think of it another way, it's like a little logistic regression. I mentioned this briefly because it's going to come up a lot in part 2, so it's a good thing to start learning about. It's this idea that in the very learning itself, you can have little mini-neural nets inside your neural nets. And so this little mini-neural net is going to be used to decide how much of my hidden state am I going to remember. And so it might learn that in this particular situation, forget everything you know. For example, oh there's a full stop. Hey, when you see a full stop, you should throw away nearly all of your hidden state. That is probably something it would learn. And that's very easy for it to learn using this little mini-neural net. And so that goes through to create my new hidden state along with the input. And then there's a second thing that happens, which is there's this gate here called Z. And what Z says is, alright, you've got some amount of your previous hidden state plus your new input, and it's going to go through to create your new state. And I'm going to let you decide to what degree do you use this new input version of your hidden state, and to what degree will you just leave the hidden state the same as before. So this thing here is called the update gate. And so it's got two choices it can make. The first is to throw away some hidden state when deciding how much to incorporate that versus my new input, and how much to update my hidden state versus just leave it exactly the same. And the equation hopefully is going to look pretty familiar to you, which is, check this out here. Remember how I said you want to start to recognize some common ways of looking at things. Well here I have a 1 minus something by a thing, and a something without the 1 minus by a thing, which remember is a linear interpolation. So in other words, the value of Z is going to decide to what degree do I have, keep the previous hidden state, and to what degree do I use the new hidden state. So that's why they draw it here as this kind of like, it's not actually a switch, but you can put it in any position. You can be like oh it's here, or it's here, or it's here to decide how much to update. So they're basically the equations. It's a little mini neural net with its own weight matrix to decide how much to update, little mini neural net with its own weight matrix to decide how much to reset. And then that's used to do an interpolation between the two hidden states. So that's called a GRU, gated recurrent network. There's the definition from the PyTorch source code. They have some slight optimizations here that if you're interested in, we can talk about them on the forum, but it's exactly the same formula we just saw. And so if you go nn.gru, then it uses this same code, but it replaces the RNN cell with this cell. And as a result, rather than having something where we needed, where we were getting a 1.54, we're now getting down to 1.40, and we can keep training even more, get right down to 1.36. So in practice, a GRU, or very nearly equivalently, we'll see in a moment, an LSTM, in practice what pretty much everybody always uses. So the RT and HT are ultimately scalars after they go through the sigmoid, but they're applied element-wise, is that correct? Yes, although of course one for each mini-batch. And on the excellent Chris Ola's blog, there's an Understanding LSTM Networks post, which you can read all about this in much more detail if you're interested. And also the other one I was dealing from here is WildML, they also have a good blog post on this. If somebody wants to be helpful, feel free to put them in the lesson wiki if you can find them. So then putting it all together, I'm now going to replace my GRU with an LSTM. I'm not going to bother showing you the cell for this, it's very similar to GRU. But the LSTM has one more piece of state in it called the cell state, not just the hidden state. So if you do use an LSTM, you now inside your hidden have to return a tuple of matrices. They're exactly the same size as the hidden state, but you just have to return a tuple. The details don't matter too much, but we can talk about it during the wiki if you're interested. When you pass in, you still pass in self.h, it still returns a new value of h, you can repackage it in the usual way. So this code is identical to the code before. One thing I've done though is I've added dropout inside my RNN, which you can do with the PyTorch RNN function. So that's going to do dropout after each time step. And I've doubled the size of my hidden layer since I've now added.5 dropout. And so my hope was that this would make it be able to learn more but be more resilient as it does so. So then I wanted to show you how to take advantage of a little bit more FastAI magic without using the layer class. And so I'm going to show you how to use callbacks, and specifically we're going to do sgdr without using the learner class. So to do that, we create our model, again just a standard PyTorch model. And this time, rather than going, remember the usual PyTorch approach is opt equals optim.atom and you pass in the parameters and the learning rate. I'm not going to do that. I'm going to use the FastAI layer optimizer class, which takes my optim class constructor from PyTorch. It takes my model, it takes my learning rate, and optionally takes weight decay. And so this class is tiny, it doesn't do very much at all. The key reason it exists is to do differential learning rates and differential weight decay. But the reason we need to use it is that all of the mechanics inside FastAI assumes that you have one of these. So if you want to use callbacks or sgdr or whatever, encode where you're not using the learner class, then you need to use, rather than saying opt equals optim.atom and here's my parameters, you instead say layer optimizer. So that gives us a layer optimizer object. And if you're interested, basically behind the scenes, you can now grab a.opt property which actually gives you the optimizer. You don't have to worry about that yourself, but that's basically what happens behind the scenes. The key thing we can now do is that we can now, when we call fit, we can pass in that optimizer and we can also pass in some callbacks. And specifically we're going to use the cosine annealing callback. And so the cosine annealing callback requires a layer optimizer object. And so what this is going to do is it's going to do cosine annealing by changing the learning rate inside this object. So the details aren't terribly important, we can talk about them on the forum. It's really the concept I wanted to get across here, which is that now that we've done this, we can say, create a cosine annealing callback which is going to update the learning rates in this layer optimizer. The length of an epoch is equal to this here. How many mini-batches are there in an epoch? Well, it's whatever the length of this data loader is. Because it's going to be doing the cosine annealing, it needs to know how often to reset. And then you can pass in the cycleMult in the usual way. And then we can even save our model automatically. Remember how there was that cycleSaveName parameter that we can pass to learn.fit. This is what it does behind the scenes. Behind the scenes, it sets an onCycleEnd callback. And so here I've defined that callback as being something that saves my model. So there's quite a lot of cool stuff that you can do with callbacks. Callbacks are basically things where you can define like at the start of training, or at the start of an epoch, or at the start of a batch, or at the end of training, or at the end of an epoch, or at the end of a batch, please call this code. And so we've written some for you, including sgdr, which is the cosine annealing callback. And then Sahar recently wrote a new callback to implement the new approach to decoupled weight decay. We use callbacks to draw those little graphs of the loss over time. So there's lots of cool stuff you can do with callbacks. So in this case, by passing in that callback, we're getting sgdr, and that's able to get us down to 1.31 here. And then we can train a little bit more and eventually get down to 1.25. And so we can now test that out. And so if we pass in a few characters of text, we get not surprisingly an e after for of those. Let's do then 400. And now we have our own Nietzsche. So Nietzsche tends to start his sections with a number and a dot. So 293, perhaps that every life have values of blood, of intercourse, when it senses there is unscrupulous his very rights and still impulse love. So it's slightly less clear than Nietzsche normally, but it gets the tone right. And it's actually quite interesting to play around with training these character-based language models to run this at different levels of loss, to get a sense of what does it look like. Like you really notice that this is like 1.25. And at slightly worse, like 1.3, this looks like total junk. There's punctuation in random places, nothing makes sense. And you start to realize that the difference between Nietzsche and random junk is not that far in language model terms. And so if you train this for a little bit longer, you'll suddenly find like, oh, it's making more and more sense. So if you are playing around with NLP stuff, particularly generative stuff like this, and the results are like kind of okay but not great, don't be disheartened because that means you're actually very, very nearly there. The difference between something which is starting to create something which almost vaguely looks English if you squint, and something that's actually a very good generation, it's not far in loss function terms. So let's take a 5 minute break, we'll come back at 7.45 and we're going to go back to vision. So now we're looking at lesson 7, Sci-Fi 10 notebook. You might have heard of Sci-Fi 10. It's a really well-known dataset in academia. And it's actually pretty old by computer vision standards. Well before ImageNet was around, there was Sci-Fi 10. You might wonder why we're going to be looking at such an old dataset. Actually, I think small datasets are much more interesting than ImageNet. Because most of the time you're likely to be working with stuff with a small number of thousands of images rather than 1.5 million images. Some of you will work with 1.5 million images, but most of you won't. So learning how to use these kind of datasets I think is much more interesting. Often also a lot of the stuff we're looking at like in medical imaging, we're looking at the specific area where there's a lung nodule, you're probably looking at 32x32 pixels at most as being the area where that lung nodule actually exists. And so Sci-Fi 10 is small both in terms of it doesn't have many images and the images are very small. And in a lot of ways it's much more challenging than something like ImageNet. In some ways it's much more interesting. And also, most importantly, you can run stuff much more quickly on it. So it's much better to test out your algorithms with something you can run quickly and is still challenging. And so I hear a lot of researchers complain about how they can't afford to study all the different versions of their algorithm properly because it's too expensive. And they're doing it on ImageNet. So it's literally a week of expensive GPU work for every study they do. And I don't understand why you would do that kind of study on ImageNet. It doesn't make sense. And so there's been a particular lot of debate about this this week because a really interesting researcher named Ali Rahami at NIPS this week gave a talk, a really great talk, about the need for rigor in experiments in deep learning. He felt like there's a lack of rigor. And I've talked to him about it quite a bit since that time. I'm not sure we yet quite understand each other as to where we're coming from, but we have very similar kinds of concerns, which is basically people aren't doing carefully tuned, carefully thought about experiments, but instead they kind of throw lots of GPUs, lots of data and consider that a day. And so this idea of saying, is my algorithm meant to be good at small images, at small data sets, or if so let's study it on CyPhar 10 rather than studying it on ImageNet and then do more studies of different versions of the algorithm, turning different bits on and off, understand which parts are actually important, and so forth. People also complain a lot about MNIST, which we've looked at before. And I would say the same thing about MNIST, which is if you're actually trying to understand which parts of your algorithm make a difference and why, using MNIST for that kind of study is a very good idea. And all these people who complain about MNIST, I think they're just showing off. They're saying, I work at Google and I have a pod of TPUs and I have $100,000 a week of time to spend on it, no worries. But I think that's all it is. It's just signaling rather than academically rigorous. So CyPhar 10, you can download from here. This person has very kindly made it available in image form. If you Google for CyPhar 10, you'll find a much less convenient form, so please use this one. It's already in the exact form you need. Once you download it, you can use it in the usual way. So here's a list of the classes that are there. Now you'll see here I've created this thing called stats. Normally when we've been using pre-trained models, we have been saying transforms from model and that's actually created the necessary transforms to convert our dataset into a normalized dataset based on the means and standard deviations of each channel in the original model that was trained. In our case though, this time we've got to train a model from scratch, so we have no such thing. So we actually need to tell it the mean and standard deviation of our data to normalize it. So in this case, I haven't included the code here to do it. You should try and try this yourself to confirm that you can do this and understand where it comes from. But this is just the mean per channel and the standard deviation per channel of all of the images. So we're going to try and create a model from scratch. So the first thing we need is some transformations. So for Sci-Fi 10, people generally do data augmentation of simply flipping randomly horizontally. So here's how we can create a specific list of augmentations to use. And then they also tend to add a little bit of padding, black padding around the edge, and then randomly pick a 32x32 spot from within that padded image. So if you add the pad parameter to any of the FastAI transform creators, it'll do that for you. And so in this case, I'm just going to add 4 pixels around each size. And so now that I've got my transforms, I can go ahead and create my image classifier data.from paths in the usual way. I'm going to use a batch size of 256 because these are pretty small, so it's going to let me do a little bit more at a time. So here's what the data looks like. So for example, here's a boat. And just to show you how tough this is, what's that? So these are the kinds of things that we want to look at. So I'm going to start out. Our student, Kerim, we saw one of his posts earlier in this course, he made this really cool notebook which shows how different optimizers work. So Kerim made this really cool notebook, I think it was maybe last week, in which he showed how to create various different optimizers from scratch. So this is kind of like the Excel thing I had, but this is the Python version of Momentum and Atom and Nesterov and Atograd, all written from scratch, which is really cool. One of the nice things he did was he showed a tiny little general purpose fully connected network generator. So we're going to start with his. So he called that SimpleNet, so are we. So here's a simple class which has a list of fully connected layers. Whenever you create a list of layers in PyTorch, you have to wrap it in nn.module list just to tell PyTorch to register these as attributes. And so then we just go ahead and flatten the data that comes in because it's fully connected layers, and then go through each layer and call that linear layer, do the ReLU to it, and at the end do a softmax. So there's a really simple approach. And so we can now take that model, and now I'm going to show you how to step up one level of the API higher. Rather than calling the fit function, we're going to create a learn object, but we're going to create a learn object from a custom model. And so we can do that by saying we want a convolutional learner, we want to create it from a model and from some data, and the model is this one. So this is just a general PyTorch model, and this is a model data object of the usual kind. And that will return a learner. So this is a bit easier than what we just saw with the RNN. We don't have to fiddle around with layer optimizers and cosine annealing callbacks and whatever. This is now a learner that we can do all the usual stuff with, but we can do it with any model that we create. So if we just go learn, that'll go ahead and print it out. So you can see we've got 3072 features coming in because we've got 32x32 pixels by 3 channels. And then we've got 40x32 features coming out of the first layer, that's going to go into the second layer, 10 features coming out because we've got the 10 CyPhy 10 categories. You can call dot summary to see that in a little bit more detail. We can do LR find, we can plot that, and we can then go fit, and we can use cycle length, and so forth. So with a simple one hidden layer, one output layer, one hidden layer model, and here we can see the number of parameters we have is that over 120,000, we get a 47% accuracy. So not great, so let's kind of try and improve it. And so the goal here is we're going to try and eventually replicate the basic kind of architecture of a ResNet. So that's what we're going to try and get to here, is gradually build up to a ResNet. So the first step is to replace our fully connected model with a convolutional model. So to remind you, a fully connected layer is simply doing a dot product. So if we had all of these data points and all of these weights, then we basically do a sum product of all of those together, in other words it's a matrix multiply, then that's a fully connected layer. And so the weight matrix is going to contain an item for every element of the input, for every element of the output. So that's why we have here a pretty big weight matrix. And so that's why we had, despite the fact that we have such a crappy accuracy, we have a lot of parameters, because in this very first layer, we've got 3072 coming in and 40 coming out, so that gives us 3000x40 parameters. And so we end up not using them very efficiently, because we're basically saying every single pixel in the input has a different weight. And of course what we really want to do is find groups of 3x3 pixels that have particular patterns to them. And remember, we call that a convolution. So a convolution looks like so. We have a little 3x3 section of our image and a corresponding 3x3 set of filters, or our filter with a 3x3 kernel, and we just do a sum product of just that 3x3 by that 3x3. And then we do that for every single part of our image. And so when we do that across the whole image, that's called a convolution. And remember, in this case we actually had multiple filters, so the result of that convolution actually had multiple, it was a tensor with an additional third dimension to it, effectively. So let's take exactly the same code that we had before, but we're going to replace nn.linear with nn.com2d. Now what I want to do in this case though is each time I have a layer, I want to make the next layer smaller. And so the way I did that in my Excel example was I used maxPooling. So maxPooling took every 2x2 section and replaced it with its maximum value. Nowadays we don't use that kind of maxPooling much at all. Instead nowadays what we tend to do is do what's called a stride2 convolution. A stride2 convolution, rather than saying let's go through every single 3x3, it says let's go through every second 3x3. So rather than moving this 3x3 one to the right, we move it two to the right. And then when we get to the end of the row, rather than moving one row down, we move two rows down. So that's called a stride2 convolution. And so a stride2 convolution has the same kind of effect as a maxPooling, which is you end up halving the resolution in each dimension. So we can ask for that by saying stride equals 2. We can say we want it to be 3x3 by saying kernel size. And then the first two parameters are exactly the same as nn.linear. They're the number of features coming in and the number of features coming out. So we create a module list of those layers. And then at the very end of that, so in this case I'm going to say I've got 3 channels coming in. The first one layer will come out with 20, then 40, and then 80. So if we look at the summary, we're going to start with a 32x32, we're going to spit out a 15x15, and then a 7x7, and then a 3x3. And so what do we do now to get that down to a prediction of one of 10 classes? What we do is we do something called adaptive maxPooling. And this is what is pretty standard now for state of the art algorithms, is that the very last layer we do a maxPool. But rather than doing like a 2x2 maxPool, we say, it doesn't have to be 2x2, it could have been 3x3, which is like replace every 3x3 pixels with its maximum, it could have been 4x4. Adaptive maxPool is where you say, I'm not going to tell you how big an area to pool, but instead I'm going to tell you how big a resolution to create. So if I said, for example, I think my input here is like 28x28. If I said do a 14x14 adaptive maxPool, that would be the same as a 2x2 maxPool, because in other words it's saying please create a 14x14 output. If I said do a 2x2 adaptive maxPool, then that would be the same as saying do a 14x14 maxPool. And so what we pretty much always do in modern CNNs is we make our penultimate layer a 1x1 adaptive maxPool. So in other words, find the single largest cell and use that as our new activation. And so once we've got that, we've now got a 1x1 tensor, or actually 1x1 by number of features tensor. So we can then on top of that go x.view, x.size, minus 1. And actually there are no other dimensions to this basically. So this is going to return a matrix of minibatch by number of features. And so then we can feed that into a linear layer with however many classes we need. So you can see here the last thing I pass in is how many classes am I trying to predict, and that's what's going to be used to create that last layer. So it goes through every convolutional layer, does a convolution, does a relu, does an adaptive maxPool. This.view just gets rid of those trailing unit axes, the 1,1 axis, which is not necessary. That allows us to feed that into our final linear layer that spits out something of size C, which here is 10. So you can now see how it works. It goes 32 to 15 to 7x7 to 3x3. The adaptive maxPool makes it 80x1x1. And then our.view makes it just minibatch size by 80. And then finally a linear layer which takes it from 80 to 10, which is what we wanted. So that's our most basic, you'd call this a fully convolutional network. So a fully convolutional network is something where every layer is convolutional except for the very last. So again we can now go lr.find. And now in this case when I did lr.find, it went through the entire dataset and was still getting better. So in other words, even the default final learning rate it tries is 10, and even at that point it was still pretty much getting better. So you can always override the final learning rate by saying end lr equals. That will get it to try more things. So here is the learning rate finder. And so I picked 10 to the minus 1, trained that for a while, and that's looking pretty good. So I tried it with a cycle length of 1, and it's starting to flatten out at about 60%. So you can see here the number of elements, the number of parameters I have here are 500, 7000, 28000, about 30,000. So I have about a quarter of the number of parameters, but my accuracy has gone up from 47% to 60%. And the time per epoch here is under 30 seconds, and here also. So the time per epoch is about the same. And that's not surprising because when you use small simple architectures, most of the time is the memory transfer, the actual time during the compute is trivial. So I'm going to refactor this slightly because I want to try and put less stuff inside my forward. And so calling relu every time doesn't seem ideal. So I'm going to create a new class called conv layer. And the conv layer class is going to contain a convolution with a kernel size of 3 and a stride of 2. One thing I'm going to do now is I'm going to add padding. Did you notice here the first layer went from 32x32 to 15x15, not 16x16. And the reason for that is that at the very edge of your convolution here, see how this first convolution, there isn't a convolution where the middle is the top left point, because there's nothing outside it. Where else if we had put a row of zeros at the top and a row of zeros at the edge of each column, we now could go all the way to the edge. So pad equals 1 adds that little layer of zeros around the edge for us. And so this way we're going to make sure that we go 32x32 to 16x16 to 8x8. It doesn't matter too much when you've got these bigger layers, but by the time you get down to 4x4, you really don't want to throw away a whole piece. So the padding becomes important. So by refactoring it to put this with its defaults here, and then in the forward I'll put the relu in here as well, it makes my conv net a little bit smaller and more to the point, it's going to be easier for me to make sure that everything is correct in the future by always using this conv layer class. So now you know not only how to create your own neural network model, but how to create your own neural network layer. So here now I can use conv layer. And this is such a cool thing about PyTorch, is a layer definition and a neural network definition are literally identical. They both have a constructor and a forward. And so any time you've got the layer, you can use it as a neural net. Any time you have a neural net, you can use it as a layer. So this is now the exact same thing as we had before. One difference is I now have padding. And another thing just to show you, you can do things differently. Back here, my max pool I did as an object, I used the class nn.adaptiveMaxPool and I stuck it in this attribute and then I called it. But this actually doesn't have any state. There's no weights inside max pooling. So I can actually do it with a little bit less code by calling it as a function. So everything that you can do as a class, you can also do as a function, it's inside this capital F, which is nn.functional. So this should be a tiny bit better because this time I've got the padding. I didn't train it for as long to actually check. So let's skip over that. So one issue here is that in the end, when I tried to add more layers, I had trouble training it. And the reason I was having trouble training it is if I used larger learning rates, it would go off to nn. And if I used smaller learning rates, it kind of takes forever and doesn't really have a chance to explore properly. So it wasn't resilient. So to make my model more resilient, I'm going to use something called batch normalization, which literally everybody calls batch norm. And batch norm is a couple of years old now, and it's been pretty transformative since it came along, because it suddenly makes it really easy to train deeper networks. So the network I'm going to create is going to have more layers. I've got 1, 2, 3, 4, 5 convolutional layers plus a fully connected layer. So back in the old days, that would be considered a pretty deep network and we would consider pretty hard to train. Nowadays it's super simple thanks to batch norm. Now to use batch norm, you can just write nn.batchnorm. But to learn about it, we're going to write it from scratch. So the basic idea of batch norm is that we've got some vector of activations. Any time I draw a vector of activations, obviously I mean you can repeat it for the mini-batch, so pretend it's a mini-batch of 1. So we've got some vector of activations and it's coming into some layer, so probably some convolutional matrix multiplication. And then something comes out the other side. So imagine this is just a matrix multiply, which was like, say it was an identity matrix. Then every time I multiply it by that across lots and lots of layers, my activations are not getting bigger, they're not getting smaller, they're not changing at all. That's all fine. But imagine if it was actually like 2, 2, 2. And so if every one of my weight matrices or filters was like that, then my activations are doubling each time. And so suddenly I've got exponential growth. And in deep models, that's going to be a disaster because my gradients are exploding at an exponential rate. And so the challenge you have is that it's very unlikely, unless you try carefully to deal with it, that your weight matrices on average are not going to cause your activations to keep getting smaller and smaller or keep getting bigger and bigger. You have to kind of carefully control things to make sure that they stay at a reasonable size. You want to keep them at a reasonable scale. So we start things off with 0 mean, standard deviation 1 by normalizing the inputs. But what we'd really like to do is to normalize every layer, not just the inputs. And so, okay fine, let's do that. So here I've created a BN layer which is exactly like my Conv layer. It's got my Conv2D with my stride, my padding. I do my Conv and my ReLU. And then I calculate the mean of each channel or of each filter and the standard deviation of each channel or each filter. And then I subtract the means and divide by the standard deviations. So now I don't actually need to normalize my input at all, because it's actually going to do it automatically. It's normalizing it per channel. And for later layers, it's normalizing it per filter. So it turns out that's not enough, because SGD is bloody minded. And so if SGD decided that it wants the weight matrix to be like so, where that matrix is something which is going to increase the values overall repeatedly, then trying to subtract the means and divide by the standard deviations just means the next mini-batch is going to try and do it again, and it'll try and do it again, and it'll try and do it again. So it turns out that this actually doesn't help. Like it literally does nothing, because SGD is just going to go ahead and undo it the next mini-batch. So what we do is we create a new multiplier for each channel and a new added value for each channel. Literally, we just start them out as the addition is just a bunch of 0s, so for the first layer, 3 0s, and the multiplier for the first layer is just 3 1s. So number of filters for the first layer is just 3. And so we then basically undo exactly what we just did, or potentially we undo them. So by saying this is an nn.parameter, that tells PyTorch you're allowed to learn these as weights. So initially it says, OK, subtract the means, divide by the standard deviations, multiply by 1, add on 0. OK, that's fine, nothing much happened there. But what it turns out is that now rather than, if it wants to kind of scale the layer up, it doesn't have to scale up every single value in the matrix, it can just scale up this single trio of numbers, self.m. If it wants to shift it all up or down a bit, it doesn't have to shift the entire weight matrix, it can just shift this trio of numbers, self.a. So I will say this, at this talk I mentioned at NIPS, Ali Rahimi's talk about rigor, he actually pointed to this paper, this BatchNorm paper, as being a particularly useful, particularly interesting paper where a lot of people don't necessarily quite know why it works. And so if you're thinking, OK, subtracting out the means and then adding some learned weights of exactly the same rank and size sounds like a weird thing to do, there are a lot of people that feel the same way. So at the moment I think the best I can say intuitively is what's going on here is that we're normalizing the data and then we're saying you can then shift it and scale it using far fewer parameters than would have been necessary if I was asking you to actually shift and scale the entire set of convolutional filters. That's the kind of basic intuition. More importantly, in practice, what this does is it basically allows us to increase our learning rates and it increases the resilience of training and allows us to add more layers. So once I added a BN layer rather than a conv layer, I found I was able to add more layers to my model and it's still trained effectively. Question. Are we worried about anything that maybe we're divided by something very small or anything like that once we do this? I think in the PyTorch version it would probably be divided by self.studs plus epsilon or something. This worked fine for me, but that is definitely something to think about if you were trying to make this more reliable. So the self.m and self.a, I'm guessing it's getting updated through backpropagation as well? Yes, so by saying it's an nn.parameter, that's how we flagged PyTorch to learn it through backprop. The other interesting thing it turns out that BatchNorm does is it regularizes. In other words, you can often decrease or remove dropout or decrease or remove weight decay when you use BatchNorm. And the reason why is if you think about it, each mini-batch is going to have a different mean and a different standard deviation to the previous mini-batch. So these things keep changing. And because they keep changing, it's kind of changing the meaning of the filters in this subtle way. And so it's adding a regularization effect because it's noise. When you add noise of any kind, it regularizes your model. I'm actually cheating a little bit here. In the real version of BatchNorm, you don't just use this batch's mean and standard deviation, but instead you take an exponentially weighted moving average standard deviation and mean. And so if you wanted an exercise to try during the week, that would be a good thing to try. But I will point out something very important here, which is if self.training. When we are doing our training loop, this will be true when it's being applied to the training set, and it will be false when it's being applied to the validation set. And this is really important because when you're going through the validation set, you do not want to be changing the meaning of the model. So this is this really important idea, is that there are some types of layer that are actually sensitive to what the mode of the network is, whether it's in training mode or as PyTorch calls it, evaluation mode, or we might say test mode. And actually, we actually had a bug a couple of weeks ago when we did our mininet for MovieLens, the collaborative filtering. We actually had F.dropout in our forward pass without protecting it with an if self.training F.dropout. As a result of which, we were actually doing dropout in the validation piece as well as the training piece, which obviously isn't what you want. So I've actually gone back and fixed this by changing it to using nn.dropout. And nn.dropout has already been written for us to check whether it's being used in training mode or not. Or alternatively, I could have added an if self.training before I use the dropout here. So it's important to think about that, and the main two, or pretty much the only two built into PyTorch where this happens is dropout and batch melt. And so interestingly, this is also a key difference in fast.ai, which no other library does, is that these means and standard deviations get updated in training mode in every other library as soon as you basically say I'm training, regardless even of whether that layer is set to trainable or not. And it turns out that with a pre-trained network, that's a terrible idea. If you have a pre-trained network, the specific values of those means and standard deviations in batch norm, if you change them, it changes the meaning of those pre-trained layers. And so in fast.ai, always by default, it won't touch those means and standard deviations if your layer is frozen. As soon as you unfreeze it, it'll start updating them, unless you've set learn.bnfreeze true. If you set learn.bnfreeze true, it says never touch these means and standard deviations. And I found in practice that that often seems to work a lot better for pre-trained models, particularly if you're working with data that's quite similar to what the pre-trained model was trained with. So I have two questions, so it looks like you did a lot more work calculating the aggregates. Looks like I did a lot of work, did you say? Like quite a lot of code here? Well you're doing more work than you would normally do. Essentially you're calculating all these aggregates as you go through each layer. Yes. Wouldn't this mean your training, like your epoch time, like blows up? No, this is like super fast. If you think about what a conv has to do, a conv has to go through every 3x3 with a stride and do this multiplication and then addition. That is a lot more work than simply calculating the per channel mean. So it adds a little bit of time, but it's less time intensive than the convolution. So how would you basically position the batch norm? Would it be right after the convolutional layer or would it be after the ReLU? We'll talk about that in a moment. So at the moment we have it after the ReLU and in the original batch norm paper, I believe that's where they put it. So there's this idea of something called an ablation study. And ablation study is something where you basically try kind of turning on and off different pieces of your model to see which bits make which impacts. And one of the things that wasn't done in the original batch norm paper was any kind of really effective ablation study. And one of the things therefore that was missing was this question which you just asked, which is where do you put the batch norm? Before the ReLU, after the ReLU, whatever. And so since that time, that oversight has caused a lot of problems because it turned out the original paper didn't actually put it in the best spot. And so then other people since then have now figured that out. And now every time I show people code where it's actually in the spot that turns out to be better, people always say, your batch norm is in the wrong spot. And I have to go back and say, no, I know that's what the paper said, but it turned out that's not actually the right spot. So it kind of causes confusion. So there's been a lot of question about that. So a little bit of a higher level question. So we started out with CyPhar data. Is the basic reasoning that you use a smaller data set to quickly train a new model, and then you take the same model and you're using a much bigger data set to get a higher accuracy level? Is that the basic? Answer the question. Maybe. So if you had a large data set, or if you were interested in the question of how good is this technique on a large data set, then yes, what you just said would be what I would do. I would start testing on a small data set which I had already discovered had the same kinds of properties as my larger data set, and therefore my conclusions would likely carry forward, and then I would test them at the end. Having said that, personally I'm actually more interested in actually studying small data sets for their own sake, because I find most people I speak to in the real world don't have a million images. They have somewhere between about 2000 and 20,000 images, seems to be much more common. So I'm very interested in having fewer rows because I think it's more valuable in practice. I'm also pretty interested in small images, not just for the reason you mentioned, which is it allows me to test things out more quickly, but also as I mentioned before, often a small part of an image actually turns out to be what you're interested in, that's certainly true in medicine. I have two questions. The first is on what you mentioned in terms of small data sets, particularly medical imaging. If you've heard of, I guess, Vicarious, a startup in the specialization in one-shot learning, so your opinions on that. And then the second being, this is related to Ali's talk at NIPS, I don't want to say it's controversial, but like Yan-LeCun, there was a really, I guess, controversial thread attacking it in terms of what you're talking about as a baseline of theory, just not keeping up with practice. And so I guess I was studying with Yan, whereas Ali actually, he tweeted at me quite a bit trying to defend he wasn't attacking Yan at all, but in fact, he was trying to support him, but I just kind of feel like a lot of theory as you go is just sort of out of date and it's hard to keep up other than an archive from Andre Kaparthi to keep up. But if the theory isn't keeping up, but the industry is the one that's actually setting the standard, then doesn't that mean that people who are actual practitioners are the ones like Yan-LeCun that are publishing the theory that are keeping up to date, whereas academic research institutions are actually behind? So I don't have any comments on the Vicarious papers because I haven't read them. I'm not aware of any of them as actually showing better results than other papers, but I think they've come a long way in the last 12 months, so that might be wrong. I think the discussion between Yan-LeCun and Ali Rahimi is very interesting because they're both smart people who have interesting things to say. Unfortunately, a lot of people took Ali's talk as meaning something which he says it didn't mean, and when I listen to his talk, I'm not sure he didn't actually mean it at the time, but he clearly doesn't mean it now. He's now said many times he was not talking about theory, he was not saying we need more theory at all. Actually he thinks we need more experiments. And so specifically, he's also now saying he wished he hadn't used the word rigor, which I also wish, because rigor is kind of meaningless and everybody can kind of say when he says rigor he means the specific thing I study. So lots of people have kind of taken his talk as being like, oh yes, this proves that nobody else should work in neural networks unless they are experts at the one thing I'm an expert in. So I'm going to catch up with him and talk more about this in January and hopefully we'll figure some more stuff out together. But basically what we can clearly agree on, and I think Jan LeCun also agrees on, is careful experiments are important. Just doing things on massive amounts of data, using massive amounts of TPUs or GPUs is not interesting of itself, and we should instead try to design experiments that give us the maximum amount of insight into what's going on. So Jeremy, is it a good statement to say something like, so dropout and bash norm are very different things. Dropout is a regularization technique, and bash norm has maybe some regularization effect, but it's actually just about convergence of the optimization method. And I would further say, I can't see any reason not to use bash norm. There are versions of bash norm that in certain situations turned out not to work so well, but people have figured out ways around that for nearly every one of those situations now. So I would always seek to find a way to use bash norm. It may be a little harder in RNNs at least, but even there, there are ways of doing bash norm in RNNs as well. So try and always use bash norm on every layer if you can. The question that somebody asked is, does it mean I can stop normalizing my data? It does, although do it anyway because it's not at all hard to do it, and at least that way the people using your data, they kind of know how you've normalized it. And particularly with these issues around a lot of libraries, in my opinion, at least not my opinion, my experiments don't deal with bash norm correctly for pre-trained models. Just remember that when somebody starts retraining, those averages and stuff are going to change for your dataset. And so if your new dataset has very different input averages, it could really cause a lot of problems. So I went through a period where I actually stopped normalizing my data. Things kind of worked, but it's probably not worth it. So the rest of this is identical. All I've done is I've changed conv layer to bn layer. But I've done one more thing, which is I'm trying to get closer and closer to modern approaches, which I've added a single convolutional layer at the start with a bigger kernel size and a stride of 1. Why have I done that? So the basic idea is that I want my first layer to have a richer input. So before my first layer had an input of just 3, because it's just 3 channels. But if I start with my image, and I kind of take a bigger area, and I do a convolution using that bigger area, in this case I'm doing 5x5, then that kind of allows me to try and find more interesting, richer features in that 5x5 area. And so then I spit out a bigger output. In this case I spit out 10 5x5 filters. And so the idea is pretty much every state-of-the-art convolutional architecture now starts out with a single conv layer with a 5x5 or 7x7 or sometimes even 11x11 convolution with quite a few filters, something like 32 filters coming out. And it's just a way of trying to, because I used a stride of 1 and a padding of kernel size minus 1 over 2, that means that my output is going to be exactly the same size as my input, but just got more filters. That's just a good way of trying to create a richer starting point for my sequence of convolutional layers. So that's the basic theory of why I've added this single convolution, which I just do once at the start, and then I just go through all my layers, and then I do my adaptive max pooling and my final classifier layer. So it's a minor tweak, but it helps. And so you'll see now I can go from 60% and after a couple it's 45%. Now after a couple it's 57%. After a few more I'm up to 68%. So you can see the batch norm and a tiny bit of the conv layer at the start, it's helping. And what's more, you can see this is still increasing. So that's looking pretty encouraging. So given that this is looking pretty good, an obvious thing to try is to try increasing the depth of the model. And now I can't just add more of my stride 2 layers because remember how it halved the size of the image each time? I'm basically down to 2x2 at the end, so I can't add much more. So what I did instead was I said, okay, here's my original layers, these are my stride 2 layers. For every one, also create a stride 1 layer. So a stride 1 layer doesn't change the size. And so now I'm saying zip my stride 2 layers and my stride 1 layers together. And so first of all do the stride 2 and then do the stride 1. So this is now actually twice as deep. So this is now twice as deep, but I end up with the exact same 2x2 that I had before. And so if I try this, here after 1, 2, 3, 4 epochs is at 65%, after 1, 2, 3 epochs, I'm still at 65%. It hasn't helped. And so the reason it hasn't helped is I'm now too deep even for batch norm to handle it. So my depth is now 1, 2, 3, 4, 5 times 2 is 10, 11, conv1, 12. So 12 layers deep, it's possible to train a standard conv net 12 layers deep, but it starts to get difficult to do it properly. And it certainly doesn't seem to be really helping much, if at all. So that's where I'm instead going to replace this with a resnet. So a resnet is our final stage. And what a resnet does is I'm going to replace our bn layer, I'm going to inherit from bn layer, and replace our forward with that. And that's it. Everything else is going to be identical. But now I'm going to do way lots of layers, I'm going to make it 4 times deeper, and it's going to train beautifully just because of that. So why does that help so much? So this is called a resnet block. And as you can see, I'm saying my predictions equals my input plus some function, in this case a convolution of my input. That's what I've written here. And so I'm now going to shuffle that around a little bit, and I'm going to say f of x equals y minus x. So that's the same thing, shuffled around. That's my prediction from the previous layer. And so what this is then doing is it's trying to fit a function to the difference between these two. And so the difference is actually the residual. So if this is what I'm trying to calculate, my actual y value, and this is the thing that I've most recently calculated, then the difference between the two is basically the error in terms of what I've calculated so far. And so this is therefore saying try to find a set of convolutional weights that attempts to fill in the amount I was off by. So in other words, if we have some inputs coming in, and then we have this function which is basically trying to predict the error, it's like how much are we off by, and then we add that on, so we basically add on this additional prediction of how much we were wrong by, and then we add on another prediction of how much were we wrong by that time, and add on another prediction of how much were we wrong by that time. Then each time we're kind of zooming in, getting closer and closer to our correct answer. At each time we're saying, we've got to a certain point, but we've still got an error, we've still got a residual, so let's try and create a model that just predicts that residual and add that on to our previous model. And then let's build another model that predicts the residual and add that on to our previous model. And if we keep doing that again and again, we should get closer and closer to our answer. And this is based on a theory called boosting, which people that have done some machine learning will have certainly come across. And so basically the trick here is that by specifying that as being the thing that we're trying to calculate, then we kind of get boosting for free. It's like because we can just juggle that around to show that actually it's just calculating a model on the residual. So that's kind of amazing. And it totally works. As you can see here, I've now got my standard batch norm layer, which is something which is going to reduce my size by 2 because it's got the stride 2. And then I've got a ResNet layer of stride 1 and another ResNet layer of stride 1. And sorry, I think I said there were 4 of these, there's actually 3 of these. So this is now 3 times deeper, I zipped through all of those. And so I've now got a function of a function of a function. So 3 layers per group, and then my start and my linear at the end. So this is now 3 times bigger than my original. And if I fit it, you can see it just keeps going up and up and up and up. I keep fitting it more, it keeps going up and up and up and up. And it's still going up when I kind of got bored. So the ResNet has been a really important development. And it's allowed us to create these really deep networks. Now the full ResNet does not quite look the way I've described it here. The full ResNet doesn't just have one convolution, but it actually has 2 convolutions. So the way people normally draw ResNet blocks is they normally say, you've got some input coming into the layer, it goes through one convolution, 2 convolutions, and then gets added back to the original input. That's the full version of a ResNet block. In my case, I've just done one convolution. And then you'll see also in every block, one of them, it's actually the first one here, is not a ResNet block, but a standard convolution with a stride of 2. This is called a bottleneck layer. And the idea is this is not a ResNet block. So from time to time, we actually change the geometry. We're doing the stride 2. In ResNet, we don't actually use just a standard convolutional layer. There's actually a different form of bottleneck block that I'm not going to teach you in this course. I'm going to teach you in part 2. But as you can see, even this somewhat simplified version of a ResNet still works pretty well. And so we can make it a little bit bigger. And so here I've just increased all of my sizes. I have still got my 3. And also I've added dropout. So at this point, I'm going to say this is, other than the minor simplification of ResNet, a reasonable approximation of a good starting point for a modern architecture. And so now I've added in my.2 dropout. I've increased the size here. And if I train this, I can train it for a while. It's going pretty well. I can then add in TTA at the end. Eventually I get 85%. And this is at a point now where, literally I wrote this whole notebook in 3 hours. We can create this thing in 3 hours. And this is like an accuracy that in 2012, 2013 was considered pretty much state of the art for Cypher 10. So this is actually pretty damn good. So nowadays, the most recent results are like 97%. There's plenty of room we can still improve, but they're all based on these techniques. Like there isn't really anything. When we start looking in part 2 at how to get this right up to state of the art, you'll see it's basically better approaches to data augmentation, better approaches to regularization, some tweaks on ResNet, but it's all basically this idea. So is the training on the residual method, looks like it's a generic thing that can be applied, non-image problems. Oh great question. Yes, it is, but it's been ignored everywhere else. In NLP, something called the transformer architecture recently appeared and was shown to be the state of the art for translation, and it's got like a simple ResNet structure in it. First time I've ever seen it in NLP. I haven't really seen anybody else take advantage of it. This general approach, we call these skip connections, this idea of skipping over a layer and doing an identity. It's been appearing a lot in computer vision, and nobody else much seems to be using it, even though there's nothing computer vision specific about it. So I think it's a big opportunity. So final stage I want to show you is how to use an extra feature of PyTorch to do something cool and it's going to be kind of a segue into part 2. It's going to be our first little hint as to what else we can build on these neural nets. And it's also going to take us all the way back to lesson 1, which is we're going to do dogs and cats. So going all the way back to dogs and cats, we're going to create a ResNet 34. So these different ResNet 34, 50, 101, they're basically just different numbers of different size blocks. It's like how many of these kind of pieces do you have before each bottleneck block, and then how many of these sets of super blocks do you have. That's all these different numbers mean. So if you look at the TorchVision source code, you can actually see the definition of these different ResNets. You'll see they're all just different parameters. So we're going to use ResNet 34. And so we're going to do this a little bit more by hand. So if this is my architecture, this is just the name of a function, then I can call it to get that model. And then true, if we look at the definition, is do I want the pre-trained, so in other words is it going to load in the pre-trained image net weights. So M now contains a model, and so I can take a look at it like so. And so you can see here what's going on, is that inside here I've got my initial 2D convolution. And here is that kernel size of 7x7. And interestingly in this case, it actually starts out with a 7x7 stripe too. There's the padding that we talked about to make sure we don't lose the edges. There's our batch norm. There's our relu. And you get the idea, right? And then so here you can now see there's a layer that contains a bunch of blocks. So here's a block which contains a conv, batch norm, relu, conv, batch norm. You can't see it printed, but after this is where it does the addition. So there's like a whole ResNet block, and then another ResNet block, and then another ResNet block. And then you can see also, sometimes you see one where there's a stripe too. So here's actually one of these bottleneck layers. So you can kind of see how this is structured. So in our case, sorry I skipped over this a little bit, but the approach that we ended up using for relu was to put it before our batch norm. See what they do here. We've got batch norm, relu, conv, batch norm, relu, conv. So you can see the order that they're using it here. And you'll find there's 3 different versions of ResNet floating around. The one which actually turns out to be the best is called the Preact ResNet, which has a different ordering again. But you can look it up. It's basically a different order of where the relu and where the batch norm sit. So we're going to start with a standard ResNet 34, and normally what we do is we need to now turn this into something that can predict dogs vs. cats. So currently the final layer has 1000 features, because ImageNet has 1000 features. So we need to get rid of this. So when you use conv learner from pre-trained in FastAI, it actually deletes this layer for you, and it also deletes this layer. And something that as far as I know is unique to FastAI is we replace this average pooling layer of size 7x7. So this is basically the adaptive pooling layer, but whoever wrote this didn't know about adaptive pooling, so they manually said, oh I know it's meant to be 7x7. So in FastAI we replace this with adaptive pooling, but we actually do both adaptive average pooling and adaptive max pooling, and we then concatenate the two together. It is something we invented, but at the same time we invented it, somebody wrote a paper about it, so we don't get any credit. But I think we're the only library that provides it, and certainly the only one that does it by default. For the purpose of this exercise though, we're going to do a simple version where we delete the last two layers. So we'll grab all the children of the model, we'll delete the last two layers, and then instead we're going to add a convolution which just has two outputs. I'll show you why in a moment. Then we're going to do our average pooling, and then we're going to do our softmax. So that's a model which is going to have, you'll see that this one has a fully connected layer at the end, this one does not have a fully connected layer at the end. But if you think about it, this convolutional layer is going to be two filters only, and it's going to be 2x, 7x, 7x. And so once we then do the average pooling, it's going to end up being just two numbers that it produces. So this is a different way of producing just two numbers. I'm not going to say it's better, I'm just going to say it's different. But there's a reason we do it. I'll show you the reason. We can now train this model in the usual way. So we can say transforms.model, image classifier data from paths, and then we can use that conv learner from model data we just learned about. I'm now going to freeze every single layer except for that one, and this is the fourth last layer, so we'll say freeze to minus 4. And so this is just training the last layer. So we get 99.1% accuracy, so this approach is working fine. And here's what we can do though. We can now do something called class activation maps. What we're going to do is we're going to try to look at this particular cat, and we're going to use a technique called class activation maps where we take our model and we ask it which parts of this image turned out to be important. And when we do this, it's going to feed out this picture it's going to create. And so as you can see here, it's found the cat. So how did it do that? The way it did that, we'll kind of work backwards, is to produce this matrix. You'll see in this matrix, there's some pretty big numbers around about here which correspond to our cat. So what is this matrix? This matrix is simply equal to the value of this feature matrix times this py vector. The py vector is simply equal to the predictions, which in this case said I'm 100% confident it's a cat. So this is just equal to the value of, if I just call the model passing in our cat, then we get our predictions. So that's just the value of our predictions. So py is just the value of our predictions. What about feet? What's that equal to? Feet is equal to the values in this layer. In other words, the value that comes out of the final convolutional layer. So it's actually the 7x7x2. And so you can see here, the shape of features is 2 filters by 7x7. So the idea is if we multiply that vector by that tensor, then it's going to end up grabbing all of the first channel, because that's a 1, and none of the second channel, because that's a 0. And so therefore it's going to return the value of the last convolutional layer for the section which lines up with being a cat. If you think about it, the first section lines up with being a cat, the second section lines up with being a dog. So if we multiply that tensor by that tensor, we end up with this matrix. And this matrix is which parts are most like a cat. Or to put it another way, in our model, the only thing that happened after the convolutional layer was an average pooling layer. So the average pooling layer took that 7x7 grid and said average out how much each part is cat-like. So my final value, my final prediction was the average cattiness of the whole thing. And so because it had to be able to average out these things to get the average cattiness, that means I could then just take this matrix and resize it to be the same size as my original cat and just overlay it on top. You get this heatmap. So the way you can use this technique at home is to basically calculate this matrix on some really big picture. You can calculate this matrix on a quick small little conv net and then zoom into the bit that has the highest value and then rerun it just on that part. So it's like, oh this is the area that seems to be the most like a cat or most like a dog that zoom into that bit. So I skipped over that pretty quickly because we ran out of time. And so we'll be learning more about these kind of approaches in part 2 and we can talk about it more on the forum. But hopefully you get the idea. The one thing I totally skipped over was how do we actually ask for that particular layer. And I'll let you read about this during the week, but basically there's a thing called a hook. So we called save features, which is this little class that we wrote that goes register forward hook. And basically a forward hook is a special PyTorch thing that every time it calculates a layer, it runs this function. It's like a callback. It's like a callback that happens every time it calculates a layer. And so in this case, it just saved the value of the particular layer that I was interested in. And so that way I was able to go inside here and grab those features out after I was done. So I call save features, that gives me my hook, and then later on I can just grab the value that I saved. So I skipped over that pretty quickly, but if you look in the PyTorch docs, they have some more information and help about that. Yes, Yannett. Do you have the? Question. Jeremy, can you spend 5 minutes talking about your journey into deep learning? And finally, how can we keep up with important research that is important to practitioners? I think I'll close more on the latter bit, which is like what now. So for those of you who are interested, you should aim to come back for part 2. If you're aiming to come back for part 2, how many people would like to come back for part 2? That's not bad, I think almost everybody. So if you want to come back for part 2, be aware of this. By that time, you're expected to have mastered all of the techniques we've learned in part 1. There's plenty of time between now and then, even if you haven't done much or any ML before. But it does assume that you're going to be working at the same level of intensity from now until then that you have been with practicing. So generally speaking, the people who did well in part 2 last year had watched each of the videos about 3 times. And some of the people actually discovered they learned some of them off by heart by mistake. So watching the videos again is helpful. And make sure you get to the point that you can recreate the notebooks without watching the videos. So to make it more interesting, obviously, try and recreate the notebooks using different data sets. And definitely then just keep up with the forum and you'll see people keep on posting more stuff about recent papers and recent advances. And over the next couple of months, you'll find increasingly less and less of it seems weird and mysterious, and more and more of it makes perfect sense. And so it's a bit of a case of just staying tenacious. There's always going to be stuff that you don't understand yet. But you'll be surprised if you go back to Lesson 1 and 2 now, you'll be like, oh, that's all trivial. So that's kind of hopefully a bit of your learning journey. The main thing I've noticed is that people who succeed are the ones who just keep working at it. If you're not coming back here every Monday, you're not going to have that forcing function. I've noticed the forum suddenly gets busy at 5pm on a Monday. It's like, oh, the course is about to start and suddenly these questions start coming in. So now that you don't have that forcing function, try and use some other technique to give yourself that little kick. Maybe you can tell your partner at home, I'm going to try and produce something every Saturday for the next 4 weeks, or I'm going to try and finish reading this paper or something. Anyway, so I hope to see you all back in March. Even regardless whether I do or don't, it's been a really great pleasure to get to know you all. And I hope to keep seeing you on the forum. Thanks very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.36, "text": " Last class of part 1, I guess the theme of part 1 is classification and regression with", "tokens": [5264, 1508, 295, 644, 502, 11, 286, 2041, 264, 6314, 295, 644, 502, 307, 21538, 293, 24590, 365], "temperature": 0.0, "avg_logprob": -0.21896661561110925, "compression_ratio": 1.6625, "no_speech_prob": 0.020323220640420914}, {"id": 1, "seek": 0, "start": 12.36, "end": 18.52, "text": " deep learning. And specifically it's about identifying and learning the best practices", "tokens": [2452, 2539, 13, 400, 4682, 309, 311, 466, 16696, 293, 2539, 264, 1151, 7525], "temperature": 0.0, "avg_logprob": -0.21896661561110925, "compression_ratio": 1.6625, "no_speech_prob": 0.020323220640420914}, {"id": 2, "seek": 0, "start": 18.52, "end": 25.36, "text": " classification and regression. And we started out with the kind of here are 3 lines of code", "tokens": [21538, 293, 24590, 13, 400, 321, 1409, 484, 365, 264, 733, 295, 510, 366, 805, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.21896661561110925, "compression_ratio": 1.6625, "no_speech_prob": 0.020323220640420914}, {"id": 3, "seek": 2536, "start": 25.36, "end": 32.08, "text": " to do image classification. And gradually we've been, well the first 4 lessons were", "tokens": [281, 360, 3256, 21538, 13, 400, 13145, 321, 600, 668, 11, 731, 264, 700, 1017, 8820, 645], "temperature": 0.0, "avg_logprob": -0.14006899761897262, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00019708745821844786}, {"id": 4, "seek": 2536, "start": 32.08, "end": 37.16, "text": " then kind of going through NLP structured data, collaborative filtering and kind of", "tokens": [550, 733, 295, 516, 807, 426, 45196, 18519, 1412, 11, 16555, 30822, 293, 733, 295], "temperature": 0.0, "avg_logprob": -0.14006899761897262, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00019708745821844786}, {"id": 5, "seek": 2536, "start": 37.16, "end": 42.6, "text": " understanding some of the key pieces and most importantly understanding how to actually", "tokens": [3701, 512, 295, 264, 2141, 3755, 293, 881, 8906, 3701, 577, 281, 767], "temperature": 0.0, "avg_logprob": -0.14006899761897262, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00019708745821844786}, {"id": 6, "seek": 2536, "start": 42.6, "end": 48.4, "text": " make these things work well in practice. And then the last 3 lessons are then kind of going", "tokens": [652, 613, 721, 589, 731, 294, 3124, 13, 400, 550, 264, 1036, 805, 8820, 366, 550, 733, 295, 516], "temperature": 0.0, "avg_logprob": -0.14006899761897262, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00019708745821844786}, {"id": 7, "seek": 2536, "start": 48.4, "end": 53.4, "text": " back over all of those topics in kind of reverse order to understand more detail about what", "tokens": [646, 670, 439, 295, 729, 8378, 294, 733, 295, 9943, 1668, 281, 1223, 544, 2607, 466, 437], "temperature": 0.0, "avg_logprob": -0.14006899761897262, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00019708745821844786}, {"id": 8, "seek": 5340, "start": 53.4, "end": 57.76, "text": " was going on and understanding what the code looks like behind the scenes and learning", "tokens": [390, 516, 322, 293, 3701, 437, 264, 3089, 1542, 411, 2261, 264, 8026, 293, 2539], "temperature": 0.0, "avg_logprob": -0.14890423040280396, "compression_ratio": 1.7117117117117118, "no_speech_prob": 4.984737824997865e-05}, {"id": 9, "seek": 5340, "start": 57.76, "end": 62.48, "text": " to write them from scratch.", "tokens": [281, 2464, 552, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.14890423040280396, "compression_ratio": 1.7117117117117118, "no_speech_prob": 4.984737824997865e-05}, {"id": 10, "seek": 5340, "start": 62.48, "end": 69.92, "text": " Part 2 of the course will move from a focus on classification and regression, which is", "tokens": [4100, 568, 295, 264, 1164, 486, 1286, 490, 257, 1879, 322, 21538, 293, 24590, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.14890423040280396, "compression_ratio": 1.7117117117117118, "no_speech_prob": 4.984737824997865e-05}, {"id": 11, "seek": 5340, "start": 69.92, "end": 76.44, "text": " kind of predicting a thing like a number or at most a small number of things like a small", "tokens": [733, 295, 32884, 257, 551, 411, 257, 1230, 420, 412, 881, 257, 1359, 1230, 295, 721, 411, 257, 1359], "temperature": 0.0, "avg_logprob": -0.14890423040280396, "compression_ratio": 1.7117117117117118, "no_speech_prob": 4.984737824997865e-05}, {"id": 12, "seek": 5340, "start": 76.44, "end": 82.02, "text": " number of labels. And we'll focus more on generative modeling. Generative modeling means", "tokens": [1230, 295, 16949, 13, 400, 321, 603, 1879, 544, 322, 1337, 1166, 15983, 13, 15409, 1166, 15983, 1355], "temperature": 0.0, "avg_logprob": -0.14890423040280396, "compression_ratio": 1.7117117117117118, "no_speech_prob": 4.984737824997865e-05}, {"id": 13, "seek": 8202, "start": 82.02, "end": 90.08, "text": " predicting kind of lots of things. For example, creating a sentence such as in neural translation", "tokens": [32884, 733, 295, 3195, 295, 721, 13, 1171, 1365, 11, 4084, 257, 8174, 1270, 382, 294, 18161, 12853], "temperature": 0.0, "avg_logprob": -0.18783006072044373, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.506959208403714e-05}, {"id": 14, "seek": 8202, "start": 90.08, "end": 98.88, "text": " or image captioning or question answering or creating an image such as in style transfer,", "tokens": [420, 3256, 31974, 278, 420, 1168, 13430, 420, 4084, 364, 3256, 1270, 382, 294, 3758, 5003, 11], "temperature": 0.0, "avg_logprob": -0.18783006072044373, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.506959208403714e-05}, {"id": 15, "seek": 8202, "start": 98.88, "end": 108.6, "text": " super resolution, segmentation and so forth. And then in part 2, it'll move away from being", "tokens": [1687, 8669, 11, 9469, 399, 293, 370, 5220, 13, 400, 550, 294, 644, 568, 11, 309, 603, 1286, 1314, 490, 885], "temperature": 0.0, "avg_logprob": -0.18783006072044373, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.506959208403714e-05}, {"id": 16, "seek": 10860, "start": 108.6, "end": 114.39999999999999, "text": " just here are some best practices, established best practices either through people that", "tokens": [445, 510, 366, 512, 1151, 7525, 11, 7545, 1151, 7525, 2139, 807, 561, 300], "temperature": 0.0, "avg_logprob": -0.16587576967604617, "compression_ratio": 1.7588932806324111, "no_speech_prob": 4.330691081122495e-05}, {"id": 17, "seek": 10860, "start": 114.39999999999999, "end": 118.96, "text": " have written papers or through research that Fast AI has done and kind of got convinced", "tokens": [362, 3720, 10577, 420, 807, 2132, 300, 15968, 7318, 575, 1096, 293, 733, 295, 658, 12561], "temperature": 0.0, "avg_logprob": -0.16587576967604617, "compression_ratio": 1.7588932806324111, "no_speech_prob": 4.330691081122495e-05}, {"id": 18, "seek": 10860, "start": 118.96, "end": 124.6, "text": " that these are best practices to some stuff which will be a little bit more speculative.", "tokens": [300, 613, 366, 1151, 7525, 281, 512, 1507, 597, 486, 312, 257, 707, 857, 544, 49415, 13], "temperature": 0.0, "avg_logprob": -0.16587576967604617, "compression_ratio": 1.7588932806324111, "no_speech_prob": 4.330691081122495e-05}, {"id": 19, "seek": 10860, "start": 124.6, "end": 131.68, "text": " Some stuff which is maybe recent papers that haven't been fully tested yet and sometimes", "tokens": [2188, 1507, 597, 307, 1310, 5162, 10577, 300, 2378, 380, 668, 4498, 8246, 1939, 293, 2171], "temperature": 0.0, "avg_logprob": -0.16587576967604617, "compression_ratio": 1.7588932806324111, "no_speech_prob": 4.330691081122495e-05}, {"id": 20, "seek": 10860, "start": 131.68, "end": 136.24, "text": " in part 2 papers will come out in the middle of the course and we'll change direction with", "tokens": [294, 644, 568, 10577, 486, 808, 484, 294, 264, 2808, 295, 264, 1164, 293, 321, 603, 1319, 3513, 365], "temperature": 0.0, "avg_logprob": -0.16587576967604617, "compression_ratio": 1.7588932806324111, "no_speech_prob": 4.330691081122495e-05}, {"id": 21, "seek": 13624, "start": 136.24, "end": 140.8, "text": " the course and study that paper because it's just interesting. And so if you're interested", "tokens": [264, 1164, 293, 2979, 300, 3035, 570, 309, 311, 445, 1880, 13, 400, 370, 498, 291, 434, 3102], "temperature": 0.0, "avg_logprob": -0.14679192523567045, "compression_ratio": 1.65, "no_speech_prob": 5.7382407248951495e-05}, {"id": 22, "seek": 13624, "start": 140.8, "end": 147.28, "text": " in kind of learning a bit more about how to read a paper and how to implement it from", "tokens": [294, 733, 295, 2539, 257, 857, 544, 466, 577, 281, 1401, 257, 3035, 293, 577, 281, 4445, 309, 490], "temperature": 0.0, "avg_logprob": -0.14679192523567045, "compression_ratio": 1.65, "no_speech_prob": 5.7382407248951495e-05}, {"id": 23, "seek": 13624, "start": 147.28, "end": 152.64000000000001, "text": " scratch and so forth, then that's another good reason to do part 2.", "tokens": [8459, 293, 370, 5220, 11, 550, 300, 311, 1071, 665, 1778, 281, 360, 644, 568, 13], "temperature": 0.0, "avg_logprob": -0.14679192523567045, "compression_ratio": 1.65, "no_speech_prob": 5.7382407248951495e-05}, {"id": 24, "seek": 13624, "start": 152.64000000000001, "end": 159.44, "text": " It still doesn't assume any particular math background, beyond kind of high school, but", "tokens": [467, 920, 1177, 380, 6552, 604, 1729, 5221, 3678, 11, 4399, 733, 295, 1090, 1395, 11, 457], "temperature": 0.0, "avg_logprob": -0.14679192523567045, "compression_ratio": 1.65, "no_speech_prob": 5.7382407248951495e-05}, {"id": 25, "seek": 13624, "start": 159.44, "end": 166.16000000000003, "text": " it does assume that you're prepared to spend time digging through the notation and understanding", "tokens": [309, 775, 6552, 300, 291, 434, 4927, 281, 3496, 565, 17343, 807, 264, 24657, 293, 3701], "temperature": 0.0, "avg_logprob": -0.14679192523567045, "compression_ratio": 1.65, "no_speech_prob": 5.7382407248951495e-05}, {"id": 26, "seek": 16616, "start": 166.16, "end": 170.88, "text": " it and converting it to code and so forth.", "tokens": [309, 293, 29942, 309, 281, 3089, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1334632303296905, "compression_ratio": 1.632034632034632, "no_speech_prob": 3.705128256115131e-05}, {"id": 27, "seek": 16616, "start": 170.88, "end": 177.96, "text": " So where we're up to is RNNs at the moment. I think one of the issues I find most with", "tokens": [407, 689, 321, 434, 493, 281, 307, 45702, 45, 82, 412, 264, 1623, 13, 286, 519, 472, 295, 264, 2663, 286, 915, 881, 365], "temperature": 0.0, "avg_logprob": -0.1334632303296905, "compression_ratio": 1.632034632034632, "no_speech_prob": 3.705128256115131e-05}, {"id": 28, "seek": 16616, "start": 177.96, "end": 183.96, "text": " teaching RNNs is trying to ensure that people understand they're not in any way different", "tokens": [4571, 45702, 45, 82, 307, 1382, 281, 5586, 300, 561, 1223, 436, 434, 406, 294, 604, 636, 819], "temperature": 0.0, "avg_logprob": -0.1334632303296905, "compression_ratio": 1.632034632034632, "no_speech_prob": 3.705128256115131e-05}, {"id": 29, "seek": 16616, "start": 183.96, "end": 191.68, "text": " or unusual or magical. They're just a standard, fully connected network. And so let's go back", "tokens": [420, 10901, 420, 12066, 13, 814, 434, 445, 257, 3832, 11, 4498, 4582, 3209, 13, 400, 370, 718, 311, 352, 646], "temperature": 0.0, "avg_logprob": -0.1334632303296905, "compression_ratio": 1.632034632034632, "no_speech_prob": 3.705128256115131e-05}, {"id": 30, "seek": 16616, "start": 191.68, "end": 194.8, "text": " to the standard, fully connected network which looks like this.", "tokens": [281, 264, 3832, 11, 4498, 4582, 3209, 597, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1334632303296905, "compression_ratio": 1.632034632034632, "no_speech_prob": 3.705128256115131e-05}, {"id": 31, "seek": 19480, "start": 194.8, "end": 202.04000000000002, "text": " So to remind you, the arrows represent one or more layer operations, generally speaking", "tokens": [407, 281, 4160, 291, 11, 264, 19669, 2906, 472, 420, 544, 4583, 7705, 11, 5101, 4124], "temperature": 0.0, "avg_logprob": -0.1888946102511498, "compression_ratio": 1.5086705202312138, "no_speech_prob": 3.535395808285102e-05}, {"id": 32, "seek": 19480, "start": 202.04000000000002, "end": 208.68, "text": " a linear followed by a non-linear function. In this case they're matrix multiplications", "tokens": [257, 8213, 6263, 538, 257, 2107, 12, 28263, 2445, 13, 682, 341, 1389, 436, 434, 8141, 17596, 763], "temperature": 0.0, "avg_logprob": -0.1888946102511498, "compression_ratio": 1.5086705202312138, "no_speech_prob": 3.535395808285102e-05}, {"id": 33, "seek": 19480, "start": 208.68, "end": 218.4, "text": " followed by ReLU or THAM. And the arrows of the same color represent exactly the same", "tokens": [6263, 538, 1300, 43, 52, 420, 3578, 2865, 13, 400, 264, 19669, 295, 264, 912, 2017, 2906, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.1888946102511498, "compression_ratio": 1.5086705202312138, "no_speech_prob": 3.535395808285102e-05}, {"id": 34, "seek": 21840, "start": 218.4, "end": 225.04000000000002, "text": " weight matrix being used. And so one thing which was just slightly different from previous", "tokens": [3364, 8141, 885, 1143, 13, 400, 370, 472, 551, 597, 390, 445, 4748, 819, 490, 3894], "temperature": 0.0, "avg_logprob": -0.16332433621088663, "compression_ratio": 1.7574468085106383, "no_speech_prob": 1.0289250894857105e-05}, {"id": 35, "seek": 21840, "start": 225.04000000000002, "end": 232.12, "text": " fully connected networks we've seen is that we have an input coming in not just at the", "tokens": [4498, 4582, 9590, 321, 600, 1612, 307, 300, 321, 362, 364, 4846, 1348, 294, 406, 445, 412, 264], "temperature": 0.0, "avg_logprob": -0.16332433621088663, "compression_ratio": 1.7574468085106383, "no_speech_prob": 1.0289250894857105e-05}, {"id": 36, "seek": 21840, "start": 232.12, "end": 235.38, "text": " first layer, but also at the second layer and also at the third layer. And we tried", "tokens": [700, 4583, 11, 457, 611, 412, 264, 1150, 4583, 293, 611, 412, 264, 2636, 4583, 13, 400, 321, 3031], "temperature": 0.0, "avg_logprob": -0.16332433621088663, "compression_ratio": 1.7574468085106383, "no_speech_prob": 1.0289250894857105e-05}, {"id": 37, "seek": 21840, "start": 235.38, "end": 240.12, "text": " a couple of approaches. One was concatenating the inputs and one was adding the inputs.", "tokens": [257, 1916, 295, 11587, 13, 1485, 390, 1588, 7186, 990, 264, 15743, 293, 472, 390, 5127, 264, 15743, 13], "temperature": 0.0, "avg_logprob": -0.16332433621088663, "compression_ratio": 1.7574468085106383, "no_speech_prob": 1.0289250894857105e-05}, {"id": 38, "seek": 21840, "start": 240.12, "end": 246.64000000000001, "text": " But there was nothing at all conceptually different about this.", "tokens": [583, 456, 390, 1825, 412, 439, 3410, 671, 819, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.16332433621088663, "compression_ratio": 1.7574468085106383, "no_speech_prob": 1.0289250894857105e-05}, {"id": 39, "seek": 24664, "start": 246.64, "end": 258.08, "text": " So that code looked like this. We had a model where we basically defined the 3 arrows colors", "tokens": [407, 300, 3089, 2956, 411, 341, 13, 492, 632, 257, 2316, 689, 321, 1936, 7642, 264, 805, 19669, 4577], "temperature": 0.0, "avg_logprob": -0.10117571694510323, "compression_ratio": 1.5142857142857142, "no_speech_prob": 2.9479986096703215e-06}, {"id": 40, "seek": 24664, "start": 258.08, "end": 266.08, "text": " we had as 3 different weight matrices. And by using the linear class we got actually", "tokens": [321, 632, 382, 805, 819, 3364, 32284, 13, 400, 538, 1228, 264, 8213, 1508, 321, 658, 767], "temperature": 0.0, "avg_logprob": -0.10117571694510323, "compression_ratio": 1.5142857142857142, "no_speech_prob": 2.9479986096703215e-06}, {"id": 41, "seek": 24664, "start": 266.08, "end": 272.36, "text": " both the weight matrix and the bias vector wrapped up for free for us. And then we went", "tokens": [1293, 264, 3364, 8141, 293, 264, 12577, 8062, 14226, 493, 337, 1737, 337, 505, 13, 400, 550, 321, 1437], "temperature": 0.0, "avg_logprob": -0.10117571694510323, "compression_ratio": 1.5142857142857142, "no_speech_prob": 2.9479986096703215e-06}, {"id": 42, "seek": 27236, "start": 272.36, "end": 279.64, "text": " through and we did each of our embeddings, put it through our first linear layer, and", "tokens": [807, 293, 321, 630, 1184, 295, 527, 12240, 29432, 11, 829, 309, 807, 527, 700, 8213, 4583, 11, 293], "temperature": 0.0, "avg_logprob": -0.16628950768774683, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.0129931979463436e-05}, {"id": 43, "seek": 27236, "start": 279.64, "end": 289.12, "text": " then we did each of our, we call them, hidden, I think they were orange arrows. And in order", "tokens": [550, 321, 630, 1184, 295, 527, 11, 321, 818, 552, 11, 7633, 11, 286, 519, 436, 645, 7671, 19669, 13, 400, 294, 1668], "temperature": 0.0, "avg_logprob": -0.16628950768774683, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.0129931979463436e-05}, {"id": 44, "seek": 27236, "start": 289.12, "end": 295.96000000000004, "text": " to avoid the fact that there's no orange arrow coming into the first one, we decided to kind", "tokens": [281, 5042, 264, 1186, 300, 456, 311, 572, 7671, 11610, 1348, 666, 264, 700, 472, 11, 321, 3047, 281, 733], "temperature": 0.0, "avg_logprob": -0.16628950768774683, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.0129931979463436e-05}, {"id": 45, "seek": 27236, "start": 295.96000000000004, "end": 301.56, "text": " of invent an empty matrix and that way every one of these rows looked the same. And so", "tokens": [295, 7962, 364, 6707, 8141, 293, 300, 636, 633, 472, 295, 613, 13241, 2956, 264, 912, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.16628950768774683, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.0129931979463436e-05}, {"id": 46, "seek": 30156, "start": 301.56, "end": 311.92, "text": " then we did exactly the same thing except we used a loop just to refactor the code.", "tokens": [550, 321, 630, 2293, 264, 912, 551, 3993, 321, 1143, 257, 6367, 445, 281, 1895, 15104, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1310292040364126, "compression_ratio": 1.6355140186915889, "no_speech_prob": 6.3391676121682394e-06}, {"id": 47, "seek": 30156, "start": 311.92, "end": 319.0, "text": " It was just a code refactoring. There was no change of anything conceptually. And since", "tokens": [467, 390, 445, 257, 3089, 1895, 578, 3662, 13, 821, 390, 572, 1319, 295, 1340, 3410, 671, 13, 400, 1670], "temperature": 0.0, "avg_logprob": -0.1310292040364126, "compression_ratio": 1.6355140186915889, "no_speech_prob": 6.3391676121682394e-06}, {"id": 48, "seek": 30156, "start": 319.0, "end": 323.86, "text": " we were doing a refactoring, we took advantage of that to increase the number of characters", "tokens": [321, 645, 884, 257, 1895, 578, 3662, 11, 321, 1890, 5002, 295, 300, 281, 3488, 264, 1230, 295, 4342], "temperature": 0.0, "avg_logprob": -0.1310292040364126, "compression_ratio": 1.6355140186915889, "no_speech_prob": 6.3391676121682394e-06}, {"id": 49, "seek": 30156, "start": 323.86, "end": 329.6, "text": " to 8 because I was too lazy to type 8 linear layers, but I'm quite happy to change the", "tokens": [281, 1649, 570, 286, 390, 886, 14847, 281, 2010, 1649, 8213, 7914, 11, 457, 286, 478, 1596, 2055, 281, 1319, 264], "temperature": 0.0, "avg_logprob": -0.1310292040364126, "compression_ratio": 1.6355140186915889, "no_speech_prob": 6.3391676121682394e-06}, {"id": 50, "seek": 32960, "start": 329.6, "end": 337.04, "text": " loop index to 8. So this now loops through this exact same thing, but we had 8 of these", "tokens": [6367, 8186, 281, 1649, 13, 407, 341, 586, 16121, 807, 341, 1900, 912, 551, 11, 457, 321, 632, 1649, 295, 613], "temperature": 0.0, "avg_logprob": -0.1385781411771421, "compression_ratio": 1.364963503649635, "no_speech_prob": 1.3709538961848011e-06}, {"id": 51, "seek": 32960, "start": 337.04, "end": 343.34000000000003, "text": " rather than 3.", "tokens": [2831, 813, 805, 13], "temperature": 0.0, "avg_logprob": -0.1385781411771421, "compression_ratio": 1.364963503649635, "no_speech_prob": 1.3709538961848011e-06}, {"id": 52, "seek": 32960, "start": 343.34000000000003, "end": 349.92, "text": " So then we refactored that again by taking advantage of nn.rnn, which basically puts", "tokens": [407, 550, 321, 1895, 578, 2769, 300, 797, 538, 1940, 5002, 295, 297, 77, 13, 81, 26384, 11, 597, 1936, 8137], "temperature": 0.0, "avg_logprob": -0.1385781411771421, "compression_ratio": 1.364963503649635, "no_speech_prob": 1.3709538961848011e-06}, {"id": 53, "seek": 34992, "start": 349.92, "end": 360.56, "text": " that loop together for us and keeps track of this h as it goes along for us. And so", "tokens": [300, 6367, 1214, 337, 505, 293, 5965, 2837, 295, 341, 276, 382, 309, 1709, 2051, 337, 505, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.15309110234995357, "compression_ratio": 1.4899328859060403, "no_speech_prob": 2.902305823226925e-06}, {"id": 54, "seek": 34992, "start": 360.56, "end": 367.24, "text": " by using that, we were able to replace the loop with a single colon. And so again, that's", "tokens": [538, 1228, 300, 11, 321, 645, 1075, 281, 7406, 264, 6367, 365, 257, 2167, 8255, 13, 400, 370, 797, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.15309110234995357, "compression_ratio": 1.4899328859060403, "no_speech_prob": 2.902305823226925e-06}, {"id": 55, "seek": 34992, "start": 367.24, "end": 375.68, "text": " just a refactoring doing exactly the same thing.", "tokens": [445, 257, 1895, 578, 3662, 884, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15309110234995357, "compression_ratio": 1.4899328859060403, "no_speech_prob": 2.902305823226925e-06}, {"id": 56, "seek": 37568, "start": 375.68, "end": 383.56, "text": " So then we looked at something which was mainly designed to save some training time, which", "tokens": [407, 550, 321, 2956, 412, 746, 597, 390, 8704, 4761, 281, 3155, 512, 3097, 565, 11, 597], "temperature": 0.0, "avg_logprob": -0.1497257792431375, "compression_ratio": 1.4094488188976377, "no_speech_prob": 1.3925449593443773e-06}, {"id": 57, "seek": 37568, "start": 383.56, "end": 398.2, "text": " was previously, if we had a big piece of text, so we've got like a movie review, we were", "tokens": [390, 8046, 11, 498, 321, 632, 257, 955, 2522, 295, 2487, 11, 370, 321, 600, 658, 411, 257, 3169, 3131, 11, 321, 645], "temperature": 0.0, "avg_logprob": -0.1497257792431375, "compression_ratio": 1.4094488188976377, "no_speech_prob": 1.3925449593443773e-06}, {"id": 58, "seek": 39820, "start": 398.2, "end": 406.44, "text": " basically splitting it up into 8-character segments and we'd grab segment number 1 and", "tokens": [1936, 30348, 309, 493, 666, 1649, 12, 7374, 14125, 19904, 293, 321, 1116, 4444, 9469, 1230, 502, 293], "temperature": 0.0, "avg_logprob": -0.1952986717224121, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.9944330890430138e-06}, {"id": 59, "seek": 39820, "start": 406.44, "end": 413.76, "text": " use that to predict the next character. But in order to make sure that we used all of", "tokens": [764, 300, 281, 6069, 264, 958, 2517, 13, 583, 294, 1668, 281, 652, 988, 300, 321, 1143, 439, 295], "temperature": 0.0, "avg_logprob": -0.1952986717224121, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.9944330890430138e-06}, {"id": 60, "seek": 39820, "start": 413.76, "end": 421.48, "text": " the data, we didn't just split it up like that. We actually said, here's our whole thing,", "tokens": [264, 1412, 11, 321, 994, 380, 445, 7472, 309, 493, 411, 300, 13, 492, 767, 848, 11, 510, 311, 527, 1379, 551, 11], "temperature": 0.0, "avg_logprob": -0.1952986717224121, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.9944330890430138e-06}, {"id": 61, "seek": 39820, "start": 421.48, "end": 426.96, "text": " let's grab, the first will be to grab this section, the second will be to grab that section,", "tokens": [718, 311, 4444, 11, 264, 700, 486, 312, 281, 4444, 341, 3541, 11, 264, 1150, 486, 312, 281, 4444, 300, 3541, 11], "temperature": 0.0, "avg_logprob": -0.1952986717224121, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.9944330890430138e-06}, {"id": 62, "seek": 42696, "start": 426.96, "end": 432.91999999999996, "text": " then that section, then that section, and each time we're predicting the next one character", "tokens": [550, 300, 3541, 11, 550, 300, 3541, 11, 293, 1184, 565, 321, 434, 32884, 264, 958, 472, 2517], "temperature": 0.0, "avg_logprob": -0.1671839614412678, "compression_ratio": 1.5747126436781609, "no_speech_prob": 6.144158305687597e-06}, {"id": 63, "seek": 42696, "start": 432.91999999999996, "end": 434.59999999999997, "text": " along.", "tokens": [2051, 13], "temperature": 0.0, "avg_logprob": -0.1671839614412678, "compression_ratio": 1.5747126436781609, "no_speech_prob": 6.144158305687597e-06}, {"id": 64, "seek": 42696, "start": 434.59999999999997, "end": 441.59999999999997, "text": " And so I was a bit concerned that that seems pretty wasteful because as we calculate this", "tokens": [400, 370, 286, 390, 257, 857, 5922, 300, 300, 2544, 1238, 5964, 906, 570, 382, 321, 8873, 341], "temperature": 0.0, "avg_logprob": -0.1671839614412678, "compression_ratio": 1.5747126436781609, "no_speech_prob": 6.144158305687597e-06}, {"id": 65, "seek": 42696, "start": 441.59999999999997, "end": 448.59999999999997, "text": " section, nearly all of it overlaps with the previous section. So instead, what we did", "tokens": [3541, 11, 6217, 439, 295, 309, 15986, 2382, 365, 264, 3894, 3541, 13, 407, 2602, 11, 437, 321, 630], "temperature": 0.0, "avg_logprob": -0.1671839614412678, "compression_ratio": 1.5747126436781609, "no_speech_prob": 6.144158305687597e-06}, {"id": 66, "seek": 44860, "start": 448.6, "end": 457.68, "text": " was we said, what if we actually did split it into non-overlapping pieces, and we said", "tokens": [390, 321, 848, 11, 437, 498, 321, 767, 630, 7472, 309, 666, 2107, 12, 3570, 15639, 3755, 11, 293, 321, 848], "temperature": 0.0, "avg_logprob": -0.15046846505367395, "compression_ratio": 1.8992805755395683, "no_speech_prob": 4.029448064102326e-06}, {"id": 67, "seek": 44860, "start": 457.68, "end": 469.24, "text": " let's grab this section here and use it to predict every one of the characters one along.", "tokens": [718, 311, 4444, 341, 3541, 510, 293, 764, 309, 281, 6069, 633, 472, 295, 264, 4342, 472, 2051, 13], "temperature": 0.0, "avg_logprob": -0.15046846505367395, "compression_ratio": 1.8992805755395683, "no_speech_prob": 4.029448064102326e-06}, {"id": 68, "seek": 44860, "start": 469.24, "end": 473.62, "text": " And then let's grab this section here and use it to predict every one of the characters", "tokens": [400, 550, 718, 311, 4444, 341, 3541, 510, 293, 764, 309, 281, 6069, 633, 472, 295, 264, 4342], "temperature": 0.0, "avg_logprob": -0.15046846505367395, "compression_ratio": 1.8992805755395683, "no_speech_prob": 4.029448064102326e-06}, {"id": 69, "seek": 47362, "start": 473.62, "end": 479.28000000000003, "text": " one along. So after we look at the first character in, we try to predict the second character.", "tokens": [472, 2051, 13, 407, 934, 321, 574, 412, 264, 700, 2517, 294, 11, 321, 853, 281, 6069, 264, 1150, 2517, 13], "temperature": 0.0, "avg_logprob": -0.17183942692254178, "compression_ratio": 1.878787878787879, "no_speech_prob": 3.4465558655938366e-06}, {"id": 70, "seek": 47362, "start": 479.28000000000003, "end": 483.0, "text": " And then after we look at the second character, we try to predict the third character, and", "tokens": [400, 550, 934, 321, 574, 412, 264, 1150, 2517, 11, 321, 853, 281, 6069, 264, 2636, 2517, 11, 293], "temperature": 0.0, "avg_logprob": -0.17183942692254178, "compression_ratio": 1.878787878787879, "no_speech_prob": 3.4465558655938366e-06}, {"id": 71, "seek": 47362, "start": 483.0, "end": 484.0, "text": " so forth.", "tokens": [370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17183942692254178, "compression_ratio": 1.878787878787879, "no_speech_prob": 3.4465558655938366e-06}, {"id": 72, "seek": 47362, "start": 484.0, "end": 490.16, "text": " So that's where we got to. And then one of you perceptive folks asked a really interesting", "tokens": [407, 300, 311, 689, 321, 658, 281, 13, 400, 550, 472, 295, 291, 43276, 488, 4024, 2351, 257, 534, 1880], "temperature": 0.0, "avg_logprob": -0.17183942692254178, "compression_ratio": 1.878787878787879, "no_speech_prob": 3.4465558655938366e-06}, {"id": 73, "seek": 47362, "start": 490.16, "end": 496.84000000000003, "text": " question or expressed a concern, which was, hey, after we got through the first point", "tokens": [1168, 420, 12675, 257, 3136, 11, 597, 390, 11, 4177, 11, 934, 321, 658, 807, 264, 700, 935], "temperature": 0.0, "avg_logprob": -0.17183942692254178, "compression_ratio": 1.878787878787879, "no_speech_prob": 3.4465558655938366e-06}, {"id": 74, "seek": 49684, "start": 496.84, "end": 509.35999999999996, "text": " here, after we got through the first point here, we kind of threw away our H activations", "tokens": [510, 11, 934, 321, 658, 807, 264, 700, 935, 510, 11, 321, 733, 295, 11918, 1314, 527, 389, 2430, 763], "temperature": 0.0, "avg_logprob": -0.1295144730719967, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.4823549463471863e-06}, {"id": 75, "seek": 49684, "start": 509.35999999999996, "end": 516.0, "text": " and started a new one, which meant that when it was trying to use character one to predict", "tokens": [293, 1409, 257, 777, 472, 11, 597, 4140, 300, 562, 309, 390, 1382, 281, 764, 2517, 472, 281, 6069], "temperature": 0.0, "avg_logprob": -0.1295144730719967, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.4823549463471863e-06}, {"id": 76, "seek": 49684, "start": 516.0, "end": 524.64, "text": " character two, it's got nothing to go on. It's only done one linear layer, and so that", "tokens": [2517, 732, 11, 309, 311, 658, 1825, 281, 352, 322, 13, 467, 311, 787, 1096, 472, 8213, 4583, 11, 293, 370, 300], "temperature": 0.0, "avg_logprob": -0.1295144730719967, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.4823549463471863e-06}, {"id": 77, "seek": 52464, "start": 524.64, "end": 529.24, "text": " seems like a problem, which indeed it is.", "tokens": [2544, 411, 257, 1154, 11, 597, 6451, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1149086518721147, "compression_ratio": 1.6524064171122994, "no_speech_prob": 1.3925449593443773e-06}, {"id": 78, "seek": 52464, "start": 529.24, "end": 535.9399999999999, "text": " So we're going to do the obvious thing, which is let's not throw away H. So let's not throw", "tokens": [407, 321, 434, 516, 281, 360, 264, 6322, 551, 11, 597, 307, 718, 311, 406, 3507, 1314, 389, 13, 407, 718, 311, 406, 3507], "temperature": 0.0, "avg_logprob": -0.1149086518721147, "compression_ratio": 1.6524064171122994, "no_speech_prob": 1.3925449593443773e-06}, {"id": 79, "seek": 52464, "start": 535.9399999999999, "end": 546.68, "text": " away that matrix at all. So in code, the big problem is here. Every time we call forward,", "tokens": [1314, 300, 8141, 412, 439, 13, 407, 294, 3089, 11, 264, 955, 1154, 307, 510, 13, 2048, 565, 321, 818, 2128, 11], "temperature": 0.0, "avg_logprob": -0.1149086518721147, "compression_ratio": 1.6524064171122994, "no_speech_prob": 1.3925449593443773e-06}, {"id": 80, "seek": 52464, "start": 546.68, "end": 553.8, "text": " so in other words, every time we do a new minibatch, we're creating our hidden state,", "tokens": [370, 294, 661, 2283, 11, 633, 565, 321, 360, 257, 777, 923, 897, 852, 11, 321, 434, 4084, 527, 7633, 1785, 11], "temperature": 0.0, "avg_logprob": -0.1149086518721147, "compression_ratio": 1.6524064171122994, "no_speech_prob": 1.3925449593443773e-06}, {"id": 81, "seek": 55380, "start": 553.8, "end": 560.4, "text": " which remember is the orange circles. We're resetting it back to a bunch of zeros. And", "tokens": [597, 1604, 307, 264, 7671, 13040, 13, 492, 434, 14322, 783, 309, 646, 281, 257, 3840, 295, 35193, 13, 400], "temperature": 0.0, "avg_logprob": -0.1337846055322764, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.078326295100851e-05}, {"id": 82, "seek": 55380, "start": 560.4, "end": 565.04, "text": " so as we go to the next non-overlapping section, we're saying, forget everything that's come", "tokens": [370, 382, 321, 352, 281, 264, 958, 2107, 12, 3570, 15639, 3541, 11, 321, 434, 1566, 11, 2870, 1203, 300, 311, 808], "temperature": 0.0, "avg_logprob": -0.1337846055322764, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.078326295100851e-05}, {"id": 83, "seek": 55380, "start": 565.04, "end": 570.4, "text": " before. But in fact, the whole point is we know exactly where we are. We're at the end", "tokens": [949, 13, 583, 294, 1186, 11, 264, 1379, 935, 307, 321, 458, 2293, 689, 321, 366, 13, 492, 434, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.1337846055322764, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.078326295100851e-05}, {"id": 84, "seek": 55380, "start": 570.4, "end": 574.16, "text": " of the previous section and about to start the next contiguous section, so let's not", "tokens": [295, 264, 3894, 3541, 293, 466, 281, 722, 264, 958, 660, 30525, 3541, 11, 370, 718, 311, 406], "temperature": 0.0, "avg_logprob": -0.1337846055322764, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.078326295100851e-05}, {"id": 85, "seek": 55380, "start": 574.16, "end": 575.5, "text": " throw it away.", "tokens": [3507, 309, 1314, 13], "temperature": 0.0, "avg_logprob": -0.1337846055322764, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.078326295100851e-05}, {"id": 86, "seek": 57550, "start": 575.5, "end": 587.32, "text": " So instead, the idea would be to cut this out, move it up to here, store it away in", "tokens": [407, 2602, 11, 264, 1558, 576, 312, 281, 1723, 341, 484, 11, 1286, 309, 493, 281, 510, 11, 3531, 309, 1314, 294], "temperature": 0.0, "avg_logprob": -0.10930540522591012, "compression_ratio": 1.4246575342465753, "no_speech_prob": 2.521570650060312e-06}, {"id": 87, "seek": 57550, "start": 587.32, "end": 593.72, "text": " self, and then kind of keep updating it. So we're going to do that. And there's going", "tokens": [2698, 11, 293, 550, 733, 295, 1066, 25113, 309, 13, 407, 321, 434, 516, 281, 360, 300, 13, 400, 456, 311, 516], "temperature": 0.0, "avg_logprob": -0.10930540522591012, "compression_ratio": 1.4246575342465753, "no_speech_prob": 2.521570650060312e-06}, {"id": 88, "seek": 57550, "start": 593.72, "end": 598.52, "text": " to be some minor details to get right.", "tokens": [281, 312, 512, 6696, 4365, 281, 483, 558, 13], "temperature": 0.0, "avg_logprob": -0.10930540522591012, "compression_ratio": 1.4246575342465753, "no_speech_prob": 2.521570650060312e-06}, {"id": 89, "seek": 59852, "start": 598.52, "end": 610.56, "text": " So let's start by looking at the model. So here's the model. It's nearly identical. Here's", "tokens": [407, 718, 311, 722, 538, 1237, 412, 264, 2316, 13, 407, 510, 311, 264, 2316, 13, 467, 311, 6217, 14800, 13, 1692, 311], "temperature": 0.0, "avg_logprob": -0.1573000635419573, "compression_ratio": 1.729032258064516, "no_speech_prob": 2.2603230718232226e-06}, {"id": 90, "seek": 59852, "start": 610.56, "end": 616.96, "text": " the model. It's nearly identical, but I've got as expected one more line in my constructor", "tokens": [264, 2316, 13, 467, 311, 6217, 14800, 11, 457, 286, 600, 658, 382, 5176, 472, 544, 1622, 294, 452, 47479], "temperature": 0.0, "avg_logprob": -0.1573000635419573, "compression_ratio": 1.729032258064516, "no_speech_prob": 2.2603230718232226e-06}, {"id": 91, "seek": 59852, "start": 616.96, "end": 624.0, "text": " where I call something called init hidden. And as expected, init hidden sets self.h to", "tokens": [689, 286, 818, 746, 1219, 3157, 7633, 13, 400, 382, 5176, 11, 3157, 7633, 6352, 2698, 13, 71, 281], "temperature": 0.0, "avg_logprob": -0.1573000635419573, "compression_ratio": 1.729032258064516, "no_speech_prob": 2.2603230718232226e-06}, {"id": 92, "seek": 62400, "start": 624.0, "end": 636.68, "text": " be a bunch of zeros. So that's entirely unsurprising. And then as you can see, RNN now takes in", "tokens": [312, 257, 3840, 295, 35193, 13, 407, 300, 311, 7696, 2693, 374, 26203, 13, 400, 550, 382, 291, 393, 536, 11, 45702, 45, 586, 2516, 294], "temperature": 0.0, "avg_logprob": -0.15313567055596244, "compression_ratio": 1.3211678832116789, "no_speech_prob": 5.896413881600893e-07}, {"id": 93, "seek": 62400, "start": 636.68, "end": 645.72, "text": " self.h and it as before spits out our new hidden activations. And so now the trick is", "tokens": [2698, 13, 71, 293, 309, 382, 949, 637, 1208, 484, 527, 777, 7633, 2430, 763, 13, 400, 370, 586, 264, 4282, 307], "temperature": 0.0, "avg_logprob": -0.15313567055596244, "compression_ratio": 1.3211678832116789, "no_speech_prob": 5.896413881600893e-07}, {"id": 94, "seek": 64572, "start": 645.72, "end": 654.1600000000001, "text": " to now store that away inside self.h. And so here's wrinkle number one. If you think", "tokens": [281, 586, 3531, 300, 1314, 1854, 2698, 13, 71, 13, 400, 370, 510, 311, 928, 14095, 1230, 472, 13, 759, 291, 519], "temperature": 0.0, "avg_logprob": -0.19823645220862496, "compression_ratio": 1.457142857142857, "no_speech_prob": 1.8738730886980193e-06}, {"id": 95, "seek": 64572, "start": 654.1600000000001, "end": 664.28, "text": " about it, if I was to simply do it like that, and now I train this on a document that's", "tokens": [466, 309, 11, 498, 286, 390, 281, 2935, 360, 309, 411, 300, 11, 293, 586, 286, 3847, 341, 322, 257, 4166, 300, 311], "temperature": 0.0, "avg_logprob": -0.19823645220862496, "compression_ratio": 1.457142857142857, "no_speech_prob": 1.8738730886980193e-06}, {"id": 96, "seek": 64572, "start": 664.28, "end": 675.64, "text": " a million characters long, then the size of this unrolled RNN has a million zeros.", "tokens": [257, 2459, 4342, 938, 11, 550, 264, 2744, 295, 341, 517, 28850, 45702, 45, 575, 257, 2459, 35193, 13], "temperature": 0.0, "avg_logprob": -0.19823645220862496, "compression_ratio": 1.457142857142857, "no_speech_prob": 1.8738730886980193e-06}, {"id": 97, "seek": 67564, "start": 675.64, "end": 682.76, "text": " And so that's fine going forwards. But when I finally get to the end and I say here's", "tokens": [400, 370, 300, 311, 2489, 516, 30126, 13, 583, 562, 286, 2721, 483, 281, 264, 917, 293, 286, 584, 510, 311], "temperature": 0.0, "avg_logprob": -0.12941860571140196, "compression_ratio": 1.5257731958762886, "no_speech_prob": 8.801075637165923e-06}, {"id": 98, "seek": 67564, "start": 682.76, "end": 688.48, "text": " my character, and actually remember we're doing multi-output now. So multi-output looks", "tokens": [452, 2517, 11, 293, 767, 1604, 321, 434, 884, 4825, 12, 346, 2582, 586, 13, 407, 4825, 12, 346, 2582, 1542], "temperature": 0.0, "avg_logprob": -0.12941860571140196, "compression_ratio": 1.5257731958762886, "no_speech_prob": 8.801075637165923e-06}, {"id": 99, "seek": 67564, "start": 688.48, "end": 695.08, "text": " like this. Or if we were to draw the unrolled version of multi-output, we would have a triangle", "tokens": [411, 341, 13, 1610, 498, 321, 645, 281, 2642, 264, 517, 28850, 3037, 295, 4825, 12, 346, 2582, 11, 321, 576, 362, 257, 13369], "temperature": 0.0, "avg_logprob": -0.12941860571140196, "compression_ratio": 1.5257731958762886, "no_speech_prob": 8.801075637165923e-06}, {"id": 100, "seek": 67564, "start": 695.08, "end": 699.36, "text": " coming off at every point.", "tokens": [1348, 766, 412, 633, 935, 13], "temperature": 0.0, "avg_logprob": -0.12941860571140196, "compression_ratio": 1.5257731958762886, "no_speech_prob": 8.801075637165923e-06}, {"id": 101, "seek": 69936, "start": 699.36, "end": 707.16, "text": " So the problem is that then when we do backpropagation, we're calculating how much does the error", "tokens": [407, 264, 1154, 307, 300, 550, 562, 321, 360, 646, 79, 1513, 559, 399, 11, 321, 434, 28258, 577, 709, 775, 264, 6713], "temperature": 0.0, "avg_logprob": -0.13624629974365235, "compression_ratio": 1.778894472361809, "no_speech_prob": 2.7264643449598225e-06}, {"id": 102, "seek": 69936, "start": 707.16, "end": 714.0, "text": " at character 1 impact the final answer. How much does the error at character 2 impact", "tokens": [412, 2517, 502, 2712, 264, 2572, 1867, 13, 1012, 709, 775, 264, 6713, 412, 2517, 568, 2712], "temperature": 0.0, "avg_logprob": -0.13624629974365235, "compression_ratio": 1.778894472361809, "no_speech_prob": 2.7264643449598225e-06}, {"id": 103, "seek": 69936, "start": 714.0, "end": 719.88, "text": " the final answer, and so forth. And so we need to go back through and say how do we", "tokens": [264, 2572, 1867, 11, 293, 370, 5220, 13, 400, 370, 321, 643, 281, 352, 646, 807, 293, 584, 577, 360, 321], "temperature": 0.0, "avg_logprob": -0.13624629974365235, "compression_ratio": 1.778894472361809, "no_speech_prob": 2.7264643449598225e-06}, {"id": 104, "seek": 69936, "start": 719.88, "end": 726.8000000000001, "text": " have to update our weights based on all of those errors. And so if there are a million", "tokens": [362, 281, 5623, 527, 17443, 2361, 322, 439, 295, 729, 13603, 13, 400, 370, 498, 456, 366, 257, 2459], "temperature": 0.0, "avg_logprob": -0.13624629974365235, "compression_ratio": 1.778894472361809, "no_speech_prob": 2.7264643449598225e-06}, {"id": 105, "seek": 72680, "start": 726.8, "end": 735.3199999999999, "text": " characters, my unrolled RNN is a million layers long, I have a 1 million layer fully connected", "tokens": [4342, 11, 452, 517, 28850, 45702, 45, 307, 257, 2459, 7914, 938, 11, 286, 362, 257, 502, 2459, 4583, 4498, 4582], "temperature": 0.0, "avg_logprob": -0.18896958886123286, "compression_ratio": 1.7597765363128492, "no_speech_prob": 5.862797479494475e-06}, {"id": 106, "seek": 72680, "start": 735.3199999999999, "end": 740.4799999999999, "text": " network. And I didn't have to write the million layers because I have the for loop, and the", "tokens": [3209, 13, 400, 286, 994, 380, 362, 281, 2464, 264, 2459, 7914, 570, 286, 362, 264, 337, 6367, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.18896958886123286, "compression_ratio": 1.7597765363128492, "no_speech_prob": 5.862797479494475e-06}, {"id": 107, "seek": 72680, "start": 740.4799999999999, "end": 749.9599999999999, "text": " for loop is hidden away behind the self.rn, but it's still there. So this is actually", "tokens": [337, 6367, 307, 7633, 1314, 2261, 264, 2698, 13, 81, 77, 11, 457, 309, 311, 920, 456, 13, 407, 341, 307, 767], "temperature": 0.0, "avg_logprob": -0.18896958886123286, "compression_ratio": 1.7597765363128492, "no_speech_prob": 5.862797479494475e-06}, {"id": 108, "seek": 72680, "start": 749.9599999999999, "end": 752.64, "text": " a 1 million layer fully connected network.", "tokens": [257, 502, 2459, 4583, 4498, 4582, 3209, 13], "temperature": 0.0, "avg_logprob": -0.18896958886123286, "compression_ratio": 1.7597765363128492, "no_speech_prob": 5.862797479494475e-06}, {"id": 109, "seek": 75264, "start": 752.64, "end": 757.1999999999999, "text": " And so the problem with that is it's going to be very memory intensive because in order", "tokens": [400, 370, 264, 1154, 365, 300, 307, 309, 311, 516, 281, 312, 588, 4675, 18957, 570, 294, 1668], "temperature": 0.0, "avg_logprob": -0.19436363379160562, "compression_ratio": 1.751219512195122, "no_speech_prob": 2.99443377116404e-06}, {"id": 110, "seek": 75264, "start": 757.1999999999999, "end": 766.76, "text": " to do the chain rule, I have to be able to multiply at every step like f-u times g-x.", "tokens": [281, 360, 264, 5021, 4978, 11, 286, 362, 281, 312, 1075, 281, 12972, 412, 633, 1823, 411, 283, 12, 84, 1413, 290, 12, 87, 13], "temperature": 0.0, "avg_logprob": -0.19436363379160562, "compression_ratio": 1.751219512195122, "no_speech_prob": 2.99443377116404e-06}, {"id": 111, "seek": 75264, "start": 766.76, "end": 773.08, "text": " So I have to remember those values u, the value of every set of layers, so I'm going", "tokens": [407, 286, 362, 281, 1604, 729, 4190, 344, 11, 264, 2158, 295, 633, 992, 295, 7914, 11, 370, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.19436363379160562, "compression_ratio": 1.751219512195122, "no_speech_prob": 2.99443377116404e-06}, {"id": 112, "seek": 75264, "start": 773.08, "end": 777.72, "text": " to have to remember all those million layers. And I'm going to have to do a million multiplications,", "tokens": [281, 362, 281, 1604, 439, 729, 2459, 7914, 13, 400, 286, 478, 516, 281, 362, 281, 360, 257, 2459, 17596, 763, 11], "temperature": 0.0, "avg_logprob": -0.19436363379160562, "compression_ratio": 1.751219512195122, "no_speech_prob": 2.99443377116404e-06}, {"id": 113, "seek": 77772, "start": 777.72, "end": 783.4, "text": " and I'm going to have to do that every batch. So that would be bad.", "tokens": [293, 286, 478, 516, 281, 362, 281, 360, 300, 633, 15245, 13, 407, 300, 576, 312, 1578, 13], "temperature": 0.0, "avg_logprob": -0.1090143133954304, "compression_ratio": 1.682051282051282, "no_speech_prob": 3.156123398184718e-07}, {"id": 114, "seek": 77772, "start": 783.4, "end": 792.8000000000001, "text": " So to avoid that, we basically say, from time to time, I want you to forget your history.", "tokens": [407, 281, 5042, 300, 11, 321, 1936, 584, 11, 490, 565, 281, 565, 11, 286, 528, 291, 281, 2870, 428, 2503, 13], "temperature": 0.0, "avg_logprob": -0.1090143133954304, "compression_ratio": 1.682051282051282, "no_speech_prob": 3.156123398184718e-07}, {"id": 115, "seek": 77772, "start": 792.8000000000001, "end": 797.24, "text": " So we can still remember the state, which is to remember what's the actual values in", "tokens": [407, 321, 393, 920, 1604, 264, 1785, 11, 597, 307, 281, 1604, 437, 311, 264, 3539, 4190, 294], "temperature": 0.0, "avg_logprob": -0.1090143133954304, "compression_ratio": 1.682051282051282, "no_speech_prob": 3.156123398184718e-07}, {"id": 116, "seek": 77772, "start": 797.24, "end": 803.0, "text": " our hidden matrix, but we can remember the state without remembering everything about", "tokens": [527, 7633, 8141, 11, 457, 321, 393, 1604, 264, 1785, 1553, 20719, 1203, 466], "temperature": 0.0, "avg_logprob": -0.1090143133954304, "compression_ratio": 1.682051282051282, "no_speech_prob": 3.156123398184718e-07}, {"id": 117, "seek": 80300, "start": 803.0, "end": 815.8, "text": " how we got there. So there's a little function called repackage variable, which literally", "tokens": [577, 321, 658, 456, 13, 407, 456, 311, 257, 707, 2445, 1219, 1085, 501, 609, 7006, 11, 597, 3736], "temperature": 0.0, "avg_logprob": -0.11035105631901668, "compression_ratio": 1.497175141242938, "no_speech_prob": 3.1875572403805563e-06}, {"id": 118, "seek": 80300, "start": 815.8, "end": 824.44, "text": " is just this. It just simply says grab the tensor out of it, because remember the tensor", "tokens": [307, 445, 341, 13, 467, 445, 2935, 1619, 4444, 264, 40863, 484, 295, 309, 11, 570, 1604, 264, 40863], "temperature": 0.0, "avg_logprob": -0.11035105631901668, "compression_ratio": 1.497175141242938, "no_speech_prob": 3.1875572403805563e-06}, {"id": 119, "seek": 80300, "start": 824.44, "end": 830.24, "text": " itself doesn't have any concept of history, and create a new variable out of that. And", "tokens": [2564, 1177, 380, 362, 604, 3410, 295, 2503, 11, 293, 1884, 257, 777, 7006, 484, 295, 300, 13, 400], "temperature": 0.0, "avg_logprob": -0.11035105631901668, "compression_ratio": 1.497175141242938, "no_speech_prob": 3.1875572403805563e-06}, {"id": 120, "seek": 83024, "start": 830.24, "end": 837.04, "text": " so this variable is going to have the same value, but no history of operations, and therefore", "tokens": [370, 341, 7006, 307, 516, 281, 362, 264, 912, 2158, 11, 457, 572, 2503, 295, 7705, 11, 293, 4412], "temperature": 0.0, "avg_logprob": -0.11116554500820401, "compression_ratio": 1.8894009216589862, "no_speech_prob": 3.905473931808956e-06}, {"id": 121, "seek": 83024, "start": 837.04, "end": 841.2, "text": " when it tries to backpropagate, it'll stop there.", "tokens": [562, 309, 9898, 281, 646, 79, 1513, 559, 473, 11, 309, 603, 1590, 456, 13], "temperature": 0.0, "avg_logprob": -0.11116554500820401, "compression_ratio": 1.8894009216589862, "no_speech_prob": 3.905473931808956e-06}, {"id": 122, "seek": 83024, "start": 841.2, "end": 845.88, "text": " So basically what we're going to do then is we're going to call this in our forward. So", "tokens": [407, 1936, 437, 321, 434, 516, 281, 360, 550, 307, 321, 434, 516, 281, 818, 341, 294, 527, 2128, 13, 407], "temperature": 0.0, "avg_logprob": -0.11116554500820401, "compression_ratio": 1.8894009216589862, "no_speech_prob": 3.905473931808956e-06}, {"id": 123, "seek": 83024, "start": 845.88, "end": 853.72, "text": " that means it's going to do 8 characters, it's going to backpropagate through 8 layers,", "tokens": [300, 1355, 309, 311, 516, 281, 360, 1649, 4342, 11, 309, 311, 516, 281, 646, 79, 1513, 559, 473, 807, 1649, 7914, 11], "temperature": 0.0, "avg_logprob": -0.11116554500820401, "compression_ratio": 1.8894009216589862, "no_speech_prob": 3.905473931808956e-06}, {"id": 124, "seek": 83024, "start": 853.72, "end": 858.52, "text": " it's going to keep track of the actual values in our hidden state, but it's going to throw", "tokens": [309, 311, 516, 281, 1066, 2837, 295, 264, 3539, 4190, 294, 527, 7633, 1785, 11, 457, 309, 311, 516, 281, 3507], "temperature": 0.0, "avg_logprob": -0.11116554500820401, "compression_ratio": 1.8894009216589862, "no_speech_prob": 3.905473931808956e-06}, {"id": 125, "seek": 85852, "start": 858.52, "end": 864.0799999999999, "text": " away at the end of those 8 its history of operations.", "tokens": [1314, 412, 264, 917, 295, 729, 1649, 1080, 2503, 295, 7705, 13], "temperature": 0.0, "avg_logprob": -0.1198380317217038, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.289319349481957e-06}, {"id": 126, "seek": 85852, "start": 864.0799999999999, "end": 871.92, "text": " So this approach is called backprop through time. When you read about it online, people", "tokens": [407, 341, 3109, 307, 1219, 646, 79, 1513, 807, 565, 13, 1133, 291, 1401, 466, 309, 2950, 11, 561], "temperature": 0.0, "avg_logprob": -0.1198380317217038, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.289319349481957e-06}, {"id": 127, "seek": 85852, "start": 871.92, "end": 878.68, "text": " make it sound like a different algorithm or some big insight or something, but it's not", "tokens": [652, 309, 1626, 411, 257, 819, 9284, 420, 512, 955, 11269, 420, 746, 11, 457, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.1198380317217038, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.289319349481957e-06}, {"id": 128, "seek": 85852, "start": 878.68, "end": 886.22, "text": " at all. It's just saying, hey, after our for loop, just throw away your history of operations", "tokens": [412, 439, 13, 467, 311, 445, 1566, 11, 4177, 11, 934, 527, 337, 6367, 11, 445, 3507, 1314, 428, 2503, 295, 7705], "temperature": 0.0, "avg_logprob": -0.1198380317217038, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.289319349481957e-06}, {"id": 129, "seek": 88622, "start": 886.22, "end": 891.28, "text": " and start afresh. So we're keeping our hidden state, but we're not keeping our hidden state's", "tokens": [293, 722, 3238, 3644, 13, 407, 321, 434, 5145, 527, 7633, 1785, 11, 457, 321, 434, 406, 5145, 527, 7633, 1785, 311], "temperature": 0.0, "avg_logprob": -0.15171297788619995, "compression_ratio": 1.6352941176470588, "no_speech_prob": 1.6536854445803328e-06}, {"id": 130, "seek": 88622, "start": 891.28, "end": 895.4200000000001, "text": " history.", "tokens": [2503, 13], "temperature": 0.0, "avg_logprob": -0.15171297788619995, "compression_ratio": 1.6352941176470588, "no_speech_prob": 1.6536854445803328e-06}, {"id": 131, "seek": 88622, "start": 895.4200000000001, "end": 904.08, "text": " So that's wrinkle number 1, that's what this repackage bar is doing. So when you see bptt,", "tokens": [407, 300, 311, 928, 14095, 1230, 502, 11, 300, 311, 437, 341, 1085, 501, 609, 2159, 307, 884, 13, 407, 562, 291, 536, 272, 662, 83, 11], "temperature": 0.0, "avg_logprob": -0.15171297788619995, "compression_ratio": 1.6352941176470588, "no_speech_prob": 1.6536854445803328e-06}, {"id": 132, "seek": 88622, "start": 904.08, "end": 909.9200000000001, "text": " that's referring to backprop through time, and you might remember we saw that in our", "tokens": [300, 311, 13761, 281, 646, 79, 1513, 807, 565, 11, 293, 291, 1062, 1604, 321, 1866, 300, 294, 527], "temperature": 0.0, "avg_logprob": -0.15171297788619995, "compression_ratio": 1.6352941176470588, "no_speech_prob": 1.6536854445803328e-06}, {"id": 133, "seek": 90992, "start": 909.92, "end": 917.1999999999999, "text": " original RNN lesson. We had a variable called bptt equals 70. So when we set that, they're", "tokens": [3380, 45702, 45, 6898, 13, 492, 632, 257, 7006, 1219, 272, 662, 83, 6915, 5285, 13, 407, 562, 321, 992, 300, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.15923014283180237, "compression_ratio": 1.6307053941908713, "no_speech_prob": 2.1568098418356385e-06}, {"id": 134, "seek": 90992, "start": 917.1999999999999, "end": 921.24, "text": " actually saying how many layers to backprop through.", "tokens": [767, 1566, 577, 867, 7914, 281, 646, 79, 1513, 807, 13], "temperature": 0.0, "avg_logprob": -0.15923014283180237, "compression_ratio": 1.6307053941908713, "no_speech_prob": 2.1568098418356385e-06}, {"id": 135, "seek": 90992, "start": 921.24, "end": 925.8, "text": " Another good reason not to backprop through too many layers is if you have any kind of", "tokens": [3996, 665, 1778, 406, 281, 646, 79, 1513, 807, 886, 867, 7914, 307, 498, 291, 362, 604, 733, 295], "temperature": 0.0, "avg_logprob": -0.15923014283180237, "compression_ratio": 1.6307053941908713, "no_speech_prob": 2.1568098418356385e-06}, {"id": 136, "seek": 90992, "start": 925.8, "end": 932.28, "text": " gradient instability like gradient explosion or gradient spanishing, the more layers you", "tokens": [16235, 34379, 411, 16235, 15673, 420, 16235, 16174, 3807, 11, 264, 544, 7914, 291], "temperature": 0.0, "avg_logprob": -0.15923014283180237, "compression_ratio": 1.6307053941908713, "no_speech_prob": 2.1568098418356385e-06}, {"id": 137, "seek": 90992, "start": 932.28, "end": 939.0799999999999, "text": " have, the harder the network gets to train. So slower and less resilient.", "tokens": [362, 11, 264, 6081, 264, 3209, 2170, 281, 3847, 13, 407, 14009, 293, 1570, 23699, 13], "temperature": 0.0, "avg_logprob": -0.15923014283180237, "compression_ratio": 1.6307053941908713, "no_speech_prob": 2.1568098418356385e-06}, {"id": 138, "seek": 93908, "start": 939.08, "end": 947.4000000000001, "text": " On the other hand, a longer value for bptt means that you're able to explicitly capture", "tokens": [1282, 264, 661, 1011, 11, 257, 2854, 2158, 337, 272, 662, 83, 1355, 300, 291, 434, 1075, 281, 20803, 7983], "temperature": 0.0, "avg_logprob": -0.16840844995835247, "compression_ratio": 1.3759398496240602, "no_speech_prob": 5.682424216502113e-06}, {"id": 139, "seek": 93908, "start": 947.4000000000001, "end": 957.4000000000001, "text": " a longer memory, more state. So that's something that you get to tune when you create your", "tokens": [257, 2854, 4675, 11, 544, 1785, 13, 407, 300, 311, 746, 300, 291, 483, 281, 10864, 562, 291, 1884, 428], "temperature": 0.0, "avg_logprob": -0.16840844995835247, "compression_ratio": 1.3759398496240602, "no_speech_prob": 5.682424216502113e-06}, {"id": 140, "seek": 93908, "start": 957.4000000000001, "end": 962.36, "text": " RNN.", "tokens": [45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.16840844995835247, "compression_ratio": 1.3759398496240602, "no_speech_prob": 5.682424216502113e-06}, {"id": 141, "seek": 96236, "start": 962.36, "end": 972.28, "text": " Wrinkle number 2 is how are we going to put the data into this. It's all very well the", "tokens": [10159, 14095, 1230, 568, 307, 577, 366, 321, 516, 281, 829, 264, 1412, 666, 341, 13, 467, 311, 439, 588, 731, 264], "temperature": 0.0, "avg_logprob": -0.14356850924557202, "compression_ratio": 1.607361963190184, "no_speech_prob": 8.398051249969285e-06}, {"id": 142, "seek": 96236, "start": 972.28, "end": 984.52, "text": " way I described it just now where we said we could do this, then we can first of all", "tokens": [636, 286, 7619, 309, 445, 586, 689, 321, 848, 321, 727, 360, 341, 11, 550, 321, 393, 700, 295, 439], "temperature": 0.0, "avg_logprob": -0.14356850924557202, "compression_ratio": 1.607361963190184, "no_speech_prob": 8.398051249969285e-06}, {"id": 143, "seek": 96236, "start": 984.52, "end": 991.52, "text": " look at this section, then this section, then this section. But we want to do a mini-batch", "tokens": [574, 412, 341, 3541, 11, 550, 341, 3541, 11, 550, 341, 3541, 13, 583, 321, 528, 281, 360, 257, 8382, 12, 65, 852], "temperature": 0.0, "avg_logprob": -0.14356850924557202, "compression_ratio": 1.607361963190184, "no_speech_prob": 8.398051249969285e-06}, {"id": 144, "seek": 99152, "start": 991.52, "end": 1003.48, "text": " at a time. We want to do a bunch at a time. So in other words, we want to say let's do", "tokens": [412, 257, 565, 13, 492, 528, 281, 360, 257, 3840, 412, 257, 565, 13, 407, 294, 661, 2283, 11, 321, 528, 281, 584, 718, 311, 360], "temperature": 0.0, "avg_logprob": -0.10943709679369656, "compression_ratio": 1.4333333333333333, "no_speech_prob": 4.289322987460764e-06}, {"id": 145, "seek": 99152, "start": 1003.48, "end": 1016.46, "text": " it like this. So mini-batch number 1 would say let's look at this section and predict", "tokens": [309, 411, 341, 13, 407, 8382, 12, 65, 852, 1230, 502, 576, 584, 718, 311, 574, 412, 341, 3541, 293, 6069], "temperature": 0.0, "avg_logprob": -0.10943709679369656, "compression_ratio": 1.4333333333333333, "no_speech_prob": 4.289322987460764e-06}, {"id": 146, "seek": 101646, "start": 1016.46, "end": 1022.58, "text": " that section. And at the same time in parallel, let's look at this totally different section", "tokens": [300, 3541, 13, 400, 412, 264, 912, 565, 294, 8952, 11, 718, 311, 574, 412, 341, 3879, 819, 3541], "temperature": 0.0, "avg_logprob": -0.12873042981649183, "compression_ratio": 2.0702702702702704, "no_speech_prob": 3.1381327971757855e-06}, {"id": 147, "seek": 101646, "start": 1022.58, "end": 1026.9, "text": " and predict this. And at the same time in parallel, let's look at this totally different", "tokens": [293, 6069, 341, 13, 400, 412, 264, 912, 565, 294, 8952, 11, 718, 311, 574, 412, 341, 3879, 819], "temperature": 0.0, "avg_logprob": -0.12873042981649183, "compression_ratio": 2.0702702702702704, "no_speech_prob": 3.1381327971757855e-06}, {"id": 148, "seek": 101646, "start": 1026.9, "end": 1030.64, "text": " section and predict this.", "tokens": [3541, 293, 6069, 341, 13], "temperature": 0.0, "avg_logprob": -0.12873042981649183, "compression_ratio": 2.0702702702702704, "no_speech_prob": 3.1381327971757855e-06}, {"id": 149, "seek": 101646, "start": 1030.64, "end": 1038.1200000000001, "text": " And so then, because remember in our hidden state, we have a vector of hidden state for", "tokens": [400, 370, 550, 11, 570, 1604, 294, 527, 7633, 1785, 11, 321, 362, 257, 8062, 295, 7633, 1785, 337], "temperature": 0.0, "avg_logprob": -0.12873042981649183, "compression_ratio": 2.0702702702702704, "no_speech_prob": 3.1381327971757855e-06}, {"id": 150, "seek": 101646, "start": 1038.1200000000001, "end": 1042.1000000000001, "text": " everything in our mini-batch. So it's going to keep track of at the end of this there's", "tokens": [1203, 294, 527, 8382, 12, 65, 852, 13, 407, 309, 311, 516, 281, 1066, 2837, 295, 412, 264, 917, 295, 341, 456, 311], "temperature": 0.0, "avg_logprob": -0.12873042981649183, "compression_ratio": 2.0702702702702704, "no_speech_prob": 3.1381327971757855e-06}, {"id": 151, "seek": 104210, "start": 1042.1, "end": 1046.9199999999998, "text": " going to be a vector here, a vector here, a vector here. And then we can move across", "tokens": [516, 281, 312, 257, 8062, 510, 11, 257, 8062, 510, 11, 257, 8062, 510, 13, 400, 550, 321, 393, 1286, 2108], "temperature": 0.0, "avg_logprob": -0.1235763168334961, "compression_ratio": 1.9567567567567568, "no_speech_prob": 7.296331659745192e-06}, {"id": 152, "seek": 104210, "start": 1046.9199999999998, "end": 1053.6, "text": " to the next one and say, for this part of the mini-batch, use this to predict that,", "tokens": [281, 264, 958, 472, 293, 584, 11, 337, 341, 644, 295, 264, 8382, 12, 65, 852, 11, 764, 341, 281, 6069, 300, 11], "temperature": 0.0, "avg_logprob": -0.1235763168334961, "compression_ratio": 1.9567567567567568, "no_speech_prob": 7.296331659745192e-06}, {"id": 153, "seek": 104210, "start": 1053.6, "end": 1058.9199999999998, "text": " and use this to predict that, and use this to predict that. So you can see that we're", "tokens": [293, 764, 341, 281, 6069, 300, 11, 293, 764, 341, 281, 6069, 300, 13, 407, 291, 393, 536, 300, 321, 434], "temperature": 0.0, "avg_logprob": -0.1235763168334961, "compression_ratio": 1.9567567567567568, "no_speech_prob": 7.296331659745192e-06}, {"id": 154, "seek": 104210, "start": 1058.9199999999998, "end": 1064.1799999999998, "text": " moving, we've got like a number of totally separate bits of our text that we're moving", "tokens": [2684, 11, 321, 600, 658, 411, 257, 1230, 295, 3879, 4994, 9239, 295, 527, 2487, 300, 321, 434, 2684], "temperature": 0.0, "avg_logprob": -0.1235763168334961, "compression_ratio": 1.9567567567567568, "no_speech_prob": 7.296331659745192e-06}, {"id": 155, "seek": 104210, "start": 1064.1799999999998, "end": 1067.48, "text": " through in parallel.", "tokens": [807, 294, 8952, 13], "temperature": 0.0, "avg_logprob": -0.1235763168334961, "compression_ratio": 1.9567567567567568, "no_speech_prob": 7.296331659745192e-06}, {"id": 156, "seek": 106748, "start": 1067.48, "end": 1075.72, "text": " So hopefully this is going to ring a few bells for you, because what happened was, back when", "tokens": [407, 4696, 341, 307, 516, 281, 4875, 257, 1326, 25474, 337, 291, 11, 570, 437, 2011, 390, 11, 646, 562], "temperature": 0.0, "avg_logprob": -0.1662119886149531, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.028942824632395e-05}, {"id": 157, "seek": 106748, "start": 1075.72, "end": 1080.16, "text": " we started looking at TorchText for the first time, we started talking about how it creates", "tokens": [321, 1409, 1237, 412, 7160, 339, 50198, 337, 264, 700, 565, 11, 321, 1409, 1417, 466, 577, 309, 7829], "temperature": 0.0, "avg_logprob": -0.1662119886149531, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.028942824632395e-05}, {"id": 158, "seek": 106748, "start": 1080.16, "end": 1088.9, "text": " these mini-batches. And I said what happened was we took our whole big long document consisting", "tokens": [613, 8382, 12, 65, 852, 279, 13, 400, 286, 848, 437, 2011, 390, 321, 1890, 527, 1379, 955, 938, 4166, 33921], "temperature": 0.0, "avg_logprob": -0.1662119886149531, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.028942824632395e-05}, {"id": 159, "seek": 106748, "start": 1088.9, "end": 1095.24, "text": " of like the entire works of Nietzsche, or all of the IMDB reviews concatenated together,", "tokens": [295, 411, 264, 2302, 1985, 295, 36583, 89, 12287, 11, 420, 439, 295, 264, 21463, 27735, 10229, 1588, 7186, 770, 1214, 11], "temperature": 0.0, "avg_logprob": -0.1662119886149531, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.028942824632395e-05}, {"id": 160, "seek": 109524, "start": 1095.24, "end": 1101.28, "text": " or whatever. And a lot of you, not surprisingly, this is really weird at first, a lot of you", "tokens": [420, 2035, 13, 400, 257, 688, 295, 291, 11, 406, 17600, 11, 341, 307, 534, 3657, 412, 700, 11, 257, 688, 295, 291], "temperature": 0.0, "avg_logprob": -0.11717088443716776, "compression_ratio": 1.8520408163265305, "no_speech_prob": 2.3187249098555185e-05}, {"id": 161, "seek": 109524, "start": 1101.28, "end": 1108.04, "text": " didn't quite hear what I said correctly. What I said was, we split this into 64 equal-sized", "tokens": [994, 380, 1596, 1568, 437, 286, 848, 8944, 13, 708, 286, 848, 390, 11, 321, 7472, 341, 666, 12145, 2681, 12, 20614], "temperature": 0.0, "avg_logprob": -0.11717088443716776, "compression_ratio": 1.8520408163265305, "no_speech_prob": 2.3187249098555185e-05}, {"id": 162, "seek": 109524, "start": 1108.04, "end": 1113.8, "text": " chunks. And a lot of your brains went, Jeremy just said we split this into chunks of size", "tokens": [24004, 13, 400, 257, 688, 295, 428, 15442, 1437, 11, 17809, 445, 848, 321, 7472, 341, 666, 24004, 295, 2744], "temperature": 0.0, "avg_logprob": -0.11717088443716776, "compression_ratio": 1.8520408163265305, "no_speech_prob": 2.3187249098555185e-05}, {"id": 163, "seek": 109524, "start": 1113.8, "end": 1121.32, "text": " 64. But that's not what Jeremy said. Jeremy said we split it into 64 equal-sized chunks.", "tokens": [12145, 13, 583, 300, 311, 406, 437, 17809, 848, 13, 17809, 848, 321, 7472, 309, 666, 12145, 2681, 12, 20614, 24004, 13], "temperature": 0.0, "avg_logprob": -0.11717088443716776, "compression_ratio": 1.8520408163265305, "no_speech_prob": 2.3187249098555185e-05}, {"id": 164, "seek": 112132, "start": 1121.32, "end": 1127.72, "text": " So if this whole thing was length 64 million, which would be a reasonable-sized corpus,", "tokens": [407, 498, 341, 1379, 551, 390, 4641, 12145, 2459, 11, 597, 576, 312, 257, 10585, 12, 20614, 1181, 31624, 11], "temperature": 0.0, "avg_logprob": -0.15431599764479803, "compression_ratio": 1.8704663212435233, "no_speech_prob": 1.9638014236988965e-06}, {"id": 165, "seek": 112132, "start": 1127.72, "end": 1135.3999999999999, "text": " not an unusual-sized corpus, then each of our 64 chunks would have been of length 1", "tokens": [406, 364, 10901, 12, 20614, 1181, 31624, 11, 550, 1184, 295, 527, 12145, 24004, 576, 362, 668, 295, 4641, 502], "temperature": 0.0, "avg_logprob": -0.15431599764479803, "compression_ratio": 1.8704663212435233, "no_speech_prob": 1.9638014236988965e-06}, {"id": 166, "seek": 112132, "start": 1135.3999999999999, "end": 1137.4399999999998, "text": " million.", "tokens": [2459, 13], "temperature": 0.0, "avg_logprob": -0.15431599764479803, "compression_ratio": 1.8704663212435233, "no_speech_prob": 1.9638014236988965e-06}, {"id": 167, "seek": 112132, "start": 1137.4399999999998, "end": 1143.3999999999999, "text": " And so then what we did was we took the first chunk of 1 million and we put it here. And", "tokens": [400, 370, 550, 437, 321, 630, 390, 321, 1890, 264, 700, 16635, 295, 502, 2459, 293, 321, 829, 309, 510, 13, 400], "temperature": 0.0, "avg_logprob": -0.15431599764479803, "compression_ratio": 1.8704663212435233, "no_speech_prob": 1.9638014236988965e-06}, {"id": 168, "seek": 112132, "start": 1143.3999999999999, "end": 1148.1599999999999, "text": " then we took the second chunk of 1 million and we put it here. The third chunk of 1 million", "tokens": [550, 321, 1890, 264, 1150, 16635, 295, 502, 2459, 293, 321, 829, 309, 510, 13, 440, 2636, 16635, 295, 502, 2459], "temperature": 0.0, "avg_logprob": -0.15431599764479803, "compression_ratio": 1.8704663212435233, "no_speech_prob": 1.9638014236988965e-06}, {"id": 169, "seek": 114816, "start": 1148.16, "end": 1158.1200000000001, "text": " and we put it here. And so forth to create 64 chunks. And then each mini-batch consisted", "tokens": [293, 321, 829, 309, 510, 13, 400, 370, 5220, 281, 1884, 12145, 24004, 13, 400, 550, 1184, 8382, 12, 65, 852, 38227], "temperature": 0.0, "avg_logprob": -0.1326187801361084, "compression_ratio": 1.408, "no_speech_prob": 1.7061814787666663e-06}, {"id": 170, "seek": 114816, "start": 1158.1200000000001, "end": 1168.64, "text": " of us going, let's split this down here and here and here. And each of these is of size", "tokens": [295, 505, 516, 11, 718, 311, 7472, 341, 760, 510, 293, 510, 293, 510, 13, 400, 1184, 295, 613, 307, 295, 2744], "temperature": 0.0, "avg_logprob": -0.1326187801361084, "compression_ratio": 1.408, "no_speech_prob": 1.7061814787666663e-06}, {"id": 171, "seek": 116864, "start": 1168.64, "end": 1178.92, "text": " B, P, T, T, which I think we had something like 70. And so what happened was we said,", "tokens": [363, 11, 430, 11, 314, 11, 314, 11, 597, 286, 519, 321, 632, 746, 411, 5285, 13, 400, 370, 437, 2011, 390, 321, 848, 11], "temperature": 0.0, "avg_logprob": -0.15110394893548426, "compression_ratio": 1.5, "no_speech_prob": 2.1568130250670947e-06}, {"id": 172, "seek": 116864, "start": 1178.92, "end": 1185.3600000000001, "text": " alright, let's look at our first mini-batch as all of these. So we do all of those at", "tokens": [5845, 11, 718, 311, 574, 412, 527, 700, 8382, 12, 65, 852, 382, 439, 295, 613, 13, 407, 321, 360, 439, 295, 729, 412], "temperature": 0.0, "avg_logprob": -0.15110394893548426, "compression_ratio": 1.5, "no_speech_prob": 2.1568130250670947e-06}, {"id": 173, "seek": 116864, "start": 1185.3600000000001, "end": 1194.5400000000002, "text": " once and predict everything offset by 1. And then at the end of that first mini-batch,", "tokens": [1564, 293, 6069, 1203, 18687, 538, 502, 13, 400, 550, 412, 264, 917, 295, 300, 700, 8382, 12, 65, 852, 11], "temperature": 0.0, "avg_logprob": -0.15110394893548426, "compression_ratio": 1.5, "no_speech_prob": 2.1568130250670947e-06}, {"id": 174, "seek": 119454, "start": 1194.54, "end": 1200.22, "text": " we went to the second chunk and used each one of these to predict the next one offset", "tokens": [321, 1437, 281, 264, 1150, 16635, 293, 1143, 1184, 472, 295, 613, 281, 6069, 264, 958, 472, 18687], "temperature": 0.0, "avg_logprob": -0.11825662130837912, "compression_ratio": 1.6805555555555556, "no_speech_prob": 1.9638005142041948e-06}, {"id": 175, "seek": 119454, "start": 1200.22, "end": 1202.8, "text": " by 1.", "tokens": [538, 502, 13], "temperature": 0.0, "avg_logprob": -0.11825662130837912, "compression_ratio": 1.6805555555555556, "no_speech_prob": 1.9638005142041948e-06}, {"id": 176, "seek": 119454, "start": 1202.8, "end": 1209.0, "text": " So that's why we did that slightly weird thing, is that we wanted to have a bunch of things", "tokens": [407, 300, 311, 983, 321, 630, 300, 4748, 3657, 551, 11, 307, 300, 321, 1415, 281, 362, 257, 3840, 295, 721], "temperature": 0.0, "avg_logprob": -0.11825662130837912, "compression_ratio": 1.6805555555555556, "no_speech_prob": 1.9638005142041948e-06}, {"id": 177, "seek": 119454, "start": 1209.0, "end": 1216.32, "text": " we can look through in parallel, each of which hopefully are far enough away from each other", "tokens": [321, 393, 574, 807, 294, 8952, 11, 1184, 295, 597, 4696, 366, 1400, 1547, 1314, 490, 1184, 661], "temperature": 0.0, "avg_logprob": -0.11825662130837912, "compression_ratio": 1.6805555555555556, "no_speech_prob": 1.9638005142041948e-06}, {"id": 178, "seek": 119454, "start": 1216.32, "end": 1221.28, "text": " that we don't have to worry about the fact that the truth is the start of this million", "tokens": [300, 321, 500, 380, 362, 281, 3292, 466, 264, 1186, 300, 264, 3494, 307, 264, 722, 295, 341, 2459], "temperature": 0.0, "avg_logprob": -0.11825662130837912, "compression_ratio": 1.6805555555555556, "no_speech_prob": 1.9638005142041948e-06}, {"id": 179, "seek": 122128, "start": 1221.28, "end": 1228.32, "text": " characters was actually in the middle of a sentence. But who cares, it only happens once", "tokens": [4342, 390, 767, 294, 264, 2808, 295, 257, 8174, 13, 583, 567, 12310, 11, 309, 787, 2314, 1564], "temperature": 0.0, "avg_logprob": -0.263606595993042, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.5689482097513974e-05}, {"id": 180, "seek": 122128, "start": 1228.32, "end": 1229.32, "text": " every million characters.", "tokens": [633, 2459, 4342, 13], "temperature": 0.0, "avg_logprob": -0.263606595993042, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.5689482097513974e-05}, {"id": 181, "seek": 122128, "start": 1229.32, "end": 1237.08, "text": " Question from the audience. I was wondering if you could talk a little bit more about", "tokens": [14464, 490, 264, 4034, 13, 286, 390, 6359, 498, 291, 727, 751, 257, 707, 857, 544, 466], "temperature": 0.0, "avg_logprob": -0.263606595993042, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.5689482097513974e-05}, {"id": 182, "seek": 122128, "start": 1237.08, "end": 1241.44, "text": " augmentation for this kind of dataset.", "tokens": [14501, 19631, 337, 341, 733, 295, 28872, 13], "temperature": 0.0, "avg_logprob": -0.263606595993042, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.5689482097513974e-05}, {"id": 183, "seek": 122128, "start": 1241.44, "end": 1246.92, "text": " Data augmentation for this kind of dataset? No I can't because I don't really know a good", "tokens": [11888, 14501, 19631, 337, 341, 733, 295, 28872, 30, 883, 286, 393, 380, 570, 286, 500, 380, 534, 458, 257, 665], "temperature": 0.0, "avg_logprob": -0.263606595993042, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.5689482097513974e-05}, {"id": 184, "seek": 124692, "start": 1246.92, "end": 1255.0800000000002, "text": " way. It's one of the things I'm going to be studying between now and part 2. There have", "tokens": [636, 13, 467, 311, 472, 295, 264, 721, 286, 478, 516, 281, 312, 7601, 1296, 586, 293, 644, 568, 13, 821, 362], "temperature": 0.0, "avg_logprob": -0.12644119262695314, "compression_ratio": 1.5635593220338984, "no_speech_prob": 7.527906291215913e-06}, {"id": 185, "seek": 124692, "start": 1255.0800000000002, "end": 1260.2, "text": " been some recent developments, particularly something we talked about in the machine learning", "tokens": [668, 512, 5162, 20862, 11, 4098, 746, 321, 2825, 466, 294, 264, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.12644119262695314, "compression_ratio": 1.5635593220338984, "no_speech_prob": 7.527906291215913e-06}, {"id": 186, "seek": 124692, "start": 1260.2, "end": 1265.68, "text": " course and I think we briefly mentioned here, which was somebody for a recent Kaggle competition", "tokens": [1164, 293, 286, 519, 321, 10515, 2835, 510, 11, 597, 390, 2618, 337, 257, 5162, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.12644119262695314, "compression_ratio": 1.5635593220338984, "no_speech_prob": 7.527906291215913e-06}, {"id": 187, "seek": 124692, "start": 1265.68, "end": 1276.72, "text": " won it by doing data augmentation by randomly inserting parts of different rows basically.", "tokens": [1582, 309, 538, 884, 1412, 14501, 19631, 538, 16979, 46567, 3166, 295, 819, 13241, 1936, 13], "temperature": 0.0, "avg_logprob": -0.12644119262695314, "compression_ratio": 1.5635593220338984, "no_speech_prob": 7.527906291215913e-06}, {"id": 188, "seek": 127672, "start": 1276.72, "end": 1280.76, "text": " Something like that may be useful here and I've seen some papers that do something like", "tokens": [6595, 411, 300, 815, 312, 4420, 510, 293, 286, 600, 1612, 512, 10577, 300, 360, 746, 411], "temperature": 0.0, "avg_logprob": -0.18930828037546643, "compression_ratio": 1.5153374233128833, "no_speech_prob": 4.069170609000139e-05}, {"id": 189, "seek": 127672, "start": 1280.76, "end": 1292.88, "text": " that. But I haven't seen any recent-ish state-of-the-art NLP papers that are doing this kind of data", "tokens": [300, 13, 583, 286, 2378, 380, 1612, 604, 5162, 12, 742, 1785, 12, 2670, 12, 3322, 12, 446, 426, 45196, 10577, 300, 366, 884, 341, 733, 295, 1412], "temperature": 0.0, "avg_logprob": -0.18930828037546643, "compression_ratio": 1.5153374233128833, "no_speech_prob": 4.069170609000139e-05}, {"id": 190, "seek": 127672, "start": 1292.88, "end": 1299.8, "text": " augmentation. So it's something we're planning to work on.", "tokens": [14501, 19631, 13, 407, 309, 311, 746, 321, 434, 5038, 281, 589, 322, 13], "temperature": 0.0, "avg_logprob": -0.18930828037546643, "compression_ratio": 1.5153374233128833, "no_speech_prob": 4.069170609000139e-05}, {"id": 191, "seek": 129980, "start": 1299.8, "end": 1307.04, "text": " How do you choose a BPTT?", "tokens": [1012, 360, 291, 2826, 257, 40533, 28178, 30], "temperature": 0.0, "avg_logprob": -0.1564272142225696, "compression_ratio": 1.3666666666666667, "no_speech_prob": 2.3921651518321596e-05}, {"id": 192, "seek": 129980, "start": 1307.04, "end": 1310.56, "text": " So there's a couple of things to think about when you pick your BPTT. The first is that", "tokens": [407, 456, 311, 257, 1916, 295, 721, 281, 519, 466, 562, 291, 1888, 428, 40533, 28178, 13, 440, 700, 307, 300], "temperature": 0.0, "avg_logprob": -0.1564272142225696, "compression_ratio": 1.3666666666666667, "no_speech_prob": 2.3921651518321596e-05}, {"id": 193, "seek": 129980, "start": 1310.56, "end": 1328.12, "text": " you'll note that the matrix size for a mini-batch has a BPTT by batch size. So one issue is", "tokens": [291, 603, 3637, 300, 264, 8141, 2744, 337, 257, 8382, 12, 65, 852, 575, 257, 40533, 28178, 538, 15245, 2744, 13, 407, 472, 2734, 307], "temperature": 0.0, "avg_logprob": -0.1564272142225696, "compression_ratio": 1.3666666666666667, "no_speech_prob": 2.3921651518321596e-05}, {"id": 194, "seek": 132812, "start": 1328.12, "end": 1335.6399999999999, "text": " your GPU RAM needs to be able to fit that by your embedding matrix. Every one of these", "tokens": [428, 18407, 14561, 2203, 281, 312, 1075, 281, 3318, 300, 538, 428, 12240, 3584, 8141, 13, 2048, 472, 295, 613], "temperature": 0.0, "avg_logprob": -0.14729982103620257, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.3496977544491529e-06}, {"id": 195, "seek": 132812, "start": 1335.6399999999999, "end": 1341.8799999999999, "text": " is going to be of length embedding length plus all of the hidden state. So one thing", "tokens": [307, 516, 281, 312, 295, 4641, 12240, 3584, 4641, 1804, 439, 295, 264, 7633, 1785, 13, 407, 472, 551], "temperature": 0.0, "avg_logprob": -0.14729982103620257, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.3496977544491529e-06}, {"id": 196, "seek": 132812, "start": 1341.8799999999999, "end": 1350.2399999999998, "text": " is if you get a CUDA out of memory error, you need to reduce one of those. If you're", "tokens": [307, 498, 291, 483, 257, 29777, 7509, 484, 295, 4675, 6713, 11, 291, 643, 281, 5407, 472, 295, 729, 13, 759, 291, 434], "temperature": 0.0, "avg_logprob": -0.14729982103620257, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.3496977544491529e-06}, {"id": 197, "seek": 135024, "start": 1350.24, "end": 1357.96, "text": " finding your training is very unstable, like your loss is shooting off to NAN suddenly,", "tokens": [5006, 428, 3097, 307, 588, 23742, 11, 411, 428, 4470, 307, 5942, 766, 281, 426, 1770, 5800, 11], "temperature": 0.0, "avg_logprob": -0.1659871396564302, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.4367479934662697e-06}, {"id": 198, "seek": 135024, "start": 1357.96, "end": 1362.52, "text": " then you could try decreasing your BPTT because you've got less layers to gradient explode", "tokens": [550, 291, 727, 853, 23223, 428, 40533, 28178, 570, 291, 600, 658, 1570, 7914, 281, 16235, 21411], "temperature": 0.0, "avg_logprob": -0.1659871396564302, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.4367479934662697e-06}, {"id": 199, "seek": 135024, "start": 1362.52, "end": 1370.2, "text": " through. If it's too slow, you could try decreasing your BPTT because it's got to do one of those", "tokens": [807, 13, 759, 309, 311, 886, 2964, 11, 291, 727, 853, 23223, 428, 40533, 28178, 570, 309, 311, 658, 281, 360, 472, 295, 729], "temperature": 0.0, "avg_logprob": -0.1659871396564302, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.4367479934662697e-06}, {"id": 200, "seek": 135024, "start": 1370.2, "end": 1378.0, "text": " steps at a time. Like that for loop can't be parallelized.", "tokens": [4439, 412, 257, 565, 13, 1743, 300, 337, 6367, 393, 380, 312, 8952, 1602, 13], "temperature": 0.0, "avg_logprob": -0.1659871396564302, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.4367479934662697e-06}, {"id": 201, "seek": 137800, "start": 1378.0, "end": 1384.04, "text": " I say that there's a recent thing called QRNN, which we'll hopefully talk about in part 2,", "tokens": [286, 584, 300, 456, 311, 257, 5162, 551, 1219, 32784, 45, 45, 11, 597, 321, 603, 4696, 751, 466, 294, 644, 568, 11], "temperature": 0.0, "avg_logprob": -0.1771412485653592, "compression_ratio": 1.6216216216216217, "no_speech_prob": 7.183227353380062e-06}, {"id": 202, "seek": 137800, "start": 1384.04, "end": 1388.4, "text": " which kind of does paralyze it, but the versions we're looking at don't paralyze it. So those", "tokens": [597, 733, 295, 775, 32645, 1381, 309, 11, 457, 264, 9606, 321, 434, 1237, 412, 500, 380, 32645, 1381, 309, 13, 407, 729], "temperature": 0.0, "avg_logprob": -0.1771412485653592, "compression_ratio": 1.6216216216216217, "no_speech_prob": 7.183227353380062e-06}, {"id": 203, "seek": 137800, "start": 1388.4, "end": 1394.32, "text": " would be the main issues, look at performance, look at memory, look at stability, and try", "tokens": [576, 312, 264, 2135, 2663, 11, 574, 412, 3389, 11, 574, 412, 4675, 11, 574, 412, 11826, 11, 293, 853], "temperature": 0.0, "avg_logprob": -0.1771412485653592, "compression_ratio": 1.6216216216216217, "no_speech_prob": 7.183227353380062e-06}, {"id": 204, "seek": 137800, "start": 1394.32, "end": 1399.76, "text": " and find a number that's as high as you can make it, but all of those things work for", "tokens": [293, 915, 257, 1230, 300, 311, 382, 1090, 382, 291, 393, 652, 309, 11, 457, 439, 295, 729, 721, 589, 337], "temperature": 0.0, "avg_logprob": -0.1771412485653592, "compression_ratio": 1.6216216216216217, "no_speech_prob": 7.183227353380062e-06}, {"id": 205, "seek": 139976, "start": 1399.76, "end": 1412.32, "text": " you. So trying to get all that chunking and lining up and everything to work is more code", "tokens": [291, 13, 407, 1382, 281, 483, 439, 300, 16635, 278, 293, 19628, 493, 293, 1203, 281, 589, 307, 544, 3089], "temperature": 0.0, "avg_logprob": -0.20445069874802682, "compression_ratio": 1.5257142857142858, "no_speech_prob": 3.237745431761141e-06}, {"id": 206, "seek": 139976, "start": 1412.32, "end": 1418.48, "text": " than I want to write. So for this section, we're going to go back and use TorchText again.", "tokens": [813, 286, 528, 281, 2464, 13, 407, 337, 341, 3541, 11, 321, 434, 516, 281, 352, 646, 293, 764, 7160, 339, 50198, 797, 13], "temperature": 0.0, "avg_logprob": -0.20445069874802682, "compression_ratio": 1.5257142857142858, "no_speech_prob": 3.237745431761141e-06}, {"id": 207, "seek": 139976, "start": 1418.48, "end": 1427.36, "text": " So when you're using APIs like FastAI and TorchText, which in this case these two APIs", "tokens": [407, 562, 291, 434, 1228, 21445, 411, 15968, 48698, 293, 7160, 339, 50198, 11, 597, 294, 341, 1389, 613, 732, 21445], "temperature": 0.0, "avg_logprob": -0.20445069874802682, "compression_ratio": 1.5257142857142858, "no_speech_prob": 3.237745431761141e-06}, {"id": 208, "seek": 142736, "start": 1427.36, "end": 1435.76, "text": " are designed to work together, you often have a choice which is like, this API has a number", "tokens": [366, 4761, 281, 589, 1214, 11, 291, 2049, 362, 257, 3922, 597, 307, 411, 11, 341, 9362, 575, 257, 1230], "temperature": 0.0, "avg_logprob": -0.14969784872872488, "compression_ratio": 1.7354260089686098, "no_speech_prob": 9.36865126277553e-06}, {"id": 209, "seek": 142736, "start": 1435.76, "end": 1441.32, "text": " of methods that expect the data in this kind of format, and you can either change your", "tokens": [295, 7150, 300, 2066, 264, 1412, 294, 341, 733, 295, 7877, 11, 293, 291, 393, 2139, 1319, 428], "temperature": 0.0, "avg_logprob": -0.14969784872872488, "compression_ratio": 1.7354260089686098, "no_speech_prob": 9.36865126277553e-06}, {"id": 210, "seek": 142736, "start": 1441.32, "end": 1448.28, "text": " data to fit that format, or you can write your own data set subclass to handle the format", "tokens": [1412, 281, 3318, 300, 7877, 11, 420, 291, 393, 2464, 428, 1065, 1412, 992, 1422, 11665, 281, 4813, 264, 7877], "temperature": 0.0, "avg_logprob": -0.14969784872872488, "compression_ratio": 1.7354260089686098, "no_speech_prob": 9.36865126277553e-06}, {"id": 211, "seek": 142736, "start": 1448.28, "end": 1451.1599999999999, "text": " that your data is already in.", "tokens": [300, 428, 1412, 307, 1217, 294, 13], "temperature": 0.0, "avg_logprob": -0.14969784872872488, "compression_ratio": 1.7354260089686098, "no_speech_prob": 9.36865126277553e-06}, {"id": 212, "seek": 142736, "start": 1451.1599999999999, "end": 1456.08, "text": " I've noticed on the forum, a lot of you are spending a lot of time writing your own data", "tokens": [286, 600, 5694, 322, 264, 17542, 11, 257, 688, 295, 291, 366, 6434, 257, 688, 295, 565, 3579, 428, 1065, 1412], "temperature": 0.0, "avg_logprob": -0.14969784872872488, "compression_ratio": 1.7354260089686098, "no_speech_prob": 9.36865126277553e-06}, {"id": 213, "seek": 145608, "start": 1456.08, "end": 1461.96, "text": " set classes, whereas I am way lazier than you, and I spend my time instead changing", "tokens": [992, 5359, 11, 9735, 286, 669, 636, 19320, 811, 813, 291, 11, 293, 286, 3496, 452, 565, 2602, 4473], "temperature": 0.0, "avg_logprob": -0.14490003171174423, "compression_ratio": 1.6308411214953271, "no_speech_prob": 1.0289418241882231e-05}, {"id": 214, "seek": 145608, "start": 1461.96, "end": 1471.3999999999999, "text": " my data to fit the data set classes I have. Either is fine, and if you realize there's", "tokens": [452, 1412, 281, 3318, 264, 1412, 992, 5359, 286, 362, 13, 13746, 307, 2489, 11, 293, 498, 291, 4325, 456, 311], "temperature": 0.0, "avg_logprob": -0.14490003171174423, "compression_ratio": 1.6308411214953271, "no_speech_prob": 1.0289418241882231e-05}, {"id": 215, "seek": 145608, "start": 1471.3999999999999, "end": 1476.72, "text": " a kind of a format of data that me and other people are likely to be seeing quite often", "tokens": [257, 733, 295, 257, 7877, 295, 1412, 300, 385, 293, 661, 561, 366, 3700, 281, 312, 2577, 1596, 2049], "temperature": 0.0, "avg_logprob": -0.14490003171174423, "compression_ratio": 1.6308411214953271, "no_speech_prob": 1.0289418241882231e-05}, {"id": 216, "seek": 145608, "start": 1476.72, "end": 1481.6799999999998, "text": " and it's not in the FastAI library, then by all means, write the data set subclass, submit", "tokens": [293, 309, 311, 406, 294, 264, 15968, 48698, 6405, 11, 550, 538, 439, 1355, 11, 2464, 264, 1412, 992, 1422, 11665, 11, 10315], "temperature": 0.0, "avg_logprob": -0.14490003171174423, "compression_ratio": 1.6308411214953271, "no_speech_prob": 1.0289418241882231e-05}, {"id": 217, "seek": 148168, "start": 1481.68, "end": 1489.04, "text": " it as a PR and then everybody can benefit. But in this case, I just kind of thought I", "tokens": [309, 382, 257, 11568, 293, 550, 2201, 393, 5121, 13, 583, 294, 341, 1389, 11, 286, 445, 733, 295, 1194, 286], "temperature": 0.0, "avg_logprob": -0.19208522464918054, "compression_ratio": 1.5990990990990992, "no_speech_prob": 2.9022978651482845e-06}, {"id": 218, "seek": 148168, "start": 1489.04, "end": 1496.64, "text": " want to have some niche data fed into TorchText. I'm just going to put it in the format that", "tokens": [528, 281, 362, 512, 19956, 1412, 4636, 666, 7160, 339, 50198, 13, 286, 478, 445, 516, 281, 829, 309, 294, 264, 7877, 300], "temperature": 0.0, "avg_logprob": -0.19208522464918054, "compression_ratio": 1.5990990990990992, "no_speech_prob": 2.9022978651482845e-06}, {"id": 219, "seek": 148168, "start": 1496.64, "end": 1502.76, "text": " TorchText already supports. So TorchText already has, or at least the FastAI wrapper around", "tokens": [7160, 339, 50198, 1217, 9346, 13, 407, 7160, 339, 50198, 1217, 575, 11, 420, 412, 1935, 264, 15968, 48698, 46906, 926], "temperature": 0.0, "avg_logprob": -0.19208522464918054, "compression_ratio": 1.5990990990990992, "no_speech_prob": 2.9022978651482845e-06}, {"id": 220, "seek": 148168, "start": 1502.76, "end": 1507.0800000000002, "text": " TorchText, already has something where you can have a training path and a validation", "tokens": [7160, 339, 50198, 11, 1217, 575, 746, 689, 291, 393, 362, 257, 3097, 3100, 293, 257, 24071], "temperature": 0.0, "avg_logprob": -0.19208522464918054, "compression_ratio": 1.5990990990990992, "no_speech_prob": 2.9022978651482845e-06}, {"id": 221, "seek": 150708, "start": 1507.08, "end": 1513.6, "text": " path and one or more text files in each path containing a bunch of stuff that's concatenated", "tokens": [3100, 293, 472, 420, 544, 2487, 7098, 294, 1184, 3100, 19273, 257, 3840, 295, 1507, 300, 311, 1588, 7186, 770], "temperature": 0.0, "avg_logprob": -0.13371755492012455, "compression_ratio": 1.6991525423728813, "no_speech_prob": 9.972832231142092e-06}, {"id": 222, "seek": 150708, "start": 1513.6, "end": 1516.08, "text": " together for your language model.", "tokens": [1214, 337, 428, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13371755492012455, "compression_ratio": 1.6991525423728813, "no_speech_prob": 9.972832231142092e-06}, {"id": 223, "seek": 150708, "start": 1516.08, "end": 1522.52, "text": " So in this case, all I did was I made a copy of my Nietzsche file, copied it into training,", "tokens": [407, 294, 341, 1389, 11, 439, 286, 630, 390, 286, 1027, 257, 5055, 295, 452, 36583, 89, 12287, 3991, 11, 25365, 309, 666, 3097, 11], "temperature": 0.0, "avg_logprob": -0.13371755492012455, "compression_ratio": 1.6991525423728813, "no_speech_prob": 9.972832231142092e-06}, {"id": 224, "seek": 150708, "start": 1522.52, "end": 1528.52, "text": " made another copy, stuck it into the validation, and then in one of them, in the training set,", "tokens": [1027, 1071, 5055, 11, 5541, 309, 666, 264, 24071, 11, 293, 550, 294, 472, 295, 552, 11, 294, 264, 3097, 992, 11], "temperature": 0.0, "avg_logprob": -0.13371755492012455, "compression_ratio": 1.6991525423728813, "no_speech_prob": 9.972832231142092e-06}, {"id": 225, "seek": 150708, "start": 1528.52, "end": 1535.0, "text": " I deleted the last 20% of rows, and in the validation set, I deleted all except for the", "tokens": [286, 22981, 264, 1036, 945, 4, 295, 13241, 11, 293, 294, 264, 24071, 992, 11, 286, 22981, 439, 3993, 337, 264], "temperature": 0.0, "avg_logprob": -0.13371755492012455, "compression_ratio": 1.6991525423728813, "no_speech_prob": 9.972832231142092e-06}, {"id": 226, "seek": 153500, "start": 1535.0, "end": 1543.16, "text": " last 20% of rows. And I was done. So in this case, I found that easier than writing a custom", "tokens": [1036, 945, 4, 295, 13241, 13, 400, 286, 390, 1096, 13, 407, 294, 341, 1389, 11, 286, 1352, 300, 3571, 813, 3579, 257, 2375], "temperature": 0.0, "avg_logprob": -0.10371985727426958, "compression_ratio": 1.5769230769230769, "no_speech_prob": 4.092889412277145e-06}, {"id": 227, "seek": 153500, "start": 1543.16, "end": 1544.44, "text": " data set class.", "tokens": [1412, 992, 1508, 13], "temperature": 0.0, "avg_logprob": -0.10371985727426958, "compression_ratio": 1.5769230769230769, "no_speech_prob": 4.092889412277145e-06}, {"id": 228, "seek": 153500, "start": 1544.44, "end": 1549.28, "text": " The other benefit of doing it that way was that I felt like it was more realistic to", "tokens": [440, 661, 5121, 295, 884, 309, 300, 636, 390, 300, 286, 2762, 411, 309, 390, 544, 12465, 281], "temperature": 0.0, "avg_logprob": -0.10371985727426958, "compression_ratio": 1.5769230769230769, "no_speech_prob": 4.092889412277145e-06}, {"id": 229, "seek": 153500, "start": 1549.28, "end": 1555.92, "text": " have a validation set that wasn't a random shuffled set of rows of text, but was like", "tokens": [362, 257, 24071, 992, 300, 2067, 380, 257, 4974, 402, 33974, 992, 295, 13241, 295, 2487, 11, 457, 390, 411], "temperature": 0.0, "avg_logprob": -0.10371985727426958, "compression_ratio": 1.5769230769230769, "no_speech_prob": 4.092889412277145e-06}, {"id": 230, "seek": 153500, "start": 1555.92, "end": 1561.16, "text": " a totally separate part of the corpus. Because I feel like in practice, you're very often", "tokens": [257, 3879, 4994, 644, 295, 264, 1181, 31624, 13, 1436, 286, 841, 411, 294, 3124, 11, 291, 434, 588, 2049], "temperature": 0.0, "avg_logprob": -0.10371985727426958, "compression_ratio": 1.5769230769230769, "no_speech_prob": 4.092889412277145e-06}, {"id": 231, "seek": 156116, "start": 1561.16, "end": 1566.96, "text": " going to be saying, I've got these books or these authors I'm learning from and then I", "tokens": [516, 281, 312, 1566, 11, 286, 600, 658, 613, 3642, 420, 613, 16552, 286, 478, 2539, 490, 293, 550, 286], "temperature": 0.0, "avg_logprob": -0.13812761306762694, "compression_ratio": 1.6111111111111112, "no_speech_prob": 6.1441201069101226e-06}, {"id": 232, "seek": 156116, "start": 1566.96, "end": 1571.28, "text": " want to apply it to these different books and these different authors. So I felt like", "tokens": [528, 281, 3079, 309, 281, 613, 819, 3642, 293, 613, 819, 16552, 13, 407, 286, 2762, 411], "temperature": 0.0, "avg_logprob": -0.13812761306762694, "compression_ratio": 1.6111111111111112, "no_speech_prob": 6.1441201069101226e-06}, {"id": 233, "seek": 156116, "start": 1571.28, "end": 1578.3200000000002, "text": " for getting a more realistic validation of my Nietzsche model, I should use a whole separate", "tokens": [337, 1242, 257, 544, 12465, 24071, 295, 452, 36583, 89, 12287, 2316, 11, 286, 820, 764, 257, 1379, 4994], "temperature": 0.0, "avg_logprob": -0.13812761306762694, "compression_ratio": 1.6111111111111112, "no_speech_prob": 6.1441201069101226e-06}, {"id": 234, "seek": 156116, "start": 1578.3200000000002, "end": 1585.8000000000002, "text": " piece of the text. So in this case, it was the last 20% of the rows of the corpus.", "tokens": [2522, 295, 264, 2487, 13, 407, 294, 341, 1389, 11, 309, 390, 264, 1036, 945, 4, 295, 264, 13241, 295, 264, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.13812761306762694, "compression_ratio": 1.6111111111111112, "no_speech_prob": 6.1441201069101226e-06}, {"id": 235, "seek": 158580, "start": 1585.8, "end": 1592.52, "text": " So I haven't created this for you intentionally, because this is the kind of stuff I want you", "tokens": [407, 286, 2378, 380, 2942, 341, 337, 291, 22062, 11, 570, 341, 307, 264, 733, 295, 1507, 286, 528, 291], "temperature": 0.0, "avg_logprob": -0.1871049404144287, "compression_ratio": 1.5666666666666667, "no_speech_prob": 2.212539220636245e-05}, {"id": 236, "seek": 158580, "start": 1592.52, "end": 1597.76, "text": " practicing, is making sure that you're familiar enough, comfortable enough with bash or whatever", "tokens": [11350, 11, 307, 1455, 988, 300, 291, 434, 4963, 1547, 11, 4619, 1547, 365, 46183, 420, 2035], "temperature": 0.0, "avg_logprob": -0.1871049404144287, "compression_ratio": 1.5666666666666667, "no_speech_prob": 2.212539220636245e-05}, {"id": 237, "seek": 158580, "start": 1597.76, "end": 1603.6, "text": " that you can create these and that you understand what they need to look like and so forth.", "tokens": [300, 291, 393, 1884, 613, 293, 300, 291, 1223, 437, 436, 643, 281, 574, 411, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1871049404144287, "compression_ratio": 1.5666666666666667, "no_speech_prob": 2.212539220636245e-05}, {"id": 238, "seek": 160360, "start": 1603.6, "end": 1617.12, "text": " So in this case, you can see I've now got a train and a validation here. So you can", "tokens": [407, 294, 341, 1389, 11, 291, 393, 536, 286, 600, 586, 658, 257, 3847, 293, 257, 24071, 510, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.18448659206958526, "compression_ratio": 1.6759259259259258, "no_speech_prob": 1.1125523087684996e-05}, {"id": 239, "seek": 160360, "start": 1617.12, "end": 1621.9199999999998, "text": " see I've literally just got one file in it. Because when you're doing a language model,", "tokens": [536, 286, 600, 3736, 445, 658, 472, 3991, 294, 309, 13, 1436, 562, 291, 434, 884, 257, 2856, 2316, 11], "temperature": 0.0, "avg_logprob": -0.18448659206958526, "compression_ratio": 1.6759259259259258, "no_speech_prob": 1.1125523087684996e-05}, {"id": 240, "seek": 160360, "start": 1621.9199999999998, "end": 1626.36, "text": " i.e. predicting the next character or predicting the next word, you don't really need separate", "tokens": [741, 13, 68, 13, 32884, 264, 958, 2517, 420, 32884, 264, 958, 1349, 11, 291, 500, 380, 534, 643, 4994], "temperature": 0.0, "avg_logprob": -0.18448659206958526, "compression_ratio": 1.6759259259259258, "no_speech_prob": 1.1125523087684996e-05}, {"id": 241, "seek": 160360, "start": 1626.36, "end": 1633.08, "text": " files. It's fine if you do have separate files, but they just get concatenated together anyway.", "tokens": [7098, 13, 467, 311, 2489, 498, 291, 360, 362, 4994, 7098, 11, 457, 436, 445, 483, 1588, 7186, 770, 1214, 4033, 13], "temperature": 0.0, "avg_logprob": -0.18448659206958526, "compression_ratio": 1.6759259259259258, "no_speech_prob": 1.1125523087684996e-05}, {"id": 242, "seek": 163308, "start": 1633.08, "end": 1640.56, "text": " So that's my source data. And so here is the same lines of code that we've seen before.", "tokens": [407, 300, 311, 452, 4009, 1412, 13, 400, 370, 510, 307, 264, 912, 3876, 295, 3089, 300, 321, 600, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.20899269160102396, "compression_ratio": 1.447674418604651, "no_speech_prob": 8.66459322423907e-06}, {"id": 243, "seek": 163308, "start": 1640.56, "end": 1643.9199999999998, "text": " Let's go over them again, because it's a couple of lessons ago.", "tokens": [961, 311, 352, 670, 552, 797, 11, 570, 309, 311, 257, 1916, 295, 8820, 2057, 13], "temperature": 0.0, "avg_logprob": -0.20899269160102396, "compression_ratio": 1.447674418604651, "no_speech_prob": 8.66459322423907e-06}, {"id": 244, "seek": 163308, "start": 1643.9199999999998, "end": 1652.34, "text": " So in TorchText, we create this thing called a field. And a field initially is just a description", "tokens": [407, 294, 7160, 339, 50198, 11, 321, 1884, 341, 551, 1219, 257, 2519, 13, 400, 257, 2519, 9105, 307, 445, 257, 3855], "temperature": 0.0, "avg_logprob": -0.20899269160102396, "compression_ratio": 1.447674418604651, "no_speech_prob": 8.66459322423907e-06}, {"id": 245, "seek": 165234, "start": 1652.34, "end": 1663.3, "text": " of how to go about preprocessing the text. In this case, I'm going to say, lowercase it.", "tokens": [295, 577, 281, 352, 466, 2666, 340, 780, 278, 264, 2487, 13, 682, 341, 1389, 11, 286, 478, 516, 281, 584, 11, 3126, 9765, 309, 13], "temperature": 0.0, "avg_logprob": -0.17982013947373138, "compression_ratio": 1.6135458167330676, "no_speech_prob": 8.139643796312157e-06}, {"id": 246, "seek": 165234, "start": 1663.3, "end": 1666.02, "text": " Now I think about it, there's no particular reason to have done this lowercase. Uppercase", "tokens": [823, 286, 519, 466, 309, 11, 456, 311, 572, 1729, 1778, 281, 362, 1096, 341, 3126, 9765, 13, 624, 427, 2869, 651], "temperature": 0.0, "avg_logprob": -0.17982013947373138, "compression_ratio": 1.6135458167330676, "no_speech_prob": 8.139643796312157e-06}, {"id": 247, "seek": 165234, "start": 1666.02, "end": 1669.98, "text": " would work fine too. And then how do I tokenize it?", "tokens": [576, 589, 2489, 886, 13, 400, 550, 577, 360, 286, 14862, 1125, 309, 30], "temperature": 0.0, "avg_logprob": -0.17982013947373138, "compression_ratio": 1.6135458167330676, "no_speech_prob": 8.139643796312157e-06}, {"id": 248, "seek": 165234, "start": 1669.98, "end": 1675.8, "text": " And so you might remember last time we used a tokenization function which largely spit", "tokens": [400, 370, 291, 1062, 1604, 1036, 565, 321, 1143, 257, 14862, 2144, 2445, 597, 11611, 22127], "temperature": 0.0, "avg_logprob": -0.17982013947373138, "compression_ratio": 1.6135458167330676, "no_speech_prob": 8.139643796312157e-06}, {"id": 249, "seek": 165234, "start": 1675.8, "end": 1680.1999999999998, "text": " on whitespace and tried to do clever things with punctuation. And that gave us the word", "tokens": [322, 21909, 17940, 293, 3031, 281, 360, 13494, 721, 365, 27006, 16073, 13, 400, 300, 2729, 505, 264, 1349], "temperature": 0.0, "avg_logprob": -0.17982013947373138, "compression_ratio": 1.6135458167330676, "no_speech_prob": 8.139643796312157e-06}, {"id": 250, "seek": 168020, "start": 1680.2, "end": 1685.64, "text": " model. In this case, I want a character model, so I actually want every character put into", "tokens": [2316, 13, 682, 341, 1389, 11, 286, 528, 257, 2517, 2316, 11, 370, 286, 767, 528, 633, 2517, 829, 666], "temperature": 0.0, "avg_logprob": -0.17955660820007324, "compression_ratio": 1.4973821989528795, "no_speech_prob": 1.1478735359560233e-05}, {"id": 251, "seek": 168020, "start": 1685.64, "end": 1696.32, "text": " a separate token. So I could just use the function list in Python, because list in Python", "tokens": [257, 4994, 14862, 13, 407, 286, 727, 445, 764, 264, 2445, 1329, 294, 15329, 11, 570, 1329, 294, 15329], "temperature": 0.0, "avg_logprob": -0.17955660820007324, "compression_ratio": 1.4973821989528795, "no_speech_prob": 1.1478735359560233e-05}, {"id": 252, "seek": 168020, "start": 1696.32, "end": 1698.3600000000001, "text": " does that.", "tokens": [775, 300, 13], "temperature": 0.0, "avg_logprob": -0.17955660820007324, "compression_ratio": 1.4973821989528795, "no_speech_prob": 1.1478735359560233e-05}, {"id": 253, "seek": 168020, "start": 1698.3600000000001, "end": 1705.76, "text": " So this is where you can kind of see understanding how libraries like TorchText and FastAI are", "tokens": [407, 341, 307, 689, 291, 393, 733, 295, 536, 3701, 577, 15148, 411, 7160, 339, 50198, 293, 15968, 48698, 366], "temperature": 0.0, "avg_logprob": -0.17955660820007324, "compression_ratio": 1.4973821989528795, "no_speech_prob": 1.1478735359560233e-05}, {"id": 254, "seek": 170576, "start": 1705.76, "end": 1712.06, "text": " designed to be extended can make your life a lot easier. So when you realize that very", "tokens": [4761, 281, 312, 10913, 393, 652, 428, 993, 257, 688, 3571, 13, 407, 562, 291, 4325, 300, 588], "temperature": 0.0, "avg_logprob": -0.1318020526273751, "compression_ratio": 1.6009852216748768, "no_speech_prob": 1.4367496987688355e-06}, {"id": 255, "seek": 170576, "start": 1712.06, "end": 1718.7, "text": " often both of these libraries kind of expect you to pass a function that does something,", "tokens": [2049, 1293, 295, 613, 15148, 733, 295, 2066, 291, 281, 1320, 257, 2445, 300, 775, 746, 11], "temperature": 0.0, "avg_logprob": -0.1318020526273751, "compression_ratio": 1.6009852216748768, "no_speech_prob": 1.4367496987688355e-06}, {"id": 256, "seek": 170576, "start": 1718.7, "end": 1724.72, "text": " and then you realize, oh I can write any function I like.", "tokens": [293, 550, 291, 4325, 11, 1954, 286, 393, 2464, 604, 2445, 286, 411, 13], "temperature": 0.0, "avg_logprob": -0.1318020526273751, "compression_ratio": 1.6009852216748768, "no_speech_prob": 1.4367496987688355e-06}, {"id": 257, "seek": 170576, "start": 1724.72, "end": 1731.52, "text": " So this is now going to mean that each mini-batch is going to contain a list of characters.", "tokens": [407, 341, 307, 586, 516, 281, 914, 300, 1184, 8382, 12, 65, 852, 307, 516, 281, 5304, 257, 1329, 295, 4342, 13], "temperature": 0.0, "avg_logprob": -0.1318020526273751, "compression_ratio": 1.6009852216748768, "no_speech_prob": 1.4367496987688355e-06}, {"id": 258, "seek": 173152, "start": 1731.52, "end": 1736.8, "text": " And so here's where we get to define all our different parameters. And so to make it the", "tokens": [400, 370, 510, 311, 689, 321, 483, 281, 6964, 439, 527, 819, 9834, 13, 400, 370, 281, 652, 309, 264], "temperature": 0.0, "avg_logprob": -0.15862873925103083, "compression_ratio": 1.6602870813397128, "no_speech_prob": 3.39313987751666e-06}, {"id": 259, "seek": 173152, "start": 1736.8, "end": 1742.8, "text": " same as previous sections of this notebook, I'm going to use the same batch size, the", "tokens": [912, 382, 3894, 10863, 295, 341, 21060, 11, 286, 478, 516, 281, 764, 264, 912, 15245, 2744, 11, 264], "temperature": 0.0, "avg_logprob": -0.15862873925103083, "compression_ratio": 1.6602870813397128, "no_speech_prob": 3.39313987751666e-06}, {"id": 260, "seek": 173152, "start": 1742.8, "end": 1746.92, "text": " same number of characters, I'm now going to rename it to be ptt since we know what that", "tokens": [912, 1230, 295, 4342, 11, 286, 478, 586, 516, 281, 36741, 309, 281, 312, 280, 6319, 1670, 321, 458, 437, 300], "temperature": 0.0, "avg_logprob": -0.15862873925103083, "compression_ratio": 1.6602870813397128, "no_speech_prob": 3.39313987751666e-06}, {"id": 261, "seek": 173152, "start": 1746.92, "end": 1755.6, "text": " means, the size of the embedding, and the size of our hidden state. Remembering that", "tokens": [1355, 11, 264, 2744, 295, 264, 12240, 3584, 11, 293, 264, 2744, 295, 527, 7633, 1785, 13, 5459, 278, 300], "temperature": 0.0, "avg_logprob": -0.15862873925103083, "compression_ratio": 1.6602870813397128, "no_speech_prob": 3.39313987751666e-06}, {"id": 262, "seek": 175560, "start": 1755.6, "end": 1765.36, "text": " size of our hidden state simply means, going all the way back to the start, and hidden", "tokens": [2744, 295, 527, 7633, 1785, 2935, 1355, 11, 516, 439, 264, 636, 646, 281, 264, 722, 11, 293, 7633], "temperature": 0.0, "avg_logprob": -0.1902923960434763, "compression_ratio": 1.737142857142857, "no_speech_prob": 5.338142727850936e-06}, {"id": 263, "seek": 175560, "start": 1765.36, "end": 1770.76, "text": " simply means the size of the state that's created by each of those orange arrows. So", "tokens": [2935, 1355, 264, 2744, 295, 264, 1785, 300, 311, 2942, 538, 1184, 295, 729, 7671, 19669, 13, 407], "temperature": 0.0, "avg_logprob": -0.1902923960434763, "compression_ratio": 1.737142857142857, "no_speech_prob": 5.338142727850936e-06}, {"id": 264, "seek": 175560, "start": 1770.76, "end": 1779.26, "text": " it's the size of each of those circles here.", "tokens": [309, 311, 264, 2744, 295, 1184, 295, 729, 13040, 510, 13], "temperature": 0.0, "avg_logprob": -0.1902923960434763, "compression_ratio": 1.737142857142857, "no_speech_prob": 5.338142727850936e-06}, {"id": 265, "seek": 175560, "start": 1779.26, "end": 1783.4399999999998, "text": " So having done that, we can then create a little dictionary saying what's our training,", "tokens": [407, 1419, 1096, 300, 11, 321, 393, 550, 1884, 257, 707, 25890, 1566, 437, 311, 527, 3097, 11], "temperature": 0.0, "avg_logprob": -0.1902923960434763, "compression_ratio": 1.737142857142857, "no_speech_prob": 5.338142727850936e-06}, {"id": 266, "seek": 178344, "start": 1783.44, "end": 1787.28, "text": " validation and test set. In this case I don't have a separate test set, so I'll just use", "tokens": [24071, 293, 1500, 992, 13, 682, 341, 1389, 286, 500, 380, 362, 257, 4994, 1500, 992, 11, 370, 286, 603, 445, 764], "temperature": 0.0, "avg_logprob": -0.17742066530837225, "compression_ratio": 1.661904761904762, "no_speech_prob": 2.1444820959004574e-05}, {"id": 267, "seek": 178344, "start": 1787.28, "end": 1794.2, "text": " the same thing. And then I can say, alright I want a language model data subclass of model", "tokens": [264, 912, 551, 13, 400, 550, 286, 393, 584, 11, 5845, 286, 528, 257, 2856, 2316, 1412, 1422, 11665, 295, 2316], "temperature": 0.0, "avg_logprob": -0.17742066530837225, "compression_ratio": 1.661904761904762, "no_speech_prob": 2.1444820959004574e-05}, {"id": 268, "seek": 178344, "start": 1794.2, "end": 1803.0, "text": " data, I'm going to grab it from text files, and this is my path, and this is my field,", "tokens": [1412, 11, 286, 478, 516, 281, 4444, 309, 490, 2487, 7098, 11, 293, 341, 307, 452, 3100, 11, 293, 341, 307, 452, 2519, 11], "temperature": 0.0, "avg_logprob": -0.17742066530837225, "compression_ratio": 1.661904761904762, "no_speech_prob": 2.1444820959004574e-05}, {"id": 269, "seek": 178344, "start": 1803.0, "end": 1811.44, "text": " which I defined earlier, and these are my files, and these are my hyperparameters.", "tokens": [597, 286, 7642, 3071, 11, 293, 613, 366, 452, 7098, 11, 293, 613, 366, 452, 9848, 2181, 335, 6202, 13], "temperature": 0.0, "avg_logprob": -0.17742066530837225, "compression_ratio": 1.661904761904762, "no_speech_prob": 2.1444820959004574e-05}, {"id": 270, "seek": 181144, "start": 1811.44, "end": 1815.16, "text": " Infrac's not going to do anything actually in this case because I don't think there's", "tokens": [682, 5779, 326, 311, 406, 516, 281, 360, 1340, 767, 294, 341, 1389, 570, 286, 500, 380, 519, 456, 311], "temperature": 0.0, "avg_logprob": -0.1743600342299912, "compression_ratio": 1.6064814814814814, "no_speech_prob": 5.0147245929110795e-06}, {"id": 271, "seek": 181144, "start": 1815.16, "end": 1822.24, "text": " going to be any character that appears less than 3 times. So that's probably redundant.", "tokens": [516, 281, 312, 604, 2517, 300, 7038, 1570, 813, 805, 1413, 13, 407, 300, 311, 1391, 40997, 13], "temperature": 0.0, "avg_logprob": -0.1743600342299912, "compression_ratio": 1.6064814814814814, "no_speech_prob": 5.0147245929110795e-06}, {"id": 272, "seek": 181144, "start": 1822.24, "end": 1829.0800000000002, "text": " So at the end of that, it says there's going to be 963 batches to go through. And so if", "tokens": [407, 412, 264, 917, 295, 300, 11, 309, 1619, 456, 311, 516, 281, 312, 24124, 18, 15245, 279, 281, 352, 807, 13, 400, 370, 498], "temperature": 0.0, "avg_logprob": -0.1743600342299912, "compression_ratio": 1.6064814814814814, "no_speech_prob": 5.0147245929110795e-06}, {"id": 273, "seek": 181144, "start": 1829.0800000000002, "end": 1835.3200000000002, "text": " you think about it, that should be equal to the number of tokens divided by the batch", "tokens": [291, 519, 466, 309, 11, 300, 820, 312, 2681, 281, 264, 1230, 295, 22667, 6666, 538, 264, 15245], "temperature": 0.0, "avg_logprob": -0.1743600342299912, "compression_ratio": 1.6064814814814814, "no_speech_prob": 5.0147245929110795e-06}, {"id": 274, "seek": 183532, "start": 1835.32, "end": 1846.12, "text": " size divided by BPTT, because that's like the size of each of those rectangles.", "tokens": [2744, 6666, 538, 40533, 28178, 11, 570, 300, 311, 411, 264, 2744, 295, 1184, 295, 729, 24077, 904, 13], "temperature": 0.0, "avg_logprob": -0.1534208059310913, "compression_ratio": 1.6, "no_speech_prob": 2.0904551547573647e-06}, {"id": 275, "seek": 183532, "start": 1846.12, "end": 1851.24, "text": " You'll find that in practice it's not exactly that. And the reason it's not exactly that", "tokens": [509, 603, 915, 300, 294, 3124, 309, 311, 406, 2293, 300, 13, 400, 264, 1778, 309, 311, 406, 2293, 300], "temperature": 0.0, "avg_logprob": -0.1534208059310913, "compression_ratio": 1.6, "no_speech_prob": 2.0904551547573647e-06}, {"id": 276, "seek": 183532, "start": 1851.24, "end": 1857.9199999999998, "text": " is that the authors of Torch Text did something pretty smart, which I think we briefly mentioned", "tokens": [307, 300, 264, 16552, 295, 7160, 339, 18643, 630, 746, 1238, 4069, 11, 597, 286, 519, 321, 10515, 2835], "temperature": 0.0, "avg_logprob": -0.1534208059310913, "compression_ratio": 1.6, "no_speech_prob": 2.0904551547573647e-06}, {"id": 277, "seek": 183532, "start": 1857.9199999999998, "end": 1862.6799999999998, "text": " this before. They said, we can't shuffle the data, like with images we like to shuffle", "tokens": [341, 949, 13, 814, 848, 11, 321, 393, 380, 39426, 264, 1412, 11, 411, 365, 5267, 321, 411, 281, 39426], "temperature": 0.0, "avg_logprob": -0.1534208059310913, "compression_ratio": 1.6, "no_speech_prob": 2.0904551547573647e-06}, {"id": 278, "seek": 186268, "start": 1862.68, "end": 1866.48, "text": " the order, so every time we see them in a different order, so a bit more randomness.", "tokens": [264, 1668, 11, 370, 633, 565, 321, 536, 552, 294, 257, 819, 1668, 11, 370, 257, 857, 544, 4974, 1287, 13], "temperature": 0.0, "avg_logprob": -0.16229336602347239, "compression_ratio": 1.5276595744680852, "no_speech_prob": 1.6797281432445743e-06}, {"id": 279, "seek": 186268, "start": 1866.48, "end": 1874.1200000000001, "text": " We can't shuffle because we need to be contiguous. But what we could do is randomize the length,", "tokens": [492, 393, 380, 39426, 570, 321, 643, 281, 312, 660, 30525, 13, 583, 437, 321, 727, 360, 307, 4974, 1125, 264, 4641, 11], "temperature": 0.0, "avg_logprob": -0.16229336602347239, "compression_ratio": 1.5276595744680852, "no_speech_prob": 1.6797281432445743e-06}, {"id": 280, "seek": 186268, "start": 1874.1200000000001, "end": 1880.52, "text": " basically randomize BPTT a little bit each time. And so that's what PyTorch does. It's", "tokens": [1936, 4974, 1125, 40533, 28178, 257, 707, 857, 1184, 565, 13, 400, 370, 300, 311, 437, 9953, 51, 284, 339, 775, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.16229336602347239, "compression_ratio": 1.5276595744680852, "no_speech_prob": 1.6797281432445743e-06}, {"id": 281, "seek": 186268, "start": 1880.52, "end": 1888.0, "text": " not always going to give us exactly 8 characters long. 5% of the time it will actually cut", "tokens": [406, 1009, 516, 281, 976, 505, 2293, 1649, 4342, 938, 13, 1025, 4, 295, 264, 565, 309, 486, 767, 1723], "temperature": 0.0, "avg_logprob": -0.16229336602347239, "compression_ratio": 1.5276595744680852, "no_speech_prob": 1.6797281432445743e-06}, {"id": 282, "seek": 188800, "start": 1888.0, "end": 1894.88, "text": " it in half, and then it's going to add on a small little standard deviation to make", "tokens": [309, 294, 1922, 11, 293, 550, 309, 311, 516, 281, 909, 322, 257, 1359, 707, 3832, 25163, 281, 652], "temperature": 0.0, "avg_logprob": -0.2933904763424035, "compression_ratio": 1.5, "no_speech_prob": 1.8058317436953075e-05}, {"id": 283, "seek": 188800, "start": 1894.88, "end": 1901.68, "text": " it slightly bigger or smaller than 4 or 8. So it's going to be slightly different to", "tokens": [309, 4748, 3801, 420, 4356, 813, 1017, 420, 1649, 13, 407, 309, 311, 516, 281, 312, 4748, 819, 281], "temperature": 0.0, "avg_logprob": -0.2933904763424035, "compression_ratio": 1.5, "no_speech_prob": 1.8058317436953075e-05}, {"id": 284, "seek": 188800, "start": 1901.68, "end": 1905.68, "text": " 8 on average.", "tokens": [1649, 322, 4274, 13], "temperature": 0.0, "avg_logprob": -0.2933904763424035, "compression_ratio": 1.5, "no_speech_prob": 1.8058317436953075e-05}, {"id": 285, "seek": 188800, "start": 1905.68, "end": 1913.28, "text": " Question asked, Is it going to be constant per minibatch?", "tokens": [14464, 2351, 11, 1119, 309, 516, 281, 312, 5754, 680, 923, 897, 852, 30], "temperature": 0.0, "avg_logprob": -0.2933904763424035, "compression_ratio": 1.5, "no_speech_prob": 1.8058317436953075e-05}, {"id": 286, "seek": 191328, "start": 1913.28, "end": 1926.04, "text": " Yes, exactly, that's right. So a minibatch has to kind of do a matrix multiplication.", "tokens": [1079, 11, 2293, 11, 300, 311, 558, 13, 407, 257, 923, 897, 852, 575, 281, 733, 295, 360, 257, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.174097102028983, "compression_ratio": 1.5818181818181818, "no_speech_prob": 2.6425796022522263e-06}, {"id": 287, "seek": 191328, "start": 1926.04, "end": 1934.28, "text": " And the minibatch size has to remain constant because we've got this h weight matrix that", "tokens": [400, 264, 923, 897, 852, 2744, 575, 281, 6222, 5754, 570, 321, 600, 658, 341, 276, 3364, 8141, 300], "temperature": 0.0, "avg_logprob": -0.174097102028983, "compression_ratio": 1.5818181818181818, "no_speech_prob": 2.6425796022522263e-06}, {"id": 288, "seek": 191328, "start": 1934.28, "end": 1942.36, "text": " has to line up in size with the size of the minibatch. But the number of the sequence", "tokens": [575, 281, 1622, 493, 294, 2744, 365, 264, 2744, 295, 264, 923, 897, 852, 13, 583, 264, 1230, 295, 264, 8310], "temperature": 0.0, "avg_logprob": -0.174097102028983, "compression_ratio": 1.5818181818181818, "no_speech_prob": 2.6425796022522263e-06}, {"id": 289, "seek": 194236, "start": 1942.36, "end": 1953.6, "text": " length can change, no problem.", "tokens": [4641, 393, 1319, 11, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.17770910263061523, "compression_ratio": 1.38, "no_speech_prob": 3.84492796001723e-06}, {"id": 290, "seek": 194236, "start": 1953.6, "end": 1959.0, "text": " So that's why we have 963. So the length of a data loader is how many minibatches. In", "tokens": [407, 300, 311, 983, 321, 362, 24124, 18, 13, 407, 264, 4641, 295, 257, 1412, 3677, 260, 307, 577, 867, 923, 897, 852, 279, 13, 682], "temperature": 0.0, "avg_logprob": -0.17770910263061523, "compression_ratio": 1.38, "no_speech_prob": 3.84492796001723e-06}, {"id": 291, "seek": 194236, "start": 1959.0, "end": 1964.8, "text": " this case it's a little bit approximate. Number of tokens is how many unique things are in", "tokens": [341, 1389, 309, 311, 257, 707, 857, 30874, 13, 5118, 295, 22667, 307, 577, 867, 3845, 721, 366, 294], "temperature": 0.0, "avg_logprob": -0.17770910263061523, "compression_ratio": 1.38, "no_speech_prob": 3.84492796001723e-06}, {"id": 292, "seek": 196480, "start": 1964.8, "end": 1974.12, "text": " the vocabulary. And remember, after we run this line, text now does not just contain", "tokens": [264, 19864, 13, 400, 1604, 11, 934, 321, 1190, 341, 1622, 11, 2487, 586, 775, 406, 445, 5304], "temperature": 0.0, "avg_logprob": -0.10135319156031455, "compression_ratio": 1.5263157894736843, "no_speech_prob": 8.398053068958689e-06}, {"id": 293, "seek": 196480, "start": 1974.12, "end": 1982.96, "text": " a description of what we want, but it also contains an extra attribute called vocab,", "tokens": [257, 3855, 295, 437, 321, 528, 11, 457, 309, 611, 8306, 364, 2857, 19667, 1219, 2329, 455, 11], "temperature": 0.0, "avg_logprob": -0.10135319156031455, "compression_ratio": 1.5263157894736843, "no_speech_prob": 8.398053068958689e-06}, {"id": 294, "seek": 196480, "start": 1982.96, "end": 1993.32, "text": " which contains stuff like a list of all of the unique items in the vocabulary and a reverse", "tokens": [597, 8306, 1507, 411, 257, 1329, 295, 439, 295, 264, 3845, 4754, 294, 264, 19864, 293, 257, 9943], "temperature": 0.0, "avg_logprob": -0.10135319156031455, "compression_ratio": 1.5263157894736843, "no_speech_prob": 8.398053068958689e-06}, {"id": 295, "seek": 199332, "start": 1993.32, "end": 2003.48, "text": " mapping from each item to its number. So that text object is now an important thing to keep", "tokens": [18350, 490, 1184, 3174, 281, 1080, 1230, 13, 407, 300, 2487, 2657, 307, 586, 364, 1021, 551, 281, 1066], "temperature": 0.0, "avg_logprob": -0.12761286142710093, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.058046675301739e-06}, {"id": 296, "seek": 199332, "start": 2003.48, "end": 2009.6399999999999, "text": " track of.", "tokens": [2837, 295, 13], "temperature": 0.0, "avg_logprob": -0.12761286142710093, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.058046675301739e-06}, {"id": 297, "seek": 199332, "start": 2009.6399999999999, "end": 2016.5, "text": " So let's now try this. So we started out by looking at the class. So the class is exactly", "tokens": [407, 718, 311, 586, 853, 341, 13, 407, 321, 1409, 484, 538, 1237, 412, 264, 1508, 13, 407, 264, 1508, 307, 2293], "temperature": 0.0, "avg_logprob": -0.12761286142710093, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.058046675301739e-06}, {"id": 298, "seek": 199332, "start": 2016.5, "end": 2022.28, "text": " the same as the class we've had before. The only key difference is to call init hidden,", "tokens": [264, 912, 382, 264, 1508, 321, 600, 632, 949, 13, 440, 787, 2141, 2649, 307, 281, 818, 3157, 7633, 11], "temperature": 0.0, "avg_logprob": -0.12761286142710093, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.058046675301739e-06}, {"id": 299, "seek": 202228, "start": 2022.28, "end": 2029.68, "text": " which sets out. So h is not a variable anymore, it's now an attribute. self.h is a variable", "tokens": [597, 6352, 484, 13, 407, 276, 307, 406, 257, 7006, 3602, 11, 309, 311, 586, 364, 19667, 13, 2698, 13, 71, 307, 257, 7006], "temperature": 0.0, "avg_logprob": -0.14013235569000243, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.4971021300880238e-05}, {"id": 300, "seek": 202228, "start": 2029.68, "end": 2033.36, "text": " containing a bunch of zeros.", "tokens": [19273, 257, 3840, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.14013235569000243, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.4971021300880238e-05}, {"id": 301, "seek": 202228, "start": 2033.36, "end": 2040.24, "text": " Now I mentioned that batch size remains constant each time, but unfortunately when I said that", "tokens": [823, 286, 2835, 300, 15245, 2744, 7023, 5754, 1184, 565, 11, 457, 7015, 562, 286, 848, 300], "temperature": 0.0, "avg_logprob": -0.14013235569000243, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.4971021300880238e-05}, {"id": 302, "seek": 202228, "start": 2040.24, "end": 2048.6, "text": " I lied to you. And the way that I lied to you is that the very last minibatch will be", "tokens": [286, 20101, 281, 291, 13, 400, 264, 636, 300, 286, 20101, 281, 291, 307, 300, 264, 588, 1036, 923, 897, 852, 486, 312], "temperature": 0.0, "avg_logprob": -0.14013235569000243, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.4971021300880238e-05}, {"id": 303, "seek": 204860, "start": 2048.6, "end": 2054.12, "text": " shorter. The very last minibatch is actually going to have less than 64. It might be exactly", "tokens": [11639, 13, 440, 588, 1036, 923, 897, 852, 307, 767, 516, 281, 362, 1570, 813, 12145, 13, 467, 1062, 312, 2293], "temperature": 0.0, "avg_logprob": -0.16690850001509472, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.664599590701982e-06}, {"id": 304, "seek": 204860, "start": 2054.12, "end": 2059.7999999999997, "text": " the right size if it so happens that this dataset is exactly divisible by bptt times", "tokens": [264, 558, 2744, 498, 309, 370, 2314, 300, 341, 28872, 307, 2293, 25974, 964, 538, 272, 662, 83, 1413], "temperature": 0.0, "avg_logprob": -0.16690850001509472, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.664599590701982e-06}, {"id": 305, "seek": 204860, "start": 2059.7999999999997, "end": 2064.2799999999997, "text": " batch size, but it probably isn't. So the last batch will probably have a little bit", "tokens": [15245, 2744, 11, 457, 309, 1391, 1943, 380, 13, 407, 264, 1036, 15245, 486, 1391, 362, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.16690850001509472, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.664599590701982e-06}, {"id": 306, "seek": 204860, "start": 2064.2799999999997, "end": 2066.36, "text": " less.", "tokens": [1570, 13], "temperature": 0.0, "avg_logprob": -0.16690850001509472, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.664599590701982e-06}, {"id": 307, "seek": 204860, "start": 2066.36, "end": 2071.02, "text": " And so that's why I do a little check here that says let's check that the batch size", "tokens": [400, 370, 300, 311, 983, 286, 360, 257, 707, 1520, 510, 300, 1619, 718, 311, 1520, 300, 264, 15245, 2744], "temperature": 0.0, "avg_logprob": -0.16690850001509472, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.664599590701982e-06}, {"id": 308, "seek": 207102, "start": 2071.02, "end": 2083.52, "text": " inside self.h. And so self.h is going to be the height, the height is going to be the", "tokens": [1854, 2698, 13, 71, 13, 400, 370, 2698, 13, 71, 307, 516, 281, 312, 264, 6681, 11, 264, 6681, 307, 516, 281, 312, 264], "temperature": 0.0, "avg_logprob": -0.13585018785032507, "compression_ratio": 1.6903225806451614, "no_speech_prob": 2.857311983461841e-06}, {"id": 309, "seek": 207102, "start": 2083.52, "end": 2088.64, "text": " number of activations and the width is going to be the minibatch size. Check that that's", "tokens": [1230, 295, 2430, 763, 293, 264, 11402, 307, 516, 281, 312, 264, 923, 897, 852, 2744, 13, 6881, 300, 300, 311], "temperature": 0.0, "avg_logprob": -0.13585018785032507, "compression_ratio": 1.6903225806451614, "no_speech_prob": 2.857311983461841e-06}, {"id": 310, "seek": 207102, "start": 2088.64, "end": 2100.6, "text": " equal to the actual batch size length that we've received. And if they're not the same,", "tokens": [2681, 281, 264, 3539, 15245, 2744, 4641, 300, 321, 600, 4613, 13, 400, 498, 436, 434, 406, 264, 912, 11], "temperature": 0.0, "avg_logprob": -0.13585018785032507, "compression_ratio": 1.6903225806451614, "no_speech_prob": 2.857311983461841e-06}, {"id": 311, "seek": 210060, "start": 2100.6, "end": 2103.14, "text": " then set it back to zeros again.", "tokens": [550, 992, 309, 646, 281, 35193, 797, 13], "temperature": 0.0, "avg_logprob": -0.1332043239048549, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.157349849265302e-06}, {"id": 312, "seek": 210060, "start": 2103.14, "end": 2108.24, "text": " So this is just a minor little wrinkle that basically at the end of each epoch, it's going", "tokens": [407, 341, 307, 445, 257, 6696, 707, 928, 14095, 300, 1936, 412, 264, 917, 295, 1184, 30992, 339, 11, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.1332043239048549, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.157349849265302e-06}, {"id": 313, "seek": 210060, "start": 2108.24, "end": 2115.54, "text": " to do like a little mini-minibatch. And so then as soon as it starts the next epoch,", "tokens": [281, 360, 411, 257, 707, 8382, 12, 2367, 897, 852, 13, 400, 370, 550, 382, 2321, 382, 309, 3719, 264, 958, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.1332043239048549, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.157349849265302e-06}, {"id": 314, "seek": 210060, "start": 2115.54, "end": 2119.24, "text": " it's going to see that they're not the same again and it'll reinitialize it to the correct", "tokens": [309, 311, 516, 281, 536, 300, 436, 434, 406, 264, 912, 797, 293, 309, 603, 6561, 270, 831, 1125, 309, 281, 264, 3006], "temperature": 0.0, "avg_logprob": -0.1332043239048549, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.157349849265302e-06}, {"id": 315, "seek": 210060, "start": 2119.24, "end": 2126.08, "text": " full batch size. So that's why if you're wondering, there's an init hidden not just in the constructor", "tokens": [1577, 15245, 2744, 13, 407, 300, 311, 983, 498, 291, 434, 6359, 11, 456, 311, 364, 3157, 7633, 406, 445, 294, 264, 47479], "temperature": 0.0, "avg_logprob": -0.1332043239048549, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.157349849265302e-06}, {"id": 316, "seek": 212608, "start": 2126.08, "end": 2134.72, "text": " but also inside forward, it's to handle this end of each epoch, start of each epoch difference.", "tokens": [457, 611, 1854, 2128, 11, 309, 311, 281, 4813, 341, 917, 295, 1184, 30992, 339, 11, 722, 295, 1184, 30992, 339, 2649, 13], "temperature": 0.0, "avg_logprob": -0.09244814203746284, "compression_ratio": 1.5511363636363635, "no_speech_prob": 6.475944474004791e-07}, {"id": 317, "seek": 212608, "start": 2134.72, "end": 2144.36, "text": " Not an important point by any means, but potentially confusing when you see it.", "tokens": [1726, 364, 1021, 935, 538, 604, 1355, 11, 457, 7263, 13181, 562, 291, 536, 309, 13], "temperature": 0.0, "avg_logprob": -0.09244814203746284, "compression_ratio": 1.5511363636363635, "no_speech_prob": 6.475944474004791e-07}, {"id": 318, "seek": 212608, "start": 2144.36, "end": 2153.12, "text": " So the last wrinkle. The last wrinkle is something which I think is something that slightly sucks", "tokens": [407, 264, 1036, 928, 14095, 13, 440, 1036, 928, 14095, 307, 746, 597, 286, 519, 307, 746, 300, 4748, 15846], "temperature": 0.0, "avg_logprob": -0.09244814203746284, "compression_ratio": 1.5511363636363635, "no_speech_prob": 6.475944474004791e-07}, {"id": 319, "seek": 215312, "start": 2153.12, "end": 2157.4, "text": " about PyTorch and maybe somebody can be nice enough to try and fix it with a PR if anybody", "tokens": [466, 9953, 51, 284, 339, 293, 1310, 2618, 393, 312, 1481, 1547, 281, 853, 293, 3191, 309, 365, 257, 11568, 498, 4472], "temperature": 0.0, "avg_logprob": -0.12876887803667048, "compression_ratio": 1.6318181818181818, "no_speech_prob": 7.646495760127436e-06}, {"id": 320, "seek": 215312, "start": 2157.4, "end": 2167.2799999999997, "text": " feels like it, which is that the loss functions such as softmax are not happy receiving a", "tokens": [3417, 411, 309, 11, 597, 307, 300, 264, 4470, 6828, 1270, 382, 2787, 41167, 366, 406, 2055, 10040, 257], "temperature": 0.0, "avg_logprob": -0.12876887803667048, "compression_ratio": 1.6318181818181818, "no_speech_prob": 7.646495760127436e-06}, {"id": 321, "seek": 215312, "start": 2167.2799999999997, "end": 2177.08, "text": " rank 3 tensor. Remember a rank 3 tensor is just another way of saying a dimension 3 array.", "tokens": [6181, 805, 40863, 13, 5459, 257, 6181, 805, 40863, 307, 445, 1071, 636, 295, 1566, 257, 10139, 805, 10225, 13], "temperature": 0.0, "avg_logprob": -0.12876887803667048, "compression_ratio": 1.6318181818181818, "no_speech_prob": 7.646495760127436e-06}, {"id": 322, "seek": 215312, "start": 2177.08, "end": 2181.56, "text": " There's no particular reason they ought to not be happy receiving a rank 3 tensor, like", "tokens": [821, 311, 572, 1729, 1778, 436, 13416, 281, 406, 312, 2055, 10040, 257, 6181, 805, 40863, 11, 411], "temperature": 0.0, "avg_logprob": -0.12876887803667048, "compression_ratio": 1.6318181818181818, "no_speech_prob": 7.646495760127436e-06}, {"id": 323, "seek": 218156, "start": 2181.56, "end": 2186.68, "text": " somebody could write some code to say a rank 3 tensor is probably a sequence length by", "tokens": [2618, 727, 2464, 512, 3089, 281, 584, 257, 6181, 805, 40863, 307, 1391, 257, 8310, 4641, 538], "temperature": 0.0, "avg_logprob": -0.19672743031676387, "compression_ratio": 1.4480874316939891, "no_speech_prob": 1.8738714970822912e-06}, {"id": 324, "seek": 218156, "start": 2186.68, "end": 2197.84, "text": " batch size by results thing, and so you should just do it for each of the two initial axes.", "tokens": [15245, 2744, 538, 3542, 551, 11, 293, 370, 291, 820, 445, 360, 309, 337, 1184, 295, 264, 732, 5883, 35387, 13], "temperature": 0.0, "avg_logprob": -0.19672743031676387, "compression_ratio": 1.4480874316939891, "no_speech_prob": 1.8738714970822912e-06}, {"id": 325, "seek": 218156, "start": 2197.84, "end": 2204.96, "text": " But no one's done that, so it expects it to be a rank 2 tensor. Funnily enough, it can", "tokens": [583, 572, 472, 311, 1096, 300, 11, 370, 309, 33280, 309, 281, 312, 257, 6181, 568, 40863, 13, 11166, 77, 953, 1547, 11, 309, 393], "temperature": 0.0, "avg_logprob": -0.19672743031676387, "compression_ratio": 1.4480874316939891, "no_speech_prob": 1.8738714970822912e-06}, {"id": 326, "seek": 220496, "start": 2204.96, "end": 2219.68, "text": " handle rank 2 or rank 4, but not rank 3.", "tokens": [4813, 6181, 568, 420, 6181, 1017, 11, 457, 406, 6181, 805, 13], "temperature": 0.0, "avg_logprob": -0.13041315580669202, "compression_ratio": 1.205607476635514, "no_speech_prob": 1.9947281089116586e-06}, {"id": 327, "seek": 220496, "start": 2219.68, "end": 2228.32, "text": " So we've got a rank 2 tensor containing for each time period, I can't remember which way", "tokens": [407, 321, 600, 658, 257, 6181, 568, 40863, 19273, 337, 1184, 565, 2896, 11, 286, 393, 380, 1604, 597, 636], "temperature": 0.0, "avg_logprob": -0.13041315580669202, "compression_ratio": 1.205607476635514, "no_speech_prob": 1.9947281089116586e-06}, {"id": 328, "seek": 222832, "start": 2228.32, "end": 2239.96, "text": " around the axes are, but for each time period, for each batch, we've got our predictions.", "tokens": [926, 264, 35387, 366, 11, 457, 337, 1184, 565, 2896, 11, 337, 1184, 15245, 11, 321, 600, 658, 527, 21264, 13], "temperature": 0.0, "avg_logprob": -0.15560733949815905, "compression_ratio": 2.0074074074074075, "no_speech_prob": 2.4824776119203307e-06}, {"id": 329, "seek": 222832, "start": 2239.96, "end": 2251.32, "text": " And then we've got our actuals for each time period, for each batch, we've got our predictions,", "tokens": [400, 550, 321, 600, 658, 527, 3539, 82, 337, 1184, 565, 2896, 11, 337, 1184, 15245, 11, 321, 600, 658, 527, 21264, 11], "temperature": 0.0, "avg_logprob": -0.15560733949815905, "compression_ratio": 2.0074074074074075, "no_speech_prob": 2.4824776119203307e-06}, {"id": 330, "seek": 222832, "start": 2251.32, "end": 2256.44, "text": " and we've got our actuals. And so we just want to check whether they're the same. And", "tokens": [293, 321, 600, 658, 527, 3539, 82, 13, 400, 370, 321, 445, 528, 281, 1520, 1968, 436, 434, 264, 912, 13, 400], "temperature": 0.0, "avg_logprob": -0.15560733949815905, "compression_ratio": 2.0074074074074075, "no_speech_prob": 2.4824776119203307e-06}, {"id": 331, "seek": 225644, "start": 2256.44, "end": 2262.68, "text": " so in an ideal world, a lost function would check item 1,1, then item 1,2, and then item", "tokens": [370, 294, 364, 7157, 1002, 11, 257, 2731, 2445, 576, 1520, 3174, 502, 11, 16, 11, 550, 3174, 502, 11, 17, 11, 293, 550, 3174], "temperature": 0.0, "avg_logprob": -0.14984292569367783, "compression_ratio": 1.5888324873096447, "no_speech_prob": 7.889247171988245e-06}, {"id": 332, "seek": 225644, "start": 2262.68, "end": 2268.56, "text": " 1,3, but since that hasn't been written, we just have to flatten them both out. We can", "tokens": [502, 11, 18, 11, 457, 1670, 300, 6132, 380, 668, 3720, 11, 321, 445, 362, 281, 24183, 552, 1293, 484, 13, 492, 393], "temperature": 0.0, "avg_logprob": -0.14984292569367783, "compression_ratio": 1.5888324873096447, "no_speech_prob": 7.889247171988245e-06}, {"id": 333, "seek": 225644, "start": 2268.56, "end": 2273.2000000000003, "text": " literally just flatten them out, put rows to rows.", "tokens": [3736, 445, 24183, 552, 484, 11, 829, 13241, 281, 13241, 13], "temperature": 0.0, "avg_logprob": -0.14984292569367783, "compression_ratio": 1.5888324873096447, "no_speech_prob": 7.889247171988245e-06}, {"id": 334, "seek": 225644, "start": 2273.2000000000003, "end": 2284.84, "text": " And so that's why here I have to use.view. And so.view says the number of columns will", "tokens": [400, 370, 300, 311, 983, 510, 286, 362, 281, 764, 2411, 1759, 13, 400, 370, 2411, 1759, 1619, 264, 1230, 295, 13766, 486], "temperature": 0.0, "avg_logprob": -0.14984292569367783, "compression_ratio": 1.5888324873096447, "no_speech_prob": 7.889247171988245e-06}, {"id": 335, "seek": 228484, "start": 2284.84, "end": 2289.6000000000004, "text": " be equal to the size of the vocab, because remember we're going to end up with a probability", "tokens": [312, 2681, 281, 264, 2744, 295, 264, 2329, 455, 11, 570, 1604, 321, 434, 516, 281, 917, 493, 365, 257, 8482], "temperature": 0.0, "avg_logprob": -0.15644656419754027, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.0129928341484629e-05}, {"id": 336, "seek": 228484, "start": 2289.6000000000004, "end": 2295.32, "text": " for each letter, and then the number of rows is however big is necessary, which will be", "tokens": [337, 1184, 5063, 11, 293, 550, 264, 1230, 295, 13241, 307, 4461, 955, 307, 4818, 11, 597, 486, 312], "temperature": 0.0, "avg_logprob": -0.15644656419754027, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.0129928341484629e-05}, {"id": 337, "seek": 228484, "start": 2295.32, "end": 2302.8, "text": " equal to batch size times bptt.", "tokens": [2681, 281, 15245, 2744, 1413, 272, 662, 83, 13], "temperature": 0.0, "avg_logprob": -0.15644656419754027, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.0129928341484629e-05}, {"id": 338, "seek": 228484, "start": 2302.8, "end": 2311.0, "text": " And then you may be wondering where I do that for the target, and the answer is TorchText", "tokens": [400, 550, 291, 815, 312, 6359, 689, 286, 360, 300, 337, 264, 3779, 11, 293, 264, 1867, 307, 7160, 339, 50198], "temperature": 0.0, "avg_logprob": -0.15644656419754027, "compression_ratio": 1.5252525252525253, "no_speech_prob": 1.0129928341484629e-05}, {"id": 339, "seek": 231100, "start": 2311.0, "end": 2315.84, "text": " knows that the target needs to look like that, so TorchText has already done that for us.", "tokens": [3255, 300, 264, 3779, 2203, 281, 574, 411, 300, 11, 370, 7160, 339, 50198, 575, 1217, 1096, 300, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.15801069871434625, "compression_ratio": 1.680672268907563, "no_speech_prob": 1.4823552874076995e-06}, {"id": 340, "seek": 231100, "start": 2315.84, "end": 2320.32, "text": " So TorchText automatically changes the target to be flattened out.", "tokens": [407, 7160, 339, 50198, 6772, 2962, 264, 3779, 281, 312, 24183, 292, 484, 13], "temperature": 0.0, "avg_logprob": -0.15801069871434625, "compression_ratio": 1.680672268907563, "no_speech_prob": 1.4823552874076995e-06}, {"id": 341, "seek": 231100, "start": 2320.32, "end": 2325.28, "text": " As you might actually remember, if you go back to lesson 4, when we actually looked", "tokens": [1018, 291, 1062, 767, 1604, 11, 498, 291, 352, 646, 281, 6898, 1017, 11, 562, 321, 767, 2956], "temperature": 0.0, "avg_logprob": -0.15801069871434625, "compression_ratio": 1.680672268907563, "no_speech_prob": 1.4823552874076995e-06}, {"id": 342, "seek": 231100, "start": 2325.28, "end": 2331.64, "text": " at a mini-batch that spat out of TorchText, we noticed actually that it was flattened,", "tokens": [412, 257, 8382, 12, 65, 852, 300, 15000, 484, 295, 7160, 339, 50198, 11, 321, 5694, 767, 300, 309, 390, 24183, 292, 11], "temperature": 0.0, "avg_logprob": -0.15801069871434625, "compression_ratio": 1.680672268907563, "no_speech_prob": 1.4823552874076995e-06}, {"id": 343, "seek": 231100, "start": 2331.64, "end": 2338.64, "text": " and I said we'll learn about why later, and so later it has now arrived.", "tokens": [293, 286, 848, 321, 603, 1466, 466, 983, 1780, 11, 293, 370, 1780, 309, 575, 586, 6678, 13], "temperature": 0.0, "avg_logprob": -0.15801069871434625, "compression_ratio": 1.680672268907563, "no_speech_prob": 1.4823552874076995e-06}, {"id": 344, "seek": 233864, "start": 2338.64, "end": 2350.3199999999997, "text": " So there are the 3 wrinkles. Get rid of the history, I guess 4 wrinkles. Recreate the", "tokens": [407, 456, 366, 264, 805, 34822, 13, 3240, 3973, 295, 264, 2503, 11, 286, 2041, 1017, 34822, 13, 9647, 265, 473, 264], "temperature": 0.0, "avg_logprob": -0.1828789145259534, "compression_ratio": 1.4068965517241379, "no_speech_prob": 3.0415881155931856e-06}, {"id": 345, "seek": 233864, "start": 2350.3199999999997, "end": 2360.2799999999997, "text": " hidden state if the batch size changes. Flatten out and then use TorchText to create mini-batches", "tokens": [7633, 1785, 498, 264, 15245, 2744, 2962, 13, 3235, 32733, 484, 293, 550, 764, 7160, 339, 50198, 281, 1884, 8382, 12, 65, 852, 279], "temperature": 0.0, "avg_logprob": -0.1828789145259534, "compression_ratio": 1.4068965517241379, "no_speech_prob": 3.0415881155931856e-06}, {"id": 346, "seek": 233864, "start": 2360.2799999999997, "end": 2362.24, "text": " that line up nicely.", "tokens": [300, 1622, 493, 9594, 13], "temperature": 0.0, "avg_logprob": -0.1828789145259534, "compression_ratio": 1.4068965517241379, "no_speech_prob": 3.0415881155931856e-06}, {"id": 347, "seek": 236224, "start": 2362.24, "end": 2370.4399999999996, "text": " So once we do those things, we can then create our model, create our optimizer with that", "tokens": [407, 1564, 321, 360, 729, 721, 11, 321, 393, 550, 1884, 527, 2316, 11, 1884, 527, 5028, 6545, 365, 300], "temperature": 0.0, "avg_logprob": -0.10477845107807833, "compression_ratio": 1.2244897959183674, "no_speech_prob": 7.646516678505577e-06}, {"id": 348, "seek": 236224, "start": 2370.4399999999996, "end": 2378.64, "text": " model's parameters, and fit it.", "tokens": [2316, 311, 9834, 11, 293, 3318, 309, 13], "temperature": 0.0, "avg_logprob": -0.10477845107807833, "compression_ratio": 1.2244897959183674, "no_speech_prob": 7.646516678505577e-06}, {"id": 349, "seek": 237864, "start": 2378.64, "end": 2396.04, "text": " One thing to be careful of here is that Softmax now, as of PyTorch 0.3, requires that we pass", "tokens": [1485, 551, 281, 312, 5026, 295, 510, 307, 300, 16985, 41167, 586, 11, 382, 295, 9953, 51, 284, 339, 1958, 13, 18, 11, 7029, 300, 321, 1320], "temperature": 0.0, "avg_logprob": -0.12414327968250621, "compression_ratio": 1.3407407407407408, "no_speech_prob": 2.4824764750519535e-06}, {"id": 350, "seek": 237864, "start": 2396.04, "end": 2404.96, "text": " in a number here saying which axis do we want to do the Softmax over. So at this point,", "tokens": [294, 257, 1230, 510, 1566, 597, 10298, 360, 321, 528, 281, 360, 264, 16985, 41167, 670, 13, 407, 412, 341, 935, 11], "temperature": 0.0, "avg_logprob": -0.12414327968250621, "compression_ratio": 1.3407407407407408, "no_speech_prob": 2.4824764750519535e-06}, {"id": 351, "seek": 240496, "start": 2404.96, "end": 2411.92, "text": " this is a 3-dimensional tensor, so we want to do the Softmax over the final axis. So", "tokens": [341, 307, 257, 805, 12, 18759, 40863, 11, 370, 321, 528, 281, 360, 264, 16985, 41167, 670, 264, 2572, 10298, 13, 407], "temperature": 0.0, "avg_logprob": -0.15760461274568024, "compression_ratio": 1.9238578680203047, "no_speech_prob": 7.4112508627877105e-06}, {"id": 352, "seek": 240496, "start": 2411.92, "end": 2417.88, "text": " when I say which axis do we do the Softmax over, remember we divide by, so we go e to", "tokens": [562, 286, 584, 597, 10298, 360, 321, 360, 264, 16985, 41167, 670, 11, 1604, 321, 9845, 538, 11, 370, 321, 352, 308, 281], "temperature": 0.0, "avg_logprob": -0.15760461274568024, "compression_ratio": 1.9238578680203047, "no_speech_prob": 7.4112508627877105e-06}, {"id": 353, "seek": 240496, "start": 2417.88, "end": 2424.48, "text": " the xi divided by the sum of e to the xi, so it's saying which axis do we sum over. So", "tokens": [264, 36800, 6666, 538, 264, 2408, 295, 308, 281, 264, 36800, 11, 370, 309, 311, 1566, 597, 10298, 360, 321, 2408, 670, 13, 407], "temperature": 0.0, "avg_logprob": -0.15760461274568024, "compression_ratio": 1.9238578680203047, "no_speech_prob": 7.4112508627877105e-06}, {"id": 354, "seek": 240496, "start": 2424.48, "end": 2426.52, "text": " which axis do we want to sum to 1.", "tokens": [597, 10298, 360, 321, 528, 281, 2408, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.15760461274568024, "compression_ratio": 1.9238578680203047, "no_speech_prob": 7.4112508627877105e-06}, {"id": 355, "seek": 240496, "start": 2426.52, "end": 2431.2400000000002, "text": " And so in this case, clearly we want to do it over the last axis because the last axis", "tokens": [400, 370, 294, 341, 1389, 11, 4448, 321, 528, 281, 360, 309, 670, 264, 1036, 10298, 570, 264, 1036, 10298], "temperature": 0.0, "avg_logprob": -0.15760461274568024, "compression_ratio": 1.9238578680203047, "no_speech_prob": 7.4112508627877105e-06}, {"id": 356, "seek": 243124, "start": 2431.24, "end": 2436.3999999999996, "text": " is the one that contains the probability per letter of the alphabet. And we want all of", "tokens": [307, 264, 472, 300, 8306, 264, 8482, 680, 5063, 295, 264, 23339, 13, 400, 321, 528, 439, 295], "temperature": 0.0, "avg_logprob": -0.1652742854335852, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.813008677549078e-06}, {"id": 357, "seek": 243124, "start": 2436.3999999999996, "end": 2440.9199999999996, "text": " those probabilities to sum to 1.", "tokens": [729, 33783, 281, 2408, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.1652742854335852, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.813008677549078e-06}, {"id": 358, "seek": 243124, "start": 2440.9199999999996, "end": 2448.66, "text": " So therefore, to run this notebook, you're going to need PyTorch 0.3 which just came", "tokens": [407, 4412, 11, 281, 1190, 341, 21060, 11, 291, 434, 516, 281, 643, 9953, 51, 284, 339, 1958, 13, 18, 597, 445, 1361], "temperature": 0.0, "avg_logprob": -0.1652742854335852, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.813008677549078e-06}, {"id": 359, "seek": 243124, "start": 2448.66, "end": 2452.2, "text": " out this week. So if you're doing this on the MOOC, you're fine. I'm sure you've got", "tokens": [484, 341, 1243, 13, 407, 498, 291, 434, 884, 341, 322, 264, 49197, 34, 11, 291, 434, 2489, 13, 286, 478, 988, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.1652742854335852, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.813008677549078e-06}, {"id": 360, "seek": 243124, "start": 2452.2, "end": 2455.2799999999997, "text": " at least 0.3 or later.", "tokens": [412, 1935, 1958, 13, 18, 420, 1780, 13], "temperature": 0.0, "avg_logprob": -0.1652742854335852, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.813008677549078e-06}, {"id": 361, "seek": 243124, "start": 2455.2799999999997, "end": 2459.3999999999996, "text": " Where else are the students here? If you just go conda env update, it will automatically", "tokens": [2305, 1646, 366, 264, 1731, 510, 30, 759, 291, 445, 352, 2224, 64, 2267, 5623, 11, 309, 486, 6772], "temperature": 0.0, "avg_logprob": -0.1652742854335852, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.813008677549078e-06}, {"id": 362, "seek": 245940, "start": 2459.4, "end": 2462.84, "text": " update you to 0.3.", "tokens": [5623, 291, 281, 1958, 13, 18, 13], "temperature": 0.0, "avg_logprob": -0.12145676455654941, "compression_ratio": 1.5336322869955157, "no_speech_prob": 3.966967142332578e-06}, {"id": 363, "seek": 245940, "start": 2462.84, "end": 2470.0, "text": " The really great news is that 0.3, although it does not yet officially support Windows,", "tokens": [440, 534, 869, 2583, 307, 300, 1958, 13, 18, 11, 4878, 309, 775, 406, 1939, 12053, 1406, 8591, 11], "temperature": 0.0, "avg_logprob": -0.12145676455654941, "compression_ratio": 1.5336322869955157, "no_speech_prob": 3.966967142332578e-06}, {"id": 364, "seek": 245940, "start": 2470.0, "end": 2475.88, "text": " it does in practice. I successfully installed 0.3 from conda yesterday by typing conda install", "tokens": [309, 775, 294, 3124, 13, 286, 10727, 8899, 1958, 13, 18, 490, 2224, 64, 5186, 538, 18444, 2224, 64, 3625], "temperature": 0.0, "avg_logprob": -0.12145676455654941, "compression_ratio": 1.5336322869955157, "no_speech_prob": 3.966967142332578e-06}, {"id": 365, "seek": 245940, "start": 2475.88, "end": 2482.38, "text": " PyTorch in Windows. I then attempted to use the entirety of lesson 1 and every single", "tokens": [9953, 51, 284, 339, 294, 8591, 13, 286, 550, 18997, 281, 764, 264, 31557, 295, 6898, 502, 293, 633, 2167], "temperature": 0.0, "avg_logprob": -0.12145676455654941, "compression_ratio": 1.5336322869955157, "no_speech_prob": 3.966967142332578e-06}, {"id": 366, "seek": 245940, "start": 2482.38, "end": 2487.6, "text": " part worked. So I actually ran it on this very laptop.", "tokens": [644, 2732, 13, 407, 286, 767, 5872, 309, 322, 341, 588, 10732, 13], "temperature": 0.0, "avg_logprob": -0.12145676455654941, "compression_ratio": 1.5336322869955157, "no_speech_prob": 3.966967142332578e-06}, {"id": 367, "seek": 248760, "start": 2487.6, "end": 2492.36, "text": " So for those who are interested in doing deep learning on their laptop, I can definitely", "tokens": [407, 337, 729, 567, 366, 3102, 294, 884, 2452, 2539, 322, 641, 10732, 11, 286, 393, 2138], "temperature": 0.0, "avg_logprob": -0.2073958759576502, "compression_ratio": 1.4105263157894736, "no_speech_prob": 2.355234755668789e-05}, {"id": 368, "seek": 248760, "start": 2492.36, "end": 2503.36, "text": " recommend the new Surface Book. The new Surface Book 15-inch has a GTX 1060 6GB GPU in it.", "tokens": [2748, 264, 777, 36052, 9476, 13, 440, 777, 36052, 9476, 2119, 12, 12415, 575, 257, 17530, 55, 1266, 4550, 1386, 8769, 18407, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.2073958759576502, "compression_ratio": 1.4105263157894736, "no_speech_prob": 2.355234755668789e-05}, {"id": 369, "seek": 248760, "start": 2503.36, "end": 2514.7999999999997, "text": " And I was getting it, it was running about 3 times slower than my 1080 Ti, which I think", "tokens": [400, 286, 390, 1242, 309, 11, 309, 390, 2614, 466, 805, 1413, 14009, 813, 452, 24547, 20456, 11, 597, 286, 519], "temperature": 0.0, "avg_logprob": -0.2073958759576502, "compression_ratio": 1.4105263157894736, "no_speech_prob": 2.355234755668789e-05}, {"id": 370, "seek": 251480, "start": 2514.8, "end": 2521.52, "text": " means it's about the same speed as an AWS P2 instance. And as you can see, it's also", "tokens": [1355, 309, 311, 466, 264, 912, 3073, 382, 364, 17650, 430, 17, 5197, 13, 400, 382, 291, 393, 536, 11, 309, 311, 611], "temperature": 0.0, "avg_logprob": -0.1759789971744313, "compression_ratio": 1.6058091286307055, "no_speech_prob": 1.0952959200949408e-05}, {"id": 371, "seek": 251480, "start": 2521.52, "end": 2527.6000000000004, "text": " a nice convertible tablet that you can write on, and it's thin and light. I've never seen", "tokens": [257, 1481, 7620, 964, 14136, 300, 291, 393, 2464, 322, 11, 293, 309, 311, 5862, 293, 1442, 13, 286, 600, 1128, 1612], "temperature": 0.0, "avg_logprob": -0.1759789971744313, "compression_ratio": 1.6058091286307055, "no_speech_prob": 1.0952959200949408e-05}, {"id": 372, "seek": 251480, "start": 2527.6000000000004, "end": 2531.2000000000003, "text": " such a good deep learning box.", "tokens": [1270, 257, 665, 2452, 2539, 2424, 13], "temperature": 0.0, "avg_logprob": -0.1759789971744313, "compression_ratio": 1.6058091286307055, "no_speech_prob": 1.0952959200949408e-05}, {"id": 373, "seek": 251480, "start": 2531.2000000000003, "end": 2537.0, "text": " Also I successfully installed Linux on it, and all of the fastai stuff worked on the", "tokens": [2743, 286, 10727, 8899, 18734, 322, 309, 11, 293, 439, 295, 264, 2370, 1301, 1507, 2732, 322, 264], "temperature": 0.0, "avg_logprob": -0.1759789971744313, "compression_ratio": 1.6058091286307055, "no_speech_prob": 1.0952959200949408e-05}, {"id": 374, "seek": 251480, "start": 2537.0, "end": 2543.48, "text": " Linux as well. So really good option if you're interested in a laptop that can run deep learning", "tokens": [18734, 382, 731, 13, 407, 534, 665, 3614, 498, 291, 434, 3102, 294, 257, 10732, 300, 393, 1190, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.1759789971744313, "compression_ratio": 1.6058091286307055, "no_speech_prob": 1.0952959200949408e-05}, {"id": 375, "seek": 254348, "start": 2543.48, "end": 2547.76, "text": " stuff.", "tokens": [1507, 13], "temperature": 0.0, "avg_logprob": -0.20408068032100282, "compression_ratio": 1.3819444444444444, "no_speech_prob": 2.84092402580427e-05}, {"id": 376, "seek": 254348, "start": 2547.76, "end": 2553.44, "text": " So that's something to be aware of with this tm=\"-1\". So then we can go ahead and construct", "tokens": [407, 300, 311, 746, 281, 312, 3650, 295, 365, 341, 256, 76, 13114, 12, 16, 1883, 407, 550, 321, 393, 352, 2286, 293, 7690], "temperature": 0.0, "avg_logprob": -0.20408068032100282, "compression_ratio": 1.3819444444444444, "no_speech_prob": 2.84092402580427e-05}, {"id": 377, "seek": 254348, "start": 2553.44, "end": 2561.92, "text": " this and we can call fit. We're basically going to get pretty similar results to what", "tokens": [341, 293, 321, 393, 818, 3318, 13, 492, 434, 1936, 516, 281, 483, 1238, 2531, 3542, 281, 437], "temperature": 0.0, "avg_logprob": -0.20408068032100282, "compression_ratio": 1.3819444444444444, "no_speech_prob": 2.84092402580427e-05}, {"id": 378, "seek": 254348, "start": 2561.92, "end": 2565.16, "text": " we got before.", "tokens": [321, 658, 949, 13], "temperature": 0.0, "avg_logprob": -0.20408068032100282, "compression_ratio": 1.3819444444444444, "no_speech_prob": 2.84092402580427e-05}, {"id": 379, "seek": 256516, "start": 2565.16, "end": 2574.08, "text": " So then we can go a bit further with our RNN by just kind of unpacking it a bit more. And", "tokens": [407, 550, 321, 393, 352, 257, 857, 3052, 365, 527, 45702, 45, 538, 445, 733, 295, 26699, 278, 309, 257, 857, 544, 13, 400], "temperature": 0.0, "avg_logprob": -0.15056813931932636, "compression_ratio": 1.6296296296296295, "no_speech_prob": 6.04888236921397e-06}, {"id": 380, "seek": 256516, "start": 2574.08, "end": 2579.2799999999997, "text": " so this is now again exactly the same thing, gives exactly the same answers, but I have", "tokens": [370, 341, 307, 586, 797, 2293, 264, 912, 551, 11, 2709, 2293, 264, 912, 6338, 11, 457, 286, 362], "temperature": 0.0, "avg_logprob": -0.15056813931932636, "compression_ratio": 1.6296296296296295, "no_speech_prob": 6.04888236921397e-06}, {"id": 381, "seek": 256516, "start": 2579.2799999999997, "end": 2589.48, "text": " removed the call to RNN. So I've got rid of this self.rn. And so this is just something", "tokens": [7261, 264, 818, 281, 45702, 45, 13, 407, 286, 600, 658, 3973, 295, 341, 2698, 13, 81, 77, 13, 400, 370, 341, 307, 445, 746], "temperature": 0.0, "avg_logprob": -0.15056813931932636, "compression_ratio": 1.6296296296296295, "no_speech_prob": 6.04888236921397e-06}, {"id": 382, "seek": 256516, "start": 2589.48, "end": 2594.56, "text": " I won't spend time on it, but you can check it out. So instead I've now defined RNN as", "tokens": [286, 1582, 380, 3496, 565, 322, 309, 11, 457, 291, 393, 1520, 309, 484, 13, 407, 2602, 286, 600, 586, 7642, 45702, 45, 382], "temperature": 0.0, "avg_logprob": -0.15056813931932636, "compression_ratio": 1.6296296296296295, "no_speech_prob": 6.04888236921397e-06}, {"id": 383, "seek": 259456, "start": 2594.56, "end": 2600.04, "text": " RNN cell, and I've copied and pasted the code above. Don't run it, this is just for your", "tokens": [45702, 45, 2815, 11, 293, 286, 600, 25365, 293, 1791, 292, 264, 3089, 3673, 13, 1468, 380, 1190, 309, 11, 341, 307, 445, 337, 428], "temperature": 0.0, "avg_logprob": -0.11171002290686782, "compression_ratio": 1.5357142857142858, "no_speech_prob": 6.144157396192895e-06}, {"id": 384, "seek": 259456, "start": 2600.04, "end": 2606.08, "text": " reference from PyTorch. This is the definition of RNN cell in PyTorch.", "tokens": [6408, 490, 9953, 51, 284, 339, 13, 639, 307, 264, 7123, 295, 45702, 45, 2815, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.11171002290686782, "compression_ratio": 1.5357142857142858, "no_speech_prob": 6.144157396192895e-06}, {"id": 385, "seek": 259456, "start": 2606.08, "end": 2612.04, "text": " And I want you to see that you can now read PyTorch source code and understand it. Not", "tokens": [400, 286, 528, 291, 281, 536, 300, 291, 393, 586, 1401, 9953, 51, 284, 339, 4009, 3089, 293, 1223, 309, 13, 1726], "temperature": 0.0, "avg_logprob": -0.11171002290686782, "compression_ratio": 1.5357142857142858, "no_speech_prob": 6.144157396192895e-06}, {"id": 386, "seek": 259456, "start": 2612.04, "end": 2617.12, "text": " only that, you'll recognize it as being something we've done before. It's a matrix multiplication", "tokens": [787, 300, 11, 291, 603, 5521, 309, 382, 885, 746, 321, 600, 1096, 949, 13, 467, 311, 257, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.11171002290686782, "compression_ratio": 1.5357142857142858, "no_speech_prob": 6.144157396192895e-06}, {"id": 387, "seek": 261712, "start": 2617.12, "end": 2625.16, "text": " of the weights by the inputs plus biases. So F.linear simply does a matrix product followed", "tokens": [295, 264, 17443, 538, 264, 15743, 1804, 32152, 13, 407, 479, 13, 28263, 2935, 775, 257, 8141, 1674, 6263], "temperature": 0.0, "avg_logprob": -0.11893731435139974, "compression_ratio": 1.4923076923076923, "no_speech_prob": 2.2125457689980976e-05}, {"id": 388, "seek": 261712, "start": 2625.16, "end": 2627.0, "text": " by an addition.", "tokens": [538, 364, 4500, 13], "temperature": 0.0, "avg_logprob": -0.11893731435139974, "compression_ratio": 1.4923076923076923, "no_speech_prob": 2.2125457689980976e-05}, {"id": 389, "seek": 261712, "start": 2627.0, "end": 2635.52, "text": " And interestingly you'll see they do not concatenate the input bit and the hidden bit. They sum", "tokens": [400, 25873, 291, 603, 536, 436, 360, 406, 1588, 7186, 473, 264, 4846, 857, 293, 264, 7633, 857, 13, 814, 2408], "temperature": 0.0, "avg_logprob": -0.11893731435139974, "compression_ratio": 1.4923076923076923, "no_speech_prob": 2.2125457689980976e-05}, {"id": 390, "seek": 261712, "start": 2635.52, "end": 2641.12, "text": " them together, which is our first approach. As I said, you can do either, neither one's", "tokens": [552, 1214, 11, 597, 307, 527, 700, 3109, 13, 1018, 286, 848, 11, 291, 393, 360, 2139, 11, 9662, 472, 311], "temperature": 0.0, "avg_logprob": -0.11893731435139974, "compression_ratio": 1.4923076923076923, "no_speech_prob": 2.2125457689980976e-05}, {"id": 391, "seek": 264112, "start": 2641.12, "end": 2648.12, "text": " right or wrong, but it's interesting to see this is the definition here.", "tokens": [558, 420, 2085, 11, 457, 309, 311, 1880, 281, 536, 341, 307, 264, 7123, 510, 13], "temperature": 0.0, "avg_logprob": -0.278980467054579, "compression_ratio": 1.45, "no_speech_prob": 8.267825251095928e-06}, {"id": 392, "seek": 264112, "start": 2648.12, "end": 2654.08, "text": " Question. Can you give us some insight about what are they using that particular activation", "tokens": [14464, 13, 1664, 291, 976, 505, 512, 11269, 466, 437, 366, 436, 1228, 300, 1729, 24433], "temperature": 0.0, "avg_logprob": -0.278980467054579, "compression_ratio": 1.45, "no_speech_prob": 8.267825251095928e-06}, {"id": 393, "seek": 264112, "start": 2654.08, "end": 2655.08, "text": " function?", "tokens": [2445, 30], "temperature": 0.0, "avg_logprob": -0.278980467054579, "compression_ratio": 1.45, "no_speech_prob": 8.267825251095928e-06}, {"id": 394, "seek": 264112, "start": 2655.08, "end": 2663.0, "text": " I think we might have briefly covered this last week, but very happy to do it again if", "tokens": [286, 519, 321, 1062, 362, 10515, 5343, 341, 1036, 1243, 11, 457, 588, 2055, 281, 360, 309, 797, 498], "temperature": 0.0, "avg_logprob": -0.278980467054579, "compression_ratio": 1.45, "no_speech_prob": 8.267825251095928e-06}, {"id": 395, "seek": 266300, "start": 2663.0, "end": 2682.52, "text": " I did. Basically, Than looks like that. So in other words, it's a sigmoid function, double", "tokens": [286, 630, 13, 8537, 11, 18289, 1542, 411, 300, 13, 407, 294, 661, 2283, 11, 309, 311, 257, 4556, 3280, 327, 2445, 11, 3834], "temperature": 0.0, "avg_logprob": -0.23764402525765554, "compression_ratio": 1.0465116279069768, "no_speech_prob": 6.33917306913645e-06}, {"id": 396, "seek": 268252, "start": 2682.52, "end": 2695.12, "text": " the height, minus 1. It's a nice function in that it's forcing it to be no smaller than", "tokens": [264, 6681, 11, 3175, 502, 13, 467, 311, 257, 1481, 2445, 294, 300, 309, 311, 19030, 309, 281, 312, 572, 4356, 813], "temperature": 0.0, "avg_logprob": -0.13260786500695634, "compression_ratio": 1.5808383233532934, "no_speech_prob": 8.939647159422748e-06}, {"id": 397, "seek": 268252, "start": 2695.12, "end": 2699.92, "text": " minus 1, no bigger than plus 1. And since we're multiplying by this weight matrix again", "tokens": [3175, 502, 11, 572, 3801, 813, 1804, 502, 13, 400, 1670, 321, 434, 30955, 538, 341, 3364, 8141, 797], "temperature": 0.0, "avg_logprob": -0.13260786500695634, "compression_ratio": 1.5808383233532934, "no_speech_prob": 8.939647159422748e-06}, {"id": 398, "seek": 268252, "start": 2699.92, "end": 2707.52, "text": " and again and again and again, we might worry that a ReLU, because it's unbounded, might", "tokens": [293, 797, 293, 797, 293, 797, 11, 321, 1062, 3292, 300, 257, 1300, 43, 52, 11, 570, 309, 311, 517, 18767, 292, 11, 1062], "temperature": 0.0, "avg_logprob": -0.13260786500695634, "compression_ratio": 1.5808383233532934, "no_speech_prob": 8.939647159422748e-06}, {"id": 399, "seek": 270752, "start": 2707.52, "end": 2714.64, "text": " have more of a gradient explosion problem. That's basically the theory. Having said that,", "tokens": [362, 544, 295, 257, 16235, 15673, 1154, 13, 663, 311, 1936, 264, 5261, 13, 10222, 848, 300, 11], "temperature": 0.0, "avg_logprob": -0.15637702527253525, "compression_ratio": 1.4912280701754386, "no_speech_prob": 8.398036698054057e-06}, {"id": 400, "seek": 270752, "start": 2714.64, "end": 2726.08, "text": " you can actually ask PyTorch for an RNN cell which uses a different non-linearity. So you", "tokens": [291, 393, 767, 1029, 9953, 51, 284, 339, 337, 364, 45702, 45, 2815, 597, 4960, 257, 819, 2107, 12, 1889, 17409, 13, 407, 291], "temperature": 0.0, "avg_logprob": -0.15637702527253525, "compression_ratio": 1.4912280701754386, "no_speech_prob": 8.398036698054057e-06}, {"id": 401, "seek": 270752, "start": 2726.08, "end": 2731.6, "text": " can see by default it uses Than, but you can ask for a ReLU as well. But most people seem", "tokens": [393, 536, 538, 7576, 309, 4960, 18289, 11, 457, 291, 393, 1029, 337, 257, 1300, 43, 52, 382, 731, 13, 583, 881, 561, 1643], "temperature": 0.0, "avg_logprob": -0.15637702527253525, "compression_ratio": 1.4912280701754386, "no_speech_prob": 8.398036698054057e-06}, {"id": 402, "seek": 270752, "start": 2731.6, "end": 2737.32, "text": " to pretty much everybody still seems to use Than as far as I can tell.", "tokens": [281, 1238, 709, 2201, 920, 2544, 281, 764, 18289, 382, 1400, 382, 286, 393, 980, 13], "temperature": 0.0, "avg_logprob": -0.15637702527253525, "compression_ratio": 1.4912280701754386, "no_speech_prob": 8.398036698054057e-06}, {"id": 403, "seek": 273732, "start": 2737.32, "end": 2741.1600000000003, "text": " So you can basically see here, this is all the same except now I've got an RNN cell,", "tokens": [407, 291, 393, 1936, 536, 510, 11, 341, 307, 439, 264, 912, 3993, 586, 286, 600, 658, 364, 45702, 45, 2815, 11], "temperature": 0.0, "avg_logprob": -0.12606544864987865, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.9833207261399366e-05}, {"id": 404, "seek": 273732, "start": 2741.1600000000003, "end": 2747.96, "text": " which means now I need to put my for loop back. And you can see every time I call my", "tokens": [597, 1355, 586, 286, 643, 281, 829, 452, 337, 6367, 646, 13, 400, 291, 393, 536, 633, 565, 286, 818, 452], "temperature": 0.0, "avg_logprob": -0.12606544864987865, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.9833207261399366e-05}, {"id": 405, "seek": 273732, "start": 2747.96, "end": 2756.04, "text": " little linear function, I just append the result onto my list. And at the end, the result", "tokens": [707, 8213, 2445, 11, 286, 445, 34116, 264, 1874, 3911, 452, 1329, 13, 400, 412, 264, 917, 11, 264, 1874], "temperature": 0.0, "avg_logprob": -0.12606544864987865, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.9833207261399366e-05}, {"id": 406, "seek": 273732, "start": 2756.04, "end": 2759.36, "text": " is that all stacked up together.", "tokens": [307, 300, 439, 28867, 493, 1214, 13], "temperature": 0.0, "avg_logprob": -0.12606544864987865, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.9833207261399366e-05}, {"id": 407, "seek": 273732, "start": 2759.36, "end": 2766.36, "text": " So I'm just trying to show you how nothing inside PyTorch is mysterious. You should find", "tokens": [407, 286, 478, 445, 1382, 281, 855, 291, 577, 1825, 1854, 9953, 51, 284, 339, 307, 13831, 13, 509, 820, 915], "temperature": 0.0, "avg_logprob": -0.12606544864987865, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.9833207261399366e-05}, {"id": 408, "seek": 276636, "start": 2766.36, "end": 2774.32, "text": " you get basically the same answer from this as the previous one. In practice, you would", "tokens": [291, 483, 1936, 264, 912, 1867, 490, 341, 382, 264, 3894, 472, 13, 682, 3124, 11, 291, 576], "temperature": 0.0, "avg_logprob": -0.17554286906593725, "compression_ratio": 1.6062176165803108, "no_speech_prob": 4.198545138933696e-05}, {"id": 409, "seek": 276636, "start": 2774.32, "end": 2778.56, "text": " never write it like this, but what you may well find in practice is that somebody will", "tokens": [1128, 2464, 309, 411, 341, 11, 457, 437, 291, 815, 731, 915, 294, 3124, 307, 300, 2618, 486], "temperature": 0.0, "avg_logprob": -0.17554286906593725, "compression_ratio": 1.6062176165803108, "no_speech_prob": 4.198545138933696e-05}, {"id": 410, "seek": 276636, "start": 2778.56, "end": 2784.92, "text": " come up with a new kind of RNN cell or a different way of keeping track of things over time or", "tokens": [808, 493, 365, 257, 777, 733, 295, 45702, 45, 2815, 420, 257, 819, 636, 295, 5145, 2837, 295, 721, 670, 565, 420], "temperature": 0.0, "avg_logprob": -0.17554286906593725, "compression_ratio": 1.6062176165803108, "no_speech_prob": 4.198545138933696e-05}, {"id": 411, "seek": 276636, "start": 2784.92, "end": 2787.1200000000003, "text": " a different way of doing regularization.", "tokens": [257, 819, 636, 295, 884, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.17554286906593725, "compression_ratio": 1.6062176165803108, "no_speech_prob": 4.198545138933696e-05}, {"id": 412, "seek": 278712, "start": 2787.12, "end": 2796.7999999999997, "text": " And so inside FastAI's code, you will find that we do this exactly this basically. We", "tokens": [400, 370, 1854, 15968, 48698, 311, 3089, 11, 291, 486, 915, 300, 321, 360, 341, 2293, 341, 1936, 13, 492], "temperature": 0.0, "avg_logprob": -0.15740509236112554, "compression_ratio": 1.548936170212766, "no_speech_prob": 4.93697598358267e-06}, {"id": 413, "seek": 278712, "start": 2796.7999999999997, "end": 2805.44, "text": " have this by hand because we use some regularization approaches that aren't supported by PyTorch.", "tokens": [362, 341, 538, 1011, 570, 321, 764, 512, 3890, 2144, 11587, 300, 3212, 380, 8104, 538, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.15740509236112554, "compression_ratio": 1.548936170212766, "no_speech_prob": 4.93697598358267e-06}, {"id": 414, "seek": 278712, "start": 2805.44, "end": 2809.3599999999997, "text": " So then another thing I'm not going to spend much time on, but I'll mention briefly, is", "tokens": [407, 550, 1071, 551, 286, 478, 406, 516, 281, 3496, 709, 565, 322, 11, 457, 286, 603, 2152, 10515, 11, 307], "temperature": 0.0, "avg_logprob": -0.15740509236112554, "compression_ratio": 1.548936170212766, "no_speech_prob": 4.93697598358267e-06}, {"id": 415, "seek": 278712, "start": 2809.3599999999997, "end": 2816.24, "text": " that nobody really uses this RNN cell in practice. And the reason we don't use that RNN cell", "tokens": [300, 5079, 534, 4960, 341, 45702, 45, 2815, 294, 3124, 13, 400, 264, 1778, 321, 500, 380, 764, 300, 45702, 45, 2815], "temperature": 0.0, "avg_logprob": -0.15740509236112554, "compression_ratio": 1.548936170212766, "no_speech_prob": 4.93697598358267e-06}, {"id": 416, "seek": 281624, "start": 2816.24, "end": 2824.2799999999997, "text": " in practice is even though the THAN is here, you do tend to find gradient explosions are", "tokens": [294, 3124, 307, 754, 1673, 264, 3578, 1770, 307, 510, 11, 291, 360, 3928, 281, 915, 16235, 36872, 366], "temperature": 0.0, "avg_logprob": -0.12082805162594643, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.9369732550985646e-06}, {"id": 417, "seek": 281624, "start": 2824.2799999999997, "end": 2829.6, "text": " still a problem. And so we have to use pretty low learning rates to get these to train and", "tokens": [920, 257, 1154, 13, 400, 370, 321, 362, 281, 764, 1238, 2295, 2539, 6846, 281, 483, 613, 281, 3847, 293], "temperature": 0.0, "avg_logprob": -0.12082805162594643, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.9369732550985646e-06}, {"id": 418, "seek": 281624, "start": 2829.6, "end": 2835.72, "text": " pretty small values for BPTT to get them to train.", "tokens": [1238, 1359, 4190, 337, 40533, 28178, 281, 483, 552, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.12082805162594643, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.9369732550985646e-06}, {"id": 419, "seek": 281624, "start": 2835.72, "end": 2841.7, "text": " So what we do instead is we replace the RNN cell with something like this. This is called", "tokens": [407, 437, 321, 360, 2602, 307, 321, 7406, 264, 45702, 45, 2815, 365, 746, 411, 341, 13, 639, 307, 1219], "temperature": 0.0, "avg_logprob": -0.12082805162594643, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.9369732550985646e-06}, {"id": 420, "seek": 284170, "start": 2841.7, "end": 2858.48, "text": " a GRU cell. And a GRU cell, there's a picture of it, and there's the equations for it. So", "tokens": [257, 10903, 52, 2815, 13, 400, 257, 10903, 52, 2815, 11, 456, 311, 257, 3036, 295, 309, 11, 293, 456, 311, 264, 11787, 337, 309, 13, 407], "temperature": 0.0, "avg_logprob": -0.1808837652206421, "compression_ratio": 1.2877697841726619, "no_speech_prob": 1.9525792595231906e-05}, {"id": 421, "seek": 284170, "start": 2858.48, "end": 2864.24, "text": " basically, I'll show you both quickly, but we'll talk about it much more in part 2. We've", "tokens": [1936, 11, 286, 603, 855, 291, 1293, 2661, 11, 457, 321, 603, 751, 466, 309, 709, 544, 294, 644, 568, 13, 492, 600], "temperature": 0.0, "avg_logprob": -0.1808837652206421, "compression_ratio": 1.2877697841726619, "no_speech_prob": 1.9525792595231906e-05}, {"id": 422, "seek": 286424, "start": 2864.24, "end": 2876.5, "text": " got our input, and our input normally goes straight in, gets multiplied by a weight matrix", "tokens": [658, 527, 4846, 11, 293, 527, 4846, 5646, 1709, 2997, 294, 11, 2170, 17207, 538, 257, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.13459610216545337, "compression_ratio": 1.6751592356687899, "no_speech_prob": 1.6442332707811147e-05}, {"id": 423, "seek": 286424, "start": 2876.5, "end": 2884.7599999999998, "text": " to create our new activations. That's not what happens, and then of course we add it", "tokens": [281, 1884, 527, 777, 2430, 763, 13, 663, 311, 406, 437, 2314, 11, 293, 550, 295, 1164, 321, 909, 309], "temperature": 0.0, "avg_logprob": -0.13459610216545337, "compression_ratio": 1.6751592356687899, "no_speech_prob": 1.6442332707811147e-05}, {"id": 424, "seek": 286424, "start": 2884.7599999999998, "end": 2890.52, "text": " to the existing activations. That's not what happens here. In this case, our input goes", "tokens": [281, 264, 6741, 2430, 763, 13, 663, 311, 406, 437, 2314, 510, 13, 682, 341, 1389, 11, 527, 4846, 1709], "temperature": 0.0, "avg_logprob": -0.13459610216545337, "compression_ratio": 1.6751592356687899, "no_speech_prob": 1.6442332707811147e-05}, {"id": 425, "seek": 289052, "start": 2890.52, "end": 2898.72, "text": " into this H tilde temporary thing. And it doesn't just get added to our previous activations,", "tokens": [666, 341, 389, 45046, 13413, 551, 13, 400, 309, 1177, 380, 445, 483, 3869, 281, 527, 3894, 2430, 763, 11], "temperature": 0.0, "avg_logprob": -0.1652626231096793, "compression_ratio": 1.569767441860465, "no_speech_prob": 3.785313083426445e-06}, {"id": 426, "seek": 289052, "start": 2898.72, "end": 2906.3, "text": " but our previous activations get multiplied by this value R. And R stands for reset. It's", "tokens": [457, 527, 3894, 2430, 763, 483, 17207, 538, 341, 2158, 497, 13, 400, 497, 7382, 337, 14322, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.1652626231096793, "compression_ratio": 1.569767441860465, "no_speech_prob": 3.785313083426445e-06}, {"id": 427, "seek": 289052, "start": 2906.3, "end": 2913.96, "text": " a reset gate. And how do we calculate this value? It goes between 0 and 1 in our reset", "tokens": [257, 14322, 8539, 13, 400, 577, 360, 321, 8873, 341, 2158, 30, 467, 1709, 1296, 1958, 293, 502, 294, 527, 14322], "temperature": 0.0, "avg_logprob": -0.1652626231096793, "compression_ratio": 1.569767441860465, "no_speech_prob": 3.785313083426445e-06}, {"id": 428, "seek": 291396, "start": 2913.96, "end": 2922.28, "text": " gate. Well the answer is, it's simply equal to a matrix product between some weight matrix", "tokens": [8539, 13, 1042, 264, 1867, 307, 11, 309, 311, 2935, 2681, 281, 257, 8141, 1674, 1296, 512, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.13283831530278273, "compression_ratio": 1.680161943319838, "no_speech_prob": 1.8448203036314226e-06}, {"id": 429, "seek": 291396, "start": 2922.28, "end": 2928.4, "text": " and the concatenation of our previous hidden state and our new input. In other words, this", "tokens": [293, 264, 1588, 7186, 399, 295, 527, 3894, 7633, 1785, 293, 527, 777, 4846, 13, 682, 661, 2283, 11, 341], "temperature": 0.0, "avg_logprob": -0.13283831530278273, "compression_ratio": 1.680161943319838, "no_speech_prob": 1.8448203036314226e-06}, {"id": 430, "seek": 291396, "start": 2928.4, "end": 2934.6, "text": " is a little one hidden layer neural net. And in particular, it's a one hidden layer neural", "tokens": [307, 257, 707, 472, 7633, 4583, 18161, 2533, 13, 400, 294, 1729, 11, 309, 311, 257, 472, 7633, 4583, 18161], "temperature": 0.0, "avg_logprob": -0.13283831530278273, "compression_ratio": 1.680161943319838, "no_speech_prob": 1.8448203036314226e-06}, {"id": 431, "seek": 291396, "start": 2934.6, "end": 2937.88, "text": " net because we then put it through the sigmoid function.", "tokens": [2533, 570, 321, 550, 829, 309, 807, 264, 4556, 3280, 327, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13283831530278273, "compression_ratio": 1.680161943319838, "no_speech_prob": 1.8448203036314226e-06}, {"id": 432, "seek": 291396, "start": 2937.88, "end": 2943.76, "text": " One of the things I hate about mathematical notation is symbols are overloaded a lot.", "tokens": [1485, 295, 264, 721, 286, 4700, 466, 18894, 24657, 307, 16944, 366, 28777, 292, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.13283831530278273, "compression_ratio": 1.680161943319838, "no_speech_prob": 1.8448203036314226e-06}, {"id": 433, "seek": 294376, "start": 2943.76, "end": 2948.2000000000003, "text": " Sometimes when you see sigma, it means standard deviation. When you see it next to a parenthesis", "tokens": [4803, 562, 291, 536, 12771, 11, 309, 1355, 3832, 25163, 13, 1133, 291, 536, 309, 958, 281, 257, 23350, 9374], "temperature": 0.0, "avg_logprob": -0.145526475376553, "compression_ratio": 1.6467065868263473, "no_speech_prob": 5.594314643531106e-06}, {"id": 434, "seek": 294376, "start": 2948.2000000000003, "end": 2966.1800000000003, "text": " like this, it means the sigmoid function. So in other words, that which looks like that.", "tokens": [411, 341, 11, 309, 1355, 264, 4556, 3280, 327, 2445, 13, 407, 294, 661, 2283, 11, 300, 597, 1542, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.145526475376553, "compression_ratio": 1.6467065868263473, "no_speech_prob": 5.594314643531106e-06}, {"id": 435, "seek": 294376, "start": 2966.1800000000003, "end": 2969.84, "text": " So this is like a little mini-neural net with no hidden layers. So to think of it another", "tokens": [407, 341, 307, 411, 257, 707, 8382, 12, 716, 1807, 2533, 365, 572, 7633, 7914, 13, 407, 281, 519, 295, 309, 1071], "temperature": 0.0, "avg_logprob": -0.145526475376553, "compression_ratio": 1.6467065868263473, "no_speech_prob": 5.594314643531106e-06}, {"id": 436, "seek": 296984, "start": 2969.84, "end": 2975.2000000000003, "text": " way, it's like a little logistic regression. I mentioned this briefly because it's going", "tokens": [636, 11, 309, 311, 411, 257, 707, 3565, 3142, 24590, 13, 286, 2835, 341, 10515, 570, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.1247427615713566, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.5008454233320663e-06}, {"id": 437, "seek": 296984, "start": 2975.2000000000003, "end": 2980.6400000000003, "text": " to come up a lot in part 2, so it's a good thing to start learning about. It's this idea", "tokens": [281, 808, 493, 257, 688, 294, 644, 568, 11, 370, 309, 311, 257, 665, 551, 281, 722, 2539, 466, 13, 467, 311, 341, 1558], "temperature": 0.0, "avg_logprob": -0.1247427615713566, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.5008454233320663e-06}, {"id": 438, "seek": 296984, "start": 2980.6400000000003, "end": 2987.56, "text": " that in the very learning itself, you can have little mini-neural nets inside your neural", "tokens": [300, 294, 264, 588, 2539, 2564, 11, 291, 393, 362, 707, 8382, 12, 716, 1807, 36170, 1854, 428, 18161], "temperature": 0.0, "avg_logprob": -0.1247427615713566, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.5008454233320663e-06}, {"id": 439, "seek": 296984, "start": 2987.56, "end": 2994.6400000000003, "text": " nets. And so this little mini-neural net is going to be used to decide how much of my", "tokens": [36170, 13, 400, 370, 341, 707, 8382, 12, 716, 1807, 2533, 307, 516, 281, 312, 1143, 281, 4536, 577, 709, 295, 452], "temperature": 0.0, "avg_logprob": -0.1247427615713566, "compression_ratio": 1.7135922330097086, "no_speech_prob": 3.5008454233320663e-06}, {"id": 440, "seek": 299464, "start": 2994.64, "end": 3001.3599999999997, "text": " hidden state am I going to remember. And so it might learn that in this particular situation,", "tokens": [7633, 1785, 669, 286, 516, 281, 1604, 13, 400, 370, 309, 1062, 1466, 300, 294, 341, 1729, 2590, 11], "temperature": 0.0, "avg_logprob": -0.13355698152021928, "compression_ratio": 1.7049808429118773, "no_speech_prob": 6.7480341385817155e-06}, {"id": 441, "seek": 299464, "start": 3001.3599999999997, "end": 3006.3599999999997, "text": " forget everything you know. For example, oh there's a full stop. Hey, when you see a full", "tokens": [2870, 1203, 291, 458, 13, 1171, 1365, 11, 1954, 456, 311, 257, 1577, 1590, 13, 1911, 11, 562, 291, 536, 257, 1577], "temperature": 0.0, "avg_logprob": -0.13355698152021928, "compression_ratio": 1.7049808429118773, "no_speech_prob": 6.7480341385817155e-06}, {"id": 442, "seek": 299464, "start": 3006.3599999999997, "end": 3010.7999999999997, "text": " stop, you should throw away nearly all of your hidden state. That is probably something", "tokens": [1590, 11, 291, 820, 3507, 1314, 6217, 439, 295, 428, 7633, 1785, 13, 663, 307, 1391, 746], "temperature": 0.0, "avg_logprob": -0.13355698152021928, "compression_ratio": 1.7049808429118773, "no_speech_prob": 6.7480341385817155e-06}, {"id": 443, "seek": 299464, "start": 3010.7999999999997, "end": 3016.24, "text": " it would learn. And that's very easy for it to learn using this little mini-neural net.", "tokens": [309, 576, 1466, 13, 400, 300, 311, 588, 1858, 337, 309, 281, 1466, 1228, 341, 707, 8382, 12, 716, 1807, 2533, 13], "temperature": 0.0, "avg_logprob": -0.13355698152021928, "compression_ratio": 1.7049808429118773, "no_speech_prob": 6.7480341385817155e-06}, {"id": 444, "seek": 299464, "start": 3016.24, "end": 3022.2799999999997, "text": " And so that goes through to create my new hidden state along with the input. And then", "tokens": [400, 370, 300, 1709, 807, 281, 1884, 452, 777, 7633, 1785, 2051, 365, 264, 4846, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.13355698152021928, "compression_ratio": 1.7049808429118773, "no_speech_prob": 6.7480341385817155e-06}, {"id": 445, "seek": 302228, "start": 3022.28, "end": 3027.4, "text": " there's a second thing that happens, which is there's this gate here called Z. And what", "tokens": [456, 311, 257, 1150, 551, 300, 2314, 11, 597, 307, 456, 311, 341, 8539, 510, 1219, 1176, 13, 400, 437], "temperature": 0.0, "avg_logprob": -0.13067395708202262, "compression_ratio": 1.8273092369477912, "no_speech_prob": 7.183235084085027e-06}, {"id": 446, "seek": 302228, "start": 3027.4, "end": 3034.52, "text": " Z says is, alright, you've got some amount of your previous hidden state plus your new", "tokens": [1176, 1619, 307, 11, 5845, 11, 291, 600, 658, 512, 2372, 295, 428, 3894, 7633, 1785, 1804, 428, 777], "temperature": 0.0, "avg_logprob": -0.13067395708202262, "compression_ratio": 1.8273092369477912, "no_speech_prob": 7.183235084085027e-06}, {"id": 447, "seek": 302228, "start": 3034.52, "end": 3040.6400000000003, "text": " input, and it's going to go through to create your new state. And I'm going to let you decide", "tokens": [4846, 11, 293, 309, 311, 516, 281, 352, 807, 281, 1884, 428, 777, 1785, 13, 400, 286, 478, 516, 281, 718, 291, 4536], "temperature": 0.0, "avg_logprob": -0.13067395708202262, "compression_ratio": 1.8273092369477912, "no_speech_prob": 7.183235084085027e-06}, {"id": 448, "seek": 302228, "start": 3040.6400000000003, "end": 3047.6800000000003, "text": " to what degree do you use this new input version of your hidden state, and to what degree will", "tokens": [281, 437, 4314, 360, 291, 764, 341, 777, 4846, 3037, 295, 428, 7633, 1785, 11, 293, 281, 437, 4314, 486], "temperature": 0.0, "avg_logprob": -0.13067395708202262, "compression_ratio": 1.8273092369477912, "no_speech_prob": 7.183235084085027e-06}, {"id": 449, "seek": 302228, "start": 3047.6800000000003, "end": 3051.6000000000004, "text": " you just leave the hidden state the same as before. So this thing here is called the update", "tokens": [291, 445, 1856, 264, 7633, 1785, 264, 912, 382, 949, 13, 407, 341, 551, 510, 307, 1219, 264, 5623], "temperature": 0.0, "avg_logprob": -0.13067395708202262, "compression_ratio": 1.8273092369477912, "no_speech_prob": 7.183235084085027e-06}, {"id": 450, "seek": 305160, "start": 3051.6, "end": 3057.7599999999998, "text": " gate. And so it's got two choices it can make. The first is to throw away some hidden state", "tokens": [8539, 13, 400, 370, 309, 311, 658, 732, 7994, 309, 393, 652, 13, 440, 700, 307, 281, 3507, 1314, 512, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.09981485288970325, "compression_ratio": 1.6181102362204725, "no_speech_prob": 1.5534961903540534e-06}, {"id": 451, "seek": 305160, "start": 3057.7599999999998, "end": 3063.4, "text": " when deciding how much to incorporate that versus my new input, and how much to update", "tokens": [562, 17990, 577, 709, 281, 16091, 300, 5717, 452, 777, 4846, 11, 293, 577, 709, 281, 5623], "temperature": 0.0, "avg_logprob": -0.09981485288970325, "compression_ratio": 1.6181102362204725, "no_speech_prob": 1.5534961903540534e-06}, {"id": 452, "seek": 305160, "start": 3063.4, "end": 3067.6, "text": " my hidden state versus just leave it exactly the same.", "tokens": [452, 7633, 1785, 5717, 445, 1856, 309, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.09981485288970325, "compression_ratio": 1.6181102362204725, "no_speech_prob": 1.5534961903540534e-06}, {"id": 453, "seek": 305160, "start": 3067.6, "end": 3073.12, "text": " And the equation hopefully is going to look pretty familiar to you, which is, check this", "tokens": [400, 264, 5367, 4696, 307, 516, 281, 574, 1238, 4963, 281, 291, 11, 597, 307, 11, 1520, 341], "temperature": 0.0, "avg_logprob": -0.09981485288970325, "compression_ratio": 1.6181102362204725, "no_speech_prob": 1.5534961903540534e-06}, {"id": 454, "seek": 305160, "start": 3073.12, "end": 3080.0, "text": " out here. Remember how I said you want to start to recognize some common ways of looking", "tokens": [484, 510, 13, 5459, 577, 286, 848, 291, 528, 281, 722, 281, 5521, 512, 2689, 2098, 295, 1237], "temperature": 0.0, "avg_logprob": -0.09981485288970325, "compression_ratio": 1.6181102362204725, "no_speech_prob": 1.5534961903540534e-06}, {"id": 455, "seek": 308000, "start": 3080.0, "end": 3089.36, "text": " at things. Well here I have a 1 minus something by a thing, and a something without the 1", "tokens": [412, 721, 13, 1042, 510, 286, 362, 257, 502, 3175, 746, 538, 257, 551, 11, 293, 257, 746, 1553, 264, 502], "temperature": 0.0, "avg_logprob": -0.15367608830548715, "compression_ratio": 1.5529411764705883, "no_speech_prob": 1.3081757970212493e-06}, {"id": 456, "seek": 308000, "start": 3089.36, "end": 3096.32, "text": " minus by a thing, which remember is a linear interpolation. So in other words, the value", "tokens": [3175, 538, 257, 551, 11, 597, 1604, 307, 257, 8213, 44902, 399, 13, 407, 294, 661, 2283, 11, 264, 2158], "temperature": 0.0, "avg_logprob": -0.15367608830548715, "compression_ratio": 1.5529411764705883, "no_speech_prob": 1.3081757970212493e-06}, {"id": 457, "seek": 308000, "start": 3096.32, "end": 3104.44, "text": " of Z is going to decide to what degree do I have, keep the previous hidden state, and", "tokens": [295, 1176, 307, 516, 281, 4536, 281, 437, 4314, 360, 286, 362, 11, 1066, 264, 3894, 7633, 1785, 11, 293], "temperature": 0.0, "avg_logprob": -0.15367608830548715, "compression_ratio": 1.5529411764705883, "no_speech_prob": 1.3081757970212493e-06}, {"id": 458, "seek": 310444, "start": 3104.44, "end": 3112.52, "text": " to what degree do I use the new hidden state. So that's why they draw it here as this kind", "tokens": [281, 437, 4314, 360, 286, 764, 264, 777, 7633, 1785, 13, 407, 300, 311, 983, 436, 2642, 309, 510, 382, 341, 733], "temperature": 0.0, "avg_logprob": -0.11545207437160795, "compression_ratio": 1.9315068493150684, "no_speech_prob": 1.5534952808593516e-06}, {"id": 459, "seek": 310444, "start": 3112.52, "end": 3117.48, "text": " of like, it's not actually a switch, but you can put it in any position. You can be like", "tokens": [295, 411, 11, 309, 311, 406, 767, 257, 3679, 11, 457, 291, 393, 829, 309, 294, 604, 2535, 13, 509, 393, 312, 411], "temperature": 0.0, "avg_logprob": -0.11545207437160795, "compression_ratio": 1.9315068493150684, "no_speech_prob": 1.5534952808593516e-06}, {"id": 460, "seek": 310444, "start": 3117.48, "end": 3124.84, "text": " oh it's here, or it's here, or it's here to decide how much to update.", "tokens": [1954, 309, 311, 510, 11, 420, 309, 311, 510, 11, 420, 309, 311, 510, 281, 4536, 577, 709, 281, 5623, 13], "temperature": 0.0, "avg_logprob": -0.11545207437160795, "compression_ratio": 1.9315068493150684, "no_speech_prob": 1.5534952808593516e-06}, {"id": 461, "seek": 310444, "start": 3124.84, "end": 3128.16, "text": " So they're basically the equations. It's a little mini neural net with its own weight", "tokens": [407, 436, 434, 1936, 264, 11787, 13, 467, 311, 257, 707, 8382, 18161, 2533, 365, 1080, 1065, 3364], "temperature": 0.0, "avg_logprob": -0.11545207437160795, "compression_ratio": 1.9315068493150684, "no_speech_prob": 1.5534952808593516e-06}, {"id": 462, "seek": 310444, "start": 3128.16, "end": 3132.2400000000002, "text": " matrix to decide how much to update, little mini neural net with its own weight matrix", "tokens": [8141, 281, 4536, 577, 709, 281, 5623, 11, 707, 8382, 18161, 2533, 365, 1080, 1065, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.11545207437160795, "compression_ratio": 1.9315068493150684, "no_speech_prob": 1.5534952808593516e-06}, {"id": 463, "seek": 313224, "start": 3132.24, "end": 3137.12, "text": " to decide how much to reset. And then that's used to do an interpolation between the two", "tokens": [281, 4536, 577, 709, 281, 14322, 13, 400, 550, 300, 311, 1143, 281, 360, 364, 44902, 399, 1296, 264, 732], "temperature": 0.0, "avg_logprob": -0.14646379470825197, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.4465710996300913e-06}, {"id": 464, "seek": 313224, "start": 3137.12, "end": 3138.4799999999996, "text": " hidden states.", "tokens": [7633, 4368, 13], "temperature": 0.0, "avg_logprob": -0.14646379470825197, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.4465710996300913e-06}, {"id": 465, "seek": 313224, "start": 3138.4799999999996, "end": 3146.24, "text": " So that's called a GRU, gated recurrent network. There's the definition from the PyTorch source", "tokens": [407, 300, 311, 1219, 257, 10903, 52, 11, 290, 770, 18680, 1753, 3209, 13, 821, 311, 264, 7123, 490, 264, 9953, 51, 284, 339, 4009], "temperature": 0.0, "avg_logprob": -0.14646379470825197, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.4465710996300913e-06}, {"id": 466, "seek": 313224, "start": 3146.24, "end": 3152.2799999999997, "text": " code. They have some slight optimizations here that if you're interested in, we can talk", "tokens": [3089, 13, 814, 362, 512, 4036, 5028, 14455, 510, 300, 498, 291, 434, 3102, 294, 11, 321, 393, 751], "temperature": 0.0, "avg_logprob": -0.14646379470825197, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.4465710996300913e-06}, {"id": 467, "seek": 313224, "start": 3152.2799999999997, "end": 3159.08, "text": " about them on the forum, but it's exactly the same formula we just saw. And so if you", "tokens": [466, 552, 322, 264, 17542, 11, 457, 309, 311, 2293, 264, 912, 8513, 321, 445, 1866, 13, 400, 370, 498, 291], "temperature": 0.0, "avg_logprob": -0.14646379470825197, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.4465710996300913e-06}, {"id": 468, "seek": 315908, "start": 3159.08, "end": 3170.24, "text": " go nn.gru, then it uses this same code, but it replaces the RNN cell with this cell. And", "tokens": [352, 297, 77, 13, 861, 84, 11, 550, 309, 4960, 341, 912, 3089, 11, 457, 309, 46734, 264, 45702, 45, 2815, 365, 341, 2815, 13, 400], "temperature": 0.0, "avg_logprob": -0.18594230513974844, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.048900104360655e-06}, {"id": 469, "seek": 315908, "start": 3170.24, "end": 3178.52, "text": " as a result, rather than having something where we needed, where we were getting a 1.54,", "tokens": [382, 257, 1874, 11, 2831, 813, 1419, 746, 689, 321, 2978, 11, 689, 321, 645, 1242, 257, 502, 13, 19563, 11], "temperature": 0.0, "avg_logprob": -0.18594230513974844, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.048900104360655e-06}, {"id": 470, "seek": 315908, "start": 3178.52, "end": 3183.96, "text": " we're now getting down to 1.40, and we can keep training even more, get right down to", "tokens": [321, 434, 586, 1242, 760, 281, 502, 13, 5254, 11, 293, 321, 393, 1066, 3097, 754, 544, 11, 483, 558, 760, 281], "temperature": 0.0, "avg_logprob": -0.18594230513974844, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.048900104360655e-06}, {"id": 471, "seek": 315908, "start": 3183.96, "end": 3185.7, "text": " 1.36.", "tokens": [502, 13, 11309, 13], "temperature": 0.0, "avg_logprob": -0.18594230513974844, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.048900104360655e-06}, {"id": 472, "seek": 318570, "start": 3185.7, "end": 3192.48, "text": " So in practice, a GRU, or very nearly equivalently, we'll see in a moment, an LSTM, in practice", "tokens": [407, 294, 3124, 11, 257, 10903, 52, 11, 420, 588, 6217, 9052, 2276, 11, 321, 603, 536, 294, 257, 1623, 11, 364, 441, 6840, 44, 11, 294, 3124], "temperature": 0.0, "avg_logprob": -0.24077657063802083, "compression_ratio": 1.3882978723404256, "no_speech_prob": 2.8573031158884987e-06}, {"id": 473, "seek": 318570, "start": 3192.48, "end": 3196.8399999999997, "text": " what pretty much everybody always uses.", "tokens": [437, 1238, 709, 2201, 1009, 4960, 13], "temperature": 0.0, "avg_logprob": -0.24077657063802083, "compression_ratio": 1.3882978723404256, "no_speech_prob": 2.8573031158884987e-06}, {"id": 474, "seek": 318570, "start": 3196.8399999999997, "end": 3205.66, "text": " So the RT and HT are ultimately scalars after they go through the sigmoid, but they're applied", "tokens": [407, 264, 21797, 293, 11751, 366, 6284, 15664, 685, 934, 436, 352, 807, 264, 4556, 3280, 327, 11, 457, 436, 434, 6456], "temperature": 0.0, "avg_logprob": -0.24077657063802083, "compression_ratio": 1.3882978723404256, "no_speech_prob": 2.8573031158884987e-06}, {"id": 475, "seek": 318570, "start": 3205.66, "end": 3207.96, "text": " element-wise, is that correct?", "tokens": [4478, 12, 3711, 11, 307, 300, 3006, 30], "temperature": 0.0, "avg_logprob": -0.24077657063802083, "compression_ratio": 1.3882978723404256, "no_speech_prob": 2.8573031158884987e-06}, {"id": 476, "seek": 320796, "start": 3207.96, "end": 3221.32, "text": " Yes, although of course one for each mini-batch.", "tokens": [1079, 11, 4878, 295, 1164, 472, 337, 1184, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.22098512498159256, "compression_ratio": 1.3352601156069364, "no_speech_prob": 2.2603101115237223e-06}, {"id": 477, "seek": 320796, "start": 3221.32, "end": 3230.88, "text": " And on the excellent Chris Ola's blog, there's an Understanding LSTM Networks post, which", "tokens": [400, 322, 264, 7103, 6688, 422, 875, 311, 6968, 11, 456, 311, 364, 36858, 441, 6840, 44, 12640, 82, 2183, 11, 597], "temperature": 0.0, "avg_logprob": -0.22098512498159256, "compression_ratio": 1.3352601156069364, "no_speech_prob": 2.2603101115237223e-06}, {"id": 478, "seek": 320796, "start": 3230.88, "end": 3235.42, "text": " you can read all about this in much more detail if you're interested. And also the other one", "tokens": [291, 393, 1401, 439, 466, 341, 294, 709, 544, 2607, 498, 291, 434, 3102, 13, 400, 611, 264, 661, 472], "temperature": 0.0, "avg_logprob": -0.22098512498159256, "compression_ratio": 1.3352601156069364, "no_speech_prob": 2.2603101115237223e-06}, {"id": 479, "seek": 323542, "start": 3235.42, "end": 3240.76, "text": " I was dealing from here is WildML, they also have a good blog post on this. If somebody", "tokens": [286, 390, 6260, 490, 510, 307, 10904, 12683, 11, 436, 611, 362, 257, 665, 6968, 2183, 322, 341, 13, 759, 2618], "temperature": 0.0, "avg_logprob": -0.11963666654100605, "compression_ratio": 1.5107296137339057, "no_speech_prob": 2.4299959477502853e-05}, {"id": 480, "seek": 323542, "start": 3240.76, "end": 3249.98, "text": " wants to be helpful, feel free to put them in the lesson wiki if you can find them.", "tokens": [2738, 281, 312, 4961, 11, 841, 1737, 281, 829, 552, 294, 264, 6898, 261, 9850, 498, 291, 393, 915, 552, 13], "temperature": 0.0, "avg_logprob": -0.11963666654100605, "compression_ratio": 1.5107296137339057, "no_speech_prob": 2.4299959477502853e-05}, {"id": 481, "seek": 323542, "start": 3249.98, "end": 3256.64, "text": " So then putting it all together, I'm now going to replace my GRU with an LSTM. I'm not going", "tokens": [407, 550, 3372, 309, 439, 1214, 11, 286, 478, 586, 516, 281, 7406, 452, 10903, 52, 365, 364, 441, 6840, 44, 13, 286, 478, 406, 516], "temperature": 0.0, "avg_logprob": -0.11963666654100605, "compression_ratio": 1.5107296137339057, "no_speech_prob": 2.4299959477502853e-05}, {"id": 482, "seek": 323542, "start": 3256.64, "end": 3261.84, "text": " to bother showing you the cell for this, it's very similar to GRU. But the LSTM has one", "tokens": [281, 8677, 4099, 291, 264, 2815, 337, 341, 11, 309, 311, 588, 2531, 281, 10903, 52, 13, 583, 264, 441, 6840, 44, 575, 472], "temperature": 0.0, "avg_logprob": -0.11963666654100605, "compression_ratio": 1.5107296137339057, "no_speech_prob": 2.4299959477502853e-05}, {"id": 483, "seek": 326184, "start": 3261.84, "end": 3267.3, "text": " more piece of state in it called the cell state, not just the hidden state. So if you", "tokens": [544, 2522, 295, 1785, 294, 309, 1219, 264, 2815, 1785, 11, 406, 445, 264, 7633, 1785, 13, 407, 498, 291], "temperature": 0.0, "avg_logprob": -0.11980832752428557, "compression_ratio": 1.6826923076923077, "no_speech_prob": 2.468247475917451e-05}, {"id": 484, "seek": 326184, "start": 3267.3, "end": 3274.1800000000003, "text": " do use an LSTM, you now inside your hidden have to return a tuple of matrices. They're", "tokens": [360, 764, 364, 441, 6840, 44, 11, 291, 586, 1854, 428, 7633, 362, 281, 2736, 257, 2604, 781, 295, 32284, 13, 814, 434], "temperature": 0.0, "avg_logprob": -0.11980832752428557, "compression_ratio": 1.6826923076923077, "no_speech_prob": 2.468247475917451e-05}, {"id": 485, "seek": 326184, "start": 3274.1800000000003, "end": 3281.1000000000004, "text": " exactly the same size as the hidden state, but you just have to return a tuple. The details", "tokens": [2293, 264, 912, 2744, 382, 264, 7633, 1785, 11, 457, 291, 445, 362, 281, 2736, 257, 2604, 781, 13, 440, 4365], "temperature": 0.0, "avg_logprob": -0.11980832752428557, "compression_ratio": 1.6826923076923077, "no_speech_prob": 2.468247475917451e-05}, {"id": 486, "seek": 326184, "start": 3281.1000000000004, "end": 3287.78, "text": " don't matter too much, but we can talk about it during the wiki if you're interested.", "tokens": [500, 380, 1871, 886, 709, 11, 457, 321, 393, 751, 466, 309, 1830, 264, 261, 9850, 498, 291, 434, 3102, 13], "temperature": 0.0, "avg_logprob": -0.11980832752428557, "compression_ratio": 1.6826923076923077, "no_speech_prob": 2.468247475917451e-05}, {"id": 487, "seek": 328778, "start": 3287.78, "end": 3292.2200000000003, "text": " When you pass in, you still pass in self.h, it still returns a new value of h, you can", "tokens": [1133, 291, 1320, 294, 11, 291, 920, 1320, 294, 2698, 13, 71, 11, 309, 920, 11247, 257, 777, 2158, 295, 276, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.13479904114730715, "compression_ratio": 1.6755725190839694, "no_speech_prob": 6.854268576717004e-06}, {"id": 488, "seek": 328778, "start": 3292.2200000000003, "end": 3297.6200000000003, "text": " repackage it in the usual way. So this code is identical to the code before. One thing", "tokens": [1085, 501, 609, 309, 294, 264, 7713, 636, 13, 407, 341, 3089, 307, 14800, 281, 264, 3089, 949, 13, 1485, 551], "temperature": 0.0, "avg_logprob": -0.13479904114730715, "compression_ratio": 1.6755725190839694, "no_speech_prob": 6.854268576717004e-06}, {"id": 489, "seek": 328778, "start": 3297.6200000000003, "end": 3305.0600000000004, "text": " I've done though is I've added dropout inside my RNN, which you can do with the PyTorch", "tokens": [286, 600, 1096, 1673, 307, 286, 600, 3869, 3270, 346, 1854, 452, 45702, 45, 11, 597, 291, 393, 360, 365, 264, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.13479904114730715, "compression_ratio": 1.6755725190839694, "no_speech_prob": 6.854268576717004e-06}, {"id": 490, "seek": 328778, "start": 3305.0600000000004, "end": 3310.2200000000003, "text": " RNN function. So that's going to do dropout after each time step. And I've doubled the", "tokens": [45702, 45, 2445, 13, 407, 300, 311, 516, 281, 360, 3270, 346, 934, 1184, 565, 1823, 13, 400, 286, 600, 24405, 264], "temperature": 0.0, "avg_logprob": -0.13479904114730715, "compression_ratio": 1.6755725190839694, "no_speech_prob": 6.854268576717004e-06}, {"id": 491, "seek": 328778, "start": 3310.2200000000003, "end": 3315.34, "text": " size of my hidden layer since I've now added.5 dropout. And so my hope was that this would", "tokens": [2744, 295, 452, 7633, 4583, 1670, 286, 600, 586, 3869, 2411, 20, 3270, 346, 13, 400, 370, 452, 1454, 390, 300, 341, 576], "temperature": 0.0, "avg_logprob": -0.13479904114730715, "compression_ratio": 1.6755725190839694, "no_speech_prob": 6.854268576717004e-06}, {"id": 492, "seek": 331534, "start": 3315.34, "end": 3324.46, "text": " make it be able to learn more but be more resilient as it does so.", "tokens": [652, 309, 312, 1075, 281, 1466, 544, 457, 312, 544, 23699, 382, 309, 775, 370, 13], "temperature": 0.0, "avg_logprob": -0.13805718055138222, "compression_ratio": 1.5060240963855422, "no_speech_prob": 1.3211906662036199e-05}, {"id": 493, "seek": 331534, "start": 3324.46, "end": 3333.38, "text": " So then I wanted to show you how to take advantage of a little bit more FastAI magic without", "tokens": [407, 550, 286, 1415, 281, 855, 291, 577, 281, 747, 5002, 295, 257, 707, 857, 544, 15968, 48698, 5585, 1553], "temperature": 0.0, "avg_logprob": -0.13805718055138222, "compression_ratio": 1.5060240963855422, "no_speech_prob": 1.3211906662036199e-05}, {"id": 494, "seek": 331534, "start": 3333.38, "end": 3341.36, "text": " using the layer class. And so I'm going to show you how to use callbacks, and specifically", "tokens": [1228, 264, 4583, 1508, 13, 400, 370, 286, 478, 516, 281, 855, 291, 577, 281, 764, 818, 17758, 11, 293, 4682], "temperature": 0.0, "avg_logprob": -0.13805718055138222, "compression_ratio": 1.5060240963855422, "no_speech_prob": 1.3211906662036199e-05}, {"id": 495, "seek": 334136, "start": 3341.36, "end": 3347.78, "text": " we're going to do sgdr without using the learner class.", "tokens": [321, 434, 516, 281, 360, 262, 70, 16753, 1553, 1228, 264, 33347, 1508, 13], "temperature": 0.0, "avg_logprob": -0.172685198161913, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.646504855074454e-06}, {"id": 496, "seek": 334136, "start": 3347.78, "end": 3353.54, "text": " So to do that, we create our model, again just a standard PyTorch model. And this time,", "tokens": [407, 281, 360, 300, 11, 321, 1884, 527, 2316, 11, 797, 445, 257, 3832, 9953, 51, 284, 339, 2316, 13, 400, 341, 565, 11], "temperature": 0.0, "avg_logprob": -0.172685198161913, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.646504855074454e-06}, {"id": 497, "seek": 334136, "start": 3353.54, "end": 3360.42, "text": " rather than going, remember the usual PyTorch approach is opt equals optim.atom and you", "tokens": [2831, 813, 516, 11, 1604, 264, 7713, 9953, 51, 284, 339, 3109, 307, 2427, 6915, 2427, 332, 13, 267, 298, 293, 291], "temperature": 0.0, "avg_logprob": -0.172685198161913, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.646504855074454e-06}, {"id": 498, "seek": 334136, "start": 3360.42, "end": 3364.7400000000002, "text": " pass in the parameters and the learning rate. I'm not going to do that. I'm going to use", "tokens": [1320, 294, 264, 9834, 293, 264, 2539, 3314, 13, 286, 478, 406, 516, 281, 360, 300, 13, 286, 478, 516, 281, 764], "temperature": 0.0, "avg_logprob": -0.172685198161913, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.646504855074454e-06}, {"id": 499, "seek": 336474, "start": 3364.74, "end": 3376.4199999999996, "text": " the FastAI layer optimizer class, which takes my optim class constructor from PyTorch. It", "tokens": [264, 15968, 48698, 4583, 5028, 6545, 1508, 11, 597, 2516, 452, 2427, 332, 1508, 47479, 490, 9953, 51, 284, 339, 13, 467], "temperature": 0.0, "avg_logprob": -0.10231101853506906, "compression_ratio": 1.494186046511628, "no_speech_prob": 1.0677021009541932e-06}, {"id": 500, "seek": 336474, "start": 3376.4199999999996, "end": 3384.7799999999997, "text": " takes my model, it takes my learning rate, and optionally takes weight decay.", "tokens": [2516, 452, 2316, 11, 309, 2516, 452, 2539, 3314, 11, 293, 3614, 379, 2516, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.10231101853506906, "compression_ratio": 1.494186046511628, "no_speech_prob": 1.0677021009541932e-06}, {"id": 501, "seek": 336474, "start": 3384.7799999999997, "end": 3391.62, "text": " And so this class is tiny, it doesn't do very much at all. The key reason it exists is to", "tokens": [400, 370, 341, 1508, 307, 5870, 11, 309, 1177, 380, 360, 588, 709, 412, 439, 13, 440, 2141, 1778, 309, 8198, 307, 281], "temperature": 0.0, "avg_logprob": -0.10231101853506906, "compression_ratio": 1.494186046511628, "no_speech_prob": 1.0677021009541932e-06}, {"id": 502, "seek": 339162, "start": 3391.62, "end": 3397.1, "text": " do differential learning rates and differential weight decay. But the reason we need to use", "tokens": [360, 15756, 2539, 6846, 293, 15756, 3364, 21039, 13, 583, 264, 1778, 321, 643, 281, 764], "temperature": 0.0, "avg_logprob": -0.128239165181699, "compression_ratio": 1.6322869955156951, "no_speech_prob": 2.48247215495212e-06}, {"id": 503, "seek": 339162, "start": 3397.1, "end": 3403.46, "text": " it is that all of the mechanics inside FastAI assumes that you have one of these. So if", "tokens": [309, 307, 300, 439, 295, 264, 12939, 1854, 15968, 48698, 37808, 300, 291, 362, 472, 295, 613, 13, 407, 498], "temperature": 0.0, "avg_logprob": -0.128239165181699, "compression_ratio": 1.6322869955156951, "no_speech_prob": 2.48247215495212e-06}, {"id": 504, "seek": 339162, "start": 3403.46, "end": 3410.38, "text": " you want to use callbacks or sgdr or whatever, encode where you're not using the learner", "tokens": [291, 528, 281, 764, 818, 17758, 420, 262, 70, 16753, 420, 2035, 11, 2058, 1429, 689, 291, 434, 406, 1228, 264, 33347], "temperature": 0.0, "avg_logprob": -0.128239165181699, "compression_ratio": 1.6322869955156951, "no_speech_prob": 2.48247215495212e-06}, {"id": 505, "seek": 339162, "start": 3410.38, "end": 3418.02, "text": " class, then you need to use, rather than saying opt equals optim.atom and here's my parameters,", "tokens": [1508, 11, 550, 291, 643, 281, 764, 11, 2831, 813, 1566, 2427, 6915, 2427, 332, 13, 267, 298, 293, 510, 311, 452, 9834, 11], "temperature": 0.0, "avg_logprob": -0.128239165181699, "compression_ratio": 1.6322869955156951, "no_speech_prob": 2.48247215495212e-06}, {"id": 506, "seek": 341802, "start": 3418.02, "end": 3423.5, "text": " you instead say layer optimizer.", "tokens": [291, 2602, 584, 4583, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.09821004480928988, "compression_ratio": 1.639344262295082, "no_speech_prob": 2.9023060506006004e-06}, {"id": 507, "seek": 341802, "start": 3423.5, "end": 3429.9, "text": " So that gives us a layer optimizer object. And if you're interested, basically behind", "tokens": [407, 300, 2709, 505, 257, 4583, 5028, 6545, 2657, 13, 400, 498, 291, 434, 3102, 11, 1936, 2261], "temperature": 0.0, "avg_logprob": -0.09821004480928988, "compression_ratio": 1.639344262295082, "no_speech_prob": 2.9023060506006004e-06}, {"id": 508, "seek": 341802, "start": 3429.9, "end": 3440.22, "text": " the scenes, you can now grab a.opt property which actually gives you the optimizer. You", "tokens": [264, 8026, 11, 291, 393, 586, 4444, 257, 2411, 5747, 4707, 597, 767, 2709, 291, 264, 5028, 6545, 13, 509], "temperature": 0.0, "avg_logprob": -0.09821004480928988, "compression_ratio": 1.639344262295082, "no_speech_prob": 2.9023060506006004e-06}, {"id": 509, "seek": 341802, "start": 3440.22, "end": 3444.28, "text": " don't have to worry about that yourself, but that's basically what happens behind the scenes.", "tokens": [500, 380, 362, 281, 3292, 466, 300, 1803, 11, 457, 300, 311, 1936, 437, 2314, 2261, 264, 8026, 13], "temperature": 0.0, "avg_logprob": -0.09821004480928988, "compression_ratio": 1.639344262295082, "no_speech_prob": 2.9023060506006004e-06}, {"id": 510, "seek": 344428, "start": 3444.28, "end": 3451.78, "text": " The key thing we can now do is that we can now, when we call fit, we can pass in that", "tokens": [440, 2141, 551, 321, 393, 586, 360, 307, 300, 321, 393, 586, 11, 562, 321, 818, 3318, 11, 321, 393, 1320, 294, 300], "temperature": 0.0, "avg_logprob": -0.10748062133789063, "compression_ratio": 1.947089947089947, "no_speech_prob": 4.495174380281242e-06}, {"id": 511, "seek": 344428, "start": 3451.78, "end": 3459.7200000000003, "text": " optimizer and we can also pass in some callbacks. And specifically we're going to use the cosine", "tokens": [5028, 6545, 293, 321, 393, 611, 1320, 294, 512, 818, 17758, 13, 400, 4682, 321, 434, 516, 281, 764, 264, 23565], "temperature": 0.0, "avg_logprob": -0.10748062133789063, "compression_ratio": 1.947089947089947, "no_speech_prob": 4.495174380281242e-06}, {"id": 512, "seek": 344428, "start": 3459.7200000000003, "end": 3467.6000000000004, "text": " annealing callback. And so the cosine annealing callback requires a layer optimizer object.", "tokens": [22256, 4270, 818, 3207, 13, 400, 370, 264, 23565, 22256, 4270, 818, 3207, 7029, 257, 4583, 5028, 6545, 2657, 13], "temperature": 0.0, "avg_logprob": -0.10748062133789063, "compression_ratio": 1.947089947089947, "no_speech_prob": 4.495174380281242e-06}, {"id": 513, "seek": 344428, "start": 3467.6000000000004, "end": 3472.2000000000003, "text": " And so what this is going to do is it's going to do cosine annealing by changing the learning", "tokens": [400, 370, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 360, 23565, 22256, 4270, 538, 4473, 264, 2539], "temperature": 0.0, "avg_logprob": -0.10748062133789063, "compression_ratio": 1.947089947089947, "no_speech_prob": 4.495174380281242e-06}, {"id": 514, "seek": 347220, "start": 3472.2, "end": 3477.02, "text": " rate inside this object.", "tokens": [3314, 1854, 341, 2657, 13], "temperature": 0.0, "avg_logprob": -0.11432887077331542, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.3931391953956336e-06}, {"id": 515, "seek": 347220, "start": 3477.02, "end": 3480.98, "text": " So the details aren't terribly important, we can talk about them on the forum. It's", "tokens": [407, 264, 4365, 3212, 380, 22903, 1021, 11, 321, 393, 751, 466, 552, 322, 264, 17542, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.11432887077331542, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.3931391953956336e-06}, {"id": 516, "seek": 347220, "start": 3480.98, "end": 3485.3799999999997, "text": " really the concept I wanted to get across here, which is that now that we've done this,", "tokens": [534, 264, 3410, 286, 1415, 281, 483, 2108, 510, 11, 597, 307, 300, 586, 300, 321, 600, 1096, 341, 11], "temperature": 0.0, "avg_logprob": -0.11432887077331542, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.3931391953956336e-06}, {"id": 517, "seek": 347220, "start": 3485.3799999999997, "end": 3490.74, "text": " we can say, create a cosine annealing callback which is going to update the learning rates", "tokens": [321, 393, 584, 11, 1884, 257, 23565, 22256, 4270, 818, 3207, 597, 307, 516, 281, 5623, 264, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.11432887077331542, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.3931391953956336e-06}, {"id": 518, "seek": 347220, "start": 3490.74, "end": 3499.02, "text": " in this layer optimizer. The length of an epoch is equal to this here. How many mini-batches", "tokens": [294, 341, 4583, 5028, 6545, 13, 440, 4641, 295, 364, 30992, 339, 307, 2681, 281, 341, 510, 13, 1012, 867, 8382, 12, 65, 852, 279], "temperature": 0.0, "avg_logprob": -0.11432887077331542, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.3931391953956336e-06}, {"id": 519, "seek": 349902, "start": 3499.02, "end": 3504.2599999999998, "text": " are there in an epoch? Well, it's whatever the length of this data loader is. Because", "tokens": [366, 456, 294, 364, 30992, 339, 30, 1042, 11, 309, 311, 2035, 264, 4641, 295, 341, 1412, 3677, 260, 307, 13, 1436], "temperature": 0.0, "avg_logprob": -0.1341281467013889, "compression_ratio": 1.6265060240963856, "no_speech_prob": 2.6016111860371893e-06}, {"id": 520, "seek": 349902, "start": 3504.2599999999998, "end": 3511.62, "text": " it's going to be doing the cosine annealing, it needs to know how often to reset. And then", "tokens": [309, 311, 516, 281, 312, 884, 264, 23565, 22256, 4270, 11, 309, 2203, 281, 458, 577, 2049, 281, 14322, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.1341281467013889, "compression_ratio": 1.6265060240963856, "no_speech_prob": 2.6016111860371893e-06}, {"id": 521, "seek": 349902, "start": 3511.62, "end": 3514.82, "text": " you can pass in the cycleMult in the usual way.", "tokens": [291, 393, 1320, 294, 264, 6586, 44, 723, 294, 264, 7713, 636, 13], "temperature": 0.0, "avg_logprob": -0.1341281467013889, "compression_ratio": 1.6265060240963856, "no_speech_prob": 2.6016111860371893e-06}, {"id": 522, "seek": 349902, "start": 3514.82, "end": 3522.7, "text": " And then we can even save our model automatically. Remember how there was that cycleSaveName", "tokens": [400, 550, 321, 393, 754, 3155, 527, 2316, 6772, 13, 5459, 577, 456, 390, 300, 6586, 46899, 45, 529], "temperature": 0.0, "avg_logprob": -0.1341281467013889, "compression_ratio": 1.6265060240963856, "no_speech_prob": 2.6016111860371893e-06}, {"id": 523, "seek": 349902, "start": 3522.7, "end": 3526.5, "text": " parameter that we can pass to learn.fit. This is what it does behind the scenes. Behind", "tokens": [13075, 300, 321, 393, 1320, 281, 1466, 13, 6845, 13, 639, 307, 437, 309, 775, 2261, 264, 8026, 13, 20475], "temperature": 0.0, "avg_logprob": -0.1341281467013889, "compression_ratio": 1.6265060240963856, "no_speech_prob": 2.6016111860371893e-06}, {"id": 524, "seek": 352650, "start": 3526.5, "end": 3532.94, "text": " the scenes, it sets an onCycleEnd callback. And so here I've defined that callback as", "tokens": [264, 8026, 11, 309, 6352, 364, 322, 34, 88, 2160, 36952, 818, 3207, 13, 400, 370, 510, 286, 600, 7642, 300, 818, 3207, 382], "temperature": 0.0, "avg_logprob": -0.13483093630883, "compression_ratio": 1.9727272727272727, "no_speech_prob": 7.112428193067899e-07}, {"id": 525, "seek": 352650, "start": 3532.94, "end": 3537.14, "text": " being something that saves my model.", "tokens": [885, 746, 300, 19155, 452, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13483093630883, "compression_ratio": 1.9727272727272727, "no_speech_prob": 7.112428193067899e-07}, {"id": 526, "seek": 352650, "start": 3537.14, "end": 3543.08, "text": " So there's quite a lot of cool stuff that you can do with callbacks. Callbacks are basically", "tokens": [407, 456, 311, 1596, 257, 688, 295, 1627, 1507, 300, 291, 393, 360, 365, 818, 17758, 13, 7807, 17758, 366, 1936], "temperature": 0.0, "avg_logprob": -0.13483093630883, "compression_ratio": 1.9727272727272727, "no_speech_prob": 7.112428193067899e-07}, {"id": 527, "seek": 352650, "start": 3543.08, "end": 3547.38, "text": " things where you can define like at the start of training, or at the start of an epoch,", "tokens": [721, 689, 291, 393, 6964, 411, 412, 264, 722, 295, 3097, 11, 420, 412, 264, 722, 295, 364, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.13483093630883, "compression_ratio": 1.9727272727272727, "no_speech_prob": 7.112428193067899e-07}, {"id": 528, "seek": 352650, "start": 3547.38, "end": 3550.58, "text": " or at the start of a batch, or at the end of training, or at the end of an epoch, or", "tokens": [420, 412, 264, 722, 295, 257, 15245, 11, 420, 412, 264, 917, 295, 3097, 11, 420, 412, 264, 917, 295, 364, 30992, 339, 11, 420], "temperature": 0.0, "avg_logprob": -0.13483093630883, "compression_ratio": 1.9727272727272727, "no_speech_prob": 7.112428193067899e-07}, {"id": 529, "seek": 352650, "start": 3550.58, "end": 3553.7, "text": " at the end of a batch, please call this code.", "tokens": [412, 264, 917, 295, 257, 15245, 11, 1767, 818, 341, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13483093630883, "compression_ratio": 1.9727272727272727, "no_speech_prob": 7.112428193067899e-07}, {"id": 530, "seek": 355370, "start": 3553.7, "end": 3561.7799999999997, "text": " And so we've written some for you, including sgdr, which is the cosine annealing callback.", "tokens": [400, 370, 321, 600, 3720, 512, 337, 291, 11, 3009, 262, 70, 16753, 11, 597, 307, 264, 23565, 22256, 4270, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.1000293697322811, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.66461869009072e-06}, {"id": 531, "seek": 355370, "start": 3561.7799999999997, "end": 3566.8599999999997, "text": " And then Sahar recently wrote a new callback to implement the new approach to decoupled", "tokens": [400, 550, 18280, 289, 3938, 4114, 257, 777, 818, 3207, 281, 4445, 264, 777, 3109, 281, 979, 263, 15551], "temperature": 0.0, "avg_logprob": -0.1000293697322811, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.66461869009072e-06}, {"id": 532, "seek": 355370, "start": 3566.8599999999997, "end": 3574.66, "text": " weight decay. We use callbacks to draw those little graphs of the loss over time. So there's", "tokens": [3364, 21039, 13, 492, 764, 818, 17758, 281, 2642, 729, 707, 24877, 295, 264, 4470, 670, 565, 13, 407, 456, 311], "temperature": 0.0, "avg_logprob": -0.1000293697322811, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.66461869009072e-06}, {"id": 533, "seek": 355370, "start": 3574.66, "end": 3576.66, "text": " lots of cool stuff you can do with callbacks.", "tokens": [3195, 295, 1627, 1507, 291, 393, 360, 365, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.1000293697322811, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.66461869009072e-06}, {"id": 534, "seek": 355370, "start": 3576.66, "end": 3583.5, "text": " So in this case, by passing in that callback, we're getting sgdr, and that's able to get", "tokens": [407, 294, 341, 1389, 11, 538, 8437, 294, 300, 818, 3207, 11, 321, 434, 1242, 262, 70, 16753, 11, 293, 300, 311, 1075, 281, 483], "temperature": 0.0, "avg_logprob": -0.1000293697322811, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.66461869009072e-06}, {"id": 535, "seek": 358350, "start": 3583.5, "end": 3593.22, "text": " us down to 1.31 here. And then we can train a little bit more and eventually get down", "tokens": [505, 760, 281, 502, 13, 12967, 510, 13, 400, 550, 321, 393, 3847, 257, 707, 857, 544, 293, 4728, 483, 760], "temperature": 0.0, "avg_logprob": -0.16184827089309692, "compression_ratio": 1.5375722543352601, "no_speech_prob": 1.0616055988066364e-05}, {"id": 536, "seek": 358350, "start": 3593.22, "end": 3595.38, "text": " to 1.25.", "tokens": [281, 502, 13, 6074, 13], "temperature": 0.0, "avg_logprob": -0.16184827089309692, "compression_ratio": 1.5375722543352601, "no_speech_prob": 1.0616055988066364e-05}, {"id": 537, "seek": 358350, "start": 3595.38, "end": 3603.66, "text": " And so we can now test that out. And so if we pass in a few characters of text, we get", "tokens": [400, 370, 321, 393, 586, 1500, 300, 484, 13, 400, 370, 498, 321, 1320, 294, 257, 1326, 4342, 295, 2487, 11, 321, 483], "temperature": 0.0, "avg_logprob": -0.16184827089309692, "compression_ratio": 1.5375722543352601, "no_speech_prob": 1.0616055988066364e-05}, {"id": 538, "seek": 358350, "start": 3603.66, "end": 3611.34, "text": " not surprisingly an e after for of those. Let's do then 400. And now we have our own", "tokens": [406, 17600, 364, 308, 934, 337, 295, 729, 13, 961, 311, 360, 550, 8423, 13, 400, 586, 321, 362, 527, 1065], "temperature": 0.0, "avg_logprob": -0.16184827089309692, "compression_ratio": 1.5375722543352601, "no_speech_prob": 1.0616055988066364e-05}, {"id": 539, "seek": 361134, "start": 3611.34, "end": 3617.26, "text": " Nietzsche. So Nietzsche tends to start his sections with a number and a dot. So 293,", "tokens": [36583, 89, 12287, 13, 407, 36583, 89, 12287, 12258, 281, 722, 702, 10863, 365, 257, 1230, 293, 257, 5893, 13, 407, 9413, 18, 11], "temperature": 0.0, "avg_logprob": -0.16132655002102994, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.1692507036495954e-05}, {"id": 540, "seek": 361134, "start": 3617.26, "end": 3622.78, "text": " perhaps that every life have values of blood, of intercourse, when it senses there is unscrupulous", "tokens": [4317, 300, 633, 993, 362, 4190, 295, 3390, 11, 295, 728, 31913, 11, 562, 309, 17057, 456, 307, 2693, 66, 11976, 6893], "temperature": 0.0, "avg_logprob": -0.16132655002102994, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.1692507036495954e-05}, {"id": 541, "seek": 361134, "start": 3622.78, "end": 3625.94, "text": " his very rights and still impulse love.", "tokens": [702, 588, 4601, 293, 920, 26857, 959, 13], "temperature": 0.0, "avg_logprob": -0.16132655002102994, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.1692507036495954e-05}, {"id": 542, "seek": 361134, "start": 3625.94, "end": 3633.3, "text": " So it's slightly less clear than Nietzsche normally, but it gets the tone right. And", "tokens": [407, 309, 311, 4748, 1570, 1850, 813, 36583, 89, 12287, 5646, 11, 457, 309, 2170, 264, 8027, 558, 13, 400], "temperature": 0.0, "avg_logprob": -0.16132655002102994, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.1692507036495954e-05}, {"id": 543, "seek": 361134, "start": 3633.3, "end": 3639.54, "text": " it's actually quite interesting to play around with training these character-based language", "tokens": [309, 311, 767, 1596, 1880, 281, 862, 926, 365, 3097, 613, 2517, 12, 6032, 2856], "temperature": 0.0, "avg_logprob": -0.16132655002102994, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.1692507036495954e-05}, {"id": 544, "seek": 363954, "start": 3639.54, "end": 3645.46, "text": " models to run this at different levels of loss, to get a sense of what does it look", "tokens": [5245, 281, 1190, 341, 412, 819, 4358, 295, 4470, 11, 281, 483, 257, 2020, 295, 437, 775, 309, 574], "temperature": 0.0, "avg_logprob": -0.1551416757944468, "compression_ratio": 1.4594594594594594, "no_speech_prob": 7.183233265095623e-06}, {"id": 545, "seek": 363954, "start": 3645.46, "end": 3646.46, "text": " like.", "tokens": [411, 13], "temperature": 0.0, "avg_logprob": -0.1551416757944468, "compression_ratio": 1.4594594594594594, "no_speech_prob": 7.183233265095623e-06}, {"id": 546, "seek": 363954, "start": 3646.46, "end": 3655.92, "text": " Like you really notice that this is like 1.25. And at slightly worse, like 1.3, this looks", "tokens": [1743, 291, 534, 3449, 300, 341, 307, 411, 502, 13, 6074, 13, 400, 412, 4748, 5324, 11, 411, 502, 13, 18, 11, 341, 1542], "temperature": 0.0, "avg_logprob": -0.1551416757944468, "compression_ratio": 1.4594594594594594, "no_speech_prob": 7.183233265095623e-06}, {"id": 547, "seek": 363954, "start": 3655.92, "end": 3663.1, "text": " like total junk. There's punctuation in random places, nothing makes sense. And you start", "tokens": [411, 3217, 19109, 13, 821, 311, 27006, 16073, 294, 4974, 3190, 11, 1825, 1669, 2020, 13, 400, 291, 722], "temperature": 0.0, "avg_logprob": -0.1551416757944468, "compression_ratio": 1.4594594594594594, "no_speech_prob": 7.183233265095623e-06}, {"id": 548, "seek": 366310, "start": 3663.1, "end": 3670.3399999999997, "text": " to realize that the difference between Nietzsche and random junk is not that far in language", "tokens": [281, 4325, 300, 264, 2649, 1296, 36583, 89, 12287, 293, 4974, 19109, 307, 406, 300, 1400, 294, 2856], "temperature": 0.0, "avg_logprob": -0.17715043732614227, "compression_ratio": 1.5657370517928286, "no_speech_prob": 3.041584704988054e-06}, {"id": 549, "seek": 366310, "start": 3670.3399999999997, "end": 3675.1, "text": " model terms. And so if you train this for a little bit longer, you'll suddenly find", "tokens": [2316, 2115, 13, 400, 370, 498, 291, 3847, 341, 337, 257, 707, 857, 2854, 11, 291, 603, 5800, 915], "temperature": 0.0, "avg_logprob": -0.17715043732614227, "compression_ratio": 1.5657370517928286, "no_speech_prob": 3.041584704988054e-06}, {"id": 550, "seek": 366310, "start": 3675.1, "end": 3678.42, "text": " like, oh, it's making more and more sense.", "tokens": [411, 11, 1954, 11, 309, 311, 1455, 544, 293, 544, 2020, 13], "temperature": 0.0, "avg_logprob": -0.17715043732614227, "compression_ratio": 1.5657370517928286, "no_speech_prob": 3.041584704988054e-06}, {"id": 551, "seek": 366310, "start": 3678.42, "end": 3685.02, "text": " So if you are playing around with NLP stuff, particularly generative stuff like this, and", "tokens": [407, 498, 291, 366, 2433, 926, 365, 426, 45196, 1507, 11, 4098, 1337, 1166, 1507, 411, 341, 11, 293], "temperature": 0.0, "avg_logprob": -0.17715043732614227, "compression_ratio": 1.5657370517928286, "no_speech_prob": 3.041584704988054e-06}, {"id": 552, "seek": 366310, "start": 3685.02, "end": 3690.7799999999997, "text": " the results are like kind of okay but not great, don't be disheartened because that", "tokens": [264, 3542, 366, 411, 733, 295, 1392, 457, 406, 869, 11, 500, 380, 312, 717, 12864, 5320, 570, 300], "temperature": 0.0, "avg_logprob": -0.17715043732614227, "compression_ratio": 1.5657370517928286, "no_speech_prob": 3.041584704988054e-06}, {"id": 553, "seek": 369078, "start": 3690.78, "end": 3695.94, "text": " means you're actually very, very nearly there. The difference between something which is", "tokens": [1355, 291, 434, 767, 588, 11, 588, 6217, 456, 13, 440, 2649, 1296, 746, 597, 307], "temperature": 0.0, "avg_logprob": -0.14706226875042094, "compression_ratio": 1.5266272189349113, "no_speech_prob": 1.3845623470842838e-05}, {"id": 554, "seek": 369078, "start": 3695.94, "end": 3700.6600000000003, "text": " starting to create something which almost vaguely looks English if you squint, and something", "tokens": [2891, 281, 1884, 746, 597, 1920, 13501, 48863, 1542, 3669, 498, 291, 2339, 686, 11, 293, 746], "temperature": 0.0, "avg_logprob": -0.14706226875042094, "compression_ratio": 1.5266272189349113, "no_speech_prob": 1.3845623470842838e-05}, {"id": 555, "seek": 369078, "start": 3700.6600000000003, "end": 3708.82, "text": " that's actually a very good generation, it's not far in loss function terms.", "tokens": [300, 311, 767, 257, 588, 665, 5125, 11, 309, 311, 406, 1400, 294, 4470, 2445, 2115, 13], "temperature": 0.0, "avg_logprob": -0.14706226875042094, "compression_ratio": 1.5266272189349113, "no_speech_prob": 1.3845623470842838e-05}, {"id": 556, "seek": 370882, "start": 3708.82, "end": 3724.5800000000004, "text": " So let's take a 5 minute break, we'll come back at 7.45 and we're going to go back to", "tokens": [407, 718, 311, 747, 257, 1025, 3456, 1821, 11, 321, 603, 808, 646, 412, 1614, 13, 8465, 293, 321, 434, 516, 281, 352, 646, 281], "temperature": 0.0, "avg_logprob": -0.19544274231483197, "compression_ratio": 1.0493827160493827, "no_speech_prob": 1.4510030268866103e-05}, {"id": 557, "seek": 372458, "start": 3724.58, "end": 3738.98, "text": " vision. So now we're looking at lesson 7, Sci-Fi 10 notebook. You might have heard of", "tokens": [5201, 13, 407, 586, 321, 434, 1237, 412, 6898, 1614, 11, 16942, 12, 13229, 1266, 21060, 13, 509, 1062, 362, 2198, 295], "temperature": 0.0, "avg_logprob": -0.2756632614135742, "compression_ratio": 1.2666666666666666, "no_speech_prob": 1.3845560715708416e-05}, {"id": 558, "seek": 372458, "start": 3738.98, "end": 3747.42, "text": " Sci-Fi 10. It's a really well-known dataset in academia. And it's actually pretty old", "tokens": [16942, 12, 13229, 1266, 13, 467, 311, 257, 534, 731, 12, 6861, 28872, 294, 28937, 13, 400, 309, 311, 767, 1238, 1331], "temperature": 0.0, "avg_logprob": -0.2756632614135742, "compression_ratio": 1.2666666666666666, "no_speech_prob": 1.3845560715708416e-05}, {"id": 559, "seek": 374742, "start": 3747.42, "end": 3755.54, "text": " by computer vision standards. Well before ImageNet was around, there was Sci-Fi 10.", "tokens": [538, 3820, 5201, 7787, 13, 1042, 949, 29903, 31890, 390, 926, 11, 456, 390, 16942, 12, 13229, 1266, 13], "temperature": 0.0, "avg_logprob": -0.16395096662567882, "compression_ratio": 1.508695652173913, "no_speech_prob": 1.5689221982029267e-05}, {"id": 560, "seek": 374742, "start": 3755.54, "end": 3761.06, "text": " You might wonder why we're going to be looking at such an old dataset. Actually, I think", "tokens": [509, 1062, 2441, 983, 321, 434, 516, 281, 312, 1237, 412, 1270, 364, 1331, 28872, 13, 5135, 11, 286, 519], "temperature": 0.0, "avg_logprob": -0.16395096662567882, "compression_ratio": 1.508695652173913, "no_speech_prob": 1.5689221982029267e-05}, {"id": 561, "seek": 374742, "start": 3761.06, "end": 3767.98, "text": " small datasets are much more interesting than ImageNet. Because most of the time you're", "tokens": [1359, 42856, 366, 709, 544, 1880, 813, 29903, 31890, 13, 1436, 881, 295, 264, 565, 291, 434], "temperature": 0.0, "avg_logprob": -0.16395096662567882, "compression_ratio": 1.508695652173913, "no_speech_prob": 1.5689221982029267e-05}, {"id": 562, "seek": 374742, "start": 3767.98, "end": 3774.42, "text": " likely to be working with stuff with a small number of thousands of images rather than", "tokens": [3700, 281, 312, 1364, 365, 1507, 365, 257, 1359, 1230, 295, 5383, 295, 5267, 2831, 813], "temperature": 0.0, "avg_logprob": -0.16395096662567882, "compression_ratio": 1.508695652173913, "no_speech_prob": 1.5689221982029267e-05}, {"id": 563, "seek": 377442, "start": 3774.42, "end": 3779.58, "text": " 1.5 million images. Some of you will work with 1.5 million images, but most of you won't.", "tokens": [502, 13, 20, 2459, 5267, 13, 2188, 295, 291, 486, 589, 365, 502, 13, 20, 2459, 5267, 11, 457, 881, 295, 291, 1582, 380, 13], "temperature": 0.0, "avg_logprob": -0.14123600407650597, "compression_ratio": 1.6705426356589148, "no_speech_prob": 4.425427960086381e-06}, {"id": 564, "seek": 377442, "start": 3779.58, "end": 3783.7000000000003, "text": " So learning how to use these kind of datasets I think is much more interesting.", "tokens": [407, 2539, 577, 281, 764, 613, 733, 295, 42856, 286, 519, 307, 709, 544, 1880, 13], "temperature": 0.0, "avg_logprob": -0.14123600407650597, "compression_ratio": 1.6705426356589148, "no_speech_prob": 4.425427960086381e-06}, {"id": 565, "seek": 377442, "start": 3783.7000000000003, "end": 3787.2200000000003, "text": " Often also a lot of the stuff we're looking at like in medical imaging, we're looking", "tokens": [20043, 611, 257, 688, 295, 264, 1507, 321, 434, 1237, 412, 411, 294, 4625, 25036, 11, 321, 434, 1237], "temperature": 0.0, "avg_logprob": -0.14123600407650597, "compression_ratio": 1.6705426356589148, "no_speech_prob": 4.425427960086381e-06}, {"id": 566, "seek": 377442, "start": 3787.2200000000003, "end": 3794.38, "text": " at the specific area where there's a lung nodule, you're probably looking at 32x32 pixels", "tokens": [412, 264, 2685, 1859, 689, 456, 311, 257, 16730, 15224, 2271, 11, 291, 434, 1391, 1237, 412, 8858, 87, 11440, 18668], "temperature": 0.0, "avg_logprob": -0.14123600407650597, "compression_ratio": 1.6705426356589148, "no_speech_prob": 4.425427960086381e-06}, {"id": 567, "seek": 377442, "start": 3794.38, "end": 3799.7000000000003, "text": " at most as being the area where that lung nodule actually exists. And so Sci-Fi 10 is", "tokens": [412, 881, 382, 885, 264, 1859, 689, 300, 16730, 15224, 2271, 767, 8198, 13, 400, 370, 16942, 12, 13229, 1266, 307], "temperature": 0.0, "avg_logprob": -0.14123600407650597, "compression_ratio": 1.6705426356589148, "no_speech_prob": 4.425427960086381e-06}, {"id": 568, "seek": 379970, "start": 3799.7, "end": 3806.3799999999997, "text": " small both in terms of it doesn't have many images and the images are very small. And", "tokens": [1359, 1293, 294, 2115, 295, 309, 1177, 380, 362, 867, 5267, 293, 264, 5267, 366, 588, 1359, 13, 400], "temperature": 0.0, "avg_logprob": -0.1546368650210801, "compression_ratio": 1.7511520737327189, "no_speech_prob": 8.446202173217898e-07}, {"id": 569, "seek": 379970, "start": 3806.3799999999997, "end": 3811.2599999999998, "text": " in a lot of ways it's much more challenging than something like ImageNet. In some ways", "tokens": [294, 257, 688, 295, 2098, 309, 311, 709, 544, 7595, 813, 746, 411, 29903, 31890, 13, 682, 512, 2098], "temperature": 0.0, "avg_logprob": -0.1546368650210801, "compression_ratio": 1.7511520737327189, "no_speech_prob": 8.446202173217898e-07}, {"id": 570, "seek": 379970, "start": 3811.2599999999998, "end": 3813.98, "text": " it's much more interesting.", "tokens": [309, 311, 709, 544, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1546368650210801, "compression_ratio": 1.7511520737327189, "no_speech_prob": 8.446202173217898e-07}, {"id": 571, "seek": 379970, "start": 3813.98, "end": 3818.62, "text": " And also, most importantly, you can run stuff much more quickly on it. So it's much better", "tokens": [400, 611, 11, 881, 8906, 11, 291, 393, 1190, 1507, 709, 544, 2661, 322, 309, 13, 407, 309, 311, 709, 1101], "temperature": 0.0, "avg_logprob": -0.1546368650210801, "compression_ratio": 1.7511520737327189, "no_speech_prob": 8.446202173217898e-07}, {"id": 572, "seek": 379970, "start": 3818.62, "end": 3825.02, "text": " to test out your algorithms with something you can run quickly and is still challenging.", "tokens": [281, 1500, 484, 428, 14642, 365, 746, 291, 393, 1190, 2661, 293, 307, 920, 7595, 13], "temperature": 0.0, "avg_logprob": -0.1546368650210801, "compression_ratio": 1.7511520737327189, "no_speech_prob": 8.446202173217898e-07}, {"id": 573, "seek": 382502, "start": 3825.02, "end": 3830.98, "text": " And so I hear a lot of researchers complain about how they can't afford to study all the", "tokens": [400, 370, 286, 1568, 257, 688, 295, 10309, 11024, 466, 577, 436, 393, 380, 6157, 281, 2979, 439, 264], "temperature": 0.0, "avg_logprob": -0.1655287375816932, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.8738693370323745e-06}, {"id": 574, "seek": 382502, "start": 3830.98, "end": 3835.34, "text": " different versions of their algorithm properly because it's too expensive. And they're doing", "tokens": [819, 9606, 295, 641, 9284, 6108, 570, 309, 311, 886, 5124, 13, 400, 436, 434, 884], "temperature": 0.0, "avg_logprob": -0.1655287375816932, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.8738693370323745e-06}, {"id": 575, "seek": 382502, "start": 3835.34, "end": 3842.54, "text": " it on ImageNet. So it's literally a week of expensive GPU work for every study they do.", "tokens": [309, 322, 29903, 31890, 13, 407, 309, 311, 3736, 257, 1243, 295, 5124, 18407, 589, 337, 633, 2979, 436, 360, 13], "temperature": 0.0, "avg_logprob": -0.1655287375816932, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.8738693370323745e-06}, {"id": 576, "seek": 382502, "start": 3842.54, "end": 3846.42, "text": " And I don't understand why you would do that kind of study on ImageNet. It doesn't make", "tokens": [400, 286, 500, 380, 1223, 983, 291, 576, 360, 300, 733, 295, 2979, 322, 29903, 31890, 13, 467, 1177, 380, 652], "temperature": 0.0, "avg_logprob": -0.1655287375816932, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.8738693370323745e-06}, {"id": 577, "seek": 382502, "start": 3846.42, "end": 3848.86, "text": " sense.", "tokens": [2020, 13], "temperature": 0.0, "avg_logprob": -0.1655287375816932, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.8738693370323745e-06}, {"id": 578, "seek": 384886, "start": 3848.86, "end": 3858.2400000000002, "text": " And so there's been a particular lot of debate about this this week because a really interesting", "tokens": [400, 370, 456, 311, 668, 257, 1729, 688, 295, 7958, 466, 341, 341, 1243, 570, 257, 534, 1880], "temperature": 0.0, "avg_logprob": -0.2098811989399924, "compression_ratio": 1.5333333333333334, "no_speech_prob": 6.540350113937166e-06}, {"id": 579, "seek": 384886, "start": 3858.2400000000002, "end": 3864.54, "text": " researcher named Ali Rahami at NIPS this week gave a talk, a really great talk, about the", "tokens": [21751, 4926, 12020, 17844, 4526, 412, 18482, 6273, 341, 1243, 2729, 257, 751, 11, 257, 534, 869, 751, 11, 466, 264], "temperature": 0.0, "avg_logprob": -0.2098811989399924, "compression_ratio": 1.5333333333333334, "no_speech_prob": 6.540350113937166e-06}, {"id": 580, "seek": 384886, "start": 3864.54, "end": 3871.7000000000003, "text": " need for rigor in experiments in deep learning. He felt like there's a lack of rigor. And", "tokens": [643, 337, 42191, 294, 12050, 294, 2452, 2539, 13, 634, 2762, 411, 456, 311, 257, 5011, 295, 42191, 13, 400], "temperature": 0.0, "avg_logprob": -0.2098811989399924, "compression_ratio": 1.5333333333333334, "no_speech_prob": 6.540350113937166e-06}, {"id": 581, "seek": 387170, "start": 3871.7, "end": 3879.3399999999997, "text": " I've talked to him about it quite a bit since that time. I'm not sure we yet quite understand", "tokens": [286, 600, 2825, 281, 796, 466, 309, 1596, 257, 857, 1670, 300, 565, 13, 286, 478, 406, 988, 321, 1939, 1596, 1223], "temperature": 0.0, "avg_logprob": -0.1453192289485488, "compression_ratio": 1.5570175438596492, "no_speech_prob": 9.97276947600767e-06}, {"id": 582, "seek": 387170, "start": 3879.3399999999997, "end": 3884.18, "text": " each other as to where we're coming from, but we have very similar kinds of concerns,", "tokens": [1184, 661, 382, 281, 689, 321, 434, 1348, 490, 11, 457, 321, 362, 588, 2531, 3685, 295, 7389, 11], "temperature": 0.0, "avg_logprob": -0.1453192289485488, "compression_ratio": 1.5570175438596492, "no_speech_prob": 9.97276947600767e-06}, {"id": 583, "seek": 387170, "start": 3884.18, "end": 3890.3399999999997, "text": " which is basically people aren't doing carefully tuned, carefully thought about experiments,", "tokens": [597, 307, 1936, 561, 3212, 380, 884, 7500, 10870, 11, 7500, 1194, 466, 12050, 11], "temperature": 0.0, "avg_logprob": -0.1453192289485488, "compression_ratio": 1.5570175438596492, "no_speech_prob": 9.97276947600767e-06}, {"id": 584, "seek": 387170, "start": 3890.3399999999997, "end": 3895.3399999999997, "text": " but instead they kind of throw lots of GPUs, lots of data and consider that a day.", "tokens": [457, 2602, 436, 733, 295, 3507, 3195, 295, 18407, 82, 11, 3195, 295, 1412, 293, 1949, 300, 257, 786, 13], "temperature": 0.0, "avg_logprob": -0.1453192289485488, "compression_ratio": 1.5570175438596492, "no_speech_prob": 9.97276947600767e-06}, {"id": 585, "seek": 389534, "start": 3895.34, "end": 3904.2200000000003, "text": " And so this idea of saying, is my algorithm meant to be good at small images, at small", "tokens": [400, 370, 341, 1558, 295, 1566, 11, 307, 452, 9284, 4140, 281, 312, 665, 412, 1359, 5267, 11, 412, 1359], "temperature": 0.0, "avg_logprob": -0.18916175050555534, "compression_ratio": 1.6007604562737643, "no_speech_prob": 5.173836598260095e-06}, {"id": 586, "seek": 389534, "start": 3904.2200000000003, "end": 3909.46, "text": " data sets, or if so let's study it on CyPhar 10 rather than studying it on ImageNet and", "tokens": [1412, 6352, 11, 420, 498, 370, 718, 311, 2979, 309, 322, 10295, 47, 5854, 1266, 2831, 813, 7601, 309, 322, 29903, 31890, 293], "temperature": 0.0, "avg_logprob": -0.18916175050555534, "compression_ratio": 1.6007604562737643, "no_speech_prob": 5.173836598260095e-06}, {"id": 587, "seek": 389534, "start": 3909.46, "end": 3913.54, "text": " then do more studies of different versions of the algorithm, turning different bits on", "tokens": [550, 360, 544, 5313, 295, 819, 9606, 295, 264, 9284, 11, 6246, 819, 9239, 322], "temperature": 0.0, "avg_logprob": -0.18916175050555534, "compression_ratio": 1.6007604562737643, "no_speech_prob": 5.173836598260095e-06}, {"id": 588, "seek": 389534, "start": 3913.54, "end": 3919.6600000000003, "text": " and off, understand which parts are actually important, and so forth.", "tokens": [293, 766, 11, 1223, 597, 3166, 366, 767, 1021, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.18916175050555534, "compression_ratio": 1.6007604562737643, "no_speech_prob": 5.173836598260095e-06}, {"id": 589, "seek": 389534, "start": 3919.6600000000003, "end": 3924.58, "text": " People also complain a lot about MNIST, which we've looked at before. And I would say the", "tokens": [3432, 611, 11024, 257, 688, 466, 376, 45, 19756, 11, 597, 321, 600, 2956, 412, 949, 13, 400, 286, 576, 584, 264], "temperature": 0.0, "avg_logprob": -0.18916175050555534, "compression_ratio": 1.6007604562737643, "no_speech_prob": 5.173836598260095e-06}, {"id": 590, "seek": 392458, "start": 3924.58, "end": 3928.42, "text": " same thing about MNIST, which is if you're actually trying to understand which parts", "tokens": [912, 551, 466, 376, 45, 19756, 11, 597, 307, 498, 291, 434, 767, 1382, 281, 1223, 597, 3166], "temperature": 0.0, "avg_logprob": -0.17141745811284975, "compression_ratio": 1.6219931271477663, "no_speech_prob": 1.6964124370133504e-05}, {"id": 591, "seek": 392458, "start": 3928.42, "end": 3932.94, "text": " of your algorithm make a difference and why, using MNIST for that kind of study is a very", "tokens": [295, 428, 9284, 652, 257, 2649, 293, 983, 11, 1228, 376, 45, 19756, 337, 300, 733, 295, 2979, 307, 257, 588], "temperature": 0.0, "avg_logprob": -0.17141745811284975, "compression_ratio": 1.6219931271477663, "no_speech_prob": 1.6964124370133504e-05}, {"id": 592, "seek": 392458, "start": 3932.94, "end": 3938.1, "text": " good idea. And all these people who complain about MNIST, I think they're just showing", "tokens": [665, 1558, 13, 400, 439, 613, 561, 567, 11024, 466, 376, 45, 19756, 11, 286, 519, 436, 434, 445, 4099], "temperature": 0.0, "avg_logprob": -0.17141745811284975, "compression_ratio": 1.6219931271477663, "no_speech_prob": 1.6964124370133504e-05}, {"id": 593, "seek": 392458, "start": 3938.1, "end": 3943.8199999999997, "text": " off. They're saying, I work at Google and I have a pod of TPUs and I have $100,000 a", "tokens": [766, 13, 814, 434, 1566, 11, 286, 589, 412, 3329, 293, 286, 362, 257, 2497, 295, 314, 8115, 82, 293, 286, 362, 1848, 6879, 11, 1360, 257], "temperature": 0.0, "avg_logprob": -0.17141745811284975, "compression_ratio": 1.6219931271477663, "no_speech_prob": 1.6964124370133504e-05}, {"id": 594, "seek": 392458, "start": 3943.8199999999997, "end": 3950.8199999999997, "text": " week of time to spend on it, no worries. But I think that's all it is. It's just signaling", "tokens": [1243, 295, 565, 281, 3496, 322, 309, 11, 572, 16340, 13, 583, 286, 519, 300, 311, 439, 309, 307, 13, 467, 311, 445, 38639], "temperature": 0.0, "avg_logprob": -0.17141745811284975, "compression_ratio": 1.6219931271477663, "no_speech_prob": 1.6964124370133504e-05}, {"id": 595, "seek": 392458, "start": 3950.8199999999997, "end": 3954.2599999999998, "text": " rather than academically rigorous.", "tokens": [2831, 813, 48944, 29882, 13], "temperature": 0.0, "avg_logprob": -0.17141745811284975, "compression_ratio": 1.6219931271477663, "no_speech_prob": 1.6964124370133504e-05}, {"id": 596, "seek": 395426, "start": 3954.26, "end": 3960.1400000000003, "text": " So CyPhar 10, you can download from here. This person has very kindly made it available", "tokens": [407, 10295, 47, 5854, 1266, 11, 291, 393, 5484, 490, 510, 13, 639, 954, 575, 588, 29736, 1027, 309, 2435], "temperature": 0.0, "avg_logprob": -0.12829701443935962, "compression_ratio": 1.5596330275229358, "no_speech_prob": 2.144474819942843e-05}, {"id": 597, "seek": 395426, "start": 3960.1400000000003, "end": 3967.1000000000004, "text": " in image form. If you Google for CyPhar 10, you'll find a much less convenient form, so", "tokens": [294, 3256, 1254, 13, 759, 291, 3329, 337, 10295, 47, 5854, 1266, 11, 291, 603, 915, 257, 709, 1570, 10851, 1254, 11, 370], "temperature": 0.0, "avg_logprob": -0.12829701443935962, "compression_ratio": 1.5596330275229358, "no_speech_prob": 2.144474819942843e-05}, {"id": 598, "seek": 395426, "start": 3967.1000000000004, "end": 3972.38, "text": " please use this one. It's already in the exact form you need. Once you download it, you can", "tokens": [1767, 764, 341, 472, 13, 467, 311, 1217, 294, 264, 1900, 1254, 291, 643, 13, 3443, 291, 5484, 309, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.12829701443935962, "compression_ratio": 1.5596330275229358, "no_speech_prob": 2.144474819942843e-05}, {"id": 599, "seek": 395426, "start": 3972.38, "end": 3981.82, "text": " use it in the usual way. So here's a list of the classes that are there.", "tokens": [764, 309, 294, 264, 7713, 636, 13, 407, 510, 311, 257, 1329, 295, 264, 5359, 300, 366, 456, 13], "temperature": 0.0, "avg_logprob": -0.12829701443935962, "compression_ratio": 1.5596330275229358, "no_speech_prob": 2.144474819942843e-05}, {"id": 600, "seek": 398182, "start": 3981.82, "end": 3986.54, "text": " Now you'll see here I've created this thing called stats. Normally when we've been using", "tokens": [823, 291, 603, 536, 510, 286, 600, 2942, 341, 551, 1219, 18152, 13, 17424, 562, 321, 600, 668, 1228], "temperature": 0.0, "avg_logprob": -0.11587103107307531, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.0953029232041445e-05}, {"id": 601, "seek": 398182, "start": 3986.54, "end": 3996.6600000000003, "text": " pre-trained models, we have been saying transforms from model and that's actually created the", "tokens": [659, 12, 17227, 2001, 5245, 11, 321, 362, 668, 1566, 35592, 490, 2316, 293, 300, 311, 767, 2942, 264], "temperature": 0.0, "avg_logprob": -0.11587103107307531, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.0953029232041445e-05}, {"id": 602, "seek": 398182, "start": 3996.6600000000003, "end": 4003.7200000000003, "text": " necessary transforms to convert our dataset into a normalized dataset based on the means", "tokens": [4818, 35592, 281, 7620, 527, 28872, 666, 257, 48704, 28872, 2361, 322, 264, 1355], "temperature": 0.0, "avg_logprob": -0.11587103107307531, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.0953029232041445e-05}, {"id": 603, "seek": 398182, "start": 4003.7200000000003, "end": 4009.26, "text": " and standard deviations of each channel in the original model that was trained. In our", "tokens": [293, 3832, 31219, 763, 295, 1184, 2269, 294, 264, 3380, 2316, 300, 390, 8895, 13, 682, 527], "temperature": 0.0, "avg_logprob": -0.11587103107307531, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.0953029232041445e-05}, {"id": 604, "seek": 400926, "start": 4009.26, "end": 4014.82, "text": " case though, this time we've got to train a model from scratch, so we have no such thing.", "tokens": [1389, 1673, 11, 341, 565, 321, 600, 658, 281, 3847, 257, 2316, 490, 8459, 11, 370, 321, 362, 572, 1270, 551, 13], "temperature": 0.0, "avg_logprob": -0.11778208586546751, "compression_ratio": 1.7519685039370079, "no_speech_prob": 4.222818006383022e-06}, {"id": 605, "seek": 400926, "start": 4014.82, "end": 4021.0600000000004, "text": " So we actually need to tell it the mean and standard deviation of our data to normalize", "tokens": [407, 321, 767, 643, 281, 980, 309, 264, 914, 293, 3832, 25163, 295, 527, 1412, 281, 2710, 1125], "temperature": 0.0, "avg_logprob": -0.11778208586546751, "compression_ratio": 1.7519685039370079, "no_speech_prob": 4.222818006383022e-06}, {"id": 606, "seek": 400926, "start": 4021.0600000000004, "end": 4026.2200000000003, "text": " it. So in this case, I haven't included the code here to do it. You should try and try", "tokens": [309, 13, 407, 294, 341, 1389, 11, 286, 2378, 380, 5556, 264, 3089, 510, 281, 360, 309, 13, 509, 820, 853, 293, 853], "temperature": 0.0, "avg_logprob": -0.11778208586546751, "compression_ratio": 1.7519685039370079, "no_speech_prob": 4.222818006383022e-06}, {"id": 607, "seek": 400926, "start": 4026.2200000000003, "end": 4030.0200000000004, "text": " this yourself to confirm that you can do this and understand where it comes from. But this", "tokens": [341, 1803, 281, 9064, 300, 291, 393, 360, 341, 293, 1223, 689, 309, 1487, 490, 13, 583, 341], "temperature": 0.0, "avg_logprob": -0.11778208586546751, "compression_ratio": 1.7519685039370079, "no_speech_prob": 4.222818006383022e-06}, {"id": 608, "seek": 400926, "start": 4030.0200000000004, "end": 4039.1800000000003, "text": " is just the mean per channel and the standard deviation per channel of all of the images.", "tokens": [307, 445, 264, 914, 680, 2269, 293, 264, 3832, 25163, 680, 2269, 295, 439, 295, 264, 5267, 13], "temperature": 0.0, "avg_logprob": -0.11778208586546751, "compression_ratio": 1.7519685039370079, "no_speech_prob": 4.222818006383022e-06}, {"id": 609, "seek": 403918, "start": 4039.18, "end": 4047.2999999999997, "text": " So we're going to try and create a model from scratch. So the first thing we need is some", "tokens": [407, 321, 434, 516, 281, 853, 293, 1884, 257, 2316, 490, 8459, 13, 407, 264, 700, 551, 321, 643, 307, 512], "temperature": 0.0, "avg_logprob": -0.17822861671447754, "compression_ratio": 1.4972067039106145, "no_speech_prob": 8.398029422096442e-06}, {"id": 610, "seek": 403918, "start": 4047.2999999999997, "end": 4054.98, "text": " transformations. So for Sci-Fi 10, people generally do data augmentation of simply flipping", "tokens": [34852, 13, 407, 337, 16942, 12, 13229, 1266, 11, 561, 5101, 360, 1412, 14501, 19631, 295, 2935, 26886], "temperature": 0.0, "avg_logprob": -0.17822861671447754, "compression_ratio": 1.4972067039106145, "no_speech_prob": 8.398029422096442e-06}, {"id": 611, "seek": 403918, "start": 4054.98, "end": 4062.8599999999997, "text": " randomly horizontally. So here's how we can create a specific list of augmentations to", "tokens": [16979, 33796, 13, 407, 510, 311, 577, 321, 393, 1884, 257, 2685, 1329, 295, 29919, 763, 281], "temperature": 0.0, "avg_logprob": -0.17822861671447754, "compression_ratio": 1.4972067039106145, "no_speech_prob": 8.398029422096442e-06}, {"id": 612, "seek": 406286, "start": 4062.86, "end": 4069.5, "text": " use. And then they also tend to add a little bit of padding, black padding around the edge,", "tokens": [764, 13, 400, 550, 436, 611, 3928, 281, 909, 257, 707, 857, 295, 39562, 11, 2211, 39562, 926, 264, 4691, 11], "temperature": 0.0, "avg_logprob": -0.14633211336637797, "compression_ratio": 1.508108108108108, "no_speech_prob": 6.339181254588766e-06}, {"id": 613, "seek": 406286, "start": 4069.5, "end": 4076.98, "text": " and then randomly pick a 32x32 spot from within that padded image. So if you add the pad parameter", "tokens": [293, 550, 16979, 1888, 257, 8858, 87, 11440, 4008, 490, 1951, 300, 6887, 9207, 3256, 13, 407, 498, 291, 909, 264, 6887, 13075], "temperature": 0.0, "avg_logprob": -0.14633211336637797, "compression_ratio": 1.508108108108108, "no_speech_prob": 6.339181254588766e-06}, {"id": 614, "seek": 406286, "start": 4076.98, "end": 4085.34, "text": " to any of the FastAI transform creators, it'll do that for you. And so in this case, I'm", "tokens": [281, 604, 295, 264, 15968, 48698, 4088, 16039, 11, 309, 603, 360, 300, 337, 291, 13, 400, 370, 294, 341, 1389, 11, 286, 478], "temperature": 0.0, "avg_logprob": -0.14633211336637797, "compression_ratio": 1.508108108108108, "no_speech_prob": 6.339181254588766e-06}, {"id": 615, "seek": 408534, "start": 4085.34, "end": 4093.7400000000002, "text": " just going to add 4 pixels around each size. And so now that I've got my transforms, I", "tokens": [445, 516, 281, 909, 1017, 18668, 926, 1184, 2744, 13, 400, 370, 586, 300, 286, 600, 658, 452, 35592, 11, 286], "temperature": 0.0, "avg_logprob": -0.11050796508789062, "compression_ratio": 1.5643153526970954, "no_speech_prob": 2.2252766029851045e-06}, {"id": 616, "seek": 408534, "start": 4093.7400000000002, "end": 4101.38, "text": " can go ahead and create my image classifier data.from paths in the usual way. I'm going", "tokens": [393, 352, 2286, 293, 1884, 452, 3256, 1508, 9902, 1412, 13, 20579, 14518, 294, 264, 7713, 636, 13, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.11050796508789062, "compression_ratio": 1.5643153526970954, "no_speech_prob": 2.2252766029851045e-06}, {"id": 617, "seek": 408534, "start": 4101.38, "end": 4106.18, "text": " to use a batch size of 256 because these are pretty small, so it's going to let me do a", "tokens": [281, 764, 257, 15245, 2744, 295, 38882, 570, 613, 366, 1238, 1359, 11, 370, 309, 311, 516, 281, 718, 385, 360, 257], "temperature": 0.0, "avg_logprob": -0.11050796508789062, "compression_ratio": 1.5643153526970954, "no_speech_prob": 2.2252766029851045e-06}, {"id": 618, "seek": 408534, "start": 4106.18, "end": 4108.26, "text": " little bit more at a time.", "tokens": [707, 857, 544, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.11050796508789062, "compression_ratio": 1.5643153526970954, "no_speech_prob": 2.2252766029851045e-06}, {"id": 619, "seek": 408534, "start": 4108.26, "end": 4113.62, "text": " So here's what the data looks like. So for example, here's a boat. And just to show you", "tokens": [407, 510, 311, 437, 264, 1412, 1542, 411, 13, 407, 337, 1365, 11, 510, 311, 257, 6582, 13, 400, 445, 281, 855, 291], "temperature": 0.0, "avg_logprob": -0.11050796508789062, "compression_ratio": 1.5643153526970954, "no_speech_prob": 2.2252766029851045e-06}, {"id": 620, "seek": 411362, "start": 4113.62, "end": 4134.46, "text": " how tough this is, what's that? So these are the kinds of things that we want to look at.", "tokens": [577, 4930, 341, 307, 11, 437, 311, 300, 30, 407, 613, 366, 264, 3685, 295, 721, 300, 321, 528, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.21104987462361655, "compression_ratio": 1.362962962962963, "no_speech_prob": 4.785073542734608e-06}, {"id": 621, "seek": 411362, "start": 4134.46, "end": 4140.94, "text": " So I'm going to start out. Our student, Kerim, we saw one of his posts earlier in this course,", "tokens": [407, 286, 478, 516, 281, 722, 484, 13, 2621, 3107, 11, 20706, 332, 11, 321, 1866, 472, 295, 702, 12300, 3071, 294, 341, 1164, 11], "temperature": 0.0, "avg_logprob": -0.21104987462361655, "compression_ratio": 1.362962962962963, "no_speech_prob": 4.785073542734608e-06}, {"id": 622, "seek": 414094, "start": 4140.94, "end": 4151.9, "text": " he made this really cool notebook which shows how different optimizers work. So Kerim made", "tokens": [415, 1027, 341, 534, 1627, 21060, 597, 3110, 577, 819, 5028, 22525, 589, 13, 407, 20706, 332, 1027], "temperature": 0.0, "avg_logprob": -0.15282937155829535, "compression_ratio": 1.691588785046729, "no_speech_prob": 1.4063828530197497e-05}, {"id": 623, "seek": 414094, "start": 4151.9, "end": 4157.339999999999, "text": " this really cool notebook, I think it was maybe last week, in which he showed how to", "tokens": [341, 534, 1627, 21060, 11, 286, 519, 309, 390, 1310, 1036, 1243, 11, 294, 597, 415, 4712, 577, 281], "temperature": 0.0, "avg_logprob": -0.15282937155829535, "compression_ratio": 1.691588785046729, "no_speech_prob": 1.4063828530197497e-05}, {"id": 624, "seek": 414094, "start": 4157.339999999999, "end": 4161.54, "text": " create various different optimizers from scratch. So this is kind of like the Excel thing I", "tokens": [1884, 3683, 819, 5028, 22525, 490, 8459, 13, 407, 341, 307, 733, 295, 411, 264, 19060, 551, 286], "temperature": 0.0, "avg_logprob": -0.15282937155829535, "compression_ratio": 1.691588785046729, "no_speech_prob": 1.4063828530197497e-05}, {"id": 625, "seek": 414094, "start": 4161.54, "end": 4167.179999999999, "text": " had, but this is the Python version of Momentum and Atom and Nesterov and Atograd, all written", "tokens": [632, 11, 457, 341, 307, 264, 15329, 3037, 295, 19093, 449, 293, 1711, 298, 293, 31581, 2032, 85, 293, 1711, 664, 6206, 11, 439, 3720], "temperature": 0.0, "avg_logprob": -0.15282937155829535, "compression_ratio": 1.691588785046729, "no_speech_prob": 1.4063828530197497e-05}, {"id": 626, "seek": 416718, "start": 4167.18, "end": 4172.780000000001, "text": " from scratch, which is really cool. One of the nice things he did was he showed a tiny", "tokens": [490, 8459, 11, 597, 307, 534, 1627, 13, 1485, 295, 264, 1481, 721, 415, 630, 390, 415, 4712, 257, 5870], "temperature": 0.0, "avg_logprob": -0.13827723208988937, "compression_ratio": 1.6596638655462186, "no_speech_prob": 6.748004580003908e-06}, {"id": 627, "seek": 416718, "start": 4172.780000000001, "end": 4178.9400000000005, "text": " little general purpose fully connected network generator. So we're going to start with his.", "tokens": [707, 2674, 4334, 4498, 4582, 3209, 19265, 13, 407, 321, 434, 516, 281, 722, 365, 702, 13], "temperature": 0.0, "avg_logprob": -0.13827723208988937, "compression_ratio": 1.6596638655462186, "no_speech_prob": 6.748004580003908e-06}, {"id": 628, "seek": 416718, "start": 4178.9400000000005, "end": 4181.66, "text": " So he called that SimpleNet, so are we.", "tokens": [407, 415, 1219, 300, 21532, 31890, 11, 370, 366, 321, 13], "temperature": 0.0, "avg_logprob": -0.13827723208988937, "compression_ratio": 1.6596638655462186, "no_speech_prob": 6.748004580003908e-06}, {"id": 629, "seek": 416718, "start": 4181.66, "end": 4190.780000000001, "text": " So here's a simple class which has a list of fully connected layers. Whenever you create", "tokens": [407, 510, 311, 257, 2199, 1508, 597, 575, 257, 1329, 295, 4498, 4582, 7914, 13, 14159, 291, 1884], "temperature": 0.0, "avg_logprob": -0.13827723208988937, "compression_ratio": 1.6596638655462186, "no_speech_prob": 6.748004580003908e-06}, {"id": 630, "seek": 416718, "start": 4190.780000000001, "end": 4196.76, "text": " a list of layers in PyTorch, you have to wrap it in nn.module list just to tell PyTorch", "tokens": [257, 1329, 295, 7914, 294, 9953, 51, 284, 339, 11, 291, 362, 281, 7019, 309, 294, 297, 77, 13, 8014, 2271, 1329, 445, 281, 980, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.13827723208988937, "compression_ratio": 1.6596638655462186, "no_speech_prob": 6.748004580003908e-06}, {"id": 631, "seek": 419676, "start": 4196.76, "end": 4204.6, "text": " to register these as attributes. And so then we just go ahead and flatten the data that", "tokens": [281, 7280, 613, 382, 17212, 13, 400, 370, 550, 321, 445, 352, 2286, 293, 24183, 264, 1412, 300], "temperature": 0.0, "avg_logprob": -0.15563043921884864, "compression_ratio": 1.6026200873362446, "no_speech_prob": 4.157348485023249e-06}, {"id": 632, "seek": 419676, "start": 4204.6, "end": 4209.22, "text": " comes in because it's fully connected layers, and then go through each layer and call that", "tokens": [1487, 294, 570, 309, 311, 4498, 4582, 7914, 11, 293, 550, 352, 807, 1184, 4583, 293, 818, 300], "temperature": 0.0, "avg_logprob": -0.15563043921884864, "compression_ratio": 1.6026200873362446, "no_speech_prob": 4.157348485023249e-06}, {"id": 633, "seek": 419676, "start": 4209.22, "end": 4217.42, "text": " linear layer, do the ReLU to it, and at the end do a softmax. So there's a really simple", "tokens": [8213, 4583, 11, 360, 264, 1300, 43, 52, 281, 309, 11, 293, 412, 264, 917, 360, 257, 2787, 41167, 13, 407, 456, 311, 257, 534, 2199], "temperature": 0.0, "avg_logprob": -0.15563043921884864, "compression_ratio": 1.6026200873362446, "no_speech_prob": 4.157348485023249e-06}, {"id": 634, "seek": 419676, "start": 4217.42, "end": 4219.26, "text": " approach.", "tokens": [3109, 13], "temperature": 0.0, "avg_logprob": -0.15563043921884864, "compression_ratio": 1.6026200873362446, "no_speech_prob": 4.157348485023249e-06}, {"id": 635, "seek": 419676, "start": 4219.26, "end": 4225.16, "text": " And so we can now take that model, and now I'm going to show you how to step up one level", "tokens": [400, 370, 321, 393, 586, 747, 300, 2316, 11, 293, 586, 286, 478, 516, 281, 855, 291, 577, 281, 1823, 493, 472, 1496], "temperature": 0.0, "avg_logprob": -0.15563043921884864, "compression_ratio": 1.6026200873362446, "no_speech_prob": 4.157348485023249e-06}, {"id": 636, "seek": 422516, "start": 4225.16, "end": 4229.26, "text": " of the API higher. Rather than calling the fit function, we're going to create a learn", "tokens": [295, 264, 9362, 2946, 13, 16571, 813, 5141, 264, 3318, 2445, 11, 321, 434, 516, 281, 1884, 257, 1466], "temperature": 0.0, "avg_logprob": -0.10723274065100628, "compression_ratio": 1.8869565217391304, "no_speech_prob": 1.3925457551522413e-06}, {"id": 637, "seek": 422516, "start": 4229.26, "end": 4235.099999999999, "text": " object, but we're going to create a learn object from a custom model. And so we can", "tokens": [2657, 11, 457, 321, 434, 516, 281, 1884, 257, 1466, 2657, 490, 257, 2375, 2316, 13, 400, 370, 321, 393], "temperature": 0.0, "avg_logprob": -0.10723274065100628, "compression_ratio": 1.8869565217391304, "no_speech_prob": 1.3925457551522413e-06}, {"id": 638, "seek": 422516, "start": 4235.099999999999, "end": 4240.34, "text": " do that by saying we want a convolutional learner, we want to create it from a model", "tokens": [360, 300, 538, 1566, 321, 528, 257, 45216, 304, 33347, 11, 321, 528, 281, 1884, 309, 490, 257, 2316], "temperature": 0.0, "avg_logprob": -0.10723274065100628, "compression_ratio": 1.8869565217391304, "no_speech_prob": 1.3925457551522413e-06}, {"id": 639, "seek": 422516, "start": 4240.34, "end": 4247.98, "text": " and from some data, and the model is this one. So this is just a general PyTorch model,", "tokens": [293, 490, 512, 1412, 11, 293, 264, 2316, 307, 341, 472, 13, 407, 341, 307, 445, 257, 2674, 9953, 51, 284, 339, 2316, 11], "temperature": 0.0, "avg_logprob": -0.10723274065100628, "compression_ratio": 1.8869565217391304, "no_speech_prob": 1.3925457551522413e-06}, {"id": 640, "seek": 422516, "start": 4247.98, "end": 4253.38, "text": " and this is a model data object of the usual kind. And that will return a learner. So this", "tokens": [293, 341, 307, 257, 2316, 1412, 2657, 295, 264, 7713, 733, 13, 400, 300, 486, 2736, 257, 33347, 13, 407, 341], "temperature": 0.0, "avg_logprob": -0.10723274065100628, "compression_ratio": 1.8869565217391304, "no_speech_prob": 1.3925457551522413e-06}, {"id": 641, "seek": 425338, "start": 4253.38, "end": 4257.46, "text": " is a bit easier than what we just saw with the RNN. We don't have to fiddle around with", "tokens": [307, 257, 857, 3571, 813, 437, 321, 445, 1866, 365, 264, 45702, 45, 13, 492, 500, 380, 362, 281, 24553, 2285, 926, 365], "temperature": 0.0, "avg_logprob": -0.14459367658271163, "compression_ratio": 1.6319702602230484, "no_speech_prob": 2.0904524262732593e-06}, {"id": 642, "seek": 425338, "start": 4257.46, "end": 4262.62, "text": " layer optimizers and cosine annealing callbacks and whatever. This is now a learner that we", "tokens": [4583, 5028, 22525, 293, 23565, 22256, 4270, 818, 17758, 293, 2035, 13, 639, 307, 586, 257, 33347, 300, 321], "temperature": 0.0, "avg_logprob": -0.14459367658271163, "compression_ratio": 1.6319702602230484, "no_speech_prob": 2.0904524262732593e-06}, {"id": 643, "seek": 425338, "start": 4262.62, "end": 4270.5, "text": " can do all the usual stuff with, but we can do it with any model that we create.", "tokens": [393, 360, 439, 264, 7713, 1507, 365, 11, 457, 321, 393, 360, 309, 365, 604, 2316, 300, 321, 1884, 13], "temperature": 0.0, "avg_logprob": -0.14459367658271163, "compression_ratio": 1.6319702602230484, "no_speech_prob": 2.0904524262732593e-06}, {"id": 644, "seek": 425338, "start": 4270.5, "end": 4277.58, "text": " So if we just go learn, that'll go ahead and print it out. So you can see we've got 3072", "tokens": [407, 498, 321, 445, 352, 1466, 11, 300, 603, 352, 2286, 293, 4482, 309, 484, 13, 407, 291, 393, 536, 321, 600, 658, 2217, 28890], "temperature": 0.0, "avg_logprob": -0.14459367658271163, "compression_ratio": 1.6319702602230484, "no_speech_prob": 2.0904524262732593e-06}, {"id": 645, "seek": 425338, "start": 4277.58, "end": 4283.36, "text": " features coming in because we've got 32x32 pixels by 3 channels. And then we've got 40x32", "tokens": [4122, 1348, 294, 570, 321, 600, 658, 8858, 87, 11440, 18668, 538, 805, 9235, 13, 400, 550, 321, 600, 658, 3356, 87, 11440], "temperature": 0.0, "avg_logprob": -0.14459367658271163, "compression_ratio": 1.6319702602230484, "no_speech_prob": 2.0904524262732593e-06}, {"id": 646, "seek": 428336, "start": 4283.36, "end": 4288.139999999999, "text": " features coming out of the first layer, that's going to go into the second layer, 10 features", "tokens": [4122, 1348, 484, 295, 264, 700, 4583, 11, 300, 311, 516, 281, 352, 666, 264, 1150, 4583, 11, 1266, 4122], "temperature": 0.0, "avg_logprob": -0.1798206500792771, "compression_ratio": 1.592039800995025, "no_speech_prob": 4.029445790365571e-06}, {"id": 647, "seek": 428336, "start": 4288.139999999999, "end": 4294.98, "text": " coming out because we've got the 10 CyPhy 10 categories.", "tokens": [1348, 484, 570, 321, 600, 658, 264, 1266, 10295, 47, 3495, 1266, 10479, 13], "temperature": 0.0, "avg_logprob": -0.1798206500792771, "compression_ratio": 1.592039800995025, "no_speech_prob": 4.029445790365571e-06}, {"id": 648, "seek": 428336, "start": 4294.98, "end": 4300.38, "text": " You can call dot summary to see that in a little bit more detail. We can do LR find,", "tokens": [509, 393, 818, 5893, 12691, 281, 536, 300, 294, 257, 707, 857, 544, 2607, 13, 492, 393, 360, 441, 49, 915, 11], "temperature": 0.0, "avg_logprob": -0.1798206500792771, "compression_ratio": 1.592039800995025, "no_speech_prob": 4.029445790365571e-06}, {"id": 649, "seek": 428336, "start": 4300.38, "end": 4307.36, "text": " we can plot that, and we can then go fit, and we can use cycle length, and so forth.", "tokens": [321, 393, 7542, 300, 11, 293, 321, 393, 550, 352, 3318, 11, 293, 321, 393, 764, 6586, 4641, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1798206500792771, "compression_ratio": 1.592039800995025, "no_speech_prob": 4.029445790365571e-06}, {"id": 650, "seek": 430736, "start": 4307.36, "end": 4320.179999999999, "text": " So with a simple one hidden layer, one output layer, one hidden layer model, and here we", "tokens": [407, 365, 257, 2199, 472, 7633, 4583, 11, 472, 5598, 4583, 11, 472, 7633, 4583, 2316, 11, 293, 510, 321], "temperature": 0.0, "avg_logprob": -0.16134273012479147, "compression_ratio": 1.359375, "no_speech_prob": 2.1568123429460684e-06}, {"id": 651, "seek": 430736, "start": 4320.179999999999, "end": 4332.259999999999, "text": " can see the number of parameters we have is that over 120,000, we get a 47% accuracy.", "tokens": [393, 536, 264, 1230, 295, 9834, 321, 362, 307, 300, 670, 10411, 11, 1360, 11, 321, 483, 257, 16953, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.16134273012479147, "compression_ratio": 1.359375, "no_speech_prob": 2.1568123429460684e-06}, {"id": 652, "seek": 433226, "start": 4332.26, "end": 4338.58, "text": " So not great, so let's kind of try and improve it. And so the goal here is we're going to", "tokens": [407, 406, 869, 11, 370, 718, 311, 733, 295, 853, 293, 3470, 309, 13, 400, 370, 264, 3387, 510, 307, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.15319670330394397, "compression_ratio": 1.67, "no_speech_prob": 2.4824764750519535e-06}, {"id": 653, "seek": 433226, "start": 4338.58, "end": 4346.34, "text": " try and eventually replicate the basic kind of architecture of a ResNet. So that's what", "tokens": [853, 293, 4728, 25356, 264, 3875, 733, 295, 9482, 295, 257, 5015, 31890, 13, 407, 300, 311, 437], "temperature": 0.0, "avg_logprob": -0.15319670330394397, "compression_ratio": 1.67, "no_speech_prob": 2.4824764750519535e-06}, {"id": 654, "seek": 433226, "start": 4346.34, "end": 4350.6, "text": " we're going to try and get to here, is gradually build up to a ResNet.", "tokens": [321, 434, 516, 281, 853, 293, 483, 281, 510, 11, 307, 13145, 1322, 493, 281, 257, 5015, 31890, 13], "temperature": 0.0, "avg_logprob": -0.15319670330394397, "compression_ratio": 1.67, "no_speech_prob": 2.4824764750519535e-06}, {"id": 655, "seek": 433226, "start": 4350.6, "end": 4357.12, "text": " So the first step is to replace our fully connected model with a convolutional model.", "tokens": [407, 264, 700, 1823, 307, 281, 7406, 527, 4498, 4582, 2316, 365, 257, 45216, 304, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15319670330394397, "compression_ratio": 1.67, "no_speech_prob": 2.4824764750519535e-06}, {"id": 656, "seek": 435712, "start": 4357.12, "end": 4371.3, "text": " So to remind you, a fully connected layer is simply doing a dot product. So if we had", "tokens": [407, 281, 4160, 291, 11, 257, 4498, 4582, 4583, 307, 2935, 884, 257, 5893, 1674, 13, 407, 498, 321, 632], "temperature": 0.0, "avg_logprob": -0.15903250737623734, "compression_ratio": 1.4132231404958677, "no_speech_prob": 1.6280497447951348e-06}, {"id": 657, "seek": 435712, "start": 4371.3, "end": 4381.54, "text": " all of these data points and all of these weights, then we basically do a sum product", "tokens": [439, 295, 613, 1412, 2793, 293, 439, 295, 613, 17443, 11, 550, 321, 1936, 360, 257, 2408, 1674], "temperature": 0.0, "avg_logprob": -0.15903250737623734, "compression_ratio": 1.4132231404958677, "no_speech_prob": 1.6280497447951348e-06}, {"id": 658, "seek": 438154, "start": 4381.54, "end": 4387.56, "text": " of all of those together, in other words it's a matrix multiply, then that's a fully connected", "tokens": [295, 439, 295, 729, 1214, 11, 294, 661, 2283, 309, 311, 257, 8141, 12972, 11, 550, 300, 311, 257, 4498, 4582], "temperature": 0.0, "avg_logprob": -0.16393835648246433, "compression_ratio": 1.630952380952381, "no_speech_prob": 9.276336641050875e-07}, {"id": 659, "seek": 438154, "start": 4387.56, "end": 4396.34, "text": " layer. And so the weight matrix is going to contain an item for every element of the input,", "tokens": [4583, 13, 400, 370, 264, 3364, 8141, 307, 516, 281, 5304, 364, 3174, 337, 633, 4478, 295, 264, 4846, 11], "temperature": 0.0, "avg_logprob": -0.16393835648246433, "compression_ratio": 1.630952380952381, "no_speech_prob": 9.276336641050875e-07}, {"id": 660, "seek": 438154, "start": 4396.34, "end": 4406.08, "text": " for every element of the output. So that's why we have here a pretty big weight matrix.", "tokens": [337, 633, 4478, 295, 264, 5598, 13, 407, 300, 311, 983, 321, 362, 510, 257, 1238, 955, 3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.16393835648246433, "compression_ratio": 1.630952380952381, "no_speech_prob": 9.276336641050875e-07}, {"id": 661, "seek": 440608, "start": 4406.08, "end": 4411.22, "text": " And so that's why we had, despite the fact that we have such a crappy accuracy, we have", "tokens": [400, 370, 300, 311, 983, 321, 632, 11, 7228, 264, 1186, 300, 321, 362, 1270, 257, 36531, 14170, 11, 321, 362], "temperature": 0.0, "avg_logprob": -0.1638497572678786, "compression_ratio": 1.6216216216216217, "no_speech_prob": 4.3139081640219956e-07}, {"id": 662, "seek": 440608, "start": 4411.22, "end": 4419.14, "text": " a lot of parameters, because in this very first layer, we've got 3072 coming in and", "tokens": [257, 688, 295, 9834, 11, 570, 294, 341, 588, 700, 4583, 11, 321, 600, 658, 2217, 28890, 1348, 294, 293], "temperature": 0.0, "avg_logprob": -0.1638497572678786, "compression_ratio": 1.6216216216216217, "no_speech_prob": 4.3139081640219956e-07}, {"id": 663, "seek": 440608, "start": 4419.14, "end": 4427.72, "text": " 40 coming out, so that gives us 3000x40 parameters. And so we end up not using them very efficiently,", "tokens": [3356, 1348, 484, 11, 370, 300, 2709, 505, 20984, 87, 5254, 9834, 13, 400, 370, 321, 917, 493, 406, 1228, 552, 588, 19621, 11], "temperature": 0.0, "avg_logprob": -0.1638497572678786, "compression_ratio": 1.6216216216216217, "no_speech_prob": 4.3139081640219956e-07}, {"id": 664, "seek": 440608, "start": 4427.72, "end": 4431.76, "text": " because we're basically saying every single pixel in the input has a different weight.", "tokens": [570, 321, 434, 1936, 1566, 633, 2167, 19261, 294, 264, 4846, 575, 257, 819, 3364, 13], "temperature": 0.0, "avg_logprob": -0.1638497572678786, "compression_ratio": 1.6216216216216217, "no_speech_prob": 4.3139081640219956e-07}, {"id": 665, "seek": 443176, "start": 4431.76, "end": 4437.2, "text": " And of course what we really want to do is find groups of 3x3 pixels that have particular", "tokens": [400, 295, 1164, 437, 321, 534, 528, 281, 360, 307, 915, 3935, 295, 805, 87, 18, 18668, 300, 362, 1729], "temperature": 0.0, "avg_logprob": -0.15438006001134072, "compression_ratio": 1.463855421686747, "no_speech_prob": 1.4823565379629144e-06}, {"id": 666, "seek": 443176, "start": 4437.2, "end": 4442.320000000001, "text": " patterns to them. And remember, we call that a convolution.", "tokens": [8294, 281, 552, 13, 400, 1604, 11, 321, 818, 300, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.15438006001134072, "compression_ratio": 1.463855421686747, "no_speech_prob": 1.4823565379629144e-06}, {"id": 667, "seek": 443176, "start": 4442.320000000001, "end": 4458.58, "text": " So a convolution looks like so. We have a little 3x3 section of our image and a corresponding", "tokens": [407, 257, 45216, 1542, 411, 370, 13, 492, 362, 257, 707, 805, 87, 18, 3541, 295, 527, 3256, 293, 257, 11760], "temperature": 0.0, "avg_logprob": -0.15438006001134072, "compression_ratio": 1.463855421686747, "no_speech_prob": 1.4823565379629144e-06}, {"id": 668, "seek": 445858, "start": 4458.58, "end": 4467.04, "text": " 3x3 set of filters, or our filter with a 3x3 kernel, and we just do a sum product of just", "tokens": [805, 87, 18, 992, 295, 15995, 11, 420, 527, 6608, 365, 257, 805, 87, 18, 28256, 11, 293, 321, 445, 360, 257, 2408, 1674, 295, 445], "temperature": 0.0, "avg_logprob": -0.11705487966537476, "compression_ratio": 1.7462686567164178, "no_speech_prob": 1.903381985357555e-06}, {"id": 669, "seek": 445858, "start": 4467.04, "end": 4476.8, "text": " that 3x3 by that 3x3. And then we do that for every single part of our image. And so", "tokens": [300, 805, 87, 18, 538, 300, 805, 87, 18, 13, 400, 550, 321, 360, 300, 337, 633, 2167, 644, 295, 527, 3256, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.11705487966537476, "compression_ratio": 1.7462686567164178, "no_speech_prob": 1.903381985357555e-06}, {"id": 670, "seek": 445858, "start": 4476.8, "end": 4481.98, "text": " when we do that across the whole image, that's called a convolution. And remember, in this", "tokens": [562, 321, 360, 300, 2108, 264, 1379, 3256, 11, 300, 311, 1219, 257, 45216, 13, 400, 1604, 11, 294, 341], "temperature": 0.0, "avg_logprob": -0.11705487966537476, "compression_ratio": 1.7462686567164178, "no_speech_prob": 1.903381985357555e-06}, {"id": 671, "seek": 445858, "start": 4481.98, "end": 4488.22, "text": " case we actually had multiple filters, so the result of that convolution actually had", "tokens": [1389, 321, 767, 632, 3866, 15995, 11, 370, 264, 1874, 295, 300, 45216, 767, 632], "temperature": 0.0, "avg_logprob": -0.11705487966537476, "compression_ratio": 1.7462686567164178, "no_speech_prob": 1.903381985357555e-06}, {"id": 672, "seek": 448822, "start": 4488.22, "end": 4496.9800000000005, "text": " multiple, it was a tensor with an additional third dimension to it, effectively.", "tokens": [3866, 11, 309, 390, 257, 40863, 365, 364, 4497, 2636, 10139, 281, 309, 11, 8659, 13], "temperature": 0.0, "avg_logprob": -0.14520561372911608, "compression_ratio": 1.4184782608695652, "no_speech_prob": 4.09290214520297e-06}, {"id": 673, "seek": 448822, "start": 4496.9800000000005, "end": 4503.14, "text": " So let's take exactly the same code that we had before, but we're going to replace nn.linear", "tokens": [407, 718, 311, 747, 2293, 264, 912, 3089, 300, 321, 632, 949, 11, 457, 321, 434, 516, 281, 7406, 297, 77, 13, 28263], "temperature": 0.0, "avg_logprob": -0.14520561372911608, "compression_ratio": 1.4184782608695652, "no_speech_prob": 4.09290214520297e-06}, {"id": 674, "seek": 448822, "start": 4503.14, "end": 4512.64, "text": " with nn.com2d. Now what I want to do in this case though is each time I have a layer, I", "tokens": [365, 297, 77, 13, 1112, 17, 67, 13, 823, 437, 286, 528, 281, 360, 294, 341, 1389, 1673, 307, 1184, 565, 286, 362, 257, 4583, 11, 286], "temperature": 0.0, "avg_logprob": -0.14520561372911608, "compression_ratio": 1.4184782608695652, "no_speech_prob": 4.09290214520297e-06}, {"id": 675, "seek": 451264, "start": 4512.64, "end": 4519.04, "text": " want to make the next layer smaller. And so the way I did that in my Excel example was", "tokens": [528, 281, 652, 264, 958, 4583, 4356, 13, 400, 370, 264, 636, 286, 630, 300, 294, 452, 19060, 1365, 390], "temperature": 0.0, "avg_logprob": -0.14245204182414264, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.425480256031733e-06}, {"id": 676, "seek": 451264, "start": 4519.04, "end": 4528.72, "text": " I used maxPooling. So maxPooling took every 2x2 section and replaced it with its maximum", "tokens": [286, 1143, 11469, 47, 1092, 278, 13, 407, 11469, 47, 1092, 278, 1890, 633, 568, 87, 17, 3541, 293, 10772, 309, 365, 1080, 6674], "temperature": 0.0, "avg_logprob": -0.14245204182414264, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.425480256031733e-06}, {"id": 677, "seek": 451264, "start": 4528.72, "end": 4531.4400000000005, "text": " value.", "tokens": [2158, 13], "temperature": 0.0, "avg_logprob": -0.14245204182414264, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.425480256031733e-06}, {"id": 678, "seek": 451264, "start": 4531.4400000000005, "end": 4537.12, "text": " Nowadays we don't use that kind of maxPooling much at all. Instead nowadays what we tend", "tokens": [28908, 321, 500, 380, 764, 300, 733, 295, 11469, 47, 1092, 278, 709, 412, 439, 13, 7156, 13434, 437, 321, 3928], "temperature": 0.0, "avg_logprob": -0.14245204182414264, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.425480256031733e-06}, {"id": 679, "seek": 453712, "start": 4537.12, "end": 4544.28, "text": " to do is do what's called a stride2 convolution. A stride2 convolution, rather than saying", "tokens": [281, 360, 307, 360, 437, 311, 1219, 257, 1056, 482, 17, 45216, 13, 316, 1056, 482, 17, 45216, 11, 2831, 813, 1566], "temperature": 0.0, "avg_logprob": -0.09749214370529373, "compression_ratio": 1.7417218543046358, "no_speech_prob": 2.5612737317715073e-06}, {"id": 680, "seek": 453712, "start": 4544.28, "end": 4555.96, "text": " let's go through every single 3x3, it says let's go through every second 3x3. So rather", "tokens": [718, 311, 352, 807, 633, 2167, 805, 87, 18, 11, 309, 1619, 718, 311, 352, 807, 633, 1150, 805, 87, 18, 13, 407, 2831], "temperature": 0.0, "avg_logprob": -0.09749214370529373, "compression_ratio": 1.7417218543046358, "no_speech_prob": 2.5612737317715073e-06}, {"id": 681, "seek": 453712, "start": 4555.96, "end": 4561.5, "text": " than moving this 3x3 one to the right, we move it two to the right. And then when we", "tokens": [813, 2684, 341, 805, 87, 18, 472, 281, 264, 558, 11, 321, 1286, 309, 732, 281, 264, 558, 13, 400, 550, 562, 321], "temperature": 0.0, "avg_logprob": -0.09749214370529373, "compression_ratio": 1.7417218543046358, "no_speech_prob": 2.5612737317715073e-06}, {"id": 682, "seek": 456150, "start": 4561.5, "end": 4567.44, "text": " get to the end of the row, rather than moving one row down, we move two rows down. So that's", "tokens": [483, 281, 264, 917, 295, 264, 5386, 11, 2831, 813, 2684, 472, 5386, 760, 11, 321, 1286, 732, 13241, 760, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.09741816649565825, "compression_ratio": 1.6822033898305084, "no_speech_prob": 1.3497008239937713e-06}, {"id": 683, "seek": 456150, "start": 4567.44, "end": 4569.8, "text": " called a stride2 convolution.", "tokens": [1219, 257, 1056, 482, 17, 45216, 13], "temperature": 0.0, "avg_logprob": -0.09741816649565825, "compression_ratio": 1.6822033898305084, "no_speech_prob": 1.3497008239937713e-06}, {"id": 684, "seek": 456150, "start": 4569.8, "end": 4575.3, "text": " And so a stride2 convolution has the same kind of effect as a maxPooling, which is you", "tokens": [400, 370, 257, 1056, 482, 17, 45216, 575, 264, 912, 733, 295, 1802, 382, 257, 11469, 47, 1092, 278, 11, 597, 307, 291], "temperature": 0.0, "avg_logprob": -0.09741816649565825, "compression_ratio": 1.6822033898305084, "no_speech_prob": 1.3497008239937713e-06}, {"id": 685, "seek": 456150, "start": 4575.3, "end": 4581.8, "text": " end up halving the resolution in each dimension. So we can ask for that by saying stride equals", "tokens": [917, 493, 7523, 798, 264, 8669, 294, 1184, 10139, 13, 407, 321, 393, 1029, 337, 300, 538, 1566, 1056, 482, 6915], "temperature": 0.0, "avg_logprob": -0.09741816649565825, "compression_ratio": 1.6822033898305084, "no_speech_prob": 1.3497008239937713e-06}, {"id": 686, "seek": 456150, "start": 4581.8, "end": 4588.48, "text": " 2. We can say we want it to be 3x3 by saying kernel size. And then the first two parameters", "tokens": [568, 13, 492, 393, 584, 321, 528, 309, 281, 312, 805, 87, 18, 538, 1566, 28256, 2744, 13, 400, 550, 264, 700, 732, 9834], "temperature": 0.0, "avg_logprob": -0.09741816649565825, "compression_ratio": 1.6822033898305084, "no_speech_prob": 1.3497008239937713e-06}, {"id": 687, "seek": 458848, "start": 4588.48, "end": 4592.679999999999, "text": " are exactly the same as nn.linear. They're the number of features coming in and the number", "tokens": [366, 2293, 264, 912, 382, 297, 77, 13, 28263, 13, 814, 434, 264, 1230, 295, 4122, 1348, 294, 293, 264, 1230], "temperature": 0.0, "avg_logprob": -0.12987701779320127, "compression_ratio": 1.721461187214612, "no_speech_prob": 1.267926677428477e-06}, {"id": 688, "seek": 458848, "start": 4592.679999999999, "end": 4595.74, "text": " of features coming out.", "tokens": [295, 4122, 1348, 484, 13], "temperature": 0.0, "avg_logprob": -0.12987701779320127, "compression_ratio": 1.721461187214612, "no_speech_prob": 1.267926677428477e-06}, {"id": 689, "seek": 458848, "start": 4595.74, "end": 4603.44, "text": " So we create a module list of those layers. And then at the very end of that, so in this", "tokens": [407, 321, 1884, 257, 10088, 1329, 295, 729, 7914, 13, 400, 550, 412, 264, 588, 917, 295, 300, 11, 370, 294, 341], "temperature": 0.0, "avg_logprob": -0.12987701779320127, "compression_ratio": 1.721461187214612, "no_speech_prob": 1.267926677428477e-06}, {"id": 690, "seek": 458848, "start": 4603.44, "end": 4608.5599999999995, "text": " case I'm going to say I've got 3 channels coming in. The first one layer will come out", "tokens": [1389, 286, 478, 516, 281, 584, 286, 600, 658, 805, 9235, 1348, 294, 13, 440, 700, 472, 4583, 486, 808, 484], "temperature": 0.0, "avg_logprob": -0.12987701779320127, "compression_ratio": 1.721461187214612, "no_speech_prob": 1.267926677428477e-06}, {"id": 691, "seek": 458848, "start": 4608.5599999999995, "end": 4613.959999999999, "text": " with 20, then 40, and then 80. So if we look at the summary, we're going to start with", "tokens": [365, 945, 11, 550, 3356, 11, 293, 550, 4688, 13, 407, 498, 321, 574, 412, 264, 12691, 11, 321, 434, 516, 281, 722, 365], "temperature": 0.0, "avg_logprob": -0.12987701779320127, "compression_ratio": 1.721461187214612, "no_speech_prob": 1.267926677428477e-06}, {"id": 692, "seek": 461396, "start": 4613.96, "end": 4624.04, "text": " a 32x32, we're going to spit out a 15x15, and then a 7x7, and then a 3x3.", "tokens": [257, 8858, 87, 11440, 11, 321, 434, 516, 281, 22127, 484, 257, 2119, 87, 5211, 11, 293, 550, 257, 1614, 87, 22, 11, 293, 550, 257, 805, 87, 18, 13], "temperature": 0.0, "avg_logprob": -0.09321039105639045, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.9333574527991004e-06}, {"id": 693, "seek": 461396, "start": 4624.04, "end": 4631.24, "text": " And so what do we do now to get that down to a prediction of one of 10 classes? What", "tokens": [400, 370, 437, 360, 321, 360, 586, 281, 483, 300, 760, 281, 257, 17630, 295, 472, 295, 1266, 5359, 30, 708], "temperature": 0.0, "avg_logprob": -0.09321039105639045, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.9333574527991004e-06}, {"id": 694, "seek": 461396, "start": 4631.24, "end": 4637.2, "text": " we do is we do something called adaptive maxPooling. And this is what is pretty standard now for", "tokens": [321, 360, 307, 321, 360, 746, 1219, 27912, 11469, 47, 1092, 278, 13, 400, 341, 307, 437, 307, 1238, 3832, 586, 337], "temperature": 0.0, "avg_logprob": -0.09321039105639045, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.9333574527991004e-06}, {"id": 695, "seek": 463720, "start": 4637.2, "end": 4645.2, "text": " state of the art algorithms, is that the very last layer we do a maxPool. But rather than", "tokens": [1785, 295, 264, 1523, 14642, 11, 307, 300, 264, 588, 1036, 4583, 321, 360, 257, 11469, 47, 1092, 13, 583, 2831, 813], "temperature": 0.0, "avg_logprob": -0.17047422582452948, "compression_ratio": 1.6045454545454545, "no_speech_prob": 6.7480382313078735e-06}, {"id": 696, "seek": 463720, "start": 4645.2, "end": 4652.24, "text": " doing like a 2x2 maxPool, we say, it doesn't have to be 2x2, it could have been 3x3, which", "tokens": [884, 411, 257, 568, 87, 17, 11469, 47, 1092, 11, 321, 584, 11, 309, 1177, 380, 362, 281, 312, 568, 87, 17, 11, 309, 727, 362, 668, 805, 87, 18, 11, 597], "temperature": 0.0, "avg_logprob": -0.17047422582452948, "compression_ratio": 1.6045454545454545, "no_speech_prob": 6.7480382313078735e-06}, {"id": 697, "seek": 463720, "start": 4652.24, "end": 4657.72, "text": " is like replace every 3x3 pixels with its maximum, it could have been 4x4. Adaptive", "tokens": [307, 411, 7406, 633, 805, 87, 18, 18668, 365, 1080, 6674, 11, 309, 727, 362, 668, 1017, 87, 19, 13, 49643, 488], "temperature": 0.0, "avg_logprob": -0.17047422582452948, "compression_ratio": 1.6045454545454545, "no_speech_prob": 6.7480382313078735e-06}, {"id": 698, "seek": 463720, "start": 4657.72, "end": 4664.5599999999995, "text": " maxPool is where you say, I'm not going to tell you how big an area to pool, but instead", "tokens": [11469, 47, 1092, 307, 689, 291, 584, 11, 286, 478, 406, 516, 281, 980, 291, 577, 955, 364, 1859, 281, 7005, 11, 457, 2602], "temperature": 0.0, "avg_logprob": -0.17047422582452948, "compression_ratio": 1.6045454545454545, "no_speech_prob": 6.7480382313078735e-06}, {"id": 699, "seek": 466456, "start": 4664.56, "end": 4669.76, "text": " I'm going to tell you how big a resolution to create.", "tokens": [286, 478, 516, 281, 980, 291, 577, 955, 257, 8669, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.1075455471149926, "compression_ratio": 1.6598984771573604, "no_speech_prob": 5.594321919488721e-06}, {"id": 700, "seek": 466456, "start": 4669.76, "end": 4679.080000000001, "text": " So if I said, for example, I think my input here is like 28x28. If I said do a 14x14 adaptive", "tokens": [407, 498, 286, 848, 11, 337, 1365, 11, 286, 519, 452, 4846, 510, 307, 411, 7562, 87, 11205, 13, 759, 286, 848, 360, 257, 3499, 87, 7271, 27912], "temperature": 0.0, "avg_logprob": -0.1075455471149926, "compression_ratio": 1.6598984771573604, "no_speech_prob": 5.594321919488721e-06}, {"id": 701, "seek": 466456, "start": 4679.080000000001, "end": 4684.04, "text": " maxPool, that would be the same as a 2x2 maxPool, because in other words it's saying please", "tokens": [11469, 47, 1092, 11, 300, 576, 312, 264, 912, 382, 257, 568, 87, 17, 11469, 47, 1092, 11, 570, 294, 661, 2283, 309, 311, 1566, 1767], "temperature": 0.0, "avg_logprob": -0.1075455471149926, "compression_ratio": 1.6598984771573604, "no_speech_prob": 5.594321919488721e-06}, {"id": 702, "seek": 466456, "start": 4684.04, "end": 4692.6, "text": " create a 14x14 output. If I said do a 2x2 adaptive maxPool, then that would be the same", "tokens": [1884, 257, 3499, 87, 7271, 5598, 13, 759, 286, 848, 360, 257, 568, 87, 17, 27912, 11469, 47, 1092, 11, 550, 300, 576, 312, 264, 912], "temperature": 0.0, "avg_logprob": -0.1075455471149926, "compression_ratio": 1.6598984771573604, "no_speech_prob": 5.594321919488721e-06}, {"id": 703, "seek": 469260, "start": 4692.6, "end": 4700.88, "text": " as saying do a 14x14 maxPool. And so what we pretty much always do in modern CNNs is", "tokens": [382, 1566, 360, 257, 3499, 87, 7271, 11469, 47, 1092, 13, 400, 370, 437, 321, 1238, 709, 1009, 360, 294, 4363, 24859, 82, 307], "temperature": 0.0, "avg_logprob": -0.11184222881610577, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.071867457852932e-06}, {"id": 704, "seek": 469260, "start": 4700.88, "end": 4711.84, "text": " we make our penultimate layer a 1x1 adaptive maxPool. So in other words, find the single", "tokens": [321, 652, 527, 3435, 723, 2905, 4583, 257, 502, 87, 16, 27912, 11469, 47, 1092, 13, 407, 294, 661, 2283, 11, 915, 264, 2167], "temperature": 0.0, "avg_logprob": -0.11184222881610577, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.071867457852932e-06}, {"id": 705, "seek": 469260, "start": 4711.84, "end": 4722.0, "text": " largest cell and use that as our new activation. And so once we've got that, we've now got", "tokens": [6443, 2815, 293, 764, 300, 382, 527, 777, 24433, 13, 400, 370, 1564, 321, 600, 658, 300, 11, 321, 600, 586, 658], "temperature": 0.0, "avg_logprob": -0.11184222881610577, "compression_ratio": 1.4666666666666666, "no_speech_prob": 7.071867457852932e-06}, {"id": 706, "seek": 472200, "start": 4722.0, "end": 4729.92, "text": " a 1x1 tensor, or actually 1x1 by number of features tensor.", "tokens": [257, 502, 87, 16, 40863, 11, 420, 767, 502, 87, 16, 538, 1230, 295, 4122, 40863, 13], "temperature": 0.0, "avg_logprob": -0.1447445856381769, "compression_ratio": 1.56875, "no_speech_prob": 7.766923772578593e-06}, {"id": 707, "seek": 472200, "start": 4729.92, "end": 4741.06, "text": " So we can then on top of that go x.view, x.size, minus 1. And actually there are no other dimensions", "tokens": [407, 321, 393, 550, 322, 1192, 295, 300, 352, 2031, 13, 1759, 11, 2031, 13, 27553, 11, 3175, 502, 13, 400, 767, 456, 366, 572, 661, 12819], "temperature": 0.0, "avg_logprob": -0.1447445856381769, "compression_ratio": 1.56875, "no_speech_prob": 7.766923772578593e-06}, {"id": 708, "seek": 472200, "start": 4741.06, "end": 4749.52, "text": " to this basically. So this is going to return a matrix of minibatch by number of features.", "tokens": [281, 341, 1936, 13, 407, 341, 307, 516, 281, 2736, 257, 8141, 295, 923, 897, 852, 538, 1230, 295, 4122, 13], "temperature": 0.0, "avg_logprob": -0.1447445856381769, "compression_ratio": 1.56875, "no_speech_prob": 7.766923772578593e-06}, {"id": 709, "seek": 474952, "start": 4749.52, "end": 4757.72, "text": " And so then we can feed that into a linear layer with however many classes we need. So", "tokens": [400, 370, 550, 321, 393, 3154, 300, 666, 257, 8213, 4583, 365, 4461, 867, 5359, 321, 643, 13, 407], "temperature": 0.0, "avg_logprob": -0.1169936345971149, "compression_ratio": 1.6859903381642511, "no_speech_prob": 2.561278733992367e-06}, {"id": 710, "seek": 474952, "start": 4757.72, "end": 4762.56, "text": " you can see here the last thing I pass in is how many classes am I trying to predict,", "tokens": [291, 393, 536, 510, 264, 1036, 551, 286, 1320, 294, 307, 577, 867, 5359, 669, 286, 1382, 281, 6069, 11], "temperature": 0.0, "avg_logprob": -0.1169936345971149, "compression_ratio": 1.6859903381642511, "no_speech_prob": 2.561278733992367e-06}, {"id": 711, "seek": 474952, "start": 4762.56, "end": 4766.14, "text": " and that's what's going to be used to create that last layer. So it goes through every", "tokens": [293, 300, 311, 437, 311, 516, 281, 312, 1143, 281, 1884, 300, 1036, 4583, 13, 407, 309, 1709, 807, 633], "temperature": 0.0, "avg_logprob": -0.1169936345971149, "compression_ratio": 1.6859903381642511, "no_speech_prob": 2.561278733992367e-06}, {"id": 712, "seek": 474952, "start": 4766.14, "end": 4775.42, "text": " convolutional layer, does a convolution, does a relu, does an adaptive maxPool. This.view", "tokens": [45216, 304, 4583, 11, 775, 257, 45216, 11, 775, 257, 1039, 84, 11, 775, 364, 27912, 11469, 47, 1092, 13, 639, 2411, 1759], "temperature": 0.0, "avg_logprob": -0.1169936345971149, "compression_ratio": 1.6859903381642511, "no_speech_prob": 2.561278733992367e-06}, {"id": 713, "seek": 477542, "start": 4775.42, "end": 4783.92, "text": " just gets rid of those trailing unit axes, the 1,1 axis, which is not necessary. That", "tokens": [445, 2170, 3973, 295, 729, 944, 4883, 4985, 35387, 11, 264, 502, 11, 16, 10298, 11, 597, 307, 406, 4818, 13, 663], "temperature": 0.0, "avg_logprob": -0.14119720458984375, "compression_ratio": 1.39247311827957, "no_speech_prob": 4.222824372845935e-06}, {"id": 714, "seek": 477542, "start": 4783.92, "end": 4791.24, "text": " allows us to feed that into our final linear layer that spits out something of size C,", "tokens": [4045, 505, 281, 3154, 300, 666, 527, 2572, 8213, 4583, 300, 637, 1208, 484, 746, 295, 2744, 383, 11], "temperature": 0.0, "avg_logprob": -0.14119720458984375, "compression_ratio": 1.39247311827957, "no_speech_prob": 4.222824372845935e-06}, {"id": 715, "seek": 477542, "start": 4791.24, "end": 4800.0, "text": " which here is 10. So you can now see how it works. It goes 32 to 15 to 7x7 to 3x3. The", "tokens": [597, 510, 307, 1266, 13, 407, 291, 393, 586, 536, 577, 309, 1985, 13, 467, 1709, 8858, 281, 2119, 281, 1614, 87, 22, 281, 805, 87, 18, 13, 440], "temperature": 0.0, "avg_logprob": -0.14119720458984375, "compression_ratio": 1.39247311827957, "no_speech_prob": 4.222824372845935e-06}, {"id": 716, "seek": 480000, "start": 4800.0, "end": 4808.6, "text": " adaptive maxPool makes it 80x1x1. And then our.view makes it just minibatch size by", "tokens": [27912, 11469, 47, 1092, 1669, 309, 4688, 87, 16, 87, 16, 13, 400, 550, 527, 2411, 1759, 1669, 309, 445, 923, 897, 852, 2744, 538], "temperature": 0.0, "avg_logprob": -0.1417358232581097, "compression_ratio": 1.6492890995260663, "no_speech_prob": 1.4367496987688355e-06}, {"id": 717, "seek": 480000, "start": 4808.6, "end": 4816.54, "text": " 80. And then finally a linear layer which takes it from 80 to 10, which is what we wanted.", "tokens": [4688, 13, 400, 550, 2721, 257, 8213, 4583, 597, 2516, 309, 490, 4688, 281, 1266, 11, 597, 307, 437, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.1417358232581097, "compression_ratio": 1.6492890995260663, "no_speech_prob": 1.4367496987688355e-06}, {"id": 718, "seek": 480000, "start": 4816.54, "end": 4822.6, "text": " So that's our most basic, you'd call this a fully convolutional network. So a fully", "tokens": [407, 300, 311, 527, 881, 3875, 11, 291, 1116, 818, 341, 257, 4498, 45216, 304, 3209, 13, 407, 257, 4498], "temperature": 0.0, "avg_logprob": -0.1417358232581097, "compression_ratio": 1.6492890995260663, "no_speech_prob": 1.4367496987688355e-06}, {"id": 719, "seek": 480000, "start": 4822.6, "end": 4828.24, "text": " convolutional network is something where every layer is convolutional except for the very", "tokens": [45216, 304, 3209, 307, 746, 689, 633, 4583, 307, 45216, 304, 3993, 337, 264, 588], "temperature": 0.0, "avg_logprob": -0.1417358232581097, "compression_ratio": 1.6492890995260663, "no_speech_prob": 1.4367496987688355e-06}, {"id": 720, "seek": 482824, "start": 4828.24, "end": 4832.32, "text": " last.", "tokens": [1036, 13], "temperature": 0.0, "avg_logprob": -0.17880215247472128, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.3709546919926652e-06}, {"id": 721, "seek": 482824, "start": 4832.32, "end": 4839.88, "text": " So again we can now go lr.find. And now in this case when I did lr.find, it went through", "tokens": [407, 797, 321, 393, 586, 352, 287, 81, 13, 35072, 13, 400, 586, 294, 341, 1389, 562, 286, 630, 287, 81, 13, 35072, 11, 309, 1437, 807], "temperature": 0.0, "avg_logprob": -0.17880215247472128, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.3709546919926652e-06}, {"id": 722, "seek": 482824, "start": 4839.88, "end": 4846.599999999999, "text": " the entire dataset and was still getting better. So in other words, even the default final", "tokens": [264, 2302, 28872, 293, 390, 920, 1242, 1101, 13, 407, 294, 661, 2283, 11, 754, 264, 7576, 2572], "temperature": 0.0, "avg_logprob": -0.17880215247472128, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.3709546919926652e-06}, {"id": 723, "seek": 482824, "start": 4846.599999999999, "end": 4851.04, "text": " learning rate it tries is 10, and even at that point it was still pretty much getting", "tokens": [2539, 3314, 309, 9898, 307, 1266, 11, 293, 754, 412, 300, 935, 309, 390, 920, 1238, 709, 1242], "temperature": 0.0, "avg_logprob": -0.17880215247472128, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.3709546919926652e-06}, {"id": 724, "seek": 482824, "start": 4851.04, "end": 4856.92, "text": " better. So you can always override the final learning rate by saying end lr equals. That", "tokens": [1101, 13, 407, 291, 393, 1009, 42321, 264, 2572, 2539, 3314, 538, 1566, 917, 287, 81, 6915, 13, 663], "temperature": 0.0, "avg_logprob": -0.17880215247472128, "compression_ratio": 1.6981132075471699, "no_speech_prob": 1.3709546919926652e-06}, {"id": 725, "seek": 485692, "start": 4856.92, "end": 4859.4, "text": " will get it to try more things.", "tokens": [486, 483, 309, 281, 853, 544, 721, 13], "temperature": 0.0, "avg_logprob": -0.19259708485704788, "compression_ratio": 1.5363636363636364, "no_speech_prob": 1.70618079664564e-06}, {"id": 726, "seek": 485692, "start": 4859.4, "end": 4868.32, "text": " So here is the learning rate finder. And so I picked 10 to the minus 1, trained that for", "tokens": [407, 510, 307, 264, 2539, 3314, 915, 260, 13, 400, 370, 286, 6183, 1266, 281, 264, 3175, 502, 11, 8895, 300, 337], "temperature": 0.0, "avg_logprob": -0.19259708485704788, "compression_ratio": 1.5363636363636364, "no_speech_prob": 1.70618079664564e-06}, {"id": 727, "seek": 485692, "start": 4868.32, "end": 4873.2, "text": " a while, and that's looking pretty good. So I tried it with a cycle length of 1, and it's", "tokens": [257, 1339, 11, 293, 300, 311, 1237, 1238, 665, 13, 407, 286, 3031, 309, 365, 257, 6586, 4641, 295, 502, 11, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.19259708485704788, "compression_ratio": 1.5363636363636364, "no_speech_prob": 1.70618079664564e-06}, {"id": 728, "seek": 485692, "start": 4873.2, "end": 4876.38, "text": " starting to flatten out at about 60%.", "tokens": [2891, 281, 24183, 484, 412, 466, 4060, 6856], "temperature": 0.0, "avg_logprob": -0.19259708485704788, "compression_ratio": 1.5363636363636364, "no_speech_prob": 1.70618079664564e-06}, {"id": 729, "seek": 485692, "start": 4876.38, "end": 4883.32, "text": " So you can see here the number of elements, the number of parameters I have here are 500,", "tokens": [407, 291, 393, 536, 510, 264, 1230, 295, 4959, 11, 264, 1230, 295, 9834, 286, 362, 510, 366, 5923, 11], "temperature": 0.0, "avg_logprob": -0.19259708485704788, "compression_ratio": 1.5363636363636364, "no_speech_prob": 1.70618079664564e-06}, {"id": 730, "seek": 488332, "start": 4883.32, "end": 4891.32, "text": " 7000, 28000, about 30,000. So I have about a quarter of the number of parameters, but", "tokens": [1614, 1360, 11, 7562, 1360, 11, 466, 2217, 11, 1360, 13, 407, 286, 362, 466, 257, 6555, 295, 264, 1230, 295, 9834, 11, 457], "temperature": 0.0, "avg_logprob": -0.11103869020269158, "compression_ratio": 1.536231884057971, "no_speech_prob": 1.5294106106011895e-06}, {"id": 731, "seek": 488332, "start": 4891.32, "end": 4901.759999999999, "text": " my accuracy has gone up from 47% to 60%. And the time per epoch here is under 30 seconds,", "tokens": [452, 14170, 575, 2780, 493, 490, 16953, 4, 281, 4060, 6856, 400, 264, 565, 680, 30992, 339, 510, 307, 833, 2217, 3949, 11], "temperature": 0.0, "avg_logprob": -0.11103869020269158, "compression_ratio": 1.536231884057971, "no_speech_prob": 1.5294106106011895e-06}, {"id": 732, "seek": 488332, "start": 4901.759999999999, "end": 4904.799999999999, "text": " and here also. So the time per epoch is about the same.", "tokens": [293, 510, 611, 13, 407, 264, 565, 680, 30992, 339, 307, 466, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.11103869020269158, "compression_ratio": 1.536231884057971, "no_speech_prob": 1.5294106106011895e-06}, {"id": 733, "seek": 488332, "start": 4904.799999999999, "end": 4909.08, "text": " And that's not surprising because when you use small simple architectures, most of the", "tokens": [400, 300, 311, 406, 8830, 570, 562, 291, 764, 1359, 2199, 6331, 1303, 11, 881, 295, 264], "temperature": 0.0, "avg_logprob": -0.11103869020269158, "compression_ratio": 1.536231884057971, "no_speech_prob": 1.5294106106011895e-06}, {"id": 734, "seek": 490908, "start": 4909.08, "end": 4917.84, "text": " time is the memory transfer, the actual time during the compute is trivial.", "tokens": [565, 307, 264, 4675, 5003, 11, 264, 3539, 565, 1830, 264, 14722, 307, 26703, 13], "temperature": 0.0, "avg_logprob": -0.13405993927356807, "compression_ratio": 1.6761904761904762, "no_speech_prob": 1.0129971997230314e-05}, {"id": 735, "seek": 490908, "start": 4917.84, "end": 4924.08, "text": " So I'm going to refactor this slightly because I want to try and put less stuff inside my", "tokens": [407, 286, 478, 516, 281, 1895, 15104, 341, 4748, 570, 286, 528, 281, 853, 293, 829, 1570, 1507, 1854, 452], "temperature": 0.0, "avg_logprob": -0.13405993927356807, "compression_ratio": 1.6761904761904762, "no_speech_prob": 1.0129971997230314e-05}, {"id": 736, "seek": 490908, "start": 4924.08, "end": 4931.22, "text": " forward. And so calling relu every time doesn't seem ideal. So I'm going to create a new class", "tokens": [2128, 13, 400, 370, 5141, 1039, 84, 633, 565, 1177, 380, 1643, 7157, 13, 407, 286, 478, 516, 281, 1884, 257, 777, 1508], "temperature": 0.0, "avg_logprob": -0.13405993927356807, "compression_ratio": 1.6761904761904762, "no_speech_prob": 1.0129971997230314e-05}, {"id": 737, "seek": 490908, "start": 4931.22, "end": 4938.04, "text": " called conv layer. And the conv layer class is going to contain a convolution with a kernel", "tokens": [1219, 3754, 4583, 13, 400, 264, 3754, 4583, 1508, 307, 516, 281, 5304, 257, 45216, 365, 257, 28256], "temperature": 0.0, "avg_logprob": -0.13405993927356807, "compression_ratio": 1.6761904761904762, "no_speech_prob": 1.0129971997230314e-05}, {"id": 738, "seek": 493804, "start": 4938.04, "end": 4940.98, "text": " size of 3 and a stride of 2.", "tokens": [2744, 295, 805, 293, 257, 1056, 482, 295, 568, 13], "temperature": 0.0, "avg_logprob": -0.10534913680132697, "compression_ratio": 1.5502645502645502, "no_speech_prob": 6.748029136360856e-06}, {"id": 739, "seek": 493804, "start": 4940.98, "end": 4945.44, "text": " One thing I'm going to do now is I'm going to add padding. Did you notice here the first", "tokens": [1485, 551, 286, 478, 516, 281, 360, 586, 307, 286, 478, 516, 281, 909, 39562, 13, 2589, 291, 3449, 510, 264, 700], "temperature": 0.0, "avg_logprob": -0.10534913680132697, "compression_ratio": 1.5502645502645502, "no_speech_prob": 6.748029136360856e-06}, {"id": 740, "seek": 493804, "start": 4945.44, "end": 4953.74, "text": " layer went from 32x32 to 15x15, not 16x16. And the reason for that is that at the very", "tokens": [4583, 1437, 490, 8858, 87, 11440, 281, 2119, 87, 5211, 11, 406, 3165, 87, 6866, 13, 400, 264, 1778, 337, 300, 307, 300, 412, 264, 588], "temperature": 0.0, "avg_logprob": -0.10534913680132697, "compression_ratio": 1.5502645502645502, "no_speech_prob": 6.748029136360856e-06}, {"id": 741, "seek": 493804, "start": 4953.74, "end": 4966.64, "text": " edge of your convolution here, see how this first convolution, there isn't a convolution", "tokens": [4691, 295, 428, 45216, 510, 11, 536, 577, 341, 700, 45216, 11, 456, 1943, 380, 257, 45216], "temperature": 0.0, "avg_logprob": -0.10534913680132697, "compression_ratio": 1.5502645502645502, "no_speech_prob": 6.748029136360856e-06}, {"id": 742, "seek": 496664, "start": 4966.64, "end": 4972.84, "text": " where the middle is the top left point, because there's nothing outside it.", "tokens": [689, 264, 2808, 307, 264, 1192, 1411, 935, 11, 570, 456, 311, 1825, 2380, 309, 13], "temperature": 0.0, "avg_logprob": -0.14152731520406317, "compression_ratio": 1.6517412935323383, "no_speech_prob": 4.2893175304925535e-06}, {"id": 743, "seek": 496664, "start": 4972.84, "end": 4978.200000000001, "text": " Where else if we had put a row of zeros at the top and a row of zeros at the edge of", "tokens": [2305, 1646, 498, 321, 632, 829, 257, 5386, 295, 35193, 412, 264, 1192, 293, 257, 5386, 295, 35193, 412, 264, 4691, 295], "temperature": 0.0, "avg_logprob": -0.14152731520406317, "compression_ratio": 1.6517412935323383, "no_speech_prob": 4.2893175304925535e-06}, {"id": 744, "seek": 496664, "start": 4978.200000000001, "end": 4988.34, "text": " each column, we now could go all the way to the edge. So pad equals 1 adds that little", "tokens": [1184, 7738, 11, 321, 586, 727, 352, 439, 264, 636, 281, 264, 4691, 13, 407, 6887, 6915, 502, 10860, 300, 707], "temperature": 0.0, "avg_logprob": -0.14152731520406317, "compression_ratio": 1.6517412935323383, "no_speech_prob": 4.2893175304925535e-06}, {"id": 745, "seek": 496664, "start": 4988.34, "end": 4992.96, "text": " layer of zeros around the edge for us. And so this way we're going to make sure that", "tokens": [4583, 295, 35193, 926, 264, 4691, 337, 505, 13, 400, 370, 341, 636, 321, 434, 516, 281, 652, 988, 300], "temperature": 0.0, "avg_logprob": -0.14152731520406317, "compression_ratio": 1.6517412935323383, "no_speech_prob": 4.2893175304925535e-06}, {"id": 746, "seek": 499296, "start": 4992.96, "end": 4999.8, "text": " we go 32x32 to 16x16 to 8x8. It doesn't matter too much when you've got these bigger layers,", "tokens": [321, 352, 8858, 87, 11440, 281, 3165, 87, 6866, 281, 1649, 87, 23, 13, 467, 1177, 380, 1871, 886, 709, 562, 291, 600, 658, 613, 3801, 7914, 11], "temperature": 0.0, "avg_logprob": -0.1497535536774492, "compression_ratio": 1.536, "no_speech_prob": 1.5779594377818285e-06}, {"id": 747, "seek": 499296, "start": 4999.8, "end": 5006.6, "text": " but by the time you get down to 4x4, you really don't want to throw away a whole piece. So", "tokens": [457, 538, 264, 565, 291, 483, 760, 281, 1017, 87, 19, 11, 291, 534, 500, 380, 528, 281, 3507, 1314, 257, 1379, 2522, 13, 407], "temperature": 0.0, "avg_logprob": -0.1497535536774492, "compression_ratio": 1.536, "no_speech_prob": 1.5779594377818285e-06}, {"id": 748, "seek": 499296, "start": 5006.6, "end": 5008.58, "text": " the padding becomes important.", "tokens": [264, 39562, 3643, 1021, 13], "temperature": 0.0, "avg_logprob": -0.1497535536774492, "compression_ratio": 1.536, "no_speech_prob": 1.5779594377818285e-06}, {"id": 749, "seek": 499296, "start": 5008.58, "end": 5014.44, "text": " So by refactoring it to put this with its defaults here, and then in the forward I'll", "tokens": [407, 538, 1895, 578, 3662, 309, 281, 829, 341, 365, 1080, 7576, 82, 510, 11, 293, 550, 294, 264, 2128, 286, 603], "temperature": 0.0, "avg_logprob": -0.1497535536774492, "compression_ratio": 1.536, "no_speech_prob": 1.5779594377818285e-06}, {"id": 750, "seek": 499296, "start": 5014.44, "end": 5021.0, "text": " put the relu in here as well, it makes my conv net a little bit smaller and more to", "tokens": [829, 264, 1039, 84, 294, 510, 382, 731, 11, 309, 1669, 452, 3754, 2533, 257, 707, 857, 4356, 293, 544, 281], "temperature": 0.0, "avg_logprob": -0.1497535536774492, "compression_ratio": 1.536, "no_speech_prob": 1.5779594377818285e-06}, {"id": 751, "seek": 502100, "start": 5021.0, "end": 5024.52, "text": " the point, it's going to be easier for me to make sure that everything is correct in", "tokens": [264, 935, 11, 309, 311, 516, 281, 312, 3571, 337, 385, 281, 652, 988, 300, 1203, 307, 3006, 294], "temperature": 0.0, "avg_logprob": -0.1345835893050484, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.295916601724457e-06}, {"id": 752, "seek": 502100, "start": 5024.52, "end": 5030.24, "text": " the future by always using this conv layer class. So now you know not only how to create", "tokens": [264, 2027, 538, 1009, 1228, 341, 3754, 4583, 1508, 13, 407, 586, 291, 458, 406, 787, 577, 281, 1884], "temperature": 0.0, "avg_logprob": -0.1345835893050484, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.295916601724457e-06}, {"id": 753, "seek": 502100, "start": 5030.24, "end": 5035.72, "text": " your own neural network model, but how to create your own neural network layer. So here", "tokens": [428, 1065, 18161, 3209, 2316, 11, 457, 577, 281, 1884, 428, 1065, 18161, 3209, 4583, 13, 407, 510], "temperature": 0.0, "avg_logprob": -0.1345835893050484, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.295916601724457e-06}, {"id": 754, "seek": 502100, "start": 5035.72, "end": 5038.84, "text": " now I can use conv layer.", "tokens": [586, 286, 393, 764, 3754, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1345835893050484, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.295916601724457e-06}, {"id": 755, "seek": 502100, "start": 5038.84, "end": 5043.7, "text": " And this is such a cool thing about PyTorch, is a layer definition and a neural network", "tokens": [400, 341, 307, 1270, 257, 1627, 551, 466, 9953, 51, 284, 339, 11, 307, 257, 4583, 7123, 293, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.1345835893050484, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.295916601724457e-06}, {"id": 756, "seek": 502100, "start": 5043.7, "end": 5049.9, "text": " definition are literally identical. They both have a constructor and a forward. And so any", "tokens": [7123, 366, 3736, 14800, 13, 814, 1293, 362, 257, 47479, 293, 257, 2128, 13, 400, 370, 604], "temperature": 0.0, "avg_logprob": -0.1345835893050484, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.295916601724457e-06}, {"id": 757, "seek": 504990, "start": 5049.9, "end": 5053.599999999999, "text": " time you've got the layer, you can use it as a neural net. Any time you have a neural", "tokens": [565, 291, 600, 658, 264, 4583, 11, 291, 393, 764, 309, 382, 257, 18161, 2533, 13, 2639, 565, 291, 362, 257, 18161], "temperature": 0.0, "avg_logprob": -0.1574590902412887, "compression_ratio": 1.6822033898305084, "no_speech_prob": 7.4112590482400265e-06}, {"id": 758, "seek": 504990, "start": 5053.599999999999, "end": 5056.82, "text": " net, you can use it as a layer.", "tokens": [2533, 11, 291, 393, 764, 309, 382, 257, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1574590902412887, "compression_ratio": 1.6822033898305084, "no_speech_prob": 7.4112590482400265e-06}, {"id": 759, "seek": 504990, "start": 5056.82, "end": 5063.4, "text": " So this is now the exact same thing as we had before. One difference is I now have padding.", "tokens": [407, 341, 307, 586, 264, 1900, 912, 551, 382, 321, 632, 949, 13, 1485, 2649, 307, 286, 586, 362, 39562, 13], "temperature": 0.0, "avg_logprob": -0.1574590902412887, "compression_ratio": 1.6822033898305084, "no_speech_prob": 7.4112590482400265e-06}, {"id": 760, "seek": 504990, "start": 5063.4, "end": 5068.98, "text": " And another thing just to show you, you can do things differently. Back here, my max pool", "tokens": [400, 1071, 551, 445, 281, 855, 291, 11, 291, 393, 360, 721, 7614, 13, 5833, 510, 11, 452, 11469, 7005], "temperature": 0.0, "avg_logprob": -0.1574590902412887, "compression_ratio": 1.6822033898305084, "no_speech_prob": 7.4112590482400265e-06}, {"id": 761, "seek": 504990, "start": 5068.98, "end": 5076.98, "text": " I did as an object, I used the class nn.adaptiveMaxPool and I stuck it in this attribute and then", "tokens": [286, 630, 382, 364, 2657, 11, 286, 1143, 264, 1508, 297, 77, 13, 345, 2796, 488, 36025, 47, 1092, 293, 286, 5541, 309, 294, 341, 19667, 293, 550], "temperature": 0.0, "avg_logprob": -0.1574590902412887, "compression_ratio": 1.6822033898305084, "no_speech_prob": 7.4112590482400265e-06}, {"id": 762, "seek": 507698, "start": 5076.98, "end": 5084.0, "text": " I called it. But this actually doesn't have any state. There's no weights inside max pooling.", "tokens": [286, 1219, 309, 13, 583, 341, 767, 1177, 380, 362, 604, 1785, 13, 821, 311, 572, 17443, 1854, 11469, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.11716335510539118, "compression_ratio": 1.6595744680851063, "no_speech_prob": 3.500846787574119e-06}, {"id": 763, "seek": 507698, "start": 5084.0, "end": 5089.0199999999995, "text": " So I can actually do it with a little bit less code by calling it as a function. So", "tokens": [407, 286, 393, 767, 360, 309, 365, 257, 707, 857, 1570, 3089, 538, 5141, 309, 382, 257, 2445, 13, 407], "temperature": 0.0, "avg_logprob": -0.11716335510539118, "compression_ratio": 1.6595744680851063, "no_speech_prob": 3.500846787574119e-06}, {"id": 764, "seek": 507698, "start": 5089.0199999999995, "end": 5092.759999999999, "text": " everything that you can do as a class, you can also do as a function, it's inside this", "tokens": [1203, 300, 291, 393, 360, 382, 257, 1508, 11, 291, 393, 611, 360, 382, 257, 2445, 11, 309, 311, 1854, 341], "temperature": 0.0, "avg_logprob": -0.11716335510539118, "compression_ratio": 1.6595744680851063, "no_speech_prob": 3.500846787574119e-06}, {"id": 765, "seek": 507698, "start": 5092.759999999999, "end": 5098.2, "text": " capital F, which is nn.functional.", "tokens": [4238, 479, 11, 597, 307, 297, 77, 13, 22845, 304, 13], "temperature": 0.0, "avg_logprob": -0.11716335510539118, "compression_ratio": 1.6595744680851063, "no_speech_prob": 3.500846787574119e-06}, {"id": 766, "seek": 507698, "start": 5098.2, "end": 5105.78, "text": " So this should be a tiny bit better because this time I've got the padding. I didn't train", "tokens": [407, 341, 820, 312, 257, 5870, 857, 1101, 570, 341, 565, 286, 600, 658, 264, 39562, 13, 286, 994, 380, 3847], "temperature": 0.0, "avg_logprob": -0.11716335510539118, "compression_ratio": 1.6595744680851063, "no_speech_prob": 3.500846787574119e-06}, {"id": 767, "seek": 510578, "start": 5105.78, "end": 5113.639999999999, "text": " it for as long to actually check. So let's skip over that.", "tokens": [309, 337, 382, 938, 281, 767, 1520, 13, 407, 718, 311, 10023, 670, 300, 13], "temperature": 0.0, "avg_logprob": -0.16094426675276322, "compression_ratio": 1.7098445595854923, "no_speech_prob": 2.5612773697503144e-06}, {"id": 768, "seek": 510578, "start": 5113.639999999999, "end": 5123.28, "text": " So one issue here is that in the end, when I tried to add more layers, I had trouble", "tokens": [407, 472, 2734, 510, 307, 300, 294, 264, 917, 11, 562, 286, 3031, 281, 909, 544, 7914, 11, 286, 632, 5253], "temperature": 0.0, "avg_logprob": -0.16094426675276322, "compression_ratio": 1.7098445595854923, "no_speech_prob": 2.5612773697503144e-06}, {"id": 769, "seek": 510578, "start": 5123.28, "end": 5130.82, "text": " training it. And the reason I was having trouble training it is if I used larger learning rates,", "tokens": [3097, 309, 13, 400, 264, 1778, 286, 390, 1419, 5253, 3097, 309, 307, 498, 286, 1143, 4833, 2539, 6846, 11], "temperature": 0.0, "avg_logprob": -0.16094426675276322, "compression_ratio": 1.7098445595854923, "no_speech_prob": 2.5612773697503144e-06}, {"id": 770, "seek": 510578, "start": 5130.82, "end": 5135.32, "text": " it would go off to nn. And if I used smaller learning rates, it kind of takes forever and", "tokens": [309, 576, 352, 766, 281, 297, 77, 13, 400, 498, 286, 1143, 4356, 2539, 6846, 11, 309, 733, 295, 2516, 5680, 293], "temperature": 0.0, "avg_logprob": -0.16094426675276322, "compression_ratio": 1.7098445595854923, "no_speech_prob": 2.5612773697503144e-06}, {"id": 771, "seek": 513532, "start": 5135.32, "end": 5140.42, "text": " doesn't really have a chance to explore properly. So it wasn't resilient.", "tokens": [1177, 380, 534, 362, 257, 2931, 281, 6839, 6108, 13, 407, 309, 2067, 380, 23699, 13], "temperature": 0.0, "avg_logprob": -0.12787123680114745, "compression_ratio": 1.716, "no_speech_prob": 2.1568134798144456e-06}, {"id": 772, "seek": 513532, "start": 5140.42, "end": 5145.86, "text": " So to make my model more resilient, I'm going to use something called batch normalization,", "tokens": [407, 281, 652, 452, 2316, 544, 23699, 11, 286, 478, 516, 281, 764, 746, 1219, 15245, 2710, 2144, 11], "temperature": 0.0, "avg_logprob": -0.12787123680114745, "compression_ratio": 1.716, "no_speech_prob": 2.1568134798144456e-06}, {"id": 773, "seek": 513532, "start": 5145.86, "end": 5153.38, "text": " which literally everybody calls batch norm. And batch norm is a couple of years old now,", "tokens": [597, 3736, 2201, 5498, 15245, 2026, 13, 400, 15245, 2026, 307, 257, 1916, 295, 924, 1331, 586, 11], "temperature": 0.0, "avg_logprob": -0.12787123680114745, "compression_ratio": 1.716, "no_speech_prob": 2.1568134798144456e-06}, {"id": 774, "seek": 513532, "start": 5153.38, "end": 5157.46, "text": " and it's been pretty transformative since it came along, because it suddenly makes it", "tokens": [293, 309, 311, 668, 1238, 36070, 1670, 309, 1361, 2051, 11, 570, 309, 5800, 1669, 309], "temperature": 0.0, "avg_logprob": -0.12787123680114745, "compression_ratio": 1.716, "no_speech_prob": 2.1568134798144456e-06}, {"id": 775, "seek": 513532, "start": 5157.46, "end": 5164.44, "text": " really easy to train deeper networks. So the network I'm going to create is going to have", "tokens": [534, 1858, 281, 3847, 7731, 9590, 13, 407, 264, 3209, 286, 478, 516, 281, 1884, 307, 516, 281, 362], "temperature": 0.0, "avg_logprob": -0.12787123680114745, "compression_ratio": 1.716, "no_speech_prob": 2.1568134798144456e-06}, {"id": 776, "seek": 516444, "start": 5164.44, "end": 5171.44, "text": " more layers. I've got 1, 2, 3, 4, 5 convolutional layers plus a fully connected layer. So back", "tokens": [544, 7914, 13, 286, 600, 658, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 45216, 304, 7914, 1804, 257, 4498, 4582, 4583, 13, 407, 646], "temperature": 0.0, "avg_logprob": -0.1745005588905484, "compression_ratio": 1.593073593073593, "no_speech_prob": 1.8738726339506684e-06}, {"id": 777, "seek": 516444, "start": 5171.44, "end": 5175.379999999999, "text": " in the old days, that would be considered a pretty deep network and we would consider", "tokens": [294, 264, 1331, 1708, 11, 300, 576, 312, 4888, 257, 1238, 2452, 3209, 293, 321, 576, 1949], "temperature": 0.0, "avg_logprob": -0.1745005588905484, "compression_ratio": 1.593073593073593, "no_speech_prob": 1.8738726339506684e-06}, {"id": 778, "seek": 516444, "start": 5175.379999999999, "end": 5180.74, "text": " pretty hard to train. Nowadays it's super simple thanks to batch norm.", "tokens": [1238, 1152, 281, 3847, 13, 28908, 309, 311, 1687, 2199, 3231, 281, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.1745005588905484, "compression_ratio": 1.593073593073593, "no_speech_prob": 1.8738726339506684e-06}, {"id": 779, "seek": 516444, "start": 5180.74, "end": 5185.82, "text": " Now to use batch norm, you can just write nn.batchnorm. But to learn about it, we're", "tokens": [823, 281, 764, 15245, 2026, 11, 291, 393, 445, 2464, 297, 77, 13, 65, 852, 13403, 13, 583, 281, 1466, 466, 309, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.1745005588905484, "compression_ratio": 1.593073593073593, "no_speech_prob": 1.8738726339506684e-06}, {"id": 780, "seek": 516444, "start": 5185.82, "end": 5188.299999999999, "text": " going to write it from scratch.", "tokens": [516, 281, 2464, 309, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1745005588905484, "compression_ratio": 1.593073593073593, "no_speech_prob": 1.8738726339506684e-06}, {"id": 781, "seek": 518830, "start": 5188.3, "end": 5196.74, "text": " So the basic idea of batch norm is that we've got some vector of activations. Any time I", "tokens": [407, 264, 3875, 1558, 295, 15245, 2026, 307, 300, 321, 600, 658, 512, 8062, 295, 2430, 763, 13, 2639, 565, 286], "temperature": 0.0, "avg_logprob": -0.13888905359351117, "compression_ratio": 1.7524752475247525, "no_speech_prob": 5.093691470392514e-06}, {"id": 782, "seek": 518830, "start": 5196.74, "end": 5200.3, "text": " draw a vector of activations, obviously I mean you can repeat it for the mini-batch,", "tokens": [2642, 257, 8062, 295, 2430, 763, 11, 2745, 286, 914, 291, 393, 7149, 309, 337, 264, 8382, 12, 65, 852, 11], "temperature": 0.0, "avg_logprob": -0.13888905359351117, "compression_ratio": 1.7524752475247525, "no_speech_prob": 5.093691470392514e-06}, {"id": 783, "seek": 518830, "start": 5200.3, "end": 5205.66, "text": " so pretend it's a mini-batch of 1. So we've got some vector of activations and it's coming", "tokens": [370, 11865, 309, 311, 257, 8382, 12, 65, 852, 295, 502, 13, 407, 321, 600, 658, 512, 8062, 295, 2430, 763, 293, 309, 311, 1348], "temperature": 0.0, "avg_logprob": -0.13888905359351117, "compression_ratio": 1.7524752475247525, "no_speech_prob": 5.093691470392514e-06}, {"id": 784, "seek": 518830, "start": 5205.66, "end": 5213.02, "text": " into some layer, so probably some convolutional matrix multiplication. And then something", "tokens": [666, 512, 4583, 11, 370, 1391, 512, 45216, 304, 8141, 27290, 13, 400, 550, 746], "temperature": 0.0, "avg_logprob": -0.13888905359351117, "compression_ratio": 1.7524752475247525, "no_speech_prob": 5.093691470392514e-06}, {"id": 785, "seek": 521302, "start": 5213.02, "end": 5221.900000000001, "text": " comes out the other side. So imagine this is just a matrix multiply, which was like,", "tokens": [1487, 484, 264, 661, 1252, 13, 407, 3811, 341, 307, 445, 257, 8141, 12972, 11, 597, 390, 411, 11], "temperature": 0.0, "avg_logprob": -0.1732641806969276, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.2679275869231788e-06}, {"id": 786, "seek": 521302, "start": 5221.900000000001, "end": 5233.3, "text": " say it was an identity matrix. Then every time I multiply it by that across lots and", "tokens": [584, 309, 390, 364, 6575, 8141, 13, 1396, 633, 565, 286, 12972, 309, 538, 300, 2108, 3195, 293], "temperature": 0.0, "avg_logprob": -0.1732641806969276, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.2679275869231788e-06}, {"id": 787, "seek": 521302, "start": 5233.3, "end": 5237.5, "text": " lots of layers, my activations are not getting bigger, they're not getting smaller, they're", "tokens": [3195, 295, 7914, 11, 452, 2430, 763, 366, 406, 1242, 3801, 11, 436, 434, 406, 1242, 4356, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.1732641806969276, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.2679275869231788e-06}, {"id": 788, "seek": 523750, "start": 5237.5, "end": 5247.26, "text": " not changing at all. That's all fine. But imagine if it was actually like 2, 2, 2. And", "tokens": [406, 4473, 412, 439, 13, 663, 311, 439, 2489, 13, 583, 3811, 498, 309, 390, 767, 411, 568, 11, 568, 11, 568, 13, 400], "temperature": 0.0, "avg_logprob": -0.17945271322171982, "compression_ratio": 1.4550264550264551, "no_speech_prob": 3.747991002001072e-07}, {"id": 789, "seek": 523750, "start": 5247.26, "end": 5252.7, "text": " so if every one of my weight matrices or filters was like that, then my activations are doubling", "tokens": [370, 498, 633, 472, 295, 452, 3364, 32284, 420, 15995, 390, 411, 300, 11, 550, 452, 2430, 763, 366, 33651], "temperature": 0.0, "avg_logprob": -0.17945271322171982, "compression_ratio": 1.4550264550264551, "no_speech_prob": 3.747991002001072e-07}, {"id": 790, "seek": 523750, "start": 5252.7, "end": 5261.1, "text": " each time. And so suddenly I've got exponential growth. And in deep models, that's going to", "tokens": [1184, 565, 13, 400, 370, 5800, 286, 600, 658, 21510, 4599, 13, 400, 294, 2452, 5245, 11, 300, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.17945271322171982, "compression_ratio": 1.4550264550264551, "no_speech_prob": 3.747991002001072e-07}, {"id": 791, "seek": 526110, "start": 5261.1, "end": 5268.38, "text": " be a disaster because my gradients are exploding at an exponential rate. And so the challenge", "tokens": [312, 257, 11293, 570, 452, 2771, 2448, 366, 35175, 412, 364, 21510, 3314, 13, 400, 370, 264, 3430], "temperature": 0.0, "avg_logprob": -0.10990853957187982, "compression_ratio": 1.6898148148148149, "no_speech_prob": 2.99443627227447e-06}, {"id": 792, "seek": 526110, "start": 5268.38, "end": 5277.620000000001, "text": " you have is that it's very unlikely, unless you try carefully to deal with it, that your", "tokens": [291, 362, 307, 300, 309, 311, 588, 17518, 11, 5969, 291, 853, 7500, 281, 2028, 365, 309, 11, 300, 428], "temperature": 0.0, "avg_logprob": -0.10990853957187982, "compression_ratio": 1.6898148148148149, "no_speech_prob": 2.99443627227447e-06}, {"id": 793, "seek": 526110, "start": 5277.620000000001, "end": 5283.860000000001, "text": " weight matrices on average are not going to cause your activations to keep getting smaller", "tokens": [3364, 32284, 322, 4274, 366, 406, 516, 281, 3082, 428, 2430, 763, 281, 1066, 1242, 4356], "temperature": 0.0, "avg_logprob": -0.10990853957187982, "compression_ratio": 1.6898148148148149, "no_speech_prob": 2.99443627227447e-06}, {"id": 794, "seek": 526110, "start": 5283.860000000001, "end": 5288.740000000001, "text": " and smaller or keep getting bigger and bigger. You have to kind of carefully control things", "tokens": [293, 4356, 420, 1066, 1242, 3801, 293, 3801, 13, 509, 362, 281, 733, 295, 7500, 1969, 721], "temperature": 0.0, "avg_logprob": -0.10990853957187982, "compression_ratio": 1.6898148148148149, "no_speech_prob": 2.99443627227447e-06}, {"id": 795, "seek": 528874, "start": 5288.74, "end": 5295.9, "text": " to make sure that they stay at a reasonable size. You want to keep them at a reasonable", "tokens": [281, 652, 988, 300, 436, 1754, 412, 257, 10585, 2744, 13, 509, 528, 281, 1066, 552, 412, 257, 10585], "temperature": 0.0, "avg_logprob": -0.19240293237898085, "compression_ratio": 1.5056179775280898, "no_speech_prob": 1.5294111790353782e-06}, {"id": 796, "seek": 528874, "start": 5295.9, "end": 5296.9, "text": " scale.", "tokens": [4373, 13], "temperature": 0.0, "avg_logprob": -0.19240293237898085, "compression_ratio": 1.5056179775280898, "no_speech_prob": 1.5294111790353782e-06}, {"id": 797, "seek": 528874, "start": 5296.9, "end": 5303.42, "text": " So we start things off with 0 mean, standard deviation 1 by normalizing the inputs. But", "tokens": [407, 321, 722, 721, 766, 365, 1958, 914, 11, 3832, 25163, 502, 538, 2710, 3319, 264, 15743, 13, 583], "temperature": 0.0, "avg_logprob": -0.19240293237898085, "compression_ratio": 1.5056179775280898, "no_speech_prob": 1.5294111790353782e-06}, {"id": 798, "seek": 528874, "start": 5303.42, "end": 5312.74, "text": " what we'd really like to do is to normalize every layer, not just the inputs. And so,", "tokens": [437, 321, 1116, 534, 411, 281, 360, 307, 281, 2710, 1125, 633, 4583, 11, 406, 445, 264, 15743, 13, 400, 370, 11], "temperature": 0.0, "avg_logprob": -0.19240293237898085, "compression_ratio": 1.5056179775280898, "no_speech_prob": 1.5294111790353782e-06}, {"id": 799, "seek": 531274, "start": 5312.74, "end": 5320.82, "text": " okay fine, let's do that. So here I've created a BN layer which is exactly like my Conv layer.", "tokens": [1392, 2489, 11, 718, 311, 360, 300, 13, 407, 510, 286, 600, 2942, 257, 363, 45, 4583, 597, 307, 2293, 411, 452, 2656, 85, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1664588009869611, "compression_ratio": 1.5307262569832403, "no_speech_prob": 3.3405351587134646e-06}, {"id": 800, "seek": 531274, "start": 5320.82, "end": 5330.0599999999995, "text": " It's got my Conv2D with my stride, my padding. I do my Conv and my ReLU. And then I calculate", "tokens": [467, 311, 658, 452, 2656, 85, 17, 35, 365, 452, 1056, 482, 11, 452, 39562, 13, 286, 360, 452, 2656, 85, 293, 452, 1300, 43, 52, 13, 400, 550, 286, 8873], "temperature": 0.0, "avg_logprob": -0.1664588009869611, "compression_ratio": 1.5307262569832403, "no_speech_prob": 3.3405351587134646e-06}, {"id": 801, "seek": 531274, "start": 5330.0599999999995, "end": 5337.38, "text": " the mean of each channel or of each filter and the standard deviation of each channel", "tokens": [264, 914, 295, 1184, 2269, 420, 295, 1184, 6608, 293, 264, 3832, 25163, 295, 1184, 2269], "temperature": 0.0, "avg_logprob": -0.1664588009869611, "compression_ratio": 1.5307262569832403, "no_speech_prob": 3.3405351587134646e-06}, {"id": 802, "seek": 533738, "start": 5337.38, "end": 5345.62, "text": " or each filter. And then I subtract the means and divide by the standard deviations. So", "tokens": [420, 1184, 6608, 13, 400, 550, 286, 16390, 264, 1355, 293, 9845, 538, 264, 3832, 31219, 763, 13, 407], "temperature": 0.0, "avg_logprob": -0.11019526509677663, "compression_ratio": 1.5344827586206897, "no_speech_prob": 3.3596782600398e-07}, {"id": 803, "seek": 533738, "start": 5345.62, "end": 5350.78, "text": " now I don't actually need to normalize my input at all, because it's actually going", "tokens": [586, 286, 500, 380, 767, 643, 281, 2710, 1125, 452, 4846, 412, 439, 11, 570, 309, 311, 767, 516], "temperature": 0.0, "avg_logprob": -0.11019526509677663, "compression_ratio": 1.5344827586206897, "no_speech_prob": 3.3596782600398e-07}, {"id": 804, "seek": 533738, "start": 5350.78, "end": 5357.58, "text": " to do it automatically. It's normalizing it per channel. And for later layers, it's normalizing", "tokens": [281, 360, 309, 6772, 13, 467, 311, 2710, 3319, 309, 680, 2269, 13, 400, 337, 1780, 7914, 11, 309, 311, 2710, 3319], "temperature": 0.0, "avg_logprob": -0.11019526509677663, "compression_ratio": 1.5344827586206897, "no_speech_prob": 3.3596782600398e-07}, {"id": 805, "seek": 535758, "start": 5357.58, "end": 5370.0199999999995, "text": " it per filter. So it turns out that's not enough, because SGD is bloody minded. And", "tokens": [309, 680, 6608, 13, 407, 309, 4523, 484, 300, 311, 406, 1547, 11, 570, 34520, 35, 307, 18938, 36707, 13, 400], "temperature": 0.0, "avg_logprob": -0.14470669627189636, "compression_ratio": 1.458100558659218, "no_speech_prob": 9.276359946852608e-07}, {"id": 806, "seek": 535758, "start": 5370.0199999999995, "end": 5377.5, "text": " so if SGD decided that it wants the weight matrix to be like so, where that matrix is", "tokens": [370, 498, 34520, 35, 3047, 300, 309, 2738, 264, 3364, 8141, 281, 312, 411, 370, 11, 689, 300, 8141, 307], "temperature": 0.0, "avg_logprob": -0.14470669627189636, "compression_ratio": 1.458100558659218, "no_speech_prob": 9.276359946852608e-07}, {"id": 807, "seek": 535758, "start": 5377.5, "end": 5386.46, "text": " something which is going to increase the values overall repeatedly, then trying to subtract", "tokens": [746, 597, 307, 516, 281, 3488, 264, 4190, 4787, 18227, 11, 550, 1382, 281, 16390], "temperature": 0.0, "avg_logprob": -0.14470669627189636, "compression_ratio": 1.458100558659218, "no_speech_prob": 9.276359946852608e-07}, {"id": 808, "seek": 538646, "start": 5386.46, "end": 5390.78, "text": " the means and divide by the standard deviations just means the next mini-batch is going to", "tokens": [264, 1355, 293, 9845, 538, 264, 3832, 31219, 763, 445, 1355, 264, 958, 8382, 12, 65, 852, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.1405972162882487, "compression_ratio": 1.8571428571428572, "no_speech_prob": 3.446572009124793e-06}, {"id": 809, "seek": 538646, "start": 5390.78, "end": 5394.46, "text": " try and do it again, and it'll try and do it again, and it'll try and do it again. So", "tokens": [853, 293, 360, 309, 797, 11, 293, 309, 603, 853, 293, 360, 309, 797, 11, 293, 309, 603, 853, 293, 360, 309, 797, 13, 407], "temperature": 0.0, "avg_logprob": -0.1405972162882487, "compression_ratio": 1.8571428571428572, "no_speech_prob": 3.446572009124793e-06}, {"id": 810, "seek": 538646, "start": 5394.46, "end": 5400.74, "text": " it turns out that this actually doesn't help. Like it literally does nothing, because SGD", "tokens": [309, 4523, 484, 300, 341, 767, 1177, 380, 854, 13, 1743, 309, 3736, 775, 1825, 11, 570, 34520, 35], "temperature": 0.0, "avg_logprob": -0.1405972162882487, "compression_ratio": 1.8571428571428572, "no_speech_prob": 3.446572009124793e-06}, {"id": 811, "seek": 538646, "start": 5400.74, "end": 5406.3, "text": " is just going to go ahead and undo it the next mini-batch.", "tokens": [307, 445, 516, 281, 352, 2286, 293, 23779, 309, 264, 958, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.1405972162882487, "compression_ratio": 1.8571428571428572, "no_speech_prob": 3.446572009124793e-06}, {"id": 812, "seek": 540630, "start": 5406.3, "end": 5418.66, "text": " So what we do is we create a new multiplier for each channel and a new added value for", "tokens": [407, 437, 321, 360, 307, 321, 1884, 257, 777, 44106, 337, 1184, 2269, 293, 257, 777, 3869, 2158, 337], "temperature": 0.0, "avg_logprob": -0.218743428792039, "compression_ratio": 1.694267515923567, "no_speech_prob": 2.726465027080849e-06}, {"id": 813, "seek": 540630, "start": 5418.66, "end": 5425.1, "text": " each channel. Literally, we just start them out as the addition is just a bunch of 0s,", "tokens": [1184, 2269, 13, 23768, 11, 321, 445, 722, 552, 484, 382, 264, 4500, 307, 445, 257, 3840, 295, 1958, 82, 11], "temperature": 0.0, "avg_logprob": -0.218743428792039, "compression_ratio": 1.694267515923567, "no_speech_prob": 2.726465027080849e-06}, {"id": 814, "seek": 540630, "start": 5425.1, "end": 5431.860000000001, "text": " so for the first layer, 3 0s, and the multiplier for the first layer is just 3 1s. So number", "tokens": [370, 337, 264, 700, 4583, 11, 805, 1958, 82, 11, 293, 264, 44106, 337, 264, 700, 4583, 307, 445, 805, 502, 82, 13, 407, 1230], "temperature": 0.0, "avg_logprob": -0.218743428792039, "compression_ratio": 1.694267515923567, "no_speech_prob": 2.726465027080849e-06}, {"id": 815, "seek": 543186, "start": 5431.86, "end": 5439.0599999999995, "text": " of filters for the first layer is just 3. And so we then basically undo exactly what", "tokens": [295, 15995, 337, 264, 700, 4583, 307, 445, 805, 13, 400, 370, 321, 550, 1936, 23779, 2293, 437], "temperature": 0.0, "avg_logprob": -0.1551272897159352, "compression_ratio": 1.4928909952606635, "no_speech_prob": 1.034851948134019e-06}, {"id": 816, "seek": 543186, "start": 5439.0599999999995, "end": 5444.9, "text": " we just did, or potentially we undo them. So by saying this is an nn.parameter, that", "tokens": [321, 445, 630, 11, 420, 7263, 321, 23779, 552, 13, 407, 538, 1566, 341, 307, 364, 297, 77, 13, 2181, 335, 2398, 11, 300], "temperature": 0.0, "avg_logprob": -0.1551272897159352, "compression_ratio": 1.4928909952606635, "no_speech_prob": 1.034851948134019e-06}, {"id": 817, "seek": 543186, "start": 5444.9, "end": 5450.46, "text": " tells PyTorch you're allowed to learn these as weights.", "tokens": [5112, 9953, 51, 284, 339, 291, 434, 4350, 281, 1466, 613, 382, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1551272897159352, "compression_ratio": 1.4928909952606635, "no_speech_prob": 1.034851948134019e-06}, {"id": 818, "seek": 543186, "start": 5450.46, "end": 5455.219999999999, "text": " So initially it says, OK, subtract the means, divide by the standard deviations, multiply", "tokens": [407, 9105, 309, 1619, 11, 2264, 11, 16390, 264, 1355, 11, 9845, 538, 264, 3832, 31219, 763, 11, 12972], "temperature": 0.0, "avg_logprob": -0.1551272897159352, "compression_ratio": 1.4928909952606635, "no_speech_prob": 1.034851948134019e-06}, {"id": 819, "seek": 545522, "start": 5455.22, "end": 5465.26, "text": " by 1, add on 0. OK, that's fine, nothing much happened there. But what it turns out is that", "tokens": [538, 502, 11, 909, 322, 1958, 13, 2264, 11, 300, 311, 2489, 11, 1825, 709, 2011, 456, 13, 583, 437, 309, 4523, 484, 307, 300], "temperature": 0.0, "avg_logprob": -0.1275849775834517, "compression_ratio": 1.4972375690607735, "no_speech_prob": 2.8130129976489116e-06}, {"id": 820, "seek": 545522, "start": 5465.26, "end": 5472.14, "text": " now rather than, if it wants to kind of scale the layer up, it doesn't have to scale up", "tokens": [586, 2831, 813, 11, 498, 309, 2738, 281, 733, 295, 4373, 264, 4583, 493, 11, 309, 1177, 380, 362, 281, 4373, 493], "temperature": 0.0, "avg_logprob": -0.1275849775834517, "compression_ratio": 1.4972375690607735, "no_speech_prob": 2.8130129976489116e-06}, {"id": 821, "seek": 545522, "start": 5472.14, "end": 5481.26, "text": " every single value in the matrix, it can just scale up this single trio of numbers, self.m.", "tokens": [633, 2167, 2158, 294, 264, 8141, 11, 309, 393, 445, 4373, 493, 341, 2167, 37274, 295, 3547, 11, 2698, 13, 76, 13], "temperature": 0.0, "avg_logprob": -0.1275849775834517, "compression_ratio": 1.4972375690607735, "no_speech_prob": 2.8130129976489116e-06}, {"id": 822, "seek": 548126, "start": 5481.26, "end": 5485.5, "text": " If it wants to shift it all up or down a bit, it doesn't have to shift the entire weight", "tokens": [759, 309, 2738, 281, 5513, 309, 439, 493, 420, 760, 257, 857, 11, 309, 1177, 380, 362, 281, 5513, 264, 2302, 3364], "temperature": 0.0, "avg_logprob": -0.13179263439807262, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.2878925872428226e-06}, {"id": 823, "seek": 548126, "start": 5485.5, "end": 5492.02, "text": " matrix, it can just shift this trio of numbers, self.a.", "tokens": [8141, 11, 309, 393, 445, 5513, 341, 37274, 295, 3547, 11, 2698, 13, 64, 13], "temperature": 0.0, "avg_logprob": -0.13179263439807262, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.2878925872428226e-06}, {"id": 824, "seek": 548126, "start": 5492.02, "end": 5498.14, "text": " So I will say this, at this talk I mentioned at NIPS, Ali Rahimi's talk about rigor, he", "tokens": [407, 286, 486, 584, 341, 11, 412, 341, 751, 286, 2835, 412, 18482, 6273, 11, 12020, 17844, 10121, 311, 751, 466, 42191, 11, 415], "temperature": 0.0, "avg_logprob": -0.13179263439807262, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.2878925872428226e-06}, {"id": 825, "seek": 548126, "start": 5498.14, "end": 5506.22, "text": " actually pointed to this paper, this BatchNorm paper, as being a particularly useful, particularly", "tokens": [767, 10932, 281, 341, 3035, 11, 341, 363, 852, 45, 687, 3035, 11, 382, 885, 257, 4098, 4420, 11, 4098], "temperature": 0.0, "avg_logprob": -0.13179263439807262, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.2878925872428226e-06}, {"id": 826, "seek": 550622, "start": 5506.22, "end": 5516.54, "text": " interesting paper where a lot of people don't necessarily quite know why it works. And so", "tokens": [1880, 3035, 689, 257, 688, 295, 561, 500, 380, 4725, 1596, 458, 983, 309, 1985, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.12457406030942315, "compression_ratio": 1.5421052631578946, "no_speech_prob": 3.844919774564914e-06}, {"id": 827, "seek": 550622, "start": 5516.54, "end": 5522.06, "text": " if you're thinking, OK, subtracting out the means and then adding some learned weights", "tokens": [498, 291, 434, 1953, 11, 2264, 11, 16390, 278, 484, 264, 1355, 293, 550, 5127, 512, 3264, 17443], "temperature": 0.0, "avg_logprob": -0.12457406030942315, "compression_ratio": 1.5421052631578946, "no_speech_prob": 3.844919774564914e-06}, {"id": 828, "seek": 550622, "start": 5522.06, "end": 5531.34, "text": " of exactly the same rank and size sounds like a weird thing to do, there are a lot of people", "tokens": [295, 2293, 264, 912, 6181, 293, 2744, 3263, 411, 257, 3657, 551, 281, 360, 11, 456, 366, 257, 688, 295, 561], "temperature": 0.0, "avg_logprob": -0.12457406030942315, "compression_ratio": 1.5421052631578946, "no_speech_prob": 3.844919774564914e-06}, {"id": 829, "seek": 550622, "start": 5531.34, "end": 5533.54, "text": " that feel the same way.", "tokens": [300, 841, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.12457406030942315, "compression_ratio": 1.5421052631578946, "no_speech_prob": 3.844919774564914e-06}, {"id": 830, "seek": 553354, "start": 5533.54, "end": 5541.78, "text": " So at the moment I think the best I can say intuitively is what's going on here is that", "tokens": [407, 412, 264, 1623, 286, 519, 264, 1151, 286, 393, 584, 46506, 307, 437, 311, 516, 322, 510, 307, 300], "temperature": 0.0, "avg_logprob": -0.11360203652154832, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.4510329492622986e-05}, {"id": 831, "seek": 553354, "start": 5541.78, "end": 5548.98, "text": " we're normalizing the data and then we're saying you can then shift it and scale it", "tokens": [321, 434, 2710, 3319, 264, 1412, 293, 550, 321, 434, 1566, 291, 393, 550, 5513, 309, 293, 4373, 309], "temperature": 0.0, "avg_logprob": -0.11360203652154832, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.4510329492622986e-05}, {"id": 832, "seek": 553354, "start": 5548.98, "end": 5554.82, "text": " using far fewer parameters than would have been necessary if I was asking you to actually", "tokens": [1228, 1400, 13366, 9834, 813, 576, 362, 668, 4818, 498, 286, 390, 3365, 291, 281, 767], "temperature": 0.0, "avg_logprob": -0.11360203652154832, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.4510329492622986e-05}, {"id": 833, "seek": 553354, "start": 5554.82, "end": 5562.1, "text": " shift and scale the entire set of convolutional filters. That's the kind of basic intuition.", "tokens": [5513, 293, 4373, 264, 2302, 992, 295, 45216, 304, 15995, 13, 663, 311, 264, 733, 295, 3875, 24002, 13], "temperature": 0.0, "avg_logprob": -0.11360203652154832, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.4510329492622986e-05}, {"id": 834, "seek": 556210, "start": 5562.1, "end": 5571.42, "text": " More importantly, in practice, what this does is it basically allows us to increase our", "tokens": [5048, 8906, 11, 294, 3124, 11, 437, 341, 775, 307, 309, 1936, 4045, 505, 281, 3488, 527], "temperature": 0.0, "avg_logprob": -0.1256875551663912, "compression_ratio": 1.5941176470588236, "no_speech_prob": 1.3007042980461847e-05}, {"id": 835, "seek": 556210, "start": 5571.42, "end": 5577.02, "text": " learning rates and it increases the resilience of training and allows us to add more layers.", "tokens": [2539, 6846, 293, 309, 8637, 264, 19980, 295, 3097, 293, 4045, 505, 281, 909, 544, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1256875551663912, "compression_ratio": 1.5941176470588236, "no_speech_prob": 1.3007042980461847e-05}, {"id": 836, "seek": 556210, "start": 5577.02, "end": 5588.660000000001, "text": " So once I added a BN layer rather than a conv layer, I found I was able to add more layers", "tokens": [407, 1564, 286, 3869, 257, 363, 45, 4583, 2831, 813, 257, 3754, 4583, 11, 286, 1352, 286, 390, 1075, 281, 909, 544, 7914], "temperature": 0.0, "avg_logprob": -0.1256875551663912, "compression_ratio": 1.5941176470588236, "no_speech_prob": 1.3007042980461847e-05}, {"id": 837, "seek": 558866, "start": 5588.66, "end": 5593.42, "text": " to my model and it's still trained effectively.", "tokens": [281, 452, 2316, 293, 309, 311, 920, 8895, 8659, 13], "temperature": 0.0, "avg_logprob": -0.33927010798799817, "compression_ratio": 1.5, "no_speech_prob": 1.6187386790988967e-05}, {"id": 838, "seek": 558866, "start": 5593.42, "end": 5601.9, "text": " Question. Are we worried about anything that maybe we're divided by something very small", "tokens": [14464, 13, 2014, 321, 5804, 466, 1340, 300, 1310, 321, 434, 6666, 538, 746, 588, 1359], "temperature": 0.0, "avg_logprob": -0.33927010798799817, "compression_ratio": 1.5, "no_speech_prob": 1.6187386790988967e-05}, {"id": 839, "seek": 558866, "start": 5601.9, "end": 5607.42, "text": " or anything like that once we do this?", "tokens": [420, 1340, 411, 300, 1564, 321, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.33927010798799817, "compression_ratio": 1.5, "no_speech_prob": 1.6187386790988967e-05}, {"id": 840, "seek": 558866, "start": 5607.42, "end": 5617.38, "text": " I think in the PyTorch version it would probably be divided by self.studs plus epsilon or something.", "tokens": [286, 519, 294, 264, 9953, 51, 284, 339, 3037, 309, 576, 1391, 312, 6666, 538, 2698, 13, 372, 32083, 1804, 17889, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.33927010798799817, "compression_ratio": 1.5, "no_speech_prob": 1.6187386790988967e-05}, {"id": 841, "seek": 561738, "start": 5617.38, "end": 5625.1, "text": " This worked fine for me, but that is definitely something to think about if you were trying", "tokens": [639, 2732, 2489, 337, 385, 11, 457, 300, 307, 2138, 746, 281, 519, 466, 498, 291, 645, 1382], "temperature": 0.0, "avg_logprob": -0.2022137694306426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 3.966966687585227e-06}, {"id": 842, "seek": 561738, "start": 5625.1, "end": 5631.46, "text": " to make this more reliable.", "tokens": [281, 652, 341, 544, 12924, 13], "temperature": 0.0, "avg_logprob": -0.2022137694306426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 3.966966687585227e-06}, {"id": 843, "seek": 561738, "start": 5631.46, "end": 5638.1, "text": " So the self.m and self.a, I'm guessing it's getting updated through backpropagation as", "tokens": [407, 264, 2698, 13, 76, 293, 2698, 13, 64, 11, 286, 478, 17939, 309, 311, 1242, 10588, 807, 646, 79, 1513, 559, 399, 382], "temperature": 0.0, "avg_logprob": -0.2022137694306426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 3.966966687585227e-06}, {"id": 844, "seek": 561738, "start": 5638.1, "end": 5639.1, "text": " well?", "tokens": [731, 30], "temperature": 0.0, "avg_logprob": -0.2022137694306426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 3.966966687585227e-06}, {"id": 845, "seek": 561738, "start": 5639.1, "end": 5645.14, "text": " Yes, so by saying it's an nn.parameter, that's how we flagged PyTorch to learn it through", "tokens": [1079, 11, 370, 538, 1566, 309, 311, 364, 297, 77, 13, 2181, 335, 2398, 11, 300, 311, 577, 321, 7166, 3004, 9953, 51, 284, 339, 281, 1466, 309, 807], "temperature": 0.0, "avg_logprob": -0.2022137694306426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 3.966966687585227e-06}, {"id": 846, "seek": 564514, "start": 5645.14, "end": 5649.900000000001, "text": " backprop.", "tokens": [646, 79, 1513, 13], "temperature": 0.0, "avg_logprob": -0.1249229470077826, "compression_ratio": 1.65, "no_speech_prob": 3.905463017872535e-06}, {"id": 847, "seek": 564514, "start": 5649.900000000001, "end": 5656.06, "text": " The other interesting thing it turns out that BatchNorm does is it regularizes.", "tokens": [440, 661, 1880, 551, 309, 4523, 484, 300, 363, 852, 45, 687, 775, 307, 309, 3890, 5660, 13], "temperature": 0.0, "avg_logprob": -0.1249229470077826, "compression_ratio": 1.65, "no_speech_prob": 3.905463017872535e-06}, {"id": 848, "seek": 564514, "start": 5656.06, "end": 5661.18, "text": " In other words, you can often decrease or remove dropout or decrease or remove weight", "tokens": [682, 661, 2283, 11, 291, 393, 2049, 11514, 420, 4159, 3270, 346, 420, 11514, 420, 4159, 3364], "temperature": 0.0, "avg_logprob": -0.1249229470077826, "compression_ratio": 1.65, "no_speech_prob": 3.905463017872535e-06}, {"id": 849, "seek": 564514, "start": 5661.18, "end": 5663.740000000001, "text": " decay when you use BatchNorm.", "tokens": [21039, 562, 291, 764, 363, 852, 45, 687, 13], "temperature": 0.0, "avg_logprob": -0.1249229470077826, "compression_ratio": 1.65, "no_speech_prob": 3.905463017872535e-06}, {"id": 850, "seek": 564514, "start": 5663.740000000001, "end": 5670.3, "text": " And the reason why is if you think about it, each mini-batch is going to have a different", "tokens": [400, 264, 1778, 983, 307, 498, 291, 519, 466, 309, 11, 1184, 8382, 12, 65, 852, 307, 516, 281, 362, 257, 819], "temperature": 0.0, "avg_logprob": -0.1249229470077826, "compression_ratio": 1.65, "no_speech_prob": 3.905463017872535e-06}, {"id": 851, "seek": 564514, "start": 5670.3, "end": 5674.46, "text": " mean and a different standard deviation to the previous mini-batch.", "tokens": [914, 293, 257, 819, 3832, 25163, 281, 264, 3894, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.1249229470077826, "compression_ratio": 1.65, "no_speech_prob": 3.905463017872535e-06}, {"id": 852, "seek": 567446, "start": 5674.46, "end": 5676.82, "text": " So these things keep changing.", "tokens": [407, 613, 721, 1066, 4473, 13], "temperature": 0.0, "avg_logprob": -0.12649199106160877, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.7108851504162885e-06}, {"id": 853, "seek": 567446, "start": 5676.82, "end": 5681.38, "text": " And because they keep changing, it's kind of changing the meaning of the filters in", "tokens": [400, 570, 436, 1066, 4473, 11, 309, 311, 733, 295, 4473, 264, 3620, 295, 264, 15995, 294], "temperature": 0.0, "avg_logprob": -0.12649199106160877, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.7108851504162885e-06}, {"id": 854, "seek": 567446, "start": 5681.38, "end": 5682.88, "text": " this subtle way.", "tokens": [341, 13743, 636, 13], "temperature": 0.0, "avg_logprob": -0.12649199106160877, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.7108851504162885e-06}, {"id": 855, "seek": 567446, "start": 5682.88, "end": 5686.26, "text": " And so it's adding a regularization effect because it's noise.", "tokens": [400, 370, 309, 311, 5127, 257, 3890, 2144, 1802, 570, 309, 311, 5658, 13], "temperature": 0.0, "avg_logprob": -0.12649199106160877, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.7108851504162885e-06}, {"id": 856, "seek": 567446, "start": 5686.26, "end": 5692.46, "text": " When you add noise of any kind, it regularizes your model.", "tokens": [1133, 291, 909, 5658, 295, 604, 733, 11, 309, 3890, 5660, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12649199106160877, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.7108851504162885e-06}, {"id": 857, "seek": 567446, "start": 5692.46, "end": 5695.62, "text": " I'm actually cheating a little bit here.", "tokens": [286, 478, 767, 18309, 257, 707, 857, 510, 13], "temperature": 0.0, "avg_logprob": -0.12649199106160877, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.7108851504162885e-06}, {"id": 858, "seek": 567446, "start": 5695.62, "end": 5702.58, "text": " In the real version of BatchNorm, you don't just use this batch's mean and standard deviation,", "tokens": [682, 264, 957, 3037, 295, 363, 852, 45, 687, 11, 291, 500, 380, 445, 764, 341, 15245, 311, 914, 293, 3832, 25163, 11], "temperature": 0.0, "avg_logprob": -0.12649199106160877, "compression_ratio": 1.6623931623931625, "no_speech_prob": 4.7108851504162885e-06}, {"id": 859, "seek": 570258, "start": 5702.58, "end": 5709.0599999999995, "text": " but instead you take an exponentially weighted moving average standard deviation and mean.", "tokens": [457, 2602, 291, 747, 364, 37330, 32807, 2684, 4274, 3832, 25163, 293, 914, 13], "temperature": 0.0, "avg_logprob": -0.10336270389786686, "compression_ratio": 1.5560538116591929, "no_speech_prob": 2.684193759705522e-06}, {"id": 860, "seek": 570258, "start": 5709.0599999999995, "end": 5714.3, "text": " And so if you wanted an exercise to try during the week, that would be a good thing to try.", "tokens": [400, 370, 498, 291, 1415, 364, 5380, 281, 853, 1830, 264, 1243, 11, 300, 576, 312, 257, 665, 551, 281, 853, 13], "temperature": 0.0, "avg_logprob": -0.10336270389786686, "compression_ratio": 1.5560538116591929, "no_speech_prob": 2.684193759705522e-06}, {"id": 861, "seek": 570258, "start": 5714.3, "end": 5719.6, "text": " But I will point out something very important here, which is if self.training.", "tokens": [583, 286, 486, 935, 484, 746, 588, 1021, 510, 11, 597, 307, 498, 2698, 13, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.10336270389786686, "compression_ratio": 1.5560538116591929, "no_speech_prob": 2.684193759705522e-06}, {"id": 862, "seek": 570258, "start": 5719.6, "end": 5727.62, "text": " When we are doing our training loop, this will be true when it's being applied to the", "tokens": [1133, 321, 366, 884, 527, 3097, 6367, 11, 341, 486, 312, 2074, 562, 309, 311, 885, 6456, 281, 264], "temperature": 0.0, "avg_logprob": -0.10336270389786686, "compression_ratio": 1.5560538116591929, "no_speech_prob": 2.684193759705522e-06}, {"id": 863, "seek": 572762, "start": 5727.62, "end": 5733.26, "text": " training set, and it will be false when it's being applied to the validation set.", "tokens": [3097, 992, 11, 293, 309, 486, 312, 7908, 562, 309, 311, 885, 6456, 281, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.10300120487008044, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.058045538433362e-06}, {"id": 864, "seek": 572762, "start": 5733.26, "end": 5736.62, "text": " And this is really important because when you're going through the validation set, you", "tokens": [400, 341, 307, 534, 1021, 570, 562, 291, 434, 516, 807, 264, 24071, 992, 11, 291], "temperature": 0.0, "avg_logprob": -0.10300120487008044, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.058045538433362e-06}, {"id": 865, "seek": 572762, "start": 5736.62, "end": 5741.34, "text": " do not want to be changing the meaning of the model.", "tokens": [360, 406, 528, 281, 312, 4473, 264, 3620, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10300120487008044, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.058045538433362e-06}, {"id": 866, "seek": 572762, "start": 5741.34, "end": 5747.9, "text": " So this is this really important idea, is that there are some types of layer that are", "tokens": [407, 341, 307, 341, 534, 1021, 1558, 11, 307, 300, 456, 366, 512, 3467, 295, 4583, 300, 366], "temperature": 0.0, "avg_logprob": -0.10300120487008044, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.058045538433362e-06}, {"id": 867, "seek": 572762, "start": 5747.9, "end": 5755.66, "text": " actually sensitive to what the mode of the network is, whether it's in training mode", "tokens": [767, 9477, 281, 437, 264, 4391, 295, 264, 3209, 307, 11, 1968, 309, 311, 294, 3097, 4391], "temperature": 0.0, "avg_logprob": -0.10300120487008044, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.058045538433362e-06}, {"id": 868, "seek": 575566, "start": 5755.66, "end": 5761.0199999999995, "text": " or as PyTorch calls it, evaluation mode, or we might say test mode.", "tokens": [420, 382, 9953, 51, 284, 339, 5498, 309, 11, 13344, 4391, 11, 420, 321, 1062, 584, 1500, 4391, 13], "temperature": 0.0, "avg_logprob": -0.15688960957077314, "compression_ratio": 1.63135593220339, "no_speech_prob": 8.267827979580034e-06}, {"id": 869, "seek": 575566, "start": 5761.0199999999995, "end": 5768.0599999999995, "text": " And actually, we actually had a bug a couple of weeks ago when we did our mininet for MovieLens,", "tokens": [400, 767, 11, 321, 767, 632, 257, 7426, 257, 1916, 295, 3259, 2057, 562, 321, 630, 527, 923, 21370, 337, 28766, 43, 694, 11], "temperature": 0.0, "avg_logprob": -0.15688960957077314, "compression_ratio": 1.63135593220339, "no_speech_prob": 8.267827979580034e-06}, {"id": 870, "seek": 575566, "start": 5768.0599999999995, "end": 5769.62, "text": " the collaborative filtering.", "tokens": [264, 16555, 30822, 13], "temperature": 0.0, "avg_logprob": -0.15688960957077314, "compression_ratio": 1.63135593220339, "no_speech_prob": 8.267827979580034e-06}, {"id": 871, "seek": 575566, "start": 5769.62, "end": 5777.54, "text": " We actually had F.dropout in our forward pass without protecting it with an if self.training", "tokens": [492, 767, 632, 479, 13, 23332, 346, 294, 527, 2128, 1320, 1553, 12316, 309, 365, 364, 498, 2698, 13, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.15688960957077314, "compression_ratio": 1.63135593220339, "no_speech_prob": 8.267827979580034e-06}, {"id": 872, "seek": 575566, "start": 5777.54, "end": 5779.24, "text": " F.dropout.", "tokens": [479, 13, 23332, 346, 13], "temperature": 0.0, "avg_logprob": -0.15688960957077314, "compression_ratio": 1.63135593220339, "no_speech_prob": 8.267827979580034e-06}, {"id": 873, "seek": 575566, "start": 5779.24, "end": 5783.9, "text": " As a result of which, we were actually doing dropout in the validation piece as well as", "tokens": [1018, 257, 1874, 295, 597, 11, 321, 645, 767, 884, 3270, 346, 294, 264, 24071, 2522, 382, 731, 382], "temperature": 0.0, "avg_logprob": -0.15688960957077314, "compression_ratio": 1.63135593220339, "no_speech_prob": 8.267827979580034e-06}, {"id": 874, "seek": 578390, "start": 5783.9, "end": 5787.299999999999, "text": " the training piece, which obviously isn't what you want.", "tokens": [264, 3097, 2522, 11, 597, 2745, 1943, 380, 437, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.13781258300110533, "compression_ratio": 1.5497630331753554, "no_speech_prob": 6.786727908547618e-07}, {"id": 875, "seek": 578390, "start": 5787.299999999999, "end": 5794.42, "text": " So I've actually gone back and fixed this by changing it to using nn.dropout.", "tokens": [407, 286, 600, 767, 2780, 646, 293, 6806, 341, 538, 4473, 309, 281, 1228, 297, 77, 13, 23332, 346, 13], "temperature": 0.0, "avg_logprob": -0.13781258300110533, "compression_ratio": 1.5497630331753554, "no_speech_prob": 6.786727908547618e-07}, {"id": 876, "seek": 578390, "start": 5794.42, "end": 5799.42, "text": " And nn.dropout has already been written for us to check whether it's being used in training", "tokens": [400, 297, 77, 13, 23332, 346, 575, 1217, 668, 3720, 337, 505, 281, 1520, 1968, 309, 311, 885, 1143, 294, 3097], "temperature": 0.0, "avg_logprob": -0.13781258300110533, "compression_ratio": 1.5497630331753554, "no_speech_prob": 6.786727908547618e-07}, {"id": 877, "seek": 578390, "start": 5799.42, "end": 5801.58, "text": " mode or not.", "tokens": [4391, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.13781258300110533, "compression_ratio": 1.5497630331753554, "no_speech_prob": 6.786727908547618e-07}, {"id": 878, "seek": 578390, "start": 5801.58, "end": 5809.7, "text": " Or alternatively, I could have added an if self.training before I use the dropout here.", "tokens": [1610, 8535, 356, 11, 286, 727, 362, 3869, 364, 498, 2698, 13, 17227, 1760, 949, 286, 764, 264, 3270, 346, 510, 13], "temperature": 0.0, "avg_logprob": -0.13781258300110533, "compression_ratio": 1.5497630331753554, "no_speech_prob": 6.786727908547618e-07}, {"id": 879, "seek": 580970, "start": 5809.7, "end": 5815.38, "text": " So it's important to think about that, and the main two, or pretty much the only two", "tokens": [407, 309, 311, 1021, 281, 519, 466, 300, 11, 293, 264, 2135, 732, 11, 420, 1238, 709, 264, 787, 732], "temperature": 0.0, "avg_logprob": -0.182350678877397, "compression_ratio": 1.4244186046511629, "no_speech_prob": 2.642577328515472e-06}, {"id": 880, "seek": 580970, "start": 5815.38, "end": 5822.3, "text": " built into PyTorch where this happens is dropout and batch melt.", "tokens": [3094, 666, 9953, 51, 284, 339, 689, 341, 2314, 307, 3270, 346, 293, 15245, 10083, 13], "temperature": 0.0, "avg_logprob": -0.182350678877397, "compression_ratio": 1.4244186046511629, "no_speech_prob": 2.642577328515472e-06}, {"id": 881, "seek": 580970, "start": 5822.3, "end": 5829.54, "text": " And so interestingly, this is also a key difference in fast.ai, which no other library does, is", "tokens": [400, 370, 25873, 11, 341, 307, 611, 257, 2141, 2649, 294, 2370, 13, 1301, 11, 597, 572, 661, 6405, 775, 11, 307], "temperature": 0.0, "avg_logprob": -0.182350678877397, "compression_ratio": 1.4244186046511629, "no_speech_prob": 2.642577328515472e-06}, {"id": 882, "seek": 582954, "start": 5829.54, "end": 5840.42, "text": " that these means and standard deviations get updated in training mode in every other library", "tokens": [300, 613, 1355, 293, 3832, 31219, 763, 483, 10588, 294, 3097, 4391, 294, 633, 661, 6405], "temperature": 0.0, "avg_logprob": -0.07707150951846616, "compression_ratio": 1.7571428571428571, "no_speech_prob": 2.443983476041467e-06}, {"id": 883, "seek": 582954, "start": 5840.42, "end": 5844.78, "text": " as soon as you basically say I'm training, regardless even of whether that layer is set", "tokens": [382, 2321, 382, 291, 1936, 584, 286, 478, 3097, 11, 10060, 754, 295, 1968, 300, 4583, 307, 992], "temperature": 0.0, "avg_logprob": -0.07707150951846616, "compression_ratio": 1.7571428571428571, "no_speech_prob": 2.443983476041467e-06}, {"id": 884, "seek": 582954, "start": 5844.78, "end": 5847.14, "text": " to trainable or not.", "tokens": [281, 3847, 712, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.07707150951846616, "compression_ratio": 1.7571428571428571, "no_speech_prob": 2.443983476041467e-06}, {"id": 885, "seek": 582954, "start": 5847.14, "end": 5851.46, "text": " And it turns out that with a pre-trained network, that's a terrible idea.", "tokens": [400, 309, 4523, 484, 300, 365, 257, 659, 12, 17227, 2001, 3209, 11, 300, 311, 257, 6237, 1558, 13], "temperature": 0.0, "avg_logprob": -0.07707150951846616, "compression_ratio": 1.7571428571428571, "no_speech_prob": 2.443983476041467e-06}, {"id": 886, "seek": 582954, "start": 5851.46, "end": 5856.1, "text": " If you have a pre-trained network, the specific values of those means and standard deviations", "tokens": [759, 291, 362, 257, 659, 12, 17227, 2001, 3209, 11, 264, 2685, 4190, 295, 729, 1355, 293, 3832, 31219, 763], "temperature": 0.0, "avg_logprob": -0.07707150951846616, "compression_ratio": 1.7571428571428571, "no_speech_prob": 2.443983476041467e-06}, {"id": 887, "seek": 585610, "start": 5856.1, "end": 5861.700000000001, "text": " in batch norm, if you change them, it changes the meaning of those pre-trained layers.", "tokens": [294, 15245, 2026, 11, 498, 291, 1319, 552, 11, 309, 2962, 264, 3620, 295, 729, 659, 12, 17227, 2001, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1366105079650879, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.356865701993229e-06}, {"id": 888, "seek": 585610, "start": 5861.700000000001, "end": 5867.620000000001, "text": " And so in fast.ai, always by default, it won't touch those means and standard deviations", "tokens": [400, 370, 294, 2370, 13, 1301, 11, 1009, 538, 7576, 11, 309, 1582, 380, 2557, 729, 1355, 293, 3832, 31219, 763], "temperature": 0.0, "avg_logprob": -0.1366105079650879, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.356865701993229e-06}, {"id": 889, "seek": 585610, "start": 5867.620000000001, "end": 5869.780000000001, "text": " if your layer is frozen.", "tokens": [498, 428, 4583, 307, 12496, 13], "temperature": 0.0, "avg_logprob": -0.1366105079650879, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.356865701993229e-06}, {"id": 890, "seek": 585610, "start": 5869.780000000001, "end": 5881.1, "text": " As soon as you unfreeze it, it'll start updating them, unless you've set learn.bnfreeze true.", "tokens": [1018, 2321, 382, 291, 3971, 701, 1381, 309, 11, 309, 603, 722, 25113, 552, 11, 5969, 291, 600, 992, 1466, 13, 19404, 10792, 1381, 2074, 13], "temperature": 0.0, "avg_logprob": -0.1366105079650879, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.356865701993229e-06}, {"id": 891, "seek": 588110, "start": 5881.1, "end": 5886.9800000000005, "text": " If you set learn.bnfreeze true, it says never touch these means and standard deviations.", "tokens": [759, 291, 992, 1466, 13, 19404, 10792, 1381, 2074, 11, 309, 1619, 1128, 2557, 613, 1355, 293, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.14673165639241537, "compression_ratio": 1.5454545454545454, "no_speech_prob": 3.393132146811695e-06}, {"id": 892, "seek": 588110, "start": 5886.9800000000005, "end": 5895.34, "text": " And I found in practice that that often seems to work a lot better for pre-trained models,", "tokens": [400, 286, 1352, 294, 3124, 300, 300, 2049, 2544, 281, 589, 257, 688, 1101, 337, 659, 12, 17227, 2001, 5245, 11], "temperature": 0.0, "avg_logprob": -0.14673165639241537, "compression_ratio": 1.5454545454545454, "no_speech_prob": 3.393132146811695e-06}, {"id": 893, "seek": 588110, "start": 5895.34, "end": 5899.660000000001, "text": " particularly if you're working with data that's quite similar to what the pre-trained model", "tokens": [4098, 498, 291, 434, 1364, 365, 1412, 300, 311, 1596, 2531, 281, 437, 264, 659, 12, 17227, 2001, 2316], "temperature": 0.0, "avg_logprob": -0.14673165639241537, "compression_ratio": 1.5454545454545454, "no_speech_prob": 3.393132146811695e-06}, {"id": 894, "seek": 588110, "start": 5899.660000000001, "end": 5903.620000000001, "text": " was trained with.", "tokens": [390, 8895, 365, 13], "temperature": 0.0, "avg_logprob": -0.14673165639241537, "compression_ratio": 1.5454545454545454, "no_speech_prob": 3.393132146811695e-06}, {"id": 895, "seek": 590362, "start": 5903.62, "end": 5914.14, "text": " So I have two questions, so it looks like you did a lot more work calculating the aggregates.", "tokens": [407, 286, 362, 732, 1651, 11, 370, 309, 1542, 411, 291, 630, 257, 688, 544, 589, 28258, 264, 16743, 1024, 13], "temperature": 0.0, "avg_logprob": -0.24983068612905648, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.269806984462775e-05}, {"id": 896, "seek": 590362, "start": 5914.14, "end": 5916.7, "text": " Looks like I did a lot of work, did you say?", "tokens": [10027, 411, 286, 630, 257, 688, 295, 589, 11, 630, 291, 584, 30], "temperature": 0.0, "avg_logprob": -0.24983068612905648, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.269806984462775e-05}, {"id": 897, "seek": 590362, "start": 5916.7, "end": 5918.9, "text": " Like quite a lot of code here?", "tokens": [1743, 1596, 257, 688, 295, 3089, 510, 30], "temperature": 0.0, "avg_logprob": -0.24983068612905648, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.269806984462775e-05}, {"id": 898, "seek": 590362, "start": 5918.9, "end": 5921.54, "text": " Well you're doing more work than you would normally do.", "tokens": [1042, 291, 434, 884, 544, 589, 813, 291, 576, 5646, 360, 13], "temperature": 0.0, "avg_logprob": -0.24983068612905648, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.269806984462775e-05}, {"id": 899, "seek": 590362, "start": 5921.54, "end": 5926.0599999999995, "text": " Essentially you're calculating all these aggregates as you go through each layer.", "tokens": [23596, 291, 434, 28258, 439, 613, 16743, 1024, 382, 291, 352, 807, 1184, 4583, 13], "temperature": 0.0, "avg_logprob": -0.24983068612905648, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.269806984462775e-05}, {"id": 900, "seek": 590362, "start": 5926.0599999999995, "end": 5927.0599999999995, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.24983068612905648, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.269806984462775e-05}, {"id": 901, "seek": 590362, "start": 5927.0599999999995, "end": 5931.86, "text": " Wouldn't this mean your training, like your epoch time, like blows up?", "tokens": [26291, 380, 341, 914, 428, 3097, 11, 411, 428, 30992, 339, 565, 11, 411, 18458, 493, 30], "temperature": 0.0, "avg_logprob": -0.24983068612905648, "compression_ratio": 1.7098214285714286, "no_speech_prob": 3.269806984462775e-05}, {"id": 902, "seek": 593186, "start": 5931.86, "end": 5933.7, "text": " No, this is like super fast.", "tokens": [883, 11, 341, 307, 411, 1687, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1594167728813327, "compression_ratio": 1.578723404255319, "no_speech_prob": 3.0894782412360655e-06}, {"id": 903, "seek": 593186, "start": 5933.7, "end": 5941.139999999999, "text": " If you think about what a conv has to do, a conv has to go through every 3x3 with a", "tokens": [759, 291, 519, 466, 437, 257, 3754, 575, 281, 360, 11, 257, 3754, 575, 281, 352, 807, 633, 805, 87, 18, 365, 257], "temperature": 0.0, "avg_logprob": -0.1594167728813327, "compression_ratio": 1.578723404255319, "no_speech_prob": 3.0894782412360655e-06}, {"id": 904, "seek": 593186, "start": 5941.139999999999, "end": 5945.299999999999, "text": " stride and do this multiplication and then addition.", "tokens": [1056, 482, 293, 360, 341, 27290, 293, 550, 4500, 13], "temperature": 0.0, "avg_logprob": -0.1594167728813327, "compression_ratio": 1.578723404255319, "no_speech_prob": 3.0894782412360655e-06}, {"id": 905, "seek": 593186, "start": 5945.299999999999, "end": 5951.66, "text": " That is a lot more work than simply calculating the per channel mean.", "tokens": [663, 307, 257, 688, 544, 589, 813, 2935, 28258, 264, 680, 2269, 914, 13], "temperature": 0.0, "avg_logprob": -0.1594167728813327, "compression_ratio": 1.578723404255319, "no_speech_prob": 3.0894782412360655e-06}, {"id": 906, "seek": 593186, "start": 5951.66, "end": 5958.0599999999995, "text": " So it adds a little bit of time, but it's less time intensive than the convolution.", "tokens": [407, 309, 10860, 257, 707, 857, 295, 565, 11, 457, 309, 311, 1570, 565, 18957, 813, 264, 45216, 13], "temperature": 0.0, "avg_logprob": -0.1594167728813327, "compression_ratio": 1.578723404255319, "no_speech_prob": 3.0894782412360655e-06}, {"id": 907, "seek": 593186, "start": 5958.0599999999995, "end": 5961.299999999999, "text": " So how would you basically position the batch norm?", "tokens": [407, 577, 576, 291, 1936, 2535, 264, 15245, 2026, 30], "temperature": 0.0, "avg_logprob": -0.1594167728813327, "compression_ratio": 1.578723404255319, "no_speech_prob": 3.0894782412360655e-06}, {"id": 908, "seek": 596130, "start": 5961.3, "end": 5966.18, "text": " Would it be right after the convolutional layer or would it be after the ReLU?", "tokens": [6068, 309, 312, 558, 934, 264, 45216, 304, 4583, 420, 576, 309, 312, 934, 264, 1300, 43, 52, 30], "temperature": 0.0, "avg_logprob": -0.19640047957257525, "compression_ratio": 1.576086956521739, "no_speech_prob": 5.422187314252369e-06}, {"id": 909, "seek": 596130, "start": 5966.18, "end": 5968.74, "text": " We'll talk about that in a moment.", "tokens": [492, 603, 751, 466, 300, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.19640047957257525, "compression_ratio": 1.576086956521739, "no_speech_prob": 5.422187314252369e-06}, {"id": 910, "seek": 596130, "start": 5968.74, "end": 5975.3, "text": " So at the moment we have it after the ReLU and in the original batch norm paper, I believe", "tokens": [407, 412, 264, 1623, 321, 362, 309, 934, 264, 1300, 43, 52, 293, 294, 264, 3380, 15245, 2026, 3035, 11, 286, 1697], "temperature": 0.0, "avg_logprob": -0.19640047957257525, "compression_ratio": 1.576086956521739, "no_speech_prob": 5.422187314252369e-06}, {"id": 911, "seek": 596130, "start": 5975.3, "end": 5981.2, "text": " that's where they put it.", "tokens": [300, 311, 689, 436, 829, 309, 13], "temperature": 0.0, "avg_logprob": -0.19640047957257525, "compression_ratio": 1.576086956521739, "no_speech_prob": 5.422187314252369e-06}, {"id": 912, "seek": 596130, "start": 5981.2, "end": 5985.62, "text": " So there's this idea of something called an ablation study.", "tokens": [407, 456, 311, 341, 1558, 295, 746, 1219, 364, 410, 24278, 2979, 13], "temperature": 0.0, "avg_logprob": -0.19640047957257525, "compression_ratio": 1.576086956521739, "no_speech_prob": 5.422187314252369e-06}, {"id": 913, "seek": 598562, "start": 5985.62, "end": 5994.62, "text": " And ablation study is something where you basically try kind of turning on and off different", "tokens": [400, 410, 24278, 2979, 307, 746, 689, 291, 1936, 853, 733, 295, 6246, 322, 293, 766, 819], "temperature": 0.0, "avg_logprob": -0.1293588500839096, "compression_ratio": 1.8204081632653062, "no_speech_prob": 1.6701083950465545e-05}, {"id": 914, "seek": 598562, "start": 5994.62, "end": 5998.94, "text": " pieces of your model to see which bits make which impacts.", "tokens": [3755, 295, 428, 2316, 281, 536, 597, 9239, 652, 597, 11606, 13], "temperature": 0.0, "avg_logprob": -0.1293588500839096, "compression_ratio": 1.8204081632653062, "no_speech_prob": 1.6701083950465545e-05}, {"id": 915, "seek": 598562, "start": 5998.94, "end": 6003.0599999999995, "text": " And one of the things that wasn't done in the original batch norm paper was any kind", "tokens": [400, 472, 295, 264, 721, 300, 2067, 380, 1096, 294, 264, 3380, 15245, 2026, 3035, 390, 604, 733], "temperature": 0.0, "avg_logprob": -0.1293588500839096, "compression_ratio": 1.8204081632653062, "no_speech_prob": 1.6701083950465545e-05}, {"id": 916, "seek": 598562, "start": 6003.0599999999995, "end": 6006.38, "text": " of really effective ablation study.", "tokens": [295, 534, 4942, 410, 24278, 2979, 13], "temperature": 0.0, "avg_logprob": -0.1293588500839096, "compression_ratio": 1.8204081632653062, "no_speech_prob": 1.6701083950465545e-05}, {"id": 917, "seek": 598562, "start": 6006.38, "end": 6009.88, "text": " And one of the things therefore that was missing was this question which you just asked, which", "tokens": [400, 472, 295, 264, 721, 4412, 300, 390, 5361, 390, 341, 1168, 597, 291, 445, 2351, 11, 597], "temperature": 0.0, "avg_logprob": -0.1293588500839096, "compression_ratio": 1.8204081632653062, "no_speech_prob": 1.6701083950465545e-05}, {"id": 918, "seek": 598562, "start": 6009.88, "end": 6012.38, "text": " is where do you put the batch norm?", "tokens": [307, 689, 360, 291, 829, 264, 15245, 2026, 30], "temperature": 0.0, "avg_logprob": -0.1293588500839096, "compression_ratio": 1.8204081632653062, "no_speech_prob": 1.6701083950465545e-05}, {"id": 919, "seek": 598562, "start": 6012.38, "end": 6014.46, "text": " Before the ReLU, after the ReLU, whatever.", "tokens": [4546, 264, 1300, 43, 52, 11, 934, 264, 1300, 43, 52, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1293588500839096, "compression_ratio": 1.8204081632653062, "no_speech_prob": 1.6701083950465545e-05}, {"id": 920, "seek": 601446, "start": 6014.46, "end": 6020.26, "text": " And so since that time, that oversight has caused a lot of problems because it turned", "tokens": [400, 370, 1670, 300, 565, 11, 300, 29146, 575, 7008, 257, 688, 295, 2740, 570, 309, 3574], "temperature": 0.0, "avg_logprob": -0.17392507721396053, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.7264586606179364e-06}, {"id": 921, "seek": 601446, "start": 6020.26, "end": 6025.06, "text": " out the original paper didn't actually put it in the best spot.", "tokens": [484, 264, 3380, 3035, 994, 380, 767, 829, 309, 294, 264, 1151, 4008, 13], "temperature": 0.0, "avg_logprob": -0.17392507721396053, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.7264586606179364e-06}, {"id": 922, "seek": 601446, "start": 6025.06, "end": 6027.9, "text": " And so then other people since then have now figured that out.", "tokens": [400, 370, 550, 661, 561, 1670, 550, 362, 586, 8932, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.17392507721396053, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.7264586606179364e-06}, {"id": 923, "seek": 601446, "start": 6027.9, "end": 6032.1, "text": " And now every time I show people code where it's actually in the spot that turns out to", "tokens": [400, 586, 633, 565, 286, 855, 561, 3089, 689, 309, 311, 767, 294, 264, 4008, 300, 4523, 484, 281], "temperature": 0.0, "avg_logprob": -0.17392507721396053, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.7264586606179364e-06}, {"id": 924, "seek": 601446, "start": 6032.1, "end": 6035.86, "text": " be better, people always say, your batch norm is in the wrong spot.", "tokens": [312, 1101, 11, 561, 1009, 584, 11, 428, 15245, 2026, 307, 294, 264, 2085, 4008, 13], "temperature": 0.0, "avg_logprob": -0.17392507721396053, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.7264586606179364e-06}, {"id": 925, "seek": 601446, "start": 6035.86, "end": 6038.78, "text": " And I have to go back and say, no, I know that's what the paper said, but it turned", "tokens": [400, 286, 362, 281, 352, 646, 293, 584, 11, 572, 11, 286, 458, 300, 311, 437, 264, 3035, 848, 11, 457, 309, 3574], "temperature": 0.0, "avg_logprob": -0.17392507721396053, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.7264586606179364e-06}, {"id": 926, "seek": 601446, "start": 6038.78, "end": 6040.7, "text": " out that's not actually the right spot.", "tokens": [484, 300, 311, 406, 767, 264, 558, 4008, 13], "temperature": 0.0, "avg_logprob": -0.17392507721396053, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.7264586606179364e-06}, {"id": 927, "seek": 601446, "start": 6040.7, "end": 6042.7, "text": " So it kind of causes confusion.", "tokens": [407, 309, 733, 295, 7700, 15075, 13], "temperature": 0.0, "avg_logprob": -0.17392507721396053, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.7264586606179364e-06}, {"id": 928, "seek": 604270, "start": 6042.7, "end": 6048.0199999999995, "text": " So there's been a lot of question about that.", "tokens": [407, 456, 311, 668, 257, 688, 295, 1168, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.22519863446553548, "compression_ratio": 1.4172185430463575, "no_speech_prob": 4.7850635382928886e-06}, {"id": 929, "seek": 604270, "start": 6048.0199999999995, "end": 6051.099999999999, "text": " So a little bit of a higher level question.", "tokens": [407, 257, 707, 857, 295, 257, 2946, 1496, 1168, 13], "temperature": 0.0, "avg_logprob": -0.22519863446553548, "compression_ratio": 1.4172185430463575, "no_speech_prob": 4.7850635382928886e-06}, {"id": 930, "seek": 604270, "start": 6051.099999999999, "end": 6056.34, "text": " So we started out with CyPhar data.", "tokens": [407, 321, 1409, 484, 365, 10295, 47, 5854, 1412, 13], "temperature": 0.0, "avg_logprob": -0.22519863446553548, "compression_ratio": 1.4172185430463575, "no_speech_prob": 4.7850635382928886e-06}, {"id": 931, "seek": 604270, "start": 6056.34, "end": 6065.9, "text": " Is the basic reasoning that you use a smaller data set to quickly train a new model, and", "tokens": [1119, 264, 3875, 21577, 300, 291, 764, 257, 4356, 1412, 992, 281, 2661, 3847, 257, 777, 2316, 11, 293], "temperature": 0.0, "avg_logprob": -0.22519863446553548, "compression_ratio": 1.4172185430463575, "no_speech_prob": 4.7850635382928886e-06}, {"id": 932, "seek": 606590, "start": 6065.9, "end": 6076.0599999999995, "text": " then you take the same model and you're using a much bigger data set to get a higher accuracy", "tokens": [550, 291, 747, 264, 912, 2316, 293, 291, 434, 1228, 257, 709, 3801, 1412, 992, 281, 483, 257, 2946, 14170], "temperature": 0.0, "avg_logprob": -0.19740877760217546, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.4823491483184625e-06}, {"id": 933, "seek": 606590, "start": 6076.0599999999995, "end": 6077.0599999999995, "text": " level?", "tokens": [1496, 30], "temperature": 0.0, "avg_logprob": -0.19740877760217546, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.4823491483184625e-06}, {"id": 934, "seek": 606590, "start": 6077.0599999999995, "end": 6078.0599999999995, "text": " Is that the basic?", "tokens": [1119, 300, 264, 3875, 30], "temperature": 0.0, "avg_logprob": -0.19740877760217546, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.4823491483184625e-06}, {"id": 935, "seek": 606590, "start": 6078.0599999999995, "end": 6079.0599999999995, "text": " Answer the question.", "tokens": [24545, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19740877760217546, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.4823491483184625e-06}, {"id": 936, "seek": 606590, "start": 6079.0599999999995, "end": 6080.0599999999995, "text": " Maybe.", "tokens": [2704, 13], "temperature": 0.0, "avg_logprob": -0.19740877760217546, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.4823491483184625e-06}, {"id": 937, "seek": 606590, "start": 6080.0599999999995, "end": 6088.0199999999995, "text": " So if you had a large data set, or if you were interested in the question of how good", "tokens": [407, 498, 291, 632, 257, 2416, 1412, 992, 11, 420, 498, 291, 645, 3102, 294, 264, 1168, 295, 577, 665], "temperature": 0.0, "avg_logprob": -0.19740877760217546, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.4823491483184625e-06}, {"id": 938, "seek": 606590, "start": 6088.0199999999995, "end": 6092.299999999999, "text": " is this technique on a large data set, then yes, what you just said would be what I would", "tokens": [307, 341, 6532, 322, 257, 2416, 1412, 992, 11, 550, 2086, 11, 437, 291, 445, 848, 576, 312, 437, 286, 576], "temperature": 0.0, "avg_logprob": -0.19740877760217546, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.4823491483184625e-06}, {"id": 939, "seek": 606590, "start": 6092.299999999999, "end": 6093.299999999999, "text": " do.", "tokens": [360, 13], "temperature": 0.0, "avg_logprob": -0.19740877760217546, "compression_ratio": 1.6108374384236452, "no_speech_prob": 1.4823491483184625e-06}, {"id": 940, "seek": 609330, "start": 6093.3, "end": 6099.06, "text": " I would start testing on a small data set which I had already discovered had the same", "tokens": [286, 576, 722, 4997, 322, 257, 1359, 1412, 992, 597, 286, 632, 1217, 6941, 632, 264, 912], "temperature": 0.0, "avg_logprob": -0.1438352411443537, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.223406777891796e-06}, {"id": 941, "seek": 609330, "start": 6099.06, "end": 6103.3, "text": " kinds of properties as my larger data set, and therefore my conclusions would likely", "tokens": [3685, 295, 7221, 382, 452, 4833, 1412, 992, 11, 293, 4412, 452, 22865, 576, 3700], "temperature": 0.0, "avg_logprob": -0.1438352411443537, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.223406777891796e-06}, {"id": 942, "seek": 609330, "start": 6103.3, "end": 6106.22, "text": " carry forward, and then I would test them at the end.", "tokens": [3985, 2128, 11, 293, 550, 286, 576, 1500, 552, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.1438352411443537, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.223406777891796e-06}, {"id": 943, "seek": 609330, "start": 6106.22, "end": 6113.7, "text": " Having said that, personally I'm actually more interested in actually studying small", "tokens": [10222, 848, 300, 11, 5665, 286, 478, 767, 544, 3102, 294, 767, 7601, 1359], "temperature": 0.0, "avg_logprob": -0.1438352411443537, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.223406777891796e-06}, {"id": 944, "seek": 609330, "start": 6113.7, "end": 6120.9800000000005, "text": " data sets for their own sake, because I find most people I speak to in the real world don't", "tokens": [1412, 6352, 337, 641, 1065, 9717, 11, 570, 286, 915, 881, 561, 286, 1710, 281, 294, 264, 957, 1002, 500, 380], "temperature": 0.0, "avg_logprob": -0.1438352411443537, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.223406777891796e-06}, {"id": 945, "seek": 609330, "start": 6120.9800000000005, "end": 6122.46, "text": " have a million images.", "tokens": [362, 257, 2459, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1438352411443537, "compression_ratio": 1.6758893280632412, "no_speech_prob": 9.223406777891796e-06}, {"id": 946, "seek": 612246, "start": 6122.46, "end": 6129.66, "text": " They have somewhere between about 2000 and 20,000 images, seems to be much more common.", "tokens": [814, 362, 4079, 1296, 466, 8132, 293, 945, 11, 1360, 5267, 11, 2544, 281, 312, 709, 544, 2689, 13], "temperature": 0.0, "avg_logprob": -0.15086945720102596, "compression_ratio": 1.5851528384279476, "no_speech_prob": 5.507562036655145e-06}, {"id": 947, "seek": 612246, "start": 6129.66, "end": 6137.9, "text": " So I'm very interested in having fewer rows because I think it's more valuable in practice.", "tokens": [407, 286, 478, 588, 3102, 294, 1419, 13366, 13241, 570, 286, 519, 309, 311, 544, 8263, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.15086945720102596, "compression_ratio": 1.5851528384279476, "no_speech_prob": 5.507562036655145e-06}, {"id": 948, "seek": 612246, "start": 6137.9, "end": 6142.16, "text": " I'm also pretty interested in small images, not just for the reason you mentioned, which", "tokens": [286, 478, 611, 1238, 3102, 294, 1359, 5267, 11, 406, 445, 337, 264, 1778, 291, 2835, 11, 597], "temperature": 0.0, "avg_logprob": -0.15086945720102596, "compression_ratio": 1.5851528384279476, "no_speech_prob": 5.507562036655145e-06}, {"id": 949, "seek": 612246, "start": 6142.16, "end": 6148.7, "text": " is it allows me to test things out more quickly, but also as I mentioned before, often a small", "tokens": [307, 309, 4045, 385, 281, 1500, 721, 484, 544, 2661, 11, 457, 611, 382, 286, 2835, 949, 11, 2049, 257, 1359], "temperature": 0.0, "avg_logprob": -0.15086945720102596, "compression_ratio": 1.5851528384279476, "no_speech_prob": 5.507562036655145e-06}, {"id": 950, "seek": 614870, "start": 6148.7, "end": 6152.58, "text": " part of an image actually turns out to be what you're interested in, that's certainly", "tokens": [644, 295, 364, 3256, 767, 4523, 484, 281, 312, 437, 291, 434, 3102, 294, 11, 300, 311, 3297], "temperature": 0.0, "avg_logprob": -0.2715870069420856, "compression_ratio": 1.579925650557621, "no_speech_prob": 2.111161666107364e-05}, {"id": 951, "seek": 614870, "start": 6152.58, "end": 6157.179999999999, "text": " true in medicine.", "tokens": [2074, 294, 7195, 13], "temperature": 0.0, "avg_logprob": -0.2715870069420856, "compression_ratio": 1.579925650557621, "no_speech_prob": 2.111161666107364e-05}, {"id": 952, "seek": 614870, "start": 6157.179999999999, "end": 6159.0599999999995, "text": " I have two questions.", "tokens": [286, 362, 732, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2715870069420856, "compression_ratio": 1.579925650557621, "no_speech_prob": 2.111161666107364e-05}, {"id": 953, "seek": 614870, "start": 6159.0599999999995, "end": 6164.7, "text": " The first is on what you mentioned in terms of small data sets, particularly medical imaging.", "tokens": [440, 700, 307, 322, 437, 291, 2835, 294, 2115, 295, 1359, 1412, 6352, 11, 4098, 4625, 25036, 13], "temperature": 0.0, "avg_logprob": -0.2715870069420856, "compression_ratio": 1.579925650557621, "no_speech_prob": 2.111161666107364e-05}, {"id": 954, "seek": 614870, "start": 6164.7, "end": 6168.9, "text": " If you've heard of, I guess, Vicarious, a startup in the specialization in one-shot", "tokens": [759, 291, 600, 2198, 295, 11, 286, 2041, 11, 691, 7953, 851, 11, 257, 18578, 294, 264, 2121, 2144, 294, 472, 12, 18402], "temperature": 0.0, "avg_logprob": -0.2715870069420856, "compression_ratio": 1.579925650557621, "no_speech_prob": 2.111161666107364e-05}, {"id": 955, "seek": 614870, "start": 6168.9, "end": 6170.78, "text": " learning, so your opinions on that.", "tokens": [2539, 11, 370, 428, 11819, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.2715870069420856, "compression_ratio": 1.579925650557621, "no_speech_prob": 2.111161666107364e-05}, {"id": 956, "seek": 614870, "start": 6170.78, "end": 6178.0599999999995, "text": " And then the second being, this is related to Ali's talk at NIPS, I don't want to say", "tokens": [400, 550, 264, 1150, 885, 11, 341, 307, 4077, 281, 12020, 311, 751, 412, 18482, 6273, 11, 286, 500, 380, 528, 281, 584], "temperature": 0.0, "avg_logprob": -0.2715870069420856, "compression_ratio": 1.579925650557621, "no_speech_prob": 2.111161666107364e-05}, {"id": 957, "seek": 617806, "start": 6178.06, "end": 6183.580000000001, "text": " it's controversial, but like Yan-LeCun, there was a really, I guess, controversial thread", "tokens": [309, 311, 17323, 11, 457, 411, 13633, 12, 11020, 34, 409, 11, 456, 390, 257, 534, 11, 286, 2041, 11, 17323, 7207], "temperature": 0.0, "avg_logprob": -0.2542815889630999, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.0005602812161669135}, {"id": 958, "seek": 617806, "start": 6183.580000000001, "end": 6188.740000000001, "text": " attacking it in terms of what you're talking about as a baseline of theory, just not keeping", "tokens": [15010, 309, 294, 2115, 295, 437, 291, 434, 1417, 466, 382, 257, 20518, 295, 5261, 11, 445, 406, 5145], "temperature": 0.0, "avg_logprob": -0.2542815889630999, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.0005602812161669135}, {"id": 959, "seek": 617806, "start": 6188.740000000001, "end": 6191.14, "text": " up with practice.", "tokens": [493, 365, 3124, 13], "temperature": 0.0, "avg_logprob": -0.2542815889630999, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.0005602812161669135}, {"id": 960, "seek": 617806, "start": 6191.14, "end": 6195.820000000001, "text": " And so I guess I was studying with Yan, whereas Ali actually, he tweeted at me quite a bit", "tokens": [400, 370, 286, 2041, 286, 390, 7601, 365, 13633, 11, 9735, 12020, 767, 11, 415, 25646, 412, 385, 1596, 257, 857], "temperature": 0.0, "avg_logprob": -0.2542815889630999, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.0005602812161669135}, {"id": 961, "seek": 617806, "start": 6195.820000000001, "end": 6204.26, "text": " trying to defend he wasn't attacking Yan at all, but in fact, he was trying to support", "tokens": [1382, 281, 8602, 415, 2067, 380, 15010, 13633, 412, 439, 11, 457, 294, 1186, 11, 415, 390, 1382, 281, 1406], "temperature": 0.0, "avg_logprob": -0.2542815889630999, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.0005602812161669135}, {"id": 962, "seek": 620426, "start": 6204.26, "end": 6209.900000000001, "text": " him, but I just kind of feel like a lot of theory as you go is just sort of out of date", "tokens": [796, 11, 457, 286, 445, 733, 295, 841, 411, 257, 688, 295, 5261, 382, 291, 352, 307, 445, 1333, 295, 484, 295, 4002], "temperature": 0.0, "avg_logprob": -0.18912581856369126, "compression_ratio": 1.7898089171974523, "no_speech_prob": 1.8057251509162597e-05}, {"id": 963, "seek": 620426, "start": 6209.900000000001, "end": 6214.26, "text": " and it's hard to keep up other than an archive from Andre Kaparthi to keep up.", "tokens": [293, 309, 311, 1152, 281, 1066, 493, 661, 813, 364, 23507, 490, 20667, 21216, 18352, 72, 281, 1066, 493, 13], "temperature": 0.0, "avg_logprob": -0.18912581856369126, "compression_ratio": 1.7898089171974523, "no_speech_prob": 1.8057251509162597e-05}, {"id": 964, "seek": 620426, "start": 6214.26, "end": 6217.900000000001, "text": " But if the theory isn't keeping up, but the industry is the one that's actually setting", "tokens": [583, 498, 264, 5261, 1943, 380, 5145, 493, 11, 457, 264, 3518, 307, 264, 472, 300, 311, 767, 3287], "temperature": 0.0, "avg_logprob": -0.18912581856369126, "compression_ratio": 1.7898089171974523, "no_speech_prob": 1.8057251509162597e-05}, {"id": 965, "seek": 620426, "start": 6217.900000000001, "end": 6223.3, "text": " the standard, then doesn't that mean that people who are actual practitioners are the", "tokens": [264, 3832, 11, 550, 1177, 380, 300, 914, 300, 561, 567, 366, 3539, 25742, 366, 264], "temperature": 0.0, "avg_logprob": -0.18912581856369126, "compression_ratio": 1.7898089171974523, "no_speech_prob": 1.8057251509162597e-05}, {"id": 966, "seek": 620426, "start": 6223.3, "end": 6227.1, "text": " ones like Yan-LeCun that are publishing the theory that are keeping up to date, whereas", "tokens": [2306, 411, 13633, 12, 11020, 34, 409, 300, 366, 17832, 264, 5261, 300, 366, 5145, 493, 281, 4002, 11, 9735], "temperature": 0.0, "avg_logprob": -0.18912581856369126, "compression_ratio": 1.7898089171974523, "no_speech_prob": 1.8057251509162597e-05}, {"id": 967, "seek": 620426, "start": 6227.1, "end": 6229.46, "text": " academic research institutions are actually behind?", "tokens": [7778, 2132, 8142, 366, 767, 2261, 30], "temperature": 0.0, "avg_logprob": -0.18912581856369126, "compression_ratio": 1.7898089171974523, "no_speech_prob": 1.8057251509162597e-05}, {"id": 968, "seek": 620426, "start": 6229.46, "end": 6232.9800000000005, "text": " So I don't have any comments on the Vicarious papers because I haven't read them.", "tokens": [407, 286, 500, 380, 362, 604, 3053, 322, 264, 691, 7953, 851, 10577, 570, 286, 2378, 380, 1401, 552, 13], "temperature": 0.0, "avg_logprob": -0.18912581856369126, "compression_ratio": 1.7898089171974523, "no_speech_prob": 1.8057251509162597e-05}, {"id": 969, "seek": 623298, "start": 6232.98, "end": 6241.0199999999995, "text": " I'm not aware of any of them as actually showing better results than other papers, but I think", "tokens": [286, 478, 406, 3650, 295, 604, 295, 552, 382, 767, 4099, 1101, 3542, 813, 661, 10577, 11, 457, 286, 519], "temperature": 0.0, "avg_logprob": -0.13708529521509544, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.1443949663080275e-05}, {"id": 970, "seek": 623298, "start": 6241.0199999999995, "end": 6245.74, "text": " they've come a long way in the last 12 months, so that might be wrong.", "tokens": [436, 600, 808, 257, 938, 636, 294, 264, 1036, 2272, 2493, 11, 370, 300, 1062, 312, 2085, 13], "temperature": 0.0, "avg_logprob": -0.13708529521509544, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.1443949663080275e-05}, {"id": 971, "seek": 623298, "start": 6245.74, "end": 6249.219999999999, "text": " I think the discussion between Yan-LeCun and Ali Rahimi is very interesting because they're", "tokens": [286, 519, 264, 5017, 1296, 13633, 12, 11020, 34, 409, 293, 12020, 17844, 10121, 307, 588, 1880, 570, 436, 434], "temperature": 0.0, "avg_logprob": -0.13708529521509544, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.1443949663080275e-05}, {"id": 972, "seek": 623298, "start": 6249.219999999999, "end": 6252.7, "text": " both smart people who have interesting things to say.", "tokens": [1293, 4069, 561, 567, 362, 1880, 721, 281, 584, 13], "temperature": 0.0, "avg_logprob": -0.13708529521509544, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.1443949663080275e-05}, {"id": 973, "seek": 623298, "start": 6252.7, "end": 6260.0599999999995, "text": " Unfortunately, a lot of people took Ali's talk as meaning something which he says it", "tokens": [8590, 11, 257, 688, 295, 561, 1890, 12020, 311, 751, 382, 3620, 746, 597, 415, 1619, 309], "temperature": 0.0, "avg_logprob": -0.13708529521509544, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.1443949663080275e-05}, {"id": 974, "seek": 626006, "start": 6260.06, "end": 6264.900000000001, "text": " didn't mean, and when I listen to his talk, I'm not sure he didn't actually mean it at", "tokens": [994, 380, 914, 11, 293, 562, 286, 2140, 281, 702, 751, 11, 286, 478, 406, 988, 415, 994, 380, 767, 914, 309, 412], "temperature": 0.0, "avg_logprob": -0.1666296974557345, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.00010888346150750294}, {"id": 975, "seek": 626006, "start": 6264.900000000001, "end": 6268.06, "text": " the time, but he clearly doesn't mean it now.", "tokens": [264, 565, 11, 457, 415, 4448, 1177, 380, 914, 309, 586, 13], "temperature": 0.0, "avg_logprob": -0.1666296974557345, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.00010888346150750294}, {"id": 976, "seek": 626006, "start": 6268.06, "end": 6272.780000000001, "text": " He's now said many times he was not talking about theory, he was not saying we need more", "tokens": [634, 311, 586, 848, 867, 1413, 415, 390, 406, 1417, 466, 5261, 11, 415, 390, 406, 1566, 321, 643, 544], "temperature": 0.0, "avg_logprob": -0.1666296974557345, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.00010888346150750294}, {"id": 977, "seek": 626006, "start": 6272.780000000001, "end": 6274.9800000000005, "text": " theory at all.", "tokens": [5261, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1666296974557345, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.00010888346150750294}, {"id": 978, "seek": 626006, "start": 6274.9800000000005, "end": 6277.38, "text": " Actually he thinks we need more experiments.", "tokens": [5135, 415, 7309, 321, 643, 544, 12050, 13], "temperature": 0.0, "avg_logprob": -0.1666296974557345, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.00010888346150750294}, {"id": 979, "seek": 626006, "start": 6277.38, "end": 6282.900000000001, "text": " And so specifically, he's also now saying he wished he hadn't used the word rigor, which", "tokens": [400, 370, 4682, 11, 415, 311, 611, 586, 1566, 415, 25811, 415, 8782, 380, 1143, 264, 1349, 42191, 11, 597], "temperature": 0.0, "avg_logprob": -0.1666296974557345, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.00010888346150750294}, {"id": 980, "seek": 626006, "start": 6282.900000000001, "end": 6288.72, "text": " I also wish, because rigor is kind of meaningless and everybody can kind of say when he says", "tokens": [286, 611, 3172, 11, 570, 42191, 307, 733, 295, 33232, 293, 2201, 393, 733, 295, 584, 562, 415, 1619], "temperature": 0.0, "avg_logprob": -0.1666296974557345, "compression_ratio": 1.7604562737642586, "no_speech_prob": 0.00010888346150750294}, {"id": 981, "seek": 628872, "start": 6288.72, "end": 6295.72, "text": " rigor he means the specific thing I study.", "tokens": [42191, 415, 1355, 264, 2685, 551, 286, 2979, 13], "temperature": 0.0, "avg_logprob": -0.15031013907967034, "compression_ratio": 1.579646017699115, "no_speech_prob": 9.516016689303797e-06}, {"id": 982, "seek": 628872, "start": 6295.72, "end": 6300.12, "text": " So lots of people have kind of taken his talk as being like, oh yes, this proves that nobody", "tokens": [407, 3195, 295, 561, 362, 733, 295, 2726, 702, 751, 382, 885, 411, 11, 1954, 2086, 11, 341, 25019, 300, 5079], "temperature": 0.0, "avg_logprob": -0.15031013907967034, "compression_ratio": 1.579646017699115, "no_speech_prob": 9.516016689303797e-06}, {"id": 983, "seek": 628872, "start": 6300.12, "end": 6305.22, "text": " else should work in neural networks unless they are experts at the one thing I'm an expert", "tokens": [1646, 820, 589, 294, 18161, 9590, 5969, 436, 366, 8572, 412, 264, 472, 551, 286, 478, 364, 5844], "temperature": 0.0, "avg_logprob": -0.15031013907967034, "compression_ratio": 1.579646017699115, "no_speech_prob": 9.516016689303797e-06}, {"id": 984, "seek": 628872, "start": 6305.22, "end": 6306.22, "text": " in.", "tokens": [294, 13], "temperature": 0.0, "avg_logprob": -0.15031013907967034, "compression_ratio": 1.579646017699115, "no_speech_prob": 9.516016689303797e-06}, {"id": 985, "seek": 628872, "start": 6306.22, "end": 6311.66, "text": " So I'm going to catch up with him and talk more about this in January and hopefully we'll", "tokens": [407, 286, 478, 516, 281, 3745, 493, 365, 796, 293, 751, 544, 466, 341, 294, 7061, 293, 4696, 321, 603], "temperature": 0.0, "avg_logprob": -0.15031013907967034, "compression_ratio": 1.579646017699115, "no_speech_prob": 9.516016689303797e-06}, {"id": 986, "seek": 628872, "start": 6311.66, "end": 6313.18, "text": " figure some more stuff out together.", "tokens": [2573, 512, 544, 1507, 484, 1214, 13], "temperature": 0.0, "avg_logprob": -0.15031013907967034, "compression_ratio": 1.579646017699115, "no_speech_prob": 9.516016689303797e-06}, {"id": 987, "seek": 631318, "start": 6313.18, "end": 6322.06, "text": " But basically what we can clearly agree on, and I think Jan LeCun also agrees on, is careful", "tokens": [583, 1936, 437, 321, 393, 4448, 3986, 322, 11, 293, 286, 519, 4956, 1456, 34, 409, 611, 26383, 322, 11, 307, 5026], "temperature": 0.0, "avg_logprob": -0.1873761681949391, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.355115066166036e-05}, {"id": 988, "seek": 631318, "start": 6322.06, "end": 6324.700000000001, "text": " experiments are important.", "tokens": [12050, 366, 1021, 13], "temperature": 0.0, "avg_logprob": -0.1873761681949391, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.355115066166036e-05}, {"id": 989, "seek": 631318, "start": 6324.700000000001, "end": 6330.52, "text": " Just doing things on massive amounts of data, using massive amounts of TPUs or GPUs is not", "tokens": [1449, 884, 721, 322, 5994, 11663, 295, 1412, 11, 1228, 5994, 11663, 295, 314, 8115, 82, 420, 18407, 82, 307, 406], "temperature": 0.0, "avg_logprob": -0.1873761681949391, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.355115066166036e-05}, {"id": 990, "seek": 631318, "start": 6330.52, "end": 6335.66, "text": " interesting of itself, and we should instead try to design experiments that give us the", "tokens": [1880, 295, 2564, 11, 293, 321, 820, 2602, 853, 281, 1715, 12050, 300, 976, 505, 264], "temperature": 0.0, "avg_logprob": -0.1873761681949391, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.355115066166036e-05}, {"id": 991, "seek": 631318, "start": 6335.66, "end": 6338.820000000001, "text": " maximum amount of insight into what's going on.", "tokens": [6674, 2372, 295, 11269, 666, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.1873761681949391, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.355115066166036e-05}, {"id": 992, "seek": 633882, "start": 6338.82, "end": 6352.0599999999995, "text": " So Jeremy, is it a good statement to say something like, so dropout and bash norm are very different", "tokens": [407, 17809, 11, 307, 309, 257, 665, 5629, 281, 584, 746, 411, 11, 370, 3270, 346, 293, 46183, 2026, 366, 588, 819], "temperature": 0.0, "avg_logprob": -0.22316685318946838, "compression_ratio": 1.5402298850574712, "no_speech_prob": 8.475974755128846e-05}, {"id": 993, "seek": 633882, "start": 6352.0599999999995, "end": 6353.0599999999995, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.22316685318946838, "compression_ratio": 1.5402298850574712, "no_speech_prob": 8.475974755128846e-05}, {"id": 994, "seek": 633882, "start": 6353.0599999999995, "end": 6359.5, "text": " Dropout is a regularization technique, and bash norm has maybe some regularization effect,", "tokens": [17675, 346, 307, 257, 3890, 2144, 6532, 11, 293, 46183, 2026, 575, 1310, 512, 3890, 2144, 1802, 11], "temperature": 0.0, "avg_logprob": -0.22316685318946838, "compression_ratio": 1.5402298850574712, "no_speech_prob": 8.475974755128846e-05}, {"id": 995, "seek": 633882, "start": 6359.5, "end": 6366.62, "text": " but it's actually just about convergence of the optimization method.", "tokens": [457, 309, 311, 767, 445, 466, 32181, 295, 264, 19618, 3170, 13], "temperature": 0.0, "avg_logprob": -0.22316685318946838, "compression_ratio": 1.5402298850574712, "no_speech_prob": 8.475974755128846e-05}, {"id": 996, "seek": 636662, "start": 6366.62, "end": 6373.94, "text": " And I would further say, I can't see any reason not to use bash norm.", "tokens": [400, 286, 576, 3052, 584, 11, 286, 393, 380, 536, 604, 1778, 406, 281, 764, 46183, 2026, 13], "temperature": 0.0, "avg_logprob": -0.1123535478269899, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.854254479549127e-06}, {"id": 997, "seek": 636662, "start": 6373.94, "end": 6380.58, "text": " There are versions of bash norm that in certain situations turned out not to work so well,", "tokens": [821, 366, 9606, 295, 46183, 2026, 300, 294, 1629, 6851, 3574, 484, 406, 281, 589, 370, 731, 11], "temperature": 0.0, "avg_logprob": -0.1123535478269899, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.854254479549127e-06}, {"id": 998, "seek": 636662, "start": 6380.58, "end": 6386.28, "text": " but people have figured out ways around that for nearly every one of those situations now.", "tokens": [457, 561, 362, 8932, 484, 2098, 926, 300, 337, 6217, 633, 472, 295, 729, 6851, 586, 13], "temperature": 0.0, "avg_logprob": -0.1123535478269899, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.854254479549127e-06}, {"id": 999, "seek": 636662, "start": 6386.28, "end": 6390.82, "text": " So I would always seek to find a way to use bash norm.", "tokens": [407, 286, 576, 1009, 8075, 281, 915, 257, 636, 281, 764, 46183, 2026, 13], "temperature": 0.0, "avg_logprob": -0.1123535478269899, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.854254479549127e-06}, {"id": 1000, "seek": 639082, "start": 6390.82, "end": 6398.259999999999, "text": " It may be a little harder in RNNs at least, but even there, there are ways of doing bash", "tokens": [467, 815, 312, 257, 707, 6081, 294, 45702, 45, 82, 412, 1935, 11, 457, 754, 456, 11, 456, 366, 2098, 295, 884, 46183], "temperature": 0.0, "avg_logprob": -0.16985492183737558, "compression_ratio": 1.4678362573099415, "no_speech_prob": 6.048876002751058e-06}, {"id": 1001, "seek": 639082, "start": 6398.259999999999, "end": 6399.9, "text": " norm in RNNs as well.", "tokens": [2026, 294, 45702, 45, 82, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16985492183737558, "compression_ratio": 1.4678362573099415, "no_speech_prob": 6.048876002751058e-06}, {"id": 1002, "seek": 639082, "start": 6399.9, "end": 6404.86, "text": " So try and always use bash norm on every layer if you can.", "tokens": [407, 853, 293, 1009, 764, 46183, 2026, 322, 633, 4583, 498, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.16985492183737558, "compression_ratio": 1.4678362573099415, "no_speech_prob": 6.048876002751058e-06}, {"id": 1003, "seek": 639082, "start": 6404.86, "end": 6414.78, "text": " The question that somebody asked is, does it mean I can stop normalizing my data?", "tokens": [440, 1168, 300, 2618, 2351, 307, 11, 775, 309, 914, 286, 393, 1590, 2710, 3319, 452, 1412, 30], "temperature": 0.0, "avg_logprob": -0.16985492183737558, "compression_ratio": 1.4678362573099415, "no_speech_prob": 6.048876002751058e-06}, {"id": 1004, "seek": 641478, "start": 6414.78, "end": 6426.219999999999, "text": " It does, although do it anyway because it's not at all hard to do it, and at least that", "tokens": [467, 775, 11, 4878, 360, 309, 4033, 570, 309, 311, 406, 412, 439, 1152, 281, 360, 309, 11, 293, 412, 1935, 300], "temperature": 0.0, "avg_logprob": -0.1869467794895172, "compression_ratio": 1.456140350877193, "no_speech_prob": 4.4951666495762765e-06}, {"id": 1005, "seek": 641478, "start": 6426.219999999999, "end": 6434.74, "text": " way the people using your data, they kind of know how you've normalized it.", "tokens": [636, 264, 561, 1228, 428, 1412, 11, 436, 733, 295, 458, 577, 291, 600, 48704, 309, 13], "temperature": 0.0, "avg_logprob": -0.1869467794895172, "compression_ratio": 1.456140350877193, "no_speech_prob": 4.4951666495762765e-06}, {"id": 1006, "seek": 641478, "start": 6434.74, "end": 6439.42, "text": " And particularly with these issues around a lot of libraries, in my opinion, at least", "tokens": [400, 4098, 365, 613, 2663, 926, 257, 688, 295, 15148, 11, 294, 452, 4800, 11, 412, 1935], "temperature": 0.0, "avg_logprob": -0.1869467794895172, "compression_ratio": 1.456140350877193, "no_speech_prob": 4.4951666495762765e-06}, {"id": 1007, "seek": 643942, "start": 6439.42, "end": 6446.74, "text": " not my opinion, my experiments don't deal with bash norm correctly for pre-trained models.", "tokens": [406, 452, 4800, 11, 452, 12050, 500, 380, 2028, 365, 46183, 2026, 8944, 337, 659, 12, 17227, 2001, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1681931149828565, "compression_ratio": 1.548780487804878, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1008, "seek": 643942, "start": 6446.74, "end": 6452.38, "text": " Just remember that when somebody starts retraining, those averages and stuff are going to change", "tokens": [1449, 1604, 300, 562, 2618, 3719, 49356, 1760, 11, 729, 42257, 293, 1507, 366, 516, 281, 1319], "temperature": 0.0, "avg_logprob": -0.1681931149828565, "compression_ratio": 1.548780487804878, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1009, "seek": 643942, "start": 6452.38, "end": 6453.9, "text": " for your dataset.", "tokens": [337, 428, 28872, 13], "temperature": 0.0, "avg_logprob": -0.1681931149828565, "compression_ratio": 1.548780487804878, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1010, "seek": 643942, "start": 6453.9, "end": 6458.14, "text": " And so if your new dataset has very different input averages, it could really cause a lot", "tokens": [400, 370, 498, 428, 777, 28872, 575, 588, 819, 4846, 42257, 11, 309, 727, 534, 3082, 257, 688], "temperature": 0.0, "avg_logprob": -0.1681931149828565, "compression_ratio": 1.548780487804878, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1011, "seek": 643942, "start": 6458.14, "end": 6460.5, "text": " of problems.", "tokens": [295, 2740, 13], "temperature": 0.0, "avg_logprob": -0.1681931149828565, "compression_ratio": 1.548780487804878, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1012, "seek": 643942, "start": 6460.5, "end": 6466.42, "text": " So I went through a period where I actually stopped normalizing my data.", "tokens": [407, 286, 1437, 807, 257, 2896, 689, 286, 767, 5936, 2710, 3319, 452, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1681931149828565, "compression_ratio": 1.548780487804878, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1013, "seek": 646642, "start": 6466.42, "end": 6474.58, "text": " Things kind of worked, but it's probably not worth it.", "tokens": [9514, 733, 295, 2732, 11, 457, 309, 311, 1391, 406, 3163, 309, 13], "temperature": 0.0, "avg_logprob": -0.16183484833816003, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.0451499292685185e-05}, {"id": 1014, "seek": 646642, "start": 6474.58, "end": 6477.34, "text": " So the rest of this is identical.", "tokens": [407, 264, 1472, 295, 341, 307, 14800, 13], "temperature": 0.0, "avg_logprob": -0.16183484833816003, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.0451499292685185e-05}, {"id": 1015, "seek": 646642, "start": 6477.34, "end": 6482.14, "text": " All I've done is I've changed conv layer to bn layer.", "tokens": [1057, 286, 600, 1096, 307, 286, 600, 3105, 3754, 4583, 281, 272, 77, 4583, 13], "temperature": 0.0, "avg_logprob": -0.16183484833816003, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.0451499292685185e-05}, {"id": 1016, "seek": 646642, "start": 6482.14, "end": 6486.54, "text": " But I've done one more thing, which is I'm trying to get closer and closer to modern", "tokens": [583, 286, 600, 1096, 472, 544, 551, 11, 597, 307, 286, 478, 1382, 281, 483, 4966, 293, 4966, 281, 4363], "temperature": 0.0, "avg_logprob": -0.16183484833816003, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.0451499292685185e-05}, {"id": 1017, "seek": 646642, "start": 6486.54, "end": 6494.3, "text": " approaches, which I've added a single convolutional layer at the start with a bigger kernel size", "tokens": [11587, 11, 597, 286, 600, 3869, 257, 2167, 45216, 304, 4583, 412, 264, 722, 365, 257, 3801, 28256, 2744], "temperature": 0.0, "avg_logprob": -0.16183484833816003, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.0451499292685185e-05}, {"id": 1018, "seek": 649430, "start": 6494.3, "end": 6496.62, "text": " and a stride of 1.", "tokens": [293, 257, 1056, 482, 295, 502, 13], "temperature": 0.0, "avg_logprob": -0.15975990941969015, "compression_ratio": 1.4044117647058822, "no_speech_prob": 2.3320687887462554e-06}, {"id": 1019, "seek": 649430, "start": 6496.62, "end": 6498.66, "text": " Why have I done that?", "tokens": [1545, 362, 286, 1096, 300, 30], "temperature": 0.0, "avg_logprob": -0.15975990941969015, "compression_ratio": 1.4044117647058822, "no_speech_prob": 2.3320687887462554e-06}, {"id": 1020, "seek": 649430, "start": 6498.66, "end": 6506.02, "text": " So the basic idea is that I want my first layer to have a richer input.", "tokens": [407, 264, 3875, 1558, 307, 300, 286, 528, 452, 700, 4583, 281, 362, 257, 29021, 4846, 13], "temperature": 0.0, "avg_logprob": -0.15975990941969015, "compression_ratio": 1.4044117647058822, "no_speech_prob": 2.3320687887462554e-06}, {"id": 1021, "seek": 649430, "start": 6506.02, "end": 6511.38, "text": " So before my first layer had an input of just 3, because it's just 3 channels.", "tokens": [407, 949, 452, 700, 4583, 632, 364, 4846, 295, 445, 805, 11, 570, 309, 311, 445, 805, 9235, 13], "temperature": 0.0, "avg_logprob": -0.15975990941969015, "compression_ratio": 1.4044117647058822, "no_speech_prob": 2.3320687887462554e-06}, {"id": 1022, "seek": 651138, "start": 6511.38, "end": 6530.34, "text": " But if I start with my image, and I kind of take a bigger area, and I do a convolution", "tokens": [583, 498, 286, 722, 365, 452, 3256, 11, 293, 286, 733, 295, 747, 257, 3801, 1859, 11, 293, 286, 360, 257, 45216], "temperature": 0.0, "avg_logprob": -0.11146483971522404, "compression_ratio": 1.4047619047619047, "no_speech_prob": 2.9480124794645235e-06}, {"id": 1023, "seek": 651138, "start": 6530.34, "end": 6540.86, "text": " using that bigger area, in this case I'm doing 5x5, then that kind of allows me to try and", "tokens": [1228, 300, 3801, 1859, 11, 294, 341, 1389, 286, 478, 884, 1025, 87, 20, 11, 550, 300, 733, 295, 4045, 385, 281, 853, 293], "temperature": 0.0, "avg_logprob": -0.11146483971522404, "compression_ratio": 1.4047619047619047, "no_speech_prob": 2.9480124794645235e-06}, {"id": 1024, "seek": 654086, "start": 6540.86, "end": 6546.679999999999, "text": " find more interesting, richer features in that 5x5 area.", "tokens": [915, 544, 1880, 11, 29021, 4122, 294, 300, 1025, 87, 20, 1859, 13], "temperature": 0.0, "avg_logprob": -0.16056052121249112, "compression_ratio": 1.4591194968553458, "no_speech_prob": 2.368793957430171e-06}, {"id": 1025, "seek": 654086, "start": 6546.679999999999, "end": 6549.74, "text": " And so then I spit out a bigger output.", "tokens": [400, 370, 550, 286, 22127, 484, 257, 3801, 5598, 13], "temperature": 0.0, "avg_logprob": -0.16056052121249112, "compression_ratio": 1.4591194968553458, "no_speech_prob": 2.368793957430171e-06}, {"id": 1026, "seek": 654086, "start": 6549.74, "end": 6555.66, "text": " In this case I spit out 10 5x5 filters.", "tokens": [682, 341, 1389, 286, 22127, 484, 1266, 1025, 87, 20, 15995, 13], "temperature": 0.0, "avg_logprob": -0.16056052121249112, "compression_ratio": 1.4591194968553458, "no_speech_prob": 2.368793957430171e-06}, {"id": 1027, "seek": 654086, "start": 6555.66, "end": 6561.9, "text": " And so the idea is pretty much every state-of-the-art convolutional architecture now starts out", "tokens": [400, 370, 264, 1558, 307, 1238, 709, 633, 1785, 12, 2670, 12, 3322, 12, 446, 45216, 304, 9482, 586, 3719, 484], "temperature": 0.0, "avg_logprob": -0.16056052121249112, "compression_ratio": 1.4591194968553458, "no_speech_prob": 2.368793957430171e-06}, {"id": 1028, "seek": 656190, "start": 6561.9, "end": 6573.379999999999, "text": " with a single conv layer with a 5x5 or 7x7 or sometimes even 11x11 convolution with quite", "tokens": [365, 257, 2167, 3754, 4583, 365, 257, 1025, 87, 20, 420, 1614, 87, 22, 420, 2171, 754, 2975, 87, 5348, 45216, 365, 1596], "temperature": 0.0, "avg_logprob": -0.16566671224740837, "compression_ratio": 1.4049079754601228, "no_speech_prob": 8.13967108115321e-06}, {"id": 1029, "seek": 656190, "start": 6573.379999999999, "end": 6581.259999999999, "text": " a few filters, something like 32 filters coming out.", "tokens": [257, 1326, 15995, 11, 746, 411, 8858, 15995, 1348, 484, 13], "temperature": 0.0, "avg_logprob": -0.16566671224740837, "compression_ratio": 1.4049079754601228, "no_speech_prob": 8.13967108115321e-06}, {"id": 1030, "seek": 656190, "start": 6581.259999999999, "end": 6589.259999999999, "text": " And it's just a way of trying to, because I used a stride of 1 and a padding of kernel", "tokens": [400, 309, 311, 445, 257, 636, 295, 1382, 281, 11, 570, 286, 1143, 257, 1056, 482, 295, 502, 293, 257, 39562, 295, 28256], "temperature": 0.0, "avg_logprob": -0.16566671224740837, "compression_ratio": 1.4049079754601228, "no_speech_prob": 8.13967108115321e-06}, {"id": 1031, "seek": 658926, "start": 6589.26, "end": 6593.74, "text": " size minus 1 over 2, that means that my output is going to be exactly the same size as my", "tokens": [2744, 3175, 502, 670, 568, 11, 300, 1355, 300, 452, 5598, 307, 516, 281, 312, 2293, 264, 912, 2744, 382, 452], "temperature": 0.0, "avg_logprob": -0.15663228483281583, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.9480092962330673e-06}, {"id": 1032, "seek": 658926, "start": 6593.74, "end": 6597.42, "text": " input, but just got more filters.", "tokens": [4846, 11, 457, 445, 658, 544, 15995, 13], "temperature": 0.0, "avg_logprob": -0.15663228483281583, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.9480092962330673e-06}, {"id": 1033, "seek": 658926, "start": 6597.42, "end": 6603.46, "text": " That's just a good way of trying to create a richer starting point for my sequence of", "tokens": [663, 311, 445, 257, 665, 636, 295, 1382, 281, 1884, 257, 29021, 2891, 935, 337, 452, 8310, 295], "temperature": 0.0, "avg_logprob": -0.15663228483281583, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.9480092962330673e-06}, {"id": 1034, "seek": 658926, "start": 6603.46, "end": 6605.7, "text": " convolutional layers.", "tokens": [45216, 304, 7914, 13], "temperature": 0.0, "avg_logprob": -0.15663228483281583, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.9480092962330673e-06}, {"id": 1035, "seek": 658926, "start": 6605.7, "end": 6611.18, "text": " So that's the basic theory of why I've added this single convolution, which I just do once", "tokens": [407, 300, 311, 264, 3875, 5261, 295, 983, 286, 600, 3869, 341, 2167, 45216, 11, 597, 286, 445, 360, 1564], "temperature": 0.0, "avg_logprob": -0.15663228483281583, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.9480092962330673e-06}, {"id": 1036, "seek": 658926, "start": 6611.18, "end": 6616.46, "text": " at the start, and then I just go through all my layers, and then I do my adaptive max pooling", "tokens": [412, 264, 722, 11, 293, 550, 286, 445, 352, 807, 439, 452, 7914, 11, 293, 550, 286, 360, 452, 27912, 11469, 7005, 278], "temperature": 0.0, "avg_logprob": -0.15663228483281583, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.9480092962330673e-06}, {"id": 1037, "seek": 658926, "start": 6616.46, "end": 6619.04, "text": " and my final classifier layer.", "tokens": [293, 452, 2572, 1508, 9902, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15663228483281583, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.9480092962330673e-06}, {"id": 1038, "seek": 661904, "start": 6619.04, "end": 6622.98, "text": " So it's a minor tweak, but it helps.", "tokens": [407, 309, 311, 257, 6696, 29879, 11, 457, 309, 3665, 13], "temperature": 0.0, "avg_logprob": -0.21313335919621015, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.184306984214345e-05}, {"id": 1039, "seek": 661904, "start": 6622.98, "end": 6633.58, "text": " And so you'll see now I can go from 60% and after a couple it's 45%.", "tokens": [400, 370, 291, 603, 536, 586, 286, 393, 352, 490, 4060, 4, 293, 934, 257, 1916, 309, 311, 6905, 6856], "temperature": 0.0, "avg_logprob": -0.21313335919621015, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.184306984214345e-05}, {"id": 1040, "seek": 661904, "start": 6633.58, "end": 6636.16, "text": " Now after a couple it's 57%.", "tokens": [823, 934, 257, 1916, 309, 311, 21423, 6856], "temperature": 0.0, "avg_logprob": -0.21313335919621015, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.184306984214345e-05}, {"id": 1041, "seek": 661904, "start": 6636.16, "end": 6638.82, "text": " After a few more I'm up to 68%.", "tokens": [2381, 257, 1326, 544, 286, 478, 493, 281, 23317, 6856], "temperature": 0.0, "avg_logprob": -0.21313335919621015, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.184306984214345e-05}, {"id": 1042, "seek": 661904, "start": 6638.82, "end": 6645.22, "text": " So you can see the batch norm and a tiny bit of the conv layer at the start, it's helping.", "tokens": [407, 291, 393, 536, 264, 15245, 2026, 293, 257, 5870, 857, 295, 264, 3754, 4583, 412, 264, 722, 11, 309, 311, 4315, 13], "temperature": 0.0, "avg_logprob": -0.21313335919621015, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.184306984214345e-05}, {"id": 1043, "seek": 661904, "start": 6645.22, "end": 6648.88, "text": " And what's more, you can see this is still increasing.", "tokens": [400, 437, 311, 544, 11, 291, 393, 536, 341, 307, 920, 5662, 13], "temperature": 0.0, "avg_logprob": -0.21313335919621015, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.184306984214345e-05}, {"id": 1044, "seek": 664888, "start": 6648.88, "end": 6652.2, "text": " So that's looking pretty encouraging.", "tokens": [407, 300, 311, 1237, 1238, 14580, 13], "temperature": 0.0, "avg_logprob": -0.10821009194979103, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.844925231533125e-06}, {"id": 1045, "seek": 664888, "start": 6652.2, "end": 6662.02, "text": " So given that this is looking pretty good, an obvious thing to try is to try increasing", "tokens": [407, 2212, 300, 341, 307, 1237, 1238, 665, 11, 364, 6322, 551, 281, 853, 307, 281, 853, 5662], "temperature": 0.0, "avg_logprob": -0.10821009194979103, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.844925231533125e-06}, {"id": 1046, "seek": 664888, "start": 6662.02, "end": 6663.7, "text": " the depth of the model.", "tokens": [264, 7161, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10821009194979103, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.844925231533125e-06}, {"id": 1047, "seek": 664888, "start": 6663.7, "end": 6670.82, "text": " And now I can't just add more of my stride 2 layers because remember how it halved the", "tokens": [400, 586, 286, 393, 380, 445, 909, 544, 295, 452, 1056, 482, 568, 7914, 570, 1604, 577, 309, 7523, 937, 264], "temperature": 0.0, "avg_logprob": -0.10821009194979103, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.844925231533125e-06}, {"id": 1048, "seek": 664888, "start": 6670.82, "end": 6673.02, "text": " size of the image each time?", "tokens": [2744, 295, 264, 3256, 1184, 565, 30], "temperature": 0.0, "avg_logprob": -0.10821009194979103, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.844925231533125e-06}, {"id": 1049, "seek": 664888, "start": 6673.02, "end": 6678.1, "text": " I'm basically down to 2x2 at the end, so I can't add much more.", "tokens": [286, 478, 1936, 760, 281, 568, 87, 17, 412, 264, 917, 11, 370, 286, 393, 380, 909, 709, 544, 13], "temperature": 0.0, "avg_logprob": -0.10821009194979103, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.844925231533125e-06}, {"id": 1050, "seek": 667810, "start": 6678.1, "end": 6683.660000000001, "text": " So what I did instead was I said, okay, here's my original layers, these are my stride 2", "tokens": [407, 437, 286, 630, 2602, 390, 286, 848, 11, 1392, 11, 510, 311, 452, 3380, 7914, 11, 613, 366, 452, 1056, 482, 568], "temperature": 0.0, "avg_logprob": -0.13795329556606784, "compression_ratio": 1.7621621621621621, "no_speech_prob": 3.0894868814357324e-06}, {"id": 1051, "seek": 667810, "start": 6683.660000000001, "end": 6684.660000000001, "text": " layers.", "tokens": [7914, 13], "temperature": 0.0, "avg_logprob": -0.13795329556606784, "compression_ratio": 1.7621621621621621, "no_speech_prob": 3.0894868814357324e-06}, {"id": 1052, "seek": 667810, "start": 6684.660000000001, "end": 6688.320000000001, "text": " For every one, also create a stride 1 layer.", "tokens": [1171, 633, 472, 11, 611, 1884, 257, 1056, 482, 502, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13795329556606784, "compression_ratio": 1.7621621621621621, "no_speech_prob": 3.0894868814357324e-06}, {"id": 1053, "seek": 667810, "start": 6688.320000000001, "end": 6690.9800000000005, "text": " So a stride 1 layer doesn't change the size.", "tokens": [407, 257, 1056, 482, 502, 4583, 1177, 380, 1319, 264, 2744, 13], "temperature": 0.0, "avg_logprob": -0.13795329556606784, "compression_ratio": 1.7621621621621621, "no_speech_prob": 3.0894868814357324e-06}, {"id": 1054, "seek": 667810, "start": 6690.9800000000005, "end": 6698.46, "text": " And so now I'm saying zip my stride 2 layers and my stride 1 layers together.", "tokens": [400, 370, 586, 286, 478, 1566, 20730, 452, 1056, 482, 568, 7914, 293, 452, 1056, 482, 502, 7914, 1214, 13], "temperature": 0.0, "avg_logprob": -0.13795329556606784, "compression_ratio": 1.7621621621621621, "no_speech_prob": 3.0894868814357324e-06}, {"id": 1055, "seek": 667810, "start": 6698.46, "end": 6702.34, "text": " And so first of all do the stride 2 and then do the stride 1.", "tokens": [400, 370, 700, 295, 439, 360, 264, 1056, 482, 568, 293, 550, 360, 264, 1056, 482, 502, 13], "temperature": 0.0, "avg_logprob": -0.13795329556606784, "compression_ratio": 1.7621621621621621, "no_speech_prob": 3.0894868814357324e-06}, {"id": 1056, "seek": 670234, "start": 6702.34, "end": 6710.7, "text": " So this is now actually twice as deep.", "tokens": [407, 341, 307, 586, 767, 6091, 382, 2452, 13], "temperature": 0.0, "avg_logprob": -0.13420281465026154, "compression_ratio": 1.525, "no_speech_prob": 1.3709560562347178e-06}, {"id": 1057, "seek": 670234, "start": 6710.7, "end": 6718.78, "text": " So this is now twice as deep, but I end up with the exact same 2x2 that I had before.", "tokens": [407, 341, 307, 586, 6091, 382, 2452, 11, 457, 286, 917, 493, 365, 264, 1900, 912, 568, 87, 17, 300, 286, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.13420281465026154, "compression_ratio": 1.525, "no_speech_prob": 1.3709560562347178e-06}, {"id": 1058, "seek": 670234, "start": 6718.78, "end": 6727.02, "text": " And so if I try this, here after 1, 2, 3, 4 epochs is at 65%, after 1, 2, 3 epochs,", "tokens": [400, 370, 498, 286, 853, 341, 11, 510, 934, 502, 11, 568, 11, 805, 11, 1017, 30992, 28346, 307, 412, 11624, 8923, 934, 502, 11, 568, 11, 805, 30992, 28346, 11], "temperature": 0.0, "avg_logprob": -0.13420281465026154, "compression_ratio": 1.525, "no_speech_prob": 1.3709560562347178e-06}, {"id": 1059, "seek": 670234, "start": 6727.02, "end": 6728.54, "text": " I'm still at 65%.", "tokens": [286, 478, 920, 412, 11624, 6856], "temperature": 0.0, "avg_logprob": -0.13420281465026154, "compression_ratio": 1.525, "no_speech_prob": 1.3709560562347178e-06}, {"id": 1060, "seek": 670234, "start": 6728.54, "end": 6730.9800000000005, "text": " It hasn't helped.", "tokens": [467, 6132, 380, 4254, 13], "temperature": 0.0, "avg_logprob": -0.13420281465026154, "compression_ratio": 1.525, "no_speech_prob": 1.3709560562347178e-06}, {"id": 1061, "seek": 673098, "start": 6730.98, "end": 6739.78, "text": " And so the reason it hasn't helped is I'm now too deep even for batch norm to handle", "tokens": [400, 370, 264, 1778, 309, 6132, 380, 4254, 307, 286, 478, 586, 886, 2452, 754, 337, 15245, 2026, 281, 4813], "temperature": 0.0, "avg_logprob": -0.15289520949460148, "compression_ratio": 1.4761904761904763, "no_speech_prob": 8.315272452819045e-07}, {"id": 1062, "seek": 673098, "start": 6739.78, "end": 6740.78, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.15289520949460148, "compression_ratio": 1.4761904761904763, "no_speech_prob": 8.315272452819045e-07}, {"id": 1063, "seek": 673098, "start": 6740.78, "end": 6751.139999999999, "text": " So my depth is now 1, 2, 3, 4, 5 times 2 is 10, 11, conv1, 12.", "tokens": [407, 452, 7161, 307, 586, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 1413, 568, 307, 1266, 11, 2975, 11, 3754, 16, 11, 2272, 13], "temperature": 0.0, "avg_logprob": -0.15289520949460148, "compression_ratio": 1.4761904761904763, "no_speech_prob": 8.315272452819045e-07}, {"id": 1064, "seek": 673098, "start": 6751.139999999999, "end": 6756.86, "text": " So 12 layers deep, it's possible to train a standard conv net 12 layers deep, but it", "tokens": [407, 2272, 7914, 2452, 11, 309, 311, 1944, 281, 3847, 257, 3832, 3754, 2533, 2272, 7914, 2452, 11, 457, 309], "temperature": 0.0, "avg_logprob": -0.15289520949460148, "compression_ratio": 1.4761904761904763, "no_speech_prob": 8.315272452819045e-07}, {"id": 1065, "seek": 673098, "start": 6756.86, "end": 6759.54, "text": " starts to get difficult to do it properly.", "tokens": [3719, 281, 483, 2252, 281, 360, 309, 6108, 13], "temperature": 0.0, "avg_logprob": -0.15289520949460148, "compression_ratio": 1.4761904761904763, "no_speech_prob": 8.315272452819045e-07}, {"id": 1066, "seek": 675954, "start": 6759.54, "end": 6763.14, "text": " And it certainly doesn't seem to be really helping much, if at all.", "tokens": [400, 309, 3297, 1177, 380, 1643, 281, 312, 534, 4315, 709, 11, 498, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.16103223653940055, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.785081728186924e-06}, {"id": 1067, "seek": 675954, "start": 6763.14, "end": 6769.14, "text": " So that's where I'm instead going to replace this with a resnet.", "tokens": [407, 300, 311, 689, 286, 478, 2602, 516, 281, 7406, 341, 365, 257, 725, 7129, 13], "temperature": 0.0, "avg_logprob": -0.16103223653940055, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.785081728186924e-06}, {"id": 1068, "seek": 675954, "start": 6769.14, "end": 6771.98, "text": " So a resnet is our final stage.", "tokens": [407, 257, 725, 7129, 307, 527, 2572, 3233, 13], "temperature": 0.0, "avg_logprob": -0.16103223653940055, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.785081728186924e-06}, {"id": 1069, "seek": 675954, "start": 6771.98, "end": 6778.98, "text": " And what a resnet does is I'm going to replace our bn layer, I'm going to inherit from bn", "tokens": [400, 437, 257, 725, 7129, 775, 307, 286, 478, 516, 281, 7406, 527, 272, 77, 4583, 11, 286, 478, 516, 281, 21389, 490, 272, 77], "temperature": 0.0, "avg_logprob": -0.16103223653940055, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.785081728186924e-06}, {"id": 1070, "seek": 675954, "start": 6778.98, "end": 6784.12, "text": " layer, and replace our forward with that.", "tokens": [4583, 11, 293, 7406, 527, 2128, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.16103223653940055, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.785081728186924e-06}, {"id": 1071, "seek": 675954, "start": 6784.12, "end": 6785.5, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.16103223653940055, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.785081728186924e-06}, {"id": 1072, "seek": 675954, "start": 6785.5, "end": 6787.86, "text": " Everything else is going to be identical.", "tokens": [5471, 1646, 307, 516, 281, 312, 14800, 13], "temperature": 0.0, "avg_logprob": -0.16103223653940055, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.785081728186924e-06}, {"id": 1073, "seek": 678786, "start": 6787.86, "end": 6793.179999999999, "text": " But now I'm going to do way lots of layers, I'm going to make it 4 times deeper, and it's", "tokens": [583, 586, 286, 478, 516, 281, 360, 636, 3195, 295, 7914, 11, 286, 478, 516, 281, 652, 309, 1017, 1413, 7731, 11, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.12570197464989835, "compression_ratio": 1.3904109589041096, "no_speech_prob": 9.87462954071816e-07}, {"id": 1074, "seek": 678786, "start": 6793.179999999999, "end": 6798.7, "text": " going to train beautifully just because of that.", "tokens": [516, 281, 3847, 16525, 445, 570, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.12570197464989835, "compression_ratio": 1.3904109589041096, "no_speech_prob": 9.87462954071816e-07}, {"id": 1075, "seek": 678786, "start": 6798.7, "end": 6802.139999999999, "text": " So why does that help so much?", "tokens": [407, 983, 775, 300, 854, 370, 709, 30], "temperature": 0.0, "avg_logprob": -0.12570197464989835, "compression_ratio": 1.3904109589041096, "no_speech_prob": 9.87462954071816e-07}, {"id": 1076, "seek": 678786, "start": 6802.139999999999, "end": 6804.46, "text": " So this is called a resnet block.", "tokens": [407, 341, 307, 1219, 257, 725, 7129, 3461, 13], "temperature": 0.0, "avg_logprob": -0.12570197464989835, "compression_ratio": 1.3904109589041096, "no_speech_prob": 9.87462954071816e-07}, {"id": 1077, "seek": 680446, "start": 6804.46, "end": 6820.18, "text": " And as you can see, I'm saying my predictions equals my input plus some function, in this", "tokens": [400, 382, 291, 393, 536, 11, 286, 478, 1566, 452, 21264, 6915, 452, 4846, 1804, 512, 2445, 11, 294, 341], "temperature": 0.0, "avg_logprob": -0.15018382183341092, "compression_ratio": 1.256198347107438, "no_speech_prob": 5.25538371221046e-06}, {"id": 1078, "seek": 680446, "start": 6820.18, "end": 6824.34, "text": " case a convolution of my input.", "tokens": [1389, 257, 45216, 295, 452, 4846, 13], "temperature": 0.0, "avg_logprob": -0.15018382183341092, "compression_ratio": 1.256198347107438, "no_speech_prob": 5.25538371221046e-06}, {"id": 1079, "seek": 680446, "start": 6824.34, "end": 6827.46, "text": " That's what I've written here.", "tokens": [663, 311, 437, 286, 600, 3720, 510, 13], "temperature": 0.0, "avg_logprob": -0.15018382183341092, "compression_ratio": 1.256198347107438, "no_speech_prob": 5.25538371221046e-06}, {"id": 1080, "seek": 682746, "start": 6827.46, "end": 6841.5, "text": " And so I'm now going to shuffle that around a little bit, and I'm going to say f of x", "tokens": [400, 370, 286, 478, 586, 516, 281, 39426, 300, 926, 257, 707, 857, 11, 293, 286, 478, 516, 281, 584, 283, 295, 2031], "temperature": 0.0, "avg_logprob": -0.16306614053660426, "compression_ratio": 1.4014598540145986, "no_speech_prob": 2.026140236921492e-06}, {"id": 1081, "seek": 682746, "start": 6841.5, "end": 6845.5, "text": " equals y minus x.", "tokens": [6915, 288, 3175, 2031, 13], "temperature": 0.0, "avg_logprob": -0.16306614053660426, "compression_ratio": 1.4014598540145986, "no_speech_prob": 2.026140236921492e-06}, {"id": 1082, "seek": 682746, "start": 6845.5, "end": 6849.5, "text": " So that's the same thing, shuffled around.", "tokens": [407, 300, 311, 264, 912, 551, 11, 402, 33974, 926, 13], "temperature": 0.0, "avg_logprob": -0.16306614053660426, "compression_ratio": 1.4014598540145986, "no_speech_prob": 2.026140236921492e-06}, {"id": 1083, "seek": 682746, "start": 6849.5, "end": 6854.74, "text": " That's my prediction from the previous layer.", "tokens": [663, 311, 452, 17630, 490, 264, 3894, 4583, 13], "temperature": 0.0, "avg_logprob": -0.16306614053660426, "compression_ratio": 1.4014598540145986, "no_speech_prob": 2.026140236921492e-06}, {"id": 1084, "seek": 685474, "start": 6854.74, "end": 6860.58, "text": " And so what this is then doing is it's trying to fit a function to the difference between", "tokens": [400, 370, 437, 341, 307, 550, 884, 307, 309, 311, 1382, 281, 3318, 257, 2445, 281, 264, 2649, 1296], "temperature": 0.0, "avg_logprob": -0.08054324296804574, "compression_ratio": 1.3831775700934579, "no_speech_prob": 4.88830096401216e-07}, {"id": 1085, "seek": 685474, "start": 6860.58, "end": 6862.62, "text": " these two.", "tokens": [613, 732, 13], "temperature": 0.0, "avg_logprob": -0.08054324296804574, "compression_ratio": 1.3831775700934579, "no_speech_prob": 4.88830096401216e-07}, {"id": 1086, "seek": 685474, "start": 6862.62, "end": 6876.9, "text": " And so the difference is actually the residual.", "tokens": [400, 370, 264, 2649, 307, 767, 264, 27980, 13], "temperature": 0.0, "avg_logprob": -0.08054324296804574, "compression_ratio": 1.3831775700934579, "no_speech_prob": 4.88830096401216e-07}, {"id": 1087, "seek": 687690, "start": 6876.9, "end": 6885.58, "text": " So if this is what I'm trying to calculate, my actual y value, and this is the thing that", "tokens": [407, 498, 341, 307, 437, 286, 478, 1382, 281, 8873, 11, 452, 3539, 288, 2158, 11, 293, 341, 307, 264, 551, 300], "temperature": 0.0, "avg_logprob": -0.09611009597778321, "compression_ratio": 1.6349206349206349, "no_speech_prob": 1.1544605058588786e-06}, {"id": 1088, "seek": 687690, "start": 6885.58, "end": 6891.179999999999, "text": " I've most recently calculated, then the difference between the two is basically the error in", "tokens": [286, 600, 881, 3938, 15598, 11, 550, 264, 2649, 1296, 264, 732, 307, 1936, 264, 6713, 294], "temperature": 0.0, "avg_logprob": -0.09611009597778321, "compression_ratio": 1.6349206349206349, "no_speech_prob": 1.1544605058588786e-06}, {"id": 1089, "seek": 687690, "start": 6891.179999999999, "end": 6893.94, "text": " terms of what I've calculated so far.", "tokens": [2115, 295, 437, 286, 600, 15598, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.09611009597778321, "compression_ratio": 1.6349206349206349, "no_speech_prob": 1.1544605058588786e-06}, {"id": 1090, "seek": 687690, "start": 6893.94, "end": 6901.219999999999, "text": " And so this is therefore saying try to find a set of convolutional weights that attempts", "tokens": [400, 370, 341, 307, 4412, 1566, 853, 281, 915, 257, 992, 295, 45216, 304, 17443, 300, 15257], "temperature": 0.0, "avg_logprob": -0.09611009597778321, "compression_ratio": 1.6349206349206349, "no_speech_prob": 1.1544605058588786e-06}, {"id": 1091, "seek": 690122, "start": 6901.22, "end": 6906.9800000000005, "text": " to fill in the amount I was off by.", "tokens": [281, 2836, 294, 264, 2372, 286, 390, 766, 538, 13], "temperature": 0.0, "avg_logprob": -0.15200074513753256, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.9638025605672738e-06}, {"id": 1092, "seek": 690122, "start": 6906.9800000000005, "end": 6917.64, "text": " So in other words, if we have some inputs coming in, and then we have this function", "tokens": [407, 294, 661, 2283, 11, 498, 321, 362, 512, 15743, 1348, 294, 11, 293, 550, 321, 362, 341, 2445], "temperature": 0.0, "avg_logprob": -0.15200074513753256, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.9638025605672738e-06}, {"id": 1093, "seek": 690122, "start": 6917.64, "end": 6924.62, "text": " which is basically trying to predict the error, it's like how much are we off by, and then", "tokens": [597, 307, 1936, 1382, 281, 6069, 264, 6713, 11, 309, 311, 411, 577, 709, 366, 321, 766, 538, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.15200074513753256, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.9638025605672738e-06}, {"id": 1094, "seek": 690122, "start": 6924.62, "end": 6929.66, "text": " we add that on, so we basically add on this additional prediction of how much we were", "tokens": [321, 909, 300, 322, 11, 370, 321, 1936, 909, 322, 341, 4497, 17630, 295, 577, 709, 321, 645], "temperature": 0.0, "avg_logprob": -0.15200074513753256, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.9638025605672738e-06}, {"id": 1095, "seek": 692966, "start": 6929.66, "end": 6935.3, "text": " wrong by, and then we add on another prediction of how much were we wrong by that time, and", "tokens": [2085, 538, 11, 293, 550, 321, 909, 322, 1071, 17630, 295, 577, 709, 645, 321, 2085, 538, 300, 565, 11, 293], "temperature": 0.0, "avg_logprob": -0.1766457203005956, "compression_ratio": 2.052863436123348, "no_speech_prob": 4.495171197049785e-06}, {"id": 1096, "seek": 692966, "start": 6935.3, "end": 6939.0599999999995, "text": " add on another prediction of how much were we wrong by that time.", "tokens": [909, 322, 1071, 17630, 295, 577, 709, 645, 321, 2085, 538, 300, 565, 13], "temperature": 0.0, "avg_logprob": -0.1766457203005956, "compression_ratio": 2.052863436123348, "no_speech_prob": 4.495171197049785e-06}, {"id": 1097, "seek": 692966, "start": 6939.0599999999995, "end": 6945.74, "text": " Then each time we're kind of zooming in, getting closer and closer to our correct answer.", "tokens": [1396, 1184, 565, 321, 434, 733, 295, 48226, 294, 11, 1242, 4966, 293, 4966, 281, 527, 3006, 1867, 13], "temperature": 0.0, "avg_logprob": -0.1766457203005956, "compression_ratio": 2.052863436123348, "no_speech_prob": 4.495171197049785e-06}, {"id": 1098, "seek": 692966, "start": 6945.74, "end": 6951.74, "text": " At each time we're saying, we've got to a certain point, but we've still got an error,", "tokens": [1711, 1184, 565, 321, 434, 1566, 11, 321, 600, 658, 281, 257, 1629, 935, 11, 457, 321, 600, 920, 658, 364, 6713, 11], "temperature": 0.0, "avg_logprob": -0.1766457203005956, "compression_ratio": 2.052863436123348, "no_speech_prob": 4.495171197049785e-06}, {"id": 1099, "seek": 692966, "start": 6951.74, "end": 6957.38, "text": " we've still got a residual, so let's try and create a model that just predicts that residual", "tokens": [321, 600, 920, 658, 257, 27980, 11, 370, 718, 311, 853, 293, 1884, 257, 2316, 300, 445, 6069, 82, 300, 27980], "temperature": 0.0, "avg_logprob": -0.1766457203005956, "compression_ratio": 2.052863436123348, "no_speech_prob": 4.495171197049785e-06}, {"id": 1100, "seek": 692966, "start": 6957.38, "end": 6959.22, "text": " and add that on to our previous model.", "tokens": [293, 909, 300, 322, 281, 527, 3894, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1766457203005956, "compression_ratio": 2.052863436123348, "no_speech_prob": 4.495171197049785e-06}, {"id": 1101, "seek": 695922, "start": 6959.22, "end": 6963.34, "text": " And then let's build another model that predicts the residual and add that on to our previous", "tokens": [400, 550, 718, 311, 1322, 1071, 2316, 300, 6069, 82, 264, 27980, 293, 909, 300, 322, 281, 527, 3894], "temperature": 0.0, "avg_logprob": -0.11599165433413022, "compression_ratio": 1.6580310880829014, "no_speech_prob": 5.422211415861966e-06}, {"id": 1102, "seek": 695922, "start": 6963.34, "end": 6964.34, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.11599165433413022, "compression_ratio": 1.6580310880829014, "no_speech_prob": 5.422211415861966e-06}, {"id": 1103, "seek": 695922, "start": 6964.34, "end": 6970.740000000001, "text": " And if we keep doing that again and again, we should get closer and closer to our answer.", "tokens": [400, 498, 321, 1066, 884, 300, 797, 293, 797, 11, 321, 820, 483, 4966, 293, 4966, 281, 527, 1867, 13], "temperature": 0.0, "avg_logprob": -0.11599165433413022, "compression_ratio": 1.6580310880829014, "no_speech_prob": 5.422211415861966e-06}, {"id": 1104, "seek": 695922, "start": 6970.740000000001, "end": 6976.7, "text": " And this is based on a theory called boosting, which people that have done some machine learning", "tokens": [400, 341, 307, 2361, 322, 257, 5261, 1219, 43117, 11, 597, 561, 300, 362, 1096, 512, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.11599165433413022, "compression_ratio": 1.6580310880829014, "no_speech_prob": 5.422211415861966e-06}, {"id": 1105, "seek": 695922, "start": 6976.7, "end": 6978.9400000000005, "text": " will have certainly come across.", "tokens": [486, 362, 3297, 808, 2108, 13], "temperature": 0.0, "avg_logprob": -0.11599165433413022, "compression_ratio": 1.6580310880829014, "no_speech_prob": 5.422211415861966e-06}, {"id": 1106, "seek": 697894, "start": 6978.94, "end": 6990.46, "text": " And so basically the trick here is that by specifying that as being the thing that we're", "tokens": [400, 370, 1936, 264, 4282, 510, 307, 300, 538, 1608, 5489, 300, 382, 885, 264, 551, 300, 321, 434], "temperature": 0.0, "avg_logprob": -0.12172869781949627, "compression_ratio": 1.5739644970414202, "no_speech_prob": 9.570820793669554e-07}, {"id": 1107, "seek": 697894, "start": 6990.46, "end": 7000.7, "text": " trying to calculate, then we kind of get boosting for free.", "tokens": [1382, 281, 8873, 11, 550, 321, 733, 295, 483, 43117, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.12172869781949627, "compression_ratio": 1.5739644970414202, "no_speech_prob": 9.570820793669554e-07}, {"id": 1108, "seek": 697894, "start": 7000.7, "end": 7005.78, "text": " It's like because we can just juggle that around to show that actually it's just calculating", "tokens": [467, 311, 411, 570, 321, 393, 445, 361, 31726, 300, 926, 281, 855, 300, 767, 309, 311, 445, 28258], "temperature": 0.0, "avg_logprob": -0.12172869781949627, "compression_ratio": 1.5739644970414202, "no_speech_prob": 9.570820793669554e-07}, {"id": 1109, "seek": 697894, "start": 7005.78, "end": 7008.46, "text": " a model on the residual.", "tokens": [257, 2316, 322, 264, 27980, 13], "temperature": 0.0, "avg_logprob": -0.12172869781949627, "compression_ratio": 1.5739644970414202, "no_speech_prob": 9.570820793669554e-07}, {"id": 1110, "seek": 700846, "start": 7008.46, "end": 7012.22, "text": " So that's kind of amazing.", "tokens": [407, 300, 311, 733, 295, 2243, 13], "temperature": 0.0, "avg_logprob": -0.18100549589912845, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.0953077435260639e-05}, {"id": 1111, "seek": 700846, "start": 7012.22, "end": 7015.94, "text": " And it totally works.", "tokens": [400, 309, 3879, 1985, 13], "temperature": 0.0, "avg_logprob": -0.18100549589912845, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.0953077435260639e-05}, {"id": 1112, "seek": 700846, "start": 7015.94, "end": 7022.18, "text": " As you can see here, I've now got my standard batch norm layer, which is something which", "tokens": [1018, 291, 393, 536, 510, 11, 286, 600, 586, 658, 452, 3832, 15245, 2026, 4583, 11, 597, 307, 746, 597], "temperature": 0.0, "avg_logprob": -0.18100549589912845, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.0953077435260639e-05}, {"id": 1113, "seek": 700846, "start": 7022.18, "end": 7026.94, "text": " is going to reduce my size by 2 because it's got the stride 2.", "tokens": [307, 516, 281, 5407, 452, 2744, 538, 568, 570, 309, 311, 658, 264, 1056, 482, 568, 13], "temperature": 0.0, "avg_logprob": -0.18100549589912845, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.0953077435260639e-05}, {"id": 1114, "seek": 700846, "start": 7026.94, "end": 7032.38, "text": " And then I've got a ResNet layer of stride 1 and another ResNet layer of stride 1.", "tokens": [400, 550, 286, 600, 658, 257, 5015, 31890, 4583, 295, 1056, 482, 502, 293, 1071, 5015, 31890, 4583, 295, 1056, 482, 502, 13], "temperature": 0.0, "avg_logprob": -0.18100549589912845, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.0953077435260639e-05}, {"id": 1115, "seek": 700846, "start": 7032.38, "end": 7035.58, "text": " And sorry, I think I said there were 4 of these, there's actually 3 of these.", "tokens": [400, 2597, 11, 286, 519, 286, 848, 456, 645, 1017, 295, 613, 11, 456, 311, 767, 805, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.18100549589912845, "compression_ratio": 1.6559633027522935, "no_speech_prob": 1.0953077435260639e-05}, {"id": 1116, "seek": 703558, "start": 7035.58, "end": 7039.1, "text": " So this is now 3 times deeper, I zipped through all of those.", "tokens": [407, 341, 307, 586, 805, 1413, 7731, 11, 286, 710, 5529, 807, 439, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.19171673542744405, "compression_ratio": 1.8606965174129353, "no_speech_prob": 5.594319645751966e-06}, {"id": 1117, "seek": 703558, "start": 7039.1, "end": 7043.46, "text": " And so I've now got a function of a function of a function.", "tokens": [400, 370, 286, 600, 586, 658, 257, 2445, 295, 257, 2445, 295, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.19171673542744405, "compression_ratio": 1.8606965174129353, "no_speech_prob": 5.594319645751966e-06}, {"id": 1118, "seek": 703558, "start": 7043.46, "end": 7050.66, "text": " So 3 layers per group, and then my start and my linear at the end.", "tokens": [407, 805, 7914, 680, 1594, 11, 293, 550, 452, 722, 293, 452, 8213, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.19171673542744405, "compression_ratio": 1.8606965174129353, "no_speech_prob": 5.594319645751966e-06}, {"id": 1119, "seek": 703558, "start": 7050.66, "end": 7055.44, "text": " So this is now 3 times bigger than my original.", "tokens": [407, 341, 307, 586, 805, 1413, 3801, 813, 452, 3380, 13], "temperature": 0.0, "avg_logprob": -0.19171673542744405, "compression_ratio": 1.8606965174129353, "no_speech_prob": 5.594319645751966e-06}, {"id": 1120, "seek": 703558, "start": 7055.44, "end": 7060.34, "text": " And if I fit it, you can see it just keeps going up and up and up and up.", "tokens": [400, 498, 286, 3318, 309, 11, 291, 393, 536, 309, 445, 5965, 516, 493, 293, 493, 293, 493, 293, 493, 13], "temperature": 0.0, "avg_logprob": -0.19171673542744405, "compression_ratio": 1.8606965174129353, "no_speech_prob": 5.594319645751966e-06}, {"id": 1121, "seek": 703558, "start": 7060.34, "end": 7063.86, "text": " I keep fitting it more, it keeps going up and up and up and up.", "tokens": [286, 1066, 15669, 309, 544, 11, 309, 5965, 516, 493, 293, 493, 293, 493, 293, 493, 13], "temperature": 0.0, "avg_logprob": -0.19171673542744405, "compression_ratio": 1.8606965174129353, "no_speech_prob": 5.594319645751966e-06}, {"id": 1122, "seek": 706386, "start": 7063.86, "end": 7068.94, "text": " And it's still going up when I kind of got bored.", "tokens": [400, 309, 311, 920, 516, 493, 562, 286, 733, 295, 658, 13521, 13], "temperature": 0.0, "avg_logprob": -0.11217742496066624, "compression_ratio": 1.4355828220858895, "no_speech_prob": 1.3709553741136915e-06}, {"id": 1123, "seek": 706386, "start": 7068.94, "end": 7077.259999999999, "text": " So the ResNet has been a really important development.", "tokens": [407, 264, 5015, 31890, 575, 668, 257, 534, 1021, 3250, 13], "temperature": 0.0, "avg_logprob": -0.11217742496066624, "compression_ratio": 1.4355828220858895, "no_speech_prob": 1.3709553741136915e-06}, {"id": 1124, "seek": 706386, "start": 7077.259999999999, "end": 7083.98, "text": " And it's allowed us to create these really deep networks.", "tokens": [400, 309, 311, 4350, 505, 281, 1884, 613, 534, 2452, 9590, 13], "temperature": 0.0, "avg_logprob": -0.11217742496066624, "compression_ratio": 1.4355828220858895, "no_speech_prob": 1.3709553741136915e-06}, {"id": 1125, "seek": 706386, "start": 7083.98, "end": 7089.46, "text": " Now the full ResNet does not quite look the way I've described it here.", "tokens": [823, 264, 1577, 5015, 31890, 775, 406, 1596, 574, 264, 636, 286, 600, 7619, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.11217742496066624, "compression_ratio": 1.4355828220858895, "no_speech_prob": 1.3709553741136915e-06}, {"id": 1126, "seek": 708946, "start": 7089.46, "end": 7096.42, "text": " The full ResNet doesn't just have one convolution, but it actually has 2 convolutions.", "tokens": [440, 1577, 5015, 31890, 1177, 380, 445, 362, 472, 45216, 11, 457, 309, 767, 575, 568, 3754, 15892, 13], "temperature": 0.0, "avg_logprob": -0.12144548948421034, "compression_ratio": 1.6865671641791045, "no_speech_prob": 2.1233756797300885e-06}, {"id": 1127, "seek": 708946, "start": 7096.42, "end": 7101.42, "text": " So the way people normally draw ResNet blocks is they normally say, you've got some input", "tokens": [407, 264, 636, 561, 5646, 2642, 5015, 31890, 8474, 307, 436, 5646, 584, 11, 291, 600, 658, 512, 4846], "temperature": 0.0, "avg_logprob": -0.12144548948421034, "compression_ratio": 1.6865671641791045, "no_speech_prob": 2.1233756797300885e-06}, {"id": 1128, "seek": 708946, "start": 7101.42, "end": 7111.78, "text": " coming into the layer, it goes through one convolution, 2 convolutions, and then gets", "tokens": [1348, 666, 264, 4583, 11, 309, 1709, 807, 472, 45216, 11, 568, 3754, 15892, 11, 293, 550, 2170], "temperature": 0.0, "avg_logprob": -0.12144548948421034, "compression_ratio": 1.6865671641791045, "no_speech_prob": 2.1233756797300885e-06}, {"id": 1129, "seek": 708946, "start": 7111.78, "end": 7116.42, "text": " added back to the original input.", "tokens": [3869, 646, 281, 264, 3380, 4846, 13], "temperature": 0.0, "avg_logprob": -0.12144548948421034, "compression_ratio": 1.6865671641791045, "no_speech_prob": 2.1233756797300885e-06}, {"id": 1130, "seek": 708946, "start": 7116.42, "end": 7118.96, "text": " That's the full version of a ResNet block.", "tokens": [663, 311, 264, 1577, 3037, 295, 257, 5015, 31890, 3461, 13], "temperature": 0.0, "avg_logprob": -0.12144548948421034, "compression_ratio": 1.6865671641791045, "no_speech_prob": 2.1233756797300885e-06}, {"id": 1131, "seek": 711896, "start": 7118.96, "end": 7122.94, "text": " In my case, I've just done one convolution.", "tokens": [682, 452, 1389, 11, 286, 600, 445, 1096, 472, 45216, 13], "temperature": 0.0, "avg_logprob": -0.14400426892266757, "compression_ratio": 1.4131736526946108, "no_speech_prob": 3.5008501981792506e-06}, {"id": 1132, "seek": 711896, "start": 7122.94, "end": 7138.06, "text": " And then you'll see also in every block, one of them, it's actually the first one here,", "tokens": [400, 550, 291, 603, 536, 611, 294, 633, 3461, 11, 472, 295, 552, 11, 309, 311, 767, 264, 700, 472, 510, 11], "temperature": 0.0, "avg_logprob": -0.14400426892266757, "compression_ratio": 1.4131736526946108, "no_speech_prob": 3.5008501981792506e-06}, {"id": 1133, "seek": 711896, "start": 7138.06, "end": 7146.0, "text": " is not a ResNet block, but a standard convolution with a stride of 2.", "tokens": [307, 406, 257, 5015, 31890, 3461, 11, 457, 257, 3832, 45216, 365, 257, 1056, 482, 295, 568, 13], "temperature": 0.0, "avg_logprob": -0.14400426892266757, "compression_ratio": 1.4131736526946108, "no_speech_prob": 3.5008501981792506e-06}, {"id": 1134, "seek": 711896, "start": 7146.0, "end": 7148.74, "text": " This is called a bottleneck layer.", "tokens": [639, 307, 1219, 257, 44641, 547, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14400426892266757, "compression_ratio": 1.4131736526946108, "no_speech_prob": 3.5008501981792506e-06}, {"id": 1135, "seek": 714874, "start": 7148.74, "end": 7151.04, "text": " And the idea is this is not a ResNet block.", "tokens": [400, 264, 1558, 307, 341, 307, 406, 257, 5015, 31890, 3461, 13], "temperature": 0.0, "avg_logprob": -0.16395723182736463, "compression_ratio": 1.6537102473498233, "no_speech_prob": 2.684190576474066e-06}, {"id": 1136, "seek": 714874, "start": 7151.04, "end": 7154.86, "text": " So from time to time, we actually change the geometry.", "tokens": [407, 490, 565, 281, 565, 11, 321, 767, 1319, 264, 18426, 13], "temperature": 0.0, "avg_logprob": -0.16395723182736463, "compression_ratio": 1.6537102473498233, "no_speech_prob": 2.684190576474066e-06}, {"id": 1137, "seek": 714874, "start": 7154.86, "end": 7156.62, "text": " We're doing the stride 2.", "tokens": [492, 434, 884, 264, 1056, 482, 568, 13], "temperature": 0.0, "avg_logprob": -0.16395723182736463, "compression_ratio": 1.6537102473498233, "no_speech_prob": 2.684190576474066e-06}, {"id": 1138, "seek": 714874, "start": 7156.62, "end": 7160.98, "text": " In ResNet, we don't actually use just a standard convolutional layer.", "tokens": [682, 5015, 31890, 11, 321, 500, 380, 767, 764, 445, 257, 3832, 45216, 304, 4583, 13], "temperature": 0.0, "avg_logprob": -0.16395723182736463, "compression_ratio": 1.6537102473498233, "no_speech_prob": 2.684190576474066e-06}, {"id": 1139, "seek": 714874, "start": 7160.98, "end": 7164.82, "text": " There's actually a different form of bottleneck block that I'm not going to teach you in this", "tokens": [821, 311, 767, 257, 819, 1254, 295, 44641, 547, 3461, 300, 286, 478, 406, 516, 281, 2924, 291, 294, 341], "temperature": 0.0, "avg_logprob": -0.16395723182736463, "compression_ratio": 1.6537102473498233, "no_speech_prob": 2.684190576474066e-06}, {"id": 1140, "seek": 714874, "start": 7164.82, "end": 7165.82, "text": " course.", "tokens": [1164, 13], "temperature": 0.0, "avg_logprob": -0.16395723182736463, "compression_ratio": 1.6537102473498233, "no_speech_prob": 2.684190576474066e-06}, {"id": 1141, "seek": 714874, "start": 7165.82, "end": 7167.5, "text": " I'm going to teach you in part 2.", "tokens": [286, 478, 516, 281, 2924, 291, 294, 644, 568, 13], "temperature": 0.0, "avg_logprob": -0.16395723182736463, "compression_ratio": 1.6537102473498233, "no_speech_prob": 2.684190576474066e-06}, {"id": 1142, "seek": 714874, "start": 7167.5, "end": 7173.179999999999, "text": " But as you can see, even this somewhat simplified version of a ResNet still works pretty well.", "tokens": [583, 382, 291, 393, 536, 11, 754, 341, 8344, 26335, 3037, 295, 257, 5015, 31890, 920, 1985, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.16395723182736463, "compression_ratio": 1.6537102473498233, "no_speech_prob": 2.684190576474066e-06}, {"id": 1143, "seek": 714874, "start": 7173.179999999999, "end": 7178.38, "text": " And so we can make it a little bit bigger.", "tokens": [400, 370, 321, 393, 652, 309, 257, 707, 857, 3801, 13], "temperature": 0.0, "avg_logprob": -0.16395723182736463, "compression_ratio": 1.6537102473498233, "no_speech_prob": 2.684190576474066e-06}, {"id": 1144, "seek": 717838, "start": 7178.38, "end": 7182.14, "text": " And so here I've just increased all of my sizes.", "tokens": [400, 370, 510, 286, 600, 445, 6505, 439, 295, 452, 11602, 13], "temperature": 0.0, "avg_logprob": -0.12484839679749031, "compression_ratio": 1.648, "no_speech_prob": 1.0616065992508084e-05}, {"id": 1145, "seek": 717838, "start": 7182.14, "end": 7184.78, "text": " I have still got my 3.", "tokens": [286, 362, 920, 658, 452, 805, 13], "temperature": 0.0, "avg_logprob": -0.12484839679749031, "compression_ratio": 1.648, "no_speech_prob": 1.0616065992508084e-05}, {"id": 1146, "seek": 717838, "start": 7184.78, "end": 7186.9400000000005, "text": " And also I've added dropout.", "tokens": [400, 611, 286, 600, 3869, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.12484839679749031, "compression_ratio": 1.648, "no_speech_prob": 1.0616065992508084e-05}, {"id": 1147, "seek": 717838, "start": 7186.9400000000005, "end": 7192.74, "text": " So at this point, I'm going to say this is, other than the minor simplification of ResNet,", "tokens": [407, 412, 341, 935, 11, 286, 478, 516, 281, 584, 341, 307, 11, 661, 813, 264, 6696, 6883, 3774, 295, 5015, 31890, 11], "temperature": 0.0, "avg_logprob": -0.12484839679749031, "compression_ratio": 1.648, "no_speech_prob": 1.0616065992508084e-05}, {"id": 1148, "seek": 717838, "start": 7192.74, "end": 7198.62, "text": " a reasonable approximation of a good starting point for a modern architecture.", "tokens": [257, 10585, 28023, 295, 257, 665, 2891, 935, 337, 257, 4363, 9482, 13], "temperature": 0.0, "avg_logprob": -0.12484839679749031, "compression_ratio": 1.648, "no_speech_prob": 1.0616065992508084e-05}, {"id": 1149, "seek": 717838, "start": 7198.62, "end": 7201.14, "text": " And so now I've added in my.2 dropout.", "tokens": [400, 370, 586, 286, 600, 3869, 294, 452, 2411, 17, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.12484839679749031, "compression_ratio": 1.648, "no_speech_prob": 1.0616065992508084e-05}, {"id": 1150, "seek": 717838, "start": 7201.14, "end": 7203.16, "text": " I've increased the size here.", "tokens": [286, 600, 6505, 264, 2744, 510, 13], "temperature": 0.0, "avg_logprob": -0.12484839679749031, "compression_ratio": 1.648, "no_speech_prob": 1.0616065992508084e-05}, {"id": 1151, "seek": 717838, "start": 7203.16, "end": 7207.06, "text": " And if I train this, I can train it for a while.", "tokens": [400, 498, 286, 3847, 341, 11, 286, 393, 3847, 309, 337, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.12484839679749031, "compression_ratio": 1.648, "no_speech_prob": 1.0616065992508084e-05}, {"id": 1152, "seek": 717838, "start": 7207.06, "end": 7208.06, "text": " It's going pretty well.", "tokens": [467, 311, 516, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.12484839679749031, "compression_ratio": 1.648, "no_speech_prob": 1.0616065992508084e-05}, {"id": 1153, "seek": 720806, "start": 7208.06, "end": 7210.38, "text": " I can then add in TTA at the end.", "tokens": [286, 393, 550, 909, 294, 314, 8241, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.2549566248411773, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.6964231690508313e-05}, {"id": 1154, "seek": 720806, "start": 7210.38, "end": 7212.820000000001, "text": " Eventually I get 85%.", "tokens": [17586, 286, 483, 14695, 6856], "temperature": 0.0, "avg_logprob": -0.2549566248411773, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.6964231690508313e-05}, {"id": 1155, "seek": 720806, "start": 7212.820000000001, "end": 7218.580000000001, "text": " And this is at a point now where, literally I wrote this whole notebook in 3 hours.", "tokens": [400, 341, 307, 412, 257, 935, 586, 689, 11, 3736, 286, 4114, 341, 1379, 21060, 294, 805, 2496, 13], "temperature": 0.0, "avg_logprob": -0.2549566248411773, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.6964231690508313e-05}, {"id": 1156, "seek": 720806, "start": 7218.580000000001, "end": 7221.18, "text": " We can create this thing in 3 hours.", "tokens": [492, 393, 1884, 341, 551, 294, 805, 2496, 13], "temperature": 0.0, "avg_logprob": -0.2549566248411773, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.6964231690508313e-05}, {"id": 1157, "seek": 720806, "start": 7221.18, "end": 7228.700000000001, "text": " And this is like an accuracy that in 2012, 2013 was considered pretty much state of the", "tokens": [400, 341, 307, 411, 364, 14170, 300, 294, 9125, 11, 9012, 390, 4888, 1238, 709, 1785, 295, 264], "temperature": 0.0, "avg_logprob": -0.2549566248411773, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.6964231690508313e-05}, {"id": 1158, "seek": 720806, "start": 7228.700000000001, "end": 7230.620000000001, "text": " art for Cypher 10.", "tokens": [1523, 337, 10295, 79, 511, 1266, 13], "temperature": 0.0, "avg_logprob": -0.2549566248411773, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.6964231690508313e-05}, {"id": 1159, "seek": 720806, "start": 7230.620000000001, "end": 7235.42, "text": " So this is actually pretty damn good.", "tokens": [407, 341, 307, 767, 1238, 8151, 665, 13], "temperature": 0.0, "avg_logprob": -0.2549566248411773, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.6964231690508313e-05}, {"id": 1160, "seek": 723542, "start": 7235.42, "end": 7241.02, "text": " So nowadays, the most recent results are like 97%.", "tokens": [407, 13434, 11, 264, 881, 5162, 3542, 366, 411, 23399, 6856], "temperature": 0.0, "avg_logprob": -0.1974927278665396, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.7778249457478523e-05}, {"id": 1161, "seek": 723542, "start": 7241.02, "end": 7245.62, "text": " There's plenty of room we can still improve, but they're all based on these techniques.", "tokens": [821, 311, 7140, 295, 1808, 321, 393, 920, 3470, 11, 457, 436, 434, 439, 2361, 322, 613, 7512, 13], "temperature": 0.0, "avg_logprob": -0.1974927278665396, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.7778249457478523e-05}, {"id": 1162, "seek": 723542, "start": 7245.62, "end": 7249.38, "text": " Like there isn't really anything.", "tokens": [1743, 456, 1943, 380, 534, 1340, 13], "temperature": 0.0, "avg_logprob": -0.1974927278665396, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.7778249457478523e-05}, {"id": 1163, "seek": 723542, "start": 7249.38, "end": 7253.78, "text": " When we start looking in part 2 at how to get this right up to state of the art, you'll", "tokens": [1133, 321, 722, 1237, 294, 644, 568, 412, 577, 281, 483, 341, 558, 493, 281, 1785, 295, 264, 1523, 11, 291, 603], "temperature": 0.0, "avg_logprob": -0.1974927278665396, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.7778249457478523e-05}, {"id": 1164, "seek": 723542, "start": 7253.78, "end": 7259.74, "text": " see it's basically better approaches to data augmentation, better approaches to regularization,", "tokens": [536, 309, 311, 1936, 1101, 11587, 281, 1412, 14501, 19631, 11, 1101, 11587, 281, 3890, 2144, 11], "temperature": 0.0, "avg_logprob": -0.1974927278665396, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.7778249457478523e-05}, {"id": 1165, "seek": 723542, "start": 7259.74, "end": 7264.78, "text": " some tweaks on ResNet, but it's all basically this idea.", "tokens": [512, 46664, 322, 5015, 31890, 11, 457, 309, 311, 439, 1936, 341, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1974927278665396, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.7778249457478523e-05}, {"id": 1166, "seek": 726478, "start": 7264.78, "end": 7275.82, "text": " So is the training on the residual method, looks like it's a generic thing that can be", "tokens": [407, 307, 264, 3097, 322, 264, 27980, 3170, 11, 1542, 411, 309, 311, 257, 19577, 551, 300, 393, 312], "temperature": 0.0, "avg_logprob": -0.2820223437415229, "compression_ratio": 1.441025641025641, "no_speech_prob": 2.5866836949717253e-05}, {"id": 1167, "seek": 726478, "start": 7275.82, "end": 7278.0599999999995, "text": " applied, non-image problems.", "tokens": [6456, 11, 2107, 12, 26624, 2740, 13], "temperature": 0.0, "avg_logprob": -0.2820223437415229, "compression_ratio": 1.441025641025641, "no_speech_prob": 2.5866836949717253e-05}, {"id": 1168, "seek": 726478, "start": 7278.0599999999995, "end": 7279.5, "text": " Oh great question.", "tokens": [876, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2820223437415229, "compression_ratio": 1.441025641025641, "no_speech_prob": 2.5866836949717253e-05}, {"id": 1169, "seek": 726478, "start": 7279.5, "end": 7284.42, "text": " Yes, it is, but it's been ignored everywhere else.", "tokens": [1079, 11, 309, 307, 11, 457, 309, 311, 668, 19735, 5315, 1646, 13], "temperature": 0.0, "avg_logprob": -0.2820223437415229, "compression_ratio": 1.441025641025641, "no_speech_prob": 2.5866836949717253e-05}, {"id": 1170, "seek": 726478, "start": 7284.42, "end": 7291.46, "text": " In NLP, something called the transformer architecture recently appeared and was shown to be the", "tokens": [682, 426, 45196, 11, 746, 1219, 264, 31782, 9482, 3938, 8516, 293, 390, 4898, 281, 312, 264], "temperature": 0.0, "avg_logprob": -0.2820223437415229, "compression_ratio": 1.441025641025641, "no_speech_prob": 2.5866836949717253e-05}, {"id": 1171, "seek": 729146, "start": 7291.46, "end": 7298.54, "text": " state of the art for translation, and it's got like a simple ResNet structure in it.", "tokens": [1785, 295, 264, 1523, 337, 12853, 11, 293, 309, 311, 658, 411, 257, 2199, 5015, 31890, 3877, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1674697228840419, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3211743862484582e-05}, {"id": 1172, "seek": 729146, "start": 7298.54, "end": 7300.18, "text": " First time I've ever seen it in NLP.", "tokens": [2386, 565, 286, 600, 1562, 1612, 309, 294, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.1674697228840419, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3211743862484582e-05}, {"id": 1173, "seek": 729146, "start": 7300.18, "end": 7304.66, "text": " I haven't really seen anybody else take advantage of it.", "tokens": [286, 2378, 380, 534, 1612, 4472, 1646, 747, 5002, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1674697228840419, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3211743862484582e-05}, {"id": 1174, "seek": 729146, "start": 7304.66, "end": 7308.42, "text": " This general approach, we call these skip connections, this idea of skipping over a", "tokens": [639, 2674, 3109, 11, 321, 818, 613, 10023, 9271, 11, 341, 1558, 295, 31533, 670, 257], "temperature": 0.0, "avg_logprob": -0.1674697228840419, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3211743862484582e-05}, {"id": 1175, "seek": 729146, "start": 7308.42, "end": 7312.22, "text": " layer and doing an identity.", "tokens": [4583, 293, 884, 364, 6575, 13], "temperature": 0.0, "avg_logprob": -0.1674697228840419, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3211743862484582e-05}, {"id": 1176, "seek": 729146, "start": 7312.22, "end": 7316.5, "text": " It's been appearing a lot in computer vision, and nobody else much seems to be using it,", "tokens": [467, 311, 668, 19870, 257, 688, 294, 3820, 5201, 11, 293, 5079, 1646, 709, 2544, 281, 312, 1228, 309, 11], "temperature": 0.0, "avg_logprob": -0.1674697228840419, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3211743862484582e-05}, {"id": 1177, "seek": 729146, "start": 7316.5, "end": 7319.72, "text": " even though there's nothing computer vision specific about it.", "tokens": [754, 1673, 456, 311, 1825, 3820, 5201, 2685, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.1674697228840419, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.3211743862484582e-05}, {"id": 1178, "seek": 731972, "start": 7319.72, "end": 7323.820000000001, "text": " So I think it's a big opportunity.", "tokens": [407, 286, 519, 309, 311, 257, 955, 2650, 13], "temperature": 0.0, "avg_logprob": -0.12876918723991326, "compression_ratio": 1.5982905982905984, "no_speech_prob": 1.3631150977744255e-05}, {"id": 1179, "seek": 731972, "start": 7323.820000000001, "end": 7332.64, "text": " So final stage I want to show you is how to use an extra feature of PyTorch to do something", "tokens": [407, 2572, 3233, 286, 528, 281, 855, 291, 307, 577, 281, 764, 364, 2857, 4111, 295, 9953, 51, 284, 339, 281, 360, 746], "temperature": 0.0, "avg_logprob": -0.12876918723991326, "compression_ratio": 1.5982905982905984, "no_speech_prob": 1.3631150977744255e-05}, {"id": 1180, "seek": 731972, "start": 7332.64, "end": 7336.780000000001, "text": " cool and it's going to be kind of a segue into part 2.", "tokens": [1627, 293, 309, 311, 516, 281, 312, 733, 295, 257, 33850, 666, 644, 568, 13], "temperature": 0.0, "avg_logprob": -0.12876918723991326, "compression_ratio": 1.5982905982905984, "no_speech_prob": 1.3631150977744255e-05}, {"id": 1181, "seek": 731972, "start": 7336.780000000001, "end": 7341.46, "text": " It's going to be our first little hint as to what else we can build on these neural", "tokens": [467, 311, 516, 281, 312, 527, 700, 707, 12075, 382, 281, 437, 1646, 321, 393, 1322, 322, 613, 18161], "temperature": 0.0, "avg_logprob": -0.12876918723991326, "compression_ratio": 1.5982905982905984, "no_speech_prob": 1.3631150977744255e-05}, {"id": 1182, "seek": 731972, "start": 7341.46, "end": 7342.46, "text": " nets.", "tokens": [36170, 13], "temperature": 0.0, "avg_logprob": -0.12876918723991326, "compression_ratio": 1.5982905982905984, "no_speech_prob": 1.3631150977744255e-05}, {"id": 1183, "seek": 731972, "start": 7342.46, "end": 7346.66, "text": " And it's also going to take us all the way back to lesson 1, which is we're going to", "tokens": [400, 309, 311, 611, 516, 281, 747, 505, 439, 264, 636, 646, 281, 6898, 502, 11, 597, 307, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.12876918723991326, "compression_ratio": 1.5982905982905984, "no_speech_prob": 1.3631150977744255e-05}, {"id": 1184, "seek": 731972, "start": 7346.66, "end": 7349.14, "text": " do dogs and cats.", "tokens": [360, 7197, 293, 11111, 13], "temperature": 0.0, "avg_logprob": -0.12876918723991326, "compression_ratio": 1.5982905982905984, "no_speech_prob": 1.3631150977744255e-05}, {"id": 1185, "seek": 734914, "start": 7349.14, "end": 7354.740000000001, "text": " So going all the way back to dogs and cats, we're going to create a ResNet 34.", "tokens": [407, 516, 439, 264, 636, 646, 281, 7197, 293, 11111, 11, 321, 434, 516, 281, 1884, 257, 5015, 31890, 12790, 13], "temperature": 0.0, "avg_logprob": -0.13500700191575654, "compression_ratio": 1.6863636363636363, "no_speech_prob": 5.17388525622664e-06}, {"id": 1186, "seek": 734914, "start": 7354.740000000001, "end": 7363.660000000001, "text": " So these different ResNet 34, 50, 101, they're basically just different numbers of different", "tokens": [407, 613, 819, 5015, 31890, 12790, 11, 2625, 11, 21055, 11, 436, 434, 1936, 445, 819, 3547, 295, 819], "temperature": 0.0, "avg_logprob": -0.13500700191575654, "compression_ratio": 1.6863636363636363, "no_speech_prob": 5.17388525622664e-06}, {"id": 1187, "seek": 734914, "start": 7363.660000000001, "end": 7365.860000000001, "text": " size blocks.", "tokens": [2744, 8474, 13], "temperature": 0.0, "avg_logprob": -0.13500700191575654, "compression_ratio": 1.6863636363636363, "no_speech_prob": 5.17388525622664e-06}, {"id": 1188, "seek": 734914, "start": 7365.860000000001, "end": 7369.9400000000005, "text": " It's like how many of these kind of pieces do you have before each bottleneck block,", "tokens": [467, 311, 411, 577, 867, 295, 613, 733, 295, 3755, 360, 291, 362, 949, 1184, 44641, 547, 3461, 11], "temperature": 0.0, "avg_logprob": -0.13500700191575654, "compression_ratio": 1.6863636363636363, "no_speech_prob": 5.17388525622664e-06}, {"id": 1189, "seek": 734914, "start": 7369.9400000000005, "end": 7373.820000000001, "text": " and then how many of these sets of super blocks do you have.", "tokens": [293, 550, 577, 867, 295, 613, 6352, 295, 1687, 8474, 360, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.13500700191575654, "compression_ratio": 1.6863636363636363, "no_speech_prob": 5.17388525622664e-06}, {"id": 1190, "seek": 734914, "start": 7373.820000000001, "end": 7375.900000000001, "text": " That's all these different numbers mean.", "tokens": [663, 311, 439, 613, 819, 3547, 914, 13], "temperature": 0.0, "avg_logprob": -0.13500700191575654, "compression_ratio": 1.6863636363636363, "no_speech_prob": 5.17388525622664e-06}, {"id": 1191, "seek": 737590, "start": 7375.9, "end": 7381.74, "text": " So if you look at the TorchVision source code, you can actually see the definition of these", "tokens": [407, 498, 291, 574, 412, 264, 7160, 339, 53, 1991, 4009, 3089, 11, 291, 393, 767, 536, 264, 7123, 295, 613], "temperature": 0.0, "avg_logprob": -0.11048341028898664, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594327376456931e-06}, {"id": 1192, "seek": 737590, "start": 7381.74, "end": 7382.74, "text": " different ResNets.", "tokens": [819, 5015, 45, 1385, 13], "temperature": 0.0, "avg_logprob": -0.11048341028898664, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594327376456931e-06}, {"id": 1193, "seek": 737590, "start": 7382.74, "end": 7389.46, "text": " You'll see they're all just different parameters.", "tokens": [509, 603, 536, 436, 434, 439, 445, 819, 9834, 13], "temperature": 0.0, "avg_logprob": -0.11048341028898664, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594327376456931e-06}, {"id": 1194, "seek": 737590, "start": 7389.46, "end": 7392.0599999999995, "text": " So we're going to use ResNet 34.", "tokens": [407, 321, 434, 516, 281, 764, 5015, 31890, 12790, 13], "temperature": 0.0, "avg_logprob": -0.11048341028898664, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594327376456931e-06}, {"id": 1195, "seek": 737590, "start": 7392.0599999999995, "end": 7396.259999999999, "text": " And so we're going to do this a little bit more by hand.", "tokens": [400, 370, 321, 434, 516, 281, 360, 341, 257, 707, 857, 544, 538, 1011, 13], "temperature": 0.0, "avg_logprob": -0.11048341028898664, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594327376456931e-06}, {"id": 1196, "seek": 737590, "start": 7396.259999999999, "end": 7401.299999999999, "text": " So if this is my architecture, this is just the name of a function, then I can call it", "tokens": [407, 498, 341, 307, 452, 9482, 11, 341, 307, 445, 264, 1315, 295, 257, 2445, 11, 550, 286, 393, 818, 309], "temperature": 0.0, "avg_logprob": -0.11048341028898664, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594327376456931e-06}, {"id": 1197, "seek": 737590, "start": 7401.299999999999, "end": 7403.139999999999, "text": " to get that model.", "tokens": [281, 483, 300, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11048341028898664, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594327376456931e-06}, {"id": 1198, "seek": 740314, "start": 7403.14, "end": 7408.62, "text": " And then true, if we look at the definition, is do I want the pre-trained, so in other", "tokens": [400, 550, 2074, 11, 498, 321, 574, 412, 264, 7123, 11, 307, 360, 286, 528, 264, 659, 12, 17227, 2001, 11, 370, 294, 661], "temperature": 0.0, "avg_logprob": -0.16342498410132625, "compression_ratio": 1.5858585858585859, "no_speech_prob": 1.8448216678734752e-06}, {"id": 1199, "seek": 740314, "start": 7408.62, "end": 7412.58, "text": " words is it going to load in the pre-trained image net weights.", "tokens": [2283, 307, 309, 516, 281, 3677, 294, 264, 659, 12, 17227, 2001, 3256, 2533, 17443, 13], "temperature": 0.0, "avg_logprob": -0.16342498410132625, "compression_ratio": 1.5858585858585859, "no_speech_prob": 1.8448216678734752e-06}, {"id": 1200, "seek": 740314, "start": 7412.58, "end": 7419.62, "text": " So M now contains a model, and so I can take a look at it like so.", "tokens": [407, 376, 586, 8306, 257, 2316, 11, 293, 370, 286, 393, 747, 257, 574, 412, 309, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.16342498410132625, "compression_ratio": 1.5858585858585859, "no_speech_prob": 1.8448216678734752e-06}, {"id": 1201, "seek": 740314, "start": 7419.62, "end": 7430.46, "text": " And so you can see here what's going on, is that inside here I've got my initial 2D convolution.", "tokens": [400, 370, 291, 393, 536, 510, 437, 311, 516, 322, 11, 307, 300, 1854, 510, 286, 600, 658, 452, 5883, 568, 35, 45216, 13], "temperature": 0.0, "avg_logprob": -0.16342498410132625, "compression_ratio": 1.5858585858585859, "no_speech_prob": 1.8448216678734752e-06}, {"id": 1202, "seek": 743046, "start": 7430.46, "end": 7434.18, "text": " And here is that kernel size of 7x7.", "tokens": [400, 510, 307, 300, 28256, 2744, 295, 1614, 87, 22, 13], "temperature": 0.0, "avg_logprob": -0.19392593234193092, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.1907778773311293e-06}, {"id": 1203, "seek": 743046, "start": 7434.18, "end": 7438.7, "text": " And interestingly in this case, it actually starts out with a 7x7 stripe too.", "tokens": [400, 25873, 294, 341, 1389, 11, 309, 767, 3719, 484, 365, 257, 1614, 87, 22, 42957, 886, 13], "temperature": 0.0, "avg_logprob": -0.19392593234193092, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.1907778773311293e-06}, {"id": 1204, "seek": 743046, "start": 7438.7, "end": 7442.56, "text": " There's the padding that we talked about to make sure we don't lose the edges.", "tokens": [821, 311, 264, 39562, 300, 321, 2825, 466, 281, 652, 988, 321, 500, 380, 3624, 264, 8819, 13], "temperature": 0.0, "avg_logprob": -0.19392593234193092, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.1907778773311293e-06}, {"id": 1205, "seek": 743046, "start": 7442.56, "end": 7444.66, "text": " There's our batch norm.", "tokens": [821, 311, 527, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.19392593234193092, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.1907778773311293e-06}, {"id": 1206, "seek": 743046, "start": 7444.66, "end": 7445.66, "text": " There's our relu.", "tokens": [821, 311, 527, 1039, 84, 13], "temperature": 0.0, "avg_logprob": -0.19392593234193092, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.1907778773311293e-06}, {"id": 1207, "seek": 743046, "start": 7445.66, "end": 7448.58, "text": " And you get the idea, right?", "tokens": [400, 291, 483, 264, 1558, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19392593234193092, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.1907778773311293e-06}, {"id": 1208, "seek": 743046, "start": 7448.58, "end": 7455.16, "text": " And then so here you can now see there's a layer that contains a bunch of blocks.", "tokens": [400, 550, 370, 510, 291, 393, 586, 536, 456, 311, 257, 4583, 300, 8306, 257, 3840, 295, 8474, 13], "temperature": 0.0, "avg_logprob": -0.19392593234193092, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.1907778773311293e-06}, {"id": 1209, "seek": 745516, "start": 7455.16, "end": 7460.62, "text": " So here's a block which contains a conv, batch norm, relu, conv, batch norm.", "tokens": [407, 510, 311, 257, 3461, 597, 8306, 257, 3754, 11, 15245, 2026, 11, 1039, 84, 11, 3754, 11, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.13164636363153873, "compression_ratio": 1.8156424581005586, "no_speech_prob": 7.002166739766835e-07}, {"id": 1210, "seek": 745516, "start": 7460.62, "end": 7465.7, "text": " You can't see it printed, but after this is where it does the addition.", "tokens": [509, 393, 380, 536, 309, 13567, 11, 457, 934, 341, 307, 689, 309, 775, 264, 4500, 13], "temperature": 0.0, "avg_logprob": -0.13164636363153873, "compression_ratio": 1.8156424581005586, "no_speech_prob": 7.002166739766835e-07}, {"id": 1211, "seek": 745516, "start": 7465.7, "end": 7469.34, "text": " So there's like a whole ResNet block, and then another ResNet block, and then another", "tokens": [407, 456, 311, 411, 257, 1379, 5015, 31890, 3461, 11, 293, 550, 1071, 5015, 31890, 3461, 11, 293, 550, 1071], "temperature": 0.0, "avg_logprob": -0.13164636363153873, "compression_ratio": 1.8156424581005586, "no_speech_prob": 7.002166739766835e-07}, {"id": 1212, "seek": 745516, "start": 7469.34, "end": 7473.599999999999, "text": " ResNet block.", "tokens": [5015, 31890, 3461, 13], "temperature": 0.0, "avg_logprob": -0.13164636363153873, "compression_ratio": 1.8156424581005586, "no_speech_prob": 7.002166739766835e-07}, {"id": 1213, "seek": 745516, "start": 7473.599999999999, "end": 7480.82, "text": " And then you can see also, sometimes you see one where there's a stripe too.", "tokens": [400, 550, 291, 393, 536, 611, 11, 2171, 291, 536, 472, 689, 456, 311, 257, 42957, 886, 13], "temperature": 0.0, "avg_logprob": -0.13164636363153873, "compression_ratio": 1.8156424581005586, "no_speech_prob": 7.002166739766835e-07}, {"id": 1214, "seek": 748082, "start": 7480.82, "end": 7487.299999999999, "text": " So here's actually one of these bottleneck layers.", "tokens": [407, 510, 311, 767, 472, 295, 613, 44641, 547, 7914, 13], "temperature": 0.0, "avg_logprob": -0.07774804592132568, "compression_ratio": 1.3529411764705883, "no_speech_prob": 2.295916601724457e-06}, {"id": 1215, "seek": 748082, "start": 7487.299999999999, "end": 7491.5, "text": " So you can kind of see how this is structured.", "tokens": [407, 291, 393, 733, 295, 536, 577, 341, 307, 18519, 13], "temperature": 0.0, "avg_logprob": -0.07774804592132568, "compression_ratio": 1.3529411764705883, "no_speech_prob": 2.295916601724457e-06}, {"id": 1216, "seek": 748082, "start": 7491.5, "end": 7500.34, "text": " So in our case, sorry I skipped over this a little bit, but the approach that we ended", "tokens": [407, 294, 527, 1389, 11, 2597, 286, 30193, 670, 341, 257, 707, 857, 11, 457, 264, 3109, 300, 321, 4590], "temperature": 0.0, "avg_logprob": -0.07774804592132568, "compression_ratio": 1.3529411764705883, "no_speech_prob": 2.295916601724457e-06}, {"id": 1217, "seek": 750034, "start": 7500.34, "end": 7517.54, "text": " up using for relu was to put it before our batch norm.", "tokens": [493, 1228, 337, 1039, 84, 390, 281, 829, 309, 949, 527, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.19801109067855344, "compression_ratio": 1.5161290322580645, "no_speech_prob": 9.874627266981406e-07}, {"id": 1218, "seek": 750034, "start": 7517.54, "end": 7518.54, "text": " See what they do here.", "tokens": [3008, 437, 436, 360, 510, 13], "temperature": 0.0, "avg_logprob": -0.19801109067855344, "compression_ratio": 1.5161290322580645, "no_speech_prob": 9.874627266981406e-07}, {"id": 1219, "seek": 750034, "start": 7518.54, "end": 7526.46, "text": " We've got batch norm, relu, conv, batch norm, relu, conv.", "tokens": [492, 600, 658, 15245, 2026, 11, 1039, 84, 11, 3754, 11, 15245, 2026, 11, 1039, 84, 11, 3754, 13], "temperature": 0.0, "avg_logprob": -0.19801109067855344, "compression_ratio": 1.5161290322580645, "no_speech_prob": 9.874627266981406e-07}, {"id": 1220, "seek": 750034, "start": 7526.46, "end": 7529.62, "text": " So you can see the order that they're using it here.", "tokens": [407, 291, 393, 536, 264, 1668, 300, 436, 434, 1228, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.19801109067855344, "compression_ratio": 1.5161290322580645, "no_speech_prob": 9.874627266981406e-07}, {"id": 1221, "seek": 752962, "start": 7529.62, "end": 7536.54, "text": " And you'll find there's 3 different versions of ResNet floating around.", "tokens": [400, 291, 603, 915, 456, 311, 805, 819, 9606, 295, 5015, 31890, 12607, 926, 13], "temperature": 0.0, "avg_logprob": -0.208745188527293, "compression_ratio": 1.5396825396825398, "no_speech_prob": 3.5559760362957604e-06}, {"id": 1222, "seek": 752962, "start": 7536.54, "end": 7541.86, "text": " The one which actually turns out to be the best is called the Preact ResNet, which has", "tokens": [440, 472, 597, 767, 4523, 484, 281, 312, 264, 1151, 307, 1219, 264, 6001, 578, 5015, 31890, 11, 597, 575], "temperature": 0.0, "avg_logprob": -0.208745188527293, "compression_ratio": 1.5396825396825398, "no_speech_prob": 3.5559760362957604e-06}, {"id": 1223, "seek": 752962, "start": 7541.86, "end": 7544.9, "text": " a different ordering again.", "tokens": [257, 819, 21739, 797, 13], "temperature": 0.0, "avg_logprob": -0.208745188527293, "compression_ratio": 1.5396825396825398, "no_speech_prob": 3.5559760362957604e-06}, {"id": 1224, "seek": 752962, "start": 7544.9, "end": 7548.18, "text": " But you can look it up.", "tokens": [583, 291, 393, 574, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.208745188527293, "compression_ratio": 1.5396825396825398, "no_speech_prob": 3.5559760362957604e-06}, {"id": 1225, "seek": 752962, "start": 7548.18, "end": 7553.32, "text": " It's basically a different order of where the relu and where the batch norm sit.", "tokens": [467, 311, 1936, 257, 819, 1668, 295, 689, 264, 1039, 84, 293, 689, 264, 15245, 2026, 1394, 13], "temperature": 0.0, "avg_logprob": -0.208745188527293, "compression_ratio": 1.5396825396825398, "no_speech_prob": 3.5559760362957604e-06}, {"id": 1226, "seek": 755332, "start": 7553.32, "end": 7561.0599999999995, "text": " So we're going to start with a standard ResNet 34, and normally what we do is we need to", "tokens": [407, 321, 434, 516, 281, 722, 365, 257, 3832, 5015, 31890, 12790, 11, 293, 5646, 437, 321, 360, 307, 321, 643, 281], "temperature": 0.0, "avg_logprob": -0.11527316229684012, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.579229415772716e-07}, {"id": 1227, "seek": 755332, "start": 7561.0599999999995, "end": 7566.5, "text": " now turn this into something that can predict dogs vs. cats.", "tokens": [586, 1261, 341, 666, 746, 300, 393, 6069, 7197, 12041, 13, 11111, 13], "temperature": 0.0, "avg_logprob": -0.11527316229684012, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.579229415772716e-07}, {"id": 1228, "seek": 755332, "start": 7566.5, "end": 7574.08, "text": " So currently the final layer has 1000 features, because ImageNet has 1000 features.", "tokens": [407, 4362, 264, 2572, 4583, 575, 9714, 4122, 11, 570, 29903, 31890, 575, 9714, 4122, 13], "temperature": 0.0, "avg_logprob": -0.11527316229684012, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.579229415772716e-07}, {"id": 1229, "seek": 755332, "start": 7574.08, "end": 7576.54, "text": " So we need to get rid of this.", "tokens": [407, 321, 643, 281, 483, 3973, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.11527316229684012, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.579229415772716e-07}, {"id": 1230, "seek": 757654, "start": 7576.54, "end": 7583.34, "text": " So when you use conv learner from pre-trained in FastAI, it actually deletes this layer", "tokens": [407, 562, 291, 764, 3754, 33347, 490, 659, 12, 17227, 2001, 294, 15968, 48698, 11, 309, 767, 1103, 37996, 341, 4583], "temperature": 0.0, "avg_logprob": -0.167580171064897, "compression_ratio": 1.5862068965517242, "no_speech_prob": 1.679728711678763e-06}, {"id": 1231, "seek": 757654, "start": 7583.34, "end": 7589.46, "text": " for you, and it also deletes this layer.", "tokens": [337, 291, 11, 293, 309, 611, 1103, 37996, 341, 4583, 13], "temperature": 0.0, "avg_logprob": -0.167580171064897, "compression_ratio": 1.5862068965517242, "no_speech_prob": 1.679728711678763e-06}, {"id": 1232, "seek": 757654, "start": 7589.46, "end": 7595.88, "text": " And something that as far as I know is unique to FastAI is we replace this average pooling", "tokens": [400, 746, 300, 382, 1400, 382, 286, 458, 307, 3845, 281, 15968, 48698, 307, 321, 7406, 341, 4274, 7005, 278], "temperature": 0.0, "avg_logprob": -0.167580171064897, "compression_ratio": 1.5862068965517242, "no_speech_prob": 1.679728711678763e-06}, {"id": 1233, "seek": 757654, "start": 7595.88, "end": 7598.54, "text": " layer of size 7x7.", "tokens": [4583, 295, 2744, 1614, 87, 22, 13], "temperature": 0.0, "avg_logprob": -0.167580171064897, "compression_ratio": 1.5862068965517242, "no_speech_prob": 1.679728711678763e-06}, {"id": 1234, "seek": 757654, "start": 7598.54, "end": 7602.56, "text": " So this is basically the adaptive pooling layer, but whoever wrote this didn't know", "tokens": [407, 341, 307, 1936, 264, 27912, 7005, 278, 4583, 11, 457, 11387, 4114, 341, 994, 380, 458], "temperature": 0.0, "avg_logprob": -0.167580171064897, "compression_ratio": 1.5862068965517242, "no_speech_prob": 1.679728711678763e-06}, {"id": 1235, "seek": 760256, "start": 7602.56, "end": 7607.56, "text": " about adaptive pooling, so they manually said, oh I know it's meant to be 7x7.", "tokens": [466, 27912, 7005, 278, 11, 370, 436, 16945, 848, 11, 1954, 286, 458, 309, 311, 4140, 281, 312, 1614, 87, 22, 13], "temperature": 0.0, "avg_logprob": -0.1623943918119601, "compression_ratio": 1.768939393939394, "no_speech_prob": 3.4465715543774422e-06}, {"id": 1236, "seek": 760256, "start": 7607.56, "end": 7611.820000000001, "text": " So in FastAI we replace this with adaptive pooling, but we actually do both adaptive", "tokens": [407, 294, 15968, 48698, 321, 7406, 341, 365, 27912, 7005, 278, 11, 457, 321, 767, 360, 1293, 27912], "temperature": 0.0, "avg_logprob": -0.1623943918119601, "compression_ratio": 1.768939393939394, "no_speech_prob": 3.4465715543774422e-06}, {"id": 1237, "seek": 760256, "start": 7611.820000000001, "end": 7619.72, "text": " average pooling and adaptive max pooling, and we then concatenate the two together.", "tokens": [4274, 7005, 278, 293, 27912, 11469, 7005, 278, 11, 293, 321, 550, 1588, 7186, 473, 264, 732, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1623943918119601, "compression_ratio": 1.768939393939394, "no_speech_prob": 3.4465715543774422e-06}, {"id": 1238, "seek": 760256, "start": 7619.72, "end": 7623.700000000001, "text": " It is something we invented, but at the same time we invented it, somebody wrote a paper", "tokens": [467, 307, 746, 321, 14479, 11, 457, 412, 264, 912, 565, 321, 14479, 309, 11, 2618, 4114, 257, 3035], "temperature": 0.0, "avg_logprob": -0.1623943918119601, "compression_ratio": 1.768939393939394, "no_speech_prob": 3.4465715543774422e-06}, {"id": 1239, "seek": 760256, "start": 7623.700000000001, "end": 7626.9400000000005, "text": " about it, so we don't get any credit.", "tokens": [466, 309, 11, 370, 321, 500, 380, 483, 604, 5397, 13], "temperature": 0.0, "avg_logprob": -0.1623943918119601, "compression_ratio": 1.768939393939394, "no_speech_prob": 3.4465715543774422e-06}, {"id": 1240, "seek": 760256, "start": 7626.9400000000005, "end": 7630.1, "text": " But I think we're the only library that provides it, and certainly the only one that does it", "tokens": [583, 286, 519, 321, 434, 264, 787, 6405, 300, 6417, 309, 11, 293, 3297, 264, 787, 472, 300, 775, 309], "temperature": 0.0, "avg_logprob": -0.1623943918119601, "compression_ratio": 1.768939393939394, "no_speech_prob": 3.4465715543774422e-06}, {"id": 1241, "seek": 763010, "start": 7630.1, "end": 7634.1, "text": " by default.", "tokens": [538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.18625162910012638, "compression_ratio": 1.625, "no_speech_prob": 1.1478697160782758e-05}, {"id": 1242, "seek": 763010, "start": 7634.1, "end": 7638.18, "text": " For the purpose of this exercise though, we're going to do a simple version where we delete", "tokens": [1171, 264, 4334, 295, 341, 5380, 1673, 11, 321, 434, 516, 281, 360, 257, 2199, 3037, 689, 321, 12097], "temperature": 0.0, "avg_logprob": -0.18625162910012638, "compression_ratio": 1.625, "no_speech_prob": 1.1478697160782758e-05}, {"id": 1243, "seek": 763010, "start": 7638.18, "end": 7639.4800000000005, "text": " the last two layers.", "tokens": [264, 1036, 732, 7914, 13], "temperature": 0.0, "avg_logprob": -0.18625162910012638, "compression_ratio": 1.625, "no_speech_prob": 1.1478697160782758e-05}, {"id": 1244, "seek": 763010, "start": 7639.4800000000005, "end": 7644.1, "text": " So we'll grab all the children of the model, we'll delete the last two layers, and then", "tokens": [407, 321, 603, 4444, 439, 264, 2227, 295, 264, 2316, 11, 321, 603, 12097, 264, 1036, 732, 7914, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.18625162910012638, "compression_ratio": 1.625, "no_speech_prob": 1.1478697160782758e-05}, {"id": 1245, "seek": 763010, "start": 7644.1, "end": 7651.02, "text": " instead we're going to add a convolution which just has two outputs.", "tokens": [2602, 321, 434, 516, 281, 909, 257, 45216, 597, 445, 575, 732, 23930, 13], "temperature": 0.0, "avg_logprob": -0.18625162910012638, "compression_ratio": 1.625, "no_speech_prob": 1.1478697160782758e-05}, {"id": 1246, "seek": 763010, "start": 7651.02, "end": 7655.14, "text": " I'll show you why in a moment.", "tokens": [286, 603, 855, 291, 983, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.18625162910012638, "compression_ratio": 1.625, "no_speech_prob": 1.1478697160782758e-05}, {"id": 1247, "seek": 765514, "start": 7655.14, "end": 7661.740000000001, "text": " Then we're going to do our average pooling, and then we're going to do our softmax.", "tokens": [1396, 321, 434, 516, 281, 360, 527, 4274, 7005, 278, 11, 293, 550, 321, 434, 516, 281, 360, 527, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.0992933105636429, "compression_ratio": 1.8369565217391304, "no_speech_prob": 3.041589934582589e-06}, {"id": 1248, "seek": 765514, "start": 7661.740000000001, "end": 7668.200000000001, "text": " So that's a model which is going to have, you'll see that this one has a fully connected", "tokens": [407, 300, 311, 257, 2316, 597, 307, 516, 281, 362, 11, 291, 603, 536, 300, 341, 472, 575, 257, 4498, 4582], "temperature": 0.0, "avg_logprob": -0.0992933105636429, "compression_ratio": 1.8369565217391304, "no_speech_prob": 3.041589934582589e-06}, {"id": 1249, "seek": 765514, "start": 7668.200000000001, "end": 7673.08, "text": " layer at the end, this one does not have a fully connected layer at the end.", "tokens": [4583, 412, 264, 917, 11, 341, 472, 775, 406, 362, 257, 4498, 4582, 4583, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.0992933105636429, "compression_ratio": 1.8369565217391304, "no_speech_prob": 3.041589934582589e-06}, {"id": 1250, "seek": 765514, "start": 7673.08, "end": 7681.740000000001, "text": " But if you think about it, this convolutional layer is going to be two filters only, and", "tokens": [583, 498, 291, 519, 466, 309, 11, 341, 45216, 304, 4583, 307, 516, 281, 312, 732, 15995, 787, 11, 293], "temperature": 0.0, "avg_logprob": -0.0992933105636429, "compression_ratio": 1.8369565217391304, "no_speech_prob": 3.041589934582589e-06}, {"id": 1251, "seek": 768174, "start": 7681.74, "end": 7685.26, "text": " it's going to be 2x, 7x, 7x.", "tokens": [309, 311, 516, 281, 312, 568, 87, 11, 1614, 87, 11, 1614, 87, 13], "temperature": 0.0, "avg_logprob": -0.1683744021824428, "compression_ratio": 1.7890625, "no_speech_prob": 6.893607178426464e-07}, {"id": 1252, "seek": 768174, "start": 7685.26, "end": 7690.3, "text": " And so once we then do the average pooling, it's going to end up being just two numbers", "tokens": [400, 370, 1564, 321, 550, 360, 264, 4274, 7005, 278, 11, 309, 311, 516, 281, 917, 493, 885, 445, 732, 3547], "temperature": 0.0, "avg_logprob": -0.1683744021824428, "compression_ratio": 1.7890625, "no_speech_prob": 6.893607178426464e-07}, {"id": 1253, "seek": 768174, "start": 7690.3, "end": 7691.3, "text": " that it produces.", "tokens": [300, 309, 14725, 13], "temperature": 0.0, "avg_logprob": -0.1683744021824428, "compression_ratio": 1.7890625, "no_speech_prob": 6.893607178426464e-07}, {"id": 1254, "seek": 768174, "start": 7691.3, "end": 7693.58, "text": " So this is a different way of producing just two numbers.", "tokens": [407, 341, 307, 257, 819, 636, 295, 10501, 445, 732, 3547, 13], "temperature": 0.0, "avg_logprob": -0.1683744021824428, "compression_ratio": 1.7890625, "no_speech_prob": 6.893607178426464e-07}, {"id": 1255, "seek": 768174, "start": 7693.58, "end": 7697.58, "text": " I'm not going to say it's better, I'm just going to say it's different.", "tokens": [286, 478, 406, 516, 281, 584, 309, 311, 1101, 11, 286, 478, 445, 516, 281, 584, 309, 311, 819, 13], "temperature": 0.0, "avg_logprob": -0.1683744021824428, "compression_ratio": 1.7890625, "no_speech_prob": 6.893607178426464e-07}, {"id": 1256, "seek": 768174, "start": 7697.58, "end": 7699.179999999999, "text": " But there's a reason we do it.", "tokens": [583, 456, 311, 257, 1778, 321, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1683744021824428, "compression_ratio": 1.7890625, "no_speech_prob": 6.893607178426464e-07}, {"id": 1257, "seek": 768174, "start": 7699.179999999999, "end": 7700.42, "text": " I'll show you the reason.", "tokens": [286, 603, 855, 291, 264, 1778, 13], "temperature": 0.0, "avg_logprob": -0.1683744021824428, "compression_ratio": 1.7890625, "no_speech_prob": 6.893607178426464e-07}, {"id": 1258, "seek": 768174, "start": 7700.42, "end": 7703.5199999999995, "text": " We can now train this model in the usual way.", "tokens": [492, 393, 586, 3847, 341, 2316, 294, 264, 7713, 636, 13], "temperature": 0.0, "avg_logprob": -0.1683744021824428, "compression_ratio": 1.7890625, "no_speech_prob": 6.893607178426464e-07}, {"id": 1259, "seek": 768174, "start": 7703.5199999999995, "end": 7709.46, "text": " So we can say transforms.model, image classifier data from paths, and then we can use that", "tokens": [407, 321, 393, 584, 35592, 13, 8014, 338, 11, 3256, 1508, 9902, 1412, 490, 14518, 11, 293, 550, 321, 393, 764, 300], "temperature": 0.0, "avg_logprob": -0.1683744021824428, "compression_ratio": 1.7890625, "no_speech_prob": 6.893607178426464e-07}, {"id": 1260, "seek": 770946, "start": 7709.46, "end": 7713.62, "text": " conv learner from model data we just learned about.", "tokens": [3754, 33347, 490, 2316, 1412, 321, 445, 3264, 466, 13], "temperature": 0.0, "avg_logprob": -0.14753990173339843, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.5294115200958913e-06}, {"id": 1261, "seek": 770946, "start": 7713.62, "end": 7721.18, "text": " I'm now going to freeze every single layer except for that one, and this is the fourth", "tokens": [286, 478, 586, 516, 281, 15959, 633, 2167, 4583, 3993, 337, 300, 472, 11, 293, 341, 307, 264, 6409], "temperature": 0.0, "avg_logprob": -0.14753990173339843, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.5294115200958913e-06}, {"id": 1262, "seek": 770946, "start": 7721.18, "end": 7725.3, "text": " last layer, so we'll say freeze to minus 4.", "tokens": [1036, 4583, 11, 370, 321, 603, 584, 15959, 281, 3175, 1017, 13], "temperature": 0.0, "avg_logprob": -0.14753990173339843, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.5294115200958913e-06}, {"id": 1263, "seek": 770946, "start": 7725.3, "end": 7728.38, "text": " And so this is just training the last layer.", "tokens": [400, 370, 341, 307, 445, 3097, 264, 1036, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14753990173339843, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.5294115200958913e-06}, {"id": 1264, "seek": 770946, "start": 7728.38, "end": 7733.26, "text": " So we get 99.1% accuracy, so this approach is working fine.", "tokens": [407, 321, 483, 11803, 13, 16, 4, 14170, 11, 370, 341, 3109, 307, 1364, 2489, 13], "temperature": 0.0, "avg_logprob": -0.14753990173339843, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.5294115200958913e-06}, {"id": 1265, "seek": 770946, "start": 7733.26, "end": 7735.26, "text": " And here's what we can do though.", "tokens": [400, 510, 311, 437, 321, 393, 360, 1673, 13], "temperature": 0.0, "avg_logprob": -0.14753990173339843, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.5294115200958913e-06}, {"id": 1266, "seek": 773526, "start": 7735.26, "end": 7745.860000000001, "text": " We can now do something called class activation maps.", "tokens": [492, 393, 586, 360, 746, 1219, 1508, 24433, 11317, 13], "temperature": 0.0, "avg_logprob": -0.09993006477893239, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.0261384179320885e-06}, {"id": 1267, "seek": 773526, "start": 7745.860000000001, "end": 7751.42, "text": " What we're going to do is we're going to try to look at this particular cat, and we're", "tokens": [708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 853, 281, 574, 412, 341, 1729, 3857, 11, 293, 321, 434], "temperature": 0.0, "avg_logprob": -0.09993006477893239, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.0261384179320885e-06}, {"id": 1268, "seek": 773526, "start": 7751.42, "end": 7756.780000000001, "text": " going to use a technique called class activation maps where we take our model and we ask it", "tokens": [516, 281, 764, 257, 6532, 1219, 1508, 24433, 11317, 689, 321, 747, 527, 2316, 293, 321, 1029, 309], "temperature": 0.0, "avg_logprob": -0.09993006477893239, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.0261384179320885e-06}, {"id": 1269, "seek": 773526, "start": 7756.780000000001, "end": 7761.06, "text": " which parts of this image turned out to be important.", "tokens": [597, 3166, 295, 341, 3256, 3574, 484, 281, 312, 1021, 13], "temperature": 0.0, "avg_logprob": -0.09993006477893239, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.0261384179320885e-06}, {"id": 1270, "seek": 776106, "start": 7761.06, "end": 7767.06, "text": " And when we do this, it's going to feed out this picture it's going to create.", "tokens": [400, 562, 321, 360, 341, 11, 309, 311, 516, 281, 3154, 484, 341, 3036, 309, 311, 516, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.13077174700223482, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.601614596642321e-06}, {"id": 1271, "seek": 776106, "start": 7767.06, "end": 7770.780000000001, "text": " And so as you can see here, it's found the cat.", "tokens": [400, 370, 382, 291, 393, 536, 510, 11, 309, 311, 1352, 264, 3857, 13], "temperature": 0.0, "avg_logprob": -0.13077174700223482, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.601614596642321e-06}, {"id": 1272, "seek": 776106, "start": 7770.780000000001, "end": 7772.38, "text": " So how did it do that?", "tokens": [407, 577, 630, 309, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.13077174700223482, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.601614596642321e-06}, {"id": 1273, "seek": 776106, "start": 7772.38, "end": 7778.34, "text": " The way it did that, we'll kind of work backwards, is to produce this matrix.", "tokens": [440, 636, 309, 630, 300, 11, 321, 603, 733, 295, 589, 12204, 11, 307, 281, 5258, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13077174700223482, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.601614596642321e-06}, {"id": 1274, "seek": 776106, "start": 7778.34, "end": 7785.22, "text": " You'll see in this matrix, there's some pretty big numbers around about here which correspond", "tokens": [509, 603, 536, 294, 341, 8141, 11, 456, 311, 512, 1238, 955, 3547, 926, 466, 510, 597, 6805], "temperature": 0.0, "avg_logprob": -0.13077174700223482, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.601614596642321e-06}, {"id": 1275, "seek": 776106, "start": 7785.22, "end": 7787.860000000001, "text": " to our cat.", "tokens": [281, 527, 3857, 13], "temperature": 0.0, "avg_logprob": -0.13077174700223482, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.601614596642321e-06}, {"id": 1276, "seek": 776106, "start": 7787.860000000001, "end": 7790.02, "text": " So what is this matrix?", "tokens": [407, 437, 307, 341, 8141, 30], "temperature": 0.0, "avg_logprob": -0.13077174700223482, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.601614596642321e-06}, {"id": 1277, "seek": 779002, "start": 7790.02, "end": 7804.620000000001, "text": " This matrix is simply equal to the value of this feature matrix times this py vector.", "tokens": [639, 8141, 307, 2935, 2681, 281, 264, 2158, 295, 341, 4111, 8141, 1413, 341, 10664, 8062, 13], "temperature": 0.0, "avg_logprob": -0.10880633354187012, "compression_ratio": 1.4615384615384615, "no_speech_prob": 5.5075988711905666e-06}, {"id": 1278, "seek": 779002, "start": 7804.620000000001, "end": 7811.780000000001, "text": " The py vector is simply equal to the predictions, which in this case said I'm 100% confident", "tokens": [440, 10664, 8062, 307, 2935, 2681, 281, 264, 21264, 11, 597, 294, 341, 1389, 848, 286, 478, 2319, 4, 6679], "temperature": 0.0, "avg_logprob": -0.10880633354187012, "compression_ratio": 1.4615384615384615, "no_speech_prob": 5.5075988711905666e-06}, {"id": 1279, "seek": 779002, "start": 7811.780000000001, "end": 7813.42, "text": " it's a cat.", "tokens": [309, 311, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.10880633354187012, "compression_ratio": 1.4615384615384615, "no_speech_prob": 5.5075988711905666e-06}, {"id": 1280, "seek": 781342, "start": 7813.42, "end": 7822.62, "text": " So this is just equal to the value of, if I just call the model passing in our cat, then", "tokens": [407, 341, 307, 445, 2681, 281, 264, 2158, 295, 11, 498, 286, 445, 818, 264, 2316, 8437, 294, 527, 3857, 11, 550], "temperature": 0.0, "avg_logprob": -0.15622882615952266, "compression_ratio": 1.8496732026143792, "no_speech_prob": 5.285511974761903e-07}, {"id": 1281, "seek": 781342, "start": 7822.62, "end": 7823.62, "text": " we get our predictions.", "tokens": [321, 483, 527, 21264, 13], "temperature": 0.0, "avg_logprob": -0.15622882615952266, "compression_ratio": 1.8496732026143792, "no_speech_prob": 5.285511974761903e-07}, {"id": 1282, "seek": 781342, "start": 7823.62, "end": 7825.66, "text": " So that's just the value of our predictions.", "tokens": [407, 300, 311, 445, 264, 2158, 295, 527, 21264, 13], "temperature": 0.0, "avg_logprob": -0.15622882615952266, "compression_ratio": 1.8496732026143792, "no_speech_prob": 5.285511974761903e-07}, {"id": 1283, "seek": 781342, "start": 7825.66, "end": 7828.9400000000005, "text": " So py is just the value of our predictions.", "tokens": [407, 10664, 307, 445, 264, 2158, 295, 527, 21264, 13], "temperature": 0.0, "avg_logprob": -0.15622882615952266, "compression_ratio": 1.8496732026143792, "no_speech_prob": 5.285511974761903e-07}, {"id": 1284, "seek": 781342, "start": 7828.9400000000005, "end": 7829.9400000000005, "text": " What about feet?", "tokens": [708, 466, 3521, 30], "temperature": 0.0, "avg_logprob": -0.15622882615952266, "compression_ratio": 1.8496732026143792, "no_speech_prob": 5.285511974761903e-07}, {"id": 1285, "seek": 781342, "start": 7829.9400000000005, "end": 7831.74, "text": " What's that equal to?", "tokens": [708, 311, 300, 2681, 281, 30], "temperature": 0.0, "avg_logprob": -0.15622882615952266, "compression_ratio": 1.8496732026143792, "no_speech_prob": 5.285511974761903e-07}, {"id": 1286, "seek": 781342, "start": 7831.74, "end": 7840.02, "text": " Feet is equal to the values in this layer.", "tokens": [3697, 302, 307, 2681, 281, 264, 4190, 294, 341, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15622882615952266, "compression_ratio": 1.8496732026143792, "no_speech_prob": 5.285511974761903e-07}, {"id": 1287, "seek": 784002, "start": 7840.02, "end": 7846.740000000001, "text": " In other words, the value that comes out of the final convolutional layer.", "tokens": [682, 661, 2283, 11, 264, 2158, 300, 1487, 484, 295, 264, 2572, 45216, 304, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1315698535354049, "compression_ratio": 1.2878787878787878, "no_speech_prob": 5.014726411900483e-06}, {"id": 1288, "seek": 784002, "start": 7846.740000000001, "end": 7852.18, "text": " So it's actually the 7x7x2.", "tokens": [407, 309, 311, 767, 264, 1614, 87, 22, 87, 17, 13], "temperature": 0.0, "avg_logprob": -0.1315698535354049, "compression_ratio": 1.2878787878787878, "no_speech_prob": 5.014726411900483e-06}, {"id": 1289, "seek": 784002, "start": 7852.18, "end": 7861.900000000001, "text": " And so you can see here, the shape of features is 2 filters by 7x7.", "tokens": [400, 370, 291, 393, 536, 510, 11, 264, 3909, 295, 4122, 307, 568, 15995, 538, 1614, 87, 22, 13], "temperature": 0.0, "avg_logprob": -0.1315698535354049, "compression_ratio": 1.2878787878787878, "no_speech_prob": 5.014726411900483e-06}, {"id": 1290, "seek": 786190, "start": 7861.9, "end": 7872.099999999999, "text": " So the idea is if we multiply that vector by that tensor, then it's going to end up", "tokens": [407, 264, 1558, 307, 498, 321, 12972, 300, 8062, 538, 300, 40863, 11, 550, 309, 311, 516, 281, 917, 493], "temperature": 0.0, "avg_logprob": -0.14494800567626953, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.7694031814462505e-06}, {"id": 1291, "seek": 786190, "start": 7872.099999999999, "end": 7878.139999999999, "text": " grabbing all of the first channel, because that's a 1, and none of the second channel,", "tokens": [23771, 439, 295, 264, 700, 2269, 11, 570, 300, 311, 257, 502, 11, 293, 6022, 295, 264, 1150, 2269, 11], "temperature": 0.0, "avg_logprob": -0.14494800567626953, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.7694031814462505e-06}, {"id": 1292, "seek": 786190, "start": 7878.139999999999, "end": 7880.5, "text": " because that's a 0.", "tokens": [570, 300, 311, 257, 1958, 13], "temperature": 0.0, "avg_logprob": -0.14494800567626953, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.7694031814462505e-06}, {"id": 1293, "seek": 786190, "start": 7880.5, "end": 7887.94, "text": " And so therefore it's going to return the value of the last convolutional layer for", "tokens": [400, 370, 4412, 309, 311, 516, 281, 2736, 264, 2158, 295, 264, 1036, 45216, 304, 4583, 337], "temperature": 0.0, "avg_logprob": -0.14494800567626953, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.7694031814462505e-06}, {"id": 1294, "seek": 786190, "start": 7887.94, "end": 7891.62, "text": " the section which lines up with being a cat.", "tokens": [264, 3541, 597, 3876, 493, 365, 885, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.14494800567626953, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.7694031814462505e-06}, {"id": 1295, "seek": 789162, "start": 7891.62, "end": 7897.099999999999, "text": " If you think about it, the first section lines up with being a cat, the second section lines", "tokens": [759, 291, 519, 466, 309, 11, 264, 700, 3541, 3876, 493, 365, 885, 257, 3857, 11, 264, 1150, 3541, 3876], "temperature": 0.0, "avg_logprob": -0.08441238796588071, "compression_ratio": 1.727699530516432, "no_speech_prob": 3.611977490436402e-06}, {"id": 1296, "seek": 789162, "start": 7897.099999999999, "end": 7898.42, "text": " up with being a dog.", "tokens": [493, 365, 885, 257, 3000, 13], "temperature": 0.0, "avg_logprob": -0.08441238796588071, "compression_ratio": 1.727699530516432, "no_speech_prob": 3.611977490436402e-06}, {"id": 1297, "seek": 789162, "start": 7898.42, "end": 7905.82, "text": " So if we multiply that tensor by that tensor, we end up with this matrix.", "tokens": [407, 498, 321, 12972, 300, 40863, 538, 300, 40863, 11, 321, 917, 493, 365, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08441238796588071, "compression_ratio": 1.727699530516432, "no_speech_prob": 3.611977490436402e-06}, {"id": 1298, "seek": 789162, "start": 7905.82, "end": 7911.66, "text": " And this matrix is which parts are most like a cat.", "tokens": [400, 341, 8141, 307, 597, 3166, 366, 881, 411, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.08441238796588071, "compression_ratio": 1.727699530516432, "no_speech_prob": 3.611977490436402e-06}, {"id": 1299, "seek": 789162, "start": 7911.66, "end": 7917.38, "text": " Or to put it another way, in our model, the only thing that happened after the convolutional", "tokens": [1610, 281, 829, 309, 1071, 636, 11, 294, 527, 2316, 11, 264, 787, 551, 300, 2011, 934, 264, 45216, 304], "temperature": 0.0, "avg_logprob": -0.08441238796588071, "compression_ratio": 1.727699530516432, "no_speech_prob": 3.611977490436402e-06}, {"id": 1300, "seek": 789162, "start": 7917.38, "end": 7920.82, "text": " layer was an average pooling layer.", "tokens": [4583, 390, 364, 4274, 7005, 278, 4583, 13], "temperature": 0.0, "avg_logprob": -0.08441238796588071, "compression_ratio": 1.727699530516432, "no_speech_prob": 3.611977490436402e-06}, {"id": 1301, "seek": 792082, "start": 7920.82, "end": 7927.36, "text": " So the average pooling layer took that 7x7 grid and said average out how much each part", "tokens": [407, 264, 4274, 7005, 278, 4583, 1890, 300, 1614, 87, 22, 10748, 293, 848, 4274, 484, 577, 709, 1184, 644], "temperature": 0.0, "avg_logprob": -0.09271522403992329, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.0030134944827296e-06}, {"id": 1302, "seek": 792082, "start": 7927.36, "end": 7928.36, "text": " is cat-like.", "tokens": [307, 3857, 12, 4092, 13], "temperature": 0.0, "avg_logprob": -0.09271522403992329, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.0030134944827296e-06}, {"id": 1303, "seek": 792082, "start": 7928.36, "end": 7937.04, "text": " So my final value, my final prediction was the average cattiness of the whole thing.", "tokens": [407, 452, 2572, 2158, 11, 452, 2572, 17630, 390, 264, 4274, 269, 1591, 1324, 295, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.09271522403992329, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.0030134944827296e-06}, {"id": 1304, "seek": 792082, "start": 7937.04, "end": 7942.639999999999, "text": " And so because it had to be able to average out these things to get the average cattiness,", "tokens": [400, 370, 570, 309, 632, 281, 312, 1075, 281, 4274, 484, 613, 721, 281, 483, 264, 4274, 269, 1591, 1324, 11], "temperature": 0.0, "avg_logprob": -0.09271522403992329, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.0030134944827296e-06}, {"id": 1305, "seek": 792082, "start": 7942.639999999999, "end": 7949.74, "text": " that means I could then just take this matrix and resize it to be the same size as my original", "tokens": [300, 1355, 286, 727, 550, 445, 747, 341, 8141, 293, 50069, 309, 281, 312, 264, 912, 2744, 382, 452, 3380], "temperature": 0.0, "avg_logprob": -0.09271522403992329, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.0030134944827296e-06}, {"id": 1306, "seek": 794974, "start": 7949.74, "end": 7952.219999999999, "text": " cat and just overlay it on top.", "tokens": [3857, 293, 445, 31741, 309, 322, 1192, 13], "temperature": 0.0, "avg_logprob": -0.20710343050669475, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.4060989289864665e-06}, {"id": 1307, "seek": 794974, "start": 7952.219999999999, "end": 7954.48, "text": " You get this heatmap.", "tokens": [509, 483, 341, 3738, 24223, 13], "temperature": 0.0, "avg_logprob": -0.20710343050669475, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.4060989289864665e-06}, {"id": 1308, "seek": 794974, "start": 7954.48, "end": 7961.58, "text": " So the way you can use this technique at home is to basically calculate this matrix on some", "tokens": [407, 264, 636, 291, 393, 764, 341, 6532, 412, 1280, 307, 281, 1936, 8873, 341, 8141, 322, 512], "temperature": 0.0, "avg_logprob": -0.20710343050669475, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.4060989289864665e-06}, {"id": 1309, "seek": 794974, "start": 7961.58, "end": 7965.0199999999995, "text": " really big picture.", "tokens": [534, 955, 3036, 13], "temperature": 0.0, "avg_logprob": -0.20710343050669475, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.4060989289864665e-06}, {"id": 1310, "seek": 794974, "start": 7965.0199999999995, "end": 7970.3, "text": " You can calculate this matrix on a quick small little conv net and then zoom into the bit", "tokens": [509, 393, 8873, 341, 8141, 322, 257, 1702, 1359, 707, 3754, 2533, 293, 550, 8863, 666, 264, 857], "temperature": 0.0, "avg_logprob": -0.20710343050669475, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.4060989289864665e-06}, {"id": 1311, "seek": 794974, "start": 7970.3, "end": 7975.74, "text": " that has the highest value and then rerun it just on that part.", "tokens": [300, 575, 264, 6343, 2158, 293, 550, 43819, 409, 309, 445, 322, 300, 644, 13], "temperature": 0.0, "avg_logprob": -0.20710343050669475, "compression_ratio": 1.696808510638298, "no_speech_prob": 2.4060989289864665e-06}, {"id": 1312, "seek": 797574, "start": 7975.74, "end": 7980.2, "text": " So it's like, oh this is the area that seems to be the most like a cat or most like a dog", "tokens": [407, 309, 311, 411, 11, 1954, 341, 307, 264, 1859, 300, 2544, 281, 312, 264, 881, 411, 257, 3857, 420, 881, 411, 257, 3000], "temperature": 0.0, "avg_logprob": -0.16585466883204006, "compression_ratio": 1.6587301587301588, "no_speech_prob": 1.1365624459358514e-06}, {"id": 1313, "seek": 797574, "start": 7980.2, "end": 7984.139999999999, "text": " that zoom into that bit.", "tokens": [300, 8863, 666, 300, 857, 13], "temperature": 0.0, "avg_logprob": -0.16585466883204006, "compression_ratio": 1.6587301587301588, "no_speech_prob": 1.1365624459358514e-06}, {"id": 1314, "seek": 797574, "start": 7984.139999999999, "end": 7989.7, "text": " So I skipped over that pretty quickly because we ran out of time.", "tokens": [407, 286, 30193, 670, 300, 1238, 2661, 570, 321, 5872, 484, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.16585466883204006, "compression_ratio": 1.6587301587301588, "no_speech_prob": 1.1365624459358514e-06}, {"id": 1315, "seek": 797574, "start": 7989.7, "end": 7993.0599999999995, "text": " And so we'll be learning more about these kind of approaches in part 2 and we can talk", "tokens": [400, 370, 321, 603, 312, 2539, 544, 466, 613, 733, 295, 11587, 294, 644, 568, 293, 321, 393, 751], "temperature": 0.0, "avg_logprob": -0.16585466883204006, "compression_ratio": 1.6587301587301588, "no_speech_prob": 1.1365624459358514e-06}, {"id": 1316, "seek": 797574, "start": 7993.0599999999995, "end": 7994.0599999999995, "text": " about it more on the forum.", "tokens": [466, 309, 544, 322, 264, 17542, 13], "temperature": 0.0, "avg_logprob": -0.16585466883204006, "compression_ratio": 1.6587301587301588, "no_speech_prob": 1.1365624459358514e-06}, {"id": 1317, "seek": 797574, "start": 7994.0599999999995, "end": 7995.42, "text": " But hopefully you get the idea.", "tokens": [583, 4696, 291, 483, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.16585466883204006, "compression_ratio": 1.6587301587301588, "no_speech_prob": 1.1365624459358514e-06}, {"id": 1318, "seek": 797574, "start": 7995.42, "end": 8002.98, "text": " The one thing I totally skipped over was how do we actually ask for that particular layer.", "tokens": [440, 472, 551, 286, 3879, 30193, 670, 390, 577, 360, 321, 767, 1029, 337, 300, 1729, 4583, 13], "temperature": 0.0, "avg_logprob": -0.16585466883204006, "compression_ratio": 1.6587301587301588, "no_speech_prob": 1.1365624459358514e-06}, {"id": 1319, "seek": 800298, "start": 8002.98, "end": 8007.66, "text": " And I'll let you read about this during the week, but basically there's a thing called", "tokens": [400, 286, 603, 718, 291, 1401, 466, 341, 1830, 264, 1243, 11, 457, 1936, 456, 311, 257, 551, 1219], "temperature": 0.0, "avg_logprob": -0.1422090703790838, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.8058348359772936e-05}, {"id": 1320, "seek": 800298, "start": 8007.66, "end": 8009.179999999999, "text": " a hook.", "tokens": [257, 6328, 13], "temperature": 0.0, "avg_logprob": -0.1422090703790838, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.8058348359772936e-05}, {"id": 1321, "seek": 800298, "start": 8009.179999999999, "end": 8017.94, "text": " So we called save features, which is this little class that we wrote that goes register", "tokens": [407, 321, 1219, 3155, 4122, 11, 597, 307, 341, 707, 1508, 300, 321, 4114, 300, 1709, 7280], "temperature": 0.0, "avg_logprob": -0.1422090703790838, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.8058348359772936e-05}, {"id": 1322, "seek": 800298, "start": 8017.94, "end": 8019.58, "text": " forward hook.", "tokens": [2128, 6328, 13], "temperature": 0.0, "avg_logprob": -0.1422090703790838, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.8058348359772936e-05}, {"id": 1323, "seek": 800298, "start": 8019.58, "end": 8024.9, "text": " And basically a forward hook is a special PyTorch thing that every time it calculates", "tokens": [400, 1936, 257, 2128, 6328, 307, 257, 2121, 9953, 51, 284, 339, 551, 300, 633, 565, 309, 4322, 1024], "temperature": 0.0, "avg_logprob": -0.1422090703790838, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.8058348359772936e-05}, {"id": 1324, "seek": 800298, "start": 8024.9, "end": 8027.78, "text": " a layer, it runs this function.", "tokens": [257, 4583, 11, 309, 6676, 341, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1422090703790838, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.8058348359772936e-05}, {"id": 1325, "seek": 800298, "start": 8027.78, "end": 8029.98, "text": " It's like a callback.", "tokens": [467, 311, 411, 257, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.1422090703790838, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.8058348359772936e-05}, {"id": 1326, "seek": 800298, "start": 8029.98, "end": 8032.86, "text": " It's like a callback that happens every time it calculates a layer.", "tokens": [467, 311, 411, 257, 818, 3207, 300, 2314, 633, 565, 309, 4322, 1024, 257, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1422090703790838, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.8058348359772936e-05}, {"id": 1327, "seek": 803286, "start": 8032.86, "end": 8039.5, "text": " And so in this case, it just saved the value of the particular layer that I was interested", "tokens": [400, 370, 294, 341, 1389, 11, 309, 445, 6624, 264, 2158, 295, 264, 1729, 4583, 300, 286, 390, 3102], "temperature": 0.0, "avg_logprob": -0.11625509995680589, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.0894807423464954e-06}, {"id": 1328, "seek": 803286, "start": 8039.5, "end": 8041.299999999999, "text": " in.", "tokens": [294, 13], "temperature": 0.0, "avg_logprob": -0.11625509995680589, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.0894807423464954e-06}, {"id": 1329, "seek": 803286, "start": 8041.299999999999, "end": 8051.0599999999995, "text": " And so that way I was able to go inside here and grab those features out after I was done.", "tokens": [400, 370, 300, 636, 286, 390, 1075, 281, 352, 1854, 510, 293, 4444, 729, 4122, 484, 934, 286, 390, 1096, 13], "temperature": 0.0, "avg_logprob": -0.11625509995680589, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.0894807423464954e-06}, {"id": 1330, "seek": 803286, "start": 8051.0599999999995, "end": 8056.219999999999, "text": " So I call save features, that gives me my hook, and then later on I can just grab the", "tokens": [407, 286, 818, 3155, 4122, 11, 300, 2709, 385, 452, 6328, 11, 293, 550, 1780, 322, 286, 393, 445, 4444, 264], "temperature": 0.0, "avg_logprob": -0.11625509995680589, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.0894807423464954e-06}, {"id": 1331, "seek": 803286, "start": 8056.219999999999, "end": 8058.42, "text": " value that I saved.", "tokens": [2158, 300, 286, 6624, 13], "temperature": 0.0, "avg_logprob": -0.11625509995680589, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.0894807423464954e-06}, {"id": 1332, "seek": 803286, "start": 8058.42, "end": 8061.86, "text": " So I skipped over that pretty quickly, but if you look in the PyTorch docs, they have", "tokens": [407, 286, 30193, 670, 300, 1238, 2661, 11, 457, 498, 291, 574, 294, 264, 9953, 51, 284, 339, 45623, 11, 436, 362], "temperature": 0.0, "avg_logprob": -0.11625509995680589, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.0894807423464954e-06}, {"id": 1333, "seek": 806186, "start": 8061.86, "end": 8064.62, "text": " some more information and help about that.", "tokens": [512, 544, 1589, 293, 854, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.3142697533895803, "compression_ratio": 1.4861111111111112, "no_speech_prob": 1.983314723474905e-05}, {"id": 1334, "seek": 806186, "start": 8064.62, "end": 8066.42, "text": " Yes, Yannett.", "tokens": [1079, 11, 398, 969, 3093, 13], "temperature": 0.0, "avg_logprob": -0.3142697533895803, "compression_ratio": 1.4861111111111112, "no_speech_prob": 1.983314723474905e-05}, {"id": 1335, "seek": 806186, "start": 8066.42, "end": 8067.42, "text": " Do you have the?", "tokens": [1144, 291, 362, 264, 30], "temperature": 0.0, "avg_logprob": -0.3142697533895803, "compression_ratio": 1.4861111111111112, "no_speech_prob": 1.983314723474905e-05}, {"id": 1336, "seek": 806186, "start": 8067.42, "end": 8068.42, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.3142697533895803, "compression_ratio": 1.4861111111111112, "no_speech_prob": 1.983314723474905e-05}, {"id": 1337, "seek": 806186, "start": 8068.42, "end": 8076.099999999999, "text": " Jeremy, can you spend 5 minutes talking about your journey into deep learning?", "tokens": [17809, 11, 393, 291, 3496, 1025, 2077, 1417, 466, 428, 4671, 666, 2452, 2539, 30], "temperature": 0.0, "avg_logprob": -0.3142697533895803, "compression_ratio": 1.4861111111111112, "no_speech_prob": 1.983314723474905e-05}, {"id": 1338, "seek": 806186, "start": 8076.099999999999, "end": 8086.38, "text": " And finally, how can we keep up with important research that is important to practitioners?", "tokens": [400, 2721, 11, 577, 393, 321, 1066, 493, 365, 1021, 2132, 300, 307, 1021, 281, 25742, 30], "temperature": 0.0, "avg_logprob": -0.3142697533895803, "compression_ratio": 1.4861111111111112, "no_speech_prob": 1.983314723474905e-05}, {"id": 1339, "seek": 806186, "start": 8086.38, "end": 8090.5, "text": " I think I'll close more on the latter bit, which is like what now.", "tokens": [286, 519, 286, 603, 1998, 544, 322, 264, 18481, 857, 11, 597, 307, 411, 437, 586, 13], "temperature": 0.0, "avg_logprob": -0.3142697533895803, "compression_ratio": 1.4861111111111112, "no_speech_prob": 1.983314723474905e-05}, {"id": 1340, "seek": 809050, "start": 8090.5, "end": 8097.56, "text": " So for those of you who are interested, you should aim to come back for part 2.", "tokens": [407, 337, 729, 295, 291, 567, 366, 3102, 11, 291, 820, 5939, 281, 808, 646, 337, 644, 568, 13], "temperature": 0.0, "avg_logprob": -0.11981894618781037, "compression_ratio": 1.7829457364341086, "no_speech_prob": 1.1842763342428952e-05}, {"id": 1341, "seek": 809050, "start": 8097.56, "end": 8100.9, "text": " If you're aiming to come back for part 2, how many people would like to come back for", "tokens": [759, 291, 434, 20253, 281, 808, 646, 337, 644, 568, 11, 577, 867, 561, 576, 411, 281, 808, 646, 337], "temperature": 0.0, "avg_logprob": -0.11981894618781037, "compression_ratio": 1.7829457364341086, "no_speech_prob": 1.1842763342428952e-05}, {"id": 1342, "seek": 809050, "start": 8100.9, "end": 8101.9, "text": " part 2?", "tokens": [644, 568, 30], "temperature": 0.0, "avg_logprob": -0.11981894618781037, "compression_ratio": 1.7829457364341086, "no_speech_prob": 1.1842763342428952e-05}, {"id": 1343, "seek": 809050, "start": 8101.9, "end": 8105.02, "text": " That's not bad, I think almost everybody.", "tokens": [663, 311, 406, 1578, 11, 286, 519, 1920, 2201, 13], "temperature": 0.0, "avg_logprob": -0.11981894618781037, "compression_ratio": 1.7829457364341086, "no_speech_prob": 1.1842763342428952e-05}, {"id": 1344, "seek": 809050, "start": 8105.02, "end": 8108.7, "text": " So if you want to come back for part 2, be aware of this.", "tokens": [407, 498, 291, 528, 281, 808, 646, 337, 644, 568, 11, 312, 3650, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.11981894618781037, "compression_ratio": 1.7829457364341086, "no_speech_prob": 1.1842763342428952e-05}, {"id": 1345, "seek": 809050, "start": 8108.7, "end": 8112.98, "text": " By that time, you're expected to have mastered all of the techniques we've learned in part", "tokens": [3146, 300, 565, 11, 291, 434, 5176, 281, 362, 38686, 439, 295, 264, 7512, 321, 600, 3264, 294, 644], "temperature": 0.0, "avg_logprob": -0.11981894618781037, "compression_ratio": 1.7829457364341086, "no_speech_prob": 1.1842763342428952e-05}, {"id": 1346, "seek": 809050, "start": 8112.98, "end": 8113.98, "text": " 1.", "tokens": [502, 13], "temperature": 0.0, "avg_logprob": -0.11981894618781037, "compression_ratio": 1.7829457364341086, "no_speech_prob": 1.1842763342428952e-05}, {"id": 1347, "seek": 809050, "start": 8113.98, "end": 8119.46, "text": " There's plenty of time between now and then, even if you haven't done much or any ML before.", "tokens": [821, 311, 7140, 295, 565, 1296, 586, 293, 550, 11, 754, 498, 291, 2378, 380, 1096, 709, 420, 604, 21601, 949, 13], "temperature": 0.0, "avg_logprob": -0.11981894618781037, "compression_ratio": 1.7829457364341086, "no_speech_prob": 1.1842763342428952e-05}, {"id": 1348, "seek": 811946, "start": 8119.46, "end": 8124.86, "text": " But it does assume that you're going to be working at the same level of intensity from", "tokens": [583, 309, 775, 6552, 300, 291, 434, 516, 281, 312, 1364, 412, 264, 912, 1496, 295, 13749, 490], "temperature": 0.0, "avg_logprob": -0.16329785755702428, "compression_ratio": 1.7075812274368232, "no_speech_prob": 2.8408905563992448e-05}, {"id": 1349, "seek": 811946, "start": 8124.86, "end": 8129.38, "text": " now until then that you have been with practicing.", "tokens": [586, 1826, 550, 300, 291, 362, 668, 365, 11350, 13], "temperature": 0.0, "avg_logprob": -0.16329785755702428, "compression_ratio": 1.7075812274368232, "no_speech_prob": 2.8408905563992448e-05}, {"id": 1350, "seek": 811946, "start": 8129.38, "end": 8133.62, "text": " So generally speaking, the people who did well in part 2 last year had watched each", "tokens": [407, 5101, 4124, 11, 264, 561, 567, 630, 731, 294, 644, 568, 1036, 1064, 632, 6337, 1184], "temperature": 0.0, "avg_logprob": -0.16329785755702428, "compression_ratio": 1.7075812274368232, "no_speech_prob": 2.8408905563992448e-05}, {"id": 1351, "seek": 811946, "start": 8133.62, "end": 8136.52, "text": " of the videos about 3 times.", "tokens": [295, 264, 2145, 466, 805, 1413, 13], "temperature": 0.0, "avg_logprob": -0.16329785755702428, "compression_ratio": 1.7075812274368232, "no_speech_prob": 2.8408905563992448e-05}, {"id": 1352, "seek": 811946, "start": 8136.52, "end": 8141.58, "text": " And some of the people actually discovered they learned some of them off by heart by", "tokens": [400, 512, 295, 264, 561, 767, 6941, 436, 3264, 512, 295, 552, 766, 538, 1917, 538], "temperature": 0.0, "avg_logprob": -0.16329785755702428, "compression_ratio": 1.7075812274368232, "no_speech_prob": 2.8408905563992448e-05}, {"id": 1353, "seek": 811946, "start": 8141.58, "end": 8142.58, "text": " mistake.", "tokens": [6146, 13], "temperature": 0.0, "avg_logprob": -0.16329785755702428, "compression_ratio": 1.7075812274368232, "no_speech_prob": 2.8408905563992448e-05}, {"id": 1354, "seek": 811946, "start": 8142.58, "end": 8144.56, "text": " So watching the videos again is helpful.", "tokens": [407, 1976, 264, 2145, 797, 307, 4961, 13], "temperature": 0.0, "avg_logprob": -0.16329785755702428, "compression_ratio": 1.7075812274368232, "no_speech_prob": 2.8408905563992448e-05}, {"id": 1355, "seek": 811946, "start": 8144.56, "end": 8148.7, "text": " And make sure you get to the point that you can recreate the notebooks without watching", "tokens": [400, 652, 988, 291, 483, 281, 264, 935, 300, 291, 393, 25833, 264, 43782, 1553, 1976], "temperature": 0.0, "avg_logprob": -0.16329785755702428, "compression_ratio": 1.7075812274368232, "no_speech_prob": 2.8408905563992448e-05}, {"id": 1356, "seek": 814870, "start": 8148.7, "end": 8149.7, "text": " the videos.", "tokens": [264, 2145, 13], "temperature": 0.0, "avg_logprob": -0.17840194702148438, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.771860287495656e-06}, {"id": 1357, "seek": 814870, "start": 8149.7, "end": 8155.0599999999995, "text": " So to make it more interesting, obviously, try and recreate the notebooks using different", "tokens": [407, 281, 652, 309, 544, 1880, 11, 2745, 11, 853, 293, 25833, 264, 43782, 1228, 819], "temperature": 0.0, "avg_logprob": -0.17840194702148438, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.771860287495656e-06}, {"id": 1358, "seek": 814870, "start": 8155.0599999999995, "end": 8158.54, "text": " data sets.", "tokens": [1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.17840194702148438, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.771860287495656e-06}, {"id": 1359, "seek": 814870, "start": 8158.54, "end": 8163.0199999999995, "text": " And definitely then just keep up with the forum and you'll see people keep on posting", "tokens": [400, 2138, 550, 445, 1066, 493, 365, 264, 17542, 293, 291, 603, 536, 561, 1066, 322, 15978], "temperature": 0.0, "avg_logprob": -0.17840194702148438, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.771860287495656e-06}, {"id": 1360, "seek": 814870, "start": 8163.0199999999995, "end": 8166.34, "text": " more stuff about recent papers and recent advances.", "tokens": [544, 1507, 466, 5162, 10577, 293, 5162, 25297, 13], "temperature": 0.0, "avg_logprob": -0.17840194702148438, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.771860287495656e-06}, {"id": 1361, "seek": 814870, "start": 8166.34, "end": 8171.38, "text": " And over the next couple of months, you'll find increasingly less and less of it seems", "tokens": [400, 670, 264, 958, 1916, 295, 2493, 11, 291, 603, 915, 12980, 1570, 293, 1570, 295, 309, 2544], "temperature": 0.0, "avg_logprob": -0.17840194702148438, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.771860287495656e-06}, {"id": 1362, "seek": 814870, "start": 8171.38, "end": 8176.5, "text": " weird and mysterious, and more and more of it makes perfect sense.", "tokens": [3657, 293, 13831, 11, 293, 544, 293, 544, 295, 309, 1669, 2176, 2020, 13], "temperature": 0.0, "avg_logprob": -0.17840194702148438, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.771860287495656e-06}, {"id": 1363, "seek": 817650, "start": 8176.5, "end": 8180.26, "text": " And so it's a bit of a case of just staying tenacious.", "tokens": [400, 370, 309, 311, 257, 857, 295, 257, 1389, 295, 445, 7939, 2064, 22641, 13], "temperature": 0.0, "avg_logprob": -0.17241045280739112, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.080394193006214e-06}, {"id": 1364, "seek": 817650, "start": 8180.26, "end": 8183.86, "text": " There's always going to be stuff that you don't understand yet.", "tokens": [821, 311, 1009, 516, 281, 312, 1507, 300, 291, 500, 380, 1223, 1939, 13], "temperature": 0.0, "avg_logprob": -0.17241045280739112, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.080394193006214e-06}, {"id": 1365, "seek": 817650, "start": 8183.86, "end": 8188.58, "text": " But you'll be surprised if you go back to Lesson 1 and 2 now, you'll be like, oh, that's", "tokens": [583, 291, 603, 312, 6100, 498, 291, 352, 646, 281, 18649, 266, 502, 293, 568, 586, 11, 291, 603, 312, 411, 11, 1954, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.17241045280739112, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.080394193006214e-06}, {"id": 1366, "seek": 817650, "start": 8188.58, "end": 8192.74, "text": " all trivial.", "tokens": [439, 26703, 13], "temperature": 0.0, "avg_logprob": -0.17241045280739112, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.080394193006214e-06}, {"id": 1367, "seek": 817650, "start": 8192.74, "end": 8199.9, "text": " So that's kind of hopefully a bit of your learning journey.", "tokens": [407, 300, 311, 733, 295, 4696, 257, 857, 295, 428, 2539, 4671, 13], "temperature": 0.0, "avg_logprob": -0.17241045280739112, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.080394193006214e-06}, {"id": 1368, "seek": 817650, "start": 8199.9, "end": 8203.86, "text": " The main thing I've noticed is that people who succeed are the ones who just keep working", "tokens": [440, 2135, 551, 286, 600, 5694, 307, 300, 561, 567, 7754, 366, 264, 2306, 567, 445, 1066, 1364], "temperature": 0.0, "avg_logprob": -0.17241045280739112, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.080394193006214e-06}, {"id": 1369, "seek": 817650, "start": 8203.86, "end": 8204.86, "text": " at it.", "tokens": [412, 309, 13], "temperature": 0.0, "avg_logprob": -0.17241045280739112, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.080394193006214e-06}, {"id": 1370, "seek": 820486, "start": 8204.86, "end": 8209.18, "text": " If you're not coming back here every Monday, you're not going to have that forcing function.", "tokens": [759, 291, 434, 406, 1348, 646, 510, 633, 8138, 11, 291, 434, 406, 516, 281, 362, 300, 19030, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1490852289032518, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.40639322125935e-05}, {"id": 1371, "seek": 820486, "start": 8209.18, "end": 8213.62, "text": " I've noticed the forum suddenly gets busy at 5pm on a Monday.", "tokens": [286, 600, 5694, 264, 17542, 5800, 2170, 5856, 412, 1025, 14395, 322, 257, 8138, 13], "temperature": 0.0, "avg_logprob": -0.1490852289032518, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.40639322125935e-05}, {"id": 1372, "seek": 820486, "start": 8213.62, "end": 8217.42, "text": " It's like, oh, the course is about to start and suddenly these questions start coming", "tokens": [467, 311, 411, 11, 1954, 11, 264, 1164, 307, 466, 281, 722, 293, 5800, 613, 1651, 722, 1348], "temperature": 0.0, "avg_logprob": -0.1490852289032518, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.40639322125935e-05}, {"id": 1373, "seek": 820486, "start": 8217.42, "end": 8218.42, "text": " in.", "tokens": [294, 13], "temperature": 0.0, "avg_logprob": -0.1490852289032518, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.40639322125935e-05}, {"id": 1374, "seek": 820486, "start": 8218.42, "end": 8224.140000000001, "text": " So now that you don't have that forcing function, try and use some other technique to give yourself", "tokens": [407, 586, 300, 291, 500, 380, 362, 300, 19030, 2445, 11, 853, 293, 764, 512, 661, 6532, 281, 976, 1803], "temperature": 0.0, "avg_logprob": -0.1490852289032518, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.40639322125935e-05}, {"id": 1375, "seek": 820486, "start": 8224.140000000001, "end": 8225.140000000001, "text": " that little kick.", "tokens": [300, 707, 4437, 13], "temperature": 0.0, "avg_logprob": -0.1490852289032518, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.40639322125935e-05}, {"id": 1376, "seek": 820486, "start": 8225.140000000001, "end": 8229.36, "text": " Maybe you can tell your partner at home, I'm going to try and produce something every Saturday", "tokens": [2704, 291, 393, 980, 428, 4975, 412, 1280, 11, 286, 478, 516, 281, 853, 293, 5258, 746, 633, 8803], "temperature": 0.0, "avg_logprob": -0.1490852289032518, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.40639322125935e-05}, {"id": 1377, "seek": 822936, "start": 8229.36, "end": 8235.060000000001, "text": " for the next 4 weeks, or I'm going to try and finish reading this paper or something.", "tokens": [337, 264, 958, 1017, 3259, 11, 420, 286, 478, 516, 281, 853, 293, 2413, 3760, 341, 3035, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.24141316186814082, "compression_ratio": 1.4455445544554455, "no_speech_prob": 1.1125007404189091e-05}, {"id": 1378, "seek": 822936, "start": 8235.060000000001, "end": 8240.42, "text": " Anyway, so I hope to see you all back in March.", "tokens": [5684, 11, 370, 286, 1454, 281, 536, 291, 439, 646, 294, 6129, 13], "temperature": 0.0, "avg_logprob": -0.24141316186814082, "compression_ratio": 1.4455445544554455, "no_speech_prob": 1.1125007404189091e-05}, {"id": 1379, "seek": 822936, "start": 8240.42, "end": 8243.78, "text": " Even regardless whether I do or don't, it's been a really great pleasure to get to know", "tokens": [2754, 10060, 1968, 286, 360, 420, 500, 380, 11, 309, 311, 668, 257, 534, 869, 6834, 281, 483, 281, 458], "temperature": 0.0, "avg_logprob": -0.24141316186814082, "compression_ratio": 1.4455445544554455, "no_speech_prob": 1.1125007404189091e-05}, {"id": 1380, "seek": 822936, "start": 8243.78, "end": 8244.980000000001, "text": " you all.", "tokens": [291, 439, 13], "temperature": 0.0, "avg_logprob": -0.24141316186814082, "compression_ratio": 1.4455445544554455, "no_speech_prob": 1.1125007404189091e-05}, {"id": 1381, "seek": 822936, "start": 8244.980000000001, "end": 8246.74, "text": " And I hope to keep seeing you on the forum.", "tokens": [400, 286, 1454, 281, 1066, 2577, 291, 322, 264, 17542, 13], "temperature": 0.0, "avg_logprob": -0.24141316186814082, "compression_ratio": 1.4455445544554455, "no_speech_prob": 1.1125007404189091e-05}, {"id": 1382, "seek": 824674, "start": 8246.74, "end": 8260.26, "text": " Thanks very much.", "tokens": [50364, 2561, 588, 709, 13, 51040], "temperature": 0.0, "avg_logprob": -0.7344117845807757, "compression_ratio": 0.68, "no_speech_prob": 0.00011914224160136655}], "language": "en"}