{"text": " So, Rachel and I started FastAI with this idea of make neural networks uncool again. It is a grand plan to be sure because currently they are terribly cool. But really there are some things that we want to see improve and that's why we're doing this course. We're actually not making any money out of this course. We're donating our fees both to the diversity fellowships that we're running and also to the Fred Hollows Foundation. I would like to briefly give a quick pitch for the Fred Hollows Foundation for those of you who aren't aware of it. Because as you know, deep learning is fantastic for computer vision. It's basically allowing computers to see for the first time. What you might not realize is that there are 3 or 4 million people in the world who can't see because they have something called cataract blindness. Cataract blindness can be cured for $25 per eye. And actually the group of people who got that price down from thousands of dollars to $25 was Fred Hollows, who was the Australian of the year some years ago. He's passed away now. But his legacy is in this foundation where if you donate $25, you're giving somebody their sight back. So as you learn to teach computers how to see, Rachel and I are also donating our fees from this to helping humans to see. So we think that's a nice little touch. So we're doing this both to help the Fred Hollows Foundation, but more importantly to help something we care a lot about, which is making deep learning more accessible. It's currently terribly exclusive. As I'm sure you've noticed, resources for teaching it tend to be quite mathematically intensive. They really seem to be focused on a certain kind of ivory tower type audience. So we're trying to create training and examples which are for non-machine learning and math experts, dealing with small data sets, giving role models of applications you can develop quickly. Today we're going to see how to create a real useful piece of deep learning code in 7 lines of code. We want to get to the point where it's easy for domain experts to work with deep learning. There are a lot of domain experts here, whether you're working with getting satellites in the air or whether you're working with analyzing the results of chemical studies or whether you're analyzing fraud at a bank. I know all those people are here in this audience. You are domain experts that we want to enable to use deep learning. At this stage, the audience for this course is coders, because that's as far as we think we can get at this point. We don't need you to be a math expert, but we do need you to be coders. I know all of you have been told of that prerequisite. We do hope that with your help, we can get to the point where non-coders will also be able to participate in this. The reason why we care about this is that there are problems like improving agricultural yields in the developing world, or making medical diagnostics available to those who don't have them. These are things that can be solved with deep learning, but they're not going to be solved by people who are at these more ivory tower firms on the whole, because they're not really that familiar with these problems. The people who are familiar with these problems are the people who work with them every day. So for example, I've had quite a bit to do with a lot of these kinds of people, such as through the World Economic Forum, I know people who are trying to help cure TB and malaria, I know people who are trying to help with agricultural issues in the developing world, and so forth, and these are all people who want to be using deep learning for things like analyzing crop imagery from satellites, or my most recent startup which was analyzing radiological studies using deep learning to deal with things like the fact that in the entire continent of Africa, there are only 7 pediatric radiologists. So most countries in Africa, no kids have access to any radiologist. They therefore have no access to any kind of modern image-based medical diagnostics. So these are some of the reasons that we're creating and running this course. So we hope that the kind of feel of this community is going to be very different to the feel of a lot of the deep learning type communities that have appeared before, that have been all about, let's trim 0.1% of this academic benchmark. This is going to be all about, let's do shit that matters to people as quickly as possible. Sometimes to do that, we're going to have to push the state of the art of the research. And where that happens, we won't be afraid to show you the state of the art of the research. The idea is that by the end of part 1 of this, you will be able to use all of the current best practices in the most important deep learning applications. If you stick around for part 2, you'll be at the cutting edge of research in most of the most important research areas. So we're not dumbing this down by any means. We're just refocusing it. So the reason why we're excited about this is that we have now the three pieces of this universal learning machine. We now have the three critical pieces, like an infinitely flexible function, all-purpose parameter fitting which is fast and scalable. So I'll tell you what I mean about these three pieces. The neural network is the function. We're going to learn about exactly how neural networks work. But the important thing about a neural network is that they are universal approximation machines. There's a mathematical proof, the universal approximation theorem, that we're going to learn all about, which tells us that this kind of mathematical function is capable of handling any kind of problem we can throw at it, whether that mathematical function be how do I translate English into Hungarian, or whether that mathematical function is how do I recognize pictures of cats, or whether that mathematical function is how do I identify unhealthy crops. It can handle any of these things. So with that mathematical function, the second thing you need is some way to fit the parameters of that function to your particular need. And there's a very simple way to do that called gradient descent, and in particular something called backward propagation, which we will learn all about in this lesson and the next lesson. The important thing is though that these two pieces together allow us to start with a function that is in theory capable of doing anything and turn it into a function that is in practice capable of doing whatever you want to do, as long as you have data that shows examples of what you want to do. The third piece, which has been missing until very recently, is being able to do this in a way that actually works with the amount of data that you have in the time that you have available. This is all changed thanks particularly to GPUs. So GPUs are graphics processing units, also called video cards, that's kind of an older term now, also called graphics cards. And these are devices inside your computer which were originally designed for playing computer games. So it's kind of like when you're looking at this alien from the left-hand side and there's light coming from above, what pixel color do I need for each place? That's basically a whole bunch of linear algebra operations, a whole bunch of matrix products. It turns out that those are the same operations that we need for deep learning. And so because of the massive amount of money in the gaming industry that was thrown at this problem, we now have incredibly cheap, incredibly powerful cards for figuring out what aliens look like. And we can now use these, therefore, to figure out how to improve medical diagnostics in Africa. So it's a nice handy little side effect. GPUs are in all of your computers, but not all of your computers are suitable for deep learning. And the reason is that programming a GPU to do deep learning really requires a particular kind of GPU, and in practice at the moment it really requires a GPU from NVIDIA because NVIDIA GPUs support a kind of programming called CUDA, which we will be learning about. There are other GPUs that do support deep learning, but they're a bit of a pain, they're not very widely used. And so one of the things we're going to be doing is making sure that all of you guys have access to an NVIDIA GPU. The good news is that in the last month, I think, Amazon has made available good quality NVIDIA GPUs for everybody for the first time. They call them very excitingly their P2 instances. So I've spent quite a bit of the last month making sure that it's really easy to use these new P2 instances. I've given you all access to a script to do that. Unfortunately, we're still at the point where they don't trust people to use these correctly, so you have to ask permission to use these P2 instances. The Data Institute folks, for anybody who does not yet have an AWS P2 instance or their own GPU server, they are going to collect all of your AWS IDs and they have a contact at Amazon who will go through and get them all approved. They haven't made any promises, they've just said they'll do what they can. So they're aware of how urgent that is. So if you email your AWS ID to Mindy, she will get that organized. We'll come back and look at AWS in more detail very shortly. The other thing that I've done is on the Wiki, I have added some information about getting set up. There is actually quite an interesting option called OVH. I'm sure by the time this is a MOOC there's going to be a lot more, but this is the only company I've come across who will give you a buy-the-month server with decent deep learning graphics cards on it. It's only $200. To give you a sense of how crazily cheap that is, if you go to their page for GPU servers, you'll see that this GTX 970 is $195 a month and then their next cheapest is $2,000 a month. It just so happens this GTX 970 is ridiculously cheap for how good it is at deep learning. The reason is that deep learning uses single precision arithmetic, in other words it uses less accurate arithmetic. These higher-end cards are designed for fluid simulations and tracking nuclear bombs and stuff like that that require double precision arithmetic. So it turns out these GTX 970s are only good for two things, games and deep learning. So the fact that you can get one of these things which has got two GTX 970s in is a really good deal. So one of the things you might consider doing in your team is maybe sharing the cost of one of these. $200 a month is pretty good compared to worrying about starting and stopping your $0.90 per hour AWS instance, particularly if AWS takes a while to say yes. So how many people here have used AWS before? So AWS is Amazon Web Services, I'm sure most of you, if not all of you, have heard of it. It's basically Amazon making their entire backend infrastructure available to everybody else to use. So rather than calling it a server, you get something they call an instance. You can think of it as basically being the same thing, it's a little computer that you get to use. In fact, not necessarily little. Some of their instances cost $14 or $15 an hour and give you like 8 or 16 graphics cards and dozens of CPUs and hundreds of gigabytes of RAM. The cool thing about AWS is that you can do a lot of work on their free instance. You can get a free instance called t2.micro, and you can get all the things set up and working on a really small dataset. And then you can switch it across if you want to run it on a big dataset, switch it across to one of these expensive things and have it run and finished within an hour or two. So that's one of the things I really like about AWS. Microsoft also have something a lot like AWS called Azure. Unfortunately their GPU instances are not yet publicly available. I've reached out to Microsoft to see if we can get access to those as well, and I'll let you know if we hear back from them. So one of the things that Rachel's done today is to start jotting down some of the common problems that people have found with their AWS installs. Getting AWS set up is a bit of a pain, so we've created a script that basically will do everything for you. But the nice thing is that that script is very easy for you to have a look at and see what's going on. So over time you can get a sense of how AWS works. Behind the scenes, AWS is using their command line interface, or CLI, which we've given you instructions on how to install. As well as using the CLI, you can also go to console.aws.amazon.com and use this graphical interface. In general, I try to avoid using this graphical interface because everything takes so much longer and it's so hard to get things to work repeatedly. But it can be nice to look around and see how everything's put together. And again, we're going to come back and see a lot more about how to use the graphical interface here, as well as how to create and use scripts. So these are some of the pieces that we want to show you. I wanted to talk a bit more before we go into more detail about some of the interesting things that we've seen happening in deep learning recently. Perhaps the thing that I found most fascinating recently was when one of the leading folks at Google Brain presented this at a conference at Stanford which showed the use of deep learning at Google. And you can see this is just 2012 to today, or maybe 2 months ago. It's gone from nothing to over 2500 projects. Now the reason I find this interesting is because this is what's going to happen to every organization and every industry over the next few months and few years. So they've kind of described how at Google it's getting used pretty much everywhere. And you can imagine probably if they redid this now today, 2 months later, it's probably going to be somewhere up here. So we kind of felt like it would be great to help kickstart lots of other organizations to start going up this ramp. That's another kind of reason we're doing this. I really like looking at applications. And we started seeing some examples of these kind of deep learning amateurs applications, if you like. This is an example of it. What these guys did is they're not machine learning or deep learning experts. They downloaded a copy of Cafe. They ran a pre-existing model. This is what we're going to learn to do today. So run a pre-existing model and use the features from that model to do something interesting. In their case, the thing they decided to do that was interesting was to take data that they already had, because they're skin lesion people, and analyze skin lesions. These are the different kinds of skin lesions that you can have. They found, for example, that the previous best for finding this particular kind of skin lesion was 15.6% accuracy. When they did this off-the-shelf Cafe pre-existing model with a simple linear thing on top, they quadrupled it to 60%. Often when you take a deep learning model and use the very simple techniques we'll learn today, you can get extraordinarily upticks compared to non-deep learning approaches. Another example of that was looking at plant diseases. There's been at least two groups that have done this in the last few months. Very successful results from people who are not deep learning or machine learning experts. Similar results in radio modulation. So these folks who are electing computer engineering people found that they could double the effective coverage area of phone networks and stuff like that. This is a massive result. And again, they used very simple approaches. It's being used in fashion. It's being used to diagnose heart disease by hedge fund analysts. So there's a particular post which I found really inspiring actually in trying to put this together, which is that Keras, which is the main library we'll be using, the author of that put together this post showing how to build powerful models using very little data. I really just wanted to give a shout-out to this and say this work that Francois has been doing has been very important in a lot of the stuff that we're going to be learning over the next few classes. The basic environment we're going to be working in most of the time is the IPython notebook or the Jupyter notebook. So let me just kind of give you a sense of what's going on here. When you have a Jupyter notebook open, you will see something which... So this is a good time to show you about starting and stopping AWS instances. So I just tried to go to my notebook on AWS and it says, it can't be reached. So my guess is that if we go back to my console, you can see I have zero running instances. So I've got zero servers currently running. So if I click that, I will see all my servers. Normally I would have one P2 server or instance and one T2 because I use the free one for kind of getting everything set up and then use the paid one once everything's working. Because I've been fiddling around with things for this class, I just have the P2 at the moment. So having gone here, one way I could start this is by going start here. But like I said, I don't much like using this GUI for stuff because it's just so much easier to do things through the command line. So one of the things that I showed you guys that you could download today is a bunch of aliases for making starting and stopping AWS really quick. If you haven't got them yet, you can find links to them on Slack or you can just go to platform.ai slash files and there's a bunch of different things here. This aws-alias.sh is a file that sets up these various aliases. So the easiest way to grab stuff on your AWS instance or server is to use something called wget. So I would right-click on this and choose copy link address and then go wget and paste in that. That will go ahead and download that file. We can take a look at that file and you'll see it's basically a bunch of lines that say alias something equals something else. And it's created aws-get-p2, aws-get-t2, aws-start, aws-ssh, aws-stop. I'm going to show you what these things do because I find them pretty convenient. Basically if I run aws-get-p2, first of all I'll say source aws-alias.sh and that just runs that file. In Bash, that's how you just run a file. And that's now caused all of those names to appear as aliases to my system. So if I now run alias-get-p2, that's going to go ahead and ask Amazon for the ID of my P2 instance, and not only does it print it, but it's going to save it into a variable called instanceID and all of my other scripts will use $instanceID. So I now want to start that instance, so I just type aws-start and that's going to go ahead and do this equivalent thing of going to the GUI, right-clicking, choosing start. The other nice thing it does is it waits until the instance is running. And then at the end, it asks the queries for the IP address and prints it out. Now the script that I have given you guys to set up these instances uses something called an elastic IP that actually keeps the same IP address every time you run it. So you should find that IP address stays the same, which makes it easier. So there is the IP. So I then have something called aws-ssh. And aws-ssh will go ahead and ssh into that instance. So all it does is basically use the username ubuntu, because that's the default username for this kind of image on AWS, at and then $instanceIP. So that's that IP address we just got. The other thing it does is to use the private key that was created when this was originally set up. Now in my case, I've actually moved that private key to be my default key, so I don't actually need that minus IP. So I just type ssh ubuntu $instanceIP, but you can just type aws-ssh and you'll see bang, here we are. So we are now inside that AWS image. One of the handy things about AWS is they have these things called AMIs, Amazon Machine Images. An AMI is basically a snapshot of a computer at a particular point in time. And you can start your own instance using a copy of that snapshot. So in the script I've given you guys, I've created and provided an AMI which has all the stuff we want installed. So that's why it is that when you use that script and log into it, you can start running things straight away. So let's do that right now. I've created a directory already for you called nbs for notebooks. So we can go ahead and type jupyter-notebook. And this is how we ask Amazon to set up a Jupyter Notebook server for us. And when it's done, it says, OK, the Jupyter Notebook is running at all IP addresses on your system, 8, 8, 8, 8. So what is our IP address? Well, it told us up here when we started it, it's 52 blah blah blah blah. So I'm going to go to my instance and I'll go 52.40.116.111 in my case, colon, and it told me that the port is 8, 8, 8, 8, so colon 8, 8, 8. So I'm just typing in that here, press Enter. I've set up a password, it's just dl-underscore-course. We can look later on at how to change that password if people want to, but I just thought it would be handy to have a password there for everybody if you want to start looking at some of your own data. And actually by default it's not going to show you anything, so I'm going to delete that. So now we can just go ahead and say new Notebook, and just say python-conderoot. And this sets up a scientific computing environment for you where you can type python commands and get back responses. So 1 plus 1. There we go, so it seems to be computing things correctly. So the basic idea here is that over there on Amazon, you have your server, it is running a program called Jupyter Notebook. Jupyter Notebook is causing a particular port, which is 8, 8, 8, 8, to be opened on that server, where if you access it, it then gives you access to this Jupyter Notebook environment. In your team, you guys can all use the same Jupyter Notebook if you want to, or you could run multiple Jupyter Notebooks on one machine. It's really pretty flexible. So now that I've created one, I could rename this, say this is Jeremy's Notebook. And so then Rachel might come along and be like, oh I want to run something as well. So she goes new and her computer and it creates a whole new one over here, and she could say file, rename, Rachel's Notebook. So if I now go back here, you can see both of these notebooks are shown to be running. So the server is running multiple kernels, they're called. And you can see back here, it's saying creating new notebook, kernel started. So each of those are totally separate. So from one of them, I say name equals Rachel, and in the other one, I say name equals Jeremy. And then over here, you'll see that they are not in any way talking to each other, they're totally separate. So that's a super handy way to do work. The other nice thing is that you can not just type code, but you can also type markdown. So I could go section, I want to talk about something here. And so as I do that, it allows me to mix and match information and code. And every piece of code that comes out, I can see where it came from. And also as you'll see, let's just put in visualizations and plots and so forth. So some of you may have come across this important concept called literate programming. Literate programming is the idea that as you code, you are documenting what you're doing in a very deep way, not just for others, but maybe more importantly for yourself. And so when you're doing data science work, work like a scientist. How many people here are in some form scientists or have been scientists? You guys will know the importance of your journal notebook. The greatest scientists, they're all stories about the kinds of notebooks they kept and how their lab notebooks worked or their lab journals worked. This is critical for data scientists too. This idea that as you do experiments, you're keeping track of what did I do, what worked, what didn't work. I can see all the people who put their hand up as scientists are all nodding right now. So this makes it super easy to do that. So be helpful to yourself and to your team by taking advantage of this. Now in order to learn to use this environment, all you have to do is press H. And when you press H, it brings up all of these keyboard shortcuts. After not very long, you will get to know all of them because they're all extremely useful. But the main ones I find particularly helpful is you hit M to turn into markdown mode, so that's the mode where you can enter text rather than code, or Y to switch it back to code again. And you certainly need to know Shift-Enter, which evaluates the cell and gives you a new cell to enter into. And you also need Escape, which pops you out of entering information and gets you back into this command mode. And then Enter to go back into edit mode again. So just to show you that, Enter to get into edit mode, Escape to get out of edit mode, Shift-Enter to evaluate. And you can see as I move around, it changes which one is highlighted. I've started to create some resources on the Wiki for helping you with Jupyter Notebook. It's still pretty early, but you guys I'm sure can help by adding more information here. One of the things I particularly mentioned is that there are some good tutorials. I thought I'd also mention my favorite book, which I now can't see here. Python for Data Analysis by Wes McKinney. It's a little old, it also covers pandas a lot, which you don't need, but it's a good book for getting familiar with this basic Python scientific programming stack. So the last ingredient that I want to introduce is Kaggle. How many people here have been to or done anything with Kaggle at any point? Anybody who is in the master's program here, I'm sure will have used Kaggle or will shortly be able to use Kaggle. Mainly because it's just a great place to get all kinds of interesting data sets. So for example, if you wanted to test your ability to do automated drug discovery, you could go to Kaggle and download the files for the Merck Molecular Activity Challenge, run some models and test them to see how they compare to the state-of-the-art by comparing to the leaderboard. So Kaggle is a place where various organizations run machine learning competitions. They generally run for about 3 months. It's super cool because they get archived essentially forever. You can download the data for them later on and find out how you would have gone in that competition. Generally speaking, if you're in the top 50%, that means you have an okay-ish model that is somewhat worthwhile. If you're in the top 20%, that means you have a very good model. If you're in the top 10%, that means you're at an expert level for this type of problem. If you're in the top 10%, it literally means you're one of the best in the world. Every time I've seen a Kaggle competition, I used to be president of Kaggle, so I'm very familiar with this. Every time I've seen a Kaggle competition, at least the top 10 generally all beat the previous best in the world, and generally are from really good machine learning experts who are going beyond anything that's been done before. It seems that the power of competition pushes people way beyond what the previous academic state of the art was. So Kaggle is a great environment to find interesting datasets and to benchmark your own approaches. So we're going to be using it for both of these purposes. Our first challenge will be dogs vs. cats. So sometimes on Kaggle they run competitions that are not done for lots of money, but sometimes they're done for free or for a bit of fun. In this case it was actually done for a particular purpose, which was, can you create an algorithm that can recognize the difference between dog photos and cat photos? The reason why was because this particular organization was using that problem as a capture, in other words to tell the difference between humans and computers. It turned out that the state of the art machine classifiers could score 80% accuracy on this task. So really this group wanted to know, can you surpass the state of the art, is this a useful capture, and then if you can surpass the state of the art, can they then use this dogs vs. cats recognizer for their pet-finding work. So really the goal here was to beat 80%. This is a great example of the kind of thing which you could use for a thousand or a million different purposes. For example, the work I did in cancer detection is this. So if you take a CT or an x-ray or an MRI and you say to a deep learning algorithm, these people have malignant cancer, these people don't, it's the same as cats vs. dogs. This is a healthy high crop yield area from satellite photos, this isn't, that's cat vs. dogs. If you say this is one kind of skin lesion and this is another kind of skin lesion. If you say this is an abstract art painting and this is not. This is an extremely valuable painting and this is not. This is a well taken photo and this is not. They're all image analysis problems that are generally classification problems. These are all examples of things people have done with this kind of technology. So cats vs. dogs, it turns out, is a very powerful format. And so if we can learn to solve this well, we can solve all of these kinds of classification problems. Not just binary, so not just this group or that group, but also things like that skin lesion example. These are 10 different types of skin lesions, which type is it? Or the crop disease example, which of these 13 crop diseases are we looking at here? An example of an actual thing that I saw was cucumber analysis. So a Japanese cucumber farmer used this approach to deep learning to do the automatic, automated all their logistics and basically had a system that would put the different grades of cucumbers into different bins automatically and make their cucumber workflow much more efficient. So if that was your idea for a startup, it's already been done. So they are all of our basic pieces. So to get started, here we are with this AWS server with a pretty empty-looking set of notebooks here. So we want to go ahead and start getting some work done. So to do that we need to download the basic files that we need. So I've sent you all of this information already. All of the information you need is on our platform.ai website. All of the notebooks are in files slash nv's. So what I'm going to do is press Control-C twice, that shuts down the notebook. So the notebook is not running. Don't worry, it saves itself automatically on a regular basis, or you can just hit S to save it right now. So shutting down the notebook, as you'll see, the Python notebook files are still sitting there. You can see actually that behind the scenes, they're just big bunches of JSON text, so you can stick them in GitHub and they'll all work perfectly well. What I generally like to do is run something called Tmux. How many people here have used Tmux or Screen before? Less than I expected. Okay, so those of you who haven't, you're going to love this trick. Tmux and Screen are programs that let you run programs on your server, close your terminal, come back later and your program will still be running in the exact same way. I don't remember if Tmux is already installed. It is. So to use it, you just go Tmux. And it looks like nothing happened except a little green bar has appeared at the bottom. But if I now hit Tmux's magic command, which is Ctrl-B, and press question mark, you can see there are lots of keystrokes that it has ready for me to use. And so one of the ones I like is Ctrl-B double quote, which creates a second window underneath this one, or Ctrl-B percent, which shows a second window next to this one. So I tend to set up a little Tmux session and get it all set up the way I want. So I'm not going to go into detail about how to do everything I show you. What I really want to do in the class most of the time is to say, here's something that exists, here's something I recommend using, here's what it's called, and during the week you can play with it. You can ask questions, you can use it in your team, and so forth. So here it is, it's called Tmux. This is what it does, and I'll show you something cool. If I now go Ctrl-B and then D to detach, close out of this altogether, it's all gone. So if I now go back into my server, I wasn't able to SSH in properly because currently $instanceip is not defined. And the reason for that is that I have to, rather than every time I start sourcing my AWS-alias.sh file, what I should do is I should go vim.bashrc.bashrc is a file that is run every time you run bash. And if I edit my bashrc file and at the end I type source AWS-alias.sh, and just to show you, I'm going to close it and reopen it. If you're wondering why my computer is going so slowly, it's because when you have these big Skype calls running at the same time as screen recording, everything slows down. So you can see now all those aliases are there. So before I SSH to $instanceid, I have to find out my correct IP address. So I can say aws get p2, get my instance ID, and then we can, you know what, I'm not sure I've got something here to actually just get the IP address. So that's interesting. As you can see, I'm kind of playing with this a little bit as I go, so I'm going to go ahead and show you how to do this. So right now the IP address only gets printed out when I start an instance. In this case, I've already got an instance running. So I'm going to edit this script and I'll change it later on. Basically I'm going to create a new alias called aws.ip. I'm going to get rid of the bit that starts the instances, get rid of the bit that waits for it to be running, and I'm just going to keep the bit that says instance.ip equals something something something source aws alias.sh. And I've got a new alias called aws.ip. And now I can go SSH u1, 2 at $instance.ip. Having said all that, because my IP address is going to be the same every time and I couldn't really be bothered waiting for all that, I'm actually going to manually put my IP address in here. So the next time I run this, I can just press up arrow and rerun that command. So I'm kind of showing you lots of ways of doing things so you can decide what your own workflow is like or come up with better ones. But here's the cool thing. I am back in my box here, and then if I say tmux attach, I am exactly back to where I came from. So whatever I had running and whatever state it was, it's still sitting there. So the particularly cool thing is that any notebooks, kernels I had running, they're all still sitting there. This is particularly helpful if you run something like those OVH servers or one of your own servers. With AWS, it's a little less helpful because you really need to shut it down to avoid paying the money. But if you've got something you can keep running, all the MSAM students will have access to the GPU server we have here at the university, which is particularly helpful for you guys. So I actually tend to use this little bottom right-hand window to permanently have Jupyter Notebook running. So that's kind of like my particular way of running. And then I tend to use this left-hand window to do other things. And in particular, I'm going to go ahead and grab my notebooks. So the easiest way to grab things is with Wget, so if I go Wget, I now have a notebook, Lesson 1. So if I go back to my Jupyter Notebook, see it's appeared, Lesson 1. If I click on it, if you're using a T2 instance, the free ones, generally speaking, particularly the first time you run something, it can take quite a long time to open. You should find the second time it's quite fast, by the way. So here is our notebook. So hopefully quite a few of you have already got to the point today that you can see this. Those of you that haven't will get plenty of help during the week. This particular notebook uses two external scripts to help. Those scripts are called utils and vgg16. So the last thing I'm going to do before I break is to grab those. So I'm going to go Wget and just pop these all in this notebooks directory, so they're all in the same place. Then unzip them. And then the only other thing you need is the data. So the data sits in the platform.ai data directory. And the data is all the dogs and cats. Now I've taken the Kaggle data and made changes to it, which I'm going to be showing you. So rather than downloading it from Kaggle, I suggest you grab it from platform.ai and I've sent you this information today as well. So I'm going to cd into data and Wget that as well. And so that's going to run for a few minutes. So while it does, I think it's a good time to have a break. Let's have a 10-minute break during which you are welcome to keep going with your install, chat to myself or Rachel or Tara or Yad, if you've got any questions, or get to know your team or just go and have a coffee. So let's get back together at 5 past 8. The previous section, I think, for some of you, and I was chatting to a couple of you during the break, was a bit of a fire hose of information because it was like, here's bash, here's AWS, here's Kaggle, here's GPUs, blah blah blah. For some of you, it was probably really boring. Most practicing data scientists probably are using all of those things already. So if you're at one extreme of the, holy shit, that was a fire hose of information, don't worry, we have all week to get through it. You'll have the video tomorrow. And by the time you're here again next week, I want to make sure that everybody who has the time and interest to work hard on it has got through all of the material. If you haven't, like you're a couple of days, maybe it's early on the weekend and you're thinking, I'm not going to get there, please let Rachel and I know that we will work with you in person to get you there. Everybody who puts the time in, I am determined to make sure can get through the material. If you don't really have the background and you don't really have the time, that's fine. Maybe you won't get through all the material. But I really am determined that everybody who's prepared and able to put in the time can get through everything. So between the community resources and the video and Rachel and I and the folks at Taro and Yad, we will help everybody. To those of you who are kind of practicing data scientists and you are familiar with all of these pieces, I apologize that it will be a bit slow for you and hopefully as we move along there will be more and more new stuff. I'm kind of hoping that for those of you who already have some level of expertise, we will continually give you ways that you can go further. So for example, at the moment I'm thinking, can you help us with these scripts to make them better, to make them simpler, to make them more powerful, to create Azure versions of them. All the stuff that we're doing to try and make deep learning as accessible as possible, can you help contribute to that? Can you help contribute to the Wiki? So for those of you who already have quite a high level of expertise, I'm really looking to make sure that there's always ways that you can push yourself. So if you're ever feeling a bit bored, let me know. It's a perfectly reasonable thing to say, hey, I'm kind of a bit bored and I'll try and give you something to do that you don't know how to do. So at this point, I downloaded dogscats.zip and I unzipped it. If you're wondering about the minus q, that's just because unzipped otherwise prints out every single file name as it goes, so that's q for quiet. So just about the most important thing for doing this kind of image classification is how the data directories are structured. In particular, you'll notice that we have a training set and a test set. That's because when we downloaded the data originally from Kaggle, it had a train.zip and a test.zip. Keras, which is the library we're going to use, expects that each class of object that you're going to recognize is in a different directory. So the one main thing I did after I downloaded it from Kaggle is that I created two directories, one called cats and one called dogs. I put all the cats in the cats and all the dogs in the dogs. When I downloaded them from Kaggle, they were all in one directory and they were called like cat.1.jpg or dog.1.jpg. So now if I ls train dog.1 star, for example, or dog.10 star. So there are 11,500 dogs in there. So that's the number of dogs and cats that we have in our training set. And so for those of you who haven't done much data science before, there's this really key concept that you have a training set and a test set. And Kaggle being a competition makes this really obvious. The files in the training set tell you what they are. Here is a dog. It's called dog.something. But if I look in the test set, they don't say anything, they're just numbers. Why is that? That's because your job in this Kaggle competition is to say, for example, for 43.jpg, is it a dog or is it a cat? So there are 12,500 images in the test directory for you to classify. Even if you're not doing a Kaggle competition, you should always do this yourself. In fact, ideally you would get one of your colleagues to do it without you being involved. To split the data into a test set and a training set, and to not let you look at the test set until you promised you're finished. Kaggle kind of enforces this. They let you submit to the leaderboard and find out how you're going. But actually the final score is given based on a totally separate set of data that is not scored. For me, before I started entering Kaggle competitions, I kind of thought that my data science process was reasonably rigorous. But once I actually started doing competitions, I realized that that level of enforcing the test training split made me a much better data scientist. You can't cheat. So I do suggest you do this in your own projects as well. Now because we also want to tune our algorithm in terms of different architectures, different parameters and so forth, which we'll talk about, it's also a very good idea to split your training set further into a training set and a validation set. We'll see a lot more about how this works. But you'll see in this case, I've created another directory called valid which has dogs and cats as well. It's structured exactly the same. And here you can see that there are 1000 cats and 1000 dogs. So when I originally downloaded from Kaggle, there were 12,500 cats and dogs in the training set. That's why in my training set there are 11,500 because I've moved 1000 of each of them to the validation set. So that's the basic data structure we have. Other than splitting things into test training and validation sets, that's the most important advice I have as a data scientist. The second most important advice I have as a data scientist is to always do nearly all of your work on a sample. A sample is a very small amount of data that you can run so quickly that everything you try you get a nearly immediate answer to. This allows you to very quickly try things, change things and get a basic process running. So I always create a sample with 100 or so items to get started with. So you'll see I have a directory called sample, and in that I have a whole separate train and valid. I did not move things there, I copied them there. The purpose of this sample directory is just to let me do things really quickly. So you'll see inside sample train, we have cats and dogs, but this time there are 8. I probably should have put more in there, I think probably more like 100 would have been good, but I think at the time I was probably using a really low-powered computer for my testing. But it's enough to check that my script is working. So now that everything is downloaded, you can see that I have in my Jupyter Notebook is automatically noticed that it's changed. I'll get rid of these zip files, and I'll get rid of the Rachel's and Jeremy's Notebooks that I was playing with, and we're ready to get started with doing some deep learning. So the goal for you guys during this week will be to replicate everything that I've done, initially just by making sure that this notebook works for you, but then to replicate it with another dataset. And so one of the things we'll do tomorrow is we'll post some ideas of other interesting Kaggle datasets you could try, and maybe other people can also post other interesting datasets they found elsewhere. So the idea will be to make sure that during the week you can run your own classification process on some dataset other than dogs and cats. But first of all, make sure that you can run this. So as you can see in this notebook, I've used Markdown cells. How many people have used Markdown before? Okay, so those of you who don't know, Markdown is what we use both in the notebook as well as on the wiki. It's basically a way of really quickly creating formatted text. There's not enough of you that aren't familiar with it that I'm going to go into it in detail. If you're not familiar with it, please Google Markdown. And you can experiment with it either on the wiki or in your notebook. As you can see though, I've basically created cells with headings and some text, and during the week you can read through these in detail. As we mentioned, we're going to try to enter the dogs and cats competition. So 25,000 labeled dog and cat photos, 12,500 in the test set, and the goal is to beat 80%. As we go along, we are going to be learning about quite a few libraries. Not too many, but enough for those of you that haven't used Python for data science before. It's going to seem like quite a few by the end of the 7 weeks. Hopefully you'll be pretty familiar with all of them. One of the really important three is Matplotlib. That's because Matplotlib does all of our plotting and visualization. On the wiki, we have a section called Python Libraries. As you can see, we have our top three listed up here. At the moment, there are just links to where they come from. I'm hoping that you guys will help us to turn this into a really rich source of information about places that you found lots of helpful stuff, answers to frequently asked questions, and so forth. But for now, if you're not familiar with one of these things, type the word, followed by tutorial into Google and you'll find lots of resources. All of these things are widely used. Keras a little bit less so because it's just a deep learning library and therefore relatively new. NumPy and Matplotlib and all these other ones, Cyclet, Learn, SciPy, there's lots of books about them, there's lots of tutorials about them, there's lots of videos about them. So Matplotlib creates plots. One of the things we need to do is to tell Jupyter Notebook what to do with those plots. Should it pop open a new window for them, should it save them? So this sent Matplotlib inline says, please show our plots in the actual Jupyter Notebook. That's pretty much the first line in every Jupyter Notebook that I create. And here's the thing I told you about, which is sometimes I want to run stuff on a sample and sometimes I want to run it on everything. And so I make it really easy for myself by having a single thing called path, which I can switch between the sample and the everything. So for now, let's just do things on the sample. And so you should do all of your work on the sample until everything's working. So as you can see, each time I've done something, I press Shift-Enter and it puts a little number after the in, showing that this is the second input cell that I've run. Like every programming language, a large amount of the power of Python comes from the libraries that you use. And to use a library in Python, you have to do two things. You have to install it and then you have to import it. In Python, I strongly recommend that you use a particular Python distribution called Anaconda. And if you're using the scripts and AMIs we provided, you are already using Anaconda. You can check what Python you're using by typing which Python, and it will tell you. You'll see that not only am I using Anaconda, but I'm using an Anaconda that is installed into my home directory. So no screwing around with sudo or any of that business. And again, if you use our AMI in scripts, this has all been done for you. With Anaconda, installing anything is as simple as typing conda, install and the name of the thing. And on Anaconda, everything has been pre-compiled, so you don't have to wait for it to compile, you don't have to worry about dependencies, you don't have to worry about anything, it just works. That is why we very highly recommend using Anaconda. It works on Mac, it works on Windows, and it works on Linux. Lots of Windows users use it, very few Linux users use it, very few Mac users use it. I think that's a mistake because lots of Mac and Linux users also have trouble with compiling dependencies and all that stuff. I suggest that everybody uses it. From time to time, you will come across something that does not have a conda installer available, in which case you have to use pip instead. In our case, I think just Theano and Keras are in that situation, but neither of those need compiling anything at all. So they're very, very easy to install. So once you've installed it by typing conda install whatever, and most things are already installed for you with our AMIs, you then have to tell Python that I want to use it in this particular session, which you do just by typing import and the thing you want to look at. So I'm not going to go through all these libraries right now, I'll go through them as we use them. One of the big three is here, which is NumPy. NumPy is the thing which, as our Wiki page describes, provides all of our basic linear algebra. How many people here have some familiarity at all with linear algebra? Nearly all of you. So if you're somebody who didn't put up your hand, I would suggest looking at the resources that Taro added to the Wiki. So go back to the home page and go to Linear Algebra for Deep Learning. Generally speaking, for any math stuff, my suggestion is to go to the Khan Academy site. Khan Academy has really great videos for introducing these kind of simple topics. We just need to know these three things, mainly the first two things for this course. And NumPy is the thing which gives you these linear algebra operations in Python. As you'll see, it makes them extremely easy to use. Pretty much everybody renames NumPy to NP. So that's what import NumPy as NP does. You'll find it in nearly everybody's script on the Internet. It'll be NP.something, not NumPy.something. In general, we try to stick with the same kind of approaches that everybody else uses so that nothing will be too unfamiliar. So we've imported some libraries that we need. We also try to provide some additional utilities and scripts for things that we think probably ought to exist but don't exist that make things easier. There's very few of them. Nearly all of it is in one script called utils. There's a cool little trick, which is that if you are using external script that you've created and you're changing it quite a bit, so now that you've got utils, feel free to add and change and do whatever you like to it. If you import it like this, import utils, re-lude utils, and then from utils import whatever you need, you can go back and re-run that cell later after you've changed utils.py and all of your changes will be there available for you to use. So for now we're just going to use one thing from our utils library called plots, which we'll see used in a moment. So our first step will be to use a pre-trained model. So what do we mean by a pre-trained model? What we mean is that somebody has already come along, downloaded millions of images off the internet and built a deep learning model that has learned to recognize the contents of those images. Nearly always when people create these pre-trained models, they use a particular dataset called ImageNet. The reason that they tend to use ImageNet, or one of the key reasons that they tend to use ImageNet, is because ImageNet has the most respected annual computer vision competition. So people that win the ImageNet challenge tend to be companies like Google and Microsoft. A couple of years ago, it tended to be people who immediately got hired by Google and Microsoft. ImageNet itself is fun to explore if you go to ImageNet and go to explore. You can check it out. And basically there are 32,000 categories. So for example we can go ImageNet, let's look at plants and plant life, let's go to crops, let's go to field crops, corn, type crops, dent corn. So here we have a number of pictures of dent corn. There are 397 of them. So the folks that create these pre-trained networks basically download a large subset of ImageNet. The competition has a thousand of these 32,000 categories that people compete on. So nearly always people just build models for these 1000. I would be remiss if I did not mention the shortcomings of the ImageNet dataset. Can anybody tell me something that they notice in common about what these photos look like or how they're structured? They're just one thing. If you look at an arbitrary photo, say from my photo album, you'll see there's a person here and a bridge there and something else here. ImageNet is carefully curated to be pictures of flint corn. These are like 312 pictures that are designed to be really good pictures of flint corn. This is an easier problem than many problems that you will be facing. So for example, I was talking to Robin from Planet Labs that broke about the work that they're doing with satellite imagery. Their satellite imagery is going to have a lot more than a picture of a piece of corn. So how many pixels is a Planet Labs photo? So pretty big, and in that couple million pixels, you're going to have what kind of area with that cover? 500 square kilometers. So there's going to be tennis courts, swimming pools, people sunbathing, all kinds of stuff. So when Robin takes this stuff to Planet Labs, he's not just going to be able to use a pre-trained network directly. But we're going to show you how you can use some of the structure of the pre-trained network even if you are not looking at photos that are this clear. Having said that, if you remember the slide I showed you earlier of the plant disease project, each of those plant disease pictures were very clearly just pictures of one thing. This certainly happens as well. But do be aware that when you're using a pre-trained network, you are inheriting the shortcomings and biases of the data it was trained from, and therefore you should always look at the data it was trained from. So being aware of that, I would say that for us, this is going to be quite a suitable kind of dataset. And as we look at the dataset, you'll see why I say that. So each year, most of the winners of the ImageNet competition make their source code and their weights available. So when I say their source code and their weights, the source code is the thing that defines, do you remember I told you there are 3 bits that give us modern deep learning? It's the infinitely flexible function, it's the way to train the parameters, it's the fast and scalable. The particular functional form is what is the neural network architecture. We're going to be learning a lot about that. But that's the source code. So generally you download the source code from the folks that built the model. The second is the parameters that were learned. So generally an ImageNet winner has trained a model for days or weeks, nowadays often on many GPUs, to find the particular set of parameters, the particular weights that make it really good at recognizing those ImageNet pictures. So you generally have to get the code and the weights. And once you have those 2 things, you can replicate that particular ImageNet winner's results. Another winner of 2014 was the Visual Geometry Group, it's an Oxford University group. And so there's this model called VGG. You'll hear about it lots. Generally speaking, every year's ImageNet winners, the particular models they used are so well used in the community that people call them by name. So the 2012 winner was AlexNet, 2014 was VGG, 2015 was Conception, 2016 was ResNet, so they all have names. VGG is a couple of years old, so it's not quite the best today. But it's special because it's the last of the really powerful simple architectures. We will get to the more complex architectures. Depending on how we go, it might be in this set of classes, if not it will be in next year's set of classes. VGG's simpler approach is not much less accurate, and for teaching purposes we're going to be looking at something that is pretty state of the art and is really easy for us to understand. So that's one of the reasons we're using VGG. Another reason we're using VGG is it's excellent for the kinds of problems that we were just talking about that Robin with his satellite imagery has, which is it's a great network for changing so that it works for your problem, even if your problem is a little different. So there's a number of reasons that VGG is a really great thing for us to be using. My strong preference is to start out by showing you how to do things that you can use tomorrow, rather than starting with the 1 plus 1 and showing you how you can do things that are useful in 6 years' time after you've got your PhD. So I'm going to start out by showing you 7 lines of code that do everything you need. To get to the punch line, the state of the art for dogs vs. cats in the academic literature is 80% accuracy. This gives you 97% accuracy. You don't need to do anything else. For you after this class to see if you can get everything working, basically your job will be can you run these 7 lines of code. If you can, then you can rerun it on your own dataset as long as you structure the directories the way that I just showed you. So what I'm going to do is I'm going to go through these 7 lines of code, or something very similar to them, line by line and show you some pictures of what we're doing along the way. I wanted to start by showing you the 7 lines of code though because we're going to be looking at all kinds of things along the way in order to really understand what's going on. And at some point you might start thinking, gosh, there's a lot to do to do deep learning. But there's not. There's a lot to do to really explain and talk about and visualize and think about deep learning. So to actually do image classification, you just need these 7 lines of code. So what does it mean to train a model that's already trained? Yes, you're getting a little bit ahead of us, but it's great to answer these questions many times. So let's try and make a starting answer at it. In this case, the VGG model has been trained to recognize photos of the 1000 types that are in the ImageNet competition. There's a number of reasons why that does not give us dogs vs. cats. Reason number one is that if we go into the animals section of ImageNet, there we are, animals, beings, beasts and brutes, domesticated I guess, dogs, hunting dogs, sporting dogs, pointers, this one. So they have 2332 pictures of Vizsla, also known as a Hungarian pointer. So you could go through and run it and go back and find all the Vizslas and the German short somethings and the sadist and say, oh they're all dog. But that's something you would have to do. So that's one kind of shortcoming of the VGG approach compared to what we actually want. The second shortcoming is that sometimes it's going to get it wrong, and it might get it wrong for really good reasons. For example, maybe this one comes back with snow. But in fact it's going to come back not just with snow, but it's going to come back with a probability for every one of the 1000 categories. So it'll be probability of.0003 that it's a mushroom, and.0002 that it's an airport, and.0004 that it's snow, and.0003 that it's a German shepherd. And so we want to take advantage of all of that information as well. So what this actually does is it does something called fine-tuning, which we're about to learn a lot about. And what fine-tuning does is it takes that pre-trained image model and says, use everything you know about the 1000 categories to figure out which one are cats and which one are dogs. That's a great question, and we're going to go back and talk about that a second time when we get there. So this code is going to work for any image recognition task with any number of categories, regardless of whether those categories are in ImageNet or not. And really the only image recognition that they're not going to do is something where you want to recognize lots of objects. This is specifically for recognizing A+. So let's see how it works. So the VGG object, let's run this, import VGG16. When something's running, it has a little star. You will probably get this warning that cuDNN is more recent than the one Theano officially supports. So this is a good time to talk about some of the layers that we have going on. In this example, we're using our VGG16+. It is sitting on top of, as we will see this in detail, Keras, which is the main deep learning library we're using and we'll talk a lot about. Keras is sitting on top of Theano, which we'll be talking about quite a bit, but less than Keras. Theano is the thing that takes Python code and turns it into compiled GPU code. Theano is sitting on top of a number of things, broadly speaking, NVIDIA's CUDA programming environment. And part of CUDA is the CUDA Deep Neural Network Library, cuDNN. For most important things in deep learning, Theano is simply calling a function inside cuDNN. So one of the things that we've set up for you in the AMIs and the scripts is to get all of this stuff stuck together and working. So Keras is all written in pure Python. And what it does is it takes your deep learning architectures and code and turns it into, in our case, Theano code. It can also turn it into TensorFlow code. TensorFlow and Theano are very similar. They're both libraries that sit on top of CUDA and provide really a kind of Python to GPU mapping and lots of libraries on top of that. TensorFlow comes out of Google and it is particularly good for stuff that Google really cares about, and in particular running things on lots and lots of GPUs. One of the things you'll hear a lot is you can't do anything with deep learning unless you have shitloads of data and shitloads of GPUs. That is totally, totally wrong, as you'll see throughout this course. It is true if you want to win ImageNet next year, you will need lots and lots of GPUs because you'll be competing for that last 0.1% against Google, against Microsoft, against Baidu. But if you're trying to recognize 10 different skin lesions, like the folks I just showed you were, they were the first people to try to do that with deep learning and they quadrupled the previous state of the art using one GPU and a very small amount of data that they had hand collected. So the reason you see a lot of this stuff about lots of GPUs and lots of data is it's part of the trying to make neural networks cool rather than uncool. It's trying to make it exclusive rather than inclusive. It's like unless you're us, you're not in the club. And I really don't want you to go in for that kind of thing. You will find again and again it's not true. As I've just shown you in fact with 7 lines of code, you can turn the state of the art from a 20% error rate to a 3% error rate and it takes about 5 minutes to run on a single GPU which costs 90 cents an hour. So I am not going to be talking much about TensorFlow in this course because it's still very early, it's still very new, it does some cool things, but not the kind of cool things that uncool people need access to. Theano, on the other hand, has been around quite a lot longer. It's much easier to use. It does not do multi-GPUs well, but it does everything else well. If you build something in Keras and you get to a point where you're like, okay, this is working great, we've got a 400% improvement in the state of the art, I want the extra 5% that comes from running this on 8 GPUs. It's a simple configuration change to change the backend to TensorFlow. Specifically I want to show you that configuration change. You'll find in your home directory, so for those of you who haven't used bash before, when you see tilde, that just means your home directory. In your home directory there is a.Keras folder and in there is a Keras.json file. This is the configuration. You'll see here backend Theano. If you change this to say TensorFlow and rerun it, it's now using TensorFlow. And TensorFlow will use all of your GPUs. If you do that, I also suggest changing this thing here that says th, which stands for Theano, to tf, which stands for TensorFlow. We may talk about that in the next course. It's a pretty minor detail. The other configuration file to be aware of is Theano.rc. You'll find a lot of Unix-y things are something.rc, is how they name their configuration files. Here is.something.rc. I want to point out that there's a really important line here, which is device equals. This is either GPU or CPU. If you're using a T2 instance, you'll find that the AMI we've created has changed the G to a C. That's because the T2 instance does not support GPU. So if you want to switch from GPU to CPU, just change the G to a C or the C to a G. So those are the two configuration pieces that you may need to know about. For this class, you won't really need to know about those because everything's been set up for you. I like to show you what's going on behind the scenes. So this warning that cuDNN is too recent, if you see any problems, try updating Theano or downgrading cuDNN. I haven't found any problems, so you can ignore that warning. It just means that we're using a more up-to-date version of cuDNN than Theano authors have tested. So we create our VGG object. In doing so, there's a whole bunch of stuff going on behind the scenes. We're going to look at all of it. So by the end of the next lesson, you'll understand every line of code in our VGG script. For now, I'll just point out that you can look at it because you downloaded it, and inside it you'll see there are 100 lines of code. So it's not very big at all. So we're going to understand all of it by the end of the next class. So now, let's treat it as a black box. There's a pre-trained network, it's called VGG16. We now have a VGG object which gives us access to that pre-trained network. With deep learning, we don't look at images or data items one at a time. We also don't look at them a whole dataset at a time. We look at them a few at a time. The number that we look at, or that little few that we look at at a time, we call either a batch or a mini-batch. A mini-batch is simply grabbing, in this case, images, a few images at a time. And the size of that is the size of the mini-batch, or the size of the batch, and computing on all of those at once. Why don't we do one at a time? The reason that we don't do one at a time is because a GPU needs to do lots of things at once to be useful. It loves running on thousands and thousands of things at the same time, because it can do all of them at the same time. So a single image is not enough to keep your GPU busy and it's slow. Why not do all of it, the whole dataset at once? First of all, your GPU only has a certain amount of memory, generally somewhere between about 2GB and about 12GB. And generally speaking, your dataset is unlikely to fit in that memory. And secondly, there's just no need to do the whole lot. Pretty much everything we want to do, we can do a small amount at a time. So in this case, I'm just going to show you how we can look at the result of this BGG model. So we're just going to do 4 at a time. So there's a get-batches command which basically says, in our BGG model, let's look inside the path that we've defined, and remember that path we made the sample path, and grab 4 at a time. So we've got an error, that's good. No such directory. Missing a trailing slash. So it's good to see these kind of errors. So we'll just go back to where we defined it, add our trailing slash, and go back down, and run it again. Okay, so we're in the sample, so there's 16 images. So let's grab one batch. And so that's going to grab 4 images and 4 labels. So here are the 4 images, and here are the 4 labels. So you can see that it's labeling at 0, 1 if it's a dog, and it'll be 1, 0 if it's a cat. So now that we've done that, so that's basically what our data looks like, we can call bgg.predict, passing in the images. And so that's now going to ignore the labels of what it actually is, and it's going to use this pre-trained model and tell us what does it think the 4 things are. In this case, it thinks they're a Rottweiler, an Egyptian cat, a toy terrier, and a Rottweiler. So you can see it's clearly made a mistake here. It's very rare that I find it makes a mistake, so I'm glad it did make one. You can see it must have been confused by all the awkward stuff going on in the background. So it's also shown you for this one that's a toy terrier, here are the probabilities of each, it's only 24% sure that it's a toy terrier. So you can see that it does actually know that it's not sure. Whereas the Rottweiler, it's very sure it's a Rottweiler. How come it's not so sure it's an Egyptian cat? Well, that's because there's lots of cats that look a little bit like an Egyptian cat, it doesn't quite know which one it is. So we could have a look at all those details and see exactly which other ones it thought it would be. We'll be looking at that in the next lesson. So the final thing I'm going to do is to show you how we can take these probabilities and turn them into a dogs vs. cats model. I'm going to do it quickly now and then I'm going to revisit this in the start of the next class, since we're out of time. So to take that 1000 probabilities, so we're just showing one probability from each, but there's actually 1000 probabilities for each. So this is the probability that it's a tench, it's a goldfish, it's a great white shark, so on and so forth. To take those 1000 probabilities and turn it into a dog vs. a cat prediction, we basically do exactly what we did before, just to say get batches that we call fine-tune. Now what fine-tune is going to do is it's going to build a new model and it's going to replace the 1000 categories with the two classes that it's found. And how does it know what the two classes are? Well that's because we have directories called cats and dogs. So the fine-tune command has now created a model that checks for cats and dogs. So creating the model is not enough, we have to actually train it. So if we then go.fit, it will then use that gradient descent method that I talked about earlier, that propagation, and it will attempt to make that model better and better at determining cats vs. dogs. Now obviously doing it on just 16 data items is A, fast, but B, not very accurate. So I can run it a few times and you can see that the accuracy is actually getting higher and higher each time, but the validation accuracy is not getting much higher. And that's because I'm running it on the sample. So if I ran it on the full dataset, it would take about 5 minutes to run and you try it when you get home, give it a go and see what accuracy you get. If you want to make the accuracy higher, just rerun this cell a bunch of times. So that's the end of today's class. It's the class which is kind of like the opening of a novel, when you have to introduce all of the characters and their backstories and stuff. So a little bit less deep learning goes on in the first class, a little bit more getting set up. Your first week is likely to be for many of you the most frustrating and challenging week because many of you will find you've got some kind of configuration problem or you don't understand how some piece of the stuff fits together. Don't worry, by the end of the 7 weeks, that stuff's all going to be straightforward and all of the interesting bit will be in the deep learning. So I think the more time you can put into this week, making sure you get all of that infrastructure stuff working and comfortable with what it is, take one of the things I've introduced today, look at the video and go and Google all the stuff that you're not already familiar with, understand how it works, anything you're unclear about, ask us and your colleagues on the Slack channel, on the forums. Teaching is the best way to learn, so go to the Wiki and try to explain the things you've learned. And make sure that you can run the code up to here that we've seen today. For those of you who are pretty familiar with most of this already, make sure you can run this code on a different dataset. And we'll talk about some possible different datasets that you can use tomorrow. Any of you that want to go further, please let Rachel or I know. We have lots of ideas for ways that you can extend this a long way. Thank you so much for coming and I'll see you next week. We'll talk to you during the week and make sure you've got your teammates' details so you can all stay in touch.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " So, Rachel and I started FastAI with this idea of make neural networks uncool again.", "tokens": [407, 11, 14246, 293, 286, 1409, 15968, 48698, 365, 341, 1558, 295, 652, 18161, 9590, 6219, 1092, 797, 13], "temperature": 0.0, "avg_logprob": -0.17154685258865357, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.003122443100437522}, {"id": 1, "seek": 0, "start": 10.0, "end": 15.72, "text": " It is a grand plan to be sure because currently they are terribly cool.", "tokens": [467, 307, 257, 2697, 1393, 281, 312, 988, 570, 4362, 436, 366, 22903, 1627, 13], "temperature": 0.0, "avg_logprob": -0.17154685258865357, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.003122443100437522}, {"id": 2, "seek": 0, "start": 15.72, "end": 23.04, "text": " But really there are some things that we want to see improve and that's why we're doing", "tokens": [583, 534, 456, 366, 512, 721, 300, 321, 528, 281, 536, 3470, 293, 300, 311, 983, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.17154685258865357, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.003122443100437522}, {"id": 3, "seek": 0, "start": 23.04, "end": 24.68, "text": " this course.", "tokens": [341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.17154685258865357, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.003122443100437522}, {"id": 4, "seek": 0, "start": 24.68, "end": 27.16, "text": " We're actually not making any money out of this course.", "tokens": [492, 434, 767, 406, 1455, 604, 1460, 484, 295, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.17154685258865357, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.003122443100437522}, {"id": 5, "seek": 2716, "start": 27.16, "end": 33.38, "text": " We're donating our fees both to the diversity fellowships that we're running and also to", "tokens": [492, 434, 36686, 527, 13370, 1293, 281, 264, 8811, 24989, 82, 300, 321, 434, 2614, 293, 611, 281], "temperature": 0.0, "avg_logprob": -0.1259263070781579, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.00012530633830465376}, {"id": 6, "seek": 2716, "start": 33.38, "end": 35.72, "text": " the Fred Hollows Foundation.", "tokens": [264, 10112, 46731, 82, 10335, 13], "temperature": 0.0, "avg_logprob": -0.1259263070781579, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.00012530633830465376}, {"id": 7, "seek": 2716, "start": 35.72, "end": 39.76, "text": " I would like to briefly give a quick pitch for the Fred Hollows Foundation for those", "tokens": [286, 576, 411, 281, 10515, 976, 257, 1702, 7293, 337, 264, 10112, 46731, 82, 10335, 337, 729], "temperature": 0.0, "avg_logprob": -0.1259263070781579, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.00012530633830465376}, {"id": 8, "seek": 2716, "start": 39.76, "end": 42.96, "text": " of you who aren't aware of it.", "tokens": [295, 291, 567, 3212, 380, 3650, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1259263070781579, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.00012530633830465376}, {"id": 9, "seek": 2716, "start": 42.96, "end": 48.879999999999995, "text": " Because as you know, deep learning is fantastic for computer vision.", "tokens": [1436, 382, 291, 458, 11, 2452, 2539, 307, 5456, 337, 3820, 5201, 13], "temperature": 0.0, "avg_logprob": -0.1259263070781579, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.00012530633830465376}, {"id": 10, "seek": 2716, "start": 48.879999999999995, "end": 53.04, "text": " It's basically allowing computers to see for the first time.", "tokens": [467, 311, 1936, 8293, 10807, 281, 536, 337, 264, 700, 565, 13], "temperature": 0.0, "avg_logprob": -0.1259263070781579, "compression_ratio": 1.665137614678899, "no_speech_prob": 0.00012530633830465376}, {"id": 11, "seek": 5304, "start": 53.04, "end": 58.68, "text": " What you might not realize is that there are 3 or 4 million people in the world who can't", "tokens": [708, 291, 1062, 406, 4325, 307, 300, 456, 366, 805, 420, 1017, 2459, 561, 294, 264, 1002, 567, 393, 380], "temperature": 0.0, "avg_logprob": -0.1537746896549147, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.4509840184473433e-05}, {"id": 12, "seek": 5304, "start": 58.68, "end": 62.32, "text": " see because they have something called cataract blindness.", "tokens": [536, 570, 436, 362, 746, 1219, 3857, 289, 578, 46101, 13], "temperature": 0.0, "avg_logprob": -0.1537746896549147, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.4509840184473433e-05}, {"id": 13, "seek": 5304, "start": 62.32, "end": 69.36, "text": " Cataract blindness can be cured for $25 per eye.", "tokens": [9565, 289, 578, 46101, 393, 312, 29617, 337, 1848, 6074, 680, 3313, 13], "temperature": 0.0, "avg_logprob": -0.1537746896549147, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.4509840184473433e-05}, {"id": 14, "seek": 5304, "start": 69.36, "end": 74.24, "text": " And actually the group of people who got that price down from thousands of dollars to $25", "tokens": [400, 767, 264, 1594, 295, 561, 567, 658, 300, 3218, 760, 490, 5383, 295, 3808, 281, 1848, 6074], "temperature": 0.0, "avg_logprob": -0.1537746896549147, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.4509840184473433e-05}, {"id": 15, "seek": 5304, "start": 74.24, "end": 77.56, "text": " was Fred Hollows, who was the Australian of the year some years ago.", "tokens": [390, 10112, 46731, 82, 11, 567, 390, 264, 13337, 295, 264, 1064, 512, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.1537746896549147, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.4509840184473433e-05}, {"id": 16, "seek": 5304, "start": 77.56, "end": 79.08, "text": " He's passed away now.", "tokens": [634, 311, 4678, 1314, 586, 13], "temperature": 0.0, "avg_logprob": -0.1537746896549147, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.4509840184473433e-05}, {"id": 17, "seek": 7908, "start": 79.08, "end": 84.88, "text": " But his legacy is in this foundation where if you donate $25, you're giving somebody", "tokens": [583, 702, 11711, 307, 294, 341, 7030, 689, 498, 291, 17751, 1848, 6074, 11, 291, 434, 2902, 2618], "temperature": 0.0, "avg_logprob": -0.09889039888486757, "compression_ratio": 1.5394736842105263, "no_speech_prob": 5.594220965576824e-06}, {"id": 18, "seek": 7908, "start": 84.88, "end": 86.17999999999999, "text": " their sight back.", "tokens": [641, 7860, 646, 13], "temperature": 0.0, "avg_logprob": -0.09889039888486757, "compression_ratio": 1.5394736842105263, "no_speech_prob": 5.594220965576824e-06}, {"id": 19, "seek": 7908, "start": 86.17999999999999, "end": 94.72, "text": " So as you learn to teach computers how to see, Rachel and I are also donating our fees", "tokens": [407, 382, 291, 1466, 281, 2924, 10807, 577, 281, 536, 11, 14246, 293, 286, 366, 611, 36686, 527, 13370], "temperature": 0.0, "avg_logprob": -0.09889039888486757, "compression_ratio": 1.5394736842105263, "no_speech_prob": 5.594220965576824e-06}, {"id": 20, "seek": 7908, "start": 94.72, "end": 96.75999999999999, "text": " from this to helping humans to see.", "tokens": [490, 341, 281, 4315, 6255, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.09889039888486757, "compression_ratio": 1.5394736842105263, "no_speech_prob": 5.594220965576824e-06}, {"id": 21, "seek": 7908, "start": 96.75999999999999, "end": 100.16, "text": " So we think that's a nice little touch.", "tokens": [407, 321, 519, 300, 311, 257, 1481, 707, 2557, 13], "temperature": 0.0, "avg_logprob": -0.09889039888486757, "compression_ratio": 1.5394736842105263, "no_speech_prob": 5.594220965576824e-06}, {"id": 22, "seek": 7908, "start": 100.16, "end": 104.88, "text": " So we're doing this both to help the Fred Hollows Foundation, but more importantly to", "tokens": [407, 321, 434, 884, 341, 1293, 281, 854, 264, 10112, 46731, 82, 10335, 11, 457, 544, 8906, 281], "temperature": 0.0, "avg_logprob": -0.09889039888486757, "compression_ratio": 1.5394736842105263, "no_speech_prob": 5.594220965576824e-06}, {"id": 23, "seek": 10488, "start": 104.88, "end": 109.72, "text": " help something we care a lot about, which is making deep learning more accessible.", "tokens": [854, 746, 321, 1127, 257, 688, 466, 11, 597, 307, 1455, 2452, 2539, 544, 9515, 13], "temperature": 0.0, "avg_logprob": -0.11947263224741046, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.9621387410734314e-06}, {"id": 24, "seek": 10488, "start": 109.72, "end": 113.11999999999999, "text": " It's currently terribly exclusive.", "tokens": [467, 311, 4362, 22903, 13005, 13], "temperature": 0.0, "avg_logprob": -0.11947263224741046, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.9621387410734314e-06}, {"id": 25, "seek": 10488, "start": 113.11999999999999, "end": 117.84, "text": " As I'm sure you've noticed, resources for teaching it tend to be quite mathematically", "tokens": [1018, 286, 478, 988, 291, 600, 5694, 11, 3593, 337, 4571, 309, 3928, 281, 312, 1596, 44003], "temperature": 0.0, "avg_logprob": -0.11947263224741046, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.9621387410734314e-06}, {"id": 26, "seek": 10488, "start": 117.84, "end": 118.84, "text": " intensive.", "tokens": [18957, 13], "temperature": 0.0, "avg_logprob": -0.11947263224741046, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.9621387410734314e-06}, {"id": 27, "seek": 10488, "start": 118.84, "end": 124.36, "text": " They really seem to be focused on a certain kind of ivory tower type audience.", "tokens": [814, 534, 1643, 281, 312, 5178, 322, 257, 1629, 733, 295, 49218, 10567, 2010, 4034, 13], "temperature": 0.0, "avg_logprob": -0.11947263224741046, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.9621387410734314e-06}, {"id": 28, "seek": 10488, "start": 124.36, "end": 131.68, "text": " So we're trying to create training and examples which are for non-machine learning and math", "tokens": [407, 321, 434, 1382, 281, 1884, 3097, 293, 5110, 597, 366, 337, 2107, 12, 46061, 2539, 293, 5221], "temperature": 0.0, "avg_logprob": -0.11947263224741046, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.9621387410734314e-06}, {"id": 29, "seek": 13168, "start": 131.68, "end": 139.08, "text": " experts, dealing with small data sets, giving role models of applications you can develop", "tokens": [8572, 11, 6260, 365, 1359, 1412, 6352, 11, 2902, 3090, 5245, 295, 5821, 291, 393, 1499], "temperature": 0.0, "avg_logprob": -0.1273027295651643, "compression_ratio": 1.6814159292035398, "no_speech_prob": 8.267722478194628e-06}, {"id": 30, "seek": 13168, "start": 139.08, "end": 140.08, "text": " quickly.", "tokens": [2661, 13], "temperature": 0.0, "avg_logprob": -0.1273027295651643, "compression_ratio": 1.6814159292035398, "no_speech_prob": 8.267722478194628e-06}, {"id": 31, "seek": 13168, "start": 140.08, "end": 146.0, "text": " Today we're going to see how to create a real useful piece of deep learning code in 7 lines", "tokens": [2692, 321, 434, 516, 281, 536, 577, 281, 1884, 257, 957, 4420, 2522, 295, 2452, 2539, 3089, 294, 1614, 3876], "temperature": 0.0, "avg_logprob": -0.1273027295651643, "compression_ratio": 1.6814159292035398, "no_speech_prob": 8.267722478194628e-06}, {"id": 32, "seek": 13168, "start": 146.0, "end": 148.92000000000002, "text": " of code.", "tokens": [295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1273027295651643, "compression_ratio": 1.6814159292035398, "no_speech_prob": 8.267722478194628e-06}, {"id": 33, "seek": 13168, "start": 148.92000000000002, "end": 156.04000000000002, "text": " We want to get to the point where it's easy for domain experts to work with deep learning.", "tokens": [492, 528, 281, 483, 281, 264, 935, 689, 309, 311, 1858, 337, 9274, 8572, 281, 589, 365, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1273027295651643, "compression_ratio": 1.6814159292035398, "no_speech_prob": 8.267722478194628e-06}, {"id": 34, "seek": 13168, "start": 156.04000000000002, "end": 159.76000000000002, "text": " There are a lot of domain experts here, whether you're working with getting satellites in", "tokens": [821, 366, 257, 688, 295, 9274, 8572, 510, 11, 1968, 291, 434, 1364, 365, 1242, 24960, 294], "temperature": 0.0, "avg_logprob": -0.1273027295651643, "compression_ratio": 1.6814159292035398, "no_speech_prob": 8.267722478194628e-06}, {"id": 35, "seek": 15976, "start": 159.76, "end": 165.84, "text": " the air or whether you're working with analyzing the results of chemical studies or whether", "tokens": [264, 1988, 420, 1968, 291, 434, 1364, 365, 23663, 264, 3542, 295, 7313, 5313, 420, 1968], "temperature": 0.0, "avg_logprob": -0.1567960538362202, "compression_ratio": 1.7103174603174602, "no_speech_prob": 9.972668522095773e-06}, {"id": 36, "seek": 15976, "start": 165.84, "end": 167.84, "text": " you're analyzing fraud at a bank.", "tokens": [291, 434, 23663, 14560, 412, 257, 3765, 13], "temperature": 0.0, "avg_logprob": -0.1567960538362202, "compression_ratio": 1.7103174603174602, "no_speech_prob": 9.972668522095773e-06}, {"id": 37, "seek": 15976, "start": 167.84, "end": 171.01999999999998, "text": " I know all those people are here in this audience.", "tokens": [286, 458, 439, 729, 561, 366, 510, 294, 341, 4034, 13], "temperature": 0.0, "avg_logprob": -0.1567960538362202, "compression_ratio": 1.7103174603174602, "no_speech_prob": 9.972668522095773e-06}, {"id": 38, "seek": 15976, "start": 171.01999999999998, "end": 176.32, "text": " You are domain experts that we want to enable to use deep learning.", "tokens": [509, 366, 9274, 8572, 300, 321, 528, 281, 9528, 281, 764, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1567960538362202, "compression_ratio": 1.7103174603174602, "no_speech_prob": 9.972668522095773e-06}, {"id": 39, "seek": 15976, "start": 176.32, "end": 182.04, "text": " At this stage, the audience for this course is coders, because that's as far as we think", "tokens": [1711, 341, 3233, 11, 264, 4034, 337, 341, 1164, 307, 17656, 433, 11, 570, 300, 311, 382, 1400, 382, 321, 519], "temperature": 0.0, "avg_logprob": -0.1567960538362202, "compression_ratio": 1.7103174603174602, "no_speech_prob": 9.972668522095773e-06}, {"id": 40, "seek": 15976, "start": 182.04, "end": 183.32, "text": " we can get at this point.", "tokens": [321, 393, 483, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.1567960538362202, "compression_ratio": 1.7103174603174602, "no_speech_prob": 9.972668522095773e-06}, {"id": 41, "seek": 15976, "start": 183.32, "end": 186.48, "text": " We don't need you to be a math expert, but we do need you to be coders.", "tokens": [492, 500, 380, 643, 291, 281, 312, 257, 5221, 5844, 11, 457, 321, 360, 643, 291, 281, 312, 17656, 433, 13], "temperature": 0.0, "avg_logprob": -0.1567960538362202, "compression_ratio": 1.7103174603174602, "no_speech_prob": 9.972668522095773e-06}, {"id": 42, "seek": 18648, "start": 186.48, "end": 189.72, "text": " I know all of you have been told of that prerequisite.", "tokens": [286, 458, 439, 295, 291, 362, 668, 1907, 295, 300, 38333, 34152, 13], "temperature": 0.0, "avg_logprob": -0.11733779391726933, "compression_ratio": 1.6642066420664208, "no_speech_prob": 8.664498636790086e-06}, {"id": 43, "seek": 18648, "start": 189.72, "end": 194.6, "text": " We do hope that with your help, we can get to the point where non-coders will also be", "tokens": [492, 360, 1454, 300, 365, 428, 854, 11, 321, 393, 483, 281, 264, 935, 689, 2107, 12, 26560, 433, 486, 611, 312], "temperature": 0.0, "avg_logprob": -0.11733779391726933, "compression_ratio": 1.6642066420664208, "no_speech_prob": 8.664498636790086e-06}, {"id": 44, "seek": 18648, "start": 194.6, "end": 196.92, "text": " able to participate in this.", "tokens": [1075, 281, 8197, 294, 341, 13], "temperature": 0.0, "avg_logprob": -0.11733779391726933, "compression_ratio": 1.6642066420664208, "no_speech_prob": 8.664498636790086e-06}, {"id": 45, "seek": 18648, "start": 196.92, "end": 203.12, "text": " The reason why we care about this is that there are problems like improving agricultural", "tokens": [440, 1778, 983, 321, 1127, 466, 341, 307, 300, 456, 366, 2740, 411, 11470, 19587], "temperature": 0.0, "avg_logprob": -0.11733779391726933, "compression_ratio": 1.6642066420664208, "no_speech_prob": 8.664498636790086e-06}, {"id": 46, "seek": 18648, "start": 203.12, "end": 207.6, "text": " yields in the developing world, or making medical diagnostics available to those who", "tokens": [32168, 294, 264, 6416, 1002, 11, 420, 1455, 4625, 43215, 1167, 2435, 281, 729, 567], "temperature": 0.0, "avg_logprob": -0.11733779391726933, "compression_ratio": 1.6642066420664208, "no_speech_prob": 8.664498636790086e-06}, {"id": 47, "seek": 18648, "start": 207.6, "end": 208.95999999999998, "text": " don't have them.", "tokens": [500, 380, 362, 552, 13], "temperature": 0.0, "avg_logprob": -0.11733779391726933, "compression_ratio": 1.6642066420664208, "no_speech_prob": 8.664498636790086e-06}, {"id": 48, "seek": 18648, "start": 208.95999999999998, "end": 213.12, "text": " These are things that can be solved with deep learning, but they're not going to be solved", "tokens": [1981, 366, 721, 300, 393, 312, 13041, 365, 2452, 2539, 11, 457, 436, 434, 406, 516, 281, 312, 13041], "temperature": 0.0, "avg_logprob": -0.11733779391726933, "compression_ratio": 1.6642066420664208, "no_speech_prob": 8.664498636790086e-06}, {"id": 49, "seek": 21312, "start": 213.12, "end": 218.04, "text": " by people who are at these more ivory tower firms on the whole, because they're not really", "tokens": [538, 561, 567, 366, 412, 613, 544, 49218, 10567, 18055, 322, 264, 1379, 11, 570, 436, 434, 406, 534], "temperature": 0.0, "avg_logprob": -0.1357960236811005, "compression_ratio": 1.904382470119522, "no_speech_prob": 2.078436955343932e-05}, {"id": 50, "seek": 21312, "start": 218.04, "end": 220.04, "text": " that familiar with these problems.", "tokens": [300, 4963, 365, 613, 2740, 13], "temperature": 0.0, "avg_logprob": -0.1357960236811005, "compression_ratio": 1.904382470119522, "no_speech_prob": 2.078436955343932e-05}, {"id": 51, "seek": 21312, "start": 220.04, "end": 224.72, "text": " The people who are familiar with these problems are the people who work with them every day.", "tokens": [440, 561, 567, 366, 4963, 365, 613, 2740, 366, 264, 561, 567, 589, 365, 552, 633, 786, 13], "temperature": 0.0, "avg_logprob": -0.1357960236811005, "compression_ratio": 1.904382470119522, "no_speech_prob": 2.078436955343932e-05}, {"id": 52, "seek": 21312, "start": 224.72, "end": 228.24, "text": " So for example, I've had quite a bit to do with a lot of these kinds of people, such", "tokens": [407, 337, 1365, 11, 286, 600, 632, 1596, 257, 857, 281, 360, 365, 257, 688, 295, 613, 3685, 295, 561, 11, 1270], "temperature": 0.0, "avg_logprob": -0.1357960236811005, "compression_ratio": 1.904382470119522, "no_speech_prob": 2.078436955343932e-05}, {"id": 53, "seek": 21312, "start": 228.24, "end": 233.04000000000002, "text": " as through the World Economic Forum, I know people who are trying to help cure TB and", "tokens": [382, 807, 264, 3937, 25776, 29704, 11, 286, 458, 561, 567, 366, 1382, 281, 854, 13698, 29711, 293], "temperature": 0.0, "avg_logprob": -0.1357960236811005, "compression_ratio": 1.904382470119522, "no_speech_prob": 2.078436955343932e-05}, {"id": 54, "seek": 21312, "start": 233.04000000000002, "end": 238.70000000000002, "text": " malaria, I know people who are trying to help with agricultural issues in the developing", "tokens": [45182, 11, 286, 458, 561, 567, 366, 1382, 281, 854, 365, 19587, 2663, 294, 264, 6416], "temperature": 0.0, "avg_logprob": -0.1357960236811005, "compression_ratio": 1.904382470119522, "no_speech_prob": 2.078436955343932e-05}, {"id": 55, "seek": 23870, "start": 238.7, "end": 243.39999999999998, "text": " world, and so forth, and these are all people who want to be using deep learning for things", "tokens": [1002, 11, 293, 370, 5220, 11, 293, 613, 366, 439, 561, 567, 528, 281, 312, 1228, 2452, 2539, 337, 721], "temperature": 0.0, "avg_logprob": -0.18629168416117572, "compression_ratio": 1.7319148936170212, "no_speech_prob": 4.494975200941553e-06}, {"id": 56, "seek": 23870, "start": 243.39999999999998, "end": 250.48, "text": " like analyzing crop imagery from satellites, or my most recent startup which was analyzing", "tokens": [411, 23663, 9086, 24340, 490, 24960, 11, 420, 452, 881, 5162, 18578, 597, 390, 23663], "temperature": 0.0, "avg_logprob": -0.18629168416117572, "compression_ratio": 1.7319148936170212, "no_speech_prob": 4.494975200941553e-06}, {"id": 57, "seek": 23870, "start": 250.48, "end": 255.56, "text": " radiological studies using deep learning to deal with things like the fact that in the", "tokens": [16335, 4383, 5313, 1228, 2452, 2539, 281, 2028, 365, 721, 411, 264, 1186, 300, 294, 264], "temperature": 0.0, "avg_logprob": -0.18629168416117572, "compression_ratio": 1.7319148936170212, "no_speech_prob": 4.494975200941553e-06}, {"id": 58, "seek": 23870, "start": 255.56, "end": 260.64, "text": " entire continent of Africa, there are only 7 pediatric radiologists.", "tokens": [2302, 18932, 295, 7349, 11, 456, 366, 787, 1614, 27477, 16335, 12256, 13], "temperature": 0.0, "avg_logprob": -0.18629168416117572, "compression_ratio": 1.7319148936170212, "no_speech_prob": 4.494975200941553e-06}, {"id": 59, "seek": 23870, "start": 260.64, "end": 268.36, "text": " So most countries in Africa, no kids have access to any radiologist.", "tokens": [407, 881, 3517, 294, 7349, 11, 572, 2301, 362, 2105, 281, 604, 16335, 9201, 13], "temperature": 0.0, "avg_logprob": -0.18629168416117572, "compression_ratio": 1.7319148936170212, "no_speech_prob": 4.494975200941553e-06}, {"id": 60, "seek": 26836, "start": 268.36, "end": 274.28000000000003, "text": " They therefore have no access to any kind of modern image-based medical diagnostics.", "tokens": [814, 4412, 362, 572, 2105, 281, 604, 733, 295, 4363, 3256, 12, 6032, 4625, 43215, 1167, 13], "temperature": 0.0, "avg_logprob": -0.18483745928892156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 2.668663910299074e-05}, {"id": 61, "seek": 26836, "start": 274.28000000000003, "end": 278.08000000000004, "text": " So these are some of the reasons that we're creating and running this course.", "tokens": [407, 613, 366, 512, 295, 264, 4112, 300, 321, 434, 4084, 293, 2614, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.18483745928892156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 2.668663910299074e-05}, {"id": 62, "seek": 26836, "start": 278.08000000000004, "end": 283.36, "text": " So we hope that the kind of feel of this community is going to be very different to the feel", "tokens": [407, 321, 1454, 300, 264, 733, 295, 841, 295, 341, 1768, 307, 516, 281, 312, 588, 819, 281, 264, 841], "temperature": 0.0, "avg_logprob": -0.18483745928892156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 2.668663910299074e-05}, {"id": 63, "seek": 26836, "start": 283.36, "end": 286.6, "text": " of a lot of the deep learning type communities that have appeared before, that have been", "tokens": [295, 257, 688, 295, 264, 2452, 2539, 2010, 4456, 300, 362, 8516, 949, 11, 300, 362, 668], "temperature": 0.0, "avg_logprob": -0.18483745928892156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 2.668663910299074e-05}, {"id": 64, "seek": 26836, "start": 286.6, "end": 292.12, "text": " all about, let's trim 0.1% of this academic benchmark.", "tokens": [439, 466, 11, 718, 311, 10445, 1958, 13, 16, 4, 295, 341, 7778, 18927, 13], "temperature": 0.0, "avg_logprob": -0.18483745928892156, "compression_ratio": 1.6764705882352942, "no_speech_prob": 2.668663910299074e-05}, {"id": 65, "seek": 29212, "start": 292.12, "end": 299.0, "text": " This is going to be all about, let's do shit that matters to people as quickly as possible.", "tokens": [639, 307, 516, 281, 312, 439, 466, 11, 718, 311, 360, 4611, 300, 7001, 281, 561, 382, 2661, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.08980556803012113, "compression_ratio": 1.757201646090535, "no_speech_prob": 1.7777700122678652e-05}, {"id": 66, "seek": 29212, "start": 299.0, "end": 302.48, "text": " Sometimes to do that, we're going to have to push the state of the art of the research.", "tokens": [4803, 281, 360, 300, 11, 321, 434, 516, 281, 362, 281, 2944, 264, 1785, 295, 264, 1523, 295, 264, 2132, 13], "temperature": 0.0, "avg_logprob": -0.08980556803012113, "compression_ratio": 1.757201646090535, "no_speech_prob": 1.7777700122678652e-05}, {"id": 67, "seek": 29212, "start": 302.48, "end": 308.48, "text": " And where that happens, we won't be afraid to show you the state of the art of the research.", "tokens": [400, 689, 300, 2314, 11, 321, 1582, 380, 312, 4638, 281, 855, 291, 264, 1785, 295, 264, 1523, 295, 264, 2132, 13], "temperature": 0.0, "avg_logprob": -0.08980556803012113, "compression_ratio": 1.757201646090535, "no_speech_prob": 1.7777700122678652e-05}, {"id": 68, "seek": 29212, "start": 308.48, "end": 313.68, "text": " The idea is that by the end of part 1 of this, you will be able to use all of the current", "tokens": [440, 1558, 307, 300, 538, 264, 917, 295, 644, 502, 295, 341, 11, 291, 486, 312, 1075, 281, 764, 439, 295, 264, 2190], "temperature": 0.0, "avg_logprob": -0.08980556803012113, "compression_ratio": 1.757201646090535, "no_speech_prob": 1.7777700122678652e-05}, {"id": 69, "seek": 29212, "start": 313.68, "end": 318.12, "text": " best practices in the most important deep learning applications.", "tokens": [1151, 7525, 294, 264, 881, 1021, 2452, 2539, 5821, 13], "temperature": 0.0, "avg_logprob": -0.08980556803012113, "compression_ratio": 1.757201646090535, "no_speech_prob": 1.7777700122678652e-05}, {"id": 70, "seek": 31812, "start": 318.12, "end": 322.84000000000003, "text": " If you stick around for part 2, you'll be at the cutting edge of research in most of", "tokens": [759, 291, 2897, 926, 337, 644, 568, 11, 291, 603, 312, 412, 264, 6492, 4691, 295, 2132, 294, 881, 295], "temperature": 0.0, "avg_logprob": -0.11999753009842103, "compression_ratio": 1.53, "no_speech_prob": 9.368378414364997e-06}, {"id": 71, "seek": 31812, "start": 322.84000000000003, "end": 324.72, "text": " the most important research areas.", "tokens": [264, 881, 1021, 2132, 3179, 13], "temperature": 0.0, "avg_logprob": -0.11999753009842103, "compression_ratio": 1.53, "no_speech_prob": 9.368378414364997e-06}, {"id": 72, "seek": 31812, "start": 324.72, "end": 327.72, "text": " So we're not dumbing this down by any means.", "tokens": [407, 321, 434, 406, 10316, 278, 341, 760, 538, 604, 1355, 13], "temperature": 0.0, "avg_logprob": -0.11999753009842103, "compression_ratio": 1.53, "no_speech_prob": 9.368378414364997e-06}, {"id": 73, "seek": 31812, "start": 327.72, "end": 329.7, "text": " We're just refocusing it.", "tokens": [492, 434, 445, 1895, 905, 7981, 309, 13], "temperature": 0.0, "avg_logprob": -0.11999753009842103, "compression_ratio": 1.53, "no_speech_prob": 9.368378414364997e-06}, {"id": 74, "seek": 31812, "start": 329.7, "end": 337.92, "text": " So the reason why we're excited about this is that we have now the three pieces of this", "tokens": [407, 264, 1778, 983, 321, 434, 2919, 466, 341, 307, 300, 321, 362, 586, 264, 1045, 3755, 295, 341], "temperature": 0.0, "avg_logprob": -0.11999753009842103, "compression_ratio": 1.53, "no_speech_prob": 9.368378414364997e-06}, {"id": 75, "seek": 31812, "start": 337.92, "end": 340.56, "text": " universal learning machine.", "tokens": [11455, 2539, 3479, 13], "temperature": 0.0, "avg_logprob": -0.11999753009842103, "compression_ratio": 1.53, "no_speech_prob": 9.368378414364997e-06}, {"id": 76, "seek": 34056, "start": 340.56, "end": 350.4, "text": " We now have the three critical pieces, like an infinitely flexible function, all-purpose", "tokens": [492, 586, 362, 264, 1045, 4924, 3755, 11, 411, 364, 36227, 11358, 2445, 11, 439, 12, 42601], "temperature": 0.0, "avg_logprob": -0.14878041716827745, "compression_ratio": 1.6551724137931034, "no_speech_prob": 9.223324013873935e-06}, {"id": 77, "seek": 34056, "start": 350.4, "end": 353.44, "text": " parameter fitting which is fast and scalable.", "tokens": [13075, 15669, 597, 307, 2370, 293, 38481, 13], "temperature": 0.0, "avg_logprob": -0.14878041716827745, "compression_ratio": 1.6551724137931034, "no_speech_prob": 9.223324013873935e-06}, {"id": 78, "seek": 34056, "start": 353.44, "end": 357.52, "text": " So I'll tell you what I mean about these three pieces.", "tokens": [407, 286, 603, 980, 291, 437, 286, 914, 466, 613, 1045, 3755, 13], "temperature": 0.0, "avg_logprob": -0.14878041716827745, "compression_ratio": 1.6551724137931034, "no_speech_prob": 9.223324013873935e-06}, {"id": 79, "seek": 34056, "start": 357.52, "end": 360.0, "text": " The neural network is the function.", "tokens": [440, 18161, 3209, 307, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.14878041716827745, "compression_ratio": 1.6551724137931034, "no_speech_prob": 9.223324013873935e-06}, {"id": 80, "seek": 34056, "start": 360.0, "end": 363.52, "text": " We're going to learn about exactly how neural networks work.", "tokens": [492, 434, 516, 281, 1466, 466, 2293, 577, 18161, 9590, 589, 13], "temperature": 0.0, "avg_logprob": -0.14878041716827745, "compression_ratio": 1.6551724137931034, "no_speech_prob": 9.223324013873935e-06}, {"id": 81, "seek": 34056, "start": 363.52, "end": 369.84000000000003, "text": " But the important thing about a neural network is that they are universal approximation machines.", "tokens": [583, 264, 1021, 551, 466, 257, 18161, 3209, 307, 300, 436, 366, 11455, 28023, 8379, 13], "temperature": 0.0, "avg_logprob": -0.14878041716827745, "compression_ratio": 1.6551724137931034, "no_speech_prob": 9.223324013873935e-06}, {"id": 82, "seek": 36984, "start": 369.84, "end": 373.71999999999997, "text": " There's a mathematical proof, the universal approximation theorem, that we're going to", "tokens": [821, 311, 257, 18894, 8177, 11, 264, 11455, 28023, 20904, 11, 300, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.1325650657575155, "compression_ratio": 1.974025974025974, "no_speech_prob": 9.818093531066552e-06}, {"id": 83, "seek": 36984, "start": 373.71999999999997, "end": 380.2, "text": " learn all about, which tells us that this kind of mathematical function is capable of", "tokens": [1466, 439, 466, 11, 597, 5112, 505, 300, 341, 733, 295, 18894, 2445, 307, 8189, 295], "temperature": 0.0, "avg_logprob": -0.1325650657575155, "compression_ratio": 1.974025974025974, "no_speech_prob": 9.818093531066552e-06}, {"id": 84, "seek": 36984, "start": 380.2, "end": 384.96, "text": " handling any kind of problem we can throw at it, whether that mathematical function", "tokens": [13175, 604, 733, 295, 1154, 321, 393, 3507, 412, 309, 11, 1968, 300, 18894, 2445], "temperature": 0.0, "avg_logprob": -0.1325650657575155, "compression_ratio": 1.974025974025974, "no_speech_prob": 9.818093531066552e-06}, {"id": 85, "seek": 36984, "start": 384.96, "end": 390.59999999999997, "text": " be how do I translate English into Hungarian, or whether that mathematical function is how", "tokens": [312, 577, 360, 286, 13799, 3669, 666, 38034, 11, 420, 1968, 300, 18894, 2445, 307, 577], "temperature": 0.0, "avg_logprob": -0.1325650657575155, "compression_ratio": 1.974025974025974, "no_speech_prob": 9.818093531066552e-06}, {"id": 86, "seek": 36984, "start": 390.59999999999997, "end": 396.28, "text": " do I recognize pictures of cats, or whether that mathematical function is how do I identify", "tokens": [360, 286, 5521, 5242, 295, 11111, 11, 420, 1968, 300, 18894, 2445, 307, 577, 360, 286, 5876], "temperature": 0.0, "avg_logprob": -0.1325650657575155, "compression_ratio": 1.974025974025974, "no_speech_prob": 9.818093531066552e-06}, {"id": 87, "seek": 36984, "start": 396.28, "end": 398.79999999999995, "text": " unhealthy crops.", "tokens": [29147, 16829, 13], "temperature": 0.0, "avg_logprob": -0.1325650657575155, "compression_ratio": 1.974025974025974, "no_speech_prob": 9.818093531066552e-06}, {"id": 88, "seek": 39880, "start": 398.8, "end": 400.6, "text": " It can handle any of these things.", "tokens": [467, 393, 4813, 604, 295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.16034036590939477, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.1478598935354967e-05}, {"id": 89, "seek": 39880, "start": 400.6, "end": 407.2, "text": " So with that mathematical function, the second thing you need is some way to fit the parameters", "tokens": [407, 365, 300, 18894, 2445, 11, 264, 1150, 551, 291, 643, 307, 512, 636, 281, 3318, 264, 9834], "temperature": 0.0, "avg_logprob": -0.16034036590939477, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.1478598935354967e-05}, {"id": 90, "seek": 39880, "start": 407.2, "end": 410.52000000000004, "text": " of that function to your particular need.", "tokens": [295, 300, 2445, 281, 428, 1729, 643, 13], "temperature": 0.0, "avg_logprob": -0.16034036590939477, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.1478598935354967e-05}, {"id": 91, "seek": 39880, "start": 410.52000000000004, "end": 415.64, "text": " And there's a very simple way to do that called gradient descent, and in particular something", "tokens": [400, 456, 311, 257, 588, 2199, 636, 281, 360, 300, 1219, 16235, 23475, 11, 293, 294, 1729, 746], "temperature": 0.0, "avg_logprob": -0.16034036590939477, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.1478598935354967e-05}, {"id": 92, "seek": 39880, "start": 415.64, "end": 422.32, "text": " called backward propagation, which we will learn all about in this lesson and the next", "tokens": [1219, 23897, 38377, 11, 597, 321, 486, 1466, 439, 466, 294, 341, 6898, 293, 264, 958], "temperature": 0.0, "avg_logprob": -0.16034036590939477, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.1478598935354967e-05}, {"id": 93, "seek": 39880, "start": 422.32, "end": 423.32, "text": " lesson.", "tokens": [6898, 13], "temperature": 0.0, "avg_logprob": -0.16034036590939477, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.1478598935354967e-05}, {"id": 94, "seek": 42332, "start": 423.32, "end": 429.92, "text": " The important thing is though that these two pieces together allow us to start with a function", "tokens": [440, 1021, 551, 307, 1673, 300, 613, 732, 3755, 1214, 2089, 505, 281, 722, 365, 257, 2445], "temperature": 0.0, "avg_logprob": -0.09459074553068694, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.5206419448077213e-05}, {"id": 95, "seek": 42332, "start": 429.92, "end": 435.36, "text": " that is in theory capable of doing anything and turn it into a function that is in practice", "tokens": [300, 307, 294, 5261, 8189, 295, 884, 1340, 293, 1261, 309, 666, 257, 2445, 300, 307, 294, 3124], "temperature": 0.0, "avg_logprob": -0.09459074553068694, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.5206419448077213e-05}, {"id": 96, "seek": 42332, "start": 435.36, "end": 440.88, "text": " capable of doing whatever you want to do, as long as you have data that shows examples", "tokens": [8189, 295, 884, 2035, 291, 528, 281, 360, 11, 382, 938, 382, 291, 362, 1412, 300, 3110, 5110], "temperature": 0.0, "avg_logprob": -0.09459074553068694, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.5206419448077213e-05}, {"id": 97, "seek": 42332, "start": 440.88, "end": 443.08, "text": " of what you want to do.", "tokens": [295, 437, 291, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.09459074553068694, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.5206419448077213e-05}, {"id": 98, "seek": 42332, "start": 443.08, "end": 447.48, "text": " The third piece, which has been missing until very recently, is being able to do this in", "tokens": [440, 2636, 2522, 11, 597, 575, 668, 5361, 1826, 588, 3938, 11, 307, 885, 1075, 281, 360, 341, 294], "temperature": 0.0, "avg_logprob": -0.09459074553068694, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.5206419448077213e-05}, {"id": 99, "seek": 42332, "start": 447.48, "end": 452.32, "text": " a way that actually works with the amount of data that you have in the time that you", "tokens": [257, 636, 300, 767, 1985, 365, 264, 2372, 295, 1412, 300, 291, 362, 294, 264, 565, 300, 291], "temperature": 0.0, "avg_logprob": -0.09459074553068694, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.5206419448077213e-05}, {"id": 100, "seek": 45232, "start": 452.32, "end": 454.28, "text": " have available.", "tokens": [362, 2435, 13], "temperature": 0.0, "avg_logprob": -0.15582787736933282, "compression_ratio": 1.6228813559322033, "no_speech_prob": 8.53020992508391e-06}, {"id": 101, "seek": 45232, "start": 454.28, "end": 459.12, "text": " This is all changed thanks particularly to GPUs.", "tokens": [639, 307, 439, 3105, 3231, 4098, 281, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.15582787736933282, "compression_ratio": 1.6228813559322033, "no_speech_prob": 8.53020992508391e-06}, {"id": 102, "seek": 45232, "start": 459.12, "end": 465.44, "text": " So GPUs are graphics processing units, also called video cards, that's kind of an older", "tokens": [407, 18407, 82, 366, 11837, 9007, 6815, 11, 611, 1219, 960, 5632, 11, 300, 311, 733, 295, 364, 4906], "temperature": 0.0, "avg_logprob": -0.15582787736933282, "compression_ratio": 1.6228813559322033, "no_speech_prob": 8.53020992508391e-06}, {"id": 103, "seek": 45232, "start": 465.44, "end": 469.71999999999997, "text": " term now, also called graphics cards.", "tokens": [1433, 586, 11, 611, 1219, 11837, 5632, 13], "temperature": 0.0, "avg_logprob": -0.15582787736933282, "compression_ratio": 1.6228813559322033, "no_speech_prob": 8.53020992508391e-06}, {"id": 104, "seek": 45232, "start": 469.71999999999997, "end": 475.38, "text": " And these are devices inside your computer which were originally designed for playing", "tokens": [400, 613, 366, 5759, 1854, 428, 3820, 597, 645, 7993, 4761, 337, 2433], "temperature": 0.0, "avg_logprob": -0.15582787736933282, "compression_ratio": 1.6228813559322033, "no_speech_prob": 8.53020992508391e-06}, {"id": 105, "seek": 45232, "start": 475.38, "end": 476.62, "text": " computer games.", "tokens": [3820, 2813, 13], "temperature": 0.0, "avg_logprob": -0.15582787736933282, "compression_ratio": 1.6228813559322033, "no_speech_prob": 8.53020992508391e-06}, {"id": 106, "seek": 45232, "start": 476.62, "end": 481.96, "text": " So it's kind of like when you're looking at this alien from the left-hand side and there's", "tokens": [407, 309, 311, 733, 295, 411, 562, 291, 434, 1237, 412, 341, 12319, 490, 264, 1411, 12, 5543, 1252, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.15582787736933282, "compression_ratio": 1.6228813559322033, "no_speech_prob": 8.53020992508391e-06}, {"id": 107, "seek": 48196, "start": 481.96, "end": 488.28, "text": " light coming from above, what pixel color do I need for each place?", "tokens": [1442, 1348, 490, 3673, 11, 437, 19261, 2017, 360, 286, 643, 337, 1184, 1081, 30], "temperature": 0.0, "avg_logprob": -0.08238656044006348, "compression_ratio": 1.6541353383458646, "no_speech_prob": 1.994710828512325e-06}, {"id": 108, "seek": 48196, "start": 488.28, "end": 494.44, "text": " That's basically a whole bunch of linear algebra operations, a whole bunch of matrix products.", "tokens": [663, 311, 1936, 257, 1379, 3840, 295, 8213, 21989, 7705, 11, 257, 1379, 3840, 295, 8141, 3383, 13], "temperature": 0.0, "avg_logprob": -0.08238656044006348, "compression_ratio": 1.6541353383458646, "no_speech_prob": 1.994710828512325e-06}, {"id": 109, "seek": 48196, "start": 494.44, "end": 498.47999999999996, "text": " It turns out that those are the same operations that we need for deep learning.", "tokens": [467, 4523, 484, 300, 729, 366, 264, 912, 7705, 300, 321, 643, 337, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.08238656044006348, "compression_ratio": 1.6541353383458646, "no_speech_prob": 1.994710828512325e-06}, {"id": 110, "seek": 48196, "start": 498.47999999999996, "end": 503.08, "text": " And so because of the massive amount of money in the gaming industry that was thrown at", "tokens": [400, 370, 570, 295, 264, 5994, 2372, 295, 1460, 294, 264, 9703, 3518, 300, 390, 11732, 412], "temperature": 0.0, "avg_logprob": -0.08238656044006348, "compression_ratio": 1.6541353383458646, "no_speech_prob": 1.994710828512325e-06}, {"id": 111, "seek": 48196, "start": 503.08, "end": 508.91999999999996, "text": " this problem, we now have incredibly cheap, incredibly powerful cards for figuring out", "tokens": [341, 1154, 11, 321, 586, 362, 6252, 7084, 11, 6252, 4005, 5632, 337, 15213, 484], "temperature": 0.0, "avg_logprob": -0.08238656044006348, "compression_ratio": 1.6541353383458646, "no_speech_prob": 1.994710828512325e-06}, {"id": 112, "seek": 48196, "start": 508.91999999999996, "end": 511.03999999999996, "text": " what aliens look like.", "tokens": [437, 21594, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.08238656044006348, "compression_ratio": 1.6541353383458646, "no_speech_prob": 1.994710828512325e-06}, {"id": 113, "seek": 51104, "start": 511.04, "end": 516.52, "text": " And we can now use these, therefore, to figure out how to improve medical diagnostics in", "tokens": [400, 321, 393, 586, 764, 613, 11, 4412, 11, 281, 2573, 484, 577, 281, 3470, 4625, 43215, 1167, 294], "temperature": 0.0, "avg_logprob": -0.1494361714618962, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.1659277333819773e-05}, {"id": 114, "seek": 51104, "start": 516.52, "end": 517.6800000000001, "text": " Africa.", "tokens": [7349, 13], "temperature": 0.0, "avg_logprob": -0.1494361714618962, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.1659277333819773e-05}, {"id": 115, "seek": 51104, "start": 517.6800000000001, "end": 521.88, "text": " So it's a nice handy little side effect.", "tokens": [407, 309, 311, 257, 1481, 13239, 707, 1252, 1802, 13], "temperature": 0.0, "avg_logprob": -0.1494361714618962, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.1659277333819773e-05}, {"id": 116, "seek": 51104, "start": 521.88, "end": 528.16, "text": " GPUs are in all of your computers, but not all of your computers are suitable for deep", "tokens": [18407, 82, 366, 294, 439, 295, 428, 10807, 11, 457, 406, 439, 295, 428, 10807, 366, 12873, 337, 2452], "temperature": 0.0, "avg_logprob": -0.1494361714618962, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.1659277333819773e-05}, {"id": 117, "seek": 51104, "start": 528.16, "end": 529.64, "text": " learning.", "tokens": [2539, 13], "temperature": 0.0, "avg_logprob": -0.1494361714618962, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.1659277333819773e-05}, {"id": 118, "seek": 51104, "start": 529.64, "end": 536.28, "text": " And the reason is that programming a GPU to do deep learning really requires a particular", "tokens": [400, 264, 1778, 307, 300, 9410, 257, 18407, 281, 360, 2452, 2539, 534, 7029, 257, 1729], "temperature": 0.0, "avg_logprob": -0.1494361714618962, "compression_ratio": 1.565217391304348, "no_speech_prob": 1.1659277333819773e-05}, {"id": 119, "seek": 53628, "start": 536.28, "end": 545.48, "text": " kind of GPU, and in practice at the moment it really requires a GPU from NVIDIA because", "tokens": [733, 295, 18407, 11, 293, 294, 3124, 412, 264, 1623, 309, 534, 7029, 257, 18407, 490, 426, 3958, 6914, 570], "temperature": 0.0, "avg_logprob": -0.12271551132202148, "compression_ratio": 1.5523012552301256, "no_speech_prob": 1.321177387580974e-05}, {"id": 120, "seek": 53628, "start": 545.48, "end": 552.64, "text": " NVIDIA GPUs support a kind of programming called CUDA, which we will be learning about.", "tokens": [426, 3958, 6914, 18407, 82, 1406, 257, 733, 295, 9410, 1219, 29777, 7509, 11, 597, 321, 486, 312, 2539, 466, 13], "temperature": 0.0, "avg_logprob": -0.12271551132202148, "compression_ratio": 1.5523012552301256, "no_speech_prob": 1.321177387580974e-05}, {"id": 121, "seek": 53628, "start": 552.64, "end": 559.12, "text": " There are other GPUs that do support deep learning, but they're a bit of a pain, they're", "tokens": [821, 366, 661, 18407, 82, 300, 360, 1406, 2452, 2539, 11, 457, 436, 434, 257, 857, 295, 257, 1822, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.12271551132202148, "compression_ratio": 1.5523012552301256, "no_speech_prob": 1.321177387580974e-05}, {"id": 122, "seek": 53628, "start": 559.12, "end": 561.04, "text": " not very widely used.", "tokens": [406, 588, 13371, 1143, 13], "temperature": 0.0, "avg_logprob": -0.12271551132202148, "compression_ratio": 1.5523012552301256, "no_speech_prob": 1.321177387580974e-05}, {"id": 123, "seek": 53628, "start": 561.04, "end": 563.9399999999999, "text": " And so one of the things we're going to be doing is making sure that all of you guys", "tokens": [400, 370, 472, 295, 264, 721, 321, 434, 516, 281, 312, 884, 307, 1455, 988, 300, 439, 295, 291, 1074], "temperature": 0.0, "avg_logprob": -0.12271551132202148, "compression_ratio": 1.5523012552301256, "no_speech_prob": 1.321177387580974e-05}, {"id": 124, "seek": 56394, "start": 563.94, "end": 567.6600000000001, "text": " have access to an NVIDIA GPU.", "tokens": [362, 2105, 281, 364, 426, 3958, 6914, 18407, 13], "temperature": 0.0, "avg_logprob": -0.13333111888957475, "compression_ratio": 1.5914893617021277, "no_speech_prob": 2.4299139113281853e-05}, {"id": 125, "seek": 56394, "start": 567.6600000000001, "end": 574.84, "text": " The good news is that in the last month, I think, Amazon has made available good quality", "tokens": [440, 665, 2583, 307, 300, 294, 264, 1036, 1618, 11, 286, 519, 11, 6795, 575, 1027, 2435, 665, 3125], "temperature": 0.0, "avg_logprob": -0.13333111888957475, "compression_ratio": 1.5914893617021277, "no_speech_prob": 2.4299139113281853e-05}, {"id": 126, "seek": 56394, "start": 574.84, "end": 578.98, "text": " NVIDIA GPUs for everybody for the first time.", "tokens": [426, 3958, 6914, 18407, 82, 337, 2201, 337, 264, 700, 565, 13], "temperature": 0.0, "avg_logprob": -0.13333111888957475, "compression_ratio": 1.5914893617021277, "no_speech_prob": 2.4299139113281853e-05}, {"id": 127, "seek": 56394, "start": 578.98, "end": 583.8800000000001, "text": " They call them very excitingly their P2 instances.", "tokens": [814, 818, 552, 588, 4670, 356, 641, 430, 17, 14519, 13], "temperature": 0.0, "avg_logprob": -0.13333111888957475, "compression_ratio": 1.5914893617021277, "no_speech_prob": 2.4299139113281853e-05}, {"id": 128, "seek": 56394, "start": 583.8800000000001, "end": 588.5200000000001, "text": " So I've spent quite a bit of the last month making sure that it's really easy to use these", "tokens": [407, 286, 600, 4418, 1596, 257, 857, 295, 264, 1036, 1618, 1455, 988, 300, 309, 311, 534, 1858, 281, 764, 613], "temperature": 0.0, "avg_logprob": -0.13333111888957475, "compression_ratio": 1.5914893617021277, "no_speech_prob": 2.4299139113281853e-05}, {"id": 129, "seek": 56394, "start": 588.5200000000001, "end": 590.32, "text": " new P2 instances.", "tokens": [777, 430, 17, 14519, 13], "temperature": 0.0, "avg_logprob": -0.13333111888957475, "compression_ratio": 1.5914893617021277, "no_speech_prob": 2.4299139113281853e-05}, {"id": 130, "seek": 56394, "start": 590.32, "end": 593.0, "text": " I've given you all access to a script to do that.", "tokens": [286, 600, 2212, 291, 439, 2105, 281, 257, 5755, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.13333111888957475, "compression_ratio": 1.5914893617021277, "no_speech_prob": 2.4299139113281853e-05}, {"id": 131, "seek": 59300, "start": 593.0, "end": 598.96, "text": " Unfortunately, we're still at the point where they don't trust people to use these correctly,", "tokens": [8590, 11, 321, 434, 920, 412, 264, 935, 689, 436, 500, 380, 3361, 561, 281, 764, 613, 8944, 11], "temperature": 0.0, "avg_logprob": -0.15269932504427636, "compression_ratio": 1.460122699386503, "no_speech_prob": 4.98554072692059e-05}, {"id": 132, "seek": 59300, "start": 598.96, "end": 611.4, "text": " so you have to ask permission to use these P2 instances.", "tokens": [370, 291, 362, 281, 1029, 11226, 281, 764, 613, 430, 17, 14519, 13], "temperature": 0.0, "avg_logprob": -0.15269932504427636, "compression_ratio": 1.460122699386503, "no_speech_prob": 4.98554072692059e-05}, {"id": 133, "seek": 59300, "start": 611.4, "end": 619.28, "text": " The Data Institute folks, for anybody who does not yet have an AWS P2 instance or their", "tokens": [440, 11888, 9446, 4024, 11, 337, 4472, 567, 775, 406, 1939, 362, 364, 17650, 430, 17, 5197, 420, 641], "temperature": 0.0, "avg_logprob": -0.15269932504427636, "compression_ratio": 1.460122699386503, "no_speech_prob": 4.98554072692059e-05}, {"id": 134, "seek": 61928, "start": 619.28, "end": 627.8399999999999, "text": " own GPU server, they are going to collect all of your AWS IDs and they have a contact", "tokens": [1065, 18407, 7154, 11, 436, 366, 516, 281, 2500, 439, 295, 428, 17650, 48212, 293, 436, 362, 257, 3385], "temperature": 0.0, "avg_logprob": -0.15456495501778342, "compression_ratio": 1.555023923444976, "no_speech_prob": 2.2251863356359536e-06}, {"id": 135, "seek": 61928, "start": 627.8399999999999, "end": 632.92, "text": " at Amazon who will go through and get them all approved.", "tokens": [412, 6795, 567, 486, 352, 807, 293, 483, 552, 439, 10826, 13], "temperature": 0.0, "avg_logprob": -0.15456495501778342, "compression_ratio": 1.555023923444976, "no_speech_prob": 2.2251863356359536e-06}, {"id": 136, "seek": 61928, "start": 632.92, "end": 635.98, "text": " They haven't made any promises, they've just said they'll do what they can.", "tokens": [814, 2378, 380, 1027, 604, 16403, 11, 436, 600, 445, 848, 436, 603, 360, 437, 436, 393, 13], "temperature": 0.0, "avg_logprob": -0.15456495501778342, "compression_ratio": 1.555023923444976, "no_speech_prob": 2.2251863356359536e-06}, {"id": 137, "seek": 61928, "start": 635.98, "end": 638.48, "text": " So they're aware of how urgent that is.", "tokens": [407, 436, 434, 3650, 295, 577, 19022, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.15456495501778342, "compression_ratio": 1.555023923444976, "no_speech_prob": 2.2251863356359536e-06}, {"id": 138, "seek": 61928, "start": 638.48, "end": 646.36, "text": " So if you email your AWS ID to Mindy, she will get that organized.", "tokens": [407, 498, 291, 3796, 428, 17650, 7348, 281, 13719, 88, 11, 750, 486, 483, 300, 9983, 13], "temperature": 0.0, "avg_logprob": -0.15456495501778342, "compression_ratio": 1.555023923444976, "no_speech_prob": 2.2251863356359536e-06}, {"id": 139, "seek": 64636, "start": 646.36, "end": 654.04, "text": " We'll come back and look at AWS in more detail very shortly.", "tokens": [492, 603, 808, 646, 293, 574, 412, 17650, 294, 544, 2607, 588, 13392, 13], "temperature": 0.0, "avg_logprob": -0.12287277894861558, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.2227729863952845e-06}, {"id": 140, "seek": 64636, "start": 654.04, "end": 662.24, "text": " The other thing that I've done is on the Wiki, I have added some information about getting", "tokens": [440, 661, 551, 300, 286, 600, 1096, 307, 322, 264, 35892, 11, 286, 362, 3869, 512, 1589, 466, 1242], "temperature": 0.0, "avg_logprob": -0.12287277894861558, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.2227729863952845e-06}, {"id": 141, "seek": 64636, "start": 662.24, "end": 668.6800000000001, "text": " set up.", "tokens": [992, 493, 13], "temperature": 0.0, "avg_logprob": -0.12287277894861558, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.2227729863952845e-06}, {"id": 142, "seek": 64636, "start": 668.6800000000001, "end": 672.6800000000001, "text": " There is actually quite an interesting option called OVH.", "tokens": [821, 307, 767, 1596, 364, 1880, 3614, 1219, 422, 53, 39, 13], "temperature": 0.0, "avg_logprob": -0.12287277894861558, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.2227729863952845e-06}, {"id": 143, "seek": 64636, "start": 672.6800000000001, "end": 675.88, "text": " I'm sure by the time this is a MOOC there's going to be a lot more, but this is the only", "tokens": [286, 478, 988, 538, 264, 565, 341, 307, 257, 49197, 34, 456, 311, 516, 281, 312, 257, 688, 544, 11, 457, 341, 307, 264, 787], "temperature": 0.0, "avg_logprob": -0.12287277894861558, "compression_ratio": 1.4641148325358853, "no_speech_prob": 4.2227729863952845e-06}, {"id": 144, "seek": 67588, "start": 675.88, "end": 681.88, "text": " company I've come across who will give you a buy-the-month server with decent deep learning", "tokens": [2237, 286, 600, 808, 2108, 567, 486, 976, 291, 257, 2256, 12, 3322, 12, 23534, 7154, 365, 8681, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.19704483449459076, "compression_ratio": 1.3414634146341464, "no_speech_prob": 1.1659319170576055e-05}, {"id": 145, "seek": 67588, "start": 681.88, "end": 684.96, "text": " graphics cards on it.", "tokens": [11837, 5632, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.19704483449459076, "compression_ratio": 1.3414634146341464, "no_speech_prob": 1.1659319170576055e-05}, {"id": 146, "seek": 67588, "start": 684.96, "end": 687.16, "text": " It's only $200.", "tokens": [467, 311, 787, 1848, 7629, 13], "temperature": 0.0, "avg_logprob": -0.19704483449459076, "compression_ratio": 1.3414634146341464, "no_speech_prob": 1.1659319170576055e-05}, {"id": 147, "seek": 67588, "start": 687.16, "end": 697.92, "text": " To give you a sense of how crazily cheap that is, if you go to their page for GPU servers,", "tokens": [1407, 976, 291, 257, 2020, 295, 577, 46348, 953, 7084, 300, 307, 11, 498, 291, 352, 281, 641, 3028, 337, 18407, 15909, 11], "temperature": 0.0, "avg_logprob": -0.19704483449459076, "compression_ratio": 1.3414634146341464, "no_speech_prob": 1.1659319170576055e-05}, {"id": 148, "seek": 69792, "start": 697.92, "end": 707.4, "text": " you'll see that this GTX 970 is $195 a month and then their next cheapest is $2,000 a month.", "tokens": [291, 603, 536, 300, 341, 17530, 55, 1722, 5867, 307, 1848, 3405, 20, 257, 1618, 293, 550, 641, 958, 29167, 307, 1848, 17, 11, 1360, 257, 1618, 13], "temperature": 0.0, "avg_logprob": -0.16069797322719911, "compression_ratio": 1.541237113402062, "no_speech_prob": 6.240741640795022e-06}, {"id": 149, "seek": 69792, "start": 707.4, "end": 715.88, "text": " It just so happens this GTX 970 is ridiculously cheap for how good it is at deep learning.", "tokens": [467, 445, 370, 2314, 341, 17530, 55, 1722, 5867, 307, 41358, 7084, 337, 577, 665, 309, 307, 412, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.16069797322719911, "compression_ratio": 1.541237113402062, "no_speech_prob": 6.240741640795022e-06}, {"id": 150, "seek": 69792, "start": 715.88, "end": 720.64, "text": " The reason is that deep learning uses single precision arithmetic, in other words it uses", "tokens": [440, 1778, 307, 300, 2452, 2539, 4960, 2167, 18356, 42973, 11, 294, 661, 2283, 309, 4960], "temperature": 0.0, "avg_logprob": -0.16069797322719911, "compression_ratio": 1.541237113402062, "no_speech_prob": 6.240741640795022e-06}, {"id": 151, "seek": 69792, "start": 720.64, "end": 722.9399999999999, "text": " less accurate arithmetic.", "tokens": [1570, 8559, 42973, 13], "temperature": 0.0, "avg_logprob": -0.16069797322719911, "compression_ratio": 1.541237113402062, "no_speech_prob": 6.240741640795022e-06}, {"id": 152, "seek": 72294, "start": 722.94, "end": 730.5600000000001, "text": " These higher-end cards are designed for fluid simulations and tracking nuclear bombs and", "tokens": [1981, 2946, 12, 521, 5632, 366, 4761, 337, 9113, 35138, 293, 11603, 8179, 19043, 293], "temperature": 0.0, "avg_logprob": -0.16312807868508733, "compression_ratio": 1.5754716981132075, "no_speech_prob": 3.0894582323526265e-06}, {"id": 153, "seek": 72294, "start": 730.5600000000001, "end": 733.7600000000001, "text": " stuff like that that require double precision arithmetic.", "tokens": [1507, 411, 300, 300, 3651, 3834, 18356, 42973, 13], "temperature": 0.0, "avg_logprob": -0.16312807868508733, "compression_ratio": 1.5754716981132075, "no_speech_prob": 3.0894582323526265e-06}, {"id": 154, "seek": 72294, "start": 733.7600000000001, "end": 740.8000000000001, "text": " So it turns out these GTX 970s are only good for two things, games and deep learning.", "tokens": [407, 309, 4523, 484, 613, 17530, 55, 1722, 5867, 82, 366, 787, 665, 337, 732, 721, 11, 2813, 293, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.16312807868508733, "compression_ratio": 1.5754716981132075, "no_speech_prob": 3.0894582323526265e-06}, {"id": 155, "seek": 72294, "start": 740.8000000000001, "end": 745.8000000000001, "text": " So the fact that you can get one of these things which has got two GTX 970s in is a", "tokens": [407, 264, 1186, 300, 291, 393, 483, 472, 295, 613, 721, 597, 575, 658, 732, 17530, 55, 1722, 5867, 82, 294, 307, 257], "temperature": 0.0, "avg_logprob": -0.16312807868508733, "compression_ratio": 1.5754716981132075, "no_speech_prob": 3.0894582323526265e-06}, {"id": 156, "seek": 72294, "start": 745.8000000000001, "end": 747.1800000000001, "text": " really good deal.", "tokens": [534, 665, 2028, 13], "temperature": 0.0, "avg_logprob": -0.16312807868508733, "compression_ratio": 1.5754716981132075, "no_speech_prob": 3.0894582323526265e-06}, {"id": 157, "seek": 74718, "start": 747.18, "end": 753.0799999999999, "text": " So one of the things you might consider doing in your team is maybe sharing the cost of", "tokens": [407, 472, 295, 264, 721, 291, 1062, 1949, 884, 294, 428, 1469, 307, 1310, 5414, 264, 2063, 295], "temperature": 0.0, "avg_logprob": -0.13265628525705048, "compression_ratio": 1.449438202247191, "no_speech_prob": 3.966908479924314e-06}, {"id": 158, "seek": 74718, "start": 753.0799999999999, "end": 754.0799999999999, "text": " one of these.", "tokens": [472, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.13265628525705048, "compression_ratio": 1.449438202247191, "no_speech_prob": 3.966908479924314e-06}, {"id": 159, "seek": 74718, "start": 754.0799999999999, "end": 761.68, "text": " $200 a month is pretty good compared to worrying about starting and stopping your $0.90 per", "tokens": [1848, 7629, 257, 1618, 307, 1238, 665, 5347, 281, 18788, 466, 2891, 293, 12767, 428, 1848, 15, 13, 7771, 680], "temperature": 0.0, "avg_logprob": -0.13265628525705048, "compression_ratio": 1.449438202247191, "no_speech_prob": 3.966908479924314e-06}, {"id": 160, "seek": 74718, "start": 761.68, "end": 771.1999999999999, "text": " hour AWS instance, particularly if AWS takes a while to say yes.", "tokens": [1773, 17650, 5197, 11, 4098, 498, 17650, 2516, 257, 1339, 281, 584, 2086, 13], "temperature": 0.0, "avg_logprob": -0.13265628525705048, "compression_ratio": 1.449438202247191, "no_speech_prob": 3.966908479924314e-06}, {"id": 161, "seek": 77120, "start": 771.2, "end": 779.32, "text": " So how many people here have used AWS before?", "tokens": [407, 577, 867, 561, 510, 362, 1143, 17650, 949, 30], "temperature": 0.0, "avg_logprob": -0.12766843471886977, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.1572661757527385e-06}, {"id": 162, "seek": 77120, "start": 779.32, "end": 785.2, "text": " So AWS is Amazon Web Services, I'm sure most of you, if not all of you, have heard of it.", "tokens": [407, 17650, 307, 6795, 9573, 12124, 11, 286, 478, 988, 881, 295, 291, 11, 498, 406, 439, 295, 291, 11, 362, 2198, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.12766843471886977, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.1572661757527385e-06}, {"id": 163, "seek": 77120, "start": 785.2, "end": 790.4200000000001, "text": " It's basically Amazon making their entire backend infrastructure available to everybody", "tokens": [467, 311, 1936, 6795, 1455, 641, 2302, 38087, 6896, 2435, 281, 2201], "temperature": 0.0, "avg_logprob": -0.12766843471886977, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.1572661757527385e-06}, {"id": 164, "seek": 77120, "start": 790.4200000000001, "end": 792.3000000000001, "text": " else to use.", "tokens": [1646, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.12766843471886977, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.1572661757527385e-06}, {"id": 165, "seek": 77120, "start": 792.3000000000001, "end": 796.76, "text": " So rather than calling it a server, you get something they call an instance.", "tokens": [407, 2831, 813, 5141, 309, 257, 7154, 11, 291, 483, 746, 436, 818, 364, 5197, 13], "temperature": 0.0, "avg_logprob": -0.12766843471886977, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.1572661757527385e-06}, {"id": 166, "seek": 77120, "start": 796.76, "end": 800.12, "text": " You can think of it as basically being the same thing, it's a little computer that you", "tokens": [509, 393, 519, 295, 309, 382, 1936, 885, 264, 912, 551, 11, 309, 311, 257, 707, 3820, 300, 291], "temperature": 0.0, "avg_logprob": -0.12766843471886977, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.1572661757527385e-06}, {"id": 167, "seek": 77120, "start": 800.12, "end": 801.12, "text": " get to use.", "tokens": [483, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.12766843471886977, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.1572661757527385e-06}, {"id": 168, "seek": 80112, "start": 801.12, "end": 803.16, "text": " In fact, not necessarily little.", "tokens": [682, 1186, 11, 406, 4725, 707, 13], "temperature": 0.0, "avg_logprob": -0.11609627544016078, "compression_ratio": 1.3689839572192513, "no_speech_prob": 1.4738764548383188e-05}, {"id": 169, "seek": 80112, "start": 803.16, "end": 810.4, "text": " Some of their instances cost $14 or $15 an hour and give you like 8 or 16 graphics cards", "tokens": [2188, 295, 641, 14519, 2063, 1848, 7271, 420, 1848, 5211, 364, 1773, 293, 976, 291, 411, 1649, 420, 3165, 11837, 5632], "temperature": 0.0, "avg_logprob": -0.11609627544016078, "compression_ratio": 1.3689839572192513, "no_speech_prob": 1.4738764548383188e-05}, {"id": 170, "seek": 80112, "start": 810.4, "end": 814.0, "text": " and dozens of CPUs and hundreds of gigabytes of RAM.", "tokens": [293, 18431, 295, 13199, 82, 293, 6779, 295, 42741, 295, 14561, 13], "temperature": 0.0, "avg_logprob": -0.11609627544016078, "compression_ratio": 1.3689839572192513, "no_speech_prob": 1.4738764548383188e-05}, {"id": 171, "seek": 80112, "start": 814.0, "end": 818.8, "text": " The cool thing about AWS is that you can do a lot of work on their free instance.", "tokens": [440, 1627, 551, 466, 17650, 307, 300, 291, 393, 360, 257, 688, 295, 589, 322, 641, 1737, 5197, 13], "temperature": 0.0, "avg_logprob": -0.11609627544016078, "compression_ratio": 1.3689839572192513, "no_speech_prob": 1.4738764548383188e-05}, {"id": 172, "seek": 81880, "start": 818.8, "end": 837.04, "text": " You can get a free instance called t2.micro, and you can get all the things set up and", "tokens": [509, 393, 483, 257, 1737, 5197, 1219, 256, 17, 13, 13195, 340, 11, 293, 291, 393, 483, 439, 264, 721, 992, 493, 293], "temperature": 0.0, "avg_logprob": -0.2182744721234855, "compression_ratio": 1.5588235294117647, "no_speech_prob": 5.01469412483857e-06}, {"id": 173, "seek": 81880, "start": 837.04, "end": 839.92, "text": " working on a really small dataset.", "tokens": [1364, 322, 257, 534, 1359, 28872, 13], "temperature": 0.0, "avg_logprob": -0.2182744721234855, "compression_ratio": 1.5588235294117647, "no_speech_prob": 5.01469412483857e-06}, {"id": 174, "seek": 81880, "start": 839.92, "end": 846.16, "text": " And then you can switch it across if you want to run it on a big dataset, switch it across", "tokens": [400, 550, 291, 393, 3679, 309, 2108, 498, 291, 528, 281, 1190, 309, 322, 257, 955, 28872, 11, 3679, 309, 2108], "temperature": 0.0, "avg_logprob": -0.2182744721234855, "compression_ratio": 1.5588235294117647, "no_speech_prob": 5.01469412483857e-06}, {"id": 175, "seek": 84616, "start": 846.16, "end": 851.0799999999999, "text": " to one of these expensive things and have it run and finished within an hour or two.", "tokens": [281, 472, 295, 613, 5124, 721, 293, 362, 309, 1190, 293, 4335, 1951, 364, 1773, 420, 732, 13], "temperature": 0.0, "avg_logprob": -0.1506673183637796, "compression_ratio": 1.560483870967742, "no_speech_prob": 1.670097844908014e-05}, {"id": 176, "seek": 84616, "start": 851.0799999999999, "end": 857.48, "text": " So that's one of the things I really like about AWS.", "tokens": [407, 300, 311, 472, 295, 264, 721, 286, 534, 411, 466, 17650, 13], "temperature": 0.0, "avg_logprob": -0.1506673183637796, "compression_ratio": 1.560483870967742, "no_speech_prob": 1.670097844908014e-05}, {"id": 177, "seek": 84616, "start": 857.48, "end": 861.56, "text": " Microsoft also have something a lot like AWS called Azure.", "tokens": [8116, 611, 362, 746, 257, 688, 411, 17650, 1219, 11969, 13], "temperature": 0.0, "avg_logprob": -0.1506673183637796, "compression_ratio": 1.560483870967742, "no_speech_prob": 1.670097844908014e-05}, {"id": 178, "seek": 84616, "start": 861.56, "end": 865.68, "text": " Unfortunately their GPU instances are not yet publicly available.", "tokens": [8590, 641, 18407, 14519, 366, 406, 1939, 14843, 2435, 13], "temperature": 0.0, "avg_logprob": -0.1506673183637796, "compression_ratio": 1.560483870967742, "no_speech_prob": 1.670097844908014e-05}, {"id": 179, "seek": 84616, "start": 865.68, "end": 869.04, "text": " I've reached out to Microsoft to see if we can get access to those as well, and I'll", "tokens": [286, 600, 6488, 484, 281, 8116, 281, 536, 498, 321, 393, 483, 2105, 281, 729, 382, 731, 11, 293, 286, 603], "temperature": 0.0, "avg_logprob": -0.1506673183637796, "compression_ratio": 1.560483870967742, "no_speech_prob": 1.670097844908014e-05}, {"id": 180, "seek": 84616, "start": 869.04, "end": 873.16, "text": " let you know if we hear back from them.", "tokens": [718, 291, 458, 498, 321, 1568, 646, 490, 552, 13], "temperature": 0.0, "avg_logprob": -0.1506673183637796, "compression_ratio": 1.560483870967742, "no_speech_prob": 1.670097844908014e-05}, {"id": 181, "seek": 87316, "start": 873.16, "end": 878.68, "text": " So one of the things that Rachel's done today is to start jotting down some of the common", "tokens": [407, 472, 295, 264, 721, 300, 14246, 311, 1096, 965, 307, 281, 722, 27873, 783, 760, 512, 295, 264, 2689], "temperature": 0.0, "avg_logprob": -0.1424660331324527, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.834216163842939e-05}, {"id": 182, "seek": 87316, "start": 878.68, "end": 883.92, "text": " problems that people have found with their AWS installs.", "tokens": [2740, 300, 561, 362, 1352, 365, 641, 17650, 3625, 82, 13], "temperature": 0.0, "avg_logprob": -0.1424660331324527, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.834216163842939e-05}, {"id": 183, "seek": 87316, "start": 883.92, "end": 890.64, "text": " Getting AWS set up is a bit of a pain, so we've created a script that basically will", "tokens": [13674, 17650, 992, 493, 307, 257, 857, 295, 257, 1822, 11, 370, 321, 600, 2942, 257, 5755, 300, 1936, 486], "temperature": 0.0, "avg_logprob": -0.1424660331324527, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.834216163842939e-05}, {"id": 184, "seek": 87316, "start": 890.64, "end": 892.28, "text": " do everything for you.", "tokens": [360, 1203, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1424660331324527, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.834216163842939e-05}, {"id": 185, "seek": 87316, "start": 892.28, "end": 896.36, "text": " But the nice thing is that that script is very easy for you to have a look at and see", "tokens": [583, 264, 1481, 551, 307, 300, 300, 5755, 307, 588, 1858, 337, 291, 281, 362, 257, 574, 412, 293, 536], "temperature": 0.0, "avg_logprob": -0.1424660331324527, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.834216163842939e-05}, {"id": 186, "seek": 87316, "start": 896.36, "end": 898.22, "text": " what's going on.", "tokens": [437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.1424660331324527, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.834216163842939e-05}, {"id": 187, "seek": 89822, "start": 898.22, "end": 907.08, "text": " So over time you can get a sense of how AWS works.", "tokens": [407, 670, 565, 291, 393, 483, 257, 2020, 295, 577, 17650, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1425909423828125, "compression_ratio": 1.4480874316939891, "no_speech_prob": 3.7266011077008443e-06}, {"id": 188, "seek": 89822, "start": 907.08, "end": 912.64, "text": " Behind the scenes, AWS is using their command line interface, or CLI, which we've given", "tokens": [20475, 264, 8026, 11, 17650, 307, 1228, 641, 5622, 1622, 9226, 11, 420, 12855, 40, 11, 597, 321, 600, 2212], "temperature": 0.0, "avg_logprob": -0.1425909423828125, "compression_ratio": 1.4480874316939891, "no_speech_prob": 3.7266011077008443e-06}, {"id": 189, "seek": 89822, "start": 912.64, "end": 916.7, "text": " you instructions on how to install.", "tokens": [291, 9415, 322, 577, 281, 3625, 13], "temperature": 0.0, "avg_logprob": -0.1425909423828125, "compression_ratio": 1.4480874316939891, "no_speech_prob": 3.7266011077008443e-06}, {"id": 190, "seek": 89822, "start": 916.7, "end": 926.72, "text": " As well as using the CLI, you can also go to console.aws.amazon.com and use this graphical", "tokens": [1018, 731, 382, 1228, 264, 12855, 40, 11, 291, 393, 611, 352, 281, 11076, 13, 12282, 13, 335, 6317, 13, 1112, 293, 764, 341, 35942], "temperature": 0.0, "avg_logprob": -0.1425909423828125, "compression_ratio": 1.4480874316939891, "no_speech_prob": 3.7266011077008443e-06}, {"id": 191, "seek": 92672, "start": 926.72, "end": 928.6800000000001, "text": " interface.", "tokens": [9226, 13], "temperature": 0.0, "avg_logprob": -0.16239886588238656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.2218760275573004e-05}, {"id": 192, "seek": 92672, "start": 928.6800000000001, "end": 933.5600000000001, "text": " In general, I try to avoid using this graphical interface because everything takes so much", "tokens": [682, 2674, 11, 286, 853, 281, 5042, 1228, 341, 35942, 9226, 570, 1203, 2516, 370, 709], "temperature": 0.0, "avg_logprob": -0.16239886588238656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.2218760275573004e-05}, {"id": 193, "seek": 92672, "start": 933.5600000000001, "end": 938.0, "text": " longer and it's so hard to get things to work repeatedly.", "tokens": [2854, 293, 309, 311, 370, 1152, 281, 483, 721, 281, 589, 18227, 13], "temperature": 0.0, "avg_logprob": -0.16239886588238656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.2218760275573004e-05}, {"id": 194, "seek": 92672, "start": 938.0, "end": 943.32, "text": " But it can be nice to look around and see how everything's put together.", "tokens": [583, 309, 393, 312, 1481, 281, 574, 926, 293, 536, 577, 1203, 311, 829, 1214, 13], "temperature": 0.0, "avg_logprob": -0.16239886588238656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.2218760275573004e-05}, {"id": 195, "seek": 92672, "start": 943.32, "end": 947.52, "text": " And again, we're going to come back and see a lot more about how to use the graphical", "tokens": [400, 797, 11, 321, 434, 516, 281, 808, 646, 293, 536, 257, 688, 544, 466, 577, 281, 764, 264, 35942], "temperature": 0.0, "avg_logprob": -0.16239886588238656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.2218760275573004e-05}, {"id": 196, "seek": 92672, "start": 947.52, "end": 956.28, "text": " interface here, as well as how to create and use scripts.", "tokens": [9226, 510, 11, 382, 731, 382, 577, 281, 1884, 293, 764, 23294, 13], "temperature": 0.0, "avg_logprob": -0.16239886588238656, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.2218760275573004e-05}, {"id": 197, "seek": 95628, "start": 956.28, "end": 964.28, "text": " So these are some of the pieces that we want to show you.", "tokens": [407, 613, 366, 512, 295, 264, 3755, 300, 321, 528, 281, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.1283469969226468, "compression_ratio": 1.7565217391304349, "no_speech_prob": 2.429954838589765e-05}, {"id": 198, "seek": 95628, "start": 964.28, "end": 967.6999999999999, "text": " I wanted to talk a bit more before we go into more detail about some of the interesting", "tokens": [286, 1415, 281, 751, 257, 857, 544, 949, 321, 352, 666, 544, 2607, 466, 512, 295, 264, 1880], "temperature": 0.0, "avg_logprob": -0.1283469969226468, "compression_ratio": 1.7565217391304349, "no_speech_prob": 2.429954838589765e-05}, {"id": 199, "seek": 95628, "start": 967.6999999999999, "end": 970.9399999999999, "text": " things that we've seen happening in deep learning recently.", "tokens": [721, 300, 321, 600, 1612, 2737, 294, 2452, 2539, 3938, 13], "temperature": 0.0, "avg_logprob": -0.1283469969226468, "compression_ratio": 1.7565217391304349, "no_speech_prob": 2.429954838589765e-05}, {"id": 200, "seek": 95628, "start": 970.9399999999999, "end": 977.4, "text": " Perhaps the thing that I found most fascinating recently was when one of the leading folks", "tokens": [10517, 264, 551, 300, 286, 1352, 881, 10343, 3938, 390, 562, 472, 295, 264, 5775, 4024], "temperature": 0.0, "avg_logprob": -0.1283469969226468, "compression_ratio": 1.7565217391304349, "no_speech_prob": 2.429954838589765e-05}, {"id": 201, "seek": 95628, "start": 977.4, "end": 984.02, "text": " at Google Brain presented this at a conference at Stanford which showed the use of deep learning", "tokens": [412, 3329, 29783, 8212, 341, 412, 257, 7586, 412, 20374, 597, 4712, 264, 764, 295, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.1283469969226468, "compression_ratio": 1.7565217391304349, "no_speech_prob": 2.429954838589765e-05}, {"id": 202, "seek": 95628, "start": 984.02, "end": 986.02, "text": " at Google.", "tokens": [412, 3329, 13], "temperature": 0.0, "avg_logprob": -0.1283469969226468, "compression_ratio": 1.7565217391304349, "no_speech_prob": 2.429954838589765e-05}, {"id": 203, "seek": 98602, "start": 986.02, "end": 993.68, "text": " And you can see this is just 2012 to today, or maybe 2 months ago.", "tokens": [400, 291, 393, 536, 341, 307, 445, 9125, 281, 965, 11, 420, 1310, 568, 2493, 2057, 13], "temperature": 0.0, "avg_logprob": -0.14817859386575633, "compression_ratio": 1.5341880341880343, "no_speech_prob": 3.16919649776537e-05}, {"id": 204, "seek": 98602, "start": 993.68, "end": 998.52, "text": " It's gone from nothing to over 2500 projects.", "tokens": [467, 311, 2780, 490, 1825, 281, 670, 41171, 4455, 13], "temperature": 0.0, "avg_logprob": -0.14817859386575633, "compression_ratio": 1.5341880341880343, "no_speech_prob": 3.16919649776537e-05}, {"id": 205, "seek": 98602, "start": 998.52, "end": 1002.88, "text": " Now the reason I find this interesting is because this is what's going to happen to", "tokens": [823, 264, 1778, 286, 915, 341, 1880, 307, 570, 341, 307, 437, 311, 516, 281, 1051, 281], "temperature": 0.0, "avg_logprob": -0.14817859386575633, "compression_ratio": 1.5341880341880343, "no_speech_prob": 3.16919649776537e-05}, {"id": 206, "seek": 98602, "start": 1002.88, "end": 1009.36, "text": " every organization and every industry over the next few months and few years.", "tokens": [633, 4475, 293, 633, 3518, 670, 264, 958, 1326, 2493, 293, 1326, 924, 13], "temperature": 0.0, "avg_logprob": -0.14817859386575633, "compression_ratio": 1.5341880341880343, "no_speech_prob": 3.16919649776537e-05}, {"id": 207, "seek": 98602, "start": 1009.36, "end": 1014.46, "text": " So they've kind of described how at Google it's getting used pretty much everywhere.", "tokens": [407, 436, 600, 733, 295, 7619, 577, 412, 3329, 309, 311, 1242, 1143, 1238, 709, 5315, 13], "temperature": 0.0, "avg_logprob": -0.14817859386575633, "compression_ratio": 1.5341880341880343, "no_speech_prob": 3.16919649776537e-05}, {"id": 208, "seek": 101446, "start": 1014.46, "end": 1017.72, "text": " And you can imagine probably if they redid this now today, 2 months later, it's probably", "tokens": [400, 291, 393, 3811, 1391, 498, 436, 2182, 327, 341, 586, 965, 11, 568, 2493, 1780, 11, 309, 311, 1391], "temperature": 0.0, "avg_logprob": -0.20147615008884007, "compression_ratio": 1.6771653543307086, "no_speech_prob": 5.255289124761475e-06}, {"id": 209, "seek": 101446, "start": 1017.72, "end": 1019.98, "text": " going to be somewhere up here.", "tokens": [516, 281, 312, 4079, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.20147615008884007, "compression_ratio": 1.6771653543307086, "no_speech_prob": 5.255289124761475e-06}, {"id": 210, "seek": 101446, "start": 1019.98, "end": 1026.14, "text": " So we kind of felt like it would be great to help kickstart lots of other organizations", "tokens": [407, 321, 733, 295, 2762, 411, 309, 576, 312, 869, 281, 854, 4437, 24419, 3195, 295, 661, 6150], "temperature": 0.0, "avg_logprob": -0.20147615008884007, "compression_ratio": 1.6771653543307086, "no_speech_prob": 5.255289124761475e-06}, {"id": 211, "seek": 101446, "start": 1026.14, "end": 1029.2, "text": " to start going up this ramp.", "tokens": [281, 722, 516, 493, 341, 12428, 13], "temperature": 0.0, "avg_logprob": -0.20147615008884007, "compression_ratio": 1.6771653543307086, "no_speech_prob": 5.255289124761475e-06}, {"id": 212, "seek": 101446, "start": 1029.2, "end": 1032.8400000000001, "text": " That's another kind of reason we're doing this.", "tokens": [663, 311, 1071, 733, 295, 1778, 321, 434, 884, 341, 13], "temperature": 0.0, "avg_logprob": -0.20147615008884007, "compression_ratio": 1.6771653543307086, "no_speech_prob": 5.255289124761475e-06}, {"id": 213, "seek": 101446, "start": 1032.8400000000001, "end": 1035.32, "text": " I really like looking at applications.", "tokens": [286, 534, 411, 1237, 412, 5821, 13], "temperature": 0.0, "avg_logprob": -0.20147615008884007, "compression_ratio": 1.6771653543307086, "no_speech_prob": 5.255289124761475e-06}, {"id": 214, "seek": 101446, "start": 1035.32, "end": 1042.08, "text": " And we started seeing some examples of these kind of deep learning amateurs applications,", "tokens": [400, 321, 1409, 2577, 512, 5110, 295, 613, 733, 295, 2452, 2539, 669, 25929, 5821, 11], "temperature": 0.0, "avg_logprob": -0.20147615008884007, "compression_ratio": 1.6771653543307086, "no_speech_prob": 5.255289124761475e-06}, {"id": 215, "seek": 101446, "start": 1042.08, "end": 1043.68, "text": " if you like.", "tokens": [498, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.20147615008884007, "compression_ratio": 1.6771653543307086, "no_speech_prob": 5.255289124761475e-06}, {"id": 216, "seek": 104368, "start": 1043.68, "end": 1046.3600000000001, "text": " This is an example of it.", "tokens": [639, 307, 364, 1365, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.16626739501953125, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.726453431023401e-06}, {"id": 217, "seek": 104368, "start": 1046.3600000000001, "end": 1050.88, "text": " What these guys did is they're not machine learning or deep learning experts.", "tokens": [708, 613, 1074, 630, 307, 436, 434, 406, 3479, 2539, 420, 2452, 2539, 8572, 13], "temperature": 0.0, "avg_logprob": -0.16626739501953125, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.726453431023401e-06}, {"id": 218, "seek": 104368, "start": 1050.88, "end": 1053.88, "text": " They downloaded a copy of Cafe.", "tokens": [814, 21748, 257, 5055, 295, 35864, 13], "temperature": 0.0, "avg_logprob": -0.16626739501953125, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.726453431023401e-06}, {"id": 219, "seek": 104368, "start": 1053.88, "end": 1055.92, "text": " They ran a pre-existing model.", "tokens": [814, 5872, 257, 659, 12, 36447, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16626739501953125, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.726453431023401e-06}, {"id": 220, "seek": 104368, "start": 1055.92, "end": 1057.52, "text": " This is what we're going to learn to do today.", "tokens": [639, 307, 437, 321, 434, 516, 281, 1466, 281, 360, 965, 13], "temperature": 0.0, "avg_logprob": -0.16626739501953125, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.726453431023401e-06}, {"id": 221, "seek": 104368, "start": 1057.52, "end": 1063.4, "text": " So run a pre-existing model and use the features from that model to do something interesting.", "tokens": [407, 1190, 257, 659, 12, 36447, 2316, 293, 764, 264, 4122, 490, 300, 2316, 281, 360, 746, 1880, 13], "temperature": 0.0, "avg_logprob": -0.16626739501953125, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.726453431023401e-06}, {"id": 222, "seek": 104368, "start": 1063.4, "end": 1066.52, "text": " In their case, the thing they decided to do that was interesting was to take data that", "tokens": [682, 641, 1389, 11, 264, 551, 436, 3047, 281, 360, 300, 390, 1880, 390, 281, 747, 1412, 300], "temperature": 0.0, "avg_logprob": -0.16626739501953125, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.726453431023401e-06}, {"id": 223, "seek": 104368, "start": 1066.52, "end": 1071.04, "text": " they already had, because they're skin lesion people, and analyze skin lesions.", "tokens": [436, 1217, 632, 11, 570, 436, 434, 3178, 1512, 313, 561, 11, 293, 12477, 3178, 1512, 626, 13], "temperature": 0.0, "avg_logprob": -0.16626739501953125, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.726453431023401e-06}, {"id": 224, "seek": 107104, "start": 1071.04, "end": 1075.36, "text": " These are the different kinds of skin lesions that you can have.", "tokens": [1981, 366, 264, 819, 3685, 295, 3178, 1512, 626, 300, 291, 393, 362, 13], "temperature": 0.0, "avg_logprob": -0.14673689096280845, "compression_ratio": 1.552, "no_speech_prob": 5.955088909104234e-06}, {"id": 225, "seek": 107104, "start": 1075.36, "end": 1081.96, "text": " They found, for example, that the previous best for finding this particular kind of skin", "tokens": [814, 1352, 11, 337, 1365, 11, 300, 264, 3894, 1151, 337, 5006, 341, 1729, 733, 295, 3178], "temperature": 0.0, "avg_logprob": -0.14673689096280845, "compression_ratio": 1.552, "no_speech_prob": 5.955088909104234e-06}, {"id": 226, "seek": 107104, "start": 1081.96, "end": 1084.8, "text": " lesion was 15.6% accuracy.", "tokens": [1512, 313, 390, 2119, 13, 21, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.14673689096280845, "compression_ratio": 1.552, "no_speech_prob": 5.955088909104234e-06}, {"id": 227, "seek": 107104, "start": 1084.8, "end": 1090.3999999999999, "text": " When they did this off-the-shelf Cafe pre-existing model with a simple linear thing on top, they", "tokens": [1133, 436, 630, 341, 766, 12, 3322, 12, 46626, 35864, 659, 12, 36447, 2316, 365, 257, 2199, 8213, 551, 322, 1192, 11, 436], "temperature": 0.0, "avg_logprob": -0.14673689096280845, "compression_ratio": 1.552, "no_speech_prob": 5.955088909104234e-06}, {"id": 228, "seek": 107104, "start": 1090.3999999999999, "end": 1094.52, "text": " quadrupled it to 60%.", "tokens": [10787, 894, 15551, 309, 281, 4060, 6856], "temperature": 0.0, "avg_logprob": -0.14673689096280845, "compression_ratio": 1.552, "no_speech_prob": 5.955088909104234e-06}, {"id": 229, "seek": 107104, "start": 1094.52, "end": 1098.8, "text": " Often when you take a deep learning model and use the very simple techniques we'll learn", "tokens": [20043, 562, 291, 747, 257, 2452, 2539, 2316, 293, 764, 264, 588, 2199, 7512, 321, 603, 1466], "temperature": 0.0, "avg_logprob": -0.14673689096280845, "compression_ratio": 1.552, "no_speech_prob": 5.955088909104234e-06}, {"id": 230, "seek": 109880, "start": 1098.8, "end": 1106.76, "text": " today, you can get extraordinarily upticks compared to non-deep learning approaches.", "tokens": [965, 11, 291, 393, 483, 34557, 493, 83, 7663, 5347, 281, 2107, 12, 38422, 2539, 11587, 13], "temperature": 0.0, "avg_logprob": -0.18755386402080584, "compression_ratio": 1.5495495495495495, "no_speech_prob": 7.766827366140205e-06}, {"id": 231, "seek": 109880, "start": 1106.76, "end": 1110.68, "text": " Another example of that was looking at plant diseases.", "tokens": [3996, 1365, 295, 300, 390, 1237, 412, 3709, 11044, 13], "temperature": 0.0, "avg_logprob": -0.18755386402080584, "compression_ratio": 1.5495495495495495, "no_speech_prob": 7.766827366140205e-06}, {"id": 232, "seek": 109880, "start": 1110.68, "end": 1116.56, "text": " There's been at least two groups that have done this in the last few months.", "tokens": [821, 311, 668, 412, 1935, 732, 3935, 300, 362, 1096, 341, 294, 264, 1036, 1326, 2493, 13], "temperature": 0.0, "avg_logprob": -0.18755386402080584, "compression_ratio": 1.5495495495495495, "no_speech_prob": 7.766827366140205e-06}, {"id": 233, "seek": 109880, "start": 1116.56, "end": 1124.1599999999999, "text": " Very successful results from people who are not deep learning or machine learning experts.", "tokens": [4372, 4406, 3542, 490, 561, 567, 366, 406, 2452, 2539, 420, 3479, 2539, 8572, 13], "temperature": 0.0, "avg_logprob": -0.18755386402080584, "compression_ratio": 1.5495495495495495, "no_speech_prob": 7.766827366140205e-06}, {"id": 234, "seek": 109880, "start": 1124.1599999999999, "end": 1127.08, "text": " Similar results in radio modulation.", "tokens": [10905, 3542, 294, 6477, 42288, 13], "temperature": 0.0, "avg_logprob": -0.18755386402080584, "compression_ratio": 1.5495495495495495, "no_speech_prob": 7.766827366140205e-06}, {"id": 235, "seek": 112708, "start": 1127.08, "end": 1133.9199999999998, "text": " So these folks who are electing computer engineering people found that they could double the effective", "tokens": [407, 613, 4024, 567, 366, 2185, 278, 3820, 7043, 561, 1352, 300, 436, 727, 3834, 264, 4942], "temperature": 0.0, "avg_logprob": -0.23857621142738744, "compression_ratio": 1.5533980582524272, "no_speech_prob": 5.093656909593847e-06}, {"id": 236, "seek": 112708, "start": 1133.9199999999998, "end": 1138.12, "text": " coverage area of phone networks and stuff like that.", "tokens": [9645, 1859, 295, 2593, 9590, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.23857621142738744, "compression_ratio": 1.5533980582524272, "no_speech_prob": 5.093656909593847e-06}, {"id": 237, "seek": 112708, "start": 1138.12, "end": 1140.08, "text": " This is a massive result.", "tokens": [639, 307, 257, 5994, 1874, 13], "temperature": 0.0, "avg_logprob": -0.23857621142738744, "compression_ratio": 1.5533980582524272, "no_speech_prob": 5.093656909593847e-06}, {"id": 238, "seek": 112708, "start": 1140.08, "end": 1144.08, "text": " And again, they used very simple approaches.", "tokens": [400, 797, 11, 436, 1143, 588, 2199, 11587, 13], "temperature": 0.0, "avg_logprob": -0.23857621142738744, "compression_ratio": 1.5533980582524272, "no_speech_prob": 5.093656909593847e-06}, {"id": 239, "seek": 112708, "start": 1144.08, "end": 1148.52, "text": " It's being used in fashion.", "tokens": [467, 311, 885, 1143, 294, 6700, 13], "temperature": 0.0, "avg_logprob": -0.23857621142738744, "compression_ratio": 1.5533980582524272, "no_speech_prob": 5.093656909593847e-06}, {"id": 240, "seek": 112708, "start": 1148.52, "end": 1154.32, "text": " It's being used to diagnose heart disease by hedge fund analysts.", "tokens": [467, 311, 885, 1143, 281, 36238, 1917, 4752, 538, 25304, 2374, 31388, 13], "temperature": 0.0, "avg_logprob": -0.23857621142738744, "compression_ratio": 1.5533980582524272, "no_speech_prob": 5.093656909593847e-06}, {"id": 241, "seek": 115432, "start": 1154.32, "end": 1161.36, "text": " So there's a particular post which I found really inspiring actually in trying to put", "tokens": [407, 456, 311, 257, 1729, 2183, 597, 286, 1352, 534, 15883, 767, 294, 1382, 281, 829], "temperature": 0.0, "avg_logprob": -0.14476045269832433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.0129536349268164e-05}, {"id": 242, "seek": 115432, "start": 1161.36, "end": 1165.84, "text": " this together, which is that Keras, which is the main library we'll be using, the author", "tokens": [341, 1214, 11, 597, 307, 300, 591, 6985, 11, 597, 307, 264, 2135, 6405, 321, 603, 312, 1228, 11, 264, 3793], "temperature": 0.0, "avg_logprob": -0.14476045269832433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.0129536349268164e-05}, {"id": 243, "seek": 115432, "start": 1165.84, "end": 1171.2, "text": " of that put together this post showing how to build powerful models using very little", "tokens": [295, 300, 829, 1214, 341, 2183, 4099, 577, 281, 1322, 4005, 5245, 1228, 588, 707], "temperature": 0.0, "avg_logprob": -0.14476045269832433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.0129536349268164e-05}, {"id": 244, "seek": 115432, "start": 1171.2, "end": 1172.2, "text": " data.", "tokens": [1412, 13], "temperature": 0.0, "avg_logprob": -0.14476045269832433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.0129536349268164e-05}, {"id": 245, "seek": 115432, "start": 1172.2, "end": 1177.1599999999999, "text": " I really just wanted to give a shout-out to this and say this work that Francois has been", "tokens": [286, 534, 445, 1415, 281, 976, 257, 8043, 12, 346, 281, 341, 293, 584, 341, 589, 300, 34695, 271, 575, 668], "temperature": 0.0, "avg_logprob": -0.14476045269832433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.0129536349268164e-05}, {"id": 246, "seek": 115432, "start": 1177.1599999999999, "end": 1180.96, "text": " doing has been very important in a lot of the stuff that we're going to be learning", "tokens": [884, 575, 668, 588, 1021, 294, 257, 688, 295, 264, 1507, 300, 321, 434, 516, 281, 312, 2539], "temperature": 0.0, "avg_logprob": -0.14476045269832433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.0129536349268164e-05}, {"id": 247, "seek": 118096, "start": 1180.96, "end": 1185.44, "text": " over the next few classes.", "tokens": [670, 264, 958, 1326, 5359, 13], "temperature": 0.0, "avg_logprob": -0.14584706379817083, "compression_ratio": 1.532967032967033, "no_speech_prob": 4.468953193281777e-05}, {"id": 248, "seek": 118096, "start": 1185.44, "end": 1192.3600000000001, "text": " The basic environment we're going to be working in most of the time is the IPython notebook", "tokens": [440, 3875, 2823, 321, 434, 516, 281, 312, 1364, 294, 881, 295, 264, 565, 307, 264, 8671, 88, 11943, 21060], "temperature": 0.0, "avg_logprob": -0.14584706379817083, "compression_ratio": 1.532967032967033, "no_speech_prob": 4.468953193281777e-05}, {"id": 249, "seek": 118096, "start": 1192.3600000000001, "end": 1195.44, "text": " or the Jupyter notebook.", "tokens": [420, 264, 22125, 88, 391, 21060, 13], "temperature": 0.0, "avg_logprob": -0.14584706379817083, "compression_ratio": 1.532967032967033, "no_speech_prob": 4.468953193281777e-05}, {"id": 250, "seek": 118096, "start": 1195.44, "end": 1198.1000000000001, "text": " So let me just kind of give you a sense of what's going on here.", "tokens": [407, 718, 385, 445, 733, 295, 976, 291, 257, 2020, 295, 437, 311, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.14584706379817083, "compression_ratio": 1.532967032967033, "no_speech_prob": 4.468953193281777e-05}, {"id": 251, "seek": 118096, "start": 1198.1000000000001, "end": 1202.4, "text": " When you have a Jupyter notebook open, you will see something which...", "tokens": [1133, 291, 362, 257, 22125, 88, 391, 21060, 1269, 11, 291, 486, 536, 746, 597, 1097], "temperature": 0.0, "avg_logprob": -0.14584706379817083, "compression_ratio": 1.532967032967033, "no_speech_prob": 4.468953193281777e-05}, {"id": 252, "seek": 120240, "start": 1202.4, "end": 1214.48, "text": " So this is a good time to show you about starting and stopping AWS instances.", "tokens": [407, 341, 307, 257, 665, 565, 281, 855, 291, 466, 2891, 293, 12767, 17650, 14519, 13], "temperature": 0.0, "avg_logprob": -0.1754155087826857, "compression_ratio": 1.4615384615384615, "no_speech_prob": 5.093562776892213e-06}, {"id": 253, "seek": 120240, "start": 1214.48, "end": 1220.0600000000002, "text": " So I just tried to go to my notebook on AWS and it says, it can't be reached.", "tokens": [407, 286, 445, 3031, 281, 352, 281, 452, 21060, 322, 17650, 293, 309, 1619, 11, 309, 393, 380, 312, 6488, 13], "temperature": 0.0, "avg_logprob": -0.1754155087826857, "compression_ratio": 1.4615384615384615, "no_speech_prob": 5.093562776892213e-06}, {"id": 254, "seek": 120240, "start": 1220.0600000000002, "end": 1229.48, "text": " So my guess is that if we go back to my console, you can see I have zero running instances.", "tokens": [407, 452, 2041, 307, 300, 498, 321, 352, 646, 281, 452, 11076, 11, 291, 393, 536, 286, 362, 4018, 2614, 14519, 13], "temperature": 0.0, "avg_logprob": -0.1754155087826857, "compression_ratio": 1.4615384615384615, "no_speech_prob": 5.093562776892213e-06}, {"id": 255, "seek": 122948, "start": 1229.48, "end": 1233.24, "text": " So I've got zero servers currently running.", "tokens": [407, 286, 600, 658, 4018, 15909, 4362, 2614, 13], "temperature": 0.0, "avg_logprob": -0.1756939206804548, "compression_ratio": 1.5991189427312775, "no_speech_prob": 8.800835530564655e-06}, {"id": 256, "seek": 122948, "start": 1233.24, "end": 1238.56, "text": " So if I click that, I will see all my servers.", "tokens": [407, 498, 286, 2052, 300, 11, 286, 486, 536, 439, 452, 15909, 13], "temperature": 0.0, "avg_logprob": -0.1756939206804548, "compression_ratio": 1.5991189427312775, "no_speech_prob": 8.800835530564655e-06}, {"id": 257, "seek": 122948, "start": 1238.56, "end": 1245.3600000000001, "text": " Normally I would have one P2 server or instance and one T2 because I use the free one for", "tokens": [17424, 286, 576, 362, 472, 430, 17, 7154, 420, 5197, 293, 472, 314, 17, 570, 286, 764, 264, 1737, 472, 337], "temperature": 0.0, "avg_logprob": -0.1756939206804548, "compression_ratio": 1.5991189427312775, "no_speech_prob": 8.800835530564655e-06}, {"id": 258, "seek": 122948, "start": 1245.3600000000001, "end": 1252.92, "text": " kind of getting everything set up and then use the paid one once everything's working.", "tokens": [733, 295, 1242, 1203, 992, 493, 293, 550, 764, 264, 4835, 472, 1564, 1203, 311, 1364, 13], "temperature": 0.0, "avg_logprob": -0.1756939206804548, "compression_ratio": 1.5991189427312775, "no_speech_prob": 8.800835530564655e-06}, {"id": 259, "seek": 122948, "start": 1252.92, "end": 1255.72, "text": " Because I've been fiddling around with things for this class, I just have the P2 at the", "tokens": [1436, 286, 600, 668, 283, 14273, 1688, 926, 365, 721, 337, 341, 1508, 11, 286, 445, 362, 264, 430, 17, 412, 264], "temperature": 0.0, "avg_logprob": -0.1756939206804548, "compression_ratio": 1.5991189427312775, "no_speech_prob": 8.800835530564655e-06}, {"id": 260, "seek": 122948, "start": 1255.72, "end": 1256.72, "text": " moment.", "tokens": [1623, 13], "temperature": 0.0, "avg_logprob": -0.1756939206804548, "compression_ratio": 1.5991189427312775, "no_speech_prob": 8.800835530564655e-06}, {"id": 261, "seek": 125672, "start": 1256.72, "end": 1264.68, "text": " So having gone here, one way I could start this is by going start here.", "tokens": [407, 1419, 2780, 510, 11, 472, 636, 286, 727, 722, 341, 307, 538, 516, 722, 510, 13], "temperature": 0.0, "avg_logprob": -0.23485449382237025, "compression_ratio": 1.380952380952381, "no_speech_prob": 2.4682112780283205e-05}, {"id": 262, "seek": 125672, "start": 1264.68, "end": 1270.88, "text": " But like I said, I don't much like using this GUI for stuff because it's just so much easier", "tokens": [583, 411, 286, 848, 11, 286, 500, 380, 709, 411, 1228, 341, 17917, 40, 337, 1507, 570, 309, 311, 445, 370, 709, 3571], "temperature": 0.0, "avg_logprob": -0.23485449382237025, "compression_ratio": 1.380952380952381, "no_speech_prob": 2.4682112780283205e-05}, {"id": 263, "seek": 125672, "start": 1270.88, "end": 1275.56, "text": " to do things through the command line.", "tokens": [281, 360, 721, 807, 264, 5622, 1622, 13], "temperature": 0.0, "avg_logprob": -0.23485449382237025, "compression_ratio": 1.380952380952381, "no_speech_prob": 2.4682112780283205e-05}, {"id": 264, "seek": 127556, "start": 1275.56, "end": 1293.48, "text": " So one of the things that I showed you guys that you could download today is a bunch of", "tokens": [407, 472, 295, 264, 721, 300, 286, 4712, 291, 1074, 300, 291, 727, 5484, 965, 307, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.11256067631608349, "compression_ratio": 1.4375, "no_speech_prob": 1.6700807464076206e-05}, {"id": 265, "seek": 127556, "start": 1293.48, "end": 1298.04, "text": " aliases for making starting and stopping AWS really quick.", "tokens": [10198, 1957, 337, 1455, 2891, 293, 12767, 17650, 534, 1702, 13], "temperature": 0.0, "avg_logprob": -0.11256067631608349, "compression_ratio": 1.4375, "no_speech_prob": 1.6700807464076206e-05}, {"id": 266, "seek": 127556, "start": 1298.04, "end": 1304.6799999999998, "text": " If you haven't got them yet, you can find links to them on Slack or you can just go", "tokens": [759, 291, 2378, 380, 658, 552, 1939, 11, 291, 393, 915, 6123, 281, 552, 322, 37211, 420, 291, 393, 445, 352], "temperature": 0.0, "avg_logprob": -0.11256067631608349, "compression_ratio": 1.4375, "no_speech_prob": 1.6700807464076206e-05}, {"id": 267, "seek": 130468, "start": 1304.68, "end": 1311.28, "text": " to platform.ai slash files and there's a bunch of different things here.", "tokens": [281, 3663, 13, 1301, 17330, 7098, 293, 456, 311, 257, 3840, 295, 819, 721, 510, 13], "temperature": 0.0, "avg_logprob": -0.1948136416348544, "compression_ratio": 1.4382716049382716, "no_speech_prob": 9.665911420597695e-06}, {"id": 268, "seek": 130468, "start": 1311.28, "end": 1317.8400000000001, "text": " This aws-alias.sh is a file that sets up these various aliases.", "tokens": [639, 1714, 82, 12, 304, 4609, 13, 2716, 307, 257, 3991, 300, 6352, 493, 613, 3683, 10198, 1957, 13], "temperature": 0.0, "avg_logprob": -0.1948136416348544, "compression_ratio": 1.4382716049382716, "no_speech_prob": 9.665911420597695e-06}, {"id": 269, "seek": 130468, "start": 1317.8400000000001, "end": 1326.64, "text": " So the easiest way to grab stuff on your AWS instance or server is to use something called", "tokens": [407, 264, 12889, 636, 281, 4444, 1507, 322, 428, 17650, 5197, 420, 7154, 307, 281, 764, 746, 1219], "temperature": 0.0, "avg_logprob": -0.1948136416348544, "compression_ratio": 1.4382716049382716, "no_speech_prob": 9.665911420597695e-06}, {"id": 270, "seek": 130468, "start": 1326.64, "end": 1327.8200000000002, "text": " wget.", "tokens": [261, 847, 13], "temperature": 0.0, "avg_logprob": -0.1948136416348544, "compression_ratio": 1.4382716049382716, "no_speech_prob": 9.665911420597695e-06}, {"id": 271, "seek": 132782, "start": 1327.82, "end": 1337.3, "text": " So I would right-click on this and choose copy link address and then go wget and paste", "tokens": [407, 286, 576, 558, 12, 18548, 322, 341, 293, 2826, 5055, 2113, 2985, 293, 550, 352, 261, 847, 293, 9163], "temperature": 0.0, "avg_logprob": -0.22708924611409506, "compression_ratio": 1.4675324675324675, "no_speech_prob": 9.223256711266004e-06}, {"id": 272, "seek": 132782, "start": 1337.3, "end": 1340.2, "text": " in that.", "tokens": [294, 300, 13], "temperature": 0.0, "avg_logprob": -0.22708924611409506, "compression_ratio": 1.4675324675324675, "no_speech_prob": 9.223256711266004e-06}, {"id": 273, "seek": 132782, "start": 1340.2, "end": 1351.2, "text": " That will go ahead and download that file.", "tokens": [663, 486, 352, 2286, 293, 5484, 300, 3991, 13], "temperature": 0.0, "avg_logprob": -0.22708924611409506, "compression_ratio": 1.4675324675324675, "no_speech_prob": 9.223256711266004e-06}, {"id": 274, "seek": 132782, "start": 1351.2, "end": 1355.12, "text": " We can take a look at that file and you'll see it's basically a bunch of lines that say", "tokens": [492, 393, 747, 257, 574, 412, 300, 3991, 293, 291, 603, 536, 309, 311, 1936, 257, 3840, 295, 3876, 300, 584], "temperature": 0.0, "avg_logprob": -0.22708924611409506, "compression_ratio": 1.4675324675324675, "no_speech_prob": 9.223256711266004e-06}, {"id": 275, "seek": 135512, "start": 1355.12, "end": 1359.28, "text": " alias something equals something else.", "tokens": [419, 4609, 746, 6915, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.1591806315412425, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.1659340998448897e-05}, {"id": 276, "seek": 135512, "start": 1359.28, "end": 1370.12, "text": " And it's created aws-get-p2, aws-get-t2, aws-start, aws-ssh, aws-stop.", "tokens": [400, 309, 311, 2942, 1714, 82, 12, 847, 12, 79, 17, 11, 1714, 82, 12, 847, 12, 83, 17, 11, 1714, 82, 12, 24419, 11, 1714, 82, 12, 82, 2716, 11, 1714, 82, 12, 13559, 13], "temperature": 0.0, "avg_logprob": -0.1591806315412425, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.1659340998448897e-05}, {"id": 277, "seek": 135512, "start": 1370.12, "end": 1375.04, "text": " I'm going to show you what these things do because I find them pretty convenient.", "tokens": [286, 478, 516, 281, 855, 291, 437, 613, 721, 360, 570, 286, 915, 552, 1238, 10851, 13], "temperature": 0.0, "avg_logprob": -0.1591806315412425, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.1659340998448897e-05}, {"id": 278, "seek": 135512, "start": 1375.04, "end": 1383.4799999999998, "text": " Basically if I run aws-get-p2, first of all I'll say source aws-alias.sh and that just", "tokens": [8537, 498, 286, 1190, 1714, 82, 12, 847, 12, 79, 17, 11, 700, 295, 439, 286, 603, 584, 4009, 1714, 82, 12, 304, 4609, 13, 2716, 293, 300, 445], "temperature": 0.0, "avg_logprob": -0.1591806315412425, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.1659340998448897e-05}, {"id": 279, "seek": 138348, "start": 1383.48, "end": 1385.48, "text": " runs that file.", "tokens": [6676, 300, 3991, 13], "temperature": 0.0, "avg_logprob": -0.18128573726600325, "compression_ratio": 1.422077922077922, "no_speech_prob": 2.726456386881182e-06}, {"id": 280, "seek": 138348, "start": 1385.48, "end": 1388.08, "text": " In Bash, that's how you just run a file.", "tokens": [682, 43068, 11, 300, 311, 577, 291, 445, 1190, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.18128573726600325, "compression_ratio": 1.422077922077922, "no_speech_prob": 2.726456386881182e-06}, {"id": 281, "seek": 138348, "start": 1388.08, "end": 1394.88, "text": " And that's now caused all of those names to appear as aliases to my system.", "tokens": [400, 300, 311, 586, 7008, 439, 295, 729, 5288, 281, 4204, 382, 10198, 1957, 281, 452, 1185, 13], "temperature": 0.0, "avg_logprob": -0.18128573726600325, "compression_ratio": 1.422077922077922, "no_speech_prob": 2.726456386881182e-06}, {"id": 282, "seek": 138348, "start": 1394.88, "end": 1408.8, "text": " So if I now run alias-get-p2, that's going to go ahead and ask Amazon for the ID of my", "tokens": [407, 498, 286, 586, 1190, 419, 4609, 12, 847, 12, 79, 17, 11, 300, 311, 516, 281, 352, 2286, 293, 1029, 6795, 337, 264, 7348, 295, 452], "temperature": 0.0, "avg_logprob": -0.18128573726600325, "compression_ratio": 1.422077922077922, "no_speech_prob": 2.726456386881182e-06}, {"id": 283, "seek": 140880, "start": 1408.8, "end": 1414.52, "text": " P2 instance, and not only does it print it, but it's going to save it into a variable", "tokens": [430, 17, 5197, 11, 293, 406, 787, 775, 309, 4482, 309, 11, 457, 309, 311, 516, 281, 3155, 309, 666, 257, 7006], "temperature": 0.0, "avg_logprob": -0.16017656740934952, "compression_ratio": 1.587378640776699, "no_speech_prob": 9.972809493774548e-06}, {"id": 284, "seek": 140880, "start": 1414.52, "end": 1421.6399999999999, "text": " called instanceID and all of my other scripts will use $instanceID.", "tokens": [1219, 5197, 2777, 293, 439, 295, 452, 661, 23294, 486, 764, 1848, 13911, 719, 2777, 13], "temperature": 0.0, "avg_logprob": -0.16017656740934952, "compression_ratio": 1.587378640776699, "no_speech_prob": 9.972809493774548e-06}, {"id": 285, "seek": 140880, "start": 1421.6399999999999, "end": 1429.32, "text": " So I now want to start that instance, so I just type aws-start and that's going to go", "tokens": [407, 286, 586, 528, 281, 722, 300, 5197, 11, 370, 286, 445, 2010, 1714, 82, 12, 24419, 293, 300, 311, 516, 281, 352], "temperature": 0.0, "avg_logprob": -0.16017656740934952, "compression_ratio": 1.587378640776699, "no_speech_prob": 9.972809493774548e-06}, {"id": 286, "seek": 140880, "start": 1429.32, "end": 1436.04, "text": " ahead and do this equivalent thing of going to the GUI, right-clicking, choosing start.", "tokens": [2286, 293, 360, 341, 10344, 551, 295, 516, 281, 264, 17917, 40, 11, 558, 12, 3474, 10401, 11, 10875, 722, 13], "temperature": 0.0, "avg_logprob": -0.16017656740934952, "compression_ratio": 1.587378640776699, "no_speech_prob": 9.972809493774548e-06}, {"id": 287, "seek": 143604, "start": 1436.04, "end": 1444.92, "text": " The other nice thing it does is it waits until the instance is running.", "tokens": [440, 661, 1481, 551, 309, 775, 307, 309, 40597, 1826, 264, 5197, 307, 2614, 13], "temperature": 0.0, "avg_logprob": -0.12521783407632406, "compression_ratio": 1.592964824120603, "no_speech_prob": 2.5215613277396187e-06}, {"id": 288, "seek": 143604, "start": 1444.92, "end": 1453.68, "text": " And then at the end, it asks the queries for the IP address and prints it out.", "tokens": [400, 550, 412, 264, 917, 11, 309, 8962, 264, 24109, 337, 264, 8671, 2985, 293, 22305, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.12521783407632406, "compression_ratio": 1.592964824120603, "no_speech_prob": 2.5215613277396187e-06}, {"id": 289, "seek": 143604, "start": 1453.68, "end": 1459.3999999999999, "text": " Now the script that I have given you guys to set up these instances uses something called", "tokens": [823, 264, 5755, 300, 286, 362, 2212, 291, 1074, 281, 992, 493, 613, 14519, 4960, 746, 1219], "temperature": 0.0, "avg_logprob": -0.12521783407632406, "compression_ratio": 1.592964824120603, "no_speech_prob": 2.5215613277396187e-06}, {"id": 290, "seek": 143604, "start": 1459.3999999999999, "end": 1464.76, "text": " an elastic IP that actually keeps the same IP address every time you run it.", "tokens": [364, 17115, 8671, 300, 767, 5965, 264, 912, 8671, 2985, 633, 565, 291, 1190, 309, 13], "temperature": 0.0, "avg_logprob": -0.12521783407632406, "compression_ratio": 1.592964824120603, "no_speech_prob": 2.5215613277396187e-06}, {"id": 291, "seek": 146476, "start": 1464.76, "end": 1469.52, "text": " So you should find that IP address stays the same, which makes it easier.", "tokens": [407, 291, 820, 915, 300, 8671, 2985, 10834, 264, 912, 11, 597, 1669, 309, 3571, 13], "temperature": 0.0, "avg_logprob": -0.182162868976593, "compression_ratio": 1.5561797752808988, "no_speech_prob": 4.356826593721053e-06}, {"id": 292, "seek": 146476, "start": 1469.52, "end": 1472.12, "text": " So there is the IP.", "tokens": [407, 456, 307, 264, 8671, 13], "temperature": 0.0, "avg_logprob": -0.182162868976593, "compression_ratio": 1.5561797752808988, "no_speech_prob": 4.356826593721053e-06}, {"id": 293, "seek": 146476, "start": 1472.12, "end": 1476.2, "text": " So I then have something called aws-ssh.", "tokens": [407, 286, 550, 362, 746, 1219, 1714, 82, 12, 82, 2716, 13], "temperature": 0.0, "avg_logprob": -0.182162868976593, "compression_ratio": 1.5561797752808988, "no_speech_prob": 4.356826593721053e-06}, {"id": 294, "seek": 146476, "start": 1476.2, "end": 1484.8799999999999, "text": " And aws-ssh will go ahead and ssh into that instance.", "tokens": [400, 1714, 82, 12, 82, 2716, 486, 352, 2286, 293, 262, 2716, 666, 300, 5197, 13], "temperature": 0.0, "avg_logprob": -0.182162868976593, "compression_ratio": 1.5561797752808988, "no_speech_prob": 4.356826593721053e-06}, {"id": 295, "seek": 146476, "start": 1484.8799999999999, "end": 1493.12, "text": " So all it does is basically use the username ubuntu, because that's the default username", "tokens": [407, 439, 309, 775, 307, 1936, 764, 264, 30351, 26709, 45605, 11, 570, 300, 311, 264, 7576, 30351], "temperature": 0.0, "avg_logprob": -0.182162868976593, "compression_ratio": 1.5561797752808988, "no_speech_prob": 4.356826593721053e-06}, {"id": 296, "seek": 149312, "start": 1493.12, "end": 1499.32, "text": " for this kind of image on AWS, at and then $instanceIP.", "tokens": [337, 341, 733, 295, 3256, 322, 17650, 11, 412, 293, 550, 1848, 13911, 719, 9139, 13], "temperature": 0.0, "avg_logprob": -0.16828711649005332, "compression_ratio": 1.522167487684729, "no_speech_prob": 1.2606757081812248e-05}, {"id": 297, "seek": 149312, "start": 1499.32, "end": 1502.2399999999998, "text": " So that's that IP address we just got.", "tokens": [407, 300, 311, 300, 8671, 2985, 321, 445, 658, 13], "temperature": 0.0, "avg_logprob": -0.16828711649005332, "compression_ratio": 1.522167487684729, "no_speech_prob": 1.2606757081812248e-05}, {"id": 298, "seek": 149312, "start": 1502.2399999999998, "end": 1508.6799999999998, "text": " The other thing it does is to use the private key that was created when this was originally", "tokens": [440, 661, 551, 309, 775, 307, 281, 764, 264, 4551, 2141, 300, 390, 2942, 562, 341, 390, 7993], "temperature": 0.0, "avg_logprob": -0.16828711649005332, "compression_ratio": 1.522167487684729, "no_speech_prob": 1.2606757081812248e-05}, {"id": 299, "seek": 149312, "start": 1508.6799999999998, "end": 1509.6799999999998, "text": " set up.", "tokens": [992, 493, 13], "temperature": 0.0, "avg_logprob": -0.16828711649005332, "compression_ratio": 1.522167487684729, "no_speech_prob": 1.2606757081812248e-05}, {"id": 300, "seek": 149312, "start": 1509.6799999999998, "end": 1514.2399999999998, "text": " Now in my case, I've actually moved that private key to be my default key, so I don't actually", "tokens": [823, 294, 452, 1389, 11, 286, 600, 767, 4259, 300, 4551, 2141, 281, 312, 452, 7576, 2141, 11, 370, 286, 500, 380, 767], "temperature": 0.0, "avg_logprob": -0.16828711649005332, "compression_ratio": 1.522167487684729, "no_speech_prob": 1.2606757081812248e-05}, {"id": 301, "seek": 149312, "start": 1514.2399999999998, "end": 1515.2399999999998, "text": " need that minus IP.", "tokens": [643, 300, 3175, 8671, 13], "temperature": 0.0, "avg_logprob": -0.16828711649005332, "compression_ratio": 1.522167487684729, "no_speech_prob": 1.2606757081812248e-05}, {"id": 302, "seek": 151524, "start": 1515.24, "end": 1524.76, "text": " So I just type ssh ubuntu $instanceIP, but you can just type aws-ssh and you'll see bang,", "tokens": [407, 286, 445, 2010, 262, 2716, 26709, 45605, 1848, 13911, 719, 9139, 11, 457, 291, 393, 445, 2010, 1714, 82, 12, 82, 2716, 293, 291, 603, 536, 8550, 11], "temperature": 0.0, "avg_logprob": -0.18283921559651692, "compression_ratio": 1.4242424242424243, "no_speech_prob": 7.411161732306937e-06}, {"id": 303, "seek": 151524, "start": 1524.76, "end": 1526.92, "text": " here we are.", "tokens": [510, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.18283921559651692, "compression_ratio": 1.4242424242424243, "no_speech_prob": 7.411161732306937e-06}, {"id": 304, "seek": 151524, "start": 1526.92, "end": 1537.8, "text": " So we are now inside that AWS image.", "tokens": [407, 321, 366, 586, 1854, 300, 17650, 3256, 13], "temperature": 0.0, "avg_logprob": -0.18283921559651692, "compression_ratio": 1.4242424242424243, "no_speech_prob": 7.411161732306937e-06}, {"id": 305, "seek": 151524, "start": 1537.8, "end": 1543.48, "text": " One of the handy things about AWS is they have these things called AMIs, Amazon Machine", "tokens": [1485, 295, 264, 13239, 721, 466, 17650, 307, 436, 362, 613, 721, 1219, 6475, 6802, 11, 6795, 22155], "temperature": 0.0, "avg_logprob": -0.18283921559651692, "compression_ratio": 1.4242424242424243, "no_speech_prob": 7.411161732306937e-06}, {"id": 306, "seek": 151524, "start": 1543.48, "end": 1544.48, "text": " Images.", "tokens": [4331, 1660, 13], "temperature": 0.0, "avg_logprob": -0.18283921559651692, "compression_ratio": 1.4242424242424243, "no_speech_prob": 7.411161732306937e-06}, {"id": 307, "seek": 154448, "start": 1544.48, "end": 1551.16, "text": " An AMI is basically a snapshot of a computer at a particular point in time.", "tokens": [1107, 6475, 40, 307, 1936, 257, 30163, 295, 257, 3820, 412, 257, 1729, 935, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.0992805801819418, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425453425938031e-06}, {"id": 308, "seek": 154448, "start": 1551.16, "end": 1557.2, "text": " And you can start your own instance using a copy of that snapshot.", "tokens": [400, 291, 393, 722, 428, 1065, 5197, 1228, 257, 5055, 295, 300, 30163, 13], "temperature": 0.0, "avg_logprob": -0.0992805801819418, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425453425938031e-06}, {"id": 309, "seek": 154448, "start": 1557.2, "end": 1563.08, "text": " So in the script I've given you guys, I've created and provided an AMI which has all", "tokens": [407, 294, 264, 5755, 286, 600, 2212, 291, 1074, 11, 286, 600, 2942, 293, 5649, 364, 6475, 40, 597, 575, 439], "temperature": 0.0, "avg_logprob": -0.0992805801819418, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425453425938031e-06}, {"id": 310, "seek": 154448, "start": 1563.08, "end": 1564.66, "text": " the stuff we want installed.", "tokens": [264, 1507, 321, 528, 8899, 13], "temperature": 0.0, "avg_logprob": -0.0992805801819418, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425453425938031e-06}, {"id": 311, "seek": 154448, "start": 1564.66, "end": 1569.84, "text": " So that's why it is that when you use that script and log into it, you can start running", "tokens": [407, 300, 311, 983, 309, 307, 300, 562, 291, 764, 300, 5755, 293, 3565, 666, 309, 11, 291, 393, 722, 2614], "temperature": 0.0, "avg_logprob": -0.0992805801819418, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425453425938031e-06}, {"id": 312, "seek": 154448, "start": 1569.84, "end": 1571.24, "text": " things straight away.", "tokens": [721, 2997, 1314, 13], "temperature": 0.0, "avg_logprob": -0.0992805801819418, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425453425938031e-06}, {"id": 313, "seek": 154448, "start": 1571.24, "end": 1572.24, "text": " So let's do that right now.", "tokens": [407, 718, 311, 360, 300, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.0992805801819418, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425453425938031e-06}, {"id": 314, "seek": 157224, "start": 1572.24, "end": 1577.56, "text": " I've created a directory already for you called nbs for notebooks.", "tokens": [286, 600, 2942, 257, 21120, 1217, 337, 291, 1219, 297, 929, 337, 43782, 13], "temperature": 0.0, "avg_logprob": -0.21054057166689918, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.3211786608735565e-05}, {"id": 315, "seek": 157224, "start": 1577.56, "end": 1582.04, "text": " So we can go ahead and type jupyter-notebook.", "tokens": [407, 321, 393, 352, 2286, 293, 2010, 361, 1010, 88, 391, 12, 22178, 2939, 13], "temperature": 0.0, "avg_logprob": -0.21054057166689918, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.3211786608735565e-05}, {"id": 316, "seek": 157224, "start": 1582.04, "end": 1588.68, "text": " And this is how we ask Amazon to set up a Jupyter Notebook server for us.", "tokens": [400, 341, 307, 577, 321, 1029, 6795, 281, 992, 493, 257, 22125, 88, 391, 11633, 2939, 7154, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.21054057166689918, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.3211786608735565e-05}, {"id": 317, "seek": 157224, "start": 1588.68, "end": 1593.52, "text": " And when it's done, it says, OK, the Jupyter Notebook is running at all IP addresses on", "tokens": [400, 562, 309, 311, 1096, 11, 309, 1619, 11, 2264, 11, 264, 22125, 88, 391, 11633, 2939, 307, 2614, 412, 439, 8671, 16862, 322], "temperature": 0.0, "avg_logprob": -0.21054057166689918, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.3211786608735565e-05}, {"id": 318, "seek": 157224, "start": 1593.52, "end": 1596.08, "text": " your system, 8, 8, 8, 8.", "tokens": [428, 1185, 11, 1649, 11, 1649, 11, 1649, 11, 1649, 13], "temperature": 0.0, "avg_logprob": -0.21054057166689918, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.3211786608735565e-05}, {"id": 319, "seek": 157224, "start": 1596.08, "end": 1598.08, "text": " So what is our IP address?", "tokens": [407, 437, 307, 527, 8671, 2985, 30], "temperature": 0.0, "avg_logprob": -0.21054057166689918, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.3211786608735565e-05}, {"id": 320, "seek": 159808, "start": 1598.08, "end": 1602.24, "text": " Well, it told us up here when we started it, it's 52 blah blah blah blah.", "tokens": [1042, 11, 309, 1907, 505, 493, 510, 562, 321, 1409, 309, 11, 309, 311, 18079, 12288, 12288, 12288, 12288, 13], "temperature": 0.0, "avg_logprob": -0.256650133335844, "compression_ratio": 1.5116279069767442, "no_speech_prob": 2.0462584870983846e-05}, {"id": 321, "seek": 159808, "start": 1602.24, "end": 1611.4399999999998, "text": " So I'm going to go to my instance and I'll go 52.40.116.111 in my case, colon, and it", "tokens": [407, 286, 478, 516, 281, 352, 281, 452, 5197, 293, 286, 603, 352, 18079, 13, 5254, 13, 5348, 21, 13, 5348, 16, 294, 452, 1389, 11, 8255, 11, 293, 309], "temperature": 0.0, "avg_logprob": -0.256650133335844, "compression_ratio": 1.5116279069767442, "no_speech_prob": 2.0462584870983846e-05}, {"id": 322, "seek": 159808, "start": 1611.4399999999998, "end": 1615.84, "text": " told me that the port is 8, 8, 8, 8, so colon 8, 8, 8.", "tokens": [1907, 385, 300, 264, 2436, 307, 1649, 11, 1649, 11, 1649, 11, 1649, 11, 370, 8255, 1649, 11, 1649, 11, 1649, 13], "temperature": 0.0, "avg_logprob": -0.256650133335844, "compression_ratio": 1.5116279069767442, "no_speech_prob": 2.0462584870983846e-05}, {"id": 323, "seek": 159808, "start": 1615.84, "end": 1621.76, "text": " So I'm just typing in that here, press Enter.", "tokens": [407, 286, 478, 445, 18444, 294, 300, 510, 11, 1886, 10399, 13], "temperature": 0.0, "avg_logprob": -0.256650133335844, "compression_ratio": 1.5116279069767442, "no_speech_prob": 2.0462584870983846e-05}, {"id": 324, "seek": 162176, "start": 1621.76, "end": 1636.08, "text": " I've set up a password, it's just dl-underscore-course.", "tokens": [286, 600, 992, 493, 257, 11524, 11, 309, 311, 445, 37873, 12, 997, 433, 12352, 12, 31913, 13], "temperature": 0.0, "avg_logprob": -0.2127284738752577, "compression_ratio": 1.511764705882353, "no_speech_prob": 8.013382284843829e-06}, {"id": 325, "seek": 162176, "start": 1636.08, "end": 1639.04, "text": " We can look later on at how to change that password if people want to, but I just thought", "tokens": [492, 393, 574, 1780, 322, 412, 577, 281, 1319, 300, 11524, 498, 561, 528, 281, 11, 457, 286, 445, 1194], "temperature": 0.0, "avg_logprob": -0.2127284738752577, "compression_ratio": 1.511764705882353, "no_speech_prob": 8.013382284843829e-06}, {"id": 326, "seek": 162176, "start": 1639.04, "end": 1642.2, "text": " it would be handy to have a password there for everybody if you want to start looking", "tokens": [309, 576, 312, 13239, 281, 362, 257, 11524, 456, 337, 2201, 498, 291, 528, 281, 722, 1237], "temperature": 0.0, "avg_logprob": -0.2127284738752577, "compression_ratio": 1.511764705882353, "no_speech_prob": 8.013382284843829e-06}, {"id": 327, "seek": 162176, "start": 1642.2, "end": 1645.84, "text": " at some of your own data.", "tokens": [412, 512, 295, 428, 1065, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2127284738752577, "compression_ratio": 1.511764705882353, "no_speech_prob": 8.013382284843829e-06}, {"id": 328, "seek": 164584, "start": 1645.84, "end": 1653.8, "text": " And actually by default it's not going to show you anything, so I'm going to delete", "tokens": [400, 767, 538, 7576, 309, 311, 406, 516, 281, 855, 291, 1340, 11, 370, 286, 478, 516, 281, 12097], "temperature": 0.0, "avg_logprob": -0.30723753320165426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.540369668073254e-06}, {"id": 329, "seek": 164584, "start": 1653.8, "end": 1655.08, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.30723753320165426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.540369668073254e-06}, {"id": 330, "seek": 164584, "start": 1655.08, "end": 1662.84, "text": " So now we can just go ahead and say new Notebook, and just say python-conderoot.", "tokens": [407, 586, 321, 393, 445, 352, 2286, 293, 584, 777, 11633, 2939, 11, 293, 445, 584, 38797, 12, 18882, 2032, 310, 13], "temperature": 0.0, "avg_logprob": -0.30723753320165426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.540369668073254e-06}, {"id": 331, "seek": 164584, "start": 1662.84, "end": 1670.36, "text": " And this sets up a scientific computing environment for you where you can type python commands", "tokens": [400, 341, 6352, 493, 257, 8134, 15866, 2823, 337, 291, 689, 291, 393, 2010, 38797, 16901], "temperature": 0.0, "avg_logprob": -0.30723753320165426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.540369668073254e-06}, {"id": 332, "seek": 164584, "start": 1670.36, "end": 1671.56, "text": " and get back responses.", "tokens": [293, 483, 646, 13019, 13], "temperature": 0.0, "avg_logprob": -0.30723753320165426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.540369668073254e-06}, {"id": 333, "seek": 164584, "start": 1671.56, "end": 1674.56, "text": " So 1 plus 1.", "tokens": [407, 502, 1804, 502, 13], "temperature": 0.0, "avg_logprob": -0.30723753320165426, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.540369668073254e-06}, {"id": 334, "seek": 167456, "start": 1674.56, "end": 1677.6399999999999, "text": " There we go, so it seems to be computing things correctly.", "tokens": [821, 321, 352, 11, 370, 309, 2544, 281, 312, 15866, 721, 8944, 13], "temperature": 0.0, "avg_logprob": -0.16396703907087737, "compression_ratio": 1.6788990825688073, "no_speech_prob": 5.338091796147637e-06}, {"id": 335, "seek": 167456, "start": 1677.6399999999999, "end": 1685.32, "text": " So the basic idea here is that over there on Amazon, you have your server, it is running", "tokens": [407, 264, 3875, 1558, 510, 307, 300, 670, 456, 322, 6795, 11, 291, 362, 428, 7154, 11, 309, 307, 2614], "temperature": 0.0, "avg_logprob": -0.16396703907087737, "compression_ratio": 1.6788990825688073, "no_speech_prob": 5.338091796147637e-06}, {"id": 336, "seek": 167456, "start": 1685.32, "end": 1688.3999999999999, "text": " a program called Jupyter Notebook.", "tokens": [257, 1461, 1219, 22125, 88, 391, 11633, 2939, 13], "temperature": 0.0, "avg_logprob": -0.16396703907087737, "compression_ratio": 1.6788990825688073, "no_speech_prob": 5.338091796147637e-06}, {"id": 337, "seek": 167456, "start": 1688.3999999999999, "end": 1695.72, "text": " Jupyter Notebook is causing a particular port, which is 8, 8, 8, 8, to be opened on that", "tokens": [22125, 88, 391, 11633, 2939, 307, 9853, 257, 1729, 2436, 11, 597, 307, 1649, 11, 1649, 11, 1649, 11, 1649, 11, 281, 312, 5625, 322, 300], "temperature": 0.0, "avg_logprob": -0.16396703907087737, "compression_ratio": 1.6788990825688073, "no_speech_prob": 5.338091796147637e-06}, {"id": 338, "seek": 167456, "start": 1695.72, "end": 1703.48, "text": " server, where if you access it, it then gives you access to this Jupyter Notebook environment.", "tokens": [7154, 11, 689, 498, 291, 2105, 309, 11, 309, 550, 2709, 291, 2105, 281, 341, 22125, 88, 391, 11633, 2939, 2823, 13], "temperature": 0.0, "avg_logprob": -0.16396703907087737, "compression_ratio": 1.6788990825688073, "no_speech_prob": 5.338091796147637e-06}, {"id": 339, "seek": 170348, "start": 1703.48, "end": 1711.96, "text": " In your team, you guys can all use the same Jupyter Notebook if you want to, or you could", "tokens": [682, 428, 1469, 11, 291, 1074, 393, 439, 764, 264, 912, 22125, 88, 391, 11633, 2939, 498, 291, 528, 281, 11, 420, 291, 727], "temperature": 0.0, "avg_logprob": -0.16802136441494556, "compression_ratio": 1.5586854460093897, "no_speech_prob": 3.5559642128646374e-06}, {"id": 340, "seek": 170348, "start": 1711.96, "end": 1714.88, "text": " run multiple Jupyter Notebooks on one machine.", "tokens": [1190, 3866, 22125, 88, 391, 11633, 15170, 322, 472, 3479, 13], "temperature": 0.0, "avg_logprob": -0.16802136441494556, "compression_ratio": 1.5586854460093897, "no_speech_prob": 3.5559642128646374e-06}, {"id": 341, "seek": 170348, "start": 1714.88, "end": 1716.92, "text": " It's really pretty flexible.", "tokens": [467, 311, 534, 1238, 11358, 13], "temperature": 0.0, "avg_logprob": -0.16802136441494556, "compression_ratio": 1.5586854460093897, "no_speech_prob": 3.5559642128646374e-06}, {"id": 342, "seek": 170348, "start": 1716.92, "end": 1727.52, "text": " So now that I've created one, I could rename this, say this is Jeremy's Notebook.", "tokens": [407, 586, 300, 286, 600, 2942, 472, 11, 286, 727, 36741, 341, 11, 584, 341, 307, 17809, 311, 11633, 2939, 13], "temperature": 0.0, "avg_logprob": -0.16802136441494556, "compression_ratio": 1.5586854460093897, "no_speech_prob": 3.5559642128646374e-06}, {"id": 343, "seek": 170348, "start": 1727.52, "end": 1732.56, "text": " And so then Rachel might come along and be like, oh I want to run something as well.", "tokens": [400, 370, 550, 14246, 1062, 808, 2051, 293, 312, 411, 11, 1954, 286, 528, 281, 1190, 746, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16802136441494556, "compression_ratio": 1.5586854460093897, "no_speech_prob": 3.5559642128646374e-06}, {"id": 344, "seek": 173256, "start": 1732.56, "end": 1741.04, "text": " So she goes new and her computer and it creates a whole new one over here, and she could say", "tokens": [407, 750, 1709, 777, 293, 720, 3820, 293, 309, 7829, 257, 1379, 777, 472, 670, 510, 11, 293, 750, 727, 584], "temperature": 0.0, "avg_logprob": -0.206349347088788, "compression_ratio": 1.5055555555555555, "no_speech_prob": 7.296317107829964e-06}, {"id": 345, "seek": 173256, "start": 1741.04, "end": 1749.12, "text": " file, rename, Rachel's Notebook.", "tokens": [3991, 11, 36741, 11, 14246, 311, 11633, 2939, 13], "temperature": 0.0, "avg_logprob": -0.206349347088788, "compression_ratio": 1.5055555555555555, "no_speech_prob": 7.296317107829964e-06}, {"id": 346, "seek": 173256, "start": 1749.12, "end": 1755.32, "text": " So if I now go back here, you can see both of these notebooks are shown to be running.", "tokens": [407, 498, 286, 586, 352, 646, 510, 11, 291, 393, 536, 1293, 295, 613, 43782, 366, 4898, 281, 312, 2614, 13], "temperature": 0.0, "avg_logprob": -0.206349347088788, "compression_ratio": 1.5055555555555555, "no_speech_prob": 7.296317107829964e-06}, {"id": 347, "seek": 173256, "start": 1755.32, "end": 1760.24, "text": " So the server is running multiple kernels, they're called.", "tokens": [407, 264, 7154, 307, 2614, 3866, 23434, 1625, 11, 436, 434, 1219, 13], "temperature": 0.0, "avg_logprob": -0.206349347088788, "compression_ratio": 1.5055555555555555, "no_speech_prob": 7.296317107829964e-06}, {"id": 348, "seek": 176024, "start": 1760.24, "end": 1764.14, "text": " And you can see back here, it's saying creating new notebook, kernel started.", "tokens": [400, 291, 393, 536, 646, 510, 11, 309, 311, 1566, 4084, 777, 21060, 11, 28256, 1409, 13], "temperature": 0.0, "avg_logprob": -0.23753405726233193, "compression_ratio": 1.675392670157068, "no_speech_prob": 5.8627470025385264e-06}, {"id": 349, "seek": 176024, "start": 1764.14, "end": 1766.32, "text": " So each of those are totally separate.", "tokens": [407, 1184, 295, 729, 366, 3879, 4994, 13], "temperature": 0.0, "avg_logprob": -0.23753405726233193, "compression_ratio": 1.675392670157068, "no_speech_prob": 5.8627470025385264e-06}, {"id": 350, "seek": 176024, "start": 1766.32, "end": 1779.36, "text": " So from one of them, I say name equals Rachel, and in the other one, I say name equals Jeremy.", "tokens": [407, 490, 472, 295, 552, 11, 286, 584, 1315, 6915, 14246, 11, 293, 294, 264, 661, 472, 11, 286, 584, 1315, 6915, 17809, 13], "temperature": 0.0, "avg_logprob": -0.23753405726233193, "compression_ratio": 1.675392670157068, "no_speech_prob": 5.8627470025385264e-06}, {"id": 351, "seek": 176024, "start": 1779.36, "end": 1786.96, "text": " And then over here, you'll see that they are not in any way talking to each other, they're", "tokens": [400, 550, 670, 510, 11, 291, 603, 536, 300, 436, 366, 406, 294, 604, 636, 1417, 281, 1184, 661, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.23753405726233193, "compression_ratio": 1.675392670157068, "no_speech_prob": 5.8627470025385264e-06}, {"id": 352, "seek": 176024, "start": 1786.96, "end": 1790.2, "text": " totally separate.", "tokens": [3879, 4994, 13], "temperature": 0.0, "avg_logprob": -0.23753405726233193, "compression_ratio": 1.675392670157068, "no_speech_prob": 5.8627470025385264e-06}, {"id": 353, "seek": 179020, "start": 1790.2, "end": 1794.44, "text": " So that's a super handy way to do work.", "tokens": [407, 300, 311, 257, 1687, 13239, 636, 281, 360, 589, 13], "temperature": 0.0, "avg_logprob": -0.1418621217882311, "compression_ratio": 1.4914285714285713, "no_speech_prob": 1.863147917902097e-05}, {"id": 354, "seek": 179020, "start": 1794.44, "end": 1800.96, "text": " The other nice thing is that you can not just type code, but you can also type markdown.", "tokens": [440, 661, 1481, 551, 307, 300, 291, 393, 406, 445, 2010, 3089, 11, 457, 291, 393, 611, 2010, 1491, 5093, 13], "temperature": 0.0, "avg_logprob": -0.1418621217882311, "compression_ratio": 1.4914285714285713, "no_speech_prob": 1.863147917902097e-05}, {"id": 355, "seek": 179020, "start": 1800.96, "end": 1813.68, "text": " So I could go section, I want to talk about something here.", "tokens": [407, 286, 727, 352, 3541, 11, 286, 528, 281, 751, 466, 746, 510, 13], "temperature": 0.0, "avg_logprob": -0.1418621217882311, "compression_ratio": 1.4914285714285713, "no_speech_prob": 1.863147917902097e-05}, {"id": 356, "seek": 179020, "start": 1813.68, "end": 1819.64, "text": " And so as I do that, it allows me to mix and match information and code.", "tokens": [400, 370, 382, 286, 360, 300, 11, 309, 4045, 385, 281, 2890, 293, 2995, 1589, 293, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1418621217882311, "compression_ratio": 1.4914285714285713, "no_speech_prob": 1.863147917902097e-05}, {"id": 357, "seek": 181964, "start": 1819.64, "end": 1823.64, "text": " And every piece of code that comes out, I can see where it came from.", "tokens": [400, 633, 2522, 295, 3089, 300, 1487, 484, 11, 286, 393, 536, 689, 309, 1361, 490, 13], "temperature": 0.0, "avg_logprob": -0.12523996711957572, "compression_ratio": 1.6491935483870968, "no_speech_prob": 8.139526471495628e-06}, {"id": 358, "seek": 181964, "start": 1823.64, "end": 1828.2800000000002, "text": " And also as you'll see, let's just put in visualizations and plots and so forth.", "tokens": [400, 611, 382, 291, 603, 536, 11, 718, 311, 445, 829, 294, 5056, 14455, 293, 28609, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12523996711957572, "compression_ratio": 1.6491935483870968, "no_speech_prob": 8.139526471495628e-06}, {"id": 359, "seek": 181964, "start": 1828.2800000000002, "end": 1833.8400000000001, "text": " So some of you may have come across this important concept called literate programming.", "tokens": [407, 512, 295, 291, 815, 362, 808, 2108, 341, 1021, 3410, 1219, 2733, 473, 9410, 13], "temperature": 0.0, "avg_logprob": -0.12523996711957572, "compression_ratio": 1.6491935483870968, "no_speech_prob": 8.139526471495628e-06}, {"id": 360, "seek": 181964, "start": 1833.8400000000001, "end": 1840.0, "text": " Literate programming is the idea that as you code, you are documenting what you're doing", "tokens": [16090, 473, 9410, 307, 264, 1558, 300, 382, 291, 3089, 11, 291, 366, 42360, 437, 291, 434, 884], "temperature": 0.0, "avg_logprob": -0.12523996711957572, "compression_ratio": 1.6491935483870968, "no_speech_prob": 8.139526471495628e-06}, {"id": 361, "seek": 181964, "start": 1840.0, "end": 1848.24, "text": " in a very deep way, not just for others, but maybe more importantly for yourself.", "tokens": [294, 257, 588, 2452, 636, 11, 406, 445, 337, 2357, 11, 457, 1310, 544, 8906, 337, 1803, 13], "temperature": 0.0, "avg_logprob": -0.12523996711957572, "compression_ratio": 1.6491935483870968, "no_speech_prob": 8.139526471495628e-06}, {"id": 362, "seek": 184824, "start": 1848.24, "end": 1852.36, "text": " And so when you're doing data science work, work like a scientist.", "tokens": [400, 370, 562, 291, 434, 884, 1412, 3497, 589, 11, 589, 411, 257, 12662, 13], "temperature": 0.0, "avg_logprob": -0.14549880081348204, "compression_ratio": 1.7300884955752212, "no_speech_prob": 6.747982752131065e-06}, {"id": 363, "seek": 184824, "start": 1852.36, "end": 1857.52, "text": " How many people here are in some form scientists or have been scientists?", "tokens": [1012, 867, 561, 510, 366, 294, 512, 1254, 7708, 420, 362, 668, 7708, 30], "temperature": 0.0, "avg_logprob": -0.14549880081348204, "compression_ratio": 1.7300884955752212, "no_speech_prob": 6.747982752131065e-06}, {"id": 364, "seek": 184824, "start": 1857.52, "end": 1863.24, "text": " You guys will know the importance of your journal notebook.", "tokens": [509, 1074, 486, 458, 264, 7379, 295, 428, 6708, 21060, 13], "temperature": 0.0, "avg_logprob": -0.14549880081348204, "compression_ratio": 1.7300884955752212, "no_speech_prob": 6.747982752131065e-06}, {"id": 365, "seek": 184824, "start": 1863.24, "end": 1868.36, "text": " The greatest scientists, they're all stories about the kinds of notebooks they kept and", "tokens": [440, 6636, 7708, 11, 436, 434, 439, 3676, 466, 264, 3685, 295, 43782, 436, 4305, 293], "temperature": 0.0, "avg_logprob": -0.14549880081348204, "compression_ratio": 1.7300884955752212, "no_speech_prob": 6.747982752131065e-06}, {"id": 366, "seek": 184824, "start": 1868.36, "end": 1872.72, "text": " how their lab notebooks worked or their lab journals worked.", "tokens": [577, 641, 2715, 43782, 2732, 420, 641, 2715, 29621, 2732, 13], "temperature": 0.0, "avg_logprob": -0.14549880081348204, "compression_ratio": 1.7300884955752212, "no_speech_prob": 6.747982752131065e-06}, {"id": 367, "seek": 184824, "start": 1872.72, "end": 1875.0, "text": " This is critical for data scientists too.", "tokens": [639, 307, 4924, 337, 1412, 7708, 886, 13], "temperature": 0.0, "avg_logprob": -0.14549880081348204, "compression_ratio": 1.7300884955752212, "no_speech_prob": 6.747982752131065e-06}, {"id": 368, "seek": 187500, "start": 1875.0, "end": 1880.24, "text": " This idea that as you do experiments, you're keeping track of what did I do, what worked,", "tokens": [639, 1558, 300, 382, 291, 360, 12050, 11, 291, 434, 5145, 2837, 295, 437, 630, 286, 360, 11, 437, 2732, 11], "temperature": 0.0, "avg_logprob": -0.1276302872417129, "compression_ratio": 1.6178861788617886, "no_speech_prob": 1.9525657990016043e-05}, {"id": 369, "seek": 187500, "start": 1880.24, "end": 1881.24, "text": " what didn't work.", "tokens": [437, 994, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.1276302872417129, "compression_ratio": 1.6178861788617886, "no_speech_prob": 1.9525657990016043e-05}, {"id": 370, "seek": 187500, "start": 1881.24, "end": 1886.08, "text": " I can see all the people who put their hand up as scientists are all nodding right now.", "tokens": [286, 393, 536, 439, 264, 561, 567, 829, 641, 1011, 493, 382, 7708, 366, 439, 15224, 3584, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.1276302872417129, "compression_ratio": 1.6178861788617886, "no_speech_prob": 1.9525657990016043e-05}, {"id": 371, "seek": 187500, "start": 1886.08, "end": 1888.4, "text": " So this makes it super easy to do that.", "tokens": [407, 341, 1669, 309, 1687, 1858, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1276302872417129, "compression_ratio": 1.6178861788617886, "no_speech_prob": 1.9525657990016043e-05}, {"id": 372, "seek": 187500, "start": 1888.4, "end": 1894.36, "text": " So be helpful to yourself and to your team by taking advantage of this.", "tokens": [407, 312, 4961, 281, 1803, 293, 281, 428, 1469, 538, 1940, 5002, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.1276302872417129, "compression_ratio": 1.6178861788617886, "no_speech_prob": 1.9525657990016043e-05}, {"id": 373, "seek": 187500, "start": 1894.36, "end": 1902.16, "text": " Now in order to learn to use this environment, all you have to do is press H. And when you", "tokens": [823, 294, 1668, 281, 1466, 281, 764, 341, 2823, 11, 439, 291, 362, 281, 360, 307, 1886, 389, 13, 400, 562, 291], "temperature": 0.0, "avg_logprob": -0.1276302872417129, "compression_ratio": 1.6178861788617886, "no_speech_prob": 1.9525657990016043e-05}, {"id": 374, "seek": 190216, "start": 1902.16, "end": 1907.48, "text": " press H, it brings up all of these keyboard shortcuts.", "tokens": [1886, 389, 11, 309, 5607, 493, 439, 295, 613, 10186, 34620, 13], "temperature": 0.0, "avg_logprob": -0.1738889838467125, "compression_ratio": 1.6072727272727272, "no_speech_prob": 1.544596307212487e-05}, {"id": 375, "seek": 190216, "start": 1907.48, "end": 1912.3200000000002, "text": " After not very long, you will get to know all of them because they're all extremely", "tokens": [2381, 406, 588, 938, 11, 291, 486, 483, 281, 458, 439, 295, 552, 570, 436, 434, 439, 4664], "temperature": 0.0, "avg_logprob": -0.1738889838467125, "compression_ratio": 1.6072727272727272, "no_speech_prob": 1.544596307212487e-05}, {"id": 376, "seek": 190216, "start": 1912.3200000000002, "end": 1913.3200000000002, "text": " useful.", "tokens": [4420, 13], "temperature": 0.0, "avg_logprob": -0.1738889838467125, "compression_ratio": 1.6072727272727272, "no_speech_prob": 1.544596307212487e-05}, {"id": 377, "seek": 190216, "start": 1913.3200000000002, "end": 1919.2, "text": " But the main ones I find particularly helpful is you hit M to turn into markdown mode, so", "tokens": [583, 264, 2135, 2306, 286, 915, 4098, 4961, 307, 291, 2045, 376, 281, 1261, 666, 1491, 5093, 4391, 11, 370], "temperature": 0.0, "avg_logprob": -0.1738889838467125, "compression_ratio": 1.6072727272727272, "no_speech_prob": 1.544596307212487e-05}, {"id": 378, "seek": 190216, "start": 1919.2, "end": 1923.88, "text": " that's the mode where you can enter text rather than code, or Y to switch it back to code", "tokens": [300, 311, 264, 4391, 689, 291, 393, 3242, 2487, 2831, 813, 3089, 11, 420, 398, 281, 3679, 309, 646, 281, 3089], "temperature": 0.0, "avg_logprob": -0.1738889838467125, "compression_ratio": 1.6072727272727272, "no_speech_prob": 1.544596307212487e-05}, {"id": 379, "seek": 190216, "start": 1923.88, "end": 1924.88, "text": " again.", "tokens": [797, 13], "temperature": 0.0, "avg_logprob": -0.1738889838467125, "compression_ratio": 1.6072727272727272, "no_speech_prob": 1.544596307212487e-05}, {"id": 380, "seek": 190216, "start": 1924.88, "end": 1929.52, "text": " And you certainly need to know Shift-Enter, which evaluates the cell and gives you a new", "tokens": [400, 291, 3297, 643, 281, 458, 28304, 12, 16257, 391, 11, 597, 6133, 1024, 264, 2815, 293, 2709, 291, 257, 777], "temperature": 0.0, "avg_logprob": -0.1738889838467125, "compression_ratio": 1.6072727272727272, "no_speech_prob": 1.544596307212487e-05}, {"id": 381, "seek": 190216, "start": 1929.52, "end": 1930.88, "text": " cell to enter into.", "tokens": [2815, 281, 3242, 666, 13], "temperature": 0.0, "avg_logprob": -0.1738889838467125, "compression_ratio": 1.6072727272727272, "no_speech_prob": 1.544596307212487e-05}, {"id": 382, "seek": 193088, "start": 1930.88, "end": 1937.2800000000002, "text": " And you also need Escape, which pops you out of entering information and gets you back", "tokens": [400, 291, 611, 643, 42960, 11, 597, 16795, 291, 484, 295, 11104, 1589, 293, 2170, 291, 646], "temperature": 0.0, "avg_logprob": -0.16145849227905273, "compression_ratio": 1.6965174129353233, "no_speech_prob": 7.411151273117866e-06}, {"id": 383, "seek": 193088, "start": 1937.2800000000002, "end": 1940.0400000000002, "text": " into this command mode.", "tokens": [666, 341, 5622, 4391, 13], "temperature": 0.0, "avg_logprob": -0.16145849227905273, "compression_ratio": 1.6965174129353233, "no_speech_prob": 7.411151273117866e-06}, {"id": 384, "seek": 193088, "start": 1940.0400000000002, "end": 1943.0800000000002, "text": " And then Enter to go back into edit mode again.", "tokens": [400, 550, 10399, 281, 352, 646, 666, 8129, 4391, 797, 13], "temperature": 0.0, "avg_logprob": -0.16145849227905273, "compression_ratio": 1.6965174129353233, "no_speech_prob": 7.411151273117866e-06}, {"id": 385, "seek": 193088, "start": 1943.0800000000002, "end": 1950.72, "text": " So just to show you that, Enter to get into edit mode, Escape to get out of edit mode,", "tokens": [407, 445, 281, 855, 291, 300, 11, 10399, 281, 483, 666, 8129, 4391, 11, 42960, 281, 483, 484, 295, 8129, 4391, 11], "temperature": 0.0, "avg_logprob": -0.16145849227905273, "compression_ratio": 1.6965174129353233, "no_speech_prob": 7.411151273117866e-06}, {"id": 386, "seek": 193088, "start": 1950.72, "end": 1953.8400000000001, "text": " Shift-Enter to evaluate.", "tokens": [28304, 12, 16257, 391, 281, 13059, 13], "temperature": 0.0, "avg_logprob": -0.16145849227905273, "compression_ratio": 1.6965174129353233, "no_speech_prob": 7.411151273117866e-06}, {"id": 387, "seek": 193088, "start": 1953.8400000000001, "end": 1959.2, "text": " And you can see as I move around, it changes which one is highlighted.", "tokens": [400, 291, 393, 536, 382, 286, 1286, 926, 11, 309, 2962, 597, 472, 307, 17173, 13], "temperature": 0.0, "avg_logprob": -0.16145849227905273, "compression_ratio": 1.6965174129353233, "no_speech_prob": 7.411151273117866e-06}, {"id": 388, "seek": 195920, "start": 1959.2, "end": 1964.32, "text": " I've started to create some resources on the Wiki for helping you with Jupyter Notebook.", "tokens": [286, 600, 1409, 281, 1884, 512, 3593, 322, 264, 35892, 337, 4315, 291, 365, 22125, 88, 391, 11633, 2939, 13], "temperature": 0.0, "avg_logprob": -0.1497696013677688, "compression_ratio": 1.4388888888888889, "no_speech_prob": 1.7231244783033617e-05}, {"id": 389, "seek": 195920, "start": 1964.32, "end": 1973.6000000000001, "text": " It's still pretty early, but you guys I'm sure can help by adding more information here.", "tokens": [467, 311, 920, 1238, 2440, 11, 457, 291, 1074, 286, 478, 988, 393, 854, 538, 5127, 544, 1589, 510, 13], "temperature": 0.0, "avg_logprob": -0.1497696013677688, "compression_ratio": 1.4388888888888889, "no_speech_prob": 1.7231244783033617e-05}, {"id": 390, "seek": 195920, "start": 1973.6000000000001, "end": 1981.4, "text": " One of the things I particularly mentioned is that there are some good tutorials.", "tokens": [1485, 295, 264, 721, 286, 4098, 2835, 307, 300, 456, 366, 512, 665, 17616, 13], "temperature": 0.0, "avg_logprob": -0.1497696013677688, "compression_ratio": 1.4388888888888889, "no_speech_prob": 1.7231244783033617e-05}, {"id": 391, "seek": 198140, "start": 1981.4, "end": 1994.2800000000002, "text": " I thought I'd also mention my favorite book, which I now can't see here.", "tokens": [286, 1194, 286, 1116, 611, 2152, 452, 2954, 1446, 11, 597, 286, 586, 393, 380, 536, 510, 13], "temperature": 0.0, "avg_logprob": -0.25932837314293034, "compression_ratio": 1.342281879194631, "no_speech_prob": 7.484359957743436e-05}, {"id": 392, "seek": 198140, "start": 1994.2800000000002, "end": 2003.3600000000001, "text": " Python for Data Analysis by Wes McKinney.", "tokens": [15329, 337, 11888, 38172, 538, 23843, 21765, 259, 2397, 13], "temperature": 0.0, "avg_logprob": -0.25932837314293034, "compression_ratio": 1.342281879194631, "no_speech_prob": 7.484359957743436e-05}, {"id": 393, "seek": 198140, "start": 2003.3600000000001, "end": 2007.8000000000002, "text": " It's a little old, it also covers pandas a lot, which you don't need, but it's a good", "tokens": [467, 311, 257, 707, 1331, 11, 309, 611, 10538, 4565, 296, 257, 688, 11, 597, 291, 500, 380, 643, 11, 457, 309, 311, 257, 665], "temperature": 0.0, "avg_logprob": -0.25932837314293034, "compression_ratio": 1.342281879194631, "no_speech_prob": 7.484359957743436e-05}, {"id": 394, "seek": 200780, "start": 2007.8, "end": 2025.24, "text": " book for getting familiar with this basic Python scientific programming stack.", "tokens": [1446, 337, 1242, 4963, 365, 341, 3875, 15329, 8134, 9410, 8630, 13], "temperature": 0.0, "avg_logprob": -0.1383322900341403, "compression_ratio": 1.2232142857142858, "no_speech_prob": 2.355206925130915e-05}, {"id": 395, "seek": 200780, "start": 2025.24, "end": 2031.52, "text": " So the last ingredient that I want to introduce is Kaggle.", "tokens": [407, 264, 1036, 14751, 300, 286, 528, 281, 5366, 307, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.1383322900341403, "compression_ratio": 1.2232142857142858, "no_speech_prob": 2.355206925130915e-05}, {"id": 396, "seek": 203152, "start": 2031.52, "end": 2040.4, "text": " How many people here have been to or done anything with Kaggle at any point?", "tokens": [1012, 867, 561, 510, 362, 668, 281, 420, 1096, 1340, 365, 48751, 22631, 412, 604, 935, 30], "temperature": 0.0, "avg_logprob": -0.24269511964586046, "compression_ratio": 1.4756756756756757, "no_speech_prob": 2.5465949875069782e-05}, {"id": 397, "seek": 203152, "start": 2040.4, "end": 2045.12, "text": " Anybody who is in the master's program here, I'm sure will have used Kaggle or will shortly", "tokens": [19082, 567, 307, 294, 264, 4505, 311, 1461, 510, 11, 286, 478, 988, 486, 362, 1143, 48751, 22631, 420, 486, 13392], "temperature": 0.0, "avg_logprob": -0.24269511964586046, "compression_ratio": 1.4756756756756757, "no_speech_prob": 2.5465949875069782e-05}, {"id": 398, "seek": 203152, "start": 2045.12, "end": 2047.2, "text": " be able to use Kaggle.", "tokens": [312, 1075, 281, 764, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.24269511964586046, "compression_ratio": 1.4756756756756757, "no_speech_prob": 2.5465949875069782e-05}, {"id": 399, "seek": 203152, "start": 2047.2, "end": 2054.4, "text": " Mainly because it's just a great place to get all kinds of interesting data sets.", "tokens": [47468, 570, 309, 311, 445, 257, 869, 1081, 281, 483, 439, 3685, 295, 1880, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.24269511964586046, "compression_ratio": 1.4756756756756757, "no_speech_prob": 2.5465949875069782e-05}, {"id": 400, "seek": 205440, "start": 2054.4, "end": 2062.12, "text": " So for example, if you wanted to test your ability to do automated drug discovery, you", "tokens": [407, 337, 1365, 11, 498, 291, 1415, 281, 1500, 428, 3485, 281, 360, 18473, 4110, 12114, 11, 291], "temperature": 0.0, "avg_logprob": -0.10742964214748807, "compression_ratio": 1.5887445887445888, "no_speech_prob": 6.240839866222814e-06}, {"id": 401, "seek": 205440, "start": 2062.12, "end": 2067.56, "text": " could go to Kaggle and download the files for the Merck Molecular Activity Challenge,", "tokens": [727, 352, 281, 48751, 22631, 293, 5484, 264, 7098, 337, 264, 6124, 547, 46914, 17792, 3251, 4253, 17517, 11], "temperature": 0.0, "avg_logprob": -0.10742964214748807, "compression_ratio": 1.5887445887445888, "no_speech_prob": 6.240839866222814e-06}, {"id": 402, "seek": 205440, "start": 2067.56, "end": 2073.12, "text": " run some models and test them to see how they compare to the state-of-the-art by comparing", "tokens": [1190, 512, 5245, 293, 1500, 552, 281, 536, 577, 436, 6794, 281, 264, 1785, 12, 2670, 12, 3322, 12, 446, 538, 15763], "temperature": 0.0, "avg_logprob": -0.10742964214748807, "compression_ratio": 1.5887445887445888, "no_speech_prob": 6.240839866222814e-06}, {"id": 403, "seek": 205440, "start": 2073.12, "end": 2076.58, "text": " to the leaderboard.", "tokens": [281, 264, 5263, 3787, 13], "temperature": 0.0, "avg_logprob": -0.10742964214748807, "compression_ratio": 1.5887445887445888, "no_speech_prob": 6.240839866222814e-06}, {"id": 404, "seek": 205440, "start": 2076.58, "end": 2083.1600000000003, "text": " So Kaggle is a place where various organizations run machine learning competitions.", "tokens": [407, 48751, 22631, 307, 257, 1081, 689, 3683, 6150, 1190, 3479, 2539, 26185, 13], "temperature": 0.0, "avg_logprob": -0.10742964214748807, "compression_ratio": 1.5887445887445888, "no_speech_prob": 6.240839866222814e-06}, {"id": 405, "seek": 208316, "start": 2083.16, "end": 2085.2799999999997, "text": " They generally run for about 3 months.", "tokens": [814, 5101, 1190, 337, 466, 805, 2493, 13], "temperature": 0.0, "avg_logprob": -0.12246596211134786, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.5689302017563023e-05}, {"id": 406, "seek": 208316, "start": 2085.2799999999997, "end": 2089.08, "text": " It's super cool because they get archived essentially forever.", "tokens": [467, 311, 1687, 1627, 570, 436, 483, 3912, 3194, 4476, 5680, 13], "temperature": 0.0, "avg_logprob": -0.12246596211134786, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.5689302017563023e-05}, {"id": 407, "seek": 208316, "start": 2089.08, "end": 2093.2, "text": " You can download the data for them later on and find out how you would have gone in that", "tokens": [509, 393, 5484, 264, 1412, 337, 552, 1780, 322, 293, 915, 484, 577, 291, 576, 362, 2780, 294, 300], "temperature": 0.0, "avg_logprob": -0.12246596211134786, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.5689302017563023e-05}, {"id": 408, "seek": 208316, "start": 2093.2, "end": 2096.08, "text": " competition.", "tokens": [6211, 13], "temperature": 0.0, "avg_logprob": -0.12246596211134786, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.5689302017563023e-05}, {"id": 409, "seek": 208316, "start": 2096.08, "end": 2103.52, "text": " Generally speaking, if you're in the top 50%, that means you have an okay-ish model that", "tokens": [21082, 4124, 11, 498, 291, 434, 294, 264, 1192, 2625, 8923, 300, 1355, 291, 362, 364, 1392, 12, 742, 2316, 300], "temperature": 0.0, "avg_logprob": -0.12246596211134786, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.5689302017563023e-05}, {"id": 410, "seek": 208316, "start": 2103.52, "end": 2106.42, "text": " is somewhat worthwhile.", "tokens": [307, 8344, 28159, 13], "temperature": 0.0, "avg_logprob": -0.12246596211134786, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.5689302017563023e-05}, {"id": 411, "seek": 208316, "start": 2106.42, "end": 2110.64, "text": " If you're in the top 20%, that means you have a very good model.", "tokens": [759, 291, 434, 294, 264, 1192, 945, 8923, 300, 1355, 291, 362, 257, 588, 665, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12246596211134786, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.5689302017563023e-05}, {"id": 412, "seek": 211064, "start": 2110.64, "end": 2119.8399999999997, "text": " If you're in the top 10%, that means you're at an expert level for this type of problem.", "tokens": [759, 291, 434, 294, 264, 1192, 1266, 8923, 300, 1355, 291, 434, 412, 364, 5844, 1496, 337, 341, 2010, 295, 1154, 13], "temperature": 0.0, "avg_logprob": -0.11035738841141804, "compression_ratio": 1.8527918781725887, "no_speech_prob": 1.482348579884274e-06}, {"id": 413, "seek": 211064, "start": 2119.8399999999997, "end": 2124.8799999999997, "text": " If you're in the top 10%, it literally means you're one of the best in the world.", "tokens": [759, 291, 434, 294, 264, 1192, 1266, 8923, 309, 3736, 1355, 291, 434, 472, 295, 264, 1151, 294, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.11035738841141804, "compression_ratio": 1.8527918781725887, "no_speech_prob": 1.482348579884274e-06}, {"id": 414, "seek": 211064, "start": 2124.8799999999997, "end": 2128.08, "text": " Every time I've seen a Kaggle competition, I used to be president of Kaggle, so I'm very", "tokens": [2048, 565, 286, 600, 1612, 257, 48751, 22631, 6211, 11, 286, 1143, 281, 312, 3868, 295, 48751, 22631, 11, 370, 286, 478, 588], "temperature": 0.0, "avg_logprob": -0.11035738841141804, "compression_ratio": 1.8527918781725887, "no_speech_prob": 1.482348579884274e-06}, {"id": 415, "seek": 211064, "start": 2128.08, "end": 2129.3199999999997, "text": " familiar with this.", "tokens": [4963, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.11035738841141804, "compression_ratio": 1.8527918781725887, "no_speech_prob": 1.482348579884274e-06}, {"id": 416, "seek": 211064, "start": 2129.3199999999997, "end": 2134.7599999999998, "text": " Every time I've seen a Kaggle competition, at least the top 10 generally all beat the", "tokens": [2048, 565, 286, 600, 1612, 257, 48751, 22631, 6211, 11, 412, 1935, 264, 1192, 1266, 5101, 439, 4224, 264], "temperature": 0.0, "avg_logprob": -0.11035738841141804, "compression_ratio": 1.8527918781725887, "no_speech_prob": 1.482348579884274e-06}, {"id": 417, "seek": 213476, "start": 2134.76, "end": 2142.6800000000003, "text": " previous best in the world, and generally are from really good machine learning experts", "tokens": [3894, 1151, 294, 264, 1002, 11, 293, 5101, 366, 490, 534, 665, 3479, 2539, 8572], "temperature": 0.0, "avg_logprob": -0.10983554265832388, "compression_ratio": 1.652, "no_speech_prob": 6.048786417522933e-06}, {"id": 418, "seek": 213476, "start": 2142.6800000000003, "end": 2145.28, "text": " who are going beyond anything that's been done before.", "tokens": [567, 366, 516, 4399, 1340, 300, 311, 668, 1096, 949, 13], "temperature": 0.0, "avg_logprob": -0.10983554265832388, "compression_ratio": 1.652, "no_speech_prob": 6.048786417522933e-06}, {"id": 419, "seek": 213476, "start": 2145.28, "end": 2151.0, "text": " It seems that the power of competition pushes people way beyond what the previous academic", "tokens": [467, 2544, 300, 264, 1347, 295, 6211, 21020, 561, 636, 4399, 437, 264, 3894, 7778], "temperature": 0.0, "avg_logprob": -0.10983554265832388, "compression_ratio": 1.652, "no_speech_prob": 6.048786417522933e-06}, {"id": 420, "seek": 213476, "start": 2151.0, "end": 2154.5400000000004, "text": " state of the art was.", "tokens": [1785, 295, 264, 1523, 390, 13], "temperature": 0.0, "avg_logprob": -0.10983554265832388, "compression_ratio": 1.652, "no_speech_prob": 6.048786417522933e-06}, {"id": 421, "seek": 213476, "start": 2154.5400000000004, "end": 2160.44, "text": " So Kaggle is a great environment to find interesting datasets and to benchmark your own approaches.", "tokens": [407, 48751, 22631, 307, 257, 869, 2823, 281, 915, 1880, 42856, 293, 281, 18927, 428, 1065, 11587, 13], "temperature": 0.0, "avg_logprob": -0.10983554265832388, "compression_ratio": 1.652, "no_speech_prob": 6.048786417522933e-06}, {"id": 422, "seek": 213476, "start": 2160.44, "end": 2164.28, "text": " So we're going to be using it for both of these purposes.", "tokens": [407, 321, 434, 516, 281, 312, 1228, 309, 337, 1293, 295, 613, 9932, 13], "temperature": 0.0, "avg_logprob": -0.10983554265832388, "compression_ratio": 1.652, "no_speech_prob": 6.048786417522933e-06}, {"id": 423, "seek": 216428, "start": 2164.28, "end": 2169.8, "text": " Our first challenge will be dogs vs. cats.", "tokens": [2621, 700, 3430, 486, 312, 7197, 12041, 13, 11111, 13], "temperature": 0.0, "avg_logprob": -0.17045404770795036, "compression_ratio": 1.5650224215246638, "no_speech_prob": 1.497067296440946e-05}, {"id": 424, "seek": 216428, "start": 2169.8, "end": 2177.7200000000003, "text": " So sometimes on Kaggle they run competitions that are not done for lots of money, but sometimes", "tokens": [407, 2171, 322, 48751, 22631, 436, 1190, 26185, 300, 366, 406, 1096, 337, 3195, 295, 1460, 11, 457, 2171], "temperature": 0.0, "avg_logprob": -0.17045404770795036, "compression_ratio": 1.5650224215246638, "no_speech_prob": 1.497067296440946e-05}, {"id": 425, "seek": 216428, "start": 2177.7200000000003, "end": 2180.6400000000003, "text": " they're done for free or for a bit of fun.", "tokens": [436, 434, 1096, 337, 1737, 420, 337, 257, 857, 295, 1019, 13], "temperature": 0.0, "avg_logprob": -0.17045404770795036, "compression_ratio": 1.5650224215246638, "no_speech_prob": 1.497067296440946e-05}, {"id": 426, "seek": 216428, "start": 2180.6400000000003, "end": 2186.48, "text": " In this case it was actually done for a particular purpose, which was, can you create an algorithm", "tokens": [682, 341, 1389, 309, 390, 767, 1096, 337, 257, 1729, 4334, 11, 597, 390, 11, 393, 291, 1884, 364, 9284], "temperature": 0.0, "avg_logprob": -0.17045404770795036, "compression_ratio": 1.5650224215246638, "no_speech_prob": 1.497067296440946e-05}, {"id": 427, "seek": 216428, "start": 2186.48, "end": 2189.92, "text": " that can recognize the difference between dog photos and cat photos?", "tokens": [300, 393, 5521, 264, 2649, 1296, 3000, 5787, 293, 3857, 5787, 30], "temperature": 0.0, "avg_logprob": -0.17045404770795036, "compression_ratio": 1.5650224215246638, "no_speech_prob": 1.497067296440946e-05}, {"id": 428, "seek": 218992, "start": 2189.92, "end": 2197.92, "text": " The reason why was because this particular organization was using that problem as a capture,", "tokens": [440, 1778, 983, 390, 570, 341, 1729, 4475, 390, 1228, 300, 1154, 382, 257, 7983, 11], "temperature": 0.0, "avg_logprob": -0.1557485068716654, "compression_ratio": 1.5810810810810811, "no_speech_prob": 4.092866674909601e-06}, {"id": 429, "seek": 218992, "start": 2197.92, "end": 2202.32, "text": " in other words to tell the difference between humans and computers.", "tokens": [294, 661, 2283, 281, 980, 264, 2649, 1296, 6255, 293, 10807, 13], "temperature": 0.0, "avg_logprob": -0.1557485068716654, "compression_ratio": 1.5810810810810811, "no_speech_prob": 4.092866674909601e-06}, {"id": 430, "seek": 218992, "start": 2202.32, "end": 2209.64, "text": " It turned out that the state of the art machine classifiers could score 80% accuracy on this", "tokens": [467, 3574, 484, 300, 264, 1785, 295, 264, 1523, 3479, 1508, 23463, 727, 6175, 4688, 4, 14170, 322, 341], "temperature": 0.0, "avg_logprob": -0.1557485068716654, "compression_ratio": 1.5810810810810811, "no_speech_prob": 4.092866674909601e-06}, {"id": 431, "seek": 218992, "start": 2209.64, "end": 2211.44, "text": " task.", "tokens": [5633, 13], "temperature": 0.0, "avg_logprob": -0.1557485068716654, "compression_ratio": 1.5810810810810811, "no_speech_prob": 4.092866674909601e-06}, {"id": 432, "seek": 218992, "start": 2211.44, "end": 2218.88, "text": " So really this group wanted to know, can you surpass the state of the art, is this a useful", "tokens": [407, 534, 341, 1594, 1415, 281, 458, 11, 393, 291, 27650, 264, 1785, 295, 264, 1523, 11, 307, 341, 257, 4420], "temperature": 0.0, "avg_logprob": -0.1557485068716654, "compression_ratio": 1.5810810810810811, "no_speech_prob": 4.092866674909601e-06}, {"id": 433, "seek": 221888, "start": 2218.88, "end": 2227.0, "text": " capture, and then if you can surpass the state of the art, can they then use this dogs vs.", "tokens": [7983, 11, 293, 550, 498, 291, 393, 27650, 264, 1785, 295, 264, 1523, 11, 393, 436, 550, 764, 341, 7197, 12041, 13], "temperature": 0.0, "avg_logprob": -0.19624812786395734, "compression_ratio": 1.5238095238095237, "no_speech_prob": 8.66456230141921e-06}, {"id": 434, "seek": 221888, "start": 2227.0, "end": 2231.92, "text": " cats recognizer for their pet-finding work.", "tokens": [11111, 3068, 6545, 337, 641, 3817, 12, 69, 9245, 589, 13], "temperature": 0.0, "avg_logprob": -0.19624812786395734, "compression_ratio": 1.5238095238095237, "no_speech_prob": 8.66456230141921e-06}, {"id": 435, "seek": 221888, "start": 2231.92, "end": 2237.8, "text": " So really the goal here was to beat 80%.", "tokens": [407, 534, 264, 3387, 510, 390, 281, 4224, 4688, 6856], "temperature": 0.0, "avg_logprob": -0.19624812786395734, "compression_ratio": 1.5238095238095237, "no_speech_prob": 8.66456230141921e-06}, {"id": 436, "seek": 221888, "start": 2237.8, "end": 2245.36, "text": " This is a great example of the kind of thing which you could use for a thousand or a million", "tokens": [639, 307, 257, 869, 1365, 295, 264, 733, 295, 551, 597, 291, 727, 764, 337, 257, 4714, 420, 257, 2459], "temperature": 0.0, "avg_logprob": -0.19624812786395734, "compression_ratio": 1.5238095238095237, "no_speech_prob": 8.66456230141921e-06}, {"id": 437, "seek": 221888, "start": 2245.36, "end": 2246.6400000000003, "text": " different purposes.", "tokens": [819, 9932, 13], "temperature": 0.0, "avg_logprob": -0.19624812786395734, "compression_ratio": 1.5238095238095237, "no_speech_prob": 8.66456230141921e-06}, {"id": 438, "seek": 224664, "start": 2246.64, "end": 2252.2799999999997, "text": " For example, the work I did in cancer detection is this.", "tokens": [1171, 1365, 11, 264, 589, 286, 630, 294, 5592, 17784, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.1447049110166488, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.530239028914366e-06}, {"id": 439, "seek": 224664, "start": 2252.2799999999997, "end": 2259.3599999999997, "text": " So if you take a CT or an x-ray or an MRI and you say to a deep learning algorithm, these", "tokens": [407, 498, 291, 747, 257, 19529, 420, 364, 2031, 12, 3458, 420, 364, 32812, 293, 291, 584, 281, 257, 2452, 2539, 9284, 11, 613], "temperature": 0.0, "avg_logprob": -0.1447049110166488, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.530239028914366e-06}, {"id": 440, "seek": 224664, "start": 2259.3599999999997, "end": 2266.68, "text": " people have malignant cancer, these people don't, it's the same as cats vs. dogs.", "tokens": [561, 362, 2806, 36818, 5592, 11, 613, 561, 500, 380, 11, 309, 311, 264, 912, 382, 11111, 12041, 13, 7197, 13], "temperature": 0.0, "avg_logprob": -0.1447049110166488, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.530239028914366e-06}, {"id": 441, "seek": 224664, "start": 2266.68, "end": 2273.16, "text": " This is a healthy high crop yield area from satellite photos, this isn't, that's cat vs.", "tokens": [639, 307, 257, 4627, 1090, 9086, 11257, 1859, 490, 16016, 5787, 11, 341, 1943, 380, 11, 300, 311, 3857, 12041, 13], "temperature": 0.0, "avg_logprob": -0.1447049110166488, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.530239028914366e-06}, {"id": 442, "seek": 224664, "start": 2273.16, "end": 2274.16, "text": " dogs.", "tokens": [7197, 13], "temperature": 0.0, "avg_logprob": -0.1447049110166488, "compression_ratio": 1.5380952380952382, "no_speech_prob": 8.530239028914366e-06}, {"id": 443, "seek": 227416, "start": 2274.16, "end": 2279.2, "text": " If you say this is one kind of skin lesion and this is another kind of skin lesion.", "tokens": [759, 291, 584, 341, 307, 472, 733, 295, 3178, 1512, 313, 293, 341, 307, 1071, 733, 295, 3178, 1512, 313, 13], "temperature": 0.0, "avg_logprob": -0.13305147326722436, "compression_ratio": 1.9707317073170731, "no_speech_prob": 5.014661837776657e-06}, {"id": 444, "seek": 227416, "start": 2279.2, "end": 2284.2799999999997, "text": " If you say this is an abstract art painting and this is not.", "tokens": [759, 291, 584, 341, 307, 364, 12649, 1523, 5370, 293, 341, 307, 406, 13], "temperature": 0.0, "avg_logprob": -0.13305147326722436, "compression_ratio": 1.9707317073170731, "no_speech_prob": 5.014661837776657e-06}, {"id": 445, "seek": 227416, "start": 2284.2799999999997, "end": 2287.8399999999997, "text": " This is an extremely valuable painting and this is not.", "tokens": [639, 307, 364, 4664, 8263, 5370, 293, 341, 307, 406, 13], "temperature": 0.0, "avg_logprob": -0.13305147326722436, "compression_ratio": 1.9707317073170731, "no_speech_prob": 5.014661837776657e-06}, {"id": 446, "seek": 227416, "start": 2287.8399999999997, "end": 2291.68, "text": " This is a well taken photo and this is not.", "tokens": [639, 307, 257, 731, 2726, 5052, 293, 341, 307, 406, 13], "temperature": 0.0, "avg_logprob": -0.13305147326722436, "compression_ratio": 1.9707317073170731, "no_speech_prob": 5.014661837776657e-06}, {"id": 447, "seek": 227416, "start": 2291.68, "end": 2297.3199999999997, "text": " They're all image analysis problems that are generally classification problems.", "tokens": [814, 434, 439, 3256, 5215, 2740, 300, 366, 5101, 21538, 2740, 13], "temperature": 0.0, "avg_logprob": -0.13305147326722436, "compression_ratio": 1.9707317073170731, "no_speech_prob": 5.014661837776657e-06}, {"id": 448, "seek": 227416, "start": 2297.3199999999997, "end": 2300.64, "text": " These are all examples of things people have done with this kind of technology.", "tokens": [1981, 366, 439, 5110, 295, 721, 561, 362, 1096, 365, 341, 733, 295, 2899, 13], "temperature": 0.0, "avg_logprob": -0.13305147326722436, "compression_ratio": 1.9707317073170731, "no_speech_prob": 5.014661837776657e-06}, {"id": 449, "seek": 230064, "start": 2300.64, "end": 2308.2799999999997, "text": " So cats vs. dogs, it turns out, is a very powerful format.", "tokens": [407, 11111, 12041, 13, 7197, 11, 309, 4523, 484, 11, 307, 257, 588, 4005, 7877, 13], "temperature": 0.0, "avg_logprob": -0.16276410420735676, "compression_ratio": 1.5497630331753554, "no_speech_prob": 3.1875374588707928e-06}, {"id": 450, "seek": 230064, "start": 2308.2799999999997, "end": 2313.04, "text": " And so if we can learn to solve this well, we can solve all of these kinds of classification", "tokens": [400, 370, 498, 321, 393, 1466, 281, 5039, 341, 731, 11, 321, 393, 5039, 439, 295, 613, 3685, 295, 21538], "temperature": 0.0, "avg_logprob": -0.16276410420735676, "compression_ratio": 1.5497630331753554, "no_speech_prob": 3.1875374588707928e-06}, {"id": 451, "seek": 230064, "start": 2313.04, "end": 2314.24, "text": " problems.", "tokens": [2740, 13], "temperature": 0.0, "avg_logprob": -0.16276410420735676, "compression_ratio": 1.5497630331753554, "no_speech_prob": 3.1875374588707928e-06}, {"id": 452, "seek": 230064, "start": 2314.24, "end": 2320.6, "text": " Not just binary, so not just this group or that group, but also things like that skin", "tokens": [1726, 445, 17434, 11, 370, 406, 445, 341, 1594, 420, 300, 1594, 11, 457, 611, 721, 411, 300, 3178], "temperature": 0.0, "avg_logprob": -0.16276410420735676, "compression_ratio": 1.5497630331753554, "no_speech_prob": 3.1875374588707928e-06}, {"id": 453, "seek": 230064, "start": 2320.6, "end": 2321.96, "text": " lesion example.", "tokens": [1512, 313, 1365, 13], "temperature": 0.0, "avg_logprob": -0.16276410420735676, "compression_ratio": 1.5497630331753554, "no_speech_prob": 3.1875374588707928e-06}, {"id": 454, "seek": 230064, "start": 2321.96, "end": 2326.16, "text": " These are 10 different types of skin lesions, which type is it?", "tokens": [1981, 366, 1266, 819, 3467, 295, 3178, 1512, 626, 11, 597, 2010, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.16276410420735676, "compression_ratio": 1.5497630331753554, "no_speech_prob": 3.1875374588707928e-06}, {"id": 455, "seek": 232616, "start": 2326.16, "end": 2333.3199999999997, "text": " Or the crop disease example, which of these 13 crop diseases are we looking at here?", "tokens": [1610, 264, 9086, 4752, 1365, 11, 597, 295, 613, 3705, 9086, 11044, 366, 321, 1237, 412, 510, 30], "temperature": 0.0, "avg_logprob": -0.21400849883620804, "compression_ratio": 1.2758620689655173, "no_speech_prob": 8.013371370907407e-06}, {"id": 456, "seek": 232616, "start": 2333.3199999999997, "end": 2350.56, "text": " An example of an actual thing that I saw was cucumber analysis.", "tokens": [1107, 1365, 295, 364, 3539, 551, 300, 286, 1866, 390, 28725, 5215, 13], "temperature": 0.0, "avg_logprob": -0.21400849883620804, "compression_ratio": 1.2758620689655173, "no_speech_prob": 8.013371370907407e-06}, {"id": 457, "seek": 235056, "start": 2350.56, "end": 2359.96, "text": " So a Japanese cucumber farmer used this approach to deep learning to do the automatic, automated", "tokens": [407, 257, 5433, 28725, 17891, 1143, 341, 3109, 281, 2452, 2539, 281, 360, 264, 12509, 11, 18473], "temperature": 0.0, "avg_logprob": -0.20324949423472086, "compression_ratio": 1.6303317535545023, "no_speech_prob": 6.438832770072622e-06}, {"id": 458, "seek": 235056, "start": 2359.96, "end": 2364.48, "text": " all their logistics and basically had a system that would put the different grades of cucumbers", "tokens": [439, 641, 27420, 293, 1936, 632, 257, 1185, 300, 576, 829, 264, 819, 18041, 295, 43354], "temperature": 0.0, "avg_logprob": -0.20324949423472086, "compression_ratio": 1.6303317535545023, "no_speech_prob": 6.438832770072622e-06}, {"id": 459, "seek": 235056, "start": 2364.48, "end": 2370.92, "text": " into different bins automatically and make their cucumber workflow much more efficient.", "tokens": [666, 819, 41275, 6772, 293, 652, 641, 28725, 20993, 709, 544, 7148, 13], "temperature": 0.0, "avg_logprob": -0.20324949423472086, "compression_ratio": 1.6303317535545023, "no_speech_prob": 6.438832770072622e-06}, {"id": 460, "seek": 235056, "start": 2370.92, "end": 2379.92, "text": " So if that was your idea for a startup, it's already been done.", "tokens": [407, 498, 300, 390, 428, 1558, 337, 257, 18578, 11, 309, 311, 1217, 668, 1096, 13], "temperature": 0.0, "avg_logprob": -0.20324949423472086, "compression_ratio": 1.6303317535545023, "no_speech_prob": 6.438832770072622e-06}, {"id": 461, "seek": 237992, "start": 2379.92, "end": 2386.2000000000003, "text": " So they are all of our basic pieces.", "tokens": [407, 436, 366, 439, 295, 527, 3875, 3755, 13], "temperature": 0.0, "avg_logprob": -0.10903846567327326, "compression_ratio": 1.4202898550724639, "no_speech_prob": 3.0716491892235354e-05}, {"id": 462, "seek": 237992, "start": 2386.2000000000003, "end": 2399.12, "text": " So to get started, here we are with this AWS server with a pretty empty-looking set of", "tokens": [407, 281, 483, 1409, 11, 510, 321, 366, 365, 341, 17650, 7154, 365, 257, 1238, 6707, 12, 16129, 992, 295], "temperature": 0.0, "avg_logprob": -0.10903846567327326, "compression_ratio": 1.4202898550724639, "no_speech_prob": 3.0716491892235354e-05}, {"id": 463, "seek": 237992, "start": 2399.12, "end": 2400.8, "text": " notebooks here.", "tokens": [43782, 510, 13], "temperature": 0.0, "avg_logprob": -0.10903846567327326, "compression_ratio": 1.4202898550724639, "no_speech_prob": 3.0716491892235354e-05}, {"id": 464, "seek": 237992, "start": 2400.8, "end": 2406.2000000000003, "text": " So we want to go ahead and start getting some work done.", "tokens": [407, 321, 528, 281, 352, 2286, 293, 722, 1242, 512, 589, 1096, 13], "temperature": 0.0, "avg_logprob": -0.10903846567327326, "compression_ratio": 1.4202898550724639, "no_speech_prob": 3.0716491892235354e-05}, {"id": 465, "seek": 240620, "start": 2406.2, "end": 2410.08, "text": " So to do that we need to download the basic files that we need.", "tokens": [407, 281, 360, 300, 321, 643, 281, 5484, 264, 3875, 7098, 300, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.18602798831078313, "compression_ratio": 1.5971223021582734, "no_speech_prob": 2.753518674580846e-05}, {"id": 466, "seek": 240620, "start": 2410.08, "end": 2414.3199999999997, "text": " So I've sent you all of this information already.", "tokens": [407, 286, 600, 2279, 291, 439, 295, 341, 1589, 1217, 13], "temperature": 0.0, "avg_logprob": -0.18602798831078313, "compression_ratio": 1.5971223021582734, "no_speech_prob": 2.753518674580846e-05}, {"id": 467, "seek": 240620, "start": 2414.3199999999997, "end": 2420.56, "text": " All of the information you need is on our platform.ai website.", "tokens": [1057, 295, 264, 1589, 291, 643, 307, 322, 527, 3663, 13, 1301, 3144, 13], "temperature": 0.0, "avg_logprob": -0.18602798831078313, "compression_ratio": 1.5971223021582734, "no_speech_prob": 2.753518674580846e-05}, {"id": 468, "seek": 240620, "start": 2420.56, "end": 2428.08, "text": " All of the notebooks are in files slash nv's.", "tokens": [1057, 295, 264, 43782, 366, 294, 7098, 17330, 297, 85, 311, 13], "temperature": 0.0, "avg_logprob": -0.18602798831078313, "compression_ratio": 1.5971223021582734, "no_speech_prob": 2.753518674580846e-05}, {"id": 469, "seek": 242808, "start": 2428.08, "end": 2438.72, "text": " So what I'm going to do is press Control-C twice, that shuts down the notebook.", "tokens": [407, 437, 286, 478, 516, 281, 360, 307, 1886, 12912, 12, 34, 6091, 11, 300, 48590, 760, 264, 21060, 13], "temperature": 0.0, "avg_logprob": -0.20001642094102018, "compression_ratio": 1.5829145728643217, "no_speech_prob": 2.8409125661710277e-05}, {"id": 470, "seek": 242808, "start": 2438.72, "end": 2440.3199999999997, "text": " So the notebook is not running.", "tokens": [407, 264, 21060, 307, 406, 2614, 13], "temperature": 0.0, "avg_logprob": -0.20001642094102018, "compression_ratio": 1.5829145728643217, "no_speech_prob": 2.8409125661710277e-05}, {"id": 471, "seek": 242808, "start": 2440.3199999999997, "end": 2445.72, "text": " Don't worry, it saves itself automatically on a regular basis, or you can just hit S", "tokens": [1468, 380, 3292, 11, 309, 19155, 2564, 6772, 322, 257, 3890, 5143, 11, 420, 291, 393, 445, 2045, 318], "temperature": 0.0, "avg_logprob": -0.20001642094102018, "compression_ratio": 1.5829145728643217, "no_speech_prob": 2.8409125661710277e-05}, {"id": 472, "seek": 242808, "start": 2445.72, "end": 2448.24, "text": " to save it right now.", "tokens": [281, 3155, 309, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.20001642094102018, "compression_ratio": 1.5829145728643217, "no_speech_prob": 2.8409125661710277e-05}, {"id": 473, "seek": 242808, "start": 2448.24, "end": 2453.92, "text": " So shutting down the notebook, as you'll see, the Python notebook files are still sitting", "tokens": [407, 36057, 760, 264, 21060, 11, 382, 291, 603, 536, 11, 264, 15329, 21060, 7098, 366, 920, 3798], "temperature": 0.0, "avg_logprob": -0.20001642094102018, "compression_ratio": 1.5829145728643217, "no_speech_prob": 2.8409125661710277e-05}, {"id": 474, "seek": 242808, "start": 2453.92, "end": 2454.92, "text": " there.", "tokens": [456, 13], "temperature": 0.0, "avg_logprob": -0.20001642094102018, "compression_ratio": 1.5829145728643217, "no_speech_prob": 2.8409125661710277e-05}, {"id": 475, "seek": 245492, "start": 2454.92, "end": 2469.0, "text": " You can see actually that behind the scenes, they're just big bunches of JSON text, so", "tokens": [509, 393, 536, 767, 300, 2261, 264, 8026, 11, 436, 434, 445, 955, 3840, 279, 295, 31828, 2487, 11, 370], "temperature": 0.0, "avg_logprob": -0.23779025444617638, "compression_ratio": 1.3883495145631068, "no_speech_prob": 1.1300699043204077e-05}, {"id": 476, "seek": 245492, "start": 2469.0, "end": 2475.28, "text": " you can stick them in GitHub and they'll all work perfectly well.", "tokens": [291, 393, 2897, 552, 294, 23331, 293, 436, 603, 439, 589, 6239, 731, 13], "temperature": 0.0, "avg_logprob": -0.23779025444617638, "compression_ratio": 1.3883495145631068, "no_speech_prob": 1.1300699043204077e-05}, {"id": 477, "seek": 245492, "start": 2475.28, "end": 2478.08, "text": " What I generally like to do is run something called Tmux.", "tokens": [708, 286, 5101, 411, 281, 360, 307, 1190, 746, 1219, 314, 76, 2449, 13], "temperature": 0.0, "avg_logprob": -0.23779025444617638, "compression_ratio": 1.3883495145631068, "no_speech_prob": 1.1300699043204077e-05}, {"id": 478, "seek": 245492, "start": 2478.08, "end": 2482.12, "text": " How many people here have used Tmux or Screen before?", "tokens": [1012, 867, 561, 510, 362, 1143, 314, 76, 2449, 420, 25823, 949, 30], "temperature": 0.0, "avg_logprob": -0.23779025444617638, "compression_ratio": 1.3883495145631068, "no_speech_prob": 1.1300699043204077e-05}, {"id": 479, "seek": 245492, "start": 2482.12, "end": 2483.12, "text": " Less than I expected.", "tokens": [18649, 813, 286, 5176, 13], "temperature": 0.0, "avg_logprob": -0.23779025444617638, "compression_ratio": 1.3883495145631068, "no_speech_prob": 1.1300699043204077e-05}, {"id": 480, "seek": 248312, "start": 2483.12, "end": 2486.8399999999997, "text": " Okay, so those of you who haven't, you're going to love this trick.", "tokens": [1033, 11, 370, 729, 295, 291, 567, 2378, 380, 11, 291, 434, 516, 281, 959, 341, 4282, 13], "temperature": 0.0, "avg_logprob": -0.15727319215473376, "compression_ratio": 1.5907335907335907, "no_speech_prob": 4.222781626594951e-06}, {"id": 481, "seek": 248312, "start": 2486.8399999999997, "end": 2495.4, "text": " Tmux and Screen are programs that let you run programs on your server, close your terminal,", "tokens": [314, 76, 2449, 293, 25823, 366, 4268, 300, 718, 291, 1190, 4268, 322, 428, 7154, 11, 1998, 428, 14709, 11], "temperature": 0.0, "avg_logprob": -0.15727319215473376, "compression_ratio": 1.5907335907335907, "no_speech_prob": 4.222781626594951e-06}, {"id": 482, "seek": 248312, "start": 2495.4, "end": 2499.4, "text": " come back later and your program will still be running in the exact same way.", "tokens": [808, 646, 1780, 293, 428, 1461, 486, 920, 312, 2614, 294, 264, 1900, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.15727319215473376, "compression_ratio": 1.5907335907335907, "no_speech_prob": 4.222781626594951e-06}, {"id": 483, "seek": 248312, "start": 2499.4, "end": 2501.88, "text": " I don't remember if Tmux is already installed.", "tokens": [286, 500, 380, 1604, 498, 314, 76, 2449, 307, 1217, 8899, 13], "temperature": 0.0, "avg_logprob": -0.15727319215473376, "compression_ratio": 1.5907335907335907, "no_speech_prob": 4.222781626594951e-06}, {"id": 484, "seek": 248312, "start": 2501.88, "end": 2502.88, "text": " It is.", "tokens": [467, 307, 13], "temperature": 0.0, "avg_logprob": -0.15727319215473376, "compression_ratio": 1.5907335907335907, "no_speech_prob": 4.222781626594951e-06}, {"id": 485, "seek": 248312, "start": 2502.88, "end": 2507.52, "text": " So to use it, you just go Tmux.", "tokens": [407, 281, 764, 309, 11, 291, 445, 352, 314, 76, 2449, 13], "temperature": 0.0, "avg_logprob": -0.15727319215473376, "compression_ratio": 1.5907335907335907, "no_speech_prob": 4.222781626594951e-06}, {"id": 486, "seek": 248312, "start": 2507.52, "end": 2512.56, "text": " And it looks like nothing happened except a little green bar has appeared at the bottom.", "tokens": [400, 309, 1542, 411, 1825, 2011, 3993, 257, 707, 3092, 2159, 575, 8516, 412, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.15727319215473376, "compression_ratio": 1.5907335907335907, "no_speech_prob": 4.222781626594951e-06}, {"id": 487, "seek": 251256, "start": 2512.56, "end": 2520.64, "text": " But if I now hit Tmux's magic command, which is Ctrl-B, and press question mark, you can", "tokens": [583, 498, 286, 586, 2045, 314, 76, 2449, 311, 5585, 5622, 11, 597, 307, 35233, 12, 33, 11, 293, 1886, 1168, 1491, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.1735234675200089, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.81821085588308e-06}, {"id": 488, "seek": 251256, "start": 2520.64, "end": 2524.52, "text": " see there are lots of keystrokes that it has ready for me to use.", "tokens": [536, 456, 366, 3195, 295, 2141, 27616, 5993, 300, 309, 575, 1919, 337, 385, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.1735234675200089, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.81821085588308e-06}, {"id": 489, "seek": 251256, "start": 2524.52, "end": 2531.7999999999997, "text": " And so one of the ones I like is Ctrl-B double quote, which creates a second window underneath", "tokens": [400, 370, 472, 295, 264, 2306, 286, 411, 307, 35233, 12, 33, 3834, 6513, 11, 597, 7829, 257, 1150, 4910, 7223], "temperature": 0.0, "avg_logprob": -0.1735234675200089, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.81821085588308e-06}, {"id": 490, "seek": 251256, "start": 2531.7999999999997, "end": 2538.7999999999997, "text": " this one, or Ctrl-B percent, which shows a second window next to this one.", "tokens": [341, 472, 11, 420, 35233, 12, 33, 3043, 11, 597, 3110, 257, 1150, 4910, 958, 281, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.1735234675200089, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.81821085588308e-06}, {"id": 491, "seek": 253880, "start": 2538.8, "end": 2549.2400000000002, "text": " So I tend to set up a little Tmux session and get it all set up the way I want.", "tokens": [407, 286, 3928, 281, 992, 493, 257, 707, 314, 76, 2449, 5481, 293, 483, 309, 439, 992, 493, 264, 636, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.12299490928649902, "compression_ratio": 1.6525821596244132, "no_speech_prob": 8.397971214435529e-06}, {"id": 492, "seek": 253880, "start": 2549.2400000000002, "end": 2555.04, "text": " So I'm not going to go into detail about how to do everything I show you.", "tokens": [407, 286, 478, 406, 516, 281, 352, 666, 2607, 466, 577, 281, 360, 1203, 286, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.12299490928649902, "compression_ratio": 1.6525821596244132, "no_speech_prob": 8.397971214435529e-06}, {"id": 493, "seek": 253880, "start": 2555.04, "end": 2560.2400000000002, "text": " What I really want to do in the class most of the time is to say, here's something that", "tokens": [708, 286, 534, 528, 281, 360, 294, 264, 1508, 881, 295, 264, 565, 307, 281, 584, 11, 510, 311, 746, 300], "temperature": 0.0, "avg_logprob": -0.12299490928649902, "compression_ratio": 1.6525821596244132, "no_speech_prob": 8.397971214435529e-06}, {"id": 494, "seek": 253880, "start": 2560.2400000000002, "end": 2566.6800000000003, "text": " exists, here's something I recommend using, here's what it's called, and during the week", "tokens": [8198, 11, 510, 311, 746, 286, 2748, 1228, 11, 510, 311, 437, 309, 311, 1219, 11, 293, 1830, 264, 1243], "temperature": 0.0, "avg_logprob": -0.12299490928649902, "compression_ratio": 1.6525821596244132, "no_speech_prob": 8.397971214435529e-06}, {"id": 495, "seek": 253880, "start": 2566.6800000000003, "end": 2567.6800000000003, "text": " you can play with it.", "tokens": [291, 393, 862, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.12299490928649902, "compression_ratio": 1.6525821596244132, "no_speech_prob": 8.397971214435529e-06}, {"id": 496, "seek": 256768, "start": 2567.68, "end": 2570.68, "text": " You can ask questions, you can use it in your team, and so forth.", "tokens": [509, 393, 1029, 1651, 11, 291, 393, 764, 309, 294, 428, 1469, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13716826500830712, "compression_ratio": 1.437125748502994, "no_speech_prob": 7.4111990215897094e-06}, {"id": 497, "seek": 256768, "start": 2570.68, "end": 2572.48, "text": " So here it is, it's called Tmux.", "tokens": [407, 510, 309, 307, 11, 309, 311, 1219, 314, 76, 2449, 13], "temperature": 0.0, "avg_logprob": -0.13716826500830712, "compression_ratio": 1.437125748502994, "no_speech_prob": 7.4111990215897094e-06}, {"id": 498, "seek": 256768, "start": 2572.48, "end": 2575.2, "text": " This is what it does, and I'll show you something cool.", "tokens": [639, 307, 437, 309, 775, 11, 293, 286, 603, 855, 291, 746, 1627, 13], "temperature": 0.0, "avg_logprob": -0.13716826500830712, "compression_ratio": 1.437125748502994, "no_speech_prob": 7.4111990215897094e-06}, {"id": 499, "seek": 256768, "start": 2575.2, "end": 2584.3999999999996, "text": " If I now go Ctrl-B and then D to detach, close out of this altogether, it's all gone.", "tokens": [759, 286, 586, 352, 35233, 12, 33, 293, 550, 413, 281, 43245, 11, 1998, 484, 295, 341, 19051, 11, 309, 311, 439, 2780, 13], "temperature": 0.0, "avg_logprob": -0.13716826500830712, "compression_ratio": 1.437125748502994, "no_speech_prob": 7.4111990215897094e-06}, {"id": 500, "seek": 258440, "start": 2584.4, "end": 2598.6, "text": " So if I now go back into my server, I wasn't able to SSH in properly because currently", "tokens": [407, 498, 286, 586, 352, 646, 666, 452, 7154, 11, 286, 2067, 380, 1075, 281, 12238, 39, 294, 6108, 570, 4362], "temperature": 0.0, "avg_logprob": -0.25477836006566096, "compression_ratio": 1.3513513513513513, "no_speech_prob": 1.1125487617391627e-05}, {"id": 501, "seek": 258440, "start": 2598.6, "end": 2601.04, "text": " $instanceip is not defined.", "tokens": [1848, 13911, 719, 647, 307, 406, 7642, 13], "temperature": 0.0, "avg_logprob": -0.25477836006566096, "compression_ratio": 1.3513513513513513, "no_speech_prob": 1.1125487617391627e-05}, {"id": 502, "seek": 258440, "start": 2601.04, "end": 2609.1600000000003, "text": " And the reason for that is that I have to, rather than every time I start sourcing my", "tokens": [400, 264, 1778, 337, 300, 307, 300, 286, 362, 281, 11, 2831, 813, 633, 565, 286, 722, 11006, 2175, 452], "temperature": 0.0, "avg_logprob": -0.25477836006566096, "compression_ratio": 1.3513513513513513, "no_speech_prob": 1.1125487617391627e-05}, {"id": 503, "seek": 260916, "start": 2609.16, "end": 2621.66, "text": " AWS-alias.sh file, what I should do is I should go vim.bashrc.bashrc is a file that is run", "tokens": [17650, 12, 304, 4609, 13, 2716, 3991, 11, 437, 286, 820, 360, 307, 286, 820, 352, 371, 332, 13, 65, 1299, 81, 66, 13, 65, 1299, 81, 66, 307, 257, 3991, 300, 307, 1190], "temperature": 0.0, "avg_logprob": -0.18853748321533204, "compression_ratio": 1.4676258992805755, "no_speech_prob": 2.977249641844537e-05}, {"id": 504, "seek": 260916, "start": 2621.66, "end": 2625.42, "text": " every time you run bash.", "tokens": [633, 565, 291, 1190, 46183, 13], "temperature": 0.0, "avg_logprob": -0.18853748321533204, "compression_ratio": 1.4676258992805755, "no_speech_prob": 2.977249641844537e-05}, {"id": 505, "seek": 260916, "start": 2625.42, "end": 2637.3599999999997, "text": " And if I edit my bashrc file and at the end I type source AWS-alias.sh, and just to show", "tokens": [400, 498, 286, 8129, 452, 46183, 81, 66, 3991, 293, 412, 264, 917, 286, 2010, 4009, 17650, 12, 304, 4609, 13, 2716, 11, 293, 445, 281, 855], "temperature": 0.0, "avg_logprob": -0.18853748321533204, "compression_ratio": 1.4676258992805755, "no_speech_prob": 2.977249641844537e-05}, {"id": 506, "seek": 263736, "start": 2637.36, "end": 2645.6800000000003, "text": " you, I'm going to close it and reopen it.", "tokens": [291, 11, 286, 478, 516, 281, 1998, 309, 293, 33861, 309, 13], "temperature": 0.0, "avg_logprob": -0.13556376747463061, "compression_ratio": 1.5044642857142858, "no_speech_prob": 9.818202670430765e-06}, {"id": 507, "seek": 263736, "start": 2645.6800000000003, "end": 2648.34, "text": " If you're wondering why my computer is going so slowly, it's because when you have these", "tokens": [759, 291, 434, 6359, 983, 452, 3820, 307, 516, 370, 5692, 11, 309, 311, 570, 562, 291, 362, 613], "temperature": 0.0, "avg_logprob": -0.13556376747463061, "compression_ratio": 1.5044642857142858, "no_speech_prob": 9.818202670430765e-06}, {"id": 508, "seek": 263736, "start": 2648.34, "end": 2654.76, "text": " big Skype calls running at the same time as screen recording, everything slows down.", "tokens": [955, 31743, 5498, 2614, 412, 264, 912, 565, 382, 2568, 6613, 11, 1203, 35789, 760, 13], "temperature": 0.0, "avg_logprob": -0.13556376747463061, "compression_ratio": 1.5044642857142858, "no_speech_prob": 9.818202670430765e-06}, {"id": 509, "seek": 263736, "start": 2654.76, "end": 2656.92, "text": " So you can see now all those aliases are there.", "tokens": [407, 291, 393, 536, 586, 439, 729, 10198, 1957, 366, 456, 13], "temperature": 0.0, "avg_logprob": -0.13556376747463061, "compression_ratio": 1.5044642857142858, "no_speech_prob": 9.818202670430765e-06}, {"id": 510, "seek": 263736, "start": 2656.92, "end": 2663.0, "text": " So before I SSH to $instanceid, I have to find out my correct IP address.", "tokens": [407, 949, 286, 12238, 39, 281, 1848, 13911, 719, 327, 11, 286, 362, 281, 915, 484, 452, 3006, 8671, 2985, 13], "temperature": 0.0, "avg_logprob": -0.13556376747463061, "compression_ratio": 1.5044642857142858, "no_speech_prob": 9.818202670430765e-06}, {"id": 511, "seek": 266300, "start": 2663.0, "end": 2680.2, "text": " So I can say aws get p2, get my instance ID, and then we can, you know what, I'm not sure", "tokens": [407, 286, 393, 584, 1714, 82, 483, 280, 17, 11, 483, 452, 5197, 7348, 11, 293, 550, 321, 393, 11, 291, 458, 437, 11, 286, 478, 406, 988], "temperature": 0.0, "avg_logprob": -0.15952931279721466, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8342580005992204e-05}, {"id": 512, "seek": 266300, "start": 2680.2, "end": 2685.36, "text": " I've got something here to actually just get the IP address.", "tokens": [286, 600, 658, 746, 510, 281, 767, 445, 483, 264, 8671, 2985, 13], "temperature": 0.0, "avg_logprob": -0.15952931279721466, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8342580005992204e-05}, {"id": 513, "seek": 266300, "start": 2685.36, "end": 2686.36, "text": " So that's interesting.", "tokens": [407, 300, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.15952931279721466, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8342580005992204e-05}, {"id": 514, "seek": 266300, "start": 2686.36, "end": 2689.92, "text": " As you can see, I'm kind of playing with this a little bit as I go, so I'm going to go ahead", "tokens": [1018, 291, 393, 536, 11, 286, 478, 733, 295, 2433, 365, 341, 257, 707, 857, 382, 286, 352, 11, 370, 286, 478, 516, 281, 352, 2286], "temperature": 0.0, "avg_logprob": -0.15952931279721466, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8342580005992204e-05}, {"id": 515, "seek": 266300, "start": 2689.92, "end": 2692.3, "text": " and show you how to do this.", "tokens": [293, 855, 291, 577, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.15952931279721466, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8342580005992204e-05}, {"id": 516, "seek": 269230, "start": 2692.3, "end": 2698.28, "text": " So right now the IP address only gets printed out when I start an instance.", "tokens": [407, 558, 586, 264, 8671, 2985, 787, 2170, 13567, 484, 562, 286, 722, 364, 5197, 13], "temperature": 0.0, "avg_logprob": -0.2223503153811219, "compression_ratio": 1.7268041237113403, "no_speech_prob": 3.0415828859986505e-06}, {"id": 517, "seek": 269230, "start": 2698.28, "end": 2701.82, "text": " In this case, I've already got an instance running.", "tokens": [682, 341, 1389, 11, 286, 600, 1217, 658, 364, 5197, 2614, 13], "temperature": 0.0, "avg_logprob": -0.2223503153811219, "compression_ratio": 1.7268041237113403, "no_speech_prob": 3.0415828859986505e-06}, {"id": 518, "seek": 269230, "start": 2701.82, "end": 2705.0, "text": " So I'm going to edit this script and I'll change it later on.", "tokens": [407, 286, 478, 516, 281, 8129, 341, 5755, 293, 286, 603, 1319, 309, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.2223503153811219, "compression_ratio": 1.7268041237113403, "no_speech_prob": 3.0415828859986505e-06}, {"id": 519, "seek": 269230, "start": 2705.0, "end": 2708.04, "text": " Basically I'm going to create a new alias called aws.ip.", "tokens": [8537, 286, 478, 516, 281, 1884, 257, 777, 419, 4609, 1219, 1714, 82, 13, 647, 13], "temperature": 0.0, "avg_logprob": -0.2223503153811219, "compression_ratio": 1.7268041237113403, "no_speech_prob": 3.0415828859986505e-06}, {"id": 520, "seek": 269230, "start": 2708.04, "end": 2718.4, "text": " I'm going to get rid of the bit that starts the instances, get rid of the bit that waits", "tokens": [286, 478, 516, 281, 483, 3973, 295, 264, 857, 300, 3719, 264, 14519, 11, 483, 3973, 295, 264, 857, 300, 40597], "temperature": 0.0, "avg_logprob": -0.2223503153811219, "compression_ratio": 1.7268041237113403, "no_speech_prob": 3.0415828859986505e-06}, {"id": 521, "seek": 271840, "start": 2718.4, "end": 2723.88, "text": " for it to be running, and I'm just going to keep the bit that says instance.ip equals", "tokens": [337, 309, 281, 312, 2614, 11, 293, 286, 478, 445, 516, 281, 1066, 264, 857, 300, 1619, 5197, 13, 647, 6915], "temperature": 0.0, "avg_logprob": -0.2592996648840002, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.860406079387758e-06}, {"id": 522, "seek": 271840, "start": 2723.88, "end": 2729.32, "text": " something something something source aws alias.sh.", "tokens": [746, 746, 746, 4009, 1714, 82, 419, 4609, 13, 2716, 13], "temperature": 0.0, "avg_logprob": -0.2592996648840002, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.860406079387758e-06}, {"id": 523, "seek": 271840, "start": 2729.32, "end": 2740.32, "text": " And I've got a new alias called aws.ip.", "tokens": [400, 286, 600, 658, 257, 777, 419, 4609, 1219, 1714, 82, 13, 647, 13], "temperature": 0.0, "avg_logprob": -0.2592996648840002, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.860406079387758e-06}, {"id": 524, "seek": 271840, "start": 2740.32, "end": 2746.7400000000002, "text": " And now I can go SSH u1, 2 at $instance.ip.", "tokens": [400, 586, 286, 393, 352, 12238, 39, 344, 16, 11, 568, 412, 1848, 13911, 719, 13, 647, 13], "temperature": 0.0, "avg_logprob": -0.2592996648840002, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.860406079387758e-06}, {"id": 525, "seek": 274674, "start": 2746.74, "end": 2752.04, "text": " Having said all that, because my IP address is going to be the same every time and I couldn't", "tokens": [10222, 848, 439, 300, 11, 570, 452, 8671, 2985, 307, 516, 281, 312, 264, 912, 633, 565, 293, 286, 2809, 380], "temperature": 0.0, "avg_logprob": -0.14772997317106829, "compression_ratio": 1.6875, "no_speech_prob": 6.747982752131065e-06}, {"id": 526, "seek": 274674, "start": 2752.04, "end": 2757.4399999999996, "text": " really be bothered waiting for all that, I'm actually going to manually put my IP address", "tokens": [534, 312, 22996, 3806, 337, 439, 300, 11, 286, 478, 767, 516, 281, 16945, 829, 452, 8671, 2985], "temperature": 0.0, "avg_logprob": -0.14772997317106829, "compression_ratio": 1.6875, "no_speech_prob": 6.747982752131065e-06}, {"id": 527, "seek": 274674, "start": 2757.4399999999996, "end": 2758.4399999999996, "text": " in here.", "tokens": [294, 510, 13], "temperature": 0.0, "avg_logprob": -0.14772997317106829, "compression_ratio": 1.6875, "no_speech_prob": 6.747982752131065e-06}, {"id": 528, "seek": 274674, "start": 2758.4399999999996, "end": 2765.4799999999996, "text": " So the next time I run this, I can just press up arrow and rerun that command.", "tokens": [407, 264, 958, 565, 286, 1190, 341, 11, 286, 393, 445, 1886, 493, 11610, 293, 43819, 409, 300, 5622, 13], "temperature": 0.0, "avg_logprob": -0.14772997317106829, "compression_ratio": 1.6875, "no_speech_prob": 6.747982752131065e-06}, {"id": 529, "seek": 274674, "start": 2765.4799999999996, "end": 2771.08, "text": " So I'm kind of showing you lots of ways of doing things so you can decide what your own", "tokens": [407, 286, 478, 733, 295, 4099, 291, 3195, 295, 2098, 295, 884, 721, 370, 291, 393, 4536, 437, 428, 1065], "temperature": 0.0, "avg_logprob": -0.14772997317106829, "compression_ratio": 1.6875, "no_speech_prob": 6.747982752131065e-06}, {"id": 530, "seek": 274674, "start": 2771.08, "end": 2773.24, "text": " workflow is like or come up with better ones.", "tokens": [20993, 307, 411, 420, 808, 493, 365, 1101, 2306, 13], "temperature": 0.0, "avg_logprob": -0.14772997317106829, "compression_ratio": 1.6875, "no_speech_prob": 6.747982752131065e-06}, {"id": 531, "seek": 274674, "start": 2773.24, "end": 2774.52, "text": " But here's the cool thing.", "tokens": [583, 510, 311, 264, 1627, 551, 13], "temperature": 0.0, "avg_logprob": -0.14772997317106829, "compression_ratio": 1.6875, "no_speech_prob": 6.747982752131065e-06}, {"id": 532, "seek": 277452, "start": 2774.52, "end": 2784.04, "text": " I am back in my box here, and then if I say tmux attach, I am exactly back to where I", "tokens": [286, 669, 646, 294, 452, 2424, 510, 11, 293, 550, 498, 286, 584, 256, 76, 2449, 5085, 11, 286, 669, 2293, 646, 281, 689, 286], "temperature": 0.0, "avg_logprob": -0.14911453367218258, "compression_ratio": 1.7292418772563176, "no_speech_prob": 4.425454790180083e-06}, {"id": 533, "seek": 277452, "start": 2784.04, "end": 2785.04, "text": " came from.", "tokens": [1361, 490, 13], "temperature": 0.0, "avg_logprob": -0.14911453367218258, "compression_ratio": 1.7292418772563176, "no_speech_prob": 4.425454790180083e-06}, {"id": 534, "seek": 277452, "start": 2785.04, "end": 2788.7599999999998, "text": " So whatever I had running and whatever state it was, it's still sitting there.", "tokens": [407, 2035, 286, 632, 2614, 293, 2035, 1785, 309, 390, 11, 309, 311, 920, 3798, 456, 13], "temperature": 0.0, "avg_logprob": -0.14911453367218258, "compression_ratio": 1.7292418772563176, "no_speech_prob": 4.425454790180083e-06}, {"id": 535, "seek": 277452, "start": 2788.7599999999998, "end": 2792.28, "text": " So the particularly cool thing is that any notebooks, kernels I had running, they're", "tokens": [407, 264, 4098, 1627, 551, 307, 300, 604, 43782, 11, 23434, 1625, 286, 632, 2614, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.14911453367218258, "compression_ratio": 1.7292418772563176, "no_speech_prob": 4.425454790180083e-06}, {"id": 536, "seek": 277452, "start": 2792.28, "end": 2795.08, "text": " all still sitting there.", "tokens": [439, 920, 3798, 456, 13], "temperature": 0.0, "avg_logprob": -0.14911453367218258, "compression_ratio": 1.7292418772563176, "no_speech_prob": 4.425454790180083e-06}, {"id": 537, "seek": 277452, "start": 2795.08, "end": 2799.04, "text": " This is particularly helpful if you run something like those OVH servers or one of your own", "tokens": [639, 307, 4098, 4961, 498, 291, 1190, 746, 411, 729, 422, 53, 39, 15909, 420, 472, 295, 428, 1065], "temperature": 0.0, "avg_logprob": -0.14911453367218258, "compression_ratio": 1.7292418772563176, "no_speech_prob": 4.425454790180083e-06}, {"id": 538, "seek": 277452, "start": 2799.04, "end": 2800.04, "text": " servers.", "tokens": [15909, 13], "temperature": 0.0, "avg_logprob": -0.14911453367218258, "compression_ratio": 1.7292418772563176, "no_speech_prob": 4.425454790180083e-06}, {"id": 539, "seek": 277452, "start": 2800.04, "end": 2804.4, "text": " With AWS, it's a little less helpful because you really need to shut it down to avoid paying", "tokens": [2022, 17650, 11, 309, 311, 257, 707, 1570, 4961, 570, 291, 534, 643, 281, 5309, 309, 760, 281, 5042, 6229], "temperature": 0.0, "avg_logprob": -0.14911453367218258, "compression_ratio": 1.7292418772563176, "no_speech_prob": 4.425454790180083e-06}, {"id": 540, "seek": 280440, "start": 2804.4, "end": 2805.4, "text": " the money.", "tokens": [264, 1460, 13], "temperature": 0.0, "avg_logprob": -0.14858015908135308, "compression_ratio": 1.5106382978723405, "no_speech_prob": 1.5936369891278446e-05}, {"id": 541, "seek": 280440, "start": 2805.4, "end": 2811.7200000000003, "text": " But if you've got something you can keep running, all the MSAM students will have access to", "tokens": [583, 498, 291, 600, 658, 746, 291, 393, 1066, 2614, 11, 439, 264, 7395, 2865, 1731, 486, 362, 2105, 281], "temperature": 0.0, "avg_logprob": -0.14858015908135308, "compression_ratio": 1.5106382978723405, "no_speech_prob": 1.5936369891278446e-05}, {"id": 542, "seek": 280440, "start": 2811.7200000000003, "end": 2819.0, "text": " the GPU server we have here at the university, which is particularly helpful for you guys.", "tokens": [264, 18407, 7154, 321, 362, 510, 412, 264, 5454, 11, 597, 307, 4098, 4961, 337, 291, 1074, 13], "temperature": 0.0, "avg_logprob": -0.14858015908135308, "compression_ratio": 1.5106382978723405, "no_speech_prob": 1.5936369891278446e-05}, {"id": 543, "seek": 280440, "start": 2819.0, "end": 2825.04, "text": " So I actually tend to use this little bottom right-hand window to permanently have Jupyter", "tokens": [407, 286, 767, 3928, 281, 764, 341, 707, 2767, 558, 12, 5543, 4910, 281, 24042, 362, 22125, 88, 391], "temperature": 0.0, "avg_logprob": -0.14858015908135308, "compression_ratio": 1.5106382978723405, "no_speech_prob": 1.5936369891278446e-05}, {"id": 544, "seek": 280440, "start": 2825.04, "end": 2826.7200000000003, "text": " Notebook running.", "tokens": [11633, 2939, 2614, 13], "temperature": 0.0, "avg_logprob": -0.14858015908135308, "compression_ratio": 1.5106382978723405, "no_speech_prob": 1.5936369891278446e-05}, {"id": 545, "seek": 280440, "start": 2826.7200000000003, "end": 2829.64, "text": " So that's kind of like my particular way of running.", "tokens": [407, 300, 311, 733, 295, 411, 452, 1729, 636, 295, 2614, 13], "temperature": 0.0, "avg_logprob": -0.14858015908135308, "compression_ratio": 1.5106382978723405, "no_speech_prob": 1.5936369891278446e-05}, {"id": 546, "seek": 282964, "start": 2829.64, "end": 2836.2799999999997, "text": " And then I tend to use this left-hand window to do other things.", "tokens": [400, 550, 286, 3928, 281, 764, 341, 1411, 12, 5543, 4910, 281, 360, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.1607693981479954, "compression_ratio": 1.2075471698113207, "no_speech_prob": 1.9222738046664745e-05}, {"id": 547, "seek": 282964, "start": 2836.2799999999997, "end": 2849.52, "text": " And in particular, I'm going to go ahead and grab my notebooks.", "tokens": [400, 294, 1729, 11, 286, 478, 516, 281, 352, 2286, 293, 4444, 452, 43782, 13], "temperature": 0.0, "avg_logprob": -0.1607693981479954, "compression_ratio": 1.2075471698113207, "no_speech_prob": 1.9222738046664745e-05}, {"id": 548, "seek": 284952, "start": 2849.52, "end": 2859.92, "text": " So the easiest way to grab things is with Wget, so if I go Wget, I now have a notebook,", "tokens": [407, 264, 12889, 636, 281, 4444, 721, 307, 365, 343, 847, 11, 370, 498, 286, 352, 343, 847, 11, 286, 586, 362, 257, 21060, 11], "temperature": 0.0, "avg_logprob": -0.23158530865685414, "compression_ratio": 1.360655737704918, "no_speech_prob": 2.586649134173058e-05}, {"id": 549, "seek": 284952, "start": 2859.92, "end": 2860.92, "text": " Lesson 1.", "tokens": [18649, 266, 502, 13], "temperature": 0.0, "avg_logprob": -0.23158530865685414, "compression_ratio": 1.360655737704918, "no_speech_prob": 2.586649134173058e-05}, {"id": 550, "seek": 284952, "start": 2860.92, "end": 2866.7599999999998, "text": " So if I go back to my Jupyter Notebook, see it's appeared, Lesson 1.", "tokens": [407, 498, 286, 352, 646, 281, 452, 22125, 88, 391, 11633, 2939, 11, 536, 309, 311, 8516, 11, 18649, 266, 502, 13], "temperature": 0.0, "avg_logprob": -0.23158530865685414, "compression_ratio": 1.360655737704918, "no_speech_prob": 2.586649134173058e-05}, {"id": 551, "seek": 286676, "start": 2866.76, "end": 2881.7200000000003, "text": " If I click on it, if you're using a T2 instance, the free ones, generally speaking, particularly", "tokens": [759, 286, 2052, 322, 309, 11, 498, 291, 434, 1228, 257, 314, 17, 5197, 11, 264, 1737, 2306, 11, 5101, 4124, 11, 4098], "temperature": 0.0, "avg_logprob": -0.1675108632733745, "compression_ratio": 1.5585585585585586, "no_speech_prob": 6.144071903690929e-06}, {"id": 552, "seek": 286676, "start": 2881.7200000000003, "end": 2885.2400000000002, "text": " the first time you run something, it can take quite a long time to open.", "tokens": [264, 700, 565, 291, 1190, 746, 11, 309, 393, 747, 1596, 257, 938, 565, 281, 1269, 13], "temperature": 0.0, "avg_logprob": -0.1675108632733745, "compression_ratio": 1.5585585585585586, "no_speech_prob": 6.144071903690929e-06}, {"id": 553, "seek": 286676, "start": 2885.2400000000002, "end": 2888.84, "text": " You should find the second time it's quite fast, by the way.", "tokens": [509, 820, 915, 264, 1150, 565, 309, 311, 1596, 2370, 11, 538, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.1675108632733745, "compression_ratio": 1.5585585585585586, "no_speech_prob": 6.144071903690929e-06}, {"id": 554, "seek": 286676, "start": 2888.84, "end": 2891.4, "text": " So here is our notebook.", "tokens": [407, 510, 307, 527, 21060, 13], "temperature": 0.0, "avg_logprob": -0.1675108632733745, "compression_ratio": 1.5585585585585586, "no_speech_prob": 6.144071903690929e-06}, {"id": 555, "seek": 286676, "start": 2891.4, "end": 2895.0, "text": " So hopefully quite a few of you have already got to the point today that you can see this.", "tokens": [407, 4696, 1596, 257, 1326, 295, 291, 362, 1217, 658, 281, 264, 935, 965, 300, 291, 393, 536, 341, 13], "temperature": 0.0, "avg_logprob": -0.1675108632733745, "compression_ratio": 1.5585585585585586, "no_speech_prob": 6.144071903690929e-06}, {"id": 556, "seek": 289500, "start": 2895.0, "end": 2900.84, "text": " Those of you that haven't will get plenty of help during the week.", "tokens": [3950, 295, 291, 300, 2378, 380, 486, 483, 7140, 295, 854, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.16310208941262866, "compression_ratio": 1.4156626506024097, "no_speech_prob": 5.682366463588551e-06}, {"id": 557, "seek": 289500, "start": 2900.84, "end": 2907.28, "text": " This particular notebook uses two external scripts to help.", "tokens": [639, 1729, 21060, 4960, 732, 8320, 23294, 281, 854, 13], "temperature": 0.0, "avg_logprob": -0.16310208941262866, "compression_ratio": 1.4156626506024097, "no_speech_prob": 5.682366463588551e-06}, {"id": 558, "seek": 289500, "start": 2907.28, "end": 2914.04, "text": " Those scripts are called utils and vgg16.", "tokens": [3950, 23294, 366, 1219, 2839, 4174, 293, 371, 1615, 6866, 13], "temperature": 0.0, "avg_logprob": -0.16310208941262866, "compression_ratio": 1.4156626506024097, "no_speech_prob": 5.682366463588551e-06}, {"id": 559, "seek": 289500, "start": 2914.04, "end": 2919.68, "text": " So the last thing I'm going to do before I break is to grab those.", "tokens": [407, 264, 1036, 551, 286, 478, 516, 281, 360, 949, 286, 1821, 307, 281, 4444, 729, 13], "temperature": 0.0, "avg_logprob": -0.16310208941262866, "compression_ratio": 1.4156626506024097, "no_speech_prob": 5.682366463588551e-06}, {"id": 560, "seek": 291968, "start": 2919.68, "end": 2925.96, "text": " So I'm going to go Wget and just pop these all in this notebooks directory, so they're", "tokens": [407, 286, 478, 516, 281, 352, 343, 847, 293, 445, 1665, 613, 439, 294, 341, 43782, 21120, 11, 370, 436, 434], "temperature": 0.0, "avg_logprob": -0.23921154163501882, "compression_ratio": 1.3383458646616542, "no_speech_prob": 1.1842964340758044e-05}, {"id": 561, "seek": 291968, "start": 2925.96, "end": 2935.16, "text": " all in the same place.", "tokens": [439, 294, 264, 912, 1081, 13], "temperature": 0.0, "avg_logprob": -0.23921154163501882, "compression_ratio": 1.3383458646616542, "no_speech_prob": 1.1842964340758044e-05}, {"id": 562, "seek": 291968, "start": 2935.16, "end": 2944.7999999999997, "text": " Then unzip them.", "tokens": [1396, 517, 27268, 552, 13], "temperature": 0.0, "avg_logprob": -0.23921154163501882, "compression_ratio": 1.3383458646616542, "no_speech_prob": 1.1842964340758044e-05}, {"id": 563, "seek": 291968, "start": 2944.7999999999997, "end": 2947.54, "text": " And then the only other thing you need is the data.", "tokens": [400, 550, 264, 787, 661, 551, 291, 643, 307, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.23921154163501882, "compression_ratio": 1.3383458646616542, "no_speech_prob": 1.1842964340758044e-05}, {"id": 564, "seek": 294754, "start": 2947.54, "end": 2954.02, "text": " So the data sits in the platform.ai data directory.", "tokens": [407, 264, 1412, 12696, 294, 264, 3663, 13, 1301, 1412, 21120, 13], "temperature": 0.0, "avg_logprob": -0.1432145917138388, "compression_ratio": 1.616580310880829, "no_speech_prob": 2.1112158719915897e-05}, {"id": 565, "seek": 294754, "start": 2954.02, "end": 2956.0, "text": " And the data is all the dogs and cats.", "tokens": [400, 264, 1412, 307, 439, 264, 7197, 293, 11111, 13], "temperature": 0.0, "avg_logprob": -0.1432145917138388, "compression_ratio": 1.616580310880829, "no_speech_prob": 2.1112158719915897e-05}, {"id": 566, "seek": 294754, "start": 2956.0, "end": 2961.48, "text": " Now I've taken the Kaggle data and made changes to it, which I'm going to be showing you.", "tokens": [823, 286, 600, 2726, 264, 48751, 22631, 1412, 293, 1027, 2962, 281, 309, 11, 597, 286, 478, 516, 281, 312, 4099, 291, 13], "temperature": 0.0, "avg_logprob": -0.1432145917138388, "compression_ratio": 1.616580310880829, "no_speech_prob": 2.1112158719915897e-05}, {"id": 567, "seek": 294754, "start": 2961.48, "end": 2966.4, "text": " So rather than downloading it from Kaggle, I suggest you grab it from platform.ai and", "tokens": [407, 2831, 813, 32529, 309, 490, 48751, 22631, 11, 286, 3402, 291, 4444, 309, 490, 3663, 13, 1301, 293], "temperature": 0.0, "avg_logprob": -0.1432145917138388, "compression_ratio": 1.616580310880829, "no_speech_prob": 2.1112158719915897e-05}, {"id": 568, "seek": 294754, "start": 2966.4, "end": 2971.56, "text": " I've sent you this information today as well.", "tokens": [286, 600, 2279, 291, 341, 1589, 965, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1432145917138388, "compression_ratio": 1.616580310880829, "no_speech_prob": 2.1112158719915897e-05}, {"id": 569, "seek": 297156, "start": 2971.56, "end": 2978.84, "text": " So I'm going to cd into data and Wget that as well.", "tokens": [407, 286, 478, 516, 281, 269, 67, 666, 1412, 293, 343, 847, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.17022830963134766, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.994391707034083e-06}, {"id": 570, "seek": 297156, "start": 2978.84, "end": 2980.4, "text": " And so that's going to run for a few minutes.", "tokens": [400, 370, 300, 311, 516, 281, 1190, 337, 257, 1326, 2077, 13], "temperature": 0.0, "avg_logprob": -0.17022830963134766, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.994391707034083e-06}, {"id": 571, "seek": 297156, "start": 2980.4, "end": 2985.48, "text": " So while it does, I think it's a good time to have a break.", "tokens": [407, 1339, 309, 775, 11, 286, 519, 309, 311, 257, 665, 565, 281, 362, 257, 1821, 13], "temperature": 0.0, "avg_logprob": -0.17022830963134766, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.994391707034083e-06}, {"id": 572, "seek": 297156, "start": 2985.48, "end": 2994.52, "text": " Let's have a 10-minute break during which you are welcome to keep going with your install,", "tokens": [961, 311, 362, 257, 1266, 12, 18256, 1821, 1830, 597, 291, 366, 2928, 281, 1066, 516, 365, 428, 3625, 11], "temperature": 0.0, "avg_logprob": -0.17022830963134766, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.994391707034083e-06}, {"id": 573, "seek": 297156, "start": 2994.52, "end": 2999.52, "text": " chat to myself or Rachel or Tara or Yad, if you've got any questions, or get to know your", "tokens": [5081, 281, 2059, 420, 14246, 420, 32182, 420, 398, 345, 11, 498, 291, 600, 658, 604, 1651, 11, 420, 483, 281, 458, 428], "temperature": 0.0, "avg_logprob": -0.17022830963134766, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.994391707034083e-06}, {"id": 574, "seek": 299952, "start": 2999.52, "end": 3002.24, "text": " team or just go and have a coffee.", "tokens": [1469, 420, 445, 352, 293, 362, 257, 4982, 13], "temperature": 0.0, "avg_logprob": -0.1697526985490826, "compression_ratio": 1.4385964912280702, "no_speech_prob": 7.889108019298874e-06}, {"id": 575, "seek": 299952, "start": 3002.24, "end": 3013.92, "text": " So let's get back together at 5 past 8.", "tokens": [407, 718, 311, 483, 646, 1214, 412, 1025, 1791, 1649, 13], "temperature": 0.0, "avg_logprob": -0.1697526985490826, "compression_ratio": 1.4385964912280702, "no_speech_prob": 7.889108019298874e-06}, {"id": 576, "seek": 299952, "start": 3013.92, "end": 3018.12, "text": " The previous section, I think, for some of you, and I was chatting to a couple of you", "tokens": [440, 3894, 3541, 11, 286, 519, 11, 337, 512, 295, 291, 11, 293, 286, 390, 24654, 281, 257, 1916, 295, 291], "temperature": 0.0, "avg_logprob": -0.1697526985490826, "compression_ratio": 1.4385964912280702, "no_speech_prob": 7.889108019298874e-06}, {"id": 577, "seek": 299952, "start": 3018.12, "end": 3024.7599999999998, "text": " during the break, was a bit of a fire hose of information because it was like, here's", "tokens": [1830, 264, 1821, 11, 390, 257, 857, 295, 257, 2610, 20061, 295, 1589, 570, 309, 390, 411, 11, 510, 311], "temperature": 0.0, "avg_logprob": -0.1697526985490826, "compression_ratio": 1.4385964912280702, "no_speech_prob": 7.889108019298874e-06}, {"id": 578, "seek": 302476, "start": 3024.76, "end": 3031.7200000000003, "text": " bash, here's AWS, here's Kaggle, here's GPUs, blah blah blah.", "tokens": [46183, 11, 510, 311, 17650, 11, 510, 311, 48751, 22631, 11, 510, 311, 18407, 82, 11, 12288, 12288, 12288, 13], "temperature": 0.0, "avg_logprob": -0.19555253693551727, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3922539185150526e-05}, {"id": 579, "seek": 302476, "start": 3031.7200000000003, "end": 3035.1200000000003, "text": " For some of you, it was probably really boring.", "tokens": [1171, 512, 295, 291, 11, 309, 390, 1391, 534, 9989, 13], "temperature": 0.0, "avg_logprob": -0.19555253693551727, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3922539185150526e-05}, {"id": 580, "seek": 302476, "start": 3035.1200000000003, "end": 3038.88, "text": " Most practicing data scientists probably are using all of those things already.", "tokens": [4534, 11350, 1412, 7708, 1391, 366, 1228, 439, 295, 729, 721, 1217, 13], "temperature": 0.0, "avg_logprob": -0.19555253693551727, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3922539185150526e-05}, {"id": 581, "seek": 302476, "start": 3038.88, "end": 3045.28, "text": " So if you're at one extreme of the, holy shit, that was a fire hose of information, don't", "tokens": [407, 498, 291, 434, 412, 472, 8084, 295, 264, 11, 10622, 4611, 11, 300, 390, 257, 2610, 20061, 295, 1589, 11, 500, 380], "temperature": 0.0, "avg_logprob": -0.19555253693551727, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3922539185150526e-05}, {"id": 582, "seek": 302476, "start": 3045.28, "end": 3049.44, "text": " worry, we have all week to get through it.", "tokens": [3292, 11, 321, 362, 439, 1243, 281, 483, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.19555253693551727, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3922539185150526e-05}, {"id": 583, "seek": 302476, "start": 3049.44, "end": 3051.76, "text": " You'll have the video tomorrow.", "tokens": [509, 603, 362, 264, 960, 4153, 13], "temperature": 0.0, "avg_logprob": -0.19555253693551727, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.3922539185150526e-05}, {"id": 584, "seek": 305176, "start": 3051.76, "end": 3055.7200000000003, "text": " And by the time you're here again next week, I want to make sure that everybody who has", "tokens": [400, 538, 264, 565, 291, 434, 510, 797, 958, 1243, 11, 286, 528, 281, 652, 988, 300, 2201, 567, 575], "temperature": 0.0, "avg_logprob": -0.13513865236376152, "compression_ratio": 1.7622641509433963, "no_speech_prob": 1.451006482966477e-05}, {"id": 585, "seek": 305176, "start": 3055.7200000000003, "end": 3061.92, "text": " the time and interest to work hard on it has got through all of the material.", "tokens": [264, 565, 293, 1179, 281, 589, 1152, 322, 309, 575, 658, 807, 439, 295, 264, 2527, 13], "temperature": 0.0, "avg_logprob": -0.13513865236376152, "compression_ratio": 1.7622641509433963, "no_speech_prob": 1.451006482966477e-05}, {"id": 586, "seek": 305176, "start": 3061.92, "end": 3067.2000000000003, "text": " If you haven't, like you're a couple of days, maybe it's early on the weekend and you're", "tokens": [759, 291, 2378, 380, 11, 411, 291, 434, 257, 1916, 295, 1708, 11, 1310, 309, 311, 2440, 322, 264, 6711, 293, 291, 434], "temperature": 0.0, "avg_logprob": -0.13513865236376152, "compression_ratio": 1.7622641509433963, "no_speech_prob": 1.451006482966477e-05}, {"id": 587, "seek": 305176, "start": 3067.2000000000003, "end": 3071.6400000000003, "text": " thinking, I'm not going to get there, please let Rachel and I know that we will work with", "tokens": [1953, 11, 286, 478, 406, 516, 281, 483, 456, 11, 1767, 718, 14246, 293, 286, 458, 300, 321, 486, 589, 365], "temperature": 0.0, "avg_logprob": -0.13513865236376152, "compression_ratio": 1.7622641509433963, "no_speech_prob": 1.451006482966477e-05}, {"id": 588, "seek": 305176, "start": 3071.6400000000003, "end": 3075.2000000000003, "text": " you in person to get you there.", "tokens": [291, 294, 954, 281, 483, 291, 456, 13], "temperature": 0.0, "avg_logprob": -0.13513865236376152, "compression_ratio": 1.7622641509433963, "no_speech_prob": 1.451006482966477e-05}, {"id": 589, "seek": 305176, "start": 3075.2000000000003, "end": 3080.46, "text": " Everybody who puts the time in, I am determined to make sure can get through the material.", "tokens": [7646, 567, 8137, 264, 565, 294, 11, 286, 669, 9540, 281, 652, 988, 393, 483, 807, 264, 2527, 13], "temperature": 0.0, "avg_logprob": -0.13513865236376152, "compression_ratio": 1.7622641509433963, "no_speech_prob": 1.451006482966477e-05}, {"id": 590, "seek": 308046, "start": 3080.46, "end": 3085.52, "text": " If you don't really have the background and you don't really have the time, that's fine.", "tokens": [759, 291, 500, 380, 534, 362, 264, 3678, 293, 291, 500, 380, 534, 362, 264, 565, 11, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.18405885355813162, "compression_ratio": 1.7423076923076923, "no_speech_prob": 1.0952927368634846e-05}, {"id": 591, "seek": 308046, "start": 3085.52, "end": 3088.36, "text": " Maybe you won't get through all the material.", "tokens": [2704, 291, 1582, 380, 483, 807, 439, 264, 2527, 13], "temperature": 0.0, "avg_logprob": -0.18405885355813162, "compression_ratio": 1.7423076923076923, "no_speech_prob": 1.0952927368634846e-05}, {"id": 592, "seek": 308046, "start": 3088.36, "end": 3093.44, "text": " But I really am determined that everybody who's prepared and able to put in the time", "tokens": [583, 286, 534, 669, 9540, 300, 2201, 567, 311, 4927, 293, 1075, 281, 829, 294, 264, 565], "temperature": 0.0, "avg_logprob": -0.18405885355813162, "compression_ratio": 1.7423076923076923, "no_speech_prob": 1.0952927368634846e-05}, {"id": 593, "seek": 308046, "start": 3093.44, "end": 3094.9, "text": " can get through everything.", "tokens": [393, 483, 807, 1203, 13], "temperature": 0.0, "avg_logprob": -0.18405885355813162, "compression_ratio": 1.7423076923076923, "no_speech_prob": 1.0952927368634846e-05}, {"id": 594, "seek": 308046, "start": 3094.9, "end": 3102.08, "text": " So between the community resources and the video and Rachel and I and the folks at Taro", "tokens": [407, 1296, 264, 1768, 3593, 293, 264, 960, 293, 14246, 293, 286, 293, 264, 4024, 412, 314, 9708], "temperature": 0.0, "avg_logprob": -0.18405885355813162, "compression_ratio": 1.7423076923076923, "no_speech_prob": 1.0952927368634846e-05}, {"id": 595, "seek": 308046, "start": 3102.08, "end": 3105.9, "text": " and Yad, we will help everybody.", "tokens": [293, 398, 345, 11, 321, 486, 854, 2201, 13], "temperature": 0.0, "avg_logprob": -0.18405885355813162, "compression_ratio": 1.7423076923076923, "no_speech_prob": 1.0952927368634846e-05}, {"id": 596, "seek": 308046, "start": 3105.9, "end": 3109.28, "text": " To those of you who are kind of practicing data scientists and you are familiar with", "tokens": [1407, 729, 295, 291, 567, 366, 733, 295, 11350, 1412, 7708, 293, 291, 366, 4963, 365], "temperature": 0.0, "avg_logprob": -0.18405885355813162, "compression_ratio": 1.7423076923076923, "no_speech_prob": 1.0952927368634846e-05}, {"id": 597, "seek": 310928, "start": 3109.28, "end": 3117.0, "text": " all of these pieces, I apologize that it will be a bit slow for you and hopefully as we", "tokens": [439, 295, 613, 3755, 11, 286, 12328, 300, 309, 486, 312, 257, 857, 2964, 337, 291, 293, 4696, 382, 321], "temperature": 0.0, "avg_logprob": -0.13552627768567813, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.710695298446808e-05}, {"id": 598, "seek": 310928, "start": 3117.0, "end": 3119.88, "text": " move along there will be more and more new stuff.", "tokens": [1286, 2051, 456, 486, 312, 544, 293, 544, 777, 1507, 13], "temperature": 0.0, "avg_logprob": -0.13552627768567813, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.710695298446808e-05}, {"id": 599, "seek": 310928, "start": 3119.88, "end": 3124.7200000000003, "text": " I'm kind of hoping that for those of you who already have some level of expertise, we will", "tokens": [286, 478, 733, 295, 7159, 300, 337, 729, 295, 291, 567, 1217, 362, 512, 1496, 295, 11769, 11, 321, 486], "temperature": 0.0, "avg_logprob": -0.13552627768567813, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.710695298446808e-05}, {"id": 600, "seek": 310928, "start": 3124.7200000000003, "end": 3129.5600000000004, "text": " continually give you ways that you can go further.", "tokens": [22277, 976, 291, 2098, 300, 291, 393, 352, 3052, 13], "temperature": 0.0, "avg_logprob": -0.13552627768567813, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.710695298446808e-05}, {"id": 601, "seek": 310928, "start": 3129.5600000000004, "end": 3134.6000000000004, "text": " So for example, at the moment I'm thinking, can you help us with these scripts to make", "tokens": [407, 337, 1365, 11, 412, 264, 1623, 286, 478, 1953, 11, 393, 291, 854, 505, 365, 613, 23294, 281, 652], "temperature": 0.0, "avg_logprob": -0.13552627768567813, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.710695298446808e-05}, {"id": 602, "seek": 313460, "start": 3134.6, "end": 3141.44, "text": " them better, to make them simpler, to make them more powerful, to create Azure versions", "tokens": [552, 1101, 11, 281, 652, 552, 18587, 11, 281, 652, 552, 544, 4005, 11, 281, 1884, 11969, 9606], "temperature": 0.0, "avg_logprob": -0.1280563780404989, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.72307227330748e-05}, {"id": 603, "seek": 313460, "start": 3141.44, "end": 3143.56, "text": " of them.", "tokens": [295, 552, 13], "temperature": 0.0, "avg_logprob": -0.1280563780404989, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.72307227330748e-05}, {"id": 604, "seek": 313460, "start": 3143.56, "end": 3147.24, "text": " All the stuff that we're doing to try and make deep learning as accessible as possible,", "tokens": [1057, 264, 1507, 300, 321, 434, 884, 281, 853, 293, 652, 2452, 2539, 382, 9515, 382, 1944, 11], "temperature": 0.0, "avg_logprob": -0.1280563780404989, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.72307227330748e-05}, {"id": 605, "seek": 313460, "start": 3147.24, "end": 3148.56, "text": " can you help contribute to that?", "tokens": [393, 291, 854, 10586, 281, 300, 30], "temperature": 0.0, "avg_logprob": -0.1280563780404989, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.72307227330748e-05}, {"id": 606, "seek": 313460, "start": 3148.56, "end": 3150.7599999999998, "text": " Can you help contribute to the Wiki?", "tokens": [1664, 291, 854, 10586, 281, 264, 35892, 30], "temperature": 0.0, "avg_logprob": -0.1280563780404989, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.72307227330748e-05}, {"id": 607, "seek": 313460, "start": 3150.7599999999998, "end": 3157.08, "text": " So for those of you who already have quite a high level of expertise, I'm really looking", "tokens": [407, 337, 729, 295, 291, 567, 1217, 362, 1596, 257, 1090, 1496, 295, 11769, 11, 286, 478, 534, 1237], "temperature": 0.0, "avg_logprob": -0.1280563780404989, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.72307227330748e-05}, {"id": 608, "seek": 313460, "start": 3157.08, "end": 3160.72, "text": " to make sure that there's always ways that you can push yourself.", "tokens": [281, 652, 988, 300, 456, 311, 1009, 2098, 300, 291, 393, 2944, 1803, 13], "temperature": 0.0, "avg_logprob": -0.1280563780404989, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.72307227330748e-05}, {"id": 609, "seek": 316072, "start": 3160.72, "end": 3166.04, "text": " So if you're ever feeling a bit bored, let me know.", "tokens": [407, 498, 291, 434, 1562, 2633, 257, 857, 13521, 11, 718, 385, 458, 13], "temperature": 0.0, "avg_logprob": -0.23451457023620606, "compression_ratio": 1.5, "no_speech_prob": 4.832158811041154e-05}, {"id": 610, "seek": 316072, "start": 3166.04, "end": 3169.64, "text": " It's a perfectly reasonable thing to say, hey, I'm kind of a bit bored and I'll try", "tokens": [467, 311, 257, 6239, 10585, 551, 281, 584, 11, 4177, 11, 286, 478, 733, 295, 257, 857, 13521, 293, 286, 603, 853], "temperature": 0.0, "avg_logprob": -0.23451457023620606, "compression_ratio": 1.5, "no_speech_prob": 4.832158811041154e-05}, {"id": 611, "seek": 316072, "start": 3169.64, "end": 3178.0, "text": " and give you something to do that you don't know how to do.", "tokens": [293, 976, 291, 746, 281, 360, 300, 291, 500, 380, 458, 577, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.23451457023620606, "compression_ratio": 1.5, "no_speech_prob": 4.832158811041154e-05}, {"id": 612, "seek": 316072, "start": 3178.0, "end": 3188.64, "text": " So at this point, I downloaded dogscats.zip and I unzipped it.", "tokens": [407, 412, 341, 935, 11, 286, 21748, 7197, 66, 1720, 13, 27268, 293, 286, 517, 89, 5529, 309, 13], "temperature": 0.0, "avg_logprob": -0.23451457023620606, "compression_ratio": 1.5, "no_speech_prob": 4.832158811041154e-05}, {"id": 613, "seek": 318864, "start": 3188.64, "end": 3192.6, "text": " If you're wondering about the minus q, that's just because unzipped otherwise prints out", "tokens": [759, 291, 434, 6359, 466, 264, 3175, 9505, 11, 300, 311, 445, 570, 517, 89, 5529, 5911, 22305, 484], "temperature": 0.0, "avg_logprob": -0.16843222148382841, "compression_ratio": 1.4918032786885247, "no_speech_prob": 1.147858529293444e-05}, {"id": 614, "seek": 318864, "start": 3192.6, "end": 3197.56, "text": " every single file name as it goes, so that's q for quiet.", "tokens": [633, 2167, 3991, 1315, 382, 309, 1709, 11, 370, 300, 311, 9505, 337, 5677, 13], "temperature": 0.0, "avg_logprob": -0.16843222148382841, "compression_ratio": 1.4918032786885247, "no_speech_prob": 1.147858529293444e-05}, {"id": 615, "seek": 318864, "start": 3197.56, "end": 3203.92, "text": " So just about the most important thing for doing this kind of image classification is", "tokens": [407, 445, 466, 264, 881, 1021, 551, 337, 884, 341, 733, 295, 3256, 21538, 307], "temperature": 0.0, "avg_logprob": -0.16843222148382841, "compression_ratio": 1.4918032786885247, "no_speech_prob": 1.147858529293444e-05}, {"id": 616, "seek": 318864, "start": 3203.92, "end": 3208.72, "text": " how the data directories are structured.", "tokens": [577, 264, 1412, 5391, 530, 366, 18519, 13], "temperature": 0.0, "avg_logprob": -0.16843222148382841, "compression_ratio": 1.4918032786885247, "no_speech_prob": 1.147858529293444e-05}, {"id": 617, "seek": 320872, "start": 3208.72, "end": 3219.08, "text": " In particular, you'll notice that we have a training set and a test set.", "tokens": [682, 1729, 11, 291, 603, 3449, 300, 321, 362, 257, 3097, 992, 293, 257, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.1671022190767176, "compression_ratio": 1.5643564356435644, "no_speech_prob": 7.071821073623141e-06}, {"id": 618, "seek": 320872, "start": 3219.08, "end": 3224.16, "text": " That's because when we downloaded the data originally from Kaggle, it had a train.zip", "tokens": [663, 311, 570, 562, 321, 21748, 264, 1412, 7993, 490, 48751, 22631, 11, 309, 632, 257, 3847, 13, 27268], "temperature": 0.0, "avg_logprob": -0.1671022190767176, "compression_ratio": 1.5643564356435644, "no_speech_prob": 7.071821073623141e-06}, {"id": 619, "seek": 320872, "start": 3224.16, "end": 3227.16, "text": " and a test.zip.", "tokens": [293, 257, 1500, 13, 27268, 13], "temperature": 0.0, "avg_logprob": -0.1671022190767176, "compression_ratio": 1.5643564356435644, "no_speech_prob": 7.071821073623141e-06}, {"id": 620, "seek": 320872, "start": 3227.16, "end": 3234.64, "text": " Keras, which is the library we're going to use, expects that each class of object that", "tokens": [591, 6985, 11, 597, 307, 264, 6405, 321, 434, 516, 281, 764, 11, 33280, 300, 1184, 1508, 295, 2657, 300], "temperature": 0.0, "avg_logprob": -0.1671022190767176, "compression_ratio": 1.5643564356435644, "no_speech_prob": 7.071821073623141e-06}, {"id": 621, "seek": 320872, "start": 3234.64, "end": 3238.16, "text": " you're going to recognize is in a different directory.", "tokens": [291, 434, 516, 281, 5521, 307, 294, 257, 819, 21120, 13], "temperature": 0.0, "avg_logprob": -0.1671022190767176, "compression_ratio": 1.5643564356435644, "no_speech_prob": 7.071821073623141e-06}, {"id": 622, "seek": 323816, "start": 3238.16, "end": 3249.2, "text": " So the one main thing I did after I downloaded it from Kaggle is that I created two directories,", "tokens": [407, 264, 472, 2135, 551, 286, 630, 934, 286, 21748, 309, 490, 48751, 22631, 307, 300, 286, 2942, 732, 5391, 530, 11], "temperature": 0.0, "avg_logprob": -0.2378206986647386, "compression_ratio": 1.8187134502923976, "no_speech_prob": 1.805834108381532e-05}, {"id": 623, "seek": 323816, "start": 3249.2, "end": 3251.08, "text": " one called cats and one called dogs.", "tokens": [472, 1219, 11111, 293, 472, 1219, 7197, 13], "temperature": 0.0, "avg_logprob": -0.2378206986647386, "compression_ratio": 1.8187134502923976, "no_speech_prob": 1.805834108381532e-05}, {"id": 624, "seek": 323816, "start": 3251.08, "end": 3254.6, "text": " I put all the cats in the cats and all the dogs in the dogs.", "tokens": [286, 829, 439, 264, 11111, 294, 264, 11111, 293, 439, 264, 7197, 294, 264, 7197, 13], "temperature": 0.0, "avg_logprob": -0.2378206986647386, "compression_ratio": 1.8187134502923976, "no_speech_prob": 1.805834108381532e-05}, {"id": 625, "seek": 323816, "start": 3254.6, "end": 3258.48, "text": " When I downloaded them from Kaggle, they were all in one directory and they were called", "tokens": [1133, 286, 21748, 552, 490, 48751, 22631, 11, 436, 645, 439, 294, 472, 21120, 293, 436, 645, 1219], "temperature": 0.0, "avg_logprob": -0.2378206986647386, "compression_ratio": 1.8187134502923976, "no_speech_prob": 1.805834108381532e-05}, {"id": 626, "seek": 323816, "start": 3258.48, "end": 3263.16, "text": " like cat.1.jpg or dog.1.jpg.", "tokens": [411, 3857, 13, 16, 13, 73, 49861, 420, 3000, 13, 16, 13, 73, 49861, 13], "temperature": 0.0, "avg_logprob": -0.2378206986647386, "compression_ratio": 1.8187134502923976, "no_speech_prob": 1.805834108381532e-05}, {"id": 627, "seek": 326316, "start": 3263.16, "end": 3288.24, "text": " So now if I ls train dog.1 star, for example, or dog.10 star.", "tokens": [407, 586, 498, 286, 287, 82, 3847, 3000, 13, 16, 3543, 11, 337, 1365, 11, 420, 3000, 13, 3279, 3543, 13], "temperature": 0.0, "avg_logprob": -0.38610450744628905, "compression_ratio": 1.0, "no_speech_prob": 2.046222834906075e-05}, {"id": 628, "seek": 328824, "start": 3288.24, "end": 3316.8399999999997, "text": " So there are 11,500 dogs in there.", "tokens": [407, 456, 366, 2975, 11, 7526, 7197, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.48314925602504183, "compression_ratio": 0.9444444444444444, "no_speech_prob": 2.111137473548297e-05}, {"id": 629, "seek": 331684, "start": 3316.84, "end": 3321.52, "text": " So that's the number of dogs and cats that we have in our training set.", "tokens": [407, 300, 311, 264, 1230, 295, 7197, 293, 11111, 300, 321, 362, 294, 527, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.12351771354675294, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.2407766563410405e-06}, {"id": 630, "seek": 331684, "start": 3321.52, "end": 3327.2000000000003, "text": " And so for those of you who haven't done much data science before, there's this really key", "tokens": [400, 370, 337, 729, 295, 291, 567, 2378, 380, 1096, 709, 1412, 3497, 949, 11, 456, 311, 341, 534, 2141], "temperature": 0.0, "avg_logprob": -0.12351771354675294, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.2407766563410405e-06}, {"id": 631, "seek": 331684, "start": 3327.2000000000003, "end": 3331.04, "text": " concept that you have a training set and a test set.", "tokens": [3410, 300, 291, 362, 257, 3097, 992, 293, 257, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.12351771354675294, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.2407766563410405e-06}, {"id": 632, "seek": 331684, "start": 3331.04, "end": 3336.1200000000003, "text": " And Kaggle being a competition makes this really obvious.", "tokens": [400, 48751, 22631, 885, 257, 6211, 1669, 341, 534, 6322, 13], "temperature": 0.0, "avg_logprob": -0.12351771354675294, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.2407766563410405e-06}, {"id": 633, "seek": 331684, "start": 3336.1200000000003, "end": 3340.0, "text": " The files in the training set tell you what they are.", "tokens": [440, 7098, 294, 264, 3097, 992, 980, 291, 437, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.12351771354675294, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.2407766563410405e-06}, {"id": 634, "seek": 331684, "start": 3340.0, "end": 3341.0, "text": " Here is a dog.", "tokens": [1692, 307, 257, 3000, 13], "temperature": 0.0, "avg_logprob": -0.12351771354675294, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.2407766563410405e-06}, {"id": 635, "seek": 331684, "start": 3341.0, "end": 3342.0, "text": " It's called dog.something.", "tokens": [467, 311, 1219, 3000, 13, 31681, 13], "temperature": 0.0, "avg_logprob": -0.12351771354675294, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.2407766563410405e-06}, {"id": 636, "seek": 334200, "start": 3342.0, "end": 3351.32, "text": " But if I look in the test set, they don't say anything, they're just numbers.", "tokens": [583, 498, 286, 574, 294, 264, 1500, 992, 11, 436, 500, 380, 584, 1340, 11, 436, 434, 445, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11902985643984666, "compression_ratio": 1.3576158940397351, "no_speech_prob": 7.183182788139675e-06}, {"id": 637, "seek": 334200, "start": 3351.32, "end": 3352.84, "text": " Why is that?", "tokens": [1545, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.11902985643984666, "compression_ratio": 1.3576158940397351, "no_speech_prob": 7.183182788139675e-06}, {"id": 638, "seek": 334200, "start": 3352.84, "end": 3359.76, "text": " That's because your job in this Kaggle competition is to say, for example, for 43.jpg, is it", "tokens": [663, 311, 570, 428, 1691, 294, 341, 48751, 22631, 6211, 307, 281, 584, 11, 337, 1365, 11, 337, 17914, 13, 73, 49861, 11, 307, 309], "temperature": 0.0, "avg_logprob": -0.11902985643984666, "compression_ratio": 1.3576158940397351, "no_speech_prob": 7.183182788139675e-06}, {"id": 639, "seek": 334200, "start": 3359.76, "end": 3361.76, "text": " a dog or is it a cat?", "tokens": [257, 3000, 420, 307, 309, 257, 3857, 30], "temperature": 0.0, "avg_logprob": -0.11902985643984666, "compression_ratio": 1.3576158940397351, "no_speech_prob": 7.183182788139675e-06}, {"id": 640, "seek": 336176, "start": 3361.76, "end": 3377.0, "text": " So there are 12,500 images in the test directory for you to classify.", "tokens": [407, 456, 366, 2272, 11, 7526, 5267, 294, 264, 1500, 21120, 337, 291, 281, 33872, 13], "temperature": 0.0, "avg_logprob": -0.11960584063862645, "compression_ratio": 1.5248868778280542, "no_speech_prob": 1.5294037893909262e-06}, {"id": 641, "seek": 336176, "start": 3377.0, "end": 3381.5200000000004, "text": " Even if you're not doing a Kaggle competition, you should always do this yourself.", "tokens": [2754, 498, 291, 434, 406, 884, 257, 48751, 22631, 6211, 11, 291, 820, 1009, 360, 341, 1803, 13], "temperature": 0.0, "avg_logprob": -0.11960584063862645, "compression_ratio": 1.5248868778280542, "no_speech_prob": 1.5294037893909262e-06}, {"id": 642, "seek": 336176, "start": 3381.5200000000004, "end": 3385.8, "text": " In fact, ideally you would get one of your colleagues to do it without you being involved.", "tokens": [682, 1186, 11, 22915, 291, 576, 483, 472, 295, 428, 7734, 281, 360, 309, 1553, 291, 885, 3288, 13], "temperature": 0.0, "avg_logprob": -0.11960584063862645, "compression_ratio": 1.5248868778280542, "no_speech_prob": 1.5294037893909262e-06}, {"id": 643, "seek": 336176, "start": 3385.8, "end": 3390.96, "text": " To split the data into a test set and a training set, and to not let you look at the test set", "tokens": [1407, 7472, 264, 1412, 666, 257, 1500, 992, 293, 257, 3097, 992, 11, 293, 281, 406, 718, 291, 574, 412, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.11960584063862645, "compression_ratio": 1.5248868778280542, "no_speech_prob": 1.5294037893909262e-06}, {"id": 644, "seek": 339096, "start": 3390.96, "end": 3395.36, "text": " until you promised you're finished.", "tokens": [1826, 291, 10768, 291, 434, 4335, 13], "temperature": 0.0, "avg_logprob": -0.21606755518651272, "compression_ratio": 1.5859030837004404, "no_speech_prob": 5.594287813437404e-06}, {"id": 645, "seek": 339096, "start": 3395.36, "end": 3397.08, "text": " Kaggle kind of enforces this.", "tokens": [48751, 22631, 733, 295, 25495, 887, 341, 13], "temperature": 0.0, "avg_logprob": -0.21606755518651272, "compression_ratio": 1.5859030837004404, "no_speech_prob": 5.594287813437404e-06}, {"id": 646, "seek": 339096, "start": 3397.08, "end": 3402.96, "text": " They let you submit to the leaderboard and find out how you're going.", "tokens": [814, 718, 291, 10315, 281, 264, 5263, 3787, 293, 915, 484, 577, 291, 434, 516, 13], "temperature": 0.0, "avg_logprob": -0.21606755518651272, "compression_ratio": 1.5859030837004404, "no_speech_prob": 5.594287813437404e-06}, {"id": 647, "seek": 339096, "start": 3402.96, "end": 3407.64, "text": " But actually the final score is given based on a totally separate set of data that is", "tokens": [583, 767, 264, 2572, 6175, 307, 2212, 2361, 322, 257, 3879, 4994, 992, 295, 1412, 300, 307], "temperature": 0.0, "avg_logprob": -0.21606755518651272, "compression_ratio": 1.5859030837004404, "no_speech_prob": 5.594287813437404e-06}, {"id": 648, "seek": 339096, "start": 3407.64, "end": 3408.64, "text": " not scored.", "tokens": [406, 18139, 13], "temperature": 0.0, "avg_logprob": -0.21606755518651272, "compression_ratio": 1.5859030837004404, "no_speech_prob": 5.594287813437404e-06}, {"id": 649, "seek": 339096, "start": 3408.64, "end": 3418.38, "text": " For me, before I started entering Kaggle competitions, I kind of thought that my data science process", "tokens": [1171, 385, 11, 949, 286, 1409, 11104, 48751, 22631, 26185, 11, 286, 733, 295, 1194, 300, 452, 1412, 3497, 1399], "temperature": 0.0, "avg_logprob": -0.21606755518651272, "compression_ratio": 1.5859030837004404, "no_speech_prob": 5.594287813437404e-06}, {"id": 650, "seek": 339096, "start": 3418.38, "end": 3419.96, "text": " was reasonably rigorous.", "tokens": [390, 23551, 29882, 13], "temperature": 0.0, "avg_logprob": -0.21606755518651272, "compression_ratio": 1.5859030837004404, "no_speech_prob": 5.594287813437404e-06}, {"id": 651, "seek": 341996, "start": 3419.96, "end": 3425.48, "text": " But once I actually started doing competitions, I realized that that level of enforcing the", "tokens": [583, 1564, 286, 767, 1409, 884, 26185, 11, 286, 5334, 300, 300, 1496, 295, 25495, 2175, 264], "temperature": 0.0, "avg_logprob": -0.18853232325339803, "compression_ratio": 1.555984555984556, "no_speech_prob": 9.66596053331159e-06}, {"id": 652, "seek": 341996, "start": 3425.48, "end": 3429.16, "text": " test training split made me a much better data scientist.", "tokens": [1500, 3097, 7472, 1027, 385, 257, 709, 1101, 1412, 12662, 13], "temperature": 0.0, "avg_logprob": -0.18853232325339803, "compression_ratio": 1.555984555984556, "no_speech_prob": 9.66596053331159e-06}, {"id": 653, "seek": 341996, "start": 3429.16, "end": 3430.36, "text": " You can't cheat.", "tokens": [509, 393, 380, 17470, 13], "temperature": 0.0, "avg_logprob": -0.18853232325339803, "compression_ratio": 1.555984555984556, "no_speech_prob": 9.66596053331159e-06}, {"id": 654, "seek": 341996, "start": 3430.36, "end": 3436.52, "text": " So I do suggest you do this in your own projects as well.", "tokens": [407, 286, 360, 3402, 291, 360, 341, 294, 428, 1065, 4455, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18853232325339803, "compression_ratio": 1.555984555984556, "no_speech_prob": 9.66596053331159e-06}, {"id": 655, "seek": 341996, "start": 3436.52, "end": 3444.04, "text": " Now because we also want to tune our algorithm in terms of different architectures, different", "tokens": [823, 570, 321, 611, 528, 281, 10864, 527, 9284, 294, 2115, 295, 819, 6331, 1303, 11, 819], "temperature": 0.0, "avg_logprob": -0.18853232325339803, "compression_ratio": 1.555984555984556, "no_speech_prob": 9.66596053331159e-06}, {"id": 656, "seek": 341996, "start": 3444.04, "end": 3448.64, "text": " parameters and so forth, which we'll talk about, it's also a very good idea to split", "tokens": [9834, 293, 370, 5220, 11, 597, 321, 603, 751, 466, 11, 309, 311, 611, 257, 588, 665, 1558, 281, 7472], "temperature": 0.0, "avg_logprob": -0.18853232325339803, "compression_ratio": 1.555984555984556, "no_speech_prob": 9.66596053331159e-06}, {"id": 657, "seek": 344864, "start": 3448.64, "end": 3452.8399999999997, "text": " your training set further into a training set and a validation set.", "tokens": [428, 3097, 992, 3052, 666, 257, 3097, 992, 293, 257, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.2234054658471084, "compression_ratio": 1.555, "no_speech_prob": 1.92228690139018e-05}, {"id": 658, "seek": 344864, "start": 3452.8399999999997, "end": 3454.6, "text": " We'll see a lot more about how this works.", "tokens": [492, 603, 536, 257, 688, 544, 466, 577, 341, 1985, 13], "temperature": 0.0, "avg_logprob": -0.2234054658471084, "compression_ratio": 1.555, "no_speech_prob": 1.92228690139018e-05}, {"id": 659, "seek": 344864, "start": 3454.6, "end": 3460.96, "text": " But you'll see in this case, I've created another directory called valid which has dogs", "tokens": [583, 291, 603, 536, 294, 341, 1389, 11, 286, 600, 2942, 1071, 21120, 1219, 7363, 597, 575, 7197], "temperature": 0.0, "avg_logprob": -0.2234054658471084, "compression_ratio": 1.555, "no_speech_prob": 1.92228690139018e-05}, {"id": 660, "seek": 344864, "start": 3460.96, "end": 3462.2799999999997, "text": " and cats as well.", "tokens": [293, 11111, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2234054658471084, "compression_ratio": 1.555, "no_speech_prob": 1.92228690139018e-05}, {"id": 661, "seek": 344864, "start": 3462.2799999999997, "end": 3469.56, "text": " It's structured exactly the same.", "tokens": [467, 311, 18519, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.2234054658471084, "compression_ratio": 1.555, "no_speech_prob": 1.92228690139018e-05}, {"id": 662, "seek": 344864, "start": 3469.56, "end": 3476.5, "text": " And here you can see that there are 1000 cats and 1000 dogs.", "tokens": [400, 510, 291, 393, 536, 300, 456, 366, 9714, 11111, 293, 9714, 7197, 13], "temperature": 0.0, "avg_logprob": -0.2234054658471084, "compression_ratio": 1.555, "no_speech_prob": 1.92228690139018e-05}, {"id": 663, "seek": 347650, "start": 3476.5, "end": 3481.02, "text": " So when I originally downloaded from Kaggle, there were 12,500 cats and dogs in the training", "tokens": [407, 562, 286, 7993, 21748, 490, 48751, 22631, 11, 456, 645, 2272, 11, 7526, 11111, 293, 7197, 294, 264, 3097], "temperature": 0.0, "avg_logprob": -0.1355400329981095, "compression_ratio": 1.82421875, "no_speech_prob": 3.1875297281658277e-06}, {"id": 664, "seek": 347650, "start": 3481.02, "end": 3482.4, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.1355400329981095, "compression_ratio": 1.82421875, "no_speech_prob": 3.1875297281658277e-06}, {"id": 665, "seek": 347650, "start": 3482.4, "end": 3487.32, "text": " That's why in my training set there are 11,500 because I've moved 1000 of each of them to", "tokens": [663, 311, 983, 294, 452, 3097, 992, 456, 366, 2975, 11, 7526, 570, 286, 600, 4259, 9714, 295, 1184, 295, 552, 281], "temperature": 0.0, "avg_logprob": -0.1355400329981095, "compression_ratio": 1.82421875, "no_speech_prob": 3.1875297281658277e-06}, {"id": 666, "seek": 347650, "start": 3487.32, "end": 3489.66, "text": " the validation set.", "tokens": [264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.1355400329981095, "compression_ratio": 1.82421875, "no_speech_prob": 3.1875297281658277e-06}, {"id": 667, "seek": 347650, "start": 3489.66, "end": 3495.6, "text": " So that's the basic data structure we have.", "tokens": [407, 300, 311, 264, 3875, 1412, 3877, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.1355400329981095, "compression_ratio": 1.82421875, "no_speech_prob": 3.1875297281658277e-06}, {"id": 668, "seek": 347650, "start": 3495.6, "end": 3499.56, "text": " Other than splitting things into test training and validation sets, that's the most important", "tokens": [5358, 813, 30348, 721, 666, 1500, 3097, 293, 24071, 6352, 11, 300, 311, 264, 881, 1021], "temperature": 0.0, "avg_logprob": -0.1355400329981095, "compression_ratio": 1.82421875, "no_speech_prob": 3.1875297281658277e-06}, {"id": 669, "seek": 347650, "start": 3499.56, "end": 3501.84, "text": " advice I have as a data scientist.", "tokens": [5192, 286, 362, 382, 257, 1412, 12662, 13], "temperature": 0.0, "avg_logprob": -0.1355400329981095, "compression_ratio": 1.82421875, "no_speech_prob": 3.1875297281658277e-06}, {"id": 670, "seek": 347650, "start": 3501.84, "end": 3506.16, "text": " The second most important advice I have as a data scientist is to always do nearly all", "tokens": [440, 1150, 881, 1021, 5192, 286, 362, 382, 257, 1412, 12662, 307, 281, 1009, 360, 6217, 439], "temperature": 0.0, "avg_logprob": -0.1355400329981095, "compression_ratio": 1.82421875, "no_speech_prob": 3.1875297281658277e-06}, {"id": 671, "seek": 350616, "start": 3506.16, "end": 3508.2, "text": " of your work on a sample.", "tokens": [295, 428, 589, 322, 257, 6889, 13], "temperature": 0.0, "avg_logprob": -0.0939801812171936, "compression_ratio": 1.595959595959596, "no_speech_prob": 1.0451367415953428e-05}, {"id": 672, "seek": 350616, "start": 3508.2, "end": 3512.92, "text": " A sample is a very small amount of data that you can run so quickly that everything you", "tokens": [316, 6889, 307, 257, 588, 1359, 2372, 295, 1412, 300, 291, 393, 1190, 370, 2661, 300, 1203, 291], "temperature": 0.0, "avg_logprob": -0.0939801812171936, "compression_ratio": 1.595959595959596, "no_speech_prob": 1.0451367415953428e-05}, {"id": 673, "seek": 350616, "start": 3512.92, "end": 3516.92, "text": " try you get a nearly immediate answer to.", "tokens": [853, 291, 483, 257, 6217, 11629, 1867, 281, 13], "temperature": 0.0, "avg_logprob": -0.0939801812171936, "compression_ratio": 1.595959595959596, "no_speech_prob": 1.0451367415953428e-05}, {"id": 674, "seek": 350616, "start": 3516.92, "end": 3524.24, "text": " This allows you to very quickly try things, change things and get a basic process running.", "tokens": [639, 4045, 291, 281, 588, 2661, 853, 721, 11, 1319, 721, 293, 483, 257, 3875, 1399, 2614, 13], "temperature": 0.0, "avg_logprob": -0.0939801812171936, "compression_ratio": 1.595959595959596, "no_speech_prob": 1.0451367415953428e-05}, {"id": 675, "seek": 350616, "start": 3524.24, "end": 3531.72, "text": " So I always create a sample with 100 or so items to get started with.", "tokens": [407, 286, 1009, 1884, 257, 6889, 365, 2319, 420, 370, 4754, 281, 483, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.0939801812171936, "compression_ratio": 1.595959595959596, "no_speech_prob": 1.0451367415953428e-05}, {"id": 676, "seek": 353172, "start": 3531.72, "end": 3539.08, "text": " So you'll see I have a directory called sample, and in that I have a whole separate train", "tokens": [407, 291, 603, 536, 286, 362, 257, 21120, 1219, 6889, 11, 293, 294, 300, 286, 362, 257, 1379, 4994, 3847], "temperature": 0.0, "avg_logprob": -0.14401592192102652, "compression_ratio": 1.4903225806451612, "no_speech_prob": 4.28931070928229e-06}, {"id": 677, "seek": 353172, "start": 3539.08, "end": 3541.2, "text": " and valid.", "tokens": [293, 7363, 13], "temperature": 0.0, "avg_logprob": -0.14401592192102652, "compression_ratio": 1.4903225806451612, "no_speech_prob": 4.28931070928229e-06}, {"id": 678, "seek": 353172, "start": 3541.2, "end": 3544.52, "text": " I did not move things there, I copied them there.", "tokens": [286, 630, 406, 1286, 721, 456, 11, 286, 25365, 552, 456, 13], "temperature": 0.0, "avg_logprob": -0.14401592192102652, "compression_ratio": 1.4903225806451612, "no_speech_prob": 4.28931070928229e-06}, {"id": 679, "seek": 353172, "start": 3544.52, "end": 3550.06, "text": " The purpose of this sample directory is just to let me do things really quickly.", "tokens": [440, 4334, 295, 341, 6889, 21120, 307, 445, 281, 718, 385, 360, 721, 534, 2661, 13], "temperature": 0.0, "avg_logprob": -0.14401592192102652, "compression_ratio": 1.4903225806451612, "no_speech_prob": 4.28931070928229e-06}, {"id": 680, "seek": 355006, "start": 3550.06, "end": 3571.32, "text": " So you'll see inside sample train, we have cats and dogs, but this time there are 8.", "tokens": [407, 291, 603, 536, 1854, 6889, 3847, 11, 321, 362, 11111, 293, 7197, 11, 457, 341, 565, 456, 366, 1649, 13], "temperature": 0.0, "avg_logprob": -0.2122644907991651, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.56593033959507e-06}, {"id": 681, "seek": 355006, "start": 3571.32, "end": 3574.16, "text": " I probably should have put more in there, I think probably more like 100 would have", "tokens": [286, 1391, 820, 362, 829, 544, 294, 456, 11, 286, 519, 1391, 544, 411, 2319, 576, 362], "temperature": 0.0, "avg_logprob": -0.2122644907991651, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.56593033959507e-06}, {"id": 682, "seek": 355006, "start": 3574.16, "end": 3578.24, "text": " been good, but I think at the time I was probably using a really low-powered computer for my", "tokens": [668, 665, 11, 457, 286, 519, 412, 264, 565, 286, 390, 1391, 1228, 257, 534, 2295, 12, 27178, 3820, 337, 452], "temperature": 0.0, "avg_logprob": -0.2122644907991651, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.56593033959507e-06}, {"id": 683, "seek": 355006, "start": 3578.24, "end": 3579.24, "text": " testing.", "tokens": [4997, 13], "temperature": 0.0, "avg_logprob": -0.2122644907991651, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.56593033959507e-06}, {"id": 684, "seek": 357924, "start": 3579.24, "end": 3584.4399999999996, "text": " But it's enough to check that my script is working.", "tokens": [583, 309, 311, 1547, 281, 1520, 300, 452, 5755, 307, 1364, 13], "temperature": 0.0, "avg_logprob": -0.21999928722642872, "compression_ratio": 1.478021978021978, "no_speech_prob": 5.862723810423631e-06}, {"id": 685, "seek": 357924, "start": 3584.4399999999996, "end": 3593.6, "text": " So now that everything is downloaded, you can see that I have in my Jupyter Notebook", "tokens": [407, 586, 300, 1203, 307, 21748, 11, 291, 393, 536, 300, 286, 362, 294, 452, 22125, 88, 391, 11633, 2939], "temperature": 0.0, "avg_logprob": -0.21999928722642872, "compression_ratio": 1.478021978021978, "no_speech_prob": 5.862723810423631e-06}, {"id": 686, "seek": 357924, "start": 3593.6, "end": 3596.68, "text": " is automatically noticed that it's changed.", "tokens": [307, 6772, 5694, 300, 309, 311, 3105, 13], "temperature": 0.0, "avg_logprob": -0.21999928722642872, "compression_ratio": 1.478021978021978, "no_speech_prob": 5.862723810423631e-06}, {"id": 687, "seek": 357924, "start": 3596.68, "end": 3604.9199999999996, "text": " I'll get rid of these zip files, and I'll get rid of the Rachel's and Jeremy's Notebooks", "tokens": [286, 603, 483, 3973, 295, 613, 20730, 7098, 11, 293, 286, 603, 483, 3973, 295, 264, 14246, 311, 293, 17809, 311, 11633, 15170], "temperature": 0.0, "avg_logprob": -0.21999928722642872, "compression_ratio": 1.478021978021978, "no_speech_prob": 5.862723810423631e-06}, {"id": 688, "seek": 360492, "start": 3604.92, "end": 3615.2400000000002, "text": " that I was playing with, and we're ready to get started with doing some deep learning.", "tokens": [300, 286, 390, 2433, 365, 11, 293, 321, 434, 1919, 281, 483, 1409, 365, 884, 512, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.14068194358579575, "compression_ratio": 1.5232558139534884, "no_speech_prob": 4.157331204623915e-06}, {"id": 689, "seek": 360492, "start": 3615.2400000000002, "end": 3625.08, "text": " So the goal for you guys during this week will be to replicate everything that I've", "tokens": [407, 264, 3387, 337, 291, 1074, 1830, 341, 1243, 486, 312, 281, 25356, 1203, 300, 286, 600], "temperature": 0.0, "avg_logprob": -0.14068194358579575, "compression_ratio": 1.5232558139534884, "no_speech_prob": 4.157331204623915e-06}, {"id": 690, "seek": 360492, "start": 3625.08, "end": 3632.38, "text": " done, initially just by making sure that this notebook works for you, but then to replicate", "tokens": [1096, 11, 9105, 445, 538, 1455, 988, 300, 341, 21060, 1985, 337, 291, 11, 457, 550, 281, 25356], "temperature": 0.0, "avg_logprob": -0.14068194358579575, "compression_ratio": 1.5232558139534884, "no_speech_prob": 4.157331204623915e-06}, {"id": 691, "seek": 363238, "start": 3632.38, "end": 3635.36, "text": " it with another dataset.", "tokens": [309, 365, 1071, 28872, 13], "temperature": 0.0, "avg_logprob": -0.1702737808227539, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.165909088740591e-05}, {"id": 692, "seek": 363238, "start": 3635.36, "end": 3641.1600000000003, "text": " And so one of the things we'll do tomorrow is we'll post some ideas of other interesting", "tokens": [400, 370, 472, 295, 264, 721, 321, 603, 360, 4153, 307, 321, 603, 2183, 512, 3487, 295, 661, 1880], "temperature": 0.0, "avg_logprob": -0.1702737808227539, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.165909088740591e-05}, {"id": 693, "seek": 363238, "start": 3641.1600000000003, "end": 3646.4, "text": " Kaggle datasets you could try, and maybe other people can also post other interesting datasets", "tokens": [48751, 22631, 42856, 291, 727, 853, 11, 293, 1310, 661, 561, 393, 611, 2183, 661, 1880, 42856], "temperature": 0.0, "avg_logprob": -0.1702737808227539, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.165909088740591e-05}, {"id": 694, "seek": 363238, "start": 3646.4, "end": 3648.56, "text": " they found elsewhere.", "tokens": [436, 1352, 14517, 13], "temperature": 0.0, "avg_logprob": -0.1702737808227539, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.165909088740591e-05}, {"id": 695, "seek": 363238, "start": 3648.56, "end": 3653.56, "text": " So the idea will be to make sure that during the week you can run your own classification", "tokens": [407, 264, 1558, 486, 312, 281, 652, 988, 300, 1830, 264, 1243, 291, 393, 1190, 428, 1065, 21538], "temperature": 0.0, "avg_logprob": -0.1702737808227539, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.165909088740591e-05}, {"id": 696, "seek": 363238, "start": 3653.56, "end": 3657.28, "text": " process on some dataset other than dogs and cats.", "tokens": [1399, 322, 512, 28872, 661, 813, 7197, 293, 11111, 13], "temperature": 0.0, "avg_logprob": -0.1702737808227539, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.165909088740591e-05}, {"id": 697, "seek": 363238, "start": 3657.28, "end": 3660.54, "text": " But first of all, make sure that you can run this.", "tokens": [583, 700, 295, 439, 11, 652, 988, 300, 291, 393, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.1702737808227539, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.165909088740591e-05}, {"id": 698, "seek": 366054, "start": 3660.54, "end": 3668.88, "text": " So as you can see in this notebook, I've used Markdown cells.", "tokens": [407, 382, 291, 393, 536, 294, 341, 21060, 11, 286, 600, 1143, 3934, 5093, 5438, 13], "temperature": 0.0, "avg_logprob": -0.20416622895460862, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.00010554194886935875}, {"id": 699, "seek": 366054, "start": 3668.88, "end": 3671.44, "text": " How many people have used Markdown before?", "tokens": [1012, 867, 561, 362, 1143, 3934, 5093, 949, 30], "temperature": 0.0, "avg_logprob": -0.20416622895460862, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.00010554194886935875}, {"id": 700, "seek": 366054, "start": 3671.44, "end": 3678.92, "text": " Okay, so those of you who don't know, Markdown is what we use both in the notebook as well", "tokens": [1033, 11, 370, 729, 295, 291, 567, 500, 380, 458, 11, 3934, 5093, 307, 437, 321, 764, 1293, 294, 264, 21060, 382, 731], "temperature": 0.0, "avg_logprob": -0.20416622895460862, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.00010554194886935875}, {"id": 701, "seek": 366054, "start": 3678.92, "end": 3680.36, "text": " as on the wiki.", "tokens": [382, 322, 264, 261, 9850, 13], "temperature": 0.0, "avg_logprob": -0.20416622895460862, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.00010554194886935875}, {"id": 702, "seek": 366054, "start": 3680.36, "end": 3684.84, "text": " It's basically a way of really quickly creating formatted text.", "tokens": [467, 311, 1936, 257, 636, 295, 534, 2661, 4084, 1254, 32509, 2487, 13], "temperature": 0.0, "avg_logprob": -0.20416622895460862, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.00010554194886935875}, {"id": 703, "seek": 366054, "start": 3684.84, "end": 3687.84, "text": " There's not enough of you that aren't familiar with it that I'm going to go into it in detail.", "tokens": [821, 311, 406, 1547, 295, 291, 300, 3212, 380, 4963, 365, 309, 300, 286, 478, 516, 281, 352, 666, 309, 294, 2607, 13], "temperature": 0.0, "avg_logprob": -0.20416622895460862, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.00010554194886935875}, {"id": 704, "seek": 368784, "start": 3687.84, "end": 3691.32, "text": " If you're not familiar with it, please Google Markdown.", "tokens": [759, 291, 434, 406, 4963, 365, 309, 11, 1767, 3329, 3934, 5093, 13], "temperature": 0.0, "avg_logprob": -0.12708594582297586, "compression_ratio": 1.6009389671361502, "no_speech_prob": 7.76679371483624e-06}, {"id": 705, "seek": 368784, "start": 3691.32, "end": 3698.1000000000004, "text": " And you can experiment with it either on the wiki or in your notebook.", "tokens": [400, 291, 393, 5120, 365, 309, 2139, 322, 264, 261, 9850, 420, 294, 428, 21060, 13], "temperature": 0.0, "avg_logprob": -0.12708594582297586, "compression_ratio": 1.6009389671361502, "no_speech_prob": 7.76679371483624e-06}, {"id": 706, "seek": 368784, "start": 3698.1000000000004, "end": 3701.92, "text": " As you can see though, I've basically created cells with headings and some text, and during", "tokens": [1018, 291, 393, 536, 1673, 11, 286, 600, 1936, 2942, 5438, 365, 1378, 1109, 293, 512, 2487, 11, 293, 1830], "temperature": 0.0, "avg_logprob": -0.12708594582297586, "compression_ratio": 1.6009389671361502, "no_speech_prob": 7.76679371483624e-06}, {"id": 707, "seek": 368784, "start": 3701.92, "end": 3705.48, "text": " the week you can read through these in detail.", "tokens": [264, 1243, 291, 393, 1401, 807, 613, 294, 2607, 13], "temperature": 0.0, "avg_logprob": -0.12708594582297586, "compression_ratio": 1.6009389671361502, "no_speech_prob": 7.76679371483624e-06}, {"id": 708, "seek": 368784, "start": 3705.48, "end": 3710.7200000000003, "text": " As we mentioned, we're going to try to enter the dogs and cats competition.", "tokens": [1018, 321, 2835, 11, 321, 434, 516, 281, 853, 281, 3242, 264, 7197, 293, 11111, 6211, 13], "temperature": 0.0, "avg_logprob": -0.12708594582297586, "compression_ratio": 1.6009389671361502, "no_speech_prob": 7.76679371483624e-06}, {"id": 709, "seek": 371072, "start": 3710.72, "end": 3724.24, "text": " So 25,000 labeled dog and cat photos, 12,500 in the test set, and the goal is to beat 80%.", "tokens": [407, 3552, 11, 1360, 21335, 3000, 293, 3857, 5787, 11, 2272, 11, 7526, 294, 264, 1500, 992, 11, 293, 264, 3387, 307, 281, 4224, 4688, 6856], "temperature": 0.0, "avg_logprob": -0.17517162405926248, "compression_ratio": 1.485981308411215, "no_speech_prob": 2.7264300115348306e-06}, {"id": 710, "seek": 371072, "start": 3724.24, "end": 3728.6, "text": " As we go along, we are going to be learning about quite a few libraries.", "tokens": [1018, 321, 352, 2051, 11, 321, 366, 516, 281, 312, 2539, 466, 1596, 257, 1326, 15148, 13], "temperature": 0.0, "avg_logprob": -0.17517162405926248, "compression_ratio": 1.485981308411215, "no_speech_prob": 2.7264300115348306e-06}, {"id": 711, "seek": 371072, "start": 3728.6, "end": 3733.8399999999997, "text": " Not too many, but enough for those of you that haven't used Python for data science", "tokens": [1726, 886, 867, 11, 457, 1547, 337, 729, 295, 291, 300, 2378, 380, 1143, 15329, 337, 1412, 3497], "temperature": 0.0, "avg_logprob": -0.17517162405926248, "compression_ratio": 1.485981308411215, "no_speech_prob": 2.7264300115348306e-06}, {"id": 712, "seek": 371072, "start": 3733.8399999999997, "end": 3734.8399999999997, "text": " before.", "tokens": [949, 13], "temperature": 0.0, "avg_logprob": -0.17517162405926248, "compression_ratio": 1.485981308411215, "no_speech_prob": 2.7264300115348306e-06}, {"id": 713, "seek": 371072, "start": 3734.8399999999997, "end": 3737.7599999999998, "text": " It's going to seem like quite a few by the end of the 7 weeks.", "tokens": [467, 311, 516, 281, 1643, 411, 1596, 257, 1326, 538, 264, 917, 295, 264, 1614, 3259, 13], "temperature": 0.0, "avg_logprob": -0.17517162405926248, "compression_ratio": 1.485981308411215, "no_speech_prob": 2.7264300115348306e-06}, {"id": 714, "seek": 373776, "start": 3737.76, "end": 3741.44, "text": " Hopefully you'll be pretty familiar with all of them.", "tokens": [10429, 291, 603, 312, 1238, 4963, 365, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.14018313428188892, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.18428906716872e-05}, {"id": 715, "seek": 373776, "start": 3741.44, "end": 3745.92, "text": " One of the really important three is Matplotlib.", "tokens": [1485, 295, 264, 534, 1021, 1045, 307, 6789, 564, 310, 38270, 13], "temperature": 0.0, "avg_logprob": -0.14018313428188892, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.18428906716872e-05}, {"id": 716, "seek": 373776, "start": 3745.92, "end": 3750.0, "text": " That's because Matplotlib does all of our plotting and visualization.", "tokens": [663, 311, 570, 6789, 564, 310, 38270, 775, 439, 295, 527, 41178, 293, 25801, 13], "temperature": 0.0, "avg_logprob": -0.14018313428188892, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.18428906716872e-05}, {"id": 717, "seek": 373776, "start": 3750.0, "end": 3755.36, "text": " On the wiki, we have a section called Python Libraries.", "tokens": [1282, 264, 261, 9850, 11, 321, 362, 257, 3541, 1219, 15329, 12006, 4889, 13], "temperature": 0.0, "avg_logprob": -0.14018313428188892, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.18428906716872e-05}, {"id": 718, "seek": 373776, "start": 3755.36, "end": 3759.88, "text": " As you can see, we have our top three listed up here.", "tokens": [1018, 291, 393, 536, 11, 321, 362, 527, 1192, 1045, 10052, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.14018313428188892, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.18428906716872e-05}, {"id": 719, "seek": 373776, "start": 3759.88, "end": 3763.7000000000003, "text": " At the moment, there are just links to where they come from.", "tokens": [1711, 264, 1623, 11, 456, 366, 445, 6123, 281, 689, 436, 808, 490, 13], "temperature": 0.0, "avg_logprob": -0.14018313428188892, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.18428906716872e-05}, {"id": 720, "seek": 376370, "start": 3763.7, "end": 3769.3199999999997, "text": " I'm hoping that you guys will help us to turn this into a really rich source of information", "tokens": [286, 478, 7159, 300, 291, 1074, 486, 854, 505, 281, 1261, 341, 666, 257, 534, 4593, 4009, 295, 1589], "temperature": 0.0, "avg_logprob": -0.2060065480460108, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.219113204977475e-05}, {"id": 721, "seek": 376370, "start": 3769.3199999999997, "end": 3774.4399999999996, "text": " about places that you found lots of helpful stuff, answers to frequently asked questions,", "tokens": [466, 3190, 300, 291, 1352, 3195, 295, 4961, 1507, 11, 6338, 281, 10374, 2351, 1651, 11], "temperature": 0.0, "avg_logprob": -0.2060065480460108, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.219113204977475e-05}, {"id": 722, "seek": 376370, "start": 3774.4399999999996, "end": 3775.4399999999996, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.2060065480460108, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.219113204977475e-05}, {"id": 723, "seek": 376370, "start": 3775.4399999999996, "end": 3779.3199999999997, "text": " But for now, if you're not familiar with one of these things, type the word, followed by", "tokens": [583, 337, 586, 11, 498, 291, 434, 406, 4963, 365, 472, 295, 613, 721, 11, 2010, 264, 1349, 11, 6263, 538], "temperature": 0.0, "avg_logprob": -0.2060065480460108, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.219113204977475e-05}, {"id": 724, "seek": 376370, "start": 3779.3199999999997, "end": 3783.8799999999997, "text": " tutorial into Google and you'll find lots of resources.", "tokens": [7073, 666, 3329, 293, 291, 603, 915, 3195, 295, 3593, 13], "temperature": 0.0, "avg_logprob": -0.2060065480460108, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.219113204977475e-05}, {"id": 725, "seek": 376370, "start": 3783.8799999999997, "end": 3787.52, "text": " All of these things are widely used.", "tokens": [1057, 295, 613, 721, 366, 13371, 1143, 13], "temperature": 0.0, "avg_logprob": -0.2060065480460108, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.219113204977475e-05}, {"id": 726, "seek": 376370, "start": 3787.52, "end": 3793.56, "text": " Keras a little bit less so because it's just a deep learning library and therefore relatively", "tokens": [591, 6985, 257, 707, 857, 1570, 370, 570, 309, 311, 445, 257, 2452, 2539, 6405, 293, 4412, 7226], "temperature": 0.0, "avg_logprob": -0.2060065480460108, "compression_ratio": 1.6526315789473685, "no_speech_prob": 3.219113204977475e-05}, {"id": 727, "seek": 379356, "start": 3793.56, "end": 3794.56, "text": " new.", "tokens": [777, 13], "temperature": 0.0, "avg_logprob": -0.19481161290949042, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.1984691051766276e-05}, {"id": 728, "seek": 379356, "start": 3794.56, "end": 3799.56, "text": " NumPy and Matplotlib and all these other ones, Cyclet, Learn, SciPy, there's lots of books", "tokens": [22592, 47, 88, 293, 6789, 564, 310, 38270, 293, 439, 613, 661, 2306, 11, 10295, 66, 2631, 11, 17216, 11, 16942, 47, 88, 11, 456, 311, 3195, 295, 3642], "temperature": 0.0, "avg_logprob": -0.19481161290949042, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.1984691051766276e-05}, {"id": 729, "seek": 379356, "start": 3799.56, "end": 3805.44, "text": " about them, there's lots of tutorials about them, there's lots of videos about them.", "tokens": [466, 552, 11, 456, 311, 3195, 295, 17616, 466, 552, 11, 456, 311, 3195, 295, 2145, 466, 552, 13], "temperature": 0.0, "avg_logprob": -0.19481161290949042, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.1984691051766276e-05}, {"id": 730, "seek": 379356, "start": 3805.44, "end": 3810.08, "text": " So Matplotlib creates plots.", "tokens": [407, 6789, 564, 310, 38270, 7829, 28609, 13], "temperature": 0.0, "avg_logprob": -0.19481161290949042, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.1984691051766276e-05}, {"id": 731, "seek": 379356, "start": 3810.08, "end": 3814.96, "text": " One of the things we need to do is to tell Jupyter Notebook what to do with those plots.", "tokens": [1485, 295, 264, 721, 321, 643, 281, 360, 307, 281, 980, 22125, 88, 391, 11633, 2939, 437, 281, 360, 365, 729, 28609, 13], "temperature": 0.0, "avg_logprob": -0.19481161290949042, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.1984691051766276e-05}, {"id": 732, "seek": 379356, "start": 3814.96, "end": 3818.56, "text": " Should it pop open a new window for them, should it save them?", "tokens": [6454, 309, 1665, 1269, 257, 777, 4910, 337, 552, 11, 820, 309, 3155, 552, 30], "temperature": 0.0, "avg_logprob": -0.19481161290949042, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.1984691051766276e-05}, {"id": 733, "seek": 381856, "start": 3818.56, "end": 3826.36, "text": " So this sent Matplotlib inline says, please show our plots in the actual Jupyter Notebook.", "tokens": [407, 341, 2279, 6789, 564, 310, 38270, 294, 1889, 1619, 11, 1767, 855, 527, 28609, 294, 264, 3539, 22125, 88, 391, 11633, 2939, 13], "temperature": 0.0, "avg_logprob": -0.12295956818953804, "compression_ratio": 1.7410358565737052, "no_speech_prob": 4.710859229817288e-06}, {"id": 734, "seek": 381856, "start": 3826.36, "end": 3831.72, "text": " That's pretty much the first line in every Jupyter Notebook that I create.", "tokens": [663, 311, 1238, 709, 264, 700, 1622, 294, 633, 22125, 88, 391, 11633, 2939, 300, 286, 1884, 13], "temperature": 0.0, "avg_logprob": -0.12295956818953804, "compression_ratio": 1.7410358565737052, "no_speech_prob": 4.710859229817288e-06}, {"id": 735, "seek": 381856, "start": 3831.72, "end": 3835.62, "text": " And here's the thing I told you about, which is sometimes I want to run stuff on a sample", "tokens": [400, 510, 311, 264, 551, 286, 1907, 291, 466, 11, 597, 307, 2171, 286, 528, 281, 1190, 1507, 322, 257, 6889], "temperature": 0.0, "avg_logprob": -0.12295956818953804, "compression_ratio": 1.7410358565737052, "no_speech_prob": 4.710859229817288e-06}, {"id": 736, "seek": 381856, "start": 3835.62, "end": 3838.44, "text": " and sometimes I want to run it on everything.", "tokens": [293, 2171, 286, 528, 281, 1190, 309, 322, 1203, 13], "temperature": 0.0, "avg_logprob": -0.12295956818953804, "compression_ratio": 1.7410358565737052, "no_speech_prob": 4.710859229817288e-06}, {"id": 737, "seek": 381856, "start": 3838.44, "end": 3842.7599999999998, "text": " And so I make it really easy for myself by having a single thing called path, which I", "tokens": [400, 370, 286, 652, 309, 534, 1858, 337, 2059, 538, 1419, 257, 2167, 551, 1219, 3100, 11, 597, 286], "temperature": 0.0, "avg_logprob": -0.12295956818953804, "compression_ratio": 1.7410358565737052, "no_speech_prob": 4.710859229817288e-06}, {"id": 738, "seek": 381856, "start": 3842.7599999999998, "end": 3845.84, "text": " can switch between the sample and the everything.", "tokens": [393, 3679, 1296, 264, 6889, 293, 264, 1203, 13], "temperature": 0.0, "avg_logprob": -0.12295956818953804, "compression_ratio": 1.7410358565737052, "no_speech_prob": 4.710859229817288e-06}, {"id": 739, "seek": 384584, "start": 3845.84, "end": 3850.48, "text": " So for now, let's just do things on the sample.", "tokens": [407, 337, 586, 11, 718, 311, 445, 360, 721, 322, 264, 6889, 13], "temperature": 0.0, "avg_logprob": -0.16859932701186378, "compression_ratio": 1.572, "no_speech_prob": 7.527918114647036e-06}, {"id": 740, "seek": 384584, "start": 3850.48, "end": 3855.56, "text": " And so you should do all of your work on the sample until everything's working.", "tokens": [400, 370, 291, 820, 360, 439, 295, 428, 589, 322, 264, 6889, 1826, 1203, 311, 1364, 13], "temperature": 0.0, "avg_logprob": -0.16859932701186378, "compression_ratio": 1.572, "no_speech_prob": 7.527918114647036e-06}, {"id": 741, "seek": 384584, "start": 3855.56, "end": 3860.52, "text": " So as you can see, each time I've done something, I press Shift-Enter and it puts a little number", "tokens": [407, 382, 291, 393, 536, 11, 1184, 565, 286, 600, 1096, 746, 11, 286, 1886, 28304, 12, 16257, 391, 293, 309, 8137, 257, 707, 1230], "temperature": 0.0, "avg_logprob": -0.16859932701186378, "compression_ratio": 1.572, "no_speech_prob": 7.527918114647036e-06}, {"id": 742, "seek": 384584, "start": 3860.52, "end": 3869.36, "text": " after the in, showing that this is the second input cell that I've run.", "tokens": [934, 264, 294, 11, 4099, 300, 341, 307, 264, 1150, 4846, 2815, 300, 286, 600, 1190, 13], "temperature": 0.0, "avg_logprob": -0.16859932701186378, "compression_ratio": 1.572, "no_speech_prob": 7.527918114647036e-06}, {"id": 743, "seek": 384584, "start": 3869.36, "end": 3874.84, "text": " Like every programming language, a large amount of the power of Python comes from the libraries", "tokens": [1743, 633, 9410, 2856, 11, 257, 2416, 2372, 295, 264, 1347, 295, 15329, 1487, 490, 264, 15148], "temperature": 0.0, "avg_logprob": -0.16859932701186378, "compression_ratio": 1.572, "no_speech_prob": 7.527918114647036e-06}, {"id": 744, "seek": 387484, "start": 3874.84, "end": 3877.0, "text": " that you use.", "tokens": [300, 291, 764, 13], "temperature": 0.0, "avg_logprob": -0.1291599963084761, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.8631388229550794e-05}, {"id": 745, "seek": 387484, "start": 3877.0, "end": 3882.28, "text": " And to use a library in Python, you have to do two things.", "tokens": [400, 281, 764, 257, 6405, 294, 15329, 11, 291, 362, 281, 360, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.1291599963084761, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.8631388229550794e-05}, {"id": 746, "seek": 387484, "start": 3882.28, "end": 3887.84, "text": " You have to install it and then you have to import it.", "tokens": [509, 362, 281, 3625, 309, 293, 550, 291, 362, 281, 974, 309, 13], "temperature": 0.0, "avg_logprob": -0.1291599963084761, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.8631388229550794e-05}, {"id": 747, "seek": 387484, "start": 3887.84, "end": 3894.58, "text": " In Python, I strongly recommend that you use a particular Python distribution called Anaconda.", "tokens": [682, 15329, 11, 286, 10613, 2748, 300, 291, 764, 257, 1729, 15329, 7316, 1219, 1107, 326, 12233, 13], "temperature": 0.0, "avg_logprob": -0.1291599963084761, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.8631388229550794e-05}, {"id": 748, "seek": 387484, "start": 3894.58, "end": 3900.1200000000003, "text": " And if you're using the scripts and AMIs we provided, you are already using Anaconda.", "tokens": [400, 498, 291, 434, 1228, 264, 23294, 293, 6475, 6802, 321, 5649, 11, 291, 366, 1217, 1228, 1107, 326, 12233, 13], "temperature": 0.0, "avg_logprob": -0.1291599963084761, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.8631388229550794e-05}, {"id": 749, "seek": 390012, "start": 3900.12, "end": 3906.6, "text": " You can check what Python you're using by typing which Python, and it will tell you.", "tokens": [509, 393, 1520, 437, 15329, 291, 434, 1228, 538, 18444, 597, 15329, 11, 293, 309, 486, 980, 291, 13], "temperature": 0.0, "avg_logprob": -0.15462065137122288, "compression_ratio": 1.688976377952756, "no_speech_prob": 1.3006903827772476e-05}, {"id": 750, "seek": 390012, "start": 3906.6, "end": 3910.8399999999997, "text": " You'll see that not only am I using Anaconda, but I'm using an Anaconda that is installed", "tokens": [509, 603, 536, 300, 406, 787, 669, 286, 1228, 1107, 326, 12233, 11, 457, 286, 478, 1228, 364, 1107, 326, 12233, 300, 307, 8899], "temperature": 0.0, "avg_logprob": -0.15462065137122288, "compression_ratio": 1.688976377952756, "no_speech_prob": 1.3006903827772476e-05}, {"id": 751, "seek": 390012, "start": 3910.8399999999997, "end": 3912.7599999999998, "text": " into my home directory.", "tokens": [666, 452, 1280, 21120, 13], "temperature": 0.0, "avg_logprob": -0.15462065137122288, "compression_ratio": 1.688976377952756, "no_speech_prob": 1.3006903827772476e-05}, {"id": 752, "seek": 390012, "start": 3912.7599999999998, "end": 3915.96, "text": " So no screwing around with sudo or any of that business.", "tokens": [407, 572, 5630, 278, 926, 365, 459, 2595, 420, 604, 295, 300, 1606, 13], "temperature": 0.0, "avg_logprob": -0.15462065137122288, "compression_ratio": 1.688976377952756, "no_speech_prob": 1.3006903827772476e-05}, {"id": 753, "seek": 390012, "start": 3915.96, "end": 3921.3599999999997, "text": " And again, if you use our AMI in scripts, this has all been done for you.", "tokens": [400, 797, 11, 498, 291, 764, 527, 6475, 40, 294, 23294, 11, 341, 575, 439, 668, 1096, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.15462065137122288, "compression_ratio": 1.688976377952756, "no_speech_prob": 1.3006903827772476e-05}, {"id": 754, "seek": 390012, "start": 3921.3599999999997, "end": 3927.3199999999997, "text": " With Anaconda, installing anything is as simple as typing conda, install and the name of the", "tokens": [2022, 1107, 326, 12233, 11, 20762, 1340, 307, 382, 2199, 382, 18444, 2224, 64, 11, 3625, 293, 264, 1315, 295, 264], "temperature": 0.0, "avg_logprob": -0.15462065137122288, "compression_ratio": 1.688976377952756, "no_speech_prob": 1.3006903827772476e-05}, {"id": 755, "seek": 390012, "start": 3927.3199999999997, "end": 3928.44, "text": " thing.", "tokens": [551, 13], "temperature": 0.0, "avg_logprob": -0.15462065137122288, "compression_ratio": 1.688976377952756, "no_speech_prob": 1.3006903827772476e-05}, {"id": 756, "seek": 392844, "start": 3928.44, "end": 3932.88, "text": " And on Anaconda, everything has been pre-compiled, so you don't have to wait for it to compile,", "tokens": [400, 322, 1107, 326, 12233, 11, 1203, 575, 668, 659, 12, 21541, 7292, 11, 370, 291, 500, 380, 362, 281, 1699, 337, 309, 281, 31413, 11], "temperature": 0.0, "avg_logprob": -0.13358665157008814, "compression_ratio": 1.9033816425120773, "no_speech_prob": 3.905452558683464e-06}, {"id": 757, "seek": 392844, "start": 3932.88, "end": 3935.96, "text": " you don't have to worry about dependencies, you don't have to worry about anything, it", "tokens": [291, 500, 380, 362, 281, 3292, 466, 36606, 11, 291, 500, 380, 362, 281, 3292, 466, 1340, 11, 309], "temperature": 0.0, "avg_logprob": -0.13358665157008814, "compression_ratio": 1.9033816425120773, "no_speech_prob": 3.905452558683464e-06}, {"id": 758, "seek": 392844, "start": 3935.96, "end": 3937.84, "text": " just works.", "tokens": [445, 1985, 13], "temperature": 0.0, "avg_logprob": -0.13358665157008814, "compression_ratio": 1.9033816425120773, "no_speech_prob": 3.905452558683464e-06}, {"id": 759, "seek": 392844, "start": 3937.84, "end": 3944.82, "text": " That is why we very highly recommend using Anaconda.", "tokens": [663, 307, 983, 321, 588, 5405, 2748, 1228, 1107, 326, 12233, 13], "temperature": 0.0, "avg_logprob": -0.13358665157008814, "compression_ratio": 1.9033816425120773, "no_speech_prob": 3.905452558683464e-06}, {"id": 760, "seek": 392844, "start": 3944.82, "end": 3950.68, "text": " It works on Mac, it works on Windows, and it works on Linux.", "tokens": [467, 1985, 322, 5707, 11, 309, 1985, 322, 8591, 11, 293, 309, 1985, 322, 18734, 13], "temperature": 0.0, "avg_logprob": -0.13358665157008814, "compression_ratio": 1.9033816425120773, "no_speech_prob": 3.905452558683464e-06}, {"id": 761, "seek": 392844, "start": 3950.68, "end": 3955.92, "text": " Lots of Windows users use it, very few Linux users use it, very few Mac users use it.", "tokens": [15908, 295, 8591, 5022, 764, 309, 11, 588, 1326, 18734, 5022, 764, 309, 11, 588, 1326, 5707, 5022, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.13358665157008814, "compression_ratio": 1.9033816425120773, "no_speech_prob": 3.905452558683464e-06}, {"id": 762, "seek": 395592, "start": 3955.92, "end": 3961.0, "text": " I think that's a mistake because lots of Mac and Linux users also have trouble with compiling", "tokens": [286, 519, 300, 311, 257, 6146, 570, 3195, 295, 5707, 293, 18734, 5022, 611, 362, 5253, 365, 715, 4883], "temperature": 0.0, "avg_logprob": -0.140809787894195, "compression_ratio": 1.6091954022988506, "no_speech_prob": 7.183137313404586e-06}, {"id": 763, "seek": 395592, "start": 3961.0, "end": 3962.4, "text": " dependencies and all that stuff.", "tokens": [36606, 293, 439, 300, 1507, 13], "temperature": 0.0, "avg_logprob": -0.140809787894195, "compression_ratio": 1.6091954022988506, "no_speech_prob": 7.183137313404586e-06}, {"id": 764, "seek": 395592, "start": 3962.4, "end": 3965.36, "text": " I suggest that everybody uses it.", "tokens": [286, 3402, 300, 2201, 4960, 309, 13], "temperature": 0.0, "avg_logprob": -0.140809787894195, "compression_ratio": 1.6091954022988506, "no_speech_prob": 7.183137313404586e-06}, {"id": 765, "seek": 395592, "start": 3965.36, "end": 3972.2400000000002, "text": " From time to time, you will come across something that does not have a conda installer available,", "tokens": [3358, 565, 281, 565, 11, 291, 486, 808, 2108, 746, 300, 775, 406, 362, 257, 2224, 64, 46620, 2435, 11], "temperature": 0.0, "avg_logprob": -0.140809787894195, "compression_ratio": 1.6091954022988506, "no_speech_prob": 7.183137313404586e-06}, {"id": 766, "seek": 395592, "start": 3972.2400000000002, "end": 3976.02, "text": " in which case you have to use pip instead.", "tokens": [294, 597, 1389, 291, 362, 281, 764, 8489, 2602, 13], "temperature": 0.0, "avg_logprob": -0.140809787894195, "compression_ratio": 1.6091954022988506, "no_speech_prob": 7.183137313404586e-06}, {"id": 767, "seek": 395592, "start": 3976.02, "end": 3983.8, "text": " In our case, I think just Theano and Keras are in that situation, but neither of those", "tokens": [682, 527, 1389, 11, 286, 519, 445, 440, 3730, 293, 591, 6985, 366, 294, 300, 2590, 11, 457, 9662, 295, 729], "temperature": 0.0, "avg_logprob": -0.140809787894195, "compression_ratio": 1.6091954022988506, "no_speech_prob": 7.183137313404586e-06}, {"id": 768, "seek": 395592, "start": 3983.8, "end": 3985.7200000000003, "text": " need compiling anything at all.", "tokens": [643, 715, 4883, 1340, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.140809787894195, "compression_ratio": 1.6091954022988506, "no_speech_prob": 7.183137313404586e-06}, {"id": 769, "seek": 398572, "start": 3985.72, "end": 3989.24, "text": " So they're very, very easy to install.", "tokens": [407, 436, 434, 588, 11, 588, 1858, 281, 3625, 13], "temperature": 0.0, "avg_logprob": -0.16792561592312033, "compression_ratio": 1.6693877551020408, "no_speech_prob": 1.4285096767707728e-05}, {"id": 770, "seek": 398572, "start": 3989.24, "end": 3993.9599999999996, "text": " So once you've installed it by typing conda install whatever, and most things are already", "tokens": [407, 1564, 291, 600, 8899, 309, 538, 18444, 2224, 64, 3625, 2035, 11, 293, 881, 721, 366, 1217], "temperature": 0.0, "avg_logprob": -0.16792561592312033, "compression_ratio": 1.6693877551020408, "no_speech_prob": 1.4285096767707728e-05}, {"id": 771, "seek": 398572, "start": 3993.9599999999996, "end": 3999.4399999999996, "text": " installed for you with our AMIs, you then have to tell Python that I want to use it", "tokens": [8899, 337, 291, 365, 527, 6475, 6802, 11, 291, 550, 362, 281, 980, 15329, 300, 286, 528, 281, 764, 309], "temperature": 0.0, "avg_logprob": -0.16792561592312033, "compression_ratio": 1.6693877551020408, "no_speech_prob": 1.4285096767707728e-05}, {"id": 772, "seek": 398572, "start": 3999.4399999999996, "end": 4004.2, "text": " in this particular session, which you do just by typing import and the thing you want to", "tokens": [294, 341, 1729, 5481, 11, 597, 291, 360, 445, 538, 18444, 974, 293, 264, 551, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.16792561592312033, "compression_ratio": 1.6693877551020408, "no_speech_prob": 1.4285096767707728e-05}, {"id": 773, "seek": 398572, "start": 4004.2, "end": 4005.2, "text": " look at.", "tokens": [574, 412, 13], "temperature": 0.0, "avg_logprob": -0.16792561592312033, "compression_ratio": 1.6693877551020408, "no_speech_prob": 1.4285096767707728e-05}, {"id": 774, "seek": 398572, "start": 4005.2, "end": 4009.2, "text": " So I'm not going to go through all these libraries right now, I'll go through them as we use", "tokens": [407, 286, 478, 406, 516, 281, 352, 807, 439, 613, 15148, 558, 586, 11, 286, 603, 352, 807, 552, 382, 321, 764], "temperature": 0.0, "avg_logprob": -0.16792561592312033, "compression_ratio": 1.6693877551020408, "no_speech_prob": 1.4285096767707728e-05}, {"id": 775, "seek": 398572, "start": 4009.2, "end": 4011.8399999999997, "text": " them.", "tokens": [552, 13], "temperature": 0.0, "avg_logprob": -0.16792561592312033, "compression_ratio": 1.6693877551020408, "no_speech_prob": 1.4285096767707728e-05}, {"id": 776, "seek": 401184, "start": 4011.84, "end": 4015.84, "text": " One of the big three is here, which is NumPy.", "tokens": [1485, 295, 264, 955, 1045, 307, 510, 11, 597, 307, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.18951218778436835, "compression_ratio": 1.5308056872037914, "no_speech_prob": 7.253929652506486e-05}, {"id": 777, "seek": 401184, "start": 4015.84, "end": 4020.96, "text": " NumPy is the thing which, as our Wiki page describes, provides all of our basic linear", "tokens": [22592, 47, 88, 307, 264, 551, 597, 11, 382, 527, 35892, 3028, 15626, 11, 6417, 439, 295, 527, 3875, 8213], "temperature": 0.0, "avg_logprob": -0.18951218778436835, "compression_ratio": 1.5308056872037914, "no_speech_prob": 7.253929652506486e-05}, {"id": 778, "seek": 401184, "start": 4020.96, "end": 4023.0, "text": " algebra.", "tokens": [21989, 13], "temperature": 0.0, "avg_logprob": -0.18951218778436835, "compression_ratio": 1.5308056872037914, "no_speech_prob": 7.253929652506486e-05}, {"id": 779, "seek": 401184, "start": 4023.0, "end": 4028.52, "text": " How many people here have some familiarity at all with linear algebra?", "tokens": [1012, 867, 561, 510, 362, 512, 49828, 412, 439, 365, 8213, 21989, 30], "temperature": 0.0, "avg_logprob": -0.18951218778436835, "compression_ratio": 1.5308056872037914, "no_speech_prob": 7.253929652506486e-05}, {"id": 780, "seek": 401184, "start": 4028.52, "end": 4029.96, "text": " Nearly all of you.", "tokens": [38000, 439, 295, 291, 13], "temperature": 0.0, "avg_logprob": -0.18951218778436835, "compression_ratio": 1.5308056872037914, "no_speech_prob": 7.253929652506486e-05}, {"id": 781, "seek": 401184, "start": 4029.96, "end": 4037.0, "text": " So if you're somebody who didn't put up your hand, I would suggest looking at the resources", "tokens": [407, 498, 291, 434, 2618, 567, 994, 380, 829, 493, 428, 1011, 11, 286, 576, 3402, 1237, 412, 264, 3593], "temperature": 0.0, "avg_logprob": -0.18951218778436835, "compression_ratio": 1.5308056872037914, "no_speech_prob": 7.253929652506486e-05}, {"id": 782, "seek": 403700, "start": 4037.0, "end": 4045.12, "text": " that Taro added to the Wiki.", "tokens": [300, 314, 9708, 3869, 281, 264, 35892, 13], "temperature": 0.0, "avg_logprob": -0.2156761832859205, "compression_ratio": 1.4728260869565217, "no_speech_prob": 3.169213232467882e-05}, {"id": 783, "seek": 403700, "start": 4045.12, "end": 4051.76, "text": " So go back to the home page and go to Linear Algebra for Deep Learning.", "tokens": [407, 352, 646, 281, 264, 1280, 3028, 293, 352, 281, 14670, 289, 967, 19983, 337, 14895, 15205, 13], "temperature": 0.0, "avg_logprob": -0.2156761832859205, "compression_ratio": 1.4728260869565217, "no_speech_prob": 3.169213232467882e-05}, {"id": 784, "seek": 403700, "start": 4051.76, "end": 4059.56, "text": " Generally speaking, for any math stuff, my suggestion is to go to the Khan Academy site.", "tokens": [21082, 4124, 11, 337, 604, 5221, 1507, 11, 452, 16541, 307, 281, 352, 281, 264, 18136, 11735, 3621, 13], "temperature": 0.0, "avg_logprob": -0.2156761832859205, "compression_ratio": 1.4728260869565217, "no_speech_prob": 3.169213232467882e-05}, {"id": 785, "seek": 403700, "start": 4059.56, "end": 4066.48, "text": " Khan Academy has really great videos for introducing these kind of simple topics.", "tokens": [18136, 11735, 575, 534, 869, 2145, 337, 15424, 613, 733, 295, 2199, 8378, 13], "temperature": 0.0, "avg_logprob": -0.2156761832859205, "compression_ratio": 1.4728260869565217, "no_speech_prob": 3.169213232467882e-05}, {"id": 786, "seek": 406648, "start": 4066.48, "end": 4072.96, "text": " We just need to know these three things, mainly the first two things for this course.", "tokens": [492, 445, 643, 281, 458, 613, 1045, 721, 11, 8704, 264, 700, 732, 721, 337, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.19525424801573463, "compression_ratio": 1.55793991416309, "no_speech_prob": 6.747901352355257e-06}, {"id": 787, "seek": 406648, "start": 4072.96, "end": 4079.92, "text": " And NumPy is the thing which gives you these linear algebra operations in Python.", "tokens": [400, 22592, 47, 88, 307, 264, 551, 597, 2709, 291, 613, 8213, 21989, 7705, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.19525424801573463, "compression_ratio": 1.55793991416309, "no_speech_prob": 6.747901352355257e-06}, {"id": 788, "seek": 406648, "start": 4079.92, "end": 4083.32, "text": " As you'll see, it makes them extremely easy to use.", "tokens": [1018, 291, 603, 536, 11, 309, 1669, 552, 4664, 1858, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.19525424801573463, "compression_ratio": 1.55793991416309, "no_speech_prob": 6.747901352355257e-06}, {"id": 789, "seek": 406648, "start": 4083.32, "end": 4087.2, "text": " Pretty much everybody renames NumPy to NP.", "tokens": [10693, 709, 2201, 8124, 1632, 22592, 47, 88, 281, 38611, 13], "temperature": 0.0, "avg_logprob": -0.19525424801573463, "compression_ratio": 1.55793991416309, "no_speech_prob": 6.747901352355257e-06}, {"id": 790, "seek": 406648, "start": 4087.2, "end": 4089.88, "text": " So that's what import NumPy as NP does.", "tokens": [407, 300, 311, 437, 974, 22592, 47, 88, 382, 38611, 775, 13], "temperature": 0.0, "avg_logprob": -0.19525424801573463, "compression_ratio": 1.55793991416309, "no_speech_prob": 6.747901352355257e-06}, {"id": 791, "seek": 406648, "start": 4089.88, "end": 4092.36, "text": " You'll find it in nearly everybody's script on the Internet.", "tokens": [509, 603, 915, 309, 294, 6217, 2201, 311, 5755, 322, 264, 7703, 13], "temperature": 0.0, "avg_logprob": -0.19525424801573463, "compression_ratio": 1.55793991416309, "no_speech_prob": 6.747901352355257e-06}, {"id": 792, "seek": 409236, "start": 4092.36, "end": 4097.68, "text": " It'll be NP.something, not NumPy.something.", "tokens": [467, 603, 312, 38611, 13, 31681, 11, 406, 22592, 47, 88, 13, 31681, 13], "temperature": 0.0, "avg_logprob": -0.12002262511810699, "compression_ratio": 1.555, "no_speech_prob": 5.68234054298955e-06}, {"id": 793, "seek": 409236, "start": 4097.68, "end": 4103.96, "text": " In general, we try to stick with the same kind of approaches that everybody else uses", "tokens": [682, 2674, 11, 321, 853, 281, 2897, 365, 264, 912, 733, 295, 11587, 300, 2201, 1646, 4960], "temperature": 0.0, "avg_logprob": -0.12002262511810699, "compression_ratio": 1.555, "no_speech_prob": 5.68234054298955e-06}, {"id": 794, "seek": 409236, "start": 4103.96, "end": 4108.16, "text": " so that nothing will be too unfamiliar.", "tokens": [370, 300, 1825, 486, 312, 886, 29415, 13], "temperature": 0.0, "avg_logprob": -0.12002262511810699, "compression_ratio": 1.555, "no_speech_prob": 5.68234054298955e-06}, {"id": 795, "seek": 409236, "start": 4108.16, "end": 4112.04, "text": " So we've imported some libraries that we need.", "tokens": [407, 321, 600, 25524, 512, 15148, 300, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.12002262511810699, "compression_ratio": 1.555, "no_speech_prob": 5.68234054298955e-06}, {"id": 796, "seek": 409236, "start": 4112.04, "end": 4117.400000000001, "text": " We also try to provide some additional utilities and scripts for things that we think probably", "tokens": [492, 611, 853, 281, 2893, 512, 4497, 30482, 293, 23294, 337, 721, 300, 321, 519, 1391], "temperature": 0.0, "avg_logprob": -0.12002262511810699, "compression_ratio": 1.555, "no_speech_prob": 5.68234054298955e-06}, {"id": 797, "seek": 411740, "start": 4117.4, "end": 4122.719999999999, "text": " ought to exist but don't exist that make things easier.", "tokens": [13416, 281, 2514, 457, 500, 380, 2514, 300, 652, 721, 3571, 13], "temperature": 0.0, "avg_logprob": -0.17300744916571945, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.5465509679634124e-05}, {"id": 798, "seek": 411740, "start": 4122.719999999999, "end": 4124.58, "text": " There's very few of them.", "tokens": [821, 311, 588, 1326, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.17300744916571945, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.5465509679634124e-05}, {"id": 799, "seek": 411740, "start": 4124.58, "end": 4128.04, "text": " Nearly all of it is in one script called utils.", "tokens": [38000, 439, 295, 309, 307, 294, 472, 5755, 1219, 2839, 4174, 13], "temperature": 0.0, "avg_logprob": -0.17300744916571945, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.5465509679634124e-05}, {"id": 800, "seek": 411740, "start": 4128.04, "end": 4133.719999999999, "text": " There's a cool little trick, which is that if you are using external script that you've", "tokens": [821, 311, 257, 1627, 707, 4282, 11, 597, 307, 300, 498, 291, 366, 1228, 8320, 5755, 300, 291, 600], "temperature": 0.0, "avg_logprob": -0.17300744916571945, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.5465509679634124e-05}, {"id": 801, "seek": 411740, "start": 4133.719999999999, "end": 4137.759999999999, "text": " created and you're changing it quite a bit, so now that you've got utils, feel free to", "tokens": [2942, 293, 291, 434, 4473, 309, 1596, 257, 857, 11, 370, 586, 300, 291, 600, 658, 2839, 4174, 11, 841, 1737, 281], "temperature": 0.0, "avg_logprob": -0.17300744916571945, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.5465509679634124e-05}, {"id": 802, "seek": 411740, "start": 4137.759999999999, "end": 4140.24, "text": " add and change and do whatever you like to it.", "tokens": [909, 293, 1319, 293, 360, 2035, 291, 411, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.17300744916571945, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.5465509679634124e-05}, {"id": 803, "seek": 411740, "start": 4140.24, "end": 4145.36, "text": " If you import it like this, import utils, re-lude utils, and then from utils import", "tokens": [759, 291, 974, 309, 411, 341, 11, 974, 2839, 4174, 11, 319, 12, 32334, 2839, 4174, 11, 293, 550, 490, 2839, 4174, 974], "temperature": 0.0, "avg_logprob": -0.17300744916571945, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.5465509679634124e-05}, {"id": 804, "seek": 414536, "start": 4145.36, "end": 4152.719999999999, "text": " whatever you need, you can go back and re-run that cell later after you've changed utils.py", "tokens": [2035, 291, 643, 11, 291, 393, 352, 646, 293, 319, 12, 12997, 300, 2815, 1780, 934, 291, 600, 3105, 2839, 4174, 13, 8200], "temperature": 0.0, "avg_logprob": -0.13262501832480741, "compression_ratio": 1.665158371040724, "no_speech_prob": 7.646458470844664e-06}, {"id": 805, "seek": 414536, "start": 4152.719999999999, "end": 4157.0, "text": " and all of your changes will be there available for you to use.", "tokens": [293, 439, 295, 428, 2962, 486, 312, 456, 2435, 337, 291, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.13262501832480741, "compression_ratio": 1.665158371040724, "no_speech_prob": 7.646458470844664e-06}, {"id": 806, "seek": 414536, "start": 4157.0, "end": 4161.839999999999, "text": " So for now we're just going to use one thing from our utils library called plots, which", "tokens": [407, 337, 586, 321, 434, 445, 516, 281, 764, 472, 551, 490, 527, 2839, 4174, 6405, 1219, 28609, 11, 597], "temperature": 0.0, "avg_logprob": -0.13262501832480741, "compression_ratio": 1.665158371040724, "no_speech_prob": 7.646458470844664e-06}, {"id": 807, "seek": 414536, "start": 4161.839999999999, "end": 4165.04, "text": " we'll see used in a moment.", "tokens": [321, 603, 536, 1143, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.13262501832480741, "compression_ratio": 1.665158371040724, "no_speech_prob": 7.646458470844664e-06}, {"id": 808, "seek": 414536, "start": 4165.04, "end": 4170.98, "text": " So our first step will be to use a pre-trained model.", "tokens": [407, 527, 700, 1823, 486, 312, 281, 764, 257, 659, 12, 17227, 2001, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13262501832480741, "compression_ratio": 1.665158371040724, "no_speech_prob": 7.646458470844664e-06}, {"id": 809, "seek": 414536, "start": 4170.98, "end": 4173.839999999999, "text": " So what do we mean by a pre-trained model?", "tokens": [407, 437, 360, 321, 914, 538, 257, 659, 12, 17227, 2001, 2316, 30], "temperature": 0.0, "avg_logprob": -0.13262501832480741, "compression_ratio": 1.665158371040724, "no_speech_prob": 7.646458470844664e-06}, {"id": 810, "seek": 417384, "start": 4173.84, "end": 4180.0, "text": " What we mean is that somebody has already come along, downloaded millions of images", "tokens": [708, 321, 914, 307, 300, 2618, 575, 1217, 808, 2051, 11, 21748, 6803, 295, 5267], "temperature": 0.0, "avg_logprob": -0.1745285775926378, "compression_ratio": 1.7035398230088497, "no_speech_prob": 2.48245419243176e-06}, {"id": 811, "seek": 417384, "start": 4180.0, "end": 4186.32, "text": " off the internet and built a deep learning model that has learned to recognize the contents", "tokens": [766, 264, 4705, 293, 3094, 257, 2452, 2539, 2316, 300, 575, 3264, 281, 5521, 264, 15768], "temperature": 0.0, "avg_logprob": -0.1745285775926378, "compression_ratio": 1.7035398230088497, "no_speech_prob": 2.48245419243176e-06}, {"id": 812, "seek": 417384, "start": 4186.32, "end": 4190.24, "text": " of those images.", "tokens": [295, 729, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1745285775926378, "compression_ratio": 1.7035398230088497, "no_speech_prob": 2.48245419243176e-06}, {"id": 813, "seek": 417384, "start": 4190.24, "end": 4196.08, "text": " Nearly always when people create these pre-trained models, they use a particular dataset called", "tokens": [38000, 1009, 562, 561, 1884, 613, 659, 12, 17227, 2001, 5245, 11, 436, 764, 257, 1729, 28872, 1219], "temperature": 0.0, "avg_logprob": -0.1745285775926378, "compression_ratio": 1.7035398230088497, "no_speech_prob": 2.48245419243176e-06}, {"id": 814, "seek": 417384, "start": 4196.08, "end": 4197.64, "text": " ImageNet.", "tokens": [29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.1745285775926378, "compression_ratio": 1.7035398230088497, "no_speech_prob": 2.48245419243176e-06}, {"id": 815, "seek": 417384, "start": 4197.64, "end": 4200.72, "text": " The reason that they tend to use ImageNet, or one of the key reasons that they tend to", "tokens": [440, 1778, 300, 436, 3928, 281, 764, 29903, 31890, 11, 420, 472, 295, 264, 2141, 4112, 300, 436, 3928, 281], "temperature": 0.0, "avg_logprob": -0.1745285775926378, "compression_ratio": 1.7035398230088497, "no_speech_prob": 2.48245419243176e-06}, {"id": 816, "seek": 420072, "start": 4200.72, "end": 4207.88, "text": " use ImageNet, is because ImageNet has the most respected annual computer vision competition.", "tokens": [764, 29903, 31890, 11, 307, 570, 29903, 31890, 575, 264, 881, 20020, 9784, 3820, 5201, 6211, 13], "temperature": 0.0, "avg_logprob": -0.24278962227606005, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.0782971912703943e-05}, {"id": 817, "seek": 420072, "start": 4207.88, "end": 4218.76, "text": " So people that win the ImageNet challenge tend to be companies like Google and Microsoft.", "tokens": [407, 561, 300, 1942, 264, 29903, 31890, 3430, 3928, 281, 312, 3431, 411, 3329, 293, 8116, 13], "temperature": 0.0, "avg_logprob": -0.24278962227606005, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.0782971912703943e-05}, {"id": 818, "seek": 420072, "start": 4218.76, "end": 4225.240000000001, "text": " A couple of years ago, it tended to be people who immediately got hired by Google and Microsoft.", "tokens": [316, 1916, 295, 924, 2057, 11, 309, 34732, 281, 312, 561, 567, 4258, 658, 13144, 538, 3329, 293, 8116, 13], "temperature": 0.0, "avg_logprob": -0.24278962227606005, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.0782971912703943e-05}, {"id": 819, "seek": 422524, "start": 4225.24, "end": 4231.48, "text": " ImageNet itself is fun to explore if you go to ImageNet and go to explore.", "tokens": [29903, 31890, 2564, 307, 1019, 281, 6839, 498, 291, 352, 281, 29903, 31890, 293, 352, 281, 6839, 13], "temperature": 0.0, "avg_logprob": -0.18116012971792647, "compression_ratio": 1.4465408805031446, "no_speech_prob": 1.7778291294234805e-05}, {"id": 820, "seek": 422524, "start": 4231.48, "end": 4233.96, "text": " You can check it out.", "tokens": [509, 393, 1520, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.18116012971792647, "compression_ratio": 1.4465408805031446, "no_speech_prob": 1.7778291294234805e-05}, {"id": 821, "seek": 422524, "start": 4233.96, "end": 4240.36, "text": " And basically there are 32,000 categories.", "tokens": [400, 1936, 456, 366, 8858, 11, 1360, 10479, 13], "temperature": 0.0, "avg_logprob": -0.18116012971792647, "compression_ratio": 1.4465408805031446, "no_speech_prob": 1.7778291294234805e-05}, {"id": 822, "seek": 422524, "start": 4240.36, "end": 4249.08, "text": " So for example we can go ImageNet, let's look at plants and plant life, let's go to crops,", "tokens": [407, 337, 1365, 321, 393, 352, 29903, 31890, 11, 718, 311, 574, 412, 5972, 293, 3709, 993, 11, 718, 311, 352, 281, 16829, 11], "temperature": 0.0, "avg_logprob": -0.18116012971792647, "compression_ratio": 1.4465408805031446, "no_speech_prob": 1.7778291294234805e-05}, {"id": 823, "seek": 424908, "start": 4249.08, "end": 4257.96, "text": " let's go to field crops, corn, type crops, dent corn.", "tokens": [718, 311, 352, 281, 2519, 16829, 11, 9046, 11, 2010, 16829, 11, 7059, 9046, 13], "temperature": 0.0, "avg_logprob": -0.1435713313874744, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.5206464922812302e-05}, {"id": 824, "seek": 424908, "start": 4257.96, "end": 4261.76, "text": " So here we have a number of pictures of dent corn.", "tokens": [407, 510, 321, 362, 257, 1230, 295, 5242, 295, 7059, 9046, 13], "temperature": 0.0, "avg_logprob": -0.1435713313874744, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.5206464922812302e-05}, {"id": 825, "seek": 424908, "start": 4261.76, "end": 4263.84, "text": " There are 397 of them.", "tokens": [821, 366, 15238, 22, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.1435713313874744, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.5206464922812302e-05}, {"id": 826, "seek": 424908, "start": 4263.84, "end": 4271.24, "text": " So the folks that create these pre-trained networks basically download a large subset", "tokens": [407, 264, 4024, 300, 1884, 613, 659, 12, 17227, 2001, 9590, 1936, 5484, 257, 2416, 25993], "temperature": 0.0, "avg_logprob": -0.1435713313874744, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.5206464922812302e-05}, {"id": 827, "seek": 424908, "start": 4271.24, "end": 4272.24, "text": " of ImageNet.", "tokens": [295, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.1435713313874744, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.5206464922812302e-05}, {"id": 828, "seek": 424908, "start": 4272.24, "end": 4278.28, "text": " The competition has a thousand of these 32,000 categories that people compete on.", "tokens": [440, 6211, 575, 257, 4714, 295, 613, 8858, 11, 1360, 10479, 300, 561, 11831, 322, 13], "temperature": 0.0, "avg_logprob": -0.1435713313874744, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.5206464922812302e-05}, {"id": 829, "seek": 427828, "start": 4278.28, "end": 4284.92, "text": " So nearly always people just build models for these 1000.", "tokens": [407, 6217, 1009, 561, 445, 1322, 5245, 337, 613, 9714, 13], "temperature": 0.0, "avg_logprob": -0.1307149198320177, "compression_ratio": 1.3795180722891567, "no_speech_prob": 3.844882485282142e-06}, {"id": 830, "seek": 427828, "start": 4284.92, "end": 4292.8, "text": " I would be remiss if I did not mention the shortcomings of the ImageNet dataset.", "tokens": [286, 576, 312, 890, 891, 498, 286, 630, 406, 2152, 264, 2099, 49886, 295, 264, 29903, 31890, 28872, 13], "temperature": 0.0, "avg_logprob": -0.1307149198320177, "compression_ratio": 1.3795180722891567, "no_speech_prob": 3.844882485282142e-06}, {"id": 831, "seek": 427828, "start": 4292.8, "end": 4301.44, "text": " Can anybody tell me something that they notice in common about what these photos look like", "tokens": [1664, 4472, 980, 385, 746, 300, 436, 3449, 294, 2689, 466, 437, 613, 5787, 574, 411], "temperature": 0.0, "avg_logprob": -0.1307149198320177, "compression_ratio": 1.3795180722891567, "no_speech_prob": 3.844882485282142e-06}, {"id": 832, "seek": 430144, "start": 4301.44, "end": 4310.44, "text": " or how they're structured?", "tokens": [420, 577, 436, 434, 18519, 30], "temperature": 0.0, "avg_logprob": -0.21281665914198933, "compression_ratio": 1.4702380952380953, "no_speech_prob": 1.0952937373076566e-05}, {"id": 833, "seek": 430144, "start": 4310.44, "end": 4312.799999999999, "text": " They're just one thing.", "tokens": [814, 434, 445, 472, 551, 13], "temperature": 0.0, "avg_logprob": -0.21281665914198933, "compression_ratio": 1.4702380952380953, "no_speech_prob": 1.0952937373076566e-05}, {"id": 834, "seek": 430144, "start": 4312.799999999999, "end": 4318.16, "text": " If you look at an arbitrary photo, say from my photo album, you'll see there's a person", "tokens": [759, 291, 574, 412, 364, 23211, 5052, 11, 584, 490, 452, 5052, 6030, 11, 291, 603, 536, 456, 311, 257, 954], "temperature": 0.0, "avg_logprob": -0.21281665914198933, "compression_ratio": 1.4702380952380953, "no_speech_prob": 1.0952937373076566e-05}, {"id": 835, "seek": 430144, "start": 4318.16, "end": 4320.679999999999, "text": " here and a bridge there and something else here.", "tokens": [510, 293, 257, 7283, 456, 293, 746, 1646, 510, 13], "temperature": 0.0, "avg_logprob": -0.21281665914198933, "compression_ratio": 1.4702380952380953, "no_speech_prob": 1.0952937373076566e-05}, {"id": 836, "seek": 430144, "start": 4320.679999999999, "end": 4326.48, "text": " ImageNet is carefully curated to be pictures of flint corn.", "tokens": [29903, 31890, 307, 7500, 47851, 281, 312, 5242, 295, 932, 686, 9046, 13], "temperature": 0.0, "avg_logprob": -0.21281665914198933, "compression_ratio": 1.4702380952380953, "no_speech_prob": 1.0952937373076566e-05}, {"id": 837, "seek": 432648, "start": 4326.48, "end": 4335.12, "text": " These are like 312 pictures that are designed to be really good pictures of flint corn.", "tokens": [1981, 366, 411, 805, 4762, 5242, 300, 366, 4761, 281, 312, 534, 665, 5242, 295, 932, 686, 9046, 13], "temperature": 0.0, "avg_logprob": -0.20311369615442612, "compression_ratio": 1.4764397905759161, "no_speech_prob": 1.9947199234593427e-06}, {"id": 838, "seek": 432648, "start": 4335.12, "end": 4342.08, "text": " This is an easier problem than many problems that you will be facing.", "tokens": [639, 307, 364, 3571, 1154, 813, 867, 2740, 300, 291, 486, 312, 7170, 13], "temperature": 0.0, "avg_logprob": -0.20311369615442612, "compression_ratio": 1.4764397905759161, "no_speech_prob": 1.9947199234593427e-06}, {"id": 839, "seek": 432648, "start": 4342.08, "end": 4350.959999999999, "text": " So for example, I was talking to Robin from Planet Labs that broke about the work that", "tokens": [407, 337, 1365, 11, 286, 390, 1417, 281, 16533, 490, 22146, 40047, 300, 6902, 466, 264, 589, 300], "temperature": 0.0, "avg_logprob": -0.20311369615442612, "compression_ratio": 1.4764397905759161, "no_speech_prob": 1.9947199234593427e-06}, {"id": 840, "seek": 432648, "start": 4350.959999999999, "end": 4353.08, "text": " they're doing with satellite imagery.", "tokens": [436, 434, 884, 365, 16016, 24340, 13], "temperature": 0.0, "avg_logprob": -0.20311369615442612, "compression_ratio": 1.4764397905759161, "no_speech_prob": 1.9947199234593427e-06}, {"id": 841, "seek": 435308, "start": 4353.08, "end": 4361.2, "text": " Their satellite imagery is going to have a lot more than a picture of a piece of corn.", "tokens": [6710, 16016, 24340, 307, 516, 281, 362, 257, 688, 544, 813, 257, 3036, 295, 257, 2522, 295, 9046, 13], "temperature": 0.0, "avg_logprob": -0.34744859778362774, "compression_ratio": 1.4576271186440677, "no_speech_prob": 1.4510355867969338e-05}, {"id": 842, "seek": 435308, "start": 4361.2, "end": 4369.44, "text": " So how many pixels is a Planet Labs photo?", "tokens": [407, 577, 867, 18668, 307, 257, 22146, 40047, 5052, 30], "temperature": 0.0, "avg_logprob": -0.34744859778362774, "compression_ratio": 1.4576271186440677, "no_speech_prob": 1.4510355867969338e-05}, {"id": 843, "seek": 435308, "start": 4369.44, "end": 4373.8, "text": " So pretty big, and in that couple million pixels, you're going to have what kind of", "tokens": [407, 1238, 955, 11, 293, 294, 300, 1916, 2459, 18668, 11, 291, 434, 516, 281, 362, 437, 733, 295], "temperature": 0.0, "avg_logprob": -0.34744859778362774, "compression_ratio": 1.4576271186440677, "no_speech_prob": 1.4510355867969338e-05}, {"id": 844, "seek": 435308, "start": 4373.8, "end": 4375.5199999999995, "text": " area with that cover?", "tokens": [1859, 365, 300, 2060, 30], "temperature": 0.0, "avg_logprob": -0.34744859778362774, "compression_ratio": 1.4576271186440677, "no_speech_prob": 1.4510355867969338e-05}, {"id": 845, "seek": 435308, "start": 4375.5199999999995, "end": 4377.5199999999995, "text": " 500 square kilometers.", "tokens": [5923, 3732, 13904, 13], "temperature": 0.0, "avg_logprob": -0.34744859778362774, "compression_ratio": 1.4576271186440677, "no_speech_prob": 1.4510355867969338e-05}, {"id": 846, "seek": 437752, "start": 4377.52, "end": 4386.280000000001, "text": " So there's going to be tennis courts, swimming pools, people sunbathing, all kinds of stuff.", "tokens": [407, 456, 311, 516, 281, 312, 18118, 14141, 11, 11989, 28688, 11, 561, 3295, 11980, 571, 11, 439, 3685, 295, 1507, 13], "temperature": 0.0, "avg_logprob": -0.12459795253792989, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.255331871012459e-06}, {"id": 847, "seek": 437752, "start": 4386.280000000001, "end": 4395.56, "text": " So when Robin takes this stuff to Planet Labs, he's not just going to be able to use a pre-trained", "tokens": [407, 562, 16533, 2516, 341, 1507, 281, 22146, 40047, 11, 415, 311, 406, 445, 516, 281, 312, 1075, 281, 764, 257, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.12459795253792989, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.255331871012459e-06}, {"id": 848, "seek": 437752, "start": 4395.56, "end": 4396.56, "text": " network directly.", "tokens": [3209, 3838, 13], "temperature": 0.0, "avg_logprob": -0.12459795253792989, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.255331871012459e-06}, {"id": 849, "seek": 437752, "start": 4396.56, "end": 4401.280000000001, "text": " But we're going to show you how you can use some of the structure of the pre-trained network", "tokens": [583, 321, 434, 516, 281, 855, 291, 577, 291, 393, 764, 512, 295, 264, 3877, 295, 264, 659, 12, 17227, 2001, 3209], "temperature": 0.0, "avg_logprob": -0.12459795253792989, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.255331871012459e-06}, {"id": 850, "seek": 437752, "start": 4401.280000000001, "end": 4406.160000000001, "text": " even if you are not looking at photos that are this clear.", "tokens": [754, 498, 291, 366, 406, 1237, 412, 5787, 300, 366, 341, 1850, 13], "temperature": 0.0, "avg_logprob": -0.12459795253792989, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.255331871012459e-06}, {"id": 851, "seek": 440616, "start": 4406.16, "end": 4411.5599999999995, "text": " Having said that, if you remember the slide I showed you earlier of the plant disease", "tokens": [10222, 848, 300, 11, 498, 291, 1604, 264, 4137, 286, 4712, 291, 3071, 295, 264, 3709, 4752], "temperature": 0.0, "avg_logprob": -0.13275675254293007, "compression_ratio": 1.7396694214876034, "no_speech_prob": 2.111208777932916e-05}, {"id": 852, "seek": 440616, "start": 4411.5599999999995, "end": 4418.639999999999, "text": " project, each of those plant disease pictures were very clearly just pictures of one thing.", "tokens": [1716, 11, 1184, 295, 729, 3709, 4752, 5242, 645, 588, 4448, 445, 5242, 295, 472, 551, 13], "temperature": 0.0, "avg_logprob": -0.13275675254293007, "compression_ratio": 1.7396694214876034, "no_speech_prob": 2.111208777932916e-05}, {"id": 853, "seek": 440616, "start": 4418.639999999999, "end": 4420.0, "text": " This certainly happens as well.", "tokens": [639, 3297, 2314, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.13275675254293007, "compression_ratio": 1.7396694214876034, "no_speech_prob": 2.111208777932916e-05}, {"id": 854, "seek": 440616, "start": 4420.0, "end": 4425.92, "text": " But do be aware that when you're using a pre-trained network, you are inheriting the shortcomings", "tokens": [583, 360, 312, 3650, 300, 562, 291, 434, 1228, 257, 659, 12, 17227, 2001, 3209, 11, 291, 366, 9484, 1748, 264, 2099, 49886], "temperature": 0.0, "avg_logprob": -0.13275675254293007, "compression_ratio": 1.7396694214876034, "no_speech_prob": 2.111208777932916e-05}, {"id": 855, "seek": 440616, "start": 4425.92, "end": 4429.88, "text": " and biases of the data it was trained from, and therefore you should always look at the", "tokens": [293, 32152, 295, 264, 1412, 309, 390, 8895, 490, 11, 293, 4412, 291, 820, 1009, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.13275675254293007, "compression_ratio": 1.7396694214876034, "no_speech_prob": 2.111208777932916e-05}, {"id": 856, "seek": 440616, "start": 4429.88, "end": 4432.96, "text": " data it was trained from.", "tokens": [1412, 309, 390, 8895, 490, 13], "temperature": 0.0, "avg_logprob": -0.13275675254293007, "compression_ratio": 1.7396694214876034, "no_speech_prob": 2.111208777932916e-05}, {"id": 857, "seek": 443296, "start": 4432.96, "end": 4439.68, "text": " So being aware of that, I would say that for us, this is going to be quite a suitable kind", "tokens": [407, 885, 3650, 295, 300, 11, 286, 576, 584, 300, 337, 505, 11, 341, 307, 516, 281, 312, 1596, 257, 12873, 733], "temperature": 0.0, "avg_logprob": -0.12989212839226974, "compression_ratio": 1.7259615384615385, "no_speech_prob": 1.5446101315319538e-05}, {"id": 858, "seek": 443296, "start": 4439.68, "end": 4440.68, "text": " of dataset.", "tokens": [295, 28872, 13], "temperature": 0.0, "avg_logprob": -0.12989212839226974, "compression_ratio": 1.7259615384615385, "no_speech_prob": 1.5446101315319538e-05}, {"id": 859, "seek": 443296, "start": 4440.68, "end": 4446.02, "text": " And as we look at the dataset, you'll see why I say that.", "tokens": [400, 382, 321, 574, 412, 264, 28872, 11, 291, 603, 536, 983, 286, 584, 300, 13], "temperature": 0.0, "avg_logprob": -0.12989212839226974, "compression_ratio": 1.7259615384615385, "no_speech_prob": 1.5446101315319538e-05}, {"id": 860, "seek": 443296, "start": 4446.02, "end": 4453.36, "text": " So each year, most of the winners of the ImageNet competition make their source code and their", "tokens": [407, 1184, 1064, 11, 881, 295, 264, 17193, 295, 264, 29903, 31890, 6211, 652, 641, 4009, 3089, 293, 641], "temperature": 0.0, "avg_logprob": -0.12989212839226974, "compression_ratio": 1.7259615384615385, "no_speech_prob": 1.5446101315319538e-05}, {"id": 861, "seek": 443296, "start": 4453.36, "end": 4455.96, "text": " weights available.", "tokens": [17443, 2435, 13], "temperature": 0.0, "avg_logprob": -0.12989212839226974, "compression_ratio": 1.7259615384615385, "no_speech_prob": 1.5446101315319538e-05}, {"id": 862, "seek": 443296, "start": 4455.96, "end": 4461.4800000000005, "text": " So when I say their source code and their weights, the source code is the thing that", "tokens": [407, 562, 286, 584, 641, 4009, 3089, 293, 641, 17443, 11, 264, 4009, 3089, 307, 264, 551, 300], "temperature": 0.0, "avg_logprob": -0.12989212839226974, "compression_ratio": 1.7259615384615385, "no_speech_prob": 1.5446101315319538e-05}, {"id": 863, "seek": 446148, "start": 4461.48, "end": 4467.599999999999, "text": " defines, do you remember I told you there are 3 bits that give us modern deep learning?", "tokens": [23122, 11, 360, 291, 1604, 286, 1907, 291, 456, 366, 805, 9239, 300, 976, 505, 4363, 2452, 2539, 30], "temperature": 0.0, "avg_logprob": -0.2346542759945518, "compression_ratio": 1.7232472324723247, "no_speech_prob": 5.422159119916614e-06}, {"id": 864, "seek": 446148, "start": 4467.599999999999, "end": 4471.959999999999, "text": " It's the infinitely flexible function, it's the way to train the parameters, it's the", "tokens": [467, 311, 264, 36227, 11358, 2445, 11, 309, 311, 264, 636, 281, 3847, 264, 9834, 11, 309, 311, 264], "temperature": 0.0, "avg_logprob": -0.2346542759945518, "compression_ratio": 1.7232472324723247, "no_speech_prob": 5.422159119916614e-06}, {"id": 865, "seek": 446148, "start": 4471.959999999999, "end": 4473.5599999999995, "text": " fast and scalable.", "tokens": [2370, 293, 38481, 13], "temperature": 0.0, "avg_logprob": -0.2346542759945518, "compression_ratio": 1.7232472324723247, "no_speech_prob": 5.422159119916614e-06}, {"id": 866, "seek": 446148, "start": 4473.5599999999995, "end": 4477.48, "text": " The particular functional form is what is the neural network architecture.", "tokens": [440, 1729, 11745, 1254, 307, 437, 307, 264, 18161, 3209, 9482, 13], "temperature": 0.0, "avg_logprob": -0.2346542759945518, "compression_ratio": 1.7232472324723247, "no_speech_prob": 5.422159119916614e-06}, {"id": 867, "seek": 446148, "start": 4477.48, "end": 4479.5599999999995, "text": " We're going to be learning a lot about that.", "tokens": [492, 434, 516, 281, 312, 2539, 257, 688, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.2346542759945518, "compression_ratio": 1.7232472324723247, "no_speech_prob": 5.422159119916614e-06}, {"id": 868, "seek": 446148, "start": 4479.5599999999995, "end": 4481.48, "text": " But that's the source code.", "tokens": [583, 300, 311, 264, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.2346542759945518, "compression_ratio": 1.7232472324723247, "no_speech_prob": 5.422159119916614e-06}, {"id": 869, "seek": 446148, "start": 4481.48, "end": 4486.08, "text": " So generally you download the source code from the folks that built the model.", "tokens": [407, 5101, 291, 5484, 264, 4009, 3089, 490, 264, 4024, 300, 3094, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2346542759945518, "compression_ratio": 1.7232472324723247, "no_speech_prob": 5.422159119916614e-06}, {"id": 870, "seek": 446148, "start": 4486.08, "end": 4488.799999999999, "text": " The second is the parameters that were learned.", "tokens": [440, 1150, 307, 264, 9834, 300, 645, 3264, 13], "temperature": 0.0, "avg_logprob": -0.2346542759945518, "compression_ratio": 1.7232472324723247, "no_speech_prob": 5.422159119916614e-06}, {"id": 871, "seek": 448880, "start": 4488.8, "end": 4495.400000000001, "text": " So generally an ImageNet winner has trained a model for days or weeks, nowadays often", "tokens": [407, 5101, 364, 29903, 31890, 8507, 575, 8895, 257, 2316, 337, 1708, 420, 3259, 11, 13434, 2049], "temperature": 0.0, "avg_logprob": -0.1224223878648546, "compression_ratio": 1.676991150442478, "no_speech_prob": 3.500823822832899e-06}, {"id": 872, "seek": 448880, "start": 4495.400000000001, "end": 4500.400000000001, "text": " on many GPUs, to find the particular set of parameters, the particular weights that make", "tokens": [322, 867, 18407, 82, 11, 281, 915, 264, 1729, 992, 295, 9834, 11, 264, 1729, 17443, 300, 652], "temperature": 0.0, "avg_logprob": -0.1224223878648546, "compression_ratio": 1.676991150442478, "no_speech_prob": 3.500823822832899e-06}, {"id": 873, "seek": 448880, "start": 4500.400000000001, "end": 4503.84, "text": " it really good at recognizing those ImageNet pictures.", "tokens": [309, 534, 665, 412, 18538, 729, 29903, 31890, 5242, 13], "temperature": 0.0, "avg_logprob": -0.1224223878648546, "compression_ratio": 1.676991150442478, "no_speech_prob": 3.500823822832899e-06}, {"id": 874, "seek": 448880, "start": 4503.84, "end": 4507.64, "text": " So you generally have to get the code and the weights.", "tokens": [407, 291, 5101, 362, 281, 483, 264, 3089, 293, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1224223878648546, "compression_ratio": 1.676991150442478, "no_speech_prob": 3.500823822832899e-06}, {"id": 875, "seek": 448880, "start": 4507.64, "end": 4513.72, "text": " And once you have those 2 things, you can replicate that particular ImageNet winner's", "tokens": [400, 1564, 291, 362, 729, 568, 721, 11, 291, 393, 25356, 300, 1729, 29903, 31890, 8507, 311], "temperature": 0.0, "avg_logprob": -0.1224223878648546, "compression_ratio": 1.676991150442478, "no_speech_prob": 3.500823822832899e-06}, {"id": 876, "seek": 448880, "start": 4513.72, "end": 4517.04, "text": " results.", "tokens": [3542, 13], "temperature": 0.0, "avg_logprob": -0.1224223878648546, "compression_ratio": 1.676991150442478, "no_speech_prob": 3.500823822832899e-06}, {"id": 877, "seek": 451704, "start": 4517.04, "end": 4525.08, "text": " Another winner of 2014 was the Visual Geometry Group, it's an Oxford University group.", "tokens": [3996, 8507, 295, 8227, 390, 264, 23187, 2876, 34730, 10500, 11, 309, 311, 364, 24786, 3535, 1594, 13], "temperature": 0.0, "avg_logprob": -0.21676131298667506, "compression_ratio": 1.4103773584905661, "no_speech_prob": 9.516109457763378e-06}, {"id": 878, "seek": 451704, "start": 4525.08, "end": 4527.28, "text": " And so there's this model called VGG.", "tokens": [400, 370, 456, 311, 341, 2316, 1219, 691, 27561, 13], "temperature": 0.0, "avg_logprob": -0.21676131298667506, "compression_ratio": 1.4103773584905661, "no_speech_prob": 9.516109457763378e-06}, {"id": 879, "seek": 451704, "start": 4527.28, "end": 4529.4, "text": " You'll hear about it lots.", "tokens": [509, 603, 1568, 466, 309, 3195, 13], "temperature": 0.0, "avg_logprob": -0.21676131298667506, "compression_ratio": 1.4103773584905661, "no_speech_prob": 9.516109457763378e-06}, {"id": 880, "seek": 451704, "start": 4529.4, "end": 4535.48, "text": " Generally speaking, every year's ImageNet winners, the particular models they used are", "tokens": [21082, 4124, 11, 633, 1064, 311, 29903, 31890, 17193, 11, 264, 1729, 5245, 436, 1143, 366], "temperature": 0.0, "avg_logprob": -0.21676131298667506, "compression_ratio": 1.4103773584905661, "no_speech_prob": 9.516109457763378e-06}, {"id": 881, "seek": 451704, "start": 4535.48, "end": 4538.96, "text": " so well used in the community that people call them by name.", "tokens": [370, 731, 1143, 294, 264, 1768, 300, 561, 818, 552, 538, 1315, 13], "temperature": 0.0, "avg_logprob": -0.21676131298667506, "compression_ratio": 1.4103773584905661, "no_speech_prob": 9.516109457763378e-06}, {"id": 882, "seek": 453896, "start": 4538.96, "end": 4552.24, "text": " So the 2012 winner was AlexNet, 2014 was VGG, 2015 was Conception, 2016 was ResNet, so", "tokens": [407, 264, 9125, 8507, 390, 5202, 31890, 11, 8227, 390, 691, 27561, 11, 7546, 390, 2656, 7311, 11, 6549, 390, 5015, 31890, 11, 370], "temperature": 0.0, "avg_logprob": -0.19155713861638848, "compression_ratio": 1.3153846153846154, "no_speech_prob": 9.368603969051037e-06}, {"id": 883, "seek": 453896, "start": 4552.24, "end": 4554.24, "text": " they all have names.", "tokens": [436, 439, 362, 5288, 13], "temperature": 0.0, "avg_logprob": -0.19155713861638848, "compression_ratio": 1.3153846153846154, "no_speech_prob": 9.368603969051037e-06}, {"id": 884, "seek": 453896, "start": 4554.24, "end": 4562.8, "text": " VGG is a couple of years old, so it's not quite the best today.", "tokens": [691, 27561, 307, 257, 1916, 295, 924, 1331, 11, 370, 309, 311, 406, 1596, 264, 1151, 965, 13], "temperature": 0.0, "avg_logprob": -0.19155713861638848, "compression_ratio": 1.3153846153846154, "no_speech_prob": 9.368603969051037e-06}, {"id": 885, "seek": 456280, "start": 4562.8, "end": 4571.400000000001, "text": " But it's special because it's the last of the really powerful simple architectures.", "tokens": [583, 309, 311, 2121, 570, 309, 311, 264, 1036, 295, 264, 534, 4005, 2199, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.16602546985332783, "compression_ratio": 1.5620915032679739, "no_speech_prob": 3.120078326901421e-05}, {"id": 886, "seek": 456280, "start": 4571.400000000001, "end": 4575.4800000000005, "text": " We will get to the more complex architectures.", "tokens": [492, 486, 483, 281, 264, 544, 3997, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.16602546985332783, "compression_ratio": 1.5620915032679739, "no_speech_prob": 3.120078326901421e-05}, {"id": 887, "seek": 456280, "start": 4575.4800000000005, "end": 4579.96, "text": " Depending on how we go, it might be in this set of classes, if not it will be in next", "tokens": [22539, 322, 577, 321, 352, 11, 309, 1062, 312, 294, 341, 992, 295, 5359, 11, 498, 406, 309, 486, 312, 294, 958], "temperature": 0.0, "avg_logprob": -0.16602546985332783, "compression_ratio": 1.5620915032679739, "no_speech_prob": 3.120078326901421e-05}, {"id": 888, "seek": 456280, "start": 4579.96, "end": 4585.76, "text": " year's set of classes.", "tokens": [1064, 311, 992, 295, 5359, 13], "temperature": 0.0, "avg_logprob": -0.16602546985332783, "compression_ratio": 1.5620915032679739, "no_speech_prob": 3.120078326901421e-05}, {"id": 889, "seek": 458576, "start": 4585.76, "end": 4595.360000000001, "text": " VGG's simpler approach is not much less accurate, and for teaching purposes we're going to be", "tokens": [691, 27561, 311, 18587, 3109, 307, 406, 709, 1570, 8559, 11, 293, 337, 4571, 9932, 321, 434, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.13636229297902325, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.0451337402628269e-05}, {"id": 890, "seek": 458576, "start": 4595.360000000001, "end": 4600.0, "text": " looking at something that is pretty state of the art and is really easy for us to understand.", "tokens": [1237, 412, 746, 300, 307, 1238, 1785, 295, 264, 1523, 293, 307, 534, 1858, 337, 505, 281, 1223, 13], "temperature": 0.0, "avg_logprob": -0.13636229297902325, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.0451337402628269e-05}, {"id": 891, "seek": 458576, "start": 4600.0, "end": 4603.66, "text": " So that's one of the reasons we're using VGG.", "tokens": [407, 300, 311, 472, 295, 264, 4112, 321, 434, 1228, 691, 27561, 13], "temperature": 0.0, "avg_logprob": -0.13636229297902325, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.0451337402628269e-05}, {"id": 892, "seek": 458576, "start": 4603.66, "end": 4608.2, "text": " Another reason we're using VGG is it's excellent for the kinds of problems that we were just", "tokens": [3996, 1778, 321, 434, 1228, 691, 27561, 307, 309, 311, 7103, 337, 264, 3685, 295, 2740, 300, 321, 645, 445], "temperature": 0.0, "avg_logprob": -0.13636229297902325, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.0451337402628269e-05}, {"id": 893, "seek": 458576, "start": 4608.2, "end": 4615.08, "text": " talking about that Robin with his satellite imagery has, which is it's a great network", "tokens": [1417, 466, 300, 16533, 365, 702, 16016, 24340, 575, 11, 597, 307, 309, 311, 257, 869, 3209], "temperature": 0.0, "avg_logprob": -0.13636229297902325, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.0451337402628269e-05}, {"id": 894, "seek": 461508, "start": 4615.08, "end": 4621.24, "text": " for changing so that it works for your problem, even if your problem is a little different.", "tokens": [337, 4473, 370, 300, 309, 1985, 337, 428, 1154, 11, 754, 498, 428, 1154, 307, 257, 707, 819, 13], "temperature": 0.0, "avg_logprob": -0.1467960968758296, "compression_ratio": 1.7, "no_speech_prob": 1.1659250958473422e-05}, {"id": 895, "seek": 461508, "start": 4621.24, "end": 4629.36, "text": " So there's a number of reasons that VGG is a really great thing for us to be using.", "tokens": [407, 456, 311, 257, 1230, 295, 4112, 300, 691, 27561, 307, 257, 534, 869, 551, 337, 505, 281, 312, 1228, 13], "temperature": 0.0, "avg_logprob": -0.1467960968758296, "compression_ratio": 1.7, "no_speech_prob": 1.1659250958473422e-05}, {"id": 896, "seek": 461508, "start": 4629.36, "end": 4636.5599999999995, "text": " My strong preference is to start out by showing you how to do things that you can use tomorrow,", "tokens": [1222, 2068, 17502, 307, 281, 722, 484, 538, 4099, 291, 577, 281, 360, 721, 300, 291, 393, 764, 4153, 11], "temperature": 0.0, "avg_logprob": -0.1467960968758296, "compression_ratio": 1.7, "no_speech_prob": 1.1659250958473422e-05}, {"id": 897, "seek": 461508, "start": 4636.5599999999995, "end": 4640.88, "text": " rather than starting with the 1 plus 1 and showing you how you can do things that are", "tokens": [2831, 813, 2891, 365, 264, 502, 1804, 502, 293, 4099, 291, 577, 291, 393, 360, 721, 300, 366], "temperature": 0.0, "avg_logprob": -0.1467960968758296, "compression_ratio": 1.7, "no_speech_prob": 1.1659250958473422e-05}, {"id": 898, "seek": 461508, "start": 4640.88, "end": 4644.36, "text": " useful in 6 years' time after you've got your PhD.", "tokens": [4420, 294, 1386, 924, 6, 565, 934, 291, 600, 658, 428, 14476, 13], "temperature": 0.0, "avg_logprob": -0.1467960968758296, "compression_ratio": 1.7, "no_speech_prob": 1.1659250958473422e-05}, {"id": 899, "seek": 464436, "start": 4644.36, "end": 4651.24, "text": " So I'm going to start out by showing you 7 lines of code that do everything you need.", "tokens": [407, 286, 478, 516, 281, 722, 484, 538, 4099, 291, 1614, 3876, 295, 3089, 300, 360, 1203, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.1681028796780494, "compression_ratio": 1.5610859728506787, "no_speech_prob": 9.223243068845477e-06}, {"id": 900, "seek": 464436, "start": 4651.24, "end": 4656.12, "text": " To get to the punch line, the state of the art for dogs vs. cats in the academic literature", "tokens": [1407, 483, 281, 264, 8135, 1622, 11, 264, 1785, 295, 264, 1523, 337, 7197, 12041, 13, 11111, 294, 264, 7778, 10394], "temperature": 0.0, "avg_logprob": -0.1681028796780494, "compression_ratio": 1.5610859728506787, "no_speech_prob": 9.223243068845477e-06}, {"id": 901, "seek": 464436, "start": 4656.12, "end": 4657.759999999999, "text": " is 80% accuracy.", "tokens": [307, 4688, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1681028796780494, "compression_ratio": 1.5610859728506787, "no_speech_prob": 9.223243068845477e-06}, {"id": 902, "seek": 464436, "start": 4657.759999999999, "end": 4661.12, "text": " This gives you 97% accuracy.", "tokens": [639, 2709, 291, 23399, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1681028796780494, "compression_ratio": 1.5610859728506787, "no_speech_prob": 9.223243068845477e-06}, {"id": 903, "seek": 464436, "start": 4661.12, "end": 4666.48, "text": " You don't need to do anything else.", "tokens": [509, 500, 380, 643, 281, 360, 1340, 1646, 13], "temperature": 0.0, "avg_logprob": -0.1681028796780494, "compression_ratio": 1.5610859728506787, "no_speech_prob": 9.223243068845477e-06}, {"id": 904, "seek": 464436, "start": 4666.48, "end": 4673.5199999999995, "text": " For you after this class to see if you can get everything working, basically your job", "tokens": [1171, 291, 934, 341, 1508, 281, 536, 498, 291, 393, 483, 1203, 1364, 11, 1936, 428, 1691], "temperature": 0.0, "avg_logprob": -0.1681028796780494, "compression_ratio": 1.5610859728506787, "no_speech_prob": 9.223243068845477e-06}, {"id": 905, "seek": 467352, "start": 4673.52, "end": 4676.88, "text": " will be can you run these 7 lines of code.", "tokens": [486, 312, 393, 291, 1190, 613, 1614, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1458485246908785, "compression_ratio": 1.7389162561576355, "no_speech_prob": 2.046244298981037e-05}, {"id": 906, "seek": 467352, "start": 4676.88, "end": 4683.92, "text": " If you can, then you can rerun it on your own dataset as long as you structure the directories", "tokens": [759, 291, 393, 11, 550, 291, 393, 43819, 409, 309, 322, 428, 1065, 28872, 382, 938, 382, 291, 3877, 264, 5391, 530], "temperature": 0.0, "avg_logprob": -0.1458485246908785, "compression_ratio": 1.7389162561576355, "no_speech_prob": 2.046244298981037e-05}, {"id": 907, "seek": 467352, "start": 4683.92, "end": 4686.02, "text": " the way that I just showed you.", "tokens": [264, 636, 300, 286, 445, 4712, 291, 13], "temperature": 0.0, "avg_logprob": -0.1458485246908785, "compression_ratio": 1.7389162561576355, "no_speech_prob": 2.046244298981037e-05}, {"id": 908, "seek": 467352, "start": 4686.02, "end": 4691.200000000001, "text": " So what I'm going to do is I'm going to go through these 7 lines of code, or something", "tokens": [407, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 352, 807, 613, 1614, 3876, 295, 3089, 11, 420, 746], "temperature": 0.0, "avg_logprob": -0.1458485246908785, "compression_ratio": 1.7389162561576355, "no_speech_prob": 2.046244298981037e-05}, {"id": 909, "seek": 467352, "start": 4691.200000000001, "end": 4697.88, "text": " very similar to them, line by line and show you some pictures of what we're doing along", "tokens": [588, 2531, 281, 552, 11, 1622, 538, 1622, 293, 855, 291, 512, 5242, 295, 437, 321, 434, 884, 2051], "temperature": 0.0, "avg_logprob": -0.1458485246908785, "compression_ratio": 1.7389162561576355, "no_speech_prob": 2.046244298981037e-05}, {"id": 910, "seek": 467352, "start": 4697.88, "end": 4700.88, "text": " the way.", "tokens": [264, 636, 13], "temperature": 0.0, "avg_logprob": -0.1458485246908785, "compression_ratio": 1.7389162561576355, "no_speech_prob": 2.046244298981037e-05}, {"id": 911, "seek": 470088, "start": 4700.88, "end": 4704.64, "text": " I wanted to start by showing you the 7 lines of code though because we're going to be looking", "tokens": [286, 1415, 281, 722, 538, 4099, 291, 264, 1614, 3876, 295, 3089, 1673, 570, 321, 434, 516, 281, 312, 1237], "temperature": 0.0, "avg_logprob": -0.15086976446286596, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.384566530759912e-05}, {"id": 912, "seek": 470088, "start": 4704.64, "end": 4709.92, "text": " at all kinds of things along the way in order to really understand what's going on.", "tokens": [412, 439, 3685, 295, 721, 2051, 264, 636, 294, 1668, 281, 534, 1223, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.15086976446286596, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.384566530759912e-05}, {"id": 913, "seek": 470088, "start": 4709.92, "end": 4715.56, "text": " And at some point you might start thinking, gosh, there's a lot to do to do deep learning.", "tokens": [400, 412, 512, 935, 291, 1062, 722, 1953, 11, 6502, 11, 456, 311, 257, 688, 281, 360, 281, 360, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.15086976446286596, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.384566530759912e-05}, {"id": 914, "seek": 470088, "start": 4715.56, "end": 4716.56, "text": " But there's not.", "tokens": [583, 456, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.15086976446286596, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.384566530759912e-05}, {"id": 915, "seek": 470088, "start": 4716.56, "end": 4721.32, "text": " There's a lot to do to really explain and talk about and visualize and think about deep", "tokens": [821, 311, 257, 688, 281, 360, 281, 534, 2903, 293, 751, 466, 293, 23273, 293, 519, 466, 2452], "temperature": 0.0, "avg_logprob": -0.15086976446286596, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.384566530759912e-05}, {"id": 916, "seek": 470088, "start": 4721.32, "end": 4722.32, "text": " learning.", "tokens": [2539, 13], "temperature": 0.0, "avg_logprob": -0.15086976446286596, "compression_ratio": 1.740909090909091, "no_speech_prob": 1.384566530759912e-05}, {"id": 917, "seek": 472232, "start": 4722.32, "end": 4742.679999999999, "text": " So to actually do image classification, you just need these 7 lines of code.", "tokens": [407, 281, 767, 360, 3256, 21538, 11, 291, 445, 643, 613, 1614, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1927494345040157, "compression_ratio": 1.4150943396226414, "no_speech_prob": 8.800995601632167e-06}, {"id": 918, "seek": 472232, "start": 4742.679999999999, "end": 4746.0, "text": " So what does it mean to train a model that's already trained?", "tokens": [407, 437, 775, 309, 914, 281, 3847, 257, 2316, 300, 311, 1217, 8895, 30], "temperature": 0.0, "avg_logprob": -0.1927494345040157, "compression_ratio": 1.4150943396226414, "no_speech_prob": 8.800995601632167e-06}, {"id": 919, "seek": 472232, "start": 4746.0, "end": 4751.44, "text": " Yes, you're getting a little bit ahead of us, but it's great to answer these questions", "tokens": [1079, 11, 291, 434, 1242, 257, 707, 857, 2286, 295, 505, 11, 457, 309, 311, 869, 281, 1867, 613, 1651], "temperature": 0.0, "avg_logprob": -0.1927494345040157, "compression_ratio": 1.4150943396226414, "no_speech_prob": 8.800995601632167e-06}, {"id": 920, "seek": 475144, "start": 4751.44, "end": 4752.44, "text": " many times.", "tokens": [867, 1413, 13], "temperature": 0.0, "avg_logprob": -0.15703765324183871, "compression_ratio": 1.3701657458563536, "no_speech_prob": 7.766856469970662e-06}, {"id": 921, "seek": 475144, "start": 4752.44, "end": 4755.08, "text": " So let's try and make a starting answer at it.", "tokens": [407, 718, 311, 853, 293, 652, 257, 2891, 1867, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.15703765324183871, "compression_ratio": 1.3701657458563536, "no_speech_prob": 7.766856469970662e-06}, {"id": 922, "seek": 475144, "start": 4755.08, "end": 4764.24, "text": " In this case, the VGG model has been trained to recognize photos of the 1000 types that", "tokens": [682, 341, 1389, 11, 264, 691, 27561, 2316, 575, 668, 8895, 281, 5521, 5787, 295, 264, 9714, 3467, 300], "temperature": 0.0, "avg_logprob": -0.15703765324183871, "compression_ratio": 1.3701657458563536, "no_speech_prob": 7.766856469970662e-06}, {"id": 923, "seek": 475144, "start": 4764.24, "end": 4768.36, "text": " are in the ImageNet competition.", "tokens": [366, 294, 264, 29903, 31890, 6211, 13], "temperature": 0.0, "avg_logprob": -0.15703765324183871, "compression_ratio": 1.3701657458563536, "no_speech_prob": 7.766856469970662e-06}, {"id": 924, "seek": 475144, "start": 4768.36, "end": 4772.96, "text": " There's a number of reasons why that does not give us dogs vs. cats.", "tokens": [821, 311, 257, 1230, 295, 4112, 983, 300, 775, 406, 976, 505, 7197, 12041, 13, 11111, 13], "temperature": 0.0, "avg_logprob": -0.15703765324183871, "compression_ratio": 1.3701657458563536, "no_speech_prob": 7.766856469970662e-06}, {"id": 925, "seek": 477296, "start": 4772.96, "end": 4781.8, "text": " Reason number one is that if we go into the animals section of ImageNet, there we are,", "tokens": [39693, 1230, 472, 307, 300, 498, 321, 352, 666, 264, 4882, 3541, 295, 29903, 31890, 11, 456, 321, 366, 11], "temperature": 0.0, "avg_logprob": -0.40122649452903053, "compression_ratio": 1.463235294117647, "no_speech_prob": 3.3736152545316145e-05}, {"id": 926, "seek": 477296, "start": 4781.8, "end": 4796.24, "text": " animals, beings, beasts and brutes, domesticated I guess, dogs, hunting dogs, sporting dogs,", "tokens": [4882, 11, 8958, 11, 37386, 293, 738, 1819, 11, 39125, 3587, 286, 2041, 11, 7197, 11, 12599, 7197, 11, 32366, 7197, 11], "temperature": 0.0, "avg_logprob": -0.40122649452903053, "compression_ratio": 1.463235294117647, "no_speech_prob": 3.3736152545316145e-05}, {"id": 927, "seek": 477296, "start": 4796.24, "end": 4799.84, "text": " pointers, this one.", "tokens": [44548, 11, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.40122649452903053, "compression_ratio": 1.463235294117647, "no_speech_prob": 3.3736152545316145e-05}, {"id": 928, "seek": 479984, "start": 4799.84, "end": 4808.28, "text": " So they have 2332 pictures of Vizsla, also known as a Hungarian pointer.", "tokens": [407, 436, 362, 6673, 11440, 5242, 295, 691, 590, 82, 875, 11, 611, 2570, 382, 257, 38034, 23918, 13], "temperature": 0.0, "avg_logprob": -0.21803272131717566, "compression_ratio": 1.5855855855855856, "no_speech_prob": 8.664438610139769e-06}, {"id": 929, "seek": 479984, "start": 4808.28, "end": 4813.96, "text": " So you could go through and run it and go back and find all the Vizslas and the German", "tokens": [407, 291, 727, 352, 807, 293, 1190, 309, 293, 352, 646, 293, 915, 439, 264, 691, 590, 82, 7743, 293, 264, 6521], "temperature": 0.0, "avg_logprob": -0.21803272131717566, "compression_ratio": 1.5855855855855856, "no_speech_prob": 8.664438610139769e-06}, {"id": 930, "seek": 479984, "start": 4813.96, "end": 4818.28, "text": " short somethings and the sadist and say, oh they're all dog.", "tokens": [2099, 746, 82, 293, 264, 4227, 468, 293, 584, 11, 1954, 436, 434, 439, 3000, 13], "temperature": 0.0, "avg_logprob": -0.21803272131717566, "compression_ratio": 1.5855855855855856, "no_speech_prob": 8.664438610139769e-06}, {"id": 931, "seek": 479984, "start": 4818.28, "end": 4820.24, "text": " But that's something you would have to do.", "tokens": [583, 300, 311, 746, 291, 576, 362, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.21803272131717566, "compression_ratio": 1.5855855855855856, "no_speech_prob": 8.664438610139769e-06}, {"id": 932, "seek": 479984, "start": 4820.24, "end": 4826.64, "text": " So that's one kind of shortcoming of the VGG approach compared to what we actually want.", "tokens": [407, 300, 311, 472, 733, 295, 2099, 6590, 295, 264, 691, 27561, 3109, 5347, 281, 437, 321, 767, 528, 13], "temperature": 0.0, "avg_logprob": -0.21803272131717566, "compression_ratio": 1.5855855855855856, "no_speech_prob": 8.664438610139769e-06}, {"id": 933, "seek": 482664, "start": 4826.64, "end": 4832.68, "text": " The second shortcoming is that sometimes it's going to get it wrong, and it might get it", "tokens": [440, 1150, 2099, 6590, 307, 300, 2171, 309, 311, 516, 281, 483, 309, 2085, 11, 293, 309, 1062, 483, 309], "temperature": 0.0, "avg_logprob": -0.19670467023496274, "compression_ratio": 1.7400881057268722, "no_speech_prob": 1.8342669136472978e-05}, {"id": 934, "seek": 482664, "start": 4832.68, "end": 4834.6, "text": " wrong for really good reasons.", "tokens": [2085, 337, 534, 665, 4112, 13], "temperature": 0.0, "avg_logprob": -0.19670467023496274, "compression_ratio": 1.7400881057268722, "no_speech_prob": 1.8342669136472978e-05}, {"id": 935, "seek": 482664, "start": 4834.6, "end": 4839.52, "text": " For example, maybe this one comes back with snow.", "tokens": [1171, 1365, 11, 1310, 341, 472, 1487, 646, 365, 5756, 13], "temperature": 0.0, "avg_logprob": -0.19670467023496274, "compression_ratio": 1.7400881057268722, "no_speech_prob": 1.8342669136472978e-05}, {"id": 936, "seek": 482664, "start": 4839.52, "end": 4842.52, "text": " But in fact it's going to come back not just with snow, but it's going to come back with", "tokens": [583, 294, 1186, 309, 311, 516, 281, 808, 646, 406, 445, 365, 5756, 11, 457, 309, 311, 516, 281, 808, 646, 365], "temperature": 0.0, "avg_logprob": -0.19670467023496274, "compression_ratio": 1.7400881057268722, "no_speech_prob": 1.8342669136472978e-05}, {"id": 937, "seek": 482664, "start": 4842.52, "end": 4845.400000000001, "text": " a probability for every one of the 1000 categories.", "tokens": [257, 8482, 337, 633, 472, 295, 264, 9714, 10479, 13], "temperature": 0.0, "avg_logprob": -0.19670467023496274, "compression_ratio": 1.7400881057268722, "no_speech_prob": 1.8342669136472978e-05}, {"id": 938, "seek": 482664, "start": 4845.400000000001, "end": 4853.12, "text": " So it'll be probability of.0003 that it's a mushroom, and.0002 that it's an airport,", "tokens": [407, 309, 603, 312, 8482, 295, 2411, 1360, 18, 300, 309, 311, 257, 12094, 11, 293, 2411, 1360, 17, 300, 309, 311, 364, 10155, 11], "temperature": 0.0, "avg_logprob": -0.19670467023496274, "compression_ratio": 1.7400881057268722, "no_speech_prob": 1.8342669136472978e-05}, {"id": 939, "seek": 485312, "start": 4853.12, "end": 4859.2, "text": " and.0004 that it's snow, and.0003 that it's a German shepherd.", "tokens": [293, 2411, 1360, 19, 300, 309, 311, 5756, 11, 293, 2411, 1360, 18, 300, 309, 311, 257, 6521, 40317, 13], "temperature": 0.0, "avg_logprob": -0.15506860484247623, "compression_ratio": 1.726530612244898, "no_speech_prob": 7.646360245416872e-06}, {"id": 940, "seek": 485312, "start": 4859.2, "end": 4864.22, "text": " And so we want to take advantage of all of that information as well.", "tokens": [400, 370, 321, 528, 281, 747, 5002, 295, 439, 295, 300, 1589, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15506860484247623, "compression_ratio": 1.726530612244898, "no_speech_prob": 7.646360245416872e-06}, {"id": 941, "seek": 485312, "start": 4864.22, "end": 4869.32, "text": " So what this actually does is it does something called fine-tuning, which we're about to learn", "tokens": [407, 437, 341, 767, 775, 307, 309, 775, 746, 1219, 2489, 12, 83, 37726, 11, 597, 321, 434, 466, 281, 1466], "temperature": 0.0, "avg_logprob": -0.15506860484247623, "compression_ratio": 1.726530612244898, "no_speech_prob": 7.646360245416872e-06}, {"id": 942, "seek": 485312, "start": 4869.32, "end": 4870.599999999999, "text": " a lot about.", "tokens": [257, 688, 466, 13], "temperature": 0.0, "avg_logprob": -0.15506860484247623, "compression_ratio": 1.726530612244898, "no_speech_prob": 7.646360245416872e-06}, {"id": 943, "seek": 485312, "start": 4870.599999999999, "end": 4875.5199999999995, "text": " And what fine-tuning does is it takes that pre-trained image model and says, use everything", "tokens": [400, 437, 2489, 12, 83, 37726, 775, 307, 309, 2516, 300, 659, 12, 17227, 2001, 3256, 2316, 293, 1619, 11, 764, 1203], "temperature": 0.0, "avg_logprob": -0.15506860484247623, "compression_ratio": 1.726530612244898, "no_speech_prob": 7.646360245416872e-06}, {"id": 944, "seek": 485312, "start": 4875.5199999999995, "end": 4882.92, "text": " you know about the 1000 categories to figure out which one are cats and which one are dogs.", "tokens": [291, 458, 466, 264, 9714, 10479, 281, 2573, 484, 597, 472, 366, 11111, 293, 597, 472, 366, 7197, 13], "temperature": 0.0, "avg_logprob": -0.15506860484247623, "compression_ratio": 1.726530612244898, "no_speech_prob": 7.646360245416872e-06}, {"id": 945, "seek": 488292, "start": 4882.92, "end": 4887.0, "text": " That's a great question, and we're going to go back and talk about that a second time", "tokens": [663, 311, 257, 869, 1168, 11, 293, 321, 434, 516, 281, 352, 646, 293, 751, 466, 300, 257, 1150, 565], "temperature": 0.0, "avg_logprob": -0.1273040566393124, "compression_ratio": 1.6768558951965065, "no_speech_prob": 4.495095708989538e-06}, {"id": 946, "seek": 488292, "start": 4887.0, "end": 4889.16, "text": " when we get there.", "tokens": [562, 321, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.1273040566393124, "compression_ratio": 1.6768558951965065, "no_speech_prob": 4.495095708989538e-06}, {"id": 947, "seek": 488292, "start": 4889.16, "end": 4894.32, "text": " So this code is going to work for any image recognition task with any number of categories,", "tokens": [407, 341, 3089, 307, 516, 281, 589, 337, 604, 3256, 11150, 5633, 365, 604, 1230, 295, 10479, 11], "temperature": 0.0, "avg_logprob": -0.1273040566393124, "compression_ratio": 1.6768558951965065, "no_speech_prob": 4.495095708989538e-06}, {"id": 948, "seek": 488292, "start": 4894.32, "end": 4899.92, "text": " regardless of whether those categories are in ImageNet or not.", "tokens": [10060, 295, 1968, 729, 10479, 366, 294, 29903, 31890, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.1273040566393124, "compression_ratio": 1.6768558951965065, "no_speech_prob": 4.495095708989538e-06}, {"id": 949, "seek": 488292, "start": 4899.92, "end": 4905.36, "text": " And really the only image recognition that they're not going to do is something where", "tokens": [400, 534, 264, 787, 3256, 11150, 300, 436, 434, 406, 516, 281, 360, 307, 746, 689], "temperature": 0.0, "avg_logprob": -0.1273040566393124, "compression_ratio": 1.6768558951965065, "no_speech_prob": 4.495095708989538e-06}, {"id": 950, "seek": 488292, "start": 4905.36, "end": 4907.96, "text": " you want to recognize lots of objects.", "tokens": [291, 528, 281, 5521, 3195, 295, 6565, 13], "temperature": 0.0, "avg_logprob": -0.1273040566393124, "compression_ratio": 1.6768558951965065, "no_speech_prob": 4.495095708989538e-06}, {"id": 951, "seek": 490796, "start": 4907.96, "end": 4913.8, "text": " This is specifically for recognizing A+.", "tokens": [639, 307, 4682, 337, 18538, 316, 45585], "temperature": 0.0, "avg_logprob": -0.2389673636509822, "compression_ratio": 1.2330827067669172, "no_speech_prob": 1.2805284313799348e-05}, {"id": 952, "seek": 490796, "start": 4913.8, "end": 4915.44, "text": " So let's see how it works.", "tokens": [407, 718, 311, 536, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.2389673636509822, "compression_ratio": 1.2330827067669172, "no_speech_prob": 1.2805284313799348e-05}, {"id": 953, "seek": 490796, "start": 4915.44, "end": 4931.0, "text": " So the VGG object, let's run this, import VGG16.", "tokens": [407, 264, 691, 27561, 2657, 11, 718, 311, 1190, 341, 11, 974, 691, 27561, 6866, 13], "temperature": 0.0, "avg_logprob": -0.2389673636509822, "compression_ratio": 1.2330827067669172, "no_speech_prob": 1.2805284313799348e-05}, {"id": 954, "seek": 490796, "start": 4931.0, "end": 4936.32, "text": " When something's running, it has a little star.", "tokens": [1133, 746, 311, 2614, 11, 309, 575, 257, 707, 3543, 13], "temperature": 0.0, "avg_logprob": -0.2389673636509822, "compression_ratio": 1.2330827067669172, "no_speech_prob": 1.2805284313799348e-05}, {"id": 955, "seek": 493632, "start": 4936.32, "end": 4940.84, "text": " You will probably get this warning that cuDNN is more recent than the one Theano officially", "tokens": [509, 486, 1391, 483, 341, 9164, 300, 2702, 35, 45, 45, 307, 544, 5162, 813, 264, 472, 440, 3730, 12053], "temperature": 0.0, "avg_logprob": -0.15181743964720307, "compression_ratio": 1.4697674418604652, "no_speech_prob": 1.5206672287604306e-05}, {"id": 956, "seek": 493632, "start": 4940.84, "end": 4941.84, "text": " supports.", "tokens": [9346, 13], "temperature": 0.0, "avg_logprob": -0.15181743964720307, "compression_ratio": 1.4697674418604652, "no_speech_prob": 1.5206672287604306e-05}, {"id": 957, "seek": 493632, "start": 4941.84, "end": 4946.16, "text": " So this is a good time to talk about some of the layers that we have going on.", "tokens": [407, 341, 307, 257, 665, 565, 281, 751, 466, 512, 295, 264, 7914, 300, 321, 362, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.15181743964720307, "compression_ratio": 1.4697674418604652, "no_speech_prob": 1.5206672287604306e-05}, {"id": 958, "seek": 493632, "start": 4946.16, "end": 4959.92, "text": " In this example, we're using our VGG16+.", "tokens": [682, 341, 1365, 11, 321, 434, 1228, 527, 691, 27561, 6866, 45585], "temperature": 0.0, "avg_logprob": -0.15181743964720307, "compression_ratio": 1.4697674418604652, "no_speech_prob": 1.5206672287604306e-05}, {"id": 959, "seek": 493632, "start": 4959.92, "end": 4965.62, "text": " It is sitting on top of, as we will see this in detail, Keras, which is the main deep learning", "tokens": [467, 307, 3798, 322, 1192, 295, 11, 382, 321, 486, 536, 341, 294, 2607, 11, 591, 6985, 11, 597, 307, 264, 2135, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.15181743964720307, "compression_ratio": 1.4697674418604652, "no_speech_prob": 1.5206672287604306e-05}, {"id": 960, "seek": 496562, "start": 4965.62, "end": 4968.04, "text": " library we're using and we'll talk a lot about.", "tokens": [6405, 321, 434, 1228, 293, 321, 603, 751, 257, 688, 466, 13], "temperature": 0.0, "avg_logprob": -0.18884883293738733, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.7778262190404348e-05}, {"id": 961, "seek": 496562, "start": 4968.04, "end": 4974.44, "text": " Keras is sitting on top of Theano, which we'll be talking about quite a bit, but less than", "tokens": [591, 6985, 307, 3798, 322, 1192, 295, 440, 3730, 11, 597, 321, 603, 312, 1417, 466, 1596, 257, 857, 11, 457, 1570, 813], "temperature": 0.0, "avg_logprob": -0.18884883293738733, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.7778262190404348e-05}, {"id": 962, "seek": 496562, "start": 4974.44, "end": 4975.44, "text": " Keras.", "tokens": [591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.18884883293738733, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.7778262190404348e-05}, {"id": 963, "seek": 496562, "start": 4975.44, "end": 4987.5199999999995, "text": " Theano is the thing that takes Python code and turns it into compiled GPU code.", "tokens": [440, 3730, 307, 264, 551, 300, 2516, 15329, 3089, 293, 4523, 309, 666, 36548, 18407, 3089, 13], "temperature": 0.0, "avg_logprob": -0.18884883293738733, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.7778262190404348e-05}, {"id": 964, "seek": 498752, "start": 4987.52, "end": 4997.0, "text": " Theano is sitting on top of a number of things, broadly speaking, NVIDIA's CUDA programming", "tokens": [440, 3730, 307, 3798, 322, 1192, 295, 257, 1230, 295, 721, 11, 19511, 4124, 11, 426, 3958, 6914, 311, 29777, 7509, 9410], "temperature": 0.0, "avg_logprob": -0.15186210779043344, "compression_ratio": 1.2246376811594204, "no_speech_prob": 4.710880148195429e-06}, {"id": 965, "seek": 498752, "start": 4997.0, "end": 4998.6, "text": " environment.", "tokens": [2823, 13], "temperature": 0.0, "avg_logprob": -0.15186210779043344, "compression_ratio": 1.2246376811594204, "no_speech_prob": 4.710880148195429e-06}, {"id": 966, "seek": 498752, "start": 4998.6, "end": 5008.280000000001, "text": " And part of CUDA is the CUDA Deep Neural Network Library, cuDNN.", "tokens": [400, 644, 295, 29777, 7509, 307, 264, 29777, 7509, 14895, 1734, 1807, 12640, 12806, 11, 2702, 35, 45, 45, 13], "temperature": 0.0, "avg_logprob": -0.15186210779043344, "compression_ratio": 1.2246376811594204, "no_speech_prob": 4.710880148195429e-06}, {"id": 967, "seek": 500828, "start": 5008.28, "end": 5017.92, "text": " For most important things in deep learning, Theano is simply calling a function inside", "tokens": [1171, 881, 1021, 721, 294, 2452, 2539, 11, 440, 3730, 307, 2935, 5141, 257, 2445, 1854], "temperature": 0.0, "avg_logprob": -0.2107900034996771, "compression_ratio": 1.3742331288343559, "no_speech_prob": 1.2805302503693383e-05}, {"id": 968, "seek": 500828, "start": 5017.92, "end": 5018.92, "text": " cuDNN.", "tokens": [2702, 35, 45, 45, 13], "temperature": 0.0, "avg_logprob": -0.2107900034996771, "compression_ratio": 1.3742331288343559, "no_speech_prob": 1.2805302503693383e-05}, {"id": 969, "seek": 500828, "start": 5018.92, "end": 5023.599999999999, "text": " So one of the things that we've set up for you in the AMIs and the scripts is to get", "tokens": [407, 472, 295, 264, 721, 300, 321, 600, 992, 493, 337, 291, 294, 264, 6475, 6802, 293, 264, 23294, 307, 281, 483], "temperature": 0.0, "avg_logprob": -0.2107900034996771, "compression_ratio": 1.3742331288343559, "no_speech_prob": 1.2805302503693383e-05}, {"id": 970, "seek": 500828, "start": 5023.599999999999, "end": 5027.16, "text": " all of this stuff stuck together and working.", "tokens": [439, 295, 341, 1507, 5541, 1214, 293, 1364, 13], "temperature": 0.0, "avg_logprob": -0.2107900034996771, "compression_ratio": 1.3742331288343559, "no_speech_prob": 1.2805302503693383e-05}, {"id": 971, "seek": 502716, "start": 5027.16, "end": 5039.76, "text": " So Keras is all written in pure Python.", "tokens": [407, 591, 6985, 307, 439, 3720, 294, 6075, 15329, 13], "temperature": 0.0, "avg_logprob": -0.22563539232526506, "compression_ratio": 1.4071428571428573, "no_speech_prob": 6.854188995930599e-06}, {"id": 972, "seek": 502716, "start": 5039.76, "end": 5047.5199999999995, "text": " And what it does is it takes your deep learning architectures and code and turns it into,", "tokens": [400, 437, 309, 775, 307, 309, 2516, 428, 2452, 2539, 6331, 1303, 293, 3089, 293, 4523, 309, 666, 11], "temperature": 0.0, "avg_logprob": -0.22563539232526506, "compression_ratio": 1.4071428571428573, "no_speech_prob": 6.854188995930599e-06}, {"id": 973, "seek": 502716, "start": 5047.5199999999995, "end": 5049.88, "text": " in our case, Theano code.", "tokens": [294, 527, 1389, 11, 440, 3730, 3089, 13], "temperature": 0.0, "avg_logprob": -0.22563539232526506, "compression_ratio": 1.4071428571428573, "no_speech_prob": 6.854188995930599e-06}, {"id": 974, "seek": 502716, "start": 5049.88, "end": 5055.72, "text": " It can also turn it into TensorFlow code.", "tokens": [467, 393, 611, 1261, 309, 666, 37624, 3089, 13], "temperature": 0.0, "avg_logprob": -0.22563539232526506, "compression_ratio": 1.4071428571428573, "no_speech_prob": 6.854188995930599e-06}, {"id": 975, "seek": 505572, "start": 5055.72, "end": 5059.4800000000005, "text": " TensorFlow and Theano are very similar.", "tokens": [37624, 293, 440, 3730, 366, 588, 2531, 13], "temperature": 0.0, "avg_logprob": -0.19918976301028404, "compression_ratio": 1.6584158415841583, "no_speech_prob": 1.3631050023832358e-05}, {"id": 976, "seek": 505572, "start": 5059.4800000000005, "end": 5066.08, "text": " They're both libraries that sit on top of CUDA and provide really a kind of Python to", "tokens": [814, 434, 1293, 15148, 300, 1394, 322, 1192, 295, 29777, 7509, 293, 2893, 534, 257, 733, 295, 15329, 281], "temperature": 0.0, "avg_logprob": -0.19918976301028404, "compression_ratio": 1.6584158415841583, "no_speech_prob": 1.3631050023832358e-05}, {"id": 977, "seek": 505572, "start": 5066.08, "end": 5072.56, "text": " GPU mapping and lots of libraries on top of that.", "tokens": [18407, 18350, 293, 3195, 295, 15148, 322, 1192, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.19918976301028404, "compression_ratio": 1.6584158415841583, "no_speech_prob": 1.3631050023832358e-05}, {"id": 978, "seek": 505572, "start": 5072.56, "end": 5080.4800000000005, "text": " TensorFlow comes out of Google and it is particularly good for stuff that Google really cares about,", "tokens": [37624, 1487, 484, 295, 3329, 293, 309, 307, 4098, 665, 337, 1507, 300, 3329, 534, 12310, 466, 11], "temperature": 0.0, "avg_logprob": -0.19918976301028404, "compression_ratio": 1.6584158415841583, "no_speech_prob": 1.3631050023832358e-05}, {"id": 979, "seek": 505572, "start": 5080.4800000000005, "end": 5085.280000000001, "text": " and in particular running things on lots and lots of GPUs.", "tokens": [293, 294, 1729, 2614, 721, 322, 3195, 293, 3195, 295, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.19918976301028404, "compression_ratio": 1.6584158415841583, "no_speech_prob": 1.3631050023832358e-05}, {"id": 980, "seek": 508528, "start": 5085.28, "end": 5090.759999999999, "text": " One of the things you'll hear a lot is you can't do anything with deep learning unless", "tokens": [1485, 295, 264, 721, 291, 603, 1568, 257, 688, 307, 291, 393, 380, 360, 1340, 365, 2452, 2539, 5969], "temperature": 0.0, "avg_logprob": -0.1272260789518003, "compression_ratio": 1.634453781512605, "no_speech_prob": 2.8408971047610976e-05}, {"id": 981, "seek": 508528, "start": 5090.759999999999, "end": 5094.04, "text": " you have shitloads of data and shitloads of GPUs.", "tokens": [291, 362, 4611, 2907, 82, 295, 1412, 293, 4611, 2907, 82, 295, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.1272260789518003, "compression_ratio": 1.634453781512605, "no_speech_prob": 2.8408971047610976e-05}, {"id": 982, "seek": 508528, "start": 5094.04, "end": 5099.28, "text": " That is totally, totally wrong, as you'll see throughout this course.", "tokens": [663, 307, 3879, 11, 3879, 2085, 11, 382, 291, 603, 536, 3710, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.1272260789518003, "compression_ratio": 1.634453781512605, "no_speech_prob": 2.8408971047610976e-05}, {"id": 983, "seek": 508528, "start": 5099.28, "end": 5104.96, "text": " It is true if you want to win ImageNet next year, you will need lots and lots of GPUs", "tokens": [467, 307, 2074, 498, 291, 528, 281, 1942, 29903, 31890, 958, 1064, 11, 291, 486, 643, 3195, 293, 3195, 295, 18407, 82], "temperature": 0.0, "avg_logprob": -0.1272260789518003, "compression_ratio": 1.634453781512605, "no_speech_prob": 2.8408971047610976e-05}, {"id": 984, "seek": 508528, "start": 5104.96, "end": 5110.5199999999995, "text": " because you'll be competing for that last 0.1% against Google, against Microsoft, against", "tokens": [570, 291, 603, 312, 15439, 337, 300, 1036, 1958, 13, 16, 4, 1970, 3329, 11, 1970, 8116, 11, 1970], "temperature": 0.0, "avg_logprob": -0.1272260789518003, "compression_ratio": 1.634453781512605, "no_speech_prob": 2.8408971047610976e-05}, {"id": 985, "seek": 508528, "start": 5110.5199999999995, "end": 5111.5199999999995, "text": " Baidu.", "tokens": [6777, 327, 84, 13], "temperature": 0.0, "avg_logprob": -0.1272260789518003, "compression_ratio": 1.634453781512605, "no_speech_prob": 2.8408971047610976e-05}, {"id": 986, "seek": 511152, "start": 5111.52, "end": 5117.72, "text": " But if you're trying to recognize 10 different skin lesions, like the folks I just showed", "tokens": [583, 498, 291, 434, 1382, 281, 5521, 1266, 819, 3178, 1512, 626, 11, 411, 264, 4024, 286, 445, 4712], "temperature": 0.0, "avg_logprob": -0.15016915116991317, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.76937726084725e-06}, {"id": 987, "seek": 511152, "start": 5117.72, "end": 5122.160000000001, "text": " you were, they were the first people to try to do that with deep learning and they quadrupled", "tokens": [291, 645, 11, 436, 645, 264, 700, 561, 281, 853, 281, 360, 300, 365, 2452, 2539, 293, 436, 10787, 894, 15551], "temperature": 0.0, "avg_logprob": -0.15016915116991317, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.76937726084725e-06}, {"id": 988, "seek": 511152, "start": 5122.160000000001, "end": 5128.820000000001, "text": " the previous state of the art using one GPU and a very small amount of data that they", "tokens": [264, 3894, 1785, 295, 264, 1523, 1228, 472, 18407, 293, 257, 588, 1359, 2372, 295, 1412, 300, 436], "temperature": 0.0, "avg_logprob": -0.15016915116991317, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.76937726084725e-06}, {"id": 989, "seek": 511152, "start": 5128.820000000001, "end": 5130.68, "text": " had hand collected.", "tokens": [632, 1011, 11087, 13], "temperature": 0.0, "avg_logprob": -0.15016915116991317, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.76937726084725e-06}, {"id": 990, "seek": 511152, "start": 5130.68, "end": 5136.68, "text": " So the reason you see a lot of this stuff about lots of GPUs and lots of data is it's", "tokens": [407, 264, 1778, 291, 536, 257, 688, 295, 341, 1507, 466, 3195, 295, 18407, 82, 293, 3195, 295, 1412, 307, 309, 311], "temperature": 0.0, "avg_logprob": -0.15016915116991317, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.76937726084725e-06}, {"id": 991, "seek": 511152, "start": 5136.68, "end": 5139.68, "text": " part of the trying to make neural networks cool rather than uncool.", "tokens": [644, 295, 264, 1382, 281, 652, 18161, 9590, 1627, 2831, 813, 6219, 1092, 13], "temperature": 0.0, "avg_logprob": -0.15016915116991317, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.76937726084725e-06}, {"id": 992, "seek": 513968, "start": 5139.68, "end": 5142.360000000001, "text": " It's trying to make it exclusive rather than inclusive.", "tokens": [467, 311, 1382, 281, 652, 309, 13005, 2831, 813, 13429, 13], "temperature": 0.0, "avg_logprob": -0.13813188246318273, "compression_ratio": 1.6141078838174274, "no_speech_prob": 9.36860578804044e-06}, {"id": 993, "seek": 513968, "start": 5142.360000000001, "end": 5146.4800000000005, "text": " It's like unless you're us, you're not in the club.", "tokens": [467, 311, 411, 5969, 291, 434, 505, 11, 291, 434, 406, 294, 264, 6482, 13], "temperature": 0.0, "avg_logprob": -0.13813188246318273, "compression_ratio": 1.6141078838174274, "no_speech_prob": 9.36860578804044e-06}, {"id": 994, "seek": 513968, "start": 5146.4800000000005, "end": 5151.16, "text": " And I really don't want you to go in for that kind of thing.", "tokens": [400, 286, 534, 500, 380, 528, 291, 281, 352, 294, 337, 300, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.13813188246318273, "compression_ratio": 1.6141078838174274, "no_speech_prob": 9.36860578804044e-06}, {"id": 995, "seek": 513968, "start": 5151.16, "end": 5155.68, "text": " You will find again and again it's not true.", "tokens": [509, 486, 915, 797, 293, 797, 309, 311, 406, 2074, 13], "temperature": 0.0, "avg_logprob": -0.13813188246318273, "compression_ratio": 1.6141078838174274, "no_speech_prob": 9.36860578804044e-06}, {"id": 996, "seek": 513968, "start": 5155.68, "end": 5160.08, "text": " As I've just shown you in fact with 7 lines of code, you can turn the state of the art", "tokens": [1018, 286, 600, 445, 4898, 291, 294, 1186, 365, 1614, 3876, 295, 3089, 11, 291, 393, 1261, 264, 1785, 295, 264, 1523], "temperature": 0.0, "avg_logprob": -0.13813188246318273, "compression_ratio": 1.6141078838174274, "no_speech_prob": 9.36860578804044e-06}, {"id": 997, "seek": 513968, "start": 5160.08, "end": 5166.400000000001, "text": " from a 20% error rate to a 3% error rate and it takes about 5 minutes to run on a single", "tokens": [490, 257, 945, 4, 6713, 3314, 281, 257, 805, 4, 6713, 3314, 293, 309, 2516, 466, 1025, 2077, 281, 1190, 322, 257, 2167], "temperature": 0.0, "avg_logprob": -0.13813188246318273, "compression_ratio": 1.6141078838174274, "no_speech_prob": 9.36860578804044e-06}, {"id": 998, "seek": 516640, "start": 5166.4, "end": 5170.04, "text": " GPU which costs 90 cents an hour.", "tokens": [18407, 597, 5497, 4289, 14941, 364, 1773, 13], "temperature": 0.0, "avg_logprob": -0.14833586143724847, "compression_ratio": 1.4615384615384615, "no_speech_prob": 7.071799245750299e-06}, {"id": 999, "seek": 516640, "start": 5170.04, "end": 5181.759999999999, "text": " So I am not going to be talking much about TensorFlow in this course because it's still", "tokens": [407, 286, 669, 406, 516, 281, 312, 1417, 709, 466, 37624, 294, 341, 1164, 570, 309, 311, 920], "temperature": 0.0, "avg_logprob": -0.14833586143724847, "compression_ratio": 1.4615384615384615, "no_speech_prob": 7.071799245750299e-06}, {"id": 1000, "seek": 516640, "start": 5181.759999999999, "end": 5187.96, "text": " very early, it's still very new, it does some cool things, but not the kind of cool things", "tokens": [588, 2440, 11, 309, 311, 920, 588, 777, 11, 309, 775, 512, 1627, 721, 11, 457, 406, 264, 733, 295, 1627, 721], "temperature": 0.0, "avg_logprob": -0.14833586143724847, "compression_ratio": 1.4615384615384615, "no_speech_prob": 7.071799245750299e-06}, {"id": 1001, "seek": 516640, "start": 5187.96, "end": 5191.4, "text": " that uncool people need access to.", "tokens": [300, 6219, 1092, 561, 643, 2105, 281, 13], "temperature": 0.0, "avg_logprob": -0.14833586143724847, "compression_ratio": 1.4615384615384615, "no_speech_prob": 7.071799245750299e-06}, {"id": 1002, "seek": 519140, "start": 5191.4, "end": 5196.879999999999, "text": " Theano, on the other hand, has been around quite a lot longer.", "tokens": [440, 3730, 11, 322, 264, 661, 1011, 11, 575, 668, 926, 1596, 257, 688, 2854, 13], "temperature": 0.0, "avg_logprob": -0.13053597013155618, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.952494312718045e-05}, {"id": 1003, "seek": 519140, "start": 5196.879999999999, "end": 5198.879999999999, "text": " It's much easier to use.", "tokens": [467, 311, 709, 3571, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.13053597013155618, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.952494312718045e-05}, {"id": 1004, "seek": 519140, "start": 5198.879999999999, "end": 5204.5199999999995, "text": " It does not do multi-GPUs well, but it does everything else well.", "tokens": [467, 775, 406, 360, 4825, 12, 38, 8115, 82, 731, 11, 457, 309, 775, 1203, 1646, 731, 13], "temperature": 0.0, "avg_logprob": -0.13053597013155618, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.952494312718045e-05}, {"id": 1005, "seek": 519140, "start": 5204.5199999999995, "end": 5213.36, "text": " If you build something in Keras and you get to a point where you're like, okay, this is", "tokens": [759, 291, 1322, 746, 294, 591, 6985, 293, 291, 483, 281, 257, 935, 689, 291, 434, 411, 11, 1392, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.13053597013155618, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.952494312718045e-05}, {"id": 1006, "seek": 519140, "start": 5213.36, "end": 5218.5599999999995, "text": " working great, we've got a 400% improvement in the state of the art, I want the extra", "tokens": [1364, 869, 11, 321, 600, 658, 257, 8423, 4, 10444, 294, 264, 1785, 295, 264, 1523, 11, 286, 528, 264, 2857], "temperature": 0.0, "avg_logprob": -0.13053597013155618, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.952494312718045e-05}, {"id": 1007, "seek": 521856, "start": 5218.56, "end": 5222.400000000001, "text": " 5% that comes from running this on 8 GPUs.", "tokens": [1025, 4, 300, 1487, 490, 2614, 341, 322, 1649, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.22987614402288123, "compression_ratio": 1.5414634146341464, "no_speech_prob": 2.0144972950220108e-05}, {"id": 1008, "seek": 521856, "start": 5222.400000000001, "end": 5227.4400000000005, "text": " It's a simple configuration change to change the backend to TensorFlow.", "tokens": [467, 311, 257, 2199, 11694, 1319, 281, 1319, 264, 38087, 281, 37624, 13], "temperature": 0.0, "avg_logprob": -0.22987614402288123, "compression_ratio": 1.5414634146341464, "no_speech_prob": 2.0144972950220108e-05}, {"id": 1009, "seek": 521856, "start": 5227.4400000000005, "end": 5233.400000000001, "text": " Specifically I want to show you that configuration change.", "tokens": [26058, 286, 528, 281, 855, 291, 300, 11694, 1319, 13], "temperature": 0.0, "avg_logprob": -0.22987614402288123, "compression_ratio": 1.5414634146341464, "no_speech_prob": 2.0144972950220108e-05}, {"id": 1010, "seek": 521856, "start": 5233.400000000001, "end": 5237.96, "text": " You'll find in your home directory, so for those of you who haven't used bash before,", "tokens": [509, 603, 915, 294, 428, 1280, 21120, 11, 370, 337, 729, 295, 291, 567, 2378, 380, 1143, 46183, 949, 11], "temperature": 0.0, "avg_logprob": -0.22987614402288123, "compression_ratio": 1.5414634146341464, "no_speech_prob": 2.0144972950220108e-05}, {"id": 1011, "seek": 521856, "start": 5237.96, "end": 5241.6, "text": " when you see tilde, that just means your home directory.", "tokens": [562, 291, 536, 45046, 11, 300, 445, 1355, 428, 1280, 21120, 13], "temperature": 0.0, "avg_logprob": -0.22987614402288123, "compression_ratio": 1.5414634146341464, "no_speech_prob": 2.0144972950220108e-05}, {"id": 1012, "seek": 524160, "start": 5241.6, "end": 5250.8, "text": " In your home directory there is a.Keras folder and in there is a Keras.json file.", "tokens": [682, 428, 1280, 21120, 456, 307, 257, 2411, 42, 6985, 10820, 293, 294, 456, 307, 257, 591, 6985, 13, 73, 3015, 3991, 13], "temperature": 0.0, "avg_logprob": -0.24576372849313835, "compression_ratio": 1.4689265536723164, "no_speech_prob": 5.955088454356883e-06}, {"id": 1013, "seek": 524160, "start": 5250.8, "end": 5252.4800000000005, "text": " This is the configuration.", "tokens": [639, 307, 264, 11694, 13], "temperature": 0.0, "avg_logprob": -0.24576372849313835, "compression_ratio": 1.4689265536723164, "no_speech_prob": 5.955088454356883e-06}, {"id": 1014, "seek": 524160, "start": 5252.4800000000005, "end": 5259.68, "text": " You'll see here backend Theano.", "tokens": [509, 603, 536, 510, 38087, 440, 3730, 13], "temperature": 0.0, "avg_logprob": -0.24576372849313835, "compression_ratio": 1.4689265536723164, "no_speech_prob": 5.955088454356883e-06}, {"id": 1015, "seek": 524160, "start": 5259.68, "end": 5265.68, "text": " If you change this to say TensorFlow and rerun it, it's now using TensorFlow.", "tokens": [759, 291, 1319, 341, 281, 584, 37624, 293, 43819, 409, 309, 11, 309, 311, 586, 1228, 37624, 13], "temperature": 0.0, "avg_logprob": -0.24576372849313835, "compression_ratio": 1.4689265536723164, "no_speech_prob": 5.955088454356883e-06}, {"id": 1016, "seek": 524160, "start": 5265.68, "end": 5269.56, "text": " And TensorFlow will use all of your GPUs.", "tokens": [400, 37624, 486, 764, 439, 295, 428, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.24576372849313835, "compression_ratio": 1.4689265536723164, "no_speech_prob": 5.955088454356883e-06}, {"id": 1017, "seek": 526956, "start": 5269.56, "end": 5273.92, "text": " If you do that, I also suggest changing this thing here that says th, which stands for", "tokens": [759, 291, 360, 300, 11, 286, 611, 3402, 4473, 341, 551, 510, 300, 1619, 258, 11, 597, 7382, 337], "temperature": 0.0, "avg_logprob": -0.21152558046228745, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.5445986718987115e-05}, {"id": 1018, "seek": 526956, "start": 5273.92, "end": 5278.88, "text": " Theano, to tf, which stands for TensorFlow.", "tokens": [440, 3730, 11, 281, 256, 69, 11, 597, 7382, 337, 37624, 13], "temperature": 0.0, "avg_logprob": -0.21152558046228745, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.5445986718987115e-05}, {"id": 1019, "seek": 526956, "start": 5278.88, "end": 5281.400000000001, "text": " We may talk about that in the next course.", "tokens": [492, 815, 751, 466, 300, 294, 264, 958, 1164, 13], "temperature": 0.0, "avg_logprob": -0.21152558046228745, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.5445986718987115e-05}, {"id": 1020, "seek": 526956, "start": 5281.400000000001, "end": 5284.56, "text": " It's a pretty minor detail.", "tokens": [467, 311, 257, 1238, 6696, 2607, 13], "temperature": 0.0, "avg_logprob": -0.21152558046228745, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.5445986718987115e-05}, {"id": 1021, "seek": 526956, "start": 5284.56, "end": 5291.92, "text": " The other configuration file to be aware of is Theano.rc.", "tokens": [440, 661, 11694, 3991, 281, 312, 3650, 295, 307, 440, 3730, 13, 81, 66, 13], "temperature": 0.0, "avg_logprob": -0.21152558046228745, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.5445986718987115e-05}, {"id": 1022, "seek": 526956, "start": 5291.92, "end": 5298.04, "text": " You'll find a lot of Unix-y things are something.rc, is how they name their configuration files.", "tokens": [509, 603, 915, 257, 688, 295, 1156, 970, 12, 88, 721, 366, 746, 13, 81, 66, 11, 307, 577, 436, 1315, 641, 11694, 7098, 13], "temperature": 0.0, "avg_logprob": -0.21152558046228745, "compression_ratio": 1.6330275229357798, "no_speech_prob": 1.5445986718987115e-05}, {"id": 1023, "seek": 529804, "start": 5298.04, "end": 5300.44, "text": " Here is.something.rc.", "tokens": [1692, 307, 2411, 31681, 13, 81, 66, 13], "temperature": 0.0, "avg_logprob": -0.15833345481327601, "compression_ratio": 1.5659574468085107, "no_speech_prob": 6.540299636981217e-06}, {"id": 1024, "seek": 529804, "start": 5300.44, "end": 5308.24, "text": " I want to point out that there's a really important line here, which is device equals.", "tokens": [286, 528, 281, 935, 484, 300, 456, 311, 257, 534, 1021, 1622, 510, 11, 597, 307, 4302, 6915, 13], "temperature": 0.0, "avg_logprob": -0.15833345481327601, "compression_ratio": 1.5659574468085107, "no_speech_prob": 6.540299636981217e-06}, {"id": 1025, "seek": 529804, "start": 5308.24, "end": 5311.16, "text": " This is either GPU or CPU.", "tokens": [639, 307, 2139, 18407, 420, 13199, 13], "temperature": 0.0, "avg_logprob": -0.15833345481327601, "compression_ratio": 1.5659574468085107, "no_speech_prob": 6.540299636981217e-06}, {"id": 1026, "seek": 529804, "start": 5311.16, "end": 5317.04, "text": " If you're using a T2 instance, you'll find that the AMI we've created has changed the", "tokens": [759, 291, 434, 1228, 257, 314, 17, 5197, 11, 291, 603, 915, 300, 264, 6475, 40, 321, 600, 2942, 575, 3105, 264], "temperature": 0.0, "avg_logprob": -0.15833345481327601, "compression_ratio": 1.5659574468085107, "no_speech_prob": 6.540299636981217e-06}, {"id": 1027, "seek": 529804, "start": 5317.04, "end": 5322.1, "text": " G to a C. That's because the T2 instance does not support GPU.", "tokens": [460, 281, 257, 383, 13, 663, 311, 570, 264, 314, 17, 5197, 775, 406, 1406, 18407, 13], "temperature": 0.0, "avg_logprob": -0.15833345481327601, "compression_ratio": 1.5659574468085107, "no_speech_prob": 6.540299636981217e-06}, {"id": 1028, "seek": 529804, "start": 5322.1, "end": 5327.32, "text": " So if you want to switch from GPU to CPU, just change the G to a C or the C to a G.", "tokens": [407, 498, 291, 528, 281, 3679, 490, 18407, 281, 13199, 11, 445, 1319, 264, 460, 281, 257, 383, 420, 264, 383, 281, 257, 460, 13], "temperature": 0.0, "avg_logprob": -0.15833345481327601, "compression_ratio": 1.5659574468085107, "no_speech_prob": 6.540299636981217e-06}, {"id": 1029, "seek": 532732, "start": 5327.32, "end": 5332.599999999999, "text": " So those are the two configuration pieces that you may need to know about.", "tokens": [407, 729, 366, 264, 732, 11694, 3755, 300, 291, 815, 643, 281, 458, 466, 13], "temperature": 0.0, "avg_logprob": -0.20488072445518093, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.6701022104825824e-05}, {"id": 1030, "seek": 532732, "start": 5332.599999999999, "end": 5336.759999999999, "text": " For this class, you won't really need to know about those because everything's been set", "tokens": [1171, 341, 1508, 11, 291, 1582, 380, 534, 643, 281, 458, 466, 729, 570, 1203, 311, 668, 992], "temperature": 0.0, "avg_logprob": -0.20488072445518093, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.6701022104825824e-05}, {"id": 1031, "seek": 532732, "start": 5336.759999999999, "end": 5337.759999999999, "text": " up for you.", "tokens": [493, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.20488072445518093, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.6701022104825824e-05}, {"id": 1032, "seek": 532732, "start": 5337.759999999999, "end": 5344.36, "text": " I like to show you what's going on behind the scenes.", "tokens": [286, 411, 281, 855, 291, 437, 311, 516, 322, 2261, 264, 8026, 13], "temperature": 0.0, "avg_logprob": -0.20488072445518093, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.6701022104825824e-05}, {"id": 1033, "seek": 532732, "start": 5344.36, "end": 5352.88, "text": " So this warning that cuDNN is too recent, if you see any problems, try updating Theano", "tokens": [407, 341, 9164, 300, 2702, 35, 45, 45, 307, 886, 5162, 11, 498, 291, 536, 604, 2740, 11, 853, 25113, 440, 3730], "temperature": 0.0, "avg_logprob": -0.20488072445518093, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.6701022104825824e-05}, {"id": 1034, "seek": 532732, "start": 5352.88, "end": 5353.88, "text": " or downgrading cuDNN.", "tokens": [420, 760, 7165, 278, 2702, 35, 45, 45, 13], "temperature": 0.0, "avg_logprob": -0.20488072445518093, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.6701022104825824e-05}, {"id": 1035, "seek": 535388, "start": 5353.88, "end": 5358.0, "text": " I haven't found any problems, so you can ignore that warning.", "tokens": [286, 2378, 380, 1352, 604, 2740, 11, 370, 291, 393, 11200, 300, 9164, 13], "temperature": 0.0, "avg_logprob": -0.14128073586357964, "compression_ratio": 1.4532019704433496, "no_speech_prob": 2.6688134312280454e-05}, {"id": 1036, "seek": 535388, "start": 5358.0, "end": 5362.68, "text": " It just means that we're using a more up-to-date version of cuDNN than Theano authors have", "tokens": [467, 445, 1355, 300, 321, 434, 1228, 257, 544, 493, 12, 1353, 12, 17393, 3037, 295, 2702, 35, 45, 45, 813, 440, 3730, 16552, 362], "temperature": 0.0, "avg_logprob": -0.14128073586357964, "compression_ratio": 1.4532019704433496, "no_speech_prob": 2.6688134312280454e-05}, {"id": 1037, "seek": 535388, "start": 5362.68, "end": 5367.4800000000005, "text": " tested.", "tokens": [8246, 13], "temperature": 0.0, "avg_logprob": -0.14128073586357964, "compression_ratio": 1.4532019704433496, "no_speech_prob": 2.6688134312280454e-05}, {"id": 1038, "seek": 535388, "start": 5367.4800000000005, "end": 5374.16, "text": " So we create our VGG object.", "tokens": [407, 321, 1884, 527, 691, 27561, 2657, 13], "temperature": 0.0, "avg_logprob": -0.14128073586357964, "compression_ratio": 1.4532019704433496, "no_speech_prob": 2.6688134312280454e-05}, {"id": 1039, "seek": 535388, "start": 5374.16, "end": 5377.8, "text": " In doing so, there's a whole bunch of stuff going on behind the scenes.", "tokens": [682, 884, 370, 11, 456, 311, 257, 1379, 3840, 295, 1507, 516, 322, 2261, 264, 8026, 13], "temperature": 0.0, "avg_logprob": -0.14128073586357964, "compression_ratio": 1.4532019704433496, "no_speech_prob": 2.6688134312280454e-05}, {"id": 1040, "seek": 535388, "start": 5377.8, "end": 5379.400000000001, "text": " We're going to look at all of it.", "tokens": [492, 434, 516, 281, 574, 412, 439, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.14128073586357964, "compression_ratio": 1.4532019704433496, "no_speech_prob": 2.6688134312280454e-05}, {"id": 1041, "seek": 537940, "start": 5379.4, "end": 5389.679999999999, "text": " So by the end of the next lesson, you'll understand every line of code in our VGG script.", "tokens": [407, 538, 264, 917, 295, 264, 958, 6898, 11, 291, 603, 1223, 633, 1622, 295, 3089, 294, 527, 691, 27561, 5755, 13], "temperature": 0.0, "avg_logprob": -0.18509112233700958, "compression_ratio": 1.605, "no_speech_prob": 3.120104520348832e-05}, {"id": 1042, "seek": 537940, "start": 5389.679999999999, "end": 5396.36, "text": " For now, I'll just point out that you can look at it because you downloaded it, and", "tokens": [1171, 586, 11, 286, 603, 445, 935, 484, 300, 291, 393, 574, 412, 309, 570, 291, 21748, 309, 11, 293], "temperature": 0.0, "avg_logprob": -0.18509112233700958, "compression_ratio": 1.605, "no_speech_prob": 3.120104520348832e-05}, {"id": 1043, "seek": 537940, "start": 5396.36, "end": 5401.44, "text": " inside it you'll see there are 100 lines of code.", "tokens": [1854, 309, 291, 603, 536, 456, 366, 2319, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.18509112233700958, "compression_ratio": 1.605, "no_speech_prob": 3.120104520348832e-05}, {"id": 1044, "seek": 537940, "start": 5401.44, "end": 5403.92, "text": " So it's not very big at all.", "tokens": [407, 309, 311, 406, 588, 955, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.18509112233700958, "compression_ratio": 1.605, "no_speech_prob": 3.120104520348832e-05}, {"id": 1045, "seek": 537940, "start": 5403.92, "end": 5408.5599999999995, "text": " So we're going to understand all of it by the end of the next class.", "tokens": [407, 321, 434, 516, 281, 1223, 439, 295, 309, 538, 264, 917, 295, 264, 958, 1508, 13], "temperature": 0.0, "avg_logprob": -0.18509112233700958, "compression_ratio": 1.605, "no_speech_prob": 3.120104520348832e-05}, {"id": 1046, "seek": 540856, "start": 5408.56, "end": 5410.6, "text": " So now, let's treat it as a black box.", "tokens": [407, 586, 11, 718, 311, 2387, 309, 382, 257, 2211, 2424, 13], "temperature": 0.0, "avg_logprob": -0.16036568107185784, "compression_ratio": 1.5561497326203209, "no_speech_prob": 1.3006899280298967e-05}, {"id": 1047, "seek": 540856, "start": 5410.6, "end": 5413.92, "text": " There's a pre-trained network, it's called VGG16.", "tokens": [821, 311, 257, 659, 12, 17227, 2001, 3209, 11, 309, 311, 1219, 691, 27561, 6866, 13], "temperature": 0.0, "avg_logprob": -0.16036568107185784, "compression_ratio": 1.5561497326203209, "no_speech_prob": 1.3006899280298967e-05}, {"id": 1048, "seek": 540856, "start": 5413.92, "end": 5427.080000000001, "text": " We now have a VGG object which gives us access to that pre-trained network.", "tokens": [492, 586, 362, 257, 691, 27561, 2657, 597, 2709, 505, 2105, 281, 300, 659, 12, 17227, 2001, 3209, 13], "temperature": 0.0, "avg_logprob": -0.16036568107185784, "compression_ratio": 1.5561497326203209, "no_speech_prob": 1.3006899280298967e-05}, {"id": 1049, "seek": 540856, "start": 5427.080000000001, "end": 5433.84, "text": " With deep learning, we don't look at images or data items one at a time.", "tokens": [2022, 2452, 2539, 11, 321, 500, 380, 574, 412, 5267, 420, 1412, 4754, 472, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.16036568107185784, "compression_ratio": 1.5561497326203209, "no_speech_prob": 1.3006899280298967e-05}, {"id": 1050, "seek": 540856, "start": 5433.84, "end": 5437.200000000001, "text": " We also don't look at them a whole dataset at a time.", "tokens": [492, 611, 500, 380, 574, 412, 552, 257, 1379, 28872, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.16036568107185784, "compression_ratio": 1.5561497326203209, "no_speech_prob": 1.3006899280298967e-05}, {"id": 1051, "seek": 543720, "start": 5437.2, "end": 5439.76, "text": " We look at them a few at a time.", "tokens": [492, 574, 412, 552, 257, 1326, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.1733393473167942, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.666015102993697e-06}, {"id": 1052, "seek": 543720, "start": 5439.76, "end": 5444.0, "text": " The number that we look at, or that little few that we look at at a time, we call either", "tokens": [440, 1230, 300, 321, 574, 412, 11, 420, 300, 707, 1326, 300, 321, 574, 412, 412, 257, 565, 11, 321, 818, 2139], "temperature": 0.0, "avg_logprob": -0.1733393473167942, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.666015102993697e-06}, {"id": 1053, "seek": 543720, "start": 5444.0, "end": 5460.08, "text": " a batch or a mini-batch.", "tokens": [257, 15245, 420, 257, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.1733393473167942, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.666015102993697e-06}, {"id": 1054, "seek": 543720, "start": 5460.08, "end": 5466.88, "text": " A mini-batch is simply grabbing, in this case, images, a few images at a time.", "tokens": [316, 8382, 12, 65, 852, 307, 2935, 23771, 11, 294, 341, 1389, 11, 5267, 11, 257, 1326, 5267, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.1733393473167942, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.666015102993697e-06}, {"id": 1055, "seek": 546688, "start": 5466.88, "end": 5472.28, "text": " And the size of that is the size of the mini-batch, or the size of the batch, and computing on", "tokens": [400, 264, 2744, 295, 300, 307, 264, 2744, 295, 264, 8382, 12, 65, 852, 11, 420, 264, 2744, 295, 264, 15245, 11, 293, 15866, 322], "temperature": 0.0, "avg_logprob": -0.1253749587319114, "compression_ratio": 1.8844221105527639, "no_speech_prob": 4.785071268997854e-06}, {"id": 1056, "seek": 546688, "start": 5472.28, "end": 5475.04, "text": " all of those at once.", "tokens": [439, 295, 729, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.1253749587319114, "compression_ratio": 1.8844221105527639, "no_speech_prob": 4.785071268997854e-06}, {"id": 1057, "seek": 546688, "start": 5475.04, "end": 5480.16, "text": " Why don't we do one at a time?", "tokens": [1545, 500, 380, 321, 360, 472, 412, 257, 565, 30], "temperature": 0.0, "avg_logprob": -0.1253749587319114, "compression_ratio": 1.8844221105527639, "no_speech_prob": 4.785071268997854e-06}, {"id": 1058, "seek": 546688, "start": 5480.16, "end": 5487.04, "text": " The reason that we don't do one at a time is because a GPU needs to do lots of things", "tokens": [440, 1778, 300, 321, 500, 380, 360, 472, 412, 257, 565, 307, 570, 257, 18407, 2203, 281, 360, 3195, 295, 721], "temperature": 0.0, "avg_logprob": -0.1253749587319114, "compression_ratio": 1.8844221105527639, "no_speech_prob": 4.785071268997854e-06}, {"id": 1059, "seek": 546688, "start": 5487.04, "end": 5488.64, "text": " at once to be useful.", "tokens": [412, 1564, 281, 312, 4420, 13], "temperature": 0.0, "avg_logprob": -0.1253749587319114, "compression_ratio": 1.8844221105527639, "no_speech_prob": 4.785071268997854e-06}, {"id": 1060, "seek": 546688, "start": 5488.64, "end": 5492.4800000000005, "text": " It loves running on thousands and thousands of things at the same time, because it can", "tokens": [467, 6752, 2614, 322, 5383, 293, 5383, 295, 721, 412, 264, 912, 565, 11, 570, 309, 393], "temperature": 0.0, "avg_logprob": -0.1253749587319114, "compression_ratio": 1.8844221105527639, "no_speech_prob": 4.785071268997854e-06}, {"id": 1061, "seek": 546688, "start": 5492.4800000000005, "end": 5494.04, "text": " do all of them at the same time.", "tokens": [360, 439, 295, 552, 412, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.1253749587319114, "compression_ratio": 1.8844221105527639, "no_speech_prob": 4.785071268997854e-06}, {"id": 1062, "seek": 549404, "start": 5494.04, "end": 5499.2, "text": " So a single image is not enough to keep your GPU busy and it's slow.", "tokens": [407, 257, 2167, 3256, 307, 406, 1547, 281, 1066, 428, 18407, 5856, 293, 309, 311, 2964, 13], "temperature": 0.0, "avg_logprob": -0.13115867142824783, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.497096036473522e-05}, {"id": 1063, "seek": 549404, "start": 5499.2, "end": 5503.12, "text": " Why not do all of it, the whole dataset at once?", "tokens": [1545, 406, 360, 439, 295, 309, 11, 264, 1379, 28872, 412, 1564, 30], "temperature": 0.0, "avg_logprob": -0.13115867142824783, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.497096036473522e-05}, {"id": 1064, "seek": 549404, "start": 5503.12, "end": 5507.88, "text": " First of all, your GPU only has a certain amount of memory, generally somewhere between", "tokens": [2386, 295, 439, 11, 428, 18407, 787, 575, 257, 1629, 2372, 295, 4675, 11, 5101, 4079, 1296], "temperature": 0.0, "avg_logprob": -0.13115867142824783, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.497096036473522e-05}, {"id": 1065, "seek": 549404, "start": 5507.88, "end": 5511.88, "text": " about 2GB and about 12GB.", "tokens": [466, 568, 8769, 293, 466, 2272, 8769, 13], "temperature": 0.0, "avg_logprob": -0.13115867142824783, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.497096036473522e-05}, {"id": 1066, "seek": 549404, "start": 5511.88, "end": 5516.16, "text": " And generally speaking, your dataset is unlikely to fit in that memory.", "tokens": [400, 5101, 4124, 11, 428, 28872, 307, 17518, 281, 3318, 294, 300, 4675, 13], "temperature": 0.0, "avg_logprob": -0.13115867142824783, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.497096036473522e-05}, {"id": 1067, "seek": 549404, "start": 5516.16, "end": 5520.5199999999995, "text": " And secondly, there's just no need to do the whole lot.", "tokens": [400, 26246, 11, 456, 311, 445, 572, 643, 281, 360, 264, 1379, 688, 13], "temperature": 0.0, "avg_logprob": -0.13115867142824783, "compression_ratio": 1.5608695652173914, "no_speech_prob": 1.497096036473522e-05}, {"id": 1068, "seek": 552052, "start": 5520.52, "end": 5524.96, "text": " Pretty much everything we want to do, we can do a small amount at a time.", "tokens": [10693, 709, 1203, 321, 528, 281, 360, 11, 321, 393, 360, 257, 1359, 2372, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.19623992022346048, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.7108610488066915e-06}, {"id": 1069, "seek": 552052, "start": 5524.96, "end": 5530.88, "text": " So in this case, I'm just going to show you how we can look at the result of this BGG", "tokens": [407, 294, 341, 1389, 11, 286, 478, 445, 516, 281, 855, 291, 577, 321, 393, 574, 412, 264, 1874, 295, 341, 363, 27561], "temperature": 0.0, "avg_logprob": -0.19623992022346048, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.7108610488066915e-06}, {"id": 1070, "seek": 552052, "start": 5530.88, "end": 5531.88, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.19623992022346048, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.7108610488066915e-06}, {"id": 1071, "seek": 552052, "start": 5531.88, "end": 5533.4800000000005, "text": " So we're just going to do 4 at a time.", "tokens": [407, 321, 434, 445, 516, 281, 360, 1017, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.19623992022346048, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.7108610488066915e-06}, {"id": 1072, "seek": 552052, "start": 5533.4800000000005, "end": 5540.200000000001, "text": " So there's a get-batches command which basically says, in our BGG model, let's look inside", "tokens": [407, 456, 311, 257, 483, 12, 65, 852, 279, 5622, 597, 1936, 1619, 11, 294, 527, 363, 27561, 2316, 11, 718, 311, 574, 1854], "temperature": 0.0, "avg_logprob": -0.19623992022346048, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.7108610488066915e-06}, {"id": 1073, "seek": 552052, "start": 5540.200000000001, "end": 5545.68, "text": " the path that we've defined, and remember that path we made the sample path, and grab", "tokens": [264, 3100, 300, 321, 600, 7642, 11, 293, 1604, 300, 3100, 321, 1027, 264, 6889, 3100, 11, 293, 4444], "temperature": 0.0, "avg_logprob": -0.19623992022346048, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.7108610488066915e-06}, {"id": 1074, "seek": 552052, "start": 5545.68, "end": 5547.96, "text": " 4 at a time.", "tokens": [1017, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.19623992022346048, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.7108610488066915e-06}, {"id": 1075, "seek": 554796, "start": 5547.96, "end": 5551.08, "text": " So we've got an error, that's good.", "tokens": [407, 321, 600, 658, 364, 6713, 11, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.2764577582330987, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.3631155525217764e-05}, {"id": 1076, "seek": 554796, "start": 5551.08, "end": 5552.08, "text": " No such directory.", "tokens": [883, 1270, 21120, 13], "temperature": 0.0, "avg_logprob": -0.2764577582330987, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.3631155525217764e-05}, {"id": 1077, "seek": 554796, "start": 5552.08, "end": 5556.88, "text": " Missing a trailing slash.", "tokens": [5275, 278, 257, 944, 4883, 17330, 13], "temperature": 0.0, "avg_logprob": -0.2764577582330987, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.3631155525217764e-05}, {"id": 1078, "seek": 554796, "start": 5556.88, "end": 5558.84, "text": " So it's good to see these kind of errors.", "tokens": [407, 309, 311, 665, 281, 536, 613, 733, 295, 13603, 13], "temperature": 0.0, "avg_logprob": -0.2764577582330987, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.3631155525217764e-05}, {"id": 1079, "seek": 554796, "start": 5558.84, "end": 5568.36, "text": " So we'll just go back to where we defined it, add our trailing slash, and go back down,", "tokens": [407, 321, 603, 445, 352, 646, 281, 689, 321, 7642, 309, 11, 909, 527, 944, 4883, 17330, 11, 293, 352, 646, 760, 11], "temperature": 0.0, "avg_logprob": -0.2764577582330987, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.3631155525217764e-05}, {"id": 1080, "seek": 554796, "start": 5568.36, "end": 5569.36, "text": " and run it again.", "tokens": [293, 1190, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.2764577582330987, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.3631155525217764e-05}, {"id": 1081, "seek": 554796, "start": 5569.36, "end": 5573.4800000000005, "text": " Okay, so we're in the sample, so there's 16 images.", "tokens": [1033, 11, 370, 321, 434, 294, 264, 6889, 11, 370, 456, 311, 3165, 5267, 13], "temperature": 0.0, "avg_logprob": -0.2764577582330987, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.3631155525217764e-05}, {"id": 1082, "seek": 554796, "start": 5573.4800000000005, "end": 5575.0, "text": " So let's grab one batch.", "tokens": [407, 718, 311, 4444, 472, 15245, 13], "temperature": 0.0, "avg_logprob": -0.2764577582330987, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.3631155525217764e-05}, {"id": 1083, "seek": 557500, "start": 5575.0, "end": 5579.28, "text": " And so that's going to grab 4 images and 4 labels.", "tokens": [400, 370, 300, 311, 516, 281, 4444, 1017, 5267, 293, 1017, 16949, 13], "temperature": 0.0, "avg_logprob": -0.2209316702450023, "compression_ratio": 1.544, "no_speech_prob": 7.411237675114535e-06}, {"id": 1084, "seek": 557500, "start": 5579.28, "end": 5587.12, "text": " So here are the 4 images, and here are the 4 labels.", "tokens": [407, 510, 366, 264, 1017, 5267, 11, 293, 510, 366, 264, 1017, 16949, 13], "temperature": 0.0, "avg_logprob": -0.2209316702450023, "compression_ratio": 1.544, "no_speech_prob": 7.411237675114535e-06}, {"id": 1085, "seek": 557500, "start": 5587.12, "end": 5598.68, "text": " So you can see that it's labeling at 0, 1 if it's a dog, and it'll be 1, 0 if it's a", "tokens": [407, 291, 393, 536, 300, 309, 311, 40244, 412, 1958, 11, 502, 498, 309, 311, 257, 3000, 11, 293, 309, 603, 312, 502, 11, 1958, 498, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.2209316702450023, "compression_ratio": 1.544, "no_speech_prob": 7.411237675114535e-06}, {"id": 1086, "seek": 557500, "start": 5598.68, "end": 5603.68, "text": " cat.", "tokens": [3857, 13], "temperature": 0.0, "avg_logprob": -0.2209316702450023, "compression_ratio": 1.544, "no_speech_prob": 7.411237675114535e-06}, {"id": 1087, "seek": 560368, "start": 5603.68, "end": 5612.240000000001, "text": " So now that we've done that, so that's basically what our data looks like, we can call bgg.predict,", "tokens": [407, 586, 300, 321, 600, 1096, 300, 11, 370, 300, 311, 1936, 437, 527, 1412, 1542, 411, 11, 321, 393, 818, 272, 1615, 13, 79, 24945, 11], "temperature": 0.0, "avg_logprob": -0.21932908965320122, "compression_ratio": 1.5921787709497206, "no_speech_prob": 5.338123628462199e-06}, {"id": 1088, "seek": 560368, "start": 5612.240000000001, "end": 5614.12, "text": " passing in the images.", "tokens": [8437, 294, 264, 5267, 13], "temperature": 0.0, "avg_logprob": -0.21932908965320122, "compression_ratio": 1.5921787709497206, "no_speech_prob": 5.338123628462199e-06}, {"id": 1089, "seek": 560368, "start": 5614.12, "end": 5617.400000000001, "text": " And so that's now going to ignore the labels of what it actually is, and it's going to", "tokens": [400, 370, 300, 311, 586, 516, 281, 11200, 264, 16949, 295, 437, 309, 767, 307, 11, 293, 309, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.21932908965320122, "compression_ratio": 1.5921787709497206, "no_speech_prob": 5.338123628462199e-06}, {"id": 1090, "seek": 560368, "start": 5617.400000000001, "end": 5622.16, "text": " use this pre-trained model and tell us what does it think the 4 things are.", "tokens": [764, 341, 659, 12, 17227, 2001, 2316, 293, 980, 505, 437, 775, 309, 519, 264, 1017, 721, 366, 13], "temperature": 0.0, "avg_logprob": -0.21932908965320122, "compression_ratio": 1.5921787709497206, "no_speech_prob": 5.338123628462199e-06}, {"id": 1091, "seek": 562216, "start": 5622.16, "end": 5634.4, "text": " In this case, it thinks they're a Rottweiler, an Egyptian cat, a toy terrier, and a Rottweiler.", "tokens": [682, 341, 1389, 11, 309, 7309, 436, 434, 257, 497, 1521, 826, 5441, 11, 364, 24257, 3857, 11, 257, 12058, 1796, 7326, 11, 293, 257, 497, 1521, 826, 5441, 13], "temperature": 0.0, "avg_logprob": -0.1847799135291058, "compression_ratio": 1.56, "no_speech_prob": 5.594289177679457e-06}, {"id": 1092, "seek": 562216, "start": 5634.4, "end": 5638.36, "text": " So you can see it's clearly made a mistake here.", "tokens": [407, 291, 393, 536, 309, 311, 4448, 1027, 257, 6146, 510, 13], "temperature": 0.0, "avg_logprob": -0.1847799135291058, "compression_ratio": 1.56, "no_speech_prob": 5.594289177679457e-06}, {"id": 1093, "seek": 562216, "start": 5638.36, "end": 5641.16, "text": " It's very rare that I find it makes a mistake, so I'm glad it did make one.", "tokens": [467, 311, 588, 5892, 300, 286, 915, 309, 1669, 257, 6146, 11, 370, 286, 478, 5404, 309, 630, 652, 472, 13], "temperature": 0.0, "avg_logprob": -0.1847799135291058, "compression_ratio": 1.56, "no_speech_prob": 5.594289177679457e-06}, {"id": 1094, "seek": 562216, "start": 5641.16, "end": 5647.72, "text": " You can see it must have been confused by all the awkward stuff going on in the background.", "tokens": [509, 393, 536, 309, 1633, 362, 668, 9019, 538, 439, 264, 11411, 1507, 516, 322, 294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.1847799135291058, "compression_ratio": 1.56, "no_speech_prob": 5.594289177679457e-06}, {"id": 1095, "seek": 564772, "start": 5647.72, "end": 5652.52, "text": " So it's also shown you for this one that's a toy terrier, here are the probabilities", "tokens": [407, 309, 311, 611, 4898, 291, 337, 341, 472, 300, 311, 257, 12058, 1796, 7326, 11, 510, 366, 264, 33783], "temperature": 0.0, "avg_logprob": -0.16875688846294695, "compression_ratio": 1.8506944444444444, "no_speech_prob": 4.092847120773513e-06}, {"id": 1096, "seek": 564772, "start": 5652.52, "end": 5657.0, "text": " of each, it's only 24% sure that it's a toy terrier.", "tokens": [295, 1184, 11, 309, 311, 787, 4022, 4, 988, 300, 309, 311, 257, 12058, 1796, 7326, 13], "temperature": 0.0, "avg_logprob": -0.16875688846294695, "compression_ratio": 1.8506944444444444, "no_speech_prob": 4.092847120773513e-06}, {"id": 1097, "seek": 564772, "start": 5657.0, "end": 5660.320000000001, "text": " So you can see that it does actually know that it's not sure.", "tokens": [407, 291, 393, 536, 300, 309, 775, 767, 458, 300, 309, 311, 406, 988, 13], "temperature": 0.0, "avg_logprob": -0.16875688846294695, "compression_ratio": 1.8506944444444444, "no_speech_prob": 4.092847120773513e-06}, {"id": 1098, "seek": 564772, "start": 5660.320000000001, "end": 5664.360000000001, "text": " Whereas the Rottweiler, it's very sure it's a Rottweiler.", "tokens": [13813, 264, 497, 1521, 826, 5441, 11, 309, 311, 588, 988, 309, 311, 257, 497, 1521, 826, 5441, 13], "temperature": 0.0, "avg_logprob": -0.16875688846294695, "compression_ratio": 1.8506944444444444, "no_speech_prob": 4.092847120773513e-06}, {"id": 1099, "seek": 564772, "start": 5664.360000000001, "end": 5666.4400000000005, "text": " How come it's not so sure it's an Egyptian cat?", "tokens": [1012, 808, 309, 311, 406, 370, 988, 309, 311, 364, 24257, 3857, 30], "temperature": 0.0, "avg_logprob": -0.16875688846294695, "compression_ratio": 1.8506944444444444, "no_speech_prob": 4.092847120773513e-06}, {"id": 1100, "seek": 564772, "start": 5666.4400000000005, "end": 5669.8, "text": " Well, that's because there's lots of cats that look a little bit like an Egyptian cat,", "tokens": [1042, 11, 300, 311, 570, 456, 311, 3195, 295, 11111, 300, 574, 257, 707, 857, 411, 364, 24257, 3857, 11], "temperature": 0.0, "avg_logprob": -0.16875688846294695, "compression_ratio": 1.8506944444444444, "no_speech_prob": 4.092847120773513e-06}, {"id": 1101, "seek": 564772, "start": 5669.8, "end": 5672.12, "text": " it doesn't quite know which one it is.", "tokens": [309, 1177, 380, 1596, 458, 597, 472, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.16875688846294695, "compression_ratio": 1.8506944444444444, "no_speech_prob": 4.092847120773513e-06}, {"id": 1102, "seek": 564772, "start": 5672.12, "end": 5676.08, "text": " So we could have a look at all those details and see exactly which other ones it thought", "tokens": [407, 321, 727, 362, 257, 574, 412, 439, 729, 4365, 293, 536, 2293, 597, 661, 2306, 309, 1194], "temperature": 0.0, "avg_logprob": -0.16875688846294695, "compression_ratio": 1.8506944444444444, "no_speech_prob": 4.092847120773513e-06}, {"id": 1103, "seek": 564772, "start": 5676.08, "end": 5677.08, "text": " it would be.", "tokens": [309, 576, 312, 13], "temperature": 0.0, "avg_logprob": -0.16875688846294695, "compression_ratio": 1.8506944444444444, "no_speech_prob": 4.092847120773513e-06}, {"id": 1104, "seek": 567708, "start": 5677.08, "end": 5681.6, "text": " We'll be looking at that in the next lesson.", "tokens": [492, 603, 312, 1237, 412, 300, 294, 264, 958, 6898, 13], "temperature": 0.0, "avg_logprob": -0.15644275574457078, "compression_ratio": 1.620879120879121, "no_speech_prob": 6.4389678300358355e-06}, {"id": 1105, "seek": 567708, "start": 5681.6, "end": 5691.16, "text": " So the final thing I'm going to do is to show you how we can take these probabilities and", "tokens": [407, 264, 2572, 551, 286, 478, 516, 281, 360, 307, 281, 855, 291, 577, 321, 393, 747, 613, 33783, 293], "temperature": 0.0, "avg_logprob": -0.15644275574457078, "compression_ratio": 1.620879120879121, "no_speech_prob": 6.4389678300358355e-06}, {"id": 1106, "seek": 567708, "start": 5691.16, "end": 5694.12, "text": " turn them into a dogs vs. cats model.", "tokens": [1261, 552, 666, 257, 7197, 12041, 13, 11111, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15644275574457078, "compression_ratio": 1.620879120879121, "no_speech_prob": 6.4389678300358355e-06}, {"id": 1107, "seek": 567708, "start": 5694.12, "end": 5699.0, "text": " I'm going to do it quickly now and then I'm going to revisit this in the start of the", "tokens": [286, 478, 516, 281, 360, 309, 2661, 586, 293, 550, 286, 478, 516, 281, 32676, 341, 294, 264, 722, 295, 264], "temperature": 0.0, "avg_logprob": -0.15644275574457078, "compression_ratio": 1.620879120879121, "no_speech_prob": 6.4389678300358355e-06}, {"id": 1108, "seek": 567708, "start": 5699.0, "end": 5701.96, "text": " next class, since we're out of time.", "tokens": [958, 1508, 11, 1670, 321, 434, 484, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.15644275574457078, "compression_ratio": 1.620879120879121, "no_speech_prob": 6.4389678300358355e-06}, {"id": 1109, "seek": 570196, "start": 5701.96, "end": 5708.64, "text": " So to take that 1000 probabilities, so we're just showing one probability from each, but", "tokens": [407, 281, 747, 300, 9714, 33783, 11, 370, 321, 434, 445, 4099, 472, 8482, 490, 1184, 11, 457], "temperature": 0.0, "avg_logprob": -0.25866006730913044, "compression_ratio": 1.7574468085106383, "no_speech_prob": 8.013363185455091e-06}, {"id": 1110, "seek": 570196, "start": 5708.64, "end": 5710.36, "text": " there's actually 1000 probabilities for each.", "tokens": [456, 311, 767, 9714, 33783, 337, 1184, 13], "temperature": 0.0, "avg_logprob": -0.25866006730913044, "compression_ratio": 1.7574468085106383, "no_speech_prob": 8.013363185455091e-06}, {"id": 1111, "seek": 570196, "start": 5710.36, "end": 5714.44, "text": " So this is the probability that it's a tench, it's a goldfish, it's a great white shark,", "tokens": [407, 341, 307, 264, 8482, 300, 309, 311, 257, 2064, 339, 11, 309, 311, 257, 3821, 11608, 11, 309, 311, 257, 869, 2418, 13327, 11], "temperature": 0.0, "avg_logprob": -0.25866006730913044, "compression_ratio": 1.7574468085106383, "no_speech_prob": 8.013363185455091e-06}, {"id": 1112, "seek": 570196, "start": 5714.44, "end": 5715.44, "text": " so on and so forth.", "tokens": [370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.25866006730913044, "compression_ratio": 1.7574468085106383, "no_speech_prob": 8.013363185455091e-06}, {"id": 1113, "seek": 570196, "start": 5715.44, "end": 5722.36, "text": " To take those 1000 probabilities and turn it into a dog vs. a cat prediction, we basically", "tokens": [1407, 747, 729, 9714, 33783, 293, 1261, 309, 666, 257, 3000, 12041, 13, 257, 3857, 17630, 11, 321, 1936], "temperature": 0.0, "avg_logprob": -0.25866006730913044, "compression_ratio": 1.7574468085106383, "no_speech_prob": 8.013363185455091e-06}, {"id": 1114, "seek": 570196, "start": 5722.36, "end": 5729.4, "text": " do exactly what we did before, just to say get batches that we call fine-tune.", "tokens": [360, 2293, 437, 321, 630, 949, 11, 445, 281, 584, 483, 15245, 279, 300, 321, 818, 2489, 12, 83, 2613, 13], "temperature": 0.0, "avg_logprob": -0.25866006730913044, "compression_ratio": 1.7574468085106383, "no_speech_prob": 8.013363185455091e-06}, {"id": 1115, "seek": 572940, "start": 5729.4, "end": 5734.12, "text": " Now what fine-tune is going to do is it's going to build a new model and it's going", "tokens": [823, 437, 2489, 12, 83, 2613, 307, 516, 281, 360, 307, 309, 311, 516, 281, 1322, 257, 777, 2316, 293, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.12560072152510934, "compression_ratio": 1.7309644670050761, "no_speech_prob": 1.4823510809947038e-06}, {"id": 1116, "seek": 572940, "start": 5734.12, "end": 5741.36, "text": " to replace the 1000 categories with the two classes that it's found.", "tokens": [281, 7406, 264, 9714, 10479, 365, 264, 732, 5359, 300, 309, 311, 1352, 13], "temperature": 0.0, "avg_logprob": -0.12560072152510934, "compression_ratio": 1.7309644670050761, "no_speech_prob": 1.4823510809947038e-06}, {"id": 1117, "seek": 572940, "start": 5741.36, "end": 5743.599999999999, "text": " And how does it know what the two classes are?", "tokens": [400, 577, 775, 309, 458, 437, 264, 732, 5359, 366, 30], "temperature": 0.0, "avg_logprob": -0.12560072152510934, "compression_ratio": 1.7309644670050761, "no_speech_prob": 1.4823510809947038e-06}, {"id": 1118, "seek": 572940, "start": 5743.599999999999, "end": 5752.92, "text": " Well that's because we have directories called cats and dogs.", "tokens": [1042, 300, 311, 570, 321, 362, 5391, 530, 1219, 11111, 293, 7197, 13], "temperature": 0.0, "avg_logprob": -0.12560072152510934, "compression_ratio": 1.7309644670050761, "no_speech_prob": 1.4823510809947038e-06}, {"id": 1119, "seek": 572940, "start": 5752.92, "end": 5758.879999999999, "text": " So the fine-tune command has now created a model that checks for cats and dogs.", "tokens": [407, 264, 2489, 12, 83, 2613, 5622, 575, 586, 2942, 257, 2316, 300, 13834, 337, 11111, 293, 7197, 13], "temperature": 0.0, "avg_logprob": -0.12560072152510934, "compression_ratio": 1.7309644670050761, "no_speech_prob": 1.4823510809947038e-06}, {"id": 1120, "seek": 575888, "start": 5758.88, "end": 5763.88, "text": " So creating the model is not enough, we have to actually train it.", "tokens": [407, 4084, 264, 2316, 307, 406, 1547, 11, 321, 362, 281, 767, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.1530273668058626, "compression_ratio": 1.5530973451327434, "no_speech_prob": 5.507553396455478e-06}, {"id": 1121, "seek": 575888, "start": 5763.88, "end": 5771.88, "text": " So if we then go.fit, it will then use that gradient descent method that I talked about", "tokens": [407, 498, 321, 550, 352, 2411, 6845, 11, 309, 486, 550, 764, 300, 16235, 23475, 3170, 300, 286, 2825, 466], "temperature": 0.0, "avg_logprob": -0.1530273668058626, "compression_ratio": 1.5530973451327434, "no_speech_prob": 5.507553396455478e-06}, {"id": 1122, "seek": 575888, "start": 5771.88, "end": 5777.84, "text": " earlier, that propagation, and it will attempt to make that model better and better at determining", "tokens": [3071, 11, 300, 38377, 11, 293, 309, 486, 5217, 281, 652, 300, 2316, 1101, 293, 1101, 412, 23751], "temperature": 0.0, "avg_logprob": -0.1530273668058626, "compression_ratio": 1.5530973451327434, "no_speech_prob": 5.507553396455478e-06}, {"id": 1123, "seek": 575888, "start": 5777.84, "end": 5779.56, "text": " cats vs. dogs.", "tokens": [11111, 12041, 13, 7197, 13], "temperature": 0.0, "avg_logprob": -0.1530273668058626, "compression_ratio": 1.5530973451327434, "no_speech_prob": 5.507553396455478e-06}, {"id": 1124, "seek": 575888, "start": 5779.56, "end": 5785.7, "text": " Now obviously doing it on just 16 data items is A, fast, but B, not very accurate.", "tokens": [823, 2745, 884, 309, 322, 445, 3165, 1412, 4754, 307, 316, 11, 2370, 11, 457, 363, 11, 406, 588, 8559, 13], "temperature": 0.0, "avg_logprob": -0.1530273668058626, "compression_ratio": 1.5530973451327434, "no_speech_prob": 5.507553396455478e-06}, {"id": 1125, "seek": 578570, "start": 5785.7, "end": 5790.88, "text": " So I can run it a few times and you can see that the accuracy is actually getting higher", "tokens": [407, 286, 393, 1190, 309, 257, 1326, 1413, 293, 291, 393, 536, 300, 264, 14170, 307, 767, 1242, 2946], "temperature": 0.0, "avg_logprob": -0.135612020889918, "compression_ratio": 1.6788990825688073, "no_speech_prob": 2.6425616397318663e-06}, {"id": 1126, "seek": 578570, "start": 5790.88, "end": 5796.24, "text": " and higher each time, but the validation accuracy is not getting much higher.", "tokens": [293, 2946, 1184, 565, 11, 457, 264, 24071, 14170, 307, 406, 1242, 709, 2946, 13], "temperature": 0.0, "avg_logprob": -0.135612020889918, "compression_ratio": 1.6788990825688073, "no_speech_prob": 2.6425616397318663e-06}, {"id": 1127, "seek": 578570, "start": 5796.24, "end": 5798.36, "text": " And that's because I'm running it on the sample.", "tokens": [400, 300, 311, 570, 286, 478, 2614, 309, 322, 264, 6889, 13], "temperature": 0.0, "avg_logprob": -0.135612020889918, "compression_ratio": 1.6788990825688073, "no_speech_prob": 2.6425616397318663e-06}, {"id": 1128, "seek": 578570, "start": 5798.36, "end": 5805.36, "text": " So if I ran it on the full dataset, it would take about 5 minutes to run and you try it", "tokens": [407, 498, 286, 5872, 309, 322, 264, 1577, 28872, 11, 309, 576, 747, 466, 1025, 2077, 281, 1190, 293, 291, 853, 309], "temperature": 0.0, "avg_logprob": -0.135612020889918, "compression_ratio": 1.6788990825688073, "no_speech_prob": 2.6425616397318663e-06}, {"id": 1129, "seek": 578570, "start": 5805.36, "end": 5808.96, "text": " when you get home, give it a go and see what accuracy you get.", "tokens": [562, 291, 483, 1280, 11, 976, 309, 257, 352, 293, 536, 437, 14170, 291, 483, 13], "temperature": 0.0, "avg_logprob": -0.135612020889918, "compression_ratio": 1.6788990825688073, "no_speech_prob": 2.6425616397318663e-06}, {"id": 1130, "seek": 580896, "start": 5808.96, "end": 5816.36, "text": " If you want to make the accuracy higher, just rerun this cell a bunch of times.", "tokens": [759, 291, 528, 281, 652, 264, 14170, 2946, 11, 445, 43819, 409, 341, 2815, 257, 3840, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.13791672224850998, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.811444220351405e-07}, {"id": 1131, "seek": 580896, "start": 5816.36, "end": 5820.84, "text": " So that's the end of today's class.", "tokens": [407, 300, 311, 264, 917, 295, 965, 311, 1508, 13], "temperature": 0.0, "avg_logprob": -0.13791672224850998, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.811444220351405e-07}, {"id": 1132, "seek": 580896, "start": 5820.84, "end": 5825.4, "text": " It's the class which is kind of like the opening of a novel, when you have to introduce all", "tokens": [467, 311, 264, 1508, 597, 307, 733, 295, 411, 264, 5193, 295, 257, 7613, 11, 562, 291, 362, 281, 5366, 439], "temperature": 0.0, "avg_logprob": -0.13791672224850998, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.811444220351405e-07}, {"id": 1133, "seek": 580896, "start": 5825.4, "end": 5827.96, "text": " of the characters and their backstories and stuff.", "tokens": [295, 264, 4342, 293, 641, 646, 372, 2083, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.13791672224850998, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.811444220351405e-07}, {"id": 1134, "seek": 580896, "start": 5827.96, "end": 5832.52, "text": " So a little bit less deep learning goes on in the first class, a little bit more getting", "tokens": [407, 257, 707, 857, 1570, 2452, 2539, 1709, 322, 294, 264, 700, 1508, 11, 257, 707, 857, 544, 1242], "temperature": 0.0, "avg_logprob": -0.13791672224850998, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.811444220351405e-07}, {"id": 1135, "seek": 580896, "start": 5832.52, "end": 5833.84, "text": " set up.", "tokens": [992, 493, 13], "temperature": 0.0, "avg_logprob": -0.13791672224850998, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.811444220351405e-07}, {"id": 1136, "seek": 583384, "start": 5833.84, "end": 5839.92, "text": " Your first week is likely to be for many of you the most frustrating and challenging week", "tokens": [2260, 700, 1243, 307, 3700, 281, 312, 337, 867, 295, 291, 264, 881, 16522, 293, 7595, 1243], "temperature": 0.0, "avg_logprob": -0.16387174810682023, "compression_ratio": 1.721189591078067, "no_speech_prob": 6.854186722193845e-06}, {"id": 1137, "seek": 583384, "start": 5839.92, "end": 5844.64, "text": " because many of you will find you've got some kind of configuration problem or you don't", "tokens": [570, 867, 295, 291, 486, 915, 291, 600, 658, 512, 733, 295, 11694, 1154, 420, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.16387174810682023, "compression_ratio": 1.721189591078067, "no_speech_prob": 6.854186722193845e-06}, {"id": 1138, "seek": 583384, "start": 5844.64, "end": 5848.28, "text": " understand how some piece of the stuff fits together.", "tokens": [1223, 577, 512, 2522, 295, 264, 1507, 9001, 1214, 13], "temperature": 0.0, "avg_logprob": -0.16387174810682023, "compression_ratio": 1.721189591078067, "no_speech_prob": 6.854186722193845e-06}, {"id": 1139, "seek": 583384, "start": 5848.28, "end": 5854.88, "text": " Don't worry, by the end of the 7 weeks, that stuff's all going to be straightforward and", "tokens": [1468, 380, 3292, 11, 538, 264, 917, 295, 264, 1614, 3259, 11, 300, 1507, 311, 439, 516, 281, 312, 15325, 293], "temperature": 0.0, "avg_logprob": -0.16387174810682023, "compression_ratio": 1.721189591078067, "no_speech_prob": 6.854186722193845e-06}, {"id": 1140, "seek": 583384, "start": 5854.88, "end": 5857.0, "text": " all of the interesting bit will be in the deep learning.", "tokens": [439, 295, 264, 1880, 857, 486, 312, 294, 264, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.16387174810682023, "compression_ratio": 1.721189591078067, "no_speech_prob": 6.854186722193845e-06}, {"id": 1141, "seek": 583384, "start": 5857.0, "end": 5862.08, "text": " So I think the more time you can put into this week, making sure you get all of that", "tokens": [407, 286, 519, 264, 544, 565, 291, 393, 829, 666, 341, 1243, 11, 1455, 988, 291, 483, 439, 295, 300], "temperature": 0.0, "avg_logprob": -0.16387174810682023, "compression_ratio": 1.721189591078067, "no_speech_prob": 6.854186722193845e-06}, {"id": 1142, "seek": 586208, "start": 5862.08, "end": 5866.6, "text": " infrastructure stuff working and comfortable with what it is, take one of the things I've", "tokens": [6896, 1507, 1364, 293, 4619, 365, 437, 309, 307, 11, 747, 472, 295, 264, 721, 286, 600], "temperature": 0.0, "avg_logprob": -0.1569923842248838, "compression_ratio": 1.7157894736842105, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1143, "seek": 586208, "start": 5866.6, "end": 5870.76, "text": " introduced today, look at the video and go and Google all the stuff that you're not already", "tokens": [7268, 965, 11, 574, 412, 264, 960, 293, 352, 293, 3329, 439, 264, 1507, 300, 291, 434, 406, 1217], "temperature": 0.0, "avg_logprob": -0.1569923842248838, "compression_ratio": 1.7157894736842105, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1144, "seek": 586208, "start": 5870.76, "end": 5875.96, "text": " familiar with, understand how it works, anything you're unclear about, ask us and your colleagues", "tokens": [4963, 365, 11, 1223, 577, 309, 1985, 11, 1340, 291, 434, 25636, 466, 11, 1029, 505, 293, 428, 7734], "temperature": 0.0, "avg_logprob": -0.1569923842248838, "compression_ratio": 1.7157894736842105, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1145, "seek": 586208, "start": 5875.96, "end": 5879.8, "text": " on the Slack channel, on the forums.", "tokens": [322, 264, 37211, 2269, 11, 322, 264, 26998, 13], "temperature": 0.0, "avg_logprob": -0.1569923842248838, "compression_ratio": 1.7157894736842105, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1146, "seek": 586208, "start": 5879.8, "end": 5883.08, "text": " Teaching is the best way to learn, so go to the Wiki and try to explain the things you've", "tokens": [34244, 307, 264, 1151, 636, 281, 1466, 11, 370, 352, 281, 264, 35892, 293, 853, 281, 2903, 264, 721, 291, 600], "temperature": 0.0, "avg_logprob": -0.1569923842248838, "compression_ratio": 1.7157894736842105, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1147, "seek": 586208, "start": 5883.08, "end": 5884.08, "text": " learned.", "tokens": [3264, 13], "temperature": 0.0, "avg_logprob": -0.1569923842248838, "compression_ratio": 1.7157894736842105, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1148, "seek": 586208, "start": 5884.08, "end": 5890.0, "text": " And make sure that you can run the code up to here that we've seen today.", "tokens": [400, 652, 988, 300, 291, 393, 1190, 264, 3089, 493, 281, 510, 300, 321, 600, 1612, 965, 13], "temperature": 0.0, "avg_logprob": -0.1569923842248838, "compression_ratio": 1.7157894736842105, "no_speech_prob": 1.2218764823046513e-05}, {"id": 1149, "seek": 589000, "start": 5890.0, "end": 5894.2, "text": " For those of you who are pretty familiar with most of this already, make sure you can run", "tokens": [1171, 729, 295, 291, 567, 366, 1238, 4963, 365, 881, 295, 341, 1217, 11, 652, 988, 291, 393, 1190], "temperature": 0.0, "avg_logprob": -0.21371245751014123, "compression_ratio": 1.7665505226480835, "no_speech_prob": 2.318547740287613e-05}, {"id": 1150, "seek": 589000, "start": 5894.2, "end": 5896.92, "text": " this code on a different dataset.", "tokens": [341, 3089, 322, 257, 819, 28872, 13], "temperature": 0.0, "avg_logprob": -0.21371245751014123, "compression_ratio": 1.7665505226480835, "no_speech_prob": 2.318547740287613e-05}, {"id": 1151, "seek": 589000, "start": 5896.92, "end": 5901.84, "text": " And we'll talk about some possible different datasets that you can use tomorrow.", "tokens": [400, 321, 603, 751, 466, 512, 1944, 819, 42856, 300, 291, 393, 764, 4153, 13], "temperature": 0.0, "avg_logprob": -0.21371245751014123, "compression_ratio": 1.7665505226480835, "no_speech_prob": 2.318547740287613e-05}, {"id": 1152, "seek": 589000, "start": 5901.84, "end": 5905.76, "text": " Any of you that want to go further, please let Rachel or I know.", "tokens": [2639, 295, 291, 300, 528, 281, 352, 3052, 11, 1767, 718, 14246, 420, 286, 458, 13], "temperature": 0.0, "avg_logprob": -0.21371245751014123, "compression_ratio": 1.7665505226480835, "no_speech_prob": 2.318547740287613e-05}, {"id": 1153, "seek": 589000, "start": 5905.76, "end": 5910.56, "text": " We have lots of ideas for ways that you can extend this a long way.", "tokens": [492, 362, 3195, 295, 3487, 337, 2098, 300, 291, 393, 10101, 341, 257, 938, 636, 13], "temperature": 0.0, "avg_logprob": -0.21371245751014123, "compression_ratio": 1.7665505226480835, "no_speech_prob": 2.318547740287613e-05}, {"id": 1154, "seek": 589000, "start": 5910.56, "end": 5912.96, "text": " Thank you so much for coming and I'll see you next week.", "tokens": [1044, 291, 370, 709, 337, 1348, 293, 286, 603, 536, 291, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.21371245751014123, "compression_ratio": 1.7665505226480835, "no_speech_prob": 2.318547740287613e-05}, {"id": 1155, "seek": 589000, "start": 5912.96, "end": 5916.72, "text": " We'll talk to you during the week and make sure you've got your teammates' details so", "tokens": [492, 603, 751, 281, 291, 1830, 264, 1243, 293, 652, 988, 291, 600, 658, 428, 20461, 6, 4365, 370], "temperature": 0.0, "avg_logprob": -0.21371245751014123, "compression_ratio": 1.7665505226480835, "no_speech_prob": 2.318547740287613e-05}, {"id": 1156, "seek": 591672, "start": 5916.72, "end": 5923.72, "text": " you can all stay in touch.", "tokens": [50364, 291, 393, 439, 1754, 294, 2557, 13, 50714], "temperature": 0.0, "avg_logprob": -0.35461840629577634, "compression_ratio": 0.7647058823529411, "no_speech_prob": 0.00019525336392689496}], "language": "en"}