{"text": " All right, I'm going to go ahead and get started. And I wanted to announce first that I just put in Slack a link to a mid-course feedback survey. And this is just for me, and it's anonymous, but it's kind of helpful for me to kind of get feedback about how things are going and if there are things that it would be helpful to adjust. So please try to fill that out today. And also if you're just auditing the course, I'm still interested in your feedback, so fill it out as well. Yeah, so I was going to start with some review. We're going to talk about the robust PCA algorithm we saw last time and kind of look at a little bit of some different information about it from how we covered it before. But I wanted to start with just a reminder, what is the SVD? Jeremy, microphone. Anyone? SVD? Single-valued, singular-valued decomposition. Yes. It's just a more perfect decomposition of a matrix and the singular value of the diagonal matrix. And then, yeah, take it. And what can you say about the UNV matrices? Oh, well, I think they can represent different things, but it's like topic over word. That's true for topic modeling, yeah. And there's, was it the left singular and the right singular components of it? Yes, those are those names. I was looking for one other property, although everything you've said is true. The orthogonal? Yes, yeah. Thanks. And then what is the truncated SVD? Truncated SVD. How do you get from the full SVD to the truncated SVD? Brad? And be sure to hold the microphone close to your mouth. Sure. So, truncated SVD is simply, at least I think it's a lower rank approximation. And I think that in the context of what we were talking about last class, we're talking about a sort of random, I'm trying to think of the exact terminology, but using a smaller matrix and then factorizing that. Yes. Right. And then, or it could be like something like a lower rank approximation or something like that. Yeah, yeah. So typically you'll think of the truncated SVD as kind of taking the full SVD and chopping off however many kind of singular values slash columns of U slash rows of V to get something smaller, which is good for data compression. And then Brad brought up last time we were looking at this randomized approximation where we were kind of taking a different version of the same matrix by getting a random projection and it had fewer columns but the same or approximately the same column space. And then taking the full SVD of that smaller matrix and letting that be kind of the truncated SVD of the full matrix. It's worth mentioning that the particular randomized approach, Rachel taught, isn't the only way to calculate truncated SVD. So if you just did a full SVD and literally threw away a bunch of the last columns and rows, that's still truncated SVD. Right. It's an inefficient way to do it. There are other ways to calculate it. But the randomized approach is a very good way of calculating the truncated SVD. Yeah. Yeah. Oh, actually Brad's got another question or comment. So I know last week I sort of mentioned that in the approach where you actually calculate the full SVD and then you top off the last and or so smallest singular values, I know that that is the best low rank approximation. Yes. How well does the random approximation compare to that? I mean, I know that obviously that's not an efficient way of doing it because you have to do the whole thing to start with. But the point of the random is that you don't have to do the whole thing. Right. So is there any sort of intuition of how well that does, how good of a job that does approximating? So I think in general it does a very good job. I'd have to look back at the paper because I think the paper does give guarantees on how good it is. Yeah. There's actually a parameter you can choose how good it is, which is that, what was it called, the thing where you like add more components than you need? Yeah, I was kind of referring to that as buffer, but yeah, we have a different name for it. Like you say, I want five, but you actually calculate it with 15 and then get rid of the last 10. So if you made that parameter big enough that it was the whole thing, it's perfect. If you make it so small that it's like a zero, that it's perfectly perfect, so you can pick. But even with like 10 extra, it's going to be like 10 to the negative of something really small, close-ness when you look, right? Yeah. Yeah, the paper, I'd have to look back to it, though, like recommends either 10 or 15 when you're doing this in practice. I'm sorry. Okay, one more. Oh, wait, Jeremy, throw the microphone back, please. And so just one last thing about the buffer. So is the intuition behind that, like if you only want the top five and you have these extra vectors, is that sort of like a place to dump what you don't? Like if you only took five, then you're kind of squeezing all the bad stuff into sort of the ones that you want. Because if you have a buffer, it's like the... If you want to have the best five, you need somewhere to put the not good stuff. I'm kind of talking really vaguely. Yeah. I think I would say that... I actually got it on full screen. What that number is referring to is how many columns you're taking in your randomized approximation of the column space. Jeremy, how do I exit full screen mode in Metamoji? Well, I want to get a new sheet here. Okay, thanks. Oh. There, thanks. I was going to say, so yeah, so what those numbers are referring to is so like if... This is like your wide matrix A that's too big to decompose. You're kind of deciding, okay, how many columns do I want to get over here? I see it as more kind of guaranteeing, like your goal is that you want the column space of A and the column space of B to be the same thing, but kind of by adding some extra ones on. I think that's kind of a more reassurance of like, okay, I'm really hitting this much of the column space of A. Right. Because it's... I guess it's like you kind of just need those column spaces to be the same up to however many singular values you're trying to get. Right. And then I guess what I was wondering is are those top, say, five, if you want, say, the most important ones, then you necessarily need that buffer to sort of put the less important stuff, right? Whereas if you just have the five, then all the less important stuff is going to be kind of forced to be within that top five. Is that a good way to think about it? So there is this element of randomness still, and so I think some of it's like if you were just getting five, if you knew that they were truly the best, I think you would be okay. But because it's random, you're kind of taking some extra... But you're right. Like it is having a... Or Tim, did you have a... What's the analogy? I guess what I thought about it is like if they have like a bunch of, I don't know, M&M's, like the best tasting ones are like in the top left hand corner. And you know, there's five best tasting ones, but you can't just like pick five from the top left corner. You kind of pick like a handful more, and then that increases the chance of you having the best five in that kind of handful. That's what the buff... That's how I saw the buff. Yeah, I like that. I like that analogy. Yeah. You just pick a little bit more so that you have the higher chance of having the best in that handful. Yes. Yeah, that's good. Thank you. Sam? Do you think we can go over what's happening when we multiply by a random matrix? Can we see why it's useful and like the different kind of contexts we can use it? Because you guys mentioned that it's useful all over the place. Sure. So you're kind of thinking of like different applications of that. What does it do, first of all? What does it do, and then why do it? I kind of get it, but I'd like to know a little more about that. Okay. Yeah, that might be something I'll return to next time. Let me write that down. Yeah, I want to think about if there's maybe like a good way we can visualize that as it's happening or something. So kind of more detail on what is a randomized projection, where else is it used? One key idea we mentioned is this idea that when you multiply by a random matrix, the result in columns, as long as there aren't too many, you're likely to be independent of each other, so you're likely to get orthogonal columns, which is kind of a good starting point for thinking about why it helps. Yeah, and that kind of plays into the idea of like by taking a buffer, you're getting even more orthogonal columns, and so you are covering kind of more of your space. Sam, again? So I guess the part in particular that I don't understand is when we multiply by the random columns, we're transforming the matrix, and if the columns and rows all correspond to like one particular feature for each one of our users, for instance, then when we transform it by multiplying by a bunch of random columns, at what point do we have to like transform it back so that we have, so we actually have like... So yeah, so the transforming it back is what happens, if you'll remember, we like take the SVD on the smaller matrix and then we multiply by Q again, which I can't remember if it's Q or Q transpose, to kind of, that's what transforms it back, since Q is orthonormal, it's its own inverse, and so that's kind of the transformation back. I remember that part. Okay. Great. So I think we've thoroughly hit the first three questions about SVD and truncated SVD, are there any other questions about it? Okay, and then I think something that I definitely wanted to kind of say more clearly than I did last time is that classical PCA is finding the best rank K estimate L of M, so minimizing the norm of M minus L where L has rank K, and as Brad said, truncated SVD is giving you the best classical PCA, so that's the kind of best rank K estimate L is going to be if you did the SVD on M, your full matrix, and then cut off however many columns you needed. That's what gives you the best estimate. So that was under the Frobenius norm? Yes. Oh, that was under the Frobenius norm. And the Frobenius norm is where you're just squaring each of your elements element-wise and summing them. And this, yeah, kind of as we said before, you could get the truncated SVD by doing the full SVD and then throwing away the values you don't need, although that's slow, and so we saw the randomized approximation as an accurate substitute for that, to kind of do it more quickly. So that's a classical PCA. Does anyone remember what robust PCA is? I'll give you a hint, we added in another matrix, so instead of decomposing M just to be L, a single matrix, we decomposed it to be L plus another type of matrix. Sam? We're decomposing it into a dense sparse, which makes it more robust to our lives. Yeah, so we're getting a sparse one now, and so it's a low-rank matrix plus a sparse matrix. And I wanted to kind of highlight with the\u2026 previously when we were looking at this background removal problem, we started off just doing the randomized SVD, or just doing an SVD to get the background, and there we're really just trying to find this low-rank matrix, and that's the background. The difference with when we go to robust PCA is now we're trying to find the low-rank matrix and the sparse matrix, and so we're actually kind of actively looking for what is the foreground. So initially, kind of when we just did SVD, we were like just looking for the background, and then we were like, okay, whatever's left over must be foreground, whereas robust PCA we're taking this very different approach of, okay, we're actively looking for both the background, which is low-rank, and the foreground, which is sparse. And then what are some\u2026 So we talk\u2026 Oh, Tim? So how does it actually do that decomposition into L plus S? Like how do you actually do the decomposition? Okay, so we'll talk about that. That's a good question in a moment. And that\u2026I'm going to kind of just highlight some key parts of the algorithm, and that's kind of part of an alternating Lagrange method. And so we're not going to get into like the details of\u2026all the details of the optimization, but yeah, I will go over kind of three\u2026what I think three key parts of the algorithm are. Yeah, good question. Jeremy, the top of the microphone fell off. So what are some other applications that we saw? So we did the background removal. Yeah, facial recognition, and in particular, we were seeing faces where there was a lot of noise. So at any time that your data is corrupted with noise, particularly large amounts of noise\u2026so one of the weaknesses of PCA\u2026classical PCA is\u2026classical PCA can handle noise that's small everywhere, but it's very brittle if there's even just like a few entries that are way off and have a huge amount of noise. And so robust PCA is good at handling that. And in that case, the low ranked data is kind of your\u2026I guess, you know, real data or accurate data, and the sparse data is your noise, but those values can be anything. So when you have grossly corrupted data, which often happens, kind of particularly with online interactions, robust PCA is useful. Any other applications? Tim? All right, hold on a moment. Why would we not use robust PCA? Like in what case would we just not\u2026like what would we do? Why would we use classical PCA? It seems robust PCA is better. That's true. I mean, I guess the downside to robust PCA is it is a little bit slower. But yeah, it is\u2026 Is it much slower? Like are there cases where it would just be much slower than classical PCA? Well, actually, I'll say too. So it's slower. It's also more finicky to train. Like I felt like there were more kind of parameters to tune when programming it. Is the name robust PCA? That's true. Yeah. And some of this is, I don't know\u2026and maybe that's like once you have it working. Like some of this was like trying to get it working from the papers, but like there was kind of like a parameter I had to change. I probably feel like, do you really think it's slower? Because it's kind of using this randomization. Like there's another argument that maybe we always should use this. Yeah. Now that you've got it working. It's kind of hard to find a good working version and you did a lot of work to get it to work properly. Now it does maybe. Maybe this is the best place to start. Yeah, I mean I think I would still, just because SVD is so easy to run, you know, when you're using someone else's implementation, I think I would probably run SVD on your problem just to make sure like, okay, I want something better. But yeah, no, I mean robust PCA has a lot of advantages and there are a lot of places it's applicable. Right. It's not like robust versus classical PCA. Oh, yeah. Why don't you use robust? How about anyone use classical? I mean since like classical PCA though is kind of a, you know, this is like very basic SVD, that like that is something. I think I was even with the background removal problem and you shouldn't do this, but I actually did the robust PCA first and then was like, oh, let me see how I do on randomized SVD. And then was like, oh, randomized SVD was actually better than I expected. So that's a up here, like this is what I was getting from, yeah, just like the randomized SVD, which is like a single line. But yeah, like I do think robust PCA is a great algorithm to use. Yeah, good question. All right. Yeah, let me maybe go back through some of the steps of the algorithm now. And this I'm going to kind of display some different information than I did last time. So hopefully that'll be helpful to get a different perspective. But yeah, just to remind you, so at the top we had kind of first went through kind of how like what does our data even mean, how we're setting it up, you know, and turning this video into a single matrix where each column is one point in time, you know, and we've kind of unwrapped the pixels to just make a straight column from each point in time. And we tried it with a rank. Here's a rank one approximation, just using a randomized SVD. And then we introduced the idea of robust PCA. We saw the faces. And then I'll just show this again. If you are really interested in optimization, these are Jupyter notebooks from Stephen Boyd, who's at Stanford, has a convex optimization short course, which looks really interesting. Yes. When I tried to read Stephen Boyd's book and watch his Stanford class, I found it kind of inaccessible. Matthew, did you see what these notebooks are? No, I did not look in detail at the notebook. So yeah, that's a good warning to have. If you put on your stuff and have trouble, I know I did. Okay, so this, and actually, I guess, before I get into the heart of the algorithm, I wanted to say, so the authors kind of define what they call a shrinkage method. And what that is is basically they take any singular values that are less than tau and round them down to zero. And so you can think of that as another way of truncating your matrix, as like, kind of just ignoring the small singular values. So this shrinkage method will show up. Got a question, which is where to find that Stephen Boyd notebook? Oh, there are links to it in here. Let me find the section. So right above robust PCA, I have something that says, if you want to learn more of the theory, which is... And some of these, I think I've mentioned this before, but I'm using... Jupyter Notebooks has some extensions, and I'm using one that has the section folding, which is really handy. Collapsible headings is what they're called for being able to fold these up and down. So that's definitely something you need to kind of install that to get it, but it makes it much easier. Then you can kind of like open and close sections and navigate around pretty easily. And actually, let me just show maybe this high level of the algorithm. So what we're doing for our low rank approximation is taking... So here they call this curly D, singular value thresholding, but that's basically where you just take the SVD, do this shrinkage operation on the singular values, so throw away the ones that are too little, multiply back together to reconstruct your matrix, and that's the approximation for the low rank matrix. The approximation for the sparse matrix is to take the... Just do the shrinkage operator of throwing away the small singular values. And this is the... Sorry, I should say the alternating aspect is to estimate the low rank one, you're taking your full matrix minus the sparse one, kind of minus this error that you're keeping track of, and then you kind of alternate back to, okay, let's estimate the sparse one, that's the full rank matrix minus the low rank one. And you're kind of going back and forth between, let me estimate the low rank one using the sparse one, and vice versa, let me estimate the sparse one using the low rank one. And so kind of how that looks in the code. So this PCP, which stands for Principle Component Pursuit, and I should say Principle Component Pursuit is just one algorithm for robust PCA, so robust PCA is kind of referring to this decomposition we want, and there are different algorithms to get there. And in this class we're using Principle Component Pursuit. Kind of the heart of it is we've got this for loop, and so then I picked off a few of the key things and added comments, and this has been updated on GitHub, but I said here, you know, we're updating our estimate of the sparse matrix by shrinking slash truncating the original, sorry, the original minus the low rank, so we're getting kind of the difference between that. And remember, we're going for original equals low rank plus sparse, so whenever we do original minus low rank, that should be sparse. Then we update our estimate of the low rank matrix by doing the truncated SVD and reconstructing it. And then something that kind of makes this more intricate is we don't actually want to calculate the full SVD, so we need to say how many, so we're using a randomized SVD, but with randomized SVD we need to say how many singular values we want, so we're giving it a rank that we're interested in, and here that's called SV. And so SV is something that we're going to update each time, because no, it would be, if we try to remember the dimensions of this matrix, I believe it was 4,800 by 11,300. We don't want to do the full rank for that. That would be a lot of values. So we're just going to do some of those. And the way it kind of tells how many to do is, so after you truncate, so when you're kind of dropping all the values that are less than, in this case it's 1 over mu, we're throwing away, if we find that we're getting more values than we need and are throwing away a lot of them because they're so small, then we just increase by one how many singular values we're getting, because we're basically doing fine. We're getting plenty since we're having to throw a bunch of them away. However, if we're keeping all of them, that's a sign that we're not getting enough singular values because they're all large enough, and so there might be more of a kind of large magnitude that we're not finding. So in that case, we add 20% of the smaller dimension, which in this case is 240, to SV. And I think this, actually let me show this now. So I added some print statements that I didn't have last time to kind of say what rank it's using each time. So here when we use the algorithm, it actually starts with just calculating rank 1, and then that was not enough, so we add 240. So that's 20% of the smaller dimension, which is 4,800. So next time we've got rank 241. And we do that, and this drops down to 49. Oh, I think that's, so what it's resetting to is how many values you kept. So we did that, and it only kept 49 of the values. Although then, oh, we kept 48 and added one. Okay, and then that time that wasn't, kind of wasn't enough, so we're going to add the 20% again, which is, and keep in mind each time we're doing this, we're doing it on a slightly different matrix, you know, because we've been alternating back and forth, updating our estimates for LNS. So then we add 240 again to get to 289. And this whole time it's keeping track of this error. Add another 240, and then our error is low enough, and so the algorithm halts. But that's kind of a, it gives a little different perspective. Let me go back to the algorithm, because there's just a third piece. So we've got this aspect of, you know, we're updating how many singular values we want to calculate each time. And then here's where we're calculating the residual. So looking at X, which was our original matrix, minus our low rank matrix, minus our sparse matrix. And so that's everything that hasn't been factored into either the low rank or sparse matrix yet. Then I also wanted to show, so this time, and this has been updated on GitHub as well, I made an examples list where I, at each step, and I just kind of randomly chose, I'm going to keep track of the picture from column 140, which is just a particular point in time. This is something that you wouldn't, row 140, because I've transposed, so there's a kind of check up here on the shape, and you, yeah, if M is less than N, then you transpose it. So yeah, but in the kind of pictures I showed before, it's column 140. So I'm just kind of keeping track of one point in time. This is something that makes it slower to kind of keep track of this list, but I just did that to get some visual output of how things are changing, and I think that can be a nice way of kind of seeing how it's working. And this shows up here. So now, actually, when I ran the algorithm, I got back my examples. And so you can see during the first, or at the end of the first iteration, this matrix on the left is the sparse matrix. It's not very good. It's just a black square, so I think it's maybe everything is zero. And here's our matrix on the right, which is already pretty good, because we've done an SVD, and so that's picking out the low rank part, but we really want to get the people as well. And so you can see next step, we're kind of starting to get some people. And then they're coming more into view. And I actually don't see a huge amount of difference between the fourth and fifth iterations here, but I thought this was kind of nice to see at the end of each iteration what is the low rank one look like and what is the sparse one look like. And we kind of talked before about how finding the sparse one, I think, is more difficult than the low rank one, just in that... Actually, no, that's not true. I was going to say with the people... I guess the sparse one, kind of as long as you have figures there, maybe you're not worried about all the edges. Yeah, disregard. But so yeah, this is what happens kind of after each iteration. Any questions, Kelsey? How unique is the solution to this? So in this case, maybe there's a pretty clear local maximum. Yeah. If you have a situation where it's less well represented by the decomposition, does where you start affect where you end up? That's a good question. So there are some additional constraints on this problem. So in general, you have to say that this algorithm is forcing the low rank matrix to not also be sparse, I believe. If you had a low rank matrix that was also sparse, then it's kind of not... So the general statement I gave you of robust PCA is actually not fully... Or is not uniquely defined. There could be several different decompositions. But I think most algorithms kind of make additional assumptions to kind of constrain it further. But... What happens if I were to give it like an image that didn't have, say, a video where the background was changing constantly or something like that? Would you potentially end up with an unstable decomposition of some kind? Yeah, I mean, I don't know that that would even converge. You could always have a unique SVD. Right. Would it be more like on the map where you could possibly come up with any different solutions? I mean, I guess the example of a video with the background changing, to me, sounds like there might be no solution in that I don't know that that could be represented as a low rank matrix plus a sparse matrix. The other factor you have is how much of a penalty... So there's kind of like a parameter of kind of how much you want to penalize the low rank error versus the sparse error of those two matrices being off. And I think if that was adjusted, that could give you a different... Different results of how bad having your sparse matrix not be fully sparse is versus your low rank matrix not being fully low rank. I don't know if this makes sense. I was just wondering, if it wasn't constantly changing, but more like in the real world you'd have more like switching scenes from time to time, would you end up with something where the low rank matrix is like a row for each scene, you know, and then the kind of thing that's multiplying by something coming at each time point how much it represents that scene? Would that work? I mean, obviously if you try it, I was just trying to think if it makes sense intuitively. Yeah, that seems... They're almost like topics. Yeah, no, that does seem possible to me. Because we figured out we could recreate the original matrix if it didn't change the background with just a single... With rank one, yeah. Yeah, no, that does sound like it would work. I'm going to try if anybody gets them. Yeah, no, I would love for you all to try running this code on different videos to see what you get. I suspect one where it's gradually panning might not work at all, but again, interesting to try. Yeah, these are good questions. Any other questions? And again, we're not going into all the details of this algorithm. I just want you to kind of have a high level idea of kind of what we're doing with the, you know, alternating between getting the low rank and the sparse and with adjusting the number of singular values we're calculating for the low rank one kind of based on are we getting lots of little ones that we can throw away or are all of them meaningful? Yes. Yeah, just a quick question. So in terms of being grossly corrupted, how do you best define that being corrupted? So I mean, I would think of that as... I mean, you would have to set some sort of like parameter, like I don't know if your air is less than tau, that small air, but I think of that as kind of something with like where there are entries that are just completely wrong. You know, it's not so much that they're like close, but they're just off. For example, in this image, right, when we're trying to separate the background with the people. So in this case, how do you define the images being corrupted? So this image is not corrupted. Yeah, that's a good question. So this image is very high quality, but the ones we saw above... Let me go back up. Okay. So these facial ones are grossly corrupted. So kind of like when you have the pixels where it's like... A lot of these white pixels are just not even close to being the right color. They're damaged somehow. Right, got it. Thanks. Yeah. And Netflix talked about having this problem. And so I would assume with Netflix, that's where the... I don't know, someone has put in a movie rating that's just wrong. It's not how they felt about the movie. And perhaps they made a mistake or accidentally entered it or weren't trying to enter what they did. Yeah, and so it's kind of important to note here. So this problem with the faces is different from... And I mean, this does... The original data set here was showing pictures of the faces lit up from different angles. So you did have kind of the same face many times with different lighting sequences, but it's really different in that kind of the faces, the low rank part, which was the background, and then the sparse component is these pixels that are completely wrong. And so here, we're only interested in the sparse component so we can get rid of it. It's not like with the background removal where it's like, oh, we're interested in the people, which are sparse. Whereas here, it's just like, okay, we want to know how to get rid of this sparse wrong stuff. And then I also wanted to highlight again, I think I have it on here, with topic modeling, we could have used robust PCA. So we just used SVD or NMF on topic modeling in the first week. But here, the low rank part could be common words that are in all documents. And then the sparse part could be a few key words from each document that kind of make that document different from the others. And I would actually, kind of going back to Tim's question of like, isn't robust PCA always going to be better? Yeah, I would be really curious to see some topic modeling done with this because it does sound really promising to me to kind of have the kind of like the sparse words defining a document. Any other questions? Okay, so we're going to move on to the LU decomposition. Let me just make sure I've done all the results. Yeah, so just again, here are, and I wrote a little kind of helper method plot images where you can enter your original matrix, the sparse and low rank ones you got back, and then just however many points in time you want to see since I think it's kind of most helpful to see, you know, them lined up next to each other from this is at time zero or kind of whatever corresponded to the zeroth column at 100 and at 1000. Yeah, let's start on LU decomposition. So above, we had used Facebook PCA, which is a randomized SVD. And then in the previous lesson, we had kind of written our own randomized range finder, which was basically just a less robust version of scikit learns one or I mean, it did less error checking for both of those use LU factorization. And so we're going to go into kind of how how to do an LU factorization, since we've kind of used some methods that use it. LU factorization factors a matrix into the product of a lower triangular matrix, that's the L and an upper triangular matrix, which is the U. Then I wanted to check who remembers Gaussian elimination. And then who who wants more review on Gaussian elimination? Okay, so let me, I'm going to go through one example. And we can even if you want to see a second example, let me know because I know it may have been a very long time since you've done Gaussian elimination, kind of depending on when you took linear algebra last. Okay, so ignore the LU down here, we're just going to be that'll come up in a moment. But we're just going to take this matrix A and want to do a Gaussian elimination on it. Actually, does anyone remember? So what's the kind of the first thing you do with Gaussian elimination? Like you're trying to reduce it to, yeah, well, I mean, there are a lot of different ways to talk about it. I was just going to say you kind of want to get rid of everything beneath one, turn those all into zeros first. Although yeah, like long range, I think you're you're doing other stuff. And this reminds me of a I think it was Trevathan that says at some point, like so much of numerical linear algebra is just like inserting zeros into matrices. But here, you kind of kind of want to take this first row, and we want to get rid of the three in the second row. So we can end down here, we're going to put a matrix. So this matrix in the bottom corner, this is not something you would have done typically in numerical linear algebra, but we're going to keep track of what we multiply by. But here, I'm going to kind of multiply this first row by three and subtract it to zero this guy out. So first row is not going to change one, negative two, negative two, negative three. And our goal is to make that zero. And so we did that, or actually, yeah, we did that by multiplying by three and subtracting. So that makes the top one would become negative six, subtract that. So negative nine plus six is negative three. Negative six and zero. That's positive six is actually zero minus negative six. And then times three, those are going to cancel out because we ended up with negative nine minus negative nine. So that's kind of step one of Gaussian elimination. Questions about that? And initially, yes? Why do we, so just to warn you, Gaussian elimination is a thing that made me never want to see linear algebra again. You can come back to it for like 20 years, so I'm very hazy on this. Why is it that we're allowed to like subtract rows from other rows? Like what are we doing? That's a good question. So really, actually let me see if I, is there a way to add more space on top, Jeremy? Well, I wanted to be able to write more, but okay. So really what we're, often where these problems come up is when you're solving a system of equations. And so you might have, I don't know, maybe this was one X minus two Y minus two Z minus three W. And then we weren't dealing with a vector B, but often you would have some value over here. Like I don't know, maybe this is five. And then you have another equation that's three X minus nine Y, no Z term, minus nine W equals 10 or something. And so one way to solve these, if you're just kind of thinking about them as systems of equations is you could multiply the first equation by negative three and add those together. Because think, you know, if you had a solution to this, you know, values for X, Y, Z and W, multiplying both sides by negative three, you know, that equation is still going to hold. You multiply by negative three on both sides and then you can add those together as kind of a legitimate thing to do as well. And that's even kind of, I think like a more intuitive, like if I gave you, actually let me just go to a place of the page that I don't think we'll need. You know, if I gave you some problem and we're like solve. Actually this might be too easy. Okay, yeah, because you'd plug in the X. But you could also think of that as like, oh, you know, like let me multiply each side by three and subtract those and then I'll get the equation just in terms of Y. So that's where this kind of idea of multiplying by rows by things and adding or subtracting them. Is that helpful, Jeremy? Yeah, I think so. I guess I'm still trying to make the connection to solving equations versus like is that equivalent to any decomposition or just this particular decomposition or why can we, why is the fact that we can do this in simultaneous equations when we can do this in matrices? Well, so the matrix is just representing the system of equations. And so- Like in general or in this particular case? Well, so I mean in this particular case if we were to write the first line as 1X minus 2I minus 2Z minus 3W and what kind of the broader problem is you might be interested in solving AX equals B and you might be interested in solving that for a lot of different values of B and you wouldn't want to have to kind of go through and solve it separately each time and so finding this fact is kind of getting ahead but finding a factorization of A is going to let us then have a quick way of solving it for a bunch of different Bs. We kind of just factor A once and then we could solve it for a bunch of Bs. So this idea of thinking of this system of equations is specific to the idea of finding a factorization. That's where we can use this technique. This yeah, yeah. But something kind of to spoil the punch line is we're going to see that Gaussian elimination is very closely linked to LU factorization and they're actually kind of even talked about almost like interchangeably. So this kind of process even though kind of in a linear algebra class you're usually not seeing that it's giving you a decomposition. It's the same method. Yeah. So going back to the decomposition, the first column with zeros beneath the diagonal. So we've gotten rid of this 3 by multiplying, oops, multiplying the top row by 3 and subtracting. How can we get rid of this negative 1? Can I say it louder? Yeah. So add the two columns together, or two rows, sorry. The first and third rows we add those together. This is a zero. And I should note that adding is actually, yes, it's like multiplying by negative 1 and subtracting, which seems like a more roundabout way to talk about it, but we're going to put a negative 1 down here to keep track of how we got this entry to zero. So yeah, add those, we've got a zero there, which is nice. This becomes 2 and this becomes 4. And then for the last one we can multiply by 3 or think of it as multiplying by negative 3 and subtracting. So let's put a negative 3 here to keep track of, and then here this becomes zero, negative 3, negative 2, this will become zero. Or does that, yeah. Oh, negative 12, I reversed my signs. Thanks. And then this one is 20? Thanks. Okay, so then the next step is we want to fill up, oh, did I write down the last, yeah. Now we want to make everything below negative 3 have a zero. And so it's nice the, row 3 has already been done for us. So I'll just put a zero here. We didn't actually have to do anything. And again, the top row is not going to be changing. The second row is not going to be changing. Actually, neither is the third row, so we're really just working on the last row at this point. It's a negative 3. And note that the reason, okay, so now that we're trying to get rid of this negative 12, we're going to use the second row because we don't want to flip this zero back, the one in the corner, flip it back into being something that's non-zero. So we use the negative 3. So multiply that row by 4 and subtract. So let's keep track of that 4. We'll write it down here. We used a 4. That becomes 0. This becomes, I guess, negative 4. And this one becomes, oh, stays negative 7. This is totally reminding me of why I hate a Gaussian elimination. Let's never do this again. The good news is once we get familiar with it, then we can get a computer to do it. So that's the benefit of learning the LU decomposition is we're going to get a computer to do this for us in the future. But I did want you to remember what the process was. Half the final assessment will be doing this by hand. Okay, so now we're getting rid of this negative 4 that's right here. So the good news is we've got the top three rows are in good shape in terms of moving to our upper triangular matrix that we want in Gaussian elimination. So really, we just need to multiply the third row by 2, or I guess negative 2, and subtract. So let's put a negative 2 in here. Keep track of that. And then, yeah, this becomes 0. I've got negative 7 plus 8 is 1. And so we're done with our Gaussian elimination. Sometimes at this point, actually, I think this is where you would stop. And then amazingly, the matrix here is actually what I had written right here. And so it's going to turn out that a is equal to L times u. So we are keeping track of what we are having to multiply each row by to add and cancel out the other row. We're just going to fill in 1s along the diagonal, and then zeros everywhere else. But yeah, this is L. So keeping track of those coefficients. And what we got from our Gaussian elimination was u. First, let me ask, are there any questions just about the process of Gaussian elimination? And I checked, and Khan Academy has several videos on this if you did want to watch more Gaussian elimination, Jeremy. Linda? So the Gaussian elimination is the process to calculate the L and the u? Yes. Yeah. And sometimes Trevathan, I think, almost uses LU decomposition and Gaussian elimination interchangeably. Matthew? Why is it called Gaussian? Gauss discovered it. I feel like other people might have simultaneously discovered it. I'm not sure. I'll look up the history on this. So linear algebra is that old? Yes. Yeah. Yeah. And I think this question of solving linear systems of equations is one that shows up a lot and is useful. Yeah, that's a good question. Any other questions? Vincent? Yeah. When we're calculating the values in L, I can get behind the lower diagonal values. But then the diagonal being 1, it's like we did 1 of row 1 to row 1, and we did 1 of row 2 to row 2. And that doesn't fit with the numbers. Yeah, with the- I'm curious about the intuition behind that. Let me think about that one. Yeah, that's a good question. Yeah. Right. Now let me check the time. Okay. So this would be a good time to stop for a break. So let's meet back in seven minutes at- yeah, at 12.08 or 12.09. And if you're bored, this is a good time to take the mid-course survey. The link is in Slack. Thanks. All right, let's get started again. Let's get started again. Yeah, so when- let me just make sure. Yeah. Okay, so we left off by doing the LU decomposition. We had this matrix A, and we got L and U here. I'm going to say- and Treffith and goes into more detail about this, but you can think about Gaussian elimination as kind of being this series of multiplying by matrices where each matrix is kind of just doing like one of the operations and kind of putting those together to get your final decomposition. But as Jeremy pointed out, Gaussian elimination is a bit tedious to do by hand, so we want to automate that and have a computer do that for us. So this is the basic LU decomposition. Sorry, what's the thing above it? You've got multiple L's there? Is that- Yeah, so that's where you think about each operation you do as a single matrix. Oh, multiplication. Yeah, yeah, and so here like the product of all of those would end up giving you, I don't know, maybe in this case like the inverse of L or something. And if you want to talk about this later, feel free to leave it for now, but why is this decomposition interesting other than the fact that it appears in something we've seen already, or is it basically just useful as a component of other algorithms? So I mean it's useful on its own for kind of the use case I mentioned before, if you're solving a linear system of equations multiple- well, there are other ways to do that as well, but it's one way of solving a linear system of equations, particularly when you're going to do it for multiple vectors B on your right-hand side of the equation. It's helpful to store these because that'll speed up getting your X's kind of once you have the L and U. Yeah, so I wanted to go through how, kind of how we have the computer doing this. So we're making a copy of A that's going to be our U, and then remember U is what- actually I should probably go back to this. U is what we ended up with at the end of our Gaussian elimination. So we're going to kind of be starting off with A, we copy that into U, and then are going through these steps to get it in this form. And then L is where we're going to store kind of what we're multiplying by. So what we do is loop through K, and then- so K is in range n minus 1, since we don't need to do anything to the first row. J- or sorry, to the last column. J is going through K plus 1 to n, so that's kind of how when we're working on a particular row, we need to do everything beneath it. And we'll set Ljk equals the ratio of Ujk to Ukk, so that's kind of seeing how, you know, whatever spot we're at compares to the diagonal, which is Ukk. And then we subtract off that times Uk, comma, to Kn is giving us the kth row, kind of the remaining columns. So this is just the process that we did before, kind of put into code. And then we return L and U at the end. If there are questions about this- and actually maybe it would be helpful, let me add a step. This might be too much, but I could try printing L and U each time. I might not be connected to the kernel. Let me check. Okay, so run this. Oh, this is not aligned well. But you can- actually, let me just print U, because that's- it'll be more interesting. U will give us kind of the matrix A that we were reducing with the Gaussian elimination. And so you can see here, first we're introducing a zero here, and then a zero beneath it, and then at the third inner loop we've completely zeroed out the column- or the columns- the spaces in the first column. Then we go into the next one and we start zeroing out everything below the diagonal in the second column, and so on. Any questions? I don't recall, but this algorithm looks slow. Like we're having to do a separate operation for every one of the diagonals, so that's already N by M, and then each time you're having to do it on a whole row. That's right here, so it's the work- yeah, the big O for this is 2 times 1 third N cubed. And what you can think of it is it's kind of like a pyramid, the amount of- whoops- the amount of work it's having to do, in that kind of you've got your outer loop, which is approximately N iterations, but then you're just going from that point up to N, so that's kind of like forming a triangle, and then within each of those you're just doing work on the kind of entries in the- like from K to N. And everything's dependent on everything else, so this doesn't look very paralysable at all. That's true, yeah. I want to make a note to think about ways to speed it up, because now I am curious. We're actually going to be talking- we're going to go in a different direction today, Jeremy, which is kind of thinking more about the stability of this, but yeah, speed is interesting too. But yeah, so that's how we get kind of the big O of N cubed, and the one third N cubed is coming from this idea of kind of like, yeah, it's pyramid-like, how much work you're having to do on each row, and typically with big O you don't talk about the coefficients, but I have them in there just in case. So yeah, but before we get to that, I just wanted to highlight that the LU factorization is useful. Solving AX equals B becomes LUX equals B. Then you can solve LY equals B, and UX equals Y, and what might be nice about solving, so solving kind of steps two and three, those are both triangular systems. Can anyone say why those would be a better thing to solve than your full system? Kelsey? Because you can sort of even like backwards substitute, just do one equation at a time. Exactly, yeah. You can take like the tip of your triangle where you just have one variable, solve that one, back substitute that into the equation that just has two variables, and so on. So it is always nice when you can get stuff into a triangular system. As for the question of memory, so above we created two new matrices, L and U. However, they can actually be stored in matrix A by overwriting the original matrix, and remember with L, since the diagonal is always one, so you don't actually have to explicitly store that. And so this is called doing something in place, and it's a really common technique in numerical linear algebra. If you ever wanted to use A again in the future, you wouldn't want to do that because you are writing over it. But one of the homework questions, and I'll probably put this homework up on, I might put it up this afternoon or Thursday, it won't be due until next Thursday, so like a week from Thursday, is to modify LU to do the algorithm in place instead of kind of creating this separate L and U. Yeah, any questions about kind of work or memory for LU decomposition? Okay, so now we're going to consider another matrix, A, which is 10 to the negative 20th, 1, 1 and 1. And actually, I want you to take a moment just on paper to use Gaussian elimination to calculate L and U. Okay...... Raise your hand if you want a little bit more time. Raise your hand if you're ready now. Okay, I'll give you another minute. Alright, does someone want to say what they got for L? Okay, Vincent, Jeremy. So row by row I've got one, zero, and ten to the twentieth one. Yes. Excellent. And then this... Yeah, so one, zero, ten to the twentieth one, and then U is ten to the negative twentieth, one, zero, one minus ten to the twentieth. Which would be approximately negative ten to the twentieth, since that's so much bigger than one. And so if we enter these as matrices, I'm going to call them L1 and U1. Here they are. Actually, first I should ask, are there questions about getting this Gaussian elimination or the LU for A? And you're probably feeling a little bit nervous because we had this ten to the twentieth and ten to the negative twentieth, which is really different from one. So we might run into some problems. So we do an LU decomposition on A, and this is using the LU that was written above. So kind of just our own implementation. And what we get is about right. Oh, I redefined everything. I shouldn't have done that. Okay. Sorry, hold on a moment. Okay. Got A being what we want. So we can check. L1 is close to L2. So remember L1 is the one we calculated by hand. L2 is the one we got from our algorithm. Or got from our implementation of LU. U1 is close to U2, so that's good. Now, if we check that L2 times U2 is close to A, we get false. It's not. So this is kind of interesting that like each component, L and U, was close to the answer, but L times U is not close to A. So this is an example that so the LU factorization is stable, but it's not backwards stable. And I'm going to kind of define what those mean in a moment, but I think it's helpful to kind of keep this picture in mind. So an algorithm F hat for a problem F. So you'll think of F hat as kind of like how you're implementing this, whereas F is like the true problem you're interested in. And we say it's stable if for every X, F hat of X minus F of Y, norm of that over the norm of F of Y is less than machine epsilon. Where Y minus X over X is order of machine epsilon. And so the way Trevathan says this, I really like, is that a stable algorithm gives nearly the right answer to nearly the right question. And so kind of translating that here, so X is kind of the right question that we're truly interested in. And we're just saying that there's some Y that's close to that. So we've got nearly the right questions. That's why it's you know, it's almost X. Where this is how we're defining almost. And then the right answer is F, but we're just getting nearly the right answer, which is F hat. So the right answer to nearly the right question. Jeremy's face. Yes. Yes. Yeah. So X is the true problem you're interested in. And Y is the problem close to that. So like in the true problem, that's a good question. And that's because we're saying that the you're getting nearly the right answer to nearly the right question. So here. X would be. I feel like it's helpful to kind of like unpack his statement in reverse order, but like nearly the right question is Y. So X is the right question. Nearly the right question is Y. The right answer is F. So we're looking at the right answer to nearly the right question would be F of Y. So to get like what was truly the correct answer to your representation Y that's not quite the true question X, but it's close. That's what's giving you the F of Y. But then we're just saying you get it. You're getting something that's close to that. Don't work for me to understand. I liked this interpretation because I think it's I mean this is like a math definition of stability. And I appreciated him kind of giving a way to like think about it in more colloquial terms. But yeah, it's OK if you don't fully get it. It's kind of thinking about like how we formalize these ideas of stability that we have like a general notion to of you know like we want the answer to be close to what we want. But kind of how do you formalize that? So fortunately, backwards stability is actually simpler than stability. And it's also stronger. So that means that anything that's backwards stable will be stable, but not the reverse, as we just saw with the LU factorization, because that's stable but not backwards stable. And so an algorithm F hat, our problem F is backwards stable. If for each X, there's a Y close to it, such that F hat of X equals F of Y. And so Trevathan says that's a backwards stable algorithm gives exactly the right answer to nearly the right question. So that's why we're taking. Yeah, F hat of X equals F of Y. And so yeah, we'll we'll probably talk about those more next time. So now let's look at the matrix 1, 1, 10 to the negative 20th 1 for we'll call that A hat. And I want you to take another moment and use Gaussian elimination to calculate what L and U are for this matrix A hat, which is similar but not identical to our A from before. Okay. Okay. And who who needs more time? All right. Does anyone want to say what they got for L? Okay, Kelsey. So before it was, yeah, before it was 10 to the 20th, now it's 10 to the negative 20th. Yeah, which is subtle. Yeah. And then before we had 1 minus 10 to the 20th in our U and it's 1 minus 10 to the negative 20th. Yeah. So how was actually I guess first I should show the punch line, which is that we take our L U decomposition of A and check if it's if A is all close to L and U and this time it is. So that's that's an improvement. Vincent, Jeremy, can you throw the microphone? Shouldn't the top row and U still be one and one? Yes, it should be. Thank you. Okay. And so then how does A compare to A hat? Look back at A. Okay, so Tim is doing the gesture to indicate that two rows were switched. So really this A hat was just like A, but we switched the rows. And first I want to say like switching the rows, if you think about this so often, you know, each row in a matrix might be a different data point, switching the rows should be totally fine if those are different data points, if you know, like different samples and some like measurements you've taken. These were systems of equations, switching the rows is fine and actually no matter what, because you can think of this as just multiplying by a permutation matrix P. So here we did 0 1 1 0 is P, multiply that by our A to get A hat, apply Gaussian elimination to P times A. And it actually turns out that it's okay to kind of permute them because at the end you could permute them back to what you had originally. And so this is what pivoting is, is to switch the rows around and basically, and this is called partial pivoting, and that's where at each step you want to choose the largest value in column K and move that row to be row K, because the problem we were getting into before was when we were dividing by a really small number, which we don't want to do, and so kind of choosing a large number is going to be more stable. Yeah, so that's going to be a homework problem of kind of adding partial pivoting to the LU factorization. And then we can see here, this is just going through, is this the, no, this is a different A than we had before. You'll get back a kind of LU and then a matrix of what your pivot, or your permutation matrix P, so that's letting you know how you permuted the rows. And then you can check that the, this is an example from Trevithan, we get the same answers. There is something called complete pivoting, which permutes the rows and the columns. We're not going to get into that into detail. It's so time consuming that it's rarely used in practice. I think it's basically never used in practice. Any questions about pivoting? Why would you want to permute the columns? So permuting the columns, you can actually kind of run into the same issue of, yeah, I guess having like numbers that are too small. And so by like permuting the columns, you're able to get the largest value, which is going to be more stable in a particular place. But yeah, it's just a lot to keep track of. This is more for something that's used, like this is more for making the computations more accurate. It's not like something we would do. I guess like all of this is to make our actual computer algorithm return a more accurate answer, or is there an analytical reason for doing this? I mean, so I guess, yeah, it's to get your computer to return a more accurate. And like part of what was going on with this example where the 10 to the negative 20th failed, I mean, that's less than machine epsilon, but it's something that's causing us to get something that's not even close as a result. I will, I guess, kind of jump to the punch line, perhaps, which is that even with partial pivoting, it's going to turn out that this algorithm is technically not stable. Complete pivoting is? Yes. Well, actually, let me confirm that. I think that it is. I'll look it up and confirm. The issue with with partial pivoting is that the matrices that would, so it's easy. We'll see in a moment. We're going to construct a matrix that is unstable. However, those matrices are so rare in kind of nature or in practice that you don't actually get them. Yeah. Good questions. Yeah, and I'll confirm about a complete pivoting, which I believe is stable, but too slow to actually use. So now we're going to look at a system of equations of this form. So here we've got a matrix where we've got ones on the diagonal, ones in the last column, negative ones below the diagonal. And we're saying that times X equals this matrix of ones with a two in the second to last entry. And so this is a very kind of particular type of matrix that we've constructed here. And so I wrote a function, makeMatrix, that will generate that for any size n to kind of have this form. And it's just. Yeah, setting the last column to ones, starting off with the identity, setting the last columns to ones and then setting everything below the diagonal to negative one. We're making this vector over here, which is going to be all ones except a two in the negative second position. I was going to have you do Gaussian elimination on the five by five system, but you might be. I see heads shaking. Yeah. Let's not do that. Also, in the interest of time and you may have had enough Gaussian, no, it wasn't. It wasn't just you. Enough Gaussian elimination practice. No, this is this is why we have computers. But here we're going to use the SciPy dot lenowg has an L U solve and into that actually pull up the. Pull this up. You're going to pass a matrix and then the vector that you're trying to, you know, this is going to solve AX equals B and you're giving it A and B and it'll return X for you. And so I'm running this for matrices of size 10, 20, 30, 40, 50, 60 to see what happens. And then I'm going to plot with the last five values in the solution are keep in mind since the system is getting bigger, you know, the first one has. 10 and then 20, 30, 40, 50, 60, but to have a way to compare them. And here we're printing them out. And if you look at the last five values, you might notice, OK, the last value for the first one, the 10 by 10 system is pretty close to one. We've got something pretty close to a half. Negative a fourth. Negative an eighth. You might see a pattern. And that seems to continue. OK, one. This is in scientific notations. You kind of have to pay attention. OK, this is really point five, negative point two five, negative point one two five. So we've got this pattern for 10, 20, 30, 40 and so on. And then notice what happens when we get to 60. Our answer. So this should kind of raise a flag for you that it seems to not be in keeping with where we were. And so here where I plotted them, it's a little bit hard to maybe see because you basically. So I've made these dashed lines of different colors. And basically these are completely on top of each other. So this like thick multicolor line, that's the solution for the last five rows for 10, 20, 30, 40, 50. And then we jump to this when we hit 60. So they don't want to say what they think is happening when any goals 60. Any guesses. And I shall give you a hint. Actually, OK, I'll just say I had written out the Gaussian elimination. I won't make you go through it. But what you end up with for this system is. This is the five by five version. This is your you. So they have all zeros in there. You've got a diagonal of ones and then the last column. This is kind of pathological. It's doubled in every entry from what we were doing because we had all those ones. And then we're saying, oh, that's equal. And so over here, something very similar was happening. One, two, four. And then nine and 17. And those are off a little bit since we had that two entry amongst our ones. Otherwise, it would have been kind of perfectly there doubling. Actually, this first question is on what I've shown. So I just took the five by five of that system. I did Gaussian elimination at home, wrote it out. This is what you get. And now if we were going to go back and backwards solve that, and I haven't written all the zeros in over here, but these would all be zeros. So let me make so often when people write that, they'll kind of just have like a big zero to show like, OK, those are all zeros in there where it doesn't say otherwise. Actually, this is a good time to point out you as a sparse matrix in this case that has lots of zeros. So if we were going to start solving this now, you know, it start with the last row 16 times X sub five equals 17. And we're going to divide through by 16. So now any kind of guesses? Maybe what was going on when we hit an equal 60. Sam. I guess two to the value at the bottom right of our upper triangular is going to be two to the 60. Yes. Plus one is it's the plus one is lost. So it's just 60. Exactly. Yeah. Yeah. So we've gotten so large that we're kind of going to lose that distinction of having and I think I think it might be two to the 59. But yeah, exact idea of kind of, yeah, like you've got to do this huge exponent. And so then I can't keep track of this like little plus one. It kind of overflows. And so we're getting getting the wrong answer. And so this is a this case where Gaussian elimination or LU factorization, even with partial pivoting fails. And I think there, yes, there, if we had had complete pivoting and we're pivoting on our columns or, you know, permuting our columns in addition to our rows, we wouldn't get this giant number that had added up to two to the 60th. And so that would have solved this. But since this case basically never arises and complete pivoting is so slow, we don't use it in practice. And here this is kind of saying that in math more formally that it has this idea of a growth factor row, which is the maximum U value over the maximum A value. And that in that times machine epsilon is part of the kind of error that you can see. And so, yeah, it's a bad thing when row is huge. But, yeah, to get back to Tim's earlier question, question complete pivoting would have solved that because we would have been making those big numbers while we were pivoting on. And so LU, many numerical linear algebra classes kind of teach LU first, although Trevithin kind of points out that it's not it's not a typical case, even though it's, you know, very widely used algorithm. But this idea that kind of Gaussian elimination with partial pivoting, so this is a quote from Trevithin, is utterly stable in practice in 50 years of computing. No matrix problems that excite an explosive instability are known to have arisen under natural circumstances, even though, you know, we saw one today that was very contrived. So, yeah, I think it is kind of interesting. And it's also something that it's the definition for stability. It's OK that we have row in there, but however, you know, if row is huge, then this kind of breaks down. Any questions about this? So actually, you say that when we have n equals 60, this lin-alq.lu-solve doesn't give us the proper answer, right? Yeah, but if we don't do this LU factor and we just try to solve it on matrix and vector, it still gives the proper answer. So why would we care about this LU factorization? Sorry, if you just try to solve it, how it gives the proper? Yes, so if we just do like lin-alq.solve without LU and we don't apply this LU factorization, it gives us the proper answer. That's a good question. So it looks like that this factorization just doesn't agree. Oh, wait, did someone else? Tim? Oh, no. OK. Oh, yes, that's a good suggestion. Sorry, I didn't take this out. This is something. This is a good question. I will answer this next time. I want to look into more what scipy lin-alq.solve is calling. You can see from the docs at the end it says it's calling LAPAC, which Rachel mentioned earlier is a particular library. I mean it also says that it's called, oh, those are just for the Hermitian symmetric. No, the person is generic. Generic, OK. So let me check that. Interestingly, according to the LAPAC docs, that does use LU factorization, but it must be doing it in some way. So this is how you would look up LAPAC, the name of a particular method. I want to look into that more because I would have guessed that they were using LU solver. Actually, can you try it just real quick with a higher power, like do like n equals 80 or something? Oh, it also factorizes into a permutation matrix. Yeah, but typically that's just a partial permutation matrix. Oh, it also kind of like the condition number. Oh. So it confuses it with error bounds and it uses iterative refinement. So it does more than. OK. Yeah, because I'm I feel very certain that it's not doing complete pivoting because that's really slow. OK. Yeah. So this is doing something much fancier. Iterative refinement is applied to improve the computed solution matrix. That's a really interesting find that that is working here. And this is often the advantage of using LAPAC, something that's been really, really well optimized. So we're at time, but I'll return to this this question next time. And I think that's otherwise most of what I had to say about LU. Yeah, LU factorization. And this is covered in chapters 20 to 22 of Trevathan.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.0, "text": " All right, I'm going to go ahead and get started.", "tokens": [1057, 558, 11, 286, 478, 516, 281, 352, 2286, 293, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.19490230893625796, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.058281317353248596}, {"id": 1, "seek": 0, "start": 9.0, "end": 15.48, "text": " And I wanted to announce first that I just put in Slack a link to a mid-course feedback", "tokens": [400, 286, 1415, 281, 7478, 700, 300, 286, 445, 829, 294, 37211, 257, 2113, 281, 257, 2062, 12, 31913, 5824], "temperature": 0.0, "avg_logprob": -0.19490230893625796, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.058281317353248596}, {"id": 2, "seek": 0, "start": 15.48, "end": 16.88, "text": " survey.", "tokens": [8984, 13], "temperature": 0.0, "avg_logprob": -0.19490230893625796, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.058281317353248596}, {"id": 3, "seek": 0, "start": 16.88, "end": 21.52, "text": " And this is just for me, and it's anonymous, but it's kind of helpful for me to kind of", "tokens": [400, 341, 307, 445, 337, 385, 11, 293, 309, 311, 24932, 11, 457, 309, 311, 733, 295, 4961, 337, 385, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.19490230893625796, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.058281317353248596}, {"id": 4, "seek": 0, "start": 21.52, "end": 24.72, "text": " get feedback about how things are going and if there are things that it would be helpful", "tokens": [483, 5824, 466, 577, 721, 366, 516, 293, 498, 456, 366, 721, 300, 309, 576, 312, 4961], "temperature": 0.0, "avg_logprob": -0.19490230893625796, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.058281317353248596}, {"id": 5, "seek": 0, "start": 24.72, "end": 25.96, "text": " to adjust.", "tokens": [281, 4369, 13], "temperature": 0.0, "avg_logprob": -0.19490230893625796, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.058281317353248596}, {"id": 6, "seek": 0, "start": 25.96, "end": 29.46, "text": " So please try to fill that out today.", "tokens": [407, 1767, 853, 281, 2836, 300, 484, 965, 13], "temperature": 0.0, "avg_logprob": -0.19490230893625796, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.058281317353248596}, {"id": 7, "seek": 2946, "start": 29.46, "end": 32.64, "text": " And also if you're just auditing the course, I'm still interested in your feedback, so", "tokens": [400, 611, 498, 291, 434, 445, 2379, 1748, 264, 1164, 11, 286, 478, 920, 3102, 294, 428, 5824, 11, 370], "temperature": 0.0, "avg_logprob": -0.24649275266207182, "compression_ratio": 1.5542168674698795, "no_speech_prob": 9.367850907437969e-06}, {"id": 8, "seek": 2946, "start": 32.64, "end": 35.64, "text": " fill it out as well.", "tokens": [2836, 309, 484, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.24649275266207182, "compression_ratio": 1.5542168674698795, "no_speech_prob": 9.367850907437969e-06}, {"id": 9, "seek": 2946, "start": 35.64, "end": 40.84, "text": " Yeah, so I was going to start with some review.", "tokens": [865, 11, 370, 286, 390, 516, 281, 722, 365, 512, 3131, 13], "temperature": 0.0, "avg_logprob": -0.24649275266207182, "compression_ratio": 1.5542168674698795, "no_speech_prob": 9.367850907437969e-06}, {"id": 10, "seek": 2946, "start": 40.84, "end": 47.72, "text": " We're going to talk about the robust PCA algorithm we saw last time and kind of look at a little", "tokens": [492, 434, 516, 281, 751, 466, 264, 13956, 6465, 32, 9284, 321, 1866, 1036, 565, 293, 733, 295, 574, 412, 257, 707], "temperature": 0.0, "avg_logprob": -0.24649275266207182, "compression_ratio": 1.5542168674698795, "no_speech_prob": 9.367850907437969e-06}, {"id": 11, "seek": 2946, "start": 47.72, "end": 50.84, "text": " bit of some different information about it from how we covered it before.", "tokens": [857, 295, 512, 819, 1589, 466, 309, 490, 577, 321, 5343, 309, 949, 13], "temperature": 0.0, "avg_logprob": -0.24649275266207182, "compression_ratio": 1.5542168674698795, "no_speech_prob": 9.367850907437969e-06}, {"id": 12, "seek": 2946, "start": 50.84, "end": 56.480000000000004, "text": " But I wanted to start with just a reminder, what is the SVD?", "tokens": [583, 286, 1415, 281, 722, 365, 445, 257, 13548, 11, 437, 307, 264, 31910, 35, 30], "temperature": 0.0, "avg_logprob": -0.24649275266207182, "compression_ratio": 1.5542168674698795, "no_speech_prob": 9.367850907437969e-06}, {"id": 13, "seek": 5648, "start": 56.48, "end": 59.839999999999996, "text": " Jeremy, microphone.", "tokens": [17809, 11, 10952, 13], "temperature": 0.0, "avg_logprob": -0.6470462062902618, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00016854779096320271}, {"id": 14, "seek": 5648, "start": 59.839999999999996, "end": 61.4, "text": " Anyone?", "tokens": [14643, 30], "temperature": 0.0, "avg_logprob": -0.6470462062902618, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00016854779096320271}, {"id": 15, "seek": 5648, "start": 61.4, "end": 62.839999999999996, "text": " SVD?", "tokens": [31910, 35, 30], "temperature": 0.0, "avg_logprob": -0.6470462062902618, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00016854779096320271}, {"id": 16, "seek": 5648, "start": 62.839999999999996, "end": 66.88, "text": " Single-valued, singular-valued decomposition.", "tokens": [31248, 12, 3337, 5827, 11, 20010, 12, 3337, 5827, 48356, 13], "temperature": 0.0, "avg_logprob": -0.6470462062902618, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00016854779096320271}, {"id": 17, "seek": 5648, "start": 66.88, "end": 67.88, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.6470462062902618, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00016854779096320271}, {"id": 18, "seek": 5648, "start": 67.88, "end": 82.36, "text": " It's just a more perfect decomposition of a matrix and the singular value of the diagonal", "tokens": [467, 311, 445, 257, 544, 2176, 48356, 295, 257, 8141, 293, 264, 20010, 2158, 295, 264, 21539], "temperature": 0.0, "avg_logprob": -0.6470462062902618, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00016854779096320271}, {"id": 19, "seek": 5648, "start": 82.36, "end": 83.36, "text": " matrix.", "tokens": [8141, 13], "temperature": 0.0, "avg_logprob": -0.6470462062902618, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00016854779096320271}, {"id": 20, "seek": 8336, "start": 83.36, "end": 86.76, "text": " And then, yeah, take it.", "tokens": [400, 550, 11, 1338, 11, 747, 309, 13], "temperature": 0.0, "avg_logprob": -0.36870350557215076, "compression_ratio": 1.6235294117647059, "no_speech_prob": 3.534851930453442e-05}, {"id": 21, "seek": 8336, "start": 86.76, "end": 89.76, "text": " And what can you say about the UNV matrices?", "tokens": [400, 437, 393, 291, 584, 466, 264, 8229, 53, 32284, 30], "temperature": 0.0, "avg_logprob": -0.36870350557215076, "compression_ratio": 1.6235294117647059, "no_speech_prob": 3.534851930453442e-05}, {"id": 22, "seek": 8336, "start": 89.76, "end": 96.32, "text": " Oh, well, I think they can represent different things, but it's like topic over word.", "tokens": [876, 11, 731, 11, 286, 519, 436, 393, 2906, 819, 721, 11, 457, 309, 311, 411, 4829, 670, 1349, 13], "temperature": 0.0, "avg_logprob": -0.36870350557215076, "compression_ratio": 1.6235294117647059, "no_speech_prob": 3.534851930453442e-05}, {"id": 23, "seek": 8336, "start": 96.32, "end": 98.32, "text": " That's true for topic modeling, yeah.", "tokens": [663, 311, 2074, 337, 4829, 15983, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.36870350557215076, "compression_ratio": 1.6235294117647059, "no_speech_prob": 3.534851930453442e-05}, {"id": 24, "seek": 8336, "start": 98.32, "end": 103.96000000000001, "text": " And there's, was it the left singular and the right singular components of it?", "tokens": [400, 456, 311, 11, 390, 309, 264, 1411, 20010, 293, 264, 558, 20010, 6677, 295, 309, 30], "temperature": 0.0, "avg_logprob": -0.36870350557215076, "compression_ratio": 1.6235294117647059, "no_speech_prob": 3.534851930453442e-05}, {"id": 25, "seek": 8336, "start": 103.96000000000001, "end": 106.08, "text": " Yes, those are those names.", "tokens": [1079, 11, 729, 366, 729, 5288, 13], "temperature": 0.0, "avg_logprob": -0.36870350557215076, "compression_ratio": 1.6235294117647059, "no_speech_prob": 3.534851930453442e-05}, {"id": 26, "seek": 8336, "start": 106.08, "end": 109.12, "text": " I was looking for one other property, although everything you've said is true.", "tokens": [286, 390, 1237, 337, 472, 661, 4707, 11, 4878, 1203, 291, 600, 848, 307, 2074, 13], "temperature": 0.0, "avg_logprob": -0.36870350557215076, "compression_ratio": 1.6235294117647059, "no_speech_prob": 3.534851930453442e-05}, {"id": 27, "seek": 8336, "start": 109.12, "end": 110.12, "text": " The orthogonal?", "tokens": [440, 41488, 30], "temperature": 0.0, "avg_logprob": -0.36870350557215076, "compression_ratio": 1.6235294117647059, "no_speech_prob": 3.534851930453442e-05}, {"id": 28, "seek": 8336, "start": 110.12, "end": 111.12, "text": " Yes, yeah.", "tokens": [1079, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.36870350557215076, "compression_ratio": 1.6235294117647059, "no_speech_prob": 3.534851930453442e-05}, {"id": 29, "seek": 8336, "start": 111.12, "end": 112.12, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.36870350557215076, "compression_ratio": 1.6235294117647059, "no_speech_prob": 3.534851930453442e-05}, {"id": 30, "seek": 11212, "start": 112.12, "end": 131.64000000000001, "text": " And then what is the truncated SVD?", "tokens": [400, 550, 437, 307, 264, 504, 409, 66, 770, 31910, 35, 30], "temperature": 0.0, "avg_logprob": -0.26901124448192365, "compression_ratio": 1.3214285714285714, "no_speech_prob": 2.078419493045658e-05}, {"id": 31, "seek": 11212, "start": 131.64000000000001, "end": 132.64000000000001, "text": " Truncated SVD.", "tokens": [1765, 409, 66, 770, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.26901124448192365, "compression_ratio": 1.3214285714285714, "no_speech_prob": 2.078419493045658e-05}, {"id": 32, "seek": 11212, "start": 132.64000000000001, "end": 137.72, "text": " How do you get from the full SVD to the truncated SVD?", "tokens": [1012, 360, 291, 483, 490, 264, 1577, 31910, 35, 281, 264, 504, 409, 66, 770, 31910, 35, 30], "temperature": 0.0, "avg_logprob": -0.26901124448192365, "compression_ratio": 1.3214285714285714, "no_speech_prob": 2.078419493045658e-05}, {"id": 33, "seek": 11212, "start": 137.72, "end": 139.72, "text": " Brad?", "tokens": [11895, 30], "temperature": 0.0, "avg_logprob": -0.26901124448192365, "compression_ratio": 1.3214285714285714, "no_speech_prob": 2.078419493045658e-05}, {"id": 34, "seek": 13972, "start": 139.72, "end": 145.44, "text": " And be sure to hold the microphone close to your mouth.", "tokens": [400, 312, 988, 281, 1797, 264, 10952, 1998, 281, 428, 4525, 13], "temperature": 0.0, "avg_logprob": -0.2388206335214468, "compression_ratio": 1.4074074074074074, "no_speech_prob": 5.0932890189869795e-06}, {"id": 35, "seek": 13972, "start": 145.44, "end": 146.44, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.2388206335214468, "compression_ratio": 1.4074074074074074, "no_speech_prob": 5.0932890189869795e-06}, {"id": 36, "seek": 13972, "start": 146.44, "end": 156.28, "text": " So, truncated SVD is simply, at least I think it's a lower rank approximation.", "tokens": [407, 11, 504, 409, 66, 770, 31910, 35, 307, 2935, 11, 412, 1935, 286, 519, 309, 311, 257, 3126, 6181, 28023, 13], "temperature": 0.0, "avg_logprob": -0.2388206335214468, "compression_ratio": 1.4074074074074074, "no_speech_prob": 5.0932890189869795e-06}, {"id": 37, "seek": 13972, "start": 156.28, "end": 161.4, "text": " And I think that in the context of what we were talking about last class, we're talking", "tokens": [400, 286, 519, 300, 294, 264, 4319, 295, 437, 321, 645, 1417, 466, 1036, 1508, 11, 321, 434, 1417], "temperature": 0.0, "avg_logprob": -0.2388206335214468, "compression_ratio": 1.4074074074074074, "no_speech_prob": 5.0932890189869795e-06}, {"id": 38, "seek": 16140, "start": 161.4, "end": 173.68, "text": " about a sort of random, I'm trying to think of the exact terminology, but using a smaller", "tokens": [466, 257, 1333, 295, 4974, 11, 286, 478, 1382, 281, 519, 295, 264, 1900, 27575, 11, 457, 1228, 257, 4356], "temperature": 0.0, "avg_logprob": -0.17859723944413033, "compression_ratio": 1.5534883720930233, "no_speech_prob": 1.8057289707940072e-05}, {"id": 39, "seek": 16140, "start": 173.68, "end": 178.36, "text": " matrix and then factorizing that.", "tokens": [8141, 293, 550, 5952, 3319, 300, 13], "temperature": 0.0, "avg_logprob": -0.17859723944413033, "compression_ratio": 1.5534883720930233, "no_speech_prob": 1.8057289707940072e-05}, {"id": 40, "seek": 16140, "start": 178.36, "end": 179.36, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.17859723944413033, "compression_ratio": 1.5534883720930233, "no_speech_prob": 1.8057289707940072e-05}, {"id": 41, "seek": 16140, "start": 179.36, "end": 180.36, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.17859723944413033, "compression_ratio": 1.5534883720930233, "no_speech_prob": 1.8057289707940072e-05}, {"id": 42, "seek": 16140, "start": 180.36, "end": 183.68, "text": " And then, or it could be like something like a lower rank approximation or something like", "tokens": [400, 550, 11, 420, 309, 727, 312, 411, 746, 411, 257, 3126, 6181, 28023, 420, 746, 411], "temperature": 0.0, "avg_logprob": -0.17859723944413033, "compression_ratio": 1.5534883720930233, "no_speech_prob": 1.8057289707940072e-05}, {"id": 43, "seek": 16140, "start": 183.68, "end": 184.68, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.17859723944413033, "compression_ratio": 1.5534883720930233, "no_speech_prob": 1.8057289707940072e-05}, {"id": 44, "seek": 16140, "start": 184.68, "end": 185.68, "text": " Yeah, yeah.", "tokens": [865, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.17859723944413033, "compression_ratio": 1.5534883720930233, "no_speech_prob": 1.8057289707940072e-05}, {"id": 45, "seek": 16140, "start": 185.68, "end": 188.84, "text": " So typically you'll think of the truncated SVD as kind of taking the full SVD and chopping", "tokens": [407, 5850, 291, 603, 519, 295, 264, 504, 409, 66, 770, 31910, 35, 382, 733, 295, 1940, 264, 1577, 31910, 35, 293, 35205], "temperature": 0.0, "avg_logprob": -0.17859723944413033, "compression_ratio": 1.5534883720930233, "no_speech_prob": 1.8057289707940072e-05}, {"id": 46, "seek": 18884, "start": 188.84, "end": 197.36, "text": " off however many kind of singular values slash columns of U slash rows of V to get something", "tokens": [766, 4461, 867, 733, 295, 20010, 4190, 17330, 13766, 295, 624, 17330, 13241, 295, 691, 281, 483, 746], "temperature": 0.0, "avg_logprob": -0.1368291322575059, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.1111545720486902e-05}, {"id": 47, "seek": 18884, "start": 197.36, "end": 199.72, "text": " smaller, which is good for data compression.", "tokens": [4356, 11, 597, 307, 665, 337, 1412, 19355, 13], "temperature": 0.0, "avg_logprob": -0.1368291322575059, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.1111545720486902e-05}, {"id": 48, "seek": 18884, "start": 199.72, "end": 204.52, "text": " And then Brad brought up last time we were looking at this randomized approximation where", "tokens": [400, 550, 11895, 3038, 493, 1036, 565, 321, 645, 1237, 412, 341, 38513, 28023, 689], "temperature": 0.0, "avg_logprob": -0.1368291322575059, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.1111545720486902e-05}, {"id": 49, "seek": 18884, "start": 204.52, "end": 211.4, "text": " we were kind of taking a different version of the same matrix by getting a random projection", "tokens": [321, 645, 733, 295, 1940, 257, 819, 3037, 295, 264, 912, 8141, 538, 1242, 257, 4974, 22743], "temperature": 0.0, "avg_logprob": -0.1368291322575059, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.1111545720486902e-05}, {"id": 50, "seek": 18884, "start": 211.4, "end": 216.8, "text": " and it had fewer columns but the same or approximately the same column space.", "tokens": [293, 309, 632, 13366, 13766, 457, 264, 912, 420, 10447, 264, 912, 7738, 1901, 13], "temperature": 0.0, "avg_logprob": -0.1368291322575059, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.1111545720486902e-05}, {"id": 51, "seek": 21680, "start": 216.8, "end": 222.76000000000002, "text": " And then taking the full SVD of that smaller matrix and letting that be kind of the truncated", "tokens": [400, 550, 1940, 264, 1577, 31910, 35, 295, 300, 4356, 8141, 293, 8295, 300, 312, 733, 295, 264, 504, 409, 66, 770], "temperature": 0.0, "avg_logprob": -0.22877208078940084, "compression_ratio": 1.768, "no_speech_prob": 5.649551530950703e-05}, {"id": 52, "seek": 21680, "start": 222.76000000000002, "end": 224.20000000000002, "text": " SVD of the full matrix.", "tokens": [31910, 35, 295, 264, 1577, 8141, 13], "temperature": 0.0, "avg_logprob": -0.22877208078940084, "compression_ratio": 1.768, "no_speech_prob": 5.649551530950703e-05}, {"id": 53, "seek": 21680, "start": 224.20000000000002, "end": 229.48000000000002, "text": " It's worth mentioning that the particular randomized approach, Rachel taught, isn't", "tokens": [467, 311, 3163, 18315, 300, 264, 1729, 38513, 3109, 11, 14246, 5928, 11, 1943, 380], "temperature": 0.0, "avg_logprob": -0.22877208078940084, "compression_ratio": 1.768, "no_speech_prob": 5.649551530950703e-05}, {"id": 54, "seek": 21680, "start": 229.48000000000002, "end": 231.48000000000002, "text": " the only way to calculate truncated SVD.", "tokens": [264, 787, 636, 281, 8873, 504, 409, 66, 770, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.22877208078940084, "compression_ratio": 1.768, "no_speech_prob": 5.649551530950703e-05}, {"id": 55, "seek": 21680, "start": 231.48000000000002, "end": 236.28, "text": " So if you just did a full SVD and literally threw away a bunch of the last columns and", "tokens": [407, 498, 291, 445, 630, 257, 1577, 31910, 35, 293, 3736, 11918, 1314, 257, 3840, 295, 264, 1036, 13766, 293], "temperature": 0.0, "avg_logprob": -0.22877208078940084, "compression_ratio": 1.768, "no_speech_prob": 5.649551530950703e-05}, {"id": 56, "seek": 21680, "start": 236.28, "end": 237.28, "text": " rows, that's still truncated SVD.", "tokens": [13241, 11, 300, 311, 920, 504, 409, 66, 770, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.22877208078940084, "compression_ratio": 1.768, "no_speech_prob": 5.649551530950703e-05}, {"id": 57, "seek": 21680, "start": 237.28, "end": 238.28, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.22877208078940084, "compression_ratio": 1.768, "no_speech_prob": 5.649551530950703e-05}, {"id": 58, "seek": 21680, "start": 238.28, "end": 239.28, "text": " It's an inefficient way to do it.", "tokens": [467, 311, 364, 43495, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.22877208078940084, "compression_ratio": 1.768, "no_speech_prob": 5.649551530950703e-05}, {"id": 59, "seek": 21680, "start": 239.28, "end": 243.28, "text": " There are other ways to calculate it.", "tokens": [821, 366, 661, 2098, 281, 8873, 309, 13], "temperature": 0.0, "avg_logprob": -0.22877208078940084, "compression_ratio": 1.768, "no_speech_prob": 5.649551530950703e-05}, {"id": 60, "seek": 24328, "start": 243.28, "end": 250.8, "text": " But the randomized approach is a very good way of calculating the truncated SVD.", "tokens": [583, 264, 38513, 3109, 307, 257, 588, 665, 636, 295, 28258, 264, 504, 409, 66, 770, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.22949961493997012, "compression_ratio": 1.5507246376811594, "no_speech_prob": 3.6119274682278046e-06}, {"id": 61, "seek": 24328, "start": 250.8, "end": 251.8, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22949961493997012, "compression_ratio": 1.5507246376811594, "no_speech_prob": 3.6119274682278046e-06}, {"id": 62, "seek": 24328, "start": 251.8, "end": 252.8, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22949961493997012, "compression_ratio": 1.5507246376811594, "no_speech_prob": 3.6119274682278046e-06}, {"id": 63, "seek": 24328, "start": 252.8, "end": 257.16, "text": " Oh, actually Brad's got another question or comment.", "tokens": [876, 11, 767, 11895, 311, 658, 1071, 1168, 420, 2871, 13], "temperature": 0.0, "avg_logprob": -0.22949961493997012, "compression_ratio": 1.5507246376811594, "no_speech_prob": 3.6119274682278046e-06}, {"id": 64, "seek": 24328, "start": 257.16, "end": 262.64, "text": " So I know last week I sort of mentioned that in the approach where you actually calculate", "tokens": [407, 286, 458, 1036, 1243, 286, 1333, 295, 2835, 300, 294, 264, 3109, 689, 291, 767, 8873], "temperature": 0.0, "avg_logprob": -0.22949961493997012, "compression_ratio": 1.5507246376811594, "no_speech_prob": 3.6119274682278046e-06}, {"id": 65, "seek": 24328, "start": 262.64, "end": 272.08, "text": " the full SVD and then you top off the last and or so smallest singular values, I know", "tokens": [264, 1577, 31910, 35, 293, 550, 291, 1192, 766, 264, 1036, 293, 420, 370, 16998, 20010, 4190, 11, 286, 458], "temperature": 0.0, "avg_logprob": -0.22949961493997012, "compression_ratio": 1.5507246376811594, "no_speech_prob": 3.6119274682278046e-06}, {"id": 66, "seek": 27208, "start": 272.08, "end": 276.44, "text": " that that is the best low rank approximation.", "tokens": [300, 300, 307, 264, 1151, 2295, 6181, 28023, 13], "temperature": 0.0, "avg_logprob": -0.2441088936545632, "compression_ratio": 1.771551724137931, "no_speech_prob": 3.5348803066881374e-05}, {"id": 67, "seek": 27208, "start": 276.44, "end": 277.44, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.2441088936545632, "compression_ratio": 1.771551724137931, "no_speech_prob": 3.5348803066881374e-05}, {"id": 68, "seek": 27208, "start": 277.44, "end": 284.84, "text": " How well does the random approximation compare to that?", "tokens": [1012, 731, 775, 264, 4974, 28023, 6794, 281, 300, 30], "temperature": 0.0, "avg_logprob": -0.2441088936545632, "compression_ratio": 1.771551724137931, "no_speech_prob": 3.5348803066881374e-05}, {"id": 69, "seek": 27208, "start": 284.84, "end": 287.52, "text": " I mean, I know that obviously that's not an efficient way of doing it because you have", "tokens": [286, 914, 11, 286, 458, 300, 2745, 300, 311, 406, 364, 7148, 636, 295, 884, 309, 570, 291, 362], "temperature": 0.0, "avg_logprob": -0.2441088936545632, "compression_ratio": 1.771551724137931, "no_speech_prob": 3.5348803066881374e-05}, {"id": 70, "seek": 27208, "start": 287.52, "end": 290.56, "text": " to do the whole thing to start with.", "tokens": [281, 360, 264, 1379, 551, 281, 722, 365, 13], "temperature": 0.0, "avg_logprob": -0.2441088936545632, "compression_ratio": 1.771551724137931, "no_speech_prob": 3.5348803066881374e-05}, {"id": 71, "seek": 27208, "start": 290.56, "end": 293.96, "text": " But the point of the random is that you don't have to do the whole thing.", "tokens": [583, 264, 935, 295, 264, 4974, 307, 300, 291, 500, 380, 362, 281, 360, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.2441088936545632, "compression_ratio": 1.771551724137931, "no_speech_prob": 3.5348803066881374e-05}, {"id": 72, "seek": 27208, "start": 293.96, "end": 294.96, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.2441088936545632, "compression_ratio": 1.771551724137931, "no_speech_prob": 3.5348803066881374e-05}, {"id": 73, "seek": 27208, "start": 294.96, "end": 301.2, "text": " So is there any sort of intuition of how well that does, how good of a job that does approximating?", "tokens": [407, 307, 456, 604, 1333, 295, 24002, 295, 577, 731, 300, 775, 11, 577, 665, 295, 257, 1691, 300, 775, 8542, 990, 30], "temperature": 0.0, "avg_logprob": -0.2441088936545632, "compression_ratio": 1.771551724137931, "no_speech_prob": 3.5348803066881374e-05}, {"id": 74, "seek": 30120, "start": 301.2, "end": 305.59999999999997, "text": " So I think in general it does a very good job.", "tokens": [407, 286, 519, 294, 2674, 309, 775, 257, 588, 665, 1691, 13], "temperature": 0.0, "avg_logprob": -0.2689732261326002, "compression_ratio": 1.6824324324324325, "no_speech_prob": 1.5206588614091743e-05}, {"id": 75, "seek": 30120, "start": 305.59999999999997, "end": 309.2, "text": " I'd have to look back at the paper because I think the paper does give guarantees on", "tokens": [286, 1116, 362, 281, 574, 646, 412, 264, 3035, 570, 286, 519, 264, 3035, 775, 976, 32567, 322], "temperature": 0.0, "avg_logprob": -0.2689732261326002, "compression_ratio": 1.6824324324324325, "no_speech_prob": 1.5206588614091743e-05}, {"id": 76, "seek": 30120, "start": 309.2, "end": 310.64, "text": " how good it is.", "tokens": [577, 665, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.2689732261326002, "compression_ratio": 1.6824324324324325, "no_speech_prob": 1.5206588614091743e-05}, {"id": 77, "seek": 30120, "start": 310.64, "end": 311.64, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2689732261326002, "compression_ratio": 1.6824324324324325, "no_speech_prob": 1.5206588614091743e-05}, {"id": 78, "seek": 30120, "start": 311.64, "end": 316.2, "text": " There's actually a parameter you can choose how good it is, which is that, what was it", "tokens": [821, 311, 767, 257, 13075, 291, 393, 2826, 577, 665, 309, 307, 11, 597, 307, 300, 11, 437, 390, 309], "temperature": 0.0, "avg_logprob": -0.2689732261326002, "compression_ratio": 1.6824324324324325, "no_speech_prob": 1.5206588614091743e-05}, {"id": 79, "seek": 30120, "start": 316.2, "end": 319.64, "text": " called, the thing where you like add more components than you need?", "tokens": [1219, 11, 264, 551, 689, 291, 411, 909, 544, 6677, 813, 291, 643, 30], "temperature": 0.0, "avg_logprob": -0.2689732261326002, "compression_ratio": 1.6824324324324325, "no_speech_prob": 1.5206588614091743e-05}, {"id": 80, "seek": 30120, "start": 319.64, "end": 324.48, "text": " Yeah, I was kind of referring to that as buffer, but yeah, we have a different name for it.", "tokens": [865, 11, 286, 390, 733, 295, 13761, 281, 300, 382, 21762, 11, 457, 1338, 11, 321, 362, 257, 819, 1315, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.2689732261326002, "compression_ratio": 1.6824324324324325, "no_speech_prob": 1.5206588614091743e-05}, {"id": 81, "seek": 30120, "start": 324.48, "end": 328.4, "text": " Like you say, I want five, but you actually calculate it with 15 and then get rid of the", "tokens": [1743, 291, 584, 11, 286, 528, 1732, 11, 457, 291, 767, 8873, 309, 365, 2119, 293, 550, 483, 3973, 295, 264], "temperature": 0.0, "avg_logprob": -0.2689732261326002, "compression_ratio": 1.6824324324324325, "no_speech_prob": 1.5206588614091743e-05}, {"id": 82, "seek": 30120, "start": 328.4, "end": 329.4, "text": " last 10.", "tokens": [1036, 1266, 13], "temperature": 0.0, "avg_logprob": -0.2689732261326002, "compression_ratio": 1.6824324324324325, "no_speech_prob": 1.5206588614091743e-05}, {"id": 83, "seek": 32940, "start": 329.4, "end": 332.67999999999995, "text": " So if you made that parameter big enough that it was the whole thing, it's perfect.", "tokens": [407, 498, 291, 1027, 300, 13075, 955, 1547, 300, 309, 390, 264, 1379, 551, 11, 309, 311, 2176, 13], "temperature": 0.0, "avg_logprob": -0.32108131408691404, "compression_ratio": 1.6742671009771988, "no_speech_prob": 3.3930853078345535e-06}, {"id": 84, "seek": 32940, "start": 332.67999999999995, "end": 338.23999999999995, "text": " If you make it so small that it's like a zero, that it's perfectly perfect, so you can pick.", "tokens": [759, 291, 652, 309, 370, 1359, 300, 309, 311, 411, 257, 4018, 11, 300, 309, 311, 6239, 2176, 11, 370, 291, 393, 1888, 13], "temperature": 0.0, "avg_logprob": -0.32108131408691404, "compression_ratio": 1.6742671009771988, "no_speech_prob": 3.3930853078345535e-06}, {"id": 85, "seek": 32940, "start": 338.23999999999995, "end": 344.32, "text": " But even with like 10 extra, it's going to be like 10 to the negative of something really", "tokens": [583, 754, 365, 411, 1266, 2857, 11, 309, 311, 516, 281, 312, 411, 1266, 281, 264, 3671, 295, 746, 534], "temperature": 0.0, "avg_logprob": -0.32108131408691404, "compression_ratio": 1.6742671009771988, "no_speech_prob": 3.3930853078345535e-06}, {"id": 86, "seek": 32940, "start": 344.32, "end": 346.67999999999995, "text": " small, close-ness when you look, right?", "tokens": [1359, 11, 1998, 12, 1287, 562, 291, 574, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.32108131408691404, "compression_ratio": 1.6742671009771988, "no_speech_prob": 3.3930853078345535e-06}, {"id": 87, "seek": 32940, "start": 346.67999999999995, "end": 347.67999999999995, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.32108131408691404, "compression_ratio": 1.6742671009771988, "no_speech_prob": 3.3930853078345535e-06}, {"id": 88, "seek": 32940, "start": 347.67999999999995, "end": 352.15999999999997, "text": " Yeah, the paper, I'd have to look back to it, though, like recommends either 10 or 15", "tokens": [865, 11, 264, 3035, 11, 286, 1116, 362, 281, 574, 646, 281, 309, 11, 1673, 11, 411, 34556, 2139, 1266, 420, 2119], "temperature": 0.0, "avg_logprob": -0.32108131408691404, "compression_ratio": 1.6742671009771988, "no_speech_prob": 3.3930853078345535e-06}, {"id": 89, "seek": 32940, "start": 352.15999999999997, "end": 353.15999999999997, "text": " when you're doing this in practice.", "tokens": [562, 291, 434, 884, 341, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.32108131408691404, "compression_ratio": 1.6742671009771988, "no_speech_prob": 3.3930853078345535e-06}, {"id": 90, "seek": 32940, "start": 353.15999999999997, "end": 354.15999999999997, "text": " I'm sorry.", "tokens": [286, 478, 2597, 13], "temperature": 0.0, "avg_logprob": -0.32108131408691404, "compression_ratio": 1.6742671009771988, "no_speech_prob": 3.3930853078345535e-06}, {"id": 91, "seek": 32940, "start": 354.15999999999997, "end": 355.15999999999997, "text": " Okay, one more.", "tokens": [1033, 11, 472, 544, 13], "temperature": 0.0, "avg_logprob": -0.32108131408691404, "compression_ratio": 1.6742671009771988, "no_speech_prob": 3.3930853078345535e-06}, {"id": 92, "seek": 32940, "start": 355.15999999999997, "end": 358.15999999999997, "text": " Oh, wait, Jeremy, throw the microphone back, please.", "tokens": [876, 11, 1699, 11, 17809, 11, 3507, 264, 10952, 646, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.32108131408691404, "compression_ratio": 1.6742671009771988, "no_speech_prob": 3.3930853078345535e-06}, {"id": 93, "seek": 35816, "start": 358.16, "end": 364.8, "text": " And so just one last thing about the buffer.", "tokens": [400, 370, 445, 472, 1036, 551, 466, 264, 21762, 13], "temperature": 0.0, "avg_logprob": -0.2127188273838588, "compression_ratio": 1.625, "no_speech_prob": 2.225254775112262e-06}, {"id": 94, "seek": 35816, "start": 364.8, "end": 369.20000000000005, "text": " So is the intuition behind that, like if you only want the top five and you have these", "tokens": [407, 307, 264, 24002, 2261, 300, 11, 411, 498, 291, 787, 528, 264, 1192, 1732, 293, 291, 362, 613], "temperature": 0.0, "avg_logprob": -0.2127188273838588, "compression_ratio": 1.625, "no_speech_prob": 2.225254775112262e-06}, {"id": 95, "seek": 35816, "start": 369.20000000000005, "end": 377.88, "text": " extra vectors, is that sort of like a place to dump what you don't?", "tokens": [2857, 18875, 11, 307, 300, 1333, 295, 411, 257, 1081, 281, 11430, 437, 291, 500, 380, 30], "temperature": 0.0, "avg_logprob": -0.2127188273838588, "compression_ratio": 1.625, "no_speech_prob": 2.225254775112262e-06}, {"id": 96, "seek": 35816, "start": 377.88, "end": 385.08000000000004, "text": " Like if you only took five, then you're kind of squeezing all the bad stuff into sort of", "tokens": [1743, 498, 291, 787, 1890, 1732, 11, 550, 291, 434, 733, 295, 36645, 439, 264, 1578, 1507, 666, 1333, 295], "temperature": 0.0, "avg_logprob": -0.2127188273838588, "compression_ratio": 1.625, "no_speech_prob": 2.225254775112262e-06}, {"id": 97, "seek": 35816, "start": 385.08000000000004, "end": 386.08000000000004, "text": " the ones that you want.", "tokens": [264, 2306, 300, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.2127188273838588, "compression_ratio": 1.625, "no_speech_prob": 2.225254775112262e-06}, {"id": 98, "seek": 38608, "start": 386.08, "end": 390.12, "text": " Because if you have a buffer, it's like the...", "tokens": [1436, 498, 291, 362, 257, 21762, 11, 309, 311, 411, 264, 485], "temperature": 0.0, "avg_logprob": -0.3717694349691901, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.4110548666794784e-06}, {"id": 99, "seek": 38608, "start": 390.12, "end": 397.88, "text": " If you want to have the best five, you need somewhere to put the not good stuff.", "tokens": [759, 291, 528, 281, 362, 264, 1151, 1732, 11, 291, 643, 4079, 281, 829, 264, 406, 665, 1507, 13], "temperature": 0.0, "avg_logprob": -0.3717694349691901, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.4110548666794784e-06}, {"id": 100, "seek": 38608, "start": 397.88, "end": 399.91999999999996, "text": " I'm kind of talking really vaguely.", "tokens": [286, 478, 733, 295, 1417, 534, 13501, 48863, 13], "temperature": 0.0, "avg_logprob": -0.3717694349691901, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.4110548666794784e-06}, {"id": 101, "seek": 38608, "start": 399.91999999999996, "end": 400.91999999999996, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3717694349691901, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.4110548666794784e-06}, {"id": 102, "seek": 38608, "start": 400.91999999999996, "end": 403.03999999999996, "text": " I think I would say that...", "tokens": [286, 519, 286, 576, 584, 300, 485], "temperature": 0.0, "avg_logprob": -0.3717694349691901, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.4110548666794784e-06}, {"id": 103, "seek": 38608, "start": 403.03999999999996, "end": 408.24, "text": " I actually got it on full screen.", "tokens": [286, 767, 658, 309, 322, 1577, 2568, 13], "temperature": 0.0, "avg_logprob": -0.3717694349691901, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.4110548666794784e-06}, {"id": 104, "seek": 40824, "start": 408.24, "end": 416.84000000000003, "text": " What that number is referring to is how many columns you're taking in your randomized approximation", "tokens": [708, 300, 1230, 307, 13761, 281, 307, 577, 867, 13766, 291, 434, 1940, 294, 428, 38513, 28023], "temperature": 0.0, "avg_logprob": -0.3375308478056495, "compression_ratio": 1.349112426035503, "no_speech_prob": 4.331474701757543e-05}, {"id": 105, "seek": 40824, "start": 416.84000000000003, "end": 418.84000000000003, "text": " of the column space.", "tokens": [295, 264, 7738, 1901, 13], "temperature": 0.0, "avg_logprob": -0.3375308478056495, "compression_ratio": 1.349112426035503, "no_speech_prob": 4.331474701757543e-05}, {"id": 106, "seek": 40824, "start": 418.84000000000003, "end": 423.52, "text": " Jeremy, how do I exit full screen mode in Metamoji?", "tokens": [17809, 11, 577, 360, 286, 11043, 1577, 2568, 4391, 294, 6377, 10502, 4013, 30], "temperature": 0.0, "avg_logprob": -0.3375308478056495, "compression_ratio": 1.349112426035503, "no_speech_prob": 4.331474701757543e-05}, {"id": 107, "seek": 40824, "start": 423.52, "end": 429.28000000000003, "text": " Well, I want to get a new sheet here.", "tokens": [1042, 11, 286, 528, 281, 483, 257, 777, 8193, 510, 13], "temperature": 0.0, "avg_logprob": -0.3375308478056495, "compression_ratio": 1.349112426035503, "no_speech_prob": 4.331474701757543e-05}, {"id": 108, "seek": 40824, "start": 429.28000000000003, "end": 431.28000000000003, "text": " Okay, thanks.", "tokens": [1033, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.3375308478056495, "compression_ratio": 1.349112426035503, "no_speech_prob": 4.331474701757543e-05}, {"id": 109, "seek": 40824, "start": 431.28000000000003, "end": 432.28000000000003, "text": " Oh.", "tokens": [876, 13], "temperature": 0.0, "avg_logprob": -0.3375308478056495, "compression_ratio": 1.349112426035503, "no_speech_prob": 4.331474701757543e-05}, {"id": 110, "seek": 43228, "start": 432.28, "end": 440.91999999999996, "text": " There, thanks.", "tokens": [821, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.2786592124164968, "compression_ratio": 1.4047619047619047, "no_speech_prob": 1.816173835322843e-06}, {"id": 111, "seek": 43228, "start": 440.91999999999996, "end": 448.03999999999996, "text": " I was going to say, so yeah, so what those numbers are referring to is so like if...", "tokens": [286, 390, 516, 281, 584, 11, 370, 1338, 11, 370, 437, 729, 3547, 366, 13761, 281, 307, 370, 411, 498, 485], "temperature": 0.0, "avg_logprob": -0.2786592124164968, "compression_ratio": 1.4047619047619047, "no_speech_prob": 1.816173835322843e-06}, {"id": 112, "seek": 43228, "start": 448.03999999999996, "end": 454.08, "text": " This is like your wide matrix A that's too big to decompose.", "tokens": [639, 307, 411, 428, 4874, 8141, 316, 300, 311, 886, 955, 281, 22867, 541, 13], "temperature": 0.0, "avg_logprob": -0.2786592124164968, "compression_ratio": 1.4047619047619047, "no_speech_prob": 1.816173835322843e-06}, {"id": 113, "seek": 43228, "start": 454.08, "end": 459.4, "text": " You're kind of deciding, okay, how many columns do I want to get over here?", "tokens": [509, 434, 733, 295, 17990, 11, 1392, 11, 577, 867, 13766, 360, 286, 528, 281, 483, 670, 510, 30], "temperature": 0.0, "avg_logprob": -0.2786592124164968, "compression_ratio": 1.4047619047619047, "no_speech_prob": 1.816173835322843e-06}, {"id": 114, "seek": 45940, "start": 459.4, "end": 462.88, "text": " I see it as more kind of guaranteeing, like your goal is that you want the column space", "tokens": [286, 536, 309, 382, 544, 733, 295, 10815, 278, 11, 411, 428, 3387, 307, 300, 291, 528, 264, 7738, 1901], "temperature": 0.0, "avg_logprob": -0.12371276987010035, "compression_ratio": 1.8538812785388128, "no_speech_prob": 1.9524828530848026e-05}, {"id": 115, "seek": 45940, "start": 462.88, "end": 468.67999999999995, "text": " of A and the column space of B to be the same thing, but kind of by adding some extra ones", "tokens": [295, 316, 293, 264, 7738, 1901, 295, 363, 281, 312, 264, 912, 551, 11, 457, 733, 295, 538, 5127, 512, 2857, 2306], "temperature": 0.0, "avg_logprob": -0.12371276987010035, "compression_ratio": 1.8538812785388128, "no_speech_prob": 1.9524828530848026e-05}, {"id": 116, "seek": 45940, "start": 468.67999999999995, "end": 471.0, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.12371276987010035, "compression_ratio": 1.8538812785388128, "no_speech_prob": 1.9524828530848026e-05}, {"id": 117, "seek": 45940, "start": 471.0, "end": 475.59999999999997, "text": " I think that's kind of a more reassurance of like, okay, I'm really hitting this much", "tokens": [286, 519, 300, 311, 733, 295, 257, 544, 19486, 5683, 295, 411, 11, 1392, 11, 286, 478, 534, 8850, 341, 709], "temperature": 0.0, "avg_logprob": -0.12371276987010035, "compression_ratio": 1.8538812785388128, "no_speech_prob": 1.9524828530848026e-05}, {"id": 118, "seek": 45940, "start": 475.59999999999997, "end": 477.84, "text": " of the column space of A.", "tokens": [295, 264, 7738, 1901, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.12371276987010035, "compression_ratio": 1.8538812785388128, "no_speech_prob": 1.9524828530848026e-05}, {"id": 119, "seek": 45940, "start": 477.84, "end": 478.84, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.12371276987010035, "compression_ratio": 1.8538812785388128, "no_speech_prob": 1.9524828530848026e-05}, {"id": 120, "seek": 45940, "start": 478.84, "end": 479.84, "text": " Because it's...", "tokens": [1436, 309, 311, 485], "temperature": 0.0, "avg_logprob": -0.12371276987010035, "compression_ratio": 1.8538812785388128, "no_speech_prob": 1.9524828530848026e-05}, {"id": 121, "seek": 45940, "start": 479.84, "end": 486.52, "text": " I guess it's like you kind of just need those column spaces to be the same up to however", "tokens": [286, 2041, 309, 311, 411, 291, 733, 295, 445, 643, 729, 7738, 7673, 281, 312, 264, 912, 493, 281, 4461], "temperature": 0.0, "avg_logprob": -0.12371276987010035, "compression_ratio": 1.8538812785388128, "no_speech_prob": 1.9524828530848026e-05}, {"id": 122, "seek": 48652, "start": 486.52, "end": 489.56, "text": " many singular values you're trying to get.", "tokens": [867, 20010, 4190, 291, 434, 1382, 281, 483, 13], "temperature": 0.0, "avg_logprob": -0.20699116161891393, "compression_ratio": 1.7012448132780082, "no_speech_prob": 9.222809239872731e-06}, {"id": 123, "seek": 48652, "start": 489.56, "end": 490.56, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.20699116161891393, "compression_ratio": 1.7012448132780082, "no_speech_prob": 9.222809239872731e-06}, {"id": 124, "seek": 48652, "start": 490.56, "end": 496.76, "text": " And then I guess what I was wondering is are those top, say, five, if you want, say, the", "tokens": [400, 550, 286, 2041, 437, 286, 390, 6359, 307, 366, 729, 1192, 11, 584, 11, 1732, 11, 498, 291, 528, 11, 584, 11, 264], "temperature": 0.0, "avg_logprob": -0.20699116161891393, "compression_ratio": 1.7012448132780082, "no_speech_prob": 9.222809239872731e-06}, {"id": 125, "seek": 48652, "start": 496.76, "end": 502.91999999999996, "text": " most important ones, then you necessarily need that buffer to sort of put the less important", "tokens": [881, 1021, 2306, 11, 550, 291, 4725, 643, 300, 21762, 281, 1333, 295, 829, 264, 1570, 1021], "temperature": 0.0, "avg_logprob": -0.20699116161891393, "compression_ratio": 1.7012448132780082, "no_speech_prob": 9.222809239872731e-06}, {"id": 126, "seek": 48652, "start": 502.91999999999996, "end": 503.91999999999996, "text": " stuff, right?", "tokens": [1507, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20699116161891393, "compression_ratio": 1.7012448132780082, "no_speech_prob": 9.222809239872731e-06}, {"id": 127, "seek": 48652, "start": 503.91999999999996, "end": 507.56, "text": " Whereas if you just have the five, then all the less important stuff is going to be kind", "tokens": [13813, 498, 291, 445, 362, 264, 1732, 11, 550, 439, 264, 1570, 1021, 1507, 307, 516, 281, 312, 733], "temperature": 0.0, "avg_logprob": -0.20699116161891393, "compression_ratio": 1.7012448132780082, "no_speech_prob": 9.222809239872731e-06}, {"id": 128, "seek": 48652, "start": 507.56, "end": 510.08, "text": " of forced to be within that top five.", "tokens": [295, 7579, 281, 312, 1951, 300, 1192, 1732, 13], "temperature": 0.0, "avg_logprob": -0.20699116161891393, "compression_ratio": 1.7012448132780082, "no_speech_prob": 9.222809239872731e-06}, {"id": 129, "seek": 48652, "start": 510.08, "end": 514.04, "text": " Is that a good way to think about it?", "tokens": [1119, 300, 257, 665, 636, 281, 519, 466, 309, 30], "temperature": 0.0, "avg_logprob": -0.20699116161891393, "compression_ratio": 1.7012448132780082, "no_speech_prob": 9.222809239872731e-06}, {"id": 130, "seek": 51404, "start": 514.04, "end": 518.04, "text": " So there is this element of randomness still, and so I think some of it's like if you were", "tokens": [407, 456, 307, 341, 4478, 295, 4974, 1287, 920, 11, 293, 370, 286, 519, 512, 295, 309, 311, 411, 498, 291, 645], "temperature": 0.4, "avg_logprob": -0.31487980694837975, "compression_ratio": 1.7007042253521127, "no_speech_prob": 7.2962220656336285e-06}, {"id": 131, "seek": 51404, "start": 518.04, "end": 524.0799999999999, "text": " just getting five, if you knew that they were truly the best, I think you would be okay.", "tokens": [445, 1242, 1732, 11, 498, 291, 2586, 300, 436, 645, 4908, 264, 1151, 11, 286, 519, 291, 576, 312, 1392, 13], "temperature": 0.4, "avg_logprob": -0.31487980694837975, "compression_ratio": 1.7007042253521127, "no_speech_prob": 7.2962220656336285e-06}, {"id": 132, "seek": 51404, "start": 524.0799999999999, "end": 527.64, "text": " But because it's random, you're kind of taking some extra...", "tokens": [583, 570, 309, 311, 4974, 11, 291, 434, 733, 295, 1940, 512, 2857, 485], "temperature": 0.4, "avg_logprob": -0.31487980694837975, "compression_ratio": 1.7007042253521127, "no_speech_prob": 7.2962220656336285e-06}, {"id": 133, "seek": 51404, "start": 527.64, "end": 529.5999999999999, "text": " But you're right.", "tokens": [583, 291, 434, 558, 13], "temperature": 0.4, "avg_logprob": -0.31487980694837975, "compression_ratio": 1.7007042253521127, "no_speech_prob": 7.2962220656336285e-06}, {"id": 134, "seek": 51404, "start": 529.5999999999999, "end": 530.5999999999999, "text": " Like it is having a...", "tokens": [1743, 309, 307, 1419, 257, 485], "temperature": 0.4, "avg_logprob": -0.31487980694837975, "compression_ratio": 1.7007042253521127, "no_speech_prob": 7.2962220656336285e-06}, {"id": 135, "seek": 51404, "start": 530.5999999999999, "end": 531.5999999999999, "text": " Or Tim, did you have a...", "tokens": [1610, 7172, 11, 630, 291, 362, 257, 485], "temperature": 0.4, "avg_logprob": -0.31487980694837975, "compression_ratio": 1.7007042253521127, "no_speech_prob": 7.2962220656336285e-06}, {"id": 136, "seek": 51404, "start": 531.5999999999999, "end": 532.5999999999999, "text": " What's the analogy?", "tokens": [708, 311, 264, 21663, 30], "temperature": 0.4, "avg_logprob": -0.31487980694837975, "compression_ratio": 1.7007042253521127, "no_speech_prob": 7.2962220656336285e-06}, {"id": 137, "seek": 51404, "start": 532.5999999999999, "end": 535.5999999999999, "text": " I guess what I thought about it is like if they have like a bunch of, I don't know, M&M's,", "tokens": [286, 2041, 437, 286, 1194, 466, 309, 307, 411, 498, 436, 362, 411, 257, 3840, 295, 11, 286, 500, 380, 458, 11, 376, 5, 44, 311, 11], "temperature": 0.4, "avg_logprob": -0.31487980694837975, "compression_ratio": 1.7007042253521127, "no_speech_prob": 7.2962220656336285e-06}, {"id": 138, "seek": 51404, "start": 535.5999999999999, "end": 542.56, "text": " like the best tasting ones are like in the top left hand corner.", "tokens": [411, 264, 1151, 26223, 2306, 366, 411, 294, 264, 1192, 1411, 1011, 4538, 13], "temperature": 0.4, "avg_logprob": -0.31487980694837975, "compression_ratio": 1.7007042253521127, "no_speech_prob": 7.2962220656336285e-06}, {"id": 139, "seek": 54256, "start": 542.56, "end": 547.4799999999999, "text": " And you know, there's five best tasting ones, but you can't just like pick five from the", "tokens": [400, 291, 458, 11, 456, 311, 1732, 1151, 26223, 2306, 11, 457, 291, 393, 380, 445, 411, 1888, 1732, 490, 264], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 140, "seek": 54256, "start": 547.4799999999999, "end": 548.4799999999999, "text": " top left corner.", "tokens": [1192, 1411, 4538, 13], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 141, "seek": 54256, "start": 548.4799999999999, "end": 551.9599999999999, "text": " You kind of pick like a handful more, and then that increases the chance of you having", "tokens": [509, 733, 295, 1888, 411, 257, 16458, 544, 11, 293, 550, 300, 8637, 264, 2931, 295, 291, 1419], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 142, "seek": 54256, "start": 551.9599999999999, "end": 554.4799999999999, "text": " the best five in that kind of handful.", "tokens": [264, 1151, 1732, 294, 300, 733, 295, 16458, 13], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 143, "seek": 54256, "start": 554.4799999999999, "end": 555.4799999999999, "text": " That's what the buff...", "tokens": [663, 311, 437, 264, 9204, 485], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 144, "seek": 54256, "start": 555.4799999999999, "end": 556.4799999999999, "text": " That's how I saw the buff.", "tokens": [663, 311, 577, 286, 1866, 264, 9204, 13], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 145, "seek": 54256, "start": 556.4799999999999, "end": 557.4799999999999, "text": " Yeah, I like that.", "tokens": [865, 11, 286, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 146, "seek": 54256, "start": 557.4799999999999, "end": 558.4799999999999, "text": " I like that analogy.", "tokens": [286, 411, 300, 21663, 13], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 147, "seek": 54256, "start": 558.4799999999999, "end": 559.4799999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 148, "seek": 54256, "start": 559.4799999999999, "end": 563.4, "text": " You just pick a little bit more so that you have the higher chance of having the best", "tokens": [509, 445, 1888, 257, 707, 857, 544, 370, 300, 291, 362, 264, 2946, 2931, 295, 1419, 264, 1151], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 149, "seek": 54256, "start": 563.4, "end": 564.4, "text": " in that handful.", "tokens": [294, 300, 16458, 13], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 150, "seek": 54256, "start": 564.4, "end": 565.4, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 151, "seek": 54256, "start": 565.4, "end": 566.4, "text": " Yeah, that's good.", "tokens": [865, 11, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 152, "seek": 54256, "start": 566.4, "end": 567.4, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 153, "seek": 54256, "start": 567.4, "end": 568.4, "text": " Sam?", "tokens": [4832, 30], "temperature": 0.0, "avg_logprob": -0.16273333928356432, "compression_ratio": 1.8991935483870968, "no_speech_prob": 4.8602755668980535e-06}, {"id": 154, "seek": 56840, "start": 568.4, "end": 578.4, "text": " Do you think we can go over what's happening when we multiply by a random matrix?", "tokens": [1144, 291, 519, 321, 393, 352, 670, 437, 311, 2737, 562, 321, 12972, 538, 257, 4974, 8141, 30], "temperature": 0.0, "avg_logprob": -0.23968978755730241, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.00013550273433793336}, {"id": 155, "seek": 56840, "start": 578.4, "end": 583.9599999999999, "text": " Can we see why it's useful and like the different kind of contexts we can use it?", "tokens": [1664, 321, 536, 983, 309, 311, 4420, 293, 411, 264, 819, 733, 295, 30628, 321, 393, 764, 309, 30], "temperature": 0.0, "avg_logprob": -0.23968978755730241, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.00013550273433793336}, {"id": 156, "seek": 56840, "start": 583.9599999999999, "end": 586.9599999999999, "text": " Because you guys mentioned that it's useful all over the place.", "tokens": [1436, 291, 1074, 2835, 300, 309, 311, 4420, 439, 670, 264, 1081, 13], "temperature": 0.0, "avg_logprob": -0.23968978755730241, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.00013550273433793336}, {"id": 157, "seek": 56840, "start": 586.9599999999999, "end": 587.9599999999999, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.23968978755730241, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.00013550273433793336}, {"id": 158, "seek": 56840, "start": 587.9599999999999, "end": 590.0799999999999, "text": " So you're kind of thinking of like different applications of that.", "tokens": [407, 291, 434, 733, 295, 1953, 295, 411, 819, 5821, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.23968978755730241, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.00013550273433793336}, {"id": 159, "seek": 56840, "start": 590.0799999999999, "end": 591.0799999999999, "text": " What does it do, first of all?", "tokens": [708, 775, 309, 360, 11, 700, 295, 439, 30], "temperature": 0.0, "avg_logprob": -0.23968978755730241, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.00013550273433793336}, {"id": 160, "seek": 56840, "start": 591.0799999999999, "end": 592.0799999999999, "text": " What does it do, and then why do it?", "tokens": [708, 775, 309, 360, 11, 293, 550, 983, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.23968978755730241, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.00013550273433793336}, {"id": 161, "seek": 56840, "start": 592.0799999999999, "end": 593.0799999999999, "text": " I kind of get it, but I'd like to know a little more about that.", "tokens": [286, 733, 295, 483, 309, 11, 457, 286, 1116, 411, 281, 458, 257, 707, 544, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.23968978755730241, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.00013550273433793336}, {"id": 162, "seek": 59308, "start": 593.08, "end": 600.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.22286222748837228, "compression_ratio": 1.3888888888888888, "no_speech_prob": 5.771764335804619e-06}, {"id": 163, "seek": 59308, "start": 600.08, "end": 604.36, "text": " Yeah, that might be something I'll return to next time.", "tokens": [865, 11, 300, 1062, 312, 746, 286, 603, 2736, 281, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.22286222748837228, "compression_ratio": 1.3888888888888888, "no_speech_prob": 5.771764335804619e-06}, {"id": 164, "seek": 59308, "start": 604.36, "end": 607.36, "text": " Let me write that down.", "tokens": [961, 385, 2464, 300, 760, 13], "temperature": 0.0, "avg_logprob": -0.22286222748837228, "compression_ratio": 1.3888888888888888, "no_speech_prob": 5.771764335804619e-06}, {"id": 165, "seek": 59308, "start": 607.36, "end": 617.5200000000001, "text": " Yeah, I want to think about if there's maybe like a good way we can visualize that as it's", "tokens": [865, 11, 286, 528, 281, 519, 466, 498, 456, 311, 1310, 411, 257, 665, 636, 321, 393, 23273, 300, 382, 309, 311], "temperature": 0.0, "avg_logprob": -0.22286222748837228, "compression_ratio": 1.3888888888888888, "no_speech_prob": 5.771764335804619e-06}, {"id": 166, "seek": 59308, "start": 617.5200000000001, "end": 619.0, "text": " happening or something.", "tokens": [2737, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.22286222748837228, "compression_ratio": 1.3888888888888888, "no_speech_prob": 5.771764335804619e-06}, {"id": 167, "seek": 61900, "start": 619.0, "end": 631.84, "text": " So kind of more detail on what is a randomized projection, where else is it used?", "tokens": [407, 733, 295, 544, 2607, 322, 437, 307, 257, 38513, 22743, 11, 689, 1646, 307, 309, 1143, 30], "temperature": 0.0, "avg_logprob": -0.16815308162144252, "compression_ratio": 1.4823529411764707, "no_speech_prob": 1.9832441466860473e-05}, {"id": 168, "seek": 61900, "start": 631.84, "end": 639.24, "text": " One key idea we mentioned is this idea that when you multiply by a random matrix, the", "tokens": [1485, 2141, 1558, 321, 2835, 307, 341, 1558, 300, 562, 291, 12972, 538, 257, 4974, 8141, 11, 264], "temperature": 0.0, "avg_logprob": -0.16815308162144252, "compression_ratio": 1.4823529411764707, "no_speech_prob": 1.9832441466860473e-05}, {"id": 169, "seek": 61900, "start": 639.24, "end": 643.64, "text": " result in columns, as long as there aren't too many, you're likely to be independent", "tokens": [1874, 294, 13766, 11, 382, 938, 382, 456, 3212, 380, 886, 867, 11, 291, 434, 3700, 281, 312, 6695], "temperature": 0.0, "avg_logprob": -0.16815308162144252, "compression_ratio": 1.4823529411764707, "no_speech_prob": 1.9832441466860473e-05}, {"id": 170, "seek": 64364, "start": 643.64, "end": 650.4, "text": " of each other, so you're likely to get orthogonal columns, which is kind of a good starting", "tokens": [295, 1184, 661, 11, 370, 291, 434, 3700, 281, 483, 41488, 13766, 11, 597, 307, 733, 295, 257, 665, 2891], "temperature": 0.0, "avg_logprob": -0.2374397560402199, "compression_ratio": 1.6230366492146597, "no_speech_prob": 4.7107569116633385e-06}, {"id": 171, "seek": 64364, "start": 650.4, "end": 654.56, "text": " point for thinking about why it helps.", "tokens": [935, 337, 1953, 466, 983, 309, 3665, 13], "temperature": 0.0, "avg_logprob": -0.2374397560402199, "compression_ratio": 1.6230366492146597, "no_speech_prob": 4.7107569116633385e-06}, {"id": 172, "seek": 64364, "start": 654.56, "end": 659.48, "text": " Yeah, and that kind of plays into the idea of like by taking a buffer, you're getting", "tokens": [865, 11, 293, 300, 733, 295, 5749, 666, 264, 1558, 295, 411, 538, 1940, 257, 21762, 11, 291, 434, 1242], "temperature": 0.0, "avg_logprob": -0.2374397560402199, "compression_ratio": 1.6230366492146597, "no_speech_prob": 4.7107569116633385e-06}, {"id": 173, "seek": 64364, "start": 659.48, "end": 665.88, "text": " even more orthogonal columns, and so you are covering kind of more of your space.", "tokens": [754, 544, 41488, 13766, 11, 293, 370, 291, 366, 10322, 733, 295, 544, 295, 428, 1901, 13], "temperature": 0.0, "avg_logprob": -0.2374397560402199, "compression_ratio": 1.6230366492146597, "no_speech_prob": 4.7107569116633385e-06}, {"id": 174, "seek": 64364, "start": 665.88, "end": 666.88, "text": " Sam, again?", "tokens": [4832, 11, 797, 30], "temperature": 0.0, "avg_logprob": -0.2374397560402199, "compression_ratio": 1.6230366492146597, "no_speech_prob": 4.7107569116633385e-06}, {"id": 175, "seek": 66688, "start": 666.88, "end": 674.72, "text": " So I guess the part in particular that I don't understand is when we multiply by the random", "tokens": [407, 286, 2041, 264, 644, 294, 1729, 300, 286, 500, 380, 1223, 307, 562, 321, 12972, 538, 264, 4974], "temperature": 0.0, "avg_logprob": -0.14717072176646037, "compression_ratio": 1.7560975609756098, "no_speech_prob": 4.397740849526599e-05}, {"id": 176, "seek": 66688, "start": 674.72, "end": 680.56, "text": " columns, we're transforming the matrix, and if the columns and rows all correspond to", "tokens": [13766, 11, 321, 434, 27210, 264, 8141, 11, 293, 498, 264, 13766, 293, 13241, 439, 6805, 281], "temperature": 0.0, "avg_logprob": -0.14717072176646037, "compression_ratio": 1.7560975609756098, "no_speech_prob": 4.397740849526599e-05}, {"id": 177, "seek": 66688, "start": 680.56, "end": 687.08, "text": " like one particular feature for each one of our users, for instance, then when we transform", "tokens": [411, 472, 1729, 4111, 337, 1184, 472, 295, 527, 5022, 11, 337, 5197, 11, 550, 562, 321, 4088], "temperature": 0.0, "avg_logprob": -0.14717072176646037, "compression_ratio": 1.7560975609756098, "no_speech_prob": 4.397740849526599e-05}, {"id": 178, "seek": 66688, "start": 687.08, "end": 693.68, "text": " it by multiplying by a bunch of random columns, at what point do we have to like transform", "tokens": [309, 538, 30955, 538, 257, 3840, 295, 4974, 13766, 11, 412, 437, 935, 360, 321, 362, 281, 411, 4088], "temperature": 0.0, "avg_logprob": -0.14717072176646037, "compression_ratio": 1.7560975609756098, "no_speech_prob": 4.397740849526599e-05}, {"id": 179, "seek": 69368, "start": 693.68, "end": 700.8399999999999, "text": " it back so that we have, so we actually have like...", "tokens": [309, 646, 370, 300, 321, 362, 11, 370, 321, 767, 362, 411, 485], "temperature": 0.0, "avg_logprob": -0.20859024903484594, "compression_ratio": 1.6724137931034482, "no_speech_prob": 1.4738768186361995e-05}, {"id": 180, "seek": 69368, "start": 700.8399999999999, "end": 704.3199999999999, "text": " So yeah, so the transforming it back is what happens, if you'll remember, we like take", "tokens": [407, 1338, 11, 370, 264, 27210, 309, 646, 307, 437, 2314, 11, 498, 291, 603, 1604, 11, 321, 411, 747], "temperature": 0.0, "avg_logprob": -0.20859024903484594, "compression_ratio": 1.6724137931034482, "no_speech_prob": 1.4738768186361995e-05}, {"id": 181, "seek": 69368, "start": 704.3199999999999, "end": 710.12, "text": " the SVD on the smaller matrix and then we multiply by Q again, which I can't remember", "tokens": [264, 31910, 35, 322, 264, 4356, 8141, 293, 550, 321, 12972, 538, 1249, 797, 11, 597, 286, 393, 380, 1604], "temperature": 0.0, "avg_logprob": -0.20859024903484594, "compression_ratio": 1.6724137931034482, "no_speech_prob": 1.4738768186361995e-05}, {"id": 182, "seek": 69368, "start": 710.12, "end": 717.64, "text": " if it's Q or Q transpose, to kind of, that's what transforms it back, since Q is orthonormal,", "tokens": [498, 309, 311, 1249, 420, 1249, 25167, 11, 281, 733, 295, 11, 300, 311, 437, 35592, 309, 646, 11, 1670, 1249, 307, 420, 11943, 24440, 11], "temperature": 0.0, "avg_logprob": -0.20859024903484594, "compression_ratio": 1.6724137931034482, "no_speech_prob": 1.4738768186361995e-05}, {"id": 183, "seek": 69368, "start": 717.64, "end": 722.4799999999999, "text": " it's its own inverse, and so that's kind of the transformation back.", "tokens": [309, 311, 1080, 1065, 17340, 11, 293, 370, 300, 311, 733, 295, 264, 9887, 646, 13], "temperature": 0.0, "avg_logprob": -0.20859024903484594, "compression_ratio": 1.6724137931034482, "no_speech_prob": 1.4738768186361995e-05}, {"id": 184, "seek": 72248, "start": 722.48, "end": 724.72, "text": " I remember that part.", "tokens": [286, 1604, 300, 644, 13], "temperature": 0.0, "avg_logprob": -0.34621231228697535, "compression_ratio": 1.3089430894308942, "no_speech_prob": 1.0952874617942143e-05}, {"id": 185, "seek": 72248, "start": 724.72, "end": 725.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.34621231228697535, "compression_ratio": 1.3089430894308942, "no_speech_prob": 1.0952874617942143e-05}, {"id": 186, "seek": 72248, "start": 725.72, "end": 726.72, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.34621231228697535, "compression_ratio": 1.3089430894308942, "no_speech_prob": 1.0952874617942143e-05}, {"id": 187, "seek": 72248, "start": 726.72, "end": 738.28, "text": " So I think we've thoroughly hit the first three questions about SVD and truncated SVD,", "tokens": [407, 286, 519, 321, 600, 17987, 2045, 264, 700, 1045, 1651, 466, 31910, 35, 293, 504, 409, 66, 770, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.34621231228697535, "compression_ratio": 1.3089430894308942, "no_speech_prob": 1.0952874617942143e-05}, {"id": 188, "seek": 72248, "start": 738.28, "end": 742.32, "text": " are there any other questions about it?", "tokens": [366, 456, 604, 661, 1651, 466, 309, 30], "temperature": 0.0, "avg_logprob": -0.34621231228697535, "compression_ratio": 1.3089430894308942, "no_speech_prob": 1.0952874617942143e-05}, {"id": 189, "seek": 74232, "start": 742.32, "end": 752.6400000000001, "text": " Okay, and then I think something that I definitely wanted to kind of say more clearly than I", "tokens": [1033, 11, 293, 550, 286, 519, 746, 300, 286, 2138, 1415, 281, 733, 295, 584, 544, 4448, 813, 286], "temperature": 0.0, "avg_logprob": -0.101230360057256, "compression_ratio": 1.4917127071823204, "no_speech_prob": 7.5277457654010504e-06}, {"id": 190, "seek": 74232, "start": 752.6400000000001, "end": 762.96, "text": " did last time is that classical PCA is finding the best rank K estimate L of M, so minimizing", "tokens": [630, 1036, 565, 307, 300, 13735, 6465, 32, 307, 5006, 264, 1151, 6181, 591, 12539, 441, 295, 376, 11, 370, 46608], "temperature": 0.0, "avg_logprob": -0.101230360057256, "compression_ratio": 1.4917127071823204, "no_speech_prob": 7.5277457654010504e-06}, {"id": 191, "seek": 74232, "start": 762.96, "end": 770.08, "text": " the norm of M minus L where L has rank K, and as Brad said, truncated SVD is giving", "tokens": [264, 2026, 295, 376, 3175, 441, 689, 441, 575, 6181, 591, 11, 293, 382, 11895, 848, 11, 504, 409, 66, 770, 31910, 35, 307, 2902], "temperature": 0.0, "avg_logprob": -0.101230360057256, "compression_ratio": 1.4917127071823204, "no_speech_prob": 7.5277457654010504e-06}, {"id": 192, "seek": 77008, "start": 770.08, "end": 780.08, "text": " you the best classical PCA, so that's the kind of best rank K estimate L is going to", "tokens": [291, 264, 1151, 13735, 6465, 32, 11, 370, 300, 311, 264, 733, 295, 1151, 6181, 591, 12539, 441, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.22532930681782384, "compression_ratio": 1.578125, "no_speech_prob": 2.930941445811186e-05}, {"id": 193, "seek": 77008, "start": 780.08, "end": 785.8000000000001, "text": " be if you did the SVD on M, your full matrix, and then cut off however many columns you", "tokens": [312, 498, 291, 630, 264, 31910, 35, 322, 376, 11, 428, 1577, 8141, 11, 293, 550, 1723, 766, 4461, 867, 13766, 291], "temperature": 0.0, "avg_logprob": -0.22532930681782384, "compression_ratio": 1.578125, "no_speech_prob": 2.930941445811186e-05}, {"id": 194, "seek": 77008, "start": 785.8000000000001, "end": 787.44, "text": " needed.", "tokens": [2978, 13], "temperature": 0.0, "avg_logprob": -0.22532930681782384, "compression_ratio": 1.578125, "no_speech_prob": 2.930941445811186e-05}, {"id": 195, "seek": 77008, "start": 787.44, "end": 789.44, "text": " That's what gives you the best estimate.", "tokens": [663, 311, 437, 2709, 291, 264, 1151, 12539, 13], "temperature": 0.0, "avg_logprob": -0.22532930681782384, "compression_ratio": 1.578125, "no_speech_prob": 2.930941445811186e-05}, {"id": 196, "seek": 77008, "start": 789.44, "end": 794.5600000000001, "text": " So that was under the Frobenius norm?", "tokens": [407, 300, 390, 833, 264, 25028, 1799, 4872, 2026, 30], "temperature": 0.0, "avg_logprob": -0.22532930681782384, "compression_ratio": 1.578125, "no_speech_prob": 2.930941445811186e-05}, {"id": 197, "seek": 77008, "start": 794.5600000000001, "end": 795.5600000000001, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.22532930681782384, "compression_ratio": 1.578125, "no_speech_prob": 2.930941445811186e-05}, {"id": 198, "seek": 77008, "start": 795.5600000000001, "end": 798.32, "text": " Oh, that was under the Frobenius norm.", "tokens": [876, 11, 300, 390, 833, 264, 25028, 1799, 4872, 2026, 13], "temperature": 0.0, "avg_logprob": -0.22532930681782384, "compression_ratio": 1.578125, "no_speech_prob": 2.930941445811186e-05}, {"id": 199, "seek": 79832, "start": 798.32, "end": 804.5600000000001, "text": " And the Frobenius norm is where you're just squaring each of your elements element-wise", "tokens": [400, 264, 25028, 1799, 4872, 2026, 307, 689, 291, 434, 445, 2339, 1921, 1184, 295, 428, 4959, 4478, 12, 3711], "temperature": 0.0, "avg_logprob": -0.1070663332939148, "compression_ratio": 1.4787234042553192, "no_speech_prob": 6.438906439143466e-06}, {"id": 200, "seek": 79832, "start": 804.5600000000001, "end": 810.9200000000001, "text": " and summing them.", "tokens": [293, 2408, 2810, 552, 13], "temperature": 0.0, "avg_logprob": -0.1070663332939148, "compression_ratio": 1.4787234042553192, "no_speech_prob": 6.438906439143466e-06}, {"id": 201, "seek": 79832, "start": 810.9200000000001, "end": 818.24, "text": " And this, yeah, kind of as we said before, you could get the truncated SVD by doing the", "tokens": [400, 341, 11, 1338, 11, 733, 295, 382, 321, 848, 949, 11, 291, 727, 483, 264, 504, 409, 66, 770, 31910, 35, 538, 884, 264], "temperature": 0.0, "avg_logprob": -0.1070663332939148, "compression_ratio": 1.4787234042553192, "no_speech_prob": 6.438906439143466e-06}, {"id": 202, "seek": 79832, "start": 818.24, "end": 823.7600000000001, "text": " full SVD and then throwing away the values you don't need, although that's slow, and", "tokens": [1577, 31910, 35, 293, 550, 10238, 1314, 264, 4190, 291, 500, 380, 643, 11, 4878, 300, 311, 2964, 11, 293], "temperature": 0.0, "avg_logprob": -0.1070663332939148, "compression_ratio": 1.4787234042553192, "no_speech_prob": 6.438906439143466e-06}, {"id": 203, "seek": 82376, "start": 823.76, "end": 829.6, "text": " so we saw the randomized approximation as an accurate substitute for that, to kind of", "tokens": [370, 321, 1866, 264, 38513, 28023, 382, 364, 8559, 15802, 337, 300, 11, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.32477666934331256, "compression_ratio": 1.2536231884057971, "no_speech_prob": 1.3496893416231615e-06}, {"id": 204, "seek": 82376, "start": 829.6, "end": 831.6, "text": " do it more quickly.", "tokens": [360, 309, 544, 2661, 13], "temperature": 0.0, "avg_logprob": -0.32477666934331256, "compression_ratio": 1.2536231884057971, "no_speech_prob": 1.3496893416231615e-06}, {"id": 205, "seek": 82376, "start": 831.6, "end": 834.4, "text": " So that's a classical PCA.", "tokens": [407, 300, 311, 257, 13735, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.32477666934331256, "compression_ratio": 1.2536231884057971, "no_speech_prob": 1.3496893416231615e-06}, {"id": 206, "seek": 82376, "start": 834.4, "end": 840.28, "text": " Does anyone remember what robust PCA is?", "tokens": [4402, 2878, 1604, 437, 13956, 6465, 32, 307, 30], "temperature": 0.0, "avg_logprob": -0.32477666934331256, "compression_ratio": 1.2536231884057971, "no_speech_prob": 1.3496893416231615e-06}, {"id": 207, "seek": 84028, "start": 840.28, "end": 856.16, "text": " I'll give you a hint, we added in another matrix, so instead of decomposing M just to", "tokens": [286, 603, 976, 291, 257, 12075, 11, 321, 3869, 294, 1071, 8141, 11, 370, 2602, 295, 22867, 6110, 376, 445, 281], "temperature": 0.0, "avg_logprob": -0.2092895694807464, "compression_ratio": 1.3801652892561984, "no_speech_prob": 2.4824282718327595e-06}, {"id": 208, "seek": 84028, "start": 856.16, "end": 864.16, "text": " be L, a single matrix, we decomposed it to be L plus another type of matrix.", "tokens": [312, 441, 11, 257, 2167, 8141, 11, 321, 22867, 1744, 309, 281, 312, 441, 1804, 1071, 2010, 295, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2092895694807464, "compression_ratio": 1.3801652892561984, "no_speech_prob": 2.4824282718327595e-06}, {"id": 209, "seek": 84028, "start": 864.16, "end": 866.16, "text": " Sam?", "tokens": [4832, 30], "temperature": 0.0, "avg_logprob": -0.2092895694807464, "compression_ratio": 1.3801652892561984, "no_speech_prob": 2.4824282718327595e-06}, {"id": 210, "seek": 86616, "start": 866.16, "end": 875.12, "text": " We're decomposing it into a dense sparse, which makes it more robust to our lives.", "tokens": [492, 434, 22867, 6110, 309, 666, 257, 18011, 637, 11668, 11, 597, 1669, 309, 544, 13956, 281, 527, 2909, 13], "temperature": 0.0, "avg_logprob": -0.2160654851835068, "compression_ratio": 1.402061855670103, "no_speech_prob": 5.09359188072267e-06}, {"id": 211, "seek": 86616, "start": 875.12, "end": 881.12, "text": " Yeah, so we're getting a sparse one now, and so it's a low-rank matrix plus a sparse matrix.", "tokens": [865, 11, 370, 321, 434, 1242, 257, 637, 11668, 472, 586, 11, 293, 370, 309, 311, 257, 2295, 12, 20479, 8141, 1804, 257, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2160654851835068, "compression_ratio": 1.402061855670103, "no_speech_prob": 5.09359188072267e-06}, {"id": 212, "seek": 86616, "start": 881.12, "end": 892.76, "text": " And I wanted to kind of highlight with the\u2026 previously when we were looking at this background", "tokens": [400, 286, 1415, 281, 733, 295, 5078, 365, 264, 1260, 8046, 562, 321, 645, 1237, 412, 341, 3678], "temperature": 0.0, "avg_logprob": -0.2160654851835068, "compression_ratio": 1.402061855670103, "no_speech_prob": 5.09359188072267e-06}, {"id": 213, "seek": 89276, "start": 892.76, "end": 908.6, "text": " removal problem, we started off just doing the randomized SVD, or just doing an SVD to", "tokens": [17933, 1154, 11, 321, 1409, 766, 445, 884, 264, 38513, 31910, 35, 11, 420, 445, 884, 364, 31910, 35, 281], "temperature": 0.0, "avg_logprob": -0.12086742802670128, "compression_ratio": 1.6045197740112995, "no_speech_prob": 9.22326034924481e-06}, {"id": 214, "seek": 89276, "start": 908.6, "end": 913.08, "text": " get the background, and there we're really just trying to find this low-rank matrix,", "tokens": [483, 264, 3678, 11, 293, 456, 321, 434, 534, 445, 1382, 281, 915, 341, 2295, 12, 20479, 8141, 11], "temperature": 0.0, "avg_logprob": -0.12086742802670128, "compression_ratio": 1.6045197740112995, "no_speech_prob": 9.22326034924481e-06}, {"id": 215, "seek": 89276, "start": 913.08, "end": 915.12, "text": " and that's the background.", "tokens": [293, 300, 311, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.12086742802670128, "compression_ratio": 1.6045197740112995, "no_speech_prob": 9.22326034924481e-06}, {"id": 216, "seek": 89276, "start": 915.12, "end": 918.52, "text": " The difference with when we go to robust PCA is now we're trying to find the low-rank", "tokens": [440, 2649, 365, 562, 321, 352, 281, 13956, 6465, 32, 307, 586, 321, 434, 1382, 281, 915, 264, 2295, 12, 20479], "temperature": 0.0, "avg_logprob": -0.12086742802670128, "compression_ratio": 1.6045197740112995, "no_speech_prob": 9.22326034924481e-06}, {"id": 217, "seek": 91852, "start": 918.52, "end": 923.36, "text": " matrix and the sparse matrix, and so we're actually kind of actively looking for what", "tokens": [8141, 293, 264, 637, 11668, 8141, 11, 293, 370, 321, 434, 767, 733, 295, 13022, 1237, 337, 437], "temperature": 0.0, "avg_logprob": -0.13943539495053497, "compression_ratio": 1.7393364928909953, "no_speech_prob": 7.766703674860764e-06}, {"id": 218, "seek": 91852, "start": 923.36, "end": 924.96, "text": " is the foreground.", "tokens": [307, 264, 32058, 13], "temperature": 0.0, "avg_logprob": -0.13943539495053497, "compression_ratio": 1.7393364928909953, "no_speech_prob": 7.766703674860764e-06}, {"id": 219, "seek": 91852, "start": 924.96, "end": 929.68, "text": " So initially, kind of when we just did SVD, we were like just looking for the background,", "tokens": [407, 9105, 11, 733, 295, 562, 321, 445, 630, 31910, 35, 11, 321, 645, 411, 445, 1237, 337, 264, 3678, 11], "temperature": 0.0, "avg_logprob": -0.13943539495053497, "compression_ratio": 1.7393364928909953, "no_speech_prob": 7.766703674860764e-06}, {"id": 220, "seek": 91852, "start": 929.68, "end": 934.24, "text": " and then we were like, okay, whatever's left over must be foreground, whereas robust PCA", "tokens": [293, 550, 321, 645, 411, 11, 1392, 11, 2035, 311, 1411, 670, 1633, 312, 32058, 11, 9735, 13956, 6465, 32], "temperature": 0.0, "avg_logprob": -0.13943539495053497, "compression_ratio": 1.7393364928909953, "no_speech_prob": 7.766703674860764e-06}, {"id": 221, "seek": 91852, "start": 934.24, "end": 938.52, "text": " we're taking this very different approach of, okay, we're actively looking for both", "tokens": [321, 434, 1940, 341, 588, 819, 3109, 295, 11, 1392, 11, 321, 434, 13022, 1237, 337, 1293], "temperature": 0.0, "avg_logprob": -0.13943539495053497, "compression_ratio": 1.7393364928909953, "no_speech_prob": 7.766703674860764e-06}, {"id": 222, "seek": 93852, "start": 938.52, "end": 953.64, "text": " the background, which is low-rank, and the foreground, which is sparse.", "tokens": [264, 3678, 11, 597, 307, 2295, 12, 20479, 11, 293, 264, 32058, 11, 597, 307, 637, 11668, 13], "temperature": 0.0, "avg_logprob": -0.2443332036336263, "compression_ratio": 1.5257731958762886, "no_speech_prob": 1.6280281442959676e-06}, {"id": 223, "seek": 93852, "start": 953.64, "end": 954.64, "text": " And then what are some\u2026", "tokens": [400, 550, 437, 366, 512, 1260], "temperature": 0.0, "avg_logprob": -0.2443332036336263, "compression_ratio": 1.5257731958762886, "no_speech_prob": 1.6280281442959676e-06}, {"id": 224, "seek": 93852, "start": 954.64, "end": 955.64, "text": " So we talk\u2026", "tokens": [407, 321, 751, 1260], "temperature": 0.0, "avg_logprob": -0.2443332036336263, "compression_ratio": 1.5257731958762886, "no_speech_prob": 1.6280281442959676e-06}, {"id": 225, "seek": 93852, "start": 955.64, "end": 956.64, "text": " Oh, Tim?", "tokens": [876, 11, 7172, 30], "temperature": 0.0, "avg_logprob": -0.2443332036336263, "compression_ratio": 1.5257731958762886, "no_speech_prob": 1.6280281442959676e-06}, {"id": 226, "seek": 93852, "start": 956.64, "end": 961.48, "text": " So how does it actually do that decomposition into L plus S?", "tokens": [407, 577, 775, 309, 767, 360, 300, 48356, 666, 441, 1804, 318, 30], "temperature": 0.0, "avg_logprob": -0.2443332036336263, "compression_ratio": 1.5257731958762886, "no_speech_prob": 1.6280281442959676e-06}, {"id": 227, "seek": 93852, "start": 961.48, "end": 963.04, "text": " Like how do you actually do the decomposition?", "tokens": [1743, 577, 360, 291, 767, 360, 264, 48356, 30], "temperature": 0.0, "avg_logprob": -0.2443332036336263, "compression_ratio": 1.5257731958762886, "no_speech_prob": 1.6280281442959676e-06}, {"id": 228, "seek": 93852, "start": 963.04, "end": 964.04, "text": " Okay, so we'll talk about that.", "tokens": [1033, 11, 370, 321, 603, 751, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.2443332036336263, "compression_ratio": 1.5257731958762886, "no_speech_prob": 1.6280281442959676e-06}, {"id": 229, "seek": 93852, "start": 964.04, "end": 966.56, "text": " That's a good question in a moment.", "tokens": [663, 311, 257, 665, 1168, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.2443332036336263, "compression_ratio": 1.5257731958762886, "no_speech_prob": 1.6280281442959676e-06}, {"id": 230, "seek": 96656, "start": 966.56, "end": 973.92, "text": " And that\u2026I'm going to kind of just highlight some key parts of the algorithm, and that's", "tokens": [400, 300, 1260, 40, 478, 516, 281, 733, 295, 445, 5078, 512, 2141, 3166, 295, 264, 9284, 11, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.2543561327588427, "compression_ratio": 1.6350710900473933, "no_speech_prob": 6.240757102204952e-06}, {"id": 231, "seek": 96656, "start": 973.92, "end": 978.16, "text": " kind of part of an alternating Lagrange method.", "tokens": [733, 295, 644, 295, 364, 40062, 24886, 14521, 3170, 13], "temperature": 0.0, "avg_logprob": -0.2543561327588427, "compression_ratio": 1.6350710900473933, "no_speech_prob": 6.240757102204952e-06}, {"id": 232, "seek": 96656, "start": 978.16, "end": 981.92, "text": " And so we're not going to get into like the details of\u2026all the details of the optimization,", "tokens": [400, 370, 321, 434, 406, 516, 281, 483, 666, 411, 264, 4365, 295, 1260, 336, 264, 4365, 295, 264, 19618, 11], "temperature": 0.0, "avg_logprob": -0.2543561327588427, "compression_ratio": 1.6350710900473933, "no_speech_prob": 6.240757102204952e-06}, {"id": 233, "seek": 96656, "start": 981.92, "end": 987.4, "text": " but yeah, I will go over kind of three\u2026what I think three key parts of the algorithm are.", "tokens": [457, 1338, 11, 286, 486, 352, 670, 733, 295, 1045, 1260, 5479, 286, 519, 1045, 2141, 3166, 295, 264, 9284, 366, 13], "temperature": 0.0, "avg_logprob": -0.2543561327588427, "compression_ratio": 1.6350710900473933, "no_speech_prob": 6.240757102204952e-06}, {"id": 234, "seek": 96656, "start": 987.4, "end": 991.16, "text": " Yeah, good question.", "tokens": [865, 11, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2543561327588427, "compression_ratio": 1.6350710900473933, "no_speech_prob": 6.240757102204952e-06}, {"id": 235, "seek": 99116, "start": 991.16, "end": 998.6, "text": " Jeremy, the top of the microphone fell off.", "tokens": [17809, 11, 264, 1192, 295, 264, 10952, 5696, 766, 13], "temperature": 0.0, "avg_logprob": -0.20863904708471054, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.9333222098794067e-06}, {"id": 236, "seek": 99116, "start": 998.6, "end": 1002.36, "text": " So what are some other applications that we saw?", "tokens": [407, 437, 366, 512, 661, 5821, 300, 321, 1866, 30], "temperature": 0.0, "avg_logprob": -0.20863904708471054, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.9333222098794067e-06}, {"id": 237, "seek": 99116, "start": 1002.36, "end": 1005.52, "text": " So we did the background removal.", "tokens": [407, 321, 630, 264, 3678, 17933, 13], "temperature": 0.0, "avg_logprob": -0.20863904708471054, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.9333222098794067e-06}, {"id": 238, "seek": 99116, "start": 1005.52, "end": 1012.9599999999999, "text": " Yeah, facial recognition, and in particular, we were seeing faces where there was a lot", "tokens": [865, 11, 15642, 11150, 11, 293, 294, 1729, 11, 321, 645, 2577, 8475, 689, 456, 390, 257, 688], "temperature": 0.0, "avg_logprob": -0.20863904708471054, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.9333222098794067e-06}, {"id": 239, "seek": 99116, "start": 1012.9599999999999, "end": 1014.26, "text": " of noise.", "tokens": [295, 5658, 13], "temperature": 0.0, "avg_logprob": -0.20863904708471054, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.9333222098794067e-06}, {"id": 240, "seek": 99116, "start": 1014.26, "end": 1020.4, "text": " So at any time that your data is corrupted with noise, particularly large amounts of", "tokens": [407, 412, 604, 565, 300, 428, 1412, 307, 39480, 365, 5658, 11, 4098, 2416, 11663, 295], "temperature": 0.0, "avg_logprob": -0.20863904708471054, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.9333222098794067e-06}, {"id": 241, "seek": 102040, "start": 1020.4, "end": 1028.28, "text": " noise\u2026so one of the weaknesses of PCA\u2026classical PCA is\u2026classical PCA can handle noise that's", "tokens": [5658, 1260, 539, 472, 295, 264, 24381, 295, 6465, 32, 1260, 11665, 804, 6465, 32, 307, 1260, 11665, 804, 6465, 32, 393, 4813, 5658, 300, 311], "temperature": 0.0, "avg_logprob": -0.1283772912355933, "compression_ratio": 1.5789473684210527, "no_speech_prob": 6.338972070807358e-06}, {"id": 242, "seek": 102040, "start": 1028.28, "end": 1033.48, "text": " small everywhere, but it's very brittle if there's even just like a few entries that", "tokens": [1359, 5315, 11, 457, 309, 311, 588, 49325, 498, 456, 311, 754, 445, 411, 257, 1326, 23041, 300], "temperature": 0.0, "avg_logprob": -0.1283772912355933, "compression_ratio": 1.5789473684210527, "no_speech_prob": 6.338972070807358e-06}, {"id": 243, "seek": 102040, "start": 1033.48, "end": 1036.44, "text": " are way off and have a huge amount of noise.", "tokens": [366, 636, 766, 293, 362, 257, 2603, 2372, 295, 5658, 13], "temperature": 0.0, "avg_logprob": -0.1283772912355933, "compression_ratio": 1.5789473684210527, "no_speech_prob": 6.338972070807358e-06}, {"id": 244, "seek": 102040, "start": 1036.44, "end": 1039.84, "text": " And so robust PCA is good at handling that.", "tokens": [400, 370, 13956, 6465, 32, 307, 665, 412, 13175, 300, 13], "temperature": 0.0, "avg_logprob": -0.1283772912355933, "compression_ratio": 1.5789473684210527, "no_speech_prob": 6.338972070807358e-06}, {"id": 245, "seek": 102040, "start": 1039.84, "end": 1046.36, "text": " And in that case, the low ranked data is kind of your\u2026I guess, you know, real data or", "tokens": [400, 294, 300, 1389, 11, 264, 2295, 20197, 1412, 307, 733, 295, 428, 1260, 40, 2041, 11, 291, 458, 11, 957, 1412, 420], "temperature": 0.0, "avg_logprob": -0.1283772912355933, "compression_ratio": 1.5789473684210527, "no_speech_prob": 6.338972070807358e-06}, {"id": 246, "seek": 104636, "start": 1046.36, "end": 1051.12, "text": " accurate data, and the sparse data is your noise, but those values can be anything.", "tokens": [8559, 1412, 11, 293, 264, 637, 11668, 1412, 307, 428, 5658, 11, 457, 729, 4190, 393, 312, 1340, 13], "temperature": 0.0, "avg_logprob": -0.2994310926682878, "compression_ratio": 1.512396694214876, "no_speech_prob": 1.952175807673484e-05}, {"id": 247, "seek": 104636, "start": 1051.12, "end": 1057.28, "text": " So when you have grossly corrupted data, which often happens, kind of particularly with online", "tokens": [407, 562, 291, 362, 11367, 356, 39480, 1412, 11, 597, 2049, 2314, 11, 733, 295, 4098, 365, 2950], "temperature": 0.0, "avg_logprob": -0.2994310926682878, "compression_ratio": 1.512396694214876, "no_speech_prob": 1.952175807673484e-05}, {"id": 248, "seek": 104636, "start": 1057.28, "end": 1064.24, "text": " interactions, robust PCA is useful.", "tokens": [13280, 11, 13956, 6465, 32, 307, 4420, 13], "temperature": 0.0, "avg_logprob": -0.2994310926682878, "compression_ratio": 1.512396694214876, "no_speech_prob": 1.952175807673484e-05}, {"id": 249, "seek": 104636, "start": 1064.24, "end": 1065.24, "text": " Any other applications?", "tokens": [2639, 661, 5821, 30], "temperature": 0.0, "avg_logprob": -0.2994310926682878, "compression_ratio": 1.512396694214876, "no_speech_prob": 1.952175807673484e-05}, {"id": 250, "seek": 104636, "start": 1065.24, "end": 1066.24, "text": " Tim?", "tokens": [7172, 30], "temperature": 0.0, "avg_logprob": -0.2994310926682878, "compression_ratio": 1.512396694214876, "no_speech_prob": 1.952175807673484e-05}, {"id": 251, "seek": 104636, "start": 1066.24, "end": 1067.24, "text": " All right, hold on a moment.", "tokens": [1057, 558, 11, 1797, 322, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.2994310926682878, "compression_ratio": 1.512396694214876, "no_speech_prob": 1.952175807673484e-05}, {"id": 252, "seek": 104636, "start": 1067.24, "end": 1068.24, "text": " Why would we not use robust PCA?", "tokens": [1545, 576, 321, 406, 764, 13956, 6465, 32, 30], "temperature": 0.0, "avg_logprob": -0.2994310926682878, "compression_ratio": 1.512396694214876, "no_speech_prob": 1.952175807673484e-05}, {"id": 253, "seek": 104636, "start": 1068.24, "end": 1069.24, "text": " Like in what case would we just not\u2026like what would we do?", "tokens": [1743, 294, 437, 1389, 576, 321, 445, 406, 1260, 4092, 437, 576, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.2994310926682878, "compression_ratio": 1.512396694214876, "no_speech_prob": 1.952175807673484e-05}, {"id": 254, "seek": 106924, "start": 1069.24, "end": 1076.24, "text": " Why would we use classical PCA?", "tokens": [1545, 576, 321, 764, 13735, 6465, 32, 30], "temperature": 0.0, "avg_logprob": -0.31530507405598956, "compression_ratio": 1.5570776255707763, "no_speech_prob": 6.1439363889803644e-06}, {"id": 255, "seek": 106924, "start": 1076.24, "end": 1079.24, "text": " It seems robust PCA is better.", "tokens": [467, 2544, 13956, 6465, 32, 307, 1101, 13], "temperature": 0.0, "avg_logprob": -0.31530507405598956, "compression_ratio": 1.5570776255707763, "no_speech_prob": 6.1439363889803644e-06}, {"id": 256, "seek": 106924, "start": 1079.24, "end": 1080.24, "text": " That's true.", "tokens": [663, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.31530507405598956, "compression_ratio": 1.5570776255707763, "no_speech_prob": 6.1439363889803644e-06}, {"id": 257, "seek": 106924, "start": 1080.24, "end": 1083.08, "text": " I mean, I guess the downside to robust PCA is it is a little bit slower.", "tokens": [286, 914, 11, 286, 2041, 264, 25060, 281, 13956, 6465, 32, 307, 309, 307, 257, 707, 857, 14009, 13], "temperature": 0.0, "avg_logprob": -0.31530507405598956, "compression_ratio": 1.5570776255707763, "no_speech_prob": 6.1439363889803644e-06}, {"id": 258, "seek": 106924, "start": 1083.08, "end": 1084.08, "text": " But yeah, it is\u2026", "tokens": [583, 1338, 11, 309, 307, 1260], "temperature": 0.0, "avg_logprob": -0.31530507405598956, "compression_ratio": 1.5570776255707763, "no_speech_prob": 6.1439363889803644e-06}, {"id": 259, "seek": 106924, "start": 1084.08, "end": 1085.08, "text": " Is it much slower?", "tokens": [1119, 309, 709, 14009, 30], "temperature": 0.0, "avg_logprob": -0.31530507405598956, "compression_ratio": 1.5570776255707763, "no_speech_prob": 6.1439363889803644e-06}, {"id": 260, "seek": 106924, "start": 1085.08, "end": 1089.08, "text": " Like are there cases where it would just be much slower than classical PCA?", "tokens": [1743, 366, 456, 3331, 689, 309, 576, 445, 312, 709, 14009, 813, 13735, 6465, 32, 30], "temperature": 0.0, "avg_logprob": -0.31530507405598956, "compression_ratio": 1.5570776255707763, "no_speech_prob": 6.1439363889803644e-06}, {"id": 261, "seek": 106924, "start": 1089.08, "end": 1090.72, "text": " Well, actually, I'll say too.", "tokens": [1042, 11, 767, 11, 286, 603, 584, 886, 13], "temperature": 0.0, "avg_logprob": -0.31530507405598956, "compression_ratio": 1.5570776255707763, "no_speech_prob": 6.1439363889803644e-06}, {"id": 262, "seek": 106924, "start": 1090.72, "end": 1091.72, "text": " So it's slower.", "tokens": [407, 309, 311, 14009, 13], "temperature": 0.0, "avg_logprob": -0.31530507405598956, "compression_ratio": 1.5570776255707763, "no_speech_prob": 6.1439363889803644e-06}, {"id": 263, "seek": 106924, "start": 1091.72, "end": 1094.24, "text": " It's also more finicky to train.", "tokens": [467, 311, 611, 544, 962, 20539, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.31530507405598956, "compression_ratio": 1.5570776255707763, "no_speech_prob": 6.1439363889803644e-06}, {"id": 264, "seek": 109424, "start": 1094.24, "end": 1099.32, "text": " Like I felt like there were more kind of parameters to tune when programming it.", "tokens": [1743, 286, 2762, 411, 456, 645, 544, 733, 295, 9834, 281, 10864, 562, 9410, 309, 13], "temperature": 0.0, "avg_logprob": -0.23707040150960287, "compression_ratio": 1.5458715596330275, "no_speech_prob": 9.817766112973914e-06}, {"id": 265, "seek": 109424, "start": 1099.32, "end": 1104.1200000000001, "text": " Is the name robust PCA?", "tokens": [1119, 264, 1315, 13956, 6465, 32, 30], "temperature": 0.0, "avg_logprob": -0.23707040150960287, "compression_ratio": 1.5458715596330275, "no_speech_prob": 9.817766112973914e-06}, {"id": 266, "seek": 109424, "start": 1104.1200000000001, "end": 1105.1200000000001, "text": " That's true.", "tokens": [663, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.23707040150960287, "compression_ratio": 1.5458715596330275, "no_speech_prob": 9.817766112973914e-06}, {"id": 267, "seek": 109424, "start": 1105.1200000000001, "end": 1106.1200000000001, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23707040150960287, "compression_ratio": 1.5458715596330275, "no_speech_prob": 9.817766112973914e-06}, {"id": 268, "seek": 109424, "start": 1106.1200000000001, "end": 1114.04, "text": " And some of this is, I don't know\u2026and maybe that's like once you have it working.", "tokens": [400, 512, 295, 341, 307, 11, 286, 500, 380, 458, 1260, 474, 1310, 300, 311, 411, 1564, 291, 362, 309, 1364, 13], "temperature": 0.0, "avg_logprob": -0.23707040150960287, "compression_ratio": 1.5458715596330275, "no_speech_prob": 9.817766112973914e-06}, {"id": 269, "seek": 109424, "start": 1114.04, "end": 1116.96, "text": " Like some of this was like trying to get it working from the papers, but like there was", "tokens": [1743, 512, 295, 341, 390, 411, 1382, 281, 483, 309, 1364, 490, 264, 10577, 11, 457, 411, 456, 390], "temperature": 0.0, "avg_logprob": -0.23707040150960287, "compression_ratio": 1.5458715596330275, "no_speech_prob": 9.817766112973914e-06}, {"id": 270, "seek": 109424, "start": 1116.96, "end": 1119.16, "text": " kind of like a parameter I had to change.", "tokens": [733, 295, 411, 257, 13075, 286, 632, 281, 1319, 13], "temperature": 0.0, "avg_logprob": -0.23707040150960287, "compression_ratio": 1.5458715596330275, "no_speech_prob": 9.817766112973914e-06}, {"id": 271, "seek": 111916, "start": 1119.16, "end": 1125.2, "text": " I probably feel like, do you really think it's slower?", "tokens": [286, 1391, 841, 411, 11, 360, 291, 534, 519, 309, 311, 14009, 30], "temperature": 0.0, "avg_logprob": -0.36468593052455356, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.6473149521043524e-05}, {"id": 272, "seek": 111916, "start": 1125.2, "end": 1129.48, "text": " Because it's kind of using this randomization.", "tokens": [1436, 309, 311, 733, 295, 1228, 341, 4974, 2144, 13], "temperature": 0.0, "avg_logprob": -0.36468593052455356, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.6473149521043524e-05}, {"id": 273, "seek": 111916, "start": 1129.48, "end": 1134.1200000000001, "text": " Like there's another argument that maybe we always should use this.", "tokens": [1743, 456, 311, 1071, 6770, 300, 1310, 321, 1009, 820, 764, 341, 13], "temperature": 0.0, "avg_logprob": -0.36468593052455356, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.6473149521043524e-05}, {"id": 274, "seek": 111916, "start": 1134.1200000000001, "end": 1135.1200000000001, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.36468593052455356, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.6473149521043524e-05}, {"id": 275, "seek": 111916, "start": 1135.1200000000001, "end": 1136.1200000000001, "text": " Now that you've got it working.", "tokens": [823, 300, 291, 600, 658, 309, 1364, 13], "temperature": 0.0, "avg_logprob": -0.36468593052455356, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.6473149521043524e-05}, {"id": 276, "seek": 111916, "start": 1136.1200000000001, "end": 1140.5600000000002, "text": " It's kind of hard to find a good working version and you did a lot of work to get it", "tokens": [467, 311, 733, 295, 1152, 281, 915, 257, 665, 1364, 3037, 293, 291, 630, 257, 688, 295, 589, 281, 483, 309], "temperature": 0.0, "avg_logprob": -0.36468593052455356, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.6473149521043524e-05}, {"id": 277, "seek": 111916, "start": 1140.5600000000002, "end": 1141.5600000000002, "text": " to work properly.", "tokens": [281, 589, 6108, 13], "temperature": 0.0, "avg_logprob": -0.36468593052455356, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.6473149521043524e-05}, {"id": 278, "seek": 111916, "start": 1141.5600000000002, "end": 1142.5600000000002, "text": " Now it does maybe.", "tokens": [823, 309, 775, 1310, 13], "temperature": 0.0, "avg_logprob": -0.36468593052455356, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.6473149521043524e-05}, {"id": 279, "seek": 111916, "start": 1142.5600000000002, "end": 1144.5600000000002, "text": " Maybe this is the best place to start.", "tokens": [2704, 341, 307, 264, 1151, 1081, 281, 722, 13], "temperature": 0.0, "avg_logprob": -0.36468593052455356, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.6473149521043524e-05}, {"id": 280, "seek": 114456, "start": 1144.56, "end": 1151.3999999999999, "text": " Yeah, I mean I think I would still, just because SVD is so easy to run, you know, when you're", "tokens": [865, 11, 286, 914, 286, 519, 286, 576, 920, 11, 445, 570, 31910, 35, 307, 370, 1858, 281, 1190, 11, 291, 458, 11, 562, 291, 434], "temperature": 0.0, "avg_logprob": -0.24407738133480675, "compression_ratio": 1.6308243727598566, "no_speech_prob": 2.2826136046205647e-05}, {"id": 281, "seek": 114456, "start": 1151.3999999999999, "end": 1155.9199999999998, "text": " using someone else's implementation, I think I would probably run SVD on your problem just", "tokens": [1228, 1580, 1646, 311, 11420, 11, 286, 519, 286, 576, 1391, 1190, 31910, 35, 322, 428, 1154, 445], "temperature": 0.0, "avg_logprob": -0.24407738133480675, "compression_ratio": 1.6308243727598566, "no_speech_prob": 2.2826136046205647e-05}, {"id": 282, "seek": 114456, "start": 1155.9199999999998, "end": 1158.76, "text": " to make sure like, okay, I want something better.", "tokens": [281, 652, 988, 411, 11, 1392, 11, 286, 528, 746, 1101, 13], "temperature": 0.0, "avg_logprob": -0.24407738133480675, "compression_ratio": 1.6308243727598566, "no_speech_prob": 2.2826136046205647e-05}, {"id": 283, "seek": 114456, "start": 1158.76, "end": 1165.28, "text": " But yeah, no, I mean robust PCA has a lot of advantages and there are a lot of places", "tokens": [583, 1338, 11, 572, 11, 286, 914, 13956, 6465, 32, 575, 257, 688, 295, 14906, 293, 456, 366, 257, 688, 295, 3190], "temperature": 0.0, "avg_logprob": -0.24407738133480675, "compression_ratio": 1.6308243727598566, "no_speech_prob": 2.2826136046205647e-05}, {"id": 284, "seek": 114456, "start": 1165.28, "end": 1166.28, "text": " it's applicable.", "tokens": [309, 311, 21142, 13], "temperature": 0.0, "avg_logprob": -0.24407738133480675, "compression_ratio": 1.6308243727598566, "no_speech_prob": 2.2826136046205647e-05}, {"id": 285, "seek": 114456, "start": 1166.28, "end": 1167.28, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.24407738133480675, "compression_ratio": 1.6308243727598566, "no_speech_prob": 2.2826136046205647e-05}, {"id": 286, "seek": 114456, "start": 1167.28, "end": 1168.28, "text": " It's not like robust versus classical PCA.", "tokens": [467, 311, 406, 411, 13956, 5717, 13735, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.24407738133480675, "compression_ratio": 1.6308243727598566, "no_speech_prob": 2.2826136046205647e-05}, {"id": 287, "seek": 114456, "start": 1168.28, "end": 1169.28, "text": " Oh, yeah.", "tokens": [876, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.24407738133480675, "compression_ratio": 1.6308243727598566, "no_speech_prob": 2.2826136046205647e-05}, {"id": 288, "seek": 114456, "start": 1169.28, "end": 1170.28, "text": " Why don't you use robust?", "tokens": [1545, 500, 380, 291, 764, 13956, 30], "temperature": 0.0, "avg_logprob": -0.24407738133480675, "compression_ratio": 1.6308243727598566, "no_speech_prob": 2.2826136046205647e-05}, {"id": 289, "seek": 114456, "start": 1170.28, "end": 1171.28, "text": " How about anyone use classical?", "tokens": [1012, 466, 2878, 764, 13735, 30], "temperature": 0.0, "avg_logprob": -0.24407738133480675, "compression_ratio": 1.6308243727598566, "no_speech_prob": 2.2826136046205647e-05}, {"id": 290, "seek": 117128, "start": 1171.28, "end": 1178.04, "text": " I mean since like classical PCA though is kind of a, you know, this is like very basic", "tokens": [286, 914, 1670, 411, 13735, 6465, 32, 1673, 307, 733, 295, 257, 11, 291, 458, 11, 341, 307, 411, 588, 3875], "temperature": 0.0, "avg_logprob": -0.23126731798486802, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.3496550081981695e-06}, {"id": 291, "seek": 117128, "start": 1178.04, "end": 1181.84, "text": " SVD, that like that is something.", "tokens": [31910, 35, 11, 300, 411, 300, 307, 746, 13], "temperature": 0.0, "avg_logprob": -0.23126731798486802, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.3496550081981695e-06}, {"id": 292, "seek": 117128, "start": 1181.84, "end": 1187.12, "text": " I think I was even with the background removal problem and you shouldn't do this, but I actually", "tokens": [286, 519, 286, 390, 754, 365, 264, 3678, 17933, 1154, 293, 291, 4659, 380, 360, 341, 11, 457, 286, 767], "temperature": 0.0, "avg_logprob": -0.23126731798486802, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.3496550081981695e-06}, {"id": 293, "seek": 117128, "start": 1187.12, "end": 1191.08, "text": " did the robust PCA first and then was like, oh, let me see how I do on randomized SVD.", "tokens": [630, 264, 13956, 6465, 32, 700, 293, 550, 390, 411, 11, 1954, 11, 718, 385, 536, 577, 286, 360, 322, 38513, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.23126731798486802, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.3496550081981695e-06}, {"id": 294, "seek": 117128, "start": 1191.08, "end": 1195.56, "text": " And then was like, oh, randomized SVD was actually better than I expected.", "tokens": [400, 550, 390, 411, 11, 1954, 11, 38513, 31910, 35, 390, 767, 1101, 813, 286, 5176, 13], "temperature": 0.0, "avg_logprob": -0.23126731798486802, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.3496550081981695e-06}, {"id": 295, "seek": 119556, "start": 1195.56, "end": 1203.3999999999999, "text": " So that's a up here, like this is what I was getting from, yeah, just like the randomized", "tokens": [407, 300, 311, 257, 493, 510, 11, 411, 341, 307, 437, 286, 390, 1242, 490, 11, 1338, 11, 445, 411, 264, 38513], "temperature": 0.0, "avg_logprob": -0.3146111314946955, "compression_ratio": 1.494949494949495, "no_speech_prob": 2.260295104861143e-06}, {"id": 296, "seek": 119556, "start": 1203.3999999999999, "end": 1205.24, "text": " SVD, which is like a single line.", "tokens": [31910, 35, 11, 597, 307, 411, 257, 2167, 1622, 13], "temperature": 0.0, "avg_logprob": -0.3146111314946955, "compression_ratio": 1.494949494949495, "no_speech_prob": 2.260295104861143e-06}, {"id": 297, "seek": 119556, "start": 1205.24, "end": 1211.96, "text": " But yeah, like I do think robust PCA is a great algorithm to use.", "tokens": [583, 1338, 11, 411, 286, 360, 519, 13956, 6465, 32, 307, 257, 869, 9284, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.3146111314946955, "compression_ratio": 1.494949494949495, "no_speech_prob": 2.260295104861143e-06}, {"id": 298, "seek": 119556, "start": 1211.96, "end": 1214.6799999999998, "text": " Yeah, good question.", "tokens": [865, 11, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3146111314946955, "compression_ratio": 1.494949494949495, "no_speech_prob": 2.260295104861143e-06}, {"id": 299, "seek": 119556, "start": 1214.6799999999998, "end": 1216.76, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.3146111314946955, "compression_ratio": 1.494949494949495, "no_speech_prob": 2.260295104861143e-06}, {"id": 300, "seek": 119556, "start": 1216.76, "end": 1222.1599999999999, "text": " Yeah, let me maybe go back through some of the steps of the algorithm now.", "tokens": [865, 11, 718, 385, 1310, 352, 646, 807, 512, 295, 264, 4439, 295, 264, 9284, 586, 13], "temperature": 0.0, "avg_logprob": -0.3146111314946955, "compression_ratio": 1.494949494949495, "no_speech_prob": 2.260295104861143e-06}, {"id": 301, "seek": 122216, "start": 1222.16, "end": 1226.4, "text": " And this I'm going to kind of display some different information than I did last time.", "tokens": [400, 341, 286, 478, 516, 281, 733, 295, 4674, 512, 819, 1589, 813, 286, 630, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.1584041002884652, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.2411125680955593e-05}, {"id": 302, "seek": 122216, "start": 1226.4, "end": 1228.8400000000001, "text": " So hopefully that'll be helpful to get a different perspective.", "tokens": [407, 4696, 300, 603, 312, 4961, 281, 483, 257, 819, 4585, 13], "temperature": 0.0, "avg_logprob": -0.1584041002884652, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.2411125680955593e-05}, {"id": 303, "seek": 122216, "start": 1228.8400000000001, "end": 1237.88, "text": " But yeah, just to remind you, so at the top we had kind of first went through kind of", "tokens": [583, 1338, 11, 445, 281, 4160, 291, 11, 370, 412, 264, 1192, 321, 632, 733, 295, 700, 1437, 807, 733, 295], "temperature": 0.0, "avg_logprob": -0.1584041002884652, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.2411125680955593e-05}, {"id": 304, "seek": 122216, "start": 1237.88, "end": 1242.0800000000002, "text": " how like what does our data even mean, how we're setting it up, you know, and turning", "tokens": [577, 411, 437, 775, 527, 1412, 754, 914, 11, 577, 321, 434, 3287, 309, 493, 11, 291, 458, 11, 293, 6246], "temperature": 0.0, "avg_logprob": -0.1584041002884652, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.2411125680955593e-05}, {"id": 305, "seek": 122216, "start": 1242.0800000000002, "end": 1249.3200000000002, "text": " this video into a single matrix where each column is one point in time, you know, and", "tokens": [341, 960, 666, 257, 2167, 8141, 689, 1184, 7738, 307, 472, 935, 294, 565, 11, 291, 458, 11, 293], "temperature": 0.0, "avg_logprob": -0.1584041002884652, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.2411125680955593e-05}, {"id": 306, "seek": 124932, "start": 1249.32, "end": 1253.2, "text": " we've kind of unwrapped the pixels to just make a straight column from each point in", "tokens": [321, 600, 733, 295, 14853, 424, 3320, 264, 18668, 281, 445, 652, 257, 2997, 7738, 490, 1184, 935, 294], "temperature": 0.0, "avg_logprob": -0.18247557339602954, "compression_ratio": 1.3898305084745763, "no_speech_prob": 5.0145563363912515e-06}, {"id": 307, "seek": 124932, "start": 1253.2, "end": 1254.9199999999998, "text": " time.", "tokens": [565, 13], "temperature": 0.0, "avg_logprob": -0.18247557339602954, "compression_ratio": 1.3898305084745763, "no_speech_prob": 5.0145563363912515e-06}, {"id": 308, "seek": 124932, "start": 1254.9199999999998, "end": 1257.76, "text": " And we tried it with a rank.", "tokens": [400, 321, 3031, 309, 365, 257, 6181, 13], "temperature": 0.0, "avg_logprob": -0.18247557339602954, "compression_ratio": 1.3898305084745763, "no_speech_prob": 5.0145563363912515e-06}, {"id": 309, "seek": 124932, "start": 1257.76, "end": 1266.36, "text": " Here's a rank one approximation, just using a randomized SVD.", "tokens": [1692, 311, 257, 6181, 472, 28023, 11, 445, 1228, 257, 38513, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.18247557339602954, "compression_ratio": 1.3898305084745763, "no_speech_prob": 5.0145563363912515e-06}, {"id": 310, "seek": 124932, "start": 1266.36, "end": 1272.52, "text": " And then we introduced the idea of robust PCA.", "tokens": [400, 550, 321, 7268, 264, 1558, 295, 13956, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.18247557339602954, "compression_ratio": 1.3898305084745763, "no_speech_prob": 5.0145563363912515e-06}, {"id": 311, "seek": 124932, "start": 1272.52, "end": 1278.04, "text": " We saw the faces.", "tokens": [492, 1866, 264, 8475, 13], "temperature": 0.0, "avg_logprob": -0.18247557339602954, "compression_ratio": 1.3898305084745763, "no_speech_prob": 5.0145563363912515e-06}, {"id": 312, "seek": 127804, "start": 1278.04, "end": 1279.92, "text": " And then I'll just show this again.", "tokens": [400, 550, 286, 603, 445, 855, 341, 797, 13], "temperature": 0.0, "avg_logprob": -0.3066498271205969, "compression_ratio": 1.4451612903225806, "no_speech_prob": 5.1425955462036654e-05}, {"id": 313, "seek": 127804, "start": 1279.92, "end": 1285.6399999999999, "text": " If you are really interested in optimization, these are Jupyter notebooks from Stephen Boyd,", "tokens": [759, 291, 366, 534, 3102, 294, 19618, 11, 613, 366, 22125, 88, 391, 43782, 490, 13391, 9486, 67, 11], "temperature": 0.0, "avg_logprob": -0.3066498271205969, "compression_ratio": 1.4451612903225806, "no_speech_prob": 5.1425955462036654e-05}, {"id": 314, "seek": 127804, "start": 1285.6399999999999, "end": 1292.44, "text": " who's at Stanford, has a convex optimization short course, which looks really interesting.", "tokens": [567, 311, 412, 20374, 11, 575, 257, 42432, 19618, 2099, 1164, 11, 597, 1542, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.3066498271205969, "compression_ratio": 1.4451612903225806, "no_speech_prob": 5.1425955462036654e-05}, {"id": 315, "seek": 127804, "start": 1292.44, "end": 1294.48, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.3066498271205969, "compression_ratio": 1.4451612903225806, "no_speech_prob": 5.1425955462036654e-05}, {"id": 316, "seek": 129448, "start": 1294.48, "end": 1309.24, "text": " When I tried to read Stephen Boyd's book and watch his Stanford class, I found it kind", "tokens": [1133, 286, 3031, 281, 1401, 13391, 9486, 67, 311, 1446, 293, 1159, 702, 20374, 1508, 11, 286, 1352, 309, 733], "temperature": 0.0, "avg_logprob": -0.36518349044624415, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.1298527169856243e-05}, {"id": 317, "seek": 129448, "start": 1309.24, "end": 1310.24, "text": " of inaccessible.", "tokens": [295, 33230, 780, 964, 13], "temperature": 0.0, "avg_logprob": -0.36518349044624415, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.1298527169856243e-05}, {"id": 318, "seek": 129448, "start": 1310.24, "end": 1311.88, "text": " Matthew, did you see what these notebooks are?", "tokens": [12434, 11, 630, 291, 536, 437, 613, 43782, 366, 30], "temperature": 0.0, "avg_logprob": -0.36518349044624415, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.1298527169856243e-05}, {"id": 319, "seek": 129448, "start": 1311.88, "end": 1314.1200000000001, "text": " No, I did not look in detail at the notebook.", "tokens": [883, 11, 286, 630, 406, 574, 294, 2607, 412, 264, 21060, 13], "temperature": 0.0, "avg_logprob": -0.36518349044624415, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.1298527169856243e-05}, {"id": 320, "seek": 129448, "start": 1314.1200000000001, "end": 1316.08, "text": " So yeah, that's a good warning to have.", "tokens": [407, 1338, 11, 300, 311, 257, 665, 9164, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.36518349044624415, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.1298527169856243e-05}, {"id": 321, "seek": 129448, "start": 1316.08, "end": 1324.32, "text": " If you put on your stuff and have trouble, I know I did.", "tokens": [759, 291, 829, 322, 428, 1507, 293, 362, 5253, 11, 286, 458, 286, 630, 13], "temperature": 0.0, "avg_logprob": -0.36518349044624415, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.1298527169856243e-05}, {"id": 322, "seek": 132432, "start": 1324.32, "end": 1329.6399999999999, "text": " Okay, so this, and actually, I guess, before I get into the heart of the algorithm, I wanted", "tokens": [1033, 11, 370, 341, 11, 293, 767, 11, 286, 2041, 11, 949, 286, 483, 666, 264, 1917, 295, 264, 9284, 11, 286, 1415], "temperature": 0.0, "avg_logprob": -0.19269392423540632, "compression_ratio": 1.6916666666666667, "no_speech_prob": 5.5619224440306425e-05}, {"id": 323, "seek": 132432, "start": 1329.6399999999999, "end": 1336.76, "text": " to say, so the authors kind of define what they call a shrinkage method.", "tokens": [281, 584, 11, 370, 264, 16552, 733, 295, 6964, 437, 436, 818, 257, 23060, 609, 3170, 13], "temperature": 0.0, "avg_logprob": -0.19269392423540632, "compression_ratio": 1.6916666666666667, "no_speech_prob": 5.5619224440306425e-05}, {"id": 324, "seek": 132432, "start": 1336.76, "end": 1345.56, "text": " And what that is is basically they take any singular values that are less than tau and", "tokens": [400, 437, 300, 307, 307, 1936, 436, 747, 604, 20010, 4190, 300, 366, 1570, 813, 17842, 293], "temperature": 0.0, "avg_logprob": -0.19269392423540632, "compression_ratio": 1.6916666666666667, "no_speech_prob": 5.5619224440306425e-05}, {"id": 325, "seek": 132432, "start": 1345.56, "end": 1347.32, "text": " round them down to zero.", "tokens": [3098, 552, 760, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.19269392423540632, "compression_ratio": 1.6916666666666667, "no_speech_prob": 5.5619224440306425e-05}, {"id": 326, "seek": 132432, "start": 1347.32, "end": 1351.1599999999999, "text": " And so you can think of that as another way of truncating your matrix, as like, kind of", "tokens": [400, 370, 291, 393, 519, 295, 300, 382, 1071, 636, 295, 504, 409, 66, 990, 428, 8141, 11, 382, 411, 11, 733, 295], "temperature": 0.0, "avg_logprob": -0.19269392423540632, "compression_ratio": 1.6916666666666667, "no_speech_prob": 5.5619224440306425e-05}, {"id": 327, "seek": 132432, "start": 1351.1599999999999, "end": 1353.8, "text": " just ignoring the small singular values.", "tokens": [445, 26258, 264, 1359, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.19269392423540632, "compression_ratio": 1.6916666666666667, "no_speech_prob": 5.5619224440306425e-05}, {"id": 328, "seek": 135380, "start": 1353.8, "end": 1355.48, "text": " So this shrinkage method will show up.", "tokens": [407, 341, 23060, 609, 3170, 486, 855, 493, 13], "temperature": 0.0, "avg_logprob": -0.2927257061004639, "compression_ratio": 1.421875, "no_speech_prob": 6.0477682382042985e-06}, {"id": 329, "seek": 135380, "start": 1355.48, "end": 1358.52, "text": " Got a question, which is where to find that Stephen Boyd notebook?", "tokens": [5803, 257, 1168, 11, 597, 307, 689, 281, 915, 300, 13391, 9486, 67, 21060, 30], "temperature": 0.0, "avg_logprob": -0.2927257061004639, "compression_ratio": 1.421875, "no_speech_prob": 6.0477682382042985e-06}, {"id": 330, "seek": 135380, "start": 1358.52, "end": 1361.44, "text": " Oh, there are links to it in here.", "tokens": [876, 11, 456, 366, 6123, 281, 309, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.2927257061004639, "compression_ratio": 1.421875, "no_speech_prob": 6.0477682382042985e-06}, {"id": 331, "seek": 135380, "start": 1361.44, "end": 1363.72, "text": " Let me find the section.", "tokens": [961, 385, 915, 264, 3541, 13], "temperature": 0.0, "avg_logprob": -0.2927257061004639, "compression_ratio": 1.421875, "no_speech_prob": 6.0477682382042985e-06}, {"id": 332, "seek": 135380, "start": 1363.72, "end": 1368.24, "text": " So right above robust PCA, I have something that says, if you want to learn more of the", "tokens": [407, 558, 3673, 13956, 6465, 32, 11, 286, 362, 746, 300, 1619, 11, 498, 291, 528, 281, 1466, 544, 295, 264], "temperature": 0.0, "avg_logprob": -0.2927257061004639, "compression_ratio": 1.421875, "no_speech_prob": 6.0477682382042985e-06}, {"id": 333, "seek": 135380, "start": 1368.24, "end": 1382.8, "text": " theory, which is...", "tokens": [5261, 11, 597, 307, 485], "temperature": 0.0, "avg_logprob": -0.2927257061004639, "compression_ratio": 1.421875, "no_speech_prob": 6.0477682382042985e-06}, {"id": 334, "seek": 138280, "start": 1382.8, "end": 1386.04, "text": " And some of these, I think I've mentioned this before, but I'm using...", "tokens": [400, 512, 295, 613, 11, 286, 519, 286, 600, 2835, 341, 949, 11, 457, 286, 478, 1228, 485], "temperature": 0.0, "avg_logprob": -0.18525834810935846, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8630598788149655e-05}, {"id": 335, "seek": 138280, "start": 1386.04, "end": 1391.72, "text": " Jupyter Notebooks has some extensions, and I'm using one that has the section folding,", "tokens": [22125, 88, 391, 11633, 15170, 575, 512, 25129, 11, 293, 286, 478, 1228, 472, 300, 575, 264, 3541, 25335, 11], "temperature": 0.0, "avg_logprob": -0.18525834810935846, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8630598788149655e-05}, {"id": 336, "seek": 138280, "start": 1391.72, "end": 1393.24, "text": " which is really handy.", "tokens": [597, 307, 534, 13239, 13], "temperature": 0.0, "avg_logprob": -0.18525834810935846, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8630598788149655e-05}, {"id": 337, "seek": 138280, "start": 1393.24, "end": 1398.84, "text": " Collapsible headings is what they're called for being able to fold these up and down.", "tokens": [4586, 2382, 964, 1378, 1109, 307, 437, 436, 434, 1219, 337, 885, 1075, 281, 4860, 613, 493, 293, 760, 13], "temperature": 0.0, "avg_logprob": -0.18525834810935846, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8630598788149655e-05}, {"id": 338, "seek": 138280, "start": 1398.84, "end": 1401.72, "text": " So that's definitely something you need to kind of install that to get it, but it makes", "tokens": [407, 300, 311, 2138, 746, 291, 643, 281, 733, 295, 3625, 300, 281, 483, 309, 11, 457, 309, 1669], "temperature": 0.0, "avg_logprob": -0.18525834810935846, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8630598788149655e-05}, {"id": 339, "seek": 138280, "start": 1401.72, "end": 1402.72, "text": " it much easier.", "tokens": [309, 709, 3571, 13], "temperature": 0.0, "avg_logprob": -0.18525834810935846, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8630598788149655e-05}, {"id": 340, "seek": 138280, "start": 1402.72, "end": 1409.36, "text": " Then you can kind of like open and close sections and navigate around pretty easily.", "tokens": [1396, 291, 393, 733, 295, 411, 1269, 293, 1998, 10863, 293, 12350, 926, 1238, 3612, 13], "temperature": 0.0, "avg_logprob": -0.18525834810935846, "compression_ratio": 1.6402877697841727, "no_speech_prob": 1.8630598788149655e-05}, {"id": 341, "seek": 140936, "start": 1409.36, "end": 1421.36, "text": " And actually, let me just show maybe this high level of the algorithm.", "tokens": [400, 767, 11, 718, 385, 445, 855, 1310, 341, 1090, 1496, 295, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.15965778796703783, "compression_ratio": 1.5365853658536586, "no_speech_prob": 9.368478458782192e-06}, {"id": 342, "seek": 140936, "start": 1421.36, "end": 1424.8799999999999, "text": " So what we're doing for our low rank approximation is taking...", "tokens": [407, 437, 321, 434, 884, 337, 527, 2295, 6181, 28023, 307, 1940, 485], "temperature": 0.0, "avg_logprob": -0.15965778796703783, "compression_ratio": 1.5365853658536586, "no_speech_prob": 9.368478458782192e-06}, {"id": 343, "seek": 140936, "start": 1424.8799999999999, "end": 1431.6, "text": " So here they call this curly D, singular value thresholding, but that's basically where you", "tokens": [407, 510, 436, 818, 341, 32066, 413, 11, 20010, 2158, 14678, 278, 11, 457, 300, 311, 1936, 689, 291], "temperature": 0.0, "avg_logprob": -0.15965778796703783, "compression_ratio": 1.5365853658536586, "no_speech_prob": 9.368478458782192e-06}, {"id": 344, "seek": 140936, "start": 1431.6, "end": 1437.32, "text": " just take the SVD, do this shrinkage operation on the singular values, so throw away the", "tokens": [445, 747, 264, 31910, 35, 11, 360, 341, 23060, 609, 6916, 322, 264, 20010, 4190, 11, 370, 3507, 1314, 264], "temperature": 0.0, "avg_logprob": -0.15965778796703783, "compression_ratio": 1.5365853658536586, "no_speech_prob": 9.368478458782192e-06}, {"id": 345, "seek": 143732, "start": 1437.32, "end": 1441.8799999999999, "text": " ones that are too little, multiply back together to reconstruct your matrix, and that's the", "tokens": [2306, 300, 366, 886, 707, 11, 12972, 646, 1214, 281, 31499, 428, 8141, 11, 293, 300, 311, 264], "temperature": 0.0, "avg_logprob": -0.13661969645639485, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.300689018535195e-05}, {"id": 346, "seek": 143732, "start": 1441.8799999999999, "end": 1445.8, "text": " approximation for the low rank matrix.", "tokens": [28023, 337, 264, 2295, 6181, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13661969645639485, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.300689018535195e-05}, {"id": 347, "seek": 143732, "start": 1445.8, "end": 1451.72, "text": " The approximation for the sparse matrix is to take the...", "tokens": [440, 28023, 337, 264, 637, 11668, 8141, 307, 281, 747, 264, 485], "temperature": 0.0, "avg_logprob": -0.13661969645639485, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.300689018535195e-05}, {"id": 348, "seek": 143732, "start": 1451.72, "end": 1456.6799999999998, "text": " Just do the shrinkage operator of throwing away the small singular values.", "tokens": [1449, 360, 264, 23060, 609, 12973, 295, 10238, 1314, 264, 1359, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.13661969645639485, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.300689018535195e-05}, {"id": 349, "seek": 143732, "start": 1456.6799999999998, "end": 1457.6799999999998, "text": " And this is the...", "tokens": [400, 341, 307, 264, 485], "temperature": 0.0, "avg_logprob": -0.13661969645639485, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.300689018535195e-05}, {"id": 350, "seek": 143732, "start": 1457.6799999999998, "end": 1462.4399999999998, "text": " Sorry, I should say the alternating aspect is to estimate the low rank one, you're taking", "tokens": [4919, 11, 286, 820, 584, 264, 40062, 4171, 307, 281, 12539, 264, 2295, 6181, 472, 11, 291, 434, 1940], "temperature": 0.0, "avg_logprob": -0.13661969645639485, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.300689018535195e-05}, {"id": 351, "seek": 146244, "start": 1462.44, "end": 1467.3200000000002, "text": " your full matrix minus the sparse one, kind of minus this error that you're keeping track", "tokens": [428, 1577, 8141, 3175, 264, 637, 11668, 472, 11, 733, 295, 3175, 341, 6713, 300, 291, 434, 5145, 2837], "temperature": 0.0, "avg_logprob": -0.10728858689130363, "compression_ratio": 2.1359223300970873, "no_speech_prob": 3.500814955259557e-06}, {"id": 352, "seek": 146244, "start": 1467.3200000000002, "end": 1472.72, "text": " of, and then you kind of alternate back to, okay, let's estimate the sparse one, that's", "tokens": [295, 11, 293, 550, 291, 733, 295, 18873, 646, 281, 11, 1392, 11, 718, 311, 12539, 264, 637, 11668, 472, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.10728858689130363, "compression_ratio": 2.1359223300970873, "no_speech_prob": 3.500814955259557e-06}, {"id": 353, "seek": 146244, "start": 1472.72, "end": 1475.56, "text": " the full rank matrix minus the low rank one.", "tokens": [264, 1577, 6181, 8141, 3175, 264, 2295, 6181, 472, 13], "temperature": 0.0, "avg_logprob": -0.10728858689130363, "compression_ratio": 2.1359223300970873, "no_speech_prob": 3.500814955259557e-06}, {"id": 354, "seek": 146244, "start": 1475.56, "end": 1480.44, "text": " And you're kind of going back and forth between, let me estimate the low rank one using the", "tokens": [400, 291, 434, 733, 295, 516, 646, 293, 5220, 1296, 11, 718, 385, 12539, 264, 2295, 6181, 472, 1228, 264], "temperature": 0.0, "avg_logprob": -0.10728858689130363, "compression_ratio": 2.1359223300970873, "no_speech_prob": 3.500814955259557e-06}, {"id": 355, "seek": 146244, "start": 1480.44, "end": 1488.66, "text": " sparse one, and vice versa, let me estimate the sparse one using the low rank one.", "tokens": [637, 11668, 472, 11, 293, 11964, 25650, 11, 718, 385, 12539, 264, 637, 11668, 472, 1228, 264, 2295, 6181, 472, 13], "temperature": 0.0, "avg_logprob": -0.10728858689130363, "compression_ratio": 2.1359223300970873, "no_speech_prob": 3.500814955259557e-06}, {"id": 356, "seek": 146244, "start": 1488.66, "end": 1491.1200000000001, "text": " And so kind of how that looks in the code.", "tokens": [400, 370, 733, 295, 577, 300, 1542, 294, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.10728858689130363, "compression_ratio": 2.1359223300970873, "no_speech_prob": 3.500814955259557e-06}, {"id": 357, "seek": 149112, "start": 1491.12, "end": 1496.4799999999998, "text": " So this PCP, which stands for Principle Component Pursuit, and I should say Principle Component", "tokens": [407, 341, 6465, 47, 11, 597, 7382, 337, 38372, 781, 6620, 30365, 430, 2156, 1983, 11, 293, 286, 820, 584, 38372, 781, 6620, 30365], "temperature": 0.0, "avg_logprob": -0.15845666237927358, "compression_ratio": 1.7901785714285714, "no_speech_prob": 4.092848030268215e-06}, {"id": 358, "seek": 149112, "start": 1496.4799999999998, "end": 1502.76, "text": " Pursuit is just one algorithm for robust PCA, so robust PCA is kind of referring to this", "tokens": [430, 2156, 1983, 307, 445, 472, 9284, 337, 13956, 6465, 32, 11, 370, 13956, 6465, 32, 307, 733, 295, 13761, 281, 341], "temperature": 0.0, "avg_logprob": -0.15845666237927358, "compression_ratio": 1.7901785714285714, "no_speech_prob": 4.092848030268215e-06}, {"id": 359, "seek": 149112, "start": 1502.76, "end": 1505.9599999999998, "text": " decomposition we want, and there are different algorithms to get there.", "tokens": [48356, 321, 528, 11, 293, 456, 366, 819, 14642, 281, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.15845666237927358, "compression_ratio": 1.7901785714285714, "no_speech_prob": 4.092848030268215e-06}, {"id": 360, "seek": 149112, "start": 1505.9599999999998, "end": 1511.32, "text": " And in this class we're using Principle Component Pursuit.", "tokens": [400, 294, 341, 1508, 321, 434, 1228, 38372, 781, 6620, 30365, 430, 2156, 1983, 13], "temperature": 0.0, "avg_logprob": -0.15845666237927358, "compression_ratio": 1.7901785714285714, "no_speech_prob": 4.092848030268215e-06}, {"id": 361, "seek": 149112, "start": 1511.32, "end": 1516.9599999999998, "text": " Kind of the heart of it is we've got this for loop, and so then I picked off a few of", "tokens": [9242, 295, 264, 1917, 295, 309, 307, 321, 600, 658, 341, 337, 6367, 11, 293, 370, 550, 286, 6183, 766, 257, 1326, 295], "temperature": 0.0, "avg_logprob": -0.15845666237927358, "compression_ratio": 1.7901785714285714, "no_speech_prob": 4.092848030268215e-06}, {"id": 362, "seek": 151696, "start": 1516.96, "end": 1522.56, "text": " the key things and added comments, and this has been updated on GitHub, but I said here,", "tokens": [264, 2141, 721, 293, 3869, 3053, 11, 293, 341, 575, 668, 10588, 322, 23331, 11, 457, 286, 848, 510, 11], "temperature": 0.0, "avg_logprob": -0.08764613677408094, "compression_ratio": 1.7605042016806722, "no_speech_prob": 1.723092282190919e-05}, {"id": 363, "seek": 151696, "start": 1522.56, "end": 1528.4, "text": " you know, we're updating our estimate of the sparse matrix by shrinking slash truncating", "tokens": [291, 458, 11, 321, 434, 25113, 527, 12539, 295, 264, 637, 11668, 8141, 538, 41684, 17330, 504, 409, 66, 990], "temperature": 0.0, "avg_logprob": -0.08764613677408094, "compression_ratio": 1.7605042016806722, "no_speech_prob": 1.723092282190919e-05}, {"id": 364, "seek": 151696, "start": 1528.4, "end": 1533.76, "text": " the original, sorry, the original minus the low rank, so we're getting kind of the difference", "tokens": [264, 3380, 11, 2597, 11, 264, 3380, 3175, 264, 2295, 6181, 11, 370, 321, 434, 1242, 733, 295, 264, 2649], "temperature": 0.0, "avg_logprob": -0.08764613677408094, "compression_ratio": 1.7605042016806722, "no_speech_prob": 1.723092282190919e-05}, {"id": 365, "seek": 151696, "start": 1533.76, "end": 1534.76, "text": " between that.", "tokens": [1296, 300, 13], "temperature": 0.0, "avg_logprob": -0.08764613677408094, "compression_ratio": 1.7605042016806722, "no_speech_prob": 1.723092282190919e-05}, {"id": 366, "seek": 151696, "start": 1534.76, "end": 1541.44, "text": " And remember, we're going for original equals low rank plus sparse, so whenever we do original", "tokens": [400, 1604, 11, 321, 434, 516, 337, 3380, 6915, 2295, 6181, 1804, 637, 11668, 11, 370, 5699, 321, 360, 3380], "temperature": 0.0, "avg_logprob": -0.08764613677408094, "compression_ratio": 1.7605042016806722, "no_speech_prob": 1.723092282190919e-05}, {"id": 367, "seek": 151696, "start": 1541.44, "end": 1544.64, "text": " minus low rank, that should be sparse.", "tokens": [3175, 2295, 6181, 11, 300, 820, 312, 637, 11668, 13], "temperature": 0.0, "avg_logprob": -0.08764613677408094, "compression_ratio": 1.7605042016806722, "no_speech_prob": 1.723092282190919e-05}, {"id": 368, "seek": 154464, "start": 1544.64, "end": 1551.68, "text": " Then we update our estimate of the low rank matrix by doing the truncated SVD and reconstructing", "tokens": [1396, 321, 5623, 527, 12539, 295, 264, 2295, 6181, 8141, 538, 884, 264, 504, 409, 66, 770, 31910, 35, 293, 31499, 278], "temperature": 0.0, "avg_logprob": -0.10910965005556743, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.7264172786090057e-06}, {"id": 369, "seek": 154464, "start": 1551.68, "end": 1553.3600000000001, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.10910965005556743, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.7264172786090057e-06}, {"id": 370, "seek": 154464, "start": 1553.3600000000001, "end": 1559.48, "text": " And then something that kind of makes this more intricate is we don't actually want to", "tokens": [400, 550, 746, 300, 733, 295, 1669, 341, 544, 38015, 307, 321, 500, 380, 767, 528, 281], "temperature": 0.0, "avg_logprob": -0.10910965005556743, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.7264172786090057e-06}, {"id": 371, "seek": 154464, "start": 1559.48, "end": 1564.3200000000002, "text": " calculate the full SVD, so we need to say how many, so we're using a randomized SVD,", "tokens": [8873, 264, 1577, 31910, 35, 11, 370, 321, 643, 281, 584, 577, 867, 11, 370, 321, 434, 1228, 257, 38513, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.10910965005556743, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.7264172786090057e-06}, {"id": 372, "seek": 154464, "start": 1564.3200000000002, "end": 1568.68, "text": " but with randomized SVD we need to say how many singular values we want, so we're giving", "tokens": [457, 365, 38513, 31910, 35, 321, 643, 281, 584, 577, 867, 20010, 4190, 321, 528, 11, 370, 321, 434, 2902], "temperature": 0.0, "avg_logprob": -0.10910965005556743, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.7264172786090057e-06}, {"id": 373, "seek": 156868, "start": 1568.68, "end": 1575.0, "text": " it a rank that we're interested in, and here that's called SV.", "tokens": [309, 257, 6181, 300, 321, 434, 3102, 294, 11, 293, 510, 300, 311, 1219, 31910, 13], "temperature": 0.0, "avg_logprob": -0.1458002453758603, "compression_ratio": 1.5530973451327434, "no_speech_prob": 2.0580223463184666e-06}, {"id": 374, "seek": 156868, "start": 1575.0, "end": 1579.72, "text": " And so SV is something that we're going to update each time, because no, it would be,", "tokens": [400, 370, 31910, 307, 746, 300, 321, 434, 516, 281, 5623, 1184, 565, 11, 570, 572, 11, 309, 576, 312, 11], "temperature": 0.0, "avg_logprob": -0.1458002453758603, "compression_ratio": 1.5530973451327434, "no_speech_prob": 2.0580223463184666e-06}, {"id": 375, "seek": 156868, "start": 1579.72, "end": 1587.28, "text": " if we try to remember the dimensions of this matrix, I believe it was 4,800 by 11,300.", "tokens": [498, 321, 853, 281, 1604, 264, 12819, 295, 341, 8141, 11, 286, 1697, 309, 390, 1017, 11, 14423, 538, 2975, 11, 12566, 13], "temperature": 0.0, "avg_logprob": -0.1458002453758603, "compression_ratio": 1.5530973451327434, "no_speech_prob": 2.0580223463184666e-06}, {"id": 376, "seek": 156868, "start": 1587.28, "end": 1589.76, "text": " We don't want to do the full rank for that.", "tokens": [492, 500, 380, 528, 281, 360, 264, 1577, 6181, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.1458002453758603, "compression_ratio": 1.5530973451327434, "no_speech_prob": 2.0580223463184666e-06}, {"id": 377, "seek": 156868, "start": 1589.76, "end": 1592.78, "text": " That would be a lot of values.", "tokens": [663, 576, 312, 257, 688, 295, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1458002453758603, "compression_ratio": 1.5530973451327434, "no_speech_prob": 2.0580223463184666e-06}, {"id": 378, "seek": 156868, "start": 1592.78, "end": 1595.2, "text": " So we're just going to do some of those.", "tokens": [407, 321, 434, 445, 516, 281, 360, 512, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.1458002453758603, "compression_ratio": 1.5530973451327434, "no_speech_prob": 2.0580223463184666e-06}, {"id": 379, "seek": 159520, "start": 1595.2, "end": 1601.6000000000001, "text": " And the way it kind of tells how many to do is, so after you truncate, so when you're", "tokens": [400, 264, 636, 309, 733, 295, 5112, 577, 867, 281, 360, 307, 11, 370, 934, 291, 504, 409, 66, 473, 11, 370, 562, 291, 434], "temperature": 0.0, "avg_logprob": -0.10082009516724753, "compression_ratio": 1.793859649122807, "no_speech_prob": 5.422120466391789e-06}, {"id": 380, "seek": 159520, "start": 1601.6000000000001, "end": 1607.8400000000001, "text": " kind of dropping all the values that are less than, in this case it's 1 over mu, we're throwing", "tokens": [733, 295, 13601, 439, 264, 4190, 300, 366, 1570, 813, 11, 294, 341, 1389, 309, 311, 502, 670, 2992, 11, 321, 434, 10238], "temperature": 0.0, "avg_logprob": -0.10082009516724753, "compression_ratio": 1.793859649122807, "no_speech_prob": 5.422120466391789e-06}, {"id": 381, "seek": 159520, "start": 1607.8400000000001, "end": 1614.92, "text": " away, if we find that we're getting more values than we need and are throwing away a lot of", "tokens": [1314, 11, 498, 321, 915, 300, 321, 434, 1242, 544, 4190, 813, 321, 643, 293, 366, 10238, 1314, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.10082009516724753, "compression_ratio": 1.793859649122807, "no_speech_prob": 5.422120466391789e-06}, {"id": 382, "seek": 159520, "start": 1614.92, "end": 1621.1200000000001, "text": " them because they're so small, then we just increase by one how many singular values we're", "tokens": [552, 570, 436, 434, 370, 1359, 11, 550, 321, 445, 3488, 538, 472, 577, 867, 20010, 4190, 321, 434], "temperature": 0.0, "avg_logprob": -0.10082009516724753, "compression_ratio": 1.793859649122807, "no_speech_prob": 5.422120466391789e-06}, {"id": 383, "seek": 159520, "start": 1621.1200000000001, "end": 1624.8400000000001, "text": " getting, because we're basically doing fine.", "tokens": [1242, 11, 570, 321, 434, 1936, 884, 2489, 13], "temperature": 0.0, "avg_logprob": -0.10082009516724753, "compression_ratio": 1.793859649122807, "no_speech_prob": 5.422120466391789e-06}, {"id": 384, "seek": 162484, "start": 1624.84, "end": 1627.1999999999998, "text": " We're getting plenty since we're having to throw a bunch of them away.", "tokens": [492, 434, 1242, 7140, 1670, 321, 434, 1419, 281, 3507, 257, 3840, 295, 552, 1314, 13], "temperature": 0.0, "avg_logprob": -0.12700658139929308, "compression_ratio": 1.6535433070866141, "no_speech_prob": 2.19076036955812e-06}, {"id": 385, "seek": 162484, "start": 1627.1999999999998, "end": 1631.4399999999998, "text": " However, if we're keeping all of them, that's a sign that we're not getting enough singular", "tokens": [2908, 11, 498, 321, 434, 5145, 439, 295, 552, 11, 300, 311, 257, 1465, 300, 321, 434, 406, 1242, 1547, 20010], "temperature": 0.0, "avg_logprob": -0.12700658139929308, "compression_ratio": 1.6535433070866141, "no_speech_prob": 2.19076036955812e-06}, {"id": 386, "seek": 162484, "start": 1631.4399999999998, "end": 1636.74, "text": " values because they're all large enough, and so there might be more of a kind of large", "tokens": [4190, 570, 436, 434, 439, 2416, 1547, 11, 293, 370, 456, 1062, 312, 544, 295, 257, 733, 295, 2416], "temperature": 0.0, "avg_logprob": -0.12700658139929308, "compression_ratio": 1.6535433070866141, "no_speech_prob": 2.19076036955812e-06}, {"id": 387, "seek": 162484, "start": 1636.74, "end": 1638.9599999999998, "text": " magnitude that we're not finding.", "tokens": [15668, 300, 321, 434, 406, 5006, 13], "temperature": 0.0, "avg_logprob": -0.12700658139929308, "compression_ratio": 1.6535433070866141, "no_speech_prob": 2.19076036955812e-06}, {"id": 388, "seek": 162484, "start": 1638.9599999999998, "end": 1648.56, "text": " So in that case, we add 20% of the smaller dimension, which in this case is 240, to SV.", "tokens": [407, 294, 300, 1389, 11, 321, 909, 945, 4, 295, 264, 4356, 10139, 11, 597, 294, 341, 1389, 307, 26837, 11, 281, 31910, 13], "temperature": 0.0, "avg_logprob": -0.12700658139929308, "compression_ratio": 1.6535433070866141, "no_speech_prob": 2.19076036955812e-06}, {"id": 389, "seek": 162484, "start": 1648.56, "end": 1651.3999999999999, "text": " And I think this, actually let me show this now.", "tokens": [400, 286, 519, 341, 11, 767, 718, 385, 855, 341, 586, 13], "temperature": 0.0, "avg_logprob": -0.12700658139929308, "compression_ratio": 1.6535433070866141, "no_speech_prob": 2.19076036955812e-06}, {"id": 390, "seek": 165140, "start": 1651.4, "end": 1659.0800000000002, "text": " So I added some print statements that I didn't have last time to kind of say what rank it's", "tokens": [407, 286, 3869, 512, 4482, 12363, 300, 286, 994, 380, 362, 1036, 565, 281, 733, 295, 584, 437, 6181, 309, 311], "temperature": 0.0, "avg_logprob": -0.11118960962062929, "compression_ratio": 1.4577114427860696, "no_speech_prob": 1.6536638440811657e-06}, {"id": 391, "seek": 165140, "start": 1659.0800000000002, "end": 1660.2800000000002, "text": " using each time.", "tokens": [1228, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.11118960962062929, "compression_ratio": 1.4577114427860696, "no_speech_prob": 1.6536638440811657e-06}, {"id": 392, "seek": 165140, "start": 1660.2800000000002, "end": 1669.4, "text": " So here when we use the algorithm, it actually starts with just calculating rank 1, and then", "tokens": [407, 510, 562, 321, 764, 264, 9284, 11, 309, 767, 3719, 365, 445, 28258, 6181, 502, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.11118960962062929, "compression_ratio": 1.4577114427860696, "no_speech_prob": 1.6536638440811657e-06}, {"id": 393, "seek": 165140, "start": 1669.4, "end": 1671.5600000000002, "text": " that was not enough, so we add 240.", "tokens": [300, 390, 406, 1547, 11, 370, 321, 909, 26837, 13], "temperature": 0.0, "avg_logprob": -0.11118960962062929, "compression_ratio": 1.4577114427860696, "no_speech_prob": 1.6536638440811657e-06}, {"id": 394, "seek": 165140, "start": 1671.5600000000002, "end": 1676.6000000000001, "text": " So that's 20% of the smaller dimension, which is 4,800.", "tokens": [407, 300, 311, 945, 4, 295, 264, 4356, 10139, 11, 597, 307, 1017, 11, 14423, 13], "temperature": 0.0, "avg_logprob": -0.11118960962062929, "compression_ratio": 1.4577114427860696, "no_speech_prob": 1.6536638440811657e-06}, {"id": 395, "seek": 167660, "start": 1676.6, "end": 1684.1599999999999, "text": " So next time we've got rank 241.", "tokens": [407, 958, 565, 321, 600, 658, 6181, 4022, 16, 13], "temperature": 0.0, "avg_logprob": -0.21532075796554337, "compression_ratio": 1.3581081081081081, "no_speech_prob": 2.561222800068208e-06}, {"id": 396, "seek": 167660, "start": 1684.1599999999999, "end": 1689.1999999999998, "text": " And we do that, and this drops down to 49.", "tokens": [400, 321, 360, 300, 11, 293, 341, 11438, 760, 281, 16513, 13], "temperature": 0.0, "avg_logprob": -0.21532075796554337, "compression_ratio": 1.3581081081081081, "no_speech_prob": 2.561222800068208e-06}, {"id": 397, "seek": 167660, "start": 1689.1999999999998, "end": 1694.56, "text": " Oh, I think that's, so what it's resetting to is how many values you kept.", "tokens": [876, 11, 286, 519, 300, 311, 11, 370, 437, 309, 311, 14322, 783, 281, 307, 577, 867, 4190, 291, 4305, 13], "temperature": 0.0, "avg_logprob": -0.21532075796554337, "compression_ratio": 1.3581081081081081, "no_speech_prob": 2.561222800068208e-06}, {"id": 398, "seek": 167660, "start": 1694.56, "end": 1704.04, "text": " So we did that, and it only kept 49 of the values.", "tokens": [407, 321, 630, 300, 11, 293, 309, 787, 4305, 16513, 295, 264, 4190, 13], "temperature": 0.0, "avg_logprob": -0.21532075796554337, "compression_ratio": 1.3581081081081081, "no_speech_prob": 2.561222800068208e-06}, {"id": 399, "seek": 170404, "start": 1704.04, "end": 1710.0, "text": " Although then, oh, we kept 48 and added one.", "tokens": [5780, 550, 11, 1954, 11, 321, 4305, 11174, 293, 3869, 472, 13], "temperature": 0.0, "avg_logprob": -0.20163813343754522, "compression_ratio": 1.5606694560669456, "no_speech_prob": 2.443932316964492e-06}, {"id": 400, "seek": 170404, "start": 1710.0, "end": 1715.3999999999999, "text": " Okay, and then that time that wasn't, kind of wasn't enough, so we're going to add the", "tokens": [1033, 11, 293, 550, 300, 565, 300, 2067, 380, 11, 733, 295, 2067, 380, 1547, 11, 370, 321, 434, 516, 281, 909, 264], "temperature": 0.0, "avg_logprob": -0.20163813343754522, "compression_ratio": 1.5606694560669456, "no_speech_prob": 2.443932316964492e-06}, {"id": 401, "seek": 170404, "start": 1715.3999999999999, "end": 1720.24, "text": " 20% again, which is, and keep in mind each time we're doing this, we're doing it on a", "tokens": [945, 4, 797, 11, 597, 307, 11, 293, 1066, 294, 1575, 1184, 565, 321, 434, 884, 341, 11, 321, 434, 884, 309, 322, 257], "temperature": 0.0, "avg_logprob": -0.20163813343754522, "compression_ratio": 1.5606694560669456, "no_speech_prob": 2.443932316964492e-06}, {"id": 402, "seek": 170404, "start": 1720.24, "end": 1723.84, "text": " slightly different matrix, you know, because we've been alternating back and forth, updating", "tokens": [4748, 819, 8141, 11, 291, 458, 11, 570, 321, 600, 668, 40062, 646, 293, 5220, 11, 25113], "temperature": 0.0, "avg_logprob": -0.20163813343754522, "compression_ratio": 1.5606694560669456, "no_speech_prob": 2.443932316964492e-06}, {"id": 403, "seek": 170404, "start": 1723.84, "end": 1725.8, "text": " our estimates for LNS.", "tokens": [527, 20561, 337, 441, 42003, 13], "temperature": 0.0, "avg_logprob": -0.20163813343754522, "compression_ratio": 1.5606694560669456, "no_speech_prob": 2.443932316964492e-06}, {"id": 404, "seek": 170404, "start": 1725.8, "end": 1730.92, "text": " So then we add 240 again to get to 289.", "tokens": [407, 550, 321, 909, 26837, 797, 281, 483, 281, 7562, 24, 13], "temperature": 0.0, "avg_logprob": -0.20163813343754522, "compression_ratio": 1.5606694560669456, "no_speech_prob": 2.443932316964492e-06}, {"id": 405, "seek": 173092, "start": 1730.92, "end": 1735.52, "text": " And this whole time it's keeping track of this error.", "tokens": [400, 341, 1379, 565, 309, 311, 5145, 2837, 295, 341, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1266818088767803, "compression_ratio": 1.6311787072243347, "no_speech_prob": 2.5215310870407848e-06}, {"id": 406, "seek": 173092, "start": 1735.52, "end": 1740.6000000000001, "text": " Add another 240, and then our error is low enough, and so the algorithm halts.", "tokens": [5349, 1071, 26837, 11, 293, 550, 527, 6713, 307, 2295, 1547, 11, 293, 370, 264, 9284, 7523, 1373, 13], "temperature": 0.0, "avg_logprob": -0.1266818088767803, "compression_ratio": 1.6311787072243347, "no_speech_prob": 2.5215310870407848e-06}, {"id": 407, "seek": 173092, "start": 1740.6000000000001, "end": 1744.48, "text": " But that's kind of a, it gives a little different perspective.", "tokens": [583, 300, 311, 733, 295, 257, 11, 309, 2709, 257, 707, 819, 4585, 13], "temperature": 0.0, "avg_logprob": -0.1266818088767803, "compression_ratio": 1.6311787072243347, "no_speech_prob": 2.5215310870407848e-06}, {"id": 408, "seek": 173092, "start": 1744.48, "end": 1749.52, "text": " Let me go back to the algorithm, because there's just a third piece.", "tokens": [961, 385, 352, 646, 281, 264, 9284, 11, 570, 456, 311, 445, 257, 2636, 2522, 13], "temperature": 0.0, "avg_logprob": -0.1266818088767803, "compression_ratio": 1.6311787072243347, "no_speech_prob": 2.5215310870407848e-06}, {"id": 409, "seek": 173092, "start": 1749.52, "end": 1753.64, "text": " So we've got this aspect of, you know, we're updating how many singular values we want", "tokens": [407, 321, 600, 658, 341, 4171, 295, 11, 291, 458, 11, 321, 434, 25113, 577, 867, 20010, 4190, 321, 528], "temperature": 0.0, "avg_logprob": -0.1266818088767803, "compression_ratio": 1.6311787072243347, "no_speech_prob": 2.5215310870407848e-06}, {"id": 410, "seek": 173092, "start": 1753.64, "end": 1756.52, "text": " to calculate each time.", "tokens": [281, 8873, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.1266818088767803, "compression_ratio": 1.6311787072243347, "no_speech_prob": 2.5215310870407848e-06}, {"id": 411, "seek": 173092, "start": 1756.52, "end": 1759.5600000000002, "text": " And then here's where we're calculating the residual.", "tokens": [400, 550, 510, 311, 689, 321, 434, 28258, 264, 27980, 13], "temperature": 0.0, "avg_logprob": -0.1266818088767803, "compression_ratio": 1.6311787072243347, "no_speech_prob": 2.5215310870407848e-06}, {"id": 412, "seek": 175956, "start": 1759.56, "end": 1765.84, "text": " So looking at X, which was our original matrix, minus our low rank matrix, minus our sparse", "tokens": [407, 1237, 412, 1783, 11, 597, 390, 527, 3380, 8141, 11, 3175, 527, 2295, 6181, 8141, 11, 3175, 527, 637, 11668], "temperature": 0.0, "avg_logprob": -0.11153124837042058, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.851388315633812e-07}, {"id": 413, "seek": 175956, "start": 1765.84, "end": 1766.84, "text": " matrix.", "tokens": [8141, 13], "temperature": 0.0, "avg_logprob": -0.11153124837042058, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.851388315633812e-07}, {"id": 414, "seek": 175956, "start": 1766.84, "end": 1771.0, "text": " And so that's everything that hasn't been factored into either the low rank or sparse", "tokens": [400, 370, 300, 311, 1203, 300, 6132, 380, 668, 1186, 2769, 666, 2139, 264, 2295, 6181, 420, 637, 11668], "temperature": 0.0, "avg_logprob": -0.11153124837042058, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.851388315633812e-07}, {"id": 415, "seek": 175956, "start": 1771.0, "end": 1775.0, "text": " matrix yet.", "tokens": [8141, 1939, 13], "temperature": 0.0, "avg_logprob": -0.11153124837042058, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.851388315633812e-07}, {"id": 416, "seek": 175956, "start": 1775.0, "end": 1779.56, "text": " Then I also wanted to show, so this time, and this has been updated on GitHub as well,", "tokens": [1396, 286, 611, 1415, 281, 855, 11, 370, 341, 565, 11, 293, 341, 575, 668, 10588, 322, 23331, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.11153124837042058, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.851388315633812e-07}, {"id": 417, "seek": 175956, "start": 1779.56, "end": 1786.32, "text": " I made an examples list where I, at each step, and I just kind of randomly chose, I'm going", "tokens": [286, 1027, 364, 5110, 1329, 689, 286, 11, 412, 1184, 1823, 11, 293, 286, 445, 733, 295, 16979, 5111, 11, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.11153124837042058, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.851388315633812e-07}, {"id": 418, "seek": 178632, "start": 1786.32, "end": 1793.52, "text": " to keep track of the picture from column 140, which is just a particular point in time.", "tokens": [281, 1066, 2837, 295, 264, 3036, 490, 7738, 21548, 11, 597, 307, 445, 257, 1729, 935, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.16005156902556725, "compression_ratio": 1.538812785388128, "no_speech_prob": 7.183008165156934e-06}, {"id": 419, "seek": 178632, "start": 1793.52, "end": 1802.32, "text": " This is something that you wouldn't, row 140, because I've transposed, so there's a kind", "tokens": [639, 307, 746, 300, 291, 2759, 380, 11, 5386, 21548, 11, 570, 286, 600, 7132, 1744, 11, 370, 456, 311, 257, 733], "temperature": 0.0, "avg_logprob": -0.16005156902556725, "compression_ratio": 1.538812785388128, "no_speech_prob": 7.183008165156934e-06}, {"id": 420, "seek": 178632, "start": 1802.32, "end": 1809.2, "text": " of check up here on the shape, and you, yeah, if M is less than N, then you transpose it.", "tokens": [295, 1520, 493, 510, 322, 264, 3909, 11, 293, 291, 11, 1338, 11, 498, 376, 307, 1570, 813, 426, 11, 550, 291, 25167, 309, 13], "temperature": 0.0, "avg_logprob": -0.16005156902556725, "compression_ratio": 1.538812785388128, "no_speech_prob": 7.183008165156934e-06}, {"id": 421, "seek": 178632, "start": 1809.2, "end": 1815.04, "text": " So yeah, but in the kind of pictures I showed before, it's column 140.", "tokens": [407, 1338, 11, 457, 294, 264, 733, 295, 5242, 286, 4712, 949, 11, 309, 311, 7738, 21548, 13], "temperature": 0.0, "avg_logprob": -0.16005156902556725, "compression_ratio": 1.538812785388128, "no_speech_prob": 7.183008165156934e-06}, {"id": 422, "seek": 181504, "start": 1815.04, "end": 1817.36, "text": " So I'm just kind of keeping track of one point in time.", "tokens": [407, 286, 478, 445, 733, 295, 5145, 2837, 295, 472, 935, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.08483826936180912, "compression_ratio": 1.712280701754386, "no_speech_prob": 7.888968866609503e-06}, {"id": 423, "seek": 181504, "start": 1817.36, "end": 1821.52, "text": " This is something that makes it slower to kind of keep track of this list, but I just", "tokens": [639, 307, 746, 300, 1669, 309, 14009, 281, 733, 295, 1066, 2837, 295, 341, 1329, 11, 457, 286, 445], "temperature": 0.0, "avg_logprob": -0.08483826936180912, "compression_ratio": 1.712280701754386, "no_speech_prob": 7.888968866609503e-06}, {"id": 424, "seek": 181504, "start": 1821.52, "end": 1825.84, "text": " did that to get some visual output of how things are changing, and I think that can", "tokens": [630, 300, 281, 483, 512, 5056, 5598, 295, 577, 721, 366, 4473, 11, 293, 286, 519, 300, 393], "temperature": 0.0, "avg_logprob": -0.08483826936180912, "compression_ratio": 1.712280701754386, "no_speech_prob": 7.888968866609503e-06}, {"id": 425, "seek": 181504, "start": 1825.84, "end": 1829.0, "text": " be a nice way of kind of seeing how it's working.", "tokens": [312, 257, 1481, 636, 295, 733, 295, 2577, 577, 309, 311, 1364, 13], "temperature": 0.0, "avg_logprob": -0.08483826936180912, "compression_ratio": 1.712280701754386, "no_speech_prob": 7.888968866609503e-06}, {"id": 426, "seek": 181504, "start": 1829.0, "end": 1830.52, "text": " And this shows up here.", "tokens": [400, 341, 3110, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.08483826936180912, "compression_ratio": 1.712280701754386, "no_speech_prob": 7.888968866609503e-06}, {"id": 427, "seek": 181504, "start": 1830.52, "end": 1837.56, "text": " So now, actually, when I ran the algorithm, I got back my examples.", "tokens": [407, 586, 11, 767, 11, 562, 286, 5872, 264, 9284, 11, 286, 658, 646, 452, 5110, 13], "temperature": 0.0, "avg_logprob": -0.08483826936180912, "compression_ratio": 1.712280701754386, "no_speech_prob": 7.888968866609503e-06}, {"id": 428, "seek": 181504, "start": 1837.56, "end": 1842.6, "text": " And so you can see during the first, or at the end of the first iteration, this matrix", "tokens": [400, 370, 291, 393, 536, 1830, 264, 700, 11, 420, 412, 264, 917, 295, 264, 700, 24784, 11, 341, 8141], "temperature": 0.0, "avg_logprob": -0.08483826936180912, "compression_ratio": 1.712280701754386, "no_speech_prob": 7.888968866609503e-06}, {"id": 429, "seek": 181504, "start": 1842.6, "end": 1844.56, "text": " on the left is the sparse matrix.", "tokens": [322, 264, 1411, 307, 264, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08483826936180912, "compression_ratio": 1.712280701754386, "no_speech_prob": 7.888968866609503e-06}, {"id": 430, "seek": 184456, "start": 1844.56, "end": 1845.56, "text": " It's not very good.", "tokens": [467, 311, 406, 588, 665, 13], "temperature": 0.0, "avg_logprob": -0.17820208781474345, "compression_ratio": 1.5737704918032787, "no_speech_prob": 5.338032678992022e-06}, {"id": 431, "seek": 184456, "start": 1845.56, "end": 1851.3999999999999, "text": " It's just a black square, so I think it's maybe everything is zero.", "tokens": [467, 311, 445, 257, 2211, 3732, 11, 370, 286, 519, 309, 311, 1310, 1203, 307, 4018, 13], "temperature": 0.0, "avg_logprob": -0.17820208781474345, "compression_ratio": 1.5737704918032787, "no_speech_prob": 5.338032678992022e-06}, {"id": 432, "seek": 184456, "start": 1851.3999999999999, "end": 1855.04, "text": " And here's our matrix on the right, which is already pretty good, because we've done", "tokens": [400, 510, 311, 527, 8141, 322, 264, 558, 11, 597, 307, 1217, 1238, 665, 11, 570, 321, 600, 1096], "temperature": 0.0, "avg_logprob": -0.17820208781474345, "compression_ratio": 1.5737704918032787, "no_speech_prob": 5.338032678992022e-06}, {"id": 433, "seek": 184456, "start": 1855.04, "end": 1859.56, "text": " an SVD, and so that's picking out the low rank part, but we really want to get the people", "tokens": [364, 31910, 35, 11, 293, 370, 300, 311, 8867, 484, 264, 2295, 6181, 644, 11, 457, 321, 534, 528, 281, 483, 264, 561], "temperature": 0.0, "avg_logprob": -0.17820208781474345, "compression_ratio": 1.5737704918032787, "no_speech_prob": 5.338032678992022e-06}, {"id": 434, "seek": 184456, "start": 1859.56, "end": 1860.56, "text": " as well.", "tokens": [382, 731, 13], "temperature": 0.0, "avg_logprob": -0.17820208781474345, "compression_ratio": 1.5737704918032787, "no_speech_prob": 5.338032678992022e-06}, {"id": 435, "seek": 184456, "start": 1860.56, "end": 1863.96, "text": " And so you can see next step, we're kind of starting to get some people.", "tokens": [400, 370, 291, 393, 536, 958, 1823, 11, 321, 434, 733, 295, 2891, 281, 483, 512, 561, 13], "temperature": 0.0, "avg_logprob": -0.17820208781474345, "compression_ratio": 1.5737704918032787, "no_speech_prob": 5.338032678992022e-06}, {"id": 436, "seek": 184456, "start": 1863.96, "end": 1868.8, "text": " And then they're coming more into view.", "tokens": [400, 550, 436, 434, 1348, 544, 666, 1910, 13], "temperature": 0.0, "avg_logprob": -0.17820208781474345, "compression_ratio": 1.5737704918032787, "no_speech_prob": 5.338032678992022e-06}, {"id": 437, "seek": 186880, "start": 1868.8, "end": 1877.32, "text": " And I actually don't see a huge amount of difference between the fourth and fifth iterations", "tokens": [400, 286, 767, 500, 380, 536, 257, 2603, 2372, 295, 2649, 1296, 264, 6409, 293, 9266, 36540], "temperature": 0.0, "avg_logprob": -0.13743603947650956, "compression_ratio": 1.6989795918367347, "no_speech_prob": 5.338064966053935e-06}, {"id": 438, "seek": 186880, "start": 1877.32, "end": 1882.76, "text": " here, but I thought this was kind of nice to see at the end of each iteration what is", "tokens": [510, 11, 457, 286, 1194, 341, 390, 733, 295, 1481, 281, 536, 412, 264, 917, 295, 1184, 24784, 437, 307], "temperature": 0.0, "avg_logprob": -0.13743603947650956, "compression_ratio": 1.6989795918367347, "no_speech_prob": 5.338064966053935e-06}, {"id": 439, "seek": 186880, "start": 1882.76, "end": 1886.68, "text": " the low rank one look like and what is the sparse one look like.", "tokens": [264, 2295, 6181, 472, 574, 411, 293, 437, 307, 264, 637, 11668, 472, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.13743603947650956, "compression_ratio": 1.6989795918367347, "no_speech_prob": 5.338064966053935e-06}, {"id": 440, "seek": 186880, "start": 1886.68, "end": 1895.28, "text": " And we kind of talked before about how finding the sparse one, I think, is more difficult", "tokens": [400, 321, 733, 295, 2825, 949, 466, 577, 5006, 264, 637, 11668, 472, 11, 286, 519, 11, 307, 544, 2252], "temperature": 0.0, "avg_logprob": -0.13743603947650956, "compression_ratio": 1.6989795918367347, "no_speech_prob": 5.338064966053935e-06}, {"id": 441, "seek": 189528, "start": 1895.28, "end": 1898.8799999999999, "text": " than the low rank one, just in that...", "tokens": [813, 264, 2295, 6181, 472, 11, 445, 294, 300, 485], "temperature": 0.0, "avg_logprob": -0.3085392912228902, "compression_ratio": 1.5, "no_speech_prob": 1.602781480869453e-06}, {"id": 442, "seek": 189528, "start": 1898.8799999999999, "end": 1901.8799999999999, "text": " Actually, no, that's not true.", "tokens": [5135, 11, 572, 11, 300, 311, 406, 2074, 13], "temperature": 0.0, "avg_logprob": -0.3085392912228902, "compression_ratio": 1.5, "no_speech_prob": 1.602781480869453e-06}, {"id": 443, "seek": 189528, "start": 1901.8799999999999, "end": 1909.28, "text": " I was going to say with the people...", "tokens": [286, 390, 516, 281, 584, 365, 264, 561, 485], "temperature": 0.0, "avg_logprob": -0.3085392912228902, "compression_ratio": 1.5, "no_speech_prob": 1.602781480869453e-06}, {"id": 444, "seek": 189528, "start": 1909.28, "end": 1913.0, "text": " I guess the sparse one, kind of as long as you have figures there, maybe you're not worried", "tokens": [286, 2041, 264, 637, 11668, 472, 11, 733, 295, 382, 938, 382, 291, 362, 9624, 456, 11, 1310, 291, 434, 406, 5804], "temperature": 0.0, "avg_logprob": -0.3085392912228902, "compression_ratio": 1.5, "no_speech_prob": 1.602781480869453e-06}, {"id": 445, "seek": 189528, "start": 1913.0, "end": 1915.0, "text": " about all the edges.", "tokens": [466, 439, 264, 8819, 13], "temperature": 0.0, "avg_logprob": -0.3085392912228902, "compression_ratio": 1.5, "no_speech_prob": 1.602781480869453e-06}, {"id": 446, "seek": 189528, "start": 1915.0, "end": 1916.0, "text": " Yeah, disregard.", "tokens": [865, 11, 44493, 13], "temperature": 0.0, "avg_logprob": -0.3085392912228902, "compression_ratio": 1.5, "no_speech_prob": 1.602781480869453e-06}, {"id": 447, "seek": 189528, "start": 1916.0, "end": 1920.24, "text": " But so yeah, this is what happens kind of after each iteration.", "tokens": [583, 370, 1338, 11, 341, 307, 437, 2314, 733, 295, 934, 1184, 24784, 13], "temperature": 0.0, "avg_logprob": -0.3085392912228902, "compression_ratio": 1.5, "no_speech_prob": 1.602781480869453e-06}, {"id": 448, "seek": 189528, "start": 1920.24, "end": 1923.24, "text": " Any questions, Kelsey?", "tokens": [2639, 1651, 11, 44714, 30], "temperature": 0.0, "avg_logprob": -0.3085392912228902, "compression_ratio": 1.5, "no_speech_prob": 1.602781480869453e-06}, {"id": 449, "seek": 192324, "start": 1923.24, "end": 1930.24, "text": " How unique is the solution to this?", "tokens": [1012, 3845, 307, 264, 3827, 281, 341, 30], "temperature": 0.0, "avg_logprob": -0.2855972713894314, "compression_ratio": 1.5297029702970297, "no_speech_prob": 4.832158811041154e-05}, {"id": 450, "seek": 192324, "start": 1930.24, "end": 1935.24, "text": " So in this case, maybe there's a pretty clear local maximum.", "tokens": [407, 294, 341, 1389, 11, 1310, 456, 311, 257, 1238, 1850, 2654, 6674, 13], "temperature": 0.0, "avg_logprob": -0.2855972713894314, "compression_ratio": 1.5297029702970297, "no_speech_prob": 4.832158811041154e-05}, {"id": 451, "seek": 192324, "start": 1935.24, "end": 1936.24, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2855972713894314, "compression_ratio": 1.5297029702970297, "no_speech_prob": 4.832158811041154e-05}, {"id": 452, "seek": 192324, "start": 1936.24, "end": 1941.24, "text": " If you have a situation where it's less well represented by the decomposition, does where", "tokens": [759, 291, 362, 257, 2590, 689, 309, 311, 1570, 731, 10379, 538, 264, 48356, 11, 775, 689], "temperature": 0.0, "avg_logprob": -0.2855972713894314, "compression_ratio": 1.5297029702970297, "no_speech_prob": 4.832158811041154e-05}, {"id": 453, "seek": 192324, "start": 1941.24, "end": 1943.24, "text": " you start affect where you end up?", "tokens": [291, 722, 3345, 689, 291, 917, 493, 30], "temperature": 0.0, "avg_logprob": -0.2855972713894314, "compression_ratio": 1.5297029702970297, "no_speech_prob": 4.832158811041154e-05}, {"id": 454, "seek": 192324, "start": 1943.24, "end": 1944.24, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2855972713894314, "compression_ratio": 1.5297029702970297, "no_speech_prob": 4.832158811041154e-05}, {"id": 455, "seek": 192324, "start": 1944.24, "end": 1947.24, "text": " So there are some additional constraints on this problem.", "tokens": [407, 456, 366, 512, 4497, 18491, 322, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.2855972713894314, "compression_ratio": 1.5297029702970297, "no_speech_prob": 4.832158811041154e-05}, {"id": 456, "seek": 194724, "start": 1947.24, "end": 1954.32, "text": " So in general, you have to say that this algorithm is forcing the low rank matrix to not also", "tokens": [407, 294, 2674, 11, 291, 362, 281, 584, 300, 341, 9284, 307, 19030, 264, 2295, 6181, 8141, 281, 406, 611], "temperature": 0.0, "avg_logprob": -0.18868504399838654, "compression_ratio": 1.5662100456621004, "no_speech_prob": 6.438869149860693e-06}, {"id": 457, "seek": 194724, "start": 1954.32, "end": 1961.8, "text": " be sparse, I believe.", "tokens": [312, 637, 11668, 11, 286, 1697, 13], "temperature": 0.0, "avg_logprob": -0.18868504399838654, "compression_ratio": 1.5662100456621004, "no_speech_prob": 6.438869149860693e-06}, {"id": 458, "seek": 194724, "start": 1961.8, "end": 1966.84, "text": " If you had a low rank matrix that was also sparse, then it's kind of not...", "tokens": [759, 291, 632, 257, 2295, 6181, 8141, 300, 390, 611, 637, 11668, 11, 550, 309, 311, 733, 295, 406, 485], "temperature": 0.0, "avg_logprob": -0.18868504399838654, "compression_ratio": 1.5662100456621004, "no_speech_prob": 6.438869149860693e-06}, {"id": 459, "seek": 194724, "start": 1966.84, "end": 1970.84, "text": " So the general statement I gave you of robust PCA is actually not fully...", "tokens": [407, 264, 2674, 5629, 286, 2729, 291, 295, 13956, 6465, 32, 307, 767, 406, 4498, 485], "temperature": 0.0, "avg_logprob": -0.18868504399838654, "compression_ratio": 1.5662100456621004, "no_speech_prob": 6.438869149860693e-06}, {"id": 460, "seek": 194724, "start": 1970.84, "end": 1972.8, "text": " Or is not uniquely defined.", "tokens": [1610, 307, 406, 31474, 7642, 13], "temperature": 0.0, "avg_logprob": -0.18868504399838654, "compression_ratio": 1.5662100456621004, "no_speech_prob": 6.438869149860693e-06}, {"id": 461, "seek": 194724, "start": 1972.8, "end": 1974.6, "text": " There could be several different decompositions.", "tokens": [821, 727, 312, 2940, 819, 22867, 329, 2451, 13], "temperature": 0.0, "avg_logprob": -0.18868504399838654, "compression_ratio": 1.5662100456621004, "no_speech_prob": 6.438869149860693e-06}, {"id": 462, "seek": 197460, "start": 1974.6, "end": 1982.0, "text": " But I think most algorithms kind of make additional assumptions to kind of constrain it further.", "tokens": [583, 286, 519, 881, 14642, 733, 295, 652, 4497, 17695, 281, 733, 295, 1817, 7146, 309, 3052, 13], "temperature": 0.0, "avg_logprob": -0.32575612319143193, "compression_ratio": 1.5186915887850467, "no_speech_prob": 1.750173032633029e-05}, {"id": 463, "seek": 197460, "start": 1982.0, "end": 1983.0, "text": " But...", "tokens": [583, 485], "temperature": 0.0, "avg_logprob": -0.32575612319143193, "compression_ratio": 1.5186915887850467, "no_speech_prob": 1.750173032633029e-05}, {"id": 464, "seek": 197460, "start": 1983.0, "end": 1990.0, "text": " What happens if I were to give it like an image that didn't have, say, a video where", "tokens": [708, 2314, 498, 286, 645, 281, 976, 309, 411, 364, 3256, 300, 994, 380, 362, 11, 584, 11, 257, 960, 689], "temperature": 0.0, "avg_logprob": -0.32575612319143193, "compression_ratio": 1.5186915887850467, "no_speech_prob": 1.750173032633029e-05}, {"id": 465, "seek": 197460, "start": 1990.0, "end": 1993.0, "text": " the background was changing constantly or something like that?", "tokens": [264, 3678, 390, 4473, 6460, 420, 746, 411, 300, 30], "temperature": 0.0, "avg_logprob": -0.32575612319143193, "compression_ratio": 1.5186915887850467, "no_speech_prob": 1.750173032633029e-05}, {"id": 466, "seek": 197460, "start": 1993.0, "end": 1998.0, "text": " Would you potentially end up with an unstable decomposition of some kind?", "tokens": [6068, 291, 7263, 917, 493, 365, 364, 23742, 48356, 295, 512, 733, 30], "temperature": 0.0, "avg_logprob": -0.32575612319143193, "compression_ratio": 1.5186915887850467, "no_speech_prob": 1.750173032633029e-05}, {"id": 467, "seek": 199800, "start": 1998.0, "end": 2005.0, "text": " Yeah, I mean, I don't know that that would even converge.", "tokens": [865, 11, 286, 914, 11, 286, 500, 380, 458, 300, 300, 576, 754, 41881, 13], "temperature": 0.0, "avg_logprob": -0.20367149006236684, "compression_ratio": 1.6370967741935485, "no_speech_prob": 5.173728368390584e-06}, {"id": 468, "seek": 199800, "start": 2005.0, "end": 2008.0, "text": " You could always have a unique SVD.", "tokens": [509, 727, 1009, 362, 257, 3845, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.20367149006236684, "compression_ratio": 1.6370967741935485, "no_speech_prob": 5.173728368390584e-06}, {"id": 469, "seek": 199800, "start": 2008.0, "end": 2009.0, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.20367149006236684, "compression_ratio": 1.6370967741935485, "no_speech_prob": 5.173728368390584e-06}, {"id": 470, "seek": 199800, "start": 2009.0, "end": 2013.0, "text": " Would it be more like on the map where you could possibly come up with any different", "tokens": [6068, 309, 312, 544, 411, 322, 264, 4471, 689, 291, 727, 6264, 808, 493, 365, 604, 819], "temperature": 0.0, "avg_logprob": -0.20367149006236684, "compression_ratio": 1.6370967741935485, "no_speech_prob": 5.173728368390584e-06}, {"id": 471, "seek": 199800, "start": 2013.0, "end": 2014.0, "text": " solutions?", "tokens": [6547, 30], "temperature": 0.0, "avg_logprob": -0.20367149006236684, "compression_ratio": 1.6370967741935485, "no_speech_prob": 5.173728368390584e-06}, {"id": 472, "seek": 199800, "start": 2014.0, "end": 2017.12, "text": " I mean, I guess the example of a video with the background changing, to me, sounds like", "tokens": [286, 914, 11, 286, 2041, 264, 1365, 295, 257, 960, 365, 264, 3678, 4473, 11, 281, 385, 11, 3263, 411], "temperature": 0.0, "avg_logprob": -0.20367149006236684, "compression_ratio": 1.6370967741935485, "no_speech_prob": 5.173728368390584e-06}, {"id": 473, "seek": 199800, "start": 2017.12, "end": 2022.08, "text": " there might be no solution in that I don't know that that could be represented as a low", "tokens": [456, 1062, 312, 572, 3827, 294, 300, 286, 500, 380, 458, 300, 300, 727, 312, 10379, 382, 257, 2295], "temperature": 0.0, "avg_logprob": -0.20367149006236684, "compression_ratio": 1.6370967741935485, "no_speech_prob": 5.173728368390584e-06}, {"id": 474, "seek": 199800, "start": 2022.08, "end": 2027.8, "text": " rank matrix plus a sparse matrix.", "tokens": [6181, 8141, 1804, 257, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.20367149006236684, "compression_ratio": 1.6370967741935485, "no_speech_prob": 5.173728368390584e-06}, {"id": 475, "seek": 202780, "start": 2027.8, "end": 2031.96, "text": " The other factor you have is how much of a penalty...", "tokens": [440, 661, 5952, 291, 362, 307, 577, 709, 295, 257, 16263, 485], "temperature": 0.0, "avg_logprob": -0.1740778019997926, "compression_ratio": 1.764, "no_speech_prob": 5.062582204118371e-05}, {"id": 476, "seek": 202780, "start": 2031.96, "end": 2037.24, "text": " So there's kind of like a parameter of kind of how much you want to penalize the low rank", "tokens": [407, 456, 311, 733, 295, 411, 257, 13075, 295, 733, 295, 577, 709, 291, 528, 281, 13661, 1125, 264, 2295, 6181], "temperature": 0.0, "avg_logprob": -0.1740778019997926, "compression_ratio": 1.764, "no_speech_prob": 5.062582204118371e-05}, {"id": 477, "seek": 202780, "start": 2037.24, "end": 2040.1599999999999, "text": " error versus the sparse error of those two matrices being off.", "tokens": [6713, 5717, 264, 637, 11668, 6713, 295, 729, 732, 32284, 885, 766, 13], "temperature": 0.0, "avg_logprob": -0.1740778019997926, "compression_ratio": 1.764, "no_speech_prob": 5.062582204118371e-05}, {"id": 478, "seek": 202780, "start": 2040.1599999999999, "end": 2044.0, "text": " And I think if that was adjusted, that could give you a different...", "tokens": [400, 286, 519, 498, 300, 390, 19871, 11, 300, 727, 976, 291, 257, 819, 485], "temperature": 0.0, "avg_logprob": -0.1740778019997926, "compression_ratio": 1.764, "no_speech_prob": 5.062582204118371e-05}, {"id": 479, "seek": 202780, "start": 2044.0, "end": 2049.32, "text": " Different results of how bad having your sparse matrix not be fully sparse is versus your", "tokens": [20825, 3542, 295, 577, 1578, 1419, 428, 637, 11668, 8141, 406, 312, 4498, 637, 11668, 307, 5717, 428], "temperature": 0.0, "avg_logprob": -0.1740778019997926, "compression_ratio": 1.764, "no_speech_prob": 5.062582204118371e-05}, {"id": 480, "seek": 202780, "start": 2049.32, "end": 2052.72, "text": " low rank matrix not being fully low rank.", "tokens": [2295, 6181, 8141, 406, 885, 4498, 2295, 6181, 13], "temperature": 0.0, "avg_logprob": -0.1740778019997926, "compression_ratio": 1.764, "no_speech_prob": 5.062582204118371e-05}, {"id": 481, "seek": 202780, "start": 2052.72, "end": 2053.72, "text": " I don't know if this makes sense.", "tokens": [286, 500, 380, 458, 498, 341, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.1740778019997926, "compression_ratio": 1.764, "no_speech_prob": 5.062582204118371e-05}, {"id": 482, "seek": 205372, "start": 2053.72, "end": 2058.68, "text": " I was just wondering, if it wasn't constantly changing, but more like in the real world", "tokens": [286, 390, 445, 6359, 11, 498, 309, 2067, 380, 6460, 4473, 11, 457, 544, 411, 294, 264, 957, 1002], "temperature": 0.0, "avg_logprob": -0.3402954839891003, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.00022328764316625893}, {"id": 483, "seek": 205372, "start": 2058.68, "end": 2064.12, "text": " you'd have more like switching scenes from time to time, would you end up with something", "tokens": [291, 1116, 362, 544, 411, 16493, 8026, 490, 565, 281, 565, 11, 576, 291, 917, 493, 365, 746], "temperature": 0.0, "avg_logprob": -0.3402954839891003, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.00022328764316625893}, {"id": 484, "seek": 205372, "start": 2064.12, "end": 2069.4399999999996, "text": " where the low rank matrix is like a row for each scene, you know, and then the kind of", "tokens": [689, 264, 2295, 6181, 8141, 307, 411, 257, 5386, 337, 1184, 4145, 11, 291, 458, 11, 293, 550, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.3402954839891003, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.00022328764316625893}, {"id": 485, "seek": 205372, "start": 2069.4399999999996, "end": 2073.68, "text": " thing that's multiplying by something coming at each time point how much it represents", "tokens": [551, 300, 311, 30955, 538, 746, 1348, 412, 1184, 565, 935, 577, 709, 309, 8855], "temperature": 0.0, "avg_logprob": -0.3402954839891003, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.00022328764316625893}, {"id": 486, "seek": 205372, "start": 2073.68, "end": 2074.68, "text": " that scene?", "tokens": [300, 4145, 30], "temperature": 0.0, "avg_logprob": -0.3402954839891003, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.00022328764316625893}, {"id": 487, "seek": 205372, "start": 2074.68, "end": 2075.68, "text": " Would that work?", "tokens": [6068, 300, 589, 30], "temperature": 0.0, "avg_logprob": -0.3402954839891003, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.00022328764316625893}, {"id": 488, "seek": 205372, "start": 2075.68, "end": 2081.68, "text": " I mean, obviously if you try it, I was just trying to think if it makes sense intuitively.", "tokens": [286, 914, 11, 2745, 498, 291, 853, 309, 11, 286, 390, 445, 1382, 281, 519, 498, 309, 1669, 2020, 46506, 13], "temperature": 0.0, "avg_logprob": -0.3402954839891003, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.00022328764316625893}, {"id": 489, "seek": 205372, "start": 2081.68, "end": 2082.68, "text": " Yeah, that seems...", "tokens": [865, 11, 300, 2544, 485], "temperature": 0.0, "avg_logprob": -0.3402954839891003, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.00022328764316625893}, {"id": 490, "seek": 208268, "start": 2082.68, "end": 2083.68, "text": " They're almost like topics.", "tokens": [814, 434, 1920, 411, 8378, 13], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 491, "seek": 208268, "start": 2083.68, "end": 2087.8799999999997, "text": " Yeah, no, that does seem possible to me.", "tokens": [865, 11, 572, 11, 300, 775, 1643, 1944, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 492, "seek": 208268, "start": 2087.8799999999997, "end": 2093.12, "text": " Because we figured out we could recreate the original matrix if it didn't change the background", "tokens": [1436, 321, 8932, 484, 321, 727, 25833, 264, 3380, 8141, 498, 309, 994, 380, 1319, 264, 3678], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 493, "seek": 208268, "start": 2093.12, "end": 2094.12, "text": " with just a single...", "tokens": [365, 445, 257, 2167, 485], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 494, "seek": 208268, "start": 2094.12, "end": 2095.12, "text": " With rank one, yeah.", "tokens": [2022, 6181, 472, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 495, "seek": 208268, "start": 2095.12, "end": 2097.12, "text": " Yeah, no, that does sound like it would work.", "tokens": [865, 11, 572, 11, 300, 775, 1626, 411, 309, 576, 589, 13], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 496, "seek": 208268, "start": 2097.12, "end": 2098.72, "text": " I'm going to try if anybody gets them.", "tokens": [286, 478, 516, 281, 853, 498, 4472, 2170, 552, 13], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 497, "seek": 208268, "start": 2098.72, "end": 2104.96, "text": " Yeah, no, I would love for you all to try running this code on different videos to see", "tokens": [865, 11, 572, 11, 286, 576, 959, 337, 291, 439, 281, 853, 2614, 341, 3089, 322, 819, 2145, 281, 536], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 498, "seek": 208268, "start": 2104.96, "end": 2105.96, "text": " what you get.", "tokens": [437, 291, 483, 13], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 499, "seek": 208268, "start": 2105.96, "end": 2111.64, "text": " I suspect one where it's gradually panning might not work at all, but again, interesting", "tokens": [286, 9091, 472, 689, 309, 311, 13145, 2462, 773, 1062, 406, 589, 412, 439, 11, 457, 797, 11, 1880], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 500, "seek": 208268, "start": 2111.64, "end": 2112.64, "text": " to try.", "tokens": [281, 853, 13], "temperature": 0.0, "avg_logprob": -0.2940341921618385, "compression_ratio": 1.678082191780822, "no_speech_prob": 6.204901728779078e-05}, {"id": 501, "seek": 211264, "start": 2112.64, "end": 2120.08, "text": " Yeah, these are good questions.", "tokens": [865, 11, 613, 366, 665, 1651, 13], "temperature": 0.0, "avg_logprob": -0.14106449484825134, "compression_ratio": 1.6902654867256637, "no_speech_prob": 7.646250196557958e-06}, {"id": 502, "seek": 211264, "start": 2120.08, "end": 2121.08, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.14106449484825134, "compression_ratio": 1.6902654867256637, "no_speech_prob": 7.646250196557958e-06}, {"id": 503, "seek": 211264, "start": 2121.08, "end": 2124.0, "text": " And again, we're not going into all the details of this algorithm.", "tokens": [400, 797, 11, 321, 434, 406, 516, 666, 439, 264, 4365, 295, 341, 9284, 13], "temperature": 0.0, "avg_logprob": -0.14106449484825134, "compression_ratio": 1.6902654867256637, "no_speech_prob": 7.646250196557958e-06}, {"id": 504, "seek": 211264, "start": 2124.0, "end": 2127.92, "text": " I just want you to kind of have a high level idea of kind of what we're doing with the,", "tokens": [286, 445, 528, 291, 281, 733, 295, 362, 257, 1090, 1496, 1558, 295, 733, 295, 437, 321, 434, 884, 365, 264, 11], "temperature": 0.0, "avg_logprob": -0.14106449484825134, "compression_ratio": 1.6902654867256637, "no_speech_prob": 7.646250196557958e-06}, {"id": 505, "seek": 211264, "start": 2127.92, "end": 2131.7999999999997, "text": " you know, alternating between getting the low rank and the sparse and with adjusting", "tokens": [291, 458, 11, 40062, 1296, 1242, 264, 2295, 6181, 293, 264, 637, 11668, 293, 365, 23559], "temperature": 0.0, "avg_logprob": -0.14106449484825134, "compression_ratio": 1.6902654867256637, "no_speech_prob": 7.646250196557958e-06}, {"id": 506, "seek": 211264, "start": 2131.7999999999997, "end": 2139.56, "text": " the number of singular values we're calculating for the low rank one kind of based on are", "tokens": [264, 1230, 295, 20010, 4190, 321, 434, 28258, 337, 264, 2295, 6181, 472, 733, 295, 2361, 322, 366], "temperature": 0.0, "avg_logprob": -0.14106449484825134, "compression_ratio": 1.6902654867256637, "no_speech_prob": 7.646250196557958e-06}, {"id": 507, "seek": 213956, "start": 2139.56, "end": 2146.92, "text": " we getting lots of little ones that we can throw away or are all of them meaningful?", "tokens": [321, 1242, 3195, 295, 707, 2306, 300, 321, 393, 3507, 1314, 420, 366, 439, 295, 552, 10995, 30], "temperature": 0.0, "avg_logprob": -0.4356179373604911, "compression_ratio": 1.18, "no_speech_prob": 7.888879736128729e-06}, {"id": 508, "seek": 213956, "start": 2146.92, "end": 2149.84, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.4356179373604911, "compression_ratio": 1.18, "no_speech_prob": 7.888879736128729e-06}, {"id": 509, "seek": 213956, "start": 2149.84, "end": 2161.92, "text": " Yeah, just a quick question.", "tokens": [865, 11, 445, 257, 1702, 1168, 13], "temperature": 0.0, "avg_logprob": -0.4356179373604911, "compression_ratio": 1.18, "no_speech_prob": 7.888879736128729e-06}, {"id": 510, "seek": 216192, "start": 2161.92, "end": 2171.88, "text": " So in terms of being grossly corrupted, how do you best define that being corrupted?", "tokens": [407, 294, 2115, 295, 885, 11367, 356, 39480, 11, 577, 360, 291, 1151, 6964, 300, 885, 39480, 30], "temperature": 0.0, "avg_logprob": -0.19131180547898816, "compression_ratio": 1.6572769953051643, "no_speech_prob": 1.045105091179721e-05}, {"id": 511, "seek": 216192, "start": 2171.88, "end": 2173.6800000000003, "text": " So I mean, I would think of that as...", "tokens": [407, 286, 914, 11, 286, 576, 519, 295, 300, 382, 485], "temperature": 0.0, "avg_logprob": -0.19131180547898816, "compression_ratio": 1.6572769953051643, "no_speech_prob": 1.045105091179721e-05}, {"id": 512, "seek": 216192, "start": 2173.6800000000003, "end": 2177.64, "text": " I mean, you would have to set some sort of like parameter, like I don't know if your", "tokens": [286, 914, 11, 291, 576, 362, 281, 992, 512, 1333, 295, 411, 13075, 11, 411, 286, 500, 380, 458, 498, 428], "temperature": 0.0, "avg_logprob": -0.19131180547898816, "compression_ratio": 1.6572769953051643, "no_speech_prob": 1.045105091179721e-05}, {"id": 513, "seek": 216192, "start": 2177.64, "end": 2183.84, "text": " air is less than tau, that small air, but I think of that as kind of something with", "tokens": [1988, 307, 1570, 813, 17842, 11, 300, 1359, 1988, 11, 457, 286, 519, 295, 300, 382, 733, 295, 746, 365], "temperature": 0.0, "avg_logprob": -0.19131180547898816, "compression_ratio": 1.6572769953051643, "no_speech_prob": 1.045105091179721e-05}, {"id": 514, "seek": 216192, "start": 2183.84, "end": 2187.0, "text": " like where there are entries that are just completely wrong.", "tokens": [411, 689, 456, 366, 23041, 300, 366, 445, 2584, 2085, 13], "temperature": 0.0, "avg_logprob": -0.19131180547898816, "compression_ratio": 1.6572769953051643, "no_speech_prob": 1.045105091179721e-05}, {"id": 515, "seek": 218700, "start": 2187.0, "end": 2192.32, "text": " You know, it's not so much that they're like close, but they're just off.", "tokens": [509, 458, 11, 309, 311, 406, 370, 709, 300, 436, 434, 411, 1998, 11, 457, 436, 434, 445, 766, 13], "temperature": 0.0, "avg_logprob": -0.23604276103358116, "compression_ratio": 1.67578125, "no_speech_prob": 1.7502199625596404e-05}, {"id": 516, "seek": 218700, "start": 2192.32, "end": 2196.2, "text": " For example, in this image, right, when we're trying to separate the background with the", "tokens": [1171, 1365, 11, 294, 341, 3256, 11, 558, 11, 562, 321, 434, 1382, 281, 4994, 264, 3678, 365, 264], "temperature": 0.0, "avg_logprob": -0.23604276103358116, "compression_ratio": 1.67578125, "no_speech_prob": 1.7502199625596404e-05}, {"id": 517, "seek": 218700, "start": 2196.2, "end": 2197.2, "text": " people.", "tokens": [561, 13], "temperature": 0.0, "avg_logprob": -0.23604276103358116, "compression_ratio": 1.67578125, "no_speech_prob": 1.7502199625596404e-05}, {"id": 518, "seek": 218700, "start": 2197.2, "end": 2201.04, "text": " So in this case, how do you define the images being corrupted?", "tokens": [407, 294, 341, 1389, 11, 577, 360, 291, 6964, 264, 5267, 885, 39480, 30], "temperature": 0.0, "avg_logprob": -0.23604276103358116, "compression_ratio": 1.67578125, "no_speech_prob": 1.7502199625596404e-05}, {"id": 519, "seek": 218700, "start": 2201.04, "end": 2202.28, "text": " So this image is not corrupted.", "tokens": [407, 341, 3256, 307, 406, 39480, 13], "temperature": 0.0, "avg_logprob": -0.23604276103358116, "compression_ratio": 1.67578125, "no_speech_prob": 1.7502199625596404e-05}, {"id": 520, "seek": 218700, "start": 2202.28, "end": 2203.56, "text": " Yeah, that's a good question.", "tokens": [865, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.23604276103358116, "compression_ratio": 1.67578125, "no_speech_prob": 1.7502199625596404e-05}, {"id": 521, "seek": 218700, "start": 2203.56, "end": 2206.84, "text": " So this image is very high quality, but the ones we saw above...", "tokens": [407, 341, 3256, 307, 588, 1090, 3125, 11, 457, 264, 2306, 321, 1866, 3673, 485], "temperature": 0.0, "avg_logprob": -0.23604276103358116, "compression_ratio": 1.67578125, "no_speech_prob": 1.7502199625596404e-05}, {"id": 522, "seek": 218700, "start": 2206.84, "end": 2210.4, "text": " Let me go back up.", "tokens": [961, 385, 352, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.23604276103358116, "compression_ratio": 1.67578125, "no_speech_prob": 1.7502199625596404e-05}, {"id": 523, "seek": 218700, "start": 2210.4, "end": 2213.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.23604276103358116, "compression_ratio": 1.67578125, "no_speech_prob": 1.7502199625596404e-05}, {"id": 524, "seek": 218700, "start": 2213.64, "end": 2216.22, "text": " So these facial ones are grossly corrupted.", "tokens": [407, 613, 15642, 2306, 366, 11367, 356, 39480, 13], "temperature": 0.0, "avg_logprob": -0.23604276103358116, "compression_ratio": 1.67578125, "no_speech_prob": 1.7502199625596404e-05}, {"id": 525, "seek": 221622, "start": 2216.22, "end": 2222.52, "text": " So kind of like when you have the pixels where it's like...", "tokens": [407, 733, 295, 411, 562, 291, 362, 264, 18668, 689, 309, 311, 411, 485], "temperature": 0.0, "avg_logprob": -0.24579991879670515, "compression_ratio": 1.6260162601626016, "no_speech_prob": 9.080278687179089e-06}, {"id": 526, "seek": 221622, "start": 2222.52, "end": 2228.0, "text": " A lot of these white pixels are just not even close to being the right color.", "tokens": [316, 688, 295, 613, 2418, 18668, 366, 445, 406, 754, 1998, 281, 885, 264, 558, 2017, 13], "temperature": 0.0, "avg_logprob": -0.24579991879670515, "compression_ratio": 1.6260162601626016, "no_speech_prob": 9.080278687179089e-06}, {"id": 527, "seek": 221622, "start": 2228.0, "end": 2229.0, "text": " They're damaged somehow.", "tokens": [814, 434, 14080, 6063, 13], "temperature": 0.0, "avg_logprob": -0.24579991879670515, "compression_ratio": 1.6260162601626016, "no_speech_prob": 9.080278687179089e-06}, {"id": 528, "seek": 221622, "start": 2229.0, "end": 2230.0, "text": " Right, got it.", "tokens": [1779, 11, 658, 309, 13], "temperature": 0.0, "avg_logprob": -0.24579991879670515, "compression_ratio": 1.6260162601626016, "no_speech_prob": 9.080278687179089e-06}, {"id": 529, "seek": 221622, "start": 2230.0, "end": 2231.0, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.24579991879670515, "compression_ratio": 1.6260162601626016, "no_speech_prob": 9.080278687179089e-06}, {"id": 530, "seek": 221622, "start": 2231.0, "end": 2232.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.24579991879670515, "compression_ratio": 1.6260162601626016, "no_speech_prob": 9.080278687179089e-06}, {"id": 531, "seek": 221622, "start": 2232.0, "end": 2233.0, "text": " And Netflix talked about having this problem.", "tokens": [400, 12778, 2825, 466, 1419, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.24579991879670515, "compression_ratio": 1.6260162601626016, "no_speech_prob": 9.080278687179089e-06}, {"id": 532, "seek": 221622, "start": 2233.0, "end": 2236.4399999999996, "text": " And so I would assume with Netflix, that's where the...", "tokens": [400, 370, 286, 576, 6552, 365, 12778, 11, 300, 311, 689, 264, 485], "temperature": 0.0, "avg_logprob": -0.24579991879670515, "compression_ratio": 1.6260162601626016, "no_speech_prob": 9.080278687179089e-06}, {"id": 533, "seek": 221622, "start": 2236.4399999999996, "end": 2240.72, "text": " I don't know, someone has put in a movie rating that's just wrong.", "tokens": [286, 500, 380, 458, 11, 1580, 575, 829, 294, 257, 3169, 10990, 300, 311, 445, 2085, 13], "temperature": 0.0, "avg_logprob": -0.24579991879670515, "compression_ratio": 1.6260162601626016, "no_speech_prob": 9.080278687179089e-06}, {"id": 534, "seek": 221622, "start": 2240.72, "end": 2245.72, "text": " It's not how they felt about the movie.", "tokens": [467, 311, 406, 577, 436, 2762, 466, 264, 3169, 13], "temperature": 0.0, "avg_logprob": -0.24579991879670515, "compression_ratio": 1.6260162601626016, "no_speech_prob": 9.080278687179089e-06}, {"id": 535, "seek": 224572, "start": 2245.72, "end": 2250.8399999999997, "text": " And perhaps they made a mistake or accidentally entered it or weren't trying to enter what", "tokens": [400, 4317, 436, 1027, 257, 6146, 420, 15715, 9065, 309, 420, 4999, 380, 1382, 281, 3242, 437], "temperature": 0.0, "avg_logprob": -0.15871665000915527, "compression_ratio": 1.6504065040650406, "no_speech_prob": 6.854136699985247e-06}, {"id": 536, "seek": 224572, "start": 2250.8399999999997, "end": 2251.8399999999997, "text": " they did.", "tokens": [436, 630, 13], "temperature": 0.0, "avg_logprob": -0.15871665000915527, "compression_ratio": 1.6504065040650406, "no_speech_prob": 6.854136699985247e-06}, {"id": 537, "seek": 224572, "start": 2251.8399999999997, "end": 2257.3999999999996, "text": " Yeah, and so it's kind of important to note here.", "tokens": [865, 11, 293, 370, 309, 311, 733, 295, 1021, 281, 3637, 510, 13], "temperature": 0.0, "avg_logprob": -0.15871665000915527, "compression_ratio": 1.6504065040650406, "no_speech_prob": 6.854136699985247e-06}, {"id": 538, "seek": 224572, "start": 2257.3999999999996, "end": 2261.04, "text": " So this problem with the faces is different from...", "tokens": [407, 341, 1154, 365, 264, 8475, 307, 819, 490, 485], "temperature": 0.0, "avg_logprob": -0.15871665000915527, "compression_ratio": 1.6504065040650406, "no_speech_prob": 6.854136699985247e-06}, {"id": 539, "seek": 224572, "start": 2261.04, "end": 2262.7599999999998, "text": " And I mean, this does...", "tokens": [400, 286, 914, 11, 341, 775, 485], "temperature": 0.0, "avg_logprob": -0.15871665000915527, "compression_ratio": 1.6504065040650406, "no_speech_prob": 6.854136699985247e-06}, {"id": 540, "seek": 224572, "start": 2262.7599999999998, "end": 2267.48, "text": " The original data set here was showing pictures of the faces lit up from different angles.", "tokens": [440, 3380, 1412, 992, 510, 390, 4099, 5242, 295, 264, 8475, 7997, 493, 490, 819, 14708, 13], "temperature": 0.0, "avg_logprob": -0.15871665000915527, "compression_ratio": 1.6504065040650406, "no_speech_prob": 6.854136699985247e-06}, {"id": 541, "seek": 224572, "start": 2267.48, "end": 2272.0, "text": " So you did have kind of the same face many times with different lighting sequences, but", "tokens": [407, 291, 630, 362, 733, 295, 264, 912, 1851, 867, 1413, 365, 819, 9577, 22978, 11, 457], "temperature": 0.0, "avg_logprob": -0.15871665000915527, "compression_ratio": 1.6504065040650406, "no_speech_prob": 6.854136699985247e-06}, {"id": 542, "seek": 227200, "start": 2272.0, "end": 2278.6, "text": " it's really different in that kind of the faces, the low rank part, which was the background,", "tokens": [309, 311, 534, 819, 294, 300, 733, 295, 264, 8475, 11, 264, 2295, 6181, 644, 11, 597, 390, 264, 3678, 11], "temperature": 0.0, "avg_logprob": -0.13852867003410094, "compression_ratio": 1.8393574297188755, "no_speech_prob": 2.947962002508575e-06}, {"id": 543, "seek": 227200, "start": 2278.6, "end": 2282.68, "text": " and then the sparse component is these pixels that are completely wrong.", "tokens": [293, 550, 264, 637, 11668, 6542, 307, 613, 18668, 300, 366, 2584, 2085, 13], "temperature": 0.0, "avg_logprob": -0.13852867003410094, "compression_ratio": 1.8393574297188755, "no_speech_prob": 2.947962002508575e-06}, {"id": 544, "seek": 227200, "start": 2282.68, "end": 2287.32, "text": " And so here, we're only interested in the sparse component so we can get rid of it.", "tokens": [400, 370, 510, 11, 321, 434, 787, 3102, 294, 264, 637, 11668, 6542, 370, 321, 393, 483, 3973, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.13852867003410094, "compression_ratio": 1.8393574297188755, "no_speech_prob": 2.947962002508575e-06}, {"id": 545, "seek": 227200, "start": 2287.32, "end": 2291.08, "text": " It's not like with the background removal where it's like, oh, we're interested in the", "tokens": [467, 311, 406, 411, 365, 264, 3678, 17933, 689, 309, 311, 411, 11, 1954, 11, 321, 434, 3102, 294, 264], "temperature": 0.0, "avg_logprob": -0.13852867003410094, "compression_ratio": 1.8393574297188755, "no_speech_prob": 2.947962002508575e-06}, {"id": 546, "seek": 227200, "start": 2291.08, "end": 2292.08, "text": " people, which are sparse.", "tokens": [561, 11, 597, 366, 637, 11668, 13], "temperature": 0.0, "avg_logprob": -0.13852867003410094, "compression_ratio": 1.8393574297188755, "no_speech_prob": 2.947962002508575e-06}, {"id": 547, "seek": 227200, "start": 2292.08, "end": 2295.36, "text": " Whereas here, it's just like, okay, we want to know how to get rid of this sparse wrong", "tokens": [13813, 510, 11, 309, 311, 445, 411, 11, 1392, 11, 321, 528, 281, 458, 577, 281, 483, 3973, 295, 341, 637, 11668, 2085], "temperature": 0.0, "avg_logprob": -0.13852867003410094, "compression_ratio": 1.8393574297188755, "no_speech_prob": 2.947962002508575e-06}, {"id": 548, "seek": 227200, "start": 2295.36, "end": 2300.4, "text": " stuff.", "tokens": [1507, 13], "temperature": 0.0, "avg_logprob": -0.13852867003410094, "compression_ratio": 1.8393574297188755, "no_speech_prob": 2.947962002508575e-06}, {"id": 549, "seek": 230040, "start": 2300.4, "end": 2305.4, "text": " And then I also wanted to highlight again, I think I have it on here, with topic modeling,", "tokens": [400, 550, 286, 611, 1415, 281, 5078, 797, 11, 286, 519, 286, 362, 309, 322, 510, 11, 365, 4829, 15983, 11], "temperature": 0.0, "avg_logprob": -0.13369511640988863, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2218421034049243e-05}, {"id": 550, "seek": 230040, "start": 2305.4, "end": 2307.1600000000003, "text": " we could have used robust PCA.", "tokens": [321, 727, 362, 1143, 13956, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.13369511640988863, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2218421034049243e-05}, {"id": 551, "seek": 230040, "start": 2307.1600000000003, "end": 2313.1600000000003, "text": " So we just used SVD or NMF on topic modeling in the first week.", "tokens": [407, 321, 445, 1143, 31910, 35, 420, 426, 44, 37, 322, 4829, 15983, 294, 264, 700, 1243, 13], "temperature": 0.0, "avg_logprob": -0.13369511640988863, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2218421034049243e-05}, {"id": 552, "seek": 230040, "start": 2313.1600000000003, "end": 2319.08, "text": " But here, the low rank part could be common words that are in all documents.", "tokens": [583, 510, 11, 264, 2295, 6181, 644, 727, 312, 2689, 2283, 300, 366, 294, 439, 8512, 13], "temperature": 0.0, "avg_logprob": -0.13369511640988863, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2218421034049243e-05}, {"id": 553, "seek": 230040, "start": 2319.08, "end": 2323.7200000000003, "text": " And then the sparse part could be a few key words from each document that kind of make", "tokens": [400, 550, 264, 637, 11668, 644, 727, 312, 257, 1326, 2141, 2283, 490, 1184, 4166, 300, 733, 295, 652], "temperature": 0.0, "avg_logprob": -0.13369511640988863, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2218421034049243e-05}, {"id": 554, "seek": 230040, "start": 2323.7200000000003, "end": 2328.8, "text": " that document different from the others.", "tokens": [300, 4166, 819, 490, 264, 2357, 13], "temperature": 0.0, "avg_logprob": -0.13369511640988863, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2218421034049243e-05}, {"id": 555, "seek": 232880, "start": 2328.8, "end": 2332.82, "text": " And I would actually, kind of going back to Tim's question of like, isn't robust PCA always", "tokens": [400, 286, 576, 767, 11, 733, 295, 516, 646, 281, 7172, 311, 1168, 295, 411, 11, 1943, 380, 13956, 6465, 32, 1009], "temperature": 0.0, "avg_logprob": -0.21235431943620955, "compression_ratio": 1.5480769230769231, "no_speech_prob": 2.4823755211400567e-06}, {"id": 556, "seek": 232880, "start": 2332.82, "end": 2333.82, "text": " going to be better?", "tokens": [516, 281, 312, 1101, 30], "temperature": 0.0, "avg_logprob": -0.21235431943620955, "compression_ratio": 1.5480769230769231, "no_speech_prob": 2.4823755211400567e-06}, {"id": 557, "seek": 232880, "start": 2333.82, "end": 2339.4, "text": " Yeah, I would be really curious to see some topic modeling done with this because it does", "tokens": [865, 11, 286, 576, 312, 534, 6369, 281, 536, 512, 4829, 15983, 1096, 365, 341, 570, 309, 775], "temperature": 0.0, "avg_logprob": -0.21235431943620955, "compression_ratio": 1.5480769230769231, "no_speech_prob": 2.4823755211400567e-06}, {"id": 558, "seek": 232880, "start": 2339.4, "end": 2343.84, "text": " sound really promising to me to kind of have the kind of like the sparse words defining", "tokens": [1626, 534, 20257, 281, 385, 281, 733, 295, 362, 264, 733, 295, 411, 264, 637, 11668, 2283, 17827], "temperature": 0.0, "avg_logprob": -0.21235431943620955, "compression_ratio": 1.5480769230769231, "no_speech_prob": 2.4823755211400567e-06}, {"id": 559, "seek": 232880, "start": 2343.84, "end": 2351.2400000000002, "text": " a document.", "tokens": [257, 4166, 13], "temperature": 0.0, "avg_logprob": -0.21235431943620955, "compression_ratio": 1.5480769230769231, "no_speech_prob": 2.4823755211400567e-06}, {"id": 560, "seek": 232880, "start": 2351.2400000000002, "end": 2355.2400000000002, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.21235431943620955, "compression_ratio": 1.5480769230769231, "no_speech_prob": 2.4823755211400567e-06}, {"id": 561, "seek": 235524, "start": 2355.24, "end": 2363.0, "text": " Okay, so we're going to move on to the LU decomposition.", "tokens": [1033, 11, 370, 321, 434, 516, 281, 1286, 322, 281, 264, 31851, 48356, 13], "temperature": 0.0, "avg_logprob": -0.1966088104248047, "compression_ratio": 1.4450261780104712, "no_speech_prob": 8.800798241281882e-06}, {"id": 562, "seek": 235524, "start": 2363.0, "end": 2368.12, "text": " Let me just make sure I've done all the results.", "tokens": [961, 385, 445, 652, 988, 286, 600, 1096, 439, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.1966088104248047, "compression_ratio": 1.4450261780104712, "no_speech_prob": 8.800798241281882e-06}, {"id": 563, "seek": 235524, "start": 2368.12, "end": 2377.3999999999996, "text": " Yeah, so just again, here are, and I wrote a little kind of helper method plot images", "tokens": [865, 11, 370, 445, 797, 11, 510, 366, 11, 293, 286, 4114, 257, 707, 733, 295, 36133, 3170, 7542, 5267], "temperature": 0.0, "avg_logprob": -0.1966088104248047, "compression_ratio": 1.4450261780104712, "no_speech_prob": 8.800798241281882e-06}, {"id": 564, "seek": 235524, "start": 2377.3999999999996, "end": 2382.24, "text": " where you can enter your original matrix, the sparse and low rank ones you got back,", "tokens": [689, 291, 393, 3242, 428, 3380, 8141, 11, 264, 637, 11668, 293, 2295, 6181, 2306, 291, 658, 646, 11], "temperature": 0.0, "avg_logprob": -0.1966088104248047, "compression_ratio": 1.4450261780104712, "no_speech_prob": 8.800798241281882e-06}, {"id": 565, "seek": 238224, "start": 2382.24, "end": 2386.3199999999997, "text": " and then just however many points in time you want to see since I think it's kind of", "tokens": [293, 550, 445, 4461, 867, 2793, 294, 565, 291, 528, 281, 536, 1670, 286, 519, 309, 311, 733, 295], "temperature": 0.0, "avg_logprob": -0.19207877861826042, "compression_ratio": 1.4947916666666667, "no_speech_prob": 4.356721092335647e-06}, {"id": 566, "seek": 238224, "start": 2386.3199999999997, "end": 2391.4399999999996, "text": " most helpful to see, you know, them lined up next to each other from this is at time", "tokens": [881, 4961, 281, 536, 11, 291, 458, 11, 552, 17189, 493, 958, 281, 1184, 661, 490, 341, 307, 412, 565], "temperature": 0.0, "avg_logprob": -0.19207877861826042, "compression_ratio": 1.4947916666666667, "no_speech_prob": 4.356721092335647e-06}, {"id": 567, "seek": 238224, "start": 2391.4399999999996, "end": 2400.2799999999997, "text": " zero or kind of whatever corresponded to the zeroth column at 100 and at 1000.", "tokens": [4018, 420, 733, 295, 2035, 6805, 292, 281, 264, 44746, 900, 7738, 412, 2319, 293, 412, 9714, 13], "temperature": 0.0, "avg_logprob": -0.19207877861826042, "compression_ratio": 1.4947916666666667, "no_speech_prob": 4.356721092335647e-06}, {"id": 568, "seek": 238224, "start": 2400.2799999999997, "end": 2405.3999999999996, "text": " Yeah, let's start on LU decomposition.", "tokens": [865, 11, 718, 311, 722, 322, 31851, 48356, 13], "temperature": 0.0, "avg_logprob": -0.19207877861826042, "compression_ratio": 1.4947916666666667, "no_speech_prob": 4.356721092335647e-06}, {"id": 569, "seek": 240540, "start": 2405.4, "end": 2412.08, "text": " So above, we had used Facebook PCA, which is a randomized SVD.", "tokens": [407, 3673, 11, 321, 632, 1143, 4384, 6465, 32, 11, 597, 307, 257, 38513, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.18225783168679416, "compression_ratio": 1.6050420168067228, "no_speech_prob": 4.181104316103301e-07}, {"id": 570, "seek": 240540, "start": 2412.08, "end": 2416.1600000000003, "text": " And then in the previous lesson, we had kind of written our own randomized range finder,", "tokens": [400, 550, 294, 264, 3894, 6898, 11, 321, 632, 733, 295, 3720, 527, 1065, 38513, 3613, 915, 260, 11], "temperature": 0.0, "avg_logprob": -0.18225783168679416, "compression_ratio": 1.6050420168067228, "no_speech_prob": 4.181104316103301e-07}, {"id": 571, "seek": 240540, "start": 2416.1600000000003, "end": 2422.52, "text": " which was basically just a less robust version of scikit learns one or I mean, it did less", "tokens": [597, 390, 1936, 445, 257, 1570, 13956, 3037, 295, 2180, 22681, 27152, 472, 420, 286, 914, 11, 309, 630, 1570], "temperature": 0.0, "avg_logprob": -0.18225783168679416, "compression_ratio": 1.6050420168067228, "no_speech_prob": 4.181104316103301e-07}, {"id": 572, "seek": 240540, "start": 2422.52, "end": 2426.6800000000003, "text": " error checking for both of those use LU factorization.", "tokens": [6713, 8568, 337, 1293, 295, 729, 764, 31851, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.18225783168679416, "compression_ratio": 1.6050420168067228, "no_speech_prob": 4.181104316103301e-07}, {"id": 573, "seek": 240540, "start": 2426.6800000000003, "end": 2432.2400000000002, "text": " And so we're going to go into kind of how how to do an LU factorization, since we've", "tokens": [400, 370, 321, 434, 516, 281, 352, 666, 733, 295, 577, 577, 281, 360, 364, 31851, 5952, 2144, 11, 1670, 321, 600], "temperature": 0.0, "avg_logprob": -0.18225783168679416, "compression_ratio": 1.6050420168067228, "no_speech_prob": 4.181104316103301e-07}, {"id": 574, "seek": 243224, "start": 2432.24, "end": 2435.68, "text": " kind of used some methods that use it.", "tokens": [733, 295, 1143, 512, 7150, 300, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.21548898341292042, "compression_ratio": 1.5126582278481013, "no_speech_prob": 3.0415649234782904e-06}, {"id": 575, "seek": 243224, "start": 2435.68, "end": 2441.68, "text": " LU factorization factors a matrix into the product of a lower triangular matrix, that's", "tokens": [31851, 5952, 2144, 6771, 257, 8141, 666, 264, 1674, 295, 257, 3126, 38190, 8141, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.21548898341292042, "compression_ratio": 1.5126582278481013, "no_speech_prob": 3.0415649234782904e-06}, {"id": 576, "seek": 243224, "start": 2441.68, "end": 2447.7999999999997, "text": " the L and an upper triangular matrix, which is the U.", "tokens": [264, 441, 293, 364, 6597, 38190, 8141, 11, 597, 307, 264, 624, 13], "temperature": 0.0, "avg_logprob": -0.21548898341292042, "compression_ratio": 1.5126582278481013, "no_speech_prob": 3.0415649234782904e-06}, {"id": 577, "seek": 243224, "start": 2447.7999999999997, "end": 2457.8399999999997, "text": " Then I wanted to check who remembers Gaussian elimination.", "tokens": [1396, 286, 1415, 281, 1520, 567, 26228, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.21548898341292042, "compression_ratio": 1.5126582278481013, "no_speech_prob": 3.0415649234782904e-06}, {"id": 578, "seek": 245784, "start": 2457.84, "end": 2462.4, "text": " And then who who wants more review on Gaussian elimination?", "tokens": [400, 550, 567, 567, 2738, 544, 3131, 322, 39148, 29224, 30], "temperature": 0.0, "avg_logprob": -0.19775280320500754, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.8447851743985666e-06}, {"id": 579, "seek": 245784, "start": 2462.4, "end": 2467.1600000000003, "text": " Okay, so let me, I'm going to go through one example.", "tokens": [1033, 11, 370, 718, 385, 11, 286, 478, 516, 281, 352, 807, 472, 1365, 13], "temperature": 0.0, "avg_logprob": -0.19775280320500754, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.8447851743985666e-06}, {"id": 580, "seek": 245784, "start": 2467.1600000000003, "end": 2470.6000000000004, "text": " And we can even if you want to see a second example, let me know because I know it may", "tokens": [400, 321, 393, 754, 498, 291, 528, 281, 536, 257, 1150, 1365, 11, 718, 385, 458, 570, 286, 458, 309, 815], "temperature": 0.0, "avg_logprob": -0.19775280320500754, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.8447851743985666e-06}, {"id": 581, "seek": 245784, "start": 2470.6000000000004, "end": 2474.56, "text": " have been a very long time since you've done Gaussian elimination, kind of depending on", "tokens": [362, 668, 257, 588, 938, 565, 1670, 291, 600, 1096, 39148, 29224, 11, 733, 295, 5413, 322], "temperature": 0.0, "avg_logprob": -0.19775280320500754, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.8447851743985666e-06}, {"id": 582, "seek": 245784, "start": 2474.56, "end": 2483.52, "text": " when you took linear algebra last.", "tokens": [562, 291, 1890, 8213, 21989, 1036, 13], "temperature": 0.0, "avg_logprob": -0.19775280320500754, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.8447851743985666e-06}, {"id": 583, "seek": 248352, "start": 2483.52, "end": 2493.7599999999998, "text": " Okay, so ignore the LU down here, we're just going to be that'll come up in a moment.", "tokens": [1033, 11, 370, 11200, 264, 31851, 760, 510, 11, 321, 434, 445, 516, 281, 312, 300, 603, 808, 493, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.24113448890479836, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.9220984540879726e-05}, {"id": 584, "seek": 248352, "start": 2493.7599999999998, "end": 2501.24, "text": " But we're just going to take this matrix A and want to do a Gaussian elimination on it.", "tokens": [583, 321, 434, 445, 516, 281, 747, 341, 8141, 316, 293, 528, 281, 360, 257, 39148, 29224, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.24113448890479836, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.9220984540879726e-05}, {"id": 585, "seek": 248352, "start": 2501.24, "end": 2504.72, "text": " Actually, does anyone remember?", "tokens": [5135, 11, 775, 2878, 1604, 30], "temperature": 0.0, "avg_logprob": -0.24113448890479836, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.9220984540879726e-05}, {"id": 586, "seek": 248352, "start": 2504.72, "end": 2510.32, "text": " So what's the kind of the first thing you do with Gaussian elimination?", "tokens": [407, 437, 311, 264, 733, 295, 264, 700, 551, 291, 360, 365, 39148, 29224, 30], "temperature": 0.0, "avg_logprob": -0.24113448890479836, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.9220984540879726e-05}, {"id": 587, "seek": 251032, "start": 2510.32, "end": 2527.88, "text": " Like you're trying to reduce it to, yeah, well, I mean, there are a lot of different", "tokens": [1743, 291, 434, 1382, 281, 5407, 309, 281, 11, 1338, 11, 731, 11, 286, 914, 11, 456, 366, 257, 688, 295, 819], "temperature": 0.0, "avg_logprob": -0.24149696410648405, "compression_ratio": 1.4038461538461537, "no_speech_prob": 5.337865331966896e-06}, {"id": 588, "seek": 251032, "start": 2527.88, "end": 2529.04, "text": " ways to talk about it.", "tokens": [2098, 281, 751, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.24149696410648405, "compression_ratio": 1.4038461538461537, "no_speech_prob": 5.337865331966896e-06}, {"id": 589, "seek": 251032, "start": 2529.04, "end": 2533.8, "text": " I was just going to say you kind of want to get rid of everything beneath one, turn those", "tokens": [286, 390, 445, 516, 281, 584, 291, 733, 295, 528, 281, 483, 3973, 295, 1203, 17149, 472, 11, 1261, 729], "temperature": 0.0, "avg_logprob": -0.24149696410648405, "compression_ratio": 1.4038461538461537, "no_speech_prob": 5.337865331966896e-06}, {"id": 590, "seek": 251032, "start": 2533.8, "end": 2535.8, "text": " all into zeros first.", "tokens": [439, 666, 35193, 700, 13], "temperature": 0.0, "avg_logprob": -0.24149696410648405, "compression_ratio": 1.4038461538461537, "no_speech_prob": 5.337865331966896e-06}, {"id": 591, "seek": 253580, "start": 2535.8, "end": 2541.1600000000003, "text": " Although yeah, like long range, I think you're you're doing other stuff.", "tokens": [5780, 1338, 11, 411, 938, 3613, 11, 286, 519, 291, 434, 291, 434, 884, 661, 1507, 13], "temperature": 0.0, "avg_logprob": -0.17736378940967246, "compression_ratio": 1.646090534979424, "no_speech_prob": 7.76668639446143e-06}, {"id": 592, "seek": 253580, "start": 2541.1600000000003, "end": 2546.0800000000004, "text": " And this reminds me of a I think it was Trevathan that says at some point, like so much of numerical", "tokens": [400, 341, 12025, 385, 295, 257, 286, 519, 309, 390, 8648, 85, 9390, 300, 1619, 412, 512, 935, 11, 411, 370, 709, 295, 29054], "temperature": 0.0, "avg_logprob": -0.17736378940967246, "compression_ratio": 1.646090534979424, "no_speech_prob": 7.76668639446143e-06}, {"id": 593, "seek": 253580, "start": 2546.0800000000004, "end": 2549.5600000000004, "text": " linear algebra is just like inserting zeros into matrices.", "tokens": [8213, 21989, 307, 445, 411, 46567, 35193, 666, 32284, 13], "temperature": 0.0, "avg_logprob": -0.17736378940967246, "compression_ratio": 1.646090534979424, "no_speech_prob": 7.76668639446143e-06}, {"id": 594, "seek": 253580, "start": 2549.5600000000004, "end": 2555.76, "text": " But here, you kind of kind of want to take this first row, and we want to get rid of", "tokens": [583, 510, 11, 291, 733, 295, 733, 295, 528, 281, 747, 341, 700, 5386, 11, 293, 321, 528, 281, 483, 3973, 295], "temperature": 0.0, "avg_logprob": -0.17736378940967246, "compression_ratio": 1.646090534979424, "no_speech_prob": 7.76668639446143e-06}, {"id": 595, "seek": 253580, "start": 2555.76, "end": 2558.04, "text": " the three in the second row.", "tokens": [264, 1045, 294, 264, 1150, 5386, 13], "temperature": 0.0, "avg_logprob": -0.17736378940967246, "compression_ratio": 1.646090534979424, "no_speech_prob": 7.76668639446143e-06}, {"id": 596, "seek": 253580, "start": 2558.04, "end": 2564.9, "text": " So we can end down here, we're going to put a matrix.", "tokens": [407, 321, 393, 917, 760, 510, 11, 321, 434, 516, 281, 829, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.17736378940967246, "compression_ratio": 1.646090534979424, "no_speech_prob": 7.76668639446143e-06}, {"id": 597, "seek": 256490, "start": 2564.9, "end": 2568.84, "text": " So this matrix in the bottom corner, this is not something you would have done typically", "tokens": [407, 341, 8141, 294, 264, 2767, 4538, 11, 341, 307, 406, 746, 291, 576, 362, 1096, 5850], "temperature": 0.0, "avg_logprob": -0.10271663476925084, "compression_ratio": 1.7161572052401746, "no_speech_prob": 6.7479422796168365e-06}, {"id": 598, "seek": 256490, "start": 2568.84, "end": 2573.04, "text": " in numerical linear algebra, but we're going to keep track of what we multiply by.", "tokens": [294, 29054, 8213, 21989, 11, 457, 321, 434, 516, 281, 1066, 2837, 295, 437, 321, 12972, 538, 13], "temperature": 0.0, "avg_logprob": -0.10271663476925084, "compression_ratio": 1.7161572052401746, "no_speech_prob": 6.7479422796168365e-06}, {"id": 599, "seek": 256490, "start": 2573.04, "end": 2581.14, "text": " But here, I'm going to kind of multiply this first row by three and subtract it to zero", "tokens": [583, 510, 11, 286, 478, 516, 281, 733, 295, 12972, 341, 700, 5386, 538, 1045, 293, 16390, 309, 281, 4018], "temperature": 0.0, "avg_logprob": -0.10271663476925084, "compression_ratio": 1.7161572052401746, "no_speech_prob": 6.7479422796168365e-06}, {"id": 600, "seek": 256490, "start": 2581.14, "end": 2582.92, "text": " this guy out.", "tokens": [341, 2146, 484, 13], "temperature": 0.0, "avg_logprob": -0.10271663476925084, "compression_ratio": 1.7161572052401746, "no_speech_prob": 6.7479422796168365e-06}, {"id": 601, "seek": 256490, "start": 2582.92, "end": 2591.52, "text": " So first row is not going to change one, negative two, negative two, negative three.", "tokens": [407, 700, 5386, 307, 406, 516, 281, 1319, 472, 11, 3671, 732, 11, 3671, 732, 11, 3671, 1045, 13], "temperature": 0.0, "avg_logprob": -0.10271663476925084, "compression_ratio": 1.7161572052401746, "no_speech_prob": 6.7479422796168365e-06}, {"id": 602, "seek": 256490, "start": 2591.52, "end": 2592.88, "text": " And our goal is to make that zero.", "tokens": [400, 527, 3387, 307, 281, 652, 300, 4018, 13], "temperature": 0.0, "avg_logprob": -0.10271663476925084, "compression_ratio": 1.7161572052401746, "no_speech_prob": 6.7479422796168365e-06}, {"id": 603, "seek": 259288, "start": 2592.88, "end": 2597.4, "text": " And so we did that, or actually, yeah, we did that by multiplying by three and subtracting.", "tokens": [400, 370, 321, 630, 300, 11, 420, 767, 11, 1338, 11, 321, 630, 300, 538, 30955, 538, 1045, 293, 16390, 278, 13], "temperature": 0.0, "avg_logprob": -0.22418401012681935, "compression_ratio": 1.7108433734939759, "no_speech_prob": 7.646362973900978e-06}, {"id": 604, "seek": 259288, "start": 2597.4, "end": 2606.28, "text": " So that makes the top one would become negative six, subtract that.", "tokens": [407, 300, 1669, 264, 1192, 472, 576, 1813, 3671, 2309, 11, 16390, 300, 13], "temperature": 0.0, "avg_logprob": -0.22418401012681935, "compression_ratio": 1.7108433734939759, "no_speech_prob": 7.646362973900978e-06}, {"id": 605, "seek": 259288, "start": 2606.28, "end": 2612.2000000000003, "text": " So negative nine plus six is negative three.", "tokens": [407, 3671, 4949, 1804, 2309, 307, 3671, 1045, 13], "temperature": 0.0, "avg_logprob": -0.22418401012681935, "compression_ratio": 1.7108433734939759, "no_speech_prob": 7.646362973900978e-06}, {"id": 606, "seek": 259288, "start": 2612.2000000000003, "end": 2613.96, "text": " Negative six and zero.", "tokens": [43230, 2309, 293, 4018, 13], "temperature": 0.0, "avg_logprob": -0.22418401012681935, "compression_ratio": 1.7108433734939759, "no_speech_prob": 7.646362973900978e-06}, {"id": 607, "seek": 259288, "start": 2613.96, "end": 2619.1600000000003, "text": " That's positive six is actually zero minus negative six.", "tokens": [663, 311, 3353, 2309, 307, 767, 4018, 3175, 3671, 2309, 13], "temperature": 0.0, "avg_logprob": -0.22418401012681935, "compression_ratio": 1.7108433734939759, "no_speech_prob": 7.646362973900978e-06}, {"id": 608, "seek": 261916, "start": 2619.16, "end": 2626.2, "text": " And then times three, those are going to cancel out because we ended up with negative nine", "tokens": [400, 550, 1413, 1045, 11, 729, 366, 516, 281, 10373, 484, 570, 321, 4590, 493, 365, 3671, 4949], "temperature": 0.0, "avg_logprob": -0.24257055318580484, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.972785846912302e-06}, {"id": 609, "seek": 261916, "start": 2626.2, "end": 2627.96, "text": " minus negative nine.", "tokens": [3175, 3671, 4949, 13], "temperature": 0.0, "avg_logprob": -0.24257055318580484, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.972785846912302e-06}, {"id": 610, "seek": 261916, "start": 2627.96, "end": 2633.48, "text": " So that's kind of step one of Gaussian elimination.", "tokens": [407, 300, 311, 733, 295, 1823, 472, 295, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.24257055318580484, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.972785846912302e-06}, {"id": 611, "seek": 261916, "start": 2633.48, "end": 2636.0, "text": " Questions about that?", "tokens": [27738, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.24257055318580484, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.972785846912302e-06}, {"id": 612, "seek": 261916, "start": 2636.0, "end": 2637.0, "text": " And initially, yes?", "tokens": [400, 9105, 11, 2086, 30], "temperature": 0.0, "avg_logprob": -0.24257055318580484, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.972785846912302e-06}, {"id": 613, "seek": 261916, "start": 2637.0, "end": 2641.92, "text": " Why do we, so just to warn you, Gaussian elimination is a thing that made me never want to see", "tokens": [1545, 360, 321, 11, 370, 445, 281, 12286, 291, 11, 39148, 29224, 307, 257, 551, 300, 1027, 385, 1128, 528, 281, 536], "temperature": 0.0, "avg_logprob": -0.24257055318580484, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.972785846912302e-06}, {"id": 614, "seek": 261916, "start": 2641.92, "end": 2642.92, "text": " linear algebra again.", "tokens": [8213, 21989, 797, 13], "temperature": 0.0, "avg_logprob": -0.24257055318580484, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.972785846912302e-06}, {"id": 615, "seek": 261916, "start": 2642.92, "end": 2648.7599999999998, "text": " You can come back to it for like 20 years, so I'm very hazy on this.", "tokens": [509, 393, 808, 646, 281, 309, 337, 411, 945, 924, 11, 370, 286, 478, 588, 324, 1229, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.24257055318580484, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.972785846912302e-06}, {"id": 616, "seek": 264876, "start": 2648.76, "end": 2651.6800000000003, "text": " Why is it that we're allowed to like subtract rows from other rows?", "tokens": [1545, 307, 309, 300, 321, 434, 4350, 281, 411, 16390, 13241, 490, 661, 13241, 30], "temperature": 0.0, "avg_logprob": -0.22417107750387752, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.138067995678284e-06}, {"id": 617, "seek": 264876, "start": 2651.6800000000003, "end": 2653.88, "text": " Like what are we doing?", "tokens": [1743, 437, 366, 321, 884, 30], "temperature": 0.0, "avg_logprob": -0.22417107750387752, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.138067995678284e-06}, {"id": 618, "seek": 264876, "start": 2653.88, "end": 2654.88, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.22417107750387752, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.138067995678284e-06}, {"id": 619, "seek": 264876, "start": 2654.88, "end": 2664.88, "text": " So really, actually let me see if I, is there a way to add more space on top, Jeremy?", "tokens": [407, 534, 11, 767, 718, 385, 536, 498, 286, 11, 307, 456, 257, 636, 281, 909, 544, 1901, 322, 1192, 11, 17809, 30], "temperature": 0.0, "avg_logprob": -0.22417107750387752, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.138067995678284e-06}, {"id": 620, "seek": 264876, "start": 2664.88, "end": 2672.32, "text": " Well, I wanted to be able to write more, but okay.", "tokens": [1042, 11, 286, 1415, 281, 312, 1075, 281, 2464, 544, 11, 457, 1392, 13], "temperature": 0.0, "avg_logprob": -0.22417107750387752, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.138067995678284e-06}, {"id": 621, "seek": 264876, "start": 2672.32, "end": 2676.92, "text": " So really what we're, often where these problems come up is when you're solving a system of", "tokens": [407, 534, 437, 321, 434, 11, 2049, 689, 613, 2740, 808, 493, 307, 562, 291, 434, 12606, 257, 1185, 295], "temperature": 0.0, "avg_logprob": -0.22417107750387752, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.138067995678284e-06}, {"id": 622, "seek": 264876, "start": 2676.92, "end": 2678.4, "text": " equations.", "tokens": [11787, 13], "temperature": 0.0, "avg_logprob": -0.22417107750387752, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.138067995678284e-06}, {"id": 623, "seek": 267840, "start": 2678.4, "end": 2688.6800000000003, "text": " And so you might have, I don't know, maybe this was one X minus two Y minus two Z minus", "tokens": [400, 370, 291, 1062, 362, 11, 286, 500, 380, 458, 11, 1310, 341, 390, 472, 1783, 3175, 732, 398, 3175, 732, 1176, 3175], "temperature": 0.0, "avg_logprob": -0.1105526606241862, "compression_ratio": 1.6737967914438503, "no_speech_prob": 2.947991788460058e-06}, {"id": 624, "seek": 267840, "start": 2688.6800000000003, "end": 2694.32, "text": " three W. And then we weren't dealing with a vector B, but often you would have some", "tokens": [1045, 343, 13, 400, 550, 321, 4999, 380, 6260, 365, 257, 8062, 363, 11, 457, 2049, 291, 576, 362, 512], "temperature": 0.0, "avg_logprob": -0.1105526606241862, "compression_ratio": 1.6737967914438503, "no_speech_prob": 2.947991788460058e-06}, {"id": 625, "seek": 267840, "start": 2694.32, "end": 2695.36, "text": " value over here.", "tokens": [2158, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.1105526606241862, "compression_ratio": 1.6737967914438503, "no_speech_prob": 2.947991788460058e-06}, {"id": 626, "seek": 267840, "start": 2695.36, "end": 2697.52, "text": " Like I don't know, maybe this is five.", "tokens": [1743, 286, 500, 380, 458, 11, 1310, 341, 307, 1732, 13], "temperature": 0.0, "avg_logprob": -0.1105526606241862, "compression_ratio": 1.6737967914438503, "no_speech_prob": 2.947991788460058e-06}, {"id": 627, "seek": 267840, "start": 2697.52, "end": 2706.2000000000003, "text": " And then you have another equation that's three X minus nine Y, no Z term, minus nine", "tokens": [400, 550, 291, 362, 1071, 5367, 300, 311, 1045, 1783, 3175, 4949, 398, 11, 572, 1176, 1433, 11, 3175, 4949], "temperature": 0.0, "avg_logprob": -0.1105526606241862, "compression_ratio": 1.6737967914438503, "no_speech_prob": 2.947991788460058e-06}, {"id": 628, "seek": 270620, "start": 2706.2, "end": 2709.9199999999996, "text": " W equals 10 or something.", "tokens": [343, 6915, 1266, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.13900948809338853, "compression_ratio": 1.5364583333333333, "no_speech_prob": 7.45368367915944e-07}, {"id": 629, "seek": 270620, "start": 2709.9199999999996, "end": 2713.96, "text": " And so one way to solve these, if you're just kind of thinking about them as systems of", "tokens": [400, 370, 472, 636, 281, 5039, 613, 11, 498, 291, 434, 445, 733, 295, 1953, 466, 552, 382, 3652, 295], "temperature": 0.0, "avg_logprob": -0.13900948809338853, "compression_ratio": 1.5364583333333333, "no_speech_prob": 7.45368367915944e-07}, {"id": 630, "seek": 270620, "start": 2713.96, "end": 2721.2, "text": " equations is you could multiply the first equation by negative three and add those together.", "tokens": [11787, 307, 291, 727, 12972, 264, 700, 5367, 538, 3671, 1045, 293, 909, 729, 1214, 13], "temperature": 0.0, "avg_logprob": -0.13900948809338853, "compression_ratio": 1.5364583333333333, "no_speech_prob": 7.45368367915944e-07}, {"id": 631, "seek": 270620, "start": 2721.2, "end": 2727.24, "text": " Because think, you know, if you had a solution to this, you know, values for X, Y, Z and", "tokens": [1436, 519, 11, 291, 458, 11, 498, 291, 632, 257, 3827, 281, 341, 11, 291, 458, 11, 4190, 337, 1783, 11, 398, 11, 1176, 293], "temperature": 0.0, "avg_logprob": -0.13900948809338853, "compression_ratio": 1.5364583333333333, "no_speech_prob": 7.45368367915944e-07}, {"id": 632, "seek": 272724, "start": 2727.24, "end": 2738.12, "text": " W, multiplying both sides by negative three, you know, that equation is still going to", "tokens": [343, 11, 30955, 1293, 4881, 538, 3671, 1045, 11, 291, 458, 11, 300, 5367, 307, 920, 516, 281], "temperature": 0.0, "avg_logprob": -0.21581084728240968, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.2482535112212645e-06}, {"id": 633, "seek": 272724, "start": 2738.12, "end": 2739.12, "text": " hold.", "tokens": [1797, 13], "temperature": 0.0, "avg_logprob": -0.21581084728240968, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.2482535112212645e-06}, {"id": 634, "seek": 272724, "start": 2739.12, "end": 2743.4399999999996, "text": " You multiply by negative three on both sides and then you can add those together as kind", "tokens": [509, 12972, 538, 3671, 1045, 322, 1293, 4881, 293, 550, 291, 393, 909, 729, 1214, 382, 733], "temperature": 0.0, "avg_logprob": -0.21581084728240968, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.2482535112212645e-06}, {"id": 635, "seek": 272724, "start": 2743.4399999999996, "end": 2746.2799999999997, "text": " of a legitimate thing to do as well.", "tokens": [295, 257, 17956, 551, 281, 360, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21581084728240968, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.2482535112212645e-06}, {"id": 636, "seek": 272724, "start": 2746.2799999999997, "end": 2753.4799999999996, "text": " And that's even kind of, I think like a more intuitive, like if I gave you, actually let", "tokens": [400, 300, 311, 754, 733, 295, 11, 286, 519, 411, 257, 544, 21769, 11, 411, 498, 286, 2729, 291, 11, 767, 718], "temperature": 0.0, "avg_logprob": -0.21581084728240968, "compression_ratio": 1.6157894736842104, "no_speech_prob": 1.2482535112212645e-06}, {"id": 637, "seek": 275348, "start": 2753.48, "end": 2762.12, "text": " me just go to a place of the page that I don't think we'll need.", "tokens": [385, 445, 352, 281, 257, 1081, 295, 264, 3028, 300, 286, 500, 380, 519, 321, 603, 643, 13], "temperature": 0.0, "avg_logprob": -0.22244344438825334, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.6015902676590485e-06}, {"id": 638, "seek": 275348, "start": 2762.12, "end": 2771.2, "text": " You know, if I gave you some problem and we're like solve.", "tokens": [509, 458, 11, 498, 286, 2729, 291, 512, 1154, 293, 321, 434, 411, 5039, 13], "temperature": 0.0, "avg_logprob": -0.22244344438825334, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.6015902676590485e-06}, {"id": 639, "seek": 275348, "start": 2771.2, "end": 2772.6, "text": " Actually this might be too easy.", "tokens": [5135, 341, 1062, 312, 886, 1858, 13], "temperature": 0.0, "avg_logprob": -0.22244344438825334, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.6015902676590485e-06}, {"id": 640, "seek": 275348, "start": 2772.6, "end": 2776.48, "text": " Okay, yeah, because you'd plug in the X. But you could also think of that as like, oh,", "tokens": [1033, 11, 1338, 11, 570, 291, 1116, 5452, 294, 264, 1783, 13, 583, 291, 727, 611, 519, 295, 300, 382, 411, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.22244344438825334, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.6015902676590485e-06}, {"id": 641, "seek": 275348, "start": 2776.48, "end": 2780.2400000000002, "text": " you know, like let me multiply each side by three and subtract those and then I'll get", "tokens": [291, 458, 11, 411, 718, 385, 12972, 1184, 1252, 538, 1045, 293, 16390, 729, 293, 550, 286, 603, 483], "temperature": 0.0, "avg_logprob": -0.22244344438825334, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.6015902676590485e-06}, {"id": 642, "seek": 275348, "start": 2780.2400000000002, "end": 2782.72, "text": " the equation just in terms of Y.", "tokens": [264, 5367, 445, 294, 2115, 295, 398, 13], "temperature": 0.0, "avg_logprob": -0.22244344438825334, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.6015902676590485e-06}, {"id": 643, "seek": 278272, "start": 2782.72, "end": 2788.48, "text": " So that's where this kind of idea of multiplying by rows by things and adding or subtracting", "tokens": [407, 300, 311, 689, 341, 733, 295, 1558, 295, 30955, 538, 13241, 538, 721, 293, 5127, 420, 16390, 278], "temperature": 0.0, "avg_logprob": -0.22901939392089843, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.4738502613909077e-05}, {"id": 644, "seek": 278272, "start": 2788.48, "end": 2789.48, "text": " them.", "tokens": [552, 13], "temperature": 0.0, "avg_logprob": -0.22901939392089843, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.4738502613909077e-05}, {"id": 645, "seek": 278272, "start": 2789.48, "end": 2790.48, "text": " Is that helpful, Jeremy?", "tokens": [1119, 300, 4961, 11, 17809, 30], "temperature": 0.0, "avg_logprob": -0.22901939392089843, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.4738502613909077e-05}, {"id": 646, "seek": 278272, "start": 2790.48, "end": 2794.8399999999997, "text": " Yeah, I think so.", "tokens": [865, 11, 286, 519, 370, 13], "temperature": 0.0, "avg_logprob": -0.22901939392089843, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.4738502613909077e-05}, {"id": 647, "seek": 278272, "start": 2794.8399999999997, "end": 2801.48, "text": " I guess I'm still trying to make the connection to solving equations versus like is that", "tokens": [286, 2041, 286, 478, 920, 1382, 281, 652, 264, 4984, 281, 12606, 11787, 5717, 411, 307, 300], "temperature": 0.0, "avg_logprob": -0.22901939392089843, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.4738502613909077e-05}, {"id": 648, "seek": 278272, "start": 2801.48, "end": 2808.24, "text": " equivalent to any decomposition or just this particular decomposition or why can we, why", "tokens": [10344, 281, 604, 48356, 420, 445, 341, 1729, 48356, 420, 983, 393, 321, 11, 983], "temperature": 0.0, "avg_logprob": -0.22901939392089843, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.4738502613909077e-05}, {"id": 649, "seek": 278272, "start": 2808.24, "end": 2812.64, "text": " is the fact that we can do this in simultaneous equations when we can do this in matrices?", "tokens": [307, 264, 1186, 300, 321, 393, 360, 341, 294, 46218, 11787, 562, 321, 393, 360, 341, 294, 32284, 30], "temperature": 0.0, "avg_logprob": -0.22901939392089843, "compression_ratio": 1.6942148760330578, "no_speech_prob": 1.4738502613909077e-05}, {"id": 650, "seek": 281264, "start": 2812.64, "end": 2817.6, "text": " Well, so the matrix is just representing the system of equations.", "tokens": [1042, 11, 370, 264, 8141, 307, 445, 13460, 264, 1185, 295, 11787, 13], "temperature": 0.0, "avg_logprob": -0.18717622756958008, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.1842804269690532e-05}, {"id": 651, "seek": 281264, "start": 2817.6, "end": 2818.6, "text": " And so-", "tokens": [400, 370, 12], "temperature": 0.0, "avg_logprob": -0.18717622756958008, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.1842804269690532e-05}, {"id": 652, "seek": 281264, "start": 2818.6, "end": 2820.0, "text": " Like in general or in this particular case?", "tokens": [1743, 294, 2674, 420, 294, 341, 1729, 1389, 30], "temperature": 0.0, "avg_logprob": -0.18717622756958008, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.1842804269690532e-05}, {"id": 653, "seek": 281264, "start": 2820.0, "end": 2826.0, "text": " Well, so I mean in this particular case if we were to write the first line as 1X minus", "tokens": [1042, 11, 370, 286, 914, 294, 341, 1729, 1389, 498, 321, 645, 281, 2464, 264, 700, 1622, 382, 502, 55, 3175], "temperature": 0.0, "avg_logprob": -0.18717622756958008, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.1842804269690532e-05}, {"id": 654, "seek": 281264, "start": 2826.0, "end": 2833.2799999999997, "text": " 2I minus 2Z minus 3W and what kind of the broader problem is you might be interested", "tokens": [568, 40, 3175, 568, 57, 3175, 805, 54, 293, 437, 733, 295, 264, 13227, 1154, 307, 291, 1062, 312, 3102], "temperature": 0.0, "avg_logprob": -0.18717622756958008, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.1842804269690532e-05}, {"id": 655, "seek": 281264, "start": 2833.2799999999997, "end": 2838.2, "text": " in solving AX equals B and you might be interested in solving that for a lot of different values", "tokens": [294, 12606, 316, 55, 6915, 363, 293, 291, 1062, 312, 3102, 294, 12606, 300, 337, 257, 688, 295, 819, 4190], "temperature": 0.0, "avg_logprob": -0.18717622756958008, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.1842804269690532e-05}, {"id": 656, "seek": 283820, "start": 2838.2, "end": 2842.7999999999997, "text": " of B and you wouldn't want to have to kind of go through and solve it separately each", "tokens": [295, 363, 293, 291, 2759, 380, 528, 281, 362, 281, 733, 295, 352, 807, 293, 5039, 309, 14759, 1184], "temperature": 0.0, "avg_logprob": -0.17451847753217142, "compression_ratio": 1.859922178988327, "no_speech_prob": 1.6186846551136114e-05}, {"id": 657, "seek": 283820, "start": 2842.7999999999997, "end": 2849.3999999999996, "text": " time and so finding this fact is kind of getting ahead but finding a factorization of A is", "tokens": [565, 293, 370, 5006, 341, 1186, 307, 733, 295, 1242, 2286, 457, 5006, 257, 5952, 2144, 295, 316, 307], "temperature": 0.0, "avg_logprob": -0.17451847753217142, "compression_ratio": 1.859922178988327, "no_speech_prob": 1.6186846551136114e-05}, {"id": 658, "seek": 283820, "start": 2849.3999999999996, "end": 2854.7599999999998, "text": " going to let us then have a quick way of solving it for a bunch of different Bs.", "tokens": [516, 281, 718, 505, 550, 362, 257, 1702, 636, 295, 12606, 309, 337, 257, 3840, 295, 819, 363, 82, 13], "temperature": 0.0, "avg_logprob": -0.17451847753217142, "compression_ratio": 1.859922178988327, "no_speech_prob": 1.6186846551136114e-05}, {"id": 659, "seek": 283820, "start": 2854.7599999999998, "end": 2859.24, "text": " We kind of just factor A once and then we could solve it for a bunch of Bs.", "tokens": [492, 733, 295, 445, 5952, 316, 1564, 293, 550, 321, 727, 5039, 309, 337, 257, 3840, 295, 363, 82, 13], "temperature": 0.0, "avg_logprob": -0.17451847753217142, "compression_ratio": 1.859922178988327, "no_speech_prob": 1.6186846551136114e-05}, {"id": 660, "seek": 283820, "start": 2859.24, "end": 2862.96, "text": " So this idea of thinking of this system of equations is specific to the idea of finding", "tokens": [407, 341, 1558, 295, 1953, 295, 341, 1185, 295, 11787, 307, 2685, 281, 264, 1558, 295, 5006], "temperature": 0.0, "avg_logprob": -0.17451847753217142, "compression_ratio": 1.859922178988327, "no_speech_prob": 1.6186846551136114e-05}, {"id": 661, "seek": 283820, "start": 2862.96, "end": 2863.96, "text": " a factorization.", "tokens": [257, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.17451847753217142, "compression_ratio": 1.859922178988327, "no_speech_prob": 1.6186846551136114e-05}, {"id": 662, "seek": 283820, "start": 2863.96, "end": 2866.7599999999998, "text": " That's where we can use this technique.", "tokens": [663, 311, 689, 321, 393, 764, 341, 6532, 13], "temperature": 0.0, "avg_logprob": -0.17451847753217142, "compression_ratio": 1.859922178988327, "no_speech_prob": 1.6186846551136114e-05}, {"id": 663, "seek": 286676, "start": 2866.76, "end": 2869.36, "text": " This yeah, yeah.", "tokens": [639, 1338, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.21401076414147202, "compression_ratio": 1.589430894308943, "no_speech_prob": 4.0928407543106005e-06}, {"id": 664, "seek": 286676, "start": 2869.36, "end": 2875.1200000000003, "text": " But something kind of to spoil the punch line is we're going to see that Gaussian elimination", "tokens": [583, 746, 733, 295, 281, 18630, 264, 8135, 1622, 307, 321, 434, 516, 281, 536, 300, 39148, 29224], "temperature": 0.0, "avg_logprob": -0.21401076414147202, "compression_ratio": 1.589430894308943, "no_speech_prob": 4.0928407543106005e-06}, {"id": 665, "seek": 286676, "start": 2875.1200000000003, "end": 2880.0, "text": " is very closely linked to LU factorization and they're actually kind of even talked about", "tokens": [307, 588, 8185, 9408, 281, 31851, 5952, 2144, 293, 436, 434, 767, 733, 295, 754, 2825, 466], "temperature": 0.0, "avg_logprob": -0.21401076414147202, "compression_ratio": 1.589430894308943, "no_speech_prob": 4.0928407543106005e-06}, {"id": 666, "seek": 286676, "start": 2880.0, "end": 2882.2000000000003, "text": " almost like interchangeably.", "tokens": [1920, 411, 30358, 1188, 13], "temperature": 0.0, "avg_logprob": -0.21401076414147202, "compression_ratio": 1.589430894308943, "no_speech_prob": 4.0928407543106005e-06}, {"id": 667, "seek": 286676, "start": 2882.2000000000003, "end": 2887.36, "text": " So this kind of process even though kind of in a linear algebra class you're usually not", "tokens": [407, 341, 733, 295, 1399, 754, 1673, 733, 295, 294, 257, 8213, 21989, 1508, 291, 434, 2673, 406], "temperature": 0.0, "avg_logprob": -0.21401076414147202, "compression_ratio": 1.589430894308943, "no_speech_prob": 4.0928407543106005e-06}, {"id": 668, "seek": 286676, "start": 2887.36, "end": 2889.36, "text": " seeing that it's giving you a decomposition.", "tokens": [2577, 300, 309, 311, 2902, 291, 257, 48356, 13], "temperature": 0.0, "avg_logprob": -0.21401076414147202, "compression_ratio": 1.589430894308943, "no_speech_prob": 4.0928407543106005e-06}, {"id": 669, "seek": 286676, "start": 2889.36, "end": 2891.36, "text": " It's the same method.", "tokens": [467, 311, 264, 912, 3170, 13], "temperature": 0.0, "avg_logprob": -0.21401076414147202, "compression_ratio": 1.589430894308943, "no_speech_prob": 4.0928407543106005e-06}, {"id": 670, "seek": 286676, "start": 2891.36, "end": 2892.36, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.21401076414147202, "compression_ratio": 1.589430894308943, "no_speech_prob": 4.0928407543106005e-06}, {"id": 671, "seek": 289236, "start": 2892.36, "end": 2901.04, "text": " So going back to the decomposition, the first column with zeros beneath the diagonal.", "tokens": [407, 516, 646, 281, 264, 48356, 11, 264, 700, 7738, 365, 35193, 17149, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.21210382695783647, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.078270361176692e-05}, {"id": 672, "seek": 289236, "start": 2901.04, "end": 2911.56, "text": " So we've gotten rid of this 3 by multiplying, oops, multiplying the top row by 3 and subtracting.", "tokens": [407, 321, 600, 5768, 3973, 295, 341, 805, 538, 30955, 11, 34166, 11, 30955, 264, 1192, 5386, 538, 805, 293, 16390, 278, 13], "temperature": 0.0, "avg_logprob": -0.21210382695783647, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.078270361176692e-05}, {"id": 673, "seek": 289236, "start": 2911.56, "end": 2916.2400000000002, "text": " How can we get rid of this negative 1?", "tokens": [1012, 393, 321, 483, 3973, 295, 341, 3671, 502, 30], "temperature": 0.0, "avg_logprob": -0.21210382695783647, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.078270361176692e-05}, {"id": 674, "seek": 291624, "start": 2916.24, "end": 2923.7599999999998, "text": " Can I say it louder?", "tokens": [1664, 286, 584, 309, 22717, 30], "temperature": 0.0, "avg_logprob": -0.22528221786663097, "compression_ratio": 1.5069767441860464, "no_speech_prob": 9.972022780857515e-06}, {"id": 675, "seek": 291624, "start": 2923.7599999999998, "end": 2925.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22528221786663097, "compression_ratio": 1.5069767441860464, "no_speech_prob": 9.972022780857515e-06}, {"id": 676, "seek": 291624, "start": 2925.2, "end": 2928.3999999999996, "text": " So add the two columns together, or two rows, sorry.", "tokens": [407, 909, 264, 732, 13766, 1214, 11, 420, 732, 13241, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.22528221786663097, "compression_ratio": 1.5069767441860464, "no_speech_prob": 9.972022780857515e-06}, {"id": 677, "seek": 291624, "start": 2928.3999999999996, "end": 2930.52, "text": " The first and third rows we add those together.", "tokens": [440, 700, 293, 2636, 13241, 321, 909, 729, 1214, 13], "temperature": 0.0, "avg_logprob": -0.22528221786663097, "compression_ratio": 1.5069767441860464, "no_speech_prob": 9.972022780857515e-06}, {"id": 678, "seek": 291624, "start": 2930.52, "end": 2932.3199999999997, "text": " This is a zero.", "tokens": [639, 307, 257, 4018, 13], "temperature": 0.0, "avg_logprob": -0.22528221786663097, "compression_ratio": 1.5069767441860464, "no_speech_prob": 9.972022780857515e-06}, {"id": 679, "seek": 291624, "start": 2932.3199999999997, "end": 2941.24, "text": " And I should note that adding is actually, yes, it's like multiplying by negative 1 and", "tokens": [400, 286, 820, 3637, 300, 5127, 307, 767, 11, 2086, 11, 309, 311, 411, 30955, 538, 3671, 502, 293], "temperature": 0.0, "avg_logprob": -0.22528221786663097, "compression_ratio": 1.5069767441860464, "no_speech_prob": 9.972022780857515e-06}, {"id": 680, "seek": 291624, "start": 2941.24, "end": 2945.8799999999997, "text": " subtracting, which seems like a more roundabout way to talk about it, but we're going to put", "tokens": [16390, 278, 11, 597, 2544, 411, 257, 544, 3098, 21970, 636, 281, 751, 466, 309, 11, 457, 321, 434, 516, 281, 829], "temperature": 0.0, "avg_logprob": -0.22528221786663097, "compression_ratio": 1.5069767441860464, "no_speech_prob": 9.972022780857515e-06}, {"id": 681, "seek": 294588, "start": 2945.88, "end": 2951.1600000000003, "text": " a negative 1 down here to keep track of how we got this entry to zero.", "tokens": [257, 3671, 502, 760, 510, 281, 1066, 2837, 295, 577, 321, 658, 341, 8729, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.15534507311307466, "compression_ratio": 1.5280898876404494, "no_speech_prob": 9.516158570477273e-06}, {"id": 682, "seek": 294588, "start": 2951.1600000000003, "end": 2957.2200000000003, "text": " So yeah, add those, we've got a zero there, which is nice.", "tokens": [407, 1338, 11, 909, 729, 11, 321, 600, 658, 257, 4018, 456, 11, 597, 307, 1481, 13], "temperature": 0.0, "avg_logprob": -0.15534507311307466, "compression_ratio": 1.5280898876404494, "no_speech_prob": 9.516158570477273e-06}, {"id": 683, "seek": 294588, "start": 2957.2200000000003, "end": 2961.0, "text": " This becomes 2 and this becomes 4.", "tokens": [639, 3643, 568, 293, 341, 3643, 1017, 13], "temperature": 0.0, "avg_logprob": -0.15534507311307466, "compression_ratio": 1.5280898876404494, "no_speech_prob": 9.516158570477273e-06}, {"id": 684, "seek": 294588, "start": 2961.0, "end": 2968.36, "text": " And then for the last one we can multiply by 3 or think of it as multiplying by negative", "tokens": [400, 550, 337, 264, 1036, 472, 321, 393, 12972, 538, 805, 420, 519, 295, 309, 382, 30955, 538, 3671], "temperature": 0.0, "avg_logprob": -0.15534507311307466, "compression_ratio": 1.5280898876404494, "no_speech_prob": 9.516158570477273e-06}, {"id": 685, "seek": 294588, "start": 2968.36, "end": 2970.2200000000003, "text": " 3 and subtracting.", "tokens": [805, 293, 16390, 278, 13], "temperature": 0.0, "avg_logprob": -0.15534507311307466, "compression_ratio": 1.5280898876404494, "no_speech_prob": 9.516158570477273e-06}, {"id": 686, "seek": 297022, "start": 2970.22, "end": 2982.04, "text": " So let's put a negative 3 here to keep track of, and then here this becomes zero, negative", "tokens": [407, 718, 311, 829, 257, 3671, 805, 510, 281, 1066, 2837, 295, 11, 293, 550, 510, 341, 3643, 4018, 11, 3671], "temperature": 0.0, "avg_logprob": -0.4578008015950521, "compression_ratio": 1.3454545454545455, "no_speech_prob": 3.041557420147001e-06}, {"id": 687, "seek": 297022, "start": 2982.04, "end": 2988.16, "text": " 3, negative 2, this will become zero.", "tokens": [805, 11, 3671, 568, 11, 341, 486, 1813, 4018, 13], "temperature": 0.0, "avg_logprob": -0.4578008015950521, "compression_ratio": 1.3454545454545455, "no_speech_prob": 3.041557420147001e-06}, {"id": 688, "seek": 297022, "start": 2988.16, "end": 2994.3599999999997, "text": " Or does that, yeah.", "tokens": [1610, 775, 300, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.4578008015950521, "compression_ratio": 1.3454545454545455, "no_speech_prob": 3.041557420147001e-06}, {"id": 689, "seek": 299436, "start": 2994.36, "end": 3002.1200000000003, "text": " Oh, negative 12, I reversed my signs.", "tokens": [876, 11, 3671, 2272, 11, 286, 30563, 452, 7880, 13], "temperature": 0.0, "avg_logprob": -0.5445208395681074, "compression_ratio": 1.0263157894736843, "no_speech_prob": 2.994388069055276e-06}, {"id": 690, "seek": 299436, "start": 3002.1200000000003, "end": 3004.2000000000003, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.5445208395681074, "compression_ratio": 1.0263157894736843, "no_speech_prob": 2.994388069055276e-06}, {"id": 691, "seek": 299436, "start": 3004.2000000000003, "end": 3009.44, "text": " And then this one is 20?", "tokens": [400, 550, 341, 472, 307, 945, 30], "temperature": 0.0, "avg_logprob": -0.5445208395681074, "compression_ratio": 1.0263157894736843, "no_speech_prob": 2.994388069055276e-06}, {"id": 692, "seek": 299436, "start": 3009.44, "end": 3012.08, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.5445208395681074, "compression_ratio": 1.0263157894736843, "no_speech_prob": 2.994388069055276e-06}, {"id": 693, "seek": 301208, "start": 3012.08, "end": 3027.64, "text": " Okay, so then the next step is we want to fill up, oh, did I write down the last, yeah.", "tokens": [1033, 11, 370, 550, 264, 958, 1823, 307, 321, 528, 281, 2836, 493, 11, 1954, 11, 630, 286, 2464, 760, 264, 1036, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.14963846337305356, "compression_ratio": 1.4047619047619047, "no_speech_prob": 1.7603255173526122e-06}, {"id": 694, "seek": 301208, "start": 3027.64, "end": 3031.16, "text": " Now we want to make everything below negative 3 have a zero.", "tokens": [823, 321, 528, 281, 652, 1203, 2507, 3671, 805, 362, 257, 4018, 13], "temperature": 0.0, "avg_logprob": -0.14963846337305356, "compression_ratio": 1.4047619047619047, "no_speech_prob": 1.7603255173526122e-06}, {"id": 695, "seek": 301208, "start": 3031.16, "end": 3039.56, "text": " And so it's nice the, row 3 has already been done for us.", "tokens": [400, 370, 309, 311, 1481, 264, 11, 5386, 805, 575, 1217, 668, 1096, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.14963846337305356, "compression_ratio": 1.4047619047619047, "no_speech_prob": 1.7603255173526122e-06}, {"id": 696, "seek": 301208, "start": 3039.56, "end": 3041.04, "text": " So I'll just put a zero here.", "tokens": [407, 286, 603, 445, 829, 257, 4018, 510, 13], "temperature": 0.0, "avg_logprob": -0.14963846337305356, "compression_ratio": 1.4047619047619047, "no_speech_prob": 1.7603255173526122e-06}, {"id": 697, "seek": 304104, "start": 3041.04, "end": 3045.12, "text": " We didn't actually have to do anything.", "tokens": [492, 994, 380, 767, 362, 281, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.20244060980307088, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.2191754144150764e-05}, {"id": 698, "seek": 304104, "start": 3045.12, "end": 3051.84, "text": " And again, the top row is not going to be changing.", "tokens": [400, 797, 11, 264, 1192, 5386, 307, 406, 516, 281, 312, 4473, 13], "temperature": 0.0, "avg_logprob": -0.20244060980307088, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.2191754144150764e-05}, {"id": 699, "seek": 304104, "start": 3051.84, "end": 3055.6, "text": " The second row is not going to be changing.", "tokens": [440, 1150, 5386, 307, 406, 516, 281, 312, 4473, 13], "temperature": 0.0, "avg_logprob": -0.20244060980307088, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.2191754144150764e-05}, {"id": 700, "seek": 304104, "start": 3055.6, "end": 3061.92, "text": " Actually, neither is the third row, so we're really just working on the last row at this", "tokens": [5135, 11, 9662, 307, 264, 2636, 5386, 11, 370, 321, 434, 534, 445, 1364, 322, 264, 1036, 5386, 412, 341], "temperature": 0.0, "avg_logprob": -0.20244060980307088, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.2191754144150764e-05}, {"id": 701, "seek": 304104, "start": 3061.92, "end": 3062.92, "text": " point.", "tokens": [935, 13], "temperature": 0.0, "avg_logprob": -0.20244060980307088, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.2191754144150764e-05}, {"id": 702, "seek": 304104, "start": 3062.92, "end": 3069.0, "text": " It's a negative 3.", "tokens": [467, 311, 257, 3671, 805, 13], "temperature": 0.0, "avg_logprob": -0.20244060980307088, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.2191754144150764e-05}, {"id": 703, "seek": 306900, "start": 3069.0, "end": 3074.56, "text": " And note that the reason, okay, so now that we're trying to get rid of this negative 12,", "tokens": [400, 3637, 300, 264, 1778, 11, 1392, 11, 370, 586, 300, 321, 434, 1382, 281, 483, 3973, 295, 341, 3671, 2272, 11], "temperature": 0.0, "avg_logprob": -0.12528488928811593, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.0451259186083917e-05}, {"id": 704, "seek": 306900, "start": 3074.56, "end": 3079.16, "text": " we're going to use the second row because we don't want to flip this zero back, the", "tokens": [321, 434, 516, 281, 764, 264, 1150, 5386, 570, 321, 500, 380, 528, 281, 7929, 341, 4018, 646, 11, 264], "temperature": 0.0, "avg_logprob": -0.12528488928811593, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.0451259186083917e-05}, {"id": 705, "seek": 306900, "start": 3079.16, "end": 3084.04, "text": " one in the corner, flip it back into being something that's non-zero.", "tokens": [472, 294, 264, 4538, 11, 7929, 309, 646, 666, 885, 746, 300, 311, 2107, 12, 32226, 13], "temperature": 0.0, "avg_logprob": -0.12528488928811593, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.0451259186083917e-05}, {"id": 706, "seek": 306900, "start": 3084.04, "end": 3085.8, "text": " So we use the negative 3.", "tokens": [407, 321, 764, 264, 3671, 805, 13], "temperature": 0.0, "avg_logprob": -0.12528488928811593, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.0451259186083917e-05}, {"id": 707, "seek": 306900, "start": 3085.8, "end": 3090.14, "text": " So multiply that row by 4 and subtract.", "tokens": [407, 12972, 300, 5386, 538, 1017, 293, 16390, 13], "temperature": 0.0, "avg_logprob": -0.12528488928811593, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.0451259186083917e-05}, {"id": 708, "seek": 306900, "start": 3090.14, "end": 3091.76, "text": " So let's keep track of that 4.", "tokens": [407, 718, 311, 1066, 2837, 295, 300, 1017, 13], "temperature": 0.0, "avg_logprob": -0.12528488928811593, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.0451259186083917e-05}, {"id": 709, "seek": 306900, "start": 3091.76, "end": 3092.76, "text": " We'll write it down here.", "tokens": [492, 603, 2464, 309, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.12528488928811593, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.0451259186083917e-05}, {"id": 710, "seek": 306900, "start": 3092.76, "end": 3098.32, "text": " We used a 4.", "tokens": [492, 1143, 257, 1017, 13], "temperature": 0.0, "avg_logprob": -0.12528488928811593, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.0451259186083917e-05}, {"id": 711, "seek": 309832, "start": 3098.32, "end": 3100.36, "text": " That becomes 0.", "tokens": [663, 3643, 1958, 13], "temperature": 0.0, "avg_logprob": -0.21451708636706388, "compression_ratio": 1.5027624309392265, "no_speech_prob": 9.818235412240028e-06}, {"id": 712, "seek": 309832, "start": 3100.36, "end": 3104.76, "text": " This becomes, I guess, negative 4.", "tokens": [639, 3643, 11, 286, 2041, 11, 3671, 1017, 13], "temperature": 0.0, "avg_logprob": -0.21451708636706388, "compression_ratio": 1.5027624309392265, "no_speech_prob": 9.818235412240028e-06}, {"id": 713, "seek": 309832, "start": 3104.76, "end": 3110.0800000000004, "text": " And this one becomes, oh, stays negative 7.", "tokens": [400, 341, 472, 3643, 11, 1954, 11, 10834, 3671, 1614, 13], "temperature": 0.0, "avg_logprob": -0.21451708636706388, "compression_ratio": 1.5027624309392265, "no_speech_prob": 9.818235412240028e-06}, {"id": 714, "seek": 309832, "start": 3110.0800000000004, "end": 3116.7200000000003, "text": " This is totally reminding me of why I hate a Gaussian elimination.", "tokens": [639, 307, 3879, 27639, 385, 295, 983, 286, 4700, 257, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.21451708636706388, "compression_ratio": 1.5027624309392265, "no_speech_prob": 9.818235412240028e-06}, {"id": 715, "seek": 309832, "start": 3116.7200000000003, "end": 3120.0800000000004, "text": " Let's never do this again.", "tokens": [961, 311, 1128, 360, 341, 797, 13], "temperature": 0.0, "avg_logprob": -0.21451708636706388, "compression_ratio": 1.5027624309392265, "no_speech_prob": 9.818235412240028e-06}, {"id": 716, "seek": 309832, "start": 3120.0800000000004, "end": 3125.6200000000003, "text": " The good news is once we get familiar with it, then we can get a computer to do it.", "tokens": [440, 665, 2583, 307, 1564, 321, 483, 4963, 365, 309, 11, 550, 321, 393, 483, 257, 3820, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.21451708636706388, "compression_ratio": 1.5027624309392265, "no_speech_prob": 9.818235412240028e-06}, {"id": 717, "seek": 312562, "start": 3125.62, "end": 3130.48, "text": " So that's the benefit of learning the LU decomposition is we're going to get a computer to do this", "tokens": [407, 300, 311, 264, 5121, 295, 2539, 264, 31851, 48356, 307, 321, 434, 516, 281, 483, 257, 3820, 281, 360, 341], "temperature": 0.0, "avg_logprob": -0.19359492048432556, "compression_ratio": 1.4874371859296482, "no_speech_prob": 5.173841145733604e-06}, {"id": 718, "seek": 312562, "start": 3130.48, "end": 3132.2799999999997, "text": " for us in the future.", "tokens": [337, 505, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.19359492048432556, "compression_ratio": 1.4874371859296482, "no_speech_prob": 5.173841145733604e-06}, {"id": 719, "seek": 312562, "start": 3132.2799999999997, "end": 3134.56, "text": " But I did want you to remember what the process was.", "tokens": [583, 286, 630, 528, 291, 281, 1604, 437, 264, 1399, 390, 13], "temperature": 0.0, "avg_logprob": -0.19359492048432556, "compression_ratio": 1.4874371859296482, "no_speech_prob": 5.173841145733604e-06}, {"id": 720, "seek": 312562, "start": 3134.56, "end": 3139.24, "text": " Half the final assessment will be doing this by hand.", "tokens": [15917, 264, 2572, 9687, 486, 312, 884, 341, 538, 1011, 13], "temperature": 0.0, "avg_logprob": -0.19359492048432556, "compression_ratio": 1.4874371859296482, "no_speech_prob": 5.173841145733604e-06}, {"id": 721, "seek": 312562, "start": 3139.24, "end": 3154.12, "text": " Okay, so now we're getting rid of this negative 4 that's right here.", "tokens": [1033, 11, 370, 586, 321, 434, 1242, 3973, 295, 341, 3671, 1017, 300, 311, 558, 510, 13], "temperature": 0.0, "avg_logprob": -0.19359492048432556, "compression_ratio": 1.4874371859296482, "no_speech_prob": 5.173841145733604e-06}, {"id": 722, "seek": 315412, "start": 3154.12, "end": 3158.74, "text": " So the good news is we've got the top three rows are in good shape in terms of moving", "tokens": [407, 264, 665, 2583, 307, 321, 600, 658, 264, 1192, 1045, 13241, 366, 294, 665, 3909, 294, 2115, 295, 2684], "temperature": 0.0, "avg_logprob": -0.10359703622213225, "compression_ratio": 1.5024875621890548, "no_speech_prob": 2.22525636672799e-06}, {"id": 723, "seek": 315412, "start": 3158.74, "end": 3170.12, "text": " to our upper triangular matrix that we want in Gaussian elimination.", "tokens": [281, 527, 6597, 38190, 8141, 300, 321, 528, 294, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.10359703622213225, "compression_ratio": 1.5024875621890548, "no_speech_prob": 2.22525636672799e-06}, {"id": 724, "seek": 315412, "start": 3170.12, "end": 3177.3199999999997, "text": " So really, we just need to multiply the third row by 2, or I guess negative 2, and subtract.", "tokens": [407, 534, 11, 321, 445, 643, 281, 12972, 264, 2636, 5386, 538, 568, 11, 420, 286, 2041, 3671, 568, 11, 293, 16390, 13], "temperature": 0.0, "avg_logprob": -0.10359703622213225, "compression_ratio": 1.5024875621890548, "no_speech_prob": 2.22525636672799e-06}, {"id": 725, "seek": 315412, "start": 3177.3199999999997, "end": 3180.2, "text": " So let's put a negative 2 in here.", "tokens": [407, 718, 311, 829, 257, 3671, 568, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.10359703622213225, "compression_ratio": 1.5024875621890548, "no_speech_prob": 2.22525636672799e-06}, {"id": 726, "seek": 315412, "start": 3180.2, "end": 3182.72, "text": " Keep track of that.", "tokens": [5527, 2837, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.10359703622213225, "compression_ratio": 1.5024875621890548, "no_speech_prob": 2.22525636672799e-06}, {"id": 727, "seek": 318272, "start": 3182.72, "end": 3186.3999999999996, "text": " And then, yeah, this becomes 0.", "tokens": [400, 550, 11, 1338, 11, 341, 3643, 1958, 13], "temperature": 0.0, "avg_logprob": -0.142717034849402, "compression_ratio": 1.4324324324324325, "no_speech_prob": 1.0676965302991448e-06}, {"id": 728, "seek": 318272, "start": 3186.3999999999996, "end": 3192.04, "text": " I've got negative 7 plus 8 is 1.", "tokens": [286, 600, 658, 3671, 1614, 1804, 1649, 307, 502, 13], "temperature": 0.0, "avg_logprob": -0.142717034849402, "compression_ratio": 1.4324324324324325, "no_speech_prob": 1.0676965302991448e-06}, {"id": 729, "seek": 318272, "start": 3192.04, "end": 3198.08, "text": " And so we're done with our Gaussian elimination.", "tokens": [400, 370, 321, 434, 1096, 365, 527, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.142717034849402, "compression_ratio": 1.4324324324324325, "no_speech_prob": 1.0676965302991448e-06}, {"id": 730, "seek": 318272, "start": 3198.08, "end": 3202.14, "text": " Sometimes at this point, actually, I think this is where you would stop.", "tokens": [4803, 412, 341, 935, 11, 767, 11, 286, 519, 341, 307, 689, 291, 576, 1590, 13], "temperature": 0.0, "avg_logprob": -0.142717034849402, "compression_ratio": 1.4324324324324325, "no_speech_prob": 1.0676965302991448e-06}, {"id": 731, "seek": 318272, "start": 3202.14, "end": 3212.48, "text": " And then amazingly, the matrix here is actually what I had written right here.", "tokens": [400, 550, 31762, 11, 264, 8141, 510, 307, 767, 437, 286, 632, 3720, 558, 510, 13], "temperature": 0.0, "avg_logprob": -0.142717034849402, "compression_ratio": 1.4324324324324325, "no_speech_prob": 1.0676965302991448e-06}, {"id": 732, "seek": 321248, "start": 3212.48, "end": 3218.16, "text": " And so it's going to turn out that a is equal to L times u.", "tokens": [400, 370, 309, 311, 516, 281, 1261, 484, 300, 257, 307, 2681, 281, 441, 1413, 344, 13], "temperature": 0.0, "avg_logprob": -0.17935972435529843, "compression_ratio": 1.55, "no_speech_prob": 4.637804522644728e-06}, {"id": 733, "seek": 321248, "start": 3218.16, "end": 3224.52, "text": " So we are keeping track of what we are having to multiply each row by to add and cancel", "tokens": [407, 321, 366, 5145, 2837, 295, 437, 321, 366, 1419, 281, 12972, 1184, 5386, 538, 281, 909, 293, 10373], "temperature": 0.0, "avg_logprob": -0.17935972435529843, "compression_ratio": 1.55, "no_speech_prob": 4.637804522644728e-06}, {"id": 734, "seek": 321248, "start": 3224.52, "end": 3225.52, "text": " out the other row.", "tokens": [484, 264, 661, 5386, 13], "temperature": 0.0, "avg_logprob": -0.17935972435529843, "compression_ratio": 1.55, "no_speech_prob": 4.637804522644728e-06}, {"id": 735, "seek": 321248, "start": 3225.52, "end": 3234.72, "text": " We're just going to fill in 1s along the diagonal, and then zeros everywhere else.", "tokens": [492, 434, 445, 516, 281, 2836, 294, 502, 82, 2051, 264, 21539, 11, 293, 550, 35193, 5315, 1646, 13], "temperature": 0.0, "avg_logprob": -0.17935972435529843, "compression_ratio": 1.55, "no_speech_prob": 4.637804522644728e-06}, {"id": 736, "seek": 321248, "start": 3234.72, "end": 3240.6, "text": " But yeah, this is L. So keeping track of those coefficients.", "tokens": [583, 1338, 11, 341, 307, 441, 13, 407, 5145, 2837, 295, 729, 31994, 13], "temperature": 0.0, "avg_logprob": -0.17935972435529843, "compression_ratio": 1.55, "no_speech_prob": 4.637804522644728e-06}, {"id": 737, "seek": 324060, "start": 3240.6, "end": 3242.68, "text": " And what we got from our Gaussian elimination was u.", "tokens": [400, 437, 321, 658, 490, 527, 39148, 29224, 390, 344, 13], "temperature": 0.0, "avg_logprob": -0.26335676392512536, "compression_ratio": 1.5197740112994351, "no_speech_prob": 3.905397989001358e-06}, {"id": 738, "seek": 324060, "start": 3242.68, "end": 3250.8399999999997, "text": " First, let me ask, are there any questions just about the process of Gaussian elimination?", "tokens": [2386, 11, 718, 385, 1029, 11, 366, 456, 604, 1651, 445, 466, 264, 1399, 295, 39148, 29224, 30], "temperature": 0.0, "avg_logprob": -0.26335676392512536, "compression_ratio": 1.5197740112994351, "no_speech_prob": 3.905397989001358e-06}, {"id": 739, "seek": 324060, "start": 3250.8399999999997, "end": 3257.7999999999997, "text": " And I checked, and Khan Academy has several videos on this if you did want to watch more", "tokens": [400, 286, 10033, 11, 293, 18136, 11735, 575, 2940, 2145, 322, 341, 498, 291, 630, 528, 281, 1159, 544], "temperature": 0.0, "avg_logprob": -0.26335676392512536, "compression_ratio": 1.5197740112994351, "no_speech_prob": 3.905397989001358e-06}, {"id": 740, "seek": 324060, "start": 3257.7999999999997, "end": 3259.7999999999997, "text": " Gaussian elimination, Jeremy.", "tokens": [39148, 29224, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.26335676392512536, "compression_ratio": 1.5197740112994351, "no_speech_prob": 3.905397989001358e-06}, {"id": 741, "seek": 324060, "start": 3259.7999999999997, "end": 3260.7999999999997, "text": " Linda?", "tokens": [20324, 30], "temperature": 0.0, "avg_logprob": -0.26335676392512536, "compression_ratio": 1.5197740112994351, "no_speech_prob": 3.905397989001358e-06}, {"id": 742, "seek": 326080, "start": 3260.8, "end": 3271.44, "text": " So the Gaussian elimination is the process to calculate the L and the u?", "tokens": [407, 264, 39148, 29224, 307, 264, 1399, 281, 8873, 264, 441, 293, 264, 344, 30], "temperature": 0.0, "avg_logprob": -0.3430891990661621, "compression_ratio": 1.435897435897436, "no_speech_prob": 8.059269589466567e-07}, {"id": 743, "seek": 326080, "start": 3271.44, "end": 3272.44, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.3430891990661621, "compression_ratio": 1.435897435897436, "no_speech_prob": 8.059269589466567e-07}, {"id": 744, "seek": 326080, "start": 3272.44, "end": 3273.44, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3430891990661621, "compression_ratio": 1.435897435897436, "no_speech_prob": 8.059269589466567e-07}, {"id": 745, "seek": 326080, "start": 3273.44, "end": 3279.04, "text": " And sometimes Trevathan, I think, almost uses LU decomposition and Gaussian elimination interchangeably.", "tokens": [400, 2171, 8648, 85, 9390, 11, 286, 519, 11, 1920, 4960, 31851, 48356, 293, 39148, 29224, 30358, 1188, 13], "temperature": 0.0, "avg_logprob": -0.3430891990661621, "compression_ratio": 1.435897435897436, "no_speech_prob": 8.059269589466567e-07}, {"id": 746, "seek": 326080, "start": 3279.04, "end": 3280.04, "text": " Matthew?", "tokens": [12434, 30], "temperature": 0.0, "avg_logprob": -0.3430891990661621, "compression_ratio": 1.435897435897436, "no_speech_prob": 8.059269589466567e-07}, {"id": 747, "seek": 326080, "start": 3280.04, "end": 3288.5600000000004, "text": " Why is it called Gaussian?", "tokens": [1545, 307, 309, 1219, 39148, 30], "temperature": 0.0, "avg_logprob": -0.3430891990661621, "compression_ratio": 1.435897435897436, "no_speech_prob": 8.059269589466567e-07}, {"id": 748, "seek": 328856, "start": 3288.56, "end": 3291.36, "text": " Gauss discovered it.", "tokens": [10384, 2023, 6941, 309, 13], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 749, "seek": 328856, "start": 3291.36, "end": 3294.12, "text": " I feel like other people might have simultaneously discovered it.", "tokens": [286, 841, 411, 661, 561, 1062, 362, 16561, 6941, 309, 13], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 750, "seek": 328856, "start": 3294.12, "end": 3295.12, "text": " I'm not sure.", "tokens": [286, 478, 406, 988, 13], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 751, "seek": 328856, "start": 3295.12, "end": 3297.12, "text": " I'll look up the history on this.", "tokens": [286, 603, 574, 493, 264, 2503, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 752, "seek": 328856, "start": 3297.12, "end": 3299.7999999999997, "text": " So linear algebra is that old?", "tokens": [407, 8213, 21989, 307, 300, 1331, 30], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 753, "seek": 328856, "start": 3299.7999999999997, "end": 3300.7999999999997, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 754, "seek": 328856, "start": 3300.7999999999997, "end": 3301.7999999999997, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 755, "seek": 328856, "start": 3301.7999999999997, "end": 3302.7999999999997, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 756, "seek": 328856, "start": 3302.7999999999997, "end": 3306.52, "text": " And I think this question of solving linear systems of equations is one that shows up", "tokens": [400, 286, 519, 341, 1168, 295, 12606, 8213, 3652, 295, 11787, 307, 472, 300, 3110, 493], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 757, "seek": 328856, "start": 3306.52, "end": 3309.52, "text": " a lot and is useful.", "tokens": [257, 688, 293, 307, 4420, 13], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 758, "seek": 328856, "start": 3309.52, "end": 3316.72, "text": " Yeah, that's a good question.", "tokens": [865, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.270928491022169, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.723098466754891e-05}, {"id": 759, "seek": 331672, "start": 3316.72, "end": 3319.64, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.3010626983642578, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.882649980369024e-05}, {"id": 760, "seek": 331672, "start": 3319.64, "end": 3320.64, "text": " Vincent?", "tokens": [28003, 30], "temperature": 0.0, "avg_logprob": -0.3010626983642578, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.882649980369024e-05}, {"id": 761, "seek": 331672, "start": 3320.64, "end": 3321.64, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3010626983642578, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.882649980369024e-05}, {"id": 762, "seek": 331672, "start": 3321.64, "end": 3333.72, "text": " When we're calculating the values in L, I can get behind the lower diagonal values.", "tokens": [1133, 321, 434, 28258, 264, 4190, 294, 441, 11, 286, 393, 483, 2261, 264, 3126, 21539, 4190, 13], "temperature": 0.0, "avg_logprob": -0.3010626983642578, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.882649980369024e-05}, {"id": 763, "seek": 331672, "start": 3333.72, "end": 3340.08, "text": " But then the diagonal being 1, it's like we did 1 of row 1 to row 1, and we did 1 of row", "tokens": [583, 550, 264, 21539, 885, 502, 11, 309, 311, 411, 321, 630, 502, 295, 5386, 502, 281, 5386, 502, 11, 293, 321, 630, 502, 295, 5386], "temperature": 0.0, "avg_logprob": -0.3010626983642578, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.882649980369024e-05}, {"id": 764, "seek": 331672, "start": 3340.08, "end": 3341.08, "text": " 2 to row 2.", "tokens": [568, 281, 5386, 568, 13], "temperature": 0.0, "avg_logprob": -0.3010626983642578, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.882649980369024e-05}, {"id": 765, "seek": 331672, "start": 3341.08, "end": 3342.08, "text": " And that doesn't fit with the numbers.", "tokens": [400, 300, 1177, 380, 3318, 365, 264, 3547, 13], "temperature": 0.0, "avg_logprob": -0.3010626983642578, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.882649980369024e-05}, {"id": 766, "seek": 331672, "start": 3342.08, "end": 3343.08, "text": " Yeah, with the-", "tokens": [865, 11, 365, 264, 12], "temperature": 0.0, "avg_logprob": -0.3010626983642578, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.882649980369024e-05}, {"id": 767, "seek": 331672, "start": 3343.08, "end": 3344.08, "text": " I'm curious about the intuition behind that.", "tokens": [286, 478, 6369, 466, 264, 24002, 2261, 300, 13], "temperature": 0.0, "avg_logprob": -0.3010626983642578, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.882649980369024e-05}, {"id": 768, "seek": 334408, "start": 3344.08, "end": 3353.7599999999998, "text": " Let me think about that one.", "tokens": [961, 385, 519, 466, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.5888314474196661, "compression_ratio": 1.118279569892473, "no_speech_prob": 2.111197136400733e-05}, {"id": 769, "seek": 334408, "start": 3353.7599999999998, "end": 3356.96, "text": " Yeah, that's a good question.", "tokens": [865, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.5888314474196661, "compression_ratio": 1.118279569892473, "no_speech_prob": 2.111197136400733e-05}, {"id": 770, "seek": 334408, "start": 3356.96, "end": 3357.96, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.5888314474196661, "compression_ratio": 1.118279569892473, "no_speech_prob": 2.111197136400733e-05}, {"id": 771, "seek": 334408, "start": 3357.96, "end": 3358.96, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.5888314474196661, "compression_ratio": 1.118279569892473, "no_speech_prob": 2.111197136400733e-05}, {"id": 772, "seek": 334408, "start": 3358.96, "end": 3363.56, "text": " Now let me check the time.", "tokens": [823, 718, 385, 1520, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.5888314474196661, "compression_ratio": 1.118279569892473, "no_speech_prob": 2.111197136400733e-05}, {"id": 773, "seek": 334408, "start": 3363.56, "end": 3364.56, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5888314474196661, "compression_ratio": 1.118279569892473, "no_speech_prob": 2.111197136400733e-05}, {"id": 774, "seek": 336456, "start": 3364.56, "end": 3376.36, "text": " So this would be a good time to stop for a break.", "tokens": [407, 341, 576, 312, 257, 665, 565, 281, 1590, 337, 257, 1821, 13], "temperature": 0.0, "avg_logprob": -0.25018109360786334, "compression_ratio": 1.3584905660377358, "no_speech_prob": 1.9637789137050277e-06}, {"id": 775, "seek": 336456, "start": 3376.36, "end": 3385.4, "text": " So let's meet back in seven minutes at- yeah, at 12.08 or 12.09.", "tokens": [407, 718, 311, 1677, 646, 294, 3407, 2077, 412, 12, 1338, 11, 412, 2272, 13, 16133, 420, 2272, 13, 13811, 13], "temperature": 0.0, "avg_logprob": -0.25018109360786334, "compression_ratio": 1.3584905660377358, "no_speech_prob": 1.9637789137050277e-06}, {"id": 776, "seek": 336456, "start": 3385.4, "end": 3391.2, "text": " And if you're bored, this is a good time to take the mid-course survey.", "tokens": [400, 498, 291, 434, 13521, 11, 341, 307, 257, 665, 565, 281, 747, 264, 2062, 12, 31913, 8984, 13], "temperature": 0.0, "avg_logprob": -0.25018109360786334, "compression_ratio": 1.3584905660377358, "no_speech_prob": 1.9637789137050277e-06}, {"id": 777, "seek": 336456, "start": 3391.2, "end": 3392.2, "text": " The link is in Slack.", "tokens": [440, 2113, 307, 294, 37211, 13], "temperature": 0.0, "avg_logprob": -0.25018109360786334, "compression_ratio": 1.3584905660377358, "no_speech_prob": 1.9637789137050277e-06}, {"id": 778, "seek": 336456, "start": 3392.2, "end": 3393.2, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.25018109360786334, "compression_ratio": 1.3584905660377358, "no_speech_prob": 1.9637789137050277e-06}, {"id": 779, "seek": 339320, "start": 3393.2, "end": 3402.0, "text": " All right, let's get started again.", "tokens": [1057, 558, 11, 718, 311, 483, 1409, 797, 13], "temperature": 0.0, "avg_logprob": -0.3505649199852577, "compression_ratio": 1.3333333333333333, "no_speech_prob": 5.0145163186243735e-06}, {"id": 780, "seek": 339320, "start": 3402.0, "end": 3404.0, "text": " Let's get started again.", "tokens": [961, 311, 483, 1409, 797, 13], "temperature": 0.0, "avg_logprob": -0.3505649199852577, "compression_ratio": 1.3333333333333333, "no_speech_prob": 5.0145163186243735e-06}, {"id": 781, "seek": 339320, "start": 3404.0, "end": 3410.2, "text": " Yeah, so when- let me just make sure.", "tokens": [865, 11, 370, 562, 12, 718, 385, 445, 652, 988, 13], "temperature": 0.0, "avg_logprob": -0.3505649199852577, "compression_ratio": 1.3333333333333333, "no_speech_prob": 5.0145163186243735e-06}, {"id": 782, "seek": 339320, "start": 3410.2, "end": 3411.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3505649199852577, "compression_ratio": 1.3333333333333333, "no_speech_prob": 5.0145163186243735e-06}, {"id": 783, "seek": 339320, "start": 3411.2, "end": 3414.9199999999996, "text": " Okay, so we left off by doing the LU decomposition.", "tokens": [1033, 11, 370, 321, 1411, 766, 538, 884, 264, 31851, 48356, 13], "temperature": 0.0, "avg_logprob": -0.3505649199852577, "compression_ratio": 1.3333333333333333, "no_speech_prob": 5.0145163186243735e-06}, {"id": 784, "seek": 341492, "start": 3414.92, "end": 3423.44, "text": " We had this matrix A, and we got L and U here.", "tokens": [492, 632, 341, 8141, 316, 11, 293, 321, 658, 441, 293, 624, 510, 13], "temperature": 0.0, "avg_logprob": -0.19564601353236608, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.637804977392079e-06}, {"id": 785, "seek": 341492, "start": 3423.44, "end": 3426.56, "text": " I'm going to say- and Treffith and goes into more detail about this, but you can think", "tokens": [286, 478, 516, 281, 584, 12, 293, 8648, 602, 355, 293, 1709, 666, 544, 2607, 466, 341, 11, 457, 291, 393, 519], "temperature": 0.0, "avg_logprob": -0.19564601353236608, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.637804977392079e-06}, {"id": 786, "seek": 341492, "start": 3426.56, "end": 3433.26, "text": " about Gaussian elimination as kind of being this series of multiplying by matrices where", "tokens": [466, 39148, 29224, 382, 733, 295, 885, 341, 2638, 295, 30955, 538, 32284, 689], "temperature": 0.0, "avg_logprob": -0.19564601353236608, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.637804977392079e-06}, {"id": 787, "seek": 341492, "start": 3433.26, "end": 3438.12, "text": " each matrix is kind of just doing like one of the operations and kind of putting those", "tokens": [1184, 8141, 307, 733, 295, 445, 884, 411, 472, 295, 264, 7705, 293, 733, 295, 3372, 729], "temperature": 0.0, "avg_logprob": -0.19564601353236608, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.637804977392079e-06}, {"id": 788, "seek": 343812, "start": 3438.12, "end": 3445.88, "text": " together to get your final decomposition.", "tokens": [1214, 281, 483, 428, 2572, 48356, 13], "temperature": 0.0, "avg_logprob": -0.22957093125089592, "compression_ratio": 1.5983935742971886, "no_speech_prob": 5.173842055228306e-06}, {"id": 789, "seek": 343812, "start": 3445.88, "end": 3450.7599999999998, "text": " But as Jeremy pointed out, Gaussian elimination is a bit tedious to do by hand, so we want", "tokens": [583, 382, 17809, 10932, 484, 11, 39148, 29224, 307, 257, 857, 38284, 281, 360, 538, 1011, 11, 370, 321, 528], "temperature": 0.0, "avg_logprob": -0.22957093125089592, "compression_ratio": 1.5983935742971886, "no_speech_prob": 5.173842055228306e-06}, {"id": 790, "seek": 343812, "start": 3450.7599999999998, "end": 3454.8399999999997, "text": " to automate that and have a computer do that for us.", "tokens": [281, 31605, 300, 293, 362, 257, 3820, 360, 300, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.22957093125089592, "compression_ratio": 1.5983935742971886, "no_speech_prob": 5.173842055228306e-06}, {"id": 791, "seek": 343812, "start": 3454.8399999999997, "end": 3457.08, "text": " So this is the basic LU decomposition.", "tokens": [407, 341, 307, 264, 3875, 31851, 48356, 13], "temperature": 0.0, "avg_logprob": -0.22957093125089592, "compression_ratio": 1.5983935742971886, "no_speech_prob": 5.173842055228306e-06}, {"id": 792, "seek": 343812, "start": 3457.08, "end": 3459.3599999999997, "text": " Sorry, what's the thing above it?", "tokens": [4919, 11, 437, 311, 264, 551, 3673, 309, 30], "temperature": 0.0, "avg_logprob": -0.22957093125089592, "compression_ratio": 1.5983935742971886, "no_speech_prob": 5.173842055228306e-06}, {"id": 793, "seek": 343812, "start": 3459.3599999999997, "end": 3461.08, "text": " You've got multiple L's there?", "tokens": [509, 600, 658, 3866, 441, 311, 456, 30], "temperature": 0.0, "avg_logprob": -0.22957093125089592, "compression_ratio": 1.5983935742971886, "no_speech_prob": 5.173842055228306e-06}, {"id": 794, "seek": 343812, "start": 3461.08, "end": 3462.08, "text": " Is that-", "tokens": [1119, 300, 12], "temperature": 0.0, "avg_logprob": -0.22957093125089592, "compression_ratio": 1.5983935742971886, "no_speech_prob": 5.173842055228306e-06}, {"id": 795, "seek": 343812, "start": 3462.08, "end": 3466.2799999999997, "text": " Yeah, so that's where you think about each operation you do as a single matrix.", "tokens": [865, 11, 370, 300, 311, 689, 291, 519, 466, 1184, 6916, 291, 360, 382, 257, 2167, 8141, 13], "temperature": 0.0, "avg_logprob": -0.22957093125089592, "compression_ratio": 1.5983935742971886, "no_speech_prob": 5.173842055228306e-06}, {"id": 796, "seek": 343812, "start": 3466.2799999999997, "end": 3467.2799999999997, "text": " Oh, multiplication.", "tokens": [876, 11, 27290, 13], "temperature": 0.0, "avg_logprob": -0.22957093125089592, "compression_ratio": 1.5983935742971886, "no_speech_prob": 5.173842055228306e-06}, {"id": 797, "seek": 346728, "start": 3467.28, "end": 3475.8, "text": " Yeah, yeah, and so here like the product of all of those would end up giving you, I don't", "tokens": [865, 11, 1338, 11, 293, 370, 510, 411, 264, 1674, 295, 439, 295, 729, 576, 917, 493, 2902, 291, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.21281306974349484, "compression_ratio": 1.652027027027027, "no_speech_prob": 5.955090273346286e-06}, {"id": 798, "seek": 346728, "start": 3475.8, "end": 3478.84, "text": " know, maybe in this case like the inverse of L or something.", "tokens": [458, 11, 1310, 294, 341, 1389, 411, 264, 17340, 295, 441, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.21281306974349484, "compression_ratio": 1.652027027027027, "no_speech_prob": 5.955090273346286e-06}, {"id": 799, "seek": 346728, "start": 3478.84, "end": 3483.96, "text": " And if you want to talk about this later, feel free to leave it for now, but why is", "tokens": [400, 498, 291, 528, 281, 751, 466, 341, 1780, 11, 841, 1737, 281, 1856, 309, 337, 586, 11, 457, 983, 307], "temperature": 0.0, "avg_logprob": -0.21281306974349484, "compression_ratio": 1.652027027027027, "no_speech_prob": 5.955090273346286e-06}, {"id": 800, "seek": 346728, "start": 3483.96, "end": 3488.0400000000004, "text": " this decomposition interesting other than the fact that it appears in something we've", "tokens": [341, 48356, 1880, 661, 813, 264, 1186, 300, 309, 7038, 294, 746, 321, 600], "temperature": 0.0, "avg_logprob": -0.21281306974349484, "compression_ratio": 1.652027027027027, "no_speech_prob": 5.955090273346286e-06}, {"id": 801, "seek": 346728, "start": 3488.0400000000004, "end": 3491.88, "text": " seen already, or is it basically just useful as a component of other algorithms?", "tokens": [1612, 1217, 11, 420, 307, 309, 1936, 445, 4420, 382, 257, 6542, 295, 661, 14642, 30], "temperature": 0.0, "avg_logprob": -0.21281306974349484, "compression_ratio": 1.652027027027027, "no_speech_prob": 5.955090273346286e-06}, {"id": 802, "seek": 346728, "start": 3491.88, "end": 3496.2000000000003, "text": " So I mean it's useful on its own for kind of the use case I mentioned before, if you're", "tokens": [407, 286, 914, 309, 311, 4420, 322, 1080, 1065, 337, 733, 295, 264, 764, 1389, 286, 2835, 949, 11, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.21281306974349484, "compression_ratio": 1.652027027027027, "no_speech_prob": 5.955090273346286e-06}, {"id": 803, "seek": 349620, "start": 3496.2, "end": 3501.3999999999996, "text": " solving a linear system of equations multiple- well, there are other ways to do that as well,", "tokens": [12606, 257, 8213, 1185, 295, 11787, 3866, 12, 731, 11, 456, 366, 661, 2098, 281, 360, 300, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.18968086657316788, "compression_ratio": 1.6901408450704225, "no_speech_prob": 2.482434183548321e-06}, {"id": 804, "seek": 349620, "start": 3501.3999999999996, "end": 3506.2, "text": " but it's one way of solving a linear system of equations, particularly when you're going", "tokens": [457, 309, 311, 472, 636, 295, 12606, 257, 8213, 1185, 295, 11787, 11, 4098, 562, 291, 434, 516], "temperature": 0.0, "avg_logprob": -0.18968086657316788, "compression_ratio": 1.6901408450704225, "no_speech_prob": 2.482434183548321e-06}, {"id": 805, "seek": 349620, "start": 3506.2, "end": 3510.68, "text": " to do it for multiple vectors B on your right-hand side of the equation.", "tokens": [281, 360, 309, 337, 3866, 18875, 363, 322, 428, 558, 12, 5543, 1252, 295, 264, 5367, 13], "temperature": 0.0, "avg_logprob": -0.18968086657316788, "compression_ratio": 1.6901408450704225, "no_speech_prob": 2.482434183548321e-06}, {"id": 806, "seek": 349620, "start": 3510.68, "end": 3519.3199999999997, "text": " It's helpful to store these because that'll speed up getting your X's kind of once you", "tokens": [467, 311, 4961, 281, 3531, 613, 570, 300, 603, 3073, 493, 1242, 428, 1783, 311, 733, 295, 1564, 291], "temperature": 0.0, "avg_logprob": -0.18968086657316788, "compression_ratio": 1.6901408450704225, "no_speech_prob": 2.482434183548321e-06}, {"id": 807, "seek": 349620, "start": 3519.3199999999997, "end": 3520.3199999999997, "text": " have the L and U.", "tokens": [362, 264, 441, 293, 624, 13], "temperature": 0.0, "avg_logprob": -0.18968086657316788, "compression_ratio": 1.6901408450704225, "no_speech_prob": 2.482434183548321e-06}, {"id": 808, "seek": 352032, "start": 3520.32, "end": 3530.6800000000003, "text": " Yeah, so I wanted to go through how, kind of how we have the computer doing this.", "tokens": [865, 11, 370, 286, 1415, 281, 352, 807, 577, 11, 733, 295, 577, 321, 362, 264, 3820, 884, 341, 13], "temperature": 0.0, "avg_logprob": -0.1435750119097821, "compression_ratio": 1.4705882352941178, "no_speech_prob": 4.495111170399468e-06}, {"id": 809, "seek": 352032, "start": 3530.6800000000003, "end": 3537.6400000000003, "text": " So we're making a copy of A that's going to be our U, and then remember U is what- actually", "tokens": [407, 321, 434, 1455, 257, 5055, 295, 316, 300, 311, 516, 281, 312, 527, 624, 11, 293, 550, 1604, 624, 307, 437, 12, 767], "temperature": 0.0, "avg_logprob": -0.1435750119097821, "compression_ratio": 1.4705882352941178, "no_speech_prob": 4.495111170399468e-06}, {"id": 810, "seek": 352032, "start": 3537.6400000000003, "end": 3544.04, "text": " I should probably go back to this.", "tokens": [286, 820, 1391, 352, 646, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.1435750119097821, "compression_ratio": 1.4705882352941178, "no_speech_prob": 4.495111170399468e-06}, {"id": 811, "seek": 352032, "start": 3544.04, "end": 3548.8, "text": " U is what we ended up with at the end of our Gaussian elimination.", "tokens": [624, 307, 437, 321, 4590, 493, 365, 412, 264, 917, 295, 527, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.1435750119097821, "compression_ratio": 1.4705882352941178, "no_speech_prob": 4.495111170399468e-06}, {"id": 812, "seek": 354880, "start": 3548.8, "end": 3555.6400000000003, "text": " So we're going to kind of be starting off with A, we copy that into U, and then are", "tokens": [407, 321, 434, 516, 281, 733, 295, 312, 2891, 766, 365, 316, 11, 321, 5055, 300, 666, 624, 11, 293, 550, 366], "temperature": 0.0, "avg_logprob": -0.09542018276149944, "compression_ratio": 1.5035971223021583, "no_speech_prob": 4.4951307245355565e-06}, {"id": 813, "seek": 354880, "start": 3555.6400000000003, "end": 3559.0, "text": " going through these steps to get it in this form.", "tokens": [516, 807, 613, 4439, 281, 483, 309, 294, 341, 1254, 13], "temperature": 0.0, "avg_logprob": -0.09542018276149944, "compression_ratio": 1.5035971223021583, "no_speech_prob": 4.4951307245355565e-06}, {"id": 814, "seek": 354880, "start": 3559.0, "end": 3573.4, "text": " And then L is where we're going to store kind of what we're multiplying by.", "tokens": [400, 550, 441, 307, 689, 321, 434, 516, 281, 3531, 733, 295, 437, 321, 434, 30955, 538, 13], "temperature": 0.0, "avg_logprob": -0.09542018276149944, "compression_ratio": 1.5035971223021583, "no_speech_prob": 4.4951307245355565e-06}, {"id": 815, "seek": 357340, "start": 3573.4, "end": 3582.6, "text": " So what we do is loop through K, and then- so K is in range n minus 1, since we don't", "tokens": [407, 437, 321, 360, 307, 6367, 807, 591, 11, 293, 550, 12, 370, 591, 307, 294, 3613, 297, 3175, 502, 11, 1670, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.2063508408792903, "compression_ratio": 1.553763440860215, "no_speech_prob": 5.862646958121331e-06}, {"id": 816, "seek": 357340, "start": 3582.6, "end": 3585.2400000000002, "text": " need to do anything to the first row.", "tokens": [643, 281, 360, 1340, 281, 264, 700, 5386, 13], "temperature": 0.0, "avg_logprob": -0.2063508408792903, "compression_ratio": 1.553763440860215, "no_speech_prob": 5.862646958121331e-06}, {"id": 817, "seek": 357340, "start": 3585.2400000000002, "end": 3587.84, "text": " J- or sorry, to the last column.", "tokens": [508, 12, 420, 2597, 11, 281, 264, 1036, 7738, 13], "temperature": 0.0, "avg_logprob": -0.2063508408792903, "compression_ratio": 1.553763440860215, "no_speech_prob": 5.862646958121331e-06}, {"id": 818, "seek": 357340, "start": 3587.84, "end": 3594.84, "text": " J is going through K plus 1 to n, so that's kind of how when we're working on a particular", "tokens": [508, 307, 516, 807, 591, 1804, 502, 281, 297, 11, 370, 300, 311, 733, 295, 577, 562, 321, 434, 1364, 322, 257, 1729], "temperature": 0.0, "avg_logprob": -0.2063508408792903, "compression_ratio": 1.553763440860215, "no_speech_prob": 5.862646958121331e-06}, {"id": 819, "seek": 357340, "start": 3594.84, "end": 3598.2400000000002, "text": " row, we need to do everything beneath it.", "tokens": [5386, 11, 321, 643, 281, 360, 1203, 17149, 309, 13], "temperature": 0.0, "avg_logprob": -0.2063508408792903, "compression_ratio": 1.553763440860215, "no_speech_prob": 5.862646958121331e-06}, {"id": 820, "seek": 359824, "start": 3598.24, "end": 3607.9599999999996, "text": " And we'll set Ljk equals the ratio of Ujk to Ukk, so that's kind of seeing how, you", "tokens": [400, 321, 603, 992, 441, 73, 74, 6915, 264, 8509, 295, 624, 73, 74, 281, 9816, 74, 11, 370, 300, 311, 733, 295, 2577, 577, 11, 291], "temperature": 0.0, "avg_logprob": -0.11799050331115722, "compression_ratio": 1.2666666666666666, "no_speech_prob": 3.966891654272331e-06}, {"id": 821, "seek": 359824, "start": 3607.9599999999996, "end": 3612.8399999999997, "text": " know, whatever spot we're at compares to the diagonal, which is Ukk.", "tokens": [458, 11, 2035, 4008, 321, 434, 412, 38334, 281, 264, 21539, 11, 597, 307, 9816, 74, 13], "temperature": 0.0, "avg_logprob": -0.11799050331115722, "compression_ratio": 1.2666666666666666, "no_speech_prob": 3.966891654272331e-06}, {"id": 822, "seek": 361284, "start": 3612.84, "end": 3630.92, "text": " And then we subtract off that times Uk, comma, to Kn is giving us the kth row, kind of the", "tokens": [400, 550, 321, 16390, 766, 300, 1413, 9816, 11, 22117, 11, 281, 10519, 307, 2902, 505, 264, 350, 392, 5386, 11, 733, 295, 264], "temperature": 0.0, "avg_logprob": -0.1551043070279635, "compression_ratio": 1.3636363636363635, "no_speech_prob": 5.043448823016661e-07}, {"id": 823, "seek": 361284, "start": 3630.92, "end": 3634.0, "text": " remaining columns.", "tokens": [8877, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1551043070279635, "compression_ratio": 1.3636363636363635, "no_speech_prob": 5.043448823016661e-07}, {"id": 824, "seek": 361284, "start": 3634.0, "end": 3640.0, "text": " So this is just the process that we did before, kind of put into code.", "tokens": [407, 341, 307, 445, 264, 1399, 300, 321, 630, 949, 11, 733, 295, 829, 666, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1551043070279635, "compression_ratio": 1.3636363636363635, "no_speech_prob": 5.043448823016661e-07}, {"id": 825, "seek": 364000, "start": 3640.0, "end": 3643.24, "text": " And then we return L and U at the end.", "tokens": [400, 550, 321, 2736, 441, 293, 624, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.23733761436060855, "compression_ratio": 1.2, "no_speech_prob": 1.8162018022849225e-06}, {"id": 826, "seek": 364000, "start": 3643.24, "end": 3654.8, "text": " If there are questions about this- and actually maybe it would be helpful, let me add a step.", "tokens": [759, 456, 366, 1651, 466, 341, 12, 293, 767, 1310, 309, 576, 312, 4961, 11, 718, 385, 909, 257, 1823, 13], "temperature": 0.0, "avg_logprob": -0.23733761436060855, "compression_ratio": 1.2, "no_speech_prob": 1.8162018022849225e-06}, {"id": 827, "seek": 365480, "start": 3654.8, "end": 3672.0, "text": " This might be too much, but I could try printing L and U each time.", "tokens": [639, 1062, 312, 886, 709, 11, 457, 286, 727, 853, 14699, 441, 293, 624, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.3708403737921464, "compression_ratio": 1.198019801980198, "no_speech_prob": 6.643291726504685e-06}, {"id": 828, "seek": 365480, "start": 3672.0, "end": 3674.04, "text": " I might not be connected to the kernel.", "tokens": [286, 1062, 406, 312, 4582, 281, 264, 28256, 13], "temperature": 0.0, "avg_logprob": -0.3708403737921464, "compression_ratio": 1.198019801980198, "no_speech_prob": 6.643291726504685e-06}, {"id": 829, "seek": 365480, "start": 3674.04, "end": 3675.04, "text": " Let me check.", "tokens": [961, 385, 1520, 13], "temperature": 0.0, "avg_logprob": -0.3708403737921464, "compression_ratio": 1.198019801980198, "no_speech_prob": 6.643291726504685e-06}, {"id": 830, "seek": 367504, "start": 3675.04, "end": 3687.8, "text": " Okay, so run this.", "tokens": [1033, 11, 370, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.41270754554054956, "compression_ratio": 1.173913043478261, "no_speech_prob": 8.397308192797936e-06}, {"id": 831, "seek": 367504, "start": 3687.8, "end": 3697.0, "text": " Oh, this is not aligned well.", "tokens": [876, 11, 341, 307, 406, 17962, 731, 13], "temperature": 0.0, "avg_logprob": -0.41270754554054956, "compression_ratio": 1.173913043478261, "no_speech_prob": 8.397308192797936e-06}, {"id": 832, "seek": 367504, "start": 3697.0, "end": 3704.8, "text": " But you can- actually, let me just print U, because that's- it'll be more interesting.", "tokens": [583, 291, 393, 12, 767, 11, 718, 385, 445, 4482, 624, 11, 570, 300, 311, 12, 309, 603, 312, 544, 1880, 13], "temperature": 0.0, "avg_logprob": -0.41270754554054956, "compression_ratio": 1.173913043478261, "no_speech_prob": 8.397308192797936e-06}, {"id": 833, "seek": 370480, "start": 3704.8, "end": 3712.04, "text": " U will give us kind of the matrix A that we were reducing with the Gaussian elimination.", "tokens": [624, 486, 976, 505, 733, 295, 264, 8141, 316, 300, 321, 645, 12245, 365, 264, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.21092838435978084, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.4063648450246546e-05}, {"id": 834, "seek": 370480, "start": 3712.04, "end": 3721.4, "text": " And so you can see here, first we're introducing a zero here, and then a zero beneath it, and", "tokens": [400, 370, 291, 393, 536, 510, 11, 700, 321, 434, 15424, 257, 4018, 510, 11, 293, 550, 257, 4018, 17149, 309, 11, 293], "temperature": 0.0, "avg_logprob": -0.21092838435978084, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.4063648450246546e-05}, {"id": 835, "seek": 370480, "start": 3721.4, "end": 3730.84, "text": " then at the third inner loop we've completely zeroed out the column- or the columns- the", "tokens": [550, 412, 264, 2636, 7284, 6367, 321, 600, 2584, 4018, 292, 484, 264, 7738, 12, 420, 264, 13766, 12, 264], "temperature": 0.0, "avg_logprob": -0.21092838435978084, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.4063648450246546e-05}, {"id": 836, "seek": 370480, "start": 3730.84, "end": 3732.1200000000003, "text": " spaces in the first column.", "tokens": [7673, 294, 264, 700, 7738, 13], "temperature": 0.0, "avg_logprob": -0.21092838435978084, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.4063648450246546e-05}, {"id": 837, "seek": 373212, "start": 3732.12, "end": 3738.3199999999997, "text": " Then we go into the next one and we start zeroing out everything below the diagonal", "tokens": [1396, 321, 352, 666, 264, 958, 472, 293, 321, 722, 4018, 278, 484, 1203, 2507, 264, 21539], "temperature": 0.0, "avg_logprob": -0.2640045748816596, "compression_ratio": 1.4480874316939891, "no_speech_prob": 2.668745037226472e-05}, {"id": 838, "seek": 373212, "start": 3738.3199999999997, "end": 3740.7999999999997, "text": " in the second column, and so on.", "tokens": [294, 264, 1150, 7738, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.2640045748816596, "compression_ratio": 1.4480874316939891, "no_speech_prob": 2.668745037226472e-05}, {"id": 839, "seek": 373212, "start": 3740.7999999999997, "end": 3741.7999999999997, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2640045748816596, "compression_ratio": 1.4480874316939891, "no_speech_prob": 2.668745037226472e-05}, {"id": 840, "seek": 373212, "start": 3741.7999999999997, "end": 3754.3199999999997, "text": " I don't recall, but this algorithm looks slow.", "tokens": [286, 500, 380, 9901, 11, 457, 341, 9284, 1542, 2964, 13], "temperature": 0.0, "avg_logprob": -0.2640045748816596, "compression_ratio": 1.4480874316939891, "no_speech_prob": 2.668745037226472e-05}, {"id": 841, "seek": 373212, "start": 3754.3199999999997, "end": 3758.44, "text": " Like we're having to do a separate operation for every one of the diagonals, so that's", "tokens": [1743, 321, 434, 1419, 281, 360, 257, 4994, 6916, 337, 633, 472, 295, 264, 17405, 1124, 11, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.2640045748816596, "compression_ratio": 1.4480874316939891, "no_speech_prob": 2.668745037226472e-05}, {"id": 842, "seek": 375844, "start": 3758.44, "end": 3764.16, "text": " already N by M, and then each time you're having to do it on a whole row.", "tokens": [1217, 426, 538, 376, 11, 293, 550, 1184, 565, 291, 434, 1419, 281, 360, 309, 322, 257, 1379, 5386, 13], "temperature": 0.0, "avg_logprob": -0.2224863071252804, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.0451226444274653e-05}, {"id": 843, "seek": 375844, "start": 3764.16, "end": 3771.44, "text": " That's right here, so it's the work- yeah, the big O for this is 2 times 1 third N cubed.", "tokens": [663, 311, 558, 510, 11, 370, 309, 311, 264, 589, 12, 1338, 11, 264, 955, 422, 337, 341, 307, 568, 1413, 502, 2636, 426, 36510, 13], "temperature": 0.0, "avg_logprob": -0.2224863071252804, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.0451226444274653e-05}, {"id": 844, "seek": 375844, "start": 3771.44, "end": 3777.68, "text": " And what you can think of it is it's kind of like a pyramid, the amount of- whoops- the", "tokens": [400, 437, 291, 393, 519, 295, 309, 307, 309, 311, 733, 295, 411, 257, 25950, 11, 264, 2372, 295, 12, 567, 3370, 12, 264], "temperature": 0.0, "avg_logprob": -0.2224863071252804, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.0451226444274653e-05}, {"id": 845, "seek": 375844, "start": 3777.68, "end": 3785.16, "text": " amount of work it's having to do, in that kind of you've got your outer loop, which", "tokens": [2372, 295, 589, 309, 311, 1419, 281, 360, 11, 294, 300, 733, 295, 291, 600, 658, 428, 10847, 6367, 11, 597], "temperature": 0.0, "avg_logprob": -0.2224863071252804, "compression_ratio": 1.5654205607476634, "no_speech_prob": 1.0451226444274653e-05}, {"id": 846, "seek": 378516, "start": 3785.16, "end": 3792.44, "text": " is approximately N iterations, but then you're just going from that point up to N, so that's", "tokens": [307, 10447, 426, 36540, 11, 457, 550, 291, 434, 445, 516, 490, 300, 935, 493, 281, 426, 11, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.14115515908042153, "compression_ratio": 1.6220095693779903, "no_speech_prob": 1.165938829217339e-05}, {"id": 847, "seek": 378516, "start": 3792.44, "end": 3800.7599999999998, "text": " kind of like forming a triangle, and then within each of those you're just doing work", "tokens": [733, 295, 411, 15745, 257, 13369, 11, 293, 550, 1951, 1184, 295, 729, 291, 434, 445, 884, 589], "temperature": 0.0, "avg_logprob": -0.14115515908042153, "compression_ratio": 1.6220095693779903, "no_speech_prob": 1.165938829217339e-05}, {"id": 848, "seek": 378516, "start": 3800.7599999999998, "end": 3804.96, "text": " on the kind of entries in the- like from K to N.", "tokens": [322, 264, 733, 295, 23041, 294, 264, 12, 411, 490, 591, 281, 426, 13], "temperature": 0.0, "avg_logprob": -0.14115515908042153, "compression_ratio": 1.6220095693779903, "no_speech_prob": 1.165938829217339e-05}, {"id": 849, "seek": 378516, "start": 3804.96, "end": 3811.6, "text": " And everything's dependent on everything else, so this doesn't look very paralysable at all.", "tokens": [400, 1203, 311, 12334, 322, 1203, 1646, 11, 370, 341, 1177, 380, 574, 588, 26009, 749, 712, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.14115515908042153, "compression_ratio": 1.6220095693779903, "no_speech_prob": 1.165938829217339e-05}, {"id": 850, "seek": 378516, "start": 3811.6, "end": 3812.6, "text": " That's true, yeah.", "tokens": [663, 311, 2074, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.14115515908042153, "compression_ratio": 1.6220095693779903, "no_speech_prob": 1.165938829217339e-05}, {"id": 851, "seek": 381260, "start": 3812.6, "end": 3822.48, "text": " I want to make a note to think about ways to speed it up, because now I am curious.", "tokens": [286, 528, 281, 652, 257, 3637, 281, 519, 466, 2098, 281, 3073, 309, 493, 11, 570, 586, 286, 669, 6369, 13], "temperature": 0.0, "avg_logprob": -0.22618268460643534, "compression_ratio": 1.6289592760180995, "no_speech_prob": 3.726602017195546e-06}, {"id": 852, "seek": 381260, "start": 3822.48, "end": 3824.72, "text": " We're actually going to be talking- we're going to go in a different direction today,", "tokens": [492, 434, 767, 516, 281, 312, 1417, 12, 321, 434, 516, 281, 352, 294, 257, 819, 3513, 965, 11], "temperature": 0.0, "avg_logprob": -0.22618268460643534, "compression_ratio": 1.6289592760180995, "no_speech_prob": 3.726602017195546e-06}, {"id": 853, "seek": 381260, "start": 3824.72, "end": 3829.16, "text": " Jeremy, which is kind of thinking more about the stability of this, but yeah, speed is", "tokens": [17809, 11, 597, 307, 733, 295, 1953, 544, 466, 264, 11826, 295, 341, 11, 457, 1338, 11, 3073, 307], "temperature": 0.0, "avg_logprob": -0.22618268460643534, "compression_ratio": 1.6289592760180995, "no_speech_prob": 3.726602017195546e-06}, {"id": 854, "seek": 381260, "start": 3829.16, "end": 3831.16, "text": " interesting too.", "tokens": [1880, 886, 13], "temperature": 0.0, "avg_logprob": -0.22618268460643534, "compression_ratio": 1.6289592760180995, "no_speech_prob": 3.726602017195546e-06}, {"id": 855, "seek": 381260, "start": 3831.16, "end": 3842.4, "text": " But yeah, so that's how we get kind of the big O of N cubed, and the one third N cubed", "tokens": [583, 1338, 11, 370, 300, 311, 577, 321, 483, 733, 295, 264, 955, 422, 295, 426, 36510, 11, 293, 264, 472, 2636, 426, 36510], "temperature": 0.0, "avg_logprob": -0.22618268460643534, "compression_ratio": 1.6289592760180995, "no_speech_prob": 3.726602017195546e-06}, {"id": 856, "seek": 384240, "start": 3842.4, "end": 3846.1600000000003, "text": " is coming from this idea of kind of like, yeah, it's pyramid-like, how much work you're", "tokens": [307, 1348, 490, 341, 1558, 295, 733, 295, 411, 11, 1338, 11, 309, 311, 25950, 12, 4092, 11, 577, 709, 589, 291, 434], "temperature": 0.0, "avg_logprob": -0.16079673091922186, "compression_ratio": 1.578125, "no_speech_prob": 1.0129742804565467e-05}, {"id": 857, "seek": 384240, "start": 3846.1600000000003, "end": 3850.84, "text": " having to do on each row, and typically with big O you don't talk about the coefficients,", "tokens": [1419, 281, 360, 322, 1184, 5386, 11, 293, 5850, 365, 955, 422, 291, 500, 380, 751, 466, 264, 31994, 11], "temperature": 0.0, "avg_logprob": -0.16079673091922186, "compression_ratio": 1.578125, "no_speech_prob": 1.0129742804565467e-05}, {"id": 858, "seek": 384240, "start": 3850.84, "end": 3854.1600000000003, "text": " but I have them in there just in case.", "tokens": [457, 286, 362, 552, 294, 456, 445, 294, 1389, 13], "temperature": 0.0, "avg_logprob": -0.16079673091922186, "compression_ratio": 1.578125, "no_speech_prob": 1.0129742804565467e-05}, {"id": 859, "seek": 384240, "start": 3854.1600000000003, "end": 3859.32, "text": " So yeah, but before we get to that, I just wanted to highlight that the LU factorization", "tokens": [407, 1338, 11, 457, 949, 321, 483, 281, 300, 11, 286, 445, 1415, 281, 5078, 300, 264, 31851, 5952, 2144], "temperature": 0.0, "avg_logprob": -0.16079673091922186, "compression_ratio": 1.578125, "no_speech_prob": 1.0129742804565467e-05}, {"id": 860, "seek": 384240, "start": 3859.32, "end": 3861.32, "text": " is useful.", "tokens": [307, 4420, 13], "temperature": 0.0, "avg_logprob": -0.16079673091922186, "compression_ratio": 1.578125, "no_speech_prob": 1.0129742804565467e-05}, {"id": 861, "seek": 384240, "start": 3861.32, "end": 3872.2400000000002, "text": " Solving AX equals B becomes LUX equals B. Then you can solve LY equals B, and UX equals", "tokens": [7026, 798, 316, 55, 6915, 363, 3643, 31851, 55, 6915, 363, 13, 1396, 291, 393, 5039, 42154, 6915, 363, 11, 293, 40176, 6915], "temperature": 0.0, "avg_logprob": -0.16079673091922186, "compression_ratio": 1.578125, "no_speech_prob": 1.0129742804565467e-05}, {"id": 862, "seek": 387224, "start": 3872.24, "end": 3878.2799999999997, "text": " Y, and what might be nice about solving, so solving kind of steps two and three, those", "tokens": [398, 11, 293, 437, 1062, 312, 1481, 466, 12606, 11, 370, 12606, 733, 295, 4439, 732, 293, 1045, 11, 729], "temperature": 0.0, "avg_logprob": -0.2599496364593506, "compression_ratio": 1.4830917874396135, "no_speech_prob": 4.222774350637337e-06}, {"id": 863, "seek": 387224, "start": 3878.2799999999997, "end": 3881.2799999999997, "text": " are both triangular systems.", "tokens": [366, 1293, 38190, 3652, 13], "temperature": 0.0, "avg_logprob": -0.2599496364593506, "compression_ratio": 1.4830917874396135, "no_speech_prob": 4.222774350637337e-06}, {"id": 864, "seek": 387224, "start": 3881.2799999999997, "end": 3885.7599999999998, "text": " Can anyone say why those would be a better thing to solve than your full system?", "tokens": [1664, 2878, 584, 983, 729, 576, 312, 257, 1101, 551, 281, 5039, 813, 428, 1577, 1185, 30], "temperature": 0.0, "avg_logprob": -0.2599496364593506, "compression_ratio": 1.4830917874396135, "no_speech_prob": 4.222774350637337e-06}, {"id": 865, "seek": 387224, "start": 3885.7599999999998, "end": 3886.7599999999998, "text": " Kelsey?", "tokens": [44714, 30], "temperature": 0.0, "avg_logprob": -0.2599496364593506, "compression_ratio": 1.4830917874396135, "no_speech_prob": 4.222774350637337e-06}, {"id": 866, "seek": 387224, "start": 3886.7599999999998, "end": 3896.9199999999996, "text": " Because you can sort of even like backwards substitute, just do one equation at a time.", "tokens": [1436, 291, 393, 1333, 295, 754, 411, 12204, 15802, 11, 445, 360, 472, 5367, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.2599496364593506, "compression_ratio": 1.4830917874396135, "no_speech_prob": 4.222774350637337e-06}, {"id": 867, "seek": 387224, "start": 3896.9199999999996, "end": 3897.9199999999996, "text": " Exactly, yeah.", "tokens": [7587, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2599496364593506, "compression_ratio": 1.4830917874396135, "no_speech_prob": 4.222774350637337e-06}, {"id": 868, "seek": 389792, "start": 3897.92, "end": 3902.8, "text": " You can take like the tip of your triangle where you just have one variable, solve that", "tokens": [509, 393, 747, 411, 264, 4125, 295, 428, 13369, 689, 291, 445, 362, 472, 7006, 11, 5039, 300], "temperature": 0.0, "avg_logprob": -0.16418402265794207, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.1381091503135394e-06}, {"id": 869, "seek": 389792, "start": 3902.8, "end": 3907.38, "text": " one, back substitute that into the equation that just has two variables, and so on.", "tokens": [472, 11, 646, 15802, 300, 666, 264, 5367, 300, 445, 575, 732, 9102, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.16418402265794207, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.1381091503135394e-06}, {"id": 870, "seek": 389792, "start": 3907.38, "end": 3910.64, "text": " So it is always nice when you can get stuff into a triangular system.", "tokens": [407, 309, 307, 1009, 1481, 562, 291, 393, 483, 1507, 666, 257, 38190, 1185, 13], "temperature": 0.0, "avg_logprob": -0.16418402265794207, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.1381091503135394e-06}, {"id": 871, "seek": 389792, "start": 3910.64, "end": 3920.08, "text": " As for the question of memory, so above we created two new matrices, L and U. However,", "tokens": [1018, 337, 264, 1168, 295, 4675, 11, 370, 3673, 321, 2942, 732, 777, 32284, 11, 441, 293, 624, 13, 2908, 11], "temperature": 0.0, "avg_logprob": -0.16418402265794207, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.1381091503135394e-06}, {"id": 872, "seek": 389792, "start": 3920.08, "end": 3925.7200000000003, "text": " they can actually be stored in matrix A by overwriting the original matrix, and remember", "tokens": [436, 393, 767, 312, 12187, 294, 8141, 316, 538, 670, 19868, 264, 3380, 8141, 11, 293, 1604], "temperature": 0.0, "avg_logprob": -0.16418402265794207, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.1381091503135394e-06}, {"id": 873, "seek": 392572, "start": 3925.72, "end": 3931.3599999999997, "text": " with L, since the diagonal is always one, so you don't actually have to explicitly store", "tokens": [365, 441, 11, 1670, 264, 21539, 307, 1009, 472, 11, 370, 291, 500, 380, 767, 362, 281, 20803, 3531], "temperature": 0.0, "avg_logprob": -0.1976783275604248, "compression_ratio": 1.592885375494071, "no_speech_prob": 3.2887037377804518e-06}, {"id": 874, "seek": 392572, "start": 3931.3599999999997, "end": 3932.8399999999997, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.1976783275604248, "compression_ratio": 1.592885375494071, "no_speech_prob": 3.2887037377804518e-06}, {"id": 875, "seek": 392572, "start": 3932.8399999999997, "end": 3938.4199999999996, "text": " And so this is called doing something in place, and it's a really common technique in numerical", "tokens": [400, 370, 341, 307, 1219, 884, 746, 294, 1081, 11, 293, 309, 311, 257, 534, 2689, 6532, 294, 29054], "temperature": 0.0, "avg_logprob": -0.1976783275604248, "compression_ratio": 1.592885375494071, "no_speech_prob": 3.2887037377804518e-06}, {"id": 876, "seek": 392572, "start": 3938.4199999999996, "end": 3942.08, "text": " linear algebra.", "tokens": [8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.1976783275604248, "compression_ratio": 1.592885375494071, "no_speech_prob": 3.2887037377804518e-06}, {"id": 877, "seek": 392572, "start": 3942.08, "end": 3945.64, "text": " If you ever wanted to use A again in the future, you wouldn't want to do that because you are", "tokens": [759, 291, 1562, 1415, 281, 764, 316, 797, 294, 264, 2027, 11, 291, 2759, 380, 528, 281, 360, 300, 570, 291, 366], "temperature": 0.0, "avg_logprob": -0.1976783275604248, "compression_ratio": 1.592885375494071, "no_speech_prob": 3.2887037377804518e-06}, {"id": 878, "seek": 392572, "start": 3945.64, "end": 3946.9199999999996, "text": " writing over it.", "tokens": [3579, 670, 309, 13], "temperature": 0.0, "avg_logprob": -0.1976783275604248, "compression_ratio": 1.592885375494071, "no_speech_prob": 3.2887037377804518e-06}, {"id": 879, "seek": 392572, "start": 3946.9199999999996, "end": 3952.16, "text": " But one of the homework questions, and I'll probably put this homework up on, I might", "tokens": [583, 472, 295, 264, 14578, 1651, 11, 293, 286, 603, 1391, 829, 341, 14578, 493, 322, 11, 286, 1062], "temperature": 0.0, "avg_logprob": -0.1976783275604248, "compression_ratio": 1.592885375494071, "no_speech_prob": 3.2887037377804518e-06}, {"id": 880, "seek": 395216, "start": 3952.16, "end": 3959.72, "text": " put it up this afternoon or Thursday, it won't be due until next Thursday, so like a week", "tokens": [829, 309, 493, 341, 6499, 420, 10383, 11, 309, 1582, 380, 312, 3462, 1826, 958, 10383, 11, 370, 411, 257, 1243], "temperature": 0.0, "avg_logprob": -0.19021285789600317, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.026111587838386e-06}, {"id": 881, "seek": 395216, "start": 3959.72, "end": 3965.08, "text": " from Thursday, is to modify LU to do the algorithm in place instead of kind of creating this", "tokens": [490, 10383, 11, 307, 281, 16927, 31851, 281, 360, 264, 9284, 294, 1081, 2602, 295, 733, 295, 4084, 341], "temperature": 0.0, "avg_logprob": -0.19021285789600317, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.026111587838386e-06}, {"id": 882, "seek": 395216, "start": 3965.08, "end": 3966.64, "text": " separate L and U.", "tokens": [4994, 441, 293, 624, 13], "temperature": 0.0, "avg_logprob": -0.19021285789600317, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.026111587838386e-06}, {"id": 883, "seek": 395216, "start": 3966.64, "end": 3974.04, "text": " Yeah, any questions about kind of work or memory for LU decomposition?", "tokens": [865, 11, 604, 1651, 466, 733, 295, 589, 420, 4675, 337, 31851, 48356, 30], "temperature": 0.0, "avg_logprob": -0.19021285789600317, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.026111587838386e-06}, {"id": 884, "seek": 397404, "start": 3974.04, "end": 3984.92, "text": " Okay, so now we're going to consider another matrix, A, which is 10 to the negative 20th,", "tokens": [1033, 11, 370, 586, 321, 434, 516, 281, 1949, 1071, 8141, 11, 316, 11, 597, 307, 1266, 281, 264, 3671, 945, 392, 11], "temperature": 0.0, "avg_logprob": -0.26355844073825413, "compression_ratio": 1.2847222222222223, "no_speech_prob": 6.048763225408038e-06}, {"id": 885, "seek": 397404, "start": 3984.92, "end": 3987.88, "text": " 1, 1 and 1.", "tokens": [502, 11, 502, 293, 502, 13], "temperature": 0.0, "avg_logprob": -0.26355844073825413, "compression_ratio": 1.2847222222222223, "no_speech_prob": 6.048763225408038e-06}, {"id": 886, "seek": 397404, "start": 3987.88, "end": 3995.2799999999997, "text": " And actually, I want you to take a moment just on paper to use Gaussian elimination", "tokens": [400, 767, 11, 286, 528, 291, 281, 747, 257, 1623, 445, 322, 3035, 281, 764, 39148, 29224], "temperature": 0.0, "avg_logprob": -0.26355844073825413, "compression_ratio": 1.2847222222222223, "no_speech_prob": 6.048763225408038e-06}, {"id": 887, "seek": 399528, "start": 3995.28, "end": 4008.1600000000003, "text": " to calculate L and U.", "tokens": [281, 8873, 441, 293, 624, 13], "temperature": 0.0, "avg_logprob": -0.6074735164642334, "compression_ratio": 0.7241379310344828, "no_speech_prob": 0.00032491475576534867}, {"id": 888, "seek": 400816, "start": 4008.16, "end": 4029.2, "text": " Okay.", "tokens": [1033, 13], "temperature": 1.0, "avg_logprob": -1.238102118174235, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.005023754667490721}, {"id": 889, "seek": 405920, "start": 4059.2, "end": 4066.2, "text": ".", "tokens": [2411], "temperature": 0.6, "avg_logprob": -0.9405954024370979, "compression_ratio": 0.45454545454545453, "no_speech_prob": 0.8933603167533875}, {"id": 890, "seek": 405920, "start": 4066.2, "end": 4071.2, "text": ".", "tokens": [2411], "temperature": 0.6, "avg_logprob": -0.9405954024370979, "compression_ratio": 0.45454545454545453, "no_speech_prob": 0.8933603167533875}, {"id": 891, "seek": 405920, "start": 4071.2, "end": 4076.2, "text": ".", "tokens": [2411], "temperature": 0.6, "avg_logprob": -0.9405954024370979, "compression_ratio": 0.45454545454545453, "no_speech_prob": 0.8933603167533875}, {"id": 892, "seek": 405920, "start": 4076.2, "end": 4081.2, "text": ".", "tokens": [2411], "temperature": 0.6, "avg_logprob": -0.9405954024370979, "compression_ratio": 0.45454545454545453, "no_speech_prob": 0.8933603167533875}, {"id": 893, "seek": 405920, "start": 4081.2, "end": 4083.2, "text": ".", "tokens": [2411], "temperature": 0.6, "avg_logprob": -0.9405954024370979, "compression_ratio": 0.45454545454545453, "no_speech_prob": 0.8933603167533875}, {"id": 894, "seek": 414320, "start": 4143.2, "end": 4156.5199999999995, "text": " Raise your hand if you want a little bit more time.", "tokens": [30062, 428, 1011, 498, 291, 528, 257, 707, 857, 544, 565, 13], "temperature": 0.0, "avg_logprob": -0.3346833476313838, "compression_ratio": 1.2054794520547945, "no_speech_prob": 0.0849810391664505}, {"id": 895, "seek": 414320, "start": 4156.5199999999995, "end": 4158.0, "text": " Raise your hand if you're ready now.", "tokens": [30062, 428, 1011, 498, 291, 434, 1919, 586, 13], "temperature": 0.0, "avg_logprob": -0.3346833476313838, "compression_ratio": 1.2054794520547945, "no_speech_prob": 0.0849810391664505}, {"id": 896, "seek": 415800, "start": 4158.0, "end": 4183.0, "text": " Okay, I'll give you another minute.", "tokens": [1033, 11, 286, 603, 976, 291, 1071, 3456, 13], "temperature": 0.0, "avg_logprob": -0.46451667638925404, "compression_ratio": 0.813953488372093, "no_speech_prob": 0.0024224226363003254}, {"id": 897, "seek": 418300, "start": 4183.0, "end": 4203.0, "text": " Alright, does someone want to say what they got for L?", "tokens": [2798, 11, 775, 1580, 528, 281, 584, 437, 436, 658, 337, 441, 30], "temperature": 0.0, "avg_logprob": -0.43345069885253906, "compression_ratio": 0.9, "no_speech_prob": 0.0002902718260884285}, {"id": 898, "seek": 420300, "start": 4203.0, "end": 4219.0, "text": " Okay, Vincent, Jeremy.", "tokens": [1033, 11, 28003, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.3728127941008537, "compression_ratio": 1.0238095238095237, "no_speech_prob": 9.752821642905474e-05}, {"id": 899, "seek": 420300, "start": 4219.0, "end": 4226.0, "text": " So row by row I've got one, zero, and ten to the twentieth one.", "tokens": [407, 5386, 538, 5386, 286, 600, 658, 472, 11, 4018, 11, 293, 2064, 281, 264, 34041, 38449, 472, 13], "temperature": 0.0, "avg_logprob": -0.3728127941008537, "compression_ratio": 1.0238095238095237, "no_speech_prob": 9.752821642905474e-05}, {"id": 900, "seek": 422600, "start": 4226.0, "end": 4237.0, "text": " Yes. Excellent. And then this...", "tokens": [1079, 13, 16723, 13, 400, 550, 341, 485], "temperature": 0.0, "avg_logprob": -0.21602391261680454, "compression_ratio": 1.5660377358490567, "no_speech_prob": 2.0462044631130993e-05}, {"id": 901, "seek": 422600, "start": 4237.0, "end": 4247.0, "text": " Yeah, so one, zero, ten to the twentieth one, and then U is ten to the negative twentieth, one, zero, one minus ten to the twentieth.", "tokens": [865, 11, 370, 472, 11, 4018, 11, 2064, 281, 264, 34041, 38449, 472, 11, 293, 550, 624, 307, 2064, 281, 264, 3671, 34041, 38449, 11, 472, 11, 4018, 11, 472, 3175, 2064, 281, 264, 34041, 38449, 13], "temperature": 0.0, "avg_logprob": -0.21602391261680454, "compression_ratio": 1.5660377358490567, "no_speech_prob": 2.0462044631130993e-05}, {"id": 902, "seek": 424700, "start": 4247.0, "end": 4257.0, "text": " Which would be approximately negative ten to the twentieth, since that's so much bigger than one.", "tokens": [3013, 576, 312, 10447, 3671, 2064, 281, 264, 34041, 38449, 11, 1670, 300, 311, 370, 709, 3801, 813, 472, 13], "temperature": 0.0, "avg_logprob": -0.07029646747517136, "compression_ratio": 1.295774647887324, "no_speech_prob": 3.96685527448426e-06}, {"id": 903, "seek": 424700, "start": 4257.0, "end": 4266.0, "text": " And so if we enter these as matrices, I'm going to call them L1 and U1.", "tokens": [400, 370, 498, 321, 3242, 613, 382, 32284, 11, 286, 478, 516, 281, 818, 552, 441, 16, 293, 624, 16, 13], "temperature": 0.0, "avg_logprob": -0.07029646747517136, "compression_ratio": 1.295774647887324, "no_speech_prob": 3.96685527448426e-06}, {"id": 904, "seek": 424700, "start": 4266.0, "end": 4271.0, "text": " Here they are.", "tokens": [1692, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.07029646747517136, "compression_ratio": 1.295774647887324, "no_speech_prob": 3.96685527448426e-06}, {"id": 905, "seek": 427100, "start": 4271.0, "end": 4281.0, "text": " Actually, first I should ask, are there questions about getting this Gaussian elimination or the LU for A?", "tokens": [5135, 11, 700, 286, 820, 1029, 11, 366, 456, 1651, 466, 1242, 341, 39148, 29224, 420, 264, 31851, 337, 316, 30], "temperature": 0.0, "avg_logprob": -0.09959742297296939, "compression_ratio": 1.5731707317073171, "no_speech_prob": 5.5074069678084925e-06}, {"id": 906, "seek": 427100, "start": 4281.0, "end": 4288.0, "text": " And you're probably feeling a little bit nervous because we had this ten to the twentieth and ten to the negative twentieth, which is really different from one.", "tokens": [400, 291, 434, 1391, 2633, 257, 707, 857, 6296, 570, 321, 632, 341, 2064, 281, 264, 34041, 38449, 293, 2064, 281, 264, 3671, 34041, 38449, 11, 597, 307, 534, 819, 490, 472, 13], "temperature": 0.0, "avg_logprob": -0.09959742297296939, "compression_ratio": 1.5731707317073171, "no_speech_prob": 5.5074069678084925e-06}, {"id": 907, "seek": 427100, "start": 4288.0, "end": 4293.0, "text": " So we might run into some problems.", "tokens": [407, 321, 1062, 1190, 666, 512, 2740, 13], "temperature": 0.0, "avg_logprob": -0.09959742297296939, "compression_ratio": 1.5731707317073171, "no_speech_prob": 5.5074069678084925e-06}, {"id": 908, "seek": 427100, "start": 4293.0, "end": 4298.0, "text": " So we do an LU decomposition on A, and this is using the LU that was written above.", "tokens": [407, 321, 360, 364, 31851, 48356, 322, 316, 11, 293, 341, 307, 1228, 264, 31851, 300, 390, 3720, 3673, 13], "temperature": 0.0, "avg_logprob": -0.09959742297296939, "compression_ratio": 1.5731707317073171, "no_speech_prob": 5.5074069678084925e-06}, {"id": 909, "seek": 429800, "start": 4298.0, "end": 4301.0, "text": " So kind of just our own implementation.", "tokens": [407, 733, 295, 445, 527, 1065, 11420, 13], "temperature": 0.0, "avg_logprob": -0.12537616841933308, "compression_ratio": 1.2424242424242424, "no_speech_prob": 1.1125413948320784e-05}, {"id": 910, "seek": 429800, "start": 4301.0, "end": 4313.0, "text": " And what we get is about right.", "tokens": [400, 437, 321, 483, 307, 466, 558, 13], "temperature": 0.0, "avg_logprob": -0.12537616841933308, "compression_ratio": 1.2424242424242424, "no_speech_prob": 1.1125413948320784e-05}, {"id": 911, "seek": 429800, "start": 4313.0, "end": 4321.0, "text": " Oh, I redefined everything. I shouldn't have done that. Okay.", "tokens": [876, 11, 286, 38818, 2001, 1203, 13, 286, 4659, 380, 362, 1096, 300, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.12537616841933308, "compression_ratio": 1.2424242424242424, "no_speech_prob": 1.1125413948320784e-05}, {"id": 912, "seek": 429800, "start": 4321.0, "end": 4324.0, "text": " Sorry, hold on a moment. Okay.", "tokens": [4919, 11, 1797, 322, 257, 1623, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.12537616841933308, "compression_ratio": 1.2424242424242424, "no_speech_prob": 1.1125413948320784e-05}, {"id": 913, "seek": 432400, "start": 4324.0, "end": 4332.0, "text": " Got A being what we want.", "tokens": [5803, 316, 885, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.10134234693315294, "compression_ratio": 1.4640522875816993, "no_speech_prob": 9.665913239587098e-06}, {"id": 914, "seek": 432400, "start": 4332.0, "end": 4340.0, "text": " So we can check. L1 is close to L2. So remember L1 is the one we calculated by hand. L2 is the one we got from our algorithm.", "tokens": [407, 321, 393, 1520, 13, 441, 16, 307, 1998, 281, 441, 17, 13, 407, 1604, 441, 16, 307, 264, 472, 321, 15598, 538, 1011, 13, 441, 17, 307, 264, 472, 321, 658, 490, 527, 9284, 13], "temperature": 0.0, "avg_logprob": -0.10134234693315294, "compression_ratio": 1.4640522875816993, "no_speech_prob": 9.665913239587098e-06}, {"id": 915, "seek": 432400, "start": 4340.0, "end": 4347.0, "text": " Or got from our implementation of LU. U1 is close to U2, so that's good.", "tokens": [1610, 658, 490, 527, 11420, 295, 31851, 13, 624, 16, 307, 1998, 281, 624, 17, 11, 370, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.10134234693315294, "compression_ratio": 1.4640522875816993, "no_speech_prob": 9.665913239587098e-06}, {"id": 916, "seek": 434700, "start": 4347.0, "end": 4355.0, "text": " Now, if we check that L2 times U2 is close to A, we get false. It's not.", "tokens": [823, 11, 498, 321, 1520, 300, 441, 17, 1413, 624, 17, 307, 1998, 281, 316, 11, 321, 483, 7908, 13, 467, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.08564295534227716, "compression_ratio": 1.380281690140845, "no_speech_prob": 3.446519031058415e-06}, {"id": 917, "seek": 434700, "start": 4355.0, "end": 4368.0, "text": " So this is kind of interesting that like each component, L and U, was close to the answer, but L times U is not close to A.", "tokens": [407, 341, 307, 733, 295, 1880, 300, 411, 1184, 6542, 11, 441, 293, 624, 11, 390, 1998, 281, 264, 1867, 11, 457, 441, 1413, 624, 307, 406, 1998, 281, 316, 13], "temperature": 0.0, "avg_logprob": -0.08564295534227716, "compression_ratio": 1.380281690140845, "no_speech_prob": 3.446519031058415e-06}, {"id": 918, "seek": 436800, "start": 4368.0, "end": 4377.0, "text": " So this is an example that so the LU factorization is stable, but it's not backwards stable.", "tokens": [407, 341, 307, 364, 1365, 300, 370, 264, 31851, 5952, 2144, 307, 8351, 11, 457, 309, 311, 406, 12204, 8351, 13], "temperature": 0.0, "avg_logprob": -0.1027783845600329, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.3081624956612359e-06}, {"id": 919, "seek": 436800, "start": 4377.0, "end": 4388.0, "text": " And I'm going to kind of define what those mean in a moment, but I think it's helpful to kind of keep this picture in mind.", "tokens": [400, 286, 478, 516, 281, 733, 295, 6964, 437, 729, 914, 294, 257, 1623, 11, 457, 286, 519, 309, 311, 4961, 281, 733, 295, 1066, 341, 3036, 294, 1575, 13], "temperature": 0.0, "avg_logprob": -0.1027783845600329, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.3081624956612359e-06}, {"id": 920, "seek": 438800, "start": 4388.0, "end": 4401.0, "text": " So an algorithm F hat for a problem F. So you'll think of F hat as kind of like how you're implementing this, whereas F is like the true problem you're interested in.", "tokens": [407, 364, 9284, 479, 2385, 337, 257, 1154, 479, 13, 407, 291, 603, 519, 295, 479, 2385, 382, 733, 295, 411, 577, 291, 434, 18114, 341, 11, 9735, 479, 307, 411, 264, 2074, 1154, 291, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.12828268828215422, "compression_ratio": 1.5797872340425532, "no_speech_prob": 2.6273577532265335e-05}, {"id": 921, "seek": 438800, "start": 4401.0, "end": 4416.0, "text": " And we say it's stable if for every X, F hat of X minus F of Y, norm of that over the norm of F of Y is less than machine epsilon.", "tokens": [400, 321, 584, 309, 311, 8351, 498, 337, 633, 1783, 11, 479, 2385, 295, 1783, 3175, 479, 295, 398, 11, 2026, 295, 300, 670, 264, 2026, 295, 479, 295, 398, 307, 1570, 813, 3479, 17889, 13], "temperature": 0.0, "avg_logprob": -0.12828268828215422, "compression_ratio": 1.5797872340425532, "no_speech_prob": 2.6273577532265335e-05}, {"id": 922, "seek": 441600, "start": 4416.0, "end": 4422.0, "text": " Where Y minus X over X is order of machine epsilon.", "tokens": [2305, 398, 3175, 1783, 670, 1783, 307, 1668, 295, 3479, 17889, 13], "temperature": 0.0, "avg_logprob": -0.09226263940861795, "compression_ratio": 1.8257261410788381, "no_speech_prob": 7.888998879934661e-06}, {"id": 923, "seek": 441600, "start": 4422.0, "end": 4431.0, "text": " And so the way Trevathan says this, I really like, is that a stable algorithm gives nearly the right answer to nearly the right question.", "tokens": [400, 370, 264, 636, 8648, 85, 9390, 1619, 341, 11, 286, 534, 411, 11, 307, 300, 257, 8351, 9284, 2709, 6217, 264, 558, 1867, 281, 6217, 264, 558, 1168, 13], "temperature": 0.0, "avg_logprob": -0.09226263940861795, "compression_ratio": 1.8257261410788381, "no_speech_prob": 7.888998879934661e-06}, {"id": 924, "seek": 441600, "start": 4431.0, "end": 4438.0, "text": " And so kind of translating that here, so X is kind of the right question that we're truly interested in.", "tokens": [400, 370, 733, 295, 35030, 300, 510, 11, 370, 1783, 307, 733, 295, 264, 558, 1168, 300, 321, 434, 4908, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.09226263940861795, "compression_ratio": 1.8257261410788381, "no_speech_prob": 7.888998879934661e-06}, {"id": 925, "seek": 441600, "start": 4438.0, "end": 4445.0, "text": " And we're just saying that there's some Y that's close to that. So we've got nearly the right questions. That's why it's you know, it's almost X.", "tokens": [400, 321, 434, 445, 1566, 300, 456, 311, 512, 398, 300, 311, 1998, 281, 300, 13, 407, 321, 600, 658, 6217, 264, 558, 1651, 13, 663, 311, 983, 309, 311, 291, 458, 11, 309, 311, 1920, 1783, 13], "temperature": 0.0, "avg_logprob": -0.09226263940861795, "compression_ratio": 1.8257261410788381, "no_speech_prob": 7.888998879934661e-06}, {"id": 926, "seek": 444500, "start": 4445.0, "end": 4457.0, "text": " Where this is how we're defining almost. And then the right answer is F, but we're just getting nearly the right answer, which is F hat.", "tokens": [2305, 341, 307, 577, 321, 434, 17827, 1920, 13, 400, 550, 264, 558, 1867, 307, 479, 11, 457, 321, 434, 445, 1242, 6217, 264, 558, 1867, 11, 597, 307, 479, 2385, 13], "temperature": 0.0, "avg_logprob": -0.12349999944368999, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.0129784641321748e-05}, {"id": 927, "seek": 444500, "start": 4457.0, "end": 4466.0, "text": " So the right answer to nearly the right question.", "tokens": [407, 264, 558, 1867, 281, 6217, 264, 558, 1168, 13], "temperature": 0.0, "avg_logprob": -0.12349999944368999, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.0129784641321748e-05}, {"id": 928, "seek": 446600, "start": 4466.0, "end": 4475.0, "text": " Jeremy's face.", "tokens": [17809, 311, 1851, 13], "temperature": 0.0, "avg_logprob": -0.22789013385772705, "compression_ratio": 1.2150537634408602, "no_speech_prob": 1.4063552043808158e-05}, {"id": 929, "seek": 446600, "start": 4475.0, "end": 4479.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.22789013385772705, "compression_ratio": 1.2150537634408602, "no_speech_prob": 1.4063552043808158e-05}, {"id": 930, "seek": 446600, "start": 4479.0, "end": 4482.0, "text": " Yes. Yeah.", "tokens": [1079, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.22789013385772705, "compression_ratio": 1.2150537634408602, "no_speech_prob": 1.4063552043808158e-05}, {"id": 931, "seek": 446600, "start": 4482.0, "end": 4490.0, "text": " So X is the true problem you're interested in. And Y is the problem close to that.", "tokens": [407, 1783, 307, 264, 2074, 1154, 291, 434, 3102, 294, 13, 400, 398, 307, 264, 1154, 1998, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.22789013385772705, "compression_ratio": 1.2150537634408602, "no_speech_prob": 1.4063552043808158e-05}, {"id": 932, "seek": 449000, "start": 4490.0, "end": 4503.0, "text": " So like in the true problem, that's a good question. And that's because we're saying that the", "tokens": [407, 411, 294, 264, 2074, 1154, 11, 300, 311, 257, 665, 1168, 13, 400, 300, 311, 570, 321, 434, 1566, 300, 264], "temperature": 0.0, "avg_logprob": -0.19004271580622747, "compression_ratio": 1.464, "no_speech_prob": 9.07983849174343e-06}, {"id": 933, "seek": 449000, "start": 4503.0, "end": 4508.0, "text": " you're getting nearly the right answer to nearly the right question.", "tokens": [291, 434, 1242, 6217, 264, 558, 1867, 281, 6217, 264, 558, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19004271580622747, "compression_ratio": 1.464, "no_speech_prob": 9.07983849174343e-06}, {"id": 934, "seek": 449000, "start": 4508.0, "end": 4512.0, "text": " So here.", "tokens": [407, 510, 13], "temperature": 0.0, "avg_logprob": -0.19004271580622747, "compression_ratio": 1.464, "no_speech_prob": 9.07983849174343e-06}, {"id": 935, "seek": 449000, "start": 4512.0, "end": 4515.0, "text": " X would be.", "tokens": [1783, 576, 312, 13], "temperature": 0.0, "avg_logprob": -0.19004271580622747, "compression_ratio": 1.464, "no_speech_prob": 9.07983849174343e-06}, {"id": 936, "seek": 451500, "start": 4515.0, "end": 4524.0, "text": " I feel like it's helpful to kind of like unpack his statement in reverse order, but like nearly the right question is Y.", "tokens": [286, 841, 411, 309, 311, 4961, 281, 733, 295, 411, 26699, 702, 5629, 294, 9943, 1668, 11, 457, 411, 6217, 264, 558, 1168, 307, 398, 13], "temperature": 0.0, "avg_logprob": -0.10441865659739873, "compression_ratio": 1.79874213836478, "no_speech_prob": 1.922218143590726e-05}, {"id": 937, "seek": 451500, "start": 4524.0, "end": 4529.0, "text": " So X is the right question. Nearly the right question is Y.", "tokens": [407, 1783, 307, 264, 558, 1168, 13, 38000, 264, 558, 1168, 307, 398, 13], "temperature": 0.0, "avg_logprob": -0.10441865659739873, "compression_ratio": 1.79874213836478, "no_speech_prob": 1.922218143590726e-05}, {"id": 938, "seek": 451500, "start": 4529.0, "end": 4537.0, "text": " The right answer is F. So we're looking at the right answer to nearly the right question would be F of Y.", "tokens": [440, 558, 1867, 307, 479, 13, 407, 321, 434, 1237, 412, 264, 558, 1867, 281, 6217, 264, 558, 1168, 576, 312, 479, 295, 398, 13], "temperature": 0.0, "avg_logprob": -0.10441865659739873, "compression_ratio": 1.79874213836478, "no_speech_prob": 1.922218143590726e-05}, {"id": 939, "seek": 453700, "start": 4537.0, "end": 4545.0, "text": " So to get like what was truly the correct answer to your representation Y that's not quite the true question X, but it's close.", "tokens": [407, 281, 483, 411, 437, 390, 4908, 264, 3006, 1867, 281, 428, 10290, 398, 300, 311, 406, 1596, 264, 2074, 1168, 1783, 11, 457, 309, 311, 1998, 13], "temperature": 0.0, "avg_logprob": -0.17124203273228236, "compression_ratio": 1.521505376344086, "no_speech_prob": 1.4284975804912392e-05}, {"id": 940, "seek": 453700, "start": 4545.0, "end": 4549.0, "text": " That's what's giving you the F of Y.", "tokens": [663, 311, 437, 311, 2902, 291, 264, 479, 295, 398, 13], "temperature": 0.0, "avg_logprob": -0.17124203273228236, "compression_ratio": 1.521505376344086, "no_speech_prob": 1.4284975804912392e-05}, {"id": 941, "seek": 453700, "start": 4549.0, "end": 4558.0, "text": " But then we're just saying you get it. You're getting something that's close to that.", "tokens": [583, 550, 321, 434, 445, 1566, 291, 483, 309, 13, 509, 434, 1242, 746, 300, 311, 1998, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.17124203273228236, "compression_ratio": 1.521505376344086, "no_speech_prob": 1.4284975804912392e-05}, {"id": 942, "seek": 453700, "start": 4558.0, "end": 4560.0, "text": " Don't work for me to understand.", "tokens": [1468, 380, 589, 337, 385, 281, 1223, 13], "temperature": 0.0, "avg_logprob": -0.17124203273228236, "compression_ratio": 1.521505376344086, "no_speech_prob": 1.4284975804912392e-05}, {"id": 943, "seek": 456000, "start": 4560.0, "end": 4567.0, "text": " I liked this interpretation because I think it's I mean this is like a math definition of stability.", "tokens": [286, 4501, 341, 14174, 570, 286, 519, 309, 311, 286, 914, 341, 307, 411, 257, 5221, 7123, 295, 11826, 13], "temperature": 0.0, "avg_logprob": -0.13767711237857216, "compression_ratio": 1.6406926406926408, "no_speech_prob": 1.5204642295429949e-05}, {"id": 944, "seek": 456000, "start": 4567.0, "end": 4574.0, "text": " And I appreciated him kind of giving a way to like think about it in more colloquial terms.", "tokens": [400, 286, 17169, 796, 733, 295, 2902, 257, 636, 281, 411, 519, 466, 309, 294, 544, 1263, 29826, 831, 2115, 13], "temperature": 0.0, "avg_logprob": -0.13767711237857216, "compression_ratio": 1.6406926406926408, "no_speech_prob": 1.5204642295429949e-05}, {"id": 945, "seek": 456000, "start": 4574.0, "end": 4579.0, "text": " But yeah, it's OK if you don't fully get it.", "tokens": [583, 1338, 11, 309, 311, 2264, 498, 291, 500, 380, 4498, 483, 309, 13], "temperature": 0.0, "avg_logprob": -0.13767711237857216, "compression_ratio": 1.6406926406926408, "no_speech_prob": 1.5204642295429949e-05}, {"id": 946, "seek": 456000, "start": 4579.0, "end": 4587.0, "text": " It's kind of thinking about like how we formalize these ideas of stability that we have like a general notion to of you know like we want the", "tokens": [467, 311, 733, 295, 1953, 466, 411, 577, 321, 9860, 1125, 613, 3487, 295, 11826, 300, 321, 362, 411, 257, 2674, 10710, 281, 295, 291, 458, 411, 321, 528, 264], "temperature": 0.0, "avg_logprob": -0.13767711237857216, "compression_ratio": 1.6406926406926408, "no_speech_prob": 1.5204642295429949e-05}, {"id": 947, "seek": 458700, "start": 4587.0, "end": 4595.0, "text": " answer to be close to what we want. But kind of how do you formalize that?", "tokens": [1867, 281, 312, 1998, 281, 437, 321, 528, 13, 583, 733, 295, 577, 360, 291, 9860, 1125, 300, 30], "temperature": 0.0, "avg_logprob": -0.10981224243899426, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041525133085088e-06}, {"id": 948, "seek": 458700, "start": 4595.0, "end": 4601.0, "text": " So fortunately, backwards stability is actually simpler than stability.", "tokens": [407, 25511, 11, 12204, 11826, 307, 767, 18587, 813, 11826, 13], "temperature": 0.0, "avg_logprob": -0.10981224243899426, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041525133085088e-06}, {"id": 949, "seek": 458700, "start": 4601.0, "end": 4606.0, "text": " And it's also stronger. So that means that anything that's backwards stable will be stable,", "tokens": [400, 309, 311, 611, 7249, 13, 407, 300, 1355, 300, 1340, 300, 311, 12204, 8351, 486, 312, 8351, 11], "temperature": 0.0, "avg_logprob": -0.10981224243899426, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041525133085088e-06}, {"id": 950, "seek": 458700, "start": 4606.0, "end": 4614.0, "text": " but not the reverse, as we just saw with the LU factorization, because that's stable but not backwards stable.", "tokens": [457, 406, 264, 9943, 11, 382, 321, 445, 1866, 365, 264, 31851, 5952, 2144, 11, 570, 300, 311, 8351, 457, 406, 12204, 8351, 13], "temperature": 0.0, "avg_logprob": -0.10981224243899426, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041525133085088e-06}, {"id": 951, "seek": 461400, "start": 4614.0, "end": 4619.0, "text": " And so an algorithm F hat, our problem F is backwards stable.", "tokens": [400, 370, 364, 9284, 479, 2385, 11, 527, 1154, 479, 307, 12204, 8351, 13], "temperature": 0.0, "avg_logprob": -0.1364242028498995, "compression_ratio": 1.5421686746987953, "no_speech_prob": 5.507341938937316e-06}, {"id": 952, "seek": 461400, "start": 4619.0, "end": 4628.0, "text": " If for each X, there's a Y close to it, such that F hat of X equals F of Y.", "tokens": [759, 337, 1184, 1783, 11, 456, 311, 257, 398, 1998, 281, 309, 11, 1270, 300, 479, 2385, 295, 1783, 6915, 479, 295, 398, 13], "temperature": 0.0, "avg_logprob": -0.1364242028498995, "compression_ratio": 1.5421686746987953, "no_speech_prob": 5.507341938937316e-06}, {"id": 953, "seek": 461400, "start": 4628.0, "end": 4636.0, "text": " And so Trevathan says that's a backwards stable algorithm gives exactly the right answer to nearly the right question.", "tokens": [400, 370, 8648, 85, 9390, 1619, 300, 311, 257, 12204, 8351, 9284, 2709, 2293, 264, 558, 1867, 281, 6217, 264, 558, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1364242028498995, "compression_ratio": 1.5421686746987953, "no_speech_prob": 5.507341938937316e-06}, {"id": 954, "seek": 463600, "start": 4636.0, "end": 4658.0, "text": " So that's why we're taking. Yeah, F hat of X equals F of Y.", "tokens": [407, 300, 311, 983, 321, 434, 1940, 13, 865, 11, 479, 2385, 295, 1783, 6915, 479, 295, 398, 13], "temperature": 0.0, "avg_logprob": -0.11528668196304985, "compression_ratio": 0.9365079365079365, "no_speech_prob": 1.3844590284861624e-05}, {"id": 955, "seek": 465800, "start": 4658.0, "end": 4670.0, "text": " And so yeah, we'll we'll probably talk about those more next time.", "tokens": [400, 370, 1338, 11, 321, 603, 321, 603, 1391, 751, 466, 729, 544, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.149073462737234, "compression_ratio": 1.481283422459893, "no_speech_prob": 5.014549060433637e-06}, {"id": 956, "seek": 465800, "start": 4670.0, "end": 4677.0, "text": " So now let's look at the matrix 1, 1, 10 to the negative 20th 1 for we'll call that A hat.", "tokens": [407, 586, 718, 311, 574, 412, 264, 8141, 502, 11, 502, 11, 1266, 281, 264, 3671, 945, 392, 502, 337, 321, 603, 818, 300, 316, 2385, 13], "temperature": 0.0, "avg_logprob": -0.149073462737234, "compression_ratio": 1.481283422459893, "no_speech_prob": 5.014549060433637e-06}, {"id": 957, "seek": 465800, "start": 4677.0, "end": 4685.0, "text": " And I want you to take another moment and use Gaussian elimination to calculate what L and U are for this matrix A hat,", "tokens": [400, 286, 528, 291, 281, 747, 1071, 1623, 293, 764, 39148, 29224, 281, 8873, 437, 441, 293, 624, 366, 337, 341, 8141, 316, 2385, 11], "temperature": 0.0, "avg_logprob": -0.149073462737234, "compression_ratio": 1.481283422459893, "no_speech_prob": 5.014549060433637e-06}, {"id": 958, "seek": 468500, "start": 4685.0, "end": 4714.0, "text": " which is similar but not identical to our A from before.", "tokens": [597, 307, 2531, 457, 406, 14800, 281, 527, 316, 490, 949, 13], "temperature": 0.0, "avg_logprob": -0.17681583762168884, "compression_ratio": 0.9333333333333333, "no_speech_prob": 0.0002268816315336153}, {"id": 959, "seek": 471400, "start": 4714.0, "end": 4739.0, "text": " Okay.", "tokens": [50364, 1033, 13, 51614], "temperature": 0.0, "avg_logprob": -0.8916154861450195, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.02503615990281105}, {"id": 960, "seek": 474400, "start": 4744.0, "end": 4769.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5832213163375854, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.9893829226493835}, {"id": 961, "seek": 476900, "start": 4769.0, "end": 4795.0, "text": " And who who needs more time?", "tokens": [400, 567, 567, 2203, 544, 565, 30], "temperature": 0.0, "avg_logprob": -0.2670534740794789, "compression_ratio": 0.875, "no_speech_prob": 1.012851862469688e-05}, {"id": 962, "seek": 479500, "start": 4795.0, "end": 4811.0, "text": " All right. Does anyone want to say what they got for L?", "tokens": [1057, 558, 13, 4402, 2878, 528, 281, 584, 437, 436, 658, 337, 441, 30], "temperature": 0.0, "avg_logprob": -0.10315781831741333, "compression_ratio": 0.9016393442622951, "no_speech_prob": 3.591091081034392e-05}, {"id": 963, "seek": 481100, "start": 4811.0, "end": 4831.0, "text": " Okay, Kelsey.", "tokens": [1033, 11, 44714, 13], "temperature": 0.0, "avg_logprob": -0.3480721116065979, "compression_ratio": 0.6190476190476191, "no_speech_prob": 3.340305511301267e-06}, {"id": 964, "seek": 483100, "start": 4831.0, "end": 4841.0, "text": " So before it was, yeah, before it was 10 to the 20th, now it's 10 to the negative 20th.", "tokens": [407, 949, 309, 390, 11, 1338, 11, 949, 309, 390, 1266, 281, 264, 945, 392, 11, 586, 309, 311, 1266, 281, 264, 3671, 945, 392, 13], "temperature": 0.0, "avg_logprob": -0.15487304632214532, "compression_ratio": 1.7317073170731707, "no_speech_prob": 3.4464051168470178e-06}, {"id": 965, "seek": 483100, "start": 4841.0, "end": 4846.0, "text": " Yeah, which is subtle.", "tokens": [865, 11, 597, 307, 13743, 13], "temperature": 0.0, "avg_logprob": -0.15487304632214532, "compression_ratio": 1.7317073170731707, "no_speech_prob": 3.4464051168470178e-06}, {"id": 966, "seek": 483100, "start": 4846.0, "end": 4854.0, "text": " Yeah. And then before we had 1 minus 10 to the 20th in our U and it's 1 minus 10 to the negative 20th.", "tokens": [865, 13, 400, 550, 949, 321, 632, 502, 3175, 1266, 281, 264, 945, 392, 294, 527, 624, 293, 309, 311, 502, 3175, 1266, 281, 264, 3671, 945, 392, 13], "temperature": 0.0, "avg_logprob": -0.15487304632214532, "compression_ratio": 1.7317073170731707, "no_speech_prob": 3.4464051168470178e-06}, {"id": 967, "seek": 485400, "start": 4854.0, "end": 4869.0, "text": " Yeah. So how was actually I guess first I should show the punch line, which is that we take our L U decomposition of A and check if it's if A is all close to L and U and this time it is.", "tokens": [865, 13, 407, 577, 390, 767, 286, 2041, 700, 286, 820, 855, 264, 8135, 1622, 11, 597, 307, 300, 321, 747, 527, 441, 624, 48356, 295, 316, 293, 1520, 498, 309, 311, 498, 316, 307, 439, 1998, 281, 441, 293, 624, 293, 341, 565, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.19325766764896016, "compression_ratio": 1.4224598930481283, "no_speech_prob": 1.172620272882341e-06}, {"id": 968, "seek": 485400, "start": 4869.0, "end": 4878.0, "text": " So that's that's an improvement. Vincent, Jeremy, can you throw the microphone?", "tokens": [407, 300, 311, 300, 311, 364, 10444, 13, 28003, 11, 17809, 11, 393, 291, 3507, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.19325766764896016, "compression_ratio": 1.4224598930481283, "no_speech_prob": 1.172620272882341e-06}, {"id": 969, "seek": 487800, "start": 4878.0, "end": 4885.0, "text": " Shouldn't the top row and U still be one and one?", "tokens": [34170, 380, 264, 1192, 5386, 293, 624, 920, 312, 472, 293, 472, 30], "temperature": 0.0, "avg_logprob": -0.176121609551566, "compression_ratio": 1.0128205128205128, "no_speech_prob": 1.0782750905491412e-05}, {"id": 970, "seek": 487800, "start": 4885.0, "end": 4897.0, "text": " Yes, it should be. Thank you.", "tokens": [1079, 11, 309, 820, 312, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.176121609551566, "compression_ratio": 1.0128205128205128, "no_speech_prob": 1.0782750905491412e-05}, {"id": 971, "seek": 489700, "start": 4897.0, "end": 4913.0, "text": " Okay. And so then how does A compare to A hat?", "tokens": [1033, 13, 400, 370, 550, 577, 775, 316, 6794, 281, 316, 2385, 30], "temperature": 0.0, "avg_logprob": -0.1351635876823874, "compression_ratio": 0.8846153846153846, "no_speech_prob": 1.2878583675046684e-06}, {"id": 972, "seek": 491300, "start": 4913.0, "end": 4928.0, "text": " Look back at A. Okay, so Tim is doing the gesture to indicate that two rows were switched. So really this A hat was just like A, but we switched the rows.", "tokens": [2053, 646, 412, 316, 13, 1033, 11, 370, 7172, 307, 884, 264, 22252, 281, 13330, 300, 732, 13241, 645, 16858, 13, 407, 534, 341, 316, 2385, 390, 445, 411, 316, 11, 457, 321, 16858, 264, 13241, 13], "temperature": 0.0, "avg_logprob": -0.12133400614668684, "compression_ratio": 1.2622950819672132, "no_speech_prob": 1.3496598967321916e-06}, {"id": 973, "seek": 492800, "start": 4928.0, "end": 4945.0, "text": " And first I want to say like switching the rows, if you think about this so often, you know, each row in a matrix might be a different data point, switching the rows should be totally fine if those are different data points, if you know, like different samples and some like measurements you've taken.", "tokens": [400, 700, 286, 528, 281, 584, 411, 16493, 264, 13241, 11, 498, 291, 519, 466, 341, 370, 2049, 11, 291, 458, 11, 1184, 5386, 294, 257, 8141, 1062, 312, 257, 819, 1412, 935, 11, 16493, 264, 13241, 820, 312, 3879, 2489, 498, 729, 366, 819, 1412, 2793, 11, 498, 291, 458, 11, 411, 819, 10938, 293, 512, 411, 15383, 291, 600, 2726, 13], "temperature": 0.0, "avg_logprob": -0.16442140892370424, "compression_ratio": 1.7005649717514124, "no_speech_prob": 3.0715866159880534e-05}, {"id": 974, "seek": 494500, "start": 4945.0, "end": 4968.0, "text": " These were systems of equations, switching the rows is fine and actually no matter what, because you can think of this as just multiplying by a permutation matrix P. So here we did 0 1 1 0 is P, multiply that by our A to get A hat, apply Gaussian elimination to P times A.", "tokens": [1981, 645, 3652, 295, 11787, 11, 16493, 264, 13241, 307, 2489, 293, 767, 572, 1871, 437, 11, 570, 291, 393, 519, 295, 341, 382, 445, 30955, 538, 257, 4784, 11380, 8141, 430, 13, 407, 510, 321, 630, 1958, 502, 502, 1958, 307, 430, 11, 12972, 300, 538, 527, 316, 281, 483, 316, 2385, 11, 3079, 39148, 29224, 281, 430, 1413, 316, 13], "temperature": 0.0, "avg_logprob": -0.14194472630818686, "compression_ratio": 1.439153439153439, "no_speech_prob": 1.3419291462923866e-05}, {"id": 975, "seek": 496800, "start": 4968.0, "end": 4976.0, "text": " And it actually turns out that it's okay to kind of permute them because at the end you could permute them back to what you had originally.", "tokens": [400, 309, 767, 4523, 484, 300, 309, 311, 1392, 281, 733, 295, 4784, 1169, 552, 570, 412, 264, 917, 291, 727, 4784, 1169, 552, 646, 281, 437, 291, 632, 7993, 13], "temperature": 0.0, "avg_logprob": -0.10112426207237638, "compression_ratio": 1.7341772151898733, "no_speech_prob": 1.4738291611138266e-05}, {"id": 976, "seek": 496800, "start": 4976.0, "end": 4996.0, "text": " And so this is what pivoting is, is to switch the rows around and basically, and this is called partial pivoting, and that's where at each step you want to choose the largest value in column K and move that row to be row K, because the problem we were getting into before", "tokens": [400, 370, 341, 307, 437, 14538, 278, 307, 11, 307, 281, 3679, 264, 13241, 926, 293, 1936, 11, 293, 341, 307, 1219, 14641, 14538, 278, 11, 293, 300, 311, 689, 412, 1184, 1823, 291, 528, 281, 2826, 264, 6443, 2158, 294, 7738, 591, 293, 1286, 300, 5386, 281, 312, 5386, 591, 11, 570, 264, 1154, 321, 645, 1242, 666, 949], "temperature": 0.0, "avg_logprob": -0.10112426207237638, "compression_ratio": 1.7341772151898733, "no_speech_prob": 1.4738291611138266e-05}, {"id": 977, "seek": 499600, "start": 4996.0, "end": 5008.0, "text": " was when we were dividing by a really small number, which we don't want to do, and so kind of choosing a large number is going to be more stable.", "tokens": [390, 562, 321, 645, 26764, 538, 257, 534, 1359, 1230, 11, 597, 321, 500, 380, 528, 281, 360, 11, 293, 370, 733, 295, 10875, 257, 2416, 1230, 307, 516, 281, 312, 544, 8351, 13], "temperature": 0.0, "avg_logprob": -0.07185409218072891, "compression_ratio": 1.4911242603550297, "no_speech_prob": 1.1125142918899655e-05}, {"id": 978, "seek": 499600, "start": 5008.0, "end": 5019.0, "text": " Yeah, so that's going to be a homework problem of kind of adding partial pivoting to the LU factorization.", "tokens": [865, 11, 370, 300, 311, 516, 281, 312, 257, 14578, 1154, 295, 733, 295, 5127, 14641, 14538, 278, 281, 264, 31851, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.07185409218072891, "compression_ratio": 1.4911242603550297, "no_speech_prob": 1.1125142918899655e-05}, {"id": 979, "seek": 501900, "start": 5019.0, "end": 5029.0, "text": " And then we can see here, this is just going through, is this the, no, this is a different A than we had before.", "tokens": [400, 550, 321, 393, 536, 510, 11, 341, 307, 445, 516, 807, 11, 307, 341, 264, 11, 572, 11, 341, 307, 257, 819, 316, 813, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.12622543257109972, "compression_ratio": 1.6181818181818182, "no_speech_prob": 8.139390956785064e-06}, {"id": 980, "seek": 501900, "start": 5029.0, "end": 5039.0, "text": " You'll get back a kind of LU and then a matrix of what your pivot, or your permutation matrix P, so that's letting you know how you permuted the rows.", "tokens": [509, 603, 483, 646, 257, 733, 295, 31851, 293, 550, 257, 8141, 295, 437, 428, 14538, 11, 420, 428, 4784, 11380, 8141, 430, 11, 370, 300, 311, 8295, 291, 458, 577, 291, 4784, 4866, 264, 13241, 13], "temperature": 0.0, "avg_logprob": -0.12622543257109972, "compression_ratio": 1.6181818181818182, "no_speech_prob": 8.139390956785064e-06}, {"id": 981, "seek": 501900, "start": 5039.0, "end": 5047.0, "text": " And then you can check that the, this is an example from Trevithan, we get the same answers.", "tokens": [400, 550, 291, 393, 1520, 300, 264, 11, 341, 307, 364, 1365, 490, 8648, 85, 355, 282, 11, 321, 483, 264, 912, 6338, 13], "temperature": 0.0, "avg_logprob": -0.12622543257109972, "compression_ratio": 1.6181818181818182, "no_speech_prob": 8.139390956785064e-06}, {"id": 982, "seek": 504700, "start": 5047.0, "end": 5052.0, "text": " There is something called complete pivoting, which permutes the rows and the columns.", "tokens": [821, 307, 746, 1219, 3566, 14538, 278, 11, 597, 4784, 1819, 264, 13241, 293, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.12392841685901988, "compression_ratio": 1.609375, "no_speech_prob": 2.6687477657105774e-05}, {"id": 983, "seek": 504700, "start": 5052.0, "end": 5059.0, "text": " We're not going to get into that into detail. It's so time consuming that it's rarely used in practice.", "tokens": [492, 434, 406, 516, 281, 483, 666, 300, 666, 2607, 13, 467, 311, 370, 565, 19867, 300, 309, 311, 13752, 1143, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.12392841685901988, "compression_ratio": 1.609375, "no_speech_prob": 2.6687477657105774e-05}, {"id": 984, "seek": 504700, "start": 5059.0, "end": 5066.0, "text": " I think it's basically never used in practice. Any questions about pivoting?", "tokens": [286, 519, 309, 311, 1936, 1128, 1143, 294, 3124, 13, 2639, 1651, 466, 14538, 278, 30], "temperature": 0.0, "avg_logprob": -0.12392841685901988, "compression_ratio": 1.609375, "no_speech_prob": 2.6687477657105774e-05}, {"id": 985, "seek": 504700, "start": 5066.0, "end": 5073.0, "text": " Why would you want to permute the columns?", "tokens": [1545, 576, 291, 528, 281, 4784, 1169, 264, 13766, 30], "temperature": 0.0, "avg_logprob": -0.12392841685901988, "compression_ratio": 1.609375, "no_speech_prob": 2.6687477657105774e-05}, {"id": 986, "seek": 507300, "start": 5073.0, "end": 5086.0, "text": " So permuting the columns, you can actually kind of run into the same issue of, yeah, I guess having like numbers that are too small.", "tokens": [407, 4784, 10861, 264, 13766, 11, 291, 393, 767, 733, 295, 1190, 666, 264, 912, 2734, 295, 11, 1338, 11, 286, 2041, 1419, 411, 3547, 300, 366, 886, 1359, 13], "temperature": 0.0, "avg_logprob": -0.08784815117164894, "compression_ratio": 1.6041666666666667, "no_speech_prob": 5.561970829148777e-05}, {"id": 987, "seek": 507300, "start": 5086.0, "end": 5093.0, "text": " And so by like permuting the columns, you're able to get the largest value, which is going to be more stable in a particular place.", "tokens": [400, 370, 538, 411, 4784, 10861, 264, 13766, 11, 291, 434, 1075, 281, 483, 264, 6443, 2158, 11, 597, 307, 516, 281, 312, 544, 8351, 294, 257, 1729, 1081, 13], "temperature": 0.0, "avg_logprob": -0.08784815117164894, "compression_ratio": 1.6041666666666667, "no_speech_prob": 5.561970829148777e-05}, {"id": 988, "seek": 507300, "start": 5093.0, "end": 5095.0, "text": " But yeah, it's just a lot to keep track of.", "tokens": [583, 1338, 11, 309, 311, 445, 257, 688, 281, 1066, 2837, 295, 13], "temperature": 0.0, "avg_logprob": -0.08784815117164894, "compression_ratio": 1.6041666666666667, "no_speech_prob": 5.561970829148777e-05}, {"id": 989, "seek": 509500, "start": 5095.0, "end": 5105.0, "text": " This is more for something that's used, like this is more for making the computations more accurate. It's not like something we would do.", "tokens": [639, 307, 544, 337, 746, 300, 311, 1143, 11, 411, 341, 307, 544, 337, 1455, 264, 2807, 763, 544, 8559, 13, 467, 311, 406, 411, 746, 321, 576, 360, 13], "temperature": 0.0, "avg_logprob": -0.13927955627441407, "compression_ratio": 1.6845238095238095, "no_speech_prob": 8.218634320655838e-05}, {"id": 990, "seek": 509500, "start": 5105.0, "end": 5115.0, "text": " I guess like all of this is to make our actual computer algorithm return a more accurate answer, or is there an analytical reason for doing this?", "tokens": [286, 2041, 411, 439, 295, 341, 307, 281, 652, 527, 3539, 3820, 9284, 2736, 257, 544, 8559, 1867, 11, 420, 307, 456, 364, 29579, 1778, 337, 884, 341, 30], "temperature": 0.0, "avg_logprob": -0.13927955627441407, "compression_ratio": 1.6845238095238095, "no_speech_prob": 8.218634320655838e-05}, {"id": 991, "seek": 511500, "start": 5115.0, "end": 5127.0, "text": " I mean, so I guess, yeah, it's to get your computer to return a more accurate. And like part of what was going on with this example where the 10 to the negative 20th failed,", "tokens": [286, 914, 11, 370, 286, 2041, 11, 1338, 11, 309, 311, 281, 483, 428, 3820, 281, 2736, 257, 544, 8559, 13, 400, 411, 644, 295, 437, 390, 516, 322, 365, 341, 1365, 689, 264, 1266, 281, 264, 3671, 945, 392, 7612, 11], "temperature": 0.0, "avg_logprob": -0.1372845723078801, "compression_ratio": 1.5431472081218274, "no_speech_prob": 1.0952715456369333e-05}, {"id": 992, "seek": 511500, "start": 5127.0, "end": 5136.0, "text": " I mean, that's less than machine epsilon, but it's something that's causing us to get something that's not even close as a result.", "tokens": [286, 914, 11, 300, 311, 1570, 813, 3479, 17889, 11, 457, 309, 311, 746, 300, 311, 9853, 505, 281, 483, 746, 300, 311, 406, 754, 1998, 382, 257, 1874, 13], "temperature": 0.0, "avg_logprob": -0.1372845723078801, "compression_ratio": 1.5431472081218274, "no_speech_prob": 1.0952715456369333e-05}, {"id": 993, "seek": 513600, "start": 5136.0, "end": 5148.0, "text": " I will, I guess, kind of jump to the punch line, perhaps, which is that even with partial pivoting, it's going to turn out that this algorithm is technically not stable.", "tokens": [286, 486, 11, 286, 2041, 11, 733, 295, 3012, 281, 264, 8135, 1622, 11, 4317, 11, 597, 307, 300, 754, 365, 14641, 14538, 278, 11, 309, 311, 516, 281, 1261, 484, 300, 341, 9284, 307, 12120, 406, 8351, 13], "temperature": 0.0, "avg_logprob": -0.0992772610156567, "compression_ratio": 1.461139896373057, "no_speech_prob": 4.157228431722615e-06}, {"id": 994, "seek": 513600, "start": 5148.0, "end": 5150.0, "text": " Complete pivoting is?", "tokens": [34687, 14538, 278, 307, 30], "temperature": 0.0, "avg_logprob": -0.0992772610156567, "compression_ratio": 1.461139896373057, "no_speech_prob": 4.157228431722615e-06}, {"id": 995, "seek": 513600, "start": 5150.0, "end": 5157.0, "text": " Yes. Well, actually, let me confirm that. I think that it is. I'll look it up and confirm.", "tokens": [1079, 13, 1042, 11, 767, 11, 718, 385, 9064, 300, 13, 286, 519, 300, 309, 307, 13, 286, 603, 574, 309, 493, 293, 9064, 13], "temperature": 0.0, "avg_logprob": -0.0992772610156567, "compression_ratio": 1.461139896373057, "no_speech_prob": 4.157228431722615e-06}, {"id": 996, "seek": 515700, "start": 5157.0, "end": 5172.0, "text": " The issue with with partial pivoting is that the matrices that would, so it's easy. We'll see in a moment. We're going to construct a matrix that is unstable.", "tokens": [440, 2734, 365, 365, 14641, 14538, 278, 307, 300, 264, 32284, 300, 576, 11, 370, 309, 311, 1858, 13, 492, 603, 536, 294, 257, 1623, 13, 492, 434, 516, 281, 7690, 257, 8141, 300, 307, 23742, 13], "temperature": 0.0, "avg_logprob": -0.10658596478975736, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.1726104958142969e-06}, {"id": 997, "seek": 515700, "start": 5172.0, "end": 5180.0, "text": " However, those matrices are so rare in kind of nature or in practice that you don't actually get them.", "tokens": [2908, 11, 729, 32284, 366, 370, 5892, 294, 733, 295, 3687, 420, 294, 3124, 300, 291, 500, 380, 767, 483, 552, 13], "temperature": 0.0, "avg_logprob": -0.10658596478975736, "compression_ratio": 1.5174418604651163, "no_speech_prob": 1.1726104958142969e-06}, {"id": 998, "seek": 518000, "start": 5180.0, "end": 5198.0, "text": " Yeah. Good questions. Yeah, and I'll confirm about a complete pivoting, which I believe is stable, but too slow to actually use.", "tokens": [865, 13, 2205, 1651, 13, 865, 11, 293, 286, 603, 9064, 466, 257, 3566, 14538, 278, 11, 597, 286, 1697, 307, 8351, 11, 457, 886, 2964, 281, 767, 764, 13], "temperature": 0.0, "avg_logprob": -0.13654488675734577, "compression_ratio": 1.1743119266055047, "no_speech_prob": 1.7603158539714059e-06}, {"id": 999, "seek": 519800, "start": 5198.0, "end": 5213.0, "text": " So now we're going to look at a system of equations of this form. So here we've got a matrix where we've got ones on the diagonal, ones in the last column, negative ones below the diagonal.", "tokens": [407, 586, 321, 434, 516, 281, 574, 412, 257, 1185, 295, 11787, 295, 341, 1254, 13, 407, 510, 321, 600, 658, 257, 8141, 689, 321, 600, 658, 2306, 322, 264, 21539, 11, 2306, 294, 264, 1036, 7738, 11, 3671, 2306, 2507, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.06788735257254706, "compression_ratio": 1.6436781609195403, "no_speech_prob": 8.664296728966292e-06}, {"id": 1000, "seek": 519800, "start": 5213.0, "end": 5221.0, "text": " And we're saying that times X equals this matrix of ones with a two in the second to last entry.", "tokens": [400, 321, 434, 1566, 300, 1413, 1783, 6915, 341, 8141, 295, 2306, 365, 257, 732, 294, 264, 1150, 281, 1036, 8729, 13], "temperature": 0.0, "avg_logprob": -0.06788735257254706, "compression_ratio": 1.6436781609195403, "no_speech_prob": 8.664296728966292e-06}, {"id": 1001, "seek": 522100, "start": 5221.0, "end": 5234.0, "text": " And so this is a very kind of particular type of matrix that we've constructed here. And so I wrote a function, makeMatrix, that will generate that for any size n to kind of have this form.", "tokens": [400, 370, 341, 307, 257, 588, 733, 295, 1729, 2010, 295, 8141, 300, 321, 600, 17083, 510, 13, 400, 370, 286, 4114, 257, 2445, 11, 652, 42325, 6579, 11, 300, 486, 8460, 300, 337, 604, 2744, 297, 281, 733, 295, 362, 341, 1254, 13], "temperature": 0.0, "avg_logprob": -0.101078767886107, "compression_ratio": 1.6681614349775784, "no_speech_prob": 7.527756679337472e-06}, {"id": 1002, "seek": 522100, "start": 5234.0, "end": 5247.0, "text": " And it's just. Yeah, setting the last column to ones, starting off with the identity, setting the last columns to ones and then setting everything below the diagonal to negative one.", "tokens": [400, 309, 311, 445, 13, 865, 11, 3287, 264, 1036, 7738, 281, 2306, 11, 2891, 766, 365, 264, 6575, 11, 3287, 264, 1036, 13766, 281, 2306, 293, 550, 3287, 1203, 2507, 264, 21539, 281, 3671, 472, 13], "temperature": 0.0, "avg_logprob": -0.101078767886107, "compression_ratio": 1.6681614349775784, "no_speech_prob": 7.527756679337472e-06}, {"id": 1003, "seek": 524700, "start": 5247.0, "end": 5260.0, "text": " We're making this vector over here, which is going to be all ones except a two in the negative second position.", "tokens": [492, 434, 1455, 341, 8062, 670, 510, 11, 597, 307, 516, 281, 312, 439, 2306, 3993, 257, 732, 294, 264, 3671, 1150, 2535, 13], "temperature": 0.0, "avg_logprob": -0.1124697389273808, "compression_ratio": 1.4409937888198758, "no_speech_prob": 2.144312566088047e-05}, {"id": 1004, "seek": 524700, "start": 5260.0, "end": 5268.0, "text": " I was going to have you do Gaussian elimination on the five by five system, but you might be. I see heads shaking. Yeah.", "tokens": [286, 390, 516, 281, 362, 291, 360, 39148, 29224, 322, 264, 1732, 538, 1732, 1185, 11, 457, 291, 1062, 312, 13, 286, 536, 8050, 15415, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.1124697389273808, "compression_ratio": 1.4409937888198758, "no_speech_prob": 2.144312566088047e-05}, {"id": 1005, "seek": 526800, "start": 5268.0, "end": 5279.0, "text": " Let's not do that. Also, in the interest of time and you may have had enough Gaussian, no, it wasn't. It wasn't just you. Enough Gaussian elimination practice.", "tokens": [961, 311, 406, 360, 300, 13, 2743, 11, 294, 264, 1179, 295, 565, 293, 291, 815, 362, 632, 1547, 39148, 11, 572, 11, 309, 2067, 380, 13, 467, 2067, 380, 445, 291, 13, 19401, 39148, 29224, 3124, 13], "temperature": 0.0, "avg_logprob": -0.30251243315547344, "compression_ratio": 1.4901960784313726, "no_speech_prob": 6.642982043558732e-06}, {"id": 1006, "seek": 526800, "start": 5279.0, "end": 5292.0, "text": " No, this is this is why we have computers. But here we're going to use the SciPy dot lenowg has an L U solve and into that actually pull up the.", "tokens": [883, 11, 341, 307, 341, 307, 983, 321, 362, 10807, 13, 583, 510, 321, 434, 516, 281, 764, 264, 16942, 47, 88, 5893, 40116, 305, 70, 575, 364, 441, 624, 5039, 293, 666, 300, 767, 2235, 493, 264, 13], "temperature": 0.0, "avg_logprob": -0.30251243315547344, "compression_ratio": 1.4901960784313726, "no_speech_prob": 6.642982043558732e-06}, {"id": 1007, "seek": 529200, "start": 5292.0, "end": 5300.0, "text": " Pull this up.", "tokens": [15074, 341, 493, 13], "temperature": 0.0, "avg_logprob": -0.10796192342584783, "compression_ratio": 1.4029850746268657, "no_speech_prob": 2.0144643713138066e-05}, {"id": 1008, "seek": 529200, "start": 5300.0, "end": 5313.0, "text": " You're going to pass a matrix and then the vector that you're trying to, you know, this is going to solve AX equals B and you're giving it A and B and it'll return X for you.", "tokens": [509, 434, 516, 281, 1320, 257, 8141, 293, 550, 264, 8062, 300, 291, 434, 1382, 281, 11, 291, 458, 11, 341, 307, 516, 281, 5039, 316, 55, 6915, 363, 293, 291, 434, 2902, 309, 316, 293, 363, 293, 309, 603, 2736, 1783, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.10796192342584783, "compression_ratio": 1.4029850746268657, "no_speech_prob": 2.0144643713138066e-05}, {"id": 1009, "seek": 531300, "start": 5313.0, "end": 5323.0, "text": " And so I'm running this for matrices of size 10, 20, 30, 40, 50, 60 to see what happens.", "tokens": [400, 370, 286, 478, 2614, 341, 337, 32284, 295, 2744, 1266, 11, 945, 11, 2217, 11, 3356, 11, 2625, 11, 4060, 281, 536, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.12107659445868599, "compression_ratio": 1.5223880597014925, "no_speech_prob": 5.093484560347861e-06}, {"id": 1010, "seek": 531300, "start": 5323.0, "end": 5331.0, "text": " And then I'm going to plot with the last five values in the solution are keep in mind since the system is getting bigger, you know, the first one has.", "tokens": [400, 550, 286, 478, 516, 281, 7542, 365, 264, 1036, 1732, 4190, 294, 264, 3827, 366, 1066, 294, 1575, 1670, 264, 1185, 307, 1242, 3801, 11, 291, 458, 11, 264, 700, 472, 575, 13], "temperature": 0.0, "avg_logprob": -0.12107659445868599, "compression_ratio": 1.5223880597014925, "no_speech_prob": 5.093484560347861e-06}, {"id": 1011, "seek": 531300, "start": 5331.0, "end": 5336.0, "text": " 10 and then 20, 30, 40, 50, 60, but to have a way to compare them.", "tokens": [1266, 293, 550, 945, 11, 2217, 11, 3356, 11, 2625, 11, 4060, 11, 457, 281, 362, 257, 636, 281, 6794, 552, 13], "temperature": 0.0, "avg_logprob": -0.12107659445868599, "compression_ratio": 1.5223880597014925, "no_speech_prob": 5.093484560347861e-06}, {"id": 1012, "seek": 533600, "start": 5336.0, "end": 5347.0, "text": " And here we're printing them out. And if you look at the last five values, you might notice, OK, the last value for the first one, the 10 by 10 system is pretty close to one.", "tokens": [400, 510, 321, 434, 14699, 552, 484, 13, 400, 498, 291, 574, 412, 264, 1036, 1732, 4190, 11, 291, 1062, 3449, 11, 2264, 11, 264, 1036, 2158, 337, 264, 700, 472, 11, 264, 1266, 538, 1266, 1185, 307, 1238, 1998, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.15193133036295572, "compression_ratio": 1.5932203389830508, "no_speech_prob": 9.817971658776514e-06}, {"id": 1013, "seek": 533600, "start": 5347.0, "end": 5352.0, "text": " We've got something pretty close to a half. Negative a fourth.", "tokens": [492, 600, 658, 746, 1238, 1998, 281, 257, 1922, 13, 43230, 257, 6409, 13], "temperature": 0.0, "avg_logprob": -0.15193133036295572, "compression_ratio": 1.5932203389830508, "no_speech_prob": 9.817971658776514e-06}, {"id": 1014, "seek": 533600, "start": 5352.0, "end": 5356.0, "text": " Negative an eighth. You might see a pattern.", "tokens": [43230, 364, 19495, 13, 509, 1062, 536, 257, 5102, 13], "temperature": 0.0, "avg_logprob": -0.15193133036295572, "compression_ratio": 1.5932203389830508, "no_speech_prob": 9.817971658776514e-06}, {"id": 1015, "seek": 535600, "start": 5356.0, "end": 5370.0, "text": " And that seems to continue. OK, one. This is in scientific notations. You kind of have to pay attention. OK, this is really point five, negative point two five, negative point one two five.", "tokens": [400, 300, 2544, 281, 2354, 13, 2264, 11, 472, 13, 639, 307, 294, 8134, 406, 763, 13, 509, 733, 295, 362, 281, 1689, 3202, 13, 2264, 11, 341, 307, 534, 935, 1732, 11, 3671, 935, 732, 1732, 11, 3671, 935, 472, 732, 1732, 13], "temperature": 0.0, "avg_logprob": -0.14155642593963236, "compression_ratio": 1.5502645502645502, "no_speech_prob": 5.33802676727646e-06}, {"id": 1016, "seek": 535600, "start": 5370.0, "end": 5378.0, "text": " So we've got this pattern for 10, 20, 30, 40 and so on. And then notice what happens when we get to 60.", "tokens": [407, 321, 600, 658, 341, 5102, 337, 1266, 11, 945, 11, 2217, 11, 3356, 293, 370, 322, 13, 400, 550, 3449, 437, 2314, 562, 321, 483, 281, 4060, 13], "temperature": 0.0, "avg_logprob": -0.14155642593963236, "compression_ratio": 1.5502645502645502, "no_speech_prob": 5.33802676727646e-06}, {"id": 1017, "seek": 537800, "start": 5378.0, "end": 5387.0, "text": " Our answer. So this should kind of raise a flag for you that it seems to not be in keeping with where we were.", "tokens": [2621, 1867, 13, 407, 341, 820, 733, 295, 5300, 257, 7166, 337, 291, 300, 309, 2544, 281, 406, 312, 294, 5145, 365, 689, 321, 645, 13], "temperature": 0.0, "avg_logprob": -0.10580564645620492, "compression_ratio": 1.4143646408839778, "no_speech_prob": 7.527492471126607e-06}, {"id": 1018, "seek": 537800, "start": 5387.0, "end": 5397.0, "text": " And so here where I plotted them, it's a little bit hard to maybe see because you basically. So I've made these dashed lines of different colors.", "tokens": [400, 370, 510, 689, 286, 43288, 552, 11, 309, 311, 257, 707, 857, 1152, 281, 1310, 536, 570, 291, 1936, 13, 407, 286, 600, 1027, 613, 8240, 292, 3876, 295, 819, 4577, 13], "temperature": 0.0, "avg_logprob": -0.10580564645620492, "compression_ratio": 1.4143646408839778, "no_speech_prob": 7.527492471126607e-06}, {"id": 1019, "seek": 539700, "start": 5397.0, "end": 5412.0, "text": " And basically these are completely on top of each other. So this like thick multicolor line, that's the solution for the last five rows for 10, 20, 30, 40, 50.", "tokens": [400, 1936, 613, 366, 2584, 322, 1192, 295, 1184, 661, 13, 407, 341, 411, 5060, 30608, 36182, 1622, 11, 300, 311, 264, 3827, 337, 264, 1036, 1732, 13241, 337, 1266, 11, 945, 11, 2217, 11, 3356, 11, 2625, 13], "temperature": 0.0, "avg_logprob": -0.14397107230292427, "compression_ratio": 1.452127659574468, "no_speech_prob": 5.453106837194355e-07}, {"id": 1020, "seek": 541200, "start": 5412.0, "end": 5432.0, "text": " And then we jump to this when we hit 60. So they don't want to say what they think is happening when any goals 60.", "tokens": [400, 550, 321, 3012, 281, 341, 562, 321, 2045, 4060, 13, 407, 436, 500, 380, 528, 281, 584, 437, 436, 519, 307, 2737, 562, 604, 5493, 4060, 13], "temperature": 0.0, "avg_logprob": -0.20634821298960093, "compression_ratio": 1.27, "no_speech_prob": 9.664319804869592e-06}, {"id": 1021, "seek": 541200, "start": 5432.0, "end": 5439.0, "text": " Any guesses.", "tokens": [2639, 42703, 13], "temperature": 0.0, "avg_logprob": -0.20634821298960093, "compression_ratio": 1.27, "no_speech_prob": 9.664319804869592e-06}, {"id": 1022, "seek": 543900, "start": 5439.0, "end": 5467.0, "text": " And I shall give you a hint.", "tokens": [400, 286, 4393, 976, 291, 257, 12075, 13], "temperature": 0.0, "avg_logprob": -0.29369471470514935, "compression_ratio": 0.7777777777777778, "no_speech_prob": 0.0003512788098305464}, {"id": 1023, "seek": 546700, "start": 5467.0, "end": 5473.0, "text": " Actually, OK, I'll just say I had written out the Gaussian elimination. I won't make you go through it.", "tokens": [5135, 11, 2264, 11, 286, 603, 445, 584, 286, 632, 3720, 484, 264, 39148, 29224, 13, 286, 1582, 380, 652, 291, 352, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.08920715405390812, "compression_ratio": 1.2638888888888888, "no_speech_prob": 6.048662271496141e-06}, {"id": 1024, "seek": 546700, "start": 5473.0, "end": 5481.0, "text": " But what you end up with for this system is.", "tokens": [583, 437, 291, 917, 493, 365, 337, 341, 1185, 307, 13], "temperature": 0.0, "avg_logprob": -0.08920715405390812, "compression_ratio": 1.2638888888888888, "no_speech_prob": 6.048662271496141e-06}, {"id": 1025, "seek": 546700, "start": 5481.0, "end": 5491.0, "text": " This is the five by five version.", "tokens": [639, 307, 264, 1732, 538, 1732, 3037, 13], "temperature": 0.0, "avg_logprob": -0.08920715405390812, "compression_ratio": 1.2638888888888888, "no_speech_prob": 6.048662271496141e-06}, {"id": 1026, "seek": 549100, "start": 5491.0, "end": 5497.0, "text": " This is your you. So they have all zeros in there. You've got a diagonal of ones and then the last column.", "tokens": [639, 307, 428, 291, 13, 407, 436, 362, 439, 35193, 294, 456, 13, 509, 600, 658, 257, 21539, 295, 2306, 293, 550, 264, 1036, 7738, 13], "temperature": 0.0, "avg_logprob": -0.10914318736006574, "compression_ratio": 1.5192307692307692, "no_speech_prob": 9.276186005990894e-07}, {"id": 1027, "seek": 549100, "start": 5497.0, "end": 5504.0, "text": " This is kind of pathological. It's doubled in every entry from what we were doing because we had all those ones.", "tokens": [639, 307, 733, 295, 3100, 4383, 13, 467, 311, 24405, 294, 633, 8729, 490, 437, 321, 645, 884, 570, 321, 632, 439, 729, 2306, 13], "temperature": 0.0, "avg_logprob": -0.10914318736006574, "compression_ratio": 1.5192307692307692, "no_speech_prob": 9.276186005990894e-07}, {"id": 1028, "seek": 549100, "start": 5504.0, "end": 5512.0, "text": " And then we're saying, oh, that's equal. And so over here, something very similar was happening.", "tokens": [400, 550, 321, 434, 1566, 11, 1954, 11, 300, 311, 2681, 13, 400, 370, 670, 510, 11, 746, 588, 2531, 390, 2737, 13], "temperature": 0.0, "avg_logprob": -0.10914318736006574, "compression_ratio": 1.5192307692307692, "no_speech_prob": 9.276186005990894e-07}, {"id": 1029, "seek": 551200, "start": 5512.0, "end": 5521.0, "text": " One, two, four. And then nine and 17. And those are off a little bit since we had that two entry amongst our ones.", "tokens": [1485, 11, 732, 11, 1451, 13, 400, 550, 4949, 293, 3282, 13, 400, 729, 366, 766, 257, 707, 857, 1670, 321, 632, 300, 732, 8729, 12918, 527, 2306, 13], "temperature": 0.0, "avg_logprob": -0.12643401519111966, "compression_ratio": 1.5, "no_speech_prob": 2.947895836769021e-06}, {"id": 1030, "seek": 551200, "start": 5521.0, "end": 5526.0, "text": " Otherwise, it would have been kind of perfectly there doubling.", "tokens": [10328, 11, 309, 576, 362, 668, 733, 295, 6239, 456, 33651, 13], "temperature": 0.0, "avg_logprob": -0.12643401519111966, "compression_ratio": 1.5, "no_speech_prob": 2.947895836769021e-06}, {"id": 1031, "seek": 551200, "start": 5526.0, "end": 5530.0, "text": " Actually, this first question is on what I've shown. So I just took the five by five of that system.", "tokens": [5135, 11, 341, 700, 1168, 307, 322, 437, 286, 600, 4898, 13, 407, 286, 445, 1890, 264, 1732, 538, 1732, 295, 300, 1185, 13], "temperature": 0.0, "avg_logprob": -0.12643401519111966, "compression_ratio": 1.5, "no_speech_prob": 2.947895836769021e-06}, {"id": 1032, "seek": 551200, "start": 5530.0, "end": 5535.0, "text": " I did Gaussian elimination at home, wrote it out. This is what you get.", "tokens": [286, 630, 39148, 29224, 412, 1280, 11, 4114, 309, 484, 13, 639, 307, 437, 291, 483, 13], "temperature": 0.0, "avg_logprob": -0.12643401519111966, "compression_ratio": 1.5, "no_speech_prob": 2.947895836769021e-06}, {"id": 1033, "seek": 553500, "start": 5535.0, "end": 5545.0, "text": " And now if we were going to go back and backwards solve that, and I haven't written all the zeros in over here, but these would all be zeros.", "tokens": [400, 586, 498, 321, 645, 516, 281, 352, 646, 293, 12204, 5039, 300, 11, 293, 286, 2378, 380, 3720, 439, 264, 35193, 294, 670, 510, 11, 457, 613, 576, 439, 312, 35193, 13], "temperature": 0.0, "avg_logprob": -0.09816465737684718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 7.646259291504975e-06}, {"id": 1034, "seek": 553500, "start": 5545.0, "end": 5555.0, "text": " So let me make so often when people write that, they'll kind of just have like a big zero to show like, OK, those are all zeros in there where it doesn't say otherwise.", "tokens": [407, 718, 385, 652, 370, 2049, 562, 561, 2464, 300, 11, 436, 603, 733, 295, 445, 362, 411, 257, 955, 4018, 281, 855, 411, 11, 2264, 11, 729, 366, 439, 35193, 294, 456, 689, 309, 1177, 380, 584, 5911, 13], "temperature": 0.0, "avg_logprob": -0.09816465737684718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 7.646259291504975e-06}, {"id": 1035, "seek": 553500, "start": 5555.0, "end": 5561.0, "text": " Actually, this is a good time to point out you as a sparse matrix in this case that has lots of zeros.", "tokens": [5135, 11, 341, 307, 257, 665, 565, 281, 935, 484, 291, 382, 257, 637, 11668, 8141, 294, 341, 1389, 300, 575, 3195, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.09816465737684718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 7.646259291504975e-06}, {"id": 1036, "seek": 556100, "start": 5561.0, "end": 5570.0, "text": " So if we were going to start solving this now, you know, it start with the last row 16 times X sub five equals 17.", "tokens": [407, 498, 321, 645, 516, 281, 722, 12606, 341, 586, 11, 291, 458, 11, 309, 722, 365, 264, 1036, 5386, 3165, 1413, 1783, 1422, 1732, 6915, 3282, 13], "temperature": 0.0, "avg_logprob": -0.14635631854717548, "compression_ratio": 1.3975903614457832, "no_speech_prob": 6.962060524529079e-06}, {"id": 1037, "seek": 556100, "start": 5570.0, "end": 5575.0, "text": " And we're going to divide through by 16. So now any kind of guesses?", "tokens": [400, 321, 434, 516, 281, 9845, 807, 538, 3165, 13, 407, 586, 604, 733, 295, 42703, 30], "temperature": 0.0, "avg_logprob": -0.14635631854717548, "compression_ratio": 1.3975903614457832, "no_speech_prob": 6.962060524529079e-06}, {"id": 1038, "seek": 556100, "start": 5575.0, "end": 5589.0, "text": " Maybe what was going on when we hit an equal 60.", "tokens": [2704, 437, 390, 516, 322, 562, 321, 2045, 364, 2681, 4060, 13], "temperature": 0.0, "avg_logprob": -0.14635631854717548, "compression_ratio": 1.3975903614457832, "no_speech_prob": 6.962060524529079e-06}, {"id": 1039, "seek": 558900, "start": 5589.0, "end": 5594.0, "text": " Sam.", "tokens": [4832, 13], "temperature": 0.0, "avg_logprob": -0.2690069587142379, "compression_ratio": 1.3781512605042017, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1040, "seek": 558900, "start": 5594.0, "end": 5601.0, "text": " I guess two to the value at the bottom right of our upper triangular is going to be two to the 60.", "tokens": [286, 2041, 732, 281, 264, 2158, 412, 264, 2767, 558, 295, 527, 6597, 38190, 307, 516, 281, 312, 732, 281, 264, 4060, 13], "temperature": 0.0, "avg_logprob": -0.2690069587142379, "compression_ratio": 1.3781512605042017, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1041, "seek": 558900, "start": 5601.0, "end": 5602.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.2690069587142379, "compression_ratio": 1.3781512605042017, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1042, "seek": 558900, "start": 5602.0, "end": 5609.0, "text": " Plus one is it's the plus one is lost. So it's just 60.", "tokens": [7721, 472, 307, 309, 311, 264, 1804, 472, 307, 2731, 13, 407, 309, 311, 445, 4060, 13], "temperature": 0.0, "avg_logprob": -0.2690069587142379, "compression_ratio": 1.3781512605042017, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1043, "seek": 560900, "start": 5609.0, "end": 5620.0, "text": " Exactly. Yeah. Yeah. So we've gotten so large that we're kind of going to lose that distinction of having and I think I think it might be two to the 59.", "tokens": [7587, 13, 865, 13, 865, 13, 407, 321, 600, 5768, 370, 2416, 300, 321, 434, 733, 295, 516, 281, 3624, 300, 16844, 295, 1419, 293, 286, 519, 286, 519, 309, 1062, 312, 732, 281, 264, 24624, 13], "temperature": 0.0, "avg_logprob": -0.13882689042524857, "compression_ratio": 1.6425339366515836, "no_speech_prob": 2.260285327793099e-06}, {"id": 1044, "seek": 560900, "start": 5620.0, "end": 5625.0, "text": " But yeah, exact idea of kind of, yeah, like you've got to do this huge exponent.", "tokens": [583, 1338, 11, 1900, 1558, 295, 733, 295, 11, 1338, 11, 411, 291, 600, 658, 281, 360, 341, 2603, 37871, 13], "temperature": 0.0, "avg_logprob": -0.13882689042524857, "compression_ratio": 1.6425339366515836, "no_speech_prob": 2.260285327793099e-06}, {"id": 1045, "seek": 560900, "start": 5625.0, "end": 5628.0, "text": " And so then I can't keep track of this like little plus one.", "tokens": [400, 370, 550, 286, 393, 380, 1066, 2837, 295, 341, 411, 707, 1804, 472, 13], "temperature": 0.0, "avg_logprob": -0.13882689042524857, "compression_ratio": 1.6425339366515836, "no_speech_prob": 2.260285327793099e-06}, {"id": 1046, "seek": 560900, "start": 5628.0, "end": 5632.0, "text": " It kind of overflows. And so we're getting getting the wrong answer.", "tokens": [467, 733, 295, 670, 33229, 13, 400, 370, 321, 434, 1242, 1242, 264, 2085, 1867, 13], "temperature": 0.0, "avg_logprob": -0.13882689042524857, "compression_ratio": 1.6425339366515836, "no_speech_prob": 2.260285327793099e-06}, {"id": 1047, "seek": 563200, "start": 5632.0, "end": 5644.0, "text": " And so this is a this case where Gaussian elimination or LU factorization, even with partial pivoting fails.", "tokens": [400, 370, 341, 307, 257, 341, 1389, 689, 39148, 29224, 420, 31851, 5952, 2144, 11, 754, 365, 14641, 14538, 278, 18199, 13], "temperature": 0.0, "avg_logprob": -0.12722975632240033, "compression_ratio": 1.5794392523364487, "no_speech_prob": 4.8602919378026854e-06}, {"id": 1048, "seek": 563200, "start": 5644.0, "end": 5655.0, "text": " And I think there, yes, there, if we had had complete pivoting and we're pivoting on our columns or, you know, permuting our columns in addition to our rows,", "tokens": [400, 286, 519, 456, 11, 2086, 11, 456, 11, 498, 321, 632, 632, 3566, 14538, 278, 293, 321, 434, 14538, 278, 322, 527, 13766, 420, 11, 291, 458, 11, 4784, 10861, 527, 13766, 294, 4500, 281, 527, 13241, 11], "temperature": 0.0, "avg_logprob": -0.12722975632240033, "compression_ratio": 1.5794392523364487, "no_speech_prob": 4.8602919378026854e-06}, {"id": 1049, "seek": 563200, "start": 5655.0, "end": 5660.0, "text": " we wouldn't get this giant number that had added up to two to the 60th.", "tokens": [321, 2759, 380, 483, 341, 7410, 1230, 300, 632, 3869, 493, 281, 732, 281, 264, 4060, 392, 13], "temperature": 0.0, "avg_logprob": -0.12722975632240033, "compression_ratio": 1.5794392523364487, "no_speech_prob": 4.8602919378026854e-06}, {"id": 1050, "seek": 566000, "start": 5660.0, "end": 5670.0, "text": " And so that would have solved this. But since this case basically never arises and complete pivoting is so slow, we don't use it in practice.", "tokens": [400, 370, 300, 576, 362, 13041, 341, 13, 583, 1670, 341, 1389, 1936, 1128, 27388, 293, 3566, 14538, 278, 307, 370, 2964, 11, 321, 500, 380, 764, 309, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.07438105583190918, "compression_ratio": 1.404109589041096, "no_speech_prob": 3.3404905934730778e-06}, {"id": 1051, "seek": 566000, "start": 5670.0, "end": 5680.0, "text": " And here this is kind of saying that in math more formally that", "tokens": [400, 510, 341, 307, 733, 295, 1566, 300, 294, 5221, 544, 25983, 300], "temperature": 0.0, "avg_logprob": -0.07438105583190918, "compression_ratio": 1.404109589041096, "no_speech_prob": 3.3404905934730778e-06}, {"id": 1052, "seek": 568000, "start": 5680.0, "end": 5691.0, "text": " it has this idea of a growth factor row, which is the maximum U value over the maximum A value. And that in", "tokens": [309, 575, 341, 1558, 295, 257, 4599, 5952, 5386, 11, 597, 307, 264, 6674, 624, 2158, 670, 264, 6674, 316, 2158, 13, 400, 300, 294], "temperature": 0.0, "avg_logprob": -0.11319849640130997, "compression_ratio": 1.4935064935064934, "no_speech_prob": 3.3404635360057e-06}, {"id": 1053, "seek": 568000, "start": 5691.0, "end": 5700.0, "text": " that times machine epsilon is part of the", "tokens": [300, 1413, 3479, 17889, 307, 644, 295, 264], "temperature": 0.0, "avg_logprob": -0.11319849640130997, "compression_ratio": 1.4935064935064934, "no_speech_prob": 3.3404635360057e-06}, {"id": 1054, "seek": 568000, "start": 5700.0, "end": 5706.0, "text": " kind of error that you can see. And so, yeah, it's a bad thing when row is huge.", "tokens": [733, 295, 6713, 300, 291, 393, 536, 13, 400, 370, 11, 1338, 11, 309, 311, 257, 1578, 551, 562, 5386, 307, 2603, 13], "temperature": 0.0, "avg_logprob": -0.11319849640130997, "compression_ratio": 1.4935064935064934, "no_speech_prob": 3.3404635360057e-06}, {"id": 1055, "seek": 570600, "start": 5706.0, "end": 5720.0, "text": " But, yeah, to get back to Tim's earlier question, question complete pivoting would have solved that because we would have been making those big numbers while we were pivoting on.", "tokens": [583, 11, 1338, 11, 281, 483, 646, 281, 7172, 311, 3071, 1168, 11, 1168, 3566, 14538, 278, 576, 362, 13041, 300, 570, 321, 576, 362, 668, 1455, 729, 955, 3547, 1339, 321, 645, 14538, 278, 322, 13], "temperature": 0.0, "avg_logprob": -0.16562650142571864, "compression_ratio": 1.5845410628019323, "no_speech_prob": 3.4465053886378882e-06}, {"id": 1056, "seek": 570600, "start": 5720.0, "end": 5732.0, "text": " And so LU, many numerical linear algebra classes kind of teach LU first, although Trevithin kind of points out that it's not it's not a typical case,", "tokens": [400, 370, 31851, 11, 867, 29054, 8213, 21989, 5359, 733, 295, 2924, 31851, 700, 11, 4878, 8648, 85, 355, 259, 733, 295, 2793, 484, 300, 309, 311, 406, 309, 311, 406, 257, 7476, 1389, 11], "temperature": 0.0, "avg_logprob": -0.16562650142571864, "compression_ratio": 1.5845410628019323, "no_speech_prob": 3.4465053886378882e-06}, {"id": 1057, "seek": 573200, "start": 5732.0, "end": 5742.0, "text": " even though it's, you know, very widely used algorithm. But this idea that", "tokens": [754, 1673, 309, 311, 11, 291, 458, 11, 588, 13371, 1143, 9284, 13, 583, 341, 1558, 300], "temperature": 0.0, "avg_logprob": -0.09378261566162109, "compression_ratio": 1.4598214285714286, "no_speech_prob": 6.04875913268188e-06}, {"id": 1058, "seek": 573200, "start": 5742.0, "end": 5750.0, "text": " kind of Gaussian elimination with partial pivoting, so this is a quote from Trevithin, is utterly stable in practice in 50 years of computing.", "tokens": [733, 295, 39148, 29224, 365, 14641, 14538, 278, 11, 370, 341, 307, 257, 6513, 490, 8648, 85, 355, 259, 11, 307, 30251, 8351, 294, 3124, 294, 2625, 924, 295, 15866, 13], "temperature": 0.0, "avg_logprob": -0.09378261566162109, "compression_ratio": 1.4598214285714286, "no_speech_prob": 6.04875913268188e-06}, {"id": 1059, "seek": 573200, "start": 5750.0, "end": 5758.0, "text": " No matrix problems that excite an explosive instability are known to have arisen under natural circumstances,", "tokens": [883, 8141, 2740, 300, 1624, 642, 364, 24630, 34379, 366, 2570, 281, 362, 594, 11106, 833, 3303, 9121, 11], "temperature": 0.0, "avg_logprob": -0.09378261566162109, "compression_ratio": 1.4598214285714286, "no_speech_prob": 6.04875913268188e-06}, {"id": 1060, "seek": 575800, "start": 5758.0, "end": 5765.0, "text": " even though, you know, we saw one today that was very contrived.", "tokens": [754, 1673, 11, 291, 458, 11, 321, 1866, 472, 965, 300, 390, 588, 660, 470, 937, 13], "temperature": 0.0, "avg_logprob": -0.14180909670316255, "compression_ratio": 1.5274725274725274, "no_speech_prob": 4.7107973841775674e-06}, {"id": 1061, "seek": 575800, "start": 5765.0, "end": 5772.0, "text": " So, yeah, I think it is kind of interesting. And it's also something that it's", "tokens": [407, 11, 1338, 11, 286, 519, 309, 307, 733, 295, 1880, 13, 400, 309, 311, 611, 746, 300, 309, 311], "temperature": 0.0, "avg_logprob": -0.14180909670316255, "compression_ratio": 1.5274725274725274, "no_speech_prob": 4.7107973841775674e-06}, {"id": 1062, "seek": 575800, "start": 5772.0, "end": 5785.0, "text": " the definition for stability. It's OK that we have row in there, but however, you know, if row is huge, then this kind of breaks down.", "tokens": [264, 7123, 337, 11826, 13, 467, 311, 2264, 300, 321, 362, 5386, 294, 456, 11, 457, 4461, 11, 291, 458, 11, 498, 5386, 307, 2603, 11, 550, 341, 733, 295, 9857, 760, 13], "temperature": 0.0, "avg_logprob": -0.14180909670316255, "compression_ratio": 1.5274725274725274, "no_speech_prob": 4.7107973841775674e-06}, {"id": 1063, "seek": 578500, "start": 5785.0, "end": 5797.0, "text": " Any questions about this?", "tokens": [2639, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.258977391503074, "compression_ratio": 1.184873949579832, "no_speech_prob": 4.984790211892687e-05}, {"id": 1064, "seek": 578500, "start": 5797.0, "end": 5808.0, "text": " So actually, you say that when we have n equals 60, this lin-alq.lu-solve doesn't give us the proper answer, right?", "tokens": [407, 767, 11, 291, 584, 300, 562, 321, 362, 297, 6915, 4060, 11, 341, 22896, 12, 304, 80, 13, 2781, 12, 30926, 303, 1177, 380, 976, 505, 264, 2296, 1867, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.258977391503074, "compression_ratio": 1.184873949579832, "no_speech_prob": 4.984790211892687e-05}, {"id": 1065, "seek": 580800, "start": 5808.0, "end": 5816.0, "text": " Yeah, but if we don't do this LU factor and we just try to solve it on matrix and vector, it still gives the proper answer.", "tokens": [865, 11, 457, 498, 321, 500, 380, 360, 341, 31851, 5952, 293, 321, 445, 853, 281, 5039, 309, 322, 8141, 293, 8062, 11, 309, 920, 2709, 264, 2296, 1867, 13], "temperature": 0.0, "avg_logprob": -0.12543034186730018, "compression_ratio": 1.56, "no_speech_prob": 2.9765398721792735e-05}, {"id": 1066, "seek": 580800, "start": 5816.0, "end": 5821.0, "text": " So why would we care about this LU factorization?", "tokens": [407, 983, 576, 321, 1127, 466, 341, 31851, 5952, 2144, 30], "temperature": 0.0, "avg_logprob": -0.12543034186730018, "compression_ratio": 1.56, "no_speech_prob": 2.9765398721792735e-05}, {"id": 1067, "seek": 580800, "start": 5821.0, "end": 5825.0, "text": " Sorry, if you just try to solve it, how it gives the proper?", "tokens": [4919, 11, 498, 291, 445, 853, 281, 5039, 309, 11, 577, 309, 2709, 264, 2296, 30], "temperature": 0.0, "avg_logprob": -0.12543034186730018, "compression_ratio": 1.56, "no_speech_prob": 2.9765398721792735e-05}, {"id": 1068, "seek": 582500, "start": 5825.0, "end": 5838.0, "text": " Yes, so if we just do like lin-alq.solve without LU and we don't apply this LU factorization, it gives us the proper answer.", "tokens": [1079, 11, 370, 498, 321, 445, 360, 411, 22896, 12, 304, 80, 13, 30926, 303, 1553, 31851, 293, 321, 500, 380, 3079, 341, 31851, 5952, 2144, 11, 309, 2709, 505, 264, 2296, 1867, 13], "temperature": 0.0, "avg_logprob": -0.13006214235649735, "compression_ratio": 1.412162162162162, "no_speech_prob": 4.8316815082216635e-05}, {"id": 1069, "seek": 582500, "start": 5838.0, "end": 5839.0, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.13006214235649735, "compression_ratio": 1.412162162162162, "no_speech_prob": 4.8316815082216635e-05}, {"id": 1070, "seek": 582500, "start": 5839.0, "end": 5850.0, "text": " So it looks like that this factorization just doesn't agree.", "tokens": [407, 309, 1542, 411, 300, 341, 5952, 2144, 445, 1177, 380, 3986, 13], "temperature": 0.0, "avg_logprob": -0.13006214235649735, "compression_ratio": 1.412162162162162, "no_speech_prob": 4.8316815082216635e-05}, {"id": 1071, "seek": 585000, "start": 5850.0, "end": 5862.0, "text": " Oh, wait, did someone else? Tim? Oh, no. OK.", "tokens": [876, 11, 1699, 11, 630, 1580, 1646, 30, 7172, 30, 876, 11, 572, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.2791992723941803, "compression_ratio": 0.9753086419753086, "no_speech_prob": 6.707821739837527e-05}, {"id": 1072, "seek": 585000, "start": 5862.0, "end": 5870.0, "text": " Oh, yes, that's a good suggestion.", "tokens": [876, 11, 2086, 11, 300, 311, 257, 665, 16541, 13], "temperature": 0.0, "avg_logprob": -0.2791992723941803, "compression_ratio": 0.9753086419753086, "no_speech_prob": 6.707821739837527e-05}, {"id": 1073, "seek": 587000, "start": 5870.0, "end": 5885.0, "text": " Sorry, I didn't take this out.", "tokens": [4919, 11, 286, 994, 380, 747, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.28787656930776745, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.0002029937895713374}, {"id": 1074, "seek": 588500, "start": 5885.0, "end": 5906.0, "text": " This is something.", "tokens": [639, 307, 746, 13], "temperature": 0.0, "avg_logprob": -0.3875752389431, "compression_ratio": 0.75, "no_speech_prob": 2.9012760478508426e-06}, {"id": 1075, "seek": 590600, "start": 5906.0, "end": 5917.0, "text": " This is a good question.", "tokens": [639, 307, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1129601240158081, "compression_ratio": 1.16, "no_speech_prob": 2.058014388239826e-06}, {"id": 1076, "seek": 590600, "start": 5917.0, "end": 5919.0, "text": " I will answer this next time.", "tokens": [286, 486, 1867, 341, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.1129601240158081, "compression_ratio": 1.16, "no_speech_prob": 2.058014388239826e-06}, {"id": 1077, "seek": 590600, "start": 5919.0, "end": 5931.0, "text": " I want to look into more what scipy lin-alq.solve is calling.", "tokens": [286, 528, 281, 574, 666, 544, 437, 2180, 8200, 22896, 12, 304, 80, 13, 30926, 303, 307, 5141, 13], "temperature": 0.0, "avg_logprob": -0.1129601240158081, "compression_ratio": 1.16, "no_speech_prob": 2.058014388239826e-06}, {"id": 1078, "seek": 593100, "start": 5931.0, "end": 5937.0, "text": " You can see from the docs at the end it says it's calling LAPAC, which Rachel mentioned earlier is a particular library.", "tokens": [509, 393, 536, 490, 264, 45623, 412, 264, 917, 309, 1619, 309, 311, 5141, 441, 4715, 4378, 11, 597, 14246, 2835, 3071, 307, 257, 1729, 6405, 13], "temperature": 0.0, "avg_logprob": -0.19646696774464734, "compression_ratio": 1.524, "no_speech_prob": 5.7381668739253655e-05}, {"id": 1079, "seek": 593100, "start": 5937.0, "end": 5941.0, "text": " I mean it also says that it's called, oh, those are just for the Hermitian symmetric.", "tokens": [286, 914, 309, 611, 1619, 300, 309, 311, 1219, 11, 1954, 11, 729, 366, 445, 337, 264, 21842, 270, 952, 32330, 13], "temperature": 0.0, "avg_logprob": -0.19646696774464734, "compression_ratio": 1.524, "no_speech_prob": 5.7381668739253655e-05}, {"id": 1080, "seek": 593100, "start": 5941.0, "end": 5942.0, "text": " No, the person is generic.", "tokens": [883, 11, 264, 954, 307, 19577, 13], "temperature": 0.0, "avg_logprob": -0.19646696774464734, "compression_ratio": 1.524, "no_speech_prob": 5.7381668739253655e-05}, {"id": 1081, "seek": 593100, "start": 5942.0, "end": 5948.0, "text": " Generic, OK. So let me check that.", "tokens": [15409, 299, 11, 2264, 13, 407, 718, 385, 1520, 300, 13], "temperature": 0.0, "avg_logprob": -0.19646696774464734, "compression_ratio": 1.524, "no_speech_prob": 5.7381668739253655e-05}, {"id": 1082, "seek": 593100, "start": 5948.0, "end": 5958.0, "text": " Interestingly, according to the LAPAC docs, that does use LU factorization, but it must be doing it in some way.", "tokens": [30564, 11, 4650, 281, 264, 441, 4715, 4378, 45623, 11, 300, 775, 764, 31851, 5952, 2144, 11, 457, 309, 1633, 312, 884, 309, 294, 512, 636, 13], "temperature": 0.0, "avg_logprob": -0.19646696774464734, "compression_ratio": 1.524, "no_speech_prob": 5.7381668739253655e-05}, {"id": 1083, "seek": 595800, "start": 5958.0, "end": 5967.0, "text": " So this is how you would look up LAPAC, the name of a particular method.", "tokens": [407, 341, 307, 577, 291, 576, 574, 493, 441, 4715, 4378, 11, 264, 1315, 295, 257, 1729, 3170, 13], "temperature": 0.0, "avg_logprob": -0.11620160511561803, "compression_ratio": 1.4272727272727272, "no_speech_prob": 2.840745037246961e-05}, {"id": 1084, "seek": 595800, "start": 5967.0, "end": 5970.0, "text": " I want to look into that more because I would have guessed that they were using LU solver.", "tokens": [286, 528, 281, 574, 666, 300, 544, 570, 286, 576, 362, 21852, 300, 436, 645, 1228, 31851, 1404, 331, 13], "temperature": 0.0, "avg_logprob": -0.11620160511561803, "compression_ratio": 1.4272727272727272, "no_speech_prob": 2.840745037246961e-05}, {"id": 1085, "seek": 595800, "start": 5970.0, "end": 5978.0, "text": " Actually, can you try it just real quick with a higher power, like do like n equals 80 or something?", "tokens": [5135, 11, 393, 291, 853, 309, 445, 957, 1702, 365, 257, 2946, 1347, 11, 411, 360, 411, 297, 6915, 4688, 420, 746, 30], "temperature": 0.0, "avg_logprob": -0.11620160511561803, "compression_ratio": 1.4272727272727272, "no_speech_prob": 2.840745037246961e-05}, {"id": 1086, "seek": 595800, "start": 5978.0, "end": 5982.0, "text": " Oh, it also factorizes into a permutation matrix.", "tokens": [876, 11, 309, 611, 5952, 5660, 666, 257, 4784, 11380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11620160511561803, "compression_ratio": 1.4272727272727272, "no_speech_prob": 2.840745037246961e-05}, {"id": 1087, "seek": 598200, "start": 5982.0, "end": 5989.0, "text": " Yeah, but typically that's just a partial permutation matrix.", "tokens": [865, 11, 457, 5850, 300, 311, 445, 257, 14641, 4784, 11380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2801339328289032, "compression_ratio": 1.3866666666666667, "no_speech_prob": 1.2218605661473703e-05}, {"id": 1088, "seek": 598200, "start": 5989.0, "end": 5992.0, "text": " Oh, it also kind of like the condition number.", "tokens": [876, 11, 309, 611, 733, 295, 411, 264, 4188, 1230, 13], "temperature": 0.0, "avg_logprob": -0.2801339328289032, "compression_ratio": 1.3866666666666667, "no_speech_prob": 1.2218605661473703e-05}, {"id": 1089, "seek": 598200, "start": 5992.0, "end": 5994.0, "text": " Oh.", "tokens": [876, 13], "temperature": 0.0, "avg_logprob": -0.2801339328289032, "compression_ratio": 1.3866666666666667, "no_speech_prob": 1.2218605661473703e-05}, {"id": 1090, "seek": 598200, "start": 5994.0, "end": 5998.0, "text": " So it confuses it with error bounds and it uses iterative refinement.", "tokens": [407, 309, 1497, 8355, 309, 365, 6713, 29905, 293, 309, 4960, 17138, 1166, 1895, 30229, 13], "temperature": 0.0, "avg_logprob": -0.2801339328289032, "compression_ratio": 1.3866666666666667, "no_speech_prob": 1.2218605661473703e-05}, {"id": 1091, "seek": 598200, "start": 5998.0, "end": 6000.0, "text": " So it does more than.", "tokens": [407, 309, 775, 544, 813, 13], "temperature": 0.0, "avg_logprob": -0.2801339328289032, "compression_ratio": 1.3866666666666667, "no_speech_prob": 1.2218605661473703e-05}, {"id": 1092, "seek": 598200, "start": 6000.0, "end": 6002.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.2801339328289032, "compression_ratio": 1.3866666666666667, "no_speech_prob": 1.2218605661473703e-05}, {"id": 1093, "seek": 600200, "start": 6002.0, "end": 6012.0, "text": " Yeah, because I'm I feel very certain that it's not doing complete pivoting because that's really slow.", "tokens": [865, 11, 570, 286, 478, 286, 841, 588, 1629, 300, 309, 311, 406, 884, 3566, 14538, 278, 570, 300, 311, 534, 2964, 13], "temperature": 0.0, "avg_logprob": -0.1033991707695855, "compression_ratio": 1.4870466321243523, "no_speech_prob": 2.9021973659837386e-06}, {"id": 1094, "seek": 600200, "start": 6012.0, "end": 6015.0, "text": " OK. Yeah. So this is doing something much fancier.", "tokens": [2264, 13, 865, 13, 407, 341, 307, 884, 746, 709, 3429, 27674, 13], "temperature": 0.0, "avg_logprob": -0.1033991707695855, "compression_ratio": 1.4870466321243523, "no_speech_prob": 2.9021973659837386e-06}, {"id": 1095, "seek": 600200, "start": 6015.0, "end": 6021.0, "text": " Iterative refinement is applied to improve the computed solution matrix.", "tokens": [286, 391, 1166, 1895, 30229, 307, 6456, 281, 3470, 264, 40610, 3827, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1033991707695855, "compression_ratio": 1.4870466321243523, "no_speech_prob": 2.9021973659837386e-06}, {"id": 1096, "seek": 600200, "start": 6021.0, "end": 6026.0, "text": " That's a really interesting find that that is working here.", "tokens": [663, 311, 257, 534, 1880, 915, 300, 300, 307, 1364, 510, 13], "temperature": 0.0, "avg_logprob": -0.1033991707695855, "compression_ratio": 1.4870466321243523, "no_speech_prob": 2.9021973659837386e-06}, {"id": 1097, "seek": 602600, "start": 6026.0, "end": 6036.0, "text": " And this is often the advantage of using LAPAC, something that's been really, really well optimized.", "tokens": [400, 341, 307, 2049, 264, 5002, 295, 1228, 441, 4715, 4378, 11, 746, 300, 311, 668, 534, 11, 534, 731, 26941, 13], "temperature": 0.0, "avg_logprob": -0.0854496123298766, "compression_ratio": 1.3892215568862276, "no_speech_prob": 2.190728082496207e-06}, {"id": 1098, "seek": 602600, "start": 6036.0, "end": 6041.0, "text": " So we're at time, but I'll return to this this question next time.", "tokens": [407, 321, 434, 412, 565, 11, 457, 286, 603, 2736, 281, 341, 341, 1168, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.0854496123298766, "compression_ratio": 1.3892215568862276, "no_speech_prob": 2.190728082496207e-06}, {"id": 1099, "seek": 602600, "start": 6041.0, "end": 6050.0, "text": " And I think that's otherwise most of what I had to say about LU.", "tokens": [400, 286, 519, 300, 311, 5911, 881, 295, 437, 286, 632, 281, 584, 466, 31851, 13], "temperature": 0.0, "avg_logprob": -0.0854496123298766, "compression_ratio": 1.3892215568862276, "no_speech_prob": 2.190728082496207e-06}, {"id": 1100, "seek": 605000, "start": 6050.0, "end": 6058.0, "text": " Yeah, LU factorization. And this is covered in chapters 20 to 22 of Trevathan.", "tokens": [50364, 865, 11, 31851, 5952, 2144, 13, 400, 341, 307, 5343, 294, 20013, 945, 281, 5853, 295, 8648, 85, 9390, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20301671650098718, "compression_ratio": 0.975, "no_speech_prob": 2.011761171161197e-05}], "language": "en"}