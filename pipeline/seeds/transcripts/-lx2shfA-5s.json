{"text": " Welcome. The penultimate lesson. So who has heard of the Google Brain Residency Program? It's pretty famous, right? I think it's probably maybe the top program to get into, the hardest deep learning program to get into the world. The reason I mention it is because it so happens one of our students was just accepted. So I'd like everybody to congratulate Sarah Hawker. Sarah Hawker is a new Google Brain resident and I thought maybe we could have a brief chat with her to find out about her path to this program. Sarah Hawker is a new Google Brain resident and I thought maybe we could have a brief chat with her to find out about her path to this program. I'll throw over the green box, try not to hurt anybody. Hello, congratulations, well done, you must be pretty excited. Very excited. It's almost a blur. I found out last week actually. Yeah. So this is a pretty huge thing to get into. So you must be like a machine learning, PhD, been coding for a few decades, so forth, yes? So actually not at all. And I think that's the exciting thing about the whole process is that really they're trying to find a diverse set of fellows, residents for the year. So there's many different buckets. There were definitely PhDs that I sat next to in the interview room who were well published and incredibly accomplished. There were also undergraduates who this would be their first job after college. And my background was that I've been in industry, I started out in economic consulting, so I did economic modeling and statistics and now I work as a data scientist. So how long have you been coding for? So I've been coding two years. Wow. And also makes me very atypical in this group. Yeah, yeah. Rachel told me you had a fan moment when Ian Goodfellow interviewed you, is that true? So I tried to keep it undercover, but it was so thrilling. I think that was the strangest thing about the whole process was that you're so nervous the whole time and you're so excited, but also you're talking to these people whose research you followed for years. And so you're just, my Google, the first hangout interview was actually with Hugo Larachelle and I started learning about neural nets, watching his MOOCs. So it was just, it feels surreal. And I think that's part of the intent of the program is to take very, very different people at different parts of their career. So you told me that before you started this course, your awareness of neural nets was a little bit of theory from a few blog posts. I mean, so how much is going through this course kind of helped you get the skills you needed to get into this program? So I wasn't coding or implementing architecture before this course. So I'd done, to be fair, I was fascinated by deep learning for a while. So I was pretty deep in the theory. But I think part of why it was blog posts was that there wasn't a coherent body of work on the subject. And so now- But now that's Ian's book. Now that's Ian's book, which I have read back to back. I mean, it's really a fantastic treatment. But I think that what this course offers, and I sense like many of you may have had the same experience, is the implementation, which was very new to me and definitely something I don't think I would have found in a different forum. Yeah. I mean, I know from talking to Rachel that some of the interview process was like- it actually sounded, the question sounded like it came straight out of the course. It was like about how to do transfer learning and all this kind of practical application stuff it seemed like. Yeah. At least in my case, it was a fairly consistent breakdown throughout the process of theory, math questions, and then project experience. And with the project, they're really trying to gauge implementation and do a knowledge of basically concepts that we've covered very thoroughly in this class. So overfitting, but overfitting distribution, understanding how to address data distribution differences. So that part, I think that that was standard throughout the whole process, was that they expect you to- they expect their candidates to have this holistic approach of both coding, but also a strong foundation in the underpinnings. So if I could give a pitch for your program, one of the amazing things that Sarah's done in her extraordinary life is to create a organization called Delta Analytics, which is applying data science for social impact projects. Is there room for people who might be interested to contact you if they're interested in doing that kind of work? Absolutely. I would honestly, I think that the caliber of this course, I would be so thrilled to work with anyone from this course going forward. So the way that it works, I'll just- we pair with nonprofits all over the world, and you work with engineers and data scientists over a six-month project. And right now, we are involved with eight different nonprofits, and then we'll start a new cycle towards the end of this year. Congratulations again, I think we're all so proud of you. Talking of great work, I also wanted to mention the work of another student who also has the great challenge of having to deal with being a fast AI intern, Brad. Brad took up the challenge I set up two weeks ago in implementing cyclical learning regs. You might remember that the cyclical learning regs paper showed faster training of neural nets and also more automated. Brad actually had it coded up super quickly. So if you jump on the forum, you'll find a link to it there. Maybe Brad you can add it to our Wiki thread. Do we have a Wiki thread yet, Rachel? Could you create one? You just click at the bottom and say make wiki. Honestly this, as I mentioned when we first taught this, this paper has been so little looked at, we don't yet really know. I have a feeling it's going to turn out that this is the best way to train every kind of neural net in every kind of situation. So I've asked Brad to try as many possible experiments here and he's going to try and keep automating it. This is exactly the kind of thing Rachel and I at Fast AI are trying to do if this really works out, is get rid of the whole question of how to set learning regs, which currently is such an artisanal thing. So congratulations Brad on getting that working. It worked really well in Keras I think with the callbacks they had. The code ends up being quite neat as well. I had this comment last time, but I'm so excited about these callbacks in Keras. You can set callbacks to stop training if it stops improving, and then you can enter another cycle to do different things and then kind of oscillate between different ways with the callbacks and fully automate this whole process. So I kind of implemented similar zigzagging things that you showed me. So Keras is really cool in that. Basically 10 lines of code will bring you so far with these callbacks. Remember that Keras has great source code. So if you look in your Anaconda, lib, Python, sitepackages, Keras directory, you'll find all the current callbacks. So when I said to Brad some tips as to how to get started, I said go here, look at the existing callbacks and see if that helps you get started. Any time you want to build something in Keras, one of the best ways to get started is to read the source code for something that it already has that's somewhat similar to what you want to build. And certainly with callbacks, that's an easy way to do that. It's been a big week in deep learning as usual. Everything I taught you in this course is now officially out of date. Hopefully though, one of the things I have noticed is it's all building on stuff that we've been learning about. I have to begin to Brad for drawing my attention to this paper, which is a new style transfer paper which can transfer to any style in real time so you don't have to build a separate network for each one. This is the kind of thing which you could absolutely turn into an app. And obviously no one's done it yet because this paper just came out, so you could be the first one to say here's an app which can create any photo into any style you like. And honestly I think this third column is the new paper. In my opinion, if you compare it to the gold standard, which is the Gatties paper we originally looked at, I actually think it's maybe better. This one looks more style-y. So there's an interesting paper. A lot of the basic ideas are the same, but as you'll see it's got some interesting approaches. GANs have had a big step forward. This is the previous state-of-the-art and first generation. This is the new. As you can see, these are now pretty much at photo-realistic, at least at 128x128. These were only 64x64, the old state-of-the-art. So this is a pretty exciting step forward in GANs. As we've talked about, people use GANs at the moment to create pictures, but they can be used as an additional loss function for any kind of generative network. So I think one of the things I always look for is what's underappreciated, what's not being used much at the moment. And I would say GANs for a wider range of generative models. If you want to create a really great super resolution thing, or if you want to create a really great automatic line drawing creator, or colorization system, or whatever, I think GANs are a good approach. Perhaps the most amazing one is this paper, which again we'll add on to the wiki, which does a bidirectional transfer even without matching pairs. So for example, clearly this would not be possible to do with you requiring matching pairs. Take a Monet and turn it into a photo. We don't have any data on how to do that because we don't have any photos of Monet, but as you can see it works incredibly well. Turn a zebra into a horse or a horse into a zebra, summer into winter, winter into summer. Again, this is a GAN-based system creating photorealistic images or very impressive artworks. I think this approach to style transfer by putting a GAN layer is perhaps more interesting than the style transfer we've come up with so far because it really has to create a painting that can't be recognized as not being a real painting, otherwise the GAN will fail. So I think this is a really interesting approach to style transfer as well. Really interesting week. These are all papers which I think any of you guys can tackle right now because they're all built on the things we've learned in this course. If anybody's interested in tackling anything this week, that would be super fun to talk about it on the forum. Question from the audience. Just to clarify, is this last paper also the GAN from the prior slide or is this a different paper? This is a different paper. I'm sorry, I'm embarrassed that I didn't actually write on it where it comes from. Rather than me doing this, maybe somebody can try and find it. I think I showed Brad and Rachel have already seen it. I'll find it on my Twitter and put it on the Wiki. We've been talking about Minshift clustering a bit from time to time, so we'll talk about it more next week in terms of applications. But the main application we've talked about so far is using it for a kind of faster preprocessing of really large data items like CT scans in order to find objects of interest, in this case, lung nodules, at microcancers. And one of the things I mentioned that I was interested in was experimenting with combining approximate nearest neighbors with Minshift clustering. So to remind you, the basic idea was that we went through each mini-batch and with each mini-batch we then basically did a distance from every element in that mini-batch through every single data item. And then we took the weighted sum of all the data items weighted by the Gaussian on those weights. And I pointed out that the vast majority of data points are far enough away that their weights are so close to 0 that we could probably ignore them. So maybe we could have tried putting an approximate nearest neighbors step beforehand, so rather than adding up the entire data set, which could be a million points, it could just be the nearest 100 neighbors. So I actually tried that during the week. I just used the existing scikit-learn.neighbors algorithm, so I haven't written my own PyTorch version. I know one of you guys on the forum has already started writing a PyTorch version. So I just tried a ball tree, which is a particular kind of approximate nearest neighbors. And the basic idea is you say, okay, here's the data that I want you to index so that I can rapidly do nearest neighbors. So now please do a query looking at the first 10 data points. And for each one of those 10 data points, show me the 3 nearest neighbors. And that returns something like this. Here are the 10 data points you passed in, and here are the 3 approximate nearest neighbors. And of course all of the time itself is its own nearest neighbor, so that's why the first one is always kind of boring. So then I thought, okay, that looks good. So then I just put that into the loop of our mean shift cluster. So I said, okay, each time we go through another epoch, index the data we have so far, so chuck it into one of these ball trees. And then when we do our distances, don't do them to every point, but instead do it into the nearest points. So I did a query here to find the 50 nearest neighbors. This then puts that into GPU. I'm turning it into a tensor and dot-cootering it. So that gives me the list of the indexes, not the data itself. So interestingly, the hardest step was the one that seems like it should be the easiest, which is I had to then look up each of these indexes into the data to return the actual points. Easy enough to do if you just look through it one step at a time, but we're trying to use the GPU here. If I tried doing it one step at a time, it made the thing take forever because GPUs are not quick at doing things one step at a time. So I created a little batch-wise indexing function, which in the end Rachel and I realized we could do actually with very, very little code. This is basically something where we could pass in our array and pass in our matrix of indexes and it would return a 3-dimensional tensor basically of every one of those points for every one of those nearest neighbors for every one of those dimensions. So that was the only bit that was remotely tricky. Other than that, the rest of the code is the same. So this worked in the sense that it sped things up, but it didn't work in the sense of the result. Here was the input we gave it, and here's the output. Now what's happened is it turns out each one of these little colored dots actually now represents 50 points. So what it's done is it's taken me too literally. It's basically found the 50 nearest neighbors to each point and created a cluster of 50 points. And once it's created these clusters of 50 points, it's now stuck. There's no way for it to cluster these any further because every time it now goes nearest neighbors, it says, oh, there's 50 points that are right on top of you. So I mentioned this to basically show the kinds of things that can go wrong, and hopefully by next week this will be fixed. It's like when I described this problem, this seemed like it was clearly going to work, and then as soon as I saw this picture, I immediately saw that it couldn't possibly work. So this is kind of the nature of trying things out. But the interesting thing is I now realize that the solution to this will be way faster than this. The solution is what Rachel describes as an approximate nearest neighbors, which is something that doesn't return the 50 probably nearest points, but returns 50 points which probabilistically closer ones are more likely to be there. But there are like zero guarantees. We want it to be less good. So if you're interested during the week, if one of you wants to try creating an approximate nearest neighbors algorithm, I think it will basically be a case of taking like LSH or Boltree or something and removing a lot of the code, removing the code that makes it better. So hopefully next week we'll see this working. But I thought it would be interesting to see the intermediate stage. And actually, this kind of thing of showing you the failures, you don't get to see most of the failures. So I was working with some of the students over the weekend on implementing something that we'll see later on today. And actually on Saturday afternoon I was sitting with Melissa going through some of this coding. And at the end of it Melissa was like, Oh wow, it's really interesting to see the process. Because you're coming on Monday and it's all just working. Where else would we spend the entire Saturday afternoon slowly going through one step at a time and constantly making mistakes and going back and trying to see what happened. So the actual process of everything you see in class, getting it working, is full of failures. In fact, Brad is currently working on building one of the two things we're doing for next week. He just came up to me before class and he was like, Yeah, it's finished running. And nothing worked at all. And I was like, Yeah, of course. Nothing ever works the first time. So part of the process is something I've been trying to talk to Brad about with his work. Recognizing that every time you build something new, it won't work means that you need to build it in a very different way. Write an IPython notebook as if you're going to be teaching it that next Monday's class. Because it won't work the first time, you're then going to have to go back and be like, Which one of these steps failed? So at every step, you want to be printing out the results, summarizing the key statistics, drawing pictures, writing down the reasoning that you did things, so that then when it doesn't work, you can go back and be like, Okay, where did this go wrong? Or even better, hopefully seeing the mistake earlier on rather than waiting until it's all finished. So there's a whole lot of data science process steps which those of you who have worked with data scientists will be pretty familiar with by now, hopefully, and those of you who haven't. It's really a case of bringing in the software engineering mindset. Lots of testing, lots of iterations. The more data scientists can learn about software engineering practices, I think, the better. So very interestingly, today or maybe yesterday, Facebook just announced that they have implemented an enormous improvement in the state of the art in approximate nearest neighbors. So you can check this out if you like, faiss. By the way, everything I mention here, I've mentioned earlier on Twitter. So if you follow me on Twitter or keep an eye on my Twitter account, you'll see all these things first, regardless of whether class is still running or not. So this was particularly interesting to me, a. because I've been talking quite a bit during this part of the course about approximate nearest neighbor. It's actually really important for deep learning. You can get a sense of how important it is by how much Facebook has invested in this. This is a multi-GPU distributed approximate nearest neighbor system that runs about 10 times faster than the previous state of the art. Now I happened to get a particular insight into this because earlier this week I was at a conference at Berkeley, and thanks very much to Melissa for letting me know about the conference, and thanks very much to the people at the conference for letting me in the day before when it had been full for months. I chatted to Jan LeCun, and I chatted to him about this thing I've been thinking about a lot during this part of the course about transfer learning. How come people seem to always use VGG for the best transfer learning results when there are so much better systems around. I've been really coming to this conclusion that the reason is that fully convolutional nets like ResNet and InceptionNet have very, very large, very, very redundant intermediate representations. So because they don't have a fully connected layer, like in ResNet, the penultimate layer is 7x7x2000, which A is huge, and B, if you look through that 7x7, most of them are the same more or less as their neighbors, because most parts of an image are similar. If you want to go back far enough through enough bottleneck layers, you actually get a fairly general representation. Like we do with VGG when we go to the first fully connected layer, you're at 28x28x500. They're far too big to work with and far too redundant. So I asked Jan LeCun about this and I said, is anybody working on the question of how do you capture the benefits of these much more accurate architectures but create efficient distributed representations. Jan LeCun was like, yes, absolutely, of course. Where do I read about it? He was like, well, we use it in Facebook all the time. Like at Facebook we take every object in Facebook and we create a compressed distributed representation of it and we save it in databases and then we give little bits of code to all the groups so that they can create simple linear models on top of these distributed representations. This is everywhere in Facebook. And I was like, oh, so is this written down somewhere? He was like, I don't know, maybe it's in a technical report somewhere. It's like one of these things that I knew it must be happening and so now I know at least one company, a very very big company, that it's happening at a huge scale. And so when they say here that this is used to search for multimedia documents that are similar to each other, if we read between the lines, they're not talking about looking at pixels or samples of audio, they're talking about activations, distributed representations. And we know now when we talk about perceptual losses, for example, how much better it is when you capture similarity using activations than you do using pixels. So here's a huge opportunity for deep learning which, sure. I read briefly a YouTube paper in which they say something like, and I'm asking you because maybe you have a better insight on how this is done, I have been very interested in understanding it better. So they basically do some kind of embedding of the user and embedding of a video, which is probably something kind of similar to this. So they first build this kind of representation of every user and every video, and then on top of that they have another deep learning network that actually does everything else. At YouTube, my understanding is most of their embeddings are based on more of a collaborative filtering approach like we did in Lesson 4. To my knowledge, they're not doing actual video features, as far as I know. But there's no reason they shouldn't be. So I now know that Facebook absolutely are. So thinking about this, like I thought about this many years, in a medical context, every medical image you have, you have a compressed distributed representation of it, such as the first poorly connected layer of VGG. It's stored in a database. And when I say a database, I actually mean a fast indexed approximate nearest neighbor's tree or structure. And now you can go and grab every medical image that displays similar symptoms to this medical image or whatever. A guy from Kaiser is giving me the thumbs up. So this kind of stuff is really exciting. And as far as I know, it hasn't really been written down anywhere, and it's not really being used anywhere much other than at least Facebook, who probably Google does this too. So that's just a bit of background. We are going to talk about the BiLSTM hegemony. These slides come from Chris Manning. Who here's come across Chris Manning before? So Chris is a linguistics professor at Stanford. He did his PhD in linguistics. And at some point in the last small number of years, he discovered that everything he learned about linguistics was basically a waste of time because now you can throw a bidirectional LSTM with attention at pretty much anything and get a better result than everything from his PhD in linguistics. So nowadays he actually teaches deep learning, and in fact his deep learning for natural language processing videos just went online today. So if you want to learn more about that, feel free. So at this conference last week, this is one of the slides he put up. He described himself as pretty disappointed about this situation. It's not where he wants linguistics to be, but there it is. And so this is what we're going to learn about today, bidirectional LSTMs with attention. So this is what happened when people started throwing bidirectional LSTMs with attention at neural translation. This looks a lot like the 2012 ImageNet picture. The red is the kind of two generations ago approach to statistical machine translation, phrase-based. The purple is the last four years of the next generation approach, which is syntax-based statistical machine translation. Neural machine translation didn't really appear properly until 2015, and this is the path that that's now on. And we're probably well beyond that now because of course Google's neural machine translation system is now online and a lot of the stuff that's coming out of that is appearing in papers now. So actually back here in about 2013, I actually gave a talk at one of the academic conferences, which is like an introduction to deep learning for people in the statistics academia who maybe weren't familiar with it. One of the things I told them was, all of you who are in NLP, start learning deep learning now because there's no question in the next three years or so it's going to be the state of the art in pretty much everything. So hopefully some of those people listen to me and today they're very happy. Even though we couldn't exactly tell at that point how that was going to happen, it was just really, really obvious that it's a system which uses distributed representations and has all of the properties that you would expect to see in something where deep learning will be successful. Today I think increasingly people are realizing that large areas of classic statistics and machine learning and the process are being replaced. So Chris described the four big wins of neural machine translation, and I think one of the most interesting ones is number 3, which is the statistical approaches tended to use n-grams. So like these three words appear, these three words appear, these three words appear next to each other in these situations. You can't get beyond about 5-grams, so a list of 5 words, because you just get this total exponential explosion of how big that dataset is. With RNNs, remember when we first learned about RNNs, we talked the reason why, stateful, memory, long-term dependencies. So this is exactly what you want if you want to make sure that your verb tense and everything else lines up with your details of your subject. They could be 20 words apart or more, so we need to be able to have that kind of state. So this is where this works very well. Interestingly, I'm not deep in the NLP world, so I wasn't quite clear on where things are with all of the states that say the art. Chris said in his talk, BIOS-TMs with attention are the state of the art for basically everything. We actually have an NLP person here. Can I ask you a quick question, Anna, about this? Is this kind of line up with your experience? You're like an NLP researcher. Anna Rammelmann Yeah, my research is mostly in information extraction. But first of all, if Chris Manning says it, it's true. So the other part of this is that in some of those areas, while this is state of the art, definitely the state of the art is not that impressive. But he's right, this is what's happened. People have found that for all those tasks, and others which are not as severe, which are kind of similar in certain ways. That is the state of the art. What Anna was talking about the kind of disappointing state of this. So on one of these slides, Chris Manning had a big frowny face, and one of the audience was like, what's with the frowny face? And he's basically describing all of the ways in which the state of the art results still are far short of where we want them to be. So NLP is by no means solved. We could kind of say basic image recognition is kind of solved, but basic NLP is not really solved. But here's a great example Chris gave of something that this approach has worked really well for, a very difficult task. It is to read a story like this, and then in the story highlights, one of the words or phrases is deleted, and the neural net has to figure it out. So if Star Wars is deleted, it'll be characters in movies have gradually become more diverse, and you have to predict Star Wars. So this is a challenging problem that really requires some in-depth understanding. The work that Chris showed, and actually he was a senior researcher on, is to basically take the query, chuck it into an embedding, take the entire story, chuck it into an RNN, take the RNN that comes out, do the whole thing with attention, and that's it. So let's talk about how to do this. So there's a really nice article on distilled.pub which has a bit of a picture of this. So imagine we're translating English into French. So here's our English. And so we're trying to translate the European economic area. So the is la. But the is la because of the gender of the noun area. So you see these little purple lines here. This is showing that this neural network model has learned that when it is looking at translating the area, specifically the, it also needs to look at the word area in order to figure out that it's la. So these purple lines are showing the weights in the neural network for translating that particular word. So when it was translating signed, this is what I was talking about long-term, long-distance dependencies, it's not just using the word signed, but also looking at what was signed in order to figure out the details of how to use this verb. And then some things, you kind of need to look at combinations. So was signed, you have multiple prepositions working together. So we need to come up with an architecture which is capable of learning these attention weights. So as we mentioned last week, this really fun paper called Grammar as a Foreign Language, Jeffrey Hinton was the senior researcher on this one, has a nice little summary. This is not where it originally came from, but a nice little summary of how attention works. So let's go through it. We don't often go through things by looking at the math, but I think in this case the math is simple enough that this is actually maybe a good practice. So let's start off by looking at the notation. So there's an encoder. So remember the encoder is the RNN, or number of layers of RNN, which are going to look through the original source sentence, so in this case the English sentence. And it's going to spit out a hidden state. And if in Keras we say return sequences equals true, we'll get a hidden state after every single English word. So if you've got 10 words in the English sentence, we'll have 10 pieces of encoder state. So the encoder state, they're going to call it H. Remember these things that are underneath or on the top generally are just the same as in NumPy as putting things in square brackets. So what they're doing here is they're telling us that TA is the number of words in the English sentence. So we've got 1 through TA bits of state, each one's called H. So that's the encoder hidden state. And then the decoder, as you run through each step trying to translate this thing into French, is creating its own hidden state. That's going to be called D. And D will go as far as TB, that will be the highest index of the French sentence. So in the end, our goal is to replace every one of these, so we can write these as DT, meaning each one of these items of decoder state. We're going to try and create something called D-T, which is going to represent the result of this attention process. So basically it's going to represent the word, in this case the representation in the hidden state of the word for area, weighted quite a lot, and economic, weighted a little bit, and everything else not much at all. So not surprisingly, the way to do that is with a weighted sum. So here, remember H, that's our encoder state. So that's the state from the RNN run over that English sentence. So the word for area and so forth. Question. To clarify, in this case, instead of creating one condensed representation vector that captures the entire English sentence, here are we just looking at the existing hidden states that get generated as the English sentence is processed word by word. It's just the difference between return sequences equals true versus return sequences equals false. So with true, it just throws away everything except the last state. Return sequences equals true is keep all the ones in the middle. And remember, importantly, this is a bi-LSTM, a bi-directional LSTM. So we've got one LSTM going forward through the sentence and another one going backward through the sentence. So every one of these pieces of encoded hidden state represents all of the words before it in that order and all of the words after it in reverse order, stuck together. And remember also it's been layered, so there's quite a few layers of non-linear neural net layers going on. So that's what this state comes from. So each element of H is already a pretty complex calculation to get there, a pretty sophisticated calculation. So we're going to take those and we're going to multiply them by some weight. See how the weight's got two indexes on? That means that the weight depends on two things. One is T, which is which word in the French translation am I trying to create right now, and I, which is which piece of encoded hidden state am I currently calculating the weight for. And because this is being calculated with a function, if we were doing this in Python, we'd probably be writing like get weight T, I. And so the line above it tells you how to go about calculating that weight. And it tells you, not surprisingly, that to calculate that weight we're going to be using softmax. We're going to be using softmax on top of some other function. So why softmax? Well, if we're doing a weighted sum, we want the weights to add up to 1. That's one reason. Secondly, most of the time in translation, the thing that you're translating is largely just one word. 1992 is translated as 1992. That's just August. But then sometimes it's mainly one word, like in, but a little bit of some others. So a softmax, because it's e to the something divided by the sum of e to the something, remember it tends to be very big for just one item in the vector and fairly small for everything else. But by using softmax, we capture that quite naturally. So that's why we use softmax to calculate these weights. So softmax of u, u is another function or the result of another function. And what is this? This is just a multilayer perceptron with one hidden layer, a neural net with one hidden layer. If you have a look, it's a couple of bits of data multiplied by weight matrices put through a non-linearity, multiplied by another weight matrix, put through another non-linearity. So that's the definition of a neural net with one hidden layer. So when it all comes down to it, what this all says is, in order to calculate the weights on each of the source encoding words, or each of the target translated words, train a tiny little neural net with one hidden layer which learns to figure out which source words to translate now. And so remember one of the things that Chris mentioned in his like, why is neural machine translation good, one of his reasons was because it's end-to-end trainable, we're going to embed this mini neural net inside the bigger RNN so that the whole thing's going to be just SGD'd all in one go. So that's what we're going to try and do. So I just want to make sure that's clear before we look at the code for how to do it. So how's a hidden state looks like? Is it like a set of activations? So the hidden state is just a normal LSTM hidden state, so it's just a vector. So anytime you want to remind yourself of what's really going on with RNNs, go back to this Lesson 5 RNN PowerPoint. We go through it and we remember, this is not an RNN, it is a basic neural net, and we could have a multi-layer neural net, and then we could have a multi-layer neural net in which we have a second input coming in, still not an RNN. We could have two things coming in at two different times, and then we could also tie these weight matrices. And the notebook that goes with this class, we do all this by hand in Keras with all the weight tying and everything, so you see every step. And then we realize, oh, that last picture could have been drawn like that. And we also realize we could do this as well. So all of these circles represent just a bunch of activations, just a vector of activations. And so we have a bunch of these layers until eventually we decide to keep some final set of activations. So this is all we're doing. Questioner. I don't know what to call it, the attentional model there. It's relatively simple, it's just a single hidden layer. Is there a reason that it wasn't more complex or some fancy architecture? Answer. I've been wondering that myself. I guess the answer is it seems to work. There's no reason you couldn't have two hidden layers. I guess it's so easy to go back and look and see what the attentions are. Presumably so far the way people are using this, they're finding that the attentional model is getting the correct stuff. I think if you try and do an attentional model for something else and that doesn't happen, I'd say yeah, check another layer. In fact, this terrific post actually shows a really cool example of this, which is on the bottom here is a sound wave, and on the top is the speech recognition. And so people are actually using attention models to automatically do speech recognition by figuring out which parts of the sound wave represent which letters. And in fact, one of the super cool things to come out this week was the Tachotron, which is one of the best names for a paper I've come across. And the Tachotron has some fantastic examples actually of what it sounds like. Let me see if I can get this. So the cool thing here is it changes depending on where the punctuation is pretty impressively, or even whether there's capitals. Check this out. The buses aren't the problem, they actually provide a solution. Does the quick brown fox jump over the lazy dog? So with this kind of end-to-end training like this, you don't have to really build anything special to make that happen. You just need to make sure that your labeled data is correct. And actually somebody pointed out something really neat the other day, which is if you want to build a speech recognition system, one easy way to do it would be grab some audible.com audiobooks and the actual books. You could have 40 hours of training data like that. In fact, if you grab Stephen Fry reading Harry Potter, then you could have every Harry Potter voice as well. So this is a super amazing technique and surprisingly enough, this single hidden layer seems to be enough to do attention pretty well. So last week we were looking at the spelling bee, where the input's worth things like zu-wiki, and we were having to figure out how to spell it. We tried it without attention and we didn't get great results. We looked at the paper from the original Badeno et al. paper that showed that with longer sentences in particular, you get much better results if you do use attention. So let's have a look and see what that looks like. So Keras doesn't really have anything to do this effectively, so I had to write something. So before I show what I wrote, let me describe what it looks like. Most of it looks exactly the same as the original spelling bee model. We have our list of phonemes as input. We have our list of letters to spell the word with as our decoder input. We chuck them both through embeddings. We then do a bidirectional RNN on the phonemes, and then we chuck an RNN on top of that, and then we chuck an RNN on top of that. You might remember from last time this getRNN function, just to remind you, there's just something that's returning an else. And by default we're saying return sequence equals true. So last week the final one was getRNN false. We put everything into a single little package and then we fed that to the RNN. But now we're leaving return sequences equals true all the way through so that we can then pass it to this special attention layer that I created. We'll look at how this works in a moment. But let's first of all talk about what does an intention layer need. It needs to know what kind of RNN layer do you want it to create. So I just pass it a function which creates an RNN layer. In your decoder, how many layers do you want to create? So I say create 3 LSTN layers in the decoder. And then what information is it going to need to create that? It's going to need all of the encoder state, and then we can do something else to make it easier. We can do something called teacher forcing. And teacher forcing is we are going to pass it, as well as the encoder state, we're going to pass it the answer, but we're going to pass it the answer for the previous time period. So in other words, if we're trying to learn how to spell the wiki, at step 1 we're not going to pass it any layer. At step 2 we'll pass it Z. At step 3 we'll pass it Y. At step 4 we'll pass it W. In other words, we're going to tell it what the previous time step's correct answer was. Why do we do that? It makes it much easier to train, particularly early on. Early on it has no idea how to spell, and so it gets most things wrong most of the time. And so the later letters, everything's wrong before it. So how can it possibly know what letter to use now? So with teacher forcing, we're going to take our input data, being all of those encoder hidden states, but we're also going to tell it, even if you got the previous letter wrong, this is what it should have been. So it's going to make it a bit easier for it. And all we do is we just concatenate together the hidden state plus that decoder input. So that's called teacher forcing. You don't have to use teacher forcing, but if you do, it just makes it faster and easier to train. So that's why we pass in this special decoder input. And just to show you, the decoder input... It's all of my labels except not the very last one. So it's going to be one less than the full number of letters to spell. And then I concatenate a column of ones onto the front. Now why is it a column of ones times Go token? And then Go token was something that I set up back here. I decided that an asterisk is going to be a special character which means this is the start of the word. So every time we're going to get past if it was a wiki, we'd get past asterisk, z, y, w, i, c, k. So that's going to be this teacher forcing input. So this attention layer is going to create a 3-layer RNN. It's going to get given all of the encoder state. It's going to get given that special decoder input. And it's going to spit out just a list of states again. So we can just do the usual thing of doing a time-distributed dense with our appropriate vocab-size softmax to turn that into our target activations. So putting aside how we calculate this, everything else is pretty familiar. So we can then just go ahead, build that model, compile it, fit it, passing in both the phonemes and those decoder inputs, train it for a while, a bit of annealing, train it a bit more, until eventually we have actually pretty good accuracy, 51%. Did you have a question, Rachel? Do you know if the current work strictly uses the hidden states of the final RNN layer, or have people tried using attention sums over different indices over multiple layers, the way we did using perceptual loss for multiple layers? That's actually a great question. The answer is, to my knowledge, people are always using the last layer hidden state. But I kind of accidentally used all of them once and got some better results. So I don't know if I screwed something up or if this is a good idea. So that might be something if you want to test it to try it out. I was like, after fixing a bug, things got worse, which is definitely a good sign. So maybe it's not a bug after all. So remember last time we had trouble with the really long words, lots of phonemes. But here's 11 phonemes and it's done it perfectly. So that's a good sign that this is handling some longer ones pretty well. Is 51% good or bad? I don't know, because spelling is not exact. There's no way you could algorithmically get 100% spelling. Is it meant to be speak like this or speak like this? This is from a test set, there's no way for it to know. So I think it's done pretty well. So the only thing remaining is how to do this attention layer. So the bad news is that the attention layer is the largest custom layer we've looked at. It's not terrible, but it's 100 lines of code. But a lot of it's pretty repetitive. So it really depends how interested you are, A, in custom layers and B, in attention as to how much you want to look at this. But basically I'll show you the key pieces. There are two really important methods that get called in your custom layer, and they are build and call. Build is the method that's called in your custom layer when your layer is actually inserted into a neural network and that causes it to get built. Once it's inserted into a Keras neural network, that's the point at which it knows how big everything has to be. Because you told it how big the input was, and then you've got the various layers connected to each other, and so eventually it can figure out how big. In fact, specifically, what is the shape of the input. So when your build method is called, you get told what is the shape of your input. And this is the point where you now have to set everything up to work with that input shape. So in this case, build gets called as soon as it finishes running this line because it knows that it got this input. And it can figure out this input shape by going all the way back through all this. Now we have two inputs. We have the encoder state and the decoder input. So we can pull those apart into the encoder shape and the decoder input shape. So some things which seem like they might be hard are actually incredibly easy. To create my 3 layers of RNNs is a single line of code. Remember I just passed in the RNN generating function, so I just call it for each layer. And with the list comprehension, I now have there is 3 RNN layers in the list. That was easy. On the other hand, to do this, I'm going to have to do it all by hand. And in fact, there's some things that are being hidden here. For example, a bias term is not included here. But we definitely want one. People often don't include bias terms because you can always avoid it by adding a column of 1s to your data. So it's common in math not to mention the bias term. But generally speaking, any time you see a neural net, unless they state otherwise, there's probably a bias term there. So in build, we're going to have to create W1, W2, and V. To create them, Keras actually has a convenient method that comes from the layer superclass called addWeight. So addWeight you basically say, what's the size of the weight matrix, what's your initialization function, and give it a name. So I just took that and called it W. So here I just go www, passing in W1, how big is that going to be, W2, how big is that going to be, here's my bias, how big is that going to be, here's my V, how big is that going to be. And then I've got two other things here, which is W3 and B3. And that's because this is kind of hand-waved over something, which is this final answer basically needs to become the hidden state of your next layer of the RNN, but it might not be the right size. So then I just added one more transformation to the end to basically make it the right size. And in some of the attention model papers they do lay that out. So I don't know whether Hinton's group just ignored it, or maybe they built it so that the shapes worked anyway, I don't know. So we've got one extra affine transformation. A minor note, for those of you playing with PyTorch, I'm sure you've discovered how cool it is that you can go self.addmodule and it keeps track of everything you need to train, all the parameters. Keras is not so clever, so that's why we have to go self.addweight for every one of these weights. Because Keras has to know what to train. When you optimize, what should you train? Unfortunately when we created all these RNNs, these RNNs have lots of weights which we haven't told Keras that we need to optimize them. So I have not found any examples of custom layers anywhere on the Internet where people have actually done this. But I figured out what you can do is basically create this little function that goes through every one of my layers, every one of those RNN layers, and finds all of the attributes that Keras needs to know about. So it needs to know what are the trainable weights, non-trainable weights, losses, updates, and constraints. So anyway, you can copy and paste this code. If you want to create a custom layer which itself contains some Keras layers, copy and paste this code. It seems to work. So the first main thing you have to create when you create a custom layer is build. Creating build is really boring because you have to get all these bloody dimensions to be correct, which is a pain in the ass. So when I built that, I had this little bunch of TensorFlow playing around code where I would just keep going in and being like, okay, if this is the size of my X, this is the size of my W1, let's try doing each calculation. So I did each calculation in a cell one step at a time and looked to see what all the shapes were and then went back and put them in. So I don't think any normal mortal person can write all of these dimensionalities and actually get them to work without doing them one little step at a time. So make things easy for yourself. So the other key thing that happens in a layer is to call pull. And call is the thing that basically that's your forward pass. This is calculate, go calculate. So you get passed in the data, the calculate on. So in this case, we actually have to step through the steps of an RNN. And Keras actually has a k.rnn function which is very, very similar to theano.scan. You guys remember theano.scan? This is basically something which calls some function for each step, it steps through each part of your input, has some initial states, and so forth. It's almost identical to theano.scan. It's a really low-level thing. And so Keras really doesn't have a convenient user-facing API for creating custom RNN code. In fact, this is something which nobody's really figured out yet. TensorFlow has just released a new kind of custom RNN API, but there isn't any documentation for it. So I was hoping to teach it in this course, but I think it's just a little too early. I don't know if it's good or not. This is a bit of an open question really. How do you create something as easy to use as Keras, but with the flexibility to design your own RNN details? As you can see, in this case it wasn't convenient at all. I had to go back and run this scan function from scratch and set everything up from scratch. So basically all the work happens in here, which means all the work is happening in this step function I created. So here's where the actual calculations are done. So in the end, when it actually gets to it, this now looks a lot like the Hinton team's code. We basically can see us doing the dot at the bias, and here's the than, and here's the v, and the softmax, and here's the bit where we do the weighted sum. And then this is this one extra step I mentioned, which is to get it to the right shape for the RNN again. And so now that I've done all of that preprocessing, I've started with X and I ended up creating this thing called H, you can now go ahead and go through all of those three RNNs calling one step on each of those. This probably shouldn't have been that hard in the end, but it's just the nature of the Keras API for this stuff that doesn't really exist that we had to go in and create it. All we really wanted to do was say, okay Keras, before you run the three decoder RNNs, take your input and modify it this way. But we have to do it for every step. That's basically what was missing, some way in Keras to easily say, change the step. I've spoken to Franz Weill, the Keras author, about this. He's well aware that this is not convenient right now and he really wants to fix it, but this is difficult to get right now and no one's quite done it yet. We have a question. I didn't get the getting it to the right shape again part. Could you explain that again? Sure. So step is being called for every time step. So at the end of it, we have to return the new hidden state. And then that new hidden state is going to come back into the next step. So in other words, the thing that we end up with here needs to be the same shape as we had here. But that doesn't happen automatically because we're actually taking this weighted sum and we're actually concatenating onto it the decoder input, we end up with something that's a totally different shape. But you can always change the shape of state by multiplying it by a matrix where the thing you multiply it by has the number of columns that you want to create. So I just made sure that W3 has the same number of columns as the H that we want had. So as a result, by the end of this, we've got something which we can feed back into the RNN again. We just need to make sure. An RNN step can't change the shape that it started with, because it needs to be able to keep going step, step, step. Every step needs to be the same tensor size. Question. You showed how you figured out, tested the tensor shapes. How did you debug the attention class itself as a whole? Are RNNs easier and cleaner in PyTorch? Would the attention class have been relatively easier in PyTorch? The main thing for me to debug was build. This bit is Jeffrey Hinton's team's equations. There's not too much to get wrong here. So really it was a case of doing it step-by-step in cells and printing out the shape at each point. You'll see that I created all of these different dimensions at the start. I made sure all of these numbers are different so that every time I saw the number 64, I knew that was the batch size. Every time I saw 4, I knew that was the time steps. 32 was the input DIMM, 48 was the output DIMM. So I could go through and I could see, okay, here's my shape here, here's my shape here. And then all I just did was chuck in some print statements, print HW2, print U, print A, and so forth. I didn't need to see the contents of them. When you print a TensorFlow tensor like that, it prints the dimensions of it. In my experience, once your dimensions work, you're done. As long as you didn't make the mistake of having multiple things having the same number of dimensions. As long as your dot product fits. If H and W2 didn't line up for rows and columns, the error you get is pretty clear. It'll be a TensorFlow will say these mismatch and it'll tell you what the dimensions are. So generally speaking, getting these things to match isn't too bad. It's no fun at all. But it's kind of menial. You'll get there eventually. So now that it's done, I've written it, so feel free to use it. As you can see, it's easy to use. At the end, we get these pretty good results. One thing to point out is when we go, we get the predictions by just saying model.predict with our test set. And then those predictions, you'll notice we go split on underscore. Why do we split on underscore? That's because when I created the vocabulary earlier on, I set underscore to be the 0th element. And remember that all of our words are padded at the end of zeros. So when the decoder predicts that this is the end of the word, it's going to spit out 0 or underscore. So this is how a decoder knows to stop. Now it doesn't actually stop. In terms of computation, the decoder's still going to keep calculating all of the rest of the steps because we don't have the ability, at least in Keras, to say stop now, everything's rectangles. So hopefully the decoder learns pretty quickly that if the previous token was underscore, the next token will always be underscore. Once you've finished, you've stayed finished. So that's a minor issue there. So that's that. So we're going to have a 7 minute break and when we come back, we're going to see this in PyTorch and we're going to use it for actual language translation. So I'll see you at 8 o'clock. So one of our students, Vincent, was at Study Group a couple of weeks ago and was writing all these eigenvalues and eigenvectors equations. He was like, what are you doing? This idea, there's something in this style transfer, it's waiting to get out, I can feel it. I was like, oh, it's interesting, keep going. Then last week I saw him doing some more and he had some strange noisy pictures on his screen. Then on Friday quite a few of us got together to do some hacking. I still saw him doing the same thing. I know there's something here, it's going to get close. He just told me he sent me an email. So here is a photo, and here is the regular style transfer result. Here's what happens when you use his new mathematical technique. Hopefully by next week I will understand this enough that either him or I can explain this to you. But one of the key differences is he's actually using the Earth movement distance, which is the basis of the Wasserstein gain. I've managed to avoid teaching about eigenvalues and eigenvectors so far. I don't know how we're going to do that. So this is got to be a paper. You've created a whole new technique. This is super exciting. So congratulations. I look forward to learning more about it. People just keep doing cool stuff. I love it. You guys are just sipping along. So let's translate English into French. Now here's a problem. The teacher forcing his students to do the math. So what this question is getting at is, why don't we use the lambda layer to do the attention calculations and then feed that into a standard RNN. Remember, those calculations are being done inside the step function. So in other words, each step uses attention to calculate the output of the step, which impacts the next step. So it needs to be inside the RNN. You can't just pre-process the whole layer. Question. Is there any reason why we used hyperbolic tan as opposed to sigmoid? Answer. No reason at all. Hyperbolic tan and sigmoid are the same thing. Hyperbolic tan goes from minus 1 to 1, sigmoid goes from 0 to 1. Teacher forcing was this thing where we're concatenating the previous correct answers embedding with our attention-weighted encoder inputs in order to help our model to keep track of where it ought to be. That helps the training. Now there's nothing wrong with training that way, but you can't use that at inference time, at test time, because you don't know the correct answer. So my Keras model here is totally cheating. I'm passing in the previous letter's correct answer to every step, but in real life I don't know that. So what we actually need to do is to at inference time, you don't use teacher forcing, but instead you take the predicted previous step's result and feed it in for the next step. I have no idea how to do that in Keras. It drove me crazy trying to figure out how to do that in Keras. That was the thing that pushed me to PyTorch. I was so sick of this goddamn attention-layer, the idea of going back and trying to put this thing in drove me crazy and I don't even know how to do it. Furthermore, we actually want this to be dynamic. It turns out the best way, if you use teacher forcing the whole time, you actually end up with a model that gets sloppy. It learns to take advantage of the fact that it's about to be told what the previous thing should have been. So it kind of uses a more speculative approach. So actually the best training approach is as it goes through the epochs, initially it uses teacher forcing every time, and halfway through it uses teacher forcing randomly half the time, and at the last epoch it doesn't use teacher forcing at all. So it kind of learns to wean itself off this extra information. And that kind of dynamic thing, that's really hard or maybe even impossible to do in Keras. So for all these reasons, I moved to PyTorch. I haven't done all of these things in PyTorch, I've done most of them, but the dynamic changing of teacher forcing, I've actually left for one of you guys to try out. Anyway, the basic ideas are all here. So let's look at the PyTorch version. Interestingly, the PyTorch version in terms of the attention model itself turns out to be way easier. But we have to write more code because there's less structure for NLP models. Like for computer vision stuff, there's the PyTorch vision project which has all that data loading and models and blah blah blah. We don't seem to have an NLP kind of version of that. So there's a bit more code to write here. So let's translate English to French. What I did was I downloaded this Giga French corpus which I'll put a link to on the wiki. Basically this is a really cool idea. What this researcher did was he went to lots of Canadian websites and used a screen scraper to figure out whether there was a little button saying switch from English to French. And the screen scraper would automatically click the button and then assume that those two screens were the same. And then he tokenized them into sentences and provides this corpus of like a billion words. So this is pretty great. I didn't really want to create a complete English-French translator because I just didn't have the time to run it for long enough. So I tried to think, what's like an interesting subset of English to French? So I thought, what if we learn to translate English questions that start with WH. What, who, where, why. So when I looked at it, it turned out that that was like 80,000 or something. There's a lot of sentences basically. But the nice thing is that all of those sentences have a somewhat similar structure. So we're going to learn everything about translating English into French, but we're going to be doing it with a slight subset. So that's why I did this regex, which says, look for things that start with WH and end with a question mark in English. And where the French can be anything at all, but it should end with a question mark. So I went through the French and English files. This is this really cool trick we've mentioned once before, that once you go open in Python, that returns a generator that you can loop through. So if you zip the two together, you've got the English questions and the French questions, and then just go through and run the regex and then return the ones that both of the regex matched. So here's the first 6 examples. That looks good. And as you can see, a lot of them are really simple and some of them are more complex. As per usual, dump that in a pickle file so we can load it in quickly later. So we've got the English questions and the French questions. I'm going to show you all the steps for real-world NLP so you can see all the pieces. We're going to do everything by hand so you can get a sense of exactly what happens. So the next step is tokenization. So tokenization is taking a sentence and turning it into words. But this is not quite straightforward because what's a word? So is that a word, or is that a word, or is that a word? And so I had to make some decisions based on my view of what was likely to work, which basically is like, I think that's a word. So I just wrote regular expressions for doing heuristic tokenization. Now you can use NLTK, the natural language toolkit, that has a bunch of tokenizers in. Honestly though, I actually found my hacky rules-based tokenizer, I was happier with it than any of the NLT tokenizers I tried with this problem. So you can see that basically, for example, if you have any letter followed by apostrophe S, then I want you to make apostrophe S a word. It's kind of more like of than anything else. Or else if it's a letter followed by an apostrophe followed by a letter, that's probably French, in which case everything after the apostrophe is one word and everything after it is another word. So you can basically see it here. Tokenize the French, que, es, que, la, moumiere. That is exactly what I want. And here's the English. And so then let's test a very accurate statement like Rachel's baby is cuter than others's. And you can see here the apostrophe S's doing the right thing as opposed to the apostrophe in the middle. So check out my tokenizer, all looks good. Makes accurate statements about Rachel's baby as well, so all good. So now that we've got that tokenizing, we can go ahead and do the standard thing that we do every time we work with an LP, which is to turn our list of words into a list of numbers. We always do it the same way basically. Create our vocabulary, what are all the possible words and how often they appear. Insert a padding character, insert the start of stream character, this is like the asterisk in the previous one. Create the reverse mapping from word to id by using this little enumerate trick. And then go through every sentence and turn it into a list of tokens by calling that dictionary. And you have a question behind you Rachel. Question asked. Do you need a stemmer before you convert into numbers? Answer. No, not at all. Different words with different stems are different words and have different translations. So we want to keep that whole thing. The tokenizer really is to do something that we think we can do in a purely rules-based way. The question of how do we deal with morphological differences is actually highly complex, varies a lot by language and we want to learn it in new or now. So this is going to end up returning the list of id's for each sentence, the vocabulary, the reverse vocabulary, and just the frequency counts for the vocabulary. Next step is to turn these words into word vectors. Earlier on we used Word2Vec because Word2Vec has these multi-word things. But for translation, I don't want multi-word things, I want single-word things. So that's why I'm going to use GloVe. So go ahead and turn that into a dictionary from the word to the word vector. Also grab some French word vectors. I found this really fantastic site that's got some nice French word vectors. So now go ahead and build a little thing that can go through my vocabulary, create a big array of zeros, and then go ahead and fill in all of those zeros with word vectors, if you can. Of course sometimes you look up a word and it's not in your word vector list, in which case you can just stick a random vector in there instead. So this is all stuff we've done many, many times now. I also am keeping track of how often I find the word, just to make sure. For English, out of 19.5 thousand in the vocab, we find 17.2 thousand word vectors in GloVe, so this is looking pretty good. So most of our word vectors are being found. And for French, a little bit less, but still not bad. That's probably because of my particular tokenization strategy. It might have been different to the tokenization strategy they used for these word vectors. Question from the audience. You still have the audio popping, crackling problem from time to time. Of course the other thing we have to do with NLP is to make everything rectangular. Notice here I'm calling a Keras function. If Keras does something you need, please use it, even if you're in PyTorch, it's PyN. I've heard a number of people say, oh I'm trying to use PyTorch, but I hate that it doesn't have pad sequences. If you import pad sequences, it has pad sequences. Train test split to grab 10% of the data. Train versus test. And here we have it. 47,000 train, 5,000 test. And here's an example of a French sentence and an English sentence and all the padding. So we now have to do all the data loading stuff ourselves. So I've gone ahead and created a getBatch function that is going to go ahead and return a random permutation of lenX. So lenX is the length of all of our sentences. So this is going to return and then grab the first batch size of them. So this will return 16 random numbers between 0 and 47,097. So then I can just return those for English and those for French. So it's okay if you don't have a data loader. This is all it actually takes to create a basic generator. This is basically doing the same thing as a generator if you don't need any data augmentation. And again, here's a piece of code you can steal. If you need a generator in PyTorch, here's a generator in PyTorch. Pass in your data and your labels and your batch size and it will return a batch of each. I mentioned last time that I created broadcasting functions for PyTorch. Basically what had happened was I had all this Keras code that worked and I wanted to port it to PyTorch. And PyTorch doesn't have broadcasting, so none of this stuff worked. So my PyTorch was way more complex than my Keras because there was.squeeze and.unsqueeze and.expand everywhere. So I wrote add, subtract, multiply, divide, and dot such that they have the exact same broadcasting semantics as Keras. This is actually pretty interesting and I really wish I had time to show you, but maybe you can have a look at it during the week and ask questions on the forum. There's so little code, so the amount of code to make all of that work is basically that and that. It's incredibly little code. But importantly I also want to show you how I build this stuff. I always use test driven development for this kind of thing. So basically I created a whole bunch of matrices, vectors, 3-dimensional tensors and 4-dimensional tensors, transposed versions of them, wrote something that checks that the two things are the same, and then I just went ahead and tried making sure that all of these things ought to be the same as all of these other things. First of all I wrote all the tests, and then I gradually went through and put in code so that the tests started passing. And then I went back and kept refactoring the code until it was simpler and simpler. So you can see in the end all of my functions are nice and small. What this meant was I could now write an attention model using almost exactly the same notation as before. So that was how I created these broadcasting operators. So given that they exist, here is a non-attention encoder. So you can see basically all it is is create some embeddings, create a GRU, and then in the forward pass run the embeddings and then run the GRU on that. And in PyTorch, a GRU, you don't have to actually write the three GRUs next to each other, you can just say number of layers equals and that's going to stack the GRUs on top of each other. So that's that. Pretty straightforward. The decoder is also pretty straightforward. Again, create the embeddings, create the GRU. For the decoder, we also need a linear layer at the end, which is the correct size for our output vocabulary. We actually have to remember for that inference time, we don't just want the state, we actually want to get out something that we can do an argmax on to find out which word we just translated this into. So we need this layer to turn it into the correct size for the French vocabulary size. So the forward pass here, this is a little different as you'll see when we get to where we actually use this. This is actually just a single step. So this is basically just you doing one letter at a time as you'll see when we get here. And here's the actual softmax on the dense layer that we created. So it's just embedding, GRU, dense layer, softmax. And we return both the new hidden state, which we'll be using for the next step, as well as the actual result of the output. Here's the attention decoder. And as you can see, rather than being 100 lines of code, it's a screen and a half. And the nice thing is, this is all basically the same as Keras. My W123, my D23, my V, my GRU, and then my final output. And then this is basically all the same as Keras as well. So I've got my dot, and add, and multiply, softmax, here's the weighted sum, here's that cat, here's the W3, add on the bias, call the GRU, and then dense layer and softmax. And again, we both return the actual predictions as well as the hidden states. Now the thing is, we have to write our own training lab, because this is PyTorch. So we're going to have to go ahead and do a bit more work here. So basically, here's the code that trains an epoch. Create an optimizer for the encoder, an optimizer for the decoder. The criterion is what they call the loss function, negative log likelihood, that's the same as the cross-entropy. Here's that get-batch function we created ourselves earlier to grab one batch of French and English, put them onto the GPU, and then call train, we'll look at that in a moment, check out the loss, from time to time print out the loss. So all the work's actually happening in train. So each of these things is less than a screen of code, but still we had to write it ourselves. So encode our decoder and encoder. And remember with PyTorch, you have to manually call zero grad. So you have to zero out the gradient at the start of your training loop, and then go through each word in your target, so we've already encoded it, and call the decoder, passing in the decoder input, the hidden state, and the encoder outputs. So then next decoder input comes out from there, key check of the loss, and then we have to call dot backward manually, we have to call the optimizer set manually, and return it. So it's not very interesting code, and it's the kind of stuff which I suspect hopefully the PyTorch community and maybe some of us will all be able to contribute to getting rid of all this boilerplate kind of code, kind of Keras style, over time. I'm sure that will happen. For now, there's the code. Now that it's there, we can create our encoder, we can create our attention decoder, we can train it for a while, and then we can test it. Now for testing, I need another function because I want to turn off teacher forcing. So you'll see in this function, this is not very well refactored, I've copied some code here, but basically I encode and then I go through my target length, call the decoder. But look here, I now take my decoder output and find the top one. So this is basically argmax. This is saying, what word did we predict? So we're not using teacher forcing, we're not saying what was the actual word, because that would be cheating, we're saying what word did we predict. So that now becomes the input to the next loop. So this is how we've turned off teacher forcing. So the kind of exercise for one or more of you guys this week, if you're interested, is to do the thing I told you earlier, which is to kind of combine this with the training loop and make the training loop as it goes through the epochs gradually move from always using teacher forcing to over time randomly using it less and less and less until at the very end it never uses teacher forcing and always uses this. If you get that working, which won't take you long, I think it's pretty straightforward, you'll get better test results than I'm showing you here. Anyway let's test it. So to do French to English, we're basically going to take our French sentence, turn it into a list of IDs by tokenizing it and turning it into IDs, padding it with zeros, all that evaluate function I just showed you, and then join it together, and there it is. So this was the correct English, this was the French we were given, and this was our prediction. So it's not the same, but it's still correct as a speaker. So that's looking pretty good. So there it is, there's translation. I guess there's a couple of things I wanted to briefly mention. One is, these are all stuff that you guys can play with if you're interested. This decoding loop, there's much better ways to do it. What I'm doing here by taking the top one every time is I'm assuming in the decoder that the top prediction is the correct one. But what if two words were nearly 50-50. This is 51%, this one is 49%. And I go, oh it's definitely the 51%. That might be a bad idea. So I'm going to actually steal some slides from Graham Newbig from the NARA Institute of Science Technology, who has shown a fantastic simple example of what you could do instead. So he's doing something slightly different, which is to say, what if we had a sentence like natural language processing bracket, NLP bracket, and your job was to figure out not how to translate it, but to figure out what part of speech each of those things were. These are weird letters, N is noun, J is adjective, VB is verb, left bracket, right bracket. So the correct answer is that natural is an adjective, language is a noun, processing is a noun, and so forth. This would be the correct path through these options. Now how would you create this path? Well, you could start out by figuring out how likely natural is in language in general to be a noun versus an adjective versus a verb and so forth. And then having done that, for every single one of those, you could figure out how likely it is for every one of these to then be a noun, and then to be an adjective, and then to be a verb, and so forth. And you could keep repeating this process, adding up the log predictions all the way along, all the way to the end, and pick the path which was the best. Now the problem is of course, that's basically 5 times 5 times 5, if you already have 5 choices, exponentially complex. And remember, in our case we're not picking from 5 things, we're picking from 40,000 or 20,000 or whatever, the vocabulary of our French language things. This is called the Viterbi algorithm. So the Viterbi algorithm for machine translation is NP-complete. If you haven't come across that before, it basically means forget it. So let's not do that, but I'm sure you can see how obvious the answer is. Rather than doing the Viterbi algorithm, why don't we just pick the top few hypotheses so far. So here are the scores for what is the word natural. It's probably not a left bracket, it's probably not a right bracket. It might be one of these. Let's assume it's one of the top 3. So given it's one of the top 3, what might be next? And again, let's just pick the top 3 combinations. And keep going through that. This is called BeamSearch. In practice, every state-of-the-art algorithm for neural language translation uses this for decoding. Now writing this, again, it's going to be less than a screen of code. I haven't written that. Why not go write it this week? Add it to this code, add BeamSearch. Here's the entire pseudo code. I'm sure you could write it in probably less code than that. So there's BeamSearch, there's one thing to mention. Question from the audience. Do you know if there are any training methods that capture the fact that what is the population of Canada and what is Canada's population are very nearly the same? Answer that question. No, I don't. But I'm not sure it even matters because on average, a better system will be one which translates those 50% of the time into one versus the other. The best translator approach I don't think is going to vary depending on the answer to that question, so I'm not sure it's that important. Could we translate between Chinese and English using this same method? Yes we can, but it would be better if we used a technique I'm going to show you next. So the technique I'm going to show you next is described in this paper, Neural Machine Translation of Rare Words with Subword Units. So interestingly, this actually came up today when I was chatting to Brad, and Brad was asking me how do I create an analysis of people's tweets using their particular vocabulary, but make it not fall apart if they use some word in the future that I haven't seen before. And that's a very similar question to how do I translate language when somebody uses a word I haven't seen before. Or more generally, maybe I don't want to use 160,000 words in my vocabulary. That's a huge embedding matrix, it takes a lot of memory, it takes a lot of time, it's going to be hard to train. So what do you do? So the answer is you use something called BPPE, which is basically an encoder. What it's going to do is it's going to take a sentence like, hello, I am Jeremy, and it's going to basically say, I'm going to try and turn this into a list of tokens that aren't necessarily the same as the words. So the first thing I'm going to do is I'm going to look at every pair of letters, HE, D, L, LL, LL, and so forth, for my whole training set. I'm going to say which pair of letters is the most common. So maybe the answer is ER. So I take those two and I turn them into a single entity called ER. So we start off with every character separate, so we're now going to combine these into a single entity called ER. And then I go through and I do that again, and maybe the next answer is that actually AM appears a lot. So then I'm going to turn that into AM. And so maybe the next thing is actually ERE is the next most common thing I can find. So now we take this and replace it with ERE. And you can keep doing this. You never cross word boundaries. So in theory we could do this forever until we end up eventually with the words again. But instead what happens with this BPE encoder is you provide a single parameter which is what is the maximum number of combiners you want to do. A common default would be like 10,000. So at the end of that, you're going to end up with 10,000 sub-word sequences. So we'll end up turning this sentence into something like H-E-L-L-O. And then there's a special end of word, so space, I space, AM space, maybe it'll then be like J, and then E-R-E, and then M-Y, something like that. Now the cool thing is that you can do this by going to this GitHub site and downloading this and running it on your file of pros, and it will spit out the exact same thing, but it'll stick at space between every one of these BPE codes. So in other words, to use this with what I just showed you, you don't have to write any code. You can just take the English and the French sentences, run it through this and it will spit out BPE encoded versions of those. Having said that, I think maybe the optimal approach would be to actually write something which first of all figured out which are the most common 20,000 words or 10,000 words and then left those words alone and maybe only run the BPE encoding on the things that are truly rare words. Because sometimes BPE encoding actually splits things up in ways that isn't quite what you want. So this is a super important technique. So for those of you that speak Chinese, you know that it's actually not at all clear where words begin and end. Not only are there no words, but grammatically in Chinese, one example would be you can have a sequence of two verbs, which basically the second verb or adjective describes the result of the first one. And so you could treat that as a single word, a perfectly reasonable thing to do, or you could treat it as three words. That would also be a perfectly reasonable thing to do, and there's no right answer. This kind of weird stuff happens all the time in Chinese. You can insert the character nir into the middle of a two-character verb and it turns that into a new word, which means that that thing can't be done. Is this a new word, is it now three words? It's very hard to tell. So instead, if you use BPE with Chinese, you can kind of tokenize it in a way that's entirely statistical. So I think that would be the answer to that question. So I want to leave translation there because I desperately wanted to insert a discussion about segmentation. We kept on talking about segmentation. I really kind of realized in the last week how exciting a particular path of segmentation has turned out to be, and I really, really wanted to explain it to you before the end of the course. So I'm going to do half of it today and half of it next week if we can. So let me show you why this is exciting. So segmentation is about taking something like this and turning it into something like this, where every color represents a thing. So this kind of pink is road, this light purple is line, this blue is footpath, this purple is car, this red is building, and so forth. So this is quite a challenging thing to do, but it's really important for anything that's going to understand the world, react to it. So clearly robotics and self-driving cars absolutely have to be able to do this very quickly and very accurately. But really a whole range of computer vision problems need to be able to do this. For example, last week we saw that hackathon-winning entry from a couple of our students that was able to say, take this cat and blur it out, or apply this style transfer to this cat. So you need a way of being able to know exactly where the cat is. And for something like, one of the things they showed was remove the background. Now you actually need to be able to do a really good job of segmenting out the cat, because if you get it slightly wrong, then you remove the background. You often see this when people use Photoshop badly. You'll end up with the bit between the ears that's still there, or their fur looks really spiky. So if you want to create the next generation Photoshop, you need to be really good at this. So there's lots of reasons to be really good at this. Now it turns out that there's a fantastic way of doing this with a fantastic name called the 100-layer tiramisu. And the 100-layer tiramisu is a fully convolutional dense net. And so therefore, we're not going to look at this yet. Instead we're going to look at the dense net, because we can't understand the tiramisu without the dense net. So here is the paper that introduced the dense net. As it turns out, you really need to also know about the dense net for other reasons. Let me show you the reason. It's only recently I fully appreciated this. Here are the results for the dense net. And if you look down here, you'll recognize that it has been compared to genuinely state-of-the-art stuff. So network-in-network, highway network, fractal net, resnet, resnet with stochastic depth, and wide resnet. These are genuinely state-of-the-art architectures. And it's being looked at on some heavily studied data sets. This is CyPhar 10, this is CyPhar 100. Plus is with data augmentation, plus is without data augmentation. So CyPhar 100, the previous state-of-the-art, this is massively well studied, was 28. And that itself was way above everybody else. This paper got 19.5%. You guys have seen enough of these now to know that you don't decrease by 30-plus percent with computer vision nowadays against state-of-the-art results. So this is like a huge advance. Now the reason this huge advance is important is because specifically these no data augmentation columns, so here's the CyPhar 10 one, similar thing, going down from 7.3 to 5.1, 20-30% decrease as well. What these represent, these without data augmentation columns, they basically represent the performance of this data set on a limited amount of data. If you're not using data augmentation, you're basically forcing it to have to work with less data. And I know that a huge number of you are wanting to build stuff where you don't have much data. So if you're one of those people, you definitely need to use DenseNet. At this stage, DenseNet is by far the state-of-the-art result for data sets where you don't have much data. So I want to teach you about DenseNet for two reasons. The first is so that next week we can learn about the Tiramisu, 100 layer Tiramisu, but also so that we can find out how to create way, way, way, way better computer vision models where you have limited data. So let's learn how to do this. And I can actually describe it in a simple sentence, a single sentence. A DenseNet is a ResNet where you replace addition with concatenation. That's actually the entirety of what a DenseNet is. But understanding what it is and why it works is more important. So let's remind ourselves about ResNet. We've looked at this many times, but there's no harm in just reminding ourselves. With ResNet, we have some input, we put it through a convolution to get some activations, and another convolution to get some activations. And we also have the identity. And this is addition. So basically we end up with our layer T plus 1 equals some function, the convolutions that is, of a layer T plus layer T itself. And then to remind you, what I've normally shown after that is to say our function equals the difference. So it's basically calculating a residual, a function that can find the error. So every time we look at ResNet, we look at it this way. So what if we do exactly the same thing, but we replace that with concatenate, drawing them together. And remember, this is just one block. So what that means is that after the first layer, we have both the result of some convolutions and the original input. We literally copied it and concatenated it. And then after the second layer, we've got some convolutions on convolutions and the original first layer of convolutions and the original input. And furthermore, that second layer of convolutions was itself operating on this concat. So that second layer of convolutions was operating both on the original data as well as on the outcome of the first set of convolutions. When people draw dense nets, they tend to draw it like this. They show every layer going to every layer after it. Now I didn't define it that way because it's much easier in practice to implement it by just saying each layer equals all of the previous layer concatenated with the convolution on top of the previous layer. So you concatenate recursively, means that you always have all of your previous layers there as well. So sometimes people get confused when they see this picture where everything is shown connecting to each other, but then you look at the code and it looks like it's only connected to the previous layer. And that's because the previous layer itself was connected to the previous layer. So because we keep concatenating, the number of filters is getting bigger and bigger. So we're going to have to be careful not to add too many filters at each layer. So the number of filters that are added at each layer, they call the growth rate. And for some reason they use the letter K. That is K for growth rate. They tend to use the values of 12 or 24. In the tiramisu paper, they tend to use the value of 16. So every layer has 12 more filters than the previous layer. And you generally can have like 100 layers. So after 100 layers, you could have 1200 or 2400 filters, which is getting to be quite a lot. Interestingly, although they kind of add up, you actually end up with less parameters than normal. So you can see that this here, CyPhar 10, even this is a state-of-the-art result, and this is with only a million parameters. So this is beating ResNet with 10 million parameters by a third. This is why it's working so well with so little data. I'm not convinced that this is the right approach for, or that it's a massively better approach for ImageNet. This is the picture for ImageNet. This is the important one. The number of flops, so this is the number of floating-point operations, the amount of time it takes your computer to do it, versus the error rate. You can see that DenseNet and ResNet have about the same error rate. DenseNet is about twice as fast, a bit less. It's still better, but it's not massively better. There are actually better architectures than ResNet for ImageNet nowadays. So really, if you're using something that's more of the 100-100,000 images range, you probably want to be using DenseNet. If it's more than 100,000 images, maybe it doesn't matter so much. So interestingly, this turned out to be something that suited Keras really well. These kind of things where you're using standard kinds of layers connected in different ways, Keras is fantastically great for it, as you'll see. So I'm going to use Cypher 10. I just copied and pasted this basically from the Keras.datasets.cypher10. So there's an example of Cypher 10. It's a funnier dataset, just 32x32 pixel images. So that's what a Cypher 10 truck looks like. They're from 0-255, I want to make them 0-1, so I have to apply them by 255. So we're going to try to figure out that this is a truck. We have 10 categories in Cypher 10. So the less code that you write, the less chance that there is for an error. So I try to refactor out everything that happens more than once. So even a simple thing like activation relu, I create a function for. Dropout, if you have dropout, I create a function for. Batch norm with a particular mode and axis is a function for. And then applying relu on top of batch norm is a function for. I always have this in it, this border mode, this L2 regularization, and this dropout, so there's a function for. Then we have batch norm, then relu, then convolution and dropout, so there's a function for. In the paper they also have something called a bottleneck layer. This is a 1x1 convolution where I basically compress the number of filters down into the growth factor times 4. So this is a way of reducing the dimensionality. You'll see here, when they use bottleneck and something called compression, they call it DenseNetBC, you can see that that reduces the number of parameters even more and therefore makes it even more accurate. So generally speaking, you'll probably be wanting to use bottleneck. But it's just a 1x1 conv with in this case 48 filters, so it's reducing the dimensionality through that. So basically what happens is you have a number of dense blocks. Each dense block basically consists of a number of these convolutions followed by concatenation. So go through each layer, convolution, concatenate. And you see how I'm actually replacing x with it. So it's concatenating to itself again and again, so it's getting longer and longer. And then from time to time, I then add in a transition block which is simply a 1x1 convolution followed by a pooling layer. So just like every computer vision model we're used to, a bunch of computation layers and then a bunch of computation, pool, bunch of computation, pool. So this looks like a pretty standard kind of architecture. And then I mentioned compression. The other thing then is in each transition block, you can optionally have this thing called compression, which normally you would set it to 0.5, that just says the number of filters, take however many filters you currently have and multiply it by 0.5. So this is basically something where every time you have a pooling layer, you also decrease the number of filters. So when you have this bottleneck layer and you have this compression of 0.5, that's what DenseNet BC refers to. Question asked. Can we do transfer learning on DenseNet? Absolutely you can. And in fact, PyTorch just came out yesterday or today and has some pre-trained DenseNet models. Having said that, because the size of the activations continues to increase and increase and increase, we again have this problem that there isn't really a nice kind of small number of activations that you could really build on top of. So I'm not sure how practical it would be, but you certainly could. So here is the whole DenseNet model. Basically there's 4 layers which aren't part of the dense blocks, which is that there's an initial 3x3 convolution, there's a global average pooling layer, and there's also a ReLU and a batch norm. So if you subtract the 4 and then divide by the number of dense blocks, that tells you how many layers you need for every block. So we do our 3x3 conv, then we go through each of those layers, create a dense block, and for every one except for the last layer, we also do a transition block. Finally do a batch norm, ReLU, global average pooling, and then a dense layer to create the right number of classes. So that's basically it. If you create that, compile it, and fit it. So I ran it last night, so I couldn't quite run it as long as they did, but I did get to 93.23, which easily beats all of the state-of-the-art. That's somewhere about 6 in the bit. So I didn't have time to run it for as long as they did, but I certainly replicated their state-of-the-art result. As you can see, using nothing but basically two screen pools of Keras. So I read through that pretty quickly, but honestly this is all stuff that you guys are pretty familiar with. So if you read the paper, it's a really easy paper to read, read the code, it's a really easy code to read. If you haven't done much implementing papers with code, this is a great place to start because there's no math in the paper, it's pretty clear, the Keras code is really easy to read, there's no new concepts, so this would be a great way to get started. And as I said, some of the students and I basically started on this on Friday and got it knocked out. So I think this is pretty exciting. It's 9 o'clock. Thanks everybody. Looking forward to Chatting Tuesday during the week. I'll see you next Monday for our last class.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.44, "text": " Welcome. The penultimate lesson. So who has heard of the Google Brain Residency Program?", "tokens": [4027, 13, 440, 3435, 723, 2905, 6898, 13, 407, 567, 575, 2198, 295, 264, 3329, 29783, 5015, 23343, 8338, 30], "temperature": 0.0, "avg_logprob": -0.2786916494369507, "compression_ratio": 1.2689655172413794, "no_speech_prob": 0.025944547727704048}, {"id": 1, "seek": 0, "start": 16.44, "end": 28.16, "text": " It's pretty famous, right? I think it's probably maybe the top program to get into, the hardest", "tokens": [467, 311, 1238, 4618, 11, 558, 30, 286, 519, 309, 311, 1391, 1310, 264, 1192, 1461, 281, 483, 666, 11, 264, 13158], "temperature": 0.0, "avg_logprob": -0.2786916494369507, "compression_ratio": 1.2689655172413794, "no_speech_prob": 0.025944547727704048}, {"id": 2, "seek": 2816, "start": 28.16, "end": 33.04, "text": " deep learning program to get into the world. The reason I mention it is because it so happens", "tokens": [2452, 2539, 1461, 281, 483, 666, 264, 1002, 13, 440, 1778, 286, 2152, 309, 307, 570, 309, 370, 2314], "temperature": 0.0, "avg_logprob": -0.17718563657818417, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.0005783867673017085}, {"id": 3, "seek": 2816, "start": 33.04, "end": 37.36, "text": " one of our students was just accepted. So I'd like everybody to congratulate Sarah Hawker.", "tokens": [472, 295, 527, 1731, 390, 445, 9035, 13, 407, 286, 1116, 411, 2201, 281, 24353, 9519, 9325, 5767, 13], "temperature": 0.0, "avg_logprob": -0.17718563657818417, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.0005783867673017085}, {"id": 4, "seek": 2816, "start": 37.36, "end": 44.480000000000004, "text": " Sarah Hawker is a new Google Brain resident and I thought maybe we could have a brief", "tokens": [9519, 9325, 5767, 307, 257, 777, 3329, 29783, 10832, 293, 286, 1194, 1310, 321, 727, 362, 257, 5353], "temperature": 0.0, "avg_logprob": -0.17718563657818417, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.0005783867673017085}, {"id": 5, "seek": 2816, "start": 44.480000000000004, "end": 48.68, "text": " chat with her to find out about her path to this program.", "tokens": [5081, 365, 720, 281, 915, 484, 466, 720, 3100, 281, 341, 1461, 13], "temperature": 0.0, "avg_logprob": -0.17718563657818417, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.0005783867673017085}, {"id": 6, "seek": 2816, "start": 48.68, "end": 56.24, "text": " Sarah Hawker is a new Google Brain resident and I thought maybe we could have a brief", "tokens": [9519, 9325, 5767, 307, 257, 777, 3329, 29783, 10832, 293, 286, 1194, 1310, 321, 727, 362, 257, 5353], "temperature": 0.0, "avg_logprob": -0.17718563657818417, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.0005783867673017085}, {"id": 7, "seek": 5624, "start": 56.24, "end": 62.400000000000006, "text": " chat with her to find out about her path to this program. I'll throw over the green box,", "tokens": [5081, 365, 720, 281, 915, 484, 466, 720, 3100, 281, 341, 1461, 13, 286, 603, 3507, 670, 264, 3092, 2424, 11], "temperature": 0.0, "avg_logprob": -0.3293758280137006, "compression_ratio": 1.4252873563218391, "no_speech_prob": 0.00032362030469812453}, {"id": 8, "seek": 5624, "start": 62.400000000000006, "end": 71.4, "text": " try not to hurt anybody. Hello, congratulations, well done, you must be pretty excited.", "tokens": [853, 406, 281, 4607, 4472, 13, 2425, 11, 13568, 11, 731, 1096, 11, 291, 1633, 312, 1238, 2919, 13], "temperature": 0.0, "avg_logprob": -0.3293758280137006, "compression_ratio": 1.4252873563218391, "no_speech_prob": 0.00032362030469812453}, {"id": 9, "seek": 5624, "start": 71.4, "end": 78.4, "text": " Very excited. It's almost a blur. I found out last week actually.", "tokens": [4372, 2919, 13, 467, 311, 1920, 257, 14257, 13, 286, 1352, 484, 1036, 1243, 767, 13], "temperature": 0.0, "avg_logprob": -0.3293758280137006, "compression_ratio": 1.4252873563218391, "no_speech_prob": 0.00032362030469812453}, {"id": 10, "seek": 5624, "start": 78.4, "end": 79.4, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3293758280137006, "compression_ratio": 1.4252873563218391, "no_speech_prob": 0.00032362030469812453}, {"id": 11, "seek": 7940, "start": 79.4, "end": 90.2, "text": " So this is a pretty huge thing to get into. So you must be like a machine learning, PhD,", "tokens": [407, 341, 307, 257, 1238, 2603, 551, 281, 483, 666, 13, 407, 291, 1633, 312, 411, 257, 3479, 2539, 11, 14476, 11], "temperature": 0.0, "avg_logprob": -0.2829691724079411, "compression_ratio": 1.4951923076923077, "no_speech_prob": 0.0004568468139041215}, {"id": 12, "seek": 7940, "start": 90.2, "end": 93.2, "text": " been coding for a few decades, so forth, yes?", "tokens": [668, 17720, 337, 257, 1326, 7878, 11, 370, 5220, 11, 2086, 30], "temperature": 0.0, "avg_logprob": -0.2829691724079411, "compression_ratio": 1.4951923076923077, "no_speech_prob": 0.0004568468139041215}, {"id": 13, "seek": 7940, "start": 93.2, "end": 99.52000000000001, "text": " So actually not at all. And I think that's the exciting thing about the whole process", "tokens": [407, 767, 406, 412, 439, 13, 400, 286, 519, 300, 311, 264, 4670, 551, 466, 264, 1379, 1399], "temperature": 0.0, "avg_logprob": -0.2829691724079411, "compression_ratio": 1.4951923076923077, "no_speech_prob": 0.0004568468139041215}, {"id": 14, "seek": 7940, "start": 99.52000000000001, "end": 107.72, "text": " is that really they're trying to find a diverse set of fellows, residents for the year. So", "tokens": [307, 300, 534, 436, 434, 1382, 281, 915, 257, 9521, 992, 295, 35595, 11, 9630, 337, 264, 1064, 13, 407], "temperature": 0.0, "avg_logprob": -0.2829691724079411, "compression_ratio": 1.4951923076923077, "no_speech_prob": 0.0004568468139041215}, {"id": 15, "seek": 10772, "start": 107.72, "end": 113.52, "text": " there's many different buckets. There were definitely PhDs that I sat next to in the", "tokens": [456, 311, 867, 819, 32191, 13, 821, 645, 2138, 14476, 82, 300, 286, 3227, 958, 281, 294, 264], "temperature": 0.0, "avg_logprob": -0.23053245544433593, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00039632691186852753}, {"id": 16, "seek": 10772, "start": 113.52, "end": 121.36, "text": " interview room who were well published and incredibly accomplished. There were also undergraduates", "tokens": [4049, 1808, 567, 645, 731, 6572, 293, 6252, 15419, 13, 821, 645, 611, 14295, 27710], "temperature": 0.0, "avg_logprob": -0.23053245544433593, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00039632691186852753}, {"id": 17, "seek": 10772, "start": 121.36, "end": 128.07999999999998, "text": " who this would be their first job after college. And my background was that I've been in industry,", "tokens": [567, 341, 576, 312, 641, 700, 1691, 934, 3859, 13, 400, 452, 3678, 390, 300, 286, 600, 668, 294, 3518, 11], "temperature": 0.0, "avg_logprob": -0.23053245544433593, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00039632691186852753}, {"id": 18, "seek": 10772, "start": 128.07999999999998, "end": 132.8, "text": " I started out in economic consulting, so I did economic modeling and statistics and now", "tokens": [286, 1409, 484, 294, 4836, 23682, 11, 370, 286, 630, 4836, 15983, 293, 12523, 293, 586], "temperature": 0.0, "avg_logprob": -0.23053245544433593, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00039632691186852753}, {"id": 19, "seek": 10772, "start": 132.8, "end": 134.36, "text": " I work as a data scientist.", "tokens": [286, 589, 382, 257, 1412, 12662, 13], "temperature": 0.0, "avg_logprob": -0.23053245544433593, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00039632691186852753}, {"id": 20, "seek": 10772, "start": 134.36, "end": 137.16, "text": " So how long have you been coding for?", "tokens": [407, 577, 938, 362, 291, 668, 17720, 337, 30], "temperature": 0.0, "avg_logprob": -0.23053245544433593, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00039632691186852753}, {"id": 21, "seek": 13716, "start": 137.16, "end": 139.28, "text": " So I've been coding two years.", "tokens": [407, 286, 600, 668, 17720, 732, 924, 13], "temperature": 0.0, "avg_logprob": -0.23049911943454188, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.00019700263510458171}, {"id": 22, "seek": 13716, "start": 139.28, "end": 140.28, "text": " Wow.", "tokens": [3153, 13], "temperature": 0.0, "avg_logprob": -0.23049911943454188, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.00019700263510458171}, {"id": 23, "seek": 13716, "start": 140.28, "end": 142.92, "text": " And also makes me very atypical in this group.", "tokens": [400, 611, 1669, 385, 588, 412, 88, 34061, 294, 341, 1594, 13], "temperature": 0.0, "avg_logprob": -0.23049911943454188, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.00019700263510458171}, {"id": 24, "seek": 13716, "start": 142.92, "end": 149.88, "text": " Yeah, yeah. Rachel told me you had a fan moment when Ian Goodfellow interviewed you, is that", "tokens": [865, 11, 1338, 13, 14246, 1907, 385, 291, 632, 257, 3429, 1623, 562, 19595, 2205, 69, 21348, 19770, 291, 11, 307, 300], "temperature": 0.0, "avg_logprob": -0.23049911943454188, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.00019700263510458171}, {"id": 25, "seek": 13716, "start": 149.88, "end": 150.88, "text": " true?", "tokens": [2074, 30], "temperature": 0.0, "avg_logprob": -0.23049911943454188, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.00019700263510458171}, {"id": 26, "seek": 13716, "start": 150.88, "end": 159.04, "text": " So I tried to keep it undercover, but it was so thrilling. I think that was the strangest", "tokens": [407, 286, 3031, 281, 1066, 309, 48099, 11, 457, 309, 390, 370, 39347, 13, 286, 519, 300, 390, 264, 24404, 377], "temperature": 0.0, "avg_logprob": -0.23049911943454188, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.00019700263510458171}, {"id": 27, "seek": 13716, "start": 159.04, "end": 163.48, "text": " thing about the whole process was that you're so nervous the whole time and you're so excited,", "tokens": [551, 466, 264, 1379, 1399, 390, 300, 291, 434, 370, 6296, 264, 1379, 565, 293, 291, 434, 370, 2919, 11], "temperature": 0.0, "avg_logprob": -0.23049911943454188, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.00019700263510458171}, {"id": 28, "seek": 16348, "start": 163.48, "end": 169.92, "text": " but also you're talking to these people whose research you followed for years. And so you're", "tokens": [457, 611, 291, 434, 1417, 281, 613, 561, 6104, 2132, 291, 6263, 337, 924, 13, 400, 370, 291, 434], "temperature": 0.0, "avg_logprob": -0.27178009269163783, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.00013723726442549378}, {"id": 29, "seek": 16348, "start": 169.92, "end": 177.72, "text": " just, my Google, the first hangout interview was actually with Hugo Larachelle and I started", "tokens": [445, 11, 452, 3329, 11, 264, 700, 3967, 346, 4049, 390, 767, 365, 32504, 11569, 608, 4434, 293, 286, 1409], "temperature": 0.0, "avg_logprob": -0.27178009269163783, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.00013723726442549378}, {"id": 30, "seek": 16348, "start": 177.72, "end": 182.44, "text": " learning about neural nets, watching his MOOCs. So it was just, it feels surreal. And I think", "tokens": [2539, 466, 18161, 36170, 11, 1976, 702, 49197, 33290, 13, 407, 309, 390, 445, 11, 309, 3417, 32513, 13, 400, 286, 519], "temperature": 0.0, "avg_logprob": -0.27178009269163783, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.00013723726442549378}, {"id": 31, "seek": 16348, "start": 182.44, "end": 190.07999999999998, "text": " that's part of the intent of the program is to take very, very different people at different", "tokens": [300, 311, 644, 295, 264, 8446, 295, 264, 1461, 307, 281, 747, 588, 11, 588, 819, 561, 412, 819], "temperature": 0.0, "avg_logprob": -0.27178009269163783, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.00013723726442549378}, {"id": 32, "seek": 16348, "start": 190.07999999999998, "end": 192.07999999999998, "text": " parts of their career.", "tokens": [3166, 295, 641, 3988, 13], "temperature": 0.0, "avg_logprob": -0.27178009269163783, "compression_ratio": 1.592741935483871, "no_speech_prob": 0.00013723726442549378}, {"id": 33, "seek": 19208, "start": 192.08, "end": 199.08, "text": " So you told me that before you started this course, your awareness of neural nets was", "tokens": [407, 291, 1907, 385, 300, 949, 291, 1409, 341, 1164, 11, 428, 8888, 295, 18161, 36170, 390], "temperature": 0.0, "avg_logprob": -0.14095240200267117, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00020289060194045305}, {"id": 34, "seek": 19208, "start": 199.08, "end": 205.44, "text": " a little bit of theory from a few blog posts. I mean, so how much is going through this", "tokens": [257, 707, 857, 295, 5261, 490, 257, 1326, 6968, 12300, 13, 286, 914, 11, 370, 577, 709, 307, 516, 807, 341], "temperature": 0.0, "avg_logprob": -0.14095240200267117, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00020289060194045305}, {"id": 35, "seek": 19208, "start": 205.44, "end": 211.52, "text": " course kind of helped you get the skills you needed to get into this program?", "tokens": [1164, 733, 295, 4254, 291, 483, 264, 3942, 291, 2978, 281, 483, 666, 341, 1461, 30], "temperature": 0.0, "avg_logprob": -0.14095240200267117, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00020289060194045305}, {"id": 36, "seek": 19208, "start": 211.52, "end": 220.12, "text": " So I wasn't coding or implementing architecture before this course. So I'd done, to be fair,", "tokens": [407, 286, 2067, 380, 17720, 420, 18114, 9482, 949, 341, 1164, 13, 407, 286, 1116, 1096, 11, 281, 312, 3143, 11], "temperature": 0.0, "avg_logprob": -0.14095240200267117, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00020289060194045305}, {"id": 37, "seek": 22012, "start": 220.12, "end": 227.96, "text": " I was fascinated by deep learning for a while. So I was pretty deep in the theory. But I", "tokens": [286, 390, 24597, 538, 2452, 2539, 337, 257, 1339, 13, 407, 286, 390, 1238, 2452, 294, 264, 5261, 13, 583, 286], "temperature": 0.0, "avg_logprob": -0.24539892297042043, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.00025659147650003433}, {"id": 38, "seek": 22012, "start": 227.96, "end": 232.56, "text": " think part of why it was blog posts was that there wasn't a coherent body of work on the", "tokens": [519, 644, 295, 983, 309, 390, 6968, 12300, 390, 300, 456, 2067, 380, 257, 36239, 1772, 295, 589, 322, 264], "temperature": 0.0, "avg_logprob": -0.24539892297042043, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.00025659147650003433}, {"id": 39, "seek": 22012, "start": 232.56, "end": 235.56, "text": " subject. And so now-", "tokens": [3983, 13, 400, 370, 586, 12], "temperature": 0.0, "avg_logprob": -0.24539892297042043, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.00025659147650003433}, {"id": 40, "seek": 22012, "start": 235.56, "end": 236.56, "text": " But now that's Ian's book.", "tokens": [583, 586, 300, 311, 19595, 311, 1446, 13], "temperature": 0.0, "avg_logprob": -0.24539892297042043, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.00025659147650003433}, {"id": 41, "seek": 22012, "start": 236.56, "end": 241.56, "text": " Now that's Ian's book, which I have read back to back. I mean, it's really a fantastic treatment.", "tokens": [823, 300, 311, 19595, 311, 1446, 11, 597, 286, 362, 1401, 646, 281, 646, 13, 286, 914, 11, 309, 311, 534, 257, 5456, 5032, 13], "temperature": 0.0, "avg_logprob": -0.24539892297042043, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.00025659147650003433}, {"id": 42, "seek": 22012, "start": 241.56, "end": 246.56, "text": " But I think that what this course offers, and I sense like many of you may have had the", "tokens": [583, 286, 519, 300, 437, 341, 1164, 7736, 11, 293, 286, 2020, 411, 867, 295, 291, 815, 362, 632, 264], "temperature": 0.0, "avg_logprob": -0.24539892297042043, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.00025659147650003433}, {"id": 43, "seek": 24656, "start": 246.56, "end": 253.56, "text": " same experience, is the implementation, which was very new to me and definitely something", "tokens": [912, 1752, 11, 307, 264, 11420, 11, 597, 390, 588, 777, 281, 385, 293, 2138, 746], "temperature": 0.0, "avg_logprob": -0.24393627166748047, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.00010068190022138879}, {"id": 44, "seek": 24656, "start": 253.56, "end": 255.56, "text": " I don't think I would have found in a different forum.", "tokens": [286, 500, 380, 519, 286, 576, 362, 1352, 294, 257, 819, 17542, 13], "temperature": 0.0, "avg_logprob": -0.24393627166748047, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.00010068190022138879}, {"id": 45, "seek": 24656, "start": 255.56, "end": 261.12, "text": " Yeah. I mean, I know from talking to Rachel that some of the interview process was like-", "tokens": [865, 13, 286, 914, 11, 286, 458, 490, 1417, 281, 14246, 300, 512, 295, 264, 4049, 1399, 390, 411, 12], "temperature": 0.0, "avg_logprob": -0.24393627166748047, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.00010068190022138879}, {"id": 46, "seek": 24656, "start": 261.12, "end": 264.56, "text": " it actually sounded, the question sounded like it came straight out of the course. It", "tokens": [309, 767, 17714, 11, 264, 1168, 17714, 411, 309, 1361, 2997, 484, 295, 264, 1164, 13, 467], "temperature": 0.0, "avg_logprob": -0.24393627166748047, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.00010068190022138879}, {"id": 47, "seek": 24656, "start": 264.56, "end": 269.64, "text": " was like about how to do transfer learning and all this kind of practical application", "tokens": [390, 411, 466, 577, 281, 360, 5003, 2539, 293, 439, 341, 733, 295, 8496, 3861], "temperature": 0.0, "avg_logprob": -0.24393627166748047, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.00010068190022138879}, {"id": 48, "seek": 24656, "start": 269.64, "end": 270.64, "text": " stuff it seemed like.", "tokens": [1507, 309, 6576, 411, 13], "temperature": 0.0, "avg_logprob": -0.24393627166748047, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.00010068190022138879}, {"id": 49, "seek": 27064, "start": 270.64, "end": 277.64, "text": " Yeah. At least in my case, it was a fairly consistent breakdown throughout the process", "tokens": [865, 13, 1711, 1935, 294, 452, 1389, 11, 309, 390, 257, 6457, 8398, 18188, 3710, 264, 1399], "temperature": 0.0, "avg_logprob": -0.20045077495085886, "compression_ratio": 1.5422222222222222, "no_speech_prob": 2.74947324214736e-05}, {"id": 50, "seek": 27064, "start": 277.64, "end": 285.64, "text": " of theory, math questions, and then project experience. And with the project, they're", "tokens": [295, 5261, 11, 5221, 1651, 11, 293, 550, 1716, 1752, 13, 400, 365, 264, 1716, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.20045077495085886, "compression_ratio": 1.5422222222222222, "no_speech_prob": 2.74947324214736e-05}, {"id": 51, "seek": 27064, "start": 285.64, "end": 292.64, "text": " really trying to gauge implementation and do a knowledge of basically concepts that", "tokens": [534, 1382, 281, 17924, 11420, 293, 360, 257, 3601, 295, 1936, 10392, 300], "temperature": 0.0, "avg_logprob": -0.20045077495085886, "compression_ratio": 1.5422222222222222, "no_speech_prob": 2.74947324214736e-05}, {"id": 52, "seek": 27064, "start": 292.64, "end": 299.64, "text": " we've covered very thoroughly in this class. So overfitting, but overfitting distribution,", "tokens": [321, 600, 5343, 588, 17987, 294, 341, 1508, 13, 407, 670, 69, 2414, 11, 457, 670, 69, 2414, 7316, 11], "temperature": 0.0, "avg_logprob": -0.20045077495085886, "compression_ratio": 1.5422222222222222, "no_speech_prob": 2.74947324214736e-05}, {"id": 53, "seek": 29964, "start": 299.64, "end": 309.64, "text": " understanding how to address data distribution differences. So that part, I think that that", "tokens": [3701, 577, 281, 2985, 1412, 7316, 7300, 13, 407, 300, 644, 11, 286, 519, 300, 300], "temperature": 0.0, "avg_logprob": -0.14140765368938446, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.00026756746228784323}, {"id": 54, "seek": 29964, "start": 309.64, "end": 318.64, "text": " was standard throughout the whole process, was that they expect you to- they expect their", "tokens": [390, 3832, 3710, 264, 1379, 1399, 11, 390, 300, 436, 2066, 291, 281, 12, 436, 2066, 641], "temperature": 0.0, "avg_logprob": -0.14140765368938446, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.00026756746228784323}, {"id": 55, "seek": 29964, "start": 318.64, "end": 324.64, "text": " candidates to have this holistic approach of both coding, but also a strong foundation", "tokens": [11255, 281, 362, 341, 30334, 3109, 295, 1293, 17720, 11, 457, 611, 257, 2068, 7030], "temperature": 0.0, "avg_logprob": -0.14140765368938446, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.00026756746228784323}, {"id": 56, "seek": 29964, "start": 324.64, "end": 327.64, "text": " in the underpinnings.", "tokens": [294, 264, 833, 17836, 24451, 13], "temperature": 0.0, "avg_logprob": -0.14140765368938446, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.00026756746228784323}, {"id": 57, "seek": 32764, "start": 327.64, "end": 333.64, "text": " So if I could give a pitch for your program, one of the amazing things that Sarah's done", "tokens": [407, 498, 286, 727, 976, 257, 7293, 337, 428, 1461, 11, 472, 295, 264, 2243, 721, 300, 9519, 311, 1096], "temperature": 0.0, "avg_logprob": -0.07049898097389623, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.000397128751501441}, {"id": 58, "seek": 32764, "start": 333.64, "end": 340.64, "text": " in her extraordinary life is to create a organization called Delta Analytics, which is applying", "tokens": [294, 720, 10581, 993, 307, 281, 1884, 257, 4475, 1219, 18183, 25944, 11, 597, 307, 9275], "temperature": 0.0, "avg_logprob": -0.07049898097389623, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.000397128751501441}, {"id": 59, "seek": 32764, "start": 340.64, "end": 346.64, "text": " data science for social impact projects. Is there room for people who might be interested", "tokens": [1412, 3497, 337, 2093, 2712, 4455, 13, 1119, 456, 1808, 337, 561, 567, 1062, 312, 3102], "temperature": 0.0, "avg_logprob": -0.07049898097389623, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.000397128751501441}, {"id": 60, "seek": 32764, "start": 346.64, "end": 351.64, "text": " to contact you if they're interested in doing that kind of work?", "tokens": [281, 3385, 291, 498, 436, 434, 3102, 294, 884, 300, 733, 295, 589, 30], "temperature": 0.0, "avg_logprob": -0.07049898097389623, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.000397128751501441}, {"id": 61, "seek": 35164, "start": 351.64, "end": 357.64, "text": " Absolutely. I would honestly, I think that the caliber of this course, I would be so", "tokens": [7021, 13, 286, 576, 6095, 11, 286, 519, 300, 264, 41946, 295, 341, 1164, 11, 286, 576, 312, 370], "temperature": 0.0, "avg_logprob": -0.10765302181243896, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.00010046610987046733}, {"id": 62, "seek": 35164, "start": 357.64, "end": 363.64, "text": " thrilled to work with anyone from this course going forward. So the way that it works, I'll", "tokens": [18744, 281, 589, 365, 2878, 490, 341, 1164, 516, 2128, 13, 407, 264, 636, 300, 309, 1985, 11, 286, 603], "temperature": 0.0, "avg_logprob": -0.10765302181243896, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.00010046610987046733}, {"id": 63, "seek": 35164, "start": 363.64, "end": 369.64, "text": " just- we pair with nonprofits all over the world, and you work with engineers and data", "tokens": [445, 12, 321, 6119, 365, 42851, 439, 670, 264, 1002, 11, 293, 291, 589, 365, 11955, 293, 1412], "temperature": 0.0, "avg_logprob": -0.10765302181243896, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.00010046610987046733}, {"id": 64, "seek": 35164, "start": 369.64, "end": 376.64, "text": " scientists over a six-month project. And right now, we are involved with eight different", "tokens": [7708, 670, 257, 2309, 12, 23534, 1716, 13, 400, 558, 586, 11, 321, 366, 3288, 365, 3180, 819], "temperature": 0.0, "avg_logprob": -0.10765302181243896, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.00010046610987046733}, {"id": 65, "seek": 35164, "start": 376.64, "end": 380.64, "text": " nonprofits, and then we'll start a new cycle towards the end of this year.", "tokens": [42851, 11, 293, 550, 321, 603, 722, 257, 777, 6586, 3030, 264, 917, 295, 341, 1064, 13], "temperature": 0.0, "avg_logprob": -0.10765302181243896, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.00010046610987046733}, {"id": 66, "seek": 38064, "start": 380.64, "end": 387.64, "text": " Congratulations again, I think we're all so proud of you.", "tokens": [9694, 797, 11, 286, 519, 321, 434, 439, 370, 4570, 295, 291, 13], "temperature": 0.0, "avg_logprob": -0.1199257771174113, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.00012144081847509369}, {"id": 67, "seek": 38064, "start": 387.64, "end": 398.64, "text": " Talking of great work, I also wanted to mention the work of another student who also has the", "tokens": [22445, 295, 869, 589, 11, 286, 611, 1415, 281, 2152, 264, 589, 295, 1071, 3107, 567, 611, 575, 264], "temperature": 0.0, "avg_logprob": -0.1199257771174113, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.00012144081847509369}, {"id": 68, "seek": 38064, "start": 398.64, "end": 404.8, "text": " great challenge of having to deal with being a fast AI intern, Brad. Brad took up the challenge", "tokens": [869, 3430, 295, 1419, 281, 2028, 365, 885, 257, 2370, 7318, 2154, 11, 11895, 13, 11895, 1890, 493, 264, 3430], "temperature": 0.0, "avg_logprob": -0.1199257771174113, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.00012144081847509369}, {"id": 69, "seek": 40480, "start": 404.8, "end": 411.6, "text": " I set up two weeks ago in implementing cyclical learning regs. You might remember that the", "tokens": [286, 992, 493, 732, 3259, 2057, 294, 18114, 19474, 804, 2539, 1121, 82, 13, 509, 1062, 1604, 300, 264], "temperature": 0.0, "avg_logprob": -0.21009035612407484, "compression_ratio": 1.5564853556485356, "no_speech_prob": 6.604264490306377e-05}, {"id": 70, "seek": 40480, "start": 411.6, "end": 419.68, "text": " cyclical learning regs paper showed faster training of neural nets and also more automated.", "tokens": [19474, 804, 2539, 1121, 82, 3035, 4712, 4663, 3097, 295, 18161, 36170, 293, 611, 544, 18473, 13], "temperature": 0.0, "avg_logprob": -0.21009035612407484, "compression_ratio": 1.5564853556485356, "no_speech_prob": 6.604264490306377e-05}, {"id": 71, "seek": 40480, "start": 419.68, "end": 426.16, "text": " Brad actually had it coded up super quickly. So if you jump on the forum, you'll find a", "tokens": [11895, 767, 632, 309, 34874, 493, 1687, 2661, 13, 407, 498, 291, 3012, 322, 264, 17542, 11, 291, 603, 915, 257], "temperature": 0.0, "avg_logprob": -0.21009035612407484, "compression_ratio": 1.5564853556485356, "no_speech_prob": 6.604264490306377e-05}, {"id": 72, "seek": 40480, "start": 426.16, "end": 429.88, "text": " link to it there. Maybe Brad you can add it to our Wiki thread. Do we have a Wiki thread", "tokens": [2113, 281, 309, 456, 13, 2704, 11895, 291, 393, 909, 309, 281, 527, 35892, 7207, 13, 1144, 321, 362, 257, 35892, 7207], "temperature": 0.0, "avg_logprob": -0.21009035612407484, "compression_ratio": 1.5564853556485356, "no_speech_prob": 6.604264490306377e-05}, {"id": 73, "seek": 40480, "start": 429.88, "end": 430.88, "text": " yet, Rachel?", "tokens": [1939, 11, 14246, 30], "temperature": 0.0, "avg_logprob": -0.21009035612407484, "compression_ratio": 1.5564853556485356, "no_speech_prob": 6.604264490306377e-05}, {"id": 74, "seek": 43088, "start": 430.88, "end": 439.44, "text": " Could you create one? You just click at the bottom and say make wiki.", "tokens": [7497, 291, 1884, 472, 30, 509, 445, 2052, 412, 264, 2767, 293, 584, 652, 261, 9850, 13], "temperature": 0.0, "avg_logprob": -0.18994808197021484, "compression_ratio": 1.543778801843318, "no_speech_prob": 3.3212414564331993e-05}, {"id": 75, "seek": 43088, "start": 439.44, "end": 443.71999999999997, "text": " Honestly this, as I mentioned when we first taught this, this paper has been so little", "tokens": [12348, 341, 11, 382, 286, 2835, 562, 321, 700, 5928, 341, 11, 341, 3035, 575, 668, 370, 707], "temperature": 0.0, "avg_logprob": -0.18994808197021484, "compression_ratio": 1.543778801843318, "no_speech_prob": 3.3212414564331993e-05}, {"id": 76, "seek": 43088, "start": 443.71999999999997, "end": 449.56, "text": " looked at, we don't yet really know. I have a feeling it's going to turn out that this", "tokens": [2956, 412, 11, 321, 500, 380, 1939, 534, 458, 13, 286, 362, 257, 2633, 309, 311, 516, 281, 1261, 484, 300, 341], "temperature": 0.0, "avg_logprob": -0.18994808197021484, "compression_ratio": 1.543778801843318, "no_speech_prob": 3.3212414564331993e-05}, {"id": 77, "seek": 43088, "start": 449.56, "end": 454.56, "text": " is the best way to train every kind of neural net in every kind of situation. So I've asked", "tokens": [307, 264, 1151, 636, 281, 3847, 633, 733, 295, 18161, 2533, 294, 633, 733, 295, 2590, 13, 407, 286, 600, 2351], "temperature": 0.0, "avg_logprob": -0.18994808197021484, "compression_ratio": 1.543778801843318, "no_speech_prob": 3.3212414564331993e-05}, {"id": 78, "seek": 45456, "start": 454.56, "end": 461.0, "text": " Brad to try as many possible experiments here and he's going to try and keep automating", "tokens": [11895, 281, 853, 382, 867, 1944, 12050, 510, 293, 415, 311, 516, 281, 853, 293, 1066, 3553, 990], "temperature": 0.0, "avg_logprob": -0.19757165714186065, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.2218943993502762e-05}, {"id": 79, "seek": 45456, "start": 461.0, "end": 465.24, "text": " it. This is exactly the kind of thing Rachel and I at Fast AI are trying to do if this", "tokens": [309, 13, 639, 307, 2293, 264, 733, 295, 551, 14246, 293, 286, 412, 15968, 7318, 366, 1382, 281, 360, 498, 341], "temperature": 0.0, "avg_logprob": -0.19757165714186065, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.2218943993502762e-05}, {"id": 80, "seek": 45456, "start": 465.24, "end": 471.24, "text": " really works out, is get rid of the whole question of how to set learning regs, which", "tokens": [534, 1985, 484, 11, 307, 483, 3973, 295, 264, 1379, 1168, 295, 577, 281, 992, 2539, 1121, 82, 11, 597], "temperature": 0.0, "avg_logprob": -0.19757165714186065, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.2218943993502762e-05}, {"id": 81, "seek": 45456, "start": 471.24, "end": 477.24, "text": " currently is such an artisanal thing.", "tokens": [4362, 307, 1270, 364, 1523, 14804, 304, 551, 13], "temperature": 0.0, "avg_logprob": -0.19757165714186065, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.2218943993502762e-05}, {"id": 82, "seek": 45456, "start": 477.24, "end": 482.92, "text": " So congratulations Brad on getting that working. It worked really well in Keras I think with", "tokens": [407, 13568, 11895, 322, 1242, 300, 1364, 13, 467, 2732, 534, 731, 294, 591, 6985, 286, 519, 365], "temperature": 0.0, "avg_logprob": -0.19757165714186065, "compression_ratio": 1.6024590163934427, "no_speech_prob": 1.2218943993502762e-05}, {"id": 83, "seek": 48292, "start": 482.92, "end": 487.68, "text": " the callbacks they had. The code ends up being quite neat as well.", "tokens": [264, 818, 17758, 436, 632, 13, 440, 3089, 5314, 493, 885, 1596, 10654, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2656984040231416, "compression_ratio": 1.5180722891566265, "no_speech_prob": 5.920652984059416e-05}, {"id": 84, "seek": 48292, "start": 487.68, "end": 497.96000000000004, "text": " I had this comment last time, but I'm so excited about these callbacks in Keras. You can set", "tokens": [286, 632, 341, 2871, 1036, 565, 11, 457, 286, 478, 370, 2919, 466, 613, 818, 17758, 294, 591, 6985, 13, 509, 393, 992], "temperature": 0.0, "avg_logprob": -0.2656984040231416, "compression_ratio": 1.5180722891566265, "no_speech_prob": 5.920652984059416e-05}, {"id": 85, "seek": 48292, "start": 497.96000000000004, "end": 508.40000000000003, "text": " callbacks to stop training if it stops improving, and then you can enter another cycle to do", "tokens": [818, 17758, 281, 1590, 3097, 498, 309, 10094, 11470, 11, 293, 550, 291, 393, 3242, 1071, 6586, 281, 360], "temperature": 0.0, "avg_logprob": -0.2656984040231416, "compression_ratio": 1.5180722891566265, "no_speech_prob": 5.920652984059416e-05}, {"id": 86, "seek": 50840, "start": 508.4, "end": 512.84, "text": " different things and then kind of oscillate between different ways with the callbacks", "tokens": [819, 721, 293, 550, 733, 295, 18225, 473, 1296, 819, 2098, 365, 264, 818, 17758], "temperature": 0.0, "avg_logprob": -0.3680255245155012, "compression_ratio": 1.5654450261780104, "no_speech_prob": 3.120105611742474e-05}, {"id": 87, "seek": 50840, "start": 512.84, "end": 517.0799999999999, "text": " and fully automate this whole process. So I kind of implemented similar zigzagging things", "tokens": [293, 4498, 31605, 341, 1379, 1399, 13, 407, 286, 733, 295, 12270, 2531, 38290, 43886, 3249, 721], "temperature": 0.0, "avg_logprob": -0.3680255245155012, "compression_ratio": 1.5654450261780104, "no_speech_prob": 3.120105611742474e-05}, {"id": 88, "seek": 50840, "start": 517.0799999999999, "end": 522.64, "text": " that you showed me. So Keras is really cool in that. Basically 10 lines of code will bring", "tokens": [300, 291, 4712, 385, 13, 407, 591, 6985, 307, 534, 1627, 294, 300, 13, 8537, 1266, 3876, 295, 3089, 486, 1565], "temperature": 0.0, "avg_logprob": -0.3680255245155012, "compression_ratio": 1.5654450261780104, "no_speech_prob": 3.120105611742474e-05}, {"id": 89, "seek": 50840, "start": 522.64, "end": 525.72, "text": " you so far with these callbacks.", "tokens": [291, 370, 1400, 365, 613, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.3680255245155012, "compression_ratio": 1.5654450261780104, "no_speech_prob": 3.120105611742474e-05}, {"id": 90, "seek": 52572, "start": 525.72, "end": 540.28, "text": " Remember that Keras has great source code. So if you look in your Anaconda, lib, Python,", "tokens": [5459, 300, 591, 6985, 575, 869, 4009, 3089, 13, 407, 498, 291, 574, 294, 428, 1107, 326, 12233, 11, 22854, 11, 15329, 11], "temperature": 0.0, "avg_logprob": -0.21895076133109428, "compression_ratio": 1.4505494505494505, "no_speech_prob": 1.723109744489193e-05}, {"id": 91, "seek": 52572, "start": 540.28, "end": 545.36, "text": " sitepackages, Keras directory, you'll find all the current callbacks. So when I said", "tokens": [3621, 9539, 1660, 11, 591, 6985, 21120, 11, 291, 603, 915, 439, 264, 2190, 818, 17758, 13, 407, 562, 286, 848], "temperature": 0.0, "avg_logprob": -0.21895076133109428, "compression_ratio": 1.4505494505494505, "no_speech_prob": 1.723109744489193e-05}, {"id": 92, "seek": 52572, "start": 545.36, "end": 550.4, "text": " to Brad some tips as to how to get started, I said go here, look at the existing callbacks", "tokens": [281, 11895, 512, 6082, 382, 281, 577, 281, 483, 1409, 11, 286, 848, 352, 510, 11, 574, 412, 264, 6741, 818, 17758], "temperature": 0.0, "avg_logprob": -0.21895076133109428, "compression_ratio": 1.4505494505494505, "no_speech_prob": 1.723109744489193e-05}, {"id": 93, "seek": 55040, "start": 550.4, "end": 556.0799999999999, "text": " and see if that helps you get started. Any time you want to build something in Keras,", "tokens": [293, 536, 498, 300, 3665, 291, 483, 1409, 13, 2639, 565, 291, 528, 281, 1322, 746, 294, 591, 6985, 11], "temperature": 0.0, "avg_logprob": -0.1231839656829834, "compression_ratio": 1.7723880597014925, "no_speech_prob": 1.3630977264256217e-05}, {"id": 94, "seek": 55040, "start": 556.0799999999999, "end": 559.6, "text": " one of the best ways to get started is to read the source code for something that it", "tokens": [472, 295, 264, 1151, 2098, 281, 483, 1409, 307, 281, 1401, 264, 4009, 3089, 337, 746, 300, 309], "temperature": 0.0, "avg_logprob": -0.1231839656829834, "compression_ratio": 1.7723880597014925, "no_speech_prob": 1.3630977264256217e-05}, {"id": 95, "seek": 55040, "start": 559.6, "end": 564.1999999999999, "text": " already has that's somewhat similar to what you want to build. And certainly with callbacks,", "tokens": [1217, 575, 300, 311, 8344, 2531, 281, 437, 291, 528, 281, 1322, 13, 400, 3297, 365, 818, 17758, 11], "temperature": 0.0, "avg_logprob": -0.1231839656829834, "compression_ratio": 1.7723880597014925, "no_speech_prob": 1.3630977264256217e-05}, {"id": 96, "seek": 55040, "start": 564.1999999999999, "end": 566.92, "text": " that's an easy way to do that.", "tokens": [300, 311, 364, 1858, 636, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1231839656829834, "compression_ratio": 1.7723880597014925, "no_speech_prob": 1.3630977264256217e-05}, {"id": 97, "seek": 55040, "start": 566.92, "end": 571.0799999999999, "text": " It's been a big week in deep learning as usual. Everything I taught you in this course is", "tokens": [467, 311, 668, 257, 955, 1243, 294, 2452, 2539, 382, 7713, 13, 5471, 286, 5928, 291, 294, 341, 1164, 307], "temperature": 0.0, "avg_logprob": -0.1231839656829834, "compression_ratio": 1.7723880597014925, "no_speech_prob": 1.3630977264256217e-05}, {"id": 98, "seek": 55040, "start": 571.0799999999999, "end": 576.6, "text": " now officially out of date. Hopefully though, one of the things I have noticed is it's all", "tokens": [586, 12053, 484, 295, 4002, 13, 10429, 1673, 11, 472, 295, 264, 721, 286, 362, 5694, 307, 309, 311, 439], "temperature": 0.0, "avg_logprob": -0.1231839656829834, "compression_ratio": 1.7723880597014925, "no_speech_prob": 1.3630977264256217e-05}, {"id": 99, "seek": 57660, "start": 576.6, "end": 583.0400000000001, "text": " building on stuff that we've been learning about. I have to begin to Brad for drawing", "tokens": [2390, 322, 1507, 300, 321, 600, 668, 2539, 466, 13, 286, 362, 281, 1841, 281, 11895, 337, 6316], "temperature": 0.0, "avg_logprob": -0.15332136421560127, "compression_ratio": 1.6846153846153846, "no_speech_prob": 2.4298493372043595e-05}, {"id": 100, "seek": 57660, "start": 583.0400000000001, "end": 589.2, "text": " my attention to this paper, which is a new style transfer paper which can transfer to", "tokens": [452, 3202, 281, 341, 3035, 11, 597, 307, 257, 777, 3758, 5003, 3035, 597, 393, 5003, 281], "temperature": 0.0, "avg_logprob": -0.15332136421560127, "compression_ratio": 1.6846153846153846, "no_speech_prob": 2.4298493372043595e-05}, {"id": 101, "seek": 57660, "start": 589.2, "end": 595.8000000000001, "text": " any style in real time so you don't have to build a separate network for each one. This", "tokens": [604, 3758, 294, 957, 565, 370, 291, 500, 380, 362, 281, 1322, 257, 4994, 3209, 337, 1184, 472, 13, 639], "temperature": 0.0, "avg_logprob": -0.15332136421560127, "compression_ratio": 1.6846153846153846, "no_speech_prob": 2.4298493372043595e-05}, {"id": 102, "seek": 57660, "start": 595.8000000000001, "end": 600.32, "text": " is the kind of thing which you could absolutely turn into an app. And obviously no one's", "tokens": [307, 264, 733, 295, 551, 597, 291, 727, 3122, 1261, 666, 364, 724, 13, 400, 2745, 572, 472, 311], "temperature": 0.0, "avg_logprob": -0.15332136421560127, "compression_ratio": 1.6846153846153846, "no_speech_prob": 2.4298493372043595e-05}, {"id": 103, "seek": 57660, "start": 600.32, "end": 605.0, "text": " done it yet because this paper just came out, so you could be the first one to say here's", "tokens": [1096, 309, 1939, 570, 341, 3035, 445, 1361, 484, 11, 370, 291, 727, 312, 264, 700, 472, 281, 584, 510, 311], "temperature": 0.0, "avg_logprob": -0.15332136421560127, "compression_ratio": 1.6846153846153846, "no_speech_prob": 2.4298493372043595e-05}, {"id": 104, "seek": 60500, "start": 605.0, "end": 609.68, "text": " an app which can create any photo into any style you like.", "tokens": [364, 724, 597, 393, 1884, 604, 5052, 666, 604, 3758, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.19432948009077325, "compression_ratio": 1.5533980582524272, "no_speech_prob": 3.4806787880370393e-05}, {"id": 105, "seek": 60500, "start": 609.68, "end": 615.92, "text": " And honestly I think this third column is the new paper. In my opinion, if you compare", "tokens": [400, 6095, 286, 519, 341, 2636, 7738, 307, 264, 777, 3035, 13, 682, 452, 4800, 11, 498, 291, 6794], "temperature": 0.0, "avg_logprob": -0.19432948009077325, "compression_ratio": 1.5533980582524272, "no_speech_prob": 3.4806787880370393e-05}, {"id": 106, "seek": 60500, "start": 615.92, "end": 619.24, "text": " it to the gold standard, which is the Gatties paper we originally looked at, I actually", "tokens": [309, 281, 264, 3821, 3832, 11, 597, 307, 264, 460, 1591, 530, 3035, 321, 7993, 2956, 412, 11, 286, 767], "temperature": 0.0, "avg_logprob": -0.19432948009077325, "compression_ratio": 1.5533980582524272, "no_speech_prob": 3.4806787880370393e-05}, {"id": 107, "seek": 60500, "start": 619.24, "end": 629.88, "text": " think it's maybe better. This one looks more style-y. So there's an interesting paper.", "tokens": [519, 309, 311, 1310, 1101, 13, 639, 472, 1542, 544, 3758, 12, 88, 13, 407, 456, 311, 364, 1880, 3035, 13], "temperature": 0.0, "avg_logprob": -0.19432948009077325, "compression_ratio": 1.5533980582524272, "no_speech_prob": 3.4806787880370393e-05}, {"id": 108, "seek": 62988, "start": 629.88, "end": 636.56, "text": " A lot of the basic ideas are the same, but as you'll see it's got some interesting approaches.", "tokens": [316, 688, 295, 264, 3875, 3487, 366, 264, 912, 11, 457, 382, 291, 603, 536, 309, 311, 658, 512, 1880, 11587, 13], "temperature": 0.0, "avg_logprob": -0.16281833123723302, "compression_ratio": 1.5508474576271187, "no_speech_prob": 4.069141868967563e-05}, {"id": 109, "seek": 62988, "start": 636.56, "end": 643.12, "text": " GANs have had a big step forward. This is the previous state-of-the-art and first generation.", "tokens": [460, 1770, 82, 362, 632, 257, 955, 1823, 2128, 13, 639, 307, 264, 3894, 1785, 12, 2670, 12, 3322, 12, 446, 293, 700, 5125, 13], "temperature": 0.0, "avg_logprob": -0.16281833123723302, "compression_ratio": 1.5508474576271187, "no_speech_prob": 4.069141868967563e-05}, {"id": 110, "seek": 62988, "start": 643.12, "end": 648.28, "text": " This is the new. As you can see, these are now pretty much at photo-realistic, at least", "tokens": [639, 307, 264, 777, 13, 1018, 291, 393, 536, 11, 613, 366, 586, 1238, 709, 412, 5052, 12, 9342, 3142, 11, 412, 1935], "temperature": 0.0, "avg_logprob": -0.16281833123723302, "compression_ratio": 1.5508474576271187, "no_speech_prob": 4.069141868967563e-05}, {"id": 111, "seek": 62988, "start": 648.28, "end": 655.68, "text": " at 128x128. These were only 64x64, the old state-of-the-art. So this is a pretty exciting", "tokens": [412, 29810, 87, 4762, 23, 13, 1981, 645, 787, 12145, 87, 19395, 11, 264, 1331, 1785, 12, 2670, 12, 3322, 12, 446, 13, 407, 341, 307, 257, 1238, 4670], "temperature": 0.0, "avg_logprob": -0.16281833123723302, "compression_ratio": 1.5508474576271187, "no_speech_prob": 4.069141868967563e-05}, {"id": 112, "seek": 65568, "start": 655.68, "end": 663.8399999999999, "text": " step forward in GANs. As we've talked about, people use GANs at the moment to create pictures,", "tokens": [1823, 2128, 294, 460, 1770, 82, 13, 1018, 321, 600, 2825, 466, 11, 561, 764, 460, 1770, 82, 412, 264, 1623, 281, 1884, 5242, 11], "temperature": 0.0, "avg_logprob": -0.14300251980217135, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.2698619179427624e-05}, {"id": 113, "seek": 65568, "start": 663.8399999999999, "end": 669.8, "text": " but they can be used as an additional loss function for any kind of generative network.", "tokens": [457, 436, 393, 312, 1143, 382, 364, 4497, 4470, 2445, 337, 604, 733, 295, 1337, 1166, 3209, 13], "temperature": 0.0, "avg_logprob": -0.14300251980217135, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.2698619179427624e-05}, {"id": 114, "seek": 65568, "start": 669.8, "end": 674.64, "text": " So I think one of the things I always look for is what's underappreciated, what's not", "tokens": [407, 286, 519, 472, 295, 264, 721, 286, 1009, 574, 337, 307, 437, 311, 833, 1746, 3326, 770, 11, 437, 311, 406], "temperature": 0.0, "avg_logprob": -0.14300251980217135, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.2698619179427624e-05}, {"id": 115, "seek": 65568, "start": 674.64, "end": 681.8, "text": " being used much at the moment. And I would say GANs for a wider range of generative models.", "tokens": [885, 1143, 709, 412, 264, 1623, 13, 400, 286, 576, 584, 460, 1770, 82, 337, 257, 11842, 3613, 295, 1337, 1166, 5245, 13], "temperature": 0.0, "avg_logprob": -0.14300251980217135, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.2698619179427624e-05}, {"id": 116, "seek": 68180, "start": 681.8, "end": 687.4399999999999, "text": " If you want to create a really great super resolution thing, or if you want to create", "tokens": [759, 291, 528, 281, 1884, 257, 534, 869, 1687, 8669, 551, 11, 420, 498, 291, 528, 281, 1884], "temperature": 0.0, "avg_logprob": -0.21605875137004446, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.80581755557796e-05}, {"id": 117, "seek": 68180, "start": 687.4399999999999, "end": 694.8399999999999, "text": " a really great automatic line drawing creator, or colorization system, or whatever, I think", "tokens": [257, 534, 869, 12509, 1622, 6316, 14181, 11, 420, 2017, 2144, 1185, 11, 420, 2035, 11, 286, 519], "temperature": 0.0, "avg_logprob": -0.21605875137004446, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.80581755557796e-05}, {"id": 118, "seek": 68180, "start": 694.8399999999999, "end": 698.76, "text": " GANs are a good approach.", "tokens": [460, 1770, 82, 366, 257, 665, 3109, 13], "temperature": 0.0, "avg_logprob": -0.21605875137004446, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.80581755557796e-05}, {"id": 119, "seek": 68180, "start": 698.76, "end": 704.0799999999999, "text": " Perhaps the most amazing one is this paper, which again we'll add on to the wiki, which", "tokens": [10517, 264, 881, 2243, 472, 307, 341, 3035, 11, 597, 797, 321, 603, 909, 322, 281, 264, 261, 9850, 11, 597], "temperature": 0.0, "avg_logprob": -0.21605875137004446, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.80581755557796e-05}, {"id": 120, "seek": 68180, "start": 704.0799999999999, "end": 711.0799999999999, "text": " does a bidirectional transfer even without matching pairs. So for example, clearly this", "tokens": [775, 257, 12957, 621, 41048, 5003, 754, 1553, 14324, 15494, 13, 407, 337, 1365, 11, 4448, 341], "temperature": 0.0, "avg_logprob": -0.21605875137004446, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.80581755557796e-05}, {"id": 121, "seek": 71108, "start": 711.08, "end": 715.2, "text": " would not be possible to do with you requiring matching pairs. Take a Monet and turn it into", "tokens": [576, 406, 312, 1944, 281, 360, 365, 291, 24165, 14324, 15494, 13, 3664, 257, 47871, 293, 1261, 309, 666], "temperature": 0.0, "avg_logprob": -0.1742436091105143, "compression_ratio": 1.653386454183267, "no_speech_prob": 2.546528776292689e-05}, {"id": 122, "seek": 71108, "start": 715.2, "end": 719.8000000000001, "text": " a photo. We don't have any data on how to do that because we don't have any photos of", "tokens": [257, 5052, 13, 492, 500, 380, 362, 604, 1412, 322, 577, 281, 360, 300, 570, 321, 500, 380, 362, 604, 5787, 295], "temperature": 0.0, "avg_logprob": -0.1742436091105143, "compression_ratio": 1.653386454183267, "no_speech_prob": 2.546528776292689e-05}, {"id": 123, "seek": 71108, "start": 719.8000000000001, "end": 725.7800000000001, "text": " Monet, but as you can see it works incredibly well. Turn a zebra into a horse or a horse", "tokens": [47871, 11, 457, 382, 291, 393, 536, 309, 1985, 6252, 731, 13, 7956, 257, 47060, 666, 257, 6832, 420, 257, 6832], "temperature": 0.0, "avg_logprob": -0.1742436091105143, "compression_ratio": 1.653386454183267, "no_speech_prob": 2.546528776292689e-05}, {"id": 124, "seek": 71108, "start": 725.7800000000001, "end": 733.0, "text": " into a zebra, summer into winter, winter into summer. Again, this is a GAN-based system", "tokens": [666, 257, 47060, 11, 4266, 666, 6355, 11, 6355, 666, 4266, 13, 3764, 11, 341, 307, 257, 460, 1770, 12, 6032, 1185], "temperature": 0.0, "avg_logprob": -0.1742436091105143, "compression_ratio": 1.653386454183267, "no_speech_prob": 2.546528776292689e-05}, {"id": 125, "seek": 71108, "start": 733.0, "end": 738.6800000000001, "text": " creating photorealistic images or very impressive artworks.", "tokens": [4084, 2409, 418, 304, 3142, 5267, 420, 588, 8992, 15829, 82, 13], "temperature": 0.0, "avg_logprob": -0.1742436091105143, "compression_ratio": 1.653386454183267, "no_speech_prob": 2.546528776292689e-05}, {"id": 126, "seek": 73868, "start": 738.68, "end": 743.12, "text": " I think this approach to style transfer by putting a GAN layer is perhaps more interesting", "tokens": [286, 519, 341, 3109, 281, 3758, 5003, 538, 3372, 257, 460, 1770, 4583, 307, 4317, 544, 1880], "temperature": 0.0, "avg_logprob": -0.14627501487731934, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.4510204891848844e-05}, {"id": 127, "seek": 73868, "start": 743.12, "end": 748.88, "text": " than the style transfer we've come up with so far because it really has to create a painting", "tokens": [813, 264, 3758, 5003, 321, 600, 808, 493, 365, 370, 1400, 570, 309, 534, 575, 281, 1884, 257, 5370], "temperature": 0.0, "avg_logprob": -0.14627501487731934, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.4510204891848844e-05}, {"id": 128, "seek": 73868, "start": 748.88, "end": 754.92, "text": " that can't be recognized as not being a real painting, otherwise the GAN will fail. So", "tokens": [300, 393, 380, 312, 9823, 382, 406, 885, 257, 957, 5370, 11, 5911, 264, 460, 1770, 486, 3061, 13, 407], "temperature": 0.0, "avg_logprob": -0.14627501487731934, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.4510204891848844e-05}, {"id": 129, "seek": 73868, "start": 754.92, "end": 762.4399999999999, "text": " I think this is a really interesting approach to style transfer as well.", "tokens": [286, 519, 341, 307, 257, 534, 1880, 3109, 281, 3758, 5003, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14627501487731934, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.4510204891848844e-05}, {"id": 130, "seek": 73868, "start": 762.4399999999999, "end": 767.0, "text": " Really interesting week. These are all papers which I think any of you guys can tackle right", "tokens": [4083, 1880, 1243, 13, 1981, 366, 439, 10577, 597, 286, 519, 604, 295, 291, 1074, 393, 14896, 558], "temperature": 0.0, "avg_logprob": -0.14627501487731934, "compression_ratio": 1.8016528925619835, "no_speech_prob": 1.4510204891848844e-05}, {"id": 131, "seek": 76700, "start": 767.0, "end": 770.78, "text": " now because they're all built on the things we've learned in this course. If anybody's", "tokens": [586, 570, 436, 434, 439, 3094, 322, 264, 721, 321, 600, 3264, 294, 341, 1164, 13, 759, 4472, 311], "temperature": 0.0, "avg_logprob": -0.2296199283084354, "compression_ratio": 1.6549019607843136, "no_speech_prob": 8.092453936114907e-05}, {"id": 132, "seek": 76700, "start": 770.78, "end": 774.8, "text": " interested in tackling anything this week, that would be super fun to talk about it on", "tokens": [3102, 294, 34415, 1340, 341, 1243, 11, 300, 576, 312, 1687, 1019, 281, 751, 466, 309, 322], "temperature": 0.0, "avg_logprob": -0.2296199283084354, "compression_ratio": 1.6549019607843136, "no_speech_prob": 8.092453936114907e-05}, {"id": 133, "seek": 76700, "start": 774.8, "end": 775.8, "text": " the forum.", "tokens": [264, 17542, 13], "temperature": 0.0, "avg_logprob": -0.2296199283084354, "compression_ratio": 1.6549019607843136, "no_speech_prob": 8.092453936114907e-05}, {"id": 134, "seek": 76700, "start": 775.8, "end": 776.8, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.2296199283084354, "compression_ratio": 1.6549019607843136, "no_speech_prob": 8.092453936114907e-05}, {"id": 135, "seek": 76700, "start": 776.8, "end": 781.68, "text": " Just to clarify, is this last paper also the GAN from the prior slide or is this a different", "tokens": [1449, 281, 17594, 11, 307, 341, 1036, 3035, 611, 264, 460, 1770, 490, 264, 4059, 4137, 420, 307, 341, 257, 819], "temperature": 0.0, "avg_logprob": -0.2296199283084354, "compression_ratio": 1.6549019607843136, "no_speech_prob": 8.092453936114907e-05}, {"id": 136, "seek": 76700, "start": 781.68, "end": 782.68, "text": " paper?", "tokens": [3035, 30], "temperature": 0.0, "avg_logprob": -0.2296199283084354, "compression_ratio": 1.6549019607843136, "no_speech_prob": 8.092453936114907e-05}, {"id": 137, "seek": 76700, "start": 782.68, "end": 785.76, "text": " This is a different paper. I'm sorry, I'm embarrassed that I didn't actually write on", "tokens": [639, 307, 257, 819, 3035, 13, 286, 478, 2597, 11, 286, 478, 16843, 300, 286, 994, 380, 767, 2464, 322], "temperature": 0.0, "avg_logprob": -0.2296199283084354, "compression_ratio": 1.6549019607843136, "no_speech_prob": 8.092453936114907e-05}, {"id": 138, "seek": 76700, "start": 785.76, "end": 795.44, "text": " it where it comes from.", "tokens": [309, 689, 309, 1487, 490, 13], "temperature": 0.0, "avg_logprob": -0.2296199283084354, "compression_ratio": 1.6549019607843136, "no_speech_prob": 8.092453936114907e-05}, {"id": 139, "seek": 79544, "start": 795.44, "end": 800.2, "text": " Rather than me doing this, maybe somebody can try and find it. I think I showed Brad", "tokens": [16571, 813, 385, 884, 341, 11, 1310, 2618, 393, 853, 293, 915, 309, 13, 286, 519, 286, 4712, 11895], "temperature": 0.0, "avg_logprob": -0.3840456008911133, "compression_ratio": 1.302325581395349, "no_speech_prob": 0.00017398150521330535}, {"id": 140, "seek": 79544, "start": 800.2, "end": 806.5200000000001, "text": " and Rachel have already seen it. I'll find it on my Twitter and put it on the Wiki.", "tokens": [293, 14246, 362, 1217, 1612, 309, 13, 286, 603, 915, 309, 322, 452, 5794, 293, 829, 309, 322, 264, 35892, 13], "temperature": 0.0, "avg_logprob": -0.3840456008911133, "compression_ratio": 1.302325581395349, "no_speech_prob": 0.00017398150521330535}, {"id": 141, "seek": 80652, "start": 806.52, "end": 832.96, "text": " We've been talking about Minshift clustering a bit from time to time, so we'll talk about", "tokens": [492, 600, 668, 1417, 466, 376, 1292, 71, 2008, 596, 48673, 257, 857, 490, 565, 281, 565, 11, 370, 321, 603, 751, 466], "temperature": 0.0, "avg_logprob": -0.23745098820439092, "compression_ratio": 1.1265822784810127, "no_speech_prob": 7.646252015547361e-06}, {"id": 142, "seek": 83296, "start": 832.96, "end": 836.96, "text": " it more next week in terms of applications. But the main application we've talked about", "tokens": [309, 544, 958, 1243, 294, 2115, 295, 5821, 13, 583, 264, 2135, 3861, 321, 600, 2825, 466], "temperature": 0.0, "avg_logprob": -0.2090884807497956, "compression_ratio": 1.579646017699115, "no_speech_prob": 1.1842255844385363e-05}, {"id": 143, "seek": 83296, "start": 836.96, "end": 845.36, "text": " so far is using it for a kind of faster preprocessing of really large data items like CT scans in", "tokens": [370, 1400, 307, 1228, 309, 337, 257, 733, 295, 4663, 2666, 340, 780, 278, 295, 534, 2416, 1412, 4754, 411, 19529, 35116, 294], "temperature": 0.0, "avg_logprob": -0.2090884807497956, "compression_ratio": 1.579646017699115, "no_speech_prob": 1.1842255844385363e-05}, {"id": 144, "seek": 83296, "start": 845.36, "end": 851.64, "text": " order to find objects of interest, in this case, lung nodules, at microcancers.", "tokens": [1668, 281, 915, 6565, 295, 1179, 11, 294, 341, 1389, 11, 16730, 15224, 3473, 11, 412, 4532, 7035, 8530, 13], "temperature": 0.0, "avg_logprob": -0.2090884807497956, "compression_ratio": 1.579646017699115, "no_speech_prob": 1.1842255844385363e-05}, {"id": 145, "seek": 83296, "start": 851.64, "end": 857.4000000000001, "text": " And one of the things I mentioned that I was interested in was experimenting with combining", "tokens": [400, 472, 295, 264, 721, 286, 2835, 300, 286, 390, 3102, 294, 390, 29070, 365, 21928], "temperature": 0.0, "avg_logprob": -0.2090884807497956, "compression_ratio": 1.579646017699115, "no_speech_prob": 1.1842255844385363e-05}, {"id": 146, "seek": 85740, "start": 857.4, "end": 865.9599999999999, "text": " approximate nearest neighbors with Minshift clustering. So to remind you, the basic idea", "tokens": [30874, 23831, 12512, 365, 376, 1292, 71, 2008, 596, 48673, 13, 407, 281, 4160, 291, 11, 264, 3875, 1558], "temperature": 0.0, "avg_logprob": -0.1714489600237678, "compression_ratio": 1.5823529411764705, "no_speech_prob": 4.985630221199244e-05}, {"id": 147, "seek": 85740, "start": 865.9599999999999, "end": 878.3199999999999, "text": " was that we went through each mini-batch and with each mini-batch we then basically did", "tokens": [390, 300, 321, 1437, 807, 1184, 8382, 12, 65, 852, 293, 365, 1184, 8382, 12, 65, 852, 321, 550, 1936, 630], "temperature": 0.0, "avg_logprob": -0.1714489600237678, "compression_ratio": 1.5823529411764705, "no_speech_prob": 4.985630221199244e-05}, {"id": 148, "seek": 85740, "start": 878.3199999999999, "end": 884.12, "text": " a distance from every element in that mini-batch through every single data item. And then we", "tokens": [257, 4560, 490, 633, 4478, 294, 300, 8382, 12, 65, 852, 807, 633, 2167, 1412, 3174, 13, 400, 550, 321], "temperature": 0.0, "avg_logprob": -0.1714489600237678, "compression_ratio": 1.5823529411764705, "no_speech_prob": 4.985630221199244e-05}, {"id": 149, "seek": 88412, "start": 884.12, "end": 891.4, "text": " took the weighted sum of all the data items weighted by the Gaussian on those weights.", "tokens": [1890, 264, 32807, 2408, 295, 439, 264, 1412, 4754, 32807, 538, 264, 39148, 322, 729, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1513907788979887, "compression_ratio": 1.73828125, "no_speech_prob": 9.66598599916324e-06}, {"id": 150, "seek": 88412, "start": 891.4, "end": 897.24, "text": " And I pointed out that the vast majority of data points are far enough away that their", "tokens": [400, 286, 10932, 484, 300, 264, 8369, 6286, 295, 1412, 2793, 366, 1400, 1547, 1314, 300, 641], "temperature": 0.0, "avg_logprob": -0.1513907788979887, "compression_ratio": 1.73828125, "no_speech_prob": 9.66598599916324e-06}, {"id": 151, "seek": 88412, "start": 897.24, "end": 901.8, "text": " weights are so close to 0 that we could probably ignore them. So maybe we could have tried", "tokens": [17443, 366, 370, 1998, 281, 1958, 300, 321, 727, 1391, 11200, 552, 13, 407, 1310, 321, 727, 362, 3031], "temperature": 0.0, "avg_logprob": -0.1513907788979887, "compression_ratio": 1.73828125, "no_speech_prob": 9.66598599916324e-06}, {"id": 152, "seek": 88412, "start": 901.8, "end": 907.4, "text": " putting an approximate nearest neighbors step beforehand, so rather than adding up the entire", "tokens": [3372, 364, 30874, 23831, 12512, 1823, 22893, 11, 370, 2831, 813, 5127, 493, 264, 2302], "temperature": 0.0, "avg_logprob": -0.1513907788979887, "compression_ratio": 1.73828125, "no_speech_prob": 9.66598599916324e-06}, {"id": 153, "seek": 88412, "start": 907.4, "end": 912.84, "text": " data set, which could be a million points, it could just be the nearest 100 neighbors.", "tokens": [1412, 992, 11, 597, 727, 312, 257, 2459, 2793, 11, 309, 727, 445, 312, 264, 23831, 2319, 12512, 13], "temperature": 0.0, "avg_logprob": -0.1513907788979887, "compression_ratio": 1.73828125, "no_speech_prob": 9.66598599916324e-06}, {"id": 154, "seek": 91284, "start": 912.84, "end": 924.24, "text": " So I actually tried that during the week. I just used the existing scikit-learn.neighbors", "tokens": [407, 286, 767, 3031, 300, 1830, 264, 1243, 13, 286, 445, 1143, 264, 6741, 2180, 22681, 12, 306, 1083, 13, 716, 910, 65, 830], "temperature": 0.0, "avg_logprob": -0.15398209571838378, "compression_ratio": 1.5708154506437768, "no_speech_prob": 3.1692219636170194e-05}, {"id": 155, "seek": 91284, "start": 924.24, "end": 930.96, "text": " algorithm, so I haven't written my own PyTorch version. I know one of you guys on the forum", "tokens": [9284, 11, 370, 286, 2378, 380, 3720, 452, 1065, 9953, 51, 284, 339, 3037, 13, 286, 458, 472, 295, 291, 1074, 322, 264, 17542], "temperature": 0.0, "avg_logprob": -0.15398209571838378, "compression_ratio": 1.5708154506437768, "no_speech_prob": 3.1692219636170194e-05}, {"id": 156, "seek": 91284, "start": 930.96, "end": 937.08, "text": " has already started writing a PyTorch version. So I just tried a ball tree, which is a particular", "tokens": [575, 1217, 1409, 3579, 257, 9953, 51, 284, 339, 3037, 13, 407, 286, 445, 3031, 257, 2594, 4230, 11, 597, 307, 257, 1729], "temperature": 0.0, "avg_logprob": -0.15398209571838378, "compression_ratio": 1.5708154506437768, "no_speech_prob": 3.1692219636170194e-05}, {"id": 157, "seek": 91284, "start": 937.08, "end": 941.96, "text": " kind of approximate nearest neighbors. And the basic idea is you say, okay, here's the", "tokens": [733, 295, 30874, 23831, 12512, 13, 400, 264, 3875, 1558, 307, 291, 584, 11, 1392, 11, 510, 311, 264], "temperature": 0.0, "avg_logprob": -0.15398209571838378, "compression_ratio": 1.5708154506437768, "no_speech_prob": 3.1692219636170194e-05}, {"id": 158, "seek": 94196, "start": 941.96, "end": 947.6800000000001, "text": " data that I want you to index so that I can rapidly do nearest neighbors. So now please", "tokens": [1412, 300, 286, 528, 291, 281, 8186, 370, 300, 286, 393, 12910, 360, 23831, 12512, 13, 407, 586, 1767], "temperature": 0.0, "avg_logprob": -0.18153225934063946, "compression_ratio": 1.8776371308016877, "no_speech_prob": 2.4682656658114865e-05}, {"id": 159, "seek": 94196, "start": 947.6800000000001, "end": 952.64, "text": " do a query looking at the first 10 data points. And for each one of those 10 data points,", "tokens": [360, 257, 14581, 1237, 412, 264, 700, 1266, 1412, 2793, 13, 400, 337, 1184, 472, 295, 729, 1266, 1412, 2793, 11], "temperature": 0.0, "avg_logprob": -0.18153225934063946, "compression_ratio": 1.8776371308016877, "no_speech_prob": 2.4682656658114865e-05}, {"id": 160, "seek": 94196, "start": 952.64, "end": 956.36, "text": " show me the 3 nearest neighbors. And that returns something like this. Here are the", "tokens": [855, 385, 264, 805, 23831, 12512, 13, 400, 300, 11247, 746, 411, 341, 13, 1692, 366, 264], "temperature": 0.0, "avg_logprob": -0.18153225934063946, "compression_ratio": 1.8776371308016877, "no_speech_prob": 2.4682656658114865e-05}, {"id": 161, "seek": 94196, "start": 956.36, "end": 961.4000000000001, "text": " 10 data points you passed in, and here are the 3 approximate nearest neighbors. And of", "tokens": [1266, 1412, 2793, 291, 4678, 294, 11, 293, 510, 366, 264, 805, 30874, 23831, 12512, 13, 400, 295], "temperature": 0.0, "avg_logprob": -0.18153225934063946, "compression_ratio": 1.8776371308016877, "no_speech_prob": 2.4682656658114865e-05}, {"id": 162, "seek": 94196, "start": 961.4000000000001, "end": 967.36, "text": " course all of the time itself is its own nearest neighbor, so that's why the first one is always", "tokens": [1164, 439, 295, 264, 565, 2564, 307, 1080, 1065, 23831, 5987, 11, 370, 300, 311, 983, 264, 700, 472, 307, 1009], "temperature": 0.0, "avg_logprob": -0.18153225934063946, "compression_ratio": 1.8776371308016877, "no_speech_prob": 2.4682656658114865e-05}, {"id": 163, "seek": 96736, "start": 967.36, "end": 974.6800000000001, "text": " kind of boring. So then I thought, okay, that looks good. So then I just put that into the", "tokens": [733, 295, 9989, 13, 407, 550, 286, 1194, 11, 1392, 11, 300, 1542, 665, 13, 407, 550, 286, 445, 829, 300, 666, 264], "temperature": 0.0, "avg_logprob": -0.17717538277308145, "compression_ratio": 1.625, "no_speech_prob": 3.1201598176267e-05}, {"id": 164, "seek": 96736, "start": 974.6800000000001, "end": 981.04, "text": " loop of our mean shift cluster. So I said, okay, each time we go through another epoch,", "tokens": [6367, 295, 527, 914, 5513, 13630, 13, 407, 286, 848, 11, 1392, 11, 1184, 565, 321, 352, 807, 1071, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.17717538277308145, "compression_ratio": 1.625, "no_speech_prob": 3.1201598176267e-05}, {"id": 165, "seek": 96736, "start": 981.04, "end": 986.96, "text": " index the data we have so far, so chuck it into one of these ball trees. And then when", "tokens": [8186, 264, 1412, 321, 362, 370, 1400, 11, 370, 20870, 309, 666, 472, 295, 613, 2594, 5852, 13, 400, 550, 562], "temperature": 0.0, "avg_logprob": -0.17717538277308145, "compression_ratio": 1.625, "no_speech_prob": 3.1201598176267e-05}, {"id": 166, "seek": 96736, "start": 986.96, "end": 994.52, "text": " we do our distances, don't do them to every point, but instead do it into the nearest", "tokens": [321, 360, 527, 22182, 11, 500, 380, 360, 552, 281, 633, 935, 11, 457, 2602, 360, 309, 666, 264, 23831], "temperature": 0.0, "avg_logprob": -0.17717538277308145, "compression_ratio": 1.625, "no_speech_prob": 3.1201598176267e-05}, {"id": 167, "seek": 99452, "start": 994.52, "end": 1001.64, "text": " points. So I did a query here to find the 50 nearest neighbors. This then puts that", "tokens": [2793, 13, 407, 286, 630, 257, 14581, 510, 281, 915, 264, 2625, 23831, 12512, 13, 639, 550, 8137, 300], "temperature": 0.0, "avg_logprob": -0.16278070616490634, "compression_ratio": 1.6271186440677967, "no_speech_prob": 8.013455953914672e-06}, {"id": 168, "seek": 99452, "start": 1001.64, "end": 1009.0799999999999, "text": " into GPU. I'm turning it into a tensor and dot-cootering it. So that gives me the list", "tokens": [666, 18407, 13, 286, 478, 6246, 309, 666, 257, 40863, 293, 5893, 12, 1291, 310, 1794, 309, 13, 407, 300, 2709, 385, 264, 1329], "temperature": 0.0, "avg_logprob": -0.16278070616490634, "compression_ratio": 1.6271186440677967, "no_speech_prob": 8.013455953914672e-06}, {"id": 169, "seek": 99452, "start": 1009.0799999999999, "end": 1014.36, "text": " of the indexes, not the data itself.", "tokens": [295, 264, 8186, 279, 11, 406, 264, 1412, 2564, 13], "temperature": 0.0, "avg_logprob": -0.16278070616490634, "compression_ratio": 1.6271186440677967, "no_speech_prob": 8.013455953914672e-06}, {"id": 170, "seek": 99452, "start": 1014.36, "end": 1018.16, "text": " So interestingly, the hardest step was the one that seems like it should be the easiest,", "tokens": [407, 25873, 11, 264, 13158, 1823, 390, 264, 472, 300, 2544, 411, 309, 820, 312, 264, 12889, 11], "temperature": 0.0, "avg_logprob": -0.16278070616490634, "compression_ratio": 1.6271186440677967, "no_speech_prob": 8.013455953914672e-06}, {"id": 171, "seek": 99452, "start": 1018.16, "end": 1023.52, "text": " which is I had to then look up each of these indexes into the data to return the actual", "tokens": [597, 307, 286, 632, 281, 550, 574, 493, 1184, 295, 613, 8186, 279, 666, 264, 1412, 281, 2736, 264, 3539], "temperature": 0.0, "avg_logprob": -0.16278070616490634, "compression_ratio": 1.6271186440677967, "no_speech_prob": 8.013455953914672e-06}, {"id": 172, "seek": 102352, "start": 1023.52, "end": 1028.8, "text": " points. Easy enough to do if you just look through it one step at a time, but we're trying", "tokens": [2793, 13, 16002, 1547, 281, 360, 498, 291, 445, 574, 807, 309, 472, 1823, 412, 257, 565, 11, 457, 321, 434, 1382], "temperature": 0.0, "avg_logprob": -0.13992783782679005, "compression_ratio": 1.6653543307086613, "no_speech_prob": 7.411224487441359e-06}, {"id": 173, "seek": 102352, "start": 1028.8, "end": 1034.24, "text": " to use the GPU here. If I tried doing it one step at a time, it made the thing take forever", "tokens": [281, 764, 264, 18407, 510, 13, 759, 286, 3031, 884, 309, 472, 1823, 412, 257, 565, 11, 309, 1027, 264, 551, 747, 5680], "temperature": 0.0, "avg_logprob": -0.13992783782679005, "compression_ratio": 1.6653543307086613, "no_speech_prob": 7.411224487441359e-06}, {"id": 174, "seek": 102352, "start": 1034.24, "end": 1038.1399999999999, "text": " because GPUs are not quick at doing things one step at a time.", "tokens": [570, 18407, 82, 366, 406, 1702, 412, 884, 721, 472, 1823, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.13992783782679005, "compression_ratio": 1.6653543307086613, "no_speech_prob": 7.411224487441359e-06}, {"id": 175, "seek": 102352, "start": 1038.1399999999999, "end": 1043.4, "text": " So I created a little batch-wise indexing function, which in the end Rachel and I realized", "tokens": [407, 286, 2942, 257, 707, 15245, 12, 3711, 8186, 278, 2445, 11, 597, 294, 264, 917, 14246, 293, 286, 5334], "temperature": 0.0, "avg_logprob": -0.13992783782679005, "compression_ratio": 1.6653543307086613, "no_speech_prob": 7.411224487441359e-06}, {"id": 176, "seek": 102352, "start": 1043.4, "end": 1047.4, "text": " we could do actually with very, very little code. This is basically something where we", "tokens": [321, 727, 360, 767, 365, 588, 11, 588, 707, 3089, 13, 639, 307, 1936, 746, 689, 321], "temperature": 0.0, "avg_logprob": -0.13992783782679005, "compression_ratio": 1.6653543307086613, "no_speech_prob": 7.411224487441359e-06}, {"id": 177, "seek": 104740, "start": 1047.4, "end": 1055.4, "text": " could pass in our array and pass in our matrix of indexes and it would return a 3-dimensional", "tokens": [727, 1320, 294, 527, 10225, 293, 1320, 294, 527, 8141, 295, 8186, 279, 293, 309, 576, 2736, 257, 805, 12, 18759], "temperature": 0.0, "avg_logprob": -0.15457516487198647, "compression_ratio": 1.8544600938967135, "no_speech_prob": 2.7693718038790394e-06}, {"id": 178, "seek": 104740, "start": 1055.4, "end": 1059.88, "text": " tensor basically of every one of those points for every one of those nearest neighbors for", "tokens": [40863, 1936, 295, 633, 472, 295, 729, 2793, 337, 633, 472, 295, 729, 23831, 12512, 337], "temperature": 0.0, "avg_logprob": -0.15457516487198647, "compression_ratio": 1.8544600938967135, "no_speech_prob": 2.7693718038790394e-06}, {"id": 179, "seek": 104740, "start": 1059.88, "end": 1061.96, "text": " every one of those dimensions.", "tokens": [633, 472, 295, 729, 12819, 13], "temperature": 0.0, "avg_logprob": -0.15457516487198647, "compression_ratio": 1.8544600938967135, "no_speech_prob": 2.7693718038790394e-06}, {"id": 180, "seek": 104740, "start": 1061.96, "end": 1067.44, "text": " So that was the only bit that was remotely tricky. Other than that, the rest of the code", "tokens": [407, 300, 390, 264, 787, 857, 300, 390, 20824, 12414, 13, 5358, 813, 300, 11, 264, 1472, 295, 264, 3089], "temperature": 0.0, "avg_logprob": -0.15457516487198647, "compression_ratio": 1.8544600938967135, "no_speech_prob": 2.7693718038790394e-06}, {"id": 181, "seek": 104740, "start": 1067.44, "end": 1075.16, "text": " is the same. So this worked in the sense that it sped things up, but it didn't work in the", "tokens": [307, 264, 912, 13, 407, 341, 2732, 294, 264, 2020, 300, 309, 637, 292, 721, 493, 11, 457, 309, 994, 380, 589, 294, 264], "temperature": 0.0, "avg_logprob": -0.15457516487198647, "compression_ratio": 1.8544600938967135, "no_speech_prob": 2.7693718038790394e-06}, {"id": 182, "seek": 107516, "start": 1075.16, "end": 1084.24, "text": " sense of the result. Here was the input we gave it, and here's the output.", "tokens": [2020, 295, 264, 1874, 13, 1692, 390, 264, 4846, 321, 2729, 309, 11, 293, 510, 311, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.116109223592849, "compression_ratio": 1.5794392523364487, "no_speech_prob": 6.643384494964266e-06}, {"id": 183, "seek": 107516, "start": 1084.24, "end": 1090.0800000000002, "text": " Now what's happened is it turns out each one of these little colored dots actually now", "tokens": [823, 437, 311, 2011, 307, 309, 4523, 484, 1184, 472, 295, 613, 707, 14332, 15026, 767, 586], "temperature": 0.0, "avg_logprob": -0.116109223592849, "compression_ratio": 1.5794392523364487, "no_speech_prob": 6.643384494964266e-06}, {"id": 184, "seek": 107516, "start": 1090.0800000000002, "end": 1096.0, "text": " represents 50 points. So what it's done is it's taken me too literally. It's basically", "tokens": [8855, 2625, 2793, 13, 407, 437, 309, 311, 1096, 307, 309, 311, 2726, 385, 886, 3736, 13, 467, 311, 1936], "temperature": 0.0, "avg_logprob": -0.116109223592849, "compression_ratio": 1.5794392523364487, "no_speech_prob": 6.643384494964266e-06}, {"id": 185, "seek": 107516, "start": 1096.0, "end": 1102.68, "text": " found the 50 nearest neighbors to each point and created a cluster of 50 points. And once", "tokens": [1352, 264, 2625, 23831, 12512, 281, 1184, 935, 293, 2942, 257, 13630, 295, 2625, 2793, 13, 400, 1564], "temperature": 0.0, "avg_logprob": -0.116109223592849, "compression_ratio": 1.5794392523364487, "no_speech_prob": 6.643384494964266e-06}, {"id": 186, "seek": 110268, "start": 1102.68, "end": 1107.4, "text": " it's created these clusters of 50 points, it's now stuck. There's no way for it to cluster", "tokens": [309, 311, 2942, 613, 23313, 295, 2625, 2793, 11, 309, 311, 586, 5541, 13, 821, 311, 572, 636, 337, 309, 281, 13630], "temperature": 0.0, "avg_logprob": -0.14880963622546586, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.13811210617132e-06}, {"id": 187, "seek": 110268, "start": 1107.4, "end": 1111.0800000000002, "text": " these any further because every time it now goes nearest neighbors, it says, oh, there's", "tokens": [613, 604, 3052, 570, 633, 565, 309, 586, 1709, 23831, 12512, 11, 309, 1619, 11, 1954, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.14880963622546586, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.13811210617132e-06}, {"id": 188, "seek": 110268, "start": 1111.0800000000002, "end": 1113.48, "text": " 50 points that are right on top of you.", "tokens": [2625, 2793, 300, 366, 558, 322, 1192, 295, 291, 13], "temperature": 0.0, "avg_logprob": -0.14880963622546586, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.13811210617132e-06}, {"id": 189, "seek": 110268, "start": 1113.48, "end": 1118.24, "text": " So I mentioned this to basically show the kinds of things that can go wrong, and hopefully", "tokens": [407, 286, 2835, 341, 281, 1936, 855, 264, 3685, 295, 721, 300, 393, 352, 2085, 11, 293, 4696], "temperature": 0.0, "avg_logprob": -0.14880963622546586, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.13811210617132e-06}, {"id": 190, "seek": 110268, "start": 1118.24, "end": 1126.64, "text": " by next week this will be fixed. It's like when I described this problem, this seemed", "tokens": [538, 958, 1243, 341, 486, 312, 6806, 13, 467, 311, 411, 562, 286, 7619, 341, 1154, 11, 341, 6576], "temperature": 0.0, "avg_logprob": -0.14880963622546586, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.13811210617132e-06}, {"id": 191, "seek": 110268, "start": 1126.64, "end": 1130.8, "text": " like it was clearly going to work, and then as soon as I saw this picture, I immediately", "tokens": [411, 309, 390, 4448, 516, 281, 589, 11, 293, 550, 382, 2321, 382, 286, 1866, 341, 3036, 11, 286, 4258], "temperature": 0.0, "avg_logprob": -0.14880963622546586, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.13811210617132e-06}, {"id": 192, "seek": 113080, "start": 1130.8, "end": 1136.36, "text": " saw that it couldn't possibly work. So this is kind of the nature of trying things out.", "tokens": [1866, 300, 309, 2809, 380, 6264, 589, 13, 407, 341, 307, 733, 295, 264, 3687, 295, 1382, 721, 484, 13], "temperature": 0.0, "avg_logprob": -0.13767750263214112, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.6028037634896464e-06}, {"id": 193, "seek": 113080, "start": 1136.36, "end": 1140.72, "text": " But the interesting thing is I now realize that the solution to this will be way faster", "tokens": [583, 264, 1880, 551, 307, 286, 586, 4325, 300, 264, 3827, 281, 341, 486, 312, 636, 4663], "temperature": 0.0, "avg_logprob": -0.13767750263214112, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.6028037634896464e-06}, {"id": 194, "seek": 113080, "start": 1140.72, "end": 1148.0, "text": " than this. The solution is what Rachel describes as an approximate nearest neighbors, which", "tokens": [813, 341, 13, 440, 3827, 307, 437, 14246, 15626, 382, 364, 30874, 23831, 12512, 11, 597], "temperature": 0.0, "avg_logprob": -0.13767750263214112, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.6028037634896464e-06}, {"id": 195, "seek": 113080, "start": 1148.0, "end": 1155.44, "text": " is something that doesn't return the 50 probably nearest points, but returns 50 points which", "tokens": [307, 746, 300, 1177, 380, 2736, 264, 2625, 1391, 23831, 2793, 11, 457, 11247, 2625, 2793, 597], "temperature": 0.0, "avg_logprob": -0.13767750263214112, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.6028037634896464e-06}, {"id": 196, "seek": 115544, "start": 1155.44, "end": 1162.8, "text": " probabilistically closer ones are more likely to be there. But there are like zero guarantees.", "tokens": [31959, 20458, 4966, 2306, 366, 544, 3700, 281, 312, 456, 13, 583, 456, 366, 411, 4018, 32567, 13], "temperature": 0.0, "avg_logprob": -0.20686897701687282, "compression_ratio": 1.625, "no_speech_prob": 5.093683739687549e-06}, {"id": 197, "seek": 115544, "start": 1162.8, "end": 1168.38, "text": " We want it to be less good. So if you're interested during the week, if one of you wants to try", "tokens": [492, 528, 309, 281, 312, 1570, 665, 13, 407, 498, 291, 434, 3102, 1830, 264, 1243, 11, 498, 472, 295, 291, 2738, 281, 853], "temperature": 0.0, "avg_logprob": -0.20686897701687282, "compression_ratio": 1.625, "no_speech_prob": 5.093683739687549e-06}, {"id": 198, "seek": 115544, "start": 1168.38, "end": 1173.88, "text": " creating an approximate nearest neighbors algorithm, I think it will basically be a case of taking", "tokens": [4084, 364, 30874, 23831, 12512, 9284, 11, 286, 519, 309, 486, 1936, 312, 257, 1389, 295, 1940], "temperature": 0.0, "avg_logprob": -0.20686897701687282, "compression_ratio": 1.625, "no_speech_prob": 5.093683739687549e-06}, {"id": 199, "seek": 115544, "start": 1173.88, "end": 1180.0, "text": " like LSH or Boltree or something and removing a lot of the code, removing the code that", "tokens": [411, 441, 17308, 420, 363, 4837, 701, 420, 746, 293, 12720, 257, 688, 295, 264, 3089, 11, 12720, 264, 3089, 300], "temperature": 0.0, "avg_logprob": -0.20686897701687282, "compression_ratio": 1.625, "no_speech_prob": 5.093683739687549e-06}, {"id": 200, "seek": 118000, "start": 1180.0, "end": 1188.92, "text": " makes it better. So hopefully next week we'll see this working. But I thought it would be", "tokens": [1669, 309, 1101, 13, 407, 4696, 958, 1243, 321, 603, 536, 341, 1364, 13, 583, 286, 1194, 309, 576, 312], "temperature": 0.0, "avg_logprob": -0.13309904869566572, "compression_ratio": 1.759825327510917, "no_speech_prob": 2.6687184799811803e-05}, {"id": 201, "seek": 118000, "start": 1188.92, "end": 1192.88, "text": " interesting to see the intermediate stage.", "tokens": [1880, 281, 536, 264, 19376, 3233, 13], "temperature": 0.0, "avg_logprob": -0.13309904869566572, "compression_ratio": 1.759825327510917, "no_speech_prob": 2.6687184799811803e-05}, {"id": 202, "seek": 118000, "start": 1192.88, "end": 1198.48, "text": " And actually, this kind of thing of showing you the failures, you don't get to see most", "tokens": [400, 767, 11, 341, 733, 295, 551, 295, 4099, 291, 264, 20774, 11, 291, 500, 380, 483, 281, 536, 881], "temperature": 0.0, "avg_logprob": -0.13309904869566572, "compression_ratio": 1.759825327510917, "no_speech_prob": 2.6687184799811803e-05}, {"id": 203, "seek": 118000, "start": 1198.48, "end": 1202.9, "text": " of the failures. So I was working with some of the students over the weekend on implementing", "tokens": [295, 264, 20774, 13, 407, 286, 390, 1364, 365, 512, 295, 264, 1731, 670, 264, 6711, 322, 18114], "temperature": 0.0, "avg_logprob": -0.13309904869566572, "compression_ratio": 1.759825327510917, "no_speech_prob": 2.6687184799811803e-05}, {"id": 204, "seek": 118000, "start": 1202.9, "end": 1208.84, "text": " something that we'll see later on today. And actually on Saturday afternoon I was sitting", "tokens": [746, 300, 321, 603, 536, 1780, 322, 965, 13, 400, 767, 322, 8803, 6499, 286, 390, 3798], "temperature": 0.0, "avg_logprob": -0.13309904869566572, "compression_ratio": 1.759825327510917, "no_speech_prob": 2.6687184799811803e-05}, {"id": 205, "seek": 120884, "start": 1208.84, "end": 1213.72, "text": " with Melissa going through some of this coding. And at the end of it Melissa was like, Oh", "tokens": [365, 22844, 516, 807, 512, 295, 341, 17720, 13, 400, 412, 264, 917, 295, 309, 22844, 390, 411, 11, 876], "temperature": 0.0, "avg_logprob": -0.26601427737797534, "compression_ratio": 1.6920289855072463, "no_speech_prob": 2.5867037038551643e-05}, {"id": 206, "seek": 120884, "start": 1213.72, "end": 1218.84, "text": " wow, it's really interesting to see the process. Because you're coming on Monday and it's all", "tokens": [6076, 11, 309, 311, 534, 1880, 281, 536, 264, 1399, 13, 1436, 291, 434, 1348, 322, 8138, 293, 309, 311, 439], "temperature": 0.0, "avg_logprob": -0.26601427737797534, "compression_ratio": 1.6920289855072463, "no_speech_prob": 2.5867037038551643e-05}, {"id": 207, "seek": 120884, "start": 1218.84, "end": 1224.72, "text": " just working. Where else would we spend the entire Saturday afternoon slowly going through", "tokens": [445, 1364, 13, 2305, 1646, 576, 321, 3496, 264, 2302, 8803, 6499, 5692, 516, 807], "temperature": 0.0, "avg_logprob": -0.26601427737797534, "compression_ratio": 1.6920289855072463, "no_speech_prob": 2.5867037038551643e-05}, {"id": 208, "seek": 120884, "start": 1224.72, "end": 1230.0, "text": " one step at a time and constantly making mistakes and going back and trying to see what happened.", "tokens": [472, 1823, 412, 257, 565, 293, 6460, 1455, 8038, 293, 516, 646, 293, 1382, 281, 536, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.26601427737797534, "compression_ratio": 1.6920289855072463, "no_speech_prob": 2.5867037038551643e-05}, {"id": 209, "seek": 120884, "start": 1230.0, "end": 1238.3999999999999, "text": " So the actual process of everything you see in class, getting it working, is full of failures.", "tokens": [407, 264, 3539, 1399, 295, 1203, 291, 536, 294, 1508, 11, 1242, 309, 1364, 11, 307, 1577, 295, 20774, 13], "temperature": 0.0, "avg_logprob": -0.26601427737797534, "compression_ratio": 1.6920289855072463, "no_speech_prob": 2.5867037038551643e-05}, {"id": 210, "seek": 123840, "start": 1238.4, "end": 1243.48, "text": " In fact, Brad is currently working on building one of the two things we're doing for next", "tokens": [682, 1186, 11, 11895, 307, 4362, 1364, 322, 2390, 472, 295, 264, 732, 721, 321, 434, 884, 337, 958], "temperature": 0.0, "avg_logprob": -0.2373713631915231, "compression_ratio": 1.6703703703703703, "no_speech_prob": 4.399950194056146e-05}, {"id": 211, "seek": 123840, "start": 1243.48, "end": 1247.72, "text": " week. He just came up to me before class and he was like, Yeah, it's finished running.", "tokens": [1243, 13, 634, 445, 1361, 493, 281, 385, 949, 1508, 293, 415, 390, 411, 11, 865, 11, 309, 311, 4335, 2614, 13], "temperature": 0.0, "avg_logprob": -0.2373713631915231, "compression_ratio": 1.6703703703703703, "no_speech_prob": 4.399950194056146e-05}, {"id": 212, "seek": 123840, "start": 1247.72, "end": 1252.3600000000001, "text": " And nothing worked at all. And I was like, Yeah, of course. Nothing ever works the first", "tokens": [400, 1825, 2732, 412, 439, 13, 400, 286, 390, 411, 11, 865, 11, 295, 1164, 13, 6693, 1562, 1985, 264, 700], "temperature": 0.0, "avg_logprob": -0.2373713631915231, "compression_ratio": 1.6703703703703703, "no_speech_prob": 4.399950194056146e-05}, {"id": 213, "seek": 123840, "start": 1252.3600000000001, "end": 1253.3600000000001, "text": " time.", "tokens": [565, 13], "temperature": 0.0, "avg_logprob": -0.2373713631915231, "compression_ratio": 1.6703703703703703, "no_speech_prob": 4.399950194056146e-05}, {"id": 214, "seek": 123840, "start": 1253.3600000000001, "end": 1262.3200000000002, "text": " So part of the process is something I've been trying to talk to Brad about with his work.", "tokens": [407, 644, 295, 264, 1399, 307, 746, 286, 600, 668, 1382, 281, 751, 281, 11895, 466, 365, 702, 589, 13], "temperature": 0.0, "avg_logprob": -0.2373713631915231, "compression_ratio": 1.6703703703703703, "no_speech_prob": 4.399950194056146e-05}, {"id": 215, "seek": 123840, "start": 1262.3200000000002, "end": 1266.52, "text": " Recognizing that every time you build something new, it won't work means that you need to", "tokens": [44682, 3319, 300, 633, 565, 291, 1322, 746, 777, 11, 309, 1582, 380, 589, 1355, 300, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.2373713631915231, "compression_ratio": 1.6703703703703703, "no_speech_prob": 4.399950194056146e-05}, {"id": 216, "seek": 126652, "start": 1266.52, "end": 1272.12, "text": " build it in a very different way. Write an IPython notebook as if you're going to be", "tokens": [1322, 309, 294, 257, 588, 819, 636, 13, 23499, 364, 8671, 88, 11943, 21060, 382, 498, 291, 434, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.13527860139545642, "compression_ratio": 1.6397058823529411, "no_speech_prob": 7.07182061887579e-06}, {"id": 217, "seek": 126652, "start": 1272.12, "end": 1276.8799999999999, "text": " teaching it that next Monday's class. Because it won't work the first time, you're then", "tokens": [4571, 309, 300, 958, 8138, 311, 1508, 13, 1436, 309, 1582, 380, 589, 264, 700, 565, 11, 291, 434, 550], "temperature": 0.0, "avg_logprob": -0.13527860139545642, "compression_ratio": 1.6397058823529411, "no_speech_prob": 7.07182061887579e-06}, {"id": 218, "seek": 126652, "start": 1276.8799999999999, "end": 1282.2, "text": " going to have to go back and be like, Which one of these steps failed? So at every step,", "tokens": [516, 281, 362, 281, 352, 646, 293, 312, 411, 11, 3013, 472, 295, 613, 4439, 7612, 30, 407, 412, 633, 1823, 11], "temperature": 0.0, "avg_logprob": -0.13527860139545642, "compression_ratio": 1.6397058823529411, "no_speech_prob": 7.07182061887579e-06}, {"id": 219, "seek": 126652, "start": 1282.2, "end": 1288.52, "text": " you want to be printing out the results, summarizing the key statistics, drawing pictures, writing", "tokens": [291, 528, 281, 312, 14699, 484, 264, 3542, 11, 14611, 3319, 264, 2141, 12523, 11, 6316, 5242, 11, 3579], "temperature": 0.0, "avg_logprob": -0.13527860139545642, "compression_ratio": 1.6397058823529411, "no_speech_prob": 7.07182061887579e-06}, {"id": 220, "seek": 126652, "start": 1288.52, "end": 1293.74, "text": " down the reasoning that you did things, so that then when it doesn't work, you can go", "tokens": [760, 264, 21577, 300, 291, 630, 721, 11, 370, 300, 550, 562, 309, 1177, 380, 589, 11, 291, 393, 352], "temperature": 0.0, "avg_logprob": -0.13527860139545642, "compression_ratio": 1.6397058823529411, "no_speech_prob": 7.07182061887579e-06}, {"id": 221, "seek": 129374, "start": 1293.74, "end": 1299.24, "text": " back and be like, Okay, where did this go wrong? Or even better, hopefully seeing the", "tokens": [646, 293, 312, 411, 11, 1033, 11, 689, 630, 341, 352, 2085, 30, 1610, 754, 1101, 11, 4696, 2577, 264], "temperature": 0.0, "avg_logprob": -0.17216421826051012, "compression_ratio": 1.6640625, "no_speech_prob": 5.422170488600386e-06}, {"id": 222, "seek": 129374, "start": 1299.24, "end": 1303.6, "text": " mistake earlier on rather than waiting until it's all finished.", "tokens": [6146, 3071, 322, 2831, 813, 3806, 1826, 309, 311, 439, 4335, 13], "temperature": 0.0, "avg_logprob": -0.17216421826051012, "compression_ratio": 1.6640625, "no_speech_prob": 5.422170488600386e-06}, {"id": 223, "seek": 129374, "start": 1303.6, "end": 1311.64, "text": " So there's a whole lot of data science process steps which those of you who have worked with", "tokens": [407, 456, 311, 257, 1379, 688, 295, 1412, 3497, 1399, 4439, 597, 729, 295, 291, 567, 362, 2732, 365], "temperature": 0.0, "avg_logprob": -0.17216421826051012, "compression_ratio": 1.6640625, "no_speech_prob": 5.422170488600386e-06}, {"id": 224, "seek": 129374, "start": 1311.64, "end": 1316.74, "text": " data scientists will be pretty familiar with by now, hopefully, and those of you who haven't.", "tokens": [1412, 7708, 486, 312, 1238, 4963, 365, 538, 586, 11, 4696, 11, 293, 729, 295, 291, 567, 2378, 380, 13], "temperature": 0.0, "avg_logprob": -0.17216421826051012, "compression_ratio": 1.6640625, "no_speech_prob": 5.422170488600386e-06}, {"id": 225, "seek": 129374, "start": 1316.74, "end": 1322.72, "text": " It's really a case of bringing in the software engineering mindset. Lots of testing, lots", "tokens": [467, 311, 534, 257, 1389, 295, 5062, 294, 264, 4722, 7043, 12543, 13, 15908, 295, 4997, 11, 3195], "temperature": 0.0, "avg_logprob": -0.17216421826051012, "compression_ratio": 1.6640625, "no_speech_prob": 5.422170488600386e-06}, {"id": 226, "seek": 132272, "start": 1322.72, "end": 1330.04, "text": " of iterations. The more data scientists can learn about software engineering practices,", "tokens": [295, 36540, 13, 440, 544, 1412, 7708, 393, 1466, 466, 4722, 7043, 7525, 11], "temperature": 0.0, "avg_logprob": -0.19209051132202148, "compression_ratio": 1.3594771241830066, "no_speech_prob": 2.4439602839265717e-06}, {"id": 227, "seek": 132272, "start": 1330.04, "end": 1335.02, "text": " I think, the better.", "tokens": [286, 519, 11, 264, 1101, 13], "temperature": 0.0, "avg_logprob": -0.19209051132202148, "compression_ratio": 1.3594771241830066, "no_speech_prob": 2.4439602839265717e-06}, {"id": 228, "seek": 132272, "start": 1335.02, "end": 1345.92, "text": " So very interestingly, today or maybe yesterday, Facebook just announced that they have implemented", "tokens": [407, 588, 25873, 11, 965, 420, 1310, 5186, 11, 4384, 445, 7548, 300, 436, 362, 12270], "temperature": 0.0, "avg_logprob": -0.19209051132202148, "compression_ratio": 1.3594771241830066, "no_speech_prob": 2.4439602839265717e-06}, {"id": 229, "seek": 134592, "start": 1345.92, "end": 1353.76, "text": " an enormous improvement in the state of the art in approximate nearest neighbors. So you", "tokens": [364, 11322, 10444, 294, 264, 1785, 295, 264, 1523, 294, 30874, 23831, 12512, 13, 407, 291], "temperature": 0.0, "avg_logprob": -0.1950333508578214, "compression_ratio": 1.6544117647058822, "no_speech_prob": 8.800871000858024e-06}, {"id": 230, "seek": 134592, "start": 1353.76, "end": 1358.6000000000001, "text": " can check this out if you like, faiss.", "tokens": [393, 1520, 341, 484, 498, 291, 411, 11, 2050, 891, 13], "temperature": 0.0, "avg_logprob": -0.1950333508578214, "compression_ratio": 1.6544117647058822, "no_speech_prob": 8.800871000858024e-06}, {"id": 231, "seek": 134592, "start": 1358.6000000000001, "end": 1362.68, "text": " By the way, everything I mention here, I've mentioned earlier on Twitter. So if you follow", "tokens": [3146, 264, 636, 11, 1203, 286, 2152, 510, 11, 286, 600, 2835, 3071, 322, 5794, 13, 407, 498, 291, 1524], "temperature": 0.0, "avg_logprob": -0.1950333508578214, "compression_ratio": 1.6544117647058822, "no_speech_prob": 8.800871000858024e-06}, {"id": 232, "seek": 134592, "start": 1362.68, "end": 1367.04, "text": " me on Twitter or keep an eye on my Twitter account, you'll see all these things first,", "tokens": [385, 322, 5794, 420, 1066, 364, 3313, 322, 452, 5794, 2696, 11, 291, 603, 536, 439, 613, 721, 700, 11], "temperature": 0.0, "avg_logprob": -0.1950333508578214, "compression_ratio": 1.6544117647058822, "no_speech_prob": 8.800871000858024e-06}, {"id": 233, "seek": 134592, "start": 1367.04, "end": 1370.52, "text": " regardless of whether class is still running or not.", "tokens": [10060, 295, 1968, 1508, 307, 920, 2614, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.1950333508578214, "compression_ratio": 1.6544117647058822, "no_speech_prob": 8.800871000858024e-06}, {"id": 234, "seek": 134592, "start": 1370.52, "end": 1375.64, "text": " So this was particularly interesting to me, a. because I've been talking quite a bit during", "tokens": [407, 341, 390, 4098, 1880, 281, 385, 11, 257, 13, 570, 286, 600, 668, 1417, 1596, 257, 857, 1830], "temperature": 0.0, "avg_logprob": -0.1950333508578214, "compression_ratio": 1.6544117647058822, "no_speech_prob": 8.800871000858024e-06}, {"id": 235, "seek": 137564, "start": 1375.64, "end": 1380.48, "text": " this part of the course about approximate nearest neighbor. It's actually really important", "tokens": [341, 644, 295, 264, 1164, 466, 30874, 23831, 5987, 13, 467, 311, 767, 534, 1021], "temperature": 0.0, "avg_logprob": -0.1794189162876295, "compression_ratio": 1.6377952755905512, "no_speech_prob": 3.16916084557306e-05}, {"id": 236, "seek": 137564, "start": 1380.48, "end": 1386.0800000000002, "text": " for deep learning. You can get a sense of how important it is by how much Facebook has", "tokens": [337, 2452, 2539, 13, 509, 393, 483, 257, 2020, 295, 577, 1021, 309, 307, 538, 577, 709, 4384, 575], "temperature": 0.0, "avg_logprob": -0.1794189162876295, "compression_ratio": 1.6377952755905512, "no_speech_prob": 3.16916084557306e-05}, {"id": 237, "seek": 137564, "start": 1386.0800000000002, "end": 1392.64, "text": " invested in this. This is a multi-GPU distributed approximate nearest neighbor system that runs", "tokens": [13104, 294, 341, 13, 639, 307, 257, 4825, 12, 38, 8115, 12631, 30874, 23831, 5987, 1185, 300, 6676], "temperature": 0.0, "avg_logprob": -0.1794189162876295, "compression_ratio": 1.6377952755905512, "no_speech_prob": 3.16916084557306e-05}, {"id": 238, "seek": 137564, "start": 1392.64, "end": 1395.8000000000002, "text": " about 10 times faster than the previous state of the art.", "tokens": [466, 1266, 1413, 4663, 813, 264, 3894, 1785, 295, 264, 1523, 13], "temperature": 0.0, "avg_logprob": -0.1794189162876295, "compression_ratio": 1.6377952755905512, "no_speech_prob": 3.16916084557306e-05}, {"id": 239, "seek": 137564, "start": 1395.8000000000002, "end": 1401.3200000000002, "text": " Now I happened to get a particular insight into this because earlier this week I was", "tokens": [823, 286, 2011, 281, 483, 257, 1729, 11269, 666, 341, 570, 3071, 341, 1243, 286, 390], "temperature": 0.0, "avg_logprob": -0.1794189162876295, "compression_ratio": 1.6377952755905512, "no_speech_prob": 3.16916084557306e-05}, {"id": 240, "seek": 140132, "start": 1401.32, "end": 1406.52, "text": " at a conference at Berkeley, and thanks very much to Melissa for letting me know about", "tokens": [412, 257, 7586, 412, 23684, 11, 293, 3231, 588, 709, 281, 22844, 337, 8295, 385, 458, 466], "temperature": 0.0, "avg_logprob": -0.18075145865386388, "compression_ratio": 1.8846153846153846, "no_speech_prob": 3.2190764613915235e-05}, {"id": 241, "seek": 140132, "start": 1406.52, "end": 1409.6, "text": " the conference, and thanks very much to the people at the conference for letting me in", "tokens": [264, 7586, 11, 293, 3231, 588, 709, 281, 264, 561, 412, 264, 7586, 337, 8295, 385, 294], "temperature": 0.0, "avg_logprob": -0.18075145865386388, "compression_ratio": 1.8846153846153846, "no_speech_prob": 3.2190764613915235e-05}, {"id": 242, "seek": 140132, "start": 1409.6, "end": 1418.1599999999999, "text": " the day before when it had been full for months. I chatted to Jan LeCun, and I chatted to him", "tokens": [264, 786, 949, 562, 309, 632, 668, 1577, 337, 2493, 13, 286, 417, 32509, 281, 4956, 1456, 34, 409, 11, 293, 286, 417, 32509, 281, 796], "temperature": 0.0, "avg_logprob": -0.18075145865386388, "compression_ratio": 1.8846153846153846, "no_speech_prob": 3.2190764613915235e-05}, {"id": 243, "seek": 140132, "start": 1418.1599999999999, "end": 1423.24, "text": " about this thing I've been thinking about a lot during this part of the course about", "tokens": [466, 341, 551, 286, 600, 668, 1953, 466, 257, 688, 1830, 341, 644, 295, 264, 1164, 466], "temperature": 0.0, "avg_logprob": -0.18075145865386388, "compression_ratio": 1.8846153846153846, "no_speech_prob": 3.2190764613915235e-05}, {"id": 244, "seek": 140132, "start": 1423.24, "end": 1431.2, "text": " transfer learning. How come people seem to always use VGG for the best transfer learning", "tokens": [5003, 2539, 13, 1012, 808, 561, 1643, 281, 1009, 764, 691, 27561, 337, 264, 1151, 5003, 2539], "temperature": 0.0, "avg_logprob": -0.18075145865386388, "compression_ratio": 1.8846153846153846, "no_speech_prob": 3.2190764613915235e-05}, {"id": 245, "seek": 143120, "start": 1431.2, "end": 1433.88, "text": " results when there are so much better systems around.", "tokens": [3542, 562, 456, 366, 370, 709, 1101, 3652, 926, 13], "temperature": 0.0, "avg_logprob": -0.16799219234569654, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.169256160617806e-05}, {"id": 246, "seek": 143120, "start": 1433.88, "end": 1439.44, "text": " I've been really coming to this conclusion that the reason is that fully convolutional", "tokens": [286, 600, 668, 534, 1348, 281, 341, 10063, 300, 264, 1778, 307, 300, 4498, 45216, 304], "temperature": 0.0, "avg_logprob": -0.16799219234569654, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.169256160617806e-05}, {"id": 247, "seek": 143120, "start": 1439.44, "end": 1448.28, "text": " nets like ResNet and InceptionNet have very, very large, very, very redundant intermediate", "tokens": [36170, 411, 5015, 31890, 293, 682, 7311, 31890, 362, 588, 11, 588, 2416, 11, 588, 11, 588, 40997, 19376], "temperature": 0.0, "avg_logprob": -0.16799219234569654, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.169256160617806e-05}, {"id": 248, "seek": 143120, "start": 1448.28, "end": 1453.2, "text": " representations. So because they don't have a fully connected layer, like in ResNet, the", "tokens": [33358, 13, 407, 570, 436, 500, 380, 362, 257, 4498, 4582, 4583, 11, 411, 294, 5015, 31890, 11, 264], "temperature": 0.0, "avg_logprob": -0.16799219234569654, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.169256160617806e-05}, {"id": 249, "seek": 145320, "start": 1453.2, "end": 1462.88, "text": " penultimate layer is 7x7x2000, which A is huge, and B, if you look through that 7x7,", "tokens": [3435, 723, 2905, 4583, 307, 1614, 87, 22, 87, 25743, 11, 597, 316, 307, 2603, 11, 293, 363, 11, 498, 291, 574, 807, 300, 1614, 87, 22, 11], "temperature": 0.0, "avg_logprob": -0.16290847622618385, "compression_ratio": 1.5271966527196652, "no_speech_prob": 4.222806182951899e-06}, {"id": 250, "seek": 145320, "start": 1462.88, "end": 1466.24, "text": " most of them are the same more or less as their neighbors, because most parts of an", "tokens": [881, 295, 552, 366, 264, 912, 544, 420, 1570, 382, 641, 12512, 11, 570, 881, 3166, 295, 364], "temperature": 0.0, "avg_logprob": -0.16290847622618385, "compression_ratio": 1.5271966527196652, "no_speech_prob": 4.222806182951899e-06}, {"id": 251, "seek": 145320, "start": 1466.24, "end": 1472.04, "text": " image are similar.", "tokens": [3256, 366, 2531, 13], "temperature": 0.0, "avg_logprob": -0.16290847622618385, "compression_ratio": 1.5271966527196652, "no_speech_prob": 4.222806182951899e-06}, {"id": 252, "seek": 145320, "start": 1472.04, "end": 1476.2, "text": " If you want to go back far enough through enough bottleneck layers, you actually get", "tokens": [759, 291, 528, 281, 352, 646, 1400, 1547, 807, 1547, 44641, 547, 7914, 11, 291, 767, 483], "temperature": 0.0, "avg_logprob": -0.16290847622618385, "compression_ratio": 1.5271966527196652, "no_speech_prob": 4.222806182951899e-06}, {"id": 253, "seek": 145320, "start": 1476.2, "end": 1480.56, "text": " a fairly general representation. Like we do with VGG when we go to the first fully connected", "tokens": [257, 6457, 2674, 10290, 13, 1743, 321, 360, 365, 691, 27561, 562, 321, 352, 281, 264, 700, 4498, 4582], "temperature": 0.0, "avg_logprob": -0.16290847622618385, "compression_ratio": 1.5271966527196652, "no_speech_prob": 4.222806182951899e-06}, {"id": 254, "seek": 148056, "start": 1480.56, "end": 1489.04, "text": " layer, you're at 28x28x500. They're far too big to work with and far too redundant. So", "tokens": [4583, 11, 291, 434, 412, 7562, 87, 11205, 87, 7526, 13, 814, 434, 1400, 886, 955, 281, 589, 365, 293, 1400, 886, 40997, 13, 407], "temperature": 0.0, "avg_logprob": -0.19541284407692394, "compression_ratio": 1.4845814977973568, "no_speech_prob": 6.144096914795227e-06}, {"id": 255, "seek": 148056, "start": 1489.04, "end": 1495.2, "text": " I asked Jan LeCun about this and I said, is anybody working on the question of how do", "tokens": [286, 2351, 4956, 1456, 34, 409, 466, 341, 293, 286, 848, 11, 307, 4472, 1364, 322, 264, 1168, 295, 577, 360], "temperature": 0.0, "avg_logprob": -0.19541284407692394, "compression_ratio": 1.4845814977973568, "no_speech_prob": 6.144096914795227e-06}, {"id": 256, "seek": 148056, "start": 1495.2, "end": 1503.84, "text": " you capture the benefits of these much more accurate architectures but create efficient", "tokens": [291, 7983, 264, 5311, 295, 613, 709, 544, 8559, 6331, 1303, 457, 1884, 7148], "temperature": 0.0, "avg_logprob": -0.19541284407692394, "compression_ratio": 1.4845814977973568, "no_speech_prob": 6.144096914795227e-06}, {"id": 257, "seek": 148056, "start": 1503.84, "end": 1510.48, "text": " distributed representations. Jan LeCun was like, yes, absolutely, of course.", "tokens": [12631, 33358, 13, 4956, 1456, 34, 409, 390, 411, 11, 2086, 11, 3122, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.19541284407692394, "compression_ratio": 1.4845814977973568, "no_speech_prob": 6.144096914795227e-06}, {"id": 258, "seek": 151048, "start": 1510.48, "end": 1515.68, "text": " Where do I read about it? He was like, well, we use it in Facebook all the time. Like at", "tokens": [2305, 360, 286, 1401, 466, 309, 30, 634, 390, 411, 11, 731, 11, 321, 764, 309, 294, 4384, 439, 264, 565, 13, 1743, 412], "temperature": 0.0, "avg_logprob": -0.22174669901529948, "compression_ratio": 1.7716535433070866, "no_speech_prob": 1.8058308342006058e-05}, {"id": 259, "seek": 151048, "start": 1515.68, "end": 1521.84, "text": " Facebook we take every object in Facebook and we create a compressed distributed representation", "tokens": [4384, 321, 747, 633, 2657, 294, 4384, 293, 321, 1884, 257, 30353, 12631, 10290], "temperature": 0.0, "avg_logprob": -0.22174669901529948, "compression_ratio": 1.7716535433070866, "no_speech_prob": 1.8058308342006058e-05}, {"id": 260, "seek": 151048, "start": 1521.84, "end": 1528.88, "text": " of it and we save it in databases and then we give little bits of code to all the groups", "tokens": [295, 309, 293, 321, 3155, 309, 294, 22380, 293, 550, 321, 976, 707, 9239, 295, 3089, 281, 439, 264, 3935], "temperature": 0.0, "avg_logprob": -0.22174669901529948, "compression_ratio": 1.7716535433070866, "no_speech_prob": 1.8058308342006058e-05}, {"id": 261, "seek": 151048, "start": 1528.88, "end": 1533.28, "text": " so that they can create simple linear models on top of these distributed representations.", "tokens": [370, 300, 436, 393, 1884, 2199, 8213, 5245, 322, 1192, 295, 613, 12631, 33358, 13], "temperature": 0.0, "avg_logprob": -0.22174669901529948, "compression_ratio": 1.7716535433070866, "no_speech_prob": 1.8058308342006058e-05}, {"id": 262, "seek": 151048, "start": 1533.28, "end": 1538.48, "text": " This is everywhere in Facebook. And I was like, oh, so is this written down somewhere?", "tokens": [639, 307, 5315, 294, 4384, 13, 400, 286, 390, 411, 11, 1954, 11, 370, 307, 341, 3720, 760, 4079, 30], "temperature": 0.0, "avg_logprob": -0.22174669901529948, "compression_ratio": 1.7716535433070866, "no_speech_prob": 1.8058308342006058e-05}, {"id": 263, "seek": 153848, "start": 1538.48, "end": 1543.64, "text": " He was like, I don't know, maybe it's in a technical report somewhere. It's like one", "tokens": [634, 390, 411, 11, 286, 500, 380, 458, 11, 1310, 309, 311, 294, 257, 6191, 2275, 4079, 13, 467, 311, 411, 472], "temperature": 0.0, "avg_logprob": -0.18234709592965934, "compression_ratio": 1.6599190283400809, "no_speech_prob": 1.1125508535769768e-05}, {"id": 264, "seek": 153848, "start": 1543.64, "end": 1548.48, "text": " of these things that I knew it must be happening and so now I know at least one company, a", "tokens": [295, 613, 721, 300, 286, 2586, 309, 1633, 312, 2737, 293, 370, 586, 286, 458, 412, 1935, 472, 2237, 11, 257], "temperature": 0.0, "avg_logprob": -0.18234709592965934, "compression_ratio": 1.6599190283400809, "no_speech_prob": 1.1125508535769768e-05}, {"id": 265, "seek": 153848, "start": 1548.48, "end": 1552.88, "text": " very very big company, that it's happening at a huge scale.", "tokens": [588, 588, 955, 2237, 11, 300, 309, 311, 2737, 412, 257, 2603, 4373, 13], "temperature": 0.0, "avg_logprob": -0.18234709592965934, "compression_ratio": 1.6599190283400809, "no_speech_prob": 1.1125508535769768e-05}, {"id": 266, "seek": 153848, "start": 1552.88, "end": 1557.16, "text": " And so when they say here that this is used to search for multimedia documents that are", "tokens": [400, 370, 562, 436, 584, 510, 300, 341, 307, 1143, 281, 3164, 337, 49202, 8512, 300, 366], "temperature": 0.0, "avg_logprob": -0.18234709592965934, "compression_ratio": 1.6599190283400809, "no_speech_prob": 1.1125508535769768e-05}, {"id": 267, "seek": 153848, "start": 1557.16, "end": 1562.14, "text": " similar to each other, if we read between the lines, they're not talking about looking", "tokens": [2531, 281, 1184, 661, 11, 498, 321, 1401, 1296, 264, 3876, 11, 436, 434, 406, 1417, 466, 1237], "temperature": 0.0, "avg_logprob": -0.18234709592965934, "compression_ratio": 1.6599190283400809, "no_speech_prob": 1.1125508535769768e-05}, {"id": 268, "seek": 156214, "start": 1562.14, "end": 1572.76, "text": " at pixels or samples of audio, they're talking about activations, distributed representations.", "tokens": [412, 18668, 420, 10938, 295, 6278, 11, 436, 434, 1417, 466, 2430, 763, 11, 12631, 33358, 13], "temperature": 0.0, "avg_logprob": -0.12215843693963413, "compression_ratio": 1.526946107784431, "no_speech_prob": 2.046228837571107e-05}, {"id": 269, "seek": 156214, "start": 1572.76, "end": 1577.92, "text": " And we know now when we talk about perceptual losses, for example, how much better it is", "tokens": [400, 321, 458, 586, 562, 321, 751, 466, 43276, 901, 15352, 11, 337, 1365, 11, 577, 709, 1101, 309, 307], "temperature": 0.0, "avg_logprob": -0.12215843693963413, "compression_ratio": 1.526946107784431, "no_speech_prob": 2.046228837571107e-05}, {"id": 270, "seek": 156214, "start": 1577.92, "end": 1583.98, "text": " when you capture similarity using activations than you do using pixels.", "tokens": [562, 291, 7983, 32194, 1228, 2430, 763, 813, 291, 360, 1228, 18668, 13], "temperature": 0.0, "avg_logprob": -0.12215843693963413, "compression_ratio": 1.526946107784431, "no_speech_prob": 2.046228837571107e-05}, {"id": 271, "seek": 158398, "start": 1583.98, "end": 1592.6, "text": " So here's a huge opportunity for deep learning which, sure.", "tokens": [407, 510, 311, 257, 2603, 2650, 337, 2452, 2539, 597, 11, 988, 13], "temperature": 0.0, "avg_logprob": -0.21842734813690184, "compression_ratio": 1.5633802816901408, "no_speech_prob": 6.296705396380275e-05}, {"id": 272, "seek": 158398, "start": 1592.6, "end": 1600.68, "text": " I read briefly a YouTube paper in which they say something like, and I'm asking you because", "tokens": [286, 1401, 10515, 257, 3088, 3035, 294, 597, 436, 584, 746, 411, 11, 293, 286, 478, 3365, 291, 570], "temperature": 0.0, "avg_logprob": -0.21842734813690184, "compression_ratio": 1.5633802816901408, "no_speech_prob": 6.296705396380275e-05}, {"id": 273, "seek": 158398, "start": 1600.68, "end": 1604.96, "text": " maybe you have a better insight on how this is done, I have been very interested in understanding", "tokens": [1310, 291, 362, 257, 1101, 11269, 322, 577, 341, 307, 1096, 11, 286, 362, 668, 588, 3102, 294, 3701], "temperature": 0.0, "avg_logprob": -0.21842734813690184, "compression_ratio": 1.5633802816901408, "no_speech_prob": 6.296705396380275e-05}, {"id": 274, "seek": 158398, "start": 1604.96, "end": 1610.6, "text": " it better. So they basically do some kind of embedding of the user and embedding of", "tokens": [309, 1101, 13, 407, 436, 1936, 360, 512, 733, 295, 12240, 3584, 295, 264, 4195, 293, 12240, 3584, 295], "temperature": 0.0, "avg_logprob": -0.21842734813690184, "compression_ratio": 1.5633802816901408, "no_speech_prob": 6.296705396380275e-05}, {"id": 275, "seek": 161060, "start": 1610.6, "end": 1616.4399999999998, "text": " a video, which is probably something kind of similar to this. So they first build this", "tokens": [257, 960, 11, 597, 307, 1391, 746, 733, 295, 2531, 281, 341, 13, 407, 436, 700, 1322, 341], "temperature": 0.0, "avg_logprob": -0.2068212096755569, "compression_ratio": 1.5980861244019138, "no_speech_prob": 2.014369601965882e-05}, {"id": 276, "seek": 161060, "start": 1616.4399999999998, "end": 1621.9199999999998, "text": " kind of representation of every user and every video, and then on top of that they have another", "tokens": [733, 295, 10290, 295, 633, 4195, 293, 633, 960, 11, 293, 550, 322, 1192, 295, 300, 436, 362, 1071], "temperature": 0.0, "avg_logprob": -0.2068212096755569, "compression_ratio": 1.5980861244019138, "no_speech_prob": 2.014369601965882e-05}, {"id": 277, "seek": 161060, "start": 1621.9199999999998, "end": 1625.6799999999998, "text": " deep learning network that actually does everything else.", "tokens": [2452, 2539, 3209, 300, 767, 775, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.2068212096755569, "compression_ratio": 1.5980861244019138, "no_speech_prob": 2.014369601965882e-05}, {"id": 278, "seek": 161060, "start": 1625.6799999999998, "end": 1633.32, "text": " At YouTube, my understanding is most of their embeddings are based on more of a collaborative", "tokens": [1711, 3088, 11, 452, 3701, 307, 881, 295, 641, 12240, 29432, 366, 2361, 322, 544, 295, 257, 16555], "temperature": 0.0, "avg_logprob": -0.2068212096755569, "compression_ratio": 1.5980861244019138, "no_speech_prob": 2.014369601965882e-05}, {"id": 279, "seek": 163332, "start": 1633.32, "end": 1640.76, "text": " filtering approach like we did in Lesson 4. To my knowledge, they're not doing actual", "tokens": [30822, 3109, 411, 321, 630, 294, 18649, 266, 1017, 13, 1407, 452, 3601, 11, 436, 434, 406, 884, 3539], "temperature": 0.0, "avg_logprob": -0.20689222687169126, "compression_ratio": 1.4554455445544554, "no_speech_prob": 7.296086550923064e-06}, {"id": 280, "seek": 163332, "start": 1640.76, "end": 1648.72, "text": " video features, as far as I know. But there's no reason they shouldn't be. So I now know", "tokens": [960, 4122, 11, 382, 1400, 382, 286, 458, 13, 583, 456, 311, 572, 1778, 436, 4659, 380, 312, 13, 407, 286, 586, 458], "temperature": 0.0, "avg_logprob": -0.20689222687169126, "compression_ratio": 1.4554455445544554, "no_speech_prob": 7.296086550923064e-06}, {"id": 281, "seek": 163332, "start": 1648.72, "end": 1650.96, "text": " that Facebook absolutely are.", "tokens": [300, 4384, 3122, 366, 13], "temperature": 0.0, "avg_logprob": -0.20689222687169126, "compression_ratio": 1.4554455445544554, "no_speech_prob": 7.296086550923064e-06}, {"id": 282, "seek": 163332, "start": 1650.96, "end": 1659.52, "text": " So thinking about this, like I thought about this many years, in a medical context, every", "tokens": [407, 1953, 466, 341, 11, 411, 286, 1194, 466, 341, 867, 924, 11, 294, 257, 4625, 4319, 11, 633], "temperature": 0.0, "avg_logprob": -0.20689222687169126, "compression_ratio": 1.4554455445544554, "no_speech_prob": 7.296086550923064e-06}, {"id": 283, "seek": 165952, "start": 1659.52, "end": 1664.28, "text": " medical image you have, you have a compressed distributed representation of it, such as", "tokens": [4625, 3256, 291, 362, 11, 291, 362, 257, 30353, 12631, 10290, 295, 309, 11, 1270, 382], "temperature": 0.0, "avg_logprob": -0.22248866640288253, "compression_ratio": 1.6180257510729614, "no_speech_prob": 1.6963858797680587e-05}, {"id": 284, "seek": 165952, "start": 1664.28, "end": 1670.2, "text": " the first poorly connected layer of VGG. It's stored in a database. And when I say a database,", "tokens": [264, 700, 22271, 4582, 4583, 295, 691, 27561, 13, 467, 311, 12187, 294, 257, 8149, 13, 400, 562, 286, 584, 257, 8149, 11], "temperature": 0.0, "avg_logprob": -0.22248866640288253, "compression_ratio": 1.6180257510729614, "no_speech_prob": 1.6963858797680587e-05}, {"id": 285, "seek": 165952, "start": 1670.2, "end": 1676.48, "text": " I actually mean a fast indexed approximate nearest neighbor's tree or structure. And", "tokens": [286, 767, 914, 257, 2370, 8186, 292, 30874, 23831, 5987, 311, 4230, 420, 3877, 13, 400], "temperature": 0.0, "avg_logprob": -0.22248866640288253, "compression_ratio": 1.6180257510729614, "no_speech_prob": 1.6963858797680587e-05}, {"id": 286, "seek": 165952, "start": 1676.48, "end": 1682.16, "text": " now you can go and grab every medical image that displays similar symptoms to this medical", "tokens": [586, 291, 393, 352, 293, 4444, 633, 4625, 3256, 300, 20119, 2531, 8332, 281, 341, 4625], "temperature": 0.0, "avg_logprob": -0.22248866640288253, "compression_ratio": 1.6180257510729614, "no_speech_prob": 1.6963858797680587e-05}, {"id": 287, "seek": 165952, "start": 1682.16, "end": 1683.16, "text": " image or whatever.", "tokens": [3256, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.22248866640288253, "compression_ratio": 1.6180257510729614, "no_speech_prob": 1.6963858797680587e-05}, {"id": 288, "seek": 168316, "start": 1683.16, "end": 1691.44, "text": " A guy from Kaiser is giving me the thumbs up. So this kind of stuff is really exciting.", "tokens": [316, 2146, 490, 42066, 307, 2902, 385, 264, 8838, 493, 13, 407, 341, 733, 295, 1507, 307, 534, 4670, 13], "temperature": 0.0, "avg_logprob": -0.22560136968439276, "compression_ratio": 1.434782608695652, "no_speech_prob": 1.670081474003382e-05}, {"id": 289, "seek": 168316, "start": 1691.44, "end": 1696.8400000000001, "text": " And as far as I know, it hasn't really been written down anywhere, and it's not really", "tokens": [400, 382, 1400, 382, 286, 458, 11, 309, 6132, 380, 534, 668, 3720, 760, 4992, 11, 293, 309, 311, 406, 534], "temperature": 0.0, "avg_logprob": -0.22560136968439276, "compression_ratio": 1.434782608695652, "no_speech_prob": 1.670081474003382e-05}, {"id": 290, "seek": 168316, "start": 1696.8400000000001, "end": 1706.52, "text": " being used anywhere much other than at least Facebook, who probably Google does this too.", "tokens": [885, 1143, 4992, 709, 661, 813, 412, 1935, 4384, 11, 567, 1391, 3329, 775, 341, 886, 13], "temperature": 0.0, "avg_logprob": -0.22560136968439276, "compression_ratio": 1.434782608695652, "no_speech_prob": 1.670081474003382e-05}, {"id": 291, "seek": 170652, "start": 1706.52, "end": 1714.58, "text": " So that's just a bit of background. We are going to talk about the BiLSTM hegemony. These", "tokens": [407, 300, 311, 445, 257, 857, 295, 3678, 13, 492, 366, 516, 281, 751, 466, 264, 13007, 43, 6840, 44, 415, 26322, 2526, 13, 1981], "temperature": 0.0, "avg_logprob": -0.16697635231437263, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.757369970320724e-05}, {"id": 292, "seek": 170652, "start": 1714.58, "end": 1720.4, "text": " slides come from Chris Manning. Who here's come across Chris Manning before? So Chris", "tokens": [9788, 808, 490, 6688, 2458, 773, 13, 2102, 510, 311, 808, 2108, 6688, 2458, 773, 949, 30, 407, 6688], "temperature": 0.0, "avg_logprob": -0.16697635231437263, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.757369970320724e-05}, {"id": 293, "seek": 170652, "start": 1720.4, "end": 1728.36, "text": " is a linguistics professor at Stanford. He did his PhD in linguistics. And at some point", "tokens": [307, 257, 21766, 6006, 8304, 412, 20374, 13, 634, 630, 702, 14476, 294, 21766, 6006, 13, 400, 412, 512, 935], "temperature": 0.0, "avg_logprob": -0.16697635231437263, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.757369970320724e-05}, {"id": 294, "seek": 170652, "start": 1728.36, "end": 1733.6399999999999, "text": " in the last small number of years, he discovered that everything he learned about linguistics", "tokens": [294, 264, 1036, 1359, 1230, 295, 924, 11, 415, 6941, 300, 1203, 415, 3264, 466, 21766, 6006], "temperature": 0.0, "avg_logprob": -0.16697635231437263, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.757369970320724e-05}, {"id": 295, "seek": 173364, "start": 1733.64, "end": 1739.88, "text": " was basically a waste of time because now you can throw a bidirectional LSTM with attention", "tokens": [390, 1936, 257, 5964, 295, 565, 570, 586, 291, 393, 3507, 257, 12957, 621, 41048, 441, 6840, 44, 365, 3202], "temperature": 0.0, "avg_logprob": -0.13412328698169226, "compression_ratio": 1.5798319327731092, "no_speech_prob": 7.183109119068831e-06}, {"id": 296, "seek": 173364, "start": 1739.88, "end": 1745.1200000000001, "text": " at pretty much anything and get a better result than everything from his PhD in linguistics.", "tokens": [412, 1238, 709, 1340, 293, 483, 257, 1101, 1874, 813, 1203, 490, 702, 14476, 294, 21766, 6006, 13], "temperature": 0.0, "avg_logprob": -0.13412328698169226, "compression_ratio": 1.5798319327731092, "no_speech_prob": 7.183109119068831e-06}, {"id": 297, "seek": 173364, "start": 1745.1200000000001, "end": 1750.48, "text": " So nowadays he actually teaches deep learning, and in fact his deep learning for natural", "tokens": [407, 13434, 415, 767, 16876, 2452, 2539, 11, 293, 294, 1186, 702, 2452, 2539, 337, 3303], "temperature": 0.0, "avg_logprob": -0.13412328698169226, "compression_ratio": 1.5798319327731092, "no_speech_prob": 7.183109119068831e-06}, {"id": 298, "seek": 173364, "start": 1750.48, "end": 1756.24, "text": " language processing videos just went online today. So if you want to learn more about", "tokens": [2856, 9007, 2145, 445, 1437, 2950, 965, 13, 407, 498, 291, 528, 281, 1466, 544, 466], "temperature": 0.0, "avg_logprob": -0.13412328698169226, "compression_ratio": 1.5798319327731092, "no_speech_prob": 7.183109119068831e-06}, {"id": 299, "seek": 173364, "start": 1756.24, "end": 1760.76, "text": " that, feel free.", "tokens": [300, 11, 841, 1737, 13], "temperature": 0.0, "avg_logprob": -0.13412328698169226, "compression_ratio": 1.5798319327731092, "no_speech_prob": 7.183109119068831e-06}, {"id": 300, "seek": 176076, "start": 1760.76, "end": 1767.68, "text": " So at this conference last week, this is one of the slides he put up. He described himself", "tokens": [407, 412, 341, 7586, 1036, 1243, 11, 341, 307, 472, 295, 264, 9788, 415, 829, 493, 13, 634, 7619, 3647], "temperature": 0.0, "avg_logprob": -0.14509511501231093, "compression_ratio": 1.6771300448430493, "no_speech_prob": 2.1444258891278878e-05}, {"id": 301, "seek": 176076, "start": 1767.68, "end": 1771.92, "text": " as pretty disappointed about this situation. It's not where he wants linguistics to be,", "tokens": [382, 1238, 13856, 466, 341, 2590, 13, 467, 311, 406, 689, 415, 2738, 21766, 6006, 281, 312, 11], "temperature": 0.0, "avg_logprob": -0.14509511501231093, "compression_ratio": 1.6771300448430493, "no_speech_prob": 2.1444258891278878e-05}, {"id": 302, "seek": 176076, "start": 1771.92, "end": 1776.16, "text": " but there it is. And so this is what we're going to learn about today, bidirectional", "tokens": [457, 456, 309, 307, 13, 400, 370, 341, 307, 437, 321, 434, 516, 281, 1466, 466, 965, 11, 12957, 621, 41048], "temperature": 0.0, "avg_logprob": -0.14509511501231093, "compression_ratio": 1.6771300448430493, "no_speech_prob": 2.1444258891278878e-05}, {"id": 303, "seek": 176076, "start": 1776.16, "end": 1780.74, "text": " LSTMs with attention.", "tokens": [441, 6840, 26386, 365, 3202, 13], "temperature": 0.0, "avg_logprob": -0.14509511501231093, "compression_ratio": 1.6771300448430493, "no_speech_prob": 2.1444258891278878e-05}, {"id": 304, "seek": 176076, "start": 1780.74, "end": 1784.72, "text": " So this is what happened when people started throwing bidirectional LSTMs with attention", "tokens": [407, 341, 307, 437, 2011, 562, 561, 1409, 10238, 12957, 621, 41048, 441, 6840, 26386, 365, 3202], "temperature": 0.0, "avg_logprob": -0.14509511501231093, "compression_ratio": 1.6771300448430493, "no_speech_prob": 2.1444258891278878e-05}, {"id": 305, "seek": 178472, "start": 1784.72, "end": 1791.84, "text": " at neural translation. This looks a lot like the 2012 ImageNet picture. The red is the", "tokens": [412, 18161, 12853, 13, 639, 1542, 257, 688, 411, 264, 9125, 29903, 31890, 3036, 13, 440, 2182, 307, 264], "temperature": 0.0, "avg_logprob": -0.14670414802355644, "compression_ratio": 1.7251184834123223, "no_speech_prob": 6.502438918687403e-05}, {"id": 306, "seek": 178472, "start": 1791.84, "end": 1800.8, "text": " kind of two generations ago approach to statistical machine translation, phrase-based. The purple", "tokens": [733, 295, 732, 10593, 2057, 3109, 281, 22820, 3479, 12853, 11, 9535, 12, 6032, 13, 440, 9656], "temperature": 0.0, "avg_logprob": -0.14670414802355644, "compression_ratio": 1.7251184834123223, "no_speech_prob": 6.502438918687403e-05}, {"id": 307, "seek": 178472, "start": 1800.8, "end": 1806.64, "text": " is the last four years of the next generation approach, which is syntax-based statistical", "tokens": [307, 264, 1036, 1451, 924, 295, 264, 958, 5125, 3109, 11, 597, 307, 28431, 12, 6032, 22820], "temperature": 0.0, "avg_logprob": -0.14670414802355644, "compression_ratio": 1.7251184834123223, "no_speech_prob": 6.502438918687403e-05}, {"id": 308, "seek": 178472, "start": 1806.64, "end": 1812.32, "text": " machine translation. Neural machine translation didn't really appear properly until 2015,", "tokens": [3479, 12853, 13, 1734, 1807, 3479, 12853, 994, 380, 534, 4204, 6108, 1826, 7546, 11], "temperature": 0.0, "avg_logprob": -0.14670414802355644, "compression_ratio": 1.7251184834123223, "no_speech_prob": 6.502438918687403e-05}, {"id": 309, "seek": 181232, "start": 1812.32, "end": 1816.72, "text": " and this is the path that that's now on. And we're probably well beyond that now because", "tokens": [293, 341, 307, 264, 3100, 300, 300, 311, 586, 322, 13, 400, 321, 434, 1391, 731, 4399, 300, 586, 570], "temperature": 0.0, "avg_logprob": -0.15703138784200205, "compression_ratio": 1.6546052631578947, "no_speech_prob": 1.1842845196952112e-05}, {"id": 310, "seek": 181232, "start": 1816.72, "end": 1821.96, "text": " of course Google's neural machine translation system is now online and a lot of the stuff", "tokens": [295, 1164, 3329, 311, 18161, 3479, 12853, 1185, 307, 586, 2950, 293, 257, 688, 295, 264, 1507], "temperature": 0.0, "avg_logprob": -0.15703138784200205, "compression_ratio": 1.6546052631578947, "no_speech_prob": 1.1842845196952112e-05}, {"id": 311, "seek": 181232, "start": 1821.96, "end": 1824.76, "text": " that's coming out of that is appearing in papers now.", "tokens": [300, 311, 1348, 484, 295, 300, 307, 19870, 294, 10577, 586, 13], "temperature": 0.0, "avg_logprob": -0.15703138784200205, "compression_ratio": 1.6546052631578947, "no_speech_prob": 1.1842845196952112e-05}, {"id": 312, "seek": 181232, "start": 1824.76, "end": 1832.96, "text": " So actually back here in about 2013, I actually gave a talk at one of the academic conferences,", "tokens": [407, 767, 646, 510, 294, 466, 9012, 11, 286, 767, 2729, 257, 751, 412, 472, 295, 264, 7778, 22032, 11], "temperature": 0.0, "avg_logprob": -0.15703138784200205, "compression_ratio": 1.6546052631578947, "no_speech_prob": 1.1842845196952112e-05}, {"id": 313, "seek": 181232, "start": 1832.96, "end": 1838.28, "text": " which is like an introduction to deep learning for people in the statistics academia who", "tokens": [597, 307, 411, 364, 9339, 281, 2452, 2539, 337, 561, 294, 264, 12523, 28937, 567], "temperature": 0.0, "avg_logprob": -0.15703138784200205, "compression_ratio": 1.6546052631578947, "no_speech_prob": 1.1842845196952112e-05}, {"id": 314, "seek": 181232, "start": 1838.28, "end": 1841.6, "text": " maybe weren't familiar with it. One of the things I told them was, all of you who are", "tokens": [1310, 4999, 380, 4963, 365, 309, 13, 1485, 295, 264, 721, 286, 1907, 552, 390, 11, 439, 295, 291, 567, 366], "temperature": 0.0, "avg_logprob": -0.15703138784200205, "compression_ratio": 1.6546052631578947, "no_speech_prob": 1.1842845196952112e-05}, {"id": 315, "seek": 184160, "start": 1841.6, "end": 1846.52, "text": " in NLP, start learning deep learning now because there's no question in the next three years", "tokens": [294, 426, 45196, 11, 722, 2539, 2452, 2539, 586, 570, 456, 311, 572, 1168, 294, 264, 958, 1045, 924], "temperature": 0.0, "avg_logprob": -0.1582687339004205, "compression_ratio": 1.6264591439688716, "no_speech_prob": 5.5942423387023155e-06}, {"id": 316, "seek": 184160, "start": 1846.52, "end": 1850.1599999999999, "text": " or so it's going to be the state of the art in pretty much everything. So hopefully some", "tokens": [420, 370, 309, 311, 516, 281, 312, 264, 1785, 295, 264, 1523, 294, 1238, 709, 1203, 13, 407, 4696, 512], "temperature": 0.0, "avg_logprob": -0.1582687339004205, "compression_ratio": 1.6264591439688716, "no_speech_prob": 5.5942423387023155e-06}, {"id": 317, "seek": 184160, "start": 1850.1599999999999, "end": 1854.6, "text": " of those people listen to me and today they're very happy.", "tokens": [295, 729, 561, 2140, 281, 385, 293, 965, 436, 434, 588, 2055, 13], "temperature": 0.0, "avg_logprob": -0.1582687339004205, "compression_ratio": 1.6264591439688716, "no_speech_prob": 5.5942423387023155e-06}, {"id": 318, "seek": 184160, "start": 1854.6, "end": 1859.9199999999998, "text": " Even though we couldn't exactly tell at that point how that was going to happen, it was", "tokens": [2754, 1673, 321, 2809, 380, 2293, 980, 412, 300, 935, 577, 300, 390, 516, 281, 1051, 11, 309, 390], "temperature": 0.0, "avg_logprob": -0.1582687339004205, "compression_ratio": 1.6264591439688716, "no_speech_prob": 5.5942423387023155e-06}, {"id": 319, "seek": 184160, "start": 1859.9199999999998, "end": 1867.4399999999998, "text": " just really, really obvious that it's a system which uses distributed representations and", "tokens": [445, 534, 11, 534, 6322, 300, 309, 311, 257, 1185, 597, 4960, 12631, 33358, 293], "temperature": 0.0, "avg_logprob": -0.1582687339004205, "compression_ratio": 1.6264591439688716, "no_speech_prob": 5.5942423387023155e-06}, {"id": 320, "seek": 186744, "start": 1867.44, "end": 1872.2, "text": " has all of the properties that you would expect to see in something where deep learning will", "tokens": [575, 439, 295, 264, 7221, 300, 291, 576, 2066, 281, 536, 294, 746, 689, 2452, 2539, 486], "temperature": 0.0, "avg_logprob": -0.20748556163949025, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.300668009207584e-05}, {"id": 321, "seek": 186744, "start": 1872.2, "end": 1882.06, "text": " be successful. Today I think increasingly people are realizing that large areas of classic", "tokens": [312, 4406, 13, 2692, 286, 519, 12980, 561, 366, 16734, 300, 2416, 3179, 295, 7230], "temperature": 0.0, "avg_logprob": -0.20748556163949025, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.300668009207584e-05}, {"id": 322, "seek": 186744, "start": 1882.06, "end": 1886.96, "text": " statistics and machine learning and the process are being replaced.", "tokens": [12523, 293, 3479, 2539, 293, 264, 1399, 366, 885, 10772, 13], "temperature": 0.0, "avg_logprob": -0.20748556163949025, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.300668009207584e-05}, {"id": 323, "seek": 186744, "start": 1886.96, "end": 1893.24, "text": " So Chris described the four big wins of neural machine translation, and I think one of the", "tokens": [407, 6688, 7619, 264, 1451, 955, 10641, 295, 18161, 3479, 12853, 11, 293, 286, 519, 472, 295, 264], "temperature": 0.0, "avg_logprob": -0.20748556163949025, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.300668009207584e-05}, {"id": 324, "seek": 189324, "start": 1893.24, "end": 1901.64, "text": " most interesting ones is number 3, which is the statistical approaches tended to use n-grams.", "tokens": [881, 1880, 2306, 307, 1230, 805, 11, 597, 307, 264, 22820, 11587, 34732, 281, 764, 297, 12, 1342, 82, 13], "temperature": 0.0, "avg_logprob": -0.18585586547851562, "compression_ratio": 1.7463414634146341, "no_speech_prob": 4.264498784323223e-05}, {"id": 325, "seek": 189324, "start": 1901.64, "end": 1906.0, "text": " So like these three words appear, these three words appear, these three words appear next", "tokens": [407, 411, 613, 1045, 2283, 4204, 11, 613, 1045, 2283, 4204, 11, 613, 1045, 2283, 4204, 958], "temperature": 0.0, "avg_logprob": -0.18585586547851562, "compression_ratio": 1.7463414634146341, "no_speech_prob": 4.264498784323223e-05}, {"id": 326, "seek": 189324, "start": 1906.0, "end": 1912.84, "text": " to each other in these situations. You can't get beyond about 5-grams, so a list of 5 words,", "tokens": [281, 1184, 661, 294, 613, 6851, 13, 509, 393, 380, 483, 4399, 466, 1025, 12, 1342, 82, 11, 370, 257, 1329, 295, 1025, 2283, 11], "temperature": 0.0, "avg_logprob": -0.18585586547851562, "compression_ratio": 1.7463414634146341, "no_speech_prob": 4.264498784323223e-05}, {"id": 327, "seek": 189324, "start": 1912.84, "end": 1918.4, "text": " because you just get this total exponential explosion of how big that dataset is.", "tokens": [570, 291, 445, 483, 341, 3217, 21510, 15673, 295, 577, 955, 300, 28872, 307, 13], "temperature": 0.0, "avg_logprob": -0.18585586547851562, "compression_ratio": 1.7463414634146341, "no_speech_prob": 4.264498784323223e-05}, {"id": 328, "seek": 191840, "start": 1918.4, "end": 1925.0, "text": " With RNNs, remember when we first learned about RNNs, we talked the reason why, stateful,", "tokens": [2022, 45702, 45, 82, 11, 1604, 562, 321, 700, 3264, 466, 45702, 45, 82, 11, 321, 2825, 264, 1778, 983, 11, 1785, 906, 11], "temperature": 0.0, "avg_logprob": -0.14210576596467392, "compression_ratio": 1.543859649122807, "no_speech_prob": 1.2805297956219874e-05}, {"id": 329, "seek": 191840, "start": 1925.0, "end": 1931.3600000000001, "text": " memory, long-term dependencies. So this is exactly what you want if you want to make", "tokens": [4675, 11, 938, 12, 7039, 36606, 13, 407, 341, 307, 2293, 437, 291, 528, 498, 291, 528, 281, 652], "temperature": 0.0, "avg_logprob": -0.14210576596467392, "compression_ratio": 1.543859649122807, "no_speech_prob": 1.2805297956219874e-05}, {"id": 330, "seek": 191840, "start": 1931.3600000000001, "end": 1938.88, "text": " sure that your verb tense and everything else lines up with your details of your subject.", "tokens": [988, 300, 428, 9595, 18760, 293, 1203, 1646, 3876, 493, 365, 428, 4365, 295, 428, 3983, 13], "temperature": 0.0, "avg_logprob": -0.14210576596467392, "compression_ratio": 1.543859649122807, "no_speech_prob": 1.2805297956219874e-05}, {"id": 331, "seek": 191840, "start": 1938.88, "end": 1943.8400000000001, "text": " They could be 20 words apart or more, so we need to be able to have that kind of state.", "tokens": [814, 727, 312, 945, 2283, 4936, 420, 544, 11, 370, 321, 643, 281, 312, 1075, 281, 362, 300, 733, 295, 1785, 13], "temperature": 0.0, "avg_logprob": -0.14210576596467392, "compression_ratio": 1.543859649122807, "no_speech_prob": 1.2805297956219874e-05}, {"id": 332, "seek": 194384, "start": 1943.84, "end": 1948.72, "text": " So this is where this works very well.", "tokens": [407, 341, 307, 689, 341, 1985, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.24000634586109834, "compression_ratio": 1.5097087378640777, "no_speech_prob": 5.338063147064531e-06}, {"id": 333, "seek": 194384, "start": 1948.72, "end": 1955.4399999999998, "text": " Interestingly, I'm not deep in the NLP world, so I wasn't quite clear on where things are", "tokens": [30564, 11, 286, 478, 406, 2452, 294, 264, 426, 45196, 1002, 11, 370, 286, 2067, 380, 1596, 1850, 322, 689, 721, 366], "temperature": 0.0, "avg_logprob": -0.24000634586109834, "compression_ratio": 1.5097087378640777, "no_speech_prob": 5.338063147064531e-06}, {"id": 334, "seek": 194384, "start": 1955.4399999999998, "end": 1962.3999999999999, "text": " with all of the states that say the art. Chris said in his talk, BIOS-TMs with attention", "tokens": [365, 439, 295, 264, 4368, 300, 584, 264, 1523, 13, 6688, 848, 294, 702, 751, 11, 23524, 4367, 12, 51, 26386, 365, 3202], "temperature": 0.0, "avg_logprob": -0.24000634586109834, "compression_ratio": 1.5097087378640777, "no_speech_prob": 5.338063147064531e-06}, {"id": 335, "seek": 194384, "start": 1962.3999999999999, "end": 1972.6799999999998, "text": " are the state of the art for basically everything. We actually have an NLP person here. Can I", "tokens": [366, 264, 1785, 295, 264, 1523, 337, 1936, 1203, 13, 492, 767, 362, 364, 426, 45196, 954, 510, 13, 1664, 286], "temperature": 0.0, "avg_logprob": -0.24000634586109834, "compression_ratio": 1.5097087378640777, "no_speech_prob": 5.338063147064531e-06}, {"id": 336, "seek": 197268, "start": 1972.68, "end": 1983.52, "text": " ask you a quick question, Anna, about this? Is this kind of line up with your experience?", "tokens": [1029, 291, 257, 1702, 1168, 11, 12899, 11, 466, 341, 30, 1119, 341, 733, 295, 1622, 493, 365, 428, 1752, 30], "temperature": 0.0, "avg_logprob": -0.42275413713957133, "compression_ratio": 1.402061855670103, "no_speech_prob": 3.2697240385459736e-05}, {"id": 337, "seek": 197268, "start": 1983.52, "end": 1986.44, "text": " You're like an NLP researcher.", "tokens": [509, 434, 411, 364, 426, 45196, 21751, 13], "temperature": 0.0, "avg_logprob": -0.42275413713957133, "compression_ratio": 1.402061855670103, "no_speech_prob": 3.2697240385459736e-05}, {"id": 338, "seek": 197268, "start": 1986.44, "end": 1988.8, "text": " Anna Rammelmann Yeah, my research is mostly in information", "tokens": [12899, 497, 5136, 338, 14912, 865, 11, 452, 2132, 307, 5240, 294, 1589], "temperature": 0.0, "avg_logprob": -0.42275413713957133, "compression_ratio": 1.402061855670103, "no_speech_prob": 3.2697240385459736e-05}, {"id": 339, "seek": 197268, "start": 1988.8, "end": 1998.6000000000001, "text": " extraction. But first of all, if Chris Manning says it, it's true. So the other part of this", "tokens": [30197, 13, 583, 700, 295, 439, 11, 498, 6688, 2458, 773, 1619, 309, 11, 309, 311, 2074, 13, 407, 264, 661, 644, 295, 341], "temperature": 0.0, "avg_logprob": -0.42275413713957133, "compression_ratio": 1.402061855670103, "no_speech_prob": 3.2697240385459736e-05}, {"id": 340, "seek": 199860, "start": 1998.6, "end": 2005.3999999999999, "text": " is that in some of those areas, while this is state of the art, definitely the state", "tokens": [307, 300, 294, 512, 295, 729, 3179, 11, 1339, 341, 307, 1785, 295, 264, 1523, 11, 2138, 264, 1785], "temperature": 0.0, "avg_logprob": -0.24671394069020341, "compression_ratio": 1.701086956521739, "no_speech_prob": 8.349074050784111e-05}, {"id": 341, "seek": 199860, "start": 2005.3999999999999, "end": 2011.76, "text": " of the art is not that impressive. But he's right, this is what's happened. People have", "tokens": [295, 264, 1523, 307, 406, 300, 8992, 13, 583, 415, 311, 558, 11, 341, 307, 437, 311, 2011, 13, 3432, 362], "temperature": 0.0, "avg_logprob": -0.24671394069020341, "compression_ratio": 1.701086956521739, "no_speech_prob": 8.349074050784111e-05}, {"id": 342, "seek": 199860, "start": 2011.76, "end": 2018.12, "text": " found that for all those tasks, and others which are not as severe, which are kind of", "tokens": [1352, 300, 337, 439, 729, 9608, 11, 293, 2357, 597, 366, 406, 382, 8922, 11, 597, 366, 733, 295], "temperature": 0.0, "avg_logprob": -0.24671394069020341, "compression_ratio": 1.701086956521739, "no_speech_prob": 8.349074050784111e-05}, {"id": 343, "seek": 199860, "start": 2018.12, "end": 2021.3999999999999, "text": " similar in certain ways. That is the state of the art.", "tokens": [2531, 294, 1629, 2098, 13, 663, 307, 264, 1785, 295, 264, 1523, 13], "temperature": 0.0, "avg_logprob": -0.24671394069020341, "compression_ratio": 1.701086956521739, "no_speech_prob": 8.349074050784111e-05}, {"id": 344, "seek": 202140, "start": 2021.4, "end": 2029.48, "text": " What Anna was talking about the kind of disappointing state of this. So on one of these slides,", "tokens": [708, 12899, 390, 1417, 466, 264, 733, 295, 25054, 1785, 295, 341, 13, 407, 322, 472, 295, 613, 9788, 11], "temperature": 0.0, "avg_logprob": -0.21341619653216862, "compression_ratio": 1.6981132075471699, "no_speech_prob": 7.296224794117734e-06}, {"id": 345, "seek": 202140, "start": 2029.48, "end": 2032.44, "text": " Chris Manning had a big frowny face, and one of the audience was like, what's with the", "tokens": [6688, 2458, 773, 632, 257, 955, 431, 648, 88, 1851, 11, 293, 472, 295, 264, 4034, 390, 411, 11, 437, 311, 365, 264], "temperature": 0.0, "avg_logprob": -0.21341619653216862, "compression_ratio": 1.6981132075471699, "no_speech_prob": 7.296224794117734e-06}, {"id": 346, "seek": 202140, "start": 2032.44, "end": 2038.72, "text": " frowny face? And he's basically describing all of the ways in which the state of the", "tokens": [431, 648, 88, 1851, 30, 400, 415, 311, 1936, 16141, 439, 295, 264, 2098, 294, 597, 264, 1785, 295, 264], "temperature": 0.0, "avg_logprob": -0.21341619653216862, "compression_ratio": 1.6981132075471699, "no_speech_prob": 7.296224794117734e-06}, {"id": 347, "seek": 202140, "start": 2038.72, "end": 2044.96, "text": " art results still are far short of where we want them to be. So NLP is by no means solved.", "tokens": [1523, 3542, 920, 366, 1400, 2099, 295, 689, 321, 528, 552, 281, 312, 13, 407, 426, 45196, 307, 538, 572, 1355, 13041, 13], "temperature": 0.0, "avg_logprob": -0.21341619653216862, "compression_ratio": 1.6981132075471699, "no_speech_prob": 7.296224794117734e-06}, {"id": 348, "seek": 202140, "start": 2044.96, "end": 2050.08, "text": " We could kind of say basic image recognition is kind of solved, but basic NLP is not really", "tokens": [492, 727, 733, 295, 584, 3875, 3256, 11150, 307, 733, 295, 13041, 11, 457, 3875, 426, 45196, 307, 406, 534], "temperature": 0.0, "avg_logprob": -0.21341619653216862, "compression_ratio": 1.6981132075471699, "no_speech_prob": 7.296224794117734e-06}, {"id": 349, "seek": 205008, "start": 2050.08, "end": 2052.2, "text": " solved.", "tokens": [13041, 13], "temperature": 0.0, "avg_logprob": -0.19955431896707285, "compression_ratio": 1.5677966101694916, "no_speech_prob": 2.9022933176747756e-06}, {"id": 350, "seek": 205008, "start": 2052.2, "end": 2057.44, "text": " But here's a great example Chris gave of something that this approach has worked really well", "tokens": [583, 510, 311, 257, 869, 1365, 6688, 2729, 295, 746, 300, 341, 3109, 575, 2732, 534, 731], "temperature": 0.0, "avg_logprob": -0.19955431896707285, "compression_ratio": 1.5677966101694916, "no_speech_prob": 2.9022933176747756e-06}, {"id": 351, "seek": 205008, "start": 2057.44, "end": 2065.2, "text": " for, a very difficult task. It is to read a story like this, and then in the story highlights,", "tokens": [337, 11, 257, 588, 2252, 5633, 13, 467, 307, 281, 1401, 257, 1657, 411, 341, 11, 293, 550, 294, 264, 1657, 14254, 11], "temperature": 0.0, "avg_logprob": -0.19955431896707285, "compression_ratio": 1.5677966101694916, "no_speech_prob": 2.9022933176747756e-06}, {"id": 352, "seek": 205008, "start": 2065.2, "end": 2071.16, "text": " one of the words or phrases is deleted, and the neural net has to figure it out. So if", "tokens": [472, 295, 264, 2283, 420, 20312, 307, 22981, 11, 293, 264, 18161, 2533, 575, 281, 2573, 309, 484, 13, 407, 498], "temperature": 0.0, "avg_logprob": -0.19955431896707285, "compression_ratio": 1.5677966101694916, "no_speech_prob": 2.9022933176747756e-06}, {"id": 353, "seek": 205008, "start": 2071.16, "end": 2076.36, "text": " Star Wars is deleted, it'll be characters in movies have gradually become more diverse,", "tokens": [5705, 9818, 307, 22981, 11, 309, 603, 312, 4342, 294, 6233, 362, 13145, 1813, 544, 9521, 11], "temperature": 0.0, "avg_logprob": -0.19955431896707285, "compression_ratio": 1.5677966101694916, "no_speech_prob": 2.9022933176747756e-06}, {"id": 354, "seek": 207636, "start": 2076.36, "end": 2082.4, "text": " and you have to predict Star Wars. So this is a challenging problem that really requires", "tokens": [293, 291, 362, 281, 6069, 5705, 9818, 13, 407, 341, 307, 257, 7595, 1154, 300, 534, 7029], "temperature": 0.0, "avg_logprob": -0.12746142052315376, "compression_ratio": 1.5233160621761659, "no_speech_prob": 8.530202649126295e-06}, {"id": 355, "seek": 207636, "start": 2082.4, "end": 2087.08, "text": " some in-depth understanding.", "tokens": [512, 294, 12, 25478, 3701, 13], "temperature": 0.0, "avg_logprob": -0.12746142052315376, "compression_ratio": 1.5233160621761659, "no_speech_prob": 8.530202649126295e-06}, {"id": 356, "seek": 207636, "start": 2087.08, "end": 2092.82, "text": " The work that Chris showed, and actually he was a senior researcher on, is to basically", "tokens": [440, 589, 300, 6688, 4712, 11, 293, 767, 415, 390, 257, 7965, 21751, 322, 11, 307, 281, 1936], "temperature": 0.0, "avg_logprob": -0.12746142052315376, "compression_ratio": 1.5233160621761659, "no_speech_prob": 8.530202649126295e-06}, {"id": 357, "seek": 207636, "start": 2092.82, "end": 2101.36, "text": " take the query, chuck it into an embedding, take the entire story, chuck it into an RNN,", "tokens": [747, 264, 14581, 11, 20870, 309, 666, 364, 12240, 3584, 11, 747, 264, 2302, 1657, 11, 20870, 309, 666, 364, 45702, 45, 11], "temperature": 0.0, "avg_logprob": -0.12746142052315376, "compression_ratio": 1.5233160621761659, "no_speech_prob": 8.530202649126295e-06}, {"id": 358, "seek": 210136, "start": 2101.36, "end": 2108.32, "text": " take the RNN that comes out, do the whole thing with attention, and that's it. So let's", "tokens": [747, 264, 45702, 45, 300, 1487, 484, 11, 360, 264, 1379, 551, 365, 3202, 11, 293, 300, 311, 309, 13, 407, 718, 311], "temperature": 0.0, "avg_logprob": -0.17197899591355098, "compression_ratio": 1.429530201342282, "no_speech_prob": 6.339149422274204e-06}, {"id": 359, "seek": 210136, "start": 2108.32, "end": 2114.7200000000003, "text": " talk about how to do this.", "tokens": [751, 466, 577, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.17197899591355098, "compression_ratio": 1.429530201342282, "no_speech_prob": 6.339149422274204e-06}, {"id": 360, "seek": 210136, "start": 2114.7200000000003, "end": 2126.48, "text": " So there's a really nice article on distilled.pub which has a bit of a picture of this. So imagine", "tokens": [407, 456, 311, 257, 534, 1481, 7222, 322, 1483, 6261, 13, 79, 836, 597, 575, 257, 857, 295, 257, 3036, 295, 341, 13, 407, 3811], "temperature": 0.0, "avg_logprob": -0.17197899591355098, "compression_ratio": 1.429530201342282, "no_speech_prob": 6.339149422274204e-06}, {"id": 361, "seek": 212648, "start": 2126.48, "end": 2132.8, "text": " we're translating English into French. So here's our English. And so we're trying to", "tokens": [321, 434, 35030, 3669, 666, 5522, 13, 407, 510, 311, 527, 3669, 13, 400, 370, 321, 434, 1382, 281], "temperature": 0.0, "avg_logprob": -0.15215217883770282, "compression_ratio": 1.5987654320987654, "no_speech_prob": 5.50754430150846e-06}, {"id": 362, "seek": 212648, "start": 2132.8, "end": 2146.64, "text": " translate the European economic area. So the is la. But the is la because of the gender", "tokens": [13799, 264, 6473, 4836, 1859, 13, 407, 264, 307, 635, 13, 583, 264, 307, 635, 570, 295, 264, 7898], "temperature": 0.0, "avg_logprob": -0.15215217883770282, "compression_ratio": 1.5987654320987654, "no_speech_prob": 5.50754430150846e-06}, {"id": 363, "seek": 212648, "start": 2146.64, "end": 2154.56, "text": " of the noun area. So you see these little purple lines here. This is showing that this", "tokens": [295, 264, 23307, 1859, 13, 407, 291, 536, 613, 707, 9656, 3876, 510, 13, 639, 307, 4099, 300, 341], "temperature": 0.0, "avg_logprob": -0.15215217883770282, "compression_ratio": 1.5987654320987654, "no_speech_prob": 5.50754430150846e-06}, {"id": 364, "seek": 215456, "start": 2154.56, "end": 2165.2, "text": " neural network model has learned that when it is looking at translating the area, specifically", "tokens": [18161, 3209, 2316, 575, 3264, 300, 562, 309, 307, 1237, 412, 35030, 264, 1859, 11, 4682], "temperature": 0.0, "avg_logprob": -0.17402577033409705, "compression_ratio": 1.6337209302325582, "no_speech_prob": 3.5008317809115397e-06}, {"id": 365, "seek": 215456, "start": 2165.2, "end": 2171.92, "text": " the, it also needs to look at the word area in order to figure out that it's la. So these", "tokens": [264, 11, 309, 611, 2203, 281, 574, 412, 264, 1349, 1859, 294, 1668, 281, 2573, 484, 300, 309, 311, 635, 13, 407, 613], "temperature": 0.0, "avg_logprob": -0.17402577033409705, "compression_ratio": 1.6337209302325582, "no_speech_prob": 3.5008317809115397e-06}, {"id": 366, "seek": 215456, "start": 2171.92, "end": 2178.32, "text": " purple lines are showing the weights in the neural network for translating that particular", "tokens": [9656, 3876, 366, 4099, 264, 17443, 294, 264, 18161, 3209, 337, 35030, 300, 1729], "temperature": 0.0, "avg_logprob": -0.17402577033409705, "compression_ratio": 1.6337209302325582, "no_speech_prob": 3.5008317809115397e-06}, {"id": 367, "seek": 215456, "start": 2178.32, "end": 2179.32, "text": " word.", "tokens": [1349, 13], "temperature": 0.0, "avg_logprob": -0.17402577033409705, "compression_ratio": 1.6337209302325582, "no_speech_prob": 3.5008317809115397e-06}, {"id": 368, "seek": 217932, "start": 2179.32, "end": 2184.6400000000003, "text": " So when it was translating signed, this is what I was talking about long-term, long-distance", "tokens": [407, 562, 309, 390, 35030, 8175, 11, 341, 307, 437, 286, 390, 1417, 466, 938, 12, 7039, 11, 938, 12, 67, 20829], "temperature": 0.0, "avg_logprob": -0.15462202436468575, "compression_ratio": 1.6376146788990826, "no_speech_prob": 4.63784135718015e-06}, {"id": 369, "seek": 217932, "start": 2184.6400000000003, "end": 2189.1600000000003, "text": " dependencies, it's not just using the word signed, but also looking at what was signed", "tokens": [36606, 11, 309, 311, 406, 445, 1228, 264, 1349, 8175, 11, 457, 611, 1237, 412, 437, 390, 8175], "temperature": 0.0, "avg_logprob": -0.15462202436468575, "compression_ratio": 1.6376146788990826, "no_speech_prob": 4.63784135718015e-06}, {"id": 370, "seek": 217932, "start": 2189.1600000000003, "end": 2196.32, "text": " in order to figure out the details of how to use this verb. And then some things, you", "tokens": [294, 1668, 281, 2573, 484, 264, 4365, 295, 577, 281, 764, 341, 9595, 13, 400, 550, 512, 721, 11, 291], "temperature": 0.0, "avg_logprob": -0.15462202436468575, "compression_ratio": 1.6376146788990826, "no_speech_prob": 4.63784135718015e-06}, {"id": 371, "seek": 217932, "start": 2196.32, "end": 2203.32, "text": " kind of need to look at combinations. So was signed, you have multiple prepositions working", "tokens": [733, 295, 643, 281, 574, 412, 21267, 13, 407, 390, 8175, 11, 291, 362, 3866, 2666, 329, 2451, 1364], "temperature": 0.0, "avg_logprob": -0.15462202436468575, "compression_ratio": 1.6376146788990826, "no_speech_prob": 4.63784135718015e-06}, {"id": 372, "seek": 220332, "start": 2203.32, "end": 2212.88, "text": " together. So we need to come up with an architecture which is capable of learning these attention", "tokens": [1214, 13, 407, 321, 643, 281, 808, 493, 365, 364, 9482, 597, 307, 8189, 295, 2539, 613, 3202], "temperature": 0.0, "avg_logprob": -0.18556093813768074, "compression_ratio": 1.4842105263157894, "no_speech_prob": 5.173863428353798e-06}, {"id": 373, "seek": 220332, "start": 2212.88, "end": 2216.1600000000003, "text": " weights.", "tokens": [17443, 13], "temperature": 0.0, "avg_logprob": -0.18556093813768074, "compression_ratio": 1.4842105263157894, "no_speech_prob": 5.173863428353798e-06}, {"id": 374, "seek": 220332, "start": 2216.1600000000003, "end": 2228.0800000000004, "text": " So as we mentioned last week, this really fun paper called Grammar as a Foreign Language,", "tokens": [407, 382, 321, 2835, 1036, 1243, 11, 341, 534, 1019, 3035, 1219, 22130, 6209, 382, 257, 20430, 24445, 11], "temperature": 0.0, "avg_logprob": -0.18556093813768074, "compression_ratio": 1.4842105263157894, "no_speech_prob": 5.173863428353798e-06}, {"id": 375, "seek": 220332, "start": 2228.0800000000004, "end": 2231.92, "text": " Jeffrey Hinton was the senior researcher on this one, has a nice little summary. This", "tokens": [28721, 389, 12442, 390, 264, 7965, 21751, 322, 341, 472, 11, 575, 257, 1481, 707, 12691, 13, 639], "temperature": 0.0, "avg_logprob": -0.18556093813768074, "compression_ratio": 1.4842105263157894, "no_speech_prob": 5.173863428353798e-06}, {"id": 376, "seek": 223192, "start": 2231.92, "end": 2238.64, "text": " is not where it originally came from, but a nice little summary of how attention works.", "tokens": [307, 406, 689, 309, 7993, 1361, 490, 11, 457, 257, 1481, 707, 12691, 295, 577, 3202, 1985, 13], "temperature": 0.0, "avg_logprob": -0.13052963488029712, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.9311149774002843e-05}, {"id": 377, "seek": 223192, "start": 2238.64, "end": 2247.44, "text": " So let's go through it. We don't often go through things by looking at the math, but", "tokens": [407, 718, 311, 352, 807, 309, 13, 492, 500, 380, 2049, 352, 807, 721, 538, 1237, 412, 264, 5221, 11, 457], "temperature": 0.0, "avg_logprob": -0.13052963488029712, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.9311149774002843e-05}, {"id": 378, "seek": 223192, "start": 2247.44, "end": 2259.98, "text": " I think in this case the math is simple enough that this is actually maybe a good practice.", "tokens": [286, 519, 294, 341, 1389, 264, 5221, 307, 2199, 1547, 300, 341, 307, 767, 1310, 257, 665, 3124, 13], "temperature": 0.0, "avg_logprob": -0.13052963488029712, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.9311149774002843e-05}, {"id": 379, "seek": 225998, "start": 2259.98, "end": 2267.16, "text": " So let's start off by looking at the notation. So there's an encoder. So remember the encoder", "tokens": [407, 718, 311, 722, 766, 538, 1237, 412, 264, 24657, 13, 407, 456, 311, 364, 2058, 19866, 13, 407, 1604, 264, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.16976584990819296, "compression_ratio": 1.641255605381166, "no_speech_prob": 8.267737939604558e-06}, {"id": 380, "seek": 225998, "start": 2267.16, "end": 2273.36, "text": " is the RNN, or number of layers of RNN, which are going to look through the original source", "tokens": [307, 264, 45702, 45, 11, 420, 1230, 295, 7914, 295, 45702, 45, 11, 597, 366, 516, 281, 574, 807, 264, 3380, 4009], "temperature": 0.0, "avg_logprob": -0.16976584990819296, "compression_ratio": 1.641255605381166, "no_speech_prob": 8.267737939604558e-06}, {"id": 381, "seek": 225998, "start": 2273.36, "end": 2280.88, "text": " sentence, so in this case the English sentence. And it's going to spit out a hidden state.", "tokens": [8174, 11, 370, 294, 341, 1389, 264, 3669, 8174, 13, 400, 309, 311, 516, 281, 22127, 484, 257, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.16976584990819296, "compression_ratio": 1.641255605381166, "no_speech_prob": 8.267737939604558e-06}, {"id": 382, "seek": 225998, "start": 2280.88, "end": 2286.3, "text": " And if in Keras we say return sequences equals true, we'll get a hidden state after every", "tokens": [400, 498, 294, 591, 6985, 321, 584, 2736, 22978, 6915, 2074, 11, 321, 603, 483, 257, 7633, 1785, 934, 633], "temperature": 0.0, "avg_logprob": -0.16976584990819296, "compression_ratio": 1.641255605381166, "no_speech_prob": 8.267737939604558e-06}, {"id": 383, "seek": 228630, "start": 2286.3, "end": 2294.6800000000003, "text": " single English word. So if you've got 10 words in the English sentence, we'll have 10 pieces", "tokens": [2167, 3669, 1349, 13, 407, 498, 291, 600, 658, 1266, 2283, 294, 264, 3669, 8174, 11, 321, 603, 362, 1266, 3755], "temperature": 0.0, "avg_logprob": -0.15153650803999466, "compression_ratio": 1.53125, "no_speech_prob": 4.157339844823582e-06}, {"id": 384, "seek": 228630, "start": 2294.6800000000003, "end": 2296.92, "text": " of encoder state.", "tokens": [295, 2058, 19866, 1785, 13], "temperature": 0.0, "avg_logprob": -0.15153650803999466, "compression_ratio": 1.53125, "no_speech_prob": 4.157339844823582e-06}, {"id": 385, "seek": 228630, "start": 2296.92, "end": 2306.6400000000003, "text": " So the encoder state, they're going to call it H. Remember these things that are underneath", "tokens": [407, 264, 2058, 19866, 1785, 11, 436, 434, 516, 281, 818, 309, 389, 13, 5459, 613, 721, 300, 366, 7223], "temperature": 0.0, "avg_logprob": -0.15153650803999466, "compression_ratio": 1.53125, "no_speech_prob": 4.157339844823582e-06}, {"id": 386, "seek": 228630, "start": 2306.6400000000003, "end": 2315.5, "text": " or on the top generally are just the same as in NumPy as putting things in square brackets.", "tokens": [420, 322, 264, 1192, 5101, 366, 445, 264, 912, 382, 294, 22592, 47, 88, 382, 3372, 721, 294, 3732, 26179, 13], "temperature": 0.0, "avg_logprob": -0.15153650803999466, "compression_ratio": 1.53125, "no_speech_prob": 4.157339844823582e-06}, {"id": 387, "seek": 231550, "start": 2315.5, "end": 2327.12, "text": " So what they're doing here is they're telling us that TA is the number of words in the English", "tokens": [407, 437, 436, 434, 884, 510, 307, 436, 434, 3585, 505, 300, 20094, 307, 264, 1230, 295, 2283, 294, 264, 3669], "temperature": 0.0, "avg_logprob": -0.14056206956694398, "compression_ratio": 1.563157894736842, "no_speech_prob": 3.187534048265661e-06}, {"id": 388, "seek": 231550, "start": 2327.12, "end": 2334.52, "text": " sentence. So we've got 1 through TA bits of state, each one's called H. So that's the", "tokens": [8174, 13, 407, 321, 600, 658, 502, 807, 20094, 9239, 295, 1785, 11, 1184, 472, 311, 1219, 389, 13, 407, 300, 311, 264], "temperature": 0.0, "avg_logprob": -0.14056206956694398, "compression_ratio": 1.563157894736842, "no_speech_prob": 3.187534048265661e-06}, {"id": 389, "seek": 231550, "start": 2334.52, "end": 2337.52, "text": " encoder hidden state.", "tokens": [2058, 19866, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.14056206956694398, "compression_ratio": 1.563157894736842, "no_speech_prob": 3.187534048265661e-06}, {"id": 390, "seek": 231550, "start": 2337.52, "end": 2343.24, "text": " And then the decoder, as you run through each step trying to translate this thing into French,", "tokens": [400, 550, 264, 979, 19866, 11, 382, 291, 1190, 807, 1184, 1823, 1382, 281, 13799, 341, 551, 666, 5522, 11], "temperature": 0.0, "avg_logprob": -0.14056206956694398, "compression_ratio": 1.563157894736842, "no_speech_prob": 3.187534048265661e-06}, {"id": 391, "seek": 234324, "start": 2343.24, "end": 2351.0, "text": " is creating its own hidden state. That's going to be called D. And D will go as far as TB,", "tokens": [307, 4084, 1080, 1065, 7633, 1785, 13, 663, 311, 516, 281, 312, 1219, 413, 13, 400, 413, 486, 352, 382, 1400, 382, 29711, 11], "temperature": 0.0, "avg_logprob": -0.19088937275445284, "compression_ratio": 1.4591194968553458, "no_speech_prob": 6.339151696010958e-06}, {"id": 392, "seek": 234324, "start": 2351.0, "end": 2357.8599999999997, "text": " that will be the highest index of the French sentence.", "tokens": [300, 486, 312, 264, 6343, 8186, 295, 264, 5522, 8174, 13], "temperature": 0.0, "avg_logprob": -0.19088937275445284, "compression_ratio": 1.4591194968553458, "no_speech_prob": 6.339151696010958e-06}, {"id": 393, "seek": 234324, "start": 2357.8599999999997, "end": 2369.08, "text": " So in the end, our goal is to replace every one of these, so we can write these as DT,", "tokens": [407, 294, 264, 917, 11, 527, 3387, 307, 281, 7406, 633, 472, 295, 613, 11, 370, 321, 393, 2464, 613, 382, 413, 51, 11], "temperature": 0.0, "avg_logprob": -0.19088937275445284, "compression_ratio": 1.4591194968553458, "no_speech_prob": 6.339151696010958e-06}, {"id": 394, "seek": 236908, "start": 2369.08, "end": 2377.2599999999998, "text": " meaning each one of these items of decoder state. We're going to try and create something", "tokens": [3620, 1184, 472, 295, 613, 4754, 295, 979, 19866, 1785, 13, 492, 434, 516, 281, 853, 293, 1884, 746], "temperature": 0.0, "avg_logprob": -0.10876928056989398, "compression_ratio": 1.6419753086419753, "no_speech_prob": 3.2377349725720705e-06}, {"id": 395, "seek": 236908, "start": 2377.2599999999998, "end": 2390.68, "text": " called D-T, which is going to represent the result of this attention process. So basically", "tokens": [1219, 413, 12, 51, 11, 597, 307, 516, 281, 2906, 264, 1874, 295, 341, 3202, 1399, 13, 407, 1936], "temperature": 0.0, "avg_logprob": -0.10876928056989398, "compression_ratio": 1.6419753086419753, "no_speech_prob": 3.2377349725720705e-06}, {"id": 396, "seek": 236908, "start": 2390.68, "end": 2397.54, "text": " it's going to represent the word, in this case the representation in the hidden state", "tokens": [309, 311, 516, 281, 2906, 264, 1349, 11, 294, 341, 1389, 264, 10290, 294, 264, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.10876928056989398, "compression_ratio": 1.6419753086419753, "no_speech_prob": 3.2377349725720705e-06}, {"id": 397, "seek": 239754, "start": 2397.54, "end": 2405.08, "text": " of the word for area, weighted quite a lot, and economic, weighted a little bit, and everything", "tokens": [295, 264, 1349, 337, 1859, 11, 32807, 1596, 257, 688, 11, 293, 4836, 11, 32807, 257, 707, 857, 11, 293, 1203], "temperature": 0.0, "avg_logprob": -0.14949287687029159, "compression_ratio": 1.4744525547445255, "no_speech_prob": 4.425460701895645e-06}, {"id": 398, "seek": 239754, "start": 2405.08, "end": 2407.4, "text": " else not much at all.", "tokens": [1646, 406, 709, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.14949287687029159, "compression_ratio": 1.4744525547445255, "no_speech_prob": 4.425460701895645e-06}, {"id": 399, "seek": 239754, "start": 2407.4, "end": 2421.8, "text": " So not surprisingly, the way to do that is with a weighted sum. So here, remember H,", "tokens": [407, 406, 17600, 11, 264, 636, 281, 360, 300, 307, 365, 257, 32807, 2408, 13, 407, 510, 11, 1604, 389, 11], "temperature": 0.0, "avg_logprob": -0.14949287687029159, "compression_ratio": 1.4744525547445255, "no_speech_prob": 4.425460701895645e-06}, {"id": 400, "seek": 242180, "start": 2421.8, "end": 2428.1600000000003, "text": " that's our encoder state. So that's the state from the RNN run over that English sentence.", "tokens": [300, 311, 527, 2058, 19866, 1785, 13, 407, 300, 311, 264, 1785, 490, 264, 45702, 45, 1190, 670, 300, 3669, 8174, 13], "temperature": 0.0, "avg_logprob": -0.2075794091385402, "compression_ratio": 1.711111111111111, "no_speech_prob": 1.3630438843392767e-05}, {"id": 401, "seek": 242180, "start": 2428.1600000000003, "end": 2430.4, "text": " So the word for area and so forth.", "tokens": [407, 264, 1349, 337, 1859, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.2075794091385402, "compression_ratio": 1.711111111111111, "no_speech_prob": 1.3630438843392767e-05}, {"id": 402, "seek": 242180, "start": 2430.4, "end": 2438.0, "text": " Question. To clarify, in this case, instead of creating one condensed representation vector", "tokens": [14464, 13, 1407, 17594, 11, 294, 341, 1389, 11, 2602, 295, 4084, 472, 36398, 10290, 8062], "temperature": 0.0, "avg_logprob": -0.2075794091385402, "compression_ratio": 1.711111111111111, "no_speech_prob": 1.3630438843392767e-05}, {"id": 403, "seek": 242180, "start": 2438.0, "end": 2443.0, "text": " that captures the entire English sentence, here are we just looking at the existing hidden", "tokens": [300, 27986, 264, 2302, 3669, 8174, 11, 510, 366, 321, 445, 1237, 412, 264, 6741, 7633], "temperature": 0.0, "avg_logprob": -0.2075794091385402, "compression_ratio": 1.711111111111111, "no_speech_prob": 1.3630438843392767e-05}, {"id": 404, "seek": 242180, "start": 2443.0, "end": 2447.0800000000004, "text": " states that get generated as the English sentence is processed word by word.", "tokens": [4368, 300, 483, 10833, 382, 264, 3669, 8174, 307, 18846, 1349, 538, 1349, 13], "temperature": 0.0, "avg_logprob": -0.2075794091385402, "compression_ratio": 1.711111111111111, "no_speech_prob": 1.3630438843392767e-05}, {"id": 405, "seek": 244708, "start": 2447.08, "end": 2451.96, "text": " It's just the difference between return sequences equals true versus return sequences equals", "tokens": [467, 311, 445, 264, 2649, 1296, 2736, 22978, 6915, 2074, 5717, 2736, 22978, 6915], "temperature": 0.0, "avg_logprob": -0.20790835789271764, "compression_ratio": 1.6735751295336787, "no_speech_prob": 1.2805332517018542e-05}, {"id": 406, "seek": 244708, "start": 2451.96, "end": 2461.6, "text": " false. So with true, it just throws away everything except the last state. Return sequences equals", "tokens": [7908, 13, 407, 365, 2074, 11, 309, 445, 19251, 1314, 1203, 3993, 264, 1036, 1785, 13, 24350, 22978, 6915], "temperature": 0.0, "avg_logprob": -0.20790835789271764, "compression_ratio": 1.6735751295336787, "no_speech_prob": 1.2805332517018542e-05}, {"id": 407, "seek": 244708, "start": 2461.6, "end": 2465.04, "text": " true is keep all the ones in the middle.", "tokens": [2074, 307, 1066, 439, 264, 2306, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.20790835789271764, "compression_ratio": 1.6735751295336787, "no_speech_prob": 1.2805332517018542e-05}, {"id": 408, "seek": 244708, "start": 2465.04, "end": 2475.2, "text": " And remember, importantly, this is a bi-LSTM, a bi-directional LSTM. So we've got one LSTM", "tokens": [400, 1604, 11, 8906, 11, 341, 307, 257, 3228, 12, 43, 6840, 44, 11, 257, 3228, 12, 18267, 41048, 441, 6840, 44, 13, 407, 321, 600, 658, 472, 441, 6840, 44], "temperature": 0.0, "avg_logprob": -0.20790835789271764, "compression_ratio": 1.6735751295336787, "no_speech_prob": 1.2805332517018542e-05}, {"id": 409, "seek": 247520, "start": 2475.2, "end": 2480.9199999999996, "text": " going forward through the sentence and another one going backward through the sentence. So", "tokens": [516, 2128, 807, 264, 8174, 293, 1071, 472, 516, 23897, 807, 264, 8174, 13, 407], "temperature": 0.0, "avg_logprob": -0.15198954870534498, "compression_ratio": 1.7464114832535884, "no_speech_prob": 1.8924809410236776e-05}, {"id": 410, "seek": 247520, "start": 2480.9199999999996, "end": 2488.1, "text": " every one of these pieces of encoded hidden state represents all of the words before it", "tokens": [633, 472, 295, 613, 3755, 295, 2058, 12340, 7633, 1785, 8855, 439, 295, 264, 2283, 949, 309], "temperature": 0.0, "avg_logprob": -0.15198954870534498, "compression_ratio": 1.7464114832535884, "no_speech_prob": 1.8924809410236776e-05}, {"id": 411, "seek": 247520, "start": 2488.1, "end": 2493.56, "text": " in that order and all of the words after it in reverse order, stuck together. And remember", "tokens": [294, 300, 1668, 293, 439, 295, 264, 2283, 934, 309, 294, 9943, 1668, 11, 5541, 1214, 13, 400, 1604], "temperature": 0.0, "avg_logprob": -0.15198954870534498, "compression_ratio": 1.7464114832535884, "no_speech_prob": 1.8924809410236776e-05}, {"id": 412, "seek": 247520, "start": 2493.56, "end": 2500.8399999999997, "text": " also it's been layered, so there's quite a few layers of non-linear neural net layers", "tokens": [611, 309, 311, 668, 34666, 11, 370, 456, 311, 1596, 257, 1326, 7914, 295, 2107, 12, 28263, 18161, 2533, 7914], "temperature": 0.0, "avg_logprob": -0.15198954870534498, "compression_ratio": 1.7464114832535884, "no_speech_prob": 1.8924809410236776e-05}, {"id": 413, "seek": 247520, "start": 2500.8399999999997, "end": 2501.8399999999997, "text": " going on.", "tokens": [516, 322, 13], "temperature": 0.0, "avg_logprob": -0.15198954870534498, "compression_ratio": 1.7464114832535884, "no_speech_prob": 1.8924809410236776e-05}, {"id": 414, "seek": 250184, "start": 2501.84, "end": 2511.6000000000004, "text": " So that's what this state comes from. So each element of H is already a pretty complex calculation", "tokens": [407, 300, 311, 437, 341, 1785, 1487, 490, 13, 407, 1184, 4478, 295, 389, 307, 1217, 257, 1238, 3997, 17108], "temperature": 0.0, "avg_logprob": -0.12691568291705588, "compression_ratio": 1.5722222222222222, "no_speech_prob": 5.255351425148547e-06}, {"id": 415, "seek": 250184, "start": 2511.6000000000004, "end": 2516.2000000000003, "text": " to get there, a pretty sophisticated calculation. So we're going to take those and we're going", "tokens": [281, 483, 456, 11, 257, 1238, 16950, 17108, 13, 407, 321, 434, 516, 281, 747, 729, 293, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.12691568291705588, "compression_ratio": 1.5722222222222222, "no_speech_prob": 5.255351425148547e-06}, {"id": 416, "seek": 250184, "start": 2516.2000000000003, "end": 2525.08, "text": " to multiply them by some weight. See how the weight's got two indexes on? That means that", "tokens": [281, 12972, 552, 538, 512, 3364, 13, 3008, 577, 264, 3364, 311, 658, 732, 8186, 279, 322, 30, 663, 1355, 300], "temperature": 0.0, "avg_logprob": -0.12691568291705588, "compression_ratio": 1.5722222222222222, "no_speech_prob": 5.255351425148547e-06}, {"id": 417, "seek": 252508, "start": 2525.08, "end": 2531.88, "text": " the weight depends on two things. One is T, which is which word in the French translation", "tokens": [264, 3364, 5946, 322, 732, 721, 13, 1485, 307, 314, 11, 597, 307, 597, 1349, 294, 264, 5522, 12853], "temperature": 0.0, "avg_logprob": -0.13321276346842448, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.0904540178889874e-06}, {"id": 418, "seek": 252508, "start": 2531.88, "end": 2540.06, "text": " am I trying to create right now, and I, which is which piece of encoded hidden state am", "tokens": [669, 286, 1382, 281, 1884, 558, 586, 11, 293, 286, 11, 597, 307, 597, 2522, 295, 2058, 12340, 7633, 1785, 669], "temperature": 0.0, "avg_logprob": -0.13321276346842448, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.0904540178889874e-06}, {"id": 419, "seek": 252508, "start": 2540.06, "end": 2543.72, "text": " I currently calculating the weight for.", "tokens": [286, 4362, 28258, 264, 3364, 337, 13], "temperature": 0.0, "avg_logprob": -0.13321276346842448, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.0904540178889874e-06}, {"id": 420, "seek": 252508, "start": 2543.72, "end": 2550.7599999999998, "text": " And because this is being calculated with a function, if we were doing this in Python,", "tokens": [400, 570, 341, 307, 885, 15598, 365, 257, 2445, 11, 498, 321, 645, 884, 341, 294, 15329, 11], "temperature": 0.0, "avg_logprob": -0.13321276346842448, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.0904540178889874e-06}, {"id": 421, "seek": 255076, "start": 2550.76, "end": 2566.1200000000003, "text": " we'd probably be writing like get weight T, I. And so the line above it tells you how", "tokens": [321, 1116, 1391, 312, 3579, 411, 483, 3364, 314, 11, 286, 13, 400, 370, 264, 1622, 3673, 309, 5112, 291, 577], "temperature": 0.0, "avg_logprob": -0.1425731907720151, "compression_ratio": 1.685897435897436, "no_speech_prob": 4.4254543354327325e-06}, {"id": 422, "seek": 255076, "start": 2566.1200000000003, "end": 2572.5200000000004, "text": " to go about calculating that weight. And it tells you, not surprisingly, that to calculate", "tokens": [281, 352, 466, 28258, 300, 3364, 13, 400, 309, 5112, 291, 11, 406, 17600, 11, 300, 281, 8873], "temperature": 0.0, "avg_logprob": -0.1425731907720151, "compression_ratio": 1.685897435897436, "no_speech_prob": 4.4254543354327325e-06}, {"id": 423, "seek": 255076, "start": 2572.5200000000004, "end": 2577.76, "text": " that weight we're going to be using softmax. We're going to be using softmax on top of", "tokens": [300, 3364, 321, 434, 516, 281, 312, 1228, 2787, 41167, 13, 492, 434, 516, 281, 312, 1228, 2787, 41167, 322, 1192, 295], "temperature": 0.0, "avg_logprob": -0.1425731907720151, "compression_ratio": 1.685897435897436, "no_speech_prob": 4.4254543354327325e-06}, {"id": 424, "seek": 257776, "start": 2577.76, "end": 2584.7200000000003, "text": " some other function. So why softmax? Well, if we're doing a weighted sum, we want the", "tokens": [512, 661, 2445, 13, 407, 983, 2787, 41167, 30, 1042, 11, 498, 321, 434, 884, 257, 32807, 2408, 11, 321, 528, 264], "temperature": 0.0, "avg_logprob": -0.14514476970090703, "compression_ratio": 1.4025974025974026, "no_speech_prob": 1.6797279158708989e-06}, {"id": 425, "seek": 257776, "start": 2584.7200000000003, "end": 2588.84, "text": " weights to add up to 1. That's one reason.", "tokens": [17443, 281, 909, 493, 281, 502, 13, 663, 311, 472, 1778, 13], "temperature": 0.0, "avg_logprob": -0.14514476970090703, "compression_ratio": 1.4025974025974026, "no_speech_prob": 1.6797279158708989e-06}, {"id": 426, "seek": 257776, "start": 2588.84, "end": 2597.8, "text": " Secondly, most of the time in translation, the thing that you're translating is largely", "tokens": [19483, 11, 881, 295, 264, 565, 294, 12853, 11, 264, 551, 300, 291, 434, 35030, 307, 11611], "temperature": 0.0, "avg_logprob": -0.14514476970090703, "compression_ratio": 1.4025974025974026, "no_speech_prob": 1.6797279158708989e-06}, {"id": 427, "seek": 259780, "start": 2597.8, "end": 2610.04, "text": " just one word. 1992 is translated as 1992. That's just August. But then sometimes it's", "tokens": [445, 472, 1349, 13, 23952, 307, 16805, 382, 23952, 13, 663, 311, 445, 6897, 13, 583, 550, 2171, 309, 311], "temperature": 0.0, "avg_logprob": -0.1534541266305106, "compression_ratio": 1.511764705882353, "no_speech_prob": 2.5612712306610774e-06}, {"id": 428, "seek": 259780, "start": 2610.04, "end": 2615.84, "text": " mainly one word, like in, but a little bit of some others. So a softmax, because it's", "tokens": [8704, 472, 1349, 11, 411, 294, 11, 457, 257, 707, 857, 295, 512, 2357, 13, 407, 257, 2787, 41167, 11, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.1534541266305106, "compression_ratio": 1.511764705882353, "no_speech_prob": 2.5612712306610774e-06}, {"id": 429, "seek": 259780, "start": 2615.84, "end": 2622.2400000000002, "text": " e to the something divided by the sum of e to the something, remember it tends to be", "tokens": [308, 281, 264, 746, 6666, 538, 264, 2408, 295, 308, 281, 264, 746, 11, 1604, 309, 12258, 281, 312], "temperature": 0.0, "avg_logprob": -0.1534541266305106, "compression_ratio": 1.511764705882353, "no_speech_prob": 2.5612712306610774e-06}, {"id": 430, "seek": 262224, "start": 2622.24, "end": 2628.56, "text": " very big for just one item in the vector and fairly small for everything else. But by using", "tokens": [588, 955, 337, 445, 472, 3174, 294, 264, 8062, 293, 6457, 1359, 337, 1203, 1646, 13, 583, 538, 1228], "temperature": 0.0, "avg_logprob": -0.1704543073412398, "compression_ratio": 1.5593220338983051, "no_speech_prob": 3.089485062446329e-06}, {"id": 431, "seek": 262224, "start": 2628.56, "end": 2638.4399999999996, "text": " softmax, we capture that quite naturally. So that's why we use softmax to calculate", "tokens": [2787, 41167, 11, 321, 7983, 300, 1596, 8195, 13, 407, 300, 311, 983, 321, 764, 2787, 41167, 281, 8873], "temperature": 0.0, "avg_logprob": -0.1704543073412398, "compression_ratio": 1.5593220338983051, "no_speech_prob": 3.089485062446329e-06}, {"id": 432, "seek": 262224, "start": 2638.4399999999996, "end": 2641.3599999999997, "text": " these weights.", "tokens": [613, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1704543073412398, "compression_ratio": 1.5593220338983051, "no_speech_prob": 3.089485062446329e-06}, {"id": 433, "seek": 262224, "start": 2641.3599999999997, "end": 2649.0, "text": " So softmax of u, u is another function or the result of another function. And what is", "tokens": [407, 2787, 41167, 295, 344, 11, 344, 307, 1071, 2445, 420, 264, 1874, 295, 1071, 2445, 13, 400, 437, 307], "temperature": 0.0, "avg_logprob": -0.1704543073412398, "compression_ratio": 1.5593220338983051, "no_speech_prob": 3.089485062446329e-06}, {"id": 434, "seek": 264900, "start": 2649.0, "end": 2656.2, "text": " this? This is just a multilayer perceptron with one hidden layer, a neural net with one", "tokens": [341, 30, 639, 307, 445, 257, 2120, 388, 11167, 43276, 2044, 365, 472, 7633, 4583, 11, 257, 18161, 2533, 365, 472], "temperature": 0.0, "avg_logprob": -0.13355138408603953, "compression_ratio": 1.638036809815951, "no_speech_prob": 1.8631573766469955e-05}, {"id": 435, "seek": 264900, "start": 2656.2, "end": 2667.4, "text": " hidden layer. If you have a look, it's a couple of bits of data multiplied by weight matrices", "tokens": [7633, 4583, 13, 759, 291, 362, 257, 574, 11, 309, 311, 257, 1916, 295, 9239, 295, 1412, 17207, 538, 3364, 32284], "temperature": 0.0, "avg_logprob": -0.13355138408603953, "compression_ratio": 1.638036809815951, "no_speech_prob": 1.8631573766469955e-05}, {"id": 436, "seek": 264900, "start": 2667.4, "end": 2674.52, "text": " put through a non-linearity, multiplied by another weight matrix, put through another", "tokens": [829, 807, 257, 2107, 12, 1889, 17409, 11, 17207, 538, 1071, 3364, 8141, 11, 829, 807, 1071], "temperature": 0.0, "avg_logprob": -0.13355138408603953, "compression_ratio": 1.638036809815951, "no_speech_prob": 1.8631573766469955e-05}, {"id": 437, "seek": 267452, "start": 2674.52, "end": 2682.32, "text": " non-linearity. So that's the definition of a neural net with one hidden layer.", "tokens": [2107, 12, 1889, 17409, 13, 407, 300, 311, 264, 7123, 295, 257, 18161, 2533, 365, 472, 7633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.10443749356625685, "compression_ratio": 1.5696969696969696, "no_speech_prob": 4.2228134589095134e-06}, {"id": 438, "seek": 267452, "start": 2682.32, "end": 2690.48, "text": " So when it all comes down to it, what this all says is, in order to calculate the weights", "tokens": [407, 562, 309, 439, 1487, 760, 281, 309, 11, 437, 341, 439, 1619, 307, 11, 294, 1668, 281, 8873, 264, 17443], "temperature": 0.0, "avg_logprob": -0.10443749356625685, "compression_ratio": 1.5696969696969696, "no_speech_prob": 4.2228134589095134e-06}, {"id": 439, "seek": 267452, "start": 2690.48, "end": 2700.64, "text": " on each of the source encoding words, or each of the target translated words, train a tiny", "tokens": [322, 1184, 295, 264, 4009, 43430, 2283, 11, 420, 1184, 295, 264, 3779, 16805, 2283, 11, 3847, 257, 5870], "temperature": 0.0, "avg_logprob": -0.10443749356625685, "compression_ratio": 1.5696969696969696, "no_speech_prob": 4.2228134589095134e-06}, {"id": 440, "seek": 270064, "start": 2700.64, "end": 2707.3199999999997, "text": " little neural net with one hidden layer which learns to figure out which source words to", "tokens": [707, 18161, 2533, 365, 472, 7633, 4583, 597, 27152, 281, 2573, 484, 597, 4009, 2283, 281], "temperature": 0.0, "avg_logprob": -0.18491619235866671, "compression_ratio": 1.6343612334801763, "no_speech_prob": 5.594314643531106e-06}, {"id": 441, "seek": 270064, "start": 2707.3199999999997, "end": 2711.4, "text": " translate now.", "tokens": [13799, 586, 13], "temperature": 0.0, "avg_logprob": -0.18491619235866671, "compression_ratio": 1.6343612334801763, "no_speech_prob": 5.594314643531106e-06}, {"id": 442, "seek": 270064, "start": 2711.4, "end": 2715.16, "text": " And so remember one of the things that Chris mentioned in his like, why is neural machine", "tokens": [400, 370, 1604, 472, 295, 264, 721, 300, 6688, 2835, 294, 702, 411, 11, 983, 307, 18161, 3479], "temperature": 0.0, "avg_logprob": -0.18491619235866671, "compression_ratio": 1.6343612334801763, "no_speech_prob": 5.594314643531106e-06}, {"id": 443, "seek": 270064, "start": 2715.16, "end": 2720.56, "text": " translation good, one of his reasons was because it's end-to-end trainable, we're going to", "tokens": [12853, 665, 11, 472, 295, 702, 4112, 390, 570, 309, 311, 917, 12, 1353, 12, 521, 3847, 712, 11, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.18491619235866671, "compression_ratio": 1.6343612334801763, "no_speech_prob": 5.594314643531106e-06}, {"id": 444, "seek": 270064, "start": 2720.56, "end": 2726.3199999999997, "text": " embed this mini neural net inside the bigger RNN so that the whole thing's going to be", "tokens": [12240, 341, 8382, 18161, 2533, 1854, 264, 3801, 45702, 45, 370, 300, 264, 1379, 551, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.18491619235866671, "compression_ratio": 1.6343612334801763, "no_speech_prob": 5.594314643531106e-06}, {"id": 445, "seek": 272632, "start": 2726.32, "end": 2734.1600000000003, "text": " just SGD'd all in one go. So that's what we're going to try and do.", "tokens": [445, 34520, 35, 1116, 439, 294, 472, 352, 13, 407, 300, 311, 437, 321, 434, 516, 281, 853, 293, 360, 13], "temperature": 0.0, "avg_logprob": -0.23062826239544412, "compression_ratio": 1.4509803921568627, "no_speech_prob": 8.939569852373097e-06}, {"id": 446, "seek": 272632, "start": 2734.1600000000003, "end": 2745.2000000000003, "text": " So I just want to make sure that's clear before we look at the code for how to do it.", "tokens": [407, 286, 445, 528, 281, 652, 988, 300, 311, 1850, 949, 321, 574, 412, 264, 3089, 337, 577, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.23062826239544412, "compression_ratio": 1.4509803921568627, "no_speech_prob": 8.939569852373097e-06}, {"id": 447, "seek": 272632, "start": 2745.2000000000003, "end": 2750.2000000000003, "text": " So how's a hidden state looks like? Is it like a set of activations?", "tokens": [407, 577, 311, 257, 7633, 1785, 1542, 411, 30, 1119, 309, 411, 257, 992, 295, 2430, 763, 30], "temperature": 0.0, "avg_logprob": -0.23062826239544412, "compression_ratio": 1.4509803921568627, "no_speech_prob": 8.939569852373097e-06}, {"id": 448, "seek": 275020, "start": 2750.2, "end": 2769.2799999999997, "text": " So the hidden state is just a normal LSTM hidden state, so it's just a vector.", "tokens": [407, 264, 7633, 1785, 307, 445, 257, 2710, 441, 6840, 44, 7633, 1785, 11, 370, 309, 311, 445, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.10898510953213306, "compression_ratio": 1.3170731707317074, "no_speech_prob": 6.240861239348305e-06}, {"id": 449, "seek": 275020, "start": 2769.2799999999997, "end": 2776.2, "text": " So anytime you want to remind yourself of what's really going on with RNNs, go back", "tokens": [407, 13038, 291, 528, 281, 4160, 1803, 295, 437, 311, 534, 516, 322, 365, 45702, 45, 82, 11, 352, 646], "temperature": 0.0, "avg_logprob": -0.10898510953213306, "compression_ratio": 1.3170731707317074, "no_speech_prob": 6.240861239348305e-06}, {"id": 450, "seek": 277620, "start": 2776.2, "end": 2782.8399999999997, "text": " to this Lesson 5 RNN PowerPoint. We go through it and we remember, this is not an RNN, it", "tokens": [281, 341, 18649, 266, 1025, 45702, 45, 25584, 13, 492, 352, 807, 309, 293, 321, 1604, 11, 341, 307, 406, 364, 45702, 45, 11, 309], "temperature": 0.0, "avg_logprob": -0.22345065162295386, "compression_ratio": 1.879396984924623, "no_speech_prob": 2.1112213289598003e-05}, {"id": 451, "seek": 277620, "start": 2782.8399999999997, "end": 2788.7999999999997, "text": " is a basic neural net, and we could have a multi-layer neural net, and then we could", "tokens": [307, 257, 3875, 18161, 2533, 11, 293, 321, 727, 362, 257, 4825, 12, 8376, 260, 18161, 2533, 11, 293, 550, 321, 727], "temperature": 0.0, "avg_logprob": -0.22345065162295386, "compression_ratio": 1.879396984924623, "no_speech_prob": 2.1112213289598003e-05}, {"id": 452, "seek": 277620, "start": 2788.7999999999997, "end": 2793.7999999999997, "text": " have a multi-layer neural net in which we have a second input coming in, still not an", "tokens": [362, 257, 4825, 12, 8376, 260, 18161, 2533, 294, 597, 321, 362, 257, 1150, 4846, 1348, 294, 11, 920, 406, 364], "temperature": 0.0, "avg_logprob": -0.22345065162295386, "compression_ratio": 1.879396984924623, "no_speech_prob": 2.1112213289598003e-05}, {"id": 453, "seek": 277620, "start": 2793.7999999999997, "end": 2797.8799999999997, "text": " RNN. We could have two things coming in at two different times, and then we could also", "tokens": [45702, 45, 13, 492, 727, 362, 732, 721, 1348, 294, 412, 732, 819, 1413, 11, 293, 550, 321, 727, 611], "temperature": 0.0, "avg_logprob": -0.22345065162295386, "compression_ratio": 1.879396984924623, "no_speech_prob": 2.1112213289598003e-05}, {"id": 454, "seek": 277620, "start": 2797.8799999999997, "end": 2799.8799999999997, "text": " tie these weight matrices.", "tokens": [7582, 613, 3364, 32284, 13], "temperature": 0.0, "avg_logprob": -0.22345065162295386, "compression_ratio": 1.879396984924623, "no_speech_prob": 2.1112213289598003e-05}, {"id": 455, "seek": 279988, "start": 2799.88, "end": 2806.44, "text": " And the notebook that goes with this class, we do all this by hand in Keras with all the", "tokens": [400, 264, 21060, 300, 1709, 365, 341, 1508, 11, 321, 360, 439, 341, 538, 1011, 294, 591, 6985, 365, 439, 264], "temperature": 0.0, "avg_logprob": -0.1277451109378896, "compression_ratio": 1.712962962962963, "no_speech_prob": 9.721508149596048e-07}, {"id": 456, "seek": 279988, "start": 2806.44, "end": 2811.76, "text": " weight tying and everything, so you see every step. And then we realize, oh, that last picture", "tokens": [3364, 32405, 293, 1203, 11, 370, 291, 536, 633, 1823, 13, 400, 550, 321, 4325, 11, 1954, 11, 300, 1036, 3036], "temperature": 0.0, "avg_logprob": -0.1277451109378896, "compression_ratio": 1.712962962962963, "no_speech_prob": 9.721508149596048e-07}, {"id": 457, "seek": 279988, "start": 2811.76, "end": 2822.28, "text": " could have been drawn like that. And we also realize we could do this as well. So all of", "tokens": [727, 362, 668, 10117, 411, 300, 13, 400, 321, 611, 4325, 321, 727, 360, 341, 382, 731, 13, 407, 439, 295], "temperature": 0.0, "avg_logprob": -0.1277451109378896, "compression_ratio": 1.712962962962963, "no_speech_prob": 9.721508149596048e-07}, {"id": 458, "seek": 279988, "start": 2822.28, "end": 2829.0, "text": " these circles represent just a bunch of activations, just a vector of activations. And so we have", "tokens": [613, 13040, 2906, 445, 257, 3840, 295, 2430, 763, 11, 445, 257, 8062, 295, 2430, 763, 13, 400, 370, 321, 362], "temperature": 0.0, "avg_logprob": -0.1277451109378896, "compression_ratio": 1.712962962962963, "no_speech_prob": 9.721508149596048e-07}, {"id": 459, "seek": 282900, "start": 2829.0, "end": 2836.64, "text": " a bunch of these layers until eventually we decide to keep some final set of activations.", "tokens": [257, 3840, 295, 613, 7914, 1826, 4728, 321, 4536, 281, 1066, 512, 2572, 992, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.2582401775178455, "compression_ratio": 1.5047169811320755, "no_speech_prob": 1.0451473826833535e-05}, {"id": 460, "seek": 282900, "start": 2836.64, "end": 2839.76, "text": " So this is all we're doing.", "tokens": [407, 341, 307, 439, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.2582401775178455, "compression_ratio": 1.5047169811320755, "no_speech_prob": 1.0451473826833535e-05}, {"id": 461, "seek": 282900, "start": 2839.76, "end": 2844.92, "text": " Questioner. I don't know what to call it, the attentional model there. It's relatively", "tokens": [14464, 260, 13, 286, 500, 380, 458, 437, 281, 818, 309, 11, 264, 3202, 304, 2316, 456, 13, 467, 311, 7226], "temperature": 0.0, "avg_logprob": -0.2582401775178455, "compression_ratio": 1.5047169811320755, "no_speech_prob": 1.0451473826833535e-05}, {"id": 462, "seek": 282900, "start": 2844.92, "end": 2849.72, "text": " simple, it's just a single hidden layer. Is there a reason that it wasn't more complex", "tokens": [2199, 11, 309, 311, 445, 257, 2167, 7633, 4583, 13, 1119, 456, 257, 1778, 300, 309, 2067, 380, 544, 3997], "temperature": 0.0, "avg_logprob": -0.2582401775178455, "compression_ratio": 1.5047169811320755, "no_speech_prob": 1.0451473826833535e-05}, {"id": 463, "seek": 282900, "start": 2849.72, "end": 2851.04, "text": " or some fancy architecture?", "tokens": [420, 512, 10247, 9482, 30], "temperature": 0.0, "avg_logprob": -0.2582401775178455, "compression_ratio": 1.5047169811320755, "no_speech_prob": 1.0451473826833535e-05}, {"id": 464, "seek": 285104, "start": 2851.04, "end": 2860.6, "text": " Answer. I've been wondering that myself. I guess the answer is it seems to work. There's", "tokens": [24545, 13, 286, 600, 668, 6359, 300, 2059, 13, 286, 2041, 264, 1867, 307, 309, 2544, 281, 589, 13, 821, 311], "temperature": 0.0, "avg_logprob": -0.16744530478189157, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.8342276234761812e-05}, {"id": 465, "seek": 285104, "start": 2860.6, "end": 2871.08, "text": " no reason you couldn't have two hidden layers. I guess it's so easy to go back and look and", "tokens": [572, 1778, 291, 2809, 380, 362, 732, 7633, 7914, 13, 286, 2041, 309, 311, 370, 1858, 281, 352, 646, 293, 574, 293], "temperature": 0.0, "avg_logprob": -0.16744530478189157, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.8342276234761812e-05}, {"id": 466, "seek": 285104, "start": 2871.08, "end": 2875.84, "text": " see what the attentions are. Presumably so far the way people are using this, they're", "tokens": [536, 437, 264, 30980, 626, 366, 13, 2718, 449, 1188, 370, 1400, 264, 636, 561, 366, 1228, 341, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.16744530478189157, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.8342276234761812e-05}, {"id": 467, "seek": 285104, "start": 2875.84, "end": 2880.08, "text": " finding that the attentional model is getting the correct stuff.", "tokens": [5006, 300, 264, 3202, 304, 2316, 307, 1242, 264, 3006, 1507, 13], "temperature": 0.0, "avg_logprob": -0.16744530478189157, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.8342276234761812e-05}, {"id": 468, "seek": 288008, "start": 2880.08, "end": 2886.04, "text": " I think if you try and do an attentional model for something else and that doesn't happen,", "tokens": [286, 519, 498, 291, 853, 293, 360, 364, 3202, 304, 2316, 337, 746, 1646, 293, 300, 1177, 380, 1051, 11], "temperature": 0.0, "avg_logprob": -0.16101674472584443, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.240890343178762e-06}, {"id": 469, "seek": 288008, "start": 2886.04, "end": 2893.64, "text": " I'd say yeah, check another layer. In fact, this terrific post actually shows a really", "tokens": [286, 1116, 584, 1338, 11, 1520, 1071, 4583, 13, 682, 1186, 11, 341, 20899, 2183, 767, 3110, 257, 534], "temperature": 0.0, "avg_logprob": -0.16101674472584443, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.240890343178762e-06}, {"id": 470, "seek": 288008, "start": 2893.64, "end": 2900.24, "text": " cool example of this, which is on the bottom here is a sound wave, and on the top is the", "tokens": [1627, 1365, 295, 341, 11, 597, 307, 322, 264, 2767, 510, 307, 257, 1626, 5772, 11, 293, 322, 264, 1192, 307, 264], "temperature": 0.0, "avg_logprob": -0.16101674472584443, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.240890343178762e-06}, {"id": 471, "seek": 288008, "start": 2900.24, "end": 2906.2799999999997, "text": " speech recognition. And so people are actually using attention models to automatically do", "tokens": [6218, 11150, 13, 400, 370, 561, 366, 767, 1228, 3202, 5245, 281, 6772, 360], "temperature": 0.0, "avg_logprob": -0.16101674472584443, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.240890343178762e-06}, {"id": 472, "seek": 290628, "start": 2906.28, "end": 2914.1600000000003, "text": " speech recognition by figuring out which parts of the sound wave represent which letters.", "tokens": [6218, 11150, 538, 15213, 484, 597, 3166, 295, 264, 1626, 5772, 2906, 597, 7825, 13], "temperature": 0.0, "avg_logprob": -0.15957228342692056, "compression_ratio": 1.5032258064516129, "no_speech_prob": 1.1300679034320638e-05}, {"id": 473, "seek": 290628, "start": 2914.1600000000003, "end": 2924.28, "text": " And in fact, one of the super cool things to come out this week was the Tachotron, which", "tokens": [400, 294, 1186, 11, 472, 295, 264, 1687, 1627, 721, 281, 808, 484, 341, 1243, 390, 264, 314, 608, 310, 2044, 11, 597], "temperature": 0.0, "avg_logprob": -0.15957228342692056, "compression_ratio": 1.5032258064516129, "no_speech_prob": 1.1300679034320638e-05}, {"id": 474, "seek": 290628, "start": 2924.28, "end": 2928.7200000000003, "text": " is one of the best names for a paper I've come across.", "tokens": [307, 472, 295, 264, 1151, 5288, 337, 257, 3035, 286, 600, 808, 2108, 13], "temperature": 0.0, "avg_logprob": -0.15957228342692056, "compression_ratio": 1.5032258064516129, "no_speech_prob": 1.1300679034320638e-05}, {"id": 475, "seek": 292872, "start": 2928.72, "end": 2939.2799999999997, "text": " And the Tachotron has some fantastic examples actually of what it sounds like. Let me see", "tokens": [400, 264, 314, 608, 310, 2044, 575, 512, 5456, 5110, 767, 295, 437, 309, 3263, 411, 13, 961, 385, 536], "temperature": 0.0, "avg_logprob": -0.29075679494373835, "compression_ratio": 1.4269662921348314, "no_speech_prob": 5.5075711316021625e-06}, {"id": 476, "seek": 292872, "start": 2939.2799999999997, "end": 2942.2799999999997, "text": " if I can get this.", "tokens": [498, 286, 393, 483, 341, 13], "temperature": 0.0, "avg_logprob": -0.29075679494373835, "compression_ratio": 1.4269662921348314, "no_speech_prob": 5.5075711316021625e-06}, {"id": 477, "seek": 292872, "start": 2942.2799999999997, "end": 2955.8399999999997, "text": " So the cool thing here is it changes depending on where the punctuation is pretty impressively,", "tokens": [407, 264, 1627, 551, 510, 307, 309, 2962, 5413, 322, 689, 264, 27006, 16073, 307, 1238, 6729, 3413, 11], "temperature": 0.0, "avg_logprob": -0.29075679494373835, "compression_ratio": 1.4269662921348314, "no_speech_prob": 5.5075711316021625e-06}, {"id": 478, "seek": 292872, "start": 2955.8399999999997, "end": 2958.3999999999996, "text": " or even whether there's capitals. Check this out.", "tokens": [420, 754, 1968, 456, 311, 1410, 11118, 13, 6881, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.29075679494373835, "compression_ratio": 1.4269662921348314, "no_speech_prob": 5.5075711316021625e-06}, {"id": 479, "seek": 295840, "start": 2958.4, "end": 2971.32, "text": " The buses aren't the problem, they actually provide a solution.", "tokens": [440, 20519, 3212, 380, 264, 1154, 11, 436, 767, 2893, 257, 3827, 13], "temperature": 0.0, "avg_logprob": -0.13556494536223235, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.400036414153874e-05}, {"id": 480, "seek": 295840, "start": 2971.32, "end": 2975.0, "text": " Does the quick brown fox jump over the lazy dog?", "tokens": [4402, 264, 1702, 6292, 21026, 3012, 670, 264, 14847, 3000, 30], "temperature": 0.0, "avg_logprob": -0.13556494536223235, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.400036414153874e-05}, {"id": 481, "seek": 295840, "start": 2975.0, "end": 2986.56, "text": " So with this kind of end-to-end training like this, you don't have to really build anything", "tokens": [407, 365, 341, 733, 295, 917, 12, 1353, 12, 521, 3097, 411, 341, 11, 291, 500, 380, 362, 281, 534, 1322, 1340], "temperature": 0.0, "avg_logprob": -0.13556494536223235, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.400036414153874e-05}, {"id": 482, "seek": 298656, "start": 2986.56, "end": 2992.96, "text": " special to make that happen. You just need to make sure that your labeled data is correct.", "tokens": [2121, 281, 652, 300, 1051, 13, 509, 445, 643, 281, 652, 988, 300, 428, 21335, 1412, 307, 3006, 13], "temperature": 0.0, "avg_logprob": -0.18391620091029576, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.520658679510234e-05}, {"id": 483, "seek": 298656, "start": 2992.96, "end": 2996.08, "text": " And actually somebody pointed out something really neat the other day, which is if you", "tokens": [400, 767, 2618, 10932, 484, 746, 534, 10654, 264, 661, 786, 11, 597, 307, 498, 291], "temperature": 0.0, "avg_logprob": -0.18391620091029576, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.520658679510234e-05}, {"id": 484, "seek": 298656, "start": 2996.08, "end": 3003.18, "text": " want to build a speech recognition system, one easy way to do it would be grab some audible.com", "tokens": [528, 281, 1322, 257, 6218, 11150, 1185, 11, 472, 1858, 636, 281, 360, 309, 576, 312, 4444, 512, 41317, 13, 1112], "temperature": 0.0, "avg_logprob": -0.18391620091029576, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.520658679510234e-05}, {"id": 485, "seek": 298656, "start": 3003.18, "end": 3009.68, "text": " audiobooks and the actual books. You could have 40 hours of training data like that.", "tokens": [40031, 82, 293, 264, 3539, 3642, 13, 509, 727, 362, 3356, 2496, 295, 3097, 1412, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.18391620091029576, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.520658679510234e-05}, {"id": 486, "seek": 298656, "start": 3009.68, "end": 3014.24, "text": " In fact, if you grab Stephen Fry reading Harry Potter, then you could have every Harry Potter", "tokens": [682, 1186, 11, 498, 291, 4444, 13391, 31822, 3760, 9378, 18115, 11, 550, 291, 727, 362, 633, 9378, 18115], "temperature": 0.0, "avg_logprob": -0.18391620091029576, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.520658679510234e-05}, {"id": 487, "seek": 301424, "start": 3014.24, "end": 3017.56, "text": " voice as well.", "tokens": [3177, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.27687305874294704, "compression_ratio": 1.4787878787878788, "no_speech_prob": 8.530192644684575e-06}, {"id": 488, "seek": 301424, "start": 3017.56, "end": 3025.6, "text": " So this is a super amazing technique and surprisingly enough, this single hidden layer seems to", "tokens": [407, 341, 307, 257, 1687, 2243, 6532, 293, 17600, 1547, 11, 341, 2167, 7633, 4583, 2544, 281], "temperature": 0.0, "avg_logprob": -0.27687305874294704, "compression_ratio": 1.4787878787878788, "no_speech_prob": 8.530192644684575e-06}, {"id": 489, "seek": 301424, "start": 3025.6, "end": 3030.72, "text": " be enough to do attention pretty well.", "tokens": [312, 1547, 281, 360, 3202, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.27687305874294704, "compression_ratio": 1.4787878787878788, "no_speech_prob": 8.530192644684575e-06}, {"id": 490, "seek": 301424, "start": 3030.72, "end": 3041.52, "text": " So last week we were looking at the spelling bee, where the input's worth things like zu-wiki,", "tokens": [407, 1036, 1243, 321, 645, 1237, 412, 264, 22254, 17479, 11, 689, 264, 4846, 311, 3163, 721, 411, 710, 84, 12, 86, 9850, 11], "temperature": 0.0, "avg_logprob": -0.27687305874294704, "compression_ratio": 1.4787878787878788, "no_speech_prob": 8.530192644684575e-06}, {"id": 491, "seek": 304152, "start": 3041.52, "end": 3048.24, "text": " and we were having to figure out how to spell it. We tried it without attention and we didn't", "tokens": [293, 321, 645, 1419, 281, 2573, 484, 577, 281, 9827, 309, 13, 492, 3031, 309, 1553, 3202, 293, 321, 994, 380], "temperature": 0.0, "avg_logprob": -0.17561354727115272, "compression_ratio": 1.66798418972332, "no_speech_prob": 8.397825695283245e-06}, {"id": 492, "seek": 304152, "start": 3048.24, "end": 3054.16, "text": " get great results. We looked at the paper from the original Badeno et al. paper that", "tokens": [483, 869, 3542, 13, 492, 2956, 412, 264, 3035, 490, 264, 3380, 11523, 5808, 1030, 419, 13, 3035, 300], "temperature": 0.0, "avg_logprob": -0.17561354727115272, "compression_ratio": 1.66798418972332, "no_speech_prob": 8.397825695283245e-06}, {"id": 493, "seek": 304152, "start": 3054.16, "end": 3058.32, "text": " showed that with longer sentences in particular, you get much better results if you do use", "tokens": [4712, 300, 365, 2854, 16579, 294, 1729, 11, 291, 483, 709, 1101, 3542, 498, 291, 360, 764], "temperature": 0.0, "avg_logprob": -0.17561354727115272, "compression_ratio": 1.66798418972332, "no_speech_prob": 8.397825695283245e-06}, {"id": 494, "seek": 304152, "start": 3058.32, "end": 3060.92, "text": " attention.", "tokens": [3202, 13], "temperature": 0.0, "avg_logprob": -0.17561354727115272, "compression_ratio": 1.66798418972332, "no_speech_prob": 8.397825695283245e-06}, {"id": 495, "seek": 304152, "start": 3060.92, "end": 3063.24, "text": " So let's have a look and see what that looks like.", "tokens": [407, 718, 311, 362, 257, 574, 293, 536, 437, 300, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.17561354727115272, "compression_ratio": 1.66798418972332, "no_speech_prob": 8.397825695283245e-06}, {"id": 496, "seek": 304152, "start": 3063.24, "end": 3070.84, "text": " So Keras doesn't really have anything to do this effectively, so I had to write something.", "tokens": [407, 591, 6985, 1177, 380, 534, 362, 1340, 281, 360, 341, 8659, 11, 370, 286, 632, 281, 2464, 746, 13], "temperature": 0.0, "avg_logprob": -0.17561354727115272, "compression_ratio": 1.66798418972332, "no_speech_prob": 8.397825695283245e-06}, {"id": 497, "seek": 307084, "start": 3070.84, "end": 3077.4, "text": " So before I show what I wrote, let me describe what it looks like. Most of it looks exactly", "tokens": [407, 949, 286, 855, 437, 286, 4114, 11, 718, 385, 6786, 437, 309, 1542, 411, 13, 4534, 295, 309, 1542, 2293], "temperature": 0.0, "avg_logprob": -0.1012852736881801, "compression_ratio": 1.608433734939759, "no_speech_prob": 8.664500455779489e-06}, {"id": 498, "seek": 307084, "start": 3077.4, "end": 3085.8, "text": " the same as the original spelling bee model. We have our list of phonemes as input. We", "tokens": [264, 912, 382, 264, 3380, 22254, 17479, 2316, 13, 492, 362, 527, 1329, 295, 30754, 443, 279, 382, 4846, 13, 492], "temperature": 0.0, "avg_logprob": -0.1012852736881801, "compression_ratio": 1.608433734939759, "no_speech_prob": 8.664500455779489e-06}, {"id": 499, "seek": 307084, "start": 3085.8, "end": 3096.2000000000003, "text": " have our list of letters to spell the word with as our decoder input. We chuck them both", "tokens": [362, 527, 1329, 295, 7825, 281, 9827, 264, 1349, 365, 382, 527, 979, 19866, 4846, 13, 492, 20870, 552, 1293], "temperature": 0.0, "avg_logprob": -0.1012852736881801, "compression_ratio": 1.608433734939759, "no_speech_prob": 8.664500455779489e-06}, {"id": 500, "seek": 309620, "start": 3096.2, "end": 3106.96, "text": " through embeddings. We then do a bidirectional RNN on the phonemes, and then we chuck an", "tokens": [807, 12240, 29432, 13, 492, 550, 360, 257, 12957, 621, 41048, 45702, 45, 322, 264, 30754, 443, 279, 11, 293, 550, 321, 20870, 364], "temperature": 0.0, "avg_logprob": -0.2512822691927251, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.8058326531900093e-05}, {"id": 501, "seek": 309620, "start": 3106.96, "end": 3111.3199999999997, "text": " RNN on top of that, and then we chuck an RNN on top of that. You might remember from last", "tokens": [45702, 45, 322, 1192, 295, 300, 11, 293, 550, 321, 20870, 364, 45702, 45, 322, 1192, 295, 300, 13, 509, 1062, 1604, 490, 1036], "temperature": 0.0, "avg_logprob": -0.2512822691927251, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.8058326531900093e-05}, {"id": 502, "seek": 309620, "start": 3111.3199999999997, "end": 3116.7599999999998, "text": " time this getRNN function, just to remind you, there's just something that's returning", "tokens": [565, 341, 483, 49, 45, 45, 2445, 11, 445, 281, 4160, 291, 11, 456, 311, 445, 746, 300, 311, 12678], "temperature": 0.0, "avg_logprob": -0.2512822691927251, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.8058326531900093e-05}, {"id": 503, "seek": 309620, "start": 3116.7599999999998, "end": 3124.6, "text": " an else. And by default we're saying return sequence equals true. So last week the final", "tokens": [364, 1646, 13, 400, 538, 7576, 321, 434, 1566, 2736, 8310, 6915, 2074, 13, 407, 1036, 1243, 264, 2572], "temperature": 0.0, "avg_logprob": -0.2512822691927251, "compression_ratio": 1.6777251184834123, "no_speech_prob": 1.8058326531900093e-05}, {"id": 504, "seek": 312460, "start": 3124.6, "end": 3130.72, "text": " one was getRNN false. We put everything into a single little package and then we fed that", "tokens": [472, 390, 483, 49, 45, 45, 7908, 13, 492, 829, 1203, 666, 257, 2167, 707, 7372, 293, 550, 321, 4636, 300], "temperature": 0.0, "avg_logprob": -0.17714164040305397, "compression_ratio": 1.6865079365079365, "no_speech_prob": 6.048879640729865e-06}, {"id": 505, "seek": 312460, "start": 3130.72, "end": 3138.7599999999998, "text": " to the RNN. But now we're leaving return sequences equals true all the way through so that we", "tokens": [281, 264, 45702, 45, 13, 583, 586, 321, 434, 5012, 2736, 22978, 6915, 2074, 439, 264, 636, 807, 370, 300, 321], "temperature": 0.0, "avg_logprob": -0.17714164040305397, "compression_ratio": 1.6865079365079365, "no_speech_prob": 6.048879640729865e-06}, {"id": 506, "seek": 312460, "start": 3138.7599999999998, "end": 3142.52, "text": " can then pass it to this special attention layer that I created.", "tokens": [393, 550, 1320, 309, 281, 341, 2121, 3202, 4583, 300, 286, 2942, 13], "temperature": 0.0, "avg_logprob": -0.17714164040305397, "compression_ratio": 1.6865079365079365, "no_speech_prob": 6.048879640729865e-06}, {"id": 507, "seek": 312460, "start": 3142.52, "end": 3147.56, "text": " We'll look at how this works in a moment. But let's first of all talk about what does", "tokens": [492, 603, 574, 412, 577, 341, 1985, 294, 257, 1623, 13, 583, 718, 311, 700, 295, 439, 751, 466, 437, 775], "temperature": 0.0, "avg_logprob": -0.17714164040305397, "compression_ratio": 1.6865079365079365, "no_speech_prob": 6.048879640729865e-06}, {"id": 508, "seek": 312460, "start": 3147.56, "end": 3154.52, "text": " an intention layer need. It needs to know what kind of RNN layer do you want it to create.", "tokens": [364, 7789, 4583, 643, 13, 467, 2203, 281, 458, 437, 733, 295, 45702, 45, 4583, 360, 291, 528, 309, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.17714164040305397, "compression_ratio": 1.6865079365079365, "no_speech_prob": 6.048879640729865e-06}, {"id": 509, "seek": 315452, "start": 3154.52, "end": 3163.56, "text": " So I just pass it a function which creates an RNN layer. In your decoder, how many layers", "tokens": [407, 286, 445, 1320, 309, 257, 2445, 597, 7829, 364, 45702, 45, 4583, 13, 682, 428, 979, 19866, 11, 577, 867, 7914], "temperature": 0.0, "avg_logprob": -0.1363952185517998, "compression_ratio": 1.6732673267326732, "no_speech_prob": 9.368658538733143e-06}, {"id": 510, "seek": 315452, "start": 3163.56, "end": 3169.92, "text": " do you want to create? So I say create 3 LSTN layers in the decoder.", "tokens": [360, 291, 528, 281, 1884, 30, 407, 286, 584, 1884, 805, 441, 6840, 45, 7914, 294, 264, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1363952185517998, "compression_ratio": 1.6732673267326732, "no_speech_prob": 9.368658538733143e-06}, {"id": 511, "seek": 315452, "start": 3169.92, "end": 3175.16, "text": " And then what information is it going to need to create that? It's going to need all of", "tokens": [400, 550, 437, 1589, 307, 309, 516, 281, 643, 281, 1884, 300, 30, 467, 311, 516, 281, 643, 439, 295], "temperature": 0.0, "avg_logprob": -0.1363952185517998, "compression_ratio": 1.6732673267326732, "no_speech_prob": 9.368658538733143e-06}, {"id": 512, "seek": 315452, "start": 3175.16, "end": 3184.04, "text": " the encoder state, and then we can do something else to make it easier. We can do something", "tokens": [264, 2058, 19866, 1785, 11, 293, 550, 321, 393, 360, 746, 1646, 281, 652, 309, 3571, 13, 492, 393, 360, 746], "temperature": 0.0, "avg_logprob": -0.1363952185517998, "compression_ratio": 1.6732673267326732, "no_speech_prob": 9.368658538733143e-06}, {"id": 513, "seek": 318404, "start": 3184.04, "end": 3190.2799999999997, "text": " called teacher forcing. And teacher forcing is we are going to pass it, as well as the", "tokens": [1219, 5027, 19030, 13, 400, 5027, 19030, 307, 321, 366, 516, 281, 1320, 309, 11, 382, 731, 382, 264], "temperature": 0.0, "avg_logprob": -0.17354626558264907, "compression_ratio": 1.9281767955801106, "no_speech_prob": 1.1300753612886183e-05}, {"id": 514, "seek": 318404, "start": 3190.2799999999997, "end": 3196.24, "text": " encoder state, we're going to pass it the answer, but we're going to pass it the answer", "tokens": [2058, 19866, 1785, 11, 321, 434, 516, 281, 1320, 309, 264, 1867, 11, 457, 321, 434, 516, 281, 1320, 309, 264, 1867], "temperature": 0.0, "avg_logprob": -0.17354626558264907, "compression_ratio": 1.9281767955801106, "no_speech_prob": 1.1300753612886183e-05}, {"id": 515, "seek": 318404, "start": 3196.24, "end": 3203.84, "text": " for the previous time period. So in other words, if we're trying to learn how to spell", "tokens": [337, 264, 3894, 565, 2896, 13, 407, 294, 661, 2283, 11, 498, 321, 434, 1382, 281, 1466, 577, 281, 9827], "temperature": 0.0, "avg_logprob": -0.17354626558264907, "compression_ratio": 1.9281767955801106, "no_speech_prob": 1.1300753612886183e-05}, {"id": 516, "seek": 318404, "start": 3203.84, "end": 3213.52, "text": " the wiki, at step 1 we're not going to pass it any layer. At step 2 we'll pass it Z. At", "tokens": [264, 261, 9850, 11, 412, 1823, 502, 321, 434, 406, 516, 281, 1320, 309, 604, 4583, 13, 1711, 1823, 568, 321, 603, 1320, 309, 1176, 13, 1711], "temperature": 0.0, "avg_logprob": -0.17354626558264907, "compression_ratio": 1.9281767955801106, "no_speech_prob": 1.1300753612886183e-05}, {"id": 517, "seek": 321352, "start": 3213.52, "end": 3218.04, "text": " step 3 we'll pass it Y. At step 4 we'll pass it W. In other words, we're going to tell", "tokens": [1823, 805, 321, 603, 1320, 309, 398, 13, 1711, 1823, 1017, 321, 603, 1320, 309, 343, 13, 682, 661, 2283, 11, 321, 434, 516, 281, 980], "temperature": 0.0, "avg_logprob": -0.11737330918459549, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.1875538297754247e-06}, {"id": 518, "seek": 321352, "start": 3218.04, "end": 3225.04, "text": " it what the previous time step's correct answer was. Why do we do that? It makes it much easier", "tokens": [309, 437, 264, 3894, 565, 1823, 311, 3006, 1867, 390, 13, 1545, 360, 321, 360, 300, 30, 467, 1669, 309, 709, 3571], "temperature": 0.0, "avg_logprob": -0.11737330918459549, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.1875538297754247e-06}, {"id": 519, "seek": 321352, "start": 3225.04, "end": 3233.44, "text": " to train, particularly early on. Early on it has no idea how to spell, and so it gets", "tokens": [281, 3847, 11, 4098, 2440, 322, 13, 18344, 322, 309, 575, 572, 1558, 577, 281, 9827, 11, 293, 370, 309, 2170], "temperature": 0.0, "avg_logprob": -0.11737330918459549, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.1875538297754247e-06}, {"id": 520, "seek": 321352, "start": 3233.44, "end": 3239.96, "text": " most things wrong most of the time. And so the later letters, everything's wrong before", "tokens": [881, 721, 2085, 881, 295, 264, 565, 13, 400, 370, 264, 1780, 7825, 11, 1203, 311, 2085, 949], "temperature": 0.0, "avg_logprob": -0.11737330918459549, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.1875538297754247e-06}, {"id": 521, "seek": 323996, "start": 3239.96, "end": 3245.68, "text": " it. So how can it possibly know what letter to use now?", "tokens": [309, 13, 407, 577, 393, 309, 6264, 458, 437, 5063, 281, 764, 586, 30], "temperature": 0.0, "avg_logprob": -0.1388705571492513, "compression_ratio": 1.5665024630541873, "no_speech_prob": 3.6119683954893844e-06}, {"id": 522, "seek": 323996, "start": 3245.68, "end": 3256.48, "text": " So with teacher forcing, we're going to take our input data, being all of those encoder", "tokens": [407, 365, 5027, 19030, 11, 321, 434, 516, 281, 747, 527, 4846, 1412, 11, 885, 439, 295, 729, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.1388705571492513, "compression_ratio": 1.5665024630541873, "no_speech_prob": 3.6119683954893844e-06}, {"id": 523, "seek": 323996, "start": 3256.48, "end": 3262.68, "text": " hidden states, but we're also going to tell it, even if you got the previous letter wrong,", "tokens": [7633, 4368, 11, 457, 321, 434, 611, 516, 281, 980, 309, 11, 754, 498, 291, 658, 264, 3894, 5063, 2085, 11], "temperature": 0.0, "avg_logprob": -0.1388705571492513, "compression_ratio": 1.5665024630541873, "no_speech_prob": 3.6119683954893844e-06}, {"id": 524, "seek": 323996, "start": 3262.68, "end": 3267.48, "text": " this is what it should have been. So it's going to make it a bit easier for it. And", "tokens": [341, 307, 437, 309, 820, 362, 668, 13, 407, 309, 311, 516, 281, 652, 309, 257, 857, 3571, 337, 309, 13, 400], "temperature": 0.0, "avg_logprob": -0.1388705571492513, "compression_ratio": 1.5665024630541873, "no_speech_prob": 3.6119683954893844e-06}, {"id": 525, "seek": 326748, "start": 3267.48, "end": 3275.16, "text": " all we do is we just concatenate together the hidden state plus that decoder input.", "tokens": [439, 321, 360, 307, 321, 445, 1588, 7186, 473, 1214, 264, 7633, 1785, 1804, 300, 979, 19866, 4846, 13], "temperature": 0.0, "avg_logprob": -0.09391540720842886, "compression_ratio": 1.587878787878788, "no_speech_prob": 1.933350631588837e-06}, {"id": 526, "seek": 326748, "start": 3275.16, "end": 3281.52, "text": " So that's called teacher forcing. You don't have to use teacher forcing, but if you do,", "tokens": [407, 300, 311, 1219, 5027, 19030, 13, 509, 500, 380, 362, 281, 764, 5027, 19030, 11, 457, 498, 291, 360, 11], "temperature": 0.0, "avg_logprob": -0.09391540720842886, "compression_ratio": 1.587878787878788, "no_speech_prob": 1.933350631588837e-06}, {"id": 527, "seek": 326748, "start": 3281.52, "end": 3291.28, "text": " it just makes it faster and easier to train. So that's why we pass in this special decoder", "tokens": [309, 445, 1669, 309, 4663, 293, 3571, 281, 3847, 13, 407, 300, 311, 983, 321, 1320, 294, 341, 2121, 979, 19866], "temperature": 0.0, "avg_logprob": -0.09391540720842886, "compression_ratio": 1.587878787878788, "no_speech_prob": 1.933350631588837e-06}, {"id": 528, "seek": 329128, "start": 3291.28, "end": 3311.6400000000003, "text": " input. And just to show you, the decoder input...", "tokens": [4846, 13, 400, 445, 281, 855, 291, 11, 264, 979, 19866, 4846, 1097], "temperature": 0.0, "avg_logprob": -0.23838508419874238, "compression_ratio": 1.2272727272727273, "no_speech_prob": 3.668847512017237e-06}, {"id": 529, "seek": 329128, "start": 3311.6400000000003, "end": 3319.48, "text": " It's all of my labels except not the very last one. So it's going to be one less than", "tokens": [467, 311, 439, 295, 452, 16949, 3993, 406, 264, 588, 1036, 472, 13, 407, 309, 311, 516, 281, 312, 472, 1570, 813], "temperature": 0.0, "avg_logprob": -0.23838508419874238, "compression_ratio": 1.2272727272727273, "no_speech_prob": 3.668847512017237e-06}, {"id": 530, "seek": 331948, "start": 3319.48, "end": 3331.0, "text": " the full number of letters to spell. And then I concatenate a column of ones onto the front.", "tokens": [264, 1577, 1230, 295, 7825, 281, 9827, 13, 400, 550, 286, 1588, 7186, 473, 257, 7738, 295, 2306, 3911, 264, 1868, 13], "temperature": 0.0, "avg_logprob": -0.2740758973724988, "compression_ratio": 1.4444444444444444, "no_speech_prob": 7.766891940264031e-06}, {"id": 531, "seek": 331948, "start": 3331.0, "end": 3338.88, "text": " Now why is it a column of ones times Go token? And then Go token was something that I set", "tokens": [823, 983, 307, 309, 257, 7738, 295, 2306, 1413, 1037, 14862, 30, 400, 550, 1037, 14862, 390, 746, 300, 286, 992], "temperature": 0.0, "avg_logprob": -0.2740758973724988, "compression_ratio": 1.4444444444444444, "no_speech_prob": 7.766891940264031e-06}, {"id": 532, "seek": 333888, "start": 3338.88, "end": 3351.92, "text": " up back here. I decided that an asterisk is going to be a special character which means", "tokens": [493, 646, 510, 13, 286, 3047, 300, 364, 257, 3120, 7797, 307, 516, 281, 312, 257, 2121, 2517, 597, 1355], "temperature": 0.0, "avg_logprob": -0.21176347732543946, "compression_ratio": 1.3488372093023255, "no_speech_prob": 2.144453719665762e-05}, {"id": 533, "seek": 333888, "start": 3351.92, "end": 3359.32, "text": " this is the start of the word. So every time we're going to get past if it was a wiki,", "tokens": [341, 307, 264, 722, 295, 264, 1349, 13, 407, 633, 565, 321, 434, 516, 281, 483, 1791, 498, 309, 390, 257, 261, 9850, 11], "temperature": 0.0, "avg_logprob": -0.21176347732543946, "compression_ratio": 1.3488372093023255, "no_speech_prob": 2.144453719665762e-05}, {"id": 534, "seek": 335932, "start": 3359.32, "end": 3369.56, "text": " we'd get past asterisk, z, y, w, i, c, k. So that's going to be this teacher forcing", "tokens": [321, 1116, 483, 1791, 257, 3120, 7797, 11, 710, 11, 288, 11, 261, 11, 741, 11, 269, 11, 350, 13, 407, 300, 311, 516, 281, 312, 341, 5027, 19030], "temperature": 0.0, "avg_logprob": -0.1623971739480662, "compression_ratio": 1.6319018404907975, "no_speech_prob": 3.089472329520504e-06}, {"id": 535, "seek": 335932, "start": 3369.56, "end": 3376.92, "text": " input.", "tokens": [4846, 13], "temperature": 0.0, "avg_logprob": -0.1623971739480662, "compression_ratio": 1.6319018404907975, "no_speech_prob": 3.089472329520504e-06}, {"id": 536, "seek": 335932, "start": 3376.92, "end": 3381.92, "text": " So this attention layer is going to create a 3-layer RNN. It's going to get given all", "tokens": [407, 341, 3202, 4583, 307, 516, 281, 1884, 257, 805, 12, 8376, 260, 45702, 45, 13, 467, 311, 516, 281, 483, 2212, 439], "temperature": 0.0, "avg_logprob": -0.1623971739480662, "compression_ratio": 1.6319018404907975, "no_speech_prob": 3.089472329520504e-06}, {"id": 537, "seek": 335932, "start": 3381.92, "end": 3387.98, "text": " of the encoder state. It's going to get given that special decoder input. And it's going", "tokens": [295, 264, 2058, 19866, 1785, 13, 467, 311, 516, 281, 483, 2212, 300, 2121, 979, 19866, 4846, 13, 400, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.1623971739480662, "compression_ratio": 1.6319018404907975, "no_speech_prob": 3.089472329520504e-06}, {"id": 538, "seek": 338798, "start": 3387.98, "end": 3396.16, "text": " to spit out just a list of states again. So we can just do the usual thing of doing a", "tokens": [281, 22127, 484, 445, 257, 1329, 295, 4368, 797, 13, 407, 321, 393, 445, 360, 264, 7713, 551, 295, 884, 257], "temperature": 0.0, "avg_logprob": -0.15929700232840874, "compression_ratio": 1.545945945945946, "no_speech_prob": 4.565931703837123e-06}, {"id": 539, "seek": 338798, "start": 3396.16, "end": 3404.52, "text": " time-distributed dense with our appropriate vocab-size softmax to turn that into our target", "tokens": [565, 12, 42649, 2024, 4866, 18011, 365, 527, 6854, 2329, 455, 12, 27553, 2787, 41167, 281, 1261, 300, 666, 527, 3779], "temperature": 0.0, "avg_logprob": -0.15929700232840874, "compression_ratio": 1.545945945945946, "no_speech_prob": 4.565931703837123e-06}, {"id": 540, "seek": 338798, "start": 3404.52, "end": 3406.16, "text": " activations.", "tokens": [2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.15929700232840874, "compression_ratio": 1.545945945945946, "no_speech_prob": 4.565931703837123e-06}, {"id": 541, "seek": 338798, "start": 3406.16, "end": 3412.76, "text": " So putting aside how we calculate this, everything else is pretty familiar. So we can then just", "tokens": [407, 3372, 7359, 577, 321, 8873, 341, 11, 1203, 1646, 307, 1238, 4963, 13, 407, 321, 393, 550, 445], "temperature": 0.0, "avg_logprob": -0.15929700232840874, "compression_ratio": 1.545945945945946, "no_speech_prob": 4.565931703837123e-06}, {"id": 542, "seek": 341276, "start": 3412.76, "end": 3423.96, "text": " go ahead, build that model, compile it, fit it, passing in both the phonemes and those", "tokens": [352, 2286, 11, 1322, 300, 2316, 11, 31413, 309, 11, 3318, 309, 11, 8437, 294, 1293, 264, 30754, 443, 279, 293, 729], "temperature": 0.0, "avg_logprob": -0.20223030211433532, "compression_ratio": 1.4580645161290322, "no_speech_prob": 4.495144366956083e-06}, {"id": 543, "seek": 341276, "start": 3423.96, "end": 3432.4, "text": " decoder inputs, train it for a while, a bit of annealing, train it a bit more, until eventually", "tokens": [979, 19866, 15743, 11, 3847, 309, 337, 257, 1339, 11, 257, 857, 295, 22256, 4270, 11, 3847, 309, 257, 857, 544, 11, 1826, 4728], "temperature": 0.0, "avg_logprob": -0.20223030211433532, "compression_ratio": 1.4580645161290322, "no_speech_prob": 4.495144366956083e-06}, {"id": 544, "seek": 341276, "start": 3432.4, "end": 3435.4, "text": " we have actually pretty good accuracy, 51%.", "tokens": [321, 362, 767, 1238, 665, 14170, 11, 18485, 6856], "temperature": 0.0, "avg_logprob": -0.20223030211433532, "compression_ratio": 1.4580645161290322, "no_speech_prob": 4.495144366956083e-06}, {"id": 545, "seek": 343540, "start": 3435.4, "end": 3445.1600000000003, "text": " Did you have a question, Rachel? Do you know if the current work strictly uses the hidden", "tokens": [2589, 291, 362, 257, 1168, 11, 14246, 30, 1144, 291, 458, 498, 264, 2190, 589, 20792, 4960, 264, 7633], "temperature": 0.0, "avg_logprob": -0.1851119297306712, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.69640716194408e-05}, {"id": 546, "seek": 343540, "start": 3445.1600000000003, "end": 3449.76, "text": " states of the final RNN layer, or have people tried using attention sums over different", "tokens": [4368, 295, 264, 2572, 45702, 45, 4583, 11, 420, 362, 561, 3031, 1228, 3202, 34499, 670, 819], "temperature": 0.0, "avg_logprob": -0.1851119297306712, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.69640716194408e-05}, {"id": 547, "seek": 343540, "start": 3449.76, "end": 3455.32, "text": " indices over multiple layers, the way we did using perceptual loss for multiple layers?", "tokens": [43840, 670, 3866, 7914, 11, 264, 636, 321, 630, 1228, 43276, 901, 4470, 337, 3866, 7914, 30], "temperature": 0.0, "avg_logprob": -0.1851119297306712, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.69640716194408e-05}, {"id": 548, "seek": 343540, "start": 3455.32, "end": 3460.96, "text": " That's actually a great question. The answer is, to my knowledge, people are always using", "tokens": [663, 311, 767, 257, 869, 1168, 13, 440, 1867, 307, 11, 281, 452, 3601, 11, 561, 366, 1009, 1228], "temperature": 0.0, "avg_logprob": -0.1851119297306712, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.69640716194408e-05}, {"id": 549, "seek": 346096, "start": 3460.96, "end": 3468.12, "text": " the last layer hidden state. But I kind of accidentally used all of them once and got", "tokens": [264, 1036, 4583, 7633, 1785, 13, 583, 286, 733, 295, 15715, 1143, 439, 295, 552, 1564, 293, 658], "temperature": 0.0, "avg_logprob": -0.16976869517359242, "compression_ratio": 1.5598086124401913, "no_speech_prob": 3.219195059500635e-05}, {"id": 550, "seek": 346096, "start": 3468.12, "end": 3473.88, "text": " some better results. So I don't know if I screwed something up or if this is a good", "tokens": [512, 1101, 3542, 13, 407, 286, 500, 380, 458, 498, 286, 20331, 746, 493, 420, 498, 341, 307, 257, 665], "temperature": 0.0, "avg_logprob": -0.16976869517359242, "compression_ratio": 1.5598086124401913, "no_speech_prob": 3.219195059500635e-05}, {"id": 551, "seek": 346096, "start": 3473.88, "end": 3481.84, "text": " idea. So that might be something if you want to test it to try it out.", "tokens": [1558, 13, 407, 300, 1062, 312, 746, 498, 291, 528, 281, 1500, 309, 281, 853, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.16976869517359242, "compression_ratio": 1.5598086124401913, "no_speech_prob": 3.219195059500635e-05}, {"id": 552, "seek": 346096, "start": 3481.84, "end": 3488.48, "text": " I was like, after fixing a bug, things got worse, which is definitely a good sign. So", "tokens": [286, 390, 411, 11, 934, 19442, 257, 7426, 11, 721, 658, 5324, 11, 597, 307, 2138, 257, 665, 1465, 13, 407], "temperature": 0.0, "avg_logprob": -0.16976869517359242, "compression_ratio": 1.5598086124401913, "no_speech_prob": 3.219195059500635e-05}, {"id": 553, "seek": 348848, "start": 3488.48, "end": 3493.52, "text": " maybe it's not a bug after all. So remember last time we had trouble with the really long", "tokens": [1310, 309, 311, 406, 257, 7426, 934, 439, 13, 407, 1604, 1036, 565, 321, 632, 5253, 365, 264, 534, 938], "temperature": 0.0, "avg_logprob": -0.15248394012451172, "compression_ratio": 1.4884792626728112, "no_speech_prob": 1.406389674230013e-05}, {"id": 554, "seek": 348848, "start": 3493.52, "end": 3500.4, "text": " words, lots of phonemes. But here's 11 phonemes and it's done it perfectly. So that's a good", "tokens": [2283, 11, 3195, 295, 30754, 443, 279, 13, 583, 510, 311, 2975, 30754, 443, 279, 293, 309, 311, 1096, 309, 6239, 13, 407, 300, 311, 257, 665], "temperature": 0.0, "avg_logprob": -0.15248394012451172, "compression_ratio": 1.4884792626728112, "no_speech_prob": 1.406389674230013e-05}, {"id": 555, "seek": 348848, "start": 3500.4, "end": 3505.32, "text": " sign that this is handling some longer ones pretty well.", "tokens": [1465, 300, 341, 307, 13175, 512, 2854, 2306, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.15248394012451172, "compression_ratio": 1.4884792626728112, "no_speech_prob": 1.406389674230013e-05}, {"id": 556, "seek": 348848, "start": 3505.32, "end": 3514.72, "text": " Is 51% good or bad? I don't know, because spelling is not exact. There's no way you", "tokens": [1119, 18485, 4, 665, 420, 1578, 30, 286, 500, 380, 458, 11, 570, 22254, 307, 406, 1900, 13, 821, 311, 572, 636, 291], "temperature": 0.0, "avg_logprob": -0.15248394012451172, "compression_ratio": 1.4884792626728112, "no_speech_prob": 1.406389674230013e-05}, {"id": 557, "seek": 351472, "start": 3514.72, "end": 3520.56, "text": " could algorithmically get 100% spelling. Is it meant to be speak like this or speak like", "tokens": [727, 9284, 984, 483, 2319, 4, 22254, 13, 1119, 309, 4140, 281, 312, 1710, 411, 341, 420, 1710, 411], "temperature": 0.0, "avg_logprob": -0.1474050848107589, "compression_ratio": 1.5193370165745856, "no_speech_prob": 1.3287641422721208e-06}, {"id": 558, "seek": 351472, "start": 3520.56, "end": 3526.48, "text": " this? This is from a test set, there's no way for it to know. So I think it's done pretty", "tokens": [341, 30, 639, 307, 490, 257, 1500, 992, 11, 456, 311, 572, 636, 337, 309, 281, 458, 13, 407, 286, 519, 309, 311, 1096, 1238], "temperature": 0.0, "avg_logprob": -0.1474050848107589, "compression_ratio": 1.5193370165745856, "no_speech_prob": 1.3287641422721208e-06}, {"id": 559, "seek": 351472, "start": 3526.48, "end": 3528.12, "text": " well.", "tokens": [731, 13], "temperature": 0.0, "avg_logprob": -0.1474050848107589, "compression_ratio": 1.5193370165745856, "no_speech_prob": 1.3287641422721208e-06}, {"id": 560, "seek": 351472, "start": 3528.12, "end": 3534.7599999999998, "text": " So the only thing remaining is how to do this attention layer. So the bad news is that the", "tokens": [407, 264, 787, 551, 8877, 307, 577, 281, 360, 341, 3202, 4583, 13, 407, 264, 1578, 2583, 307, 300, 264], "temperature": 0.0, "avg_logprob": -0.1474050848107589, "compression_ratio": 1.5193370165745856, "no_speech_prob": 1.3287641422721208e-06}, {"id": 561, "seek": 353476, "start": 3534.76, "end": 3552.32, "text": " attention layer is the largest custom layer we've looked at. It's not terrible, but it's", "tokens": [3202, 4583, 307, 264, 6443, 2375, 4583, 321, 600, 2956, 412, 13, 467, 311, 406, 6237, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.17187096761620563, "compression_ratio": 1.3333333333333333, "no_speech_prob": 2.9479908789653564e-06}, {"id": 562, "seek": 353476, "start": 3552.32, "end": 3563.32, "text": " 100 lines of code. But a lot of it's pretty repetitive. So it really depends how interested", "tokens": [2319, 3876, 295, 3089, 13, 583, 257, 688, 295, 309, 311, 1238, 29404, 13, 407, 309, 534, 5946, 577, 3102], "temperature": 0.0, "avg_logprob": -0.17187096761620563, "compression_ratio": 1.3333333333333333, "no_speech_prob": 2.9479908789653564e-06}, {"id": 563, "seek": 356332, "start": 3563.32, "end": 3569.0, "text": " you are, A, in custom layers and B, in attention as to how much you want to look at this. But", "tokens": [291, 366, 11, 316, 11, 294, 2375, 7914, 293, 363, 11, 294, 3202, 382, 281, 577, 709, 291, 528, 281, 574, 412, 341, 13, 583], "temperature": 0.0, "avg_logprob": -0.1364104820020271, "compression_ratio": 1.7678571428571428, "no_speech_prob": 6.643328106292756e-06}, {"id": 564, "seek": 356332, "start": 3569.0, "end": 3573.04, "text": " basically I'll show you the key pieces.", "tokens": [1936, 286, 603, 855, 291, 264, 2141, 3755, 13], "temperature": 0.0, "avg_logprob": -0.1364104820020271, "compression_ratio": 1.7678571428571428, "no_speech_prob": 6.643328106292756e-06}, {"id": 565, "seek": 356332, "start": 3573.04, "end": 3577.2000000000003, "text": " There are two really important methods that get called in your custom layer, and they", "tokens": [821, 366, 732, 534, 1021, 7150, 300, 483, 1219, 294, 428, 2375, 4583, 11, 293, 436], "temperature": 0.0, "avg_logprob": -0.1364104820020271, "compression_ratio": 1.7678571428571428, "no_speech_prob": 6.643328106292756e-06}, {"id": 566, "seek": 356332, "start": 3577.2000000000003, "end": 3584.5800000000004, "text": " are build and call. Build is the method that's called in your custom layer when your layer", "tokens": [366, 1322, 293, 818, 13, 11875, 307, 264, 3170, 300, 311, 1219, 294, 428, 2375, 4583, 562, 428, 4583], "temperature": 0.0, "avg_logprob": -0.1364104820020271, "compression_ratio": 1.7678571428571428, "no_speech_prob": 6.643328106292756e-06}, {"id": 567, "seek": 356332, "start": 3584.5800000000004, "end": 3591.1600000000003, "text": " is actually inserted into a neural network and that causes it to get built. Once it's", "tokens": [307, 767, 27992, 666, 257, 18161, 3209, 293, 300, 7700, 309, 281, 483, 3094, 13, 3443, 309, 311], "temperature": 0.0, "avg_logprob": -0.1364104820020271, "compression_ratio": 1.7678571428571428, "no_speech_prob": 6.643328106292756e-06}, {"id": 568, "seek": 359116, "start": 3591.16, "end": 3597.08, "text": " inserted into a Keras neural network, that's the point at which it knows how big everything", "tokens": [27992, 666, 257, 591, 6985, 18161, 3209, 11, 300, 311, 264, 935, 412, 597, 309, 3255, 577, 955, 1203], "temperature": 0.0, "avg_logprob": -0.13181734936577932, "compression_ratio": 1.7755905511811023, "no_speech_prob": 2.0904442408209434e-06}, {"id": 569, "seek": 359116, "start": 3597.08, "end": 3602.08, "text": " has to be. Because you told it how big the input was, and then you've got the various", "tokens": [575, 281, 312, 13, 1436, 291, 1907, 309, 577, 955, 264, 4846, 390, 11, 293, 550, 291, 600, 658, 264, 3683], "temperature": 0.0, "avg_logprob": -0.13181734936577932, "compression_ratio": 1.7755905511811023, "no_speech_prob": 2.0904442408209434e-06}, {"id": 570, "seek": 359116, "start": 3602.08, "end": 3609.08, "text": " layers connected to each other, and so eventually it can figure out how big. In fact, specifically,", "tokens": [7914, 4582, 281, 1184, 661, 11, 293, 370, 4728, 309, 393, 2573, 484, 577, 955, 13, 682, 1186, 11, 4682, 11], "temperature": 0.0, "avg_logprob": -0.13181734936577932, "compression_ratio": 1.7755905511811023, "no_speech_prob": 2.0904442408209434e-06}, {"id": 571, "seek": 359116, "start": 3609.08, "end": 3614.64, "text": " what is the shape of the input. So when your build method is called, you get told what", "tokens": [437, 307, 264, 3909, 295, 264, 4846, 13, 407, 562, 428, 1322, 3170, 307, 1219, 11, 291, 483, 1907, 437], "temperature": 0.0, "avg_logprob": -0.13181734936577932, "compression_ratio": 1.7755905511811023, "no_speech_prob": 2.0904442408209434e-06}, {"id": 572, "seek": 359116, "start": 3614.64, "end": 3619.44, "text": " is the shape of your input. And this is the point where you now have to set everything", "tokens": [307, 264, 3909, 295, 428, 4846, 13, 400, 341, 307, 264, 935, 689, 291, 586, 362, 281, 992, 1203], "temperature": 0.0, "avg_logprob": -0.13181734936577932, "compression_ratio": 1.7755905511811023, "no_speech_prob": 2.0904442408209434e-06}, {"id": 573, "seek": 361944, "start": 3619.44, "end": 3626.64, "text": " up to work with that input shape. So in this case, build gets called as soon as it finishes", "tokens": [493, 281, 589, 365, 300, 4846, 3909, 13, 407, 294, 341, 1389, 11, 1322, 2170, 1219, 382, 2321, 382, 309, 23615], "temperature": 0.0, "avg_logprob": -0.1188020935977798, "compression_ratio": 1.6294416243654823, "no_speech_prob": 2.406090516160475e-06}, {"id": 574, "seek": 361944, "start": 3626.64, "end": 3633.28, "text": " running this line because it knows that it got this input. And it can figure out this", "tokens": [2614, 341, 1622, 570, 309, 3255, 300, 309, 658, 341, 4846, 13, 400, 309, 393, 2573, 484, 341], "temperature": 0.0, "avg_logprob": -0.1188020935977798, "compression_ratio": 1.6294416243654823, "no_speech_prob": 2.406090516160475e-06}, {"id": 575, "seek": 361944, "start": 3633.28, "end": 3637.84, "text": " input shape by going all the way back through all this.", "tokens": [4846, 3909, 538, 516, 439, 264, 636, 646, 807, 439, 341, 13], "temperature": 0.0, "avg_logprob": -0.1188020935977798, "compression_ratio": 1.6294416243654823, "no_speech_prob": 2.406090516160475e-06}, {"id": 576, "seek": 361944, "start": 3637.84, "end": 3644.6, "text": " Now we have two inputs. We have the encoder state and the decoder input. So we can pull", "tokens": [823, 321, 362, 732, 15743, 13, 492, 362, 264, 2058, 19866, 1785, 293, 264, 979, 19866, 4846, 13, 407, 321, 393, 2235], "temperature": 0.0, "avg_logprob": -0.1188020935977798, "compression_ratio": 1.6294416243654823, "no_speech_prob": 2.406090516160475e-06}, {"id": 577, "seek": 364460, "start": 3644.6, "end": 3651.2, "text": " those apart into the encoder shape and the decoder input shape.", "tokens": [729, 4936, 666, 264, 2058, 19866, 3909, 293, 264, 979, 19866, 4846, 3909, 13], "temperature": 0.0, "avg_logprob": -0.1505247638339088, "compression_ratio": 1.509090909090909, "no_speech_prob": 6.2408926169155166e-06}, {"id": 578, "seek": 364460, "start": 3651.2, "end": 3654.8399999999997, "text": " So some things which seem like they might be hard are actually incredibly easy. To create", "tokens": [407, 512, 721, 597, 1643, 411, 436, 1062, 312, 1152, 366, 767, 6252, 1858, 13, 1407, 1884], "temperature": 0.0, "avg_logprob": -0.1505247638339088, "compression_ratio": 1.509090909090909, "no_speech_prob": 6.2408926169155166e-06}, {"id": 579, "seek": 364460, "start": 3654.8399999999997, "end": 3662.2799999999997, "text": " my 3 layers of RNNs is a single line of code. Remember I just passed in the RNN generating", "tokens": [452, 805, 7914, 295, 45702, 45, 82, 307, 257, 2167, 1622, 295, 3089, 13, 5459, 286, 445, 4678, 294, 264, 45702, 45, 17746], "temperature": 0.0, "avg_logprob": -0.1505247638339088, "compression_ratio": 1.509090909090909, "no_speech_prob": 6.2408926169155166e-06}, {"id": 580, "seek": 364460, "start": 3662.2799999999997, "end": 3669.7999999999997, "text": " function, so I just call it for each layer. And with the list comprehension, I now have", "tokens": [2445, 11, 370, 286, 445, 818, 309, 337, 1184, 4583, 13, 400, 365, 264, 1329, 44991, 11, 286, 586, 362], "temperature": 0.0, "avg_logprob": -0.1505247638339088, "compression_ratio": 1.509090909090909, "no_speech_prob": 6.2408926169155166e-06}, {"id": 581, "seek": 366980, "start": 3669.8, "end": 3676.0, "text": " there is 3 RNN layers in the list. That was easy.", "tokens": [456, 307, 805, 45702, 45, 7914, 294, 264, 1329, 13, 663, 390, 1858, 13], "temperature": 0.0, "avg_logprob": -0.18855239605081492, "compression_ratio": 1.4952830188679245, "no_speech_prob": 4.637852725863922e-06}, {"id": 582, "seek": 366980, "start": 3676.0, "end": 3685.0800000000004, "text": " On the other hand, to do this, I'm going to have to do it all by hand. And in fact, there's", "tokens": [1282, 264, 661, 1011, 11, 281, 360, 341, 11, 286, 478, 516, 281, 362, 281, 360, 309, 439, 538, 1011, 13, 400, 294, 1186, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.18855239605081492, "compression_ratio": 1.4952830188679245, "no_speech_prob": 4.637852725863922e-06}, {"id": 583, "seek": 366980, "start": 3685.0800000000004, "end": 3690.5600000000004, "text": " some things that are being hidden here. For example, a bias term is not included here.", "tokens": [512, 721, 300, 366, 885, 7633, 510, 13, 1171, 1365, 11, 257, 12577, 1433, 307, 406, 5556, 510, 13], "temperature": 0.0, "avg_logprob": -0.18855239605081492, "compression_ratio": 1.4952830188679245, "no_speech_prob": 4.637852725863922e-06}, {"id": 584, "seek": 366980, "start": 3690.5600000000004, "end": 3697.6800000000003, "text": " But we definitely want one. People often don't include bias terms because you can always", "tokens": [583, 321, 2138, 528, 472, 13, 3432, 2049, 500, 380, 4090, 12577, 2115, 570, 291, 393, 1009], "temperature": 0.0, "avg_logprob": -0.18855239605081492, "compression_ratio": 1.4952830188679245, "no_speech_prob": 4.637852725863922e-06}, {"id": 585, "seek": 369768, "start": 3697.68, "end": 3704.3199999999997, "text": " avoid it by adding a column of 1s to your data. So it's common in math not to mention", "tokens": [5042, 309, 538, 5127, 257, 7738, 295, 502, 82, 281, 428, 1412, 13, 407, 309, 311, 2689, 294, 5221, 406, 281, 2152], "temperature": 0.0, "avg_logprob": -0.15067474408583206, "compression_ratio": 1.5, "no_speech_prob": 5.0147127694799565e-06}, {"id": 586, "seek": 369768, "start": 3704.3199999999997, "end": 3709.56, "text": " the bias term. But generally speaking, any time you see a neural net, unless they state", "tokens": [264, 12577, 1433, 13, 583, 5101, 4124, 11, 604, 565, 291, 536, 257, 18161, 2533, 11, 5969, 436, 1785], "temperature": 0.0, "avg_logprob": -0.15067474408583206, "compression_ratio": 1.5, "no_speech_prob": 5.0147127694799565e-06}, {"id": 587, "seek": 369768, "start": 3709.56, "end": 3714.08, "text": " otherwise, there's probably a bias term there.", "tokens": [5911, 11, 456, 311, 1391, 257, 12577, 1433, 456, 13], "temperature": 0.0, "avg_logprob": -0.15067474408583206, "compression_ratio": 1.5, "no_speech_prob": 5.0147127694799565e-06}, {"id": 588, "seek": 369768, "start": 3714.08, "end": 3725.6, "text": " So in build, we're going to have to create W1, W2, and V. To create them, Keras actually", "tokens": [407, 294, 1322, 11, 321, 434, 516, 281, 362, 281, 1884, 343, 16, 11, 343, 17, 11, 293, 691, 13, 1407, 1884, 552, 11, 591, 6985, 767], "temperature": 0.0, "avg_logprob": -0.15067474408583206, "compression_ratio": 1.5, "no_speech_prob": 5.0147127694799565e-06}, {"id": 589, "seek": 372560, "start": 3725.6, "end": 3732.88, "text": " has a convenient method that comes from the layer superclass called addWeight. So addWeight", "tokens": [575, 257, 10851, 3170, 300, 1487, 490, 264, 4583, 1687, 11665, 1219, 909, 4360, 397, 13, 407, 909, 4360, 397], "temperature": 0.0, "avg_logprob": -0.2043589068130708, "compression_ratio": 1.5229885057471264, "no_speech_prob": 9.972830412152689e-06}, {"id": 590, "seek": 372560, "start": 3732.88, "end": 3738.16, "text": " you basically say, what's the size of the weight matrix, what's your initialization", "tokens": [291, 1936, 584, 11, 437, 311, 264, 2744, 295, 264, 3364, 8141, 11, 437, 311, 428, 5883, 2144], "temperature": 0.0, "avg_logprob": -0.2043589068130708, "compression_ratio": 1.5229885057471264, "no_speech_prob": 9.972830412152689e-06}, {"id": 591, "seek": 372560, "start": 3738.16, "end": 3751.2799999999997, "text": " function, and give it a name. So I just took that and called it W. So here I just go www,", "tokens": [2445, 11, 293, 976, 309, 257, 1315, 13, 407, 286, 445, 1890, 300, 293, 1219, 309, 343, 13, 407, 510, 286, 445, 352, 12520, 11], "temperature": 0.0, "avg_logprob": -0.2043589068130708, "compression_ratio": 1.5229885057471264, "no_speech_prob": 9.972830412152689e-06}, {"id": 592, "seek": 375128, "start": 3751.28, "end": 3757.96, "text": " passing in W1, how big is that going to be, W2, how big is that going to be, here's my", "tokens": [8437, 294, 343, 16, 11, 577, 955, 307, 300, 516, 281, 312, 11, 343, 17, 11, 577, 955, 307, 300, 516, 281, 312, 11, 510, 311, 452], "temperature": 0.0, "avg_logprob": -0.15775858438931978, "compression_ratio": 1.9662921348314606, "no_speech_prob": 7.296283911273349e-06}, {"id": 593, "seek": 375128, "start": 3757.96, "end": 3764.32, "text": " bias, how big is that going to be, here's my V, how big is that going to be. And then", "tokens": [12577, 11, 577, 955, 307, 300, 516, 281, 312, 11, 510, 311, 452, 691, 11, 577, 955, 307, 300, 516, 281, 312, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.15775858438931978, "compression_ratio": 1.9662921348314606, "no_speech_prob": 7.296283911273349e-06}, {"id": 594, "seek": 375128, "start": 3764.32, "end": 3770.6400000000003, "text": " I've got two other things here, which is W3 and B3. And that's because this is kind of", "tokens": [286, 600, 658, 732, 661, 721, 510, 11, 597, 307, 343, 18, 293, 363, 18, 13, 400, 300, 311, 570, 341, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.15775858438931978, "compression_ratio": 1.9662921348314606, "no_speech_prob": 7.296283911273349e-06}, {"id": 595, "seek": 375128, "start": 3770.6400000000003, "end": 3778.7200000000003, "text": " hand-waved over something, which is this final answer basically needs to become the hidden", "tokens": [1011, 12, 86, 12865, 670, 746, 11, 597, 307, 341, 2572, 1867, 1936, 2203, 281, 1813, 264, 7633], "temperature": 0.0, "avg_logprob": -0.15775858438931978, "compression_ratio": 1.9662921348314606, "no_speech_prob": 7.296283911273349e-06}, {"id": 596, "seek": 377872, "start": 3778.72, "end": 3785.3599999999997, "text": " state of your next layer of the RNN, but it might not be the right size. So then I just", "tokens": [1785, 295, 428, 958, 4583, 295, 264, 45702, 45, 11, 457, 309, 1062, 406, 312, 264, 558, 2744, 13, 407, 550, 286, 445], "temperature": 0.0, "avg_logprob": -0.18850300528786398, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.7778373148757964e-05}, {"id": 597, "seek": 377872, "start": 3785.3599999999997, "end": 3792.3599999999997, "text": " added one more transformation to the end to basically make it the right size.", "tokens": [3869, 472, 544, 9887, 281, 264, 917, 281, 1936, 652, 309, 264, 558, 2744, 13], "temperature": 0.0, "avg_logprob": -0.18850300528786398, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.7778373148757964e-05}, {"id": 598, "seek": 377872, "start": 3792.3599999999997, "end": 3799.12, "text": " And in some of the attention model papers they do lay that out. So I don't know whether", "tokens": [400, 294, 512, 295, 264, 3202, 2316, 10577, 436, 360, 2360, 300, 484, 13, 407, 286, 500, 380, 458, 1968], "temperature": 0.0, "avg_logprob": -0.18850300528786398, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.7778373148757964e-05}, {"id": 599, "seek": 377872, "start": 3799.12, "end": 3804.8399999999997, "text": " Hinton's group just ignored it, or maybe they built it so that the shapes worked anyway,", "tokens": [389, 12442, 311, 1594, 445, 19735, 309, 11, 420, 1310, 436, 3094, 309, 370, 300, 264, 10854, 2732, 4033, 11], "temperature": 0.0, "avg_logprob": -0.18850300528786398, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.7778373148757964e-05}, {"id": 600, "seek": 380484, "start": 3804.84, "end": 3812.1600000000003, "text": " I don't know. So we've got one extra affine transformation.", "tokens": [286, 500, 380, 458, 13, 407, 321, 600, 658, 472, 2857, 2096, 533, 9887, 13], "temperature": 0.0, "avg_logprob": -0.18238738275343372, "compression_ratio": 1.4930875576036866, "no_speech_prob": 7.527892194048036e-06}, {"id": 601, "seek": 380484, "start": 3812.1600000000003, "end": 3818.2000000000003, "text": " A minor note, for those of you playing with PyTorch, I'm sure you've discovered how cool", "tokens": [316, 6696, 3637, 11, 337, 729, 295, 291, 2433, 365, 9953, 51, 284, 339, 11, 286, 478, 988, 291, 600, 6941, 577, 1627], "temperature": 0.0, "avg_logprob": -0.18238738275343372, "compression_ratio": 1.4930875576036866, "no_speech_prob": 7.527892194048036e-06}, {"id": 602, "seek": 380484, "start": 3818.2000000000003, "end": 3823.56, "text": " it is that you can go self.addmodule and it keeps track of everything you need to train,", "tokens": [309, 307, 300, 291, 393, 352, 2698, 13, 25224, 8014, 2271, 293, 309, 5965, 2837, 295, 1203, 291, 643, 281, 3847, 11], "temperature": 0.0, "avg_logprob": -0.18238738275343372, "compression_ratio": 1.4930875576036866, "no_speech_prob": 7.527892194048036e-06}, {"id": 603, "seek": 380484, "start": 3823.56, "end": 3830.2400000000002, "text": " all the parameters. Keras is not so clever, so that's why we have to go self.addweight", "tokens": [439, 264, 9834, 13, 591, 6985, 307, 406, 370, 13494, 11, 370, 300, 311, 983, 321, 362, 281, 352, 2698, 13, 25224, 12329], "temperature": 0.0, "avg_logprob": -0.18238738275343372, "compression_ratio": 1.4930875576036866, "no_speech_prob": 7.527892194048036e-06}, {"id": 604, "seek": 383024, "start": 3830.24, "end": 3835.9199999999996, "text": " for every one of these weights. Because Keras has to know what to train. When you optimize,", "tokens": [337, 633, 472, 295, 613, 17443, 13, 1436, 591, 6985, 575, 281, 458, 437, 281, 3847, 13, 1133, 291, 19719, 11], "temperature": 0.0, "avg_logprob": -0.14433229346024362, "compression_ratio": 1.5850622406639003, "no_speech_prob": 4.7108810576901305e-06}, {"id": 605, "seek": 383024, "start": 3835.9199999999996, "end": 3837.3599999999997, "text": " what should you train?", "tokens": [437, 820, 291, 3847, 30], "temperature": 0.0, "avg_logprob": -0.14433229346024362, "compression_ratio": 1.5850622406639003, "no_speech_prob": 4.7108810576901305e-06}, {"id": 606, "seek": 383024, "start": 3837.3599999999997, "end": 3843.16, "text": " Unfortunately when we created all these RNNs, these RNNs have lots of weights which we haven't", "tokens": [8590, 562, 321, 2942, 439, 613, 45702, 45, 82, 11, 613, 45702, 45, 82, 362, 3195, 295, 17443, 597, 321, 2378, 380], "temperature": 0.0, "avg_logprob": -0.14433229346024362, "compression_ratio": 1.5850622406639003, "no_speech_prob": 4.7108810576901305e-06}, {"id": 607, "seek": 383024, "start": 3843.16, "end": 3851.3599999999997, "text": " told Keras that we need to optimize them. So I have not found any examples of custom", "tokens": [1907, 591, 6985, 300, 321, 643, 281, 19719, 552, 13, 407, 286, 362, 406, 1352, 604, 5110, 295, 2375], "temperature": 0.0, "avg_logprob": -0.14433229346024362, "compression_ratio": 1.5850622406639003, "no_speech_prob": 4.7108810576901305e-06}, {"id": 608, "seek": 383024, "start": 3851.3599999999997, "end": 3857.52, "text": " layers anywhere on the Internet where people have actually done this. But I figured out", "tokens": [7914, 4992, 322, 264, 7703, 689, 561, 362, 767, 1096, 341, 13, 583, 286, 8932, 484], "temperature": 0.0, "avg_logprob": -0.14433229346024362, "compression_ratio": 1.5850622406639003, "no_speech_prob": 4.7108810576901305e-06}, {"id": 609, "seek": 385752, "start": 3857.52, "end": 3864.56, "text": " what you can do is basically create this little function that goes through every one of my", "tokens": [437, 291, 393, 360, 307, 1936, 1884, 341, 707, 2445, 300, 1709, 807, 633, 472, 295, 452], "temperature": 0.0, "avg_logprob": -0.1753029871468592, "compression_ratio": 1.748878923766816, "no_speech_prob": 7.4111708272539545e-06}, {"id": 610, "seek": 385752, "start": 3864.56, "end": 3871.56, "text": " layers, every one of those RNN layers, and finds all of the attributes that Keras needs", "tokens": [7914, 11, 633, 472, 295, 729, 45702, 45, 7914, 11, 293, 10704, 439, 295, 264, 17212, 300, 591, 6985, 2203], "temperature": 0.0, "avg_logprob": -0.1753029871468592, "compression_ratio": 1.748878923766816, "no_speech_prob": 7.4111708272539545e-06}, {"id": 611, "seek": 385752, "start": 3871.56, "end": 3875.44, "text": " to know about. So it needs to know what are the trainable weights, non-trainable weights,", "tokens": [281, 458, 466, 13, 407, 309, 2203, 281, 458, 437, 366, 264, 3847, 712, 17443, 11, 2107, 12, 83, 7146, 712, 17443, 11], "temperature": 0.0, "avg_logprob": -0.1753029871468592, "compression_ratio": 1.748878923766816, "no_speech_prob": 7.4111708272539545e-06}, {"id": 612, "seek": 385752, "start": 3875.44, "end": 3877.44, "text": " losses, updates, and constraints.", "tokens": [15352, 11, 9205, 11, 293, 18491, 13], "temperature": 0.0, "avg_logprob": -0.1753029871468592, "compression_ratio": 1.748878923766816, "no_speech_prob": 7.4111708272539545e-06}, {"id": 613, "seek": 385752, "start": 3877.44, "end": 3881.28, "text": " So anyway, you can copy and paste this code. If you want to create a custom layer which", "tokens": [407, 4033, 11, 291, 393, 5055, 293, 9163, 341, 3089, 13, 759, 291, 528, 281, 1884, 257, 2375, 4583, 597], "temperature": 0.0, "avg_logprob": -0.1753029871468592, "compression_ratio": 1.748878923766816, "no_speech_prob": 7.4111708272539545e-06}, {"id": 614, "seek": 388128, "start": 3881.28, "end": 3890.4, "text": " itself contains some Keras layers, copy and paste this code. It seems to work.", "tokens": [2564, 8306, 512, 591, 6985, 7914, 11, 5055, 293, 9163, 341, 3089, 13, 467, 2544, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.11477800338499007, "compression_ratio": 1.5174418604651163, "no_speech_prob": 5.422180947789457e-06}, {"id": 615, "seek": 388128, "start": 3890.4, "end": 3897.32, "text": " So the first main thing you have to create when you create a custom layer is build. Creating", "tokens": [407, 264, 700, 2135, 551, 291, 362, 281, 1884, 562, 291, 1884, 257, 2375, 4583, 307, 1322, 13, 40002], "temperature": 0.0, "avg_logprob": -0.11477800338499007, "compression_ratio": 1.5174418604651163, "no_speech_prob": 5.422180947789457e-06}, {"id": 616, "seek": 388128, "start": 3897.32, "end": 3903.0400000000004, "text": " build is really boring because you have to get all these bloody dimensions to be correct,", "tokens": [1322, 307, 534, 9989, 570, 291, 362, 281, 483, 439, 613, 18938, 12819, 281, 312, 3006, 11], "temperature": 0.0, "avg_logprob": -0.11477800338499007, "compression_ratio": 1.5174418604651163, "no_speech_prob": 5.422180947789457e-06}, {"id": 617, "seek": 390304, "start": 3903.04, "end": 3912.88, "text": " which is a pain in the ass. So when I built that, I had this little bunch of TensorFlow", "tokens": [597, 307, 257, 1822, 294, 264, 1256, 13, 407, 562, 286, 3094, 300, 11, 286, 632, 341, 707, 3840, 295, 37624], "temperature": 0.0, "avg_logprob": -0.15486958686341631, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.045122462528525e-05}, {"id": 618, "seek": 390304, "start": 3912.88, "end": 3917.12, "text": " playing around code where I would just keep going in and being like, okay, if this is", "tokens": [2433, 926, 3089, 689, 286, 576, 445, 1066, 516, 294, 293, 885, 411, 11, 1392, 11, 498, 341, 307], "temperature": 0.0, "avg_logprob": -0.15486958686341631, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.045122462528525e-05}, {"id": 619, "seek": 390304, "start": 3917.12, "end": 3922.48, "text": " the size of my X, this is the size of my W1, let's try doing each calculation. So I did", "tokens": [264, 2744, 295, 452, 1783, 11, 341, 307, 264, 2744, 295, 452, 343, 16, 11, 718, 311, 853, 884, 1184, 17108, 13, 407, 286, 630], "temperature": 0.0, "avg_logprob": -0.15486958686341631, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.045122462528525e-05}, {"id": 620, "seek": 390304, "start": 3922.48, "end": 3928.24, "text": " each calculation in a cell one step at a time and looked to see what all the shapes were", "tokens": [1184, 17108, 294, 257, 2815, 472, 1823, 412, 257, 565, 293, 2956, 281, 536, 437, 439, 264, 10854, 645], "temperature": 0.0, "avg_logprob": -0.15486958686341631, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.045122462528525e-05}, {"id": 621, "seek": 392824, "start": 3928.24, "end": 3934.8799999999997, "text": " and then went back and put them in. So I don't think any normal mortal person can write all", "tokens": [293, 550, 1437, 646, 293, 829, 552, 294, 13, 407, 286, 500, 380, 519, 604, 2710, 27624, 954, 393, 2464, 439], "temperature": 0.0, "avg_logprob": -0.17042224014861673, "compression_ratio": 1.59, "no_speech_prob": 8.939616236602888e-06}, {"id": 622, "seek": 392824, "start": 3934.8799999999997, "end": 3938.7999999999997, "text": " of these dimensionalities and actually get them to work without doing them one little", "tokens": [295, 613, 18795, 1088, 293, 767, 483, 552, 281, 589, 1553, 884, 552, 472, 707], "temperature": 0.0, "avg_logprob": -0.17042224014861673, "compression_ratio": 1.59, "no_speech_prob": 8.939616236602888e-06}, {"id": 623, "seek": 392824, "start": 3938.7999999999997, "end": 3944.3199999999997, "text": " step at a time. So make things easy for yourself.", "tokens": [1823, 412, 257, 565, 13, 407, 652, 721, 1858, 337, 1803, 13], "temperature": 0.0, "avg_logprob": -0.17042224014861673, "compression_ratio": 1.59, "no_speech_prob": 8.939616236602888e-06}, {"id": 624, "seek": 392824, "start": 3944.3199999999997, "end": 3951.9599999999996, "text": " So the other key thing that happens in a layer is to call pull. And call is the thing that", "tokens": [407, 264, 661, 2141, 551, 300, 2314, 294, 257, 4583, 307, 281, 818, 2235, 13, 400, 818, 307, 264, 551, 300], "temperature": 0.0, "avg_logprob": -0.17042224014861673, "compression_ratio": 1.59, "no_speech_prob": 8.939616236602888e-06}, {"id": 625, "seek": 395196, "start": 3951.96, "end": 3960.2400000000002, "text": " basically that's your forward pass. This is calculate, go calculate. So you get passed", "tokens": [1936, 300, 311, 428, 2128, 1320, 13, 639, 307, 8873, 11, 352, 8873, 13, 407, 291, 483, 4678], "temperature": 0.0, "avg_logprob": -0.2096594897183505, "compression_ratio": 1.4369747899159664, "no_speech_prob": 6.854274033685215e-06}, {"id": 626, "seek": 395196, "start": 3960.2400000000002, "end": 3973.76, "text": " in the data, the calculate on. So in this case, we actually have to step through the", "tokens": [294, 264, 1412, 11, 264, 8873, 322, 13, 407, 294, 341, 1389, 11, 321, 767, 362, 281, 1823, 807, 264], "temperature": 0.0, "avg_logprob": -0.2096594897183505, "compression_ratio": 1.4369747899159664, "no_speech_prob": 6.854274033685215e-06}, {"id": 627, "seek": 397376, "start": 3973.76, "end": 3983.6000000000004, "text": " steps of an RNN. And Keras actually has a k.rnn function which is very, very similar", "tokens": [4439, 295, 364, 45702, 45, 13, 400, 591, 6985, 767, 575, 257, 350, 13, 81, 26384, 2445, 597, 307, 588, 11, 588, 2531], "temperature": 0.0, "avg_logprob": -0.1686007355990475, "compression_ratio": 1.5, "no_speech_prob": 7.527935849793721e-06}, {"id": 628, "seek": 397376, "start": 3983.6000000000004, "end": 3990.8, "text": " to theano.scan. You guys remember theano.scan? This is basically something which calls some", "tokens": [281, 264, 3730, 13, 4417, 282, 13, 509, 1074, 1604, 264, 3730, 13, 4417, 282, 30, 639, 307, 1936, 746, 597, 5498, 512], "temperature": 0.0, "avg_logprob": -0.1686007355990475, "compression_ratio": 1.5, "no_speech_prob": 7.527935849793721e-06}, {"id": 629, "seek": 397376, "start": 3990.8, "end": 3999.36, "text": " function for each step, it steps through each part of your input, has some initial states,", "tokens": [2445, 337, 1184, 1823, 11, 309, 4439, 807, 1184, 644, 295, 428, 4846, 11, 575, 512, 5883, 4368, 11], "temperature": 0.0, "avg_logprob": -0.1686007355990475, "compression_ratio": 1.5, "no_speech_prob": 7.527935849793721e-06}, {"id": 630, "seek": 399936, "start": 3999.36, "end": 4006.2400000000002, "text": " and so forth. It's almost identical to theano.scan. It's a really low-level thing.", "tokens": [293, 370, 5220, 13, 467, 311, 1920, 14800, 281, 264, 3730, 13, 4417, 282, 13, 467, 311, 257, 534, 2295, 12, 12418, 551, 13], "temperature": 0.0, "avg_logprob": -0.12997113412885525, "compression_ratio": 1.4457831325301205, "no_speech_prob": 8.801055628282484e-06}, {"id": 631, "seek": 399936, "start": 4006.2400000000002, "end": 4019.6, "text": " And so Keras really doesn't have a convenient user-facing API for creating custom RNN code.", "tokens": [400, 370, 591, 6985, 534, 1177, 380, 362, 257, 10851, 4195, 12, 44046, 9362, 337, 4084, 2375, 45702, 45, 3089, 13], "temperature": 0.0, "avg_logprob": -0.12997113412885525, "compression_ratio": 1.4457831325301205, "no_speech_prob": 8.801055628282484e-06}, {"id": 632, "seek": 399936, "start": 4019.6, "end": 4025.7200000000003, "text": " In fact, this is something which nobody's really figured out yet.", "tokens": [682, 1186, 11, 341, 307, 746, 597, 5079, 311, 534, 8932, 484, 1939, 13], "temperature": 0.0, "avg_logprob": -0.12997113412885525, "compression_ratio": 1.4457831325301205, "no_speech_prob": 8.801055628282484e-06}, {"id": 633, "seek": 402572, "start": 4025.72, "end": 4035.08, "text": " TensorFlow has just released a new kind of custom RNN API, but there isn't any documentation", "tokens": [37624, 575, 445, 4736, 257, 777, 733, 295, 2375, 45702, 45, 9362, 11, 457, 456, 1943, 380, 604, 14333], "temperature": 0.0, "avg_logprob": -0.17874268123081752, "compression_ratio": 1.4523809523809523, "no_speech_prob": 4.49515300715575e-06}, {"id": 634, "seek": 402572, "start": 4035.08, "end": 4041.08, "text": " for it. So I was hoping to teach it in this course, but I think it's just a little too", "tokens": [337, 309, 13, 407, 286, 390, 7159, 281, 2924, 309, 294, 341, 1164, 11, 457, 286, 519, 309, 311, 445, 257, 707, 886], "temperature": 0.0, "avg_logprob": -0.17874268123081752, "compression_ratio": 1.4523809523809523, "no_speech_prob": 4.49515300715575e-06}, {"id": 635, "seek": 402572, "start": 4041.08, "end": 4045.16, "text": " early. I don't know if it's good or not.", "tokens": [2440, 13, 286, 500, 380, 458, 498, 309, 311, 665, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.17874268123081752, "compression_ratio": 1.4523809523809523, "no_speech_prob": 4.49515300715575e-06}, {"id": 636, "seek": 402572, "start": 4045.16, "end": 4051.16, "text": " This is a bit of an open question really. How do you create something as easy to use", "tokens": [639, 307, 257, 857, 295, 364, 1269, 1168, 534, 13, 1012, 360, 291, 1884, 746, 382, 1858, 281, 764], "temperature": 0.0, "avg_logprob": -0.17874268123081752, "compression_ratio": 1.4523809523809523, "no_speech_prob": 4.49515300715575e-06}, {"id": 637, "seek": 405116, "start": 4051.16, "end": 4059.04, "text": " as Keras, but with the flexibility to design your own RNN details? As you can see, in this", "tokens": [382, 591, 6985, 11, 457, 365, 264, 12635, 281, 1715, 428, 1065, 45702, 45, 4365, 30, 1018, 291, 393, 536, 11, 294, 341], "temperature": 0.0, "avg_logprob": -0.13526596167148688, "compression_ratio": 1.5376884422110553, "no_speech_prob": 9.818249054660555e-06}, {"id": 638, "seek": 405116, "start": 4059.04, "end": 4067.56, "text": " case it wasn't convenient at all. I had to go back and run this scan function from scratch", "tokens": [1389, 309, 2067, 380, 10851, 412, 439, 13, 286, 632, 281, 352, 646, 293, 1190, 341, 11049, 2445, 490, 8459], "temperature": 0.0, "avg_logprob": -0.13526596167148688, "compression_ratio": 1.5376884422110553, "no_speech_prob": 9.818249054660555e-06}, {"id": 639, "seek": 405116, "start": 4067.56, "end": 4070.56, "text": " and set everything up from scratch.", "tokens": [293, 992, 1203, 493, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.13526596167148688, "compression_ratio": 1.5376884422110553, "no_speech_prob": 9.818249054660555e-06}, {"id": 640, "seek": 405116, "start": 4070.56, "end": 4075.08, "text": " So basically all the work happens in here, which means all the work is happening in this", "tokens": [407, 1936, 439, 264, 589, 2314, 294, 510, 11, 597, 1355, 439, 264, 589, 307, 2737, 294, 341], "temperature": 0.0, "avg_logprob": -0.13526596167148688, "compression_ratio": 1.5376884422110553, "no_speech_prob": 9.818249054660555e-06}, {"id": 641, "seek": 407508, "start": 4075.08, "end": 4082.2799999999997, "text": " step function I created. So here's where the actual calculations are done. So in the end,", "tokens": [1823, 2445, 286, 2942, 13, 407, 510, 311, 689, 264, 3539, 20448, 366, 1096, 13, 407, 294, 264, 917, 11], "temperature": 0.0, "avg_logprob": -0.2110630241600243, "compression_ratio": 1.6024096385542168, "no_speech_prob": 9.223391316481866e-06}, {"id": 642, "seek": 407508, "start": 4082.2799999999997, "end": 4091.52, "text": " when it actually gets to it, this now looks a lot like the Hinton team's code. We basically", "tokens": [562, 309, 767, 2170, 281, 309, 11, 341, 586, 1542, 257, 688, 411, 264, 389, 12442, 1469, 311, 3089, 13, 492, 1936], "temperature": 0.0, "avg_logprob": -0.2110630241600243, "compression_ratio": 1.6024096385542168, "no_speech_prob": 9.223391316481866e-06}, {"id": 643, "seek": 407508, "start": 4091.52, "end": 4099.84, "text": " can see us doing the dot at the bias, and here's the than, and here's the v, and the", "tokens": [393, 536, 505, 884, 264, 5893, 412, 264, 12577, 11, 293, 510, 311, 264, 813, 11, 293, 510, 311, 264, 371, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.2110630241600243, "compression_ratio": 1.6024096385542168, "no_speech_prob": 9.223391316481866e-06}, {"id": 644, "seek": 409984, "start": 4099.84, "end": 4106.12, "text": " softmax, and here's the bit where we do the weighted sum. And then this is this one extra", "tokens": [2787, 41167, 11, 293, 510, 311, 264, 857, 689, 321, 360, 264, 32807, 2408, 13, 400, 550, 341, 307, 341, 472, 2857], "temperature": 0.0, "avg_logprob": -0.1388192127660378, "compression_ratio": 1.627906976744186, "no_speech_prob": 7.183198249549605e-06}, {"id": 645, "seek": 409984, "start": 4106.12, "end": 4110.72, "text": " step I mentioned, which is to get it to the right shape for the RNN again.", "tokens": [1823, 286, 2835, 11, 597, 307, 281, 483, 309, 281, 264, 558, 3909, 337, 264, 45702, 45, 797, 13], "temperature": 0.0, "avg_logprob": -0.1388192127660378, "compression_ratio": 1.627906976744186, "no_speech_prob": 7.183198249549605e-06}, {"id": 646, "seek": 409984, "start": 4110.72, "end": 4117.92, "text": " And so now that I've done all of that preprocessing, I've started with X and I ended up creating", "tokens": [400, 370, 586, 300, 286, 600, 1096, 439, 295, 300, 2666, 340, 780, 278, 11, 286, 600, 1409, 365, 1783, 293, 286, 4590, 493, 4084], "temperature": 0.0, "avg_logprob": -0.1388192127660378, "compression_ratio": 1.627906976744186, "no_speech_prob": 7.183198249549605e-06}, {"id": 647, "seek": 409984, "start": 4117.92, "end": 4124.22, "text": " this thing called H, you can now go ahead and go through all of those three RNNs calling", "tokens": [341, 551, 1219, 389, 11, 291, 393, 586, 352, 2286, 293, 352, 807, 439, 295, 729, 1045, 45702, 45, 82, 5141], "temperature": 0.0, "avg_logprob": -0.1388192127660378, "compression_ratio": 1.627906976744186, "no_speech_prob": 7.183198249549605e-06}, {"id": 648, "seek": 412422, "start": 4124.22, "end": 4133.96, "text": " one step on each of those. This probably shouldn't have been that hard in the end, but it's just", "tokens": [472, 1823, 322, 1184, 295, 729, 13, 639, 1391, 4659, 380, 362, 668, 300, 1152, 294, 264, 917, 11, 457, 309, 311, 445], "temperature": 0.0, "avg_logprob": -0.17872556433620224, "compression_ratio": 1.5181347150259068, "no_speech_prob": 6.1441164689313155e-06}, {"id": 649, "seek": 412422, "start": 4133.96, "end": 4138.26, "text": " the nature of the Keras API for this stuff that doesn't really exist that we had to go", "tokens": [264, 3687, 295, 264, 591, 6985, 9362, 337, 341, 1507, 300, 1177, 380, 534, 2514, 300, 321, 632, 281, 352], "temperature": 0.0, "avg_logprob": -0.17872556433620224, "compression_ratio": 1.5181347150259068, "no_speech_prob": 6.1441164689313155e-06}, {"id": 650, "seek": 412422, "start": 4138.26, "end": 4139.900000000001, "text": " in and create it.", "tokens": [294, 293, 1884, 309, 13], "temperature": 0.0, "avg_logprob": -0.17872556433620224, "compression_ratio": 1.5181347150259068, "no_speech_prob": 6.1441164689313155e-06}, {"id": 651, "seek": 412422, "start": 4139.900000000001, "end": 4148.72, "text": " All we really wanted to do was say, okay Keras, before you run the three decoder RNNs, take", "tokens": [1057, 321, 534, 1415, 281, 360, 390, 584, 11, 1392, 591, 6985, 11, 949, 291, 1190, 264, 1045, 979, 19866, 45702, 45, 82, 11, 747], "temperature": 0.0, "avg_logprob": -0.17872556433620224, "compression_ratio": 1.5181347150259068, "no_speech_prob": 6.1441164689313155e-06}, {"id": 652, "seek": 414872, "start": 4148.72, "end": 4160.84, "text": " your input and modify it this way. But we have to do it for every step. That's basically", "tokens": [428, 4846, 293, 16927, 309, 341, 636, 13, 583, 321, 362, 281, 360, 309, 337, 633, 1823, 13, 663, 311, 1936], "temperature": 0.0, "avg_logprob": -0.19420435163709854, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.9033760736419936e-06}, {"id": 653, "seek": 414872, "start": 4160.84, "end": 4166.56, "text": " what was missing, some way in Keras to easily say, change the step.", "tokens": [437, 390, 5361, 11, 512, 636, 294, 591, 6985, 281, 3612, 584, 11, 1319, 264, 1823, 13], "temperature": 0.0, "avg_logprob": -0.19420435163709854, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.9033760736419936e-06}, {"id": 654, "seek": 414872, "start": 4166.56, "end": 4172.84, "text": " I've spoken to Franz Weill, the Keras author, about this. He's well aware that this is not", "tokens": [286, 600, 10759, 281, 33084, 492, 373, 11, 264, 591, 6985, 3793, 11, 466, 341, 13, 634, 311, 731, 3650, 300, 341, 307, 406], "temperature": 0.0, "avg_logprob": -0.19420435163709854, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.9033760736419936e-06}, {"id": 655, "seek": 414872, "start": 4172.84, "end": 4178.360000000001, "text": " convenient right now and he really wants to fix it, but this is difficult to get right", "tokens": [10851, 558, 586, 293, 415, 534, 2738, 281, 3191, 309, 11, 457, 341, 307, 2252, 281, 483, 558], "temperature": 0.0, "avg_logprob": -0.19420435163709854, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.9033760736419936e-06}, {"id": 656, "seek": 417836, "start": 4178.36, "end": 4182.0, "text": " now and no one's quite done it yet.", "tokens": [586, 293, 572, 472, 311, 1596, 1096, 309, 1939, 13], "temperature": 0.0, "avg_logprob": -0.2393788755609748, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.8924736650660634e-05}, {"id": 657, "seek": 417836, "start": 4182.0, "end": 4186.92, "text": " We have a question. I didn't get the getting it to the right shape again part. Could you", "tokens": [492, 362, 257, 1168, 13, 286, 994, 380, 483, 264, 1242, 309, 281, 264, 558, 3909, 797, 644, 13, 7497, 291], "temperature": 0.0, "avg_logprob": -0.2393788755609748, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.8924736650660634e-05}, {"id": 658, "seek": 417836, "start": 4186.92, "end": 4187.92, "text": " explain that again?", "tokens": [2903, 300, 797, 30], "temperature": 0.0, "avg_logprob": -0.2393788755609748, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.8924736650660634e-05}, {"id": 659, "seek": 417836, "start": 4187.92, "end": 4200.48, "text": " Sure. So step is being called for every time step. So at the end of it, we have to return", "tokens": [4894, 13, 407, 1823, 307, 885, 1219, 337, 633, 565, 1823, 13, 407, 412, 264, 917, 295, 309, 11, 321, 362, 281, 2736], "temperature": 0.0, "avg_logprob": -0.2393788755609748, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.8924736650660634e-05}, {"id": 660, "seek": 417836, "start": 4200.48, "end": 4205.48, "text": " the new hidden state. And then that new hidden state is going to come back into the next", "tokens": [264, 777, 7633, 1785, 13, 400, 550, 300, 777, 7633, 1785, 307, 516, 281, 808, 646, 666, 264, 958], "temperature": 0.0, "avg_logprob": -0.2393788755609748, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.8924736650660634e-05}, {"id": 661, "seek": 420548, "start": 4205.48, "end": 4212.5199999999995, "text": " step. So in other words, the thing that we end up with here needs to be the same shape", "tokens": [1823, 13, 407, 294, 661, 2283, 11, 264, 551, 300, 321, 917, 493, 365, 510, 2203, 281, 312, 264, 912, 3909], "temperature": 0.0, "avg_logprob": -0.08060933748881022, "compression_ratio": 1.3834586466165413, "no_speech_prob": 4.495119355851784e-06}, {"id": 662, "seek": 420548, "start": 4212.5199999999995, "end": 4231.36, "text": " as we had here. But that doesn't happen automatically because we're actually taking this weighted", "tokens": [382, 321, 632, 510, 13, 583, 300, 1177, 380, 1051, 6772, 570, 321, 434, 767, 1940, 341, 32807], "temperature": 0.0, "avg_logprob": -0.08060933748881022, "compression_ratio": 1.3834586466165413, "no_speech_prob": 4.495119355851784e-06}, {"id": 663, "seek": 423136, "start": 4231.36, "end": 4239.16, "text": " sum and we're actually concatenating onto it the decoder input, we end up with something", "tokens": [2408, 293, 321, 434, 767, 1588, 7186, 990, 3911, 309, 264, 979, 19866, 4846, 11, 321, 917, 493, 365, 746], "temperature": 0.0, "avg_logprob": -0.16160016377766928, "compression_ratio": 1.5538461538461539, "no_speech_prob": 6.962191946513485e-06}, {"id": 664, "seek": 423136, "start": 4239.16, "end": 4243.96, "text": " that's a totally different shape.", "tokens": [300, 311, 257, 3879, 819, 3909, 13], "temperature": 0.0, "avg_logprob": -0.16160016377766928, "compression_ratio": 1.5538461538461539, "no_speech_prob": 6.962191946513485e-06}, {"id": 665, "seek": 423136, "start": 4243.96, "end": 4251.0, "text": " But you can always change the shape of state by multiplying it by a matrix where the thing", "tokens": [583, 291, 393, 1009, 1319, 264, 3909, 295, 1785, 538, 30955, 309, 538, 257, 8141, 689, 264, 551], "temperature": 0.0, "avg_logprob": -0.16160016377766928, "compression_ratio": 1.5538461538461539, "no_speech_prob": 6.962191946513485e-06}, {"id": 666, "seek": 423136, "start": 4251.0, "end": 4256.599999999999, "text": " you multiply it by has the number of columns that you want to create. So I just made sure", "tokens": [291, 12972, 309, 538, 575, 264, 1230, 295, 13766, 300, 291, 528, 281, 1884, 13, 407, 286, 445, 1027, 988], "temperature": 0.0, "avg_logprob": -0.16160016377766928, "compression_ratio": 1.5538461538461539, "no_speech_prob": 6.962191946513485e-06}, {"id": 667, "seek": 425660, "start": 4256.6, "end": 4264.200000000001, "text": " that W3 has the same number of columns as the H that we want had. So as a result, by", "tokens": [300, 343, 18, 575, 264, 912, 1230, 295, 13766, 382, 264, 389, 300, 321, 528, 632, 13, 407, 382, 257, 1874, 11, 538], "temperature": 0.0, "avg_logprob": -0.2032416315362005, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.144057806523051e-06}, {"id": 668, "seek": 425660, "start": 4264.200000000001, "end": 4270.200000000001, "text": " the end of this, we've got something which we can feed back into the RNN again. We just", "tokens": [264, 917, 295, 341, 11, 321, 600, 658, 746, 597, 321, 393, 3154, 646, 666, 264, 45702, 45, 797, 13, 492, 445], "temperature": 0.0, "avg_logprob": -0.2032416315362005, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.144057806523051e-06}, {"id": 669, "seek": 425660, "start": 4270.200000000001, "end": 4271.200000000001, "text": " need to make sure.", "tokens": [643, 281, 652, 988, 13], "temperature": 0.0, "avg_logprob": -0.2032416315362005, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.144057806523051e-06}, {"id": 670, "seek": 425660, "start": 4271.200000000001, "end": 4275.72, "text": " An RNN step can't change the shape that it started with, because it needs to be able", "tokens": [1107, 45702, 45, 1823, 393, 380, 1319, 264, 3909, 300, 309, 1409, 365, 11, 570, 309, 2203, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.2032416315362005, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.144057806523051e-06}, {"id": 671, "seek": 425660, "start": 4275.72, "end": 4281.08, "text": " to keep going step, step, step. Every step needs to be the same tensor size.", "tokens": [281, 1066, 516, 1823, 11, 1823, 11, 1823, 13, 2048, 1823, 2203, 281, 312, 264, 912, 40863, 2744, 13], "temperature": 0.0, "avg_logprob": -0.2032416315362005, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.144057806523051e-06}, {"id": 672, "seek": 428108, "start": 4281.08, "end": 4287.8, "text": " Question. You showed how you figured out, tested the tensor shapes. How did you debug", "tokens": [14464, 13, 509, 4712, 577, 291, 8932, 484, 11, 8246, 264, 40863, 10854, 13, 1012, 630, 291, 24083], "temperature": 0.0, "avg_logprob": -0.22183154059238122, "compression_ratio": 1.4935064935064934, "no_speech_prob": 1.2606597920239437e-05}, {"id": 673, "seek": 428108, "start": 4287.8, "end": 4295.24, "text": " the attention class itself as a whole? Are RNNs easier and cleaner in PyTorch? Would", "tokens": [264, 3202, 1508, 2564, 382, 257, 1379, 30, 2014, 45702, 45, 82, 3571, 293, 16532, 294, 9953, 51, 284, 339, 30, 6068], "temperature": 0.0, "avg_logprob": -0.22183154059238122, "compression_ratio": 1.4935064935064934, "no_speech_prob": 1.2606597920239437e-05}, {"id": 674, "seek": 428108, "start": 4295.24, "end": 4298.8, "text": " the attention class have been relatively easier in PyTorch?", "tokens": [264, 3202, 1508, 362, 668, 7226, 3571, 294, 9953, 51, 284, 339, 30], "temperature": 0.0, "avg_logprob": -0.22183154059238122, "compression_ratio": 1.4935064935064934, "no_speech_prob": 1.2606597920239437e-05}, {"id": 675, "seek": 429880, "start": 4298.8, "end": 4316.28, "text": " The main thing for me to debug was build. This bit is Jeffrey Hinton's team's equations.", "tokens": [440, 2135, 551, 337, 385, 281, 24083, 390, 1322, 13, 639, 857, 307, 28721, 389, 12442, 311, 1469, 311, 11787, 13], "temperature": 0.0, "avg_logprob": -0.2134253370995615, "compression_ratio": 1.3208955223880596, "no_speech_prob": 7.296183866856154e-06}, {"id": 676, "seek": 429880, "start": 4316.28, "end": 4328.24, "text": " There's not too much to get wrong here. So really it was a case of doing it step-by-step", "tokens": [821, 311, 406, 886, 709, 281, 483, 2085, 510, 13, 407, 534, 309, 390, 257, 1389, 295, 884, 309, 1823, 12, 2322, 12, 16792], "temperature": 0.0, "avg_logprob": -0.2134253370995615, "compression_ratio": 1.3208955223880596, "no_speech_prob": 7.296183866856154e-06}, {"id": 677, "seek": 432824, "start": 4328.24, "end": 4338.44, "text": " in cells and printing out the shape at each point. You'll see that I created all of these", "tokens": [294, 5438, 293, 14699, 484, 264, 3909, 412, 1184, 935, 13, 509, 603, 536, 300, 286, 2942, 439, 295, 613], "temperature": 0.0, "avg_logprob": -0.1806457068330498, "compression_ratio": 1.7185929648241205, "no_speech_prob": 1.2606706150108948e-05}, {"id": 678, "seek": 432824, "start": 4338.44, "end": 4343.639999999999, "text": " different dimensions at the start. I made sure all of these numbers are different so", "tokens": [819, 12819, 412, 264, 722, 13, 286, 1027, 988, 439, 295, 613, 3547, 366, 819, 370], "temperature": 0.0, "avg_logprob": -0.1806457068330498, "compression_ratio": 1.7185929648241205, "no_speech_prob": 1.2606706150108948e-05}, {"id": 679, "seek": 432824, "start": 4343.639999999999, "end": 4347.08, "text": " that every time I saw the number 64, I knew that was the batch size. Every time I saw", "tokens": [300, 633, 565, 286, 1866, 264, 1230, 12145, 11, 286, 2586, 300, 390, 264, 15245, 2744, 13, 2048, 565, 286, 1866], "temperature": 0.0, "avg_logprob": -0.1806457068330498, "compression_ratio": 1.7185929648241205, "no_speech_prob": 1.2606706150108948e-05}, {"id": 680, "seek": 432824, "start": 4347.08, "end": 4353.34, "text": " 4, I knew that was the time steps. 32 was the input DIMM, 48 was the output DIMM.", "tokens": [1017, 11, 286, 2586, 300, 390, 264, 565, 4439, 13, 8858, 390, 264, 4846, 413, 6324, 44, 11, 11174, 390, 264, 5598, 413, 6324, 44, 13], "temperature": 0.0, "avg_logprob": -0.1806457068330498, "compression_ratio": 1.7185929648241205, "no_speech_prob": 1.2606706150108948e-05}, {"id": 681, "seek": 435334, "start": 4353.34, "end": 4361.64, "text": " So I could go through and I could see, okay, here's my shape here, here's my shape here.", "tokens": [407, 286, 727, 352, 807, 293, 286, 727, 536, 11, 1392, 11, 510, 311, 452, 3909, 510, 11, 510, 311, 452, 3909, 510, 13], "temperature": 0.0, "avg_logprob": -0.203338623046875, "compression_ratio": 1.6556603773584906, "no_speech_prob": 4.22278617406846e-06}, {"id": 682, "seek": 435334, "start": 4361.64, "end": 4368.64, "text": " And then all I just did was chuck in some print statements, print HW2, print U, print", "tokens": [400, 550, 439, 286, 445, 630, 390, 20870, 294, 512, 4482, 12363, 11, 4482, 389, 54, 17, 11, 4482, 624, 11, 4482], "temperature": 0.0, "avg_logprob": -0.203338623046875, "compression_ratio": 1.6556603773584906, "no_speech_prob": 4.22278617406846e-06}, {"id": 683, "seek": 435334, "start": 4368.64, "end": 4374.74, "text": " A, and so forth. I didn't need to see the contents of them. When you print a TensorFlow", "tokens": [316, 11, 293, 370, 5220, 13, 286, 994, 380, 643, 281, 536, 264, 15768, 295, 552, 13, 1133, 291, 4482, 257, 37624], "temperature": 0.0, "avg_logprob": -0.203338623046875, "compression_ratio": 1.6556603773584906, "no_speech_prob": 4.22278617406846e-06}, {"id": 684, "seek": 435334, "start": 4374.74, "end": 4381.02, "text": " tensor like that, it prints the dimensions of it. In my experience, once your dimensions", "tokens": [40863, 411, 300, 11, 309, 22305, 264, 12819, 295, 309, 13, 682, 452, 1752, 11, 1564, 428, 12819], "temperature": 0.0, "avg_logprob": -0.203338623046875, "compression_ratio": 1.6556603773584906, "no_speech_prob": 4.22278617406846e-06}, {"id": 685, "seek": 438102, "start": 4381.02, "end": 4386.6, "text": " work, you're done. As long as you didn't make the mistake of having multiple things having", "tokens": [589, 11, 291, 434, 1096, 13, 1018, 938, 382, 291, 994, 380, 652, 264, 6146, 295, 1419, 3866, 721, 1419], "temperature": 0.0, "avg_logprob": -0.1915613810221354, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.664458619023208e-06}, {"id": 686, "seek": 438102, "start": 4386.6, "end": 4399.360000000001, "text": " the same number of dimensions. As long as your dot product fits. If H and W2 didn't", "tokens": [264, 912, 1230, 295, 12819, 13, 1018, 938, 382, 428, 5893, 1674, 9001, 13, 759, 389, 293, 343, 17, 994, 380], "temperature": 0.0, "avg_logprob": -0.1915613810221354, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.664458619023208e-06}, {"id": 687, "seek": 438102, "start": 4399.360000000001, "end": 4404.240000000001, "text": " line up for rows and columns, the error you get is pretty clear. It'll be a TensorFlow", "tokens": [1622, 493, 337, 13241, 293, 13766, 11, 264, 6713, 291, 483, 307, 1238, 1850, 13, 467, 603, 312, 257, 37624], "temperature": 0.0, "avg_logprob": -0.1915613810221354, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.664458619023208e-06}, {"id": 688, "seek": 438102, "start": 4404.240000000001, "end": 4410.160000000001, "text": " will say these mismatch and it'll tell you what the dimensions are. So generally speaking,", "tokens": [486, 584, 613, 23220, 852, 293, 309, 603, 980, 291, 437, 264, 12819, 366, 13, 407, 5101, 4124, 11], "temperature": 0.0, "avg_logprob": -0.1915613810221354, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.664458619023208e-06}, {"id": 689, "seek": 441016, "start": 4410.16, "end": 4423.24, "text": " getting these things to match isn't too bad. It's no fun at all. But it's kind of menial.", "tokens": [1242, 613, 721, 281, 2995, 1943, 380, 886, 1578, 13, 467, 311, 572, 1019, 412, 439, 13, 583, 309, 311, 733, 295, 1706, 831, 13], "temperature": 0.0, "avg_logprob": -0.20567022470327523, "compression_ratio": 1.3624161073825503, "no_speech_prob": 1.09530574263772e-05}, {"id": 690, "seek": 441016, "start": 4423.24, "end": 4425.36, "text": " You'll get there eventually.", "tokens": [509, 603, 483, 456, 4728, 13], "temperature": 0.0, "avg_logprob": -0.20567022470327523, "compression_ratio": 1.3624161073825503, "no_speech_prob": 1.09530574263772e-05}, {"id": 691, "seek": 441016, "start": 4425.36, "end": 4432.0, "text": " So now that it's done, I've written it, so feel free to use it. As you can see, it's", "tokens": [407, 586, 300, 309, 311, 1096, 11, 286, 600, 3720, 309, 11, 370, 841, 1737, 281, 764, 309, 13, 1018, 291, 393, 536, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.20567022470327523, "compression_ratio": 1.3624161073825503, "no_speech_prob": 1.09530574263772e-05}, {"id": 692, "seek": 443200, "start": 4432.0, "end": 4443.3, "text": " easy to use. At the end, we get these pretty good results. One thing to point out is when", "tokens": [1858, 281, 764, 13, 1711, 264, 917, 11, 321, 483, 613, 1238, 665, 3542, 13, 1485, 551, 281, 935, 484, 307, 562], "temperature": 0.0, "avg_logprob": -0.20663705164072466, "compression_ratio": 1.4193548387096775, "no_speech_prob": 3.44654722539417e-06}, {"id": 693, "seek": 443200, "start": 4443.3, "end": 4457.68, "text": " we go, we get the predictions by just saying model.predict with our test set. And then", "tokens": [321, 352, 11, 321, 483, 264, 21264, 538, 445, 1566, 2316, 13, 79, 24945, 365, 527, 1500, 992, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.20663705164072466, "compression_ratio": 1.4193548387096775, "no_speech_prob": 3.44654722539417e-06}, {"id": 694, "seek": 445768, "start": 4457.68, "end": 4473.0, "text": " those predictions, you'll notice we go split on underscore. Why do we split on underscore?", "tokens": [729, 21264, 11, 291, 603, 3449, 321, 352, 7472, 322, 37556, 13, 1545, 360, 321, 7472, 322, 37556, 30], "temperature": 0.0, "avg_logprob": -0.20543042937321448, "compression_ratio": 1.4860335195530727, "no_speech_prob": 2.8573024337674724e-06}, {"id": 695, "seek": 445768, "start": 4473.0, "end": 4479.280000000001, "text": " That's because when I created the vocabulary earlier on, I set underscore to be the 0th", "tokens": [663, 311, 570, 562, 286, 2942, 264, 19864, 3071, 322, 11, 286, 992, 37556, 281, 312, 264, 1958, 392], "temperature": 0.0, "avg_logprob": -0.20543042937321448, "compression_ratio": 1.4860335195530727, "no_speech_prob": 2.8573024337674724e-06}, {"id": 696, "seek": 445768, "start": 4479.280000000001, "end": 4486.16, "text": " element. And remember that all of our words are padded at the end of zeros. So when the", "tokens": [4478, 13, 400, 1604, 300, 439, 295, 527, 2283, 366, 6887, 9207, 412, 264, 917, 295, 35193, 13, 407, 562, 264], "temperature": 0.0, "avg_logprob": -0.20543042937321448, "compression_ratio": 1.4860335195530727, "no_speech_prob": 2.8573024337674724e-06}, {"id": 697, "seek": 448616, "start": 4486.16, "end": 4494.599999999999, "text": " decoder predicts that this is the end of the word, it's going to spit out 0 or underscore.", "tokens": [979, 19866, 6069, 82, 300, 341, 307, 264, 917, 295, 264, 1349, 11, 309, 311, 516, 281, 22127, 484, 1958, 420, 37556, 13], "temperature": 0.0, "avg_logprob": -0.13804903262999, "compression_ratio": 1.5916230366492146, "no_speech_prob": 2.1568082502199104e-06}, {"id": 698, "seek": 448616, "start": 4494.599999999999, "end": 4499.04, "text": " So this is how a decoder knows to stop.", "tokens": [407, 341, 307, 577, 257, 979, 19866, 3255, 281, 1590, 13], "temperature": 0.0, "avg_logprob": -0.13804903262999, "compression_ratio": 1.5916230366492146, "no_speech_prob": 2.1568082502199104e-06}, {"id": 699, "seek": 448616, "start": 4499.04, "end": 4504.5199999999995, "text": " Now it doesn't actually stop. In terms of computation, the decoder's still going to", "tokens": [823, 309, 1177, 380, 767, 1590, 13, 682, 2115, 295, 24903, 11, 264, 979, 19866, 311, 920, 516, 281], "temperature": 0.0, "avg_logprob": -0.13804903262999, "compression_ratio": 1.5916230366492146, "no_speech_prob": 2.1568082502199104e-06}, {"id": 700, "seek": 448616, "start": 4504.5199999999995, "end": 4508.72, "text": " keep calculating all of the rest of the steps because we don't have the ability, at least", "tokens": [1066, 28258, 439, 295, 264, 1472, 295, 264, 4439, 570, 321, 500, 380, 362, 264, 3485, 11, 412, 1935], "temperature": 0.0, "avg_logprob": -0.13804903262999, "compression_ratio": 1.5916230366492146, "no_speech_prob": 2.1568082502199104e-06}, {"id": 701, "seek": 450872, "start": 4508.72, "end": 4518.280000000001, "text": " in Keras, to say stop now, everything's rectangles. So hopefully the decoder learns pretty quickly", "tokens": [294, 591, 6985, 11, 281, 584, 1590, 586, 11, 1203, 311, 24077, 904, 13, 407, 4696, 264, 979, 19866, 27152, 1238, 2661], "temperature": 0.0, "avg_logprob": -0.17256030607759282, "compression_ratio": 1.6046511627906976, "no_speech_prob": 3.187552920280723e-06}, {"id": 702, "seek": 450872, "start": 4518.280000000001, "end": 4523.8, "text": " that if the previous token was underscore, the next token will always be underscore.", "tokens": [300, 498, 264, 3894, 14862, 390, 37556, 11, 264, 958, 14862, 486, 1009, 312, 37556, 13], "temperature": 0.0, "avg_logprob": -0.17256030607759282, "compression_ratio": 1.6046511627906976, "no_speech_prob": 3.187552920280723e-06}, {"id": 703, "seek": 450872, "start": 4523.8, "end": 4530.04, "text": " Once you've finished, you've stayed finished. So that's a minor issue there.", "tokens": [3443, 291, 600, 4335, 11, 291, 600, 9181, 4335, 13, 407, 300, 311, 257, 6696, 2734, 456, 13], "temperature": 0.0, "avg_logprob": -0.17256030607759282, "compression_ratio": 1.6046511627906976, "no_speech_prob": 3.187552920280723e-06}, {"id": 704, "seek": 450872, "start": 4530.04, "end": 4535.400000000001, "text": " So that's that. So we're going to have a 7 minute break and when we come back, we're", "tokens": [407, 300, 311, 300, 13, 407, 321, 434, 516, 281, 362, 257, 1614, 3456, 1821, 293, 562, 321, 808, 646, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.17256030607759282, "compression_ratio": 1.6046511627906976, "no_speech_prob": 3.187552920280723e-06}, {"id": 705, "seek": 453540, "start": 4535.4, "end": 4539.719999999999, "text": " going to see this in PyTorch and we're going to use it for actual language translation.", "tokens": [516, 281, 536, 341, 294, 9953, 51, 284, 339, 293, 321, 434, 516, 281, 764, 309, 337, 3539, 2856, 12853, 13], "temperature": 0.0, "avg_logprob": -0.18508884112040203, "compression_ratio": 1.3708609271523178, "no_speech_prob": 3.7635760236298665e-05}, {"id": 706, "seek": 453540, "start": 4539.719999999999, "end": 4557.44, "text": " So I'll see you at 8 o'clock.", "tokens": [407, 286, 603, 536, 291, 412, 1649, 277, 6, 9023, 13], "temperature": 0.0, "avg_logprob": -0.18508884112040203, "compression_ratio": 1.3708609271523178, "no_speech_prob": 3.7635760236298665e-05}, {"id": 707, "seek": 453540, "start": 4557.44, "end": 4564.16, "text": " So one of our students, Vincent, was at Study Group a couple of weeks ago and was writing", "tokens": [407, 472, 295, 527, 1731, 11, 28003, 11, 390, 412, 27039, 10500, 257, 1916, 295, 3259, 2057, 293, 390, 3579], "temperature": 0.0, "avg_logprob": -0.18508884112040203, "compression_ratio": 1.3708609271523178, "no_speech_prob": 3.7635760236298665e-05}, {"id": 708, "seek": 456416, "start": 4564.16, "end": 4570.2, "text": " all these eigenvalues and eigenvectors equations. He was like, what are you doing? This idea,", "tokens": [439, 613, 10446, 46033, 293, 10446, 303, 5547, 11787, 13, 634, 390, 411, 11, 437, 366, 291, 884, 30, 639, 1558, 11], "temperature": 0.0, "avg_logprob": -0.3440704751521983, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.00015354742936324328}, {"id": 709, "seek": 456416, "start": 4570.2, "end": 4575.76, "text": " there's something in this style transfer, it's waiting to get out, I can feel it. I", "tokens": [456, 311, 746, 294, 341, 3758, 5003, 11, 309, 311, 3806, 281, 483, 484, 11, 286, 393, 841, 309, 13, 286], "temperature": 0.0, "avg_logprob": -0.3440704751521983, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.00015354742936324328}, {"id": 710, "seek": 456416, "start": 4575.76, "end": 4581.2, "text": " was like, oh, it's interesting, keep going. Then last week I saw him doing some more and", "tokens": [390, 411, 11, 1954, 11, 309, 311, 1880, 11, 1066, 516, 13, 1396, 1036, 1243, 286, 1866, 796, 884, 512, 544, 293], "temperature": 0.0, "avg_logprob": -0.3440704751521983, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.00015354742936324328}, {"id": 711, "seek": 456416, "start": 4581.2, "end": 4587.4, "text": " he had some strange noisy pictures on his screen. Then on Friday quite a few of us got", "tokens": [415, 632, 512, 5861, 24518, 5242, 322, 702, 2568, 13, 1396, 322, 6984, 1596, 257, 1326, 295, 505, 658], "temperature": 0.0, "avg_logprob": -0.3440704751521983, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.00015354742936324328}, {"id": 712, "seek": 458740, "start": 4587.4, "end": 4595.32, "text": " together to do some hacking. I still saw him doing the same thing. I know there's something", "tokens": [1214, 281, 360, 512, 31422, 13, 286, 920, 1866, 796, 884, 264, 912, 551, 13, 286, 458, 456, 311, 746], "temperature": 0.0, "avg_logprob": -0.23482187589009604, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.7231202946277335e-05}, {"id": 713, "seek": 458740, "start": 4595.32, "end": 4603.5199999999995, "text": " here, it's going to get close. He just told me he sent me an email. So here is a photo,", "tokens": [510, 11, 309, 311, 516, 281, 483, 1998, 13, 634, 445, 1907, 385, 415, 2279, 385, 364, 3796, 13, 407, 510, 307, 257, 5052, 11], "temperature": 0.0, "avg_logprob": -0.23482187589009604, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.7231202946277335e-05}, {"id": 714, "seek": 458740, "start": 4603.5199999999995, "end": 4616.36, "text": " and here is the regular style transfer result. Here's what happens when you use his new mathematical", "tokens": [293, 510, 307, 264, 3890, 3758, 5003, 1874, 13, 1692, 311, 437, 2314, 562, 291, 764, 702, 777, 18894], "temperature": 0.0, "avg_logprob": -0.23482187589009604, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.7231202946277335e-05}, {"id": 715, "seek": 461636, "start": 4616.36, "end": 4626.5599999999995, "text": " technique. Hopefully by next week I will understand this enough that either him or I can explain", "tokens": [6532, 13, 10429, 538, 958, 1243, 286, 486, 1223, 341, 1547, 300, 2139, 796, 420, 286, 393, 2903], "temperature": 0.0, "avg_logprob": -0.23814708467513795, "compression_ratio": 1.436842105263158, "no_speech_prob": 5.0642578571569175e-05}, {"id": 716, "seek": 461636, "start": 4626.5599999999995, "end": 4630.48, "text": " this to you. But one of the key differences is he's actually using the Earth movement", "tokens": [341, 281, 291, 13, 583, 472, 295, 264, 2141, 7300, 307, 415, 311, 767, 1228, 264, 4755, 3963], "temperature": 0.0, "avg_logprob": -0.23814708467513795, "compression_ratio": 1.436842105263158, "no_speech_prob": 5.0642578571569175e-05}, {"id": 717, "seek": 461636, "start": 4630.48, "end": 4639.0, "text": " distance, which is the basis of the Wasserstein gain. I've managed to avoid teaching about", "tokens": [4560, 11, 597, 307, 264, 5143, 295, 264, 17351, 9089, 6052, 13, 286, 600, 6453, 281, 5042, 4571, 466], "temperature": 0.0, "avg_logprob": -0.23814708467513795, "compression_ratio": 1.436842105263158, "no_speech_prob": 5.0642578571569175e-05}, {"id": 718, "seek": 463900, "start": 4639.0, "end": 4649.56, "text": " eigenvalues and eigenvectors so far. I don't know how we're going to do that.", "tokens": [10446, 46033, 293, 10446, 303, 5547, 370, 1400, 13, 286, 500, 380, 458, 577, 321, 434, 516, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.22492757135508012, "compression_ratio": 1.310077519379845, "no_speech_prob": 3.3212450944120064e-05}, {"id": 719, "seek": 463900, "start": 4649.56, "end": 4662.12, "text": " So this is got to be a paper. You've created a whole new technique. This is super exciting.", "tokens": [407, 341, 307, 658, 281, 312, 257, 3035, 13, 509, 600, 2942, 257, 1379, 777, 6532, 13, 639, 307, 1687, 4670, 13], "temperature": 0.0, "avg_logprob": -0.22492757135508012, "compression_ratio": 1.310077519379845, "no_speech_prob": 3.3212450944120064e-05}, {"id": 720, "seek": 466212, "start": 4662.12, "end": 4673.8, "text": " So congratulations. I look forward to learning more about it. People just keep doing cool", "tokens": [407, 13568, 13, 286, 574, 2128, 281, 2539, 544, 466, 309, 13, 3432, 445, 1066, 884, 1627], "temperature": 0.0, "avg_logprob": -0.20141985302879697, "compression_ratio": 1.0853658536585367, "no_speech_prob": 3.0240731575759128e-05}, {"id": 721, "seek": 467380, "start": 4673.8, "end": 4692.28, "text": " stuff. I love it. You guys are just sipping along.", "tokens": [1507, 13, 286, 959, 309, 13, 509, 1074, 366, 445, 1511, 3759, 2051, 13], "temperature": 0.0, "avg_logprob": -0.142961409356859, "compression_ratio": 0.8620689655172413, "no_speech_prob": 3.5008029044547584e-06}, {"id": 722, "seek": 469228, "start": 4692.28, "end": 4704.44, "text": " So let's translate English into French. Now here's a problem. The teacher forcing his", "tokens": [407, 718, 311, 13799, 3669, 666, 5522, 13, 823, 510, 311, 257, 1154, 13, 440, 5027, 19030, 702], "temperature": 0.0, "avg_logprob": -0.3287353082136674, "compression_ratio": 1.0365853658536586, "no_speech_prob": 3.647645644377917e-05}, {"id": 723, "seek": 470444, "start": 4704.44, "end": 4731.16, "text": " students to do the math. So what this question is getting at is, why don't we use the lambda", "tokens": [1731, 281, 360, 264, 5221, 13, 407, 437, 341, 1168, 307, 1242, 412, 307, 11, 983, 500, 380, 321, 764, 264, 13607], "temperature": 0.0, "avg_logprob": -0.5711380151601938, "compression_ratio": 1.108433734939759, "no_speech_prob": 0.00010389224189566448}, {"id": 724, "seek": 473116, "start": 4731.16, "end": 4742.48, "text": " layer to do the attention calculations and then feed that into a standard RNN. Remember,", "tokens": [4583, 281, 360, 264, 3202, 20448, 293, 550, 3154, 300, 666, 257, 3832, 45702, 45, 13, 5459, 11], "temperature": 0.0, "avg_logprob": -0.18463211524777295, "compression_ratio": 1.6884422110552764, "no_speech_prob": 6.301959365373477e-05}, {"id": 725, "seek": 473116, "start": 4742.48, "end": 4748.08, "text": " those calculations are being done inside the step function. So in other words, each step", "tokens": [729, 20448, 366, 885, 1096, 1854, 264, 1823, 2445, 13, 407, 294, 661, 2283, 11, 1184, 1823], "temperature": 0.0, "avg_logprob": -0.18463211524777295, "compression_ratio": 1.6884422110552764, "no_speech_prob": 6.301959365373477e-05}, {"id": 726, "seek": 473116, "start": 4748.08, "end": 4753.12, "text": " uses attention to calculate the output of the step, which impacts the next step. So", "tokens": [4960, 3202, 281, 8873, 264, 5598, 295, 264, 1823, 11, 597, 11606, 264, 958, 1823, 13, 407], "temperature": 0.0, "avg_logprob": -0.18463211524777295, "compression_ratio": 1.6884422110552764, "no_speech_prob": 6.301959365373477e-05}, {"id": 727, "seek": 473116, "start": 4753.12, "end": 4759.08, "text": " it needs to be inside the RNN. You can't just pre-process the whole layer.", "tokens": [309, 2203, 281, 312, 1854, 264, 45702, 45, 13, 509, 393, 380, 445, 659, 12, 41075, 264, 1379, 4583, 13], "temperature": 0.0, "avg_logprob": -0.18463211524777295, "compression_ratio": 1.6884422110552764, "no_speech_prob": 6.301959365373477e-05}, {"id": 728, "seek": 475908, "start": 4759.08, "end": 4767.24, "text": " Question. Is there any reason why we used hyperbolic tan as opposed to sigmoid?", "tokens": [14464, 13, 1119, 456, 604, 1778, 983, 321, 1143, 9848, 65, 7940, 7603, 382, 8851, 281, 4556, 3280, 327, 30], "temperature": 0.0, "avg_logprob": -0.2526359277613023, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.00014425192784983665}, {"id": 729, "seek": 475908, "start": 4767.24, "end": 4775.08, "text": " Answer. No reason at all. Hyperbolic tan and sigmoid are the same thing. Hyperbolic tan", "tokens": [24545, 13, 883, 1778, 412, 439, 13, 29592, 65, 7940, 7603, 293, 4556, 3280, 327, 366, 264, 912, 551, 13, 29592, 65, 7940, 7603], "temperature": 0.0, "avg_logprob": -0.2526359277613023, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.00014425192784983665}, {"id": 730, "seek": 475908, "start": 4775.08, "end": 4784.96, "text": " goes from minus 1 to 1, sigmoid goes from 0 to 1.", "tokens": [1709, 490, 3175, 502, 281, 502, 11, 4556, 3280, 327, 1709, 490, 1958, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.2526359277613023, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.00014425192784983665}, {"id": 731, "seek": 478496, "start": 4784.96, "end": 4797.0, "text": " Teacher forcing was this thing where we're concatenating the previous correct answers", "tokens": [19745, 19030, 390, 341, 551, 689, 321, 434, 1588, 7186, 990, 264, 3894, 3006, 6338], "temperature": 0.0, "avg_logprob": -0.11838704347610474, "compression_ratio": 1.4065040650406504, "no_speech_prob": 1.2411322131811175e-05}, {"id": 732, "seek": 478496, "start": 4797.0, "end": 4810.74, "text": " embedding with our attention-weighted encoder inputs in order to help our model to keep", "tokens": [12240, 3584, 365, 527, 3202, 12, 12329, 292, 2058, 19866, 15743, 294, 1668, 281, 854, 527, 2316, 281, 1066], "temperature": 0.0, "avg_logprob": -0.11838704347610474, "compression_ratio": 1.4065040650406504, "no_speech_prob": 1.2411322131811175e-05}, {"id": 733, "seek": 481074, "start": 4810.74, "end": 4815.24, "text": " track of where it ought to be. That helps the training. Now there's nothing wrong with", "tokens": [2837, 295, 689, 309, 13416, 281, 312, 13, 663, 3665, 264, 3097, 13, 823, 456, 311, 1825, 2085, 365], "temperature": 0.0, "avg_logprob": -0.12800549825032553, "compression_ratio": 1.5260416666666667, "no_speech_prob": 8.059417382355605e-07}, {"id": 734, "seek": 481074, "start": 4815.24, "end": 4824.74, "text": " training that way, but you can't use that at inference time, at test time, because you", "tokens": [3097, 300, 636, 11, 457, 291, 393, 380, 764, 300, 412, 38253, 565, 11, 412, 1500, 565, 11, 570, 291], "temperature": 0.0, "avg_logprob": -0.12800549825032553, "compression_ratio": 1.5260416666666667, "no_speech_prob": 8.059417382355605e-07}, {"id": 735, "seek": 481074, "start": 4824.74, "end": 4827.9, "text": " don't know the correct answer.", "tokens": [500, 380, 458, 264, 3006, 1867, 13], "temperature": 0.0, "avg_logprob": -0.12800549825032553, "compression_ratio": 1.5260416666666667, "no_speech_prob": 8.059417382355605e-07}, {"id": 736, "seek": 481074, "start": 4827.9, "end": 4839.4, "text": " So my Keras model here is totally cheating. I'm passing in the previous letter's correct", "tokens": [407, 452, 591, 6985, 2316, 510, 307, 3879, 18309, 13, 286, 478, 8437, 294, 264, 3894, 5063, 311, 3006], "temperature": 0.0, "avg_logprob": -0.12800549825032553, "compression_ratio": 1.5260416666666667, "no_speech_prob": 8.059417382355605e-07}, {"id": 737, "seek": 483940, "start": 4839.4, "end": 4847.679999999999, "text": " answer to every step, but in real life I don't know that. So what we actually need to do", "tokens": [1867, 281, 633, 1823, 11, 457, 294, 957, 993, 286, 500, 380, 458, 300, 13, 407, 437, 321, 767, 643, 281, 360], "temperature": 0.0, "avg_logprob": -0.11589942976485851, "compression_ratio": 1.5631067961165048, "no_speech_prob": 2.090444468194619e-06}, {"id": 738, "seek": 483940, "start": 4847.679999999999, "end": 4855.28, "text": " is to at inference time, you don't use teacher forcing, but instead you take the predicted", "tokens": [307, 281, 412, 38253, 565, 11, 291, 500, 380, 764, 5027, 19030, 11, 457, 2602, 291, 747, 264, 19147], "temperature": 0.0, "avg_logprob": -0.11589942976485851, "compression_ratio": 1.5631067961165048, "no_speech_prob": 2.090444468194619e-06}, {"id": 739, "seek": 483940, "start": 4855.28, "end": 4863.08, "text": " previous step's result and feed it in for the next step.", "tokens": [3894, 1823, 311, 1874, 293, 3154, 309, 294, 337, 264, 958, 1823, 13], "temperature": 0.0, "avg_logprob": -0.11589942976485851, "compression_ratio": 1.5631067961165048, "no_speech_prob": 2.090444468194619e-06}, {"id": 740, "seek": 483940, "start": 4863.08, "end": 4868.5199999999995, "text": " I have no idea how to do that in Keras. It drove me crazy trying to figure out how to", "tokens": [286, 362, 572, 1558, 577, 281, 360, 300, 294, 591, 6985, 13, 467, 13226, 385, 3219, 1382, 281, 2573, 484, 577, 281], "temperature": 0.0, "avg_logprob": -0.11589942976485851, "compression_ratio": 1.5631067961165048, "no_speech_prob": 2.090444468194619e-06}, {"id": 741, "seek": 486852, "start": 4868.52, "end": 4877.080000000001, "text": " do that in Keras. That was the thing that pushed me to PyTorch. I was so sick of this", "tokens": [360, 300, 294, 591, 6985, 13, 663, 390, 264, 551, 300, 9152, 385, 281, 9953, 51, 284, 339, 13, 286, 390, 370, 4998, 295, 341], "temperature": 0.0, "avg_logprob": -0.17399604167413274, "compression_ratio": 1.6, "no_speech_prob": 3.6688493310066406e-06}, {"id": 742, "seek": 486852, "start": 4877.080000000001, "end": 4881.6, "text": " goddamn attention-layer, the idea of going back and trying to put this thing in drove", "tokens": [32951, 3202, 12, 8376, 260, 11, 264, 1558, 295, 516, 646, 293, 1382, 281, 829, 341, 551, 294, 13226], "temperature": 0.0, "avg_logprob": -0.17399604167413274, "compression_ratio": 1.6, "no_speech_prob": 3.6688493310066406e-06}, {"id": 743, "seek": 486852, "start": 4881.6, "end": 4885.280000000001, "text": " me crazy and I don't even know how to do it.", "tokens": [385, 3219, 293, 286, 500, 380, 754, 458, 577, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.17399604167413274, "compression_ratio": 1.6, "no_speech_prob": 3.6688493310066406e-06}, {"id": 744, "seek": 486852, "start": 4885.280000000001, "end": 4890.92, "text": " Furthermore, we actually want this to be dynamic. It turns out the best way, if you use teacher", "tokens": [23999, 11, 321, 767, 528, 341, 281, 312, 8546, 13, 467, 4523, 484, 264, 1151, 636, 11, 498, 291, 764, 5027], "temperature": 0.0, "avg_logprob": -0.17399604167413274, "compression_ratio": 1.6, "no_speech_prob": 3.6688493310066406e-06}, {"id": 745, "seek": 486852, "start": 4890.92, "end": 4898.4800000000005, "text": " forcing the whole time, you actually end up with a model that gets sloppy. It learns to", "tokens": [19030, 264, 1379, 565, 11, 291, 767, 917, 493, 365, 257, 2316, 300, 2170, 43684, 13, 467, 27152, 281], "temperature": 0.0, "avg_logprob": -0.17399604167413274, "compression_ratio": 1.6, "no_speech_prob": 3.6688493310066406e-06}, {"id": 746, "seek": 489848, "start": 4898.48, "end": 4903.16, "text": " take advantage of the fact that it's about to be told what the previous thing should", "tokens": [747, 5002, 295, 264, 1186, 300, 309, 311, 466, 281, 312, 1907, 437, 264, 3894, 551, 820], "temperature": 0.0, "avg_logprob": -0.15850686033566794, "compression_ratio": 1.8577981651376148, "no_speech_prob": 9.516180398350116e-06}, {"id": 747, "seek": 489848, "start": 4903.16, "end": 4907.08, "text": " have been. So it kind of uses a more speculative approach.", "tokens": [362, 668, 13, 407, 309, 733, 295, 4960, 257, 544, 49415, 3109, 13], "temperature": 0.0, "avg_logprob": -0.15850686033566794, "compression_ratio": 1.8577981651376148, "no_speech_prob": 9.516180398350116e-06}, {"id": 748, "seek": 489848, "start": 4907.08, "end": 4912.5599999999995, "text": " So actually the best training approach is as it goes through the epochs, initially it", "tokens": [407, 767, 264, 1151, 3097, 3109, 307, 382, 309, 1709, 807, 264, 30992, 28346, 11, 9105, 309], "temperature": 0.0, "avg_logprob": -0.15850686033566794, "compression_ratio": 1.8577981651376148, "no_speech_prob": 9.516180398350116e-06}, {"id": 749, "seek": 489848, "start": 4912.5599999999995, "end": 4918.839999999999, "text": " uses teacher forcing every time, and halfway through it uses teacher forcing randomly half", "tokens": [4960, 5027, 19030, 633, 565, 11, 293, 15461, 807, 309, 4960, 5027, 19030, 16979, 1922], "temperature": 0.0, "avg_logprob": -0.15850686033566794, "compression_ratio": 1.8577981651376148, "no_speech_prob": 9.516180398350116e-06}, {"id": 750, "seek": 489848, "start": 4918.839999999999, "end": 4923.44, "text": " the time, and at the last epoch it doesn't use teacher forcing at all. So it kind of", "tokens": [264, 565, 11, 293, 412, 264, 1036, 30992, 339, 309, 1177, 380, 764, 5027, 19030, 412, 439, 13, 407, 309, 733, 295], "temperature": 0.0, "avg_logprob": -0.15850686033566794, "compression_ratio": 1.8577981651376148, "no_speech_prob": 9.516180398350116e-06}, {"id": 751, "seek": 492344, "start": 4923.44, "end": 4932.799999999999, "text": " learns to wean itself off this extra information. And that kind of dynamic thing, that's really", "tokens": [27152, 281, 321, 282, 2564, 766, 341, 2857, 1589, 13, 400, 300, 733, 295, 8546, 551, 11, 300, 311, 534], "temperature": 0.0, "avg_logprob": -0.12612817528542508, "compression_ratio": 1.5748792270531402, "no_speech_prob": 8.579208952141926e-07}, {"id": 752, "seek": 492344, "start": 4932.799999999999, "end": 4935.799999999999, "text": " hard or maybe even impossible to do in Keras.", "tokens": [1152, 420, 1310, 754, 6243, 281, 360, 294, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.12612817528542508, "compression_ratio": 1.5748792270531402, "no_speech_prob": 8.579208952141926e-07}, {"id": 753, "seek": 492344, "start": 4935.799999999999, "end": 4944.48, "text": " So for all these reasons, I moved to PyTorch. I haven't done all of these things in PyTorch,", "tokens": [407, 337, 439, 613, 4112, 11, 286, 4259, 281, 9953, 51, 284, 339, 13, 286, 2378, 380, 1096, 439, 295, 613, 721, 294, 9953, 51, 284, 339, 11], "temperature": 0.0, "avg_logprob": -0.12612817528542508, "compression_ratio": 1.5748792270531402, "no_speech_prob": 8.579208952141926e-07}, {"id": 754, "seek": 492344, "start": 4944.48, "end": 4949.919999999999, "text": " I've done most of them, but the dynamic changing of teacher forcing, I've actually left for", "tokens": [286, 600, 1096, 881, 295, 552, 11, 457, 264, 8546, 4473, 295, 5027, 19030, 11, 286, 600, 767, 1411, 337], "temperature": 0.0, "avg_logprob": -0.12612817528542508, "compression_ratio": 1.5748792270531402, "no_speech_prob": 8.579208952141926e-07}, {"id": 755, "seek": 494992, "start": 4949.92, "end": 4953.56, "text": " one of you guys to try out. Anyway, the basic ideas are all here.", "tokens": [472, 295, 291, 1074, 281, 853, 484, 13, 5684, 11, 264, 3875, 3487, 366, 439, 510, 13], "temperature": 0.0, "avg_logprob": -0.13300595367163942, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.889160769991577e-06}, {"id": 756, "seek": 494992, "start": 4953.56, "end": 4959.0, "text": " So let's look at the PyTorch version. Interestingly, the PyTorch version in terms of the attention", "tokens": [407, 718, 311, 574, 412, 264, 9953, 51, 284, 339, 3037, 13, 30564, 11, 264, 9953, 51, 284, 339, 3037, 294, 2115, 295, 264, 3202], "temperature": 0.0, "avg_logprob": -0.13300595367163942, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.889160769991577e-06}, {"id": 757, "seek": 494992, "start": 4959.0, "end": 4965.6, "text": " model itself turns out to be way easier. But we have to write more code because there's", "tokens": [2316, 2564, 4523, 484, 281, 312, 636, 3571, 13, 583, 321, 362, 281, 2464, 544, 3089, 570, 456, 311], "temperature": 0.0, "avg_logprob": -0.13300595367163942, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.889160769991577e-06}, {"id": 758, "seek": 494992, "start": 4965.6, "end": 4973.32, "text": " less structure for NLP models. Like for computer vision stuff, there's the PyTorch vision project", "tokens": [1570, 3877, 337, 426, 45196, 5245, 13, 1743, 337, 3820, 5201, 1507, 11, 456, 311, 264, 9953, 51, 284, 339, 5201, 1716], "temperature": 0.0, "avg_logprob": -0.13300595367163942, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.889160769991577e-06}, {"id": 759, "seek": 494992, "start": 4973.32, "end": 4978.28, "text": " which has all that data loading and models and blah blah blah. We don't seem to have", "tokens": [597, 575, 439, 300, 1412, 15114, 293, 5245, 293, 12288, 12288, 12288, 13, 492, 500, 380, 1643, 281, 362], "temperature": 0.0, "avg_logprob": -0.13300595367163942, "compression_ratio": 1.6539923954372624, "no_speech_prob": 7.889160769991577e-06}, {"id": 760, "seek": 497828, "start": 4978.28, "end": 4984.88, "text": " an NLP kind of version of that. So there's a bit more code to write here.", "tokens": [364, 426, 45196, 733, 295, 3037, 295, 300, 13, 407, 456, 311, 257, 857, 544, 3089, 281, 2464, 510, 13], "temperature": 0.0, "avg_logprob": -0.17171365442410322, "compression_ratio": 1.4444444444444444, "no_speech_prob": 3.7852846617170144e-06}, {"id": 761, "seek": 497828, "start": 4984.88, "end": 4996.4, "text": " So let's translate English to French. What I did was I downloaded this Giga French corpus", "tokens": [407, 718, 311, 13799, 3669, 281, 5522, 13, 708, 286, 630, 390, 286, 21748, 341, 460, 9900, 5522, 1181, 31624], "temperature": 0.0, "avg_logprob": -0.17171365442410322, "compression_ratio": 1.4444444444444444, "no_speech_prob": 3.7852846617170144e-06}, {"id": 762, "seek": 497828, "start": 4996.4, "end": 5002.48, "text": " which I'll put a link to on the wiki. Basically this is a really cool idea. What this researcher", "tokens": [597, 286, 603, 829, 257, 2113, 281, 322, 264, 261, 9850, 13, 8537, 341, 307, 257, 534, 1627, 1558, 13, 708, 341, 21751], "temperature": 0.0, "avg_logprob": -0.17171365442410322, "compression_ratio": 1.4444444444444444, "no_speech_prob": 3.7852846617170144e-06}, {"id": 763, "seek": 500248, "start": 5002.48, "end": 5008.48, "text": " did was he went to lots of Canadian websites and used a screen scraper to figure out whether", "tokens": [630, 390, 415, 1437, 281, 3195, 295, 12641, 12891, 293, 1143, 257, 2568, 13943, 610, 281, 2573, 484, 1968], "temperature": 0.0, "avg_logprob": -0.14862964726701566, "compression_ratio": 1.6435185185185186, "no_speech_prob": 1.2411279385560192e-05}, {"id": 764, "seek": 500248, "start": 5008.48, "end": 5012.4, "text": " there was a little button saying switch from English to French. And the screen scraper", "tokens": [456, 390, 257, 707, 2960, 1566, 3679, 490, 3669, 281, 5522, 13, 400, 264, 2568, 13943, 610], "temperature": 0.0, "avg_logprob": -0.14862964726701566, "compression_ratio": 1.6435185185185186, "no_speech_prob": 1.2411279385560192e-05}, {"id": 765, "seek": 500248, "start": 5012.4, "end": 5019.24, "text": " would automatically click the button and then assume that those two screens were the same.", "tokens": [576, 6772, 2052, 264, 2960, 293, 550, 6552, 300, 729, 732, 11171, 645, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.14862964726701566, "compression_ratio": 1.6435185185185186, "no_speech_prob": 1.2411279385560192e-05}, {"id": 766, "seek": 500248, "start": 5019.24, "end": 5024.32, "text": " And then he tokenized them into sentences and provides this corpus of like a billion", "tokens": [400, 550, 415, 14862, 1602, 552, 666, 16579, 293, 6417, 341, 1181, 31624, 295, 411, 257, 5218], "temperature": 0.0, "avg_logprob": -0.14862964726701566, "compression_ratio": 1.6435185185185186, "no_speech_prob": 1.2411279385560192e-05}, {"id": 767, "seek": 502432, "start": 5024.32, "end": 5036.24, "text": " words. So this is pretty great. I didn't really want to create a complete English-French translator", "tokens": [2283, 13, 407, 341, 307, 1238, 869, 13, 286, 994, 380, 534, 528, 281, 1884, 257, 3566, 3669, 12, 37, 4442, 35223], "temperature": 0.0, "avg_logprob": -0.14775938457912868, "compression_ratio": 1.5053191489361701, "no_speech_prob": 3.7266038361849496e-06}, {"id": 768, "seek": 502432, "start": 5036.24, "end": 5039.44, "text": " because I just didn't have the time to run it for long enough. So I tried to think, what's", "tokens": [570, 286, 445, 994, 380, 362, 264, 565, 281, 1190, 309, 337, 938, 1547, 13, 407, 286, 3031, 281, 519, 11, 437, 311], "temperature": 0.0, "avg_logprob": -0.14775938457912868, "compression_ratio": 1.5053191489361701, "no_speech_prob": 3.7266038361849496e-06}, {"id": 769, "seek": 502432, "start": 5039.44, "end": 5046.04, "text": " like an interesting subset of English to French? So I thought, what if we learn to translate", "tokens": [411, 364, 1880, 25993, 295, 3669, 281, 5522, 30, 407, 286, 1194, 11, 437, 498, 321, 1466, 281, 13799], "temperature": 0.0, "avg_logprob": -0.14775938457912868, "compression_ratio": 1.5053191489361701, "no_speech_prob": 3.7266038361849496e-06}, {"id": 770, "seek": 504604, "start": 5046.04, "end": 5055.64, "text": " English questions that start with WH. What, who, where, why. So when I looked at it, it", "tokens": [3669, 1651, 300, 722, 365, 8183, 13, 708, 11, 567, 11, 689, 11, 983, 13, 407, 562, 286, 2956, 412, 309, 11, 309], "temperature": 0.0, "avg_logprob": -0.22815197535923548, "compression_ratio": 1.4835164835164836, "no_speech_prob": 8.397973942919634e-06}, {"id": 771, "seek": 504604, "start": 5055.64, "end": 5067.24, "text": " turned out that that was like 80,000 or something. There's a lot of sentences basically. But", "tokens": [3574, 484, 300, 300, 390, 411, 4688, 11, 1360, 420, 746, 13, 821, 311, 257, 688, 295, 16579, 1936, 13, 583], "temperature": 0.0, "avg_logprob": -0.22815197535923548, "compression_ratio": 1.4835164835164836, "no_speech_prob": 8.397973942919634e-06}, {"id": 772, "seek": 504604, "start": 5067.24, "end": 5074.96, "text": " the nice thing is that all of those sentences have a somewhat similar structure. So we're", "tokens": [264, 1481, 551, 307, 300, 439, 295, 729, 16579, 362, 257, 8344, 2531, 3877, 13, 407, 321, 434], "temperature": 0.0, "avg_logprob": -0.22815197535923548, "compression_ratio": 1.4835164835164836, "no_speech_prob": 8.397973942919634e-06}, {"id": 773, "seek": 507496, "start": 5074.96, "end": 5077.92, "text": " going to learn everything about translating English into French, but we're going to be", "tokens": [516, 281, 1466, 1203, 466, 35030, 3669, 666, 5522, 11, 457, 321, 434, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.1610660780043829, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.601594133011531e-06}, {"id": 774, "seek": 507496, "start": 5077.92, "end": 5080.32, "text": " doing it with a slight subset.", "tokens": [884, 309, 365, 257, 4036, 25993, 13], "temperature": 0.0, "avg_logprob": -0.1610660780043829, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.601594133011531e-06}, {"id": 775, "seek": 507496, "start": 5080.32, "end": 5086.6, "text": " So that's why I did this regex, which says, look for things that start with WH and end", "tokens": [407, 300, 311, 983, 286, 630, 341, 319, 432, 87, 11, 597, 1619, 11, 574, 337, 721, 300, 722, 365, 8183, 293, 917], "temperature": 0.0, "avg_logprob": -0.1610660780043829, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.601594133011531e-06}, {"id": 776, "seek": 507496, "start": 5086.6, "end": 5092.64, "text": " with a question mark in English. And where the French can be anything at all, but it", "tokens": [365, 257, 1168, 1491, 294, 3669, 13, 400, 689, 264, 5522, 393, 312, 1340, 412, 439, 11, 457, 309], "temperature": 0.0, "avg_logprob": -0.1610660780043829, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.601594133011531e-06}, {"id": 777, "seek": 507496, "start": 5092.64, "end": 5096.2, "text": " should end with a question mark.", "tokens": [820, 917, 365, 257, 1168, 1491, 13], "temperature": 0.0, "avg_logprob": -0.1610660780043829, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.601594133011531e-06}, {"id": 778, "seek": 509620, "start": 5096.2, "end": 5107.48, "text": " So I went through the French and English files. This is this really cool trick we've mentioned", "tokens": [407, 286, 1437, 807, 264, 5522, 293, 3669, 7098, 13, 639, 307, 341, 534, 1627, 4282, 321, 600, 2835], "temperature": 0.0, "avg_logprob": -0.15208933246669484, "compression_ratio": 1.546448087431694, "no_speech_prob": 9.818055332289077e-06}, {"id": 779, "seek": 509620, "start": 5107.48, "end": 5112.04, "text": " once before, that once you go open in Python, that returns a generator that you can loop", "tokens": [1564, 949, 11, 300, 1564, 291, 352, 1269, 294, 15329, 11, 300, 11247, 257, 19265, 300, 291, 393, 6367], "temperature": 0.0, "avg_logprob": -0.15208933246669484, "compression_ratio": 1.546448087431694, "no_speech_prob": 9.818055332289077e-06}, {"id": 780, "seek": 509620, "start": 5112.04, "end": 5117.8, "text": " through. So if you zip the two together, you've got the English questions and the French questions,", "tokens": [807, 13, 407, 498, 291, 20730, 264, 732, 1214, 11, 291, 600, 658, 264, 3669, 1651, 293, 264, 5522, 1651, 11], "temperature": 0.0, "avg_logprob": -0.15208933246669484, "compression_ratio": 1.546448087431694, "no_speech_prob": 9.818055332289077e-06}, {"id": 781, "seek": 511780, "start": 5117.8, "end": 5126.68, "text": " and then just go through and run the regex and then return the ones that both of the", "tokens": [293, 550, 445, 352, 807, 293, 1190, 264, 319, 432, 87, 293, 550, 2736, 264, 2306, 300, 1293, 295, 264], "temperature": 0.0, "avg_logprob": -0.19443469101123595, "compression_ratio": 1.6, "no_speech_prob": 9.818178114073817e-06}, {"id": 782, "seek": 511780, "start": 5126.68, "end": 5128.68, "text": " regex matched.", "tokens": [319, 432, 87, 21447, 13], "temperature": 0.0, "avg_logprob": -0.19443469101123595, "compression_ratio": 1.6, "no_speech_prob": 9.818178114073817e-06}, {"id": 783, "seek": 511780, "start": 5128.68, "end": 5134.400000000001, "text": " So here's the first 6 examples. That looks good. And as you can see, a lot of them are", "tokens": [407, 510, 311, 264, 700, 1386, 5110, 13, 663, 1542, 665, 13, 400, 382, 291, 393, 536, 11, 257, 688, 295, 552, 366], "temperature": 0.0, "avg_logprob": -0.19443469101123595, "compression_ratio": 1.6, "no_speech_prob": 9.818178114073817e-06}, {"id": 784, "seek": 511780, "start": 5134.400000000001, "end": 5143.64, "text": " really simple and some of them are more complex. As per usual, dump that in a pickle file so", "tokens": [534, 2199, 293, 512, 295, 552, 366, 544, 3997, 13, 1018, 680, 7713, 11, 11430, 300, 294, 257, 31433, 3991, 370], "temperature": 0.0, "avg_logprob": -0.19443469101123595, "compression_ratio": 1.6, "no_speech_prob": 9.818178114073817e-06}, {"id": 785, "seek": 511780, "start": 5143.64, "end": 5146.72, "text": " we can load it in quickly later.", "tokens": [321, 393, 3677, 309, 294, 2661, 1780, 13], "temperature": 0.0, "avg_logprob": -0.19443469101123595, "compression_ratio": 1.6, "no_speech_prob": 9.818178114073817e-06}, {"id": 786, "seek": 514672, "start": 5146.72, "end": 5151.52, "text": " So we've got the English questions and the French questions. I'm going to show you all", "tokens": [407, 321, 600, 658, 264, 3669, 1651, 293, 264, 5522, 1651, 13, 286, 478, 516, 281, 855, 291, 439], "temperature": 0.0, "avg_logprob": -0.14651349011589498, "compression_ratio": 1.6206896551724137, "no_speech_prob": 1.363113824481843e-05}, {"id": 787, "seek": 514672, "start": 5151.52, "end": 5155.84, "text": " the steps for real-world NLP so you can see all the pieces. We're going to do everything", "tokens": [264, 4439, 337, 957, 12, 13217, 426, 45196, 370, 291, 393, 536, 439, 264, 3755, 13, 492, 434, 516, 281, 360, 1203], "temperature": 0.0, "avg_logprob": -0.14651349011589498, "compression_ratio": 1.6206896551724137, "no_speech_prob": 1.363113824481843e-05}, {"id": 788, "seek": 514672, "start": 5155.84, "end": 5159.6, "text": " by hand so you can get a sense of exactly what happens.", "tokens": [538, 1011, 370, 291, 393, 483, 257, 2020, 295, 2293, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.14651349011589498, "compression_ratio": 1.6206896551724137, "no_speech_prob": 1.363113824481843e-05}, {"id": 789, "seek": 514672, "start": 5159.6, "end": 5170.320000000001, "text": " So the next step is tokenization. So tokenization is taking a sentence and turning it into words.", "tokens": [407, 264, 958, 1823, 307, 14862, 2144, 13, 407, 14862, 2144, 307, 1940, 257, 8174, 293, 6246, 309, 666, 2283, 13], "temperature": 0.0, "avg_logprob": -0.14651349011589498, "compression_ratio": 1.6206896551724137, "no_speech_prob": 1.363113824481843e-05}, {"id": 790, "seek": 517032, "start": 5170.32, "end": 5181.08, "text": " But this is not quite straightforward because what's a word? So is that a word, or is that", "tokens": [583, 341, 307, 406, 1596, 15325, 570, 437, 311, 257, 1349, 30, 407, 307, 300, 257, 1349, 11, 420, 307, 300], "temperature": 0.0, "avg_logprob": -0.1909426398899244, "compression_ratio": 1.5769230769230769, "no_speech_prob": 9.368637620355003e-06}, {"id": 791, "seek": 517032, "start": 5181.08, "end": 5189.12, "text": " a word, or is that a word? And so I had to make some decisions based on my view of what", "tokens": [257, 1349, 11, 420, 307, 300, 257, 1349, 30, 400, 370, 286, 632, 281, 652, 512, 5327, 2361, 322, 452, 1910, 295, 437], "temperature": 0.0, "avg_logprob": -0.1909426398899244, "compression_ratio": 1.5769230769230769, "no_speech_prob": 9.368637620355003e-06}, {"id": 792, "seek": 517032, "start": 5189.12, "end": 5194.12, "text": " was likely to work, which basically is like, I think that's a word.", "tokens": [390, 3700, 281, 589, 11, 597, 1936, 307, 411, 11, 286, 519, 300, 311, 257, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1909426398899244, "compression_ratio": 1.5769230769230769, "no_speech_prob": 9.368637620355003e-06}, {"id": 793, "seek": 519412, "start": 5194.12, "end": 5204.48, "text": " So I just wrote regular expressions for doing heuristic tokenization. Now you can use NLTK,", "tokens": [407, 286, 445, 4114, 3890, 15277, 337, 884, 415, 374, 3142, 14862, 2144, 13, 823, 291, 393, 764, 426, 43, 51, 42, 11], "temperature": 0.0, "avg_logprob": -0.19477774645831133, "compression_ratio": 1.4705882352941178, "no_speech_prob": 7.411194474116201e-06}, {"id": 794, "seek": 519412, "start": 5204.48, "end": 5210.5199999999995, "text": " the natural language toolkit, that has a bunch of tokenizers in. Honestly though, I actually", "tokens": [264, 3303, 2856, 40167, 11, 300, 575, 257, 3840, 295, 14862, 22525, 294, 13, 12348, 1673, 11, 286, 767], "temperature": 0.0, "avg_logprob": -0.19477774645831133, "compression_ratio": 1.4705882352941178, "no_speech_prob": 7.411194474116201e-06}, {"id": 795, "seek": 519412, "start": 5210.5199999999995, "end": 5218.64, "text": " found my hacky rules-based tokenizer, I was happier with it than any of the NLT tokenizers", "tokens": [1352, 452, 10339, 88, 4474, 12, 6032, 14862, 6545, 11, 286, 390, 20423, 365, 309, 813, 604, 295, 264, 426, 43, 51, 14862, 22525], "temperature": 0.0, "avg_logprob": -0.19477774645831133, "compression_ratio": 1.4705882352941178, "no_speech_prob": 7.411194474116201e-06}, {"id": 796, "seek": 521864, "start": 5218.64, "end": 5226.280000000001, "text": " I tried with this problem. So you can see that basically, for example, if you have any", "tokens": [286, 3031, 365, 341, 1154, 13, 407, 291, 393, 536, 300, 1936, 11, 337, 1365, 11, 498, 291, 362, 604], "temperature": 0.0, "avg_logprob": -0.1407069206237793, "compression_ratio": 1.6971153846153846, "no_speech_prob": 4.4254420572542585e-06}, {"id": 797, "seek": 521864, "start": 5226.280000000001, "end": 5233.04, "text": " letter followed by apostrophe S, then I want you to make apostrophe S a word. It's kind", "tokens": [5063, 6263, 538, 19484, 27194, 318, 11, 550, 286, 528, 291, 281, 652, 19484, 27194, 318, 257, 1349, 13, 467, 311, 733], "temperature": 0.0, "avg_logprob": -0.1407069206237793, "compression_ratio": 1.6971153846153846, "no_speech_prob": 4.4254420572542585e-06}, {"id": 798, "seek": 521864, "start": 5233.04, "end": 5239.96, "text": " of more like of than anything else. Or else if it's a letter followed by an apostrophe", "tokens": [295, 544, 411, 295, 813, 1340, 1646, 13, 1610, 1646, 498, 309, 311, 257, 5063, 6263, 538, 364, 19484, 27194], "temperature": 0.0, "avg_logprob": -0.1407069206237793, "compression_ratio": 1.6971153846153846, "no_speech_prob": 4.4254420572542585e-06}, {"id": 799, "seek": 521864, "start": 5239.96, "end": 5245.160000000001, "text": " followed by a letter, that's probably French, in which case everything after the apostrophe", "tokens": [6263, 538, 257, 5063, 11, 300, 311, 1391, 5522, 11, 294, 597, 1389, 1203, 934, 264, 19484, 27194], "temperature": 0.0, "avg_logprob": -0.1407069206237793, "compression_ratio": 1.6971153846153846, "no_speech_prob": 4.4254420572542585e-06}, {"id": 800, "seek": 524516, "start": 5245.16, "end": 5250.44, "text": " is one word and everything after it is another word. So you can basically see it here. Tokenize", "tokens": [307, 472, 1349, 293, 1203, 934, 309, 307, 1071, 1349, 13, 407, 291, 393, 1936, 536, 309, 510, 13, 314, 8406, 1125], "temperature": 0.0, "avg_logprob": -0.23680385888791552, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.3845777175447438e-05}, {"id": 801, "seek": 524516, "start": 5250.44, "end": 5260.92, "text": " the French, que, es, que, la, moumiere. That is exactly what I want. And here's the English.", "tokens": [264, 5522, 11, 631, 11, 785, 11, 631, 11, 635, 11, 275, 263, 3057, 323, 13, 663, 307, 2293, 437, 286, 528, 13, 400, 510, 311, 264, 3669, 13], "temperature": 0.0, "avg_logprob": -0.23680385888791552, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.3845777175447438e-05}, {"id": 802, "seek": 524516, "start": 5260.92, "end": 5266.16, "text": " And so then let's test a very accurate statement like Rachel's baby is cuter than others's.", "tokens": [400, 370, 550, 718, 311, 1500, 257, 588, 8559, 5629, 411, 14246, 311, 3186, 307, 1723, 260, 813, 2357, 311, 13], "temperature": 0.0, "avg_logprob": -0.23680385888791552, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.3845777175447438e-05}, {"id": 803, "seek": 524516, "start": 5266.16, "end": 5270.639999999999, "text": " And you can see here the apostrophe S's doing the right thing as opposed to the apostrophe", "tokens": [400, 291, 393, 536, 510, 264, 19484, 27194, 318, 311, 884, 264, 558, 551, 382, 8851, 281, 264, 19484, 27194], "temperature": 0.0, "avg_logprob": -0.23680385888791552, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.3845777175447438e-05}, {"id": 804, "seek": 527064, "start": 5270.64, "end": 5276.200000000001, "text": " in the middle. So check out my tokenizer, all looks good. Makes accurate statements", "tokens": [294, 264, 2808, 13, 407, 1520, 484, 452, 14862, 6545, 11, 439, 1542, 665, 13, 25245, 8559, 12363], "temperature": 0.0, "avg_logprob": -0.1705859841652287, "compression_ratio": 1.5903614457831325, "no_speech_prob": 1.130051077780081e-05}, {"id": 805, "seek": 527064, "start": 5276.200000000001, "end": 5279.92, "text": " about Rachel's baby as well, so all good.", "tokens": [466, 14246, 311, 3186, 382, 731, 11, 370, 439, 665, 13], "temperature": 0.0, "avg_logprob": -0.1705859841652287, "compression_ratio": 1.5903614457831325, "no_speech_prob": 1.130051077780081e-05}, {"id": 806, "seek": 527064, "start": 5279.92, "end": 5284.320000000001, "text": " So now that we've got that tokenizing, we can go ahead and do the standard thing that", "tokens": [407, 586, 300, 321, 600, 658, 300, 14862, 3319, 11, 321, 393, 352, 2286, 293, 360, 264, 3832, 551, 300], "temperature": 0.0, "avg_logprob": -0.1705859841652287, "compression_ratio": 1.5903614457831325, "no_speech_prob": 1.130051077780081e-05}, {"id": 807, "seek": 527064, "start": 5284.320000000001, "end": 5288.6, "text": " we do every time we work with an LP, which is to turn our list of words into a list of", "tokens": [321, 360, 633, 565, 321, 589, 365, 364, 38095, 11, 597, 307, 281, 1261, 527, 1329, 295, 2283, 666, 257, 1329, 295], "temperature": 0.0, "avg_logprob": -0.1705859841652287, "compression_ratio": 1.5903614457831325, "no_speech_prob": 1.130051077780081e-05}, {"id": 808, "seek": 527064, "start": 5288.6, "end": 5294.160000000001, "text": " numbers. We always do it the same way basically. Create our vocabulary, what are all the possible", "tokens": [3547, 13, 492, 1009, 360, 309, 264, 912, 636, 1936, 13, 20248, 527, 19864, 11, 437, 366, 439, 264, 1944], "temperature": 0.0, "avg_logprob": -0.1705859841652287, "compression_ratio": 1.5903614457831325, "no_speech_prob": 1.130051077780081e-05}, {"id": 809, "seek": 529416, "start": 5294.16, "end": 5301.72, "text": " words and how often they appear. Insert a padding character, insert the start of stream", "tokens": [2283, 293, 577, 2049, 436, 4204, 13, 36487, 257, 39562, 2517, 11, 8969, 264, 722, 295, 4309], "temperature": 0.0, "avg_logprob": -0.21100102771412244, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.3845728062733542e-05}, {"id": 810, "seek": 529416, "start": 5301.72, "end": 5309.08, "text": " character, this is like the asterisk in the previous one. Create the reverse mapping from", "tokens": [2517, 11, 341, 307, 411, 264, 257, 3120, 7797, 294, 264, 3894, 472, 13, 20248, 264, 9943, 18350, 490], "temperature": 0.0, "avg_logprob": -0.21100102771412244, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.3845728062733542e-05}, {"id": 811, "seek": 529416, "start": 5309.08, "end": 5314.48, "text": " word to id by using this little enumerate trick. And then go through every sentence", "tokens": [1349, 281, 4496, 538, 1228, 341, 707, 465, 15583, 473, 4282, 13, 400, 550, 352, 807, 633, 8174], "temperature": 0.0, "avg_logprob": -0.21100102771412244, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.3845728062733542e-05}, {"id": 812, "seek": 529416, "start": 5314.48, "end": 5319.68, "text": " and turn it into a list of tokens by calling that dictionary. And you have a question behind", "tokens": [293, 1261, 309, 666, 257, 1329, 295, 22667, 538, 5141, 300, 25890, 13, 400, 291, 362, 257, 1168, 2261], "temperature": 0.0, "avg_logprob": -0.21100102771412244, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.3845728062733542e-05}, {"id": 813, "seek": 529416, "start": 5319.68, "end": 5320.68, "text": " you Rachel.", "tokens": [291, 14246, 13], "temperature": 0.0, "avg_logprob": -0.21100102771412244, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.3845728062733542e-05}, {"id": 814, "seek": 532068, "start": 5320.68, "end": 5326.400000000001, "text": " Question asked. Do you need a stemmer before you convert into numbers?", "tokens": [14464, 2351, 13, 1144, 291, 643, 257, 12312, 936, 949, 291, 7620, 666, 3547, 30], "temperature": 0.0, "avg_logprob": -0.24230127334594725, "compression_ratio": 1.5942028985507246, "no_speech_prob": 1.568926927575376e-05}, {"id": 815, "seek": 532068, "start": 5326.400000000001, "end": 5335.92, "text": " Answer. No, not at all. Different words with different stems are different words and have", "tokens": [24545, 13, 883, 11, 406, 412, 439, 13, 20825, 2283, 365, 819, 27600, 366, 819, 2283, 293, 362], "temperature": 0.0, "avg_logprob": -0.24230127334594725, "compression_ratio": 1.5942028985507246, "no_speech_prob": 1.568926927575376e-05}, {"id": 816, "seek": 532068, "start": 5335.92, "end": 5345.52, "text": " different translations. So we want to keep that whole thing. The tokenizer really is", "tokens": [819, 37578, 13, 407, 321, 528, 281, 1066, 300, 1379, 551, 13, 440, 14862, 6545, 534, 307], "temperature": 0.0, "avg_logprob": -0.24230127334594725, "compression_ratio": 1.5942028985507246, "no_speech_prob": 1.568926927575376e-05}, {"id": 817, "seek": 532068, "start": 5345.52, "end": 5348.92, "text": " to do something that we think we can do in a purely rules-based way. The question of", "tokens": [281, 360, 746, 300, 321, 519, 321, 393, 360, 294, 257, 17491, 4474, 12, 6032, 636, 13, 440, 1168, 295], "temperature": 0.0, "avg_logprob": -0.24230127334594725, "compression_ratio": 1.5942028985507246, "no_speech_prob": 1.568926927575376e-05}, {"id": 818, "seek": 534892, "start": 5348.92, "end": 5353.92, "text": " how do we deal with morphological differences is actually highly complex, varies a lot by", "tokens": [577, 360, 321, 2028, 365, 25778, 4383, 7300, 307, 767, 5405, 3997, 11, 21716, 257, 688, 538], "temperature": 0.0, "avg_logprob": -0.23014169269137913, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944185371277854e-06}, {"id": 819, "seek": 534892, "start": 5353.92, "end": 5358.36, "text": " language and we want to learn it in new or now.", "tokens": [2856, 293, 321, 528, 281, 1466, 309, 294, 777, 420, 586, 13], "temperature": 0.0, "avg_logprob": -0.23014169269137913, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944185371277854e-06}, {"id": 820, "seek": 534892, "start": 5358.36, "end": 5366.08, "text": " So this is going to end up returning the list of id's for each sentence, the vocabulary,", "tokens": [407, 341, 307, 516, 281, 917, 493, 12678, 264, 1329, 295, 4496, 311, 337, 1184, 8174, 11, 264, 19864, 11], "temperature": 0.0, "avg_logprob": -0.23014169269137913, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944185371277854e-06}, {"id": 821, "seek": 534892, "start": 5366.08, "end": 5373.6, "text": " the reverse vocabulary, and just the frequency counts for the vocabulary.", "tokens": [264, 9943, 19864, 11, 293, 445, 264, 7893, 14893, 337, 264, 19864, 13], "temperature": 0.0, "avg_logprob": -0.23014169269137913, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944185371277854e-06}, {"id": 822, "seek": 537360, "start": 5373.6, "end": 5382.0, "text": " Next step is to turn these words into word vectors. Earlier on we used Word2Vec because", "tokens": [3087, 1823, 307, 281, 1261, 613, 2283, 666, 1349, 18875, 13, 24552, 322, 321, 1143, 8725, 17, 53, 3045, 570], "temperature": 0.0, "avg_logprob": -0.1900471473226742, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.2606751624844037e-05}, {"id": 823, "seek": 537360, "start": 5382.0, "end": 5387.68, "text": " Word2Vec has these multi-word things. But for translation, I don't want multi-word things,", "tokens": [8725, 17, 53, 3045, 575, 613, 4825, 12, 7462, 721, 13, 583, 337, 12853, 11, 286, 500, 380, 528, 4825, 12, 7462, 721, 11], "temperature": 0.0, "avg_logprob": -0.1900471473226742, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.2606751624844037e-05}, {"id": 824, "seek": 537360, "start": 5387.68, "end": 5392.64, "text": " I want single-word things. So that's why I'm going to use GloVe. So go ahead and turn that", "tokens": [286, 528, 2167, 12, 7462, 721, 13, 407, 300, 311, 983, 286, 478, 516, 281, 764, 10786, 53, 68, 13, 407, 352, 2286, 293, 1261, 300], "temperature": 0.0, "avg_logprob": -0.1900471473226742, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.2606751624844037e-05}, {"id": 825, "seek": 537360, "start": 5392.64, "end": 5399.8, "text": " into a dictionary from the word to the word vector. Also grab some French word vectors.", "tokens": [666, 257, 25890, 490, 264, 1349, 281, 264, 1349, 8062, 13, 2743, 4444, 512, 5522, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1900471473226742, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.2606751624844037e-05}, {"id": 826, "seek": 539980, "start": 5399.8, "end": 5406.8, "text": " I found this really fantastic site that's got some nice French word vectors.", "tokens": [286, 1352, 341, 534, 5456, 3621, 300, 311, 658, 512, 1481, 5522, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.17460483102237476, "compression_ratio": 1.6066350710900474, "no_speech_prob": 5.144093302078545e-05}, {"id": 827, "seek": 539980, "start": 5406.8, "end": 5413.56, "text": " So now go ahead and build a little thing that can go through my vocabulary, create a big", "tokens": [407, 586, 352, 2286, 293, 1322, 257, 707, 551, 300, 393, 352, 807, 452, 19864, 11, 1884, 257, 955], "temperature": 0.0, "avg_logprob": -0.17460483102237476, "compression_ratio": 1.6066350710900474, "no_speech_prob": 5.144093302078545e-05}, {"id": 828, "seek": 539980, "start": 5413.56, "end": 5419.92, "text": " array of zeros, and then go ahead and fill in all of those zeros with word vectors, if", "tokens": [10225, 295, 35193, 11, 293, 550, 352, 2286, 293, 2836, 294, 439, 295, 729, 35193, 365, 1349, 18875, 11, 498], "temperature": 0.0, "avg_logprob": -0.17460483102237476, "compression_ratio": 1.6066350710900474, "no_speech_prob": 5.144093302078545e-05}, {"id": 829, "seek": 539980, "start": 5419.92, "end": 5426.16, "text": " you can. Of course sometimes you look up a word and it's not in your word vector list,", "tokens": [291, 393, 13, 2720, 1164, 2171, 291, 574, 493, 257, 1349, 293, 309, 311, 406, 294, 428, 1349, 8062, 1329, 11], "temperature": 0.0, "avg_logprob": -0.17460483102237476, "compression_ratio": 1.6066350710900474, "no_speech_prob": 5.144093302078545e-05}, {"id": 830, "seek": 542616, "start": 5426.16, "end": 5430.68, "text": " in which case you can just stick a random vector in there instead. So this is all stuff", "tokens": [294, 597, 1389, 291, 393, 445, 2897, 257, 4974, 8062, 294, 456, 2602, 13, 407, 341, 307, 439, 1507], "temperature": 0.0, "avg_logprob": -0.19111692125552168, "compression_ratio": 1.5614754098360655, "no_speech_prob": 1.3419797141978052e-05}, {"id": 831, "seek": 542616, "start": 5430.68, "end": 5436.12, "text": " we've done many, many times now.", "tokens": [321, 600, 1096, 867, 11, 867, 1413, 586, 13], "temperature": 0.0, "avg_logprob": -0.19111692125552168, "compression_ratio": 1.5614754098360655, "no_speech_prob": 1.3419797141978052e-05}, {"id": 832, "seek": 542616, "start": 5436.12, "end": 5440.72, "text": " I also am keeping track of how often I find the word, just to make sure. For English,", "tokens": [286, 611, 669, 5145, 2837, 295, 577, 2049, 286, 915, 264, 1349, 11, 445, 281, 652, 988, 13, 1171, 3669, 11], "temperature": 0.0, "avg_logprob": -0.19111692125552168, "compression_ratio": 1.5614754098360655, "no_speech_prob": 1.3419797141978052e-05}, {"id": 833, "seek": 542616, "start": 5440.72, "end": 5447.8, "text": " out of 19.5 thousand in the vocab, we find 17.2 thousand word vectors in GloVe, so this", "tokens": [484, 295, 1294, 13, 20, 4714, 294, 264, 2329, 455, 11, 321, 915, 3282, 13, 17, 4714, 1349, 18875, 294, 10786, 53, 68, 11, 370, 341], "temperature": 0.0, "avg_logprob": -0.19111692125552168, "compression_ratio": 1.5614754098360655, "no_speech_prob": 1.3419797141978052e-05}, {"id": 834, "seek": 542616, "start": 5447.8, "end": 5452.599999999999, "text": " is looking pretty good. So most of our word vectors are being found. And for French, a", "tokens": [307, 1237, 1238, 665, 13, 407, 881, 295, 527, 1349, 18875, 366, 885, 1352, 13, 400, 337, 5522, 11, 257], "temperature": 0.0, "avg_logprob": -0.19111692125552168, "compression_ratio": 1.5614754098360655, "no_speech_prob": 1.3419797141978052e-05}, {"id": 835, "seek": 545260, "start": 5452.6, "end": 5457.76, "text": " little bit less, but still not bad. That's probably because of my particular tokenization", "tokens": [707, 857, 1570, 11, 457, 920, 406, 1578, 13, 663, 311, 1391, 570, 295, 452, 1729, 14862, 2144], "temperature": 0.0, "avg_logprob": -0.31029040571572125, "compression_ratio": 1.5508021390374331, "no_speech_prob": 4.0692735638003796e-05}, {"id": 836, "seek": 545260, "start": 5457.76, "end": 5461.360000000001, "text": " strategy. It might have been different to the tokenization strategy they used for these", "tokens": [5206, 13, 467, 1062, 362, 668, 819, 281, 264, 14862, 2144, 5206, 436, 1143, 337, 613], "temperature": 0.0, "avg_logprob": -0.31029040571572125, "compression_ratio": 1.5508021390374331, "no_speech_prob": 4.0692735638003796e-05}, {"id": 837, "seek": 545260, "start": 5461.360000000001, "end": 5462.360000000001, "text": " word vectors.", "tokens": [1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.31029040571572125, "compression_ratio": 1.5508021390374331, "no_speech_prob": 4.0692735638003796e-05}, {"id": 838, "seek": 545260, "start": 5462.360000000001, "end": 5466.400000000001, "text": " Question from the audience. You still have the audio popping, crackling problem from", "tokens": [14464, 490, 264, 4034, 13, 509, 920, 362, 264, 6278, 18374, 11, 6226, 1688, 1154, 490], "temperature": 0.0, "avg_logprob": -0.31029040571572125, "compression_ratio": 1.5508021390374331, "no_speech_prob": 4.0692735638003796e-05}, {"id": 839, "seek": 545260, "start": 5466.400000000001, "end": 5469.400000000001, "text": " time to time.", "tokens": [565, 281, 565, 13], "temperature": 0.0, "avg_logprob": -0.31029040571572125, "compression_ratio": 1.5508021390374331, "no_speech_prob": 4.0692735638003796e-05}, {"id": 840, "seek": 546940, "start": 5469.4, "end": 5490.5599999999995, "text": " Of course the other thing we have to do with NLP is to make everything rectangular. Notice", "tokens": [2720, 1164, 264, 661, 551, 321, 362, 281, 360, 365, 426, 45196, 307, 281, 652, 1203, 31167, 13, 13428], "temperature": 0.0, "avg_logprob": -0.14923263103403944, "compression_ratio": 1.3161764705882353, "no_speech_prob": 1.8342287148698233e-05}, {"id": 841, "seek": 546940, "start": 5490.5599999999995, "end": 5496.36, "text": " here I'm calling a Keras function. If Keras does something you need, please use it, even", "tokens": [510, 286, 478, 5141, 257, 591, 6985, 2445, 13, 759, 591, 6985, 775, 746, 291, 643, 11, 1767, 764, 309, 11, 754], "temperature": 0.0, "avg_logprob": -0.14923263103403944, "compression_ratio": 1.3161764705882353, "no_speech_prob": 1.8342287148698233e-05}, {"id": 842, "seek": 549636, "start": 5496.36, "end": 5503.24, "text": " if you're in PyTorch, it's PyN. I've heard a number of people say, oh I'm trying to use", "tokens": [498, 291, 434, 294, 9953, 51, 284, 339, 11, 309, 311, 9953, 45, 13, 286, 600, 2198, 257, 1230, 295, 561, 584, 11, 1954, 286, 478, 1382, 281, 764], "temperature": 0.0, "avg_logprob": -0.2898410373263889, "compression_ratio": 1.4894736842105263, "no_speech_prob": 6.0139456763863564e-05}, {"id": 843, "seek": 549636, "start": 5503.24, "end": 5507.759999999999, "text": " PyTorch, but I hate that it doesn't have pad sequences. If you import pad sequences, it", "tokens": [9953, 51, 284, 339, 11, 457, 286, 4700, 300, 309, 1177, 380, 362, 6887, 22978, 13, 759, 291, 974, 6887, 22978, 11, 309], "temperature": 0.0, "avg_logprob": -0.2898410373263889, "compression_ratio": 1.4894736842105263, "no_speech_prob": 6.0139456763863564e-05}, {"id": 844, "seek": 549636, "start": 5507.759999999999, "end": 5510.759999999999, "text": " has pad sequences.", "tokens": [575, 6887, 22978, 13], "temperature": 0.0, "avg_logprob": -0.2898410373263889, "compression_ratio": 1.4894736842105263, "no_speech_prob": 6.0139456763863564e-05}, {"id": 845, "seek": 549636, "start": 5510.759999999999, "end": 5522.5199999999995, "text": " Train test split to grab 10% of the data. Train versus test. And here we have it. 47,000", "tokens": [28029, 1500, 7472, 281, 4444, 1266, 4, 295, 264, 1412, 13, 28029, 5717, 1500, 13, 400, 510, 321, 362, 309, 13, 16953, 11, 1360], "temperature": 0.0, "avg_logprob": -0.2898410373263889, "compression_ratio": 1.4894736842105263, "no_speech_prob": 6.0139456763863564e-05}, {"id": 846, "seek": 552252, "start": 5522.52, "end": 5531.120000000001, "text": " train, 5,000 test. And here's an example of a French sentence and an English sentence", "tokens": [3847, 11, 1025, 11, 1360, 1500, 13, 400, 510, 311, 364, 1365, 295, 257, 5522, 8174, 293, 364, 3669, 8174], "temperature": 0.0, "avg_logprob": -0.1986381782675689, "compression_ratio": 1.3900709219858156, "no_speech_prob": 2.026132506216527e-06}, {"id": 847, "seek": 552252, "start": 5531.120000000001, "end": 5535.400000000001, "text": " and all the padding.", "tokens": [293, 439, 264, 39562, 13], "temperature": 0.0, "avg_logprob": -0.1986381782675689, "compression_ratio": 1.3900709219858156, "no_speech_prob": 2.026132506216527e-06}, {"id": 848, "seek": 552252, "start": 5535.400000000001, "end": 5541.040000000001, "text": " So we now have to do all the data loading stuff ourselves. So I've gone ahead and created", "tokens": [407, 321, 586, 362, 281, 360, 439, 264, 1412, 15114, 1507, 4175, 13, 407, 286, 600, 2780, 2286, 293, 2942], "temperature": 0.0, "avg_logprob": -0.1986381782675689, "compression_ratio": 1.3900709219858156, "no_speech_prob": 2.026132506216527e-06}, {"id": 849, "seek": 554104, "start": 5541.04, "end": 5554.8, "text": " a getBatch function that is going to go ahead and return a random permutation of lenX. So", "tokens": [257, 483, 33, 852, 2445, 300, 307, 516, 281, 352, 2286, 293, 2736, 257, 4974, 4784, 11380, 295, 40116, 55, 13, 407], "temperature": 0.0, "avg_logprob": -0.18895645664162833, "compression_ratio": 1.5705882352941176, "no_speech_prob": 1.5206707757897675e-05}, {"id": 850, "seek": 554104, "start": 5554.8, "end": 5560.8, "text": " lenX is the length of all of our sentences. So this is going to return and then grab the", "tokens": [40116, 55, 307, 264, 4641, 295, 439, 295, 527, 16579, 13, 407, 341, 307, 516, 281, 2736, 293, 550, 4444, 264], "temperature": 0.0, "avg_logprob": -0.18895645664162833, "compression_ratio": 1.5705882352941176, "no_speech_prob": 1.5206707757897675e-05}, {"id": 851, "seek": 554104, "start": 5560.8, "end": 5569.4, "text": " first batch size of them. So this will return 16 random numbers between 0 and 47,097. So", "tokens": [700, 15245, 2744, 295, 552, 13, 407, 341, 486, 2736, 3165, 4974, 3547, 1296, 1958, 293, 16953, 11, 13811, 22, 13, 407], "temperature": 0.0, "avg_logprob": -0.18895645664162833, "compression_ratio": 1.5705882352941176, "no_speech_prob": 1.5206707757897675e-05}, {"id": 852, "seek": 556940, "start": 5569.4, "end": 5573.48, "text": " then I can just return those for English and those for French.", "tokens": [550, 286, 393, 445, 2736, 729, 337, 3669, 293, 729, 337, 5522, 13], "temperature": 0.0, "avg_logprob": -0.10142759765897479, "compression_ratio": 1.7682403433476395, "no_speech_prob": 2.902298547269311e-06}, {"id": 853, "seek": 556940, "start": 5573.48, "end": 5578.44, "text": " So it's okay if you don't have a data loader. This is all it actually takes to create a", "tokens": [407, 309, 311, 1392, 498, 291, 500, 380, 362, 257, 1412, 3677, 260, 13, 639, 307, 439, 309, 767, 2516, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.10142759765897479, "compression_ratio": 1.7682403433476395, "no_speech_prob": 2.902298547269311e-06}, {"id": 854, "seek": 556940, "start": 5578.44, "end": 5585.5199999999995, "text": " basic generator. This is basically doing the same thing as a generator if you don't need", "tokens": [3875, 19265, 13, 639, 307, 1936, 884, 264, 912, 551, 382, 257, 19265, 498, 291, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.10142759765897479, "compression_ratio": 1.7682403433476395, "no_speech_prob": 2.902298547269311e-06}, {"id": 855, "seek": 556940, "start": 5585.5199999999995, "end": 5592.28, "text": " any data augmentation. And again, here's a piece of code you can steal. If you need a", "tokens": [604, 1412, 14501, 19631, 13, 400, 797, 11, 510, 311, 257, 2522, 295, 3089, 291, 393, 11009, 13, 759, 291, 643, 257], "temperature": 0.0, "avg_logprob": -0.10142759765897479, "compression_ratio": 1.7682403433476395, "no_speech_prob": 2.902298547269311e-06}, {"id": 856, "seek": 556940, "start": 5592.28, "end": 5599.08, "text": " generator in PyTorch, here's a generator in PyTorch. Pass in your data and your labels", "tokens": [19265, 294, 9953, 51, 284, 339, 11, 510, 311, 257, 19265, 294, 9953, 51, 284, 339, 13, 10319, 294, 428, 1412, 293, 428, 16949], "temperature": 0.0, "avg_logprob": -0.10142759765897479, "compression_ratio": 1.7682403433476395, "no_speech_prob": 2.902298547269311e-06}, {"id": 857, "seek": 559908, "start": 5599.08, "end": 5613.88, "text": " and your batch size and it will return a batch of each.", "tokens": [293, 428, 15245, 2744, 293, 309, 486, 2736, 257, 15245, 295, 1184, 13], "temperature": 0.0, "avg_logprob": -0.13946000949756518, "compression_ratio": 1.2564102564102564, "no_speech_prob": 6.643405868089758e-06}, {"id": 858, "seek": 559908, "start": 5613.88, "end": 5622.32, "text": " I mentioned last time that I created broadcasting functions for PyTorch. Basically what had", "tokens": [286, 2835, 1036, 565, 300, 286, 2942, 30024, 6828, 337, 9953, 51, 284, 339, 13, 8537, 437, 632], "temperature": 0.0, "avg_logprob": -0.13946000949756518, "compression_ratio": 1.2564102564102564, "no_speech_prob": 6.643405868089758e-06}, {"id": 859, "seek": 562232, "start": 5622.32, "end": 5629.5599999999995, "text": " happened was I had all this Keras code that worked and I wanted to port it to PyTorch.", "tokens": [2011, 390, 286, 632, 439, 341, 591, 6985, 3089, 300, 2732, 293, 286, 1415, 281, 2436, 309, 281, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.14565275892426696, "compression_ratio": 1.5084745762711864, "no_speech_prob": 4.637847723643063e-06}, {"id": 860, "seek": 562232, "start": 5629.5599999999995, "end": 5634.4, "text": " And PyTorch doesn't have broadcasting, so none of this stuff worked. So my PyTorch was", "tokens": [400, 9953, 51, 284, 339, 1177, 380, 362, 30024, 11, 370, 6022, 295, 341, 1507, 2732, 13, 407, 452, 9953, 51, 284, 339, 390], "temperature": 0.0, "avg_logprob": -0.14565275892426696, "compression_ratio": 1.5084745762711864, "no_speech_prob": 4.637847723643063e-06}, {"id": 861, "seek": 562232, "start": 5634.4, "end": 5642.0, "text": " way more complex than my Keras because there was.squeeze and.unsqueeze and.expand everywhere.", "tokens": [636, 544, 3997, 813, 452, 591, 6985, 570, 456, 390, 2411, 44516, 10670, 293, 2411, 409, 44516, 10670, 293, 2411, 15952, 474, 5315, 13], "temperature": 0.0, "avg_logprob": -0.14565275892426696, "compression_ratio": 1.5084745762711864, "no_speech_prob": 4.637847723643063e-06}, {"id": 862, "seek": 564200, "start": 5642.0, "end": 5653.88, "text": " So I wrote add, subtract, multiply, divide, and dot such that they have the exact same", "tokens": [407, 286, 4114, 909, 11, 16390, 11, 12972, 11, 9845, 11, 293, 5893, 1270, 300, 436, 362, 264, 1900, 912], "temperature": 0.0, "avg_logprob": -0.1641561190287272, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.7603360902285203e-06}, {"id": 863, "seek": 564200, "start": 5653.88, "end": 5660.48, "text": " broadcasting semantics as Keras. This is actually pretty interesting and I really wish I had", "tokens": [30024, 4361, 45298, 382, 591, 6985, 13, 639, 307, 767, 1238, 1880, 293, 286, 534, 3172, 286, 632], "temperature": 0.0, "avg_logprob": -0.1641561190287272, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.7603360902285203e-06}, {"id": 864, "seek": 564200, "start": 5660.48, "end": 5663.68, "text": " time to show you, but maybe you can have a look at it during the week and ask questions", "tokens": [565, 281, 855, 291, 11, 457, 1310, 291, 393, 362, 257, 574, 412, 309, 1830, 264, 1243, 293, 1029, 1651], "temperature": 0.0, "avg_logprob": -0.1641561190287272, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.7603360902285203e-06}, {"id": 865, "seek": 564200, "start": 5663.68, "end": 5670.48, "text": " on the forum. There's so little code, so the amount of code to make all of that work is", "tokens": [322, 264, 17542, 13, 821, 311, 370, 707, 3089, 11, 370, 264, 2372, 295, 3089, 281, 652, 439, 295, 300, 589, 307], "temperature": 0.0, "avg_logprob": -0.1641561190287272, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.7603360902285203e-06}, {"id": 866, "seek": 567048, "start": 5670.48, "end": 5678.759999999999, "text": " basically that and that. It's incredibly little code.", "tokens": [1936, 300, 293, 300, 13, 467, 311, 6252, 707, 3089, 13], "temperature": 0.0, "avg_logprob": -0.17894556552548951, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.184298798762029e-05}, {"id": 867, "seek": 567048, "start": 5678.759999999999, "end": 5682.5199999999995, "text": " But importantly I also want to show you how I build this stuff. I always use test driven", "tokens": [583, 8906, 286, 611, 528, 281, 855, 291, 577, 286, 1322, 341, 1507, 13, 286, 1009, 764, 1500, 9555], "temperature": 0.0, "avg_logprob": -0.17894556552548951, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.184298798762029e-05}, {"id": 868, "seek": 567048, "start": 5682.5199999999995, "end": 5688.759999999999, "text": " development for this kind of thing. So basically I created a whole bunch of matrices, vectors,", "tokens": [3250, 337, 341, 733, 295, 551, 13, 407, 1936, 286, 2942, 257, 1379, 3840, 295, 32284, 11, 18875, 11], "temperature": 0.0, "avg_logprob": -0.17894556552548951, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.184298798762029e-05}, {"id": 869, "seek": 567048, "start": 5688.759999999999, "end": 5694.679999999999, "text": " 3-dimensional tensors and 4-dimensional tensors, transposed versions of them, wrote something", "tokens": [805, 12, 18759, 10688, 830, 293, 1017, 12, 18759, 10688, 830, 11, 7132, 1744, 9606, 295, 552, 11, 4114, 746], "temperature": 0.0, "avg_logprob": -0.17894556552548951, "compression_ratio": 1.5687203791469195, "no_speech_prob": 1.184298798762029e-05}, {"id": 870, "seek": 569468, "start": 5694.68, "end": 5700.92, "text": " that checks that the two things are the same, and then I just went ahead and tried making", "tokens": [300, 13834, 300, 264, 732, 721, 366, 264, 912, 11, 293, 550, 286, 445, 1437, 2286, 293, 3031, 1455], "temperature": 0.0, "avg_logprob": -0.190333762865388, "compression_ratio": 1.7881773399014778, "no_speech_prob": 1.5206641364784446e-05}, {"id": 871, "seek": 569468, "start": 5700.92, "end": 5706.360000000001, "text": " sure that all of these things ought to be the same as all of these other things. First", "tokens": [988, 300, 439, 295, 613, 721, 13416, 281, 312, 264, 912, 382, 439, 295, 613, 661, 721, 13, 2386], "temperature": 0.0, "avg_logprob": -0.190333762865388, "compression_ratio": 1.7881773399014778, "no_speech_prob": 1.5206641364784446e-05}, {"id": 872, "seek": 569468, "start": 5706.360000000001, "end": 5714.280000000001, "text": " of all I wrote all the tests, and then I gradually went through and put in code so that the tests", "tokens": [295, 439, 286, 4114, 439, 264, 6921, 11, 293, 550, 286, 13145, 1437, 807, 293, 829, 294, 3089, 370, 300, 264, 6921], "temperature": 0.0, "avg_logprob": -0.190333762865388, "compression_ratio": 1.7881773399014778, "no_speech_prob": 1.5206641364784446e-05}, {"id": 873, "seek": 569468, "start": 5714.280000000001, "end": 5720.400000000001, "text": " started passing. And then I went back and kept refactoring the code until it was simpler", "tokens": [1409, 8437, 13, 400, 550, 286, 1437, 646, 293, 4305, 1895, 578, 3662, 264, 3089, 1826, 309, 390, 18587], "temperature": 0.0, "avg_logprob": -0.190333762865388, "compression_ratio": 1.7881773399014778, "no_speech_prob": 1.5206641364784446e-05}, {"id": 874, "seek": 572040, "start": 5720.4, "end": 5726.799999999999, "text": " and simpler. So you can see in the end all of my functions are nice and small. What this", "tokens": [293, 18587, 13, 407, 291, 393, 536, 294, 264, 917, 439, 295, 452, 6828, 366, 1481, 293, 1359, 13, 708, 341], "temperature": 0.0, "avg_logprob": -0.17094638891387404, "compression_ratio": 1.50625, "no_speech_prob": 1.3709491213376168e-06}, {"id": 875, "seek": 572040, "start": 5726.799999999999, "end": 5733.679999999999, "text": " meant was I could now write an attention model using almost exactly the same notation as", "tokens": [4140, 390, 286, 727, 586, 2464, 364, 3202, 2316, 1228, 1920, 2293, 264, 912, 24657, 382], "temperature": 0.0, "avg_logprob": -0.17094638891387404, "compression_ratio": 1.50625, "no_speech_prob": 1.3709491213376168e-06}, {"id": 876, "seek": 572040, "start": 5733.679999999999, "end": 5742.28, "text": " before. So that was how I created these broadcasting operators.", "tokens": [949, 13, 407, 300, 390, 577, 286, 2942, 613, 30024, 19077, 13], "temperature": 0.0, "avg_logprob": -0.17094638891387404, "compression_ratio": 1.50625, "no_speech_prob": 1.3709491213376168e-06}, {"id": 877, "seek": 574228, "start": 5742.28, "end": 5752.48, "text": " So given that they exist, here is a non-attention encoder. So you can see basically all it is", "tokens": [407, 2212, 300, 436, 2514, 11, 510, 307, 257, 2107, 12, 1591, 1251, 2058, 19866, 13, 407, 291, 393, 536, 1936, 439, 309, 307], "temperature": 0.0, "avg_logprob": -0.17606354937141325, "compression_ratio": 1.5632183908045978, "no_speech_prob": 4.710857865575235e-06}, {"id": 878, "seek": 574228, "start": 5752.48, "end": 5763.8, "text": " is create some embeddings, create a GRU, and then in the forward pass run the embeddings", "tokens": [307, 1884, 512, 12240, 29432, 11, 1884, 257, 10903, 52, 11, 293, 550, 294, 264, 2128, 1320, 1190, 264, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.17606354937141325, "compression_ratio": 1.5632183908045978, "no_speech_prob": 4.710857865575235e-06}, {"id": 879, "seek": 574228, "start": 5763.8, "end": 5772.24, "text": " and then run the GRU on that. And in PyTorch, a GRU, you don't have to actually write the", "tokens": [293, 550, 1190, 264, 10903, 52, 322, 300, 13, 400, 294, 9953, 51, 284, 339, 11, 257, 10903, 52, 11, 291, 500, 380, 362, 281, 767, 2464, 264], "temperature": 0.0, "avg_logprob": -0.17606354937141325, "compression_ratio": 1.5632183908045978, "no_speech_prob": 4.710857865575235e-06}, {"id": 880, "seek": 577224, "start": 5772.24, "end": 5776.719999999999, "text": " three GRUs next to each other, you can just say number of layers equals and that's going", "tokens": [1045, 10903, 29211, 958, 281, 1184, 661, 11, 291, 393, 445, 584, 1230, 295, 7914, 6915, 293, 300, 311, 516], "temperature": 0.0, "avg_logprob": -0.18203190191468197, "compression_ratio": 1.5636363636363637, "no_speech_prob": 1.4285420547821559e-05}, {"id": 881, "seek": 577224, "start": 5776.719999999999, "end": 5787.5199999999995, "text": " to stack the GRUs on top of each other. So that's that. Pretty straightforward.", "tokens": [281, 8630, 264, 10903, 29211, 322, 1192, 295, 1184, 661, 13, 407, 300, 311, 300, 13, 10693, 15325, 13], "temperature": 0.0, "avg_logprob": -0.18203190191468197, "compression_ratio": 1.5636363636363637, "no_speech_prob": 1.4285420547821559e-05}, {"id": 882, "seek": 577224, "start": 5787.5199999999995, "end": 5797.96, "text": " The decoder is also pretty straightforward. Again, create the embeddings, create the GRU.", "tokens": [440, 979, 19866, 307, 611, 1238, 15325, 13, 3764, 11, 1884, 264, 12240, 29432, 11, 1884, 264, 10903, 52, 13], "temperature": 0.0, "avg_logprob": -0.18203190191468197, "compression_ratio": 1.5636363636363637, "no_speech_prob": 1.4285420547821559e-05}, {"id": 883, "seek": 579796, "start": 5797.96, "end": 5804.52, "text": " For the decoder, we also need a linear layer at the end, which is the correct size for", "tokens": [1171, 264, 979, 19866, 11, 321, 611, 643, 257, 8213, 4583, 412, 264, 917, 11, 597, 307, 264, 3006, 2744, 337], "temperature": 0.0, "avg_logprob": -0.19171732321552848, "compression_ratio": 1.6121495327102804, "no_speech_prob": 1.1300688129267655e-05}, {"id": 884, "seek": 579796, "start": 5804.52, "end": 5809.8, "text": " our output vocabulary. We actually have to remember for that inference time, we don't", "tokens": [527, 5598, 19864, 13, 492, 767, 362, 281, 1604, 337, 300, 38253, 565, 11, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.19171732321552848, "compression_ratio": 1.6121495327102804, "no_speech_prob": 1.1300688129267655e-05}, {"id": 885, "seek": 579796, "start": 5809.8, "end": 5814.0, "text": " just want the state, we actually want to get out something that we can do an argmax on", "tokens": [445, 528, 264, 1785, 11, 321, 767, 528, 281, 483, 484, 746, 300, 321, 393, 360, 364, 3882, 41167, 322], "temperature": 0.0, "avg_logprob": -0.19171732321552848, "compression_ratio": 1.6121495327102804, "no_speech_prob": 1.1300688129267655e-05}, {"id": 886, "seek": 579796, "start": 5814.0, "end": 5819.96, "text": " to find out which word we just translated this into. So we need this layer to turn it", "tokens": [281, 915, 484, 597, 1349, 321, 445, 16805, 341, 666, 13, 407, 321, 643, 341, 4583, 281, 1261, 309], "temperature": 0.0, "avg_logprob": -0.19171732321552848, "compression_ratio": 1.6121495327102804, "no_speech_prob": 1.1300688129267655e-05}, {"id": 887, "seek": 581996, "start": 5819.96, "end": 5829.32, "text": " into the correct size for the French vocabulary size. So the forward pass here, this is a", "tokens": [666, 264, 3006, 2744, 337, 264, 5522, 19864, 2744, 13, 407, 264, 2128, 1320, 510, 11, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.15846841726730118, "compression_ratio": 1.6325301204819278, "no_speech_prob": 7.646510312042665e-06}, {"id": 888, "seek": 581996, "start": 5829.32, "end": 5833.12, "text": " little different as you'll see when we get to where we actually use this. This is actually", "tokens": [707, 819, 382, 291, 603, 536, 562, 321, 483, 281, 689, 321, 767, 764, 341, 13, 639, 307, 767], "temperature": 0.0, "avg_logprob": -0.15846841726730118, "compression_ratio": 1.6325301204819278, "no_speech_prob": 7.646510312042665e-06}, {"id": 889, "seek": 581996, "start": 5833.12, "end": 5846.64, "text": " just a single step. So this is basically just you doing one letter at a time as you'll see", "tokens": [445, 257, 2167, 1823, 13, 407, 341, 307, 1936, 445, 291, 884, 472, 5063, 412, 257, 565, 382, 291, 603, 536], "temperature": 0.0, "avg_logprob": -0.15846841726730118, "compression_ratio": 1.6325301204819278, "no_speech_prob": 7.646510312042665e-06}, {"id": 890, "seek": 584664, "start": 5846.64, "end": 5854.4800000000005, "text": " when we get here. And here's the actual softmax on the dense layer that we created. So it's", "tokens": [562, 321, 483, 510, 13, 400, 510, 311, 264, 3539, 2787, 41167, 322, 264, 18011, 4583, 300, 321, 2942, 13, 407, 309, 311], "temperature": 0.0, "avg_logprob": -0.16469114120692424, "compression_ratio": 1.532544378698225, "no_speech_prob": 3.5008408758585574e-06}, {"id": 891, "seek": 584664, "start": 5854.4800000000005, "end": 5864.320000000001, "text": " just embedding, GRU, dense layer, softmax. And we return both the new hidden state, which", "tokens": [445, 12240, 3584, 11, 10903, 52, 11, 18011, 4583, 11, 2787, 41167, 13, 400, 321, 2736, 1293, 264, 777, 7633, 1785, 11, 597], "temperature": 0.0, "avg_logprob": -0.16469114120692424, "compression_ratio": 1.532544378698225, "no_speech_prob": 3.5008408758585574e-06}, {"id": 892, "seek": 584664, "start": 5864.320000000001, "end": 5873.4800000000005, "text": " we'll be using for the next step, as well as the actual result of the output.", "tokens": [321, 603, 312, 1228, 337, 264, 958, 1823, 11, 382, 731, 382, 264, 3539, 1874, 295, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.16469114120692424, "compression_ratio": 1.532544378698225, "no_speech_prob": 3.5008408758585574e-06}, {"id": 893, "seek": 587348, "start": 5873.48, "end": 5879.679999999999, "text": " Here's the attention decoder. And as you can see, rather than being 100 lines of code,", "tokens": [1692, 311, 264, 3202, 979, 19866, 13, 400, 382, 291, 393, 536, 11, 2831, 813, 885, 2319, 3876, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.18404424926381052, "compression_ratio": 1.5085714285714287, "no_speech_prob": 3.4465624594304245e-06}, {"id": 894, "seek": 587348, "start": 5879.679999999999, "end": 5891.04, "text": " it's a screen and a half. And the nice thing is, this is all basically the same as Keras.", "tokens": [309, 311, 257, 2568, 293, 257, 1922, 13, 400, 264, 1481, 551, 307, 11, 341, 307, 439, 1936, 264, 912, 382, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.18404424926381052, "compression_ratio": 1.5085714285714287, "no_speech_prob": 3.4465624594304245e-06}, {"id": 895, "seek": 587348, "start": 5891.04, "end": 5902.879999999999, "text": " My W123, my D23, my V, my GRU, and then my final output. And then this is basically all", "tokens": [1222, 343, 4762, 18, 11, 452, 413, 9356, 11, 452, 691, 11, 452, 10903, 52, 11, 293, 550, 452, 2572, 5598, 13, 400, 550, 341, 307, 1936, 439], "temperature": 0.0, "avg_logprob": -0.18404424926381052, "compression_ratio": 1.5085714285714287, "no_speech_prob": 3.4465624594304245e-06}, {"id": 896, "seek": 590288, "start": 5902.88, "end": 5912.6, "text": " the same as Keras as well. So I've got my dot, and add, and multiply, softmax, here's the", "tokens": [264, 912, 382, 591, 6985, 382, 731, 13, 407, 286, 600, 658, 452, 5893, 11, 293, 909, 11, 293, 12972, 11, 2787, 41167, 11, 510, 311, 264], "temperature": 0.0, "avg_logprob": -0.21435650324417374, "compression_ratio": 1.3565891472868217, "no_speech_prob": 1.451041316613555e-05}, {"id": 897, "seek": 590288, "start": 5912.6, "end": 5924.56, "text": " weighted sum, here's that cat, here's the W3, add on the bias, call the GRU, and then", "tokens": [32807, 2408, 11, 510, 311, 300, 3857, 11, 510, 311, 264, 343, 18, 11, 909, 322, 264, 12577, 11, 818, 264, 10903, 52, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.21435650324417374, "compression_ratio": 1.3565891472868217, "no_speech_prob": 1.451041316613555e-05}, {"id": 898, "seek": 592456, "start": 5924.56, "end": 5934.64, "text": " dense layer and softmax. And again, we both return the actual predictions as well as the", "tokens": [18011, 4583, 293, 2787, 41167, 13, 400, 797, 11, 321, 1293, 2736, 264, 3539, 21264, 382, 731, 382, 264], "temperature": 0.0, "avg_logprob": -0.2075654298831255, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.6797263242551708e-06}, {"id": 899, "seek": 592456, "start": 5934.64, "end": 5937.200000000001, "text": " hidden states.", "tokens": [7633, 4368, 13], "temperature": 0.0, "avg_logprob": -0.2075654298831255, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.6797263242551708e-06}, {"id": 900, "seek": 592456, "start": 5937.200000000001, "end": 5947.84, "text": " Now the thing is, we have to write our own training lab, because this is PyTorch. So", "tokens": [823, 264, 551, 307, 11, 321, 362, 281, 2464, 527, 1065, 3097, 2715, 11, 570, 341, 307, 9953, 51, 284, 339, 13, 407], "temperature": 0.0, "avg_logprob": -0.2075654298831255, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.6797263242551708e-06}, {"id": 901, "seek": 592456, "start": 5947.84, "end": 5954.200000000001, "text": " we're going to have to go ahead and do a bit more work here. So basically, here's the code", "tokens": [321, 434, 516, 281, 362, 281, 352, 2286, 293, 360, 257, 857, 544, 589, 510, 13, 407, 1936, 11, 510, 311, 264, 3089], "temperature": 0.0, "avg_logprob": -0.2075654298831255, "compression_ratio": 1.4761904761904763, "no_speech_prob": 1.6797263242551708e-06}, {"id": 902, "seek": 595420, "start": 5954.2, "end": 5961.12, "text": " that trains an epoch. Create an optimizer for the encoder, an optimizer for the decoder.", "tokens": [300, 16329, 364, 30992, 339, 13, 20248, 364, 5028, 6545, 337, 264, 2058, 19866, 11, 364, 5028, 6545, 337, 264, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.25160806485922027, "compression_ratio": 1.6196581196581197, "no_speech_prob": 4.1334686102345586e-05}, {"id": 903, "seek": 595420, "start": 5961.12, "end": 5966.639999999999, "text": " The criterion is what they call the loss function, negative log likelihood, that's the same as", "tokens": [440, 46691, 307, 437, 436, 818, 264, 4470, 2445, 11, 3671, 3565, 22119, 11, 300, 311, 264, 912, 382], "temperature": 0.0, "avg_logprob": -0.25160806485922027, "compression_ratio": 1.6196581196581197, "no_speech_prob": 4.1334686102345586e-05}, {"id": 904, "seek": 595420, "start": 5966.639999999999, "end": 5967.639999999999, "text": " the cross-entropy.", "tokens": [264, 3278, 12, 317, 27514, 13], "temperature": 0.0, "avg_logprob": -0.25160806485922027, "compression_ratio": 1.6196581196581197, "no_speech_prob": 4.1334686102345586e-05}, {"id": 905, "seek": 595420, "start": 5967.639999999999, "end": 5973.16, "text": " Here's that get-batch function we created ourselves earlier to grab one batch of French", "tokens": [1692, 311, 300, 483, 12, 65, 852, 2445, 321, 2942, 4175, 3071, 281, 4444, 472, 15245, 295, 5522], "temperature": 0.0, "avg_logprob": -0.25160806485922027, "compression_ratio": 1.6196581196581197, "no_speech_prob": 4.1334686102345586e-05}, {"id": 906, "seek": 595420, "start": 5973.16, "end": 5979.84, "text": " and English, put them onto the GPU, and then call train, we'll look at that in a moment,", "tokens": [293, 3669, 11, 829, 552, 3911, 264, 18407, 11, 293, 550, 818, 3847, 11, 321, 603, 574, 412, 300, 294, 257, 1623, 11], "temperature": 0.0, "avg_logprob": -0.25160806485922027, "compression_ratio": 1.6196581196581197, "no_speech_prob": 4.1334686102345586e-05}, {"id": 907, "seek": 597984, "start": 5979.84, "end": 5985.360000000001, "text": " check out the loss, from time to time print out the loss. So all the work's actually happening", "tokens": [1520, 484, 264, 4470, 11, 490, 565, 281, 565, 4482, 484, 264, 4470, 13, 407, 439, 264, 589, 311, 767, 2737], "temperature": 0.0, "avg_logprob": -0.24864485263824462, "compression_ratio": 1.5449735449735449, "no_speech_prob": 4.7108974285947625e-06}, {"id": 908, "seek": 597984, "start": 5985.360000000001, "end": 5992.4400000000005, "text": " in train. So each of these things is less than a screen of code, but still we had to", "tokens": [294, 3847, 13, 407, 1184, 295, 613, 721, 307, 1570, 813, 257, 2568, 295, 3089, 11, 457, 920, 321, 632, 281], "temperature": 0.0, "avg_logprob": -0.24864485263824462, "compression_ratio": 1.5449735449735449, "no_speech_prob": 4.7108974285947625e-06}, {"id": 909, "seek": 597984, "start": 5992.4400000000005, "end": 5995.84, "text": " write it ourselves.", "tokens": [2464, 309, 4175, 13], "temperature": 0.0, "avg_logprob": -0.24864485263824462, "compression_ratio": 1.5449735449735449, "no_speech_prob": 4.7108974285947625e-06}, {"id": 910, "seek": 597984, "start": 5995.84, "end": 6005.8, "text": " So encode our decoder and encoder. And remember with PyTorch, you have to manually call zero", "tokens": [407, 2058, 1429, 527, 979, 19866, 293, 2058, 19866, 13, 400, 1604, 365, 9953, 51, 284, 339, 11, 291, 362, 281, 16945, 818, 4018], "temperature": 0.0, "avg_logprob": -0.24864485263824462, "compression_ratio": 1.5449735449735449, "no_speech_prob": 4.7108974285947625e-06}, {"id": 911, "seek": 600580, "start": 6005.8, "end": 6011.4800000000005, "text": " grad. So you have to zero out the gradient at the start of your training loop, and then", "tokens": [2771, 13, 407, 291, 362, 281, 4018, 484, 264, 16235, 412, 264, 722, 295, 428, 3097, 6367, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.16738584306504992, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.6701244021533057e-05}, {"id": 912, "seek": 600580, "start": 6011.4800000000005, "end": 6021.08, "text": " go through each word in your target, so we've already encoded it, and call the decoder,", "tokens": [352, 807, 1184, 1349, 294, 428, 3779, 11, 370, 321, 600, 1217, 2058, 12340, 309, 11, 293, 818, 264, 979, 19866, 11], "temperature": 0.0, "avg_logprob": -0.16738584306504992, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.6701244021533057e-05}, {"id": 913, "seek": 600580, "start": 6021.08, "end": 6031.4400000000005, "text": " passing in the decoder input, the hidden state, and the encoder outputs. So then next decoder", "tokens": [8437, 294, 264, 979, 19866, 4846, 11, 264, 7633, 1785, 11, 293, 264, 2058, 19866, 23930, 13, 407, 550, 958, 979, 19866], "temperature": 0.0, "avg_logprob": -0.16738584306504992, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.6701244021533057e-05}, {"id": 914, "seek": 603144, "start": 6031.44, "end": 6037.96, "text": " input comes out from there, key check of the loss, and then we have to call dot backward", "tokens": [4846, 1487, 484, 490, 456, 11, 2141, 1520, 295, 264, 4470, 11, 293, 550, 321, 362, 281, 818, 5893, 23897], "temperature": 0.0, "avg_logprob": -0.1796791006665711, "compression_ratio": 1.6653386454183268, "no_speech_prob": 3.187544734828407e-06}, {"id": 915, "seek": 603144, "start": 6037.96, "end": 6044.5599999999995, "text": " manually, we have to call the optimizer set manually, and return it.", "tokens": [16945, 11, 321, 362, 281, 818, 264, 5028, 6545, 992, 16945, 11, 293, 2736, 309, 13], "temperature": 0.0, "avg_logprob": -0.1796791006665711, "compression_ratio": 1.6653386454183268, "no_speech_prob": 3.187544734828407e-06}, {"id": 916, "seek": 603144, "start": 6044.5599999999995, "end": 6049.86, "text": " So it's not very interesting code, and it's the kind of stuff which I suspect hopefully", "tokens": [407, 309, 311, 406, 588, 1880, 3089, 11, 293, 309, 311, 264, 733, 295, 1507, 597, 286, 9091, 4696], "temperature": 0.0, "avg_logprob": -0.1796791006665711, "compression_ratio": 1.6653386454183268, "no_speech_prob": 3.187544734828407e-06}, {"id": 917, "seek": 603144, "start": 6049.86, "end": 6054.48, "text": " the PyTorch community and maybe some of us will all be able to contribute to getting", "tokens": [264, 9953, 51, 284, 339, 1768, 293, 1310, 512, 295, 505, 486, 439, 312, 1075, 281, 10586, 281, 1242], "temperature": 0.0, "avg_logprob": -0.1796791006665711, "compression_ratio": 1.6653386454183268, "no_speech_prob": 3.187544734828407e-06}, {"id": 918, "seek": 603144, "start": 6054.48, "end": 6060.2, "text": " rid of all this boilerplate kind of code, kind of Keras style, over time. I'm sure that", "tokens": [3973, 295, 439, 341, 39228, 37008, 733, 295, 3089, 11, 733, 295, 591, 6985, 3758, 11, 670, 565, 13, 286, 478, 988, 300], "temperature": 0.0, "avg_logprob": -0.1796791006665711, "compression_ratio": 1.6653386454183268, "no_speech_prob": 3.187544734828407e-06}, {"id": 919, "seek": 606020, "start": 6060.2, "end": 6063.2, "text": " will happen.", "tokens": [486, 1051, 13], "temperature": 0.0, "avg_logprob": -0.1802082435757506, "compression_ratio": 1.7028301886792452, "no_speech_prob": 6.438952368625905e-06}, {"id": 920, "seek": 606020, "start": 6063.2, "end": 6068.599999999999, "text": " For now, there's the code. Now that it's there, we can create our encoder, we can create our", "tokens": [1171, 586, 11, 456, 311, 264, 3089, 13, 823, 300, 309, 311, 456, 11, 321, 393, 1884, 527, 2058, 19866, 11, 321, 393, 1884, 527], "temperature": 0.0, "avg_logprob": -0.1802082435757506, "compression_ratio": 1.7028301886792452, "no_speech_prob": 6.438952368625905e-06}, {"id": 921, "seek": 606020, "start": 6068.599999999999, "end": 6075.5199999999995, "text": " attention decoder, we can train it for a while, and then we can test it.", "tokens": [3202, 979, 19866, 11, 321, 393, 3847, 309, 337, 257, 1339, 11, 293, 550, 321, 393, 1500, 309, 13], "temperature": 0.0, "avg_logprob": -0.1802082435757506, "compression_ratio": 1.7028301886792452, "no_speech_prob": 6.438952368625905e-06}, {"id": 922, "seek": 606020, "start": 6075.5199999999995, "end": 6081.32, "text": " Now for testing, I need another function because I want to turn off teacher forcing. So you'll", "tokens": [823, 337, 4997, 11, 286, 643, 1071, 2445, 570, 286, 528, 281, 1261, 766, 5027, 337, 2175, 13, 407, 291, 603], "temperature": 0.0, "avg_logprob": -0.1802082435757506, "compression_ratio": 1.7028301886792452, "no_speech_prob": 6.438952368625905e-06}, {"id": 923, "seek": 606020, "start": 6081.32, "end": 6086.5199999999995, "text": " see in this function, this is not very well refactored, I've copied some code here, but", "tokens": [536, 294, 341, 2445, 11, 341, 307, 406, 588, 731, 1895, 578, 2769, 11, 286, 600, 25365, 512, 3089, 510, 11, 457], "temperature": 0.0, "avg_logprob": -0.1802082435757506, "compression_ratio": 1.7028301886792452, "no_speech_prob": 6.438952368625905e-06}, {"id": 924, "seek": 608652, "start": 6086.52, "end": 6094.52, "text": " basically I encode and then I go through my target length, call the decoder. But look", "tokens": [1936, 286, 2058, 1429, 293, 550, 286, 352, 807, 452, 3779, 4641, 11, 818, 264, 979, 19866, 13, 583, 574], "temperature": 0.0, "avg_logprob": -0.22186453263838213, "compression_ratio": 1.6682926829268292, "no_speech_prob": 9.223384950018954e-06}, {"id": 925, "seek": 608652, "start": 6094.52, "end": 6101.88, "text": " here, I now take my decoder output and find the top one. So this is basically argmax.", "tokens": [510, 11, 286, 586, 747, 452, 979, 19866, 5598, 293, 915, 264, 1192, 472, 13, 407, 341, 307, 1936, 3882, 41167, 13], "temperature": 0.0, "avg_logprob": -0.22186453263838213, "compression_ratio": 1.6682926829268292, "no_speech_prob": 9.223384950018954e-06}, {"id": 926, "seek": 608652, "start": 6101.88, "end": 6107.8, "text": " This is saying, what word did we predict? So we're not using teacher forcing, we're", "tokens": [639, 307, 1566, 11, 437, 1349, 630, 321, 6069, 30, 407, 321, 434, 406, 1228, 5027, 337, 2175, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.22186453263838213, "compression_ratio": 1.6682926829268292, "no_speech_prob": 9.223384950018954e-06}, {"id": 927, "seek": 608652, "start": 6107.8, "end": 6112.200000000001, "text": " not saying what was the actual word, because that would be cheating, we're saying what", "tokens": [406, 1566, 437, 390, 264, 3539, 1349, 11, 570, 300, 576, 312, 18309, 11, 321, 434, 1566, 437], "temperature": 0.0, "avg_logprob": -0.22186453263838213, "compression_ratio": 1.6682926829268292, "no_speech_prob": 9.223384950018954e-06}, {"id": 928, "seek": 611220, "start": 6112.2, "end": 6121.12, "text": " word did we predict. So that now becomes the input to the next loop. So this is how we've", "tokens": [1349, 630, 321, 6069, 13, 407, 300, 586, 3643, 264, 4846, 281, 264, 958, 6367, 13, 407, 341, 307, 577, 321, 600], "temperature": 0.0, "avg_logprob": -0.09850208046510048, "compression_ratio": 1.7008928571428572, "no_speech_prob": 2.1233724964986322e-06}, {"id": 929, "seek": 611220, "start": 6121.12, "end": 6123.88, "text": " turned off teacher forcing.", "tokens": [3574, 766, 5027, 337, 2175, 13], "temperature": 0.0, "avg_logprob": -0.09850208046510048, "compression_ratio": 1.7008928571428572, "no_speech_prob": 2.1233724964986322e-06}, {"id": 930, "seek": 611220, "start": 6123.88, "end": 6129.5599999999995, "text": " So the kind of exercise for one or more of you guys this week, if you're interested,", "tokens": [407, 264, 733, 295, 5380, 337, 472, 420, 544, 295, 291, 1074, 341, 1243, 11, 498, 291, 434, 3102, 11], "temperature": 0.0, "avg_logprob": -0.09850208046510048, "compression_ratio": 1.7008928571428572, "no_speech_prob": 2.1233724964986322e-06}, {"id": 931, "seek": 611220, "start": 6129.5599999999995, "end": 6134.72, "text": " is to do the thing I told you earlier, which is to kind of combine this with the training", "tokens": [307, 281, 360, 264, 551, 286, 1907, 291, 3071, 11, 597, 307, 281, 733, 295, 10432, 341, 365, 264, 3097], "temperature": 0.0, "avg_logprob": -0.09850208046510048, "compression_ratio": 1.7008928571428572, "no_speech_prob": 2.1233724964986322e-06}, {"id": 932, "seek": 611220, "start": 6134.72, "end": 6140.96, "text": " loop and make the training loop as it goes through the epochs gradually move from always", "tokens": [6367, 293, 652, 264, 3097, 6367, 382, 309, 1709, 807, 264, 30992, 28346, 13145, 1286, 490, 1009], "temperature": 0.0, "avg_logprob": -0.09850208046510048, "compression_ratio": 1.7008928571428572, "no_speech_prob": 2.1233724964986322e-06}, {"id": 933, "seek": 614096, "start": 6140.96, "end": 6147.2, "text": " using teacher forcing to over time randomly using it less and less and less until at the", "tokens": [1228, 5027, 337, 2175, 281, 670, 565, 16979, 1228, 309, 1570, 293, 1570, 293, 1570, 1826, 412, 264], "temperature": 0.0, "avg_logprob": -0.18427933586968315, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.280535707337549e-05}, {"id": 934, "seek": 614096, "start": 6147.2, "end": 6152.52, "text": " very end it never uses teacher forcing and always uses this. If you get that working,", "tokens": [588, 917, 309, 1128, 4960, 5027, 337, 2175, 293, 1009, 4960, 341, 13, 759, 291, 483, 300, 1364, 11], "temperature": 0.0, "avg_logprob": -0.18427933586968315, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.280535707337549e-05}, {"id": 935, "seek": 614096, "start": 6152.52, "end": 6159.68, "text": " which won't take you long, I think it's pretty straightforward, you'll get better test results", "tokens": [597, 1582, 380, 747, 291, 938, 11, 286, 519, 309, 311, 1238, 15325, 11, 291, 603, 483, 1101, 1500, 3542], "temperature": 0.0, "avg_logprob": -0.18427933586968315, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.280535707337549e-05}, {"id": 936, "seek": 614096, "start": 6159.68, "end": 6162.4800000000005, "text": " than I'm showing you here.", "tokens": [813, 286, 478, 4099, 291, 510, 13], "temperature": 0.0, "avg_logprob": -0.18427933586968315, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.280535707337549e-05}, {"id": 937, "seek": 614096, "start": 6162.4800000000005, "end": 6170.16, "text": " Anyway let's test it. So to do French to English, we're basically going to take our French sentence,", "tokens": [5684, 718, 311, 1500, 309, 13, 407, 281, 360, 5522, 281, 3669, 11, 321, 434, 1936, 516, 281, 747, 527, 5522, 8174, 11], "temperature": 0.0, "avg_logprob": -0.18427933586968315, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.280535707337549e-05}, {"id": 938, "seek": 617016, "start": 6170.16, "end": 6177.32, "text": " turn it into a list of IDs by tokenizing it and turning it into IDs, padding it with zeros,", "tokens": [1261, 309, 666, 257, 1329, 295, 48212, 538, 14862, 3319, 309, 293, 6246, 309, 666, 48212, 11, 39562, 309, 365, 35193, 11], "temperature": 0.0, "avg_logprob": -0.1548175811767578, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.5689309293520637e-05}, {"id": 939, "seek": 617016, "start": 6177.32, "end": 6185.599999999999, "text": " all that evaluate function I just showed you, and then join it together, and there it is.", "tokens": [439, 300, 13059, 2445, 286, 445, 4712, 291, 11, 293, 550, 3917, 309, 1214, 11, 293, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1548175811767578, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.5689309293520637e-05}, {"id": 940, "seek": 617016, "start": 6185.599999999999, "end": 6192.0, "text": " So this was the correct English, this was the French we were given, and this was our", "tokens": [407, 341, 390, 264, 3006, 3669, 11, 341, 390, 264, 5522, 321, 645, 2212, 11, 293, 341, 390, 527], "temperature": 0.0, "avg_logprob": -0.1548175811767578, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.5689309293520637e-05}, {"id": 941, "seek": 619200, "start": 6192.0, "end": 6201.04, "text": " prediction. So it's not the same, but it's still correct as a speaker. So that's looking", "tokens": [17630, 13, 407, 309, 311, 406, 264, 912, 11, 457, 309, 311, 920, 3006, 382, 257, 8145, 13, 407, 300, 311, 1237], "temperature": 0.0, "avg_logprob": -0.16576733087238513, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.766869202896487e-06}, {"id": 942, "seek": 619200, "start": 6201.04, "end": 6203.72, "text": " pretty good.", "tokens": [1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.16576733087238513, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.766869202896487e-06}, {"id": 943, "seek": 619200, "start": 6203.72, "end": 6211.96, "text": " So there it is, there's translation. I guess there's a couple of things I wanted to briefly", "tokens": [407, 456, 309, 307, 11, 456, 311, 12853, 13, 286, 2041, 456, 311, 257, 1916, 295, 721, 286, 1415, 281, 10515], "temperature": 0.0, "avg_logprob": -0.16576733087238513, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.766869202896487e-06}, {"id": 944, "seek": 619200, "start": 6211.96, "end": 6220.02, "text": " mention. One is, these are all stuff that you guys can play with if you're interested.", "tokens": [2152, 13, 1485, 307, 11, 613, 366, 439, 1507, 300, 291, 1074, 393, 862, 365, 498, 291, 434, 3102, 13], "temperature": 0.0, "avg_logprob": -0.16576733087238513, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.766869202896487e-06}, {"id": 945, "seek": 622002, "start": 6220.02, "end": 6227.84, "text": " This decoding loop, there's much better ways to do it. What I'm doing here by taking the", "tokens": [639, 979, 8616, 6367, 11, 456, 311, 709, 1101, 2098, 281, 360, 309, 13, 708, 286, 478, 884, 510, 538, 1940, 264], "temperature": 0.0, "avg_logprob": -0.194188770494963, "compression_ratio": 1.4224598930481283, "no_speech_prob": 6.240834409254603e-06}, {"id": 946, "seek": 622002, "start": 6227.84, "end": 6237.280000000001, "text": " top one every time is I'm assuming in the decoder that the top prediction is the correct", "tokens": [1192, 472, 633, 565, 307, 286, 478, 11926, 294, 264, 979, 19866, 300, 264, 1192, 17630, 307, 264, 3006], "temperature": 0.0, "avg_logprob": -0.194188770494963, "compression_ratio": 1.4224598930481283, "no_speech_prob": 6.240834409254603e-06}, {"id": 947, "seek": 622002, "start": 6237.280000000001, "end": 6245.280000000001, "text": " one. But what if two words were nearly 50-50. This is 51%, this one is 49%. And I go, oh", "tokens": [472, 13, 583, 437, 498, 732, 2283, 645, 6217, 2625, 12, 2803, 13, 639, 307, 18485, 8923, 341, 472, 307, 16513, 6856, 400, 286, 352, 11, 1954], "temperature": 0.0, "avg_logprob": -0.194188770494963, "compression_ratio": 1.4224598930481283, "no_speech_prob": 6.240834409254603e-06}, {"id": 948, "seek": 624528, "start": 6245.28, "end": 6251.32, "text": " it's definitely the 51%. That might be a bad idea. So I'm going to actually steal some", "tokens": [309, 311, 2138, 264, 18485, 6856, 663, 1062, 312, 257, 1578, 1558, 13, 407, 286, 478, 516, 281, 767, 11009, 512], "temperature": 0.0, "avg_logprob": -0.18089354665655838, "compression_ratio": 1.3142857142857143, "no_speech_prob": 1.0615988685458433e-05}, {"id": 949, "seek": 624528, "start": 6251.32, "end": 6262.5599999999995, "text": " slides from Graham Newbig from the NARA Institute of Science Technology, who has shown a fantastic", "tokens": [9788, 490, 22691, 1873, 37660, 490, 264, 426, 26715, 9446, 295, 8976, 15037, 11, 567, 575, 4898, 257, 5456], "temperature": 0.0, "avg_logprob": -0.18089354665655838, "compression_ratio": 1.3142857142857143, "no_speech_prob": 1.0615988685458433e-05}, {"id": 950, "seek": 624528, "start": 6262.5599999999995, "end": 6266.719999999999, "text": " simple example of what you could do instead.", "tokens": [2199, 1365, 295, 437, 291, 727, 360, 2602, 13], "temperature": 0.0, "avg_logprob": -0.18089354665655838, "compression_ratio": 1.3142857142857143, "no_speech_prob": 1.0615988685458433e-05}, {"id": 951, "seek": 626672, "start": 6266.72, "end": 6277.12, "text": " So he's doing something slightly different, which is to say, what if we had a sentence", "tokens": [407, 415, 311, 884, 746, 4748, 819, 11, 597, 307, 281, 584, 11, 437, 498, 321, 632, 257, 8174], "temperature": 0.0, "avg_logprob": -0.13998636603355408, "compression_ratio": 1.5057471264367817, "no_speech_prob": 9.818182661547326e-06}, {"id": 952, "seek": 626672, "start": 6277.12, "end": 6283.88, "text": " like natural language processing bracket, NLP bracket, and your job was to figure out", "tokens": [411, 3303, 2856, 9007, 16904, 11, 426, 45196, 16904, 11, 293, 428, 1691, 390, 281, 2573, 484], "temperature": 0.0, "avg_logprob": -0.13998636603355408, "compression_ratio": 1.5057471264367817, "no_speech_prob": 9.818182661547326e-06}, {"id": 953, "seek": 626672, "start": 6283.88, "end": 6290.2, "text": " not how to translate it, but to figure out what part of speech each of those things were.", "tokens": [406, 577, 281, 13799, 309, 11, 457, 281, 2573, 484, 437, 644, 295, 6218, 1184, 295, 729, 721, 645, 13], "temperature": 0.0, "avg_logprob": -0.13998636603355408, "compression_ratio": 1.5057471264367817, "no_speech_prob": 9.818182661547326e-06}, {"id": 954, "seek": 629020, "start": 6290.2, "end": 6300.2, "text": " These are weird letters, N is noun, J is adjective, VB is verb, left bracket, right bracket.", "tokens": [1981, 366, 3657, 7825, 11, 426, 307, 23307, 11, 508, 307, 44129, 11, 691, 33, 307, 9595, 11, 1411, 16904, 11, 558, 16904, 13], "temperature": 0.0, "avg_logprob": -0.17495088434931058, "compression_ratio": 1.5670731707317074, "no_speech_prob": 3.446564278419828e-06}, {"id": 955, "seek": 629020, "start": 6300.2, "end": 6306.5599999999995, "text": " So the correct answer is that natural is an adjective, language is a noun, processing", "tokens": [407, 264, 3006, 1867, 307, 300, 3303, 307, 364, 44129, 11, 2856, 307, 257, 23307, 11, 9007], "temperature": 0.0, "avg_logprob": -0.17495088434931058, "compression_ratio": 1.5670731707317074, "no_speech_prob": 3.446564278419828e-06}, {"id": 956, "seek": 629020, "start": 6306.5599999999995, "end": 6313.08, "text": " is a noun, and so forth. This would be the correct path through these options.", "tokens": [307, 257, 23307, 11, 293, 370, 5220, 13, 639, 576, 312, 264, 3006, 3100, 807, 613, 3956, 13], "temperature": 0.0, "avg_logprob": -0.17495088434931058, "compression_ratio": 1.5670731707317074, "no_speech_prob": 3.446564278419828e-06}, {"id": 957, "seek": 631308, "start": 6313.08, "end": 6325.96, "text": " Now how would you create this path? Well, you could start out by figuring out how likely", "tokens": [823, 577, 576, 291, 1884, 341, 3100, 30, 1042, 11, 291, 727, 722, 484, 538, 15213, 484, 577, 3700], "temperature": 0.0, "avg_logprob": -0.20521419698541815, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.339175797620555e-06}, {"id": 958, "seek": 631308, "start": 6325.96, "end": 6331.64, "text": " natural is in language in general to be a noun versus an adjective versus a verb and", "tokens": [3303, 307, 294, 2856, 294, 2674, 281, 312, 257, 23307, 5717, 364, 44129, 5717, 257, 9595, 293], "temperature": 0.0, "avg_logprob": -0.20521419698541815, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.339175797620555e-06}, {"id": 959, "seek": 631308, "start": 6331.64, "end": 6337.24, "text": " so forth. And then having done that, for every single one of those, you could figure out", "tokens": [370, 5220, 13, 400, 550, 1419, 1096, 300, 11, 337, 633, 2167, 472, 295, 729, 11, 291, 727, 2573, 484], "temperature": 0.0, "avg_logprob": -0.20521419698541815, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.339175797620555e-06}, {"id": 960, "seek": 631308, "start": 6337.24, "end": 6342.64, "text": " how likely it is for every one of these to then be a noun, and then to be an adjective,", "tokens": [577, 3700, 309, 307, 337, 633, 472, 295, 613, 281, 550, 312, 257, 23307, 11, 293, 550, 281, 312, 364, 44129, 11], "temperature": 0.0, "avg_logprob": -0.20521419698541815, "compression_ratio": 1.7857142857142858, "no_speech_prob": 6.339175797620555e-06}, {"id": 961, "seek": 634264, "start": 6342.64, "end": 6347.8, "text": " and then to be a verb, and so forth. And you could keep repeating this process, adding", "tokens": [293, 550, 281, 312, 257, 9595, 11, 293, 370, 5220, 13, 400, 291, 727, 1066, 18617, 341, 1399, 11, 5127], "temperature": 0.0, "avg_logprob": -0.19717237684461805, "compression_ratio": 1.6163793103448276, "no_speech_prob": 1.696431718301028e-05}, {"id": 962, "seek": 634264, "start": 6347.8, "end": 6354.88, "text": " up the log predictions all the way along, all the way to the end, and pick the path", "tokens": [493, 264, 3565, 21264, 439, 264, 636, 2051, 11, 439, 264, 636, 281, 264, 917, 11, 293, 1888, 264, 3100], "temperature": 0.0, "avg_logprob": -0.19717237684461805, "compression_ratio": 1.6163793103448276, "no_speech_prob": 1.696431718301028e-05}, {"id": 963, "seek": 634264, "start": 6354.88, "end": 6356.84, "text": " which was the best.", "tokens": [597, 390, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.19717237684461805, "compression_ratio": 1.6163793103448276, "no_speech_prob": 1.696431718301028e-05}, {"id": 964, "seek": 634264, "start": 6356.84, "end": 6363.12, "text": " Now the problem is of course, that's basically 5 times 5 times 5, if you already have 5 choices,", "tokens": [823, 264, 1154, 307, 295, 1164, 11, 300, 311, 1936, 1025, 1413, 1025, 1413, 1025, 11, 498, 291, 1217, 362, 1025, 7994, 11], "temperature": 0.0, "avg_logprob": -0.19717237684461805, "compression_ratio": 1.6163793103448276, "no_speech_prob": 1.696431718301028e-05}, {"id": 965, "seek": 634264, "start": 6363.12, "end": 6369.240000000001, "text": " exponentially complex. And remember, in our case we're not picking from 5 things, we're", "tokens": [37330, 3997, 13, 400, 1604, 11, 294, 527, 1389, 321, 434, 406, 8867, 490, 1025, 721, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.19717237684461805, "compression_ratio": 1.6163793103448276, "no_speech_prob": 1.696431718301028e-05}, {"id": 966, "seek": 636924, "start": 6369.24, "end": 6377.679999999999, "text": " picking from 40,000 or 20,000 or whatever, the vocabulary of our French language things.", "tokens": [8867, 490, 3356, 11, 1360, 420, 945, 11, 1360, 420, 2035, 11, 264, 19864, 295, 527, 5522, 2856, 721, 13], "temperature": 0.0, "avg_logprob": -0.14304658868810632, "compression_ratio": 1.5219298245614035, "no_speech_prob": 6.962140560062835e-06}, {"id": 967, "seek": 636924, "start": 6377.679999999999, "end": 6382.5599999999995, "text": " This is called the Viterbi algorithm. So the Viterbi algorithm for machine translation", "tokens": [639, 307, 1219, 264, 691, 1681, 5614, 9284, 13, 407, 264, 691, 1681, 5614, 9284, 337, 3479, 12853], "temperature": 0.0, "avg_logprob": -0.14304658868810632, "compression_ratio": 1.5219298245614035, "no_speech_prob": 6.962140560062835e-06}, {"id": 968, "seek": 636924, "start": 6382.5599999999995, "end": 6388.4, "text": " is NP-complete. If you haven't come across that before, it basically means forget it.", "tokens": [307, 38611, 12, 1112, 17220, 13, 759, 291, 2378, 380, 808, 2108, 300, 949, 11, 309, 1936, 1355, 2870, 309, 13], "temperature": 0.0, "avg_logprob": -0.14304658868810632, "compression_ratio": 1.5219298245614035, "no_speech_prob": 6.962140560062835e-06}, {"id": 969, "seek": 636924, "start": 6388.4, "end": 6397.66, "text": " So let's not do that, but I'm sure you can see how obvious the answer is. Rather than", "tokens": [407, 718, 311, 406, 360, 300, 11, 457, 286, 478, 988, 291, 393, 536, 577, 6322, 264, 1867, 307, 13, 16571, 813], "temperature": 0.0, "avg_logprob": -0.14304658868810632, "compression_ratio": 1.5219298245614035, "no_speech_prob": 6.962140560062835e-06}, {"id": 970, "seek": 639766, "start": 6397.66, "end": 6405.48, "text": " doing the Viterbi algorithm, why don't we just pick the top few hypotheses so far.", "tokens": [884, 264, 691, 1681, 5614, 9284, 11, 983, 500, 380, 321, 445, 1888, 264, 1192, 1326, 49969, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.12599148069109237, "compression_ratio": 1.5454545454545454, "no_speech_prob": 9.81810262601357e-06}, {"id": 971, "seek": 639766, "start": 6405.48, "end": 6413.5199999999995, "text": " So here are the scores for what is the word natural. It's probably not a left bracket,", "tokens": [407, 510, 366, 264, 13444, 337, 437, 307, 264, 1349, 3303, 13, 467, 311, 1391, 406, 257, 1411, 16904, 11], "temperature": 0.0, "avg_logprob": -0.12599148069109237, "compression_ratio": 1.5454545454545454, "no_speech_prob": 9.81810262601357e-06}, {"id": 972, "seek": 639766, "start": 6413.5199999999995, "end": 6420.639999999999, "text": " it's probably not a right bracket. It might be one of these. Let's assume it's one of", "tokens": [309, 311, 1391, 406, 257, 558, 16904, 13, 467, 1062, 312, 472, 295, 613, 13, 961, 311, 6552, 309, 311, 472, 295], "temperature": 0.0, "avg_logprob": -0.12599148069109237, "compression_ratio": 1.5454545454545454, "no_speech_prob": 9.81810262601357e-06}, {"id": 973, "seek": 642064, "start": 6420.64, "end": 6429.200000000001, "text": " the top 3. So given it's one of the top 3, what might be next? And again, let's just", "tokens": [264, 1192, 805, 13, 407, 2212, 309, 311, 472, 295, 264, 1192, 805, 11, 437, 1062, 312, 958, 30, 400, 797, 11, 718, 311, 445], "temperature": 0.0, "avg_logprob": -0.21782724380493165, "compression_ratio": 1.5084033613445378, "no_speech_prob": 4.029394858662272e-06}, {"id": 974, "seek": 642064, "start": 6429.200000000001, "end": 6436.280000000001, "text": " pick the top 3 combinations. And keep going through that. This is called BeamSearch. In", "tokens": [1888, 264, 1192, 805, 21267, 13, 400, 1066, 516, 807, 300, 13, 639, 307, 1219, 40916, 10637, 1178, 13, 682], "temperature": 0.0, "avg_logprob": -0.21782724380493165, "compression_ratio": 1.5084033613445378, "no_speech_prob": 4.029394858662272e-06}, {"id": 975, "seek": 642064, "start": 6436.280000000001, "end": 6442.240000000001, "text": " practice, every state-of-the-art algorithm for neural language translation uses this", "tokens": [3124, 11, 633, 1785, 12, 2670, 12, 3322, 12, 446, 9284, 337, 18161, 2856, 12853, 4960, 341], "temperature": 0.0, "avg_logprob": -0.21782724380493165, "compression_ratio": 1.5084033613445378, "no_speech_prob": 4.029394858662272e-06}, {"id": 976, "seek": 642064, "start": 6442.240000000001, "end": 6444.240000000001, "text": " for decoding.", "tokens": [337, 979, 8616, 13], "temperature": 0.0, "avg_logprob": -0.21782724380493165, "compression_ratio": 1.5084033613445378, "no_speech_prob": 4.029394858662272e-06}, {"id": 977, "seek": 642064, "start": 6444.240000000001, "end": 6449.0, "text": " Now writing this, again, it's going to be less than a screen of code. I haven't written", "tokens": [823, 3579, 341, 11, 797, 11, 309, 311, 516, 281, 312, 1570, 813, 257, 2568, 295, 3089, 13, 286, 2378, 380, 3720], "temperature": 0.0, "avg_logprob": -0.21782724380493165, "compression_ratio": 1.5084033613445378, "no_speech_prob": 4.029394858662272e-06}, {"id": 978, "seek": 644900, "start": 6449.0, "end": 6457.96, "text": " that. Why not go write it this week? Add it to this code, add BeamSearch. Here's the entire", "tokens": [300, 13, 1545, 406, 352, 2464, 309, 341, 1243, 30, 5349, 309, 281, 341, 3089, 11, 909, 40916, 10637, 1178, 13, 1692, 311, 264, 2302], "temperature": 0.0, "avg_logprob": -0.3575095324449136, "compression_ratio": 1.50920245398773, "no_speech_prob": 3.647784978966229e-05}, {"id": 979, "seek": 644900, "start": 6457.96, "end": 6464.28, "text": " pseudo code. I'm sure you could write it in probably less code than that.", "tokens": [35899, 3089, 13, 286, 478, 988, 291, 727, 2464, 309, 294, 1391, 1570, 3089, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.3575095324449136, "compression_ratio": 1.50920245398773, "no_speech_prob": 3.647784978966229e-05}, {"id": 980, "seek": 644900, "start": 6464.28, "end": 6466.28, "text": " So there's BeamSearch, there's one thing to mention.", "tokens": [407, 456, 311, 40916, 10637, 1178, 11, 456, 311, 472, 551, 281, 2152, 13], "temperature": 0.0, "avg_logprob": -0.3575095324449136, "compression_ratio": 1.50920245398773, "no_speech_prob": 3.647784978966229e-05}, {"id": 981, "seek": 644900, "start": 6466.28, "end": 6473.28, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.3575095324449136, "compression_ratio": 1.50920245398773, "no_speech_prob": 3.647784978966229e-05}, {"id": 982, "seek": 647328, "start": 6473.28, "end": 6479.8, "text": " Do you know if there are any training methods that capture the fact that what is the population", "tokens": [1144, 291, 458, 498, 456, 366, 604, 3097, 7150, 300, 7983, 264, 1186, 300, 437, 307, 264, 4415], "temperature": 0.0, "avg_logprob": -0.23249726884820487, "compression_ratio": 1.5869565217391304, "no_speech_prob": 5.064138531452045e-05}, {"id": 983, "seek": 647328, "start": 6479.8, "end": 6483.2, "text": " of Canada and what is Canada's population are very nearly the same?", "tokens": [295, 6309, 293, 437, 307, 6309, 311, 4415, 366, 588, 6217, 264, 912, 30], "temperature": 0.0, "avg_logprob": -0.23249726884820487, "compression_ratio": 1.5869565217391304, "no_speech_prob": 5.064138531452045e-05}, {"id": 984, "seek": 647328, "start": 6483.2, "end": 6484.2, "text": " Answer that question.", "tokens": [24545, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.23249726884820487, "compression_ratio": 1.5869565217391304, "no_speech_prob": 5.064138531452045e-05}, {"id": 985, "seek": 647328, "start": 6484.2, "end": 6494.0, "text": " No, I don't. But I'm not sure it even matters because on average, a better system will be", "tokens": [883, 11, 286, 500, 380, 13, 583, 286, 478, 406, 988, 309, 754, 7001, 570, 322, 4274, 11, 257, 1101, 1185, 486, 312], "temperature": 0.0, "avg_logprob": -0.23249726884820487, "compression_ratio": 1.5869565217391304, "no_speech_prob": 5.064138531452045e-05}, {"id": 986, "seek": 647328, "start": 6494.0, "end": 6502.219999999999, "text": " one which translates those 50% of the time into one versus the other. The best translator", "tokens": [472, 597, 28468, 729, 2625, 4, 295, 264, 565, 666, 472, 5717, 264, 661, 13, 440, 1151, 35223], "temperature": 0.0, "avg_logprob": -0.23249726884820487, "compression_ratio": 1.5869565217391304, "no_speech_prob": 5.064138531452045e-05}, {"id": 987, "seek": 650222, "start": 6502.22, "end": 6506.08, "text": " approach I don't think is going to vary depending on the answer to that question, so I'm not", "tokens": [3109, 286, 500, 380, 519, 307, 516, 281, 10559, 5413, 322, 264, 1867, 281, 300, 1168, 11, 370, 286, 478, 406], "temperature": 0.0, "avg_logprob": -0.20760846473801303, "compression_ratio": 1.4550264550264551, "no_speech_prob": 5.255326868791599e-06}, {"id": 988, "seek": 650222, "start": 6506.08, "end": 6509.08, "text": " sure it's that important.", "tokens": [988, 309, 311, 300, 1021, 13], "temperature": 0.0, "avg_logprob": -0.20760846473801303, "compression_ratio": 1.4550264550264551, "no_speech_prob": 5.255326868791599e-06}, {"id": 989, "seek": 650222, "start": 6509.08, "end": 6512.8, "text": " Could we translate between Chinese and English using this same method?", "tokens": [7497, 321, 13799, 1296, 4649, 293, 3669, 1228, 341, 912, 3170, 30], "temperature": 0.0, "avg_logprob": -0.20760846473801303, "compression_ratio": 1.4550264550264551, "no_speech_prob": 5.255326868791599e-06}, {"id": 990, "seek": 650222, "start": 6512.8, "end": 6518.400000000001, "text": " Yes we can, but it would be better if we used a technique I'm going to show you next.", "tokens": [1079, 321, 393, 11, 457, 309, 576, 312, 1101, 498, 321, 1143, 257, 6532, 286, 478, 516, 281, 855, 291, 958, 13], "temperature": 0.0, "avg_logprob": -0.20760846473801303, "compression_ratio": 1.4550264550264551, "no_speech_prob": 5.255326868791599e-06}, {"id": 991, "seek": 651840, "start": 6518.4, "end": 6533.36, "text": " So the technique I'm going to show you next is described in this paper, Neural Machine", "tokens": [407, 264, 6532, 286, 478, 516, 281, 855, 291, 958, 307, 7619, 294, 341, 3035, 11, 1734, 1807, 22155], "temperature": 0.0, "avg_logprob": -0.1090671249798366, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.4951107156521175e-06}, {"id": 992, "seek": 651840, "start": 6533.36, "end": 6536.599999999999, "text": " Translation of Rare Words with Subword Units.", "tokens": [6531, 24278, 295, 43920, 32857, 365, 8511, 7462, 1156, 1208, 13], "temperature": 0.0, "avg_logprob": -0.1090671249798366, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.4951107156521175e-06}, {"id": 993, "seek": 651840, "start": 6536.599999999999, "end": 6541.879999999999, "text": " So interestingly, this actually came up today when I was chatting to Brad, and Brad was", "tokens": [407, 25873, 11, 341, 767, 1361, 493, 965, 562, 286, 390, 24654, 281, 11895, 11, 293, 11895, 390], "temperature": 0.0, "avg_logprob": -0.1090671249798366, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.4951107156521175e-06}, {"id": 994, "seek": 654188, "start": 6541.88, "end": 6550.4800000000005, "text": " asking me how do I create an analysis of people's tweets using their particular vocabulary,", "tokens": [3365, 385, 577, 360, 286, 1884, 364, 5215, 295, 561, 311, 25671, 1228, 641, 1729, 19864, 11], "temperature": 0.0, "avg_logprob": -0.1474231481552124, "compression_ratio": 1.5955555555555556, "no_speech_prob": 6.540287358802743e-06}, {"id": 995, "seek": 654188, "start": 6550.4800000000005, "end": 6558.2, "text": " but make it not fall apart if they use some word in the future that I haven't seen before.", "tokens": [457, 652, 309, 406, 2100, 4936, 498, 436, 764, 512, 1349, 294, 264, 2027, 300, 286, 2378, 380, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.1474231481552124, "compression_ratio": 1.5955555555555556, "no_speech_prob": 6.540287358802743e-06}, {"id": 996, "seek": 654188, "start": 6558.2, "end": 6563.28, "text": " And that's a very similar question to how do I translate language when somebody uses", "tokens": [400, 300, 311, 257, 588, 2531, 1168, 281, 577, 360, 286, 13799, 2856, 562, 2618, 4960], "temperature": 0.0, "avg_logprob": -0.1474231481552124, "compression_ratio": 1.5955555555555556, "no_speech_prob": 6.540287358802743e-06}, {"id": 997, "seek": 654188, "start": 6563.28, "end": 6569.400000000001, "text": " a word I haven't seen before. Or more generally, maybe I don't want to use 160,000 words in", "tokens": [257, 1349, 286, 2378, 380, 1612, 949, 13, 1610, 544, 5101, 11, 1310, 286, 500, 380, 528, 281, 764, 21243, 11, 1360, 2283, 294], "temperature": 0.0, "avg_logprob": -0.1474231481552124, "compression_ratio": 1.5955555555555556, "no_speech_prob": 6.540287358802743e-06}, {"id": 998, "seek": 656940, "start": 6569.4, "end": 6572.759999999999, "text": " my vocabulary. That's a huge embedding matrix, it takes a lot of memory, it takes a lot of", "tokens": [452, 19864, 13, 663, 311, 257, 2603, 12240, 3584, 8141, 11, 309, 2516, 257, 688, 295, 4675, 11, 309, 2516, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.2458864468247143, "compression_ratio": 1.4528301886792452, "no_speech_prob": 4.029397587146377e-06}, {"id": 999, "seek": 656940, "start": 6572.759999999999, "end": 6577.44, "text": " time, it's going to be hard to train. So what do you do?", "tokens": [565, 11, 309, 311, 516, 281, 312, 1152, 281, 3847, 13, 407, 437, 360, 291, 360, 30], "temperature": 0.0, "avg_logprob": -0.2458864468247143, "compression_ratio": 1.4528301886792452, "no_speech_prob": 4.029397587146377e-06}, {"id": 1000, "seek": 656940, "start": 6577.44, "end": 6592.839999999999, "text": " So the answer is you use something called BPPE, which is basically an encoder. What", "tokens": [407, 264, 1867, 307, 291, 764, 746, 1219, 40533, 5208, 11, 597, 307, 1936, 364, 2058, 19866, 13, 708], "temperature": 0.0, "avg_logprob": -0.2458864468247143, "compression_ratio": 1.4528301886792452, "no_speech_prob": 4.029397587146377e-06}, {"id": 1001, "seek": 659284, "start": 6592.84, "end": 6607.0, "text": " it's going to do is it's going to take a sentence like, hello, I am Jeremy, and it's going to", "tokens": [309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 747, 257, 8174, 411, 11, 7751, 11, 286, 669, 17809, 11, 293, 309, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.1978536269244026, "compression_ratio": 1.6704545454545454, "no_speech_prob": 8.013423212105408e-06}, {"id": 1002, "seek": 659284, "start": 6607.0, "end": 6613.52, "text": " basically say, I'm going to try and turn this into a list of tokens that aren't necessarily", "tokens": [1936, 584, 11, 286, 478, 516, 281, 853, 293, 1261, 341, 666, 257, 1329, 295, 22667, 300, 3212, 380, 4725], "temperature": 0.0, "avg_logprob": -0.1978536269244026, "compression_ratio": 1.6704545454545454, "no_speech_prob": 8.013423212105408e-06}, {"id": 1003, "seek": 659284, "start": 6613.52, "end": 6614.52, "text": " the same as the words.", "tokens": [264, 912, 382, 264, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1978536269244026, "compression_ratio": 1.6704545454545454, "no_speech_prob": 8.013423212105408e-06}, {"id": 1004, "seek": 659284, "start": 6614.52, "end": 6618.4400000000005, "text": " So the first thing I'm going to do is I'm going to look at every pair of letters, HE,", "tokens": [407, 264, 700, 551, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 574, 412, 633, 6119, 295, 7825, 11, 11827, 11], "temperature": 0.0, "avg_logprob": -0.1978536269244026, "compression_ratio": 1.6704545454545454, "no_speech_prob": 8.013423212105408e-06}, {"id": 1005, "seek": 661844, "start": 6618.44, "end": 6625.0, "text": " D, L, LL, LL, and so forth, for my whole training set. I'm going to say which pair of letters", "tokens": [413, 11, 441, 11, 441, 43, 11, 441, 43, 11, 293, 370, 5220, 11, 337, 452, 1379, 3097, 992, 13, 286, 478, 516, 281, 584, 597, 6119, 295, 7825], "temperature": 0.0, "avg_logprob": -0.26612164424015927, "compression_ratio": 1.46448087431694, "no_speech_prob": 1.1843006177514326e-05}, {"id": 1006, "seek": 661844, "start": 6625.0, "end": 6639.759999999999, "text": " is the most common. So maybe the answer is ER. So I take those two and I turn them into", "tokens": [307, 264, 881, 2689, 13, 407, 1310, 264, 1867, 307, 14929, 13, 407, 286, 747, 729, 732, 293, 286, 1261, 552, 666], "temperature": 0.0, "avg_logprob": -0.26612164424015927, "compression_ratio": 1.46448087431694, "no_speech_prob": 1.1843006177514326e-05}, {"id": 1007, "seek": 661844, "start": 6639.759999999999, "end": 6644.599999999999, "text": " a single entity called ER. So we start off with every character separate, so we're now", "tokens": [257, 2167, 13977, 1219, 14929, 13, 407, 321, 722, 766, 365, 633, 2517, 4994, 11, 370, 321, 434, 586], "temperature": 0.0, "avg_logprob": -0.26612164424015927, "compression_ratio": 1.46448087431694, "no_speech_prob": 1.1843006177514326e-05}, {"id": 1008, "seek": 664460, "start": 6644.6, "end": 6650.72, "text": " going to combine these into a single entity called ER. And then I go through and I do", "tokens": [516, 281, 10432, 613, 666, 257, 2167, 13977, 1219, 14929, 13, 400, 550, 286, 352, 807, 293, 286, 360], "temperature": 0.0, "avg_logprob": -0.19508674270228335, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.9222881746827625e-05}, {"id": 1009, "seek": 664460, "start": 6650.72, "end": 6657.160000000001, "text": " that again, and maybe the next answer is that actually AM appears a lot. So then I'm going", "tokens": [300, 797, 11, 293, 1310, 264, 958, 1867, 307, 300, 767, 6475, 7038, 257, 688, 13, 407, 550, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.19508674270228335, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.9222881746827625e-05}, {"id": 1010, "seek": 664460, "start": 6657.160000000001, "end": 6660.88, "text": " to turn that into AM.", "tokens": [281, 1261, 300, 666, 6475, 13], "temperature": 0.0, "avg_logprob": -0.19508674270228335, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.9222881746827625e-05}, {"id": 1011, "seek": 664460, "start": 6660.88, "end": 6667.160000000001, "text": " And so maybe the next thing is actually ERE is the next most common thing I can find.", "tokens": [400, 370, 1310, 264, 958, 551, 307, 767, 462, 3850, 307, 264, 958, 881, 2689, 551, 286, 393, 915, 13], "temperature": 0.0, "avg_logprob": -0.19508674270228335, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.9222881746827625e-05}, {"id": 1012, "seek": 666716, "start": 6667.16, "end": 6676.74, "text": " So now we take this and replace it with ERE. And you can keep doing this. You never cross", "tokens": [407, 586, 321, 747, 341, 293, 7406, 309, 365, 462, 3850, 13, 400, 291, 393, 1066, 884, 341, 13, 509, 1128, 3278], "temperature": 0.0, "avg_logprob": -0.13150929856574398, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.771880296379095e-06}, {"id": 1013, "seek": 666716, "start": 6676.74, "end": 6683.12, "text": " word boundaries. So in theory we could do this forever until we end up eventually with", "tokens": [1349, 13180, 13, 407, 294, 5261, 321, 727, 360, 341, 5680, 1826, 321, 917, 493, 4728, 365], "temperature": 0.0, "avg_logprob": -0.13150929856574398, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.771880296379095e-06}, {"id": 1014, "seek": 666716, "start": 6683.12, "end": 6689.08, "text": " the words again. But instead what happens with this BPE encoder is you provide a single", "tokens": [264, 2283, 797, 13, 583, 2602, 437, 2314, 365, 341, 363, 5208, 2058, 19866, 307, 291, 2893, 257, 2167], "temperature": 0.0, "avg_logprob": -0.13150929856574398, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.771880296379095e-06}, {"id": 1015, "seek": 666716, "start": 6689.08, "end": 6695.04, "text": " parameter which is what is the maximum number of combiners you want to do. A common default", "tokens": [13075, 597, 307, 437, 307, 264, 6674, 1230, 295, 38514, 433, 291, 528, 281, 360, 13, 316, 2689, 7576], "temperature": 0.0, "avg_logprob": -0.13150929856574398, "compression_ratio": 1.5344827586206897, "no_speech_prob": 5.771880296379095e-06}, {"id": 1016, "seek": 669504, "start": 6695.04, "end": 6702.32, "text": " would be like 10,000. So at the end of that, you're going to end up with 10,000 sub-word", "tokens": [576, 312, 411, 1266, 11, 1360, 13, 407, 412, 264, 917, 295, 300, 11, 291, 434, 516, 281, 917, 493, 365, 1266, 11, 1360, 1422, 12, 7462], "temperature": 0.0, "avg_logprob": -0.1794152895609538, "compression_ratio": 1.2937062937062938, "no_speech_prob": 6.048868726793444e-06}, {"id": 1017, "seek": 669504, "start": 6702.32, "end": 6713.32, "text": " sequences. So we'll end up turning this sentence into something like H-E-L-L-O. And then there's", "tokens": [22978, 13, 407, 321, 603, 917, 493, 6246, 341, 8174, 666, 746, 411, 389, 12, 36, 12, 43, 12, 43, 12, 46, 13, 400, 550, 456, 311], "temperature": 0.0, "avg_logprob": -0.1794152895609538, "compression_ratio": 1.2937062937062938, "no_speech_prob": 6.048868726793444e-06}, {"id": 1018, "seek": 671332, "start": 6713.32, "end": 6725.759999999999, "text": " a special end of word, so space, I space, AM space, maybe it'll then be like J, and", "tokens": [257, 2121, 917, 295, 1349, 11, 370, 1901, 11, 286, 1901, 11, 6475, 1901, 11, 1310, 309, 603, 550, 312, 411, 508, 11, 293], "temperature": 0.0, "avg_logprob": -0.26321806836484085, "compression_ratio": 1.4533333333333334, "no_speech_prob": 4.637842721422203e-06}, {"id": 1019, "seek": 671332, "start": 6725.759999999999, "end": 6728.96, "text": " then E-R-E, and then M-Y, something like that.", "tokens": [550, 462, 12, 49, 12, 36, 11, 293, 550, 376, 12, 56, 11, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.26321806836484085, "compression_ratio": 1.4533333333333334, "no_speech_prob": 4.637842721422203e-06}, {"id": 1020, "seek": 671332, "start": 6728.96, "end": 6741.759999999999, "text": " Now the cool thing is that you can do this by going to this GitHub site and downloading", "tokens": [823, 264, 1627, 551, 307, 300, 291, 393, 360, 341, 538, 516, 281, 341, 23331, 3621, 293, 32529], "temperature": 0.0, "avg_logprob": -0.26321806836484085, "compression_ratio": 1.4533333333333334, "no_speech_prob": 4.637842721422203e-06}, {"id": 1021, "seek": 674176, "start": 6741.76, "end": 6754.8, "text": " this and running it on your file of pros, and it will spit out the exact same thing,", "tokens": [341, 293, 2614, 309, 322, 428, 3991, 295, 6267, 11, 293, 309, 486, 22127, 484, 264, 1900, 912, 551, 11], "temperature": 0.0, "avg_logprob": -0.15367352621895927, "compression_ratio": 1.4189944134078212, "no_speech_prob": 6.747988209099276e-06}, {"id": 1022, "seek": 674176, "start": 6754.8, "end": 6762.96, "text": " but it'll stick at space between every one of these BPE codes. So in other words, to", "tokens": [457, 309, 603, 2897, 412, 1901, 1296, 633, 472, 295, 613, 363, 5208, 14211, 13, 407, 294, 661, 2283, 11, 281], "temperature": 0.0, "avg_logprob": -0.15367352621895927, "compression_ratio": 1.4189944134078212, "no_speech_prob": 6.747988209099276e-06}, {"id": 1023, "seek": 674176, "start": 6762.96, "end": 6767.76, "text": " use this with what I just showed you, you don't have to write any code. You can just", "tokens": [764, 341, 365, 437, 286, 445, 4712, 291, 11, 291, 500, 380, 362, 281, 2464, 604, 3089, 13, 509, 393, 445], "temperature": 0.0, "avg_logprob": -0.15367352621895927, "compression_ratio": 1.4189944134078212, "no_speech_prob": 6.747988209099276e-06}, {"id": 1024, "seek": 676776, "start": 6767.76, "end": 6777.84, "text": " take the English and the French sentences, run it through this and it will spit out BPE", "tokens": [747, 264, 3669, 293, 264, 5522, 16579, 11, 1190, 309, 807, 341, 293, 309, 486, 22127, 484, 363, 5208], "temperature": 0.0, "avg_logprob": -0.14381652408176, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.209861807183188e-06}, {"id": 1025, "seek": 676776, "start": 6777.84, "end": 6782.4400000000005, "text": " encoded versions of those.", "tokens": [2058, 12340, 9606, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.14381652408176, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.209861807183188e-06}, {"id": 1026, "seek": 676776, "start": 6782.4400000000005, "end": 6789.4400000000005, "text": " Having said that, I think maybe the optimal approach would be to actually write something", "tokens": [10222, 848, 300, 11, 286, 519, 1310, 264, 16252, 3109, 576, 312, 281, 767, 2464, 746], "temperature": 0.0, "avg_logprob": -0.14381652408176, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.209861807183188e-06}, {"id": 1027, "seek": 676776, "start": 6789.4400000000005, "end": 6795.4400000000005, "text": " which first of all figured out which are the most common 20,000 words or 10,000 words and", "tokens": [597, 700, 295, 439, 8932, 484, 597, 366, 264, 881, 2689, 945, 11, 1360, 2283, 420, 1266, 11, 1360, 2283, 293], "temperature": 0.0, "avg_logprob": -0.14381652408176, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.209861807183188e-06}, {"id": 1028, "seek": 679544, "start": 6795.44, "end": 6803.36, "text": " then left those words alone and maybe only run the BPE encoding on the things that are", "tokens": [550, 1411, 729, 2283, 3312, 293, 1310, 787, 1190, 264, 363, 5208, 43430, 322, 264, 721, 300, 366], "temperature": 0.0, "avg_logprob": -0.21282242156647063, "compression_ratio": 1.6592920353982301, "no_speech_prob": 1.6797167745608022e-06}, {"id": 1029, "seek": 679544, "start": 6803.36, "end": 6809.799999999999, "text": " truly rare words. Because sometimes BPE encoding actually splits things up in ways that isn't", "tokens": [4908, 5892, 2283, 13, 1436, 2171, 363, 5208, 43430, 767, 37741, 721, 493, 294, 2098, 300, 1943, 380], "temperature": 0.0, "avg_logprob": -0.21282242156647063, "compression_ratio": 1.6592920353982301, "no_speech_prob": 1.6797167745608022e-06}, {"id": 1030, "seek": 679544, "start": 6809.799999999999, "end": 6812.919999999999, "text": " quite what you want.", "tokens": [1596, 437, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.21282242156647063, "compression_ratio": 1.6592920353982301, "no_speech_prob": 1.6797167745608022e-06}, {"id": 1031, "seek": 679544, "start": 6812.919999999999, "end": 6819.28, "text": " So this is a super important technique. So for those of you that speak Chinese, you know", "tokens": [407, 341, 307, 257, 1687, 1021, 6532, 13, 407, 337, 729, 295, 291, 300, 1710, 4649, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.21282242156647063, "compression_ratio": 1.6592920353982301, "no_speech_prob": 1.6797167745608022e-06}, {"id": 1032, "seek": 679544, "start": 6819.28, "end": 6825.16, "text": " that it's actually not at all clear where words begin and end. Not only are there no", "tokens": [300, 309, 311, 767, 406, 412, 439, 1850, 689, 2283, 1841, 293, 917, 13, 1726, 787, 366, 456, 572], "temperature": 0.0, "avg_logprob": -0.21282242156647063, "compression_ratio": 1.6592920353982301, "no_speech_prob": 1.6797167745608022e-06}, {"id": 1033, "seek": 682516, "start": 6825.16, "end": 6842.0, "text": " words, but grammatically in Chinese, one example would be you can have a sequence of two verbs,", "tokens": [2283, 11, 457, 17570, 5030, 294, 4649, 11, 472, 1365, 576, 312, 291, 393, 362, 257, 8310, 295, 732, 30051, 11], "temperature": 0.0, "avg_logprob": -0.2359743799482073, "compression_ratio": 1.7352941176470589, "no_speech_prob": 2.4299612050526775e-05}, {"id": 1034, "seek": 682516, "start": 6842.0, "end": 6846.639999999999, "text": " which basically the second verb or adjective describes the result of the first one. And", "tokens": [597, 1936, 264, 1150, 9595, 420, 44129, 15626, 264, 1874, 295, 264, 700, 472, 13, 400], "temperature": 0.0, "avg_logprob": -0.2359743799482073, "compression_ratio": 1.7352941176470589, "no_speech_prob": 2.4299612050526775e-05}, {"id": 1035, "seek": 682516, "start": 6846.639999999999, "end": 6851.36, "text": " so you could treat that as a single word, a perfectly reasonable thing to do, or you", "tokens": [370, 291, 727, 2387, 300, 382, 257, 2167, 1349, 11, 257, 6239, 10585, 551, 281, 360, 11, 420, 291], "temperature": 0.0, "avg_logprob": -0.2359743799482073, "compression_ratio": 1.7352941176470589, "no_speech_prob": 2.4299612050526775e-05}, {"id": 1036, "seek": 682516, "start": 6851.36, "end": 6854.08, "text": " could treat it as three words. That would also be a perfectly reasonable thing to do,", "tokens": [727, 2387, 309, 382, 1045, 2283, 13, 663, 576, 611, 312, 257, 6239, 10585, 551, 281, 360, 11], "temperature": 0.0, "avg_logprob": -0.2359743799482073, "compression_ratio": 1.7352941176470589, "no_speech_prob": 2.4299612050526775e-05}, {"id": 1037, "seek": 685408, "start": 6854.08, "end": 6860.5599999999995, "text": " and there's no right answer. This kind of weird stuff happens all the time in Chinese.", "tokens": [293, 456, 311, 572, 558, 1867, 13, 639, 733, 295, 3657, 1507, 2314, 439, 264, 565, 294, 4649, 13], "temperature": 0.0, "avg_logprob": -0.19393674532572427, "compression_ratio": 1.5650224215246638, "no_speech_prob": 2.5612298486521468e-06}, {"id": 1038, "seek": 685408, "start": 6860.5599999999995, "end": 6868.24, "text": " You can insert the character nir into the middle of a two-character verb and it turns", "tokens": [509, 393, 8969, 264, 2517, 297, 347, 666, 264, 2808, 295, 257, 732, 12, 7374, 14125, 9595, 293, 309, 4523], "temperature": 0.0, "avg_logprob": -0.19393674532572427, "compression_ratio": 1.5650224215246638, "no_speech_prob": 2.5612298486521468e-06}, {"id": 1039, "seek": 685408, "start": 6868.24, "end": 6873.2, "text": " that into a new word, which means that that thing can't be done. Is this a new word, is", "tokens": [300, 666, 257, 777, 1349, 11, 597, 1355, 300, 300, 551, 393, 380, 312, 1096, 13, 1119, 341, 257, 777, 1349, 11, 307], "temperature": 0.0, "avg_logprob": -0.19393674532572427, "compression_ratio": 1.5650224215246638, "no_speech_prob": 2.5612298486521468e-06}, {"id": 1040, "seek": 685408, "start": 6873.2, "end": 6879.72, "text": " it now three words? It's very hard to tell. So instead, if you use BPE with Chinese, you", "tokens": [309, 586, 1045, 2283, 30, 467, 311, 588, 1152, 281, 980, 13, 407, 2602, 11, 498, 291, 764, 363, 5208, 365, 4649, 11, 291], "temperature": 0.0, "avg_logprob": -0.19393674532572427, "compression_ratio": 1.5650224215246638, "no_speech_prob": 2.5612298486521468e-06}, {"id": 1041, "seek": 687972, "start": 6879.72, "end": 6885.08, "text": " can kind of tokenize it in a way that's entirely statistical. So I think that would be the", "tokens": [393, 733, 295, 14862, 1125, 309, 294, 257, 636, 300, 311, 7696, 22820, 13, 407, 286, 519, 300, 576, 312, 264], "temperature": 0.0, "avg_logprob": -0.17087716472392178, "compression_ratio": 1.4676258992805755, "no_speech_prob": 2.6272869945387356e-05}, {"id": 1042, "seek": 687972, "start": 6885.08, "end": 6889.0, "text": " answer to that question.", "tokens": [1867, 281, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.17087716472392178, "compression_ratio": 1.4676258992805755, "no_speech_prob": 2.6272869945387356e-05}, {"id": 1043, "seek": 687972, "start": 6889.0, "end": 6900.4800000000005, "text": " So I want to leave translation there because I desperately wanted to insert a discussion", "tokens": [407, 286, 528, 281, 1856, 12853, 456, 570, 286, 23726, 1415, 281, 8969, 257, 5017], "temperature": 0.0, "avg_logprob": -0.17087716472392178, "compression_ratio": 1.4676258992805755, "no_speech_prob": 2.6272869945387356e-05}, {"id": 1044, "seek": 690048, "start": 6900.48, "end": 6911.679999999999, "text": " about segmentation. We kept on talking about segmentation. I really kind of realized in", "tokens": [466, 9469, 399, 13, 492, 4305, 322, 1417, 466, 9469, 399, 13, 286, 534, 733, 295, 5334, 294], "temperature": 0.0, "avg_logprob": -0.15004467102418462, "compression_ratio": 1.7074468085106382, "no_speech_prob": 0.00015114607231225818}, {"id": 1045, "seek": 690048, "start": 6911.679999999999, "end": 6917.2, "text": " the last week how exciting a particular path of segmentation has turned out to be, and", "tokens": [264, 1036, 1243, 577, 4670, 257, 1729, 3100, 295, 9469, 399, 575, 3574, 484, 281, 312, 11, 293], "temperature": 0.0, "avg_logprob": -0.15004467102418462, "compression_ratio": 1.7074468085106382, "no_speech_prob": 0.00015114607231225818}, {"id": 1046, "seek": 690048, "start": 6917.2, "end": 6921.5199999999995, "text": " I really, really wanted to explain it to you before the end of the course. So I'm going", "tokens": [286, 534, 11, 534, 1415, 281, 2903, 309, 281, 291, 949, 264, 917, 295, 264, 1164, 13, 407, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.15004467102418462, "compression_ratio": 1.7074468085106382, "no_speech_prob": 0.00015114607231225818}, {"id": 1047, "seek": 690048, "start": 6921.5199999999995, "end": 6927.759999999999, "text": " to do half of it today and half of it next week if we can.", "tokens": [281, 360, 1922, 295, 309, 965, 293, 1922, 295, 309, 958, 1243, 498, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.15004467102418462, "compression_ratio": 1.7074468085106382, "no_speech_prob": 0.00015114607231225818}, {"id": 1048, "seek": 692776, "start": 6927.76, "end": 6939.52, "text": " So let me show you why this is exciting. So segmentation is about taking something like", "tokens": [407, 718, 385, 855, 291, 983, 341, 307, 4670, 13, 407, 9469, 399, 307, 466, 1940, 746, 411], "temperature": 0.0, "avg_logprob": -0.11897150919987605, "compression_ratio": 1.6625, "no_speech_prob": 1.078310924640391e-05}, {"id": 1049, "seek": 692776, "start": 6939.52, "end": 6948.2, "text": " this and turning it into something like this, where every color represents a thing. So this", "tokens": [341, 293, 6246, 309, 666, 746, 411, 341, 11, 689, 633, 2017, 8855, 257, 551, 13, 407, 341], "temperature": 0.0, "avg_logprob": -0.11897150919987605, "compression_ratio": 1.6625, "no_speech_prob": 1.078310924640391e-05}, {"id": 1050, "seek": 692776, "start": 6948.2, "end": 6955.56, "text": " kind of pink is road, this light purple is line, this blue is footpath, this purple is", "tokens": [733, 295, 7022, 307, 3060, 11, 341, 1442, 9656, 307, 1622, 11, 341, 3344, 307, 2671, 31852, 11, 341, 9656, 307], "temperature": 0.0, "avg_logprob": -0.11897150919987605, "compression_ratio": 1.6625, "no_speech_prob": 1.078310924640391e-05}, {"id": 1051, "seek": 695556, "start": 6955.56, "end": 6962.400000000001, "text": " car, this red is building, and so forth. So this is quite a challenging thing to do, but", "tokens": [1032, 11, 341, 2182, 307, 2390, 11, 293, 370, 5220, 13, 407, 341, 307, 1596, 257, 7595, 551, 281, 360, 11, 457], "temperature": 0.0, "avg_logprob": -0.16804383440715512, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.0615995961416047e-05}, {"id": 1052, "seek": 695556, "start": 6962.400000000001, "end": 6968.120000000001, "text": " it's really important for anything that's going to understand the world, react to it.", "tokens": [309, 311, 534, 1021, 337, 1340, 300, 311, 516, 281, 1223, 264, 1002, 11, 4515, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.16804383440715512, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.0615995961416047e-05}, {"id": 1053, "seek": 695556, "start": 6968.120000000001, "end": 6972.120000000001, "text": " So clearly robotics and self-driving cars absolutely have to be able to do this very", "tokens": [407, 4448, 34145, 293, 2698, 12, 47094, 5163, 3122, 362, 281, 312, 1075, 281, 360, 341, 588], "temperature": 0.0, "avg_logprob": -0.16804383440715512, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.0615995961416047e-05}, {"id": 1054, "seek": 695556, "start": 6972.120000000001, "end": 6977.64, "text": " quickly and very accurately. But really a whole range of computer vision problems need", "tokens": [2661, 293, 588, 20095, 13, 583, 534, 257, 1379, 3613, 295, 3820, 5201, 2740, 643], "temperature": 0.0, "avg_logprob": -0.16804383440715512, "compression_ratio": 1.6093023255813954, "no_speech_prob": 1.0615995961416047e-05}, {"id": 1055, "seek": 697764, "start": 6977.64, "end": 6988.76, "text": " to be able to do this. For example, last week we saw that hackathon-winning entry from a", "tokens": [281, 312, 1075, 281, 360, 341, 13, 1171, 1365, 11, 1036, 1243, 321, 1866, 300, 10339, 18660, 12, 32960, 8729, 490, 257], "temperature": 0.0, "avg_logprob": -0.1326523644583566, "compression_ratio": 1.543859649122807, "no_speech_prob": 3.4465538192307577e-06}, {"id": 1056, "seek": 697764, "start": 6988.76, "end": 6996.240000000001, "text": " couple of our students that was able to say, take this cat and blur it out, or apply this", "tokens": [1916, 295, 527, 1731, 300, 390, 1075, 281, 584, 11, 747, 341, 3857, 293, 14257, 309, 484, 11, 420, 3079, 341], "temperature": 0.0, "avg_logprob": -0.1326523644583566, "compression_ratio": 1.543859649122807, "no_speech_prob": 3.4465538192307577e-06}, {"id": 1057, "seek": 697764, "start": 6996.240000000001, "end": 7002.52, "text": " style transfer to this cat. So you need a way of being able to know exactly where the", "tokens": [3758, 5003, 281, 341, 3857, 13, 407, 291, 643, 257, 636, 295, 885, 1075, 281, 458, 2293, 689, 264], "temperature": 0.0, "avg_logprob": -0.1326523644583566, "compression_ratio": 1.543859649122807, "no_speech_prob": 3.4465538192307577e-06}, {"id": 1058, "seek": 700252, "start": 7002.52, "end": 7009.64, "text": " cat is. And for something like, one of the things they showed was remove the background.", "tokens": [3857, 307, 13, 400, 337, 746, 411, 11, 472, 295, 264, 721, 436, 4712, 390, 4159, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.17599324063137844, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.111225330736488e-05}, {"id": 1059, "seek": 700252, "start": 7009.64, "end": 7013.160000000001, "text": " Now you actually need to be able to do a really good job of segmenting out the cat, because", "tokens": [823, 291, 767, 643, 281, 312, 1075, 281, 360, 257, 534, 665, 1691, 295, 9469, 278, 484, 264, 3857, 11, 570], "temperature": 0.0, "avg_logprob": -0.17599324063137844, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.111225330736488e-05}, {"id": 1060, "seek": 700252, "start": 7013.160000000001, "end": 7019.280000000001, "text": " if you get it slightly wrong, then you remove the background. You often see this when people", "tokens": [498, 291, 483, 309, 4748, 2085, 11, 550, 291, 4159, 264, 3678, 13, 509, 2049, 536, 341, 562, 561], "temperature": 0.0, "avg_logprob": -0.17599324063137844, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.111225330736488e-05}, {"id": 1061, "seek": 700252, "start": 7019.280000000001, "end": 7025.040000000001, "text": " use Photoshop badly. You'll end up with the bit between the ears that's still there, or", "tokens": [764, 20821, 13425, 13, 509, 603, 917, 493, 365, 264, 857, 1296, 264, 8798, 300, 311, 920, 456, 11, 420], "temperature": 0.0, "avg_logprob": -0.17599324063137844, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.111225330736488e-05}, {"id": 1062, "seek": 700252, "start": 7025.040000000001, "end": 7030.200000000001, "text": " their fur looks really spiky. So if you want to create the next generation Photoshop, you", "tokens": [641, 2687, 1542, 534, 637, 1035, 88, 13, 407, 498, 291, 528, 281, 1884, 264, 958, 5125, 20821, 11, 291], "temperature": 0.0, "avg_logprob": -0.17599324063137844, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.111225330736488e-05}, {"id": 1063, "seek": 703020, "start": 7030.2, "end": 7036.4, "text": " need to be really good at this. So there's lots of reasons to be really good at this.", "tokens": [643, 281, 312, 534, 665, 412, 341, 13, 407, 456, 311, 3195, 295, 4112, 281, 312, 534, 665, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.14747107231010825, "compression_ratio": 1.9375, "no_speech_prob": 5.422183676273562e-06}, {"id": 1064, "seek": 703020, "start": 7036.4, "end": 7041.44, "text": " Now it turns out that there's a fantastic way of doing this with a fantastic name called", "tokens": [823, 309, 4523, 484, 300, 456, 311, 257, 5456, 636, 295, 884, 341, 365, 257, 5456, 1315, 1219], "temperature": 0.0, "avg_logprob": -0.14747107231010825, "compression_ratio": 1.9375, "no_speech_prob": 5.422183676273562e-06}, {"id": 1065, "seek": 703020, "start": 7041.44, "end": 7048.5199999999995, "text": " the 100-layer tiramisu. And the 100-layer tiramisu is a fully convolutional dense net.", "tokens": [264, 2319, 12, 8376, 260, 13807, 335, 25871, 13, 400, 264, 2319, 12, 8376, 260, 13807, 335, 25871, 307, 257, 4498, 45216, 304, 18011, 2533, 13], "temperature": 0.0, "avg_logprob": -0.14747107231010825, "compression_ratio": 1.9375, "no_speech_prob": 5.422183676273562e-06}, {"id": 1066, "seek": 703020, "start": 7048.5199999999995, "end": 7052.599999999999, "text": " And so therefore, we're not going to look at this yet. Instead we're going to look at", "tokens": [400, 370, 4412, 11, 321, 434, 406, 516, 281, 574, 412, 341, 1939, 13, 7156, 321, 434, 516, 281, 574, 412], "temperature": 0.0, "avg_logprob": -0.14747107231010825, "compression_ratio": 1.9375, "no_speech_prob": 5.422183676273562e-06}, {"id": 1067, "seek": 703020, "start": 7052.599999999999, "end": 7058.92, "text": " the dense net, because we can't understand the tiramisu without the dense net. So here", "tokens": [264, 18011, 2533, 11, 570, 321, 393, 380, 1223, 264, 13807, 335, 25871, 1553, 264, 18011, 2533, 13, 407, 510], "temperature": 0.0, "avg_logprob": -0.14747107231010825, "compression_ratio": 1.9375, "no_speech_prob": 5.422183676273562e-06}, {"id": 1068, "seek": 705892, "start": 7058.92, "end": 7068.08, "text": " is the paper that introduced the dense net. As it turns out, you really need to also know", "tokens": [307, 264, 3035, 300, 7268, 264, 18011, 2533, 13, 1018, 309, 4523, 484, 11, 291, 534, 643, 281, 611, 458], "temperature": 0.0, "avg_logprob": -0.16578163971772064, "compression_ratio": 1.6201117318435754, "no_speech_prob": 3.0415685614570975e-06}, {"id": 1069, "seek": 705892, "start": 7068.08, "end": 7072.32, "text": " about the dense net for other reasons. Let me show you the reason. It's only recently", "tokens": [466, 264, 18011, 2533, 337, 661, 4112, 13, 961, 385, 855, 291, 264, 1778, 13, 467, 311, 787, 3938], "temperature": 0.0, "avg_logprob": -0.16578163971772064, "compression_ratio": 1.6201117318435754, "no_speech_prob": 3.0415685614570975e-06}, {"id": 1070, "seek": 705892, "start": 7072.32, "end": 7075.68, "text": " I fully appreciated this.", "tokens": [286, 4498, 17169, 341, 13], "temperature": 0.0, "avg_logprob": -0.16578163971772064, "compression_ratio": 1.6201117318435754, "no_speech_prob": 3.0415685614570975e-06}, {"id": 1071, "seek": 705892, "start": 7075.68, "end": 7082.08, "text": " Here are the results for the dense net. And if you look down here, you'll recognize that", "tokens": [1692, 366, 264, 3542, 337, 264, 18011, 2533, 13, 400, 498, 291, 574, 760, 510, 11, 291, 603, 5521, 300], "temperature": 0.0, "avg_logprob": -0.16578163971772064, "compression_ratio": 1.6201117318435754, "no_speech_prob": 3.0415685614570975e-06}, {"id": 1072, "seek": 708208, "start": 7082.08, "end": 7090.16, "text": " it has been compared to genuinely state-of-the-art stuff. So network-in-network, highway network,", "tokens": [309, 575, 668, 5347, 281, 17839, 1785, 12, 2670, 12, 3322, 12, 446, 1507, 13, 407, 3209, 12, 259, 12, 7129, 1902, 11, 17205, 3209, 11], "temperature": 0.0, "avg_logprob": -0.19192549867450065, "compression_ratio": 1.6851851851851851, "no_speech_prob": 3.3405206067982363e-06}, {"id": 1073, "seek": 708208, "start": 7090.16, "end": 7096.24, "text": " fractal net, resnet, resnet with stochastic depth, and wide resnet. These are genuinely", "tokens": [17948, 304, 2533, 11, 725, 7129, 11, 725, 7129, 365, 342, 8997, 2750, 7161, 11, 293, 4874, 725, 7129, 13, 1981, 366, 17839], "temperature": 0.0, "avg_logprob": -0.19192549867450065, "compression_ratio": 1.6851851851851851, "no_speech_prob": 3.3405206067982363e-06}, {"id": 1074, "seek": 708208, "start": 7096.24, "end": 7104.0, "text": " state-of-the-art architectures. And it's being looked at on some heavily studied data sets.", "tokens": [1785, 12, 2670, 12, 3322, 12, 446, 6331, 1303, 13, 400, 309, 311, 885, 2956, 412, 322, 512, 10950, 9454, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.19192549867450065, "compression_ratio": 1.6851851851851851, "no_speech_prob": 3.3405206067982363e-06}, {"id": 1075, "seek": 708208, "start": 7104.0, "end": 7111.0, "text": " This is CyPhar 10, this is CyPhar 100. Plus is with data augmentation, plus is without", "tokens": [639, 307, 10295, 47, 5854, 1266, 11, 341, 307, 10295, 47, 5854, 2319, 13, 7721, 307, 365, 1412, 14501, 19631, 11, 1804, 307, 1553], "temperature": 0.0, "avg_logprob": -0.19192549867450065, "compression_ratio": 1.6851851851851851, "no_speech_prob": 3.3405206067982363e-06}, {"id": 1076, "seek": 711100, "start": 7111.0, "end": 7120.28, "text": " data augmentation. So CyPhar 100, the previous state-of-the-art, this is massively well studied,", "tokens": [1412, 14501, 19631, 13, 407, 10295, 47, 5854, 2319, 11, 264, 3894, 1785, 12, 2670, 12, 3322, 12, 446, 11, 341, 307, 29379, 731, 9454, 11], "temperature": 0.0, "avg_logprob": -0.1445712916056315, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.1911041610801476e-06}, {"id": 1077, "seek": 711100, "start": 7120.28, "end": 7129.76, "text": " was 28. And that itself was way above everybody else. This paper got 19.5%. You guys have", "tokens": [390, 7562, 13, 400, 300, 2564, 390, 636, 3673, 2201, 1646, 13, 639, 3035, 658, 1294, 13, 20, 6856, 509, 1074, 362], "temperature": 0.0, "avg_logprob": -0.1445712916056315, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.1911041610801476e-06}, {"id": 1078, "seek": 711100, "start": 7129.76, "end": 7136.88, "text": " seen enough of these now to know that you don't decrease by 30-plus percent with computer", "tokens": [1612, 1547, 295, 613, 586, 281, 458, 300, 291, 500, 380, 11514, 538, 2217, 12, 18954, 3043, 365, 3820], "temperature": 0.0, "avg_logprob": -0.1445712916056315, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.1911041610801476e-06}, {"id": 1079, "seek": 713688, "start": 7136.88, "end": 7141.4800000000005, "text": " vision nowadays against state-of-the-art results. So this is like a huge advance.", "tokens": [5201, 13434, 1970, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 13, 407, 341, 307, 411, 257, 2603, 7295, 13], "temperature": 0.0, "avg_logprob": -0.20213631660707534, "compression_ratio": 1.547008547008547, "no_speech_prob": 2.4439771095785545e-06}, {"id": 1080, "seek": 713688, "start": 7141.4800000000005, "end": 7146.52, "text": " Now the reason this huge advance is important is because specifically these no data augmentation", "tokens": [823, 264, 1778, 341, 2603, 7295, 307, 1021, 307, 570, 4682, 613, 572, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.20213631660707534, "compression_ratio": 1.547008547008547, "no_speech_prob": 2.4439771095785545e-06}, {"id": 1081, "seek": 713688, "start": 7146.52, "end": 7155.36, "text": " columns, so here's the CyPhar 10 one, similar thing, going down from 7.3 to 5.1, 20-30%", "tokens": [13766, 11, 370, 510, 311, 264, 10295, 47, 5854, 1266, 472, 11, 2531, 551, 11, 516, 760, 490, 1614, 13, 18, 281, 1025, 13, 16, 11, 945, 12, 3446, 4], "temperature": 0.0, "avg_logprob": -0.20213631660707534, "compression_ratio": 1.547008547008547, "no_speech_prob": 2.4439771095785545e-06}, {"id": 1082, "seek": 713688, "start": 7155.36, "end": 7162.4400000000005, "text": " decrease as well. What these represent, these without data augmentation columns, they basically", "tokens": [11514, 382, 731, 13, 708, 613, 2906, 11, 613, 1553, 1412, 14501, 19631, 13766, 11, 436, 1936], "temperature": 0.0, "avg_logprob": -0.20213631660707534, "compression_ratio": 1.547008547008547, "no_speech_prob": 2.4439771095785545e-06}, {"id": 1083, "seek": 716244, "start": 7162.44, "end": 7168.24, "text": " represent the performance of this data set on a limited amount of data. If you're not", "tokens": [2906, 264, 3389, 295, 341, 1412, 992, 322, 257, 5567, 2372, 295, 1412, 13, 759, 291, 434, 406], "temperature": 0.0, "avg_logprob": -0.09965669980613134, "compression_ratio": 1.6116071428571428, "no_speech_prob": 4.029419869766571e-06}, {"id": 1084, "seek": 716244, "start": 7168.24, "end": 7174.32, "text": " using data augmentation, you're basically forcing it to have to work with less data.", "tokens": [1228, 1412, 14501, 19631, 11, 291, 434, 1936, 19030, 309, 281, 362, 281, 589, 365, 1570, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09965669980613134, "compression_ratio": 1.6116071428571428, "no_speech_prob": 4.029419869766571e-06}, {"id": 1085, "seek": 716244, "start": 7174.32, "end": 7181.419999999999, "text": " And I know that a huge number of you are wanting to build stuff where you don't have much data.", "tokens": [400, 286, 458, 300, 257, 2603, 1230, 295, 291, 366, 7935, 281, 1322, 1507, 689, 291, 500, 380, 362, 709, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09965669980613134, "compression_ratio": 1.6116071428571428, "no_speech_prob": 4.029419869766571e-06}, {"id": 1086, "seek": 716244, "start": 7181.419999999999, "end": 7186.96, "text": " So if you're one of those people, you definitely need to use DenseNet. At this stage, DenseNet", "tokens": [407, 498, 291, 434, 472, 295, 729, 561, 11, 291, 2138, 643, 281, 764, 413, 1288, 31890, 13, 1711, 341, 3233, 11, 413, 1288, 31890], "temperature": 0.0, "avg_logprob": -0.09965669980613134, "compression_ratio": 1.6116071428571428, "no_speech_prob": 4.029419869766571e-06}, {"id": 1087, "seek": 718696, "start": 7186.96, "end": 7195.92, "text": " is by far the state-of-the-art result for data sets where you don't have much data.", "tokens": [307, 538, 1400, 264, 1785, 12, 2670, 12, 3322, 12, 446, 1874, 337, 1412, 6352, 689, 291, 500, 380, 362, 709, 1412, 13], "temperature": 0.0, "avg_logprob": -0.10607758748162653, "compression_ratio": 1.599078341013825, "no_speech_prob": 2.601595724627259e-06}, {"id": 1088, "seek": 718696, "start": 7195.92, "end": 7199.4, "text": " So I want to teach you about DenseNet for two reasons. The first is so that next week", "tokens": [407, 286, 528, 281, 2924, 291, 466, 413, 1288, 31890, 337, 732, 4112, 13, 440, 700, 307, 370, 300, 958, 1243], "temperature": 0.0, "avg_logprob": -0.10607758748162653, "compression_ratio": 1.599078341013825, "no_speech_prob": 2.601595724627259e-06}, {"id": 1089, "seek": 718696, "start": 7199.4, "end": 7205.96, "text": " we can learn about the Tiramisu, 100 layer Tiramisu, but also so that we can find out", "tokens": [321, 393, 1466, 466, 264, 314, 40359, 25871, 11, 2319, 4583, 314, 40359, 25871, 11, 457, 611, 370, 300, 321, 393, 915, 484], "temperature": 0.0, "avg_logprob": -0.10607758748162653, "compression_ratio": 1.599078341013825, "no_speech_prob": 2.601595724627259e-06}, {"id": 1090, "seek": 718696, "start": 7205.96, "end": 7214.32, "text": " how to create way, way, way, way better computer vision models where you have limited data.", "tokens": [577, 281, 1884, 636, 11, 636, 11, 636, 11, 636, 1101, 3820, 5201, 5245, 689, 291, 362, 5567, 1412, 13], "temperature": 0.0, "avg_logprob": -0.10607758748162653, "compression_ratio": 1.599078341013825, "no_speech_prob": 2.601595724627259e-06}, {"id": 1091, "seek": 721432, "start": 7214.32, "end": 7220.36, "text": " So let's learn how to do this. And I can actually describe it in a simple sentence, a single", "tokens": [407, 718, 311, 1466, 577, 281, 360, 341, 13, 400, 286, 393, 767, 6786, 309, 294, 257, 2199, 8174, 11, 257, 2167], "temperature": 0.0, "avg_logprob": -0.1357652637320505, "compression_ratio": 1.5, "no_speech_prob": 2.52155427915568e-06}, {"id": 1092, "seek": 721432, "start": 7220.36, "end": 7229.88, "text": " sentence. A DenseNet is a ResNet where you replace addition with concatenation. That's", "tokens": [8174, 13, 316, 413, 1288, 31890, 307, 257, 5015, 31890, 689, 291, 7406, 4500, 365, 1588, 7186, 399, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.1357652637320505, "compression_ratio": 1.5, "no_speech_prob": 2.52155427915568e-06}, {"id": 1093, "seek": 721432, "start": 7229.88, "end": 7238.799999999999, "text": " actually the entirety of what a DenseNet is. But understanding what it is and why it works", "tokens": [767, 264, 31557, 295, 437, 257, 413, 1288, 31890, 307, 13, 583, 3701, 437, 309, 307, 293, 983, 309, 1985], "temperature": 0.0, "avg_logprob": -0.1357652637320505, "compression_ratio": 1.5, "no_speech_prob": 2.52155427915568e-06}, {"id": 1094, "seek": 723880, "start": 7238.8, "end": 7245.84, "text": " is more important. So let's remind ourselves about ResNet. We've looked at this many times,", "tokens": [307, 544, 1021, 13, 407, 718, 311, 4160, 4175, 466, 5015, 31890, 13, 492, 600, 2956, 412, 341, 867, 1413, 11], "temperature": 0.0, "avg_logprob": -0.13780855095904807, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.048864179319935e-06}, {"id": 1095, "seek": 723880, "start": 7245.84, "end": 7257.24, "text": " but there's no harm in just reminding ourselves. With ResNet, we have some input, we put it", "tokens": [457, 456, 311, 572, 6491, 294, 445, 27639, 4175, 13, 2022, 5015, 31890, 11, 321, 362, 512, 4846, 11, 321, 829, 309], "temperature": 0.0, "avg_logprob": -0.13780855095904807, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.048864179319935e-06}, {"id": 1096, "seek": 723880, "start": 7257.24, "end": 7267.320000000001, "text": " through a convolution to get some activations, and another convolution to get some activations.", "tokens": [807, 257, 45216, 281, 483, 512, 2430, 763, 11, 293, 1071, 45216, 281, 483, 512, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.13780855095904807, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.048864179319935e-06}, {"id": 1097, "seek": 726732, "start": 7267.32, "end": 7285.32, "text": " And we also have the identity. And this is addition. So basically we end up with our", "tokens": [400, 321, 611, 362, 264, 6575, 13, 400, 341, 307, 4500, 13, 407, 1936, 321, 917, 493, 365, 527], "temperature": 0.0, "avg_logprob": -0.22476544587508493, "compression_ratio": 1.0632911392405062, "no_speech_prob": 7.411169917759253e-06}, {"id": 1098, "seek": 728532, "start": 7285.32, "end": 7301.759999999999, "text": " layer T plus 1 equals some function, the convolutions that is, of a layer T plus layer T itself.", "tokens": [4583, 314, 1804, 502, 6915, 512, 2445, 11, 264, 3754, 15892, 300, 307, 11, 295, 257, 4583, 314, 1804, 4583, 314, 2564, 13], "temperature": 0.0, "avg_logprob": -0.26844391999421297, "compression_ratio": 1.1851851851851851, "no_speech_prob": 4.425452061695978e-06}, {"id": 1099, "seek": 730176, "start": 7301.76, "end": 7316.16, "text": " And then to remind you, what I've normally shown after that is to say our function equals", "tokens": [400, 550, 281, 4160, 291, 11, 437, 286, 600, 5646, 4898, 934, 300, 307, 281, 584, 527, 2445, 6915], "temperature": 0.0, "avg_logprob": -0.23788471655412155, "compression_ratio": 1.3357664233576643, "no_speech_prob": 4.222803909215145e-06}, {"id": 1100, "seek": 730176, "start": 7316.16, "end": 7323.8, "text": " the difference. So it's basically calculating a residual, a function that can find the error.", "tokens": [264, 2649, 13, 407, 309, 311, 1936, 28258, 257, 27980, 11, 257, 2445, 300, 393, 915, 264, 6713, 13], "temperature": 0.0, "avg_logprob": -0.23788471655412155, "compression_ratio": 1.3357664233576643, "no_speech_prob": 4.222803909215145e-06}, {"id": 1101, "seek": 732380, "start": 7323.8, "end": 7331.76, "text": " So every time we look at ResNet, we look at it this way. So what if we do exactly the", "tokens": [407, 633, 565, 321, 574, 412, 5015, 31890, 11, 321, 574, 412, 309, 341, 636, 13, 407, 437, 498, 321, 360, 2293, 264], "temperature": 0.0, "avg_logprob": -0.15315658219006598, "compression_ratio": 1.3409090909090908, "no_speech_prob": 4.936956429446582e-06}, {"id": 1102, "seek": 732380, "start": 7331.76, "end": 7347.84, "text": " same thing, but we replace that with concatenate, drawing them together. And remember, this", "tokens": [912, 551, 11, 457, 321, 7406, 300, 365, 1588, 7186, 473, 11, 6316, 552, 1214, 13, 400, 1604, 11, 341], "temperature": 0.0, "avg_logprob": -0.15315658219006598, "compression_ratio": 1.3409090909090908, "no_speech_prob": 4.936956429446582e-06}, {"id": 1103, "seek": 734784, "start": 7347.84, "end": 7358.32, "text": " is just one block. So what that means is that after the first layer, we have both the result", "tokens": [307, 445, 472, 3461, 13, 407, 437, 300, 1355, 307, 300, 934, 264, 700, 4583, 11, 321, 362, 1293, 264, 1874], "temperature": 0.0, "avg_logprob": -0.197076754136519, "compression_ratio": 1.6729559748427674, "no_speech_prob": 5.255378255242249e-06}, {"id": 1104, "seek": 734784, "start": 7358.32, "end": 7366.4800000000005, "text": " of some convolutions and the original input. We literally copied it and concatenated it.", "tokens": [295, 512, 3754, 15892, 293, 264, 3380, 4846, 13, 492, 3736, 25365, 309, 293, 1588, 7186, 770, 309, 13], "temperature": 0.0, "avg_logprob": -0.197076754136519, "compression_ratio": 1.6729559748427674, "no_speech_prob": 5.255378255242249e-06}, {"id": 1105, "seek": 734784, "start": 7366.4800000000005, "end": 7374.04, "text": " And then after the second layer, we've got some convolutions on convolutions and the", "tokens": [400, 550, 934, 264, 1150, 4583, 11, 321, 600, 658, 512, 3754, 15892, 322, 3754, 15892, 293, 264], "temperature": 0.0, "avg_logprob": -0.197076754136519, "compression_ratio": 1.6729559748427674, "no_speech_prob": 5.255378255242249e-06}, {"id": 1106, "seek": 737404, "start": 7374.04, "end": 7381.04, "text": " original first layer of convolutions and the original input. And furthermore, that second", "tokens": [3380, 700, 4583, 295, 3754, 15892, 293, 264, 3380, 4846, 13, 400, 3052, 3138, 11, 300, 1150], "temperature": 0.0, "avg_logprob": -0.15040026138077922, "compression_ratio": 1.952054794520548, "no_speech_prob": 2.8573069812409813e-06}, {"id": 1107, "seek": 737404, "start": 7381.04, "end": 7387.8, "text": " layer of convolutions was itself operating on this concat. So that second layer of convolutions", "tokens": [4583, 295, 3754, 15892, 390, 2564, 7447, 322, 341, 1588, 267, 13, 407, 300, 1150, 4583, 295, 3754, 15892], "temperature": 0.0, "avg_logprob": -0.15040026138077922, "compression_ratio": 1.952054794520548, "no_speech_prob": 2.8573069812409813e-06}, {"id": 1108, "seek": 737404, "start": 7387.8, "end": 7395.08, "text": " was operating both on the original data as well as on the outcome of the first set of", "tokens": [390, 7447, 1293, 322, 264, 3380, 1412, 382, 731, 382, 322, 264, 9700, 295, 264, 700, 992, 295], "temperature": 0.0, "avg_logprob": -0.15040026138077922, "compression_ratio": 1.952054794520548, "no_speech_prob": 2.8573069812409813e-06}, {"id": 1109, "seek": 737404, "start": 7395.08, "end": 7396.08, "text": " convolutions.", "tokens": [3754, 15892, 13], "temperature": 0.0, "avg_logprob": -0.15040026138077922, "compression_ratio": 1.952054794520548, "no_speech_prob": 2.8573069812409813e-06}, {"id": 1110, "seek": 739608, "start": 7396.08, "end": 7405.28, "text": " When people draw dense nets, they tend to draw it like this. They show every layer going", "tokens": [1133, 561, 2642, 18011, 36170, 11, 436, 3928, 281, 2642, 309, 411, 341, 13, 814, 855, 633, 4583, 516], "temperature": 0.0, "avg_logprob": -0.13515347103739894, "compression_ratio": 1.662037037037037, "no_speech_prob": 1.529408791611786e-06}, {"id": 1111, "seek": 739608, "start": 7405.28, "end": 7412.08, "text": " to every layer after it. Now I didn't define it that way because it's much easier in practice", "tokens": [281, 633, 4583, 934, 309, 13, 823, 286, 994, 380, 6964, 309, 300, 636, 570, 309, 311, 709, 3571, 294, 3124], "temperature": 0.0, "avg_logprob": -0.13515347103739894, "compression_ratio": 1.662037037037037, "no_speech_prob": 1.529408791611786e-06}, {"id": 1112, "seek": 739608, "start": 7412.08, "end": 7418.32, "text": " to implement it by just saying each layer equals all of the previous layer concatenated", "tokens": [281, 4445, 309, 538, 445, 1566, 1184, 4583, 6915, 439, 295, 264, 3894, 4583, 1588, 7186, 770], "temperature": 0.0, "avg_logprob": -0.13515347103739894, "compression_ratio": 1.662037037037037, "no_speech_prob": 1.529408791611786e-06}, {"id": 1113, "seek": 739608, "start": 7418.32, "end": 7424.68, "text": " with the convolution on top of the previous layer. So you concatenate recursively, means", "tokens": [365, 264, 45216, 322, 1192, 295, 264, 3894, 4583, 13, 407, 291, 1588, 7186, 473, 20560, 3413, 11, 1355], "temperature": 0.0, "avg_logprob": -0.13515347103739894, "compression_ratio": 1.662037037037037, "no_speech_prob": 1.529408791611786e-06}, {"id": 1114, "seek": 742468, "start": 7424.68, "end": 7430.280000000001, "text": " that you always have all of your previous layers there as well. So sometimes people", "tokens": [300, 291, 1009, 362, 439, 295, 428, 3894, 7914, 456, 382, 731, 13, 407, 2171, 561], "temperature": 0.0, "avg_logprob": -0.15104870981984325, "compression_ratio": 1.766497461928934, "no_speech_prob": 8.939608960645273e-06}, {"id": 1115, "seek": 742468, "start": 7430.280000000001, "end": 7435.64, "text": " get confused when they see this picture where everything is shown connecting to each other,", "tokens": [483, 9019, 562, 436, 536, 341, 3036, 689, 1203, 307, 4898, 11015, 281, 1184, 661, 11], "temperature": 0.0, "avg_logprob": -0.15104870981984325, "compression_ratio": 1.766497461928934, "no_speech_prob": 8.939608960645273e-06}, {"id": 1116, "seek": 742468, "start": 7435.64, "end": 7440.240000000001, "text": " but then you look at the code and it looks like it's only connected to the previous layer.", "tokens": [457, 550, 291, 574, 412, 264, 3089, 293, 309, 1542, 411, 309, 311, 787, 4582, 281, 264, 3894, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15104870981984325, "compression_ratio": 1.766497461928934, "no_speech_prob": 8.939608960645273e-06}, {"id": 1117, "seek": 742468, "start": 7440.240000000001, "end": 7448.3, "text": " And that's because the previous layer itself was connected to the previous layer.", "tokens": [400, 300, 311, 570, 264, 3894, 4583, 2564, 390, 4582, 281, 264, 3894, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15104870981984325, "compression_ratio": 1.766497461928934, "no_speech_prob": 8.939608960645273e-06}, {"id": 1118, "seek": 744830, "start": 7448.3, "end": 7460.8, "text": " So because we keep concatenating, the number of filters is getting bigger and bigger. So", "tokens": [407, 570, 321, 1066, 1588, 7186, 990, 11, 264, 1230, 295, 15995, 307, 1242, 3801, 293, 3801, 13, 407], "temperature": 0.0, "avg_logprob": -0.08785987936932108, "compression_ratio": 1.4435483870967742, "no_speech_prob": 5.714998678740812e-07}, {"id": 1119, "seek": 744830, "start": 7460.8, "end": 7468.2, "text": " we're going to have to be careful not to add too many filters at each layer. So the number", "tokens": [321, 434, 516, 281, 362, 281, 312, 5026, 406, 281, 909, 886, 867, 15995, 412, 1184, 4583, 13, 407, 264, 1230], "temperature": 0.0, "avg_logprob": -0.08785987936932108, "compression_ratio": 1.4435483870967742, "no_speech_prob": 5.714998678740812e-07}, {"id": 1120, "seek": 746820, "start": 7468.2, "end": 7478.4, "text": " of filters that are added at each layer, they call the growth rate. And for some reason", "tokens": [295, 15995, 300, 366, 3869, 412, 1184, 4583, 11, 436, 818, 264, 4599, 3314, 13, 400, 337, 512, 1778], "temperature": 0.0, "avg_logprob": -0.16520400757485248, "compression_ratio": 1.403225806451613, "no_speech_prob": 7.571136393380584e-07}, {"id": 1121, "seek": 746820, "start": 7478.4, "end": 7489.88, "text": " they use the letter K. That is K for growth rate. They tend to use the values of 12 or", "tokens": [436, 764, 264, 5063, 591, 13, 663, 307, 591, 337, 4599, 3314, 13, 814, 3928, 281, 764, 264, 4190, 295, 2272, 420], "temperature": 0.0, "avg_logprob": -0.16520400757485248, "compression_ratio": 1.403225806451613, "no_speech_prob": 7.571136393380584e-07}, {"id": 1122, "seek": 748988, "start": 7489.88, "end": 7499.04, "text": " 24. In the tiramisu paper, they tend to use the value of 16. So every layer has 12 more", "tokens": [4022, 13, 682, 264, 13807, 335, 25871, 3035, 11, 436, 3928, 281, 764, 264, 2158, 295, 3165, 13, 407, 633, 4583, 575, 2272, 544], "temperature": 0.0, "avg_logprob": -0.1560427188873291, "compression_ratio": 1.3703703703703705, "no_speech_prob": 1.0348492196499137e-06}, {"id": 1123, "seek": 748988, "start": 7499.04, "end": 7508.08, "text": " filters than the previous layer. And you generally can have like 100 layers. So after 100 layers,", "tokens": [15995, 813, 264, 3894, 4583, 13, 400, 291, 5101, 393, 362, 411, 2319, 7914, 13, 407, 934, 2319, 7914, 11], "temperature": 0.0, "avg_logprob": -0.1560427188873291, "compression_ratio": 1.3703703703703705, "no_speech_prob": 1.0348492196499137e-06}, {"id": 1124, "seek": 750808, "start": 7508.08, "end": 7520.28, "text": " you could have 1200 or 2400 filters, which is getting to be quite a lot.", "tokens": [291, 727, 362, 29139, 420, 4022, 628, 15995, 11, 597, 307, 1242, 281, 312, 1596, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.1854089411293588, "compression_ratio": 1.2635658914728682, "no_speech_prob": 1.3287698266140069e-06}, {"id": 1125, "seek": 750808, "start": 7520.28, "end": 7528.84, "text": " Interestingly, although they kind of add up, you actually end up with less parameters than", "tokens": [30564, 11, 4878, 436, 733, 295, 909, 493, 11, 291, 767, 917, 493, 365, 1570, 9834, 813], "temperature": 0.0, "avg_logprob": -0.1854089411293588, "compression_ratio": 1.2635658914728682, "no_speech_prob": 1.3287698266140069e-06}, {"id": 1126, "seek": 752884, "start": 7528.84, "end": 7538.88, "text": " normal. So you can see that this here, CyPhar 10, even this is a state-of-the-art result,", "tokens": [2710, 13, 407, 291, 393, 536, 300, 341, 510, 11, 10295, 47, 5854, 1266, 11, 754, 341, 307, 257, 1785, 12, 2670, 12, 3322, 12, 446, 1874, 11], "temperature": 0.0, "avg_logprob": -0.19955367512173122, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.0415799301408697e-06}, {"id": 1127, "seek": 752884, "start": 7538.88, "end": 7544.64, "text": " and this is with only a million parameters. So this is beating ResNet with 10 million", "tokens": [293, 341, 307, 365, 787, 257, 2459, 9834, 13, 407, 341, 307, 13497, 5015, 31890, 365, 1266, 2459], "temperature": 0.0, "avg_logprob": -0.19955367512173122, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.0415799301408697e-06}, {"id": 1128, "seek": 752884, "start": 7544.64, "end": 7555.360000000001, "text": " parameters by a third. This is why it's working so well with so little data.", "tokens": [9834, 538, 257, 2636, 13, 639, 307, 983, 309, 311, 1364, 370, 731, 365, 370, 707, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19955367512173122, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.0415799301408697e-06}, {"id": 1129, "seek": 755536, "start": 7555.36, "end": 7562.92, "text": " I'm not convinced that this is the right approach for, or that it's a massively better approach", "tokens": [286, 478, 406, 12561, 300, 341, 307, 264, 558, 3109, 337, 11, 420, 300, 309, 311, 257, 29379, 1101, 3109], "temperature": 0.0, "avg_logprob": -0.17073257764180502, "compression_ratio": 1.7201834862385321, "no_speech_prob": 7.183228717622114e-06}, {"id": 1130, "seek": 755536, "start": 7562.92, "end": 7570.48, "text": " for ImageNet. This is the picture for ImageNet. This is the important one. The number of flops,", "tokens": [337, 29903, 31890, 13, 639, 307, 264, 3036, 337, 29903, 31890, 13, 639, 307, 264, 1021, 472, 13, 440, 1230, 295, 932, 3370, 11], "temperature": 0.0, "avg_logprob": -0.17073257764180502, "compression_ratio": 1.7201834862385321, "no_speech_prob": 7.183228717622114e-06}, {"id": 1131, "seek": 755536, "start": 7570.48, "end": 7574.4, "text": " so this is the number of floating-point operations, the amount of time it takes your computer", "tokens": [370, 341, 307, 264, 1230, 295, 12607, 12, 6053, 7705, 11, 264, 2372, 295, 565, 309, 2516, 428, 3820], "temperature": 0.0, "avg_logprob": -0.17073257764180502, "compression_ratio": 1.7201834862385321, "no_speech_prob": 7.183228717622114e-06}, {"id": 1132, "seek": 755536, "start": 7574.4, "end": 7583.08, "text": " to do it, versus the error rate. You can see that DenseNet and ResNet have about the same", "tokens": [281, 360, 309, 11, 5717, 264, 6713, 3314, 13, 509, 393, 536, 300, 413, 1288, 31890, 293, 5015, 31890, 362, 466, 264, 912], "temperature": 0.0, "avg_logprob": -0.17073257764180502, "compression_ratio": 1.7201834862385321, "no_speech_prob": 7.183228717622114e-06}, {"id": 1133, "seek": 758308, "start": 7583.08, "end": 7592.68, "text": " error rate. DenseNet is about twice as fast, a bit less. It's still better, but it's not", "tokens": [6713, 3314, 13, 413, 1288, 31890, 307, 466, 6091, 382, 2370, 11, 257, 857, 1570, 13, 467, 311, 920, 1101, 11, 457, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.22641277313232422, "compression_ratio": 1.3308823529411764, "no_speech_prob": 2.769399770841119e-06}, {"id": 1134, "seek": 758308, "start": 7592.68, "end": 7601.64, "text": " massively better. There are actually better architectures than ResNet for ImageNet nowadays.", "tokens": [29379, 1101, 13, 821, 366, 767, 1101, 6331, 1303, 813, 5015, 31890, 337, 29903, 31890, 13434, 13], "temperature": 0.0, "avg_logprob": -0.22641277313232422, "compression_ratio": 1.3308823529411764, "no_speech_prob": 2.769399770841119e-06}, {"id": 1135, "seek": 760164, "start": 7601.64, "end": 7615.52, "text": " So really, if you're using something that's more of the 100-100,000 images range, you", "tokens": [407, 534, 11, 498, 291, 434, 1228, 746, 300, 311, 544, 295, 264, 2319, 12, 6879, 11, 1360, 5267, 3613, 11, 291], "temperature": 0.0, "avg_logprob": -0.19182410993074117, "compression_ratio": 1.3428571428571427, "no_speech_prob": 7.183220532169798e-06}, {"id": 1136, "seek": 760164, "start": 7615.52, "end": 7620.52, "text": " probably want to be using DenseNet. If it's more than 100,000 images, maybe it doesn't", "tokens": [1391, 528, 281, 312, 1228, 413, 1288, 31890, 13, 759, 309, 311, 544, 813, 2319, 11, 1360, 5267, 11, 1310, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.19182410993074117, "compression_ratio": 1.3428571428571427, "no_speech_prob": 7.183220532169798e-06}, {"id": 1137, "seek": 760164, "start": 7620.52, "end": 7621.52, "text": " matter so much.", "tokens": [1871, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.19182410993074117, "compression_ratio": 1.3428571428571427, "no_speech_prob": 7.183220532169798e-06}, {"id": 1138, "seek": 762152, "start": 7621.52, "end": 7638.96, "text": " So interestingly, this turned out to be something that suited Keras really well. These kind", "tokens": [407, 25873, 11, 341, 3574, 484, 281, 312, 746, 300, 24736, 591, 6985, 534, 731, 13, 1981, 733], "temperature": 0.0, "avg_logprob": -0.15614706132470108, "compression_ratio": 1.3533834586466165, "no_speech_prob": 1.6028049003580236e-06}, {"id": 1139, "seek": 762152, "start": 7638.96, "end": 7646.68, "text": " of things where you're using standard kinds of layers connected in different ways, Keras", "tokens": [295, 721, 689, 291, 434, 1228, 3832, 3685, 295, 7914, 4582, 294, 819, 2098, 11, 591, 6985], "temperature": 0.0, "avg_logprob": -0.15614706132470108, "compression_ratio": 1.3533834586466165, "no_speech_prob": 1.6028049003580236e-06}, {"id": 1140, "seek": 764668, "start": 7646.68, "end": 7653.320000000001, "text": " is fantastically great for it, as you'll see. So I'm going to use Cypher 10. I just copied", "tokens": [307, 4115, 22808, 869, 337, 309, 11, 382, 291, 603, 536, 13, 407, 286, 478, 516, 281, 764, 10295, 79, 511, 1266, 13, 286, 445, 25365], "temperature": 0.0, "avg_logprob": -0.18714663746592763, "compression_ratio": 1.4814814814814814, "no_speech_prob": 1.4509954780805856e-05}, {"id": 1141, "seek": 764668, "start": 7653.320000000001, "end": 7662.200000000001, "text": " and pasted this basically from the Keras.datasets.cypher10. So there's an example of Cypher 10. It's a", "tokens": [293, 1791, 292, 341, 1936, 490, 264, 591, 6985, 13, 20367, 296, 1385, 13, 1344, 79, 511, 3279, 13, 407, 456, 311, 364, 1365, 295, 10295, 79, 511, 1266, 13, 467, 311, 257], "temperature": 0.0, "avg_logprob": -0.18714663746592763, "compression_ratio": 1.4814814814814814, "no_speech_prob": 1.4509954780805856e-05}, {"id": 1142, "seek": 764668, "start": 7662.200000000001, "end": 7670.8, "text": " funnier dataset, just 32x32 pixel images. So that's what a Cypher 10 truck looks like.", "tokens": [1019, 19165, 28872, 11, 445, 8858, 87, 11440, 19261, 5267, 13, 407, 300, 311, 437, 257, 10295, 79, 511, 1266, 5898, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.18714663746592763, "compression_ratio": 1.4814814814814814, "no_speech_prob": 1.4509954780805856e-05}, {"id": 1143, "seek": 767080, "start": 7670.8, "end": 7677.66, "text": " They're from 0-255, I want to make them 0-1, so I have to apply them by 255.", "tokens": [814, 434, 490, 1958, 12, 6074, 20, 11, 286, 528, 281, 652, 552, 1958, 12, 16, 11, 370, 286, 362, 281, 3079, 552, 538, 3552, 20, 13], "temperature": 0.0, "avg_logprob": -0.22923012239387236, "compression_ratio": 1.4628571428571429, "no_speech_prob": 6.144102371763438e-06}, {"id": 1144, "seek": 767080, "start": 7677.66, "end": 7682.4800000000005, "text": " So we're going to try to figure out that this is a truck. We have 10 categories in Cypher", "tokens": [407, 321, 434, 516, 281, 853, 281, 2573, 484, 300, 341, 307, 257, 5898, 13, 492, 362, 1266, 10479, 294, 10295, 79, 511], "temperature": 0.0, "avg_logprob": -0.22923012239387236, "compression_ratio": 1.4628571428571429, "no_speech_prob": 6.144102371763438e-06}, {"id": 1145, "seek": 767080, "start": 7682.4800000000005, "end": 7686.56, "text": " 10.", "tokens": [1266, 13], "temperature": 0.0, "avg_logprob": -0.22923012239387236, "compression_ratio": 1.4628571428571429, "no_speech_prob": 6.144102371763438e-06}, {"id": 1146, "seek": 767080, "start": 7686.56, "end": 7695.360000000001, "text": " So the less code that you write, the less chance that there is for an error. So I try", "tokens": [407, 264, 1570, 3089, 300, 291, 2464, 11, 264, 1570, 2931, 300, 456, 307, 337, 364, 6713, 13, 407, 286, 853], "temperature": 0.0, "avg_logprob": -0.22923012239387236, "compression_ratio": 1.4628571428571429, "no_speech_prob": 6.144102371763438e-06}, {"id": 1147, "seek": 769536, "start": 7695.36, "end": 7701.5199999999995, "text": " to refactor out everything that happens more than once. So even a simple thing like activation", "tokens": [281, 1895, 15104, 484, 1203, 300, 2314, 544, 813, 1564, 13, 407, 754, 257, 2199, 551, 411, 24433], "temperature": 0.0, "avg_logprob": -0.20998429663387347, "compression_ratio": 1.7039106145251397, "no_speech_prob": 4.157336661592126e-06}, {"id": 1148, "seek": 769536, "start": 7701.5199999999995, "end": 7710.88, "text": " relu, I create a function for. Dropout, if you have dropout, I create a function for.", "tokens": [1039, 84, 11, 286, 1884, 257, 2445, 337, 13, 17675, 346, 11, 498, 291, 362, 3270, 346, 11, 286, 1884, 257, 2445, 337, 13], "temperature": 0.0, "avg_logprob": -0.20998429663387347, "compression_ratio": 1.7039106145251397, "no_speech_prob": 4.157336661592126e-06}, {"id": 1149, "seek": 769536, "start": 7710.88, "end": 7716.48, "text": " Batch norm with a particular mode and axis is a function for. And then applying relu", "tokens": [363, 852, 2026, 365, 257, 1729, 4391, 293, 10298, 307, 257, 2445, 337, 13, 400, 550, 9275, 1039, 84], "temperature": 0.0, "avg_logprob": -0.20998429663387347, "compression_ratio": 1.7039106145251397, "no_speech_prob": 4.157336661592126e-06}, {"id": 1150, "seek": 769536, "start": 7716.48, "end": 7722.5599999999995, "text": " on top of batch norm is a function for.", "tokens": [322, 1192, 295, 15245, 2026, 307, 257, 2445, 337, 13], "temperature": 0.0, "avg_logprob": -0.20998429663387347, "compression_ratio": 1.7039106145251397, "no_speech_prob": 4.157336661592126e-06}, {"id": 1151, "seek": 772256, "start": 7722.56, "end": 7730.56, "text": " I always have this in it, this border mode, this L2 regularization, and this dropout,", "tokens": [286, 1009, 362, 341, 294, 309, 11, 341, 7838, 4391, 11, 341, 441, 17, 3890, 2144, 11, 293, 341, 3270, 346, 11], "temperature": 0.0, "avg_logprob": -0.17016796949433116, "compression_ratio": 1.6629213483146068, "no_speech_prob": 3.9669735087954905e-06}, {"id": 1152, "seek": 772256, "start": 7730.56, "end": 7744.4800000000005, "text": " so there's a function for. Then we have batch norm, then relu, then convolution and dropout,", "tokens": [370, 456, 311, 257, 2445, 337, 13, 1396, 321, 362, 15245, 2026, 11, 550, 1039, 84, 11, 550, 45216, 293, 3270, 346, 11], "temperature": 0.0, "avg_logprob": -0.17016796949433116, "compression_ratio": 1.6629213483146068, "no_speech_prob": 3.9669735087954905e-06}, {"id": 1153, "seek": 772256, "start": 7744.4800000000005, "end": 7746.6, "text": " so there's a function for.", "tokens": [370, 456, 311, 257, 2445, 337, 13], "temperature": 0.0, "avg_logprob": -0.17016796949433116, "compression_ratio": 1.6629213483146068, "no_speech_prob": 3.9669735087954905e-06}, {"id": 1154, "seek": 772256, "start": 7746.6, "end": 7752.4400000000005, "text": " In the paper they also have something called a bottleneck layer. This is a 1x1 convolution", "tokens": [682, 264, 3035, 436, 611, 362, 746, 1219, 257, 44641, 547, 4583, 13, 639, 307, 257, 502, 87, 16, 45216], "temperature": 0.0, "avg_logprob": -0.17016796949433116, "compression_ratio": 1.6629213483146068, "no_speech_prob": 3.9669735087954905e-06}, {"id": 1155, "seek": 775244, "start": 7752.44, "end": 7766.639999999999, "text": " where I basically compress the number of filters down into the growth factor times 4. So this", "tokens": [689, 286, 1936, 14778, 264, 1230, 295, 15995, 760, 666, 264, 4599, 5952, 1413, 1017, 13, 407, 341], "temperature": 0.0, "avg_logprob": -0.16339289252437764, "compression_ratio": 1.508108108108108, "no_speech_prob": 9.080426025320776e-06}, {"id": 1156, "seek": 775244, "start": 7766.639999999999, "end": 7773.839999999999, "text": " is a way of reducing the dimensionality. You'll see here, when they use bottleneck and something", "tokens": [307, 257, 636, 295, 12245, 264, 10139, 1860, 13, 509, 603, 536, 510, 11, 562, 436, 764, 44641, 547, 293, 746], "temperature": 0.0, "avg_logprob": -0.16339289252437764, "compression_ratio": 1.508108108108108, "no_speech_prob": 9.080426025320776e-06}, {"id": 1157, "seek": 775244, "start": 7773.839999999999, "end": 7780.5199999999995, "text": " called compression, they call it DenseNetBC, you can see that that reduces the number of", "tokens": [1219, 19355, 11, 436, 818, 309, 413, 1288, 31890, 7869, 11, 291, 393, 536, 300, 300, 18081, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.16339289252437764, "compression_ratio": 1.508108108108108, "no_speech_prob": 9.080426025320776e-06}, {"id": 1158, "seek": 778052, "start": 7780.52, "end": 7787.120000000001, "text": " parameters even more and therefore makes it even more accurate. So generally speaking,", "tokens": [9834, 754, 544, 293, 4412, 1669, 309, 754, 544, 8559, 13, 407, 5101, 4124, 11], "temperature": 0.0, "avg_logprob": -0.17195569054555085, "compression_ratio": 1.369942196531792, "no_speech_prob": 9.972888619813602e-06}, {"id": 1159, "seek": 778052, "start": 7787.120000000001, "end": 7794.6, "text": " you'll probably be wanting to use bottleneck. But it's just a 1x1 conv with in this case", "tokens": [291, 603, 1391, 312, 7935, 281, 764, 44641, 547, 13, 583, 309, 311, 445, 257, 502, 87, 16, 3754, 365, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.17195569054555085, "compression_ratio": 1.369942196531792, "no_speech_prob": 9.972888619813602e-06}, {"id": 1160, "seek": 778052, "start": 7794.6, "end": 7800.88, "text": " 48 filters, so it's reducing the dimensionality through that.", "tokens": [11174, 15995, 11, 370, 309, 311, 12245, 264, 10139, 1860, 807, 300, 13], "temperature": 0.0, "avg_logprob": -0.17195569054555085, "compression_ratio": 1.369942196531792, "no_speech_prob": 9.972888619813602e-06}, {"id": 1161, "seek": 780088, "start": 7800.88, "end": 7811.64, "text": " So basically what happens is you have a number of dense blocks. Each dense block basically", "tokens": [407, 1936, 437, 2314, 307, 291, 362, 257, 1230, 295, 18011, 8474, 13, 6947, 18011, 3461, 1936], "temperature": 0.0, "avg_logprob": -0.13544878815159653, "compression_ratio": 1.6265060240963856, "no_speech_prob": 4.565949893731158e-06}, {"id": 1162, "seek": 780088, "start": 7811.64, "end": 7821.72, "text": " consists of a number of these convolutions followed by concatenation. So go through each", "tokens": [14689, 295, 257, 1230, 295, 613, 3754, 15892, 6263, 538, 1588, 7186, 399, 13, 407, 352, 807, 1184], "temperature": 0.0, "avg_logprob": -0.13544878815159653, "compression_ratio": 1.6265060240963856, "no_speech_prob": 4.565949893731158e-06}, {"id": 1163, "seek": 780088, "start": 7821.72, "end": 7828.24, "text": " layer, convolution, concatenate. And you see how I'm actually replacing x with it. So it's", "tokens": [4583, 11, 45216, 11, 1588, 7186, 473, 13, 400, 291, 536, 577, 286, 478, 767, 19139, 2031, 365, 309, 13, 407, 309, 311], "temperature": 0.0, "avg_logprob": -0.13544878815159653, "compression_ratio": 1.6265060240963856, "no_speech_prob": 4.565949893731158e-06}, {"id": 1164, "seek": 782824, "start": 7828.24, "end": 7834.4, "text": " concatenating to itself again and again, so it's getting longer and longer. And then from", "tokens": [1588, 7186, 990, 281, 2564, 797, 293, 797, 11, 370, 309, 311, 1242, 2854, 293, 2854, 13, 400, 550, 490], "temperature": 0.0, "avg_logprob": -0.1146792003086635, "compression_ratio": 1.4946236559139785, "no_speech_prob": 8.267828889074735e-06}, {"id": 1165, "seek": 782824, "start": 7834.4, "end": 7845.639999999999, "text": " time to time, I then add in a transition block which is simply a 1x1 convolution followed", "tokens": [565, 281, 565, 11, 286, 550, 909, 294, 257, 6034, 3461, 597, 307, 2935, 257, 502, 87, 16, 45216, 6263], "temperature": 0.0, "avg_logprob": -0.1146792003086635, "compression_ratio": 1.4946236559139785, "no_speech_prob": 8.267828889074735e-06}, {"id": 1166, "seek": 782824, "start": 7845.639999999999, "end": 7851.96, "text": " by a pooling layer. So just like every computer vision model we're used to, a bunch of computation", "tokens": [538, 257, 7005, 278, 4583, 13, 407, 445, 411, 633, 3820, 5201, 2316, 321, 434, 1143, 281, 11, 257, 3840, 295, 24903], "temperature": 0.0, "avg_logprob": -0.1146792003086635, "compression_ratio": 1.4946236559139785, "no_speech_prob": 8.267828889074735e-06}, {"id": 1167, "seek": 785196, "start": 7851.96, "end": 7860.2, "text": " layers and then a bunch of computation, pool, bunch of computation, pool. So this looks", "tokens": [7914, 293, 550, 257, 3840, 295, 24903, 11, 7005, 11, 3840, 295, 24903, 11, 7005, 13, 407, 341, 1542], "temperature": 0.0, "avg_logprob": -0.25149185180664063, "compression_ratio": 1.6373056994818653, "no_speech_prob": 2.6425739179103402e-06}, {"id": 1168, "seek": 785196, "start": 7860.2, "end": 7867.32, "text": " like a pretty standard kind of architecture.", "tokens": [411, 257, 1238, 3832, 733, 295, 9482, 13], "temperature": 0.0, "avg_logprob": -0.25149185180664063, "compression_ratio": 1.6373056994818653, "no_speech_prob": 2.6425739179103402e-06}, {"id": 1169, "seek": 785196, "start": 7867.32, "end": 7872.16, "text": " And then I mentioned compression. The other thing then is in each transition block, you", "tokens": [400, 550, 286, 2835, 19355, 13, 440, 661, 551, 550, 307, 294, 1184, 6034, 3461, 11, 291], "temperature": 0.0, "avg_logprob": -0.25149185180664063, "compression_ratio": 1.6373056994818653, "no_speech_prob": 2.6425739179103402e-06}, {"id": 1170, "seek": 785196, "start": 7872.16, "end": 7877.24, "text": " can optionally have this thing called compression, which normally you would set it to 0.5, that", "tokens": [393, 3614, 379, 362, 341, 551, 1219, 19355, 11, 597, 5646, 291, 576, 992, 309, 281, 1958, 13, 20, 11, 300], "temperature": 0.0, "avg_logprob": -0.25149185180664063, "compression_ratio": 1.6373056994818653, "no_speech_prob": 2.6425739179103402e-06}, {"id": 1171, "seek": 787724, "start": 7877.24, "end": 7884.5199999999995, "text": " just says the number of filters, take however many filters you currently have and multiply", "tokens": [445, 1619, 264, 1230, 295, 15995, 11, 747, 4461, 867, 15995, 291, 4362, 362, 293, 12972], "temperature": 0.0, "avg_logprob": -0.17536930865552053, "compression_ratio": 1.6157635467980296, "no_speech_prob": 1.3845899047737475e-05}, {"id": 1172, "seek": 787724, "start": 7884.5199999999995, "end": 7889.639999999999, "text": " it by 0.5. So this is basically something where every time you have a pooling layer,", "tokens": [309, 538, 1958, 13, 20, 13, 407, 341, 307, 1936, 746, 689, 633, 565, 291, 362, 257, 7005, 278, 4583, 11], "temperature": 0.0, "avg_logprob": -0.17536930865552053, "compression_ratio": 1.6157635467980296, "no_speech_prob": 1.3845899047737475e-05}, {"id": 1173, "seek": 787724, "start": 7889.639999999999, "end": 7893.84, "text": " you also decrease the number of filters. So when you have this bottleneck layer and you", "tokens": [291, 611, 11514, 264, 1230, 295, 15995, 13, 407, 562, 291, 362, 341, 44641, 547, 4583, 293, 291], "temperature": 0.0, "avg_logprob": -0.17536930865552053, "compression_ratio": 1.6157635467980296, "no_speech_prob": 1.3845899047737475e-05}, {"id": 1174, "seek": 787724, "start": 7893.84, "end": 7900.92, "text": " have this compression of 0.5, that's what DenseNet BC refers to.", "tokens": [362, 341, 19355, 295, 1958, 13, 20, 11, 300, 311, 437, 413, 1288, 31890, 14359, 14942, 281, 13], "temperature": 0.0, "avg_logprob": -0.17536930865552053, "compression_ratio": 1.6157635467980296, "no_speech_prob": 1.3845899047737475e-05}, {"id": 1175, "seek": 790092, "start": 7900.92, "end": 7910.2, "text": " Question asked. Can we do transfer learning on DenseNet? Absolutely you can. And in fact,", "tokens": [14464, 2351, 13, 1664, 321, 360, 5003, 2539, 322, 413, 1288, 31890, 30, 7021, 291, 393, 13, 400, 294, 1186, 11], "temperature": 0.0, "avg_logprob": -0.2069186167930489, "compression_ratio": 1.403225806451613, "no_speech_prob": 1.2218985830259044e-05}, {"id": 1176, "seek": 790092, "start": 7910.2, "end": 7921.08, "text": " PyTorch just came out yesterday or today and has some pre-trained DenseNet models.", "tokens": [9953, 51, 284, 339, 445, 1361, 484, 5186, 420, 965, 293, 575, 512, 659, 12, 17227, 2001, 413, 1288, 31890, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2069186167930489, "compression_ratio": 1.403225806451613, "no_speech_prob": 1.2218985830259044e-05}, {"id": 1177, "seek": 790092, "start": 7921.08, "end": 7928.92, "text": " Having said that, because the size of the activations continues to increase and increase", "tokens": [10222, 848, 300, 11, 570, 264, 2744, 295, 264, 2430, 763, 6515, 281, 3488, 293, 3488], "temperature": 0.0, "avg_logprob": -0.2069186167930489, "compression_ratio": 1.403225806451613, "no_speech_prob": 1.2218985830259044e-05}, {"id": 1178, "seek": 792892, "start": 7928.92, "end": 7937.96, "text": " and increase, we again have this problem that there isn't really a nice kind of small number", "tokens": [293, 3488, 11, 321, 797, 362, 341, 1154, 300, 456, 1943, 380, 534, 257, 1481, 733, 295, 1359, 1230], "temperature": 0.0, "avg_logprob": -0.1703914295543324, "compression_ratio": 1.436241610738255, "no_speech_prob": 1.2029539902869146e-05}, {"id": 1179, "seek": 792892, "start": 7937.96, "end": 7944.36, "text": " of activations that you could really build on top of. So I'm not sure how practical it", "tokens": [295, 2430, 763, 300, 291, 727, 534, 1322, 322, 1192, 295, 13, 407, 286, 478, 406, 988, 577, 8496, 309], "temperature": 0.0, "avg_logprob": -0.1703914295543324, "compression_ratio": 1.436241610738255, "no_speech_prob": 1.2029539902869146e-05}, {"id": 1180, "seek": 792892, "start": 7944.36, "end": 7950.92, "text": " would be, but you certainly could.", "tokens": [576, 312, 11, 457, 291, 3297, 727, 13], "temperature": 0.0, "avg_logprob": -0.1703914295543324, "compression_ratio": 1.436241610738255, "no_speech_prob": 1.2029539902869146e-05}, {"id": 1181, "seek": 795092, "start": 7950.92, "end": 7963.56, "text": " So here is the whole DenseNet model. Basically there's 4 layers which aren't part of the", "tokens": [407, 510, 307, 264, 1379, 413, 1288, 31890, 2316, 13, 8537, 456, 311, 1017, 7914, 597, 3212, 380, 644, 295, 264], "temperature": 0.0, "avg_logprob": -0.15813765987273184, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.5559653497330146e-06}, {"id": 1182, "seek": 795092, "start": 7963.56, "end": 7970.2, "text": " dense blocks, which is that there's an initial 3x3 convolution, there's a global average", "tokens": [18011, 8474, 11, 597, 307, 300, 456, 311, 364, 5883, 805, 87, 18, 45216, 11, 456, 311, 257, 4338, 4274], "temperature": 0.0, "avg_logprob": -0.15813765987273184, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.5559653497330146e-06}, {"id": 1183, "seek": 795092, "start": 7970.2, "end": 7975.08, "text": " pooling layer, and there's also a ReLU and a batch norm. So if you subtract the 4 and", "tokens": [7005, 278, 4583, 11, 293, 456, 311, 611, 257, 1300, 43, 52, 293, 257, 15245, 2026, 13, 407, 498, 291, 16390, 264, 1017, 293], "temperature": 0.0, "avg_logprob": -0.15813765987273184, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.5559653497330146e-06}, {"id": 1184, "seek": 795092, "start": 7975.08, "end": 7979.04, "text": " then divide by the number of dense blocks, that tells you how many layers you need for", "tokens": [550, 9845, 538, 264, 1230, 295, 18011, 8474, 11, 300, 5112, 291, 577, 867, 7914, 291, 643, 337], "temperature": 0.0, "avg_logprob": -0.15813765987273184, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.5559653497330146e-06}, {"id": 1185, "seek": 797904, "start": 7979.04, "end": 7988.2, "text": " every block. So we do our 3x3 conv, then we go through each of those layers, create a", "tokens": [633, 3461, 13, 407, 321, 360, 527, 805, 87, 18, 3754, 11, 550, 321, 352, 807, 1184, 295, 729, 7914, 11, 1884, 257], "temperature": 0.0, "avg_logprob": -0.17690976025307015, "compression_ratio": 1.5232558139534884, "no_speech_prob": 3.90546074413578e-06}, {"id": 1186, "seek": 797904, "start": 7988.2, "end": 7996.0, "text": " dense block, and for every one except for the last layer, we also do a transition block.", "tokens": [18011, 3461, 11, 293, 337, 633, 472, 3993, 337, 264, 1036, 4583, 11, 321, 611, 360, 257, 6034, 3461, 13], "temperature": 0.0, "avg_logprob": -0.17690976025307015, "compression_ratio": 1.5232558139534884, "no_speech_prob": 3.90546074413578e-06}, {"id": 1187, "seek": 797904, "start": 7996.0, "end": 8002.0, "text": " Finally do a batch norm, ReLU, global average pooling, and then a dense layer to create", "tokens": [6288, 360, 257, 15245, 2026, 11, 1300, 43, 52, 11, 4338, 4274, 7005, 278, 11, 293, 550, 257, 18011, 4583, 281, 1884], "temperature": 0.0, "avg_logprob": -0.17690976025307015, "compression_ratio": 1.5232558139534884, "no_speech_prob": 3.90546074413578e-06}, {"id": 1188, "seek": 800200, "start": 8002.0, "end": 8011.76, "text": " the right number of classes. So that's basically it. If you create that, compile it, and fit", "tokens": [264, 558, 1230, 295, 5359, 13, 407, 300, 311, 1936, 309, 13, 759, 291, 1884, 300, 11, 31413, 309, 11, 293, 3318], "temperature": 0.0, "avg_logprob": -0.16501930524718086, "compression_ratio": 1.3587786259541985, "no_speech_prob": 7.411241313093342e-06}, {"id": 1189, "seek": 800200, "start": 8011.76, "end": 8020.96, "text": " it. So I ran it last night, so I couldn't quite run it as long as they did, but I did", "tokens": [309, 13, 407, 286, 5872, 309, 1036, 1818, 11, 370, 286, 2809, 380, 1596, 1190, 309, 382, 938, 382, 436, 630, 11, 457, 286, 630], "temperature": 0.0, "avg_logprob": -0.16501930524718086, "compression_ratio": 1.3587786259541985, "no_speech_prob": 7.411241313093342e-06}, {"id": 1190, "seek": 802096, "start": 8020.96, "end": 8032.76, "text": " get to 93.23, which easily beats all of the state-of-the-art. That's somewhere about 6", "tokens": [483, 281, 28876, 13, 9356, 11, 597, 3612, 16447, 439, 295, 264, 1785, 12, 2670, 12, 3322, 12, 446, 13, 663, 311, 4079, 466, 1386], "temperature": 0.0, "avg_logprob": -0.1854553100390312, "compression_ratio": 1.4331550802139037, "no_speech_prob": 9.665947800385766e-06}, {"id": 1191, "seek": 802096, "start": 8032.76, "end": 8038.96, "text": " in the bit. So I didn't have time to run it for as long as they did, but I certainly replicated", "tokens": [294, 264, 857, 13, 407, 286, 994, 380, 362, 565, 281, 1190, 309, 337, 382, 938, 382, 436, 630, 11, 457, 286, 3297, 46365], "temperature": 0.0, "avg_logprob": -0.1854553100390312, "compression_ratio": 1.4331550802139037, "no_speech_prob": 9.665947800385766e-06}, {"id": 1192, "seek": 802096, "start": 8038.96, "end": 8046.8, "text": " their state-of-the-art result. As you can see, using nothing but basically two screen", "tokens": [641, 1785, 12, 2670, 12, 3322, 12, 446, 1874, 13, 1018, 291, 393, 536, 11, 1228, 1825, 457, 1936, 732, 2568], "temperature": 0.0, "avg_logprob": -0.1854553100390312, "compression_ratio": 1.4331550802139037, "no_speech_prob": 9.665947800385766e-06}, {"id": 1193, "seek": 804680, "start": 8046.8, "end": 8052.360000000001, "text": " pools of Keras. So I read through that pretty quickly, but honestly this is all stuff that", "tokens": [28688, 295, 591, 6985, 13, 407, 286, 1401, 807, 300, 1238, 2661, 11, 457, 6095, 341, 307, 439, 1507, 300], "temperature": 0.0, "avg_logprob": -0.1847991943359375, "compression_ratio": 1.8974358974358974, "no_speech_prob": 3.373656727490015e-05}, {"id": 1194, "seek": 804680, "start": 8052.360000000001, "end": 8057.12, "text": " you guys are pretty familiar with. So if you read the paper, it's a really easy paper to", "tokens": [291, 1074, 366, 1238, 4963, 365, 13, 407, 498, 291, 1401, 264, 3035, 11, 309, 311, 257, 534, 1858, 3035, 281], "temperature": 0.0, "avg_logprob": -0.1847991943359375, "compression_ratio": 1.8974358974358974, "no_speech_prob": 3.373656727490015e-05}, {"id": 1195, "seek": 804680, "start": 8057.12, "end": 8063.68, "text": " read, read the code, it's a really easy code to read. If you haven't done much implementing", "tokens": [1401, 11, 1401, 264, 3089, 11, 309, 311, 257, 534, 1858, 3089, 281, 1401, 13, 759, 291, 2378, 380, 1096, 709, 18114], "temperature": 0.0, "avg_logprob": -0.1847991943359375, "compression_ratio": 1.8974358974358974, "no_speech_prob": 3.373656727490015e-05}, {"id": 1196, "seek": 804680, "start": 8063.68, "end": 8069.04, "text": " papers with code, this is a great place to start because there's no math in the paper,", "tokens": [10577, 365, 3089, 11, 341, 307, 257, 869, 1081, 281, 722, 570, 456, 311, 572, 5221, 294, 264, 3035, 11], "temperature": 0.0, "avg_logprob": -0.1847991943359375, "compression_ratio": 1.8974358974358974, "no_speech_prob": 3.373656727490015e-05}, {"id": 1197, "seek": 804680, "start": 8069.04, "end": 8074.28, "text": " it's pretty clear, the Keras code is really easy to read, there's no new concepts, so", "tokens": [309, 311, 1238, 1850, 11, 264, 591, 6985, 3089, 307, 534, 1858, 281, 1401, 11, 456, 311, 572, 777, 10392, 11, 370], "temperature": 0.0, "avg_logprob": -0.1847991943359375, "compression_ratio": 1.8974358974358974, "no_speech_prob": 3.373656727490015e-05}, {"id": 1198, "seek": 807428, "start": 8074.28, "end": 8079.48, "text": " this would be a great way to get started. And as I said, some of the students and I", "tokens": [341, 576, 312, 257, 869, 636, 281, 483, 1409, 13, 400, 382, 286, 848, 11, 512, 295, 264, 1731, 293, 286], "temperature": 0.0, "avg_logprob": -0.2559241099529956, "compression_ratio": 1.460093896713615, "no_speech_prob": 2.977110489155166e-05}, {"id": 1199, "seek": 807428, "start": 8079.48, "end": 8086.12, "text": " basically started on this on Friday and got it knocked out. So I think this is pretty", "tokens": [1936, 1409, 322, 341, 322, 6984, 293, 658, 309, 16914, 484, 13, 407, 286, 519, 341, 307, 1238], "temperature": 0.0, "avg_logprob": -0.2559241099529956, "compression_ratio": 1.460093896713615, "no_speech_prob": 2.977110489155166e-05}, {"id": 1200, "seek": 807428, "start": 8086.12, "end": 8087.12, "text": " exciting.", "tokens": [4670, 13], "temperature": 0.0, "avg_logprob": -0.2559241099529956, "compression_ratio": 1.460093896713615, "no_speech_prob": 2.977110489155166e-05}, {"id": 1201, "seek": 807428, "start": 8087.12, "end": 8091.88, "text": " It's 9 o'clock. Thanks everybody. Looking forward to Chatting Tuesday during the week.", "tokens": [467, 311, 1722, 277, 6, 9023, 13, 2561, 2201, 13, 11053, 2128, 281, 27503, 783, 10017, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.2559241099529956, "compression_ratio": 1.460093896713615, "no_speech_prob": 2.977110489155166e-05}, {"id": 1202, "seek": 809188, "start": 8091.88, "end": 8108.24, "text": " I'll see you next Monday for our last class.", "tokens": [50364, 286, 603, 536, 291, 958, 8138, 337, 527, 1036, 1508, 13, 51182], "temperature": 0.0, "avg_logprob": -0.4029902390071324, "compression_ratio": 0.88, "no_speech_prob": 8.457552030449733e-05}], "language": "en"}