{"text": " Okay, hi everybody welcome back good to see you all here It's been another busy week of deep learning Lots of cool things going on and like last week I wanted to highlight a few really interesting articles that some of some of you folks have written The tally wrote One of the best articles I've seen for a while. I think actually talking about differential learning rates and stochastic gradient descent with restarts Be sure to check it out if you can because what he's done. I feel like he's done a great job of Kind of positioning in a place that you can get a lot out of it You know regardless of your background, but for those who want to go further He's also got links to like the academic papers that came from and kind of graphs of showing examples of all of all the things He's talking about and I think it's a it's a particularly Nicely done article so a good kind of role model for technical communication One of the things I've liked about you know seeing people post these Post these articles during the week is the discussion on the forums have also been like really great. There's been a lot of a lot of people helping out like Explaining things you know which you know maybe there's parts of the post bit where people have said actually that's not quite how it works And people have learned new things that way people have come up with new ideas as a result as well These discussions of stochastic gradient descent with restarts and cyclical learning rates has been a few of them actually Anand Saha has written another great post Talking about a similar Similar topic and why it works so well and again lots of great pictures and references to Papers and most importantly perhaps code showing how it actually works Mark Hoffman Covered the same topic at kind of a nice introductory level. I think really really kind of clear intuition many canta Talk specifically about differential learning rates and why it's interesting and again providing some nice context people not familiar with Transfer learning you know going back saying like well. What is transfer learning? Why is that interesting and Given that why could differential learning rates be helpful? and then One thing I particularly liked about Arjun's Article was that he talked not just about the technology that we're looking at but also talked about some of the implications Particularly from a commercial point of view so thinking about like based on some of the things we've learned about so far What are some of the implications that that has you know in real life? And lots of background lots of pictures And then discussing some of the yeah some of the implications So there's been lots of great stuff online and thanks to everybody for all the great work that you've been doing As we talked about last week if you're Kind of vaguely wondering about writing something, but you're feeling a bit intimidated about it because you've never really written a technical post before Just jump in you know it's it's it's it's a really Welcoming and encouraging group I think to to work with So we're going to have a kind of an interesting lesson today, which is we're going to cover a Whole lot of different applications, so we've spent quite a lot of time on computer vision and today We're going to try if we can to get through three totally different areas Structured learning so looking at kind of how you look at So we're going to start out looking at structured learning or structured data learning by which I mean Building models on top of things that look more like database tables So kind of columns of different types of data. They might be financial or geographical or whatever We're going to look at using deep learning for language natural language processing And we're going to look at using deep learning for recommendation systems, and so we're going to cover these At a very high level and the focus will be on Here is how to use the software to do it More than here is what's going on behind the scenes and then the next three lessons Will be digging into the details of what's been going on behind the scenes and also coming back to Looking at a lot of the details of computer vision that we kind of skipped over so far So the focus today is really on like how do you actually do these applications? And we'll kind of talk briefly about some of the concepts involved Before we do I did want to talk about one key New concept Which is dropout and you might have seen dropout mentioned a bunch of times already and got the got the impression that this is something Important and indeed it is So to look at dropout. I'm going to look at the the dog breeds Current cable competition that's going on and what I've done is I've got a head and I've created a pre-trained network as per usual And I've passed in pre compute equals true and so that's going to Pre compute the activations that come out of the last convolutional layer remember an activation is just a number It's a number just to remind you an activation like here is one activation, it's a number and specifically the activations are calculated based on some weights Also called parameters that make up Kernels or filters and they get applied to the previous layers activations which could well be the inputs Or they could themselves be the results of other calculations. Okay, so when we say activation keep remembering We're talking about a number that's been calculated So we pre compute some activations and then what we do is we put on top of that a bunch of additional initially randomly generated Fully connected layers, so we're just going to do some matrix modifications on top of these just like in our Excel worksheet at the very end We had this matrix that we just did a matrix multiplication, but So what you can actually do is if you just type The name of your learner object you can actually see what's in it. You can see the layers in it So when I was previously been skipping over a little bit about are we add a few layers to the end? These are actually the layers that we add We're going to do batch norm in the last lesson. So don't worry about that for now a linear layer simply means a matrix multiply okay, so this is a matrix which has a thousand and twenty four rows and 512 columns and so in other words, it's going to take in a thousand and twenty four activations and spit out 512 activations And then we have a relu remember is just replace the negatives with zero We'll skip over the batch norm. We'll come back to drop out and then we have a second linear layer that takes those 512 activations from the previous linear layer and puts them through a new matrix multiply 512 by 120 spits out a new 120 activations and then finally put that through Softmax and for those of you that don't remember softmax. We looked at that last year last week it's this idea that we basically just Take the creep the activation. Let's say for dog Go either the power of that and then divide that into the sum of either the pair of all the activations So that was the thing that adds up to one all of them add up to one and each one individually is between zero and one Okay, so That's that's what we added on top and that's the thing when we have pre compute equals true That's the thing we train so I wanted to talk about what this dropout is and what this P is because it's a really important thing that we get to choose so a dropout layer with P equals zero point five Literally does this we go over to our spreadsheet and let's pick any layer with some activations and let's say okay I'm going to apply dropout where the P of zero point five to cons two what that means is I go through and with a 50% chance I Pick a cell I pick an activation so I picked like half of them randomly and I delete them Okay That's that's what dropout is right so it's so the P equals point five means what's the probability of? deleting that cell right so when I delete those cells If you have a look like look at the output It doesn't actually change by very much at all just a little bit particularly because remember it's going through a max pooling layer Right so it's only going to change it at all if it was actually the maximum in that group of four And furthermore, it's just one piece of you know if it's going into a convolution rather than into a max pool It's just one piece of that that filter so interestingly The idea of like randomly throwing away half of the activations in a layer Has a really interesting result and one important thing to mention is each mini-batch We throw away a different random half of activations in that layer and so what it means is It forces it to not overfit right in other words if there's some particular activation That's really learnt just that exact That exact dog or that exact cat right then when that gets dropped out The whole thing now isn't going to work as well. It's not going to recognize that image right so it has to in order for this To work it has to try and find a representation that That actually continues to work even as random half of the activations get thrown away every time All right, so it's it's it's I guess about four years old now three or four years old and it's been Absolutely critical in Making modern deep learning work and the reason why is it really just about solved? The problem of generalization for us before dropout came along if you try to train a model with lots of parameters and You were overfitting And you already tried all the data augmentation you could and you already had as much data as you could You there were some other things you could try but to a large degree you were kind of stuck right and so then Jeffrey Hinton and his colleagues came up with this this dropout idea that was loosely inspired by the way the brain works And also loosely inspired by Jeffrey Hinton's experience in bank telecuse apparently and Yeah, somehow they came up with this amazing idea of like hey, let's let's try throwing things away at random and so as you could imagine if your P was like point oh one then you're throwing away 1% of your activations for that layer at random It's not going to randomly change things up very much at all So it's not really going to protect you from Overfitting much at all on the other hand if your P was point nine nine Then that would be like going through the whole thing and throwing away nearly everything right and That would be very hard for it to overfit, so that would be great for generalization, but it's also going to kill your accuracy so this is kind of Playoff between high P values generalized well, but will decrease your training accuracy and low P values Will generalize less well, but will give you a less good training accuracy So for those of you that have been wondering why is it that particularly early in training are my validation? Losses better than my training losses right which seems otherwise really surprising hopefully some of you have been wondering why that is because On a data set that it never gets to see you wouldn't expect the losses to ever be That's better and the reason why is because when we look at the validation set we turn off dropout Right so in other words when you're doing inference when you're trying to say is this a cat or is this a dog? We certainly don't want to be including Random dropout there right we want to be using the best model we can Okay, so that's why early in training in particular We actually see that our validation Accuracy and loss tends to be better If we're using dropout, okay, so yes They have to do anything to accommodate for the fact that you are throwing away some That's a great question so We don't but pie torch does so pie torch behind the scenes does two things if you say P equals point five It throws away half of the activations but it also Doubles all the activations that are already there so on average the kind of the average activation doesn't change Which is pretty pretty neat trick? So yeah, you don't have to worry about it basically it's done for you So if we say so you can pass in peas This is the this is the p value for all of the added layers to say With fast AI what dropout do you want on each of the layers in these these added layers? It won't change the dropout in the pre-trained network like the hope is that that's already been Pre-trained with some appropriate level of dropout We don't change it but on these layers that we add you can say how much and so you can see here I said peas equals point five so my first dropout has point five my second dropout has point five All right, you remember coming to the input of this Was the output of the last convolutional layer of pre-trained network and we go away and we actually throw away half of that Before you can start go through our linear layer Throw away the negatives Throw away half of the result of that go through another linear layer and then pass that to our softmax for minor numerical precision Reasons it turns out to be better to take the log of the softmax than the softmax directly And that's why you'll have noticed that when you actually get predictions out of our models you always have to go NP dot next of the predictions Again the details as to why aren't important so if we want to Try removing dropout we could go peas equals zero Right and you'll see where else before we started with the point seven six accuracy in the first epoch now We've got a point eight accuracy in the first epoch All right, so by not doing dropout our first epoch worked better not surprisingly because we're not throwing anything away but by the third epoch here we had 84.8 and Here we have 84.1. So it started out better and ended up worse So even after three epochs you can already see we're massively overfitting right we've got point three loss on the train And point five loss on the validation And so if you look now you can see in the resulting model. There's no dropout at all So if the P is zero we don't even add it to the model Another thing to mention is you might have noticed that what we've been doing is we've been adding two Linear layers In our additional layers you don't have to do that by the way There's actually a parameter called extra fully connected layers that you can basically pass a list But how long do you want or how big do you want each of the additional fully connected layers to be and so by default? Well you need to have at least one right because you need something that takes the output of the convolutional layer which in this case is a size thousand twenty four and turns it into the number of Classes you have perhaps versus dogs would be two dog breeds would be a hundred and twenty Planet satellite seventeen whatever that's you always need one linear layer at least and you can't pick how big that is that's defined by your problem But you can choose what the other size is or if it happens at all So if we were to pass in an empty list Then now we're saying don't add any additional linear layers just the one that we have to have Right so here if you've got peas equals zero extra fully connected layers is empty. This is like the minimum possible Kind of top model we can put on top and again like if we do that You Can see above we actually end up with in this case a Reasonably good result because we're not training it for very long and this particular pre-trained network It's very well suited to this particular problem. Yes, you know So Jeremy what kind of P should we we using? By default so the one that's there by default for the first layer Is point two five and for the second layer is zero point five That seems to work pretty well For most things right so like it's it's it you you don't necessarily need to change it at all Basically if you find it's overfitting Just start bumping it up so try first of all setting it to zero point five that'll set them both to zero point five If it's still overfitting a lot try zero point seven like you can you can narrow down And like there's not that many numbers Change right and if you're under fitting Then you can try making it lower It's unlikely you would need to make it much lower because like even in these dogs versus cat situations You know we don't seem to have to make it lower, so it's more likely you'd be increasing it to like point six or point seven But you can fiddle around I find these the ones that are there by default seem to work pretty well most of the time So one place I I actually did increase this Was in the dog breeds one I did set them both to point five when I used a Bigger model so like resnet 34 has less parameters, so it doesn't overfit as much Then when I started bumping pumping it up through like a resnet 50 which has a lot more parameters I noticed it started overfitting so then I also increased my dropout so as you use like Bigger models you'll often need to add more dropout. Can you pass that over there, please? If we set B to point five roughly what percentage is it 50% 50% Was there a few positive back Thanks, is there a particular way in which you can determine if data is being overfitted Yeah You can see that the like here you can see that the training error is a loss is much lower than the validation loss You can't tell if it's like too overfitted like Zero overfitting is not generally optimal like the only way to find that out is Remember the only thing you're trying to do is to get this number low right the validation loss number low So in the end you kind of have to play around with a few different things and see which thing ends up getting the validation Loss low, but you're kind of going to feel over time for your particular problem What does overfitting what does too much overfitting look like? Great so So that's dropout, and we're going to be using that a lot and remember. It's there by default service here another question So I have two questions one is So when it says the dropout rate is point five Is does it like you know a delete each cell with a probability of? 5.5. All right does it just pick 50% randomly. I mean I know both effectively It's the former yep, okay, okay second question is why why does the average activation matter? well it matters because the remember if you look at the Excel spreadsheet that the result of this cell for example is equal to These nine Multiplied by each of these nine right and add it up So if we deleted half of these then that would also cause this number to half which would cause like Everything else after that to change and so if you change What it means you know like you then you're changing something that used to say like oh Fluffy ears are fluffy if this is greater than point six now It's only fluffy if it's greater than point three like you're changing the meaning of everything so you the goal here is to delete things without changing Where are you using a linear activation for one of the earlier activations? Why are we using linear? Yeah? Why that particular activation? Because that's what this set of layers is so we've we've the the pre-trained network is or is the convolutional network and That's pre computed so we don't see it so what that spits out is a vector so the only choice We have is to use linear layers at this point Okay Can we have different level of dropout by layer? Yes? Absolutely how to do that great so So you can absolutely have different dropout by layer, and that's why this is actually called peas So you can pass in an array here, so if I went zero comma 0.2 for example and then extra fully connect to it I might add 512 right then that's going to be zero dropout before the first of them and 0.2 dropout before the second of them And I must admit I don't have a great intuition even after doing this for a few years for like When should earlier or later layers have different amounts of dropout? It's still something I kind of play with and I can't quite find rules of thumb So if some of you come up with some good rules of thumb, I'd love to hear about them. I think if in doubt You can use the same dropout in every fully connected layer The other thing you can try is often people only put dropout on the very last Linear layer, so that'd be the two things to try So Jeremy, why do you monitor the log loss the loss instead of the accuracy going up? Well because the loss is the only thing that we can see For both the validation set and the training set so it's nice to be able to compare them Also as we'll learn about Later the loss is the thing that we're actually optimizing so It's it's kind of a little more. It's a little easier to monitor that and understand what that means Can you pass it over there? So with dropout we are kind of adding some random noise every iteration right so So that means that we don't do as much learning right? So that's right, so we have to play around with the learning rate and it doesn't seem to impact the learning rate Enough that I've ever noticed it. I Would say you're probably right in theory it might but not enough that it's ever affected me Okay, so let's talk about this So let's talk about this structured data problem and so to remind you we were looking at Kaggle's Rossman competition, which is a German Chain of supermarkets, I believe And you can find this in less than three Rossman and The main data set is the one where we were looking to say at a particular store How much did they sell? Okay, and there's a few big key pieces of information one is what was the date another was were they open? Did they have a promotion on? Was it a holiday in that state and was it a holiday? For school a state holiday there or was it a school holiday there? And then we had some more information about stores like what for this store What kind of stuff did they tend to sell what kind of store are they how far away the competition and so forth so? With a data set like this. There's really two main kinds of column. There's columns that we think of as Categorical they have a number of levels. So the assortment Column is categorical and it has levels such as a B and C Where else something like competition distance we would call continuous It has a number attached to work where differences or ratios even of that number have some kind of meaning and so we need to Deal with these two things quite differently. Okay, so anybody who's done any Machine learning of any kind will be familiar with using continuous columns If you've done any linear regression, for example, you can just like multiply them by parameters for instance Categorical columns we're going to have to think about a little bit more We're not going to go through the data cleaning we're going to assume that that's and feature engineering we're going to assume all that's been done And so basically at the end of that we have a list of columns and the in this case I didn't do any of the thinking around the Feature engineering or data cleaning myself. This is all directly from the third place winners of this competition And so they came up with all of these different Columns that they found useful and so You'll notice the list here is a list of the things that we're going to treat as categorical variables Numbers like year a month and day although We could treat them as continuous like they the diff, you know differences between 2000 and 2003 is meaningful We don't have to right and you'll see shortly how how categorical Variables are treated but basically if we decide to make something a categorical variable What we're telling our neural net down the track is that for every different level of say year, you know 2000 2001 2002 You can treat it totally differently Whereas if we say it's continuous, it's going to have to come up with some kind of like function some kind of smooth ish Function right and so often even for things like year that actually are continuous But they don't actually have many distinct levels. It often works better to treat it as categorical So another good example day of week, right? So like day of week between 0 and 6 It's a number and it means something like the difference between 3 and 5 is 2 days and has meaning but if you think about like how would Sales in a store vary by day of week It could well be that like, you know Saturdays and Sundays are over here and Fridays are over here and Wednesdays over here Like each day is going to behave Kind of qualitatively differently, right? So by saying this is the categorical variable as you'll see we're going to let the neural net do that Right. So this thing where we get where we say Which are continuous and which are categorical to some extent? This is a modeling decision you get to make now if something is Coded in your data is like a B and C or you know, Jeremy and your net or whatever You actually you're going to have to call that categorical, right? There's no way to treat that directly as a continuous variable on the other hand if it starts out as a continuous variable like age or day of week You get to decide Whether you want to treat it as continuous or categorical Categorical okay, so summarize if it's categorical in the data It's going to have to be categorical in the model if it's continuous in the data You get to pick whether to make it continuous or categorical in the model So in this case again, what I just did whatever the third place winners of this competition did These are the ones that they decided to use as categorical These were the ones they decided to use as continuous and you can see that basically The continuous ones are all of the ones which are actual floating point numbers like competition Distance actually has a decimal place to it right and temperature actually has a decimal place to it So these would be very hard to make Categorical because they have many many levels, right? Like if it's like five digits of floating point then potentially there will be as many levels as there are as there are rows and And by the way the word we use to say how many levels are in a category we use the word cardinality Right so if you hear me say cardinality for example the cardinality of the day of week Variable is seven because there are seven different days of the week Do you have a heuristic for one to bin continuous variables or do you ever been variables I don't ever been continuous variables So yeah So one thing we could do with like max temperature is group it into not 10 10 to 20 20 to 30 and then call that categorical interestingly a paper just came out last week in which a group of researchers found that sometimes bidding can be helpful but Literally came out in the last week and until that time I haven't seen anything in deep learning saying that so I haven't I haven't Looked at it myself until this week. I would have said it's a bad idea now. I have to think differently I guess maybe it is sometimes So if you're using Year as a category what happens when you run the model on a year? It's never seen so you trained it in Well, we'll get there. Yeah, the short answer is it'll be treated as an unknown category and so pandas Which is the underlying data frame thing? We're using with categories as a special category called unknown and if it sees a category it hasn't seen before It gets treated as unknown So for our deep learning model unknown would just be another category If our data set training the data set doesn't have a category and The test has unknown how will it be? It'll just be part of this unknown category Well, it's still predict it'll predict something right like it'll just have the value Zero behind the scenes and if there's been any unknowns of any kind in the training set then it'll learn a way to predict unknown if it hasn't it's going to have some random vector and so that's a Interesting detail around training that we probably won't talk about in this part of the course But we can certainly talk about on the forum Okay, so we've got our categorical and continuous variable lists defined in this case there was a 800,000 rows So 800,000 dates basically by stores And so you can now take all of these columns Loop through each one and Replace it in the data frame where the version where you say take it and change its type to category Okay, and so that just that's just a pandas thing. So I'm not going to teach you pandas There's plenty of books particularly Wes McKinney's books book on Python for data analysis is great But hopefully it's intuitive as to what's going on even if you haven't seen the specific syntax before so we're going to turn that Column into a categorical column And then for the continuous variables, we're going to make them all 32 bit floating point and for the reason for that is that PI torch Expects everything to be 32 bit floating point. Okay, so like some of these include like 1 0 things like I Can't see them straight away, but anyway some of them yeah Like was there a promo was was a holiday and so that'll become the floating point values 1 and 0 instance. Okay, so I try to do as much of my work as possible on small data sets for when I'm working with images that generally means resizing the images to like 64 by 64 or 128 by 128 We can't do that with structured data. So instead I tend to take a sample. So I randomly pick a few rows So I start running with a sample and I can use exactly the same Thing that we've seen before for getting a validation set we can use the same way to get some random Random row numbers to use in a random sample. Okay, so this is just a bunch of random numbers And Then okay, so that's going to be a size 150,000 rather than 840,000 And so my data before I go any further it basically looks like this you can see I've got some booleans here. I've got some Integers here of various different scales as my year 2014 And I've got some letters here. So even though I said Please call that a pandas category Pandas still displays that in the notebook as strings, right? It's just stored in internally differently so then the fast AI library has a special little function called process data frame and Process data frame takes a data frame and you tell it what's my dependent variable All right, and it does a few different things The first thing is it's pulls out that dependent variable and puts it into a separate variable Okay, and deletes it from the original data frame So DF now does not have the sales column in where else why just contains a sales column Something else that it does is it does scaling so neural nets Really like to have the input data to all be somewhere around zero with a standard deviation it's somewhere around one right so we can always take our data and Subtract the mean and divide by the standard deviation to make that happen So that's what do scale equals true does and it actually returns a special object Which keeps track of what mean and standard deviation did it use for that normalizing? So you can then do the same thing to the test set later? It also handles missing values so missing values Categorical variables just become the idea zero and then all the other categories become one two three four five for that categorical variable for continuous variables it replaces the Missing value with the median and creates a new column That's a Boolean and just says is this missing or not and I'm going to skip over this pretty quickly because we talk about This in detail in the machine learning course. Okay, so if you've got any questions about this part That would be a good place to go. It's nothing deep learning specific there So you can see afterwards year 2014 For example has become year two okay because these categorical variables have all been replaced with With contiguous integers starting at zero Right and the reason for that is later on we're going to be putting them into a matrix Right and so we wouldn't want the matrix to be 2014 rows long when it could just be two rows long okay, so that's the basic idea there and you'll see that the a C for example has been replaced in the same way with one of them three Okay, so we now have a data frame which does not contain the dependent variable and where everything is a number Okay And so that's that's where we need to get to to do deep learning and all of the stage above that as I said we Talk about in detail in the machine learning course nothing deep learning specific about any of it This is exactly what we throw into our random forests as well. So Another thing we talk about a lot in the machine learning course of course is validation sets In this case we need to predict the next two weeks of sales Right. It's not like pick a random set of sales, but we have to pick the next two weeks of sales That was what the Kaggle competition folks told us to do And therefore I'm going to create a validation set which is the last two weeks of of my training set to try and make it as similar to the test set as possible and We just posted actually Rachel wrote this thing last week about Creating validation sets. So if you go to fast today, I you can check that out We'll put that in the lesson wiki as well But it's basically a summary of a recent machine learning lesson that we did The videos are available for that as well. And this is kind of a written a written summary of it Okay So yeah, so Rachel and I spent a lot of time thinking about kind of you know How do you need to think about validation sets and training sets and test sets and so forth and that's all there But again, nothing deep learning specific. So let's get straight to the deep learning action. Okay so in this particular competition as always with any competition or any kind of Machine learning project you really need to make sure you have a strong understanding of your metric How are you going to be judged here? And in this case, you know capital makes it easy They tell us how we're going to be judged and so we're going to be judged on the roots mean squared Percentage error, right? So we're going to say like oh you predicted three It was actually three point three So you were ten percent out and then we're going to average all those percents, right? And remember I warned you that You are going to need to make sure you know logarithms really well, right? And so in this case from you know, we're basically being saying your prediction divided by the actual the mean of that right is the thing that we care about and and so we don't have a Metric in pytorch called root mean squared percent error We could actually easily create it by the way If you look at the source code, you'll see like it's you know a line of code but easier still would be to realize that That if you have That right then you could replace a with like log of a dash and B with like log of B dash and then you can replace that whole thing with a subtraction That's just the rule of logs, right? And so if you don't know that rule and you know, make sure you go look it up because it's super helpful but it means in this case all we need to do is to Take the log of our data Which I actually did earlier in this Notebook and when you take the log of the data getting the root means great error Will actually get you the root means great percent error for free Okay, but then when we want to like print out our root means great percent error We actually have to go either the power of it Again, right and then we can actually return the percent difference. So that's all that's going on here It's again not really deep learning specific at all So Here we finally get to the deep learning Alright, so as per usual like you'll see everything we look at today looks exactly the same as everything We've looked at so far, which is first we create a model data object Something that has a validation set Training set an optional test set built into it from that. We will get a learner We will then Optionally called learner dot LR find will then called learner dot fit It'll be all the same parameters and everything that you've seen many times before Okay, so the difference though is obviously we're not going to go Image classifier data dot from CSV or dot from paths We need to get some different kind of model data. And so for stuff that is in rows and columns We use columnar model data Okay, but this will return an object with basically the same API That you're familiar with and rather than from paths or from CSV. This is from data frame Okay, so this gets passed a few things The path here is just used for it to know where should it store? Like model files or stuff like that, right? This is just basically saying where do you want to store anything that you save later? This is the list of the indexes of the rows that we want to put in the validation set we created earlier Here's our data frame Okay and then Let's have a look here's this is where we did the log, right? So I took the the Y that came out of prop DF that dependent variable. I logged it and I call that YL Right, so we tell it When we create our model data, we need to tell it that's our dependent variable Okay, so so far we've got list of the stuff to go in the validation set, which is what's our independent variables? What's our dependent variables and then we have to tell it which things do we want treated as categorical? Right because remember by this time Everything's a number Right, so it could do the whole thing as if it's continuous. It would just be totally meaningless, right? So we need to tell it which things do we want to treat as categories? And so here we just pass in That list of names that we used before Okay, and then a bunch of the parameters are the same as the ones you're used to for example You can set the batch size here. So after we do that, we've got a You know standard Model data object. There's a train train DL Attribute there's a val DL attribute a train DS attribute of our DS attribute. It's got a length It's got all the stuff exactly like it did in all of our image based data objects Okay. So now we need to create the the model or create the learner and so to skip ahead a little bit We're basically going to pass in something that looks pretty familiar We're going to be passing saying from our model from our model data Create a learner that is suitable for it And we'll basically be passing in a few other bits of information which will include How much dropout to use at the very start? How many how many activations to have in each layer how much dropout to use at the at the later layers? But then there's a couple of extra things that we need to learn about and specifically it's this thing called embeddings So this is really the key new concept we have to learn about all right, so All we're doing basically is we're going to take our Let's forget about categorical variables for a moment and just think about the continuous variables All right for our continuous variables all we're going to do Is we're going to grab them all? Okay, so for our continuous variables, we're basically going to say like okay, here's a Big list of all of our continuous variables like the minimum temperature and the maximum temperature and the distance to the nearest competitor And so forth right and so here's just a bunch of floating point numbers and so basically what the neuron that's going to do is it's going to take that that 1d array or or vector or to be very DL like rank one tensor All means the same thing okay, so we're going to take our rank one tensor and let's put it through a matrix multiplication So let's say this has got like I don't know 20 continuous variables, and then we can put it through a matrix which Must have 20 rows. That's how matrix multiplication works, and then we can decide how many columns we want right? So maybe we decided a hundred right and so that matrix multiplication is going to spit out a new length 100 rank one tensor Okay, that's that's what that's what a linear. That's what a matrix product does and that's the definition of a linear layer in deep learning Okay, and so then the next thing we do is we can put that through a relu right which means we throw away the negatives Okay, and now we can put that through another matrix product. Okay, so this is going to have to have a hundred rows by definition And we can have as many columns as we like and so let's say maybe this was The last layer so the next thing we're trying to do is to predict sales So there's just one Value we're trying to predict the sales so we could put it through a Metrics product that just had one column and that's going to spit out a single number Right so that's like That's kind of like a one layer Neural net if you like now in practice you know we wouldn't make it one layer, so we would actually have like You know maybe we'd have 50 here and so then that gives us a 50 long vector and then Maybe we then put that into our final 50 by one And that spits out a single number and one reason I wanted to change that there was to point out. You know rally you You would never put value in the last layer. I could never want to throw away the negatives because that the softmax The softmax Needs negatives in it because it's the negatives that are the things that allow it to create low probabilities That's minor detail, but it's useful to remember okay, so basically So Basically a simple view of a Fully connected Neural net is something that takes in as an input a rank one tensor it's bits it through a linear layer an Activation layer another linear layer a softmax and That's the output Okay And so we could obviously decide to add more Linear layers we could decide maybe to add dropout Right so these are some of the decisions that we we get to make right, but we there's not that much we can do right? There's not much really crazy architecture stuff to do so when we come back to image Models later in the course we're going to learn about all the weird things that go on and like res nets and inception networks And blah blah blah, but in these fully connected networks. They're really pretty simple. They're just interspersed linear layers that is matrix products and Activation functions like value and a softmax at the end And if it's not classification which actually ours is not classification in this case. We're trying to predict sales There isn't even a softmax Right we don't want it to be between 0 and 1 Okay, so we can just throw away the last activation altogether If we have time we can talk about a slight trick we can do there, but for now we can think of it that way So that was all assuming that everything was continuous right, but what about categorical right so we've got like a day of week right and We're going to treat it as categorical right so it's like Saturday Sunday Monday Six Friday Okay, how do we feed that in because I want to find a way of getting that in so that we still end up with a rank one tensor of floats and so the trick is this we create a new little matrix of With seven rows and As many columns as we choose right so let's pick four right so here's our Seven rows and four columns All right, and basically what we do is let's add our categorical variables to the end So let's say the first row was Sunday Right then what we do is we do a lookup into this matrix and we say oh here's Sunday We do a lookup into here, and we grab This row and so this matrix we basically fill with floating point numbers, so we're going to end up grabbing a little Subset of four floating point numbers. It's Sundays particular for floating point numbers And so that way we convert Sunday into a rank one tensor of four floating point numbers and initially those four numbers are Random right and in fact this whole thing we initially start out random okay But then we're going to put that through our Neural net right so we basically then take those four numbers, and we throw we remove Sunday instead we add Our four numbers on here right so we've turned our categorical thing into a floating point vector Right and so now we can just put that through our neural net Just like before and at the very end we find out the loss and then we can figure out which direction is down and Do gradient descent in that direction and eventually that will find its way back To this little list of four numbers, and it'll say okay those random numbers weren't very good This one needs to go up a bit that one needs to go up a bit that one needs to go down a bit that one Needs to go up a bit and so we'll actually update our original those four numbers in that matrix and We'll do this again and again and again And so this this matrix will stop looking random and it will start looking more and more like like The exact four numbers that happen to work best for Sunday the exact four numbers that happen to work best for Friday and so forth And so in other words this matrix is just another bunch of weights in our neural net right and so matrices of this type are called embedding matrices So an embedding matrix is something where we start out with an integer Between zero and the maximum number of levels of that category We literally index into a matrix to find a particular row so if it was the level was one we take the first row We grab that row and we append it to all of our continuous variables and So we now have a new Vector of continuous variables and then we can do the same thing so let's say zip code Right so we could like have an embedding matrix. Let's say there are 5,000 zip codes It would be 5,000 rows long as wide as we decide maybe it's 50 wide and so we'd say okay. Here's 9 4 0 0 3 That zip code is index number 4 in our matrix so go down and we find the fourth row We'll grab those 50 numbers and append those Onto our big vector and then everything after that is just the same we just put it through our linear layer value linear layer What are those four numbers Represent that's a great question and we'll learn more about that without when we look at collaborative filtering For now they represent no more or no less than any other parameter in our neural net you know they're just They're just parameters that we're learning that happen to end up giving us a good loss We will discover later that these particular parameters often however are human interpretable and quite can be quite interesting But that's a side effect of them. It's not Fundamental they're just four random numbers for now that we're that we're learning or sets of four random numbers To have a good heuristic for the dimensionality of the embedding matrix, so why four here I sure do So What I first of all did was I made a little list of every categorical variable and its cardinality Okay, so there they all are so there's a hundred and that's a thousand plus different stores Apparently in Rossman's Network There are eight days of the week That's because there are seven days of the week plus one leftover for unknown Even if there were no missing values in the original data I always still set aside one just in case there's a missing or an unknown or something different in the test set Again four years, but there's actually three plus room for an unknown and so forth right so what I do My rule of thumb is this Take the cardinality of the variable Divide it by two But don't make it bigger than 50 Okay, so These are my embedding matrices so my store matrix so they've that has to have a thousand one hundred and sixteen rows Because I need to look up right to find his store number three, and then it's going to return back a Rank one tensor of length 50 Day of week, it's going to look up into which one of the eight and return the thing of length four So what you typically build an embedding matrix for each categorical feature yes Yeah, so that's what I've done here, so I've said For C in categorical variables See how many categories there are and then for each of those things create one of these and Then this is called embedding sizes, and then you may have noticed that that's actually the first thing that we pass To get learner and so that tells it for every categorical variable. That's the embedding matrix to use for that variable Just behind you doesn't yes So besides our random initialization are there other ways actually initialize embedding Yes And no there's two ways one is random the other is pre trained and We'll probably talk about pre trained more later in the course, but the basic idea though is if somebody else at Rossman Who'd already trained a neural net? Just like you you would use a pre trained net from image net to look at pictures of cats and dogs If somebody else has pre trained a network to predict cheese sales in Rossman You may as well start with their embedding matrix of stores to predict liquor sales in Rossman And this is what happens for example at At Pinterest and instacart they both use this technique instacart uses it for routing their shoppers Pinterest uses it for deciding what to display on a web page when you go there, and they have embedding matrices of products in instacart's case of stores That gets shared in the organization so people don't have to train new ones So for the embedding size Why wouldn't you just use like the one-hot scheme and just What is the advantage of doing this as opposed to just doing a question so so we could easily as you point out have Instead of passing in these four numbers we could instead of passed in seven numbers all zeros, but one of them is a one and that also is a list of floats and That would totally work And that's how Generally speaking categorical variables have been used in statistics for many years. It's called dummy variable coding The problem is that in that case? the concept of Sunday Could only ever be associated with a single floating point number Right and so it basically gets this kind of linear behavior. It says like Sunday is more or less of a single thing Yeah, what's not just interactions is saying like now Sunday is a concept in four-dimensional space, right? And so what we tend to find happen is that these? embedding vectors Tend to get these kind of rich semantic concepts so for example if it turns out that weekends Kind of have a different behavior you'll tend to see that Saturday and Sunday will have like some particular number higher or more likely it turns out that certain days of the week are associated with higher sales of Certain kinds of goods that you kind of can't go without I don't know like gas or milk say Where else there might be other products like? like wine for example Like wine that tend to be associated with like the days before weekends or holidays right, so there might be kind of a column which is like To what extent is this day of the week kind of associated with people going out? You know so basically yeah by by having this higher dimensionality vector rather than just a single number It gives the deep learning Network a chance to learn these rich Representations and so this idea of an embedding is actually what's called a distributed representation It's kind of the fun most fundamental concept of neural networks It's this idea that a concept in a neural network has a kind of a high dimensional representation and often it can be hard to interpret because the idea is like each of these Numbers in this vector doesn't even have to have just one meaning you know it could mean one thing if this is low and that One's high and something else if that one's high and that one's low because it's going through this kind of rich nonlinear Function right and so it's this It's this rich representation that allows it to learn such such such interesting relationships Kind of tough oh another question sure All right, I'll speak louder so are there Is an embedding so I get the fundamental of the like the word vector word to vec vector algebra You can run on this but are the embedding suited suitable for certain types of variables like Or are these only suitable for? I mean are there different categories that that the embeddings are suitable for and embedding is suitable for any categorical variable Okay, so so the only thing it can't really work well at all for would be something that is too high Cardinality so like in other words we had likes whatever it was 600,000 rows if you had a variable with 600,000 levels That's just not a useful Categorical variable you could bucketize it I guess But yeah in general like you can see here that the third place getters in this competition and Really decided that everything that was not too high cardinality They put them all as categorical variables, and I think that's a good rule of thumb You know if you can make it a categorical variable you may as well Because that way it can learn this rich distributed representation where else if you leave it as continuous You know the most it can do is to kind of try and find a Little a single functional form that fits it well I have a question so You were saying that you are kind of increasing the dimension But actually in in most cases we will use a one-hot encoding which has even a bigger dimension That so so in a way you are also reducing, but in the most rich. I think that's that's that's fair again it yeah it like Yes, you know you can think of it as one-hot encoding which actually is high dimensional, but it's not Meaningfully high dimensional because everything except one is zero I'm saying that also because even this will reduce the amount of memory and things like this that you have to write The terms is better. You're absolutely right We're absolutely right and and so we may as well go ahead and actually describe like what's going on with the matrix algebra behind The scene see this if this doesn't quite make sense you can kind of skip over it But for some people I know this really helps If we started out with something saying this is Sunday right We could represent this as a one-hot encoded vector right and so Sunday you know maybe was position here, so that would be a one and then the rest of zeros Okay, and then we've got our Embedding matrix right with eight rows and in this case four columns One way to think of this actually is a matrix product, right? So I said you could think of this as like looking up the number one You know and finding like its index in the array But if you think about it, that's actually Identical to doing a matrix product between a one-hot encoded vector and the embedding matrix Like you're going to go zero times this row one times this row zero times this row And so it's like a one-hot embedding matrix product is identical to doing a lookup and so Some people in the bad old days actually implemented embedding matrices by doing a one-hot encoding and then a matrix product and in fact a lot of like machine learning Methods still kind of do that But as you know it was kind of alluding to it's that's terribly inefficient So all of the modern libraries implement this as taken take an integer and do a lookup into an array But the nice thing about realizing that it's actually a matrix product mathematically is it makes it more obvious How the gradients are going to flow so when we do stochastic gradient descent, it's we can think of it as just another Linear layer. Okay doesn't say that's like a somewhat minor detail, but hopefully for some of you it helps Could you touch on using dates and times as categoricals and how that affects seasonality? Yeah, absolutely. That's a great question Did I cover dates at all last week? I remember No, okay So I covered dates in a lot of detail in the machine learning course, but it's worth briefly mentioning here There's a fast AI function called add date part Which takes a data frame and a column name That column name needs to be a date It removes unless you've got drop equals false it optionally removes the column from the data frame and replaces it with lots of columns representing all of the useful information about that date like Day of week day of month month of year year is at the start of a quarter is at the end of a quarter basically everything that pandas Gives us And so that way we end up when we look at our List of features where you can see them here, right? Yeah, month week day day of week, etc. So these all get created for us by add date part so we end up with you know this Eight long embedding matrix so I guess eight rows by four column embedding matrix for day of week and And conceptually that allows us or allows our model to create some pretty interesting time series models right like it can if there's something that has a seven day period cycle That kind of goes up on Mondays and down on Wednesdays, but only for dairy and only in Berlin It can totally do that, but it has all the information it needs to do that So this turns out to be a really fantastic way to deal with time series So I'm really glad you asked the question you just need to make sure that That the the cycle indicator in your time series exists as a column So if you didn't have a column there called day of week it would be very very difficult for the neural network to somehow learn to do like a Divide mod 7 and then somehow look that up in an embedding matrix I get not impossible, but really hard would use lots of computation wouldn't do it very well so an example of the kind of thing that you need to think about might be Holidays for example, you know or if you were doing something in you know of sales of beverages in San Francisco You probably want a list of like when when are that when is the ballgame on at AT&T Park? All right, because that's going to impact how many people that are drinking beer in so much All right, so you need to make sure that the kind of the basic indicators or Or periodicities or whatever are there in your data and as long as they are the neuron it's going to learn to use them So I'm kind of trying to skip over some of the non deep learning parts All right, so The key thing here is that we've got our model data that came from the data frame We tell it how big to make the embedding matrices We also have to tell it of the columns in that data frame How many of those? Categorical variables or how many of them are continuous variables? So the actual parameter is number of continuous variables So you can hear you can see we just pass in how many columns are there minus how many categorical variables are there? so then that way the The neural net knows how to create something that puts the continuous variables over here and the categorical variables over there The embedding matrix has its own dropout All right, so this is the dropout applied to the embedding matrix This is the number of activations in the first linear layer the number of activations in the second linear layer The dropout in the first linear layer the dropout for the second linear layer This bit we won't worry about for now. And then finally is how many outputs do we want to create? Okay, so this is the output of the last linear layer and obviously it's one because we want to predict a single number Which is sales? Okay So after that we now have a learner where we can call LR find and we get the standard looking shape and we can say what? amount do we want to use and we can then go ahead and Start training using exactly the same API we've seen before So this is all identical You can pass in. I'm not sure if you've seen this before Custom metrics what this does is it just says please print out a number at the end of every epoch by calling this function and this is a function we defined a little bit earlier, which was the root means grad percentage error first of all going either the power of our Sales because our sales were originally logged So this doesn't trade change the training at all. It just it's just something to print out So we train that for a while and You know we've got some benefits that the original people that built this don't have specifically we've got things like Cyclical not cyclical learning rate stochastic gradient descent with restarts, and so it's actually interesting to have a look and compare Although our validation set isn't identical to the test set it's very similar It's a two-week period that is at the end of the training data, so our numbers should be similar and if we look at what we get point oh nine seven and compare that to the Leaderboard public leaderboard You can see we're kind of Let's have a look in the top actually that's interesting There's a big difference between the public and private leaderboard it would have it's would have been right at the top of the private leaderboard But only in the top 30 or 40 on the public leaderboard, so not quite sure, but you can see like we're certainly in the top end of this competition I I actually tried running the third place getters code and Their final result was over point one so I actually think that we're should be comparing the private leaderboard So anyway, so you can see there basically there's a technique for dealing with time series and structured data and you know interestingly The group that that used this technique they actually wrote a paper about it. That's linked in this notebook When you compare it to the folks that won this competition and came second They did the other folks did way more feature engineering like the winners of this competition were actually subject matter experts in logistics sales forecasting and so they had their own like code to create lots and lots of features and And talking to the folks at Pinterest who built their very similar model for recommendations for Pinterest They said the same thing which is that when they switched from gradient boosting machines to deep learning they did like way way way less Feature engineering it was a much much simpler model and requires much less maintenance And so this is like one of the big benefits of using this approach to deep learning you can get state-of-the-art results But with a lot less work Yes Are you using any I'm series in any of these fits indirectly Absolutely using what we just saw we have a day of week month of year all that stuff columns And most of them are being treated as categories. So we're building a distributed representation of January We're building a distributed representation of Sunday. We're building a distributed representation of Christmas So we're not using any Classic time series techniques all we're doing is True fully connected layers in a neuron Matrix that's what Exactly exactly. Yes, so the embedding matrix is able to deal with this stuff like day of week periodicity and so forth in a way richer way than any Standard time series technique I've ever come across one last question The matrix in the earlier models when we did the CNN we did not pass it during the fit We passed it when the data was When we got the data So we're not passing anything to fit just the learning rate and the number of cycles In this case we're passing in metrics because we want to print out some extra stuff There is a difference in that we're calling data dot get learner so with The imaging approach We just go learner dot trained and pass it the data but In for these kinds of models in fact for a lot of the models the model that we build Depends on the data in this case. We actually need to know like what embedding matrices do we have? And stuff like that so in this case it's actually the data object that creates the learner So yeah, it is it is a bit upside down to what we've seen before Yeah, so just to summarize or maybe I'm confused So in this case what we are doing is like we have some kind of a structured data We did feature engineering We got some column in our database or something similar Apparent is data frame. Yeah, yeah data frame and then we are mapping it to deep learning by using this in Embedding matrix for the categorical variables so the continuous we just put them straight in So so all I need to do is like if I have if I have already have a feature engineering model Yeah, then to map it to deep learning. I just have to figure out which one I can move into categorical And then yeah, it learn by itself. Yeah, great question So yes exactly if you want to use this on your own database So yes exactly if you want to use this on your own data set Step one is list the categorical variable names list the continuous variable names Put it in a data frame the pandas data frame Step two is to Create a list of which row indexes do you want new validation set? step three Is to call this line of code? Using this exact like these exact you can just copy and paste it step four is to create your list of how big you want each embedding matrix to be and Then step five is to call get learner You can use these exact parameters to start with And if it overfits or underfits you can fit it with them and then the final step is to call Fit so yeah, almost all of this code will be nearly identical Have a couple of questions one is How is data augmentation can be used in this case and the second one is? What what are dropouts doing in here okay, so data augmentation. I have no idea. I mean that's a really interesting question I think it's got to be domain specific I've never seen any paper or anybody in industry doing data augmentation with structured data and deep learning So I don't I think it can be done. I just haven't seen it done What is dropout doing? Exactly the same as before so at each point we have The output of each of these linear layers is just a Rank one tensor and so dropout is going to go ahead and say let's throw away half of the activations and the very first dropout embedding dropout literally goes through the embedding matrix and says Let's throw away half the activations That's it okay, let's take a break and let's come back at five past eight Okay, thanks everybody So now We're going to move into something Equally exciting actually before I do I just mentioned that I had a good question during the break which was What's the downside like? Like look almost no one's using this Why not And and basically I think the answer is like as we discussed before No one in academia almost is working on this because it's not something that people really publish on And as a result there haven't been really great examples where people could look at and say oh here's a technique that works well So let's have our company implement it but perhaps equally importantly Until now with this fast AI library there hasn't been any Way to to do it conveniently if you wanted to implement one of these models You had to write all the custom code Yourself where else now as we discussed it's you know six It's basically a six-step process you know involving about you know not much more than six lines of code So the reason I mentioned this is to say like I think there are a lot of big commercial and scientific Opportunities to use this to solve problems that previously haven't been solved very well before So like I'll be really interested to hear if some of you try This out you know maybe on like Old Kaggle competitions you might find like oh, I would have won this if I'd use this technique That would be interesting or if you've got some data set you work with at work You know some kind of different model that you've been doing with a GBM or a random forest does this help You know the thing I I'm still somewhat new to this I've been doing this for Basically since the start of the year was when I started working on these structured deep learning models So I haven't had enough opportunity to know Where might it fail it's worked for nearly everything I've tried it with so far but yeah, I think this class is the first time that There's going to be like more than half a dozen people in the world who actually are working on this so I think you know as A group we're going to hopefully learn a lot and build some interesting things And this would be a great thing if you're thinking of writing a post about something or here's an area that There's a couple of that. There's a post from Instacart about what they did Pinterest has a Riley AI video about what they did that's about it, and there's two academic papers Both about Kaggle competition victories one from a Yoshi Yoshi Benjo and his group They won a taxi destination forecasting competition and then also the one linked for this Rossman competition, so Yeah, there's some background on that all right so language natural language processing Is the area which is? Kind of like the most up-and-coming area of deep learning. It's kind of like two or three years behind computer vision in deep learning it was kind of like the the second area that deep learning started getting really popular in and You know computer vision Got to the point where it was like the clear state-of-the-art For most computer vision things maybe in like 2014 You know and some things in like 2012 and in NLP. We're still at the point where For a lot of things deep learning is now the state of the art, but not quite everything but as you'll see the state of kind of The software and some of the concepts is much less mature than it is for computer vision So in general none of the stuff we talked about after computer vision is going to be as like Settled as the computer vision and stuff was so NLP One of the interesting things is in the last few months Some of the good ideas from computer vision have started to spread into NLP for the first time and we've seen some really big Advances so a lot of the stuff you'll see in NLP is is pretty new So I'm going to start with a particular Kind of NLP problem and one of the things you'll find in NLP is like there are particular problems You can solve and they have particular names and so there's a particular kind of problem in NLP called language modeling and Language modeling has a very specific definition. It means build a model where given a few words of a sentence Can you predict what the next word is going to be? So if you're using your mobile phone and you're typing away and you press space and then it says like this is what the next Word might be like Swift key does this like really well and Swift key actually uses deep learning for this That's that's a language model. Okay, so it has a very specific meaning when we say language modeling We mean a model that can predict the next word of a sentence So let me give you an example. I downloaded about 18 months worth of papers from archive so for those of you that don't know it archive is The most popular preprint server in this community and various others and has you know lots of academic papers and so I grabbed the Abstracts and the topics for each and so here's an example so the category of this particular paper was computer CSNI is computer science and networking and Then the summary so let the abstract of the paper Was saying the exploitation of mm wave bands is one of the key enabler for 5g mobile blah blah blah Okay, so here's like an example a piece of text from my language model So I trained a language model on this archive data set that I downloaded and then I built a simple little test which basically You would pass it some like priming text So you say like oh imagine you started reading a document that said category is computer science Networking and the summary is algorithms that and then I said, please write An archive abstract so it said that if it's a networking algorithms that Use the same network as a single node are not able to achieve the same performance as a traditional network based routing algorithms in this paper We propose a novel routing scheme, but okay So it it's learned by reading archive papers that somebody who was saying algorithms that Where the word cat CSNI came before it is going to talk like this and remember it started out Not knowing English at all, right? It actually started out with an embedding matrix for every word in English that was random Okay, and by reading lots of archive papers it weren't what kind of words followed others So then I tried what if we said cat computer science computer vision summary algorithms that Use the same data to perform image classification are increasingly being used to improve the performance of image classification algorithms And this paper we propose a novel method for image classification using a deep convolutional neural network parentheses CNN So you can see like it's kind of like almost the same sentence as back here But things are just changed into this world of computer vision rather than networking So I tried something else which is like, okay category computer vision and I created the world's shortest ever abstract algorithms and then I said title on And the title of this is going to be on the performance of deep learning for image classification Eos is end of string. So that's like end of title What if it was networking summary algorithms title on the performance of wireless networks as opposed to? Towards computer vision towards a new approach to image classification networking Towards a new approach to the analysis of wireless networks So like I find this mind-blowing right I started out with some random matrices Which are like literally no No pre-trained anything. I fed it 18 months worth of archive articles and it learned not only how to write English pretty well, but also after you say something's a convolutional neural network you should then use parentheses to say what it's called and Furthermore that the kinds of things people talk to say create algorithms for in computer vision are performing image classification and in networking are Achieving the same performance as traditional network-based routing algorithms. So like a language model is Can be like incredibly deep and subtle Right and so we're going to try and build that But actually not because we care about this at all We're going to build it because we're going to try and create a pre-trained model what we're actually going to try and do is take IMDB movie reviews and Figure out whether they're positive or negative So if you think about it, this is a lot like cats versus dogs That's a classification algorithm, but rather than an image we're going to have the text of a review So I'd really like to use a pre-trained network Like I would at least like a net to start with a network that knows how to read English right and so My view was like okay to know how to read English means you should be able to like predict the next word of a sentence so what if we pre-train a language model and Then use that pre-trained language model and then just like in computer vision Stick some new layers on the end and ask it instead of to predicting the next word in the sentence instead predict Whether something is positive or negative So when I started working on this this was actually a new idea Unfortunately in the last couple of months I've been doing it You know a few people have actually a couple people have started publishing this and so this has moved from being a totally new idea to being a you know somewhat new idea so so this idea of creating a language model Making that the pre-trained model for a classification model is what we're going to learn to do now And so the idea is we're really kind of trying to leverage exactly what we learned in our computer vision work Which is how do we do fine-tuning to create powerful classification models? Yes, you know So why don't you think that doing just directly what you want to do? Doesn't work better Well a because it doesn't just turns out it doesn't empirically And the reason it doesn't is a number of things first of all as we know Fine-tuning a pre-trained network is really powerful Right so if we can get it to learn some related tasks first then we can use all that information To try and help it on the second pass The other reason is IMDB movie reviews You know up to a thousand words long They're pretty big and so after reading a thousand words knowing nothing about How English is structured or even what the concept of a word is? Or punctuation or whatever at the end of this thousand? Integers, you know they end up being integers all you get is a one or a zero Positive or negative and so trying to like learn the entire structure of English and then how it expresses positive and negative sentiments from a single number Is just too much to expect So by building a language model first we can try to build a neural network that kind of understands The English of movie reviews, and then we hope that some of the things that's learned about Are going to be useful in deciding whether something's a positive or a negative movie review. That's a great question Thanks is this similar to the car RNN by our pathy Yeah, this is somewhat similar to car RNN by car pathy so the famous car as in chr RNN Try to predict the next letter given a number of previous letters Language models generally work at a word level they don't have to And doing things at a word level turns out to be Can be quite a bit more powerful and we're going to focus on word level modeling in this course And then to what extent are these generated words? Actual copies of what it found in the in the training data set or are these completely Random things that it actually learned and how do we know how to distinguish between those two? Yeah, I mean these are all good questions up the words are definitely words We've seen before the work because it's not at a character level so it can only give us the word it's seen before the sentences there's A number of kind of rigorous ways of doing it But I think the easiest is to get a sense of like well here are two like different categories Where it's kind of created very similar concepts, but mixing them up in just the right way like it would be very hard To to do what we've seen here just by like spitting back things that seen before But you could of course actually go back and check you know have you seen that sentence before? Or like a string distance have you seen a similar sentence before in this case? Oh and of course another way to do it is the length most importantly when we train the language model as we'll see We'll have a validation set and so we're trying to predict the next word Of something that's never seen before and so if it's good at doing that it should be good at generating text In this case the purpose The purpose is not to generate text that was just a fun example And so I'm not really going to study that too much But you know you during the week totally can like you can totally build Your you know great American novel generator or whatever there are actually some tricks to To using language models to generate text that I'm not using here. They're pretty simple We can talk about them on the forum if you like, but my focus is actually on classification So I think that's the thing which is a incredibly powerful like text classification I Don't know you're a hedge fund you want to like read every article as soon as it comes out through Reuters or Twitter or whatever and Immediately identify things which in the past have caused you know massive market drops That's the classification model or you want to? recognize all of the customer service queries which tend to be associated with people who Who leave your you know who? Cancel their contracts in the next month. That's a classification problem, so like it's a really powerful kind of thing for data journalism Activision that activism law commerce so forth right like I'm trying to class documents into whether they're part of legal discovery or not part of legal discovery Okay, so you get the idea so In terms of stuff we're importing we're importing a few new things here one of this bunch of things we're importing is Torch text torch text is pi torches like NLP Library and so fast AI is designed to work hand-in-hand with porch text as you'll see and then there's a few Text specific sub bits of faster fast AI that we'll be using So we're going to be working with the IMDB large movie review data set. It's very very well studied in academia you know lots and lots of people over the years have studied this data set 50,000 reviews highly polarized reviews either positive or negative each one has been Classified by sentiment okay, so we're going to try first of all however to create a language model So we're going to ignore the sentiment entirely all right So just like the dogs and cats pre train the model to do one thing and then fine-tune it to do something else Because this kind of idea in NLP is is so so so new There's basically no models you can download for this so we're going to have to create error right, so Having downloaded the data you can use the link here. We do the usual stuff saying the path to it training and validation path And as you can see it looks pretty pretty traditional compared to vision. There's a directory of training. There's a directory of test We don't actually have separate test and validation in this case and Just like in in vision the training directory has a bunch of files in it In this case not representing images, but representing movie reviews So we could cat one of those files and here we learn about the classic Zombie Gettin movie I have to say with a name like zombie Gettin and An atom bomb on the front cover I was expecting a flat-out chop-socky fun-coup Rent it if you Want to get stoned on a Friday night and laugh with your buddies don't rent it if you're an uptight weenie or want a zombie Movie with lots of flesh eating. I think I'm going to enjoy zombie getting so all right, so we've learned something today All right, so we can just use standard unique stuff to see like how many words are in the data set so the training set We've got 17 and a half million words Test set we've got five point six million words So here's These are this is IMDB so IMDB is yeah random people this is not a New York Times listed review as far as I know Okay, so Before we can do anything with text we have to turn it into a list of tokens A token is basically like a word right so we're going to try and turn this eventually into a list of numbers So the first step is to turn it into a list of words That's called tokenization and NLP NLP has a huge lot of jargon that will will learn over time One thing that's a bit tricky though when we're doing tokenization is here I've tokenized that review and then joined it back up with spaces and you'll see here that wasn't Has become two tokens which makes perfect sense right was and is two things right? Dot dot dot has become one token Right where else lots of exclamation marks has become lots of tokens so like a good tokenizer will do a good job of recognizing like pieces of an English sentence each separate piece of punctuation will be separated and Each part of a multi-part word will be separated as appropriate so Spacey is I think it's an Australian developed piece of software actually that does lots of NLP stuff It's got the best tokenizer. I know and so Fast AI is designed to work well with the spacey tokenizer as its torch text so here's an example of tokenization Alright, so what we do with torch text is we basically have to start out by creating something called a field and A field is a definition of how to pre-process some text and so here's an example of the definition of a field It says I want to lowercase the text and I want to tokenize it with the function called spacey tokenize Okay, so it hasn't done anything yet. We're just telling it when we do do something. This is what to do and so that we're going to store that Description of what to do in a thing called capital text And so this is this is none of this is but this is not fast AI specific at all This is part of torch text you can go to the torch text website read the docs. There's not lots of docs yet This is all very very new so Probably the best information you'll find about it is in this lesson, but there's some more information on this site All right, so what we can now do is go ahead and create the usual Fast AI model data object, okay And so to create the model data object we have to provide a few bits of information We have to say what's the training set so the path to the text files the validation set and the test set In this case just to keep things simple. I don't have a separate validation and test set So I'm going to pass in the validation set for both of those two things, right? So now we can create our model data object as per usual the first thing we give it as the path The second thing we give it is the torch text field definition of how to pre-process that text The third thing we give it is the dictionary or the list of all of the files we have train validation test As per usual we can pass in a batch size and then we've got a special special couple of extra things here One is a very commonly used in NLP minimum frequency what this says is In a moment, we're going to be replacing every one of these words with an integer Which basically will be a unique index for every word and this basically says if there are any words that occur less than ten times Just call it unknown But don't think of it as a word, but we'll see that in more detail in a moment And then we're going to see this in more detail as well BPTT stands for back prop through time And this is where we define how long a sentence Where we? Stick on the GPU at once so we're going to break them up and in this case. We're going to break them up into sentences of 70 tokens or less on the whole so we're going to see all this in a moment Right so after building our model data object right what it actually does is it's going to fill this text field With an additional attribute called vocab, and this is a really important NLP concept I'm sorry there's so many NLP concepts which throw at you kind of quickly, but we'll see them a few times right Vocab is the vocabulary and the vocabulary in NLP has a very specific meaning it is What is the list of unique words that appear in this text? So every one of them is going to get a unique index, so let's take a look right here is text dot vocab dot I2s this stands for this is all torched text not fast AI text dot vocab dot int to string Maps the integer zero to unknown the integer one the padding integer to the Comma dot and a of two and so forth right so this is the first 12 elements of the array Of the vocab from the IMDB movie review, and it's been sorted by frequency Except for the first two special ones so for example. We can then go backwards s to I string to int Here is the it's in position zero one two so string to it the is two So the vocab lets us take a word and map it to an integer or take an integer and map it to a word Right and so that means that we can then take the first 12 tokens for example of our text and turn them into 12 ints so for example here is of the you can see seven two and Here you can see seven two right so we're going to be working in this Form did you have a question? Yeah, could you pass that back there? Is it a common to any stemming or limitizing? Not really no generally tokenization is is what we want like with a language model We you know to keep it as general as possible we want to know what's coming next and so like whether it's Future tense or past tense or plural or singular like we don't really know which things are going to be interesting in which aren't so It seems that it's generally best to kind of leave it alone as much as possible be the short answer You know having said that as I say this is all pretty new So if there are some particular areas that some researcher maybe has already discovered that some other kinds of pre-processing a helpful you Know I wouldn't be surprised not to know about it So when you're dealing with um you don't natural language is in context important context is very important So if you're breaking if you're using us the specie tokenizer literally just looking at individual words no no we're not looking at words This is this look. This is I just don't get some of the big premises of this like they're in order Yeah, so just because we replaced I With the number 12 these are still in that order yeah There is a different way of dealing with natural language called a bag of words and bag of words you do throw away The order in the context and in the machine learning course will be learning about working with bag of words representations But my belief is that they are No longer useful or in the verge of becoming no longer useful We're starting to learn how to use deep learning to use context properly now But it's kind of for the first time it's really like only in the last few months Right so I mentioned that we've got two numbers batch size and BPTT back prop through time so this is kind of subtle so So we've got some big long piece of text Okay, so we've got some big long piece of text you know here's our sentence. It's a bunch of words right and Actually what happens in a language model is even though we have lots of movie reviews They actually all get concatenated Together into one big block of text right so it's basically predicts the next word In this huge long thing which is all of the IMDB movie reviews concatenate together, so this thing is you know? What do we say it was like tens of millions of words long and so what we do is We split it up into batches First right so these like are our spits into batches right and so if we said we want a batch size of 64 we actually break the whatever was 60 million words into the 64 sections right and then we take each one of the 64 sections and We move it Like underneath the previous one I Didn't do a great job of that Right move it underneath So we end up with a matrix Which is 64 Actually I think we've moved them across wise so it's actually I think just transpose it we end up with a matrix It's like 64 columns Wide and the length let's say the original was 64 million right then the length is like 10 million long Right so each of these represents 164th of our entire IMDB review set and so that's our starting point so then what we do is We then grab a little chunk of this at a time and those chunk lengths are approximately equal to BP TT which I think we had equal to 70 so we basically grab a little 70 long Section and that's the first thing we chuck into our GPU. That's a batch Right so a batch is always of length of width 64 or batch size and each bit is a sequence of length up to 70 So let me show you All right, so here if I go take my train data loader I don't have you folks have tried playing with this yet But you can take any data loader wrap it with it I to turn it into an iterator and then call next on it to grab a batch of Data just as if you were a neural net you get exactly what the neural net gets and you can see here we get back a 75 by 64 Tensor right so it's 64 wide right and I said it's approximately 70 high and but not exactly And that's actually kind of interesting a really neat trick that torch text does is they randomly change The back prop through time number every time so each epoch. It's getting slightly different bits of text This is kind of like in computer vision. We randomly shuffle the images We can't randomly shuffle the words right because we need to be in the right order So instead we randomly move their breakpoints a little bit okay, so this is the equivalent so in other words this This here is of length 75 right there's a there's an ellipsis in the middle And that represents the first 75 words of the first review right where else this 75 here represents the first 75 words of this of the second of the 64 segments That's it have to go in like 10 million words to find that one right and so here's the first 75 words of the last of those 64 segments okay, and so then what we have down here is The next Sequence right so 51 there's 51 615 there's 615 25 there's 25 right and in this case It actually is of the same size It's also 75 plus 64 but for minor technical reasons being flattened out Into a single vector that basically it's exactly the same as this matrix, but it's just moved down by one because we're trying to predict the next word right so that all happens for us right if we ask for and this is the fast AI now if you ask for a language model data object then it's going to create these batches of batch size width by BP TT height Bits of our language corpus along with the same thing Shuffled along by one word right and so we're always going to try and predict the next word So why don't you instead of just arbitrarily choosing 64 Why don't you choose like like 64 is a large number maybe like do it by Sentences and make it a large number and then pad it with zero or something If you you know so that you actually have a one full sentence per line Basically wouldn't that make more sense not really because remember we're using columns, right? So each of our columns is of length about 10 million, right? So although it's true that those columns aren't always exactly finishing on a full stop. They're so damn long We don't care Is there like 10 million long? Right and we're trying to also each each line contains multiple sentences column contains multiple Senses yeah, it's it's of length about 10 million and it contains many many many many many sentences Because remember the first thing we did was take the whole thing and split it into 64 groups Okay, great So, um, I found this up, you know pertaining to this question this thing about like What's in this language model matrix a little mind-bending for quite a while So don't worry if it takes a while and you have to ask a thousand questions on the forum. That's fine right, but Go back and listen to what I just said in this lecture again go back to that bit where I showed you splitting it up to 64 and moving them around and try it with some sentences in Excel or Something and see if you can do a better job of explaining it than I did Because this is like how torch text works And then what fast AI adds on is this idea of like kind of how to build a language model out of it a language model out of it, although actually a lot of that's stolen from torch text as well like this sometimes where torch text starts And fast AI ends is or vice versa is a little subtle They really work closely together Okay so Now that we have a model data object That can feed us Batches we can go ahead and create a model right and so in this case We're going to create an embedding matrix and our vocab We can see how big our vocab was Let's have a look back here so we can see here in the model data object there are four thousand six hundred and two Kind of pieces that we're going to go through that's basically equal to the number of The total length of everything divided by batch size times BPTT and this one I wanted to show you NT I've got the definition up here number of unique tokens NT is the number of tokens That's the size of our vocab. So we've got three 34,945 unique words And notice the unique words that had to appear at least ten times Okay, because otherwise they've been replaced withunk The length of the data set is one Because as far as the language model is concerned there's only one Thing which is the whole corpus right and then that thing has Here it is twenty point six million words, you know, right so those 4445 things are used to create an embedding matrix of Number of rows is equal to 34 9 4 5 Right and so the first one represents unc the second one represents pad The third one was dot the fourth one was comma this one I'm just guessing with that and so forth right and so each one of these gets an embedding vector So this is literally identical to what we did Before the break, right? This is a categorical variable It's just a very high cardinality categorical variable and furthermore. It's the only variable Right. This is pretty standard in NLP. You have a variable which is a word, right? We have a single categorical variable single column basically, and it's it's a 34,000 945 cardinality categorical variable and so we're going to create an embedding matrix for it So m size is the size of the embedding vector 200 Okay, so that's going to be length 200 a lot bigger than our previous embedding vectors Not surprising because a word has a lot more nuance to it than the concept of Sunday right or Rossman's Berlin store or whatever right? So it's generally an embedding size for a word will be somewhere between about 50 and about 600 Okay, so I've kind of gone somewhere in the middle We then have to say as per usual how many activations do you want in your layers? So we're going to use 500 and then how many layers do you want in your neural net? We're going to use three Okay This is a minor technical detail it turns out that We're going to learn later about the atom optimizer That basically the defaults where it don't work very well with these kinds of models So we just have to change some of these you know, basically any time you're doing NLP. You should probably include this line It because it works pretty well So having done that we can now again take our model data object and grab a model out of it and We can pass in a few different things What optimization function do we want how big an embedding do we want how many hidden? activate how many activations number of hidden how many layers and How much dropout of many different kinds? So this language model we're going to use is a very recent development called AWD LSTM by Stephen Meridy who's an LP researcher based in San Francisco and his main contribution really was to show like How to put dropout all over the place in in these NLP models So we're not going to worry now We'll do this in the last lecture is worrying about like what all that like what is the architecture and what are all these dropouts? For now just know it's the same as per usual if you try to build an NLP model and your under fitting Then decrease all of these dropouts if you're over fitting then increase all of these dropouts in roughly this ratio Okay, that's that's my rule of thumb and it again. This is such a recent paper Nobody else is working on this model anyway, so there's not a lot of guidance, but I found this these ratios work Well, that's what Stephen's been using as well There's another kind of way we can avoid over fitting that we'll talk about in the last class again for now this one actually works Totally reliably so all of your NLP models probably want this particular line of code And then this one we're going to talk about at the end last lecture as well you can always include this basically what it says is When you do When you look at your gradients and you multiply them by the learning rate and you decide how much to update your weights by This says clip them like literally like Like don't let them be more than 0.3 and this is quite a cool little trick right because like if your learning rates pretty high and And you kind of don't want to get in that situation We talked about where you're kind of got this kind of thing where you go You know rather than little step little step little step instead you go like oh too big oh too big right with Gradient clipping it kind of goes this far and it's like oh my goodness. I'm going too far. I'll stop And that's basically what gradient looking does so Anyway, so these are a bunch of parameters the details don't matter too much right now. You can just deal these And then we can go ahead and call fit With exactly the same parameters as usual So Jeremy, um, they're all these older work embedding things like like Word to vague and globe. So I have two questions about that one is How are those different from these and the second question? Why don't you initialize them with one of those? Yeah, so So basically that's a great question. So basically People have pre trained These embedding matrices before to do various other tasks. They're not whole pre trained models They're just a pre trained embedding matrix and you can download them and as you know It says they have names like word to vec and love and they're literally just a matrix There's no reason we couldn't download them really it's just like kind of I Found that Building a whole pre trained model in this way Didn't seem to benefit much if at all from using pre trained word vectors where else using a whole pre trained language model Made it much bigger difference. So like you remember what a big those of you who saw word to vec It made a big splash when it came out I'm finding this technique of pre trained language models seems much more powerful basically But I think we can combine both to make them a little better still And what is the model that you have used like how can I know the architecture of the model? So we'll be learning about the model architecture in the last lesson for now, it's a recurrent neural network Using some LSTM long short-term memory Okay so So we had lots of details that we're skipping over But you know you can do all this without any of those details we go ahead and fit the model I found that this language model took quite a while to fit so I kind of like ran it for a while Noticed it was still under fitting safe where it was up to Ran it a bit more with longer cycle length saved it again. It still Was kind of under fitting You know run it again and kind of finally got to the point where it's like kind of honestly I kind of ran out of patience So I just like saved it at that point And I did the same kind of test that we looked at before so I was like Oh, it wasn't quite what I was expecting, but I realized anyway the best and then I was like, okay Let's see how that goes the best performance was one of the movie was a little bit Oh, I say, okay. It looks like the language models working pretty well So I've pre trained the language model And so now I want to use it Fine tune it to do classification Sentiment classification now, obviously if I'm going to use a pre-trained model, I need to use exactly the same vocab But the word the still needs to map to the number two so that I can look up the vector for that Right, so that's why I first of all load back up My my field object the thing with the vocab in right now in this case if I run it straight afterwards This is unnecessary. It's already in memory But this means I can come back to this later right in a new session basically I can then go ahead and say okay. I've never got one more field right in addition to my Field which represents the reviews. I've also got a field which represents the label Okay And the details are too important here Now this time I need to not treat the whole thing as one big Piece of text but every review is separate because each one has a different sentiment attached to it And it so happens that torch text already has a data set that does that for IMDB. So I just used IMDB built into torch text So basically once we've done all that we end up with something where we can like grab for a particular example We can grab its label positive and Here's some of the text. This is another great Tom Berendon movie blah blah blah blah. All right, so This is all not nothing faster. I specific here. We'll come back to it in the last lecture But torch text docs can help understand what's going on. All you need to know is that Once you've used this special talks torch text thing called splits to grab a splits object You can pass it straight into Fast AI text data from splits and that basically converts a torch text Object into a fast AI object we can train on so as soon as you've done that you can just go ahead and say Get model right and that gets us our learner And then we can load into it the pre-trained model the language model right, and so we can now take that pre-trained language model and And use the stuff that we're kind of familiar with right so we can Make sure that you know all it's at the last layer is frozen train it a bit Unfreeze it train it a bit and the nice thing is once you've got a pre-trained Language model it actually trains super fast. You can see here. It's like a couple of minutes Epoch and it only took me to get my is my best one here already took me like 10 epochs So it's like 20 minutes to train this bit. It's really fast And I ended up with 94 point five percent So how good is ninety four point five percent well it so happens that? Actually one of Steve and Merida's colleagues James Bradbury recently created a paper Looking at the state of like where they tried to create a new state-of-the-art for a bunch of NLP things and one of the things I looked at was IMDB and they actually have here a list of the current world's best for IMDB and even with stuff that is highly specialized for sentiment analysis the best anybody had previously come up with is 94.1 so in other words this technique Getting 94.5. It's literally Better than anybody has created in the world before as far as we know for as far as James Bradbury knows so So when I say like there are big opportunities to use this I mean like This is a technique that nobody else currently has access to which you know you could like it. You know whatever IBM has in Watson or whatever any big company has you know that they're advertising Unless they have some secret source that they're not publishing which they don't right because people get you know if they have a better thing They publish it Then you now have access to a better text classification method than has ever existed before so I really hope that you know you can try This out and see how you go There may be some things that works really well on and others that it doesn't work as well on I don't know I think this kind of sweet spot here that we had about 25,000 You know short to medium-sized documents if you don't have at least that much text it may be hard to train a different language model But having said that there's a lot more we could do here right and we won't be able to do it in part one of this Course we're doing part two but for example we could start like training language models that look at like You know lots and lots of medical journals, and then we could like make a downloadable medical language model that then anybody could use to like fine-tune on like a prostate cancer subset of medical literature for instance like There's so much we could do it's kind of exciting and then you know to your net point We could also combine this with like pre-trained word vectors. It's like even without Trying that hard like you know we even without news like We could have pre-trained a Wikipedia say corpus language model and then fine-tuned it into a IMDB language model and then fine-tune that into an IBM IMDB sentiment analysis model, and we would have got something better than this So like this. I really think this is the tip of the iceberg And I was talking there's a really fantastic researcher called Sebastian Ruda who is Basically the only NLP researcher. I know who's been really really writing a lot about pre-training and fine-tuning and transfer learning and NLP and I was asking him like why isn't this happening more and His view was it's because there isn't the software to make it easy you know So I'm actually going to share this lecture with with him tomorrow Because you know it feels like there's you know hopefully going to be a lot of stuff coming out now that we're making it really easy to do this Okay We're kind of out of time so what I'll do is I'll quickly look at Collaborative filtering introduction, and then we'll finish it next time the collaborative filtering. There's very very little you to learn We've basically learned everything we're going to need to do this week Collaborative filtering will will cover this quite quickly next week, and then we're going to do a really deep dive into collaborative filtering next week Where we're going to learn about like we're actually going to from scratch learn how to do stochastic gradient descent How to create loss functions how they work exactly and then we'll go from there We'll gradually build back up to really deeply understand What's going on in the structured models, and then what's going on in confidence And then finally what's going on in recurrent neural networks, and hopefully we'll be able to build them all From scratch okay, so this is kind of a going to be really important this movie lens data set because we're going to use it to learn a lot of like Really foundational theory and kind of math behind it so the movie lens data set This is basically what it looks like It contains a bunch of ratings. It says user number one watched movie number 31, and they gave it a rating of two and a half at this particular time and Then they watched movie 1029 and they gave it a rating of three and they watched rating one one movie 1172 And they gave it a rating of four okay, and so forth So this is the ratings table. This is really the only one that matters and our goal will be for some user We haven't seen before sorry for some user movie combination. We haven't seen before we have to predict if they'll like it Right and so this is how recommendation systems are built This is how like Amazon decides what books to recommend Netflix decides what movies to recommend and so forth To make it more interesting we'll also actually download a list of movies So each movie we're actually going to have the title and so for that question earlier about like what's actually going to be in these? Embedding matrices, how do we interpret them? We're actually going to be able to look and see How that's working? So basically this is kind of like what we're creating this is kind of cross tab of users By movies right and so feel free to look ahead during the week You'll see basically as per usual collab filter data set from CSP Model data get learner learn dot fit and we're done And you won't be surprised to hear when we then take that and we can correct the benchmarks It seems to be better than the benchmarks where you looked at so that'll basically be it and then next week We'll have a deep dive, and we'll see how to actually build this from scratch all right. See you next week Thank you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.34, "text": " Okay, hi everybody welcome back good to see you all here", "tokens": [1033, 11, 4879, 2201, 2928, 646, 665, 281, 536, 291, 439, 510], "temperature": 0.0, "avg_logprob": -0.2321017338679387, "compression_ratio": 1.4804469273743017, "no_speech_prob": 0.00805879570543766}, {"id": 1, "seek": 0, "start": 6.24, "end": 10.86, "text": " It's been another busy week of deep learning", "tokens": [467, 311, 668, 1071, 5856, 1243, 295, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.2321017338679387, "compression_ratio": 1.4804469273743017, "no_speech_prob": 0.00805879570543766}, {"id": 2, "seek": 0, "start": 12.96, "end": 16.44, "text": " Lots of cool things going on and like last week", "tokens": [15908, 295, 1627, 721, 516, 322, 293, 411, 1036, 1243], "temperature": 0.0, "avg_logprob": -0.2321017338679387, "compression_ratio": 1.4804469273743017, "no_speech_prob": 0.00805879570543766}, {"id": 3, "seek": 0, "start": 16.44, "end": 21.28, "text": " I wanted to highlight a few really interesting articles that some of some of you folks have written", "tokens": [286, 1415, 281, 5078, 257, 1326, 534, 1880, 11290, 300, 512, 295, 512, 295, 291, 4024, 362, 3720], "temperature": 0.0, "avg_logprob": -0.2321017338679387, "compression_ratio": 1.4804469273743017, "no_speech_prob": 0.00805879570543766}, {"id": 4, "seek": 0, "start": 24.36, "end": 26.36, "text": " The tally wrote", "tokens": [440, 256, 379, 4114], "temperature": 0.0, "avg_logprob": -0.2321017338679387, "compression_ratio": 1.4804469273743017, "no_speech_prob": 0.00805879570543766}, {"id": 5, "seek": 2636, "start": 26.36, "end": 32.86, "text": " One of the best articles I've seen for a while. I think actually talking about", "tokens": [1485, 295, 264, 1151, 11290, 286, 600, 1612, 337, 257, 1339, 13, 286, 519, 767, 1417, 466], "temperature": 0.0, "avg_logprob": -0.16825626293818155, "compression_ratio": 1.5685483870967742, "no_speech_prob": 0.00013131349987816066}, {"id": 6, "seek": 2636, "start": 33.82, "end": 36.72, "text": " differential learning rates and stochastic gradient descent with restarts", "tokens": [15756, 2539, 6846, 293, 342, 8997, 2750, 16235, 23475, 365, 1472, 11814], "temperature": 0.0, "avg_logprob": -0.16825626293818155, "compression_ratio": 1.5685483870967742, "no_speech_prob": 0.00013131349987816066}, {"id": 7, "seek": 2636, "start": 37.82, "end": 43.04, "text": " Be sure to check it out if you can because what he's done. I feel like he's done a great job of", "tokens": [879, 988, 281, 1520, 309, 484, 498, 291, 393, 570, 437, 415, 311, 1096, 13, 286, 841, 411, 415, 311, 1096, 257, 869, 1691, 295], "temperature": 0.0, "avg_logprob": -0.16825626293818155, "compression_ratio": 1.5685483870967742, "no_speech_prob": 0.00013131349987816066}, {"id": 8, "seek": 2636, "start": 44.78, "end": 48.379999999999995, "text": " Kind of positioning in a place that you can get a lot out of it", "tokens": [9242, 295, 26381, 294, 257, 1081, 300, 291, 393, 483, 257, 688, 484, 295, 309], "temperature": 0.0, "avg_logprob": -0.16825626293818155, "compression_ratio": 1.5685483870967742, "no_speech_prob": 0.00013131349987816066}, {"id": 9, "seek": 2636, "start": 48.379999999999995, "end": 52.18, "text": " You know regardless of your background, but for those who want to go further", "tokens": [509, 458, 10060, 295, 428, 3678, 11, 457, 337, 729, 567, 528, 281, 352, 3052], "temperature": 0.0, "avg_logprob": -0.16825626293818155, "compression_ratio": 1.5685483870967742, "no_speech_prob": 0.00013131349987816066}, {"id": 10, "seek": 5218, "start": 52.18, "end": 59.1, "text": " He's also got links to like the academic papers that came from and kind of graphs of showing examples of all of all the things", "tokens": [634, 311, 611, 658, 6123, 281, 411, 264, 7778, 10577, 300, 1361, 490, 293, 733, 295, 24877, 295, 4099, 5110, 295, 439, 295, 439, 264, 721], "temperature": 0.0, "avg_logprob": -0.16999921258890405, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.8227601180551574e-05}, {"id": 11, "seek": 5218, "start": 59.1, "end": 62.5, "text": " He's talking about and I think it's a it's a particularly", "tokens": [634, 311, 1417, 466, 293, 286, 519, 309, 311, 257, 309, 311, 257, 4098], "temperature": 0.0, "avg_logprob": -0.16999921258890405, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.8227601180551574e-05}, {"id": 12, "seek": 5218, "start": 63.3, "end": 68.62, "text": " Nicely done article so a good kind of role model for technical communication", "tokens": [14776, 736, 1096, 7222, 370, 257, 665, 733, 295, 3090, 2316, 337, 6191, 6101], "temperature": 0.0, "avg_logprob": -0.16999921258890405, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.8227601180551574e-05}, {"id": 13, "seek": 5218, "start": 69.34, "end": 72.62, "text": " One of the things I've liked about you know seeing people post these", "tokens": [1485, 295, 264, 721, 286, 600, 4501, 466, 291, 458, 2577, 561, 2183, 613], "temperature": 0.0, "avg_logprob": -0.16999921258890405, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.8227601180551574e-05}, {"id": 14, "seek": 5218, "start": 73.42, "end": 79.74000000000001, "text": " Post these articles during the week is the discussion on the forums have also been like really great. There's been a lot of a", "tokens": [10223, 613, 11290, 1830, 264, 1243, 307, 264, 5017, 322, 264, 26998, 362, 611, 668, 411, 534, 869, 13, 821, 311, 668, 257, 688, 295, 257], "temperature": 0.0, "avg_logprob": -0.16999921258890405, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.8227601180551574e-05}, {"id": 15, "seek": 7974, "start": 79.74, "end": 82.17999999999999, "text": " lot of people helping out like", "tokens": [688, 295, 561, 4315, 484, 411], "temperature": 0.0, "avg_logprob": -0.2122472616342398, "compression_ratio": 1.741444866920152, "no_speech_prob": 2.0462415704969317e-05}, {"id": 16, "seek": 7974, "start": 82.97999999999999, "end": 88.67999999999999, "text": " Explaining things you know which you know maybe there's parts of the post bit where people have said actually that's not quite how it works", "tokens": [12514, 3686, 721, 291, 458, 597, 291, 458, 1310, 456, 311, 3166, 295, 264, 2183, 857, 689, 561, 362, 848, 767, 300, 311, 406, 1596, 577, 309, 1985], "temperature": 0.0, "avg_logprob": -0.2122472616342398, "compression_ratio": 1.741444866920152, "no_speech_prob": 2.0462415704969317e-05}, {"id": 17, "seek": 7974, "start": 88.67999999999999, "end": 93.66, "text": " And people have learned new things that way people have come up with new ideas as a result as well", "tokens": [400, 561, 362, 3264, 777, 721, 300, 636, 561, 362, 808, 493, 365, 777, 3487, 382, 257, 1874, 382, 731], "temperature": 0.0, "avg_logprob": -0.2122472616342398, "compression_ratio": 1.741444866920152, "no_speech_prob": 2.0462415704969317e-05}, {"id": 18, "seek": 7974, "start": 95.3, "end": 101.28, "text": " These discussions of stochastic gradient descent with restarts and cyclical learning rates has been a few of them actually", "tokens": [1981, 11088, 295, 342, 8997, 2750, 16235, 23475, 365, 1472, 11814, 293, 19474, 804, 2539, 6846, 575, 668, 257, 1326, 295, 552, 767], "temperature": 0.0, "avg_logprob": -0.2122472616342398, "compression_ratio": 1.741444866920152, "no_speech_prob": 2.0462415704969317e-05}, {"id": 19, "seek": 7974, "start": 102.06, "end": 104.78, "text": " Anand Saha has written another great post", "tokens": [1107, 474, 318, 4408, 575, 3720, 1071, 869, 2183], "temperature": 0.0, "avg_logprob": -0.2122472616342398, "compression_ratio": 1.741444866920152, "no_speech_prob": 2.0462415704969317e-05}, {"id": 20, "seek": 7974, "start": 106.25999999999999, "end": 108.25999999999999, "text": " Talking about a similar", "tokens": [22445, 466, 257, 2531], "temperature": 0.0, "avg_logprob": -0.2122472616342398, "compression_ratio": 1.741444866920152, "no_speech_prob": 2.0462415704969317e-05}, {"id": 21, "seek": 10826, "start": 108.26, "end": 113.82000000000001, "text": " Similar topic and why it works so well and again lots of great pictures and references to", "tokens": [10905, 4829, 293, 983, 309, 1985, 370, 731, 293, 797, 3195, 295, 869, 5242, 293, 15400, 281], "temperature": 0.0, "avg_logprob": -0.2534404516220093, "compression_ratio": 1.6, "no_speech_prob": 7.76676824898459e-06}, {"id": 22, "seek": 10826, "start": 115.14, "end": 119.66000000000001, "text": " Papers and most importantly perhaps code showing how it actually works", "tokens": [430, 14441, 293, 881, 8906, 4317, 3089, 4099, 577, 309, 767, 1985], "temperature": 0.0, "avg_logprob": -0.2534404516220093, "compression_ratio": 1.6, "no_speech_prob": 7.76676824898459e-06}, {"id": 23, "seek": 10826, "start": 122.38000000000001, "end": 123.86, "text": " Mark Hoffman", "tokens": [3934, 29135, 1601], "temperature": 0.0, "avg_logprob": -0.2534404516220093, "compression_ratio": 1.6, "no_speech_prob": 7.76676824898459e-06}, {"id": 24, "seek": 10826, "start": 123.86, "end": 130.22, "text": " Covered the same topic at kind of a nice introductory level. I think really really kind of clear intuition", "tokens": [19106, 292, 264, 912, 4829, 412, 733, 295, 257, 1481, 39048, 1496, 13, 286, 519, 534, 534, 733, 295, 1850, 24002], "temperature": 0.0, "avg_logprob": -0.2534404516220093, "compression_ratio": 1.6, "no_speech_prob": 7.76676824898459e-06}, {"id": 25, "seek": 10826, "start": 131.46, "end": 132.78, "text": " many canta", "tokens": [867, 393, 1328], "temperature": 0.0, "avg_logprob": -0.2534404516220093, "compression_ratio": 1.6, "no_speech_prob": 7.76676824898459e-06}, {"id": 26, "seek": 13278, "start": 132.78, "end": 141.22, "text": " Talk specifically about differential learning rates and why it's interesting and again providing some nice context people not familiar with", "tokens": [8780, 4682, 466, 15756, 2539, 6846, 293, 983, 309, 311, 1880, 293, 797, 6530, 512, 1481, 4319, 561, 406, 4963, 365], "temperature": 0.0, "avg_logprob": -0.21233149937220983, "compression_ratio": 1.7902621722846441, "no_speech_prob": 6.643360393354669e-06}, {"id": 27, "seek": 13278, "start": 141.3, "end": 146.74, "text": " Transfer learning you know going back saying like well. What is transfer learning? Why is that interesting and", "tokens": [35025, 2539, 291, 458, 516, 646, 1566, 411, 731, 13, 708, 307, 5003, 2539, 30, 1545, 307, 300, 1880, 293], "temperature": 0.0, "avg_logprob": -0.21233149937220983, "compression_ratio": 1.7902621722846441, "no_speech_prob": 6.643360393354669e-06}, {"id": 28, "seek": 13278, "start": 147.22, "end": 150.34, "text": " Given that why could differential learning rates be helpful?", "tokens": [18600, 300, 983, 727, 15756, 2539, 6846, 312, 4961, 30], "temperature": 0.0, "avg_logprob": -0.21233149937220983, "compression_ratio": 1.7902621722846441, "no_speech_prob": 6.643360393354669e-06}, {"id": 29, "seek": 13278, "start": 151.5, "end": 153.5, "text": " and then", "tokens": [293, 550], "temperature": 0.0, "avg_logprob": -0.21233149937220983, "compression_ratio": 1.7902621722846441, "no_speech_prob": 6.643360393354669e-06}, {"id": 30, "seek": 13278, "start": 153.5, "end": 155.5, "text": " One thing I particularly liked about Arjun's", "tokens": [1485, 551, 286, 4098, 4501, 466, 1587, 10010, 311], "temperature": 0.0, "avg_logprob": -0.21233149937220983, "compression_ratio": 1.7902621722846441, "no_speech_prob": 6.643360393354669e-06}, {"id": 31, "seek": 13278, "start": 156.18, "end": 161.92000000000002, "text": " Article was that he talked not just about the technology that we're looking at but also talked about some of the", "tokens": [35230, 390, 300, 415, 2825, 406, 445, 466, 264, 2899, 300, 321, 434, 1237, 412, 457, 611, 2825, 466, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.21233149937220983, "compression_ratio": 1.7902621722846441, "no_speech_prob": 6.643360393354669e-06}, {"id": 32, "seek": 16192, "start": 161.92, "end": 163.48, "text": " implications", "tokens": [16602], "temperature": 0.0, "avg_logprob": -0.18529159464734665, "compression_ratio": 1.7914893617021277, "no_speech_prob": 2.9479729164449964e-06}, {"id": 33, "seek": 16192, "start": 163.48, "end": 169.83999999999997, "text": " Particularly from a commercial point of view so thinking about like based on some of the things we've learned about so far", "tokens": [32281, 490, 257, 6841, 935, 295, 1910, 370, 1953, 466, 411, 2361, 322, 512, 295, 264, 721, 321, 600, 3264, 466, 370, 1400], "temperature": 0.0, "avg_logprob": -0.18529159464734665, "compression_ratio": 1.7914893617021277, "no_speech_prob": 2.9479729164449964e-06}, {"id": 34, "seek": 16192, "start": 169.83999999999997, "end": 173.17999999999998, "text": " What are some of the implications that that has you know in real life?", "tokens": [708, 366, 512, 295, 264, 16602, 300, 300, 575, 291, 458, 294, 957, 993, 30], "temperature": 0.0, "avg_logprob": -0.18529159464734665, "compression_ratio": 1.7914893617021277, "no_speech_prob": 2.9479729164449964e-06}, {"id": 35, "seek": 16192, "start": 173.79999999999998, "end": 176.2, "text": " And lots of background lots of pictures", "tokens": [400, 3195, 295, 3678, 3195, 295, 5242], "temperature": 0.0, "avg_logprob": -0.18529159464734665, "compression_ratio": 1.7914893617021277, "no_speech_prob": 2.9479729164449964e-06}, {"id": 36, "seek": 16192, "start": 177.07999999999998, "end": 180.42, "text": " And then discussing some of the yeah some of the implications", "tokens": [400, 550, 10850, 512, 295, 264, 1338, 512, 295, 264, 16602], "temperature": 0.0, "avg_logprob": -0.18529159464734665, "compression_ratio": 1.7914893617021277, "no_speech_prob": 2.9479729164449964e-06}, {"id": 37, "seek": 16192, "start": 181.27999999999997, "end": 186.66, "text": " So there's been lots of great stuff online and thanks to everybody for all the great work that you've been doing", "tokens": [407, 456, 311, 668, 3195, 295, 869, 1507, 2950, 293, 3231, 281, 2201, 337, 439, 264, 869, 589, 300, 291, 600, 668, 884], "temperature": 0.0, "avg_logprob": -0.18529159464734665, "compression_ratio": 1.7914893617021277, "no_speech_prob": 2.9479729164449964e-06}, {"id": 38, "seek": 18666, "start": 186.66, "end": 190.82, "text": " As we talked about last week if you're", "tokens": [1018, 321, 2825, 466, 1036, 1243, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.15128636611135382, "compression_ratio": 1.676595744680851, "no_speech_prob": 1.7603211972527788e-06}, {"id": 39, "seek": 18666, "start": 191.38, "end": 197.62, "text": " Kind of vaguely wondering about writing something, but you're feeling a bit intimidated about it because you've never really written a technical post before", "tokens": [9242, 295, 13501, 48863, 6359, 466, 3579, 746, 11, 457, 291, 434, 2633, 257, 857, 40234, 466, 309, 570, 291, 600, 1128, 534, 3720, 257, 6191, 2183, 949], "temperature": 0.0, "avg_logprob": -0.15128636611135382, "compression_ratio": 1.676595744680851, "no_speech_prob": 1.7603211972527788e-06}, {"id": 40, "seek": 18666, "start": 197.94, "end": 201.98, "text": " Just jump in you know it's it's it's it's a really", "tokens": [1449, 3012, 294, 291, 458, 309, 311, 309, 311, 309, 311, 309, 311, 257, 534], "temperature": 0.0, "avg_logprob": -0.15128636611135382, "compression_ratio": 1.676595744680851, "no_speech_prob": 1.7603211972527788e-06}, {"id": 41, "seek": 18666, "start": 203.1, "end": 206.35999999999999, "text": " Welcoming and encouraging group I think to to work with", "tokens": [3778, 6590, 293, 14580, 1594, 286, 519, 281, 281, 589, 365], "temperature": 0.0, "avg_logprob": -0.15128636611135382, "compression_ratio": 1.676595744680851, "no_speech_prob": 1.7603211972527788e-06}, {"id": 42, "seek": 20636, "start": 206.36, "end": 215.88000000000002, "text": " So we're going to have a kind of an interesting lesson today, which is we're going to cover a", "tokens": [407, 321, 434, 516, 281, 362, 257, 733, 295, 364, 1880, 6898, 965, 11, 597, 307, 321, 434, 516, 281, 2060, 257], "temperature": 0.0, "avg_logprob": -0.13793061654778976, "compression_ratio": 1.663265306122449, "no_speech_prob": 1.9033507214771817e-06}, {"id": 43, "seek": 20636, "start": 217.16000000000003, "end": 223.44000000000003, "text": " Whole lot of different applications, so we've spent quite a lot of time on computer vision and today", "tokens": [30336, 688, 295, 819, 5821, 11, 370, 321, 600, 4418, 1596, 257, 688, 295, 565, 322, 3820, 5201, 293, 965], "temperature": 0.0, "avg_logprob": -0.13793061654778976, "compression_ratio": 1.663265306122449, "no_speech_prob": 1.9033507214771817e-06}, {"id": 44, "seek": 20636, "start": 223.44000000000003, "end": 228.14000000000001, "text": " We're going to try if we can to get through three totally different areas", "tokens": [492, 434, 516, 281, 853, 498, 321, 393, 281, 483, 807, 1045, 3879, 819, 3179], "temperature": 0.0, "avg_logprob": -0.13793061654778976, "compression_ratio": 1.663265306122449, "no_speech_prob": 1.9033507214771817e-06}, {"id": 45, "seek": 20636, "start": 229.04000000000002, "end": 232.16000000000003, "text": " Structured learning so looking at kind of how you look at", "tokens": [745, 46847, 2539, 370, 1237, 412, 733, 295, 577, 291, 574, 412], "temperature": 0.0, "avg_logprob": -0.13793061654778976, "compression_ratio": 1.663265306122449, "no_speech_prob": 1.9033507214771817e-06}, {"id": 46, "seek": 23216, "start": 232.16, "end": 239.35999999999999, "text": " So we're going to start out looking at structured learning or structured data learning by which I mean", "tokens": [407, 321, 434, 516, 281, 722, 484, 1237, 412, 18519, 2539, 420, 18519, 1412, 2539, 538, 597, 286, 914], "temperature": 0.0, "avg_logprob": -0.17264090498832807, "compression_ratio": 1.669811320754717, "no_speech_prob": 4.7850435294094495e-06}, {"id": 47, "seek": 23216, "start": 240.16, "end": 244.4, "text": " Building models on top of things that look more like database tables", "tokens": [18974, 5245, 322, 1192, 295, 721, 300, 574, 544, 411, 8149, 8020], "temperature": 0.0, "avg_logprob": -0.17264090498832807, "compression_ratio": 1.669811320754717, "no_speech_prob": 4.7850435294094495e-06}, {"id": 48, "seek": 23216, "start": 244.72, "end": 250.28, "text": " So kind of columns of different types of data. They might be financial or geographical or whatever", "tokens": [407, 733, 295, 13766, 295, 819, 3467, 295, 1412, 13, 814, 1062, 312, 4669, 420, 39872, 420, 2035], "temperature": 0.0, "avg_logprob": -0.17264090498832807, "compression_ratio": 1.669811320754717, "no_speech_prob": 4.7850435294094495e-06}, {"id": 49, "seek": 23216, "start": 251.28, "end": 256.6, "text": " We're going to look at using deep learning for language natural language processing", "tokens": [492, 434, 516, 281, 574, 412, 1228, 2452, 2539, 337, 2856, 3303, 2856, 9007], "temperature": 0.0, "avg_logprob": -0.17264090498832807, "compression_ratio": 1.669811320754717, "no_speech_prob": 4.7850435294094495e-06}, {"id": 50, "seek": 25660, "start": 256.6, "end": 262.44, "text": " And we're going to look at using deep learning for recommendation systems, and so we're going to cover these", "tokens": [400, 321, 434, 516, 281, 574, 412, 1228, 2452, 2539, 337, 11879, 3652, 11, 293, 370, 321, 434, 516, 281, 2060, 613], "temperature": 0.0, "avg_logprob": -0.14512608362280804, "compression_ratio": 1.7746478873239437, "no_speech_prob": 8.990927540253324e-07}, {"id": 51, "seek": 25660, "start": 263.16, "end": 266.64000000000004, "text": " At a very high level and the focus will be on", "tokens": [1711, 257, 588, 1090, 1496, 293, 264, 1879, 486, 312, 322], "temperature": 0.0, "avg_logprob": -0.14512608362280804, "compression_ratio": 1.7746478873239437, "no_speech_prob": 8.990927540253324e-07}, {"id": 52, "seek": 25660, "start": 268.64000000000004, "end": 271.08000000000004, "text": " Here is how to use the software to do it", "tokens": [1692, 307, 577, 281, 764, 264, 4722, 281, 360, 309], "temperature": 0.0, "avg_logprob": -0.14512608362280804, "compression_ratio": 1.7746478873239437, "no_speech_prob": 8.990927540253324e-07}, {"id": 53, "seek": 25660, "start": 271.52000000000004, "end": 276.16, "text": " More than here is what's going on behind the scenes and then the next three lessons", "tokens": [5048, 813, 510, 307, 437, 311, 516, 322, 2261, 264, 8026, 293, 550, 264, 958, 1045, 8820], "temperature": 0.0, "avg_logprob": -0.14512608362280804, "compression_ratio": 1.7746478873239437, "no_speech_prob": 8.990927540253324e-07}, {"id": 54, "seek": 25660, "start": 276.76000000000005, "end": 281.92, "text": " Will be digging into the details of what's been going on behind the scenes and also coming back to", "tokens": [3099, 312, 17343, 666, 264, 4365, 295, 437, 311, 668, 516, 322, 2261, 264, 8026, 293, 611, 1348, 646, 281], "temperature": 0.0, "avg_logprob": -0.14512608362280804, "compression_ratio": 1.7746478873239437, "no_speech_prob": 8.990927540253324e-07}, {"id": 55, "seek": 28192, "start": 281.92, "end": 287.72, "text": " Looking at a lot of the details of computer vision that we kind of skipped over so far", "tokens": [11053, 412, 257, 688, 295, 264, 4365, 295, 3820, 5201, 300, 321, 733, 295, 30193, 670, 370, 1400], "temperature": 0.0, "avg_logprob": -0.1490537581905242, "compression_ratio": 1.6058091286307055, "no_speech_prob": 3.6119631658948492e-06}, {"id": 56, "seek": 28192, "start": 288.12, "end": 293.84000000000003, "text": " So the focus today is really on like how do you actually do these applications?", "tokens": [407, 264, 1879, 965, 307, 534, 322, 411, 577, 360, 291, 767, 360, 613, 5821, 30], "temperature": 0.0, "avg_logprob": -0.1490537581905242, "compression_ratio": 1.6058091286307055, "no_speech_prob": 3.6119631658948492e-06}, {"id": 57, "seek": 28192, "start": 293.84000000000003, "end": 297.36, "text": " And we'll kind of talk briefly about some of the concepts involved", "tokens": [400, 321, 603, 733, 295, 751, 10515, 466, 512, 295, 264, 10392, 3288], "temperature": 0.0, "avg_logprob": -0.1490537581905242, "compression_ratio": 1.6058091286307055, "no_speech_prob": 3.6119631658948492e-06}, {"id": 58, "seek": 28192, "start": 299.68, "end": 302.20000000000005, "text": " Before we do I did want to talk about one key", "tokens": [4546, 321, 360, 286, 630, 528, 281, 751, 466, 472, 2141], "temperature": 0.0, "avg_logprob": -0.1490537581905242, "compression_ratio": 1.6058091286307055, "no_speech_prob": 3.6119631658948492e-06}, {"id": 59, "seek": 28192, "start": 304.6, "end": 306.48, "text": " New concept", "tokens": [1873, 3410], "temperature": 0.0, "avg_logprob": -0.1490537581905242, "compression_ratio": 1.6058091286307055, "no_speech_prob": 3.6119631658948492e-06}, {"id": 60, "seek": 30648, "start": 306.48, "end": 313.04, "text": " Which is dropout and you might have seen dropout mentioned a bunch of times already and got the got the impression that this is something", "tokens": [3013, 307, 3270, 346, 293, 291, 1062, 362, 1612, 3270, 346, 2835, 257, 3840, 295, 1413, 1217, 293, 658, 264, 658, 264, 9995, 300, 341, 307, 746], "temperature": 0.0, "avg_logprob": -0.1860224697568001, "compression_ratio": 1.7489711934156378, "no_speech_prob": 1.1478648048068862e-05}, {"id": 61, "seek": 30648, "start": 313.04, "end": 315.04, "text": " Important and indeed it is", "tokens": [42908, 293, 6451, 309, 307], "temperature": 0.0, "avg_logprob": -0.1860224697568001, "compression_ratio": 1.7489711934156378, "no_speech_prob": 1.1478648048068862e-05}, {"id": 62, "seek": 30648, "start": 315.32, "end": 318.74, "text": " So to look at dropout. I'm going to look at the the dog breeds", "tokens": [407, 281, 574, 412, 3270, 346, 13, 286, 478, 516, 281, 574, 412, 264, 264, 3000, 41609], "temperature": 0.0, "avg_logprob": -0.1860224697568001, "compression_ratio": 1.7489711934156378, "no_speech_prob": 1.1478648048068862e-05}, {"id": 63, "seek": 30648, "start": 321.12, "end": 327.12, "text": " Current cable competition that's going on and what I've done is I've got a head and I've created a", "tokens": [15629, 8220, 6211, 300, 311, 516, 322, 293, 437, 286, 600, 1096, 307, 286, 600, 658, 257, 1378, 293, 286, 600, 2942, 257], "temperature": 0.0, "avg_logprob": -0.1860224697568001, "compression_ratio": 1.7489711934156378, "no_speech_prob": 1.1478648048068862e-05}, {"id": 64, "seek": 30648, "start": 328.24, "end": 330.24, "text": " pre-trained network as per usual", "tokens": [659, 12, 17227, 2001, 3209, 382, 680, 7713], "temperature": 0.0, "avg_logprob": -0.1860224697568001, "compression_ratio": 1.7489711934156378, "no_speech_prob": 1.1478648048068862e-05}, {"id": 65, "seek": 30648, "start": 330.64000000000004, "end": 334.32, "text": " And I've passed in pre compute equals true and so that's going to", "tokens": [400, 286, 600, 4678, 294, 659, 14722, 6915, 2074, 293, 370, 300, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.1860224697568001, "compression_ratio": 1.7489711934156378, "no_speech_prob": 1.1478648048068862e-05}, {"id": 66, "seek": 33432, "start": 334.32, "end": 341.84, "text": " Pre compute the activations that come out of the last convolutional layer remember an activation is just a number", "tokens": [6001, 14722, 264, 2430, 763, 300, 808, 484, 295, 264, 1036, 45216, 304, 4583, 1604, 364, 24433, 307, 445, 257, 1230], "temperature": 0.0, "avg_logprob": -0.27608628954206194, "compression_ratio": 1.7657142857142858, "no_speech_prob": 6.962227416806854e-06}, {"id": 67, "seek": 33432, "start": 342.2, "end": 346.04, "text": " It's a number just to remind you an activation", "tokens": [467, 311, 257, 1230, 445, 281, 4160, 291, 364, 24433], "temperature": 0.0, "avg_logprob": -0.27608628954206194, "compression_ratio": 1.7657142857142858, "no_speech_prob": 6.962227416806854e-06}, {"id": 68, "seek": 33432, "start": 346.48, "end": 349.64, "text": " like here is one activation, it's a number and", "tokens": [411, 510, 307, 472, 24433, 11, 309, 311, 257, 1230, 293], "temperature": 0.0, "avg_logprob": -0.27608628954206194, "compression_ratio": 1.7657142857142858, "no_speech_prob": 6.962227416806854e-06}, {"id": 69, "seek": 33432, "start": 350.36, "end": 354.71999999999997, "text": " specifically the activations are calculated based on some weights", "tokens": [4682, 264, 2430, 763, 366, 15598, 2361, 322, 512, 17443], "temperature": 0.0, "avg_logprob": -0.27608628954206194, "compression_ratio": 1.7657142857142858, "no_speech_prob": 6.962227416806854e-06}, {"id": 70, "seek": 33432, "start": 355.44, "end": 358.15999999999997, "text": " Also called parameters that make up", "tokens": [2743, 1219, 9834, 300, 652, 493], "temperature": 0.0, "avg_logprob": -0.27608628954206194, "compression_ratio": 1.7657142857142858, "no_speech_prob": 6.962227416806854e-06}, {"id": 71, "seek": 35816, "start": 358.16, "end": 365.56, "text": " Kernels or filters and they get applied to the previous layers activations which could well be the inputs", "tokens": [40224, 1625, 420, 15995, 293, 436, 483, 6456, 281, 264, 3894, 7914, 2430, 763, 597, 727, 731, 312, 264, 15743], "temperature": 0.0, "avg_logprob": -0.19362950325012207, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8738721792033175e-06}, {"id": 72, "seek": 35816, "start": 365.72, "end": 371.44000000000005, "text": " Or they could themselves be the results of other calculations. Okay, so when we say activation keep remembering", "tokens": [1610, 436, 727, 2969, 312, 264, 3542, 295, 661, 20448, 13, 1033, 11, 370, 562, 321, 584, 24433, 1066, 20719], "temperature": 0.0, "avg_logprob": -0.19362950325012207, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8738721792033175e-06}, {"id": 73, "seek": 35816, "start": 371.48, "end": 373.6, "text": " We're talking about a number that's been calculated", "tokens": [492, 434, 1417, 466, 257, 1230, 300, 311, 668, 15598], "temperature": 0.0, "avg_logprob": -0.19362950325012207, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8738721792033175e-06}, {"id": 74, "seek": 35816, "start": 374.72, "end": 382.36, "text": " So we pre compute some activations and then what we do is we put on top of that a bunch of additional", "tokens": [407, 321, 659, 14722, 512, 2430, 763, 293, 550, 437, 321, 360, 307, 321, 829, 322, 1192, 295, 300, 257, 3840, 295, 4497], "temperature": 0.0, "avg_logprob": -0.19362950325012207, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8738721792033175e-06}, {"id": 75, "seek": 35816, "start": 382.88, "end": 384.88, "text": " initially randomly generated", "tokens": [9105, 16979, 10833], "temperature": 0.0, "avg_logprob": -0.19362950325012207, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8738721792033175e-06}, {"id": 76, "seek": 38488, "start": 384.88, "end": 391.2, "text": " Fully connected layers, so we're just going to do some matrix modifications on top of these just like in our Excel worksheet", "tokens": [479, 2150, 4582, 7914, 11, 370, 321, 434, 445, 516, 281, 360, 512, 8141, 26881, 322, 1192, 295, 613, 445, 411, 294, 527, 19060, 49890], "temperature": 0.0, "avg_logprob": -0.17363336716575184, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.7264627533440944e-06}, {"id": 77, "seek": 38488, "start": 391.96, "end": 393.96, "text": " at the very end", "tokens": [412, 264, 588, 917], "temperature": 0.0, "avg_logprob": -0.17363336716575184, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.7264627533440944e-06}, {"id": 78, "seek": 38488, "start": 395.71999999999997, "end": 399.92, "text": " We had this matrix that we just did a matrix multiplication, but", "tokens": [492, 632, 341, 8141, 300, 321, 445, 630, 257, 8141, 27290, 11, 457], "temperature": 0.0, "avg_logprob": -0.17363336716575184, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.7264627533440944e-06}, {"id": 79, "seek": 38488, "start": 401.56, "end": 405.38, "text": " So what you can actually do is if you just type", "tokens": [407, 437, 291, 393, 767, 360, 307, 498, 291, 445, 2010], "temperature": 0.0, "avg_logprob": -0.17363336716575184, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.7264627533440944e-06}, {"id": 80, "seek": 38488, "start": 405.88, "end": 411.3, "text": " The name of your learner object you can actually see what's in it. You can see the layers in it", "tokens": [440, 1315, 295, 428, 33347, 2657, 291, 393, 767, 536, 437, 311, 294, 309, 13, 509, 393, 536, 264, 7914, 294, 309], "temperature": 0.0, "avg_logprob": -0.17363336716575184, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.7264627533440944e-06}, {"id": 81, "seek": 41130, "start": 411.3, "end": 416.36, "text": " So when I was previously been skipping over a little bit about are we add a few layers to the end?", "tokens": [407, 562, 286, 390, 8046, 668, 31533, 670, 257, 707, 857, 466, 366, 321, 909, 257, 1326, 7914, 281, 264, 917, 30], "temperature": 0.0, "avg_logprob": -0.17144178732847556, "compression_ratio": 1.7518796992481203, "no_speech_prob": 1.191105184261687e-06}, {"id": 82, "seek": 41130, "start": 416.44, "end": 418.44, "text": " These are actually the layers that we add", "tokens": [1981, 366, 767, 264, 7914, 300, 321, 909], "temperature": 0.0, "avg_logprob": -0.17144178732847556, "compression_ratio": 1.7518796992481203, "no_speech_prob": 1.191105184261687e-06}, {"id": 83, "seek": 41130, "start": 419.36, "end": 426.66, "text": " We're going to do batch norm in the last lesson. So don't worry about that for now a linear layer simply means a matrix multiply", "tokens": [492, 434, 516, 281, 360, 15245, 2026, 294, 264, 1036, 6898, 13, 407, 500, 380, 3292, 466, 300, 337, 586, 257, 8213, 4583, 2935, 1355, 257, 8141, 12972], "temperature": 0.0, "avg_logprob": -0.17144178732847556, "compression_ratio": 1.7518796992481203, "no_speech_prob": 1.191105184261687e-06}, {"id": 84, "seek": 41130, "start": 426.84000000000003, "end": 430.36, "text": " okay, so this is a matrix which has a thousand and twenty four rows and", "tokens": [1392, 11, 370, 341, 307, 257, 8141, 597, 575, 257, 4714, 293, 7699, 1451, 13241, 293], "temperature": 0.0, "avg_logprob": -0.17144178732847556, "compression_ratio": 1.7518796992481203, "no_speech_prob": 1.191105184261687e-06}, {"id": 85, "seek": 41130, "start": 431.64, "end": 437.52, "text": " 512 columns and so in other words, it's going to take in a thousand and twenty four activations and spit out", "tokens": [1025, 4762, 13766, 293, 370, 294, 661, 2283, 11, 309, 311, 516, 281, 747, 294, 257, 4714, 293, 7699, 1451, 2430, 763, 293, 22127, 484], "temperature": 0.0, "avg_logprob": -0.17144178732847556, "compression_ratio": 1.7518796992481203, "no_speech_prob": 1.191105184261687e-06}, {"id": 86, "seek": 41130, "start": 438.36, "end": 440.36, "text": " 512 activations", "tokens": [1025, 4762, 2430, 763], "temperature": 0.0, "avg_logprob": -0.17144178732847556, "compression_ratio": 1.7518796992481203, "no_speech_prob": 1.191105184261687e-06}, {"id": 87, "seek": 44036, "start": 440.36, "end": 444.6, "text": " And then we have a relu remember is just replace the negatives with zero", "tokens": [400, 550, 321, 362, 257, 1039, 84, 1604, 307, 445, 7406, 264, 40019, 365, 4018], "temperature": 0.0, "avg_logprob": -0.21134611538478307, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.289318894734606e-06}, {"id": 88, "seek": 44036, "start": 445.8, "end": 450.8, "text": " We'll skip over the batch norm. We'll come back to drop out and then we have a second linear layer that takes those", "tokens": [492, 603, 10023, 670, 264, 15245, 2026, 13, 492, 603, 808, 646, 281, 3270, 484, 293, 550, 321, 362, 257, 1150, 8213, 4583, 300, 2516, 729], "temperature": 0.0, "avg_logprob": -0.21134611538478307, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.289318894734606e-06}, {"id": 89, "seek": 44036, "start": 451.24, "end": 455.88, "text": " 512 activations from the previous linear layer and puts them through a new matrix multiply", "tokens": [1025, 4762, 2430, 763, 490, 264, 3894, 8213, 4583, 293, 8137, 552, 807, 257, 777, 8141, 12972], "temperature": 0.0, "avg_logprob": -0.21134611538478307, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.289318894734606e-06}, {"id": 90, "seek": 44036, "start": 456.76, "end": 458.64, "text": " 512 by 120", "tokens": [1025, 4762, 538, 10411], "temperature": 0.0, "avg_logprob": -0.21134611538478307, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.289318894734606e-06}, {"id": 91, "seek": 44036, "start": 458.64, "end": 462.88, "text": " spits out a new 120 activations and then finally put that through", "tokens": [637, 1208, 484, 257, 777, 10411, 2430, 763, 293, 550, 2721, 829, 300, 807], "temperature": 0.0, "avg_logprob": -0.21134611538478307, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.289318894734606e-06}, {"id": 92, "seek": 44036, "start": 463.48, "end": 469.18, "text": " Softmax and for those of you that don't remember softmax. We looked at that last year last week", "tokens": [16985, 41167, 293, 337, 729, 295, 291, 300, 500, 380, 1604, 2787, 41167, 13, 492, 2956, 412, 300, 1036, 1064, 1036, 1243], "temperature": 0.0, "avg_logprob": -0.21134611538478307, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.289318894734606e-06}, {"id": 93, "seek": 46918, "start": 469.18, "end": 471.66, "text": " it's this idea that we basically just", "tokens": [309, 311, 341, 1558, 300, 321, 1936, 445], "temperature": 0.0, "avg_logprob": -0.2014217013404483, "compression_ratio": 1.7903930131004366, "no_speech_prob": 5.4221977734414395e-06}, {"id": 94, "seek": 46918, "start": 472.62, "end": 475.7, "text": " Take the creep the activation. Let's say for dog", "tokens": [3664, 264, 9626, 264, 24433, 13, 961, 311, 584, 337, 3000], "temperature": 0.0, "avg_logprob": -0.2014217013404483, "compression_ratio": 1.7903930131004366, "no_speech_prob": 5.4221977734414395e-06}, {"id": 95, "seek": 46918, "start": 476.54, "end": 482.94, "text": " Go either the power of that and then divide that into the sum of either the pair of all the activations", "tokens": [1037, 2139, 264, 1347, 295, 300, 293, 550, 9845, 300, 666, 264, 2408, 295, 2139, 264, 6119, 295, 439, 264, 2430, 763], "temperature": 0.0, "avg_logprob": -0.2014217013404483, "compression_ratio": 1.7903930131004366, "no_speech_prob": 5.4221977734414395e-06}, {"id": 96, "seek": 46918, "start": 482.98, "end": 488.6, "text": " So that was the thing that adds up to one all of them add up to one and each one individually is between zero and one", "tokens": [407, 300, 390, 264, 551, 300, 10860, 493, 281, 472, 439, 295, 552, 909, 493, 281, 472, 293, 1184, 472, 16652, 307, 1296, 4018, 293, 472], "temperature": 0.0, "avg_logprob": -0.2014217013404483, "compression_ratio": 1.7903930131004366, "no_speech_prob": 5.4221977734414395e-06}, {"id": 97, "seek": 46918, "start": 488.9, "end": 490.9, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.2014217013404483, "compression_ratio": 1.7903930131004366, "no_speech_prob": 5.4221977734414395e-06}, {"id": 98, "seek": 46918, "start": 490.94, "end": 495.94, "text": " That's that's what we added on top and that's the thing when we have pre compute equals true", "tokens": [663, 311, 300, 311, 437, 321, 3869, 322, 1192, 293, 300, 311, 264, 551, 562, 321, 362, 659, 14722, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.2014217013404483, "compression_ratio": 1.7903930131004366, "no_speech_prob": 5.4221977734414395e-06}, {"id": 99, "seek": 49594, "start": 495.94, "end": 502.38, "text": " That's the thing we train so I wanted to talk about what this dropout is and what this P is because it's a really important", "tokens": [663, 311, 264, 551, 321, 3847, 370, 286, 1415, 281, 751, 466, 437, 341, 3270, 346, 307, 293, 437, 341, 430, 307, 570, 309, 311, 257, 534, 1021], "temperature": 0.0, "avg_logprob": -0.20462166635613693, "compression_ratio": 1.7265625, "no_speech_prob": 1.994726744669606e-06}, {"id": 100, "seek": 49594, "start": 503.02, "end": 505.02, "text": " thing that we get to choose so", "tokens": [551, 300, 321, 483, 281, 2826, 370], "temperature": 0.0, "avg_logprob": -0.20462166635613693, "compression_ratio": 1.7265625, "no_speech_prob": 1.994726744669606e-06}, {"id": 101, "seek": 49594, "start": 505.3, "end": 508.34, "text": " a dropout layer with P equals zero point five", "tokens": [257, 3270, 346, 4583, 365, 430, 6915, 4018, 935, 1732], "temperature": 0.0, "avg_logprob": -0.20462166635613693, "compression_ratio": 1.7265625, "no_speech_prob": 1.994726744669606e-06}, {"id": 102, "seek": 49594, "start": 508.78, "end": 514.8, "text": " Literally does this we go over to our spreadsheet and let's pick any layer with some activations and let's say okay", "tokens": [23768, 775, 341, 321, 352, 670, 281, 527, 27733, 293, 718, 311, 1888, 604, 4583, 365, 512, 2430, 763, 293, 718, 311, 584, 1392], "temperature": 0.0, "avg_logprob": -0.20462166635613693, "compression_ratio": 1.7265625, "no_speech_prob": 1.994726744669606e-06}, {"id": 103, "seek": 49594, "start": 514.86, "end": 522.04, "text": " I'm going to apply dropout where the P of zero point five to cons two what that means is I go through and", "tokens": [286, 478, 516, 281, 3079, 3270, 346, 689, 264, 430, 295, 4018, 935, 1732, 281, 1014, 732, 437, 300, 1355, 307, 286, 352, 807, 293], "temperature": 0.0, "avg_logprob": -0.20462166635613693, "compression_ratio": 1.7265625, "no_speech_prob": 1.994726744669606e-06}, {"id": 104, "seek": 49594, "start": 522.78, "end": 524.78, "text": " with a 50% chance I", "tokens": [365, 257, 2625, 4, 2931, 286], "temperature": 0.0, "avg_logprob": -0.20462166635613693, "compression_ratio": 1.7265625, "no_speech_prob": 1.994726744669606e-06}, {"id": 105, "seek": 52478, "start": 524.78, "end": 531.8, "text": " Pick a cell I pick an activation so I picked like half of them randomly and I delete them", "tokens": [14129, 257, 2815, 286, 1888, 364, 24433, 370, 286, 6183, 411, 1922, 295, 552, 16979, 293, 286, 12097, 552], "temperature": 0.0, "avg_logprob": -0.24701439579830894, "compression_ratio": 1.620879120879121, "no_speech_prob": 1.1726406228262931e-06}, {"id": 106, "seek": 52478, "start": 533.72, "end": 534.9, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.24701439579830894, "compression_ratio": 1.620879120879121, "no_speech_prob": 1.1726406228262931e-06}, {"id": 107, "seek": 52478, "start": 534.9, "end": 542.1, "text": " That's that's what dropout is right so it's so the P equals point five means what's the probability of?", "tokens": [663, 311, 300, 311, 437, 3270, 346, 307, 558, 370, 309, 311, 370, 264, 430, 6915, 935, 1732, 1355, 437, 311, 264, 8482, 295, 30], "temperature": 0.0, "avg_logprob": -0.24701439579830894, "compression_ratio": 1.620879120879121, "no_speech_prob": 1.1726406228262931e-06}, {"id": 108, "seek": 52478, "start": 542.54, "end": 544.54, "text": " deleting that cell", "tokens": [48946, 300, 2815], "temperature": 0.0, "avg_logprob": -0.24701439579830894, "compression_ratio": 1.620879120879121, "no_speech_prob": 1.1726406228262931e-06}, {"id": 109, "seek": 52478, "start": 544.5799999999999, "end": 547.74, "text": " right so when I delete those cells", "tokens": [558, 370, 562, 286, 12097, 729, 5438], "temperature": 0.0, "avg_logprob": -0.24701439579830894, "compression_ratio": 1.620879120879121, "no_speech_prob": 1.1726406228262931e-06}, {"id": 110, "seek": 52478, "start": 548.86, "end": 551.6, "text": " If you have a look like look at the output", "tokens": [759, 291, 362, 257, 574, 411, 574, 412, 264, 5598], "temperature": 0.0, "avg_logprob": -0.24701439579830894, "compression_ratio": 1.620879120879121, "no_speech_prob": 1.1726406228262931e-06}, {"id": 111, "seek": 55160, "start": 551.6, "end": 557.82, "text": " It doesn't actually change by very much at all just a little bit particularly because remember it's going through a max pooling layer", "tokens": [467, 1177, 380, 767, 1319, 538, 588, 709, 412, 439, 445, 257, 707, 857, 4098, 570, 1604, 309, 311, 516, 807, 257, 11469, 7005, 278, 4583], "temperature": 0.0, "avg_logprob": -0.12641721963882446, "compression_ratio": 1.761061946902655, "no_speech_prob": 5.122893185216526e-07}, {"id": 112, "seek": 55160, "start": 557.82, "end": 562.7, "text": " Right so it's only going to change it at all if it was actually the maximum in that group of four", "tokens": [1779, 370, 309, 311, 787, 516, 281, 1319, 309, 412, 439, 498, 309, 390, 767, 264, 6674, 294, 300, 1594, 295, 1451], "temperature": 0.0, "avg_logprob": -0.12641721963882446, "compression_ratio": 1.761061946902655, "no_speech_prob": 5.122893185216526e-07}, {"id": 113, "seek": 55160, "start": 563.5, "end": 570.5, "text": " And furthermore, it's just one piece of you know if it's going into a convolution rather than into a max pool", "tokens": [400, 3052, 3138, 11, 309, 311, 445, 472, 2522, 295, 291, 458, 498, 309, 311, 516, 666, 257, 45216, 2831, 813, 666, 257, 11469, 7005], "temperature": 0.0, "avg_logprob": -0.12641721963882446, "compression_ratio": 1.761061946902655, "no_speech_prob": 5.122893185216526e-07}, {"id": 114, "seek": 55160, "start": 570.5, "end": 572.5, "text": " It's just one piece of that that filter", "tokens": [467, 311, 445, 472, 2522, 295, 300, 300, 6608], "temperature": 0.0, "avg_logprob": -0.12641721963882446, "compression_ratio": 1.761061946902655, "no_speech_prob": 5.122893185216526e-07}, {"id": 115, "seek": 55160, "start": 573.74, "end": 575.74, "text": " so interestingly", "tokens": [370, 25873], "temperature": 0.0, "avg_logprob": -0.12641721963882446, "compression_ratio": 1.761061946902655, "no_speech_prob": 5.122893185216526e-07}, {"id": 116, "seek": 57574, "start": 575.74, "end": 581.86, "text": " The idea of like randomly throwing away half of the activations in a layer", "tokens": [440, 1558, 295, 411, 16979, 10238, 1314, 1922, 295, 264, 2430, 763, 294, 257, 4583], "temperature": 0.0, "avg_logprob": -0.18970152047964242, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.2482674947023042e-06}, {"id": 117, "seek": 57574, "start": 582.62, "end": 589.0, "text": " Has a really interesting result and one important thing to mention is each mini-batch", "tokens": [8646, 257, 534, 1880, 1874, 293, 472, 1021, 551, 281, 2152, 307, 1184, 8382, 12, 65, 852], "temperature": 0.0, "avg_logprob": -0.18970152047964242, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.2482674947023042e-06}, {"id": 118, "seek": 57574, "start": 589.42, "end": 596.1800000000001, "text": " We throw away a different random half of activations in that layer and so what it means is", "tokens": [492, 3507, 1314, 257, 819, 4974, 1922, 295, 2430, 763, 294, 300, 4583, 293, 370, 437, 309, 1355, 307], "temperature": 0.0, "avg_logprob": -0.18970152047964242, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.2482674947023042e-06}, {"id": 119, "seek": 57574, "start": 596.74, "end": 602.98, "text": " It forces it to not overfit right in other words if there's some particular activation", "tokens": [467, 5874, 309, 281, 406, 670, 6845, 558, 294, 661, 2283, 498, 456, 311, 512, 1729, 24433], "temperature": 0.0, "avg_logprob": -0.18970152047964242, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.2482674947023042e-06}, {"id": 120, "seek": 60298, "start": 602.98, "end": 605.7, "text": " That's really learnt just that exact", "tokens": [663, 311, 534, 18991, 445, 300, 1900], "temperature": 0.0, "avg_logprob": -0.17084510509784406, "compression_ratio": 1.7488584474885844, "no_speech_prob": 2.994432179548312e-06}, {"id": 121, "seek": 60298, "start": 606.7, "end": 612.22, "text": " That exact dog or that exact cat right then when that gets dropped out", "tokens": [663, 1900, 3000, 420, 300, 1900, 3857, 558, 550, 562, 300, 2170, 8119, 484], "temperature": 0.0, "avg_logprob": -0.17084510509784406, "compression_ratio": 1.7488584474885844, "no_speech_prob": 2.994432179548312e-06}, {"id": 122, "seek": 60298, "start": 612.9, "end": 619.0600000000001, "text": " The whole thing now isn't going to work as well. It's not going to recognize that image right so it has to in order for this", "tokens": [440, 1379, 551, 586, 1943, 380, 516, 281, 589, 382, 731, 13, 467, 311, 406, 516, 281, 5521, 300, 3256, 558, 370, 309, 575, 281, 294, 1668, 337, 341], "temperature": 0.0, "avg_logprob": -0.17084510509784406, "compression_ratio": 1.7488584474885844, "no_speech_prob": 2.994432179548312e-06}, {"id": 123, "seek": 60298, "start": 619.0600000000001, "end": 623.1, "text": " To work it has to try and find a representation that", "tokens": [1407, 589, 309, 575, 281, 853, 293, 915, 257, 10290, 300], "temperature": 0.0, "avg_logprob": -0.17084510509784406, "compression_ratio": 1.7488584474885844, "no_speech_prob": 2.994432179548312e-06}, {"id": 124, "seek": 60298, "start": 624.5, "end": 631.36, "text": " That actually continues to work even as random half of the activations get thrown away every time", "tokens": [663, 767, 6515, 281, 589, 754, 382, 4974, 1922, 295, 264, 2430, 763, 483, 11732, 1314, 633, 565], "temperature": 0.0, "avg_logprob": -0.17084510509784406, "compression_ratio": 1.7488584474885844, "no_speech_prob": 2.994432179548312e-06}, {"id": 125, "seek": 63136, "start": 631.36, "end": 639.04, "text": " All right, so it's it's it's I guess about four years old now three or four years old and it's been", "tokens": [1057, 558, 11, 370, 309, 311, 309, 311, 309, 311, 286, 2041, 466, 1451, 924, 1331, 586, 1045, 420, 1451, 924, 1331, 293, 309, 311, 668], "temperature": 0.0, "avg_logprob": -0.2129026543010365, "compression_ratio": 1.5656108597285068, "no_speech_prob": 1.0030114481196506e-06}, {"id": 126, "seek": 63136, "start": 641.32, "end": 643.12, "text": " Absolutely critical in", "tokens": [7021, 4924, 294], "temperature": 0.0, "avg_logprob": -0.2129026543010365, "compression_ratio": 1.5656108597285068, "no_speech_prob": 1.0030114481196506e-06}, {"id": 127, "seek": 63136, "start": 643.12, "end": 649.16, "text": " Making modern deep learning work and the reason why is it really just about solved?", "tokens": [14595, 4363, 2452, 2539, 589, 293, 264, 1778, 983, 307, 309, 534, 445, 466, 13041, 30], "temperature": 0.0, "avg_logprob": -0.2129026543010365, "compression_ratio": 1.5656108597285068, "no_speech_prob": 1.0030114481196506e-06}, {"id": 128, "seek": 63136, "start": 649.72, "end": 653.46, "text": " The problem of generalization for us before dropout came along", "tokens": [440, 1154, 295, 2674, 2144, 337, 505, 949, 3270, 346, 1361, 2051], "temperature": 0.0, "avg_logprob": -0.2129026543010365, "compression_ratio": 1.5656108597285068, "no_speech_prob": 1.0030114481196506e-06}, {"id": 129, "seek": 63136, "start": 654.44, "end": 658.48, "text": " if you try to train a model with lots of parameters and", "tokens": [498, 291, 853, 281, 3847, 257, 2316, 365, 3195, 295, 9834, 293], "temperature": 0.0, "avg_logprob": -0.2129026543010365, "compression_ratio": 1.5656108597285068, "no_speech_prob": 1.0030114481196506e-06}, {"id": 130, "seek": 63136, "start": 658.9200000000001, "end": 660.9200000000001, "text": " You were overfitting", "tokens": [509, 645, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.2129026543010365, "compression_ratio": 1.5656108597285068, "no_speech_prob": 1.0030114481196506e-06}, {"id": 131, "seek": 66092, "start": 660.92, "end": 666.4, "text": " And you already tried all the data augmentation you could and you already had as much data as you could", "tokens": [400, 291, 1217, 3031, 439, 264, 1412, 14501, 19631, 291, 727, 293, 291, 1217, 632, 382, 709, 1412, 382, 291, 727], "temperature": 0.0, "avg_logprob": -0.19145004195396345, "compression_ratio": 1.7782426778242677, "no_speech_prob": 7.411213118757587e-06}, {"id": 132, "seek": 66092, "start": 666.76, "end": 671.1999999999999, "text": " You there were some other things you could try but to a large degree you were kind of stuck", "tokens": [509, 456, 645, 512, 661, 721, 291, 727, 853, 457, 281, 257, 2416, 4314, 291, 645, 733, 295, 5541], "temperature": 0.0, "avg_logprob": -0.19145004195396345, "compression_ratio": 1.7782426778242677, "no_speech_prob": 7.411213118757587e-06}, {"id": 133, "seek": 66092, "start": 671.5999999999999, "end": 673.68, "text": " right and so then", "tokens": [558, 293, 370, 550], "temperature": 0.0, "avg_logprob": -0.19145004195396345, "compression_ratio": 1.7782426778242677, "no_speech_prob": 7.411213118757587e-06}, {"id": 134, "seek": 66092, "start": 674.88, "end": 682.1999999999999, "text": " Jeffrey Hinton and his colleagues came up with this this dropout idea that was loosely inspired by the way the brain works", "tokens": [28721, 389, 12442, 293, 702, 7734, 1361, 493, 365, 341, 341, 3270, 346, 1558, 300, 390, 37966, 7547, 538, 264, 636, 264, 3567, 1985], "temperature": 0.0, "avg_logprob": -0.19145004195396345, "compression_ratio": 1.7782426778242677, "no_speech_prob": 7.411213118757587e-06}, {"id": 135, "seek": 66092, "start": 683.04, "end": 688.0799999999999, "text": " And also loosely inspired by Jeffrey Hinton's experience in bank telecuse apparently", "tokens": [400, 611, 37966, 7547, 538, 28721, 389, 12442, 311, 1752, 294, 3765, 4304, 66, 438, 7970], "temperature": 0.0, "avg_logprob": -0.19145004195396345, "compression_ratio": 1.7782426778242677, "no_speech_prob": 7.411213118757587e-06}, {"id": 136, "seek": 66092, "start": 688.8, "end": 690.1999999999999, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.19145004195396345, "compression_ratio": 1.7782426778242677, "no_speech_prob": 7.411213118757587e-06}, {"id": 137, "seek": 69020, "start": 690.2, "end": 696.08, "text": " Yeah, somehow they came up with this amazing idea of like hey, let's let's try throwing things away at random and", "tokens": [865, 11, 6063, 436, 1361, 493, 365, 341, 2243, 1558, 295, 411, 4177, 11, 718, 311, 718, 311, 853, 10238, 721, 1314, 412, 4974, 293], "temperature": 0.0, "avg_logprob": -0.2328166961669922, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041580157514545e-06}, {"id": 138, "seek": 69020, "start": 697.08, "end": 699.88, "text": " so as you could imagine if your", "tokens": [370, 382, 291, 727, 3811, 498, 428], "temperature": 0.0, "avg_logprob": -0.2328166961669922, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041580157514545e-06}, {"id": 139, "seek": 69020, "start": 700.1600000000001, "end": 707.6400000000001, "text": " P was like point oh one then you're throwing away 1% of your activations for that layer at random", "tokens": [430, 390, 411, 935, 1954, 472, 550, 291, 434, 10238, 1314, 502, 4, 295, 428, 2430, 763, 337, 300, 4583, 412, 4974], "temperature": 0.0, "avg_logprob": -0.2328166961669922, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041580157514545e-06}, {"id": 140, "seek": 69020, "start": 707.76, "end": 711.5200000000001, "text": " It's not going to randomly change things up very much at all", "tokens": [467, 311, 406, 516, 281, 16979, 1319, 721, 493, 588, 709, 412, 439], "temperature": 0.0, "avg_logprob": -0.2328166961669922, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041580157514545e-06}, {"id": 141, "seek": 69020, "start": 712.44, "end": 714.8000000000001, "text": " So it's not really going to protect you from", "tokens": [407, 309, 311, 406, 534, 516, 281, 2371, 291, 490], "temperature": 0.0, "avg_logprob": -0.2328166961669922, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041580157514545e-06}, {"id": 142, "seek": 71480, "start": 714.8, "end": 720.0799999999999, "text": " Overfitting much at all on the other hand if your P was point nine nine", "tokens": [4886, 69, 2414, 709, 412, 439, 322, 264, 661, 1011, 498, 428, 430, 390, 935, 4949, 4949], "temperature": 0.0, "avg_logprob": -0.20597543716430664, "compression_ratio": 1.6476683937823835, "no_speech_prob": 1.3287733509059763e-06}, {"id": 143, "seek": 71480, "start": 720.3599999999999, "end": 726.4399999999999, "text": " Then that would be like going through the whole thing and throwing away nearly everything right and", "tokens": [1396, 300, 576, 312, 411, 516, 807, 264, 1379, 551, 293, 10238, 1314, 6217, 1203, 558, 293], "temperature": 0.0, "avg_logprob": -0.20597543716430664, "compression_ratio": 1.6476683937823835, "no_speech_prob": 1.3287733509059763e-06}, {"id": 144, "seek": 71480, "start": 727.5999999999999, "end": 734.8, "text": " That would be very hard for it to overfit, so that would be great for generalization, but it's also going to kill your", "tokens": [663, 576, 312, 588, 1152, 337, 309, 281, 670, 6845, 11, 370, 300, 576, 312, 869, 337, 2674, 2144, 11, 457, 309, 311, 611, 516, 281, 1961, 428], "temperature": 0.0, "avg_logprob": -0.20597543716430664, "compression_ratio": 1.6476683937823835, "no_speech_prob": 1.3287733509059763e-06}, {"id": 145, "seek": 71480, "start": 735.76, "end": 736.92, "text": " accuracy", "tokens": [14170], "temperature": 0.0, "avg_logprob": -0.20597543716430664, "compression_ratio": 1.6476683937823835, "no_speech_prob": 1.3287733509059763e-06}, {"id": 146, "seek": 71480, "start": 736.92, "end": 738.92, "text": " so this is kind of", "tokens": [370, 341, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.20597543716430664, "compression_ratio": 1.6476683937823835, "no_speech_prob": 1.3287733509059763e-06}, {"id": 147, "seek": 73892, "start": 738.92, "end": 746.92, "text": " Playoff between high P values generalized well, but will decrease your training accuracy and low P values", "tokens": [5506, 4506, 1296, 1090, 430, 4190, 44498, 731, 11, 457, 486, 11514, 428, 3097, 14170, 293, 2295, 430, 4190], "temperature": 0.0, "avg_logprob": -0.221835530322531, "compression_ratio": 1.8319327731092436, "no_speech_prob": 3.1875476906861877e-06}, {"id": 148, "seek": 73892, "start": 746.92, "end": 750.76, "text": " Will generalize less well, but will give you a less good training accuracy", "tokens": [3099, 2674, 1125, 1570, 731, 11, 457, 486, 976, 291, 257, 1570, 665, 3097, 14170], "temperature": 0.0, "avg_logprob": -0.221835530322531, "compression_ratio": 1.8319327731092436, "no_speech_prob": 3.1875476906861877e-06}, {"id": 149, "seek": 73892, "start": 751.76, "end": 757.4799999999999, "text": " So for those of you that have been wondering why is it that particularly early in training are my validation?", "tokens": [407, 337, 729, 295, 291, 300, 362, 668, 6359, 983, 307, 309, 300, 4098, 2440, 294, 3097, 366, 452, 24071, 30], "temperature": 0.0, "avg_logprob": -0.221835530322531, "compression_ratio": 1.8319327731092436, "no_speech_prob": 3.1875476906861877e-06}, {"id": 150, "seek": 73892, "start": 758.0799999999999, "end": 765.7199999999999, "text": " Losses better than my training losses right which seems otherwise really surprising hopefully some of you have been wondering why that is", "tokens": [441, 772, 279, 1101, 813, 452, 3097, 15352, 558, 597, 2544, 5911, 534, 8830, 4696, 512, 295, 291, 362, 668, 6359, 983, 300, 307], "temperature": 0.0, "avg_logprob": -0.221835530322531, "compression_ratio": 1.8319327731092436, "no_speech_prob": 3.1875476906861877e-06}, {"id": 151, "seek": 73892, "start": 766.36, "end": 767.4399999999999, "text": " because", "tokens": [570], "temperature": 0.0, "avg_logprob": -0.221835530322531, "compression_ratio": 1.8319327731092436, "no_speech_prob": 3.1875476906861877e-06}, {"id": 152, "seek": 76744, "start": 767.44, "end": 771.8800000000001, "text": " On a data set that it never gets to see you wouldn't expect the losses to ever be", "tokens": [1282, 257, 1412, 992, 300, 309, 1128, 2170, 281, 536, 291, 2759, 380, 2066, 264, 15352, 281, 1562, 312], "temperature": 0.0, "avg_logprob": -0.13749904799879642, "compression_ratio": 1.7279693486590038, "no_speech_prob": 1.6028053551053745e-06}, {"id": 153, "seek": 76744, "start": 772.24, "end": 778.2, "text": " That's better and the reason why is because when we look at the validation set we turn off dropout", "tokens": [663, 311, 1101, 293, 264, 1778, 983, 307, 570, 562, 321, 574, 412, 264, 24071, 992, 321, 1261, 766, 3270, 346], "temperature": 0.0, "avg_logprob": -0.13749904799879642, "compression_ratio": 1.7279693486590038, "no_speech_prob": 1.6028053551053745e-06}, {"id": 154, "seek": 76744, "start": 778.6, "end": 783.24, "text": " Right so in other words when you're doing inference when you're trying to say is this a cat or is this a dog?", "tokens": [1779, 370, 294, 661, 2283, 562, 291, 434, 884, 38253, 562, 291, 434, 1382, 281, 584, 307, 341, 257, 3857, 420, 307, 341, 257, 3000, 30], "temperature": 0.0, "avg_logprob": -0.13749904799879642, "compression_ratio": 1.7279693486590038, "no_speech_prob": 1.6028053551053745e-06}, {"id": 155, "seek": 76744, "start": 783.8000000000001, "end": 785.8000000000001, "text": " We certainly don't want to be including", "tokens": [492, 3297, 500, 380, 528, 281, 312, 3009], "temperature": 0.0, "avg_logprob": -0.13749904799879642, "compression_ratio": 1.7279693486590038, "no_speech_prob": 1.6028053551053745e-06}, {"id": 156, "seek": 76744, "start": 786.32, "end": 790.32, "text": " Random dropout there right we want to be using the best model we can", "tokens": [37603, 3270, 346, 456, 558, 321, 528, 281, 312, 1228, 264, 1151, 2316, 321, 393], "temperature": 0.0, "avg_logprob": -0.13749904799879642, "compression_ratio": 1.7279693486590038, "no_speech_prob": 1.6028053551053745e-06}, {"id": 157, "seek": 76744, "start": 790.9200000000001, "end": 793.96, "text": " Okay, so that's why early in training in particular", "tokens": [1033, 11, 370, 300, 311, 983, 2440, 294, 3097, 294, 1729], "temperature": 0.0, "avg_logprob": -0.13749904799879642, "compression_ratio": 1.7279693486590038, "no_speech_prob": 1.6028053551053745e-06}, {"id": 158, "seek": 79396, "start": 793.96, "end": 796.9200000000001, "text": " We actually see that our validation", "tokens": [492, 767, 536, 300, 527, 24071], "temperature": 0.0, "avg_logprob": -0.19867250779095819, "compression_ratio": 1.52803738317757, "no_speech_prob": 6.540365120599745e-06}, {"id": 159, "seek": 79396, "start": 797.9200000000001, "end": 799.9200000000001, "text": " Accuracy and loss tends to be better", "tokens": [5725, 374, 2551, 293, 4470, 12258, 281, 312, 1101], "temperature": 0.0, "avg_logprob": -0.19867250779095819, "compression_ratio": 1.52803738317757, "no_speech_prob": 6.540365120599745e-06}, {"id": 160, "seek": 79396, "start": 800.64, "end": 804.02, "text": " If we're using dropout, okay, so yes", "tokens": [759, 321, 434, 1228, 3270, 346, 11, 1392, 11, 370, 2086], "temperature": 0.0, "avg_logprob": -0.19867250779095819, "compression_ratio": 1.52803738317757, "no_speech_prob": 6.540365120599745e-06}, {"id": 161, "seek": 79396, "start": 807.0, "end": 810.9200000000001, "text": " They have to do anything to accommodate for the fact that you are throwing away some", "tokens": [814, 362, 281, 360, 1340, 281, 21410, 337, 264, 1186, 300, 291, 366, 10238, 1314, 512], "temperature": 0.0, "avg_logprob": -0.19867250779095819, "compression_ratio": 1.52803738317757, "no_speech_prob": 6.540365120599745e-06}, {"id": 162, "seek": 79396, "start": 812.2800000000001, "end": 814.2800000000001, "text": " That's a great question so", "tokens": [663, 311, 257, 869, 1168, 370], "temperature": 0.0, "avg_logprob": -0.19867250779095819, "compression_ratio": 1.52803738317757, "no_speech_prob": 6.540365120599745e-06}, {"id": 163, "seek": 79396, "start": 815.08, "end": 821.9200000000001, "text": " We don't but pie torch does so pie torch behind the scenes does two things if you say P equals point five", "tokens": [492, 500, 380, 457, 1730, 27822, 775, 370, 1730, 27822, 2261, 264, 8026, 775, 732, 721, 498, 291, 584, 430, 6915, 935, 1732], "temperature": 0.0, "avg_logprob": -0.19867250779095819, "compression_ratio": 1.52803738317757, "no_speech_prob": 6.540365120599745e-06}, {"id": 164, "seek": 82192, "start": 821.92, "end": 825.36, "text": " It throws away half of the activations", "tokens": [467, 19251, 1314, 1922, 295, 264, 2430, 763], "temperature": 0.0, "avg_logprob": -0.19197188483344185, "compression_ratio": 1.5989583333333333, "no_speech_prob": 7.002159918556572e-07}, {"id": 165, "seek": 82192, "start": 826.24, "end": 828.12, "text": " but it also", "tokens": [457, 309, 611], "temperature": 0.0, "avg_logprob": -0.19197188483344185, "compression_ratio": 1.5989583333333333, "no_speech_prob": 7.002159918556572e-07}, {"id": 166, "seek": 82192, "start": 828.12, "end": 835.04, "text": " Doubles all the activations that are already there so on average the kind of the average activation doesn't change", "tokens": [13200, 8806, 439, 264, 2430, 763, 300, 366, 1217, 456, 370, 322, 4274, 264, 733, 295, 264, 4274, 24433, 1177, 380, 1319], "temperature": 0.0, "avg_logprob": -0.19197188483344185, "compression_ratio": 1.5989583333333333, "no_speech_prob": 7.002159918556572e-07}, {"id": 167, "seek": 82192, "start": 835.52, "end": 837.5999999999999, "text": " Which is pretty pretty neat trick?", "tokens": [3013, 307, 1238, 1238, 10654, 4282, 30], "temperature": 0.0, "avg_logprob": -0.19197188483344185, "compression_ratio": 1.5989583333333333, "no_speech_prob": 7.002159918556572e-07}, {"id": 168, "seek": 82192, "start": 839.4399999999999, "end": 842.8399999999999, "text": " So yeah, you don't have to worry about it basically it's done for you", "tokens": [407, 1338, 11, 291, 500, 380, 362, 281, 3292, 466, 309, 1936, 309, 311, 1096, 337, 291], "temperature": 0.0, "avg_logprob": -0.19197188483344185, "compression_ratio": 1.5989583333333333, "no_speech_prob": 7.002159918556572e-07}, {"id": 169, "seek": 82192, "start": 844.64, "end": 848.24, "text": " So if we say so you can pass in peas", "tokens": [407, 498, 321, 584, 370, 291, 393, 1320, 294, 24494], "temperature": 0.0, "avg_logprob": -0.19197188483344185, "compression_ratio": 1.5989583333333333, "no_speech_prob": 7.002159918556572e-07}, {"id": 170, "seek": 84824, "start": 848.24, "end": 853.84, "text": " This is the this is the p value for all of the added layers to say", "tokens": [639, 307, 264, 341, 307, 264, 280, 2158, 337, 439, 295, 264, 3869, 7914, 281, 584], "temperature": 0.0, "avg_logprob": -0.1444361277988979, "compression_ratio": 1.821917808219178, "no_speech_prob": 2.9480047487595584e-06}, {"id": 171, "seek": 84824, "start": 854.52, "end": 859.4, "text": " With fast AI what dropout do you want on each of the layers in these these added layers?", "tokens": [2022, 2370, 7318, 437, 3270, 346, 360, 291, 528, 322, 1184, 295, 264, 7914, 294, 613, 613, 3869, 7914, 30], "temperature": 0.0, "avg_logprob": -0.1444361277988979, "compression_ratio": 1.821917808219178, "no_speech_prob": 2.9480047487595584e-06}, {"id": 172, "seek": 84824, "start": 859.64, "end": 865.44, "text": " It won't change the dropout in the pre-trained network like the hope is that that's already been", "tokens": [467, 1582, 380, 1319, 264, 3270, 346, 294, 264, 659, 12, 17227, 2001, 3209, 411, 264, 1454, 307, 300, 300, 311, 1217, 668], "temperature": 0.0, "avg_logprob": -0.1444361277988979, "compression_ratio": 1.821917808219178, "no_speech_prob": 2.9480047487595584e-06}, {"id": 173, "seek": 84824, "start": 866.24, "end": 868.44, "text": " Pre-trained with some appropriate level of dropout", "tokens": [6001, 12, 17227, 2001, 365, 512, 6854, 1496, 295, 3270, 346], "temperature": 0.0, "avg_logprob": -0.1444361277988979, "compression_ratio": 1.821917808219178, "no_speech_prob": 2.9480047487595584e-06}, {"id": 174, "seek": 84824, "start": 868.44, "end": 873.1, "text": " We don't change it but on these layers that we add you can say how much and so you can see here", "tokens": [492, 500, 380, 1319, 309, 457, 322, 613, 7914, 300, 321, 909, 291, 393, 584, 577, 709, 293, 370, 291, 393, 536, 510], "temperature": 0.0, "avg_logprob": -0.1444361277988979, "compression_ratio": 1.821917808219178, "no_speech_prob": 2.9480047487595584e-06}, {"id": 175, "seek": 87310, "start": 873.1, "end": 879.48, "text": " I said peas equals point five so my first dropout has point five my second dropout has point five", "tokens": [286, 848, 24494, 6915, 935, 1732, 370, 452, 700, 3270, 346, 575, 935, 1732, 452, 1150, 3270, 346, 575, 935, 1732], "temperature": 0.0, "avg_logprob": -0.20679909328244767, "compression_ratio": 1.891213389121339, "no_speech_prob": 5.255371434031986e-06}, {"id": 176, "seek": 87310, "start": 879.48, "end": 882.24, "text": " All right, you remember coming to the input of this", "tokens": [1057, 558, 11, 291, 1604, 1348, 281, 264, 4846, 295, 341], "temperature": 0.0, "avg_logprob": -0.20679909328244767, "compression_ratio": 1.891213389121339, "no_speech_prob": 5.255371434031986e-06}, {"id": 177, "seek": 87310, "start": 883.76, "end": 890.46, "text": " Was the output of the last convolutional layer of pre-trained network and we go away and we actually throw away half of that", "tokens": [3027, 264, 5598, 295, 264, 1036, 45216, 304, 4583, 295, 659, 12, 17227, 2001, 3209, 293, 321, 352, 1314, 293, 321, 767, 3507, 1314, 1922, 295, 300], "temperature": 0.0, "avg_logprob": -0.20679909328244767, "compression_ratio": 1.891213389121339, "no_speech_prob": 5.255371434031986e-06}, {"id": 178, "seek": 87310, "start": 890.6800000000001, "end": 893.36, "text": " Before you can start go through our linear layer", "tokens": [4546, 291, 393, 722, 352, 807, 527, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.20679909328244767, "compression_ratio": 1.891213389121339, "no_speech_prob": 5.255371434031986e-06}, {"id": 179, "seek": 87310, "start": 894.12, "end": 896.12, "text": " Throw away the negatives", "tokens": [22228, 1314, 264, 40019], "temperature": 0.0, "avg_logprob": -0.20679909328244767, "compression_ratio": 1.891213389121339, "no_speech_prob": 5.255371434031986e-06}, {"id": 180, "seek": 89612, "start": 896.12, "end": 903.88, "text": " Throw away half of the result of that go through another linear layer and then pass that to our softmax for", "tokens": [22228, 1314, 1922, 295, 264, 1874, 295, 300, 352, 807, 1071, 8213, 4583, 293, 550, 1320, 300, 281, 527, 2787, 41167, 337], "temperature": 0.0, "avg_logprob": -0.23907102195961963, "compression_ratio": 1.724, "no_speech_prob": 1.7061768176063197e-06}, {"id": 181, "seek": 89612, "start": 904.68, "end": 906.68, "text": " minor numerical precision", "tokens": [6696, 29054, 18356], "temperature": 0.0, "avg_logprob": -0.23907102195961963, "compression_ratio": 1.724, "no_speech_prob": 1.7061768176063197e-06}, {"id": 182, "seek": 89612, "start": 906.92, "end": 912.1, "text": " Reasons it turns out to be better to take the log of the softmax than the softmax directly", "tokens": [1300, 3646, 309, 4523, 484, 281, 312, 1101, 281, 747, 264, 3565, 295, 264, 2787, 41167, 813, 264, 2787, 41167, 3838], "temperature": 0.0, "avg_logprob": -0.23907102195961963, "compression_ratio": 1.724, "no_speech_prob": 1.7061768176063197e-06}, {"id": 183, "seek": 89612, "start": 912.1, "end": 917.44, "text": " And that's why you'll have noticed that when you actually get predictions out of our models you always have to go", "tokens": [400, 300, 311, 983, 291, 603, 362, 5694, 300, 562, 291, 767, 483, 21264, 484, 295, 527, 5245, 291, 1009, 362, 281, 352], "temperature": 0.0, "avg_logprob": -0.23907102195961963, "compression_ratio": 1.724, "no_speech_prob": 1.7061768176063197e-06}, {"id": 184, "seek": 89612, "start": 917.92, "end": 920.04, "text": " NP dot next of the predictions", "tokens": [38611, 5893, 958, 295, 264, 21264], "temperature": 0.0, "avg_logprob": -0.23907102195961963, "compression_ratio": 1.724, "no_speech_prob": 1.7061768176063197e-06}, {"id": 185, "seek": 89612, "start": 921.04, "end": 925.24, "text": " Again the details as to why aren't important so if we want to", "tokens": [3764, 264, 4365, 382, 281, 983, 3212, 380, 1021, 370, 498, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.23907102195961963, "compression_ratio": 1.724, "no_speech_prob": 1.7061768176063197e-06}, {"id": 186, "seek": 92524, "start": 925.24, "end": 929.44, "text": " Try removing dropout we could go peas equals zero", "tokens": [6526, 12720, 3270, 346, 321, 727, 352, 24494, 6915, 4018], "temperature": 0.0, "avg_logprob": -0.20270309791908608, "compression_ratio": 1.7244094488188977, "no_speech_prob": 1.2679263363679638e-06}, {"id": 187, "seek": 92524, "start": 930.2, "end": 935.6800000000001, "text": " Right and you'll see where else before we started with the point seven six accuracy in the first epoch now", "tokens": [1779, 293, 291, 603, 536, 689, 1646, 949, 321, 1409, 365, 264, 935, 3407, 2309, 14170, 294, 264, 700, 30992, 339, 586], "temperature": 0.0, "avg_logprob": -0.20270309791908608, "compression_ratio": 1.7244094488188977, "no_speech_prob": 1.2679263363679638e-06}, {"id": 188, "seek": 92524, "start": 935.6800000000001, "end": 937.6800000000001, "text": " We've got a point eight accuracy in the first epoch", "tokens": [492, 600, 658, 257, 935, 3180, 14170, 294, 264, 700, 30992, 339], "temperature": 0.0, "avg_logprob": -0.20270309791908608, "compression_ratio": 1.7244094488188977, "no_speech_prob": 1.2679263363679638e-06}, {"id": 189, "seek": 92524, "start": 937.76, "end": 944.28, "text": " All right, so by not doing dropout our first epoch worked better not surprisingly because we're not throwing anything away", "tokens": [1057, 558, 11, 370, 538, 406, 884, 3270, 346, 527, 700, 30992, 339, 2732, 1101, 406, 17600, 570, 321, 434, 406, 10238, 1340, 1314], "temperature": 0.0, "avg_logprob": -0.20270309791908608, "compression_ratio": 1.7244094488188977, "no_speech_prob": 1.2679263363679638e-06}, {"id": 190, "seek": 92524, "start": 944.96, "end": 948.16, "text": " but by the third epoch here we had 84.8 and", "tokens": [457, 538, 264, 2636, 30992, 339, 510, 321, 632, 29018, 13, 23, 293], "temperature": 0.0, "avg_logprob": -0.20270309791908608, "compression_ratio": 1.7244094488188977, "no_speech_prob": 1.2679263363679638e-06}, {"id": 191, "seek": 92524, "start": 948.8, "end": 953.08, "text": " Here we have 84.1. So it started out better and ended up worse", "tokens": [1692, 321, 362, 29018, 13, 16, 13, 407, 309, 1409, 484, 1101, 293, 4590, 493, 5324], "temperature": 0.0, "avg_logprob": -0.20270309791908608, "compression_ratio": 1.7244094488188977, "no_speech_prob": 1.2679263363679638e-06}, {"id": 192, "seek": 95308, "start": 953.08, "end": 960.2800000000001, "text": " So even after three epochs you can already see we're massively overfitting right we've got point three loss on the train", "tokens": [407, 754, 934, 1045, 30992, 28346, 291, 393, 1217, 536, 321, 434, 29379, 670, 69, 2414, 558, 321, 600, 658, 935, 1045, 4470, 322, 264, 3847], "temperature": 0.0, "avg_logprob": -0.15837861995885869, "compression_ratio": 1.7167381974248928, "no_speech_prob": 3.089487790930434e-06}, {"id": 193, "seek": 95308, "start": 960.6800000000001, "end": 963.6, "text": " And point five loss on the validation", "tokens": [400, 935, 1732, 4470, 322, 264, 24071], "temperature": 0.0, "avg_logprob": -0.15837861995885869, "compression_ratio": 1.7167381974248928, "no_speech_prob": 3.089487790930434e-06}, {"id": 194, "seek": 95308, "start": 965.48, "end": 971.8000000000001, "text": " And so if you look now you can see in the resulting model. There's no dropout at all", "tokens": [400, 370, 498, 291, 574, 586, 291, 393, 536, 294, 264, 16505, 2316, 13, 821, 311, 572, 3270, 346, 412, 439], "temperature": 0.0, "avg_logprob": -0.15837861995885869, "compression_ratio": 1.7167381974248928, "no_speech_prob": 3.089487790930434e-06}, {"id": 195, "seek": 95308, "start": 971.8000000000001, "end": 974.88, "text": " So if the P is zero we don't even add it to the model", "tokens": [407, 498, 264, 430, 307, 4018, 321, 500, 380, 754, 909, 309, 281, 264, 2316], "temperature": 0.0, "avg_logprob": -0.15837861995885869, "compression_ratio": 1.7167381974248928, "no_speech_prob": 3.089487790930434e-06}, {"id": 196, "seek": 97488, "start": 974.88, "end": 982.72, "text": " Another thing to mention is you might have noticed that what we've been doing is we've been adding two", "tokens": [3996, 551, 281, 2152, 307, 291, 1062, 362, 5694, 300, 437, 321, 600, 668, 884, 307, 321, 600, 668, 5127, 732], "temperature": 0.0, "avg_logprob": -0.17962482411374328, "compression_ratio": 1.7723214285714286, "no_speech_prob": 8.579215773352189e-07}, {"id": 197, "seek": 97488, "start": 984.24, "end": 986.24, "text": " Linear layers", "tokens": [14670, 289, 7914], "temperature": 0.0, "avg_logprob": -0.17962482411374328, "compression_ratio": 1.7723214285714286, "no_speech_prob": 8.579215773352189e-07}, {"id": 198, "seek": 97488, "start": 986.24, "end": 989.64, "text": " In our additional layers you don't have to do that by the way", "tokens": [682, 527, 4497, 7914, 291, 500, 380, 362, 281, 360, 300, 538, 264, 636], "temperature": 0.0, "avg_logprob": -0.17962482411374328, "compression_ratio": 1.7723214285714286, "no_speech_prob": 8.579215773352189e-07}, {"id": 199, "seek": 97488, "start": 989.64, "end": 996.52, "text": " There's actually a parameter called extra fully connected layers that you can basically pass a list", "tokens": [821, 311, 767, 257, 13075, 1219, 2857, 4498, 4582, 7914, 300, 291, 393, 1936, 1320, 257, 1329], "temperature": 0.0, "avg_logprob": -0.17962482411374328, "compression_ratio": 1.7723214285714286, "no_speech_prob": 8.579215773352189e-07}, {"id": 200, "seek": 97488, "start": 996.84, "end": 1003.28, "text": " But how long do you want or how big do you want each of the additional fully connected layers to be and so by default?", "tokens": [583, 577, 938, 360, 291, 528, 420, 577, 955, 360, 291, 528, 1184, 295, 264, 4497, 4498, 4582, 7914, 281, 312, 293, 370, 538, 7576, 30], "temperature": 0.0, "avg_logprob": -0.17962482411374328, "compression_ratio": 1.7723214285714286, "no_speech_prob": 8.579215773352189e-07}, {"id": 201, "seek": 100328, "start": 1003.28, "end": 1005.76, "text": " Well you need to have at least one", "tokens": [1042, 291, 643, 281, 362, 412, 1935, 472], "temperature": 0.0, "avg_logprob": -0.2225645065307617, "compression_ratio": 1.71875, "no_speech_prob": 1.4367457197295153e-06}, {"id": 202, "seek": 100328, "start": 1006.3199999999999, "end": 1013.0, "text": " right because you need something that takes the output of the convolutional layer which in this case is a size thousand twenty four and", "tokens": [558, 570, 291, 643, 746, 300, 2516, 264, 5598, 295, 264, 45216, 304, 4583, 597, 294, 341, 1389, 307, 257, 2744, 4714, 7699, 1451, 293], "temperature": 0.0, "avg_logprob": -0.2225645065307617, "compression_ratio": 1.71875, "no_speech_prob": 1.4367457197295153e-06}, {"id": 203, "seek": 100328, "start": 1013.24, "end": 1015.24, "text": " turns it into the number of", "tokens": [4523, 309, 666, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.2225645065307617, "compression_ratio": 1.71875, "no_speech_prob": 1.4367457197295153e-06}, {"id": 204, "seek": 100328, "start": 1016.0, "end": 1020.92, "text": " Classes you have perhaps versus dogs would be two dog breeds would be a hundred and twenty", "tokens": [9471, 279, 291, 362, 4317, 5717, 7197, 576, 312, 732, 3000, 41609, 576, 312, 257, 3262, 293, 7699], "temperature": 0.0, "avg_logprob": -0.2225645065307617, "compression_ratio": 1.71875, "no_speech_prob": 1.4367457197295153e-06}, {"id": 205, "seek": 100328, "start": 1021.8, "end": 1028.92, "text": " Planet satellite seventeen whatever that's you always need one linear layer at least and you can't pick how big that is that's", "tokens": [22146, 16016, 39532, 2035, 300, 311, 291, 1009, 643, 472, 8213, 4583, 412, 1935, 293, 291, 393, 380, 1888, 577, 955, 300, 307, 300, 311], "temperature": 0.0, "avg_logprob": -0.2225645065307617, "compression_ratio": 1.71875, "no_speech_prob": 1.4367457197295153e-06}, {"id": 206, "seek": 100328, "start": 1029.2, "end": 1031.2, "text": " defined by your problem", "tokens": [7642, 538, 428, 1154], "temperature": 0.0, "avg_logprob": -0.2225645065307617, "compression_ratio": 1.71875, "no_speech_prob": 1.4367457197295153e-06}, {"id": 207, "seek": 103120, "start": 1031.2, "end": 1035.2, "text": " But you can choose what the other size is or if it happens at all", "tokens": [583, 291, 393, 2826, 437, 264, 661, 2744, 307, 420, 498, 309, 2314, 412, 439], "temperature": 0.0, "avg_logprob": -0.16491979360580444, "compression_ratio": 1.598326359832636, "no_speech_prob": 3.2887348879739875e-06}, {"id": 208, "seek": 103120, "start": 1035.64, "end": 1037.64, "text": " So if we were to pass in an empty list", "tokens": [407, 498, 321, 645, 281, 1320, 294, 364, 6707, 1329], "temperature": 0.0, "avg_logprob": -0.16491979360580444, "compression_ratio": 1.598326359832636, "no_speech_prob": 3.2887348879739875e-06}, {"id": 209, "seek": 103120, "start": 1038.16, "end": 1042.76, "text": " Then now we're saying don't add any additional linear layers just the one that we have to have", "tokens": [1396, 586, 321, 434, 1566, 500, 380, 909, 604, 4497, 8213, 7914, 445, 264, 472, 300, 321, 362, 281, 362], "temperature": 0.0, "avg_logprob": -0.16491979360580444, "compression_ratio": 1.598326359832636, "no_speech_prob": 3.2887348879739875e-06}, {"id": 210, "seek": 103120, "start": 1043.28, "end": 1050.72, "text": " Right so here if you've got peas equals zero extra fully connected layers is empty. This is like the minimum possible", "tokens": [1779, 370, 510, 498, 291, 600, 658, 24494, 6915, 4018, 2857, 4498, 4582, 7914, 307, 6707, 13, 639, 307, 411, 264, 7285, 1944], "temperature": 0.0, "avg_logprob": -0.16491979360580444, "compression_ratio": 1.598326359832636, "no_speech_prob": 3.2887348879739875e-06}, {"id": 211, "seek": 103120, "start": 1052.28, "end": 1057.8400000000001, "text": " Kind of top model we can put on top and again like if we do that", "tokens": [9242, 295, 1192, 2316, 321, 393, 829, 322, 1192, 293, 797, 411, 498, 321, 360, 300], "temperature": 0.0, "avg_logprob": -0.16491979360580444, "compression_ratio": 1.598326359832636, "no_speech_prob": 3.2887348879739875e-06}, {"id": 212, "seek": 105784, "start": 1057.84, "end": 1059.84, "text": " You", "tokens": [509], "temperature": 0.0, "avg_logprob": -0.2505235993460323, "compression_ratio": 1.5225225225225225, "no_speech_prob": 4.63781543658115e-06}, {"id": 213, "seek": 105784, "start": 1060.48, "end": 1063.9599999999998, "text": " Can see above we actually end up with in this case a", "tokens": [1664, 536, 3673, 321, 767, 917, 493, 365, 294, 341, 1389, 257], "temperature": 0.0, "avg_logprob": -0.2505235993460323, "compression_ratio": 1.5225225225225225, "no_speech_prob": 4.63781543658115e-06}, {"id": 214, "seek": 105784, "start": 1064.9599999999998, "end": 1070.4399999999998, "text": " Reasonably good result because we're not training it for very long and this particular pre-trained network", "tokens": [39693, 1188, 665, 1874, 570, 321, 434, 406, 3097, 309, 337, 588, 938, 293, 341, 1729, 659, 12, 17227, 2001, 3209], "temperature": 0.0, "avg_logprob": -0.2505235993460323, "compression_ratio": 1.5225225225225225, "no_speech_prob": 4.63781543658115e-06}, {"id": 215, "seek": 105784, "start": 1070.48, "end": 1074.06, "text": " It's very well suited to this particular problem. Yes, you know", "tokens": [467, 311, 588, 731, 24736, 281, 341, 1729, 1154, 13, 1079, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.2505235993460323, "compression_ratio": 1.5225225225225225, "no_speech_prob": 4.63781543658115e-06}, {"id": 216, "seek": 105784, "start": 1075.36, "end": 1079.0, "text": " So Jeremy what kind of P should we we using?", "tokens": [407, 17809, 437, 733, 295, 430, 820, 321, 321, 1228, 30], "temperature": 0.0, "avg_logprob": -0.2505235993460323, "compression_ratio": 1.5225225225225225, "no_speech_prob": 4.63781543658115e-06}, {"id": 217, "seek": 105784, "start": 1079.76, "end": 1084.1599999999999, "text": " By default so the one that's there by default", "tokens": [3146, 7576, 370, 264, 472, 300, 311, 456, 538, 7576], "temperature": 0.0, "avg_logprob": -0.2505235993460323, "compression_ratio": 1.5225225225225225, "no_speech_prob": 4.63781543658115e-06}, {"id": 218, "seek": 105784, "start": 1084.76, "end": 1086.76, "text": " for the first layer", "tokens": [337, 264, 700, 4583], "temperature": 0.0, "avg_logprob": -0.2505235993460323, "compression_ratio": 1.5225225225225225, "no_speech_prob": 4.63781543658115e-06}, {"id": 219, "seek": 108676, "start": 1086.76, "end": 1090.82, "text": " Is point two five and for the second layer is zero point five", "tokens": [1119, 935, 732, 1732, 293, 337, 264, 1150, 4583, 307, 4018, 935, 1732], "temperature": 0.0, "avg_logprob": -0.17131852236661044, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.2679253131864243e-06}, {"id": 220, "seek": 108676, "start": 1091.92, "end": 1093.92, "text": " That seems to work pretty well", "tokens": [663, 2544, 281, 589, 1238, 731], "temperature": 0.0, "avg_logprob": -0.17131852236661044, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.2679253131864243e-06}, {"id": 221, "seek": 108676, "start": 1094.76, "end": 1099.8, "text": " For most things right so like it's it's it you you don't necessarily need to change it at all", "tokens": [1171, 881, 721, 558, 370, 411, 309, 311, 309, 311, 309, 291, 291, 500, 380, 4725, 643, 281, 1319, 309, 412, 439], "temperature": 0.0, "avg_logprob": -0.17131852236661044, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.2679253131864243e-06}, {"id": 222, "seek": 108676, "start": 1101.2, "end": 1103.2, "text": " Basically if you find it's overfitting", "tokens": [8537, 498, 291, 915, 309, 311, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.17131852236661044, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.2679253131864243e-06}, {"id": 223, "seek": 108676, "start": 1103.44, "end": 1109.94, "text": " Just start bumping it up so try first of all setting it to zero point five that'll set them both to zero point five", "tokens": [1449, 722, 9961, 278, 309, 493, 370, 853, 700, 295, 439, 3287, 309, 281, 4018, 935, 1732, 300, 603, 992, 552, 1293, 281, 4018, 935, 1732], "temperature": 0.0, "avg_logprob": -0.17131852236661044, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.2679253131864243e-06}, {"id": 224, "seek": 108676, "start": 1109.94, "end": 1114.32, "text": " If it's still overfitting a lot try zero point seven like you can you can narrow down", "tokens": [759, 309, 311, 920, 670, 69, 2414, 257, 688, 853, 4018, 935, 3407, 411, 291, 393, 291, 393, 9432, 760], "temperature": 0.0, "avg_logprob": -0.17131852236661044, "compression_ratio": 1.7941176470588236, "no_speech_prob": 1.2679253131864243e-06}, {"id": 225, "seek": 111432, "start": 1114.32, "end": 1116.48, "text": " And like there's not that many", "tokens": [400, 411, 456, 311, 406, 300, 867], "temperature": 0.0, "avg_logprob": -0.19618501334354796, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.994724243559176e-06}, {"id": 226, "seek": 111432, "start": 1117.48, "end": 1118.56, "text": " numbers", "tokens": [3547], "temperature": 0.0, "avg_logprob": -0.19618501334354796, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.994724243559176e-06}, {"id": 227, "seek": 111432, "start": 1118.56, "end": 1120.76, "text": " Change right and if you're under fitting", "tokens": [15060, 558, 293, 498, 291, 434, 833, 15669], "temperature": 0.0, "avg_logprob": -0.19618501334354796, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.994724243559176e-06}, {"id": 228, "seek": 111432, "start": 1122.0, "end": 1124.1599999999999, "text": " Then you can try making it lower", "tokens": [1396, 291, 393, 853, 1455, 309, 3126], "temperature": 0.0, "avg_logprob": -0.19618501334354796, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.994724243559176e-06}, {"id": 229, "seek": 111432, "start": 1125.3999999999999, "end": 1131.58, "text": " It's unlikely you would need to make it much lower because like even in these dogs versus cat situations", "tokens": [467, 311, 17518, 291, 576, 643, 281, 652, 309, 709, 3126, 570, 411, 754, 294, 613, 7197, 5717, 3857, 6851], "temperature": 0.0, "avg_logprob": -0.19618501334354796, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.994724243559176e-06}, {"id": 230, "seek": 111432, "start": 1133.32, "end": 1138.8, "text": " You know we don't seem to have to make it lower, so it's more likely you'd be increasing it to like point six or point seven", "tokens": [509, 458, 321, 500, 380, 1643, 281, 362, 281, 652, 309, 3126, 11, 370, 309, 311, 544, 3700, 291, 1116, 312, 5662, 309, 281, 411, 935, 2309, 420, 935, 3407], "temperature": 0.0, "avg_logprob": -0.19618501334354796, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.994724243559176e-06}, {"id": 231, "seek": 113880, "start": 1138.8, "end": 1145.6599999999999, "text": " But you can fiddle around I find these the ones that are there by default seem to work pretty well most of the time", "tokens": [583, 291, 393, 24553, 2285, 926, 286, 915, 613, 264, 2306, 300, 366, 456, 538, 7576, 1643, 281, 589, 1238, 731, 881, 295, 264, 565], "temperature": 0.0, "avg_logprob": -0.19145822970666618, "compression_ratio": 1.5984555984555984, "no_speech_prob": 4.02944533561822e-06}, {"id": 232, "seek": 113880, "start": 1146.44, "end": 1149.34, "text": " So one place I I actually did increase this", "tokens": [407, 472, 1081, 286, 286, 767, 630, 3488, 341], "temperature": 0.0, "avg_logprob": -0.19145822970666618, "compression_ratio": 1.5984555984555984, "no_speech_prob": 4.02944533561822e-06}, {"id": 233, "seek": 113880, "start": 1150.44, "end": 1154.08, "text": " Was in the dog breeds one I did set them both to point five", "tokens": [3027, 294, 264, 3000, 41609, 472, 286, 630, 992, 552, 1293, 281, 935, 1732], "temperature": 0.0, "avg_logprob": -0.19145822970666618, "compression_ratio": 1.5984555984555984, "no_speech_prob": 4.02944533561822e-06}, {"id": 234, "seek": 113880, "start": 1154.76, "end": 1156.76, "text": " when I used a", "tokens": [562, 286, 1143, 257], "temperature": 0.0, "avg_logprob": -0.19145822970666618, "compression_ratio": 1.5984555984555984, "no_speech_prob": 4.02944533561822e-06}, {"id": 235, "seek": 113880, "start": 1157.0, "end": 1162.68, "text": " Bigger model so like resnet 34 has less parameters, so it doesn't overfit as much", "tokens": [5429, 1321, 2316, 370, 411, 725, 7129, 12790, 575, 1570, 9834, 11, 370, 309, 1177, 380, 670, 6845, 382, 709], "temperature": 0.0, "avg_logprob": -0.19145822970666618, "compression_ratio": 1.5984555984555984, "no_speech_prob": 4.02944533561822e-06}, {"id": 236, "seek": 113880, "start": 1163.2, "end": 1167.76, "text": " Then when I started bumping pumping it up through like a resnet 50 which has a lot more parameters", "tokens": [1396, 562, 286, 1409, 9961, 278, 27131, 309, 493, 807, 411, 257, 725, 7129, 2625, 597, 575, 257, 688, 544, 9834], "temperature": 0.0, "avg_logprob": -0.19145822970666618, "compression_ratio": 1.5984555984555984, "no_speech_prob": 4.02944533561822e-06}, {"id": 237, "seek": 116776, "start": 1167.76, "end": 1172.94, "text": " I noticed it started overfitting so then I also increased my dropout so as you use like", "tokens": [286, 5694, 309, 1409, 670, 69, 2414, 370, 550, 286, 611, 6505, 452, 3270, 346, 370, 382, 291, 764, 411], "temperature": 0.0, "avg_logprob": -0.40342372172587626, "compression_ratio": 1.439153439153439, "no_speech_prob": 1.2218933989061043e-05}, {"id": 238, "seek": 116776, "start": 1173.44, "end": 1179.04, "text": " Bigger models you'll often need to add more dropout. Can you pass that over there, please?", "tokens": [5429, 1321, 5245, 291, 603, 2049, 643, 281, 909, 544, 3270, 346, 13, 1664, 291, 1320, 300, 670, 456, 11, 1767, 30], "temperature": 0.0, "avg_logprob": -0.40342372172587626, "compression_ratio": 1.439153439153439, "no_speech_prob": 1.2218933989061043e-05}, {"id": 239, "seek": 116776, "start": 1182.52, "end": 1187.92, "text": " If we set B to point five roughly what percentage is it 50% 50%", "tokens": [759, 321, 992, 363, 281, 935, 1732, 9810, 437, 9668, 307, 309, 2625, 4, 2625, 4], "temperature": 0.0, "avg_logprob": -0.40342372172587626, "compression_ratio": 1.439153439153439, "no_speech_prob": 1.2218933989061043e-05}, {"id": 240, "seek": 116776, "start": 1189.72, "end": 1191.72, "text": " Was there a few positive back", "tokens": [3027, 456, 257, 1326, 3353, 646], "temperature": 0.0, "avg_logprob": -0.40342372172587626, "compression_ratio": 1.439153439153439, "no_speech_prob": 1.2218933989061043e-05}, {"id": 241, "seek": 119172, "start": 1191.72, "end": 1198.2, "text": " Thanks, is there a particular way in which you can determine if data is being overfitted", "tokens": [2561, 11, 307, 456, 257, 1729, 636, 294, 597, 291, 393, 6997, 498, 1412, 307, 885, 670, 69, 3944], "temperature": 0.0, "avg_logprob": -0.22599905187433417, "compression_ratio": 1.7282051282051283, "no_speech_prob": 4.637839083443396e-06}, {"id": 242, "seek": 119172, "start": 1199.6000000000001, "end": 1201.32, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.22599905187433417, "compression_ratio": 1.7282051282051283, "no_speech_prob": 4.637839083443396e-06}, {"id": 243, "seek": 119172, "start": 1201.32, "end": 1209.8, "text": " You can see that the like here you can see that the training error is a loss is much lower than the validation loss", "tokens": [509, 393, 536, 300, 264, 411, 510, 291, 393, 536, 300, 264, 3097, 6713, 307, 257, 4470, 307, 709, 3126, 813, 264, 24071, 4470], "temperature": 0.0, "avg_logprob": -0.22599905187433417, "compression_ratio": 1.7282051282051283, "no_speech_prob": 4.637839083443396e-06}, {"id": 244, "seek": 119172, "start": 1210.56, "end": 1212.56, "text": " You can't tell if it's like", "tokens": [509, 393, 380, 980, 498, 309, 311, 411], "temperature": 0.0, "avg_logprob": -0.22599905187433417, "compression_ratio": 1.7282051282051283, "no_speech_prob": 4.637839083443396e-06}, {"id": 245, "seek": 119172, "start": 1212.96, "end": 1215.08, "text": " too overfitted like", "tokens": [886, 670, 69, 3944, 411], "temperature": 0.0, "avg_logprob": -0.22599905187433417, "compression_ratio": 1.7282051282051283, "no_speech_prob": 4.637839083443396e-06}, {"id": 246, "seek": 119172, "start": 1215.8, "end": 1219.92, "text": " Zero overfitting is not generally optimal like the only way to find that out is", "tokens": [17182, 670, 69, 2414, 307, 406, 5101, 16252, 411, 264, 787, 636, 281, 915, 300, 484, 307], "temperature": 0.0, "avg_logprob": -0.22599905187433417, "compression_ratio": 1.7282051282051283, "no_speech_prob": 4.637839083443396e-06}, {"id": 247, "seek": 121992, "start": 1219.92, "end": 1224.48, "text": " Remember the only thing you're trying to do is to get this number low right the validation loss number low", "tokens": [5459, 264, 787, 551, 291, 434, 1382, 281, 360, 307, 281, 483, 341, 1230, 2295, 558, 264, 24071, 4470, 1230, 2295], "temperature": 0.0, "avg_logprob": -0.1338650898266864, "compression_ratio": 1.7264573991031391, "no_speech_prob": 3.2887312499951804e-06}, {"id": 248, "seek": 121992, "start": 1224.6000000000001, "end": 1231.1200000000001, "text": " So in the end you kind of have to play around with a few different things and see which thing ends up getting the validation", "tokens": [407, 294, 264, 917, 291, 733, 295, 362, 281, 862, 926, 365, 257, 1326, 819, 721, 293, 536, 597, 551, 5314, 493, 1242, 264, 24071], "temperature": 0.0, "avg_logprob": -0.1338650898266864, "compression_ratio": 1.7264573991031391, "no_speech_prob": 3.2887312499951804e-06}, {"id": 249, "seek": 121992, "start": 1231.1200000000001, "end": 1236.3600000000001, "text": " Loss low, but you're kind of going to feel over time for your particular problem", "tokens": [441, 772, 2295, 11, 457, 291, 434, 733, 295, 516, 281, 841, 670, 565, 337, 428, 1729, 1154], "temperature": 0.0, "avg_logprob": -0.1338650898266864, "compression_ratio": 1.7264573991031391, "no_speech_prob": 3.2887312499951804e-06}, {"id": 250, "seek": 121992, "start": 1236.76, "end": 1240.28, "text": " What does overfitting what does too much overfitting look like?", "tokens": [708, 775, 670, 69, 2414, 437, 775, 886, 709, 670, 69, 2414, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.1338650898266864, "compression_ratio": 1.7264573991031391, "no_speech_prob": 3.2887312499951804e-06}, {"id": 251, "seek": 121992, "start": 1242.88, "end": 1244.88, "text": " Great so", "tokens": [3769, 370], "temperature": 0.0, "avg_logprob": -0.1338650898266864, "compression_ratio": 1.7264573991031391, "no_speech_prob": 3.2887312499951804e-06}, {"id": 252, "seek": 124488, "start": 1244.88, "end": 1250.92, "text": " So that's dropout, and we're going to be using that a lot and remember. It's there by default service here another question", "tokens": [407, 300, 311, 3270, 346, 11, 293, 321, 434, 516, 281, 312, 1228, 300, 257, 688, 293, 1604, 13, 467, 311, 456, 538, 7576, 2643, 510, 1071, 1168], "temperature": 0.0, "avg_logprob": -0.3432177471858199, "compression_ratio": 1.524229074889868, "no_speech_prob": 1.568891639180947e-05}, {"id": 253, "seek": 124488, "start": 1252.48, "end": 1255.5600000000002, "text": " So I have two questions one is", "tokens": [407, 286, 362, 732, 1651, 472, 307], "temperature": 0.0, "avg_logprob": -0.3432177471858199, "compression_ratio": 1.524229074889868, "no_speech_prob": 1.568891639180947e-05}, {"id": 254, "seek": 124488, "start": 1256.2800000000002, "end": 1260.3200000000002, "text": " So when it says the dropout rate is point five", "tokens": [407, 562, 309, 1619, 264, 3270, 346, 3314, 307, 935, 1732], "temperature": 0.0, "avg_logprob": -0.3432177471858199, "compression_ratio": 1.524229074889868, "no_speech_prob": 1.568891639180947e-05}, {"id": 255, "seek": 124488, "start": 1260.88, "end": 1265.5200000000002, "text": " Is does it like you know a delete each cell with a probability of?", "tokens": [1119, 775, 309, 411, 291, 458, 257, 12097, 1184, 2815, 365, 257, 8482, 295, 30], "temperature": 0.0, "avg_logprob": -0.3432177471858199, "compression_ratio": 1.524229074889868, "no_speech_prob": 1.568891639180947e-05}, {"id": 256, "seek": 124488, "start": 1266.48, "end": 1271.2, "text": " 5.5. All right does it just pick 50% randomly. I mean I know both effectively", "tokens": [1025, 13, 20, 13, 1057, 558, 775, 309, 445, 1888, 2625, 4, 16979, 13, 286, 914, 286, 458, 1293, 8659], "temperature": 0.0, "avg_logprob": -0.3432177471858199, "compression_ratio": 1.524229074889868, "no_speech_prob": 1.568891639180947e-05}, {"id": 257, "seek": 127120, "start": 1271.2, "end": 1277.92, "text": " It's the former yep, okay, okay second question is why why does the average activation matter?", "tokens": [467, 311, 264, 5819, 18633, 11, 1392, 11, 1392, 1150, 1168, 307, 983, 983, 775, 264, 4274, 24433, 1871, 30], "temperature": 0.0, "avg_logprob": -0.27182686713434034, "compression_ratio": 1.4683544303797469, "no_speech_prob": 3.785282842727611e-06}, {"id": 258, "seek": 127120, "start": 1278.92, "end": 1281.2, "text": " well it matters because the", "tokens": [731, 309, 7001, 570, 264], "temperature": 0.0, "avg_logprob": -0.27182686713434034, "compression_ratio": 1.4683544303797469, "no_speech_prob": 3.785282842727611e-06}, {"id": 259, "seek": 127120, "start": 1281.8400000000001, "end": 1283.64, "text": " remember if you look at the", "tokens": [1604, 498, 291, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.27182686713434034, "compression_ratio": 1.4683544303797469, "no_speech_prob": 3.785282842727611e-06}, {"id": 260, "seek": 127120, "start": 1283.64, "end": 1286.72, "text": " Excel spreadsheet that the result of", "tokens": [19060, 27733, 300, 264, 1874, 295], "temperature": 0.0, "avg_logprob": -0.27182686713434034, "compression_ratio": 1.4683544303797469, "no_speech_prob": 3.785282842727611e-06}, {"id": 261, "seek": 127120, "start": 1287.76, "end": 1290.0, "text": " this cell for example is", "tokens": [341, 2815, 337, 1365, 307], "temperature": 0.0, "avg_logprob": -0.27182686713434034, "compression_ratio": 1.4683544303797469, "no_speech_prob": 3.785282842727611e-06}, {"id": 262, "seek": 127120, "start": 1290.76, "end": 1292.76, "text": " equal to", "tokens": [2681, 281], "temperature": 0.0, "avg_logprob": -0.27182686713434034, "compression_ratio": 1.4683544303797469, "no_speech_prob": 3.785282842727611e-06}, {"id": 263, "seek": 127120, "start": 1295.88, "end": 1297.88, "text": " These nine", "tokens": [1981, 4949], "temperature": 0.0, "avg_logprob": -0.27182686713434034, "compression_ratio": 1.4683544303797469, "no_speech_prob": 3.785282842727611e-06}, {"id": 264, "seek": 129788, "start": 1297.88, "end": 1301.74, "text": " Multiplied by each of these nine right and add it up", "tokens": [29238, 564, 1091, 538, 1184, 295, 613, 4949, 558, 293, 909, 309, 493], "temperature": 0.0, "avg_logprob": -0.17432718195466915, "compression_ratio": 1.8935361216730038, "no_speech_prob": 2.190774694099673e-06}, {"id": 265, "seek": 129788, "start": 1301.7600000000002, "end": 1308.5600000000002, "text": " So if we deleted half of these then that would also cause this number to half which would cause like", "tokens": [407, 498, 321, 22981, 1922, 295, 613, 550, 300, 576, 611, 3082, 341, 1230, 281, 1922, 597, 576, 3082, 411], "temperature": 0.0, "avg_logprob": -0.17432718195466915, "compression_ratio": 1.8935361216730038, "no_speech_prob": 2.190774694099673e-06}, {"id": 266, "seek": 129788, "start": 1308.96, "end": 1311.64, "text": " Everything else after that to change and so if you change", "tokens": [5471, 1646, 934, 300, 281, 1319, 293, 370, 498, 291, 1319], "temperature": 0.0, "avg_logprob": -0.17432718195466915, "compression_ratio": 1.8935361216730038, "no_speech_prob": 2.190774694099673e-06}, {"id": 267, "seek": 129788, "start": 1312.7600000000002, "end": 1316.6200000000001, "text": " What it means you know like you then you're changing something that used to say like oh", "tokens": [708, 309, 1355, 291, 458, 411, 291, 550, 291, 434, 4473, 746, 300, 1143, 281, 584, 411, 1954], "temperature": 0.0, "avg_logprob": -0.17432718195466915, "compression_ratio": 1.8935361216730038, "no_speech_prob": 2.190774694099673e-06}, {"id": 268, "seek": 129788, "start": 1317.16, "end": 1320.72, "text": " Fluffy ears are fluffy if this is greater than point six now", "tokens": [3235, 14297, 8798, 366, 22778, 498, 341, 307, 5044, 813, 935, 2309, 586], "temperature": 0.0, "avg_logprob": -0.17432718195466915, "compression_ratio": 1.8935361216730038, "no_speech_prob": 2.190774694099673e-06}, {"id": 269, "seek": 129788, "start": 1320.72, "end": 1327.0, "text": " It's only fluffy if it's greater than point three like you're changing the meaning of everything so you the goal here is to delete things", "tokens": [467, 311, 787, 22778, 498, 309, 311, 5044, 813, 935, 1045, 411, 291, 434, 4473, 264, 3620, 295, 1203, 370, 291, 264, 3387, 510, 307, 281, 12097, 721], "temperature": 0.0, "avg_logprob": -0.17432718195466915, "compression_ratio": 1.8935361216730038, "no_speech_prob": 2.190774694099673e-06}, {"id": 270, "seek": 132700, "start": 1327.0, "end": 1329.0, "text": " without changing", "tokens": [1553, 4473], "temperature": 0.0, "avg_logprob": -0.24142936950034283, "compression_ratio": 1.7064220183486238, "no_speech_prob": 8.398016689170618e-06}, {"id": 271, "seek": 132700, "start": 1332.8, "end": 1337.56, "text": " Where are you using a linear activation for one of the earlier activations?", "tokens": [2305, 366, 291, 1228, 257, 8213, 24433, 337, 472, 295, 264, 3071, 2430, 763, 30], "temperature": 0.0, "avg_logprob": -0.24142936950034283, "compression_ratio": 1.7064220183486238, "no_speech_prob": 8.398016689170618e-06}, {"id": 272, "seek": 132700, "start": 1338.2, "end": 1341.48, "text": " Why are we using linear? Yeah? Why that particular activation?", "tokens": [1545, 366, 321, 1228, 8213, 30, 865, 30, 1545, 300, 1729, 24433, 30], "temperature": 0.0, "avg_logprob": -0.24142936950034283, "compression_ratio": 1.7064220183486238, "no_speech_prob": 8.398016689170618e-06}, {"id": 273, "seek": 132700, "start": 1342.08, "end": 1349.2, "text": " Because that's what this set of layers is so we've we've the the pre-trained network is or is the convolutional network and", "tokens": [1436, 300, 311, 437, 341, 992, 295, 7914, 307, 370, 321, 600, 321, 600, 264, 264, 659, 12, 17227, 2001, 3209, 307, 420, 307, 264, 45216, 304, 3209, 293], "temperature": 0.0, "avg_logprob": -0.24142936950034283, "compression_ratio": 1.7064220183486238, "no_speech_prob": 8.398016689170618e-06}, {"id": 274, "seek": 132700, "start": 1349.44, "end": 1356.72, "text": " That's pre computed so we don't see it so what that spits out is a vector so the only choice", "tokens": [663, 311, 659, 40610, 370, 321, 500, 380, 536, 309, 370, 437, 300, 637, 1208, 484, 307, 257, 8062, 370, 264, 787, 3922], "temperature": 0.0, "avg_logprob": -0.24142936950034283, "compression_ratio": 1.7064220183486238, "no_speech_prob": 8.398016689170618e-06}, {"id": 275, "seek": 135672, "start": 1356.72, "end": 1359.6200000000001, "text": " We have is to use linear layers at this point", "tokens": [492, 362, 307, 281, 764, 8213, 7914, 412, 341, 935], "temperature": 0.0, "avg_logprob": -0.23997786550810843, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5215510959242238e-06}, {"id": 276, "seek": 135672, "start": 1360.3600000000001, "end": 1361.76, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.23997786550810843, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5215510959242238e-06}, {"id": 277, "seek": 135672, "start": 1361.76, "end": 1368.76, "text": " Can we have different level of dropout by layer? Yes? Absolutely how to do that great so", "tokens": [1664, 321, 362, 819, 1496, 295, 3270, 346, 538, 4583, 30, 1079, 30, 7021, 577, 281, 360, 300, 869, 370], "temperature": 0.0, "avg_logprob": -0.23997786550810843, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5215510959242238e-06}, {"id": 278, "seek": 135672, "start": 1369.88, "end": 1374.74, "text": " So you can absolutely have different dropout by layer, and that's why this is actually called peas", "tokens": [407, 291, 393, 3122, 362, 819, 3270, 346, 538, 4583, 11, 293, 300, 311, 983, 341, 307, 767, 1219, 24494], "temperature": 0.0, "avg_logprob": -0.23997786550810843, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5215510959242238e-06}, {"id": 279, "seek": 135672, "start": 1375.48, "end": 1378.4, "text": " So you can pass in an array here, so if I went zero", "tokens": [407, 291, 393, 1320, 294, 364, 10225, 510, 11, 370, 498, 286, 1437, 4018], "temperature": 0.0, "avg_logprob": -0.23997786550810843, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5215510959242238e-06}, {"id": 280, "seek": 135672, "start": 1379.16, "end": 1380.48, "text": " comma", "tokens": [22117], "temperature": 0.0, "avg_logprob": -0.23997786550810843, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5215510959242238e-06}, {"id": 281, "seek": 135672, "start": 1380.48, "end": 1385.1200000000001, "text": " 0.2 for example and then extra fully connect to it I might add 512", "tokens": [1958, 13, 17, 337, 1365, 293, 550, 2857, 4498, 1745, 281, 309, 286, 1062, 909, 1025, 4762], "temperature": 0.0, "avg_logprob": -0.23997786550810843, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5215510959242238e-06}, {"id": 282, "seek": 138512, "start": 1385.12, "end": 1389.84, "text": " right then that's going to be zero dropout before the first of them and", "tokens": [558, 550, 300, 311, 516, 281, 312, 4018, 3270, 346, 949, 264, 700, 295, 552, 293], "temperature": 0.0, "avg_logprob": -0.21422307756212022, "compression_ratio": 1.6090909090909091, "no_speech_prob": 6.144143753772369e-06}, {"id": 283, "seek": 138512, "start": 1390.2399999999998, "end": 1392.8, "text": " 0.2 dropout before the second of them", "tokens": [1958, 13, 17, 3270, 346, 949, 264, 1150, 295, 552], "temperature": 0.0, "avg_logprob": -0.21422307756212022, "compression_ratio": 1.6090909090909091, "no_speech_prob": 6.144143753772369e-06}, {"id": 284, "seek": 138512, "start": 1394.84, "end": 1400.62, "text": " And I must admit I don't have a great intuition even after doing this for a few years for like", "tokens": [400, 286, 1633, 9796, 286, 500, 380, 362, 257, 869, 24002, 754, 934, 884, 341, 337, 257, 1326, 924, 337, 411], "temperature": 0.0, "avg_logprob": -0.21422307756212022, "compression_ratio": 1.6090909090909091, "no_speech_prob": 6.144143753772369e-06}, {"id": 285, "seek": 138512, "start": 1402.0, "end": 1406.6399999999999, "text": " When should earlier or later layers have different amounts of dropout?", "tokens": [1133, 820, 3071, 420, 1780, 7914, 362, 819, 11663, 295, 3270, 346, 30], "temperature": 0.0, "avg_logprob": -0.21422307756212022, "compression_ratio": 1.6090909090909091, "no_speech_prob": 6.144143753772369e-06}, {"id": 286, "seek": 138512, "start": 1407.32, "end": 1411.9799999999998, "text": " It's still something I kind of play with and I can't quite find rules of thumb", "tokens": [467, 311, 920, 746, 286, 733, 295, 862, 365, 293, 286, 393, 380, 1596, 915, 4474, 295, 9298], "temperature": 0.0, "avg_logprob": -0.21422307756212022, "compression_ratio": 1.6090909090909091, "no_speech_prob": 6.144143753772369e-06}, {"id": 287, "seek": 141198, "start": 1411.98, "end": 1417.84, "text": " So if some of you come up with some good rules of thumb, I'd love to hear about them. I think if in doubt", "tokens": [407, 498, 512, 295, 291, 808, 493, 365, 512, 665, 4474, 295, 9298, 11, 286, 1116, 959, 281, 1568, 466, 552, 13, 286, 519, 498, 294, 6385], "temperature": 0.0, "avg_logprob": -0.14638614654541016, "compression_ratio": 1.6042553191489362, "no_speech_prob": 4.936943696520757e-06}, {"id": 288, "seek": 141198, "start": 1419.52, "end": 1422.06, "text": " You can use the same dropout in every fully connected layer", "tokens": [509, 393, 764, 264, 912, 3270, 346, 294, 633, 4498, 4582, 4583], "temperature": 0.0, "avg_logprob": -0.14638614654541016, "compression_ratio": 1.6042553191489362, "no_speech_prob": 4.936943696520757e-06}, {"id": 289, "seek": 141198, "start": 1423.0, "end": 1427.24, "text": " The other thing you can try is often people only put dropout on the very last", "tokens": [440, 661, 551, 291, 393, 853, 307, 2049, 561, 787, 829, 3270, 346, 322, 264, 588, 1036], "temperature": 0.0, "avg_logprob": -0.14638614654541016, "compression_ratio": 1.6042553191489362, "no_speech_prob": 4.936943696520757e-06}, {"id": 290, "seek": 141198, "start": 1428.04, "end": 1430.44, "text": " Linear layer, so that'd be the two things to try", "tokens": [14670, 289, 4583, 11, 370, 300, 1116, 312, 264, 732, 721, 281, 853], "temperature": 0.0, "avg_logprob": -0.14638614654541016, "compression_ratio": 1.6042553191489362, "no_speech_prob": 4.936943696520757e-06}, {"id": 291, "seek": 143044, "start": 1430.44, "end": 1438.8400000000001, "text": " So Jeremy, why do you monitor the log loss the loss instead of the accuracy going up?", "tokens": [407, 17809, 11, 983, 360, 291, 6002, 264, 3565, 4470, 264, 4470, 2602, 295, 264, 14170, 516, 493, 30], "temperature": 0.0, "avg_logprob": -0.4123695569160657, "compression_ratio": 1.6084656084656084, "no_speech_prob": 5.682310074917041e-06}, {"id": 292, "seek": 143044, "start": 1439.3200000000002, "end": 1442.88, "text": " Well because the loss is the only thing that we can see", "tokens": [1042, 570, 264, 4470, 307, 264, 787, 551, 300, 321, 393, 536], "temperature": 0.0, "avg_logprob": -0.4123695569160657, "compression_ratio": 1.6084656084656084, "no_speech_prob": 5.682310074917041e-06}, {"id": 293, "seek": 143044, "start": 1445.6000000000001, "end": 1451.48, "text": " For both the validation set and the training set so it's nice to be able to compare them", "tokens": [1171, 1293, 264, 24071, 992, 293, 264, 3097, 992, 370, 309, 311, 1481, 281, 312, 1075, 281, 6794, 552], "temperature": 0.0, "avg_logprob": -0.4123695569160657, "compression_ratio": 1.6084656084656084, "no_speech_prob": 5.682310074917041e-06}, {"id": 294, "seek": 143044, "start": 1452.44, "end": 1454.44, "text": " Also as we'll learn about", "tokens": [2743, 382, 321, 603, 1466, 466], "temperature": 0.0, "avg_logprob": -0.4123695569160657, "compression_ratio": 1.6084656084656084, "no_speech_prob": 5.682310074917041e-06}, {"id": 295, "seek": 143044, "start": 1454.48, "end": 1457.44, "text": " Later the loss is the thing that we're actually", "tokens": [11965, 264, 4470, 307, 264, 551, 300, 321, 434, 767], "temperature": 0.0, "avg_logprob": -0.4123695569160657, "compression_ratio": 1.6084656084656084, "no_speech_prob": 5.682310074917041e-06}, {"id": 296, "seek": 145744, "start": 1457.44, "end": 1460.16, "text": " optimizing so", "tokens": [40425, 370], "temperature": 0.0, "avg_logprob": -0.4726807058674015, "compression_ratio": 1.5885714285714285, "no_speech_prob": 1.6700962078175507e-05}, {"id": 297, "seek": 145744, "start": 1460.68, "end": 1466.4, "text": " It's it's kind of a little more. It's a little easier to monitor that and understand what that means", "tokens": [467, 311, 309, 311, 733, 295, 257, 707, 544, 13, 467, 311, 257, 707, 3571, 281, 6002, 300, 293, 1223, 437, 300, 1355], "temperature": 0.0, "avg_logprob": -0.4726807058674015, "compression_ratio": 1.5885714285714285, "no_speech_prob": 1.6700962078175507e-05}, {"id": 298, "seek": 145744, "start": 1467.88, "end": 1469.88, "text": " Can you pass it over there?", "tokens": [1664, 291, 1320, 309, 670, 456, 30], "temperature": 0.0, "avg_logprob": -0.4726807058674015, "compression_ratio": 1.5885714285714285, "no_speech_prob": 1.6700962078175507e-05}, {"id": 299, "seek": 145744, "start": 1472.04, "end": 1476.96, "text": " So with dropout we are kind of adding some random noise every iteration right so", "tokens": [407, 365, 3270, 346, 321, 366, 733, 295, 5127, 512, 4974, 5658, 633, 24784, 558, 370], "temperature": 0.0, "avg_logprob": -0.4726807058674015, "compression_ratio": 1.5885714285714285, "no_speech_prob": 1.6700962078175507e-05}, {"id": 300, "seek": 145744, "start": 1477.8, "end": 1481.16, "text": " So that means that we don't do as much learning right?", "tokens": [407, 300, 1355, 300, 321, 500, 380, 360, 382, 709, 2539, 558, 30], "temperature": 0.0, "avg_logprob": -0.4726807058674015, "compression_ratio": 1.5885714285714285, "no_speech_prob": 1.6700962078175507e-05}, {"id": 301, "seek": 148116, "start": 1481.16, "end": 1487.8400000000001, "text": " So that's right, so we have to play around with the learning rate and it doesn't seem to impact the learning rate", "tokens": [407, 300, 311, 558, 11, 370, 321, 362, 281, 862, 926, 365, 264, 2539, 3314, 293, 309, 1177, 380, 1643, 281, 2712, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.35161563712106625, "compression_ratio": 1.5511363636363635, "no_speech_prob": 8.267767952929717e-06}, {"id": 302, "seek": 148116, "start": 1488.72, "end": 1490.72, "text": " Enough that I've ever noticed it. I", "tokens": [19401, 300, 286, 600, 1562, 5694, 309, 13, 286], "temperature": 0.0, "avg_logprob": -0.35161563712106625, "compression_ratio": 1.5511363636363635, "no_speech_prob": 8.267767952929717e-06}, {"id": 303, "seek": 148116, "start": 1491.28, "end": 1496.44, "text": " Would say you're probably right in theory it might but not enough that it's ever affected me", "tokens": [6068, 584, 291, 434, 1391, 558, 294, 5261, 309, 1062, 457, 406, 1547, 300, 309, 311, 1562, 8028, 385], "temperature": 0.0, "avg_logprob": -0.35161563712106625, "compression_ratio": 1.5511363636363635, "no_speech_prob": 8.267767952929717e-06}, {"id": 304, "seek": 148116, "start": 1501.1200000000001, "end": 1504.8400000000001, "text": " Okay, so let's talk about this", "tokens": [1033, 11, 370, 718, 311, 751, 466, 341], "temperature": 0.0, "avg_logprob": -0.35161563712106625, "compression_ratio": 1.5511363636363635, "no_speech_prob": 8.267767952929717e-06}, {"id": 305, "seek": 150484, "start": 1504.84, "end": 1512.6799999999998, "text": " So let's talk about this structured data problem and so to remind you we were looking at", "tokens": [407, 718, 311, 751, 466, 341, 18519, 1412, 1154, 293, 370, 281, 4160, 291, 321, 645, 1237, 412], "temperature": 0.0, "avg_logprob": -0.2503278708156151, "compression_ratio": 1.51, "no_speech_prob": 6.33912668490666e-06}, {"id": 306, "seek": 150484, "start": 1513.12, "end": 1516.32, "text": " Kaggle's Rossman competition, which is a German", "tokens": [48751, 22631, 311, 16140, 1601, 6211, 11, 597, 307, 257, 6521], "temperature": 0.0, "avg_logprob": -0.2503278708156151, "compression_ratio": 1.51, "no_speech_prob": 6.33912668490666e-06}, {"id": 307, "seek": 150484, "start": 1517.52, "end": 1519.8799999999999, "text": " Chain of supermarkets, I believe", "tokens": [33252, 295, 1687, 48850, 11, 286, 1697], "temperature": 0.0, "avg_logprob": -0.2503278708156151, "compression_ratio": 1.51, "no_speech_prob": 6.33912668490666e-06}, {"id": 308, "seek": 150484, "start": 1520.52, "end": 1523.52, "text": " And you can find this in less than three Rossman", "tokens": [400, 291, 393, 915, 341, 294, 1570, 813, 1045, 16140, 1601], "temperature": 0.0, "avg_logprob": -0.2503278708156151, "compression_ratio": 1.51, "no_speech_prob": 6.33912668490666e-06}, {"id": 309, "seek": 150484, "start": 1524.3999999999999, "end": 1526.3999999999999, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2503278708156151, "compression_ratio": 1.51, "no_speech_prob": 6.33912668490666e-06}, {"id": 310, "seek": 150484, "start": 1526.8, "end": 1533.08, "text": " The main data set is the one where we were looking to say at a particular store", "tokens": [440, 2135, 1412, 992, 307, 264, 472, 689, 321, 645, 1237, 281, 584, 412, 257, 1729, 3531], "temperature": 0.0, "avg_logprob": -0.2503278708156151, "compression_ratio": 1.51, "no_speech_prob": 6.33912668490666e-06}, {"id": 311, "seek": 153308, "start": 1533.08, "end": 1535.08, "text": " How much did they sell?", "tokens": [1012, 709, 630, 436, 3607, 30], "temperature": 0.0, "avg_logprob": -0.20095743684687167, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.495149369176943e-06}, {"id": 312, "seek": 153308, "start": 1536.1999999999998, "end": 1542.28, "text": " Okay, and there's a few big key pieces of information one is what was the date another was were they open?", "tokens": [1033, 11, 293, 456, 311, 257, 1326, 955, 2141, 3755, 295, 1589, 472, 307, 437, 390, 264, 4002, 1071, 390, 645, 436, 1269, 30], "temperature": 0.0, "avg_logprob": -0.20095743684687167, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.495149369176943e-06}, {"id": 313, "seek": 153308, "start": 1542.8799999999999, "end": 1544.8799999999999, "text": " Did they have a promotion on?", "tokens": [2589, 436, 362, 257, 15783, 322, 30], "temperature": 0.0, "avg_logprob": -0.20095743684687167, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.495149369176943e-06}, {"id": 314, "seek": 153308, "start": 1545.4399999999998, "end": 1548.8799999999999, "text": " Was it a holiday in that state and was it a holiday?", "tokens": [3027, 309, 257, 9960, 294, 300, 1785, 293, 390, 309, 257, 9960, 30], "temperature": 0.0, "avg_logprob": -0.20095743684687167, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.495149369176943e-06}, {"id": 315, "seek": 153308, "start": 1549.52, "end": 1553.08, "text": " For school a state holiday there or was it a school holiday there?", "tokens": [1171, 1395, 257, 1785, 9960, 456, 420, 390, 309, 257, 1395, 9960, 456, 30], "temperature": 0.0, "avg_logprob": -0.20095743684687167, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.495149369176943e-06}, {"id": 316, "seek": 153308, "start": 1553.3999999999999, "end": 1557.24, "text": " And then we had some more information about stores like what for this store", "tokens": [400, 550, 321, 632, 512, 544, 1589, 466, 9512, 411, 437, 337, 341, 3531], "temperature": 0.0, "avg_logprob": -0.20095743684687167, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.495149369176943e-06}, {"id": 317, "seek": 155724, "start": 1557.24, "end": 1563.28, "text": " What kind of stuff did they tend to sell what kind of store are they how far away the competition and so forth so?", "tokens": [708, 733, 295, 1507, 630, 436, 3928, 281, 3607, 437, 733, 295, 3531, 366, 436, 577, 1400, 1314, 264, 6211, 293, 370, 5220, 370, 30], "temperature": 0.0, "avg_logprob": -0.1505385610792372, "compression_ratio": 1.6390243902439023, "no_speech_prob": 1.1015933978342218e-06}, {"id": 318, "seek": 155724, "start": 1564.0, "end": 1570.0, "text": " With a data set like this. There's really two main kinds of column. There's columns that we think of as", "tokens": [2022, 257, 1412, 992, 411, 341, 13, 821, 311, 534, 732, 2135, 3685, 295, 7738, 13, 821, 311, 13766, 300, 321, 519, 295, 382], "temperature": 0.0, "avg_logprob": -0.1505385610792372, "compression_ratio": 1.6390243902439023, "no_speech_prob": 1.1015933978342218e-06}, {"id": 319, "seek": 155724, "start": 1570.64, "end": 1574.04, "text": " Categorical they have a number of levels. So the assortment", "tokens": [383, 2968, 284, 804, 436, 362, 257, 1230, 295, 4358, 13, 407, 264, 1256, 477, 518], "temperature": 0.0, "avg_logprob": -0.1505385610792372, "compression_ratio": 1.6390243902439023, "no_speech_prob": 1.1015933978342218e-06}, {"id": 320, "seek": 155724, "start": 1574.64, "end": 1579.24, "text": " Column is categorical and it has levels such as a B and C", "tokens": [4004, 16449, 307, 19250, 804, 293, 309, 575, 4358, 1270, 382, 257, 363, 293, 383], "temperature": 0.0, "avg_logprob": -0.1505385610792372, "compression_ratio": 1.6390243902439023, "no_speech_prob": 1.1015933978342218e-06}, {"id": 321, "seek": 157924, "start": 1579.24, "end": 1585.4, "text": " Where else something like competition distance we would call continuous", "tokens": [2305, 1646, 746, 411, 6211, 4560, 321, 576, 818, 10957], "temperature": 0.0, "avg_logprob": -0.17159253358840942, "compression_ratio": 1.6487455197132617, "no_speech_prob": 4.356864792498527e-06}, {"id": 322, "seek": 157924, "start": 1585.4, "end": 1592.84, "text": " It has a number attached to work where differences or ratios even of that number have some kind of meaning and so we need to", "tokens": [467, 575, 257, 1230, 8570, 281, 589, 689, 7300, 420, 32435, 754, 295, 300, 1230, 362, 512, 733, 295, 3620, 293, 370, 321, 643, 281], "temperature": 0.0, "avg_logprob": -0.17159253358840942, "compression_ratio": 1.6487455197132617, "no_speech_prob": 4.356864792498527e-06}, {"id": 323, "seek": 157924, "start": 1592.84, "end": 1598.28, "text": " Deal with these two things quite differently. Okay, so anybody who's done any", "tokens": [27227, 365, 613, 732, 721, 1596, 7614, 13, 1033, 11, 370, 4472, 567, 311, 1096, 604], "temperature": 0.0, "avg_logprob": -0.17159253358840942, "compression_ratio": 1.6487455197132617, "no_speech_prob": 4.356864792498527e-06}, {"id": 324, "seek": 157924, "start": 1599.28, "end": 1603.2, "text": " Machine learning of any kind will be familiar with using continuous columns", "tokens": [22155, 2539, 295, 604, 733, 486, 312, 4963, 365, 1228, 10957, 13766], "temperature": 0.0, "avg_logprob": -0.17159253358840942, "compression_ratio": 1.6487455197132617, "no_speech_prob": 4.356864792498527e-06}, {"id": 325, "seek": 157924, "start": 1603.2, "end": 1608.68, "text": " If you've done any linear regression, for example, you can just like multiply them by parameters for instance", "tokens": [759, 291, 600, 1096, 604, 8213, 24590, 11, 337, 1365, 11, 291, 393, 445, 411, 12972, 552, 538, 9834, 337, 5197], "temperature": 0.0, "avg_logprob": -0.17159253358840942, "compression_ratio": 1.6487455197132617, "no_speech_prob": 4.356864792498527e-06}, {"id": 326, "seek": 160868, "start": 1608.68, "end": 1612.48, "text": " Categorical columns we're going to have to think about a little bit more", "tokens": [383, 2968, 284, 804, 13766, 321, 434, 516, 281, 362, 281, 519, 466, 257, 707, 857, 544], "temperature": 0.0, "avg_logprob": -0.1734050257822101, "compression_ratio": 1.8072916666666667, "no_speech_prob": 1.4285352335718926e-05}, {"id": 327, "seek": 160868, "start": 1614.3600000000001, "end": 1620.28, "text": " We're not going to go through the data cleaning we're going to assume that that's and feature engineering we're going to assume all that's been done", "tokens": [492, 434, 406, 516, 281, 352, 807, 264, 1412, 8924, 321, 434, 516, 281, 6552, 300, 300, 311, 293, 4111, 7043, 321, 434, 516, 281, 6552, 439, 300, 311, 668, 1096], "temperature": 0.0, "avg_logprob": -0.1734050257822101, "compression_ratio": 1.8072916666666667, "no_speech_prob": 1.4285352335718926e-05}, {"id": 328, "seek": 160868, "start": 1622.28, "end": 1624.28, "text": " And so basically", "tokens": [400, 370, 1936], "temperature": 0.0, "avg_logprob": -0.1734050257822101, "compression_ratio": 1.8072916666666667, "no_speech_prob": 1.4285352335718926e-05}, {"id": 329, "seek": 160868, "start": 1624.3200000000002, "end": 1630.0, "text": " at the end of that we have a list of columns and the in this case I", "tokens": [412, 264, 917, 295, 300, 321, 362, 257, 1329, 295, 13766, 293, 264, 294, 341, 1389, 286], "temperature": 0.0, "avg_logprob": -0.1734050257822101, "compression_ratio": 1.8072916666666667, "no_speech_prob": 1.4285352335718926e-05}, {"id": 330, "seek": 160868, "start": 1631.28, "end": 1634.26, "text": " didn't do any of the thinking around the", "tokens": [994, 380, 360, 604, 295, 264, 1953, 926, 264], "temperature": 0.0, "avg_logprob": -0.1734050257822101, "compression_ratio": 1.8072916666666667, "no_speech_prob": 1.4285352335718926e-05}, {"id": 331, "seek": 163426, "start": 1634.26, "end": 1641.02, "text": " Feature engineering or data cleaning myself. This is all directly from the third place winners of this competition", "tokens": [3697, 1503, 7043, 420, 1412, 8924, 2059, 13, 639, 307, 439, 3838, 490, 264, 2636, 1081, 17193, 295, 341, 6211], "temperature": 0.0, "avg_logprob": -0.2183613945456112, "compression_ratio": 1.5458715596330275, "no_speech_prob": 2.9480038392648567e-06}, {"id": 332, "seek": 163426, "start": 1642.1, "end": 1644.94, "text": " And so they came up with all of these different", "tokens": [400, 370, 436, 1361, 493, 365, 439, 295, 613, 819], "temperature": 0.0, "avg_logprob": -0.2183613945456112, "compression_ratio": 1.5458715596330275, "no_speech_prob": 2.9480038392648567e-06}, {"id": 333, "seek": 163426, "start": 1646.78, "end": 1648.78, "text": " Columns that they found useful", "tokens": [4004, 449, 3695, 300, 436, 1352, 4420], "temperature": 0.0, "avg_logprob": -0.2183613945456112, "compression_ratio": 1.5458715596330275, "no_speech_prob": 2.9480038392648567e-06}, {"id": 334, "seek": 163426, "start": 1648.78, "end": 1650.66, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.2183613945456112, "compression_ratio": 1.5458715596330275, "no_speech_prob": 2.9480038392648567e-06}, {"id": 335, "seek": 163426, "start": 1650.66, "end": 1655.5, "text": " You'll notice the list here is a list of the things that we're going to treat as", "tokens": [509, 603, 3449, 264, 1329, 510, 307, 257, 1329, 295, 264, 721, 300, 321, 434, 516, 281, 2387, 382], "temperature": 0.0, "avg_logprob": -0.2183613945456112, "compression_ratio": 1.5458715596330275, "no_speech_prob": 2.9480038392648567e-06}, {"id": 336, "seek": 163426, "start": 1656.14, "end": 1658.14, "text": " categorical variables", "tokens": [19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.2183613945456112, "compression_ratio": 1.5458715596330275, "no_speech_prob": 2.9480038392648567e-06}, {"id": 337, "seek": 163426, "start": 1659.34, "end": 1662.5, "text": " Numbers like year a month and day", "tokens": [22592, 1616, 411, 1064, 257, 1618, 293, 786], "temperature": 0.0, "avg_logprob": -0.2183613945456112, "compression_ratio": 1.5458715596330275, "no_speech_prob": 2.9480038392648567e-06}, {"id": 338, "seek": 166250, "start": 1662.5, "end": 1664.1, "text": " although", "tokens": [4878], "temperature": 0.0, "avg_logprob": -0.223507077506419, "compression_ratio": 1.6317991631799162, "no_speech_prob": 1.1189412134626764e-06}, {"id": 339, "seek": 166250, "start": 1664.1, "end": 1671.22, "text": " We could treat them as continuous like they the diff, you know differences between 2000 and 2003 is meaningful", "tokens": [492, 727, 2387, 552, 382, 10957, 411, 436, 264, 7593, 11, 291, 458, 7300, 1296, 8132, 293, 16416, 307, 10995], "temperature": 0.0, "avg_logprob": -0.223507077506419, "compression_ratio": 1.6317991631799162, "no_speech_prob": 1.1189412134626764e-06}, {"id": 340, "seek": 166250, "start": 1671.68, "end": 1675.12, "text": " We don't have to right and you'll see shortly how", "tokens": [492, 500, 380, 362, 281, 558, 293, 291, 603, 536, 13392, 577], "temperature": 0.0, "avg_logprob": -0.223507077506419, "compression_ratio": 1.6317991631799162, "no_speech_prob": 1.1189412134626764e-06}, {"id": 341, "seek": 166250, "start": 1676.26, "end": 1678.26, "text": " how categorical", "tokens": [577, 19250, 804], "temperature": 0.0, "avg_logprob": -0.223507077506419, "compression_ratio": 1.6317991631799162, "no_speech_prob": 1.1189412134626764e-06}, {"id": 342, "seek": 166250, "start": 1679.86, "end": 1684.54, "text": " Variables are treated but basically if we decide to make something a categorical variable", "tokens": [32511, 2965, 366, 8668, 457, 1936, 498, 321, 4536, 281, 652, 746, 257, 19250, 804, 7006], "temperature": 0.0, "avg_logprob": -0.223507077506419, "compression_ratio": 1.6317991631799162, "no_speech_prob": 1.1189412134626764e-06}, {"id": 343, "seek": 168454, "start": 1684.54, "end": 1692.54, "text": " What we're telling our neural net down the track is that for every different level of say year, you know 2000 2001 2002", "tokens": [708, 321, 434, 3585, 527, 18161, 2533, 760, 264, 2837, 307, 300, 337, 633, 819, 1496, 295, 584, 1064, 11, 291, 458, 8132, 16382, 17822], "temperature": 0.0, "avg_logprob": -0.1737119501287287, "compression_ratio": 1.69140625, "no_speech_prob": 2.5759524646673526e-07}, {"id": 344, "seek": 168454, "start": 1693.34, "end": 1695.34, "text": " You can treat it totally differently", "tokens": [509, 393, 2387, 309, 3879, 7614], "temperature": 0.0, "avg_logprob": -0.1737119501287287, "compression_ratio": 1.69140625, "no_speech_prob": 2.5759524646673526e-07}, {"id": 345, "seek": 168454, "start": 1695.8999999999999, "end": 1702.1399999999999, "text": " Whereas if we say it's continuous, it's going to have to come up with some kind of like function some kind of smooth ish", "tokens": [13813, 498, 321, 584, 309, 311, 10957, 11, 309, 311, 516, 281, 362, 281, 808, 493, 365, 512, 733, 295, 411, 2445, 512, 733, 295, 5508, 307, 71], "temperature": 0.0, "avg_logprob": -0.1737119501287287, "compression_ratio": 1.69140625, "no_speech_prob": 2.5759524646673526e-07}, {"id": 346, "seek": 168454, "start": 1702.74, "end": 1709.1399999999999, "text": " Function right and so often even for things like year that actually are continuous", "tokens": [11166, 882, 558, 293, 370, 2049, 754, 337, 721, 411, 1064, 300, 767, 366, 10957], "temperature": 0.0, "avg_logprob": -0.1737119501287287, "compression_ratio": 1.69140625, "no_speech_prob": 2.5759524646673526e-07}, {"id": 347, "seek": 170914, "start": 1709.14, "end": 1715.8600000000001, "text": " But they don't actually have many distinct levels. It often works better to treat it as categorical", "tokens": [583, 436, 500, 380, 767, 362, 867, 10644, 4358, 13, 467, 2049, 1985, 1101, 281, 2387, 309, 382, 19250, 804], "temperature": 0.0, "avg_logprob": -0.1901960701778017, "compression_ratio": 1.5550660792951543, "no_speech_prob": 5.368741540223709e-07}, {"id": 348, "seek": 170914, "start": 1716.42, "end": 1722.1000000000001, "text": " So another good example day of week, right? So like day of week between 0 and 6", "tokens": [407, 1071, 665, 1365, 786, 295, 1243, 11, 558, 30, 407, 411, 786, 295, 1243, 1296, 1958, 293, 1386], "temperature": 0.0, "avg_logprob": -0.1901960701778017, "compression_ratio": 1.5550660792951543, "no_speech_prob": 5.368741540223709e-07}, {"id": 349, "seek": 170914, "start": 1723.42, "end": 1730.7800000000002, "text": " It's a number and it means something like the difference between 3 and 5 is 2 days and has meaning but if you think about like how would", "tokens": [467, 311, 257, 1230, 293, 309, 1355, 746, 411, 264, 2649, 1296, 805, 293, 1025, 307, 568, 1708, 293, 575, 3620, 457, 498, 291, 519, 466, 411, 577, 576], "temperature": 0.0, "avg_logprob": -0.1901960701778017, "compression_ratio": 1.5550660792951543, "no_speech_prob": 5.368741540223709e-07}, {"id": 350, "seek": 170914, "start": 1731.8200000000002, "end": 1734.3600000000001, "text": " Sales in a store vary by day of week", "tokens": [23467, 294, 257, 3531, 10559, 538, 786, 295, 1243], "temperature": 0.0, "avg_logprob": -0.1901960701778017, "compression_ratio": 1.5550660792951543, "no_speech_prob": 5.368741540223709e-07}, {"id": 351, "seek": 173436, "start": 1734.36, "end": 1740.8799999999999, "text": " It could well be that like, you know Saturdays and Sundays are over here and Fridays are over here and Wednesdays over here", "tokens": [467, 727, 731, 312, 300, 411, 11, 291, 458, 8803, 82, 293, 44857, 366, 670, 510, 293, 46306, 366, 670, 510, 293, 10579, 82, 670, 510], "temperature": 0.0, "avg_logprob": -0.15870323181152343, "compression_ratio": 1.7132075471698114, "no_speech_prob": 7.338202863138577e-07}, {"id": 352, "seek": 173436, "start": 1740.8799999999999, "end": 1743.04, "text": " Like each day is going to behave", "tokens": [1743, 1184, 786, 307, 516, 281, 15158], "temperature": 0.0, "avg_logprob": -0.15870323181152343, "compression_ratio": 1.7132075471698114, "no_speech_prob": 7.338202863138577e-07}, {"id": 353, "seek": 173436, "start": 1743.7199999999998, "end": 1746.3999999999999, "text": " Kind of qualitatively differently, right?", "tokens": [9242, 295, 31312, 356, 7614, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15870323181152343, "compression_ratio": 1.7132075471698114, "no_speech_prob": 7.338202863138577e-07}, {"id": 354, "seek": 173436, "start": 1746.3999999999999, "end": 1753.04, "text": " So by saying this is the categorical variable as you'll see we're going to let the neural net do that", "tokens": [407, 538, 1566, 341, 307, 264, 19250, 804, 7006, 382, 291, 603, 536, 321, 434, 516, 281, 718, 264, 18161, 2533, 360, 300], "temperature": 0.0, "avg_logprob": -0.15870323181152343, "compression_ratio": 1.7132075471698114, "no_speech_prob": 7.338202863138577e-07}, {"id": 355, "seek": 173436, "start": 1753.04, "end": 1755.9799999999998, "text": " Right. So this thing where we get where we say", "tokens": [1779, 13, 407, 341, 551, 689, 321, 483, 689, 321, 584], "temperature": 0.0, "avg_logprob": -0.15870323181152343, "compression_ratio": 1.7132075471698114, "no_speech_prob": 7.338202863138577e-07}, {"id": 356, "seek": 173436, "start": 1756.52, "end": 1762.6, "text": " Which are continuous and which are categorical to some extent? This is a modeling decision you get to make", "tokens": [3013, 366, 10957, 293, 597, 366, 19250, 804, 281, 512, 8396, 30, 639, 307, 257, 15983, 3537, 291, 483, 281, 652], "temperature": 0.0, "avg_logprob": -0.15870323181152343, "compression_ratio": 1.7132075471698114, "no_speech_prob": 7.338202863138577e-07}, {"id": 357, "seek": 176260, "start": 1762.6, "end": 1765.48, "text": " now if something is", "tokens": [586, 498, 746, 307], "temperature": 0.0, "avg_logprob": -0.2280369749163637, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.0904524262732593e-06}, {"id": 358, "seek": 176260, "start": 1766.1599999999999, "end": 1772.8799999999999, "text": " Coded in your data is like a B and C or you know, Jeremy and your net or whatever", "tokens": [383, 12340, 294, 428, 1412, 307, 411, 257, 363, 293, 383, 420, 291, 458, 11, 17809, 293, 428, 2533, 420, 2035], "temperature": 0.0, "avg_logprob": -0.2280369749163637, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.0904524262732593e-06}, {"id": 359, "seek": 176260, "start": 1773.1999999999998, "end": 1776.3999999999999, "text": " You actually you're going to have to call that categorical, right?", "tokens": [509, 767, 291, 434, 516, 281, 362, 281, 818, 300, 19250, 804, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2280369749163637, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.0904524262732593e-06}, {"id": 360, "seek": 176260, "start": 1776.3999999999999, "end": 1784.36, "text": " There's no way to treat that directly as a continuous variable on the other hand if it starts out as a continuous variable like age", "tokens": [821, 311, 572, 636, 281, 2387, 300, 3838, 382, 257, 10957, 7006, 322, 264, 661, 1011, 498, 309, 3719, 484, 382, 257, 10957, 7006, 411, 3205], "temperature": 0.0, "avg_logprob": -0.2280369749163637, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.0904524262732593e-06}, {"id": 361, "seek": 176260, "start": 1784.48, "end": 1786.48, "text": " or day of week", "tokens": [420, 786, 295, 1243], "temperature": 0.0, "avg_logprob": -0.2280369749163637, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.0904524262732593e-06}, {"id": 362, "seek": 176260, "start": 1786.48, "end": 1788.28, "text": " You get to decide", "tokens": [509, 483, 281, 4536], "temperature": 0.0, "avg_logprob": -0.2280369749163637, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.0904524262732593e-06}, {"id": 363, "seek": 176260, "start": 1788.28, "end": 1790.4399999999998, "text": " Whether you want to treat it as continuous or categorical", "tokens": [8503, 291, 528, 281, 2387, 309, 382, 10957, 420, 19250, 804], "temperature": 0.0, "avg_logprob": -0.2280369749163637, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.0904524262732593e-06}, {"id": 364, "seek": 179044, "start": 1790.44, "end": 1794.0800000000002, "text": " Categorical okay, so summarize if it's categorical in the data", "tokens": [383, 2968, 284, 804, 1392, 11, 370, 20858, 498, 309, 311, 19250, 804, 294, 264, 1412], "temperature": 0.0, "avg_logprob": -0.15487430331943272, "compression_ratio": 2.031111111111111, "no_speech_prob": 1.0348513796998304e-06}, {"id": 365, "seek": 179044, "start": 1794.2, "end": 1798.24, "text": " It's going to have to be categorical in the model if it's continuous in the data", "tokens": [467, 311, 516, 281, 362, 281, 312, 19250, 804, 294, 264, 2316, 498, 309, 311, 10957, 294, 264, 1412], "temperature": 0.0, "avg_logprob": -0.15487430331943272, "compression_ratio": 2.031111111111111, "no_speech_prob": 1.0348513796998304e-06}, {"id": 366, "seek": 179044, "start": 1798.52, "end": 1802.68, "text": " You get to pick whether to make it continuous or categorical in the model", "tokens": [509, 483, 281, 1888, 1968, 281, 652, 309, 10957, 420, 19250, 804, 294, 264, 2316], "temperature": 0.0, "avg_logprob": -0.15487430331943272, "compression_ratio": 2.031111111111111, "no_speech_prob": 1.0348513796998304e-06}, {"id": 367, "seek": 179044, "start": 1804.0, "end": 1809.42, "text": " So in this case again, what I just did whatever the third place winners of this competition did", "tokens": [407, 294, 341, 1389, 797, 11, 437, 286, 445, 630, 2035, 264, 2636, 1081, 17193, 295, 341, 6211, 630], "temperature": 0.0, "avg_logprob": -0.15487430331943272, "compression_ratio": 2.031111111111111, "no_speech_prob": 1.0348513796998304e-06}, {"id": 368, "seek": 179044, "start": 1809.42, "end": 1812.28, "text": " These are the ones that they decided to use as categorical", "tokens": [1981, 366, 264, 2306, 300, 436, 3047, 281, 764, 382, 19250, 804], "temperature": 0.0, "avg_logprob": -0.15487430331943272, "compression_ratio": 2.031111111111111, "no_speech_prob": 1.0348513796998304e-06}, {"id": 369, "seek": 179044, "start": 1812.28, "end": 1816.44, "text": " These were the ones they decided to use as continuous and you can see that basically", "tokens": [1981, 645, 264, 2306, 436, 3047, 281, 764, 382, 10957, 293, 291, 393, 536, 300, 1936], "temperature": 0.0, "avg_logprob": -0.15487430331943272, "compression_ratio": 2.031111111111111, "no_speech_prob": 1.0348513796998304e-06}, {"id": 370, "seek": 181644, "start": 1816.44, "end": 1824.44, "text": " The continuous ones are all of the ones which are actual floating point numbers like competition", "tokens": [440, 10957, 2306, 366, 439, 295, 264, 2306, 597, 366, 3539, 12607, 935, 3547, 411, 6211], "temperature": 0.0, "avg_logprob": -0.19388294219970703, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.422414564141945e-07}, {"id": 371, "seek": 181644, "start": 1824.68, "end": 1830.0800000000002, "text": " Distance actually has a decimal place to it right and temperature actually has a decimal place to it", "tokens": [9840, 719, 767, 575, 257, 26601, 1081, 281, 309, 558, 293, 4292, 767, 575, 257, 26601, 1081, 281, 309], "temperature": 0.0, "avg_logprob": -0.19388294219970703, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.422414564141945e-07}, {"id": 372, "seek": 181644, "start": 1830.0800000000002, "end": 1832.0800000000002, "text": " So these would be very hard to make", "tokens": [407, 613, 576, 312, 588, 1152, 281, 652], "temperature": 0.0, "avg_logprob": -0.19388294219970703, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.422414564141945e-07}, {"id": 373, "seek": 181644, "start": 1832.56, "end": 1835.56, "text": " Categorical because they have many many levels, right?", "tokens": [383, 2968, 284, 804, 570, 436, 362, 867, 867, 4358, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19388294219970703, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.422414564141945e-07}, {"id": 374, "seek": 181644, "start": 1836.0, "end": 1841.8400000000001, "text": " Like if it's like five digits of floating point then potentially there will be as many levels as there are", "tokens": [1743, 498, 309, 311, 411, 1732, 27011, 295, 12607, 935, 550, 7263, 456, 486, 312, 382, 867, 4358, 382, 456, 366], "temperature": 0.0, "avg_logprob": -0.19388294219970703, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.422414564141945e-07}, {"id": 375, "seek": 181644, "start": 1843.16, "end": 1845.16, "text": " as there are rows and", "tokens": [382, 456, 366, 13241, 293], "temperature": 0.0, "avg_logprob": -0.19388294219970703, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.422414564141945e-07}, {"id": 376, "seek": 184516, "start": 1845.16, "end": 1851.02, "text": " And by the way the word we use to say how many levels are in a category we use the word cardinality", "tokens": [400, 538, 264, 636, 264, 1349, 321, 764, 281, 584, 577, 867, 4358, 366, 294, 257, 7719, 321, 764, 264, 1349, 2920, 259, 1860], "temperature": 0.0, "avg_logprob": -0.22368380363951337, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.785071268997854e-06}, {"id": 377, "seek": 184516, "start": 1851.02, "end": 1855.24, "text": " Right so if you hear me say cardinality for example the cardinality of the day of week", "tokens": [1779, 370, 498, 291, 1568, 385, 584, 2920, 259, 1860, 337, 1365, 264, 2920, 259, 1860, 295, 264, 786, 295, 1243], "temperature": 0.0, "avg_logprob": -0.22368380363951337, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.785071268997854e-06}, {"id": 378, "seek": 184516, "start": 1855.64, "end": 1858.52, "text": " Variable is seven because there are seven different days of the week", "tokens": [32511, 712, 307, 3407, 570, 456, 366, 3407, 819, 1708, 295, 264, 1243], "temperature": 0.0, "avg_logprob": -0.22368380363951337, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.785071268997854e-06}, {"id": 379, "seek": 184516, "start": 1862.5600000000002, "end": 1868.92, "text": " Do you have a heuristic for one to bin continuous variables or do you ever been variables I don't ever been continuous variables", "tokens": [1144, 291, 362, 257, 415, 374, 3142, 337, 472, 281, 5171, 10957, 9102, 420, 360, 291, 1562, 668, 9102, 286, 500, 380, 1562, 668, 10957, 9102], "temperature": 0.0, "avg_logprob": -0.22368380363951337, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.785071268997854e-06}, {"id": 380, "seek": 186892, "start": 1868.92, "end": 1872.92, "text": " So yeah", "tokens": [407, 1338], "temperature": 0.0, "avg_logprob": -0.22399363310440726, "compression_ratio": 1.6553191489361703, "no_speech_prob": 2.225273192379973e-06}, {"id": 381, "seek": 186892, "start": 1872.92, "end": 1881.0, "text": " So one thing we could do with like max temperature is group it into not 10 10 to 20 20 to 30 and then call that categorical", "tokens": [407, 472, 551, 321, 727, 360, 365, 411, 11469, 4292, 307, 1594, 309, 666, 406, 1266, 1266, 281, 945, 945, 281, 2217, 293, 550, 818, 300, 19250, 804], "temperature": 0.0, "avg_logprob": -0.22399363310440726, "compression_ratio": 1.6553191489361703, "no_speech_prob": 2.225273192379973e-06}, {"id": 382, "seek": 186892, "start": 1882.0, "end": 1885.48, "text": " interestingly a paper just came out last week in which a", "tokens": [25873, 257, 3035, 445, 1361, 484, 1036, 1243, 294, 597, 257], "temperature": 0.0, "avg_logprob": -0.22399363310440726, "compression_ratio": 1.6553191489361703, "no_speech_prob": 2.225273192379973e-06}, {"id": 383, "seek": 186892, "start": 1886.48, "end": 1890.92, "text": " group of researchers found that sometimes bidding can be helpful but", "tokens": [1594, 295, 10309, 1352, 300, 2171, 39702, 393, 312, 4961, 457], "temperature": 0.0, "avg_logprob": -0.22399363310440726, "compression_ratio": 1.6553191489361703, "no_speech_prob": 2.225273192379973e-06}, {"id": 384, "seek": 186892, "start": 1891.52, "end": 1897.2, "text": " Literally came out in the last week and until that time I haven't seen anything in deep learning saying that so I haven't I haven't", "tokens": [23768, 1361, 484, 294, 264, 1036, 1243, 293, 1826, 300, 565, 286, 2378, 380, 1612, 1340, 294, 2452, 2539, 1566, 300, 370, 286, 2378, 380, 286, 2378, 380], "temperature": 0.0, "avg_logprob": -0.22399363310440726, "compression_ratio": 1.6553191489361703, "no_speech_prob": 2.225273192379973e-06}, {"id": 385, "seek": 189720, "start": 1897.2, "end": 1903.18, "text": " Looked at it myself until this week. I would have said it's a bad idea now. I have to think differently", "tokens": [2053, 292, 412, 309, 2059, 1826, 341, 1243, 13, 286, 576, 362, 848, 309, 311, 257, 1578, 1558, 586, 13, 286, 362, 281, 519, 7614], "temperature": 0.0, "avg_logprob": -0.17524514739046393, "compression_ratio": 1.536480686695279, "no_speech_prob": 2.994420356117189e-06}, {"id": 386, "seek": 189720, "start": 1903.18, "end": 1905.18, "text": " I guess maybe it is sometimes", "tokens": [286, 2041, 1310, 309, 307, 2171], "temperature": 0.0, "avg_logprob": -0.17524514739046393, "compression_ratio": 1.536480686695279, "no_speech_prob": 2.994420356117189e-06}, {"id": 387, "seek": 189720, "start": 1909.48, "end": 1911.48, "text": " So if you're using", "tokens": [407, 498, 291, 434, 1228], "temperature": 0.0, "avg_logprob": -0.17524514739046393, "compression_ratio": 1.536480686695279, "no_speech_prob": 2.994420356117189e-06}, {"id": 388, "seek": 189720, "start": 1912.0800000000002, "end": 1918.1200000000001, "text": " Year as a category what happens when you run the model on a year? It's never seen so you trained it in", "tokens": [10289, 382, 257, 7719, 437, 2314, 562, 291, 1190, 264, 2316, 322, 257, 1064, 30, 467, 311, 1128, 1612, 370, 291, 8895, 309, 294], "temperature": 0.0, "avg_logprob": -0.17524514739046393, "compression_ratio": 1.536480686695279, "no_speech_prob": 2.994420356117189e-06}, {"id": 389, "seek": 189720, "start": 1918.68, "end": 1925.54, "text": " Well, we'll get there. Yeah, the short answer is it'll be treated as an unknown category and so pandas", "tokens": [1042, 11, 321, 603, 483, 456, 13, 865, 11, 264, 2099, 1867, 307, 309, 603, 312, 8668, 382, 364, 9841, 7719, 293, 370, 4565, 296], "temperature": 0.0, "avg_logprob": -0.17524514739046393, "compression_ratio": 1.536480686695279, "no_speech_prob": 2.994420356117189e-06}, {"id": 390, "seek": 192554, "start": 1925.54, "end": 1928.84, "text": " Which is the underlying data frame thing?", "tokens": [3013, 307, 264, 14217, 1412, 3920, 551, 30], "temperature": 0.0, "avg_logprob": -0.22062177543180533, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.0289330930390861e-05}, {"id": 391, "seek": 192554, "start": 1928.84, "end": 1934.68, "text": " We're using with categories as a special category called unknown and if it sees a category it hasn't seen before", "tokens": [492, 434, 1228, 365, 10479, 382, 257, 2121, 7719, 1219, 9841, 293, 498, 309, 8194, 257, 7719, 309, 6132, 380, 1612, 949], "temperature": 0.0, "avg_logprob": -0.22062177543180533, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.0289330930390861e-05}, {"id": 392, "seek": 192554, "start": 1935.0, "end": 1937.0, "text": " It gets treated as unknown", "tokens": [467, 2170, 8668, 382, 9841], "temperature": 0.0, "avg_logprob": -0.22062177543180533, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.0289330930390861e-05}, {"id": 393, "seek": 192554, "start": 1939.04, "end": 1942.84, "text": " So for our deep learning model unknown would just be another category", "tokens": [407, 337, 527, 2452, 2539, 2316, 9841, 576, 445, 312, 1071, 7719], "temperature": 0.0, "avg_logprob": -0.22062177543180533, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.0289330930390861e-05}, {"id": 394, "seek": 192554, "start": 1946.0, "end": 1951.52, "text": " If our data set training the data set doesn't have a category and", "tokens": [759, 527, 1412, 992, 3097, 264, 1412, 992, 1177, 380, 362, 257, 7719, 293], "temperature": 0.0, "avg_logprob": -0.22062177543180533, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.0289330930390861e-05}, {"id": 395, "seek": 195152, "start": 1951.52, "end": 1958.24, "text": " The test has unknown how will it be? It'll just be part of this unknown category", "tokens": [440, 1500, 575, 9841, 577, 486, 309, 312, 30, 467, 603, 445, 312, 644, 295, 341, 9841, 7719], "temperature": 0.0, "avg_logprob": -0.2575323952568902, "compression_ratio": 1.704225352112676, "no_speech_prob": 3.726590193764423e-06}, {"id": 396, "seek": 195152, "start": 1958.24, "end": 1963.0, "text": " Well, it's still predict it'll predict something right like it'll just have the value", "tokens": [1042, 11, 309, 311, 920, 6069, 309, 603, 6069, 746, 558, 411, 309, 603, 445, 362, 264, 2158], "temperature": 0.0, "avg_logprob": -0.2575323952568902, "compression_ratio": 1.704225352112676, "no_speech_prob": 3.726590193764423e-06}, {"id": 397, "seek": 195152, "start": 1963.4, "end": 1969.4, "text": " Zero behind the scenes and if there's been any unknowns of any kind in the training set then it'll learn", "tokens": [17182, 2261, 264, 8026, 293, 498, 456, 311, 668, 604, 46048, 295, 604, 733, 294, 264, 3097, 992, 550, 309, 603, 1466], "temperature": 0.0, "avg_logprob": -0.2575323952568902, "compression_ratio": 1.704225352112676, "no_speech_prob": 3.726590193764423e-06}, {"id": 398, "seek": 195152, "start": 1970.0, "end": 1976.54, "text": " a way to predict unknown if it hasn't it's going to have some random vector and so that's a", "tokens": [257, 636, 281, 6069, 9841, 498, 309, 6132, 380, 309, 311, 516, 281, 362, 512, 4974, 8062, 293, 370, 300, 311, 257], "temperature": 0.0, "avg_logprob": -0.2575323952568902, "compression_ratio": 1.704225352112676, "no_speech_prob": 3.726590193764423e-06}, {"id": 399, "seek": 197654, "start": 1976.54, "end": 1981.74, "text": " Interesting detail around training that we probably won't talk about in this part of the course", "tokens": [14711, 2607, 926, 3097, 300, 321, 1391, 1582, 380, 751, 466, 294, 341, 644, 295, 264, 1164], "temperature": 0.0, "avg_logprob": -0.18106896200297792, "compression_ratio": 1.5345622119815667, "no_speech_prob": 1.9947217424487462e-06}, {"id": 400, "seek": 197654, "start": 1981.74, "end": 1983.74, "text": " But we can certainly talk about on the forum", "tokens": [583, 321, 393, 3297, 751, 466, 322, 264, 17542], "temperature": 0.0, "avg_logprob": -0.18106896200297792, "compression_ratio": 1.5345622119815667, "no_speech_prob": 1.9947217424487462e-06}, {"id": 401, "seek": 197654, "start": 1986.02, "end": 1992.7, "text": " Okay, so we've got our categorical and continuous variable lists defined in this case there was a", "tokens": [1033, 11, 370, 321, 600, 658, 527, 19250, 804, 293, 10957, 7006, 14511, 7642, 294, 341, 1389, 456, 390, 257], "temperature": 0.0, "avg_logprob": -0.18106896200297792, "compression_ratio": 1.5345622119815667, "no_speech_prob": 1.9947217424487462e-06}, {"id": 402, "seek": 197654, "start": 1993.42, "end": 1995.42, "text": " 800,000 rows", "tokens": [13083, 11, 1360, 13241], "temperature": 0.0, "avg_logprob": -0.18106896200297792, "compression_ratio": 1.5345622119815667, "no_speech_prob": 1.9947217424487462e-06}, {"id": 403, "seek": 197654, "start": 1995.5, "end": 1998.54, "text": " So 800,000 dates basically by stores", "tokens": [407, 13083, 11, 1360, 11691, 1936, 538, 9512], "temperature": 0.0, "avg_logprob": -0.18106896200297792, "compression_ratio": 1.5345622119815667, "no_speech_prob": 1.9947217424487462e-06}, {"id": 404, "seek": 199854, "start": 1998.54, "end": 2005.1399999999999, "text": " And so you can now take all of these columns", "tokens": [400, 370, 291, 393, 586, 747, 439, 295, 613, 13766], "temperature": 0.0, "avg_logprob": -0.17203207637952722, "compression_ratio": 1.5541125541125542, "no_speech_prob": 1.1365610816937988e-06}, {"id": 405, "seek": 199854, "start": 2006.46, "end": 2008.46, "text": " Loop through each one and", "tokens": [45660, 807, 1184, 472, 293], "temperature": 0.0, "avg_logprob": -0.17203207637952722, "compression_ratio": 1.5541125541125542, "no_speech_prob": 1.1365610816937988e-06}, {"id": 406, "seek": 199854, "start": 2008.8999999999999, "end": 2014.82, "text": " Replace it in the data frame where the version where you say take it and change its type to category", "tokens": [1300, 6742, 309, 294, 264, 1412, 3920, 689, 264, 3037, 689, 291, 584, 747, 309, 293, 1319, 1080, 2010, 281, 7719], "temperature": 0.0, "avg_logprob": -0.17203207637952722, "compression_ratio": 1.5541125541125542, "no_speech_prob": 1.1365610816937988e-06}, {"id": 407, "seek": 199854, "start": 2015.7, "end": 2021.1399999999999, "text": " Okay, and so that just that's just a pandas thing. So I'm not going to teach you pandas", "tokens": [1033, 11, 293, 370, 300, 445, 300, 311, 445, 257, 4565, 296, 551, 13, 407, 286, 478, 406, 516, 281, 2924, 291, 4565, 296], "temperature": 0.0, "avg_logprob": -0.17203207637952722, "compression_ratio": 1.5541125541125542, "no_speech_prob": 1.1365610816937988e-06}, {"id": 408, "seek": 199854, "start": 2021.1399999999999, "end": 2027.12, "text": " There's plenty of books particularly Wes McKinney's books book on Python for data analysis is great", "tokens": [821, 311, 7140, 295, 3642, 4098, 23843, 21765, 259, 2397, 311, 3642, 1446, 322, 15329, 337, 1412, 5215, 307, 869], "temperature": 0.0, "avg_logprob": -0.17203207637952722, "compression_ratio": 1.5541125541125542, "no_speech_prob": 1.1365610816937988e-06}, {"id": 409, "seek": 202712, "start": 2027.12, "end": 2033.8, "text": " But hopefully it's intuitive as to what's going on even if you haven't seen the specific syntax before so we're going to turn that", "tokens": [583, 4696, 309, 311, 21769, 382, 281, 437, 311, 516, 322, 754, 498, 291, 2378, 380, 1612, 264, 2685, 28431, 949, 370, 321, 434, 516, 281, 1261, 300], "temperature": 0.0, "avg_logprob": -0.21240814286048967, "compression_ratio": 1.6721991701244814, "no_speech_prob": 1.1189403039679746e-06}, {"id": 410, "seek": 202712, "start": 2034.08, "end": 2036.28, "text": " Column into a categorical column", "tokens": [4004, 16449, 666, 257, 19250, 804, 7738], "temperature": 0.0, "avg_logprob": -0.21240814286048967, "compression_ratio": 1.6721991701244814, "no_speech_prob": 1.1189403039679746e-06}, {"id": 411, "seek": 202712, "start": 2036.9199999999998, "end": 2039.9599999999998, "text": " And then for the continuous variables, we're going to make them all", "tokens": [400, 550, 337, 264, 10957, 9102, 11, 321, 434, 516, 281, 652, 552, 439], "temperature": 0.0, "avg_logprob": -0.21240814286048967, "compression_ratio": 1.6721991701244814, "no_speech_prob": 1.1189403039679746e-06}, {"id": 412, "seek": 202712, "start": 2040.7199999999998, "end": 2045.4399999999998, "text": " 32 bit floating point and for the reason for that is that PI torch", "tokens": [8858, 857, 12607, 935, 293, 337, 264, 1778, 337, 300, 307, 300, 27176, 27822], "temperature": 0.0, "avg_logprob": -0.21240814286048967, "compression_ratio": 1.6721991701244814, "no_speech_prob": 1.1189403039679746e-06}, {"id": 413, "seek": 202712, "start": 2047.0, "end": 2053.52, "text": " Expects everything to be 32 bit floating point. Okay, so like some of these include like", "tokens": [46318, 82, 1203, 281, 312, 8858, 857, 12607, 935, 13, 1033, 11, 370, 411, 512, 295, 613, 4090, 411], "temperature": 0.0, "avg_logprob": -0.21240814286048967, "compression_ratio": 1.6721991701244814, "no_speech_prob": 1.1189403039679746e-06}, {"id": 414, "seek": 205352, "start": 2053.52, "end": 2056.56, "text": " 1 0 things like I", "tokens": [502, 1958, 721, 411, 286], "temperature": 0.0, "avg_logprob": -0.3209257875935415, "compression_ratio": 1.5203619909502262, "no_speech_prob": 3.611971351347165e-06}, {"id": 415, "seek": 205352, "start": 2058.88, "end": 2061.56, "text": " Can't see them straight away, but anyway some of them yeah", "tokens": [1664, 380, 536, 552, 2997, 1314, 11, 457, 4033, 512, 295, 552, 1338], "temperature": 0.0, "avg_logprob": -0.3209257875935415, "compression_ratio": 1.5203619909502262, "no_speech_prob": 3.611971351347165e-06}, {"id": 416, "seek": 205352, "start": 2061.56, "end": 2069.64, "text": " Like was there a promo was was a holiday and so that'll become the floating point values 1 and 0 instance. Okay, so", "tokens": [1743, 390, 456, 257, 26750, 390, 390, 257, 9960, 293, 370, 300, 603, 1813, 264, 12607, 935, 4190, 502, 293, 1958, 5197, 13, 1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.3209257875935415, "compression_ratio": 1.5203619909502262, "no_speech_prob": 3.611971351347165e-06}, {"id": 417, "seek": 205352, "start": 2072.12, "end": 2075.68, "text": " I try to do as much of my work as possible on", "tokens": [286, 853, 281, 360, 382, 709, 295, 452, 589, 382, 1944, 322], "temperature": 0.0, "avg_logprob": -0.3209257875935415, "compression_ratio": 1.5203619909502262, "no_speech_prob": 3.611971351347165e-06}, {"id": 418, "seek": 205352, "start": 2076.4, "end": 2078.04, "text": " small data sets", "tokens": [1359, 1412, 6352], "temperature": 0.0, "avg_logprob": -0.3209257875935415, "compression_ratio": 1.5203619909502262, "no_speech_prob": 3.611971351347165e-06}, {"id": 419, "seek": 205352, "start": 2078.04, "end": 2082.56, "text": " for when I'm working with images that generally means resizing the images to like", "tokens": [337, 562, 286, 478, 1364, 365, 5267, 300, 5101, 1355, 725, 3319, 264, 5267, 281, 411], "temperature": 0.0, "avg_logprob": -0.3209257875935415, "compression_ratio": 1.5203619909502262, "no_speech_prob": 3.611971351347165e-06}, {"id": 420, "seek": 208256, "start": 2082.56, "end": 2085.52, "text": " 64 by 64 or 128 by 128", "tokens": [12145, 538, 12145, 420, 29810, 538, 29810], "temperature": 0.0, "avg_logprob": -0.1746235466003418, "compression_ratio": 1.6495726495726495, "no_speech_prob": 8.801046533335466e-06}, {"id": 421, "seek": 208256, "start": 2085.88, "end": 2092.64, "text": " We can't do that with structured data. So instead I tend to take a sample. So I randomly pick a few rows", "tokens": [492, 393, 380, 360, 300, 365, 18519, 1412, 13, 407, 2602, 286, 3928, 281, 747, 257, 6889, 13, 407, 286, 16979, 1888, 257, 1326, 13241], "temperature": 0.0, "avg_logprob": -0.1746235466003418, "compression_ratio": 1.6495726495726495, "no_speech_prob": 8.801046533335466e-06}, {"id": 422, "seek": 208256, "start": 2093.12, "end": 2096.12, "text": " So I start running with a sample and I can use exactly the same", "tokens": [407, 286, 722, 2614, 365, 257, 6889, 293, 286, 393, 764, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.1746235466003418, "compression_ratio": 1.6495726495726495, "no_speech_prob": 8.801046533335466e-06}, {"id": 423, "seek": 208256, "start": 2096.7599999999998, "end": 2102.48, "text": " Thing that we've seen before for getting a validation set we can use the same way to get some random", "tokens": [30902, 300, 321, 600, 1612, 949, 337, 1242, 257, 24071, 992, 321, 393, 764, 264, 912, 636, 281, 483, 512, 4974], "temperature": 0.0, "avg_logprob": -0.1746235466003418, "compression_ratio": 1.6495726495726495, "no_speech_prob": 8.801046533335466e-06}, {"id": 424, "seek": 208256, "start": 2103.2, "end": 2109.32, "text": " Random row numbers to use in a random sample. Okay, so this is just a bunch of random numbers", "tokens": [37603, 5386, 3547, 281, 764, 294, 257, 4974, 6889, 13, 1033, 11, 370, 341, 307, 445, 257, 3840, 295, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.1746235466003418, "compression_ratio": 1.6495726495726495, "no_speech_prob": 8.801046533335466e-06}, {"id": 425, "seek": 210932, "start": 2109.32, "end": 2111.32, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.20874583884461284, "compression_ratio": 1.44, "no_speech_prob": 1.1478691703814548e-05}, {"id": 426, "seek": 210932, "start": 2113.56, "end": 2118.1600000000003, "text": " Then okay, so that's going to be a size 150,000 rather than 840,000", "tokens": [1396, 1392, 11, 370, 300, 311, 516, 281, 312, 257, 2744, 8451, 11, 1360, 2831, 813, 1649, 5254, 11, 1360], "temperature": 0.0, "avg_logprob": -0.20874583884461284, "compression_ratio": 1.44, "no_speech_prob": 1.1478691703814548e-05}, {"id": 427, "seek": 210932, "start": 2120.84, "end": 2129.28, "text": " And so my data before I go any further it basically looks like this you can see I've got some booleans here. I've got some", "tokens": [400, 370, 452, 1412, 949, 286, 352, 604, 3052, 309, 1936, 1542, 411, 341, 291, 393, 536, 286, 600, 658, 512, 748, 4812, 599, 510, 13, 286, 600, 658, 512], "temperature": 0.0, "avg_logprob": -0.20874583884461284, "compression_ratio": 1.44, "no_speech_prob": 1.1478691703814548e-05}, {"id": 428, "seek": 210932, "start": 2130.96, "end": 2135.28, "text": " Integers here of various different scales as my year 2014", "tokens": [5681, 1146, 433, 510, 295, 3683, 819, 17408, 382, 452, 1064, 8227], "temperature": 0.0, "avg_logprob": -0.20874583884461284, "compression_ratio": 1.44, "no_speech_prob": 1.1478691703814548e-05}, {"id": 429, "seek": 213528, "start": 2135.28, "end": 2139.92, "text": " And I've got some letters here. So even though I said", "tokens": [400, 286, 600, 658, 512, 7825, 510, 13, 407, 754, 1673, 286, 848], "temperature": 0.0, "avg_logprob": -0.1905580343202103, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.4144742408461752e-06}, {"id": 430, "seek": 213528, "start": 2140.52, "end": 2142.92, "text": " Please call that a pandas category", "tokens": [2555, 818, 300, 257, 4565, 296, 7719], "temperature": 0.0, "avg_logprob": -0.1905580343202103, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.4144742408461752e-06}, {"id": 431, "seek": 213528, "start": 2144.2000000000003, "end": 2148.32, "text": " Pandas still displays that in the notebook as strings, right?", "tokens": [16995, 296, 920, 20119, 300, 294, 264, 21060, 382, 13985, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1905580343202103, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.4144742408461752e-06}, {"id": 432, "seek": 213528, "start": 2149.0400000000004, "end": 2151.48, "text": " It's just stored in internally differently", "tokens": [467, 311, 445, 12187, 294, 19501, 7614], "temperature": 0.0, "avg_logprob": -0.1905580343202103, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.4144742408461752e-06}, {"id": 433, "seek": 213528, "start": 2152.1600000000003, "end": 2157.44, "text": " so then the fast AI library has a special little function called process data frame and", "tokens": [370, 550, 264, 2370, 7318, 6405, 575, 257, 2121, 707, 2445, 1219, 1399, 1412, 3920, 293], "temperature": 0.0, "avg_logprob": -0.1905580343202103, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.4144742408461752e-06}, {"id": 434, "seek": 213528, "start": 2158.1600000000003, "end": 2163.2000000000003, "text": " Process data frame takes a data frame and you tell it what's my dependent variable", "tokens": [31093, 1412, 3920, 2516, 257, 1412, 3920, 293, 291, 980, 309, 437, 311, 452, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.1905580343202103, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.4144742408461752e-06}, {"id": 435, "seek": 216320, "start": 2163.2, "end": 2165.72, "text": " All right, and it does a few different things", "tokens": [1057, 558, 11, 293, 309, 775, 257, 1326, 819, 721], "temperature": 0.0, "avg_logprob": -0.1891574270931291, "compression_ratio": 1.6267942583732058, "no_speech_prob": 1.6536864677618723e-06}, {"id": 436, "seek": 216320, "start": 2165.72, "end": 2170.64, "text": " The first thing is it's pulls out that dependent variable and puts it into a separate variable", "tokens": [440, 700, 551, 307, 309, 311, 16982, 484, 300, 12334, 7006, 293, 8137, 309, 666, 257, 4994, 7006], "temperature": 0.0, "avg_logprob": -0.1891574270931291, "compression_ratio": 1.6267942583732058, "no_speech_prob": 1.6536864677618723e-06}, {"id": 437, "seek": 216320, "start": 2170.96, "end": 2173.7599999999998, "text": " Okay, and deletes it from the original data frame", "tokens": [1033, 11, 293, 1103, 37996, 309, 490, 264, 3380, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.1891574270931291, "compression_ratio": 1.6267942583732058, "no_speech_prob": 1.6536864677618723e-06}, {"id": 438, "seek": 216320, "start": 2173.8399999999997, "end": 2179.8999999999996, "text": " So DF now does not have the sales column in where else why just contains a sales column", "tokens": [407, 48336, 586, 775, 406, 362, 264, 5763, 7738, 294, 689, 1646, 983, 445, 8306, 257, 5763, 7738], "temperature": 0.0, "avg_logprob": -0.1891574270931291, "compression_ratio": 1.6267942583732058, "no_speech_prob": 1.6536864677618723e-06}, {"id": 439, "seek": 216320, "start": 2181.9199999999996, "end": 2186.68, "text": " Something else that it does is it does scaling so neural nets", "tokens": [6595, 1646, 300, 309, 775, 307, 309, 775, 21589, 370, 18161, 36170], "temperature": 0.0, "avg_logprob": -0.1891574270931291, "compression_ratio": 1.6267942583732058, "no_speech_prob": 1.6536864677618723e-06}, {"id": 440, "seek": 218668, "start": 2186.68, "end": 2193.8399999999997, "text": " Really like to have the input data to all be somewhere around zero with a standard deviation", "tokens": [4083, 411, 281, 362, 264, 4846, 1412, 281, 439, 312, 4079, 926, 4018, 365, 257, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.19975204467773439, "compression_ratio": 1.79296875, "no_speech_prob": 2.368793047935469e-06}, {"id": 441, "seek": 218668, "start": 2193.8399999999997, "end": 2197.2, "text": " it's somewhere around one right so we can always take our data and", "tokens": [309, 311, 4079, 926, 472, 558, 370, 321, 393, 1009, 747, 527, 1412, 293], "temperature": 0.0, "avg_logprob": -0.19975204467773439, "compression_ratio": 1.79296875, "no_speech_prob": 2.368793047935469e-06}, {"id": 442, "seek": 218668, "start": 2199.04, "end": 2202.68, "text": " Subtract the mean and divide by the standard deviation to make that happen", "tokens": [8511, 83, 1897, 264, 914, 293, 9845, 538, 264, 3832, 25163, 281, 652, 300, 1051], "temperature": 0.0, "avg_logprob": -0.19975204467773439, "compression_ratio": 1.79296875, "no_speech_prob": 2.368793047935469e-06}, {"id": 443, "seek": 218668, "start": 2203.08, "end": 2207.7599999999998, "text": " So that's what do scale equals true does and it actually returns a special object", "tokens": [407, 300, 311, 437, 360, 4373, 6915, 2074, 775, 293, 309, 767, 11247, 257, 2121, 2657], "temperature": 0.0, "avg_logprob": -0.19975204467773439, "compression_ratio": 1.79296875, "no_speech_prob": 2.368793047935469e-06}, {"id": 444, "seek": 218668, "start": 2207.8399999999997, "end": 2212.3599999999997, "text": " Which keeps track of what mean and standard deviation did it use for that normalizing?", "tokens": [3013, 5965, 2837, 295, 437, 914, 293, 3832, 25163, 630, 309, 764, 337, 300, 2710, 3319, 30], "temperature": 0.0, "avg_logprob": -0.19975204467773439, "compression_ratio": 1.79296875, "no_speech_prob": 2.368793047935469e-06}, {"id": 445, "seek": 221236, "start": 2212.36, "end": 2216.1800000000003, "text": " So you can then do the same thing to the test set later?", "tokens": [407, 291, 393, 550, 360, 264, 912, 551, 281, 264, 1500, 992, 1780, 30], "temperature": 0.0, "avg_logprob": -0.2661885284795994, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.7264602522336645e-06}, {"id": 446, "seek": 221236, "start": 2217.1400000000003, "end": 2219.1400000000003, "text": " It also handles missing values", "tokens": [467, 611, 18722, 5361, 4190], "temperature": 0.0, "avg_logprob": -0.2661885284795994, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.7264602522336645e-06}, {"id": 447, "seek": 221236, "start": 2219.86, "end": 2220.98, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2661885284795994, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.7264602522336645e-06}, {"id": 448, "seek": 221236, "start": 2220.98, "end": 2222.98, "text": " missing values", "tokens": [5361, 4190], "temperature": 0.0, "avg_logprob": -0.2661885284795994, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.7264602522336645e-06}, {"id": 449, "seek": 221236, "start": 2222.98, "end": 2230.86, "text": " Categorical variables just become the idea zero and then all the other categories become one two three four five for that categorical variable", "tokens": [383, 2968, 284, 804, 9102, 445, 1813, 264, 1558, 4018, 293, 550, 439, 264, 661, 10479, 1813, 472, 732, 1045, 1451, 1732, 337, 300, 19250, 804, 7006], "temperature": 0.0, "avg_logprob": -0.2661885284795994, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.7264602522336645e-06}, {"id": 450, "seek": 221236, "start": 2231.54, "end": 2235.06, "text": " for continuous variables it replaces the", "tokens": [337, 10957, 9102, 309, 46734, 264], "temperature": 0.0, "avg_logprob": -0.2661885284795994, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.7264602522336645e-06}, {"id": 451, "seek": 221236, "start": 2236.06, "end": 2240.1, "text": " Missing value with the median and creates a new column", "tokens": [5275, 278, 2158, 365, 264, 26779, 293, 7829, 257, 777, 7738], "temperature": 0.0, "avg_logprob": -0.2661885284795994, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.7264602522336645e-06}, {"id": 452, "seek": 224010, "start": 2240.1, "end": 2245.9, "text": " That's a Boolean and just says is this missing or not and I'm going to skip over this pretty quickly because we talk about", "tokens": [663, 311, 257, 23351, 28499, 293, 445, 1619, 307, 341, 5361, 420, 406, 293, 286, 478, 516, 281, 10023, 670, 341, 1238, 2661, 570, 321, 751, 466], "temperature": 0.0, "avg_logprob": -0.2190717819633834, "compression_ratio": 1.584717607973422, "no_speech_prob": 2.726462071223068e-06}, {"id": 453, "seek": 224010, "start": 2245.9, "end": 2250.8199999999997, "text": " This in detail in the machine learning course. Okay, so if you've got any questions about this part", "tokens": [639, 294, 2607, 294, 264, 3479, 2539, 1164, 13, 1033, 11, 370, 498, 291, 600, 658, 604, 1651, 466, 341, 644], "temperature": 0.0, "avg_logprob": -0.2190717819633834, "compression_ratio": 1.584717607973422, "no_speech_prob": 2.726462071223068e-06}, {"id": 454, "seek": 224010, "start": 2251.62, "end": 2256.02, "text": " That would be a good place to go. It's nothing deep learning specific there", "tokens": [663, 576, 312, 257, 665, 1081, 281, 352, 13, 467, 311, 1825, 2452, 2539, 2685, 456], "temperature": 0.0, "avg_logprob": -0.2190717819633834, "compression_ratio": 1.584717607973422, "no_speech_prob": 2.726462071223068e-06}, {"id": 455, "seek": 224010, "start": 2256.42, "end": 2259.2999999999997, "text": " So you can see afterwards year 2014", "tokens": [407, 291, 393, 536, 10543, 1064, 8227], "temperature": 0.0, "avg_logprob": -0.2190717819633834, "compression_ratio": 1.584717607973422, "no_speech_prob": 2.726462071223068e-06}, {"id": 456, "seek": 224010, "start": 2259.8199999999997, "end": 2264.58, "text": " For example has become year two okay because these categorical variables have all been replaced with", "tokens": [1171, 1365, 575, 1813, 1064, 732, 1392, 570, 613, 19250, 804, 9102, 362, 439, 668, 10772, 365], "temperature": 0.0, "avg_logprob": -0.2190717819633834, "compression_ratio": 1.584717607973422, "no_speech_prob": 2.726462071223068e-06}, {"id": 457, "seek": 224010, "start": 2266.06, "end": 2268.46, "text": " With contiguous integers starting at zero", "tokens": [2022, 660, 30525, 41674, 2891, 412, 4018], "temperature": 0.0, "avg_logprob": -0.2190717819633834, "compression_ratio": 1.584717607973422, "no_speech_prob": 2.726462071223068e-06}, {"id": 458, "seek": 226846, "start": 2268.46, "end": 2273.7400000000002, "text": " Right and the reason for that is later on we're going to be putting them into a matrix", "tokens": [1779, 293, 264, 1778, 337, 300, 307, 1780, 322, 321, 434, 516, 281, 312, 3372, 552, 666, 257, 8141], "temperature": 0.0, "avg_logprob": -0.2048053923107329, "compression_ratio": 1.6576923076923078, "no_speech_prob": 6.048894647392444e-06}, {"id": 459, "seek": 226846, "start": 2274.14, "end": 2279.48, "text": " Right and so we wouldn't want the matrix to be 2014 rows long when it could just be two rows long", "tokens": [1779, 293, 370, 321, 2759, 380, 528, 264, 8141, 281, 312, 8227, 13241, 938, 562, 309, 727, 445, 312, 732, 13241, 938], "temperature": 0.0, "avg_logprob": -0.2048053923107329, "compression_ratio": 1.6576923076923078, "no_speech_prob": 6.048894647392444e-06}, {"id": 460, "seek": 226846, "start": 2279.82, "end": 2284.44, "text": " okay, so that's the basic idea there and you'll see that the a", "tokens": [1392, 11, 370, 300, 311, 264, 3875, 1558, 456, 293, 291, 603, 536, 300, 264, 257], "temperature": 0.0, "avg_logprob": -0.2048053923107329, "compression_ratio": 1.6576923076923078, "no_speech_prob": 6.048894647392444e-06}, {"id": 461, "seek": 226846, "start": 2285.18, "end": 2289.88, "text": " C for example has been replaced in the same way with one of them three", "tokens": [383, 337, 1365, 575, 668, 10772, 294, 264, 912, 636, 365, 472, 295, 552, 1045], "temperature": 0.0, "avg_logprob": -0.2048053923107329, "compression_ratio": 1.6576923076923078, "no_speech_prob": 6.048894647392444e-06}, {"id": 462, "seek": 228988, "start": 2289.88, "end": 2298.0, "text": " Okay, so we now have a data frame which does not contain the dependent variable and where everything is a number", "tokens": [1033, 11, 370, 321, 586, 362, 257, 1412, 3920, 597, 775, 406, 5304, 264, 12334, 7006, 293, 689, 1203, 307, 257, 1230], "temperature": 0.0, "avg_logprob": -0.15035963571199806, "compression_ratio": 1.6265560165975104, "no_speech_prob": 4.3568629735091235e-06}, {"id": 463, "seek": 228988, "start": 2298.44, "end": 2298.96, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.15035963571199806, "compression_ratio": 1.6265560165975104, "no_speech_prob": 4.3568629735091235e-06}, {"id": 464, "seek": 228988, "start": 2298.96, "end": 2305.26, "text": " And so that's that's where we need to get to to do deep learning and all of the stage above that as I said we", "tokens": [400, 370, 300, 311, 300, 311, 689, 321, 643, 281, 483, 281, 281, 360, 2452, 2539, 293, 439, 295, 264, 3233, 3673, 300, 382, 286, 848, 321], "temperature": 0.0, "avg_logprob": -0.15035963571199806, "compression_ratio": 1.6265560165975104, "no_speech_prob": 4.3568629735091235e-06}, {"id": 465, "seek": 228988, "start": 2305.26, "end": 2309.88, "text": " Talk about in detail in the machine learning course nothing deep learning specific about any of it", "tokens": [8780, 466, 294, 2607, 294, 264, 3479, 2539, 1164, 1825, 2452, 2539, 2685, 466, 604, 295, 309], "temperature": 0.0, "avg_logprob": -0.15035963571199806, "compression_ratio": 1.6265560165975104, "no_speech_prob": 4.3568629735091235e-06}, {"id": 466, "seek": 228988, "start": 2309.98, "end": 2314.12, "text": " This is exactly what we throw into our random forests as well. So", "tokens": [639, 307, 2293, 437, 321, 3507, 666, 527, 4974, 21700, 382, 731, 13, 407], "temperature": 0.0, "avg_logprob": -0.15035963571199806, "compression_ratio": 1.6265560165975104, "no_speech_prob": 4.3568629735091235e-06}, {"id": 467, "seek": 231412, "start": 2314.12, "end": 2320.2, "text": " Another thing we talk about a lot in the machine learning course of course is validation sets", "tokens": [3996, 551, 321, 751, 466, 257, 688, 294, 264, 3479, 2539, 1164, 295, 1164, 307, 24071, 6352], "temperature": 0.0, "avg_logprob": -0.20251998901367188, "compression_ratio": 1.7443946188340806, "no_speech_prob": 2.6425802843732527e-06}, {"id": 468, "seek": 231412, "start": 2321.08, "end": 2325.8399999999997, "text": " In this case we need to predict the next two weeks of sales", "tokens": [682, 341, 1389, 321, 643, 281, 6069, 264, 958, 732, 3259, 295, 5763], "temperature": 0.0, "avg_logprob": -0.20251998901367188, "compression_ratio": 1.7443946188340806, "no_speech_prob": 2.6425802843732527e-06}, {"id": 469, "seek": 231412, "start": 2326.52, "end": 2332.24, "text": " Right. It's not like pick a random set of sales, but we have to pick the next two weeks of sales", "tokens": [1779, 13, 467, 311, 406, 411, 1888, 257, 4974, 992, 295, 5763, 11, 457, 321, 362, 281, 1888, 264, 958, 732, 3259, 295, 5763], "temperature": 0.0, "avg_logprob": -0.20251998901367188, "compression_ratio": 1.7443946188340806, "no_speech_prob": 2.6425802843732527e-06}, {"id": 470, "seek": 231412, "start": 2332.24, "end": 2335.42, "text": " That was what the Kaggle competition folks told us to do", "tokens": [663, 390, 437, 264, 48751, 22631, 6211, 4024, 1907, 505, 281, 360], "temperature": 0.0, "avg_logprob": -0.20251998901367188, "compression_ratio": 1.7443946188340806, "no_speech_prob": 2.6425802843732527e-06}, {"id": 471, "seek": 231412, "start": 2335.96, "end": 2341.56, "text": " And therefore I'm going to create a validation set which is the last two weeks of", "tokens": [400, 4412, 286, 478, 516, 281, 1884, 257, 24071, 992, 597, 307, 264, 1036, 732, 3259, 295], "temperature": 0.0, "avg_logprob": -0.20251998901367188, "compression_ratio": 1.7443946188340806, "no_speech_prob": 2.6425802843732527e-06}, {"id": 472, "seek": 234156, "start": 2341.56, "end": 2346.64, "text": " of my training set to try and make it as similar to the test set as possible and", "tokens": [295, 452, 3097, 992, 281, 853, 293, 652, 309, 382, 2531, 281, 264, 1500, 992, 382, 1944, 293], "temperature": 0.0, "avg_logprob": -0.18902644932827103, "compression_ratio": 1.6628787878787878, "no_speech_prob": 4.637849997379817e-06}, {"id": 473, "seek": 234156, "start": 2347.4, "end": 2351.2799999999997, "text": " We just posted actually Rachel wrote this thing last week about", "tokens": [492, 445, 9437, 767, 14246, 4114, 341, 551, 1036, 1243, 466], "temperature": 0.0, "avg_logprob": -0.18902644932827103, "compression_ratio": 1.6628787878787878, "no_speech_prob": 4.637849997379817e-06}, {"id": 474, "seek": 234156, "start": 2352.2, "end": 2356.32, "text": " Creating validation sets. So if you go to fast today, I you can check that out", "tokens": [40002, 24071, 6352, 13, 407, 498, 291, 352, 281, 2370, 965, 11, 286, 291, 393, 1520, 300, 484], "temperature": 0.0, "avg_logprob": -0.18902644932827103, "compression_ratio": 1.6628787878787878, "no_speech_prob": 4.637849997379817e-06}, {"id": 475, "seek": 234156, "start": 2356.32, "end": 2358.84, "text": " We'll put that in the lesson wiki as well", "tokens": [492, 603, 829, 300, 294, 264, 6898, 261, 9850, 382, 731], "temperature": 0.0, "avg_logprob": -0.18902644932827103, "compression_ratio": 1.6628787878787878, "no_speech_prob": 4.637849997379817e-06}, {"id": 476, "seek": 234156, "start": 2359.12, "end": 2365.18, "text": " But it's basically a summary of a recent machine learning lesson that we did", "tokens": [583, 309, 311, 1936, 257, 12691, 295, 257, 5162, 3479, 2539, 6898, 300, 321, 630], "temperature": 0.0, "avg_logprob": -0.18902644932827103, "compression_ratio": 1.6628787878787878, "no_speech_prob": 4.637849997379817e-06}, {"id": 477, "seek": 236518, "start": 2365.18, "end": 2371.02, "text": " The videos are available for that as well. And this is kind of a written a written summary of it", "tokens": [440, 2145, 366, 2435, 337, 300, 382, 731, 13, 400, 341, 307, 733, 295, 257, 3720, 257, 3720, 12691, 295, 309], "temperature": 0.0, "avg_logprob": -0.13192251105057565, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.785041710420046e-06}, {"id": 478, "seek": 236518, "start": 2371.94, "end": 2373.94, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.13192251105057565, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.785041710420046e-06}, {"id": 479, "seek": 236518, "start": 2376.06, "end": 2379.7599999999998, "text": " So yeah, so Rachel and I spent a lot of time thinking about kind of you know", "tokens": [407, 1338, 11, 370, 14246, 293, 286, 4418, 257, 688, 295, 565, 1953, 466, 733, 295, 291, 458], "temperature": 0.0, "avg_logprob": -0.13192251105057565, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.785041710420046e-06}, {"id": 480, "seek": 236518, "start": 2379.94, "end": 2385.5, "text": " How do you need to think about validation sets and training sets and test sets and so forth and that's all there", "tokens": [1012, 360, 291, 643, 281, 519, 466, 24071, 6352, 293, 3097, 6352, 293, 1500, 6352, 293, 370, 5220, 293, 300, 311, 439, 456], "temperature": 0.0, "avg_logprob": -0.13192251105057565, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.785041710420046e-06}, {"id": 481, "seek": 236518, "start": 2386.1, "end": 2391.4199999999996, "text": " But again, nothing deep learning specific. So let's get straight to the deep learning action. Okay", "tokens": [583, 797, 11, 1825, 2452, 2539, 2685, 13, 407, 718, 311, 483, 2997, 281, 264, 2452, 2539, 3069, 13, 1033], "temperature": 0.0, "avg_logprob": -0.13192251105057565, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.785041710420046e-06}, {"id": 482, "seek": 239142, "start": 2391.42, "end": 2398.7400000000002, "text": " so in this particular competition as always with any competition or any kind of", "tokens": [370, 294, 341, 1729, 6211, 382, 1009, 365, 604, 6211, 420, 604, 733, 295], "temperature": 0.0, "avg_logprob": -0.17250561264325986, "compression_ratio": 1.7710843373493976, "no_speech_prob": 2.8130016289651394e-06}, {"id": 483, "seek": 239142, "start": 2399.94, "end": 2405.3, "text": " Machine learning project you really need to make sure you have a strong understanding of your metric", "tokens": [22155, 2539, 1716, 291, 534, 643, 281, 652, 988, 291, 362, 257, 2068, 3701, 295, 428, 20678], "temperature": 0.0, "avg_logprob": -0.17250561264325986, "compression_ratio": 1.7710843373493976, "no_speech_prob": 2.8130016289651394e-06}, {"id": 484, "seek": 239142, "start": 2405.58, "end": 2407.58, "text": " How are you going to be judged here?", "tokens": [1012, 366, 291, 516, 281, 312, 27485, 510, 30], "temperature": 0.0, "avg_logprob": -0.17250561264325986, "compression_ratio": 1.7710843373493976, "no_speech_prob": 2.8130016289651394e-06}, {"id": 485, "seek": 239142, "start": 2407.58, "end": 2409.46, "text": " And in this case, you know capital makes it easy", "tokens": [400, 294, 341, 1389, 11, 291, 458, 4238, 1669, 309, 1858], "temperature": 0.0, "avg_logprob": -0.17250561264325986, "compression_ratio": 1.7710843373493976, "no_speech_prob": 2.8130016289651394e-06}, {"id": 486, "seek": 239142, "start": 2409.46, "end": 2413.88, "text": " They tell us how we're going to be judged and so we're going to be judged on the roots mean squared", "tokens": [814, 980, 505, 577, 321, 434, 516, 281, 312, 27485, 293, 370, 321, 434, 516, 281, 312, 27485, 322, 264, 10669, 914, 8889], "temperature": 0.0, "avg_logprob": -0.17250561264325986, "compression_ratio": 1.7710843373493976, "no_speech_prob": 2.8130016289651394e-06}, {"id": 487, "seek": 239142, "start": 2414.46, "end": 2419.1800000000003, "text": " Percentage error, right? So we're going to say like oh you predicted three", "tokens": [3026, 2207, 609, 6713, 11, 558, 30, 407, 321, 434, 516, 281, 584, 411, 1954, 291, 19147, 1045], "temperature": 0.0, "avg_logprob": -0.17250561264325986, "compression_ratio": 1.7710843373493976, "no_speech_prob": 2.8130016289651394e-06}, {"id": 488, "seek": 241918, "start": 2419.18, "end": 2422.02, "text": " It was actually three point three", "tokens": [467, 390, 767, 1045, 935, 1045], "temperature": 0.0, "avg_logprob": -0.252419040077611, "compression_ratio": 1.7048458149779735, "no_speech_prob": 8.315256536661764e-07}, {"id": 489, "seek": 241918, "start": 2422.14, "end": 2430.14, "text": " So you were ten percent out and then we're going to average all those percents, right? And remember I warned you that", "tokens": [407, 291, 645, 2064, 3043, 484, 293, 550, 321, 434, 516, 281, 4274, 439, 729, 680, 36976, 11, 558, 30, 400, 1604, 286, 21284, 291, 300], "temperature": 0.0, "avg_logprob": -0.252419040077611, "compression_ratio": 1.7048458149779735, "no_speech_prob": 8.315256536661764e-07}, {"id": 490, "seek": 241918, "start": 2431.5, "end": 2436.5, "text": " You are going to need to make sure you know logarithms really well, right?", "tokens": [509, 366, 516, 281, 643, 281, 652, 988, 291, 458, 41473, 355, 2592, 534, 731, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.252419040077611, "compression_ratio": 1.7048458149779735, "no_speech_prob": 8.315256536661764e-07}, {"id": 491, "seek": 241918, "start": 2436.5, "end": 2443.8999999999996, "text": " And so in this case from you know, we're basically being saying your prediction divided by the actual the mean of that", "tokens": [400, 370, 294, 341, 1389, 490, 291, 458, 11, 321, 434, 1936, 885, 1566, 428, 17630, 6666, 538, 264, 3539, 264, 914, 295, 300], "temperature": 0.0, "avg_logprob": -0.252419040077611, "compression_ratio": 1.7048458149779735, "no_speech_prob": 8.315256536661764e-07}, {"id": 492, "seek": 241918, "start": 2444.3399999999997, "end": 2448.14, "text": " right is the thing that we care about and", "tokens": [558, 307, 264, 551, 300, 321, 1127, 466, 293], "temperature": 0.0, "avg_logprob": -0.252419040077611, "compression_ratio": 1.7048458149779735, "no_speech_prob": 8.315256536661764e-07}, {"id": 493, "seek": 244814, "start": 2448.14, "end": 2451.14, "text": " and so we don't have a", "tokens": [293, 370, 321, 500, 380, 362, 257], "temperature": 0.0, "avg_logprob": -0.22881047865923712, "compression_ratio": 1.566326530612245, "no_speech_prob": 3.9054639273672365e-06}, {"id": 494, "seek": 244814, "start": 2452.22, "end": 2454.92, "text": " Metric in pytorch called root mean squared percent error", "tokens": [6377, 1341, 294, 25878, 284, 339, 1219, 5593, 914, 8889, 3043, 6713], "temperature": 0.0, "avg_logprob": -0.22881047865923712, "compression_ratio": 1.566326530612245, "no_speech_prob": 3.9054639273672365e-06}, {"id": 495, "seek": 244814, "start": 2455.66, "end": 2458.3199999999997, "text": " We could actually easily create it by the way", "tokens": [492, 727, 767, 3612, 1884, 309, 538, 264, 636], "temperature": 0.0, "avg_logprob": -0.22881047865923712, "compression_ratio": 1.566326530612245, "no_speech_prob": 3.9054639273672365e-06}, {"id": 496, "seek": 244814, "start": 2458.7, "end": 2462.7, "text": " If you look at the source code, you'll see like it's you know a line of code", "tokens": [759, 291, 574, 412, 264, 4009, 3089, 11, 291, 603, 536, 411, 309, 311, 291, 458, 257, 1622, 295, 3089], "temperature": 0.0, "avg_logprob": -0.22881047865923712, "compression_ratio": 1.566326530612245, "no_speech_prob": 3.9054639273672365e-06}, {"id": 497, "seek": 244814, "start": 2462.7, "end": 2464.7, "text": " but easier still would be to realize that", "tokens": [457, 3571, 920, 576, 312, 281, 4325, 300], "temperature": 0.0, "avg_logprob": -0.22881047865923712, "compression_ratio": 1.566326530612245, "no_speech_prob": 3.9054639273672365e-06}, {"id": 498, "seek": 244814, "start": 2467.2599999999998, "end": 2469.2599999999998, "text": " That if you have", "tokens": [663, 498, 291, 362], "temperature": 0.0, "avg_logprob": -0.22881047865923712, "compression_ratio": 1.566326530612245, "no_speech_prob": 3.9054639273672365e-06}, {"id": 499, "seek": 244814, "start": 2469.8599999999997, "end": 2473.3399999999997, "text": " That right then you could replace a with like", "tokens": [663, 558, 550, 291, 727, 7406, 257, 365, 411], "temperature": 0.0, "avg_logprob": -0.22881047865923712, "compression_ratio": 1.566326530612245, "no_speech_prob": 3.9054639273672365e-06}, {"id": 500, "seek": 247334, "start": 2473.34, "end": 2480.98, "text": " log of a dash and B with like log of B dash and then you can replace that whole thing with a", "tokens": [3565, 295, 257, 8240, 293, 363, 365, 411, 3565, 295, 363, 8240, 293, 550, 291, 393, 7406, 300, 1379, 551, 365, 257], "temperature": 0.0, "avg_logprob": -0.1947454320322169, "compression_ratio": 1.600896860986547, "no_speech_prob": 5.682410574081587e-06}, {"id": 501, "seek": 247334, "start": 2481.58, "end": 2482.86, "text": " subtraction", "tokens": [16390, 313], "temperature": 0.0, "avg_logprob": -0.1947454320322169, "compression_ratio": 1.600896860986547, "no_speech_prob": 5.682410574081587e-06}, {"id": 502, "seek": 247334, "start": 2482.86, "end": 2485.78, "text": " That's just the rule of logs, right?", "tokens": [663, 311, 445, 264, 4978, 295, 20820, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1947454320322169, "compression_ratio": 1.600896860986547, "no_speech_prob": 5.682410574081587e-06}, {"id": 503, "seek": 247334, "start": 2485.78, "end": 2491.2200000000003, "text": " And so if you don't know that rule and you know, make sure you go look it up because it's super helpful", "tokens": [400, 370, 498, 291, 500, 380, 458, 300, 4978, 293, 291, 458, 11, 652, 988, 291, 352, 574, 309, 493, 570, 309, 311, 1687, 4961], "temperature": 0.0, "avg_logprob": -0.1947454320322169, "compression_ratio": 1.600896860986547, "no_speech_prob": 5.682410574081587e-06}, {"id": 504, "seek": 247334, "start": 2491.38, "end": 2494.54, "text": " but it means in this case all we need to do is to", "tokens": [457, 309, 1355, 294, 341, 1389, 439, 321, 643, 281, 360, 307, 281], "temperature": 0.0, "avg_logprob": -0.1947454320322169, "compression_ratio": 1.600896860986547, "no_speech_prob": 5.682410574081587e-06}, {"id": 505, "seek": 247334, "start": 2495.9, "end": 2498.6600000000003, "text": " Take the log of our data", "tokens": [3664, 264, 3565, 295, 527, 1412], "temperature": 0.0, "avg_logprob": -0.1947454320322169, "compression_ratio": 1.600896860986547, "no_speech_prob": 5.682410574081587e-06}, {"id": 506, "seek": 247334, "start": 2499.26, "end": 2501.26, "text": " Which I actually did earlier in this", "tokens": [3013, 286, 767, 630, 3071, 294, 341], "temperature": 0.0, "avg_logprob": -0.1947454320322169, "compression_ratio": 1.600896860986547, "no_speech_prob": 5.682410574081587e-06}, {"id": 507, "seek": 250126, "start": 2501.26, "end": 2505.96, "text": " Notebook and when you take the log of the data getting the root means great error", "tokens": [11633, 2939, 293, 562, 291, 747, 264, 3565, 295, 264, 1412, 1242, 264, 5593, 1355, 869, 6713], "temperature": 0.0, "avg_logprob": -0.17774389304366767, "compression_ratio": 1.8376068376068375, "no_speech_prob": 5.771887572336709e-06}, {"id": 508, "seek": 250126, "start": 2506.1400000000003, "end": 2510.1200000000003, "text": " Will actually get you the root means great percent error for free", "tokens": [3099, 767, 483, 291, 264, 5593, 1355, 869, 3043, 6713, 337, 1737], "temperature": 0.0, "avg_logprob": -0.17774389304366767, "compression_ratio": 1.8376068376068375, "no_speech_prob": 5.771887572336709e-06}, {"id": 509, "seek": 250126, "start": 2510.26, "end": 2515.5, "text": " Okay, but then when we want to like print out our root means great percent error", "tokens": [1033, 11, 457, 550, 562, 321, 528, 281, 411, 4482, 484, 527, 5593, 1355, 869, 3043, 6713], "temperature": 0.0, "avg_logprob": -0.17774389304366767, "compression_ratio": 1.8376068376068375, "no_speech_prob": 5.771887572336709e-06}, {"id": 510, "seek": 250126, "start": 2515.6600000000003, "end": 2518.6600000000003, "text": " We actually have to go either the power of it", "tokens": [492, 767, 362, 281, 352, 2139, 264, 1347, 295, 309], "temperature": 0.0, "avg_logprob": -0.17774389304366767, "compression_ratio": 1.8376068376068375, "no_speech_prob": 5.771887572336709e-06}, {"id": 511, "seek": 250126, "start": 2519.2200000000003, "end": 2524.82, "text": " Again, right and then we can actually return the percent difference. So that's all that's going on here", "tokens": [3764, 11, 558, 293, 550, 321, 393, 767, 2736, 264, 3043, 2649, 13, 407, 300, 311, 439, 300, 311, 516, 322, 510], "temperature": 0.0, "avg_logprob": -0.17774389304366767, "compression_ratio": 1.8376068376068375, "no_speech_prob": 5.771887572336709e-06}, {"id": 512, "seek": 250126, "start": 2524.82, "end": 2527.0, "text": " It's again not really deep learning specific at all", "tokens": [467, 311, 797, 406, 534, 2452, 2539, 2685, 412, 439], "temperature": 0.0, "avg_logprob": -0.17774389304366767, "compression_ratio": 1.8376068376068375, "no_speech_prob": 5.771887572336709e-06}, {"id": 513, "seek": 252700, "start": 2527.0, "end": 2529.0, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.17602464285763827, "compression_ratio": 1.5746606334841629, "no_speech_prob": 9.570812835590914e-07}, {"id": 514, "seek": 252700, "start": 2529.8, "end": 2531.8, "text": " Here we finally get to the deep learning", "tokens": [1692, 321, 2721, 483, 281, 264, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.17602464285763827, "compression_ratio": 1.5746606334841629, "no_speech_prob": 9.570812835590914e-07}, {"id": 515, "seek": 252700, "start": 2532.16, "end": 2538.64, "text": " Alright, so as per usual like you'll see everything we look at today looks exactly the same as everything", "tokens": [2798, 11, 370, 382, 680, 7713, 411, 291, 603, 536, 1203, 321, 574, 412, 965, 1542, 2293, 264, 912, 382, 1203], "temperature": 0.0, "avg_logprob": -0.17602464285763827, "compression_ratio": 1.5746606334841629, "no_speech_prob": 9.570812835590914e-07}, {"id": 516, "seek": 252700, "start": 2538.64, "end": 2542.64, "text": " We've looked at so far, which is first we create a model data object", "tokens": [492, 600, 2956, 412, 370, 1400, 11, 597, 307, 700, 321, 1884, 257, 2316, 1412, 2657], "temperature": 0.0, "avg_logprob": -0.17602464285763827, "compression_ratio": 1.5746606334841629, "no_speech_prob": 9.570812835590914e-07}, {"id": 517, "seek": 252700, "start": 2543.4, "end": 2545.44, "text": " Something that has a validation set", "tokens": [6595, 300, 575, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.17602464285763827, "compression_ratio": 1.5746606334841629, "no_speech_prob": 9.570812835590914e-07}, {"id": 518, "seek": 252700, "start": 2546.16, "end": 2550.64, "text": " Training set an optional test set built into it from that. We will get a learner", "tokens": [20620, 992, 364, 17312, 1500, 992, 3094, 666, 309, 490, 300, 13, 492, 486, 483, 257, 33347], "temperature": 0.0, "avg_logprob": -0.17602464285763827, "compression_ratio": 1.5746606334841629, "no_speech_prob": 9.570812835590914e-07}, {"id": 519, "seek": 252700, "start": 2551.2, "end": 2552.76, "text": " We will then", "tokens": [492, 486, 550], "temperature": 0.0, "avg_logprob": -0.17602464285763827, "compression_ratio": 1.5746606334841629, "no_speech_prob": 9.570812835590914e-07}, {"id": 520, "seek": 255276, "start": 2552.76, "end": 2557.6000000000004, "text": " Optionally called learner dot LR find will then called learner dot fit", "tokens": [29284, 379, 1219, 33347, 5893, 441, 49, 915, 486, 550, 1219, 33347, 5893, 3318], "temperature": 0.0, "avg_logprob": -0.17436410983403525, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.6536846487724688e-06}, {"id": 521, "seek": 255276, "start": 2557.6000000000004, "end": 2561.5600000000004, "text": " It'll be all the same parameters and everything that you've seen many times before", "tokens": [467, 603, 312, 439, 264, 912, 9834, 293, 1203, 300, 291, 600, 1612, 867, 1413, 949], "temperature": 0.0, "avg_logprob": -0.17436410983403525, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.6536846487724688e-06}, {"id": 522, "seek": 255276, "start": 2561.8, "end": 2565.6000000000004, "text": " Okay, so the difference though is obviously we're not going to go", "tokens": [1033, 11, 370, 264, 2649, 1673, 307, 2745, 321, 434, 406, 516, 281, 352], "temperature": 0.0, "avg_logprob": -0.17436410983403525, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.6536846487724688e-06}, {"id": 523, "seek": 255276, "start": 2567.0400000000004, "end": 2570.92, "text": " Image classifier data dot from CSV or dot from paths", "tokens": [29903, 1508, 9902, 1412, 5893, 490, 48814, 420, 5893, 490, 14518], "temperature": 0.0, "avg_logprob": -0.17436410983403525, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.6536846487724688e-06}, {"id": 524, "seek": 255276, "start": 2570.92, "end": 2576.5600000000004, "text": " We need to get some different kind of model data. And so for stuff that is in rows and columns", "tokens": [492, 643, 281, 483, 512, 819, 733, 295, 2316, 1412, 13, 400, 370, 337, 1507, 300, 307, 294, 13241, 293, 13766], "temperature": 0.0, "avg_logprob": -0.17436410983403525, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.6536846487724688e-06}, {"id": 525, "seek": 255276, "start": 2577.0, "end": 2579.0, "text": " We use columnar model data", "tokens": [492, 764, 7738, 289, 2316, 1412], "temperature": 0.0, "avg_logprob": -0.17436410983403525, "compression_ratio": 1.6416666666666666, "no_speech_prob": 1.6536846487724688e-06}, {"id": 526, "seek": 257900, "start": 2579.0, "end": 2582.8, "text": " Okay, but this will return an object with basically the same API", "tokens": [1033, 11, 457, 341, 486, 2736, 364, 2657, 365, 1936, 264, 912, 9362], "temperature": 0.0, "avg_logprob": -0.1386760109349301, "compression_ratio": 1.6208333333333333, "no_speech_prob": 2.684192168089794e-06}, {"id": 527, "seek": 257900, "start": 2583.32, "end": 2589.98, "text": " That you're familiar with and rather than from paths or from CSV. This is from data frame", "tokens": [663, 291, 434, 4963, 365, 293, 2831, 813, 490, 14518, 420, 490, 48814, 13, 639, 307, 490, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.1386760109349301, "compression_ratio": 1.6208333333333333, "no_speech_prob": 2.684192168089794e-06}, {"id": 528, "seek": 257900, "start": 2590.12, "end": 2592.56, "text": " Okay, so this gets passed a few things", "tokens": [1033, 11, 370, 341, 2170, 4678, 257, 1326, 721], "temperature": 0.0, "avg_logprob": -0.1386760109349301, "compression_ratio": 1.6208333333333333, "no_speech_prob": 2.684192168089794e-06}, {"id": 529, "seek": 257900, "start": 2593.08, "end": 2597.6, "text": " The path here is just used for it to know where should it store?", "tokens": [440, 3100, 510, 307, 445, 1143, 337, 309, 281, 458, 689, 820, 309, 3531, 30], "temperature": 0.0, "avg_logprob": -0.1386760109349301, "compression_ratio": 1.6208333333333333, "no_speech_prob": 2.684192168089794e-06}, {"id": 530, "seek": 257900, "start": 2598.16, "end": 2604.16, "text": " Like model files or stuff like that, right? This is just basically saying where do you want to store anything that you save later?", "tokens": [1743, 2316, 7098, 420, 1507, 411, 300, 11, 558, 30, 639, 307, 445, 1936, 1566, 689, 360, 291, 528, 281, 3531, 1340, 300, 291, 3155, 1780, 30], "temperature": 0.0, "avg_logprob": -0.1386760109349301, "compression_ratio": 1.6208333333333333, "no_speech_prob": 2.684192168089794e-06}, {"id": 531, "seek": 260416, "start": 2604.16, "end": 2611.12, "text": " This is the list of the indexes of the rows that we want to put in the validation set we created earlier", "tokens": [639, 307, 264, 1329, 295, 264, 8186, 279, 295, 264, 13241, 300, 321, 528, 281, 829, 294, 264, 24071, 992, 321, 2942, 3071], "temperature": 0.0, "avg_logprob": -0.2380573071931538, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.39313737640623e-06}, {"id": 532, "seek": 260416, "start": 2611.6, "end": 2613.6, "text": " Here's our data frame", "tokens": [1692, 311, 527, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.2380573071931538, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.39313737640623e-06}, {"id": 533, "seek": 260416, "start": 2613.7999999999997, "end": 2615.0, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2380573071931538, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.39313737640623e-06}, {"id": 534, "seek": 260416, "start": 2615.0, "end": 2617.0, "text": " and then", "tokens": [293, 550], "temperature": 0.0, "avg_logprob": -0.2380573071931538, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.39313737640623e-06}, {"id": 535, "seek": 260416, "start": 2619.0, "end": 2621.7999999999997, "text": " Let's have a look here's this is where we did the log, right?", "tokens": [961, 311, 362, 257, 574, 510, 311, 341, 307, 689, 321, 630, 264, 3565, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2380573071931538, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.39313737640623e-06}, {"id": 536, "seek": 260416, "start": 2621.7999999999997, "end": 2628.16, "text": " So I took the the Y that came out of prop DF that dependent variable. I logged it and I call that YL", "tokens": [407, 286, 1890, 264, 264, 398, 300, 1361, 484, 295, 2365, 48336, 300, 12334, 7006, 13, 286, 27231, 309, 293, 286, 818, 300, 398, 43], "temperature": 0.0, "avg_logprob": -0.2380573071931538, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.39313737640623e-06}, {"id": 537, "seek": 260416, "start": 2628.68, "end": 2630.68, "text": " Right, so we tell it", "tokens": [1779, 11, 370, 321, 980, 309], "temperature": 0.0, "avg_logprob": -0.2380573071931538, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.39313737640623e-06}, {"id": 538, "seek": 263068, "start": 2630.68, "end": 2634.24, "text": " When we create our model data, we need to tell it that's our dependent variable", "tokens": [1133, 321, 1884, 527, 2316, 1412, 11, 321, 643, 281, 980, 309, 300, 311, 527, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.18615412930829808, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.816219992178958e-06}, {"id": 539, "seek": 263068, "start": 2634.8799999999997, "end": 2640.68, "text": " Okay, so so far we've got list of the stuff to go in the validation set, which is what's our independent variables?", "tokens": [1033, 11, 370, 370, 1400, 321, 600, 658, 1329, 295, 264, 1507, 281, 352, 294, 264, 24071, 992, 11, 597, 307, 437, 311, 527, 6695, 9102, 30], "temperature": 0.0, "avg_logprob": -0.18615412930829808, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.816219992178958e-06}, {"id": 540, "seek": 263068, "start": 2640.8399999999997, "end": 2646.9199999999996, "text": " What's our dependent variables and then we have to tell it which things do we want treated as categorical?", "tokens": [708, 311, 527, 12334, 9102, 293, 550, 321, 362, 281, 980, 309, 597, 721, 360, 321, 528, 8668, 382, 19250, 804, 30], "temperature": 0.0, "avg_logprob": -0.18615412930829808, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.816219992178958e-06}, {"id": 541, "seek": 263068, "start": 2647.3199999999997, "end": 2649.72, "text": " Right because remember by this time", "tokens": [1779, 570, 1604, 538, 341, 565], "temperature": 0.0, "avg_logprob": -0.18615412930829808, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.816219992178958e-06}, {"id": 542, "seek": 263068, "start": 2652.68, "end": 2654.68, "text": " Everything's a number", "tokens": [5471, 311, 257, 1230], "temperature": 0.0, "avg_logprob": -0.18615412930829808, "compression_ratio": 1.7713178294573644, "no_speech_prob": 1.816219992178958e-06}, {"id": 543, "seek": 265468, "start": 2654.68, "end": 2661.04, "text": " Right, so it could do the whole thing as if it's continuous. It would just be totally meaningless, right?", "tokens": [1779, 11, 370, 309, 727, 360, 264, 1379, 551, 382, 498, 309, 311, 10957, 13, 467, 576, 445, 312, 3879, 33232, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16077415898161115, "compression_ratio": 1.572549019607843, "no_speech_prob": 2.9480063403752865e-06}, {"id": 544, "seek": 265468, "start": 2661.24, "end": 2666.68, "text": " So we need to tell it which things do we want to treat as categories? And so here we just pass in", "tokens": [407, 321, 643, 281, 980, 309, 597, 721, 360, 321, 528, 281, 2387, 382, 10479, 30, 400, 370, 510, 321, 445, 1320, 294], "temperature": 0.0, "avg_logprob": -0.16077415898161115, "compression_ratio": 1.572549019607843, "no_speech_prob": 2.9480063403752865e-06}, {"id": 545, "seek": 265468, "start": 2667.24, "end": 2670.12, "text": " That list of names that we used before", "tokens": [663, 1329, 295, 5288, 300, 321, 1143, 949], "temperature": 0.0, "avg_logprob": -0.16077415898161115, "compression_ratio": 1.572549019607843, "no_speech_prob": 2.9480063403752865e-06}, {"id": 546, "seek": 265468, "start": 2670.96, "end": 2676.68, "text": " Okay, and then a bunch of the parameters are the same as the ones you're used to for example", "tokens": [1033, 11, 293, 550, 257, 3840, 295, 264, 9834, 366, 264, 912, 382, 264, 2306, 291, 434, 1143, 281, 337, 1365], "temperature": 0.0, "avg_logprob": -0.16077415898161115, "compression_ratio": 1.572549019607843, "no_speech_prob": 2.9480063403752865e-06}, {"id": 547, "seek": 265468, "start": 2676.68, "end": 2682.2, "text": " You can set the batch size here. So after we do that, we've got a", "tokens": [509, 393, 992, 264, 15245, 2744, 510, 13, 407, 934, 321, 360, 300, 11, 321, 600, 658, 257], "temperature": 0.0, "avg_logprob": -0.16077415898161115, "compression_ratio": 1.572549019607843, "no_speech_prob": 2.9480063403752865e-06}, {"id": 548, "seek": 268220, "start": 2682.2, "end": 2684.8399999999997, "text": " You know standard", "tokens": [509, 458, 3832], "temperature": 0.0, "avg_logprob": -0.25526968303479647, "compression_ratio": 1.705, "no_speech_prob": 4.495164830586873e-06}, {"id": 549, "seek": 268220, "start": 2685.4399999999996, "end": 2688.4199999999996, "text": " Model data object. There's a train train DL", "tokens": [17105, 1412, 2657, 13, 821, 311, 257, 3847, 3847, 413, 43], "temperature": 0.0, "avg_logprob": -0.25526968303479647, "compression_ratio": 1.705, "no_speech_prob": 4.495164830586873e-06}, {"id": 550, "seek": 268220, "start": 2689.12, "end": 2694.8399999999997, "text": " Attribute there's a val DL attribute a train DS attribute of our DS attribute. It's got a length", "tokens": [7298, 2024, 1169, 456, 311, 257, 1323, 413, 43, 19667, 257, 3847, 15816, 19667, 295, 527, 15816, 19667, 13, 467, 311, 658, 257, 4641], "temperature": 0.0, "avg_logprob": -0.25526968303479647, "compression_ratio": 1.705, "no_speech_prob": 4.495164830586873e-06}, {"id": 551, "seek": 268220, "start": 2694.8399999999997, "end": 2698.3199999999997, "text": " It's got all the stuff exactly like it did in all of our", "tokens": [467, 311, 658, 439, 264, 1507, 2293, 411, 309, 630, 294, 439, 295, 527], "temperature": 0.0, "avg_logprob": -0.25526968303479647, "compression_ratio": 1.705, "no_speech_prob": 4.495164830586873e-06}, {"id": 552, "seek": 268220, "start": 2699.96, "end": 2701.96, "text": " image based", "tokens": [3256, 2361], "temperature": 0.0, "avg_logprob": -0.25526968303479647, "compression_ratio": 1.705, "no_speech_prob": 4.495164830586873e-06}, {"id": 553, "seek": 268220, "start": 2701.96, "end": 2703.8399999999997, "text": " data objects", "tokens": [1412, 6565], "temperature": 0.0, "avg_logprob": -0.25526968303479647, "compression_ratio": 1.705, "no_speech_prob": 4.495164830586873e-06}, {"id": 554, "seek": 268220, "start": 2703.8399999999997, "end": 2710.3199999999997, "text": " Okay. So now we need to create the the model or create the learner and so to skip ahead a little bit", "tokens": [1033, 13, 407, 586, 321, 643, 281, 1884, 264, 264, 2316, 420, 1884, 264, 33347, 293, 370, 281, 10023, 2286, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.25526968303479647, "compression_ratio": 1.705, "no_speech_prob": 4.495164830586873e-06}, {"id": 555, "seek": 271032, "start": 2710.32, "end": 2715.04, "text": " We're basically going to pass in something that looks pretty familiar", "tokens": [492, 434, 1936, 516, 281, 1320, 294, 746, 300, 1542, 1238, 4963], "temperature": 0.0, "avg_logprob": -0.16900586600255485, "compression_ratio": 1.7982456140350878, "no_speech_prob": 8.939623512560502e-06}, {"id": 556, "seek": 271032, "start": 2715.04, "end": 2718.6000000000004, "text": " We're going to be passing saying from our model from our model data", "tokens": [492, 434, 516, 281, 312, 8437, 1566, 490, 527, 2316, 490, 527, 2316, 1412], "temperature": 0.0, "avg_logprob": -0.16900586600255485, "compression_ratio": 1.7982456140350878, "no_speech_prob": 8.939623512560502e-06}, {"id": 557, "seek": 271032, "start": 2719.2000000000003, "end": 2721.6200000000003, "text": " Create a learner that is suitable for it", "tokens": [20248, 257, 33347, 300, 307, 12873, 337, 309], "temperature": 0.0, "avg_logprob": -0.16900586600255485, "compression_ratio": 1.7982456140350878, "no_speech_prob": 8.939623512560502e-06}, {"id": 558, "seek": 271032, "start": 2722.32, "end": 2727.04, "text": " And we'll basically be passing in a few other bits of information which will include", "tokens": [400, 321, 603, 1936, 312, 8437, 294, 257, 1326, 661, 9239, 295, 1589, 597, 486, 4090], "temperature": 0.0, "avg_logprob": -0.16900586600255485, "compression_ratio": 1.7982456140350878, "no_speech_prob": 8.939623512560502e-06}, {"id": 559, "seek": 271032, "start": 2727.6800000000003, "end": 2729.98, "text": " How much dropout to use at the very start?", "tokens": [1012, 709, 3270, 346, 281, 764, 412, 264, 588, 722, 30], "temperature": 0.0, "avg_logprob": -0.16900586600255485, "compression_ratio": 1.7982456140350878, "no_speech_prob": 8.939623512560502e-06}, {"id": 560, "seek": 271032, "start": 2731.36, "end": 2738.1600000000003, "text": " How many how many activations to have in each layer how much dropout to use at the at the later layers?", "tokens": [1012, 867, 577, 867, 2430, 763, 281, 362, 294, 1184, 4583, 577, 709, 3270, 346, 281, 764, 412, 264, 412, 264, 1780, 7914, 30], "temperature": 0.0, "avg_logprob": -0.16900586600255485, "compression_ratio": 1.7982456140350878, "no_speech_prob": 8.939623512560502e-06}, {"id": 561, "seek": 273816, "start": 2738.16, "end": 2745.68, "text": " But then there's a couple of extra things that we need to learn about and specifically it's this thing called embeddings", "tokens": [583, 550, 456, 311, 257, 1916, 295, 2857, 721, 300, 321, 643, 281, 1466, 466, 293, 4682, 309, 311, 341, 551, 1219, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.15443804264068603, "compression_ratio": 1.6730769230769231, "no_speech_prob": 4.860424724029144e-06}, {"id": 562, "seek": 273816, "start": 2747.3199999999997, "end": 2754.08, "text": " So this is really the key new concept we have to learn about all right, so", "tokens": [407, 341, 307, 534, 264, 2141, 777, 3410, 321, 362, 281, 1466, 466, 439, 558, 11, 370], "temperature": 0.0, "avg_logprob": -0.15443804264068603, "compression_ratio": 1.6730769230769231, "no_speech_prob": 4.860424724029144e-06}, {"id": 563, "seek": 273816, "start": 2756.0, "end": 2759.52, "text": " All we're doing basically is we're going to take our", "tokens": [1057, 321, 434, 884, 1936, 307, 321, 434, 516, 281, 747, 527], "temperature": 0.0, "avg_logprob": -0.15443804264068603, "compression_ratio": 1.6730769230769231, "no_speech_prob": 4.860424724029144e-06}, {"id": 564, "seek": 273816, "start": 2761.3999999999996, "end": 2765.96, "text": " Let's forget about categorical variables for a moment and just think about the continuous variables", "tokens": [961, 311, 2870, 466, 19250, 804, 9102, 337, 257, 1623, 293, 445, 519, 466, 264, 10957, 9102], "temperature": 0.0, "avg_logprob": -0.15443804264068603, "compression_ratio": 1.6730769230769231, "no_speech_prob": 4.860424724029144e-06}, {"id": 565, "seek": 276596, "start": 2765.96, "end": 2769.7, "text": " All right for our continuous variables all we're going to do", "tokens": [1057, 558, 337, 527, 10957, 9102, 439, 321, 434, 516, 281, 360], "temperature": 0.0, "avg_logprob": -0.19068527221679688, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.682409664586885e-06}, {"id": 566, "seek": 276596, "start": 2770.84, "end": 2772.84, "text": " Is we're going to grab them all?", "tokens": [1119, 321, 434, 516, 281, 4444, 552, 439, 30], "temperature": 0.0, "avg_logprob": -0.19068527221679688, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.682409664586885e-06}, {"id": 567, "seek": 276596, "start": 2776.92, "end": 2781.12, "text": " Okay, so for our continuous variables, we're basically going to say like okay, here's a", "tokens": [1033, 11, 370, 337, 527, 10957, 9102, 11, 321, 434, 1936, 516, 281, 584, 411, 1392, 11, 510, 311, 257], "temperature": 0.0, "avg_logprob": -0.19068527221679688, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.682409664586885e-06}, {"id": 568, "seek": 276596, "start": 2782.56, "end": 2790.12, "text": " Big list of all of our continuous variables like the minimum temperature and the maximum temperature and the distance to the nearest competitor", "tokens": [5429, 1329, 295, 439, 295, 527, 10957, 9102, 411, 264, 7285, 4292, 293, 264, 6674, 4292, 293, 264, 4560, 281, 264, 23831, 27266], "temperature": 0.0, "avg_logprob": -0.19068527221679688, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.682409664586885e-06}, {"id": 569, "seek": 276596, "start": 2790.2400000000002, "end": 2794.84, "text": " And so forth right and so here's just a bunch of floating point numbers", "tokens": [400, 370, 5220, 558, 293, 370, 510, 311, 445, 257, 3840, 295, 12607, 935, 3547], "temperature": 0.0, "avg_logprob": -0.19068527221679688, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.682409664586885e-06}, {"id": 570, "seek": 279484, "start": 2794.84, "end": 2801.28, "text": " and so basically what the neuron that's going to do is it's going to take that that 1d array or or", "tokens": [293, 370, 1936, 437, 264, 34090, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 747, 300, 300, 502, 67, 10225, 420, 420], "temperature": 0.0, "avg_logprob": -0.2360785956521636, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.4824755655572517e-06}, {"id": 571, "seek": 279484, "start": 2801.96, "end": 2803.96, "text": " vector or to be very", "tokens": [8062, 420, 281, 312, 588], "temperature": 0.0, "avg_logprob": -0.2360785956521636, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.4824755655572517e-06}, {"id": 572, "seek": 279484, "start": 2804.6000000000004, "end": 2806.2400000000002, "text": " DL like", "tokens": [413, 43, 411], "temperature": 0.0, "avg_logprob": -0.2360785956521636, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.4824755655572517e-06}, {"id": 573, "seek": 279484, "start": 2806.2400000000002, "end": 2808.1600000000003, "text": " rank one tensor", "tokens": [6181, 472, 40863], "temperature": 0.0, "avg_logprob": -0.2360785956521636, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.4824755655572517e-06}, {"id": 574, "seek": 279484, "start": 2808.1600000000003, "end": 2815.1200000000003, "text": " All means the same thing okay, so we're going to take our rank one tensor and let's put it through a matrix multiplication", "tokens": [1057, 1355, 264, 912, 551, 1392, 11, 370, 321, 434, 516, 281, 747, 527, 6181, 472, 40863, 293, 718, 311, 829, 309, 807, 257, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.2360785956521636, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.4824755655572517e-06}, {"id": 575, "seek": 279484, "start": 2815.2000000000003, "end": 2817.88, "text": " So let's say this has got like I don't know 20", "tokens": [407, 718, 311, 584, 341, 575, 658, 411, 286, 500, 380, 458, 945], "temperature": 0.0, "avg_logprob": -0.2360785956521636, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.4824755655572517e-06}, {"id": 576, "seek": 279484, "start": 2818.88, "end": 2823.2000000000003, "text": " continuous variables, and then we can put it through a matrix which", "tokens": [10957, 9102, 11, 293, 550, 321, 393, 829, 309, 807, 257, 8141, 597], "temperature": 0.0, "avg_logprob": -0.2360785956521636, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.4824755655572517e-06}, {"id": 577, "seek": 282320, "start": 2823.2, "end": 2830.3999999999996, "text": " Must have 20 rows. That's how matrix multiplication works, and then we can decide how many columns we want right?", "tokens": [13252, 362, 945, 13241, 13, 663, 311, 577, 8141, 27290, 1985, 11, 293, 550, 321, 393, 4536, 577, 867, 13766, 321, 528, 558, 30], "temperature": 0.0, "avg_logprob": -0.17820400887347282, "compression_ratio": 1.7149321266968325, "no_speech_prob": 1.051148387887224e-06}, {"id": 578, "seek": 282320, "start": 2830.3999999999996, "end": 2835.8399999999997, "text": " So maybe we decided a hundred right and so that matrix multiplication is going to spit out a new", "tokens": [407, 1310, 321, 3047, 257, 3262, 558, 293, 370, 300, 8141, 27290, 307, 516, 281, 22127, 484, 257, 777], "temperature": 0.0, "avg_logprob": -0.17820400887347282, "compression_ratio": 1.7149321266968325, "no_speech_prob": 1.051148387887224e-06}, {"id": 579, "seek": 282320, "start": 2837.24, "end": 2839.04, "text": " length 100", "tokens": [4641, 2319], "temperature": 0.0, "avg_logprob": -0.17820400887347282, "compression_ratio": 1.7149321266968325, "no_speech_prob": 1.051148387887224e-06}, {"id": 580, "seek": 282320, "start": 2839.04, "end": 2840.8399999999997, "text": " rank one tensor", "tokens": [6181, 472, 40863], "temperature": 0.0, "avg_logprob": -0.17820400887347282, "compression_ratio": 1.7149321266968325, "no_speech_prob": 1.051148387887224e-06}, {"id": 581, "seek": 282320, "start": 2840.8399999999997, "end": 2848.08, "text": " Okay, that's that's what that's what a linear. That's what a matrix product does and that's the definition of a linear layer", "tokens": [1033, 11, 300, 311, 300, 311, 437, 300, 311, 437, 257, 8213, 13, 663, 311, 437, 257, 8141, 1674, 775, 293, 300, 311, 264, 7123, 295, 257, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.17820400887347282, "compression_ratio": 1.7149321266968325, "no_speech_prob": 1.051148387887224e-06}, {"id": 582, "seek": 282320, "start": 2848.6, "end": 2850.16, "text": " in deep learning", "tokens": [294, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.17820400887347282, "compression_ratio": 1.7149321266968325, "no_speech_prob": 1.051148387887224e-06}, {"id": 583, "seek": 285016, "start": 2850.16, "end": 2857.2, "text": " Okay, and so then the next thing we do is we can put that through a relu right which means we throw away the negatives", "tokens": [1033, 11, 293, 370, 550, 264, 958, 551, 321, 360, 307, 321, 393, 829, 300, 807, 257, 1039, 84, 558, 597, 1355, 321, 3507, 1314, 264, 40019], "temperature": 0.0, "avg_logprob": -0.14782929870317568, "compression_ratio": 1.7735042735042734, "no_speech_prob": 5.95512301515555e-06}, {"id": 584, "seek": 285016, "start": 2858.08, "end": 2865.12, "text": " Okay, and now we can put that through another matrix product. Okay, so this is going to have to have a hundred rows by definition", "tokens": [1033, 11, 293, 586, 321, 393, 829, 300, 807, 1071, 8141, 1674, 13, 1033, 11, 370, 341, 307, 516, 281, 362, 281, 362, 257, 3262, 13241, 538, 7123], "temperature": 0.0, "avg_logprob": -0.14782929870317568, "compression_ratio": 1.7735042735042734, "no_speech_prob": 5.95512301515555e-06}, {"id": 585, "seek": 285016, "start": 2866.04, "end": 2870.24, "text": " And we can have as many columns as we like and so let's say maybe this was", "tokens": [400, 321, 393, 362, 382, 867, 13766, 382, 321, 411, 293, 370, 718, 311, 584, 1310, 341, 390], "temperature": 0.0, "avg_logprob": -0.14782929870317568, "compression_ratio": 1.7735042735042734, "no_speech_prob": 5.95512301515555e-06}, {"id": 586, "seek": 285016, "start": 2870.92, "end": 2875.2, "text": " The last layer so the next thing we're trying to do is to predict sales", "tokens": [440, 1036, 4583, 370, 264, 958, 551, 321, 434, 1382, 281, 360, 307, 281, 6069, 5763], "temperature": 0.0, "avg_logprob": -0.14782929870317568, "compression_ratio": 1.7735042735042734, "no_speech_prob": 5.95512301515555e-06}, {"id": 587, "seek": 285016, "start": 2875.7799999999997, "end": 2877.7799999999997, "text": " So there's just one", "tokens": [407, 456, 311, 445, 472], "temperature": 0.0, "avg_logprob": -0.14782929870317568, "compression_ratio": 1.7735042735042734, "no_speech_prob": 5.95512301515555e-06}, {"id": 588, "seek": 287778, "start": 2877.78, "end": 2880.42, "text": " Value we're trying to predict the sales so we could put it through a", "tokens": [39352, 321, 434, 1382, 281, 6069, 264, 5763, 370, 321, 727, 829, 309, 807, 257], "temperature": 0.0, "avg_logprob": -0.18977557032941336, "compression_ratio": 1.6256410256410256, "no_speech_prob": 4.860436547460267e-06}, {"id": 589, "seek": 287778, "start": 2880.98, "end": 2885.6200000000003, "text": " Metrics product that just had one column and that's going to spit out a single number", "tokens": [6377, 10716, 1674, 300, 445, 632, 472, 7738, 293, 300, 311, 516, 281, 22127, 484, 257, 2167, 1230], "temperature": 0.0, "avg_logprob": -0.18977557032941336, "compression_ratio": 1.6256410256410256, "no_speech_prob": 4.860436547460267e-06}, {"id": 590, "seek": 287778, "start": 2886.34, "end": 2888.34, "text": " Right so that's like", "tokens": [1779, 370, 300, 311, 411], "temperature": 0.0, "avg_logprob": -0.18977557032941336, "compression_ratio": 1.6256410256410256, "no_speech_prob": 4.860436547460267e-06}, {"id": 591, "seek": 287778, "start": 2888.98, "end": 2891.48, "text": " That's kind of like a one layer", "tokens": [663, 311, 733, 295, 411, 257, 472, 4583], "temperature": 0.0, "avg_logprob": -0.18977557032941336, "compression_ratio": 1.6256410256410256, "no_speech_prob": 4.860436547460267e-06}, {"id": 592, "seek": 287778, "start": 2893.7000000000003, "end": 2900.46, "text": " Neural net if you like now in practice you know we wouldn't make it one layer, so we would actually have like", "tokens": [1734, 1807, 2533, 498, 291, 411, 586, 294, 3124, 291, 458, 321, 2759, 380, 652, 309, 472, 4583, 11, 370, 321, 576, 767, 362, 411], "temperature": 0.0, "avg_logprob": -0.18977557032941336, "compression_ratio": 1.6256410256410256, "no_speech_prob": 4.860436547460267e-06}, {"id": 593, "seek": 290046, "start": 2900.46, "end": 2910.06, "text": " You know maybe we'd have 50 here and so then that gives us a 50 long vector and", "tokens": [509, 458, 1310, 321, 1116, 362, 2625, 510, 293, 370, 550, 300, 2709, 505, 257, 2625, 938, 8062, 293], "temperature": 0.0, "avg_logprob": -0.22031059265136718, "compression_ratio": 1.6523809523809523, "no_speech_prob": 3.726629074662924e-06}, {"id": 594, "seek": 290046, "start": 2911.2200000000003, "end": 2912.86, "text": " then", "tokens": [550], "temperature": 0.0, "avg_logprob": -0.22031059265136718, "compression_ratio": 1.6523809523809523, "no_speech_prob": 3.726629074662924e-06}, {"id": 595, "seek": 290046, "start": 2912.86, "end": 2915.06, "text": " Maybe we then put that into our final", "tokens": [2704, 321, 550, 829, 300, 666, 527, 2572], "temperature": 0.0, "avg_logprob": -0.22031059265136718, "compression_ratio": 1.6523809523809523, "no_speech_prob": 3.726629074662924e-06}, {"id": 596, "seek": 290046, "start": 2916.82, "end": 2918.82, "text": " 50 by one", "tokens": [2625, 538, 472], "temperature": 0.0, "avg_logprob": -0.22031059265136718, "compression_ratio": 1.6523809523809523, "no_speech_prob": 3.726629074662924e-06}, {"id": 597, "seek": 290046, "start": 2919.2200000000003, "end": 2924.94, "text": " And that spits out a single number and one reason I wanted to change that there was to point out. You know rally you", "tokens": [400, 300, 637, 1208, 484, 257, 2167, 1230, 293, 472, 1778, 286, 1415, 281, 1319, 300, 456, 390, 281, 935, 484, 13, 509, 458, 17584, 291], "temperature": 0.0, "avg_logprob": -0.22031059265136718, "compression_ratio": 1.6523809523809523, "no_speech_prob": 3.726629074662924e-06}, {"id": 598, "seek": 292494, "start": 2924.94, "end": 2931.78, "text": " You would never put value in the last layer. I could never want to throw away the negatives because that the softmax", "tokens": [509, 576, 1128, 829, 2158, 294, 264, 1036, 4583, 13, 286, 727, 1128, 528, 281, 3507, 1314, 264, 40019, 570, 300, 264, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.18616976803296234, "compression_ratio": 1.6684782608695652, "no_speech_prob": 3.844922048301669e-06}, {"id": 599, "seek": 292494, "start": 2935.54, "end": 2937.38, "text": " The softmax", "tokens": [440, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.18616976803296234, "compression_ratio": 1.6684782608695652, "no_speech_prob": 3.844922048301669e-06}, {"id": 600, "seek": 292494, "start": 2937.38, "end": 2942.86, "text": " Needs negatives in it because it's the negatives that are the things that allow it to create low probabilities", "tokens": [1734, 5147, 40019, 294, 309, 570, 309, 311, 264, 40019, 300, 366, 264, 721, 300, 2089, 309, 281, 1884, 2295, 33783], "temperature": 0.0, "avg_logprob": -0.18616976803296234, "compression_ratio": 1.6684782608695652, "no_speech_prob": 3.844922048301669e-06}, {"id": 601, "seek": 292494, "start": 2943.86, "end": 2948.02, "text": " That's minor detail, but it's useful to remember okay, so basically", "tokens": [663, 311, 6696, 2607, 11, 457, 309, 311, 4420, 281, 1604, 1392, 11, 370, 1936], "temperature": 0.0, "avg_logprob": -0.18616976803296234, "compression_ratio": 1.6684782608695652, "no_speech_prob": 3.844922048301669e-06}, {"id": 602, "seek": 294802, "start": 2948.02, "end": 2950.02, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.3925296995374892, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.209861352435837e-06}, {"id": 603, "seek": 294802, "start": 2954.46, "end": 2958.1, "text": " Basically a simple view of a", "tokens": [8537, 257, 2199, 1910, 295, 257], "temperature": 0.0, "avg_logprob": -0.3925296995374892, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.209861352435837e-06}, {"id": 604, "seek": 294802, "start": 2959.5, "end": 2961.1, "text": " Fully connected", "tokens": [479, 2150, 4582], "temperature": 0.0, "avg_logprob": -0.3925296995374892, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.209861352435837e-06}, {"id": 605, "seek": 294802, "start": 2961.1, "end": 2966.2599999999998, "text": " Neural net is something that takes in as an input a rank one tensor", "tokens": [1734, 1807, 2533, 307, 746, 300, 2516, 294, 382, 364, 4846, 257, 6181, 472, 40863], "temperature": 0.0, "avg_logprob": -0.3925296995374892, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.209861352435837e-06}, {"id": 606, "seek": 294802, "start": 2967.02, "end": 2969.74, "text": " it's bits it through a linear layer an", "tokens": [309, 311, 9239, 309, 807, 257, 8213, 4583, 364], "temperature": 0.0, "avg_logprob": -0.3925296995374892, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.209861352435837e-06}, {"id": 607, "seek": 294802, "start": 2971.78, "end": 2974.7, "text": " Activation layer another linear layer", "tokens": [28550, 399, 4583, 1071, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.3925296995374892, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.209861352435837e-06}, {"id": 608, "seek": 297470, "start": 2974.7, "end": 2977.7, "text": " a softmax and", "tokens": [257, 2787, 41167, 293], "temperature": 0.0, "avg_logprob": -0.22500338179341864, "compression_ratio": 1.5922330097087378, "no_speech_prob": 6.240908078325447e-06}, {"id": 609, "seek": 297470, "start": 2979.54, "end": 2981.54, "text": " That's the output", "tokens": [663, 311, 264, 5598], "temperature": 0.0, "avg_logprob": -0.22500338179341864, "compression_ratio": 1.5922330097087378, "no_speech_prob": 6.240908078325447e-06}, {"id": 610, "seek": 297470, "start": 2981.98, "end": 2983.14, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.22500338179341864, "compression_ratio": 1.5922330097087378, "no_speech_prob": 6.240908078325447e-06}, {"id": 611, "seek": 297470, "start": 2983.14, "end": 2986.8999999999996, "text": " And so we could obviously decide to add more", "tokens": [400, 370, 321, 727, 2745, 4536, 281, 909, 544], "temperature": 0.0, "avg_logprob": -0.22500338179341864, "compression_ratio": 1.5922330097087378, "no_speech_prob": 6.240908078325447e-06}, {"id": 612, "seek": 297470, "start": 2987.62, "end": 2991.02, "text": " Linear layers we could decide maybe to add dropout", "tokens": [14670, 289, 7914, 321, 727, 4536, 1310, 281, 909, 3270, 346], "temperature": 0.0, "avg_logprob": -0.22500338179341864, "compression_ratio": 1.5922330097087378, "no_speech_prob": 6.240908078325447e-06}, {"id": 613, "seek": 297470, "start": 2992.3399999999997, "end": 2999.1, "text": " Right so these are some of the decisions that we we get to make right, but we there's not that much we can do right?", "tokens": [1779, 370, 613, 366, 512, 295, 264, 5327, 300, 321, 321, 483, 281, 652, 558, 11, 457, 321, 456, 311, 406, 300, 709, 321, 393, 360, 558, 30], "temperature": 0.0, "avg_logprob": -0.22500338179341864, "compression_ratio": 1.5922330097087378, "no_speech_prob": 6.240908078325447e-06}, {"id": 614, "seek": 297470, "start": 2999.1, "end": 3003.3999999999996, "text": " There's not much really crazy architecture stuff to do so when we come back to", "tokens": [821, 311, 406, 709, 534, 3219, 9482, 1507, 281, 360, 370, 562, 321, 808, 646, 281], "temperature": 0.0, "avg_logprob": -0.22500338179341864, "compression_ratio": 1.5922330097087378, "no_speech_prob": 6.240908078325447e-06}, {"id": 615, "seek": 300340, "start": 3003.4, "end": 3004.88, "text": " image", "tokens": [3256], "temperature": 0.0, "avg_logprob": -0.19672614837361274, "compression_ratio": 1.721189591078067, "no_speech_prob": 1.6280462205031654e-06}, {"id": 616, "seek": 300340, "start": 3004.88, "end": 3010.88, "text": " Models later in the course we're going to learn about all the weird things that go on and like res nets and inception networks", "tokens": [6583, 1625, 1780, 294, 264, 1164, 321, 434, 516, 281, 1466, 466, 439, 264, 3657, 721, 300, 352, 322, 293, 411, 725, 36170, 293, 49834, 9590], "temperature": 0.0, "avg_logprob": -0.19672614837361274, "compression_ratio": 1.721189591078067, "no_speech_prob": 1.6280462205031654e-06}, {"id": 617, "seek": 300340, "start": 3010.88, "end": 3016.6800000000003, "text": " And blah blah blah, but in these fully connected networks. They're really pretty simple. They're just interspersed", "tokens": [400, 12288, 12288, 12288, 11, 457, 294, 613, 4498, 4582, 9590, 13, 814, 434, 534, 1238, 2199, 13, 814, 434, 445, 728, 4952, 433, 292], "temperature": 0.0, "avg_logprob": -0.19672614837361274, "compression_ratio": 1.721189591078067, "no_speech_prob": 1.6280462205031654e-06}, {"id": 618, "seek": 300340, "start": 3017.4, "end": 3019.6, "text": " linear layers that is matrix products and", "tokens": [8213, 7914, 300, 307, 8141, 3383, 293], "temperature": 0.0, "avg_logprob": -0.19672614837361274, "compression_ratio": 1.721189591078067, "no_speech_prob": 1.6280462205031654e-06}, {"id": 619, "seek": 300340, "start": 3020.4, "end": 3024.6800000000003, "text": " Activation functions like value and a softmax at the end", "tokens": [28550, 399, 6828, 411, 2158, 293, 257, 2787, 41167, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.19672614837361274, "compression_ratio": 1.721189591078067, "no_speech_prob": 1.6280462205031654e-06}, {"id": 620, "seek": 300340, "start": 3026.28, "end": 3031.92, "text": " And if it's not classification which actually ours is not classification in this case. We're trying to predict sales", "tokens": [400, 498, 309, 311, 406, 21538, 597, 767, 11896, 307, 406, 21538, 294, 341, 1389, 13, 492, 434, 1382, 281, 6069, 5763], "temperature": 0.0, "avg_logprob": -0.19672614837361274, "compression_ratio": 1.721189591078067, "no_speech_prob": 1.6280462205031654e-06}, {"id": 621, "seek": 303192, "start": 3031.92, "end": 3033.92, "text": " There isn't even a softmax", "tokens": [821, 1943, 380, 754, 257, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.15598824149683901, "compression_ratio": 1.6123348017621146, "no_speech_prob": 2.684186483747908e-06}, {"id": 622, "seek": 303192, "start": 3034.44, "end": 3036.64, "text": " Right we don't want it to be between 0 and 1", "tokens": [1779, 321, 500, 380, 528, 309, 281, 312, 1296, 1958, 293, 502], "temperature": 0.0, "avg_logprob": -0.15598824149683901, "compression_ratio": 1.6123348017621146, "no_speech_prob": 2.684186483747908e-06}, {"id": 623, "seek": 303192, "start": 3037.8, "end": 3041.6800000000003, "text": " Okay, so we can just throw away the last activation altogether", "tokens": [1033, 11, 370, 321, 393, 445, 3507, 1314, 264, 1036, 24433, 19051], "temperature": 0.0, "avg_logprob": -0.15598824149683901, "compression_ratio": 1.6123348017621146, "no_speech_prob": 2.684186483747908e-06}, {"id": 624, "seek": 303192, "start": 3043.08, "end": 3048.96, "text": " If we have time we can talk about a slight trick we can do there, but for now we can think of it that way", "tokens": [759, 321, 362, 565, 321, 393, 751, 466, 257, 4036, 4282, 321, 393, 360, 456, 11, 457, 337, 586, 321, 393, 519, 295, 309, 300, 636], "temperature": 0.0, "avg_logprob": -0.15598824149683901, "compression_ratio": 1.6123348017621146, "no_speech_prob": 2.684186483747908e-06}, {"id": 625, "seek": 303192, "start": 3049.8, "end": 3057.4, "text": " So that was all assuming that everything was continuous right, but what about categorical right so we've got like", "tokens": [407, 300, 390, 439, 11926, 300, 1203, 390, 10957, 558, 11, 457, 437, 466, 19250, 804, 558, 370, 321, 600, 658, 411], "temperature": 0.0, "avg_logprob": -0.15598824149683901, "compression_ratio": 1.6123348017621146, "no_speech_prob": 2.684186483747908e-06}, {"id": 626, "seek": 305740, "start": 3057.4, "end": 3062.4, "text": " a day of week right and", "tokens": [257, 786, 295, 1243, 558, 293], "temperature": 0.0, "avg_logprob": -0.2634183822139617, "compression_ratio": 1.448051948051948, "no_speech_prob": 3.966973963542841e-06}, {"id": 627, "seek": 305740, "start": 3064.48, "end": 3070.2000000000003, "text": " We're going to treat it as categorical right so it's like Saturday Sunday Monday", "tokens": [492, 434, 516, 281, 2387, 309, 382, 19250, 804, 558, 370, 309, 311, 411, 8803, 7776, 8138], "temperature": 0.0, "avg_logprob": -0.2634183822139617, "compression_ratio": 1.448051948051948, "no_speech_prob": 3.966973963542841e-06}, {"id": 628, "seek": 305740, "start": 3073.48, "end": 3075.48, "text": " Six Friday", "tokens": [11678, 6984], "temperature": 0.0, "avg_logprob": -0.2634183822139617, "compression_ratio": 1.448051948051948, "no_speech_prob": 3.966973963542841e-06}, {"id": 629, "seek": 305740, "start": 3076.76, "end": 3082.96, "text": " Okay, how do we feed that in because I want to find a way of getting that in so that we still end up with a", "tokens": [1033, 11, 577, 360, 321, 3154, 300, 294, 570, 286, 528, 281, 915, 257, 636, 295, 1242, 300, 294, 370, 300, 321, 920, 917, 493, 365, 257], "temperature": 0.0, "avg_logprob": -0.2634183822139617, "compression_ratio": 1.448051948051948, "no_speech_prob": 3.966973963542841e-06}, {"id": 630, "seek": 308296, "start": 3082.96, "end": 3089.28, "text": " rank one tensor of floats and so the trick is this we create a new little matrix of", "tokens": [6181, 472, 40863, 295, 37878, 293, 370, 264, 4282, 307, 341, 321, 1884, 257, 777, 707, 8141, 295], "temperature": 0.0, "avg_logprob": -0.23787333284105575, "compression_ratio": 1.5298507462686568, "no_speech_prob": 2.8573097097250866e-06}, {"id": 631, "seek": 308296, "start": 3091.64, "end": 3093.64, "text": " With seven rows and", "tokens": [2022, 3407, 13241, 293], "temperature": 0.0, "avg_logprob": -0.23787333284105575, "compression_ratio": 1.5298507462686568, "no_speech_prob": 2.8573097097250866e-06}, {"id": 632, "seek": 308296, "start": 3094.96, "end": 3100.92, "text": " As many columns as we choose right so let's pick four right so here's our", "tokens": [1018, 867, 13766, 382, 321, 2826, 558, 370, 718, 311, 1888, 1451, 558, 370, 510, 311, 527], "temperature": 0.0, "avg_logprob": -0.23787333284105575, "compression_ratio": 1.5298507462686568, "no_speech_prob": 2.8573097097250866e-06}, {"id": 633, "seek": 308296, "start": 3102.4, "end": 3104.4, "text": " Seven rows and", "tokens": [14868, 13241, 293], "temperature": 0.0, "avg_logprob": -0.23787333284105575, "compression_ratio": 1.5298507462686568, "no_speech_prob": 2.8573097097250866e-06}, {"id": 634, "seek": 308296, "start": 3104.88, "end": 3106.88, "text": " four columns", "tokens": [1451, 13766], "temperature": 0.0, "avg_logprob": -0.23787333284105575, "compression_ratio": 1.5298507462686568, "no_speech_prob": 2.8573097097250866e-06}, {"id": 635, "seek": 310688, "start": 3106.88, "end": 3112.9, "text": " All right, and basically what we do is let's add our categorical variables to the end", "tokens": [1057, 558, 11, 293, 1936, 437, 321, 360, 307, 718, 311, 909, 527, 19250, 804, 9102, 281, 264, 917], "temperature": 0.0, "avg_logprob": -0.15909567657782106, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.051148046826711e-06}, {"id": 636, "seek": 310688, "start": 3112.9, "end": 3115.4, "text": " So let's say the first row was Sunday", "tokens": [407, 718, 311, 584, 264, 700, 5386, 390, 7776], "temperature": 0.0, "avg_logprob": -0.15909567657782106, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.051148046826711e-06}, {"id": 637, "seek": 310688, "start": 3116.7200000000003, "end": 3121.6, "text": " Right then what we do is we do a lookup into this matrix and we say oh here's Sunday", "tokens": [1779, 550, 437, 321, 360, 307, 321, 360, 257, 574, 1010, 666, 341, 8141, 293, 321, 584, 1954, 510, 311, 7776], "temperature": 0.0, "avg_logprob": -0.15909567657782106, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.051148046826711e-06}, {"id": 638, "seek": 310688, "start": 3121.6400000000003, "end": 3124.36, "text": " We do a lookup into here, and we grab", "tokens": [492, 360, 257, 574, 1010, 666, 510, 11, 293, 321, 4444], "temperature": 0.0, "avg_logprob": -0.15909567657782106, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.051148046826711e-06}, {"id": 639, "seek": 310688, "start": 3124.96, "end": 3131.52, "text": " This row and so this matrix we basically fill with floating point numbers, so we're going to end up grabbing a", "tokens": [639, 5386, 293, 370, 341, 8141, 321, 1936, 2836, 365, 12607, 935, 3547, 11, 370, 321, 434, 516, 281, 917, 493, 23771, 257], "temperature": 0.0, "avg_logprob": -0.15909567657782106, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.051148046826711e-06}, {"id": 640, "seek": 310688, "start": 3132.56, "end": 3134.04, "text": " little", "tokens": [707], "temperature": 0.0, "avg_logprob": -0.15909567657782106, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.051148046826711e-06}, {"id": 641, "seek": 313404, "start": 3134.04, "end": 3140.72, "text": " Subset of four floating point numbers. It's Sundays particular for floating point numbers", "tokens": [8511, 3854, 295, 1451, 12607, 935, 3547, 13, 467, 311, 44857, 1729, 337, 12607, 935, 3547], "temperature": 0.0, "avg_logprob": -0.24619537063791783, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.276699195950641e-07}, {"id": 642, "seek": 313404, "start": 3141.52, "end": 3143.52, "text": " And so that way we convert", "tokens": [400, 370, 300, 636, 321, 7620], "temperature": 0.0, "avg_logprob": -0.24619537063791783, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.276699195950641e-07}, {"id": 643, "seek": 313404, "start": 3144.2, "end": 3145.7599999999998, "text": " Sunday", "tokens": [7776], "temperature": 0.0, "avg_logprob": -0.24619537063791783, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.276699195950641e-07}, {"id": 644, "seek": 313404, "start": 3145.7599999999998, "end": 3152.16, "text": " into a rank one tensor of four floating point numbers and initially those four numbers are", "tokens": [666, 257, 6181, 472, 40863, 295, 1451, 12607, 935, 3547, 293, 9105, 729, 1451, 3547, 366], "temperature": 0.0, "avg_logprob": -0.24619537063791783, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.276699195950641e-07}, {"id": 645, "seek": 313404, "start": 3152.6, "end": 3157.08, "text": " Random right and in fact this whole thing we initially start out", "tokens": [37603, 558, 293, 294, 1186, 341, 1379, 551, 321, 9105, 722, 484], "temperature": 0.0, "avg_logprob": -0.24619537063791783, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.276699195950641e-07}, {"id": 646, "seek": 313404, "start": 3158.04, "end": 3160.04, "text": " random okay", "tokens": [4974, 1392], "temperature": 0.0, "avg_logprob": -0.24619537063791783, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.276699195950641e-07}, {"id": 647, "seek": 313404, "start": 3160.16, "end": 3162.4, "text": " But then we're going to put that through our", "tokens": [583, 550, 321, 434, 516, 281, 829, 300, 807, 527], "temperature": 0.0, "avg_logprob": -0.24619537063791783, "compression_ratio": 1.7777777777777777, "no_speech_prob": 6.276699195950641e-07}, {"id": 648, "seek": 316240, "start": 3162.4, "end": 3169.0, "text": " Neural net right so we basically then take those four numbers, and we throw we remove Sunday instead we add", "tokens": [1734, 1807, 2533, 558, 370, 321, 1936, 550, 747, 729, 1451, 3547, 11, 293, 321, 3507, 321, 4159, 7776, 2602, 321, 909], "temperature": 0.0, "avg_logprob": -0.1924797621640292, "compression_ratio": 1.6919642857142858, "no_speech_prob": 2.1568134798144456e-06}, {"id": 649, "seek": 316240, "start": 3169.6800000000003, "end": 3176.38, "text": " Our four numbers on here right so we've turned our categorical thing into a floating point vector", "tokens": [2621, 1451, 3547, 322, 510, 558, 370, 321, 600, 3574, 527, 19250, 804, 551, 666, 257, 12607, 935, 8062], "temperature": 0.0, "avg_logprob": -0.1924797621640292, "compression_ratio": 1.6919642857142858, "no_speech_prob": 2.1568134798144456e-06}, {"id": 650, "seek": 316240, "start": 3176.92, "end": 3180.1, "text": " Right and so now we can just put that through our neural net", "tokens": [1779, 293, 370, 586, 321, 393, 445, 829, 300, 807, 527, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.1924797621640292, "compression_ratio": 1.6919642857142858, "no_speech_prob": 2.1568134798144456e-06}, {"id": 651, "seek": 316240, "start": 3180.56, "end": 3188.1600000000003, "text": " Just like before and at the very end we find out the loss and then we can figure out which direction is down and", "tokens": [1449, 411, 949, 293, 412, 264, 588, 917, 321, 915, 484, 264, 4470, 293, 550, 321, 393, 2573, 484, 597, 3513, 307, 760, 293], "temperature": 0.0, "avg_logprob": -0.1924797621640292, "compression_ratio": 1.6919642857142858, "no_speech_prob": 2.1568134798144456e-06}, {"id": 652, "seek": 318816, "start": 3188.16, "end": 3192.68, "text": " Do gradient descent in that direction and eventually that will find its way back", "tokens": [1144, 16235, 23475, 294, 300, 3513, 293, 4728, 300, 486, 915, 1080, 636, 646], "temperature": 0.0, "avg_logprob": -0.19035115317692833, "compression_ratio": 2.1061224489795918, "no_speech_prob": 1.392543481415487e-06}, {"id": 653, "seek": 318816, "start": 3193.3199999999997, "end": 3198.56, "text": " To this little list of four numbers, and it'll say okay those random numbers weren't very good", "tokens": [1407, 341, 707, 1329, 295, 1451, 3547, 11, 293, 309, 603, 584, 1392, 729, 4974, 3547, 4999, 380, 588, 665], "temperature": 0.0, "avg_logprob": -0.19035115317692833, "compression_ratio": 2.1061224489795918, "no_speech_prob": 1.392543481415487e-06}, {"id": 654, "seek": 318816, "start": 3198.6, "end": 3202.92, "text": " This one needs to go up a bit that one needs to go up a bit that one needs to go down a bit that one", "tokens": [639, 472, 2203, 281, 352, 493, 257, 857, 300, 472, 2203, 281, 352, 493, 257, 857, 300, 472, 2203, 281, 352, 760, 257, 857, 300, 472], "temperature": 0.0, "avg_logprob": -0.19035115317692833, "compression_ratio": 2.1061224489795918, "no_speech_prob": 1.392543481415487e-06}, {"id": 655, "seek": 318816, "start": 3202.92, "end": 3204.92, "text": " Needs to go up a bit and so we'll actually update", "tokens": [1734, 5147, 281, 352, 493, 257, 857, 293, 370, 321, 603, 767, 5623], "temperature": 0.0, "avg_logprob": -0.19035115317692833, "compression_ratio": 2.1061224489795918, "no_speech_prob": 1.392543481415487e-06}, {"id": 656, "seek": 318816, "start": 3205.96, "end": 3209.3199999999997, "text": " our original those four numbers in that matrix and", "tokens": [527, 3380, 729, 1451, 3547, 294, 300, 8141, 293], "temperature": 0.0, "avg_logprob": -0.19035115317692833, "compression_ratio": 2.1061224489795918, "no_speech_prob": 1.392543481415487e-06}, {"id": 657, "seek": 318816, "start": 3210.0, "end": 3211.7599999999998, "text": " We'll do this again and again and again", "tokens": [492, 603, 360, 341, 797, 293, 797, 293, 797], "temperature": 0.0, "avg_logprob": -0.19035115317692833, "compression_ratio": 2.1061224489795918, "no_speech_prob": 1.392543481415487e-06}, {"id": 658, "seek": 318816, "start": 3211.7599999999998, "end": 3217.66, "text": " And so this this matrix will stop looking random and it will start looking more and more like like", "tokens": [400, 370, 341, 341, 8141, 486, 1590, 1237, 4974, 293, 309, 486, 722, 1237, 544, 293, 544, 411, 411], "temperature": 0.0, "avg_logprob": -0.19035115317692833, "compression_ratio": 2.1061224489795918, "no_speech_prob": 1.392543481415487e-06}, {"id": 659, "seek": 321766, "start": 3217.66, "end": 3225.12, "text": " The exact four numbers that happen to work best for Sunday the exact four numbers that happen to work best for Friday and so forth", "tokens": [440, 1900, 1451, 3547, 300, 1051, 281, 589, 1151, 337, 7776, 264, 1900, 1451, 3547, 300, 1051, 281, 589, 1151, 337, 6984, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.2082745361328125, "compression_ratio": 1.8830409356725146, "no_speech_prob": 9.132527907240728e-07}, {"id": 660, "seek": 321766, "start": 3225.7, "end": 3231.02, "text": " And so in other words this matrix is just another bunch of weights", "tokens": [400, 370, 294, 661, 2283, 341, 8141, 307, 445, 1071, 3840, 295, 17443], "temperature": 0.0, "avg_logprob": -0.2082745361328125, "compression_ratio": 1.8830409356725146, "no_speech_prob": 9.132527907240728e-07}, {"id": 661, "seek": 321766, "start": 3231.94, "end": 3233.8199999999997, "text": " in our neural net", "tokens": [294, 527, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.2082745361328125, "compression_ratio": 1.8830409356725146, "no_speech_prob": 9.132527907240728e-07}, {"id": 662, "seek": 321766, "start": 3233.8199999999997, "end": 3237.18, "text": " right and so matrices of this type are called", "tokens": [558, 293, 370, 32284, 295, 341, 2010, 366, 1219], "temperature": 0.0, "avg_logprob": -0.2082745361328125, "compression_ratio": 1.8830409356725146, "no_speech_prob": 9.132527907240728e-07}, {"id": 663, "seek": 321766, "start": 3238.5, "end": 3240.5, "text": " embedding matrices", "tokens": [12240, 3584, 32284], "temperature": 0.0, "avg_logprob": -0.2082745361328125, "compression_ratio": 1.8830409356725146, "no_speech_prob": 9.132527907240728e-07}, {"id": 664, "seek": 324050, "start": 3240.5, "end": 3249.34, "text": " So an embedding matrix is something where we start out with an", "tokens": [407, 364, 12240, 3584, 8141, 307, 746, 689, 321, 722, 484, 365, 364], "temperature": 0.0, "avg_logprob": -0.24282146144557643, "compression_ratio": 1.594871794871795, "no_speech_prob": 1.0030109933722997e-06}, {"id": 665, "seek": 324050, "start": 3250.06, "end": 3251.34, "text": " integer", "tokens": [24922], "temperature": 0.0, "avg_logprob": -0.24282146144557643, "compression_ratio": 1.594871794871795, "no_speech_prob": 1.0030109933722997e-06}, {"id": 666, "seek": 324050, "start": 3251.34, "end": 3255.42, "text": " Between zero and the maximum number of levels of that category", "tokens": [18967, 4018, 293, 264, 6674, 1230, 295, 4358, 295, 300, 7719], "temperature": 0.0, "avg_logprob": -0.24282146144557643, "compression_ratio": 1.594871794871795, "no_speech_prob": 1.0030109933722997e-06}, {"id": 667, "seek": 324050, "start": 3256.42, "end": 3264.44, "text": " We literally index into a matrix to find a particular row so if it was the level was one we take the first row", "tokens": [492, 3736, 8186, 666, 257, 8141, 281, 915, 257, 1729, 5386, 370, 498, 309, 390, 264, 1496, 390, 472, 321, 747, 264, 700, 5386], "temperature": 0.0, "avg_logprob": -0.24282146144557643, "compression_ratio": 1.594871794871795, "no_speech_prob": 1.0030109933722997e-06}, {"id": 668, "seek": 326444, "start": 3264.44, "end": 3271.08, "text": " We grab that row and we append it to all of our continuous variables and", "tokens": [492, 4444, 300, 5386, 293, 321, 34116, 309, 281, 439, 295, 527, 10957, 9102, 293], "temperature": 0.0, "avg_logprob": -0.17175817489624023, "compression_ratio": 1.6380090497737556, "no_speech_prob": 2.5612744138925336e-06}, {"id": 669, "seek": 326444, "start": 3272.0, "end": 3274.0, "text": " So we now have a new", "tokens": [407, 321, 586, 362, 257, 777], "temperature": 0.0, "avg_logprob": -0.17175817489624023, "compression_ratio": 1.6380090497737556, "no_speech_prob": 2.5612744138925336e-06}, {"id": 670, "seek": 326444, "start": 3275.0, "end": 3279.54, "text": " Vector of continuous variables and then we can do the same thing so let's say zip code", "tokens": [691, 20814, 295, 10957, 9102, 293, 550, 321, 393, 360, 264, 912, 551, 370, 718, 311, 584, 20730, 3089], "temperature": 0.0, "avg_logprob": -0.17175817489624023, "compression_ratio": 1.6380090497737556, "no_speech_prob": 2.5612744138925336e-06}, {"id": 671, "seek": 326444, "start": 3280.32, "end": 3285.2400000000002, "text": " Right so we could like have an embedding matrix. Let's say there are 5,000 zip codes", "tokens": [1779, 370, 321, 727, 411, 362, 364, 12240, 3584, 8141, 13, 961, 311, 584, 456, 366, 1025, 11, 1360, 20730, 14211], "temperature": 0.0, "avg_logprob": -0.17175817489624023, "compression_ratio": 1.6380090497737556, "no_speech_prob": 2.5612744138925336e-06}, {"id": 672, "seek": 326444, "start": 3285.32, "end": 3292.16, "text": " It would be 5,000 rows long as wide as we decide maybe it's 50 wide and so we'd say okay. Here's", "tokens": [467, 576, 312, 1025, 11, 1360, 13241, 938, 382, 4874, 382, 321, 4536, 1310, 309, 311, 2625, 4874, 293, 370, 321, 1116, 584, 1392, 13, 1692, 311], "temperature": 0.0, "avg_logprob": -0.17175817489624023, "compression_ratio": 1.6380090497737556, "no_speech_prob": 2.5612744138925336e-06}, {"id": 673, "seek": 329216, "start": 3292.16, "end": 3295.0, "text": " 9 4 0 0 3", "tokens": [1722, 1017, 1958, 1958, 805], "temperature": 0.0, "avg_logprob": -0.28145544307748066, "compression_ratio": 1.6345381526104417, "no_speech_prob": 5.014713224227307e-06}, {"id": 674, "seek": 329216, "start": 3295.48, "end": 3300.2, "text": " That zip code is index number 4 in our matrix so go down and we find the fourth row", "tokens": [663, 20730, 3089, 307, 8186, 1230, 1017, 294, 527, 8141, 370, 352, 760, 293, 321, 915, 264, 6409, 5386], "temperature": 0.0, "avg_logprob": -0.28145544307748066, "compression_ratio": 1.6345381526104417, "no_speech_prob": 5.014713224227307e-06}, {"id": 675, "seek": 329216, "start": 3300.44, "end": 3303.2799999999997, "text": " We'll grab those 50 numbers and append those", "tokens": [492, 603, 4444, 729, 2625, 3547, 293, 34116, 729], "temperature": 0.0, "avg_logprob": -0.28145544307748066, "compression_ratio": 1.6345381526104417, "no_speech_prob": 5.014713224227307e-06}, {"id": 676, "seek": 329216, "start": 3303.92, "end": 3309.94, "text": " Onto our big vector and then everything after that is just the same we just put it through our linear layer value linear layer", "tokens": [16980, 78, 527, 955, 8062, 293, 550, 1203, 934, 300, 307, 445, 264, 912, 321, 445, 829, 309, 807, 527, 8213, 4583, 2158, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.28145544307748066, "compression_ratio": 1.6345381526104417, "no_speech_prob": 5.014713224227307e-06}, {"id": 677, "seek": 329216, "start": 3313.16, "end": 3315.16, "text": " What are those four numbers", "tokens": [708, 366, 729, 1451, 3547], "temperature": 0.0, "avg_logprob": -0.28145544307748066, "compression_ratio": 1.6345381526104417, "no_speech_prob": 5.014713224227307e-06}, {"id": 678, "seek": 329216, "start": 3315.52, "end": 3320.7599999999998, "text": " Represent that's a great question and we'll learn more about that without when we look at collaborative filtering", "tokens": [19945, 300, 311, 257, 869, 1168, 293, 321, 603, 1466, 544, 466, 300, 1553, 562, 321, 574, 412, 16555, 30822], "temperature": 0.0, "avg_logprob": -0.28145544307748066, "compression_ratio": 1.6345381526104417, "no_speech_prob": 5.014713224227307e-06}, {"id": 679, "seek": 332076, "start": 3320.76, "end": 3328.6400000000003, "text": " For now they represent no more or no less than any other parameter in our neural net you know they're just", "tokens": [1171, 586, 436, 2906, 572, 544, 420, 572, 1570, 813, 604, 661, 13075, 294, 527, 18161, 2533, 291, 458, 436, 434, 445], "temperature": 0.0, "avg_logprob": -0.1813999811808268, "compression_ratio": 1.7338709677419355, "no_speech_prob": 3.5008358736376977e-06}, {"id": 680, "seek": 332076, "start": 3329.84, "end": 3334.7400000000002, "text": " They're just parameters that we're learning that happen to end up giving us a good loss", "tokens": [814, 434, 445, 9834, 300, 321, 434, 2539, 300, 1051, 281, 917, 493, 2902, 505, 257, 665, 4470], "temperature": 0.0, "avg_logprob": -0.1813999811808268, "compression_ratio": 1.7338709677419355, "no_speech_prob": 3.5008358736376977e-06}, {"id": 681, "seek": 332076, "start": 3335.76, "end": 3342.28, "text": " We will discover later that these particular parameters often however are human interpretable and quite can be quite interesting", "tokens": [492, 486, 4411, 1780, 300, 613, 1729, 9834, 2049, 4461, 366, 1952, 7302, 712, 293, 1596, 393, 312, 1596, 1880], "temperature": 0.0, "avg_logprob": -0.1813999811808268, "compression_ratio": 1.7338709677419355, "no_speech_prob": 3.5008358736376977e-06}, {"id": 682, "seek": 332076, "start": 3342.5200000000004, "end": 3345.6800000000003, "text": " But that's a side effect of them. It's not", "tokens": [583, 300, 311, 257, 1252, 1802, 295, 552, 13, 467, 311, 406], "temperature": 0.0, "avg_logprob": -0.1813999811808268, "compression_ratio": 1.7338709677419355, "no_speech_prob": 3.5008358736376977e-06}, {"id": 683, "seek": 334568, "start": 3345.68, "end": 3352.96, "text": " Fundamental they're just four random numbers for now that we're that we're learning or sets of four random numbers", "tokens": [13493, 44538, 436, 434, 445, 1451, 4974, 3547, 337, 586, 300, 321, 434, 300, 321, 434, 2539, 420, 6352, 295, 1451, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.20932827677045548, "compression_ratio": 1.5069444444444444, "no_speech_prob": 1.1478653505037073e-05}, {"id": 684, "seek": 334568, "start": 3356.8799999999997, "end": 3363.46, "text": " To have a good heuristic for the dimensionality of the embedding matrix, so why four here I sure do", "tokens": [1407, 362, 257, 665, 415, 374, 3142, 337, 264, 10139, 1860, 295, 264, 12240, 3584, 8141, 11, 370, 983, 1451, 510, 286, 988, 360], "temperature": 0.0, "avg_logprob": -0.20932827677045548, "compression_ratio": 1.5069444444444444, "no_speech_prob": 1.1478653505037073e-05}, {"id": 685, "seek": 334568, "start": 3369.44, "end": 3370.96, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.20932827677045548, "compression_ratio": 1.5069444444444444, "no_speech_prob": 1.1478653505037073e-05}, {"id": 686, "seek": 337096, "start": 3370.96, "end": 3377.5, "text": " What I first of all did was I made a little list of every categorical variable and its cardinality", "tokens": [708, 286, 700, 295, 439, 630, 390, 286, 1027, 257, 707, 1329, 295, 633, 19250, 804, 7006, 293, 1080, 2920, 259, 1860], "temperature": 0.0, "avg_logprob": -0.14852187508030942, "compression_ratio": 1.6333333333333333, "no_speech_prob": 4.565949438983807e-06}, {"id": 687, "seek": 337096, "start": 3378.44, "end": 3383.48, "text": " Okay, so there they all are so there's a hundred and that's a thousand plus different stores", "tokens": [1033, 11, 370, 456, 436, 439, 366, 370, 456, 311, 257, 3262, 293, 300, 311, 257, 4714, 1804, 819, 9512], "temperature": 0.0, "avg_logprob": -0.14852187508030942, "compression_ratio": 1.6333333333333333, "no_speech_prob": 4.565949438983807e-06}, {"id": 688, "seek": 337096, "start": 3384.64, "end": 3386.64, "text": " Apparently in Rossman's Network", "tokens": [16755, 294, 16140, 1601, 311, 12640], "temperature": 0.0, "avg_logprob": -0.14852187508030942, "compression_ratio": 1.6333333333333333, "no_speech_prob": 4.565949438983807e-06}, {"id": 689, "seek": 337096, "start": 3387.08, "end": 3388.76, "text": " There are eight days of the week", "tokens": [821, 366, 3180, 1708, 295, 264, 1243], "temperature": 0.0, "avg_logprob": -0.14852187508030942, "compression_ratio": 1.6333333333333333, "no_speech_prob": 4.565949438983807e-06}, {"id": 690, "seek": 337096, "start": 3388.76, "end": 3392.7200000000003, "text": " That's because there are seven days of the week plus one leftover for unknown", "tokens": [663, 311, 570, 456, 366, 3407, 1708, 295, 264, 1243, 1804, 472, 27373, 337, 9841], "temperature": 0.0, "avg_logprob": -0.14852187508030942, "compression_ratio": 1.6333333333333333, "no_speech_prob": 4.565949438983807e-06}, {"id": 691, "seek": 337096, "start": 3393.04, "end": 3396.08, "text": " Even if there were no missing values in the original data", "tokens": [2754, 498, 456, 645, 572, 5361, 4190, 294, 264, 3380, 1412], "temperature": 0.0, "avg_logprob": -0.14852187508030942, "compression_ratio": 1.6333333333333333, "no_speech_prob": 4.565949438983807e-06}, {"id": 692, "seek": 339608, "start": 3396.08, "end": 3401.92, "text": " I always still set aside one just in case there's a missing or an unknown or something different in the test set", "tokens": [286, 1009, 920, 992, 7359, 472, 445, 294, 1389, 456, 311, 257, 5361, 420, 364, 9841, 420, 746, 819, 294, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.13140236152397408, "compression_ratio": 1.509009009009009, "no_speech_prob": 2.642580511746928e-06}, {"id": 693, "seek": 339608, "start": 3403.0, "end": 3409.4, "text": " Again four years, but there's actually three plus room for an unknown and so forth right so what I do", "tokens": [3764, 1451, 924, 11, 457, 456, 311, 767, 1045, 1804, 1808, 337, 364, 9841, 293, 370, 5220, 558, 370, 437, 286, 360], "temperature": 0.0, "avg_logprob": -0.13140236152397408, "compression_ratio": 1.509009009009009, "no_speech_prob": 2.642580511746928e-06}, {"id": 694, "seek": 339608, "start": 3410.3199999999997, "end": 3412.3199999999997, "text": " My rule of thumb is this", "tokens": [1222, 4978, 295, 9298, 307, 341], "temperature": 0.0, "avg_logprob": -0.13140236152397408, "compression_ratio": 1.509009009009009, "no_speech_prob": 2.642580511746928e-06}, {"id": 695, "seek": 339608, "start": 3414.12, "end": 3416.62, "text": " Take the cardinality of the variable", "tokens": [3664, 264, 2920, 259, 1860, 295, 264, 7006], "temperature": 0.0, "avg_logprob": -0.13140236152397408, "compression_ratio": 1.509009009009009, "no_speech_prob": 2.642580511746928e-06}, {"id": 696, "seek": 339608, "start": 3417.68, "end": 3419.68, "text": " Divide it by two", "tokens": [9886, 482, 309, 538, 732], "temperature": 0.0, "avg_logprob": -0.13140236152397408, "compression_ratio": 1.509009009009009, "no_speech_prob": 2.642580511746928e-06}, {"id": 697, "seek": 339608, "start": 3419.72, "end": 3421.72, "text": " But don't make it bigger than 50", "tokens": [583, 500, 380, 652, 309, 3801, 813, 2625], "temperature": 0.0, "avg_logprob": -0.13140236152397408, "compression_ratio": 1.509009009009009, "no_speech_prob": 2.642580511746928e-06}, {"id": 698, "seek": 339608, "start": 3422.72, "end": 3424.72, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.13140236152397408, "compression_ratio": 1.509009009009009, "no_speech_prob": 2.642580511746928e-06}, {"id": 699, "seek": 342472, "start": 3424.72, "end": 3432.68, "text": " These are my embedding matrices so my store matrix so they've that has to have a thousand one hundred and sixteen rows", "tokens": [1981, 366, 452, 12240, 3584, 32284, 370, 452, 3531, 8141, 370, 436, 600, 300, 575, 281, 362, 257, 4714, 472, 3262, 293, 27847, 13241], "temperature": 0.0, "avg_logprob": -0.2294702973476676, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.9771612389595248e-05}, {"id": 700, "seek": 342472, "start": 3432.7999999999997, "end": 3438.3999999999996, "text": " Because I need to look up right to find his store number three, and then it's going to return back a", "tokens": [1436, 286, 643, 281, 574, 493, 558, 281, 915, 702, 3531, 1230, 1045, 11, 293, 550, 309, 311, 516, 281, 2736, 646, 257], "temperature": 0.0, "avg_logprob": -0.2294702973476676, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.9771612389595248e-05}, {"id": 701, "seek": 342472, "start": 3439.3999999999996, "end": 3441.3999999999996, "text": " Rank one tensor of length 50", "tokens": [35921, 472, 40863, 295, 4641, 2625], "temperature": 0.0, "avg_logprob": -0.2294702973476676, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.9771612389595248e-05}, {"id": 702, "seek": 342472, "start": 3441.9599999999996, "end": 3448.4399999999996, "text": " Day of week, it's going to look up into which one of the eight and return the thing of length four", "tokens": [5226, 295, 1243, 11, 309, 311, 516, 281, 574, 493, 666, 597, 472, 295, 264, 3180, 293, 2736, 264, 551, 295, 4641, 1451], "temperature": 0.0, "avg_logprob": -0.2294702973476676, "compression_ratio": 1.6214953271028036, "no_speech_prob": 2.9771612389595248e-05}, {"id": 703, "seek": 344844, "start": 3448.44, "end": 3455.76, "text": " So what you typically build an embedding matrix for each categorical feature yes", "tokens": [407, 437, 291, 5850, 1322, 364, 12240, 3584, 8141, 337, 1184, 19250, 804, 4111, 2086], "temperature": 0.0, "avg_logprob": -0.2534087685977711, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.1125490345875733e-05}, {"id": 704, "seek": 344844, "start": 3456.8, "end": 3459.38, "text": " Yeah, so that's what I've done here, so I've said", "tokens": [865, 11, 370, 300, 311, 437, 286, 600, 1096, 510, 11, 370, 286, 600, 848], "temperature": 0.0, "avg_logprob": -0.2534087685977711, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.1125490345875733e-05}, {"id": 705, "seek": 344844, "start": 3461.16, "end": 3464.14, "text": " For C in categorical variables", "tokens": [1171, 383, 294, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.2534087685977711, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.1125490345875733e-05}, {"id": 706, "seek": 344844, "start": 3465.64, "end": 3468.12, "text": " See how many categories there are and", "tokens": [3008, 577, 867, 10479, 456, 366, 293], "temperature": 0.0, "avg_logprob": -0.2534087685977711, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.1125490345875733e-05}, {"id": 707, "seek": 344844, "start": 3469.28, "end": 3472.16, "text": " then for each of those things", "tokens": [550, 337, 1184, 295, 729, 721], "temperature": 0.0, "avg_logprob": -0.2534087685977711, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.1125490345875733e-05}, {"id": 708, "seek": 344844, "start": 3473.28, "end": 3475.28, "text": " create one of these and", "tokens": [1884, 472, 295, 613, 293], "temperature": 0.0, "avg_logprob": -0.2534087685977711, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.1125490345875733e-05}, {"id": 709, "seek": 347528, "start": 3475.28, "end": 3482.1600000000003, "text": " Then this is called embedding sizes, and then you may have noticed that that's actually the first thing that we pass", "tokens": [1396, 341, 307, 1219, 12240, 3584, 11602, 11, 293, 550, 291, 815, 362, 5694, 300, 300, 311, 767, 264, 700, 551, 300, 321, 1320], "temperature": 0.0, "avg_logprob": -0.2600296974182129, "compression_ratio": 1.680952380952381, "no_speech_prob": 5.594285084953299e-06}, {"id": 710, "seek": 347528, "start": 3482.36, "end": 3489.6800000000003, "text": " To get learner and so that tells it for every categorical variable. That's the embedding matrix to use for that variable", "tokens": [1407, 483, 33347, 293, 370, 300, 5112, 309, 337, 633, 19250, 804, 7006, 13, 663, 311, 264, 12240, 3584, 8141, 281, 764, 337, 300, 7006], "temperature": 0.0, "avg_logprob": -0.2600296974182129, "compression_ratio": 1.680952380952381, "no_speech_prob": 5.594285084953299e-06}, {"id": 711, "seek": 347528, "start": 3490.32, "end": 3492.44, "text": " Just behind you doesn't yes", "tokens": [1449, 2261, 291, 1177, 380, 2086], "temperature": 0.0, "avg_logprob": -0.2600296974182129, "compression_ratio": 1.680952380952381, "no_speech_prob": 5.594285084953299e-06}, {"id": 712, "seek": 347528, "start": 3494.76, "end": 3501.0400000000004, "text": " So besides our random initialization are there other ways actually initialize embedding", "tokens": [407, 11868, 527, 4974, 5883, 2144, 366, 456, 661, 2098, 767, 5883, 1125, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.2600296974182129, "compression_ratio": 1.680952380952381, "no_speech_prob": 5.594285084953299e-06}, {"id": 713, "seek": 350104, "start": 3501.04, "end": 3503.04, "text": " Yes", "tokens": [1079], "temperature": 0.0, "avg_logprob": -0.21426006278606377, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.9637982404674403e-06}, {"id": 714, "seek": 350104, "start": 3503.92, "end": 3508.44, "text": " And no there's two ways one is random the other is pre trained and", "tokens": [400, 572, 456, 311, 732, 2098, 472, 307, 4974, 264, 661, 307, 659, 8895, 293], "temperature": 0.0, "avg_logprob": -0.21426006278606377, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.9637982404674403e-06}, {"id": 715, "seek": 350104, "start": 3509.32, "end": 3515.04, "text": " We'll probably talk about pre trained more later in the course, but the basic idea though is if somebody else at Rossman", "tokens": [492, 603, 1391, 751, 466, 659, 8895, 544, 1780, 294, 264, 1164, 11, 457, 264, 3875, 1558, 1673, 307, 498, 2618, 1646, 412, 16140, 1601], "temperature": 0.0, "avg_logprob": -0.21426006278606377, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.9637982404674403e-06}, {"id": 716, "seek": 350104, "start": 3515.04, "end": 3517.04, "text": " Who'd already trained a neural net?", "tokens": [2102, 1116, 1217, 8895, 257, 18161, 2533, 30], "temperature": 0.0, "avg_logprob": -0.21426006278606377, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.9637982404674403e-06}, {"id": 717, "seek": 350104, "start": 3517.2, "end": 3522.0, "text": " Just like you you would use a pre trained net from image net to look at pictures of cats and dogs", "tokens": [1449, 411, 291, 291, 576, 764, 257, 659, 8895, 2533, 490, 3256, 2533, 281, 574, 412, 5242, 295, 11111, 293, 7197], "temperature": 0.0, "avg_logprob": -0.21426006278606377, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.9637982404674403e-06}, {"id": 718, "seek": 350104, "start": 3522.2, "end": 3527.08, "text": " If somebody else has pre trained a network to predict cheese sales in Rossman", "tokens": [759, 2618, 1646, 575, 659, 8895, 257, 3209, 281, 6069, 5399, 5763, 294, 16140, 1601], "temperature": 0.0, "avg_logprob": -0.21426006278606377, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.9637982404674403e-06}, {"id": 719, "seek": 352708, "start": 3527.08, "end": 3532.64, "text": " You may as well start with their embedding matrix of stores to predict liquor sales in Rossman", "tokens": [509, 815, 382, 731, 722, 365, 641, 12240, 3584, 8141, 295, 9512, 281, 6069, 29162, 5763, 294, 16140, 1601], "temperature": 0.0, "avg_logprob": -0.1809111143413343, "compression_ratio": 1.7180616740088106, "no_speech_prob": 5.0936773732246365e-06}, {"id": 720, "seek": 352708, "start": 3533.16, "end": 3535.2799999999997, "text": " And this is what happens for example at", "tokens": [400, 341, 307, 437, 2314, 337, 1365, 412], "temperature": 0.0, "avg_logprob": -0.1809111143413343, "compression_ratio": 1.7180616740088106, "no_speech_prob": 5.0936773732246365e-06}, {"id": 721, "seek": 352708, "start": 3536.08, "end": 3543.2, "text": " At Pinterest and instacart they both use this technique instacart uses it for routing their shoppers", "tokens": [1711, 37986, 293, 1058, 326, 446, 436, 1293, 764, 341, 6532, 1058, 326, 446, 4960, 309, 337, 32722, 641, 3945, 21819], "temperature": 0.0, "avg_logprob": -0.1809111143413343, "compression_ratio": 1.7180616740088106, "no_speech_prob": 5.0936773732246365e-06}, {"id": 722, "seek": 352708, "start": 3543.64, "end": 3548.92, "text": " Pinterest uses it for deciding what to display on a web page when you go there, and they have", "tokens": [37986, 4960, 309, 337, 17990, 437, 281, 4674, 322, 257, 3670, 3028, 562, 291, 352, 456, 11, 293, 436, 362], "temperature": 0.0, "avg_logprob": -0.1809111143413343, "compression_ratio": 1.7180616740088106, "no_speech_prob": 5.0936773732246365e-06}, {"id": 723, "seek": 352708, "start": 3549.56, "end": 3551.56, "text": " embedding matrices of products", "tokens": [12240, 3584, 32284, 295, 3383], "temperature": 0.0, "avg_logprob": -0.1809111143413343, "compression_ratio": 1.7180616740088106, "no_speech_prob": 5.0936773732246365e-06}, {"id": 724, "seek": 352708, "start": 3551.96, "end": 3554.64, "text": " in instacart's case of stores", "tokens": [294, 1058, 326, 446, 311, 1389, 295, 9512], "temperature": 0.0, "avg_logprob": -0.1809111143413343, "compression_ratio": 1.7180616740088106, "no_speech_prob": 5.0936773732246365e-06}, {"id": 725, "seek": 355464, "start": 3554.64, "end": 3559.7999999999997, "text": " That gets shared in the organization so people don't have to train new ones", "tokens": [663, 2170, 5507, 294, 264, 4475, 370, 561, 500, 380, 362, 281, 3847, 777, 2306], "temperature": 0.0, "avg_logprob": -0.19812993035800214, "compression_ratio": 1.541899441340782, "no_speech_prob": 1.3211553778091911e-05}, {"id": 726, "seek": 355464, "start": 3563.7599999999998, "end": 3565.7599999999998, "text": " So for the embedding size", "tokens": [407, 337, 264, 12240, 3584, 2744], "temperature": 0.0, "avg_logprob": -0.19812993035800214, "compression_ratio": 1.541899441340782, "no_speech_prob": 1.3211553778091911e-05}, {"id": 727, "seek": 355464, "start": 3567.3599999999997, "end": 3571.4, "text": " Why wouldn't you just use like the one-hot scheme and just", "tokens": [1545, 2759, 380, 291, 445, 764, 411, 264, 472, 12, 12194, 12232, 293, 445], "temperature": 0.0, "avg_logprob": -0.19812993035800214, "compression_ratio": 1.541899441340782, "no_speech_prob": 1.3211553778091911e-05}, {"id": 728, "seek": 355464, "start": 3572.52, "end": 3580.48, "text": " What is the advantage of doing this as opposed to just doing a question so so we could easily as you point out have", "tokens": [708, 307, 264, 5002, 295, 884, 341, 382, 8851, 281, 445, 884, 257, 1168, 370, 370, 321, 727, 3612, 382, 291, 935, 484, 362], "temperature": 0.0, "avg_logprob": -0.19812993035800214, "compression_ratio": 1.541899441340782, "no_speech_prob": 1.3211553778091911e-05}, {"id": 729, "seek": 358048, "start": 3580.48, "end": 3587.68, "text": " Instead of passing in these four numbers we could instead of passed in seven numbers", "tokens": [7156, 295, 8437, 294, 613, 1451, 3547, 321, 727, 2602, 295, 4678, 294, 3407, 3547], "temperature": 0.0, "avg_logprob": -0.2297384422945689, "compression_ratio": 1.625, "no_speech_prob": 5.42216230314807e-06}, {"id": 730, "seek": 358048, "start": 3588.56, "end": 3593.32, "text": " all zeros, but one of them is a one and that also is a list of floats and", "tokens": [439, 35193, 11, 457, 472, 295, 552, 307, 257, 472, 293, 300, 611, 307, 257, 1329, 295, 37878, 293], "temperature": 0.0, "avg_logprob": -0.2297384422945689, "compression_ratio": 1.625, "no_speech_prob": 5.42216230314807e-06}, {"id": 731, "seek": 358048, "start": 3594.8, "end": 3596.8, "text": " That would totally work", "tokens": [663, 576, 3879, 589], "temperature": 0.0, "avg_logprob": -0.2297384422945689, "compression_ratio": 1.625, "no_speech_prob": 5.42216230314807e-06}, {"id": 732, "seek": 358048, "start": 3597.04, "end": 3599.04, "text": " And that's how", "tokens": [400, 300, 311, 577], "temperature": 0.0, "avg_logprob": -0.2297384422945689, "compression_ratio": 1.625, "no_speech_prob": 5.42216230314807e-06}, {"id": 733, "seek": 358048, "start": 3599.88, "end": 3606.48, "text": " Generally speaking categorical variables have been used in statistics for many years. It's called dummy variable coding", "tokens": [21082, 4124, 19250, 804, 9102, 362, 668, 1143, 294, 12523, 337, 867, 924, 13, 467, 311, 1219, 35064, 7006, 17720], "temperature": 0.0, "avg_logprob": -0.2297384422945689, "compression_ratio": 1.625, "no_speech_prob": 5.42216230314807e-06}, {"id": 734, "seek": 360648, "start": 3606.48, "end": 3609.76, "text": " The problem is that in that case?", "tokens": [440, 1154, 307, 300, 294, 300, 1389, 30], "temperature": 0.0, "avg_logprob": -0.2036355181438167, "compression_ratio": 1.5426008968609866, "no_speech_prob": 8.851548614075e-07}, {"id": 735, "seek": 360648, "start": 3610.56, "end": 3612.56, "text": " the concept of Sunday", "tokens": [264, 3410, 295, 7776], "temperature": 0.0, "avg_logprob": -0.2036355181438167, "compression_ratio": 1.5426008968609866, "no_speech_prob": 8.851548614075e-07}, {"id": 736, "seek": 360648, "start": 3613.32, "end": 3616.86, "text": " Could only ever be associated with a single floating point number", "tokens": [7497, 787, 1562, 312, 6615, 365, 257, 2167, 12607, 935, 1230], "temperature": 0.0, "avg_logprob": -0.2036355181438167, "compression_ratio": 1.5426008968609866, "no_speech_prob": 8.851548614075e-07}, {"id": 737, "seek": 360648, "start": 3617.6, "end": 3625.98, "text": " Right and so it basically gets this kind of linear behavior. It says like Sunday is more or less of a single thing", "tokens": [1779, 293, 370, 309, 1936, 2170, 341, 733, 295, 8213, 5223, 13, 467, 1619, 411, 7776, 307, 544, 420, 1570, 295, 257, 2167, 551], "temperature": 0.0, "avg_logprob": -0.2036355181438167, "compression_ratio": 1.5426008968609866, "no_speech_prob": 8.851548614075e-07}, {"id": 738, "seek": 360648, "start": 3627.16, "end": 3633.0, "text": " Yeah, what's not just interactions is saying like now Sunday is a concept in four-dimensional space, right?", "tokens": [865, 11, 437, 311, 406, 445, 13280, 307, 1566, 411, 586, 7776, 307, 257, 3410, 294, 1451, 12, 18759, 1901, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2036355181438167, "compression_ratio": 1.5426008968609866, "no_speech_prob": 8.851548614075e-07}, {"id": 739, "seek": 363300, "start": 3633.0, "end": 3636.78, "text": " And so what we tend to find happen is that these?", "tokens": [400, 370, 437, 321, 3928, 281, 915, 1051, 307, 300, 613, 30], "temperature": 0.0, "avg_logprob": -0.22496932203119452, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.936957793688634e-06}, {"id": 740, "seek": 363300, "start": 3637.52, "end": 3639.36, "text": " embedding vectors", "tokens": [12240, 3584, 18875], "temperature": 0.0, "avg_logprob": -0.22496932203119452, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.936957793688634e-06}, {"id": 741, "seek": 363300, "start": 3639.36, "end": 3643.76, "text": " Tend to get these kind of rich semantic concepts so for example", "tokens": [314, 521, 281, 483, 613, 733, 295, 4593, 47982, 10392, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.22496932203119452, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.936957793688634e-06}, {"id": 742, "seek": 363300, "start": 3644.52, "end": 3646.52, "text": " if it turns out that", "tokens": [498, 309, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.22496932203119452, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.936957793688634e-06}, {"id": 743, "seek": 363300, "start": 3647.88, "end": 3649.6, "text": " weekends", "tokens": [23595], "temperature": 0.0, "avg_logprob": -0.22496932203119452, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.936957793688634e-06}, {"id": 744, "seek": 363300, "start": 3649.6, "end": 3655.92, "text": " Kind of have a different behavior you'll tend to see that Saturday and Sunday will have like some particular number higher or", "tokens": [9242, 295, 362, 257, 819, 5223, 291, 603, 3928, 281, 536, 300, 8803, 293, 7776, 486, 362, 411, 512, 1729, 1230, 2946, 420], "temperature": 0.0, "avg_logprob": -0.22496932203119452, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.936957793688634e-06}, {"id": 745, "seek": 365592, "start": 3655.92, "end": 3663.28, "text": " more likely it turns out that certain days of the week are associated with higher sales of", "tokens": [544, 3700, 309, 4523, 484, 300, 1629, 1708, 295, 264, 1243, 366, 6615, 365, 2946, 5763, 295], "temperature": 0.0, "avg_logprob": -0.17232282044457609, "compression_ratio": 1.5337423312883436, "no_speech_prob": 1.5056970141813508e-06}, {"id": 746, "seek": 365592, "start": 3666.96, "end": 3672.64, "text": " Certain kinds of goods that you kind of can't go without I don't know like gas or milk say", "tokens": [13407, 3685, 295, 10179, 300, 291, 733, 295, 393, 380, 352, 1553, 286, 500, 380, 458, 411, 4211, 420, 5392, 584], "temperature": 0.0, "avg_logprob": -0.17232282044457609, "compression_ratio": 1.5337423312883436, "no_speech_prob": 1.5056970141813508e-06}, {"id": 747, "seek": 365592, "start": 3673.32, "end": 3676.12, "text": " Where else there might be other products like?", "tokens": [2305, 1646, 456, 1062, 312, 661, 3383, 411, 30], "temperature": 0.0, "avg_logprob": -0.17232282044457609, "compression_ratio": 1.5337423312883436, "no_speech_prob": 1.5056970141813508e-06}, {"id": 748, "seek": 365592, "start": 3677.2400000000002, "end": 3679.2400000000002, "text": " like wine for example", "tokens": [411, 7209, 337, 1365], "temperature": 0.0, "avg_logprob": -0.17232282044457609, "compression_ratio": 1.5337423312883436, "no_speech_prob": 1.5056970141813508e-06}, {"id": 749, "seek": 367924, "start": 3679.24, "end": 3686.7599999999998, "text": " Like wine that tend to be associated with like the days before weekends or holidays", "tokens": [1743, 7209, 300, 3928, 281, 312, 6615, 365, 411, 264, 1708, 949, 23595, 420, 15734], "temperature": 0.0, "avg_logprob": -0.1919528584421417, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.769389539025724e-06}, {"id": 750, "seek": 367924, "start": 3687.3999999999996, "end": 3690.3199999999997, "text": " right, so there might be kind of a column which is like", "tokens": [558, 11, 370, 456, 1062, 312, 733, 295, 257, 7738, 597, 307, 411], "temperature": 0.0, "avg_logprob": -0.1919528584421417, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.769389539025724e-06}, {"id": 751, "seek": 367924, "start": 3691.72, "end": 3697.8399999999997, "text": " To what extent is this day of the week kind of associated with people going out?", "tokens": [1407, 437, 8396, 307, 341, 786, 295, 264, 1243, 733, 295, 6615, 365, 561, 516, 484, 30], "temperature": 0.0, "avg_logprob": -0.1919528584421417, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.769389539025724e-06}, {"id": 752, "seek": 367924, "start": 3698.4799999999996, "end": 3705.24, "text": " You know so basically yeah by by having this higher dimensionality vector rather than just a single number", "tokens": [509, 458, 370, 1936, 1338, 538, 538, 1419, 341, 2946, 10139, 1860, 8062, 2831, 813, 445, 257, 2167, 1230], "temperature": 0.0, "avg_logprob": -0.1919528584421417, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.769389539025724e-06}, {"id": 753, "seek": 367924, "start": 3705.56, "end": 3707.56, "text": " It gives the deep learning", "tokens": [467, 2709, 264, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.1919528584421417, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.769389539025724e-06}, {"id": 754, "seek": 370756, "start": 3707.56, "end": 3710.48, "text": " Network a chance to learn these rich", "tokens": [12640, 257, 2931, 281, 1466, 613, 4593], "temperature": 0.0, "avg_logprob": -0.18372555006118046, "compression_ratio": 1.7924528301886793, "no_speech_prob": 2.3090674972081615e-07}, {"id": 755, "seek": 370756, "start": 3711.4, "end": 3718.6, "text": " Representations and so this idea of an embedding is actually what's called a distributed representation", "tokens": [19945, 763, 293, 370, 341, 1558, 295, 364, 12240, 3584, 307, 767, 437, 311, 1219, 257, 12631, 10290], "temperature": 0.0, "avg_logprob": -0.18372555006118046, "compression_ratio": 1.7924528301886793, "no_speech_prob": 2.3090674972081615e-07}, {"id": 756, "seek": 370756, "start": 3718.6, "end": 3722.44, "text": " It's kind of the fun most fundamental concept of neural networks", "tokens": [467, 311, 733, 295, 264, 1019, 881, 8088, 3410, 295, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.18372555006118046, "compression_ratio": 1.7924528301886793, "no_speech_prob": 2.3090674972081615e-07}, {"id": 757, "seek": 370756, "start": 3722.44, "end": 3729.0, "text": " It's this idea that a concept in a neural network has a kind of a high dimensional", "tokens": [467, 311, 341, 1558, 300, 257, 3410, 294, 257, 18161, 3209, 575, 257, 733, 295, 257, 1090, 18795], "temperature": 0.0, "avg_logprob": -0.18372555006118046, "compression_ratio": 1.7924528301886793, "no_speech_prob": 2.3090674972081615e-07}, {"id": 758, "seek": 370756, "start": 3730.08, "end": 3734.72, "text": " representation and often it can be hard to interpret because the idea is like each of these", "tokens": [10290, 293, 2049, 309, 393, 312, 1152, 281, 7302, 570, 264, 1558, 307, 411, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.18372555006118046, "compression_ratio": 1.7924528301886793, "no_speech_prob": 2.3090674972081615e-07}, {"id": 759, "seek": 373472, "start": 3734.72, "end": 3741.16, "text": " Numbers in this vector doesn't even have to have just one meaning you know it could mean one thing if this is low and that", "tokens": [22592, 1616, 294, 341, 8062, 1177, 380, 754, 362, 281, 362, 445, 472, 3620, 291, 458, 309, 727, 914, 472, 551, 498, 341, 307, 2295, 293, 300], "temperature": 0.0, "avg_logprob": -0.22697187929737325, "compression_ratio": 1.8026315789473684, "no_speech_prob": 1.3631017282023095e-05}, {"id": 760, "seek": 373472, "start": 3741.16, "end": 3746.8799999999997, "text": " One's high and something else if that one's high and that one's low because it's going through this kind of rich nonlinear", "tokens": [1485, 311, 1090, 293, 746, 1646, 498, 300, 472, 311, 1090, 293, 300, 472, 311, 2295, 570, 309, 311, 516, 807, 341, 733, 295, 4593, 2107, 28263], "temperature": 0.0, "avg_logprob": -0.22697187929737325, "compression_ratio": 1.8026315789473684, "no_speech_prob": 1.3631017282023095e-05}, {"id": 761, "seek": 373472, "start": 3747.7599999999998, "end": 3750.2, "text": " Function right and so it's this", "tokens": [11166, 882, 558, 293, 370, 309, 311, 341], "temperature": 0.0, "avg_logprob": -0.22697187929737325, "compression_ratio": 1.8026315789473684, "no_speech_prob": 1.3631017282023095e-05}, {"id": 762, "seek": 373472, "start": 3750.9599999999996, "end": 3757.12, "text": " It's this rich representation that allows it to learn such such such interesting", "tokens": [467, 311, 341, 4593, 10290, 300, 4045, 309, 281, 1466, 1270, 1270, 1270, 1880], "temperature": 0.0, "avg_logprob": -0.22697187929737325, "compression_ratio": 1.8026315789473684, "no_speech_prob": 1.3631017282023095e-05}, {"id": 763, "seek": 373472, "start": 3758.52, "end": 3760.52, "text": " relationships", "tokens": [6159], "temperature": 0.0, "avg_logprob": -0.22697187929737325, "compression_ratio": 1.8026315789473684, "no_speech_prob": 1.3631017282023095e-05}, {"id": 764, "seek": 373472, "start": 3760.52, "end": 3762.98, "text": " Kind of tough oh another question sure", "tokens": [9242, 295, 4930, 1954, 1071, 1168, 988], "temperature": 0.0, "avg_logprob": -0.22697187929737325, "compression_ratio": 1.8026315789473684, "no_speech_prob": 1.3631017282023095e-05}, {"id": 765, "seek": 376298, "start": 3762.98, "end": 3766.22, "text": " All right, I'll speak louder so are there", "tokens": [1057, 558, 11, 286, 603, 1710, 22717, 370, 366, 456], "temperature": 0.0, "avg_logprob": -0.269120604127318, "compression_ratio": 1.9043062200956937, "no_speech_prob": 4.610638279700652e-05}, {"id": 766, "seek": 376298, "start": 3768.02, "end": 3775.06, "text": " Is an embedding so I get the fundamental of the like the word vector word to vec vector algebra", "tokens": [1119, 364, 12240, 3584, 370, 286, 483, 264, 8088, 295, 264, 411, 264, 1349, 8062, 1349, 281, 42021, 8062, 21989], "temperature": 0.0, "avg_logprob": -0.269120604127318, "compression_ratio": 1.9043062200956937, "no_speech_prob": 4.610638279700652e-05}, {"id": 767, "seek": 376298, "start": 3775.06, "end": 3781.1, "text": " You can run on this but are the embedding suited suitable for certain types of variables like", "tokens": [509, 393, 1190, 322, 341, 457, 366, 264, 12240, 3584, 24736, 12873, 337, 1629, 3467, 295, 9102, 411], "temperature": 0.0, "avg_logprob": -0.269120604127318, "compression_ratio": 1.9043062200956937, "no_speech_prob": 4.610638279700652e-05}, {"id": 768, "seek": 376298, "start": 3781.26, "end": 3783.82, "text": " Or are these only suitable for?", "tokens": [1610, 366, 613, 787, 12873, 337, 30], "temperature": 0.0, "avg_logprob": -0.269120604127318, "compression_ratio": 1.9043062200956937, "no_speech_prob": 4.610638279700652e-05}, {"id": 769, "seek": 376298, "start": 3784.3, "end": 3791.14, "text": " I mean are there different categories that that the embeddings are suitable for and embedding is suitable for any categorical variable", "tokens": [286, 914, 366, 456, 819, 10479, 300, 300, 264, 12240, 29432, 366, 12873, 337, 293, 12240, 3584, 307, 12873, 337, 604, 19250, 804, 7006], "temperature": 0.0, "avg_logprob": -0.269120604127318, "compression_ratio": 1.9043062200956937, "no_speech_prob": 4.610638279700652e-05}, {"id": 770, "seek": 379114, "start": 3791.14, "end": 3799.2999999999997, "text": " Okay, so so the only thing it can't really work well at all for would be something that is too high", "tokens": [1033, 11, 370, 370, 264, 787, 551, 309, 393, 380, 534, 589, 731, 412, 439, 337, 576, 312, 746, 300, 307, 886, 1090], "temperature": 0.0, "avg_logprob": -0.2173903879493174, "compression_ratio": 1.5755102040816327, "no_speech_prob": 2.090450152536505e-06}, {"id": 771, "seek": 379114, "start": 3799.62, "end": 3806.68, "text": " Cardinality so like in other words we had likes whatever it was 600,000 rows if you had a variable with 600,000 levels", "tokens": [11877, 259, 1860, 370, 411, 294, 661, 2283, 321, 632, 5902, 2035, 309, 390, 11849, 11, 1360, 13241, 498, 291, 632, 257, 7006, 365, 11849, 11, 1360, 4358], "temperature": 0.0, "avg_logprob": -0.2173903879493174, "compression_ratio": 1.5755102040816327, "no_speech_prob": 2.090450152536505e-06}, {"id": 772, "seek": 379114, "start": 3807.2999999999997, "end": 3809.2999999999997, "text": " That's just not a useful", "tokens": [663, 311, 445, 406, 257, 4420], "temperature": 0.0, "avg_logprob": -0.2173903879493174, "compression_ratio": 1.5755102040816327, "no_speech_prob": 2.090450152536505e-06}, {"id": 773, "seek": 379114, "start": 3810.42, "end": 3813.7, "text": " Categorical variable you could bucketize it I guess", "tokens": [383, 2968, 284, 804, 7006, 291, 727, 13058, 1125, 309, 286, 2041], "temperature": 0.0, "avg_logprob": -0.2173903879493174, "compression_ratio": 1.5755102040816327, "no_speech_prob": 2.090450152536505e-06}, {"id": 774, "seek": 379114, "start": 3814.74, "end": 3819.9, "text": " But yeah in general like you can see here that the third place getters in this competition", "tokens": [583, 1338, 294, 2674, 411, 291, 393, 536, 510, 300, 264, 2636, 1081, 483, 1559, 294, 341, 6211], "temperature": 0.0, "avg_logprob": -0.2173903879493174, "compression_ratio": 1.5755102040816327, "no_speech_prob": 2.090450152536505e-06}, {"id": 775, "seek": 381990, "start": 3819.9, "end": 3821.1, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.24235374813988095, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.844914772344055e-06}, {"id": 776, "seek": 381990, "start": 3821.1, "end": 3825.9, "text": " Really decided that everything that was not too high cardinality", "tokens": [4083, 3047, 300, 1203, 300, 390, 406, 886, 1090, 2920, 259, 1860], "temperature": 0.0, "avg_logprob": -0.24235374813988095, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.844914772344055e-06}, {"id": 777, "seek": 381990, "start": 3825.9, "end": 3829.34, "text": " They put them all as categorical variables, and I think that's a good rule of thumb", "tokens": [814, 829, 552, 439, 382, 19250, 804, 9102, 11, 293, 286, 519, 300, 311, 257, 665, 4978, 295, 9298], "temperature": 0.0, "avg_logprob": -0.24235374813988095, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.844914772344055e-06}, {"id": 778, "seek": 381990, "start": 3829.5, "end": 3833.26, "text": " You know if you can make it a categorical variable you may as well", "tokens": [509, 458, 498, 291, 393, 652, 309, 257, 19250, 804, 7006, 291, 815, 382, 731], "temperature": 0.0, "avg_logprob": -0.24235374813988095, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.844914772344055e-06}, {"id": 779, "seek": 381990, "start": 3833.6600000000003, "end": 3838.94, "text": " Because that way it can learn this rich distributed representation where else if you leave it as continuous", "tokens": [1436, 300, 636, 309, 393, 1466, 341, 4593, 12631, 10290, 689, 1646, 498, 291, 1856, 309, 382, 10957], "temperature": 0.0, "avg_logprob": -0.24235374813988095, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.844914772344055e-06}, {"id": 780, "seek": 381990, "start": 3839.2200000000003, "end": 3842.62, "text": " You know the most it can do is to kind of try and find a", "tokens": [509, 458, 264, 881, 309, 393, 360, 307, 281, 733, 295, 853, 293, 915, 257], "temperature": 0.0, "avg_logprob": -0.24235374813988095, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.844914772344055e-06}, {"id": 781, "seek": 381990, "start": 3843.1800000000003, "end": 3845.6, "text": " Little a single functional form that fits it well", "tokens": [8022, 257, 2167, 11745, 1254, 300, 9001, 309, 731], "temperature": 0.0, "avg_logprob": -0.24235374813988095, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.844914772344055e-06}, {"id": 782, "seek": 384560, "start": 3845.6, "end": 3848.5, "text": " I have a question so", "tokens": [286, 362, 257, 1168, 370], "temperature": 0.0, "avg_logprob": -0.261865541046741, "compression_ratio": 1.7743362831858407, "no_speech_prob": 4.936799541610526e-06}, {"id": 783, "seek": 384560, "start": 3849.24, "end": 3853.0, "text": " You were saying that you are kind of increasing the dimension", "tokens": [509, 645, 1566, 300, 291, 366, 733, 295, 5662, 264, 10139], "temperature": 0.0, "avg_logprob": -0.261865541046741, "compression_ratio": 1.7743362831858407, "no_speech_prob": 4.936799541610526e-06}, {"id": 784, "seek": 384560, "start": 3853.0, "end": 3859.72, "text": " But actually in in most cases we will use a one-hot encoding which has even a bigger dimension", "tokens": [583, 767, 294, 294, 881, 3331, 321, 486, 764, 257, 472, 12, 12194, 43430, 597, 575, 754, 257, 3801, 10139], "temperature": 0.0, "avg_logprob": -0.261865541046741, "compression_ratio": 1.7743362831858407, "no_speech_prob": 4.936799541610526e-06}, {"id": 785, "seek": 384560, "start": 3860.08, "end": 3868.2799999999997, "text": " That so so in a way you are also reducing, but in the most rich. I think that's that's that's fair again it yeah it like", "tokens": [663, 370, 370, 294, 257, 636, 291, 366, 611, 12245, 11, 457, 294, 264, 881, 4593, 13, 286, 519, 300, 311, 300, 311, 300, 311, 3143, 797, 309, 1338, 309, 411], "temperature": 0.0, "avg_logprob": -0.261865541046741, "compression_ratio": 1.7743362831858407, "no_speech_prob": 4.936799541610526e-06}, {"id": 786, "seek": 386828, "start": 3868.28, "end": 3874.84, "text": " Yes, you know you can think of it as one-hot encoding which actually is high dimensional, but it's not", "tokens": [1079, 11, 291, 458, 291, 393, 519, 295, 309, 382, 472, 12, 12194, 43430, 597, 767, 307, 1090, 18795, 11, 457, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.246250121824203, "compression_ratio": 1.726984126984127, "no_speech_prob": 5.0936091611220036e-06}, {"id": 787, "seek": 386828, "start": 3875.5600000000004, "end": 3878.28, "text": " Meaningfully high dimensional because everything except one is zero", "tokens": [19948, 2277, 1090, 18795, 570, 1203, 3993, 472, 307, 4018], "temperature": 0.0, "avg_logprob": -0.246250121824203, "compression_ratio": 1.726984126984127, "no_speech_prob": 5.0936091611220036e-06}, {"id": 788, "seek": 386828, "start": 3878.28, "end": 3883.7200000000003, "text": " I'm saying that also because even this will reduce the amount of memory and things like this that you have to write", "tokens": [286, 478, 1566, 300, 611, 570, 754, 341, 486, 5407, 264, 2372, 295, 4675, 293, 721, 411, 341, 300, 291, 362, 281, 2464], "temperature": 0.0, "avg_logprob": -0.246250121824203, "compression_ratio": 1.726984126984127, "no_speech_prob": 5.0936091611220036e-06}, {"id": 789, "seek": 386828, "start": 3883.7200000000003, "end": 3885.8, "text": " The terms is better. You're absolutely right", "tokens": [440, 2115, 307, 1101, 13, 509, 434, 3122, 558], "temperature": 0.0, "avg_logprob": -0.246250121824203, "compression_ratio": 1.726984126984127, "no_speech_prob": 5.0936091611220036e-06}, {"id": 790, "seek": 386828, "start": 3885.8, "end": 3892.6400000000003, "text": " We're absolutely right and and so we may as well go ahead and actually describe like what's going on with the matrix algebra behind", "tokens": [492, 434, 3122, 558, 293, 293, 370, 321, 815, 382, 731, 352, 2286, 293, 767, 6786, 411, 437, 311, 516, 322, 365, 264, 8141, 21989, 2261], "temperature": 0.0, "avg_logprob": -0.246250121824203, "compression_ratio": 1.726984126984127, "no_speech_prob": 5.0936091611220036e-06}, {"id": 791, "seek": 386828, "start": 3892.6400000000003, "end": 3896.6400000000003, "text": " The scene see this if this doesn't quite make sense you can kind of skip over it", "tokens": [440, 4145, 536, 341, 498, 341, 1177, 380, 1596, 652, 2020, 291, 393, 733, 295, 10023, 670, 309], "temperature": 0.0, "avg_logprob": -0.246250121824203, "compression_ratio": 1.726984126984127, "no_speech_prob": 5.0936091611220036e-06}, {"id": 792, "seek": 389664, "start": 3896.64, "end": 3898.64, "text": " But for some people I know this really helps", "tokens": [583, 337, 512, 561, 286, 458, 341, 534, 3665], "temperature": 0.0, "avg_logprob": -0.1620103592096373, "compression_ratio": 1.5518867924528301, "no_speech_prob": 1.414473103977798e-06}, {"id": 793, "seek": 389664, "start": 3899.2, "end": 3903.2999999999997, "text": " If we started out with something saying this is Sunday", "tokens": [759, 321, 1409, 484, 365, 746, 1566, 341, 307, 7776], "temperature": 0.0, "avg_logprob": -0.1620103592096373, "compression_ratio": 1.5518867924528301, "no_speech_prob": 1.414473103977798e-06}, {"id": 794, "seek": 389664, "start": 3903.8799999999997, "end": 3905.3199999999997, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.1620103592096373, "compression_ratio": 1.5518867924528301, "no_speech_prob": 1.414473103977798e-06}, {"id": 795, "seek": 389664, "start": 3905.3199999999997, "end": 3909.64, "text": " We could represent this as a one-hot encoded vector right and so", "tokens": [492, 727, 2906, 341, 382, 257, 472, 12, 12194, 2058, 12340, 8062, 558, 293, 370], "temperature": 0.0, "avg_logprob": -0.1620103592096373, "compression_ratio": 1.5518867924528301, "no_speech_prob": 1.414473103977798e-06}, {"id": 796, "seek": 389664, "start": 3911.3599999999997, "end": 3916.58, "text": " Sunday you know maybe was position here, so that would be a one and then the rest of zeros", "tokens": [7776, 291, 458, 1310, 390, 2535, 510, 11, 370, 300, 576, 312, 257, 472, 293, 550, 264, 1472, 295, 35193], "temperature": 0.0, "avg_logprob": -0.1620103592096373, "compression_ratio": 1.5518867924528301, "no_speech_prob": 1.414473103977798e-06}, {"id": 797, "seek": 389664, "start": 3918.48, "end": 3920.48, "text": " Okay, and then we've got our", "tokens": [1033, 11, 293, 550, 321, 600, 658, 527], "temperature": 0.0, "avg_logprob": -0.1620103592096373, "compression_ratio": 1.5518867924528301, "no_speech_prob": 1.414473103977798e-06}, {"id": 798, "seek": 392048, "start": 3920.48, "end": 3928.52, "text": " Embedding matrix right with eight rows and in this case four columns", "tokens": [24234, 292, 3584, 8141, 558, 365, 3180, 13241, 293, 294, 341, 1389, 1451, 13766], "temperature": 0.0, "avg_logprob": -0.1492819086710612, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.3925416624260833e-06}, {"id": 799, "seek": 392048, "start": 3932.96, "end": 3936.76, "text": " One way to think of this actually is a matrix product, right?", "tokens": [1485, 636, 281, 519, 295, 341, 767, 307, 257, 8141, 1674, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1492819086710612, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.3925416624260833e-06}, {"id": 800, "seek": 392048, "start": 3936.76, "end": 3941.28, "text": " So I said you could think of this as like looking up the number one", "tokens": [407, 286, 848, 291, 727, 519, 295, 341, 382, 411, 1237, 493, 264, 1230, 472], "temperature": 0.0, "avg_logprob": -0.1492819086710612, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.3925416624260833e-06}, {"id": 801, "seek": 392048, "start": 3941.68, "end": 3944.84, "text": " You know and finding like its index in the array", "tokens": [509, 458, 293, 5006, 411, 1080, 8186, 294, 264, 10225], "temperature": 0.0, "avg_logprob": -0.1492819086710612, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.3925416624260833e-06}, {"id": 802, "seek": 392048, "start": 3945.56, "end": 3947.56, "text": " But if you think about it, that's actually", "tokens": [583, 498, 291, 519, 466, 309, 11, 300, 311, 767], "temperature": 0.0, "avg_logprob": -0.1492819086710612, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.3925416624260833e-06}, {"id": 803, "seek": 394756, "start": 3947.56, "end": 3954.96, "text": " Identical to doing a matrix product between a one-hot encoded vector and the embedding matrix", "tokens": [25905, 804, 281, 884, 257, 8141, 1674, 1296, 257, 472, 12, 12194, 2058, 12340, 8062, 293, 264, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.19122082354074502, "compression_ratio": 1.7978142076502732, "no_speech_prob": 4.425476163305575e-06}, {"id": 804, "seek": 394756, "start": 3955.2799999999997, "end": 3962.06, "text": " Like you're going to go zero times this row one times this row zero times this row", "tokens": [1743, 291, 434, 516, 281, 352, 4018, 1413, 341, 5386, 472, 1413, 341, 5386, 4018, 1413, 341, 5386], "temperature": 0.0, "avg_logprob": -0.19122082354074502, "compression_ratio": 1.7978142076502732, "no_speech_prob": 4.425476163305575e-06}, {"id": 805, "seek": 394756, "start": 3962.06, "end": 3966.74, "text": " And so it's like a one-hot embedding matrix product is identical", "tokens": [400, 370, 309, 311, 411, 257, 472, 12, 12194, 12240, 3584, 8141, 1674, 307, 14800], "temperature": 0.0, "avg_logprob": -0.19122082354074502, "compression_ratio": 1.7978142076502732, "no_speech_prob": 4.425476163305575e-06}, {"id": 806, "seek": 394756, "start": 3967.36, "end": 3969.68, "text": " to doing a lookup and so", "tokens": [281, 884, 257, 574, 1010, 293, 370], "temperature": 0.0, "avg_logprob": -0.19122082354074502, "compression_ratio": 1.7978142076502732, "no_speech_prob": 4.425476163305575e-06}, {"id": 807, "seek": 394756, "start": 3971.68, "end": 3975.68, "text": " Some people in the bad old days actually implemented embedding", "tokens": [2188, 561, 294, 264, 1578, 1331, 1708, 767, 12270, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.19122082354074502, "compression_ratio": 1.7978142076502732, "no_speech_prob": 4.425476163305575e-06}, {"id": 808, "seek": 397568, "start": 3975.68, "end": 3982.24, "text": " matrices by doing a one-hot encoding and then a matrix product and in fact a lot of like machine learning", "tokens": [32284, 538, 884, 257, 472, 12, 12194, 43430, 293, 550, 257, 8141, 1674, 293, 294, 1186, 257, 688, 295, 411, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.19892403573700876, "compression_ratio": 1.704, "no_speech_prob": 1.0188075521000428e-06}, {"id": 809, "seek": 397568, "start": 3982.56, "end": 3984.56, "text": " Methods still kind of do that", "tokens": [25285, 82, 920, 733, 295, 360, 300], "temperature": 0.0, "avg_logprob": -0.19892403573700876, "compression_ratio": 1.704, "no_speech_prob": 1.0188075521000428e-06}, {"id": 810, "seek": 397568, "start": 3985.64, "end": 3989.8399999999997, "text": " But as you know it was kind of alluding to it's that's terribly inefficient", "tokens": [583, 382, 291, 458, 309, 390, 733, 295, 439, 33703, 281, 309, 311, 300, 311, 22903, 43495], "temperature": 0.0, "avg_logprob": -0.19892403573700876, "compression_ratio": 1.704, "no_speech_prob": 1.0188075521000428e-06}, {"id": 811, "seek": 397568, "start": 3990.16, "end": 3997.06, "text": " So all of the modern libraries implement this as taken take an integer and do a lookup into an array", "tokens": [407, 439, 295, 264, 4363, 15148, 4445, 341, 382, 2726, 747, 364, 24922, 293, 360, 257, 574, 1010, 666, 364, 10225], "temperature": 0.0, "avg_logprob": -0.19892403573700876, "compression_ratio": 1.704, "no_speech_prob": 1.0188075521000428e-06}, {"id": 812, "seek": 397568, "start": 3997.16, "end": 4002.64, "text": " But the nice thing about realizing that it's actually a matrix product mathematically is it makes it more obvious", "tokens": [583, 264, 1481, 551, 466, 16734, 300, 309, 311, 767, 257, 8141, 1674, 44003, 307, 309, 1669, 309, 544, 6322], "temperature": 0.0, "avg_logprob": -0.19892403573700876, "compression_ratio": 1.704, "no_speech_prob": 1.0188075521000428e-06}, {"id": 813, "seek": 400264, "start": 4002.64, "end": 4010.04, "text": " How the gradients are going to flow so when we do stochastic gradient descent, it's we can think of it as just another", "tokens": [1012, 264, 2771, 2448, 366, 516, 281, 3095, 370, 562, 321, 360, 342, 8997, 2750, 16235, 23475, 11, 309, 311, 321, 393, 519, 295, 309, 382, 445, 1071], "temperature": 0.0, "avg_logprob": -0.2178234629111715, "compression_ratio": 1.5225563909774436, "no_speech_prob": 1.2029517165501602e-05}, {"id": 814, "seek": 400264, "start": 4010.8399999999997, "end": 4016.68, "text": " Linear layer. Okay doesn't say that's like a somewhat minor detail, but hopefully for some of you it helps", "tokens": [14670, 289, 4583, 13, 1033, 1177, 380, 584, 300, 311, 411, 257, 8344, 6696, 2607, 11, 457, 4696, 337, 512, 295, 291, 309, 3665], "temperature": 0.0, "avg_logprob": -0.2178234629111715, "compression_ratio": 1.5225563909774436, "no_speech_prob": 1.2029517165501602e-05}, {"id": 815, "seek": 400264, "start": 4019.0, "end": 4026.3599999999997, "text": " Could you touch on using dates and times as categoricals and how that affects seasonality? Yeah, absolutely. That's a great question", "tokens": [7497, 291, 2557, 322, 1228, 11691, 293, 1413, 382, 19250, 804, 82, 293, 577, 300, 11807, 3196, 1860, 30, 865, 11, 3122, 13, 663, 311, 257, 869, 1168], "temperature": 0.0, "avg_logprob": -0.2178234629111715, "compression_ratio": 1.5225563909774436, "no_speech_prob": 1.2029517165501602e-05}, {"id": 816, "seek": 400264, "start": 4027.7999999999997, "end": 4030.16, "text": " Did I cover dates at all last week? I remember", "tokens": [2589, 286, 2060, 11691, 412, 439, 1036, 1243, 30, 286, 1604], "temperature": 0.0, "avg_logprob": -0.2178234629111715, "compression_ratio": 1.5225563909774436, "no_speech_prob": 1.2029517165501602e-05}, {"id": 817, "seek": 403016, "start": 4030.16, "end": 4032.16, "text": " No, okay", "tokens": [883, 11, 1392], "temperature": 0.0, "avg_logprob": -0.17507132777461298, "compression_ratio": 1.5330188679245282, "no_speech_prob": 2.295910007887869e-06}, {"id": 818, "seek": 403016, "start": 4033.68, "end": 4039.12, "text": " So I covered dates in a lot of detail in the machine learning course, but it's worth briefly mentioning here", "tokens": [407, 286, 5343, 11691, 294, 257, 688, 295, 2607, 294, 264, 3479, 2539, 1164, 11, 457, 309, 311, 3163, 10515, 18315, 510], "temperature": 0.0, "avg_logprob": -0.17507132777461298, "compression_ratio": 1.5330188679245282, "no_speech_prob": 2.295910007887869e-06}, {"id": 819, "seek": 403016, "start": 4043.72, "end": 4046.92, "text": " There's a fast AI function called add date part", "tokens": [821, 311, 257, 2370, 7318, 2445, 1219, 909, 4002, 644], "temperature": 0.0, "avg_logprob": -0.17507132777461298, "compression_ratio": 1.5330188679245282, "no_speech_prob": 2.295910007887869e-06}, {"id": 820, "seek": 403016, "start": 4048.48, "end": 4050.64, "text": " Which takes a data frame and a column name", "tokens": [3013, 2516, 257, 1412, 3920, 293, 257, 7738, 1315], "temperature": 0.0, "avg_logprob": -0.17507132777461298, "compression_ratio": 1.5330188679245282, "no_speech_prob": 2.295910007887869e-06}, {"id": 821, "seek": 403016, "start": 4051.92, "end": 4053.92, "text": " That column name needs to be a date", "tokens": [663, 7738, 1315, 2203, 281, 312, 257, 4002], "temperature": 0.0, "avg_logprob": -0.17507132777461298, "compression_ratio": 1.5330188679245282, "no_speech_prob": 2.295910007887869e-06}, {"id": 822, "seek": 405392, "start": 4053.92, "end": 4061.2400000000002, "text": " It removes unless you've got drop equals false it optionally removes the column from the data frame and", "tokens": [467, 30445, 5969, 291, 600, 658, 3270, 6915, 7908, 309, 3614, 379, 30445, 264, 7738, 490, 264, 1412, 3920, 293], "temperature": 0.0, "avg_logprob": -0.19395840167999268, "compression_ratio": 1.7244444444444444, "no_speech_prob": 1.1189401902811369e-06}, {"id": 823, "seek": 405392, "start": 4061.7200000000003, "end": 4067.6800000000003, "text": " replaces it with lots of columns representing all of the useful information about that date like", "tokens": [46734, 309, 365, 3195, 295, 13766, 13460, 439, 295, 264, 4420, 1589, 466, 300, 4002, 411], "temperature": 0.0, "avg_logprob": -0.19395840167999268, "compression_ratio": 1.7244444444444444, "no_speech_prob": 1.1189401902811369e-06}, {"id": 824, "seek": 405392, "start": 4068.2000000000003, "end": 4073.28, "text": " Day of week day of month month of year year is at the start of a quarter is at the end of a quarter", "tokens": [5226, 295, 1243, 786, 295, 1618, 1618, 295, 1064, 1064, 307, 412, 264, 722, 295, 257, 6555, 307, 412, 264, 917, 295, 257, 6555], "temperature": 0.0, "avg_logprob": -0.19395840167999268, "compression_ratio": 1.7244444444444444, "no_speech_prob": 1.1189401902811369e-06}, {"id": 825, "seek": 405392, "start": 4073.64, "end": 4075.64, "text": " basically everything that pandas", "tokens": [1936, 1203, 300, 4565, 296], "temperature": 0.0, "avg_logprob": -0.19395840167999268, "compression_ratio": 1.7244444444444444, "no_speech_prob": 1.1189401902811369e-06}, {"id": 826, "seek": 405392, "start": 4075.88, "end": 4077.48, "text": " Gives us", "tokens": [460, 1539, 505], "temperature": 0.0, "avg_logprob": -0.19395840167999268, "compression_ratio": 1.7244444444444444, "no_speech_prob": 1.1189401902811369e-06}, {"id": 827, "seek": 405392, "start": 4077.48, "end": 4080.2000000000003, "text": " And so that way we end up", "tokens": [400, 370, 300, 636, 321, 917, 493], "temperature": 0.0, "avg_logprob": -0.19395840167999268, "compression_ratio": 1.7244444444444444, "no_speech_prob": 1.1189401902811369e-06}, {"id": 828, "seek": 405392, "start": 4080.8, "end": 4082.56, "text": " when we look at our", "tokens": [562, 321, 574, 412, 527], "temperature": 0.0, "avg_logprob": -0.19395840167999268, "compression_ratio": 1.7244444444444444, "no_speech_prob": 1.1189401902811369e-06}, {"id": 829, "seek": 408256, "start": 4082.56, "end": 4085.04, "text": " List of features where you can see them here, right?", "tokens": [17668, 295, 4122, 689, 291, 393, 536, 552, 510, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2311988782279099, "compression_ratio": 1.5138121546961325, "no_speech_prob": 3.2377427032770356e-06}, {"id": 830, "seek": 408256, "start": 4085.7999999999997, "end": 4091.52, "text": " Yeah, month week day day of week, etc. So these all get created for us by add date part", "tokens": [865, 11, 1618, 1243, 786, 786, 295, 1243, 11, 5183, 13, 407, 613, 439, 483, 2942, 337, 505, 538, 909, 4002, 644], "temperature": 0.0, "avg_logprob": -0.2311988782279099, "compression_ratio": 1.5138121546961325, "no_speech_prob": 3.2377427032770356e-06}, {"id": 831, "seek": 408256, "start": 4092.68, "end": 4094.68, "text": " so we end up with", "tokens": [370, 321, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.2311988782279099, "compression_ratio": 1.5138121546961325, "no_speech_prob": 3.2377427032770356e-06}, {"id": 832, "seek": 408256, "start": 4095.12, "end": 4097.12, "text": " you know this", "tokens": [291, 458, 341], "temperature": 0.0, "avg_logprob": -0.2311988782279099, "compression_ratio": 1.5138121546961325, "no_speech_prob": 3.2377427032770356e-06}, {"id": 833, "seek": 408256, "start": 4098.44, "end": 4100.2, "text": " Eight long", "tokens": [17708, 938], "temperature": 0.0, "avg_logprob": -0.2311988782279099, "compression_ratio": 1.5138121546961325, "no_speech_prob": 3.2377427032770356e-06}, {"id": 834, "seek": 408256, "start": 4100.2, "end": 4101.44, "text": " embedding", "tokens": [12240, 3584], "temperature": 0.0, "avg_logprob": -0.2311988782279099, "compression_ratio": 1.5138121546961325, "no_speech_prob": 3.2377427032770356e-06}, {"id": 835, "seek": 408256, "start": 4101.44, "end": 4106.72, "text": " matrix so I guess eight rows by four column embedding matrix for day of week and", "tokens": [8141, 370, 286, 2041, 3180, 13241, 538, 1451, 7738, 12240, 3584, 8141, 337, 786, 295, 1243, 293], "temperature": 0.0, "avg_logprob": -0.2311988782279099, "compression_ratio": 1.5138121546961325, "no_speech_prob": 3.2377427032770356e-06}, {"id": 836, "seek": 410672, "start": 4106.72, "end": 4114.56, "text": " And conceptually that allows us or allows our model to create some pretty interesting time series models", "tokens": [400, 3410, 671, 300, 4045, 505, 420, 4045, 527, 2316, 281, 1884, 512, 1238, 1880, 565, 2638, 5245], "temperature": 0.0, "avg_logprob": -0.17009807861957355, "compression_ratio": 1.6375, "no_speech_prob": 9.72151951827982e-07}, {"id": 837, "seek": 410672, "start": 4114.92, "end": 4118.08, "text": " right like it can if there's something that has a", "tokens": [558, 411, 309, 393, 498, 456, 311, 746, 300, 575, 257], "temperature": 0.0, "avg_logprob": -0.17009807861957355, "compression_ratio": 1.6375, "no_speech_prob": 9.72151951827982e-07}, {"id": 838, "seek": 410672, "start": 4118.64, "end": 4120.76, "text": " seven day period cycle", "tokens": [3407, 786, 2896, 6586], "temperature": 0.0, "avg_logprob": -0.17009807861957355, "compression_ratio": 1.6375, "no_speech_prob": 9.72151951827982e-07}, {"id": 839, "seek": 410672, "start": 4121.400000000001, "end": 4127.04, "text": " That kind of goes up on Mondays and down on Wednesdays, but only for dairy and only in Berlin", "tokens": [663, 733, 295, 1709, 493, 322, 7492, 3772, 293, 760, 322, 10579, 82, 11, 457, 787, 337, 21276, 293, 787, 294, 13848], "temperature": 0.0, "avg_logprob": -0.17009807861957355, "compression_ratio": 1.6375, "no_speech_prob": 9.72151951827982e-07}, {"id": 840, "seek": 410672, "start": 4127.8, "end": 4131.04, "text": " It can totally do that, but it has all the information it needs", "tokens": [467, 393, 3879, 360, 300, 11, 457, 309, 575, 439, 264, 1589, 309, 2203], "temperature": 0.0, "avg_logprob": -0.17009807861957355, "compression_ratio": 1.6375, "no_speech_prob": 9.72151951827982e-07}, {"id": 841, "seek": 410672, "start": 4131.8, "end": 4133.360000000001, "text": " to do that", "tokens": [281, 360, 300], "temperature": 0.0, "avg_logprob": -0.17009807861957355, "compression_ratio": 1.6375, "no_speech_prob": 9.72151951827982e-07}, {"id": 842, "seek": 413336, "start": 4133.36, "end": 4137.96, "text": " So this turns out to be a really fantastic way to deal with time series", "tokens": [407, 341, 4523, 484, 281, 312, 257, 534, 5456, 636, 281, 2028, 365, 565, 2638], "temperature": 0.0, "avg_logprob": -0.17222522994846973, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.6797223452158505e-06}, {"id": 843, "seek": 413336, "start": 4137.96, "end": 4141.719999999999, "text": " So I'm really glad you asked the question you just need to make sure that", "tokens": [407, 286, 478, 534, 5404, 291, 2351, 264, 1168, 291, 445, 643, 281, 652, 988, 300], "temperature": 0.0, "avg_logprob": -0.17222522994846973, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.6797223452158505e-06}, {"id": 844, "seek": 413336, "start": 4142.599999999999, "end": 4147.839999999999, "text": " That the the cycle indicator in your time series exists as a column", "tokens": [663, 264, 264, 6586, 16961, 294, 428, 565, 2638, 8198, 382, 257, 7738], "temperature": 0.0, "avg_logprob": -0.17222522994846973, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.6797223452158505e-06}, {"id": 845, "seek": 413336, "start": 4147.839999999999, "end": 4151.339999999999, "text": " So if you didn't have a column there called day of week", "tokens": [407, 498, 291, 994, 380, 362, 257, 7738, 456, 1219, 786, 295, 1243], "temperature": 0.0, "avg_logprob": -0.17222522994846973, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.6797223452158505e-06}, {"id": 846, "seek": 413336, "start": 4151.5199999999995, "end": 4157.08, "text": " it would be very very difficult for the neural network to somehow learn to do like a", "tokens": [309, 576, 312, 588, 588, 2252, 337, 264, 18161, 3209, 281, 6063, 1466, 281, 360, 411, 257], "temperature": 0.0, "avg_logprob": -0.17222522994846973, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.6797223452158505e-06}, {"id": 847, "seek": 413336, "start": 4157.5199999999995, "end": 4161.0, "text": " Divide mod 7 and then somehow look that up in an embedding matrix", "tokens": [9886, 482, 1072, 1614, 293, 550, 6063, 574, 300, 493, 294, 364, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.17222522994846973, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.6797223452158505e-06}, {"id": 848, "seek": 416100, "start": 4161.0, "end": 4166.84, "text": " I get not impossible, but really hard would use lots of computation wouldn't do it very well", "tokens": [286, 483, 406, 6243, 11, 457, 534, 1152, 576, 764, 3195, 295, 24903, 2759, 380, 360, 309, 588, 731], "temperature": 0.0, "avg_logprob": -0.234415279921665, "compression_ratio": 1.5593220338983051, "no_speech_prob": 7.527927209594054e-06}, {"id": 849, "seek": 416100, "start": 4167.6, "end": 4170.9, "text": " so an example of the kind of thing that you need to think about might be", "tokens": [370, 364, 1365, 295, 264, 733, 295, 551, 300, 291, 643, 281, 519, 466, 1062, 312], "temperature": 0.0, "avg_logprob": -0.234415279921665, "compression_ratio": 1.5593220338983051, "no_speech_prob": 7.527927209594054e-06}, {"id": 850, "seek": 416100, "start": 4173.8, "end": 4179.56, "text": " Holidays for example, you know or if you were doing something in you know of sales of", "tokens": [11086, 13177, 337, 1365, 11, 291, 458, 420, 498, 291, 645, 884, 746, 294, 291, 458, 295, 5763, 295], "temperature": 0.0, "avg_logprob": -0.234415279921665, "compression_ratio": 1.5593220338983051, "no_speech_prob": 7.527927209594054e-06}, {"id": 851, "seek": 416100, "start": 4180.08, "end": 4181.88, "text": " beverages in San Francisco", "tokens": [47401, 294, 5271, 12279], "temperature": 0.0, "avg_logprob": -0.234415279921665, "compression_ratio": 1.5593220338983051, "no_speech_prob": 7.527927209594054e-06}, {"id": 852, "seek": 416100, "start": 4181.88, "end": 4187.16, "text": " You probably want a list of like when when are that when is the ballgame on at AT&T Park?", "tokens": [509, 1391, 528, 257, 1329, 295, 411, 562, 562, 366, 300, 562, 307, 264, 2594, 15038, 322, 412, 8872, 5, 51, 4964, 30], "temperature": 0.0, "avg_logprob": -0.234415279921665, "compression_ratio": 1.5593220338983051, "no_speech_prob": 7.527927209594054e-06}, {"id": 853, "seek": 418716, "start": 4187.16, "end": 4191.8, "text": " All right, because that's going to impact how many people that are drinking beer in so much", "tokens": [1057, 558, 11, 570, 300, 311, 516, 281, 2712, 577, 867, 561, 300, 366, 7583, 8795, 294, 370, 709], "temperature": 0.0, "avg_logprob": -0.2243158665109188, "compression_ratio": 1.7302325581395348, "no_speech_prob": 3.2887221550481627e-06}, {"id": 854, "seek": 418716, "start": 4192.2, "end": 4196.599999999999, "text": " All right, so you need to make sure that the kind of the basic indicators or", "tokens": [1057, 558, 11, 370, 291, 643, 281, 652, 988, 300, 264, 733, 295, 264, 3875, 22176, 420], "temperature": 0.0, "avg_logprob": -0.2243158665109188, "compression_ratio": 1.7302325581395348, "no_speech_prob": 3.2887221550481627e-06}, {"id": 855, "seek": 418716, "start": 4197.12, "end": 4203.24, "text": " Or periodicities or whatever are there in your data and as long as they are the neuron it's going to learn to use them", "tokens": [1610, 27790, 1088, 420, 2035, 366, 456, 294, 428, 1412, 293, 382, 938, 382, 436, 366, 264, 34090, 309, 311, 516, 281, 1466, 281, 764, 552], "temperature": 0.0, "avg_logprob": -0.2243158665109188, "compression_ratio": 1.7302325581395348, "no_speech_prob": 3.2887221550481627e-06}, {"id": 856, "seek": 418716, "start": 4204.32, "end": 4208.4, "text": " So I'm kind of trying to skip over some of the non deep learning parts", "tokens": [407, 286, 478, 733, 295, 1382, 281, 10023, 670, 512, 295, 264, 2107, 2452, 2539, 3166], "temperature": 0.0, "avg_logprob": -0.2243158665109188, "compression_ratio": 1.7302325581395348, "no_speech_prob": 3.2887221550481627e-06}, {"id": 857, "seek": 418716, "start": 4211.4, "end": 4213.36, "text": " All right, so", "tokens": [1057, 558, 11, 370], "temperature": 0.0, "avg_logprob": -0.2243158665109188, "compression_ratio": 1.7302325581395348, "no_speech_prob": 3.2887221550481627e-06}, {"id": 858, "seek": 421336, "start": 4213.36, "end": 4217.58, "text": " The key thing here is that we've got our model data that came from the data frame", "tokens": [440, 2141, 551, 510, 307, 300, 321, 600, 658, 527, 2316, 1412, 300, 1361, 490, 264, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.1379741061817516, "compression_ratio": 1.935897435897436, "no_speech_prob": 1.1125503078801557e-05}, {"id": 859, "seek": 421336, "start": 4217.92, "end": 4221.299999999999, "text": " We tell it how big to make the embedding matrices", "tokens": [492, 980, 309, 577, 955, 281, 652, 264, 12240, 3584, 32284], "temperature": 0.0, "avg_logprob": -0.1379741061817516, "compression_ratio": 1.935897435897436, "no_speech_prob": 1.1125503078801557e-05}, {"id": 860, "seek": 421336, "start": 4222.179999999999, "end": 4226.599999999999, "text": " We also have to tell it of the columns in that data frame", "tokens": [492, 611, 362, 281, 980, 309, 295, 264, 13766, 294, 300, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.1379741061817516, "compression_ratio": 1.935897435897436, "no_speech_prob": 1.1125503078801557e-05}, {"id": 861, "seek": 421336, "start": 4227.4, "end": 4229.4, "text": " How many of those?", "tokens": [1012, 867, 295, 729, 30], "temperature": 0.0, "avg_logprob": -0.1379741061817516, "compression_ratio": 1.935897435897436, "no_speech_prob": 1.1125503078801557e-05}, {"id": 862, "seek": 421336, "start": 4230.08, "end": 4232.94, "text": " Categorical variables or how many of them are continuous variables?", "tokens": [383, 2968, 284, 804, 9102, 420, 577, 867, 295, 552, 366, 10957, 9102, 30], "temperature": 0.0, "avg_logprob": -0.1379741061817516, "compression_ratio": 1.935897435897436, "no_speech_prob": 1.1125503078801557e-05}, {"id": 863, "seek": 421336, "start": 4233.0199999999995, "end": 4236.58, "text": " So the actual parameter is number of continuous variables", "tokens": [407, 264, 3539, 13075, 307, 1230, 295, 10957, 9102], "temperature": 0.0, "avg_logprob": -0.1379741061817516, "compression_ratio": 1.935897435897436, "no_speech_prob": 1.1125503078801557e-05}, {"id": 864, "seek": 423658, "start": 4236.58, "end": 4243.0599999999995, "text": " So you can hear you can see we just pass in how many columns are there minus how many categorical variables are there?", "tokens": [407, 291, 393, 1568, 291, 393, 536, 321, 445, 1320, 294, 577, 867, 13766, 366, 456, 3175, 577, 867, 19250, 804, 9102, 366, 456, 30], "temperature": 0.0, "avg_logprob": -0.170753999189897, "compression_ratio": 1.8480392156862746, "no_speech_prob": 1.6536856719540083e-06}, {"id": 865, "seek": 423658, "start": 4243.14, "end": 4245.14, "text": " so then that way the", "tokens": [370, 550, 300, 636, 264], "temperature": 0.0, "avg_logprob": -0.170753999189897, "compression_ratio": 1.8480392156862746, "no_speech_prob": 1.6536856719540083e-06}, {"id": 866, "seek": 423658, "start": 4246.1, "end": 4252.46, "text": " The neural net knows how to create something that puts the continuous variables over here and the categorical variables over there", "tokens": [440, 18161, 2533, 3255, 577, 281, 1884, 746, 300, 8137, 264, 10957, 9102, 670, 510, 293, 264, 19250, 804, 9102, 670, 456], "temperature": 0.0, "avg_logprob": -0.170753999189897, "compression_ratio": 1.8480392156862746, "no_speech_prob": 1.6536856719540083e-06}, {"id": 867, "seek": 423658, "start": 4254.54, "end": 4257.9, "text": " The embedding matrix has its own dropout", "tokens": [440, 12240, 3584, 8141, 575, 1080, 1065, 3270, 346], "temperature": 0.0, "avg_logprob": -0.170753999189897, "compression_ratio": 1.8480392156862746, "no_speech_prob": 1.6536856719540083e-06}, {"id": 868, "seek": 423658, "start": 4258.42, "end": 4261.32, "text": " All right, so this is the dropout applied to the embedding matrix", "tokens": [1057, 558, 11, 370, 341, 307, 264, 3270, 346, 6456, 281, 264, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.170753999189897, "compression_ratio": 1.8480392156862746, "no_speech_prob": 1.6536856719540083e-06}, {"id": 869, "seek": 426132, "start": 4261.32, "end": 4267.299999999999, "text": " This is the number of activations in the first linear layer the number of activations in the second linear layer", "tokens": [639, 307, 264, 1230, 295, 2430, 763, 294, 264, 700, 8213, 4583, 264, 1230, 295, 2430, 763, 294, 264, 1150, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.15301034109933034, "compression_ratio": 2.028436018957346, "no_speech_prob": 4.356865247245878e-06}, {"id": 870, "seek": 426132, "start": 4267.92, "end": 4271.84, "text": " The dropout in the first linear layer the dropout for the second linear layer", "tokens": [440, 3270, 346, 294, 264, 700, 8213, 4583, 264, 3270, 346, 337, 264, 1150, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.15301034109933034, "compression_ratio": 2.028436018957346, "no_speech_prob": 4.356865247245878e-06}, {"id": 871, "seek": 426132, "start": 4272.12, "end": 4276.92, "text": " This bit we won't worry about for now. And then finally is how many outputs do we want to create?", "tokens": [639, 857, 321, 1582, 380, 3292, 466, 337, 586, 13, 400, 550, 2721, 307, 577, 867, 23930, 360, 321, 528, 281, 1884, 30], "temperature": 0.0, "avg_logprob": -0.15301034109933034, "compression_ratio": 2.028436018957346, "no_speech_prob": 4.356865247245878e-06}, {"id": 872, "seek": 426132, "start": 4277.679999999999, "end": 4283.04, "text": " Okay, so this is the output of the last linear layer and obviously it's one because we want to predict a single number", "tokens": [1033, 11, 370, 341, 307, 264, 5598, 295, 264, 1036, 8213, 4583, 293, 2745, 309, 311, 472, 570, 321, 528, 281, 6069, 257, 2167, 1230], "temperature": 0.0, "avg_logprob": -0.15301034109933034, "compression_ratio": 2.028436018957346, "no_speech_prob": 4.356865247245878e-06}, {"id": 873, "seek": 426132, "start": 4283.599999999999, "end": 4285.48, "text": " Which is sales?", "tokens": [3013, 307, 5763, 30], "temperature": 0.0, "avg_logprob": -0.15301034109933034, "compression_ratio": 2.028436018957346, "no_speech_prob": 4.356865247245878e-06}, {"id": 874, "seek": 426132, "start": 4285.48, "end": 4286.679999999999, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.15301034109933034, "compression_ratio": 2.028436018957346, "no_speech_prob": 4.356865247245878e-06}, {"id": 875, "seek": 428668, "start": 4286.68, "end": 4293.96, "text": " So after that we now have a learner where we can call LR find and we get the standard looking shape and we can say what?", "tokens": [407, 934, 300, 321, 586, 362, 257, 33347, 689, 321, 393, 818, 441, 49, 915, 293, 321, 483, 264, 3832, 1237, 3909, 293, 321, 393, 584, 437, 30], "temperature": 0.0, "avg_logprob": -0.18935451951137808, "compression_ratio": 1.5490196078431373, "no_speech_prob": 5.1739002628892194e-06}, {"id": 876, "seek": 428668, "start": 4294.280000000001, "end": 4296.280000000001, "text": " amount do we want to use and", "tokens": [2372, 360, 321, 528, 281, 764, 293], "temperature": 0.0, "avg_logprob": -0.18935451951137808, "compression_ratio": 1.5490196078431373, "no_speech_prob": 5.1739002628892194e-06}, {"id": 877, "seek": 428668, "start": 4296.4400000000005, "end": 4298.4400000000005, "text": " we can then go ahead and", "tokens": [321, 393, 550, 352, 2286, 293], "temperature": 0.0, "avg_logprob": -0.18935451951137808, "compression_ratio": 1.5490196078431373, "no_speech_prob": 5.1739002628892194e-06}, {"id": 878, "seek": 428668, "start": 4299.6, "end": 4304.12, "text": " Start training using exactly the same API we've seen before", "tokens": [6481, 3097, 1228, 2293, 264, 912, 9362, 321, 600, 1612, 949], "temperature": 0.0, "avg_logprob": -0.18935451951137808, "compression_ratio": 1.5490196078431373, "no_speech_prob": 5.1739002628892194e-06}, {"id": 879, "seek": 428668, "start": 4304.92, "end": 4306.92, "text": " So this is all identical", "tokens": [407, 341, 307, 439, 14800], "temperature": 0.0, "avg_logprob": -0.18935451951137808, "compression_ratio": 1.5490196078431373, "no_speech_prob": 5.1739002628892194e-06}, {"id": 880, "seek": 428668, "start": 4307.08, "end": 4310.12, "text": " You can pass in. I'm not sure if you've seen this before", "tokens": [509, 393, 1320, 294, 13, 286, 478, 406, 988, 498, 291, 600, 1612, 341, 949], "temperature": 0.0, "avg_logprob": -0.18935451951137808, "compression_ratio": 1.5490196078431373, "no_speech_prob": 5.1739002628892194e-06}, {"id": 881, "seek": 431012, "start": 4310.12, "end": 4317.36, "text": " Custom metrics what this does is it just says please print out a number at the end of every epoch by calling", "tokens": [16649, 16367, 437, 341, 775, 307, 309, 445, 1619, 1767, 4482, 484, 257, 1230, 412, 264, 917, 295, 633, 30992, 339, 538, 5141], "temperature": 0.0, "avg_logprob": -0.20349988750382966, "compression_ratio": 1.686046511627907, "no_speech_prob": 5.3381222642201465e-06}, {"id": 882, "seek": 431012, "start": 4317.88, "end": 4322.16, "text": " this function and this is a function we defined a little bit earlier, which was the", "tokens": [341, 2445, 293, 341, 307, 257, 2445, 321, 7642, 257, 707, 857, 3071, 11, 597, 390, 264], "temperature": 0.0, "avg_logprob": -0.20349988750382966, "compression_ratio": 1.686046511627907, "no_speech_prob": 5.3381222642201465e-06}, {"id": 883, "seek": 431012, "start": 4323.04, "end": 4327.36, "text": " root means grad percentage error first of all going either the power of our", "tokens": [5593, 1355, 2771, 9668, 6713, 700, 295, 439, 516, 2139, 264, 1347, 295, 527], "temperature": 0.0, "avg_logprob": -0.20349988750382966, "compression_ratio": 1.686046511627907, "no_speech_prob": 5.3381222642201465e-06}, {"id": 884, "seek": 431012, "start": 4327.92, "end": 4331.5599999999995, "text": " Sales because our sales were originally logged", "tokens": [23467, 570, 527, 5763, 645, 7993, 27231], "temperature": 0.0, "avg_logprob": -0.20349988750382966, "compression_ratio": 1.686046511627907, "no_speech_prob": 5.3381222642201465e-06}, {"id": 885, "seek": 431012, "start": 4331.92, "end": 4336.76, "text": " So this doesn't trade change the training at all. It just it's just something to print out", "tokens": [407, 341, 1177, 380, 4923, 1319, 264, 3097, 412, 439, 13, 467, 445, 309, 311, 445, 746, 281, 4482, 484], "temperature": 0.0, "avg_logprob": -0.20349988750382966, "compression_ratio": 1.686046511627907, "no_speech_prob": 5.3381222642201465e-06}, {"id": 886, "seek": 431012, "start": 4337.68, "end": 4339.68, "text": " So we train that for a while", "tokens": [407, 321, 3847, 300, 337, 257, 1339], "temperature": 0.0, "avg_logprob": -0.20349988750382966, "compression_ratio": 1.686046511627907, "no_speech_prob": 5.3381222642201465e-06}, {"id": 887, "seek": 433968, "start": 4339.68, "end": 4341.68, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2177328382219587, "compression_ratio": 1.596244131455399, "no_speech_prob": 3.446566324782907e-06}, {"id": 888, "seek": 433968, "start": 4341.68, "end": 4349.5, "text": " You know we've got some benefits that the original people that built this don't have specifically we've got things like", "tokens": [509, 458, 321, 600, 658, 512, 5311, 300, 264, 3380, 561, 300, 3094, 341, 500, 380, 362, 4682, 321, 600, 658, 721, 411], "temperature": 0.0, "avg_logprob": -0.2177328382219587, "compression_ratio": 1.596244131455399, "no_speech_prob": 3.446566324782907e-06}, {"id": 889, "seek": 433968, "start": 4350.72, "end": 4357.4400000000005, "text": " Cyclical not cyclical learning rate stochastic gradient descent with restarts, and so it's actually interesting to have a look and compare", "tokens": [49173, 804, 406, 19474, 804, 2539, 3314, 342, 8997, 2750, 16235, 23475, 365, 1472, 11814, 11, 293, 370, 309, 311, 767, 1880, 281, 362, 257, 574, 293, 6794], "temperature": 0.0, "avg_logprob": -0.2177328382219587, "compression_ratio": 1.596244131455399, "no_speech_prob": 3.446566324782907e-06}, {"id": 890, "seek": 433968, "start": 4360.56, "end": 4365.72, "text": " Although our validation set isn't identical to the test set it's very similar", "tokens": [5780, 527, 24071, 992, 1943, 380, 14800, 281, 264, 1500, 992, 309, 311, 588, 2531], "temperature": 0.0, "avg_logprob": -0.2177328382219587, "compression_ratio": 1.596244131455399, "no_speech_prob": 3.446566324782907e-06}, {"id": 891, "seek": 436572, "start": 4365.72, "end": 4371.68, "text": " It's a two-week period that is at the end of the training data, so our numbers should be similar", "tokens": [467, 311, 257, 732, 12, 23188, 2896, 300, 307, 412, 264, 917, 295, 264, 3097, 1412, 11, 370, 527, 3547, 820, 312, 2531], "temperature": 0.0, "avg_logprob": -0.19861644744873047, "compression_ratio": 1.5268817204301075, "no_speech_prob": 6.786719950468978e-07}, {"id": 892, "seek": 436572, "start": 4371.68, "end": 4377.38, "text": " and if we look at what we get point oh nine seven and compare that to the", "tokens": [293, 498, 321, 574, 412, 437, 321, 483, 935, 1954, 4949, 3407, 293, 6794, 300, 281, 264], "temperature": 0.0, "avg_logprob": -0.19861644744873047, "compression_ratio": 1.5268817204301075, "no_speech_prob": 6.786719950468978e-07}, {"id": 893, "seek": 436572, "start": 4378.88, "end": 4380.88, "text": " Leaderboard public leaderboard", "tokens": [22650, 3787, 1908, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.19861644744873047, "compression_ratio": 1.5268817204301075, "no_speech_prob": 6.786719950468978e-07}, {"id": 894, "seek": 436572, "start": 4385.52, "end": 4387.52, "text": " You can see we're kind of", "tokens": [509, 393, 536, 321, 434, 733, 295], "temperature": 0.0, "avg_logprob": -0.19861644744873047, "compression_ratio": 1.5268817204301075, "no_speech_prob": 6.786719950468978e-07}, {"id": 895, "seek": 438752, "start": 4387.52, "end": 4393.96, "text": " Let's have a look in the top actually that's interesting", "tokens": [961, 311, 362, 257, 574, 294, 264, 1192, 767, 300, 311, 1880], "temperature": 0.0, "avg_logprob": -0.2099841361822084, "compression_ratio": 1.7170731707317073, "no_speech_prob": 3.2377370189351495e-06}, {"id": 896, "seek": 438752, "start": 4396.280000000001, "end": 4402.080000000001, "text": " There's a big difference between the public and private leaderboard it would have it's would have been right at the top of the private leaderboard", "tokens": [821, 311, 257, 955, 2649, 1296, 264, 1908, 293, 4551, 5263, 3787, 309, 576, 362, 309, 311, 576, 362, 668, 558, 412, 264, 1192, 295, 264, 4551, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.2099841361822084, "compression_ratio": 1.7170731707317073, "no_speech_prob": 3.2377370189351495e-06}, {"id": 897, "seek": 438752, "start": 4403.120000000001, "end": 4408.4400000000005, "text": " But only in the top 30 or 40 on the public leaderboard, so not quite sure, but you can see like we're certainly in", "tokens": [583, 787, 294, 264, 1192, 2217, 420, 3356, 322, 264, 1908, 5263, 3787, 11, 370, 406, 1596, 988, 11, 457, 291, 393, 536, 411, 321, 434, 3297, 294], "temperature": 0.0, "avg_logprob": -0.2099841361822084, "compression_ratio": 1.7170731707317073, "no_speech_prob": 3.2377370189351495e-06}, {"id": 898, "seek": 438752, "start": 4409.160000000001, "end": 4413.160000000001, "text": " the top end of this competition I", "tokens": [264, 1192, 917, 295, 341, 6211, 286], "temperature": 0.0, "avg_logprob": -0.2099841361822084, "compression_ratio": 1.7170731707317073, "no_speech_prob": 3.2377370189351495e-06}, {"id": 899, "seek": 441316, "start": 4413.16, "end": 4418.12, "text": " I actually tried running the third place getters code and", "tokens": [286, 767, 3031, 2614, 264, 2636, 1081, 483, 1559, 3089, 293], "temperature": 0.0, "avg_logprob": -0.28765302464581916, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.785291028179927e-06}, {"id": 900, "seek": 441316, "start": 4418.68, "end": 4426.24, "text": " Their final result was over point one so I actually think that we're should be comparing the private leaderboard", "tokens": [6710, 2572, 1874, 390, 670, 935, 472, 370, 286, 767, 519, 300, 321, 434, 820, 312, 15763, 264, 4551, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.28765302464581916, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.785291028179927e-06}, {"id": 901, "seek": 441316, "start": 4428.88, "end": 4433.5599999999995, "text": " So anyway, so you can see there basically there's a technique for dealing with", "tokens": [407, 4033, 11, 370, 291, 393, 536, 456, 1936, 456, 311, 257, 6532, 337, 6260, 365], "temperature": 0.0, "avg_logprob": -0.28765302464581916, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.785291028179927e-06}, {"id": 902, "seek": 441316, "start": 4434.12, "end": 4436.12, "text": " time series and", "tokens": [565, 2638, 293], "temperature": 0.0, "avg_logprob": -0.28765302464581916, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.785291028179927e-06}, {"id": 903, "seek": 441316, "start": 4436.28, "end": 4438.76, "text": " structured data and you know interestingly", "tokens": [18519, 1412, 293, 291, 458, 25873], "temperature": 0.0, "avg_logprob": -0.28765302464581916, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.785291028179927e-06}, {"id": 904, "seek": 443876, "start": 4438.76, "end": 4444.68, "text": " The group that that used this technique they actually wrote a paper about it. That's linked in this notebook", "tokens": [440, 1594, 300, 300, 1143, 341, 6532, 436, 767, 4114, 257, 3035, 466, 309, 13, 663, 311, 9408, 294, 341, 21060], "temperature": 0.0, "avg_logprob": -0.14714049184045128, "compression_ratio": 1.7366255144032923, "no_speech_prob": 2.4060839223238872e-06}, {"id": 905, "seek": 443876, "start": 4446.84, "end": 4451.6, "text": " When you compare it to the folks that won this competition and came second", "tokens": [1133, 291, 6794, 309, 281, 264, 4024, 300, 1582, 341, 6211, 293, 1361, 1150], "temperature": 0.0, "avg_logprob": -0.14714049184045128, "compression_ratio": 1.7366255144032923, "no_speech_prob": 2.4060839223238872e-06}, {"id": 906, "seek": 443876, "start": 4452.04, "end": 4458.280000000001, "text": " They did the other folks did way more feature engineering like the winners of this competition were actually", "tokens": [814, 630, 264, 661, 4024, 630, 636, 544, 4111, 7043, 411, 264, 17193, 295, 341, 6211, 645, 767], "temperature": 0.0, "avg_logprob": -0.14714049184045128, "compression_ratio": 1.7366255144032923, "no_speech_prob": 2.4060839223238872e-06}, {"id": 907, "seek": 443876, "start": 4459.0, "end": 4467.2, "text": " subject matter experts in logistics sales forecasting and so they had their own like code to create lots and lots of features and", "tokens": [3983, 1871, 8572, 294, 27420, 5763, 44331, 293, 370, 436, 632, 641, 1065, 411, 3089, 281, 1884, 3195, 293, 3195, 295, 4122, 293], "temperature": 0.0, "avg_logprob": -0.14714049184045128, "compression_ratio": 1.7366255144032923, "no_speech_prob": 2.4060839223238872e-06}, {"id": 908, "seek": 446720, "start": 4467.2, "end": 4473.44, "text": " And talking to the folks at Pinterest who built their very similar model for recommendations for Pinterest", "tokens": [400, 1417, 281, 264, 4024, 412, 37986, 567, 3094, 641, 588, 2531, 2316, 337, 10434, 337, 37986], "temperature": 0.0, "avg_logprob": -0.15141767614028034, "compression_ratio": 1.707142857142857, "no_speech_prob": 7.5278335316397715e-06}, {"id": 909, "seek": 446720, "start": 4473.44, "end": 4481.36, "text": " They said the same thing which is that when they switched from gradient boosting machines to deep learning they did like way way way less", "tokens": [814, 848, 264, 912, 551, 597, 307, 300, 562, 436, 16858, 490, 16235, 43117, 8379, 281, 2452, 2539, 436, 630, 411, 636, 636, 636, 1570], "temperature": 0.0, "avg_logprob": -0.15141767614028034, "compression_ratio": 1.707142857142857, "no_speech_prob": 7.5278335316397715e-06}, {"id": 910, "seek": 446720, "start": 4482.679999999999, "end": 4487.679999999999, "text": " Feature engineering it was a much much simpler model and requires much less maintenance", "tokens": [3697, 1503, 7043, 309, 390, 257, 709, 709, 18587, 2316, 293, 7029, 709, 1570, 11258], "temperature": 0.0, "avg_logprob": -0.15141767614028034, "compression_ratio": 1.707142857142857, "no_speech_prob": 7.5278335316397715e-06}, {"id": 911, "seek": 446720, "start": 4488.44, "end": 4494.4, "text": " And so this is like one of the big benefits of using this approach to deep learning you can get state-of-the-art results", "tokens": [400, 370, 341, 307, 411, 472, 295, 264, 955, 5311, 295, 1228, 341, 3109, 281, 2452, 2539, 291, 393, 483, 1785, 12, 2670, 12, 3322, 12, 446, 3542], "temperature": 0.0, "avg_logprob": -0.15141767614028034, "compression_ratio": 1.707142857142857, "no_speech_prob": 7.5278335316397715e-06}, {"id": 912, "seek": 449440, "start": 4494.4, "end": 4496.4, "text": " But with a lot less work", "tokens": [583, 365, 257, 688, 1570, 589], "temperature": 0.0, "avg_logprob": -0.22431924608018664, "compression_ratio": 1.4871794871794872, "no_speech_prob": 2.5457071387791075e-05}, {"id": 913, "seek": 449440, "start": 4498.12, "end": 4500.12, "text": " Yes", "tokens": [1079], "temperature": 0.0, "avg_logprob": -0.22431924608018664, "compression_ratio": 1.4871794871794872, "no_speech_prob": 2.5457071387791075e-05}, {"id": 914, "seek": 449440, "start": 4502.04, "end": 4506.96, "text": " Are you using any I'm series in any of these fits", "tokens": [2014, 291, 1228, 604, 286, 478, 2638, 294, 604, 295, 613, 9001], "temperature": 0.0, "avg_logprob": -0.22431924608018664, "compression_ratio": 1.4871794871794872, "no_speech_prob": 2.5457071387791075e-05}, {"id": 915, "seek": 449440, "start": 4508.5199999999995, "end": 4510.28, "text": " indirectly", "tokens": [37779], "temperature": 0.0, "avg_logprob": -0.22431924608018664, "compression_ratio": 1.4871794871794872, "no_speech_prob": 2.5457071387791075e-05}, {"id": 916, "seek": 449440, "start": 4510.28, "end": 4516.44, "text": " Absolutely using what we just saw we have a day of week month of year all that stuff columns", "tokens": [7021, 1228, 437, 321, 445, 1866, 321, 362, 257, 786, 295, 1243, 1618, 295, 1064, 439, 300, 1507, 13766], "temperature": 0.0, "avg_logprob": -0.22431924608018664, "compression_ratio": 1.4871794871794872, "no_speech_prob": 2.5457071387791075e-05}, {"id": 917, "seek": 449440, "start": 4517.24, "end": 4522.98, "text": " And most of them are being treated as categories. So we're building a distributed representation of January", "tokens": [400, 881, 295, 552, 366, 885, 8668, 382, 10479, 13, 407, 321, 434, 2390, 257, 12631, 10290, 295, 7061], "temperature": 0.0, "avg_logprob": -0.22431924608018664, "compression_ratio": 1.4871794871794872, "no_speech_prob": 2.5457071387791075e-05}, {"id": 918, "seek": 452298, "start": 4522.98, "end": 4528.44, "text": " We're building a distributed representation of Sunday. We're building a distributed representation of Christmas", "tokens": [492, 434, 2390, 257, 12631, 10290, 295, 7776, 13, 492, 434, 2390, 257, 12631, 10290, 295, 5272], "temperature": 0.0, "avg_logprob": -0.2520247141520182, "compression_ratio": 1.6464646464646464, "no_speech_prob": 1.2606687050720211e-05}, {"id": 919, "seek": 452298, "start": 4528.74, "end": 4530.74, "text": " So we're not using any", "tokens": [407, 321, 434, 406, 1228, 604], "temperature": 0.0, "avg_logprob": -0.2520247141520182, "compression_ratio": 1.6464646464646464, "no_speech_prob": 1.2606687050720211e-05}, {"id": 920, "seek": 452298, "start": 4532.139999999999, "end": 4535.78, "text": " Classic time series techniques all we're doing is", "tokens": [25008, 565, 2638, 7512, 439, 321, 434, 884, 307], "temperature": 0.0, "avg_logprob": -0.2520247141520182, "compression_ratio": 1.6464646464646464, "no_speech_prob": 1.2606687050720211e-05}, {"id": 921, "seek": 452298, "start": 4536.86, "end": 4539.179999999999, "text": " True fully connected layers in a neuron", "tokens": [13587, 4498, 4582, 7914, 294, 257, 34090], "temperature": 0.0, "avg_logprob": -0.2520247141520182, "compression_ratio": 1.6464646464646464, "no_speech_prob": 1.2606687050720211e-05}, {"id": 922, "seek": 452298, "start": 4541.339999999999, "end": 4543.339999999999, "text": " Matrix that's what", "tokens": [36274, 300, 311, 437], "temperature": 0.0, "avg_logprob": -0.2520247141520182, "compression_ratio": 1.6464646464646464, "no_speech_prob": 1.2606687050720211e-05}, {"id": 923, "seek": 452298, "start": 4544.0599999999995, "end": 4548.419999999999, "text": " Exactly exactly. Yes, so the embedding matrix is able to deal with this stuff like", "tokens": [7587, 2293, 13, 1079, 11, 370, 264, 12240, 3584, 8141, 307, 1075, 281, 2028, 365, 341, 1507, 411], "temperature": 0.0, "avg_logprob": -0.2520247141520182, "compression_ratio": 1.6464646464646464, "no_speech_prob": 1.2606687050720211e-05}, {"id": 924, "seek": 454842, "start": 4548.42, "end": 4552.46, "text": " day of week periodicity and so forth in a way", "tokens": [786, 295, 1243, 27790, 507, 293, 370, 5220, 294, 257, 636], "temperature": 0.0, "avg_logprob": -0.22402167964626002, "compression_ratio": 1.5363128491620113, "no_speech_prob": 1.0952559023280628e-05}, {"id": 925, "seek": 454842, "start": 4553.78, "end": 4555.78, "text": " richer way than any", "tokens": [29021, 636, 813, 604], "temperature": 0.0, "avg_logprob": -0.22402167964626002, "compression_ratio": 1.5363128491620113, "no_speech_prob": 1.0952559023280628e-05}, {"id": 926, "seek": 454842, "start": 4556.42, "end": 4558.42, "text": " Standard time series technique I've ever come across", "tokens": [21298, 565, 2638, 6532, 286, 600, 1562, 808, 2108], "temperature": 0.0, "avg_logprob": -0.22402167964626002, "compression_ratio": 1.5363128491620113, "no_speech_prob": 1.0952559023280628e-05}, {"id": 927, "seek": 454842, "start": 4559.34, "end": 4561.34, "text": " one last question", "tokens": [472, 1036, 1168], "temperature": 0.0, "avg_logprob": -0.22402167964626002, "compression_ratio": 1.5363128491620113, "no_speech_prob": 1.0952559023280628e-05}, {"id": 928, "seek": 454842, "start": 4561.42, "end": 4568.22, "text": " The matrix in the earlier models when we did the CNN we did not pass it during the fit", "tokens": [440, 8141, 294, 264, 3071, 5245, 562, 321, 630, 264, 24859, 321, 630, 406, 1320, 309, 1830, 264, 3318], "temperature": 0.0, "avg_logprob": -0.22402167964626002, "compression_ratio": 1.5363128491620113, "no_speech_prob": 1.0952559023280628e-05}, {"id": 929, "seek": 454842, "start": 4568.46, "end": 4570.62, "text": " We passed it when the data was", "tokens": [492, 4678, 309, 562, 264, 1412, 390], "temperature": 0.0, "avg_logprob": -0.22402167964626002, "compression_ratio": 1.5363128491620113, "no_speech_prob": 1.0952559023280628e-05}, {"id": 930, "seek": 454842, "start": 4571.54, "end": 4573.54, "text": " When we got the data", "tokens": [1133, 321, 658, 264, 1412], "temperature": 0.0, "avg_logprob": -0.22402167964626002, "compression_ratio": 1.5363128491620113, "no_speech_prob": 1.0952559023280628e-05}, {"id": 931, "seek": 457354, "start": 4573.54, "end": 4578.98, "text": " So we're not passing anything to fit just the learning rate and the number of cycles", "tokens": [407, 321, 434, 406, 8437, 1340, 281, 3318, 445, 264, 2539, 3314, 293, 264, 1230, 295, 17796], "temperature": 0.0, "avg_logprob": -0.2184474162566356, "compression_ratio": 1.6307692307692307, "no_speech_prob": 5.014689577365061e-06}, {"id": 932, "seek": 457354, "start": 4579.46, "end": 4582.78, "text": " In this case we're passing in metrics because we want to print out some extra stuff", "tokens": [682, 341, 1389, 321, 434, 8437, 294, 16367, 570, 321, 528, 281, 4482, 484, 512, 2857, 1507], "temperature": 0.0, "avg_logprob": -0.2184474162566356, "compression_ratio": 1.6307692307692307, "no_speech_prob": 5.014689577365061e-06}, {"id": 933, "seek": 457354, "start": 4584.3, "end": 4589.46, "text": " There is a difference in that we're calling data dot get learner so with", "tokens": [821, 307, 257, 2649, 294, 300, 321, 434, 5141, 1412, 5893, 483, 33347, 370, 365], "temperature": 0.0, "avg_logprob": -0.2184474162566356, "compression_ratio": 1.6307692307692307, "no_speech_prob": 5.014689577365061e-06}, {"id": 934, "seek": 457354, "start": 4591.26, "end": 4593.26, "text": " The imaging approach", "tokens": [440, 25036, 3109], "temperature": 0.0, "avg_logprob": -0.2184474162566356, "compression_ratio": 1.6307692307692307, "no_speech_prob": 5.014689577365061e-06}, {"id": 935, "seek": 457354, "start": 4594.58, "end": 4598.38, "text": " We just go learner dot trained and pass it the data", "tokens": [492, 445, 352, 33347, 5893, 8895, 293, 1320, 309, 264, 1412], "temperature": 0.0, "avg_logprob": -0.2184474162566356, "compression_ratio": 1.6307692307692307, "no_speech_prob": 5.014689577365061e-06}, {"id": 936, "seek": 457354, "start": 4599.42, "end": 4600.66, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.2184474162566356, "compression_ratio": 1.6307692307692307, "no_speech_prob": 5.014689577365061e-06}, {"id": 937, "seek": 460066, "start": 4600.66, "end": 4606.7, "text": " In for these kinds of models in fact for a lot of the models the model that we build", "tokens": [682, 337, 613, 3685, 295, 5245, 294, 1186, 337, 257, 688, 295, 264, 5245, 264, 2316, 300, 321, 1322], "temperature": 0.0, "avg_logprob": -0.1964491421414405, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.026127958743018e-06}, {"id": 938, "seek": 460066, "start": 4607.62, "end": 4613.38, "text": " Depends on the data in this case. We actually need to know like what embedding matrices do we have?", "tokens": [4056, 2581, 322, 264, 1412, 294, 341, 1389, 13, 492, 767, 643, 281, 458, 411, 437, 12240, 3584, 32284, 360, 321, 362, 30], "temperature": 0.0, "avg_logprob": -0.1964491421414405, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.026127958743018e-06}, {"id": 939, "seek": 460066, "start": 4613.94, "end": 4619.18, "text": " And stuff like that so in this case it's actually the data object that creates the learner", "tokens": [400, 1507, 411, 300, 370, 294, 341, 1389, 309, 311, 767, 264, 1412, 2657, 300, 7829, 264, 33347], "temperature": 0.0, "avg_logprob": -0.1964491421414405, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.026127958743018e-06}, {"id": 940, "seek": 460066, "start": 4619.18, "end": 4622.46, "text": " So yeah, it is it is a bit upside down to what we've seen before", "tokens": [407, 1338, 11, 309, 307, 309, 307, 257, 857, 14119, 760, 281, 437, 321, 600, 1612, 949], "temperature": 0.0, "avg_logprob": -0.1964491421414405, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.026127958743018e-06}, {"id": 941, "seek": 462246, "start": 4622.46, "end": 4627.66, "text": " Yeah, so just to summarize or maybe I'm confused", "tokens": [865, 11, 370, 445, 281, 20858, 420, 1310, 286, 478, 9019], "temperature": 0.0, "avg_logprob": -0.48696855955486057, "compression_ratio": 1.541062801932367, "no_speech_prob": 2.2824538973509334e-05}, {"id": 942, "seek": 462246, "start": 4628.78, "end": 4634.22, "text": " So in this case what we are doing is like we have some kind of a structured data", "tokens": [407, 294, 341, 1389, 437, 321, 366, 884, 307, 411, 321, 362, 512, 733, 295, 257, 18519, 1412], "temperature": 0.0, "avg_logprob": -0.48696855955486057, "compression_ratio": 1.541062801932367, "no_speech_prob": 2.2824538973509334e-05}, {"id": 943, "seek": 462246, "start": 4634.22, "end": 4636.22, "text": " We did feature engineering", "tokens": [492, 630, 4111, 7043], "temperature": 0.0, "avg_logprob": -0.48696855955486057, "compression_ratio": 1.541062801932367, "no_speech_prob": 2.2824538973509334e-05}, {"id": 944, "seek": 462246, "start": 4636.78, "end": 4641.46, "text": " We got some column in our database or something similar", "tokens": [492, 658, 512, 7738, 294, 527, 8149, 420, 746, 2531], "temperature": 0.0, "avg_logprob": -0.48696855955486057, "compression_ratio": 1.541062801932367, "no_speech_prob": 2.2824538973509334e-05}, {"id": 945, "seek": 462246, "start": 4642.02, "end": 4648.74, "text": " Apparent is data frame. Yeah, yeah data frame and then we are mapping it to deep learning by using this in", "tokens": [3132, 1898, 307, 1412, 3920, 13, 865, 11, 1338, 1412, 3920, 293, 550, 321, 366, 18350, 309, 281, 2452, 2539, 538, 1228, 341, 294], "temperature": 0.0, "avg_logprob": -0.48696855955486057, "compression_ratio": 1.541062801932367, "no_speech_prob": 2.2824538973509334e-05}, {"id": 946, "seek": 464874, "start": 4648.74, "end": 4654.46, "text": " Embedding matrix for the categorical variables so the continuous we just put them straight in", "tokens": [24234, 292, 3584, 8141, 337, 264, 19250, 804, 9102, 370, 264, 10957, 321, 445, 829, 552, 2997, 294], "temperature": 0.0, "avg_logprob": -0.38808694216284423, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.373626168468036e-05}, {"id": 947, "seek": 464874, "start": 4654.46, "end": 4661.66, "text": " So so all I need to do is like if I have if I have already have a feature engineering model", "tokens": [407, 370, 439, 286, 643, 281, 360, 307, 411, 498, 286, 362, 498, 286, 362, 1217, 362, 257, 4111, 7043, 2316], "temperature": 0.0, "avg_logprob": -0.38808694216284423, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.373626168468036e-05}, {"id": 948, "seek": 464874, "start": 4661.66, "end": 4667.78, "text": " Yeah, then to map it to deep learning. I just have to figure out which one I can move into categorical", "tokens": [865, 11, 550, 281, 4471, 309, 281, 2452, 2539, 13, 286, 445, 362, 281, 2573, 484, 597, 472, 286, 393, 1286, 666, 19250, 804], "temperature": 0.0, "avg_logprob": -0.38808694216284423, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.373626168468036e-05}, {"id": 949, "seek": 464874, "start": 4667.78, "end": 4670.38, "text": " And then yeah, it learn by itself. Yeah, great question", "tokens": [400, 550, 1338, 11, 309, 1466, 538, 2564, 13, 865, 11, 869, 1168], "temperature": 0.0, "avg_logprob": -0.38808694216284423, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.373626168468036e-05}, {"id": 950, "seek": 464874, "start": 4670.38, "end": 4674.7, "text": " So yes exactly if you want to use this on your own database", "tokens": [407, 2086, 2293, 498, 291, 528, 281, 764, 341, 322, 428, 1065, 8149], "temperature": 0.0, "avg_logprob": -0.38808694216284423, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.373626168468036e-05}, {"id": 951, "seek": 467470, "start": 4674.7, "end": 4679.099999999999, "text": " So yes exactly if you want to use this on your own data set", "tokens": [407, 2086, 2293, 498, 291, 528, 281, 764, 341, 322, 428, 1065, 1412, 992], "temperature": 0.0, "avg_logprob": -0.21535535760827967, "compression_ratio": 1.6568047337278107, "no_speech_prob": 2.8573056169989286e-06}, {"id": 952, "seek": 467470, "start": 4679.94, "end": 4685.099999999999, "text": " Step one is list the categorical variable names list the continuous variable names", "tokens": [5470, 472, 307, 1329, 264, 19250, 804, 7006, 5288, 1329, 264, 10957, 7006, 5288], "temperature": 0.0, "avg_logprob": -0.21535535760827967, "compression_ratio": 1.6568047337278107, "no_speech_prob": 2.8573056169989286e-06}, {"id": 953, "seek": 467470, "start": 4686.62, "end": 4688.94, "text": " Put it in a data frame the pandas data frame", "tokens": [4935, 309, 294, 257, 1412, 3920, 264, 4565, 296, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.21535535760827967, "compression_ratio": 1.6568047337278107, "no_speech_prob": 2.8573056169989286e-06}, {"id": 954, "seek": 467470, "start": 4690.62, "end": 4692.74, "text": " Step two is to", "tokens": [5470, 732, 307, 281], "temperature": 0.0, "avg_logprob": -0.21535535760827967, "compression_ratio": 1.6568047337278107, "no_speech_prob": 2.8573056169989286e-06}, {"id": 955, "seek": 467470, "start": 4694.94, "end": 4698.74, "text": " Create a list of which row indexes do you want new validation set?", "tokens": [20248, 257, 1329, 295, 597, 5386, 8186, 279, 360, 291, 528, 777, 24071, 992, 30], "temperature": 0.0, "avg_logprob": -0.21535535760827967, "compression_ratio": 1.6568047337278107, "no_speech_prob": 2.8573056169989286e-06}, {"id": 956, "seek": 467470, "start": 4700.0199999999995, "end": 4702.0199999999995, "text": " step three", "tokens": [1823, 1045], "temperature": 0.0, "avg_logprob": -0.21535535760827967, "compression_ratio": 1.6568047337278107, "no_speech_prob": 2.8573056169989286e-06}, {"id": 957, "seek": 470202, "start": 4702.02, "end": 4704.540000000001, "text": " Is to call this line of code?", "tokens": [1119, 281, 818, 341, 1622, 295, 3089, 30], "temperature": 0.0, "avg_logprob": -0.1936269985732212, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.857305844372604e-06}, {"id": 958, "seek": 470202, "start": 4705.38, "end": 4708.64, "text": " Using this exact like these exact you can just copy and paste it", "tokens": [11142, 341, 1900, 411, 613, 1900, 291, 393, 445, 5055, 293, 9163, 309], "temperature": 0.0, "avg_logprob": -0.1936269985732212, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.857305844372604e-06}, {"id": 959, "seek": 470202, "start": 4709.820000000001, "end": 4715.26, "text": " step four is to create your list of how big you want each embedding matrix to be and", "tokens": [1823, 1451, 307, 281, 1884, 428, 1329, 295, 577, 955, 291, 528, 1184, 12240, 3584, 8141, 281, 312, 293], "temperature": 0.0, "avg_logprob": -0.1936269985732212, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.857305844372604e-06}, {"id": 960, "seek": 470202, "start": 4716.740000000001, "end": 4719.620000000001, "text": " Then step five is to call get learner", "tokens": [1396, 1823, 1732, 307, 281, 818, 483, 33347], "temperature": 0.0, "avg_logprob": -0.1936269985732212, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.857305844372604e-06}, {"id": 961, "seek": 470202, "start": 4720.34, "end": 4722.900000000001, "text": " You can use these exact parameters to start with", "tokens": [509, 393, 764, 613, 1900, 9834, 281, 722, 365], "temperature": 0.0, "avg_logprob": -0.1936269985732212, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.857305844372604e-06}, {"id": 962, "seek": 470202, "start": 4723.820000000001, "end": 4729.38, "text": " And if it overfits or underfits you can fit it with them and then the final step is to call", "tokens": [400, 498, 309, 670, 13979, 420, 833, 13979, 291, 393, 3318, 309, 365, 552, 293, 550, 264, 2572, 1823, 307, 281, 818], "temperature": 0.0, "avg_logprob": -0.1936269985732212, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.857305844372604e-06}, {"id": 963, "seek": 472938, "start": 4729.38, "end": 4734.58, "text": " Fit so yeah, almost all of this code will be nearly identical", "tokens": [29263, 370, 1338, 11, 1920, 439, 295, 341, 3089, 486, 312, 6217, 14800], "temperature": 0.0, "avg_logprob": -0.26181737263997396, "compression_ratio": 1.5181347150259068, "no_speech_prob": 2.2125097530079074e-05}, {"id": 964, "seek": 472938, "start": 4740.1, "end": 4742.58, "text": " Have a couple of questions one is", "tokens": [3560, 257, 1916, 295, 1651, 472, 307], "temperature": 0.0, "avg_logprob": -0.26181737263997396, "compression_ratio": 1.5181347150259068, "no_speech_prob": 2.2125097530079074e-05}, {"id": 965, "seek": 472938, "start": 4743.66, "end": 4749.3, "text": " How is data augmentation can be used in this case and the second one is?", "tokens": [1012, 307, 1412, 14501, 19631, 393, 312, 1143, 294, 341, 1389, 293, 264, 1150, 472, 307, 30], "temperature": 0.0, "avg_logprob": -0.26181737263997396, "compression_ratio": 1.5181347150259068, "no_speech_prob": 2.2125097530079074e-05}, {"id": 966, "seek": 474930, "start": 4749.3, "end": 4758.4400000000005, "text": " What what are dropouts doing in here okay, so data augmentation. I have no idea. I mean that's a really interesting question", "tokens": [708, 437, 366, 3270, 7711, 884, 294, 510, 1392, 11, 370, 1412, 14501, 19631, 13, 286, 362, 572, 1558, 13, 286, 914, 300, 311, 257, 534, 1880, 1168], "temperature": 0.0, "avg_logprob": -0.22253860965851816, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.497083758295048e-05}, {"id": 967, "seek": 474930, "start": 4761.06, "end": 4763.14, "text": " I think it's got to be domain specific", "tokens": [286, 519, 309, 311, 658, 281, 312, 9274, 2685], "temperature": 0.0, "avg_logprob": -0.22253860965851816, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.497083758295048e-05}, {"id": 968, "seek": 474930, "start": 4763.14, "end": 4768.38, "text": " I've never seen any paper or anybody in industry doing data augmentation with structured data and deep learning", "tokens": [286, 600, 1128, 1612, 604, 3035, 420, 4472, 294, 3518, 884, 1412, 14501, 19631, 365, 18519, 1412, 293, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.22253860965851816, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.497083758295048e-05}, {"id": 969, "seek": 474930, "start": 4768.62, "end": 4772.1, "text": " So I don't I think it can be done. I just haven't seen it done", "tokens": [407, 286, 500, 380, 286, 519, 309, 393, 312, 1096, 13, 286, 445, 2378, 380, 1612, 309, 1096], "temperature": 0.0, "avg_logprob": -0.22253860965851816, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.497083758295048e-05}, {"id": 970, "seek": 474930, "start": 4773.18, "end": 4775.18, "text": " What is dropout doing?", "tokens": [708, 307, 3270, 346, 884, 30], "temperature": 0.0, "avg_logprob": -0.22253860965851816, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.497083758295048e-05}, {"id": 971, "seek": 477518, "start": 4775.18, "end": 4779.5, "text": " Exactly the same as before so at each point", "tokens": [7587, 264, 912, 382, 949, 370, 412, 1184, 935], "temperature": 0.0, "avg_logprob": -0.17143347422281902, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.7061779544746969e-06}, {"id": 972, "seek": 477518, "start": 4780.42, "end": 4782.42, "text": " we have", "tokens": [321, 362], "temperature": 0.0, "avg_logprob": -0.17143347422281902, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.7061779544746969e-06}, {"id": 973, "seek": 477518, "start": 4785.46, "end": 4788.66, "text": " The output of each of these linear layers is just a", "tokens": [440, 5598, 295, 1184, 295, 613, 8213, 7914, 307, 445, 257], "temperature": 0.0, "avg_logprob": -0.17143347422281902, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.7061779544746969e-06}, {"id": 974, "seek": 477518, "start": 4789.38, "end": 4796.42, "text": " Rank one tensor and so dropout is going to go ahead and say let's throw away half of the activations", "tokens": [35921, 472, 40863, 293, 370, 3270, 346, 307, 516, 281, 352, 2286, 293, 584, 718, 311, 3507, 1314, 1922, 295, 264, 2430, 763], "temperature": 0.0, "avg_logprob": -0.17143347422281902, "compression_ratio": 1.597883597883598, "no_speech_prob": 1.7061779544746969e-06}, {"id": 975, "seek": 479642, "start": 4796.42, "end": 4803.46, "text": " and the very first dropout embedding dropout literally goes through the embedding matrix and says", "tokens": [293, 264, 588, 700, 3270, 346, 12240, 3584, 3270, 346, 3736, 1709, 807, 264, 12240, 3584, 8141, 293, 1619], "temperature": 0.0, "avg_logprob": -0.16780890448618743, "compression_ratio": 1.5064935064935066, "no_speech_prob": 9.132524496635597e-07}, {"id": 976, "seek": 479642, "start": 4805.38, "end": 4807.9800000000005, "text": " Let's throw away half the activations", "tokens": [961, 311, 3507, 1314, 1922, 264, 2430, 763], "temperature": 0.0, "avg_logprob": -0.16780890448618743, "compression_ratio": 1.5064935064935066, "no_speech_prob": 9.132524496635597e-07}, {"id": 977, "seek": 479642, "start": 4809.86, "end": 4816.9800000000005, "text": " That's it okay, let's take a break and let's come back at five past eight", "tokens": [663, 311, 309, 1392, 11, 718, 311, 747, 257, 1821, 293, 718, 311, 808, 646, 412, 1732, 1791, 3180], "temperature": 0.0, "avg_logprob": -0.16780890448618743, "compression_ratio": 1.5064935064935066, "no_speech_prob": 9.132524496635597e-07}, {"id": 978, "seek": 481698, "start": 4816.98, "end": 4824.98, "text": " Okay, thanks everybody", "tokens": [1033, 11, 3231, 2201], "temperature": 0.0, "avg_logprob": -0.2567011576432448, "compression_ratio": 1.3266666666666667, "no_speech_prob": 4.029426690976834e-06}, {"id": 979, "seek": 481698, "start": 4828.0199999999995, "end": 4830.0199999999995, "text": " So now", "tokens": [407, 586], "temperature": 0.0, "avg_logprob": -0.2567011576432448, "compression_ratio": 1.3266666666666667, "no_speech_prob": 4.029426690976834e-06}, {"id": 980, "seek": 481698, "start": 4830.7, "end": 4832.7, "text": " We're going to move into something", "tokens": [492, 434, 516, 281, 1286, 666, 746], "temperature": 0.0, "avg_logprob": -0.2567011576432448, "compression_ratio": 1.3266666666666667, "no_speech_prob": 4.029426690976834e-06}, {"id": 981, "seek": 481698, "start": 4833.62, "end": 4840.299999999999, "text": " Equally exciting actually before I do I just mentioned that I had a good question during the break which was", "tokens": [15624, 379, 4670, 767, 949, 286, 360, 286, 445, 2835, 300, 286, 632, 257, 665, 1168, 1830, 264, 1821, 597, 390], "temperature": 0.0, "avg_logprob": -0.2567011576432448, "compression_ratio": 1.3266666666666667, "no_speech_prob": 4.029426690976834e-06}, {"id": 982, "seek": 481698, "start": 4841.299999999999, "end": 4843.44, "text": " What's the downside like?", "tokens": [708, 311, 264, 25060, 411, 30], "temperature": 0.0, "avg_logprob": -0.2567011576432448, "compression_ratio": 1.3266666666666667, "no_speech_prob": 4.029426690976834e-06}, {"id": 983, "seek": 484344, "start": 4843.44, "end": 4846.839999999999, "text": " Like look almost no one's using this", "tokens": [1743, 574, 1920, 572, 472, 311, 1228, 341], "temperature": 0.0, "avg_logprob": -0.19986069997151693, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.7264525215286994e-06}, {"id": 984, "seek": 484344, "start": 4848.879999999999, "end": 4850.599999999999, "text": " Why not", "tokens": [1545, 406], "temperature": 0.0, "avg_logprob": -0.19986069997151693, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.7264525215286994e-06}, {"id": 985, "seek": 484344, "start": 4850.599999999999, "end": 4854.679999999999, "text": " And and basically I think the answer is like as we discussed before", "tokens": [400, 293, 1936, 286, 519, 264, 1867, 307, 411, 382, 321, 7152, 949], "temperature": 0.0, "avg_logprob": -0.19986069997151693, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.7264525215286994e-06}, {"id": 986, "seek": 484344, "start": 4855.16, "end": 4860.28, "text": " No one in academia almost is working on this because it's not something that people really publish on", "tokens": [883, 472, 294, 28937, 1920, 307, 1364, 322, 341, 570, 309, 311, 406, 746, 300, 561, 534, 11374, 322], "temperature": 0.0, "avg_logprob": -0.19986069997151693, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.7264525215286994e-06}, {"id": 987, "seek": 484344, "start": 4862.4, "end": 4869.0199999999995, "text": " And as a result there haven't been really great examples where people could look at and say oh here's a technique that works well", "tokens": [400, 382, 257, 1874, 456, 2378, 380, 668, 534, 869, 5110, 689, 561, 727, 574, 412, 293, 584, 1954, 510, 311, 257, 6532, 300, 1985, 731], "temperature": 0.0, "avg_logprob": -0.19986069997151693, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.7264525215286994e-06}, {"id": 988, "seek": 484344, "start": 4869.0199999999995, "end": 4871.12, "text": " So let's have our company implement it", "tokens": [407, 718, 311, 362, 527, 2237, 4445, 309], "temperature": 0.0, "avg_logprob": -0.19986069997151693, "compression_ratio": 1.6437768240343347, "no_speech_prob": 2.7264525215286994e-06}, {"id": 989, "seek": 487112, "start": 4871.12, "end": 4873.88, "text": " but perhaps equally importantly", "tokens": [457, 4317, 12309, 8906], "temperature": 0.0, "avg_logprob": -0.22283757816661487, "compression_ratio": 1.5844155844155845, "no_speech_prob": 2.295901822435553e-06}, {"id": 990, "seek": 487112, "start": 4874.8, "end": 4878.5199999999995, "text": " Until now with this fast AI library there hasn't been any", "tokens": [9088, 586, 365, 341, 2370, 7318, 6405, 456, 6132, 380, 668, 604], "temperature": 0.0, "avg_logprob": -0.22283757816661487, "compression_ratio": 1.5844155844155845, "no_speech_prob": 2.295901822435553e-06}, {"id": 991, "seek": 487112, "start": 4879.08, "end": 4884.4, "text": " Way to to do it conveniently if you wanted to implement one of these models", "tokens": [9558, 281, 281, 360, 309, 44375, 498, 291, 1415, 281, 4445, 472, 295, 613, 5245], "temperature": 0.0, "avg_logprob": -0.22283757816661487, "compression_ratio": 1.5844155844155845, "no_speech_prob": 2.295901822435553e-06}, {"id": 992, "seek": 487112, "start": 4884.84, "end": 4887.12, "text": " You had to write all the custom code", "tokens": [509, 632, 281, 2464, 439, 264, 2375, 3089], "temperature": 0.0, "avg_logprob": -0.22283757816661487, "compression_ratio": 1.5844155844155845, "no_speech_prob": 2.295901822435553e-06}, {"id": 993, "seek": 487112, "start": 4887.84, "end": 4891.9, "text": " Yourself where else now as we discussed it's you know six", "tokens": [2260, 927, 689, 1646, 586, 382, 321, 7152, 309, 311, 291, 458, 2309], "temperature": 0.0, "avg_logprob": -0.22283757816661487, "compression_ratio": 1.5844155844155845, "no_speech_prob": 2.295901822435553e-06}, {"id": 994, "seek": 489190, "start": 4891.9, "end": 4900.339999999999, "text": " It's basically a six-step process you know involving about you know not much more than six lines of code", "tokens": [467, 311, 1936, 257, 2309, 12, 16792, 1399, 291, 458, 17030, 466, 291, 458, 406, 709, 544, 813, 2309, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.15440991413162417, "compression_ratio": 1.5764192139737991, "no_speech_prob": 2.332057874809834e-06}, {"id": 995, "seek": 489190, "start": 4902.099999999999, "end": 4908.86, "text": " So the reason I mentioned this is to say like I think there are a lot of big commercial and scientific", "tokens": [407, 264, 1778, 286, 2835, 341, 307, 281, 584, 411, 286, 519, 456, 366, 257, 688, 295, 955, 6841, 293, 8134], "temperature": 0.0, "avg_logprob": -0.15440991413162417, "compression_ratio": 1.5764192139737991, "no_speech_prob": 2.332057874809834e-06}, {"id": 996, "seek": 489190, "start": 4909.54, "end": 4915.86, "text": " Opportunities to use this to solve problems that previously haven't been solved very well before", "tokens": [39441, 1088, 281, 764, 341, 281, 5039, 2740, 300, 8046, 2378, 380, 668, 13041, 588, 731, 949], "temperature": 0.0, "avg_logprob": -0.15440991413162417, "compression_ratio": 1.5764192139737991, "no_speech_prob": 2.332057874809834e-06}, {"id": 997, "seek": 489190, "start": 4917.299999999999, "end": 4920.259999999999, "text": " So like I'll be really interested to hear if some of you", "tokens": [407, 411, 286, 603, 312, 534, 3102, 281, 1568, 498, 512, 295, 291], "temperature": 0.0, "avg_logprob": -0.15440991413162417, "compression_ratio": 1.5764192139737991, "no_speech_prob": 2.332057874809834e-06}, {"id": 998, "seek": 492026, "start": 4920.26, "end": 4921.42, "text": " try", "tokens": [853], "temperature": 0.0, "avg_logprob": -0.22490184141857789, "compression_ratio": 1.6866952789699572, "no_speech_prob": 7.889165317465086e-06}, {"id": 999, "seek": 492026, "start": 4921.42, "end": 4923.820000000001, "text": " This out you know maybe on like", "tokens": [639, 484, 291, 458, 1310, 322, 411], "temperature": 0.0, "avg_logprob": -0.22490184141857789, "compression_ratio": 1.6866952789699572, "no_speech_prob": 7.889165317465086e-06}, {"id": 1000, "seek": 492026, "start": 4924.5, "end": 4929.42, "text": " Old Kaggle competitions you might find like oh, I would have won this if I'd use this technique", "tokens": [8633, 48751, 22631, 26185, 291, 1062, 915, 411, 1954, 11, 286, 576, 362, 1582, 341, 498, 286, 1116, 764, 341, 6532], "temperature": 0.0, "avg_logprob": -0.22490184141857789, "compression_ratio": 1.6866952789699572, "no_speech_prob": 7.889165317465086e-06}, {"id": 1001, "seek": 492026, "start": 4929.42, "end": 4933.9400000000005, "text": " That would be interesting or if you've got some data set you work with at work", "tokens": [663, 576, 312, 1880, 420, 498, 291, 600, 658, 512, 1412, 992, 291, 589, 365, 412, 589], "temperature": 0.0, "avg_logprob": -0.22490184141857789, "compression_ratio": 1.6866952789699572, "no_speech_prob": 7.889165317465086e-06}, {"id": 1002, "seek": 492026, "start": 4933.9400000000005, "end": 4938.74, "text": " You know some kind of different model that you've been doing with a GBM or a random forest does this help", "tokens": [509, 458, 512, 733, 295, 819, 2316, 300, 291, 600, 668, 884, 365, 257, 460, 18345, 420, 257, 4974, 6719, 775, 341, 854], "temperature": 0.0, "avg_logprob": -0.22490184141857789, "compression_ratio": 1.6866952789699572, "no_speech_prob": 7.889165317465086e-06}, {"id": 1003, "seek": 492026, "start": 4940.74, "end": 4946.3, "text": " You know the thing I I'm still somewhat new to this I've been doing this for", "tokens": [509, 458, 264, 551, 286, 286, 478, 920, 8344, 777, 281, 341, 286, 600, 668, 884, 341, 337], "temperature": 0.0, "avg_logprob": -0.22490184141857789, "compression_ratio": 1.6866952789699572, "no_speech_prob": 7.889165317465086e-06}, {"id": 1004, "seek": 494630, "start": 4946.3, "end": 4951.900000000001, "text": " Basically since the start of the year was when I started working on these structured deep learning models", "tokens": [8537, 1670, 264, 722, 295, 264, 1064, 390, 562, 286, 1409, 1364, 322, 613, 18519, 2452, 2539, 5245], "temperature": 0.0, "avg_logprob": -0.1627373014177595, "compression_ratio": 1.6689655172413793, "no_speech_prob": 9.080283234652597e-06}, {"id": 1005, "seek": 494630, "start": 4952.22, "end": 4954.7, "text": " So I haven't had enough opportunity to know", "tokens": [407, 286, 2378, 380, 632, 1547, 2650, 281, 458], "temperature": 0.0, "avg_logprob": -0.1627373014177595, "compression_ratio": 1.6689655172413793, "no_speech_prob": 9.080283234652597e-06}, {"id": 1006, "seek": 494630, "start": 4955.58, "end": 4959.52, "text": " Where might it fail it's worked for nearly everything I've tried it with so far", "tokens": [2305, 1062, 309, 3061, 309, 311, 2732, 337, 6217, 1203, 286, 600, 3031, 309, 365, 370, 1400], "temperature": 0.0, "avg_logprob": -0.1627373014177595, "compression_ratio": 1.6689655172413793, "no_speech_prob": 9.080283234652597e-06}, {"id": 1007, "seek": 494630, "start": 4960.7, "end": 4964.74, "text": " but yeah, I think this class is the first time that", "tokens": [457, 1338, 11, 286, 519, 341, 1508, 307, 264, 700, 565, 300], "temperature": 0.0, "avg_logprob": -0.1627373014177595, "compression_ratio": 1.6689655172413793, "no_speech_prob": 9.080283234652597e-06}, {"id": 1008, "seek": 494630, "start": 4965.5, "end": 4971.34, "text": " There's going to be like more than half a dozen people in the world who actually are working on this so I think you know as", "tokens": [821, 311, 516, 281, 312, 411, 544, 813, 1922, 257, 16654, 561, 294, 264, 1002, 567, 767, 366, 1364, 322, 341, 370, 286, 519, 291, 458, 382], "temperature": 0.0, "avg_logprob": -0.1627373014177595, "compression_ratio": 1.6689655172413793, "no_speech_prob": 9.080283234652597e-06}, {"id": 1009, "seek": 494630, "start": 4971.34, "end": 4975.16, "text": " A group we're going to hopefully learn a lot and build some interesting things", "tokens": [316, 1594, 321, 434, 516, 281, 4696, 1466, 257, 688, 293, 1322, 512, 1880, 721], "temperature": 0.0, "avg_logprob": -0.1627373014177595, "compression_ratio": 1.6689655172413793, "no_speech_prob": 9.080283234652597e-06}, {"id": 1010, "seek": 497516, "start": 4975.16, "end": 4980.28, "text": " And this would be a great thing if you're thinking of writing a post about something or here's an area that", "tokens": [400, 341, 576, 312, 257, 869, 551, 498, 291, 434, 1953, 295, 3579, 257, 2183, 466, 746, 420, 510, 311, 364, 1859, 300], "temperature": 0.0, "avg_logprob": -0.2474477119052533, "compression_ratio": 1.6754385964912282, "no_speech_prob": 1.2805284313799348e-05}, {"id": 1011, "seek": 497516, "start": 4981.46, "end": 4985.3, "text": " There's a couple of that. There's a post from Instacart about what they did", "tokens": [821, 311, 257, 1916, 295, 300, 13, 821, 311, 257, 2183, 490, 2730, 326, 446, 466, 437, 436, 630], "temperature": 0.0, "avg_logprob": -0.2474477119052533, "compression_ratio": 1.6754385964912282, "no_speech_prob": 1.2805284313799348e-05}, {"id": 1012, "seek": 497516, "start": 4986.34, "end": 4988.34, "text": " Pinterest has a", "tokens": [37986, 575, 257], "temperature": 0.0, "avg_logprob": -0.2474477119052533, "compression_ratio": 1.6754385964912282, "no_speech_prob": 1.2805284313799348e-05}, {"id": 1013, "seek": 497516, "start": 4988.9, "end": 4993.9, "text": " Riley AI video about what they did that's about it, and there's two academic papers", "tokens": [31373, 7318, 960, 466, 437, 436, 630, 300, 311, 466, 309, 11, 293, 456, 311, 732, 7778, 10577], "temperature": 0.0, "avg_logprob": -0.2474477119052533, "compression_ratio": 1.6754385964912282, "no_speech_prob": 1.2805284313799348e-05}, {"id": 1014, "seek": 497516, "start": 4994.5, "end": 5001.5199999999995, "text": " Both about Kaggle competition victories one from a Yoshi Yoshi Benjo and his group", "tokens": [6767, 466, 48751, 22631, 6211, 38259, 472, 490, 257, 45676, 45676, 3964, 5134, 293, 702, 1594], "temperature": 0.0, "avg_logprob": -0.2474477119052533, "compression_ratio": 1.6754385964912282, "no_speech_prob": 1.2805284313799348e-05}, {"id": 1015, "seek": 497516, "start": 5001.9, "end": 5003.9, "text": " They won a taxi", "tokens": [814, 1582, 257, 18984], "temperature": 0.0, "avg_logprob": -0.2474477119052533, "compression_ratio": 1.6754385964912282, "no_speech_prob": 1.2805284313799348e-05}, {"id": 1016, "seek": 500390, "start": 5003.9, "end": 5008.339999999999, "text": " destination forecasting competition and then also the one linked", "tokens": [12236, 44331, 6211, 293, 550, 611, 264, 472, 9408], "temperature": 0.0, "avg_logprob": -0.2237240228897486, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.260311021018424e-06}, {"id": 1017, "seek": 500390, "start": 5009.099999999999, "end": 5011.58, "text": " for this Rossman competition, so", "tokens": [337, 341, 16140, 1601, 6211, 11, 370], "temperature": 0.0, "avg_logprob": -0.2237240228897486, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.260311021018424e-06}, {"id": 1018, "seek": 500390, "start": 5012.54, "end": 5014.62, "text": " Yeah, there's some background on that all right", "tokens": [865, 11, 456, 311, 512, 3678, 322, 300, 439, 558], "temperature": 0.0, "avg_logprob": -0.2237240228897486, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.260311021018424e-06}, {"id": 1019, "seek": 500390, "start": 5015.379999999999, "end": 5017.379999999999, "text": " so language", "tokens": [370, 2856], "temperature": 0.0, "avg_logprob": -0.2237240228897486, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.260311021018424e-06}, {"id": 1020, "seek": 500390, "start": 5017.94, "end": 5019.94, "text": " natural language processing", "tokens": [3303, 2856, 9007], "temperature": 0.0, "avg_logprob": -0.2237240228897486, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.260311021018424e-06}, {"id": 1021, "seek": 500390, "start": 5020.0599999999995, "end": 5023.339999999999, "text": " Is the area which is?", "tokens": [1119, 264, 1859, 597, 307, 30], "temperature": 0.0, "avg_logprob": -0.2237240228897486, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.260311021018424e-06}, {"id": 1022, "seek": 500390, "start": 5024.179999999999, "end": 5029.54, "text": " Kind of like the most up-and-coming area of deep learning. It's kind of like two or three years behind", "tokens": [9242, 295, 411, 264, 881, 493, 12, 474, 12, 6590, 1859, 295, 2452, 2539, 13, 467, 311, 733, 295, 411, 732, 420, 1045, 924, 2261], "temperature": 0.0, "avg_logprob": -0.2237240228897486, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.260311021018424e-06}, {"id": 1023, "seek": 502954, "start": 5029.54, "end": 5038.22, "text": " computer vision in deep learning it was kind of like the the second area that deep learning started getting really popular in and", "tokens": [3820, 5201, 294, 2452, 2539, 309, 390, 733, 295, 411, 264, 264, 1150, 1859, 300, 2452, 2539, 1409, 1242, 534, 3743, 294, 293], "temperature": 0.0, "avg_logprob": -0.1937009699204389, "compression_ratio": 1.7575757575757576, "no_speech_prob": 6.04884098720504e-06}, {"id": 1024, "seek": 502954, "start": 5039.34, "end": 5041.34, "text": " You know computer vision", "tokens": [509, 458, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.1937009699204389, "compression_ratio": 1.7575757575757576, "no_speech_prob": 6.04884098720504e-06}, {"id": 1025, "seek": 502954, "start": 5041.66, "end": 5044.74, "text": " Got to the point where it was like the clear state-of-the-art", "tokens": [5803, 281, 264, 935, 689, 309, 390, 411, 264, 1850, 1785, 12, 2670, 12, 3322, 12, 446], "temperature": 0.0, "avg_logprob": -0.1937009699204389, "compression_ratio": 1.7575757575757576, "no_speech_prob": 6.04884098720504e-06}, {"id": 1026, "seek": 502954, "start": 5045.54, "end": 5048.9, "text": " For most computer vision things maybe in like 2014", "tokens": [1171, 881, 3820, 5201, 721, 1310, 294, 411, 8227], "temperature": 0.0, "avg_logprob": -0.1937009699204389, "compression_ratio": 1.7575757575757576, "no_speech_prob": 6.04884098720504e-06}, {"id": 1027, "seek": 502954, "start": 5049.14, "end": 5054.78, "text": " You know and some things in like 2012 and in NLP. We're still at the point where", "tokens": [509, 458, 293, 512, 721, 294, 411, 9125, 293, 294, 426, 45196, 13, 492, 434, 920, 412, 264, 935, 689], "temperature": 0.0, "avg_logprob": -0.1937009699204389, "compression_ratio": 1.7575757575757576, "no_speech_prob": 6.04884098720504e-06}, {"id": 1028, "seek": 505478, "start": 5054.78, "end": 5059.62, "text": " For a lot of things deep learning is now the state of the art, but not quite everything", "tokens": [1171, 257, 688, 295, 721, 2452, 2539, 307, 586, 264, 1785, 295, 264, 1523, 11, 457, 406, 1596, 1203], "temperature": 0.0, "avg_logprob": -0.18662467532687718, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.5779520481373766e-06}, {"id": 1029, "seek": 505478, "start": 5060.219999999999, "end": 5063.259999999999, "text": " but as you'll see the state of kind of", "tokens": [457, 382, 291, 603, 536, 264, 1785, 295, 733, 295], "temperature": 0.0, "avg_logprob": -0.18662467532687718, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.5779520481373766e-06}, {"id": 1030, "seek": 505478, "start": 5063.86, "end": 5070.5599999999995, "text": " The software and some of the concepts is much less mature than it is for computer vision", "tokens": [440, 4722, 293, 512, 295, 264, 10392, 307, 709, 1570, 14442, 813, 309, 307, 337, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.18662467532687718, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.5779520481373766e-06}, {"id": 1031, "seek": 505478, "start": 5071.42, "end": 5076.639999999999, "text": " So in general none of the stuff we talked about after computer vision is going to be as like", "tokens": [407, 294, 2674, 6022, 295, 264, 1507, 321, 2825, 466, 934, 3820, 5201, 307, 516, 281, 312, 382, 411], "temperature": 0.0, "avg_logprob": -0.18662467532687718, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.5779520481373766e-06}, {"id": 1032, "seek": 505478, "start": 5077.46, "end": 5081.0199999999995, "text": " Settled as the computer vision and stuff was so NLP", "tokens": [318, 3093, 1493, 382, 264, 3820, 5201, 293, 1507, 390, 370, 426, 45196], "temperature": 0.0, "avg_logprob": -0.18662467532687718, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.5779520481373766e-06}, {"id": 1033, "seek": 508102, "start": 5081.02, "end": 5084.26, "text": " One of the interesting things is in the last few months", "tokens": [1485, 295, 264, 1880, 721, 307, 294, 264, 1036, 1326, 2493], "temperature": 0.0, "avg_logprob": -0.143244257679692, "compression_ratio": 1.7449392712550607, "no_speech_prob": 1.8162104424845893e-06}, {"id": 1034, "seek": 508102, "start": 5084.740000000001, "end": 5091.22, "text": " Some of the good ideas from computer vision have started to spread into NLP for the first time and we've seen some really big", "tokens": [2188, 295, 264, 665, 3487, 490, 3820, 5201, 362, 1409, 281, 3974, 666, 426, 45196, 337, 264, 700, 565, 293, 321, 600, 1612, 512, 534, 955], "temperature": 0.0, "avg_logprob": -0.143244257679692, "compression_ratio": 1.7449392712550607, "no_speech_prob": 1.8162104424845893e-06}, {"id": 1035, "seek": 508102, "start": 5091.22, "end": 5094.96, "text": " Advances so a lot of the stuff you'll see in NLP is is pretty new", "tokens": [13634, 2676, 370, 257, 688, 295, 264, 1507, 291, 603, 536, 294, 426, 45196, 307, 307, 1238, 777], "temperature": 0.0, "avg_logprob": -0.143244257679692, "compression_ratio": 1.7449392712550607, "no_speech_prob": 1.8162104424845893e-06}, {"id": 1036, "seek": 508102, "start": 5096.9400000000005, "end": 5098.9400000000005, "text": " So I'm going to start with a particular", "tokens": [407, 286, 478, 516, 281, 722, 365, 257, 1729], "temperature": 0.0, "avg_logprob": -0.143244257679692, "compression_ratio": 1.7449392712550607, "no_speech_prob": 1.8162104424845893e-06}, {"id": 1037, "seek": 508102, "start": 5100.34, "end": 5105.580000000001, "text": " Kind of NLP problem and one of the things you'll find in NLP is like there are particular problems", "tokens": [9242, 295, 426, 45196, 1154, 293, 472, 295, 264, 721, 291, 603, 915, 294, 426, 45196, 307, 411, 456, 366, 1729, 2740], "temperature": 0.0, "avg_logprob": -0.143244257679692, "compression_ratio": 1.7449392712550607, "no_speech_prob": 1.8162104424845893e-06}, {"id": 1038, "seek": 508102, "start": 5105.580000000001, "end": 5107.620000000001, "text": " You can solve and they have particular names", "tokens": [509, 393, 5039, 293, 436, 362, 1729, 5288], "temperature": 0.0, "avg_logprob": -0.143244257679692, "compression_ratio": 1.7449392712550607, "no_speech_prob": 1.8162104424845893e-06}, {"id": 1039, "seek": 510762, "start": 5107.62, "end": 5112.0199999999995, "text": " and so there's a particular kind of problem in NLP called language modeling and", "tokens": [293, 370, 456, 311, 257, 1729, 733, 295, 1154, 294, 426, 45196, 1219, 2856, 15983, 293], "temperature": 0.0, "avg_logprob": -0.1911906842832212, "compression_ratio": 1.6917562724014337, "no_speech_prob": 6.962211955396924e-06}, {"id": 1040, "seek": 510762, "start": 5112.54, "end": 5120.24, "text": " Language modeling has a very specific definition. It means build a model where given a few words of a sentence", "tokens": [24445, 15983, 575, 257, 588, 2685, 7123, 13, 467, 1355, 1322, 257, 2316, 689, 2212, 257, 1326, 2283, 295, 257, 8174], "temperature": 0.0, "avg_logprob": -0.1911906842832212, "compression_ratio": 1.6917562724014337, "no_speech_prob": 6.962211955396924e-06}, {"id": 1041, "seek": 510762, "start": 5120.66, "end": 5123.16, "text": " Can you predict what the next word is going to be?", "tokens": [1664, 291, 6069, 437, 264, 958, 1349, 307, 516, 281, 312, 30], "temperature": 0.0, "avg_logprob": -0.1911906842832212, "compression_ratio": 1.6917562724014337, "no_speech_prob": 6.962211955396924e-06}, {"id": 1042, "seek": 510762, "start": 5123.9, "end": 5130.74, "text": " So if you're using your mobile phone and you're typing away and you press space and then it says like this is what the next", "tokens": [407, 498, 291, 434, 1228, 428, 6013, 2593, 293, 291, 434, 18444, 1314, 293, 291, 1886, 1901, 293, 550, 309, 1619, 411, 341, 307, 437, 264, 958], "temperature": 0.0, "avg_logprob": -0.1911906842832212, "compression_ratio": 1.6917562724014337, "no_speech_prob": 6.962211955396924e-06}, {"id": 1043, "seek": 510762, "start": 5130.74, "end": 5136.68, "text": " Word might be like Swift key does this like really well and Swift key actually uses deep learning for this", "tokens": [8725, 1062, 312, 411, 25539, 2141, 775, 341, 411, 534, 731, 293, 25539, 2141, 767, 4960, 2452, 2539, 337, 341], "temperature": 0.0, "avg_logprob": -0.1911906842832212, "compression_ratio": 1.6917562724014337, "no_speech_prob": 6.962211955396924e-06}, {"id": 1044, "seek": 513668, "start": 5136.68, "end": 5143.0, "text": " That's that's a language model. Okay, so it has a very specific meaning when we say language modeling", "tokens": [663, 311, 300, 311, 257, 2856, 2316, 13, 1033, 11, 370, 309, 575, 257, 588, 2685, 3620, 562, 321, 584, 2856, 15983], "temperature": 0.0, "avg_logprob": -0.1962875743488689, "compression_ratio": 1.5458333333333334, "no_speech_prob": 4.157343482802389e-06}, {"id": 1045, "seek": 513668, "start": 5143.0, "end": 5146.92, "text": " We mean a model that can predict the next word of a sentence", "tokens": [492, 914, 257, 2316, 300, 393, 6069, 264, 958, 1349, 295, 257, 8174], "temperature": 0.0, "avg_logprob": -0.1962875743488689, "compression_ratio": 1.5458333333333334, "no_speech_prob": 4.157343482802389e-06}, {"id": 1046, "seek": 513668, "start": 5148.04, "end": 5150.52, "text": " So let me give you an example. I downloaded", "tokens": [407, 718, 385, 976, 291, 364, 1365, 13, 286, 21748], "temperature": 0.0, "avg_logprob": -0.1962875743488689, "compression_ratio": 1.5458333333333334, "no_speech_prob": 4.157343482802389e-06}, {"id": 1047, "seek": 513668, "start": 5151.8, "end": 5153.96, "text": " about 18 months worth of", "tokens": [466, 2443, 2493, 3163, 295], "temperature": 0.0, "avg_logprob": -0.1962875743488689, "compression_ratio": 1.5458333333333334, "no_speech_prob": 4.157343482802389e-06}, {"id": 1048, "seek": 513668, "start": 5155.12, "end": 5159.12, "text": " papers from archive so for those of you that don't know it archive is", "tokens": [10577, 490, 23507, 370, 337, 729, 295, 291, 300, 500, 380, 458, 309, 23507, 307], "temperature": 0.0, "avg_logprob": -0.1962875743488689, "compression_ratio": 1.5458333333333334, "no_speech_prob": 4.157343482802389e-06}, {"id": 1049, "seek": 513668, "start": 5159.8, "end": 5165.1, "text": " The most popular preprint server in this community and various others", "tokens": [440, 881, 3743, 659, 14030, 7154, 294, 341, 1768, 293, 3683, 2357], "temperature": 0.0, "avg_logprob": -0.1962875743488689, "compression_ratio": 1.5458333333333334, "no_speech_prob": 4.157343482802389e-06}, {"id": 1050, "seek": 516510, "start": 5165.1, "end": 5168.5, "text": " and has you know lots of academic papers and", "tokens": [293, 575, 291, 458, 3195, 295, 7778, 10577, 293], "temperature": 0.0, "avg_logprob": -0.29655893325805666, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.1875563308858545e-06}, {"id": 1051, "seek": 516510, "start": 5169.58, "end": 5171.58, "text": " so I grabbed the", "tokens": [370, 286, 18607, 264], "temperature": 0.0, "avg_logprob": -0.29655893325805666, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.1875563308858545e-06}, {"id": 1052, "seek": 516510, "start": 5172.900000000001, "end": 5175.860000000001, "text": " Abstracts and the topics for each and so here's an example", "tokens": [46853, 1897, 82, 293, 264, 8378, 337, 1184, 293, 370, 510, 311, 364, 1365], "temperature": 0.0, "avg_logprob": -0.29655893325805666, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.1875563308858545e-06}, {"id": 1053, "seek": 516510, "start": 5176.3, "end": 5182.1, "text": " so the category of this particular paper was computer CSNI is computer science and networking and", "tokens": [370, 264, 7719, 295, 341, 1729, 3035, 390, 3820, 9460, 42496, 307, 3820, 3497, 293, 17985, 293], "temperature": 0.0, "avg_logprob": -0.29655893325805666, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.1875563308858545e-06}, {"id": 1054, "seek": 516510, "start": 5182.700000000001, "end": 5185.200000000001, "text": " Then the summary so let the abstract of the paper", "tokens": [1396, 264, 12691, 370, 718, 264, 12649, 295, 264, 3035], "temperature": 0.0, "avg_logprob": -0.29655893325805666, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.1875563308858545e-06}, {"id": 1055, "seek": 516510, "start": 5185.54, "end": 5190.9400000000005, "text": " Was saying the exploitation of mm wave bands is one of the key enabler for 5g mobile blah blah blah", "tokens": [3027, 1566, 264, 33122, 295, 11169, 5772, 13543, 307, 472, 295, 264, 2141, 465, 455, 1918, 337, 1025, 70, 6013, 12288, 12288, 12288], "temperature": 0.0, "avg_logprob": -0.29655893325805666, "compression_ratio": 1.680672268907563, "no_speech_prob": 3.1875563308858545e-06}, {"id": 1056, "seek": 519094, "start": 5190.94, "end": 5198.259999999999, "text": " Okay, so here's like an example a piece of text from my language model", "tokens": [1033, 11, 370, 510, 311, 411, 364, 1365, 257, 2522, 295, 2487, 490, 452, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.16978847063504732, "compression_ratio": 1.5727699530516432, "no_speech_prob": 1.6280451973216259e-06}, {"id": 1057, "seek": 519094, "start": 5199.46, "end": 5205.94, "text": " So I trained a language model on this archive data set that I downloaded and then I built a simple little test", "tokens": [407, 286, 8895, 257, 2856, 2316, 322, 341, 23507, 1412, 992, 300, 286, 21748, 293, 550, 286, 3094, 257, 2199, 707, 1500], "temperature": 0.0, "avg_logprob": -0.16978847063504732, "compression_ratio": 1.5727699530516432, "no_speech_prob": 1.6280451973216259e-06}, {"id": 1058, "seek": 519094, "start": 5206.259999999999, "end": 5208.259999999999, "text": " which basically", "tokens": [597, 1936], "temperature": 0.0, "avg_logprob": -0.16978847063504732, "compression_ratio": 1.5727699530516432, "no_speech_prob": 1.6280451973216259e-06}, {"id": 1059, "seek": 519094, "start": 5208.54, "end": 5212.139999999999, "text": " You would pass it some like priming text", "tokens": [509, 576, 1320, 309, 512, 411, 2886, 278, 2487], "temperature": 0.0, "avg_logprob": -0.16978847063504732, "compression_ratio": 1.5727699530516432, "no_speech_prob": 1.6280451973216259e-06}, {"id": 1060, "seek": 519094, "start": 5212.139999999999, "end": 5217.7, "text": " So you say like oh imagine you started reading a document that said category is computer science", "tokens": [407, 291, 584, 411, 1954, 3811, 291, 1409, 3760, 257, 4166, 300, 848, 7719, 307, 3820, 3497], "temperature": 0.0, "avg_logprob": -0.16978847063504732, "compression_ratio": 1.5727699530516432, "no_speech_prob": 1.6280451973216259e-06}, {"id": 1061, "seek": 521770, "start": 5217.7, "end": 5223.099999999999, "text": " Networking and the summary is algorithms that and then I said, please write", "tokens": [12640, 278, 293, 264, 12691, 307, 14642, 300, 293, 550, 286, 848, 11, 1767, 2464], "temperature": 0.0, "avg_logprob": -0.20649165295540017, "compression_ratio": 1.825531914893617, "no_speech_prob": 1.3496976407623151e-06}, {"id": 1062, "seek": 521770, "start": 5224.0199999999995, "end": 5228.179999999999, "text": " An archive abstract so it said that if it's a networking", "tokens": [1107, 23507, 12649, 370, 309, 848, 300, 498, 309, 311, 257, 17985], "temperature": 0.0, "avg_logprob": -0.20649165295540017, "compression_ratio": 1.825531914893617, "no_speech_prob": 1.3496976407623151e-06}, {"id": 1063, "seek": 521770, "start": 5228.94, "end": 5230.26, "text": " algorithms that", "tokens": [14642, 300], "temperature": 0.0, "avg_logprob": -0.20649165295540017, "compression_ratio": 1.825531914893617, "no_speech_prob": 1.3496976407623151e-06}, {"id": 1064, "seek": 521770, "start": 5230.26, "end": 5237.099999999999, "text": " Use the same network as a single node are not able to achieve the same performance as a traditional network based routing algorithms in this paper", "tokens": [8278, 264, 912, 3209, 382, 257, 2167, 9984, 366, 406, 1075, 281, 4584, 264, 912, 3389, 382, 257, 5164, 3209, 2361, 32722, 14642, 294, 341, 3035], "temperature": 0.0, "avg_logprob": -0.20649165295540017, "compression_ratio": 1.825531914893617, "no_speech_prob": 1.3496976407623151e-06}, {"id": 1065, "seek": 521770, "start": 5237.099999999999, "end": 5239.72, "text": " We propose a novel routing scheme, but okay", "tokens": [492, 17421, 257, 7613, 32722, 12232, 11, 457, 1392], "temperature": 0.0, "avg_logprob": -0.20649165295540017, "compression_ratio": 1.825531914893617, "no_speech_prob": 1.3496976407623151e-06}, {"id": 1066, "seek": 521770, "start": 5239.72, "end": 5246.5, "text": " So it it's learned by reading archive papers that somebody who was saying algorithms that", "tokens": [407, 309, 309, 311, 3264, 538, 3760, 23507, 10577, 300, 2618, 567, 390, 1566, 14642, 300], "temperature": 0.0, "avg_logprob": -0.20649165295540017, "compression_ratio": 1.825531914893617, "no_speech_prob": 1.3496976407623151e-06}, {"id": 1067, "seek": 524650, "start": 5246.5, "end": 5254.14, "text": " Where the word cat CSNI came before it is going to talk like this and remember it started out", "tokens": [2305, 264, 1349, 3857, 9460, 42496, 1361, 949, 309, 307, 516, 281, 751, 411, 341, 293, 1604, 309, 1409, 484], "temperature": 0.0, "avg_logprob": -0.1913535670230263, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.495160283113364e-06}, {"id": 1068, "seek": 524650, "start": 5254.14, "end": 5256.24, "text": " Not knowing English at all, right?", "tokens": [1726, 5276, 3669, 412, 439, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1913535670230263, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.495160283113364e-06}, {"id": 1069, "seek": 524650, "start": 5256.24, "end": 5262.18, "text": " It actually started out with an embedding matrix for every word in English that was random", "tokens": [467, 767, 1409, 484, 365, 364, 12240, 3584, 8141, 337, 633, 1349, 294, 3669, 300, 390, 4974], "temperature": 0.0, "avg_logprob": -0.1913535670230263, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.495160283113364e-06}, {"id": 1070, "seek": 524650, "start": 5263.02, "end": 5267.7, "text": " Okay, and by reading lots of archive papers it weren't what kind of words followed others", "tokens": [1033, 11, 293, 538, 3760, 3195, 295, 23507, 10577, 309, 4999, 380, 437, 733, 295, 2283, 6263, 2357], "temperature": 0.0, "avg_logprob": -0.1913535670230263, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.495160283113364e-06}, {"id": 1071, "seek": 524650, "start": 5267.7, "end": 5272.06, "text": " So then I tried what if we said cat computer science computer vision", "tokens": [407, 550, 286, 3031, 437, 498, 321, 848, 3857, 3820, 3497, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.1913535670230263, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.495160283113364e-06}, {"id": 1072, "seek": 524650, "start": 5272.82, "end": 5274.22, "text": " summary", "tokens": [12691], "temperature": 0.0, "avg_logprob": -0.1913535670230263, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.495160283113364e-06}, {"id": 1073, "seek": 524650, "start": 5274.22, "end": 5275.82, "text": " algorithms that", "tokens": [14642, 300], "temperature": 0.0, "avg_logprob": -0.1913535670230263, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.495160283113364e-06}, {"id": 1074, "seek": 527582, "start": 5275.82, "end": 5283.58, "text": " Use the same data to perform image classification are increasingly being used to improve the performance of image classification algorithms", "tokens": [8278, 264, 912, 1412, 281, 2042, 3256, 21538, 366, 12980, 885, 1143, 281, 3470, 264, 3389, 295, 3256, 21538, 14642], "temperature": 0.0, "avg_logprob": -0.16135822458470123, "compression_ratio": 1.7266187050359711, "no_speech_prob": 2.9479995191650232e-06}, {"id": 1075, "seek": 527582, "start": 5283.58, "end": 5290.0199999999995, "text": " And this paper we propose a novel method for image classification using a deep convolutional neural network parentheses CNN", "tokens": [400, 341, 3035, 321, 17421, 257, 7613, 3170, 337, 3256, 21538, 1228, 257, 2452, 45216, 304, 18161, 3209, 34153, 24859], "temperature": 0.0, "avg_logprob": -0.16135822458470123, "compression_ratio": 1.7266187050359711, "no_speech_prob": 2.9479995191650232e-06}, {"id": 1076, "seek": 527582, "start": 5290.38, "end": 5295.94, "text": " So you can see like it's kind of like almost the same sentence as back here", "tokens": [407, 291, 393, 536, 411, 309, 311, 733, 295, 411, 1920, 264, 912, 8174, 382, 646, 510], "temperature": 0.0, "avg_logprob": -0.16135822458470123, "compression_ratio": 1.7266187050359711, "no_speech_prob": 2.9479995191650232e-06}, {"id": 1077, "seek": 527582, "start": 5295.94, "end": 5301.24, "text": " But things are just changed into this world of computer vision rather than networking", "tokens": [583, 721, 366, 445, 3105, 666, 341, 1002, 295, 3820, 5201, 2831, 813, 17985], "temperature": 0.0, "avg_logprob": -0.16135822458470123, "compression_ratio": 1.7266187050359711, "no_speech_prob": 2.9479995191650232e-06}, {"id": 1078, "seek": 527582, "start": 5301.74, "end": 5304.299999999999, "text": " So I tried something else which is like, okay category", "tokens": [407, 286, 3031, 746, 1646, 597, 307, 411, 11, 1392, 7719], "temperature": 0.0, "avg_logprob": -0.16135822458470123, "compression_ratio": 1.7266187050359711, "no_speech_prob": 2.9479995191650232e-06}, {"id": 1079, "seek": 530430, "start": 5304.3, "end": 5311.4400000000005, "text": " computer vision and I created the world's shortest ever abstract algorithms and then I said title on", "tokens": [3820, 5201, 293, 286, 2942, 264, 1002, 311, 31875, 1562, 12649, 14642, 293, 550, 286, 848, 4876, 322], "temperature": 0.0, "avg_logprob": -0.21579478337214544, "compression_ratio": 1.6777251184834123, "no_speech_prob": 8.939634426496923e-06}, {"id": 1080, "seek": 530430, "start": 5312.2, "end": 5317.320000000001, "text": " And the title of this is going to be on the performance of deep learning for image classification", "tokens": [400, 264, 4876, 295, 341, 307, 516, 281, 312, 322, 264, 3389, 295, 2452, 2539, 337, 3256, 21538], "temperature": 0.0, "avg_logprob": -0.21579478337214544, "compression_ratio": 1.6777251184834123, "no_speech_prob": 8.939634426496923e-06}, {"id": 1081, "seek": 530430, "start": 5317.96, "end": 5320.74, "text": " Eos is end of string. So that's like end of title", "tokens": [462, 329, 307, 917, 295, 6798, 13, 407, 300, 311, 411, 917, 295, 4876], "temperature": 0.0, "avg_logprob": -0.21579478337214544, "compression_ratio": 1.6777251184834123, "no_speech_prob": 8.939634426496923e-06}, {"id": 1082, "seek": 530430, "start": 5321.400000000001, "end": 5328.400000000001, "text": " What if it was networking summary algorithms title on the performance of wireless networks as opposed to?", "tokens": [708, 498, 309, 390, 17985, 12691, 14642, 4876, 322, 264, 3389, 295, 14720, 9590, 382, 8851, 281, 30], "temperature": 0.0, "avg_logprob": -0.21579478337214544, "compression_ratio": 1.6777251184834123, "no_speech_prob": 8.939634426496923e-06}, {"id": 1083, "seek": 532840, "start": 5328.4, "end": 5334.0199999999995, "text": " Towards computer vision towards a new approach to image classification networking", "tokens": [48938, 3820, 5201, 3030, 257, 777, 3109, 281, 3256, 21538, 17985], "temperature": 0.0, "avg_logprob": -0.26407264709472655, "compression_ratio": 1.576036866359447, "no_speech_prob": 2.4439823391730897e-06}, {"id": 1084, "seek": 532840, "start": 5334.5199999999995, "end": 5337.839999999999, "text": " Towards a new approach to the analysis of wireless networks", "tokens": [48938, 257, 777, 3109, 281, 264, 5215, 295, 14720, 9590], "temperature": 0.0, "avg_logprob": -0.26407264709472655, "compression_ratio": 1.576036866359447, "no_speech_prob": 2.4439823391730897e-06}, {"id": 1085, "seek": 532840, "start": 5338.36, "end": 5344.04, "text": " So like I find this mind-blowing right I started out with some random matrices", "tokens": [407, 411, 286, 915, 341, 1575, 12, 43788, 558, 286, 1409, 484, 365, 512, 4974, 32284], "temperature": 0.0, "avg_logprob": -0.26407264709472655, "compression_ratio": 1.576036866359447, "no_speech_prob": 2.4439823391730897e-06}, {"id": 1086, "seek": 532840, "start": 5344.92, "end": 5347.28, "text": " Which are like literally no", "tokens": [3013, 366, 411, 3736, 572], "temperature": 0.0, "avg_logprob": -0.26407264709472655, "compression_ratio": 1.576036866359447, "no_speech_prob": 2.4439823391730897e-06}, {"id": 1087, "seek": 532840, "start": 5348.04, "end": 5354.24, "text": " No pre-trained anything. I fed it 18 months worth of archive articles and it learned not only", "tokens": [883, 659, 12, 17227, 2001, 1340, 13, 286, 4636, 309, 2443, 2493, 3163, 295, 23507, 11290, 293, 309, 3264, 406, 787], "temperature": 0.0, "avg_logprob": -0.26407264709472655, "compression_ratio": 1.576036866359447, "no_speech_prob": 2.4439823391730897e-06}, {"id": 1088, "seek": 535424, "start": 5354.24, "end": 5361.44, "text": " how to write English pretty well, but also after you say something's a convolutional neural network", "tokens": [577, 281, 2464, 3669, 1238, 731, 11, 457, 611, 934, 291, 584, 746, 311, 257, 45216, 304, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.2313324684320494, "compression_ratio": 1.6420233463035019, "no_speech_prob": 2.260313976876205e-06}, {"id": 1089, "seek": 535424, "start": 5361.44, "end": 5364.4, "text": " you should then use parentheses to say what it's called and", "tokens": [291, 820, 550, 764, 34153, 281, 584, 437, 309, 311, 1219, 293], "temperature": 0.0, "avg_logprob": -0.2313324684320494, "compression_ratio": 1.6420233463035019, "no_speech_prob": 2.260313976876205e-06}, {"id": 1090, "seek": 535424, "start": 5365.04, "end": 5370.92, "text": " Furthermore that the kinds of things people talk to say create algorithms for in computer vision are", "tokens": [23999, 300, 264, 3685, 295, 721, 561, 751, 281, 584, 1884, 14642, 337, 294, 3820, 5201, 366], "temperature": 0.0, "avg_logprob": -0.2313324684320494, "compression_ratio": 1.6420233463035019, "no_speech_prob": 2.260313976876205e-06}, {"id": 1091, "seek": 535424, "start": 5371.4, "end": 5374.24, "text": " performing image classification and in networking are", "tokens": [10205, 3256, 21538, 293, 294, 17985, 366], "temperature": 0.0, "avg_logprob": -0.2313324684320494, "compression_ratio": 1.6420233463035019, "no_speech_prob": 2.260313976876205e-06}, {"id": 1092, "seek": 535424, "start": 5376.5199999999995, "end": 5382.48, "text": " Achieving the same performance as traditional network-based routing algorithms. So like a language model is", "tokens": [15847, 30244, 264, 912, 3389, 382, 5164, 3209, 12, 6032, 32722, 14642, 13, 407, 411, 257, 2856, 2316, 307], "temperature": 0.0, "avg_logprob": -0.2313324684320494, "compression_ratio": 1.6420233463035019, "no_speech_prob": 2.260313976876205e-06}, {"id": 1093, "seek": 538248, "start": 5382.48, "end": 5387.44, "text": " Can be like incredibly deep and subtle", "tokens": [1664, 312, 411, 6252, 2452, 293, 13743], "temperature": 0.0, "avg_logprob": -0.1993846522951589, "compression_ratio": 1.7347826086956522, "no_speech_prob": 2.260318751723389e-06}, {"id": 1094, "seek": 538248, "start": 5387.919999999999, "end": 5390.48, "text": " Right and so we're going to try and build that", "tokens": [1779, 293, 370, 321, 434, 516, 281, 853, 293, 1322, 300], "temperature": 0.0, "avg_logprob": -0.1993846522951589, "compression_ratio": 1.7347826086956522, "no_speech_prob": 2.260318751723389e-06}, {"id": 1095, "seek": 538248, "start": 5391.08, "end": 5393.419999999999, "text": " But actually not because we care about this at all", "tokens": [583, 767, 406, 570, 321, 1127, 466, 341, 412, 439], "temperature": 0.0, "avg_logprob": -0.1993846522951589, "compression_ratio": 1.7347826086956522, "no_speech_prob": 2.260318751723389e-06}, {"id": 1096, "seek": 538248, "start": 5394.5599999999995, "end": 5398.339999999999, "text": " We're going to build it because we're going to try and create a pre-trained model", "tokens": [492, 434, 516, 281, 1322, 309, 570, 321, 434, 516, 281, 853, 293, 1884, 257, 659, 12, 17227, 2001, 2316], "temperature": 0.0, "avg_logprob": -0.1993846522951589, "compression_ratio": 1.7347826086956522, "no_speech_prob": 2.260318751723389e-06}, {"id": 1097, "seek": 538248, "start": 5398.339999999999, "end": 5402.959999999999, "text": " what we're actually going to try and do is take IMDB movie reviews and", "tokens": [437, 321, 434, 767, 516, 281, 853, 293, 360, 307, 747, 21463, 27735, 3169, 10229, 293], "temperature": 0.0, "avg_logprob": -0.1993846522951589, "compression_ratio": 1.7347826086956522, "no_speech_prob": 2.260318751723389e-06}, {"id": 1098, "seek": 538248, "start": 5403.44, "end": 5405.759999999999, "text": " Figure out whether they're positive or negative", "tokens": [43225, 484, 1968, 436, 434, 3353, 420, 3671], "temperature": 0.0, "avg_logprob": -0.1993846522951589, "compression_ratio": 1.7347826086956522, "no_speech_prob": 2.260318751723389e-06}, {"id": 1099, "seek": 538248, "start": 5406.839999999999, "end": 5409.679999999999, "text": " So if you think about it, this is a lot like cats versus dogs", "tokens": [407, 498, 291, 519, 466, 309, 11, 341, 307, 257, 688, 411, 11111, 5717, 7197], "temperature": 0.0, "avg_logprob": -0.1993846522951589, "compression_ratio": 1.7347826086956522, "no_speech_prob": 2.260318751723389e-06}, {"id": 1100, "seek": 540968, "start": 5409.68, "end": 5415.6, "text": " That's a classification algorithm, but rather than an image we're going to have the text of a review", "tokens": [663, 311, 257, 21538, 9284, 11, 457, 2831, 813, 364, 3256, 321, 434, 516, 281, 362, 264, 2487, 295, 257, 3131], "temperature": 0.0, "avg_logprob": -0.16697787347240983, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.3081737506581703e-06}, {"id": 1101, "seek": 540968, "start": 5417.4800000000005, "end": 5419.4800000000005, "text": " So I'd really like to use a pre-trained network", "tokens": [407, 286, 1116, 534, 411, 281, 764, 257, 659, 12, 17227, 2001, 3209], "temperature": 0.0, "avg_logprob": -0.16697787347240983, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.3081737506581703e-06}, {"id": 1102, "seek": 540968, "start": 5419.84, "end": 5425.4800000000005, "text": " Like I would at least like a net to start with a network that knows how to read English", "tokens": [1743, 286, 576, 412, 1935, 411, 257, 2533, 281, 722, 365, 257, 3209, 300, 3255, 577, 281, 1401, 3669], "temperature": 0.0, "avg_logprob": -0.16697787347240983, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.3081737506581703e-06}, {"id": 1103, "seek": 540968, "start": 5425.88, "end": 5427.88, "text": " right and so", "tokens": [558, 293, 370], "temperature": 0.0, "avg_logprob": -0.16697787347240983, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.3081737506581703e-06}, {"id": 1104, "seek": 540968, "start": 5428.360000000001, "end": 5434.72, "text": " My view was like okay to know how to read English means you should be able to like predict the next word of a sentence", "tokens": [1222, 1910, 390, 411, 1392, 281, 458, 577, 281, 1401, 3669, 1355, 291, 820, 312, 1075, 281, 411, 6069, 264, 958, 1349, 295, 257, 8174], "temperature": 0.0, "avg_logprob": -0.16697787347240983, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.3081737506581703e-06}, {"id": 1105, "seek": 540968, "start": 5435.240000000001, "end": 5438.72, "text": " so what if we pre-train a language model and", "tokens": [370, 437, 498, 321, 659, 12, 83, 7146, 257, 2856, 2316, 293], "temperature": 0.0, "avg_logprob": -0.16697787347240983, "compression_ratio": 1.6586345381526104, "no_speech_prob": 1.3081737506581703e-06}, {"id": 1106, "seek": 543872, "start": 5438.72, "end": 5443.54, "text": " Then use that pre-trained language model and then just like in computer vision", "tokens": [1396, 764, 300, 659, 12, 17227, 2001, 2856, 2316, 293, 550, 445, 411, 294, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.18773651123046875, "compression_ratio": 1.767605633802817, "no_speech_prob": 6.962203315197257e-06}, {"id": 1107, "seek": 543872, "start": 5443.96, "end": 5450.860000000001, "text": " Stick some new layers on the end and ask it instead of to predicting the next word in the sentence instead predict", "tokens": [22744, 512, 777, 7914, 322, 264, 917, 293, 1029, 309, 2602, 295, 281, 32884, 264, 958, 1349, 294, 264, 8174, 2602, 6069], "temperature": 0.0, "avg_logprob": -0.18773651123046875, "compression_ratio": 1.767605633802817, "no_speech_prob": 6.962203315197257e-06}, {"id": 1108, "seek": 543872, "start": 5450.96, "end": 5452.96, "text": " Whether something is positive or negative", "tokens": [8503, 746, 307, 3353, 420, 3671], "temperature": 0.0, "avg_logprob": -0.18773651123046875, "compression_ratio": 1.767605633802817, "no_speech_prob": 6.962203315197257e-06}, {"id": 1109, "seek": 543872, "start": 5453.64, "end": 5457.64, "text": " So when I started working on this this was actually a new idea", "tokens": [407, 562, 286, 1409, 1364, 322, 341, 341, 390, 767, 257, 777, 1558], "temperature": 0.0, "avg_logprob": -0.18773651123046875, "compression_ratio": 1.767605633802817, "no_speech_prob": 6.962203315197257e-06}, {"id": 1110, "seek": 543872, "start": 5459.12, "end": 5461.38, "text": " Unfortunately in the last couple of months I've been doing it", "tokens": [8590, 294, 264, 1036, 1916, 295, 2493, 286, 600, 668, 884, 309], "temperature": 0.0, "avg_logprob": -0.18773651123046875, "compression_ratio": 1.767605633802817, "no_speech_prob": 6.962203315197257e-06}, {"id": 1111, "seek": 546138, "start": 5461.38, "end": 5468.84, "text": " You know a few people have actually a couple people have started publishing this and so this has moved from being a totally new idea to being a you", "tokens": [509, 458, 257, 1326, 561, 362, 767, 257, 1916, 561, 362, 1409, 17832, 341, 293, 370, 341, 575, 4259, 490, 885, 257, 3879, 777, 1558, 281, 885, 257, 291], "temperature": 0.0, "avg_logprob": -0.12802555046829522, "compression_ratio": 1.7974683544303798, "no_speech_prob": 4.5659485294891056e-06}, {"id": 1112, "seek": 546138, "start": 5468.84, "end": 5470.84, "text": " know somewhat new idea", "tokens": [458, 8344, 777, 1558], "temperature": 0.0, "avg_logprob": -0.12802555046829522, "compression_ratio": 1.7974683544303798, "no_speech_prob": 4.5659485294891056e-06}, {"id": 1113, "seek": 546138, "start": 5471.08, "end": 5472.4400000000005, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.12802555046829522, "compression_ratio": 1.7974683544303798, "no_speech_prob": 4.5659485294891056e-06}, {"id": 1114, "seek": 546138, "start": 5472.4400000000005, "end": 5474.4400000000005, "text": " so this idea of", "tokens": [370, 341, 1558, 295], "temperature": 0.0, "avg_logprob": -0.12802555046829522, "compression_ratio": 1.7974683544303798, "no_speech_prob": 4.5659485294891056e-06}, {"id": 1115, "seek": 546138, "start": 5474.84, "end": 5476.84, "text": " creating a language model", "tokens": [4084, 257, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.12802555046829522, "compression_ratio": 1.7974683544303798, "no_speech_prob": 4.5659485294891056e-06}, {"id": 1116, "seek": 546138, "start": 5476.88, "end": 5482.400000000001, "text": " Making that the pre-trained model for a classification model is what we're going to learn to do now", "tokens": [14595, 300, 264, 659, 12, 17227, 2001, 2316, 337, 257, 21538, 2316, 307, 437, 321, 434, 516, 281, 1466, 281, 360, 586], "temperature": 0.0, "avg_logprob": -0.12802555046829522, "compression_ratio": 1.7974683544303798, "no_speech_prob": 4.5659485294891056e-06}, {"id": 1117, "seek": 546138, "start": 5482.400000000001, "end": 5488.4800000000005, "text": " And so the idea is we're really kind of trying to leverage exactly what we learned in our computer vision work", "tokens": [400, 370, 264, 1558, 307, 321, 434, 534, 733, 295, 1382, 281, 13982, 2293, 437, 321, 3264, 294, 527, 3820, 5201, 589], "temperature": 0.0, "avg_logprob": -0.12802555046829522, "compression_ratio": 1.7974683544303798, "no_speech_prob": 4.5659485294891056e-06}, {"id": 1118, "seek": 548848, "start": 5488.48, "end": 5493.839999999999, "text": " Which is how do we do fine-tuning to create powerful classification models? Yes, you know", "tokens": [3013, 307, 577, 360, 321, 360, 2489, 12, 83, 37726, 281, 1884, 4005, 21538, 5245, 30, 1079, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.21541875951430378, "compression_ratio": 1.5242718446601942, "no_speech_prob": 3.4465531371097313e-06}, {"id": 1119, "seek": 548848, "start": 5495.32, "end": 5500.879999999999, "text": " So why don't you think that doing just directly what you want to do?", "tokens": [407, 983, 500, 380, 291, 519, 300, 884, 445, 3838, 437, 291, 528, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.21541875951430378, "compression_ratio": 1.5242718446601942, "no_speech_prob": 3.4465531371097313e-06}, {"id": 1120, "seek": 548848, "start": 5501.959999999999, "end": 5503.679999999999, "text": " Doesn't work better", "tokens": [12955, 380, 589, 1101], "temperature": 0.0, "avg_logprob": -0.21541875951430378, "compression_ratio": 1.5242718446601942, "no_speech_prob": 3.4465531371097313e-06}, {"id": 1121, "seek": 548848, "start": 5503.679999999999, "end": 5508.32, "text": " Well a because it doesn't just turns out it doesn't empirically", "tokens": [1042, 257, 570, 309, 1177, 380, 445, 4523, 484, 309, 1177, 380, 25790, 984], "temperature": 0.0, "avg_logprob": -0.21541875951430378, "compression_ratio": 1.5242718446601942, "no_speech_prob": 3.4465531371097313e-06}, {"id": 1122, "seek": 548848, "start": 5508.959999999999, "end": 5512.48, "text": " And the reason it doesn't is a number of things", "tokens": [400, 264, 1778, 309, 1177, 380, 307, 257, 1230, 295, 721], "temperature": 0.0, "avg_logprob": -0.21541875951430378, "compression_ratio": 1.5242718446601942, "no_speech_prob": 3.4465531371097313e-06}, {"id": 1123, "seek": 548848, "start": 5513.719999999999, "end": 5516.2, "text": " first of all as we know", "tokens": [700, 295, 439, 382, 321, 458], "temperature": 0.0, "avg_logprob": -0.21541875951430378, "compression_ratio": 1.5242718446601942, "no_speech_prob": 3.4465531371097313e-06}, {"id": 1124, "seek": 551620, "start": 5516.2, "end": 5519.72, "text": " Fine-tuning a pre-trained network is really powerful", "tokens": [12024, 12, 83, 37726, 257, 659, 12, 17227, 2001, 3209, 307, 534, 4005], "temperature": 0.0, "avg_logprob": -0.22125727480108087, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.0511458867767942e-06}, {"id": 1125, "seek": 551620, "start": 5520.0, "end": 5526.96, "text": " Right so if we can get it to learn some related tasks first then we can use all that information", "tokens": [1779, 370, 498, 321, 393, 483, 309, 281, 1466, 512, 4077, 9608, 700, 550, 321, 393, 764, 439, 300, 1589], "temperature": 0.0, "avg_logprob": -0.22125727480108087, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.0511458867767942e-06}, {"id": 1126, "seek": 551620, "start": 5528.16, "end": 5530.16, "text": " To try and help it on the second pass", "tokens": [1407, 853, 293, 854, 309, 322, 264, 1150, 1320], "temperature": 0.0, "avg_logprob": -0.22125727480108087, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.0511458867767942e-06}, {"id": 1127, "seek": 551620, "start": 5532.36, "end": 5534.36, "text": " The other reason is", "tokens": [440, 661, 1778, 307], "temperature": 0.0, "avg_logprob": -0.22125727480108087, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.0511458867767942e-06}, {"id": 1128, "seek": 551620, "start": 5535.36, "end": 5537.12, "text": " IMDB movie reviews", "tokens": [21463, 27735, 3169, 10229], "temperature": 0.0, "avg_logprob": -0.22125727480108087, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.0511458867767942e-06}, {"id": 1129, "seek": 551620, "start": 5537.12, "end": 5539.3, "text": " You know up to a thousand words long", "tokens": [509, 458, 493, 281, 257, 4714, 2283, 938], "temperature": 0.0, "avg_logprob": -0.22125727480108087, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.0511458867767942e-06}, {"id": 1130, "seek": 551620, "start": 5539.84, "end": 5544.28, "text": " They're pretty big and so after reading a thousand words knowing nothing about", "tokens": [814, 434, 1238, 955, 293, 370, 934, 3760, 257, 4714, 2283, 5276, 1825, 466], "temperature": 0.0, "avg_logprob": -0.22125727480108087, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.0511458867767942e-06}, {"id": 1131, "seek": 554428, "start": 5544.28, "end": 5548.32, "text": " How English is structured or even what the concept of a word is?", "tokens": [1012, 3669, 307, 18519, 420, 754, 437, 264, 3410, 295, 257, 1349, 307, 30], "temperature": 0.0, "avg_logprob": -0.23034607185112252, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.5779560271766968e-06}, {"id": 1132, "seek": 554428, "start": 5549.12, "end": 5552.96, "text": " Or punctuation or whatever at the end of this thousand?", "tokens": [1610, 27006, 16073, 420, 2035, 412, 264, 917, 295, 341, 4714, 30], "temperature": 0.0, "avg_logprob": -0.23034607185112252, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.5779560271766968e-06}, {"id": 1133, "seek": 554428, "start": 5553.759999999999, "end": 5557.08, "text": " Integers, you know they end up being integers all you get is a one or a zero", "tokens": [5681, 1146, 433, 11, 291, 458, 436, 917, 493, 885, 41674, 439, 291, 483, 307, 257, 472, 420, 257, 4018], "temperature": 0.0, "avg_logprob": -0.23034607185112252, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.5779560271766968e-06}, {"id": 1134, "seek": 554428, "start": 5558.32, "end": 5566.24, "text": " Positive or negative and so trying to like learn the entire structure of English and then how it expresses positive and negative sentiments from a single number", "tokens": [46326, 420, 3671, 293, 370, 1382, 281, 411, 1466, 264, 2302, 3877, 295, 3669, 293, 550, 577, 309, 39204, 3353, 293, 3671, 41146, 490, 257, 2167, 1230], "temperature": 0.0, "avg_logprob": -0.23034607185112252, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.5779560271766968e-06}, {"id": 1135, "seek": 554428, "start": 5566.84, "end": 5568.84, "text": " Is just too much to expect", "tokens": [1119, 445, 886, 709, 281, 2066], "temperature": 0.0, "avg_logprob": -0.23034607185112252, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.5779560271766968e-06}, {"id": 1136, "seek": 556884, "start": 5568.84, "end": 5574.88, "text": " So by building a language model first we can try to build a neural network that kind of understands", "tokens": [407, 538, 2390, 257, 2856, 2316, 700, 321, 393, 853, 281, 1322, 257, 18161, 3209, 300, 733, 295, 15146], "temperature": 0.0, "avg_logprob": -0.26186968031383695, "compression_ratio": 1.5720524017467248, "no_speech_prob": 1.184246411867207e-05}, {"id": 1137, "seek": 556884, "start": 5575.32, "end": 5580.88, "text": " The English of movie reviews, and then we hope that some of the things that's learned about", "tokens": [440, 3669, 295, 3169, 10229, 11, 293, 550, 321, 1454, 300, 512, 295, 264, 721, 300, 311, 3264, 466], "temperature": 0.0, "avg_logprob": -0.26186968031383695, "compression_ratio": 1.5720524017467248, "no_speech_prob": 1.184246411867207e-05}, {"id": 1138, "seek": 556884, "start": 5581.6, "end": 5587.08, "text": " Are going to be useful in deciding whether something's a positive or a negative movie review. That's a great question", "tokens": [2014, 516, 281, 312, 4420, 294, 17990, 1968, 746, 311, 257, 3353, 420, 257, 3671, 3169, 3131, 13, 663, 311, 257, 869, 1168], "temperature": 0.0, "avg_logprob": -0.26186968031383695, "compression_ratio": 1.5720524017467248, "no_speech_prob": 1.184246411867207e-05}, {"id": 1139, "seek": 556884, "start": 5590.96, "end": 5595.84, "text": " Thanks is this similar to the car RNN by our pathy", "tokens": [2561, 307, 341, 2531, 281, 264, 1032, 45702, 45, 538, 527, 3100, 88], "temperature": 0.0, "avg_logprob": -0.26186968031383695, "compression_ratio": 1.5720524017467248, "no_speech_prob": 1.184246411867207e-05}, {"id": 1140, "seek": 559584, "start": 5595.84, "end": 5602.8, "text": " Yeah, this is somewhat similar to car RNN by car pathy so the famous car as in chr RNN", "tokens": [865, 11, 341, 307, 8344, 2531, 281, 1032, 45702, 45, 538, 1032, 3100, 88, 370, 264, 4618, 1032, 382, 294, 417, 81, 45702, 45], "temperature": 0.0, "avg_logprob": -0.24364278627478558, "compression_ratio": 1.5991189427312775, "no_speech_prob": 1.1478396118036471e-05}, {"id": 1141, "seek": 559584, "start": 5605.2, "end": 5609.12, "text": " Try to predict the next letter given a number of previous letters", "tokens": [6526, 281, 6069, 264, 958, 5063, 2212, 257, 1230, 295, 3894, 7825], "temperature": 0.0, "avg_logprob": -0.24364278627478558, "compression_ratio": 1.5991189427312775, "no_speech_prob": 1.1478396118036471e-05}, {"id": 1142, "seek": 559584, "start": 5609.72, "end": 5613.4800000000005, "text": " Language models generally work at a word level they don't have to", "tokens": [24445, 5245, 5101, 589, 412, 257, 1349, 1496, 436, 500, 380, 362, 281], "temperature": 0.0, "avg_logprob": -0.24364278627478558, "compression_ratio": 1.5991189427312775, "no_speech_prob": 1.1478396118036471e-05}, {"id": 1143, "seek": 559584, "start": 5614.2, "end": 5617.16, "text": " And doing things at a word level turns out to be", "tokens": [400, 884, 721, 412, 257, 1349, 1496, 4523, 484, 281, 312], "temperature": 0.0, "avg_logprob": -0.24364278627478558, "compression_ratio": 1.5991189427312775, "no_speech_prob": 1.1478396118036471e-05}, {"id": 1144, "seek": 559584, "start": 5617.96, "end": 5622.9800000000005, "text": " Can be quite a bit more powerful and we're going to focus on word level modeling in this course", "tokens": [1664, 312, 1596, 257, 857, 544, 4005, 293, 321, 434, 516, 281, 1879, 322, 1349, 1496, 15983, 294, 341, 1164], "temperature": 0.0, "avg_logprob": -0.24364278627478558, "compression_ratio": 1.5991189427312775, "no_speech_prob": 1.1478396118036471e-05}, {"id": 1145, "seek": 562298, "start": 5622.98, "end": 5627.379999999999, "text": " And then to what extent are these generated words?", "tokens": [400, 550, 281, 437, 8396, 366, 613, 10833, 2283, 30], "temperature": 0.0, "avg_logprob": -0.1729504613593073, "compression_ratio": 1.7137254901960783, "no_speech_prob": 8.39774111227598e-06}, {"id": 1146, "seek": 562298, "start": 5629.58, "end": 5634.0599999999995, "text": " Actual copies of what it found in the in the training data set or are these completely", "tokens": [3251, 901, 14341, 295, 437, 309, 1352, 294, 264, 294, 264, 3097, 1412, 992, 420, 366, 613, 2584], "temperature": 0.0, "avg_logprob": -0.1729504613593073, "compression_ratio": 1.7137254901960783, "no_speech_prob": 8.39774111227598e-06}, {"id": 1147, "seek": 562298, "start": 5635.419999999999, "end": 5640.219999999999, "text": " Random things that it actually learned and how do we know how to distinguish between those two?", "tokens": [37603, 721, 300, 309, 767, 3264, 293, 577, 360, 321, 458, 577, 281, 20206, 1296, 729, 732, 30], "temperature": 0.0, "avg_logprob": -0.1729504613593073, "compression_ratio": 1.7137254901960783, "no_speech_prob": 8.39774111227598e-06}, {"id": 1148, "seek": 562298, "start": 5640.58, "end": 5644.299999999999, "text": " Yeah, I mean these are all good questions up the words are definitely words", "tokens": [865, 11, 286, 914, 613, 366, 439, 665, 1651, 493, 264, 2283, 366, 2138, 2283], "temperature": 0.0, "avg_logprob": -0.1729504613593073, "compression_ratio": 1.7137254901960783, "no_speech_prob": 8.39774111227598e-06}, {"id": 1149, "seek": 562298, "start": 5644.299999999999, "end": 5650.0599999999995, "text": " We've seen before the work because it's not at a character level so it can only give us the word it's seen before the sentences", "tokens": [492, 600, 1612, 949, 264, 589, 570, 309, 311, 406, 412, 257, 2517, 1496, 370, 309, 393, 787, 976, 505, 264, 1349, 309, 311, 1612, 949, 264, 16579], "temperature": 0.0, "avg_logprob": -0.1729504613593073, "compression_ratio": 1.7137254901960783, "no_speech_prob": 8.39774111227598e-06}, {"id": 1150, "seek": 565006, "start": 5650.06, "end": 5651.900000000001, "text": " there's", "tokens": [456, 311], "temperature": 0.0, "avg_logprob": -0.18140922974203236, "compression_ratio": 1.6679245283018869, "no_speech_prob": 4.157305284024915e-06}, {"id": 1151, "seek": 565006, "start": 5651.900000000001, "end": 5654.38, "text": " A number of kind of rigorous ways of doing it", "tokens": [316, 1230, 295, 733, 295, 29882, 2098, 295, 884, 309], "temperature": 0.0, "avg_logprob": -0.18140922974203236, "compression_ratio": 1.6679245283018869, "no_speech_prob": 4.157305284024915e-06}, {"id": 1152, "seek": 565006, "start": 5654.38, "end": 5659.780000000001, "text": " But I think the easiest is to get a sense of like well here are two like different categories", "tokens": [583, 286, 519, 264, 12889, 307, 281, 483, 257, 2020, 295, 411, 731, 510, 366, 732, 411, 819, 10479], "temperature": 0.0, "avg_logprob": -0.18140922974203236, "compression_ratio": 1.6679245283018869, "no_speech_prob": 4.157305284024915e-06}, {"id": 1153, "seek": 565006, "start": 5660.26, "end": 5667.620000000001, "text": " Where it's kind of created very similar concepts, but mixing them up in just the right way like it would be very hard", "tokens": [2305, 309, 311, 733, 295, 2942, 588, 2531, 10392, 11, 457, 11983, 552, 493, 294, 445, 264, 558, 636, 411, 309, 576, 312, 588, 1152], "temperature": 0.0, "avg_logprob": -0.18140922974203236, "compression_ratio": 1.6679245283018869, "no_speech_prob": 4.157305284024915e-06}, {"id": 1154, "seek": 565006, "start": 5668.26, "end": 5673.5, "text": " To to do what we've seen here just by like spitting back things that seen before", "tokens": [1407, 281, 360, 437, 321, 600, 1612, 510, 445, 538, 411, 637, 2414, 646, 721, 300, 1612, 949], "temperature": 0.0, "avg_logprob": -0.18140922974203236, "compression_ratio": 1.6679245283018869, "no_speech_prob": 4.157305284024915e-06}, {"id": 1155, "seek": 565006, "start": 5674.22, "end": 5678.9400000000005, "text": " But you could of course actually go back and check you know have you seen that sentence before?", "tokens": [583, 291, 727, 295, 1164, 767, 352, 646, 293, 1520, 291, 458, 362, 291, 1612, 300, 8174, 949, 30], "temperature": 0.0, "avg_logprob": -0.18140922974203236, "compression_ratio": 1.6679245283018869, "no_speech_prob": 4.157305284024915e-06}, {"id": 1156, "seek": 567894, "start": 5678.94, "end": 5683.74, "text": " Or like a string distance have you seen a similar sentence before in this case?", "tokens": [1610, 411, 257, 6798, 4560, 362, 291, 1612, 257, 2531, 8174, 949, 294, 341, 1389, 30], "temperature": 0.0, "avg_logprob": -0.1756922403971354, "compression_ratio": 1.7350746268656716, "no_speech_prob": 1.9223010895075276e-05}, {"id": 1157, "seek": 567894, "start": 5685.099999999999, "end": 5691.099999999999, "text": " Oh and of course another way to do it is the length most importantly when we train the language model as we'll see", "tokens": [876, 293, 295, 1164, 1071, 636, 281, 360, 309, 307, 264, 4641, 881, 8906, 562, 321, 3847, 264, 2856, 2316, 382, 321, 603, 536], "temperature": 0.0, "avg_logprob": -0.1756922403971354, "compression_ratio": 1.7350746268656716, "no_speech_prob": 1.9223010895075276e-05}, {"id": 1158, "seek": 567894, "start": 5691.179999999999, "end": 5694.54, "text": " We'll have a validation set and so we're trying to predict the next word", "tokens": [492, 603, 362, 257, 24071, 992, 293, 370, 321, 434, 1382, 281, 6069, 264, 958, 1349], "temperature": 0.0, "avg_logprob": -0.1756922403971354, "compression_ratio": 1.7350746268656716, "no_speech_prob": 1.9223010895075276e-05}, {"id": 1159, "seek": 567894, "start": 5695.179999999999, "end": 5700.98, "text": " Of something that's never seen before and so if it's good at doing that it should be good at generating text", "tokens": [2720, 746, 300, 311, 1128, 1612, 949, 293, 370, 498, 309, 311, 665, 412, 884, 300, 309, 820, 312, 665, 412, 17746, 2487], "temperature": 0.0, "avg_logprob": -0.1756922403971354, "compression_ratio": 1.7350746268656716, "no_speech_prob": 1.9223010895075276e-05}, {"id": 1160, "seek": 567894, "start": 5701.339999999999, "end": 5703.339999999999, "text": " In this case the purpose", "tokens": [682, 341, 1389, 264, 4334], "temperature": 0.0, "avg_logprob": -0.1756922403971354, "compression_ratio": 1.7350746268656716, "no_speech_prob": 1.9223010895075276e-05}, {"id": 1161, "seek": 567894, "start": 5703.419999999999, "end": 5706.219999999999, "text": " The purpose is not to generate text that was just a fun example", "tokens": [440, 4334, 307, 406, 281, 8460, 2487, 300, 390, 445, 257, 1019, 1365], "temperature": 0.0, "avg_logprob": -0.1756922403971354, "compression_ratio": 1.7350746268656716, "no_speech_prob": 1.9223010895075276e-05}, {"id": 1162, "seek": 570622, "start": 5706.22, "end": 5709.38, "text": " And so I'm not really going to study that too much", "tokens": [400, 370, 286, 478, 406, 534, 516, 281, 2979, 300, 886, 709], "temperature": 0.0, "avg_logprob": -0.1973500044449516, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1163, "seek": 570622, "start": 5709.62, "end": 5714.66, "text": " But you know you during the week totally can like you can totally build", "tokens": [583, 291, 458, 291, 1830, 264, 1243, 3879, 393, 411, 291, 393, 3879, 1322], "temperature": 0.0, "avg_logprob": -0.1973500044449516, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1164, "seek": 570622, "start": 5715.46, "end": 5718.9400000000005, "text": " Your you know great American novel generator or whatever", "tokens": [2260, 291, 458, 869, 2665, 7613, 19265, 420, 2035], "temperature": 0.0, "avg_logprob": -0.1973500044449516, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1165, "seek": 570622, "start": 5719.780000000001, "end": 5721.780000000001, "text": " there are actually some tricks to", "tokens": [456, 366, 767, 512, 11733, 281], "temperature": 0.0, "avg_logprob": -0.1973500044449516, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1166, "seek": 570622, "start": 5722.42, "end": 5727.96, "text": " To using language models to generate text that I'm not using here. They're pretty simple", "tokens": [1407, 1228, 2856, 5245, 281, 8460, 2487, 300, 286, 478, 406, 1228, 510, 13, 814, 434, 1238, 2199], "temperature": 0.0, "avg_logprob": -0.1973500044449516, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1167, "seek": 570622, "start": 5727.96, "end": 5733.08, "text": " We can talk about them on the forum if you like, but my focus is actually on classification", "tokens": [492, 393, 751, 466, 552, 322, 264, 17542, 498, 291, 411, 11, 457, 452, 1879, 307, 767, 322, 21538], "temperature": 0.0, "avg_logprob": -0.1973500044449516, "compression_ratio": 1.6554621848739495, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1168, "seek": 573308, "start": 5733.08, "end": 5736.44, "text": " So I think that's the thing which is a", "tokens": [407, 286, 519, 300, 311, 264, 551, 597, 307, 257], "temperature": 0.0, "avg_logprob": -0.25388954346438486, "compression_ratio": 1.6063348416289593, "no_speech_prob": 9.818243597692344e-06}, {"id": 1169, "seek": 573308, "start": 5737.44, "end": 5739.0, "text": " incredibly powerful", "tokens": [6252, 4005], "temperature": 0.0, "avg_logprob": -0.25388954346438486, "compression_ratio": 1.6063348416289593, "no_speech_prob": 9.818243597692344e-06}, {"id": 1170, "seek": 573308, "start": 5739.0, "end": 5741.0, "text": " like text classification I", "tokens": [411, 2487, 21538, 286], "temperature": 0.0, "avg_logprob": -0.25388954346438486, "compression_ratio": 1.6063348416289593, "no_speech_prob": 9.818243597692344e-06}, {"id": 1171, "seek": 573308, "start": 5741.76, "end": 5749.66, "text": " Don't know you're a hedge fund you want to like read every article as soon as it comes out through Reuters or Twitter or whatever and", "tokens": [1468, 380, 458, 291, 434, 257, 25304, 2374, 291, 528, 281, 411, 1401, 633, 7222, 382, 2321, 382, 309, 1487, 484, 807, 1300, 48396, 420, 5794, 420, 2035, 293], "temperature": 0.0, "avg_logprob": -0.25388954346438486, "compression_ratio": 1.6063348416289593, "no_speech_prob": 9.818243597692344e-06}, {"id": 1172, "seek": 573308, "start": 5749.92, "end": 5755.72, "text": " Immediately identify things which in the past have caused you know massive market drops", "tokens": [34457, 5876, 721, 597, 294, 264, 1791, 362, 7008, 291, 458, 5994, 2142, 11438], "temperature": 0.0, "avg_logprob": -0.25388954346438486, "compression_ratio": 1.6063348416289593, "no_speech_prob": 9.818243597692344e-06}, {"id": 1173, "seek": 573308, "start": 5756.04, "end": 5759.6, "text": " That's the classification model or you want to?", "tokens": [663, 311, 264, 21538, 2316, 420, 291, 528, 281, 30], "temperature": 0.0, "avg_logprob": -0.25388954346438486, "compression_ratio": 1.6063348416289593, "no_speech_prob": 9.818243597692344e-06}, {"id": 1174, "seek": 575960, "start": 5759.6, "end": 5766.96, "text": " recognize all of the customer service queries which tend to be associated with people who", "tokens": [5521, 439, 295, 264, 5474, 2643, 24109, 597, 3928, 281, 312, 6615, 365, 561, 567], "temperature": 0.0, "avg_logprob": -0.2562891470419394, "compression_ratio": 1.462962962962963, "no_speech_prob": 8.939521649153903e-06}, {"id": 1175, "seek": 575960, "start": 5767.68, "end": 5769.88, "text": " Who leave your you know who?", "tokens": [2102, 1856, 428, 291, 458, 567, 30], "temperature": 0.0, "avg_logprob": -0.2562891470419394, "compression_ratio": 1.462962962962963, "no_speech_prob": 8.939521649153903e-06}, {"id": 1176, "seek": 575960, "start": 5770.400000000001, "end": 5777.76, "text": " Cancel their contracts in the next month. That's a classification problem, so like it's a really powerful kind of thing for", "tokens": [1664, 4933, 641, 13952, 294, 264, 958, 1618, 13, 663, 311, 257, 21538, 1154, 11, 370, 411, 309, 311, 257, 534, 4005, 733, 295, 551, 337], "temperature": 0.0, "avg_logprob": -0.2562891470419394, "compression_ratio": 1.462962962962963, "no_speech_prob": 8.939521649153903e-06}, {"id": 1177, "seek": 575960, "start": 5778.400000000001, "end": 5780.400000000001, "text": " data journalism", "tokens": [1412, 23191], "temperature": 0.0, "avg_logprob": -0.2562891470419394, "compression_ratio": 1.462962962962963, "no_speech_prob": 8.939521649153903e-06}, {"id": 1178, "seek": 575960, "start": 5780.56, "end": 5783.160000000001, "text": " Activision that activism law", "tokens": [28550, 1991, 300, 29040, 2101], "temperature": 0.0, "avg_logprob": -0.2562891470419394, "compression_ratio": 1.462962962962963, "no_speech_prob": 8.939521649153903e-06}, {"id": 1179, "seek": 575960, "start": 5784.240000000001, "end": 5786.620000000001, "text": " commerce so forth right like", "tokens": [26320, 370, 5220, 558, 411], "temperature": 0.0, "avg_logprob": -0.2562891470419394, "compression_ratio": 1.462962962962963, "no_speech_prob": 8.939521649153903e-06}, {"id": 1180, "seek": 578662, "start": 5786.62, "end": 5792.66, "text": " I'm trying to class documents into whether they're part of legal discovery or not part of legal discovery", "tokens": [286, 478, 1382, 281, 1508, 8512, 666, 1968, 436, 434, 644, 295, 5089, 12114, 420, 406, 644, 295, 5089, 12114], "temperature": 0.0, "avg_logprob": -0.2402636292692903, "compression_ratio": 1.7126436781609196, "no_speech_prob": 1.7330437458440429e-06}, {"id": 1181, "seek": 578662, "start": 5794.099999999999, "end": 5796.9, "text": " Okay, so you get the idea so", "tokens": [1033, 11, 370, 291, 483, 264, 1558, 370], "temperature": 0.0, "avg_logprob": -0.2402636292692903, "compression_ratio": 1.7126436781609196, "no_speech_prob": 1.7330437458440429e-06}, {"id": 1182, "seek": 578662, "start": 5798.26, "end": 5801.82, "text": " In terms of stuff we're importing we're importing a few new things here", "tokens": [682, 2115, 295, 1507, 321, 434, 43866, 321, 434, 43866, 257, 1326, 777, 721, 510], "temperature": 0.0, "avg_logprob": -0.2402636292692903, "compression_ratio": 1.7126436781609196, "no_speech_prob": 1.7330437458440429e-06}, {"id": 1183, "seek": 578662, "start": 5802.58, "end": 5805.18, "text": " one of this bunch of things we're importing is", "tokens": [472, 295, 341, 3840, 295, 721, 321, 434, 43866, 307], "temperature": 0.0, "avg_logprob": -0.2402636292692903, "compression_ratio": 1.7126436781609196, "no_speech_prob": 1.7330437458440429e-06}, {"id": 1184, "seek": 578662, "start": 5806.58, "end": 5811.7, "text": " Torch text torch text is pi torches like NLP", "tokens": [7160, 339, 2487, 27822, 2487, 307, 3895, 3930, 3781, 411, 426, 45196], "temperature": 0.0, "avg_logprob": -0.2402636292692903, "compression_ratio": 1.7126436781609196, "no_speech_prob": 1.7330437458440429e-06}, {"id": 1185, "seek": 581170, "start": 5811.7, "end": 5819.16, "text": " Library and so fast AI is designed to work hand-in-hand with porch text as you'll see and then there's a few", "tokens": [12806, 293, 370, 2370, 7318, 307, 4761, 281, 589, 1011, 12, 259, 12, 5543, 365, 35513, 2487, 382, 291, 603, 536, 293, 550, 456, 311, 257, 1326], "temperature": 0.0, "avg_logprob": -0.22570936730567445, "compression_ratio": 1.5614035087719298, "no_speech_prob": 4.49514800493489e-06}, {"id": 1186, "seek": 581170, "start": 5820.5, "end": 5824.0199999999995, "text": " Text specific sub bits of faster fast AI that we'll be using", "tokens": [18643, 2685, 1422, 9239, 295, 4663, 2370, 7318, 300, 321, 603, 312, 1228], "temperature": 0.0, "avg_logprob": -0.22570936730567445, "compression_ratio": 1.5614035087719298, "no_speech_prob": 4.49514800493489e-06}, {"id": 1187, "seek": 581170, "start": 5825.66, "end": 5833.12, "text": " So we're going to be working with the IMDB large movie review data set. It's very very well studied in academia", "tokens": [407, 321, 434, 516, 281, 312, 1364, 365, 264, 21463, 27735, 2416, 3169, 3131, 1412, 992, 13, 467, 311, 588, 588, 731, 9454, 294, 28937], "temperature": 0.0, "avg_logprob": -0.22570936730567445, "compression_ratio": 1.5614035087719298, "no_speech_prob": 4.49514800493489e-06}, {"id": 1188, "seek": 581170, "start": 5833.62, "end": 5835.34, "text": " you know", "tokens": [291, 458], "temperature": 0.0, "avg_logprob": -0.22570936730567445, "compression_ratio": 1.5614035087719298, "no_speech_prob": 4.49514800493489e-06}, {"id": 1189, "seek": 581170, "start": 5835.34, "end": 5837.66, "text": " lots and lots of people over the years have", "tokens": [3195, 293, 3195, 295, 561, 670, 264, 924, 362], "temperature": 0.0, "avg_logprob": -0.22570936730567445, "compression_ratio": 1.5614035087719298, "no_speech_prob": 4.49514800493489e-06}, {"id": 1190, "seek": 581170, "start": 5838.74, "end": 5840.74, "text": " studied this data set", "tokens": [9454, 341, 1412, 992], "temperature": 0.0, "avg_logprob": -0.22570936730567445, "compression_ratio": 1.5614035087719298, "no_speech_prob": 4.49514800493489e-06}, {"id": 1191, "seek": 584074, "start": 5840.74, "end": 5847.0199999999995, "text": " 50,000 reviews highly polarized reviews either positive or negative each one has been", "tokens": [2625, 11, 1360, 10229, 5405, 48623, 10229, 2139, 3353, 420, 3671, 1184, 472, 575, 668], "temperature": 0.0, "avg_logprob": -0.16376474168565539, "compression_ratio": 1.646341463414634, "no_speech_prob": 2.6577222911328136e-07}, {"id": 1192, "seek": 584074, "start": 5847.78, "end": 5853.58, "text": " Classified by sentiment okay, so we're going to try first of all however to create a language model", "tokens": [9471, 2587, 538, 16149, 1392, 11, 370, 321, 434, 516, 281, 853, 700, 295, 439, 4461, 281, 1884, 257, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.16376474168565539, "compression_ratio": 1.646341463414634, "no_speech_prob": 2.6577222911328136e-07}, {"id": 1193, "seek": 584074, "start": 5853.58, "end": 5856.179999999999, "text": " So we're going to ignore the sentiment entirely all right", "tokens": [407, 321, 434, 516, 281, 11200, 264, 16149, 7696, 439, 558], "temperature": 0.0, "avg_logprob": -0.16376474168565539, "compression_ratio": 1.646341463414634, "no_speech_prob": 2.6577222911328136e-07}, {"id": 1194, "seek": 584074, "start": 5856.179999999999, "end": 5861.34, "text": " So just like the dogs and cats pre train the model to do one thing and then fine-tune it to do something else", "tokens": [407, 445, 411, 264, 7197, 293, 11111, 659, 3847, 264, 2316, 281, 360, 472, 551, 293, 550, 2489, 12, 83, 2613, 309, 281, 360, 746, 1646], "temperature": 0.0, "avg_logprob": -0.16376474168565539, "compression_ratio": 1.646341463414634, "no_speech_prob": 2.6577222911328136e-07}, {"id": 1195, "seek": 584074, "start": 5862.62, "end": 5867.46, "text": " Because this kind of idea in NLP is is so so so new", "tokens": [1436, 341, 733, 295, 1558, 294, 426, 45196, 307, 307, 370, 370, 370, 777], "temperature": 0.0, "avg_logprob": -0.16376474168565539, "compression_ratio": 1.646341463414634, "no_speech_prob": 2.6577222911328136e-07}, {"id": 1196, "seek": 586746, "start": 5867.46, "end": 5872.94, "text": " There's basically no models you can download for this so we're going to have to create error", "tokens": [821, 311, 1936, 572, 5245, 291, 393, 5484, 337, 341, 370, 321, 434, 516, 281, 362, 281, 1884, 6713], "temperature": 0.0, "avg_logprob": -0.1775123051234654, "compression_ratio": 1.8101265822784811, "no_speech_prob": 4.495152552408399e-06}, {"id": 1197, "seek": 586746, "start": 5873.66, "end": 5875.66, "text": " right, so", "tokens": [558, 11, 370], "temperature": 0.0, "avg_logprob": -0.1775123051234654, "compression_ratio": 1.8101265822784811, "no_speech_prob": 4.495152552408399e-06}, {"id": 1198, "seek": 586746, "start": 5876.46, "end": 5883.26, "text": " Having downloaded the data you can use the link here. We do the usual stuff saying the path to it training and validation path", "tokens": [10222, 21748, 264, 1412, 291, 393, 764, 264, 2113, 510, 13, 492, 360, 264, 7713, 1507, 1566, 264, 3100, 281, 309, 3097, 293, 24071, 3100], "temperature": 0.0, "avg_logprob": -0.1775123051234654, "compression_ratio": 1.8101265822784811, "no_speech_prob": 4.495152552408399e-06}, {"id": 1199, "seek": 586746, "start": 5884.34, "end": 5891.72, "text": " And as you can see it looks pretty pretty traditional compared to vision. There's a directory of training. There's a directory of test", "tokens": [400, 382, 291, 393, 536, 309, 1542, 1238, 1238, 5164, 5347, 281, 5201, 13, 821, 311, 257, 21120, 295, 3097, 13, 821, 311, 257, 21120, 295, 1500], "temperature": 0.0, "avg_logprob": -0.1775123051234654, "compression_ratio": 1.8101265822784811, "no_speech_prob": 4.495152552408399e-06}, {"id": 1200, "seek": 586746, "start": 5892.1, "end": 5895.06, "text": " We don't actually have separate test and validation in this case", "tokens": [492, 500, 380, 767, 362, 4994, 1500, 293, 24071, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.1775123051234654, "compression_ratio": 1.8101265822784811, "no_speech_prob": 4.495152552408399e-06}, {"id": 1201, "seek": 589506, "start": 5895.06, "end": 5897.02, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.31375355190700954, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.527928119088756e-06}, {"id": 1202, "seek": 589506, "start": 5897.02, "end": 5902.46, "text": " Just like in in vision the training directory has a bunch of files in it", "tokens": [1449, 411, 294, 294, 5201, 264, 3097, 21120, 575, 257, 3840, 295, 7098, 294, 309], "temperature": 0.0, "avg_logprob": -0.31375355190700954, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.527928119088756e-06}, {"id": 1203, "seek": 589506, "start": 5902.9800000000005, "end": 5906.9400000000005, "text": " In this case not representing images, but representing movie reviews", "tokens": [682, 341, 1389, 406, 13460, 5267, 11, 457, 13460, 3169, 10229], "temperature": 0.0, "avg_logprob": -0.31375355190700954, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.527928119088756e-06}, {"id": 1204, "seek": 589506, "start": 5908.5, "end": 5914.860000000001, "text": " So we could cat one of those files and here we learn about the classic", "tokens": [407, 321, 727, 3857, 472, 295, 729, 7098, 293, 510, 321, 1466, 466, 264, 7230], "temperature": 0.0, "avg_logprob": -0.31375355190700954, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.527928119088756e-06}, {"id": 1205, "seek": 589506, "start": 5915.26, "end": 5920.06, "text": " Zombie Gettin movie I have to say with a name like zombie Gettin and", "tokens": [48952, 460, 3093, 259, 3169, 286, 362, 281, 584, 365, 257, 1315, 411, 20310, 460, 3093, 259, 293], "temperature": 0.0, "avg_logprob": -0.31375355190700954, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.527928119088756e-06}, {"id": 1206, "seek": 592006, "start": 5920.06, "end": 5925.06, "text": " An atom bomb on the front cover I was expecting a flat-out chop-socky fun-coup", "tokens": [1107, 12018, 7851, 322, 264, 1868, 2060, 286, 390, 9650, 257, 4962, 12, 346, 7931, 12, 539, 547, 88, 1019, 12, 66, 1250], "temperature": 0.0, "avg_logprob": -0.21813871039718877, "compression_ratio": 1.6126760563380282, "no_speech_prob": 1.670104029471986e-05}, {"id": 1207, "seek": 592006, "start": 5927.1, "end": 5928.860000000001, "text": " Rent it if you", "tokens": [42743, 309, 498, 291], "temperature": 0.0, "avg_logprob": -0.21813871039718877, "compression_ratio": 1.6126760563380282, "no_speech_prob": 1.670104029471986e-05}, {"id": 1208, "seek": 592006, "start": 5928.860000000001, "end": 5934.620000000001, "text": " Want to get stoned on a Friday night and laugh with your buddies don't rent it if you're an uptight weenie or want a zombie", "tokens": [11773, 281, 483, 342, 19009, 322, 257, 6984, 1818, 293, 5801, 365, 428, 30649, 500, 380, 6214, 309, 498, 291, 434, 364, 493, 41738, 321, 268, 414, 420, 528, 257, 20310], "temperature": 0.0, "avg_logprob": -0.21813871039718877, "compression_ratio": 1.6126760563380282, "no_speech_prob": 1.670104029471986e-05}, {"id": 1209, "seek": 592006, "start": 5934.620000000001, "end": 5940.38, "text": " Movie with lots of flesh eating. I think I'm going to enjoy zombie getting so all right, so we've learned something today", "tokens": [28766, 365, 3195, 295, 12497, 3936, 13, 286, 519, 286, 478, 516, 281, 2103, 20310, 1242, 370, 439, 558, 11, 370, 321, 600, 3264, 746, 965], "temperature": 0.0, "avg_logprob": -0.21813871039718877, "compression_ratio": 1.6126760563380282, "no_speech_prob": 1.670104029471986e-05}, {"id": 1210, "seek": 592006, "start": 5942.42, "end": 5948.860000000001, "text": " All right, so we can just use standard unique stuff to see like how many words are in the data set so the training set", "tokens": [1057, 558, 11, 370, 321, 393, 445, 764, 3832, 3845, 1507, 281, 536, 411, 577, 867, 2283, 366, 294, 264, 1412, 992, 370, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.21813871039718877, "compression_ratio": 1.6126760563380282, "no_speech_prob": 1.670104029471986e-05}, {"id": 1211, "seek": 594886, "start": 5948.86, "end": 5950.86, "text": " We've got", "tokens": [492, 600, 658], "temperature": 0.0, "avg_logprob": -0.22458638566913028, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.1125507626275066e-05}, {"id": 1212, "seek": 594886, "start": 5950.86, "end": 5952.86, "text": " 17 and a half million words", "tokens": [3282, 293, 257, 1922, 2459, 2283], "temperature": 0.0, "avg_logprob": -0.22458638566913028, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.1125507626275066e-05}, {"id": 1213, "seek": 594886, "start": 5953.42, "end": 5956.32, "text": " Test set we've got five point six million words", "tokens": [9279, 992, 321, 600, 658, 1732, 935, 2309, 2459, 2283], "temperature": 0.0, "avg_logprob": -0.22458638566913028, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.1125507626275066e-05}, {"id": 1214, "seek": 594886, "start": 5958.259999999999, "end": 5960.259999999999, "text": " So here's", "tokens": [407, 510, 311], "temperature": 0.0, "avg_logprob": -0.22458638566913028, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.1125507626275066e-05}, {"id": 1215, "seek": 594886, "start": 5962.339999999999, "end": 5969.94, "text": " These are this is IMDB so IMDB is yeah random people this is not a New York Times listed review as far as I know", "tokens": [1981, 366, 341, 307, 21463, 27735, 370, 21463, 27735, 307, 1338, 4974, 561, 341, 307, 406, 257, 1873, 3609, 11366, 10052, 3131, 382, 1400, 382, 286, 458], "temperature": 0.0, "avg_logprob": -0.22458638566913028, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.1125507626275066e-05}, {"id": 1216, "seek": 594886, "start": 5973.62, "end": 5975.62, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.22458638566913028, "compression_ratio": 1.4090909090909092, "no_speech_prob": 1.1125507626275066e-05}, {"id": 1217, "seek": 597562, "start": 5975.62, "end": 5981.62, "text": " Before we can do anything with text we have to turn it into a list of tokens", "tokens": [4546, 321, 393, 360, 1340, 365, 2487, 321, 362, 281, 1261, 309, 666, 257, 1329, 295, 22667], "temperature": 0.0, "avg_logprob": -0.1551818664257343, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267786142823752e-06}, {"id": 1218, "seek": 597562, "start": 5981.62, "end": 5987.22, "text": " A token is basically like a word right so we're going to try and turn this eventually into a list of numbers", "tokens": [316, 14862, 307, 1936, 411, 257, 1349, 558, 370, 321, 434, 516, 281, 853, 293, 1261, 341, 4728, 666, 257, 1329, 295, 3547], "temperature": 0.0, "avg_logprob": -0.1551818664257343, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267786142823752e-06}, {"id": 1219, "seek": 597562, "start": 5987.26, "end": 5989.58, "text": " So the first step is to turn it into a list of words", "tokens": [407, 264, 700, 1823, 307, 281, 1261, 309, 666, 257, 1329, 295, 2283], "temperature": 0.0, "avg_logprob": -0.1551818664257343, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267786142823752e-06}, {"id": 1220, "seek": 597562, "start": 5990.42, "end": 5996.16, "text": " That's called tokenization and NLP NLP has a huge lot of jargon that will will learn over time", "tokens": [663, 311, 1219, 14862, 2144, 293, 426, 45196, 426, 45196, 575, 257, 2603, 688, 295, 15181, 10660, 300, 486, 486, 1466, 670, 565], "temperature": 0.0, "avg_logprob": -0.1551818664257343, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267786142823752e-06}, {"id": 1221, "seek": 597562, "start": 5997.54, "end": 6002.74, "text": " One thing that's a bit tricky though when we're doing tokenization is here", "tokens": [1485, 551, 300, 311, 257, 857, 12414, 1673, 562, 321, 434, 884, 14862, 2144, 307, 510], "temperature": 0.0, "avg_logprob": -0.1551818664257343, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267786142823752e-06}, {"id": 1222, "seek": 600274, "start": 6002.74, "end": 6008.78, "text": " I've tokenized that review and then joined it back up with spaces and you'll see here that wasn't", "tokens": [286, 600, 14862, 1602, 300, 3131, 293, 550, 6869, 309, 646, 493, 365, 7673, 293, 291, 603, 536, 510, 300, 2067, 380], "temperature": 0.0, "avg_logprob": -0.20292068663097562, "compression_ratio": 1.696078431372549, "no_speech_prob": 1.9637989225884667e-06}, {"id": 1223, "seek": 600274, "start": 6009.219999999999, "end": 6016.36, "text": " Has become two tokens which makes perfect sense right was and is two things right?", "tokens": [8646, 1813, 732, 22667, 597, 1669, 2176, 2020, 558, 390, 293, 307, 732, 721, 558, 30], "temperature": 0.0, "avg_logprob": -0.20292068663097562, "compression_ratio": 1.696078431372549, "no_speech_prob": 1.9637989225884667e-06}, {"id": 1224, "seek": 600274, "start": 6017.54, "end": 6020.3, "text": " Dot dot dot has become one token", "tokens": [38753, 5893, 5893, 575, 1813, 472, 14862], "temperature": 0.0, "avg_logprob": -0.20292068663097562, "compression_ratio": 1.696078431372549, "no_speech_prob": 1.9637989225884667e-06}, {"id": 1225, "seek": 600274, "start": 6021.099999999999, "end": 6027.0, "text": " Right where else lots of exclamation marks has become lots of tokens so like a good tokenizer", "tokens": [1779, 689, 1646, 3195, 295, 1624, 43233, 10640, 575, 1813, 3195, 295, 22667, 370, 411, 257, 665, 14862, 6545], "temperature": 0.0, "avg_logprob": -0.20292068663097562, "compression_ratio": 1.696078431372549, "no_speech_prob": 1.9637989225884667e-06}, {"id": 1226, "seek": 600274, "start": 6027.46, "end": 6030.3, "text": " will do a good job of recognizing like", "tokens": [486, 360, 257, 665, 1691, 295, 18538, 411], "temperature": 0.0, "avg_logprob": -0.20292068663097562, "compression_ratio": 1.696078431372549, "no_speech_prob": 1.9637989225884667e-06}, {"id": 1227, "seek": 603030, "start": 6030.3, "end": 6036.22, "text": " pieces of an English sentence each separate piece of punctuation will be separated and", "tokens": [3755, 295, 364, 3669, 8174, 1184, 4994, 2522, 295, 27006, 16073, 486, 312, 12005, 293], "temperature": 0.0, "avg_logprob": -0.21820113585167325, "compression_ratio": 1.6352459016393444, "no_speech_prob": 4.356848421593895e-06}, {"id": 1228, "seek": 603030, "start": 6037.34, "end": 6042.54, "text": " Each part of a multi-part word will be separated as appropriate so", "tokens": [6947, 644, 295, 257, 4825, 12, 6971, 1349, 486, 312, 12005, 382, 6854, 370], "temperature": 0.0, "avg_logprob": -0.21820113585167325, "compression_ratio": 1.6352459016393444, "no_speech_prob": 4.356848421593895e-06}, {"id": 1229, "seek": 603030, "start": 6043.9400000000005, "end": 6049.28, "text": " Spacey is I think it's an Australian developed piece of software actually that does lots of NLP stuff", "tokens": [8705, 88, 307, 286, 519, 309, 311, 364, 13337, 4743, 2522, 295, 4722, 767, 300, 775, 3195, 295, 426, 45196, 1507], "temperature": 0.0, "avg_logprob": -0.21820113585167325, "compression_ratio": 1.6352459016393444, "no_speech_prob": 4.356848421593895e-06}, {"id": 1230, "seek": 603030, "start": 6049.34, "end": 6052.22, "text": " It's got the best tokenizer. I know and so", "tokens": [467, 311, 658, 264, 1151, 14862, 6545, 13, 286, 458, 293, 370], "temperature": 0.0, "avg_logprob": -0.21820113585167325, "compression_ratio": 1.6352459016393444, "no_speech_prob": 4.356848421593895e-06}, {"id": 1231, "seek": 603030, "start": 6052.74, "end": 6059.1, "text": " Fast AI is designed to work well with the spacey tokenizer as its torch text so here's an example of", "tokens": [15968, 7318, 307, 4761, 281, 589, 731, 365, 264, 1901, 88, 14862, 6545, 382, 1080, 27822, 2487, 370, 510, 311, 364, 1365, 295], "temperature": 0.0, "avg_logprob": -0.21820113585167325, "compression_ratio": 1.6352459016393444, "no_speech_prob": 4.356848421593895e-06}, {"id": 1232, "seek": 605910, "start": 6059.1, "end": 6060.9400000000005, "text": " tokenization", "tokens": [14862, 2144], "temperature": 0.0, "avg_logprob": -0.16292666529749963, "compression_ratio": 1.7411764705882353, "no_speech_prob": 5.173879344511079e-06}, {"id": 1233, "seek": 605910, "start": 6060.9400000000005, "end": 6068.14, "text": " Alright, so what we do with torch text is we basically have to start out by creating something called a field and", "tokens": [2798, 11, 370, 437, 321, 360, 365, 27822, 2487, 307, 321, 1936, 362, 281, 722, 484, 538, 4084, 746, 1219, 257, 2519, 293], "temperature": 0.0, "avg_logprob": -0.16292666529749963, "compression_ratio": 1.7411764705882353, "no_speech_prob": 5.173879344511079e-06}, {"id": 1234, "seek": 605910, "start": 6068.42, "end": 6075.18, "text": " A field is a definition of how to pre-process some text and so here's an example of the definition of a field", "tokens": [316, 2519, 307, 257, 7123, 295, 577, 281, 659, 12, 41075, 512, 2487, 293, 370, 510, 311, 364, 1365, 295, 264, 7123, 295, 257, 2519], "temperature": 0.0, "avg_logprob": -0.16292666529749963, "compression_ratio": 1.7411764705882353, "no_speech_prob": 5.173879344511079e-06}, {"id": 1235, "seek": 605910, "start": 6075.18, "end": 6083.18, "text": " It says I want to lowercase the text and I want to tokenize it with the function called spacey tokenize", "tokens": [467, 1619, 286, 528, 281, 3126, 9765, 264, 2487, 293, 286, 528, 281, 14862, 1125, 309, 365, 264, 2445, 1219, 1901, 88, 14862, 1125], "temperature": 0.0, "avg_logprob": -0.16292666529749963, "compression_ratio": 1.7411764705882353, "no_speech_prob": 5.173879344511079e-06}, {"id": 1236, "seek": 608318, "start": 6083.18, "end": 6090.1, "text": " Okay, so it hasn't done anything yet. We're just telling it when we do do something. This is what to do and so that we're going to", "tokens": [1033, 11, 370, 309, 6132, 380, 1096, 1340, 1939, 13, 492, 434, 445, 3585, 309, 562, 321, 360, 360, 746, 13, 639, 307, 437, 281, 360, 293, 370, 300, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.1509634032285303, "compression_ratio": 1.8434163701067616, "no_speech_prob": 3.1875481454335386e-06}, {"id": 1237, "seek": 608318, "start": 6090.1, "end": 6091.42, "text": " store that", "tokens": [3531, 300], "temperature": 0.0, "avg_logprob": -0.1509634032285303, "compression_ratio": 1.8434163701067616, "no_speech_prob": 3.1875481454335386e-06}, {"id": 1238, "seek": 608318, "start": 6091.42, "end": 6094.66, "text": " Description of what to do in a thing called capital text", "tokens": [3885, 12432, 295, 437, 281, 360, 294, 257, 551, 1219, 4238, 2487], "temperature": 0.0, "avg_logprob": -0.1509634032285303, "compression_ratio": 1.8434163701067616, "no_speech_prob": 3.1875481454335386e-06}, {"id": 1239, "seek": 608318, "start": 6095.66, "end": 6099.900000000001, "text": " And so this is this is none of this is but this is not fast AI specific at all", "tokens": [400, 370, 341, 307, 341, 307, 6022, 295, 341, 307, 457, 341, 307, 406, 2370, 7318, 2685, 412, 439], "temperature": 0.0, "avg_logprob": -0.1509634032285303, "compression_ratio": 1.8434163701067616, "no_speech_prob": 3.1875481454335386e-06}, {"id": 1240, "seek": 608318, "start": 6099.900000000001, "end": 6105.38, "text": " This is part of torch text you can go to the torch text website read the docs. There's not lots of docs yet", "tokens": [639, 307, 644, 295, 27822, 2487, 291, 393, 352, 281, 264, 27822, 2487, 3144, 1401, 264, 45623, 13, 821, 311, 406, 3195, 295, 45623, 1939], "temperature": 0.0, "avg_logprob": -0.1509634032285303, "compression_ratio": 1.8434163701067616, "no_speech_prob": 3.1875481454335386e-06}, {"id": 1241, "seek": 608318, "start": 6105.38, "end": 6107.68, "text": " This is all very very new so", "tokens": [639, 307, 439, 588, 588, 777, 370], "temperature": 0.0, "avg_logprob": -0.1509634032285303, "compression_ratio": 1.8434163701067616, "no_speech_prob": 3.1875481454335386e-06}, {"id": 1242, "seek": 610768, "start": 6107.68, "end": 6114.280000000001, "text": " Probably the best information you'll find about it is in this lesson, but there's some more information on this site", "tokens": [9210, 264, 1151, 1589, 291, 603, 915, 466, 309, 307, 294, 341, 6898, 11, 457, 456, 311, 512, 544, 1589, 322, 341, 3621], "temperature": 0.0, "avg_logprob": -0.1919650447611906, "compression_ratio": 1.7148936170212765, "no_speech_prob": 1.4593715604860336e-06}, {"id": 1243, "seek": 610768, "start": 6114.4800000000005, "end": 6119.72, "text": " All right, so what we can now do is go ahead and create the usual", "tokens": [1057, 558, 11, 370, 437, 321, 393, 586, 360, 307, 352, 2286, 293, 1884, 264, 7713], "temperature": 0.0, "avg_logprob": -0.1919650447611906, "compression_ratio": 1.7148936170212765, "no_speech_prob": 1.4593715604860336e-06}, {"id": 1244, "seek": 610768, "start": 6120.320000000001, "end": 6123.58, "text": " Fast AI model data object, okay", "tokens": [15968, 7318, 2316, 1412, 2657, 11, 1392], "temperature": 0.0, "avg_logprob": -0.1919650447611906, "compression_ratio": 1.7148936170212765, "no_speech_prob": 1.4593715604860336e-06}, {"id": 1245, "seek": 610768, "start": 6124.12, "end": 6127.84, "text": " And so to create the model data object we have to provide a few bits of information", "tokens": [400, 370, 281, 1884, 264, 2316, 1412, 2657, 321, 362, 281, 2893, 257, 1326, 9239, 295, 1589], "temperature": 0.0, "avg_logprob": -0.1919650447611906, "compression_ratio": 1.7148936170212765, "no_speech_prob": 1.4593715604860336e-06}, {"id": 1246, "seek": 610768, "start": 6128.280000000001, "end": 6135.54, "text": " We have to say what's the training set so the path to the text files the validation set and the test set", "tokens": [492, 362, 281, 584, 437, 311, 264, 3097, 992, 370, 264, 3100, 281, 264, 2487, 7098, 264, 24071, 992, 293, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.1919650447611906, "compression_ratio": 1.7148936170212765, "no_speech_prob": 1.4593715604860336e-06}, {"id": 1247, "seek": 613554, "start": 6135.54, "end": 6140.3, "text": " In this case just to keep things simple. I don't have a separate validation and test set", "tokens": [682, 341, 1389, 445, 281, 1066, 721, 2199, 13, 286, 500, 380, 362, 257, 4994, 24071, 293, 1500, 992], "temperature": 0.0, "avg_logprob": -0.13090947696140834, "compression_ratio": 1.7953667953667953, "no_speech_prob": 3.393132146811695e-06}, {"id": 1248, "seek": 613554, "start": 6140.3, "end": 6144.62, "text": " So I'm going to pass in the validation set for both of those two things, right?", "tokens": [407, 286, 478, 516, 281, 1320, 294, 264, 24071, 992, 337, 1293, 295, 729, 732, 721, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13090947696140834, "compression_ratio": 1.7953667953667953, "no_speech_prob": 3.393132146811695e-06}, {"id": 1249, "seek": 613554, "start": 6144.62, "end": 6149.62, "text": " So now we can create our model data object as per usual the first thing we give it as the path", "tokens": [407, 586, 321, 393, 1884, 527, 2316, 1412, 2657, 382, 680, 7713, 264, 700, 551, 321, 976, 309, 382, 264, 3100], "temperature": 0.0, "avg_logprob": -0.13090947696140834, "compression_ratio": 1.7953667953667953, "no_speech_prob": 3.393132146811695e-06}, {"id": 1250, "seek": 613554, "start": 6151.06, "end": 6156.98, "text": " The second thing we give it is the torch text field definition of how to pre-process that text", "tokens": [440, 1150, 551, 321, 976, 309, 307, 264, 27822, 2487, 2519, 7123, 295, 577, 281, 659, 12, 41075, 300, 2487], "temperature": 0.0, "avg_logprob": -0.13090947696140834, "compression_ratio": 1.7953667953667953, "no_speech_prob": 3.393132146811695e-06}, {"id": 1251, "seek": 615698, "start": 6156.98, "end": 6164.54, "text": " The third thing we give it is the dictionary or the list of all of the files we have train validation test", "tokens": [440, 2636, 551, 321, 976, 309, 307, 264, 25890, 420, 264, 1329, 295, 439, 295, 264, 7098, 321, 362, 3847, 24071, 1500], "temperature": 0.0, "avg_logprob": -0.1594867164438421, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.769396814983338e-06}, {"id": 1252, "seek": 615698, "start": 6165.66, "end": 6171.9, "text": " As per usual we can pass in a batch size and then we've got a special special couple of extra things here", "tokens": [1018, 680, 7713, 321, 393, 1320, 294, 257, 15245, 2744, 293, 550, 321, 600, 658, 257, 2121, 2121, 1916, 295, 2857, 721, 510], "temperature": 0.0, "avg_logprob": -0.1594867164438421, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.769396814983338e-06}, {"id": 1253, "seek": 615698, "start": 6172.7, "end": 6178.799999999999, "text": " One is a very commonly used in NLP minimum frequency what this says is", "tokens": [1485, 307, 257, 588, 12719, 1143, 294, 426, 45196, 7285, 7893, 437, 341, 1619, 307], "temperature": 0.0, "avg_logprob": -0.1594867164438421, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.769396814983338e-06}, {"id": 1254, "seek": 615698, "start": 6180.0199999999995, "end": 6184.82, "text": " In a moment, we're going to be replacing every one of these words with an integer", "tokens": [682, 257, 1623, 11, 321, 434, 516, 281, 312, 19139, 633, 472, 295, 613, 2283, 365, 364, 24922], "temperature": 0.0, "avg_logprob": -0.1594867164438421, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.769396814983338e-06}, {"id": 1255, "seek": 618482, "start": 6184.82, "end": 6191.719999999999, "text": " Which basically will be a unique index for every word and this basically says if there are any words that occur", "tokens": [3013, 1936, 486, 312, 257, 3845, 8186, 337, 633, 1349, 293, 341, 1936, 1619, 498, 456, 366, 604, 2283, 300, 5160], "temperature": 0.0, "avg_logprob": -0.23427176244050554, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.0616077815939207e-05}, {"id": 1256, "seek": 618482, "start": 6191.98, "end": 6193.98, "text": " less than ten times", "tokens": [1570, 813, 2064, 1413], "temperature": 0.0, "avg_logprob": -0.23427176244050554, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.0616077815939207e-05}, {"id": 1257, "seek": 618482, "start": 6194.38, "end": 6196.219999999999, "text": " Just call it unknown", "tokens": [1449, 818, 309, 9841], "temperature": 0.0, "avg_logprob": -0.23427176244050554, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.0616077815939207e-05}, {"id": 1258, "seek": 618482, "start": 6196.219999999999, "end": 6200.74, "text": " But don't think of it as a word, but we'll see that in more detail in a moment", "tokens": [583, 500, 380, 519, 295, 309, 382, 257, 1349, 11, 457, 321, 603, 536, 300, 294, 544, 2607, 294, 257, 1623], "temperature": 0.0, "avg_logprob": -0.23427176244050554, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.0616077815939207e-05}, {"id": 1259, "seek": 618482, "start": 6200.74, "end": 6203.5, "text": " And then we're going to see this in more detail as well", "tokens": [400, 550, 321, 434, 516, 281, 536, 341, 294, 544, 2607, 382, 731], "temperature": 0.0, "avg_logprob": -0.23427176244050554, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.0616077815939207e-05}, {"id": 1260, "seek": 618482, "start": 6204.0199999999995, "end": 6206.78, "text": " BPTT stands for back prop through time", "tokens": [363, 47, 28178, 7382, 337, 646, 2365, 807, 565], "temperature": 0.0, "avg_logprob": -0.23427176244050554, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.0616077815939207e-05}, {"id": 1261, "seek": 618482, "start": 6207.58, "end": 6210.799999999999, "text": " And this is where we define how long a sentence", "tokens": [400, 341, 307, 689, 321, 6964, 577, 938, 257, 8174], "temperature": 0.0, "avg_logprob": -0.23427176244050554, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.0616077815939207e-05}, {"id": 1262, "seek": 618482, "start": 6211.259999999999, "end": 6212.9, "text": " Where we?", "tokens": [2305, 321, 30], "temperature": 0.0, "avg_logprob": -0.23427176244050554, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.0616077815939207e-05}, {"id": 1263, "seek": 621290, "start": 6212.9, "end": 6218.86, "text": " Stick on the GPU at once so we're going to break them up and in this case. We're going to break them up into sentences of", "tokens": [22744, 322, 264, 18407, 412, 1564, 370, 321, 434, 516, 281, 1821, 552, 493, 293, 294, 341, 1389, 13, 492, 434, 516, 281, 1821, 552, 493, 666, 16579, 295], "temperature": 0.0, "avg_logprob": -0.17608308792114258, "compression_ratio": 1.6866952789699572, "no_speech_prob": 1.9033781200050726e-06}, {"id": 1264, "seek": 621290, "start": 6219.86, "end": 6224.92, "text": " 70 tokens or less on the whole so we're going to see all this in a moment", "tokens": [5285, 22667, 420, 1570, 322, 264, 1379, 370, 321, 434, 516, 281, 536, 439, 341, 294, 257, 1623], "temperature": 0.0, "avg_logprob": -0.17608308792114258, "compression_ratio": 1.6866952789699572, "no_speech_prob": 1.9033781200050726e-06}, {"id": 1265, "seek": 621290, "start": 6225.86, "end": 6234.16, "text": " Right so after building our model data object right what it actually does is it's going to fill this text field", "tokens": [1779, 370, 934, 2390, 527, 2316, 1412, 2657, 558, 437, 309, 767, 775, 307, 309, 311, 516, 281, 2836, 341, 2487, 2519], "temperature": 0.0, "avg_logprob": -0.17608308792114258, "compression_ratio": 1.6866952789699572, "no_speech_prob": 1.9033781200050726e-06}, {"id": 1266, "seek": 621290, "start": 6234.7, "end": 6241.04, "text": " With an additional attribute called vocab, and this is a really important NLP concept", "tokens": [2022, 364, 4497, 19667, 1219, 2329, 455, 11, 293, 341, 307, 257, 534, 1021, 426, 45196, 3410], "temperature": 0.0, "avg_logprob": -0.17608308792114258, "compression_ratio": 1.6866952789699572, "no_speech_prob": 1.9033781200050726e-06}, {"id": 1267, "seek": 624104, "start": 6241.04, "end": 6245.98, "text": " I'm sorry there's so many NLP concepts which throw at you kind of quickly, but we'll see them a few times", "tokens": [286, 478, 2597, 456, 311, 370, 867, 426, 45196, 10392, 597, 3507, 412, 291, 733, 295, 2661, 11, 457, 321, 603, 536, 552, 257, 1326, 1413], "temperature": 0.0, "avg_logprob": -0.2183918707149545, "compression_ratio": 1.5903083700440528, "no_speech_prob": 9.080398740479723e-06}, {"id": 1268, "seek": 624104, "start": 6246.42, "end": 6248.1, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.2183918707149545, "compression_ratio": 1.5903083700440528, "no_speech_prob": 9.080398740479723e-06}, {"id": 1269, "seek": 624104, "start": 6248.1, "end": 6254.1, "text": " Vocab is the vocabulary and the vocabulary in NLP has a very specific meaning it is", "tokens": [8993, 455, 307, 264, 19864, 293, 264, 19864, 294, 426, 45196, 575, 257, 588, 2685, 3620, 309, 307], "temperature": 0.0, "avg_logprob": -0.2183918707149545, "compression_ratio": 1.5903083700440528, "no_speech_prob": 9.080398740479723e-06}, {"id": 1270, "seek": 624104, "start": 6254.1, "end": 6257.14, "text": " What is the list of unique words that appear in this text?", "tokens": [708, 307, 264, 1329, 295, 3845, 2283, 300, 4204, 294, 341, 2487, 30], "temperature": 0.0, "avg_logprob": -0.2183918707149545, "compression_ratio": 1.5903083700440528, "no_speech_prob": 9.080398740479723e-06}, {"id": 1271, "seek": 624104, "start": 6258.06, "end": 6265.5, "text": " So every one of them is going to get a unique index, so let's take a look right here is text dot vocab dot", "tokens": [407, 633, 472, 295, 552, 307, 516, 281, 483, 257, 3845, 8186, 11, 370, 718, 311, 747, 257, 574, 558, 510, 307, 2487, 5893, 2329, 455, 5893], "temperature": 0.0, "avg_logprob": -0.2183918707149545, "compression_ratio": 1.5903083700440528, "no_speech_prob": 9.080398740479723e-06}, {"id": 1272, "seek": 626550, "start": 6265.5, "end": 6272.34, "text": " I2s this stands for this is all torched text not fast AI text dot vocab dot int to string", "tokens": [286, 17, 82, 341, 7382, 337, 341, 307, 439, 3930, 19318, 2487, 406, 2370, 7318, 2487, 5893, 2329, 455, 5893, 560, 281, 6798], "temperature": 0.0, "avg_logprob": -0.34885751499849205, "compression_ratio": 1.58125, "no_speech_prob": 7.766897397232242e-06}, {"id": 1273, "seek": 626550, "start": 6272.82, "end": 6279.5, "text": " Maps the integer zero to unknown the integer one the padding integer to the", "tokens": [28978, 264, 24922, 4018, 281, 9841, 264, 24922, 472, 264, 39562, 24922, 281, 264], "temperature": 0.0, "avg_logprob": -0.34885751499849205, "compression_ratio": 1.58125, "no_speech_prob": 7.766897397232242e-06}, {"id": 1274, "seek": 626550, "start": 6280.14, "end": 6285.54, "text": " Comma dot and a of two and so forth right so this is the first 12", "tokens": [3046, 64, 5893, 293, 257, 295, 732, 293, 370, 5220, 558, 370, 341, 307, 264, 700, 2272], "temperature": 0.0, "avg_logprob": -0.34885751499849205, "compression_ratio": 1.58125, "no_speech_prob": 7.766897397232242e-06}, {"id": 1275, "seek": 626550, "start": 6286.94, "end": 6288.94, "text": " elements of the array", "tokens": [4959, 295, 264, 10225], "temperature": 0.0, "avg_logprob": -0.34885751499849205, "compression_ratio": 1.58125, "no_speech_prob": 7.766897397232242e-06}, {"id": 1276, "seek": 628894, "start": 6288.94, "end": 6295.82, "text": " Of the vocab from the IMDB movie review, and it's been sorted by frequency", "tokens": [2720, 264, 2329, 455, 490, 264, 21463, 27735, 3169, 3131, 11, 293, 309, 311, 668, 25462, 538, 7893], "temperature": 0.0, "avg_logprob": -0.19505081595955315, "compression_ratio": 1.6394230769230769, "no_speech_prob": 3.0894802875991445e-06}, {"id": 1277, "seek": 628894, "start": 6296.9, "end": 6302.9, "text": " Except for the first two special ones so for example. We can then go backwards s to I string to int", "tokens": [16192, 337, 264, 700, 732, 2121, 2306, 370, 337, 1365, 13, 492, 393, 550, 352, 12204, 262, 281, 286, 6798, 281, 560], "temperature": 0.0, "avg_logprob": -0.19505081595955315, "compression_ratio": 1.6394230769230769, "no_speech_prob": 3.0894802875991445e-06}, {"id": 1278, "seek": 628894, "start": 6303.379999999999, "end": 6309.48, "text": " Here is the it's in position zero one two so string to it the is two", "tokens": [1692, 307, 264, 309, 311, 294, 2535, 4018, 472, 732, 370, 6798, 281, 309, 264, 307, 732], "temperature": 0.0, "avg_logprob": -0.19505081595955315, "compression_ratio": 1.6394230769230769, "no_speech_prob": 3.0894802875991445e-06}, {"id": 1279, "seek": 630948, "start": 6309.48, "end": 6318.28, "text": " So the vocab lets us take a word and map it to an integer or take an integer and map it to a word", "tokens": [407, 264, 2329, 455, 6653, 505, 747, 257, 1349, 293, 4471, 309, 281, 364, 24922, 420, 747, 364, 24922, 293, 4471, 309, 281, 257, 1349], "temperature": 0.0, "avg_logprob": -0.2077543799941604, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.684190576474066e-06}, {"id": 1280, "seek": 630948, "start": 6319.08, "end": 6322.08, "text": " Right and so that means that we can then take", "tokens": [1779, 293, 370, 300, 1355, 300, 321, 393, 550, 747], "temperature": 0.0, "avg_logprob": -0.2077543799941604, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.684190576474066e-06}, {"id": 1281, "seek": 630948, "start": 6322.879999999999, "end": 6328.36, "text": " the first 12 tokens for example of our text and turn them into", "tokens": [264, 700, 2272, 22667, 337, 1365, 295, 527, 2487, 293, 1261, 552, 666], "temperature": 0.0, "avg_logprob": -0.2077543799941604, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.684190576474066e-06}, {"id": 1282, "seek": 630948, "start": 6329.719999999999, "end": 6335.959999999999, "text": " 12 ints so for example here is of the you can see seven two and", "tokens": [2272, 560, 82, 370, 337, 1365, 510, 307, 295, 264, 291, 393, 536, 3407, 732, 293], "temperature": 0.0, "avg_logprob": -0.2077543799941604, "compression_ratio": 1.6981132075471699, "no_speech_prob": 2.684190576474066e-06}, {"id": 1283, "seek": 633596, "start": 6335.96, "end": 6342.44, "text": " Here you can see seven two right so we're going to be working in this", "tokens": [1692, 291, 393, 536, 3407, 732, 558, 370, 321, 434, 516, 281, 312, 1364, 294, 341], "temperature": 0.0, "avg_logprob": -0.23038832346598306, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.067699713530601e-06}, {"id": 1284, "seek": 633596, "start": 6343.28, "end": 6346.0, "text": " Form did you have a question? Yeah, could you pass that back there?", "tokens": [10126, 630, 291, 362, 257, 1168, 30, 865, 11, 727, 291, 1320, 300, 646, 456, 30], "temperature": 0.0, "avg_logprob": -0.23038832346598306, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.067699713530601e-06}, {"id": 1285, "seek": 633596, "start": 6347.96, "end": 6350.72, "text": " Is it a common to any stemming or limitizing?", "tokens": [1119, 309, 257, 2689, 281, 604, 12312, 2810, 420, 4948, 3319, 30], "temperature": 0.0, "avg_logprob": -0.23038832346598306, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.067699713530601e-06}, {"id": 1286, "seek": 633596, "start": 6351.92, "end": 6357.84, "text": " Not really no generally tokenization is is what we want like with a language model", "tokens": [1726, 534, 572, 5101, 14862, 2144, 307, 307, 437, 321, 528, 411, 365, 257, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.23038832346598306, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.067699713530601e-06}, {"id": 1287, "seek": 633596, "start": 6359.6, "end": 6364.72, "text": " We you know to keep it as general as possible we want to know what's coming next and so like whether it's", "tokens": [492, 291, 458, 281, 1066, 309, 382, 2674, 382, 1944, 321, 528, 281, 458, 437, 311, 1348, 958, 293, 370, 411, 1968, 309, 311], "temperature": 0.0, "avg_logprob": -0.23038832346598306, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.067699713530601e-06}, {"id": 1288, "seek": 636472, "start": 6364.72, "end": 6371.92, "text": " Future tense or past tense or plural or singular like we don't really know which things are going to be interesting in which aren't", "tokens": [20805, 18760, 420, 1791, 18760, 420, 25377, 420, 20010, 411, 321, 500, 380, 534, 458, 597, 721, 366, 516, 281, 312, 1880, 294, 597, 3212, 380], "temperature": 0.0, "avg_logprob": -0.16619998892557988, "compression_ratio": 1.6875, "no_speech_prob": 3.905462563125184e-06}, {"id": 1289, "seek": 636472, "start": 6372.96, "end": 6374.96, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.16619998892557988, "compression_ratio": 1.6875, "no_speech_prob": 3.905462563125184e-06}, {"id": 1290, "seek": 636472, "start": 6375.72, "end": 6382.56, "text": " It seems that it's generally best to kind of leave it alone as much as possible be the short answer", "tokens": [467, 2544, 300, 309, 311, 5101, 1151, 281, 733, 295, 1856, 309, 3312, 382, 709, 382, 1944, 312, 264, 2099, 1867], "temperature": 0.0, "avg_logprob": -0.16619998892557988, "compression_ratio": 1.6875, "no_speech_prob": 3.905462563125184e-06}, {"id": 1291, "seek": 636472, "start": 6384.2, "end": 6386.72, "text": " You know having said that as I say this is all pretty new", "tokens": [509, 458, 1419, 848, 300, 382, 286, 584, 341, 307, 439, 1238, 777], "temperature": 0.0, "avg_logprob": -0.16619998892557988, "compression_ratio": 1.6875, "no_speech_prob": 3.905462563125184e-06}, {"id": 1292, "seek": 638672, "start": 6386.72, "end": 6394.400000000001, "text": " So if there are some particular areas that some researcher maybe has already discovered that some other kinds of pre-processing a helpful you", "tokens": [407, 498, 456, 366, 512, 1729, 3179, 300, 512, 21751, 1310, 575, 1217, 6941, 300, 512, 661, 3685, 295, 659, 12, 41075, 278, 257, 4961, 291], "temperature": 0.0, "avg_logprob": -0.34839463233947754, "compression_ratio": 1.693798449612403, "no_speech_prob": 1.6963842426775955e-05}, {"id": 1293, "seek": 638672, "start": 6394.76, "end": 6397.26, "text": " Know I wouldn't be surprised not to know about it", "tokens": [10265, 286, 2759, 380, 312, 6100, 406, 281, 458, 466, 309], "temperature": 0.0, "avg_logprob": -0.34839463233947754, "compression_ratio": 1.693798449612403, "no_speech_prob": 1.6963842426775955e-05}, {"id": 1294, "seek": 638672, "start": 6398.76, "end": 6404.84, "text": " So when you're dealing with um you don't natural language is in context important context is very important", "tokens": [407, 562, 291, 434, 6260, 365, 1105, 291, 500, 380, 3303, 2856, 307, 294, 4319, 1021, 4319, 307, 588, 1021], "temperature": 0.0, "avg_logprob": -0.34839463233947754, "compression_ratio": 1.693798449612403, "no_speech_prob": 1.6963842426775955e-05}, {"id": 1295, "seek": 638672, "start": 6405.12, "end": 6411.8, "text": " So if you're breaking if you're using us the specie tokenizer literally just looking at individual words no no we're not looking at words", "tokens": [407, 498, 291, 434, 7697, 498, 291, 434, 1228, 505, 264, 1608, 414, 14862, 6545, 3736, 445, 1237, 412, 2609, 2283, 572, 572, 321, 434, 406, 1237, 412, 2283], "temperature": 0.0, "avg_logprob": -0.34839463233947754, "compression_ratio": 1.693798449612403, "no_speech_prob": 1.6963842426775955e-05}, {"id": 1296, "seek": 641180, "start": 6411.8, "end": 6417.76, "text": " This is this look. This is I just don't get some of the big premises of this like they're in order", "tokens": [639, 307, 341, 574, 13, 639, 307, 286, 445, 500, 380, 483, 512, 295, 264, 955, 34266, 295, 341, 411, 436, 434, 294, 1668], "temperature": 0.0, "avg_logprob": -0.16143878782638396, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.2377231491409475e-06}, {"id": 1297, "seek": 641180, "start": 6418.4400000000005, "end": 6421.0, "text": " Yeah, so just because we replaced I", "tokens": [865, 11, 370, 445, 570, 321, 10772, 286], "temperature": 0.0, "avg_logprob": -0.16143878782638396, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.2377231491409475e-06}, {"id": 1298, "seek": 641180, "start": 6421.56, "end": 6426.5, "text": " With the number 12 these are still in that order yeah", "tokens": [2022, 264, 1230, 2272, 613, 366, 920, 294, 300, 1668, 1338], "temperature": 0.0, "avg_logprob": -0.16143878782638396, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.2377231491409475e-06}, {"id": 1299, "seek": 641180, "start": 6427.400000000001, "end": 6433.24, "text": " There is a different way of dealing with natural language called a bag of words and bag of words you do throw away", "tokens": [821, 307, 257, 819, 636, 295, 6260, 365, 3303, 2856, 1219, 257, 3411, 295, 2283, 293, 3411, 295, 2283, 291, 360, 3507, 1314], "temperature": 0.0, "avg_logprob": -0.16143878782638396, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.2377231491409475e-06}, {"id": 1300, "seek": 641180, "start": 6433.320000000001, "end": 6438.96, "text": " The order in the context and in the machine learning course will be learning about working with bag of words representations", "tokens": [440, 1668, 294, 264, 4319, 293, 294, 264, 3479, 2539, 1164, 486, 312, 2539, 466, 1364, 365, 3411, 295, 2283, 33358], "temperature": 0.0, "avg_logprob": -0.16143878782638396, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.2377231491409475e-06}, {"id": 1301, "seek": 643896, "start": 6438.96, "end": 6441.56, "text": " But my belief is that they are", "tokens": [583, 452, 7107, 307, 300, 436, 366], "temperature": 0.0, "avg_logprob": -0.2235381914221722, "compression_ratio": 1.5956521739130434, "no_speech_prob": 3.905461198883131e-06}, {"id": 1302, "seek": 643896, "start": 6442.44, "end": 6446.6, "text": " No longer useful or in the verge of becoming no longer useful", "tokens": [883, 2854, 4420, 420, 294, 264, 37164, 295, 5617, 572, 2854, 4420], "temperature": 0.0, "avg_logprob": -0.2235381914221722, "compression_ratio": 1.5956521739130434, "no_speech_prob": 3.905461198883131e-06}, {"id": 1303, "seek": 643896, "start": 6446.6, "end": 6451.04, "text": " We're starting to learn how to use deep learning to use context properly now", "tokens": [492, 434, 2891, 281, 1466, 577, 281, 764, 2452, 2539, 281, 764, 4319, 6108, 586], "temperature": 0.0, "avg_logprob": -0.2235381914221722, "compression_ratio": 1.5956521739130434, "no_speech_prob": 3.905461198883131e-06}, {"id": 1304, "seek": 643896, "start": 6452.68, "end": 6457.16, "text": " But it's kind of for the first time it's really like only in the last few months", "tokens": [583, 309, 311, 733, 295, 337, 264, 700, 565, 309, 311, 534, 411, 787, 294, 264, 1036, 1326, 2493], "temperature": 0.0, "avg_logprob": -0.2235381914221722, "compression_ratio": 1.5956521739130434, "no_speech_prob": 3.905461198883131e-06}, {"id": 1305, "seek": 643896, "start": 6458.72, "end": 6462.8, "text": " Right so I mentioned that we've got two numbers batch size and", "tokens": [1779, 370, 286, 2835, 300, 321, 600, 658, 732, 3547, 15245, 2744, 293], "temperature": 0.0, "avg_logprob": -0.2235381914221722, "compression_ratio": 1.5956521739130434, "no_speech_prob": 3.905461198883131e-06}, {"id": 1306, "seek": 643896, "start": 6463.4800000000005, "end": 6467.76, "text": " BPTT back prop through time so this is kind of subtle", "tokens": [40533, 28178, 646, 2365, 807, 565, 370, 341, 307, 733, 295, 13743], "temperature": 0.0, "avg_logprob": -0.2235381914221722, "compression_ratio": 1.5956521739130434, "no_speech_prob": 3.905461198883131e-06}, {"id": 1307, "seek": 646776, "start": 6467.76, "end": 6469.76, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2197457927547089, "compression_ratio": 1.5747126436781609, "no_speech_prob": 2.6425734631629894e-06}, {"id": 1308, "seek": 646776, "start": 6473.12, "end": 6476.84, "text": " So we've got some big long piece of text", "tokens": [407, 321, 600, 658, 512, 955, 938, 2522, 295, 2487], "temperature": 0.0, "avg_logprob": -0.2197457927547089, "compression_ratio": 1.5747126436781609, "no_speech_prob": 2.6425734631629894e-06}, {"id": 1309, "seek": 646776, "start": 6478.92, "end": 6483.52, "text": " Okay, so we've got some big long piece of text you know here's our sentence. It's a bunch of words right and", "tokens": [1033, 11, 370, 321, 600, 658, 512, 955, 938, 2522, 295, 2487, 291, 458, 510, 311, 527, 8174, 13, 467, 311, 257, 3840, 295, 2283, 558, 293], "temperature": 0.0, "avg_logprob": -0.2197457927547089, "compression_ratio": 1.5747126436781609, "no_speech_prob": 2.6425734631629894e-06}, {"id": 1310, "seek": 646776, "start": 6486.360000000001, "end": 6490.4800000000005, "text": " Actually what happens in a language model is even though we have lots of movie reviews", "tokens": [5135, 437, 2314, 294, 257, 2856, 2316, 307, 754, 1673, 321, 362, 3195, 295, 3169, 10229], "temperature": 0.0, "avg_logprob": -0.2197457927547089, "compression_ratio": 1.5747126436781609, "no_speech_prob": 2.6425734631629894e-06}, {"id": 1311, "seek": 646776, "start": 6490.88, "end": 6492.88, "text": " They actually all get concatenated", "tokens": [814, 767, 439, 483, 1588, 7186, 770], "temperature": 0.0, "avg_logprob": -0.2197457927547089, "compression_ratio": 1.5747126436781609, "no_speech_prob": 2.6425734631629894e-06}, {"id": 1312, "seek": 649288, "start": 6492.88, "end": 6498.6, "text": " Together into one big block of text right so it's basically predicts the next word", "tokens": [15911, 666, 472, 955, 3461, 295, 2487, 558, 370, 309, 311, 1936, 6069, 82, 264, 958, 1349], "temperature": 0.0, "avg_logprob": -0.21996848813949094, "compression_ratio": 1.5945945945945945, "no_speech_prob": 6.962195129744941e-06}, {"id": 1313, "seek": 649288, "start": 6499.400000000001, "end": 6505.84, "text": " In this huge long thing which is all of the IMDB movie reviews concatenate together, so this thing is you know?", "tokens": [682, 341, 2603, 938, 551, 597, 307, 439, 295, 264, 21463, 27735, 3169, 10229, 1588, 7186, 473, 1214, 11, 370, 341, 551, 307, 291, 458, 30], "temperature": 0.0, "avg_logprob": -0.21996848813949094, "compression_ratio": 1.5945945945945945, "no_speech_prob": 6.962195129744941e-06}, {"id": 1314, "seek": 649288, "start": 6506.36, "end": 6512.4800000000005, "text": " What do we say it was like tens of millions of words long and so what we do is", "tokens": [708, 360, 321, 584, 309, 390, 411, 10688, 295, 6803, 295, 2283, 938, 293, 370, 437, 321, 360, 307], "temperature": 0.0, "avg_logprob": -0.21996848813949094, "compression_ratio": 1.5945945945945945, "no_speech_prob": 6.962195129744941e-06}, {"id": 1315, "seek": 649288, "start": 6513.56, "end": 6516.04, "text": " We split it up into batches", "tokens": [492, 7472, 309, 493, 666, 15245, 279], "temperature": 0.0, "avg_logprob": -0.21996848813949094, "compression_ratio": 1.5945945945945945, "no_speech_prob": 6.962195129744941e-06}, {"id": 1316, "seek": 649288, "start": 6516.84, "end": 6520.08, "text": " First right so these like are our spits into batches", "tokens": [2386, 558, 370, 613, 411, 366, 527, 637, 1208, 666, 15245, 279], "temperature": 0.0, "avg_logprob": -0.21996848813949094, "compression_ratio": 1.5945945945945945, "no_speech_prob": 6.962195129744941e-06}, {"id": 1317, "seek": 652008, "start": 6520.08, "end": 6524.82, "text": " right and so if we said we want a batch size of", "tokens": [558, 293, 370, 498, 321, 848, 321, 528, 257, 15245, 2744, 295], "temperature": 0.0, "avg_logprob": -0.21538817541939873, "compression_ratio": 1.5029585798816567, "no_speech_prob": 2.332066515009501e-06}, {"id": 1318, "seek": 652008, "start": 6525.72, "end": 6531.64, "text": " 64 we actually break the whatever was 60 million words into the 64", "tokens": [12145, 321, 767, 1821, 264, 2035, 390, 4060, 2459, 2283, 666, 264, 12145], "temperature": 0.0, "avg_logprob": -0.21538817541939873, "compression_ratio": 1.5029585798816567, "no_speech_prob": 2.332066515009501e-06}, {"id": 1319, "seek": 652008, "start": 6532.48, "end": 6533.68, "text": " sections", "tokens": [10863], "temperature": 0.0, "avg_logprob": -0.21538817541939873, "compression_ratio": 1.5029585798816567, "no_speech_prob": 2.332066515009501e-06}, {"id": 1320, "seek": 652008, "start": 6533.68, "end": 6539.04, "text": " right and then we take each one of the 64 sections and", "tokens": [558, 293, 550, 321, 747, 1184, 472, 295, 264, 12145, 10863, 293], "temperature": 0.0, "avg_logprob": -0.21538817541939873, "compression_ratio": 1.5029585798816567, "no_speech_prob": 2.332066515009501e-06}, {"id": 1321, "seek": 652008, "start": 6540.32, "end": 6542.32, "text": " We move it", "tokens": [492, 1286, 309], "temperature": 0.0, "avg_logprob": -0.21538817541939873, "compression_ratio": 1.5029585798816567, "no_speech_prob": 2.332066515009501e-06}, {"id": 1322, "seek": 652008, "start": 6543.96, "end": 6547.0, "text": " Like underneath the previous one I", "tokens": [1743, 7223, 264, 3894, 472, 286], "temperature": 0.0, "avg_logprob": -0.21538817541939873, "compression_ratio": 1.5029585798816567, "no_speech_prob": 2.332066515009501e-06}, {"id": 1323, "seek": 654700, "start": 6547.0, "end": 6549.28, "text": " Didn't do a great job of that", "tokens": [11151, 380, 360, 257, 869, 1691, 295, 300], "temperature": 0.0, "avg_logprob": -0.28274981180826825, "compression_ratio": 1.4609929078014185, "no_speech_prob": 2.0580432646966074e-06}, {"id": 1324, "seek": 654700, "start": 6551.56, "end": 6554.24, "text": " Right move it underneath", "tokens": [1779, 1286, 309, 7223], "temperature": 0.0, "avg_logprob": -0.28274981180826825, "compression_ratio": 1.4609929078014185, "no_speech_prob": 2.0580432646966074e-06}, {"id": 1325, "seek": 654700, "start": 6555.76, "end": 6558.3, "text": " So we end up with a matrix", "tokens": [407, 321, 917, 493, 365, 257, 8141], "temperature": 0.0, "avg_logprob": -0.28274981180826825, "compression_ratio": 1.4609929078014185, "no_speech_prob": 2.0580432646966074e-06}, {"id": 1326, "seek": 654700, "start": 6560.12, "end": 6562.12, "text": " Which is", "tokens": [3013, 307], "temperature": 0.0, "avg_logprob": -0.28274981180826825, "compression_ratio": 1.4609929078014185, "no_speech_prob": 2.0580432646966074e-06}, {"id": 1327, "seek": 654700, "start": 6569.08, "end": 6571.08, "text": " 64", "tokens": [12145], "temperature": 0.0, "avg_logprob": -0.28274981180826825, "compression_ratio": 1.4609929078014185, "no_speech_prob": 2.0580432646966074e-06}, {"id": 1328, "seek": 654700, "start": 6571.72, "end": 6576.32, "text": " Actually I think we've moved them across wise so it's actually I think just transpose it we end up with a matrix", "tokens": [5135, 286, 519, 321, 600, 4259, 552, 2108, 10829, 370, 309, 311, 767, 286, 519, 445, 25167, 309, 321, 917, 493, 365, 257, 8141], "temperature": 0.0, "avg_logprob": -0.28274981180826825, "compression_ratio": 1.4609929078014185, "no_speech_prob": 2.0580432646966074e-06}, {"id": 1329, "seek": 657632, "start": 6576.32, "end": 6578.16, "text": " It's like 64", "tokens": [467, 311, 411, 12145], "temperature": 0.0, "avg_logprob": -0.2972426479809905, "compression_ratio": 1.5, "no_speech_prob": 2.5612700937927e-06}, {"id": 1330, "seek": 657632, "start": 6578.16, "end": 6579.38, "text": " columns", "tokens": [13766], "temperature": 0.0, "avg_logprob": -0.2972426479809905, "compression_ratio": 1.5, "no_speech_prob": 2.5612700937927e-06}, {"id": 1331, "seek": 657632, "start": 6579.38, "end": 6586.94, "text": " Wide and the length let's say the original was 64 million right then the length is like", "tokens": [42543, 293, 264, 4641, 718, 311, 584, 264, 3380, 390, 12145, 2459, 558, 550, 264, 4641, 307, 411], "temperature": 0.0, "avg_logprob": -0.2972426479809905, "compression_ratio": 1.5, "no_speech_prob": 2.5612700937927e-06}, {"id": 1332, "seek": 657632, "start": 6587.5199999999995, "end": 6589.5199999999995, "text": " 10 million long", "tokens": [1266, 2459, 938], "temperature": 0.0, "avg_logprob": -0.2972426479809905, "compression_ratio": 1.5, "no_speech_prob": 2.5612700937927e-06}, {"id": 1333, "seek": 657632, "start": 6590.08, "end": 6592.78, "text": " Right so each of these represents", "tokens": [1779, 370, 1184, 295, 613, 8855], "temperature": 0.0, "avg_logprob": -0.2972426479809905, "compression_ratio": 1.5, "no_speech_prob": 2.5612700937927e-06}, {"id": 1334, "seek": 657632, "start": 6594.28, "end": 6596.28, "text": " 164th of our entire", "tokens": [3165, 19, 392, 295, 527, 2302], "temperature": 0.0, "avg_logprob": -0.2972426479809905, "compression_ratio": 1.5, "no_speech_prob": 2.5612700937927e-06}, {"id": 1335, "seek": 657632, "start": 6597.24, "end": 6600.719999999999, "text": " IMDB review set and so that's our starting point", "tokens": [21463, 27735, 3131, 992, 293, 370, 300, 311, 527, 2891, 935], "temperature": 0.0, "avg_logprob": -0.2972426479809905, "compression_ratio": 1.5, "no_speech_prob": 2.5612700937927e-06}, {"id": 1336, "seek": 657632, "start": 6601.679999999999, "end": 6603.679999999999, "text": " so then what we do is", "tokens": [370, 550, 437, 321, 360, 307], "temperature": 0.0, "avg_logprob": -0.2972426479809905, "compression_ratio": 1.5, "no_speech_prob": 2.5612700937927e-06}, {"id": 1337, "seek": 660368, "start": 6603.68, "end": 6611.320000000001, "text": " We then grab a little chunk of this at a time and those chunk lengths are approximately equal to", "tokens": [492, 550, 4444, 257, 707, 16635, 295, 341, 412, 257, 565, 293, 729, 16635, 26329, 366, 10447, 2681, 281], "temperature": 0.0, "avg_logprob": -0.20315283871768566, "compression_ratio": 1.6168224299065421, "no_speech_prob": 2.4824714728310937e-06}, {"id": 1338, "seek": 660368, "start": 6612.56, "end": 6617.04, "text": " BP TT which I think we had equal to 70 so we basically grab a little", "tokens": [40533, 32576, 597, 286, 519, 321, 632, 2681, 281, 5285, 370, 321, 1936, 4444, 257, 707], "temperature": 0.0, "avg_logprob": -0.20315283871768566, "compression_ratio": 1.6168224299065421, "no_speech_prob": 2.4824714728310937e-06}, {"id": 1339, "seek": 660368, "start": 6618.04, "end": 6619.52, "text": " 70 long", "tokens": [5285, 938], "temperature": 0.0, "avg_logprob": -0.20315283871768566, "compression_ratio": 1.6168224299065421, "no_speech_prob": 2.4824714728310937e-06}, {"id": 1340, "seek": 660368, "start": 6619.52, "end": 6624.320000000001, "text": " Section and that's the first thing we chuck into our GPU. That's a batch", "tokens": [21804, 293, 300, 311, 264, 700, 551, 321, 20870, 666, 527, 18407, 13, 663, 311, 257, 15245], "temperature": 0.0, "avg_logprob": -0.20315283871768566, "compression_ratio": 1.6168224299065421, "no_speech_prob": 2.4824714728310937e-06}, {"id": 1341, "seek": 660368, "start": 6624.8, "end": 6628.04, "text": " Right so a batch is always of length of width", "tokens": [1779, 370, 257, 15245, 307, 1009, 295, 4641, 295, 11402], "temperature": 0.0, "avg_logprob": -0.20315283871768566, "compression_ratio": 1.6168224299065421, "no_speech_prob": 2.4824714728310937e-06}, {"id": 1342, "seek": 662804, "start": 6628.04, "end": 6635.22, "text": " 64 or batch size and each bit is a sequence of length up to 70", "tokens": [12145, 420, 15245, 2744, 293, 1184, 857, 307, 257, 8310, 295, 4641, 493, 281, 5285], "temperature": 0.0, "avg_logprob": -0.20769038242576396, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.6016077754320577e-06}, {"id": 1343, "seek": 662804, "start": 6635.64, "end": 6637.24, "text": " So let me show you", "tokens": [407, 718, 385, 855, 291], "temperature": 0.0, "avg_logprob": -0.20769038242576396, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.6016077754320577e-06}, {"id": 1344, "seek": 662804, "start": 6637.24, "end": 6642.08, "text": " All right, so here if I go take my train data loader", "tokens": [1057, 558, 11, 370, 510, 498, 286, 352, 747, 452, 3847, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.20769038242576396, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.6016077754320577e-06}, {"id": 1345, "seek": 662804, "start": 6642.08, "end": 6644.98, "text": " I don't have you folks have tried playing with this yet", "tokens": [286, 500, 380, 362, 291, 4024, 362, 3031, 2433, 365, 341, 1939], "temperature": 0.0, "avg_logprob": -0.20769038242576396, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.6016077754320577e-06}, {"id": 1346, "seek": 662804, "start": 6644.98, "end": 6647.48, "text": " But you can take any data loader wrap it with it", "tokens": [583, 291, 393, 747, 604, 1412, 3677, 260, 7019, 309, 365, 309], "temperature": 0.0, "avg_logprob": -0.20769038242576396, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.6016077754320577e-06}, {"id": 1347, "seek": 662804, "start": 6647.48, "end": 6652.88, "text": " I to turn it into an iterator and then call next on it to grab a batch of", "tokens": [286, 281, 1261, 309, 666, 364, 17138, 1639, 293, 550, 818, 958, 322, 309, 281, 4444, 257, 15245, 295], "temperature": 0.0, "avg_logprob": -0.20769038242576396, "compression_ratio": 1.6385542168674698, "no_speech_prob": 2.6016077754320577e-06}, {"id": 1348, "seek": 665288, "start": 6652.88, "end": 6659.76, "text": " Data just as if you were a neural net you get exactly what the neural net gets and you can see here we get back a", "tokens": [11888, 445, 382, 498, 291, 645, 257, 18161, 2533, 291, 483, 2293, 437, 264, 18161, 2533, 2170, 293, 291, 393, 536, 510, 321, 483, 646, 257], "temperature": 0.0, "avg_logprob": -0.2308767777455004, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.5534935755567858e-06}, {"id": 1349, "seek": 665288, "start": 6661.04, "end": 6663.04, "text": " 75 by 64", "tokens": [9562, 538, 12145], "temperature": 0.0, "avg_logprob": -0.2308767777455004, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.5534935755567858e-06}, {"id": 1350, "seek": 665288, "start": 6664.04, "end": 6670.0, "text": " Tensor right so it's 64 wide right and I said it's approximately", "tokens": [34306, 558, 370, 309, 311, 12145, 4874, 558, 293, 286, 848, 309, 311, 10447], "temperature": 0.0, "avg_logprob": -0.2308767777455004, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.5534935755567858e-06}, {"id": 1351, "seek": 665288, "start": 6671.12, "end": 6674.4800000000005, "text": " 70 high and but not exactly", "tokens": [5285, 1090, 293, 457, 406, 2293], "temperature": 0.0, "avg_logprob": -0.2308767777455004, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.5534935755567858e-06}, {"id": 1352, "seek": 667448, "start": 6674.48, "end": 6682.08, "text": " And that's actually kind of interesting a really neat trick that torch text does is they randomly change", "tokens": [400, 300, 311, 767, 733, 295, 1880, 257, 534, 10654, 4282, 300, 27822, 2487, 775, 307, 436, 16979, 1319], "temperature": 0.0, "avg_logprob": -0.16411596321197877, "compression_ratio": 1.6396396396396395, "no_speech_prob": 8.714321211300557e-07}, {"id": 1353, "seek": 667448, "start": 6682.719999999999, "end": 6689.08, "text": " The back prop through time number every time so each epoch. It's getting slightly different", "tokens": [440, 646, 2365, 807, 565, 1230, 633, 565, 370, 1184, 30992, 339, 13, 467, 311, 1242, 4748, 819], "temperature": 0.0, "avg_logprob": -0.16411596321197877, "compression_ratio": 1.6396396396396395, "no_speech_prob": 8.714321211300557e-07}, {"id": 1354, "seek": 667448, "start": 6690.24, "end": 6692.2, "text": " bits of text", "tokens": [9239, 295, 2487], "temperature": 0.0, "avg_logprob": -0.16411596321197877, "compression_ratio": 1.6396396396396395, "no_speech_prob": 8.714321211300557e-07}, {"id": 1355, "seek": 667448, "start": 6692.2, "end": 6697.08, "text": " This is kind of like in computer vision. We randomly shuffle the images", "tokens": [639, 307, 733, 295, 411, 294, 3820, 5201, 13, 492, 16979, 39426, 264, 5267], "temperature": 0.0, "avg_logprob": -0.16411596321197877, "compression_ratio": 1.6396396396396395, "no_speech_prob": 8.714321211300557e-07}, {"id": 1356, "seek": 667448, "start": 6697.259999999999, "end": 6701.919999999999, "text": " We can't randomly shuffle the words right because we need to be in the right order", "tokens": [492, 393, 380, 16979, 39426, 264, 2283, 558, 570, 321, 643, 281, 312, 294, 264, 558, 1668], "temperature": 0.0, "avg_logprob": -0.16411596321197877, "compression_ratio": 1.6396396396396395, "no_speech_prob": 8.714321211300557e-07}, {"id": 1357, "seek": 670192, "start": 6701.92, "end": 6707.26, "text": " So instead we randomly move their breakpoints a little bit okay, so this is the equivalent", "tokens": [407, 2602, 321, 16979, 1286, 641, 1821, 20552, 257, 707, 857, 1392, 11, 370, 341, 307, 264, 10344], "temperature": 0.0, "avg_logprob": -0.1847157685653023, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.561269411671674e-06}, {"id": 1358, "seek": 670192, "start": 6708.36, "end": 6710.36, "text": " so in other words this", "tokens": [370, 294, 661, 2283, 341], "temperature": 0.0, "avg_logprob": -0.1847157685653023, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.561269411671674e-06}, {"id": 1359, "seek": 670192, "start": 6714.92, "end": 6720.4400000000005, "text": " This here is of length 75 right there's a there's an ellipsis in the middle", "tokens": [639, 510, 307, 295, 4641, 9562, 558, 456, 311, 257, 456, 311, 364, 8284, 2600, 271, 294, 264, 2808], "temperature": 0.0, "avg_logprob": -0.1847157685653023, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.561269411671674e-06}, {"id": 1360, "seek": 670192, "start": 6720.96, "end": 6728.92, "text": " And that represents the first 75 words of the first review right where else this 75 here", "tokens": [400, 300, 8855, 264, 700, 9562, 2283, 295, 264, 700, 3131, 558, 689, 1646, 341, 9562, 510], "temperature": 0.0, "avg_logprob": -0.1847157685653023, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.561269411671674e-06}, {"id": 1361, "seek": 672892, "start": 6728.92, "end": 6735.08, "text": " represents the first 75 words of this of the second of the 64 segments", "tokens": [8855, 264, 700, 9562, 2283, 295, 341, 295, 264, 1150, 295, 264, 12145, 19904], "temperature": 0.0, "avg_logprob": -0.20008961158462718, "compression_ratio": 1.7159763313609468, "no_speech_prob": 2.8130043574492447e-06}, {"id": 1362, "seek": 672892, "start": 6735.36, "end": 6740.82, "text": " That's it have to go in like 10 million words to find that one right and so here's the first", "tokens": [663, 311, 309, 362, 281, 352, 294, 411, 1266, 2459, 2283, 281, 915, 300, 472, 558, 293, 370, 510, 311, 264, 700], "temperature": 0.0, "avg_logprob": -0.20008961158462718, "compression_ratio": 1.7159763313609468, "no_speech_prob": 2.8130043574492447e-06}, {"id": 1363, "seek": 672892, "start": 6741.84, "end": 6747.84, "text": " 75 words of the last of those 64 segments okay, and so then what we have", "tokens": [9562, 2283, 295, 264, 1036, 295, 729, 12145, 19904, 1392, 11, 293, 370, 550, 437, 321, 362], "temperature": 0.0, "avg_logprob": -0.20008961158462718, "compression_ratio": 1.7159763313609468, "no_speech_prob": 2.8130043574492447e-06}, {"id": 1364, "seek": 672892, "start": 6748.96, "end": 6750.96, "text": " down here is", "tokens": [760, 510, 307], "temperature": 0.0, "avg_logprob": -0.20008961158462718, "compression_ratio": 1.7159763313609468, "no_speech_prob": 2.8130043574492447e-06}, {"id": 1365, "seek": 672892, "start": 6752.56, "end": 6754.56, "text": " The next", "tokens": [440, 958], "temperature": 0.0, "avg_logprob": -0.20008961158462718, "compression_ratio": 1.7159763313609468, "no_speech_prob": 2.8130043574492447e-06}, {"id": 1366, "seek": 675456, "start": 6754.56, "end": 6759.120000000001, "text": " Sequence right so 51 there's 51", "tokens": [46859, 655, 558, 370, 18485, 456, 311, 18485], "temperature": 0.0, "avg_logprob": -0.2371354958949945, "compression_ratio": 1.5567567567567568, "no_speech_prob": 7.766908311168663e-06}, {"id": 1367, "seek": 675456, "start": 6759.72, "end": 6765.200000000001, "text": " 615 there's 615 25 there's 25 right and in this case", "tokens": [1386, 5211, 456, 311, 1386, 5211, 3552, 456, 311, 3552, 558, 293, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.2371354958949945, "compression_ratio": 1.5567567567567568, "no_speech_prob": 7.766908311168663e-06}, {"id": 1368, "seek": 675456, "start": 6765.8, "end": 6767.84, "text": " It actually is of the same size", "tokens": [467, 767, 307, 295, 264, 912, 2744], "temperature": 0.0, "avg_logprob": -0.2371354958949945, "compression_ratio": 1.5567567567567568, "no_speech_prob": 7.766908311168663e-06}, {"id": 1369, "seek": 675456, "start": 6767.84, "end": 6773.120000000001, "text": " It's also 75 plus 64 but for minor technical reasons being flattened out", "tokens": [467, 311, 611, 9562, 1804, 12145, 457, 337, 6696, 6191, 4112, 885, 24183, 292, 484], "temperature": 0.0, "avg_logprob": -0.2371354958949945, "compression_ratio": 1.5567567567567568, "no_speech_prob": 7.766908311168663e-06}, {"id": 1370, "seek": 675456, "start": 6773.72, "end": 6781.160000000001, "text": " Into a single vector that basically it's exactly the same as this matrix, but it's just moved down", "tokens": [23373, 257, 2167, 8062, 300, 1936, 309, 311, 2293, 264, 912, 382, 341, 8141, 11, 457, 309, 311, 445, 4259, 760], "temperature": 0.0, "avg_logprob": -0.2371354958949945, "compression_ratio": 1.5567567567567568, "no_speech_prob": 7.766908311168663e-06}, {"id": 1371, "seek": 678116, "start": 6781.16, "end": 6785.76, "text": " by one because we're trying to predict the next word", "tokens": [538, 472, 570, 321, 434, 1382, 281, 6069, 264, 958, 1349], "temperature": 0.0, "avg_logprob": -0.23632184196920955, "compression_ratio": 1.4795321637426901, "no_speech_prob": 2.0261359168216586e-06}, {"id": 1372, "seek": 678116, "start": 6786.28, "end": 6794.599999999999, "text": " right so that all happens for us right if we ask for and this is the fast AI now if you ask for a language model", "tokens": [558, 370, 300, 439, 2314, 337, 505, 558, 498, 321, 1029, 337, 293, 341, 307, 264, 2370, 7318, 586, 498, 291, 1029, 337, 257, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.23632184196920955, "compression_ratio": 1.4795321637426901, "no_speech_prob": 2.0261359168216586e-06}, {"id": 1373, "seek": 678116, "start": 6794.599999999999, "end": 6796.44, "text": " data object", "tokens": [1412, 2657], "temperature": 0.0, "avg_logprob": -0.23632184196920955, "compression_ratio": 1.4795321637426901, "no_speech_prob": 2.0261359168216586e-06}, {"id": 1374, "seek": 678116, "start": 6796.44, "end": 6798.84, "text": " then it's going to create these batches of", "tokens": [550, 309, 311, 516, 281, 1884, 613, 15245, 279, 295], "temperature": 0.0, "avg_logprob": -0.23632184196920955, "compression_ratio": 1.4795321637426901, "no_speech_prob": 2.0261359168216586e-06}, {"id": 1375, "seek": 678116, "start": 6800.32, "end": 6804.0199999999995, "text": " batch size width by BP TT height", "tokens": [15245, 2744, 11402, 538, 40533, 32576, 6681], "temperature": 0.0, "avg_logprob": -0.23632184196920955, "compression_ratio": 1.4795321637426901, "no_speech_prob": 2.0261359168216586e-06}, {"id": 1376, "seek": 680402, "start": 6804.02, "end": 6809.820000000001, "text": " Bits of our language corpus along with the same thing", "tokens": [363, 1208, 295, 527, 2856, 1181, 31624, 2051, 365, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.143176947067033, "compression_ratio": 1.4887640449438202, "no_speech_prob": 6.438886430260027e-06}, {"id": 1377, "seek": 680402, "start": 6810.38, "end": 6816.14, "text": " Shuffled along by one word right and so we're always going to try and predict the next word", "tokens": [1160, 33974, 2051, 538, 472, 1349, 558, 293, 370, 321, 434, 1009, 516, 281, 853, 293, 6069, 264, 958, 1349], "temperature": 0.0, "avg_logprob": -0.143176947067033, "compression_ratio": 1.4887640449438202, "no_speech_prob": 6.438886430260027e-06}, {"id": 1378, "seek": 680402, "start": 6821.38, "end": 6827.14, "text": " So why don't you instead of just arbitrarily choosing 64", "tokens": [407, 983, 500, 380, 291, 2602, 295, 445, 19071, 3289, 10875, 12145], "temperature": 0.0, "avg_logprob": -0.143176947067033, "compression_ratio": 1.4887640449438202, "no_speech_prob": 6.438886430260027e-06}, {"id": 1379, "seek": 682714, "start": 6827.14, "end": 6833.14, "text": " Why don't you choose like like 64 is a large number maybe like do it by", "tokens": [1545, 500, 380, 291, 2826, 411, 411, 12145, 307, 257, 2416, 1230, 1310, 411, 360, 309, 538], "temperature": 0.0, "avg_logprob": -0.32977618341860565, "compression_ratio": 1.609442060085837, "no_speech_prob": 3.555856892489828e-06}, {"id": 1380, "seek": 682714, "start": 6833.780000000001, "end": 6838.38, "text": " Sentences and make it a large number and then pad it with zero or something", "tokens": [23652, 2667, 293, 652, 309, 257, 2416, 1230, 293, 550, 6887, 309, 365, 4018, 420, 746], "temperature": 0.0, "avg_logprob": -0.32977618341860565, "compression_ratio": 1.609442060085837, "no_speech_prob": 3.555856892489828e-06}, {"id": 1381, "seek": 682714, "start": 6839.38, "end": 6843.9800000000005, "text": " If you you know so that you actually have a one full sentence per line", "tokens": [759, 291, 291, 458, 370, 300, 291, 767, 362, 257, 472, 1577, 8174, 680, 1622], "temperature": 0.0, "avg_logprob": -0.32977618341860565, "compression_ratio": 1.609442060085837, "no_speech_prob": 3.555856892489828e-06}, {"id": 1382, "seek": 682714, "start": 6844.660000000001, "end": 6849.06, "text": " Basically wouldn't that make more sense not really because remember we're using columns, right?", "tokens": [8537, 2759, 380, 300, 652, 544, 2020, 406, 534, 570, 1604, 321, 434, 1228, 13766, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.32977618341860565, "compression_ratio": 1.609442060085837, "no_speech_prob": 3.555856892489828e-06}, {"id": 1383, "seek": 682714, "start": 6849.06, "end": 6852.34, "text": " So each of our columns is of length about 10 million, right?", "tokens": [407, 1184, 295, 527, 13766, 307, 295, 4641, 466, 1266, 2459, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.32977618341860565, "compression_ratio": 1.609442060085837, "no_speech_prob": 3.555856892489828e-06}, {"id": 1384, "seek": 685234, "start": 6852.34, "end": 6859.14, "text": " So although it's true that those columns aren't always exactly finishing on a full stop. They're so damn long", "tokens": [407, 4878, 309, 311, 2074, 300, 729, 13766, 3212, 380, 1009, 2293, 12693, 322, 257, 1577, 1590, 13, 814, 434, 370, 8151, 938], "temperature": 0.0, "avg_logprob": -0.45139053889683317, "compression_ratio": 1.7184466019417475, "no_speech_prob": 1.3006996596232057e-05}, {"id": 1385, "seek": 685234, "start": 6859.66, "end": 6861.66, "text": " We don't care", "tokens": [492, 500, 380, 1127], "temperature": 0.0, "avg_logprob": -0.45139053889683317, "compression_ratio": 1.7184466019417475, "no_speech_prob": 1.3006996596232057e-05}, {"id": 1386, "seek": 685234, "start": 6861.66, "end": 6863.66, "text": " Is there like 10 million long?", "tokens": [1119, 456, 411, 1266, 2459, 938, 30], "temperature": 0.0, "avg_logprob": -0.45139053889683317, "compression_ratio": 1.7184466019417475, "no_speech_prob": 1.3006996596232057e-05}, {"id": 1387, "seek": 685234, "start": 6864.46, "end": 6871.26, "text": " Right and we're trying to also each each line contains multiple sentences column contains multiple", "tokens": [1779, 293, 321, 434, 1382, 281, 611, 1184, 1184, 1622, 8306, 3866, 16579, 7738, 8306, 3866], "temperature": 0.0, "avg_logprob": -0.45139053889683317, "compression_ratio": 1.7184466019417475, "no_speech_prob": 1.3006996596232057e-05}, {"id": 1388, "seek": 685234, "start": 6872.06, "end": 6877.66, "text": " Senses yeah, it's it's of length about 10 million and it contains many many many many many sentences", "tokens": [318, 9085, 1338, 11, 309, 311, 309, 311, 295, 4641, 466, 1266, 2459, 293, 309, 8306, 867, 867, 867, 867, 867, 16579], "temperature": 0.0, "avg_logprob": -0.45139053889683317, "compression_ratio": 1.7184466019417475, "no_speech_prob": 1.3006996596232057e-05}, {"id": 1389, "seek": 687766, "start": 6877.66, "end": 6882.22, "text": " Because remember the first thing we did was take the whole thing and split it into 64 groups", "tokens": [1436, 1604, 264, 700, 551, 321, 630, 390, 747, 264, 1379, 551, 293, 7472, 309, 666, 12145, 3935], "temperature": 0.0, "avg_logprob": -0.3030151859406502, "compression_ratio": 1.5617021276595744, "no_speech_prob": 4.785057626577327e-06}, {"id": 1390, "seek": 687766, "start": 6886.3, "end": 6888.3, "text": " Okay, great", "tokens": [1033, 11, 869], "temperature": 0.0, "avg_logprob": -0.3030151859406502, "compression_ratio": 1.5617021276595744, "no_speech_prob": 4.785057626577327e-06}, {"id": 1391, "seek": 687766, "start": 6889.42, "end": 6894.66, "text": " So, um, I found this up, you know pertaining to this question this thing about like", "tokens": [407, 11, 1105, 11, 286, 1352, 341, 493, 11, 291, 458, 49582, 281, 341, 1168, 341, 551, 466, 411], "temperature": 0.0, "avg_logprob": -0.3030151859406502, "compression_ratio": 1.5617021276595744, "no_speech_prob": 4.785057626577327e-06}, {"id": 1392, "seek": 687766, "start": 6895.34, "end": 6900.58, "text": " What's in this language model matrix a little mind-bending for quite a while", "tokens": [708, 311, 294, 341, 2856, 2316, 8141, 257, 707, 1575, 12, 65, 2029, 337, 1596, 257, 1339], "temperature": 0.0, "avg_logprob": -0.3030151859406502, "compression_ratio": 1.5617021276595744, "no_speech_prob": 4.785057626577327e-06}, {"id": 1393, "seek": 687766, "start": 6900.58, "end": 6906.099999999999, "text": " So don't worry if it takes a while and you have to ask a thousand questions on the forum. That's fine", "tokens": [407, 500, 380, 3292, 498, 309, 2516, 257, 1339, 293, 291, 362, 281, 1029, 257, 4714, 1651, 322, 264, 17542, 13, 663, 311, 2489], "temperature": 0.0, "avg_logprob": -0.3030151859406502, "compression_ratio": 1.5617021276595744, "no_speech_prob": 4.785057626577327e-06}, {"id": 1394, "seek": 690610, "start": 6906.1, "end": 6908.1, "text": " right, but", "tokens": [558, 11, 457], "temperature": 0.0, "avg_logprob": -0.30263334054213303, "compression_ratio": 1.5968992248062015, "no_speech_prob": 5.014690032112412e-06}, {"id": 1395, "seek": 690610, "start": 6908.22, "end": 6910.820000000001, "text": " Go back and listen to what I just said in this lecture again", "tokens": [1037, 646, 293, 2140, 281, 437, 286, 445, 848, 294, 341, 7991, 797], "temperature": 0.0, "avg_logprob": -0.30263334054213303, "compression_ratio": 1.5968992248062015, "no_speech_prob": 5.014690032112412e-06}, {"id": 1396, "seek": 690610, "start": 6910.820000000001, "end": 6916.660000000001, "text": " go back to that bit where I showed you splitting it up to 64 and moving them around and try it with some sentences in Excel or", "tokens": [352, 646, 281, 300, 857, 689, 286, 4712, 291, 30348, 309, 493, 281, 12145, 293, 2684, 552, 926, 293, 853, 309, 365, 512, 16579, 294, 19060, 420], "temperature": 0.0, "avg_logprob": -0.30263334054213303, "compression_ratio": 1.5968992248062015, "no_speech_prob": 5.014690032112412e-06}, {"id": 1397, "seek": 690610, "start": 6916.660000000001, "end": 6920.820000000001, "text": " Something and see if you can do a better job of explaining it than I did", "tokens": [6595, 293, 536, 498, 291, 393, 360, 257, 1101, 1691, 295, 13468, 309, 813, 286, 630], "temperature": 0.0, "avg_logprob": -0.30263334054213303, "compression_ratio": 1.5968992248062015, "no_speech_prob": 5.014690032112412e-06}, {"id": 1398, "seek": 690610, "start": 6921.54, "end": 6924.780000000001, "text": " Because this is like how torch text works", "tokens": [1436, 341, 307, 411, 577, 27822, 2487, 1985], "temperature": 0.0, "avg_logprob": -0.30263334054213303, "compression_ratio": 1.5968992248062015, "no_speech_prob": 5.014690032112412e-06}, {"id": 1399, "seek": 690610, "start": 6925.660000000001, "end": 6932.02, "text": " And then what fast AI adds on is this idea of like kind of how to build a language model out of it", "tokens": [400, 550, 437, 2370, 7318, 10860, 322, 307, 341, 1558, 295, 411, 733, 295, 577, 281, 1322, 257, 2856, 2316, 484, 295, 309], "temperature": 0.0, "avg_logprob": -0.30263334054213303, "compression_ratio": 1.5968992248062015, "no_speech_prob": 5.014690032112412e-06}, {"id": 1400, "seek": 693202, "start": 6932.02, "end": 6938.700000000001, "text": " a language model out of it, although actually a lot of that's stolen from torch text as well like this sometimes where torch text starts", "tokens": [257, 2856, 2316, 484, 295, 309, 11, 4878, 767, 257, 688, 295, 300, 311, 15900, 490, 27822, 2487, 382, 731, 411, 341, 2171, 689, 27822, 2487, 3719], "temperature": 0.0, "avg_logprob": -0.24967627274362664, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.2252704638958676e-06}, {"id": 1401, "seek": 693202, "start": 6938.700000000001, "end": 6941.740000000001, "text": " And fast AI ends is or vice versa is a little", "tokens": [400, 2370, 7318, 5314, 307, 420, 11964, 25650, 307, 257, 707], "temperature": 0.0, "avg_logprob": -0.24967627274362664, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.2252704638958676e-06}, {"id": 1402, "seek": 693202, "start": 6942.34, "end": 6943.3, "text": " subtle", "tokens": [13743], "temperature": 0.0, "avg_logprob": -0.24967627274362664, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.2252704638958676e-06}, {"id": 1403, "seek": 693202, "start": 6943.3, "end": 6945.3, "text": " They really work closely together", "tokens": [814, 534, 589, 8185, 1214], "temperature": 0.0, "avg_logprob": -0.24967627274362664, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.2252704638958676e-06}, {"id": 1404, "seek": 693202, "start": 6945.5, "end": 6946.580000000001, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.24967627274362664, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.2252704638958676e-06}, {"id": 1405, "seek": 693202, "start": 6946.580000000001, "end": 6948.02, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.24967627274362664, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.2252704638958676e-06}, {"id": 1406, "seek": 693202, "start": 6948.02, "end": 6950.9800000000005, "text": " Now that we have a model data object", "tokens": [823, 300, 321, 362, 257, 2316, 1412, 2657], "temperature": 0.0, "avg_logprob": -0.24967627274362664, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.2252704638958676e-06}, {"id": 1407, "seek": 693202, "start": 6951.540000000001, "end": 6953.540000000001, "text": " That can feed us", "tokens": [663, 393, 3154, 505], "temperature": 0.0, "avg_logprob": -0.24967627274362664, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.2252704638958676e-06}, {"id": 1408, "seek": 693202, "start": 6953.780000000001, "end": 6959.280000000001, "text": " Batches we can go ahead and create a model right and so in this case", "tokens": [363, 852, 279, 321, 393, 352, 2286, 293, 1884, 257, 2316, 558, 293, 370, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.24967627274362664, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.2252704638958676e-06}, {"id": 1409, "seek": 695928, "start": 6959.28, "end": 6964.12, "text": " We're going to create an embedding matrix and our vocab", "tokens": [492, 434, 516, 281, 1884, 364, 12240, 3584, 8141, 293, 527, 2329, 455], "temperature": 0.0, "avg_logprob": -0.17212586992242362, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0188056194238015e-06}, {"id": 1410, "seek": 695928, "start": 6964.8, "end": 6966.8, "text": " We can see how big our vocab was", "tokens": [492, 393, 536, 577, 955, 527, 2329, 455, 390], "temperature": 0.0, "avg_logprob": -0.17212586992242362, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0188056194238015e-06}, {"id": 1411, "seek": 695928, "start": 6968.5199999999995, "end": 6976.5199999999995, "text": " Let's have a look back here so we can see here in the model data object there are four thousand six hundred and two", "tokens": [961, 311, 362, 257, 574, 646, 510, 370, 321, 393, 536, 510, 294, 264, 2316, 1412, 2657, 456, 366, 1451, 4714, 2309, 3262, 293, 732], "temperature": 0.0, "avg_logprob": -0.17212586992242362, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0188056194238015e-06}, {"id": 1412, "seek": 695928, "start": 6978.44, "end": 6982.44, "text": " Kind of pieces that we're going to go through that's basically equal to the number of", "tokens": [9242, 295, 3755, 300, 321, 434, 516, 281, 352, 807, 300, 311, 1936, 2681, 281, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.17212586992242362, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0188056194238015e-06}, {"id": 1413, "seek": 698244, "start": 6982.44, "end": 6989.679999999999, "text": " The total length of everything divided by batch size times BPTT and this one", "tokens": [440, 3217, 4641, 295, 1203, 6666, 538, 15245, 2744, 1413, 363, 47, 28178, 293, 341, 472], "temperature": 0.0, "avg_logprob": -0.2522929280074601, "compression_ratio": 1.5569620253164558, "no_speech_prob": 9.132525633503974e-07}, {"id": 1414, "seek": 698244, "start": 6989.679999999999, "end": 6996.08, "text": " I wanted to show you NT I've got the definition up here number of unique tokens NT is the number of tokens", "tokens": [286, 1415, 281, 855, 291, 43452, 286, 600, 658, 264, 7123, 493, 510, 1230, 295, 3845, 22667, 43452, 307, 264, 1230, 295, 22667], "temperature": 0.0, "avg_logprob": -0.2522929280074601, "compression_ratio": 1.5569620253164558, "no_speech_prob": 9.132525633503974e-07}, {"id": 1415, "seek": 698244, "start": 6996.24, "end": 6999.04, "text": " That's the size of our vocab. So we've got three", "tokens": [663, 311, 264, 2744, 295, 527, 2329, 455, 13, 407, 321, 600, 658, 1045], "temperature": 0.0, "avg_logprob": -0.2522929280074601, "compression_ratio": 1.5569620253164558, "no_speech_prob": 9.132525633503974e-07}, {"id": 1416, "seek": 698244, "start": 7000.32, "end": 7002.04, "text": " 34,945", "tokens": [12790, 11, 24, 8465], "temperature": 0.0, "avg_logprob": -0.2522929280074601, "compression_ratio": 1.5569620253164558, "no_speech_prob": 9.132525633503974e-07}, {"id": 1417, "seek": 698244, "start": 7002.04, "end": 7003.719999999999, "text": " unique words", "tokens": [3845, 2283], "temperature": 0.0, "avg_logprob": -0.2522929280074601, "compression_ratio": 1.5569620253164558, "no_speech_prob": 9.132525633503974e-07}, {"id": 1418, "seek": 698244, "start": 7003.719999999999, "end": 7006.9, "text": " And notice the unique words that had to appear at least ten times", "tokens": [400, 3449, 264, 3845, 2283, 300, 632, 281, 4204, 412, 1935, 2064, 1413], "temperature": 0.0, "avg_logprob": -0.2522929280074601, "compression_ratio": 1.5569620253164558, "no_speech_prob": 9.132525633503974e-07}, {"id": 1419, "seek": 698244, "start": 7007.599999999999, "end": 7010.28, "text": " Okay, because otherwise they've been replaced with", "tokens": [1033, 11, 570, 5911, 436, 600, 668, 10772, 365], "temperature": 0.0, "avg_logprob": -0.2522929280074601, "compression_ratio": 1.5569620253164558, "no_speech_prob": 9.132525633503974e-07}, {"id": 1420, "seek": 701028, "start": 7010.28, "end": 7012.28, "text": "unk", "tokens": [3197], "temperature": 0.0, "avg_logprob": -0.3203453302383423, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.443980974931037e-06}, {"id": 1421, "seek": 701028, "start": 7015.24, "end": 7017.36, "text": " The length of the data set is one", "tokens": [440, 4641, 295, 264, 1412, 992, 307, 472], "temperature": 0.0, "avg_logprob": -0.3203453302383423, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.443980974931037e-06}, {"id": 1422, "seek": 701028, "start": 7018.24, "end": 7020.86, "text": " Because as far as the language model is concerned there's only one", "tokens": [1436, 382, 1400, 382, 264, 2856, 2316, 307, 5922, 456, 311, 787, 472], "temperature": 0.0, "avg_logprob": -0.3203453302383423, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.443980974931037e-06}, {"id": 1423, "seek": 701028, "start": 7021.759999999999, "end": 7026.5199999999995, "text": " Thing which is the whole corpus right and then that thing has", "tokens": [30902, 597, 307, 264, 1379, 1181, 31624, 558, 293, 550, 300, 551, 575], "temperature": 0.0, "avg_logprob": -0.3203453302383423, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.443980974931037e-06}, {"id": 1424, "seek": 701028, "start": 7027.4, "end": 7029.48, "text": " Here it is twenty point six million", "tokens": [1692, 309, 307, 7699, 935, 2309, 2459], "temperature": 0.0, "avg_logprob": -0.3203453302383423, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.443980974931037e-06}, {"id": 1425, "seek": 701028, "start": 7030.36, "end": 7032.36, "text": " words, you know, right", "tokens": [2283, 11, 291, 458, 11, 558], "temperature": 0.0, "avg_logprob": -0.3203453302383423, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.443980974931037e-06}, {"id": 1426, "seek": 701028, "start": 7032.84, "end": 7034.84, "text": " so those", "tokens": [370, 729], "temperature": 0.0, "avg_logprob": -0.3203453302383423, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.443980974931037e-06}, {"id": 1427, "seek": 703484, "start": 7034.84, "end": 7039.84, "text": " 4445 things are used to create an embedding matrix of", "tokens": [16408, 8465, 721, 366, 1143, 281, 1884, 364, 12240, 3584, 8141, 295], "temperature": 0.0, "avg_logprob": -0.305003213100746, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.4824678348522866e-06}, {"id": 1428, "seek": 703484, "start": 7041.56, "end": 7043.56, "text": " Number of rows is equal to", "tokens": [5118, 295, 13241, 307, 2681, 281], "temperature": 0.0, "avg_logprob": -0.305003213100746, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.4824678348522866e-06}, {"id": 1429, "seek": 703484, "start": 7046.08, "end": 7047.72, "text": " 34", "tokens": [12790], "temperature": 0.0, "avg_logprob": -0.305003213100746, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.4824678348522866e-06}, {"id": 1430, "seek": 703484, "start": 7047.72, "end": 7049.4400000000005, "text": " 9 4 5", "tokens": [1722, 1017, 1025], "temperature": 0.0, "avg_logprob": -0.305003213100746, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.4824678348522866e-06}, {"id": 1431, "seek": 703484, "start": 7049.4400000000005, "end": 7055.24, "text": " Right and so the first one represents unc the second one represents pad", "tokens": [1779, 293, 370, 264, 700, 472, 8855, 6219, 264, 1150, 472, 8855, 6887], "temperature": 0.0, "avg_logprob": -0.305003213100746, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.4824678348522866e-06}, {"id": 1432, "seek": 703484, "start": 7055.84, "end": 7059.74, "text": " The third one was dot the fourth one was comma this one", "tokens": [440, 2636, 472, 390, 5893, 264, 6409, 472, 390, 22117, 341, 472], "temperature": 0.0, "avg_logprob": -0.305003213100746, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.4824678348522866e-06}, {"id": 1433, "seek": 705974, "start": 7059.74, "end": 7065.44, "text": " I'm just guessing with that and so forth right and so each one of these gets an", "tokens": [286, 478, 445, 17939, 365, 300, 293, 370, 5220, 558, 293, 370, 1184, 472, 295, 613, 2170, 364], "temperature": 0.0, "avg_logprob": -0.21141688677729392, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1434, "seek": 705974, "start": 7066.2, "end": 7067.679999999999, "text": " embedding vector", "tokens": [12240, 3584, 8062], "temperature": 0.0, "avg_logprob": -0.21141688677729392, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1435, "seek": 705974, "start": 7067.679999999999, "end": 7070.48, "text": " So this is literally identical to what we did", "tokens": [407, 341, 307, 3736, 14800, 281, 437, 321, 630], "temperature": 0.0, "avg_logprob": -0.21141688677729392, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1436, "seek": 705974, "start": 7071.24, "end": 7074.76, "text": " Before the break, right? This is a categorical variable", "tokens": [4546, 264, 1821, 11, 558, 30, 639, 307, 257, 19250, 804, 7006], "temperature": 0.0, "avg_logprob": -0.21141688677729392, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1437, "seek": 705974, "start": 7075.12, "end": 7080.639999999999, "text": " It's just a very high cardinality categorical variable and furthermore. It's the only variable", "tokens": [467, 311, 445, 257, 588, 1090, 2920, 259, 1860, 19250, 804, 7006, 293, 3052, 3138, 13, 467, 311, 264, 787, 7006], "temperature": 0.0, "avg_logprob": -0.21141688677729392, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1438, "seek": 705974, "start": 7081.04, "end": 7087.5199999999995, "text": " Right. This is pretty standard in NLP. You have a variable which is a word, right?", "tokens": [1779, 13, 639, 307, 1238, 3832, 294, 426, 45196, 13, 509, 362, 257, 7006, 597, 307, 257, 1349, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21141688677729392, "compression_ratio": 1.6563876651982379, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1439, "seek": 708752, "start": 7087.52, "end": 7091.200000000001, "text": " We have a single categorical variable single column", "tokens": [492, 362, 257, 2167, 19250, 804, 7006, 2167, 7738], "temperature": 0.0, "avg_logprob": -0.21518461094346159, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.816216695260664e-06}, {"id": 1440, "seek": 708752, "start": 7091.84, "end": 7093.84, "text": " basically, and it's it's a", "tokens": [1936, 11, 293, 309, 311, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.21518461094346159, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.816216695260664e-06}, {"id": 1441, "seek": 708752, "start": 7094.92, "end": 7096.360000000001, "text": " 34,000", "tokens": [12790, 11, 1360], "temperature": 0.0, "avg_logprob": -0.21518461094346159, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.816216695260664e-06}, {"id": 1442, "seek": 708752, "start": 7096.360000000001, "end": 7101.88, "text": " 945 cardinality categorical variable and so we're going to create an embedding matrix for it", "tokens": [1722, 8465, 2920, 259, 1860, 19250, 804, 7006, 293, 370, 321, 434, 516, 281, 1884, 364, 12240, 3584, 8141, 337, 309], "temperature": 0.0, "avg_logprob": -0.21518461094346159, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.816216695260664e-06}, {"id": 1443, "seek": 708752, "start": 7102.64, "end": 7107.240000000001, "text": " So m size is the size of the embedding vector 200", "tokens": [407, 275, 2744, 307, 264, 2744, 295, 264, 12240, 3584, 8062, 2331], "temperature": 0.0, "avg_logprob": -0.21518461094346159, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.816216695260664e-06}, {"id": 1444, "seek": 708752, "start": 7107.52, "end": 7112.8, "text": " Okay, so that's going to be length 200 a lot bigger than our previous embedding vectors", "tokens": [1033, 11, 370, 300, 311, 516, 281, 312, 4641, 2331, 257, 688, 3801, 813, 527, 3894, 12240, 3584, 18875], "temperature": 0.0, "avg_logprob": -0.21518461094346159, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.816216695260664e-06}, {"id": 1445, "seek": 711280, "start": 7112.8, "end": 7118.9400000000005, "text": " Not surprising because a word has a lot more nuance to it than the concept of Sunday", "tokens": [1726, 8830, 570, 257, 1349, 575, 257, 688, 544, 42625, 281, 309, 813, 264, 3410, 295, 7776], "temperature": 0.0, "avg_logprob": -0.1668719542653937, "compression_ratio": 1.5041322314049588, "no_speech_prob": 1.6119913937018282e-07}, {"id": 1446, "seek": 711280, "start": 7119.56, "end": 7120.8, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.1668719542653937, "compression_ratio": 1.5041322314049588, "no_speech_prob": 1.6119913937018282e-07}, {"id": 1447, "seek": 711280, "start": 7120.8, "end": 7122.04, "text": " or", "tokens": [420], "temperature": 0.0, "avg_logprob": -0.1668719542653937, "compression_ratio": 1.5041322314049588, "no_speech_prob": 1.6119913937018282e-07}, {"id": 1448, "seek": 711280, "start": 7122.04, "end": 7124.6, "text": " Rossman's Berlin store or whatever right?", "tokens": [16140, 1601, 311, 13848, 3531, 420, 2035, 558, 30], "temperature": 0.0, "avg_logprob": -0.1668719542653937, "compression_ratio": 1.5041322314049588, "no_speech_prob": 1.6119913937018282e-07}, {"id": 1449, "seek": 711280, "start": 7124.6, "end": 7130.400000000001, "text": " So it's generally an embedding size for a word will be somewhere between about 50 and about 600", "tokens": [407, 309, 311, 5101, 364, 12240, 3584, 2744, 337, 257, 1349, 486, 312, 4079, 1296, 466, 2625, 293, 466, 11849], "temperature": 0.0, "avg_logprob": -0.1668719542653937, "compression_ratio": 1.5041322314049588, "no_speech_prob": 1.6119913937018282e-07}, {"id": 1450, "seek": 711280, "start": 7131.0, "end": 7133.0, "text": " Okay, so I've kind of gone somewhere in the middle", "tokens": [1033, 11, 370, 286, 600, 733, 295, 2780, 4079, 294, 264, 2808], "temperature": 0.0, "avg_logprob": -0.1668719542653937, "compression_ratio": 1.5041322314049588, "no_speech_prob": 1.6119913937018282e-07}, {"id": 1451, "seek": 711280, "start": 7135.0, "end": 7139.12, "text": " We then have to say as per usual how many activations do you want in your layers?", "tokens": [492, 550, 362, 281, 584, 382, 680, 7713, 577, 867, 2430, 763, 360, 291, 528, 294, 428, 7914, 30], "temperature": 0.0, "avg_logprob": -0.1668719542653937, "compression_ratio": 1.5041322314049588, "no_speech_prob": 1.6119913937018282e-07}, {"id": 1452, "seek": 713912, "start": 7139.12, "end": 7144.88, "text": " So we're going to use 500 and then how many layers do you want in your neural net? We're going to use three", "tokens": [407, 321, 434, 516, 281, 764, 5923, 293, 550, 577, 867, 7914, 360, 291, 528, 294, 428, 18161, 2533, 30, 492, 434, 516, 281, 764, 1045], "temperature": 0.0, "avg_logprob": -0.18399480537131982, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.222818006383022e-06}, {"id": 1453, "seek": 713912, "start": 7145.36, "end": 7147.36, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.18399480537131982, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.222818006383022e-06}, {"id": 1454, "seek": 713912, "start": 7148.16, "end": 7151.5199999999995, "text": " This is a minor technical detail it turns out that", "tokens": [639, 307, 257, 6696, 6191, 2607, 309, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.18399480537131982, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.222818006383022e-06}, {"id": 1455, "seek": 713912, "start": 7152.12, "end": 7154.44, "text": " We're going to learn later about the atom optimizer", "tokens": [492, 434, 516, 281, 1466, 1780, 466, 264, 12018, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.18399480537131982, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.222818006383022e-06}, {"id": 1456, "seek": 713912, "start": 7155.04, "end": 7158.72, "text": " That basically the defaults where it don't work very well with these kinds of models", "tokens": [663, 1936, 264, 7576, 82, 689, 309, 500, 380, 589, 588, 731, 365, 613, 3685, 295, 5245], "temperature": 0.0, "avg_logprob": -0.18399480537131982, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.222818006383022e-06}, {"id": 1457, "seek": 713912, "start": 7158.72, "end": 7164.68, "text": " So we just have to change some of these you know, basically any time you're doing NLP. You should probably", "tokens": [407, 321, 445, 362, 281, 1319, 512, 295, 613, 291, 458, 11, 1936, 604, 565, 291, 434, 884, 426, 45196, 13, 509, 820, 1391], "temperature": 0.0, "avg_logprob": -0.18399480537131982, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.222818006383022e-06}, {"id": 1458, "seek": 713912, "start": 7165.28, "end": 7167.28, "text": " include this line", "tokens": [4090, 341, 1622], "temperature": 0.0, "avg_logprob": -0.18399480537131982, "compression_ratio": 1.6037735849056605, "no_speech_prob": 4.222818006383022e-06}, {"id": 1459, "seek": 716728, "start": 7167.28, "end": 7169.28, "text": " It because it works pretty well", "tokens": [467, 570, 309, 1985, 1238, 731], "temperature": 0.0, "avg_logprob": -0.20821072838523172, "compression_ratio": 1.7061611374407584, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1460, "seek": 716728, "start": 7171.12, "end": 7176.5199999999995, "text": " So having done that we can now again take our model data object and grab a model out of it and", "tokens": [407, 1419, 1096, 300, 321, 393, 586, 797, 747, 527, 2316, 1412, 2657, 293, 4444, 257, 2316, 484, 295, 309, 293], "temperature": 0.0, "avg_logprob": -0.20821072838523172, "compression_ratio": 1.7061611374407584, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1461, "seek": 716728, "start": 7176.88, "end": 7178.88, "text": " We can pass in a few different things", "tokens": [492, 393, 1320, 294, 257, 1326, 819, 721], "temperature": 0.0, "avg_logprob": -0.20821072838523172, "compression_ratio": 1.7061611374407584, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1462, "seek": 716728, "start": 7179.44, "end": 7184.28, "text": " What optimization function do we want how big an embedding do we want how many hidden?", "tokens": [708, 19618, 2445, 360, 321, 528, 577, 955, 364, 12240, 3584, 360, 321, 528, 577, 867, 7633, 30], "temperature": 0.0, "avg_logprob": -0.20821072838523172, "compression_ratio": 1.7061611374407584, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1463, "seek": 716728, "start": 7184.5599999999995, "end": 7188.36, "text": " activate how many activations number of hidden how many layers and", "tokens": [13615, 577, 867, 2430, 763, 1230, 295, 7633, 577, 867, 7914, 293], "temperature": 0.0, "avg_logprob": -0.20821072838523172, "compression_ratio": 1.7061611374407584, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1464, "seek": 716728, "start": 7189.639999999999, "end": 7192.5199999999995, "text": " How much dropout of many different kinds?", "tokens": [1012, 709, 3270, 346, 295, 867, 819, 3685, 30], "temperature": 0.0, "avg_logprob": -0.20821072838523172, "compression_ratio": 1.7061611374407584, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1465, "seek": 719252, "start": 7192.52, "end": 7198.06, "text": " So this language model we're going to use is a very recent development called", "tokens": [407, 341, 2856, 2316, 321, 434, 516, 281, 764, 307, 257, 588, 5162, 3250, 1219], "temperature": 0.0, "avg_logprob": -0.22558798745413808, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.029430328955641e-06}, {"id": 1466, "seek": 719252, "start": 7198.4800000000005, "end": 7205.88, "text": " AWD LSTM by Stephen Meridy who's an LP researcher based in San Francisco and his main contribution", "tokens": [25815, 35, 441, 6840, 44, 538, 13391, 6124, 38836, 567, 311, 364, 38095, 21751, 2361, 294, 5271, 12279, 293, 702, 2135, 13150], "temperature": 0.0, "avg_logprob": -0.22558798745413808, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.029430328955641e-06}, {"id": 1467, "seek": 719252, "start": 7206.200000000001, "end": 7208.200000000001, "text": " really was to show like", "tokens": [534, 390, 281, 855, 411], "temperature": 0.0, "avg_logprob": -0.22558798745413808, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.029430328955641e-06}, {"id": 1468, "seek": 719252, "start": 7208.6, "end": 7213.360000000001, "text": " How to put dropout all over the place in in these NLP models", "tokens": [1012, 281, 829, 3270, 346, 439, 670, 264, 1081, 294, 294, 613, 426, 45196, 5245], "temperature": 0.0, "avg_logprob": -0.22558798745413808, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.029430328955641e-06}, {"id": 1469, "seek": 719252, "start": 7214.040000000001, "end": 7215.76, "text": " So we're not going to worry now", "tokens": [407, 321, 434, 406, 516, 281, 3292, 586], "temperature": 0.0, "avg_logprob": -0.22558798745413808, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.029430328955641e-06}, {"id": 1470, "seek": 719252, "start": 7215.76, "end": 7221.56, "text": " We'll do this in the last lecture is worrying about like what all that like what is the architecture and what are all these dropouts?", "tokens": [492, 603, 360, 341, 294, 264, 1036, 7991, 307, 18788, 466, 411, 437, 439, 300, 411, 437, 307, 264, 9482, 293, 437, 366, 439, 613, 3270, 7711, 30], "temperature": 0.0, "avg_logprob": -0.22558798745413808, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.029430328955641e-06}, {"id": 1471, "seek": 722156, "start": 7221.56, "end": 7228.320000000001, "text": " For now just know it's the same as per usual if you try to build an NLP model and your under fitting", "tokens": [1171, 586, 445, 458, 309, 311, 264, 912, 382, 680, 7713, 498, 291, 853, 281, 1322, 364, 426, 45196, 2316, 293, 428, 833, 15669], "temperature": 0.0, "avg_logprob": -0.15520109849817612, "compression_ratio": 1.7012448132780082, "no_speech_prob": 4.356852514320053e-06}, {"id": 1472, "seek": 722156, "start": 7228.52, "end": 7236.0, "text": " Then decrease all of these dropouts if you're over fitting then increase all of these dropouts in roughly this ratio", "tokens": [1396, 11514, 439, 295, 613, 3270, 7711, 498, 291, 434, 670, 15669, 550, 3488, 439, 295, 613, 3270, 7711, 294, 9810, 341, 8509], "temperature": 0.0, "avg_logprob": -0.15520109849817612, "compression_ratio": 1.7012448132780082, "no_speech_prob": 4.356852514320053e-06}, {"id": 1473, "seek": 722156, "start": 7236.6, "end": 7242.240000000001, "text": " Okay, that's that's my rule of thumb and it again. This is such a recent paper", "tokens": [1033, 11, 300, 311, 300, 311, 452, 4978, 295, 9298, 293, 309, 797, 13, 639, 307, 1270, 257, 5162, 3035], "temperature": 0.0, "avg_logprob": -0.15520109849817612, "compression_ratio": 1.7012448132780082, "no_speech_prob": 4.356852514320053e-06}, {"id": 1474, "seek": 722156, "start": 7243.68, "end": 7249.240000000001, "text": " Nobody else is working on this model anyway, so there's not a lot of guidance, but I found this these ratios work", "tokens": [9297, 1646, 307, 1364, 322, 341, 2316, 4033, 11, 370, 456, 311, 406, 257, 688, 295, 10056, 11, 457, 286, 1352, 341, 613, 32435, 589], "temperature": 0.0, "avg_logprob": -0.15520109849817612, "compression_ratio": 1.7012448132780082, "no_speech_prob": 4.356852514320053e-06}, {"id": 1475, "seek": 724924, "start": 7249.24, "end": 7251.48, "text": " Well, that's what Stephen's been using as well", "tokens": [1042, 11, 300, 311, 437, 13391, 311, 668, 1228, 382, 731], "temperature": 0.0, "avg_logprob": -0.14902352762746288, "compression_ratio": 1.6554621848739495, "no_speech_prob": 5.173883437237237e-06}, {"id": 1476, "seek": 724924, "start": 7253.36, "end": 7260.5199999999995, "text": " There's another kind of way we can avoid over fitting that we'll talk about in the last class again for now this one actually works", "tokens": [821, 311, 1071, 733, 295, 636, 321, 393, 5042, 670, 15669, 300, 321, 603, 751, 466, 294, 264, 1036, 1508, 797, 337, 586, 341, 472, 767, 1985], "temperature": 0.0, "avg_logprob": -0.14902352762746288, "compression_ratio": 1.6554621848739495, "no_speech_prob": 5.173883437237237e-06}, {"id": 1477, "seek": 724924, "start": 7260.599999999999, "end": 7265.599999999999, "text": " Totally reliably so all of your NLP models probably want this particular line of code", "tokens": [22837, 49927, 370, 439, 295, 428, 426, 45196, 5245, 1391, 528, 341, 1729, 1622, 295, 3089], "temperature": 0.0, "avg_logprob": -0.14902352762746288, "compression_ratio": 1.6554621848739495, "no_speech_prob": 5.173883437237237e-06}, {"id": 1478, "seek": 724924, "start": 7267.5199999999995, "end": 7274.719999999999, "text": " And then this one we're going to talk about at the end last lecture as well you can always include this basically what it says is", "tokens": [400, 550, 341, 472, 321, 434, 516, 281, 751, 466, 412, 264, 917, 1036, 7991, 382, 731, 291, 393, 1009, 4090, 341, 1936, 437, 309, 1619, 307], "temperature": 0.0, "avg_logprob": -0.14902352762746288, "compression_ratio": 1.6554621848739495, "no_speech_prob": 5.173883437237237e-06}, {"id": 1479, "seek": 727472, "start": 7274.72, "end": 7276.72, "text": " When you do", "tokens": [1133, 291, 360], "temperature": 0.0, "avg_logprob": -0.2733576472212629, "compression_ratio": 1.609375, "no_speech_prob": 1.844817688834155e-06}, {"id": 1480, "seek": 727472, "start": 7279.2, "end": 7285.780000000001, "text": " When you look at your gradients and you multiply them by the learning rate and you decide how much to update your weights by", "tokens": [1133, 291, 574, 412, 428, 2771, 2448, 293, 291, 12972, 552, 538, 264, 2539, 3314, 293, 291, 4536, 577, 709, 281, 5623, 428, 17443, 538], "temperature": 0.0, "avg_logprob": -0.2733576472212629, "compression_ratio": 1.609375, "no_speech_prob": 1.844817688834155e-06}, {"id": 1481, "seek": 727472, "start": 7286.56, "end": 7288.6, "text": " This says clip them", "tokens": [639, 1619, 7353, 552], "temperature": 0.0, "avg_logprob": -0.2733576472212629, "compression_ratio": 1.609375, "no_speech_prob": 1.844817688834155e-06}, {"id": 1482, "seek": 727472, "start": 7289.360000000001, "end": 7291.16, "text": " like literally like", "tokens": [411, 3736, 411], "temperature": 0.0, "avg_logprob": -0.2733576472212629, "compression_ratio": 1.609375, "no_speech_prob": 1.844817688834155e-06}, {"id": 1483, "seek": 727472, "start": 7291.16, "end": 7293.16, "text": " Like don't let them be more than", "tokens": [1743, 500, 380, 718, 552, 312, 544, 813], "temperature": 0.0, "avg_logprob": -0.2733576472212629, "compression_ratio": 1.609375, "no_speech_prob": 1.844817688834155e-06}, {"id": 1484, "seek": 727472, "start": 7294.240000000001, "end": 7295.56, "text": " 0.3", "tokens": [1958, 13, 18], "temperature": 0.0, "avg_logprob": -0.2733576472212629, "compression_ratio": 1.609375, "no_speech_prob": 1.844817688834155e-06}, {"id": 1485, "seek": 727472, "start": 7295.56, "end": 7299.56, "text": " and this is quite a cool little trick right because like", "tokens": [293, 341, 307, 1596, 257, 1627, 707, 4282, 558, 570, 411], "temperature": 0.0, "avg_logprob": -0.2733576472212629, "compression_ratio": 1.609375, "no_speech_prob": 1.844817688834155e-06}, {"id": 1486, "seek": 727472, "start": 7301.0, "end": 7303.84, "text": " if your learning rates pretty high and", "tokens": [498, 428, 2539, 6846, 1238, 1090, 293], "temperature": 0.0, "avg_logprob": -0.2733576472212629, "compression_ratio": 1.609375, "no_speech_prob": 1.844817688834155e-06}, {"id": 1487, "seek": 730384, "start": 7303.84, "end": 7306.2, "text": " And you kind of don't want to get in that situation", "tokens": [400, 291, 733, 295, 500, 380, 528, 281, 483, 294, 300, 2590], "temperature": 0.0, "avg_logprob": -0.21377037510727392, "compression_ratio": 1.8, "no_speech_prob": 1.6797245052657672e-06}, {"id": 1488, "seek": 730384, "start": 7306.76, "end": 7311.8, "text": " We talked about where you're kind of got this kind of thing where you go", "tokens": [492, 2825, 466, 689, 291, 434, 733, 295, 658, 341, 733, 295, 551, 689, 291, 352], "temperature": 0.0, "avg_logprob": -0.21377037510727392, "compression_ratio": 1.8, "no_speech_prob": 1.6797245052657672e-06}, {"id": 1489, "seek": 730384, "start": 7314.08, "end": 7320.78, "text": " You know rather than little step little step little step instead you go like oh too big oh too big right with", "tokens": [509, 458, 2831, 813, 707, 1823, 707, 1823, 707, 1823, 2602, 291, 352, 411, 1954, 886, 955, 1954, 886, 955, 558, 365], "temperature": 0.0, "avg_logprob": -0.21377037510727392, "compression_ratio": 1.8, "no_speech_prob": 1.6797245052657672e-06}, {"id": 1490, "seek": 730384, "start": 7320.84, "end": 7325.900000000001, "text": " Gradient clipping it kind of goes this far and it's like oh my goodness. I'm going too far. I'll stop", "tokens": [16710, 1196, 49320, 309, 733, 295, 1709, 341, 1400, 293, 309, 311, 411, 1954, 452, 8387, 13, 286, 478, 516, 886, 1400, 13, 286, 603, 1590], "temperature": 0.0, "avg_logprob": -0.21377037510727392, "compression_ratio": 1.8, "no_speech_prob": 1.6797245052657672e-06}, {"id": 1491, "seek": 730384, "start": 7326.68, "end": 7328.88, "text": " And that's basically what gradient looking does", "tokens": [400, 300, 311, 1936, 437, 16235, 1237, 775], "temperature": 0.0, "avg_logprob": -0.21377037510727392, "compression_ratio": 1.8, "no_speech_prob": 1.6797245052657672e-06}, {"id": 1492, "seek": 730384, "start": 7330.04, "end": 7331.96, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.21377037510727392, "compression_ratio": 1.8, "no_speech_prob": 1.6797245052657672e-06}, {"id": 1493, "seek": 733196, "start": 7331.96, "end": 7337.0, "text": " Anyway, so these are a bunch of parameters the details don't matter too much right now. You can just deal these", "tokens": [5684, 11, 370, 613, 366, 257, 3840, 295, 9834, 264, 4365, 500, 380, 1871, 886, 709, 558, 586, 13, 509, 393, 445, 2028, 613], "temperature": 0.0, "avg_logprob": -0.28227438245500835, "compression_ratio": 1.4719101123595506, "no_speech_prob": 2.368767809457495e-06}, {"id": 1494, "seek": 733196, "start": 7338.32, "end": 7340.32, "text": " And then we can go ahead and call", "tokens": [400, 550, 321, 393, 352, 2286, 293, 818], "temperature": 0.0, "avg_logprob": -0.28227438245500835, "compression_ratio": 1.4719101123595506, "no_speech_prob": 2.368767809457495e-06}, {"id": 1495, "seek": 733196, "start": 7341.0, "end": 7342.12, "text": " fit", "tokens": [3318], "temperature": 0.0, "avg_logprob": -0.28227438245500835, "compression_ratio": 1.4719101123595506, "no_speech_prob": 2.368767809457495e-06}, {"id": 1496, "seek": 733196, "start": 7342.12, "end": 7344.5, "text": " With exactly the same parameters as usual", "tokens": [2022, 2293, 264, 912, 9834, 382, 7713], "temperature": 0.0, "avg_logprob": -0.28227438245500835, "compression_ratio": 1.4719101123595506, "no_speech_prob": 2.368767809457495e-06}, {"id": 1497, "seek": 733196, "start": 7349.32, "end": 7352.96, "text": " So Jeremy, um, they're all these older", "tokens": [407, 17809, 11, 1105, 11, 436, 434, 439, 613, 4906], "temperature": 0.0, "avg_logprob": -0.28227438245500835, "compression_ratio": 1.4719101123595506, "no_speech_prob": 2.368767809457495e-06}, {"id": 1498, "seek": 733196, "start": 7354.2, "end": 7356.44, "text": " work embedding things like like", "tokens": [589, 12240, 3584, 721, 411, 411], "temperature": 0.0, "avg_logprob": -0.28227438245500835, "compression_ratio": 1.4719101123595506, "no_speech_prob": 2.368767809457495e-06}, {"id": 1499, "seek": 735644, "start": 7356.44, "end": 7361.879999999999, "text": " Word to vague and globe. So I have two questions about that one is", "tokens": [8725, 281, 24247, 293, 15371, 13, 407, 286, 362, 732, 1651, 466, 300, 472, 307], "temperature": 0.0, "avg_logprob": -0.18349904453053195, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.5779511386426748e-06}, {"id": 1500, "seek": 735644, "start": 7362.599999999999, "end": 7370.919999999999, "text": " How are those different from these and the second question? Why don't you initialize them with one of those? Yeah, so", "tokens": [1012, 366, 729, 819, 490, 613, 293, 264, 1150, 1168, 30, 1545, 500, 380, 291, 5883, 1125, 552, 365, 472, 295, 729, 30, 865, 11, 370], "temperature": 0.0, "avg_logprob": -0.18349904453053195, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.5779511386426748e-06}, {"id": 1501, "seek": 735644, "start": 7372.12, "end": 7374.5599999999995, "text": " So basically that's a great question. So basically", "tokens": [407, 1936, 300, 311, 257, 869, 1168, 13, 407, 1936], "temperature": 0.0, "avg_logprob": -0.18349904453053195, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.5779511386426748e-06}, {"id": 1502, "seek": 735644, "start": 7375.839999999999, "end": 7377.839999999999, "text": " People have pre trained", "tokens": [3432, 362, 659, 8895], "temperature": 0.0, "avg_logprob": -0.18349904453053195, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.5779511386426748e-06}, {"id": 1503, "seek": 735644, "start": 7378.36, "end": 7383.879999999999, "text": " These embedding matrices before to do various other tasks. They're not whole pre trained models", "tokens": [1981, 12240, 3584, 32284, 949, 281, 360, 3683, 661, 9608, 13, 814, 434, 406, 1379, 659, 8895, 5245], "temperature": 0.0, "avg_logprob": -0.18349904453053195, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.5779511386426748e-06}, {"id": 1504, "seek": 738388, "start": 7383.88, "end": 7388.36, "text": " They're just a pre trained embedding matrix and you can download them and as you know", "tokens": [814, 434, 445, 257, 659, 8895, 12240, 3584, 8141, 293, 291, 393, 5484, 552, 293, 382, 291, 458], "temperature": 0.0, "avg_logprob": -0.19473325578789963, "compression_ratio": 1.6486486486486487, "no_speech_prob": 2.2603164779866347e-06}, {"id": 1505, "seek": 738388, "start": 7388.36, "end": 7392.28, "text": " It says they have names like word to vec and love and they're literally just a matrix", "tokens": [467, 1619, 436, 362, 5288, 411, 1349, 281, 42021, 293, 959, 293, 436, 434, 3736, 445, 257, 8141], "temperature": 0.0, "avg_logprob": -0.19473325578789963, "compression_ratio": 1.6486486486486487, "no_speech_prob": 2.2603164779866347e-06}, {"id": 1506, "seek": 738388, "start": 7394.84, "end": 7401.16, "text": " There's no reason we couldn't download them really it's just like kind of I", "tokens": [821, 311, 572, 1778, 321, 2809, 380, 5484, 552, 534, 309, 311, 445, 411, 733, 295, 286], "temperature": 0.0, "avg_logprob": -0.19473325578789963, "compression_ratio": 1.6486486486486487, "no_speech_prob": 2.2603164779866347e-06}, {"id": 1507, "seek": 738388, "start": 7404.04, "end": 7405.88, "text": " Found that", "tokens": [8207, 300], "temperature": 0.0, "avg_logprob": -0.19473325578789963, "compression_ratio": 1.6486486486486487, "no_speech_prob": 2.2603164779866347e-06}, {"id": 1508, "seek": 738388, "start": 7405.88, "end": 7408.24, "text": " Building a whole pre trained model in this way", "tokens": [18974, 257, 1379, 659, 8895, 2316, 294, 341, 636], "temperature": 0.0, "avg_logprob": -0.19473325578789963, "compression_ratio": 1.6486486486486487, "no_speech_prob": 2.2603164779866347e-06}, {"id": 1509, "seek": 740824, "start": 7408.24, "end": 7415.12, "text": " Didn't seem to benefit much if at all from using pre trained word vectors where else using a whole pre trained language model", "tokens": [11151, 380, 1643, 281, 5121, 709, 498, 412, 439, 490, 1228, 659, 8895, 1349, 18875, 689, 1646, 1228, 257, 1379, 659, 8895, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.1885689384058902, "compression_ratio": 1.68, "no_speech_prob": 4.785050350619713e-06}, {"id": 1510, "seek": 740824, "start": 7415.719999999999, "end": 7420.719999999999, "text": " Made it much bigger difference. So like you remember what a big those of you who saw word to vec", "tokens": [18330, 309, 709, 3801, 2649, 13, 407, 411, 291, 1604, 437, 257, 955, 729, 295, 291, 567, 1866, 1349, 281, 42021], "temperature": 0.0, "avg_logprob": -0.1885689384058902, "compression_ratio": 1.68, "no_speech_prob": 4.785050350619713e-06}, {"id": 1511, "seek": 740824, "start": 7420.719999999999, "end": 7422.8, "text": " It made a big splash when it came out", "tokens": [467, 1027, 257, 955, 25757, 562, 309, 1361, 484], "temperature": 0.0, "avg_logprob": -0.1885689384058902, "compression_ratio": 1.68, "no_speech_prob": 4.785050350619713e-06}, {"id": 1512, "seek": 740824, "start": 7423.12, "end": 7430.24, "text": " I'm finding this technique of pre trained language models seems much more powerful basically", "tokens": [286, 478, 5006, 341, 6532, 295, 659, 8895, 2856, 5245, 2544, 709, 544, 4005, 1936], "temperature": 0.0, "avg_logprob": -0.1885689384058902, "compression_ratio": 1.68, "no_speech_prob": 4.785050350619713e-06}, {"id": 1513, "seek": 740824, "start": 7430.24, "end": 7433.639999999999, "text": " But I think we can combine both to make them a little better still", "tokens": [583, 286, 519, 321, 393, 10432, 1293, 281, 652, 552, 257, 707, 1101, 920], "temperature": 0.0, "avg_logprob": -0.1885689384058902, "compression_ratio": 1.68, "no_speech_prob": 4.785050350619713e-06}, {"id": 1514, "seek": 743364, "start": 7433.64, "end": 7440.0, "text": " And what is the model that you have used like how can I know the architecture of the model?", "tokens": [400, 437, 307, 264, 2316, 300, 291, 362, 1143, 411, 577, 393, 286, 458, 264, 9482, 295, 264, 2316, 30], "temperature": 0.0, "avg_logprob": -0.2805123567581177, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.2606646123458631e-05}, {"id": 1515, "seek": 743364, "start": 7441.52, "end": 7448.0, "text": " So we'll be learning about the model architecture in the last lesson for now, it's a recurrent neural network", "tokens": [407, 321, 603, 312, 2539, 466, 264, 2316, 9482, 294, 264, 1036, 6898, 337, 586, 11, 309, 311, 257, 18680, 1753, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.2805123567581177, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.2606646123458631e-05}, {"id": 1516, "seek": 743364, "start": 7448.6, "end": 7451.62, "text": " Using some LSTM long short-term memory", "tokens": [11142, 512, 441, 6840, 44, 938, 2099, 12, 7039, 4675], "temperature": 0.0, "avg_logprob": -0.2805123567581177, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.2606646123458631e-05}, {"id": 1517, "seek": 743364, "start": 7454.200000000001, "end": 7456.200000000001, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2805123567581177, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.2606646123458631e-05}, {"id": 1518, "seek": 743364, "start": 7456.240000000001, "end": 7457.76, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2805123567581177, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.2606646123458631e-05}, {"id": 1519, "seek": 743364, "start": 7457.76, "end": 7460.160000000001, "text": " So we had lots of details that we're skipping over", "tokens": [407, 321, 632, 3195, 295, 4365, 300, 321, 434, 31533, 670], "temperature": 0.0, "avg_logprob": -0.2805123567581177, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.2606646123458631e-05}, {"id": 1520, "seek": 746016, "start": 7460.16, "end": 7465.58, "text": " But you know you can do all this without any of those details we go ahead and fit the model", "tokens": [583, 291, 458, 291, 393, 360, 439, 341, 1553, 604, 295, 729, 4365, 321, 352, 2286, 293, 3318, 264, 2316], "temperature": 0.0, "avg_logprob": -0.15670364185915156, "compression_ratio": 1.8188976377952757, "no_speech_prob": 1.1478630767669529e-05}, {"id": 1521, "seek": 746016, "start": 7465.96, "end": 7471.28, "text": " I found that this language model took quite a while to fit so I kind of like ran it for a while", "tokens": [286, 1352, 300, 341, 2856, 2316, 1890, 1596, 257, 1339, 281, 3318, 370, 286, 733, 295, 411, 5872, 309, 337, 257, 1339], "temperature": 0.0, "avg_logprob": -0.15670364185915156, "compression_ratio": 1.8188976377952757, "no_speech_prob": 1.1478630767669529e-05}, {"id": 1522, "seek": 746016, "start": 7471.8, "end": 7474.88, "text": " Noticed it was still under fitting safe where it was up to", "tokens": [1726, 4233, 309, 390, 920, 833, 15669, 3273, 689, 309, 390, 493, 281], "temperature": 0.0, "avg_logprob": -0.15670364185915156, "compression_ratio": 1.8188976377952757, "no_speech_prob": 1.1478630767669529e-05}, {"id": 1523, "seek": 746016, "start": 7475.5199999999995, "end": 7479.5199999999995, "text": " Ran it a bit more with longer cycle length saved it again. It still", "tokens": [27948, 309, 257, 857, 544, 365, 2854, 6586, 4641, 6624, 309, 797, 13, 467, 920], "temperature": 0.0, "avg_logprob": -0.15670364185915156, "compression_ratio": 1.8188976377952757, "no_speech_prob": 1.1478630767669529e-05}, {"id": 1524, "seek": 746016, "start": 7480.2, "end": 7482.2, "text": " Was kind of under fitting", "tokens": [3027, 733, 295, 833, 15669], "temperature": 0.0, "avg_logprob": -0.15670364185915156, "compression_ratio": 1.8188976377952757, "no_speech_prob": 1.1478630767669529e-05}, {"id": 1525, "seek": 746016, "start": 7482.2, "end": 7487.88, "text": " You know run it again and kind of finally got to the point where it's like kind of honestly I kind of ran out of", "tokens": [509, 458, 1190, 309, 797, 293, 733, 295, 2721, 658, 281, 264, 935, 689, 309, 311, 411, 733, 295, 6095, 286, 733, 295, 5872, 484, 295], "temperature": 0.0, "avg_logprob": -0.15670364185915156, "compression_ratio": 1.8188976377952757, "no_speech_prob": 1.1478630767669529e-05}, {"id": 1526, "seek": 746016, "start": 7487.88, "end": 7489.0, "text": " patience", "tokens": [14826], "temperature": 0.0, "avg_logprob": -0.15670364185915156, "compression_ratio": 1.8188976377952757, "no_speech_prob": 1.1478630767669529e-05}, {"id": 1527, "seek": 748900, "start": 7489.0, "end": 7491.64, "text": " So I just like saved it at that point", "tokens": [407, 286, 445, 411, 6624, 309, 412, 300, 935], "temperature": 0.0, "avg_logprob": -0.22026066626271895, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.3845818102709018e-05}, {"id": 1528, "seek": 748900, "start": 7492.72, "end": 7497.28, "text": " And I did the same kind of test that we looked at before so I was like", "tokens": [400, 286, 630, 264, 912, 733, 295, 1500, 300, 321, 2956, 412, 949, 370, 286, 390, 411], "temperature": 0.0, "avg_logprob": -0.22026066626271895, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.3845818102709018e-05}, {"id": 1529, "seek": 748900, "start": 7497.28, "end": 7501.08, "text": " Oh, it wasn't quite what I was expecting, but I realized anyway the best and then I was like, okay", "tokens": [876, 11, 309, 2067, 380, 1596, 437, 286, 390, 9650, 11, 457, 286, 5334, 4033, 264, 1151, 293, 550, 286, 390, 411, 11, 1392], "temperature": 0.0, "avg_logprob": -0.22026066626271895, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.3845818102709018e-05}, {"id": 1530, "seek": 748900, "start": 7501.08, "end": 7504.04, "text": " Let's see how that goes the best performance was one of the movie was a little bit", "tokens": [961, 311, 536, 577, 300, 1709, 264, 1151, 3389, 390, 472, 295, 264, 3169, 390, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.22026066626271895, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.3845818102709018e-05}, {"id": 1531, "seek": 748900, "start": 7504.04, "end": 7507.36, "text": " Oh, I say, okay. It looks like the language models working pretty well", "tokens": [876, 11, 286, 584, 11, 1392, 13, 467, 1542, 411, 264, 2856, 5245, 1364, 1238, 731], "temperature": 0.0, "avg_logprob": -0.22026066626271895, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.3845818102709018e-05}, {"id": 1532, "seek": 748900, "start": 7509.2, "end": 7511.8, "text": " So I've pre trained the language model", "tokens": [407, 286, 600, 659, 8895, 264, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.22026066626271895, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.3845818102709018e-05}, {"id": 1533, "seek": 748900, "start": 7512.96, "end": 7514.96, "text": " And so now I want to use it", "tokens": [400, 370, 586, 286, 528, 281, 764, 309], "temperature": 0.0, "avg_logprob": -0.22026066626271895, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.3845818102709018e-05}, {"id": 1534, "seek": 748900, "start": 7515.64, "end": 7517.64, "text": " Fine tune it to do classification", "tokens": [12024, 10864, 309, 281, 360, 21538], "temperature": 0.0, "avg_logprob": -0.22026066626271895, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.3845818102709018e-05}, {"id": 1535, "seek": 751764, "start": 7517.64, "end": 7523.64, "text": " Sentiment classification now, obviously if I'm going to use a pre-trained model, I need to use exactly the same vocab", "tokens": [23652, 2328, 21538, 586, 11, 2745, 498, 286, 478, 516, 281, 764, 257, 659, 12, 17227, 2001, 2316, 11, 286, 643, 281, 764, 2293, 264, 912, 2329, 455], "temperature": 0.0, "avg_logprob": -0.20227937878302807, "compression_ratio": 1.5875486381322956, "no_speech_prob": 5.1738638831011485e-06}, {"id": 1536, "seek": 751764, "start": 7523.84, "end": 7531.0, "text": " But the word the still needs to map to the number two so that I can look up the vector for that", "tokens": [583, 264, 1349, 264, 920, 2203, 281, 4471, 281, 264, 1230, 732, 370, 300, 286, 393, 574, 493, 264, 8062, 337, 300], "temperature": 0.0, "avg_logprob": -0.20227937878302807, "compression_ratio": 1.5875486381322956, "no_speech_prob": 5.1738638831011485e-06}, {"id": 1537, "seek": 751764, "start": 7531.240000000001, "end": 7535.280000000001, "text": " Right, so that's why I first of all load back up", "tokens": [1779, 11, 370, 300, 311, 983, 286, 700, 295, 439, 3677, 646, 493], "temperature": 0.0, "avg_logprob": -0.20227937878302807, "compression_ratio": 1.5875486381322956, "no_speech_prob": 5.1738638831011485e-06}, {"id": 1538, "seek": 751764, "start": 7535.64, "end": 7542.8, "text": " My my field object the thing with the vocab in right now in this case if I run it straight afterwards", "tokens": [1222, 452, 2519, 2657, 264, 551, 365, 264, 2329, 455, 294, 558, 586, 294, 341, 1389, 498, 286, 1190, 309, 2997, 10543], "temperature": 0.0, "avg_logprob": -0.20227937878302807, "compression_ratio": 1.5875486381322956, "no_speech_prob": 5.1738638831011485e-06}, {"id": 1539, "seek": 751764, "start": 7542.8, "end": 7545.12, "text": " This is unnecessary. It's already in memory", "tokens": [639, 307, 19350, 13, 467, 311, 1217, 294, 4675], "temperature": 0.0, "avg_logprob": -0.20227937878302807, "compression_ratio": 1.5875486381322956, "no_speech_prob": 5.1738638831011485e-06}, {"id": 1540, "seek": 754512, "start": 7545.12, "end": 7550.88, "text": " But this means I can come back to this later right in a new session basically", "tokens": [583, 341, 1355, 286, 393, 808, 646, 281, 341, 1780, 558, 294, 257, 777, 5481, 1936], "temperature": 0.0, "avg_logprob": -0.24104857712649227, "compression_ratio": 1.6435185185185186, "no_speech_prob": 2.443984612909844e-06}, {"id": 1541, "seek": 754512, "start": 7553.64, "end": 7559.0, "text": " I can then go ahead and say okay. I've never got one more field right in addition to my", "tokens": [286, 393, 550, 352, 2286, 293, 584, 1392, 13, 286, 600, 1128, 658, 472, 544, 2519, 558, 294, 4500, 281, 452], "temperature": 0.0, "avg_logprob": -0.24104857712649227, "compression_ratio": 1.6435185185185186, "no_speech_prob": 2.443984612909844e-06}, {"id": 1542, "seek": 754512, "start": 7559.28, "end": 7564.0, "text": " Field which represents the reviews. I've also got a field which represents the label", "tokens": [17952, 597, 8855, 264, 10229, 13, 286, 600, 611, 658, 257, 2519, 597, 8855, 264, 7645], "temperature": 0.0, "avg_logprob": -0.24104857712649227, "compression_ratio": 1.6435185185185186, "no_speech_prob": 2.443984612909844e-06}, {"id": 1543, "seek": 754512, "start": 7564.5199999999995, "end": 7565.76, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.24104857712649227, "compression_ratio": 1.6435185185185186, "no_speech_prob": 2.443984612909844e-06}, {"id": 1544, "seek": 754512, "start": 7565.76, "end": 7568.32, "text": " And the details are too important here", "tokens": [400, 264, 4365, 366, 886, 1021, 510], "temperature": 0.0, "avg_logprob": -0.24104857712649227, "compression_ratio": 1.6435185185185186, "no_speech_prob": 2.443984612909844e-06}, {"id": 1545, "seek": 754512, "start": 7569.08, "end": 7574.16, "text": " Now this time I need to not treat the whole thing as one big", "tokens": [823, 341, 565, 286, 643, 281, 406, 2387, 264, 1379, 551, 382, 472, 955], "temperature": 0.0, "avg_logprob": -0.24104857712649227, "compression_ratio": 1.6435185185185186, "no_speech_prob": 2.443984612909844e-06}, {"id": 1546, "seek": 757416, "start": 7574.16, "end": 7580.4, "text": " Piece of text but every review is separate because each one has a different sentiment attached to it", "tokens": [42868, 295, 2487, 457, 633, 3131, 307, 4994, 570, 1184, 472, 575, 257, 819, 16149, 8570, 281, 309], "temperature": 0.0, "avg_logprob": -0.22376484913868946, "compression_ratio": 1.6263345195729537, "no_speech_prob": 1.6187401342904195e-05}, {"id": 1547, "seek": 757416, "start": 7580.4, "end": 7587.32, "text": " And it so happens that torch text already has a data set that does that for IMDB. So I just used IMDB", "tokens": [400, 309, 370, 2314, 300, 27822, 2487, 1217, 575, 257, 1412, 992, 300, 775, 300, 337, 21463, 27735, 13, 407, 286, 445, 1143, 21463, 27735], "temperature": 0.0, "avg_logprob": -0.22376484913868946, "compression_ratio": 1.6263345195729537, "no_speech_prob": 1.6187401342904195e-05}, {"id": 1548, "seek": 757416, "start": 7588.16, "end": 7590.16, "text": " built into torch text", "tokens": [3094, 666, 27822, 2487], "temperature": 0.0, "avg_logprob": -0.22376484913868946, "compression_ratio": 1.6263345195729537, "no_speech_prob": 1.6187401342904195e-05}, {"id": 1549, "seek": 757416, "start": 7590.4, "end": 7596.12, "text": " So basically once we've done all that we end up with something where we can like grab for a particular example", "tokens": [407, 1936, 1564, 321, 600, 1096, 439, 300, 321, 917, 493, 365, 746, 689, 321, 393, 411, 4444, 337, 257, 1729, 1365], "temperature": 0.0, "avg_logprob": -0.22376484913868946, "compression_ratio": 1.6263345195729537, "no_speech_prob": 1.6187401342904195e-05}, {"id": 1550, "seek": 757416, "start": 7596.72, "end": 7598.639999999999, "text": " We can grab its label", "tokens": [492, 393, 4444, 1080, 7645], "temperature": 0.0, "avg_logprob": -0.22376484913868946, "compression_ratio": 1.6263345195729537, "no_speech_prob": 1.6187401342904195e-05}, {"id": 1551, "seek": 757416, "start": 7598.639999999999, "end": 7600.04, "text": " positive and", "tokens": [3353, 293], "temperature": 0.0, "avg_logprob": -0.22376484913868946, "compression_ratio": 1.6263345195729537, "no_speech_prob": 1.6187401342904195e-05}, {"id": 1552, "seek": 760004, "start": 7600.04, "end": 7605.24, "text": " Here's some of the text. This is another great Tom Berendon movie blah blah blah blah. All right, so", "tokens": [1692, 311, 512, 295, 264, 2487, 13, 639, 307, 1071, 869, 5041, 5637, 521, 266, 3169, 12288, 12288, 12288, 12288, 13, 1057, 558, 11, 370], "temperature": 0.0, "avg_logprob": -0.24131738662719726, "compression_ratio": 1.611336032388664, "no_speech_prob": 1.5936358977342024e-05}, {"id": 1553, "seek": 760004, "start": 7606.48, "end": 7611.16, "text": " This is all not nothing faster. I specific here. We'll come back to it in the last lecture", "tokens": [639, 307, 439, 406, 1825, 4663, 13, 286, 2685, 510, 13, 492, 603, 808, 646, 281, 309, 294, 264, 1036, 7991], "temperature": 0.0, "avg_logprob": -0.24131738662719726, "compression_ratio": 1.611336032388664, "no_speech_prob": 1.5936358977342024e-05}, {"id": 1554, "seek": 760004, "start": 7611.72, "end": 7616.44, "text": " But torch text docs can help understand what's going on. All you need to know is that", "tokens": [583, 27822, 2487, 45623, 393, 854, 1223, 437, 311, 516, 322, 13, 1057, 291, 643, 281, 458, 307, 300], "temperature": 0.0, "avg_logprob": -0.24131738662719726, "compression_ratio": 1.611336032388664, "no_speech_prob": 1.5936358977342024e-05}, {"id": 1555, "seek": 760004, "start": 7617.12, "end": 7622.86, "text": " Once you've used this special talks torch text thing called splits to grab a splits object", "tokens": [3443, 291, 600, 1143, 341, 2121, 6686, 27822, 2487, 551, 1219, 37741, 281, 4444, 257, 37741, 2657], "temperature": 0.0, "avg_logprob": -0.24131738662719726, "compression_ratio": 1.611336032388664, "no_speech_prob": 1.5936358977342024e-05}, {"id": 1556, "seek": 760004, "start": 7622.88, "end": 7624.88, "text": " You can pass it straight into", "tokens": [509, 393, 1320, 309, 2997, 666], "temperature": 0.0, "avg_logprob": -0.24131738662719726, "compression_ratio": 1.611336032388664, "no_speech_prob": 1.5936358977342024e-05}, {"id": 1557, "seek": 762488, "start": 7624.88, "end": 7630.16, "text": " Fast AI text data from splits and that basically converts a torch text", "tokens": [15968, 7318, 2487, 1412, 490, 37741, 293, 300, 1936, 38874, 257, 27822, 2487], "temperature": 0.0, "avg_logprob": -0.14190913818694734, "compression_ratio": 1.7673267326732673, "no_speech_prob": 4.4951589188713115e-06}, {"id": 1558, "seek": 762488, "start": 7630.84, "end": 7637.52, "text": " Object into a fast AI object we can train on so as soon as you've done that you can just go ahead and say", "tokens": [24753, 666, 257, 2370, 7318, 2657, 321, 393, 3847, 322, 370, 382, 2321, 382, 291, 600, 1096, 300, 291, 393, 445, 352, 2286, 293, 584], "temperature": 0.0, "avg_logprob": -0.14190913818694734, "compression_ratio": 1.7673267326732673, "no_speech_prob": 4.4951589188713115e-06}, {"id": 1559, "seek": 762488, "start": 7638.24, "end": 7640.72, "text": " Get model right and that gets us our learner", "tokens": [3240, 2316, 558, 293, 300, 2170, 505, 527, 33347], "temperature": 0.0, "avg_logprob": -0.14190913818694734, "compression_ratio": 1.7673267326732673, "no_speech_prob": 4.4951589188713115e-06}, {"id": 1560, "seek": 762488, "start": 7641.76, "end": 7646.88, "text": " And then we can load into it the pre-trained model the language model", "tokens": [400, 550, 321, 393, 3677, 666, 309, 264, 659, 12, 17227, 2001, 2316, 264, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.14190913818694734, "compression_ratio": 1.7673267326732673, "no_speech_prob": 4.4951589188713115e-06}, {"id": 1561, "seek": 762488, "start": 7647.4800000000005, "end": 7650.96, "text": " right, and so we can now take that pre-trained language model and", "tokens": [558, 11, 293, 370, 321, 393, 586, 747, 300, 659, 12, 17227, 2001, 2856, 2316, 293], "temperature": 0.0, "avg_logprob": -0.14190913818694734, "compression_ratio": 1.7673267326732673, "no_speech_prob": 4.4951589188713115e-06}, {"id": 1562, "seek": 765096, "start": 7650.96, "end": 7655.32, "text": " And use the stuff that we're kind of familiar with right so we can", "tokens": [400, 764, 264, 1507, 300, 321, 434, 733, 295, 4963, 365, 558, 370, 321, 393], "temperature": 0.0, "avg_logprob": -0.20624670548872515, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.857304252756876e-06}, {"id": 1563, "seek": 765096, "start": 7656.04, "end": 7660.16, "text": " Make sure that you know all it's at the last layer is frozen train it a bit", "tokens": [4387, 988, 300, 291, 458, 439, 309, 311, 412, 264, 1036, 4583, 307, 12496, 3847, 309, 257, 857], "temperature": 0.0, "avg_logprob": -0.20624670548872515, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.857304252756876e-06}, {"id": 1564, "seek": 765096, "start": 7660.64, "end": 7665.36, "text": " Unfreeze it train it a bit and the nice thing is once you've got a pre-trained", "tokens": [8170, 701, 1381, 309, 3847, 309, 257, 857, 293, 264, 1481, 551, 307, 1564, 291, 600, 658, 257, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.20624670548872515, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.857304252756876e-06}, {"id": 1565, "seek": 765096, "start": 7666.08, "end": 7670.4, "text": " Language model it actually trains super fast. You can see here. It's like a couple of minutes", "tokens": [24445, 2316, 309, 767, 16329, 1687, 2370, 13, 509, 393, 536, 510, 13, 467, 311, 411, 257, 1916, 295, 2077], "temperature": 0.0, "avg_logprob": -0.20624670548872515, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.857304252756876e-06}, {"id": 1566, "seek": 765096, "start": 7671.28, "end": 7677.92, "text": " Epoch and it only took me to get my is my best one here already took me like 10 epochs", "tokens": [462, 2259, 339, 293, 309, 787, 1890, 385, 281, 483, 452, 307, 452, 1151, 472, 510, 1217, 1890, 385, 411, 1266, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.20624670548872515, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.857304252756876e-06}, {"id": 1567, "seek": 767792, "start": 7677.92, "end": 7681.0, "text": " So it's like 20 minutes to train this bit. It's really fast", "tokens": [407, 309, 311, 411, 945, 2077, 281, 3847, 341, 857, 13, 467, 311, 534, 2370], "temperature": 0.0, "avg_logprob": -0.246207994835399, "compression_ratio": 1.5423076923076924, "no_speech_prob": 6.144136932562105e-06}, {"id": 1568, "seek": 767792, "start": 7681.92, "end": 7683.92, "text": " And I ended up with", "tokens": [400, 286, 4590, 493, 365], "temperature": 0.0, "avg_logprob": -0.246207994835399, "compression_ratio": 1.5423076923076924, "no_speech_prob": 6.144136932562105e-06}, {"id": 1569, "seek": 767792, "start": 7684.08, "end": 7686.08, "text": " 94 point five percent", "tokens": [30849, 935, 1732, 3043], "temperature": 0.0, "avg_logprob": -0.246207994835399, "compression_ratio": 1.5423076923076924, "no_speech_prob": 6.144136932562105e-06}, {"id": 1570, "seek": 767792, "start": 7686.68, "end": 7691.36, "text": " So how good is ninety four point five percent well it so happens that?", "tokens": [407, 577, 665, 307, 25063, 1451, 935, 1732, 3043, 731, 309, 370, 2314, 300, 30], "temperature": 0.0, "avg_logprob": -0.246207994835399, "compression_ratio": 1.5423076923076924, "no_speech_prob": 6.144136932562105e-06}, {"id": 1571, "seek": 767792, "start": 7693.0, "end": 7697.2, "text": " Actually one of Steve and Merida's colleagues James Bradbury recently created a paper", "tokens": [5135, 472, 295, 7466, 293, 6124, 2887, 311, 7734, 5678, 11895, 22536, 3938, 2942, 257, 3035], "temperature": 0.0, "avg_logprob": -0.246207994835399, "compression_ratio": 1.5423076923076924, "no_speech_prob": 6.144136932562105e-06}, {"id": 1572, "seek": 767792, "start": 7700.52, "end": 7706.0, "text": " Looking at the state of like where they tried to create a new state-of-the-art for a bunch of NLP things and one of the things", "tokens": [11053, 412, 264, 1785, 295, 411, 689, 436, 3031, 281, 1884, 257, 777, 1785, 12, 2670, 12, 3322, 12, 446, 337, 257, 3840, 295, 426, 45196, 721, 293, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.246207994835399, "compression_ratio": 1.5423076923076924, "no_speech_prob": 6.144136932562105e-06}, {"id": 1573, "seek": 767792, "start": 7706.0, "end": 7707.32, "text": " I looked at was", "tokens": [286, 2956, 412, 390], "temperature": 0.0, "avg_logprob": -0.246207994835399, "compression_ratio": 1.5423076923076924, "no_speech_prob": 6.144136932562105e-06}, {"id": 1574, "seek": 770732, "start": 7707.32, "end": 7713.639999999999, "text": " IMDB and they actually have here a list of the current world's best for", "tokens": [21463, 27735, 293, 436, 767, 362, 510, 257, 1329, 295, 264, 2190, 1002, 311, 1151, 337], "temperature": 0.0, "avg_logprob": -0.2285856767134233, "compression_ratio": 1.5574468085106383, "no_speech_prob": 4.425454790180083e-06}, {"id": 1575, "seek": 770732, "start": 7714.36, "end": 7722.0, "text": " IMDB and even with stuff that is highly specialized for sentiment analysis the best anybody had previously come up with is", "tokens": [21463, 27735, 293, 754, 365, 1507, 300, 307, 5405, 19813, 337, 16149, 5215, 264, 1151, 4472, 632, 8046, 808, 493, 365, 307], "temperature": 0.0, "avg_logprob": -0.2285856767134233, "compression_ratio": 1.5574468085106383, "no_speech_prob": 4.425454790180083e-06}, {"id": 1576, "seek": 770732, "start": 7722.719999999999, "end": 7725.599999999999, "text": " 94.1 so in other words this technique", "tokens": [30849, 13, 16, 370, 294, 661, 2283, 341, 6532], "temperature": 0.0, "avg_logprob": -0.2285856767134233, "compression_ratio": 1.5574468085106383, "no_speech_prob": 4.425454790180083e-06}, {"id": 1577, "seek": 770732, "start": 7726.5199999999995, "end": 7729.0, "text": " Getting 94.5. It's literally", "tokens": [13674, 30849, 13, 20, 13, 467, 311, 3736], "temperature": 0.0, "avg_logprob": -0.2285856767134233, "compression_ratio": 1.5574468085106383, "no_speech_prob": 4.425454790180083e-06}, {"id": 1578, "seek": 772900, "start": 7729.0, "end": 7737.68, "text": " Better than anybody has created in the world before as far as we know for as far as James Bradbury knows so", "tokens": [15753, 813, 4472, 575, 2942, 294, 264, 1002, 949, 382, 1400, 382, 321, 458, 337, 382, 1400, 382, 5678, 11895, 22536, 3255, 370], "temperature": 0.0, "avg_logprob": -0.17240458865498387, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.6536748717044247e-06}, {"id": 1579, "seek": 772900, "start": 7738.8, "end": 7743.18, "text": " So when I say like there are big opportunities to use this I mean like", "tokens": [407, 562, 286, 584, 411, 456, 366, 955, 4786, 281, 764, 341, 286, 914, 411], "temperature": 0.0, "avg_logprob": -0.17240458865498387, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.6536748717044247e-06}, {"id": 1580, "seek": 772900, "start": 7743.56, "end": 7750.48, "text": " This is a technique that nobody else currently has access to which you know you could like it. You know whatever", "tokens": [639, 307, 257, 6532, 300, 5079, 1646, 4362, 575, 2105, 281, 597, 291, 458, 291, 727, 411, 309, 13, 509, 458, 2035], "temperature": 0.0, "avg_logprob": -0.17240458865498387, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.6536748717044247e-06}, {"id": 1581, "seek": 772900, "start": 7751.56, "end": 7757.06, "text": " IBM has in Watson or whatever any big company has you know that they're advertising", "tokens": [23487, 575, 294, 25640, 420, 2035, 604, 955, 2237, 575, 291, 458, 300, 436, 434, 13097], "temperature": 0.0, "avg_logprob": -0.17240458865498387, "compression_ratio": 1.6233766233766234, "no_speech_prob": 1.6536748717044247e-06}, {"id": 1582, "seek": 775706, "start": 7757.06, "end": 7764.14, "text": " Unless they have some secret source that they're not publishing which they don't right because people get you know if they have a better thing", "tokens": [16581, 436, 362, 512, 4054, 4009, 300, 436, 434, 406, 17832, 597, 436, 500, 380, 558, 570, 561, 483, 291, 458, 498, 436, 362, 257, 1101, 551], "temperature": 0.0, "avg_logprob": -0.1482868194580078, "compression_ratio": 1.817490494296578, "no_speech_prob": 9.080362360691652e-06}, {"id": 1583, "seek": 775706, "start": 7764.14, "end": 7765.42, "text": " They publish it", "tokens": [814, 11374, 309], "temperature": 0.0, "avg_logprob": -0.1482868194580078, "compression_ratio": 1.817490494296578, "no_speech_prob": 9.080362360691652e-06}, {"id": 1584, "seek": 775706, "start": 7765.42, "end": 7772.820000000001, "text": " Then you now have access to a better text classification method than has ever existed before so I really hope that you know you can try", "tokens": [1396, 291, 586, 362, 2105, 281, 257, 1101, 2487, 21538, 3170, 813, 575, 1562, 13135, 949, 370, 286, 534, 1454, 300, 291, 458, 291, 393, 853], "temperature": 0.0, "avg_logprob": -0.1482868194580078, "compression_ratio": 1.817490494296578, "no_speech_prob": 9.080362360691652e-06}, {"id": 1585, "seek": 775706, "start": 7772.820000000001, "end": 7775.14, "text": " This out and see how you go", "tokens": [639, 484, 293, 536, 577, 291, 352], "temperature": 0.0, "avg_logprob": -0.1482868194580078, "compression_ratio": 1.817490494296578, "no_speech_prob": 9.080362360691652e-06}, {"id": 1586, "seek": 775706, "start": 7777.26, "end": 7781.900000000001, "text": " There may be some things that works really well on and others that it doesn't work as well on I don't know", "tokens": [821, 815, 312, 512, 721, 300, 1985, 534, 731, 322, 293, 2357, 300, 309, 1177, 380, 589, 382, 731, 322, 286, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.1482868194580078, "compression_ratio": 1.817490494296578, "no_speech_prob": 9.080362360691652e-06}, {"id": 1587, "seek": 775706, "start": 7781.900000000001, "end": 7786.4800000000005, "text": " I think this kind of sweet spot here that we had", "tokens": [286, 519, 341, 733, 295, 3844, 4008, 510, 300, 321, 632], "temperature": 0.0, "avg_logprob": -0.1482868194580078, "compression_ratio": 1.817490494296578, "no_speech_prob": 9.080362360691652e-06}, {"id": 1588, "seek": 778648, "start": 7786.48, "end": 7787.919999999999, "text": " about", "tokens": [466], "temperature": 0.0, "avg_logprob": -0.1440554686955043, "compression_ratio": 1.6777777777777778, "no_speech_prob": 8.939621693571098e-06}, {"id": 1589, "seek": 778648, "start": 7787.919999999999, "end": 7789.0, "text": " 25,000", "tokens": [3552, 11, 1360], "temperature": 0.0, "avg_logprob": -0.1440554686955043, "compression_ratio": 1.6777777777777778, "no_speech_prob": 8.939621693571098e-06}, {"id": 1590, "seek": 778648, "start": 7789.0, "end": 7796.5599999999995, "text": " You know short to medium-sized documents if you don't have at least that much text it may be hard to train a different language model", "tokens": [509, 458, 2099, 281, 6399, 12, 20614, 8512, 498, 291, 500, 380, 362, 412, 1935, 300, 709, 2487, 309, 815, 312, 1152, 281, 3847, 257, 819, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.1440554686955043, "compression_ratio": 1.6777777777777778, "no_speech_prob": 8.939621693571098e-06}, {"id": 1591, "seek": 778648, "start": 7797.04, "end": 7802.4, "text": " But having said that there's a lot more we could do here right and we won't be able to do it in part one of this", "tokens": [583, 1419, 848, 300, 456, 311, 257, 688, 544, 321, 727, 360, 510, 558, 293, 321, 1582, 380, 312, 1075, 281, 360, 309, 294, 644, 472, 295, 341], "temperature": 0.0, "avg_logprob": -0.1440554686955043, "compression_ratio": 1.6777777777777778, "no_speech_prob": 8.939621693571098e-06}, {"id": 1592, "seek": 778648, "start": 7802.4, "end": 7808.879999999999, "text": " Course we're doing part two but for example we could start like training language models that look at like", "tokens": [27327, 321, 434, 884, 644, 732, 457, 337, 1365, 321, 727, 722, 411, 3097, 2856, 5245, 300, 574, 412, 411], "temperature": 0.0, "avg_logprob": -0.1440554686955043, "compression_ratio": 1.6777777777777778, "no_speech_prob": 8.939621693571098e-06}, {"id": 1593, "seek": 778648, "start": 7809.5199999999995, "end": 7813.879999999999, "text": " You know lots and lots of medical journals, and then we could like make a downloadable", "tokens": [509, 458, 3195, 293, 3195, 295, 4625, 29621, 11, 293, 550, 321, 727, 411, 652, 257, 5484, 712], "temperature": 0.0, "avg_logprob": -0.1440554686955043, "compression_ratio": 1.6777777777777778, "no_speech_prob": 8.939621693571098e-06}, {"id": 1594, "seek": 781388, "start": 7813.88, "end": 7819.36, "text": " medical language model that then anybody could use to like fine-tune on like a", "tokens": [4625, 2856, 2316, 300, 550, 4472, 727, 764, 281, 411, 2489, 12, 83, 2613, 322, 411, 257], "temperature": 0.0, "avg_logprob": -0.19047180089083585, "compression_ratio": 1.6591928251121075, "no_speech_prob": 4.181179917850386e-07}, {"id": 1595, "seek": 781388, "start": 7820.28, "end": 7823.96, "text": " prostate cancer subset of medical literature for instance like", "tokens": [36108, 5592, 25993, 295, 4625, 10394, 337, 5197, 411], "temperature": 0.0, "avg_logprob": -0.19047180089083585, "compression_ratio": 1.6591928251121075, "no_speech_prob": 4.181179917850386e-07}, {"id": 1596, "seek": 781388, "start": 7824.4800000000005, "end": 7829.08, "text": " There's so much we could do it's kind of exciting and then you know to your net point", "tokens": [821, 311, 370, 709, 321, 727, 360, 309, 311, 733, 295, 4670, 293, 550, 291, 458, 281, 428, 2533, 935], "temperature": 0.0, "avg_logprob": -0.19047180089083585, "compression_ratio": 1.6591928251121075, "no_speech_prob": 4.181179917850386e-07}, {"id": 1597, "seek": 781388, "start": 7829.08, "end": 7833.64, "text": " We could also combine this with like pre-trained word vectors. It's like even without", "tokens": [492, 727, 611, 10432, 341, 365, 411, 659, 12, 17227, 2001, 1349, 18875, 13, 467, 311, 411, 754, 1553], "temperature": 0.0, "avg_logprob": -0.19047180089083585, "compression_ratio": 1.6591928251121075, "no_speech_prob": 4.181179917850386e-07}, {"id": 1598, "seek": 781388, "start": 7834.76, "end": 7837.78, "text": " Trying that hard like you know we even without news like", "tokens": [20180, 300, 1152, 411, 291, 458, 321, 754, 1553, 2583, 411], "temperature": 0.0, "avg_logprob": -0.19047180089083585, "compression_ratio": 1.6591928251121075, "no_speech_prob": 4.181179917850386e-07}, {"id": 1599, "seek": 783778, "start": 7837.78, "end": 7844.74, "text": " We could have pre-trained a Wikipedia say corpus language model and then fine-tuned it into a", "tokens": [492, 727, 362, 659, 12, 17227, 2001, 257, 28999, 584, 1181, 31624, 2856, 2316, 293, 550, 2489, 12, 83, 43703, 309, 666, 257], "temperature": 0.0, "avg_logprob": -0.2503237931624703, "compression_ratio": 1.654867256637168, "no_speech_prob": 1.4367456060426775e-06}, {"id": 1600, "seek": 783778, "start": 7845.86, "end": 7853.08, "text": " IMDB language model and then fine-tune that into an IBM IMDB sentiment analysis model, and we would have got something better than this", "tokens": [21463, 27735, 2856, 2316, 293, 550, 2489, 12, 83, 2613, 300, 666, 364, 23487, 21463, 27735, 16149, 5215, 2316, 11, 293, 321, 576, 362, 658, 746, 1101, 813, 341], "temperature": 0.0, "avg_logprob": -0.2503237931624703, "compression_ratio": 1.654867256637168, "no_speech_prob": 1.4367456060426775e-06}, {"id": 1601, "seek": 783778, "start": 7853.98, "end": 7856.759999999999, "text": " So like this. I really think this is the tip of the iceberg", "tokens": [407, 411, 341, 13, 286, 534, 519, 341, 307, 264, 4125, 295, 264, 38880], "temperature": 0.0, "avg_logprob": -0.2503237931624703, "compression_ratio": 1.654867256637168, "no_speech_prob": 1.4367456060426775e-06}, {"id": 1602, "seek": 783778, "start": 7858.0599999999995, "end": 7864.5, "text": " And I was talking there's a really fantastic researcher called Sebastian Ruda who is", "tokens": [400, 286, 390, 1417, 456, 311, 257, 534, 5456, 21751, 1219, 31102, 497, 11152, 567, 307], "temperature": 0.0, "avg_logprob": -0.2503237931624703, "compression_ratio": 1.654867256637168, "no_speech_prob": 1.4367456060426775e-06}, {"id": 1603, "seek": 786450, "start": 7864.5, "end": 7870.06, "text": " Basically the only NLP researcher. I know who's been really really writing a lot about", "tokens": [8537, 264, 787, 426, 45196, 21751, 13, 286, 458, 567, 311, 668, 534, 534, 3579, 257, 688, 466], "temperature": 0.0, "avg_logprob": -0.2083971334058185, "compression_ratio": 1.6261682242990654, "no_speech_prob": 5.50754930372932e-06}, {"id": 1604, "seek": 786450, "start": 7871.3, "end": 7878.08, "text": " pre-training and fine-tuning and transfer learning and NLP and I was asking him like why isn't this happening more and", "tokens": [659, 12, 17227, 1760, 293, 2489, 12, 83, 37726, 293, 5003, 2539, 293, 426, 45196, 293, 286, 390, 3365, 796, 411, 983, 1943, 380, 341, 2737, 544, 293], "temperature": 0.0, "avg_logprob": -0.2083971334058185, "compression_ratio": 1.6261682242990654, "no_speech_prob": 5.50754930372932e-06}, {"id": 1605, "seek": 786450, "start": 7878.54, "end": 7883.52, "text": " His view was it's because there isn't the software to make it easy you know", "tokens": [2812, 1910, 390, 309, 311, 570, 456, 1943, 380, 264, 4722, 281, 652, 309, 1858, 291, 458], "temperature": 0.0, "avg_logprob": -0.2083971334058185, "compression_ratio": 1.6261682242990654, "no_speech_prob": 5.50754930372932e-06}, {"id": 1606, "seek": 786450, "start": 7884.06, "end": 7887.78, "text": " So I'm actually going to share this lecture with with him tomorrow", "tokens": [407, 286, 478, 767, 516, 281, 2073, 341, 7991, 365, 365, 796, 4153], "temperature": 0.0, "avg_logprob": -0.2083971334058185, "compression_ratio": 1.6261682242990654, "no_speech_prob": 5.50754930372932e-06}, {"id": 1607, "seek": 788778, "start": 7887.78, "end": 7896.599999999999, "text": " Because you know it feels like there's you know hopefully going to be a lot of stuff coming out now that we're making it really easy", "tokens": [1436, 291, 458, 309, 3417, 411, 456, 311, 291, 458, 4696, 516, 281, 312, 257, 688, 295, 1507, 1348, 484, 586, 300, 321, 434, 1455, 309, 534, 1858], "temperature": 0.0, "avg_logprob": -0.2154435244473544, "compression_ratio": 1.6970954356846473, "no_speech_prob": 8.139622877934016e-06}, {"id": 1608, "seek": 788778, "start": 7897.139999999999, "end": 7899.139999999999, "text": " to do this", "tokens": [281, 360, 341], "temperature": 0.0, "avg_logprob": -0.2154435244473544, "compression_ratio": 1.6970954356846473, "no_speech_prob": 8.139622877934016e-06}, {"id": 1609, "seek": 788778, "start": 7899.42, "end": 7901.42, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2154435244473544, "compression_ratio": 1.6970954356846473, "no_speech_prob": 8.139622877934016e-06}, {"id": 1610, "seek": 788778, "start": 7901.66, "end": 7905.62, "text": " We're kind of out of time so what I'll do is I'll quickly look at", "tokens": [492, 434, 733, 295, 484, 295, 565, 370, 437, 286, 603, 360, 307, 286, 603, 2661, 574, 412], "temperature": 0.0, "avg_logprob": -0.2154435244473544, "compression_ratio": 1.6970954356846473, "no_speech_prob": 8.139622877934016e-06}, {"id": 1611, "seek": 788778, "start": 7906.5, "end": 7913.4, "text": " Collaborative filtering introduction, and then we'll finish it next time the collaborative filtering. There's very very little you to learn", "tokens": [44483, 1166, 30822, 9339, 11, 293, 550, 321, 603, 2413, 309, 958, 565, 264, 16555, 30822, 13, 821, 311, 588, 588, 707, 291, 281, 1466], "temperature": 0.0, "avg_logprob": -0.2154435244473544, "compression_ratio": 1.6970954356846473, "no_speech_prob": 8.139622877934016e-06}, {"id": 1612, "seek": 788778, "start": 7913.5, "end": 7916.34, "text": " We've basically learned everything we're going to need", "tokens": [492, 600, 1936, 3264, 1203, 321, 434, 516, 281, 643], "temperature": 0.0, "avg_logprob": -0.2154435244473544, "compression_ratio": 1.6970954356846473, "no_speech_prob": 8.139622877934016e-06}, {"id": 1613, "seek": 791634, "start": 7916.34, "end": 7918.34, "text": " to do this week", "tokens": [281, 360, 341, 1243], "temperature": 0.0, "avg_logprob": -0.1883563995361328, "compression_ratio": 1.8565217391304347, "no_speech_prob": 4.9369532462151255e-06}, {"id": 1614, "seek": 791634, "start": 7918.860000000001, "end": 7927.3, "text": " Collaborative filtering will will cover this quite quickly next week, and then we're going to do a really deep dive into collaborative filtering next week", "tokens": [44483, 1166, 30822, 486, 486, 2060, 341, 1596, 2661, 958, 1243, 11, 293, 550, 321, 434, 516, 281, 360, 257, 534, 2452, 9192, 666, 16555, 30822, 958, 1243], "temperature": 0.0, "avg_logprob": -0.1883563995361328, "compression_ratio": 1.8565217391304347, "no_speech_prob": 4.9369532462151255e-06}, {"id": 1615, "seek": 791634, "start": 7928.02, "end": 7933.860000000001, "text": " Where we're going to learn about like we're actually going to from scratch learn how to do stochastic gradient descent", "tokens": [2305, 321, 434, 516, 281, 1466, 466, 411, 321, 434, 767, 516, 281, 490, 8459, 1466, 577, 281, 360, 342, 8997, 2750, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.1883563995361328, "compression_ratio": 1.8565217391304347, "no_speech_prob": 4.9369532462151255e-06}, {"id": 1616, "seek": 791634, "start": 7934.78, "end": 7939.5, "text": " How to create loss functions how they work exactly and then we'll go from there", "tokens": [1012, 281, 1884, 4470, 6828, 577, 436, 589, 2293, 293, 550, 321, 603, 352, 490, 456], "temperature": 0.0, "avg_logprob": -0.1883563995361328, "compression_ratio": 1.8565217391304347, "no_speech_prob": 4.9369532462151255e-06}, {"id": 1617, "seek": 791634, "start": 7939.5, "end": 7942.860000000001, "text": " We'll gradually build back up to really deeply understand", "tokens": [492, 603, 13145, 1322, 646, 493, 281, 534, 8760, 1223], "temperature": 0.0, "avg_logprob": -0.1883563995361328, "compression_ratio": 1.8565217391304347, "no_speech_prob": 4.9369532462151255e-06}, {"id": 1618, "seek": 794286, "start": 7942.86, "end": 7948.0599999999995, "text": " What's going on in the structured models, and then what's going on in confidence", "tokens": [708, 311, 516, 322, 294, 264, 18519, 5245, 11, 293, 550, 437, 311, 516, 322, 294, 6687], "temperature": 0.0, "avg_logprob": -0.15903096680247455, "compression_ratio": 1.8380566801619433, "no_speech_prob": 1.963795511983335e-06}, {"id": 1619, "seek": 794286, "start": 7948.0599999999995, "end": 7952.38, "text": " And then finally what's going on in recurrent neural networks, and hopefully we'll be able to build them all", "tokens": [400, 550, 2721, 437, 311, 516, 322, 294, 18680, 1753, 18161, 9590, 11, 293, 4696, 321, 603, 312, 1075, 281, 1322, 552, 439], "temperature": 0.0, "avg_logprob": -0.15903096680247455, "compression_ratio": 1.8380566801619433, "no_speech_prob": 1.963795511983335e-06}, {"id": 1620, "seek": 794286, "start": 7952.98, "end": 7958.98, "text": " From scratch okay, so this is kind of a going to be really important this movie lens data set because we're going to use it", "tokens": [3358, 8459, 1392, 11, 370, 341, 307, 733, 295, 257, 516, 281, 312, 534, 1021, 341, 3169, 6765, 1412, 992, 570, 321, 434, 516, 281, 764, 309], "temperature": 0.0, "avg_logprob": -0.15903096680247455, "compression_ratio": 1.8380566801619433, "no_speech_prob": 1.963795511983335e-06}, {"id": 1621, "seek": 794286, "start": 7958.98, "end": 7960.86, "text": " to learn a lot of like", "tokens": [281, 1466, 257, 688, 295, 411], "temperature": 0.0, "avg_logprob": -0.15903096680247455, "compression_ratio": 1.8380566801619433, "no_speech_prob": 1.963795511983335e-06}, {"id": 1622, "seek": 794286, "start": 7960.86, "end": 7967.44, "text": " Really foundational theory and kind of math behind it so the movie lens data set", "tokens": [4083, 32195, 5261, 293, 733, 295, 5221, 2261, 309, 370, 264, 3169, 6765, 1412, 992], "temperature": 0.0, "avg_logprob": -0.15903096680247455, "compression_ratio": 1.8380566801619433, "no_speech_prob": 1.963795511983335e-06}, {"id": 1623, "seek": 794286, "start": 7968.38, "end": 7970.38, "text": " This is basically what it looks like", "tokens": [639, 307, 1936, 437, 309, 1542, 411], "temperature": 0.0, "avg_logprob": -0.15903096680247455, "compression_ratio": 1.8380566801619433, "no_speech_prob": 1.963795511983335e-06}, {"id": 1624, "seek": 797038, "start": 7970.38, "end": 7974.16, "text": " It contains a bunch of ratings. It says user number one", "tokens": [467, 8306, 257, 3840, 295, 24603, 13, 467, 1619, 4195, 1230, 472], "temperature": 0.0, "avg_logprob": -0.19949835997361404, "compression_ratio": 1.8660714285714286, "no_speech_prob": 3.3931310099433176e-06}, {"id": 1625, "seek": 797038, "start": 7974.900000000001, "end": 7981.14, "text": " watched movie number 31, and they gave it a rating of two and a half at this particular time and", "tokens": [6337, 3169, 1230, 10353, 11, 293, 436, 2729, 309, 257, 10990, 295, 732, 293, 257, 1922, 412, 341, 1729, 565, 293], "temperature": 0.0, "avg_logprob": -0.19949835997361404, "compression_ratio": 1.8660714285714286, "no_speech_prob": 3.3931310099433176e-06}, {"id": 1626, "seek": 797038, "start": 7982.06, "end": 7988.54, "text": " Then they watched movie 1029 and they gave it a rating of three and they watched rating one one movie 1172", "tokens": [1396, 436, 6337, 3169, 1266, 11871, 293, 436, 2729, 309, 257, 10990, 295, 1045, 293, 436, 6337, 10990, 472, 472, 3169, 2975, 28890], "temperature": 0.0, "avg_logprob": -0.19949835997361404, "compression_ratio": 1.8660714285714286, "no_speech_prob": 3.3931310099433176e-06}, {"id": 1627, "seek": 797038, "start": 7988.54, "end": 7991.04, "text": " And they gave it a rating of four okay, and so forth", "tokens": [400, 436, 2729, 309, 257, 10990, 295, 1451, 1392, 11, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.19949835997361404, "compression_ratio": 1.8660714285714286, "no_speech_prob": 3.3931310099433176e-06}, {"id": 1628, "seek": 797038, "start": 7991.62, "end": 7998.78, "text": " So this is the ratings table. This is really the only one that matters and our goal will be for some user", "tokens": [407, 341, 307, 264, 24603, 3199, 13, 639, 307, 534, 264, 787, 472, 300, 7001, 293, 527, 3387, 486, 312, 337, 512, 4195], "temperature": 0.0, "avg_logprob": -0.19949835997361404, "compression_ratio": 1.8660714285714286, "no_speech_prob": 3.3931310099433176e-06}, {"id": 1629, "seek": 799878, "start": 7998.78, "end": 8005.599999999999, "text": " We haven't seen before sorry for some user movie combination. We haven't seen before we have to predict if they'll like it", "tokens": [492, 2378, 380, 1612, 949, 2597, 337, 512, 4195, 3169, 6562, 13, 492, 2378, 380, 1612, 949, 321, 362, 281, 6069, 498, 436, 603, 411, 309], "temperature": 0.0, "avg_logprob": -0.13089599959347226, "compression_ratio": 1.8550185873605949, "no_speech_prob": 6.439010121539468e-06}, {"id": 1630, "seek": 799878, "start": 8006.139999999999, "end": 8009.259999999999, "text": " Right and so this is how recommendation systems are built", "tokens": [1779, 293, 370, 341, 307, 577, 11879, 3652, 366, 3094], "temperature": 0.0, "avg_logprob": -0.13089599959347226, "compression_ratio": 1.8550185873605949, "no_speech_prob": 6.439010121539468e-06}, {"id": 1631, "seek": 799878, "start": 8009.259999999999, "end": 8014.92, "text": " This is how like Amazon decides what books to recommend Netflix decides what movies to recommend and so forth", "tokens": [639, 307, 577, 411, 6795, 14898, 437, 3642, 281, 2748, 12778, 14898, 437, 6233, 281, 2748, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.13089599959347226, "compression_ratio": 1.8550185873605949, "no_speech_prob": 6.439010121539468e-06}, {"id": 1632, "seek": 799878, "start": 8016.98, "end": 8020.599999999999, "text": " To make it more interesting we'll also actually download a list of movies", "tokens": [1407, 652, 309, 544, 1880, 321, 603, 611, 767, 5484, 257, 1329, 295, 6233], "temperature": 0.0, "avg_logprob": -0.13089599959347226, "compression_ratio": 1.8550185873605949, "no_speech_prob": 6.439010121539468e-06}, {"id": 1633, "seek": 799878, "start": 8020.98, "end": 8026.54, "text": " So each movie we're actually going to have the title and so for that question earlier about like what's actually going to be in these?", "tokens": [407, 1184, 3169, 321, 434, 767, 516, 281, 362, 264, 4876, 293, 370, 337, 300, 1168, 3071, 466, 411, 437, 311, 767, 516, 281, 312, 294, 613, 30], "temperature": 0.0, "avg_logprob": -0.13089599959347226, "compression_ratio": 1.8550185873605949, "no_speech_prob": 6.439010121539468e-06}, {"id": 1634, "seek": 802654, "start": 8026.54, "end": 8030.28, "text": " Embedding matrices, how do we interpret them? We're actually going to be able to look and see", "tokens": [24234, 292, 3584, 32284, 11, 577, 360, 321, 7302, 552, 30, 492, 434, 767, 516, 281, 312, 1075, 281, 574, 293, 536], "temperature": 0.0, "avg_logprob": -0.19585218234938018, "compression_ratio": 1.5958333333333334, "no_speech_prob": 1.6442367268609814e-05}, {"id": 1635, "seek": 802654, "start": 8031.0199999999995, "end": 8032.72, "text": " How that's working?", "tokens": [1012, 300, 311, 1364, 30], "temperature": 0.0, "avg_logprob": -0.19585218234938018, "compression_ratio": 1.5958333333333334, "no_speech_prob": 1.6442367268609814e-05}, {"id": 1636, "seek": 802654, "start": 8032.72, "end": 8039.1, "text": " So basically this is kind of like what we're creating this is kind of cross tab of users", "tokens": [407, 1936, 341, 307, 733, 295, 411, 437, 321, 434, 4084, 341, 307, 733, 295, 3278, 4421, 295, 5022], "temperature": 0.0, "avg_logprob": -0.19585218234938018, "compression_ratio": 1.5958333333333334, "no_speech_prob": 1.6442367268609814e-05}, {"id": 1637, "seek": 802654, "start": 8039.94, "end": 8044.14, "text": " By movies right and so feel free to look ahead during the week", "tokens": [3146, 6233, 558, 293, 370, 841, 1737, 281, 574, 2286, 1830, 264, 1243], "temperature": 0.0, "avg_logprob": -0.19585218234938018, "compression_ratio": 1.5958333333333334, "no_speech_prob": 1.6442367268609814e-05}, {"id": 1638, "seek": 802654, "start": 8044.22, "end": 8048.3, "text": " You'll see basically as per usual collab filter data set from CSP", "tokens": [509, 603, 536, 1936, 382, 680, 7713, 44228, 6608, 1412, 992, 490, 9460, 47], "temperature": 0.0, "avg_logprob": -0.19585218234938018, "compression_ratio": 1.5958333333333334, "no_speech_prob": 1.6442367268609814e-05}, {"id": 1639, "seek": 802654, "start": 8049.14, "end": 8052.82, "text": " Model data get learner learn dot fit and we're done", "tokens": [17105, 1412, 483, 33347, 1466, 5893, 3318, 293, 321, 434, 1096], "temperature": 0.0, "avg_logprob": -0.19585218234938018, "compression_ratio": 1.5958333333333334, "no_speech_prob": 1.6442367268609814e-05}, {"id": 1640, "seek": 805282, "start": 8052.82, "end": 8056.7, "text": " And you won't be surprised to hear when we then take that and we can correct the benchmarks", "tokens": [400, 291, 1582, 380, 312, 6100, 281, 1568, 562, 321, 550, 747, 300, 293, 321, 393, 3006, 264, 43751], "temperature": 0.0, "avg_logprob": -0.15110765184674943, "compression_ratio": 1.6040609137055837, "no_speech_prob": 1.9831773897749372e-05}, {"id": 1641, "seek": 805282, "start": 8056.7, "end": 8062.0199999999995, "text": " It seems to be better than the benchmarks where you looked at so that'll basically be it and then next week", "tokens": [467, 2544, 281, 312, 1101, 813, 264, 43751, 689, 291, 2956, 412, 370, 300, 603, 1936, 312, 309, 293, 550, 958, 1243], "temperature": 0.0, "avg_logprob": -0.15110765184674943, "compression_ratio": 1.6040609137055837, "no_speech_prob": 1.9831773897749372e-05}, {"id": 1642, "seek": 805282, "start": 8062.0199999999995, "end": 8067.62, "text": " We'll have a deep dive, and we'll see how to actually build this from scratch all right. See you next week", "tokens": [492, 603, 362, 257, 2452, 9192, 11, 293, 321, 603, 536, 577, 281, 767, 1322, 341, 490, 8459, 439, 558, 13, 3008, 291, 958, 1243], "temperature": 0.0, "avg_logprob": -0.15110765184674943, "compression_ratio": 1.6040609137055837, "no_speech_prob": 1.9831773897749372e-05}, {"id": 1643, "seek": 806762, "start": 8067.62, "end": 8082.62, "text": " Thank you", "tokens": [50364, 1044, 291, 51114], "temperature": 0.0, "avg_logprob": -0.7466822147369385, "compression_ratio": 0.5294117647058824, "no_speech_prob": 0.0004435574810486287}], "language": "en"}