{"text": " Hi all and welcome to lesson 15. And what we're going to endeavor to do today is to create a convolutional autoencoder. And in the process we will see why doing that well is a tricky thing to do. And time permitting we will begin to work on a framework, a deep learning framework to make life a lot easier. Not sure how far we'll get on that today time wise, so let's see how we go and get straight into it. So okay, so today let's start by talking, before we can create a convolutional autoencoder, we need to talk about convolutions and what are they and what are they for. Broadly speaking, convolutions are something that allows us to tell our neural network a little bit about the structure of the problem that's going to make it a lot easier for it to solve the problem. And in particular the structure of our problem is we're doing things with images. Images are laid out on a grid, a 2D grid for black and white or a 3D for color or a 4D for a color video or whatever. And so we would say, you know, there's a relationship between the pixels going across and the pixels going down. They tend to be similar to each other. Differences in those pixels across those dimensions tend to have meaning. Sets patterns of pixels that appear in different places often represent the same thing. So for example a cat in the top left is still a cat even if it's in the bottom right. These kinds of prior information is something that is naturally captured by a convolutional neural network, something that uses convolutions. Generally speaking this is a good thing because it means that we will be able to use less parameters and less computation because more of that information about the problem we're solving is kind of encoded directly into our architecture. There are other architectures that don't encode that prior information as strongly, such as a multi-layer perceptron, which we've been looking at so far, or a transformer's network which we haven't looked at yet. Those kinds of architectures could potentially give us, well they do give us more flexibility. And given enough time, compute and data, they could potentially find things that maybe CNNs would struggle to find. So we're not always going to use convolutional neural networks, but they're a pretty good starting point and certainly something important to understand. They're not just used for images, we can also take advantage of one-dimensional convolutions for language-based tasks for instance. So convolutions come up a lot. So in this notebook, one thing you'll notice that might be of interest is we are importing stuff from MiniAI now. Now MiniAI is this little library that we're starting to create. And we're creating it using nbdev. So we've got a miniai.training and a miniai.datasets. And so if we look for example at the datasets notebook, it starts with something that says that the default export module is called datasets. And some of the cells have a export directive on them. And at the very bottom we had something that called nbdev export. Now what that's going to do is it's going to create a file called datasets.py. Just here, datasets.py. And it contains those cells that we exported. And why is it called miniai.datasets? That's because everything for nbdev is stored in settings.any. And there's something here saying create a library lib name called miniai. You can't use this library until you install it. Now we haven't uploaded it to PyPy, like we made a pip installable package from a public server. But you can actually install a local directory as if it's a Python module that you've kind of installed from the internet. And to do that you say pip install in the usual way. But you say minus e, that stands for editable. And that means set up the current directory as a Python module. Well current directory, actually any directory you like. I'll just put dot to mean the current directory. And so you'll see that's going to go ahead and actually install my library. And so after I've done that, I can now import things from that library, as you see. So this is just the same as before. We're going to grab our MNIST dataset, and we're going to create a convolutional neural network on it. So before we do that, we're going to talk about what are convolutions. And one of my favorite descriptions of convolutions comes from the student in our, I think it was our very first course, Matt Clinesmith, who wrote this really nice Medium article, CNNs from different viewpoints, which I'm going to steal from. And here's the basic idea. Say that this is our image. It's a 3x3 image with 9 pixels labeled from A to J as capital letters. Now a convolution uses something called a kernel. And a kernel is just another tensor. In this case, it's a 2x2 matrix. Again so in this one we're going to have alpha, beta, gamma, delta as our four values in this convolution. Now in this kernel, oh, now one thing I'll mention, I can't remember if I've said this before, is the Greek letters are things that you want to be able to, I think I have mentioned this, you want to be able to pronounce them. So if you don't know how to read these and say what these names are, make sure you head over to Wikipedia or whatever and learn the names of all the Greek letters so that you can, because they come up all the time. So what happens when we apply a convolution with this 2x2 kernel to this 3x3 image? I mean it doesn't have to be an image, in this case it's just a rank 2 tensor, but it might represent an image. What happens is we take the kernel and we overlay it over the first little 2x2 subgrid, like so. And specifically what we do is we match color to color. So the output of this first 2x2 overlay would be alpha times a plus beta times b plus gamma times d plus delta times e. And that would yield some value, p, and that's going to end up in the top left of a 2x2 output. So the top right of the 2x2 output we're going to slide, it's like a sliding window, we're going to slide our kernel over to here and apply each of our coefficients to these respectively colored squares. And then ditto for the bottom left, and then ditto for the bottom right. So we end up with this equation. p, as we discussed, is alpha a plus beta b plus gamma d plus delta e plus some bias term. q, so the top right, as you can see it's just alpha in this case times b. And so we're just multiplying them together and adding them up, multiply together, add them up, multiply together and add them up. So we're basically, you can imagine that we're basically flattening these out into rank 1 classes, into vectors, and then doing a dot product would be one way of thinking about what's happening as we slide this kernel over these windows. And so this is called a convolution. So let's try and create a convolution. So for example, let's grab our training images and take a look at one. And let's create a 3x3 kernel. So remember a kernel is just, we've already, a kernel appears a lot of times in computer science and math. We've already seen the term kernel to mean a piece of code that we run on a GPU across lots of parallel kind of virtual devices or potentially in a grid. There's a similar idea here. We've got a computation, which is in this case kind of this dot product or something like a dot product, sliding over, occurring lots of times over a grid. But it's, yeah, it's a bit different. It's kind of another use of the word kernel. So in this case, a kernel is a, in this case, it's going to be a rank 2 tensor. And so let's create a kernel with these values in the 3x3 matrix rank 2 tensor. And we could draw what that looks like. Not surprisingly, it just looks like a bunch of lines. Oops. Okay. So what would happen if we slide this over, just these nine pixels over this 28 by 28? Well what's going to happen is if we've got some, the top left, for example, 3x3 section has these names, then we're going to end up with negative a1, because the top three are all negative, right? Negative a1 minus a2 minus a3. The next are just zero. So that won't do anything. And then plus a7 plus a8 plus a9. Why is that interesting? That's interesting. Well, let's try. Here, what I've done here is I've grabbed just the first 13 rows and first 23 columns of our image. And I'm actually showing the numbers and also using grey kind of conditional formatting, if you like, or the equivalent in pandas, to show this top bit. So we're looking at just this top bit. So what happens if we take rows 3, 4, and 5? Remember this is not inclusive, right? So it's rows 3, 4, and 5. Columns 14, 15, 16. So we're looking at these three here. What's that going to give us if we multiply it by this kernel? It gives us a fairly large positive value, because the three that we have negatives on is the top row. Well, they're all zero. And the three that we have positives on, they're all close to 1. So we end up with quite a large number. What about the same columns but for rows 7, 8, 9? 7, 8, 9. Here, the top is all positive and the bottom is all zero. So that means that we're going to get a lot of negative terms. And not surprisingly, that's exactly what we see. If we do this, kind of a dot product equivalent, which all you need in NumPy to do that is just an element-wise multiplication followed by a sum. So that's going to be quite a large negative number. And so perhaps you're seeing what this is doing, and maybe you got a hint from the name of the tensor we created. It's something that is going to find the top edge. So this one is a top edge, so it's a positive. And this one is a bottom edge, so it's a negative. So we would like to apply that, this kernel, to every single 3x3 section in here. So we could do that by creating a little apply kernel function that takes some particular row and some particular column and some particular tensor as a kernel and does that multiplication. dot sum that we just saw. So for example, we could replicate this one by calling apply kernel. And this here is the center of that 3x3 grid area. And so there's that same number, 2.97. So now we could apply that kernel to every one of the 3x3 windows in this 28x28 image. So we're going to be sliding over like this red bit sliding over here, but we've actually got a 28x28 input, not just a 5x5 input. So to get all of the coordinates, let's just simplify to do this 5x5, we can go, we can create a list comprehension. We can take i through every value in range 5. And then for each of those, we can take j for every value in range 5. And so if we just look at that tuple, you can see we get a list of lists containing all of those coordinates. So this is a list comprehension in a list comprehension, which when you first say it may be surprising or confusing, but it's a really helpful idiom, and I certainly recommend getting used to it. Now, what we're going to do is we're not just going to create the cup, this tuple, but we're actually going to call apply kernel for each of those. So if we go through from 1 to 27, well actually 1 to 26, because 27 is exclusive. So we're going to go through everything from 1 to 26. And then for each of those, go through from 1 to 26 again and call apply kernel. And that's going to give us the result of applying that convolutional kernel to every one of those coordinates. And there's the result. And you can see what it's done, as we hoped, is it is highlighting the top edges. So yeah, you might find that kind of surprising that it's that easy to do this kind of image processing. We're literally just doing an element-wise multiplication and a sum for each window. Okay, so that is called a convolution. So we can do another convolution. This time we could do one with a left edge tensor. As you can see, it looks just a rotated version or transposed version, I guess, of our top edge tensor. And so if we apply that kernel, so this time we're going to apply the left edge kernel. And so notice here that we're actually passing in a function. Right, we're passing in a function. Sorry, actually not a function, is it? It's just a tensor, actually. So we're going to pass in the left edge tensor for the same list comprehension, inner list comprehension. And this time we're getting back the left edges. It's highlighting all of the left edges in the digit. So yeah, this is basically what's happening here, is that a two by two can be looped over an image, creating these outputs. Now you'll see here that in the process of doing so, we are losing the outermost pixels of our image. We'll learn about how to fix that later. But just for now, notice that as we are putting our three by three through, for example, in this five by five, there's only one, two, three places that we can put it going across, five places, because we need some kind of edge. All right, so that's cool. That's a convolution. And hopefully if you remember back to kind of the Zeiler and Fergus pictures from lesson one, you might recognize that the kind of first layer of a convolutional network is often looking for kind of edges and gradients and things like that. And this is how it does it. And then when convolutions on top of convolutions with nonlinear activations between them can combine those into curves or corners or stuff like that, and so on and so forth. Okay, so how do we do this quickly? Because currently this is going to be super, super slow doing this in Python. So one of the very earliest, or probably the earliest publicly available general purpose deep learning, GPU accelerated deep learning thing I saw was called Caffe. That was created by somebody called Yangqing Jia. And he actually described what happened, where Caffe, how Caffe went about implementing a fast convolution on a GPU. And basically he said, well, I had two months to do it and I had to finish my thesis. And so I ended up doing something where I said, well, there was some other code out there, Kujewski, who you might've come across, him and Hinton set up a little startup, which Google bought and that kind of became the start of Google's deep learning, the Google brain basically. So Kujewski had all this fancy stuff in his library, but Yangqing Jia said, oh, I didn't know how to do all that stuff. So I said, well, I already know how to multiply matrices. So maybe I can convert a convolution into a matrix multiplication. And so that I became known as IMtocole. IMtocole is a way of converting a convolution into a matrix multiply. And so actually, I don't know if, I suspect Yangqing Jia could have accidentally reinvented it because it actually had been around for a while, even at the point that he was writing his thesis, I believe. So it was actually, this is the place I believe it was created in this paper. So that was in 2006, which is a while ago. And so this is actually from that paper. And what they describe is, let's say you are putting this two by two kernel over this three by three bit of an image. So here you've got this, this window needs to match to this bit of this window, right? What you could do is you could unwrap this to one, one, two, two, sorry, one, two, one, two, downwards to here, one, two, one, two, to unroll it like so. And you could unroll the kernel here. Yeah, so this is one, two, one, one. So this is bit is here, one, two, one, one. And then you could unroll the kernel one, one, two, two to here, one, one, two, two. And then once they've been flattened out and moved in that way, and then you'll do exactly the same thing for this next patch here, two, oh, one, three. You flatten it out and put it here, two, oh, one, three. So if you basically take those kernels and flatten them out in this format, then you end up with a matrix multiply. If you multiply this matrix by this matrix, you'll end up with the output that you want from the convolution. So this is basically a way of unrolling your kernels and your input features into matrices, such as when you do the matrix multiply, you get the right answer. So it's a kind of a nifty trick. And so that is called imtocol. I guess we're kind of cheating a little bit. Implementing that is kind of boring. It's just a bunch of copying and tensor manipulation. So I actually haven't done it. Instead I've linked to a NumPy implementation, which is here. And it also, part of it is this getIndices, which is here. And as you can see, it's a little bit tedious with repeats and tiles and reshapes and whatnot. So I'm not going to call it homework, but if you want to practice your tensor indexing manipulation skills, try creating a PyTorch version from scratch. I got to admit, I didn't bother. Instead I used the one that's built into PyTorch. And in PyTorch it's called Unfold. So if we take our image, and PyTorch expects there to be a batch axis and a dimension and a channel dimension. So we'll add two unit leading dimensions to it. And we can unfold our input for a 3x3. And that will give us a 9x676 input. And so then we can take that, and... Whoopsie daisy. We can take that, and then we'll take our kernel and just flatten it out into a vector. So view changes the shape, and minus one just says dump everything into this dimension. So that's going to create a 9 long vector. Length 9 vector. And so now we can do the matrix multiply. Just like they've done here, of the kernel matrix, that's our weights, by the unrolled input features. And so that gives us a 676 long. We can then view that as 26x26. And we get back, as we hoped, our left edge tensor result. And so this is, yeah, this is how we can kind of, from scratch, create a better implementation of convolutions. The reason I'm cheating, I'm allowed to cheat here, is because we did actually create convolutions from scratch. We're not always creating the GPU optimized versions from scratch, which was never something I promised. So I think that's fair. But it's cool that we can kind of hacker out a GPU optimized version in the same way that the kind of original deep learning library did. So if we use apply kernel, we get nearly 9 milliseconds. If we use unfold with matrix multiply, we get 20 microseconds. So that's what, about 400 times faster. So that's pretty cool. Now of course we don't have to use unfold and matrix multiply, because PyTorch has a conv2d. So we can run that. And that, interestingly, is about the same speed. At least on GPU. But this would also work on GPU just as well. Yeah, I'm not sure this will always be the case. In this case it's a pretty small image. I haven't experimented a whole lot to see whereabouts there's a big difference in speeds between these. Obviously I always just use f.conv2d. But if there's some more tricky convolution you need to do with some weird thing around channels or dimensions or something, you can always try this unfold trick. It's nice to know it's there, I think. So we could do the same thing for diagonal edges. So here's our diagonal edge kernel. Or the other diagonal. So if we just grab the first 16 images, then we can do a convolution on our whole batch with all of our kernels at once. So this is a nice optimized thing that we can do. And you end up with your 26 by 26, you've got your 4 kernels, and you've got your 16 images. And so that's summarized here. So that's generally what we're doing to get good GPU acceleration, is we're doing a bunch of kernels and a bunch of images all at once, across all of their pixels. And so here we go. That's what happens when we take a look at our various kernels for a particular image. Left edge, I guess top edge, and then diagonal, top left, and top right. Okay, so that is optimized convolutions. And that works just as well on the CPU or GPU. The GPU will be faster if you have one. Now how do we deal with the problem that we're losing one pixel on each side? What we can do is we can add something called padding. And for padding, what we basically do is, rather than starting our window here, we start it right over here. And actually we'd be up one as well. And so these three on the left here, we just take the input for each of those as zero. So we're basically just assuming that they're all zero. I mean, there's other options we could choose. We could assume they're the same as the one next to them. There's various things we can do, but the simplest and the one we normally do is just assume that they're zero. So now, so let's say for example, this is called one pixel padding. Let's say we did two pixel padding. So we had two pixel padding with a five by five input, and a four by four kernel. So that grays our kernel. Then we're going to start right up way over here on the corner. And then you can see what happens as we slide the kernel over. There's all the spots that it's going to take. And so this dotted line area is the area that we're kind of effectively going through. But all of these white bits we're just going to treat as zero. And so then this green is the output size we end up with, which is going to be six by six for a five by five input. I should mention even numbered edge kernels are not used very often. We normally used odd numbered kernels. If you use, for example, a three by three kernel and one pixel of padding, you will get back the same size you start with. If you use five by five with three pixels of padding, you'll end up with the same size you start with. So generally, odd numbered edge size kernels are easier to deal with, to make sure you end up with the same thing you start with. Okay, so yeah, so as it says here, if you've got an odd numbered size, ks by ks size kernel, then ks truncate divide two. That's what slash slash means will give you the right size. And so another trick you can do is you don't always have to just move your window across by one each time. You could move it by a different amount each time. The amount you move it by is called the stride. So for example, here's a case of doing a stride two. So with stride two padding one. So we start out here, and then we jump across two, and then we jump across two, and then we go to the next row. So that's called a stride two convolution. Stride two convolutions are handy because they actually reduce the dimensionality of your input by a factor of two. And that's actually what we want to do a lot. For example, with an autoencoder, we want to do that. And in fact, for most classification architectures, we do exactly that. We keep on reducing the grid size by a factor of two again and again and again, using stride two convolutions with padding of one. So that's strides and padding. So let's go ahead and create a conv net using these approaches. So we're going to get our size of our training set. This is all the same as before. Number of categories, number of digits, size of our hidden layer. So previously with our sequential linear models, with our MLPs, we basically went from the number of pixels to the number of hidden, and then a value, and then the number of hidden to the number of outputs. So here's the equivalent with a convolution. Now the problem is that you can't just do that because the output is not now 10 probabilities for each item in our batch, but it's 10 probabilities for each item in our batch for each of 28 by 28 pixels, because we don't even have a stride or anything. So you can't just use the same simple approach that we had for MLP. We have to be a bit more careful. So to make life easier, let's create a little conv function that does a conv2d with a stride of two, optionally followed by an activation. So if act is true, we will add in a ReLU activation. So this is going to either return a conv2d or a little sequential containing a conv2d followed by a ReLU. And so now we can create a CNN from, you know, from scratch as a sequential model. And so since activation is true by default, this is going to take our 28 by 28 image, starting with one channel and creating an output of four channels. So this is the number of in. This is the number of filters. Sometimes we'll say filters to describe the number of kind of channels that our convolution has. That's the number of outputs. And it's very similar to the idea of the number of outputs in a linear layer, except this is the number of outputs in your convolution. So what I like to do when I create stuff like this is I add a little comment just to remind myself what is my grid size after this. So I had a 28 by 28 input. So then I've then put it through a stride2 conv. So the output of this will be 14 by 14. So then we'll do the same thing again, but this time we'll go from a four channel input to an eight channel output, and then from eight to 16. So by this point, we're now down to a four by four, and then down to a two by two. And then finally, we're down to a one by one. So on the very last layer, we won't add an activation. And the very last layer is going to create 10 outputs. And since we're now down to a one by one, we can just call flatten, and that's going to remove those unnecessary unit axes. So if we take that, pop our mini-batch through it, we end up with exactly what we want. A 16 by 10. So for each of our 16 images, we've got 10 probabilities of each possible digit. So if we take our training set and make it into 28 by 28 images, and we do the same thing for a validation set, and then we create two data sets, one for each, which we'll call train data set and valid data set. And we're now going to train this on the GPU. Now if you've got a Mac, you can use a device called, well if you've got an Apple Silicon Mac, you've got a device called MPS, which is going to use your Mac's GPU. Or if you've got Nvidia, you can use CUDA, which will use your Nvidia GPU. CUDA's 10 times or more, possibly much more faster than a Mac, so you definitely want to use Nvidia if you can. But if you're just running it on a Mac laptop or whatever, you can use MPS. So basically you want to know what device to use. Do we want to use CUDA or MPS? You can check. You can check torch.backends.mps.isavailable to see if you're running on a Mac with MPS. You can check torch.cuda.isavailable to see if you've got an Nvidia GPU, in which case you've got CUDA. And if you've got neither, of course you'll have to use the CPU to do computation. So I've created a little function here, to device, which takes a tensor or a dictionary or a list of tensors or whatever, and a device to move it to, and it just goes through and moves everything onto that device. Or if it's a dictionary, a dictionary of things, values moved onto that device. So there's your handy little function. And so we can create a custom collate function, which calls the PyTorch default collation function and then puts those tensors onto our device. And so with that, we've now got enough to run, train this neural net on the GPU. We created this getDls function in the last lesson. So we're going to use that, passing in the data sets that we just created, and our default collation function. We're going to create our optimizer using our CNNs parameters. And then we call fit. Now fit, remember, we also created in our last lesson. And it's done. So what I did then was I reduced the learning rate by a factor of four, and ran it again. And eventually, yeah, I got to a fairly similar accuracy to what we did on our MLP. So yeah, we've got a convolutional network working. I think that's pretty encouraging. And it's nice that to train it, we didn't have to write much code, right? We were able to use code that we had already built. We were able to use the data set class that we made, the getDls function that we made, and the fit function that we made. And you know, because those things are written in a fairly general way, they work just as well for a conv net as they did for an MLP. Nothing had to change. So that was nice. Notice I had to take the model and put it on the device as well. So that will go through and basically put all of the tensors that are in that model onto the MPS or CUDA device, if appropriate. So if we've got a batch size of 64, and as we do, one channel, 28 by 28, so then our axes are batch, channel, height, width. So normally, this is referred to as nchw. So n, generally when you see n in a paper or whatever, in this way, it's referring to the batch size. N being the number, that's the mnemonic, the number of items in the batch. C is the number of channels, height by width, nchw. TensorFlow doesn't use that. TensorFlow uses nhwc. So we generally call these, that channels last, since channels are at the end. And this one we normally call channels first. Now of course, it's not actually channels first. It's actually channel second, but we ignore the batch bit. In some models, particularly some more modern models, it turns out the channels last is faster. So PyTorch has recently added support for channels last, and so you'll see that being used more and more as well. All right, so a couple of comments and questions from our chat. The first is, Sam Watkins pointing out that we've actually had a bit of a win here, which is that the number of parameters in our CNN is pretty small by comparison. So in the MLP version, the number of parameters is equal to basically the size of this matrix. So m times nh. Right. Oh, plus the number in this, which will be nh times 10. And you know, something that at some point we probably should do is actually create something that allows us to automatically calculate the number of parameters. And I'm ignoring the bias there, of course. Let's see, what would be a good way to do that? Maybe np.product? There we go. So what we could do is just calculate this automatically by doing a little list comprehension here. So there's the number of parameters across all of the different layers, so both bias and weights. And then we could, I guess, just, well, we could just use, well, let's use PyTorch. So we could turn that into a tensor and sum it up. Oops. So that's the number in our MLP. And then the number in our simple CNN. So that's pretty cool. So we've gone down from 40,000 to 5,000 and got about the same number there. Oh, thank you, Jonathan. Jonathan's reminding me that there's a better way than np.product o.shape, which is just to say o.numberofelements numel. Same thing. Very nice. Now one person asked a very good question, which is, I thought convolutional neural networks can handle any sized image. And actually, no, this convolutional network cannot handle any sized image. This convolutional neural network only handles images that, once they go through these stride2 comms, end up with a 1x1. Because otherwise you can't dot flatten it and end up with 16x10. So we will learn how to create convnets that can handle any sized input, but there's nothing particularly about a convnet that necessitates that it has to be any sized input that it can handle. OK, so just let's briefly finish this section off by talking about this, particularly I want to talk about the idea of receptive field. Consider this one input channel, four output channel, three by three kernel. So that's just to show you what we're doing here. So simpleCNN, this is the model we created. Remember it was like a sequential model containing sequential models, because that's how our conv function works. So simpleCNN0 is our first layer, it contains both a conv and a relu. So simpleCNN00 is the actual conv. So if we grab that, call it conv1. It's a 4x1x3x3, so number of outputs, number of input channels, and height by width of the kernel, and then it's got its bias as well. So that's how we could kind of deconstruct what's going on with our weight matrices, or our parameters inside a convolution. Now I'm going to switch over to Excel. So in the lesson notes on the course website or on the forum, you'll find we've got an Excel workbook. Oh, Wassim reminded me that there is a nice trick we can do. I do want to do that actually, because I love this trick. Oh, I just deleted everything though. Let's put them all back. Here we go. Which is, you actually don't need square brackets. The square brackets is a list comprehension. Without the square brackets, it's called a generator. And it, oh no, you can't use it there. Maybe that only works with numpy. Maybe that only works with numpy. Ah, okay, so, wait, that's the list. No that doesn't work either. So much for that. I'm kind of curious now. Maybe torch.sum? Nope. Just sum? Oh, okay. I don't want to use Python's sum. That's interesting. I feel like all of them should handle generators, but there you go. Okay, so open up the conv example spreadsheet. And what you'll see on the conv example worksheet page is something that looks a lot like the number seven. So this is a number seven that I got straight from MNIST. Let's see. Okay, so you can see over here we have a number seven. This is a number seven from MNIST that I have copied into Excel. And then you can see over here we've got like a top edge kernel being applied, and over here we've got a right edge kernel being applied. This might be surprising you, because you might be thinking, wait a tick, Jeremy. If Excel doesn't do convolutional neural networks, well, actually it does. So if I zoom in in Excel, you'll see actually these numbers are in fact conditional formatting applied to a bunch of spreadsheet cells. And so what I did was I copied the actual pixel values into Excel, and then applied conditional formatting. And so now you can see what the digit is actually made of. So you can see here I've created our top edge filter. And here I've created our left edge filter. And so here I am applying that filter to that window. And so here you can see it looks a lot like NumPy, it's just a sum product. And you might not be aware of this, but in Excel you can actually do broadcasting. You have to hit Apple Shift Enter or Control Shift Enter, and it puts these little curly brackets around it. It's called an array formula. It basically lets you do broadcasting, or simple broadcasting, in Excel. And so this is how I created this top edge filtered version in Excel. And the left edge version is exactly the same, just a different kernel. And as you can see, if I click on it, it's applying this filter to this input area, and so forth. Okay, so we can then, I just arbitrarily picked some different values here. And so something to notice now in my second layer, so here's conv1, here's conv2, it's got a bit more work to do. We actually need two filters, because we need to add together this bit here, applied to this, with this kernel applied, and this bit here, with this kernel applied. So you actually need one set of 3x3 for each input, and also I want to set two separate outputs, so I actually end up needing a 2x2x3x3 weights matrix, or weights tensor, I should say, which you might remember is exactly what we had in PyTorch. We had a rank 4 tensor. So if I have a look at this one, you see exactly the same thing. This input is using this kernel applied to here, and this kernel applied to here. So that's important to remember, that you have these rank 4 tensors. And so then, rather than doing stride2 conv, I did something else, which is actually a bit out of favor nowadays, but it's another option, which is to do something called max pooling, to reduce my dimensionality. So you can see here I've got 28x28, I've reduced it down here to 14x14. And the way I did it was simply to take the max of each little 2x2 area. So that's all that's been done there. So that's called max pooling. And so max pooling has the same effect as a stride2 conv, not mathematically identical, the same effect, which is it does a convolution and reduces the grid size by 2 on each dimension. So then, how do we create a single output if we don't keep doing this until we get to 1x1, which I'm too lazy to do in Excel. Well, one approach, and again this is a little bit out of favor as well, but one approach we can do is we can take every one of these, we've now got 14x14, and apply a dense layer to it. And so what I've done here is I've got a big, imagine this is basically all being flattened out into a vector. And so here we've got sum product of this by this plus the sum product of this by this, and that gives us a single number. And so that is how we could then optimize that in order to optimize our weight matrices. Now, and then, you know, the more modern approach, we don't use this kind of dense layer much anymore, it still appears a bit. The main place that you see this used is in a network called VGG, which is very old now, I think it might be 2013 or something. But it's actually still used. And that's because for certain things like something called style transfer, or in general perceptual losses, people still find VGG seems to work better. So you still actually see this approach nowadays sometimes. The more common approach, however, nowadays is we take the penultimate layer, and we just simply take the average of all of the activations. So nowadays, we would simply, the Excel way of doing it would be literally simply say average of the penultimate layer. And that is called global average pooling. Everything has a fancy word, a fancy phrase, but that's all it is. Take the average, it's called global average pooling. Or you could take the max, whatever, that would be global max pooling. So anyway, the main reason I wanted to show you this was to do something which I think is pretty interesting, which is to take something in our, I'm just going to zoom out a little bit here. Let's take something in our max pool here. And I'm going to say trace precedence to show you, here it is, the area that it's coming from. Okay, so it's coming from these four numbers. Now if I trace precedence again, saying what's actually impacting this, obviously the kernel's impacting it. And then you can see that the input area here is a bit bigger. And then if I trace precedence again, then you can see the input area is bigger still. So this number here is calculated from all of these numbers in the input. This area in the input is called the receptive field of this unit. And so the receptive field in this case is one, two, three, four, five, six by six. And that means that a pixel way up here in the top right has literally no ability to impact that activation. It's not part of its receptive field. If you have a whole bunch of strad2coms, each time you have one, the receptive field is going to get twice as big. So the receptive field at the end of a deep network is actually very large. But the inputs closest to the middle of the receptive field have the biggest kind of say in the output, because they implicitly appear the most often in all of these kind of dot products that are inside this convolutional window. So the receptive field is not just like a single binary on-off thing. Literally all the stuff that's not got precedence here is not part of it at all. But the closer to the center of the receptive field, the more impact it's going to have, the more ability it's got to change this number. So the receptive field is a really important concept. And yeah, playing around with Excel's precedent arrows, I think is a nice way to to see that, at least in my opinion. And apart from anything else, it's great fun creating a convolutional neural network in Excel. I thought so anyway. Okay, so let's take a seven minute break. I'll see you back after that to talk about a convolutional autoencoder. All right. Okay, welcome back. We're going to have a look now at the autoencoder notebook. So we're just going to import all of our usual stuff. And we've got one more of our own modules to import now as well. And this time we are going to switch to a different, we're going to switch to a different data set, which is the fashion MNIST data set. We can take advantage of the stuff that we did in O5 datasets, and the HuggingFace stuff to load it. So we've seen this a little bit before, back in our datasets one here. And we never actually built any models with it. So let's first of all do that. So this is just going to convert each thing, each image into a tensor. That's going to be an in-place transform. Remember we created this decorator. And so we can call dataset dictionary with transform. This is all stuff we've done before. And so here we have our example of a sneaker. All right. And we will create our collation function. Collating a dictionary for that dataset. That's something to remind, you should remind yourself we built that ourselves in the datasets notebook. And let's actually make our collate function something that does to device, which we wrote in our last notebook. And we'll create a little data loaders function here, which is going to go through each item in the dataset dictionary and create a data loader for it and give us a dictionary of data loaders. Okay. So now we've got a data loader for training and a data loader for validation. So we can grab the X and Y batch by just calling next on that iterator, as we've done before. We can grab the, let's look at each of these in turn, actually. We've done all this before, but it's a couple of weeks ago. So just to remind you, we can get the names of the features. And so we can then get, create an item getter for our Ys. And we can, so we'll call that the label getter. We can apply that to our labels to get the titles of everything in our mini batch. And we can then call our show images that we created with that mini batch, with those titles. And here we have our fashion MNIST mini batch. Okay. So let's create a classifier. And we're just going to use exactly the same code copy and posted from the previous notebook. So here is our sequential model. And we are going to grab the parameters of the CNN. And the CNN, I've actually moved it over to the device. The default device was what we created in our last notebook. And as you can see, it's fitting. Now our first problem is it's fitting very slowly, which is kind of annoying. So why is it running pretty slowly? Let's think about, let's have a look at our data set. So when it's finally finished, let's take a look at an item from the data set. Actually let's not look at the data set. Let's actually go all the way back to the data set dictionary. So before it gets transformed, data set dictionary. And let's grab the training part of that. And let's grab one item. And actually we can see here the problem. For MNIST, we had all of the data loaded into memory into a single big tensor. But this HuggingFace one is created in a much more kind of normal way, which is each image is a totally separate PNG image. It's not all pre-converted into a single thing. Why is that a problem? Well, the reason it's a problem is that our data loader is spending all of its time decoding these PNGs. So if I train here. Okay so while I'm training, I can type Htop and you can see that basically my CPU is 100% used. Now that's a bit weird because I've actually got 64 CPUs. Why is it using just one of them is the first problem. But why does it matter that it's using 100% CPU? Well the reason it matters, let's run it again so you can see. Why does it matter that our CPU is 100% and why is it making it so slow? Well the reason why is if we look at NVIDIA SMI daemon, that will monitor our GPUs utilization. I've got three GPUs, so I say to choose just the zeroth index one. And you'll see this column here SM. This stands for symmetric multiprocessor. It's like the equivalent of like CPU usage. And generally we're only using up 1% of our one GPU. So no wonder it's so slow. So the first thing we want to do then is try to make things faster. Now to make things faster we want to be using more than one CPU to decode our PNGs. And as it turns out that's actually pretty easy to do. You just have to add a extra argument to your data loaders, which is here num underscore workers. And so I can say use eight CPUs for example. Now if I recreate the data loaders and then try to get the next one, uh oh, now I've got an error. And the error is rather quirky. And what it's saying is, oh, you're now trying to use multiple processors. And generally in Python and in PyTorch, using multiple processors things start to get complicated. And one of the things that absolutely just doesn't work is you can't actually have your data loader put things onto the GPU in your separate processors. It just doesn't work. So the reason for this error is actually because of the fact that we used a collate function that put things on the device. That's incompatible, unfortunately, with using multiple workers. So that's a problem. And the answer to that problem, sadly, is that we would have to actually rewrite our fit function entirely. So there's annoying thing number one. We don't want to be rewriting our fit function again and again. We want to have a single fit function. So okay, so there's a problem that we're going to have to think about. Problem number two is that this is not very accurate. 87%. Well, I mean, is it accurate? It's easy enough to find out. There's a really nice website called Papers with Code. And it will tell you a little leaderboard. And we can see whether we're any good. And the answer is we're not very good at all. So these papers had 96%, 94%, 92%. So yeah, we're not looking great. So how do we improve that? There's a lot of things we could try, but pretty much all of them are going to involve modifying our fit function again, and in reasonably complicated ways. So we've still got a bit of an issue there. Let's put that aside, because what we actually wanted to do is create an autoencoder. So to remind you about what an autoencoder is, and we're going to be able to go into a bit more detail now, we're going to start with our input image, which is going to be 28 by 28. It's a number 3, right? And it's a 28 by 28. And we're going to put it through, for example, a stride2.conv. And that's going to have an output of a 14 by 14. And we can have more channels. So say maybe, so this is 28 by 28 by 1. Let's do 14 by 14 by 2. So we've reduced the height and width by 2, but added an extra channel. So overall, this is a 2x decrease in parameters. And then we could do another stride2.conv, and that would give us a 7 by 7. And again, we can choose however many channels we want, but let's say we choose 4. So now compared to our original, we've now got a times 4 reduction. And so we could do that a few times, or we could just stay there. And so this is compressing. And so then what we could do is then somehow have a convolution layer, or group of layers, which does a convolution and also increases the size. There is actually something called a transposed convolution, which I'll leave you to look up if you're interested, which can do that. Also known as a, rather weirdly, a stride1.5 convolution. But there's actually a really simple way to do this, which is to say, let's say you've got a bunch of pixels. Let's say we've got a 3 by 3 pixels that looks like this. 1, 0, 1, 1, say. We could make that into a 6 by 6 very easily, which is we could simply, let's get these out. We could simply copy that pixel there into the first 4. Copy that pixel there into these 4. And so you can see, and then copy this pixel here into these 4. And so we're simply turning each pixel into 4 pixels. And so this is called nearest neighbor upsampling. Now that's not a convolution, that's just copying. But what we could then do is we could then apply a stride1 convolution to that. And that would allow us to double the grid size with a convolution. And that's what we're going to do. So our autoencoder is going to need a deconvolutional layer. And that's going to contain two layers. Upsampling nearest neighbor, scale factor of 2, followed by a conv2d with a stride of 1. OK. And you can see for padding, I just put kernel size slash slash 2. So that's a truncating division, because that always works for any odd-sized kernel. So as before, we will have an optional activation function. And then we will create a sequential using star layers. So that's going to pass in each layer as a separate argument, which is what sequential expects. OK. So, let's write a new fitness function. I just basically copied it over from our previous one, going through each epoch. But I've pulled out a vowel into a separate function. But it's basically doing the same thing. OK. So here is our autoencoder. And so we're going to... It's a bit tricky, because I wanted to go down by 1, 2, 3, to get to a 4 by 4 by 8. But starting at 28 by 28, you can't divide that three times and get an integer. So what I first do is I zero pad, so add padding of 2 on each side to get a 32 by 32 input. So if I then do a conv with two channel output, that gives us 16 by 16 by 2. And then again to get an 8 by 8 by 4. And then again to get a 4 by 4 by 8. So this is doing an 8x compression. And then we can call deconv to do exactly the same thing in reverse. The final one with no activation. And then we can truncate off those two pixels off the edge. Slightly surprisingly, PyTorch lets you pass negative 2 to zero padding to crop off the final two pixels. And then we'll add a sigmoid, which will force everything to go between 0 and 1, which of course is what we need. And then we will use mseloss to compare those pixels to our input pixels. And so a big difference we've got here now is that our loss function is being applied to the output of the model and itself. We don't have yb here. We have xb. So we're trying to recreate our original. And again this is a bit annoying that we have to create our own fit function. Anyway so we can now see what is the mseloss. And it's not going to be particularly human readable. But it's a number we can see if it goes down. And so then we can create... then we can do our SGD with the parameters of our autoencoder with mseloss, call that fit function we just wrote. And I won't wait for it to run, because as you can see it's really slow for reasons we've discussed. I've run it before. And what we want is to see that the original, which is here, gets recreated. And the answer is, oh, not really. I mean they're roughly the same things. But there's no point having an autoencoder which can't even recreate the originals. The idea would be that if these looked almost identical to these, then we'd say, wow, this is a fantastic network at compressing things by eight times. So I found this very fiddly to try and get this to work at all. Something that I discovered can get it to start training is to start with a really low learning rate for a few epochs, and then increase the learning rate after a few epochs. I mean, at least it gets it to train and show something vaguely sensible. But let's see. It still looks pretty crummy. This one here I got actually by switching to Atom, and I actually removed the tricky bit. I removed these two as well. But yeah, I couldn't get this to recreate anything very reasonable or any reasonable amount of time. And why is this not working very well? There's so many reasons it could be. Do we need a better optimizer? Do we need a better architecture? Do we need to use a variational autoencoder? You know, there's a thousand things we could try. But doing it like this is going to drive us crazy. We need to be able to really rapidly try things, and all kinds of different things. And so what I often see in projects or on Kaggle or whatever, people's code looks kind of like this. It looks a little like manual. And then their iteration speed is too slow. We need to be able to really rapidly try things. So we're not going to keep doing stuff manually anymore. This is where we take a halt, and we say, okay, let's build up a framework that we can use to rapidly try things and understand when things are working and when things aren't working. So we're going to start creating a learner. So what is a learner? It's basically the idea is this learner is going to be something that we build, which will allow us to try anything that we can imagine very quickly. And we will build then on top of that learner things that will allow us to introspect what's going on inside a model, will allow us to do multiprocess CUDA to go fast. It will allow us to add things like data augmentation. It will allow us to try a wide variety of architectures quickly, and so forth. So that's going to be the idea. And of course we're going to create it from scratch. And so let's start with fashion.mnist as before. And let's create a data loaders class, which is going to look a bit like what we had before, where we're just going to pass in, this couldn't be simpler, right? We're just going to pass in two data loaders and store them away. And I'm going to create a class method from dataset dictionary. And what that's going to do is it's going to call data loader on each of the dataset dictionary items with our batch size and instantiate our class. So if you haven't seen class method before, it's what allows us to say data loaders dot something in order to construct this. We could have put this in init just as well, but we'll be building more complex data loaders things later. So I thought we might start by getting the basic structure right. So this is all pretty much the same as what we've had before. I'm not doing anything on the device here, because as we know that didn't really work. Okay. Oh, this is an old thing. I don't need to coo anymore. So we're going to use to device, which I think came from... There we go. So here's an example of a very simple learner that fits on one screen. And this is basically going to replace our fit function. So a learner is going to be something that is going to train or learn a particular model using a particular set of data loaders, a particular loss function, some particular learning rate, and some particular optimizer or some particular optimization function. Now normally I, you know, most people would often kind of store each of these away separately by writing like self dot model equals model, blah, blah, blah. Right. So that's what we talked about before. That's, you know, that kind of huge amounts of boilerplate. It's more stuff that you can get wrong and it's more stuff to mean that you have to read to understand the code and yeah, don't like that kind of repetition. So instead we just call fastcore dot store attra to do that all in one line. Okay. So that's basically the idea with a class is to think about what's the information it's going to need. So you pass that all to the constructor, store it away. And then our fit function is going to, we've got the basic stuff that we have for keeping track of accuracy. This is only work for stuff that's a classification where we can use accuracy. Put the model on our device, create the optimizer, store how many epochs we're going through. Then for each epoch we'll call the one epoch function and the one epoch function we're going to either do train or evaluation. So we pass in true if we're training and false if we're evaluating. And they're basically almost the same. We basically set the model to training mode or not. We then decide whether to use the validation set or the training set based on whether we're training. And then we go through each batch in the data loader and call one batch. And one batch is then the thing which is going to put our batch onto the device, call our model, call our loss function. And then if we're training, then do our backward step, our optimizer step in our zero gradient. And then finally calculate our metrics or our stats. And so here's where we calculate our metrics. So that's basically what we have there. So let's go back to using an MLP. We call fit. And away it goes. This is an error here, pointed out by Kevin. Thank you. self.model.to. One thing I guess we could try now is we think that maybe we can use more than one process. So let's try that. Oh, it's so fast. I didn't even see. There it goes. You can see all four CPUs being used at once. Bang, it's done. Okay, so that's pretty great. Let's see how fast it looks here. Bump, bump. All right, lovely. Okay, so that's a good sign. So we've got a learner that can fit things, but it's not very flexible. It's not going to help us, for example, with our autoencoder, because there's no way of like changing which things are used for predicting with or for calculating with. We can't use it for anything except things that involve accuracy with a binary classification. Sorry, is that right? Sorry, yeah, a multi-class classification. It's not flexible at all, but it's a start. And so I wanted to basically put this all on one screen so you can see what the basic learner looks like. All right, so how do we do things other than multi-class accuracy? I decided to create a metric class. And basically a metric class is something where we are going to define subclasses of it that calculate particular metrics. So for example, here I've got a subclass of a metric called accuracy. So if you haven't done subclasses before, you can basically think of this as saying, please copy and paste all the code from here into here for me, but the bit that says def calc, replace it with this version. So in fact, this would be identical to copying and pasting this whole thing, typing accuracy here and replacing the definition of calc with that. That's what is happening here when we do subclassing. It's basically copying and pasting all that code in there for us. It's actually more powerful than that. There's more we can do with it. But in this case, this is all that's happening with this subclassing. And this is called... Actually, I'll leave that. That's fine. Okay, so the accuracy metric is here. And then this is kind of our really basic metric, which is we're going to use just for loss. And so what happens is we're going to... Let's for example create an accuracy metric object. We're basically going to add in many batches of data. So for example, here's a many batches of inputs and predictions. Here's another many batch of inputs and predictions. And then we're going to call .value and it will calculate the accuracy. Now .value is a neat little thing. It doesn't require parentheses after it because it's called a property. And so a property is something that just calculates automatically without having to put parentheses. That's all a property is. Well property getter, anyway. And so they look like this. You give it a name. And so we are going to be... Each time we call add, we are going to be storing that input and that target. And also the number of items in the many batch, optionally. For now that's just always going to be one. And you can see here that we then call .calc, which is going to call the accuracy calc. So just see how often they're equal. And then we're going to append to the list of values that calculation. And we're also going to append to the list of ends. In this case just one. And so then to calculate the value, we just do that. So that's all that's happening for accuracy. And then we can do for loss, we can just use metric directly. Because metric directly will just calculate the average of whatever it's passed. So we can say, oh, add the number 0.6, so the target's optional. And we're saying this is a mini batch of size 32. So that's going to be the end. And then add the value 0.9 with a mini batch size of 2. And then get the value. And as you can see, that's exactly the same as the weighted average of 0.6 and 0.9 with weights of 32 and 2. So we've created a metric class. And so that's something that we can use to create any metric we like just by overriding calc. Or we could create totally things from scratch as long as they have an add and a value. Okay, so we're now going to change our learner. And what we're going to do is we're going to keep the same basic structure. So there's going to be fit. It's going to go through each epoch. It's going to call one epoch passing in true and false as for training and validation. One epoch is going to go through each batch in the data loader and call one batch. One batch is going to do the prediction, get loss. And if it's training, it's going to do the backward step and zero grad. But there's a few other things going on. So let's take a look. Well, actually, let's just look at it in use first. So when we use it, we're going to be creating a learner with the model data loaders, loss function, learning rate, and some callbacks, which we'll learn about in a moment. And we call fit. And it's going to do our thing. And look, we're going to have charts and stuff. All right, so the basic idea is going to look very similar. So we're going to call fit. So when we construct it, we're going to be passing in exactly the same things as before. But we've got one extra thing, callbacks, which we'll see in a moment. Store the attributes as before. And we're going to be doing some stuff with the callbacks. So when we call fit for this number of epochs, we're going to store away how many epochs we're going to do. We're also going to store away the actual range that we're going to loop through as self.epoch. So here's that looping through self.epoch. We're going to create the optimizer using the optimizer function and the parameters. And then we're going to call underscore fit. Now what on earth is underscore fit? Why didn't we just copy and paste this into here? Why do this? It's because we've created this special decorator with callbacks. So what does that do? So it's up here with callbacks. With callbacks is a class. It's going to just store one thing, which is the name. In this case, the name is fit. And what it's going to do is, now this is the decorator, right? So when we call it, remember decorators get passed a function. So it's going to get passed this whole function. And that's going to be called f. So dunder call, remember, is what happens when a class is treated, an object is treated as if it's a function. So it's going to get passed this function. So this function is underscore fit. And so what we want to do is we want to return a different function. It's going to of course call the function that we were asked to call using the arguments and keyword arguments we were asked to use. But before it calls that function, it's going to call a special method called callback, passing in the string before, in this case, before underscore fit. After it's completed, it's going to call that method called callback and passing the string after underscore fit. And it's going to wrap the whole thing in a try except block. And it's going to be looking for an exception called cancel fit exception. And if it gets one, it's not going to complain. So let me explain what's going on with all of those things. Let's look at an example of a callback. So for example, here is a callback called device CB, device callback. And before fit will be called automatically before that underscore fit method is called. And it's going to put the model onto our device, CUDA or MPS, if we have one. Otherwise it'll just be on GPU. So what's going to happen here? So it's going to call, we're going to call fit, it's going to go through these lines of code, it's then going to call underscore fit. Underscore fit is not this function. Underscore fit is this function, where f is this function. So it's going to call our learner.callback passing in before underscore fit. And callback is defined here. What's callback going to do? It's going to be past the string before underscore fit. It's going to then go through each of our callbacks, sorted based on their order. And you can see here our callbacks can have an order. And it's going to look at that callback and try to get an attribute called before underscore fit. And it will find one. And so then it's going to call that method. Now if that method doesn't exist, it doesn't appear at all, then getAttr will return this instead. Identity is a function, just here. This is an identity function. All it does is whatever arguments it gets passed, it returns them. And if it's not passed any arguments, it just returns. So there's a lot of Python going on here. And that is why we did that foundations lesson. And so for people who haven't done a lot of this Python, there's going to be a lot of stuff to experiment with and learn about. And so do ask on the forums if any of these bits get confusing. But the best way to learn about these things is to open up this Jupyter notebook and try and create really simple versions of things. So for example, let's try identity. How exactly does identity work? I could call it and it gets nothing. I can call it with 1, it gets back 1. I could call it with a, it gets back a. I could call it with a1. And how is it doing that exactly? So remember, we can add a breakpoint. And this would be a great time to really test your debugging skills. So remember in our debugger, we can hit H to find out what the commands are. But you really should do a tutorial on the debugger if you're not familiar with it. And then we can step through each one. So I can now print args. And there's actually a trick which I like, is that args is actually a command, funnily enough, which will just tell you the arguments to any function, regardless of what they're called. Which is kind of nice. And so then we can step through by pressing N. And after this, we can check, like, OK, what is x now? And what is args now? So remember to really experiment with these things. So anyway, we're going to talk about this a lot more in the next lesson. But before that, if you're not familiar with try-except blocks, you know, spend some time practicing them. If you're not familiar with decorators, well, we've seen them before. So go back and look at them again really carefully. If you're not familiar with the debugger, practice with that. If you haven't spent much time with getattra, remind yourself about that. So try to get yourself really familiar and comfortable, as much as possible, with the pieces. Because if you're not comfortable with the pieces, then the way we put the pieces together is going to be confusing. There's actually something in education, in kind of the theory of education, called cognitive load theory. And the theory of cognitive, basically, cognitive load theory says if you're trying to learn something but your cognitive load is really high because of all lots of other things going on at the same time, you're not going to learn it. So it's going to be hard for you to learn this framework that we're building if you have too much cognitive load of like, what the hell's a decorator, or what the hell's getattra, or what does sorted do, or what's partial, you know, all these things. Now I actually spent quite a bit of time trying to make this as simple as possible, but also as flexible as it needs to be for the rest of the course. And this is as simple as I could get it. So these are kind of things that you actually do have to learn. But in doing so, you're going to be able to write some really, you know, powerful and general code yourself. So hopefully you'll find this a really valuable and mind-expanding exercise in bringing high-level software engineering skills to your data science work. Okay, so with that, this looks like a good place to leave it, and look forward to seeing you next time. Bye!", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.16, "text": " Hi all and welcome to lesson 15.", "tokens": [50364, 2421, 439, 293, 2928, 281, 6898, 2119, 13, 50572], "temperature": 0.0, "avg_logprob": -0.2778888562830483, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.00022867121151648462}, {"id": 1, "seek": 0, "start": 4.16, "end": 13.24, "text": " And what we're going to endeavor to do today is to create a convolutional autoencoder.", "tokens": [50572, 400, 437, 321, 434, 516, 281, 34975, 281, 360, 965, 307, 281, 1884, 257, 45216, 304, 8399, 22660, 19866, 13, 51026], "temperature": 0.0, "avg_logprob": -0.2778888562830483, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.00022867121151648462}, {"id": 2, "seek": 0, "start": 13.24, "end": 20.6, "text": " And in the process we will see why doing that well is a tricky thing to do.", "tokens": [51026, 400, 294, 264, 1399, 321, 486, 536, 983, 884, 300, 731, 307, 257, 12414, 551, 281, 360, 13, 51394], "temperature": 0.0, "avg_logprob": -0.2778888562830483, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.00022867121151648462}, {"id": 3, "seek": 0, "start": 20.6, "end": 26.76, "text": " And time permitting we will begin to work on a framework, a deep learning framework", "tokens": [51394, 400, 565, 4784, 2414, 321, 486, 1841, 281, 589, 322, 257, 8388, 11, 257, 2452, 2539, 8388, 51702], "temperature": 0.0, "avg_logprob": -0.2778888562830483, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.00022867121151648462}, {"id": 4, "seek": 0, "start": 26.76, "end": 29.92, "text": " to make life a lot easier.", "tokens": [51702, 281, 652, 993, 257, 688, 3571, 13, 51860], "temperature": 0.0, "avg_logprob": -0.2778888562830483, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.00022867121151648462}, {"id": 5, "seek": 2992, "start": 29.92, "end": 34.36, "text": " Not sure how far we'll get on that today time wise, so let's see how we go and get", "tokens": [50364, 1726, 988, 577, 1400, 321, 603, 483, 322, 300, 965, 565, 10829, 11, 370, 718, 311, 536, 577, 321, 352, 293, 483, 50586], "temperature": 0.0, "avg_logprob": -0.3025599161783854, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0003981754998676479}, {"id": 6, "seek": 2992, "start": 34.36, "end": 37.04, "text": " straight into it.", "tokens": [50586, 2997, 666, 309, 13, 50720], "temperature": 0.0, "avg_logprob": -0.3025599161783854, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0003981754998676479}, {"id": 7, "seek": 2992, "start": 37.04, "end": 45.18, "text": " So okay, so today let's start by talking, before we can create a convolutional autoencoder,", "tokens": [50720, 407, 1392, 11, 370, 965, 718, 311, 722, 538, 1417, 11, 949, 321, 393, 1884, 257, 45216, 304, 8399, 22660, 19866, 11, 51127], "temperature": 0.0, "avg_logprob": -0.3025599161783854, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0003981754998676479}, {"id": 8, "seek": 2992, "start": 45.18, "end": 52.400000000000006, "text": " we need to talk about convolutions and what are they and what are they for.", "tokens": [51127, 321, 643, 281, 751, 466, 3754, 15892, 293, 437, 366, 436, 293, 437, 366, 436, 337, 13, 51488], "temperature": 0.0, "avg_logprob": -0.3025599161783854, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0003981754998676479}, {"id": 9, "seek": 5240, "start": 52.4, "end": 60.56, "text": " Broadly speaking, convolutions are something that allows us to tell our neural network", "tokens": [50364, 14074, 356, 4124, 11, 3754, 15892, 366, 746, 300, 4045, 505, 281, 980, 527, 18161, 3209, 50772], "temperature": 0.0, "avg_logprob": -0.2334453264872233, "compression_ratio": 1.64, "no_speech_prob": 0.0006070543313398957}, {"id": 10, "seek": 5240, "start": 60.56, "end": 65.16, "text": " a little bit about the structure of the problem that's going to make it a lot easier for it", "tokens": [50772, 257, 707, 857, 466, 264, 3877, 295, 264, 1154, 300, 311, 516, 281, 652, 309, 257, 688, 3571, 337, 309, 51002], "temperature": 0.0, "avg_logprob": -0.2334453264872233, "compression_ratio": 1.64, "no_speech_prob": 0.0006070543313398957}, {"id": 11, "seek": 5240, "start": 65.16, "end": 66.64, "text": " to solve the problem.", "tokens": [51002, 281, 5039, 264, 1154, 13, 51076], "temperature": 0.0, "avg_logprob": -0.2334453264872233, "compression_ratio": 1.64, "no_speech_prob": 0.0006070543313398957}, {"id": 12, "seek": 5240, "start": 66.64, "end": 71.28, "text": " And in particular the structure of our problem is we're doing things with images.", "tokens": [51076, 400, 294, 1729, 264, 3877, 295, 527, 1154, 307, 321, 434, 884, 721, 365, 5267, 13, 51308], "temperature": 0.0, "avg_logprob": -0.2334453264872233, "compression_ratio": 1.64, "no_speech_prob": 0.0006070543313398957}, {"id": 13, "seek": 5240, "start": 71.28, "end": 81.72, "text": " Images are laid out on a grid, a 2D grid for black and white or a 3D for color or a 4D", "tokens": [51308, 4331, 1660, 366, 9897, 484, 322, 257, 10748, 11, 257, 568, 35, 10748, 337, 2211, 293, 2418, 420, 257, 805, 35, 337, 2017, 420, 257, 1017, 35, 51830], "temperature": 0.0, "avg_logprob": -0.2334453264872233, "compression_ratio": 1.64, "no_speech_prob": 0.0006070543313398957}, {"id": 14, "seek": 8172, "start": 81.72, "end": 84.76, "text": " for a color video or whatever.", "tokens": [50364, 337, 257, 2017, 960, 420, 2035, 13, 50516], "temperature": 0.0, "avg_logprob": -0.2525237931145562, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.832558988709934e-05}, {"id": 15, "seek": 8172, "start": 84.76, "end": 90.32, "text": " And so we would say, you know, there's a relationship between the pixels going across and the pixels", "tokens": [50516, 400, 370, 321, 576, 584, 11, 291, 458, 11, 456, 311, 257, 2480, 1296, 264, 18668, 516, 2108, 293, 264, 18668, 50794], "temperature": 0.0, "avg_logprob": -0.2525237931145562, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.832558988709934e-05}, {"id": 16, "seek": 8172, "start": 90.32, "end": 91.32, "text": " going down.", "tokens": [50794, 516, 760, 13, 50844], "temperature": 0.0, "avg_logprob": -0.2525237931145562, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.832558988709934e-05}, {"id": 17, "seek": 8172, "start": 91.32, "end": 93.8, "text": " They tend to be similar to each other.", "tokens": [50844, 814, 3928, 281, 312, 2531, 281, 1184, 661, 13, 50968], "temperature": 0.0, "avg_logprob": -0.2525237931145562, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.832558988709934e-05}, {"id": 18, "seek": 8172, "start": 93.8, "end": 99.75999999999999, "text": " Differences in those pixels across those dimensions tend to have meaning.", "tokens": [50968, 413, 12612, 2667, 294, 729, 18668, 2108, 729, 12819, 3928, 281, 362, 3620, 13, 51266], "temperature": 0.0, "avg_logprob": -0.2525237931145562, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.832558988709934e-05}, {"id": 19, "seek": 8172, "start": 99.75999999999999, "end": 104.68, "text": " Sets patterns of pixels that appear in different places often represent the same thing.", "tokens": [51266, 318, 1385, 8294, 295, 18668, 300, 4204, 294, 819, 3190, 2049, 2906, 264, 912, 551, 13, 51512], "temperature": 0.0, "avg_logprob": -0.2525237931145562, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.832558988709934e-05}, {"id": 20, "seek": 8172, "start": 104.68, "end": 109.56, "text": " So for example a cat in the top left is still a cat even if it's in the bottom right.", "tokens": [51512, 407, 337, 1365, 257, 3857, 294, 264, 1192, 1411, 307, 920, 257, 3857, 754, 498, 309, 311, 294, 264, 2767, 558, 13, 51756], "temperature": 0.0, "avg_logprob": -0.2525237931145562, "compression_ratio": 1.7131474103585658, "no_speech_prob": 4.832558988709934e-05}, {"id": 21, "seek": 10956, "start": 109.56, "end": 117.84, "text": " These kinds of prior information is something that is naturally captured by a convolutional", "tokens": [50364, 1981, 3685, 295, 4059, 1589, 307, 746, 300, 307, 8195, 11828, 538, 257, 45216, 304, 50778], "temperature": 0.0, "avg_logprob": -0.24899625174606901, "compression_ratio": 1.7072072072072073, "no_speech_prob": 3.535619907779619e-05}, {"id": 22, "seek": 10956, "start": 117.84, "end": 121.8, "text": " neural network, something that uses convolutions.", "tokens": [50778, 18161, 3209, 11, 746, 300, 4960, 3754, 15892, 13, 50976], "temperature": 0.0, "avg_logprob": -0.24899625174606901, "compression_ratio": 1.7072072072072073, "no_speech_prob": 3.535619907779619e-05}, {"id": 23, "seek": 10956, "start": 121.8, "end": 126.1, "text": " Generally speaking this is a good thing because it means that we will be able to use less", "tokens": [50976, 21082, 4124, 341, 307, 257, 665, 551, 570, 309, 1355, 300, 321, 486, 312, 1075, 281, 764, 1570, 51191], "temperature": 0.0, "avg_logprob": -0.24899625174606901, "compression_ratio": 1.7072072072072073, "no_speech_prob": 3.535619907779619e-05}, {"id": 24, "seek": 10956, "start": 126.1, "end": 131.72, "text": " parameters and less computation because more of that information about the problem we're", "tokens": [51191, 9834, 293, 1570, 24903, 570, 544, 295, 300, 1589, 466, 264, 1154, 321, 434, 51472], "temperature": 0.0, "avg_logprob": -0.24899625174606901, "compression_ratio": 1.7072072072072073, "no_speech_prob": 3.535619907779619e-05}, {"id": 25, "seek": 10956, "start": 131.72, "end": 137.4, "text": " solving is kind of encoded directly into our architecture.", "tokens": [51472, 12606, 307, 733, 295, 2058, 12340, 3838, 666, 527, 9482, 13, 51756], "temperature": 0.0, "avg_logprob": -0.24899625174606901, "compression_ratio": 1.7072072072072073, "no_speech_prob": 3.535619907779619e-05}, {"id": 26, "seek": 13740, "start": 137.4, "end": 144.32, "text": " There are other architectures that don't encode that prior information as strongly,", "tokens": [50364, 821, 366, 661, 6331, 1303, 300, 500, 380, 2058, 1429, 300, 4059, 1589, 382, 10613, 11, 50710], "temperature": 0.0, "avg_logprob": -0.26916998624801636, "compression_ratio": 1.6285714285714286, "no_speech_prob": 6.814755033701658e-05}, {"id": 27, "seek": 13740, "start": 144.32, "end": 148.44, "text": " such as a multi-layer perceptron, which we've been looking at so far, or a transformer's", "tokens": [50710, 1270, 382, 257, 4825, 12, 8376, 260, 43276, 2044, 11, 597, 321, 600, 668, 1237, 412, 370, 1400, 11, 420, 257, 31782, 311, 50916], "temperature": 0.0, "avg_logprob": -0.26916998624801636, "compression_ratio": 1.6285714285714286, "no_speech_prob": 6.814755033701658e-05}, {"id": 28, "seek": 13740, "start": 148.44, "end": 152.08, "text": " network which we haven't looked at yet.", "tokens": [50916, 3209, 597, 321, 2378, 380, 2956, 412, 1939, 13, 51098], "temperature": 0.0, "avg_logprob": -0.26916998624801636, "compression_ratio": 1.6285714285714286, "no_speech_prob": 6.814755033701658e-05}, {"id": 29, "seek": 13740, "start": 152.08, "end": 158.92000000000002, "text": " Those kinds of architectures could potentially give us, well they do give us more flexibility.", "tokens": [51098, 3950, 3685, 295, 6331, 1303, 727, 7263, 976, 505, 11, 731, 436, 360, 976, 505, 544, 12635, 13, 51440], "temperature": 0.0, "avg_logprob": -0.26916998624801636, "compression_ratio": 1.6285714285714286, "no_speech_prob": 6.814755033701658e-05}, {"id": 30, "seek": 13740, "start": 158.92000000000002, "end": 165.28, "text": " And given enough time, compute and data, they could potentially find things that maybe CNNs", "tokens": [51440, 400, 2212, 1547, 565, 11, 14722, 293, 1412, 11, 436, 727, 7263, 915, 721, 300, 1310, 24859, 82, 51758], "temperature": 0.0, "avg_logprob": -0.26916998624801636, "compression_ratio": 1.6285714285714286, "no_speech_prob": 6.814755033701658e-05}, {"id": 31, "seek": 16528, "start": 165.28, "end": 168.64000000000001, "text": " would struggle to find.", "tokens": [50364, 576, 7799, 281, 915, 13, 50532], "temperature": 0.0, "avg_logprob": -0.27702554842320887, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.0005442057154141366}, {"id": 32, "seek": 16528, "start": 168.64000000000001, "end": 173.0, "text": " So we're not always going to use convolutional neural networks, but they're a pretty good", "tokens": [50532, 407, 321, 434, 406, 1009, 516, 281, 764, 45216, 304, 18161, 9590, 11, 457, 436, 434, 257, 1238, 665, 50750], "temperature": 0.0, "avg_logprob": -0.27702554842320887, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.0005442057154141366}, {"id": 33, "seek": 16528, "start": 173.0, "end": 176.72, "text": " starting point and certainly something important to understand.", "tokens": [50750, 2891, 935, 293, 3297, 746, 1021, 281, 1223, 13, 50936], "temperature": 0.0, "avg_logprob": -0.27702554842320887, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.0005442057154141366}, {"id": 34, "seek": 16528, "start": 176.72, "end": 183.76, "text": " They're not just used for images, we can also take advantage of one-dimensional convolutions", "tokens": [50936, 814, 434, 406, 445, 1143, 337, 5267, 11, 321, 393, 611, 747, 5002, 295, 472, 12, 18759, 3754, 15892, 51288], "temperature": 0.0, "avg_logprob": -0.27702554842320887, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.0005442057154141366}, {"id": 35, "seek": 16528, "start": 183.76, "end": 187.2, "text": " for language-based tasks for instance.", "tokens": [51288, 337, 2856, 12, 6032, 9608, 337, 5197, 13, 51460], "temperature": 0.0, "avg_logprob": -0.27702554842320887, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.0005442057154141366}, {"id": 36, "seek": 16528, "start": 187.2, "end": 191.58, "text": " So convolutions come up a lot.", "tokens": [51460, 407, 3754, 15892, 808, 493, 257, 688, 13, 51679], "temperature": 0.0, "avg_logprob": -0.27702554842320887, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.0005442057154141366}, {"id": 37, "seek": 19158, "start": 191.58, "end": 198.14000000000001, "text": " So in this notebook, one thing you'll notice that might be of interest is we are importing", "tokens": [50364, 407, 294, 341, 21060, 11, 472, 551, 291, 603, 3449, 300, 1062, 312, 295, 1179, 307, 321, 366, 43866, 50692], "temperature": 0.0, "avg_logprob": -0.25570685584265906, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.000732166925445199}, {"id": 38, "seek": 19158, "start": 198.14000000000001, "end": 200.5, "text": " stuff from MiniAI now.", "tokens": [50692, 1507, 490, 18239, 48698, 586, 13, 50810], "temperature": 0.0, "avg_logprob": -0.25570685584265906, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.000732166925445199}, {"id": 39, "seek": 19158, "start": 200.5, "end": 204.78, "text": " Now MiniAI is this little library that we're starting to create.", "tokens": [50810, 823, 18239, 48698, 307, 341, 707, 6405, 300, 321, 434, 2891, 281, 1884, 13, 51024], "temperature": 0.0, "avg_logprob": -0.25570685584265906, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.000732166925445199}, {"id": 40, "seek": 19158, "start": 204.78, "end": 207.28, "text": " And we're creating it using nbdev.", "tokens": [51024, 400, 321, 434, 4084, 309, 1228, 297, 65, 40343, 13, 51149], "temperature": 0.0, "avg_logprob": -0.25570685584265906, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.000732166925445199}, {"id": 41, "seek": 19158, "start": 207.28, "end": 211.46, "text": " So we've got a miniai.training and a miniai.datasets.", "tokens": [51149, 407, 321, 600, 658, 257, 8382, 1301, 13, 17227, 1760, 293, 257, 8382, 1301, 13, 20367, 296, 1385, 13, 51358], "temperature": 0.0, "avg_logprob": -0.25570685584265906, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.000732166925445199}, {"id": 42, "seek": 19158, "start": 211.46, "end": 216.42000000000002, "text": " And so if we look for example at the datasets notebook, it starts with something that says", "tokens": [51358, 400, 370, 498, 321, 574, 337, 1365, 412, 264, 42856, 21060, 11, 309, 3719, 365, 746, 300, 1619, 51606], "temperature": 0.0, "avg_logprob": -0.25570685584265906, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.000732166925445199}, {"id": 43, "seek": 19158, "start": 216.42000000000002, "end": 221.26000000000002, "text": " that the default export module is called datasets.", "tokens": [51606, 300, 264, 7576, 10725, 10088, 307, 1219, 42856, 13, 51848], "temperature": 0.0, "avg_logprob": -0.25570685584265906, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.000732166925445199}, {"id": 44, "seek": 22126, "start": 221.94, "end": 227.26, "text": " And some of the cells have a export directive on them.", "tokens": [50398, 400, 512, 295, 264, 5438, 362, 257, 10725, 45444, 322, 552, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2550540864467621, "compression_ratio": 1.5379310344827586, "no_speech_prob": 1.3846001820638776e-05}, {"id": 45, "seek": 22126, "start": 227.26, "end": 232.7, "text": " And at the very bottom we had something that called nbdev export.", "tokens": [50664, 400, 412, 264, 588, 2767, 321, 632, 746, 300, 1219, 297, 65, 40343, 10725, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2550540864467621, "compression_ratio": 1.5379310344827586, "no_speech_prob": 1.3846001820638776e-05}, {"id": 46, "seek": 22126, "start": 232.7, "end": 243.82, "text": " Now what that's going to do is it's going to create a file called datasets.py.", "tokens": [50936, 823, 437, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 1884, 257, 3991, 1219, 42856, 13, 8200, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2550540864467621, "compression_ratio": 1.5379310344827586, "no_speech_prob": 1.3846001820638776e-05}, {"id": 47, "seek": 22126, "start": 243.82, "end": 248.66, "text": " Just here, datasets.py.", "tokens": [51492, 1449, 510, 11, 42856, 13, 8200, 13, 51734], "temperature": 0.0, "avg_logprob": -0.2550540864467621, "compression_ratio": 1.5379310344827586, "no_speech_prob": 1.3846001820638776e-05}, {"id": 48, "seek": 24866, "start": 248.66, "end": 257.42, "text": " And it contains those cells that we exported.", "tokens": [50364, 400, 309, 8306, 729, 5438, 300, 321, 42055, 13, 50802], "temperature": 0.0, "avg_logprob": -0.23649519843024178, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00016092992154881358}, {"id": 49, "seek": 24866, "start": 257.42, "end": 261.86, "text": " And why is it called miniai.datasets?", "tokens": [50802, 400, 983, 307, 309, 1219, 8382, 1301, 13, 20367, 296, 1385, 30, 51024], "temperature": 0.0, "avg_logprob": -0.23649519843024178, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00016092992154881358}, {"id": 50, "seek": 24866, "start": 261.86, "end": 265.7, "text": " That's because everything for nbdev is stored in settings.any.", "tokens": [51024, 663, 311, 570, 1203, 337, 297, 65, 40343, 307, 12187, 294, 6257, 13, 1325, 13, 51216], "temperature": 0.0, "avg_logprob": -0.23649519843024178, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00016092992154881358}, {"id": 51, "seek": 24866, "start": 265.7, "end": 272.46, "text": " And there's something here saying create a library lib name called miniai.", "tokens": [51216, 400, 456, 311, 746, 510, 1566, 1884, 257, 6405, 22854, 1315, 1219, 8382, 1301, 13, 51554], "temperature": 0.0, "avg_logprob": -0.23649519843024178, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00016092992154881358}, {"id": 52, "seek": 24866, "start": 272.46, "end": 275.42, "text": " You can't use this library until you install it.", "tokens": [51554, 509, 393, 380, 764, 341, 6405, 1826, 291, 3625, 309, 13, 51702], "temperature": 0.0, "avg_logprob": -0.23649519843024178, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00016092992154881358}, {"id": 53, "seek": 27542, "start": 275.46000000000004, "end": 282.46000000000004, "text": " Now we haven't uploaded it to PyPy, like we made a pip installable package from a public", "tokens": [50366, 823, 321, 2378, 380, 17135, 309, 281, 9953, 47, 88, 11, 411, 321, 1027, 257, 8489, 3625, 712, 7372, 490, 257, 1908, 50716], "temperature": 0.0, "avg_logprob": -0.3225000251844091, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0038243771996349096}, {"id": 54, "seek": 27542, "start": 282.46000000000004, "end": 283.66, "text": " server.", "tokens": [50716, 7154, 13, 50776], "temperature": 0.0, "avg_logprob": -0.3225000251844091, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0038243771996349096}, {"id": 55, "seek": 27542, "start": 283.66, "end": 292.22, "text": " But you can actually install a local directory as if it's a Python module that you've kind", "tokens": [50776, 583, 291, 393, 767, 3625, 257, 2654, 21120, 382, 498, 309, 311, 257, 15329, 10088, 300, 291, 600, 733, 51204], "temperature": 0.0, "avg_logprob": -0.3225000251844091, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0038243771996349096}, {"id": 56, "seek": 27542, "start": 292.22, "end": 293.78000000000003, "text": " of installed from the internet.", "tokens": [51204, 295, 8899, 490, 264, 4705, 13, 51282], "temperature": 0.0, "avg_logprob": -0.3225000251844091, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0038243771996349096}, {"id": 57, "seek": 27542, "start": 293.78000000000003, "end": 297.5, "text": " And to do that you say pip install in the usual way.", "tokens": [51282, 400, 281, 360, 300, 291, 584, 8489, 3625, 294, 264, 7713, 636, 13, 51468], "temperature": 0.0, "avg_logprob": -0.3225000251844091, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0038243771996349096}, {"id": 58, "seek": 27542, "start": 297.5, "end": 300.14, "text": " But you say minus e, that stands for editable.", "tokens": [51468, 583, 291, 584, 3175, 308, 11, 300, 7382, 337, 8129, 712, 13, 51600], "temperature": 0.0, "avg_logprob": -0.3225000251844091, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0038243771996349096}, {"id": 59, "seek": 27542, "start": 300.14, "end": 304.34000000000003, "text": " And that means set up the current directory as a Python module.", "tokens": [51600, 400, 300, 1355, 992, 493, 264, 2190, 21120, 382, 257, 15329, 10088, 13, 51810], "temperature": 0.0, "avg_logprob": -0.3225000251844091, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0038243771996349096}, {"id": 60, "seek": 30434, "start": 304.34, "end": 306.14, "text": " Well current directory, actually any directory you like.", "tokens": [50364, 1042, 2190, 21120, 11, 767, 604, 21120, 291, 411, 13, 50454], "temperature": 0.0, "avg_logprob": -0.32004594802856445, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.0011335666058585048}, {"id": 61, "seek": 30434, "start": 306.14, "end": 308.65999999999997, "text": " I'll just put dot to mean the current directory.", "tokens": [50454, 286, 603, 445, 829, 5893, 281, 914, 264, 2190, 21120, 13, 50580], "temperature": 0.0, "avg_logprob": -0.32004594802856445, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.0011335666058585048}, {"id": 62, "seek": 30434, "start": 308.65999999999997, "end": 315.5, "text": " And so you'll see that's going to go ahead and actually install my library.", "tokens": [50580, 400, 370, 291, 603, 536, 300, 311, 516, 281, 352, 2286, 293, 767, 3625, 452, 6405, 13, 50922], "temperature": 0.0, "avg_logprob": -0.32004594802856445, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.0011335666058585048}, {"id": 63, "seek": 30434, "start": 315.5, "end": 329.97999999999996, "text": " And so after I've done that, I can now import things from that library, as you see.", "tokens": [50922, 400, 370, 934, 286, 600, 1096, 300, 11, 286, 393, 586, 974, 721, 490, 300, 6405, 11, 382, 291, 536, 13, 51646], "temperature": 0.0, "avg_logprob": -0.32004594802856445, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.0011335666058585048}, {"id": 64, "seek": 30434, "start": 329.97999999999996, "end": 331.21999999999997, "text": " So this is just the same as before.", "tokens": [51646, 407, 341, 307, 445, 264, 912, 382, 949, 13, 51708], "temperature": 0.0, "avg_logprob": -0.32004594802856445, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.0011335666058585048}, {"id": 65, "seek": 33122, "start": 331.22, "end": 335.3, "text": " We're going to grab our MNIST dataset, and we're going to create a convolutional neural", "tokens": [50364, 492, 434, 516, 281, 4444, 527, 376, 45, 19756, 28872, 11, 293, 321, 434, 516, 281, 1884, 257, 45216, 304, 18161, 50568], "temperature": 0.0, "avg_logprob": -0.26959612989050197, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.002672960516065359}, {"id": 66, "seek": 33122, "start": 335.3, "end": 336.3, "text": " network on it.", "tokens": [50568, 3209, 322, 309, 13, 50618], "temperature": 0.0, "avg_logprob": -0.26959612989050197, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.002672960516065359}, {"id": 67, "seek": 33122, "start": 336.3, "end": 340.62, "text": " So before we do that, we're going to talk about what are convolutions.", "tokens": [50618, 407, 949, 321, 360, 300, 11, 321, 434, 516, 281, 751, 466, 437, 366, 3754, 15892, 13, 50834], "temperature": 0.0, "avg_logprob": -0.26959612989050197, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.002672960516065359}, {"id": 68, "seek": 33122, "start": 340.62, "end": 345.02000000000004, "text": " And one of my favorite descriptions of convolutions comes from the student in our, I think it", "tokens": [50834, 400, 472, 295, 452, 2954, 24406, 295, 3754, 15892, 1487, 490, 264, 3107, 294, 527, 11, 286, 519, 309, 51054], "temperature": 0.0, "avg_logprob": -0.26959612989050197, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.002672960516065359}, {"id": 69, "seek": 33122, "start": 345.02000000000004, "end": 353.1, "text": " was our very first course, Matt Clinesmith, who wrote this really nice Medium article,", "tokens": [51054, 390, 527, 588, 700, 1164, 11, 7397, 2033, 1652, 41708, 11, 567, 4114, 341, 534, 1481, 38915, 7222, 11, 51458], "temperature": 0.0, "avg_logprob": -0.26959612989050197, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.002672960516065359}, {"id": 70, "seek": 33122, "start": 353.1, "end": 356.22, "text": " CNNs from different viewpoints, which I'm going to steal from.", "tokens": [51458, 24859, 82, 490, 819, 1910, 20552, 11, 597, 286, 478, 516, 281, 11009, 490, 13, 51614], "temperature": 0.0, "avg_logprob": -0.26959612989050197, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.002672960516065359}, {"id": 71, "seek": 33122, "start": 356.22, "end": 357.34000000000003, "text": " And here's the basic idea.", "tokens": [51614, 400, 510, 311, 264, 3875, 1558, 13, 51670], "temperature": 0.0, "avg_logprob": -0.26959612989050197, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.002672960516065359}, {"id": 72, "seek": 33122, "start": 357.34000000000003, "end": 360.3, "text": " Say that this is our image.", "tokens": [51670, 6463, 300, 341, 307, 527, 3256, 13, 51818], "temperature": 0.0, "avg_logprob": -0.26959612989050197, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.002672960516065359}, {"id": 73, "seek": 36030, "start": 360.38, "end": 368.14, "text": " It's a 3x3 image with 9 pixels labeled from A to J as capital letters.", "tokens": [50368, 467, 311, 257, 805, 87, 18, 3256, 365, 1722, 18668, 21335, 490, 316, 281, 508, 382, 4238, 7825, 13, 50756], "temperature": 0.0, "avg_logprob": -0.31781380752037314, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.0001511818845756352}, {"id": 74, "seek": 36030, "start": 368.14, "end": 371.7, "text": " Now a convolution uses something called a kernel.", "tokens": [50756, 823, 257, 45216, 4960, 746, 1219, 257, 28256, 13, 50934], "temperature": 0.0, "avg_logprob": -0.31781380752037314, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.0001511818845756352}, {"id": 75, "seek": 36030, "start": 371.7, "end": 376.02000000000004, "text": " And a kernel is just another tensor.", "tokens": [50934, 400, 257, 28256, 307, 445, 1071, 40863, 13, 51150], "temperature": 0.0, "avg_logprob": -0.31781380752037314, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.0001511818845756352}, {"id": 76, "seek": 36030, "start": 376.02000000000004, "end": 378.62, "text": " In this case, it's a 2x2 matrix.", "tokens": [51150, 682, 341, 1389, 11, 309, 311, 257, 568, 87, 17, 8141, 13, 51280], "temperature": 0.0, "avg_logprob": -0.31781380752037314, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.0001511818845756352}, {"id": 77, "seek": 36030, "start": 378.62, "end": 386.62, "text": " Again so in this one we're going to have alpha, beta, gamma, delta as our four values in this", "tokens": [51280, 3764, 370, 294, 341, 472, 321, 434, 516, 281, 362, 8961, 11, 9861, 11, 15546, 11, 8289, 382, 527, 1451, 4190, 294, 341, 51680], "temperature": 0.0, "avg_logprob": -0.31781380752037314, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.0001511818845756352}, {"id": 78, "seek": 36030, "start": 386.62, "end": 387.62, "text": " convolution.", "tokens": [51680, 45216, 13, 51730], "temperature": 0.0, "avg_logprob": -0.31781380752037314, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.0001511818845756352}, {"id": 79, "seek": 38762, "start": 388.62, "end": 391.86, "text": " Now in this kernel, oh, now one thing I'll mention, I can't remember if I've said this", "tokens": [50414, 823, 294, 341, 28256, 11, 1954, 11, 586, 472, 551, 286, 603, 2152, 11, 286, 393, 380, 1604, 498, 286, 600, 848, 341, 50576], "temperature": 0.0, "avg_logprob": -0.2787979973687066, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0008830362348817289}, {"id": 80, "seek": 38762, "start": 391.86, "end": 397.54, "text": " before, is the Greek letters are things that you want to be able to, I think I have mentioned", "tokens": [50576, 949, 11, 307, 264, 10281, 7825, 366, 721, 300, 291, 528, 281, 312, 1075, 281, 11, 286, 519, 286, 362, 2835, 50860], "temperature": 0.0, "avg_logprob": -0.2787979973687066, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0008830362348817289}, {"id": 81, "seek": 38762, "start": 397.54, "end": 399.3, "text": " this, you want to be able to pronounce them.", "tokens": [50860, 341, 11, 291, 528, 281, 312, 1075, 281, 19567, 552, 13, 50948], "temperature": 0.0, "avg_logprob": -0.2787979973687066, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0008830362348817289}, {"id": 82, "seek": 38762, "start": 399.3, "end": 404.3, "text": " So if you don't know how to read these and say what these names are, make sure you head", "tokens": [50948, 407, 498, 291, 500, 380, 458, 577, 281, 1401, 613, 293, 584, 437, 613, 5288, 366, 11, 652, 988, 291, 1378, 51198], "temperature": 0.0, "avg_logprob": -0.2787979973687066, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0008830362348817289}, {"id": 83, "seek": 38762, "start": 404.3, "end": 409.14, "text": " over to Wikipedia or whatever and learn the names of all the Greek letters so that you", "tokens": [51198, 670, 281, 28999, 420, 2035, 293, 1466, 264, 5288, 295, 439, 264, 10281, 7825, 370, 300, 291, 51440], "temperature": 0.0, "avg_logprob": -0.2787979973687066, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0008830362348817289}, {"id": 84, "seek": 38762, "start": 409.14, "end": 411.5, "text": " can, because they come up all the time.", "tokens": [51440, 393, 11, 570, 436, 808, 493, 439, 264, 565, 13, 51558], "temperature": 0.0, "avg_logprob": -0.2787979973687066, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0008830362348817289}, {"id": 85, "seek": 41150, "start": 412.38, "end": 422.42, "text": " So what happens when we apply a convolution with this 2x2 kernel to this 3x3 image?", "tokens": [50408, 407, 437, 2314, 562, 321, 3079, 257, 45216, 365, 341, 568, 87, 17, 28256, 281, 341, 805, 87, 18, 3256, 30, 50910], "temperature": 0.0, "avg_logprob": -0.2512706046880678, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0013458504108712077}, {"id": 86, "seek": 41150, "start": 422.42, "end": 427.02, "text": " I mean it doesn't have to be an image, in this case it's just a rank 2 tensor, but it", "tokens": [50910, 286, 914, 309, 1177, 380, 362, 281, 312, 364, 3256, 11, 294, 341, 1389, 309, 311, 445, 257, 6181, 568, 40863, 11, 457, 309, 51140], "temperature": 0.0, "avg_logprob": -0.2512706046880678, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0013458504108712077}, {"id": 87, "seek": 41150, "start": 427.02, "end": 429.18, "text": " might represent an image.", "tokens": [51140, 1062, 2906, 364, 3256, 13, 51248], "temperature": 0.0, "avg_logprob": -0.2512706046880678, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0013458504108712077}, {"id": 88, "seek": 41150, "start": 429.18, "end": 437.9, "text": " What happens is we take the kernel and we overlay it over the first little 2x2 subgrid,", "tokens": [51248, 708, 2314, 307, 321, 747, 264, 28256, 293, 321, 31741, 309, 670, 264, 700, 707, 568, 87, 17, 1422, 35320, 11, 51684], "temperature": 0.0, "avg_logprob": -0.2512706046880678, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0013458504108712077}, {"id": 89, "seek": 41150, "start": 437.9, "end": 439.34, "text": " like so.", "tokens": [51684, 411, 370, 13, 51756], "temperature": 0.0, "avg_logprob": -0.2512706046880678, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0013458504108712077}, {"id": 90, "seek": 43934, "start": 439.34, "end": 442.78, "text": " And specifically what we do is we match color to color.", "tokens": [50364, 400, 4682, 437, 321, 360, 307, 321, 2995, 2017, 281, 2017, 13, 50536], "temperature": 0.0, "avg_logprob": -0.2048939741574801, "compression_ratio": 1.7156398104265402, "no_speech_prob": 8.750286360736936e-05}, {"id": 91, "seek": 43934, "start": 442.78, "end": 451.58, "text": " So the output of this first 2x2 overlay would be alpha times a plus beta times b plus gamma", "tokens": [50536, 407, 264, 5598, 295, 341, 700, 568, 87, 17, 31741, 576, 312, 8961, 1413, 257, 1804, 9861, 1413, 272, 1804, 15546, 50976], "temperature": 0.0, "avg_logprob": -0.2048939741574801, "compression_ratio": 1.7156398104265402, "no_speech_prob": 8.750286360736936e-05}, {"id": 92, "seek": 43934, "start": 451.58, "end": 455.14, "text": " times d plus delta times e.", "tokens": [50976, 1413, 274, 1804, 8289, 1413, 308, 13, 51154], "temperature": 0.0, "avg_logprob": -0.2048939741574801, "compression_ratio": 1.7156398104265402, "no_speech_prob": 8.750286360736936e-05}, {"id": 93, "seek": 43934, "start": 455.14, "end": 462.41999999999996, "text": " And that would yield some value, p, and that's going to end up in the top left of a 2x2 output.", "tokens": [51154, 400, 300, 576, 11257, 512, 2158, 11, 280, 11, 293, 300, 311, 516, 281, 917, 493, 294, 264, 1192, 1411, 295, 257, 568, 87, 17, 5598, 13, 51518], "temperature": 0.0, "avg_logprob": -0.2048939741574801, "compression_ratio": 1.7156398104265402, "no_speech_prob": 8.750286360736936e-05}, {"id": 94, "seek": 43934, "start": 462.41999999999996, "end": 467.09999999999997, "text": " So the top right of the 2x2 output we're going to slide, it's like a sliding window, we're", "tokens": [51518, 407, 264, 1192, 558, 295, 264, 568, 87, 17, 5598, 321, 434, 516, 281, 4137, 11, 309, 311, 411, 257, 21169, 4910, 11, 321, 434, 51752], "temperature": 0.0, "avg_logprob": -0.2048939741574801, "compression_ratio": 1.7156398104265402, "no_speech_prob": 8.750286360736936e-05}, {"id": 95, "seek": 46710, "start": 467.1, "end": 476.14000000000004, "text": " going to slide our kernel over to here and apply each of our coefficients to these respectively", "tokens": [50364, 516, 281, 4137, 527, 28256, 670, 281, 510, 293, 3079, 1184, 295, 527, 31994, 281, 613, 25009, 50816], "temperature": 0.0, "avg_logprob": -0.3000632013593401, "compression_ratio": 1.5460992907801419, "no_speech_prob": 5.1442180847516283e-05}, {"id": 96, "seek": 46710, "start": 476.14000000000004, "end": 478.32000000000005, "text": " colored squares.", "tokens": [50816, 14332, 19368, 13, 50925], "temperature": 0.0, "avg_logprob": -0.3000632013593401, "compression_ratio": 1.5460992907801419, "no_speech_prob": 5.1442180847516283e-05}, {"id": 97, "seek": 46710, "start": 478.32000000000005, "end": 483.54, "text": " And then ditto for the bottom left, and then ditto for the bottom right.", "tokens": [50925, 400, 550, 274, 34924, 337, 264, 2767, 1411, 11, 293, 550, 274, 34924, 337, 264, 2767, 558, 13, 51186], "temperature": 0.0, "avg_logprob": -0.3000632013593401, "compression_ratio": 1.5460992907801419, "no_speech_prob": 5.1442180847516283e-05}, {"id": 98, "seek": 46710, "start": 483.54, "end": 485.38, "text": " So we end up with this equation.", "tokens": [51186, 407, 321, 917, 493, 365, 341, 5367, 13, 51278], "temperature": 0.0, "avg_logprob": -0.3000632013593401, "compression_ratio": 1.5460992907801419, "no_speech_prob": 5.1442180847516283e-05}, {"id": 99, "seek": 48538, "start": 485.38, "end": 492.62, "text": " p, as we discussed, is alpha a plus beta b plus gamma d plus delta e plus some bias", "tokens": [50364, 280, 11, 382, 321, 7152, 11, 307, 8961, 257, 1804, 9861, 272, 1804, 15546, 274, 1804, 8289, 308, 1804, 512, 12577, 50726], "temperature": 0.0, "avg_logprob": -0.3067027422098013, "compression_ratio": 1.855072463768116, "no_speech_prob": 0.006097280420362949}, {"id": 100, "seek": 48538, "start": 492.62, "end": 493.62, "text": " term.", "tokens": [50726, 1433, 13, 50776], "temperature": 0.0, "avg_logprob": -0.3067027422098013, "compression_ratio": 1.855072463768116, "no_speech_prob": 0.006097280420362949}, {"id": 101, "seek": 48538, "start": 493.62, "end": 502.7, "text": " q, so the top right, as you can see it's just alpha in this case times b.", "tokens": [50776, 9505, 11, 370, 264, 1192, 558, 11, 382, 291, 393, 536, 309, 311, 445, 8961, 294, 341, 1389, 1413, 272, 13, 51230], "temperature": 0.0, "avg_logprob": -0.3067027422098013, "compression_ratio": 1.855072463768116, "no_speech_prob": 0.006097280420362949}, {"id": 102, "seek": 48538, "start": 502.7, "end": 506.78, "text": " And so we're just multiplying them together and adding them up, multiply together, add", "tokens": [51230, 400, 370, 321, 434, 445, 30955, 552, 1214, 293, 5127, 552, 493, 11, 12972, 1214, 11, 909, 51434], "temperature": 0.0, "avg_logprob": -0.3067027422098013, "compression_ratio": 1.855072463768116, "no_speech_prob": 0.006097280420362949}, {"id": 103, "seek": 48538, "start": 506.78, "end": 508.94, "text": " them up, multiply together and add them up.", "tokens": [51434, 552, 493, 11, 12972, 1214, 293, 909, 552, 493, 13, 51542], "temperature": 0.0, "avg_logprob": -0.3067027422098013, "compression_ratio": 1.855072463768116, "no_speech_prob": 0.006097280420362949}, {"id": 104, "seek": 48538, "start": 508.94, "end": 514.7, "text": " So we're basically, you can imagine that we're basically flattening these out into rank 1", "tokens": [51542, 407, 321, 434, 1936, 11, 291, 393, 3811, 300, 321, 434, 1936, 24183, 278, 613, 484, 666, 6181, 502, 51830], "temperature": 0.0, "avg_logprob": -0.3067027422098013, "compression_ratio": 1.855072463768116, "no_speech_prob": 0.006097280420362949}, {"id": 105, "seek": 51470, "start": 515.0200000000001, "end": 518.3000000000001, "text": " classes, into vectors, and then doing a dot product would be one way of thinking about", "tokens": [50380, 5359, 11, 666, 18875, 11, 293, 550, 884, 257, 5893, 1674, 576, 312, 472, 636, 295, 1953, 466, 50544], "temperature": 0.0, "avg_logprob": -0.2724636312116656, "compression_ratio": 1.4671052631578947, "no_speech_prob": 0.004007280338555574}, {"id": 106, "seek": 51470, "start": 518.3000000000001, "end": 521.94, "text": " what's happening as we slide this kernel over these windows.", "tokens": [50544, 437, 311, 2737, 382, 321, 4137, 341, 28256, 670, 613, 9309, 13, 50726], "temperature": 0.0, "avg_logprob": -0.2724636312116656, "compression_ratio": 1.4671052631578947, "no_speech_prob": 0.004007280338555574}, {"id": 107, "seek": 51470, "start": 521.94, "end": 526.3000000000001, "text": " And so this is called a convolution.", "tokens": [50726, 400, 370, 341, 307, 1219, 257, 45216, 13, 50944], "temperature": 0.0, "avg_logprob": -0.2724636312116656, "compression_ratio": 1.4671052631578947, "no_speech_prob": 0.004007280338555574}, {"id": 108, "seek": 51470, "start": 526.3000000000001, "end": 532.1400000000001, "text": " So let's try and create a convolution.", "tokens": [50944, 407, 718, 311, 853, 293, 1884, 257, 45216, 13, 51236], "temperature": 0.0, "avg_logprob": -0.2724636312116656, "compression_ratio": 1.4671052631578947, "no_speech_prob": 0.004007280338555574}, {"id": 109, "seek": 53214, "start": 532.14, "end": 545.54, "text": " So for example, let's grab our training images and take a look at one.", "tokens": [50364, 407, 337, 1365, 11, 718, 311, 4444, 527, 3097, 5267, 293, 747, 257, 574, 412, 472, 13, 51034], "temperature": 0.0, "avg_logprob": -0.2750185356765497, "compression_ratio": 1.395973154362416, "no_speech_prob": 0.001098704757168889}, {"id": 110, "seek": 53214, "start": 545.54, "end": 549.54, "text": " And let's create a 3x3 kernel.", "tokens": [51034, 400, 718, 311, 1884, 257, 805, 87, 18, 28256, 13, 51234], "temperature": 0.0, "avg_logprob": -0.2750185356765497, "compression_ratio": 1.395973154362416, "no_speech_prob": 0.001098704757168889}, {"id": 111, "seek": 53214, "start": 549.54, "end": 554.6999999999999, "text": " So remember a kernel is just, we've already, a kernel appears a lot of times in computer", "tokens": [51234, 407, 1604, 257, 28256, 307, 445, 11, 321, 600, 1217, 11, 257, 28256, 7038, 257, 688, 295, 1413, 294, 3820, 51492], "temperature": 0.0, "avg_logprob": -0.2750185356765497, "compression_ratio": 1.395973154362416, "no_speech_prob": 0.001098704757168889}, {"id": 112, "seek": 53214, "start": 554.6999999999999, "end": 555.9, "text": " science and math.", "tokens": [51492, 3497, 293, 5221, 13, 51552], "temperature": 0.0, "avg_logprob": -0.2750185356765497, "compression_ratio": 1.395973154362416, "no_speech_prob": 0.001098704757168889}, {"id": 113, "seek": 55590, "start": 555.9, "end": 564.3, "text": " We've already seen the term kernel to mean a piece of code that we run on a GPU across", "tokens": [50364, 492, 600, 1217, 1612, 264, 1433, 28256, 281, 914, 257, 2522, 295, 3089, 300, 321, 1190, 322, 257, 18407, 2108, 50784], "temperature": 0.0, "avg_logprob": -0.2752018356323242, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.0040071806870400906}, {"id": 114, "seek": 55590, "start": 564.3, "end": 570.54, "text": " lots of parallel kind of virtual devices or potentially in a grid.", "tokens": [50784, 3195, 295, 8952, 733, 295, 6374, 5759, 420, 7263, 294, 257, 10748, 13, 51096], "temperature": 0.0, "avg_logprob": -0.2752018356323242, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.0040071806870400906}, {"id": 115, "seek": 55590, "start": 570.54, "end": 571.9599999999999, "text": " There's a similar idea here.", "tokens": [51096, 821, 311, 257, 2531, 1558, 510, 13, 51167], "temperature": 0.0, "avg_logprob": -0.2752018356323242, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.0040071806870400906}, {"id": 116, "seek": 55590, "start": 571.9599999999999, "end": 575.5799999999999, "text": " We've got a computation, which is in this case kind of this dot product or something", "tokens": [51167, 492, 600, 658, 257, 24903, 11, 597, 307, 294, 341, 1389, 733, 295, 341, 5893, 1674, 420, 746, 51348], "temperature": 0.0, "avg_logprob": -0.2752018356323242, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.0040071806870400906}, {"id": 117, "seek": 55590, "start": 575.5799999999999, "end": 580.86, "text": " like a dot product, sliding over, occurring lots of times over a grid.", "tokens": [51348, 411, 257, 5893, 1674, 11, 21169, 670, 11, 18386, 3195, 295, 1413, 670, 257, 10748, 13, 51612], "temperature": 0.0, "avg_logprob": -0.2752018356323242, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.0040071806870400906}, {"id": 118, "seek": 55590, "start": 580.86, "end": 585.74, "text": " But it's, yeah, it's a bit different.", "tokens": [51612, 583, 309, 311, 11, 1338, 11, 309, 311, 257, 857, 819, 13, 51856], "temperature": 0.0, "avg_logprob": -0.2752018356323242, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.0040071806870400906}, {"id": 119, "seek": 58574, "start": 586.58, "end": 587.58, "text": " It's kind of another use of the word kernel.", "tokens": [50406, 467, 311, 733, 295, 1071, 764, 295, 264, 1349, 28256, 13, 50456], "temperature": 0.0, "avg_logprob": -0.297457218170166, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0032730456441640854}, {"id": 120, "seek": 58574, "start": 587.58, "end": 592.58, "text": " So in this case, a kernel is a, in this case, it's going to be a rank 2 tensor.", "tokens": [50456, 407, 294, 341, 1389, 11, 257, 28256, 307, 257, 11, 294, 341, 1389, 11, 309, 311, 516, 281, 312, 257, 6181, 568, 40863, 13, 50706], "temperature": 0.0, "avg_logprob": -0.297457218170166, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0032730456441640854}, {"id": 121, "seek": 58574, "start": 592.58, "end": 599.34, "text": " And so let's create a kernel with these values in the 3x3 matrix rank 2 tensor.", "tokens": [50706, 400, 370, 718, 311, 1884, 257, 28256, 365, 613, 4190, 294, 264, 805, 87, 18, 8141, 6181, 568, 40863, 13, 51044], "temperature": 0.0, "avg_logprob": -0.297457218170166, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0032730456441640854}, {"id": 122, "seek": 58574, "start": 599.34, "end": 601.9, "text": " And we could draw what that looks like.", "tokens": [51044, 400, 321, 727, 2642, 437, 300, 1542, 411, 13, 51172], "temperature": 0.0, "avg_logprob": -0.297457218170166, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0032730456441640854}, {"id": 123, "seek": 58574, "start": 601.9, "end": 605.1, "text": " Not surprisingly, it just looks like a bunch of lines.", "tokens": [51172, 1726, 17600, 11, 309, 445, 1542, 411, 257, 3840, 295, 3876, 13, 51332], "temperature": 0.0, "avg_logprob": -0.297457218170166, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0032730456441640854}, {"id": 124, "seek": 58574, "start": 605.1, "end": 606.7, "text": " Oops.", "tokens": [51332, 21726, 13, 51412], "temperature": 0.0, "avg_logprob": -0.297457218170166, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0032730456441640854}, {"id": 125, "seek": 58574, "start": 606.7, "end": 608.38, "text": " Okay.", "tokens": [51412, 1033, 13, 51496], "temperature": 0.0, "avg_logprob": -0.297457218170166, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0032730456441640854}, {"id": 126, "seek": 60838, "start": 608.38, "end": 618.3, "text": " So what would happen if we slide this over, just these nine pixels over this 28 by 28?", "tokens": [50364, 407, 437, 576, 1051, 498, 321, 4137, 341, 670, 11, 445, 613, 4949, 18668, 670, 341, 7562, 538, 7562, 30, 50860], "temperature": 0.0, "avg_logprob": -0.2813175621382687, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0033764054533094168}, {"id": 127, "seek": 60838, "start": 618.3, "end": 624.38, "text": " Well what's going to happen is if we've got some, the top left, for example, 3x3 section", "tokens": [50860, 1042, 437, 311, 516, 281, 1051, 307, 498, 321, 600, 658, 512, 11, 264, 1192, 1411, 11, 337, 1365, 11, 805, 87, 18, 3541, 51164], "temperature": 0.0, "avg_logprob": -0.2813175621382687, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0033764054533094168}, {"id": 128, "seek": 60838, "start": 624.38, "end": 629.66, "text": " has these names, then we're going to end up with negative a1, because the top three are", "tokens": [51164, 575, 613, 5288, 11, 550, 321, 434, 516, 281, 917, 493, 365, 3671, 257, 16, 11, 570, 264, 1192, 1045, 366, 51428], "temperature": 0.0, "avg_logprob": -0.2813175621382687, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0033764054533094168}, {"id": 129, "seek": 60838, "start": 629.66, "end": 630.98, "text": " all negative, right?", "tokens": [51428, 439, 3671, 11, 558, 30, 51494], "temperature": 0.0, "avg_logprob": -0.2813175621382687, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0033764054533094168}, {"id": 130, "seek": 60838, "start": 630.98, "end": 634.38, "text": " Negative a1 minus a2 minus a3.", "tokens": [51494, 43230, 257, 16, 3175, 257, 17, 3175, 257, 18, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2813175621382687, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0033764054533094168}, {"id": 131, "seek": 60838, "start": 634.38, "end": 636.06, "text": " The next are just zero.", "tokens": [51664, 440, 958, 366, 445, 4018, 13, 51748], "temperature": 0.0, "avg_logprob": -0.2813175621382687, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0033764054533094168}, {"id": 132, "seek": 60838, "start": 636.06, "end": 637.58, "text": " So that won't do anything.", "tokens": [51748, 407, 300, 1582, 380, 360, 1340, 13, 51824], "temperature": 0.0, "avg_logprob": -0.2813175621382687, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0033764054533094168}, {"id": 133, "seek": 63758, "start": 637.7800000000001, "end": 643.4200000000001, "text": " And then plus a7 plus a8 plus a9.", "tokens": [50374, 400, 550, 1804, 257, 22, 1804, 257, 23, 1804, 257, 24, 13, 50656], "temperature": 0.0, "avg_logprob": -0.3117357107309195, "compression_ratio": 1.3448275862068966, "no_speech_prob": 0.02033190242946148}, {"id": 134, "seek": 63758, "start": 643.4200000000001, "end": 647.1800000000001, "text": " Why is that interesting?", "tokens": [50656, 1545, 307, 300, 1880, 30, 50844], "temperature": 0.0, "avg_logprob": -0.3117357107309195, "compression_ratio": 1.3448275862068966, "no_speech_prob": 0.02033190242946148}, {"id": 135, "seek": 63758, "start": 647.1800000000001, "end": 648.1800000000001, "text": " That's interesting.", "tokens": [50844, 663, 311, 1880, 13, 50894], "temperature": 0.0, "avg_logprob": -0.3117357107309195, "compression_ratio": 1.3448275862068966, "no_speech_prob": 0.02033190242946148}, {"id": 136, "seek": 63758, "start": 648.1800000000001, "end": 650.1800000000001, "text": " Well, let's try.", "tokens": [50894, 1042, 11, 718, 311, 853, 13, 50994], "temperature": 0.0, "avg_logprob": -0.3117357107309195, "compression_ratio": 1.3448275862068966, "no_speech_prob": 0.02033190242946148}, {"id": 137, "seek": 63758, "start": 650.1800000000001, "end": 657.58, "text": " Here, what I've done here is I've grabbed just the first 13 rows and first 23 columns", "tokens": [50994, 1692, 11, 437, 286, 600, 1096, 510, 307, 286, 600, 18607, 445, 264, 700, 3705, 13241, 293, 700, 6673, 13766, 51364], "temperature": 0.0, "avg_logprob": -0.3117357107309195, "compression_ratio": 1.3448275862068966, "no_speech_prob": 0.02033190242946148}, {"id": 138, "seek": 63758, "start": 657.58, "end": 660.5, "text": " of our image.", "tokens": [51364, 295, 527, 3256, 13, 51510], "temperature": 0.0, "avg_logprob": -0.3117357107309195, "compression_ratio": 1.3448275862068966, "no_speech_prob": 0.02033190242946148}, {"id": 139, "seek": 66050, "start": 660.5, "end": 668.34, "text": " And I'm actually showing the numbers and also using grey kind of conditional formatting,", "tokens": [50364, 400, 286, 478, 767, 4099, 264, 3547, 293, 611, 1228, 16578, 733, 295, 27708, 39366, 11, 50756], "temperature": 0.0, "avg_logprob": -0.2666082010640726, "compression_ratio": 1.4550264550264551, "no_speech_prob": 0.004609478637576103}, {"id": 140, "seek": 66050, "start": 668.34, "end": 672.14, "text": " if you like, or the equivalent in pandas, to show this top bit.", "tokens": [50756, 498, 291, 411, 11, 420, 264, 10344, 294, 4565, 296, 11, 281, 855, 341, 1192, 857, 13, 50946], "temperature": 0.0, "avg_logprob": -0.2666082010640726, "compression_ratio": 1.4550264550264551, "no_speech_prob": 0.004609478637576103}, {"id": 141, "seek": 66050, "start": 672.14, "end": 678.5, "text": " So we're looking at just this top bit.", "tokens": [50946, 407, 321, 434, 1237, 412, 445, 341, 1192, 857, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2666082010640726, "compression_ratio": 1.4550264550264551, "no_speech_prob": 0.004609478637576103}, {"id": 142, "seek": 66050, "start": 678.5, "end": 683.98, "text": " So what happens if we take rows 3, 4, and 5?", "tokens": [51264, 407, 437, 2314, 498, 321, 747, 13241, 805, 11, 1017, 11, 293, 1025, 30, 51538], "temperature": 0.0, "avg_logprob": -0.2666082010640726, "compression_ratio": 1.4550264550264551, "no_speech_prob": 0.004609478637576103}, {"id": 143, "seek": 66050, "start": 683.98, "end": 686.7, "text": " Remember this is not inclusive, right?", "tokens": [51538, 5459, 341, 307, 406, 13429, 11, 558, 30, 51674], "temperature": 0.0, "avg_logprob": -0.2666082010640726, "compression_ratio": 1.4550264550264551, "no_speech_prob": 0.004609478637576103}, {"id": 144, "seek": 68670, "start": 686.7, "end": 689.0600000000001, "text": " So it's rows 3, 4, and 5.", "tokens": [50364, 407, 309, 311, 13241, 805, 11, 1017, 11, 293, 1025, 13, 50482], "temperature": 0.0, "avg_logprob": -0.22962227296293453, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.010013437829911709}, {"id": 145, "seek": 68670, "start": 689.0600000000001, "end": 692.0200000000001, "text": " Columns 14, 15, 16.", "tokens": [50482, 4004, 449, 3695, 3499, 11, 2119, 11, 3165, 13, 50630], "temperature": 0.0, "avg_logprob": -0.22962227296293453, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.010013437829911709}, {"id": 146, "seek": 68670, "start": 692.0200000000001, "end": 695.6600000000001, "text": " So we're looking at these three here.", "tokens": [50630, 407, 321, 434, 1237, 412, 613, 1045, 510, 13, 50812], "temperature": 0.0, "avg_logprob": -0.22962227296293453, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.010013437829911709}, {"id": 147, "seek": 68670, "start": 695.6600000000001, "end": 702.1, "text": " What's that going to give us if we multiply it by this kernel?", "tokens": [50812, 708, 311, 300, 516, 281, 976, 505, 498, 321, 12972, 309, 538, 341, 28256, 30, 51134], "temperature": 0.0, "avg_logprob": -0.22962227296293453, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.010013437829911709}, {"id": 148, "seek": 68670, "start": 702.1, "end": 710.0400000000001, "text": " It gives us a fairly large positive value, because the three that we have negatives on", "tokens": [51134, 467, 2709, 505, 257, 6457, 2416, 3353, 2158, 11, 570, 264, 1045, 300, 321, 362, 40019, 322, 51531], "temperature": 0.0, "avg_logprob": -0.22962227296293453, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.010013437829911709}, {"id": 149, "seek": 68670, "start": 710.0400000000001, "end": 711.0400000000001, "text": " is the top row.", "tokens": [51531, 307, 264, 1192, 5386, 13, 51581], "temperature": 0.0, "avg_logprob": -0.22962227296293453, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.010013437829911709}, {"id": 150, "seek": 68670, "start": 711.0400000000001, "end": 712.5400000000001, "text": " Well, they're all zero.", "tokens": [51581, 1042, 11, 436, 434, 439, 4018, 13, 51656], "temperature": 0.0, "avg_logprob": -0.22962227296293453, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.010013437829911709}, {"id": 151, "seek": 71254, "start": 712.54, "end": 717.8199999999999, "text": " And the three that we have positives on, they're all close to 1.", "tokens": [50364, 400, 264, 1045, 300, 321, 362, 35127, 322, 11, 436, 434, 439, 1998, 281, 502, 13, 50628], "temperature": 0.0, "avg_logprob": -0.24066491220511643, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.004609362687915564}, {"id": 152, "seek": 71254, "start": 717.8199999999999, "end": 720.78, "text": " So we end up with quite a large number.", "tokens": [50628, 407, 321, 917, 493, 365, 1596, 257, 2416, 1230, 13, 50776], "temperature": 0.0, "avg_logprob": -0.24066491220511643, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.004609362687915564}, {"id": 153, "seek": 71254, "start": 720.78, "end": 726.38, "text": " What about the same columns but for rows 7, 8, 9?", "tokens": [50776, 708, 466, 264, 912, 13766, 457, 337, 13241, 1614, 11, 1649, 11, 1722, 30, 51056], "temperature": 0.0, "avg_logprob": -0.24066491220511643, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.004609362687915564}, {"id": 154, "seek": 71254, "start": 726.38, "end": 728.9, "text": " 7, 8, 9.", "tokens": [51056, 1614, 11, 1649, 11, 1722, 13, 51182], "temperature": 0.0, "avg_logprob": -0.24066491220511643, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.004609362687915564}, {"id": 155, "seek": 71254, "start": 728.9, "end": 733.9599999999999, "text": " Here, the top is all positive and the bottom is all zero.", "tokens": [51182, 1692, 11, 264, 1192, 307, 439, 3353, 293, 264, 2767, 307, 439, 4018, 13, 51435], "temperature": 0.0, "avg_logprob": -0.24066491220511643, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.004609362687915564}, {"id": 156, "seek": 71254, "start": 733.9599999999999, "end": 738.26, "text": " So that means that we're going to get a lot of negative terms.", "tokens": [51435, 407, 300, 1355, 300, 321, 434, 516, 281, 483, 257, 688, 295, 3671, 2115, 13, 51650], "temperature": 0.0, "avg_logprob": -0.24066491220511643, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.004609362687915564}, {"id": 157, "seek": 71254, "start": 738.26, "end": 740.8199999999999, "text": " And not surprisingly, that's exactly what we see.", "tokens": [51650, 400, 406, 17600, 11, 300, 311, 2293, 437, 321, 536, 13, 51778], "temperature": 0.0, "avg_logprob": -0.24066491220511643, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.004609362687915564}, {"id": 158, "seek": 74082, "start": 740.82, "end": 748.82, "text": " If we do this, kind of a dot product equivalent, which all you need in NumPy to do that is", "tokens": [50364, 759, 321, 360, 341, 11, 733, 295, 257, 5893, 1674, 10344, 11, 597, 439, 291, 643, 294, 22592, 47, 88, 281, 360, 300, 307, 50764], "temperature": 0.0, "avg_logprob": -0.21655414415442426, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0033244036603718996}, {"id": 159, "seek": 74082, "start": 748.82, "end": 752.7800000000001, "text": " just an element-wise multiplication followed by a sum.", "tokens": [50764, 445, 364, 4478, 12, 3711, 27290, 6263, 538, 257, 2408, 13, 50962], "temperature": 0.0, "avg_logprob": -0.21655414415442426, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0033244036603718996}, {"id": 160, "seek": 74082, "start": 752.7800000000001, "end": 755.6600000000001, "text": " So that's going to be quite a large negative number.", "tokens": [50962, 407, 300, 311, 516, 281, 312, 1596, 257, 2416, 3671, 1230, 13, 51106], "temperature": 0.0, "avg_logprob": -0.21655414415442426, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0033244036603718996}, {"id": 161, "seek": 74082, "start": 755.6600000000001, "end": 759.34, "text": " And so perhaps you're seeing what this is doing, and maybe you got a hint from the name", "tokens": [51106, 400, 370, 4317, 291, 434, 2577, 437, 341, 307, 884, 11, 293, 1310, 291, 658, 257, 12075, 490, 264, 1315, 51290], "temperature": 0.0, "avg_logprob": -0.21655414415442426, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0033244036603718996}, {"id": 162, "seek": 74082, "start": 759.34, "end": 761.5, "text": " of the tensor we created.", "tokens": [51290, 295, 264, 40863, 321, 2942, 13, 51398], "temperature": 0.0, "avg_logprob": -0.21655414415442426, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0033244036603718996}, {"id": 163, "seek": 74082, "start": 761.5, "end": 765.82, "text": " It's something that is going to find the top edge.", "tokens": [51398, 467, 311, 746, 300, 307, 516, 281, 915, 264, 1192, 4691, 13, 51614], "temperature": 0.0, "avg_logprob": -0.21655414415442426, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0033244036603718996}, {"id": 164, "seek": 74082, "start": 765.82, "end": 768.8000000000001, "text": " So this one is a top edge, so it's a positive.", "tokens": [51614, 407, 341, 472, 307, 257, 1192, 4691, 11, 370, 309, 311, 257, 3353, 13, 51763], "temperature": 0.0, "avg_logprob": -0.21655414415442426, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0033244036603718996}, {"id": 165, "seek": 76880, "start": 768.8, "end": 772.92, "text": " And this one is a bottom edge, so it's a negative.", "tokens": [50364, 400, 341, 472, 307, 257, 2767, 4691, 11, 370, 309, 311, 257, 3671, 13, 50570], "temperature": 0.0, "avg_logprob": -0.2635656014466897, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.00016865260840859264}, {"id": 166, "seek": 76880, "start": 772.92, "end": 782.92, "text": " So we would like to apply that, this kernel, to every single 3x3 section in here.", "tokens": [50570, 407, 321, 576, 411, 281, 3079, 300, 11, 341, 28256, 11, 281, 633, 2167, 805, 87, 18, 3541, 294, 510, 13, 51070], "temperature": 0.0, "avg_logprob": -0.2635656014466897, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.00016865260840859264}, {"id": 167, "seek": 76880, "start": 782.92, "end": 788.4399999999999, "text": " So we could do that by creating a little apply kernel function that takes some particular", "tokens": [51070, 407, 321, 727, 360, 300, 538, 4084, 257, 707, 3079, 28256, 2445, 300, 2516, 512, 1729, 51346], "temperature": 0.0, "avg_logprob": -0.2635656014466897, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.00016865260840859264}, {"id": 168, "seek": 76880, "start": 788.4399999999999, "end": 797.8, "text": " row and some particular column and some particular tensor as a kernel and does that multiplication.", "tokens": [51346, 5386, 293, 512, 1729, 7738, 293, 512, 1729, 40863, 382, 257, 28256, 293, 775, 300, 27290, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2635656014466897, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.00016865260840859264}, {"id": 169, "seek": 79780, "start": 798.8, "end": 801.8399999999999, "text": " dot sum that we just saw.", "tokens": [50414, 5893, 2408, 300, 321, 445, 1866, 13, 50566], "temperature": 0.0, "avg_logprob": -0.22118247427591464, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.00021654326701536775}, {"id": 170, "seek": 79780, "start": 801.8399999999999, "end": 808.04, "text": " So for example, we could replicate this one by calling apply kernel.", "tokens": [50566, 407, 337, 1365, 11, 321, 727, 25356, 341, 472, 538, 5141, 3079, 28256, 13, 50876], "temperature": 0.0, "avg_logprob": -0.22118247427591464, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.00021654326701536775}, {"id": 171, "seek": 79780, "start": 808.04, "end": 813.3199999999999, "text": " And this here is the center of that 3x3 grid area.", "tokens": [50876, 400, 341, 510, 307, 264, 3056, 295, 300, 805, 87, 18, 10748, 1859, 13, 51140], "temperature": 0.0, "avg_logprob": -0.22118247427591464, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.00021654326701536775}, {"id": 172, "seek": 79780, "start": 813.3199999999999, "end": 816.9599999999999, "text": " And so there's that same number, 2.97.", "tokens": [51140, 400, 370, 456, 311, 300, 912, 1230, 11, 568, 13, 23247, 13, 51322], "temperature": 0.0, "avg_logprob": -0.22118247427591464, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.00021654326701536775}, {"id": 173, "seek": 79780, "start": 816.9599999999999, "end": 826.56, "text": " So now we could apply that kernel to every one of the 3x3 windows in this 28x28 image.", "tokens": [51322, 407, 586, 321, 727, 3079, 300, 28256, 281, 633, 472, 295, 264, 805, 87, 18, 9309, 294, 341, 7562, 87, 11205, 3256, 13, 51802], "temperature": 0.0, "avg_logprob": -0.22118247427591464, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.00021654326701536775}, {"id": 174, "seek": 82656, "start": 826.56, "end": 831.16, "text": " So we're going to be sliding over like this red bit sliding over here, but we've actually", "tokens": [50364, 407, 321, 434, 516, 281, 312, 21169, 670, 411, 341, 2182, 857, 21169, 670, 510, 11, 457, 321, 600, 767, 50594], "temperature": 0.0, "avg_logprob": -0.21987830268012154, "compression_ratio": 1.65, "no_speech_prob": 7.84378353273496e-05}, {"id": 175, "seek": 82656, "start": 831.16, "end": 835.64, "text": " got a 28x28 input, not just a 5x5 input.", "tokens": [50594, 658, 257, 7562, 87, 11205, 4846, 11, 406, 445, 257, 1025, 87, 20, 4846, 13, 50818], "temperature": 0.0, "avg_logprob": -0.21987830268012154, "compression_ratio": 1.65, "no_speech_prob": 7.84378353273496e-05}, {"id": 176, "seek": 82656, "start": 835.64, "end": 842.0799999999999, "text": " So to get all of the coordinates, let's just simplify to do this 5x5, we can go, we can", "tokens": [50818, 407, 281, 483, 439, 295, 264, 21056, 11, 718, 311, 445, 20460, 281, 360, 341, 1025, 87, 20, 11, 321, 393, 352, 11, 321, 393, 51140], "temperature": 0.0, "avg_logprob": -0.21987830268012154, "compression_ratio": 1.65, "no_speech_prob": 7.84378353273496e-05}, {"id": 177, "seek": 82656, "start": 842.0799999999999, "end": 843.68, "text": " create a list comprehension.", "tokens": [51140, 1884, 257, 1329, 44991, 13, 51220], "temperature": 0.0, "avg_logprob": -0.21987830268012154, "compression_ratio": 1.65, "no_speech_prob": 7.84378353273496e-05}, {"id": 178, "seek": 82656, "start": 843.68, "end": 848.02, "text": " We can take i through every value in range 5.", "tokens": [51220, 492, 393, 747, 741, 807, 633, 2158, 294, 3613, 1025, 13, 51437], "temperature": 0.0, "avg_logprob": -0.21987830268012154, "compression_ratio": 1.65, "no_speech_prob": 7.84378353273496e-05}, {"id": 179, "seek": 82656, "start": 848.02, "end": 854.2399999999999, "text": " And then for each of those, we can take j for every value in range 5.", "tokens": [51437, 400, 550, 337, 1184, 295, 729, 11, 321, 393, 747, 361, 337, 633, 2158, 294, 3613, 1025, 13, 51748], "temperature": 0.0, "avg_logprob": -0.21987830268012154, "compression_ratio": 1.65, "no_speech_prob": 7.84378353273496e-05}, {"id": 180, "seek": 85424, "start": 854.24, "end": 861.32, "text": " And so if we just look at that tuple, you can see we get a list of lists containing", "tokens": [50364, 400, 370, 498, 321, 445, 574, 412, 300, 2604, 781, 11, 291, 393, 536, 321, 483, 257, 1329, 295, 14511, 19273, 50718], "temperature": 0.0, "avg_logprob": -0.19720199440099015, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0025508319959044456}, {"id": 181, "seek": 85424, "start": 861.32, "end": 865.38, "text": " all of those coordinates.", "tokens": [50718, 439, 295, 729, 21056, 13, 50921], "temperature": 0.0, "avg_logprob": -0.19720199440099015, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0025508319959044456}, {"id": 182, "seek": 85424, "start": 865.38, "end": 872.12, "text": " So this is a list comprehension in a list comprehension, which when you first say it", "tokens": [50921, 407, 341, 307, 257, 1329, 44991, 294, 257, 1329, 44991, 11, 597, 562, 291, 700, 584, 309, 51258], "temperature": 0.0, "avg_logprob": -0.19720199440099015, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0025508319959044456}, {"id": 183, "seek": 85424, "start": 872.12, "end": 881.1, "text": " may be surprising or confusing, but it's a really helpful idiom, and I certainly recommend", "tokens": [51258, 815, 312, 8830, 420, 13181, 11, 457, 309, 311, 257, 534, 4961, 18014, 298, 11, 293, 286, 3297, 2748, 51707], "temperature": 0.0, "avg_logprob": -0.19720199440099015, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0025508319959044456}, {"id": 184, "seek": 85424, "start": 881.1, "end": 883.84, "text": " getting used to it.", "tokens": [51707, 1242, 1143, 281, 309, 13, 51844], "temperature": 0.0, "avg_logprob": -0.19720199440099015, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0025508319959044456}, {"id": 185, "seek": 88384, "start": 884.44, "end": 890.2800000000001, "text": " Now, what we're going to do is we're not just going to create the cup, this tuple, but we're", "tokens": [50394, 823, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 406, 445, 516, 281, 1884, 264, 4414, 11, 341, 2604, 781, 11, 457, 321, 434, 50686], "temperature": 0.0, "avg_logprob": -0.22360693239698223, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.003945371136069298}, {"id": 186, "seek": 88384, "start": 890.2800000000001, "end": 894.6600000000001, "text": " actually going to call apply kernel for each of those.", "tokens": [50686, 767, 516, 281, 818, 3079, 28256, 337, 1184, 295, 729, 13, 50905], "temperature": 0.0, "avg_logprob": -0.22360693239698223, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.003945371136069298}, {"id": 187, "seek": 88384, "start": 894.6600000000001, "end": 903.9, "text": " So if we go through from 1 to 27, well actually 1 to 26, because 27 is exclusive.", "tokens": [50905, 407, 498, 321, 352, 807, 490, 502, 281, 7634, 11, 731, 767, 502, 281, 7551, 11, 570, 7634, 307, 13005, 13, 51367], "temperature": 0.0, "avg_logprob": -0.22360693239698223, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.003945371136069298}, {"id": 188, "seek": 88384, "start": 903.9, "end": 907.32, "text": " So we're going to go through everything from 1 to 26.", "tokens": [51367, 407, 321, 434, 516, 281, 352, 807, 1203, 490, 502, 281, 7551, 13, 51538], "temperature": 0.0, "avg_logprob": -0.22360693239698223, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.003945371136069298}, {"id": 189, "seek": 88384, "start": 907.32, "end": 912.9200000000001, "text": " And then for each of those, go through from 1 to 26 again and call apply kernel.", "tokens": [51538, 400, 550, 337, 1184, 295, 729, 11, 352, 807, 490, 502, 281, 7551, 797, 293, 818, 3079, 28256, 13, 51818], "temperature": 0.0, "avg_logprob": -0.22360693239698223, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.003945371136069298}, {"id": 190, "seek": 91292, "start": 913.0, "end": 917.3199999999999, "text": " And that's going to give us the result of applying that convolutional kernel to every", "tokens": [50368, 400, 300, 311, 516, 281, 976, 505, 264, 1874, 295, 9275, 300, 45216, 304, 28256, 281, 633, 50584], "temperature": 0.0, "avg_logprob": -0.22628615623296694, "compression_ratio": 1.6512820512820512, "no_speech_prob": 1.321194849879248e-05}, {"id": 191, "seek": 91292, "start": 917.3199999999999, "end": 920.4, "text": " one of those coordinates.", "tokens": [50584, 472, 295, 729, 21056, 13, 50738], "temperature": 0.0, "avg_logprob": -0.22628615623296694, "compression_ratio": 1.6512820512820512, "no_speech_prob": 1.321194849879248e-05}, {"id": 192, "seek": 91292, "start": 920.4, "end": 922.0, "text": " And there's the result.", "tokens": [50738, 400, 456, 311, 264, 1874, 13, 50818], "temperature": 0.0, "avg_logprob": -0.22628615623296694, "compression_ratio": 1.6512820512820512, "no_speech_prob": 1.321194849879248e-05}, {"id": 193, "seek": 91292, "start": 922.0, "end": 928.36, "text": " And you can see what it's done, as we hoped, is it is highlighting the top edges.", "tokens": [50818, 400, 291, 393, 536, 437, 309, 311, 1096, 11, 382, 321, 19737, 11, 307, 309, 307, 26551, 264, 1192, 8819, 13, 51136], "temperature": 0.0, "avg_logprob": -0.22628615623296694, "compression_ratio": 1.6512820512820512, "no_speech_prob": 1.321194849879248e-05}, {"id": 194, "seek": 91292, "start": 928.36, "end": 935.4399999999999, "text": " So yeah, you might find that kind of surprising that it's that easy to do this kind of image", "tokens": [51136, 407, 1338, 11, 291, 1062, 915, 300, 733, 295, 8830, 300, 309, 311, 300, 1858, 281, 360, 341, 733, 295, 3256, 51490], "temperature": 0.0, "avg_logprob": -0.22628615623296694, "compression_ratio": 1.6512820512820512, "no_speech_prob": 1.321194849879248e-05}, {"id": 195, "seek": 91292, "start": 935.4399999999999, "end": 936.76, "text": " processing.", "tokens": [51490, 9007, 13, 51556], "temperature": 0.0, "avg_logprob": -0.22628615623296694, "compression_ratio": 1.6512820512820512, "no_speech_prob": 1.321194849879248e-05}, {"id": 196, "seek": 93676, "start": 936.76, "end": 945.76, "text": " We're literally just doing an element-wise multiplication and a sum for each window.", "tokens": [50364, 492, 434, 3736, 445, 884, 364, 4478, 12, 3711, 27290, 293, 257, 2408, 337, 1184, 4910, 13, 50814], "temperature": 0.0, "avg_logprob": -0.31938003358386813, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.013636481016874313}, {"id": 197, "seek": 93676, "start": 945.76, "end": 954.6, "text": " Okay, so that is called a convolution.", "tokens": [50814, 1033, 11, 370, 300, 307, 1219, 257, 45216, 13, 51256], "temperature": 0.0, "avg_logprob": -0.31938003358386813, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.013636481016874313}, {"id": 198, "seek": 93676, "start": 954.6, "end": 956.16, "text": " So we can do another convolution.", "tokens": [51256, 407, 321, 393, 360, 1071, 45216, 13, 51334], "temperature": 0.0, "avg_logprob": -0.31938003358386813, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.013636481016874313}, {"id": 199, "seek": 93676, "start": 956.16, "end": 959.2, "text": " This time we could do one with a left edge tensor.", "tokens": [51334, 639, 565, 321, 727, 360, 472, 365, 257, 1411, 4691, 40863, 13, 51486], "temperature": 0.0, "avg_logprob": -0.31938003358386813, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.013636481016874313}, {"id": 200, "seek": 93676, "start": 959.2, "end": 965.4, "text": " As you can see, it looks just a rotated version or transposed version, I guess, of our top", "tokens": [51486, 1018, 291, 393, 536, 11, 309, 1542, 445, 257, 42146, 3037, 420, 7132, 1744, 3037, 11, 286, 2041, 11, 295, 527, 1192, 51796], "temperature": 0.0, "avg_logprob": -0.31938003358386813, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.013636481016874313}, {"id": 201, "seek": 93676, "start": 965.4, "end": 966.4, "text": " edge tensor.", "tokens": [51796, 4691, 40863, 13, 51846], "temperature": 0.0, "avg_logprob": -0.31938003358386813, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.013636481016874313}, {"id": 202, "seek": 96640, "start": 967.4, "end": 973.28, "text": " And so if we apply that kernel, so this time we're going to apply the left edge kernel.", "tokens": [50414, 400, 370, 498, 321, 3079, 300, 28256, 11, 370, 341, 565, 321, 434, 516, 281, 3079, 264, 1411, 4691, 28256, 13, 50708], "temperature": 0.0, "avg_logprob": -0.37991197141882493, "compression_ratio": 1.6821192052980132, "no_speech_prob": 0.00035697012208402157}, {"id": 203, "seek": 96640, "start": 973.28, "end": 977.0799999999999, "text": " And so notice here that we're actually passing in a function.", "tokens": [50708, 400, 370, 3449, 510, 300, 321, 434, 767, 8437, 294, 257, 2445, 13, 50898], "temperature": 0.0, "avg_logprob": -0.37991197141882493, "compression_ratio": 1.6821192052980132, "no_speech_prob": 0.00035697012208402157}, {"id": 204, "seek": 96640, "start": 977.0799999999999, "end": 979.52, "text": " Right, we're passing in a function.", "tokens": [50898, 1779, 11, 321, 434, 8437, 294, 257, 2445, 13, 51020], "temperature": 0.0, "avg_logprob": -0.37991197141882493, "compression_ratio": 1.6821192052980132, "no_speech_prob": 0.00035697012208402157}, {"id": 205, "seek": 96640, "start": 979.52, "end": 982.48, "text": " Sorry, actually not a function, is it?", "tokens": [51020, 4919, 11, 767, 406, 257, 2445, 11, 307, 309, 30, 51168], "temperature": 0.0, "avg_logprob": -0.37991197141882493, "compression_ratio": 1.6821192052980132, "no_speech_prob": 0.00035697012208402157}, {"id": 206, "seek": 96640, "start": 982.48, "end": 987.76, "text": " It's just a tensor, actually.", "tokens": [51168, 467, 311, 445, 257, 40863, 11, 767, 13, 51432], "temperature": 0.0, "avg_logprob": -0.37991197141882493, "compression_ratio": 1.6821192052980132, "no_speech_prob": 0.00035697012208402157}, {"id": 207, "seek": 98776, "start": 987.76, "end": 995.64, "text": " So we're going to pass in the left edge tensor for the same list comprehension, inner list", "tokens": [50364, 407, 321, 434, 516, 281, 1320, 294, 264, 1411, 4691, 40863, 337, 264, 912, 1329, 44991, 11, 7284, 1329, 50758], "temperature": 0.0, "avg_logprob": -0.24159935699112114, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.0003053399850614369}, {"id": 208, "seek": 98776, "start": 995.64, "end": 997.08, "text": " comprehension.", "tokens": [50758, 44991, 13, 50830], "temperature": 0.0, "avg_logprob": -0.24159935699112114, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.0003053399850614369}, {"id": 209, "seek": 98776, "start": 997.08, "end": 1000.2, "text": " And this time we're getting back the left edges.", "tokens": [50830, 400, 341, 565, 321, 434, 1242, 646, 264, 1411, 8819, 13, 50986], "temperature": 0.0, "avg_logprob": -0.24159935699112114, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.0003053399850614369}, {"id": 210, "seek": 98776, "start": 1000.2, "end": 1005.36, "text": " It's highlighting all of the left edges in the digit.", "tokens": [50986, 467, 311, 26551, 439, 295, 264, 1411, 8819, 294, 264, 14293, 13, 51244], "temperature": 0.0, "avg_logprob": -0.24159935699112114, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.0003053399850614369}, {"id": 211, "seek": 98776, "start": 1005.36, "end": 1012.8, "text": " So yeah, this is basically what's happening here, is that a two by two can be looped over", "tokens": [51244, 407, 1338, 11, 341, 307, 1936, 437, 311, 2737, 510, 11, 307, 300, 257, 732, 538, 732, 393, 312, 6367, 292, 670, 51616], "temperature": 0.0, "avg_logprob": -0.24159935699112114, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.0003053399850614369}, {"id": 212, "seek": 98776, "start": 1012.8, "end": 1017.4399999999999, "text": " an image, creating these outputs.", "tokens": [51616, 364, 3256, 11, 4084, 613, 23930, 13, 51848], "temperature": 0.0, "avg_logprob": -0.24159935699112114, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.0003053399850614369}, {"id": 213, "seek": 101744, "start": 1018.12, "end": 1029.52, "text": " Now you'll see here that in the process of doing so, we are losing the outermost pixels", "tokens": [50398, 823, 291, 603, 536, 510, 300, 294, 264, 1399, 295, 884, 370, 11, 321, 366, 7027, 264, 484, 966, 555, 18668, 50968], "temperature": 0.0, "avg_logprob": -0.21482685936821833, "compression_ratio": 1.5940594059405941, "no_speech_prob": 5.255390533420723e-06}, {"id": 214, "seek": 101744, "start": 1029.52, "end": 1030.64, "text": " of our image.", "tokens": [50968, 295, 527, 3256, 13, 51024], "temperature": 0.0, "avg_logprob": -0.21482685936821833, "compression_ratio": 1.5940594059405941, "no_speech_prob": 5.255390533420723e-06}, {"id": 215, "seek": 101744, "start": 1030.64, "end": 1032.0800000000002, "text": " We'll learn about how to fix that later.", "tokens": [51024, 492, 603, 1466, 466, 577, 281, 3191, 300, 1780, 13, 51096], "temperature": 0.0, "avg_logprob": -0.21482685936821833, "compression_ratio": 1.5940594059405941, "no_speech_prob": 5.255390533420723e-06}, {"id": 216, "seek": 101744, "start": 1032.0800000000002, "end": 1038.44, "text": " But just for now, notice that as we are putting our three by three through, for example, in", "tokens": [51096, 583, 445, 337, 586, 11, 3449, 300, 382, 321, 366, 3372, 527, 1045, 538, 1045, 807, 11, 337, 1365, 11, 294, 51414], "temperature": 0.0, "avg_logprob": -0.21482685936821833, "compression_ratio": 1.5940594059405941, "no_speech_prob": 5.255390533420723e-06}, {"id": 217, "seek": 101744, "start": 1038.44, "end": 1044.52, "text": " this five by five, there's only one, two, three places that we can put it going across,", "tokens": [51414, 341, 1732, 538, 1732, 11, 456, 311, 787, 472, 11, 732, 11, 1045, 3190, 300, 321, 393, 829, 309, 516, 2108, 11, 51718], "temperature": 0.0, "avg_logprob": -0.21482685936821833, "compression_ratio": 1.5940594059405941, "no_speech_prob": 5.255390533420723e-06}, {"id": 218, "seek": 104452, "start": 1044.6, "end": 1047.0, "text": " five places, because we need some kind of edge.", "tokens": [50368, 1732, 3190, 11, 570, 321, 643, 512, 733, 295, 4691, 13, 50488], "temperature": 0.0, "avg_logprob": -0.2987011218893117, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.0016229600878432393}, {"id": 219, "seek": 104452, "start": 1047.0, "end": 1050.28, "text": " All right, so that's cool.", "tokens": [50488, 1057, 558, 11, 370, 300, 311, 1627, 13, 50652], "temperature": 0.0, "avg_logprob": -0.2987011218893117, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.0016229600878432393}, {"id": 220, "seek": 104452, "start": 1050.28, "end": 1051.28, "text": " That's a convolution.", "tokens": [50652, 663, 311, 257, 45216, 13, 50702], "temperature": 0.0, "avg_logprob": -0.2987011218893117, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.0016229600878432393}, {"id": 221, "seek": 104452, "start": 1051.28, "end": 1056.56, "text": " And hopefully if you remember back to kind of the Zeiler and Fergus pictures from lesson", "tokens": [50702, 400, 4696, 498, 291, 1604, 646, 281, 733, 295, 264, 4853, 5441, 293, 36790, 5242, 490, 6898, 50966], "temperature": 0.0, "avg_logprob": -0.2987011218893117, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.0016229600878432393}, {"id": 222, "seek": 104452, "start": 1056.56, "end": 1060.8, "text": " one, you might recognize that the kind of first layer of a convolutional network is", "tokens": [50966, 472, 11, 291, 1062, 5521, 300, 264, 733, 295, 700, 4583, 295, 257, 45216, 304, 3209, 307, 51178], "temperature": 0.0, "avg_logprob": -0.2987011218893117, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.0016229600878432393}, {"id": 223, "seek": 104452, "start": 1060.8, "end": 1063.92, "text": " often looking for kind of edges and gradients and things like that.", "tokens": [51178, 2049, 1237, 337, 733, 295, 8819, 293, 2771, 2448, 293, 721, 411, 300, 13, 51334], "temperature": 0.0, "avg_logprob": -0.2987011218893117, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.0016229600878432393}, {"id": 224, "seek": 104452, "start": 1063.92, "end": 1066.12, "text": " And this is how it does it.", "tokens": [51334, 400, 341, 307, 577, 309, 775, 309, 13, 51444], "temperature": 0.0, "avg_logprob": -0.2987011218893117, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.0016229600878432393}, {"id": 225, "seek": 104452, "start": 1066.12, "end": 1071.44, "text": " And then when convolutions on top of convolutions with nonlinear activations between them can", "tokens": [51444, 400, 550, 562, 3754, 15892, 322, 1192, 295, 3754, 15892, 365, 2107, 28263, 2430, 763, 1296, 552, 393, 51710], "temperature": 0.0, "avg_logprob": -0.2987011218893117, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.0016229600878432393}, {"id": 226, "seek": 107144, "start": 1071.44, "end": 1078.48, "text": " combine those into curves or corners or stuff like that, and so on and so forth.", "tokens": [50364, 10432, 729, 666, 19490, 420, 12413, 420, 1507, 411, 300, 11, 293, 370, 322, 293, 370, 5220, 13, 50716], "temperature": 0.0, "avg_logprob": -0.33309794581213664, "compression_ratio": 1.5633187772925765, "no_speech_prob": 0.0034833690151572227}, {"id": 227, "seek": 107144, "start": 1078.48, "end": 1081.1200000000001, "text": " Okay, so how do we do this quickly?", "tokens": [50716, 1033, 11, 370, 577, 360, 321, 360, 341, 2661, 30, 50848], "temperature": 0.0, "avg_logprob": -0.33309794581213664, "compression_ratio": 1.5633187772925765, "no_speech_prob": 0.0034833690151572227}, {"id": 228, "seek": 107144, "start": 1081.1200000000001, "end": 1085.2, "text": " Because currently this is going to be super, super slow doing this in Python.", "tokens": [50848, 1436, 4362, 341, 307, 516, 281, 312, 1687, 11, 1687, 2964, 884, 341, 294, 15329, 13, 51052], "temperature": 0.0, "avg_logprob": -0.33309794581213664, "compression_ratio": 1.5633187772925765, "no_speech_prob": 0.0034833690151572227}, {"id": 229, "seek": 107144, "start": 1085.2, "end": 1093.3200000000002, "text": " So one of the very earliest, or probably the earliest publicly available general purpose", "tokens": [51052, 407, 472, 295, 264, 588, 20573, 11, 420, 1391, 264, 20573, 14843, 2435, 2674, 4334, 51458], "temperature": 0.0, "avg_logprob": -0.33309794581213664, "compression_ratio": 1.5633187772925765, "no_speech_prob": 0.0034833690151572227}, {"id": 230, "seek": 107144, "start": 1093.3200000000002, "end": 1099.52, "text": " deep learning, GPU accelerated deep learning thing I saw was called Caffe.", "tokens": [51458, 2452, 2539, 11, 18407, 29763, 2452, 2539, 551, 286, 1866, 390, 1219, 383, 23629, 13, 51768], "temperature": 0.0, "avg_logprob": -0.33309794581213664, "compression_ratio": 1.5633187772925765, "no_speech_prob": 0.0034833690151572227}, {"id": 231, "seek": 109952, "start": 1099.6, "end": 1102.56, "text": " That was created by somebody called Yangqing Jia.", "tokens": [50368, 663, 390, 2942, 538, 2618, 1219, 11978, 40055, 29242, 13, 50516], "temperature": 0.0, "avg_logprob": -0.3152397907141483, "compression_ratio": 1.4, "no_speech_prob": 0.000732172338757664}, {"id": 232, "seek": 109952, "start": 1102.56, "end": 1113.48, "text": " And he actually described what happened, where Caffe, how Caffe went about implementing a", "tokens": [50516, 400, 415, 767, 7619, 437, 2011, 11, 689, 383, 23629, 11, 577, 383, 23629, 1437, 466, 18114, 257, 51062], "temperature": 0.0, "avg_logprob": -0.3152397907141483, "compression_ratio": 1.4, "no_speech_prob": 0.000732172338757664}, {"id": 233, "seek": 109952, "start": 1113.48, "end": 1117.8799999999999, "text": " fast convolution on a GPU.", "tokens": [51062, 2370, 45216, 322, 257, 18407, 13, 51282], "temperature": 0.0, "avg_logprob": -0.3152397907141483, "compression_ratio": 1.4, "no_speech_prob": 0.000732172338757664}, {"id": 234, "seek": 109952, "start": 1117.8799999999999, "end": 1124.7, "text": " And basically he said, well, I had two months to do it and I had to finish my thesis.", "tokens": [51282, 400, 1936, 415, 848, 11, 731, 11, 286, 632, 732, 2493, 281, 360, 309, 293, 286, 632, 281, 2413, 452, 22288, 13, 51623], "temperature": 0.0, "avg_logprob": -0.3152397907141483, "compression_ratio": 1.4, "no_speech_prob": 0.000732172338757664}, {"id": 235, "seek": 112470, "start": 1124.74, "end": 1132.26, "text": " And so I ended up doing something where I said, well, there was some other code out", "tokens": [50366, 400, 370, 286, 4590, 493, 884, 746, 689, 286, 848, 11, 731, 11, 456, 390, 512, 661, 3089, 484, 50742], "temperature": 0.0, "avg_logprob": -0.3197259053145305, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.001524749444797635}, {"id": 236, "seek": 112470, "start": 1132.26, "end": 1141.1000000000001, "text": " there, Kujewski, who you might've come across, him and Hinton set up a little startup, which", "tokens": [50742, 456, 11, 591, 4579, 1023, 18020, 11, 567, 291, 1062, 600, 808, 2108, 11, 796, 293, 389, 12442, 992, 493, 257, 707, 18578, 11, 597, 51184], "temperature": 0.0, "avg_logprob": -0.3197259053145305, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.001524749444797635}, {"id": 237, "seek": 112470, "start": 1141.1000000000001, "end": 1145.74, "text": " Google bought and that kind of became the start of Google's deep learning, the Google", "tokens": [51184, 3329, 4243, 293, 300, 733, 295, 3062, 264, 722, 295, 3329, 311, 2452, 2539, 11, 264, 3329, 51416], "temperature": 0.0, "avg_logprob": -0.3197259053145305, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.001524749444797635}, {"id": 238, "seek": 112470, "start": 1145.74, "end": 1146.74, "text": " brain basically.", "tokens": [51416, 3567, 1936, 13, 51466], "temperature": 0.0, "avg_logprob": -0.3197259053145305, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.001524749444797635}, {"id": 239, "seek": 112470, "start": 1146.74, "end": 1152.82, "text": " So Kujewski had all this fancy stuff in his library, but Yangqing Jia said, oh, I didn't", "tokens": [51466, 407, 591, 4579, 1023, 18020, 632, 439, 341, 10247, 1507, 294, 702, 6405, 11, 457, 11978, 40055, 29242, 848, 11, 1954, 11, 286, 994, 380, 51770], "temperature": 0.0, "avg_logprob": -0.3197259053145305, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.001524749444797635}, {"id": 240, "seek": 115282, "start": 1152.82, "end": 1154.74, "text": " know how to do all that stuff.", "tokens": [50364, 458, 577, 281, 360, 439, 300, 1507, 13, 50460], "temperature": 0.0, "avg_logprob": -0.272732940880028, "compression_ratio": 1.6878980891719746, "no_speech_prob": 0.022285012528300285}, {"id": 241, "seek": 115282, "start": 1154.74, "end": 1158.06, "text": " So I said, well, I already know how to multiply matrices.", "tokens": [50460, 407, 286, 848, 11, 731, 11, 286, 1217, 458, 577, 281, 12972, 32284, 13, 50626], "temperature": 0.0, "avg_logprob": -0.272732940880028, "compression_ratio": 1.6878980891719746, "no_speech_prob": 0.022285012528300285}, {"id": 242, "seek": 115282, "start": 1158.06, "end": 1163.1, "text": " So maybe I can convert a convolution into a matrix multiplication.", "tokens": [50626, 407, 1310, 286, 393, 7620, 257, 45216, 666, 257, 8141, 27290, 13, 50878], "temperature": 0.0, "avg_logprob": -0.272732940880028, "compression_ratio": 1.6878980891719746, "no_speech_prob": 0.022285012528300285}, {"id": 243, "seek": 115282, "start": 1163.1, "end": 1169.9399999999998, "text": " And so that I became known as IMtocole.", "tokens": [50878, 400, 370, 300, 286, 3062, 2570, 382, 21463, 83, 905, 4812, 13, 51220], "temperature": 0.0, "avg_logprob": -0.272732940880028, "compression_ratio": 1.6878980891719746, "no_speech_prob": 0.022285012528300285}, {"id": 244, "seek": 115282, "start": 1169.9399999999998, "end": 1178.74, "text": " IMtocole is a way of converting a convolution into a matrix multiply.", "tokens": [51220, 21463, 83, 905, 4812, 307, 257, 636, 295, 29942, 257, 45216, 666, 257, 8141, 12972, 13, 51660], "temperature": 0.0, "avg_logprob": -0.272732940880028, "compression_ratio": 1.6878980891719746, "no_speech_prob": 0.022285012528300285}, {"id": 245, "seek": 117874, "start": 1178.74, "end": 1185.22, "text": " And so actually, I don't know if, I suspect Yangqing Jia could have accidentally reinvented", "tokens": [50364, 400, 370, 767, 11, 286, 500, 380, 458, 498, 11, 286, 9091, 11978, 40055, 29242, 727, 362, 15715, 33477, 292, 50688], "temperature": 0.0, "avg_logprob": -0.29444940122839525, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.010816176421940327}, {"id": 246, "seek": 117874, "start": 1185.22, "end": 1191.28, "text": " it because it actually had been around for a while, even at the point that he was writing", "tokens": [50688, 309, 570, 309, 767, 632, 668, 926, 337, 257, 1339, 11, 754, 412, 264, 935, 300, 415, 390, 3579, 50991], "temperature": 0.0, "avg_logprob": -0.29444940122839525, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.010816176421940327}, {"id": 247, "seek": 117874, "start": 1191.28, "end": 1196.14, "text": " his thesis, I believe.", "tokens": [50991, 702, 22288, 11, 286, 1697, 13, 51234], "temperature": 0.0, "avg_logprob": -0.29444940122839525, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.010816176421940327}, {"id": 248, "seek": 117874, "start": 1196.14, "end": 1201.98, "text": " So it was actually, this is the place I believe it was created in this paper.", "tokens": [51234, 407, 309, 390, 767, 11, 341, 307, 264, 1081, 286, 1697, 309, 390, 2942, 294, 341, 3035, 13, 51526], "temperature": 0.0, "avg_logprob": -0.29444940122839525, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.010816176421940327}, {"id": 249, "seek": 120198, "start": 1201.98, "end": 1209.66, "text": " So that was in 2006, which is a while ago.", "tokens": [50364, 407, 300, 390, 294, 14062, 11, 597, 307, 257, 1339, 2057, 13, 50748], "temperature": 0.0, "avg_logprob": -0.2279465522295163, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.008847092278301716}, {"id": 250, "seek": 120198, "start": 1209.66, "end": 1212.9, "text": " And so this is actually from that paper.", "tokens": [50748, 400, 370, 341, 307, 767, 490, 300, 3035, 13, 50910], "temperature": 0.0, "avg_logprob": -0.2279465522295163, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.008847092278301716}, {"id": 251, "seek": 120198, "start": 1212.9, "end": 1223.18, "text": " And what they describe is, let's say you are putting this two by two kernel over this three", "tokens": [50910, 400, 437, 436, 6786, 307, 11, 718, 311, 584, 291, 366, 3372, 341, 732, 538, 732, 28256, 670, 341, 1045, 51424], "temperature": 0.0, "avg_logprob": -0.2279465522295163, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.008847092278301716}, {"id": 252, "seek": 120198, "start": 1223.18, "end": 1224.78, "text": " by three bit of an image.", "tokens": [51424, 538, 1045, 857, 295, 364, 3256, 13, 51504], "temperature": 0.0, "avg_logprob": -0.2279465522295163, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.008847092278301716}, {"id": 253, "seek": 120198, "start": 1224.78, "end": 1229.82, "text": " So here you've got this, this window needs to match to this bit of this window, right?", "tokens": [51504, 407, 510, 291, 600, 658, 341, 11, 341, 4910, 2203, 281, 2995, 281, 341, 857, 295, 341, 4910, 11, 558, 30, 51756], "temperature": 0.0, "avg_logprob": -0.2279465522295163, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.008847092278301716}, {"id": 254, "seek": 122982, "start": 1229.82, "end": 1235.4199999999998, "text": " What you could do is you could unwrap this to one, one, two, two, sorry, one, two, one,", "tokens": [50364, 708, 291, 727, 360, 307, 291, 727, 14853, 4007, 341, 281, 472, 11, 472, 11, 732, 11, 732, 11, 2597, 11, 472, 11, 732, 11, 472, 11, 50644], "temperature": 0.0, "avg_logprob": -0.29381021118164063, "compression_ratio": 2.3642384105960264, "no_speech_prob": 0.053391434252262115}, {"id": 255, "seek": 122982, "start": 1235.4199999999998, "end": 1240.78, "text": " two, downwards to here, one, two, one, two, to unroll it like so.", "tokens": [50644, 732, 11, 39880, 281, 510, 11, 472, 11, 732, 11, 472, 11, 732, 11, 281, 517, 3970, 309, 411, 370, 13, 50912], "temperature": 0.0, "avg_logprob": -0.29381021118164063, "compression_ratio": 2.3642384105960264, "no_speech_prob": 0.053391434252262115}, {"id": 256, "seek": 122982, "start": 1240.78, "end": 1244.3799999999999, "text": " And you could unroll the kernel here.", "tokens": [50912, 400, 291, 727, 517, 3970, 264, 28256, 510, 13, 51092], "temperature": 0.0, "avg_logprob": -0.29381021118164063, "compression_ratio": 2.3642384105960264, "no_speech_prob": 0.053391434252262115}, {"id": 257, "seek": 122982, "start": 1244.3799999999999, "end": 1249.4199999999998, "text": " Yeah, so this is one, two, one, one.", "tokens": [51092, 865, 11, 370, 341, 307, 472, 11, 732, 11, 472, 11, 472, 13, 51344], "temperature": 0.0, "avg_logprob": -0.29381021118164063, "compression_ratio": 2.3642384105960264, "no_speech_prob": 0.053391434252262115}, {"id": 258, "seek": 122982, "start": 1249.4199999999998, "end": 1251.8799999999999, "text": " So this is bit is here, one, two, one, one.", "tokens": [51344, 407, 341, 307, 857, 307, 510, 11, 472, 11, 732, 11, 472, 11, 472, 13, 51467], "temperature": 0.0, "avg_logprob": -0.29381021118164063, "compression_ratio": 2.3642384105960264, "no_speech_prob": 0.053391434252262115}, {"id": 259, "seek": 122982, "start": 1251.8799999999999, "end": 1257.82, "text": " And then you could unroll the kernel one, one, two, two to here, one, one, two, two.", "tokens": [51467, 400, 550, 291, 727, 517, 3970, 264, 28256, 472, 11, 472, 11, 732, 11, 732, 281, 510, 11, 472, 11, 472, 11, 732, 11, 732, 13, 51764], "temperature": 0.0, "avg_logprob": -0.29381021118164063, "compression_ratio": 2.3642384105960264, "no_speech_prob": 0.053391434252262115}, {"id": 260, "seek": 125782, "start": 1257.82, "end": 1262.62, "text": " And then once they've been flattened out and moved in that way, and then you'll do", "tokens": [50364, 400, 550, 1564, 436, 600, 668, 24183, 292, 484, 293, 4259, 294, 300, 636, 11, 293, 550, 291, 603, 360, 50604], "temperature": 0.0, "avg_logprob": -0.2157287116812057, "compression_ratio": 1.8432203389830508, "no_speech_prob": 0.00045831030001863837}, {"id": 261, "seek": 125782, "start": 1262.62, "end": 1266.9399999999998, "text": " exactly the same thing for this next patch here, two, oh, one, three.", "tokens": [50604, 2293, 264, 912, 551, 337, 341, 958, 9972, 510, 11, 732, 11, 1954, 11, 472, 11, 1045, 13, 50820], "temperature": 0.0, "avg_logprob": -0.2157287116812057, "compression_ratio": 1.8432203389830508, "no_speech_prob": 0.00045831030001863837}, {"id": 262, "seek": 125782, "start": 1266.9399999999998, "end": 1269.62, "text": " You flatten it out and put it here, two, oh, one, three.", "tokens": [50820, 509, 24183, 309, 484, 293, 829, 309, 510, 11, 732, 11, 1954, 11, 472, 11, 1045, 13, 50954], "temperature": 0.0, "avg_logprob": -0.2157287116812057, "compression_ratio": 1.8432203389830508, "no_speech_prob": 0.00045831030001863837}, {"id": 263, "seek": 125782, "start": 1269.62, "end": 1274.46, "text": " So if you basically take those kernels and flatten them out in this format, then you", "tokens": [50954, 407, 498, 291, 1936, 747, 729, 23434, 1625, 293, 24183, 552, 484, 294, 341, 7877, 11, 550, 291, 51196], "temperature": 0.0, "avg_logprob": -0.2157287116812057, "compression_ratio": 1.8432203389830508, "no_speech_prob": 0.00045831030001863837}, {"id": 264, "seek": 125782, "start": 1274.46, "end": 1275.86, "text": " end up with a matrix multiply.", "tokens": [51196, 917, 493, 365, 257, 8141, 12972, 13, 51266], "temperature": 0.0, "avg_logprob": -0.2157287116812057, "compression_ratio": 1.8432203389830508, "no_speech_prob": 0.00045831030001863837}, {"id": 265, "seek": 125782, "start": 1275.86, "end": 1284.46, "text": " If you multiply this matrix by this matrix, you'll end up with the output that you want", "tokens": [51266, 759, 291, 12972, 341, 8141, 538, 341, 8141, 11, 291, 603, 917, 493, 365, 264, 5598, 300, 291, 528, 51696], "temperature": 0.0, "avg_logprob": -0.2157287116812057, "compression_ratio": 1.8432203389830508, "no_speech_prob": 0.00045831030001863837}, {"id": 266, "seek": 125782, "start": 1284.46, "end": 1285.72, "text": " from the convolution.", "tokens": [51696, 490, 264, 45216, 13, 51759], "temperature": 0.0, "avg_logprob": -0.2157287116812057, "compression_ratio": 1.8432203389830508, "no_speech_prob": 0.00045831030001863837}, {"id": 267, "seek": 128572, "start": 1285.72, "end": 1293.16, "text": " So this is basically a way of unrolling your kernels and your input features into matrices,", "tokens": [50364, 407, 341, 307, 1936, 257, 636, 295, 517, 18688, 428, 23434, 1625, 293, 428, 4846, 4122, 666, 32284, 11, 50736], "temperature": 0.0, "avg_logprob": -0.2433528556480064, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0006563676870428026}, {"id": 268, "seek": 128572, "start": 1293.16, "end": 1296.8, "text": " such as when you do the matrix multiply, you get the right answer.", "tokens": [50736, 1270, 382, 562, 291, 360, 264, 8141, 12972, 11, 291, 483, 264, 558, 1867, 13, 50918], "temperature": 0.0, "avg_logprob": -0.2433528556480064, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0006563676870428026}, {"id": 269, "seek": 128572, "start": 1296.8, "end": 1299.08, "text": " So it's a kind of a nifty trick.", "tokens": [50918, 407, 309, 311, 257, 733, 295, 257, 297, 37177, 4282, 13, 51032], "temperature": 0.0, "avg_logprob": -0.2433528556480064, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0006563676870428026}, {"id": 270, "seek": 128572, "start": 1299.08, "end": 1304.56, "text": " And so that is called imtocol.", "tokens": [51032, 400, 370, 300, 307, 1219, 566, 83, 4154, 13, 51306], "temperature": 0.0, "avg_logprob": -0.2433528556480064, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0006563676870428026}, {"id": 271, "seek": 128572, "start": 1304.56, "end": 1306.52, "text": " I guess we're kind of cheating a little bit.", "tokens": [51306, 286, 2041, 321, 434, 733, 295, 18309, 257, 707, 857, 13, 51404], "temperature": 0.0, "avg_logprob": -0.2433528556480064, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0006563676870428026}, {"id": 272, "seek": 128572, "start": 1306.52, "end": 1307.88, "text": " Implementing that is kind of boring.", "tokens": [51404, 4331, 43704, 278, 300, 307, 733, 295, 9989, 13, 51472], "temperature": 0.0, "avg_logprob": -0.2433528556480064, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0006563676870428026}, {"id": 273, "seek": 128572, "start": 1307.88, "end": 1311.16, "text": " It's just a bunch of copying and tensor manipulation.", "tokens": [51472, 467, 311, 445, 257, 3840, 295, 27976, 293, 40863, 26475, 13, 51636], "temperature": 0.0, "avg_logprob": -0.2433528556480064, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0006563676870428026}, {"id": 274, "seek": 128572, "start": 1311.16, "end": 1313.72, "text": " So I actually haven't done it.", "tokens": [51636, 407, 286, 767, 2378, 380, 1096, 309, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2433528556480064, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0006563676870428026}, {"id": 275, "seek": 131372, "start": 1313.72, "end": 1321.2, "text": " Instead I've linked to a NumPy implementation, which is here.", "tokens": [50364, 7156, 286, 600, 9408, 281, 257, 22592, 47, 88, 11420, 11, 597, 307, 510, 13, 50738], "temperature": 0.0, "avg_logprob": -0.20400133798288744, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.001987766707316041}, {"id": 276, "seek": 131372, "start": 1321.2, "end": 1329.68, "text": " And it also, part of it is this getIndices, which is here.", "tokens": [50738, 400, 309, 611, 11, 644, 295, 309, 307, 341, 483, 21790, 1473, 11, 597, 307, 510, 13, 51162], "temperature": 0.0, "avg_logprob": -0.20400133798288744, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.001987766707316041}, {"id": 277, "seek": 131372, "start": 1329.68, "end": 1335.0, "text": " And as you can see, it's a little bit tedious with repeats and tiles and reshapes and whatnot.", "tokens": [51162, 400, 382, 291, 393, 536, 11, 309, 311, 257, 707, 857, 38284, 365, 35038, 293, 21982, 293, 725, 71, 569, 279, 293, 25882, 13, 51428], "temperature": 0.0, "avg_logprob": -0.20400133798288744, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.001987766707316041}, {"id": 278, "seek": 131372, "start": 1335.0, "end": 1341.88, "text": " So I'm not going to call it homework, but if you want to practice your tensor indexing", "tokens": [51428, 407, 286, 478, 406, 516, 281, 818, 309, 14578, 11, 457, 498, 291, 528, 281, 3124, 428, 40863, 8186, 278, 51772], "temperature": 0.0, "avg_logprob": -0.20400133798288744, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.001987766707316041}, {"id": 279, "seek": 134188, "start": 1342.0400000000002, "end": 1345.68, "text": " manipulation skills, try creating a PyTorch version from scratch.", "tokens": [50372, 26475, 3942, 11, 853, 4084, 257, 9953, 51, 284, 339, 3037, 490, 8459, 13, 50554], "temperature": 0.0, "avg_logprob": -0.2601463794708252, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.0005614775000140071}, {"id": 280, "seek": 134188, "start": 1345.68, "end": 1348.2800000000002, "text": " I got to admit, I didn't bother.", "tokens": [50554, 286, 658, 281, 9796, 11, 286, 994, 380, 8677, 13, 50684], "temperature": 0.0, "avg_logprob": -0.2601463794708252, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.0005614775000140071}, {"id": 281, "seek": 134188, "start": 1348.2800000000002, "end": 1350.5200000000002, "text": " Instead I used the one that's built into PyTorch.", "tokens": [50684, 7156, 286, 1143, 264, 472, 300, 311, 3094, 666, 9953, 51, 284, 339, 13, 50796], "temperature": 0.0, "avg_logprob": -0.2601463794708252, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.0005614775000140071}, {"id": 282, "seek": 134188, "start": 1350.5200000000002, "end": 1355.88, "text": " And in PyTorch it's called Unfold.", "tokens": [50796, 400, 294, 9953, 51, 284, 339, 309, 311, 1219, 8170, 2641, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2601463794708252, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.0005614775000140071}, {"id": 283, "seek": 134188, "start": 1355.88, "end": 1365.48, "text": " So if we take our image, and PyTorch expects there to be a batch axis and a dimension and", "tokens": [51064, 407, 498, 321, 747, 527, 3256, 11, 293, 9953, 51, 284, 339, 33280, 456, 281, 312, 257, 15245, 10298, 293, 257, 10139, 293, 51544], "temperature": 0.0, "avg_logprob": -0.2601463794708252, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.0005614775000140071}, {"id": 284, "seek": 134188, "start": 1365.48, "end": 1367.24, "text": " a channel dimension.", "tokens": [51544, 257, 2269, 10139, 13, 51632], "temperature": 0.0, "avg_logprob": -0.2601463794708252, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.0005614775000140071}, {"id": 285, "seek": 134188, "start": 1367.24, "end": 1371.24, "text": " So we'll add two unit leading dimensions to it.", "tokens": [51632, 407, 321, 603, 909, 732, 4985, 5775, 12819, 281, 309, 13, 51832], "temperature": 0.0, "avg_logprob": -0.2601463794708252, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.0005614775000140071}, {"id": 286, "seek": 137124, "start": 1371.28, "end": 1378.36, "text": " And we can unfold our input for a 3x3.", "tokens": [50366, 400, 321, 393, 17980, 527, 4846, 337, 257, 805, 87, 18, 13, 50720], "temperature": 0.0, "avg_logprob": -0.38229105423907844, "compression_ratio": 1.1851851851851851, "no_speech_prob": 0.018263686448335648}, {"id": 287, "seek": 137124, "start": 1378.36, "end": 1387.16, "text": " And that will give us a 9x676 input.", "tokens": [50720, 400, 300, 486, 976, 505, 257, 1722, 87, 22452, 21, 4846, 13, 51160], "temperature": 0.0, "avg_logprob": -0.38229105423907844, "compression_ratio": 1.1851851851851851, "no_speech_prob": 0.018263686448335648}, {"id": 288, "seek": 137124, "start": 1387.16, "end": 1390.4, "text": " And so then we can take that, and...", "tokens": [51160, 400, 370, 550, 321, 393, 747, 300, 11, 293, 485, 51322], "temperature": 0.0, "avg_logprob": -0.38229105423907844, "compression_ratio": 1.1851851851851851, "no_speech_prob": 0.018263686448335648}, {"id": 289, "seek": 137124, "start": 1390.4, "end": 1392.92, "text": " Whoopsie daisy.", "tokens": [51322, 45263, 414, 1120, 14169, 13, 51448], "temperature": 0.0, "avg_logprob": -0.38229105423907844, "compression_ratio": 1.1851851851851851, "no_speech_prob": 0.018263686448335648}, {"id": 290, "seek": 139292, "start": 1392.92, "end": 1402.6000000000001, "text": " We can take that, and then we'll take our kernel and just flatten it out into a vector.", "tokens": [50364, 492, 393, 747, 300, 11, 293, 550, 321, 603, 747, 527, 28256, 293, 445, 24183, 309, 484, 666, 257, 8062, 13, 50848], "temperature": 0.0, "avg_logprob": -0.2898227541070235, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.0011513783829286695}, {"id": 291, "seek": 139292, "start": 1402.6000000000001, "end": 1410.4, "text": " So view changes the shape, and minus one just says dump everything into this dimension.", "tokens": [50848, 407, 1910, 2962, 264, 3909, 11, 293, 3175, 472, 445, 1619, 11430, 1203, 666, 341, 10139, 13, 51238], "temperature": 0.0, "avg_logprob": -0.2898227541070235, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.0011513783829286695}, {"id": 292, "seek": 139292, "start": 1410.4, "end": 1416.0800000000002, "text": " So that's going to create a 9 long vector.", "tokens": [51238, 407, 300, 311, 516, 281, 1884, 257, 1722, 938, 8062, 13, 51522], "temperature": 0.0, "avg_logprob": -0.2898227541070235, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.0011513783829286695}, {"id": 293, "seek": 139292, "start": 1416.0800000000002, "end": 1417.44, "text": " Length 9 vector.", "tokens": [51522, 441, 4206, 1722, 8062, 13, 51590], "temperature": 0.0, "avg_logprob": -0.2898227541070235, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.0011513783829286695}, {"id": 294, "seek": 139292, "start": 1417.44, "end": 1420.8600000000001, "text": " And so now we can do the matrix multiply.", "tokens": [51590, 400, 370, 586, 321, 393, 360, 264, 8141, 12972, 13, 51761], "temperature": 0.0, "avg_logprob": -0.2898227541070235, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.0011513783829286695}, {"id": 295, "seek": 142086, "start": 1420.86, "end": 1428.9799999999998, "text": " Just like they've done here, of the kernel matrix, that's our weights, by the unrolled", "tokens": [50364, 1449, 411, 436, 600, 1096, 510, 11, 295, 264, 28256, 8141, 11, 300, 311, 527, 17443, 11, 538, 264, 517, 28850, 50770], "temperature": 0.0, "avg_logprob": -0.219737966295699, "compression_ratio": 1.3696969696969696, "no_speech_prob": 0.0009253828902728856}, {"id": 296, "seek": 142086, "start": 1428.9799999999998, "end": 1432.1, "text": " input features.", "tokens": [50770, 4846, 4122, 13, 50926], "temperature": 0.0, "avg_logprob": -0.219737966295699, "compression_ratio": 1.3696969696969696, "no_speech_prob": 0.0009253828902728856}, {"id": 297, "seek": 142086, "start": 1432.1, "end": 1434.78, "text": " And so that gives us a 676 long.", "tokens": [50926, 400, 370, 300, 2709, 505, 257, 23879, 21, 938, 13, 51060], "temperature": 0.0, "avg_logprob": -0.219737966295699, "compression_ratio": 1.3696969696969696, "no_speech_prob": 0.0009253828902728856}, {"id": 298, "seek": 142086, "start": 1434.78, "end": 1438.1399999999999, "text": " We can then view that as 26x26.", "tokens": [51060, 492, 393, 550, 1910, 300, 382, 7551, 87, 10880, 13, 51228], "temperature": 0.0, "avg_logprob": -0.219737966295699, "compression_ratio": 1.3696969696969696, "no_speech_prob": 0.0009253828902728856}, {"id": 299, "seek": 142086, "start": 1438.1399999999999, "end": 1445.1599999999999, "text": " And we get back, as we hoped, our left edge tensor result.", "tokens": [51228, 400, 321, 483, 646, 11, 382, 321, 19737, 11, 527, 1411, 4691, 40863, 1874, 13, 51579], "temperature": 0.0, "avg_logprob": -0.219737966295699, "compression_ratio": 1.3696969696969696, "no_speech_prob": 0.0009253828902728856}, {"id": 300, "seek": 144516, "start": 1445.16, "end": 1456.3600000000001, "text": " And so this is, yeah, this is how we can kind of, from scratch, create a better implementation", "tokens": [50364, 400, 370, 341, 307, 11, 1338, 11, 341, 307, 577, 321, 393, 733, 295, 11, 490, 8459, 11, 1884, 257, 1101, 11420, 50924], "temperature": 0.0, "avg_logprob": -0.2099212512635348, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.02161395363509655}, {"id": 301, "seek": 144516, "start": 1456.3600000000001, "end": 1457.8000000000002, "text": " of convolutions.", "tokens": [50924, 295, 3754, 15892, 13, 50996], "temperature": 0.0, "avg_logprob": -0.2099212512635348, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.02161395363509655}, {"id": 302, "seek": 144516, "start": 1457.8000000000002, "end": 1462.2, "text": " The reason I'm cheating, I'm allowed to cheat here, is because we did actually create convolutions", "tokens": [50996, 440, 1778, 286, 478, 18309, 11, 286, 478, 4350, 281, 17470, 510, 11, 307, 570, 321, 630, 767, 1884, 3754, 15892, 51216], "temperature": 0.0, "avg_logprob": -0.2099212512635348, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.02161395363509655}, {"id": 303, "seek": 144516, "start": 1462.2, "end": 1463.48, "text": " from scratch.", "tokens": [51216, 490, 8459, 13, 51280], "temperature": 0.0, "avg_logprob": -0.2099212512635348, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.02161395363509655}, {"id": 304, "seek": 144516, "start": 1463.48, "end": 1467.1200000000001, "text": " We're not always creating the GPU optimized versions from scratch, which was never something", "tokens": [51280, 492, 434, 406, 1009, 4084, 264, 18407, 26941, 9606, 490, 8459, 11, 597, 390, 1128, 746, 51462], "temperature": 0.0, "avg_logprob": -0.2099212512635348, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.02161395363509655}, {"id": 305, "seek": 144516, "start": 1467.1200000000001, "end": 1468.1200000000001, "text": " I promised.", "tokens": [51462, 286, 10768, 13, 51512], "temperature": 0.0, "avg_logprob": -0.2099212512635348, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.02161395363509655}, {"id": 306, "seek": 144516, "start": 1468.1200000000001, "end": 1469.1200000000001, "text": " So I think that's fair.", "tokens": [51512, 407, 286, 519, 300, 311, 3143, 13, 51562], "temperature": 0.0, "avg_logprob": -0.2099212512635348, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.02161395363509655}, {"id": 307, "seek": 144516, "start": 1469.1200000000001, "end": 1473.96, "text": " But it's cool that we can kind of hacker out a GPU optimized version in the same way that", "tokens": [51562, 583, 309, 311, 1627, 300, 321, 393, 733, 295, 38155, 484, 257, 18407, 26941, 3037, 294, 264, 912, 636, 300, 51804], "temperature": 0.0, "avg_logprob": -0.2099212512635348, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.02161395363509655}, {"id": 308, "seek": 147396, "start": 1474.0, "end": 1477.56, "text": " the kind of original deep learning library did.", "tokens": [50366, 264, 733, 295, 3380, 2452, 2539, 6405, 630, 13, 50544], "temperature": 0.0, "avg_logprob": -0.25635164701021634, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.029758496209979057}, {"id": 309, "seek": 147396, "start": 1477.56, "end": 1487.0, "text": " So if we use apply kernel, we get nearly 9 milliseconds.", "tokens": [50544, 407, 498, 321, 764, 3079, 28256, 11, 321, 483, 6217, 1722, 34184, 13, 51016], "temperature": 0.0, "avg_logprob": -0.25635164701021634, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.029758496209979057}, {"id": 310, "seek": 147396, "start": 1487.0, "end": 1495.9, "text": " If we use unfold with matrix multiply, we get 20 microseconds.", "tokens": [51016, 759, 321, 764, 17980, 365, 8141, 12972, 11, 321, 483, 945, 3123, 37841, 28750, 13, 51461], "temperature": 0.0, "avg_logprob": -0.25635164701021634, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.029758496209979057}, {"id": 311, "seek": 147396, "start": 1495.9, "end": 1498.52, "text": " So that's what, about 400 times faster.", "tokens": [51461, 407, 300, 311, 437, 11, 466, 8423, 1413, 4663, 13, 51592], "temperature": 0.0, "avg_logprob": -0.25635164701021634, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.029758496209979057}, {"id": 312, "seek": 147396, "start": 1498.52, "end": 1500.76, "text": " So that's pretty cool.", "tokens": [51592, 407, 300, 311, 1238, 1627, 13, 51704], "temperature": 0.0, "avg_logprob": -0.25635164701021634, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.029758496209979057}, {"id": 313, "seek": 150076, "start": 1500.76, "end": 1506.52, "text": " Now of course we don't have to use unfold and matrix multiply, because PyTorch has a", "tokens": [50364, 823, 295, 1164, 321, 500, 380, 362, 281, 764, 17980, 293, 8141, 12972, 11, 570, 9953, 51, 284, 339, 575, 257, 50652], "temperature": 0.0, "avg_logprob": -0.29263870162193223, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.007121250033378601}, {"id": 314, "seek": 150076, "start": 1506.52, "end": 1508.14, "text": " conv2d.", "tokens": [50652, 3754, 17, 67, 13, 50733], "temperature": 0.0, "avg_logprob": -0.29263870162193223, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.007121250033378601}, {"id": 315, "seek": 150076, "start": 1508.14, "end": 1510.2, "text": " So we can run that.", "tokens": [50733, 407, 321, 393, 1190, 300, 13, 50836], "temperature": 0.0, "avg_logprob": -0.29263870162193223, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.007121250033378601}, {"id": 316, "seek": 150076, "start": 1510.2, "end": 1514.36, "text": " And that, interestingly, is about the same speed.", "tokens": [50836, 400, 300, 11, 25873, 11, 307, 466, 264, 912, 3073, 13, 51044], "temperature": 0.0, "avg_logprob": -0.29263870162193223, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.007121250033378601}, {"id": 317, "seek": 150076, "start": 1514.36, "end": 1516.44, "text": " At least on GPU.", "tokens": [51044, 1711, 1935, 322, 18407, 13, 51148], "temperature": 0.0, "avg_logprob": -0.29263870162193223, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.007121250033378601}, {"id": 318, "seek": 150076, "start": 1516.44, "end": 1520.0, "text": " But this would also work on GPU just as well.", "tokens": [51148, 583, 341, 576, 611, 589, 322, 18407, 445, 382, 731, 13, 51326], "temperature": 0.0, "avg_logprob": -0.29263870162193223, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.007121250033378601}, {"id": 319, "seek": 150076, "start": 1520.0, "end": 1523.96, "text": " Yeah, I'm not sure this will always be the case.", "tokens": [51326, 865, 11, 286, 478, 406, 988, 341, 486, 1009, 312, 264, 1389, 13, 51524], "temperature": 0.0, "avg_logprob": -0.29263870162193223, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.007121250033378601}, {"id": 320, "seek": 150076, "start": 1523.96, "end": 1527.92, "text": " In this case it's a pretty small image.", "tokens": [51524, 682, 341, 1389, 309, 311, 257, 1238, 1359, 3256, 13, 51722], "temperature": 0.0, "avg_logprob": -0.29263870162193223, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.007121250033378601}, {"id": 321, "seek": 152792, "start": 1527.92, "end": 1532.8000000000002, "text": " I haven't experimented a whole lot to see whereabouts there's a big difference in speeds", "tokens": [50364, 286, 2378, 380, 5120, 292, 257, 1379, 688, 281, 536, 689, 41620, 456, 311, 257, 955, 2649, 294, 16411, 50608], "temperature": 0.0, "avg_logprob": -0.20822594715998724, "compression_ratio": 1.5846774193548387, "no_speech_prob": 0.0027575793210417032}, {"id": 322, "seek": 152792, "start": 1532.8000000000002, "end": 1533.8000000000002, "text": " between these.", "tokens": [50608, 1296, 613, 13, 50658], "temperature": 0.0, "avg_logprob": -0.20822594715998724, "compression_ratio": 1.5846774193548387, "no_speech_prob": 0.0027575793210417032}, {"id": 323, "seek": 152792, "start": 1533.8000000000002, "end": 1536.72, "text": " Obviously I always just use f.conv2d.", "tokens": [50658, 7580, 286, 1009, 445, 764, 283, 13, 1671, 85, 17, 67, 13, 50804], "temperature": 0.0, "avg_logprob": -0.20822594715998724, "compression_ratio": 1.5846774193548387, "no_speech_prob": 0.0027575793210417032}, {"id": 324, "seek": 152792, "start": 1536.72, "end": 1541.1200000000001, "text": " But if there's some more tricky convolution you need to do with some weird thing around", "tokens": [50804, 583, 498, 456, 311, 512, 544, 12414, 45216, 291, 643, 281, 360, 365, 512, 3657, 551, 926, 51024], "temperature": 0.0, "avg_logprob": -0.20822594715998724, "compression_ratio": 1.5846774193548387, "no_speech_prob": 0.0027575793210417032}, {"id": 325, "seek": 152792, "start": 1541.1200000000001, "end": 1545.52, "text": " channels or dimensions or something, you can always try this unfold trick.", "tokens": [51024, 9235, 420, 12819, 420, 746, 11, 291, 393, 1009, 853, 341, 17980, 4282, 13, 51244], "temperature": 0.0, "avg_logprob": -0.20822594715998724, "compression_ratio": 1.5846774193548387, "no_speech_prob": 0.0027575793210417032}, {"id": 326, "seek": 152792, "start": 1545.52, "end": 1548.7, "text": " It's nice to know it's there, I think.", "tokens": [51244, 467, 311, 1481, 281, 458, 309, 311, 456, 11, 286, 519, 13, 51403], "temperature": 0.0, "avg_logprob": -0.20822594715998724, "compression_ratio": 1.5846774193548387, "no_speech_prob": 0.0027575793210417032}, {"id": 327, "seek": 152792, "start": 1548.7, "end": 1551.94, "text": " So we could do the same thing for diagonal edges.", "tokens": [51403, 407, 321, 727, 360, 264, 912, 551, 337, 21539, 8819, 13, 51565], "temperature": 0.0, "avg_logprob": -0.20822594715998724, "compression_ratio": 1.5846774193548387, "no_speech_prob": 0.0027575793210417032}, {"id": 328, "seek": 155194, "start": 1551.94, "end": 1557.02, "text": " So here's our diagonal edge kernel.", "tokens": [50364, 407, 510, 311, 527, 21539, 4691, 28256, 13, 50618], "temperature": 0.0, "avg_logprob": -0.2594497020427997, "compression_ratio": 1.3458646616541354, "no_speech_prob": 0.002323121065273881}, {"id": 329, "seek": 155194, "start": 1557.02, "end": 1563.18, "text": " Or the other diagonal.", "tokens": [50618, 1610, 264, 661, 21539, 13, 50926], "temperature": 0.0, "avg_logprob": -0.2594497020427997, "compression_ratio": 1.3458646616541354, "no_speech_prob": 0.002323121065273881}, {"id": 330, "seek": 155194, "start": 1563.18, "end": 1578.64, "text": " So if we just grab the first 16 images, then we can do a convolution on our whole batch", "tokens": [50926, 407, 498, 321, 445, 4444, 264, 700, 3165, 5267, 11, 550, 321, 393, 360, 257, 45216, 322, 527, 1379, 15245, 51699], "temperature": 0.0, "avg_logprob": -0.2594497020427997, "compression_ratio": 1.3458646616541354, "no_speech_prob": 0.002323121065273881}, {"id": 331, "seek": 155194, "start": 1578.64, "end": 1581.74, "text": " with all of our kernels at once.", "tokens": [51699, 365, 439, 295, 527, 23434, 1625, 412, 1564, 13, 51854], "temperature": 0.0, "avg_logprob": -0.2594497020427997, "compression_ratio": 1.3458646616541354, "no_speech_prob": 0.002323121065273881}, {"id": 332, "seek": 158174, "start": 1582.54, "end": 1587.98, "text": " So this is a nice optimized thing that we can do.", "tokens": [50404, 407, 341, 307, 257, 1481, 26941, 551, 300, 321, 393, 360, 13, 50676], "temperature": 0.0, "avg_logprob": -0.22965065313845265, "compression_ratio": 1.6280193236714975, "no_speech_prob": 3.480806844891049e-05}, {"id": 333, "seek": 158174, "start": 1587.98, "end": 1598.14, "text": " And you end up with your 26 by 26, you've got your 4 kernels, and you've got your 16", "tokens": [50676, 400, 291, 917, 493, 365, 428, 7551, 538, 7551, 11, 291, 600, 658, 428, 1017, 23434, 1625, 11, 293, 291, 600, 658, 428, 3165, 51184], "temperature": 0.0, "avg_logprob": -0.22965065313845265, "compression_ratio": 1.6280193236714975, "no_speech_prob": 3.480806844891049e-05}, {"id": 334, "seek": 158174, "start": 1598.14, "end": 1599.22, "text": " images.", "tokens": [51184, 5267, 13, 51238], "temperature": 0.0, "avg_logprob": -0.22965065313845265, "compression_ratio": 1.6280193236714975, "no_speech_prob": 3.480806844891049e-05}, {"id": 335, "seek": 158174, "start": 1599.22, "end": 1600.86, "text": " And so that's summarized here.", "tokens": [51238, 400, 370, 300, 311, 14611, 1602, 510, 13, 51320], "temperature": 0.0, "avg_logprob": -0.22965065313845265, "compression_ratio": 1.6280193236714975, "no_speech_prob": 3.480806844891049e-05}, {"id": 336, "seek": 158174, "start": 1600.86, "end": 1604.42, "text": " So that's generally what we're doing to get good GPU acceleration, is we're doing a bunch", "tokens": [51320, 407, 300, 311, 5101, 437, 321, 434, 884, 281, 483, 665, 18407, 17162, 11, 307, 321, 434, 884, 257, 3840, 51498], "temperature": 0.0, "avg_logprob": -0.22965065313845265, "compression_ratio": 1.6280193236714975, "no_speech_prob": 3.480806844891049e-05}, {"id": 337, "seek": 158174, "start": 1604.42, "end": 1611.02, "text": " of kernels and a bunch of images all at once, across all of their pixels.", "tokens": [51498, 295, 23434, 1625, 293, 257, 3840, 295, 5267, 439, 412, 1564, 11, 2108, 439, 295, 641, 18668, 13, 51828], "temperature": 0.0, "avg_logprob": -0.22965065313845265, "compression_ratio": 1.6280193236714975, "no_speech_prob": 3.480806844891049e-05}, {"id": 338, "seek": 161102, "start": 1611.02, "end": 1614.02, "text": " And so here we go.", "tokens": [50364, 400, 370, 510, 321, 352, 13, 50514], "temperature": 0.0, "avg_logprob": -0.3267371933181564, "compression_ratio": 1.3926701570680629, "no_speech_prob": 1.544628139527049e-05}, {"id": 339, "seek": 161102, "start": 1614.02, "end": 1624.62, "text": " That's what happens when we take a look at our various kernels for a particular image.", "tokens": [50514, 663, 311, 437, 2314, 562, 321, 747, 257, 574, 412, 527, 3683, 23434, 1625, 337, 257, 1729, 3256, 13, 51044], "temperature": 0.0, "avg_logprob": -0.3267371933181564, "compression_ratio": 1.3926701570680629, "no_speech_prob": 1.544628139527049e-05}, {"id": 340, "seek": 161102, "start": 1624.62, "end": 1631.02, "text": " Left edge, I guess top edge, and then diagonal, top left, and top right.", "tokens": [51044, 16405, 4691, 11, 286, 2041, 1192, 4691, 11, 293, 550, 21539, 11, 1192, 1411, 11, 293, 1192, 558, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3267371933181564, "compression_ratio": 1.3926701570680629, "no_speech_prob": 1.544628139527049e-05}, {"id": 341, "seek": 161102, "start": 1631.02, "end": 1636.0, "text": " Okay, so that is optimized convolutions.", "tokens": [51364, 1033, 11, 370, 300, 307, 26941, 3754, 15892, 13, 51613], "temperature": 0.0, "avg_logprob": -0.3267371933181564, "compression_ratio": 1.3926701570680629, "no_speech_prob": 1.544628139527049e-05}, {"id": 342, "seek": 161102, "start": 1636.0, "end": 1638.1399999999999, "text": " And that works just as well on the CPU or GPU.", "tokens": [51613, 400, 300, 1985, 445, 382, 731, 322, 264, 13199, 420, 18407, 13, 51720], "temperature": 0.0, "avg_logprob": -0.3267371933181564, "compression_ratio": 1.3926701570680629, "no_speech_prob": 1.544628139527049e-05}, {"id": 343, "seek": 163814, "start": 1638.26, "end": 1642.14, "text": " The GPU will be faster if you have one.", "tokens": [50370, 440, 18407, 486, 312, 4663, 498, 291, 362, 472, 13, 50564], "temperature": 0.0, "avg_logprob": -0.27645776488564233, "compression_ratio": 1.551219512195122, "no_speech_prob": 0.0011335548479110003}, {"id": 344, "seek": 163814, "start": 1642.14, "end": 1649.6200000000001, "text": " Now how do we deal with the problem that we're losing one pixel on each side?", "tokens": [50564, 823, 577, 360, 321, 2028, 365, 264, 1154, 300, 321, 434, 7027, 472, 19261, 322, 1184, 1252, 30, 50938], "temperature": 0.0, "avg_logprob": -0.27645776488564233, "compression_ratio": 1.551219512195122, "no_speech_prob": 0.0011335548479110003}, {"id": 345, "seek": 163814, "start": 1649.6200000000001, "end": 1654.14, "text": " What we can do is we can add something called padding.", "tokens": [50938, 708, 321, 393, 360, 307, 321, 393, 909, 746, 1219, 39562, 13, 51164], "temperature": 0.0, "avg_logprob": -0.27645776488564233, "compression_ratio": 1.551219512195122, "no_speech_prob": 0.0011335548479110003}, {"id": 346, "seek": 163814, "start": 1654.14, "end": 1659.5400000000002, "text": " And for padding, what we basically do is, rather than starting our window here, we start", "tokens": [51164, 400, 337, 39562, 11, 437, 321, 1936, 360, 307, 11, 2831, 813, 2891, 527, 4910, 510, 11, 321, 722, 51434], "temperature": 0.0, "avg_logprob": -0.27645776488564233, "compression_ratio": 1.551219512195122, "no_speech_prob": 0.0011335548479110003}, {"id": 347, "seek": 163814, "start": 1659.5400000000002, "end": 1662.18, "text": " it right over here.", "tokens": [51434, 309, 558, 670, 510, 13, 51566], "temperature": 0.0, "avg_logprob": -0.27645776488564233, "compression_ratio": 1.551219512195122, "no_speech_prob": 0.0011335548479110003}, {"id": 348, "seek": 163814, "start": 1662.18, "end": 1665.46, "text": " And actually we'd be up one as well.", "tokens": [51566, 400, 767, 321, 1116, 312, 493, 472, 382, 731, 13, 51730], "temperature": 0.0, "avg_logprob": -0.27645776488564233, "compression_ratio": 1.551219512195122, "no_speech_prob": 0.0011335548479110003}, {"id": 349, "seek": 166546, "start": 1665.46, "end": 1678.6200000000001, "text": " And so these three on the left here, we just take the input for each of those as zero.", "tokens": [50364, 400, 370, 613, 1045, 322, 264, 1411, 510, 11, 321, 445, 747, 264, 4846, 337, 1184, 295, 729, 382, 4018, 13, 51022], "temperature": 0.0, "avg_logprob": -0.2499880839868919, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0010484603699296713}, {"id": 350, "seek": 166546, "start": 1678.6200000000001, "end": 1681.18, "text": " So we're basically just assuming that they're all zero.", "tokens": [51022, 407, 321, 434, 1936, 445, 11926, 300, 436, 434, 439, 4018, 13, 51150], "temperature": 0.0, "avg_logprob": -0.2499880839868919, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0010484603699296713}, {"id": 351, "seek": 166546, "start": 1681.18, "end": 1683.54, "text": " I mean, there's other options we could choose.", "tokens": [51150, 286, 914, 11, 456, 311, 661, 3956, 321, 727, 2826, 13, 51268], "temperature": 0.0, "avg_logprob": -0.2499880839868919, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0010484603699296713}, {"id": 352, "seek": 166546, "start": 1683.54, "end": 1687.74, "text": " We could assume they're the same as the one next to them.", "tokens": [51268, 492, 727, 6552, 436, 434, 264, 912, 382, 264, 472, 958, 281, 552, 13, 51478], "temperature": 0.0, "avg_logprob": -0.2499880839868919, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0010484603699296713}, {"id": 353, "seek": 166546, "start": 1687.74, "end": 1690.26, "text": " There's various things we can do, but the simplest and the one we normally do is just", "tokens": [51478, 821, 311, 3683, 721, 321, 393, 360, 11, 457, 264, 22811, 293, 264, 472, 321, 5646, 360, 307, 445, 51604], "temperature": 0.0, "avg_logprob": -0.2499880839868919, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0010484603699296713}, {"id": 354, "seek": 166546, "start": 1690.26, "end": 1692.74, "text": " assume that they're zero.", "tokens": [51604, 6552, 300, 436, 434, 4018, 13, 51728], "temperature": 0.0, "avg_logprob": -0.2499880839868919, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0010484603699296713}, {"id": 355, "seek": 169274, "start": 1692.74, "end": 1702.46, "text": " So now, so let's say for example, this is called one pixel padding.", "tokens": [50364, 407, 586, 11, 370, 718, 311, 584, 337, 1365, 11, 341, 307, 1219, 472, 19261, 39562, 13, 50850], "temperature": 0.0, "avg_logprob": -0.29690558910369874, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.00039821185055188835}, {"id": 356, "seek": 169274, "start": 1702.46, "end": 1704.7, "text": " Let's say we did two pixel padding.", "tokens": [50850, 961, 311, 584, 321, 630, 732, 19261, 39562, 13, 50962], "temperature": 0.0, "avg_logprob": -0.29690558910369874, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.00039821185055188835}, {"id": 357, "seek": 169274, "start": 1704.7, "end": 1713.46, "text": " So we had two pixel padding with a five by five input, and a four by four kernel.", "tokens": [50962, 407, 321, 632, 732, 19261, 39562, 365, 257, 1732, 538, 1732, 4846, 11, 293, 257, 1451, 538, 1451, 28256, 13, 51400], "temperature": 0.0, "avg_logprob": -0.29690558910369874, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.00039821185055188835}, {"id": 358, "seek": 169274, "start": 1713.46, "end": 1716.58, "text": " So that grays our kernel.", "tokens": [51400, 407, 300, 677, 3772, 527, 28256, 13, 51556], "temperature": 0.0, "avg_logprob": -0.29690558910369874, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.00039821185055188835}, {"id": 359, "seek": 169274, "start": 1716.58, "end": 1720.86, "text": " Then we're going to start right up way over here on the corner.", "tokens": [51556, 1396, 321, 434, 516, 281, 722, 558, 493, 636, 670, 510, 322, 264, 4538, 13, 51770], "temperature": 0.0, "avg_logprob": -0.29690558910369874, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.00039821185055188835}, {"id": 360, "seek": 172086, "start": 1720.9799999999998, "end": 1726.3799999999999, "text": " And then you can see what happens as we slide the kernel over.", "tokens": [50370, 400, 550, 291, 393, 536, 437, 2314, 382, 321, 4137, 264, 28256, 670, 13, 50640], "temperature": 0.0, "avg_logprob": -0.2544572867599188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006070744711905718}, {"id": 361, "seek": 172086, "start": 1726.3799999999999, "end": 1728.28, "text": " There's all the spots that it's going to take.", "tokens": [50640, 821, 311, 439, 264, 10681, 300, 309, 311, 516, 281, 747, 13, 50735], "temperature": 0.0, "avg_logprob": -0.2544572867599188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006070744711905718}, {"id": 362, "seek": 172086, "start": 1728.28, "end": 1735.3, "text": " And so this dotted line area is the area that we're kind of effectively going through.", "tokens": [50735, 400, 370, 341, 37459, 1622, 1859, 307, 264, 1859, 300, 321, 434, 733, 295, 8659, 516, 807, 13, 51086], "temperature": 0.0, "avg_logprob": -0.2544572867599188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006070744711905718}, {"id": 363, "seek": 172086, "start": 1735.3, "end": 1738.4199999999998, "text": " But all of these white bits we're just going to treat as zero.", "tokens": [51086, 583, 439, 295, 613, 2418, 9239, 321, 434, 445, 516, 281, 2387, 382, 4018, 13, 51242], "temperature": 0.0, "avg_logprob": -0.2544572867599188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006070744711905718}, {"id": 364, "seek": 172086, "start": 1738.4199999999998, "end": 1742.3999999999999, "text": " And so then this green is the output size we end up with, which is going to be six by", "tokens": [51242, 400, 370, 550, 341, 3092, 307, 264, 5598, 2744, 321, 917, 493, 365, 11, 597, 307, 516, 281, 312, 2309, 538, 51441], "temperature": 0.0, "avg_logprob": -0.2544572867599188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006070744711905718}, {"id": 365, "seek": 172086, "start": 1742.3999999999999, "end": 1747.86, "text": " six for a five by five input.", "tokens": [51441, 2309, 337, 257, 1732, 538, 1732, 4846, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2544572867599188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006070744711905718}, {"id": 366, "seek": 174786, "start": 1747.86, "end": 1754.34, "text": " I should mention even numbered edge kernels are not used very often.", "tokens": [50364, 286, 820, 2152, 754, 40936, 4691, 23434, 1625, 366, 406, 1143, 588, 2049, 13, 50688], "temperature": 0.0, "avg_logprob": -0.22620183771306818, "compression_ratio": 1.917808219178082, "no_speech_prob": 0.00048029518802650273}, {"id": 367, "seek": 174786, "start": 1754.34, "end": 1755.9799999999998, "text": " We normally used odd numbered kernels.", "tokens": [50688, 492, 5646, 1143, 7401, 40936, 23434, 1625, 13, 50770], "temperature": 0.0, "avg_logprob": -0.22620183771306818, "compression_ratio": 1.917808219178082, "no_speech_prob": 0.00048029518802650273}, {"id": 368, "seek": 174786, "start": 1755.9799999999998, "end": 1761.1799999999998, "text": " If you use, for example, a three by three kernel and one pixel of padding, you will", "tokens": [50770, 759, 291, 764, 11, 337, 1365, 11, 257, 1045, 538, 1045, 28256, 293, 472, 19261, 295, 39562, 11, 291, 486, 51030], "temperature": 0.0, "avg_logprob": -0.22620183771306818, "compression_ratio": 1.917808219178082, "no_speech_prob": 0.00048029518802650273}, {"id": 369, "seek": 174786, "start": 1761.1799999999998, "end": 1763.54, "text": " get back the same size you start with.", "tokens": [51030, 483, 646, 264, 912, 2744, 291, 722, 365, 13, 51148], "temperature": 0.0, "avg_logprob": -0.22620183771306818, "compression_ratio": 1.917808219178082, "no_speech_prob": 0.00048029518802650273}, {"id": 370, "seek": 174786, "start": 1763.54, "end": 1769.06, "text": " If you use five by five with three pixels of padding, you'll end up with the same size", "tokens": [51148, 759, 291, 764, 1732, 538, 1732, 365, 1045, 18668, 295, 39562, 11, 291, 603, 917, 493, 365, 264, 912, 2744, 51424], "temperature": 0.0, "avg_logprob": -0.22620183771306818, "compression_ratio": 1.917808219178082, "no_speech_prob": 0.00048029518802650273}, {"id": 371, "seek": 174786, "start": 1769.06, "end": 1770.06, "text": " you start with.", "tokens": [51424, 291, 722, 365, 13, 51474], "temperature": 0.0, "avg_logprob": -0.22620183771306818, "compression_ratio": 1.917808219178082, "no_speech_prob": 0.00048029518802650273}, {"id": 372, "seek": 174786, "start": 1770.06, "end": 1775.1799999999998, "text": " So generally, odd numbered edge size kernels are easier to deal with, to make sure you", "tokens": [51474, 407, 5101, 11, 7401, 40936, 4691, 2744, 23434, 1625, 366, 3571, 281, 2028, 365, 11, 281, 652, 988, 291, 51730], "temperature": 0.0, "avg_logprob": -0.22620183771306818, "compression_ratio": 1.917808219178082, "no_speech_prob": 0.00048029518802650273}, {"id": 373, "seek": 177518, "start": 1775.3400000000001, "end": 1777.5800000000002, "text": " end up with the same thing you start with.", "tokens": [50372, 917, 493, 365, 264, 912, 551, 291, 722, 365, 13, 50484], "temperature": 0.0, "avg_logprob": -0.38075723782391613, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.01566300168633461}, {"id": 374, "seek": 177518, "start": 1777.5800000000002, "end": 1787.74, "text": " Okay, so yeah, so as it says here, if you've got an odd numbered size, ks by ks size kernel,", "tokens": [50484, 1033, 11, 370, 1338, 11, 370, 382, 309, 1619, 510, 11, 498, 291, 600, 658, 364, 7401, 40936, 2744, 11, 350, 82, 538, 350, 82, 2744, 28256, 11, 50992], "temperature": 0.0, "avg_logprob": -0.38075723782391613, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.01566300168633461}, {"id": 375, "seek": 177518, "start": 1787.74, "end": 1793.1000000000001, "text": " then ks truncate divide two.", "tokens": [50992, 550, 350, 82, 504, 409, 66, 473, 9845, 732, 13, 51260], "temperature": 0.0, "avg_logprob": -0.38075723782391613, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.01566300168633461}, {"id": 376, "seek": 177518, "start": 1793.1000000000001, "end": 1797.1000000000001, "text": " That's what slash slash means will give you the right size.", "tokens": [51260, 663, 311, 437, 17330, 17330, 1355, 486, 976, 291, 264, 558, 2744, 13, 51460], "temperature": 0.0, "avg_logprob": -0.38075723782391613, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.01566300168633461}, {"id": 377, "seek": 179710, "start": 1797.1, "end": 1805.1999999999998, "text": " And so another trick you can do is you don't always have to just move your window across", "tokens": [50364, 400, 370, 1071, 4282, 291, 393, 360, 307, 291, 500, 380, 1009, 362, 281, 445, 1286, 428, 4910, 2108, 50769], "temperature": 0.0, "avg_logprob": -0.2175353442051614, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.02332758530974388}, {"id": 378, "seek": 179710, "start": 1805.1999999999998, "end": 1807.02, "text": " by one each time.", "tokens": [50769, 538, 472, 1184, 565, 13, 50860], "temperature": 0.0, "avg_logprob": -0.2175353442051614, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.02332758530974388}, {"id": 379, "seek": 179710, "start": 1807.02, "end": 1809.58, "text": " You could move it by a different amount each time.", "tokens": [50860, 509, 727, 1286, 309, 538, 257, 819, 2372, 1184, 565, 13, 50988], "temperature": 0.0, "avg_logprob": -0.2175353442051614, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.02332758530974388}, {"id": 380, "seek": 179710, "start": 1809.58, "end": 1813.3, "text": " The amount you move it by is called the stride.", "tokens": [50988, 440, 2372, 291, 1286, 309, 538, 307, 1219, 264, 1056, 482, 13, 51174], "temperature": 0.0, "avg_logprob": -0.2175353442051614, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.02332758530974388}, {"id": 381, "seek": 179710, "start": 1813.3, "end": 1816.04, "text": " So for example, here's a case of doing a stride two.", "tokens": [51174, 407, 337, 1365, 11, 510, 311, 257, 1389, 295, 884, 257, 1056, 482, 732, 13, 51311], "temperature": 0.0, "avg_logprob": -0.2175353442051614, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.02332758530974388}, {"id": 382, "seek": 179710, "start": 1816.04, "end": 1817.54, "text": " So with stride two padding one.", "tokens": [51311, 407, 365, 1056, 482, 732, 39562, 472, 13, 51386], "temperature": 0.0, "avg_logprob": -0.2175353442051614, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.02332758530974388}, {"id": 383, "seek": 179710, "start": 1817.54, "end": 1822.8999999999999, "text": " So we start out here, and then we jump across two, and then we jump across two, and then", "tokens": [51386, 407, 321, 722, 484, 510, 11, 293, 550, 321, 3012, 2108, 732, 11, 293, 550, 321, 3012, 2108, 732, 11, 293, 550, 51654], "temperature": 0.0, "avg_logprob": -0.2175353442051614, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.02332758530974388}, {"id": 384, "seek": 179710, "start": 1822.8999999999999, "end": 1824.6999999999998, "text": " we go to the next row.", "tokens": [51654, 321, 352, 281, 264, 958, 5386, 13, 51744], "temperature": 0.0, "avg_logprob": -0.2175353442051614, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.02332758530974388}, {"id": 385, "seek": 179710, "start": 1824.6999999999998, "end": 1826.6599999999999, "text": " So that's called a stride two convolution.", "tokens": [51744, 407, 300, 311, 1219, 257, 1056, 482, 732, 45216, 13, 51842], "temperature": 0.0, "avg_logprob": -0.2175353442051614, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.02332758530974388}, {"id": 386, "seek": 182666, "start": 1827.22, "end": 1832.74, "text": " Stride two convolutions are handy because they actually reduce the dimensionality of", "tokens": [50392, 8251, 482, 732, 3754, 15892, 366, 13239, 570, 436, 767, 5407, 264, 10139, 1860, 295, 50668], "temperature": 0.0, "avg_logprob": -0.23326744971337257, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.00025315600214526057}, {"id": 387, "seek": 182666, "start": 1832.74, "end": 1837.94, "text": " your input by a factor of two.", "tokens": [50668, 428, 4846, 538, 257, 5952, 295, 732, 13, 50928], "temperature": 0.0, "avg_logprob": -0.23326744971337257, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.00025315600214526057}, {"id": 388, "seek": 182666, "start": 1837.94, "end": 1841.8200000000002, "text": " And that's actually what we want to do a lot.", "tokens": [50928, 400, 300, 311, 767, 437, 321, 528, 281, 360, 257, 688, 13, 51122], "temperature": 0.0, "avg_logprob": -0.23326744971337257, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.00025315600214526057}, {"id": 389, "seek": 182666, "start": 1841.8200000000002, "end": 1845.8000000000002, "text": " For example, with an autoencoder, we want to do that.", "tokens": [51122, 1171, 1365, 11, 365, 364, 8399, 22660, 19866, 11, 321, 528, 281, 360, 300, 13, 51321], "temperature": 0.0, "avg_logprob": -0.23326744971337257, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.00025315600214526057}, {"id": 390, "seek": 182666, "start": 1845.8000000000002, "end": 1850.5400000000002, "text": " And in fact, for most classification architectures, we do exactly that.", "tokens": [51321, 400, 294, 1186, 11, 337, 881, 21538, 6331, 1303, 11, 321, 360, 2293, 300, 13, 51558], "temperature": 0.0, "avg_logprob": -0.23326744971337257, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.00025315600214526057}, {"id": 391, "seek": 185054, "start": 1850.54, "end": 1858.02, "text": " We keep on reducing the grid size by a factor of two again and again and again, using stride", "tokens": [50364, 492, 1066, 322, 12245, 264, 10748, 2744, 538, 257, 5952, 295, 732, 797, 293, 797, 293, 797, 11, 1228, 1056, 482, 50738], "temperature": 0.0, "avg_logprob": -0.2715759710832076, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.008985164575278759}, {"id": 392, "seek": 185054, "start": 1858.02, "end": 1861.96, "text": " two convolutions with padding of one.", "tokens": [50738, 732, 3754, 15892, 365, 39562, 295, 472, 13, 50935], "temperature": 0.0, "avg_logprob": -0.2715759710832076, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.008985164575278759}, {"id": 393, "seek": 185054, "start": 1861.96, "end": 1863.98, "text": " So that's strides and padding.", "tokens": [50935, 407, 300, 311, 1056, 1875, 293, 39562, 13, 51036], "temperature": 0.0, "avg_logprob": -0.2715759710832076, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.008985164575278759}, {"id": 394, "seek": 185054, "start": 1863.98, "end": 1869.78, "text": " So let's go ahead and create a conv net using these approaches.", "tokens": [51036, 407, 718, 311, 352, 2286, 293, 1884, 257, 220, 1671, 85, 2533, 1228, 613, 11587, 13, 51326], "temperature": 0.0, "avg_logprob": -0.2715759710832076, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.008985164575278759}, {"id": 395, "seek": 185054, "start": 1869.78, "end": 1874.56, "text": " So we're going to get our size of our training set.", "tokens": [51326, 407, 321, 434, 516, 281, 483, 527, 2744, 295, 527, 3097, 992, 13, 51565], "temperature": 0.0, "avg_logprob": -0.2715759710832076, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.008985164575278759}, {"id": 396, "seek": 185054, "start": 1874.56, "end": 1875.76, "text": " This is all the same as before.", "tokens": [51565, 639, 307, 439, 264, 912, 382, 949, 13, 51625], "temperature": 0.0, "avg_logprob": -0.2715759710832076, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.008985164575278759}, {"id": 397, "seek": 187576, "start": 1875.76, "end": 1882.56, "text": " Number of categories, number of digits, size of our hidden layer.", "tokens": [50364, 5118, 295, 10479, 11, 1230, 295, 27011, 11, 2744, 295, 527, 7633, 4583, 13, 50704], "temperature": 0.0, "avg_logprob": -0.3628144025802612, "compression_ratio": 1.3220338983050848, "no_speech_prob": 0.09137734025716782}, {"id": 398, "seek": 187576, "start": 1882.56, "end": 1893.72, "text": " So", "tokens": [50704, 407, 51262], "temperature": 0.0, "avg_logprob": -0.3628144025802612, "compression_ratio": 1.3220338983050848, "no_speech_prob": 0.09137734025716782}, {"id": 399, "seek": 187576, "start": 1893.72, "end": 1905.02, "text": " previously with our sequential linear models, with our MLPs, we basically went from the", "tokens": [51262, 8046, 365, 527, 42881, 8213, 5245, 11, 365, 527, 21601, 23043, 11, 321, 1936, 1437, 490, 264, 51827], "temperature": 0.0, "avg_logprob": -0.3628144025802612, "compression_ratio": 1.3220338983050848, "no_speech_prob": 0.09137734025716782}, {"id": 400, "seek": 190502, "start": 1905.02, "end": 1913.02, "text": " number of pixels to the number of hidden, and then a value, and then the number of hidden", "tokens": [50364, 1230, 295, 18668, 281, 264, 1230, 295, 7633, 11, 293, 550, 257, 2158, 11, 293, 550, 264, 1230, 295, 7633, 50764], "temperature": 0.0, "avg_logprob": -0.24299389461301407, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0015978374285623431}, {"id": 401, "seek": 190502, "start": 1913.02, "end": 1914.98, "text": " to the number of outputs.", "tokens": [50764, 281, 264, 1230, 295, 23930, 13, 50862], "temperature": 0.0, "avg_logprob": -0.24299389461301407, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0015978374285623431}, {"id": 402, "seek": 190502, "start": 1914.98, "end": 1918.78, "text": " So here's the equivalent with a convolution.", "tokens": [50862, 407, 510, 311, 264, 10344, 365, 257, 45216, 13, 51052], "temperature": 0.0, "avg_logprob": -0.24299389461301407, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0015978374285623431}, {"id": 403, "seek": 190502, "start": 1918.78, "end": 1924.18, "text": " Now the problem is that you can't just do that because the output is not now 10 probabilities", "tokens": [51052, 823, 264, 1154, 307, 300, 291, 393, 380, 445, 360, 300, 570, 264, 5598, 307, 406, 586, 1266, 33783, 51322], "temperature": 0.0, "avg_logprob": -0.24299389461301407, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0015978374285623431}, {"id": 404, "seek": 190502, "start": 1924.18, "end": 1931.1, "text": " for each item in our batch, but it's 10 probabilities for each item in our batch for each of 28", "tokens": [51322, 337, 1184, 3174, 294, 527, 15245, 11, 457, 309, 311, 1266, 33783, 337, 1184, 3174, 294, 527, 15245, 337, 1184, 295, 7562, 51668], "temperature": 0.0, "avg_logprob": -0.24299389461301407, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0015978374285623431}, {"id": 405, "seek": 190502, "start": 1931.1, "end": 1934.42, "text": " by 28 pixels, because we don't even have a stride or anything.", "tokens": [51668, 538, 7562, 18668, 11, 570, 321, 500, 380, 754, 362, 257, 1056, 482, 420, 1340, 13, 51834], "temperature": 0.0, "avg_logprob": -0.24299389461301407, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0015978374285623431}, {"id": 406, "seek": 193442, "start": 1934.8200000000002, "end": 1939.0600000000002, "text": " So you can't just use the same simple approach that we had for MLP.", "tokens": [50384, 407, 291, 393, 380, 445, 764, 264, 912, 2199, 3109, 300, 321, 632, 337, 21601, 47, 13, 50596], "temperature": 0.0, "avg_logprob": -0.19786874179182382, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.0289506462868303e-05}, {"id": 407, "seek": 193442, "start": 1939.0600000000002, "end": 1941.8200000000002, "text": " We have to be a bit more careful.", "tokens": [50596, 492, 362, 281, 312, 257, 857, 544, 5026, 13, 50734], "temperature": 0.0, "avg_logprob": -0.19786874179182382, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.0289506462868303e-05}, {"id": 408, "seek": 193442, "start": 1941.8200000000002, "end": 1949.3000000000002, "text": " So to make life easier, let's create a little conv function that does a conv2d with a stride", "tokens": [50734, 407, 281, 652, 993, 3571, 11, 718, 311, 1884, 257, 707, 3754, 2445, 300, 775, 257, 3754, 17, 67, 365, 257, 1056, 482, 51108], "temperature": 0.0, "avg_logprob": -0.19786874179182382, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.0289506462868303e-05}, {"id": 409, "seek": 193442, "start": 1949.3000000000002, "end": 1953.22, "text": " of two, optionally followed by an activation.", "tokens": [51108, 295, 732, 11, 3614, 379, 6263, 538, 364, 24433, 13, 51304], "temperature": 0.0, "avg_logprob": -0.19786874179182382, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.0289506462868303e-05}, {"id": 410, "seek": 193442, "start": 1953.22, "end": 1960.76, "text": " So if act is true, we will add in a ReLU activation.", "tokens": [51304, 407, 498, 605, 307, 2074, 11, 321, 486, 909, 294, 257, 1300, 43, 52, 24433, 13, 51681], "temperature": 0.0, "avg_logprob": -0.19786874179182382, "compression_ratio": 1.4723618090452262, "no_speech_prob": 1.0289506462868303e-05}, {"id": 411, "seek": 196076, "start": 1960.76, "end": 1966.8, "text": " So this is going to either return a conv2d or a little sequential containing a conv2d", "tokens": [50364, 407, 341, 307, 516, 281, 2139, 2736, 257, 3754, 17, 67, 420, 257, 707, 42881, 19273, 257, 3754, 17, 67, 50666], "temperature": 0.0, "avg_logprob": -0.24237785717048269, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0021826736629009247}, {"id": 412, "seek": 196076, "start": 1966.8, "end": 1968.86, "text": " followed by a ReLU.", "tokens": [50666, 6263, 538, 257, 1300, 43, 52, 13, 50769], "temperature": 0.0, "avg_logprob": -0.24237785717048269, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0021826736629009247}, {"id": 413, "seek": 196076, "start": 1968.86, "end": 1976.72, "text": " And so now we can create a CNN from, you know, from scratch as a sequential model.", "tokens": [50769, 400, 370, 586, 321, 393, 1884, 257, 24859, 490, 11, 291, 458, 11, 490, 8459, 382, 257, 42881, 2316, 13, 51162], "temperature": 0.0, "avg_logprob": -0.24237785717048269, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0021826736629009247}, {"id": 414, "seek": 196076, "start": 1976.72, "end": 1984.12, "text": " And so since activation is true by default, this is going to take our 28 by 28 image,", "tokens": [51162, 400, 370, 1670, 24433, 307, 2074, 538, 7576, 11, 341, 307, 516, 281, 747, 527, 7562, 538, 7562, 3256, 11, 51532], "temperature": 0.0, "avg_logprob": -0.24237785717048269, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0021826736629009247}, {"id": 415, "seek": 196076, "start": 1984.12, "end": 1988.52, "text": " starting with one channel and creating an output of four channels.", "tokens": [51532, 2891, 365, 472, 2269, 293, 4084, 364, 5598, 295, 1451, 9235, 13, 51752], "temperature": 0.0, "avg_logprob": -0.24237785717048269, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0021826736629009247}, {"id": 416, "seek": 196076, "start": 1988.52, "end": 1990.54, "text": " So this is the number of in.", "tokens": [51752, 407, 341, 307, 264, 1230, 295, 294, 13, 51853], "temperature": 0.0, "avg_logprob": -0.24237785717048269, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0021826736629009247}, {"id": 417, "seek": 199054, "start": 1990.62, "end": 1992.1, "text": " This is the number of filters.", "tokens": [50368, 639, 307, 264, 1230, 295, 15995, 13, 50442], "temperature": 0.0, "avg_logprob": -0.22356096903483072, "compression_ratio": 1.8373983739837398, "no_speech_prob": 2.2474070647149347e-05}, {"id": 418, "seek": 199054, "start": 1992.1, "end": 1996.62, "text": " Sometimes we'll say filters to describe the number of kind of channels that our convolution", "tokens": [50442, 4803, 321, 603, 584, 15995, 281, 6786, 264, 1230, 295, 733, 295, 9235, 300, 527, 45216, 50668], "temperature": 0.0, "avg_logprob": -0.22356096903483072, "compression_ratio": 1.8373983739837398, "no_speech_prob": 2.2474070647149347e-05}, {"id": 419, "seek": 199054, "start": 1996.62, "end": 1997.62, "text": " has.", "tokens": [50668, 575, 13, 50718], "temperature": 0.0, "avg_logprob": -0.22356096903483072, "compression_ratio": 1.8373983739837398, "no_speech_prob": 2.2474070647149347e-05}, {"id": 420, "seek": 199054, "start": 1997.62, "end": 1999.5, "text": " That's the number of outputs.", "tokens": [50718, 663, 311, 264, 1230, 295, 23930, 13, 50812], "temperature": 0.0, "avg_logprob": -0.22356096903483072, "compression_ratio": 1.8373983739837398, "no_speech_prob": 2.2474070647149347e-05}, {"id": 421, "seek": 199054, "start": 1999.5, "end": 2003.8999999999999, "text": " And it's very similar to the idea of the number of outputs in a linear layer, except this", "tokens": [50812, 400, 309, 311, 588, 2531, 281, 264, 1558, 295, 264, 1230, 295, 23930, 294, 257, 8213, 4583, 11, 3993, 341, 51032], "temperature": 0.0, "avg_logprob": -0.22356096903483072, "compression_ratio": 1.8373983739837398, "no_speech_prob": 2.2474070647149347e-05}, {"id": 422, "seek": 199054, "start": 2003.8999999999999, "end": 2008.5, "text": " is the number of outputs in your convolution.", "tokens": [51032, 307, 264, 1230, 295, 23930, 294, 428, 45216, 13, 51262], "temperature": 0.0, "avg_logprob": -0.22356096903483072, "compression_ratio": 1.8373983739837398, "no_speech_prob": 2.2474070647149347e-05}, {"id": 423, "seek": 199054, "start": 2008.5, "end": 2012.78, "text": " So what I like to do when I create stuff like this is I add a little comment just to remind", "tokens": [51262, 407, 437, 286, 411, 281, 360, 562, 286, 1884, 1507, 411, 341, 307, 286, 909, 257, 707, 2871, 445, 281, 4160, 51476], "temperature": 0.0, "avg_logprob": -0.22356096903483072, "compression_ratio": 1.8373983739837398, "no_speech_prob": 2.2474070647149347e-05}, {"id": 424, "seek": 199054, "start": 2012.78, "end": 2015.6599999999999, "text": " myself what is my grid size after this.", "tokens": [51476, 2059, 437, 307, 452, 10748, 2744, 934, 341, 13, 51620], "temperature": 0.0, "avg_logprob": -0.22356096903483072, "compression_ratio": 1.8373983739837398, "no_speech_prob": 2.2474070647149347e-05}, {"id": 425, "seek": 199054, "start": 2015.6599999999999, "end": 2018.1, "text": " So I had a 28 by 28 input.", "tokens": [51620, 407, 286, 632, 257, 7562, 538, 7562, 4846, 13, 51742], "temperature": 0.0, "avg_logprob": -0.22356096903483072, "compression_ratio": 1.8373983739837398, "no_speech_prob": 2.2474070647149347e-05}, {"id": 426, "seek": 201810, "start": 2018.1, "end": 2021.02, "text": " So then I've then put it through a stride2 conv.", "tokens": [50364, 407, 550, 286, 600, 550, 829, 309, 807, 257, 1056, 482, 17, 3754, 13, 50510], "temperature": 0.0, "avg_logprob": -0.21774619709361684, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.00021318631479516625}, {"id": 427, "seek": 201810, "start": 2021.02, "end": 2024.5, "text": " So the output of this will be 14 by 14.", "tokens": [50510, 407, 264, 5598, 295, 341, 486, 312, 3499, 538, 3499, 13, 50684], "temperature": 0.0, "avg_logprob": -0.21774619709361684, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.00021318631479516625}, {"id": 428, "seek": 201810, "start": 2024.5, "end": 2028.86, "text": " So then we'll do the same thing again, but this time we'll go from a four channel input", "tokens": [50684, 407, 550, 321, 603, 360, 264, 912, 551, 797, 11, 457, 341, 565, 321, 603, 352, 490, 257, 1451, 2269, 4846, 50902], "temperature": 0.0, "avg_logprob": -0.21774619709361684, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.00021318631479516625}, {"id": 429, "seek": 201810, "start": 2028.86, "end": 2033.4199999999998, "text": " to an eight channel output, and then from eight to 16.", "tokens": [50902, 281, 364, 3180, 2269, 5598, 11, 293, 550, 490, 3180, 281, 3165, 13, 51130], "temperature": 0.0, "avg_logprob": -0.21774619709361684, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.00021318631479516625}, {"id": 430, "seek": 201810, "start": 2033.4199999999998, "end": 2041.34, "text": " So by this point, we're now down to a four by four, and then down to a two by two.", "tokens": [51130, 407, 538, 341, 935, 11, 321, 434, 586, 760, 281, 257, 1451, 538, 1451, 11, 293, 550, 760, 281, 257, 732, 538, 732, 13, 51526], "temperature": 0.0, "avg_logprob": -0.21774619709361684, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.00021318631479516625}, {"id": 431, "seek": 201810, "start": 2041.34, "end": 2044.74, "text": " And then finally, we're down to a one by one.", "tokens": [51526, 400, 550, 2721, 11, 321, 434, 760, 281, 257, 472, 538, 472, 13, 51696], "temperature": 0.0, "avg_logprob": -0.21774619709361684, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.00021318631479516625}, {"id": 432, "seek": 204474, "start": 2044.74, "end": 2049.2, "text": " So on the very last layer, we won't add an activation.", "tokens": [50364, 407, 322, 264, 588, 1036, 4583, 11, 321, 1582, 380, 909, 364, 24433, 13, 50587], "temperature": 0.0, "avg_logprob": -0.2584939684186663, "compression_ratio": 1.5488372093023255, "no_speech_prob": 0.0012448230991140008}, {"id": 433, "seek": 204474, "start": 2049.2, "end": 2053.0, "text": " And the very last layer is going to create 10 outputs.", "tokens": [50587, 400, 264, 588, 1036, 4583, 307, 516, 281, 1884, 1266, 23930, 13, 50777], "temperature": 0.0, "avg_logprob": -0.2584939684186663, "compression_ratio": 1.5488372093023255, "no_speech_prob": 0.0012448230991140008}, {"id": 434, "seek": 204474, "start": 2053.0, "end": 2057.38, "text": " And since we're now down to a one by one, we can just call flatten, and that's going", "tokens": [50777, 400, 1670, 321, 434, 586, 760, 281, 257, 472, 538, 472, 11, 321, 393, 445, 818, 24183, 11, 293, 300, 311, 516, 50996], "temperature": 0.0, "avg_logprob": -0.2584939684186663, "compression_ratio": 1.5488372093023255, "no_speech_prob": 0.0012448230991140008}, {"id": 435, "seek": 204474, "start": 2057.38, "end": 2061.58, "text": " to remove those unnecessary unit axes.", "tokens": [50996, 281, 4159, 729, 19350, 4985, 35387, 13, 51206], "temperature": 0.0, "avg_logprob": -0.2584939684186663, "compression_ratio": 1.5488372093023255, "no_speech_prob": 0.0012448230991140008}, {"id": 436, "seek": 204474, "start": 2061.58, "end": 2067.86, "text": " So if we take that, pop our mini-batch through it, we end up with exactly what we want.", "tokens": [51206, 407, 498, 321, 747, 300, 11, 1665, 527, 8382, 12, 65, 852, 807, 309, 11, 321, 917, 493, 365, 2293, 437, 321, 528, 13, 51520], "temperature": 0.0, "avg_logprob": -0.2584939684186663, "compression_ratio": 1.5488372093023255, "no_speech_prob": 0.0012448230991140008}, {"id": 437, "seek": 204474, "start": 2067.86, "end": 2068.94, "text": " A 16 by 10.", "tokens": [51520, 316, 3165, 538, 1266, 13, 51574], "temperature": 0.0, "avg_logprob": -0.2584939684186663, "compression_ratio": 1.5488372093023255, "no_speech_prob": 0.0012448230991140008}, {"id": 438, "seek": 206894, "start": 2068.94, "end": 2080.86, "text": " So for each of our 16 images, we've got 10 probabilities of each possible digit.", "tokens": [50364, 407, 337, 1184, 295, 527, 3165, 5267, 11, 321, 600, 658, 1266, 33783, 295, 1184, 1944, 14293, 13, 50960], "temperature": 0.0, "avg_logprob": -0.24844942213613777, "compression_ratio": 1.6132596685082874, "no_speech_prob": 0.0025908814277499914}, {"id": 439, "seek": 206894, "start": 2080.86, "end": 2086.14, "text": " So if we take our training set and make it into 28 by 28 images, and we do the same thing", "tokens": [50960, 407, 498, 321, 747, 527, 3097, 992, 293, 652, 309, 666, 7562, 538, 7562, 5267, 11, 293, 321, 360, 264, 912, 551, 51224], "temperature": 0.0, "avg_logprob": -0.24844942213613777, "compression_ratio": 1.6132596685082874, "no_speech_prob": 0.0025908814277499914}, {"id": 440, "seek": 206894, "start": 2086.14, "end": 2093.42, "text": " for a validation set, and then we create two data sets, one for each, which we'll call", "tokens": [51224, 337, 257, 24071, 992, 11, 293, 550, 321, 1884, 732, 1412, 6352, 11, 472, 337, 1184, 11, 597, 321, 603, 818, 51588], "temperature": 0.0, "avg_logprob": -0.24844942213613777, "compression_ratio": 1.6132596685082874, "no_speech_prob": 0.0025908814277499914}, {"id": 441, "seek": 206894, "start": 2093.42, "end": 2096.7000000000003, "text": " train data set and valid data set.", "tokens": [51588, 3847, 1412, 992, 293, 7363, 1412, 992, 13, 51752], "temperature": 0.0, "avg_logprob": -0.24844942213613777, "compression_ratio": 1.6132596685082874, "no_speech_prob": 0.0025908814277499914}, {"id": 442, "seek": 209670, "start": 2096.74, "end": 2101.8599999999997, "text": " And we're now going to train this on the GPU.", "tokens": [50366, 400, 321, 434, 586, 516, 281, 3847, 341, 322, 264, 18407, 13, 50622], "temperature": 0.0, "avg_logprob": -0.27962304927684645, "compression_ratio": 1.75, "no_speech_prob": 0.014728132635354996}, {"id": 443, "seek": 209670, "start": 2101.8599999999997, "end": 2108.9399999999996, "text": " Now if you've got a Mac, you can use a device called, well if you've got an Apple Silicon", "tokens": [50622, 823, 498, 291, 600, 658, 257, 5707, 11, 291, 393, 764, 257, 4302, 1219, 11, 731, 498, 291, 600, 658, 364, 6373, 25351, 50976], "temperature": 0.0, "avg_logprob": -0.27962304927684645, "compression_ratio": 1.75, "no_speech_prob": 0.014728132635354996}, {"id": 444, "seek": 209670, "start": 2108.9399999999996, "end": 2114.8199999999997, "text": " Mac, you've got a device called MPS, which is going to use your Mac's GPU.", "tokens": [50976, 5707, 11, 291, 600, 658, 257, 4302, 1219, 376, 6273, 11, 597, 307, 516, 281, 764, 428, 5707, 311, 18407, 13, 51270], "temperature": 0.0, "avg_logprob": -0.27962304927684645, "compression_ratio": 1.75, "no_speech_prob": 0.014728132635354996}, {"id": 445, "seek": 209670, "start": 2114.8199999999997, "end": 2119.02, "text": " Or if you've got Nvidia, you can use CUDA, which will use your Nvidia GPU.", "tokens": [51270, 1610, 498, 291, 600, 658, 46284, 11, 291, 393, 764, 29777, 7509, 11, 597, 486, 764, 428, 46284, 18407, 13, 51480], "temperature": 0.0, "avg_logprob": -0.27962304927684645, "compression_ratio": 1.75, "no_speech_prob": 0.014728132635354996}, {"id": 446, "seek": 209670, "start": 2119.02, "end": 2125.5, "text": " CUDA's 10 times or more, possibly much more faster than a Mac, so you definitely want", "tokens": [51480, 29777, 7509, 311, 1266, 1413, 420, 544, 11, 6264, 709, 544, 4663, 813, 257, 5707, 11, 370, 291, 2138, 528, 51804], "temperature": 0.0, "avg_logprob": -0.27962304927684645, "compression_ratio": 1.75, "no_speech_prob": 0.014728132635354996}, {"id": 447, "seek": 212550, "start": 2125.5, "end": 2129.3, "text": " to use Nvidia if you can.", "tokens": [50364, 281, 764, 46284, 498, 291, 393, 13, 50554], "temperature": 0.0, "avg_logprob": -0.1996316401163737, "compression_ratio": 1.84765625, "no_speech_prob": 0.07807280868291855}, {"id": 448, "seek": 212550, "start": 2129.3, "end": 2132.66, "text": " But if you're just running it on a Mac laptop or whatever, you can use MPS.", "tokens": [50554, 583, 498, 291, 434, 445, 2614, 309, 322, 257, 5707, 10732, 420, 2035, 11, 291, 393, 764, 376, 6273, 13, 50722], "temperature": 0.0, "avg_logprob": -0.1996316401163737, "compression_ratio": 1.84765625, "no_speech_prob": 0.07807280868291855}, {"id": 449, "seek": 212550, "start": 2132.66, "end": 2134.94, "text": " So basically you want to know what device to use.", "tokens": [50722, 407, 1936, 291, 528, 281, 458, 437, 4302, 281, 764, 13, 50836], "temperature": 0.0, "avg_logprob": -0.1996316401163737, "compression_ratio": 1.84765625, "no_speech_prob": 0.07807280868291855}, {"id": 450, "seek": 212550, "start": 2134.94, "end": 2137.06, "text": " Do we want to use CUDA or MPS?", "tokens": [50836, 1144, 321, 528, 281, 764, 29777, 7509, 420, 376, 6273, 30, 50942], "temperature": 0.0, "avg_logprob": -0.1996316401163737, "compression_ratio": 1.84765625, "no_speech_prob": 0.07807280868291855}, {"id": 451, "seek": 212550, "start": 2137.06, "end": 2138.54, "text": " You can check.", "tokens": [50942, 509, 393, 1520, 13, 51016], "temperature": 0.0, "avg_logprob": -0.1996316401163737, "compression_ratio": 1.84765625, "no_speech_prob": 0.07807280868291855}, {"id": 452, "seek": 212550, "start": 2138.54, "end": 2144.46, "text": " You can check torch.backends.mps.isavailable to see if you're running on a Mac with MPS.", "tokens": [51016, 509, 393, 1520, 27822, 13, 3207, 2581, 13, 76, 1878, 13, 271, 706, 32699, 281, 536, 498, 291, 434, 2614, 322, 257, 5707, 365, 376, 6273, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1996316401163737, "compression_ratio": 1.84765625, "no_speech_prob": 0.07807280868291855}, {"id": 453, "seek": 212550, "start": 2144.46, "end": 2149.14, "text": " You can check torch.cuda.isavailable to see if you've got an Nvidia GPU, in which case", "tokens": [51312, 509, 393, 1520, 27822, 13, 66, 11152, 13, 271, 706, 32699, 281, 536, 498, 291, 600, 658, 364, 46284, 18407, 11, 294, 597, 1389, 51546], "temperature": 0.0, "avg_logprob": -0.1996316401163737, "compression_ratio": 1.84765625, "no_speech_prob": 0.07807280868291855}, {"id": 454, "seek": 212550, "start": 2149.14, "end": 2150.14, "text": " you've got CUDA.", "tokens": [51546, 291, 600, 658, 29777, 7509, 13, 51596], "temperature": 0.0, "avg_logprob": -0.1996316401163737, "compression_ratio": 1.84765625, "no_speech_prob": 0.07807280868291855}, {"id": 455, "seek": 212550, "start": 2150.14, "end": 2155.46, "text": " And if you've got neither, of course you'll have to use the CPU to do computation.", "tokens": [51596, 400, 498, 291, 600, 658, 9662, 11, 295, 1164, 291, 603, 362, 281, 764, 264, 13199, 281, 360, 24903, 13, 51862], "temperature": 0.0, "avg_logprob": -0.1996316401163737, "compression_ratio": 1.84765625, "no_speech_prob": 0.07807280868291855}, {"id": 456, "seek": 215546, "start": 2156.42, "end": 2162.34, "text": " So I've created a little function here, to device, which takes a tensor or a dictionary", "tokens": [50412, 407, 286, 600, 2942, 257, 707, 2445, 510, 11, 281, 4302, 11, 597, 2516, 257, 40863, 420, 257, 25890, 50708], "temperature": 0.0, "avg_logprob": -0.25432657641033796, "compression_ratio": 1.7659574468085106, "no_speech_prob": 2.2125545001472346e-05}, {"id": 457, "seek": 215546, "start": 2162.34, "end": 2169.26, "text": " or a list of tensors or whatever, and a device to move it to, and it just goes through and", "tokens": [50708, 420, 257, 1329, 295, 10688, 830, 420, 2035, 11, 293, 257, 4302, 281, 1286, 309, 281, 11, 293, 309, 445, 1709, 807, 293, 51054], "temperature": 0.0, "avg_logprob": -0.25432657641033796, "compression_ratio": 1.7659574468085106, "no_speech_prob": 2.2125545001472346e-05}, {"id": 458, "seek": 215546, "start": 2169.26, "end": 2173.02, "text": " moves everything onto that device.", "tokens": [51054, 6067, 1203, 3911, 300, 4302, 13, 51242], "temperature": 0.0, "avg_logprob": -0.25432657641033796, "compression_ratio": 1.7659574468085106, "no_speech_prob": 2.2125545001472346e-05}, {"id": 459, "seek": 215546, "start": 2173.02, "end": 2177.62, "text": " Or if it's a dictionary, a dictionary of things, values moved onto that device.", "tokens": [51242, 1610, 498, 309, 311, 257, 25890, 11, 257, 25890, 295, 721, 11, 4190, 4259, 3911, 300, 4302, 13, 51472], "temperature": 0.0, "avg_logprob": -0.25432657641033796, "compression_ratio": 1.7659574468085106, "no_speech_prob": 2.2125545001472346e-05}, {"id": 460, "seek": 215546, "start": 2177.62, "end": 2179.62, "text": " So there's your handy little function.", "tokens": [51472, 407, 456, 311, 428, 13239, 707, 2445, 13, 51572], "temperature": 0.0, "avg_logprob": -0.25432657641033796, "compression_ratio": 1.7659574468085106, "no_speech_prob": 2.2125545001472346e-05}, {"id": 461, "seek": 217962, "start": 2179.62, "end": 2188.74, "text": " And so we can create a custom collate function, which calls the PyTorch default collation", "tokens": [50364, 400, 370, 321, 393, 1884, 257, 2375, 1263, 473, 2445, 11, 597, 5498, 264, 9953, 51, 284, 339, 7576, 1263, 399, 50820], "temperature": 0.0, "avg_logprob": -0.2257683563232422, "compression_ratio": 1.541899441340782, "no_speech_prob": 0.00023413468443322927}, {"id": 462, "seek": 217962, "start": 2188.74, "end": 2193.6, "text": " function and then puts those tensors onto our device.", "tokens": [50820, 2445, 293, 550, 8137, 729, 10688, 830, 3911, 527, 4302, 13, 51063], "temperature": 0.0, "avg_logprob": -0.2257683563232422, "compression_ratio": 1.541899441340782, "no_speech_prob": 0.00023413468443322927}, {"id": 463, "seek": 217962, "start": 2193.6, "end": 2202.2799999999997, "text": " And so with that, we've now got enough to run, train this neural net on the GPU.", "tokens": [51063, 400, 370, 365, 300, 11, 321, 600, 586, 658, 1547, 281, 1190, 11, 3847, 341, 18161, 2533, 322, 264, 18407, 13, 51497], "temperature": 0.0, "avg_logprob": -0.2257683563232422, "compression_ratio": 1.541899441340782, "no_speech_prob": 0.00023413468443322927}, {"id": 464, "seek": 217962, "start": 2202.2799999999997, "end": 2206.54, "text": " We created this getDls function in the last lesson.", "tokens": [51497, 492, 2942, 341, 483, 35, 11784, 2445, 294, 264, 1036, 6898, 13, 51710], "temperature": 0.0, "avg_logprob": -0.2257683563232422, "compression_ratio": 1.541899441340782, "no_speech_prob": 0.00023413468443322927}, {"id": 465, "seek": 220654, "start": 2206.54, "end": 2213.5, "text": " So we're going to use that, passing in the data sets that we just created, and our default", "tokens": [50364, 407, 321, 434, 516, 281, 764, 300, 11, 8437, 294, 264, 1412, 6352, 300, 321, 445, 2942, 11, 293, 527, 7576, 50712], "temperature": 0.0, "avg_logprob": -0.2770715193314986, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.0005792860756628215}, {"id": 466, "seek": 220654, "start": 2213.5, "end": 2214.5, "text": " collation function.", "tokens": [50712, 1263, 399, 2445, 13, 50762], "temperature": 0.0, "avg_logprob": -0.2770715193314986, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.0005792860756628215}, {"id": 467, "seek": 220654, "start": 2214.5, "end": 2221.22, "text": " We're going to create our optimizer using our CNNs parameters.", "tokens": [50762, 492, 434, 516, 281, 1884, 527, 5028, 6545, 1228, 527, 24859, 82, 9834, 13, 51098], "temperature": 0.0, "avg_logprob": -0.2770715193314986, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.0005792860756628215}, {"id": 468, "seek": 220654, "start": 2221.22, "end": 2224.02, "text": " And then we call fit.", "tokens": [51098, 400, 550, 321, 818, 3318, 13, 51238], "temperature": 0.0, "avg_logprob": -0.2770715193314986, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.0005792860756628215}, {"id": 469, "seek": 220654, "start": 2224.02, "end": 2227.96, "text": " Now fit, remember, we also created in our last lesson.", "tokens": [51238, 823, 3318, 11, 1604, 11, 321, 611, 2942, 294, 527, 1036, 6898, 13, 51435], "temperature": 0.0, "avg_logprob": -0.2770715193314986, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.0005792860756628215}, {"id": 470, "seek": 220654, "start": 2227.96, "end": 2229.7799999999997, "text": " And it's done.", "tokens": [51435, 400, 309, 311, 1096, 13, 51526], "temperature": 0.0, "avg_logprob": -0.2770715193314986, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.0005792860756628215}, {"id": 471, "seek": 222978, "start": 2229.78, "end": 2237.7400000000002, "text": " So what I did then was I reduced the learning rate by a factor of four, and ran it again.", "tokens": [50364, 407, 437, 286, 630, 550, 390, 286, 9212, 264, 2539, 3314, 538, 257, 5952, 295, 1451, 11, 293, 5872, 309, 797, 13, 50762], "temperature": 0.0, "avg_logprob": -0.2182438830111889, "compression_ratio": 1.518181818181818, "no_speech_prob": 0.001597797847352922}, {"id": 472, "seek": 222978, "start": 2237.7400000000002, "end": 2249.2000000000003, "text": " And eventually, yeah, I got to a fairly similar accuracy to what we did on our MLP.", "tokens": [50762, 400, 4728, 11, 1338, 11, 286, 658, 281, 257, 6457, 2531, 14170, 281, 437, 321, 630, 322, 527, 21601, 47, 13, 51335], "temperature": 0.0, "avg_logprob": -0.2182438830111889, "compression_ratio": 1.518181818181818, "no_speech_prob": 0.001597797847352922}, {"id": 473, "seek": 222978, "start": 2249.2000000000003, "end": 2252.3, "text": " So yeah, we've got a convolutional network working.", "tokens": [51335, 407, 1338, 11, 321, 600, 658, 257, 45216, 304, 3209, 1364, 13, 51490], "temperature": 0.0, "avg_logprob": -0.2182438830111889, "compression_ratio": 1.518181818181818, "no_speech_prob": 0.001597797847352922}, {"id": 474, "seek": 222978, "start": 2252.3, "end": 2254.1400000000003, "text": " I think that's pretty encouraging.", "tokens": [51490, 286, 519, 300, 311, 1238, 14580, 13, 51582], "temperature": 0.0, "avg_logprob": -0.2182438830111889, "compression_ratio": 1.518181818181818, "no_speech_prob": 0.001597797847352922}, {"id": 475, "seek": 222978, "start": 2254.1400000000003, "end": 2258.34, "text": " And it's nice that to train it, we didn't have to write much code, right?", "tokens": [51582, 400, 309, 311, 1481, 300, 281, 3847, 309, 11, 321, 994, 380, 362, 281, 2464, 709, 3089, 11, 558, 30, 51792], "temperature": 0.0, "avg_logprob": -0.2182438830111889, "compression_ratio": 1.518181818181818, "no_speech_prob": 0.001597797847352922}, {"id": 476, "seek": 225834, "start": 2258.34, "end": 2261.2400000000002, "text": " We were able to use code that we had already built.", "tokens": [50364, 492, 645, 1075, 281, 764, 3089, 300, 321, 632, 1217, 3094, 13, 50509], "temperature": 0.0, "avg_logprob": -0.23833620750297935, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.00011774422455346212}, {"id": 477, "seek": 225834, "start": 2261.2400000000002, "end": 2267.34, "text": " We were able to use the data set class that we made, the getDls function that we made,", "tokens": [50509, 492, 645, 1075, 281, 764, 264, 1412, 992, 1508, 300, 321, 1027, 11, 264, 483, 35, 11784, 2445, 300, 321, 1027, 11, 50814], "temperature": 0.0, "avg_logprob": -0.23833620750297935, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.00011774422455346212}, {"id": 478, "seek": 225834, "start": 2267.34, "end": 2269.38, "text": " and the fit function that we made.", "tokens": [50814, 293, 264, 3318, 2445, 300, 321, 1027, 13, 50916], "temperature": 0.0, "avg_logprob": -0.23833620750297935, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.00011774422455346212}, {"id": 479, "seek": 225834, "start": 2269.38, "end": 2276.6200000000003, "text": " And you know, because those things are written in a fairly general way, they work just as", "tokens": [50916, 400, 291, 458, 11, 570, 729, 721, 366, 3720, 294, 257, 6457, 2674, 636, 11, 436, 589, 445, 382, 51278], "temperature": 0.0, "avg_logprob": -0.23833620750297935, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.00011774422455346212}, {"id": 480, "seek": 225834, "start": 2276.6200000000003, "end": 2279.1400000000003, "text": " well for a conv net as they did for an MLP.", "tokens": [51278, 731, 337, 257, 3754, 2533, 382, 436, 630, 337, 364, 21601, 47, 13, 51404], "temperature": 0.0, "avg_logprob": -0.23833620750297935, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.00011774422455346212}, {"id": 481, "seek": 225834, "start": 2279.1400000000003, "end": 2280.1400000000003, "text": " Nothing had to change.", "tokens": [51404, 6693, 632, 281, 1319, 13, 51454], "temperature": 0.0, "avg_logprob": -0.23833620750297935, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.00011774422455346212}, {"id": 482, "seek": 225834, "start": 2280.1400000000003, "end": 2282.6600000000003, "text": " So that was nice.", "tokens": [51454, 407, 300, 390, 1481, 13, 51580], "temperature": 0.0, "avg_logprob": -0.23833620750297935, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.00011774422455346212}, {"id": 483, "seek": 225834, "start": 2282.6600000000003, "end": 2287.9, "text": " Notice I had to take the model and put it on the device as well.", "tokens": [51580, 13428, 286, 632, 281, 747, 264, 2316, 293, 829, 309, 322, 264, 4302, 382, 731, 13, 51842], "temperature": 0.0, "avg_logprob": -0.23833620750297935, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.00011774422455346212}, {"id": 484, "seek": 228790, "start": 2288.46, "end": 2292.7400000000002, "text": " So that will go through and basically put all of the tensors that are in that model", "tokens": [50392, 407, 300, 486, 352, 807, 293, 1936, 829, 439, 295, 264, 10688, 830, 300, 366, 294, 300, 2316, 50606], "temperature": 0.0, "avg_logprob": -0.2186353047688802, "compression_ratio": 1.4390243902439024, "no_speech_prob": 0.0003150383708998561}, {"id": 485, "seek": 228790, "start": 2292.7400000000002, "end": 2302.06, "text": " onto the MPS or CUDA device, if appropriate.", "tokens": [50606, 3911, 264, 376, 6273, 420, 29777, 7509, 4302, 11, 498, 6854, 13, 51072], "temperature": 0.0, "avg_logprob": -0.2186353047688802, "compression_ratio": 1.4390243902439024, "no_speech_prob": 0.0003150383708998561}, {"id": 486, "seek": 228790, "start": 2302.06, "end": 2309.5, "text": " So if we've got a batch size of 64, and as we do, one channel, 28 by 28, so then our", "tokens": [51072, 407, 498, 321, 600, 658, 257, 15245, 2744, 295, 12145, 11, 293, 382, 321, 360, 11, 472, 2269, 11, 7562, 538, 7562, 11, 370, 550, 527, 51444], "temperature": 0.0, "avg_logprob": -0.2186353047688802, "compression_ratio": 1.4390243902439024, "no_speech_prob": 0.0003150383708998561}, {"id": 487, "seek": 228790, "start": 2309.5, "end": 2312.98, "text": " axes are batch, channel, height, width.", "tokens": [51444, 35387, 366, 15245, 11, 2269, 11, 6681, 11, 11402, 13, 51618], "temperature": 0.0, "avg_logprob": -0.2186353047688802, "compression_ratio": 1.4390243902439024, "no_speech_prob": 0.0003150383708998561}, {"id": 488, "seek": 228790, "start": 2312.98, "end": 2317.1800000000003, "text": " So normally, this is referred to as nchw.", "tokens": [51618, 407, 5646, 11, 341, 307, 10839, 281, 382, 297, 339, 86, 13, 51828], "temperature": 0.0, "avg_logprob": -0.2186353047688802, "compression_ratio": 1.4390243902439024, "no_speech_prob": 0.0003150383708998561}, {"id": 489, "seek": 231718, "start": 2317.46, "end": 2323.4199999999996, "text": " So n, generally when you see n in a paper or whatever, in this way, it's referring to", "tokens": [50378, 407, 297, 11, 5101, 562, 291, 536, 297, 294, 257, 3035, 420, 2035, 11, 294, 341, 636, 11, 309, 311, 13761, 281, 50676], "temperature": 0.0, "avg_logprob": -0.29502528837357445, "compression_ratio": 1.55, "no_speech_prob": 2.1782641852041706e-05}, {"id": 490, "seek": 231718, "start": 2323.4199999999996, "end": 2325.02, "text": " the batch size.", "tokens": [50676, 264, 15245, 2744, 13, 50756], "temperature": 0.0, "avg_logprob": -0.29502528837357445, "compression_ratio": 1.55, "no_speech_prob": 2.1782641852041706e-05}, {"id": 491, "seek": 231718, "start": 2325.02, "end": 2330.18, "text": " N being the number, that's the mnemonic, the number of items in the batch.", "tokens": [50756, 426, 885, 264, 1230, 11, 300, 311, 264, 275, 25989, 11630, 11, 264, 1230, 295, 4754, 294, 264, 15245, 13, 51014], "temperature": 0.0, "avg_logprob": -0.29502528837357445, "compression_ratio": 1.55, "no_speech_prob": 2.1782641852041706e-05}, {"id": 492, "seek": 231718, "start": 2330.18, "end": 2335.8599999999997, "text": " C is the number of channels, height by width, nchw.", "tokens": [51014, 383, 307, 264, 1230, 295, 9235, 11, 6681, 538, 11402, 11, 297, 339, 86, 13, 51298], "temperature": 0.0, "avg_logprob": -0.29502528837357445, "compression_ratio": 1.55, "no_speech_prob": 2.1782641852041706e-05}, {"id": 493, "seek": 231718, "start": 2335.8599999999997, "end": 2337.94, "text": " TensorFlow doesn't use that.", "tokens": [51298, 37624, 1177, 380, 764, 300, 13, 51402], "temperature": 0.0, "avg_logprob": -0.29502528837357445, "compression_ratio": 1.55, "no_speech_prob": 2.1782641852041706e-05}, {"id": 494, "seek": 231718, "start": 2337.94, "end": 2342.06, "text": " TensorFlow uses nhwc.", "tokens": [51402, 37624, 4960, 6245, 86, 66, 13, 51608], "temperature": 0.0, "avg_logprob": -0.29502528837357445, "compression_ratio": 1.55, "no_speech_prob": 2.1782641852041706e-05}, {"id": 495, "seek": 234206, "start": 2342.06, "end": 2349.34, "text": " So we generally call these, that channels last, since channels are at the end.", "tokens": [50364, 407, 321, 5101, 818, 613, 11, 300, 9235, 1036, 11, 1670, 9235, 366, 412, 264, 917, 13, 50728], "temperature": 0.0, "avg_logprob": -0.3034518368272896, "compression_ratio": 1.772972972972973, "no_speech_prob": 0.00030534749384969473}, {"id": 496, "seek": 234206, "start": 2349.34, "end": 2352.98, "text": " And this one we normally call channels first.", "tokens": [50728, 400, 341, 472, 321, 5646, 818, 9235, 700, 13, 50910], "temperature": 0.0, "avg_logprob": -0.3034518368272896, "compression_ratio": 1.772972972972973, "no_speech_prob": 0.00030534749384969473}, {"id": 497, "seek": 234206, "start": 2352.98, "end": 2359.14, "text": " Now of course, it's not actually channels first.", "tokens": [50910, 823, 295, 1164, 11, 309, 311, 406, 767, 9235, 700, 13, 51218], "temperature": 0.0, "avg_logprob": -0.3034518368272896, "compression_ratio": 1.772972972972973, "no_speech_prob": 0.00030534749384969473}, {"id": 498, "seek": 234206, "start": 2359.14, "end": 2364.7, "text": " It's actually channel second, but we ignore the batch bit.", "tokens": [51218, 467, 311, 767, 2269, 1150, 11, 457, 321, 11200, 264, 15245, 857, 13, 51496], "temperature": 0.0, "avg_logprob": -0.3034518368272896, "compression_ratio": 1.772972972972973, "no_speech_prob": 0.00030534749384969473}, {"id": 499, "seek": 234206, "start": 2364.7, "end": 2369.02, "text": " In some models, particularly some more modern models, it turns out the channels last is", "tokens": [51496, 682, 512, 5245, 11, 4098, 512, 544, 4363, 5245, 11, 309, 4523, 484, 264, 9235, 1036, 307, 51712], "temperature": 0.0, "avg_logprob": -0.3034518368272896, "compression_ratio": 1.772972972972973, "no_speech_prob": 0.00030534749384969473}, {"id": 500, "seek": 234206, "start": 2369.02, "end": 2371.1, "text": " faster.", "tokens": [51712, 4663, 13, 51816], "temperature": 0.0, "avg_logprob": -0.3034518368272896, "compression_ratio": 1.772972972972973, "no_speech_prob": 0.00030534749384969473}, {"id": 501, "seek": 237110, "start": 2371.14, "end": 2377.02, "text": " So PyTorch has recently added support for channels last, and so you'll see that being", "tokens": [50366, 407, 9953, 51, 284, 339, 575, 3938, 3869, 1406, 337, 9235, 1036, 11, 293, 370, 291, 603, 536, 300, 885, 50660], "temperature": 0.0, "avg_logprob": -0.23716504838731553, "compression_ratio": 1.4934497816593886, "no_speech_prob": 3.1201827368931845e-05}, {"id": 502, "seek": 237110, "start": 2377.02, "end": 2380.1, "text": " used more and more as well.", "tokens": [50660, 1143, 544, 293, 544, 382, 731, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23716504838731553, "compression_ratio": 1.4934497816593886, "no_speech_prob": 3.1201827368931845e-05}, {"id": 503, "seek": 237110, "start": 2380.1, "end": 2388.02, "text": " All right, so a couple of comments and questions from our chat.", "tokens": [50814, 1057, 558, 11, 370, 257, 1916, 295, 3053, 293, 1651, 490, 527, 5081, 13, 51210], "temperature": 0.0, "avg_logprob": -0.23716504838731553, "compression_ratio": 1.4934497816593886, "no_speech_prob": 3.1201827368931845e-05}, {"id": 504, "seek": 237110, "start": 2388.02, "end": 2394.14, "text": " The first is, Sam Watkins pointing out that we've actually had a bit of a win here, which", "tokens": [51210, 440, 700, 307, 11, 4832, 12593, 10277, 12166, 484, 300, 321, 600, 767, 632, 257, 857, 295, 257, 1942, 510, 11, 597, 51516], "temperature": 0.0, "avg_logprob": -0.23716504838731553, "compression_ratio": 1.4934497816593886, "no_speech_prob": 3.1201827368931845e-05}, {"id": 505, "seek": 237110, "start": 2394.14, "end": 2400.54, "text": " is that the number of parameters in our CNN is pretty small by comparison.", "tokens": [51516, 307, 300, 264, 1230, 295, 9834, 294, 527, 24859, 307, 1238, 1359, 538, 9660, 13, 51836], "temperature": 0.0, "avg_logprob": -0.23716504838731553, "compression_ratio": 1.4934497816593886, "no_speech_prob": 3.1201827368931845e-05}, {"id": 506, "seek": 240054, "start": 2400.98, "end": 2410.74, "text": " So in the MLP version, the number of parameters is equal to basically the size of this matrix.", "tokens": [50386, 407, 294, 264, 21601, 47, 3037, 11, 264, 1230, 295, 9834, 307, 2681, 281, 1936, 264, 2744, 295, 341, 8141, 13, 50874], "temperature": 0.0, "avg_logprob": -0.49699175357818604, "compression_ratio": 1.1595744680851063, "no_speech_prob": 0.00019716896349564195}, {"id": 507, "seek": 240054, "start": 2410.74, "end": 2414.14, "text": " So m times nh.", "tokens": [50874, 407, 275, 1413, 6245, 13, 51044], "temperature": 0.0, "avg_logprob": -0.49699175357818604, "compression_ratio": 1.1595744680851063, "no_speech_prob": 0.00019716896349564195}, {"id": 508, "seek": 241414, "start": 2415.14, "end": 2417.14, "text": " Right.", "tokens": [50414, 1779, 13, 50514], "temperature": 0.0, "avg_logprob": -0.5351694197881789, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.004133822862058878}, {"id": 509, "seek": 241414, "start": 2420.7799999999997, "end": 2425.8199999999997, "text": " Oh, plus the number in this, which will be nh times 10.", "tokens": [50696, 876, 11, 1804, 264, 1230, 294, 341, 11, 597, 486, 312, 6245, 1413, 1266, 13, 50948], "temperature": 0.0, "avg_logprob": -0.5351694197881789, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.004133822862058878}, {"id": 510, "seek": 241414, "start": 2430.3799999999997, "end": 2434.18, "text": " And you know, something that at some point we probably should do is actually create something", "tokens": [51176, 400, 291, 458, 11, 746, 300, 412, 512, 935, 321, 1391, 820, 360, 307, 767, 1884, 746, 51366], "temperature": 0.0, "avg_logprob": -0.5351694197881789, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.004133822862058878}, {"id": 511, "seek": 243418, "start": 2434.22, "end": 2445.58, "text": " that allows us to automatically calculate the number of parameters.", "tokens": [50366, 300, 4045, 505, 281, 6772, 8873, 264, 1230, 295, 9834, 13, 50934], "temperature": 0.0, "avg_logprob": -0.40224162015047943, "compression_ratio": 1.2519685039370079, "no_speech_prob": 0.0010649305768311024}, {"id": 512, "seek": 243418, "start": 2445.58, "end": 2449.22, "text": " And I'm ignoring the bias there, of course.", "tokens": [50934, 400, 286, 478, 26258, 264, 12577, 456, 11, 295, 1164, 13, 51116], "temperature": 0.0, "avg_logprob": -0.40224162015047943, "compression_ratio": 1.2519685039370079, "no_speech_prob": 0.0010649305768311024}, {"id": 513, "seek": 243418, "start": 2449.22, "end": 2462.94, "text": " Let's see, what would be a good way to do that?", "tokens": [51116, 961, 311, 536, 11, 437, 576, 312, 257, 665, 636, 281, 360, 300, 30, 51802], "temperature": 0.0, "avg_logprob": -0.40224162015047943, "compression_ratio": 1.2519685039370079, "no_speech_prob": 0.0010649305768311024}, {"id": 514, "seek": 246294, "start": 2462.98, "end": 2464.98, "text": " Maybe np.product?", "tokens": [50366, 2704, 33808, 13, 33244, 30, 50466], "temperature": 0.0, "avg_logprob": -0.35700522150312153, "compression_ratio": 1.396103896103896, "no_speech_prob": 0.0002653014089446515}, {"id": 515, "seek": 246294, "start": 2464.98, "end": 2468.66, "text": " There we go.", "tokens": [50466, 821, 321, 352, 13, 50650], "temperature": 0.0, "avg_logprob": -0.35700522150312153, "compression_ratio": 1.396103896103896, "no_speech_prob": 0.0002653014089446515}, {"id": 516, "seek": 246294, "start": 2468.66, "end": 2483.58, "text": " So what we could do is just calculate this automatically by doing a little list comprehension", "tokens": [50650, 407, 437, 321, 727, 360, 307, 445, 8873, 341, 6772, 538, 884, 257, 707, 1329, 44991, 51396], "temperature": 0.0, "avg_logprob": -0.35700522150312153, "compression_ratio": 1.396103896103896, "no_speech_prob": 0.0002653014089446515}, {"id": 517, "seek": 246294, "start": 2483.58, "end": 2484.58, "text": " here.", "tokens": [51396, 510, 13, 51446], "temperature": 0.0, "avg_logprob": -0.35700522150312153, "compression_ratio": 1.396103896103896, "no_speech_prob": 0.0002653014089446515}, {"id": 518, "seek": 246294, "start": 2484.58, "end": 2489.2200000000003, "text": " So there's the number of parameters across all of the different layers, so both bias", "tokens": [51446, 407, 456, 311, 264, 1230, 295, 9834, 2108, 439, 295, 264, 819, 7914, 11, 370, 1293, 12577, 51678], "temperature": 0.0, "avg_logprob": -0.35700522150312153, "compression_ratio": 1.396103896103896, "no_speech_prob": 0.0002653014089446515}, {"id": 519, "seek": 248922, "start": 2489.2599999999998, "end": 2491.14, "text": " and weights.", "tokens": [50366, 293, 17443, 13, 50460], "temperature": 0.0, "avg_logprob": -0.2775231516638467, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.013427967205643654}, {"id": 520, "seek": 248922, "start": 2491.14, "end": 2498.3799999999997, "text": " And then we could, I guess, just, well, we could just use, well, let's use PyTorch.", "tokens": [50460, 400, 550, 321, 727, 11, 286, 2041, 11, 445, 11, 731, 11, 321, 727, 445, 764, 11, 731, 11, 718, 311, 764, 9953, 51, 284, 339, 13, 50822], "temperature": 0.0, "avg_logprob": -0.2775231516638467, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.013427967205643654}, {"id": 521, "seek": 248922, "start": 2498.3799999999997, "end": 2505.14, "text": " So we could turn that into a tensor and sum it up.", "tokens": [50822, 407, 321, 727, 1261, 300, 666, 257, 40863, 293, 2408, 309, 493, 13, 51160], "temperature": 0.0, "avg_logprob": -0.2775231516638467, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.013427967205643654}, {"id": 522, "seek": 248922, "start": 2505.14, "end": 2506.8999999999996, "text": " Oops.", "tokens": [51160, 21726, 13, 51248], "temperature": 0.0, "avg_logprob": -0.2775231516638467, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.013427967205643654}, {"id": 523, "seek": 248922, "start": 2506.8999999999996, "end": 2509.9399999999996, "text": " So that's the number in our MLP.", "tokens": [51248, 407, 300, 311, 264, 1230, 294, 527, 21601, 47, 13, 51400], "temperature": 0.0, "avg_logprob": -0.2775231516638467, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.013427967205643654}, {"id": 524, "seek": 248922, "start": 2509.9399999999996, "end": 2517.7799999999997, "text": " And then the number in our simple CNN.", "tokens": [51400, 400, 550, 264, 1230, 294, 527, 2199, 24859, 13, 51792], "temperature": 0.0, "avg_logprob": -0.2775231516638467, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.013427967205643654}, {"id": 525, "seek": 248922, "start": 2517.7799999999997, "end": 2518.7799999999997, "text": " So that's pretty cool.", "tokens": [51792, 407, 300, 311, 1238, 1627, 13, 51842], "temperature": 0.0, "avg_logprob": -0.2775231516638467, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.013427967205643654}, {"id": 526, "seek": 251878, "start": 2519.34, "end": 2524.5400000000004, "text": " So we've gone down from 40,000 to 5,000 and got about the same number there.", "tokens": [50392, 407, 321, 600, 2780, 760, 490, 3356, 11, 1360, 281, 1025, 11, 1360, 293, 658, 466, 264, 912, 1230, 456, 13, 50652], "temperature": 0.0, "avg_logprob": -0.3163868233009621, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0021156901493668556}, {"id": 527, "seek": 251878, "start": 2524.5400000000004, "end": 2526.86, "text": " Oh, thank you, Jonathan.", "tokens": [50652, 876, 11, 1309, 291, 11, 15471, 13, 50768], "temperature": 0.0, "avg_logprob": -0.3163868233009621, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0021156901493668556}, {"id": 528, "seek": 251878, "start": 2526.86, "end": 2532.98, "text": " Jonathan's reminding me that there's a better way than np.product o.shape, which is just", "tokens": [50768, 15471, 311, 27639, 385, 300, 456, 311, 257, 1101, 636, 813, 33808, 13, 33244, 277, 13, 82, 42406, 11, 597, 307, 445, 51074], "temperature": 0.0, "avg_logprob": -0.3163868233009621, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0021156901493668556}, {"id": 529, "seek": 251878, "start": 2532.98, "end": 2542.6600000000003, "text": " to say o.numberofelements numel.", "tokens": [51074, 281, 584, 277, 13, 41261, 2670, 16884, 1117, 1031, 338, 13, 51558], "temperature": 0.0, "avg_logprob": -0.3163868233009621, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0021156901493668556}, {"id": 530, "seek": 251878, "start": 2542.6600000000003, "end": 2544.7000000000003, "text": " Same thing.", "tokens": [51558, 10635, 551, 13, 51660], "temperature": 0.0, "avg_logprob": -0.3163868233009621, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0021156901493668556}, {"id": 531, "seek": 251878, "start": 2544.7000000000003, "end": 2547.38, "text": " Very nice.", "tokens": [51660, 4372, 1481, 13, 51794], "temperature": 0.0, "avg_logprob": -0.3163868233009621, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0021156901493668556}, {"id": 532, "seek": 254738, "start": 2547.38, "end": 2557.62, "text": " Now one person asked a very good question, which is, I thought convolutional neural networks", "tokens": [50364, 823, 472, 954, 2351, 257, 588, 665, 1168, 11, 597, 307, 11, 286, 1194, 45216, 304, 18161, 9590, 50876], "temperature": 0.0, "avg_logprob": -0.3129976987838745, "compression_ratio": 1.6861702127659575, "no_speech_prob": 0.0006361870327964425}, {"id": 533, "seek": 254738, "start": 2557.62, "end": 2560.1800000000003, "text": " can handle any sized image.", "tokens": [50876, 393, 4813, 604, 20004, 3256, 13, 51004], "temperature": 0.0, "avg_logprob": -0.3129976987838745, "compression_ratio": 1.6861702127659575, "no_speech_prob": 0.0006361870327964425}, {"id": 534, "seek": 254738, "start": 2560.1800000000003, "end": 2563.42, "text": " And actually, no, this convolutional network cannot handle any sized image.", "tokens": [51004, 400, 767, 11, 572, 11, 341, 45216, 304, 3209, 2644, 4813, 604, 20004, 3256, 13, 51166], "temperature": 0.0, "avg_logprob": -0.3129976987838745, "compression_ratio": 1.6861702127659575, "no_speech_prob": 0.0006361870327964425}, {"id": 535, "seek": 254738, "start": 2563.42, "end": 2568.86, "text": " This convolutional neural network only handles images that, once they go through these stride2", "tokens": [51166, 639, 45216, 304, 18161, 3209, 787, 18722, 5267, 300, 11, 1564, 436, 352, 807, 613, 1056, 482, 17, 51438], "temperature": 0.0, "avg_logprob": -0.3129976987838745, "compression_ratio": 1.6861702127659575, "no_speech_prob": 0.0006361870327964425}, {"id": 536, "seek": 254738, "start": 2568.86, "end": 2571.7000000000003, "text": " comms, end up with a 1x1.", "tokens": [51438, 800, 82, 11, 917, 493, 365, 257, 502, 87, 16, 13, 51580], "temperature": 0.0, "avg_logprob": -0.3129976987838745, "compression_ratio": 1.6861702127659575, "no_speech_prob": 0.0006361870327964425}, {"id": 537, "seek": 257170, "start": 2571.7, "end": 2579.06, "text": " Because otherwise you can't dot flatten it and end up with 16x10.", "tokens": [50364, 1436, 5911, 291, 393, 380, 5893, 24183, 309, 293, 917, 493, 365, 3165, 87, 3279, 13, 50732], "temperature": 0.0, "avg_logprob": -0.28077147988712087, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.027584712952375412}, {"id": 538, "seek": 257170, "start": 2579.06, "end": 2586.7, "text": " So we will learn how to create convnets that can handle any sized input, but there's nothing", "tokens": [50732, 407, 321, 486, 1466, 577, 281, 1884, 3754, 77, 1385, 300, 393, 4813, 604, 20004, 4846, 11, 457, 456, 311, 1825, 51114], "temperature": 0.0, "avg_logprob": -0.28077147988712087, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.027584712952375412}, {"id": 539, "seek": 257170, "start": 2586.7, "end": 2590.74, "text": " particularly about a convnet that necessitates that it has to be any sized input that it", "tokens": [51114, 4098, 466, 257, 3754, 7129, 300, 2688, 30035, 300, 309, 575, 281, 312, 604, 20004, 4846, 300, 309, 51316], "temperature": 0.0, "avg_logprob": -0.28077147988712087, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.027584712952375412}, {"id": 540, "seek": 257170, "start": 2590.74, "end": 2592.46, "text": " can handle.", "tokens": [51316, 393, 4813, 13, 51402], "temperature": 0.0, "avg_logprob": -0.28077147988712087, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.027584712952375412}, {"id": 541, "seek": 259246, "start": 2592.46, "end": 2604.1, "text": " OK, so just let's briefly finish this section off by talking about this, particularly I", "tokens": [50364, 2264, 11, 370, 445, 718, 311, 10515, 2413, 341, 3541, 766, 538, 1417, 466, 341, 11, 4098, 286, 50946], "temperature": 0.0, "avg_logprob": -0.43121878306070965, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.07368825376033783}, {"id": 542, "seek": 259246, "start": 2604.1, "end": 2608.34, "text": " want to talk about the idea of receptive field.", "tokens": [50946, 528, 281, 751, 466, 264, 1558, 295, 45838, 2519, 13, 51158], "temperature": 0.0, "avg_logprob": -0.43121878306070965, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.07368825376033783}, {"id": 543, "seek": 259246, "start": 2608.34, "end": 2615.46, "text": " Consider this one input channel, four output channel, three by three kernel.", "tokens": [51158, 17416, 341, 472, 4846, 2269, 11, 1451, 5598, 2269, 11, 1045, 538, 1045, 28256, 13, 51514], "temperature": 0.0, "avg_logprob": -0.43121878306070965, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.07368825376033783}, {"id": 544, "seek": 259246, "start": 2615.46, "end": 2618.86, "text": " So that's just to show you what we're doing here.", "tokens": [51514, 407, 300, 311, 445, 281, 855, 291, 437, 321, 434, 884, 510, 13, 51684], "temperature": 0.0, "avg_logprob": -0.43121878306070965, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.07368825376033783}, {"id": 545, "seek": 261886, "start": 2618.86, "end": 2626.6200000000003, "text": " So simpleCNN, this is the model we created.", "tokens": [50364, 407, 2199, 34, 45, 45, 11, 341, 307, 264, 2316, 321, 2942, 13, 50752], "temperature": 0.0, "avg_logprob": -0.3187108781602648, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.26890310645103455}, {"id": 546, "seek": 261886, "start": 2626.6200000000003, "end": 2629.7000000000003, "text": " Remember it was like a sequential model containing sequential models, because that's how our", "tokens": [50752, 5459, 309, 390, 411, 257, 42881, 2316, 19273, 42881, 5245, 11, 570, 300, 311, 577, 527, 50906], "temperature": 0.0, "avg_logprob": -0.3187108781602648, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.26890310645103455}, {"id": 547, "seek": 261886, "start": 2629.7000000000003, "end": 2631.1600000000003, "text": " conv function works.", "tokens": [50906, 3754, 2445, 1985, 13, 50979], "temperature": 0.0, "avg_logprob": -0.3187108781602648, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.26890310645103455}, {"id": 548, "seek": 261886, "start": 2631.1600000000003, "end": 2637.1600000000003, "text": " So simpleCNN0 is our first layer, it contains both a conv and a relu.", "tokens": [50979, 407, 2199, 34, 45, 45, 15, 307, 527, 700, 4583, 11, 309, 8306, 1293, 257, 3754, 293, 257, 1039, 84, 13, 51279], "temperature": 0.0, "avg_logprob": -0.3187108781602648, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.26890310645103455}, {"id": 549, "seek": 261886, "start": 2637.1600000000003, "end": 2641.42, "text": " So simpleCNN00 is the actual conv.", "tokens": [51279, 407, 2199, 34, 45, 45, 628, 307, 264, 3539, 3754, 13, 51492], "temperature": 0.0, "avg_logprob": -0.3187108781602648, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.26890310645103455}, {"id": 550, "seek": 261886, "start": 2641.42, "end": 2644.2200000000003, "text": " So if we grab that, call it conv1.", "tokens": [51492, 407, 498, 321, 4444, 300, 11, 818, 309, 3754, 16, 13, 51632], "temperature": 0.0, "avg_logprob": -0.3187108781602648, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.26890310645103455}, {"id": 551, "seek": 264422, "start": 2644.22, "end": 2654.18, "text": " It's a 4x1x3x3, so number of outputs, number of input channels, and height by width of", "tokens": [50364, 467, 311, 257, 1017, 87, 16, 87, 18, 87, 18, 11, 370, 1230, 295, 23930, 11, 1230, 295, 4846, 9235, 11, 293, 6681, 538, 11402, 295, 50862], "temperature": 0.0, "avg_logprob": -0.2792622357949443, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.15609067678451538}, {"id": 552, "seek": 264422, "start": 2654.18, "end": 2658.74, "text": " the kernel, and then it's got its bias as well.", "tokens": [50862, 264, 28256, 11, 293, 550, 309, 311, 658, 1080, 12577, 382, 731, 13, 51090], "temperature": 0.0, "avg_logprob": -0.2792622357949443, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.15609067678451538}, {"id": 553, "seek": 264422, "start": 2658.74, "end": 2665.7799999999997, "text": " So that's how we could kind of deconstruct what's going on with our weight matrices,", "tokens": [51090, 407, 300, 311, 577, 321, 727, 733, 295, 49473, 1757, 437, 311, 516, 322, 365, 527, 3364, 32284, 11, 51442], "temperature": 0.0, "avg_logprob": -0.2792622357949443, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.15609067678451538}, {"id": 554, "seek": 264422, "start": 2665.7799999999997, "end": 2668.54, "text": " or our parameters inside a convolution.", "tokens": [51442, 420, 527, 9834, 1854, 257, 45216, 13, 51580], "temperature": 0.0, "avg_logprob": -0.2792622357949443, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.15609067678451538}, {"id": 555, "seek": 264422, "start": 2668.54, "end": 2673.66, "text": " Now I'm going to switch over to Excel.", "tokens": [51580, 823, 286, 478, 516, 281, 3679, 670, 281, 19060, 13, 51836], "temperature": 0.0, "avg_logprob": -0.2792622357949443, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.15609067678451538}, {"id": 556, "seek": 267366, "start": 2674.1, "end": 2680.8599999999997, "text": " So in the lesson notes on the course website or on the forum, you'll find we've got an", "tokens": [50386, 407, 294, 264, 6898, 5570, 322, 264, 1164, 3144, 420, 322, 264, 17542, 11, 291, 603, 915, 321, 600, 658, 364, 50724], "temperature": 0.0, "avg_logprob": -0.35846612305767767, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.00019716807582881302}, {"id": 557, "seek": 267366, "start": 2680.8599999999997, "end": 2682.5, "text": " Excel workbook.", "tokens": [50724, 19060, 589, 2939, 13, 50806], "temperature": 0.0, "avg_logprob": -0.35846612305767767, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.00019716807582881302}, {"id": 558, "seek": 267366, "start": 2682.5, "end": 2689.14, "text": " Oh, Wassim reminded me that there is a nice trick we can do.", "tokens": [50806, 876, 11, 42998, 332, 15920, 385, 300, 456, 307, 257, 1481, 4282, 321, 393, 360, 13, 51138], "temperature": 0.0, "avg_logprob": -0.35846612305767767, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.00019716807582881302}, {"id": 559, "seek": 267366, "start": 2689.14, "end": 2692.06, "text": " I do want to do that actually, because I love this trick.", "tokens": [51138, 286, 360, 528, 281, 360, 300, 767, 11, 570, 286, 959, 341, 4282, 13, 51284], "temperature": 0.0, "avg_logprob": -0.35846612305767767, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.00019716807582881302}, {"id": 560, "seek": 267366, "start": 2692.06, "end": 2695.2599999999998, "text": " Oh, I just deleted everything though.", "tokens": [51284, 876, 11, 286, 445, 22981, 1203, 1673, 13, 51444], "temperature": 0.0, "avg_logprob": -0.35846612305767767, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.00019716807582881302}, {"id": 561, "seek": 267366, "start": 2695.2599999999998, "end": 2697.1, "text": " Let's put them all back.", "tokens": [51444, 961, 311, 829, 552, 439, 646, 13, 51536], "temperature": 0.0, "avg_logprob": -0.35846612305767767, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.00019716807582881302}, {"id": 562, "seek": 267366, "start": 2697.1, "end": 2698.1, "text": " Here we go.", "tokens": [51536, 1692, 321, 352, 13, 51586], "temperature": 0.0, "avg_logprob": -0.35846612305767767, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.00019716807582881302}, {"id": 563, "seek": 267366, "start": 2698.1, "end": 2700.22, "text": " Which is, you actually don't need square brackets.", "tokens": [51586, 3013, 307, 11, 291, 767, 500, 380, 643, 3732, 26179, 13, 51692], "temperature": 0.0, "avg_logprob": -0.35846612305767767, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.00019716807582881302}, {"id": 564, "seek": 267366, "start": 2700.22, "end": 2702.3399999999997, "text": " The square brackets is a list comprehension.", "tokens": [51692, 440, 3732, 26179, 307, 257, 1329, 44991, 13, 51798], "temperature": 0.0, "avg_logprob": -0.35846612305767767, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.00019716807582881302}, {"id": 565, "seek": 270234, "start": 2702.34, "end": 2705.7000000000003, "text": " Without the square brackets, it's called a generator.", "tokens": [50364, 9129, 264, 3732, 26179, 11, 309, 311, 1219, 257, 19265, 13, 50532], "temperature": 0.0, "avg_logprob": -0.4887198886355838, "compression_ratio": 1.547945205479452, "no_speech_prob": 0.09268439561128616}, {"id": 566, "seek": 270234, "start": 2705.7000000000003, "end": 2708.78, "text": " And it, oh no, you can't use it there.", "tokens": [50532, 400, 309, 11, 1954, 572, 11, 291, 393, 380, 764, 309, 456, 13, 50686], "temperature": 0.0, "avg_logprob": -0.4887198886355838, "compression_ratio": 1.547945205479452, "no_speech_prob": 0.09268439561128616}, {"id": 567, "seek": 270234, "start": 2708.78, "end": 2712.6600000000003, "text": " Maybe that only works with numpy.", "tokens": [50686, 2704, 300, 787, 1985, 365, 1031, 8200, 13, 50880], "temperature": 0.0, "avg_logprob": -0.4887198886355838, "compression_ratio": 1.547945205479452, "no_speech_prob": 0.09268439561128616}, {"id": 568, "seek": 270234, "start": 2712.6600000000003, "end": 2715.46, "text": " Maybe that only works with numpy.", "tokens": [50880, 2704, 300, 787, 1985, 365, 1031, 8200, 13, 51020], "temperature": 0.0, "avg_logprob": -0.4887198886355838, "compression_ratio": 1.547945205479452, "no_speech_prob": 0.09268439561128616}, {"id": 569, "seek": 270234, "start": 2715.46, "end": 2727.46, "text": " Ah, okay, so, wait, that's the list.", "tokens": [51020, 2438, 11, 1392, 11, 370, 11, 1699, 11, 300, 311, 264, 1329, 13, 51620], "temperature": 0.0, "avg_logprob": -0.4887198886355838, "compression_ratio": 1.547945205479452, "no_speech_prob": 0.09268439561128616}, {"id": 570, "seek": 270234, "start": 2727.46, "end": 2728.46, "text": " No that doesn't work either.", "tokens": [51620, 883, 300, 1177, 380, 589, 2139, 13, 51670], "temperature": 0.0, "avg_logprob": -0.4887198886355838, "compression_ratio": 1.547945205479452, "no_speech_prob": 0.09268439561128616}, {"id": 571, "seek": 272846, "start": 2728.46, "end": 2735.5, "text": " So much for that.", "tokens": [50364, 407, 709, 337, 300, 13, 50716], "temperature": 0.0, "avg_logprob": -0.6205899773574457, "compression_ratio": 0.9883720930232558, "no_speech_prob": 0.24214154481887817}, {"id": 572, "seek": 272846, "start": 2735.5, "end": 2740.82, "text": " I'm kind of curious now.", "tokens": [50716, 286, 478, 733, 295, 6369, 586, 13, 50982], "temperature": 0.0, "avg_logprob": -0.6205899773574457, "compression_ratio": 0.9883720930232558, "no_speech_prob": 0.24214154481887817}, {"id": 573, "seek": 272846, "start": 2740.82, "end": 2744.82, "text": " Maybe torch.sum?", "tokens": [50982, 2704, 27822, 13, 82, 449, 30, 51182], "temperature": 0.0, "avg_logprob": -0.6205899773574457, "compression_ratio": 0.9883720930232558, "no_speech_prob": 0.24214154481887817}, {"id": 574, "seek": 272846, "start": 2744.82, "end": 2747.82, "text": " Nope.", "tokens": [51182, 12172, 13, 51332], "temperature": 0.0, "avg_logprob": -0.6205899773574457, "compression_ratio": 0.9883720930232558, "no_speech_prob": 0.24214154481887817}, {"id": 575, "seek": 272846, "start": 2747.82, "end": 2752.82, "text": " Just sum?", "tokens": [51332, 1449, 2408, 30, 51582], "temperature": 0.0, "avg_logprob": -0.6205899773574457, "compression_ratio": 0.9883720930232558, "no_speech_prob": 0.24214154481887817}, {"id": 576, "seek": 272846, "start": 2752.82, "end": 2755.82, "text": " Oh, okay.", "tokens": [51582, 876, 11, 1392, 13, 51732], "temperature": 0.0, "avg_logprob": -0.6205899773574457, "compression_ratio": 0.9883720930232558, "no_speech_prob": 0.24214154481887817}, {"id": 577, "seek": 275582, "start": 2756.1800000000003, "end": 2759.1800000000003, "text": " I don't want to use Python's sum.", "tokens": [50382, 286, 500, 380, 528, 281, 764, 15329, 311, 2408, 13, 50532], "temperature": 0.0, "avg_logprob": -0.27415303548177083, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0037653069011867046}, {"id": 578, "seek": 275582, "start": 2759.1800000000003, "end": 2760.1800000000003, "text": " That's interesting.", "tokens": [50532, 663, 311, 1880, 13, 50582], "temperature": 0.0, "avg_logprob": -0.27415303548177083, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0037653069011867046}, {"id": 579, "seek": 275582, "start": 2760.1800000000003, "end": 2767.1800000000003, "text": " I feel like all of them should handle generators, but there you go.", "tokens": [50582, 286, 841, 411, 439, 295, 552, 820, 4813, 38662, 11, 457, 456, 291, 352, 13, 50932], "temperature": 0.0, "avg_logprob": -0.27415303548177083, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0037653069011867046}, {"id": 580, "seek": 275582, "start": 2767.1800000000003, "end": 2775.9, "text": " Okay, so open up the conv example spreadsheet.", "tokens": [50932, 1033, 11, 370, 1269, 493, 264, 3754, 1365, 27733, 13, 51368], "temperature": 0.0, "avg_logprob": -0.27415303548177083, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0037653069011867046}, {"id": 581, "seek": 275582, "start": 2775.9, "end": 2783.98, "text": " And what you'll see on the conv example worksheet page is something that looks a lot like the", "tokens": [51368, 400, 437, 291, 603, 536, 322, 264, 3754, 1365, 49890, 3028, 307, 746, 300, 1542, 257, 688, 411, 264, 51772], "temperature": 0.0, "avg_logprob": -0.27415303548177083, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0037653069011867046}, {"id": 582, "seek": 275582, "start": 2783.98, "end": 2784.98, "text": " number seven.", "tokens": [51772, 1230, 3407, 13, 51822], "temperature": 0.0, "avg_logprob": -0.27415303548177083, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0037653069011867046}, {"id": 583, "seek": 278498, "start": 2785.14, "end": 2788.66, "text": " So this is a number seven that I got straight from MNIST.", "tokens": [50372, 407, 341, 307, 257, 1230, 3407, 300, 286, 658, 2997, 490, 376, 45, 19756, 13, 50548], "temperature": 0.0, "avg_logprob": -0.3289403413471423, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.005384920630604029}, {"id": 584, "seek": 278498, "start": 2788.66, "end": 2791.34, "text": " Let's see.", "tokens": [50548, 961, 311, 536, 13, 50682], "temperature": 0.0, "avg_logprob": -0.3289403413471423, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.005384920630604029}, {"id": 585, "seek": 278498, "start": 2791.34, "end": 2795.9, "text": " Okay, so you can see over here we have a number seven.", "tokens": [50682, 1033, 11, 370, 291, 393, 536, 670, 510, 321, 362, 257, 1230, 3407, 13, 50910], "temperature": 0.0, "avg_logprob": -0.3289403413471423, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.005384920630604029}, {"id": 586, "seek": 278498, "start": 2795.9, "end": 2801.46, "text": " This is a number seven from MNIST that I have copied into Excel.", "tokens": [50910, 639, 307, 257, 1230, 3407, 490, 376, 45, 19756, 300, 286, 362, 25365, 666, 19060, 13, 51188], "temperature": 0.0, "avg_logprob": -0.3289403413471423, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.005384920630604029}, {"id": 587, "seek": 278498, "start": 2801.46, "end": 2806.18, "text": " And then you can see over here we've got like a top edge kernel being applied, and over", "tokens": [51188, 400, 550, 291, 393, 536, 670, 510, 321, 600, 658, 411, 257, 1192, 4691, 28256, 885, 6456, 11, 293, 670, 51424], "temperature": 0.0, "avg_logprob": -0.3289403413471423, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.005384920630604029}, {"id": 588, "seek": 278498, "start": 2806.18, "end": 2809.66, "text": " here we've got a right edge kernel being applied.", "tokens": [51424, 510, 321, 600, 658, 257, 558, 4691, 28256, 885, 6456, 13, 51598], "temperature": 0.0, "avg_logprob": -0.3289403413471423, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.005384920630604029}, {"id": 589, "seek": 278498, "start": 2809.66, "end": 2813.98, "text": " This might be surprising you, because you might be thinking, wait a tick, Jeremy.", "tokens": [51598, 639, 1062, 312, 8830, 291, 11, 570, 291, 1062, 312, 1953, 11, 1699, 257, 5204, 11, 17809, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3289403413471423, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.005384920630604029}, {"id": 590, "seek": 281398, "start": 2813.98, "end": 2824.66, "text": " If Excel doesn't do convolutional neural networks, well, actually it does.", "tokens": [50364, 759, 19060, 1177, 380, 360, 45216, 304, 18161, 9590, 11, 731, 11, 767, 309, 775, 13, 50898], "temperature": 0.0, "avg_logprob": -0.2447863565364354, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.0018102095928043127}, {"id": 591, "seek": 281398, "start": 2824.66, "end": 2831.44, "text": " So if I zoom in in Excel, you'll see actually these numbers are in fact conditional formatting", "tokens": [50898, 407, 498, 286, 8863, 294, 294, 19060, 11, 291, 603, 536, 767, 613, 3547, 366, 294, 1186, 27708, 39366, 51237], "temperature": 0.0, "avg_logprob": -0.2447863565364354, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.0018102095928043127}, {"id": 592, "seek": 281398, "start": 2831.44, "end": 2835.04, "text": " applied to a bunch of spreadsheet cells.", "tokens": [51237, 6456, 281, 257, 3840, 295, 27733, 5438, 13, 51417], "temperature": 0.0, "avg_logprob": -0.2447863565364354, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.0018102095928043127}, {"id": 593, "seek": 281398, "start": 2835.04, "end": 2840.5, "text": " And so what I did was I copied the actual pixel values into Excel, and then applied", "tokens": [51417, 400, 370, 437, 286, 630, 390, 286, 25365, 264, 3539, 19261, 4190, 666, 19060, 11, 293, 550, 6456, 51690], "temperature": 0.0, "avg_logprob": -0.2447863565364354, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.0018102095928043127}, {"id": 594, "seek": 284050, "start": 2840.5, "end": 2841.72, "text": " conditional formatting.", "tokens": [50364, 27708, 39366, 13, 50425], "temperature": 0.0, "avg_logprob": -0.2560213851928711, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.012241199612617493}, {"id": 595, "seek": 284050, "start": 2841.72, "end": 2845.82, "text": " And so now you can see what the digit is actually made of.", "tokens": [50425, 400, 370, 586, 291, 393, 536, 437, 264, 14293, 307, 767, 1027, 295, 13, 50630], "temperature": 0.0, "avg_logprob": -0.2560213851928711, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.012241199612617493}, {"id": 596, "seek": 284050, "start": 2845.82, "end": 2854.86, "text": " So you can see here I've created our top edge filter.", "tokens": [50630, 407, 291, 393, 536, 510, 286, 600, 2942, 527, 1192, 4691, 6608, 13, 51082], "temperature": 0.0, "avg_logprob": -0.2560213851928711, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.012241199612617493}, {"id": 597, "seek": 284050, "start": 2854.86, "end": 2859.22, "text": " And here I've created our left edge filter.", "tokens": [51082, 400, 510, 286, 600, 2942, 527, 1411, 4691, 6608, 13, 51300], "temperature": 0.0, "avg_logprob": -0.2560213851928711, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.012241199612617493}, {"id": 598, "seek": 285922, "start": 2859.22, "end": 2872.0, "text": " And so here I am applying that filter to that window.", "tokens": [50364, 400, 370, 510, 286, 669, 9275, 300, 6608, 281, 300, 4910, 13, 51003], "temperature": 0.0, "avg_logprob": -0.20006818771362306, "compression_ratio": 1.367741935483871, "no_speech_prob": 0.0032224119640886784}, {"id": 599, "seek": 285922, "start": 2872.0, "end": 2878.06, "text": " And so here you can see it looks a lot like NumPy, it's just a sum product.", "tokens": [51003, 400, 370, 510, 291, 393, 536, 309, 1542, 257, 688, 411, 22592, 47, 88, 11, 309, 311, 445, 257, 2408, 1674, 13, 51306], "temperature": 0.0, "avg_logprob": -0.20006818771362306, "compression_ratio": 1.367741935483871, "no_speech_prob": 0.0032224119640886784}, {"id": 600, "seek": 285922, "start": 2878.06, "end": 2886.7799999999997, "text": " And you might not be aware of this, but in Excel you can actually do broadcasting.", "tokens": [51306, 400, 291, 1062, 406, 312, 3650, 295, 341, 11, 457, 294, 19060, 291, 393, 767, 360, 30024, 13, 51742], "temperature": 0.0, "avg_logprob": -0.20006818771362306, "compression_ratio": 1.367741935483871, "no_speech_prob": 0.0032224119640886784}, {"id": 601, "seek": 288678, "start": 2886.78, "end": 2892.1400000000003, "text": " You have to hit Apple Shift Enter or Control Shift Enter, and it puts these little curly", "tokens": [50364, 509, 362, 281, 2045, 6373, 28304, 10399, 420, 12912, 28304, 10399, 11, 293, 309, 8137, 613, 707, 32066, 50632], "temperature": 0.0, "avg_logprob": -0.26012234304143095, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.001133563811890781}, {"id": 602, "seek": 288678, "start": 2892.1400000000003, "end": 2893.1400000000003, "text": " brackets around it.", "tokens": [50632, 26179, 926, 309, 13, 50682], "temperature": 0.0, "avg_logprob": -0.26012234304143095, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.001133563811890781}, {"id": 603, "seek": 288678, "start": 2893.1400000000003, "end": 2894.1400000000003, "text": " It's called an array formula.", "tokens": [50682, 467, 311, 1219, 364, 10225, 8513, 13, 50732], "temperature": 0.0, "avg_logprob": -0.26012234304143095, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.001133563811890781}, {"id": 604, "seek": 288678, "start": 2894.1400000000003, "end": 2899.28, "text": " It basically lets you do broadcasting, or simple broadcasting, in Excel.", "tokens": [50732, 467, 1936, 6653, 291, 360, 30024, 11, 420, 2199, 30024, 11, 294, 19060, 13, 50989], "temperature": 0.0, "avg_logprob": -0.26012234304143095, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.001133563811890781}, {"id": 605, "seek": 288678, "start": 2899.28, "end": 2905.0400000000004, "text": " And so this is how I created this top edge filtered version in Excel.", "tokens": [50989, 400, 370, 341, 307, 577, 286, 2942, 341, 1192, 4691, 37111, 3037, 294, 19060, 13, 51277], "temperature": 0.0, "avg_logprob": -0.26012234304143095, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.001133563811890781}, {"id": 606, "seek": 288678, "start": 2905.0400000000004, "end": 2910.7000000000003, "text": " And the left edge version is exactly the same, just a different kernel.", "tokens": [51277, 400, 264, 1411, 4691, 3037, 307, 2293, 264, 912, 11, 445, 257, 819, 28256, 13, 51560], "temperature": 0.0, "avg_logprob": -0.26012234304143095, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.001133563811890781}, {"id": 607, "seek": 291070, "start": 2910.7, "end": 2918.46, "text": " And as you can see, if I click on it, it's applying this filter to this input area, and", "tokens": [50364, 400, 382, 291, 393, 536, 11, 498, 286, 2052, 322, 309, 11, 309, 311, 9275, 341, 6608, 281, 341, 4846, 1859, 11, 293, 50752], "temperature": 0.0, "avg_logprob": -0.30969030007548715, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.056649260222911835}, {"id": 608, "seek": 291070, "start": 2918.46, "end": 2920.1, "text": " so forth.", "tokens": [50752, 370, 5220, 13, 50834], "temperature": 0.0, "avg_logprob": -0.30969030007548715, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.056649260222911835}, {"id": 609, "seek": 291070, "start": 2920.1, "end": 2929.8599999999997, "text": " Okay, so we can then, I just arbitrarily picked some different values here.", "tokens": [50834, 1033, 11, 370, 321, 393, 550, 11, 286, 445, 19071, 3289, 6183, 512, 819, 4190, 510, 13, 51322], "temperature": 0.0, "avg_logprob": -0.30969030007548715, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.056649260222911835}, {"id": 610, "seek": 291070, "start": 2929.8599999999997, "end": 2937.3799999999997, "text": " And so something to notice now in my second layer, so here's conv1, here's conv2, it's", "tokens": [51322, 400, 370, 746, 281, 3449, 586, 294, 452, 1150, 4583, 11, 370, 510, 311, 3754, 16, 11, 510, 311, 3754, 17, 11, 309, 311, 51698], "temperature": 0.0, "avg_logprob": -0.30969030007548715, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.056649260222911835}, {"id": 611, "seek": 291070, "start": 2937.3799999999997, "end": 2938.4199999999996, "text": " got a bit more work to do.", "tokens": [51698, 658, 257, 857, 544, 589, 281, 360, 13, 51750], "temperature": 0.0, "avg_logprob": -0.30969030007548715, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.056649260222911835}, {"id": 612, "seek": 293842, "start": 2938.42, "end": 2949.58, "text": " We actually need two filters, because we need to add together this bit here, applied", "tokens": [50364, 492, 767, 643, 732, 15995, 11, 570, 321, 643, 281, 909, 1214, 341, 857, 510, 11, 6456, 50922], "temperature": 0.0, "avg_logprob": -0.2960772294264573, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002323152730241418}, {"id": 613, "seek": 293842, "start": 2949.58, "end": 2956.82, "text": " to this, with this kernel applied, and this bit here, with this kernel applied.", "tokens": [50922, 281, 341, 11, 365, 341, 28256, 6456, 11, 293, 341, 857, 510, 11, 365, 341, 28256, 6456, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2960772294264573, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002323152730241418}, {"id": 614, "seek": 293842, "start": 2956.82, "end": 2966.98, "text": " So you actually need one set of 3x3 for each input, and also I want to set two separate", "tokens": [51284, 407, 291, 767, 643, 472, 992, 295, 805, 87, 18, 337, 1184, 4846, 11, 293, 611, 286, 528, 281, 992, 732, 4994, 51792], "temperature": 0.0, "avg_logprob": -0.2960772294264573, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002323152730241418}, {"id": 615, "seek": 296698, "start": 2967.06, "end": 2978.7400000000002, "text": " outputs, so I actually end up needing a 2x2x3x3 weights matrix, or weights tensor, I should", "tokens": [50368, 23930, 11, 370, 286, 767, 917, 493, 18006, 257, 568, 87, 17, 87, 18, 87, 18, 17443, 8141, 11, 420, 17443, 40863, 11, 286, 820, 50952], "temperature": 0.0, "avg_logprob": -0.2213687495181435, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002472618129104376}, {"id": 616, "seek": 296698, "start": 2978.7400000000002, "end": 2981.86, "text": " say, which you might remember is exactly what we had in PyTorch.", "tokens": [50952, 584, 11, 597, 291, 1062, 1604, 307, 2293, 437, 321, 632, 294, 9953, 51, 284, 339, 13, 51108], "temperature": 0.0, "avg_logprob": -0.2213687495181435, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002472618129104376}, {"id": 617, "seek": 296698, "start": 2981.86, "end": 2984.5, "text": " We had a rank 4 tensor.", "tokens": [51108, 492, 632, 257, 6181, 1017, 40863, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2213687495181435, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002472618129104376}, {"id": 618, "seek": 296698, "start": 2984.5, "end": 2989.26, "text": " So if I have a look at this one, you see exactly the same thing.", "tokens": [51240, 407, 498, 286, 362, 257, 574, 412, 341, 472, 11, 291, 536, 2293, 264, 912, 551, 13, 51478], "temperature": 0.0, "avg_logprob": -0.2213687495181435, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002472618129104376}, {"id": 619, "seek": 296698, "start": 2989.26, "end": 2995.98, "text": " This input is using this kernel applied to here, and this kernel applied to here.", "tokens": [51478, 639, 4846, 307, 1228, 341, 28256, 6456, 281, 510, 11, 293, 341, 28256, 6456, 281, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2213687495181435, "compression_ratio": 1.5797101449275361, "no_speech_prob": 0.002472618129104376}, {"id": 620, "seek": 299598, "start": 2995.98, "end": 3000.72, "text": " So that's important to remember, that you have these rank 4 tensors.", "tokens": [50364, 407, 300, 311, 1021, 281, 1604, 11, 300, 291, 362, 613, 6181, 1017, 10688, 830, 13, 50601], "temperature": 0.0, "avg_logprob": -0.2446284294128418, "compression_ratio": 1.5347826086956522, "no_speech_prob": 9.461255831411108e-05}, {"id": 621, "seek": 299598, "start": 3000.72, "end": 3008.42, "text": " And so then, rather than doing stride2 conv, I did something else, which is actually a", "tokens": [50601, 400, 370, 550, 11, 2831, 813, 884, 1056, 482, 17, 3754, 11, 286, 630, 746, 1646, 11, 597, 307, 767, 257, 50986], "temperature": 0.0, "avg_logprob": -0.2446284294128418, "compression_ratio": 1.5347826086956522, "no_speech_prob": 9.461255831411108e-05}, {"id": 622, "seek": 299598, "start": 3008.42, "end": 3011.94, "text": " bit out of favor nowadays, but it's another option, which is to do something called max", "tokens": [50986, 857, 484, 295, 2294, 13434, 11, 457, 309, 311, 1071, 3614, 11, 597, 307, 281, 360, 746, 1219, 11469, 51162], "temperature": 0.0, "avg_logprob": -0.2446284294128418, "compression_ratio": 1.5347826086956522, "no_speech_prob": 9.461255831411108e-05}, {"id": 623, "seek": 299598, "start": 3011.94, "end": 3014.78, "text": " pooling, to reduce my dimensionality.", "tokens": [51162, 7005, 278, 11, 281, 5407, 452, 10139, 1860, 13, 51304], "temperature": 0.0, "avg_logprob": -0.2446284294128418, "compression_ratio": 1.5347826086956522, "no_speech_prob": 9.461255831411108e-05}, {"id": 624, "seek": 299598, "start": 3014.78, "end": 3021.34, "text": " So you can see here I've got 28x28, I've reduced it down here to 14x14.", "tokens": [51304, 407, 291, 393, 536, 510, 286, 600, 658, 7562, 87, 11205, 11, 286, 600, 9212, 309, 760, 510, 281, 3499, 87, 7271, 13, 51632], "temperature": 0.0, "avg_logprob": -0.2446284294128418, "compression_ratio": 1.5347826086956522, "no_speech_prob": 9.461255831411108e-05}, {"id": 625, "seek": 302134, "start": 3021.34, "end": 3030.1800000000003, "text": " And the way I did it was simply to take the max of each little 2x2 area.", "tokens": [50364, 400, 264, 636, 286, 630, 309, 390, 2935, 281, 747, 264, 11469, 295, 1184, 707, 568, 87, 17, 1859, 13, 50806], "temperature": 0.0, "avg_logprob": -0.24478249908775412, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.0047551048919558525}, {"id": 626, "seek": 302134, "start": 3030.1800000000003, "end": 3033.96, "text": " So that's all that's been done there.", "tokens": [50806, 407, 300, 311, 439, 300, 311, 668, 1096, 456, 13, 50995], "temperature": 0.0, "avg_logprob": -0.24478249908775412, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.0047551048919558525}, {"id": 627, "seek": 302134, "start": 3033.96, "end": 3035.6200000000003, "text": " So that's called max pooling.", "tokens": [50995, 407, 300, 311, 1219, 11469, 7005, 278, 13, 51078], "temperature": 0.0, "avg_logprob": -0.24478249908775412, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.0047551048919558525}, {"id": 628, "seek": 302134, "start": 3035.6200000000003, "end": 3040.5, "text": " And so max pooling has the same effect as a stride2 conv, not mathematically identical,", "tokens": [51078, 400, 370, 11469, 7005, 278, 575, 264, 912, 1802, 382, 257, 1056, 482, 17, 3754, 11, 406, 44003, 4496, 317, 804, 11, 51322], "temperature": 0.0, "avg_logprob": -0.24478249908775412, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.0047551048919558525}, {"id": 629, "seek": 302134, "start": 3040.5, "end": 3048.7000000000003, "text": " the same effect, which is it does a convolution and reduces the grid size by 2 on each dimension.", "tokens": [51322, 264, 912, 1802, 11, 597, 307, 309, 775, 257, 45216, 293, 18081, 264, 10748, 2744, 538, 568, 322, 1184, 10139, 13, 51732], "temperature": 0.0, "avg_logprob": -0.24478249908775412, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.0047551048919558525}, {"id": 630, "seek": 304870, "start": 3048.7, "end": 3053.2999999999997, "text": " So then, how do we create a single output if we don't keep doing this until we get to", "tokens": [50364, 407, 550, 11, 577, 360, 321, 1884, 257, 2167, 5598, 498, 321, 500, 380, 1066, 884, 341, 1826, 321, 483, 281, 50594], "temperature": 0.0, "avg_logprob": -0.2298620382944743, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.00014883805124554783}, {"id": 631, "seek": 304870, "start": 3053.2999999999997, "end": 3056.5, "text": " 1x1, which I'm too lazy to do in Excel.", "tokens": [50594, 502, 87, 16, 11, 597, 286, 478, 886, 14847, 281, 360, 294, 19060, 13, 50754], "temperature": 0.0, "avg_logprob": -0.2298620382944743, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.00014883805124554783}, {"id": 632, "seek": 304870, "start": 3056.5, "end": 3060.3799999999997, "text": " Well, one approach, and again this is a little bit out of favor as well, but one approach", "tokens": [50754, 1042, 11, 472, 3109, 11, 293, 797, 341, 307, 257, 707, 857, 484, 295, 2294, 382, 731, 11, 457, 472, 3109, 50948], "temperature": 0.0, "avg_logprob": -0.2298620382944743, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.00014883805124554783}, {"id": 633, "seek": 304870, "start": 3060.3799999999997, "end": 3069.8599999999997, "text": " we can do is we can take every one of these, we've now got 14x14, and apply a dense layer", "tokens": [50948, 321, 393, 360, 307, 321, 393, 747, 633, 472, 295, 613, 11, 321, 600, 586, 658, 3499, 87, 7271, 11, 293, 3079, 257, 18011, 4583, 51422], "temperature": 0.0, "avg_logprob": -0.2298620382944743, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.00014883805124554783}, {"id": 634, "seek": 304870, "start": 3069.8599999999997, "end": 3071.8999999999996, "text": " to it.", "tokens": [51422, 281, 309, 13, 51524], "temperature": 0.0, "avg_logprob": -0.2298620382944743, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.00014883805124554783}, {"id": 635, "seek": 304870, "start": 3071.8999999999996, "end": 3077.48, "text": " And so what I've done here is I've got a big, imagine this is basically all being flattened", "tokens": [51524, 400, 370, 437, 286, 600, 1096, 510, 307, 286, 600, 658, 257, 955, 11, 3811, 341, 307, 1936, 439, 885, 24183, 292, 51803], "temperature": 0.0, "avg_logprob": -0.2298620382944743, "compression_ratio": 1.584313725490196, "no_speech_prob": 0.00014883805124554783}, {"id": 636, "seek": 307748, "start": 3077.48, "end": 3079.6, "text": " out into a vector.", "tokens": [50364, 484, 666, 257, 8062, 13, 50470], "temperature": 0.0, "avg_logprob": -0.26565822478263607, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.0032730584498494864}, {"id": 637, "seek": 307748, "start": 3079.6, "end": 3090.4, "text": " And so here we've got sum product of this by this plus the sum product of this by this,", "tokens": [50470, 400, 370, 510, 321, 600, 658, 2408, 1674, 295, 341, 538, 341, 1804, 264, 2408, 1674, 295, 341, 538, 341, 11, 51010], "temperature": 0.0, "avg_logprob": -0.26565822478263607, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.0032730584498494864}, {"id": 638, "seek": 307748, "start": 3090.4, "end": 3095.0, "text": " and that gives us a single number.", "tokens": [51010, 293, 300, 2709, 505, 257, 2167, 1230, 13, 51240], "temperature": 0.0, "avg_logprob": -0.26565822478263607, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.0032730584498494864}, {"id": 639, "seek": 307748, "start": 3095.0, "end": 3102.84, "text": " And so that is how we could then optimize that in order to optimize our weight matrices.", "tokens": [51240, 400, 370, 300, 307, 577, 321, 727, 550, 19719, 300, 294, 1668, 281, 19719, 527, 3364, 32284, 13, 51632], "temperature": 0.0, "avg_logprob": -0.26565822478263607, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.0032730584498494864}, {"id": 640, "seek": 310284, "start": 3102.84, "end": 3109.92, "text": " Now, and then, you know, the more modern approach, we don't use this kind of dense", "tokens": [50364, 823, 11, 293, 550, 11, 291, 458, 11, 264, 544, 4363, 3109, 11, 321, 500, 380, 764, 341, 733, 295, 18011, 50718], "temperature": 0.0, "avg_logprob": -0.27191023393110797, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.00793751422315836}, {"id": 641, "seek": 310284, "start": 3109.92, "end": 3112.32, "text": " layer much anymore, it still appears a bit.", "tokens": [50718, 4583, 709, 3602, 11, 309, 920, 7038, 257, 857, 13, 50838], "temperature": 0.0, "avg_logprob": -0.27191023393110797, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.00793751422315836}, {"id": 642, "seek": 310284, "start": 3112.32, "end": 3121.0, "text": " The main place that you see this used is in a network called VGG, which is very old now,", "tokens": [50838, 440, 2135, 1081, 300, 291, 536, 341, 1143, 307, 294, 257, 3209, 1219, 691, 27561, 11, 597, 307, 588, 1331, 586, 11, 51272], "temperature": 0.0, "avg_logprob": -0.27191023393110797, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.00793751422315836}, {"id": 643, "seek": 310284, "start": 3121.0, "end": 3124.76, "text": " I think it might be 2013 or something.", "tokens": [51272, 286, 519, 309, 1062, 312, 9012, 420, 746, 13, 51460], "temperature": 0.0, "avg_logprob": -0.27191023393110797, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.00793751422315836}, {"id": 644, "seek": 310284, "start": 3124.76, "end": 3126.48, "text": " But it's actually still used.", "tokens": [51460, 583, 309, 311, 767, 920, 1143, 13, 51546], "temperature": 0.0, "avg_logprob": -0.27191023393110797, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.00793751422315836}, {"id": 645, "seek": 310284, "start": 3126.48, "end": 3132.52, "text": " And that's because for certain things like something called style transfer, or in general", "tokens": [51546, 400, 300, 311, 570, 337, 1629, 721, 411, 746, 1219, 3758, 5003, 11, 420, 294, 2674, 51848], "temperature": 0.0, "avg_logprob": -0.27191023393110797, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.00793751422315836}, {"id": 646, "seek": 313252, "start": 3132.52, "end": 3139.32, "text": " perceptual losses, people still find VGG seems to work better.", "tokens": [50364, 43276, 901, 15352, 11, 561, 920, 915, 691, 27561, 2544, 281, 589, 1101, 13, 50704], "temperature": 0.0, "avg_logprob": -0.27181641073787915, "compression_ratio": 1.6308411214953271, "no_speech_prob": 7.602473488077521e-05}, {"id": 647, "seek": 313252, "start": 3139.32, "end": 3143.44, "text": " So you still actually see this approach nowadays sometimes.", "tokens": [50704, 407, 291, 920, 767, 536, 341, 3109, 13434, 2171, 13, 50910], "temperature": 0.0, "avg_logprob": -0.27181641073787915, "compression_ratio": 1.6308411214953271, "no_speech_prob": 7.602473488077521e-05}, {"id": 648, "seek": 313252, "start": 3143.44, "end": 3149.96, "text": " The more common approach, however, nowadays is we take the penultimate layer, and we just", "tokens": [50910, 440, 544, 2689, 3109, 11, 4461, 11, 13434, 307, 321, 747, 264, 3435, 723, 2905, 4583, 11, 293, 321, 445, 51236], "temperature": 0.0, "avg_logprob": -0.27181641073787915, "compression_ratio": 1.6308411214953271, "no_speech_prob": 7.602473488077521e-05}, {"id": 649, "seek": 313252, "start": 3149.96, "end": 3155.1, "text": " simply take the average of all of the activations.", "tokens": [51236, 2935, 747, 264, 4274, 295, 439, 295, 264, 2430, 763, 13, 51493], "temperature": 0.0, "avg_logprob": -0.27181641073787915, "compression_ratio": 1.6308411214953271, "no_speech_prob": 7.602473488077521e-05}, {"id": 650, "seek": 313252, "start": 3155.1, "end": 3160.52, "text": " So nowadays, we would simply, the Excel way of doing it would be literally simply say", "tokens": [51493, 407, 13434, 11, 321, 576, 2935, 11, 264, 19060, 636, 295, 884, 309, 576, 312, 3736, 2935, 584, 51764], "temperature": 0.0, "avg_logprob": -0.27181641073787915, "compression_ratio": 1.6308411214953271, "no_speech_prob": 7.602473488077521e-05}, {"id": 651, "seek": 316052, "start": 3160.52, "end": 3167.4, "text": " average of the penultimate layer.", "tokens": [50364, 4274, 295, 264, 3435, 723, 2905, 4583, 13, 50708], "temperature": 0.0, "avg_logprob": -0.25195829073588055, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.08269169926643372}, {"id": 652, "seek": 316052, "start": 3167.4, "end": 3172.44, "text": " And that is called global average pooling.", "tokens": [50708, 400, 300, 307, 1219, 4338, 4274, 7005, 278, 13, 50960], "temperature": 0.0, "avg_logprob": -0.25195829073588055, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.08269169926643372}, {"id": 653, "seek": 316052, "start": 3172.44, "end": 3176.04, "text": " Everything has a fancy word, a fancy phrase, but that's all it is.", "tokens": [50960, 5471, 575, 257, 10247, 1349, 11, 257, 10247, 9535, 11, 457, 300, 311, 439, 309, 307, 13, 51140], "temperature": 0.0, "avg_logprob": -0.25195829073588055, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.08269169926643372}, {"id": 654, "seek": 316052, "start": 3176.04, "end": 3177.84, "text": " Take the average, it's called global average pooling.", "tokens": [51140, 3664, 264, 4274, 11, 309, 311, 1219, 4338, 4274, 7005, 278, 13, 51230], "temperature": 0.0, "avg_logprob": -0.25195829073588055, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.08269169926643372}, {"id": 655, "seek": 316052, "start": 3177.84, "end": 3183.52, "text": " Or you could take the max, whatever, that would be global max pooling.", "tokens": [51230, 1610, 291, 727, 747, 264, 11469, 11, 2035, 11, 300, 576, 312, 4338, 11469, 7005, 278, 13, 51514], "temperature": 0.0, "avg_logprob": -0.25195829073588055, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.08269169926643372}, {"id": 656, "seek": 316052, "start": 3183.52, "end": 3187.16, "text": " So anyway, the main reason I wanted to show you this was to do something which I think", "tokens": [51514, 407, 4033, 11, 264, 2135, 1778, 286, 1415, 281, 855, 291, 341, 390, 281, 360, 746, 597, 286, 519, 51696], "temperature": 0.0, "avg_logprob": -0.25195829073588055, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.08269169926643372}, {"id": 657, "seek": 318716, "start": 3187.16, "end": 3194.3199999999997, "text": " is pretty interesting, which is to take something in our, I'm just going to zoom out a little", "tokens": [50364, 307, 1238, 1880, 11, 597, 307, 281, 747, 746, 294, 527, 11, 286, 478, 445, 516, 281, 8863, 484, 257, 707, 50722], "temperature": 0.0, "avg_logprob": -0.22951655160813106, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.010818188078701496}, {"id": 658, "seek": 318716, "start": 3194.3199999999997, "end": 3197.96, "text": " bit here.", "tokens": [50722, 857, 510, 13, 50904], "temperature": 0.0, "avg_logprob": -0.22951655160813106, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.010818188078701496}, {"id": 659, "seek": 318716, "start": 3197.96, "end": 3206.3199999999997, "text": " Let's take something in our max pool here.", "tokens": [50904, 961, 311, 747, 746, 294, 527, 11469, 7005, 510, 13, 51322], "temperature": 0.0, "avg_logprob": -0.22951655160813106, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.010818188078701496}, {"id": 660, "seek": 318716, "start": 3206.3199999999997, "end": 3211.7999999999997, "text": " And I'm going to say trace precedence to show you, here it is, the area that it's coming", "tokens": [51322, 400, 286, 478, 516, 281, 584, 13508, 16969, 655, 281, 855, 291, 11, 510, 309, 307, 11, 264, 1859, 300, 309, 311, 1348, 51596], "temperature": 0.0, "avg_logprob": -0.22951655160813106, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.010818188078701496}, {"id": 661, "seek": 318716, "start": 3211.7999999999997, "end": 3212.7999999999997, "text": " from.", "tokens": [51596, 490, 13, 51646], "temperature": 0.0, "avg_logprob": -0.22951655160813106, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.010818188078701496}, {"id": 662, "seek": 318716, "start": 3212.7999999999997, "end": 3214.8399999999997, "text": " Okay, so it's coming from these four numbers.", "tokens": [51646, 1033, 11, 370, 309, 311, 1348, 490, 613, 1451, 3547, 13, 51748], "temperature": 0.0, "avg_logprob": -0.22951655160813106, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.010818188078701496}, {"id": 663, "seek": 321484, "start": 3214.84, "end": 3221.52, "text": " Now if I trace precedence again, saying what's actually impacting this, obviously the kernel's", "tokens": [50364, 823, 498, 286, 13508, 16969, 655, 797, 11, 1566, 437, 311, 767, 29963, 341, 11, 2745, 264, 28256, 311, 50698], "temperature": 0.0, "avg_logprob": -0.22619892234232888, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.0006667012348771095}, {"id": 664, "seek": 321484, "start": 3221.52, "end": 3223.1200000000003, "text": " impacting it.", "tokens": [50698, 29963, 309, 13, 50778], "temperature": 0.0, "avg_logprob": -0.22619892234232888, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.0006667012348771095}, {"id": 665, "seek": 321484, "start": 3223.1200000000003, "end": 3228.8, "text": " And then you can see that the input area here is a bit bigger.", "tokens": [50778, 400, 550, 291, 393, 536, 300, 264, 4846, 1859, 510, 307, 257, 857, 3801, 13, 51062], "temperature": 0.0, "avg_logprob": -0.22619892234232888, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.0006667012348771095}, {"id": 666, "seek": 321484, "start": 3228.8, "end": 3237.48, "text": " And then if I trace precedence again, then you can see the input area is bigger still.", "tokens": [51062, 400, 550, 498, 286, 13508, 16969, 655, 797, 11, 550, 291, 393, 536, 264, 4846, 1859, 307, 3801, 920, 13, 51496], "temperature": 0.0, "avg_logprob": -0.22619892234232888, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.0006667012348771095}, {"id": 667, "seek": 323748, "start": 3237.48, "end": 3245.8, "text": " So this number here is calculated from all of these numbers in the input.", "tokens": [50364, 407, 341, 1230, 510, 307, 15598, 490, 439, 295, 613, 3547, 294, 264, 4846, 13, 50780], "temperature": 0.0, "avg_logprob": -0.23968963623046874, "compression_ratio": 1.6115107913669064, "no_speech_prob": 0.004468290600925684}, {"id": 668, "seek": 323748, "start": 3245.8, "end": 3252.92, "text": " This area in the input is called the receptive field of this unit.", "tokens": [50780, 639, 1859, 294, 264, 4846, 307, 1219, 264, 45838, 2519, 295, 341, 4985, 13, 51136], "temperature": 0.0, "avg_logprob": -0.23968963623046874, "compression_ratio": 1.6115107913669064, "no_speech_prob": 0.004468290600925684}, {"id": 669, "seek": 323748, "start": 3252.92, "end": 3262.16, "text": " And so the receptive field in this case is one, two, three, four, five, six by six.", "tokens": [51136, 400, 370, 264, 45838, 2519, 294, 341, 1389, 307, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 2309, 538, 2309, 13, 51598], "temperature": 0.0, "avg_logprob": -0.23968963623046874, "compression_ratio": 1.6115107913669064, "no_speech_prob": 0.004468290600925684}, {"id": 670, "seek": 326216, "start": 3262.16, "end": 3267.3999999999996, "text": " And that means that a pixel way up here in the top right has literally no ability to", "tokens": [50364, 400, 300, 1355, 300, 257, 19261, 636, 493, 510, 294, 264, 1192, 558, 575, 3736, 572, 3485, 281, 50626], "temperature": 0.0, "avg_logprob": -0.24985715845128992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.00039203822962008417}, {"id": 671, "seek": 326216, "start": 3267.3999999999996, "end": 3270.08, "text": " impact that activation.", "tokens": [50626, 2712, 300, 24433, 13, 50760], "temperature": 0.0, "avg_logprob": -0.24985715845128992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.00039203822962008417}, {"id": 672, "seek": 326216, "start": 3270.08, "end": 3273.5, "text": " It's not part of its receptive field.", "tokens": [50760, 467, 311, 406, 644, 295, 1080, 45838, 2519, 13, 50931], "temperature": 0.0, "avg_logprob": -0.24985715845128992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.00039203822962008417}, {"id": 673, "seek": 326216, "start": 3273.5, "end": 3277.7999999999997, "text": " If you have a whole bunch of strad2coms, each time you have one, the receptive field is", "tokens": [50931, 759, 291, 362, 257, 1379, 3840, 295, 1056, 345, 17, 66, 4785, 11, 1184, 565, 291, 362, 472, 11, 264, 45838, 2519, 307, 51146], "temperature": 0.0, "avg_logprob": -0.24985715845128992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.00039203822962008417}, {"id": 674, "seek": 326216, "start": 3277.7999999999997, "end": 3279.7999999999997, "text": " going to get twice as big.", "tokens": [51146, 516, 281, 483, 6091, 382, 955, 13, 51246], "temperature": 0.0, "avg_logprob": -0.24985715845128992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.00039203822962008417}, {"id": 675, "seek": 326216, "start": 3279.7999999999997, "end": 3285.56, "text": " So the receptive field at the end of a deep network is actually very large.", "tokens": [51246, 407, 264, 45838, 2519, 412, 264, 917, 295, 257, 2452, 3209, 307, 767, 588, 2416, 13, 51534], "temperature": 0.0, "avg_logprob": -0.24985715845128992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.00039203822962008417}, {"id": 676, "seek": 328556, "start": 3285.56, "end": 3293.12, "text": " But the inputs closest to the middle of the receptive field have the biggest kind of say", "tokens": [50364, 583, 264, 15743, 13699, 281, 264, 2808, 295, 264, 45838, 2519, 362, 264, 3880, 733, 295, 584, 50742], "temperature": 0.0, "avg_logprob": -0.2900422861878301, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0016228881431743503}, {"id": 677, "seek": 328556, "start": 3293.12, "end": 3301.16, "text": " in the output, because they implicitly appear the most often in all of these kind of dot", "tokens": [50742, 294, 264, 5598, 11, 570, 436, 26947, 356, 4204, 264, 881, 2049, 294, 439, 295, 613, 733, 295, 5893, 51144], "temperature": 0.0, "avg_logprob": -0.2900422861878301, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0016228881431743503}, {"id": 678, "seek": 328556, "start": 3301.16, "end": 3306.24, "text": " products that are inside this convolutional window.", "tokens": [51144, 3383, 300, 366, 1854, 341, 45216, 304, 4910, 13, 51398], "temperature": 0.0, "avg_logprob": -0.2900422861878301, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0016228881431743503}, {"id": 679, "seek": 328556, "start": 3306.24, "end": 3310.94, "text": " So the receptive field is not just like a single binary on-off thing.", "tokens": [51398, 407, 264, 45838, 2519, 307, 406, 445, 411, 257, 2167, 17434, 322, 12, 4506, 551, 13, 51633], "temperature": 0.0, "avg_logprob": -0.2900422861878301, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0016228881431743503}, {"id": 680, "seek": 331094, "start": 3310.94, "end": 3316.46, "text": " Literally all the stuff that's not got precedence here is not part of it at all.", "tokens": [50364, 23768, 439, 264, 1507, 300, 311, 406, 658, 16969, 655, 510, 307, 406, 644, 295, 309, 412, 439, 13, 50640], "temperature": 0.0, "avg_logprob": -0.29863950317981197, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.01826341450214386}, {"id": 681, "seek": 331094, "start": 3316.46, "end": 3320.86, "text": " But the closer to the center of the receptive field, the more impact it's going to have,", "tokens": [50640, 583, 264, 4966, 281, 264, 3056, 295, 264, 45838, 2519, 11, 264, 544, 2712, 309, 311, 516, 281, 362, 11, 50860], "temperature": 0.0, "avg_logprob": -0.29863950317981197, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.01826341450214386}, {"id": 682, "seek": 331094, "start": 3320.86, "end": 3324.34, "text": " the more ability it's got to change this number.", "tokens": [50860, 264, 544, 3485, 309, 311, 658, 281, 1319, 341, 1230, 13, 51034], "temperature": 0.0, "avg_logprob": -0.29863950317981197, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.01826341450214386}, {"id": 683, "seek": 331094, "start": 3324.34, "end": 3329.18, "text": " So the receptive field is a really important concept.", "tokens": [51034, 407, 264, 45838, 2519, 307, 257, 534, 1021, 3410, 13, 51276], "temperature": 0.0, "avg_logprob": -0.29863950317981197, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.01826341450214386}, {"id": 684, "seek": 331094, "start": 3329.18, "end": 3336.3, "text": " And yeah, playing around with Excel's precedent arrows, I think is a nice way to to see that,", "tokens": [51276, 400, 1338, 11, 2433, 926, 365, 19060, 311, 37388, 19669, 11, 286, 519, 307, 257, 1481, 636, 281, 281, 536, 300, 11, 51632], "temperature": 0.0, "avg_logprob": -0.29863950317981197, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.01826341450214386}, {"id": 685, "seek": 331094, "start": 3336.3, "end": 3339.7000000000003, "text": " at least in my opinion.", "tokens": [51632, 412, 1935, 294, 452, 4800, 13, 51802], "temperature": 0.0, "avg_logprob": -0.29863950317981197, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.01826341450214386}, {"id": 686, "seek": 333970, "start": 3339.7, "end": 3344.58, "text": " And apart from anything else, it's great fun creating a convolutional neural network in Excel.", "tokens": [50364, 400, 4936, 490, 1340, 1646, 11, 309, 311, 869, 1019, 4084, 257, 45216, 304, 18161, 3209, 294, 19060, 13, 50608], "temperature": 0.0, "avg_logprob": -0.433181247195682, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0014103383291512728}, {"id": 687, "seek": 333970, "start": 3344.58, "end": 3349.54, "text": " I thought so anyway.", "tokens": [50608, 286, 1194, 370, 4033, 13, 50856], "temperature": 0.0, "avg_logprob": -0.433181247195682, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0014103383291512728}, {"id": 688, "seek": 333970, "start": 3349.54, "end": 3353.4199999999996, "text": " Okay, so let's take a seven minute break.", "tokens": [50856, 1033, 11, 370, 718, 311, 747, 257, 3407, 3456, 1821, 13, 51050], "temperature": 0.0, "avg_logprob": -0.433181247195682, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0014103383291512728}, {"id": 689, "seek": 333970, "start": 3353.4199999999996, "end": 3360.74, "text": " I'll see you back after that to talk about a convolutional autoencoder.", "tokens": [51050, 286, 603, 536, 291, 646, 934, 300, 281, 751, 466, 257, 45216, 304, 8399, 22660, 19866, 13, 51416], "temperature": 0.0, "avg_logprob": -0.433181247195682, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0014103383291512728}, {"id": 690, "seek": 333970, "start": 3360.74, "end": 3363.2599999999998, "text": " All right.", "tokens": [51416, 1057, 558, 13, 51542], "temperature": 0.0, "avg_logprob": -0.433181247195682, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0014103383291512728}, {"id": 691, "seek": 333970, "start": 3363.2599999999998, "end": 3366.58, "text": " Okay, welcome back.", "tokens": [51542, 1033, 11, 2928, 646, 13, 51708], "temperature": 0.0, "avg_logprob": -0.433181247195682, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0014103383291512728}, {"id": 692, "seek": 336658, "start": 3366.58, "end": 3372.42, "text": " We're going to have a look now at the autoencoder notebook.", "tokens": [50364, 492, 434, 516, 281, 362, 257, 574, 586, 412, 264, 8399, 22660, 19866, 21060, 13, 50656], "temperature": 0.0, "avg_logprob": -0.2527940300073517, "compression_ratio": 1.7597765363128492, "no_speech_prob": 0.006488129496574402}, {"id": 693, "seek": 336658, "start": 3372.42, "end": 3375.18, "text": " So we're just going to import all of our usual stuff.", "tokens": [50656, 407, 321, 434, 445, 516, 281, 974, 439, 295, 527, 7713, 1507, 13, 50794], "temperature": 0.0, "avg_logprob": -0.2527940300073517, "compression_ratio": 1.7597765363128492, "no_speech_prob": 0.006488129496574402}, {"id": 694, "seek": 336658, "start": 3375.18, "end": 3382.1, "text": " And we've got one more of our own modules to import now as well.", "tokens": [50794, 400, 321, 600, 658, 472, 544, 295, 527, 1065, 16679, 281, 974, 586, 382, 731, 13, 51140], "temperature": 0.0, "avg_logprob": -0.2527940300073517, "compression_ratio": 1.7597765363128492, "no_speech_prob": 0.006488129496574402}, {"id": 695, "seek": 336658, "start": 3382.1, "end": 3389.58, "text": " And this time we are going to switch to a different, we're going to switch to a different", "tokens": [51140, 400, 341, 565, 321, 366, 516, 281, 3679, 281, 257, 819, 11, 321, 434, 516, 281, 3679, 281, 257, 819, 51514], "temperature": 0.0, "avg_logprob": -0.2527940300073517, "compression_ratio": 1.7597765363128492, "no_speech_prob": 0.006488129496574402}, {"id": 696, "seek": 336658, "start": 3389.58, "end": 3394.2, "text": " data set, which is the fashion MNIST data set.", "tokens": [51514, 1412, 992, 11, 597, 307, 264, 6700, 376, 45, 19756, 1412, 992, 13, 51745], "temperature": 0.0, "avg_logprob": -0.2527940300073517, "compression_ratio": 1.7597765363128492, "no_speech_prob": 0.006488129496574402}, {"id": 697, "seek": 339420, "start": 3394.2, "end": 3402.8799999999997, "text": " We can take advantage of the stuff that we did in O5 datasets, and the HuggingFace stuff", "tokens": [50364, 492, 393, 747, 5002, 295, 264, 1507, 300, 321, 630, 294, 422, 20, 42856, 11, 293, 264, 46892, 3249, 37, 617, 1507, 50798], "temperature": 0.0, "avg_logprob": -0.2411550573400549, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.001098723616451025}, {"id": 698, "seek": 339420, "start": 3402.8799999999997, "end": 3404.08, "text": " to load it.", "tokens": [50798, 281, 3677, 309, 13, 50858], "temperature": 0.0, "avg_logprob": -0.2411550573400549, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.001098723616451025}, {"id": 699, "seek": 339420, "start": 3404.08, "end": 3414.8999999999996, "text": " So we've seen this a little bit before, back in our datasets one here.", "tokens": [50858, 407, 321, 600, 1612, 341, 257, 707, 857, 949, 11, 646, 294, 527, 42856, 472, 510, 13, 51399], "temperature": 0.0, "avg_logprob": -0.2411550573400549, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.001098723616451025}, {"id": 700, "seek": 339420, "start": 3414.8999999999996, "end": 3418.4399999999996, "text": " And we never actually built any models with it.", "tokens": [51399, 400, 321, 1128, 767, 3094, 604, 5245, 365, 309, 13, 51576], "temperature": 0.0, "avg_logprob": -0.2411550573400549, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.001098723616451025}, {"id": 701, "seek": 339420, "start": 3418.4399999999996, "end": 3422.6, "text": " So let's first of all do that.", "tokens": [51576, 407, 718, 311, 700, 295, 439, 360, 300, 13, 51784], "temperature": 0.0, "avg_logprob": -0.2411550573400549, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.001098723616451025}, {"id": 702, "seek": 342260, "start": 3422.6, "end": 3429.36, "text": " So this is just going to convert each thing, each image into a tensor.", "tokens": [50364, 407, 341, 307, 445, 516, 281, 7620, 1184, 551, 11, 1184, 3256, 666, 257, 40863, 13, 50702], "temperature": 0.0, "avg_logprob": -0.30985032944452195, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.0013250157935544848}, {"id": 703, "seek": 342260, "start": 3429.36, "end": 3430.92, "text": " That's going to be an in-place transform.", "tokens": [50702, 663, 311, 516, 281, 312, 364, 294, 12, 6742, 4088, 13, 50780], "temperature": 0.0, "avg_logprob": -0.30985032944452195, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.0013250157935544848}, {"id": 704, "seek": 342260, "start": 3430.92, "end": 3434.02, "text": " Remember we created this decorator.", "tokens": [50780, 5459, 321, 2942, 341, 7919, 1639, 13, 50935], "temperature": 0.0, "avg_logprob": -0.30985032944452195, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.0013250157935544848}, {"id": 705, "seek": 342260, "start": 3434.02, "end": 3437.12, "text": " And so we can call dataset dictionary with transform.", "tokens": [50935, 400, 370, 321, 393, 818, 28872, 25890, 365, 4088, 13, 51090], "temperature": 0.0, "avg_logprob": -0.30985032944452195, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.0013250157935544848}, {"id": 706, "seek": 342260, "start": 3437.12, "end": 3442.24, "text": " This is all stuff we've done before.", "tokens": [51090, 639, 307, 439, 1507, 321, 600, 1096, 949, 13, 51346], "temperature": 0.0, "avg_logprob": -0.30985032944452195, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.0013250157935544848}, {"id": 707, "seek": 342260, "start": 3442.24, "end": 3448.36, "text": " And so here we have our example of a sneaker.", "tokens": [51346, 400, 370, 510, 321, 362, 527, 1365, 295, 257, 9244, 4003, 13, 51652], "temperature": 0.0, "avg_logprob": -0.30985032944452195, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.0013250157935544848}, {"id": 708, "seek": 342260, "start": 3448.36, "end": 3452.08, "text": " All right.", "tokens": [51652, 1057, 558, 13, 51838], "temperature": 0.0, "avg_logprob": -0.30985032944452195, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.0013250157935544848}, {"id": 709, "seek": 345208, "start": 3452.56, "end": 3455.56, "text": " And we will create our collation function.", "tokens": [50388, 400, 321, 486, 1884, 527, 1263, 399, 2445, 13, 50538], "temperature": 0.0, "avg_logprob": -0.3050854532342208, "compression_ratio": 1.8156682027649769, "no_speech_prob": 0.0001072086306521669}, {"id": 710, "seek": 345208, "start": 3455.56, "end": 3458.64, "text": " Collating a dictionary for that dataset.", "tokens": [50538, 4586, 990, 257, 25890, 337, 300, 28872, 13, 50692], "temperature": 0.0, "avg_logprob": -0.3050854532342208, "compression_ratio": 1.8156682027649769, "no_speech_prob": 0.0001072086306521669}, {"id": 711, "seek": 345208, "start": 3458.64, "end": 3462.56, "text": " That's something to remind, you should remind yourself we built that ourselves in the datasets", "tokens": [50692, 663, 311, 746, 281, 4160, 11, 291, 820, 4160, 1803, 321, 3094, 300, 4175, 294, 264, 42856, 50888], "temperature": 0.0, "avg_logprob": -0.3050854532342208, "compression_ratio": 1.8156682027649769, "no_speech_prob": 0.0001072086306521669}, {"id": 712, "seek": 345208, "start": 3462.56, "end": 3465.2, "text": " notebook.", "tokens": [50888, 21060, 13, 51020], "temperature": 0.0, "avg_logprob": -0.3050854532342208, "compression_ratio": 1.8156682027649769, "no_speech_prob": 0.0001072086306521669}, {"id": 713, "seek": 345208, "start": 3465.2, "end": 3472.36, "text": " And let's actually make our collate function something that does to device, which we wrote", "tokens": [51020, 400, 718, 311, 767, 652, 527, 1263, 473, 2445, 746, 300, 775, 281, 4302, 11, 597, 321, 4114, 51378], "temperature": 0.0, "avg_logprob": -0.3050854532342208, "compression_ratio": 1.8156682027649769, "no_speech_prob": 0.0001072086306521669}, {"id": 714, "seek": 345208, "start": 3472.36, "end": 3475.52, "text": " in our last notebook.", "tokens": [51378, 294, 527, 1036, 21060, 13, 51536], "temperature": 0.0, "avg_logprob": -0.3050854532342208, "compression_ratio": 1.8156682027649769, "no_speech_prob": 0.0001072086306521669}, {"id": 715, "seek": 345208, "start": 3475.52, "end": 3481.08, "text": " And we'll create a little data loaders function here, which is going to go through each item", "tokens": [51536, 400, 321, 603, 1884, 257, 707, 1412, 3677, 433, 2445, 510, 11, 597, 307, 516, 281, 352, 807, 1184, 3174, 51814], "temperature": 0.0, "avg_logprob": -0.3050854532342208, "compression_ratio": 1.8156682027649769, "no_speech_prob": 0.0001072086306521669}, {"id": 716, "seek": 348108, "start": 3481.08, "end": 3487.04, "text": " in the dataset dictionary and create a data loader for it and give us a dictionary of", "tokens": [50364, 294, 264, 28872, 25890, 293, 1884, 257, 1412, 3677, 260, 337, 309, 293, 976, 505, 257, 25890, 295, 50662], "temperature": 0.0, "avg_logprob": -0.3345574487613726, "compression_ratio": 1.6993865030674846, "no_speech_prob": 8.481099212076515e-05}, {"id": 717, "seek": 348108, "start": 3487.04, "end": 3489.44, "text": " data loaders.", "tokens": [50662, 1412, 3677, 433, 13, 50782], "temperature": 0.0, "avg_logprob": -0.3345574487613726, "compression_ratio": 1.6993865030674846, "no_speech_prob": 8.481099212076515e-05}, {"id": 718, "seek": 348108, "start": 3489.44, "end": 3491.36, "text": " Okay.", "tokens": [50782, 1033, 13, 50878], "temperature": 0.0, "avg_logprob": -0.3345574487613726, "compression_ratio": 1.6993865030674846, "no_speech_prob": 8.481099212076515e-05}, {"id": 719, "seek": 348108, "start": 3491.36, "end": 3501.4, "text": " So now we've got a data loader for training and a data loader for validation.", "tokens": [50878, 407, 586, 321, 600, 658, 257, 1412, 3677, 260, 337, 3097, 293, 257, 1412, 3677, 260, 337, 24071, 13, 51380], "temperature": 0.0, "avg_logprob": -0.3345574487613726, "compression_ratio": 1.6993865030674846, "no_speech_prob": 8.481099212076515e-05}, {"id": 720, "seek": 348108, "start": 3501.4, "end": 3510.6, "text": " So we can grab the X and Y batch by just calling next on that iterator, as we've done before.", "tokens": [51380, 407, 321, 393, 4444, 264, 1783, 293, 398, 15245, 538, 445, 5141, 958, 322, 300, 17138, 1639, 11, 382, 321, 600, 1096, 949, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3345574487613726, "compression_ratio": 1.6993865030674846, "no_speech_prob": 8.481099212076515e-05}, {"id": 721, "seek": 351060, "start": 3511.6, "end": 3518.04, "text": " We can grab the, let's look at each of these in turn, actually.", "tokens": [50414, 492, 393, 4444, 264, 11, 718, 311, 574, 412, 1184, 295, 613, 294, 1261, 11, 767, 13, 50736], "temperature": 0.0, "avg_logprob": -0.2585081163343492, "compression_ratio": 1.598901098901099, "no_speech_prob": 3.4268472518306226e-05}, {"id": 722, "seek": 351060, "start": 3518.04, "end": 3520.44, "text": " We've done all this before, but it's a couple of weeks ago.", "tokens": [50736, 492, 600, 1096, 439, 341, 949, 11, 457, 309, 311, 257, 1916, 295, 3259, 2057, 13, 50856], "temperature": 0.0, "avg_logprob": -0.2585081163343492, "compression_ratio": 1.598901098901099, "no_speech_prob": 3.4268472518306226e-05}, {"id": 723, "seek": 351060, "start": 3520.44, "end": 3526.72, "text": " So just to remind you, we can get the names of the features.", "tokens": [50856, 407, 445, 281, 4160, 291, 11, 321, 393, 483, 264, 5288, 295, 264, 4122, 13, 51170], "temperature": 0.0, "avg_logprob": -0.2585081163343492, "compression_ratio": 1.598901098901099, "no_speech_prob": 3.4268472518306226e-05}, {"id": 724, "seek": 351060, "start": 3526.72, "end": 3532.3199999999997, "text": " And so we can then get, create an item getter for our Ys.", "tokens": [51170, 400, 370, 321, 393, 550, 483, 11, 1884, 364, 3174, 483, 391, 337, 527, 398, 82, 13, 51450], "temperature": 0.0, "avg_logprob": -0.2585081163343492, "compression_ratio": 1.598901098901099, "no_speech_prob": 3.4268472518306226e-05}, {"id": 725, "seek": 351060, "start": 3532.3199999999997, "end": 3534.2, "text": " And we can, so we'll call that the label getter.", "tokens": [51450, 400, 321, 393, 11, 370, 321, 603, 818, 300, 264, 7645, 483, 391, 13, 51544], "temperature": 0.0, "avg_logprob": -0.2585081163343492, "compression_ratio": 1.598901098901099, "no_speech_prob": 3.4268472518306226e-05}, {"id": 726, "seek": 353420, "start": 3534.6, "end": 3542.3599999999997, "text": " We can apply that to our labels to get the titles of everything in our mini batch.", "tokens": [50384, 492, 393, 3079, 300, 281, 527, 16949, 281, 483, 264, 12992, 295, 1203, 294, 527, 8382, 15245, 13, 50772], "temperature": 0.0, "avg_logprob": -0.3460485339164734, "compression_ratio": 1.5266666666666666, "no_speech_prob": 0.0011335561284795403}, {"id": 727, "seek": 353420, "start": 3542.3599999999997, "end": 3547.96, "text": " And we can then call our show images that we created with that mini batch, with those", "tokens": [50772, 400, 321, 393, 550, 818, 527, 855, 5267, 300, 321, 2942, 365, 300, 8382, 15245, 11, 365, 729, 51052], "temperature": 0.0, "avg_logprob": -0.3460485339164734, "compression_ratio": 1.5266666666666666, "no_speech_prob": 0.0011335561284795403}, {"id": 728, "seek": 353420, "start": 3547.96, "end": 3549.56, "text": " titles.", "tokens": [51052, 12992, 13, 51132], "temperature": 0.0, "avg_logprob": -0.3460485339164734, "compression_ratio": 1.5266666666666666, "no_speech_prob": 0.0011335561284795403}, {"id": 729, "seek": 353420, "start": 3549.56, "end": 3555.68, "text": " And here we have our fashion MNIST mini batch.", "tokens": [51132, 400, 510, 321, 362, 527, 6700, 376, 45, 19756, 8382, 15245, 13, 51438], "temperature": 0.0, "avg_logprob": -0.3460485339164734, "compression_ratio": 1.5266666666666666, "no_speech_prob": 0.0011335561284795403}, {"id": 730, "seek": 353420, "start": 3555.68, "end": 3559.9199999999996, "text": " Okay.", "tokens": [51438, 1033, 13, 51650], "temperature": 0.0, "avg_logprob": -0.3460485339164734, "compression_ratio": 1.5266666666666666, "no_speech_prob": 0.0011335561284795403}, {"id": 731, "seek": 355992, "start": 3560.12, "end": 3562.64, "text": " So let's create a classifier.", "tokens": [50374, 407, 718, 311, 1884, 257, 1508, 9902, 13, 50500], "temperature": 0.0, "avg_logprob": -0.2774367332458496, "compression_ratio": 1.278688524590164, "no_speech_prob": 0.007345655933022499}, {"id": 732, "seek": 355992, "start": 3562.64, "end": 3569.96, "text": " And we're just going to use exactly the same code copy and posted from the previous notebook.", "tokens": [50500, 400, 321, 434, 445, 516, 281, 764, 2293, 264, 912, 3089, 5055, 293, 9437, 490, 264, 3894, 21060, 13, 50866], "temperature": 0.0, "avg_logprob": -0.2774367332458496, "compression_ratio": 1.278688524590164, "no_speech_prob": 0.007345655933022499}, {"id": 733, "seek": 355992, "start": 3569.96, "end": 3579.28, "text": " So here is our sequential model.", "tokens": [50866, 407, 510, 307, 527, 42881, 2316, 13, 51332], "temperature": 0.0, "avg_logprob": -0.2774367332458496, "compression_ratio": 1.278688524590164, "no_speech_prob": 0.007345655933022499}, {"id": 734, "seek": 357928, "start": 3579.28, "end": 3589.32, "text": " And we are going to grab the parameters of the CNN.", "tokens": [50364, 400, 321, 366, 516, 281, 4444, 264, 9834, 295, 264, 24859, 13, 50866], "temperature": 0.0, "avg_logprob": -0.24602372305733816, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.009412494488060474}, {"id": 735, "seek": 357928, "start": 3589.32, "end": 3595.2400000000002, "text": " And the CNN, I've actually moved it over to the device.", "tokens": [50866, 400, 264, 24859, 11, 286, 600, 767, 4259, 309, 670, 281, 264, 4302, 13, 51162], "temperature": 0.0, "avg_logprob": -0.24602372305733816, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.009412494488060474}, {"id": 736, "seek": 357928, "start": 3595.2400000000002, "end": 3597.84, "text": " The default device was what we created in our last notebook.", "tokens": [51162, 440, 7576, 4302, 390, 437, 321, 2942, 294, 527, 1036, 21060, 13, 51292], "temperature": 0.0, "avg_logprob": -0.24602372305733816, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.009412494488060474}, {"id": 737, "seek": 357928, "start": 3597.84, "end": 3601.0, "text": " And as you can see, it's fitting.", "tokens": [51292, 400, 382, 291, 393, 536, 11, 309, 311, 15669, 13, 51450], "temperature": 0.0, "avg_logprob": -0.24602372305733816, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.009412494488060474}, {"id": 738, "seek": 357928, "start": 3601.0, "end": 3608.0400000000004, "text": " Now our first problem is it's fitting very slowly, which is kind of annoying.", "tokens": [51450, 823, 527, 700, 1154, 307, 309, 311, 15669, 588, 5692, 11, 597, 307, 733, 295, 11304, 13, 51802], "temperature": 0.0, "avg_logprob": -0.24602372305733816, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.009412494488060474}, {"id": 739, "seek": 360804, "start": 3608.04, "end": 3616.4, "text": " So why is it running pretty slowly?", "tokens": [50364, 407, 983, 307, 309, 2614, 1238, 5692, 30, 50782], "temperature": 0.0, "avg_logprob": -0.21979546848731704, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.00031999769271351397}, {"id": 740, "seek": 360804, "start": 3616.4, "end": 3618.8, "text": " Let's think about, let's have a look at our data set.", "tokens": [50782, 961, 311, 519, 466, 11, 718, 311, 362, 257, 574, 412, 527, 1412, 992, 13, 50902], "temperature": 0.0, "avg_logprob": -0.21979546848731704, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.00031999769271351397}, {"id": 741, "seek": 360804, "start": 3618.8, "end": 3627.32, "text": " So when it's finally finished, let's take a look at an item from the data set.", "tokens": [50902, 407, 562, 309, 311, 2721, 4335, 11, 718, 311, 747, 257, 574, 412, 364, 3174, 490, 264, 1412, 992, 13, 51328], "temperature": 0.0, "avg_logprob": -0.21979546848731704, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.00031999769271351397}, {"id": 742, "seek": 360804, "start": 3627.32, "end": 3628.84, "text": " Actually let's not look at the data set.", "tokens": [51328, 5135, 718, 311, 406, 574, 412, 264, 1412, 992, 13, 51404], "temperature": 0.0, "avg_logprob": -0.21979546848731704, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.00031999769271351397}, {"id": 743, "seek": 360804, "start": 3628.84, "end": 3633.68, "text": " Let's actually go all the way back to the data set dictionary.", "tokens": [51404, 961, 311, 767, 352, 439, 264, 636, 646, 281, 264, 1412, 992, 25890, 13, 51646], "temperature": 0.0, "avg_logprob": -0.21979546848731704, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.00031999769271351397}, {"id": 744, "seek": 363368, "start": 3633.68, "end": 3640.06, "text": " So before it gets transformed, data set dictionary.", "tokens": [50364, 407, 949, 309, 2170, 16894, 11, 1412, 992, 25890, 13, 50683], "temperature": 0.0, "avg_logprob": -0.27567726023056927, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.00157308136112988}, {"id": 745, "seek": 363368, "start": 3640.06, "end": 3645.2, "text": " And let's grab the training part of that.", "tokens": [50683, 400, 718, 311, 4444, 264, 3097, 644, 295, 300, 13, 50940], "temperature": 0.0, "avg_logprob": -0.27567726023056927, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.00157308136112988}, {"id": 746, "seek": 363368, "start": 3645.2, "end": 3649.2, "text": " And let's grab one item.", "tokens": [50940, 400, 718, 311, 4444, 472, 3174, 13, 51140], "temperature": 0.0, "avg_logprob": -0.27567726023056927, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.00157308136112988}, {"id": 747, "seek": 363368, "start": 3649.2, "end": 3651.68, "text": " And actually we can see here the problem.", "tokens": [51140, 400, 767, 321, 393, 536, 510, 264, 1154, 13, 51264], "temperature": 0.0, "avg_logprob": -0.27567726023056927, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.00157308136112988}, {"id": 748, "seek": 363368, "start": 3651.68, "end": 3659.72, "text": " For MNIST, we had all of the data loaded into memory into a single big tensor.", "tokens": [51264, 1171, 376, 45, 19756, 11, 321, 632, 439, 295, 264, 1412, 13210, 666, 4675, 666, 257, 2167, 955, 40863, 13, 51666], "temperature": 0.0, "avg_logprob": -0.27567726023056927, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.00157308136112988}, {"id": 749, "seek": 365972, "start": 3659.7599999999998, "end": 3666.52, "text": " But this HuggingFace one is created in a much more kind of normal way, which is each image", "tokens": [50366, 583, 341, 46892, 3249, 37, 617, 472, 307, 2942, 294, 257, 709, 544, 733, 295, 2710, 636, 11, 597, 307, 1184, 3256, 50704], "temperature": 0.0, "avg_logprob": -0.25456957070224256, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.0014550455380231142}, {"id": 750, "seek": 365972, "start": 3666.52, "end": 3669.3399999999997, "text": " is a totally separate PNG image.", "tokens": [50704, 307, 257, 3879, 4994, 430, 30237, 3256, 13, 50845], "temperature": 0.0, "avg_logprob": -0.25456957070224256, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.0014550455380231142}, {"id": 751, "seek": 365972, "start": 3669.3399999999997, "end": 3676.3199999999997, "text": " It's not all pre-converted into a single thing.", "tokens": [50845, 467, 311, 406, 439, 659, 12, 1671, 18537, 666, 257, 2167, 551, 13, 51194], "temperature": 0.0, "avg_logprob": -0.25456957070224256, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.0014550455380231142}, {"id": 752, "seek": 365972, "start": 3676.3199999999997, "end": 3677.3199999999997, "text": " Why is that a problem?", "tokens": [51194, 1545, 307, 300, 257, 1154, 30, 51244], "temperature": 0.0, "avg_logprob": -0.25456957070224256, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.0014550455380231142}, {"id": 753, "seek": 365972, "start": 3677.3199999999997, "end": 3688.16, "text": " Well, the reason it's a problem is that our data loader is spending all of its time decoding", "tokens": [51244, 1042, 11, 264, 1778, 309, 311, 257, 1154, 307, 300, 527, 1412, 3677, 260, 307, 6434, 439, 295, 1080, 565, 979, 8616, 51786], "temperature": 0.0, "avg_logprob": -0.25456957070224256, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.0014550455380231142}, {"id": 754, "seek": 368816, "start": 3688.16, "end": 3692.48, "text": " these PNGs.", "tokens": [50364, 613, 430, 30237, 82, 13, 50580], "temperature": 0.0, "avg_logprob": -0.3345237239714592, "compression_ratio": 1.396135265700483, "no_speech_prob": 0.005819897633045912}, {"id": 755, "seek": 368816, "start": 3692.48, "end": 3698.48, "text": " So if I train here.", "tokens": [50580, 407, 498, 286, 3847, 510, 13, 50880], "temperature": 0.0, "avg_logprob": -0.3345237239714592, "compression_ratio": 1.396135265700483, "no_speech_prob": 0.005819897633045912}, {"id": 756, "seek": 368816, "start": 3698.48, "end": 3704.6, "text": " Okay so while I'm training, I can type Htop and you can see that basically my CPU is 100%", "tokens": [50880, 1033, 370, 1339, 286, 478, 3097, 11, 286, 393, 2010, 389, 19337, 293, 291, 393, 536, 300, 1936, 452, 13199, 307, 2319, 4, 51186], "temperature": 0.0, "avg_logprob": -0.3345237239714592, "compression_ratio": 1.396135265700483, "no_speech_prob": 0.005819897633045912}, {"id": 757, "seek": 368816, "start": 3704.6, "end": 3705.6, "text": " used.", "tokens": [51186, 1143, 13, 51236], "temperature": 0.0, "avg_logprob": -0.3345237239714592, "compression_ratio": 1.396135265700483, "no_speech_prob": 0.005819897633045912}, {"id": 758, "seek": 368816, "start": 3705.6, "end": 3709.08, "text": " Now that's a bit weird because I've actually got 64 CPUs.", "tokens": [51236, 823, 300, 311, 257, 857, 3657, 570, 286, 600, 767, 658, 12145, 13199, 82, 13, 51410], "temperature": 0.0, "avg_logprob": -0.3345237239714592, "compression_ratio": 1.396135265700483, "no_speech_prob": 0.005819897633045912}, {"id": 759, "seek": 368816, "start": 3709.08, "end": 3712.0, "text": " Why is it using just one of them is the first problem.", "tokens": [51410, 1545, 307, 309, 1228, 445, 472, 295, 552, 307, 264, 700, 1154, 13, 51556], "temperature": 0.0, "avg_logprob": -0.3345237239714592, "compression_ratio": 1.396135265700483, "no_speech_prob": 0.005819897633045912}, {"id": 760, "seek": 368816, "start": 3712.0, "end": 3714.92, "text": " But why does it matter that it's using 100% CPU?", "tokens": [51556, 583, 983, 775, 309, 1871, 300, 309, 311, 1228, 2319, 4, 13199, 30, 51702], "temperature": 0.0, "avg_logprob": -0.3345237239714592, "compression_ratio": 1.396135265700483, "no_speech_prob": 0.005819897633045912}, {"id": 761, "seek": 371492, "start": 3714.92, "end": 3719.6800000000003, "text": " Well the reason it matters, let's run it again so you can see.", "tokens": [50364, 1042, 264, 1778, 309, 7001, 11, 718, 311, 1190, 309, 797, 370, 291, 393, 536, 13, 50602], "temperature": 0.0, "avg_logprob": -0.312921085357666, "compression_ratio": 1.4844444444444445, "no_speech_prob": 0.0013458236353471875}, {"id": 762, "seek": 371492, "start": 3719.6800000000003, "end": 3726.16, "text": " Why does it matter that our CPU is 100% and why is it making it so slow?", "tokens": [50602, 1545, 775, 309, 1871, 300, 527, 13199, 307, 2319, 4, 293, 983, 307, 309, 1455, 309, 370, 2964, 30, 50926], "temperature": 0.0, "avg_logprob": -0.312921085357666, "compression_ratio": 1.4844444444444445, "no_speech_prob": 0.0013458236353471875}, {"id": 763, "seek": 371492, "start": 3726.16, "end": 3736.2400000000002, "text": " Well the reason why is if we look at NVIDIA SMI daemon, that will monitor our GPUs utilization.", "tokens": [50926, 1042, 264, 1778, 983, 307, 498, 321, 574, 412, 426, 3958, 6914, 13115, 40, 1120, 36228, 11, 300, 486, 6002, 527, 18407, 82, 37074, 13, 51430], "temperature": 0.0, "avg_logprob": -0.312921085357666, "compression_ratio": 1.4844444444444445, "no_speech_prob": 0.0013458236353471875}, {"id": 764, "seek": 371492, "start": 3736.2400000000002, "end": 3740.64, "text": " I've got three GPUs, so I say to choose just the zeroth index one.", "tokens": [51430, 286, 600, 658, 1045, 18407, 82, 11, 370, 286, 584, 281, 2826, 445, 264, 44746, 900, 8186, 472, 13, 51650], "temperature": 0.0, "avg_logprob": -0.312921085357666, "compression_ratio": 1.4844444444444445, "no_speech_prob": 0.0013458236353471875}, {"id": 765, "seek": 371492, "start": 3740.64, "end": 3743.2000000000003, "text": " And you'll see this column here SM.", "tokens": [51650, 400, 291, 603, 536, 341, 7738, 510, 13115, 13, 51778], "temperature": 0.0, "avg_logprob": -0.312921085357666, "compression_ratio": 1.4844444444444445, "no_speech_prob": 0.0013458236353471875}, {"id": 766, "seek": 374320, "start": 3743.2, "end": 3745.2799999999997, "text": " This stands for symmetric multiprocessor.", "tokens": [50364, 639, 7382, 337, 32330, 3311, 340, 25432, 13, 50468], "temperature": 0.0, "avg_logprob": -0.23207222412679798, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00017130696505773813}, {"id": 767, "seek": 374320, "start": 3745.2799999999997, "end": 3748.3599999999997, "text": " It's like the equivalent of like CPU usage.", "tokens": [50468, 467, 311, 411, 264, 10344, 295, 411, 13199, 14924, 13, 50622], "temperature": 0.0, "avg_logprob": -0.23207222412679798, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00017130696505773813}, {"id": 768, "seek": 374320, "start": 3748.3599999999997, "end": 3753.52, "text": " And generally we're only using up 1% of our one GPU.", "tokens": [50622, 400, 5101, 321, 434, 787, 1228, 493, 502, 4, 295, 527, 472, 18407, 13, 50880], "temperature": 0.0, "avg_logprob": -0.23207222412679798, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00017130696505773813}, {"id": 769, "seek": 374320, "start": 3753.52, "end": 3757.02, "text": " So no wonder it's so slow.", "tokens": [50880, 407, 572, 2441, 309, 311, 370, 2964, 13, 51055], "temperature": 0.0, "avg_logprob": -0.23207222412679798, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00017130696505773813}, {"id": 770, "seek": 374320, "start": 3757.02, "end": 3762.64, "text": " So the first thing we want to do then is try to make things faster.", "tokens": [51055, 407, 264, 700, 551, 321, 528, 281, 360, 550, 307, 853, 281, 652, 721, 4663, 13, 51336], "temperature": 0.0, "avg_logprob": -0.23207222412679798, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00017130696505773813}, {"id": 771, "seek": 374320, "start": 3762.64, "end": 3769.3999999999996, "text": " Now to make things faster we want to be using more than one CPU to decode our PNGs.", "tokens": [51336, 823, 281, 652, 721, 4663, 321, 528, 281, 312, 1228, 544, 813, 472, 13199, 281, 979, 1429, 527, 430, 30237, 82, 13, 51674], "temperature": 0.0, "avg_logprob": -0.23207222412679798, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00017130696505773813}, {"id": 772, "seek": 374320, "start": 3769.3999999999996, "end": 3772.7999999999997, "text": " And as it turns out that's actually pretty easy to do.", "tokens": [51674, 400, 382, 309, 4523, 484, 300, 311, 767, 1238, 1858, 281, 360, 13, 51844], "temperature": 0.0, "avg_logprob": -0.23207222412679798, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00017130696505773813}, {"id": 773, "seek": 377280, "start": 3772.8, "end": 3790.36, "text": " You just have to add a extra argument to your data loaders, which is here num underscore", "tokens": [50364, 509, 445, 362, 281, 909, 257, 2857, 6770, 281, 428, 1412, 3677, 433, 11, 597, 307, 510, 1031, 37556, 51242], "temperature": 0.0, "avg_logprob": -0.30652204373987707, "compression_ratio": 1.1932773109243697, "no_speech_prob": 0.0005703131901100278}, {"id": 774, "seek": 377280, "start": 3790.36, "end": 3794.32, "text": " workers.", "tokens": [51242, 5600, 13, 51440], "temperature": 0.0, "avg_logprob": -0.30652204373987707, "compression_ratio": 1.1932773109243697, "no_speech_prob": 0.0005703131901100278}, {"id": 775, "seek": 377280, "start": 3794.32, "end": 3798.5600000000004, "text": " And so I can say use eight CPUs for example.", "tokens": [51440, 400, 370, 286, 393, 584, 764, 3180, 13199, 82, 337, 1365, 13, 51652], "temperature": 0.0, "avg_logprob": -0.30652204373987707, "compression_ratio": 1.1932773109243697, "no_speech_prob": 0.0005703131901100278}, {"id": 776, "seek": 379856, "start": 3798.56, "end": 3804.4, "text": " Now if I recreate the data loaders and then try to get the next one, uh oh, now I've got", "tokens": [50364, 823, 498, 286, 25833, 264, 1412, 3677, 433, 293, 550, 853, 281, 483, 264, 958, 472, 11, 2232, 1954, 11, 586, 286, 600, 658, 50656], "temperature": 0.0, "avg_logprob": -0.31472242446172805, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.036217138171195984}, {"id": 777, "seek": 379856, "start": 3804.4, "end": 3805.64, "text": " an error.", "tokens": [50656, 364, 6713, 13, 50718], "temperature": 0.0, "avg_logprob": -0.31472242446172805, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.036217138171195984}, {"id": 778, "seek": 379856, "start": 3805.64, "end": 3811.92, "text": " And the error is rather quirky.", "tokens": [50718, 400, 264, 6713, 307, 2831, 49515, 13, 51032], "temperature": 0.0, "avg_logprob": -0.31472242446172805, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.036217138171195984}, {"id": 779, "seek": 379856, "start": 3811.92, "end": 3816.48, "text": " And what it's saying is, oh, you're now trying to use multiple processors.", "tokens": [51032, 400, 437, 309, 311, 1566, 307, 11, 1954, 11, 291, 434, 586, 1382, 281, 764, 3866, 27751, 13, 51260], "temperature": 0.0, "avg_logprob": -0.31472242446172805, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.036217138171195984}, {"id": 780, "seek": 379856, "start": 3816.48, "end": 3821.7599999999998, "text": " And generally in Python and in PyTorch, using multiple processors things start to get complicated.", "tokens": [51260, 400, 5101, 294, 15329, 293, 294, 9953, 51, 284, 339, 11, 1228, 3866, 27751, 721, 722, 281, 483, 6179, 13, 51524], "temperature": 0.0, "avg_logprob": -0.31472242446172805, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.036217138171195984}, {"id": 781, "seek": 382176, "start": 3821.76, "end": 3829.0800000000004, "text": " And one of the things that absolutely just doesn't work is you can't actually have your", "tokens": [50364, 400, 472, 295, 264, 721, 300, 3122, 445, 1177, 380, 589, 307, 291, 393, 380, 767, 362, 428, 50730], "temperature": 0.0, "avg_logprob": -0.25131815592447915, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.0009253682219423354}, {"id": 782, "seek": 382176, "start": 3829.0800000000004, "end": 3836.96, "text": " data loader put things onto the GPU in your separate processors.", "tokens": [50730, 1412, 3677, 260, 829, 721, 3911, 264, 18407, 294, 428, 4994, 27751, 13, 51124], "temperature": 0.0, "avg_logprob": -0.25131815592447915, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.0009253682219423354}, {"id": 783, "seek": 382176, "start": 3836.96, "end": 3838.0400000000004, "text": " It just doesn't work.", "tokens": [51124, 467, 445, 1177, 380, 589, 13, 51178], "temperature": 0.0, "avg_logprob": -0.25131815592447915, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.0009253682219423354}, {"id": 784, "seek": 382176, "start": 3838.0400000000004, "end": 3847.86, "text": " So the reason for this error is actually because of the fact that we used a collate function", "tokens": [51178, 407, 264, 1778, 337, 341, 6713, 307, 767, 570, 295, 264, 1186, 300, 321, 1143, 257, 1263, 473, 2445, 51669], "temperature": 0.0, "avg_logprob": -0.25131815592447915, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.0009253682219423354}, {"id": 785, "seek": 382176, "start": 3847.86, "end": 3849.2400000000002, "text": " that put things on the device.", "tokens": [51669, 300, 829, 721, 322, 264, 4302, 13, 51738], "temperature": 0.0, "avg_logprob": -0.25131815592447915, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.0009253682219423354}, {"id": 786, "seek": 384924, "start": 3849.24, "end": 3856.3199999999997, "text": " That's incompatible, unfortunately, with using multiple workers.", "tokens": [50364, 663, 311, 40393, 267, 964, 11, 7015, 11, 365, 1228, 3866, 5600, 13, 50718], "temperature": 0.0, "avg_logprob": -0.2635553703933466, "compression_ratio": 1.44375, "no_speech_prob": 0.0007437022286467254}, {"id": 787, "seek": 384924, "start": 3856.3199999999997, "end": 3861.52, "text": " So that's a problem.", "tokens": [50718, 407, 300, 311, 257, 1154, 13, 50978], "temperature": 0.0, "avg_logprob": -0.2635553703933466, "compression_ratio": 1.44375, "no_speech_prob": 0.0007437022286467254}, {"id": 788, "seek": 384924, "start": 3861.52, "end": 3870.72, "text": " And the answer to that problem, sadly, is that we would have to actually rewrite our", "tokens": [50978, 400, 264, 1867, 281, 300, 1154, 11, 22023, 11, 307, 300, 321, 576, 362, 281, 767, 28132, 527, 51438], "temperature": 0.0, "avg_logprob": -0.2635553703933466, "compression_ratio": 1.44375, "no_speech_prob": 0.0007437022286467254}, {"id": 789, "seek": 384924, "start": 3870.72, "end": 3876.3999999999996, "text": " fit function entirely.", "tokens": [51438, 3318, 2445, 7696, 13, 51722], "temperature": 0.0, "avg_logprob": -0.2635553703933466, "compression_ratio": 1.44375, "no_speech_prob": 0.0007437022286467254}, {"id": 790, "seek": 384924, "start": 3876.3999999999996, "end": 3878.3999999999996, "text": " So there's annoying thing number one.", "tokens": [51722, 407, 456, 311, 11304, 551, 1230, 472, 13, 51822], "temperature": 0.0, "avg_logprob": -0.2635553703933466, "compression_ratio": 1.44375, "no_speech_prob": 0.0007437022286467254}, {"id": 791, "seek": 387840, "start": 3878.44, "end": 3881.92, "text": " We don't want to be rewriting our fit function again and again.", "tokens": [50366, 492, 500, 380, 528, 281, 312, 319, 19868, 527, 3318, 2445, 797, 293, 797, 13, 50540], "temperature": 0.0, "avg_logprob": -0.28316185691139917, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0007208369206637144}, {"id": 792, "seek": 387840, "start": 3881.92, "end": 3884.1600000000003, "text": " We want to have a single fit function.", "tokens": [50540, 492, 528, 281, 362, 257, 2167, 3318, 2445, 13, 50652], "temperature": 0.0, "avg_logprob": -0.28316185691139917, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0007208369206637144}, {"id": 793, "seek": 387840, "start": 3884.1600000000003, "end": 3890.7200000000003, "text": " So okay, so there's a problem that we're going to have to think about.", "tokens": [50652, 407, 1392, 11, 370, 456, 311, 257, 1154, 300, 321, 434, 516, 281, 362, 281, 519, 466, 13, 50980], "temperature": 0.0, "avg_logprob": -0.28316185691139917, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0007208369206637144}, {"id": 794, "seek": 387840, "start": 3890.7200000000003, "end": 3895.96, "text": " Problem number two is that this is not very accurate.", "tokens": [50980, 11676, 1230, 732, 307, 300, 341, 307, 406, 588, 8559, 13, 51242], "temperature": 0.0, "avg_logprob": -0.28316185691139917, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0007208369206637144}, {"id": 795, "seek": 387840, "start": 3895.96, "end": 3896.96, "text": " 87%.", "tokens": [51242, 27990, 6856, 51292], "temperature": 0.0, "avg_logprob": -0.28316185691139917, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0007208369206637144}, {"id": 796, "seek": 387840, "start": 3896.96, "end": 3899.76, "text": " Well, I mean, is it accurate?", "tokens": [51292, 1042, 11, 286, 914, 11, 307, 309, 8559, 30, 51432], "temperature": 0.0, "avg_logprob": -0.28316185691139917, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0007208369206637144}, {"id": 797, "seek": 387840, "start": 3899.76, "end": 3901.04, "text": " It's easy enough to find out.", "tokens": [51432, 467, 311, 1858, 1547, 281, 915, 484, 13, 51496], "temperature": 0.0, "avg_logprob": -0.28316185691139917, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0007208369206637144}, {"id": 798, "seek": 390104, "start": 3901.04, "end": 3908.88, "text": " There's a really nice website called Papers with Code.", "tokens": [50364, 821, 311, 257, 534, 1481, 3144, 1219, 430, 14441, 365, 15549, 13, 50756], "temperature": 0.0, "avg_logprob": -0.2949236234029134, "compression_ratio": 1.3893129770992367, "no_speech_prob": 0.07368889451026917}, {"id": 799, "seek": 390104, "start": 3908.88, "end": 3916.84, "text": " And it will tell you a little leaderboard.", "tokens": [50756, 400, 309, 486, 980, 291, 257, 707, 5263, 3787, 13, 51154], "temperature": 0.0, "avg_logprob": -0.2949236234029134, "compression_ratio": 1.3893129770992367, "no_speech_prob": 0.07368889451026917}, {"id": 800, "seek": 390104, "start": 3916.84, "end": 3919.72, "text": " And we can see whether we're any good.", "tokens": [51154, 400, 321, 393, 536, 1968, 321, 434, 604, 665, 13, 51298], "temperature": 0.0, "avg_logprob": -0.2949236234029134, "compression_ratio": 1.3893129770992367, "no_speech_prob": 0.07368889451026917}, {"id": 801, "seek": 390104, "start": 3919.72, "end": 3923.6, "text": " And the answer is we're not very good at all.", "tokens": [51298, 400, 264, 1867, 307, 321, 434, 406, 588, 665, 412, 439, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2949236234029134, "compression_ratio": 1.3893129770992367, "no_speech_prob": 0.07368889451026917}, {"id": 802, "seek": 392360, "start": 3923.6, "end": 3932.16, "text": " So these papers had 96%, 94%, 92%.", "tokens": [50364, 407, 613, 10577, 632, 24124, 8923, 30849, 8923, 28225, 6856, 50792], "temperature": 0.0, "avg_logprob": -0.2248077051980155, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.003429558826610446}, {"id": 803, "seek": 392360, "start": 3932.16, "end": 3937.7999999999997, "text": " So yeah, we're not looking great.", "tokens": [50792, 407, 1338, 11, 321, 434, 406, 1237, 869, 13, 51074], "temperature": 0.0, "avg_logprob": -0.2248077051980155, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.003429558826610446}, {"id": 804, "seek": 392360, "start": 3937.7999999999997, "end": 3942.92, "text": " So how do we improve that?", "tokens": [51074, 407, 577, 360, 321, 3470, 300, 30, 51330], "temperature": 0.0, "avg_logprob": -0.2248077051980155, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.003429558826610446}, {"id": 805, "seek": 392360, "start": 3942.92, "end": 3948.12, "text": " There's a lot of things we could try, but pretty much all of them are going to involve", "tokens": [51330, 821, 311, 257, 688, 295, 721, 321, 727, 853, 11, 457, 1238, 709, 439, 295, 552, 366, 516, 281, 9494, 51590], "temperature": 0.0, "avg_logprob": -0.2248077051980155, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.003429558826610446}, {"id": 806, "seek": 394812, "start": 3948.12, "end": 3954.0, "text": " modifying our fit function again, and in reasonably complicated ways.", "tokens": [50364, 42626, 527, 3318, 2445, 797, 11, 293, 294, 23551, 6179, 2098, 13, 50658], "temperature": 0.0, "avg_logprob": -0.20713891528901598, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.011507753282785416}, {"id": 807, "seek": 394812, "start": 3954.0, "end": 3956.6, "text": " So we've still got a bit of an issue there.", "tokens": [50658, 407, 321, 600, 920, 658, 257, 857, 295, 364, 2734, 456, 13, 50788], "temperature": 0.0, "avg_logprob": -0.20713891528901598, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.011507753282785416}, {"id": 808, "seek": 394812, "start": 3956.6, "end": 3962.7599999999998, "text": " Let's put that aside, because what we actually wanted to do is create an autoencoder.", "tokens": [50788, 961, 311, 829, 300, 7359, 11, 570, 437, 321, 767, 1415, 281, 360, 307, 1884, 364, 8399, 22660, 19866, 13, 51096], "temperature": 0.0, "avg_logprob": -0.20713891528901598, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.011507753282785416}, {"id": 809, "seek": 394812, "start": 3962.7599999999998, "end": 3970.56, "text": " So to remind you about what an autoencoder is, and we're going to be able to go into", "tokens": [51096, 407, 281, 4160, 291, 466, 437, 364, 8399, 22660, 19866, 307, 11, 293, 321, 434, 516, 281, 312, 1075, 281, 352, 666, 51486], "temperature": 0.0, "avg_logprob": -0.20713891528901598, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.011507753282785416}, {"id": 810, "seek": 394812, "start": 3970.56, "end": 3975.48, "text": " a bit more detail now, we're going to start with our input image, which is going to be", "tokens": [51486, 257, 857, 544, 2607, 586, 11, 321, 434, 516, 281, 722, 365, 527, 4846, 3256, 11, 597, 307, 516, 281, 312, 51732], "temperature": 0.0, "avg_logprob": -0.20713891528901598, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.011507753282785416}, {"id": 811, "seek": 394812, "start": 3975.48, "end": 3977.2, "text": " 28 by 28.", "tokens": [51732, 7562, 538, 7562, 13, 51818], "temperature": 0.0, "avg_logprob": -0.20713891528901598, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.011507753282785416}, {"id": 812, "seek": 397720, "start": 3977.2799999999997, "end": 3978.68, "text": " It's a number 3, right?", "tokens": [50368, 467, 311, 257, 1230, 805, 11, 558, 30, 50438], "temperature": 0.0, "avg_logprob": -0.30648663169459295, "compression_ratio": 1.4709302325581395, "no_speech_prob": 0.0011159783462062478}, {"id": 813, "seek": 397720, "start": 3978.68, "end": 3982.96, "text": " And it's a 28 by 28.", "tokens": [50438, 400, 309, 311, 257, 7562, 538, 7562, 13, 50652], "temperature": 0.0, "avg_logprob": -0.30648663169459295, "compression_ratio": 1.4709302325581395, "no_speech_prob": 0.0011159783462062478}, {"id": 814, "seek": 397720, "start": 3982.96, "end": 3990.8799999999997, "text": " And we're going to put it through, for example, a stride2.conv.", "tokens": [50652, 400, 321, 434, 516, 281, 829, 309, 807, 11, 337, 1365, 11, 257, 1056, 482, 17, 13, 1671, 85, 13, 51048], "temperature": 0.0, "avg_logprob": -0.30648663169459295, "compression_ratio": 1.4709302325581395, "no_speech_prob": 0.0011159783462062478}, {"id": 815, "seek": 397720, "start": 3990.8799999999997, "end": 3997.3999999999996, "text": " And that's going to have an output of a 14 by 14.", "tokens": [51048, 400, 300, 311, 516, 281, 362, 364, 5598, 295, 257, 3499, 538, 3499, 13, 51374], "temperature": 0.0, "avg_logprob": -0.30648663169459295, "compression_ratio": 1.4709302325581395, "no_speech_prob": 0.0011159783462062478}, {"id": 816, "seek": 397720, "start": 3997.3999999999996, "end": 3999.68, "text": " And we can have more channels.", "tokens": [51374, 400, 321, 393, 362, 544, 9235, 13, 51488], "temperature": 0.0, "avg_logprob": -0.30648663169459295, "compression_ratio": 1.4709302325581395, "no_speech_prob": 0.0011159783462062478}, {"id": 817, "seek": 397720, "start": 3999.68, "end": 4003.3599999999997, "text": " So say maybe, so this is 28 by 28 by 1.", "tokens": [51488, 407, 584, 1310, 11, 370, 341, 307, 7562, 538, 7562, 538, 502, 13, 51672], "temperature": 0.0, "avg_logprob": -0.30648663169459295, "compression_ratio": 1.4709302325581395, "no_speech_prob": 0.0011159783462062478}, {"id": 818, "seek": 397720, "start": 4003.3599999999997, "end": 4006.52, "text": " Let's do 14 by 14 by 2.", "tokens": [51672, 961, 311, 360, 3499, 538, 3499, 538, 568, 13, 51830], "temperature": 0.0, "avg_logprob": -0.30648663169459295, "compression_ratio": 1.4709302325581395, "no_speech_prob": 0.0011159783462062478}, {"id": 819, "seek": 400652, "start": 4006.52, "end": 4011.28, "text": " So we've reduced the height and width by 2, but added an extra channel.", "tokens": [50364, 407, 321, 600, 9212, 264, 6681, 293, 11402, 538, 568, 11, 457, 3869, 364, 2857, 2269, 13, 50602], "temperature": 0.0, "avg_logprob": -0.18375261306762694, "compression_ratio": 1.5422222222222222, "no_speech_prob": 6.502804171759635e-05}, {"id": 820, "seek": 400652, "start": 4011.28, "end": 4016.88, "text": " So overall, this is a 2x decrease in parameters.", "tokens": [50602, 407, 4787, 11, 341, 307, 257, 568, 87, 11514, 294, 9834, 13, 50882], "temperature": 0.0, "avg_logprob": -0.18375261306762694, "compression_ratio": 1.5422222222222222, "no_speech_prob": 6.502804171759635e-05}, {"id": 821, "seek": 400652, "start": 4016.88, "end": 4024.08, "text": " And then we could do another stride2.conv, and that would give us a 7 by 7.", "tokens": [50882, 400, 550, 321, 727, 360, 1071, 1056, 482, 17, 13, 1671, 85, 11, 293, 300, 576, 976, 505, 257, 1614, 538, 1614, 13, 51242], "temperature": 0.0, "avg_logprob": -0.18375261306762694, "compression_ratio": 1.5422222222222222, "no_speech_prob": 6.502804171759635e-05}, {"id": 822, "seek": 400652, "start": 4024.08, "end": 4028.6, "text": " And again, we can choose however many channels we want, but let's say we choose 4.", "tokens": [51242, 400, 797, 11, 321, 393, 2826, 4461, 867, 9235, 321, 528, 11, 457, 718, 311, 584, 321, 2826, 1017, 13, 51468], "temperature": 0.0, "avg_logprob": -0.18375261306762694, "compression_ratio": 1.5422222222222222, "no_speech_prob": 6.502804171759635e-05}, {"id": 823, "seek": 400652, "start": 4028.6, "end": 4034.86, "text": " So now compared to our original, we've now got a times 4 reduction.", "tokens": [51468, 407, 586, 5347, 281, 527, 3380, 11, 321, 600, 586, 658, 257, 1413, 1017, 11004, 13, 51781], "temperature": 0.0, "avg_logprob": -0.18375261306762694, "compression_ratio": 1.5422222222222222, "no_speech_prob": 6.502804171759635e-05}, {"id": 824, "seek": 403486, "start": 4034.86, "end": 4038.6200000000003, "text": " And so we could do that a few times, or we could just stay there.", "tokens": [50364, 400, 370, 321, 727, 360, 300, 257, 1326, 1413, 11, 420, 321, 727, 445, 1754, 456, 13, 50552], "temperature": 0.0, "avg_logprob": -0.2155848890542984, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0002453677880112082}, {"id": 825, "seek": 403486, "start": 4038.6200000000003, "end": 4045.7400000000002, "text": " And so this is compressing.", "tokens": [50552, 400, 370, 341, 307, 14778, 278, 13, 50908], "temperature": 0.0, "avg_logprob": -0.2155848890542984, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0002453677880112082}, {"id": 826, "seek": 403486, "start": 4045.7400000000002, "end": 4054.7400000000002, "text": " And so then what we could do is then somehow have a convolution layer, or group of layers,", "tokens": [50908, 400, 370, 550, 437, 321, 727, 360, 307, 550, 6063, 362, 257, 45216, 4583, 11, 420, 1594, 295, 7914, 11, 51358], "temperature": 0.0, "avg_logprob": -0.2155848890542984, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0002453677880112082}, {"id": 827, "seek": 403486, "start": 4054.7400000000002, "end": 4061.46, "text": " which does a convolution and also increases the size.", "tokens": [51358, 597, 775, 257, 45216, 293, 611, 8637, 264, 2744, 13, 51694], "temperature": 0.0, "avg_logprob": -0.2155848890542984, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0002453677880112082}, {"id": 828, "seek": 406146, "start": 4061.56, "end": 4069.06, "text": " There is actually something called a transposed convolution, which I'll leave you to look", "tokens": [50369, 821, 307, 767, 746, 1219, 257, 7132, 1744, 45216, 11, 597, 286, 603, 1856, 291, 281, 574, 50744], "temperature": 0.0, "avg_logprob": -0.2516968363807315, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0003150318516418338}, {"id": 829, "seek": 406146, "start": 4069.06, "end": 4073.2200000000003, "text": " up if you're interested, which can do that.", "tokens": [50744, 493, 498, 291, 434, 3102, 11, 597, 393, 360, 300, 13, 50952], "temperature": 0.0, "avg_logprob": -0.2516968363807315, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0003150318516418338}, {"id": 830, "seek": 406146, "start": 4073.2200000000003, "end": 4079.46, "text": " Also known as a, rather weirdly, a stride1.5 convolution.", "tokens": [50952, 2743, 2570, 382, 257, 11, 2831, 48931, 11, 257, 1056, 482, 16, 13, 20, 45216, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2516968363807315, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0003150318516418338}, {"id": 831, "seek": 406146, "start": 4079.46, "end": 4083.62, "text": " But there's actually a really simple way to do this, which is to say, let's say you've", "tokens": [51264, 583, 456, 311, 767, 257, 534, 2199, 636, 281, 360, 341, 11, 597, 307, 281, 584, 11, 718, 311, 584, 291, 600, 51472], "temperature": 0.0, "avg_logprob": -0.2516968363807315, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0003150318516418338}, {"id": 832, "seek": 406146, "start": 4083.62, "end": 4084.62, "text": " got a bunch of pixels.", "tokens": [51472, 658, 257, 3840, 295, 18668, 13, 51522], "temperature": 0.0, "avg_logprob": -0.2516968363807315, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0003150318516418338}, {"id": 833, "seek": 408462, "start": 4084.62, "end": 4091.02, "text": " Let's say we've got a 3 by 3 pixels that looks like this.", "tokens": [50364, 961, 311, 584, 321, 600, 658, 257, 805, 538, 805, 18668, 300, 1542, 411, 341, 13, 50684], "temperature": 0.0, "avg_logprob": -0.38594528834025066, "compression_ratio": 1.2846153846153847, "no_speech_prob": 0.008577023632824421}, {"id": 834, "seek": 408462, "start": 4091.02, "end": 4094.7, "text": " 1, 0, 1, 1, say.", "tokens": [50684, 502, 11, 1958, 11, 502, 11, 502, 11, 584, 13, 50868], "temperature": 0.0, "avg_logprob": -0.38594528834025066, "compression_ratio": 1.2846153846153847, "no_speech_prob": 0.008577023632824421}, {"id": 835, "seek": 408462, "start": 4094.7, "end": 4107.74, "text": " We could make that into a 6 by 6 very easily, which is we could simply, let's get these", "tokens": [50868, 492, 727, 652, 300, 666, 257, 1386, 538, 1386, 588, 3612, 11, 597, 307, 321, 727, 2935, 11, 718, 311, 483, 613, 51520], "temperature": 0.0, "avg_logprob": -0.38594528834025066, "compression_ratio": 1.2846153846153847, "no_speech_prob": 0.008577023632824421}, {"id": 836, "seek": 408462, "start": 4107.74, "end": 4109.0199999999995, "text": " out.", "tokens": [51520, 484, 13, 51584], "temperature": 0.0, "avg_logprob": -0.38594528834025066, "compression_ratio": 1.2846153846153847, "no_speech_prob": 0.008577023632824421}, {"id": 837, "seek": 410902, "start": 4109.02, "end": 4115.160000000001, "text": " We could simply copy that pixel there into the first 4.", "tokens": [50364, 492, 727, 2935, 5055, 300, 19261, 456, 666, 264, 700, 1017, 13, 50671], "temperature": 0.0, "avg_logprob": -0.23966762937348465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0009253764874301851}, {"id": 838, "seek": 410902, "start": 4115.160000000001, "end": 4118.96, "text": " Copy that pixel there into these 4.", "tokens": [50671, 25653, 300, 19261, 456, 666, 613, 1017, 13, 50861], "temperature": 0.0, "avg_logprob": -0.23966762937348465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0009253764874301851}, {"id": 839, "seek": 410902, "start": 4118.96, "end": 4123.580000000001, "text": " And so you can see, and then copy this pixel here into these 4.", "tokens": [50861, 400, 370, 291, 393, 536, 11, 293, 550, 5055, 341, 19261, 510, 666, 613, 1017, 13, 51092], "temperature": 0.0, "avg_logprob": -0.23966762937348465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0009253764874301851}, {"id": 840, "seek": 410902, "start": 4123.580000000001, "end": 4129.700000000001, "text": " And so we're simply turning each pixel into 4 pixels.", "tokens": [51092, 400, 370, 321, 434, 2935, 6246, 1184, 19261, 666, 1017, 18668, 13, 51398], "temperature": 0.0, "avg_logprob": -0.23966762937348465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0009253764874301851}, {"id": 841, "seek": 412970, "start": 4129.7, "end": 4141.22, "text": " And so this is called nearest neighbor upsampling.", "tokens": [50364, 400, 370, 341, 307, 1219, 23831, 5987, 15497, 335, 11970, 13, 50940], "temperature": 0.0, "avg_logprob": -0.2650704528346206, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.0018101874738931656}, {"id": 842, "seek": 412970, "start": 4141.22, "end": 4144.5, "text": " Now that's not a convolution, that's just copying.", "tokens": [50940, 823, 300, 311, 406, 257, 45216, 11, 300, 311, 445, 27976, 13, 51104], "temperature": 0.0, "avg_logprob": -0.2650704528346206, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.0018101874738931656}, {"id": 843, "seek": 412970, "start": 4144.5, "end": 4153.34, "text": " But what we could then do is we could then apply a stride1 convolution to that.", "tokens": [51104, 583, 437, 321, 727, 550, 360, 307, 321, 727, 550, 3079, 257, 1056, 482, 16, 45216, 281, 300, 13, 51546], "temperature": 0.0, "avg_logprob": -0.2650704528346206, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.0018101874738931656}, {"id": 844, "seek": 412970, "start": 4153.34, "end": 4158.46, "text": " And that would allow us to double the grid size with a convolution.", "tokens": [51546, 400, 300, 576, 2089, 505, 281, 3834, 264, 10748, 2744, 365, 257, 45216, 13, 51802], "temperature": 0.0, "avg_logprob": -0.2650704528346206, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.0018101874738931656}, {"id": 845, "seek": 415846, "start": 4158.78, "end": 4161.06, "text": " And that's what we're going to do.", "tokens": [50380, 400, 300, 311, 437, 321, 434, 516, 281, 360, 13, 50494], "temperature": 0.0, "avg_logprob": -0.2743646654032044, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0053848871029913425}, {"id": 846, "seek": 415846, "start": 4161.06, "end": 4166.3, "text": " So our autoencoder is going to need a deconvolutional layer.", "tokens": [50494, 407, 527, 8399, 22660, 19866, 307, 516, 281, 643, 257, 979, 266, 85, 3386, 304, 4583, 13, 50756], "temperature": 0.0, "avg_logprob": -0.2743646654032044, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0053848871029913425}, {"id": 847, "seek": 415846, "start": 4166.3, "end": 4169.62, "text": " And that's going to contain two layers.", "tokens": [50756, 400, 300, 311, 516, 281, 5304, 732, 7914, 13, 50922], "temperature": 0.0, "avg_logprob": -0.2743646654032044, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0053848871029913425}, {"id": 848, "seek": 415846, "start": 4169.62, "end": 4175.58, "text": " Upsampling nearest neighbor, scale factor of 2, followed by a conv2d with a stride of", "tokens": [50922, 624, 1878, 335, 11970, 23831, 5987, 11, 4373, 5952, 295, 568, 11, 6263, 538, 257, 3754, 17, 67, 365, 257, 1056, 482, 295, 51220], "temperature": 0.0, "avg_logprob": -0.2743646654032044, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0053848871029913425}, {"id": 849, "seek": 415846, "start": 4175.58, "end": 4176.58, "text": " 1.", "tokens": [51220, 502, 13, 51270], "temperature": 0.0, "avg_logprob": -0.2743646654032044, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0053848871029913425}, {"id": 850, "seek": 415846, "start": 4176.58, "end": 4177.58, "text": " OK.", "tokens": [51270, 2264, 13, 51320], "temperature": 0.0, "avg_logprob": -0.2743646654032044, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0053848871029913425}, {"id": 851, "seek": 415846, "start": 4177.58, "end": 4182.38, "text": " And you can see for padding, I just put kernel size slash slash 2.", "tokens": [51320, 400, 291, 393, 536, 337, 39562, 11, 286, 445, 829, 28256, 2744, 17330, 17330, 568, 13, 51560], "temperature": 0.0, "avg_logprob": -0.2743646654032044, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0053848871029913425}, {"id": 852, "seek": 415846, "start": 4182.38, "end": 4188.36, "text": " So that's a truncating division, because that always works for any odd-sized kernel.", "tokens": [51560, 407, 300, 311, 257, 504, 409, 66, 990, 10044, 11, 570, 300, 1009, 1985, 337, 604, 7401, 12, 20614, 28256, 13, 51859], "temperature": 0.0, "avg_logprob": -0.2743646654032044, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0053848871029913425}, {"id": 853, "seek": 418836, "start": 4189.259999999999, "end": 4193.12, "text": " So as before, we will have an optional activation function.", "tokens": [50409, 407, 382, 949, 11, 321, 486, 362, 364, 17312, 24433, 2445, 13, 50602], "temperature": 0.0, "avg_logprob": -0.3705076687577842, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.00023050638264976442}, {"id": 854, "seek": 418836, "start": 4193.12, "end": 4196.96, "text": " And then we will create a sequential using star layers.", "tokens": [50602, 400, 550, 321, 486, 1884, 257, 42881, 1228, 3543, 7914, 13, 50794], "temperature": 0.0, "avg_logprob": -0.3705076687577842, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.00023050638264976442}, {"id": 855, "seek": 418836, "start": 4196.96, "end": 4200.4, "text": " So that's going to pass in each layer as a separate argument, which is what sequential", "tokens": [50794, 407, 300, 311, 516, 281, 1320, 294, 1184, 4583, 382, 257, 4994, 6770, 11, 597, 307, 437, 42881, 50966], "temperature": 0.0, "avg_logprob": -0.3705076687577842, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.00023050638264976442}, {"id": 856, "seek": 418836, "start": 4200.4, "end": 4203.88, "text": " expects.", "tokens": [50966, 33280, 13, 51140], "temperature": 0.0, "avg_logprob": -0.3705076687577842, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.00023050638264976442}, {"id": 857, "seek": 418836, "start": 4203.88, "end": 4207.48, "text": " OK.", "tokens": [51140, 2264, 13, 51320], "temperature": 0.0, "avg_logprob": -0.3705076687577842, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.00023050638264976442}, {"id": 858, "seek": 418836, "start": 4207.48, "end": 4214.08, "text": " So, let's write a new fitness function.", "tokens": [51320, 407, 11, 718, 311, 2464, 257, 777, 15303, 2445, 13, 51650], "temperature": 0.0, "avg_logprob": -0.3705076687577842, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.00023050638264976442}, {"id": 859, "seek": 421408, "start": 4215.08, "end": 4221.2, "text": " I just basically copied it over from our previous one, going through each epoch.", "tokens": [50414, 286, 445, 1936, 25365, 309, 670, 490, 527, 3894, 472, 11, 516, 807, 1184, 30992, 339, 13, 50720], "temperature": 0.0, "avg_logprob": -0.35350472586495535, "compression_ratio": 1.3708609271523178, "no_speech_prob": 0.00013341948215384036}, {"id": 860, "seek": 421408, "start": 4221.2, "end": 4225.2, "text": " But I've pulled out a vowel into a separate function.", "tokens": [50720, 583, 286, 600, 7373, 484, 257, 220, 85, 305, 338, 666, 257, 4994, 2445, 13, 50920], "temperature": 0.0, "avg_logprob": -0.35350472586495535, "compression_ratio": 1.3708609271523178, "no_speech_prob": 0.00013341948215384036}, {"id": 861, "seek": 421408, "start": 4225.2, "end": 4230.48, "text": " But it's basically doing the same thing.", "tokens": [50920, 583, 309, 311, 1936, 884, 264, 912, 551, 13, 51184], "temperature": 0.0, "avg_logprob": -0.35350472586495535, "compression_ratio": 1.3708609271523178, "no_speech_prob": 0.00013341948215384036}, {"id": 862, "seek": 421408, "start": 4230.48, "end": 4232.84, "text": " OK.", "tokens": [51184, 2264, 13, 51302], "temperature": 0.0, "avg_logprob": -0.35350472586495535, "compression_ratio": 1.3708609271523178, "no_speech_prob": 0.00013341948215384036}, {"id": 863, "seek": 421408, "start": 4232.84, "end": 4241.36, "text": " So here is our autoencoder.", "tokens": [51302, 407, 510, 307, 527, 8399, 22660, 19866, 13, 51728], "temperature": 0.0, "avg_logprob": -0.35350472586495535, "compression_ratio": 1.3708609271523178, "no_speech_prob": 0.00013341948215384036}, {"id": 864, "seek": 424136, "start": 4241.44, "end": 4245.679999999999, "text": " And so we're going to...", "tokens": [50368, 400, 370, 321, 434, 516, 281, 485, 50580], "temperature": 0.0, "avg_logprob": -0.21906890449943123, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.00022693285427521914}, {"id": 865, "seek": 424136, "start": 4245.679999999999, "end": 4256.24, "text": " It's a bit tricky, because I wanted to go down by 1, 2, 3, to get to a 4 by 4 by 8.", "tokens": [50580, 467, 311, 257, 857, 12414, 11, 570, 286, 1415, 281, 352, 760, 538, 502, 11, 568, 11, 805, 11, 281, 483, 281, 257, 1017, 538, 1017, 538, 1649, 13, 51108], "temperature": 0.0, "avg_logprob": -0.21906890449943123, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.00022693285427521914}, {"id": 866, "seek": 424136, "start": 4256.24, "end": 4261.92, "text": " But starting at 28 by 28, you can't divide that three times and get an integer.", "tokens": [51108, 583, 2891, 412, 7562, 538, 7562, 11, 291, 393, 380, 9845, 300, 1045, 1413, 293, 483, 364, 24922, 13, 51392], "temperature": 0.0, "avg_logprob": -0.21906890449943123, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.00022693285427521914}, {"id": 867, "seek": 424136, "start": 4261.92, "end": 4269.0599999999995, "text": " So what I first do is I zero pad, so add padding of 2 on each side to get a 32 by 32 input.", "tokens": [51392, 407, 437, 286, 700, 360, 307, 286, 4018, 6887, 11, 370, 909, 39562, 295, 568, 322, 1184, 1252, 281, 483, 257, 8858, 538, 8858, 4846, 13, 51749], "temperature": 0.0, "avg_logprob": -0.21906890449943123, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.00022693285427521914}, {"id": 868, "seek": 426906, "start": 4269.06, "end": 4274.740000000001, "text": " So if I then do a conv with two channel output, that gives us 16 by 16 by 2.", "tokens": [50364, 407, 498, 286, 550, 360, 257, 3754, 365, 732, 2269, 5598, 11, 300, 2709, 505, 3165, 538, 3165, 538, 568, 13, 50648], "temperature": 0.0, "avg_logprob": -0.23904200756188596, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0004728515923488885}, {"id": 869, "seek": 426906, "start": 4274.740000000001, "end": 4277.38, "text": " And then again to get an 8 by 8 by 4.", "tokens": [50648, 400, 550, 797, 281, 483, 364, 1649, 538, 1649, 538, 1017, 13, 50780], "temperature": 0.0, "avg_logprob": -0.23904200756188596, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0004728515923488885}, {"id": 870, "seek": 426906, "start": 4277.38, "end": 4279.5, "text": " And then again to get a 4 by 4 by 8.", "tokens": [50780, 400, 550, 797, 281, 483, 257, 1017, 538, 1017, 538, 1649, 13, 50886], "temperature": 0.0, "avg_logprob": -0.23904200756188596, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0004728515923488885}, {"id": 871, "seek": 426906, "start": 4279.5, "end": 4281.9400000000005, "text": " So this is doing an 8x compression.", "tokens": [50886, 407, 341, 307, 884, 364, 1649, 87, 19355, 13, 51008], "temperature": 0.0, "avg_logprob": -0.23904200756188596, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0004728515923488885}, {"id": 872, "seek": 426906, "start": 4281.9400000000005, "end": 4286.42, "text": " And then we can call deconv to do exactly the same thing in reverse.", "tokens": [51008, 400, 550, 321, 393, 818, 979, 266, 85, 281, 360, 2293, 264, 912, 551, 294, 9943, 13, 51232], "temperature": 0.0, "avg_logprob": -0.23904200756188596, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0004728515923488885}, {"id": 873, "seek": 426906, "start": 4286.42, "end": 4288.54, "text": " The final one with no activation.", "tokens": [51232, 440, 2572, 472, 365, 572, 24433, 13, 51338], "temperature": 0.0, "avg_logprob": -0.23904200756188596, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0004728515923488885}, {"id": 874, "seek": 426906, "start": 4288.54, "end": 4292.22, "text": " And then we can truncate off those two pixels off the edge.", "tokens": [51338, 400, 550, 321, 393, 504, 409, 66, 473, 766, 729, 732, 18668, 766, 264, 4691, 13, 51522], "temperature": 0.0, "avg_logprob": -0.23904200756188596, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0004728515923488885}, {"id": 875, "seek": 426906, "start": 4292.22, "end": 4298.06, "text": " Slightly surprisingly, PyTorch lets you pass negative 2 to zero padding to crop off the", "tokens": [51522, 318, 44872, 17600, 11, 9953, 51, 284, 339, 6653, 291, 1320, 3671, 568, 281, 4018, 39562, 281, 9086, 766, 264, 51814], "temperature": 0.0, "avg_logprob": -0.23904200756188596, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0004728515923488885}, {"id": 876, "seek": 429806, "start": 4298.06, "end": 4300.34, "text": " final two pixels.", "tokens": [50364, 2572, 732, 18668, 13, 50478], "temperature": 0.0, "avg_logprob": -0.214589789362237, "compression_ratio": 1.6267281105990783, "no_speech_prob": 5.225222776061855e-05}, {"id": 877, "seek": 429806, "start": 4300.34, "end": 4304.9400000000005, "text": " And then we'll add a sigmoid, which will force everything to go between 0 and 1, which of", "tokens": [50478, 400, 550, 321, 603, 909, 257, 4556, 3280, 327, 11, 597, 486, 3464, 1203, 281, 352, 1296, 1958, 293, 502, 11, 597, 295, 50708], "temperature": 0.0, "avg_logprob": -0.214589789362237, "compression_ratio": 1.6267281105990783, "no_speech_prob": 5.225222776061855e-05}, {"id": 878, "seek": 429806, "start": 4304.9400000000005, "end": 4307.42, "text": " course is what we need.", "tokens": [50708, 1164, 307, 437, 321, 643, 13, 50832], "temperature": 0.0, "avg_logprob": -0.214589789362237, "compression_ratio": 1.6267281105990783, "no_speech_prob": 5.225222776061855e-05}, {"id": 879, "seek": 429806, "start": 4307.42, "end": 4315.580000000001, "text": " And then we will use mseloss to compare those pixels to our input pixels.", "tokens": [50832, 400, 550, 321, 486, 764, 275, 790, 772, 281, 6794, 729, 18668, 281, 527, 4846, 18668, 13, 51240], "temperature": 0.0, "avg_logprob": -0.214589789362237, "compression_ratio": 1.6267281105990783, "no_speech_prob": 5.225222776061855e-05}, {"id": 880, "seek": 429806, "start": 4315.580000000001, "end": 4320.3, "text": " And so a big difference we've got here now is that our loss function is being applied", "tokens": [51240, 400, 370, 257, 955, 2649, 321, 600, 658, 510, 586, 307, 300, 527, 4470, 2445, 307, 885, 6456, 51476], "temperature": 0.0, "avg_logprob": -0.214589789362237, "compression_ratio": 1.6267281105990783, "no_speech_prob": 5.225222776061855e-05}, {"id": 881, "seek": 429806, "start": 4320.3, "end": 4324.14, "text": " to the output of the model and itself.", "tokens": [51476, 281, 264, 5598, 295, 264, 2316, 293, 2564, 13, 51668], "temperature": 0.0, "avg_logprob": -0.214589789362237, "compression_ratio": 1.6267281105990783, "no_speech_prob": 5.225222776061855e-05}, {"id": 882, "seek": 429806, "start": 4324.14, "end": 4325.860000000001, "text": " We don't have yb here.", "tokens": [51668, 492, 500, 380, 362, 288, 65, 510, 13, 51754], "temperature": 0.0, "avg_logprob": -0.214589789362237, "compression_ratio": 1.6267281105990783, "no_speech_prob": 5.225222776061855e-05}, {"id": 883, "seek": 432586, "start": 4325.86, "end": 4334.38, "text": " We have xb.", "tokens": [50364, 492, 362, 2031, 65, 13, 50790], "temperature": 0.0, "avg_logprob": -0.290301229895615, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0028008813969790936}, {"id": 884, "seek": 432586, "start": 4334.38, "end": 4337.219999999999, "text": " So we're trying to recreate our original.", "tokens": [50790, 407, 321, 434, 1382, 281, 25833, 527, 3380, 13, 50932], "temperature": 0.0, "avg_logprob": -0.290301229895615, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0028008813969790936}, {"id": 885, "seek": 432586, "start": 4337.219999999999, "end": 4343.46, "text": " And again this is a bit annoying that we have to create our own fit function.", "tokens": [50932, 400, 797, 341, 307, 257, 857, 11304, 300, 321, 362, 281, 1884, 527, 1065, 3318, 2445, 13, 51244], "temperature": 0.0, "avg_logprob": -0.290301229895615, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0028008813969790936}, {"id": 886, "seek": 432586, "start": 4343.46, "end": 4346.46, "text": " Anyway so we can now see what is the mseloss.", "tokens": [51244, 5684, 370, 321, 393, 586, 536, 437, 307, 264, 275, 790, 772, 13, 51394], "temperature": 0.0, "avg_logprob": -0.290301229895615, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0028008813969790936}, {"id": 887, "seek": 432586, "start": 4346.46, "end": 4352.099999999999, "text": " And it's not going to be particularly human readable.", "tokens": [51394, 400, 309, 311, 406, 516, 281, 312, 4098, 1952, 49857, 13, 51676], "temperature": 0.0, "avg_logprob": -0.290301229895615, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0028008813969790936}, {"id": 888, "seek": 432586, "start": 4352.099999999999, "end": 4355.299999999999, "text": " But it's a number we can see if it goes down.", "tokens": [51676, 583, 309, 311, 257, 1230, 321, 393, 536, 498, 309, 1709, 760, 13, 51836], "temperature": 0.0, "avg_logprob": -0.290301229895615, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0028008813969790936}, {"id": 889, "seek": 435530, "start": 4355.3, "end": 4367.860000000001, "text": " And so then we can create... then we can do our SGD with the parameters of our autoencoder", "tokens": [50364, 400, 370, 550, 321, 393, 1884, 485, 550, 321, 393, 360, 527, 34520, 35, 365, 264, 9834, 295, 527, 8399, 22660, 19866, 50992], "temperature": 0.0, "avg_logprob": -0.2793573899702592, "compression_ratio": 1.4591194968553458, "no_speech_prob": 0.002396700903773308}, {"id": 890, "seek": 435530, "start": 4367.860000000001, "end": 4377.860000000001, "text": " with mseloss, call that fit function we just wrote.", "tokens": [50992, 365, 275, 790, 772, 11, 818, 300, 3318, 2445, 321, 445, 4114, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2793573899702592, "compression_ratio": 1.4591194968553458, "no_speech_prob": 0.002396700903773308}, {"id": 891, "seek": 435530, "start": 4377.860000000001, "end": 4383.02, "text": " And I won't wait for it to run, because as you can see it's really slow for reasons we've", "tokens": [51492, 400, 286, 1582, 380, 1699, 337, 309, 281, 1190, 11, 570, 382, 291, 393, 536, 309, 311, 534, 2964, 337, 4112, 321, 600, 51750], "temperature": 0.0, "avg_logprob": -0.2793573899702592, "compression_ratio": 1.4591194968553458, "no_speech_prob": 0.002396700903773308}, {"id": 892, "seek": 438302, "start": 4383.02, "end": 4384.02, "text": " discussed.", "tokens": [50364, 7152, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3278335702830347, "compression_ratio": 1.348148148148148, "no_speech_prob": 0.0031726702582091093}, {"id": 893, "seek": 438302, "start": 4384.02, "end": 4387.820000000001, "text": " I've run it before.", "tokens": [50414, 286, 600, 1190, 309, 949, 13, 50604], "temperature": 0.0, "avg_logprob": -0.3278335702830347, "compression_ratio": 1.348148148148148, "no_speech_prob": 0.0031726702582091093}, {"id": 894, "seek": 438302, "start": 4387.820000000001, "end": 4401.1, "text": " And what we want is to see that the original, which is here, gets recreated.", "tokens": [50604, 400, 437, 321, 528, 307, 281, 536, 300, 264, 3380, 11, 597, 307, 510, 11, 2170, 850, 26559, 13, 51268], "temperature": 0.0, "avg_logprob": -0.3278335702830347, "compression_ratio": 1.348148148148148, "no_speech_prob": 0.0031726702582091093}, {"id": 895, "seek": 438302, "start": 4401.1, "end": 4407.820000000001, "text": " And the answer is, oh, not really.", "tokens": [51268, 400, 264, 1867, 307, 11, 1954, 11, 406, 534, 13, 51604], "temperature": 0.0, "avg_logprob": -0.3278335702830347, "compression_ratio": 1.348148148148148, "no_speech_prob": 0.0031726702582091093}, {"id": 896, "seek": 438302, "start": 4407.820000000001, "end": 4411.900000000001, "text": " I mean they're roughly the same things.", "tokens": [51604, 286, 914, 436, 434, 9810, 264, 912, 721, 13, 51808], "temperature": 0.0, "avg_logprob": -0.3278335702830347, "compression_ratio": 1.348148148148148, "no_speech_prob": 0.0031726702582091093}, {"id": 897, "seek": 441190, "start": 4411.9, "end": 4419.0599999999995, "text": " But there's no point having an autoencoder which can't even recreate the originals.", "tokens": [50364, 583, 456, 311, 572, 935, 1419, 364, 8399, 22660, 19866, 597, 393, 380, 754, 25833, 264, 4957, 1124, 13, 50722], "temperature": 0.0, "avg_logprob": -0.24362311363220215, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0013670171611011028}, {"id": 898, "seek": 441190, "start": 4419.0599999999995, "end": 4424.099999999999, "text": " The idea would be that if these looked almost identical to these, then we'd say, wow, this", "tokens": [50722, 440, 1558, 576, 312, 300, 498, 613, 2956, 1920, 14800, 281, 613, 11, 550, 321, 1116, 584, 11, 6076, 11, 341, 50974], "temperature": 0.0, "avg_logprob": -0.24362311363220215, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0013670171611011028}, {"id": 899, "seek": 441190, "start": 4424.099999999999, "end": 4434.179999999999, "text": " is a fantastic network at compressing things by eight times.", "tokens": [50974, 307, 257, 5456, 3209, 412, 14778, 278, 721, 538, 3180, 1413, 13, 51478], "temperature": 0.0, "avg_logprob": -0.24362311363220215, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0013670171611011028}, {"id": 900, "seek": 441190, "start": 4434.179999999999, "end": 4437.86, "text": " So I found this very fiddly to try and get this to work at all.", "tokens": [51478, 407, 286, 1352, 341, 588, 283, 14273, 356, 281, 853, 293, 483, 341, 281, 589, 412, 439, 13, 51662], "temperature": 0.0, "avg_logprob": -0.24362311363220215, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0013670171611011028}, {"id": 901, "seek": 443786, "start": 4437.86, "end": 4441.62, "text": " Something that I discovered can get it to start training is to start with a really low", "tokens": [50364, 6595, 300, 286, 6941, 393, 483, 309, 281, 722, 3097, 307, 281, 722, 365, 257, 534, 2295, 50552], "temperature": 0.0, "avg_logprob": -0.305055911724384, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.011330652981996536}, {"id": 902, "seek": 443786, "start": 4441.62, "end": 4452.54, "text": " learning rate for a few epochs, and then increase the learning rate after a few epochs.", "tokens": [50552, 2539, 3314, 337, 257, 1326, 30992, 28346, 11, 293, 550, 3488, 264, 2539, 3314, 934, 257, 1326, 30992, 28346, 13, 51098], "temperature": 0.0, "avg_logprob": -0.305055911724384, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.011330652981996536}, {"id": 903, "seek": 443786, "start": 4452.54, "end": 4458.42, "text": " I mean, at least it gets it to train and show something vaguely sensible.", "tokens": [51098, 286, 914, 11, 412, 1935, 309, 2170, 309, 281, 3847, 293, 855, 746, 13501, 48863, 25380, 13, 51392], "temperature": 0.0, "avg_logprob": -0.305055911724384, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.011330652981996536}, {"id": 904, "seek": 443786, "start": 4458.42, "end": 4461.5, "text": " But let's see.", "tokens": [51392, 583, 718, 311, 536, 13, 51546], "temperature": 0.0, "avg_logprob": -0.305055911724384, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.011330652981996536}, {"id": 905, "seek": 443786, "start": 4461.5, "end": 4463.54, "text": " It still looks pretty crummy.", "tokens": [51546, 467, 920, 1542, 1238, 941, 8620, 13, 51648], "temperature": 0.0, "avg_logprob": -0.305055911724384, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.011330652981996536}, {"id": 906, "seek": 446354, "start": 4463.54, "end": 4471.46, "text": " This one here I got actually by switching to Atom, and I actually removed the tricky", "tokens": [50364, 639, 472, 510, 286, 658, 767, 538, 16493, 281, 1711, 298, 11, 293, 286, 767, 7261, 264, 12414, 50760], "temperature": 0.0, "avg_logprob": -0.314430660671658, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0070114159025251865}, {"id": 907, "seek": 446354, "start": 4471.46, "end": 4472.46, "text": " bit.", "tokens": [50760, 857, 13, 50810], "temperature": 0.0, "avg_logprob": -0.314430660671658, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0070114159025251865}, {"id": 908, "seek": 446354, "start": 4472.46, "end": 4474.86, "text": " I removed these two as well.", "tokens": [50810, 286, 7261, 613, 732, 382, 731, 13, 50930], "temperature": 0.0, "avg_logprob": -0.314430660671658, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0070114159025251865}, {"id": 909, "seek": 446354, "start": 4474.86, "end": 4480.1, "text": " But yeah, I couldn't get this to recreate anything very reasonable or any reasonable", "tokens": [50930, 583, 1338, 11, 286, 2809, 380, 483, 341, 281, 25833, 1340, 588, 10585, 420, 604, 10585, 51192], "temperature": 0.0, "avg_logprob": -0.314430660671658, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0070114159025251865}, {"id": 910, "seek": 446354, "start": 4480.1, "end": 4482.1, "text": " amount of time.", "tokens": [51192, 2372, 295, 565, 13, 51292], "temperature": 0.0, "avg_logprob": -0.314430660671658, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0070114159025251865}, {"id": 911, "seek": 446354, "start": 4482.1, "end": 4487.42, "text": " And why is this not working very well?", "tokens": [51292, 400, 983, 307, 341, 406, 1364, 588, 731, 30, 51558], "temperature": 0.0, "avg_logprob": -0.314430660671658, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0070114159025251865}, {"id": 912, "seek": 446354, "start": 4487.42, "end": 4491.18, "text": " There's so many reasons it could be.", "tokens": [51558, 821, 311, 370, 867, 4112, 309, 727, 312, 13, 51746], "temperature": 0.0, "avg_logprob": -0.314430660671658, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0070114159025251865}, {"id": 913, "seek": 449118, "start": 4491.18, "end": 4492.38, "text": " Do we need a better optimizer?", "tokens": [50364, 1144, 321, 643, 257, 1101, 5028, 6545, 30, 50424], "temperature": 0.0, "avg_logprob": -0.23956654522870038, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.0012842926662415266}, {"id": 914, "seek": 449118, "start": 4492.38, "end": 4494.5, "text": " Do we need a better architecture?", "tokens": [50424, 1144, 321, 643, 257, 1101, 9482, 30, 50530], "temperature": 0.0, "avg_logprob": -0.23956654522870038, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.0012842926662415266}, {"id": 915, "seek": 449118, "start": 4494.5, "end": 4498.46, "text": " Do we need to use a variational autoencoder?", "tokens": [50530, 1144, 321, 643, 281, 764, 257, 3034, 1478, 8399, 22660, 19866, 30, 50728], "temperature": 0.0, "avg_logprob": -0.23956654522870038, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.0012842926662415266}, {"id": 916, "seek": 449118, "start": 4498.46, "end": 4501.780000000001, "text": " You know, there's a thousand things we could try.", "tokens": [50728, 509, 458, 11, 456, 311, 257, 4714, 721, 321, 727, 853, 13, 50894], "temperature": 0.0, "avg_logprob": -0.23956654522870038, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.0012842926662415266}, {"id": 917, "seek": 449118, "start": 4501.780000000001, "end": 4506.1, "text": " But doing it like this is going to drive us crazy.", "tokens": [50894, 583, 884, 309, 411, 341, 307, 516, 281, 3332, 505, 3219, 13, 51110], "temperature": 0.0, "avg_logprob": -0.23956654522870038, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.0012842926662415266}, {"id": 918, "seek": 449118, "start": 4506.1, "end": 4512.5, "text": " We need to be able to really rapidly try things, and all kinds of different things.", "tokens": [51110, 492, 643, 281, 312, 1075, 281, 534, 12910, 853, 721, 11, 293, 439, 3685, 295, 819, 721, 13, 51430], "temperature": 0.0, "avg_logprob": -0.23956654522870038, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.0012842926662415266}, {"id": 919, "seek": 449118, "start": 4512.5, "end": 4519.8, "text": " And so what I often see in projects or on Kaggle or whatever, people's code looks kind", "tokens": [51430, 400, 370, 437, 286, 2049, 536, 294, 4455, 420, 322, 48751, 22631, 420, 2035, 11, 561, 311, 3089, 1542, 733, 51795], "temperature": 0.0, "avg_logprob": -0.23956654522870038, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.0012842926662415266}, {"id": 920, "seek": 449118, "start": 4519.8, "end": 4520.8, "text": " of like this.", "tokens": [51795, 295, 411, 341, 13, 51845], "temperature": 0.0, "avg_logprob": -0.23956654522870038, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.0012842926662415266}, {"id": 921, "seek": 452080, "start": 4520.92, "end": 4523.360000000001, "text": " It looks a little like manual.", "tokens": [50370, 467, 1542, 257, 707, 411, 9688, 13, 50492], "temperature": 0.0, "avg_logprob": -0.29754677334347285, "compression_ratio": 1.459016393442623, "no_speech_prob": 5.475971192936413e-05}, {"id": 922, "seek": 452080, "start": 4523.360000000001, "end": 4529.46, "text": " And then their iteration speed is too slow.", "tokens": [50492, 400, 550, 641, 24784, 3073, 307, 886, 2964, 13, 50797], "temperature": 0.0, "avg_logprob": -0.29754677334347285, "compression_ratio": 1.459016393442623, "no_speech_prob": 5.475971192936413e-05}, {"id": 923, "seek": 452080, "start": 4529.46, "end": 4531.84, "text": " We need to be able to really rapidly try things.", "tokens": [50797, 492, 643, 281, 312, 1075, 281, 534, 12910, 853, 721, 13, 50916], "temperature": 0.0, "avg_logprob": -0.29754677334347285, "compression_ratio": 1.459016393442623, "no_speech_prob": 5.475971192936413e-05}, {"id": 924, "seek": 452080, "start": 4531.84, "end": 4535.28, "text": " So we're not going to keep doing stuff manually anymore.", "tokens": [50916, 407, 321, 434, 406, 516, 281, 1066, 884, 1507, 16945, 3602, 13, 51088], "temperature": 0.0, "avg_logprob": -0.29754677334347285, "compression_ratio": 1.459016393442623, "no_speech_prob": 5.475971192936413e-05}, {"id": 925, "seek": 452080, "start": 4535.28, "end": 4544.58, "text": " This is where we take a halt, and we say, okay, let's build up a framework that we can", "tokens": [51088, 639, 307, 689, 321, 747, 257, 12479, 11, 293, 321, 584, 11, 1392, 11, 718, 311, 1322, 493, 257, 8388, 300, 321, 393, 51553], "temperature": 0.0, "avg_logprob": -0.29754677334347285, "compression_ratio": 1.459016393442623, "no_speech_prob": 5.475971192936413e-05}, {"id": 926, "seek": 454458, "start": 4544.58, "end": 4550.76, "text": " use to rapidly try things and understand when things are working and when things aren't", "tokens": [50364, 764, 281, 12910, 853, 721, 293, 1223, 562, 721, 366, 1364, 293, 562, 721, 3212, 380, 50673], "temperature": 0.0, "avg_logprob": -0.30784848370129547, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.04884406924247742}, {"id": 927, "seek": 454458, "start": 4550.76, "end": 4552.72, "text": " working.", "tokens": [50673, 1364, 13, 50771], "temperature": 0.0, "avg_logprob": -0.30784848370129547, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.04884406924247742}, {"id": 928, "seek": 454458, "start": 4552.72, "end": 4559.9, "text": " So we're going to start creating a learner.", "tokens": [50771, 407, 321, 434, 516, 281, 722, 4084, 257, 33347, 13, 51130], "temperature": 0.0, "avg_logprob": -0.30784848370129547, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.04884406924247742}, {"id": 929, "seek": 454458, "start": 4559.9, "end": 4561.12, "text": " So what is a learner?", "tokens": [51130, 407, 437, 307, 257, 33347, 30, 51191], "temperature": 0.0, "avg_logprob": -0.30784848370129547, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.04884406924247742}, {"id": 930, "seek": 454458, "start": 4561.12, "end": 4565.5599999999995, "text": " It's basically the idea is this learner is going to be something that we build, which", "tokens": [51191, 467, 311, 1936, 264, 1558, 307, 341, 33347, 307, 516, 281, 312, 746, 300, 321, 1322, 11, 597, 51413], "temperature": 0.0, "avg_logprob": -0.30784848370129547, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.04884406924247742}, {"id": 931, "seek": 454458, "start": 4565.5599999999995, "end": 4571.58, "text": " will allow us to try anything that we can imagine very quickly.", "tokens": [51413, 486, 2089, 505, 281, 853, 1340, 300, 321, 393, 3811, 588, 2661, 13, 51714], "temperature": 0.0, "avg_logprob": -0.30784848370129547, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.04884406924247742}, {"id": 932, "seek": 457158, "start": 4571.58, "end": 4575.92, "text": " And we will build then on top of that learner things that will allow us to introspect what's", "tokens": [50364, 400, 321, 486, 1322, 550, 322, 1192, 295, 300, 33347, 721, 300, 486, 2089, 505, 281, 560, 28713, 437, 311, 50581], "temperature": 0.0, "avg_logprob": -0.27942194032274986, "compression_ratio": 1.74, "no_speech_prob": 0.010986470617353916}, {"id": 933, "seek": 457158, "start": 4575.92, "end": 4581.9, "text": " going on inside a model, will allow us to do multiprocess CUDA to go fast.", "tokens": [50581, 516, 322, 1854, 257, 2316, 11, 486, 2089, 505, 281, 360, 3311, 340, 780, 29777, 7509, 281, 352, 2370, 13, 50880], "temperature": 0.0, "avg_logprob": -0.27942194032274986, "compression_ratio": 1.74, "no_speech_prob": 0.010986470617353916}, {"id": 934, "seek": 457158, "start": 4581.9, "end": 4584.44, "text": " It will allow us to add things like data augmentation.", "tokens": [50880, 467, 486, 2089, 505, 281, 909, 721, 411, 1412, 14501, 19631, 13, 51007], "temperature": 0.0, "avg_logprob": -0.27942194032274986, "compression_ratio": 1.74, "no_speech_prob": 0.010986470617353916}, {"id": 935, "seek": 457158, "start": 4584.44, "end": 4588.9, "text": " It will allow us to try a wide variety of architectures quickly, and so forth.", "tokens": [51007, 467, 486, 2089, 505, 281, 853, 257, 4874, 5673, 295, 6331, 1303, 2661, 11, 293, 370, 5220, 13, 51230], "temperature": 0.0, "avg_logprob": -0.27942194032274986, "compression_ratio": 1.74, "no_speech_prob": 0.010986470617353916}, {"id": 936, "seek": 457158, "start": 4588.9, "end": 4591.2, "text": " So that's going to be the idea.", "tokens": [51230, 407, 300, 311, 516, 281, 312, 264, 1558, 13, 51345], "temperature": 0.0, "avg_logprob": -0.27942194032274986, "compression_ratio": 1.74, "no_speech_prob": 0.010986470617353916}, {"id": 937, "seek": 457158, "start": 4591.2, "end": 4594.08, "text": " And of course we're going to create it from scratch.", "tokens": [51345, 400, 295, 1164, 321, 434, 516, 281, 1884, 309, 490, 8459, 13, 51489], "temperature": 0.0, "avg_logprob": -0.27942194032274986, "compression_ratio": 1.74, "no_speech_prob": 0.010986470617353916}, {"id": 938, "seek": 457158, "start": 4594.08, "end": 4598.84, "text": " And so let's start with fashion.mnist as before.", "tokens": [51489, 400, 370, 718, 311, 722, 365, 6700, 13, 76, 77, 468, 382, 949, 13, 51727], "temperature": 0.0, "avg_logprob": -0.27942194032274986, "compression_ratio": 1.74, "no_speech_prob": 0.010986470617353916}, {"id": 939, "seek": 459884, "start": 4598.84, "end": 4608.360000000001, "text": " And let's create a data loaders class, which is going to look a bit like what we had before,", "tokens": [50364, 400, 718, 311, 1884, 257, 1412, 3677, 433, 1508, 11, 597, 307, 516, 281, 574, 257, 857, 411, 437, 321, 632, 949, 11, 50840], "temperature": 0.0, "avg_logprob": -0.289606070216698, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.0006771922344341874}, {"id": 940, "seek": 459884, "start": 4608.360000000001, "end": 4613.38, "text": " where we're just going to pass in, this couldn't be simpler, right?", "tokens": [50840, 689, 321, 434, 445, 516, 281, 1320, 294, 11, 341, 2809, 380, 312, 18587, 11, 558, 30, 51091], "temperature": 0.0, "avg_logprob": -0.289606070216698, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.0006771922344341874}, {"id": 941, "seek": 459884, "start": 4613.38, "end": 4619.78, "text": " We're just going to pass in two data loaders and store them away.", "tokens": [51091, 492, 434, 445, 516, 281, 1320, 294, 732, 1412, 3677, 433, 293, 3531, 552, 1314, 13, 51411], "temperature": 0.0, "avg_logprob": -0.289606070216698, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.0006771922344341874}, {"id": 942, "seek": 459884, "start": 4619.78, "end": 4626.14, "text": " And I'm going to create a class method from dataset dictionary.", "tokens": [51411, 400, 286, 478, 516, 281, 1884, 257, 1508, 3170, 490, 28872, 25890, 13, 51729], "temperature": 0.0, "avg_logprob": -0.289606070216698, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.0006771922344341874}, {"id": 943, "seek": 462614, "start": 4626.14, "end": 4631.64, "text": " And what that's going to do is it's going to call data loader on each of the dataset", "tokens": [50364, 400, 437, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 818, 1412, 3677, 260, 322, 1184, 295, 264, 28872, 50639], "temperature": 0.0, "avg_logprob": -0.24598768985632694, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.0003006152983289212}, {"id": 944, "seek": 462614, "start": 4631.64, "end": 4638.240000000001, "text": " dictionary items with our batch size and instantiate our class.", "tokens": [50639, 25890, 4754, 365, 527, 15245, 2744, 293, 9836, 13024, 527, 1508, 13, 50969], "temperature": 0.0, "avg_logprob": -0.24598768985632694, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.0003006152983289212}, {"id": 945, "seek": 462614, "start": 4638.240000000001, "end": 4643.360000000001, "text": " So if you haven't seen class method before, it's what allows us to say data loaders dot", "tokens": [50969, 407, 498, 291, 2378, 380, 1612, 1508, 3170, 949, 11, 309, 311, 437, 4045, 505, 281, 584, 1412, 3677, 433, 5893, 51225], "temperature": 0.0, "avg_logprob": -0.24598768985632694, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.0003006152983289212}, {"id": 946, "seek": 462614, "start": 4643.360000000001, "end": 4647.04, "text": " something in order to construct this.", "tokens": [51225, 746, 294, 1668, 281, 7690, 341, 13, 51409], "temperature": 0.0, "avg_logprob": -0.24598768985632694, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.0003006152983289212}, {"id": 947, "seek": 462614, "start": 4647.04, "end": 4651.4800000000005, "text": " We could have put this in init just as well, but we'll be building more complex data loaders", "tokens": [51409, 492, 727, 362, 829, 341, 294, 3157, 445, 382, 731, 11, 457, 321, 603, 312, 2390, 544, 3997, 1412, 3677, 433, 51631], "temperature": 0.0, "avg_logprob": -0.24598768985632694, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.0003006152983289212}, {"id": 948, "seek": 462614, "start": 4651.4800000000005, "end": 4652.740000000001, "text": " things later.", "tokens": [51631, 721, 1780, 13, 51694], "temperature": 0.0, "avg_logprob": -0.24598768985632694, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.0003006152983289212}, {"id": 949, "seek": 465274, "start": 4652.74, "end": 4656.88, "text": " So I thought we might start by getting the basic structure right.", "tokens": [50364, 407, 286, 1194, 321, 1062, 722, 538, 1242, 264, 3875, 3877, 558, 13, 50571], "temperature": 0.0, "avg_logprob": -0.3568295137381848, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.011686545796692371}, {"id": 950, "seek": 465274, "start": 4656.88, "end": 4658.8, "text": " So this is all pretty much the same as what we've had before.", "tokens": [50571, 407, 341, 307, 439, 1238, 709, 264, 912, 382, 437, 321, 600, 632, 949, 13, 50667], "temperature": 0.0, "avg_logprob": -0.3568295137381848, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.011686545796692371}, {"id": 951, "seek": 465274, "start": 4658.8, "end": 4665.44, "text": " I'm not doing anything on the device here, because as we know that didn't really work.", "tokens": [50667, 286, 478, 406, 884, 1340, 322, 264, 4302, 510, 11, 570, 382, 321, 458, 300, 994, 380, 534, 589, 13, 50999], "temperature": 0.0, "avg_logprob": -0.3568295137381848, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.011686545796692371}, {"id": 952, "seek": 465274, "start": 4665.44, "end": 4667.639999999999, "text": " Okay.", "tokens": [50999, 1033, 13, 51109], "temperature": 0.0, "avg_logprob": -0.3568295137381848, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.011686545796692371}, {"id": 953, "seek": 465274, "start": 4667.639999999999, "end": 4672.4, "text": " Oh, this is an old thing.", "tokens": [51109, 876, 11, 341, 307, 364, 1331, 551, 13, 51347], "temperature": 0.0, "avg_logprob": -0.3568295137381848, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.011686545796692371}, {"id": 954, "seek": 465274, "start": 4672.4, "end": 4676.46, "text": " I don't need to coo anymore.", "tokens": [51347, 286, 500, 380, 643, 281, 598, 78, 3602, 13, 51550], "temperature": 0.0, "avg_logprob": -0.3568295137381848, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.011686545796692371}, {"id": 955, "seek": 467646, "start": 4676.4800000000005, "end": 4682.08, "text": " So we're going to use to device, which I think came from...", "tokens": [50365, 407, 321, 434, 516, 281, 764, 281, 4302, 11, 597, 286, 519, 1361, 490, 485, 50645], "temperature": 0.0, "avg_logprob": -0.34873495783124653, "compression_ratio": 1.3513513513513513, "no_speech_prob": 0.0018967455253005028}, {"id": 956, "seek": 467646, "start": 4682.08, "end": 4689.88, "text": " There we go.", "tokens": [50645, 821, 321, 352, 13, 51035], "temperature": 0.0, "avg_logprob": -0.34873495783124653, "compression_ratio": 1.3513513513513513, "no_speech_prob": 0.0018967455253005028}, {"id": 957, "seek": 467646, "start": 4689.88, "end": 4696.4800000000005, "text": " So here's an example of a very simple learner that fits on one screen.", "tokens": [51035, 407, 510, 311, 364, 1365, 295, 257, 588, 2199, 33347, 300, 9001, 322, 472, 2568, 13, 51365], "temperature": 0.0, "avg_logprob": -0.34873495783124653, "compression_ratio": 1.3513513513513513, "no_speech_prob": 0.0018967455253005028}, {"id": 958, "seek": 467646, "start": 4696.4800000000005, "end": 4700.52, "text": " And this is basically going to replace our fit function.", "tokens": [51365, 400, 341, 307, 1936, 516, 281, 7406, 527, 3318, 2445, 13, 51567], "temperature": 0.0, "avg_logprob": -0.34873495783124653, "compression_ratio": 1.3513513513513513, "no_speech_prob": 0.0018967455253005028}, {"id": 959, "seek": 470052, "start": 4700.580000000001, "end": 4706.9400000000005, "text": " So a learner is going to be something that is going to train or learn a particular model", "tokens": [50367, 407, 257, 33347, 307, 516, 281, 312, 746, 300, 307, 516, 281, 3847, 420, 1466, 257, 1729, 2316, 50685], "temperature": 0.0, "avg_logprob": -0.2591980330798091, "compression_ratio": 1.8716814159292035, "no_speech_prob": 0.0011513261124491692}, {"id": 960, "seek": 470052, "start": 4706.9400000000005, "end": 4712.9400000000005, "text": " using a particular set of data loaders, a particular loss function, some particular", "tokens": [50685, 1228, 257, 1729, 992, 295, 1412, 3677, 433, 11, 257, 1729, 4470, 2445, 11, 512, 1729, 50985], "temperature": 0.0, "avg_logprob": -0.2591980330798091, "compression_ratio": 1.8716814159292035, "no_speech_prob": 0.0011513261124491692}, {"id": 961, "seek": 470052, "start": 4712.9400000000005, "end": 4719.18, "text": " learning rate, and some particular optimizer or some particular optimization function.", "tokens": [50985, 2539, 3314, 11, 293, 512, 1729, 5028, 6545, 420, 512, 1729, 19618, 2445, 13, 51297], "temperature": 0.0, "avg_logprob": -0.2591980330798091, "compression_ratio": 1.8716814159292035, "no_speech_prob": 0.0011513261124491692}, {"id": 962, "seek": 470052, "start": 4719.18, "end": 4724.42, "text": " Now normally I, you know, most people would often kind of store each of these away separately", "tokens": [51297, 823, 5646, 286, 11, 291, 458, 11, 881, 561, 576, 2049, 733, 295, 3531, 1184, 295, 613, 1314, 14759, 51559], "temperature": 0.0, "avg_logprob": -0.2591980330798091, "compression_ratio": 1.8716814159292035, "no_speech_prob": 0.0011513261124491692}, {"id": 963, "seek": 470052, "start": 4724.42, "end": 4729.18, "text": " by writing like self dot model equals model, blah, blah, blah.", "tokens": [51559, 538, 3579, 411, 2698, 5893, 2316, 6915, 2316, 11, 12288, 11, 12288, 11, 12288, 13, 51797], "temperature": 0.0, "avg_logprob": -0.2591980330798091, "compression_ratio": 1.8716814159292035, "no_speech_prob": 0.0011513261124491692}, {"id": 964, "seek": 470052, "start": 4729.18, "end": 4730.18, "text": " Right.", "tokens": [51797, 1779, 13, 51847], "temperature": 0.0, "avg_logprob": -0.2591980330798091, "compression_ratio": 1.8716814159292035, "no_speech_prob": 0.0011513261124491692}, {"id": 965, "seek": 473018, "start": 4730.84, "end": 4732.04, "text": " So that's what we talked about before.", "tokens": [50397, 407, 300, 311, 437, 321, 2825, 466, 949, 13, 50457], "temperature": 0.0, "avg_logprob": -0.3035301152807083, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0007672919309698045}, {"id": 966, "seek": 473018, "start": 4732.04, "end": 4735.740000000001, "text": " That's, you know, that kind of huge amounts of boilerplate.", "tokens": [50457, 663, 311, 11, 291, 458, 11, 300, 733, 295, 2603, 11663, 295, 39228, 37008, 13, 50642], "temperature": 0.0, "avg_logprob": -0.3035301152807083, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0007672919309698045}, {"id": 967, "seek": 473018, "start": 4735.740000000001, "end": 4740.16, "text": " It's more stuff that you can get wrong and it's more stuff to mean that you have to read", "tokens": [50642, 467, 311, 544, 1507, 300, 291, 393, 483, 2085, 293, 309, 311, 544, 1507, 281, 914, 300, 291, 362, 281, 1401, 50863], "temperature": 0.0, "avg_logprob": -0.3035301152807083, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0007672919309698045}, {"id": 968, "seek": 473018, "start": 4740.16, "end": 4744.46, "text": " to understand the code and yeah, don't like that kind of repetition.", "tokens": [50863, 281, 1223, 264, 3089, 293, 1338, 11, 500, 380, 411, 300, 733, 295, 30432, 13, 51078], "temperature": 0.0, "avg_logprob": -0.3035301152807083, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0007672919309698045}, {"id": 969, "seek": 473018, "start": 4744.46, "end": 4749.0, "text": " So instead we just call fastcore dot store attra to do that all in one line.", "tokens": [51078, 407, 2602, 321, 445, 818, 2370, 12352, 5893, 3531, 412, 17227, 281, 360, 300, 439, 294, 472, 1622, 13, 51305], "temperature": 0.0, "avg_logprob": -0.3035301152807083, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0007672919309698045}, {"id": 970, "seek": 473018, "start": 4749.0, "end": 4750.0, "text": " Okay.", "tokens": [51305, 1033, 13, 51355], "temperature": 0.0, "avg_logprob": -0.3035301152807083, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0007672919309698045}, {"id": 971, "seek": 473018, "start": 4750.0, "end": 4753.62, "text": " So that's basically the idea with a class is to think about what's the information it's", "tokens": [51355, 407, 300, 311, 1936, 264, 1558, 365, 257, 1508, 307, 281, 519, 466, 437, 311, 264, 1589, 309, 311, 51536], "temperature": 0.0, "avg_logprob": -0.3035301152807083, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0007672919309698045}, {"id": 972, "seek": 473018, "start": 4753.62, "end": 4754.62, "text": " going to need.", "tokens": [51536, 516, 281, 643, 13, 51586], "temperature": 0.0, "avg_logprob": -0.3035301152807083, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0007672919309698045}, {"id": 973, "seek": 473018, "start": 4754.62, "end": 4759.02, "text": " So you pass that all to the constructor, store it away.", "tokens": [51586, 407, 291, 1320, 300, 439, 281, 264, 47479, 11, 3531, 309, 1314, 13, 51806], "temperature": 0.0, "avg_logprob": -0.3035301152807083, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0007672919309698045}, {"id": 974, "seek": 475902, "start": 4759.02, "end": 4771.42, "text": " And then our fit function is going to, we've got the basic stuff that we have for keeping", "tokens": [50364, 400, 550, 527, 3318, 2445, 307, 516, 281, 11, 321, 600, 658, 264, 3875, 1507, 300, 321, 362, 337, 5145, 50984], "temperature": 0.0, "avg_logprob": -0.25833819309870404, "compression_ratio": 1.406015037593985, "no_speech_prob": 0.0001442591892555356}, {"id": 975, "seek": 475902, "start": 4771.42, "end": 4773.42, "text": " track of accuracy.", "tokens": [50984, 2837, 295, 14170, 13, 51084], "temperature": 0.0, "avg_logprob": -0.25833819309870404, "compression_ratio": 1.406015037593985, "no_speech_prob": 0.0001442591892555356}, {"id": 976, "seek": 475902, "start": 4773.42, "end": 4783.26, "text": " This is only work for stuff that's a classification where we can use accuracy.", "tokens": [51084, 639, 307, 787, 589, 337, 1507, 300, 311, 257, 21538, 689, 321, 393, 764, 14170, 13, 51576], "temperature": 0.0, "avg_logprob": -0.25833819309870404, "compression_ratio": 1.406015037593985, "no_speech_prob": 0.0001442591892555356}, {"id": 977, "seek": 478326, "start": 4783.26, "end": 4789.9400000000005, "text": " Put the model on our device, create the optimizer, store how many epochs we're going through.", "tokens": [50364, 4935, 264, 2316, 322, 527, 4302, 11, 1884, 264, 5028, 6545, 11, 3531, 577, 867, 30992, 28346, 321, 434, 516, 807, 13, 50698], "temperature": 0.0, "avg_logprob": -0.24318212509155274, "compression_ratio": 1.8309178743961352, "no_speech_prob": 0.03567497432231903}, {"id": 978, "seek": 478326, "start": 4789.9400000000005, "end": 4795.5, "text": " Then for each epoch we'll call the one epoch function and the one epoch function we're", "tokens": [50698, 1396, 337, 1184, 30992, 339, 321, 603, 818, 264, 472, 30992, 339, 2445, 293, 264, 472, 30992, 339, 2445, 321, 434, 50976], "temperature": 0.0, "avg_logprob": -0.24318212509155274, "compression_ratio": 1.8309178743961352, "no_speech_prob": 0.03567497432231903}, {"id": 979, "seek": 478326, "start": 4795.5, "end": 4798.14, "text": " going to either do train or evaluation.", "tokens": [50976, 516, 281, 2139, 360, 3847, 420, 13344, 13, 51108], "temperature": 0.0, "avg_logprob": -0.24318212509155274, "compression_ratio": 1.8309178743961352, "no_speech_prob": 0.03567497432231903}, {"id": 980, "seek": 478326, "start": 4798.14, "end": 4803.860000000001, "text": " So we pass in true if we're training and false if we're evaluating.", "tokens": [51108, 407, 321, 1320, 294, 2074, 498, 321, 434, 3097, 293, 7908, 498, 321, 434, 27479, 13, 51394], "temperature": 0.0, "avg_logprob": -0.24318212509155274, "compression_ratio": 1.8309178743961352, "no_speech_prob": 0.03567497432231903}, {"id": 981, "seek": 478326, "start": 4803.860000000001, "end": 4806.76, "text": " And they're basically almost the same.", "tokens": [51394, 400, 436, 434, 1936, 1920, 264, 912, 13, 51539], "temperature": 0.0, "avg_logprob": -0.24318212509155274, "compression_ratio": 1.8309178743961352, "no_speech_prob": 0.03567497432231903}, {"id": 982, "seek": 478326, "start": 4806.76, "end": 4811.780000000001, "text": " We basically set the model to training mode or not.", "tokens": [51539, 492, 1936, 992, 264, 2316, 281, 3097, 4391, 420, 406, 13, 51790], "temperature": 0.0, "avg_logprob": -0.24318212509155274, "compression_ratio": 1.8309178743961352, "no_speech_prob": 0.03567497432231903}, {"id": 983, "seek": 481178, "start": 4811.78, "end": 4815.62, "text": " We then decide whether to use the validation set or the training set based on whether we're", "tokens": [50364, 492, 550, 4536, 1968, 281, 764, 264, 24071, 992, 420, 264, 3097, 992, 2361, 322, 1968, 321, 434, 50556], "temperature": 0.0, "avg_logprob": -0.28334193480642217, "compression_ratio": 1.755952380952381, "no_speech_prob": 0.0006361865089274943}, {"id": 984, "seek": 481178, "start": 4815.62, "end": 4817.5, "text": " training.", "tokens": [50556, 3097, 13, 50650], "temperature": 0.0, "avg_logprob": -0.28334193480642217, "compression_ratio": 1.755952380952381, "no_speech_prob": 0.0006361865089274943}, {"id": 985, "seek": 481178, "start": 4817.5, "end": 4825.62, "text": " And then we go through each batch in the data loader and call one batch.", "tokens": [50650, 400, 550, 321, 352, 807, 1184, 15245, 294, 264, 1412, 3677, 260, 293, 818, 472, 15245, 13, 51056], "temperature": 0.0, "avg_logprob": -0.28334193480642217, "compression_ratio": 1.755952380952381, "no_speech_prob": 0.0006361865089274943}, {"id": 986, "seek": 481178, "start": 4825.62, "end": 4833.38, "text": " And one batch is then the thing which is going to put our batch onto the device, call our", "tokens": [51056, 400, 472, 15245, 307, 550, 264, 551, 597, 307, 516, 281, 829, 527, 15245, 3911, 264, 4302, 11, 818, 527, 51444], "temperature": 0.0, "avg_logprob": -0.28334193480642217, "compression_ratio": 1.755952380952381, "no_speech_prob": 0.0006361865089274943}, {"id": 987, "seek": 481178, "start": 4833.38, "end": 4836.8, "text": " model, call our loss function.", "tokens": [51444, 2316, 11, 818, 527, 4470, 2445, 13, 51615], "temperature": 0.0, "avg_logprob": -0.28334193480642217, "compression_ratio": 1.755952380952381, "no_speech_prob": 0.0006361865089274943}, {"id": 988, "seek": 483680, "start": 4836.8, "end": 4844.12, "text": " And then if we're training, then do our backward step, our optimizer step in our zero gradient.", "tokens": [50364, 400, 550, 498, 321, 434, 3097, 11, 550, 360, 527, 23897, 1823, 11, 527, 5028, 6545, 1823, 294, 527, 4018, 16235, 13, 50730], "temperature": 0.0, "avg_logprob": -0.23035286568306587, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.004538316745311022}, {"id": 989, "seek": 483680, "start": 4844.12, "end": 4847.400000000001, "text": " And then finally calculate our metrics or our stats.", "tokens": [50730, 400, 550, 2721, 8873, 527, 16367, 420, 527, 18152, 13, 50894], "temperature": 0.0, "avg_logprob": -0.23035286568306587, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.004538316745311022}, {"id": 990, "seek": 483680, "start": 4847.400000000001, "end": 4850.06, "text": " And so here's where we calculate our metrics.", "tokens": [50894, 400, 370, 510, 311, 689, 321, 8873, 527, 16367, 13, 51027], "temperature": 0.0, "avg_logprob": -0.23035286568306587, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.004538316745311022}, {"id": 991, "seek": 483680, "start": 4850.06, "end": 4858.22, "text": " So that's basically what we have there.", "tokens": [51027, 407, 300, 311, 1936, 437, 321, 362, 456, 13, 51435], "temperature": 0.0, "avg_logprob": -0.23035286568306587, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.004538316745311022}, {"id": 992, "seek": 483680, "start": 4858.22, "end": 4862.9800000000005, "text": " So let's go back to using an MLP.", "tokens": [51435, 407, 718, 311, 352, 646, 281, 1228, 364, 21601, 47, 13, 51673], "temperature": 0.0, "avg_logprob": -0.23035286568306587, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.004538316745311022}, {"id": 993, "seek": 486298, "start": 4862.98, "end": 4869.099999999999, "text": " We call fit.", "tokens": [50364, 492, 818, 3318, 13, 50670], "temperature": 0.0, "avg_logprob": -0.3258462465726412, "compression_ratio": 1.3129251700680271, "no_speech_prob": 0.00941221509128809}, {"id": 994, "seek": 486298, "start": 4869.099999999999, "end": 4873.959999999999, "text": " And away it goes.", "tokens": [50670, 400, 1314, 309, 1709, 13, 50913], "temperature": 0.0, "avg_logprob": -0.3258462465726412, "compression_ratio": 1.3129251700680271, "no_speech_prob": 0.00941221509128809}, {"id": 995, "seek": 486298, "start": 4873.959999999999, "end": 4877.299999999999, "text": " This is an error here, pointed out by Kevin.", "tokens": [50913, 639, 307, 364, 6713, 510, 11, 10932, 484, 538, 9954, 13, 51080], "temperature": 0.0, "avg_logprob": -0.3258462465726412, "compression_ratio": 1.3129251700680271, "no_speech_prob": 0.00941221509128809}, {"id": 996, "seek": 486298, "start": 4877.299999999999, "end": 4878.299999999999, "text": " Thank you.", "tokens": [51080, 1044, 291, 13, 51130], "temperature": 0.0, "avg_logprob": -0.3258462465726412, "compression_ratio": 1.3129251700680271, "no_speech_prob": 0.00941221509128809}, {"id": 997, "seek": 486298, "start": 4878.299999999999, "end": 4879.299999999999, "text": " self.model.to.", "tokens": [51130, 2698, 13, 8014, 338, 13, 1353, 13, 51180], "temperature": 0.0, "avg_logprob": -0.3258462465726412, "compression_ratio": 1.3129251700680271, "no_speech_prob": 0.00941221509128809}, {"id": 998, "seek": 486298, "start": 4879.299999999999, "end": 4891.099999999999, "text": " One thing I guess we could try now is we think that maybe we can use more than one process.", "tokens": [51180, 1485, 551, 286, 2041, 321, 727, 853, 586, 307, 321, 519, 300, 1310, 321, 393, 764, 544, 813, 472, 1399, 13, 51770], "temperature": 0.0, "avg_logprob": -0.3258462465726412, "compression_ratio": 1.3129251700680271, "no_speech_prob": 0.00941221509128809}, {"id": 999, "seek": 489110, "start": 4891.22, "end": 4893.22, "text": " So let's try that.", "tokens": [50370, 407, 718, 311, 853, 300, 13, 50470], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1000, "seek": 489110, "start": 4893.22, "end": 4900.660000000001, "text": " Oh, it's so fast.", "tokens": [50470, 876, 11, 309, 311, 370, 2370, 13, 50842], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1001, "seek": 489110, "start": 4900.660000000001, "end": 4901.660000000001, "text": " I didn't even see.", "tokens": [50842, 286, 994, 380, 754, 536, 13, 50892], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1002, "seek": 489110, "start": 4901.660000000001, "end": 4902.900000000001, "text": " There it goes.", "tokens": [50892, 821, 309, 1709, 13, 50954], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1003, "seek": 489110, "start": 4902.900000000001, "end": 4905.54, "text": " You can see all four CPUs being used at once.", "tokens": [50954, 509, 393, 536, 439, 1451, 13199, 82, 885, 1143, 412, 1564, 13, 51086], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1004, "seek": 489110, "start": 4905.54, "end": 4907.26, "text": " Bang, it's done.", "tokens": [51086, 11538, 11, 309, 311, 1096, 13, 51172], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1005, "seek": 489110, "start": 4907.26, "end": 4909.620000000001, "text": " Okay, so that's pretty great.", "tokens": [51172, 1033, 11, 370, 300, 311, 1238, 869, 13, 51290], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1006, "seek": 489110, "start": 4909.620000000001, "end": 4911.22, "text": " Let's see how fast it looks here.", "tokens": [51290, 961, 311, 536, 577, 2370, 309, 1542, 510, 13, 51370], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1007, "seek": 489110, "start": 4911.22, "end": 4912.620000000001, "text": " Bump, bump.", "tokens": [51370, 363, 1420, 11, 9961, 13, 51440], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1008, "seek": 489110, "start": 4912.620000000001, "end": 4915.06, "text": " All right, lovely.", "tokens": [51440, 1057, 558, 11, 7496, 13, 51562], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1009, "seek": 489110, "start": 4915.06, "end": 4918.14, "text": " Okay, so that's a good sign.", "tokens": [51562, 1033, 11, 370, 300, 311, 257, 665, 1465, 13, 51716], "temperature": 0.0, "avg_logprob": -0.48140387959999614, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.0064881411381065845}, {"id": 1010, "seek": 491814, "start": 4918.18, "end": 4925.42, "text": " So we've got a learner that can fit things, but it's not very flexible.", "tokens": [50366, 407, 321, 600, 658, 257, 33347, 300, 393, 3318, 721, 11, 457, 309, 311, 406, 588, 11358, 13, 50728], "temperature": 0.0, "avg_logprob": -0.3609505123562283, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.006097381003201008}, {"id": 1011, "seek": 491814, "start": 4925.42, "end": 4930.820000000001, "text": " It's not going to help us, for example, with our autoencoder, because there's no way of", "tokens": [50728, 467, 311, 406, 516, 281, 854, 505, 11, 337, 1365, 11, 365, 527, 8399, 22660, 19866, 11, 570, 456, 311, 572, 636, 295, 50998], "temperature": 0.0, "avg_logprob": -0.3609505123562283, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.006097381003201008}, {"id": 1012, "seek": 491814, "start": 4930.820000000001, "end": 4937.26, "text": " like changing which things are used for predicting with or for calculating with.", "tokens": [50998, 411, 4473, 597, 721, 366, 1143, 337, 32884, 365, 420, 337, 28258, 365, 13, 51320], "temperature": 0.0, "avg_logprob": -0.3609505123562283, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.006097381003201008}, {"id": 1013, "seek": 491814, "start": 4937.26, "end": 4942.3, "text": " We can't use it for anything except things that involve accuracy with a binary classification.", "tokens": [51320, 492, 393, 380, 764, 309, 337, 1340, 3993, 721, 300, 9494, 14170, 365, 257, 17434, 21538, 13, 51572], "temperature": 0.0, "avg_logprob": -0.3609505123562283, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.006097381003201008}, {"id": 1014, "seek": 491814, "start": 4942.3, "end": 4945.5, "text": " Sorry, is that right?", "tokens": [51572, 4919, 11, 307, 300, 558, 30, 51732], "temperature": 0.0, "avg_logprob": -0.3609505123562283, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.006097381003201008}, {"id": 1015, "seek": 494550, "start": 4945.86, "end": 4950.62, "text": " Sorry, yeah, a multi-class classification.", "tokens": [50382, 4919, 11, 1338, 11, 257, 4825, 12, 11665, 21538, 13, 50620], "temperature": 0.0, "avg_logprob": -0.29052824587435333, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.00021654341253452003}, {"id": 1016, "seek": 494550, "start": 4950.62, "end": 4952.74, "text": " It's not flexible at all, but it's a start.", "tokens": [50620, 467, 311, 406, 11358, 412, 439, 11, 457, 309, 311, 257, 722, 13, 50726], "temperature": 0.0, "avg_logprob": -0.29052824587435333, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.00021654341253452003}, {"id": 1017, "seek": 494550, "start": 4952.74, "end": 4956.26, "text": " And so I wanted to basically put this all on one screen so you can see what the basic", "tokens": [50726, 400, 370, 286, 1415, 281, 1936, 829, 341, 439, 322, 472, 2568, 370, 291, 393, 536, 437, 264, 3875, 50902], "temperature": 0.0, "avg_logprob": -0.29052824587435333, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.00021654341253452003}, {"id": 1018, "seek": 494550, "start": 4956.26, "end": 4958.74, "text": " learner looks like.", "tokens": [50902, 33347, 1542, 411, 13, 51026], "temperature": 0.0, "avg_logprob": -0.29052824587435333, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.00021654341253452003}, {"id": 1019, "seek": 494550, "start": 4958.74, "end": 4969.06, "text": " All right, so how do we do things other than multi-class accuracy?", "tokens": [51026, 1057, 558, 11, 370, 577, 360, 321, 360, 721, 661, 813, 4825, 12, 11665, 14170, 30, 51542], "temperature": 0.0, "avg_logprob": -0.29052824587435333, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.00021654341253452003}, {"id": 1020, "seek": 496906, "start": 4969.06, "end": 4974.860000000001, "text": " I decided to create a metric class.", "tokens": [50364, 286, 3047, 281, 1884, 257, 20678, 1508, 13, 50654], "temperature": 0.0, "avg_logprob": -0.2360615015029907, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.001548737520352006}, {"id": 1021, "seek": 496906, "start": 4974.860000000001, "end": 4983.1, "text": " And basically a metric class is something where we are going to define subclasses of", "tokens": [50654, 400, 1936, 257, 20678, 1508, 307, 746, 689, 321, 366, 516, 281, 6964, 1422, 11665, 279, 295, 51066], "temperature": 0.0, "avg_logprob": -0.2360615015029907, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.001548737520352006}, {"id": 1022, "seek": 496906, "start": 4983.1, "end": 4986.02, "text": " it that calculate particular metrics.", "tokens": [51066, 309, 300, 8873, 1729, 16367, 13, 51212], "temperature": 0.0, "avg_logprob": -0.2360615015029907, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.001548737520352006}, {"id": 1023, "seek": 496906, "start": 4986.02, "end": 4990.360000000001, "text": " So for example, here I've got a subclass of a metric called accuracy.", "tokens": [51212, 407, 337, 1365, 11, 510, 286, 600, 658, 257, 1422, 11665, 295, 257, 20678, 1219, 14170, 13, 51429], "temperature": 0.0, "avg_logprob": -0.2360615015029907, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.001548737520352006}, {"id": 1024, "seek": 496906, "start": 4990.360000000001, "end": 4997.54, "text": " So if you haven't done subclasses before, you can basically think of this as saying,", "tokens": [51429, 407, 498, 291, 2378, 380, 1096, 1422, 11665, 279, 949, 11, 291, 393, 1936, 519, 295, 341, 382, 1566, 11, 51788], "temperature": 0.0, "avg_logprob": -0.2360615015029907, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.001548737520352006}, {"id": 1025, "seek": 499754, "start": 4997.54, "end": 5004.74, "text": " please copy and paste all the code from here into here for me, but the bit that says def", "tokens": [50364, 1767, 5055, 293, 9163, 439, 264, 3089, 490, 510, 666, 510, 337, 385, 11, 457, 264, 857, 300, 1619, 1060, 50724], "temperature": 0.0, "avg_logprob": -0.22673304875691733, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.0003199978091288358}, {"id": 1026, "seek": 499754, "start": 5004.74, "end": 5007.46, "text": " calc, replace it with this version.", "tokens": [50724, 2104, 66, 11, 7406, 309, 365, 341, 3037, 13, 50860], "temperature": 0.0, "avg_logprob": -0.22673304875691733, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.0003199978091288358}, {"id": 1027, "seek": 499754, "start": 5007.46, "end": 5014.5, "text": " So in fact, this would be identical to copying and pasting this whole thing, typing accuracy", "tokens": [50860, 407, 294, 1186, 11, 341, 576, 312, 14800, 281, 27976, 293, 1791, 278, 341, 1379, 551, 11, 18444, 14170, 51212], "temperature": 0.0, "avg_logprob": -0.22673304875691733, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.0003199978091288358}, {"id": 1028, "seek": 499754, "start": 5014.5, "end": 5023.18, "text": " here and replacing the definition of calc with that.", "tokens": [51212, 510, 293, 19139, 264, 7123, 295, 2104, 66, 365, 300, 13, 51646], "temperature": 0.0, "avg_logprob": -0.22673304875691733, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.0003199978091288358}, {"id": 1029, "seek": 499754, "start": 5023.18, "end": 5026.18, "text": " That's what is happening here when we do subclassing.", "tokens": [51646, 663, 311, 437, 307, 2737, 510, 562, 321, 360, 1422, 11665, 278, 13, 51796], "temperature": 0.0, "avg_logprob": -0.22673304875691733, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.0003199978091288358}, {"id": 1030, "seek": 502618, "start": 5026.26, "end": 5030.780000000001, "text": " It's basically copying and pasting all that code in there for us.", "tokens": [50368, 467, 311, 1936, 27976, 293, 1791, 278, 439, 300, 3089, 294, 456, 337, 505, 13, 50594], "temperature": 0.0, "avg_logprob": -0.33009432129940747, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007672885549254715}, {"id": 1031, "seek": 502618, "start": 5030.780000000001, "end": 5032.9800000000005, "text": " It's actually more powerful than that.", "tokens": [50594, 467, 311, 767, 544, 4005, 813, 300, 13, 50704], "temperature": 0.0, "avg_logprob": -0.33009432129940747, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007672885549254715}, {"id": 1032, "seek": 502618, "start": 5032.9800000000005, "end": 5033.9800000000005, "text": " There's more we can do with it.", "tokens": [50704, 821, 311, 544, 321, 393, 360, 365, 309, 13, 50754], "temperature": 0.0, "avg_logprob": -0.33009432129940747, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007672885549254715}, {"id": 1033, "seek": 502618, "start": 5033.9800000000005, "end": 5037.780000000001, "text": " But in this case, this is all that's happening with this subclassing.", "tokens": [50754, 583, 294, 341, 1389, 11, 341, 307, 439, 300, 311, 2737, 365, 341, 1422, 11665, 278, 13, 50944], "temperature": 0.0, "avg_logprob": -0.33009432129940747, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007672885549254715}, {"id": 1034, "seek": 502618, "start": 5037.780000000001, "end": 5039.34, "text": " And this is called...", "tokens": [50944, 400, 341, 307, 1219, 485, 51022], "temperature": 0.0, "avg_logprob": -0.33009432129940747, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007672885549254715}, {"id": 1035, "seek": 502618, "start": 5039.34, "end": 5041.22, "text": " Actually, I'll leave that.", "tokens": [51022, 5135, 11, 286, 603, 1856, 300, 13, 51116], "temperature": 0.0, "avg_logprob": -0.33009432129940747, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007672885549254715}, {"id": 1036, "seek": 502618, "start": 5041.22, "end": 5042.700000000001, "text": " That's fine.", "tokens": [51116, 663, 311, 2489, 13, 51190], "temperature": 0.0, "avg_logprob": -0.33009432129940747, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007672885549254715}, {"id": 1037, "seek": 502618, "start": 5042.700000000001, "end": 5047.780000000001, "text": " Okay, so the accuracy metric is here.", "tokens": [51190, 1033, 11, 370, 264, 14170, 20678, 307, 510, 13, 51444], "temperature": 0.0, "avg_logprob": -0.33009432129940747, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007672885549254715}, {"id": 1038, "seek": 502618, "start": 5047.780000000001, "end": 5051.54, "text": " And then this is kind of our really basic metric, which is we're going to use just for", "tokens": [51444, 400, 550, 341, 307, 733, 295, 527, 534, 3875, 20678, 11, 597, 307, 321, 434, 516, 281, 764, 445, 337, 51632], "temperature": 0.0, "avg_logprob": -0.33009432129940747, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007672885549254715}, {"id": 1039, "seek": 502618, "start": 5051.54, "end": 5053.42, "text": " loss.", "tokens": [51632, 4470, 13, 51726], "temperature": 0.0, "avg_logprob": -0.33009432129940747, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007672885549254715}, {"id": 1040, "seek": 505342, "start": 5053.42, "end": 5057.74, "text": " And so what happens is we're going to...", "tokens": [50364, 400, 370, 437, 2314, 307, 321, 434, 516, 281, 485, 50580], "temperature": 0.0, "avg_logprob": -0.2717032769713739, "compression_ratio": 1.7836538461538463, "no_speech_prob": 3.8229089113883674e-05}, {"id": 1041, "seek": 505342, "start": 5057.74, "end": 5062.22, "text": " Let's for example create an accuracy metric object.", "tokens": [50580, 961, 311, 337, 1365, 1884, 364, 14170, 20678, 2657, 13, 50804], "temperature": 0.0, "avg_logprob": -0.2717032769713739, "compression_ratio": 1.7836538461538463, "no_speech_prob": 3.8229089113883674e-05}, {"id": 1042, "seek": 505342, "start": 5062.22, "end": 5066.02, "text": " We're basically going to add in many batches of data.", "tokens": [50804, 492, 434, 1936, 516, 281, 909, 294, 867, 15245, 279, 295, 1412, 13, 50994], "temperature": 0.0, "avg_logprob": -0.2717032769713739, "compression_ratio": 1.7836538461538463, "no_speech_prob": 3.8229089113883674e-05}, {"id": 1043, "seek": 505342, "start": 5066.02, "end": 5070.8, "text": " So for example, here's a many batches of inputs and predictions.", "tokens": [50994, 407, 337, 1365, 11, 510, 311, 257, 867, 15245, 279, 295, 15743, 293, 21264, 13, 51233], "temperature": 0.0, "avg_logprob": -0.2717032769713739, "compression_ratio": 1.7836538461538463, "no_speech_prob": 3.8229089113883674e-05}, {"id": 1044, "seek": 505342, "start": 5070.8, "end": 5074.06, "text": " Here's another many batch of inputs and predictions.", "tokens": [51233, 1692, 311, 1071, 867, 15245, 295, 15743, 293, 21264, 13, 51396], "temperature": 0.0, "avg_logprob": -0.2717032769713739, "compression_ratio": 1.7836538461538463, "no_speech_prob": 3.8229089113883674e-05}, {"id": 1045, "seek": 505342, "start": 5074.06, "end": 5079.62, "text": " And then we're going to call .value and it will calculate the accuracy.", "tokens": [51396, 400, 550, 321, 434, 516, 281, 818, 2411, 29155, 293, 309, 486, 8873, 264, 14170, 13, 51674], "temperature": 0.0, "avg_logprob": -0.2717032769713739, "compression_ratio": 1.7836538461538463, "no_speech_prob": 3.8229089113883674e-05}, {"id": 1046, "seek": 505342, "start": 5079.62, "end": 5082.5, "text": " Now .value is a neat little thing.", "tokens": [51674, 823, 2411, 29155, 307, 257, 10654, 707, 551, 13, 51818], "temperature": 0.0, "avg_logprob": -0.2717032769713739, "compression_ratio": 1.7836538461538463, "no_speech_prob": 3.8229089113883674e-05}, {"id": 1047, "seek": 508250, "start": 5082.58, "end": 5087.1, "text": " It doesn't require parentheses after it because it's called a property.", "tokens": [50368, 467, 1177, 380, 3651, 34153, 934, 309, 570, 309, 311, 1219, 257, 4707, 13, 50594], "temperature": 0.0, "avg_logprob": -0.23764528012743183, "compression_ratio": 1.740909090909091, "no_speech_prob": 5.391081504058093e-05}, {"id": 1048, "seek": 508250, "start": 5087.1, "end": 5092.94, "text": " And so a property is something that just calculates automatically without having to put parentheses.", "tokens": [50594, 400, 370, 257, 4707, 307, 746, 300, 445, 4322, 1024, 6772, 1553, 1419, 281, 829, 34153, 13, 50886], "temperature": 0.0, "avg_logprob": -0.23764528012743183, "compression_ratio": 1.740909090909091, "no_speech_prob": 5.391081504058093e-05}, {"id": 1049, "seek": 508250, "start": 5092.94, "end": 5095.02, "text": " That's all a property is.", "tokens": [50886, 663, 311, 439, 257, 4707, 307, 13, 50990], "temperature": 0.0, "avg_logprob": -0.23764528012743183, "compression_ratio": 1.740909090909091, "no_speech_prob": 5.391081504058093e-05}, {"id": 1050, "seek": 508250, "start": 5095.02, "end": 5096.9, "text": " Well property getter, anyway.", "tokens": [50990, 1042, 4707, 483, 391, 11, 4033, 13, 51084], "temperature": 0.0, "avg_logprob": -0.23764528012743183, "compression_ratio": 1.740909090909091, "no_speech_prob": 5.391081504058093e-05}, {"id": 1051, "seek": 508250, "start": 5096.9, "end": 5097.9, "text": " And so they look like this.", "tokens": [51084, 400, 370, 436, 574, 411, 341, 13, 51134], "temperature": 0.0, "avg_logprob": -0.23764528012743183, "compression_ratio": 1.740909090909091, "no_speech_prob": 5.391081504058093e-05}, {"id": 1052, "seek": 508250, "start": 5097.9, "end": 5099.7, "text": " You give it a name.", "tokens": [51134, 509, 976, 309, 257, 1315, 13, 51224], "temperature": 0.0, "avg_logprob": -0.23764528012743183, "compression_ratio": 1.740909090909091, "no_speech_prob": 5.391081504058093e-05}, {"id": 1053, "seek": 508250, "start": 5099.7, "end": 5102.78, "text": " And so we are going to be...", "tokens": [51224, 400, 370, 321, 366, 516, 281, 312, 485, 51378], "temperature": 0.0, "avg_logprob": -0.23764528012743183, "compression_ratio": 1.740909090909091, "no_speech_prob": 5.391081504058093e-05}, {"id": 1054, "seek": 508250, "start": 5102.78, "end": 5111.94, "text": " Each time we call add, we are going to be storing that input and that target.", "tokens": [51378, 6947, 565, 321, 818, 909, 11, 321, 366, 516, 281, 312, 26085, 300, 4846, 293, 300, 3779, 13, 51836], "temperature": 0.0, "avg_logprob": -0.23764528012743183, "compression_ratio": 1.740909090909091, "no_speech_prob": 5.391081504058093e-05}, {"id": 1055, "seek": 511194, "start": 5112.379999999999, "end": 5117.299999999999, "text": " And also the number of items in the many batch, optionally.", "tokens": [50386, 400, 611, 264, 1230, 295, 4754, 294, 264, 867, 15245, 11, 3614, 379, 13, 50632], "temperature": 0.0, "avg_logprob": -0.2914094497908407, "compression_ratio": 1.4430379746835442, "no_speech_prob": 4.2647443478927016e-05}, {"id": 1056, "seek": 511194, "start": 5117.299999999999, "end": 5121.46, "text": " For now that's just always going to be one.", "tokens": [50632, 1171, 586, 300, 311, 445, 1009, 516, 281, 312, 472, 13, 50840], "temperature": 0.0, "avg_logprob": -0.2914094497908407, "compression_ratio": 1.4430379746835442, "no_speech_prob": 4.2647443478927016e-05}, {"id": 1057, "seek": 511194, "start": 5121.46, "end": 5129.04, "text": " And you can see here that we then call .calc, which is going to call the accuracy calc.", "tokens": [50840, 400, 291, 393, 536, 510, 300, 321, 550, 818, 2411, 9895, 66, 11, 597, 307, 516, 281, 818, 264, 14170, 2104, 66, 13, 51219], "temperature": 0.0, "avg_logprob": -0.2914094497908407, "compression_ratio": 1.4430379746835442, "no_speech_prob": 4.2647443478927016e-05}, {"id": 1058, "seek": 511194, "start": 5129.04, "end": 5134.98, "text": " So just see how often they're equal.", "tokens": [51219, 407, 445, 536, 577, 2049, 436, 434, 2681, 13, 51516], "temperature": 0.0, "avg_logprob": -0.2914094497908407, "compression_ratio": 1.4430379746835442, "no_speech_prob": 4.2647443478927016e-05}, {"id": 1059, "seek": 513498, "start": 5134.98, "end": 5143.24, "text": " And then we're going to append to the list of values that calculation.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 34116, 281, 264, 1329, 295, 4190, 300, 17108, 13, 50777], "temperature": 0.0, "avg_logprob": -0.25750182656680837, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.006289689335972071}, {"id": 1060, "seek": 513498, "start": 5143.24, "end": 5145.54, "text": " And we're also going to append to the list of ends.", "tokens": [50777, 400, 321, 434, 611, 516, 281, 34116, 281, 264, 1329, 295, 5314, 13, 50892], "temperature": 0.0, "avg_logprob": -0.25750182656680837, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.006289689335972071}, {"id": 1061, "seek": 513498, "start": 5145.54, "end": 5147.78, "text": " In this case just one.", "tokens": [50892, 682, 341, 1389, 445, 472, 13, 51004], "temperature": 0.0, "avg_logprob": -0.25750182656680837, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.006289689335972071}, {"id": 1062, "seek": 513498, "start": 5147.78, "end": 5152.7, "text": " And so then to calculate the value, we just do that.", "tokens": [51004, 400, 370, 550, 281, 8873, 264, 2158, 11, 321, 445, 360, 300, 13, 51250], "temperature": 0.0, "avg_logprob": -0.25750182656680837, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.006289689335972071}, {"id": 1063, "seek": 513498, "start": 5152.7, "end": 5155.5199999999995, "text": " So that's all that's happening for accuracy.", "tokens": [51250, 407, 300, 311, 439, 300, 311, 2737, 337, 14170, 13, 51391], "temperature": 0.0, "avg_logprob": -0.25750182656680837, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.006289689335972071}, {"id": 1064, "seek": 513498, "start": 5155.5199999999995, "end": 5159.379999999999, "text": " And then we can do for loss, we can just use metric directly.", "tokens": [51391, 400, 550, 321, 393, 360, 337, 4470, 11, 321, 393, 445, 764, 20678, 3838, 13, 51584], "temperature": 0.0, "avg_logprob": -0.25750182656680837, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.006289689335972071}, {"id": 1065, "seek": 513498, "start": 5159.379999999999, "end": 5163.459999999999, "text": " Because metric directly will just calculate the average of whatever it's passed.", "tokens": [51584, 1436, 20678, 3838, 486, 445, 8873, 264, 4274, 295, 2035, 309, 311, 4678, 13, 51788], "temperature": 0.0, "avg_logprob": -0.25750182656680837, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.006289689335972071}, {"id": 1066, "seek": 516346, "start": 5163.46, "end": 5168.5, "text": " So we can say, oh, add the number 0.6, so the target's optional.", "tokens": [50364, 407, 321, 393, 584, 11, 1954, 11, 909, 264, 1230, 1958, 13, 21, 11, 370, 264, 3779, 311, 17312, 13, 50616], "temperature": 0.0, "avg_logprob": -0.2555853366851807, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.008315249346196651}, {"id": 1067, "seek": 516346, "start": 5168.5, "end": 5171.46, "text": " And we're saying this is a mini batch of size 32.", "tokens": [50616, 400, 321, 434, 1566, 341, 307, 257, 8382, 15245, 295, 2744, 8858, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2555853366851807, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.008315249346196651}, {"id": 1068, "seek": 516346, "start": 5171.46, "end": 5173.02, "text": " So that's going to be the end.", "tokens": [50764, 407, 300, 311, 516, 281, 312, 264, 917, 13, 50842], "temperature": 0.0, "avg_logprob": -0.2555853366851807, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.008315249346196651}, {"id": 1069, "seek": 516346, "start": 5173.02, "end": 5177.94, "text": " And then add the value 0.9 with a mini batch size of 2.", "tokens": [50842, 400, 550, 909, 264, 2158, 1958, 13, 24, 365, 257, 8382, 15245, 2744, 295, 568, 13, 51088], "temperature": 0.0, "avg_logprob": -0.2555853366851807, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.008315249346196651}, {"id": 1070, "seek": 516346, "start": 5177.94, "end": 5179.72, "text": " And then get the value.", "tokens": [51088, 400, 550, 483, 264, 2158, 13, 51177], "temperature": 0.0, "avg_logprob": -0.2555853366851807, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.008315249346196651}, {"id": 1071, "seek": 516346, "start": 5179.72, "end": 5186.26, "text": " And as you can see, that's exactly the same as the weighted average of 0.6 and 0.9 with", "tokens": [51177, 400, 382, 291, 393, 536, 11, 300, 311, 2293, 264, 912, 382, 264, 32807, 4274, 295, 1958, 13, 21, 293, 1958, 13, 24, 365, 51504], "temperature": 0.0, "avg_logprob": -0.2555853366851807, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.008315249346196651}, {"id": 1072, "seek": 516346, "start": 5186.26, "end": 5188.9800000000005, "text": " weights of 32 and 2.", "tokens": [51504, 17443, 295, 8858, 293, 568, 13, 51640], "temperature": 0.0, "avg_logprob": -0.2555853366851807, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.008315249346196651}, {"id": 1073, "seek": 516346, "start": 5188.9800000000005, "end": 5190.82, "text": " So we've created a metric class.", "tokens": [51640, 407, 321, 600, 2942, 257, 20678, 1508, 13, 51732], "temperature": 0.0, "avg_logprob": -0.2555853366851807, "compression_ratio": 1.6912442396313363, "no_speech_prob": 0.008315249346196651}, {"id": 1074, "seek": 519082, "start": 5190.86, "end": 5196.66, "text": " And so that's something that we can use to create any metric we like just by overriding", "tokens": [50366, 400, 370, 300, 311, 746, 300, 321, 393, 764, 281, 1884, 604, 20678, 321, 411, 445, 538, 670, 81, 2819, 50656], "temperature": 0.0, "avg_logprob": -0.26475547608875094, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.0004955264157615602}, {"id": 1075, "seek": 519082, "start": 5196.66, "end": 5199.299999999999, "text": " calc.", "tokens": [50656, 2104, 66, 13, 50788], "temperature": 0.0, "avg_logprob": -0.26475547608875094, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.0004955264157615602}, {"id": 1076, "seek": 519082, "start": 5199.299999999999, "end": 5205.66, "text": " Or we could create totally things from scratch as long as they have an add and a value.", "tokens": [50788, 1610, 321, 727, 1884, 3879, 721, 490, 8459, 382, 938, 382, 436, 362, 364, 909, 293, 257, 2158, 13, 51106], "temperature": 0.0, "avg_logprob": -0.26475547608875094, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.0004955264157615602}, {"id": 1077, "seek": 519082, "start": 5205.66, "end": 5213.82, "text": " Okay, so we're now going to change our learner.", "tokens": [51106, 1033, 11, 370, 321, 434, 586, 516, 281, 1319, 527, 33347, 13, 51514], "temperature": 0.0, "avg_logprob": -0.26475547608875094, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.0004955264157615602}, {"id": 1078, "seek": 519082, "start": 5213.82, "end": 5219.74, "text": " And what we're going to do is we're going to keep the same basic structure.", "tokens": [51514, 400, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1066, 264, 912, 3875, 3877, 13, 51810], "temperature": 0.0, "avg_logprob": -0.26475547608875094, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.0004955264157615602}, {"id": 1079, "seek": 521974, "start": 5219.74, "end": 5220.74, "text": " So there's going to be fit.", "tokens": [50364, 407, 456, 311, 516, 281, 312, 3318, 13, 50414], "temperature": 0.0, "avg_logprob": -0.24027979255902884, "compression_ratio": 1.898936170212766, "no_speech_prob": 0.00844552181661129}, {"id": 1080, "seek": 521974, "start": 5220.74, "end": 5223.0199999999995, "text": " It's going to go through each epoch.", "tokens": [50414, 467, 311, 516, 281, 352, 807, 1184, 30992, 339, 13, 50528], "temperature": 0.0, "avg_logprob": -0.24027979255902884, "compression_ratio": 1.898936170212766, "no_speech_prob": 0.00844552181661129}, {"id": 1081, "seek": 521974, "start": 5223.0199999999995, "end": 5229.58, "text": " It's going to call one epoch passing in true and false as for training and validation.", "tokens": [50528, 467, 311, 516, 281, 818, 472, 30992, 339, 8437, 294, 2074, 293, 7908, 382, 337, 3097, 293, 24071, 13, 50856], "temperature": 0.0, "avg_logprob": -0.24027979255902884, "compression_ratio": 1.898936170212766, "no_speech_prob": 0.00844552181661129}, {"id": 1082, "seek": 521974, "start": 5229.58, "end": 5237.44, "text": " One epoch is going to go through each batch in the data loader and call one batch.", "tokens": [50856, 1485, 30992, 339, 307, 516, 281, 352, 807, 1184, 15245, 294, 264, 1412, 3677, 260, 293, 818, 472, 15245, 13, 51249], "temperature": 0.0, "avg_logprob": -0.24027979255902884, "compression_ratio": 1.898936170212766, "no_speech_prob": 0.00844552181661129}, {"id": 1083, "seek": 521974, "start": 5237.44, "end": 5241.86, "text": " One batch is going to do the prediction, get loss.", "tokens": [51249, 1485, 15245, 307, 516, 281, 360, 264, 17630, 11, 483, 4470, 13, 51470], "temperature": 0.0, "avg_logprob": -0.24027979255902884, "compression_ratio": 1.898936170212766, "no_speech_prob": 0.00844552181661129}, {"id": 1084, "seek": 521974, "start": 5241.86, "end": 5247.82, "text": " And if it's training, it's going to do the backward step and zero grad.", "tokens": [51470, 400, 498, 309, 311, 3097, 11, 309, 311, 516, 281, 360, 264, 23897, 1823, 293, 4018, 2771, 13, 51768], "temperature": 0.0, "avg_logprob": -0.24027979255902884, "compression_ratio": 1.898936170212766, "no_speech_prob": 0.00844552181661129}, {"id": 1085, "seek": 524782, "start": 5247.9, "end": 5250.7, "text": " But there's a few other things going on.", "tokens": [50368, 583, 456, 311, 257, 1326, 661, 721, 516, 322, 13, 50508], "temperature": 0.0, "avg_logprob": -0.2616236243449466, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.013427755795419216}, {"id": 1086, "seek": 524782, "start": 5250.7, "end": 5253.099999999999, "text": " So let's take a look.", "tokens": [50508, 407, 718, 311, 747, 257, 574, 13, 50628], "temperature": 0.0, "avg_logprob": -0.2616236243449466, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.013427755795419216}, {"id": 1087, "seek": 524782, "start": 5253.099999999999, "end": 5255.98, "text": " Well, actually, let's just look at it in use first.", "tokens": [50628, 1042, 11, 767, 11, 718, 311, 445, 574, 412, 309, 294, 764, 700, 13, 50772], "temperature": 0.0, "avg_logprob": -0.2616236243449466, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.013427755795419216}, {"id": 1088, "seek": 524782, "start": 5255.98, "end": 5262.42, "text": " So when we use it, we're going to be creating a learner with the model data loaders, loss", "tokens": [50772, 407, 562, 321, 764, 309, 11, 321, 434, 516, 281, 312, 4084, 257, 33347, 365, 264, 2316, 1412, 3677, 433, 11, 4470, 51094], "temperature": 0.0, "avg_logprob": -0.2616236243449466, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.013427755795419216}, {"id": 1089, "seek": 524782, "start": 5262.42, "end": 5266.219999999999, "text": " function, learning rate, and some callbacks, which we'll learn about in a moment.", "tokens": [51094, 2445, 11, 2539, 3314, 11, 293, 512, 818, 17758, 11, 597, 321, 603, 1466, 466, 294, 257, 1623, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2616236243449466, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.013427755795419216}, {"id": 1090, "seek": 524782, "start": 5266.219999999999, "end": 5267.74, "text": " And we call fit.", "tokens": [51284, 400, 321, 818, 3318, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2616236243449466, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.013427755795419216}, {"id": 1091, "seek": 524782, "start": 5267.74, "end": 5268.74, "text": " And it's going to do our thing.", "tokens": [51360, 400, 309, 311, 516, 281, 360, 527, 551, 13, 51410], "temperature": 0.0, "avg_logprob": -0.2616236243449466, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.013427755795419216}, {"id": 1092, "seek": 524782, "start": 5268.74, "end": 5270.34, "text": " And look, we're going to have charts and stuff.", "tokens": [51410, 400, 574, 11, 321, 434, 516, 281, 362, 17767, 293, 1507, 13, 51490], "temperature": 0.0, "avg_logprob": -0.2616236243449466, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.013427755795419216}, {"id": 1093, "seek": 524782, "start": 5270.34, "end": 5273.259999999999, "text": " All right, so the basic idea is going to look very similar.", "tokens": [51490, 1057, 558, 11, 370, 264, 3875, 1558, 307, 516, 281, 574, 588, 2531, 13, 51636], "temperature": 0.0, "avg_logprob": -0.2616236243449466, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.013427755795419216}, {"id": 1094, "seek": 524782, "start": 5273.259999999999, "end": 5277.0599999999995, "text": " So we're going to call fit.", "tokens": [51636, 407, 321, 434, 516, 281, 818, 3318, 13, 51826], "temperature": 0.0, "avg_logprob": -0.2616236243449466, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.013427755795419216}, {"id": 1095, "seek": 527706, "start": 5277.3, "end": 5281.820000000001, "text": " So when we construct it, we're going to be passing in exactly the same things as before.", "tokens": [50376, 407, 562, 321, 7690, 309, 11, 321, 434, 516, 281, 312, 8437, 294, 2293, 264, 912, 721, 382, 949, 13, 50602], "temperature": 0.0, "avg_logprob": -0.2152310162782669, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.0003150381671730429}, {"id": 1096, "seek": 527706, "start": 5281.820000000001, "end": 5286.740000000001, "text": " But we've got one extra thing, callbacks, which we'll see in a moment.", "tokens": [50602, 583, 321, 600, 658, 472, 2857, 551, 11, 818, 17758, 11, 597, 321, 603, 536, 294, 257, 1623, 13, 50848], "temperature": 0.0, "avg_logprob": -0.2152310162782669, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.0003150381671730429}, {"id": 1097, "seek": 527706, "start": 5286.740000000001, "end": 5289.5, "text": " Store the attributes as before.", "tokens": [50848, 17242, 264, 17212, 382, 949, 13, 50986], "temperature": 0.0, "avg_logprob": -0.2152310162782669, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.0003150381671730429}, {"id": 1098, "seek": 527706, "start": 5289.5, "end": 5291.860000000001, "text": " And we're going to be doing some stuff with the callbacks.", "tokens": [50986, 400, 321, 434, 516, 281, 312, 884, 512, 1507, 365, 264, 818, 17758, 13, 51104], "temperature": 0.0, "avg_logprob": -0.2152310162782669, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.0003150381671730429}, {"id": 1099, "seek": 527706, "start": 5291.860000000001, "end": 5298.46, "text": " So when we call fit for this number of epochs, we're going to store away how many epochs", "tokens": [51104, 407, 562, 321, 818, 3318, 337, 341, 1230, 295, 30992, 28346, 11, 321, 434, 516, 281, 3531, 1314, 577, 867, 30992, 28346, 51434], "temperature": 0.0, "avg_logprob": -0.2152310162782669, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.0003150381671730429}, {"id": 1100, "seek": 527706, "start": 5298.46, "end": 5300.06, "text": " we're going to do.", "tokens": [51434, 321, 434, 516, 281, 360, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2152310162782669, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.0003150381671730429}, {"id": 1101, "seek": 527706, "start": 5300.06, "end": 5305.02, "text": " We're also going to store away the actual range that we're going to loop through as", "tokens": [51514, 492, 434, 611, 516, 281, 3531, 1314, 264, 3539, 3613, 300, 321, 434, 516, 281, 6367, 807, 382, 51762], "temperature": 0.0, "avg_logprob": -0.2152310162782669, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.0003150381671730429}, {"id": 1102, "seek": 527706, "start": 5305.02, "end": 5306.02, "text": " self.epoch.", "tokens": [51762, 2698, 13, 595, 8997, 13, 51812], "temperature": 0.0, "avg_logprob": -0.2152310162782669, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.0003150381671730429}, {"id": 1103, "seek": 530602, "start": 5306.02, "end": 5309.660000000001, "text": " So here's that looping through self.epoch.", "tokens": [50364, 407, 510, 311, 300, 6367, 278, 807, 2698, 13, 595, 8997, 13, 50546], "temperature": 0.0, "avg_logprob": -0.22485921716177334, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.805851024982985e-05}, {"id": 1104, "seek": 530602, "start": 5309.660000000001, "end": 5319.860000000001, "text": " We're going to create the optimizer using the optimizer function and the parameters.", "tokens": [50546, 492, 434, 516, 281, 1884, 264, 5028, 6545, 1228, 264, 5028, 6545, 2445, 293, 264, 9834, 13, 51056], "temperature": 0.0, "avg_logprob": -0.22485921716177334, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.805851024982985e-05}, {"id": 1105, "seek": 530602, "start": 5319.860000000001, "end": 5321.540000000001, "text": " And then we're going to call underscore fit.", "tokens": [51056, 400, 550, 321, 434, 516, 281, 818, 37556, 3318, 13, 51140], "temperature": 0.0, "avg_logprob": -0.22485921716177334, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.805851024982985e-05}, {"id": 1106, "seek": 530602, "start": 5321.540000000001, "end": 5323.26, "text": " Now what on earth is underscore fit?", "tokens": [51140, 823, 437, 322, 4120, 307, 37556, 3318, 30, 51226], "temperature": 0.0, "avg_logprob": -0.22485921716177334, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.805851024982985e-05}, {"id": 1107, "seek": 530602, "start": 5323.26, "end": 5327.860000000001, "text": " Why didn't we just copy and paste this into here?", "tokens": [51226, 1545, 994, 380, 321, 445, 5055, 293, 9163, 341, 666, 510, 30, 51456], "temperature": 0.0, "avg_logprob": -0.22485921716177334, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.805851024982985e-05}, {"id": 1108, "seek": 530602, "start": 5327.860000000001, "end": 5329.22, "text": " Why do this?", "tokens": [51456, 1545, 360, 341, 30, 51524], "temperature": 0.0, "avg_logprob": -0.22485921716177334, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.805851024982985e-05}, {"id": 1109, "seek": 530602, "start": 5329.22, "end": 5335.46, "text": " It's because we've created this special decorator with callbacks.", "tokens": [51524, 467, 311, 570, 321, 600, 2942, 341, 2121, 7919, 1639, 365, 818, 17758, 13, 51836], "temperature": 0.0, "avg_logprob": -0.22485921716177334, "compression_ratio": 1.6568627450980393, "no_speech_prob": 1.805851024982985e-05}, {"id": 1110, "seek": 533546, "start": 5335.9, "end": 5337.02, "text": " So what does that do?", "tokens": [50386, 407, 437, 775, 300, 360, 30, 50442], "temperature": 0.0, "avg_logprob": -0.254206298499979, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.00013765455514658242}, {"id": 1111, "seek": 533546, "start": 5337.02, "end": 5340.9, "text": " So it's up here with callbacks.", "tokens": [50442, 407, 309, 311, 493, 510, 365, 818, 17758, 13, 50636], "temperature": 0.0, "avg_logprob": -0.254206298499979, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.00013765455514658242}, {"id": 1112, "seek": 533546, "start": 5340.9, "end": 5343.8, "text": " With callbacks is a class.", "tokens": [50636, 2022, 818, 17758, 307, 257, 1508, 13, 50781], "temperature": 0.0, "avg_logprob": -0.254206298499979, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.00013765455514658242}, {"id": 1113, "seek": 533546, "start": 5343.8, "end": 5347.02, "text": " It's going to just store one thing, which is the name.", "tokens": [50781, 467, 311, 516, 281, 445, 3531, 472, 551, 11, 597, 307, 264, 1315, 13, 50942], "temperature": 0.0, "avg_logprob": -0.254206298499979, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.00013765455514658242}, {"id": 1114, "seek": 533546, "start": 5347.02, "end": 5352.62, "text": " In this case, the name is fit.", "tokens": [50942, 682, 341, 1389, 11, 264, 1315, 307, 3318, 13, 51222], "temperature": 0.0, "avg_logprob": -0.254206298499979, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.00013765455514658242}, {"id": 1115, "seek": 533546, "start": 5352.62, "end": 5359.36, "text": " And what it's going to do is, now this is the decorator, right?", "tokens": [51222, 400, 437, 309, 311, 516, 281, 360, 307, 11, 586, 341, 307, 264, 7919, 1639, 11, 558, 30, 51559], "temperature": 0.0, "avg_logprob": -0.254206298499979, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.00013765455514658242}, {"id": 1116, "seek": 533546, "start": 5359.36, "end": 5365.22, "text": " So when we call it, remember decorators get passed a function.", "tokens": [51559, 407, 562, 321, 818, 309, 11, 1604, 7919, 3391, 483, 4678, 257, 2445, 13, 51852], "temperature": 0.0, "avg_logprob": -0.254206298499979, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.00013765455514658242}, {"id": 1117, "seek": 536522, "start": 5365.9800000000005, "end": 5369.14, "text": " So it's going to get passed this whole function.", "tokens": [50402, 407, 309, 311, 516, 281, 483, 4678, 341, 1379, 2445, 13, 50560], "temperature": 0.0, "avg_logprob": -0.24989772665089574, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1118, "seek": 536522, "start": 5369.14, "end": 5370.14, "text": " And that's going to be called f.", "tokens": [50560, 400, 300, 311, 516, 281, 312, 1219, 283, 13, 50610], "temperature": 0.0, "avg_logprob": -0.24989772665089574, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1119, "seek": 536522, "start": 5370.14, "end": 5376.02, "text": " So dunder call, remember, is what happens when a class is treated, an object is treated", "tokens": [50610, 407, 274, 6617, 818, 11, 1604, 11, 307, 437, 2314, 562, 257, 1508, 307, 8668, 11, 364, 2657, 307, 8668, 50904], "temperature": 0.0, "avg_logprob": -0.24989772665089574, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1120, "seek": 536522, "start": 5376.02, "end": 5377.62, "text": " as if it's a function.", "tokens": [50904, 382, 498, 309, 311, 257, 2445, 13, 50984], "temperature": 0.0, "avg_logprob": -0.24989772665089574, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1121, "seek": 536522, "start": 5377.62, "end": 5379.5, "text": " So it's going to get passed this function.", "tokens": [50984, 407, 309, 311, 516, 281, 483, 4678, 341, 2445, 13, 51078], "temperature": 0.0, "avg_logprob": -0.24989772665089574, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1122, "seek": 536522, "start": 5379.5, "end": 5382.56, "text": " So this function is underscore fit.", "tokens": [51078, 407, 341, 2445, 307, 37556, 3318, 13, 51231], "temperature": 0.0, "avg_logprob": -0.24989772665089574, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1123, "seek": 536522, "start": 5382.56, "end": 5386.52, "text": " And so what we want to do is we want to return a different function.", "tokens": [51231, 400, 370, 437, 321, 528, 281, 360, 307, 321, 528, 281, 2736, 257, 819, 2445, 13, 51429], "temperature": 0.0, "avg_logprob": -0.24989772665089574, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1124, "seek": 536522, "start": 5386.52, "end": 5393.06, "text": " It's going to of course call the function that we were asked to call using the arguments", "tokens": [51429, 467, 311, 516, 281, 295, 1164, 818, 264, 2445, 300, 321, 645, 2351, 281, 818, 1228, 264, 12869, 51756], "temperature": 0.0, "avg_logprob": -0.24989772665089574, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1125, "seek": 539306, "start": 5393.06, "end": 5396.22, "text": " and keyword arguments we were asked to use.", "tokens": [50364, 293, 20428, 12869, 321, 645, 2351, 281, 764, 13, 50522], "temperature": 0.0, "avg_logprob": -0.26126467927973324, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.00048029684694483876}, {"id": 1126, "seek": 539306, "start": 5396.22, "end": 5403.620000000001, "text": " But before it calls that function, it's going to call a special method called callback,", "tokens": [50522, 583, 949, 309, 5498, 300, 2445, 11, 309, 311, 516, 281, 818, 257, 2121, 3170, 1219, 818, 3207, 11, 50892], "temperature": 0.0, "avg_logprob": -0.26126467927973324, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.00048029684694483876}, {"id": 1127, "seek": 539306, "start": 5403.620000000001, "end": 5409.22, "text": " passing in the string before, in this case, before underscore fit.", "tokens": [50892, 8437, 294, 264, 6798, 949, 11, 294, 341, 1389, 11, 949, 37556, 3318, 13, 51172], "temperature": 0.0, "avg_logprob": -0.26126467927973324, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.00048029684694483876}, {"id": 1128, "seek": 539306, "start": 5409.22, "end": 5414.080000000001, "text": " After it's completed, it's going to call that method called callback and passing the string", "tokens": [51172, 2381, 309, 311, 7365, 11, 309, 311, 516, 281, 818, 300, 3170, 1219, 818, 3207, 293, 8437, 264, 6798, 51415], "temperature": 0.0, "avg_logprob": -0.26126467927973324, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.00048029684694483876}, {"id": 1129, "seek": 539306, "start": 5414.080000000001, "end": 5417.1, "text": " after underscore fit.", "tokens": [51415, 934, 37556, 3318, 13, 51566], "temperature": 0.0, "avg_logprob": -0.26126467927973324, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.00048029684694483876}, {"id": 1130, "seek": 539306, "start": 5417.1, "end": 5421.240000000001, "text": " And it's going to wrap the whole thing in a try except block.", "tokens": [51566, 400, 309, 311, 516, 281, 7019, 264, 1379, 551, 294, 257, 853, 3993, 3461, 13, 51773], "temperature": 0.0, "avg_logprob": -0.26126467927973324, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.00048029684694483876}, {"id": 1131, "seek": 542124, "start": 5421.24, "end": 5429.86, "text": " And it's going to be looking for an exception called cancel fit exception.", "tokens": [50364, 400, 309, 311, 516, 281, 312, 1237, 337, 364, 11183, 1219, 10373, 3318, 11183, 13, 50795], "temperature": 0.0, "avg_logprob": -0.1947853300306532, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.001000498770736158}, {"id": 1132, "seek": 542124, "start": 5429.86, "end": 5433.04, "text": " And if it gets one, it's not going to complain.", "tokens": [50795, 400, 498, 309, 2170, 472, 11, 309, 311, 406, 516, 281, 11024, 13, 50954], "temperature": 0.0, "avg_logprob": -0.1947853300306532, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.001000498770736158}, {"id": 1133, "seek": 542124, "start": 5433.04, "end": 5437.28, "text": " So let me explain what's going on with all of those things.", "tokens": [50954, 407, 718, 385, 2903, 437, 311, 516, 322, 365, 439, 295, 729, 721, 13, 51166], "temperature": 0.0, "avg_logprob": -0.1947853300306532, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.001000498770736158}, {"id": 1134, "seek": 542124, "start": 5437.28, "end": 5445.16, "text": " Let's look at an example of a callback.", "tokens": [51166, 961, 311, 574, 412, 364, 1365, 295, 257, 818, 3207, 13, 51560], "temperature": 0.0, "avg_logprob": -0.1947853300306532, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.001000498770736158}, {"id": 1135, "seek": 544516, "start": 5445.16, "end": 5451.28, "text": " So for example, here is a callback called device CB, device callback.", "tokens": [50364, 407, 337, 1365, 11, 510, 307, 257, 818, 3207, 1219, 4302, 18745, 11, 4302, 818, 3207, 13, 50670], "temperature": 0.0, "avg_logprob": -0.25962728477386104, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.0023964939173310995}, {"id": 1136, "seek": 544516, "start": 5451.28, "end": 5458.44, "text": " And before fit will be called automatically before that underscore fit method is called.", "tokens": [50670, 400, 949, 3318, 486, 312, 1219, 6772, 949, 300, 37556, 3318, 3170, 307, 1219, 13, 51028], "temperature": 0.0, "avg_logprob": -0.25962728477386104, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.0023964939173310995}, {"id": 1137, "seek": 544516, "start": 5458.44, "end": 5466.88, "text": " And it's going to put the model onto our device, CUDA or MPS, if we have one.", "tokens": [51028, 400, 309, 311, 516, 281, 829, 264, 2316, 3911, 527, 4302, 11, 29777, 7509, 420, 376, 6273, 11, 498, 321, 362, 472, 13, 51450], "temperature": 0.0, "avg_logprob": -0.25962728477386104, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.0023964939173310995}, {"id": 1138, "seek": 544516, "start": 5466.88, "end": 5471.46, "text": " Otherwise it'll just be on GPU.", "tokens": [51450, 10328, 309, 603, 445, 312, 322, 18407, 13, 51679], "temperature": 0.0, "avg_logprob": -0.25962728477386104, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.0023964939173310995}, {"id": 1139, "seek": 544516, "start": 5471.46, "end": 5472.639999999999, "text": " So what's going to happen here?", "tokens": [51679, 407, 437, 311, 516, 281, 1051, 510, 30, 51738], "temperature": 0.0, "avg_logprob": -0.25962728477386104, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.0023964939173310995}, {"id": 1140, "seek": 547264, "start": 5472.64, "end": 5477.4400000000005, "text": " So it's going to call, we're going to call fit, it's going to go through these lines", "tokens": [50364, 407, 309, 311, 516, 281, 818, 11, 321, 434, 516, 281, 818, 3318, 11, 309, 311, 516, 281, 352, 807, 613, 3876, 50604], "temperature": 0.0, "avg_logprob": -0.2193884295086528, "compression_ratio": 2.125, "no_speech_prob": 0.0018101846799254417}, {"id": 1141, "seek": 547264, "start": 5477.4400000000005, "end": 5481.26, "text": " of code, it's then going to call underscore fit.", "tokens": [50604, 295, 3089, 11, 309, 311, 550, 516, 281, 818, 37556, 3318, 13, 50795], "temperature": 0.0, "avg_logprob": -0.2193884295086528, "compression_ratio": 2.125, "no_speech_prob": 0.0018101846799254417}, {"id": 1142, "seek": 547264, "start": 5481.26, "end": 5484.18, "text": " Underscore fit is not this function.", "tokens": [50795, 2719, 433, 12352, 3318, 307, 406, 341, 2445, 13, 50941], "temperature": 0.0, "avg_logprob": -0.2193884295086528, "compression_ratio": 2.125, "no_speech_prob": 0.0018101846799254417}, {"id": 1143, "seek": 547264, "start": 5484.18, "end": 5491.46, "text": " Underscore fit is this function, where f is this function.", "tokens": [50941, 2719, 433, 12352, 3318, 307, 341, 2445, 11, 689, 283, 307, 341, 2445, 13, 51305], "temperature": 0.0, "avg_logprob": -0.2193884295086528, "compression_ratio": 2.125, "no_speech_prob": 0.0018101846799254417}, {"id": 1144, "seek": 547264, "start": 5491.46, "end": 5499.08, "text": " So it's going to call our learner.callback passing in before underscore fit.", "tokens": [51305, 407, 309, 311, 516, 281, 818, 527, 33347, 13, 45459, 3207, 8437, 294, 949, 37556, 3318, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2193884295086528, "compression_ratio": 2.125, "no_speech_prob": 0.0018101846799254417}, {"id": 1145, "seek": 549908, "start": 5499.08, "end": 5503.94, "text": " And callback is defined here.", "tokens": [50364, 400, 818, 3207, 307, 7642, 510, 13, 50607], "temperature": 0.0, "avg_logprob": -0.2535753921723702, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0034834127873182297}, {"id": 1146, "seek": 549908, "start": 5503.94, "end": 5504.94, "text": " What's callback going to do?", "tokens": [50607, 708, 311, 818, 3207, 516, 281, 360, 30, 50657], "temperature": 0.0, "avg_logprob": -0.2535753921723702, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0034834127873182297}, {"id": 1147, "seek": 549908, "start": 5504.94, "end": 5509.64, "text": " It's going to be past the string before underscore fit.", "tokens": [50657, 467, 311, 516, 281, 312, 1791, 264, 6798, 949, 37556, 3318, 13, 50892], "temperature": 0.0, "avg_logprob": -0.2535753921723702, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0034834127873182297}, {"id": 1148, "seek": 549908, "start": 5509.64, "end": 5517.74, "text": " It's going to then go through each of our callbacks, sorted based on their order.", "tokens": [50892, 467, 311, 516, 281, 550, 352, 807, 1184, 295, 527, 818, 17758, 11, 25462, 2361, 322, 641, 1668, 13, 51297], "temperature": 0.0, "avg_logprob": -0.2535753921723702, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0034834127873182297}, {"id": 1149, "seek": 549908, "start": 5517.74, "end": 5521.88, "text": " And you can see here our callbacks can have an order.", "tokens": [51297, 400, 291, 393, 536, 510, 527, 818, 17758, 393, 362, 364, 1668, 13, 51504], "temperature": 0.0, "avg_logprob": -0.2535753921723702, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0034834127873182297}, {"id": 1150, "seek": 552188, "start": 5521.88, "end": 5529.8, "text": " And it's going to look at that callback and try to get an attribute called before underscore", "tokens": [50364, 400, 309, 311, 516, 281, 574, 412, 300, 818, 3207, 293, 853, 281, 483, 364, 19667, 1219, 949, 37556, 50760], "temperature": 0.0, "avg_logprob": -0.26179291072644684, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.015423398464918137}, {"id": 1151, "seek": 552188, "start": 5529.8, "end": 5533.6, "text": " fit.", "tokens": [50760, 3318, 13, 50950], "temperature": 0.0, "avg_logprob": -0.26179291072644684, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.015423398464918137}, {"id": 1152, "seek": 552188, "start": 5533.6, "end": 5536.64, "text": " And it will find one.", "tokens": [50950, 400, 309, 486, 915, 472, 13, 51102], "temperature": 0.0, "avg_logprob": -0.26179291072644684, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.015423398464918137}, {"id": 1153, "seek": 552188, "start": 5536.64, "end": 5541.82, "text": " And so then it's going to call that method.", "tokens": [51102, 400, 370, 550, 309, 311, 516, 281, 818, 300, 3170, 13, 51361], "temperature": 0.0, "avg_logprob": -0.26179291072644684, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.015423398464918137}, {"id": 1154, "seek": 552188, "start": 5541.82, "end": 5547.92, "text": " Now if that method doesn't exist, it doesn't appear at all, then getAttr will return this", "tokens": [51361, 823, 498, 300, 3170, 1177, 380, 2514, 11, 309, 1177, 380, 4204, 412, 439, 11, 550, 483, 38151, 81, 486, 2736, 341, 51666], "temperature": 0.0, "avg_logprob": -0.26179291072644684, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.015423398464918137}, {"id": 1155, "seek": 552188, "start": 5547.92, "end": 5550.12, "text": " instead.", "tokens": [51666, 2602, 13, 51776], "temperature": 0.0, "avg_logprob": -0.26179291072644684, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.015423398464918137}, {"id": 1156, "seek": 555012, "start": 5550.12, "end": 5554.18, "text": " Identity is a function, just here.", "tokens": [50364, 25905, 507, 307, 257, 2445, 11, 445, 510, 13, 50567], "temperature": 0.0, "avg_logprob": -0.24849032117174819, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0003920405579265207}, {"id": 1157, "seek": 555012, "start": 5554.18, "end": 5555.64, "text": " This is an identity function.", "tokens": [50567, 639, 307, 364, 6575, 2445, 13, 50640], "temperature": 0.0, "avg_logprob": -0.24849032117174819, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0003920405579265207}, {"id": 1158, "seek": 555012, "start": 5555.64, "end": 5560.92, "text": " All it does is whatever arguments it gets passed, it returns them.", "tokens": [50640, 1057, 309, 775, 307, 2035, 12869, 309, 2170, 4678, 11, 309, 11247, 552, 13, 50904], "temperature": 0.0, "avg_logprob": -0.24849032117174819, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0003920405579265207}, {"id": 1159, "seek": 555012, "start": 5560.92, "end": 5567.48, "text": " And if it's not passed any arguments, it just returns.", "tokens": [50904, 400, 498, 309, 311, 406, 4678, 604, 12869, 11, 309, 445, 11247, 13, 51232], "temperature": 0.0, "avg_logprob": -0.24849032117174819, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0003920405579265207}, {"id": 1160, "seek": 555012, "start": 5567.48, "end": 5571.88, "text": " So there's a lot of Python going on here.", "tokens": [51232, 407, 456, 311, 257, 688, 295, 15329, 516, 322, 510, 13, 51452], "temperature": 0.0, "avg_logprob": -0.24849032117174819, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0003920405579265207}, {"id": 1161, "seek": 555012, "start": 5571.88, "end": 5579.28, "text": " And that is why we did that foundations lesson.", "tokens": [51452, 400, 300, 307, 983, 321, 630, 300, 22467, 6898, 13, 51822], "temperature": 0.0, "avg_logprob": -0.24849032117174819, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0003920405579265207}, {"id": 1162, "seek": 557928, "start": 5579.44, "end": 5586.12, "text": " And so for people who haven't done a lot of this Python, there's going to be a lot of", "tokens": [50372, 400, 370, 337, 561, 567, 2378, 380, 1096, 257, 688, 295, 341, 15329, 11, 456, 311, 516, 281, 312, 257, 688, 295, 50706], "temperature": 0.0, "avg_logprob": -0.21673859988941865, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0007672869833186269}, {"id": 1163, "seek": 557928, "start": 5586.12, "end": 5593.12, "text": " stuff to experiment with and learn about.", "tokens": [50706, 1507, 281, 5120, 365, 293, 1466, 466, 13, 51056], "temperature": 0.0, "avg_logprob": -0.21673859988941865, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0007672869833186269}, {"id": 1164, "seek": 557928, "start": 5593.12, "end": 5599.099999999999, "text": " And so do ask on the forums if any of these bits get confusing.", "tokens": [51056, 400, 370, 360, 1029, 322, 264, 26998, 498, 604, 295, 613, 9239, 483, 13181, 13, 51355], "temperature": 0.0, "avg_logprob": -0.21673859988941865, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0007672869833186269}, {"id": 1165, "seek": 557928, "start": 5599.099999999999, "end": 5604.24, "text": " But the best way to learn about these things is to open up this Jupyter notebook and try", "tokens": [51355, 583, 264, 1151, 636, 281, 1466, 466, 613, 721, 307, 281, 1269, 493, 341, 22125, 88, 391, 21060, 293, 853, 51612], "temperature": 0.0, "avg_logprob": -0.21673859988941865, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0007672869833186269}, {"id": 1166, "seek": 557928, "start": 5604.24, "end": 5608.16, "text": " and create really simple versions of things.", "tokens": [51612, 293, 1884, 534, 2199, 9606, 295, 721, 13, 51808], "temperature": 0.0, "avg_logprob": -0.21673859988941865, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0007672869833186269}, {"id": 1167, "seek": 560816, "start": 5608.16, "end": 5616.48, "text": " So for example, let's try identity.", "tokens": [50364, 407, 337, 1365, 11, 718, 311, 853, 6575, 13, 50780], "temperature": 0.0, "avg_logprob": -0.38786215100969584, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.030674723908305168}, {"id": 1168, "seek": 560816, "start": 5616.48, "end": 5621.72, "text": " How exactly does identity work?", "tokens": [50780, 1012, 2293, 775, 6575, 589, 30, 51042], "temperature": 0.0, "avg_logprob": -0.38786215100969584, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.030674723908305168}, {"id": 1169, "seek": 560816, "start": 5621.72, "end": 5623.2, "text": " I could call it and it gets nothing.", "tokens": [51042, 286, 727, 818, 309, 293, 309, 2170, 1825, 13, 51116], "temperature": 0.0, "avg_logprob": -0.38786215100969584, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.030674723908305168}, {"id": 1170, "seek": 560816, "start": 5623.2, "end": 5625.44, "text": " I can call it with 1, it gets back 1.", "tokens": [51116, 286, 393, 818, 309, 365, 502, 11, 309, 2170, 646, 502, 13, 51228], "temperature": 0.0, "avg_logprob": -0.38786215100969584, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.030674723908305168}, {"id": 1171, "seek": 560816, "start": 5625.44, "end": 5628.48, "text": " I could call it with a, it gets back a.", "tokens": [51228, 286, 727, 818, 309, 365, 257, 11, 309, 2170, 646, 257, 13, 51380], "temperature": 0.0, "avg_logprob": -0.38786215100969584, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.030674723908305168}, {"id": 1172, "seek": 560816, "start": 5628.48, "end": 5635.36, "text": " I could call it with a1.", "tokens": [51380, 286, 727, 818, 309, 365, 257, 16, 13, 51724], "temperature": 0.0, "avg_logprob": -0.38786215100969584, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.030674723908305168}, {"id": 1173, "seek": 563536, "start": 5636.36, "end": 5642.5599999999995, "text": " And how is it doing that exactly?", "tokens": [50414, 400, 577, 307, 309, 884, 300, 2293, 30, 50724], "temperature": 0.0, "avg_logprob": -0.2566694034043179, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.015663404017686844}, {"id": 1174, "seek": 563536, "start": 5642.5599999999995, "end": 5647.679999999999, "text": " So remember, we can add a breakpoint.", "tokens": [50724, 407, 1604, 11, 321, 393, 909, 257, 1821, 6053, 13, 50980], "temperature": 0.0, "avg_logprob": -0.2566694034043179, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.015663404017686844}, {"id": 1175, "seek": 563536, "start": 5647.679999999999, "end": 5652.32, "text": " And this would be a great time to really test your debugging skills.", "tokens": [50980, 400, 341, 576, 312, 257, 869, 565, 281, 534, 1500, 428, 45592, 3942, 13, 51212], "temperature": 0.0, "avg_logprob": -0.2566694034043179, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.015663404017686844}, {"id": 1176, "seek": 563536, "start": 5652.32, "end": 5655.88, "text": " So remember in our debugger, we can hit H to find out what the commands are.", "tokens": [51212, 407, 1604, 294, 527, 24083, 1321, 11, 321, 393, 2045, 389, 281, 915, 484, 437, 264, 16901, 366, 13, 51390], "temperature": 0.0, "avg_logprob": -0.2566694034043179, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.015663404017686844}, {"id": 1177, "seek": 563536, "start": 5655.88, "end": 5659.36, "text": " But you really should do a tutorial on the debugger if you're not familiar with it.", "tokens": [51390, 583, 291, 534, 820, 360, 257, 7073, 322, 264, 24083, 1321, 498, 291, 434, 406, 4963, 365, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2566694034043179, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.015663404017686844}, {"id": 1178, "seek": 563536, "start": 5659.36, "end": 5661.28, "text": " And then we can step through each one.", "tokens": [51564, 400, 550, 321, 393, 1823, 807, 1184, 472, 13, 51660], "temperature": 0.0, "avg_logprob": -0.2566694034043179, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.015663404017686844}, {"id": 1179, "seek": 566128, "start": 5661.28, "end": 5666.32, "text": " So I can now print args.", "tokens": [50364, 407, 286, 393, 586, 4482, 3882, 82, 13, 50616], "temperature": 0.0, "avg_logprob": -0.27086471557617187, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0025908774696290493}, {"id": 1180, "seek": 566128, "start": 5666.32, "end": 5670.88, "text": " And there's actually a trick which I like, is that args is actually a command, funnily", "tokens": [50616, 400, 456, 311, 767, 257, 4282, 597, 286, 411, 11, 307, 300, 3882, 82, 307, 767, 257, 5622, 11, 1019, 77, 953, 50844], "temperature": 0.0, "avg_logprob": -0.27086471557617187, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0025908774696290493}, {"id": 1181, "seek": 566128, "start": 5670.88, "end": 5674.28, "text": " enough, which will just tell you the arguments to any function, regardless of what they're", "tokens": [50844, 1547, 11, 597, 486, 445, 980, 291, 264, 12869, 281, 604, 2445, 11, 10060, 295, 437, 436, 434, 51014], "temperature": 0.0, "avg_logprob": -0.27086471557617187, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0025908774696290493}, {"id": 1182, "seek": 566128, "start": 5674.28, "end": 5675.28, "text": " called.", "tokens": [51014, 1219, 13, 51064], "temperature": 0.0, "avg_logprob": -0.27086471557617187, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0025908774696290493}, {"id": 1183, "seek": 566128, "start": 5675.28, "end": 5678.34, "text": " Which is kind of nice.", "tokens": [51064, 3013, 307, 733, 295, 1481, 13, 51217], "temperature": 0.0, "avg_logprob": -0.27086471557617187, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0025908774696290493}, {"id": 1184, "seek": 566128, "start": 5678.34, "end": 5683.28, "text": " And so then we can step through by pressing N.", "tokens": [51217, 400, 370, 550, 321, 393, 1823, 807, 538, 12417, 426, 13, 51464], "temperature": 0.0, "avg_logprob": -0.27086471557617187, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0025908774696290493}, {"id": 1185, "seek": 566128, "start": 5683.28, "end": 5688.719999999999, "text": " And after this, we can check, like, OK, what is x now?", "tokens": [51464, 400, 934, 341, 11, 321, 393, 1520, 11, 411, 11, 2264, 11, 437, 307, 2031, 586, 30, 51736], "temperature": 0.0, "avg_logprob": -0.27086471557617187, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0025908774696290493}, {"id": 1186, "seek": 568872, "start": 5688.72, "end": 5691.88, "text": " And what is args now?", "tokens": [50364, 400, 437, 307, 3882, 82, 586, 30, 50522], "temperature": 0.0, "avg_logprob": -0.24423165754838425, "compression_ratio": 1.3964497041420119, "no_speech_prob": 0.00394532922655344}, {"id": 1187, "seek": 568872, "start": 5691.88, "end": 5701.0, "text": " So remember to really experiment with these things.", "tokens": [50522, 407, 1604, 281, 534, 5120, 365, 613, 721, 13, 50978], "temperature": 0.0, "avg_logprob": -0.24423165754838425, "compression_ratio": 1.3964497041420119, "no_speech_prob": 0.00394532922655344}, {"id": 1188, "seek": 568872, "start": 5701.0, "end": 5710.76, "text": " So anyway, we're going to talk about this a lot more in the next lesson.", "tokens": [50978, 407, 4033, 11, 321, 434, 516, 281, 751, 466, 341, 257, 688, 544, 294, 264, 958, 6898, 13, 51466], "temperature": 0.0, "avg_logprob": -0.24423165754838425, "compression_ratio": 1.3964497041420119, "no_speech_prob": 0.00394532922655344}, {"id": 1189, "seek": 568872, "start": 5710.76, "end": 5717.76, "text": " But before that, if you're not familiar with try-except blocks, you know, spend some time", "tokens": [51466, 583, 949, 300, 11, 498, 291, 434, 406, 4963, 365, 853, 12, 3121, 1336, 8474, 11, 291, 458, 11, 3496, 512, 565, 51816], "temperature": 0.0, "avg_logprob": -0.24423165754838425, "compression_ratio": 1.3964497041420119, "no_speech_prob": 0.00394532922655344}, {"id": 1190, "seek": 571776, "start": 5717.8, "end": 5718.8, "text": " practicing them.", "tokens": [50366, 11350, 552, 13, 50416], "temperature": 0.0, "avg_logprob": -0.23620475051749465, "compression_ratio": 1.904564315352697, "no_speech_prob": 0.0010004937648773193}, {"id": 1191, "seek": 571776, "start": 5718.8, "end": 5722.24, "text": " If you're not familiar with decorators, well, we've seen them before.", "tokens": [50416, 759, 291, 434, 406, 4963, 365, 7919, 3391, 11, 731, 11, 321, 600, 1612, 552, 949, 13, 50588], "temperature": 0.0, "avg_logprob": -0.23620475051749465, "compression_ratio": 1.904564315352697, "no_speech_prob": 0.0010004937648773193}, {"id": 1192, "seek": 571776, "start": 5722.24, "end": 5726.400000000001, "text": " So go back and look at them again really carefully.", "tokens": [50588, 407, 352, 646, 293, 574, 412, 552, 797, 534, 7500, 13, 50796], "temperature": 0.0, "avg_logprob": -0.23620475051749465, "compression_ratio": 1.904564315352697, "no_speech_prob": 0.0010004937648773193}, {"id": 1193, "seek": 571776, "start": 5726.400000000001, "end": 5729.84, "text": " If you're not familiar with the debugger, practice with that.", "tokens": [50796, 759, 291, 434, 406, 4963, 365, 264, 24083, 1321, 11, 3124, 365, 300, 13, 50968], "temperature": 0.0, "avg_logprob": -0.23620475051749465, "compression_ratio": 1.904564315352697, "no_speech_prob": 0.0010004937648773193}, {"id": 1194, "seek": 571776, "start": 5729.84, "end": 5735.92, "text": " If you haven't spent much time with getattra, remind yourself about that.", "tokens": [50968, 759, 291, 2378, 380, 4418, 709, 565, 365, 483, 1591, 424, 11, 4160, 1803, 466, 300, 13, 51272], "temperature": 0.0, "avg_logprob": -0.23620475051749465, "compression_ratio": 1.904564315352697, "no_speech_prob": 0.0010004937648773193}, {"id": 1195, "seek": 571776, "start": 5735.92, "end": 5741.0, "text": " So try to get yourself really familiar and comfortable, as much as possible, with the", "tokens": [51272, 407, 853, 281, 483, 1803, 534, 4963, 293, 4619, 11, 382, 709, 382, 1944, 11, 365, 264, 51526], "temperature": 0.0, "avg_logprob": -0.23620475051749465, "compression_ratio": 1.904564315352697, "no_speech_prob": 0.0010004937648773193}, {"id": 1196, "seek": 571776, "start": 5741.0, "end": 5742.68, "text": " pieces.", "tokens": [51526, 3755, 13, 51610], "temperature": 0.0, "avg_logprob": -0.23620475051749465, "compression_ratio": 1.904564315352697, "no_speech_prob": 0.0010004937648773193}, {"id": 1197, "seek": 571776, "start": 5742.68, "end": 5746.0, "text": " Because if you're not comfortable with the pieces, then the way we put the pieces together", "tokens": [51610, 1436, 498, 291, 434, 406, 4619, 365, 264, 3755, 11, 550, 264, 636, 321, 829, 264, 3755, 1214, 51776], "temperature": 0.0, "avg_logprob": -0.23620475051749465, "compression_ratio": 1.904564315352697, "no_speech_prob": 0.0010004937648773193}, {"id": 1198, "seek": 574600, "start": 5746.04, "end": 5747.8, "text": " is going to be confusing.", "tokens": [50366, 307, 516, 281, 312, 13181, 13, 50454], "temperature": 0.0, "avg_logprob": -0.28107912606055585, "compression_ratio": 1.9356223175965666, "no_speech_prob": 0.0010005021467804909}, {"id": 1199, "seek": 574600, "start": 5747.8, "end": 5752.88, "text": " There's actually something in education, in kind of the theory of education, called cognitive", "tokens": [50454, 821, 311, 767, 746, 294, 3309, 11, 294, 733, 295, 264, 5261, 295, 3309, 11, 1219, 15605, 50708], "temperature": 0.0, "avg_logprob": -0.28107912606055585, "compression_ratio": 1.9356223175965666, "no_speech_prob": 0.0010005021467804909}, {"id": 1200, "seek": 574600, "start": 5752.88, "end": 5754.2, "text": " load theory.", "tokens": [50708, 3677, 5261, 13, 50774], "temperature": 0.0, "avg_logprob": -0.28107912606055585, "compression_ratio": 1.9356223175965666, "no_speech_prob": 0.0010005021467804909}, {"id": 1201, "seek": 574600, "start": 5754.2, "end": 5758.6, "text": " And the theory of cognitive, basically, cognitive load theory says if you're trying to learn", "tokens": [50774, 400, 264, 5261, 295, 15605, 11, 1936, 11, 15605, 3677, 5261, 1619, 498, 291, 434, 1382, 281, 1466, 50994], "temperature": 0.0, "avg_logprob": -0.28107912606055585, "compression_ratio": 1.9356223175965666, "no_speech_prob": 0.0010005021467804909}, {"id": 1202, "seek": 574600, "start": 5758.6, "end": 5765.52, "text": " something but your cognitive load is really high because of all lots of other things going", "tokens": [50994, 746, 457, 428, 15605, 3677, 307, 534, 1090, 570, 295, 439, 3195, 295, 661, 721, 516, 51340], "temperature": 0.0, "avg_logprob": -0.28107912606055585, "compression_ratio": 1.9356223175965666, "no_speech_prob": 0.0010005021467804909}, {"id": 1203, "seek": 574600, "start": 5765.52, "end": 5769.08, "text": " on at the same time, you're not going to learn it.", "tokens": [51340, 322, 412, 264, 912, 565, 11, 291, 434, 406, 516, 281, 1466, 309, 13, 51518], "temperature": 0.0, "avg_logprob": -0.28107912606055585, "compression_ratio": 1.9356223175965666, "no_speech_prob": 0.0010005021467804909}, {"id": 1204, "seek": 574600, "start": 5769.08, "end": 5775.16, "text": " So it's going to be hard for you to learn this framework that we're building if you", "tokens": [51518, 407, 309, 311, 516, 281, 312, 1152, 337, 291, 281, 1466, 341, 8388, 300, 321, 434, 2390, 498, 291, 51822], "temperature": 0.0, "avg_logprob": -0.28107912606055585, "compression_ratio": 1.9356223175965666, "no_speech_prob": 0.0010005021467804909}, {"id": 1205, "seek": 577516, "start": 5775.32, "end": 5778.72, "text": " have too much cognitive load of like, what the hell's a decorator, or what the hell's", "tokens": [50372, 362, 886, 709, 15605, 3677, 295, 411, 11, 437, 264, 4921, 311, 257, 7919, 1639, 11, 420, 437, 264, 4921, 311, 50542], "temperature": 0.0, "avg_logprob": -0.2551946721525274, "compression_ratio": 1.7154471544715446, "no_speech_prob": 0.005301689729094505}, {"id": 1206, "seek": 577516, "start": 5778.72, "end": 5785.16, "text": " getattra, or what does sorted do, or what's partial, you know, all these things.", "tokens": [50542, 483, 1591, 424, 11, 420, 437, 775, 262, 14813, 360, 11, 420, 437, 311, 14641, 11, 291, 458, 11, 439, 613, 721, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2551946721525274, "compression_ratio": 1.7154471544715446, "no_speech_prob": 0.005301689729094505}, {"id": 1207, "seek": 577516, "start": 5785.16, "end": 5792.88, "text": " Now I actually spent quite a bit of time trying to make this as simple as possible, but also", "tokens": [50864, 823, 286, 767, 4418, 1596, 257, 857, 295, 565, 1382, 281, 652, 341, 382, 2199, 382, 1944, 11, 457, 611, 51250], "temperature": 0.0, "avg_logprob": -0.2551946721525274, "compression_ratio": 1.7154471544715446, "no_speech_prob": 0.005301689729094505}, {"id": 1208, "seek": 577516, "start": 5792.88, "end": 5797.0, "text": " as flexible as it needs to be for the rest of the course.", "tokens": [51250, 382, 11358, 382, 309, 2203, 281, 312, 337, 264, 1472, 295, 264, 1164, 13, 51456], "temperature": 0.0, "avg_logprob": -0.2551946721525274, "compression_ratio": 1.7154471544715446, "no_speech_prob": 0.005301689729094505}, {"id": 1209, "seek": 577516, "start": 5797.0, "end": 5800.04, "text": " And this is as simple as I could get it.", "tokens": [51456, 400, 341, 307, 382, 2199, 382, 286, 727, 483, 309, 13, 51608], "temperature": 0.0, "avg_logprob": -0.2551946721525274, "compression_ratio": 1.7154471544715446, "no_speech_prob": 0.005301689729094505}, {"id": 1210, "seek": 577516, "start": 5800.04, "end": 5804.72, "text": " So these are kind of things that you actually do have to learn.", "tokens": [51608, 407, 613, 366, 733, 295, 721, 300, 291, 767, 360, 362, 281, 1466, 13, 51842], "temperature": 0.0, "avg_logprob": -0.2551946721525274, "compression_ratio": 1.7154471544715446, "no_speech_prob": 0.005301689729094505}, {"id": 1211, "seek": 580472, "start": 5805.280000000001, "end": 5812.6, "text": " But in doing so, you're going to be able to write some really, you know, powerful and", "tokens": [50392, 583, 294, 884, 370, 11, 291, 434, 516, 281, 312, 1075, 281, 2464, 512, 534, 11, 291, 458, 11, 4005, 293, 50758], "temperature": 0.0, "avg_logprob": -0.25949945120975887, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.0005976629909127951}, {"id": 1212, "seek": 580472, "start": 5812.6, "end": 5815.320000000001, "text": " general code yourself.", "tokens": [50758, 2674, 3089, 1803, 13, 50894], "temperature": 0.0, "avg_logprob": -0.25949945120975887, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.0005976629909127951}, {"id": 1213, "seek": 580472, "start": 5815.320000000001, "end": 5823.4400000000005, "text": " So hopefully you'll find this a really valuable and mind-expanding exercise in bringing high-level", "tokens": [50894, 407, 4696, 291, 603, 915, 341, 257, 534, 8263, 293, 1575, 12, 15952, 42389, 5380, 294, 5062, 1090, 12, 12418, 51300], "temperature": 0.0, "avg_logprob": -0.25949945120975887, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.0005976629909127951}, {"id": 1214, "seek": 580472, "start": 5823.4400000000005, "end": 5827.56, "text": " software engineering skills to your data science work.", "tokens": [51300, 4722, 7043, 3942, 281, 428, 1412, 3497, 589, 13, 51506], "temperature": 0.0, "avg_logprob": -0.25949945120975887, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.0005976629909127951}, {"id": 1215, "seek": 580472, "start": 5827.56, "end": 5833.84, "text": " Okay, so with that, this looks like a good place to leave it, and look forward to seeing", "tokens": [51506, 1033, 11, 370, 365, 300, 11, 341, 1542, 411, 257, 665, 1081, 281, 1856, 309, 11, 293, 574, 2128, 281, 2577, 51820], "temperature": 0.0, "avg_logprob": -0.25949945120975887, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.0005976629909127951}, {"id": 1216, "seek": 583384, "start": 5833.96, "end": 5834.96, "text": " you next time.", "tokens": [50370, 291, 958, 565, 13, 50420], "temperature": 0.0, "avg_logprob": -0.5274210409684614, "compression_ratio": 0.7037037037037037, "no_speech_prob": 0.036748308688402176}, {"id": 1217, "seek": 583384, "start": 5834.96, "end": 5835.46, "text": " Bye!", "tokens": [50420, 4621, 0, 50445], "temperature": 0.0, "avg_logprob": -0.5274210409684614, "compression_ratio": 0.7037037037037037, "no_speech_prob": 0.036748308688402176}], "language": "en"}