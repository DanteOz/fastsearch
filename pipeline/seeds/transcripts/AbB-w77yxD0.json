{"text": " I'm going to go ahead and get started. I wanted to make a few announcements. One is that I will not be here next week, so there will be no class on Tuesday, and then Thursday is the exam, and David will be here. David Juminski will be proctoring the exam, but I will have email so you can still email me questions about anything. Tuesday is when the final draft of the blog post is due, and also I've put up homework three, and it's just a single problem, but that's due Tuesday also. And then I'm not going to have my normal office hours this Friday, but I could meet with you earlier in the day Friday, so if you want to meet on Friday or on Thursday afternoon, just let me know. All right. Yeah, so I was going to follow up with, we had some loose ends from last time for lesson six that I've added, kind of added into the bottom of the notebook. Just to remind you, lesson six was about different ways of calculating the linear regression least squares. And so this is coming from a question Tim asked about kind of full versus reduced factorizations. And I've kind of been skimming over this, but I really are hand waving and not wanting to get into the detail, but it's actually good to look at, and it helped me realize I had an error in the SBD code, which is why it was so slow. And so I'll show you that in a moment. But the idea is there's both a full SBD and a reduced SBD, and they're both referred to as SBD, so it's not always clear. And the idea is that with the full SBD, U is a square matrix. And so for the kind of final, I guess, M minus N columns of U, you're just filling out what's needed for an orthonormal basis. But these are getting multiplied by all zeros. So you've kind of also added several rows, all of zero, to the base of sigma. So it doesn't, like you want these columns to be such that you're creating an orthonormal basis in U, but beyond that, they actually don't have like a connection with your original data set. So that's kind of what's going on with U. And the reduced form is you're just finding kind of the columns that you need to represent A. And so notice both of these, you're getting your original matrix A back, since here you've kind of got these zeroing out. The full SBD would be useful if you needed U to be a kind of square orthonormal matrix. Are there questions about this? Linda? I don't think I get. So if the full SBD is not a square matrix, then how is it a singular, like on the diagonal, it shows the importance factor. And if it's not square, how does it show for the last part of? OK, yeah, that's a great question. So the diagonal of sigma is what's giving you the importance. And so there's no importance corresponding to these vectors right here. And that's because those vectors aren't even a part of A. So their importance is like kind of like zero. You know, they show up zero amount in A. So if they are not a part of A, where do they come from? And so that comes from just trying to create an orthonormal basis. And so there that's kind of like around the idea of like, oh, if you want you to be square and orthonormal, you kind of need to come up with these additional columns, even though they're not a part of A. That's a good question. Any other questions about this? Tim? And Linda, can you pass the microphone back? So I can pass V. When you did the reduced SVD, the V star didn't have to be square either. It could have been rectangular. So is that because you're dropping some of the non-important singular values out of sigma? Oh, so for the yeah. And so I should also note that the reduced SVD is different than the truncated SVD. So yeah, for truncated SVD, what you would do is if I can write on the screen. OK, I didn't sync it. The idea with reduced SVD, let's use the mouse, is you're kind of sorry, truncated SVD is you're cutting off this kind of right hand side here. Or no, you would be cutting off the bottom. Well, yeah, you want to cut off rows of V. Yeah, and so then you have to cut off the bottom few rows of V. And so that's when you end up with the rectangular V. And that would also cut off some columns, more columns of V, right? Yes, exactly. Yeah, and you cut off more columns of V. So but in reduced SVD, V star remains square. Yes, yes. Good question. Are there questions about this? OK, so doing this, I thought to check on SVD, and it turns out the default is to do the full SVD. And so I'll let you guess which one of these do you think is faster to calculate, the full or reduced? People are laughing, but yeah, so the reduced is faster to calculate. And so above in my least squares linear regression, I was calculating the full. So let me go back to SVD. So what you need to do is use the full matrices equals false parameter. And so this is here in the least squares SVD. And then that takes out and I should have known this. I was like slicing off the first end. So I should have known like I'm calculating too many, the fact that I'm having to throw these away. So then I re-ran all the timing and SVD is a lot more reasonable now. So let me show you. So here this is this first table. Is this a good size for the maybe one bigger? No, that's too big to fit. The SVD is still for most of these a bit a little bit slower than the others here, though. You'll notice SVD is slightly faster than QR in the case that's a thousand by twenty. We've got 0.019 compared to 0.018 for SVD. And again, just to kind of summarize, this is a bunch of different methods for finding the least squares linear regression being applied to the same set of matrices that were randomly generated. We went through in a loop and randomly generated matrices of different sizes, solved the problem, I guess five different ways and saw how the time and the error compared. Let me see if anything else stands out as particularly noteworthy. But yeah, so this puts SVD in a much, much more reasonable speed range. Are there questions or kind of general questions on this this comparison? We're about to go into some specific examples where we get worse error rates for some of the some of the algorithms. So overall, what seems to be the fastest? And this might depend on the size. Tim, can you pass the microphone forward? Did you say it, Linda? Yeah, Tolesky is the fastest for most of them. Correct. Any other observations about this? OK, let's go back. Oh, and then while we're talking about full versus reduced, I wanted to say that QR also has full and reduced versions. And so it's a very similar idea if you've got a rectangular A and the reduced version, you're just getting a rectangular Q and a square R, which is upper triangular. And then for the full version, you're adding additional columns onto Q to make an orthonormal basis. And then you're adding rows of zeros to R. So those are going to cancel out. So these columns of Q are not actually part of A, but they're nice if you wanted a square square matrix that was orthonormal. OK, so I wanted to address. So kind of comparing these algorithms, if we just look at speed, Tolesky seems really good in a lot. And then also taking the matrix inverse seemed to be pretty fast. And I think we were getting that it had the same air as everything else. Let me confirm. Yeah, we were getting the same air as everything else. But in practice, you never want to do this. And so I wanted to talk more about why that is. And that's because matrix inversion is unstable. And so here we're going to look at a specific example called a Hilbert matrix. And let me actually start by doing a smaller one just so you can see what it looks like. So the Hilbert matrix is basically kind of these fractions. You can see you've got one, a half, a third, a fourth, a fifth is how the matrix is constructed. And it's known to have a poor condition number. So here I'm just using NumPy's lenowge.inverse method. So I've created the Hilbert matrix that's 14 by 14. Let me run that again. I invert it. And then ideally, a times a inverse minus the identity, what would you expect that to give you? Say it louder. Zero. Yes. So you'd expect it to give you zero. I take the norm of that and I'm getting five, which is a bad sign. And so it's C. And we can even look at a times a inverse here. And you can see this is not very close to zero. Many of these decimals have numbers kind of in the tenths place. This even has like a negative. Oh, wow. This even has a two. Yeah. So this is quite far from zero. So I think this is kind of a nice illustration. And this is not even a huge matrix. So we're using 14 by 14 and we're using NumPy's inverse. And we are not getting back to the identity with that. And then we can also check. So I mentioned last time there's something called the condition number of a matrix. And actually, does anyone remember what the formula for the condition number is? OK, I'll pull that up. It's the norm of a times the norm of a inverse. And larger condition numbers are bad. So you want the condition number to be smaller. Let me find it. So that's kind of the formal definition in general. But for a matrix, here it is. Norm of a times norm of a inverse. And it's something that it shows up all the time in kind of different theorems around conditioning. And so it's useful to have it as a quantity. And Linda, can you pass the microphone to Tim? Is that defined only for square matrices? Is that only defined for square matrices? That's a good question. I guess you have a suit that non-square matrices kind of have an inverse. Yeah, I'll look that up and get back to you because there is something called the pseudo inverse. But yeah, I don't know if that's... Yeah, I think you're typically talking about square matrices with it. But yeah, that's a good question. Yeah, and so if we look at the condition number of a, it's 10 to the 17th magnitude, which is a really bad sign that it's so large. And so that's... I would say that's the primary reason that you don't want to calculate a matrix inverse. And there are a few other issues, I thought I wrote them down, that can come up. One is if you have a sparse matrix in particular, when you calculate the inverse, it stops being sparse. And so memory-wise, that could be really bad, particularly if you had a matrix that was large enough that you can't store the non-sparse version. Calculating it's inverse is kind of, yeah, making this really huge matrix. But I think the primary thing is the instability. So this is a little bit of a cheat. So I'm still on this Hilbert matrix, and so I run all of the options. Well, you'll notice I run all the options except Cholesky. And so we didn't focus on this last time. So this is a difficult question, but does anyone have a guess of why, maybe why we can't use Cholesky here? Okay, I just, oh, Matthew, can you pass the microphone? Because it's unstable like you said. No, so that's matrix inversion is unstable. It doesn't require the matrix inversion to. No. So yeah, Cholesky entails coming up with the, you're coming up with a triangular, or yeah, triangular matrix, which times its transpose equals A transpose A. Yeah, this I just briefly mentioned. The Cholesky factorization is only guaranteed to exist if the matrix is positive definite. And so this matrix actually is not positive definite. So that's a down, significant downside to Cholesky is that even though it's really fast, you can't use it in all cases. And so we can't use it here. So then, yeah, so I had to take it out. I mean, you can try running it and you get an error basically that there's a negative singular value if you attempt to do it. So here the we've got this huge error for I don't know why I have this twice. Delete one of these. Yeah, we've got this big error for the taking the inverse, which is not surprising. And speed wise, so QR factorization, SVD and SciPy's implementation are all fairly similar. But SVD was actually fastest, although I have to confess I ran this a few times and I don't think that was consistently the case, but I wanted to save it because I was having trouble finding a case where SVD was the best. But here really, I mean, they're so close that there's not like a clear winner. But yeah, this illustrates kind of you definitely don't want to be taking the inverse and you can't take Teleski. Oh, and can you throw the microphone, Matthew? So any reason that after inversion that the matrix is going to just get more dense? I guess it's like there's nothing that guarantees that you would have zeros in other places. You kind of think like the process for taking inversion is it's like doing Gaussian elimination only instead of having like a single column, you have like the identity matrix is one way to calculate the inverse. And so there you're kind of basically like adding and dividing things to every location as you go through and zero out your original matrix. But yes, you're kind of like putting values in where you had zeros when you do that Gaussian elimination. Yes, I wanted to talk a little bit about runtime. And this is not exactly big O because I've included constants here and, you know, with big O notation, you are or I mean, I don't even know if it's OK for me to call this runtime. But this is kind of normally you drop the constants. But I just wanted to show them because here most things are and cubed. And so it kind of matters what the constant is. So Cholesky is one third and cubed. And so that's a reason that it's kind of faster than these others. So QR is kind of a formula involving powers of third order terms. And then something else to highlight is that matrix multiplication is N cubed and solving a triangular system is N squared. And so if you'll remember, this kind of goes through with the Cholesky factorization. What you end up with is actually maybe I'll go back up to how I had it written. OK, here's the Cholesky factorization. So here our process was we take A transpose A, X equals A transpose B. So we are having to do a matrix multiplication to get this A transpose A. Then we do the Cholesky factorization and get R transpose R where R is upper triangular. And then we're solving R transpose W equals the right hand side. And the reason that's nice is that's a triangular system because R is triangular. And how can you solve a triangular system? Anyone I haven't heard from today? OK, yeah. Can you throw the microphone to Vincent? Yes, back substitution. Exactly. So bottom row, you only have a single entry. Actually, this is our transpose. It really would be the top row in this case. So you can just divide through and get that value, plug that into the next equation, which only has two variables, plug it in, solve for the other one, and so on. And so that's N squared. So that's a quicker, quicker way of doing this. Whereas with the kind of naive approach, even when you get, let me find it. Even once you get this inverse, you still have to do a matrix multiplication. So you're doing A transpose A, calculating the inverse, which takes N cubed. And then you're multiplying that by A transpose, which is another matrix multiplication. So that's slow. So really, kind of what I wanted to highlight here is that matrix multiplication is N cubed and solving a triangular system is N squared. Triangular system is N squared. So yeah, and I found these from a slide. This was actually from the convex optimization course I linked to a few notebooks ago. They had like a numerical linear algebra background lesson. And so this is just showing why Cholesky is fast. So here I'm looking at another very specific type of matrix called the Vandermonde matrix. And this is coming from an example in Trevithin. And so Trevithin tells us kind of if we normalize by this, he's calculated the kind of true solution on a very high precision computer and has that it should be one. So here, so we're creating this matrix by taking powers of T. So we have kind of evenly spaced points between zero and one. And then we're taking different powers of them. And then doing E to the sine of four times that. And so we can check that using QR, we're getting 10 to the negative seventh, which isn't perfect, but is reasonable. This is another condition number of to the power of 17, which is bad news. So we run it for the four. And again, Cholesky doesn't work on this one because it's not positive definite. And then here we'll see again, there's a big error for the naive approach where we take the inverse. And then time wise, QR is a bit a bit faster. Well, really, the naive approach is the fastest, which is very wrong. But then QR factorization is the fastest of the ones that are correct. Any questions? OK. And then one more example, and this is a little bit redundant. This is with a low rank matrix. So really, I've just kind of created a set of rows and then I've stacked them on top of each other. So there's a huge amount of redundancy in this matrix. And running that, we get a large amount of error for the naive solution. And then we do have more error than we've had previously for the others. And SVD is noticeably worse than QR in this case, although SVD is quicker. Yeah, so I guess kind of in summary, Cholesky is typically fastest when it works, but Cholesky can only be used on symmetric positive definite matrices. And then also Cholesky is, well, I guess we weren't really able to do it. It's unstable for matrices with high condition numbers or low rank. So linear regression via QR has been recommended by numerical analysts as the standard method for years. Trevathan describes it as good for daily use. And Trevathan says that their problems were SVD is more stable, although I was not able to recreate that. But my hope with this lesson was to kind of get to see what's going on underneath the hood with linear regression least squares, and also to see that there are different approaches for it. And depending on your problem, there might be some solution or some approaches better than others. Questions? Okay, in that case, we're going to start on Notebook 7. Page rank with, well, page rank with eigen-decompositions. So new start, new topic. I wanted to introduce two tools that are just useful in general, and we're going to use them today, but you can use them for a lot of problems. One is a library called PSUtil, which lets you check your memory usage. And here we're using a larger data set than we've used before, so this can be nice to have. So you'd have to pip install this and then import it. Here we're kind of getting the process ID, the process memory info. And then I've written a helper method memory usage that just returns the... So RSS memory stands for resident set size memory. And so we're kind of just returning that memory divided by the total memory to get the memory usage. So this could be a nice thing to monitor if you're having trouble with running out of memory. And then TQDM is a kind of nice library that gives you progress bars. And so this can be good if you are running a for loop with things that are slow. It gives you like a more visual appearance of how you're going. And so this is just a simple example. I'm using sleep to make this a slow for loop. Here I'm running it, and I kind of don't see anything until it's finished. To use TQDM, you just wrap that around whatever you're iterating over. So here we've wrapped range 10 with TQDM. Now running the exact same loop, you see we get this progress bar that's updating as you go. So to get started, can someone remind us what the SVD is? Matthew and Vincent, can you throw the mic? Single value decomposition. Yes, singular value decomposition. And what does it give you? It gives you three different vacancies. One would be the singular value. The first and third are normal vacancies. Regarding the topic or whatever. Right, and that's actually kind of getting to the next question of what are some applications of SVD. So yeah, topic modeling was one of them. Yeah, so you're getting U, which is... So in the full version, U and V are both orthonormal square matrices. In the reduced version, U has orthonormal columns. Yeah, so Matthew got us started. What are some other applications of SVD? Roger? PCA? Yes. Yeah, so PCA is SVD. And what are we doing to get the low rank approximation? Exactly, yeah. So this is also called truncated SVD, where we kind of drop off part of them and just take the first few. Exactly. Other applications? There's one we spent an entire week on. Yeah, background removal. So we got pretty good results just using SVD. And then we used the robust PCA or primary component pursuit, and that used SVD as a step and kind of iteratively did multiple SVDs. And then just now we saw SVD as a way to calculate the least squares linear regression. Yeah, so hopefully you're convinced that SVD is useful and shows up lots of places. I'm kind of a little bit off topic, but I'm doing a workshop this afternoon on word embeddings, such as Word2Vec, which is like a library of word embeddings. And so I was looking at the problem of bias in Word2Vec. So word embeddings give you these analogies, which can be really great, like Spain is to Madrid, as Italy is to Rome. But they also have been found to have kind of biased analogies, such as father is to doctor, as mother is to nurse, or man is to computer programmer, as woman is to homemaker. And so there's been work done around how can you kind of remove this bias. And so I was reading a paper on it and they used SVD as part of their debiasing process. So I thought that was pretty neat. I was like, oh, this is a relevant application of SVD. So I just wanted to highlight that. Yeah, so a few different ways to think about SVD are data compression. So this comes up with the PCA or when you're dropping many of your kind of lower singular values, the less informative parts, you're, you know, getting something that fits in a smaller space. Another another way to think about or related way to think about it is SVD trades a large number of features for a smaller set of better features. And I think that's that's kind of what we saw with topic modeling of, you know, you could have a feature for each word or we could get the smaller set of features that are kind of groups of words that have, you know, related topics. And then I think this is kind of neat, but it's all matrices are actually diagonal. If you change the bases on the domain and range and. So this might be a good time. I was going to show the three blue one brown change of basis video. Maybe let me do that now. So, if I have a vector sitting here in 2D space, we have a standard way to describe it with coordinates. Can everyone hear, or does that need to be louder? Louder. Number. If I have a vector sitting here in 2D space, we have a standard way to describe it with coordinates. In this case, the vector has coordinates three, two, which means going from its tail to its tip involves moving three units to the right and two units up. Now, the more linear algebra oriented way to describe coordinates is to think of each of these numbers as a scalar, the thing that stretches or squishes vectors. You think of that first coordinate as scaling I hat, the vector with length one pointing to the right. While the second coordinate scales J hat, the vector with length one pointing straight up. The tip to tail sum of those two scale vectors is what the coordinates are meant to describe. You can think of these two special vectors as encapsulating all of the implicit assumptions of our coordinate system. The fact that the first number indicates rightward motion, that the second one indicates upward motion, exactly how far a unit of distance is, all of that is tied up in the choice of I hat and J hat as the vectors which our scalar coordinates are meant to actually scale. Any way to translate between vectors and set of numbers is called a coordinate system, and the two special vectors I hat and J hat are called the basis vectors of our standard coordinate system. What I'd like to talk about here is the idea of using a different set of basis vectors. For example, let's say you have a friend, Jennifer, who uses a different set of basis vectors, which I'll call B1 and B2. Her first basis vector, B1, points up and to the right a little bit, and her second vector, B2, points left and up. Now take another look at that vector that I showed earlier, the one that you and I would describe using the coordinates 3, 2, using our basis vectors I hat and J hat. Jennifer would actually describe this vector with the coordinates 5 thirds and 1 third. What this means is that the particular way to get to that vector using our two basis vectors is to scale B1 by 5 thirds, scale B2 by 1 third, then add them both together. In a little bit I'll show you how you could have figured out those two numbers, 5 thirds and 1 third. In general, whenever Jennifer uses coordinates to describe a vector, she thinks of her first coordinate as scaling B1, the second coordinate as scaling B2, and she adds the results. What she gets will typically be completely different from the vector that you and I would think of as having those coordinates. To be a little more precise about the setup here, her first basis vector, B1, is something that we would describe with the coordinates 2, 1, and her second basis vector, B2, is something that we would describe as negative 1, 1. But it's important to realize, from her perspective in her system, those vectors have coordinates 1, 0, and 0, 1. They are what define the meaning of the coordinates 1, 0, and 0, 1 in her world. So, in effect, we're speaking different languages. We're all looking at the same vectors in space, but Jennifer uses different words and numbers to describe them. Let me say a quick word about how I'm representing things here. When I animate 2D space, I typically use this square grid. But that grid is just a construct, a way to visualize our coordinate system, and so it depends on our choice of basis. Space itself has no intrinsic grid. Jennifer might draw her own grid, which would be an equally made-up construct meant as nothing more than a visual tool to help follow the meaning of her coordinates. Her origin, though, would actually line up with ours, since everybody agrees on what the coordinates 0, 0 should mean. It's the thing that you get when you scale any vector by 0. But the direction of her axes and the spacing of her grid lines will be different, depending on her choice of basis vectors. So, after all this is set up, a pretty natural question to ask is how do we translate between coordinate systems? If, for example, Jennifer describes a vector with coordinates negative 1, 2, what would that be in our coordinate system? How do you translate from her language to ours? Well, what our coordinates are saying is that this vector is negative 1 times b1 plus 2 times b2. And from our perspective, b1 has coordinates 2, 1, and b2 has coordinates negative 1, 1. So we can actually compute negative 1 times b1 plus 2 times b2 as they're represented in our coordinate system. And working this out, you get a vector with coordinates negative 4, 1. So that's how we would describe the vector that she thinks of as negative 1, 2. This process here of scaling each of her basis vectors by the corresponding coordinates of some vector, then adding them together, might feel somewhat familiar. It's matrix vector multiplication, with a matrix whose columns represent Jennifer's basis vectors in our language. In fact, once you understand matrix vector multiplication as applying a certain linear transformation, say by watching what I view to be the most important video in this series, Chapter 3, there's a pretty intuitive way to think about what's going on here. A matrix whose columns represent Jennifer's basis vectors can be thought of as a transformation that moves our basis vectors, i-hat and j-hat, the things we think of when we say 1, 0 and 0, 1, to Jennifer's basis vectors, the things she thinks of when she says 1, 0 and 0, 1. To show how this works, let's walk through what it would mean to take the vector that we think of as having coordinates negative 1, 2, and applying that transformation. Before the linear transformation, we're thinking of this vector as a certain linear combination of our basis vectors, negative 1 times i-hat plus 2 times j-hat. And the key feature of a linear transformation is that the resulting vector will be that same linear combination but of the new basis vectors, negative 1 times the place where i-hat lands plus 2 times the place where j-hat lands. So what this matrix does is transform our misconception of what Jennifer means into the actual vector that she's referring to. I remember that when I was first learning this, it always felt kind of backwards to me. Geometrically, this matrix transforms our grid into Jennifer's grid, but numerically, it's translating a vector described in her language to our language. What made it finally click for me was thinking about how it takes our misconception of what Jennifer means, the vector we get using the same coordinates but in our system, then it transforms it into the vector that she really meant. What about going the other way around? In the example I used earlier this video, when I had the vector with coordinates 3, 2 in our system, how did I compute that it would have coordinates 5 thirds and 1 third in Jennifer's system? You start with that change of basis matrix that translates Jennifer's language into ours, then you take its inverse. Remember, the inverse of a transformation is a new transformation that corresponds to playing that first one backwards. In practice, especially when you're working in more than two dimensions, you'd use a computer to compute the matrix that actually represents this inverse. In this case, the inverse of the change of basis matrix that has Jennifer's basis as its columns ends up working out to have columns 1 third, negative 1 third, and 1 third, 2 thirds. So for example, to see what the vector 3, 2 looks like in Jennifer's system, we multiply this inverse change of basis matrix by the vector 3, 2, which works out to be 5 thirds, 1 third. So that, in a nutshell, is how to translate the description of individual vectors back and forth between coordinate systems. The matrix whose columns represent Jennifer's basis vectors, but written in our coordinates, translates vectors from her language into our language. And the inverse matrix does the opposite. But vectors aren't the only thing that we describe using coordinates. For this next part, it's important that you're all comfortable representing transformations with matrices, and that you know how matrix multiplication corresponds to composing successive transformations. Definitely pause and take a look at chapters 3 and 4 if any of that feels uneasy. Consider some linear transformation, like a 90 degree counterclockwise rotation. When you and I represent this with a matrix, we follow where the basis vectors i-hat and j-hat each go. i-hat ends up at the spot with coordinates 0, 1, and j-hat ends up at the spot with coordinates negative 1, 0. So those coordinates become the columns of our matrix. But this representation is heavily tied up in our choice of basis vectors, from the fact that we're following i-hat and j-hat in the first place, to the fact that we're recording their landing spots in our own coordinate system. How would Jennifer describe this same 90 degree rotation of space? You might be tempted to just translate the columns of our rotation matrix into Jennifer's language, but that's not quite right. Those columns represent where our basis vectors i-hat and j-hat go, but the matrix that Jennifer wants should represent where her basis vectors land, and it needs to describe those landing spots in her language. Here's a common way to think of how this is done. Start with any vector written in Jennifer's language. Rather than trying to follow what happens to it in terms of her language, first we're going to translate it into our language using the change of basis matrix, the one whose columns represent her basis vectors in our language. This gives us the same vector, but now written in our language. Then, apply the transformation matrix to what you get by multiplying it on the left. This tells us where that vector lands, but still in our language. So as a last step, apply the inverse change of basis matrix, multiply it on the left as usual, to get the transformed vector, but now in Jennifer's language. Since we can do this with any vector written in her language, first applying the change of basis, then the transformation, then the inverse change of basis, that composition of three matrices gives us the transformation matrix in Jennifer's language. It takes in a vector of her language and spits out the transformed version of that vector in her language. For this specific example, when Jennifer's basis vectors look like 2,1 and negative 1,1 in our language, and when the transformation is a 90 degree rotation, the product of these three matrices, if you work through it, has columns 1 third, 5 thirds, and negative 2 thirds, negative 1 third. So if Jennifer multiplies that matrix by the coordinates of a vector in her system, it will return the 90 degree rotated version of that vector expressed in her coordinate system. In general, whenever you see an expression like A inverse times M times A, it suggests a mathematical sort of empathy. That middle matrix represents a transformation of some kind as you see it, and the outer two matrices represent the empathy, the shift in perspective. And the full matrix product represents that same transformation, but as someone else sees it. For those of you wondering why we care about alternate coordinate systems, the next video on eigenvectors and eigenvalues will give a really important example of this. See you then! Alright, so I really like that analogy of change of basis being like translating between languages, and the idea of empathy with multiplying by change of basis and inverse to change the basis back. So we'll be using those ideas a bit. We're not going to watch the eigen decomposition video in here, but you might want to watch that at home if you're interested. I think this is a good time to take a break. It's 11.55, so let's meet back in eight minutes, like at 12.03. And if you have questions about the video, maybe write them down and then we can talk about them when we meet back. Thanks. So yeah, we just saw the video on change of basis. Yeah, I really like the analogy about translating between languages. I also like that it showed, again, we've talked about how you can think of matrix multiplication as taking a linear combination of the columns, and that's what was going on with kind of the original basis. Sorry, with the set of Jennifer's basis, and then we're taking coefficients to translate into that. Were there any questions about the video? Okay. Yeah, so I wanted to highlight, so this is coming back. We had this statement, all matrices are diagonal if you use change of basis. And so that's kind of what's going on with this U sigma V. We can think of that as a change of basis to get into a space where you've got a diagonal matrix. And this, I regret that I didn't put a picture of this in here. I may be able to do this next time. A lot of times with PCA, this shows up. You'll kind of have the picture of, you know, like what the principal axes are. And you can think of that as kind of the basis vectors of this new basis that better represents your data set. Yeah, so we've been talking about SVD in terms of matrices. I'm not sure if this is going. But we can also think of it in terms of the individual vectors. And so if we think of SVD as giving us the vectors VJ and UJ, then we could have A times vector V equals sigma times vector U. And I kind of am showing the answers here. But my question was going to be, does this remind you of anything? And the answer is eigen decomposition. And just to remind you, if it's been a while since you've seen eigen decomposition, I define it here. Does anyone remember what the definition of an eigenvector, an eigenvalue is? Brad and can Roger throw the microphone? For a given matrix A, an eigenvector is a vector that when transformed by A is just a scaled, that vector is a scaled by a value lambda, which is the eigenvalue. Exactly. Yeah. So transforming V by A, which is like doing A times V is just scaling V. So it's equal to lambda times V. And so that's the definition of eigenvectors and eigenvalues. And so hopefully this looks somewhat similar to what we were talking about with A times V equals sigma times U. Kind of the key difference here is that V and U don't have to be equal. So those could be different values. But otherwise, we're talking about kind of when is matrix multiplication by a vector like scaling a vector? So going back here, and I want to go into great detail about it, but SVD is a generalization of eigendecompositions. So not all matrices have eigenvalues, but all matrices have singular values. And we could probably guess that SVD was more general by the fact that U and V don't have to be the same vector. So we're going to switch from talking about SVD to talking about how to find the eigenvalues, which is a kind of very closely related problem. So everything we say about eigendecomposition, remember that it's relevant to SVD. And we've just seen this list of applications of SVD kind of throughout this course. And here I link to several other kind of resources for more information about SVD. So the best classical methods for computing the SVD are all variants on methods for computing eigenvalues. And eigendecompositions are useful on their own as well. So a few practical applications of eigendecompositions. One is rapid matrix powers. And so the idea is that if you knew that A was equal to V times lambda times V inverse, here V is the eigenvectors, lambda is the eigenvalues. What would be true of lambda as a matrix? What property does this matrix have? Do you want to grab the microphone? Lambda has to be diagonal. Exactly. Yeah, lambda has to be diagonal. And that's kind of this idea of going back and forth between, you know, we can write eigenvectors and eigenvalues individually as vectors, you know, kind of what I had here. Or we could kind of put all the eigenvectors together and say, really, this is A times a matrix V, where the columns are the different eigenvectors equals, oh, except. To make that work, you have to make lambda into a matrix. And what that matrix is, is just diagonal with the little lambdas along the diagonal. Questions about getting between these? And so here this would often these would have a subscript. So if I say these are all sub i, then we're putting those together. Actually, let me write it out. I'm going to say everyone's been very quiet today, so it's hard for me to read. So she want me to go slower, quicker. But you could think of this as A times V1, V2, V3, and so on equals lambda 1, lambda 2, one down, slide over, down to lambda n. So this is V1, V2, up to Vn. So this is kind of a way to consolidate all those individual equations for the different Vi and the different lambda i into matrices. And then the idea with the rapid matrix powers is, you know, suppose we're interested in taking A to some power k. Doing that the naive way would be doing A times A, multiplying that by A, multiplying that by A. And what is what's the runtime for matrix multiplication? Tim, you want to throw the microphone to Sam? Yes, exactly. It's n cubed, which is really slow. So another way we could think about this. So now we've got A equals. Did I write this backwards? I'm sorry about this. Av and this side is V lambda. Yeah, I think that's correct. Thank you. Yeah, and the reason why that's true is because we want to be taking multiples of. Let me write this out. Yeah, we're interested in the columns V on this side. And so writing it like this. What this gives us is basically taking the first column, multiplying that by our matrix V is just going to give us lambda 1 V1. Then we're doing lambda 2 times V2 and so on. Sorry about that confusion. This should be Av times V lambda. Which then can be rewritten as A equals V lambda V inverse. So then going back to our problem of wanting the kth power, we can do A equals V lambda V inverse. The kth power and any ideas about how this could be reduced? Tim and Sam, can you throw the microphone back to Tim? Exactly. Yeah, so if you were to write these out a bunch of times and as Tim said, you end up with V lambda V inverse times V lambda V inverse times V lambda and so on. And this V inverse times V and V inverse times V are going to cancel out. However many, however many entries you have and you end up with the lambda to the k V inverse. And why is it okay that we have lambda to the k? Matthew? Oh, and do you want to throw the? It's diagonal so it's really easy to take the kth power. Exactly. Yeah, so since it's diagonal, taking the kth power is not a big deal at all. So this is a much more efficient way to, sorry there should be a kth power here, to compute the kth power of A. This is one application of eigenvectors. You can also use them to find the nth Fibonacci number, which is kind of neat. I'll just briefly show this one. But basically you can think of getting the Fibonacci numbers as going, taking this matrix 1 1 and 1 0 and multiplying that by 1 0 and then kind of continuing to do that to get the, you know, this is giving you on the top row the sum of the previous two things you have if you enter the previous two here. And then apply what we just saw with taking nth powers. It's kind of a fun application. And then we're not going to get into this in this class, but the behavior of ODEs. It's often if you're interested in the long term behavior of ODEs, what you'll end up needing to do is finding the eigen decomposition. And then mark off chains, which we will kind of be seeing today. Yeah, so we watched the three blue one brown video. Gilbert Strang, who's written a kind of classic linear algebra textbook, had a nice quote, eigenvalues are a way to see into the heart of a matrix. All the difficulties of matrices are swept away. But there's something, you know, really kind of fundamental about a matrix that's expressed in its eigenvalues. And then just a just some vocabulary that you might come across is Hermitian. And that means the matrix is equal to its conjugate transpose. In the case where you're dealing with real values, that's just saying that it's the same as being symmetric. It's equal to its transpose. And that's only if you have complex values, then you'd be flipping the sign on the complex values. But so far purposes, when you hear Hermitian, you can think symmetric. And then to add two useful theorems are if A is symmetric, then the eigenvalues of A are real and A equals Q lambda Q transpose. So that's really handy that here Q is its inverse is its transpose. And then if A is triangular, its eigenvalues are equal to its diagonal entries. This is a little bit of a spoiler, but if you are interested in the eigenvalues of a matrix, it would really be nice to have it in triangular form because then you can just get them from the diagonal. So today we're going to start with the power method, which finds just one eigenvector. And that's the basis for PageRank. And there's a paper, I really like the title of this, the $25 billion eigenvector, the linear algebra behind Google. So this is a very real world application of an eigenvector being important. And so we're going to be using a data set from DDPedia, which I think is a really neat resource. But they've compiled a bunch of Wikipedia data, as well as a lot of different classifications and categories of it. They have it in a bunch of different languages, and it's all kind of freely available. And so the data set we'll be using is kind of showing which pages link to which other pages. And so we'll be doing, so kind of the basic idea behind the original PageRank algorithm for Google search was that pages that a bunch of other pages link to must be more important, kind of that so many people are linking to that page. And prior to that, like the very early, kind of like Yahoo! pages had editors that like hand went out and selected links to be listed. And so this was kind of a big change to have an algorithm do it and kind of tell you how important different pages are. And so what we'll be doing is using this Wikipedia data of what pages are linking to what pages to kind of see what the most important pages are based on links. And I guess the other piece of that is you're normalizing for, you know, if a page links to 100 other pages, it's less special that they're kind of linking to each of those pages than if a page only links to two other pages. And that kind of carries more weight. So yeah, here this is just a little information about the full DBPedia data set has 38 million labels, abstracts in 125 different languages, 25 million links to images. So this is, I think, something to keep in mind for future projects. And so I sent a Slack message this weekend. This can be kind of slow to download. So you might want to wait till after class if you haven't done it already. So here this is just kind of opening, opening the data. I don't know why I got pasted in there. What we're going to do is construct a graph adjacency matrix about which pages point to which. And so this is a kind of very simple example of if you just had four pages, if A is pointing to B, C and D, we could represent that here in the first column. This is a little bit confusing. A is a matrix here. Up here it's the first node. We're putting ones to indicate the first node points to the second, third and fourth nodes. Then B only points to C. So the second node is only pointing to the third node. You can see in the second row of this matrix that there's just a one in the third spot, but nowhere else. So that one represents B is pointing to C. Down here, C is just pointing to A. So you can see that we have a one in the first spot saying C is pointing to A. C doesn't point to anything else. And then D just points to C. So in the third spot we've got a one. And so this is a directed graph. So the order definitely matters. It's not symmetric. Are there questions about this representation? So taking the power A squared will tell you how many ways there are to get from one page to another page in two steps. And actually, I found these notes, which I liked, and they use the example of airlines traveling. So they just have this kind of smaller graph, I guess in Massachusetts, between which towns can you fly between. They represent that as a matrix with these zeros and ones. And then they kind of take the power and say, OK, by taking the second power, we see there's one two-step sequence from C to F. So that shows up in A, B, C. We've got a one here. There's one way to just with two legs of your journey to get from C to F. There are five three-step sequences between C and F. So coming over here, yeah. So C is the third row, F is the sixth column, and we've got five. This is for M cubed. Are there questions about this? This would show up in logistics problems. All right. So the format of our data is it's kind of in files in these lists. And so we have both the redirects and the links, and we need to use both. And the redirects are just to kind of figure out which page is redirecting to other ones. And so the first one is kind of the source page that it's telling you it's a redirect or a link. And then the third argument is telling you the destination page it's pointing to. So this is just some kind of data processing to read in the lines, to split them. The redirects we're putting into this dictionary. I want to kind of come down. I think it's more interesting to kind of look at what we've created. So we have something called index map. And I've just an index map is a dictionary. But to see what's in it, I'm just kind of popping off a random element from it. And in general, generally you don't want to do this because I think that that alters your dictionary. But I just wanted to kind of it's always important to be able to see what your data is like. And I think it's more informative to see how our process data looks than to go through each step of the processing. But here it says 1940 Cincinnati Red's team issue relates to this index 9991173. So then we've got two lists of source and destination. And those are just the list of the same length and their list of indices. And so they're kind of telling you, you know, the seventh spot in source is the index of which page is a source pointing to the seventh spot in destination. And this so this is a very inefficient way to be working with these lists. And I'm doing this to kind of illustrate what they represent. But this isn't how you would actually be using them. But so I know Cincinnati Red's team issue is 9991173. Where does that even show up in the source list? It just shows up at one index 119077649. And so then I can look that up in the destination and I get 9991050. So the we have, I guess, two different types of indices going on. It's the indices being used by the index map and then the. Indices of source and destination, which just correspond to each other. All this to say, then we look up 9991050 and find out that that is page W711-2. I just want to say I actually tried popping like several elements off to see if I could find something more. Maybe more interesting, but some of them are like for pages that have been deleted because I think the DVDpedia data set is, you know, not super current. Wikipedia gets changed a lot. So this is actually the best I could do. But I went to Wikipedia and I looked up Cincinnati Red's team issue and it redirected me to W711-2. So that that that's showing that our data is representing what we want and we can get information from it. Although it does involve this kind of. You know, like having to search for where does the index appear in source? And then what does that correspond to in destination? And this is all talking about a pack of baseball cards. If you are wondering. And we can even open this link in a new tab. And it says W711-2 is also known as the 1940 Cincinnati Reds team issue, which is a baseball card set. And we'll take a look at it just to see some of the things that show up about baseball, Cincinnati Reds, Detroit Tigers. And then there are a bunch of names of players in here. And so let me just say that kind of one more time how this is working. Index map is a mapping between names of Wikipedia pages and a number representing them. And then we can look for those numbers in the source or destination list. And the. And then the source and destination lists kind of need to be paired together because things show up. You know, like the third entry of the source list is pointing to the third entry of the destination list. So are there questions about this? OK, so now I'm interested in so 9991050 is this W711-2 page, which represents this pack of baseball cards. And I want to know where that shows up in the source. So kind of what pages is the source pointing at? And I get back. It's actually a check of this. I get 47. So there are a lot that's saying that this Wikipedia page has 47 outgoing links on it, which seems very plausible to me. And then now I look those up in my index map to see what pages they correspond to. And so that's what I have here. And they correspond to baseball, baseball, Ohio, Cincinnati, Flash Thompson, 1940, 1938, Lonnie Frey, Cincinnati Reds, Ernie Lombardi. So this seems correct. Like our data is what we think it is. We've got the information that this W7 page is linking to all these other baseball related pages and names of the players. I just had a screen capture of that. So any questions about that, kind of that aspect of the data processing? OK, so then now we're going to use sparse.coo to create a sparse sparse matrix and then we're going to convert it to CSR. So two questions. First, what are coo and CSR? Matthew has the microphone if someone wants to help see. I think coo is the coordinate representation. Exactly. Yes. Yeah. So these are two different sparse representations. And so why would we create it with coo and then convert it in the next line? OK, Kelsey. Exactly. Yeah. So coo, I think kind of makes the most sense logically because with CSR, we have to do that kind of counting of we keep track of a row pointer. And so how many you know, how many values are in each row? When do we need to update our row pointer, which would be kind of a pain to do by hand. And there's efficient conversion between the different types. But coo, it's very natural to say this is our data and we want to do it by destination, comma source. So this is our rows are going to be the destinations, the columns are the sources. Put that into a sparse matrix and then convert it to CSR. And I wanted to highlight this. So this is the page that we've seen several times explaining. Kind of about the different sparse formats. And it points out over here at the bottom advantage of CSR method over the coordinate wise method is that. So the number of operations to perform matrix vector multiplication are the same for the two. However, the number of memory accesses is reduced by factor of two in the CSR method. So that's and this is a nice example of. You know, if you were just thinking about big O, it's going to be the same in terms of operations. However, this idea of memory access being something that matters and can be can slow us down. CSR is a lot better. Questions about that. OK, and so we'll be doing surprisingly some matrix vector multiplications, and so it's going to be faster to use CSR and that will have fewer memory accesses. And then I highly recommend if you're doing this at home, all this processing and creating your matrix can be a bit slow. And so when you get to this point, it's a good idea to save your matrix and you can use Pickle, which is a Python library for kind of compressing things. And then that way, if you want to come back to this later, you're not having to recreate your matrix every every single time you're using this notebook. This is something I recommend in general for any sort of data project where it's slow to compute the data. So here we. We save our index map and our data matrix X. I can check here X is a this is quite a large matrix about close to 12 million by 12 million sparse matrix. And then it's nice that it tells us this. It's so it was. It was I asked by a factor. Oh, OK, yeah, 100. Thank you. 120 million by 120 million matrix, and then it's got 94 million non zero entries, which is a lot less than 120 million squared. So we're saving a lot of space. Any questions about the setup? So now we're going to get into the power method. This is going to be how we're calculating our eigenvector. And again, our idea is that if you know, if you're just looking at Wikipedia is kind of like a stand in for the Internet, we're trying to find like what the most important pages are. Actually, I'm going to. Skip this part well. So this is. Similar to what we talked about below with the matrix powers, but the idea is that a matrix. So a matrix is diagonalizable if it has n linearly independent eigenvectors V1 through VN. And then any any vector can be expressed as a linear combination of the eigenvectors. And that's really handy because when we're looking at the matrix times a vector, having how it's expressed as an eigenvector or expressed in terms of the eigenvectors, let's just multiply them by the eigenvalue instead of having to do a matrix multiplication. And let me even say I want to. I want to write that one out. So let me start on a new page. But so if any vector W. Can be written as a linear combination for some scalars, call them CJ. Of the eigenvectors VJ, and that's because the V form the eigenvectors form a basis for your space. Then when you want to do a W. Pull this A inside. Remember C is just a scalar, so we can pull the matrix into that. And how can I rewrite this? The point is that the vectors V are eigenvectors of A. And wait, this one throw the microphone to Valentine. So can we just substitute a VJ with lambda J VJ? Exactly. Yes. Yeah. And this is just kind of the definition of what it means to be an eigenvector is that instead of having to multiply by a matrix, you can just multiply by a scalar. And so this is really nice because. There is nothing special about W here. W was any vector and it was being represented as a linear combination of the eigenvectors. And so we're kind of saying, oh, for any vector, you don't actually have to do the matrix multiplication. You can just use the eigenvalues, kind of multiplying by the basis. Questions about this? And so something to keep in mind is that you are taking powers of A, this and I'm not going to write it all out, but this basically becomes. The eigenvalue to a power. And so it's. That's significant with the eigenvalues are. And so. With our with our. Adjacency graph of the connections between the different web pages. This is also basically kind of like if you normalized it, it would be like the Markov chain, you know, like probabilities of going from one one page to another. So you can think of someone was randomly surfing the web and just happened to click on different links. Where would they end up? You can kind of get that behavior from. From a and so you're thinking about these kind of repeated powers of, you know, someone's kind of on the Internet or on Wikipedia. And if they're just like randomly clicking links, you know, over time, kind of what are the most important pages or what pages are they going to go to most often? And so that's why I will be kind of talking about powers of a because it's. You're kind of going from page to page again and again. Yeah, so we're going to we're going to normalize our matrix, and this is necessary. Yeah, in terms of thinking about probabilities also. Yeah, this keeps it as we take these huge powers from from getting too large. And you can do that using num pies some method over a particular axis axis. Yeah, so down here we have the power method. What we'll do is. Kind of use our our data. Then we're going to want to get the. Kind of the indices, so a dot indices will give us the indices that are non zero and we want to select those. To sum up, so we kind of going back to. If we just had that small four by four graph with the zeros and ones were interested in the non zero entries. I would want to if we had three non zero entries, one one one, we would want to change that to one third, one third, one third for that row. We'll take scores is a vector of ones of length and times the square root of a dot sum divided by N squared. And this is just kind of an initial guess that people are or that the pages kind of all have equal importance. And then we do a time scores at the norm, normalize and continue iterating a time scores again and so on. And so what this is doing is you can also kind of think of. Think of it as you know, if you have a thousand people on Wikipedia randomly clicking links, you know, you're seeing where they go step over step after step. If you did this with enough people for a long enough time, you would find this kind of distribution of like more people are on this page and not many people are on these pages. Yeah, so this is kind of the idea behind the power method. Why, why do you think we're normalizing the score on each iteration? And the scores just kind of the percent of people on the different pages. Matthew and Roger has the mic. Oh, great catch and throw. So that. And so this is a little bit confusing. We actually normalized twice, so we kind of that's what we're doing up here is trying to like normalize the counts by row and then down here. I called it scores because it's kind of like the importance of the different pages. But you could also think of that as like the percent of people on each page. So kind of the question I was asking was why are we normalizing the scores here? Yes, yeah, so the issue that could arise was the values could get way too small and underflow to zero. Or we could also have problems with the values getting way too big if we don't normalize and exploding. So that's why kind of whenever it's good, whenever you're doing an iterative process where you're multiplying every time, it's really good to think about normalizing because you don't want things to be. Exploding or vanishing. And so then here we can check like what are the scores we get here. We've done it for 10 iterations and the the top kind of the top pages are living people year of birth missing. United States, United Kingdom, race and ethnicity in the United States, census France. And so what this is is these are the pages that have kind of the most links pointing to them. And this is actually not not as interesting as it would be if we were using like the whole Internet or something. Right. Because here all people have a link pointing to the living people category on Wikipedia. So that's why that's a super scene is a super popular page because there would be a ton of links pointing to that kind of for any entry of a person and then your birth missing. It seems reasonable that actually a lot of people probably don't have their year of birth miss listed on Wikipedia and that those pages all point to this year of birth missing page. So this this makes sense as a like OK these are pages that I think a ton of links could be pointing to. So that sense they're important but it's not the same sense that you would probably get with the broader Internet of like hey this is a really popular page. Linda. Thank you for the microphone. So in the score so like 3.5 answer the score is low to high and then you show X. Does it mean that journey is most like brought up. This is this is confusing what I'm out putting here is actually the norm and that was just to kind of keep track of how the norm is is changing. And then I was the one question before the others is that what is like the third one. And so this is what's kind of turning all those ones into fractions of kind of percent. So you know before we had like you know a links to be C and D and we want to convert those to one third one third one third because that's your probability of going to each page. I just actually to answer when this. I was going to see if I could display the last. The last 10. I'll modify that for next time and we can see yeah like what the least important or least link to pages are because that might be fun. Other questions about this. So I have. Something called Krylov subspaces and those are kind of the spaces spanned by a times B a squared times B a cubed times B a to the fourth times B. And so I just wanted to highlight that we're we're kind of getting that here or we are getting that here by taking powers of a each time we go through this for loop. The other thing to note and we'll see this when we get to the QR algorithm is that the convergence rate of this method is the ratio of the largest eigenvalue to the second largest eigenvalue. So that could be good or bad depending on what the eigenvalues are and there's something called adding shifts basically where you're kind of subtracting off one of the eigenvalues to move it over which speeds up the convergence and then you add that value back on once it's once it's converged. And this this technique of shifts or deflation show up in a kind of number of numerical linear algebra algorithms. And so we were not going to get into great detail about them here, but just to be aware also that so deflation can be used to find eigenvalues are other than the greatest one. So here you know we were just finding the most largest eigenvalue the most significant eigenvector. But there are ways that this method could be modified to find others. Matthew. And he has the microphone. So, so intuitively that's just how kind of like how many iterations you're having to do to get to a reasonable answer. So you can think of that in a lot of cases that would be like how your air is decreasing with each iteration. So it's always a fraction between you. I mean I guess if you I'll look into how it's formally defined. It's kind of often talked about in this like more intuitive way. But yeah I'll look at the formal definition of that. Other questions. I hope for for next time I do want to cover the QR algorithm. I think that we won't finish all of this lesson seven notebook because we've run out of time a bit with the course and I want to I want to cover stuff in the lesson eight notebook. So we'll see a little bit more here and the notebook will be up if you're interested in this topic and want to want to go further but we won't. I think it's very unlikely that we would cover the Arnoldi iteration at this point. Just given time. So definitely be sure to make sure you've downloaded the lesson eight notebook before before next time and we will do a little bit more with the lesson seven notebook. All right.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.0, "text": " I'm going to go ahead and get started. I wanted to make a few announcements. One is that I will not be here next week, so there will be no class on Tuesday, and then Thursday is the exam, and David will be here.", "tokens": [286, 478, 516, 281, 352, 2286, 293, 483, 1409, 13, 286, 1415, 281, 652, 257, 1326, 23785, 13, 1485, 307, 300, 286, 486, 406, 312, 510, 958, 1243, 11, 370, 456, 486, 312, 572, 1508, 322, 10017, 11, 293, 550, 10383, 307, 264, 1139, 11, 293, 4389, 486, 312, 510, 13], "temperature": 0.0, "avg_logprob": -0.21145945503598168, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.01064495462924242}, {"id": 1, "seek": 0, "start": 15.0, "end": 22.0, "text": " David Juminski will be proctoring the exam, but I will have email so you can still email me questions about anything.", "tokens": [4389, 508, 449, 38984, 486, 312, 447, 1672, 278, 264, 1139, 11, 457, 286, 486, 362, 3796, 370, 291, 393, 920, 3796, 385, 1651, 466, 1340, 13], "temperature": 0.0, "avg_logprob": -0.21145945503598168, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.01064495462924242}, {"id": 2, "seek": 2200, "start": 22.0, "end": 35.0, "text": " Tuesday is when the final draft of the blog post is due, and also I've put up homework three, and it's just a single problem, but that's due Tuesday also.", "tokens": [10017, 307, 562, 264, 2572, 11206, 295, 264, 6968, 2183, 307, 3462, 11, 293, 611, 286, 600, 829, 493, 14578, 1045, 11, 293, 309, 311, 445, 257, 2167, 1154, 11, 457, 300, 311, 3462, 10017, 611, 13], "temperature": 0.0, "avg_logprob": -0.058431739156896416, "compression_ratio": 1.5954545454545455, "no_speech_prob": 6.0125683376099914e-05}, {"id": 3, "seek": 2200, "start": 35.0, "end": 47.0, "text": " And then I'm not going to have my normal office hours this Friday, but I could meet with you earlier in the day Friday, so if you want to meet on Friday or on Thursday afternoon, just let me know.", "tokens": [400, 550, 286, 478, 406, 516, 281, 362, 452, 2710, 3398, 2496, 341, 6984, 11, 457, 286, 727, 1677, 365, 291, 3071, 294, 264, 786, 6984, 11, 370, 498, 291, 528, 281, 1677, 322, 6984, 420, 322, 10383, 6499, 11, 445, 718, 385, 458, 13], "temperature": 0.0, "avg_logprob": -0.058431739156896416, "compression_ratio": 1.5954545454545455, "no_speech_prob": 6.0125683376099914e-05}, {"id": 4, "seek": 4700, "start": 47.0, "end": 61.0, "text": " All right. Yeah, so I was going to follow up with, we had some loose ends from last time for lesson six that I've added, kind of added into the bottom of the notebook.", "tokens": [1057, 558, 13, 865, 11, 370, 286, 390, 516, 281, 1524, 493, 365, 11, 321, 632, 512, 9612, 5314, 490, 1036, 565, 337, 6898, 2309, 300, 286, 600, 3869, 11, 733, 295, 3869, 666, 264, 2767, 295, 264, 21060, 13], "temperature": 0.0, "avg_logprob": -0.12294922608595628, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.00015591409464832395}, {"id": 5, "seek": 4700, "start": 61.0, "end": 71.0, "text": " Just to remind you, lesson six was about different ways of calculating the linear regression least squares.", "tokens": [1449, 281, 4160, 291, 11, 6898, 2309, 390, 466, 819, 2098, 295, 28258, 264, 8213, 24590, 1935, 19368, 13], "temperature": 0.0, "avg_logprob": -0.12294922608595628, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.00015591409464832395}, {"id": 6, "seek": 7100, "start": 71.0, "end": 78.0, "text": " And so this is coming from a question Tim asked about kind of full versus reduced factorizations.", "tokens": [400, 370, 341, 307, 1348, 490, 257, 1168, 7172, 2351, 466, 733, 295, 1577, 5717, 9212, 5952, 14455, 13], "temperature": 0.0, "avg_logprob": -0.08771981963192124, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.480435043456964e-05}, {"id": 7, "seek": 7100, "start": 78.0, "end": 92.0, "text": " And I've kind of been skimming over this, but I really are hand waving and not wanting to get into the detail, but it's actually good to look at, and it helped me realize I had an error in the SBD code, which is why it was so slow.", "tokens": [400, 286, 600, 733, 295, 668, 1110, 40471, 670, 341, 11, 457, 286, 534, 366, 1011, 35347, 293, 406, 7935, 281, 483, 666, 264, 2607, 11, 457, 309, 311, 767, 665, 281, 574, 412, 11, 293, 309, 4254, 385, 4325, 286, 632, 364, 6713, 294, 264, 26944, 35, 3089, 11, 597, 307, 983, 309, 390, 370, 2964, 13], "temperature": 0.0, "avg_logprob": -0.08771981963192124, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.480435043456964e-05}, {"id": 8, "seek": 9200, "start": 92.0, "end": 105.0, "text": " And so I'll show you that in a moment. But the idea is there's both a full SBD and a reduced SBD, and they're both referred to as SBD, so it's not always clear.", "tokens": [400, 370, 286, 603, 855, 291, 300, 294, 257, 1623, 13, 583, 264, 1558, 307, 456, 311, 1293, 257, 1577, 26944, 35, 293, 257, 9212, 26944, 35, 11, 293, 436, 434, 1293, 10839, 281, 382, 26944, 35, 11, 370, 309, 311, 406, 1009, 1850, 13], "temperature": 0.0, "avg_logprob": -0.06063628904890306, "compression_ratio": 1.5374449339207048, "no_speech_prob": 1.0782990102597978e-05}, {"id": 9, "seek": 9200, "start": 105.0, "end": 121.0, "text": " And the idea is that with the full SBD, U is a square matrix. And so for the kind of final, I guess, M minus N columns of U, you're just filling out what's needed for an orthonormal basis.", "tokens": [400, 264, 1558, 307, 300, 365, 264, 1577, 26944, 35, 11, 624, 307, 257, 3732, 8141, 13, 400, 370, 337, 264, 733, 295, 2572, 11, 286, 2041, 11, 376, 3175, 426, 13766, 295, 624, 11, 291, 434, 445, 10623, 484, 437, 311, 2978, 337, 364, 420, 11943, 24440, 5143, 13], "temperature": 0.0, "avg_logprob": -0.06063628904890306, "compression_ratio": 1.5374449339207048, "no_speech_prob": 1.0782990102597978e-05}, {"id": 10, "seek": 12100, "start": 121.0, "end": 129.0, "text": " But these are getting multiplied by all zeros. So you've kind of also added several rows, all of zero, to the base of sigma.", "tokens": [583, 613, 366, 1242, 17207, 538, 439, 35193, 13, 407, 291, 600, 733, 295, 611, 3869, 2940, 13241, 11, 439, 295, 4018, 11, 281, 264, 3096, 295, 12771, 13], "temperature": 0.0, "avg_logprob": -0.08156596085964105, "compression_ratio": 1.5217391304347827, "no_speech_prob": 7.527821253461298e-06}, {"id": 11, "seek": 12100, "start": 129.0, "end": 142.0, "text": " So it doesn't, like you want these columns to be such that you're creating an orthonormal basis in U, but beyond that, they actually don't have like a connection with your original data set.", "tokens": [407, 309, 1177, 380, 11, 411, 291, 528, 613, 13766, 281, 312, 1270, 300, 291, 434, 4084, 364, 420, 11943, 24440, 5143, 294, 624, 11, 457, 4399, 300, 11, 436, 767, 500, 380, 362, 411, 257, 4984, 365, 428, 3380, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.08156596085964105, "compression_ratio": 1.5217391304347827, "no_speech_prob": 7.527821253461298e-06}, {"id": 12, "seek": 14200, "start": 142.0, "end": 154.0, "text": " So that's kind of what's going on with U. And the reduced form is you're just finding kind of the columns that you need to represent A.", "tokens": [407, 300, 311, 733, 295, 437, 311, 516, 322, 365, 624, 13, 400, 264, 9212, 1254, 307, 291, 434, 445, 5006, 733, 295, 264, 13766, 300, 291, 643, 281, 2906, 316, 13], "temperature": 0.0, "avg_logprob": -0.07028683749112216, "compression_ratio": 1.5481927710843373, "no_speech_prob": 1.816195890569361e-06}, {"id": 13, "seek": 14200, "start": 154.0, "end": 162.0, "text": " And so notice both of these, you're getting your original matrix A back, since here you've kind of got these zeroing out.", "tokens": [400, 370, 3449, 1293, 295, 613, 11, 291, 434, 1242, 428, 3380, 8141, 316, 646, 11, 1670, 510, 291, 600, 733, 295, 658, 613, 4018, 278, 484, 13], "temperature": 0.0, "avg_logprob": -0.07028683749112216, "compression_ratio": 1.5481927710843373, "no_speech_prob": 1.816195890569361e-06}, {"id": 14, "seek": 16200, "start": 162.0, "end": 175.0, "text": " The full SBD would be useful if you needed U to be a kind of square orthonormal matrix. Are there questions about this? Linda?", "tokens": [440, 1577, 26944, 35, 576, 312, 4420, 498, 291, 2978, 624, 281, 312, 257, 733, 295, 3732, 420, 11943, 24440, 8141, 13, 2014, 456, 1651, 466, 341, 30, 20324, 30], "temperature": 0.0, "avg_logprob": -0.10913618873147403, "compression_ratio": 1.1454545454545455, "no_speech_prob": 4.092850304004969e-06}, {"id": 15, "seek": 17500, "start": 175.0, "end": 196.0, "text": " I don't think I get. So if the full SBD is not a square matrix, then how is it a singular, like on the diagonal, it shows the importance factor.", "tokens": [286, 500, 380, 519, 286, 483, 13, 407, 498, 264, 1577, 26944, 35, 307, 406, 257, 3732, 8141, 11, 550, 577, 307, 309, 257, 20010, 11, 411, 322, 264, 21539, 11, 309, 3110, 264, 7379, 5952, 13], "temperature": 0.0, "avg_logprob": -0.2212493896484375, "compression_ratio": 1.4275862068965517, "no_speech_prob": 1.3419053175312001e-05}, {"id": 16, "seek": 17500, "start": 196.0, "end": 202.0, "text": " And if it's not square, how does it show for the last part of?", "tokens": [400, 498, 309, 311, 406, 3732, 11, 577, 775, 309, 855, 337, 264, 1036, 644, 295, 30], "temperature": 0.0, "avg_logprob": -0.2212493896484375, "compression_ratio": 1.4275862068965517, "no_speech_prob": 1.3419053175312001e-05}, {"id": 17, "seek": 20200, "start": 202.0, "end": 208.0, "text": " OK, yeah, that's a great question. So the diagonal of sigma is what's giving you the importance.", "tokens": [2264, 11, 1338, 11, 300, 311, 257, 869, 1168, 13, 407, 264, 21539, 295, 12771, 307, 437, 311, 2902, 291, 264, 7379, 13], "temperature": 0.0, "avg_logprob": -0.08429386741236637, "compression_ratio": 1.6940639269406392, "no_speech_prob": 2.8408756406861357e-05}, {"id": 18, "seek": 20200, "start": 208.0, "end": 219.0, "text": " And so there's no importance corresponding to these vectors right here. And that's because those vectors aren't even a part of A.", "tokens": [400, 370, 456, 311, 572, 7379, 11760, 281, 613, 18875, 558, 510, 13, 400, 300, 311, 570, 729, 18875, 3212, 380, 754, 257, 644, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.08429386741236637, "compression_ratio": 1.6940639269406392, "no_speech_prob": 2.8408756406861357e-05}, {"id": 19, "seek": 20200, "start": 219.0, "end": 229.0, "text": " So their importance is like kind of like zero. You know, they show up zero amount in A. So if they are not a part of A, where do they come from?", "tokens": [407, 641, 7379, 307, 411, 733, 295, 411, 4018, 13, 509, 458, 11, 436, 855, 493, 4018, 2372, 294, 316, 13, 407, 498, 436, 366, 406, 257, 644, 295, 316, 11, 689, 360, 436, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.08429386741236637, "compression_ratio": 1.6940639269406392, "no_speech_prob": 2.8408756406861357e-05}, {"id": 20, "seek": 22900, "start": 229.0, "end": 233.0, "text": " And so that comes from just trying to create an orthonormal basis.", "tokens": [400, 370, 300, 1487, 490, 445, 1382, 281, 1884, 364, 420, 11943, 24440, 5143, 13], "temperature": 0.0, "avg_logprob": -0.05669329383156516, "compression_ratio": 1.6277056277056277, "no_speech_prob": 7.183152774814516e-06}, {"id": 21, "seek": 22900, "start": 233.0, "end": 240.0, "text": " And so there that's kind of like around the idea of like, oh, if you want you to be square and orthonormal,", "tokens": [400, 370, 456, 300, 311, 733, 295, 411, 926, 264, 1558, 295, 411, 11, 1954, 11, 498, 291, 528, 291, 281, 312, 3732, 293, 420, 11943, 24440, 11], "temperature": 0.0, "avg_logprob": -0.05669329383156516, "compression_ratio": 1.6277056277056277, "no_speech_prob": 7.183152774814516e-06}, {"id": 22, "seek": 22900, "start": 240.0, "end": 247.0, "text": " you kind of need to come up with these additional columns, even though they're not a part of A.", "tokens": [291, 733, 295, 643, 281, 808, 493, 365, 613, 4497, 13766, 11, 754, 1673, 436, 434, 406, 257, 644, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.05669329383156516, "compression_ratio": 1.6277056277056277, "no_speech_prob": 7.183152774814516e-06}, {"id": 23, "seek": 22900, "start": 247.0, "end": 258.0, "text": " That's a good question. Any other questions about this? Tim? And Linda, can you pass the microphone back?", "tokens": [663, 311, 257, 665, 1168, 13, 2639, 661, 1651, 466, 341, 30, 7172, 30, 400, 20324, 11, 393, 291, 1320, 264, 10952, 646, 30], "temperature": 0.0, "avg_logprob": -0.05669329383156516, "compression_ratio": 1.6277056277056277, "no_speech_prob": 7.183152774814516e-06}, {"id": 24, "seek": 25800, "start": 258.0, "end": 264.0, "text": " So I can pass V. When you did the reduced SVD, the V star didn't have to be square either.", "tokens": [407, 286, 393, 1320, 691, 13, 1133, 291, 630, 264, 9212, 31910, 35, 11, 264, 691, 3543, 994, 380, 362, 281, 312, 3732, 2139, 13], "temperature": 0.0, "avg_logprob": -0.12769211297747732, "compression_ratio": 1.5215311004784688, "no_speech_prob": 2.318541555723641e-05}, {"id": 25, "seek": 25800, "start": 264.0, "end": 272.0, "text": " It could have been rectangular. So is that because you're dropping some of the non-important singular values out of sigma?", "tokens": [467, 727, 362, 668, 31167, 13, 407, 307, 300, 570, 291, 434, 13601, 512, 295, 264, 2107, 12, 41654, 20010, 4190, 484, 295, 12771, 30], "temperature": 0.0, "avg_logprob": -0.12769211297747732, "compression_ratio": 1.5215311004784688, "no_speech_prob": 2.318541555723641e-05}, {"id": 26, "seek": 25800, "start": 272.0, "end": 280.0, "text": " Oh, so for the yeah. And so I should also note that the reduced SVD is different than the truncated SVD.", "tokens": [876, 11, 370, 337, 264, 1338, 13, 400, 370, 286, 820, 611, 3637, 300, 264, 9212, 31910, 35, 307, 819, 813, 264, 504, 409, 66, 770, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.12769211297747732, "compression_ratio": 1.5215311004784688, "no_speech_prob": 2.318541555723641e-05}, {"id": 27, "seek": 28000, "start": 280.0, "end": 291.0, "text": " So yeah, for truncated SVD, what you would do is if I can write on the screen.", "tokens": [407, 1338, 11, 337, 504, 409, 66, 770, 31910, 35, 11, 437, 291, 576, 360, 307, 498, 286, 393, 2464, 322, 264, 2568, 13], "temperature": 0.0, "avg_logprob": -0.14895562280582475, "compression_ratio": 1.4787878787878788, "no_speech_prob": 1.6186935681616887e-05}, {"id": 28, "seek": 28000, "start": 291.0, "end": 299.0, "text": " OK, I didn't sync it. The idea with reduced SVD, let's use the mouse, is you're kind of sorry,", "tokens": [2264, 11, 286, 994, 380, 20271, 309, 13, 440, 1558, 365, 9212, 31910, 35, 11, 718, 311, 764, 264, 9719, 11, 307, 291, 434, 733, 295, 2597, 11], "temperature": 0.0, "avg_logprob": -0.14895562280582475, "compression_ratio": 1.4787878787878788, "no_speech_prob": 1.6186935681616887e-05}, {"id": 29, "seek": 28000, "start": 299.0, "end": 305.0, "text": " truncated SVD is you're cutting off this kind of right hand side here.", "tokens": [504, 409, 66, 770, 31910, 35, 307, 291, 434, 6492, 766, 341, 733, 295, 558, 1011, 1252, 510, 13], "temperature": 0.0, "avg_logprob": -0.14895562280582475, "compression_ratio": 1.4787878787878788, "no_speech_prob": 1.6186935681616887e-05}, {"id": 30, "seek": 30500, "start": 305.0, "end": 315.0, "text": " Or no, you would be cutting off the bottom. Well, yeah, you want to cut off rows of V.", "tokens": [1610, 572, 11, 291, 576, 312, 6492, 766, 264, 2767, 13, 1042, 11, 1338, 11, 291, 528, 281, 1723, 766, 13241, 295, 691, 13], "temperature": 0.0, "avg_logprob": -0.16039246384815503, "compression_ratio": 1.8579545454545454, "no_speech_prob": 5.143802991369739e-05}, {"id": 31, "seek": 30500, "start": 315.0, "end": 321.0, "text": " Yeah, and so then you have to cut off the bottom few rows of V. And so that's when you end up with the rectangular V.", "tokens": [865, 11, 293, 370, 550, 291, 362, 281, 1723, 766, 264, 2767, 1326, 13241, 295, 691, 13, 400, 370, 300, 311, 562, 291, 917, 493, 365, 264, 31167, 691, 13], "temperature": 0.0, "avg_logprob": -0.16039246384815503, "compression_ratio": 1.8579545454545454, "no_speech_prob": 5.143802991369739e-05}, {"id": 32, "seek": 30500, "start": 321.0, "end": 328.0, "text": " And that would also cut off some columns, more columns of V, right? Yes, exactly. Yeah, and you cut off more columns of V.", "tokens": [400, 300, 576, 611, 1723, 766, 512, 13766, 11, 544, 13766, 295, 691, 11, 558, 30, 1079, 11, 2293, 13, 865, 11, 293, 291, 1723, 766, 544, 13766, 295, 691, 13], "temperature": 0.0, "avg_logprob": -0.16039246384815503, "compression_ratio": 1.8579545454545454, "no_speech_prob": 5.143802991369739e-05}, {"id": 33, "seek": 32800, "start": 328.0, "end": 337.0, "text": " So but in reduced SVD, V star remains square. Yes, yes. Good question.", "tokens": [407, 457, 294, 9212, 31910, 35, 11, 691, 3543, 7023, 3732, 13, 1079, 11, 2086, 13, 2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.11914450223328638, "compression_ratio": 1.3333333333333333, "no_speech_prob": 7.071690561133437e-06}, {"id": 34, "seek": 32800, "start": 337.0, "end": 344.0, "text": " Are there questions about this?", "tokens": [2014, 456, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.11914450223328638, "compression_ratio": 1.3333333333333333, "no_speech_prob": 7.071690561133437e-06}, {"id": 35, "seek": 32800, "start": 344.0, "end": 352.0, "text": " OK, so doing this, I thought to check on SVD, and it turns out the default is to do the full SVD.", "tokens": [2264, 11, 370, 884, 341, 11, 286, 1194, 281, 1520, 322, 31910, 35, 11, 293, 309, 4523, 484, 264, 7576, 307, 281, 360, 264, 1577, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.11914450223328638, "compression_ratio": 1.3333333333333333, "no_speech_prob": 7.071690561133437e-06}, {"id": 36, "seek": 35200, "start": 352.0, "end": 362.0, "text": " And so I'll let you guess which one of these do you think is faster to calculate, the full or reduced?", "tokens": [400, 370, 286, 603, 718, 291, 2041, 597, 472, 295, 613, 360, 291, 519, 307, 4663, 281, 8873, 11, 264, 1577, 420, 9212, 30], "temperature": 0.0, "avg_logprob": -0.09075461612658554, "compression_ratio": 1.6203703703703705, "no_speech_prob": 9.665915058576502e-06}, {"id": 37, "seek": 35200, "start": 362.0, "end": 366.0, "text": " People are laughing, but yeah, so the reduced is faster to calculate.", "tokens": [3432, 366, 5059, 11, 457, 1338, 11, 370, 264, 9212, 307, 4663, 281, 8873, 13], "temperature": 0.0, "avg_logprob": -0.09075461612658554, "compression_ratio": 1.6203703703703705, "no_speech_prob": 9.665915058576502e-06}, {"id": 38, "seek": 35200, "start": 366.0, "end": 375.0, "text": " And so above in my least squares linear regression, I was calculating the full. So let me go back to SVD.", "tokens": [400, 370, 3673, 294, 452, 1935, 19368, 8213, 24590, 11, 286, 390, 28258, 264, 1577, 13, 407, 718, 385, 352, 646, 281, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.09075461612658554, "compression_ratio": 1.6203703703703705, "no_speech_prob": 9.665915058576502e-06}, {"id": 39, "seek": 35200, "start": 375.0, "end": 380.0, "text": " So what you need to do is use the full matrices equals false parameter.", "tokens": [407, 437, 291, 643, 281, 360, 307, 764, 264, 1577, 32284, 6915, 7908, 13075, 13], "temperature": 0.0, "avg_logprob": -0.09075461612658554, "compression_ratio": 1.6203703703703705, "no_speech_prob": 9.665915058576502e-06}, {"id": 40, "seek": 38000, "start": 380.0, "end": 387.0, "text": " And so this is here in the least squares SVD.", "tokens": [400, 370, 341, 307, 510, 294, 264, 1935, 19368, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.1100621223449707, "compression_ratio": 1.5192307692307692, "no_speech_prob": 8.530222658009734e-06}, {"id": 41, "seek": 38000, "start": 387.0, "end": 391.0, "text": " And then that takes out and I should have known this. I was like slicing off the first end.", "tokens": [400, 550, 300, 2516, 484, 293, 286, 820, 362, 2570, 341, 13, 286, 390, 411, 46586, 766, 264, 700, 917, 13], "temperature": 0.0, "avg_logprob": -0.1100621223449707, "compression_ratio": 1.5192307692307692, "no_speech_prob": 8.530222658009734e-06}, {"id": 42, "seek": 38000, "start": 391.0, "end": 404.0, "text": " So I should have known like I'm calculating too many, the fact that I'm having to throw these away.", "tokens": [407, 286, 820, 362, 2570, 411, 286, 478, 28258, 886, 867, 11, 264, 1186, 300, 286, 478, 1419, 281, 3507, 613, 1314, 13], "temperature": 0.0, "avg_logprob": -0.1100621223449707, "compression_ratio": 1.5192307692307692, "no_speech_prob": 8.530222658009734e-06}, {"id": 43, "seek": 40400, "start": 404.0, "end": 412.0, "text": " So then I re-ran all the timing and SVD is a lot more reasonable now. So let me show you.", "tokens": [407, 550, 286, 319, 12, 4257, 439, 264, 10822, 293, 31910, 35, 307, 257, 688, 544, 10585, 586, 13, 407, 718, 385, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.14293431674732882, "compression_ratio": 1.5185185185185186, "no_speech_prob": 4.565924427879509e-06}, {"id": 44, "seek": 40400, "start": 412.0, "end": 422.0, "text": " So here this is this first table. Is this a good size for the maybe one bigger?", "tokens": [407, 510, 341, 307, 341, 700, 3199, 13, 1119, 341, 257, 665, 2744, 337, 264, 1310, 472, 3801, 30], "temperature": 0.0, "avg_logprob": -0.14293431674732882, "compression_ratio": 1.5185185185185186, "no_speech_prob": 4.565924427879509e-06}, {"id": 45, "seek": 40400, "start": 422.0, "end": 425.0, "text": " No, that's too big to fit.", "tokens": [883, 11, 300, 311, 886, 955, 281, 3318, 13], "temperature": 0.0, "avg_logprob": -0.14293431674732882, "compression_ratio": 1.5185185185185186, "no_speech_prob": 4.565924427879509e-06}, {"id": 46, "seek": 40400, "start": 425.0, "end": 433.0, "text": " The SVD is still for most of these a bit a little bit slower than the others here, though.", "tokens": [440, 31910, 35, 307, 920, 337, 881, 295, 613, 257, 857, 257, 707, 857, 14009, 813, 264, 2357, 510, 11, 1673, 13], "temperature": 0.0, "avg_logprob": -0.14293431674732882, "compression_ratio": 1.5185185185185186, "no_speech_prob": 4.565924427879509e-06}, {"id": 47, "seek": 43300, "start": 433.0, "end": 440.0, "text": " You'll notice SVD is slightly faster than QR in the case that's a thousand by twenty.", "tokens": [509, 603, 3449, 31910, 35, 307, 4748, 4663, 813, 32784, 294, 264, 1389, 300, 311, 257, 4714, 538, 7699, 13], "temperature": 0.0, "avg_logprob": -0.10022023212478821, "compression_ratio": 1.4355555555555555, "no_speech_prob": 9.972613952413667e-06}, {"id": 48, "seek": 43300, "start": 440.0, "end": 446.0, "text": " We've got 0.019 compared to 0.018 for SVD.", "tokens": [492, 600, 658, 1958, 13, 15, 3405, 5347, 281, 1958, 13, 15, 6494, 337, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.10022023212478821, "compression_ratio": 1.4355555555555555, "no_speech_prob": 9.972613952413667e-06}, {"id": 49, "seek": 43300, "start": 446.0, "end": 453.0, "text": " And again, just to kind of summarize, this is a bunch of different methods for finding the least squares linear regression", "tokens": [400, 797, 11, 445, 281, 733, 295, 20858, 11, 341, 307, 257, 3840, 295, 819, 7150, 337, 5006, 264, 1935, 19368, 8213, 24590], "temperature": 0.0, "avg_logprob": -0.10022023212478821, "compression_ratio": 1.4355555555555555, "no_speech_prob": 9.972613952413667e-06}, {"id": 50, "seek": 43300, "start": 453.0, "end": 458.0, "text": " being applied to the same set of matrices that were randomly generated.", "tokens": [885, 6456, 281, 264, 912, 992, 295, 32284, 300, 645, 16979, 10833, 13], "temperature": 0.0, "avg_logprob": -0.10022023212478821, "compression_ratio": 1.4355555555555555, "no_speech_prob": 9.972613952413667e-06}, {"id": 51, "seek": 45800, "start": 458.0, "end": 464.0, "text": " We went through in a loop and randomly generated matrices of different sizes, solved the problem,", "tokens": [492, 1437, 807, 294, 257, 6367, 293, 16979, 10833, 32284, 295, 819, 11602, 11, 13041, 264, 1154, 11], "temperature": 0.0, "avg_logprob": -0.15929133551461355, "compression_ratio": 1.4451219512195121, "no_speech_prob": 1.7603197193238884e-06}, {"id": 52, "seek": 45800, "start": 464.0, "end": 473.0, "text": " I guess five different ways and saw how the time and the error compared.", "tokens": [286, 2041, 1732, 819, 2098, 293, 1866, 577, 264, 565, 293, 264, 6713, 5347, 13], "temperature": 0.0, "avg_logprob": -0.15929133551461355, "compression_ratio": 1.4451219512195121, "no_speech_prob": 1.7603197193238884e-06}, {"id": 53, "seek": 45800, "start": 473.0, "end": 480.0, "text": " Let me see if anything else stands out as particularly noteworthy.", "tokens": [961, 385, 536, 498, 1340, 1646, 7382, 484, 382, 4098, 406, 1023, 2652, 88, 13], "temperature": 0.0, "avg_logprob": -0.15929133551461355, "compression_ratio": 1.4451219512195121, "no_speech_prob": 1.7603197193238884e-06}, {"id": 54, "seek": 48000, "start": 480.0, "end": 490.0, "text": " But yeah, so this puts SVD in a much, much more reasonable speed range.", "tokens": [583, 1338, 11, 370, 341, 8137, 31910, 35, 294, 257, 709, 11, 709, 544, 10585, 3073, 3613, 13], "temperature": 0.0, "avg_logprob": -0.09488654515099904, "compression_ratio": 1.497142857142857, "no_speech_prob": 3.4464117106836056e-06}, {"id": 55, "seek": 48000, "start": 490.0, "end": 495.0, "text": " Are there questions or kind of general questions on this this comparison?", "tokens": [2014, 456, 1651, 420, 733, 295, 2674, 1651, 322, 341, 341, 9660, 30], "temperature": 0.0, "avg_logprob": -0.09488654515099904, "compression_ratio": 1.497142857142857, "no_speech_prob": 3.4464117106836056e-06}, {"id": 56, "seek": 48000, "start": 495.0, "end": 506.0, "text": " We're about to go into some specific examples where we get worse error rates for some of the some of the algorithms.", "tokens": [492, 434, 466, 281, 352, 666, 512, 2685, 5110, 689, 321, 483, 5324, 6713, 6846, 337, 512, 295, 264, 512, 295, 264, 14642, 13], "temperature": 0.0, "avg_logprob": -0.09488654515099904, "compression_ratio": 1.497142857142857, "no_speech_prob": 3.4464117106836056e-06}, {"id": 57, "seek": 50600, "start": 506.0, "end": 530.0, "text": " So overall, what seems to be the fastest?", "tokens": [407, 4787, 11, 437, 2544, 281, 312, 264, 14573, 30], "temperature": 0.0, "avg_logprob": -0.10066186530249459, "compression_ratio": 0.8367346938775511, "no_speech_prob": 4.425246515893377e-06}, {"id": 58, "seek": 53000, "start": 530.0, "end": 539.0, "text": " And this might depend on the size.", "tokens": [400, 341, 1062, 5672, 322, 264, 2744, 13], "temperature": 0.0, "avg_logprob": -0.23120892301518867, "compression_ratio": 1.226890756302521, "no_speech_prob": 2.2957856344874017e-06}, {"id": 59, "seek": 53000, "start": 539.0, "end": 543.0, "text": " Tim, can you pass the microphone forward?", "tokens": [7172, 11, 393, 291, 1320, 264, 10952, 2128, 30], "temperature": 0.0, "avg_logprob": -0.23120892301518867, "compression_ratio": 1.226890756302521, "no_speech_prob": 2.2957856344874017e-06}, {"id": 60, "seek": 53000, "start": 543.0, "end": 549.0, "text": " Did you say it, Linda?", "tokens": [2589, 291, 584, 309, 11, 20324, 30], "temperature": 0.0, "avg_logprob": -0.23120892301518867, "compression_ratio": 1.226890756302521, "no_speech_prob": 2.2957856344874017e-06}, {"id": 61, "seek": 53000, "start": 549.0, "end": 553.0, "text": " Yeah, Tolesky is the fastest for most of them.", "tokens": [865, 11, 314, 7456, 4133, 307, 264, 14573, 337, 881, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.23120892301518867, "compression_ratio": 1.226890756302521, "no_speech_prob": 2.2957856344874017e-06}, {"id": 62, "seek": 55300, "start": 553.0, "end": 562.0, "text": " Correct. Any other observations about this?", "tokens": [12753, 13, 2639, 661, 18163, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.07585996024462642, "compression_ratio": 1.3093525179856116, "no_speech_prob": 4.029250703752041e-06}, {"id": 63, "seek": 55300, "start": 562.0, "end": 565.0, "text": " OK, let's go back.", "tokens": [2264, 11, 718, 311, 352, 646, 13], "temperature": 0.0, "avg_logprob": -0.07585996024462642, "compression_ratio": 1.3093525179856116, "no_speech_prob": 4.029250703752041e-06}, {"id": 64, "seek": 55300, "start": 565.0, "end": 577.0, "text": " Oh, and then while we're talking about full versus reduced, I wanted to say that QR also has full and reduced versions.", "tokens": [876, 11, 293, 550, 1339, 321, 434, 1417, 466, 1577, 5717, 9212, 11, 286, 1415, 281, 584, 300, 32784, 611, 575, 1577, 293, 9212, 9606, 13], "temperature": 0.0, "avg_logprob": -0.07585996024462642, "compression_ratio": 1.3093525179856116, "no_speech_prob": 4.029250703752041e-06}, {"id": 65, "seek": 57700, "start": 577.0, "end": 586.0, "text": " And so it's a very similar idea if you've got a rectangular A and the reduced version, you're just getting a rectangular Q and a square R,", "tokens": [400, 370, 309, 311, 257, 588, 2531, 1558, 498, 291, 600, 658, 257, 31167, 316, 293, 264, 9212, 3037, 11, 291, 434, 445, 1242, 257, 31167, 1249, 293, 257, 3732, 497, 11], "temperature": 0.0, "avg_logprob": -0.0955209732055664, "compression_ratio": 1.673170731707317, "no_speech_prob": 7.76629622123437e-06}, {"id": 66, "seek": 57700, "start": 586.0, "end": 590.0, "text": " which is upper triangular.", "tokens": [597, 307, 6597, 38190, 13], "temperature": 0.0, "avg_logprob": -0.0955209732055664, "compression_ratio": 1.673170731707317, "no_speech_prob": 7.76629622123437e-06}, {"id": 67, "seek": 57700, "start": 590.0, "end": 596.0, "text": " And then for the full version, you're adding additional columns onto Q to make an orthonormal basis.", "tokens": [400, 550, 337, 264, 1577, 3037, 11, 291, 434, 5127, 4497, 13766, 3911, 1249, 281, 652, 364, 420, 11943, 24440, 5143, 13], "temperature": 0.0, "avg_logprob": -0.0955209732055664, "compression_ratio": 1.673170731707317, "no_speech_prob": 7.76629622123437e-06}, {"id": 68, "seek": 57700, "start": 596.0, "end": 600.0, "text": " And then you're adding rows of zeros to R. So those are going to cancel out.", "tokens": [400, 550, 291, 434, 5127, 13241, 295, 35193, 281, 497, 13, 407, 729, 366, 516, 281, 10373, 484, 13], "temperature": 0.0, "avg_logprob": -0.0955209732055664, "compression_ratio": 1.673170731707317, "no_speech_prob": 7.76629622123437e-06}, {"id": 69, "seek": 60000, "start": 600.0, "end": 616.0, "text": " So these columns of Q are not actually part of A, but they're nice if you wanted a square square matrix that was orthonormal.", "tokens": [407, 613, 13766, 295, 1249, 366, 406, 767, 644, 295, 316, 11, 457, 436, 434, 1481, 498, 291, 1415, 257, 3732, 3732, 8141, 300, 390, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.09443157997684203, "compression_ratio": 1.4111111111111112, "no_speech_prob": 1.7880344103105017e-06}, {"id": 70, "seek": 60000, "start": 616.0, "end": 618.0, "text": " OK, so I wanted to address.", "tokens": [2264, 11, 370, 286, 1415, 281, 2985, 13], "temperature": 0.0, "avg_logprob": -0.09443157997684203, "compression_ratio": 1.4111111111111112, "no_speech_prob": 1.7880344103105017e-06}, {"id": 71, "seek": 60000, "start": 618.0, "end": 623.0, "text": " So kind of comparing these algorithms, if we just look at speed, Tolesky seems really good in a lot.", "tokens": [407, 733, 295, 15763, 613, 14642, 11, 498, 321, 445, 574, 412, 3073, 11, 314, 7456, 4133, 2544, 534, 665, 294, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.09443157997684203, "compression_ratio": 1.4111111111111112, "no_speech_prob": 1.7880344103105017e-06}, {"id": 72, "seek": 62300, "start": 623.0, "end": 634.0, "text": " And then also taking the matrix inverse seemed to be pretty fast. And I think we were getting that it had the same air as everything else.", "tokens": [400, 550, 611, 1940, 264, 8141, 17340, 6576, 281, 312, 1238, 2370, 13, 400, 286, 519, 321, 645, 1242, 300, 309, 632, 264, 912, 1988, 382, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.07714107666892567, "compression_ratio": 1.7412935323383085, "no_speech_prob": 3.041512400159263e-06}, {"id": 73, "seek": 62300, "start": 634.0, "end": 638.0, "text": " Let me confirm. Yeah, we were getting the same air as everything else.", "tokens": [961, 385, 9064, 13, 865, 11, 321, 645, 1242, 264, 912, 1988, 382, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.07714107666892567, "compression_ratio": 1.7412935323383085, "no_speech_prob": 3.041512400159263e-06}, {"id": 74, "seek": 62300, "start": 638.0, "end": 643.0, "text": " But in practice, you never want to do this. And so I wanted to talk more about why that is.", "tokens": [583, 294, 3124, 11, 291, 1128, 528, 281, 360, 341, 13, 400, 370, 286, 1415, 281, 751, 544, 466, 983, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.07714107666892567, "compression_ratio": 1.7412935323383085, "no_speech_prob": 3.041512400159263e-06}, {"id": 75, "seek": 62300, "start": 643.0, "end": 647.0, "text": " And that's because matrix inversion is unstable.", "tokens": [400, 300, 311, 570, 8141, 43576, 307, 23742, 13], "temperature": 0.0, "avg_logprob": -0.07714107666892567, "compression_ratio": 1.7412935323383085, "no_speech_prob": 3.041512400159263e-06}, {"id": 76, "seek": 64700, "start": 647.0, "end": 653.0, "text": " And so here we're going to look at a specific example called a Hilbert matrix.", "tokens": [400, 370, 510, 321, 434, 516, 281, 574, 412, 257, 2685, 1365, 1219, 257, 19914, 4290, 8141, 13], "temperature": 0.0, "avg_logprob": -0.06677037061646927, "compression_ratio": 1.5960591133004927, "no_speech_prob": 5.594215053861262e-06}, {"id": 77, "seek": 64700, "start": 653.0, "end": 662.0, "text": " And let me actually start by doing a smaller one just so you can see what it looks like.", "tokens": [400, 718, 385, 767, 722, 538, 884, 257, 4356, 472, 445, 370, 291, 393, 536, 437, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.06677037061646927, "compression_ratio": 1.5960591133004927, "no_speech_prob": 5.594215053861262e-06}, {"id": 78, "seek": 64700, "start": 662.0, "end": 666.0, "text": " So the Hilbert matrix is basically kind of these fractions.", "tokens": [407, 264, 19914, 4290, 8141, 307, 1936, 733, 295, 613, 36058, 13], "temperature": 0.0, "avg_logprob": -0.06677037061646927, "compression_ratio": 1.5960591133004927, "no_speech_prob": 5.594215053861262e-06}, {"id": 79, "seek": 64700, "start": 666.0, "end": 674.0, "text": " You can see you've got one, a half, a third, a fourth, a fifth is how the matrix is constructed.", "tokens": [509, 393, 536, 291, 600, 658, 472, 11, 257, 1922, 11, 257, 2636, 11, 257, 6409, 11, 257, 9266, 307, 577, 264, 8141, 307, 17083, 13], "temperature": 0.0, "avg_logprob": -0.06677037061646927, "compression_ratio": 1.5960591133004927, "no_speech_prob": 5.594215053861262e-06}, {"id": 80, "seek": 67400, "start": 674.0, "end": 683.0, "text": " And it's known to have a poor condition number.", "tokens": [400, 309, 311, 2570, 281, 362, 257, 4716, 4188, 1230, 13], "temperature": 0.0, "avg_logprob": -0.18254624382924226, "compression_ratio": 1.2377622377622377, "no_speech_prob": 6.339013452816289e-06}, {"id": 81, "seek": 67400, "start": 683.0, "end": 689.0, "text": " So here I'm just using NumPy's lenowge.inverse method.", "tokens": [407, 510, 286, 478, 445, 1228, 22592, 47, 88, 311, 40116, 305, 432, 13, 259, 4308, 3170, 13], "temperature": 0.0, "avg_logprob": -0.18254624382924226, "compression_ratio": 1.2377622377622377, "no_speech_prob": 6.339013452816289e-06}, {"id": 82, "seek": 67400, "start": 689.0, "end": 693.0, "text": " So I've created the Hilbert matrix that's 14 by 14.", "tokens": [407, 286, 600, 2942, 264, 19914, 4290, 8141, 300, 311, 3499, 538, 3499, 13], "temperature": 0.0, "avg_logprob": -0.18254624382924226, "compression_ratio": 1.2377622377622377, "no_speech_prob": 6.339013452816289e-06}, {"id": 83, "seek": 67400, "start": 693.0, "end": 697.0, "text": " Let me run that again.", "tokens": [961, 385, 1190, 300, 797, 13], "temperature": 0.0, "avg_logprob": -0.18254624382924226, "compression_ratio": 1.2377622377622377, "no_speech_prob": 6.339013452816289e-06}, {"id": 84, "seek": 69700, "start": 697.0, "end": 712.0, "text": " I invert it. And then ideally, a times a inverse minus the identity, what would you expect that to give you?", "tokens": [286, 33966, 309, 13, 400, 550, 22915, 11, 257, 1413, 257, 17340, 3175, 264, 6575, 11, 437, 576, 291, 2066, 300, 281, 976, 291, 30], "temperature": 0.0, "avg_logprob": -0.12420737225076427, "compression_ratio": 1.451219512195122, "no_speech_prob": 5.771742507931776e-06}, {"id": 85, "seek": 69700, "start": 712.0, "end": 717.0, "text": " Say it louder. Zero. Yes. So you'd expect it to give you zero.", "tokens": [6463, 309, 22717, 13, 17182, 13, 1079, 13, 407, 291, 1116, 2066, 309, 281, 976, 291, 4018, 13], "temperature": 0.0, "avg_logprob": -0.12420737225076427, "compression_ratio": 1.451219512195122, "no_speech_prob": 5.771742507931776e-06}, {"id": 86, "seek": 69700, "start": 717.0, "end": 722.0, "text": " I take the norm of that and I'm getting five, which is a bad sign.", "tokens": [286, 747, 264, 2026, 295, 300, 293, 286, 478, 1242, 1732, 11, 597, 307, 257, 1578, 1465, 13], "temperature": 0.0, "avg_logprob": -0.12420737225076427, "compression_ratio": 1.451219512195122, "no_speech_prob": 5.771742507931776e-06}, {"id": 87, "seek": 72200, "start": 722.0, "end": 729.0, "text": " And so it's C. And we can even look at a times a inverse here. And you can see this is not very close to zero.", "tokens": [400, 370, 309, 311, 383, 13, 400, 321, 393, 754, 574, 412, 257, 1413, 257, 17340, 510, 13, 400, 291, 393, 536, 341, 307, 406, 588, 1998, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.10426969074067616, "compression_ratio": 1.6053811659192825, "no_speech_prob": 7.2962184276548214e-06}, {"id": 88, "seek": 72200, "start": 729.0, "end": 735.0, "text": " Many of these decimals have numbers kind of in the tenths place.", "tokens": [5126, 295, 613, 979, 332, 1124, 362, 3547, 733, 295, 294, 264, 27269, 82, 1081, 13], "temperature": 0.0, "avg_logprob": -0.10426969074067616, "compression_ratio": 1.6053811659192825, "no_speech_prob": 7.2962184276548214e-06}, {"id": 89, "seek": 72200, "start": 735.0, "end": 740.0, "text": " This even has like a negative. Oh, wow. This even has a two.", "tokens": [639, 754, 575, 411, 257, 3671, 13, 876, 11, 6076, 13, 639, 754, 575, 257, 732, 13], "temperature": 0.0, "avg_logprob": -0.10426969074067616, "compression_ratio": 1.6053811659192825, "no_speech_prob": 7.2962184276548214e-06}, {"id": 90, "seek": 72200, "start": 740.0, "end": 744.0, "text": " Yeah. So this is quite far from zero.", "tokens": [865, 13, 407, 341, 307, 1596, 1400, 490, 4018, 13], "temperature": 0.0, "avg_logprob": -0.10426969074067616, "compression_ratio": 1.6053811659192825, "no_speech_prob": 7.2962184276548214e-06}, {"id": 91, "seek": 72200, "start": 744.0, "end": 748.0, "text": " So I think this is kind of a nice illustration. And this is not even a huge matrix.", "tokens": [407, 286, 519, 341, 307, 733, 295, 257, 1481, 22645, 13, 400, 341, 307, 406, 754, 257, 2603, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10426969074067616, "compression_ratio": 1.6053811659192825, "no_speech_prob": 7.2962184276548214e-06}, {"id": 92, "seek": 74800, "start": 748.0, "end": 753.0, "text": " So we're using 14 by 14 and we're using NumPy's inverse.", "tokens": [407, 321, 434, 1228, 3499, 538, 3499, 293, 321, 434, 1228, 22592, 47, 88, 311, 17340, 13], "temperature": 0.0, "avg_logprob": -0.22344204584757488, "compression_ratio": 1.4203821656050954, "no_speech_prob": 1.0129760994459502e-05}, {"id": 93, "seek": 74800, "start": 753.0, "end": 763.0, "text": " And we are not getting back to the identity with that.", "tokens": [400, 321, 366, 406, 1242, 646, 281, 264, 6575, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.22344204584757488, "compression_ratio": 1.4203821656050954, "no_speech_prob": 1.0129760994459502e-05}, {"id": 94, "seek": 74800, "start": 763.0, "end": 768.0, "text": " And then we can also check. So I mentioned last time there's something called the condition number of a matrix.", "tokens": [400, 550, 321, 393, 611, 1520, 13, 407, 286, 2835, 1036, 565, 456, 311, 746, 1219, 264, 4188, 1230, 295, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.22344204584757488, "compression_ratio": 1.4203821656050954, "no_speech_prob": 1.0129760994459502e-05}, {"id": 95, "seek": 76800, "start": 768.0, "end": 781.0, "text": " And actually, does anyone remember what the formula for the condition number is?", "tokens": [400, 767, 11, 775, 2878, 1604, 437, 264, 8513, 337, 264, 4188, 1230, 307, 30], "temperature": 0.0, "avg_logprob": -0.06734854276063013, "compression_ratio": 1.5592105263157894, "no_speech_prob": 4.092823928658618e-06}, {"id": 96, "seek": 76800, "start": 781.0, "end": 787.0, "text": " OK, I'll pull that up. It's the norm of a times the norm of a inverse.", "tokens": [2264, 11, 286, 603, 2235, 300, 493, 13, 467, 311, 264, 2026, 295, 257, 1413, 264, 2026, 295, 257, 17340, 13], "temperature": 0.0, "avg_logprob": -0.06734854276063013, "compression_ratio": 1.5592105263157894, "no_speech_prob": 4.092823928658618e-06}, {"id": 97, "seek": 76800, "start": 787.0, "end": 792.0, "text": " And larger condition numbers are bad. So you want the condition number to be smaller.", "tokens": [400, 4833, 4188, 3547, 366, 1578, 13, 407, 291, 528, 264, 4188, 1230, 281, 312, 4356, 13], "temperature": 0.0, "avg_logprob": -0.06734854276063013, "compression_ratio": 1.5592105263157894, "no_speech_prob": 4.092823928658618e-06}, {"id": 98, "seek": 79200, "start": 792.0, "end": 800.0, "text": " Let me find it. So that's kind of the formal definition in general.", "tokens": [961, 385, 915, 309, 13, 407, 300, 311, 733, 295, 264, 9860, 7123, 294, 2674, 13], "temperature": 0.0, "avg_logprob": -0.06148165121845815, "compression_ratio": 1.5804878048780489, "no_speech_prob": 2.6424161205795826e-06}, {"id": 99, "seek": 79200, "start": 800.0, "end": 806.0, "text": " But for a matrix, here it is. Norm of a times norm of a inverse.", "tokens": [583, 337, 257, 8141, 11, 510, 309, 307, 13, 8702, 295, 257, 1413, 2026, 295, 257, 17340, 13], "temperature": 0.0, "avg_logprob": -0.06148165121845815, "compression_ratio": 1.5804878048780489, "no_speech_prob": 2.6424161205795826e-06}, {"id": 100, "seek": 79200, "start": 806.0, "end": 812.0, "text": " And it's something that it shows up all the time in kind of different theorems around conditioning.", "tokens": [400, 309, 311, 746, 300, 309, 3110, 493, 439, 264, 565, 294, 733, 295, 819, 10299, 2592, 926, 21901, 13], "temperature": 0.0, "avg_logprob": -0.06148165121845815, "compression_ratio": 1.5804878048780489, "no_speech_prob": 2.6424161205795826e-06}, {"id": 101, "seek": 79200, "start": 812.0, "end": 819.0, "text": " And so it's useful to have it as a quantity. And Linda, can you pass the microphone to Tim?", "tokens": [400, 370, 309, 311, 4420, 281, 362, 309, 382, 257, 11275, 13, 400, 20324, 11, 393, 291, 1320, 264, 10952, 281, 7172, 30], "temperature": 0.0, "avg_logprob": -0.06148165121845815, "compression_ratio": 1.5804878048780489, "no_speech_prob": 2.6424161205795826e-06}, {"id": 102, "seek": 81900, "start": 819.0, "end": 829.0, "text": " Is that defined only for square matrices? Is that only defined for square matrices?", "tokens": [1119, 300, 7642, 787, 337, 3732, 32284, 30, 1119, 300, 787, 7642, 337, 3732, 32284, 30], "temperature": 0.0, "avg_logprob": -0.18982160264167233, "compression_ratio": 1.6411764705882352, "no_speech_prob": 3.882693636114709e-05}, {"id": 103, "seek": 81900, "start": 829.0, "end": 842.0, "text": " That's a good question. I guess you have a suit that non-square matrices kind of have an inverse.", "tokens": [663, 311, 257, 665, 1168, 13, 286, 2041, 291, 362, 257, 5722, 300, 2107, 12, 33292, 543, 32284, 733, 295, 362, 364, 17340, 13], "temperature": 0.0, "avg_logprob": -0.18982160264167233, "compression_ratio": 1.6411764705882352, "no_speech_prob": 3.882693636114709e-05}, {"id": 104, "seek": 81900, "start": 842.0, "end": 846.0, "text": " Yeah, I'll look that up and get back to you because there is something called the pseudo inverse.", "tokens": [865, 11, 286, 603, 574, 300, 493, 293, 483, 646, 281, 291, 570, 456, 307, 746, 1219, 264, 35899, 17340, 13], "temperature": 0.0, "avg_logprob": -0.18982160264167233, "compression_ratio": 1.6411764705882352, "no_speech_prob": 3.882693636114709e-05}, {"id": 105, "seek": 84600, "start": 846.0, "end": 852.0, "text": " But yeah, I don't know if that's... Yeah, I think you're typically talking about square matrices with it.", "tokens": [583, 1338, 11, 286, 500, 380, 458, 498, 300, 311, 485, 865, 11, 286, 519, 291, 434, 5850, 1417, 466, 3732, 32284, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.09725075733812549, "compression_ratio": 1.4728260869565217, "no_speech_prob": 1.8924343748949468e-05}, {"id": 106, "seek": 84600, "start": 852.0, "end": 857.0, "text": " But yeah, that's a good question.", "tokens": [583, 1338, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.09725075733812549, "compression_ratio": 1.4728260869565217, "no_speech_prob": 1.8924343748949468e-05}, {"id": 107, "seek": 84600, "start": 857.0, "end": 864.0, "text": " Yeah, and so if we look at the condition number of a, it's 10 to the 17th magnitude,", "tokens": [865, 11, 293, 370, 498, 321, 574, 412, 264, 4188, 1230, 295, 257, 11, 309, 311, 1266, 281, 264, 3282, 392, 15668, 11], "temperature": 0.0, "avg_logprob": -0.09725075733812549, "compression_ratio": 1.4728260869565217, "no_speech_prob": 1.8924343748949468e-05}, {"id": 108, "seek": 84600, "start": 864.0, "end": 875.0, "text": " which is a really bad sign that it's so large.", "tokens": [597, 307, 257, 534, 1578, 1465, 300, 309, 311, 370, 2416, 13], "temperature": 0.0, "avg_logprob": -0.09725075733812549, "compression_ratio": 1.4728260869565217, "no_speech_prob": 1.8924343748949468e-05}, {"id": 109, "seek": 87500, "start": 875.0, "end": 884.0, "text": " And so that's... I would say that's the primary reason that you don't want to calculate a matrix inverse.", "tokens": [400, 370, 300, 311, 485, 286, 576, 584, 300, 311, 264, 6194, 1778, 300, 291, 500, 380, 528, 281, 8873, 257, 8141, 17340, 13], "temperature": 0.0, "avg_logprob": -0.09865092277526856, "compression_ratio": 1.7117903930131004, "no_speech_prob": 7.183094567153603e-06}, {"id": 110, "seek": 87500, "start": 884.0, "end": 890.0, "text": " And there are a few other issues, I thought I wrote them down, that can come up.", "tokens": [400, 456, 366, 257, 1326, 661, 2663, 11, 286, 1194, 286, 4114, 552, 760, 11, 300, 393, 808, 493, 13], "temperature": 0.0, "avg_logprob": -0.09865092277526856, "compression_ratio": 1.7117903930131004, "no_speech_prob": 7.183094567153603e-06}, {"id": 111, "seek": 87500, "start": 890.0, "end": 897.0, "text": " One is if you have a sparse matrix in particular, when you calculate the inverse, it stops being sparse.", "tokens": [1485, 307, 498, 291, 362, 257, 637, 11668, 8141, 294, 1729, 11, 562, 291, 8873, 264, 17340, 11, 309, 10094, 885, 637, 11668, 13], "temperature": 0.0, "avg_logprob": -0.09865092277526856, "compression_ratio": 1.7117903930131004, "no_speech_prob": 7.183094567153603e-06}, {"id": 112, "seek": 87500, "start": 897.0, "end": 902.0, "text": " And so memory-wise, that could be really bad, particularly if you had a matrix that was large enough", "tokens": [400, 370, 4675, 12, 3711, 11, 300, 727, 312, 534, 1578, 11, 4098, 498, 291, 632, 257, 8141, 300, 390, 2416, 1547], "temperature": 0.0, "avg_logprob": -0.09865092277526856, "compression_ratio": 1.7117903930131004, "no_speech_prob": 7.183094567153603e-06}, {"id": 113, "seek": 90200, "start": 902.0, "end": 906.0, "text": " that you can't store the non-sparse version.", "tokens": [300, 291, 393, 380, 3531, 264, 2107, 12, 4952, 11668, 3037, 13], "temperature": 0.0, "avg_logprob": -0.08112911383310954, "compression_ratio": 1.3758389261744965, "no_speech_prob": 2.4824507818266284e-06}, {"id": 114, "seek": 90200, "start": 906.0, "end": 911.0, "text": " Calculating it's inverse is kind of, yeah, making this really huge matrix.", "tokens": [3511, 2444, 990, 309, 311, 17340, 307, 733, 295, 11, 1338, 11, 1455, 341, 534, 2603, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08112911383310954, "compression_ratio": 1.3758389261744965, "no_speech_prob": 2.4824507818266284e-06}, {"id": 115, "seek": 90200, "start": 911.0, "end": 920.0, "text": " But I think the primary thing is the instability.", "tokens": [583, 286, 519, 264, 6194, 551, 307, 264, 34379, 13], "temperature": 0.0, "avg_logprob": -0.08112911383310954, "compression_ratio": 1.3758389261744965, "no_speech_prob": 2.4824507818266284e-06}, {"id": 116, "seek": 90200, "start": 920.0, "end": 927.0, "text": " So this is a little bit of a cheat.", "tokens": [407, 341, 307, 257, 707, 857, 295, 257, 17470, 13], "temperature": 0.0, "avg_logprob": -0.08112911383310954, "compression_ratio": 1.3758389261744965, "no_speech_prob": 2.4824507818266284e-06}, {"id": 117, "seek": 92700, "start": 927.0, "end": 933.0, "text": " So I'm still on this Hilbert matrix, and so I run all of the options.", "tokens": [407, 286, 478, 920, 322, 341, 19914, 4290, 8141, 11, 293, 370, 286, 1190, 439, 295, 264, 3956, 13], "temperature": 0.0, "avg_logprob": -0.08331305806229754, "compression_ratio": 1.5027027027027027, "no_speech_prob": 2.5464847567491233e-05}, {"id": 118, "seek": 92700, "start": 933.0, "end": 936.0, "text": " Well, you'll notice I run all the options except Cholesky.", "tokens": [1042, 11, 291, 603, 3449, 286, 1190, 439, 264, 3956, 3993, 761, 7456, 4133, 13], "temperature": 0.0, "avg_logprob": -0.08331305806229754, "compression_ratio": 1.5027027027027027, "no_speech_prob": 2.5464847567491233e-05}, {"id": 119, "seek": 92700, "start": 936.0, "end": 939.0, "text": " And so we didn't focus on this last time.", "tokens": [400, 370, 321, 994, 380, 1879, 322, 341, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.08331305806229754, "compression_ratio": 1.5027027027027027, "no_speech_prob": 2.5464847567491233e-05}, {"id": 120, "seek": 92700, "start": 939.0, "end": 953.0, "text": " So this is a difficult question, but does anyone have a guess of why, maybe why we can't use Cholesky here?", "tokens": [407, 341, 307, 257, 2252, 1168, 11, 457, 775, 2878, 362, 257, 2041, 295, 983, 11, 1310, 983, 321, 393, 380, 764, 761, 7456, 4133, 510, 30], "temperature": 0.0, "avg_logprob": -0.08331305806229754, "compression_ratio": 1.5027027027027027, "no_speech_prob": 2.5464847567491233e-05}, {"id": 121, "seek": 95300, "start": 953.0, "end": 959.0, "text": " Okay, I just, oh, Matthew, can you pass the microphone?", "tokens": [1033, 11, 286, 445, 11, 1954, 11, 12434, 11, 393, 291, 1320, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.23198712177765676, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.53540490323212e-05}, {"id": 122, "seek": 95300, "start": 959.0, "end": 962.0, "text": " Because it's unstable like you said.", "tokens": [1436, 309, 311, 23742, 411, 291, 848, 13], "temperature": 0.0, "avg_logprob": -0.23198712177765676, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.53540490323212e-05}, {"id": 123, "seek": 95300, "start": 962.0, "end": 965.0, "text": " No, so that's matrix inversion is unstable.", "tokens": [883, 11, 370, 300, 311, 8141, 43576, 307, 23742, 13], "temperature": 0.0, "avg_logprob": -0.23198712177765676, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.53540490323212e-05}, {"id": 124, "seek": 95300, "start": 965.0, "end": 968.0, "text": " It doesn't require the matrix inversion to.", "tokens": [467, 1177, 380, 3651, 264, 8141, 43576, 281, 13], "temperature": 0.0, "avg_logprob": -0.23198712177765676, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.53540490323212e-05}, {"id": 125, "seek": 95300, "start": 968.0, "end": 970.0, "text": " No.", "tokens": [883, 13], "temperature": 0.0, "avg_logprob": -0.23198712177765676, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.53540490323212e-05}, {"id": 126, "seek": 95300, "start": 970.0, "end": 977.0, "text": " So yeah, Cholesky entails coming up with the, you're coming up with a triangular,", "tokens": [407, 1338, 11, 761, 7456, 4133, 50133, 1348, 493, 365, 264, 11, 291, 434, 1348, 493, 365, 257, 38190, 11], "temperature": 0.0, "avg_logprob": -0.23198712177765676, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.53540490323212e-05}, {"id": 127, "seek": 97700, "start": 977.0, "end": 988.0, "text": " or yeah, triangular matrix, which times its transpose equals A transpose A.", "tokens": [420, 1338, 11, 38190, 8141, 11, 597, 1413, 1080, 25167, 6915, 316, 25167, 316, 13], "temperature": 0.0, "avg_logprob": -0.08819790355494765, "compression_ratio": 1.5481927710843373, "no_speech_prob": 4.49509661848424e-06}, {"id": 128, "seek": 97700, "start": 988.0, "end": 990.0, "text": " Yeah, this I just briefly mentioned.", "tokens": [865, 11, 341, 286, 445, 10515, 2835, 13], "temperature": 0.0, "avg_logprob": -0.08819790355494765, "compression_ratio": 1.5481927710843373, "no_speech_prob": 4.49509661848424e-06}, {"id": 129, "seek": 97700, "start": 990.0, "end": 1000.0, "text": " The Cholesky factorization is only guaranteed to exist if the matrix is positive definite.", "tokens": [440, 761, 7456, 4133, 5952, 2144, 307, 787, 18031, 281, 2514, 498, 264, 8141, 307, 3353, 25131, 13], "temperature": 0.0, "avg_logprob": -0.08819790355494765, "compression_ratio": 1.5481927710843373, "no_speech_prob": 4.49509661848424e-06}, {"id": 130, "seek": 97700, "start": 1000.0, "end": 1003.0, "text": " And so this matrix actually is not positive definite.", "tokens": [400, 370, 341, 8141, 767, 307, 406, 3353, 25131, 13], "temperature": 0.0, "avg_logprob": -0.08819790355494765, "compression_ratio": 1.5481927710843373, "no_speech_prob": 4.49509661848424e-06}, {"id": 131, "seek": 100300, "start": 1003.0, "end": 1009.0, "text": " So that's a down, significant downside to Cholesky is that even though it's really fast, you can't use it in all cases.", "tokens": [407, 300, 311, 257, 760, 11, 4776, 25060, 281, 761, 7456, 4133, 307, 300, 754, 1673, 309, 311, 534, 2370, 11, 291, 393, 380, 764, 309, 294, 439, 3331, 13], "temperature": 0.0, "avg_logprob": -0.07295274204678005, "compression_ratio": 1.5778894472361809, "no_speech_prob": 6.854193088656757e-06}, {"id": 132, "seek": 100300, "start": 1009.0, "end": 1016.0, "text": " And so we can't use it here.", "tokens": [400, 370, 321, 393, 380, 764, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.07295274204678005, "compression_ratio": 1.5778894472361809, "no_speech_prob": 6.854193088656757e-06}, {"id": 133, "seek": 100300, "start": 1016.0, "end": 1018.0, "text": " So then, yeah, so I had to take it out.", "tokens": [407, 550, 11, 1338, 11, 370, 286, 632, 281, 747, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.07295274204678005, "compression_ratio": 1.5778894472361809, "no_speech_prob": 6.854193088656757e-06}, {"id": 134, "seek": 100300, "start": 1018.0, "end": 1028.0, "text": " I mean, you can try running it and you get an error basically that there's a negative singular value if you attempt to do it.", "tokens": [286, 914, 11, 291, 393, 853, 2614, 309, 293, 291, 483, 364, 6713, 1936, 300, 456, 311, 257, 3671, 20010, 2158, 498, 291, 5217, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.07295274204678005, "compression_ratio": 1.5778894472361809, "no_speech_prob": 6.854193088656757e-06}, {"id": 135, "seek": 102800, "start": 1028.0, "end": 1038.0, "text": " So here the we've got this huge error for I don't know why I have this twice.", "tokens": [407, 510, 264, 321, 600, 658, 341, 2603, 6713, 337, 286, 500, 380, 458, 983, 286, 362, 341, 6091, 13], "temperature": 0.0, "avg_logprob": -0.1396635312300462, "compression_ratio": 1.421875, "no_speech_prob": 7.296192507055821e-06}, {"id": 136, "seek": 102800, "start": 1038.0, "end": 1042.0, "text": " Delete one of these.", "tokens": [49452, 472, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.1396635312300462, "compression_ratio": 1.421875, "no_speech_prob": 7.296192507055821e-06}, {"id": 137, "seek": 102800, "start": 1042.0, "end": 1049.0, "text": " Yeah, we've got this big error for the taking the inverse, which is not surprising.", "tokens": [865, 11, 321, 600, 658, 341, 955, 6713, 337, 264, 1940, 264, 17340, 11, 597, 307, 406, 8830, 13], "temperature": 0.0, "avg_logprob": -0.1396635312300462, "compression_ratio": 1.421875, "no_speech_prob": 7.296192507055821e-06}, {"id": 138, "seek": 104900, "start": 1049.0, "end": 1059.0, "text": " And speed wise, so QR factorization, SVD and SciPy's implementation are all fairly similar.", "tokens": [400, 3073, 10829, 11, 370, 32784, 5952, 2144, 11, 31910, 35, 293, 16942, 47, 88, 311, 11420, 366, 439, 6457, 2531, 13], "temperature": 0.0, "avg_logprob": -0.09830148547303443, "compression_ratio": 1.5595238095238095, "no_speech_prob": 4.2227811718476005e-06}, {"id": 139, "seek": 104900, "start": 1059.0, "end": 1065.0, "text": " But SVD was actually fastest, although I have to confess I ran this a few times and I don't think that was consistently the case,", "tokens": [583, 31910, 35, 390, 767, 14573, 11, 4878, 286, 362, 281, 19367, 286, 5872, 341, 257, 1326, 1413, 293, 286, 500, 380, 519, 300, 390, 14961, 264, 1389, 11], "temperature": 0.0, "avg_logprob": -0.09830148547303443, "compression_ratio": 1.5595238095238095, "no_speech_prob": 4.2227811718476005e-06}, {"id": 140, "seek": 104900, "start": 1065.0, "end": 1070.0, "text": " but I wanted to save it because I was having trouble finding a case where SVD was the best.", "tokens": [457, 286, 1415, 281, 3155, 309, 570, 286, 390, 1419, 5253, 5006, 257, 1389, 689, 31910, 35, 390, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.09830148547303443, "compression_ratio": 1.5595238095238095, "no_speech_prob": 4.2227811718476005e-06}, {"id": 141, "seek": 104900, "start": 1070.0, "end": 1077.0, "text": " But here really, I mean, they're so close that there's not like a clear winner.", "tokens": [583, 510, 534, 11, 286, 914, 11, 436, 434, 370, 1998, 300, 456, 311, 406, 411, 257, 1850, 8507, 13], "temperature": 0.0, "avg_logprob": -0.09830148547303443, "compression_ratio": 1.5595238095238095, "no_speech_prob": 4.2227811718476005e-06}, {"id": 142, "seek": 107700, "start": 1077.0, "end": 1086.0, "text": " But yeah, this illustrates kind of you definitely don't want to be taking the inverse and you can't take Teleski.", "tokens": [583, 1338, 11, 341, 41718, 733, 295, 291, 2138, 500, 380, 528, 281, 312, 1940, 264, 17340, 293, 291, 393, 380, 747, 314, 6972, 2984, 13], "temperature": 0.0, "avg_logprob": -0.17291207646214685, "compression_ratio": 1.2698412698412698, "no_speech_prob": 2.1781313989777118e-05}, {"id": 143, "seek": 107700, "start": 1086.0, "end": 1094.0, "text": " Oh, and can you throw the microphone, Matthew?", "tokens": [876, 11, 293, 393, 291, 3507, 264, 10952, 11, 12434, 30], "temperature": 0.0, "avg_logprob": -0.17291207646214685, "compression_ratio": 1.2698412698412698, "no_speech_prob": 2.1781313989777118e-05}, {"id": 144, "seek": 109400, "start": 1094.0, "end": 1108.0, "text": " So any reason that after inversion that the matrix is going to just get more dense?", "tokens": [407, 604, 1778, 300, 934, 43576, 300, 264, 8141, 307, 516, 281, 445, 483, 544, 18011, 30], "temperature": 0.0, "avg_logprob": -0.10529553303953076, "compression_ratio": 1.5542857142857143, "no_speech_prob": 2.2123476810520515e-05}, {"id": 145, "seek": 109400, "start": 1108.0, "end": 1114.0, "text": " I guess it's like there's nothing that guarantees that you would have zeros in other places.", "tokens": [286, 2041, 309, 311, 411, 456, 311, 1825, 300, 32567, 300, 291, 576, 362, 35193, 294, 661, 3190, 13], "temperature": 0.0, "avg_logprob": -0.10529553303953076, "compression_ratio": 1.5542857142857143, "no_speech_prob": 2.2123476810520515e-05}, {"id": 146, "seek": 109400, "start": 1114.0, "end": 1122.0, "text": " You kind of think like the process for taking inversion is it's like doing Gaussian elimination", "tokens": [509, 733, 295, 519, 411, 264, 1399, 337, 1940, 43576, 307, 309, 311, 411, 884, 39148, 29224], "temperature": 0.0, "avg_logprob": -0.10529553303953076, "compression_ratio": 1.5542857142857143, "no_speech_prob": 2.2123476810520515e-05}, {"id": 147, "seek": 112200, "start": 1122.0, "end": 1130.0, "text": " only instead of having like a single column, you have like the identity matrix is one way to calculate the inverse.", "tokens": [787, 2602, 295, 1419, 411, 257, 2167, 7738, 11, 291, 362, 411, 264, 6575, 8141, 307, 472, 636, 281, 8873, 264, 17340, 13], "temperature": 0.0, "avg_logprob": -0.06281299676213946, "compression_ratio": 1.5515151515151515, "no_speech_prob": 4.710828761744779e-06}, {"id": 148, "seek": 112200, "start": 1130.0, "end": 1143.0, "text": " And so there you're kind of basically like adding and dividing things to every location as you go through and zero out your original matrix.", "tokens": [400, 370, 456, 291, 434, 733, 295, 1936, 411, 5127, 293, 26764, 721, 281, 633, 4914, 382, 291, 352, 807, 293, 4018, 484, 428, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.06281299676213946, "compression_ratio": 1.5515151515151515, "no_speech_prob": 4.710828761744779e-06}, {"id": 149, "seek": 114300, "start": 1143.0, "end": 1157.0, "text": " But yes, you're kind of like putting values in where you had zeros when you do that Gaussian elimination.", "tokens": [583, 2086, 11, 291, 434, 733, 295, 411, 3372, 4190, 294, 689, 291, 632, 35193, 562, 291, 360, 300, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.12520220279693603, "compression_ratio": 1.25, "no_speech_prob": 1.733027374939411e-06}, {"id": 150, "seek": 114300, "start": 1157.0, "end": 1160.0, "text": " Yes, I wanted to talk a little bit about runtime.", "tokens": [1079, 11, 286, 1415, 281, 751, 257, 707, 857, 466, 34474, 13], "temperature": 0.0, "avg_logprob": -0.12520220279693603, "compression_ratio": 1.25, "no_speech_prob": 1.733027374939411e-06}, {"id": 151, "seek": 116000, "start": 1160.0, "end": 1173.0, "text": " And this is not exactly big O because I've included constants here and, you know, with big O notation, you are or I mean,", "tokens": [400, 341, 307, 406, 2293, 955, 422, 570, 286, 600, 5556, 35870, 510, 293, 11, 291, 458, 11, 365, 955, 422, 24657, 11, 291, 366, 420, 286, 914, 11], "temperature": 0.0, "avg_logprob": -0.13280082043306327, "compression_ratio": 1.5459183673469388, "no_speech_prob": 9.08008951228112e-06}, {"id": 152, "seek": 116000, "start": 1173.0, "end": 1175.0, "text": " I don't even know if it's OK for me to call this runtime.", "tokens": [286, 500, 380, 754, 458, 498, 309, 311, 2264, 337, 385, 281, 818, 341, 34474, 13], "temperature": 0.0, "avg_logprob": -0.13280082043306327, "compression_ratio": 1.5459183673469388, "no_speech_prob": 9.08008951228112e-06}, {"id": 153, "seek": 116000, "start": 1175.0, "end": 1180.0, "text": " But this is kind of normally you drop the constants.", "tokens": [583, 341, 307, 733, 295, 5646, 291, 3270, 264, 35870, 13], "temperature": 0.0, "avg_logprob": -0.13280082043306327, "compression_ratio": 1.5459183673469388, "no_speech_prob": 9.08008951228112e-06}, {"id": 154, "seek": 116000, "start": 1180.0, "end": 1184.0, "text": " But I just wanted to show them because here most things are and cubed.", "tokens": [583, 286, 445, 1415, 281, 855, 552, 570, 510, 881, 721, 366, 293, 36510, 13], "temperature": 0.0, "avg_logprob": -0.13280082043306327, "compression_ratio": 1.5459183673469388, "no_speech_prob": 9.08008951228112e-06}, {"id": 155, "seek": 118400, "start": 1184.0, "end": 1190.0, "text": " And so it kind of matters what the constant is.", "tokens": [400, 370, 309, 733, 295, 7001, 437, 264, 5754, 307, 13], "temperature": 0.0, "avg_logprob": -0.07942970463486969, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.628022687327757e-06}, {"id": 156, "seek": 118400, "start": 1190.0, "end": 1195.0, "text": " So Cholesky is one third and cubed.", "tokens": [407, 761, 7456, 4133, 307, 472, 2636, 293, 36510, 13], "temperature": 0.0, "avg_logprob": -0.07942970463486969, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.628022687327757e-06}, {"id": 157, "seek": 118400, "start": 1195.0, "end": 1199.0, "text": " And so that's a reason that it's kind of faster than these others.", "tokens": [400, 370, 300, 311, 257, 1778, 300, 309, 311, 733, 295, 4663, 813, 613, 2357, 13], "temperature": 0.0, "avg_logprob": -0.07942970463486969, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.628022687327757e-06}, {"id": 158, "seek": 118400, "start": 1199.0, "end": 1209.0, "text": " So QR is kind of a formula involving powers of third order terms.", "tokens": [407, 32784, 307, 733, 295, 257, 8513, 17030, 8674, 295, 2636, 1668, 2115, 13], "temperature": 0.0, "avg_logprob": -0.07942970463486969, "compression_ratio": 1.469387755102041, "no_speech_prob": 1.628022687327757e-06}, {"id": 159, "seek": 120900, "start": 1209.0, "end": 1219.0, "text": " And then something else to highlight is that matrix multiplication is N cubed and solving a triangular system is N squared.", "tokens": [400, 550, 746, 1646, 281, 5078, 307, 300, 8141, 27290, 307, 426, 36510, 293, 12606, 257, 38190, 1185, 307, 426, 8889, 13], "temperature": 0.0, "avg_logprob": -0.10238530238469441, "compression_ratio": 1.3571428571428572, "no_speech_prob": 1.6280132513202261e-06}, {"id": 160, "seek": 120900, "start": 1219.0, "end": 1225.0, "text": " And so if you'll remember, this kind of goes through with the Cholesky factorization.", "tokens": [400, 370, 498, 291, 603, 1604, 11, 341, 733, 295, 1709, 807, 365, 264, 761, 7456, 4133, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.10238530238469441, "compression_ratio": 1.3571428571428572, "no_speech_prob": 1.6280132513202261e-06}, {"id": 161, "seek": 122500, "start": 1225.0, "end": 1251.0, "text": " What you end up with is actually maybe I'll go back up to how I had it written.", "tokens": [708, 291, 917, 493, 365, 307, 767, 1310, 286, 603, 352, 646, 493, 281, 577, 286, 632, 309, 3720, 13], "temperature": 0.0, "avg_logprob": -0.08081691960493724, "compression_ratio": 1.0128205128205128, "no_speech_prob": 1.593496563145891e-05}, {"id": 162, "seek": 125100, "start": 1251.0, "end": 1255.0, "text": " OK, here's the Cholesky factorization.", "tokens": [2264, 11, 510, 311, 264, 761, 7456, 4133, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.08032035827636719, "compression_ratio": 1.685, "no_speech_prob": 5.594173671852332e-06}, {"id": 163, "seek": 125100, "start": 1255.0, "end": 1261.0, "text": " So here our process was we take A transpose A, X equals A transpose B.", "tokens": [407, 510, 527, 1399, 390, 321, 747, 316, 25167, 316, 11, 1783, 6915, 316, 25167, 363, 13], "temperature": 0.0, "avg_logprob": -0.08032035827636719, "compression_ratio": 1.685, "no_speech_prob": 5.594173671852332e-06}, {"id": 164, "seek": 125100, "start": 1261.0, "end": 1265.0, "text": " So we are having to do a matrix multiplication to get this A transpose A.", "tokens": [407, 321, 366, 1419, 281, 360, 257, 8141, 27290, 281, 483, 341, 316, 25167, 316, 13], "temperature": 0.0, "avg_logprob": -0.08032035827636719, "compression_ratio": 1.685, "no_speech_prob": 5.594173671852332e-06}, {"id": 165, "seek": 125100, "start": 1265.0, "end": 1272.0, "text": " Then we do the Cholesky factorization and get R transpose R where R is upper triangular.", "tokens": [1396, 321, 360, 264, 761, 7456, 4133, 5952, 2144, 293, 483, 497, 25167, 497, 689, 497, 307, 6597, 38190, 13], "temperature": 0.0, "avg_logprob": -0.08032035827636719, "compression_ratio": 1.685, "no_speech_prob": 5.594173671852332e-06}, {"id": 166, "seek": 125100, "start": 1272.0, "end": 1277.0, "text": " And then we're solving R transpose W equals the right hand side.", "tokens": [400, 550, 321, 434, 12606, 497, 25167, 343, 6915, 264, 558, 1011, 1252, 13], "temperature": 0.0, "avg_logprob": -0.08032035827636719, "compression_ratio": 1.685, "no_speech_prob": 5.594173671852332e-06}, {"id": 167, "seek": 127700, "start": 1277.0, "end": 1282.0, "text": " And the reason that's nice is that's a triangular system because R is triangular.", "tokens": [400, 264, 1778, 300, 311, 1481, 307, 300, 311, 257, 38190, 1185, 570, 497, 307, 38190, 13], "temperature": 0.0, "avg_logprob": -0.10618502753121513, "compression_ratio": 1.4285714285714286, "no_speech_prob": 4.565860308503034e-06}, {"id": 168, "seek": 127700, "start": 1282.0, "end": 1288.0, "text": " And how can you solve a triangular system?", "tokens": [400, 577, 393, 291, 5039, 257, 38190, 1185, 30], "temperature": 0.0, "avg_logprob": -0.10618502753121513, "compression_ratio": 1.4285714285714286, "no_speech_prob": 4.565860308503034e-06}, {"id": 169, "seek": 127700, "start": 1288.0, "end": 1297.0, "text": " Anyone I haven't heard from today?", "tokens": [14643, 286, 2378, 380, 2198, 490, 965, 30], "temperature": 0.0, "avg_logprob": -0.10618502753121513, "compression_ratio": 1.4285714285714286, "no_speech_prob": 4.565860308503034e-06}, {"id": 170, "seek": 127700, "start": 1297.0, "end": 1303.0, "text": " OK, yeah. Can you throw the microphone to Vincent?", "tokens": [2264, 11, 1338, 13, 1664, 291, 3507, 264, 10952, 281, 28003, 30], "temperature": 0.0, "avg_logprob": -0.10618502753121513, "compression_ratio": 1.4285714285714286, "no_speech_prob": 4.565860308503034e-06}, {"id": 171, "seek": 130300, "start": 1303.0, "end": 1309.0, "text": " Yes, back substitution. Exactly. So bottom row, you only have a single entry.", "tokens": [1079, 11, 646, 35827, 13, 7587, 13, 407, 2767, 5386, 11, 291, 787, 362, 257, 2167, 8729, 13], "temperature": 0.0, "avg_logprob": -0.1325665168391848, "compression_ratio": 1.582995951417004, "no_speech_prob": 9.168876567855477e-05}, {"id": 172, "seek": 130300, "start": 1309.0, "end": 1313.0, "text": " Actually, this is our transpose. It really would be the top row in this case.", "tokens": [5135, 11, 341, 307, 527, 25167, 13, 467, 534, 576, 312, 264, 1192, 5386, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.1325665168391848, "compression_ratio": 1.582995951417004, "no_speech_prob": 9.168876567855477e-05}, {"id": 173, "seek": 130300, "start": 1313.0, "end": 1322.0, "text": " So you can just divide through and get that value, plug that into the next equation, which only has two variables, plug it in, solve for the other one, and so on.", "tokens": [407, 291, 393, 445, 9845, 807, 293, 483, 300, 2158, 11, 5452, 300, 666, 264, 958, 5367, 11, 597, 787, 575, 732, 9102, 11, 5452, 309, 294, 11, 5039, 337, 264, 661, 472, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1325665168391848, "compression_ratio": 1.582995951417004, "no_speech_prob": 9.168876567855477e-05}, {"id": 174, "seek": 130300, "start": 1322.0, "end": 1326.0, "text": " And so that's N squared. So that's a quicker, quicker way of doing this.", "tokens": [400, 370, 300, 311, 426, 8889, 13, 407, 300, 311, 257, 16255, 11, 16255, 636, 295, 884, 341, 13], "temperature": 0.0, "avg_logprob": -0.1325665168391848, "compression_ratio": 1.582995951417004, "no_speech_prob": 9.168876567855477e-05}, {"id": 175, "seek": 132600, "start": 1326.0, "end": 1338.0, "text": " Whereas with the kind of naive approach, even when you get, let me find it.", "tokens": [13813, 365, 264, 733, 295, 29052, 3109, 11, 754, 562, 291, 483, 11, 718, 385, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.0775741416138488, "compression_ratio": 1.6358974358974359, "no_speech_prob": 1.3925338180342806e-06}, {"id": 176, "seek": 132600, "start": 1338.0, "end": 1342.0, "text": " Even once you get this inverse, you still have to do a matrix multiplication.", "tokens": [2754, 1564, 291, 483, 341, 17340, 11, 291, 920, 362, 281, 360, 257, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.0775741416138488, "compression_ratio": 1.6358974358974359, "no_speech_prob": 1.3925338180342806e-06}, {"id": 177, "seek": 132600, "start": 1342.0, "end": 1346.0, "text": " So you're doing A transpose A, calculating the inverse, which takes N cubed.", "tokens": [407, 291, 434, 884, 316, 25167, 316, 11, 28258, 264, 17340, 11, 597, 2516, 426, 36510, 13], "temperature": 0.0, "avg_logprob": -0.0775741416138488, "compression_ratio": 1.6358974358974359, "no_speech_prob": 1.3925338180342806e-06}, {"id": 178, "seek": 132600, "start": 1346.0, "end": 1351.0, "text": " And then you're multiplying that by A transpose, which is another matrix multiplication.", "tokens": [400, 550, 291, 434, 30955, 300, 538, 316, 25167, 11, 597, 307, 1071, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.0775741416138488, "compression_ratio": 1.6358974358974359, "no_speech_prob": 1.3925338180342806e-06}, {"id": 179, "seek": 135100, "start": 1351.0, "end": 1363.0, "text": " So that's slow. So really, kind of what I wanted to highlight here is that matrix multiplication is N cubed and solving a triangular system is N squared.", "tokens": [407, 300, 311, 2964, 13, 407, 534, 11, 733, 295, 437, 286, 1415, 281, 5078, 510, 307, 300, 8141, 27290, 307, 426, 36510, 293, 12606, 257, 38190, 1185, 307, 426, 8889, 13], "temperature": 0.0, "avg_logprob": -0.08812637951063074, "compression_ratio": 1.4566929133858268, "no_speech_prob": 1.184281336463755e-05}, {"id": 180, "seek": 135100, "start": 1363.0, "end": 1378.0, "text": " Triangular system is N squared.", "tokens": [10931, 656, 1040, 1185, 307, 426, 8889, 13], "temperature": 0.0, "avg_logprob": -0.08812637951063074, "compression_ratio": 1.4566929133858268, "no_speech_prob": 1.184281336463755e-05}, {"id": 181, "seek": 137800, "start": 1378.0, "end": 1387.0, "text": " So yeah, and I found these from a slide. This was actually from the convex optimization course I linked to a few notebooks ago.", "tokens": [407, 1338, 11, 293, 286, 1352, 613, 490, 257, 4137, 13, 639, 390, 767, 490, 264, 42432, 19618, 1164, 286, 9408, 281, 257, 1326, 43782, 2057, 13], "temperature": 0.0, "avg_logprob": -0.1469651255114325, "compression_ratio": 1.377906976744186, "no_speech_prob": 2.1567661860899534e-06}, {"id": 182, "seek": 137800, "start": 1387.0, "end": 1393.0, "text": " They had like a numerical linear algebra background lesson.", "tokens": [814, 632, 411, 257, 29054, 8213, 21989, 3678, 6898, 13], "temperature": 0.0, "avg_logprob": -0.1469651255114325, "compression_ratio": 1.377906976744186, "no_speech_prob": 2.1567661860899534e-06}, {"id": 183, "seek": 137800, "start": 1393.0, "end": 1401.0, "text": " And so this is just showing why Cholesky is fast.", "tokens": [400, 370, 341, 307, 445, 4099, 983, 761, 7456, 4133, 307, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1469651255114325, "compression_ratio": 1.377906976744186, "no_speech_prob": 2.1567661860899534e-06}, {"id": 184, "seek": 140100, "start": 1401.0, "end": 1410.0, "text": " So here I'm looking at another very specific type of matrix called the Vandermonde matrix.", "tokens": [407, 510, 286, 478, 1237, 412, 1071, 588, 2685, 2010, 295, 8141, 1219, 264, 691, 474, 966, 7259, 8141, 13], "temperature": 0.0, "avg_logprob": -0.09657398462295533, "compression_ratio": 1.5174129353233832, "no_speech_prob": 3.6687627016362967e-06}, {"id": 185, "seek": 140100, "start": 1410.0, "end": 1416.0, "text": " And this is coming from an example in Trevithin.", "tokens": [400, 341, 307, 1348, 490, 364, 1365, 294, 8648, 85, 355, 259, 13], "temperature": 0.0, "avg_logprob": -0.09657398462295533, "compression_ratio": 1.5174129353233832, "no_speech_prob": 3.6687627016362967e-06}, {"id": 186, "seek": 140100, "start": 1416.0, "end": 1429.0, "text": " And so Trevithin tells us kind of if we normalize by this, he's calculated the kind of true solution on a very high precision computer and has that it should be one.", "tokens": [400, 370, 8648, 85, 355, 259, 5112, 505, 733, 295, 498, 321, 2710, 1125, 538, 341, 11, 415, 311, 15598, 264, 733, 295, 2074, 3827, 322, 257, 588, 1090, 18356, 3820, 293, 575, 300, 309, 820, 312, 472, 13], "temperature": 0.0, "avg_logprob": -0.09657398462295533, "compression_ratio": 1.5174129353233832, "no_speech_prob": 3.6687627016362967e-06}, {"id": 187, "seek": 142900, "start": 1429.0, "end": 1432.0, "text": " So here, so we're creating this matrix by taking powers of T.", "tokens": [407, 510, 11, 370, 321, 434, 4084, 341, 8141, 538, 1940, 8674, 295, 314, 13], "temperature": 0.0, "avg_logprob": -0.11864175467655577, "compression_ratio": 1.476510067114094, "no_speech_prob": 1.2410844647092745e-05}, {"id": 188, "seek": 142900, "start": 1432.0, "end": 1442.0, "text": " So we have kind of evenly spaced points between zero and one. And then we're taking different powers of them.", "tokens": [407, 321, 362, 733, 295, 17658, 43766, 2793, 1296, 4018, 293, 472, 13, 400, 550, 321, 434, 1940, 819, 8674, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.11864175467655577, "compression_ratio": 1.476510067114094, "no_speech_prob": 1.2410844647092745e-05}, {"id": 189, "seek": 142900, "start": 1442.0, "end": 1453.0, "text": " And then doing E to the sine of four times that.", "tokens": [400, 550, 884, 462, 281, 264, 18609, 295, 1451, 1413, 300, 13], "temperature": 0.0, "avg_logprob": -0.11864175467655577, "compression_ratio": 1.476510067114094, "no_speech_prob": 1.2410844647092745e-05}, {"id": 190, "seek": 145300, "start": 1453.0, "end": 1462.0, "text": " And so we can check that using QR, we're getting 10 to the negative seventh, which isn't perfect, but is reasonable.", "tokens": [400, 370, 321, 393, 1520, 300, 1228, 32784, 11, 321, 434, 1242, 1266, 281, 264, 3671, 17875, 11, 597, 1943, 380, 2176, 11, 457, 307, 10585, 13], "temperature": 0.0, "avg_logprob": -0.05542621733267096, "compression_ratio": 1.4656862745098038, "no_speech_prob": 2.482400759618031e-06}, {"id": 191, "seek": 145300, "start": 1462.0, "end": 1470.0, "text": " This is another condition number of to the power of 17, which is bad news.", "tokens": [639, 307, 1071, 4188, 1230, 295, 281, 264, 1347, 295, 3282, 11, 597, 307, 1578, 2583, 13], "temperature": 0.0, "avg_logprob": -0.05542621733267096, "compression_ratio": 1.4656862745098038, "no_speech_prob": 2.482400759618031e-06}, {"id": 192, "seek": 145300, "start": 1470.0, "end": 1478.0, "text": " So we run it for the four. And again, Cholesky doesn't work on this one because it's not positive definite.", "tokens": [407, 321, 1190, 309, 337, 264, 1451, 13, 400, 797, 11, 761, 7456, 4133, 1177, 380, 589, 322, 341, 472, 570, 309, 311, 406, 3353, 25131, 13], "temperature": 0.0, "avg_logprob": -0.05542621733267096, "compression_ratio": 1.4656862745098038, "no_speech_prob": 2.482400759618031e-06}, {"id": 193, "seek": 147800, "start": 1478.0, "end": 1487.0, "text": " And then here we'll see again, there's a big error for the naive approach where we take the inverse.", "tokens": [400, 550, 510, 321, 603, 536, 797, 11, 456, 311, 257, 955, 6713, 337, 264, 29052, 3109, 689, 321, 747, 264, 17340, 13], "temperature": 0.0, "avg_logprob": -0.0882975095278257, "compression_ratio": 1.6214689265536724, "no_speech_prob": 4.637694473785814e-06}, {"id": 194, "seek": 147800, "start": 1487.0, "end": 1492.0, "text": " And then time wise, QR is a bit a bit faster.", "tokens": [400, 550, 565, 10829, 11, 32784, 307, 257, 857, 257, 857, 4663, 13], "temperature": 0.0, "avg_logprob": -0.0882975095278257, "compression_ratio": 1.6214689265536724, "no_speech_prob": 4.637694473785814e-06}, {"id": 195, "seek": 147800, "start": 1492.0, "end": 1496.0, "text": " Well, really, the naive approach is the fastest, which is very wrong.", "tokens": [1042, 11, 534, 11, 264, 29052, 3109, 307, 264, 14573, 11, 597, 307, 588, 2085, 13], "temperature": 0.0, "avg_logprob": -0.0882975095278257, "compression_ratio": 1.6214689265536724, "no_speech_prob": 4.637694473785814e-06}, {"id": 196, "seek": 147800, "start": 1496.0, "end": 1504.0, "text": " But then QR factorization is the fastest of the ones that are correct.", "tokens": [583, 550, 32784, 5952, 2144, 307, 264, 14573, 295, 264, 2306, 300, 366, 3006, 13], "temperature": 0.0, "avg_logprob": -0.0882975095278257, "compression_ratio": 1.6214689265536724, "no_speech_prob": 4.637694473785814e-06}, {"id": 197, "seek": 150400, "start": 1504.0, "end": 1513.0, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.10606311463020943, "compression_ratio": 1.1414141414141414, "no_speech_prob": 2.7264234176982427e-06}, {"id": 198, "seek": 150400, "start": 1513.0, "end": 1518.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.10606311463020943, "compression_ratio": 1.1414141414141414, "no_speech_prob": 2.7264234176982427e-06}, {"id": 199, "seek": 150400, "start": 1518.0, "end": 1524.0, "text": " And then one more example, and this is a little bit redundant.", "tokens": [400, 550, 472, 544, 1365, 11, 293, 341, 307, 257, 707, 857, 40997, 13], "temperature": 0.0, "avg_logprob": -0.10606311463020943, "compression_ratio": 1.1414141414141414, "no_speech_prob": 2.7264234176982427e-06}, {"id": 200, "seek": 150400, "start": 1524.0, "end": 1527.0, "text": " This is with a low rank matrix.", "tokens": [639, 307, 365, 257, 2295, 6181, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10606311463020943, "compression_ratio": 1.1414141414141414, "no_speech_prob": 2.7264234176982427e-06}, {"id": 201, "seek": 152700, "start": 1527.0, "end": 1534.0, "text": " So really, I've just kind of created a set of rows and then I've stacked them on top of each other.", "tokens": [407, 534, 11, 286, 600, 445, 733, 295, 2942, 257, 992, 295, 13241, 293, 550, 286, 600, 28867, 552, 322, 1192, 295, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.06824660301208496, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.27618373225414e-07}, {"id": 202, "seek": 152700, "start": 1534.0, "end": 1538.0, "text": " So there's a huge amount of redundancy in this matrix.", "tokens": [407, 456, 311, 257, 2603, 2372, 295, 27830, 6717, 294, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.06824660301208496, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.27618373225414e-07}, {"id": 203, "seek": 152700, "start": 1538.0, "end": 1547.0, "text": " And running that, we get a large amount of error for the naive solution.", "tokens": [400, 2614, 300, 11, 321, 483, 257, 2416, 2372, 295, 6713, 337, 264, 29052, 3827, 13], "temperature": 0.0, "avg_logprob": -0.06824660301208496, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.27618373225414e-07}, {"id": 204, "seek": 152700, "start": 1547.0, "end": 1551.0, "text": " And then we do have more error than we've had previously for the others.", "tokens": [400, 550, 321, 360, 362, 544, 6713, 813, 321, 600, 632, 8046, 337, 264, 2357, 13], "temperature": 0.0, "avg_logprob": -0.06824660301208496, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.27618373225414e-07}, {"id": 205, "seek": 155100, "start": 1551.0, "end": 1563.0, "text": " And SVD is noticeably worse than QR in this case, although SVD is quicker.", "tokens": [400, 31910, 35, 307, 3449, 1188, 5324, 813, 32784, 294, 341, 1389, 11, 4878, 31910, 35, 307, 16255, 13], "temperature": 0.0, "avg_logprob": -0.07566686536444993, "compression_ratio": 1.3592814371257484, "no_speech_prob": 4.9368331929144915e-06}, {"id": 206, "seek": 155100, "start": 1563.0, "end": 1569.0, "text": " Yeah, so I guess kind of in summary, Cholesky is typically fastest when it works,", "tokens": [865, 11, 370, 286, 2041, 733, 295, 294, 12691, 11, 761, 7456, 4133, 307, 5850, 14573, 562, 309, 1985, 11], "temperature": 0.0, "avg_logprob": -0.07566686536444993, "compression_ratio": 1.3592814371257484, "no_speech_prob": 4.9368331929144915e-06}, {"id": 207, "seek": 155100, "start": 1569.0, "end": 1577.0, "text": " but Cholesky can only be used on symmetric positive definite matrices.", "tokens": [457, 761, 7456, 4133, 393, 787, 312, 1143, 322, 32330, 3353, 25131, 32284, 13], "temperature": 0.0, "avg_logprob": -0.07566686536444993, "compression_ratio": 1.3592814371257484, "no_speech_prob": 4.9368331929144915e-06}, {"id": 208, "seek": 157700, "start": 1577.0, "end": 1584.0, "text": " And then also Cholesky is, well, I guess we weren't really able to do it.", "tokens": [400, 550, 611, 761, 7456, 4133, 307, 11, 731, 11, 286, 2041, 321, 4999, 380, 534, 1075, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.0790497701462001, "compression_ratio": 1.438423645320197, "no_speech_prob": 3.1874938031251077e-06}, {"id": 209, "seek": 157700, "start": 1584.0, "end": 1588.0, "text": " It's unstable for matrices with high condition numbers or low rank.", "tokens": [467, 311, 23742, 337, 32284, 365, 1090, 4188, 3547, 420, 2295, 6181, 13], "temperature": 0.0, "avg_logprob": -0.0790497701462001, "compression_ratio": 1.438423645320197, "no_speech_prob": 3.1874938031251077e-06}, {"id": 210, "seek": 157700, "start": 1588.0, "end": 1597.0, "text": " So linear regression via QR has been recommended by numerical analysts as the standard method for years.", "tokens": [407, 8213, 24590, 5766, 32784, 575, 668, 9628, 538, 29054, 31388, 382, 264, 3832, 3170, 337, 924, 13], "temperature": 0.0, "avg_logprob": -0.0790497701462001, "compression_ratio": 1.438423645320197, "no_speech_prob": 3.1874938031251077e-06}, {"id": 211, "seek": 157700, "start": 1597.0, "end": 1600.0, "text": " Trevathan describes it as good for daily use.", "tokens": [8648, 85, 9390, 15626, 309, 382, 665, 337, 5212, 764, 13], "temperature": 0.0, "avg_logprob": -0.0790497701462001, "compression_ratio": 1.438423645320197, "no_speech_prob": 3.1874938031251077e-06}, {"id": 212, "seek": 160000, "start": 1600.0, "end": 1610.0, "text": " And Trevathan says that their problems were SVD is more stable, although I was not able to recreate that.", "tokens": [400, 8648, 85, 9390, 1619, 300, 641, 2740, 645, 31910, 35, 307, 544, 8351, 11, 4878, 286, 390, 406, 1075, 281, 25833, 300, 13], "temperature": 0.0, "avg_logprob": -0.08133513586861747, "compression_ratio": 1.5, "no_speech_prob": 2.1233347524685087e-06}, {"id": 213, "seek": 160000, "start": 1610.0, "end": 1618.0, "text": " But my hope with this lesson was to kind of get to see what's going on underneath the hood with linear regression least squares,", "tokens": [583, 452, 1454, 365, 341, 6898, 390, 281, 733, 295, 483, 281, 536, 437, 311, 516, 322, 7223, 264, 13376, 365, 8213, 24590, 1935, 19368, 11], "temperature": 0.0, "avg_logprob": -0.08133513586861747, "compression_ratio": 1.5, "no_speech_prob": 2.1233347524685087e-06}, {"id": 214, "seek": 160000, "start": 1618.0, "end": 1621.0, "text": " and also to see that there are different approaches for it.", "tokens": [293, 611, 281, 536, 300, 456, 366, 819, 11587, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.08133513586861747, "compression_ratio": 1.5, "no_speech_prob": 2.1233347524685087e-06}, {"id": 215, "seek": 162100, "start": 1621.0, "end": 1632.0, "text": " And depending on your problem, there might be some solution or some approaches better than others.", "tokens": [400, 5413, 322, 428, 1154, 11, 456, 1062, 312, 512, 3827, 420, 512, 11587, 1101, 813, 2357, 13], "temperature": 0.0, "avg_logprob": -0.08894727780268742, "compression_ratio": 1.1473684210526316, "no_speech_prob": 3.61182242158975e-06}, {"id": 216, "seek": 162100, "start": 1632.0, "end": 1644.0, "text": " Questions?", "tokens": [27738, 30], "temperature": 0.0, "avg_logprob": -0.08894727780268742, "compression_ratio": 1.1473684210526316, "no_speech_prob": 3.61182242158975e-06}, {"id": 217, "seek": 164400, "start": 1644.0, "end": 1663.0, "text": " Okay, in that case, we're going to start on Notebook 7.", "tokens": [1033, 11, 294, 300, 1389, 11, 321, 434, 516, 281, 722, 322, 11633, 2939, 1614, 13], "temperature": 0.0, "avg_logprob": -0.23939305857608192, "compression_ratio": 1.175257731958763, "no_speech_prob": 2.247074735350907e-05}, {"id": 218, "seek": 164400, "start": 1663.0, "end": 1671.0, "text": " Page rank with, well, page rank with eigen-decompositions.", "tokens": [21217, 6181, 365, 11, 731, 11, 3028, 6181, 365, 10446, 12, 1479, 21541, 329, 2451, 13], "temperature": 0.0, "avg_logprob": -0.23939305857608192, "compression_ratio": 1.175257731958763, "no_speech_prob": 2.247074735350907e-05}, {"id": 219, "seek": 167100, "start": 1671.0, "end": 1675.0, "text": " So new start, new topic.", "tokens": [407, 777, 722, 11, 777, 4829, 13], "temperature": 0.0, "avg_logprob": -0.06900996228922969, "compression_ratio": 1.502262443438914, "no_speech_prob": 1.2028137462039012e-05}, {"id": 220, "seek": 167100, "start": 1675.0, "end": 1680.0, "text": " I wanted to introduce two tools that are just useful in general, and we're going to use them today,", "tokens": [286, 1415, 281, 5366, 732, 3873, 300, 366, 445, 4420, 294, 2674, 11, 293, 321, 434, 516, 281, 764, 552, 965, 11], "temperature": 0.0, "avg_logprob": -0.06900996228922969, "compression_ratio": 1.502262443438914, "no_speech_prob": 1.2028137462039012e-05}, {"id": 221, "seek": 167100, "start": 1680.0, "end": 1683.0, "text": " but you can use them for a lot of problems.", "tokens": [457, 291, 393, 764, 552, 337, 257, 688, 295, 2740, 13], "temperature": 0.0, "avg_logprob": -0.06900996228922969, "compression_ratio": 1.502262443438914, "no_speech_prob": 1.2028137462039012e-05}, {"id": 222, "seek": 167100, "start": 1683.0, "end": 1689.0, "text": " One is a library called PSUtil, which lets you check your memory usage.", "tokens": [1485, 307, 257, 6405, 1219, 8168, 52, 20007, 11, 597, 6653, 291, 1520, 428, 4675, 14924, 13], "temperature": 0.0, "avg_logprob": -0.06900996228922969, "compression_ratio": 1.502262443438914, "no_speech_prob": 1.2028137462039012e-05}, {"id": 223, "seek": 167100, "start": 1689.0, "end": 1695.0, "text": " And here we're using a larger data set than we've used before, so this can be nice to have.", "tokens": [400, 510, 321, 434, 1228, 257, 4833, 1412, 992, 813, 321, 600, 1143, 949, 11, 370, 341, 393, 312, 1481, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.06900996228922969, "compression_ratio": 1.502262443438914, "no_speech_prob": 1.2028137462039012e-05}, {"id": 224, "seek": 169500, "start": 1695.0, "end": 1702.0, "text": " So you'd have to pip install this and then import it.", "tokens": [407, 291, 1116, 362, 281, 8489, 3625, 341, 293, 550, 974, 309, 13], "temperature": 0.0, "avg_logprob": -0.07350559795604032, "compression_ratio": 1.198019801980198, "no_speech_prob": 4.49500930699287e-06}, {"id": 225, "seek": 169500, "start": 1702.0, "end": 1711.0, "text": " Here we're kind of getting the process ID, the process memory info.", "tokens": [1692, 321, 434, 733, 295, 1242, 264, 1399, 7348, 11, 264, 1399, 4675, 13614, 13], "temperature": 0.0, "avg_logprob": -0.07350559795604032, "compression_ratio": 1.198019801980198, "no_speech_prob": 4.49500930699287e-06}, {"id": 226, "seek": 171100, "start": 1711.0, "end": 1735.0, "text": " And then I've written a helper method memory usage that just returns the...", "tokens": [400, 550, 286, 600, 3720, 257, 36133, 3170, 4675, 14924, 300, 445, 11247, 264, 485], "temperature": 0.0, "avg_logprob": -0.1710615785498368, "compression_ratio": 0.9868421052631579, "no_speech_prob": 3.187478569088853e-06}, {"id": 227, "seek": 173500, "start": 1735.0, "end": 1741.0, "text": " So RSS memory stands for resident set size memory.", "tokens": [407, 497, 21929, 4675, 7382, 337, 10832, 992, 2744, 4675, 13], "temperature": 0.0, "avg_logprob": -0.07648723125457764, "compression_ratio": 1.5594059405940595, "no_speech_prob": 4.565753897622926e-06}, {"id": 228, "seek": 173500, "start": 1741.0, "end": 1747.0, "text": " And so we're kind of just returning that memory divided by the total memory to get the memory usage.", "tokens": [400, 370, 321, 434, 733, 295, 445, 12678, 300, 4675, 6666, 538, 264, 3217, 4675, 281, 483, 264, 4675, 14924, 13], "temperature": 0.0, "avg_logprob": -0.07648723125457764, "compression_ratio": 1.5594059405940595, "no_speech_prob": 4.565753897622926e-06}, {"id": 229, "seek": 173500, "start": 1747.0, "end": 1754.0, "text": " So this could be a nice thing to monitor if you're having trouble with running out of memory.", "tokens": [407, 341, 727, 312, 257, 1481, 551, 281, 6002, 498, 291, 434, 1419, 5253, 365, 2614, 484, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.07648723125457764, "compression_ratio": 1.5594059405940595, "no_speech_prob": 4.565753897622926e-06}, {"id": 230, "seek": 173500, "start": 1754.0, "end": 1763.0, "text": " And then TQDM is a kind of nice library that gives you progress bars.", "tokens": [400, 550, 314, 48, 35, 44, 307, 257, 733, 295, 1481, 6405, 300, 2709, 291, 4205, 10228, 13], "temperature": 0.0, "avg_logprob": -0.07648723125457764, "compression_ratio": 1.5594059405940595, "no_speech_prob": 4.565753897622926e-06}, {"id": 231, "seek": 176300, "start": 1763.0, "end": 1768.0, "text": " And so this can be good if you are running a for loop with things that are slow.", "tokens": [400, 370, 341, 393, 312, 665, 498, 291, 366, 2614, 257, 337, 6367, 365, 721, 300, 366, 2964, 13], "temperature": 0.0, "avg_logprob": -0.0468666913374415, "compression_ratio": 1.5847457627118644, "no_speech_prob": 7.766682756482624e-06}, {"id": 232, "seek": 176300, "start": 1768.0, "end": 1771.0, "text": " It gives you like a more visual appearance of how you're going.", "tokens": [467, 2709, 291, 411, 257, 544, 5056, 8967, 295, 577, 291, 434, 516, 13], "temperature": 0.0, "avg_logprob": -0.0468666913374415, "compression_ratio": 1.5847457627118644, "no_speech_prob": 7.766682756482624e-06}, {"id": 233, "seek": 176300, "start": 1771.0, "end": 1773.0, "text": " And so this is just a simple example.", "tokens": [400, 370, 341, 307, 445, 257, 2199, 1365, 13], "temperature": 0.0, "avg_logprob": -0.0468666913374415, "compression_ratio": 1.5847457627118644, "no_speech_prob": 7.766682756482624e-06}, {"id": 234, "seek": 176300, "start": 1773.0, "end": 1780.0, "text": " I'm using sleep to make this a slow for loop.", "tokens": [286, 478, 1228, 2817, 281, 652, 341, 257, 2964, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.0468666913374415, "compression_ratio": 1.5847457627118644, "no_speech_prob": 7.766682756482624e-06}, {"id": 235, "seek": 176300, "start": 1780.0, "end": 1784.0, "text": " Here I'm running it, and I kind of don't see anything until it's finished.", "tokens": [1692, 286, 478, 2614, 309, 11, 293, 286, 733, 295, 500, 380, 536, 1340, 1826, 309, 311, 4335, 13], "temperature": 0.0, "avg_logprob": -0.0468666913374415, "compression_ratio": 1.5847457627118644, "no_speech_prob": 7.766682756482624e-06}, {"id": 236, "seek": 176300, "start": 1784.0, "end": 1792.0, "text": " To use TQDM, you just wrap that around whatever you're iterating over.", "tokens": [1407, 764, 314, 48, 35, 44, 11, 291, 445, 7019, 300, 926, 2035, 291, 434, 17138, 990, 670, 13], "temperature": 0.0, "avg_logprob": -0.0468666913374415, "compression_ratio": 1.5847457627118644, "no_speech_prob": 7.766682756482624e-06}, {"id": 237, "seek": 179200, "start": 1792.0, "end": 1798.0, "text": " So here we've wrapped range 10 with TQDM.", "tokens": [407, 510, 321, 600, 14226, 3613, 1266, 365, 314, 48, 35, 44, 13], "temperature": 0.0, "avg_logprob": -0.0721745252609253, "compression_ratio": 1.1551724137931034, "no_speech_prob": 5.954834250587737e-06}, {"id": 238, "seek": 179200, "start": 1798.0, "end": 1810.0, "text": " Now running the exact same loop, you see we get this progress bar that's updating as you go.", "tokens": [823, 2614, 264, 1900, 912, 6367, 11, 291, 536, 321, 483, 341, 4205, 2159, 300, 311, 25113, 382, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.0721745252609253, "compression_ratio": 1.1551724137931034, "no_speech_prob": 5.954834250587737e-06}, {"id": 239, "seek": 181000, "start": 1810.0, "end": 1829.0, "text": " So to get started, can someone remind us what the SVD is?", "tokens": [407, 281, 483, 1409, 11, 393, 1580, 4160, 505, 437, 264, 31910, 35, 307, 30], "temperature": 0.0, "avg_logprob": -0.148844203433475, "compression_ratio": 1.18348623853211, "no_speech_prob": 6.338878392853076e-06}, {"id": 240, "seek": 181000, "start": 1829.0, "end": 1834.0, "text": " Matthew and Vincent, can you throw the mic?", "tokens": [12434, 293, 28003, 11, 393, 291, 3507, 264, 3123, 30], "temperature": 0.0, "avg_logprob": -0.148844203433475, "compression_ratio": 1.18348623853211, "no_speech_prob": 6.338878392853076e-06}, {"id": 241, "seek": 181000, "start": 1834.0, "end": 1836.0, "text": " Single value decomposition.", "tokens": [31248, 2158, 48356, 13], "temperature": 0.0, "avg_logprob": -0.148844203433475, "compression_ratio": 1.18348623853211, "no_speech_prob": 6.338878392853076e-06}, {"id": 242, "seek": 183600, "start": 1836.0, "end": 1841.0, "text": " Yes, singular value decomposition. And what does it give you?", "tokens": [1079, 11, 20010, 2158, 48356, 13, 400, 437, 775, 309, 976, 291, 30], "temperature": 0.0, "avg_logprob": -0.22642950578169388, "compression_ratio": 1.5170731707317073, "no_speech_prob": 7.141024252632633e-05}, {"id": 243, "seek": 183600, "start": 1841.0, "end": 1847.0, "text": " It gives you three different vacancies. One would be the singular value.", "tokens": [467, 2709, 291, 1045, 819, 2842, 32286, 13, 1485, 576, 312, 264, 20010, 2158, 13], "temperature": 0.0, "avg_logprob": -0.22642950578169388, "compression_ratio": 1.5170731707317073, "no_speech_prob": 7.141024252632633e-05}, {"id": 244, "seek": 183600, "start": 1847.0, "end": 1856.0, "text": " The first and third are normal vacancies.", "tokens": [440, 700, 293, 2636, 366, 2710, 2842, 32286, 13], "temperature": 0.0, "avg_logprob": -0.22642950578169388, "compression_ratio": 1.5170731707317073, "no_speech_prob": 7.141024252632633e-05}, {"id": 245, "seek": 183600, "start": 1856.0, "end": 1859.0, "text": " Regarding the topic or whatever.", "tokens": [35523, 264, 4829, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.22642950578169388, "compression_ratio": 1.5170731707317073, "no_speech_prob": 7.141024252632633e-05}, {"id": 246, "seek": 183600, "start": 1859.0, "end": 1863.0, "text": " Right, and that's actually kind of getting to the next question of what are some applications of SVD.", "tokens": [1779, 11, 293, 300, 311, 767, 733, 295, 1242, 281, 264, 958, 1168, 295, 437, 366, 512, 5821, 295, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.22642950578169388, "compression_ratio": 1.5170731707317073, "no_speech_prob": 7.141024252632633e-05}, {"id": 247, "seek": 186300, "start": 1863.0, "end": 1867.0, "text": " So yeah, topic modeling was one of them.", "tokens": [407, 1338, 11, 4829, 15983, 390, 472, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.10755232175191244, "compression_ratio": 1.408450704225352, "no_speech_prob": 1.4970480151532684e-05}, {"id": 248, "seek": 186300, "start": 1867.0, "end": 1871.0, "text": " Yeah, so you're getting U, which is...", "tokens": [865, 11, 370, 291, 434, 1242, 624, 11, 597, 307, 485], "temperature": 0.0, "avg_logprob": -0.10755232175191244, "compression_ratio": 1.408450704225352, "no_speech_prob": 1.4970480151532684e-05}, {"id": 249, "seek": 186300, "start": 1871.0, "end": 1875.0, "text": " So in the full version, U and V are both orthonormal square matrices.", "tokens": [407, 294, 264, 1577, 3037, 11, 624, 293, 691, 366, 1293, 420, 11943, 24440, 3732, 32284, 13], "temperature": 0.0, "avg_logprob": -0.10755232175191244, "compression_ratio": 1.408450704225352, "no_speech_prob": 1.4970480151532684e-05}, {"id": 250, "seek": 186300, "start": 1875.0, "end": 1880.0, "text": " In the reduced version, U has orthonormal columns.", "tokens": [682, 264, 9212, 3037, 11, 624, 575, 420, 11943, 24440, 13766, 13], "temperature": 0.0, "avg_logprob": -0.10755232175191244, "compression_ratio": 1.408450704225352, "no_speech_prob": 1.4970480151532684e-05}, {"id": 251, "seek": 188000, "start": 1880.0, "end": 1900.0, "text": " Yeah, so Matthew got us started. What are some other applications of SVD?", "tokens": [865, 11, 370, 12434, 658, 505, 1409, 13, 708, 366, 512, 661, 5821, 295, 31910, 35, 30], "temperature": 0.0, "avg_logprob": -0.1289678414662679, "compression_ratio": 0.9883720930232558, "no_speech_prob": 5.828518624184653e-05}, {"id": 252, "seek": 188000, "start": 1900.0, "end": 1906.0, "text": " Roger?", "tokens": [17666, 30], "temperature": 0.0, "avg_logprob": -0.1289678414662679, "compression_ratio": 0.9883720930232558, "no_speech_prob": 5.828518624184653e-05}, {"id": 253, "seek": 188000, "start": 1906.0, "end": 1908.0, "text": " PCA?", "tokens": [6465, 32, 30], "temperature": 0.0, "avg_logprob": -0.1289678414662679, "compression_ratio": 0.9883720930232558, "no_speech_prob": 5.828518624184653e-05}, {"id": 254, "seek": 190800, "start": 1908.0, "end": 1916.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.1480852005973695, "compression_ratio": 1.3012820512820513, "no_speech_prob": 7.842964259907603e-05}, {"id": 255, "seek": 190800, "start": 1916.0, "end": 1925.0, "text": " Yeah, so PCA is SVD. And what are we doing to get the low rank approximation?", "tokens": [865, 11, 370, 6465, 32, 307, 31910, 35, 13, 400, 437, 366, 321, 884, 281, 483, 264, 2295, 6181, 28023, 30], "temperature": 0.0, "avg_logprob": -0.1480852005973695, "compression_ratio": 1.3012820512820513, "no_speech_prob": 7.842964259907603e-05}, {"id": 256, "seek": 190800, "start": 1925.0, "end": 1931.0, "text": " Exactly, yeah. So this is also called truncated SVD, where we kind of drop off part of them and just take the first few.", "tokens": [7587, 11, 1338, 13, 407, 341, 307, 611, 1219, 504, 409, 66, 770, 31910, 35, 11, 689, 321, 733, 295, 3270, 766, 644, 295, 552, 293, 445, 747, 264, 700, 1326, 13], "temperature": 0.0, "avg_logprob": -0.1480852005973695, "compression_ratio": 1.3012820512820513, "no_speech_prob": 7.842964259907603e-05}, {"id": 257, "seek": 193100, "start": 1931.0, "end": 1945.0, "text": " Exactly. Other applications?", "tokens": [7587, 13, 5358, 5821, 30], "temperature": 0.0, "avg_logprob": -0.08260159265427362, "compression_ratio": 0.9714285714285714, "no_speech_prob": 2.2124688257463276e-05}, {"id": 258, "seek": 193100, "start": 1945.0, "end": 1955.0, "text": " There's one we spent an entire week on.", "tokens": [821, 311, 472, 321, 4418, 364, 2302, 1243, 322, 13], "temperature": 0.0, "avg_logprob": -0.08260159265427362, "compression_ratio": 0.9714285714285714, "no_speech_prob": 2.2124688257463276e-05}, {"id": 259, "seek": 195500, "start": 1955.0, "end": 1961.0, "text": " Yeah, background removal. So we got pretty good results just using SVD.", "tokens": [865, 11, 3678, 17933, 13, 407, 321, 658, 1238, 665, 3542, 445, 1228, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.07991109212239583, "compression_ratio": 1.489795918367347, "no_speech_prob": 1.3211328223405872e-05}, {"id": 260, "seek": 195500, "start": 1961.0, "end": 1973.0, "text": " And then we used the robust PCA or primary component pursuit, and that used SVD as a step and kind of iteratively did multiple SVDs.", "tokens": [400, 550, 321, 1143, 264, 13956, 6465, 32, 420, 6194, 6542, 23365, 11, 293, 300, 1143, 31910, 35, 382, 257, 1823, 293, 733, 295, 17138, 19020, 630, 3866, 31910, 35, 82, 13], "temperature": 0.0, "avg_logprob": -0.07991109212239583, "compression_ratio": 1.489795918367347, "no_speech_prob": 1.3211328223405872e-05}, {"id": 261, "seek": 195500, "start": 1973.0, "end": 1982.0, "text": " And then just now we saw SVD as a way to calculate the least squares linear regression.", "tokens": [400, 550, 445, 586, 321, 1866, 31910, 35, 382, 257, 636, 281, 8873, 264, 1935, 19368, 8213, 24590, 13], "temperature": 0.0, "avg_logprob": -0.07991109212239583, "compression_ratio": 1.489795918367347, "no_speech_prob": 1.3211328223405872e-05}, {"id": 262, "seek": 198200, "start": 1982.0, "end": 1988.0, "text": " Yeah, so hopefully you're convinced that SVD is useful and shows up lots of places.", "tokens": [865, 11, 370, 4696, 291, 434, 12561, 300, 31910, 35, 307, 4420, 293, 3110, 493, 3195, 295, 3190, 13], "temperature": 0.0, "avg_logprob": -0.08579289376198708, "compression_ratio": 1.6, "no_speech_prob": 1.6441568732261658e-05}, {"id": 263, "seek": 198200, "start": 1988.0, "end": 1998.0, "text": " I'm kind of a little bit off topic, but I'm doing a workshop this afternoon on word embeddings, such as Word2Vec, which is like a library of word embeddings.", "tokens": [286, 478, 733, 295, 257, 707, 857, 766, 4829, 11, 457, 286, 478, 884, 257, 13541, 341, 6499, 322, 1349, 12240, 29432, 11, 1270, 382, 8725, 17, 53, 3045, 11, 597, 307, 411, 257, 6405, 295, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.08579289376198708, "compression_ratio": 1.6, "no_speech_prob": 1.6441568732261658e-05}, {"id": 264, "seek": 198200, "start": 1998.0, "end": 2010.0, "text": " And so I was looking at the problem of bias in Word2Vec. So word embeddings give you these analogies, which can be really great, like Spain is to Madrid, as Italy is to Rome.", "tokens": [400, 370, 286, 390, 1237, 412, 264, 1154, 295, 12577, 294, 8725, 17, 53, 3045, 13, 407, 1349, 12240, 29432, 976, 291, 613, 16660, 530, 11, 597, 393, 312, 534, 869, 11, 411, 12838, 307, 281, 22091, 11, 382, 10705, 307, 281, 12043, 13], "temperature": 0.0, "avg_logprob": -0.08579289376198708, "compression_ratio": 1.6, "no_speech_prob": 1.6441568732261658e-05}, {"id": 265, "seek": 201000, "start": 2010.0, "end": 2021.0, "text": " But they also have been found to have kind of biased analogies, such as father is to doctor, as mother is to nurse, or man is to computer programmer, as woman is to homemaker.", "tokens": [583, 436, 611, 362, 668, 1352, 281, 362, 733, 295, 28035, 16660, 530, 11, 1270, 382, 3086, 307, 281, 4631, 11, 382, 2895, 307, 281, 14012, 11, 420, 587, 307, 281, 3820, 32116, 11, 382, 3059, 307, 281, 1280, 18821, 13], "temperature": 0.0, "avg_logprob": -0.08662403589007499, "compression_ratio": 1.6142857142857143, "no_speech_prob": 3.3403382531105308e-06}, {"id": 266, "seek": 201000, "start": 2021.0, "end": 2030.0, "text": " And so there's been work done around how can you kind of remove this bias. And so I was reading a paper on it and they used SVD as part of their debiasing process.", "tokens": [400, 370, 456, 311, 668, 589, 1096, 926, 577, 393, 291, 733, 295, 4159, 341, 12577, 13, 400, 370, 286, 390, 3760, 257, 3035, 322, 309, 293, 436, 1143, 31910, 35, 382, 644, 295, 641, 3001, 72, 3349, 1399, 13], "temperature": 0.0, "avg_logprob": -0.08662403589007499, "compression_ratio": 1.6142857142857143, "no_speech_prob": 3.3403382531105308e-06}, {"id": 267, "seek": 203000, "start": 2030.0, "end": 2040.0, "text": " So I thought that was pretty neat. I was like, oh, this is a relevant application of SVD. So I just wanted to highlight that.", "tokens": [407, 286, 1194, 300, 390, 1238, 10654, 13, 286, 390, 411, 11, 1954, 11, 341, 307, 257, 7340, 3861, 295, 31910, 35, 13, 407, 286, 445, 1415, 281, 5078, 300, 13], "temperature": 0.0, "avg_logprob": -0.08039658339982181, "compression_ratio": 1.5396825396825398, "no_speech_prob": 8.800611794868018e-06}, {"id": 268, "seek": 203000, "start": 2040.0, "end": 2046.0, "text": " Yeah, so a few different ways to think about SVD are data compression.", "tokens": [865, 11, 370, 257, 1326, 819, 2098, 281, 519, 466, 31910, 35, 366, 1412, 19355, 13], "temperature": 0.0, "avg_logprob": -0.08039658339982181, "compression_ratio": 1.5396825396825398, "no_speech_prob": 8.800611794868018e-06}, {"id": 269, "seek": 203000, "start": 2046.0, "end": 2059.0, "text": " So this comes up with the PCA or when you're dropping many of your kind of lower singular values, the less informative parts, you're, you know, getting something that fits in a smaller space.", "tokens": [407, 341, 1487, 493, 365, 264, 6465, 32, 420, 562, 291, 434, 13601, 867, 295, 428, 733, 295, 3126, 20010, 4190, 11, 264, 1570, 27759, 3166, 11, 291, 434, 11, 291, 458, 11, 1242, 746, 300, 9001, 294, 257, 4356, 1901, 13], "temperature": 0.0, "avg_logprob": -0.08039658339982181, "compression_ratio": 1.5396825396825398, "no_speech_prob": 8.800611794868018e-06}, {"id": 270, "seek": 205900, "start": 2059.0, "end": 2069.0, "text": " Another another way to think about or related way to think about it is SVD trades a large number of features for a smaller set of better features.", "tokens": [3996, 1071, 636, 281, 519, 466, 420, 4077, 636, 281, 519, 466, 309, 307, 31910, 35, 21287, 257, 2416, 1230, 295, 4122, 337, 257, 4356, 992, 295, 1101, 4122, 13], "temperature": 0.0, "avg_logprob": -0.07641866769683495, "compression_ratio": 1.7962085308056872, "no_speech_prob": 9.817906175157987e-06}, {"id": 271, "seek": 205900, "start": 2069.0, "end": 2084.0, "text": " And I think that's that's kind of what we saw with topic modeling of, you know, you could have a feature for each word or we could get the smaller set of features that are kind of groups of words that have, you know, related topics.", "tokens": [400, 286, 519, 300, 311, 300, 311, 733, 295, 437, 321, 1866, 365, 4829, 15983, 295, 11, 291, 458, 11, 291, 727, 362, 257, 4111, 337, 1184, 1349, 420, 321, 727, 483, 264, 4356, 992, 295, 4122, 300, 366, 733, 295, 3935, 295, 2283, 300, 362, 11, 291, 458, 11, 4077, 8378, 13], "temperature": 0.0, "avg_logprob": -0.07641866769683495, "compression_ratio": 1.7962085308056872, "no_speech_prob": 9.817906175157987e-06}, {"id": 272, "seek": 208400, "start": 2084.0, "end": 2096.0, "text": " And then I think this is kind of neat, but it's all matrices are actually diagonal. If you change the bases on the domain and range and.", "tokens": [400, 550, 286, 519, 341, 307, 733, 295, 10654, 11, 457, 309, 311, 439, 32284, 366, 767, 21539, 13, 759, 291, 1319, 264, 17949, 322, 264, 9274, 293, 3613, 293, 13], "temperature": 0.0, "avg_logprob": -0.14480382374354772, "compression_ratio": 1.2592592592592593, "no_speech_prob": 3.6687342799268663e-06}, {"id": 273, "seek": 209600, "start": 2096.0, "end": 2124.0, "text": " So this might be a good time. I was going to show the three blue one brown change of basis video. Maybe let me do that now.", "tokens": [407, 341, 1062, 312, 257, 665, 565, 13, 286, 390, 516, 281, 855, 264, 1045, 3344, 472, 6292, 1319, 295, 5143, 960, 13, 2704, 718, 385, 360, 300, 586, 13], "temperature": 0.0, "avg_logprob": -0.1600319076986874, "compression_ratio": 1.1826923076923077, "no_speech_prob": 2.429061169095803e-05}, {"id": 274, "seek": 212400, "start": 2124.0, "end": 2132.0, "text": " So,", "tokens": [407, 11], "temperature": 0.0, "avg_logprob": -0.46281890869140624, "compression_ratio": 1.2635658914728682, "no_speech_prob": 7.248533802339807e-05}, {"id": 275, "seek": 212400, "start": 2132.0, "end": 2137.0, "text": " if I have a vector sitting here in 2D space, we have a standard way to describe it with coordinates.", "tokens": [498, 286, 362, 257, 8062, 3798, 510, 294, 568, 35, 1901, 11, 321, 362, 257, 3832, 636, 281, 6786, 309, 365, 21056, 13], "temperature": 0.0, "avg_logprob": -0.46281890869140624, "compression_ratio": 1.2635658914728682, "no_speech_prob": 7.248533802339807e-05}, {"id": 276, "seek": 212400, "start": 2137.0, "end": 2140.0, "text": " Can everyone hear, or does that need to be louder?", "tokens": [1664, 1518, 1568, 11, 420, 775, 300, 643, 281, 312, 22717, 30], "temperature": 0.0, "avg_logprob": -0.46281890869140624, "compression_ratio": 1.2635658914728682, "no_speech_prob": 7.248533802339807e-05}, {"id": 277, "seek": 212400, "start": 2140.0, "end": 2146.0, "text": " Louder.", "tokens": [7272, 1068, 13], "temperature": 0.0, "avg_logprob": -0.46281890869140624, "compression_ratio": 1.2635658914728682, "no_speech_prob": 7.248533802339807e-05}, {"id": 278, "seek": 214600, "start": 2146.0, "end": 2159.0, "text": " Number.", "tokens": [5118, 13], "temperature": 0.0, "avg_logprob": -0.14765119552612305, "compression_ratio": 1.5380116959064327, "no_speech_prob": 1.4059091881790664e-05}, {"id": 279, "seek": 214600, "start": 2159.0, "end": 2164.0, "text": " If I have a vector sitting here in 2D space, we have a standard way to describe it with coordinates.", "tokens": [759, 286, 362, 257, 8062, 3798, 510, 294, 568, 35, 1901, 11, 321, 362, 257, 3832, 636, 281, 6786, 309, 365, 21056, 13], "temperature": 0.0, "avg_logprob": -0.14765119552612305, "compression_ratio": 1.5380116959064327, "no_speech_prob": 1.4059091881790664e-05}, {"id": 280, "seek": 214600, "start": 2164.0, "end": 2173.0, "text": " In this case, the vector has coordinates three, two, which means going from its tail to its tip involves moving three units to the right and two units up.", "tokens": [682, 341, 1389, 11, 264, 8062, 575, 21056, 1045, 11, 732, 11, 597, 1355, 516, 490, 1080, 6838, 281, 1080, 4125, 11626, 2684, 1045, 6815, 281, 264, 558, 293, 732, 6815, 493, 13], "temperature": 0.0, "avg_logprob": -0.14765119552612305, "compression_ratio": 1.5380116959064327, "no_speech_prob": 1.4059091881790664e-05}, {"id": 281, "seek": 217300, "start": 2173.0, "end": 2182.0, "text": " Now, the more linear algebra oriented way to describe coordinates is to think of each of these numbers as a scalar, the thing that stretches or squishes vectors.", "tokens": [823, 11, 264, 544, 8213, 21989, 21841, 636, 281, 6786, 21056, 307, 281, 519, 295, 1184, 295, 613, 3547, 382, 257, 39684, 11, 264, 551, 300, 29058, 420, 2339, 16423, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1279854244656033, "compression_ratio": 1.9106382978723404, "no_speech_prob": 1.5934117982396856e-05}, {"id": 282, "seek": 217300, "start": 2182.0, "end": 2189.0, "text": " You think of that first coordinate as scaling I hat, the vector with length one pointing to the right.", "tokens": [509, 519, 295, 300, 700, 15670, 382, 21589, 286, 2385, 11, 264, 8062, 365, 4641, 472, 12166, 281, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.1279854244656033, "compression_ratio": 1.9106382978723404, "no_speech_prob": 1.5934117982396856e-05}, {"id": 283, "seek": 217300, "start": 2189.0, "end": 2195.0, "text": " While the second coordinate scales J hat, the vector with length one pointing straight up.", "tokens": [3987, 264, 1150, 15670, 17408, 508, 2385, 11, 264, 8062, 365, 4641, 472, 12166, 2997, 493, 13], "temperature": 0.0, "avg_logprob": -0.1279854244656033, "compression_ratio": 1.9106382978723404, "no_speech_prob": 1.5934117982396856e-05}, {"id": 284, "seek": 217300, "start": 2195.0, "end": 2201.0, "text": " The tip to tail sum of those two scale vectors is what the coordinates are meant to describe.", "tokens": [440, 4125, 281, 6838, 2408, 295, 729, 732, 4373, 18875, 307, 437, 264, 21056, 366, 4140, 281, 6786, 13], "temperature": 0.0, "avg_logprob": -0.1279854244656033, "compression_ratio": 1.9106382978723404, "no_speech_prob": 1.5934117982396856e-05}, {"id": 285, "seek": 220100, "start": 2201.0, "end": 2208.0, "text": " You can think of these two special vectors as encapsulating all of the implicit assumptions of our coordinate system.", "tokens": [509, 393, 519, 295, 613, 732, 2121, 18875, 382, 38745, 12162, 439, 295, 264, 26947, 17695, 295, 527, 15670, 1185, 13], "temperature": 0.0, "avg_logprob": -0.05966975829180549, "compression_ratio": 1.7212389380530972, "no_speech_prob": 4.468230690690689e-05}, {"id": 286, "seek": 220100, "start": 2208.0, "end": 2215.0, "text": " The fact that the first number indicates rightward motion, that the second one indicates upward motion, exactly how far a unit of distance is,", "tokens": [440, 1186, 300, 264, 700, 1230, 16203, 558, 1007, 5394, 11, 300, 264, 1150, 472, 16203, 23452, 5394, 11, 2293, 577, 1400, 257, 4985, 295, 4560, 307, 11], "temperature": 0.0, "avg_logprob": -0.05966975829180549, "compression_ratio": 1.7212389380530972, "no_speech_prob": 4.468230690690689e-05}, {"id": 287, "seek": 220100, "start": 2215.0, "end": 2223.0, "text": " all of that is tied up in the choice of I hat and J hat as the vectors which our scalar coordinates are meant to actually scale.", "tokens": [439, 295, 300, 307, 9601, 493, 294, 264, 3922, 295, 286, 2385, 293, 508, 2385, 382, 264, 18875, 597, 527, 39684, 21056, 366, 4140, 281, 767, 4373, 13], "temperature": 0.0, "avg_logprob": -0.05966975829180549, "compression_ratio": 1.7212389380530972, "no_speech_prob": 4.468230690690689e-05}, {"id": 288, "seek": 222300, "start": 2223.0, "end": 2235.0, "text": " Any way to translate between vectors and set of numbers is called a coordinate system, and the two special vectors I hat and J hat are called the basis vectors of our standard coordinate system.", "tokens": [2639, 636, 281, 13799, 1296, 18875, 293, 992, 295, 3547, 307, 1219, 257, 15670, 1185, 11, 293, 264, 732, 2121, 18875, 286, 2385, 293, 508, 2385, 366, 1219, 264, 5143, 18875, 295, 527, 3832, 15670, 1185, 13], "temperature": 0.0, "avg_logprob": -0.05181319443220945, "compression_ratio": 1.7413793103448276, "no_speech_prob": 4.029341653222218e-06}, {"id": 289, "seek": 222300, "start": 2235.0, "end": 2241.0, "text": " What I'd like to talk about here is the idea of using a different set of basis vectors.", "tokens": [708, 286, 1116, 411, 281, 751, 466, 510, 307, 264, 1558, 295, 1228, 257, 819, 992, 295, 5143, 18875, 13], "temperature": 0.0, "avg_logprob": -0.05181319443220945, "compression_ratio": 1.7413793103448276, "no_speech_prob": 4.029341653222218e-06}, {"id": 290, "seek": 222300, "start": 2241.0, "end": 2249.0, "text": " For example, let's say you have a friend, Jennifer, who uses a different set of basis vectors, which I'll call B1 and B2.", "tokens": [1171, 1365, 11, 718, 311, 584, 291, 362, 257, 1277, 11, 14351, 11, 567, 4960, 257, 819, 992, 295, 5143, 18875, 11, 597, 286, 603, 818, 363, 16, 293, 363, 17, 13], "temperature": 0.0, "avg_logprob": -0.05181319443220945, "compression_ratio": 1.7413793103448276, "no_speech_prob": 4.029341653222218e-06}, {"id": 291, "seek": 224900, "start": 2249.0, "end": 2257.0, "text": " Her first basis vector, B1, points up and to the right a little bit, and her second vector, B2, points left and up.", "tokens": [3204, 700, 5143, 8062, 11, 363, 16, 11, 2793, 493, 293, 281, 264, 558, 257, 707, 857, 11, 293, 720, 1150, 8062, 11, 363, 17, 11, 2793, 1411, 293, 493, 13], "temperature": 0.0, "avg_logprob": -0.06633738109043666, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.2029179742967244e-05}, {"id": 292, "seek": 224900, "start": 2257.0, "end": 2266.0, "text": " Now take another look at that vector that I showed earlier, the one that you and I would describe using the coordinates 3, 2, using our basis vectors I hat and J hat.", "tokens": [823, 747, 1071, 574, 412, 300, 8062, 300, 286, 4712, 3071, 11, 264, 472, 300, 291, 293, 286, 576, 6786, 1228, 264, 21056, 805, 11, 568, 11, 1228, 527, 5143, 18875, 286, 2385, 293, 508, 2385, 13], "temperature": 0.0, "avg_logprob": -0.06633738109043666, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.2029179742967244e-05}, {"id": 293, "seek": 224900, "start": 2266.0, "end": 2272.0, "text": " Jennifer would actually describe this vector with the coordinates 5 thirds and 1 third.", "tokens": [14351, 576, 767, 6786, 341, 8062, 365, 264, 21056, 1025, 34552, 293, 502, 2636, 13], "temperature": 0.0, "avg_logprob": -0.06633738109043666, "compression_ratio": 1.7289719626168225, "no_speech_prob": 1.2029179742967244e-05}, {"id": 294, "seek": 227200, "start": 2272.0, "end": 2286.0, "text": " What this means is that the particular way to get to that vector using our two basis vectors is to scale B1 by 5 thirds, scale B2 by 1 third, then add them both together.", "tokens": [708, 341, 1355, 307, 300, 264, 1729, 636, 281, 483, 281, 300, 8062, 1228, 527, 732, 5143, 18875, 307, 281, 4373, 363, 16, 538, 1025, 34552, 11, 4373, 363, 17, 538, 502, 2636, 11, 550, 909, 552, 1293, 1214, 13], "temperature": 0.0, "avg_logprob": -0.05850997652326311, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.3925277926318813e-06}, {"id": 295, "seek": 227200, "start": 2286.0, "end": 2291.0, "text": " In a little bit I'll show you how you could have figured out those two numbers, 5 thirds and 1 third.", "tokens": [682, 257, 707, 857, 286, 603, 855, 291, 577, 291, 727, 362, 8932, 484, 729, 732, 3547, 11, 1025, 34552, 293, 502, 2636, 13], "temperature": 0.0, "avg_logprob": -0.05850997652326311, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.3925277926318813e-06}, {"id": 296, "seek": 229100, "start": 2291.0, "end": 2303.0, "text": " In general, whenever Jennifer uses coordinates to describe a vector, she thinks of her first coordinate as scaling B1, the second coordinate as scaling B2, and she adds the results.", "tokens": [682, 2674, 11, 5699, 14351, 4960, 21056, 281, 6786, 257, 8062, 11, 750, 7309, 295, 720, 700, 15670, 382, 21589, 363, 16, 11, 264, 1150, 15670, 382, 21589, 363, 17, 11, 293, 750, 10860, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.0452908891620058, "compression_ratio": 1.617801047120419, "no_speech_prob": 4.784948941960465e-06}, {"id": 297, "seek": 229100, "start": 2303.0, "end": 2310.0, "text": " What she gets will typically be completely different from the vector that you and I would think of as having those coordinates.", "tokens": [708, 750, 2170, 486, 5850, 312, 2584, 819, 490, 264, 8062, 300, 291, 293, 286, 576, 519, 295, 382, 1419, 729, 21056, 13], "temperature": 0.0, "avg_logprob": -0.0452908891620058, "compression_ratio": 1.617801047120419, "no_speech_prob": 4.784948941960465e-06}, {"id": 298, "seek": 231000, "start": 2310.0, "end": 2324.0, "text": " To be a little more precise about the setup here, her first basis vector, B1, is something that we would describe with the coordinates 2, 1, and her second basis vector, B2, is something that we would describe as negative 1, 1.", "tokens": [1407, 312, 257, 707, 544, 13600, 466, 264, 8657, 510, 11, 720, 700, 5143, 8062, 11, 363, 16, 11, 307, 746, 300, 321, 576, 6786, 365, 264, 21056, 568, 11, 502, 11, 293, 720, 1150, 5143, 8062, 11, 363, 17, 11, 307, 746, 300, 321, 576, 6786, 382, 3671, 502, 11, 502, 13], "temperature": 0.0, "avg_logprob": -0.06886053522792432, "compression_ratio": 1.826839826839827, "no_speech_prob": 7.888911568443291e-06}, {"id": 299, "seek": 231000, "start": 2324.0, "end": 2331.0, "text": " But it's important to realize, from her perspective in her system, those vectors have coordinates 1, 0, and 0, 1.", "tokens": [583, 309, 311, 1021, 281, 4325, 11, 490, 720, 4585, 294, 720, 1185, 11, 729, 18875, 362, 21056, 502, 11, 1958, 11, 293, 1958, 11, 502, 13], "temperature": 0.0, "avg_logprob": -0.06886053522792432, "compression_ratio": 1.826839826839827, "no_speech_prob": 7.888911568443291e-06}, {"id": 300, "seek": 231000, "start": 2331.0, "end": 2338.0, "text": " They are what define the meaning of the coordinates 1, 0, and 0, 1 in her world.", "tokens": [814, 366, 437, 6964, 264, 3620, 295, 264, 21056, 502, 11, 1958, 11, 293, 1958, 11, 502, 294, 720, 1002, 13], "temperature": 0.0, "avg_logprob": -0.06886053522792432, "compression_ratio": 1.826839826839827, "no_speech_prob": 7.888911568443291e-06}, {"id": 301, "seek": 233800, "start": 2338.0, "end": 2349.0, "text": " So, in effect, we're speaking different languages. We're all looking at the same vectors in space, but Jennifer uses different words and numbers to describe them.", "tokens": [407, 11, 294, 1802, 11, 321, 434, 4124, 819, 8650, 13, 492, 434, 439, 1237, 412, 264, 912, 18875, 294, 1901, 11, 457, 14351, 4960, 819, 2283, 293, 3547, 281, 6786, 552, 13], "temperature": 0.0, "avg_logprob": -0.05980620635183234, "compression_ratio": 1.5303030303030303, "no_speech_prob": 2.467704689479433e-05}, {"id": 302, "seek": 233800, "start": 2349.0, "end": 2356.0, "text": " Let me say a quick word about how I'm representing things here. When I animate 2D space, I typically use this square grid.", "tokens": [961, 385, 584, 257, 1702, 1349, 466, 577, 286, 478, 13460, 721, 510, 13, 1133, 286, 36439, 568, 35, 1901, 11, 286, 5850, 764, 341, 3732, 10748, 13], "temperature": 0.0, "avg_logprob": -0.05980620635183234, "compression_ratio": 1.5303030303030303, "no_speech_prob": 2.467704689479433e-05}, {"id": 303, "seek": 233800, "start": 2356.0, "end": 2364.0, "text": " But that grid is just a construct, a way to visualize our coordinate system, and so it depends on our choice of basis.", "tokens": [583, 300, 10748, 307, 445, 257, 7690, 11, 257, 636, 281, 23273, 527, 15670, 1185, 11, 293, 370, 309, 5946, 322, 527, 3922, 295, 5143, 13], "temperature": 0.0, "avg_logprob": -0.05980620635183234, "compression_ratio": 1.5303030303030303, "no_speech_prob": 2.467704689479433e-05}, {"id": 304, "seek": 236400, "start": 2364.0, "end": 2379.0, "text": " Space itself has no intrinsic grid. Jennifer might draw her own grid, which would be an equally made-up construct meant as nothing more than a visual tool to help follow the meaning of her coordinates.", "tokens": [8705, 2564, 575, 572, 35698, 10748, 13, 14351, 1062, 2642, 720, 1065, 10748, 11, 597, 576, 312, 364, 12309, 1027, 12, 1010, 7690, 4140, 382, 1825, 544, 813, 257, 5056, 2290, 281, 854, 1524, 264, 3620, 295, 720, 21056, 13], "temperature": 0.0, "avg_logprob": -0.05217674752356301, "compression_ratio": 1.5311004784688995, "no_speech_prob": 1.4737463970959652e-05}, {"id": 305, "seek": 236400, "start": 2379.0, "end": 2385.0, "text": " Her origin, though, would actually line up with ours, since everybody agrees on what the coordinates 0, 0 should mean.", "tokens": [3204, 4957, 11, 1673, 11, 576, 767, 1622, 493, 365, 11896, 11, 1670, 2201, 26383, 322, 437, 264, 21056, 1958, 11, 1958, 820, 914, 13], "temperature": 0.0, "avg_logprob": -0.05217674752356301, "compression_ratio": 1.5311004784688995, "no_speech_prob": 1.4737463970959652e-05}, {"id": 306, "seek": 238500, "start": 2385.0, "end": 2396.0, "text": " It's the thing that you get when you scale any vector by 0. But the direction of her axes and the spacing of her grid lines will be different, depending on her choice of basis vectors.", "tokens": [467, 311, 264, 551, 300, 291, 483, 562, 291, 4373, 604, 8062, 538, 1958, 13, 583, 264, 3513, 295, 720, 35387, 293, 264, 27739, 295, 720, 10748, 3876, 486, 312, 819, 11, 5413, 322, 720, 3922, 295, 5143, 18875, 13], "temperature": 0.0, "avg_logprob": -0.0626799347474403, "compression_ratio": 1.6030534351145038, "no_speech_prob": 2.521450142012327e-06}, {"id": 307, "seek": 238500, "start": 2396.0, "end": 2403.0, "text": " So, after all this is set up, a pretty natural question to ask is how do we translate between coordinate systems?", "tokens": [407, 11, 934, 439, 341, 307, 992, 493, 11, 257, 1238, 3303, 1168, 281, 1029, 307, 577, 360, 321, 13799, 1296, 15670, 3652, 30], "temperature": 0.0, "avg_logprob": -0.0626799347474403, "compression_ratio": 1.6030534351145038, "no_speech_prob": 2.521450142012327e-06}, {"id": 308, "seek": 238500, "start": 2403.0, "end": 2412.0, "text": " If, for example, Jennifer describes a vector with coordinates negative 1, 2, what would that be in our coordinate system?", "tokens": [759, 11, 337, 1365, 11, 14351, 15626, 257, 8062, 365, 21056, 3671, 502, 11, 568, 11, 437, 576, 300, 312, 294, 527, 15670, 1185, 30], "temperature": 0.0, "avg_logprob": -0.0626799347474403, "compression_ratio": 1.6030534351145038, "no_speech_prob": 2.521450142012327e-06}, {"id": 309, "seek": 241200, "start": 2412.0, "end": 2415.0, "text": " How do you translate from her language to ours?", "tokens": [1012, 360, 291, 13799, 490, 720, 2856, 281, 11896, 30], "temperature": 0.0, "avg_logprob": -0.06301356852054596, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.1125204764539376e-05}, {"id": 310, "seek": 241200, "start": 2415.0, "end": 2425.0, "text": " Well, what our coordinates are saying is that this vector is negative 1 times b1 plus 2 times b2.", "tokens": [1042, 11, 437, 527, 21056, 366, 1566, 307, 300, 341, 8062, 307, 3671, 502, 1413, 272, 16, 1804, 568, 1413, 272, 17, 13], "temperature": 0.0, "avg_logprob": -0.06301356852054596, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.1125204764539376e-05}, {"id": 311, "seek": 241200, "start": 2425.0, "end": 2435.0, "text": " And from our perspective, b1 has coordinates 2, 1, and b2 has coordinates negative 1, 1.", "tokens": [400, 490, 527, 4585, 11, 272, 16, 575, 21056, 568, 11, 502, 11, 293, 272, 17, 575, 21056, 3671, 502, 11, 502, 13], "temperature": 0.0, "avg_logprob": -0.06301356852054596, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.1125204764539376e-05}, {"id": 312, "seek": 243500, "start": 2435.0, "end": 2443.0, "text": " So we can actually compute negative 1 times b1 plus 2 times b2 as they're represented in our coordinate system.", "tokens": [407, 321, 393, 767, 14722, 3671, 502, 1413, 272, 16, 1804, 568, 1413, 272, 17, 382, 436, 434, 10379, 294, 527, 15670, 1185, 13], "temperature": 0.0, "avg_logprob": -0.04426954702003715, "compression_ratio": 1.6719367588932805, "no_speech_prob": 2.9943648769403808e-06}, {"id": 313, "seek": 243500, "start": 2443.0, "end": 2449.0, "text": " And working this out, you get a vector with coordinates negative 4, 1.", "tokens": [400, 1364, 341, 484, 11, 291, 483, 257, 8062, 365, 21056, 3671, 1017, 11, 502, 13], "temperature": 0.0, "avg_logprob": -0.04426954702003715, "compression_ratio": 1.6719367588932805, "no_speech_prob": 2.9943648769403808e-06}, {"id": 314, "seek": 243500, "start": 2449.0, "end": 2454.0, "text": " So that's how we would describe the vector that she thinks of as negative 1, 2.", "tokens": [407, 300, 311, 577, 321, 576, 6786, 264, 8062, 300, 750, 7309, 295, 382, 3671, 502, 11, 568, 13], "temperature": 0.0, "avg_logprob": -0.04426954702003715, "compression_ratio": 1.6719367588932805, "no_speech_prob": 2.9943648769403808e-06}, {"id": 315, "seek": 243500, "start": 2454.0, "end": 2464.0, "text": " This process here of scaling each of her basis vectors by the corresponding coordinates of some vector, then adding them together, might feel somewhat familiar.", "tokens": [639, 1399, 510, 295, 21589, 1184, 295, 720, 5143, 18875, 538, 264, 11760, 21056, 295, 512, 8062, 11, 550, 5127, 552, 1214, 11, 1062, 841, 8344, 4963, 13], "temperature": 0.0, "avg_logprob": -0.04426954702003715, "compression_ratio": 1.6719367588932805, "no_speech_prob": 2.9943648769403808e-06}, {"id": 316, "seek": 246400, "start": 2464.0, "end": 2471.0, "text": " It's matrix vector multiplication, with a matrix whose columns represent Jennifer's basis vectors in our language.", "tokens": [467, 311, 8141, 8062, 27290, 11, 365, 257, 8141, 6104, 13766, 2906, 14351, 311, 5143, 18875, 294, 527, 2856, 13], "temperature": 0.0, "avg_logprob": -0.047554808855056765, "compression_ratio": 1.6343612334801763, "no_speech_prob": 3.071539686061442e-05}, {"id": 317, "seek": 246400, "start": 2471.0, "end": 2477.0, "text": " In fact, once you understand matrix vector multiplication as applying a certain linear transformation,", "tokens": [682, 1186, 11, 1564, 291, 1223, 8141, 8062, 27290, 382, 9275, 257, 1629, 8213, 9887, 11], "temperature": 0.0, "avg_logprob": -0.047554808855056765, "compression_ratio": 1.6343612334801763, "no_speech_prob": 3.071539686061442e-05}, {"id": 318, "seek": 246400, "start": 2477.0, "end": 2481.0, "text": " say by watching what I view to be the most important video in this series, Chapter 3,", "tokens": [584, 538, 1976, 437, 286, 1910, 281, 312, 264, 881, 1021, 960, 294, 341, 2638, 11, 18874, 805, 11], "temperature": 0.0, "avg_logprob": -0.047554808855056765, "compression_ratio": 1.6343612334801763, "no_speech_prob": 3.071539686061442e-05}, {"id": 319, "seek": 246400, "start": 2481.0, "end": 2485.0, "text": " there's a pretty intuitive way to think about what's going on here.", "tokens": [456, 311, 257, 1238, 21769, 636, 281, 519, 466, 437, 311, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.047554808855056765, "compression_ratio": 1.6343612334801763, "no_speech_prob": 3.071539686061442e-05}, {"id": 320, "seek": 248500, "start": 2485.0, "end": 2494.0, "text": " A matrix whose columns represent Jennifer's basis vectors can be thought of as a transformation that moves our basis vectors, i-hat and j-hat,", "tokens": [316, 8141, 6104, 13766, 2906, 14351, 311, 5143, 18875, 393, 312, 1194, 295, 382, 257, 9887, 300, 6067, 527, 5143, 18875, 11, 741, 12, 15178, 293, 361, 12, 15178, 11], "temperature": 0.0, "avg_logprob": -0.04698892541833826, "compression_ratio": 1.834710743801653, "no_speech_prob": 7.296015155588975e-06}, {"id": 321, "seek": 248500, "start": 2494.0, "end": 2504.0, "text": " the things we think of when we say 1, 0 and 0, 1, to Jennifer's basis vectors, the things she thinks of when she says 1, 0 and 0, 1.", "tokens": [264, 721, 321, 519, 295, 562, 321, 584, 502, 11, 1958, 293, 1958, 11, 502, 11, 281, 14351, 311, 5143, 18875, 11, 264, 721, 750, 7309, 295, 562, 750, 1619, 502, 11, 1958, 293, 1958, 11, 502, 13], "temperature": 0.0, "avg_logprob": -0.04698892541833826, "compression_ratio": 1.834710743801653, "no_speech_prob": 7.296015155588975e-06}, {"id": 322, "seek": 248500, "start": 2504.0, "end": 2514.0, "text": " To show how this works, let's walk through what it would mean to take the vector that we think of as having coordinates negative 1, 2, and applying that transformation.", "tokens": [1407, 855, 577, 341, 1985, 11, 718, 311, 1792, 807, 437, 309, 576, 914, 281, 747, 264, 8062, 300, 321, 519, 295, 382, 1419, 21056, 3671, 502, 11, 568, 11, 293, 9275, 300, 9887, 13], "temperature": 0.0, "avg_logprob": -0.04698892541833826, "compression_ratio": 1.834710743801653, "no_speech_prob": 7.296015155588975e-06}, {"id": 323, "seek": 251400, "start": 2514.0, "end": 2520.0, "text": " Before the linear transformation, we're thinking of this vector as a certain linear combination of our basis vectors,", "tokens": [4546, 264, 8213, 9887, 11, 321, 434, 1953, 295, 341, 8062, 382, 257, 1629, 8213, 6562, 295, 527, 5143, 18875, 11], "temperature": 0.0, "avg_logprob": -0.04394669585175567, "compression_ratio": 2.0103092783505154, "no_speech_prob": 1.0128704161616042e-05}, {"id": 324, "seek": 251400, "start": 2520.0, "end": 2523.0, "text": " negative 1 times i-hat plus 2 times j-hat.", "tokens": [3671, 502, 1413, 741, 12, 15178, 1804, 568, 1413, 361, 12, 15178, 13], "temperature": 0.0, "avg_logprob": -0.04394669585175567, "compression_ratio": 2.0103092783505154, "no_speech_prob": 1.0128704161616042e-05}, {"id": 325, "seek": 251400, "start": 2523.0, "end": 2532.0, "text": " And the key feature of a linear transformation is that the resulting vector will be that same linear combination but of the new basis vectors,", "tokens": [400, 264, 2141, 4111, 295, 257, 8213, 9887, 307, 300, 264, 16505, 8062, 486, 312, 300, 912, 8213, 6562, 457, 295, 264, 777, 5143, 18875, 11], "temperature": 0.0, "avg_logprob": -0.04394669585175567, "compression_ratio": 2.0103092783505154, "no_speech_prob": 1.0128704161616042e-05}, {"id": 326, "seek": 251400, "start": 2532.0, "end": 2539.0, "text": " negative 1 times the place where i-hat lands plus 2 times the place where j-hat lands.", "tokens": [3671, 502, 1413, 264, 1081, 689, 741, 12, 15178, 5949, 1804, 568, 1413, 264, 1081, 689, 361, 12, 15178, 5949, 13], "temperature": 0.0, "avg_logprob": -0.04394669585175567, "compression_ratio": 2.0103092783505154, "no_speech_prob": 1.0128704161616042e-05}, {"id": 327, "seek": 253900, "start": 2539.0, "end": 2550.0, "text": " So what this matrix does is transform our misconception of what Jennifer means into the actual vector that she's referring to.", "tokens": [407, 437, 341, 8141, 775, 307, 4088, 527, 41350, 295, 437, 14351, 1355, 666, 264, 3539, 8062, 300, 750, 311, 13761, 281, 13], "temperature": 0.0, "avg_logprob": -0.04892681284648616, "compression_ratio": 1.662162162162162, "no_speech_prob": 2.947752591353492e-06}, {"id": 328, "seek": 253900, "start": 2550.0, "end": 2554.0, "text": " I remember that when I was first learning this, it always felt kind of backwards to me.", "tokens": [286, 1604, 300, 562, 286, 390, 700, 2539, 341, 11, 309, 1009, 2762, 733, 295, 12204, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.04892681284648616, "compression_ratio": 1.662162162162162, "no_speech_prob": 2.947752591353492e-06}, {"id": 329, "seek": 253900, "start": 2554.0, "end": 2567.0, "text": " Geometrically, this matrix transforms our grid into Jennifer's grid, but numerically, it's translating a vector described in her language to our language.", "tokens": [2876, 649, 81, 984, 11, 341, 8141, 35592, 527, 10748, 666, 14351, 311, 10748, 11, 457, 7866, 984, 11, 309, 311, 35030, 257, 8062, 7619, 294, 720, 2856, 281, 527, 2856, 13], "temperature": 0.0, "avg_logprob": -0.04892681284648616, "compression_ratio": 1.662162162162162, "no_speech_prob": 2.947752591353492e-06}, {"id": 330, "seek": 256700, "start": 2567.0, "end": 2572.0, "text": " What made it finally click for me was thinking about how it takes our misconception of what Jennifer means,", "tokens": [708, 1027, 309, 2721, 2052, 337, 385, 390, 1953, 466, 577, 309, 2516, 527, 41350, 295, 437, 14351, 1355, 11], "temperature": 0.0, "avg_logprob": -0.06266535160153411, "compression_ratio": 1.6832579185520362, "no_speech_prob": 2.506201963115018e-05}, {"id": 331, "seek": 256700, "start": 2572.0, "end": 2581.0, "text": " the vector we get using the same coordinates but in our system, then it transforms it into the vector that she really meant.", "tokens": [264, 8062, 321, 483, 1228, 264, 912, 21056, 457, 294, 527, 1185, 11, 550, 309, 35592, 309, 666, 264, 8062, 300, 750, 534, 4140, 13], "temperature": 0.0, "avg_logprob": -0.06266535160153411, "compression_ratio": 1.6832579185520362, "no_speech_prob": 2.506201963115018e-05}, {"id": 332, "seek": 256700, "start": 2581.0, "end": 2584.0, "text": " What about going the other way around?", "tokens": [708, 466, 516, 264, 661, 636, 926, 30], "temperature": 0.0, "avg_logprob": -0.06266535160153411, "compression_ratio": 1.6832579185520362, "no_speech_prob": 2.506201963115018e-05}, {"id": 333, "seek": 256700, "start": 2584.0, "end": 2589.0, "text": " In the example I used earlier this video, when I had the vector with coordinates 3, 2 in our system,", "tokens": [682, 264, 1365, 286, 1143, 3071, 341, 960, 11, 562, 286, 632, 264, 8062, 365, 21056, 805, 11, 568, 294, 527, 1185, 11], "temperature": 0.0, "avg_logprob": -0.06266535160153411, "compression_ratio": 1.6832579185520362, "no_speech_prob": 2.506201963115018e-05}, {"id": 334, "seek": 258900, "start": 2589.0, "end": 2598.0, "text": " how did I compute that it would have coordinates 5 thirds and 1 third in Jennifer's system?", "tokens": [577, 630, 286, 14722, 300, 309, 576, 362, 21056, 1025, 34552, 293, 502, 2636, 294, 14351, 311, 1185, 30], "temperature": 0.0, "avg_logprob": -0.04988508293594139, "compression_ratio": 1.6, "no_speech_prob": 1.6280064301099628e-06}, {"id": 335, "seek": 258900, "start": 2598.0, "end": 2609.0, "text": " You start with that change of basis matrix that translates Jennifer's language into ours, then you take its inverse.", "tokens": [509, 722, 365, 300, 1319, 295, 5143, 8141, 300, 28468, 14351, 311, 2856, 666, 11896, 11, 550, 291, 747, 1080, 17340, 13], "temperature": 0.0, "avg_logprob": -0.04988508293594139, "compression_ratio": 1.6, "no_speech_prob": 1.6280064301099628e-06}, {"id": 336, "seek": 258900, "start": 2609.0, "end": 2616.0, "text": " Remember, the inverse of a transformation is a new transformation that corresponds to playing that first one backwards.", "tokens": [5459, 11, 264, 17340, 295, 257, 9887, 307, 257, 777, 9887, 300, 23249, 281, 2433, 300, 700, 472, 12204, 13], "temperature": 0.0, "avg_logprob": -0.04988508293594139, "compression_ratio": 1.6, "no_speech_prob": 1.6280064301099628e-06}, {"id": 337, "seek": 261600, "start": 2616.0, "end": 2623.0, "text": " In practice, especially when you're working in more than two dimensions, you'd use a computer to compute the matrix that actually represents this inverse.", "tokens": [682, 3124, 11, 2318, 562, 291, 434, 1364, 294, 544, 813, 732, 12819, 11, 291, 1116, 764, 257, 3820, 281, 14722, 264, 8141, 300, 767, 8855, 341, 17340, 13], "temperature": 0.0, "avg_logprob": -0.07295015611146626, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.0951506737910677e-05}, {"id": 338, "seek": 261600, "start": 2623.0, "end": 2636.0, "text": " In this case, the inverse of the change of basis matrix that has Jennifer's basis as its columns ends up working out to have columns 1 third, negative 1 third, and 1 third, 2 thirds.", "tokens": [682, 341, 1389, 11, 264, 17340, 295, 264, 1319, 295, 5143, 8141, 300, 575, 14351, 311, 5143, 382, 1080, 13766, 5314, 493, 1364, 484, 281, 362, 13766, 502, 2636, 11, 3671, 502, 2636, 11, 293, 502, 2636, 11, 568, 34552, 13], "temperature": 0.0, "avg_logprob": -0.07295015611146626, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.0951506737910677e-05}, {"id": 339, "seek": 263600, "start": 2636.0, "end": 2653.0, "text": " So for example, to see what the vector 3, 2 looks like in Jennifer's system, we multiply this inverse change of basis matrix by the vector 3, 2, which works out to be 5 thirds, 1 third.", "tokens": [407, 337, 1365, 11, 281, 536, 437, 264, 8062, 805, 11, 568, 1542, 411, 294, 14351, 311, 1185, 11, 321, 12972, 341, 17340, 1319, 295, 5143, 8141, 538, 264, 8062, 805, 11, 568, 11, 597, 1985, 484, 281, 312, 1025, 34552, 11, 502, 2636, 13], "temperature": 0.0, "avg_logprob": -0.05018461072767103, "compression_ratio": 1.5048543689320388, "no_speech_prob": 6.438825039367657e-06}, {"id": 340, "seek": 263600, "start": 2653.0, "end": 2661.0, "text": " So that, in a nutshell, is how to translate the description of individual vectors back and forth between coordinate systems.", "tokens": [407, 300, 11, 294, 257, 37711, 11, 307, 577, 281, 13799, 264, 3855, 295, 2609, 18875, 646, 293, 5220, 1296, 15670, 3652, 13], "temperature": 0.0, "avg_logprob": -0.05018461072767103, "compression_ratio": 1.5048543689320388, "no_speech_prob": 6.438825039367657e-06}, {"id": 341, "seek": 266100, "start": 2661.0, "end": 2672.0, "text": " The matrix whose columns represent Jennifer's basis vectors, but written in our coordinates, translates vectors from her language into our language.", "tokens": [440, 8141, 6104, 13766, 2906, 14351, 311, 5143, 18875, 11, 457, 3720, 294, 527, 21056, 11, 28468, 18875, 490, 720, 2856, 666, 527, 2856, 13], "temperature": 0.0, "avg_logprob": -0.057091342078314886, "compression_ratio": 1.5384615384615385, "no_speech_prob": 3.071393075515516e-05}, {"id": 342, "seek": 266100, "start": 2672.0, "end": 2677.0, "text": " And the inverse matrix does the opposite.", "tokens": [400, 264, 17340, 8141, 775, 264, 6182, 13], "temperature": 0.0, "avg_logprob": -0.057091342078314886, "compression_ratio": 1.5384615384615385, "no_speech_prob": 3.071393075515516e-05}, {"id": 343, "seek": 266100, "start": 2677.0, "end": 2681.0, "text": " But vectors aren't the only thing that we describe using coordinates.", "tokens": [583, 18875, 3212, 380, 264, 787, 551, 300, 321, 6786, 1228, 21056, 13], "temperature": 0.0, "avg_logprob": -0.057091342078314886, "compression_ratio": 1.5384615384615385, "no_speech_prob": 3.071393075515516e-05}, {"id": 344, "seek": 268100, "start": 2681.0, "end": 2692.0, "text": " For this next part, it's important that you're all comfortable representing transformations with matrices, and that you know how matrix multiplication corresponds to composing successive transformations.", "tokens": [1171, 341, 958, 644, 11, 309, 311, 1021, 300, 291, 434, 439, 4619, 13460, 34852, 365, 32284, 11, 293, 300, 291, 458, 577, 8141, 27290, 23249, 281, 715, 6110, 48043, 34852, 13], "temperature": 0.0, "avg_logprob": -0.058336888040815084, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.4508923413814045e-05}, {"id": 345, "seek": 268100, "start": 2692.0, "end": 2699.0, "text": " Definitely pause and take a look at chapters 3 and 4 if any of that feels uneasy.", "tokens": [12151, 10465, 293, 747, 257, 574, 412, 20013, 805, 293, 1017, 498, 604, 295, 300, 3417, 48589, 13], "temperature": 0.0, "avg_logprob": -0.058336888040815084, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.4508923413814045e-05}, {"id": 346, "seek": 269900, "start": 2699.0, "end": 2711.0, "text": " Consider some linear transformation, like a 90 degree counterclockwise rotation. When you and I represent this with a matrix, we follow where the basis vectors i-hat and j-hat each go.", "tokens": [17416, 512, 8213, 9887, 11, 411, 257, 4289, 4314, 5682, 48685, 12447, 13, 1133, 291, 293, 286, 2906, 341, 365, 257, 8141, 11, 321, 1524, 689, 264, 5143, 18875, 741, 12, 15178, 293, 361, 12, 15178, 1184, 352, 13], "temperature": 0.0, "avg_logprob": -0.06374953774844899, "compression_ratio": 1.6990291262135921, "no_speech_prob": 8.939377039496321e-06}, {"id": 347, "seek": 269900, "start": 2711.0, "end": 2722.0, "text": " i-hat ends up at the spot with coordinates 0, 1, and j-hat ends up at the spot with coordinates negative 1, 0. So those coordinates become the columns of our matrix.", "tokens": [741, 12, 15178, 5314, 493, 412, 264, 4008, 365, 21056, 1958, 11, 502, 11, 293, 361, 12, 15178, 5314, 493, 412, 264, 4008, 365, 21056, 3671, 502, 11, 1958, 13, 407, 729, 21056, 1813, 264, 13766, 295, 527, 8141, 13], "temperature": 0.0, "avg_logprob": -0.06374953774844899, "compression_ratio": 1.6990291262135921, "no_speech_prob": 8.939377039496321e-06}, {"id": 348, "seek": 272200, "start": 2722.0, "end": 2736.0, "text": " But this representation is heavily tied up in our choice of basis vectors, from the fact that we're following i-hat and j-hat in the first place, to the fact that we're recording their landing spots in our own coordinate system.", "tokens": [583, 341, 10290, 307, 10950, 9601, 493, 294, 527, 3922, 295, 5143, 18875, 11, 490, 264, 1186, 300, 321, 434, 3480, 741, 12, 15178, 293, 361, 12, 15178, 294, 264, 700, 1081, 11, 281, 264, 1186, 300, 321, 434, 6613, 641, 11202, 10681, 294, 527, 1065, 15670, 1185, 13], "temperature": 0.0, "avg_logprob": -0.02848702758105833, "compression_ratio": 1.5364583333333333, "no_speech_prob": 3.0892977065377636e-06}, {"id": 349, "seek": 272200, "start": 2736.0, "end": 2746.0, "text": " How would Jennifer describe this same 90 degree rotation of space?", "tokens": [1012, 576, 14351, 6786, 341, 912, 4289, 4314, 12447, 295, 1901, 30], "temperature": 0.0, "avg_logprob": -0.02848702758105833, "compression_ratio": 1.5364583333333333, "no_speech_prob": 3.0892977065377636e-06}, {"id": 350, "seek": 274600, "start": 2746.0, "end": 2753.0, "text": " You might be tempted to just translate the columns of our rotation matrix into Jennifer's language, but that's not quite right.", "tokens": [509, 1062, 312, 29941, 281, 445, 13799, 264, 13766, 295, 527, 12447, 8141, 666, 14351, 311, 2856, 11, 457, 300, 311, 406, 1596, 558, 13], "temperature": 0.0, "avg_logprob": -0.04098059693161322, "compression_ratio": 1.7791164658634537, "no_speech_prob": 4.2226847654092126e-06}, {"id": 351, "seek": 274600, "start": 2753.0, "end": 2766.0, "text": " Those columns represent where our basis vectors i-hat and j-hat go, but the matrix that Jennifer wants should represent where her basis vectors land, and it needs to describe those landing spots in her language.", "tokens": [3950, 13766, 2906, 689, 527, 5143, 18875, 741, 12, 15178, 293, 361, 12, 15178, 352, 11, 457, 264, 8141, 300, 14351, 2738, 820, 2906, 689, 720, 5143, 18875, 2117, 11, 293, 309, 2203, 281, 6786, 729, 11202, 10681, 294, 720, 2856, 13], "temperature": 0.0, "avg_logprob": -0.04098059693161322, "compression_ratio": 1.7791164658634537, "no_speech_prob": 4.2226847654092126e-06}, {"id": 352, "seek": 274600, "start": 2766.0, "end": 2772.0, "text": " Here's a common way to think of how this is done. Start with any vector written in Jennifer's language.", "tokens": [1692, 311, 257, 2689, 636, 281, 519, 295, 577, 341, 307, 1096, 13, 6481, 365, 604, 8062, 3720, 294, 14351, 311, 2856, 13], "temperature": 0.0, "avg_logprob": -0.04098059693161322, "compression_ratio": 1.7791164658634537, "no_speech_prob": 4.2226847654092126e-06}, {"id": 353, "seek": 277200, "start": 2772.0, "end": 2785.0, "text": " Rather than trying to follow what happens to it in terms of her language, first we're going to translate it into our language using the change of basis matrix, the one whose columns represent her basis vectors in our language.", "tokens": [16571, 813, 1382, 281, 1524, 437, 2314, 281, 309, 294, 2115, 295, 720, 2856, 11, 700, 321, 434, 516, 281, 13799, 309, 666, 527, 2856, 1228, 264, 1319, 295, 5143, 8141, 11, 264, 472, 6104, 13766, 2906, 720, 5143, 18875, 294, 527, 2856, 13], "temperature": 0.0, "avg_logprob": -0.04848185273789868, "compression_ratio": 1.860759493670886, "no_speech_prob": 1.777792385837529e-05}, {"id": 354, "seek": 277200, "start": 2785.0, "end": 2789.0, "text": " This gives us the same vector, but now written in our language.", "tokens": [639, 2709, 505, 264, 912, 8062, 11, 457, 586, 3720, 294, 527, 2856, 13], "temperature": 0.0, "avg_logprob": -0.04848185273789868, "compression_ratio": 1.860759493670886, "no_speech_prob": 1.777792385837529e-05}, {"id": 355, "seek": 277200, "start": 2789.0, "end": 2799.0, "text": " Then, apply the transformation matrix to what you get by multiplying it on the left. This tells us where that vector lands, but still in our language.", "tokens": [1396, 11, 3079, 264, 9887, 8141, 281, 437, 291, 483, 538, 30955, 309, 322, 264, 1411, 13, 639, 5112, 505, 689, 300, 8062, 5949, 11, 457, 920, 294, 527, 2856, 13], "temperature": 0.0, "avg_logprob": -0.04848185273789868, "compression_ratio": 1.860759493670886, "no_speech_prob": 1.777792385837529e-05}, {"id": 356, "seek": 279900, "start": 2799.0, "end": 2809.0, "text": " So as a last step, apply the inverse change of basis matrix, multiply it on the left as usual, to get the transformed vector, but now in Jennifer's language.", "tokens": [407, 382, 257, 1036, 1823, 11, 3079, 264, 17340, 1319, 295, 5143, 8141, 11, 12972, 309, 322, 264, 1411, 382, 7713, 11, 281, 483, 264, 16894, 8062, 11, 457, 586, 294, 14351, 311, 2856, 13], "temperature": 0.0, "avg_logprob": -0.056503070874160594, "compression_ratio": 1.9428571428571428, "no_speech_prob": 9.276305377170502e-07}, {"id": 357, "seek": 279900, "start": 2809.0, "end": 2819.0, "text": " Since we can do this with any vector written in her language, first applying the change of basis, then the transformation, then the inverse change of basis,", "tokens": [4162, 321, 393, 360, 341, 365, 604, 8062, 3720, 294, 720, 2856, 11, 700, 9275, 264, 1319, 295, 5143, 11, 550, 264, 9887, 11, 550, 264, 17340, 1319, 295, 5143, 11], "temperature": 0.0, "avg_logprob": -0.056503070874160594, "compression_ratio": 1.9428571428571428, "no_speech_prob": 9.276305377170502e-07}, {"id": 358, "seek": 279900, "start": 2819.0, "end": 2825.0, "text": " that composition of three matrices gives us the transformation matrix in Jennifer's language.", "tokens": [300, 12686, 295, 1045, 32284, 2709, 505, 264, 9887, 8141, 294, 14351, 311, 2856, 13], "temperature": 0.0, "avg_logprob": -0.056503070874160594, "compression_ratio": 1.9428571428571428, "no_speech_prob": 9.276305377170502e-07}, {"id": 359, "seek": 282500, "start": 2825.0, "end": 2831.0, "text": " It takes in a vector of her language and spits out the transformed version of that vector in her language.", "tokens": [467, 2516, 294, 257, 8062, 295, 720, 2856, 293, 637, 1208, 484, 264, 16894, 3037, 295, 300, 8062, 294, 720, 2856, 13], "temperature": 0.0, "avg_logprob": -0.07421365190059581, "compression_ratio": 1.7186147186147187, "no_speech_prob": 5.255186351860175e-06}, {"id": 360, "seek": 282500, "start": 2831.0, "end": 2841.0, "text": " For this specific example, when Jennifer's basis vectors look like 2,1 and negative 1,1 in our language, and when the transformation is a 90 degree rotation,", "tokens": [1171, 341, 2685, 1365, 11, 562, 14351, 311, 5143, 18875, 574, 411, 568, 11, 16, 293, 3671, 502, 11, 16, 294, 527, 2856, 11, 293, 562, 264, 9887, 307, 257, 4289, 4314, 12447, 11], "temperature": 0.0, "avg_logprob": -0.07421365190059581, "compression_ratio": 1.7186147186147187, "no_speech_prob": 5.255186351860175e-06}, {"id": 361, "seek": 282500, "start": 2841.0, "end": 2850.0, "text": " the product of these three matrices, if you work through it, has columns 1 third, 5 thirds, and negative 2 thirds, negative 1 third.", "tokens": [264, 1674, 295, 613, 1045, 32284, 11, 498, 291, 589, 807, 309, 11, 575, 13766, 502, 2636, 11, 1025, 34552, 11, 293, 3671, 568, 34552, 11, 3671, 502, 2636, 13], "temperature": 0.0, "avg_logprob": -0.07421365190059581, "compression_ratio": 1.7186147186147187, "no_speech_prob": 5.255186351860175e-06}, {"id": 362, "seek": 285000, "start": 2850.0, "end": 2865.0, "text": " So if Jennifer multiplies that matrix by the coordinates of a vector in her system, it will return the 90 degree rotated version of that vector expressed in her coordinate system.", "tokens": [407, 498, 14351, 12788, 530, 300, 8141, 538, 264, 21056, 295, 257, 8062, 294, 720, 1185, 11, 309, 486, 2736, 264, 4289, 4314, 42146, 3037, 295, 300, 8062, 12675, 294, 720, 15670, 1185, 13], "temperature": 0.0, "avg_logprob": -0.055812131613492966, "compression_ratio": 1.5360824742268042, "no_speech_prob": 4.860049102717312e-06}, {"id": 363, "seek": 285000, "start": 2865.0, "end": 2873.0, "text": " In general, whenever you see an expression like A inverse times M times A, it suggests a mathematical sort of empathy.", "tokens": [682, 2674, 11, 5699, 291, 536, 364, 6114, 411, 316, 17340, 1413, 376, 1413, 316, 11, 309, 13409, 257, 18894, 1333, 295, 18701, 13], "temperature": 0.0, "avg_logprob": -0.055812131613492966, "compression_ratio": 1.5360824742268042, "no_speech_prob": 4.860049102717312e-06}, {"id": 364, "seek": 287300, "start": 2873.0, "end": 2882.0, "text": " That middle matrix represents a transformation of some kind as you see it, and the outer two matrices represent the empathy, the shift in perspective.", "tokens": [663, 2808, 8141, 8855, 257, 9887, 295, 512, 733, 382, 291, 536, 309, 11, 293, 264, 10847, 732, 32284, 2906, 264, 18701, 11, 264, 5513, 294, 4585, 13], "temperature": 0.0, "avg_logprob": -0.07031825455752286, "compression_ratio": 1.6640625, "no_speech_prob": 9.654445420892444e-06}, {"id": 365, "seek": 287300, "start": 2882.0, "end": 2888.0, "text": " And the full matrix product represents that same transformation, but as someone else sees it.", "tokens": [400, 264, 1577, 8141, 1674, 8855, 300, 912, 9887, 11, 457, 382, 1580, 1646, 8194, 309, 13], "temperature": 0.0, "avg_logprob": -0.07031825455752286, "compression_ratio": 1.6640625, "no_speech_prob": 9.654445420892444e-06}, {"id": 366, "seek": 288800, "start": 2888.0, "end": 2903.0, "text": " For those of you wondering why we care about alternate coordinate systems, the next video on eigenvectors and eigenvalues will give a really important example of this. See you then!", "tokens": [1171, 729, 295, 291, 6359, 983, 321, 1127, 466, 18873, 15670, 3652, 11, 264, 958, 960, 322, 10446, 303, 5547, 293, 10446, 46033, 486, 976, 257, 534, 1021, 1365, 295, 341, 13, 3008, 291, 550, 0], "temperature": 0.0, "avg_logprob": -0.062217545509338376, "compression_ratio": 1.3507462686567164, "no_speech_prob": 3.269354783697054e-05}, {"id": 367, "seek": 290300, "start": 2903.0, "end": 2921.0, "text": " Alright, so I really like that analogy of change of basis being like translating between languages, and the idea of empathy with multiplying by change of basis and inverse to change the basis back.", "tokens": [2798, 11, 370, 286, 534, 411, 300, 21663, 295, 1319, 295, 5143, 885, 411, 35030, 1296, 8650, 11, 293, 264, 1558, 295, 18701, 365, 30955, 538, 1319, 295, 5143, 293, 17340, 281, 1319, 264, 5143, 646, 13], "temperature": 0.0, "avg_logprob": -0.12233216762542724, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.019694814458489418}, {"id": 368, "seek": 290300, "start": 2921.0, "end": 2931.0, "text": " So we'll be using those ideas a bit. We're not going to watch the eigen decomposition video in here, but you might want to watch that at home if you're interested.", "tokens": [407, 321, 603, 312, 1228, 729, 3487, 257, 857, 13, 492, 434, 406, 516, 281, 1159, 264, 10446, 48356, 960, 294, 510, 11, 457, 291, 1062, 528, 281, 1159, 300, 412, 1280, 498, 291, 434, 3102, 13], "temperature": 0.0, "avg_logprob": -0.12233216762542724, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.019694814458489418}, {"id": 369, "seek": 293100, "start": 2931.0, "end": 2939.0, "text": " I think this is a good time to take a break. It's 11.55, so let's meet back in eight minutes, like at 12.03.", "tokens": [286, 519, 341, 307, 257, 665, 565, 281, 747, 257, 1821, 13, 467, 311, 2975, 13, 13622, 11, 370, 718, 311, 1677, 646, 294, 3180, 2077, 11, 411, 412, 2272, 13, 11592, 13], "temperature": 0.0, "avg_logprob": -0.10788207925776, "compression_ratio": 1.5304347826086957, "no_speech_prob": 0.0001880556665128097}, {"id": 370, "seek": 293100, "start": 2939.0, "end": 2947.0, "text": " And if you have questions about the video, maybe write them down and then we can talk about them when we meet back. Thanks.", "tokens": [400, 498, 291, 362, 1651, 466, 264, 960, 11, 1310, 2464, 552, 760, 293, 550, 321, 393, 751, 466, 552, 562, 321, 1677, 646, 13, 2561, 13], "temperature": 0.0, "avg_logprob": -0.10788207925776, "compression_ratio": 1.5304347826086957, "no_speech_prob": 0.0001880556665128097}, {"id": 371, "seek": 293100, "start": 2947.0, "end": 2957.0, "text": " So yeah, we just saw the video on change of basis. Yeah, I really like the analogy about translating between languages.", "tokens": [407, 1338, 11, 321, 445, 1866, 264, 960, 322, 1319, 295, 5143, 13, 865, 11, 286, 534, 411, 264, 21663, 466, 35030, 1296, 8650, 13], "temperature": 0.0, "avg_logprob": -0.10788207925776, "compression_ratio": 1.5304347826086957, "no_speech_prob": 0.0001880556665128097}, {"id": 372, "seek": 295700, "start": 2957.0, "end": 2970.0, "text": " I also like that it showed, again, we've talked about how you can think of matrix multiplication as taking a linear combination of the columns, and that's what was going on with kind of the original basis.", "tokens": [286, 611, 411, 300, 309, 4712, 11, 797, 11, 321, 600, 2825, 466, 577, 291, 393, 519, 295, 8141, 27290, 382, 1940, 257, 8213, 6562, 295, 264, 13766, 11, 293, 300, 311, 437, 390, 516, 322, 365, 733, 295, 264, 3380, 5143, 13], "temperature": 0.0, "avg_logprob": -0.13307562852517152, "compression_ratio": 1.5772727272727274, "no_speech_prob": 0.0001970603334484622}, {"id": 373, "seek": 295700, "start": 2970.0, "end": 2984.0, "text": " Sorry, with the set of Jennifer's basis, and then we're taking coefficients to translate into that. Were there any questions about the video?", "tokens": [4919, 11, 365, 264, 992, 295, 14351, 311, 5143, 11, 293, 550, 321, 434, 1940, 31994, 281, 13799, 666, 300, 13, 12448, 456, 604, 1651, 466, 264, 960, 30], "temperature": 0.0, "avg_logprob": -0.13307562852517152, "compression_ratio": 1.5772727272727274, "no_speech_prob": 0.0001970603334484622}, {"id": 374, "seek": 298400, "start": 2984.0, "end": 2997.0, "text": " Okay. Yeah, so I wanted to highlight, so this is coming back. We had this statement, all matrices are diagonal if you use change of basis.", "tokens": [1033, 13, 865, 11, 370, 286, 1415, 281, 5078, 11, 370, 341, 307, 1348, 646, 13, 492, 632, 341, 5629, 11, 439, 32284, 366, 21539, 498, 291, 764, 1319, 295, 5143, 13], "temperature": 0.0, "avg_logprob": -0.13571086487212738, "compression_ratio": 1.544502617801047, "no_speech_prob": 6.604524241993204e-05}, {"id": 375, "seek": 298400, "start": 2997.0, "end": 3011.0, "text": " And so that's kind of what's going on with this U sigma V. We can think of that as a change of basis to get into a space where you've got a diagonal matrix.", "tokens": [400, 370, 300, 311, 733, 295, 437, 311, 516, 322, 365, 341, 624, 12771, 691, 13, 492, 393, 519, 295, 300, 382, 257, 1319, 295, 5143, 281, 483, 666, 257, 1901, 689, 291, 600, 658, 257, 21539, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13571086487212738, "compression_ratio": 1.544502617801047, "no_speech_prob": 6.604524241993204e-05}, {"id": 376, "seek": 301100, "start": 3011.0, "end": 3016.0, "text": " And this, I regret that I didn't put a picture of this in here. I may be able to do this next time.", "tokens": [400, 341, 11, 286, 10879, 300, 286, 994, 380, 829, 257, 3036, 295, 341, 294, 510, 13, 286, 815, 312, 1075, 281, 360, 341, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.115197674611981, "compression_ratio": 1.5865384615384615, "no_speech_prob": 4.006092422059737e-05}, {"id": 377, "seek": 301100, "start": 3016.0, "end": 3024.0, "text": " A lot of times with PCA, this shows up. You'll kind of have the picture of, you know, like what the principal axes are.", "tokens": [316, 688, 295, 1413, 365, 6465, 32, 11, 341, 3110, 493, 13, 509, 603, 733, 295, 362, 264, 3036, 295, 11, 291, 458, 11, 411, 437, 264, 9716, 35387, 366, 13], "temperature": 0.0, "avg_logprob": -0.115197674611981, "compression_ratio": 1.5865384615384615, "no_speech_prob": 4.006092422059737e-05}, {"id": 378, "seek": 301100, "start": 3024.0, "end": 3033.0, "text": " And you can think of that as kind of the basis vectors of this new basis that better represents your data set.", "tokens": [400, 291, 393, 519, 295, 300, 382, 733, 295, 264, 5143, 18875, 295, 341, 777, 5143, 300, 1101, 8855, 428, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.115197674611981, "compression_ratio": 1.5865384615384615, "no_speech_prob": 4.006092422059737e-05}, {"id": 379, "seek": 303300, "start": 3033.0, "end": 3047.0, "text": " Yeah, so we've been talking about SVD in terms of matrices. I'm not sure if this is going. But we can also think of it in terms of the individual vectors.", "tokens": [865, 11, 370, 321, 600, 668, 1417, 466, 31910, 35, 294, 2115, 295, 32284, 13, 286, 478, 406, 988, 498, 341, 307, 516, 13, 583, 321, 393, 611, 519, 295, 309, 294, 2115, 295, 264, 2609, 18875, 13], "temperature": 0.0, "avg_logprob": -0.12135069710867745, "compression_ratio": 1.543956043956044, "no_speech_prob": 5.507479727384634e-06}, {"id": 380, "seek": 303300, "start": 3047.0, "end": 3060.0, "text": " And so if we think of SVD as giving us the vectors VJ and UJ, then we could have A times vector V equals sigma times vector U.", "tokens": [400, 370, 498, 321, 519, 295, 31910, 35, 382, 2902, 505, 264, 18875, 691, 41, 293, 624, 41, 11, 550, 321, 727, 362, 316, 1413, 8062, 691, 6915, 12771, 1413, 8062, 624, 13], "temperature": 0.0, "avg_logprob": -0.12135069710867745, "compression_ratio": 1.543956043956044, "no_speech_prob": 5.507479727384634e-06}, {"id": 381, "seek": 306000, "start": 3060.0, "end": 3068.0, "text": " And I kind of am showing the answers here. But my question was going to be, does this remind you of anything?", "tokens": [400, 286, 733, 295, 669, 4099, 264, 6338, 510, 13, 583, 452, 1168, 390, 516, 281, 312, 11, 775, 341, 4160, 291, 295, 1340, 30], "temperature": 0.0, "avg_logprob": -0.1427764516127737, "compression_ratio": 1.2982456140350878, "no_speech_prob": 1.9524924937286414e-05}, {"id": 382, "seek": 306000, "start": 3068.0, "end": 3074.0, "text": " And the answer is eigen decomposition.", "tokens": [400, 264, 1867, 307, 10446, 48356, 13], "temperature": 0.0, "avg_logprob": -0.1427764516127737, "compression_ratio": 1.2982456140350878, "no_speech_prob": 1.9524924937286414e-05}, {"id": 383, "seek": 307400, "start": 3074.0, "end": 3092.0, "text": " And just to remind you, if it's been a while since you've seen eigen decomposition, I define it here.", "tokens": [400, 445, 281, 4160, 291, 11, 498, 309, 311, 668, 257, 1339, 1670, 291, 600, 1612, 10446, 48356, 11, 286, 6964, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.13059451750346593, "compression_ratio": 1.1222222222222222, "no_speech_prob": 3.1874546948529314e-06}, {"id": 384, "seek": 309200, "start": 3092.0, "end": 3105.0, "text": " Does anyone remember what the definition of an eigenvector, an eigenvalue is? Brad and can Roger throw the microphone?", "tokens": [4402, 2878, 1604, 437, 264, 7123, 295, 364, 10446, 303, 1672, 11, 364, 10446, 29155, 307, 30, 11895, 293, 393, 17666, 3507, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.17768169271534887, "compression_ratio": 1.1683168316831682, "no_speech_prob": 2.0142560970271006e-05}, {"id": 385, "seek": 310500, "start": 3105.0, "end": 3122.0, "text": " For a given matrix A, an eigenvector is a vector that when transformed by A is just a scaled, that vector is a scaled by a value lambda, which is the eigenvalue.", "tokens": [1171, 257, 2212, 8141, 316, 11, 364, 10446, 303, 1672, 307, 257, 8062, 300, 562, 16894, 538, 316, 307, 445, 257, 36039, 11, 300, 8062, 307, 257, 36039, 538, 257, 2158, 13607, 11, 597, 307, 264, 10446, 29155, 13], "temperature": 0.0, "avg_logprob": -0.15371462356212529, "compression_ratio": 1.412280701754386, "no_speech_prob": 4.784921657119412e-06}, {"id": 386, "seek": 312200, "start": 3122.0, "end": 3139.0, "text": " Exactly. Yeah. So transforming V by A, which is like doing A times V is just scaling V. So it's equal to lambda times V. And so that's the definition of eigenvectors and eigenvalues.", "tokens": [7587, 13, 865, 13, 407, 27210, 691, 538, 316, 11, 597, 307, 411, 884, 316, 1413, 691, 307, 445, 21589, 691, 13, 407, 309, 311, 2681, 281, 13607, 1413, 691, 13, 400, 370, 300, 311, 264, 7123, 295, 10446, 303, 5547, 293, 10446, 46033, 13], "temperature": 0.0, "avg_logprob": -0.07158835293495491, "compression_ratio": 1.5076923076923077, "no_speech_prob": 5.955055712547619e-06}, {"id": 387, "seek": 312200, "start": 3139.0, "end": 3149.0, "text": " And so hopefully this looks somewhat similar to what we were talking about with A times V equals sigma times U.", "tokens": [400, 370, 4696, 341, 1542, 8344, 2531, 281, 437, 321, 645, 1417, 466, 365, 316, 1413, 691, 6915, 12771, 1413, 624, 13], "temperature": 0.0, "avg_logprob": -0.07158835293495491, "compression_ratio": 1.5076923076923077, "no_speech_prob": 5.955055712547619e-06}, {"id": 388, "seek": 314900, "start": 3149.0, "end": 3157.0, "text": " Kind of the key difference here is that V and U don't have to be equal. So those could be different values.", "tokens": [9242, 295, 264, 2141, 2649, 510, 307, 300, 691, 293, 624, 500, 380, 362, 281, 312, 2681, 13, 407, 729, 727, 312, 819, 4190, 13], "temperature": 0.0, "avg_logprob": -0.07778110870948204, "compression_ratio": 1.3782051282051282, "no_speech_prob": 4.495116627367679e-06}, {"id": 389, "seek": 314900, "start": 3157.0, "end": 3171.0, "text": " But otherwise, we're talking about kind of when is matrix multiplication by a vector like scaling a vector?", "tokens": [583, 5911, 11, 321, 434, 1417, 466, 733, 295, 562, 307, 8141, 27290, 538, 257, 8062, 411, 21589, 257, 8062, 30], "temperature": 0.0, "avg_logprob": -0.07778110870948204, "compression_ratio": 1.3782051282051282, "no_speech_prob": 4.495116627367679e-06}, {"id": 390, "seek": 317100, "start": 3171.0, "end": 3182.0, "text": " So going back here, and I want to go into great detail about it, but SVD is a generalization of eigendecompositions.", "tokens": [407, 516, 646, 510, 11, 293, 286, 528, 281, 352, 666, 869, 2607, 466, 309, 11, 457, 31910, 35, 307, 257, 2674, 2144, 295, 10446, 1479, 21541, 329, 2451, 13], "temperature": 0.0, "avg_logprob": -0.07495670914649963, "compression_ratio": 1.5692307692307692, "no_speech_prob": 6.74777493259171e-06}, {"id": 391, "seek": 317100, "start": 3182.0, "end": 3188.0, "text": " So not all matrices have eigenvalues, but all matrices have singular values.", "tokens": [407, 406, 439, 32284, 362, 10446, 46033, 11, 457, 439, 32284, 362, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.07495670914649963, "compression_ratio": 1.5692307692307692, "no_speech_prob": 6.74777493259171e-06}, {"id": 392, "seek": 317100, "start": 3188.0, "end": 3195.0, "text": " And we could probably guess that SVD was more general by the fact that U and V don't have to be the same vector.", "tokens": [400, 321, 727, 1391, 2041, 300, 31910, 35, 390, 544, 2674, 538, 264, 1186, 300, 624, 293, 691, 500, 380, 362, 281, 312, 264, 912, 8062, 13], "temperature": 0.0, "avg_logprob": -0.07495670914649963, "compression_ratio": 1.5692307692307692, "no_speech_prob": 6.74777493259171e-06}, {"id": 393, "seek": 319500, "start": 3195.0, "end": 3208.0, "text": " So we're going to switch from talking about SVD to talking about how to find the eigenvalues, which is a kind of very closely related problem.", "tokens": [407, 321, 434, 516, 281, 3679, 490, 1417, 466, 31910, 35, 281, 1417, 466, 577, 281, 915, 264, 10446, 46033, 11, 597, 307, 257, 733, 295, 588, 8185, 4077, 1154, 13], "temperature": 0.0, "avg_logprob": -0.043679422453830115, "compression_ratio": 1.5270935960591132, "no_speech_prob": 5.955015694780741e-06}, {"id": 394, "seek": 319500, "start": 3208.0, "end": 3214.0, "text": " So everything we say about eigendecomposition, remember that it's relevant to SVD.", "tokens": [407, 1203, 321, 584, 466, 10446, 1479, 21541, 5830, 11, 1604, 300, 309, 311, 7340, 281, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.043679422453830115, "compression_ratio": 1.5270935960591132, "no_speech_prob": 5.955015694780741e-06}, {"id": 395, "seek": 319500, "start": 3214.0, "end": 3219.0, "text": " And we've just seen this list of applications of SVD kind of throughout this course.", "tokens": [400, 321, 600, 445, 1612, 341, 1329, 295, 5821, 295, 31910, 35, 733, 295, 3710, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.043679422453830115, "compression_ratio": 1.5270935960591132, "no_speech_prob": 5.955015694780741e-06}, {"id": 396, "seek": 321900, "start": 3219.0, "end": 3228.0, "text": " And here I link to several other kind of resources for more information about SVD.", "tokens": [400, 510, 286, 2113, 281, 2940, 661, 733, 295, 3593, 337, 544, 1589, 466, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.07335136306117958, "compression_ratio": 1.6777777777777778, "no_speech_prob": 5.422111371444771e-06}, {"id": 397, "seek": 321900, "start": 3228.0, "end": 3239.0, "text": " So the best classical methods for computing the SVD are all variants on methods for computing eigenvalues.", "tokens": [407, 264, 1151, 13735, 7150, 337, 15866, 264, 31910, 35, 366, 439, 21669, 322, 7150, 337, 15866, 10446, 46033, 13], "temperature": 0.0, "avg_logprob": -0.07335136306117958, "compression_ratio": 1.6777777777777778, "no_speech_prob": 5.422111371444771e-06}, {"id": 398, "seek": 321900, "start": 3239.0, "end": 3247.0, "text": " And eigendecompositions are useful on their own as well. So a few practical applications of eigendecompositions.", "tokens": [400, 10446, 1479, 21541, 329, 2451, 366, 4420, 322, 641, 1065, 382, 731, 13, 407, 257, 1326, 8496, 5821, 295, 10446, 1479, 21541, 329, 2451, 13], "temperature": 0.0, "avg_logprob": -0.07335136306117958, "compression_ratio": 1.6777777777777778, "no_speech_prob": 5.422111371444771e-06}, {"id": 399, "seek": 324700, "start": 3247.0, "end": 3260.0, "text": " One is rapid matrix powers.", "tokens": [1485, 307, 7558, 8141, 8674, 13], "temperature": 0.0, "avg_logprob": -0.26245965957641604, "compression_ratio": 0.7714285714285715, "no_speech_prob": 2.7107673304271884e-05}, {"id": 400, "seek": 326000, "start": 3260.0, "end": 3281.0, "text": " And so the idea is that if you knew that A was equal to V times lambda times V inverse, here V is the eigenvectors, lambda is the eigenvalues.", "tokens": [400, 370, 264, 1558, 307, 300, 498, 291, 2586, 300, 316, 390, 2681, 281, 691, 1413, 13607, 1413, 691, 17340, 11, 510, 691, 307, 264, 10446, 303, 5547, 11, 13607, 307, 264, 10446, 46033, 13], "temperature": 0.0, "avg_logprob": -0.10974694520999224, "compression_ratio": 1.3523809523809525, "no_speech_prob": 3.668735189421568e-06}, {"id": 401, "seek": 328100, "start": 3281.0, "end": 3298.0, "text": " What would be true of lambda as a matrix? What property does this matrix have?", "tokens": [708, 576, 312, 2074, 295, 13607, 382, 257, 8141, 30, 708, 4707, 775, 341, 8141, 362, 30], "temperature": 0.0, "avg_logprob": -0.17647479562198415, "compression_ratio": 1.3969465648854962, "no_speech_prob": 2.5610604552639415e-06}, {"id": 402, "seek": 328100, "start": 3298.0, "end": 3303.0, "text": " Do you want to grab the microphone?", "tokens": [1144, 291, 528, 281, 4444, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.17647479562198415, "compression_ratio": 1.3969465648854962, "no_speech_prob": 2.5610604552639415e-06}, {"id": 403, "seek": 328100, "start": 3303.0, "end": 3305.0, "text": " Lambda has to be diagonal.", "tokens": [45691, 575, 281, 312, 21539, 13], "temperature": 0.0, "avg_logprob": -0.17647479562198415, "compression_ratio": 1.3969465648854962, "no_speech_prob": 2.5610604552639415e-06}, {"id": 404, "seek": 328100, "start": 3305.0, "end": 3308.0, "text": " Exactly. Yeah, lambda has to be diagonal.", "tokens": [7587, 13, 865, 11, 13607, 575, 281, 312, 21539, 13], "temperature": 0.0, "avg_logprob": -0.17647479562198415, "compression_ratio": 1.3969465648854962, "no_speech_prob": 2.5610604552639415e-06}, {"id": 405, "seek": 330800, "start": 3308.0, "end": 3321.0, "text": " And that's kind of this idea of going back and forth between, you know, we can write eigenvectors and eigenvalues individually as vectors, you know, kind of what I had here.", "tokens": [400, 300, 311, 733, 295, 341, 1558, 295, 516, 646, 293, 5220, 1296, 11, 291, 458, 11, 321, 393, 2464, 10446, 303, 5547, 293, 10446, 46033, 16652, 382, 18875, 11, 291, 458, 11, 733, 295, 437, 286, 632, 510, 13], "temperature": 0.0, "avg_logprob": -0.11891746520996094, "compression_ratio": 1.6715686274509804, "no_speech_prob": 2.6273060939274728e-05}, {"id": 406, "seek": 330800, "start": 3321.0, "end": 3337.0, "text": " Or we could kind of put all the eigenvectors together and say, really, this is A times a matrix V, where the columns are the different eigenvectors equals, oh, except.", "tokens": [1610, 321, 727, 733, 295, 829, 439, 264, 10446, 303, 5547, 1214, 293, 584, 11, 534, 11, 341, 307, 316, 1413, 257, 8141, 691, 11, 689, 264, 13766, 366, 264, 819, 10446, 303, 5547, 6915, 11, 1954, 11, 3993, 13], "temperature": 0.0, "avg_logprob": -0.11891746520996094, "compression_ratio": 1.6715686274509804, "no_speech_prob": 2.6273060939274728e-05}, {"id": 407, "seek": 333700, "start": 3337.0, "end": 3342.0, "text": " To make that work, you have to make lambda into a matrix.", "tokens": [1407, 652, 300, 589, 11, 291, 362, 281, 652, 13607, 666, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11391083399454753, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.3630750800075475e-05}, {"id": 408, "seek": 333700, "start": 3342.0, "end": 3350.0, "text": " And what that matrix is, is just diagonal with the little lambdas along the diagonal.", "tokens": [400, 437, 300, 8141, 307, 11, 307, 445, 21539, 365, 264, 707, 10097, 27476, 2051, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.11391083399454753, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.3630750800075475e-05}, {"id": 409, "seek": 333700, "start": 3350.0, "end": 3355.0, "text": " Questions about getting between these? And so here this would often these would have a subscript.", "tokens": [27738, 466, 1242, 1296, 613, 30, 400, 370, 510, 341, 576, 2049, 613, 576, 362, 257, 2325, 662, 13], "temperature": 0.0, "avg_logprob": -0.11391083399454753, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.3630750800075475e-05}, {"id": 410, "seek": 333700, "start": 3355.0, "end": 3360.0, "text": " So if I say these are all sub i, then we're putting those together.", "tokens": [407, 498, 286, 584, 613, 366, 439, 1422, 741, 11, 550, 321, 434, 3372, 729, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11391083399454753, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.3630750800075475e-05}, {"id": 411, "seek": 336000, "start": 3360.0, "end": 3367.0, "text": " Actually, let me write it out. I'm going to say everyone's been very quiet today, so it's hard for me to read.", "tokens": [5135, 11, 718, 385, 2464, 309, 484, 13, 286, 478, 516, 281, 584, 1518, 311, 668, 588, 5677, 965, 11, 370, 309, 311, 1152, 337, 385, 281, 1401, 13], "temperature": 0.0, "avg_logprob": -0.1951378928290473, "compression_ratio": 1.2436974789915967, "no_speech_prob": 7.527783509431174e-06}, {"id": 412, "seek": 336000, "start": 3367.0, "end": 3371.0, "text": " So she want me to go slower, quicker.", "tokens": [407, 750, 528, 385, 281, 352, 14009, 11, 16255, 13], "temperature": 0.0, "avg_logprob": -0.1951378928290473, "compression_ratio": 1.2436974789915967, "no_speech_prob": 7.527783509431174e-06}, {"id": 413, "seek": 337100, "start": 3371.0, "end": 3398.0, "text": " But you could think of this as A times V1, V2, V3, and so on equals lambda 1, lambda 2, one down, slide over, down to lambda n.", "tokens": [583, 291, 727, 519, 295, 341, 382, 316, 1413, 691, 16, 11, 691, 17, 11, 691, 18, 11, 293, 370, 322, 6915, 13607, 502, 11, 13607, 568, 11, 472, 760, 11, 4137, 670, 11, 760, 281, 13607, 297, 13], "temperature": 0.0, "avg_logprob": -0.29130478792412334, "compression_ratio": 1.2211538461538463, "no_speech_prob": 3.96690666093491e-06}, {"id": 414, "seek": 339800, "start": 3398.0, "end": 3405.0, "text": " So this is V1, V2, up to Vn.", "tokens": [407, 341, 307, 691, 16, 11, 691, 17, 11, 493, 281, 691, 77, 13], "temperature": 0.0, "avg_logprob": -0.1748003323872884, "compression_ratio": 1.3728813559322033, "no_speech_prob": 6.540315098391147e-06}, {"id": 415, "seek": 339800, "start": 3405.0, "end": 3417.0, "text": " So this is kind of a way to consolidate all those individual equations for the different Vi and the different lambda i into matrices.", "tokens": [407, 341, 307, 733, 295, 257, 636, 281, 49521, 439, 729, 2609, 11787, 337, 264, 819, 6626, 293, 264, 819, 13607, 741, 666, 32284, 13], "temperature": 0.0, "avg_logprob": -0.1748003323872884, "compression_ratio": 1.3728813559322033, "no_speech_prob": 6.540315098391147e-06}, {"id": 416, "seek": 341700, "start": 3417.0, "end": 3430.0, "text": " And then the idea with the rapid matrix powers is, you know, suppose we're interested in taking A to some power k.", "tokens": [400, 550, 264, 1558, 365, 264, 7558, 8141, 8674, 307, 11, 291, 458, 11, 7297, 321, 434, 3102, 294, 1940, 316, 281, 512, 1347, 350, 13], "temperature": 0.0, "avg_logprob": -0.09621666978906702, "compression_ratio": 1.5289855072463767, "no_speech_prob": 8.397672900173347e-06}, {"id": 417, "seek": 341700, "start": 3430.0, "end": 3436.0, "text": " Doing that the naive way would be doing A times A, multiplying that by A, multiplying that by A.", "tokens": [18496, 300, 264, 29052, 636, 576, 312, 884, 316, 1413, 316, 11, 30955, 300, 538, 316, 11, 30955, 300, 538, 316, 13], "temperature": 0.0, "avg_logprob": -0.09621666978906702, "compression_ratio": 1.5289855072463767, "no_speech_prob": 8.397672900173347e-06}, {"id": 418, "seek": 343600, "start": 3436.0, "end": 3449.0, "text": " And what is what's the runtime for matrix multiplication?", "tokens": [400, 437, 307, 437, 311, 264, 34474, 337, 8141, 27290, 30], "temperature": 0.0, "avg_logprob": -0.17018281329761853, "compression_ratio": 1.2142857142857142, "no_speech_prob": 2.769336788333021e-06}, {"id": 419, "seek": 343600, "start": 3449.0, "end": 3454.0, "text": " Tim, you want to throw the microphone to Sam?", "tokens": [7172, 11, 291, 528, 281, 3507, 264, 10952, 281, 4832, 30], "temperature": 0.0, "avg_logprob": -0.17018281329761853, "compression_ratio": 1.2142857142857142, "no_speech_prob": 2.769336788333021e-06}, {"id": 420, "seek": 343600, "start": 3454.0, "end": 3458.0, "text": " Yes, exactly. It's n cubed, which is really slow.", "tokens": [1079, 11, 2293, 13, 467, 311, 297, 36510, 11, 597, 307, 534, 2964, 13], "temperature": 0.0, "avg_logprob": -0.17018281329761853, "compression_ratio": 1.2142857142857142, "no_speech_prob": 2.769336788333021e-06}, {"id": 421, "seek": 345800, "start": 3458.0, "end": 3472.0, "text": " So another way we could think about this. So now we've got A equals.", "tokens": [407, 1071, 636, 321, 727, 519, 466, 341, 13, 407, 586, 321, 600, 658, 316, 6915, 13], "temperature": 0.0, "avg_logprob": -0.08167927018527327, "compression_ratio": 1.054945054945055, "no_speech_prob": 6.74757984597818e-06}, {"id": 422, "seek": 345800, "start": 3472.0, "end": 3479.0, "text": " Did I write this backwards?", "tokens": [2589, 286, 2464, 341, 12204, 30], "temperature": 0.0, "avg_logprob": -0.08167927018527327, "compression_ratio": 1.054945054945055, "no_speech_prob": 6.74757984597818e-06}, {"id": 423, "seek": 347900, "start": 3479.0, "end": 3489.0, "text": " I'm sorry about this.", "tokens": [286, 478, 2597, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.20338598374397523, "compression_ratio": 1.0337078651685394, "no_speech_prob": 9.97264669422293e-06}, {"id": 424, "seek": 347900, "start": 3489.0, "end": 3503.0, "text": " Av and this side is V lambda. Yeah, I think that's correct. Thank you.", "tokens": [11667, 293, 341, 1252, 307, 691, 13607, 13, 865, 11, 286, 519, 300, 311, 3006, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.20338598374397523, "compression_ratio": 1.0337078651685394, "no_speech_prob": 9.97264669422293e-06}, {"id": 425, "seek": 350300, "start": 3503.0, "end": 3514.0, "text": " Yeah, and the reason why that's true is because we want to be taking multiples of.", "tokens": [865, 11, 293, 264, 1778, 983, 300, 311, 2074, 307, 570, 321, 528, 281, 312, 1940, 46099, 295, 13], "temperature": 0.0, "avg_logprob": -0.12997076822363812, "compression_ratio": 1.2926829268292683, "no_speech_prob": 7.646320227649994e-06}, {"id": 426, "seek": 350300, "start": 3514.0, "end": 3519.0, "text": " Let me write this out.", "tokens": [961, 385, 2464, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.12997076822363812, "compression_ratio": 1.2926829268292683, "no_speech_prob": 7.646320227649994e-06}, {"id": 427, "seek": 350300, "start": 3519.0, "end": 3532.0, "text": " Yeah, we're interested in the columns V on this side.", "tokens": [865, 11, 321, 434, 3102, 294, 264, 13766, 691, 322, 341, 1252, 13], "temperature": 0.0, "avg_logprob": -0.12997076822363812, "compression_ratio": 1.2926829268292683, "no_speech_prob": 7.646320227649994e-06}, {"id": 428, "seek": 353200, "start": 3532.0, "end": 3541.0, "text": " And so writing it like this.", "tokens": [400, 370, 3579, 309, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11780890396663121, "compression_ratio": 1.364864864864865, "no_speech_prob": 1.078291916201124e-05}, {"id": 429, "seek": 353200, "start": 3541.0, "end": 3549.0, "text": " What this gives us is basically taking the first column, multiplying that by our matrix V is just going to give us lambda 1 V1.", "tokens": [708, 341, 2709, 505, 307, 1936, 1940, 264, 700, 7738, 11, 30955, 300, 538, 527, 8141, 691, 307, 445, 516, 281, 976, 505, 13607, 502, 691, 16, 13], "temperature": 0.0, "avg_logprob": -0.11780890396663121, "compression_ratio": 1.364864864864865, "no_speech_prob": 1.078291916201124e-05}, {"id": 430, "seek": 353200, "start": 3549.0, "end": 3556.0, "text": " Then we're doing lambda 2 times V2 and so on.", "tokens": [1396, 321, 434, 884, 13607, 568, 1413, 691, 17, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.11780890396663121, "compression_ratio": 1.364864864864865, "no_speech_prob": 1.078291916201124e-05}, {"id": 431, "seek": 355600, "start": 3556.0, "end": 3565.0, "text": " Sorry about that confusion. This should be Av times V lambda.", "tokens": [4919, 466, 300, 15075, 13, 639, 820, 312, 11667, 1413, 691, 13607, 13], "temperature": 0.0, "avg_logprob": -0.18530695366136957, "compression_ratio": 1.1862745098039216, "no_speech_prob": 3.966928943555104e-06}, {"id": 432, "seek": 355600, "start": 3565.0, "end": 3575.0, "text": " Which then can be rewritten as A equals V lambda V inverse.", "tokens": [3013, 550, 393, 312, 319, 26859, 382, 316, 6915, 691, 13607, 691, 17340, 13], "temperature": 0.0, "avg_logprob": -0.18530695366136957, "compression_ratio": 1.1862745098039216, "no_speech_prob": 3.966928943555104e-06}, {"id": 433, "seek": 357500, "start": 3575.0, "end": 3587.0, "text": " So then going back to our problem of wanting the kth power, we can do A equals V lambda V inverse.", "tokens": [407, 550, 516, 646, 281, 527, 1154, 295, 7935, 264, 350, 392, 1347, 11, 321, 393, 360, 316, 6915, 691, 13607, 691, 17340, 13], "temperature": 0.0, "avg_logprob": -0.15859789198095148, "compression_ratio": 1.282258064516129, "no_speech_prob": 3.0415239962167107e-06}, {"id": 434, "seek": 357500, "start": 3587.0, "end": 3599.0, "text": " The kth power and any ideas about how this could be reduced?", "tokens": [440, 350, 392, 1347, 293, 604, 3487, 466, 577, 341, 727, 312, 9212, 30], "temperature": 0.0, "avg_logprob": -0.15859789198095148, "compression_ratio": 1.282258064516129, "no_speech_prob": 3.0415239962167107e-06}, {"id": 435, "seek": 359900, "start": 3599.0, "end": 3611.0, "text": " Tim and Sam, can you throw the microphone back to Tim?", "tokens": [7172, 293, 4832, 11, 393, 291, 3507, 264, 10952, 646, 281, 7172, 30], "temperature": 0.0, "avg_logprob": -0.20654728299095518, "compression_ratio": 0.9130434782608695, "no_speech_prob": 7.030629058135673e-05}, {"id": 436, "seek": 359900, "start": 3611.0, "end": 3614.0, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.20654728299095518, "compression_ratio": 0.9130434782608695, "no_speech_prob": 7.030629058135673e-05}, {"id": 437, "seek": 361400, "start": 3614.0, "end": 3629.0, "text": " Yeah, so if you were to write these out a bunch of times and as Tim said, you end up with V lambda V inverse times V lambda V inverse times V lambda and so on.", "tokens": [865, 11, 370, 498, 291, 645, 281, 2464, 613, 484, 257, 3840, 295, 1413, 293, 382, 7172, 848, 11, 291, 917, 493, 365, 691, 13607, 691, 17340, 1413, 691, 13607, 691, 17340, 1413, 691, 13607, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.13054236243752873, "compression_ratio": 1.9069767441860466, "no_speech_prob": 3.2189789635594934e-05}, {"id": 438, "seek": 361400, "start": 3629.0, "end": 3635.0, "text": " And this V inverse times V and V inverse times V are going to cancel out.", "tokens": [400, 341, 691, 17340, 1413, 691, 293, 691, 17340, 1413, 691, 366, 516, 281, 10373, 484, 13], "temperature": 0.0, "avg_logprob": -0.13054236243752873, "compression_ratio": 1.9069767441860466, "no_speech_prob": 3.2189789635594934e-05}, {"id": 439, "seek": 361400, "start": 3635.0, "end": 3643.0, "text": " However many, however many entries you have and you end up with the lambda to the k V inverse.", "tokens": [2908, 867, 11, 4461, 867, 23041, 291, 362, 293, 291, 917, 493, 365, 264, 13607, 281, 264, 350, 691, 17340, 13], "temperature": 0.0, "avg_logprob": -0.13054236243752873, "compression_ratio": 1.9069767441860466, "no_speech_prob": 3.2189789635594934e-05}, {"id": 440, "seek": 364300, "start": 3643.0, "end": 3650.0, "text": " And why is it okay that we have lambda to the k?", "tokens": [400, 983, 307, 309, 1392, 300, 321, 362, 13607, 281, 264, 350, 30], "temperature": 0.0, "avg_logprob": -0.1613305496842894, "compression_ratio": 1.453416149068323, "no_speech_prob": 1.0288958947057836e-05}, {"id": 441, "seek": 364300, "start": 3650.0, "end": 3656.0, "text": " Matthew? Oh, and do you want to throw the?", "tokens": [12434, 30, 876, 11, 293, 360, 291, 528, 281, 3507, 264, 30], "temperature": 0.0, "avg_logprob": -0.1613305496842894, "compression_ratio": 1.453416149068323, "no_speech_prob": 1.0288958947057836e-05}, {"id": 442, "seek": 364300, "start": 3656.0, "end": 3659.0, "text": " It's diagonal so it's really easy to take the kth power.", "tokens": [467, 311, 21539, 370, 309, 311, 534, 1858, 281, 747, 264, 350, 392, 1347, 13], "temperature": 0.0, "avg_logprob": -0.1613305496842894, "compression_ratio": 1.453416149068323, "no_speech_prob": 1.0288958947057836e-05}, {"id": 443, "seek": 364300, "start": 3659.0, "end": 3664.0, "text": " Exactly. Yeah, so since it's diagonal, taking the kth power is not a big deal at all.", "tokens": [7587, 13, 865, 11, 370, 1670, 309, 311, 21539, 11, 1940, 264, 350, 392, 1347, 307, 406, 257, 955, 2028, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1613305496842894, "compression_ratio": 1.453416149068323, "no_speech_prob": 1.0288958947057836e-05}, {"id": 444, "seek": 366400, "start": 3664.0, "end": 3673.0, "text": " So this is a much more efficient way to, sorry there should be a kth power here, to compute the kth power of A.", "tokens": [407, 341, 307, 257, 709, 544, 7148, 636, 281, 11, 2597, 456, 820, 312, 257, 350, 392, 1347, 510, 11, 281, 14722, 264, 350, 392, 1347, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.105787431492525, "compression_ratio": 1.4171779141104295, "no_speech_prob": 4.425414772413205e-06}, {"id": 445, "seek": 366400, "start": 3673.0, "end": 3679.0, "text": " This is one application of eigenvectors.", "tokens": [639, 307, 472, 3861, 295, 10446, 303, 5547, 13], "temperature": 0.0, "avg_logprob": -0.105787431492525, "compression_ratio": 1.4171779141104295, "no_speech_prob": 4.425414772413205e-06}, {"id": 446, "seek": 366400, "start": 3679.0, "end": 3688.0, "text": " You can also use them to find the nth Fibonacci number, which is kind of neat.", "tokens": [509, 393, 611, 764, 552, 281, 915, 264, 297, 392, 479, 897, 266, 43870, 1230, 11, 597, 307, 733, 295, 10654, 13], "temperature": 0.0, "avg_logprob": -0.105787431492525, "compression_ratio": 1.4171779141104295, "no_speech_prob": 4.425414772413205e-06}, {"id": 447, "seek": 368800, "start": 3688.0, "end": 3694.0, "text": " I'll just briefly show this one.", "tokens": [286, 603, 445, 10515, 855, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.13050418676331985, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.092835752089741e-06}, {"id": 448, "seek": 368800, "start": 3694.0, "end": 3710.0, "text": " But basically you can think of getting the Fibonacci numbers as going, taking this matrix 1 1 and 1 0 and multiplying that by 1 0 and then kind of continuing to do that to get the,", "tokens": [583, 1936, 291, 393, 519, 295, 1242, 264, 479, 897, 266, 43870, 3547, 382, 516, 11, 1940, 341, 8141, 502, 502, 293, 502, 1958, 293, 30955, 300, 538, 502, 1958, 293, 550, 733, 295, 9289, 281, 360, 300, 281, 483, 264, 11], "temperature": 0.0, "avg_logprob": -0.13050418676331985, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.092835752089741e-06}, {"id": 449, "seek": 368800, "start": 3710.0, "end": 3717.0, "text": " you know, this is giving you on the top row the sum of the previous two things you have if you enter the previous two here.", "tokens": [291, 458, 11, 341, 307, 2902, 291, 322, 264, 1192, 5386, 264, 2408, 295, 264, 3894, 732, 721, 291, 362, 498, 291, 3242, 264, 3894, 732, 510, 13], "temperature": 0.0, "avg_logprob": -0.13050418676331985, "compression_ratio": 1.6280193236714975, "no_speech_prob": 4.092835752089741e-06}, {"id": 450, "seek": 371700, "start": 3717.0, "end": 3721.0, "text": " And then apply what we just saw with taking nth powers.", "tokens": [400, 550, 3079, 437, 321, 445, 1866, 365, 1940, 297, 392, 8674, 13], "temperature": 0.0, "avg_logprob": -0.1019988757808034, "compression_ratio": 1.59375, "no_speech_prob": 2.5215356345142936e-06}, {"id": 451, "seek": 371700, "start": 3721.0, "end": 3734.0, "text": " It's kind of a fun application.", "tokens": [467, 311, 733, 295, 257, 1019, 3861, 13], "temperature": 0.0, "avg_logprob": -0.1019988757808034, "compression_ratio": 1.59375, "no_speech_prob": 2.5215356345142936e-06}, {"id": 452, "seek": 371700, "start": 3734.0, "end": 3738.0, "text": " And then we're not going to get into this in this class, but the behavior of ODEs.", "tokens": [400, 550, 321, 434, 406, 516, 281, 483, 666, 341, 294, 341, 1508, 11, 457, 264, 5223, 295, 48447, 20442, 13], "temperature": 0.0, "avg_logprob": -0.1019988757808034, "compression_ratio": 1.59375, "no_speech_prob": 2.5215356345142936e-06}, {"id": 453, "seek": 371700, "start": 3738.0, "end": 3746.0, "text": " It's often if you're interested in the long term behavior of ODEs, what you'll end up needing to do is finding the eigen decomposition.", "tokens": [467, 311, 2049, 498, 291, 434, 3102, 294, 264, 938, 1433, 5223, 295, 48447, 20442, 11, 437, 291, 603, 917, 493, 18006, 281, 360, 307, 5006, 264, 10446, 48356, 13], "temperature": 0.0, "avg_logprob": -0.1019988757808034, "compression_ratio": 1.59375, "no_speech_prob": 2.5215356345142936e-06}, {"id": 454, "seek": 374600, "start": 3746.0, "end": 3752.0, "text": " And then mark off chains, which we will kind of be seeing today.", "tokens": [400, 550, 1491, 766, 12626, 11, 597, 321, 486, 733, 295, 312, 2577, 965, 13], "temperature": 0.0, "avg_logprob": -0.13639471530914307, "compression_ratio": 1.4834123222748816, "no_speech_prob": 2.6686206183512695e-05}, {"id": 455, "seek": 374600, "start": 3752.0, "end": 3757.0, "text": " Yeah, so we watched the three blue one brown video.", "tokens": [865, 11, 370, 321, 6337, 264, 1045, 3344, 472, 6292, 960, 13], "temperature": 0.0, "avg_logprob": -0.13639471530914307, "compression_ratio": 1.4834123222748816, "no_speech_prob": 2.6686206183512695e-05}, {"id": 456, "seek": 374600, "start": 3757.0, "end": 3765.0, "text": " Gilbert Strang, who's written a kind of classic linear algebra textbook, had a nice quote, eigenvalues are a way to see into the heart of a matrix.", "tokens": [39003, 8251, 656, 11, 567, 311, 3720, 257, 733, 295, 7230, 8213, 21989, 25591, 11, 632, 257, 1481, 6513, 11, 10446, 46033, 366, 257, 636, 281, 536, 666, 264, 1917, 295, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13639471530914307, "compression_ratio": 1.4834123222748816, "no_speech_prob": 2.6686206183512695e-05}, {"id": 457, "seek": 374600, "start": 3765.0, "end": 3769.0, "text": " All the difficulties of matrices are swept away.", "tokens": [1057, 264, 14399, 295, 32284, 366, 31791, 1314, 13], "temperature": 0.0, "avg_logprob": -0.13639471530914307, "compression_ratio": 1.4834123222748816, "no_speech_prob": 2.6686206183512695e-05}, {"id": 458, "seek": 376900, "start": 3769.0, "end": 3778.0, "text": " But there's something, you know, really kind of fundamental about a matrix that's expressed in its eigenvalues.", "tokens": [583, 456, 311, 746, 11, 291, 458, 11, 534, 733, 295, 8088, 466, 257, 8141, 300, 311, 12675, 294, 1080, 10446, 46033, 13], "temperature": 0.0, "avg_logprob": -0.09009363274825247, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.947929715446662e-06}, {"id": 459, "seek": 376900, "start": 3778.0, "end": 3783.0, "text": " And then just a just some vocabulary that you might come across is Hermitian.", "tokens": [400, 550, 445, 257, 445, 512, 19864, 300, 291, 1062, 808, 2108, 307, 21842, 270, 952, 13], "temperature": 0.0, "avg_logprob": -0.09009363274825247, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.947929715446662e-06}, {"id": 460, "seek": 376900, "start": 3783.0, "end": 3787.0, "text": " And that means the matrix is equal to its conjugate transpose.", "tokens": [400, 300, 1355, 264, 8141, 307, 2681, 281, 1080, 45064, 25167, 13], "temperature": 0.0, "avg_logprob": -0.09009363274825247, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.947929715446662e-06}, {"id": 461, "seek": 376900, "start": 3787.0, "end": 3794.0, "text": " In the case where you're dealing with real values, that's just saying that it's the same as being symmetric.", "tokens": [682, 264, 1389, 689, 291, 434, 6260, 365, 957, 4190, 11, 300, 311, 445, 1566, 300, 309, 311, 264, 912, 382, 885, 32330, 13], "temperature": 0.0, "avg_logprob": -0.09009363274825247, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.947929715446662e-06}, {"id": 462, "seek": 376900, "start": 3794.0, "end": 3797.0, "text": " It's equal to its transpose.", "tokens": [467, 311, 2681, 281, 1080, 25167, 13], "temperature": 0.0, "avg_logprob": -0.09009363274825247, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.947929715446662e-06}, {"id": 463, "seek": 379700, "start": 3797.0, "end": 3803.0, "text": " And that's only if you have complex values, then you'd be flipping the sign on the complex values.", "tokens": [400, 300, 311, 787, 498, 291, 362, 3997, 4190, 11, 550, 291, 1116, 312, 26886, 264, 1465, 322, 264, 3997, 4190, 13], "temperature": 0.0, "avg_logprob": -0.18742613358931107, "compression_ratio": 1.568421052631579, "no_speech_prob": 4.289257049094886e-06}, {"id": 464, "seek": 379700, "start": 3803.0, "end": 3811.0, "text": " But so far purposes, when you hear Hermitian, you can think symmetric.", "tokens": [583, 370, 1400, 9932, 11, 562, 291, 1568, 21842, 270, 952, 11, 291, 393, 519, 32330, 13], "temperature": 0.0, "avg_logprob": -0.18742613358931107, "compression_ratio": 1.568421052631579, "no_speech_prob": 4.289257049094886e-06}, {"id": 465, "seek": 379700, "start": 3811.0, "end": 3824.0, "text": " And then to add two useful theorems are if A is symmetric, then the eigenvalues of A are real and A equals Q lambda Q transpose.", "tokens": [400, 550, 281, 909, 732, 4420, 10299, 2592, 366, 498, 316, 307, 32330, 11, 550, 264, 10446, 46033, 295, 316, 366, 957, 293, 316, 6915, 1249, 13607, 1249, 25167, 13], "temperature": 0.0, "avg_logprob": -0.18742613358931107, "compression_ratio": 1.568421052631579, "no_speech_prob": 4.289257049094886e-06}, {"id": 466, "seek": 382400, "start": 3824.0, "end": 3837.0, "text": " So that's really handy that here Q is its inverse is its transpose.", "tokens": [407, 300, 311, 534, 13239, 300, 510, 1249, 307, 1080, 17340, 307, 1080, 25167, 13], "temperature": 0.0, "avg_logprob": -0.07222167138130434, "compression_ratio": 1.5, "no_speech_prob": 7.338182399507787e-07}, {"id": 467, "seek": 382400, "start": 3837.0, "end": 3843.0, "text": " And then if A is triangular, its eigenvalues are equal to its diagonal entries.", "tokens": [400, 550, 498, 316, 307, 38190, 11, 1080, 10446, 46033, 366, 2681, 281, 1080, 21539, 23041, 13], "temperature": 0.0, "avg_logprob": -0.07222167138130434, "compression_ratio": 1.5, "no_speech_prob": 7.338182399507787e-07}, {"id": 468, "seek": 382400, "start": 3843.0, "end": 3849.0, "text": " This is a little bit of a spoiler, but if you are interested in the eigenvalues of a matrix,", "tokens": [639, 307, 257, 707, 857, 295, 257, 26927, 11, 457, 498, 291, 366, 3102, 294, 264, 10446, 46033, 295, 257, 8141, 11], "temperature": 0.0, "avg_logprob": -0.07222167138130434, "compression_ratio": 1.5, "no_speech_prob": 7.338182399507787e-07}, {"id": 469, "seek": 384900, "start": 3849.0, "end": 3860.0, "text": " it would really be nice to have it in triangular form because then you can just get them from the diagonal.", "tokens": [309, 576, 534, 312, 1481, 281, 362, 309, 294, 38190, 1254, 570, 550, 291, 393, 445, 483, 552, 490, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.10861551496717665, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.4738551726622973e-05}, {"id": 470, "seek": 384900, "start": 3860.0, "end": 3866.0, "text": " So today we're going to start with the power method, which finds just one eigenvector.", "tokens": [407, 965, 321, 434, 516, 281, 722, 365, 264, 1347, 3170, 11, 597, 10704, 445, 472, 10446, 303, 1672, 13], "temperature": 0.0, "avg_logprob": -0.10861551496717665, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.4738551726622973e-05}, {"id": 471, "seek": 384900, "start": 3866.0, "end": 3869.0, "text": " And that's the basis for PageRank.", "tokens": [400, 300, 311, 264, 5143, 337, 21217, 49, 657, 13], "temperature": 0.0, "avg_logprob": -0.10861551496717665, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.4738551726622973e-05}, {"id": 472, "seek": 384900, "start": 3869.0, "end": 3878.0, "text": " And there's a paper, I really like the title of this, the $25 billion eigenvector, the linear algebra behind Google.", "tokens": [400, 456, 311, 257, 3035, 11, 286, 534, 411, 264, 4876, 295, 341, 11, 264, 1848, 6074, 5218, 10446, 303, 1672, 11, 264, 8213, 21989, 2261, 3329, 13], "temperature": 0.0, "avg_logprob": -0.10861551496717665, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.4738551726622973e-05}, {"id": 473, "seek": 387800, "start": 3878.0, "end": 3885.0, "text": " So this is a very real world application of an eigenvector being important.", "tokens": [407, 341, 307, 257, 588, 957, 1002, 3861, 295, 364, 10446, 303, 1672, 885, 1021, 13], "temperature": 0.0, "avg_logprob": -0.10507852084016146, "compression_ratio": 1.44, "no_speech_prob": 6.048739578545792e-06}, {"id": 474, "seek": 387800, "start": 3885.0, "end": 3891.0, "text": " And so we're going to be using a data set from DDPedia, which I think is a really neat resource.", "tokens": [400, 370, 321, 434, 516, 281, 312, 1228, 257, 1412, 992, 490, 413, 11373, 14212, 11, 597, 286, 519, 307, 257, 534, 10654, 7684, 13], "temperature": 0.0, "avg_logprob": -0.10507852084016146, "compression_ratio": 1.44, "no_speech_prob": 6.048739578545792e-06}, {"id": 475, "seek": 387800, "start": 3891.0, "end": 3903.0, "text": " But they've compiled a bunch of Wikipedia data, as well as a lot of different classifications and categories of it.", "tokens": [583, 436, 600, 36548, 257, 3840, 295, 28999, 1412, 11, 382, 731, 382, 257, 688, 295, 819, 1508, 7833, 293, 10479, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.10507852084016146, "compression_ratio": 1.44, "no_speech_prob": 6.048739578545792e-06}, {"id": 476, "seek": 390300, "start": 3903.0, "end": 3910.0, "text": " They have it in a bunch of different languages, and it's all kind of freely available.", "tokens": [814, 362, 309, 294, 257, 3840, 295, 819, 8650, 11, 293, 309, 311, 439, 733, 295, 16433, 2435, 13], "temperature": 0.0, "avg_logprob": -0.09565444039826346, "compression_ratio": 1.7468354430379747, "no_speech_prob": 9.665296602179296e-06}, {"id": 477, "seek": 390300, "start": 3910.0, "end": 3916.0, "text": " And so the data set we'll be using is kind of showing which pages link to which other pages.", "tokens": [400, 370, 264, 1412, 992, 321, 603, 312, 1228, 307, 733, 295, 4099, 597, 7183, 2113, 281, 597, 661, 7183, 13], "temperature": 0.0, "avg_logprob": -0.09565444039826346, "compression_ratio": 1.7468354430379747, "no_speech_prob": 9.665296602179296e-06}, {"id": 478, "seek": 390300, "start": 3916.0, "end": 3923.0, "text": " And so we'll be doing, so kind of the basic idea behind the original PageRank algorithm for Google search", "tokens": [400, 370, 321, 603, 312, 884, 11, 370, 733, 295, 264, 3875, 1558, 2261, 264, 3380, 21217, 49, 657, 9284, 337, 3329, 3164], "temperature": 0.0, "avg_logprob": -0.09565444039826346, "compression_ratio": 1.7468354430379747, "no_speech_prob": 9.665296602179296e-06}, {"id": 479, "seek": 390300, "start": 3923.0, "end": 3931.0, "text": " was that pages that a bunch of other pages link to must be more important, kind of that so many people are linking to that page.", "tokens": [390, 300, 7183, 300, 257, 3840, 295, 661, 7183, 2113, 281, 1633, 312, 544, 1021, 11, 733, 295, 300, 370, 867, 561, 366, 25775, 281, 300, 3028, 13], "temperature": 0.0, "avg_logprob": -0.09565444039826346, "compression_ratio": 1.7468354430379747, "no_speech_prob": 9.665296602179296e-06}, {"id": 480, "seek": 393100, "start": 3931.0, "end": 3943.0, "text": " And prior to that, like the very early, kind of like Yahoo! pages had editors that like hand went out and selected links to be listed.", "tokens": [400, 4059, 281, 300, 11, 411, 264, 588, 2440, 11, 733, 295, 411, 41757, 0, 7183, 632, 31446, 300, 411, 1011, 1437, 484, 293, 8209, 6123, 281, 312, 10052, 13], "temperature": 0.0, "avg_logprob": -0.08133876566984216, "compression_ratio": 1.7965367965367964, "no_speech_prob": 7.410642410832224e-06}, {"id": 481, "seek": 393100, "start": 3943.0, "end": 3950.0, "text": " And so this was kind of a big change to have an algorithm do it and kind of tell you how important different pages are.", "tokens": [400, 370, 341, 390, 733, 295, 257, 955, 1319, 281, 362, 364, 9284, 360, 309, 293, 733, 295, 980, 291, 577, 1021, 819, 7183, 366, 13], "temperature": 0.0, "avg_logprob": -0.08133876566984216, "compression_ratio": 1.7965367965367964, "no_speech_prob": 7.410642410832224e-06}, {"id": 482, "seek": 393100, "start": 3950.0, "end": 3960.0, "text": " And so what we'll be doing is using this Wikipedia data of what pages are linking to what pages to kind of see what the most important pages are based on links.", "tokens": [400, 370, 437, 321, 603, 312, 884, 307, 1228, 341, 28999, 1412, 295, 437, 7183, 366, 25775, 281, 437, 7183, 281, 733, 295, 536, 437, 264, 881, 1021, 7183, 366, 2361, 322, 6123, 13], "temperature": 0.0, "avg_logprob": -0.08133876566984216, "compression_ratio": 1.7965367965367964, "no_speech_prob": 7.410642410832224e-06}, {"id": 483, "seek": 396000, "start": 3960.0, "end": 3968.0, "text": " And I guess the other piece of that is you're normalizing for, you know, if a page links to 100 other pages,", "tokens": [400, 286, 2041, 264, 661, 2522, 295, 300, 307, 291, 434, 2710, 3319, 337, 11, 291, 458, 11, 498, 257, 3028, 6123, 281, 2319, 661, 7183, 11], "temperature": 0.0, "avg_logprob": -0.11923467352035198, "compression_ratio": 1.6008771929824561, "no_speech_prob": 4.494816948863445e-06}, {"id": 484, "seek": 396000, "start": 3968.0, "end": 3974.0, "text": " it's less special that they're kind of linking to each of those pages than if a page only links to two other pages.", "tokens": [309, 311, 1570, 2121, 300, 436, 434, 733, 295, 25775, 281, 1184, 295, 729, 7183, 813, 498, 257, 3028, 787, 6123, 281, 732, 661, 7183, 13], "temperature": 0.0, "avg_logprob": -0.11923467352035198, "compression_ratio": 1.6008771929824561, "no_speech_prob": 4.494816948863445e-06}, {"id": 485, "seek": 396000, "start": 3974.0, "end": 3979.0, "text": " And that kind of carries more weight.", "tokens": [400, 300, 733, 295, 16402, 544, 3364, 13], "temperature": 0.0, "avg_logprob": -0.11923467352035198, "compression_ratio": 1.6008771929824561, "no_speech_prob": 4.494816948863445e-06}, {"id": 486, "seek": 396000, "start": 3979.0, "end": 3986.0, "text": " So yeah, here this is just a little information about the full DBPedia data set has 38 million labels,", "tokens": [407, 1338, 11, 510, 341, 307, 445, 257, 707, 1589, 466, 264, 1577, 26754, 47, 14212, 1412, 992, 575, 12843, 2459, 16949, 11], "temperature": 0.0, "avg_logprob": -0.11923467352035198, "compression_ratio": 1.6008771929824561, "no_speech_prob": 4.494816948863445e-06}, {"id": 487, "seek": 398600, "start": 3986.0, "end": 3991.0, "text": " abstracts in 125 different languages, 25 million links to images.", "tokens": [12649, 82, 294, 25276, 819, 8650, 11, 3552, 2459, 6123, 281, 5267, 13], "temperature": 0.0, "avg_logprob": -0.06062505086263021, "compression_ratio": 1.4356435643564356, "no_speech_prob": 2.331980340386508e-06}, {"id": 488, "seek": 398600, "start": 3991.0, "end": 3999.0, "text": " So this is, I think, something to keep in mind for future projects.", "tokens": [407, 341, 307, 11, 286, 519, 11, 746, 281, 1066, 294, 1575, 337, 2027, 4455, 13], "temperature": 0.0, "avg_logprob": -0.06062505086263021, "compression_ratio": 1.4356435643564356, "no_speech_prob": 2.331980340386508e-06}, {"id": 489, "seek": 398600, "start": 3999.0, "end": 4004.0, "text": " And so I sent a Slack message this weekend. This can be kind of slow to download.", "tokens": [400, 370, 286, 2279, 257, 37211, 3636, 341, 6711, 13, 639, 393, 312, 733, 295, 2964, 281, 5484, 13], "temperature": 0.0, "avg_logprob": -0.06062505086263021, "compression_ratio": 1.4356435643564356, "no_speech_prob": 2.331980340386508e-06}, {"id": 490, "seek": 398600, "start": 4004.0, "end": 4013.0, "text": " So you might want to wait till after class if you haven't done it already.", "tokens": [407, 291, 1062, 528, 281, 1699, 4288, 934, 1508, 498, 291, 2378, 380, 1096, 309, 1217, 13], "temperature": 0.0, "avg_logprob": -0.06062505086263021, "compression_ratio": 1.4356435643564356, "no_speech_prob": 2.331980340386508e-06}, {"id": 491, "seek": 401300, "start": 4013.0, "end": 4018.0, "text": " So here this is just kind of opening, opening the data.", "tokens": [407, 510, 341, 307, 445, 733, 295, 5193, 11, 5193, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11230228060767763, "compression_ratio": 1.5204081632653061, "no_speech_prob": 3.6686424209619872e-06}, {"id": 492, "seek": 401300, "start": 4018.0, "end": 4022.0, "text": " I don't know why I got pasted in there.", "tokens": [286, 500, 380, 458, 983, 286, 658, 1791, 292, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.11230228060767763, "compression_ratio": 1.5204081632653061, "no_speech_prob": 3.6686424209619872e-06}, {"id": 493, "seek": 401300, "start": 4022.0, "end": 4029.0, "text": " What we're going to do is construct a graph adjacency matrix about which pages point to which.", "tokens": [708, 321, 434, 516, 281, 360, 307, 7690, 257, 4295, 22940, 3020, 8141, 466, 597, 7183, 935, 281, 597, 13], "temperature": 0.0, "avg_logprob": -0.11230228060767763, "compression_ratio": 1.5204081632653061, "no_speech_prob": 3.6686424209619872e-06}, {"id": 494, "seek": 401300, "start": 4029.0, "end": 4038.0, "text": " And so this is a kind of very simple example of if you just had four pages, if A is pointing to B, C and D,", "tokens": [400, 370, 341, 307, 257, 733, 295, 588, 2199, 1365, 295, 498, 291, 445, 632, 1451, 7183, 11, 498, 316, 307, 12166, 281, 363, 11, 383, 293, 413, 11], "temperature": 0.0, "avg_logprob": -0.11230228060767763, "compression_ratio": 1.5204081632653061, "no_speech_prob": 3.6686424209619872e-06}, {"id": 495, "seek": 403800, "start": 4038.0, "end": 4043.0, "text": " we could represent that here in the first column. This is a little bit confusing. A is a matrix here.", "tokens": [321, 727, 2906, 300, 510, 294, 264, 700, 7738, 13, 639, 307, 257, 707, 857, 13181, 13, 316, 307, 257, 8141, 510, 13], "temperature": 0.0, "avg_logprob": -0.049307602756428266, "compression_ratio": 1.8133333333333332, "no_speech_prob": 1.6700641936040483e-05}, {"id": 496, "seek": 403800, "start": 4043.0, "end": 4046.0, "text": " Up here it's the first node.", "tokens": [5858, 510, 309, 311, 264, 700, 9984, 13], "temperature": 0.0, "avg_logprob": -0.049307602756428266, "compression_ratio": 1.8133333333333332, "no_speech_prob": 1.6700641936040483e-05}, {"id": 497, "seek": 403800, "start": 4046.0, "end": 4053.0, "text": " We're putting ones to indicate the first node points to the second, third and fourth nodes.", "tokens": [492, 434, 3372, 2306, 281, 13330, 264, 700, 9984, 2793, 281, 264, 1150, 11, 2636, 293, 6409, 13891, 13], "temperature": 0.0, "avg_logprob": -0.049307602756428266, "compression_ratio": 1.8133333333333332, "no_speech_prob": 1.6700641936040483e-05}, {"id": 498, "seek": 403800, "start": 4053.0, "end": 4059.0, "text": " Then B only points to C. So the second node is only pointing to the third node.", "tokens": [1396, 363, 787, 2793, 281, 383, 13, 407, 264, 1150, 9984, 307, 787, 12166, 281, 264, 2636, 9984, 13], "temperature": 0.0, "avg_logprob": -0.049307602756428266, "compression_ratio": 1.8133333333333332, "no_speech_prob": 1.6700641936040483e-05}, {"id": 499, "seek": 403800, "start": 4059.0, "end": 4066.0, "text": " You can see in the second row of this matrix that there's just a one in the third spot, but nowhere else.", "tokens": [509, 393, 536, 294, 264, 1150, 5386, 295, 341, 8141, 300, 456, 311, 445, 257, 472, 294, 264, 2636, 4008, 11, 457, 11159, 1646, 13], "temperature": 0.0, "avg_logprob": -0.049307602756428266, "compression_ratio": 1.8133333333333332, "no_speech_prob": 1.6700641936040483e-05}, {"id": 500, "seek": 406600, "start": 4066.0, "end": 4070.0, "text": " So that one represents B is pointing to C.", "tokens": [407, 300, 472, 8855, 363, 307, 12166, 281, 383, 13], "temperature": 0.0, "avg_logprob": -0.09164354205131531, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.209856463901815e-06}, {"id": 501, "seek": 406600, "start": 4070.0, "end": 4078.0, "text": " Down here, C is just pointing to A. So you can see that we have a one in the first spot saying C is pointing to A.", "tokens": [9506, 510, 11, 383, 307, 445, 12166, 281, 316, 13, 407, 291, 393, 536, 300, 321, 362, 257, 472, 294, 264, 700, 4008, 1566, 383, 307, 12166, 281, 316, 13], "temperature": 0.0, "avg_logprob": -0.09164354205131531, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.209856463901815e-06}, {"id": 502, "seek": 406600, "start": 4078.0, "end": 4087.0, "text": " C doesn't point to anything else. And then D just points to C. So in the third spot we've got a one.", "tokens": [383, 1177, 380, 935, 281, 1340, 1646, 13, 400, 550, 413, 445, 2793, 281, 383, 13, 407, 294, 264, 2636, 4008, 321, 600, 658, 257, 472, 13], "temperature": 0.0, "avg_logprob": -0.09164354205131531, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.209856463901815e-06}, {"id": 503, "seek": 406600, "start": 4087.0, "end": 4095.0, "text": " And so this is a directed graph. So the order definitely matters. It's not symmetric.", "tokens": [400, 370, 341, 307, 257, 12898, 4295, 13, 407, 264, 1668, 2138, 7001, 13, 467, 311, 406, 32330, 13], "temperature": 0.0, "avg_logprob": -0.09164354205131531, "compression_ratio": 1.6618357487922706, "no_speech_prob": 1.209856463901815e-06}, {"id": 504, "seek": 409500, "start": 4095.0, "end": 4104.0, "text": " Are there questions about this representation?", "tokens": [2014, 456, 1651, 466, 341, 10290, 30], "temperature": 0.0, "avg_logprob": -0.13356955846150717, "compression_ratio": 1.375, "no_speech_prob": 2.212490107922349e-05}, {"id": 505, "seek": 409500, "start": 4104.0, "end": 4117.0, "text": " So taking the power A squared will tell you how many ways there are to get from one page to another page in two steps.", "tokens": [407, 1940, 264, 1347, 316, 8889, 486, 980, 291, 577, 867, 2098, 456, 366, 281, 483, 490, 472, 3028, 281, 1071, 3028, 294, 732, 4439, 13], "temperature": 0.0, "avg_logprob": -0.13356955846150717, "compression_ratio": 1.375, "no_speech_prob": 2.212490107922349e-05}, {"id": 506, "seek": 411700, "start": 4117.0, "end": 4127.0, "text": " And actually, I found these notes, which I liked, and they use the example of airlines traveling.", "tokens": [400, 767, 11, 286, 1352, 613, 5570, 11, 597, 286, 4501, 11, 293, 436, 764, 264, 1365, 295, 37147, 9712, 13], "temperature": 0.0, "avg_logprob": -0.15958552360534667, "compression_ratio": 1.4093959731543624, "no_speech_prob": 8.939529834606219e-06}, {"id": 507, "seek": 411700, "start": 4127.0, "end": 4140.0, "text": " So they just have this kind of smaller graph, I guess in Massachusetts, between which towns can you fly between.", "tokens": [407, 436, 445, 362, 341, 733, 295, 4356, 4295, 11, 286, 2041, 294, 19979, 11, 1296, 597, 18104, 393, 291, 3603, 1296, 13], "temperature": 0.0, "avg_logprob": -0.15958552360534667, "compression_ratio": 1.4093959731543624, "no_speech_prob": 8.939529834606219e-06}, {"id": 508, "seek": 414000, "start": 4140.0, "end": 4152.0, "text": " They represent that as a matrix with these zeros and ones.", "tokens": [814, 2906, 300, 382, 257, 8141, 365, 613, 35193, 293, 2306, 13], "temperature": 0.0, "avg_logprob": -0.09230687571506874, "compression_ratio": 1.3478260869565217, "no_speech_prob": 9.516056707070675e-06}, {"id": 509, "seek": 414000, "start": 4152.0, "end": 4163.0, "text": " And then they kind of take the power and say, OK, by taking the second power, we see there's one two-step sequence from C to F.", "tokens": [400, 550, 436, 733, 295, 747, 264, 1347, 293, 584, 11, 2264, 11, 538, 1940, 264, 1150, 1347, 11, 321, 536, 456, 311, 472, 732, 12, 16792, 8310, 490, 383, 281, 479, 13], "temperature": 0.0, "avg_logprob": -0.09230687571506874, "compression_ratio": 1.3478260869565217, "no_speech_prob": 9.516056707070675e-06}, {"id": 510, "seek": 416300, "start": 4163.0, "end": 4174.0, "text": " So that shows up in A, B, C. We've got a one here. There's one way to just with two legs of your journey to get from C to F.", "tokens": [407, 300, 3110, 493, 294, 316, 11, 363, 11, 383, 13, 492, 600, 658, 257, 472, 510, 13, 821, 311, 472, 636, 281, 445, 365, 732, 5668, 295, 428, 4671, 281, 483, 490, 383, 281, 479, 13], "temperature": 0.0, "avg_logprob": -0.07130899857938959, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.3931098641915014e-06}, {"id": 511, "seek": 416300, "start": 4174.0, "end": 4180.0, "text": " There are five three-step sequences between C and F. So coming over here, yeah.", "tokens": [821, 366, 1732, 1045, 12, 16792, 22978, 1296, 383, 293, 479, 13, 407, 1348, 670, 510, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.07130899857938959, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.3931098641915014e-06}, {"id": 512, "seek": 416300, "start": 4180.0, "end": 4187.0, "text": " So C is the third row, F is the sixth column, and we've got five. This is for M cubed.", "tokens": [407, 383, 307, 264, 2636, 5386, 11, 479, 307, 264, 15102, 7738, 11, 293, 321, 600, 658, 1732, 13, 639, 307, 337, 376, 36510, 13], "temperature": 0.0, "avg_logprob": -0.07130899857938959, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.3931098641915014e-06}, {"id": 513, "seek": 418700, "start": 4187.0, "end": 4204.0, "text": " Are there questions about this? This would show up in logistics problems.", "tokens": [2014, 456, 1651, 466, 341, 30, 639, 576, 855, 493, 294, 27420, 2740, 13], "temperature": 0.0, "avg_logprob": -0.11042923223776896, "compression_ratio": 1.4331210191082802, "no_speech_prob": 8.800878276815638e-06}, {"id": 514, "seek": 418700, "start": 4204.0, "end": 4209.0, "text": " All right. So the format of our data is it's kind of in files in these lists.", "tokens": [1057, 558, 13, 407, 264, 7877, 295, 527, 1412, 307, 309, 311, 733, 295, 294, 7098, 294, 613, 14511, 13], "temperature": 0.0, "avg_logprob": -0.11042923223776896, "compression_ratio": 1.4331210191082802, "no_speech_prob": 8.800878276815638e-06}, {"id": 515, "seek": 418700, "start": 4209.0, "end": 4213.0, "text": " And so we have both the redirects and the links, and we need to use both.", "tokens": [400, 370, 321, 362, 1293, 264, 29066, 82, 293, 264, 6123, 11, 293, 321, 643, 281, 764, 1293, 13], "temperature": 0.0, "avg_logprob": -0.11042923223776896, "compression_ratio": 1.4331210191082802, "no_speech_prob": 8.800878276815638e-06}, {"id": 516, "seek": 421300, "start": 4213.0, "end": 4221.0, "text": " And the redirects are just to kind of figure out which page is redirecting to other ones.", "tokens": [400, 264, 29066, 82, 366, 445, 281, 733, 295, 2573, 484, 597, 3028, 307, 29066, 278, 281, 661, 2306, 13], "temperature": 0.0, "avg_logprob": -0.05587796954547658, "compression_ratio": 1.7070063694267517, "no_speech_prob": 5.862651960342191e-06}, {"id": 517, "seek": 421300, "start": 4221.0, "end": 4228.0, "text": " And so the first one is kind of the source page that it's telling you it's a redirect or a link.", "tokens": [400, 370, 264, 700, 472, 307, 733, 295, 264, 4009, 3028, 300, 309, 311, 3585, 291, 309, 311, 257, 29066, 420, 257, 2113, 13], "temperature": 0.0, "avg_logprob": -0.05587796954547658, "compression_ratio": 1.7070063694267517, "no_speech_prob": 5.862651960342191e-06}, {"id": 518, "seek": 421300, "start": 4228.0, "end": 4235.0, "text": " And then the third argument is telling you the destination page it's pointing to.", "tokens": [400, 550, 264, 2636, 6770, 307, 3585, 291, 264, 12236, 3028, 309, 311, 12166, 281, 13], "temperature": 0.0, "avg_logprob": -0.05587796954547658, "compression_ratio": 1.7070063694267517, "no_speech_prob": 5.862651960342191e-06}, {"id": 519, "seek": 423500, "start": 4235.0, "end": 4254.0, "text": " So this is just some kind of data processing to read in the lines, to split them.", "tokens": [407, 341, 307, 445, 512, 733, 295, 1412, 9007, 281, 1401, 294, 264, 3876, 11, 281, 7472, 552, 13], "temperature": 0.0, "avg_logprob": -0.12436419895717075, "compression_ratio": 1.2242990654205608, "no_speech_prob": 4.565885774354683e-06}, {"id": 520, "seek": 423500, "start": 4254.0, "end": 4262.0, "text": " The redirects we're putting into this dictionary.", "tokens": [440, 29066, 82, 321, 434, 3372, 666, 341, 25890, 13], "temperature": 0.0, "avg_logprob": -0.12436419895717075, "compression_ratio": 1.2242990654205608, "no_speech_prob": 4.565885774354683e-06}, {"id": 521, "seek": 426200, "start": 4262.0, "end": 4269.0, "text": " I want to kind of come down. I think it's more interesting to kind of look at what we've created.", "tokens": [286, 528, 281, 733, 295, 808, 760, 13, 286, 519, 309, 311, 544, 1880, 281, 733, 295, 574, 412, 437, 321, 600, 2942, 13], "temperature": 0.0, "avg_logprob": -0.0875697135925293, "compression_ratio": 1.6605504587155964, "no_speech_prob": 2.8855702112196013e-05}, {"id": 522, "seek": 426200, "start": 4269.0, "end": 4277.0, "text": " So we have something called index map. And I've just an index map is a dictionary.", "tokens": [407, 321, 362, 746, 1219, 8186, 4471, 13, 400, 286, 600, 445, 364, 8186, 4471, 307, 257, 25890, 13], "temperature": 0.0, "avg_logprob": -0.0875697135925293, "compression_ratio": 1.6605504587155964, "no_speech_prob": 2.8855702112196013e-05}, {"id": 523, "seek": 426200, "start": 4277.0, "end": 4284.0, "text": " But to see what's in it, I'm just kind of popping off a random element from it.", "tokens": [583, 281, 536, 437, 311, 294, 309, 11, 286, 478, 445, 733, 295, 18374, 766, 257, 4974, 4478, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.0875697135925293, "compression_ratio": 1.6605504587155964, "no_speech_prob": 2.8855702112196013e-05}, {"id": 524, "seek": 426200, "start": 4284.0, "end": 4289.0, "text": " And in general, generally you don't want to do this because I think that that alters your dictionary.", "tokens": [400, 294, 2674, 11, 5101, 291, 500, 380, 528, 281, 360, 341, 570, 286, 519, 300, 300, 419, 1559, 428, 25890, 13], "temperature": 0.0, "avg_logprob": -0.0875697135925293, "compression_ratio": 1.6605504587155964, "no_speech_prob": 2.8855702112196013e-05}, {"id": 525, "seek": 428900, "start": 4289.0, "end": 4293.0, "text": " But I just wanted to kind of it's always important to be able to see what your data is like.", "tokens": [583, 286, 445, 1415, 281, 733, 295, 309, 311, 1009, 1021, 281, 312, 1075, 281, 536, 437, 428, 1412, 307, 411, 13], "temperature": 0.0, "avg_logprob": -0.07899281749986622, "compression_ratio": 1.4378109452736318, "no_speech_prob": 1.1658972653094679e-05}, {"id": 526, "seek": 428900, "start": 4293.0, "end": 4301.0, "text": " And I think it's more informative to see how our process data looks than to go through each step of the processing.", "tokens": [400, 286, 519, 309, 311, 544, 27759, 281, 536, 577, 527, 1399, 1412, 1542, 813, 281, 352, 807, 1184, 1823, 295, 264, 9007, 13], "temperature": 0.0, "avg_logprob": -0.07899281749986622, "compression_ratio": 1.4378109452736318, "no_speech_prob": 1.1658972653094679e-05}, {"id": 527, "seek": 428900, "start": 4301.0, "end": 4314.0, "text": " But here it says 1940 Cincinnati Red's team issue relates to this index 9991173.", "tokens": [583, 510, 309, 1619, 24158, 45951, 4477, 311, 1469, 2734, 16155, 281, 341, 8186, 1722, 8494, 5348, 33396, 13], "temperature": 0.0, "avg_logprob": -0.07899281749986622, "compression_ratio": 1.4378109452736318, "no_speech_prob": 1.1658972653094679e-05}, {"id": 528, "seek": 431400, "start": 4314.0, "end": 4322.0, "text": " So then we've got two lists of source and destination. And those are just the list of the same length and their list of indices.", "tokens": [407, 550, 321, 600, 658, 732, 14511, 295, 4009, 293, 12236, 13, 400, 729, 366, 445, 264, 1329, 295, 264, 912, 4641, 293, 641, 1329, 295, 43840, 13], "temperature": 0.0, "avg_logprob": -0.10111348573551622, "compression_ratio": 1.8090452261306533, "no_speech_prob": 1.450957734050462e-05}, {"id": 529, "seek": 431400, "start": 4322.0, "end": 4333.0, "text": " And so they're kind of telling you, you know, the seventh spot in source is the index of which page is a source pointing to the seventh spot in destination.", "tokens": [400, 370, 436, 434, 733, 295, 3585, 291, 11, 291, 458, 11, 264, 17875, 4008, 294, 4009, 307, 264, 8186, 295, 597, 3028, 307, 257, 4009, 12166, 281, 264, 17875, 4008, 294, 12236, 13], "temperature": 0.0, "avg_logprob": -0.10111348573551622, "compression_ratio": 1.8090452261306533, "no_speech_prob": 1.450957734050462e-05}, {"id": 530, "seek": 431400, "start": 4333.0, "end": 4337.0, "text": " And this so this is a very inefficient way to be working with these lists.", "tokens": [400, 341, 370, 341, 307, 257, 588, 43495, 636, 281, 312, 1364, 365, 613, 14511, 13], "temperature": 0.0, "avg_logprob": -0.10111348573551622, "compression_ratio": 1.8090452261306533, "no_speech_prob": 1.450957734050462e-05}, {"id": 531, "seek": 433700, "start": 4337.0, "end": 4345.0, "text": " And I'm doing this to kind of illustrate what they represent. But this isn't how you would actually be using them.", "tokens": [400, 286, 478, 884, 341, 281, 733, 295, 23221, 437, 436, 2906, 13, 583, 341, 1943, 380, 577, 291, 576, 767, 312, 1228, 552, 13], "temperature": 0.0, "avg_logprob": -0.09587796529134114, "compression_ratio": 1.3650793650793651, "no_speech_prob": 2.6425398118590238e-06}, {"id": 532, "seek": 433700, "start": 4345.0, "end": 4352.0, "text": " But so I know Cincinnati Red's team issue is 9991173. Where does that even show up in the source list?", "tokens": [583, 370, 286, 458, 45951, 4477, 311, 1469, 2734, 307, 1722, 8494, 5348, 33396, 13, 2305, 775, 300, 754, 855, 493, 294, 264, 4009, 1329, 30], "temperature": 0.0, "avg_logprob": -0.09587796529134114, "compression_ratio": 1.3650793650793651, "no_speech_prob": 2.6425398118590238e-06}, {"id": 533, "seek": 433700, "start": 4352.0, "end": 4363.0, "text": " It just shows up at one index 119077649.", "tokens": [467, 445, 3110, 493, 412, 472, 8186, 2975, 7771, 17512, 21, 14938, 13], "temperature": 0.0, "avg_logprob": -0.09587796529134114, "compression_ratio": 1.3650793650793651, "no_speech_prob": 2.6425398118590238e-06}, {"id": 534, "seek": 436300, "start": 4363.0, "end": 4370.0, "text": " And so then I can look that up in the destination and I get 9991050.", "tokens": [400, 370, 550, 286, 393, 574, 300, 493, 294, 264, 12236, 293, 286, 483, 1722, 8494, 3279, 2803, 13], "temperature": 0.0, "avg_logprob": -0.09823298118483852, "compression_ratio": 1.4887640449438202, "no_speech_prob": 5.255285032035317e-06}, {"id": 535, "seek": 436300, "start": 4370.0, "end": 4381.0, "text": " So the we have, I guess, two different types of indices going on. It's the indices being used by the index map and then the.", "tokens": [407, 264, 321, 362, 11, 286, 2041, 11, 732, 819, 3467, 295, 43840, 516, 322, 13, 467, 311, 264, 43840, 885, 1143, 538, 264, 8186, 4471, 293, 550, 264, 13], "temperature": 0.0, "avg_logprob": -0.09823298118483852, "compression_ratio": 1.4887640449438202, "no_speech_prob": 5.255285032035317e-06}, {"id": 536, "seek": 436300, "start": 4381.0, "end": 4387.0, "text": " Indices of source and destination, which just correspond to each other.", "tokens": [2333, 1473, 295, 4009, 293, 12236, 11, 597, 445, 6805, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.09823298118483852, "compression_ratio": 1.4887640449438202, "no_speech_prob": 5.255285032035317e-06}, {"id": 537, "seek": 438700, "start": 4387.0, "end": 4398.0, "text": " All this to say, then we look up 9991050 and find out that that is page W711-2.", "tokens": [1057, 341, 281, 584, 11, 550, 321, 574, 493, 1722, 8494, 3279, 2803, 293, 915, 484, 300, 300, 307, 3028, 343, 22, 5348, 12, 17, 13], "temperature": 0.0, "avg_logprob": -0.1571085294087728, "compression_ratio": 1.4741379310344827, "no_speech_prob": 7.646404810657259e-06}, {"id": 538, "seek": 438700, "start": 4398.0, "end": 4405.0, "text": " I just want to say I actually tried popping like several elements off to see if I could find something more.", "tokens": [286, 445, 528, 281, 584, 286, 767, 3031, 18374, 411, 2940, 4959, 766, 281, 536, 498, 286, 727, 915, 746, 544, 13], "temperature": 0.0, "avg_logprob": -0.1571085294087728, "compression_ratio": 1.4741379310344827, "no_speech_prob": 7.646404810657259e-06}, {"id": 539, "seek": 438700, "start": 4405.0, "end": 4415.0, "text": " Maybe more interesting, but some of them are like for pages that have been deleted because I think the DVDpedia data set is, you know, not super current.", "tokens": [2704, 544, 1880, 11, 457, 512, 295, 552, 366, 411, 337, 7183, 300, 362, 668, 22981, 570, 286, 519, 264, 21187, 3452, 654, 1412, 992, 307, 11, 291, 458, 11, 406, 1687, 2190, 13], "temperature": 0.0, "avg_logprob": -0.1571085294087728, "compression_ratio": 1.4741379310344827, "no_speech_prob": 7.646404810657259e-06}, {"id": 540, "seek": 441500, "start": 4415.0, "end": 4418.0, "text": " Wikipedia gets changed a lot. So this is actually the best I could do.", "tokens": [28999, 2170, 3105, 257, 688, 13, 407, 341, 307, 767, 264, 1151, 286, 727, 360, 13], "temperature": 0.0, "avg_logprob": -0.11088154267291633, "compression_ratio": 1.5393700787401574, "no_speech_prob": 6.643240794801386e-06}, {"id": 541, "seek": 441500, "start": 4418.0, "end": 4428.0, "text": " But I went to Wikipedia and I looked up Cincinnati Red's team issue and it redirected me to W711-2.", "tokens": [583, 286, 1437, 281, 28999, 293, 286, 2956, 493, 45951, 4477, 311, 1469, 2734, 293, 309, 29066, 292, 385, 281, 343, 22, 5348, 12, 17, 13], "temperature": 0.0, "avg_logprob": -0.11088154267291633, "compression_ratio": 1.5393700787401574, "no_speech_prob": 6.643240794801386e-06}, {"id": 542, "seek": 441500, "start": 4428.0, "end": 4435.0, "text": " So that that that's showing that our data is representing what we want and we can get information from it.", "tokens": [407, 300, 300, 300, 311, 4099, 300, 527, 1412, 307, 13460, 437, 321, 528, 293, 321, 393, 483, 1589, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.11088154267291633, "compression_ratio": 1.5393700787401574, "no_speech_prob": 6.643240794801386e-06}, {"id": 543, "seek": 441500, "start": 4435.0, "end": 4441.0, "text": " Although it does involve this kind of. You know, like having to search for where does the index appear in source?", "tokens": [5780, 309, 775, 9494, 341, 733, 295, 13, 509, 458, 11, 411, 1419, 281, 3164, 337, 689, 775, 264, 8186, 4204, 294, 4009, 30], "temperature": 0.0, "avg_logprob": -0.11088154267291633, "compression_ratio": 1.5393700787401574, "no_speech_prob": 6.643240794801386e-06}, {"id": 544, "seek": 444100, "start": 4441.0, "end": 4445.0, "text": " And then what does that correspond to in destination?", "tokens": [400, 550, 437, 775, 300, 6805, 281, 294, 12236, 30], "temperature": 0.0, "avg_logprob": -0.1475047788758209, "compression_ratio": 1.3756906077348066, "no_speech_prob": 8.267531484307256e-06}, {"id": 545, "seek": 444100, "start": 4445.0, "end": 4451.0, "text": " And this is all talking about a pack of baseball cards. If you are wondering.", "tokens": [400, 341, 307, 439, 1417, 466, 257, 2844, 295, 14323, 5632, 13, 759, 291, 366, 6359, 13], "temperature": 0.0, "avg_logprob": -0.1475047788758209, "compression_ratio": 1.3756906077348066, "no_speech_prob": 8.267531484307256e-06}, {"id": 546, "seek": 444100, "start": 4451.0, "end": 4460.0, "text": " And we can even open this link in a new tab.", "tokens": [400, 321, 393, 754, 1269, 341, 2113, 294, 257, 777, 4421, 13], "temperature": 0.0, "avg_logprob": -0.1475047788758209, "compression_ratio": 1.3756906077348066, "no_speech_prob": 8.267531484307256e-06}, {"id": 547, "seek": 444100, "start": 4460.0, "end": 4470.0, "text": " And it says W711-2 is also known as the 1940 Cincinnati Reds team issue,", "tokens": [400, 309, 1619, 343, 22, 5348, 12, 17, 307, 611, 2570, 382, 264, 24158, 45951, 4477, 82, 1469, 2734, 11], "temperature": 0.0, "avg_logprob": -0.1475047788758209, "compression_ratio": 1.3756906077348066, "no_speech_prob": 8.267531484307256e-06}, {"id": 548, "seek": 447000, "start": 4470.0, "end": 4474.0, "text": " which is a baseball card set.", "tokens": [597, 307, 257, 14323, 2920, 992, 13], "temperature": 0.0, "avg_logprob": -0.07799006779988607, "compression_ratio": 1.489247311827957, "no_speech_prob": 1.165883440990001e-05}, {"id": 549, "seek": 447000, "start": 4474.0, "end": 4482.0, "text": " And we'll take a look at it just to see some of the things that show up about baseball, Cincinnati Reds, Detroit Tigers.", "tokens": [400, 321, 603, 747, 257, 574, 412, 309, 445, 281, 536, 512, 295, 264, 721, 300, 855, 493, 466, 14323, 11, 45951, 4477, 82, 11, 20887, 37699, 13], "temperature": 0.0, "avg_logprob": -0.07799006779988607, "compression_ratio": 1.489247311827957, "no_speech_prob": 1.165883440990001e-05}, {"id": 550, "seek": 447000, "start": 4482.0, "end": 4490.0, "text": " And then there are a bunch of names of players in here.", "tokens": [400, 550, 456, 366, 257, 3840, 295, 5288, 295, 4150, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.07799006779988607, "compression_ratio": 1.489247311827957, "no_speech_prob": 1.165883440990001e-05}, {"id": 551, "seek": 447000, "start": 4490.0, "end": 4493.0, "text": " And so let me just say that kind of one more time how this is working.", "tokens": [400, 370, 718, 385, 445, 584, 300, 733, 295, 472, 544, 565, 577, 341, 307, 1364, 13], "temperature": 0.0, "avg_logprob": -0.07799006779988607, "compression_ratio": 1.489247311827957, "no_speech_prob": 1.165883440990001e-05}, {"id": 552, "seek": 449300, "start": 4493.0, "end": 4501.0, "text": " Index map is a mapping between names of Wikipedia pages and a number representing them.", "tokens": [33552, 4471, 307, 257, 18350, 1296, 5288, 295, 28999, 7183, 293, 257, 1230, 13460, 552, 13], "temperature": 0.0, "avg_logprob": -0.07220688427195829, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.425362931215204e-06}, {"id": 553, "seek": 449300, "start": 4501.0, "end": 4505.0, "text": " And then we can look for those numbers in the source or destination list.", "tokens": [400, 550, 321, 393, 574, 337, 729, 3547, 294, 264, 4009, 420, 12236, 1329, 13], "temperature": 0.0, "avg_logprob": -0.07220688427195829, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.425362931215204e-06}, {"id": 554, "seek": 449300, "start": 4505.0, "end": 4515.0, "text": " And the. And then the source and destination lists kind of need to be paired together because things show up.", "tokens": [400, 264, 13, 400, 550, 264, 4009, 293, 12236, 14511, 733, 295, 643, 281, 312, 25699, 1214, 570, 721, 855, 493, 13], "temperature": 0.0, "avg_logprob": -0.07220688427195829, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.425362931215204e-06}, {"id": 555, "seek": 449300, "start": 4515.0, "end": 4521.0, "text": " You know, like the third entry of the source list is pointing to the third entry of the destination list.", "tokens": [509, 458, 11, 411, 264, 2636, 8729, 295, 264, 4009, 1329, 307, 12166, 281, 264, 2636, 8729, 295, 264, 12236, 1329, 13], "temperature": 0.0, "avg_logprob": -0.07220688427195829, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.425362931215204e-06}, {"id": 556, "seek": 452100, "start": 4521.0, "end": 4531.0, "text": " So are there questions about this?", "tokens": [407, 366, 456, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.11088982293772143, "compression_ratio": 1.1370967741935485, "no_speech_prob": 4.157277544436511e-06}, {"id": 557, "seek": 452100, "start": 4531.0, "end": 4544.0, "text": " OK, so now I'm interested in so 9991050 is this W711-2 page, which represents this pack of baseball cards.", "tokens": [2264, 11, 370, 586, 286, 478, 3102, 294, 370, 1722, 8494, 3279, 2803, 307, 341, 343, 22, 5348, 12, 17, 3028, 11, 597, 8855, 341, 2844, 295, 14323, 5632, 13], "temperature": 0.0, "avg_logprob": -0.11088982293772143, "compression_ratio": 1.1370967741935485, "no_speech_prob": 4.157277544436511e-06}, {"id": 558, "seek": 454400, "start": 4544.0, "end": 4552.0, "text": " And I want to know where that shows up in the source. So kind of what pages is the source pointing at?", "tokens": [400, 286, 528, 281, 458, 689, 300, 3110, 493, 294, 264, 4009, 13, 407, 733, 295, 437, 7183, 307, 264, 4009, 12166, 412, 30], "temperature": 0.0, "avg_logprob": -0.1880462557770485, "compression_ratio": 1.2735042735042734, "no_speech_prob": 3.7265413084242027e-06}, {"id": 559, "seek": 454400, "start": 4552.0, "end": 4562.0, "text": " And I get back. It's actually a check of this.", "tokens": [400, 286, 483, 646, 13, 467, 311, 767, 257, 1520, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.1880462557770485, "compression_ratio": 1.2735042735042734, "no_speech_prob": 3.7265413084242027e-06}, {"id": 560, "seek": 456200, "start": 4562.0, "end": 4578.0, "text": " I get 47. So there are a lot that's saying that this Wikipedia page has 47 outgoing links on it, which seems very plausible to me.", "tokens": [286, 483, 16953, 13, 407, 456, 366, 257, 688, 300, 311, 1566, 300, 341, 28999, 3028, 575, 16953, 41565, 6123, 322, 309, 11, 597, 2544, 588, 39925, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.08484796177257191, "compression_ratio": 1.3653846153846154, "no_speech_prob": 2.4439661956421332e-06}, {"id": 561, "seek": 456200, "start": 4578.0, "end": 4584.0, "text": " And then now I look those up in my index map to see what pages they correspond to.", "tokens": [400, 550, 586, 286, 574, 729, 493, 294, 452, 8186, 4471, 281, 536, 437, 7183, 436, 6805, 281, 13], "temperature": 0.0, "avg_logprob": -0.08484796177257191, "compression_ratio": 1.3653846153846154, "no_speech_prob": 2.4439661956421332e-06}, {"id": 562, "seek": 458400, "start": 4584.0, "end": 4596.0, "text": " And so that's what I have here. And they correspond to baseball, baseball, Ohio, Cincinnati, Flash Thompson, 1940, 1938, Lonnie Frey, Cincinnati Reds, Ernie Lombardi.", "tokens": [400, 370, 300, 311, 437, 286, 362, 510, 13, 400, 436, 6805, 281, 14323, 11, 14323, 11, 14469, 11, 45951, 11, 20232, 23460, 11, 24158, 11, 46398, 11, 35927, 2766, 6142, 88, 11, 45951, 4477, 82, 11, 3300, 2766, 441, 3548, 38126, 13], "temperature": 0.0, "avg_logprob": -0.0897784180693574, "compression_ratio": 1.4957264957264957, "no_speech_prob": 3.726585646290914e-06}, {"id": 563, "seek": 458400, "start": 4596.0, "end": 4599.0, "text": " So this seems correct. Like our data is what we think it is.", "tokens": [407, 341, 2544, 3006, 13, 1743, 527, 1412, 307, 437, 321, 519, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.0897784180693574, "compression_ratio": 1.4957264957264957, "no_speech_prob": 3.726585646290914e-06}, {"id": 564, "seek": 458400, "start": 4599.0, "end": 4612.0, "text": " We've got the information that this W7 page is linking to all these other baseball related pages and names of the players.", "tokens": [492, 600, 658, 264, 1589, 300, 341, 343, 22, 3028, 307, 25775, 281, 439, 613, 661, 14323, 4077, 7183, 293, 5288, 295, 264, 4150, 13], "temperature": 0.0, "avg_logprob": -0.0897784180693574, "compression_ratio": 1.4957264957264957, "no_speech_prob": 3.726585646290914e-06}, {"id": 565, "seek": 461200, "start": 4612.0, "end": 4630.0, "text": " I just had a screen capture of that. So any questions about that, kind of that aspect of the data processing?", "tokens": [286, 445, 632, 257, 2568, 7983, 295, 300, 13, 407, 604, 1651, 466, 300, 11, 733, 295, 300, 4171, 295, 264, 1412, 9007, 30], "temperature": 0.0, "avg_logprob": -0.16912143571036203, "compression_ratio": 1.184782608695652, "no_speech_prob": 3.219013524358161e-05}, {"id": 566, "seek": 463000, "start": 4630.0, "end": 4642.0, "text": " OK, so then now we're going to use sparse.coo to create a sparse sparse matrix and then we're going to convert it to CSR. So two questions.", "tokens": [2264, 11, 370, 550, 586, 321, 434, 516, 281, 764, 637, 11668, 13, 1291, 78, 281, 1884, 257, 637, 11668, 637, 11668, 8141, 293, 550, 321, 434, 516, 281, 7620, 309, 281, 9460, 49, 13, 407, 732, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1545278549194336, "compression_ratio": 1.3658536585365855, "no_speech_prob": 1.2804734069504775e-05}, {"id": 567, "seek": 463000, "start": 4642.0, "end": 4655.0, "text": " First, what are coo and CSR?", "tokens": [2386, 11, 437, 366, 598, 78, 293, 9460, 49, 30], "temperature": 0.0, "avg_logprob": -0.1545278549194336, "compression_ratio": 1.3658536585365855, "no_speech_prob": 1.2804734069504775e-05}, {"id": 568, "seek": 465500, "start": 4655.0, "end": 4662.0, "text": " Matthew has the microphone if someone wants to help see.", "tokens": [12434, 575, 264, 10952, 498, 1580, 2738, 281, 854, 536, 13], "temperature": 0.0, "avg_logprob": -0.25927439757755827, "compression_ratio": 1.1808510638297873, "no_speech_prob": 7.071596428431803e-06}, {"id": 569, "seek": 465500, "start": 4662.0, "end": 4670.0, "text": " I think coo is the coordinate representation. Exactly.", "tokens": [286, 519, 598, 78, 307, 264, 15670, 10290, 13, 7587, 13], "temperature": 0.0, "avg_logprob": -0.25927439757755827, "compression_ratio": 1.1808510638297873, "no_speech_prob": 7.071596428431803e-06}, {"id": 570, "seek": 467000, "start": 4670.0, "end": 4687.0, "text": " Yes. Yeah. So these are two different sparse representations. And so why would we create it with coo and then convert it in the next line?", "tokens": [1079, 13, 865, 13, 407, 613, 366, 732, 819, 637, 11668, 33358, 13, 400, 370, 983, 576, 321, 1884, 309, 365, 598, 78, 293, 550, 7620, 309, 294, 264, 958, 1622, 30], "temperature": 0.0, "avg_logprob": -0.1249501591637021, "compression_ratio": 1.2396694214876034, "no_speech_prob": 1.2028696801280603e-05}, {"id": 571, "seek": 467000, "start": 4687.0, "end": 4695.0, "text": " OK, Kelsey.", "tokens": [2264, 11, 44714, 13], "temperature": 0.0, "avg_logprob": -0.1249501591637021, "compression_ratio": 1.2396694214876034, "no_speech_prob": 1.2028696801280603e-05}, {"id": 572, "seek": 469500, "start": 4695.0, "end": 4705.0, "text": " Exactly. Yeah. So coo, I think kind of makes the most sense logically because with CSR, we have to do that kind of counting of we keep track of a row pointer.", "tokens": [7587, 13, 865, 13, 407, 598, 78, 11, 286, 519, 733, 295, 1669, 264, 881, 2020, 38887, 570, 365, 9460, 49, 11, 321, 362, 281, 360, 300, 733, 295, 13251, 295, 321, 1066, 2837, 295, 257, 5386, 23918, 13], "temperature": 0.0, "avg_logprob": -0.11597770849863688, "compression_ratio": 1.6172413793103448, "no_speech_prob": 9.817893442232162e-06}, {"id": 573, "seek": 469500, "start": 4705.0, "end": 4714.0, "text": " And so how many you know, how many values are in each row? When do we need to update our row pointer, which would be kind of a pain to do by hand.", "tokens": [400, 370, 577, 867, 291, 458, 11, 577, 867, 4190, 366, 294, 1184, 5386, 30, 1133, 360, 321, 643, 281, 5623, 527, 5386, 23918, 11, 597, 576, 312, 733, 295, 257, 1822, 281, 360, 538, 1011, 13], "temperature": 0.0, "avg_logprob": -0.11597770849863688, "compression_ratio": 1.6172413793103448, "no_speech_prob": 9.817893442232162e-06}, {"id": 574, "seek": 469500, "start": 4714.0, "end": 4724.0, "text": " And there's efficient conversion between the different types. But coo, it's very natural to say this is our data and we want to do it by destination, comma source.", "tokens": [400, 456, 311, 7148, 14298, 1296, 264, 819, 3467, 13, 583, 598, 78, 11, 309, 311, 588, 3303, 281, 584, 341, 307, 527, 1412, 293, 321, 528, 281, 360, 309, 538, 12236, 11, 22117, 4009, 13], "temperature": 0.0, "avg_logprob": -0.11597770849863688, "compression_ratio": 1.6172413793103448, "no_speech_prob": 9.817893442232162e-06}, {"id": 575, "seek": 472400, "start": 4724.0, "end": 4730.0, "text": " So this is our rows are going to be the destinations, the columns are the sources.", "tokens": [407, 341, 307, 527, 13241, 366, 516, 281, 312, 264, 37787, 11, 264, 13766, 366, 264, 7139, 13], "temperature": 0.0, "avg_logprob": -0.09872572951846653, "compression_ratio": 1.4787234042553192, "no_speech_prob": 4.784906650456833e-06}, {"id": 576, "seek": 472400, "start": 4730.0, "end": 4737.0, "text": " Put that into a sparse matrix and then convert it to CSR. And I wanted to highlight this.", "tokens": [4935, 300, 666, 257, 637, 11668, 8141, 293, 550, 7620, 309, 281, 9460, 49, 13, 400, 286, 1415, 281, 5078, 341, 13], "temperature": 0.0, "avg_logprob": -0.09872572951846653, "compression_ratio": 1.4787234042553192, "no_speech_prob": 4.784906650456833e-06}, {"id": 577, "seek": 472400, "start": 4737.0, "end": 4743.0, "text": " So this is the page that we've seen several times explaining.", "tokens": [407, 341, 307, 264, 3028, 300, 321, 600, 1612, 2940, 1413, 13468, 13], "temperature": 0.0, "avg_logprob": -0.09872572951846653, "compression_ratio": 1.4787234042553192, "no_speech_prob": 4.784906650456833e-06}, {"id": 578, "seek": 472400, "start": 4743.0, "end": 4747.0, "text": " Kind of about the different sparse formats.", "tokens": [9242, 295, 466, 264, 819, 637, 11668, 25879, 13], "temperature": 0.0, "avg_logprob": -0.09872572951846653, "compression_ratio": 1.4787234042553192, "no_speech_prob": 4.784906650456833e-06}, {"id": 579, "seek": 474700, "start": 4747.0, "end": 4756.0, "text": " And it points out over here at the bottom advantage of CSR method over the coordinate wise method is that.", "tokens": [400, 309, 2793, 484, 670, 510, 412, 264, 2767, 5002, 295, 9460, 49, 3170, 670, 264, 15670, 10829, 3170, 307, 300, 13], "temperature": 0.0, "avg_logprob": -0.06554849112211768, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.963723661901895e-06}, {"id": 580, "seek": 474700, "start": 4756.0, "end": 4763.0, "text": " So the number of operations to perform matrix vector multiplication are the same for the two.", "tokens": [407, 264, 1230, 295, 7705, 281, 2042, 8141, 8062, 27290, 366, 264, 912, 337, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.06554849112211768, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.963723661901895e-06}, {"id": 581, "seek": 474700, "start": 4763.0, "end": 4771.0, "text": " However, the number of memory accesses is reduced by factor of two in the CSR method.", "tokens": [2908, 11, 264, 1230, 295, 4675, 2105, 279, 307, 9212, 538, 5952, 295, 732, 294, 264, 9460, 49, 3170, 13], "temperature": 0.0, "avg_logprob": -0.06554849112211768, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.963723661901895e-06}, {"id": 582, "seek": 477100, "start": 4771.0, "end": 4781.0, "text": " So that's and this is a nice example of. You know, if you were just thinking about big O, it's going to be the same in terms of operations.", "tokens": [407, 300, 311, 293, 341, 307, 257, 1481, 1365, 295, 13, 509, 458, 11, 498, 291, 645, 445, 1953, 466, 955, 422, 11, 309, 311, 516, 281, 312, 264, 912, 294, 2115, 295, 7705, 13], "temperature": 0.0, "avg_logprob": -0.0875093280405238, "compression_ratio": 1.4189944134078212, "no_speech_prob": 2.521449232517625e-06}, {"id": 583, "seek": 477100, "start": 4781.0, "end": 4786.0, "text": " However, this idea of memory access being something that matters and can be can slow us down.", "tokens": [2908, 11, 341, 1558, 295, 4675, 2105, 885, 746, 300, 7001, 293, 393, 312, 393, 2964, 505, 760, 13], "temperature": 0.0, "avg_logprob": -0.0875093280405238, "compression_ratio": 1.4189944134078212, "no_speech_prob": 2.521449232517625e-06}, {"id": 584, "seek": 477100, "start": 4786.0, "end": 4791.0, "text": " CSR is a lot better.", "tokens": [9460, 49, 307, 257, 688, 1101, 13], "temperature": 0.0, "avg_logprob": -0.0875093280405238, "compression_ratio": 1.4189944134078212, "no_speech_prob": 2.521449232517625e-06}, {"id": 585, "seek": 479100, "start": 4791.0, "end": 4802.0, "text": " Questions about that.", "tokens": [27738, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.10317888467208199, "compression_ratio": 1.313868613138686, "no_speech_prob": 1.9221697584725916e-05}, {"id": 586, "seek": 479100, "start": 4802.0, "end": 4819.0, "text": " OK, and so we'll be doing surprisingly some matrix vector multiplications, and so it's going to be faster to use CSR and that will have fewer memory accesses.", "tokens": [2264, 11, 293, 370, 321, 603, 312, 884, 17600, 512, 8141, 8062, 17596, 763, 11, 293, 370, 309, 311, 516, 281, 312, 4663, 281, 764, 9460, 49, 293, 300, 486, 362, 13366, 4675, 2105, 279, 13], "temperature": 0.0, "avg_logprob": -0.10317888467208199, "compression_ratio": 1.313868613138686, "no_speech_prob": 1.9221697584725916e-05}, {"id": 587, "seek": 481900, "start": 4819.0, "end": 4826.0, "text": " And then I highly recommend if you're doing this at home, all this processing and creating your matrix can be a bit slow.", "tokens": [400, 550, 286, 5405, 2748, 498, 291, 434, 884, 341, 412, 1280, 11, 439, 341, 9007, 293, 4084, 428, 8141, 393, 312, 257, 857, 2964, 13], "temperature": 0.0, "avg_logprob": -0.04083460569381714, "compression_ratio": 1.7327935222672064, "no_speech_prob": 8.528702892363071e-06}, {"id": 588, "seek": 481900, "start": 4826.0, "end": 4834.0, "text": " And so when you get to this point, it's a good idea to save your matrix and you can use Pickle, which is a Python library for kind of compressing things.", "tokens": [400, 370, 562, 291, 483, 281, 341, 935, 11, 309, 311, 257, 665, 1558, 281, 3155, 428, 8141, 293, 291, 393, 764, 14129, 306, 11, 597, 307, 257, 15329, 6405, 337, 733, 295, 14778, 278, 721, 13], "temperature": 0.0, "avg_logprob": -0.04083460569381714, "compression_ratio": 1.7327935222672064, "no_speech_prob": 8.528702892363071e-06}, {"id": 589, "seek": 481900, "start": 4834.0, "end": 4842.0, "text": " And then that way, if you want to come back to this later, you're not having to recreate your matrix every every single time you're using this notebook.", "tokens": [400, 550, 300, 636, 11, 498, 291, 528, 281, 808, 646, 281, 341, 1780, 11, 291, 434, 406, 1419, 281, 25833, 428, 8141, 633, 633, 2167, 565, 291, 434, 1228, 341, 21060, 13], "temperature": 0.0, "avg_logprob": -0.04083460569381714, "compression_ratio": 1.7327935222672064, "no_speech_prob": 8.528702892363071e-06}, {"id": 590, "seek": 484200, "start": 4842.0, "end": 4850.0, "text": " This is something I recommend in general for any sort of data project where it's slow to compute the data.", "tokens": [639, 307, 746, 286, 2748, 294, 2674, 337, 604, 1333, 295, 1412, 1716, 689, 309, 311, 2964, 281, 14722, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1351656368800572, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.092654307896737e-06}, {"id": 591, "seek": 484200, "start": 4850.0, "end": 4858.0, "text": " So here we. We save our index map and our data matrix X.", "tokens": [407, 510, 321, 13, 492, 3155, 527, 8186, 4471, 293, 527, 1412, 8141, 1783, 13], "temperature": 0.0, "avg_logprob": -0.1351656368800572, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.092654307896737e-06}, {"id": 592, "seek": 484200, "start": 4858.0, "end": 4869.0, "text": " I can check here X is a this is quite a large matrix about close to 12 million by 12 million sparse matrix.", "tokens": [286, 393, 1520, 510, 1783, 307, 257, 341, 307, 1596, 257, 2416, 8141, 466, 1998, 281, 2272, 2459, 538, 2272, 2459, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1351656368800572, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.092654307896737e-06}, {"id": 593, "seek": 486900, "start": 4869.0, "end": 4875.0, "text": " And then it's nice that it tells us this. It's so it was.", "tokens": [400, 550, 309, 311, 1481, 300, 309, 5112, 505, 341, 13, 467, 311, 370, 309, 390, 13], "temperature": 0.0, "avg_logprob": -0.32033032462710426, "compression_ratio": 1.1386138613861385, "no_speech_prob": 2.0903426047880203e-06}, {"id": 594, "seek": 486900, "start": 4875.0, "end": 4881.0, "text": " It was I asked by a factor. Oh, OK, yeah, 100. Thank you.", "tokens": [467, 390, 286, 2351, 538, 257, 5952, 13, 876, 11, 2264, 11, 1338, 11, 2319, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.32033032462710426, "compression_ratio": 1.1386138613861385, "no_speech_prob": 2.0903426047880203e-06}, {"id": 595, "seek": 488100, "start": 4881.0, "end": 4902.0, "text": " 120 million by 120 million matrix, and then it's got 94 million non zero entries, which is a lot less than 120 million squared. So we're saving a lot of space.", "tokens": [10411, 2459, 538, 10411, 2459, 8141, 11, 293, 550, 309, 311, 658, 30849, 2459, 2107, 4018, 23041, 11, 597, 307, 257, 688, 1570, 813, 10411, 2459, 8889, 13, 407, 321, 434, 6816, 257, 688, 295, 1901, 13], "temperature": 0.0, "avg_logprob": -0.12107942542251275, "compression_ratio": 1.3571428571428572, "no_speech_prob": 8.851330335346574e-07}, {"id": 596, "seek": 488100, "start": 4902.0, "end": 4910.0, "text": " Any questions about the setup?", "tokens": [2639, 1651, 466, 264, 8657, 30], "temperature": 0.0, "avg_logprob": -0.12107942542251275, "compression_ratio": 1.3571428571428572, "no_speech_prob": 8.851330335346574e-07}, {"id": 597, "seek": 491000, "start": 4910.0, "end": 4915.0, "text": " So now we're going to get into the power method. This is going to be how we're calculating our eigenvector.", "tokens": [407, 586, 321, 434, 516, 281, 483, 666, 264, 1347, 3170, 13, 639, 307, 516, 281, 312, 577, 321, 434, 28258, 527, 10446, 303, 1672, 13], "temperature": 0.0, "avg_logprob": -0.11045372163927233, "compression_ratio": 1.5425531914893618, "no_speech_prob": 8.6638901848346e-06}, {"id": 598, "seek": 491000, "start": 4915.0, "end": 4934.0, "text": " And again, our idea is that if you know, if you're just looking at Wikipedia is kind of like a stand in for the Internet, we're trying to find like what the most important pages are.", "tokens": [400, 797, 11, 527, 1558, 307, 300, 498, 291, 458, 11, 498, 291, 434, 445, 1237, 412, 28999, 307, 733, 295, 411, 257, 1463, 294, 337, 264, 7703, 11, 321, 434, 1382, 281, 915, 411, 437, 264, 881, 1021, 7183, 366, 13], "temperature": 0.0, "avg_logprob": -0.11045372163927233, "compression_ratio": 1.5425531914893618, "no_speech_prob": 8.6638901848346e-06}, {"id": 599, "seek": 493400, "start": 4934.0, "end": 4943.0, "text": " Actually, I'm going to. Skip this part well. So this is.", "tokens": [5135, 11, 286, 478, 516, 281, 13, 46405, 341, 644, 731, 13, 407, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.19243838970477764, "compression_ratio": 1.3953488372093024, "no_speech_prob": 1.4063093658478465e-05}, {"id": 600, "seek": 493400, "start": 4943.0, "end": 4950.0, "text": " Similar to what we talked about below with the matrix powers, but the idea is that a matrix.", "tokens": [10905, 281, 437, 321, 2825, 466, 2507, 365, 264, 8141, 8674, 11, 457, 264, 1558, 307, 300, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.19243838970477764, "compression_ratio": 1.3953488372093024, "no_speech_prob": 1.4063093658478465e-05}, {"id": 601, "seek": 493400, "start": 4950.0, "end": 4958.0, "text": " So a matrix is diagonalizable if it has n linearly independent eigenvectors V1 through VN.", "tokens": [407, 257, 8141, 307, 21539, 22395, 498, 309, 575, 297, 43586, 6695, 10446, 303, 5547, 691, 16, 807, 691, 45, 13], "temperature": 0.0, "avg_logprob": -0.19243838970477764, "compression_ratio": 1.3953488372093024, "no_speech_prob": 1.4063093658478465e-05}, {"id": 602, "seek": 495800, "start": 4958.0, "end": 4965.0, "text": " And then any any vector can be expressed as a linear combination of the eigenvectors.", "tokens": [400, 550, 604, 604, 8062, 393, 312, 12675, 382, 257, 8213, 6562, 295, 264, 10446, 303, 5547, 13], "temperature": 0.0, "avg_logprob": -0.07170455963885197, "compression_ratio": 1.6866666666666668, "no_speech_prob": 5.255190899333684e-06}, {"id": 603, "seek": 495800, "start": 4965.0, "end": 4979.0, "text": " And that's really handy because when we're looking at the matrix times a vector, having how it's expressed as an eigenvector or expressed in terms of the eigenvectors,", "tokens": [400, 300, 311, 534, 13239, 570, 562, 321, 434, 1237, 412, 264, 8141, 1413, 257, 8062, 11, 1419, 577, 309, 311, 12675, 382, 364, 10446, 303, 1672, 420, 12675, 294, 2115, 295, 264, 10446, 303, 5547, 11], "temperature": 0.0, "avg_logprob": -0.07170455963885197, "compression_ratio": 1.6866666666666668, "no_speech_prob": 5.255190899333684e-06}, {"id": 604, "seek": 497900, "start": 4979.0, "end": 4991.0, "text": " let's just multiply them by the eigenvalue instead of having to do a matrix multiplication.", "tokens": [718, 311, 445, 12972, 552, 538, 264, 10446, 29155, 2602, 295, 1419, 281, 360, 257, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.1672270731492476, "compression_ratio": 1.1375, "no_speech_prob": 2.5612034733057953e-06}, {"id": 605, "seek": 499100, "start": 4991.0, "end": 5020.0, "text": " And let me even say I want to. I want to write that one out.", "tokens": [400, 718, 385, 754, 584, 286, 528, 281, 13, 286, 528, 281, 2464, 300, 472, 484, 13], "temperature": 0.0, "avg_logprob": -0.053693476177397226, "compression_ratio": 1.0169491525423728, "no_speech_prob": 3.943413685192354e-05}, {"id": 606, "seek": 502000, "start": 5020.0, "end": 5022.0, "text": " So let me start on a new page.", "tokens": [407, 718, 385, 722, 322, 257, 777, 3028, 13], "temperature": 0.0, "avg_logprob": -0.16594468846040614, "compression_ratio": 1.4516129032258065, "no_speech_prob": 1.834226532082539e-05}, {"id": 607, "seek": 502000, "start": 5022.0, "end": 5026.0, "text": " But so if any vector W.", "tokens": [583, 370, 498, 604, 8062, 343, 13], "temperature": 0.0, "avg_logprob": -0.16594468846040614, "compression_ratio": 1.4516129032258065, "no_speech_prob": 1.834226532082539e-05}, {"id": 608, "seek": 502000, "start": 5026.0, "end": 5032.0, "text": " Can be written as a linear combination for some scalars, call them CJ.", "tokens": [1664, 312, 3720, 382, 257, 8213, 6562, 337, 512, 15664, 685, 11, 818, 552, 42285, 13], "temperature": 0.0, "avg_logprob": -0.16594468846040614, "compression_ratio": 1.4516129032258065, "no_speech_prob": 1.834226532082539e-05}, {"id": 609, "seek": 502000, "start": 5032.0, "end": 5039.0, "text": " Of the eigenvectors VJ, and that's because the V form the eigenvectors form a basis for your space.", "tokens": [2720, 264, 10446, 303, 5547, 691, 41, 11, 293, 300, 311, 570, 264, 691, 1254, 264, 10446, 303, 5547, 1254, 257, 5143, 337, 428, 1901, 13], "temperature": 0.0, "avg_logprob": -0.16594468846040614, "compression_ratio": 1.4516129032258065, "no_speech_prob": 1.834226532082539e-05}, {"id": 610, "seek": 503900, "start": 5039.0, "end": 5054.0, "text": " Then when you want to do a W.", "tokens": [1396, 562, 291, 528, 281, 360, 257, 343, 13], "temperature": 0.0, "avg_logprob": -0.20617057147778964, "compression_ratio": 1.1274509803921569, "no_speech_prob": 4.710864686785499e-06}, {"id": 611, "seek": 503900, "start": 5054.0, "end": 5058.0, "text": " Pull this A inside.", "tokens": [15074, 341, 316, 1854, 13], "temperature": 0.0, "avg_logprob": -0.20617057147778964, "compression_ratio": 1.1274509803921569, "no_speech_prob": 4.710864686785499e-06}, {"id": 612, "seek": 503900, "start": 5058.0, "end": 5064.0, "text": " Remember C is just a scalar, so we can pull the matrix into that.", "tokens": [5459, 383, 307, 445, 257, 39684, 11, 370, 321, 393, 2235, 264, 8141, 666, 300, 13], "temperature": 0.0, "avg_logprob": -0.20617057147778964, "compression_ratio": 1.1274509803921569, "no_speech_prob": 4.710864686785499e-06}, {"id": 613, "seek": 506400, "start": 5064.0, "end": 5076.0, "text": " And how can I rewrite this?", "tokens": [400, 577, 393, 286, 28132, 341, 30], "temperature": 0.0, "avg_logprob": -0.11192928660999645, "compression_ratio": 0.7714285714285715, "no_speech_prob": 1.6963194866548292e-05}, {"id": 614, "seek": 507600, "start": 5076.0, "end": 5105.0, "text": " The point is that the vectors V are eigenvectors of A.", "tokens": [440, 935, 307, 300, 264, 18875, 691, 366, 10446, 303, 5547, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.2617810567220052, "compression_ratio": 1.0, "no_speech_prob": 5.388826320995577e-05}, {"id": 615, "seek": 510500, "start": 5105.0, "end": 5110.0, "text": " And wait, this one throw the microphone to Valentine.", "tokens": [400, 1699, 11, 341, 472, 3507, 264, 10952, 281, 24359, 13], "temperature": 0.0, "avg_logprob": -0.25028201861259264, "compression_ratio": 1.4818652849740932, "no_speech_prob": 3.763005224755034e-05}, {"id": 616, "seek": 510500, "start": 5110.0, "end": 5118.0, "text": " So can we just substitute a VJ with lambda J VJ? Exactly. Yes.", "tokens": [407, 393, 321, 445, 15802, 257, 691, 41, 365, 13607, 508, 691, 41, 30, 7587, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.25028201861259264, "compression_ratio": 1.4818652849740932, "no_speech_prob": 3.763005224755034e-05}, {"id": 617, "seek": 510500, "start": 5118.0, "end": 5120.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.25028201861259264, "compression_ratio": 1.4818652849740932, "no_speech_prob": 3.763005224755034e-05}, {"id": 618, "seek": 510500, "start": 5120.0, "end": 5130.0, "text": " And this is just kind of the definition of what it means to be an eigenvector is that instead of having to multiply by a matrix, you can just multiply by a scalar.", "tokens": [400, 341, 307, 445, 733, 295, 264, 7123, 295, 437, 309, 1355, 281, 312, 364, 10446, 303, 1672, 307, 300, 2602, 295, 1419, 281, 12972, 538, 257, 8141, 11, 291, 393, 445, 12972, 538, 257, 39684, 13], "temperature": 0.0, "avg_logprob": -0.25028201861259264, "compression_ratio": 1.4818652849740932, "no_speech_prob": 3.763005224755034e-05}, {"id": 619, "seek": 513000, "start": 5130.0, "end": 5136.0, "text": " And so this is really nice because.", "tokens": [400, 370, 341, 307, 534, 1481, 570, 13], "temperature": 0.0, "avg_logprob": -0.1547095833755121, "compression_ratio": 1.3306451612903225, "no_speech_prob": 2.7967720598098822e-05}, {"id": 620, "seek": 513000, "start": 5136.0, "end": 5144.0, "text": " There is nothing special about W here. W was any vector and it was being represented as a linear combination of the eigenvectors.", "tokens": [821, 307, 1825, 2121, 466, 343, 510, 13, 343, 390, 604, 8062, 293, 309, 390, 885, 10379, 382, 257, 8213, 6562, 295, 264, 10446, 303, 5547, 13], "temperature": 0.0, "avg_logprob": -0.1547095833755121, "compression_ratio": 1.3306451612903225, "no_speech_prob": 2.7967720598098822e-05}, {"id": 621, "seek": 514400, "start": 5144.0, "end": 5160.0, "text": " And so we're kind of saying, oh, for any vector, you don't actually have to do the matrix multiplication. You can just use the eigenvalues, kind of multiplying by the basis.", "tokens": [400, 370, 321, 434, 733, 295, 1566, 11, 1954, 11, 337, 604, 8062, 11, 291, 500, 380, 767, 362, 281, 360, 264, 8141, 27290, 13, 509, 393, 445, 764, 264, 10446, 46033, 11, 733, 295, 30955, 538, 264, 5143, 13], "temperature": 0.0, "avg_logprob": -0.13205708503723146, "compression_ratio": 1.3448275862068966, "no_speech_prob": 9.223192137142178e-06}, {"id": 622, "seek": 514400, "start": 5160.0, "end": 5167.0, "text": " Questions about this?", "tokens": [27738, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.13205708503723146, "compression_ratio": 1.3448275862068966, "no_speech_prob": 9.223192137142178e-06}, {"id": 623, "seek": 516700, "start": 5167.0, "end": 5180.0, "text": " And so something to keep in mind is that you are taking powers of A, this and I'm not going to write it all out, but this basically becomes.", "tokens": [400, 370, 746, 281, 1066, 294, 1575, 307, 300, 291, 366, 1940, 8674, 295, 316, 11, 341, 293, 286, 478, 406, 516, 281, 2464, 309, 439, 484, 11, 457, 341, 1936, 3643, 13], "temperature": 0.0, "avg_logprob": -0.1691940724849701, "compression_ratio": 1.4705882352941178, "no_speech_prob": 9.97250754153356e-06}, {"id": 624, "seek": 516700, "start": 5180.0, "end": 5182.0, "text": " The eigenvalue to a power.", "tokens": [440, 10446, 29155, 281, 257, 1347, 13], "temperature": 0.0, "avg_logprob": -0.1691940724849701, "compression_ratio": 1.4705882352941178, "no_speech_prob": 9.97250754153356e-06}, {"id": 625, "seek": 516700, "start": 5182.0, "end": 5184.0, "text": " And so it's.", "tokens": [400, 370, 309, 311, 13], "temperature": 0.0, "avg_logprob": -0.1691940724849701, "compression_ratio": 1.4705882352941178, "no_speech_prob": 9.97250754153356e-06}, {"id": 626, "seek": 516700, "start": 5184.0, "end": 5194.0, "text": " That's significant with the eigenvalues are.", "tokens": [663, 311, 4776, 365, 264, 10446, 46033, 366, 13], "temperature": 0.0, "avg_logprob": -0.1691940724849701, "compression_ratio": 1.4705882352941178, "no_speech_prob": 9.97250754153356e-06}, {"id": 627, "seek": 519400, "start": 5194.0, "end": 5198.0, "text": " And so.", "tokens": [400, 370, 13], "temperature": 0.0, "avg_logprob": -0.11540538614446466, "compression_ratio": 1.5407725321888412, "no_speech_prob": 1.4509400898532476e-05}, {"id": 628, "seek": 519400, "start": 5198.0, "end": 5201.0, "text": " With our with our.", "tokens": [2022, 527, 365, 527, 13], "temperature": 0.0, "avg_logprob": -0.11540538614446466, "compression_ratio": 1.5407725321888412, "no_speech_prob": 1.4509400898532476e-05}, {"id": 629, "seek": 519400, "start": 5201.0, "end": 5204.0, "text": " Adjacency graph of the connections between the different web pages.", "tokens": [1999, 19586, 3020, 4295, 295, 264, 9271, 1296, 264, 819, 3670, 7183, 13], "temperature": 0.0, "avg_logprob": -0.11540538614446466, "compression_ratio": 1.5407725321888412, "no_speech_prob": 1.4509400898532476e-05}, {"id": 630, "seek": 519400, "start": 5204.0, "end": 5216.0, "text": " This is also basically kind of like if you normalized it, it would be like the Markov chain, you know, like probabilities of going from one one page to another.", "tokens": [639, 307, 611, 1936, 733, 295, 411, 498, 291, 48704, 309, 11, 309, 576, 312, 411, 264, 3934, 5179, 5021, 11, 291, 458, 11, 411, 33783, 295, 516, 490, 472, 472, 3028, 281, 1071, 13], "temperature": 0.0, "avg_logprob": -0.11540538614446466, "compression_ratio": 1.5407725321888412, "no_speech_prob": 1.4509400898532476e-05}, {"id": 631, "seek": 519400, "start": 5216.0, "end": 5221.0, "text": " So you can think of someone was randomly surfing the web and just happened to click on different links.", "tokens": [407, 291, 393, 519, 295, 1580, 390, 16979, 34181, 264, 3670, 293, 445, 2011, 281, 2052, 322, 819, 6123, 13], "temperature": 0.0, "avg_logprob": -0.11540538614446466, "compression_ratio": 1.5407725321888412, "no_speech_prob": 1.4509400898532476e-05}, {"id": 632, "seek": 522100, "start": 5221.0, "end": 5226.0, "text": " Where would they end up? You can kind of get that behavior from.", "tokens": [2305, 576, 436, 917, 493, 30, 509, 393, 733, 295, 483, 300, 5223, 490, 13], "temperature": 0.0, "avg_logprob": -0.09544428554149943, "compression_ratio": 1.7019607843137254, "no_speech_prob": 4.0927807276602834e-06}, {"id": 633, "seek": 522100, "start": 5226.0, "end": 5234.0, "text": " From a and so you're thinking about these kind of repeated powers of, you know, someone's kind of on the Internet or on Wikipedia.", "tokens": [3358, 257, 293, 370, 291, 434, 1953, 466, 613, 733, 295, 10477, 8674, 295, 11, 291, 458, 11, 1580, 311, 733, 295, 322, 264, 7703, 420, 322, 28999, 13], "temperature": 0.0, "avg_logprob": -0.09544428554149943, "compression_ratio": 1.7019607843137254, "no_speech_prob": 4.0927807276602834e-06}, {"id": 634, "seek": 522100, "start": 5234.0, "end": 5242.0, "text": " And if they're just like randomly clicking links, you know, over time, kind of what are the most important pages or what pages are they going to go to most often?", "tokens": [400, 498, 436, 434, 445, 411, 16979, 9697, 6123, 11, 291, 458, 11, 670, 565, 11, 733, 295, 437, 366, 264, 881, 1021, 7183, 420, 437, 7183, 366, 436, 516, 281, 352, 281, 881, 2049, 30], "temperature": 0.0, "avg_logprob": -0.09544428554149943, "compression_ratio": 1.7019607843137254, "no_speech_prob": 4.0927807276602834e-06}, {"id": 635, "seek": 522100, "start": 5242.0, "end": 5247.0, "text": " And so that's why I will be kind of talking about powers of a because it's.", "tokens": [400, 370, 300, 311, 983, 286, 486, 312, 733, 295, 1417, 466, 8674, 295, 257, 570, 309, 311, 13], "temperature": 0.0, "avg_logprob": -0.09544428554149943, "compression_ratio": 1.7019607843137254, "no_speech_prob": 4.0927807276602834e-06}, {"id": 636, "seek": 524700, "start": 5247.0, "end": 5259.0, "text": " You're kind of going from page to page again and again.", "tokens": [509, 434, 733, 295, 516, 490, 3028, 281, 3028, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.0871235686288753, "compression_ratio": 1.6265060240963856, "no_speech_prob": 1.7501939510111697e-05}, {"id": 637, "seek": 524700, "start": 5259.0, "end": 5265.0, "text": " Yeah, so we're going to we're going to normalize our matrix, and this is necessary.", "tokens": [865, 11, 370, 321, 434, 516, 281, 321, 434, 516, 281, 2710, 1125, 527, 8141, 11, 293, 341, 307, 4818, 13], "temperature": 0.0, "avg_logprob": -0.0871235686288753, "compression_ratio": 1.6265060240963856, "no_speech_prob": 1.7501939510111697e-05}, {"id": 638, "seek": 524700, "start": 5265.0, "end": 5270.0, "text": " Yeah, in terms of thinking about probabilities also.", "tokens": [865, 11, 294, 2115, 295, 1953, 466, 33783, 611, 13], "temperature": 0.0, "avg_logprob": -0.0871235686288753, "compression_ratio": 1.6265060240963856, "no_speech_prob": 1.7501939510111697e-05}, {"id": 639, "seek": 524700, "start": 5270.0, "end": 5275.0, "text": " Yeah, this keeps it as we take these huge powers from from getting too large.", "tokens": [865, 11, 341, 5965, 309, 382, 321, 747, 613, 2603, 8674, 490, 490, 1242, 886, 2416, 13], "temperature": 0.0, "avg_logprob": -0.0871235686288753, "compression_ratio": 1.6265060240963856, "no_speech_prob": 1.7501939510111697e-05}, {"id": 640, "seek": 527500, "start": 5275.0, "end": 5288.0, "text": " And you can do that using num pies some method over a particular axis axis.", "tokens": [400, 291, 393, 360, 300, 1228, 1031, 29640, 512, 3170, 670, 257, 1729, 10298, 10298, 13], "temperature": 0.0, "avg_logprob": -0.16790990829467772, "compression_ratio": 1.28125, "no_speech_prob": 2.840869638021104e-05}, {"id": 641, "seek": 527500, "start": 5288.0, "end": 5294.0, "text": " Yeah, so down here we have the power method.", "tokens": [865, 11, 370, 760, 510, 321, 362, 264, 1347, 3170, 13], "temperature": 0.0, "avg_logprob": -0.16790990829467772, "compression_ratio": 1.28125, "no_speech_prob": 2.840869638021104e-05}, {"id": 642, "seek": 527500, "start": 5294.0, "end": 5296.0, "text": " What we'll do is.", "tokens": [708, 321, 603, 360, 307, 13], "temperature": 0.0, "avg_logprob": -0.16790990829467772, "compression_ratio": 1.28125, "no_speech_prob": 2.840869638021104e-05}, {"id": 643, "seek": 527500, "start": 5296.0, "end": 5299.0, "text": " Kind of use our our data.", "tokens": [9242, 295, 764, 527, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16790990829467772, "compression_ratio": 1.28125, "no_speech_prob": 2.840869638021104e-05}, {"id": 644, "seek": 529900, "start": 5299.0, "end": 5306.0, "text": " Then we're going to want to get the.", "tokens": [1396, 321, 434, 516, 281, 528, 281, 483, 264, 13], "temperature": 0.0, "avg_logprob": -0.1577409397472035, "compression_ratio": 1.4645669291338583, "no_speech_prob": 1.5445288227056153e-05}, {"id": 645, "seek": 529900, "start": 5306.0, "end": 5315.0, "text": " Kind of the indices, so a dot indices will give us the indices that are non zero and we want to select those.", "tokens": [9242, 295, 264, 43840, 11, 370, 257, 5893, 43840, 486, 976, 505, 264, 43840, 300, 366, 2107, 4018, 293, 321, 528, 281, 3048, 729, 13], "temperature": 0.0, "avg_logprob": -0.1577409397472035, "compression_ratio": 1.4645669291338583, "no_speech_prob": 1.5445288227056153e-05}, {"id": 646, "seek": 529900, "start": 5315.0, "end": 5321.0, "text": " To sum up, so we kind of going back to.", "tokens": [1407, 2408, 493, 11, 370, 321, 733, 295, 516, 646, 281, 13], "temperature": 0.0, "avg_logprob": -0.1577409397472035, "compression_ratio": 1.4645669291338583, "no_speech_prob": 1.5445288227056153e-05}, {"id": 647, "seek": 532100, "start": 5321.0, "end": 5330.0, "text": " If we just had that small four by four graph with the zeros and ones were interested in the non zero entries.", "tokens": [759, 321, 445, 632, 300, 1359, 1451, 538, 1451, 4295, 365, 264, 35193, 293, 2306, 645, 3102, 294, 264, 2107, 4018, 23041, 13], "temperature": 0.0, "avg_logprob": -0.11689648032188416, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.3550675905426033e-05}, {"id": 648, "seek": 532100, "start": 5330.0, "end": 5339.0, "text": " I would want to if we had three non zero entries, one one one, we would want to change that to one third, one third, one third for that row.", "tokens": [286, 576, 528, 281, 498, 321, 632, 1045, 2107, 4018, 23041, 11, 472, 472, 472, 11, 321, 576, 528, 281, 1319, 300, 281, 472, 2636, 11, 472, 2636, 11, 472, 2636, 337, 300, 5386, 13], "temperature": 0.0, "avg_logprob": -0.11689648032188416, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.3550675905426033e-05}, {"id": 649, "seek": 533900, "start": 5339.0, "end": 5351.0, "text": " We'll take scores is a vector of ones of length and times the square root of a dot sum divided by N squared.", "tokens": [492, 603, 747, 13444, 307, 257, 8062, 295, 2306, 295, 4641, 293, 1413, 264, 3732, 5593, 295, 257, 5893, 2408, 6666, 538, 426, 8889, 13], "temperature": 0.0, "avg_logprob": -0.11641440568146882, "compression_ratio": 1.4503311258278146, "no_speech_prob": 2.947892653537565e-06}, {"id": 650, "seek": 533900, "start": 5351.0, "end": 5362.0, "text": " And this is just kind of an initial guess that people are or that the pages kind of all have equal importance.", "tokens": [400, 341, 307, 445, 733, 295, 364, 5883, 2041, 300, 561, 366, 420, 300, 264, 7183, 733, 295, 439, 362, 2681, 7379, 13], "temperature": 0.0, "avg_logprob": -0.11641440568146882, "compression_ratio": 1.4503311258278146, "no_speech_prob": 2.947892653537565e-06}, {"id": 651, "seek": 536200, "start": 5362.0, "end": 5376.0, "text": " And then we do a time scores at the norm, normalize and continue iterating a time scores again and so on.", "tokens": [400, 550, 321, 360, 257, 565, 13444, 412, 264, 2026, 11, 2710, 1125, 293, 2354, 17138, 990, 257, 565, 13444, 797, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1410310791759956, "compression_ratio": 1.6564102564102565, "no_speech_prob": 6.143916380096925e-06}, {"id": 652, "seek": 536200, "start": 5376.0, "end": 5381.0, "text": " And so what this is doing is you can also kind of think of.", "tokens": [400, 370, 437, 341, 307, 884, 307, 291, 393, 611, 733, 295, 519, 295, 13], "temperature": 0.0, "avg_logprob": -0.1410310791759956, "compression_ratio": 1.6564102564102565, "no_speech_prob": 6.143916380096925e-06}, {"id": 653, "seek": 536200, "start": 5381.0, "end": 5389.0, "text": " Think of it as you know, if you have a thousand people on Wikipedia randomly clicking links, you know, you're seeing where they go step over step after step.", "tokens": [6557, 295, 309, 382, 291, 458, 11, 498, 291, 362, 257, 4714, 561, 322, 28999, 16979, 9697, 6123, 11, 291, 458, 11, 291, 434, 2577, 689, 436, 352, 1823, 670, 1823, 934, 1823, 13], "temperature": 0.0, "avg_logprob": -0.1410310791759956, "compression_ratio": 1.6564102564102565, "no_speech_prob": 6.143916380096925e-06}, {"id": 654, "seek": 538900, "start": 5389.0, "end": 5402.0, "text": " If you did this with enough people for a long enough time, you would find this kind of distribution of like more people are on this page and not many people are on these pages.", "tokens": [759, 291, 630, 341, 365, 1547, 561, 337, 257, 938, 1547, 565, 11, 291, 576, 915, 341, 733, 295, 7316, 295, 411, 544, 561, 366, 322, 341, 3028, 293, 406, 867, 561, 366, 322, 613, 7183, 13], "temperature": 0.0, "avg_logprob": -0.08203720728556316, "compression_ratio": 1.6170212765957446, "no_speech_prob": 1.0029879149442422e-06}, {"id": 655, "seek": 538900, "start": 5402.0, "end": 5405.0, "text": " Yeah, so this is kind of the idea behind the power method.", "tokens": [865, 11, 370, 341, 307, 733, 295, 264, 1558, 2261, 264, 1347, 3170, 13], "temperature": 0.0, "avg_logprob": -0.08203720728556316, "compression_ratio": 1.6170212765957446, "no_speech_prob": 1.0029879149442422e-06}, {"id": 656, "seek": 538900, "start": 5405.0, "end": 5410.0, "text": " Why, why do you think we're normalizing the score on each iteration?", "tokens": [1545, 11, 983, 360, 291, 519, 321, 434, 2710, 3319, 264, 6175, 322, 1184, 24784, 30], "temperature": 0.0, "avg_logprob": -0.08203720728556316, "compression_ratio": 1.6170212765957446, "no_speech_prob": 1.0029879149442422e-06}, {"id": 657, "seek": 541000, "start": 5410.0, "end": 5422.0, "text": " And the scores just kind of the percent of people on the different pages.", "tokens": [400, 264, 13444, 445, 733, 295, 264, 3043, 295, 561, 322, 264, 819, 7183, 13], "temperature": 0.0, "avg_logprob": -0.17026220049176896, "compression_ratio": 1.1818181818181819, "no_speech_prob": 4.4251819417695515e-06}, {"id": 658, "seek": 541000, "start": 5422.0, "end": 5427.0, "text": " Matthew and Roger has the mic.", "tokens": [12434, 293, 17666, 575, 264, 3123, 13], "temperature": 0.0, "avg_logprob": -0.17026220049176896, "compression_ratio": 1.1818181818181819, "no_speech_prob": 4.4251819417695515e-06}, {"id": 659, "seek": 542700, "start": 5427.0, "end": 5442.0, "text": " Oh, great catch and throw.", "tokens": [876, 11, 869, 3745, 293, 3507, 13], "temperature": 0.0, "avg_logprob": -0.341400146484375, "compression_ratio": 0.8536585365853658, "no_speech_prob": 1.147805687651271e-05}, {"id": 660, "seek": 542700, "start": 5442.0, "end": 5447.0, "text": " So that.", "tokens": [407, 300, 13], "temperature": 0.0, "avg_logprob": -0.341400146484375, "compression_ratio": 0.8536585365853658, "no_speech_prob": 1.147805687651271e-05}, {"id": 661, "seek": 544700, "start": 5447.0, "end": 5457.0, "text": " And so this is a little bit confusing. We actually normalized twice, so we kind of that's what we're doing up here is trying to like normalize the counts by row and then down here.", "tokens": [400, 370, 341, 307, 257, 707, 857, 13181, 13, 492, 767, 48704, 6091, 11, 370, 321, 733, 295, 300, 311, 437, 321, 434, 884, 493, 510, 307, 1382, 281, 411, 2710, 1125, 264, 14893, 538, 5386, 293, 550, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.12008937008409615, "compression_ratio": 1.6009389671361502, "no_speech_prob": 8.013026672415435e-06}, {"id": 662, "seek": 544700, "start": 5457.0, "end": 5461.0, "text": " I called it scores because it's kind of like the importance of the different pages.", "tokens": [286, 1219, 309, 13444, 570, 309, 311, 733, 295, 411, 264, 7379, 295, 264, 819, 7183, 13], "temperature": 0.0, "avg_logprob": -0.12008937008409615, "compression_ratio": 1.6009389671361502, "no_speech_prob": 8.013026672415435e-06}, {"id": 663, "seek": 544700, "start": 5461.0, "end": 5467.0, "text": " But you could also think of that as like the percent of people on each page.", "tokens": [583, 291, 727, 611, 519, 295, 300, 382, 411, 264, 3043, 295, 561, 322, 1184, 3028, 13], "temperature": 0.0, "avg_logprob": -0.12008937008409615, "compression_ratio": 1.6009389671361502, "no_speech_prob": 8.013026672415435e-06}, {"id": 664, "seek": 546700, "start": 5467.0, "end": 5486.0, "text": " So kind of the question I was asking was why are we normalizing the scores here?", "tokens": [407, 733, 295, 264, 1168, 286, 390, 3365, 390, 983, 366, 321, 2710, 3319, 264, 13444, 510, 30], "temperature": 0.0, "avg_logprob": -0.25272779031233356, "compression_ratio": 1.0389610389610389, "no_speech_prob": 7.646232916158624e-06}, {"id": 665, "seek": 548600, "start": 5486.0, "end": 5498.0, "text": " Yes, yeah, so the issue that could arise was the values could get way too small and underflow to zero.", "tokens": [1079, 11, 1338, 11, 370, 264, 2734, 300, 727, 20288, 390, 264, 4190, 727, 483, 636, 886, 1359, 293, 833, 10565, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.12532701297682158, "compression_ratio": 1.711297071129707, "no_speech_prob": 4.637624897441128e-06}, {"id": 666, "seek": 548600, "start": 5498.0, "end": 5504.0, "text": " Or we could also have problems with the values getting way too big if we don't normalize and exploding.", "tokens": [1610, 321, 727, 611, 362, 2740, 365, 264, 4190, 1242, 636, 886, 955, 498, 321, 500, 380, 2710, 1125, 293, 35175, 13], "temperature": 0.0, "avg_logprob": -0.12532701297682158, "compression_ratio": 1.711297071129707, "no_speech_prob": 4.637624897441128e-06}, {"id": 667, "seek": 548600, "start": 5504.0, "end": 5515.0, "text": " So that's why kind of whenever it's good, whenever you're doing an iterative process where you're multiplying every time, it's really good to think about normalizing because you don't want things to be.", "tokens": [407, 300, 311, 983, 733, 295, 5699, 309, 311, 665, 11, 5699, 291, 434, 884, 364, 17138, 1166, 1399, 689, 291, 434, 30955, 633, 565, 11, 309, 311, 534, 665, 281, 519, 466, 2710, 3319, 570, 291, 500, 380, 528, 721, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.12532701297682158, "compression_ratio": 1.711297071129707, "no_speech_prob": 4.637624897441128e-06}, {"id": 668, "seek": 551500, "start": 5515.0, "end": 5533.0, "text": " Exploding or vanishing.", "tokens": [12514, 8616, 420, 3161, 3807, 13], "temperature": 0.0, "avg_logprob": -0.18804423809051513, "compression_ratio": 0.7931034482758621, "no_speech_prob": 7.2959578574227635e-06}, {"id": 669, "seek": 553300, "start": 5533.0, "end": 5551.0, "text": " And so then here we can check like what are the scores we get here. We've done it for 10 iterations and the the top kind of the top pages are living people year of birth missing.", "tokens": [400, 370, 550, 510, 321, 393, 1520, 411, 437, 366, 264, 13444, 321, 483, 510, 13, 492, 600, 1096, 309, 337, 1266, 36540, 293, 264, 264, 1192, 733, 295, 264, 1192, 7183, 366, 2647, 561, 1064, 295, 3965, 5361, 13], "temperature": 0.0, "avg_logprob": -0.15602688562302364, "compression_ratio": 1.5142857142857142, "no_speech_prob": 7.181875389505876e-06}, {"id": 670, "seek": 553300, "start": 5551.0, "end": 5557.0, "text": " United States, United Kingdom, race and ethnicity in the United States, census France.", "tokens": [2824, 3040, 11, 2824, 11277, 11, 4569, 293, 33774, 294, 264, 2824, 3040, 11, 23725, 6190, 13], "temperature": 0.0, "avg_logprob": -0.15602688562302364, "compression_ratio": 1.5142857142857142, "no_speech_prob": 7.181875389505876e-06}, {"id": 671, "seek": 555700, "start": 5557.0, "end": 5563.0, "text": " And so what this is is these are the pages that have kind of the most links pointing to them.", "tokens": [400, 370, 437, 341, 307, 307, 613, 366, 264, 7183, 300, 362, 733, 295, 264, 881, 6123, 12166, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.07801187379019602, "compression_ratio": 1.6117021276595744, "no_speech_prob": 1.4283431482908782e-05}, {"id": 672, "seek": 555700, "start": 5563.0, "end": 5570.0, "text": " And this is actually not not as interesting as it would be if we were using like the whole Internet or something.", "tokens": [400, 341, 307, 767, 406, 406, 382, 1880, 382, 309, 576, 312, 498, 321, 645, 1228, 411, 264, 1379, 7703, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.07801187379019602, "compression_ratio": 1.6117021276595744, "no_speech_prob": 1.4283431482908782e-05}, {"id": 673, "seek": 555700, "start": 5570.0, "end": 5577.0, "text": " Right. Because here all people have a link pointing to the living people category on Wikipedia.", "tokens": [1779, 13, 1436, 510, 439, 561, 362, 257, 2113, 12166, 281, 264, 2647, 561, 7719, 322, 28999, 13], "temperature": 0.0, "avg_logprob": -0.07801187379019602, "compression_ratio": 1.6117021276595744, "no_speech_prob": 1.4283431482908782e-05}, {"id": 674, "seek": 557700, "start": 5577.0, "end": 5589.0, "text": " So that's why that's a super scene is a super popular page because there would be a ton of links pointing to that kind of for any entry of a person and then your birth missing.", "tokens": [407, 300, 311, 983, 300, 311, 257, 1687, 4145, 307, 257, 1687, 3743, 3028, 570, 456, 576, 312, 257, 2952, 295, 6123, 12166, 281, 300, 733, 295, 337, 604, 8729, 295, 257, 954, 293, 550, 428, 3965, 5361, 13], "temperature": 0.0, "avg_logprob": -0.11494901180267333, "compression_ratio": 1.6807511737089202, "no_speech_prob": 1.4144358146950253e-06}, {"id": 675, "seek": 557700, "start": 5589.0, "end": 5600.0, "text": " It seems reasonable that actually a lot of people probably don't have their year of birth miss listed on Wikipedia and that those pages all point to this year of birth missing page.", "tokens": [467, 2544, 10585, 300, 767, 257, 688, 295, 561, 1391, 500, 380, 362, 641, 1064, 295, 3965, 1713, 10052, 322, 28999, 293, 300, 729, 7183, 439, 935, 281, 341, 1064, 295, 3965, 5361, 3028, 13], "temperature": 0.0, "avg_logprob": -0.11494901180267333, "compression_ratio": 1.6807511737089202, "no_speech_prob": 1.4144358146950253e-06}, {"id": 676, "seek": 560000, "start": 5600.0, "end": 5607.0, "text": " So this this makes sense as a like OK these are pages that I think a ton of links could be pointing to.", "tokens": [407, 341, 341, 1669, 2020, 382, 257, 411, 2264, 613, 366, 7183, 300, 286, 519, 257, 2952, 295, 6123, 727, 312, 12166, 281, 13], "temperature": 0.0, "avg_logprob": -0.1485550339157517, "compression_ratio": 1.538860103626943, "no_speech_prob": 3.500616912788246e-06}, {"id": 677, "seek": 560000, "start": 5607.0, "end": 5619.0, "text": " So that sense they're important but it's not the same sense that you would probably get with the broader Internet of like hey this is a really popular page.", "tokens": [407, 300, 2020, 436, 434, 1021, 457, 309, 311, 406, 264, 912, 2020, 300, 291, 576, 1391, 483, 365, 264, 13227, 7703, 295, 411, 4177, 341, 307, 257, 534, 3743, 3028, 13], "temperature": 0.0, "avg_logprob": -0.1485550339157517, "compression_ratio": 1.538860103626943, "no_speech_prob": 3.500616912788246e-06}, {"id": 678, "seek": 560000, "start": 5619.0, "end": 5620.0, "text": " Linda.", "tokens": [20324, 13], "temperature": 0.0, "avg_logprob": -0.1485550339157517, "compression_ratio": 1.538860103626943, "no_speech_prob": 3.500616912788246e-06}, {"id": 679, "seek": 560000, "start": 5620.0, "end": 5627.0, "text": " Thank you for the microphone.", "tokens": [1044, 291, 337, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.1485550339157517, "compression_ratio": 1.538860103626943, "no_speech_prob": 3.500616912788246e-06}, {"id": 680, "seek": 562700, "start": 5627.0, "end": 5640.0, "text": " So in the score so like 3.5 answer the score is low to high and then you show X.", "tokens": [407, 294, 264, 6175, 370, 411, 805, 13, 20, 1867, 264, 6175, 307, 2295, 281, 1090, 293, 550, 291, 855, 1783, 13], "temperature": 0.0, "avg_logprob": -0.5125125493758764, "compression_ratio": 1.2242990654205608, "no_speech_prob": 8.937653547036462e-06}, {"id": 681, "seek": 562700, "start": 5640.0, "end": 5646.0, "text": " Does it mean that journey is most like brought up.", "tokens": [4402, 309, 914, 300, 4671, 307, 881, 411, 3038, 493, 13], "temperature": 0.0, "avg_logprob": -0.5125125493758764, "compression_ratio": 1.2242990654205608, "no_speech_prob": 8.937653547036462e-06}, {"id": 682, "seek": 564600, "start": 5646.0, "end": 5658.0, "text": " This is this is confusing what I'm out putting here is actually the norm and that was just to kind of keep track of how the norm is is changing.", "tokens": [639, 307, 341, 307, 13181, 437, 286, 478, 484, 3372, 510, 307, 767, 264, 2026, 293, 300, 390, 445, 281, 733, 295, 1066, 2837, 295, 577, 264, 2026, 307, 307, 4473, 13], "temperature": 0.0, "avg_logprob": -0.27908912457917867, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.288786385586718e-06}, {"id": 683, "seek": 564600, "start": 5658.0, "end": 5668.0, "text": " And then I was the one question before the others is that what is like the third one.", "tokens": [400, 550, 286, 390, 264, 472, 1168, 949, 264, 2357, 307, 300, 437, 307, 411, 264, 2636, 472, 13], "temperature": 0.0, "avg_logprob": -0.27908912457917867, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.288786385586718e-06}, {"id": 684, "seek": 566800, "start": 5668.0, "end": 5678.0, "text": " And so this is what's kind of turning all those ones into fractions of kind of percent.", "tokens": [400, 370, 341, 307, 437, 311, 733, 295, 6246, 439, 729, 2306, 666, 36058, 295, 733, 295, 3043, 13], "temperature": 0.0, "avg_logprob": -0.11632347840529222, "compression_ratio": 1.5739644970414202, "no_speech_prob": 3.3401447581127286e-06}, {"id": 685, "seek": 566800, "start": 5678.0, "end": 5689.0, "text": " So you know before we had like you know a links to be C and D and we want to convert those to one third one third one third because that's your probability of going to each page.", "tokens": [407, 291, 458, 949, 321, 632, 411, 291, 458, 257, 6123, 281, 312, 383, 293, 413, 293, 321, 528, 281, 7620, 729, 281, 472, 2636, 472, 2636, 472, 2636, 570, 300, 311, 428, 8482, 295, 516, 281, 1184, 3028, 13], "temperature": 0.0, "avg_logprob": -0.11632347840529222, "compression_ratio": 1.5739644970414202, "no_speech_prob": 3.3401447581127286e-06}, {"id": 686, "seek": 568900, "start": 5689.0, "end": 5698.0, "text": " I just actually to answer when this.", "tokens": [286, 445, 767, 281, 1867, 562, 341, 13], "temperature": 0.0, "avg_logprob": -0.26646775007247925, "compression_ratio": 1.1149425287356323, "no_speech_prob": 3.9051997191563714e-06}, {"id": 687, "seek": 568900, "start": 5698.0, "end": 5706.0, "text": " I was going to see if I could display the last.", "tokens": [286, 390, 516, 281, 536, 498, 286, 727, 4674, 264, 1036, 13], "temperature": 0.0, "avg_logprob": -0.26646775007247925, "compression_ratio": 1.1149425287356323, "no_speech_prob": 3.9051997191563714e-06}, {"id": 688, "seek": 568900, "start": 5706.0, "end": 5717.0, "text": " The last 10.", "tokens": [440, 1036, 1266, 13], "temperature": 0.0, "avg_logprob": -0.26646775007247925, "compression_ratio": 1.1149425287356323, "no_speech_prob": 3.9051997191563714e-06}, {"id": 689, "seek": 571700, "start": 5717.0, "end": 5738.0, "text": " I'll modify that for next time and we can see yeah like what the least important or least link to pages are because that might be fun.", "tokens": [286, 603, 16927, 300, 337, 958, 565, 293, 321, 393, 536, 1338, 411, 437, 264, 1935, 1021, 420, 1935, 2113, 281, 7183, 366, 570, 300, 1062, 312, 1019, 13], "temperature": 0.0, "avg_logprob": -0.1656729235793605, "compression_ratio": 1.276190476190476, "no_speech_prob": 1.8629018086357974e-05}, {"id": 690, "seek": 573800, "start": 5738.0, "end": 5758.0, "text": " Other questions about this.", "tokens": [5358, 1651, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.17917648951212564, "compression_ratio": 0.7714285714285715, "no_speech_prob": 3.168870534864254e-05}, {"id": 691, "seek": 575800, "start": 5758.0, "end": 5774.0, "text": " So I have.", "tokens": [407, 286, 362, 13], "temperature": 0.0, "avg_logprob": -0.4520927667617798, "compression_ratio": 0.5555555555555556, "no_speech_prob": 3.3404503483325243e-06}, {"id": 692, "seek": 577400, "start": 5774.0, "end": 5788.0, "text": " Something called Krylov subspaces and those are kind of the spaces spanned by a times B a squared times B a cubed times B a to the fourth times B.", "tokens": [6595, 1219, 37747, 28257, 2090, 79, 2116, 293, 729, 366, 733, 295, 264, 7673, 637, 5943, 538, 257, 1413, 363, 257, 8889, 1413, 363, 257, 36510, 1413, 363, 257, 281, 264, 6409, 1413, 363, 13], "temperature": 0.0, "avg_logprob": -0.18833647018823868, "compression_ratio": 1.3773584905660377, "no_speech_prob": 1.1125189303129446e-05}, {"id": 693, "seek": 578800, "start": 5788.0, "end": 5804.0, "text": " And so I just wanted to highlight that we're we're kind of getting that here or we are getting that here by taking powers of a each time we go through this for loop.", "tokens": [400, 370, 286, 445, 1415, 281, 5078, 300, 321, 434, 321, 434, 733, 295, 1242, 300, 510, 420, 321, 366, 1242, 300, 510, 538, 1940, 8674, 295, 257, 1184, 565, 321, 352, 807, 341, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.07868449280901653, "compression_ratio": 1.4224137931034482, "no_speech_prob": 5.014510406908812e-06}, {"id": 694, "seek": 580400, "start": 5804.0, "end": 5818.0, "text": " The other thing to note and we'll see this when we get to the QR algorithm is that the convergence rate of this method is the ratio of the largest eigenvalue to the second largest eigenvalue.", "tokens": [440, 661, 551, 281, 3637, 293, 321, 603, 536, 341, 562, 321, 483, 281, 264, 32784, 9284, 307, 300, 264, 32181, 3314, 295, 341, 3170, 307, 264, 8509, 295, 264, 6443, 10446, 29155, 281, 264, 1150, 6443, 10446, 29155, 13], "temperature": 0.0, "avg_logprob": -0.07957428192423883, "compression_ratio": 1.9285714285714286, "no_speech_prob": 3.373439540155232e-05}, {"id": 695, "seek": 580400, "start": 5818.0, "end": 5833.0, "text": " So that could be good or bad depending on what the eigenvalues are and there's something called adding shifts basically where you're kind of subtracting off one of the eigenvalues to move it over which speeds up the convergence and then you add that value back on once it's once it's converged.", "tokens": [407, 300, 727, 312, 665, 420, 1578, 5413, 322, 437, 264, 10446, 46033, 366, 293, 456, 311, 746, 1219, 5127, 19201, 1936, 689, 291, 434, 733, 295, 16390, 278, 766, 472, 295, 264, 10446, 46033, 281, 1286, 309, 670, 597, 16411, 493, 264, 32181, 293, 550, 291, 909, 300, 2158, 646, 322, 1564, 309, 311, 1564, 309, 311, 9652, 3004, 13], "temperature": 0.0, "avg_logprob": -0.07957428192423883, "compression_ratio": 1.9285714285714286, "no_speech_prob": 3.373439540155232e-05}, {"id": 696, "seek": 583300, "start": 5833.0, "end": 5845.0, "text": " And this this technique of shifts or deflation show up in a kind of number of numerical linear algebra algorithms.", "tokens": [400, 341, 341, 6532, 295, 19201, 420, 1060, 24278, 855, 493, 294, 257, 733, 295, 1230, 295, 29054, 8213, 21989, 14642, 13], "temperature": 0.0, "avg_logprob": -0.1105402974940058, "compression_ratio": 1.5934065934065933, "no_speech_prob": 1.983187576115597e-05}, {"id": 697, "seek": 583300, "start": 5845.0, "end": 5854.0, "text": " And so we were not going to get into great detail about them here, but just to be aware also that so deflation can be used to find eigenvalues are other than the greatest one.", "tokens": [400, 370, 321, 645, 406, 516, 281, 483, 666, 869, 2607, 466, 552, 510, 11, 457, 445, 281, 312, 3650, 611, 300, 370, 1060, 24278, 393, 312, 1143, 281, 915, 10446, 46033, 366, 661, 813, 264, 6636, 472, 13], "temperature": 0.0, "avg_logprob": -0.1105402974940058, "compression_ratio": 1.5934065934065933, "no_speech_prob": 1.983187576115597e-05}, {"id": 698, "seek": 585400, "start": 5854.0, "end": 5867.0, "text": " So here you know we were just finding the most largest eigenvalue the most significant eigenvector. But there are ways that this method could be modified to find others.", "tokens": [407, 510, 291, 458, 321, 645, 445, 5006, 264, 881, 6443, 10446, 29155, 264, 881, 4776, 10446, 303, 1672, 13, 583, 456, 366, 2098, 300, 341, 3170, 727, 312, 15873, 281, 915, 2357, 13], "temperature": 0.0, "avg_logprob": -0.2805501796581127, "compression_ratio": 1.4217687074829932, "no_speech_prob": 1.012935990729602e-05}, {"id": 699, "seek": 585400, "start": 5867.0, "end": 5869.0, "text": " Matthew.", "tokens": [12434, 13], "temperature": 0.0, "avg_logprob": -0.2805501796581127, "compression_ratio": 1.4217687074829932, "no_speech_prob": 1.012935990729602e-05}, {"id": 700, "seek": 585400, "start": 5869.0, "end": 5873.0, "text": " And he has the microphone.", "tokens": [400, 415, 575, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.2805501796581127, "compression_ratio": 1.4217687074829932, "no_speech_prob": 1.012935990729602e-05}, {"id": 701, "seek": 585400, "start": 5873.0, "end": 5880.0, "text": " So,", "tokens": [407, 11], "temperature": 0.0, "avg_logprob": -0.2805501796581127, "compression_ratio": 1.4217687074829932, "no_speech_prob": 1.012935990729602e-05}, {"id": 702, "seek": 588000, "start": 5880.0, "end": 5889.0, "text": " so intuitively that's just how kind of like how many iterations you're having to do to get to a reasonable answer.", "tokens": [370, 46506, 300, 311, 445, 577, 733, 295, 411, 577, 867, 36540, 291, 434, 1419, 281, 360, 281, 483, 281, 257, 10585, 1867, 13], "temperature": 0.0, "avg_logprob": -0.16426043877234825, "compression_ratio": 1.5748502994011977, "no_speech_prob": 9.605295781511813e-05}, {"id": 703, "seek": 588000, "start": 5889.0, "end": 5899.0, "text": " So you can think of that in a lot of cases that would be like how your air is decreasing with each iteration.", "tokens": [407, 291, 393, 519, 295, 300, 294, 257, 688, 295, 3331, 300, 576, 312, 411, 577, 428, 1988, 307, 23223, 365, 1184, 24784, 13], "temperature": 0.0, "avg_logprob": -0.16426043877234825, "compression_ratio": 1.5748502994011977, "no_speech_prob": 9.605295781511813e-05}, {"id": 704, "seek": 588000, "start": 5899.0, "end": 5907.0, "text": " So it's always a fraction between you.", "tokens": [407, 309, 311, 1009, 257, 14135, 1296, 291, 13], "temperature": 0.0, "avg_logprob": -0.16426043877234825, "compression_ratio": 1.5748502994011977, "no_speech_prob": 9.605295781511813e-05}, {"id": 705, "seek": 590700, "start": 5907.0, "end": 5915.0, "text": " I mean I guess if you I'll look into how it's formally defined. It's kind of often talked about in this like more intuitive way.", "tokens": [286, 914, 286, 2041, 498, 291, 286, 603, 574, 666, 577, 309, 311, 25983, 7642, 13, 467, 311, 733, 295, 2049, 2825, 466, 294, 341, 411, 544, 21769, 636, 13], "temperature": 0.0, "avg_logprob": -0.18436288833618164, "compression_ratio": 1.3923076923076922, "no_speech_prob": 2.6270146918250248e-05}, {"id": 706, "seek": 590700, "start": 5915.0, "end": 5926.0, "text": " But yeah I'll look at the formal definition of that.", "tokens": [583, 1338, 286, 603, 574, 412, 264, 9860, 7123, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.18436288833618164, "compression_ratio": 1.3923076923076922, "no_speech_prob": 2.6270146918250248e-05}, {"id": 707, "seek": 592600, "start": 5926.0, "end": 5938.0, "text": " Other questions.", "tokens": [5358, 1651, 13], "temperature": 0.0, "avg_logprob": -0.5416699137006488, "compression_ratio": 0.6666666666666666, "no_speech_prob": 1.55329178141983e-06}, {"id": 708, "seek": 593800, "start": 5938.0, "end": 5959.0, "text": " I hope for for next time I do want to cover the QR algorithm. I think that we won't finish all of this lesson seven notebook because we've run out of time a bit with the course and I want to I want to cover stuff in the lesson eight notebook.", "tokens": [286, 1454, 337, 337, 958, 565, 286, 360, 528, 281, 2060, 264, 32784, 9284, 13, 286, 519, 300, 321, 1582, 380, 2413, 439, 295, 341, 6898, 3407, 21060, 570, 321, 600, 1190, 484, 295, 565, 257, 857, 365, 264, 1164, 293, 286, 528, 281, 286, 528, 281, 2060, 1507, 294, 264, 6898, 3180, 21060, 13], "temperature": 0.0, "avg_logprob": -0.132390264737404, "compression_ratio": 1.5512820512820513, "no_speech_prob": 7.64555898058461e-06}, {"id": 709, "seek": 595900, "start": 5959.0, "end": 5975.0, "text": " So we'll see a little bit more here and the notebook will be up if you're interested in this topic and want to want to go further but we won't. I think it's very unlikely that we would cover the Arnoldi iteration at this point.", "tokens": [407, 321, 603, 536, 257, 707, 857, 544, 510, 293, 264, 21060, 486, 312, 493, 498, 291, 434, 3102, 294, 341, 4829, 293, 528, 281, 528, 281, 352, 3052, 457, 321, 1582, 380, 13, 286, 519, 309, 311, 588, 17518, 300, 321, 576, 2060, 264, 30406, 72, 24784, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.11946834401881441, "compression_ratio": 1.738396624472574, "no_speech_prob": 4.936457116855308e-06}, {"id": 710, "seek": 595900, "start": 5975.0, "end": 5987.0, "text": " Just given time. So definitely be sure to make sure you've downloaded the lesson eight notebook before before next time and we will do a little bit more with the lesson seven notebook.", "tokens": [1449, 2212, 565, 13, 407, 2138, 312, 988, 281, 652, 988, 291, 600, 21748, 264, 6898, 3180, 21060, 949, 949, 958, 565, 293, 321, 486, 360, 257, 707, 857, 544, 365, 264, 6898, 3407, 21060, 13], "temperature": 0.0, "avg_logprob": -0.11946834401881441, "compression_ratio": 1.738396624472574, "no_speech_prob": 4.936457116855308e-06}, {"id": 711, "seek": 598700, "start": 5987.0, "end": 5989.0, "text": " All right.", "tokens": [50364, 1057, 558, 13, 50464], "temperature": 0.0, "avg_logprob": -0.45804091294606525, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.00041276327101513743}], "language": "en"}